1
00:00:00,000 --> 00:00:02,400
go over that again

2
00:00:02,480 --> 00:00:06,080
we have here

3
00:00:06,170 --> 00:00:08,110
but it's a and b

4
00:00:08,210 --> 00:00:10,530
what it comes down from above

5
00:00:10,530 --> 00:00:12,240
i want to run through

6
00:00:12,290 --> 00:00:16,750
one of their runs out

7
00:00:16,820 --> 00:00:20,210
and we collect is one of rob's hearing but could be

8
00:00:21,220 --> 00:00:25,100
and this one is collected in buckets is also conductor

9
00:00:25,160 --> 00:00:26,990
the brain can be

10
00:00:26,990 --> 00:00:29,150
it's connected to see that's crucial

11
00:00:29,170 --> 00:00:31,950
and the paint can be is connected with the

12
00:00:31,980 --> 00:00:33,050
and here

13
00:00:33,130 --> 00:00:35,110
there are two balls

14
00:00:35,200 --> 00:00:36,280
which i can bring

15
00:00:36,280 --> 00:00:39,730
close together i run water

16
00:00:39,740 --> 00:00:41,990
and after a while i see is part

17
00:00:43,470 --> 00:00:46,920
i can even see is sparked when the distance is something like six millimetres we

18
00:00:46,960 --> 00:00:49,890
be about potential difference of about twenty

19
00:00:53,190 --> 00:00:55,830
how does it work

20
00:00:57,130 --> 00:00:59,860
water has a ph of seven

21
00:00:59,910 --> 00:01:03,280
that means one in ten two to seven molecule

22
00:01:03,280 --> 00:01:05,370
it's ionized

23
00:01:05,410 --> 00:01:08,510
i have always minus

24
00:01:08,520 --> 00:01:10,570
eight was

25
00:01:10,590 --> 00:01:14,390
those are going to be the current carriers

26
00:01:14,440 --> 00:01:19,450
ions are doing the work you are doing the job

27
00:01:19,530 --> 00:01:24,810
let's make an enlargement here of ten a

28
00:01:24,830 --> 00:01:28,560
and can they let us assume that purely by chance

29
00:01:28,610 --> 00:01:30,420
it has a little bit of

30
00:01:30,470 --> 00:01:34,590
the positive charge on it could be negative just with positive for

31
00:01:34,800 --> 00:01:37,810
here cases where you will see

32
00:01:37,890 --> 00:01:41,300
just by chance like you have a little bit of net charge and not completely

33
00:01:42,080 --> 00:01:46,650
so this can has a little bit of positive x-factor structure

34
00:01:46,730 --> 00:01:48,570
now here's the drop

35
00:01:48,630 --> 00:01:51,390
from the spout

36
00:01:51,440 --> 00:01:53,150
what's going to happen

37
00:01:53,220 --> 00:01:56,150
through induction polarisation

38
00:01:56,170 --> 00:02:00,050
you get a little bit of extra excess negative charge here a little bit of

39
00:02:00,050 --> 00:02:01,510
access positive there

40
00:02:01,610 --> 00:02:05,690
because the positive repel each other and the negative being attracted

41
00:02:05,690 --> 00:02:08,900
so it explores goes a little bit up and the always minus comes a little

42
00:02:08,900 --> 00:02:09,750
bit down

43
00:02:09,800 --> 00:02:11,980
but now to drop breaks

44
00:02:11,990 --> 00:02:13,440
and he goes to draw

45
00:02:13,530 --> 00:02:16,710
so it's a little bit negative

46
00:02:16,750 --> 00:02:19,830
so now a little bit of negative drop

47
00:02:19,860 --> 00:02:20,890
come down

48
00:02:20,900 --> 00:02:24,240
so this becomes negative

49
00:02:24,340 --> 00:02:26,350
but this is connected with me

50
00:02:26,370 --> 00:02:28,870
toby becomes negative

51
00:02:28,880 --> 00:02:32,660
what do you think is going to happen i was the drops that fall through

52
00:02:34,000 --> 00:02:36,300
they are going to become positive

53
00:02:36,310 --> 00:02:38,860
because if b is negative

54
00:02:38,960 --> 00:02:42,160
then of course this will be reversed the bottom will be positive the torque will

55
00:02:42,160 --> 00:02:43,200
be negative

56
00:02:43,270 --> 00:02:46,090
so those drops now to the going to fall through

57
00:02:46,110 --> 00:02:47,770
i'm going to be positive

58
00:02:47,820 --> 00:02:50,300
closely becomes positive

59
00:02:50,300 --> 00:02:52,740
but c is connected with a

60
00:02:52,740 --> 00:02:55,120
so a becomes more positive

61
00:02:55,140 --> 00:02:57,720
and so a can do even better job now

62
00:02:57,720 --> 00:02:58,670
on these

63
00:02:58,690 --> 00:03:01,330
one of the polarizing even more

64
00:03:01,390 --> 00:03:03,610
so you get a runaway process

65
00:03:03,620 --> 00:03:06,010
and the whole system feeds on itself

66
00:03:07,030 --> 00:03:09,000
the potential difference here

67
00:03:09,020 --> 00:03:10,160
become so high

68
00:03:10,190 --> 00:03:13,090
thank you exceed three million volt meter

69
00:03:13,110 --> 00:03:15,190
and then you get a breakdown

70
00:03:15,230 --> 00:03:17,810
and you see is part

71
00:03:17,900 --> 00:03:21,140
you can think of a continuous stream of water

72
00:03:21,190 --> 00:03:23,570
as a stream of individual dropped

73
00:03:23,590 --> 00:03:25,120
so it also works

74
00:03:25,190 --> 00:03:30,350
if you just have a regular stream of water going down

75
00:03:30,400 --> 00:03:32,540
who is doing the work here

76
00:03:32,590 --> 00:03:34,620
someone has to do the work

77
00:03:34,670 --> 00:03:36,140
you have a battery

78
00:03:36,150 --> 00:03:38,000
the battery is being charged

79
00:03:38,000 --> 00:03:39,720
and then it is discharged

80
00:03:39,730 --> 00:03:42,470
so this part was going to work

81
00:03:42,480 --> 00:03:43,810
any idea

82
00:03:43,830 --> 00:03:45,690
if you thought about that

83
00:03:48,280 --> 00:03:49,220
very good

84
00:03:49,270 --> 00:03:51,650
granted that is doing the work

85
00:03:51,710 --> 00:03:53,770
we can see that very easily

86
00:03:55,940 --> 00:03:56,890
the current

87
00:03:56,920 --> 00:04:01,260
that is flowing and the electric field

88
00:04:01,290 --> 00:04:02,870
how is the current flowing

89
00:04:02,950 --> 00:04:05,770
if the negative charge is going down

90
00:04:05,830 --> 00:04:09,090
we all agree that the current is going up

91
00:04:09,140 --> 00:04:11,410
if positive charges going down

92
00:04:11,460 --> 00:04:15,520
we agreed that the current is going down

93
00:04:15,550 --> 00:04:17,520
this side of the out

94
00:04:18,560 --> 00:04:21,880
will be slightly positive this is slightly negative

95
00:04:21,900 --> 00:04:24,700
because the age plus

96
00:04:24,720 --> 00:04:26,640
more abundant here than there

97
00:04:26,710 --> 00:04:29,830
so we're going to get the current through the water

98
00:04:29,920 --> 00:04:32,200
this direction

99
00:04:32,200 --> 00:04:33,190
x and y

100
00:04:33,200 --> 00:04:36,330
of the loss between the predicted

101
00:04:36,330 --> 00:04:38,300
g of x and the true

102
00:04:39,850 --> 00:04:42,050
and the cumulative error would be

103
00:04:42,070 --> 00:04:43,960
as i said before the summer

104
00:04:44,000 --> 00:04:48,750
of the errors made at each prediction so at each data point you make a

105
00:04:48,750 --> 00:04:50,320
prediction and you compute the

106
00:04:52,570 --> 00:04:55,040
in the online setting at least you would allow

107
00:04:55,070 --> 00:04:56,960
g to change

108
00:04:56,980 --> 00:05:01,140
as time goes so as you get more instances would allow you to change so

109
00:05:01,140 --> 00:05:02,820
it should be more GI

110
00:05:02,830 --> 00:05:05,260
written here

111
00:05:05,380 --> 00:05:11,920
OK know the notation i will use g n as to denote my the function

112
00:05:11,920 --> 00:05:14,620
returned by my my algorithm

113
00:05:14,630 --> 00:05:20,190
so the the small and will will mean that's my GENI depends on the data

114
00:05:20,210 --> 00:05:23,890
on this on the training set of data of size n

115
00:05:23,910 --> 00:05:26,570
OK and you can given the loss

116
00:05:26,600 --> 00:05:30,210
defining this or this way you can define the optimal

117
00:05:30,590 --> 00:05:31,830
or the function

118
00:05:31,830 --> 00:05:34,240
at the the optimal loss

119
00:05:34,260 --> 00:05:36,100
by just are

120
00:05:40,420 --> 00:05:41,960
if you

121
00:05:42,000 --> 00:05:44,200
river rewrite using this notation

122
00:05:44,210 --> 00:05:46,310
the quantities before

123
00:05:46,320 --> 00:05:51,170
so in the case of model identification so

124
00:05:51,190 --> 00:05:55,790
what is in blue is what i consider and these as also possibilities which i

125
00:05:55,790 --> 00:05:58,180
don't consider

126
00:05:58,200 --> 00:06:01,470
if you do motherly identification you comparing

127
00:06:01,480 --> 00:06:05,420
gn with stories from star here

128
00:06:05,430 --> 00:06:08,550
and you have some kind of this tends to be between those two functions and

129
00:06:08,550 --> 00:06:10,280
you try to minimize it

130
00:06:11,090 --> 00:06:14,860
here we do not risk minimisation so we try to minimize the loss

131
00:06:14,880 --> 00:06:18,310
sorry trilogy here

132
00:06:18,350 --> 00:06:21,960
OK so and

133
00:06:21,970 --> 00:06:24,800
so you should mean over g

134
00:06:24,840 --> 00:06:26,790
of the loss k

135
00:06:26,860 --> 00:06:30,080
then you have

136
00:06:30,090 --> 00:06:33,410
what they call the average case approach where

137
00:06:33,430 --> 00:06:35,430
what you try to minimize it

138
00:06:35,440 --> 00:06:37,290
the sum over

139
00:06:37,300 --> 00:06:42,550
a certain set of possible distributions are possible generation mechanisms

140
00:06:42,570 --> 00:06:47,140
of the difference between the loss of terrorism and the loss of the best one

141
00:06:47,170 --> 00:06:50,780
the minimax approach

142
00:06:50,790 --> 00:06:57,090
you take the maximum over this restricted set of probability distributions and the regret approach

143
00:06:57,100 --> 00:07:01,720
you take the maximum over all possible distributions but you compare

144
00:07:01,730 --> 00:07:05,890
you're lost with the loss of functions in the reference class

145
00:07:06,040 --> 00:07:07,250
capital g

146
00:07:07,610 --> 00:07:11,510
that makes sense for

147
00:07:15,710 --> 00:07:23,840
OK so this is a small technical point but it's quite important so when you

148
00:07:23,840 --> 00:07:28,210
consider such a quantity and you try to prove lower bounds but something that you

149
00:07:28,210 --> 00:07:30,060
may encounter sometimes

150
00:07:30,080 --> 00:07:34,660
what you do is you construct as i said before bad

151
00:07:34,710 --> 00:07:41,130
distributions are given another reason you construct the distribution that will make this algorithm make

152
00:07:41,340 --> 00:07:44,170
as many mistakes as possible

153
00:07:45,730 --> 00:07:49,100
you can construct this p in in a way that depends

154
00:07:49,120 --> 00:07:51,320
on the sample size or not

155
00:07:51,330 --> 00:07:54,960
so even if you do it in a way that depend on the sample size

156
00:07:54,960 --> 00:07:56,850
you what is called the week

157
00:07:56,870 --> 00:07:58,090
lower bound

158
00:07:58,090 --> 00:08:02,120
where if if the but he does not depend on the sample size you get

159
00:08:02,120 --> 00:08:05,240
the stronger bond this it's more interesting because

160
00:08:06,650 --> 00:08:10,320
the problem is fixed so p fixed but what is not fixed is the sample

161
00:08:10,320 --> 00:08:14,150
size you can get more instances of a given problem in what you're interested in

162
00:08:14,150 --> 00:08:18,360
is whether when the problem is fixed and you get more instances whether you will

163
00:08:18,360 --> 00:08:23,840
eventually be able to get to low value for this quantity or not

164
00:08:23,860 --> 00:08:27,590
so we strongly robots are more interesting

165
00:08:32,380 --> 00:08:34,090
i want to finish

166
00:08:34,110 --> 00:08:36,070
by giving you

167
00:08:36,080 --> 00:08:41,470
a few statements of kind of negative results in that setting so once you have

168
00:08:41,520 --> 00:08:46,150
set of these all these notation you can show several things

169
00:08:46,180 --> 00:08:51,170
what are the which are called the no free lunch theorems a simple one is

170
00:08:51,170 --> 00:08:54,920
so this is kind of the area reach after a certain number of epochs for

171
00:08:54,920 --> 00:09:00,690
for a number of votes for is the size of the learning right and this

172
00:09:00,690 --> 00:09:03,850
is the predicted value for money right exactly

173
00:09:03,880 --> 00:09:07,080
is the minimum that means that it predicts the optimality right

174
00:09:07,100 --> 00:09:10,520
these ideas as as you want

175
00:09:10,520 --> 00:09:13,000
it works for four

176
00:09:13,020 --> 00:09:15,880
members things also

177
00:09:16,620 --> 00:09:21,170
i think i'm going to stop him at the time and perhaps some question thank

178
00:09:21,170 --> 00:09:22,850
you for your attention

179
00:09:33,080 --> 00:09:34,810
i think this

180
00:09:34,830 --> 00:09:37,360
there was

181
00:09:37,380 --> 00:09:43,440
OK so also across networks have their own set of problems that actually chose to

182
00:09:43,540 --> 00:09:48,730
quite extensively so we have the initial banjo had some papers that showed that if

183
00:09:48,730 --> 00:09:51,920
you want those because something really useful

184
00:09:51,940 --> 00:09:53,940
one is going to be so intrinsically hard

185
00:09:53,960 --> 00:09:58,810
so that's sort of put a really big damper on on people work on on

186
00:09:58,810 --> 00:10:02,170
our current that some people claim to have algorithms that work pretty well but it's

187
00:10:02,170 --> 00:10:02,940
not clear

188
00:10:02,960 --> 00:10:07,060
so the so there was something is they work well i mean i'm not an

189
00:10:07,060 --> 00:10:10,540
those techniques necessary applied

190
00:10:10,540 --> 00:10:13,920
i think it's

191
00:10:13,940 --> 00:10:19,580
in addition

192
00:10:22,210 --> 00:10:24,900
used to specify

193
00:10:24,900 --> 00:10:28,100
OK so guess that know

194
00:10:28,100 --> 00:10:32,150
the remarkable is sort of orthogonal to choosing the right step size

195
00:10:32,170 --> 00:10:36,230
we like to do is basically allows you to if you have to consider that

196
00:10:36,230 --> 00:10:42,480
oliver broccoli does any preconditions the space so is gonna essentially had the effect of

197
00:10:42,480 --> 00:10:44,080
artificially increasing the

198
00:10:44,080 --> 00:10:48,830
running rating dimensions whether the loss has low low curvature and to decrease its interactions

199
00:10:48,830 --> 00:10:52,460
with the has high curvature to like preconditioning then you still have to choose the

200
00:10:52,460 --> 00:10:55,400
constant there was also the

201
00:10:55,420 --> 00:10:58,930
the step size k which is kind of a global constant global multiplayer for all

202
00:10:58,930 --> 00:11:05,310
of those all of those dates and for that you can use the technique that

203
00:11:05,540 --> 00:11:08,080
we all mentioned or this technique of

204
00:11:08,170 --> 00:11:11,560
stochastic estimation of it because out the

205
00:11:11,790 --> 00:11:16,270
i don't use that very often because heuristics the heuristic way so simple and it

206
00:11:16,270 --> 00:11:21,540
works well enough that is really not worth it

207
00:11:36,790 --> 00:11:41,190
well it depends what you're interested in so

208
00:11:41,270 --> 00:11:42,920
so if you're interested in

209
00:11:44,040 --> 00:11:48,350
you know some of the industry applications data mining you know probabilistic regression is going

210
00:11:48,350 --> 00:11:51,520
to do a good job for use of syllogistic logic action

211
00:11:51,730 --> 00:11:55,940
but if we interested in is building intelligent machines

212
00:11:55,980 --> 00:12:00,270
then please is to regression find something else but so that's where you really have

213
00:12:00,270 --> 00:12:04,330
to think about more more sophisticated models

214
00:12:04,600 --> 00:12:09,150
like city planning writes say a unsupervised learning for canopy training data that where there

215
00:12:09,150 --> 00:12:13,900
was a whole you know so like special session on this so you may have

216
00:12:13,940 --> 00:12:15,460
now i have attended

217
00:12:15,480 --> 00:12:18,330
that's kind of

218
00:12:18,350 --> 00:12:22,440
the best hope we have at the moment said this is pretty much the only

219
00:12:22,440 --> 00:12:25,920
one to

220
00:12:25,960 --> 00:12:28,650
he was

221
00:12:34,080 --> 00:12:35,500
right so you

222
00:12:35,500 --> 00:12:38,650
so thing to explain again way

223
00:12:39,350 --> 00:12:46,730
with that was kind of the argument i i tried to make which is that

224
00:12:46,750 --> 00:12:49,490
you know this could be a lot of it with a large that this would

225
00:12:49,490 --> 00:12:52,400
be a lot more dimensions that are

226
00:12:52,420 --> 00:12:54,130
or more directions that are

227
00:12:54,150 --> 00:12:55,980
the considered solutions

228
00:12:56,000 --> 00:12:58,500
and in the limit is

229
00:12:58,500 --> 00:13:02,420
is that images were the first layer for example the two layer neural net is

230
00:13:02,420 --> 00:13:05,290
very very large so large that all the way already there so no need to

231
00:13:05,290 --> 00:13:09,380
run them at all the big the problem becomes effectively combats

232
00:13:09,400 --> 00:13:12,250
not technically convex but certainly convex

233
00:13:12,270 --> 00:13:16,940
so that's going to discuss the argument that there is this image this analogy make

234
00:13:17,560 --> 00:13:23,350
if you think about a nearly square because we're trying to is easy something in

235
00:13:23,350 --> 00:13:31,250
trying to find the best you square solution to two regression problem for example if

236
00:13:31,270 --> 00:13:34,580
you have fewer samples new dimensions is very simple to solve

237
00:13:34,580 --> 00:13:39,190
if you have more more samples and dimension is very simple to solve right at

238
00:13:39,190 --> 00:13:43,980
the time when you have exactly the number of samples is dimensions then the has

239
00:13:45,040 --> 00:13:47,690
extremely ill conditioned right at that point

240
00:13:47,710 --> 00:13:50,210
it's actually a condition number is infinity

241
00:13:50,230 --> 00:13:55,540
the you know is algorithms converge it's really bad when when the system is just

242
00:13:55,540 --> 00:14:00,730
the right size it's really that it's much better this way over private which we

243
00:14:00,730 --> 00:14:02,650
in which case you need to regularize

244
00:14:02,690 --> 00:14:06,380
or if its way onto parameterized which case it's going to self realized

245
00:14:06,400 --> 00:14:10,900
so what is the start is that you make a system where the goal is

246
00:14:10,900 --> 00:14:13,710
to be new regularizer help

247
00:14:13,730 --> 00:14:17,440
OK and people neural nets didn't didn't used to do this because

248
00:14:17,460 --> 00:14:21,830
we already washed by the fact that these decisions were telling us you know you

249
00:14:21,830 --> 00:14:24,690
shouldn't have parameters and you have to examples

250
00:14:24,710 --> 00:14:26,060
it's just not true

251
00:14:26,060 --> 00:14:30,000
you want a lot of parameters in training samples and industry arise

252
00:14:33,520 --> 00:14:36,790
here one later

253
00:14:36,810 --> 00:14:40,650
nine years and they are an example

254
00:14:40,750 --> 00:14:47,860
so because of this experiment to make right turns so to say that actually happened

255
00:14:47,860 --> 00:14:54,440
tried but could be written about five minutes but no i mean something is not

256
00:14:54,440 --> 00:14:58,270
right for example is with gaussian that if you if you make them bigger secrets

257
00:14:59,210 --> 00:15:01,940
maps for example to very large numbers

258
00:15:02,000 --> 00:15:06,750
two situation for example you have ten times more parameters in the model and you

259
00:15:06,750 --> 00:15:08,020
have training samples

260
00:15:08,040 --> 00:15:10,460
they actually work better

261
00:15:10,520 --> 00:15:13,380
you your training

262
00:15:17,500 --> 00:15:21,920
optimism what you get

263
00:15:21,940 --> 00:15:25,290
if you think wins is reasonably effective for

264
00:15:25,340 --> 00:15:27,000
o is free

265
00:15:27,020 --> 00:15:28,790
i think

266
00:15:28,850 --> 00:15:32,690
maybe we should start with three lives

267
00:15:32,710 --> 00:15:36,360
which is to think about what i like

268
00:15:37,040 --> 00:15:42,440
but that's the reason it's OK

269
00:15:42,520 --> 00:15:43,600
OK so

270
00:15:43,630 --> 00:15:46,790
so the question was you know if

271
00:15:46,790 --> 00:15:50,940
if there's a reasonably good methods for knowledge free supervised learning and then we should

272
00:15:52,230 --> 00:15:53,770
try to push the

273
00:15:53,810 --> 00:15:56,730
towards to supervise running

274
00:15:56,810 --> 00:16:00,100
and we could we would be able to solve more complex problems so first of

275
00:16:00,710 --> 00:16:05,130
i i don't believe there is any knowledge three supervised learning methods that just doesn't

276
00:16:05,130 --> 00:16:10,040
exist is it's not knowledge free and it's not unbiased so that certain

277
00:16:10,080 --> 00:16:13,520
yeah it looks like this general purpose the same where we like to think that

278
00:16:13,520 --> 00:16:18,190
our brains general purpose but it's not true at all it's very very very specialized

279
00:16:18,210 --> 00:16:21,920
and so there are certain types of functions so with the gas and for kernel

280
00:16:21,920 --> 00:16:22,900
for example

281
00:16:24,460 --> 00:16:27,130
efficiently implement certain types of functions

282
00:16:27,130 --> 00:16:30,360
i would be would be horribly inefficient for all kinds of functions is likely to

283
00:16:30,360 --> 00:16:34,440
be inefficient for many many more functions than it will be efficient you know that

284
00:16:34,440 --> 00:16:36,380
the number of functions to be efficient for

285
00:16:36,400 --> 00:16:40,130
so every model is restricted in fact there is no free lunch

286
00:16:40,150 --> 00:16:43,690
there that show that you know if you have a generic learning model is really

287
00:16:43,690 --> 00:16:47,860
general actually done anything because you know there's is no it's going to previously mentioned

288
00:16:47,860 --> 00:16:53,420
about so so every model specific and the fact that we think it's generally sit

289
00:16:53,420 --> 00:16:54,730
in vision

290
00:16:55,860 --> 00:16:58,310
this is not

291
00:16:59,080 --> 00:17:01,230
so there

292
00:17:02,290 --> 00:17:04,920
like it's random

293
00:17:05,020 --> 00:17:07,210
fictitious knowledge

294
00:17:07,210 --> 00:17:11,170
so it should be here can be fitted better or equally in this case

295
00:17:11,220 --> 00:17:14,570
all data can be fit perfectly so he called the whole

296
00:17:15,650 --> 00:17:16,880
the size of

297
00:17:20,510 --> 00:17:22,590
of sample space

298
00:17:22,620 --> 00:17:24,610
the rank is large again OK now

299
00:17:24,620 --> 00:17:26,400
take inappropriate

300
00:17:27,700 --> 00:17:29,060
assume the data

301
00:17:29,070 --> 00:17:31,870
like parabola

302
00:17:31,890 --> 00:17:35,670
and you ask regret quadratic fit

303
00:17:37,490 --> 00:17:40,980
quite adequately norms are quite rigid class i mean it's not very flexible can only

304
00:17:42,100 --> 00:17:43,730
quadratic data

305
00:17:44,850 --> 00:17:48,410
and you data are quadratic so did quite well

306
00:17:48,520 --> 00:17:51,710
that means not too many other d prime

307
00:17:51,720 --> 00:17:53,560
can be fitted as well

308
00:17:53,640 --> 00:17:56,130
so the rank will be small

309
00:17:56,360 --> 00:17:59,640
and the natural thing to do is now you select the model complexity that has

310
00:17:59,640 --> 00:18:01,860
many mineral offering

311
00:18:04,630 --> 00:18:06,060
so counting

312
00:18:06,250 --> 00:18:12,110
all the because data seems like the daunting task but for linear basis function regression

313
00:18:12,450 --> 00:18:17,770
you have a closed form solution which just involves the computation of the determinant

314
00:18:17,770 --> 00:18:22,080
you can i mean what first accounting it gets to measure i mean you look

315
00:18:22,080 --> 00:18:26,130
for the volume of the volume in sweden which is just reduced to determine you

316
00:18:26,130 --> 00:18:28,560
can compute it efficiently

317
00:18:30,590 --> 00:18:32,330
the maximum

318
00:18:32,350 --> 00:18:36,970
like the palace maximum likelihood like i big and MDL

319
00:18:38,740 --> 00:18:41,560
log only depends on the regression

320
00:18:45,900 --> 00:18:49,170
when is for k nearest neighbour we only have the function f had we don't

321
00:18:49,170 --> 00:18:52,310
start with the underlying class optimise so you start

322
00:18:52,330 --> 00:18:55,250
with a hat

323
00:18:55,300 --> 00:18:57,370
it also works for k nearest neighbour

324
00:18:58,440 --> 00:19:04,380
it depends on your loss function takes the loss function to counter and the

325
00:19:04,410 --> 00:19:05,320
does it

326
00:19:05,330 --> 00:19:06,590
sort of

327
00:19:06,610 --> 00:19:08,400
on the right

328
00:19:09,600 --> 00:19:11,960
it works without a stochastic noise model

329
00:19:12,010 --> 00:19:14,850
mean bayesian rationally just across the more

330
00:19:14,910 --> 00:19:15,820
noise model

331
00:19:15,870 --> 00:19:17,450
and here

332
00:19:17,490 --> 00:19:19,760
you just need the regression function

333
00:19:19,770 --> 00:19:25,860
so the polynomial or the k nearest neighbour regression

334
00:19:27,200 --> 00:19:31,790
the most important thing is you can directly apply to nonparametric regression like

335
00:19:31,800 --> 00:19:37,020
that cannot

336
00:19:37,020 --> 00:19:38,830
so that was the

337
00:19:38,850 --> 00:19:44,100
brief introduction to model selection so remember two ways empirical model selection which is more

338
00:19:44,100 --> 00:19:46,240
faithful to cross validation

339
00:19:46,330 --> 00:19:51,390
a theoretical model selection which is often associated with the big time

340
00:19:58,770 --> 00:20:03,610
next is how we have take real problems

341
00:20:03,940 --> 00:20:10,440
the real world is called the place data is complex all these simplistic methods

342
00:20:10,440 --> 00:20:12,170
i'm not really useful

343
00:20:12,200 --> 00:20:15,560
there are various ways of going to

344
00:20:15,610 --> 00:20:19,110
complex domains one is one powerful and

345
00:20:19,390 --> 00:20:23,350
systematic one is probabilistic graphical models bayesian networks

346
00:20:23,430 --> 00:20:26,740
undirected bayesian networks

347
00:20:28,160 --> 00:20:29,440
three models

348
00:20:29,560 --> 00:20:30,990
other nonparametric

349
00:20:34,330 --> 00:20:36,780
if you have density estimation problems

350
00:20:36,780 --> 00:20:40,610
you can use approximate inference variational methods

351
00:20:40,640 --> 00:20:43,070
sampling methods very important

352
00:20:43,620 --> 00:20:48,600
and at the end i will talk about how to combine different models

353
00:20:48,640 --> 00:20:49,780
quite different

354
00:20:49,790 --> 00:20:54,190
o point is posting

355
00:20:54,230 --> 00:20:56,330
OK graphical models

356
00:20:56,390 --> 00:21:00,110
yes there ought to be a whole lecture

357
00:21:00,270 --> 00:21:03,980
by to be area about graphical models can be brief year

358
00:21:06,170 --> 00:21:11,010
have a feature vector x one x two x three two x seven

359
00:21:14,040 --> 00:21:17,250
so it's a classification problem

360
00:21:17,260 --> 00:21:18,780
spam not spam

361
00:21:18,790 --> 00:21:24,460
and you want to infer the probability that the email is spam so what you

362
00:21:24,460 --> 00:21:26,450
have is

363
00:21:26,460 --> 00:21:36,000
now let's let's the density estimation problem

364
00:21:36,000 --> 00:21:37,730
OK so you want to infer

365
00:21:37,770 --> 00:21:38,760
so you have your

366
00:21:38,800 --> 00:21:40,320
you have fewer

367
00:21:40,350 --> 00:21:43,700
i mean you could regard seven class level if you want to

368
00:21:43,740 --> 00:21:45,620
but it doesn't really matter so

369
00:21:45,730 --> 00:21:49,010
you have a feature vector x one x seven here

370
00:21:49,240 --> 00:21:54,370
a lot of data and you want to infer the probability distribution from which samples

371
00:21:54,370 --> 00:21:56,460
the next one two

372
00:21:59,600 --> 00:22:07,500
if you look at that as assume you features are binary

373
00:22:07,800 --> 00:22:10,630
i mean the most simple case there are binary

374
00:22:10,690 --> 00:22:12,560
then you have to go to the

375
00:22:12,660 --> 00:22:16,010
the other thing is example two to the seventh

376
00:22:16,010 --> 00:22:18,570
this how the twenty eight possible inputs

377
00:22:19,560 --> 00:22:22,630
i mean these probabilities in the interval zero one

378
00:22:22,660 --> 00:22:24,300
but assume that just be

379
00:22:26,350 --> 00:22:28,980
number zero and one

380
00:22:29,030 --> 00:22:30,150
there are one

381
00:22:30,210 --> 00:22:33,490
then you would have to go to the power of hundred twenty eight

382
00:22:33,540 --> 00:22:35,030
possible functions

383
00:22:35,040 --> 00:22:36,970
so there already a huge space

384
00:22:37,560 --> 00:22:38,890
roughly you could say

385
00:22:38,910 --> 00:22:42,150
to reliably for this function if you take the whole cluster space you need the

386
00:22:42,150 --> 00:22:44,130
sample size of two

387
00:22:44,140 --> 00:22:46,710
on the twenty eighth which you

388
00:22:46,710 --> 00:22:47,670
never have

389
00:22:47,690 --> 00:22:52,640
because it's close to the size of the universe already

390
00:22:52,670 --> 00:22:54,290
so what you need

391
00:22:54,300 --> 00:22:59,000
again and again you need either a restricted model class war

392
00:22:59,010 --> 00:23:01,210
some other selection criteria

393
00:23:01,260 --> 00:23:03,300
and one nice way

394
00:23:05,040 --> 00:23:07,160
i respect your space is

395
00:23:07,220 --> 00:23:10,920
to consider graphical models of bayes networks so

396
00:23:12,490 --> 00:23:16,140
you have some understanding or not you can learn that

397
00:23:16,160 --> 00:23:20,100
how these very are related to each other for instance they are

398
00:23:20,110 --> 00:23:21,220
i know that

399
00:23:21,220 --> 00:23:24,410
there is you no just directly influenced like from

400
00:23:24,510 --> 00:23:28,870
the number in laughlin porn for instance

401
00:23:28,900 --> 00:23:32,380
but the indirect words you know which

402
00:23:32,440 --> 00:23:37,240
contribute to the probability of of these words and so on and you could have

403
00:23:37,240 --> 00:23:39,340
some specific models or

404
00:23:40,080 --> 00:23:40,860
and that

405
00:23:40,870 --> 00:23:42,740
consort with them

406
00:23:44,240 --> 00:23:47,110
implies console you have some medical model

407
00:23:47,240 --> 00:23:51,480
which you can write down here is a graphical models one

408
00:23:51,480 --> 00:23:52,850
which effectively

409
00:23:52,850 --> 00:23:54,640
you might do related theory

410
00:23:54,650 --> 00:23:57,550
and and up with the bound which is so far

411
00:23:57,560 --> 00:23:59,670
the true performance you're likely to see

412
00:23:59,680 --> 00:24:02,700
that is not very useful to tell the practitioner well i'm not going to do

413
00:24:02,700 --> 00:24:07,160
worse than the universe exploding and it's not very useful thing so

414
00:24:07,170 --> 00:24:10,390
that has for a long time been the problem of statistical learning theory but in

415
00:24:10,390 --> 00:24:13,770
certain settings they do have nontrivial bounds that are useful

416
00:24:14,240 --> 00:24:23,560
and i think it's becoming more exciting area

417
00:24:23,590 --> 00:24:25,800
there's also another philosophical

418
00:24:25,820 --> 00:24:27,880
can which

419
00:24:27,900 --> 00:24:30,780
can be referred to as the bayesian view of some people would just say that

420
00:24:30,780 --> 00:24:35,300
it's probabilistic modeling view and a lot more my work has been in this area

421
00:24:36,510 --> 00:24:37,690
you should

422
00:24:37,720 --> 00:24:42,000
probably downweight anything i say about this error up weight anything that sort of statistical

423
00:24:42,000 --> 00:24:44,880
learning theory

424
00:24:47,360 --> 00:24:49,970
the bayesian says what steps back and says

425
00:24:50,720 --> 00:24:54,930
what we really doing is is inference if

426
00:24:54,980 --> 00:24:58,860
if we are trying to use data from the world to set a bunch of

427
00:24:58,860 --> 00:25:02,400
numbers in a computer program a computer program is because we don't know what the

428
00:25:02,500 --> 00:25:03,450
number should be

429
00:25:04,570 --> 00:25:06,730
what we're trying to do

430
00:25:08,440 --> 00:25:10,510
work out what those numbers should be

431
00:25:10,550 --> 00:25:13,770
but we did have some idea beforehand

432
00:25:13,770 --> 00:25:16,990
what the weight vector should be this whole idea of regularisation which turns out to

433
00:25:16,990 --> 00:25:18,340
be important says

434
00:25:18,410 --> 00:25:22,850
i have some prior beliefs about what a good system would look like

435
00:25:22,900 --> 00:25:26,720
and then given those probabilities about what a good system should look like getting data

436
00:25:26,750 --> 00:25:28,740
and that data is the future

437
00:25:28,900 --> 00:25:34,760
and under certain assumptions assumptions if you're doing something of that flavour taking prior beliefs

438
00:25:34,800 --> 00:25:37,760
observing data and coming up with this terrible

439
00:25:37,860 --> 00:25:42,220
there is only one way to do which is to use bayesian statistics

440
00:25:42,240 --> 00:25:44,630
i emphasise under certain assumptions

441
00:25:44,670 --> 00:25:49,820
those assumptions arguably don't always hold in machine learning setting there are cases where i

442
00:25:49,820 --> 00:25:52,800
think the basic idea is certainly a very useful one thing

443
00:25:54,420 --> 00:25:56,060
i'm going to give you

444
00:25:56,170 --> 00:25:57,620
very quickly

445
00:25:57,660 --> 00:25:59,430
overview of

446
00:25:59,480 --> 00:26:02,270
roughly what the bayesian view on machine learning is

447
00:26:03,390 --> 00:26:07,370
given the heat of the day and that we haven't had the introductory mass

448
00:26:07,380 --> 00:26:08,820
lecture yet

449
00:26:08,850 --> 00:26:12,390
i don't expect everybody to follow this but i would like to ask questions if

450
00:26:12,390 --> 00:26:13,170
you don't

451
00:26:13,190 --> 00:26:16,220
and if you can't follow everything i'm saying

452
00:26:16,230 --> 00:26:20,440
then the graphical model selection will probably reveal a lot of this material so hopefully

453
00:26:20,440 --> 00:26:23,170
you'll just get some flavour so that when you see this again it will make

454
00:26:23,170 --> 00:26:25,660
more sense the second time

455
00:26:28,410 --> 00:26:33,930
we have a model of this form where we have a weight vector which is

456
00:26:33,980 --> 00:26:36,340
the length of

457
00:26:36,360 --> 00:26:39,530
this matrix have a weight vector that this long we have one weight for every

458
00:26:43,160 --> 00:26:46,470
we have some prior beliefs about what that weight vector might be like

459
00:26:46,520 --> 00:26:49,440
it's probably not too big we might think so

460
00:26:49,450 --> 00:26:52,570
that's exactly the view fertilization so

461
00:26:52,600 --> 00:26:57,800
in the bayesian view we do everything with probability distribution so instead of just saying

462
00:26:57,800 --> 00:27:02,340
i'd like to penalize large values what i do is i put down what's called

463
00:27:02,340 --> 00:27:04,520
the prior probability distribution

464
00:27:04,520 --> 00:27:08,490
here w that just says

465
00:27:08,550 --> 00:27:12,720
what my beliefs about what might be so if i only had one way

466
00:27:12,770 --> 00:27:15,870
i could use something like a guassian distribution

467
00:27:15,880 --> 00:27:21,190
centered around there that says very high probability that the value is somewhere near zero

468
00:27:21,600 --> 00:27:25,630
and very small probability that we have some large value sort of

469
00:27:25,640 --> 00:27:27,600
more than three say

470
00:27:27,640 --> 00:27:32,230
and for a vector we can just use some multi very guessing distribution

471
00:27:32,320 --> 00:27:36,890
i guess in prior over everywhere

472
00:27:38,500 --> 00:27:42,310
what bayesian story that does is

473
00:27:45,700 --> 00:27:48,970
OK you specify your prior now you're allowed to look at the dataset so the

474
00:27:48,970 --> 00:27:52,930
dataset it's made exactly the vector y and i just calling that the and all

475
00:27:52,930 --> 00:27:53,980
of his life

476
00:27:54,020 --> 00:27:55,620
so now we can say well

477
00:27:55,620 --> 00:27:58,640
distribution over what the weight i like now

478
00:27:58,660 --> 00:28:00,910
given that we've seen the data

479
00:28:00,920 --> 00:28:05,130
and that is given by bayes rule

480
00:28:05,820 --> 00:28:07,820
it's proportional to

481
00:28:07,870 --> 00:28:11,450
our prior beliefs about what the weights were before we saw the data

482
00:28:11,510 --> 00:28:13,310
multiplied by

483
00:28:13,350 --> 00:28:18,500
how probable the data that we've seen are given that particular setting of the weight

484
00:28:19,350 --> 00:28:25,550
so for any given weight we can score how could that weighted by combining and

485
00:28:25,550 --> 00:28:28,830
how well it matches the data

486
00:28:31,350 --> 00:28:35,630
then it might be that one particular way tapestry distribution

487
00:28:35,650 --> 00:28:40,820
is tighter because we know more and it might be centered around the different places

488
00:28:40,870 --> 00:28:43,870
so rather than coming up with one particular white

489
00:28:43,880 --> 00:28:48,540
we can say how plausible we think a whole bunch of settings of the weights

490
00:28:48,560 --> 00:28:50,860
that might seem like it with thing to do

491
00:28:50,880 --> 00:28:54,750
it might not depends on sort of what your background as well

492
00:28:54,780 --> 00:28:58,390
he told tell you anything about statistics first

493
00:28:59,850 --> 00:29:03,310
so what do we actually do to do with this

494
00:29:03,330 --> 00:29:05,910
in the story i told you the first time and we had a single weight

495
00:29:05,910 --> 00:29:09,910
vector that we'd fitted and then to make predictions it was obvious what today we

496
00:29:09,910 --> 00:29:13,890
just plugged in the one weight vector that we knew and we applied to our

497
00:29:13,890 --> 00:29:16,440
person we got some sort of answer

498
00:29:16,460 --> 00:29:20,240
now we're going to have to deal with the fact that we've got this distribution

499
00:29:20,240 --> 00:29:23,410
we don't know what the weight vector is so it doesn't seem to make sense

500
00:29:23,420 --> 00:29:27,260
to say one value it could be any of these values and we don't know

501
00:29:27,310 --> 00:29:29,410
so what we do is

502
00:29:29,430 --> 00:29:33,860
we minimize our expected loss

503
00:29:33,870 --> 00:29:39,680
and the expectation is under the distribution represents our uncertainty

504
00:29:40,610 --> 00:29:45,580
so we're going to make some predictions calling why that's sort of estimate of what

505
00:29:46,360 --> 00:29:47,850
output it is

506
00:29:49,860 --> 00:29:55,830
that's the minimum value under an expectation so some of the probability distribution of an

507
00:29:57,130 --> 00:29:58,340
of loss

508
00:30:00,630 --> 00:30:03,390
if we can to pick a particular why

509
00:30:03,400 --> 00:30:07,590
then if the true value y started will suffer this loss

510
00:30:07,600 --> 00:30:10,610
but we of course we don't know what the true value is

511
00:30:10,620 --> 00:30:13,110
so what we need to do is average over

512
00:30:13,830 --> 00:30:15,100
each of the

513
00:30:15,110 --> 00:30:20,060
possible true values weighted by how probable we think they are given the data set

514
00:30:20,070 --> 00:30:22,900
so we're going to have to make future predictions text are

515
00:30:22,940 --> 00:30:26,550
we don't know why stop but we can say what we think i distribution over

516
00:30:26,550 --> 00:30:31,830
it is given the data set and the average over that

517
00:30:31,830 --> 00:30:35,100
something to avoid confusion if you're reading up on this stuff

518
00:30:35,110 --> 00:30:38,010
is this something called the bayes classifier

519
00:30:38,040 --> 00:30:42,380
which has absolutely nothing to do with bayesian inference is kind of confusing thing the

520
00:30:42,380 --> 00:30:47,410
besides one retreating here this is this is prototype example

521
00:30:47,430 --> 00:30:49,780
let's specialize in this

522
00:30:55,260 --> 00:31:00,930
that's another is the LP duality theorem you can restrict yourself

523
00:31:00,930 --> 00:31:01,820
to be

524
00:31:01,840 --> 00:31:04,390
and it should

525
00:31:04,430 --> 00:31:07,030
we can restrict more

526
00:31:07,050 --> 00:31:09,820
two where we have two matroid polytope

527
00:31:09,820 --> 00:31:13,890
now remember for matroid polytope vertices

528
00:31:13,910 --> 00:31:15,800
are zero and i

529
00:31:15,860 --> 00:31:21,360
the vertices are the zero one vectors of the independent sets in the matrix

530
00:31:21,410 --> 00:31:24,550
in other words the polymatroid is simply the whole

531
00:31:25,950 --> 00:31:29,490
so one vectors of the independent sets

532
00:31:29,530 --> 00:31:33,300
now that pollinator intersection theorem says that

533
00:31:33,320 --> 00:31:34,890
where the polymer

534
00:31:34,930 --> 00:31:36,820
hope to two matroids

535
00:31:36,840 --> 00:31:39,930
the vertices of the intersection

536
00:31:39,950 --> 00:31:43,590
are precisely the vectors that are the vertices of

537
00:31:43,640 --> 00:31:45,300
of both of

538
00:31:46,240 --> 00:31:50,160
no no vertices at all are

539
00:31:50,160 --> 00:31:54,510
they they they the vectors of the sets that are independent both

540
00:31:54,570 --> 00:31:55,700
mean right

541
00:31:55,740 --> 00:31:58,840
and furthermore you had TTI and this

542
00:31:58,840 --> 00:32:00,910
now the interesting point here

543
00:32:02,780 --> 00:32:05,070
it doesn't work for three matrix

544
00:32:05,110 --> 00:32:08,590
you take the third matroid trade intersect

545
00:32:08,620 --> 00:32:10,510
you get garbage

546
00:32:10,550 --> 00:32:14,470
and now let's apply this to the

547
00:32:14,490 --> 00:32:19,530
optimum branchings whereas the out branches of first going to specialize more

548
00:32:19,590 --> 00:32:21,550
in your wrote book

549
00:32:21,610 --> 00:32:25,530
and you may making right book this is the only major intersection theorem that they're

550
00:32:25,530 --> 00:32:27,660
going to get to you

551
00:32:27,680 --> 00:32:35,590
because they don't know about linear pro they get their stone non-linear programming unfortunately

552
00:32:35,610 --> 00:32:38,240
now i don't know how it is now that there was a time when even

553
00:32:38,280 --> 00:32:42,470
computer scientist in a linear programming

554
00:32:42,740 --> 00:32:47,590
a typical master and still doesn't know linear programming because

555
00:32:47,610 --> 00:32:49,360
linear programming is

556
00:32:49,380 --> 00:32:53,570
so to relegated by pure mathematicians two

557
00:32:53,590 --> 00:32:55,160
you know the dirty

558
00:32:55,160 --> 00:32:58,280
stuff that practical people like you do right

559
00:32:58,320 --> 00:33:03,640
they don't realize that they're totally impractical people like me that love interest to a

560
00:33:03,660 --> 00:33:07,120
linear programming also so

561
00:33:07,140 --> 00:33:10,660
this is the the book set that the

562
00:33:10,700 --> 00:33:13,010
math department use

563
00:33:13,030 --> 00:33:14,890
is only that there is

564
00:33:14,930 --> 00:33:17,780
where everything's been specialized

565
00:33:18,610 --> 00:33:21,950
cardinality is

566
00:33:21,990 --> 00:33:26,120
but we've seen that is simply a special case

567
00:33:26,180 --> 00:33:27,660
the polytope

568
00:33:27,740 --> 00:33:31,660
intersection there

569
00:33:31,700 --> 00:33:35,110
now here's our first example applications

570
00:33:35,160 --> 00:33:38,820
a branching rooted at the node we've already described that

571
00:33:38,910 --> 00:33:41,490
what of branching is

572
00:33:41,510 --> 00:33:44,720
well let's describe branching in another way

573
00:33:44,760 --> 00:33:47,820
it's the spanning tree

574
00:33:47,880 --> 00:33:52,260
the spanning trees of a graph for the basis of a matroid aren't they

575
00:33:52,280 --> 00:33:56,490
the access to that are from our supergraph of the independent sets of a matroid

576
00:33:56,490 --> 00:34:00,090
the basis of that to the spanning trees of

577
00:34:00,140 --> 00:34:03,410
so in one of two i was so we're going to restrict ourselves to the

578
00:34:03,410 --> 00:34:05,410
basis that is not simply

579
00:34:05,820 --> 00:34:08,760
change one of the inequalities to integration

580
00:34:09,140 --> 00:34:16,640
so the basis of one of the major the spanning trees of this

581
00:34:16,640 --> 00:34:18,680
directed graph

582
00:34:18,740 --> 00:34:20,490
can you guess what the

583
00:34:20,490 --> 00:34:22,090
other major it is

584
00:34:22,140 --> 00:34:26,740
the other major in a subset of edges is independent

585
00:34:26,760 --> 00:34:34,410
it it's is for each node different from the road there is exactly one entry

586
00:34:34,430 --> 00:34:40,050
a subset of edges is independent and the second matrix for every node differ from

587
00:34:40,050 --> 00:34:44,260
the exactly one edge that's that enters

588
00:34:44,300 --> 00:34:47,200
is that good definition of branching

589
00:34:47,220 --> 00:34:51,570
a spanning tree such that every no different from the root has exactly one it

590
00:34:53,140 --> 00:34:55,740
these are two metro and so

591
00:34:55,740 --> 00:35:01,010
finding an optimal rembrandt is finally

592
00:35:01,050 --> 00:35:05,890
OP is optimizing over those two metropolitan right

593
00:35:05,970 --> 00:35:07,700
that's all there is to it

594
00:35:07,990 --> 00:35:12,660
i actually direct in this case the direct algorithm is

595
00:35:12,680 --> 00:35:14,780
it is especially elegant

596
00:35:14,780 --> 00:35:19,740
so i briefly actually here you don't have to you don't have to know about

597
00:35:19,890 --> 00:35:24,120
trade you don't even have to know about linear program in order to

598
00:35:25,240 --> 00:35:30,910
this it's more complicated than the number of algorithms for finding an optimal spanning tree

599
00:35:30,970 --> 00:35:32,800
but not much more

600
00:35:32,820 --> 00:35:37,430
you choose the best actually should no different from road

601
00:35:37,430 --> 00:35:42,360
and when a directed cycle is created by this

602
00:35:44,320 --> 00:35:50,280
chooses choose the best edge to for each node different choose the best edge directed

603
00:35:51,070 --> 00:35:54,860
well you live liable to create a cycle directed cycles in doing so

604
00:35:54,890 --> 00:35:59,390
when that happens you shrink the directed cycle track away was going

605
00:35:59,410 --> 00:36:03,720
it continues to treat this sort of like and i choose the best edge directed

606
00:36:03,720 --> 00:36:05,200
toward that

607
00:36:05,220 --> 00:36:08,820
you might create another direct disciple shrink it away

608
00:36:08,820 --> 00:36:13,280
eventually this is going to stop where you have a brain change

609
00:36:13,340 --> 00:36:18,740
in all in the back of course you have to say what that means

610
00:36:21,640 --> 00:36:28,740
eventually you're going to get a branching in this graph of all these pseudo knowledge

611
00:36:28,820 --> 00:36:30,660
there you start phase one

612
00:36:30,700 --> 00:36:34,140
now you simply expand the pseudo nodes

613
00:36:34,200 --> 00:36:38,160
in the cycles in the reverse order they choose from the

614
00:36:38,160 --> 00:36:41,680
you fill them in from motorcycles

615
00:36:41,700 --> 00:36:45,760
it's a time expander cycle you there is a unique way

616
00:36:45,780 --> 00:36:49,010
you get a branching

617
00:36:49,030 --> 00:36:53,140
and that set of edges expand another attack back up to seven that year at

618
00:36:53,140 --> 00:36:57,390
the moment that's out what could be more beautiful

619
00:36:58,050 --> 00:37:04,360
but that is this is an application that the way that was discovered was is

620
00:37:04,360 --> 00:37:05,640
an application

621
00:37:05,640 --> 00:37:08,320
of matroid intersection

622
00:37:09,110 --> 00:37:11,760
here's another quickly here's another

623
00:37:11,760 --> 00:37:15,970
application matrix infection important matroid sums

624
00:37:16,030 --> 00:37:18,380
we've got a family of matroids

625
00:37:18,390 --> 00:37:20,620
and so by

626
00:37:20,680 --> 00:37:25,180
and there's a specified rank functions are

627
00:37:25,180 --> 00:37:26,950
now we say

628
00:37:27,800 --> 00:37:30,640
a subset of the elements

629
00:37:30,660 --> 00:37:32,910
is independent

630
00:37:32,930 --> 00:37:34,820
remember that

631
00:37:34,860 --> 00:37:40,890
some elements is independent if its cardinality

632
00:37:40,950 --> 00:37:42,160
for every so

633
00:37:42,160 --> 00:37:43,840
one such a

634
00:37:43,860 --> 00:37:51,610
such as independent if for every subset s j

635
00:37:51,780 --> 00:37:56,160
this inequality holds

636
00:37:56,160 --> 00:38:00,470
well OK so that's an independence assumptions

637
00:38:00,490 --> 00:38:02,720
but in fact

638
00:38:02,720 --> 00:38:06,150
and if you want to do really accurate just two more steps as you go

639
00:38:06,150 --> 00:38:07,880
first sort of sensible

640
00:38:07,990 --> 00:38:11,970
a sensible thing to

641
00:38:12,880 --> 00:38:16,200
what we're doing then when we learn deep belief net

642
00:38:17,050 --> 00:38:19,280
we have this model was according to this model

643
00:38:19,300 --> 00:38:21,700
we learn these weights

644
00:38:21,720 --> 00:38:23,630
using our approximation

645
00:38:23,630 --> 00:38:27,200
having learned those waves

646
00:38:27,220 --> 00:38:30,070
we then freeze them here

647
00:38:30,110 --> 00:38:31,360
we take the

648
00:38:31,400 --> 00:38:34,900
aggregated posterior distribution get then we try to model that

649
00:38:34,990 --> 00:38:37,110
which involves learning these weights

650
00:38:37,150 --> 00:38:41,720
but with all the weights tied together so we really need is infinite

651
00:38:41,740 --> 00:38:45,680
and we keep doing that until you bought

652
00:38:45,750 --> 00:38:53,170
it's very hard to know how many leisure you should use zoom will tell you

653
00:38:53,180 --> 00:38:55,720
that you should use too many less according to his

654
00:38:55,820 --> 00:38:59,110
because he was only using rather small net

655
00:38:59,150 --> 00:39:00,570
because he was doing is

656
00:39:03,180 --> 00:39:04,950
my belief is that

657
00:39:04,950 --> 00:39:06,970
actually things very robust

658
00:39:07,030 --> 00:39:10,150
and you got me something for evolution to do

659
00:39:10,150 --> 00:39:14,860
and what evolution is going to do is decide roughly how many lesser-known units

660
00:39:17,930 --> 00:39:20,780
now there's one little snack to this whole argument

661
00:39:20,800 --> 00:39:22,070
which is that

662
00:39:24,240 --> 00:39:27,470
when these weights with the same as these weights

663
00:39:27,490 --> 00:39:33,110
this stuff up here constructed prior distribution that this exactly complimentary to like it so

664
00:39:33,110 --> 00:39:34,740
i can do exact inference

665
00:39:34,780 --> 00:39:39,650
by taking this binary vector most find by these weights and putting the second

666
00:39:39,650 --> 00:39:42,860
once start changing these weights

667
00:39:42,880 --> 00:39:46,740
if they use the transpose these weights which are different it won't be doing exact

668
00:39:48,030 --> 00:39:51,150
so i'm going to lose because i'm not doing inference right

669
00:39:51,170 --> 00:39:53,720
however when i change these weights

670
00:39:53,780 --> 00:39:56,900
i'll be getting a better model of the aggregated posterior is

671
00:39:56,950 --> 00:40:00,550
so i'm going to win because i get a better model of the aggregated posterior

672
00:40:00,570 --> 00:40:02,430
and the question is

673
00:40:02,470 --> 00:40:05,950
how does what i lose by doing inference from compared with what i win by

674
00:40:05,950 --> 00:40:07,180
getting better model

675
00:40:07,200 --> 00:40:09,550
and the answer is you win more than you lose

676
00:40:09,570 --> 00:40:11,180
when you right and the national bank

677
00:40:11,180 --> 00:40:14,200
so when i change these weights

678
00:40:14,200 --> 00:40:15,450
i get a better

679
00:40:15,490 --> 00:40:17,550
variational bound

680
00:40:17,550 --> 00:40:21,090
and even though my inferences and right anymore it's still pretty good

681
00:40:21,110 --> 00:40:22,840
it started off being perfect

682
00:40:22,930 --> 00:40:27,310
the change was this not quite perfect but it's still the case of these almost

683
00:40:31,300 --> 00:40:32,090
it's about

684
00:40:32,090 --> 00:40:33,510
that's what typically happens

685
00:40:33,630 --> 00:40:42,440
that's all proved in this paper which is the basically from the stuff

686
00:40:42,490 --> 00:40:45,380
i want to quickly

687
00:40:45,390 --> 00:40:49,100
so much more going to do

688
00:40:55,220 --> 00:40:59,880
so this is not remarkable change in equilibrium has worry which is one so get

689
00:40:59,880 --> 00:41:02,940
big you don't get very far from the data

690
00:41:02,970 --> 00:41:04,220
and there might be

691
00:41:04,230 --> 00:41:07,770
huge low-energy regions never visit

692
00:41:07,770 --> 00:41:11,220
and of course those will mean that the probability of things you do this is

693
00:41:11,220 --> 00:41:15,380
actually very low in the models distribution because the models putting all its weight on

694
00:41:15,380 --> 00:41:17,890
these the things that you never see

695
00:41:17,910 --> 00:41:19,270
because you never see the

696
00:41:19,270 --> 00:41:23,340
you don't realise that you think you've got a good model but you haven't

697
00:41:25,280 --> 00:41:29,470
it's amazing that given that could have it doesn't really screw you

698
00:41:29,480 --> 00:41:32,730
in general it doesn't describe anything not just fine with this sort of one step

699
00:41:34,480 --> 00:41:43,100
but there's nothing you can do which is what mark radford neal did to begin

700
00:41:44,170 --> 00:41:45,690
which is to say

701
00:41:48,320 --> 00:41:50,100
instead of

702
00:41:50,140 --> 00:41:53,300
running my by starting at the data each time

703
00:41:53,310 --> 00:41:55,680
why don't i do the following

704
00:41:55,770 --> 00:42:00,470
i'm going to have a number of markov chains

705
00:42:00,480 --> 00:42:04,300
and these markov chains i don't we initialize the the data

706
00:42:04,390 --> 00:42:06,350
this is received it of

707
00:42:06,360 --> 00:42:08,850
we just keep running these markov chains

708
00:42:08,890 --> 00:42:11,010
and as a change the weights

709
00:42:11,010 --> 00:42:13,890
so change the ways the markov chain rings

710
00:42:13,910 --> 00:42:17,240
so each time and update the weights i run each of these markov chains a

711
00:42:17,240 --> 00:42:18,730
little bit

712
00:42:18,770 --> 00:42:21,980
now if the markov chain was already close to equilibrium

713
00:42:21,980 --> 00:42:23,780
and i didn't change the weights much

714
00:42:23,860 --> 00:42:25,850
will stay close to

715
00:42:25,890 --> 00:42:29,890
so as long as i change the way slowly enough all these markov chains are

716
00:42:29,890 --> 00:42:33,100
running will be close to the close to equilibrium

717
00:42:33,110 --> 00:42:38,470
so they'll do just fine for estimating this sort of the i according to the

718
00:42:38,470 --> 00:42:39,760
model this

719
00:42:39,760 --> 00:42:42,060
when the models the group

720
00:42:42,090 --> 00:42:45,430
so these persistent chains for estimating that

721
00:42:45,480 --> 00:42:48,390
and then first meeting the i share with the data i just take the data

722
00:42:48,780 --> 00:42:51,630
i data in units and just look at the correlation

723
00:42:51,640 --> 00:42:53,720
so it's not easy thing to do

724
00:42:53,760 --> 00:42:57,810
that was the original idea bots machine so you would you be away you look

725
00:42:57,810 --> 00:43:01,240
at things in yesterday's correlations and then you go to sleep and you'd imagine things

726
00:43:01,240 --> 00:43:04,270
by running is marketing for a long time you to measure the correlations

727
00:43:04,390 --> 00:43:07,480
and then you take the difference those two statistics and that's a lot

728
00:43:07,530 --> 00:43:09,630
it's actually the idea was used

729
00:43:09,670 --> 00:43:11,310
as you're asleep you

730
00:43:11,320 --> 00:43:14,150
measure these models statistics and then the next day

731
00:43:14,190 --> 00:43:17,690
you change your weight in proportion to the difference between this decision measure when your

732
00:43:17,690 --> 00:43:20,470
weight the models decision is when you're asleep

733
00:43:20,490 --> 00:43:24,110
so that that's how you actually work

734
00:43:24,170 --> 00:43:29,260
according to that theory which may not be right

735
00:43:29,270 --> 00:43:31,260
we can try and i

736
00:43:31,310 --> 00:43:33,840
implement that learning algorithm

737
00:43:33,850 --> 00:43:36,090
and it turns out it works very well

738
00:43:36,110 --> 00:43:40,850
and it works very well not at all for the reason that you might think

739
00:43:41,940 --> 00:43:44,850
so one thing to think about this chain is doing something very similar to the

740
00:43:45,720 --> 00:43:49,780
because you start off with very small weights was easy to reach equilibrium and human

741
00:43:49,780 --> 00:43:51,140
the weights get bigger

742
00:43:51,150 --> 00:43:54,260
so if you look at this chain it looks very like a simulated annealing chain

743
00:43:54,270 --> 00:43:56,850
where the temperature is getting long

744
00:43:56,860 --> 00:44:01,210
and in fact if you are persistent chain for a long time during the conduct

745
00:44:01,210 --> 00:44:02,730
close to equilibrium

746
00:44:02,770 --> 00:44:05,910
then if you started with the final weights and ran for the entire length of

747
00:44:05,910 --> 00:44:07,850
time you learned for

748
00:44:07,860 --> 00:44:09,690
to try and get to equilibrium

749
00:44:09,740 --> 00:44:14,510
because it was doing this sort of thinking start small weights and gradually declines

750
00:44:14,520 --> 00:44:23,880
but it turns out you can learn much faster than that and also will predict

751
00:44:23,880 --> 00:44:27,070
and in fact you only need about a hundred of these facts actually was just

752
00:44:27,070 --> 00:44:31,720
one of these chains which is very surprising

753
00:44:31,740 --> 00:44:37,310
and that's the reason why the learning works much better than you'd expect

754
00:44:40,390 --> 00:44:41,510
i'm not going

755
00:44:43,060 --> 00:44:45,390
so what's happening when you do learning is

756
00:44:45,410 --> 00:44:47,600
you're changing the weights by

757
00:44:47,640 --> 00:44:50,110
you raise them by the statistics you measured

758
00:44:50,140 --> 00:44:51,640
with the data

759
00:44:51,650 --> 00:44:55,550
and you love them according to the correlations you measured in these

760
00:44:55,590 --> 00:44:58,820
markov chains that you running to estimate what the model believes

761
00:44:58,840 --> 00:45:01,010
what that means is

762
00:45:01,060 --> 00:45:05,800
if you think of these markov chains like fantasy particles moving around energy surface

763
00:45:05,810 --> 00:45:07,950
whatever fancy particle is

764
00:45:07,970 --> 00:45:11,530
the learning will say raise the energy there

765
00:45:11,550 --> 00:45:15,190
and that makes the fantasy particles go somewhere else

766
00:45:15,240 --> 00:45:18,190
so you can get trapped in local optima

767
00:45:18,230 --> 00:45:21,640
OK if you get always the same you might have some deep local optima the

768
00:45:21,640 --> 00:45:23,550
particle might just sort of thing here

769
00:45:23,600 --> 00:45:26,560
trying to jump out but is very high in the various states

770
00:45:26,680 --> 00:45:27,910
but if your learning

771
00:45:27,940 --> 00:45:30,050
using this particle for learning

772
00:45:30,050 --> 00:45:31,550
it is stuck in the middle

773
00:45:31,570 --> 00:45:35,630
i mean it was going to keep going up until it falls out the

774
00:45:35,630 --> 00:45:38,840
so the a funny interaction between the learning algorithm

775
00:45:38,900 --> 00:45:41,030
and these markov chains

776
00:45:41,070 --> 00:45:45,390
the has the effect of making the markov chains mix extremely fast

777
00:45:45,390 --> 00:45:47,330
only the gradient 0

778
00:45:47,350 --> 00:45:50,220
but the Hessian

779
00:45:50,240 --> 00:45:54,030
is not to be the figure of the inverse of the Hessian is not too

780
00:45:54,910 --> 00:45:59,180
this some positive numbers so that the norm of the Hessian matrix is not too

781
00:45:59,180 --> 00:46:01,640
big know what I mean by the norm of the Hessian matrix

782
00:46:02,310 --> 00:46:08,030
and that I'm going to need to put somewhere

783
00:46:08,040 --> 00:46:15,600
and then put the norm over here so the norm of a matrix what's called

784
00:46:15,600 --> 00:46:17,810
the operator norm of a matrix

785
00:46:17,990 --> 00:46:19,310
it is

786
00:46:19,730 --> 00:46:26,510
the maximum norm and max subject to a norm x less than or equal to

787
00:46:27,890 --> 00:46:31,220
which assesses OK how big

788
00:46:31,510 --> 00:46:34,410
when I think of them as a linear operator

789
00:46:34,540 --> 00:46:40,180
and I hit any x with enormous circa 1 with how big could because

790
00:46:40,310 --> 00:46:43,510
came out that coincides with the definitions

791
00:46:43,540 --> 00:46:48,470
yes no less singular 1 equal 1 doesn't really matter

792
00:46:56,430 --> 00:46:58,080
I'm so this says

793
00:46:58,550 --> 00:47:01,030
so right and this is the norm

794
00:47:01,040 --> 00:47:03,100
on linear operators

795
00:47:03,120 --> 00:47:04,580
on matrices

796
00:47:04,640 --> 00:47:10,350
so it may not you look a little confused not OK right so this is

797
00:47:10,350 --> 00:47:14,720
a norm and is the norm of the inverse of the Hessian is not too

798
00:47:14,720 --> 00:47:21,940
bad that's the 1st condition the 2nd edition says

799
00:47:22,080 --> 00:47:27,540
there exists a beta unit is the neighborhood so that long as axis

800
00:47:29,700 --> 00:47:35,330
the distance from x to the optimum is at most data this condition holds what

801
00:47:35,330 --> 00:47:37,120
is this kind of a condition called

802
00:47:37,750 --> 00:47:44,690
and if you know the name yes it's called a Lipschitz so a Lipschitz condition

803
00:47:44,830 --> 00:47:48,490
when we say something is Lipschitz it means that

804
00:47:49,450 --> 00:47:51,350
we can bound relative change

805
00:47:52,070 --> 00:47:56,530
so what this says that the Hessian is Ralph is Lipschitz

806
00:47:56,540 --> 00:47:59,560
and effect as well I used a capital l here

807
00:47:59,620 --> 00:48:01,160
for lipshitz

808
00:48:01,220 --> 00:48:06,760
OK so this is this is looking from sufficiently close to x star

809
00:48:07,510 --> 00:48:09,740
the difference in the Hessians

810
00:48:09,760 --> 00:48:14,640
is bounded by some absolute constant times the distance

811
00:48:14,780 --> 00:48:17,390
for X is from dot

812
00:48:17,560 --> 00:48:20,540
you have a question OK

813
00:48:20,560 --> 00:48:25,740
OK so that's that these by the way ladies gentlemen this is all technical jock

814
00:48:25,760 --> 00:48:30,830
in away so that at a certain level of becomes valuable to study the interplay

815
00:48:30,830 --> 00:48:36,310
here but right now because because because I didn't give a loose interpretation of this

816
00:48:36,310 --> 00:48:38,280
and it's going to be very loops

817
00:48:38,390 --> 00:48:42,430
OK for so this is the 1st Edition says the inverse of my question is

818
00:48:42,430 --> 00:48:47,450
not too big which means my Hessian is I can bounded away from being singular

819
00:48:47,450 --> 00:48:49,350
you can think of it that way

820
00:48:49,370 --> 00:48:52,260
OK and this

821
00:48:52,310 --> 00:48:56,010
in a neighborhood around for a point sufficiently close

822
00:48:57,200 --> 00:49:00,100
my point where the gradient is 0

823
00:49:00,120 --> 00:49:01,890
my Hessian is lipshitz

824
00:49:01,890 --> 00:49:07,660
and then the 3rd condition says is my current point is really close to x

825
00:49:07,660 --> 00:49:11,990
star in the following since its distance from x star in effect strictly less than

826
00:49:11,990 --> 00:49:13,140
some gamma

827
00:49:13,160 --> 00:49:17,640
which is going to be the minimum of data and 2 h over 3

828
00:49:18,740 --> 00:49:20,620
why the 2 and why the 3

829
00:49:22,780 --> 00:49:26,830
OK because the arithmetic will work out nicely

830
00:49:26,830 --> 00:49:30,810
OK so this is the knowledge cash

831
00:49:31,200 --> 00:49:34,160
this is my Hessian is well behaved

832
00:49:34,260 --> 00:49:37,030
in the neighborhood of x star

833
00:49:37,100 --> 00:49:40,830
if I start sufficiently close to x star

834
00:49:41,100 --> 00:49:45,220
and of my Hessian in fact now is well behaved in a neighborhood but I

835
00:49:45,220 --> 00:49:48,970
can but added that it's it's nonsingular

836
00:49:49,990 --> 00:49:54,410
and then I get quadratic convergence

837
00:49:58,580 --> 00:50:01,720
I'm gonna prove this later in the lecture

838
00:50:02,910 --> 00:50:07,010
but I just want people to digest it for a little bit and want people

839
00:50:07,010 --> 00:50:08,450
to see

840
00:50:08,470 --> 00:50:10,720
thank you

841
00:50:12,260 --> 00:50:20,760
a little bit about it so we can let me do a simple example

842
00:50:20,950 --> 00:50:24,620
the people see what's going on here

843
00:50:26,770 --> 00:50:32,580
my 1st example is going to be a one-dimensional example and then you available to

844
00:50:32,580 --> 00:50:35,250
see exactly what's happening

845
00:50:36,570 --> 00:50:41,210
it's a form of one-dimensional example I'm going to choose the function and I'm up

846
00:50:41,290 --> 00:50:49,610
to you your name will show that's action which should so suppose for example I

847
00:50:49,610 --> 00:50:56,450
have a one-dimensional function f of x peers 7 X minus log of X

848
00:50:58,990 --> 00:51:04,010
what's the great people state of greedy will talk about 1st derivative so what's the

849
00:51:04,010 --> 00:51:06,560
1st derivative

850
00:51:06,580 --> 00:51:12,810
the 7 minus 1 over X and what's the 2nd derivatives

851
00:51:12,810 --> 00:51:14,990
1 over X squared tank

852
00:51:15,250 --> 00:51:22,990
and now your name is like rags OK rods OK

853
00:51:23,210 --> 00:51:29,370
rods say what's the Newton was the Newton direction going to be at x

854
00:51:30,280 --> 00:51:43,530
minus the amount of data to will away

855
00:51:44,530 --> 00:51:49,490
it has 1 over square squares you move from 1 over X squared inverse which

856
00:51:49,490 --> 00:51:56,730
is just XQuery right the fact that would be the Newton direction and that says

857
00:51:56,730 --> 00:52:02,310
the next iteration of xn is going to be x minus

858
00:52:02,710 --> 00:52:04,130
and you work this out

859
00:52:04,760 --> 00:52:08,730
have appropriate adjustments OK so

860
00:52:13,720 --> 00:52:21,530
X minus I got it right yes I got

861
00:52:21,530 --> 00:52:23,270
the reaction you're looking at

862
00:52:23,320 --> 00:52:27,790
how much you form in a certain amount of time

863
00:52:27,830 --> 00:52:29,050
so here

864
00:52:29,080 --> 00:52:32,800
it might be what the data would look like as time goes on you see

865
00:52:32,800 --> 00:52:38,260
an increase in then it starts to level off

866
00:52:38,280 --> 00:52:41,330
you can consider the average rate

867
00:52:41,380 --> 00:52:50,650
which would be defined as some changing concentration over particular change in time

868
00:52:50,670 --> 00:52:54,560
you could express that as the change are delta

869
00:52:54,610 --> 00:53:01,130
of the concentration of an old over the change in time delta time

870
00:53:01,140 --> 00:53:05,950
and you could pick some places and calculate an average rate

871
00:53:05,960 --> 00:53:12,530
so you could draw lines over two different concentrations at two different times and calculate

872
00:53:12,530 --> 00:53:15,600
what the rate is in that interval

873
00:53:16,430 --> 00:53:21,790
so we can do that the average rate two different concentrations at those two different

874
00:53:21,790 --> 00:53:23,160
different times

875
00:53:23,210 --> 00:53:26,430
changing concentration over change in time

876
00:53:26,470 --> 00:53:28,530
and that gives you a value

877
00:53:28,550 --> 00:53:29,610
but this

878
00:53:29,620 --> 00:53:32,350
this average rate than depends

879
00:53:32,370 --> 00:53:35,140
on time interval chosen so

880
00:53:35,190 --> 00:53:38,390
it's going to be a different rate here then it might have been down here

881
00:53:38,400 --> 00:53:40,220
up here so

882
00:53:40,270 --> 00:53:44,930
that tells you may be something but it doesn't necessarily tell you all the things

883
00:53:44,930 --> 00:53:49,630
that are going on because it just depends on a particular area

884
00:53:49,640 --> 00:53:56,000
so instead of considering average rate you can start talking about instantaneous rate the rate

885
00:53:56,000 --> 00:53:59,360
at a particular time

886
00:53:59,380 --> 00:54:04,350
and so that's the instantaneous rate

887
00:54:04,360 --> 00:54:07,590
and so you really wanting to know what is the the rate they have a

888
00:54:07,590 --> 00:54:12,850
hundred and fifty second at that particular moment in time what was the rate

889
00:54:12,870 --> 00:54:15,690
so here it is talking about the limit

890
00:54:15,700 --> 00:54:19,040
as that change in time approaches zero

891
00:54:19,050 --> 00:54:24,650
and that can also be expressed in terms of d concentration of an o t

892
00:54:24,660 --> 00:54:27,930
so the change in concentration of NO

893
00:54:27,970 --> 00:54:33,990
with time at a particular particular moment

894
00:54:34,000 --> 00:54:35,250
so as

895
00:54:35,260 --> 00:54:37,230
that change in time

896
00:54:37,250 --> 00:54:38,830
approaches zero

897
00:54:38,840 --> 00:54:44,030
the rate becomes the same as the slope of the line tangent to that particular

898
00:54:44,040 --> 00:54:48,290
point in time at time t the one you're interested in

899
00:54:48,340 --> 00:54:52,360
so it's a find the instantaneous rate at one hundred and fifty

900
00:54:52,370 --> 00:54:54,290
a second series

901
00:54:54,420 --> 00:54:57,160
so we can do that

902
00:54:57,180 --> 00:54:59,440
the rate at that particular time

903
00:54:59,450 --> 00:55:03,410
so we're going to draw a line tangent to the curve

904
00:55:03,420 --> 00:55:08,180
and figure out the slope of that line

905
00:55:08,190 --> 00:55:11,990
so we can do that find the slope

906
00:55:12,050 --> 00:55:14,490
get some different points

907
00:55:14,540 --> 00:55:18,270
and look at the instantaneous rate

908
00:55:18,290 --> 00:55:22,530
so we can calculate the slope of that line

909
00:55:24,220 --> 00:55:26,920
and come up with a rate so

910
00:55:26,930 --> 00:55:30,910
in this case seven point seven times ten to the minus five and this is

911
00:55:30,910 --> 00:55:32,700
in molar per second

912
00:55:32,710 --> 00:55:34,310
which is the common u

913
00:55:34,600 --> 00:55:37,090
unit four great

914
00:55:37,200 --> 00:55:43,370
OK so it tells us about a particular instantaneous rate at one at one given

915
00:55:45,870 --> 00:55:52,030
and an important thing to consider is an initial rate so that's the instantaneous rate

916
00:55:52,030 --> 00:55:54,890
at time equals zero so the initial

917
00:55:54,930 --> 00:55:58,520
initial rate of the reaction

918
00:55:58,530 --> 00:56:02,750
so that's a little bit about measuring rates and we'll talk more about plotting data

919
00:56:02,750 --> 00:56:08,300
when you're trying to get out rate constants and other other factors

920
00:56:09,360 --> 00:56:11,840
so rate expressions

921
00:56:11,890 --> 00:56:17,860
so we can express the rate of the reaction when we're measuring

922
00:56:17,870 --> 00:56:22,840
a reaction you can often pick to measure the increase in the amount of products

923
00:56:22,840 --> 00:56:24,260
being formed

924
00:56:24,280 --> 00:56:30,540
you can also consider the decrease in the amount of reactants so you can measure

925
00:56:30,540 --> 00:56:37,120
rate either way looking for disappearance or looking for appearance and often decisions about how

926
00:56:37,120 --> 00:56:42,150
you're going to measure the rate depends on how easy it is to measure changes

927
00:56:42,150 --> 00:56:48,420
something so there's usually practical considerations that go on here

928
00:56:48,460 --> 00:56:54,050
so then the rate can be expressed in terms of the disappearance

929
00:56:54,070 --> 00:56:57,910
of one of the reactance here and o two so

930
00:56:57,940 --> 00:57:03,550
the rate is going to depend on how fast that disappearing

931
00:57:04,560 --> 00:57:10,470
the disappearance of the second reactant over here

932
00:57:10,520 --> 00:57:12,340
and the appearance

933
00:57:12,350 --> 00:57:14,770
then one of the product

934
00:57:15,990 --> 00:57:17,770
the other product

935
00:57:17,780 --> 00:57:20,110
and this of course is only true

936
00:57:20,130 --> 00:57:22,480
if we assume there's is no intermediates

937
00:57:22,490 --> 00:57:24,780
because sometimes the rate at which

938
00:57:24,840 --> 00:57:26,900
reactor will disappear

939
00:57:26,910 --> 00:57:30,740
will not be the same as the rate at which it will appear because it's

940
00:57:30,740 --> 00:57:35,200
what the steps involved in between the this comes back to mechanism which we're going

941
00:57:35,200 --> 00:57:38,300
to talk about in the next week

942
00:57:38,310 --> 00:57:45,150
so but this is what's known as the great expressions

943
00:57:45,210 --> 00:57:47,450
so let's look at an example

944
00:57:47,460 --> 00:57:48,990
so here interaction

945
00:57:49,020 --> 00:57:53,800
two h i guess going to h two gas because i two gas

946
00:57:55,800 --> 00:57:56,900
so the right

947
00:57:56,910 --> 00:58:01,320
could be considered in terms of the disappearance of the reactant

948
00:58:01,370 --> 00:58:04,750
and here we see that for every two

949
00:58:04,760 --> 00:58:09,510
h either disappears that's decompose we have one

950
00:58:09,530 --> 00:58:11,340
h two and one

951
00:58:11,340 --> 00:58:12,500
i two

952
00:58:12,520 --> 00:58:14,930
so for going to set up or rate expression

953
00:58:14,940 --> 00:58:18,590
we're going to have to multiply this term by half

954
00:58:18,590 --> 00:58:22,770
if it's going to be equal to the rate at which

955
00:58:22,820 --> 00:58:26,120
h two appears or the rate at which

956
00:58:26,180 --> 00:58:32,460
i two appears so the stoichiometry here matters so if you just looked at the

957
00:58:32,460 --> 00:58:35,230
rate of disappearance of HIV

958
00:58:35,240 --> 00:58:39,210
you it wouldn't be equal to help assets for me because two are going away

959
00:58:39,210 --> 00:58:45,170
for every one of each of these that is formed

960
00:58:45,180 --> 00:58:46,830
so then in general

961
00:58:46,850 --> 00:58:50,790
you would write the expression this way

962
00:58:50,800 --> 00:58:52,840
so minus

963
00:58:52,850 --> 00:58:57,080
minus one over a where a is the stoichiometry

964
00:58:57,560 --> 00:58:59,310
reacting in a

965
00:58:59,330 --> 00:59:01,100
d eighty t

966
00:59:01,150 --> 00:59:02,940
find one over b

967
00:59:03,010 --> 00:59:04,820
dp dt

968
00:59:04,880 --> 00:59:06,970
plus one over c

969
00:59:06,970 --> 00:59:08,760
d dt

970
00:59:08,780 --> 00:59:10,590
plus one over t

971
00:59:12,870 --> 00:59:15,120
that's hard to say

972
00:59:15,130 --> 00:59:19,380
right so those are rate expressions

973
00:59:19,450 --> 00:59:21,270
now we come to rate law

974
00:59:21,330 --> 00:59:28,520
so great

975
00:59:28,560 --> 00:59:33,460
is the relationship between race and concentrations

976
00:59:36,440 --> 00:59:39,290
there's going to be related by

977
00:59:39,320 --> 00:59:44,360
a proportionality constant called little pay rate constant

978
00:59:44,370 --> 00:59:46,250
so big k was why

979
00:59:46,300 --> 00:59:53,430
equilibrium constant and now we have little k which is the the rate constant lies

980
00:59:53,450 --> 00:59:57,840
going to tell you how the rate depends on the things in the reactions will

981
00:59:57,840 --> 01:00:01,840
be rate constants and there's going to be concentration terms

982
01:00:01,850 --> 01:00:06,530
and it has to be experimentally determined what that rate law is

983
01:00:06,530 --> 01:00:12,200
you have to pay for it by accepting to be less accurate

984
01:00:12,220 --> 01:00:18,630
lots of things to make more mistakes or having more training examples

985
01:00:18,720 --> 01:00:23,700
this image products and understanding hospital situation people could be very first

986
01:00:23,780 --> 01:00:27,220
but there is the monkeys opening or in practice is that

987
01:00:27,450 --> 01:00:29,430
when you have explanations when

988
01:00:29,490 --> 01:00:34,050
there is already discussion about explainable on systems where you say well beaten on network

989
01:00:34,300 --> 01:00:38,530
is not explainable because you cannot understand what all rules the following

990
01:00:38,570 --> 01:00:40,510
and something like the decision tree

991
01:00:40,510 --> 01:00:43,450
it's nice explanation

992
01:00:43,490 --> 01:00:47,360
but you also know that when you take a decision tree with simple explanations not

993
01:00:48,090 --> 01:00:49,510
as well

994
01:00:49,590 --> 01:00:52,680
that's up to the fact that structure

995
01:00:53,570 --> 01:00:56,010
this is an example of this idea

996
01:00:56,010 --> 01:00:58,110
but if you want an explanation

997
01:00:58,110 --> 01:01:03,800
you have to pay for it by accepting a lower accuracy

998
01:01:03,860 --> 01:01:06,610
that's not very satisfactory form in the

999
01:01:06,660 --> 01:01:08,360
practical proposals the

1000
01:01:08,380 --> 01:01:13,150
and that in his view this is what we can the map

1001
01:01:13,200 --> 01:01:14,470
so well

1002
01:01:14,570 --> 01:01:17,200
to change it you have to work quite hard

1003
01:01:17,220 --> 01:01:22,010
and so it's not possible to change by taking another viewpoint of the problem

1004
01:01:22,050 --> 01:01:26,190
but it's not going to be as simple that

1005
01:01:26,340 --> 01:01:30,220
is that the question

1006
01:01:31,410 --> 01:01:39,820
no blood was very optimistic because we can still act win situation that we don't

1007
01:01:47,550 --> 01:01:54,340
that we know less nothing more likely to share

1008
01:02:08,700 --> 01:02:12,450
i understand what you said and this is why the that science is going to

1009
01:02:12,450 --> 01:02:14,260
change dramatically

1010
01:02:14,300 --> 01:02:19,220
if there is an opportunity for acting well without full understanding

1011
01:02:19,260 --> 01:02:20,700
it's unthinkable

1012
01:02:20,780 --> 01:02:24,110
people will not take advantage of people over the next centuries

1013
01:02:24,160 --> 01:02:26,820
but that we also means accepting

1014
01:02:27,550 --> 01:02:33,050
a modification in the way we understand science and modification in the way we need

1015
01:02:33,050 --> 01:02:35,820
an explanation

1016
01:02:47,490 --> 01:02:52,380
my expensive when they have to with very precise i find it and unreadable

1017
01:02:52,390 --> 01:03:01,530
just very painful to read so i think she less impact

1018
01:03:01,590 --> 01:03:14,860
but i

1019
01:03:14,930 --> 01:03:20,240
that's optimistic way because OK we want to buy explanation letters by more examples but

1020
01:03:20,300 --> 01:03:21,860
the pessimistic way

1021
01:03:21,880 --> 01:03:25,660
these are the examples we get that fixed the limited

1022
01:03:25,700 --> 01:03:28,220
and if we have an opportunity to act but

1023
01:03:28,240 --> 01:03:30,410
on the basis of these examples only

1024
01:03:30,450 --> 01:03:31,490
o dollars

1025
01:03:31,550 --> 01:03:34,340
will take the opportunity to win win

1026
01:03:34,450 --> 01:03:44,970
even though they don't understand

1027
01:03:45,030 --> 01:03:49,800
i'd say not many in this form of the final

1028
01:03:49,860 --> 01:03:52,280
they are now

1029
01:03:52,380 --> 01:04:05,610
i guess people have something to work on it

1030
01:04:06,680 --> 01:04:08,090
but the i mean

1031
01:04:08,110 --> 01:04:11,430
i'm not aware of political was all that

1032
01:04:11,430 --> 01:04:13,860
with some marginal from by

1033
01:04:13,970 --> 01:04:15,660
that the third way

1034
01:04:17,050 --> 01:04:19,970
the so the way that can be done in this article

1035
01:04:19,990 --> 01:04:25,760
the second battalion the first the what the most striking element i know about

1036
01:04:25,820 --> 01:04:29,030
is to show that whatever

1037
01:04:29,050 --> 01:04:31,840
guarantees is given for the task is

1038
01:04:31,860 --> 01:04:33,510
made using

1039
01:04:33,510 --> 01:04:36,320
mathematical tools derive from the second

1040
01:04:36,320 --> 01:04:40,800
so you prove the second and then you make approximation to prove the first

1041
01:04:40,820 --> 01:04:42,860
that led

1042
01:04:43,220 --> 01:04:46,320
as i think that the second actually more essential

1043
01:04:46,320 --> 01:04:50,590
the second is purely combinatorial the first one has been made

1044
01:04:50,610 --> 01:04:52,780
work in infinity the

1045
01:04:52,820 --> 01:05:04,470
in continuous space by using tweaks and losing some constant factor of the way

1046
01:05:04,570 --> 01:05:08,840
so in the third part

1047
01:05:08,890 --> 01:05:10,280
is discussing some

1048
01:05:10,300 --> 01:05:15,430
cases of non inference things we can consider doing

1049
01:05:15,430 --> 01:05:18,200
and in this part

1050
01:05:18,280 --> 01:05:19,530
there are two

1051
01:05:19,630 --> 01:05:22,550
quite interesting propositions that

1052
01:05:22,590 --> 01:05:26,320
i'm going to describe

1053
01:05:26,340 --> 01:05:28,320
the first thing is that

1054
01:05:28,380 --> 01:05:33,090
large margins are basically wrong

1055
01:05:33,320 --> 01:05:37,360
we have of course when you do not imagine in the feature space

1056
01:05:37,380 --> 01:05:41,880
what the large margin means completely depending on how you define the space

1057
01:05:41,930 --> 01:05:46,530
and how do you find the space is completely arbitrary

1058
01:05:46,550 --> 01:05:51,470
this is related to the parameterisation problem in bayesian theory like

1059
01:05:51,660 --> 01:05:55,030
if you take a business that sort of like common but this is how i

1060
01:05:55,030 --> 01:05:56,280
understand it

1061
01:05:56,300 --> 01:05:59,530
if you take a base in the port

1062
01:05:59,550 --> 01:06:02,260
you have to arbitrate parameterisation

1063
01:06:02,280 --> 01:06:07,200
one is the parametrisation of the input is how do you present your pocket

1064
01:06:07,260 --> 01:06:10,380
suppose you want to recognise handwritten digits

1065
01:06:10,430 --> 01:06:14,570
well you take the square root of pixels to prison by zero and one with

1066
01:06:14,590 --> 01:06:16,050
completely arbitrary

1067
01:06:16,110 --> 01:06:19,450
we know for a fact that i doesn't work that way and the digits were

1068
01:06:19,450 --> 01:06:22,150
designed to be easy way by

1069
01:06:22,160 --> 01:06:25,980
so you could think of for presenting digits biases the most rock or by a

1070
01:06:25,980 --> 01:06:29,740
circular are of pixels or something else

1071
01:06:29,800 --> 01:06:33,590
and of course when you're describe the function

1072
01:06:33,590 --> 01:06:37,180
that that operate on the basis the way

1073
01:06:37,180 --> 01:06:40,430
you said that the space where you tries you put them

1074
01:06:40,470 --> 01:06:42,840
is extremely important

1075
01:06:42,930 --> 01:06:45,260
the second parametrisation issues

1076
01:06:45,260 --> 01:06:49,740
is the parameter space of the functions how you parametrized functions

1077
01:06:49,740 --> 01:06:52,840
and in the bayesian theory this is very important because this is how to define

1078
01:06:52,860 --> 01:06:58,240
the prior of the priorities defined on the parameterisation of the functions

1079
01:06:58,280 --> 01:07:00,930
and if you look in the bayesian

1080
01:07:00,970 --> 01:07:04,930
you realize that there is a lot of thinking about this parametrisation of the function

1081
01:07:04,930 --> 01:07:07,320
so there is that physics u

1082
01:07:07,340 --> 01:07:09,680
i recognise this is about a factor

1083
01:07:09,720 --> 01:07:11,140
and then use your

1084
01:07:12,940 --> 01:07:15,960
the probability of the state is exponentially

1085
01:07:16,000 --> 01:07:18,070
proportional to the energy

1086
01:07:18,090 --> 01:07:20,300
that what is the connection between

1087
01:07:20,310 --> 01:07:21,880
such density

1088
01:07:21,930 --> 01:07:22,960
in space

1089
01:07:22,970 --> 01:07:25,280
and they can that reasoning and values

1090
01:07:25,290 --> 01:07:27,850
that are computed from the matrix and

1091
01:07:27,880 --> 01:07:32,600
and one thing that interested me was going to use this for

1092
01:07:32,620 --> 01:07:33,960
analysis of of

1093
01:07:34,000 --> 01:07:36,740
dynamical systems

1094
01:07:37,890 --> 01:07:41,020
to do this we need some mathematical analysis

1095
01:07:43,380 --> 01:07:47,750
here's the first observation for celebrations that the matrix m

1096
01:07:47,750 --> 01:07:50,460
of which are computing the

1097
01:07:50,470 --> 01:07:51,660
you can vectors

1098
01:07:51,690 --> 01:07:54,680
is a joint semantic one

1099
01:07:54,780 --> 01:07:57,500
and we all know that symmetric scissors

1100
01:07:57,510 --> 01:08:00,520
are always the analyzable and they have

1101
01:08:00,570 --> 01:08:02,590
a set of

1102
01:08:02,600 --> 01:08:04,800
there you can vectors are

1103
01:08:04,850 --> 01:08:06,230
they all follow model

1104
01:08:06,250 --> 01:08:09,020
orthogonal and they form the basis

1105
01:08:09,030 --> 01:08:11,000
of of are and

1106
01:08:11,790 --> 01:08:17,160
therefore the same holds for the next markov matrix that we've just constructed

1107
01:08:17,250 --> 01:08:19,730
moreover if you took ceylon

1108
01:08:19,780 --> 01:08:24,430
large enough all points are connected you can move from each point to each other

1109
01:08:25,500 --> 01:08:26,820
and therefore

1110
01:08:26,880 --> 01:08:31,250
the first thing about this matrix is one with multiplicity one

1111
01:08:31,290 --> 01:08:32,280
and it has

1112
01:08:32,290 --> 01:08:35,060
in addition a sequence of n minus one

1113
01:08:35,150 --> 01:08:39,950
a nonincreasing and values on the j

1114
01:08:39,990 --> 01:08:44,450
and as i said both the writing vectors on the left eigenvectors they for basis

1115
01:08:44,450 --> 01:08:46,940
of our own

1116
01:08:49,910 --> 01:08:51,700
first writing conductor

1117
01:08:52,850 --> 01:08:56,840
the steady state distribution of the random walk

1118
01:08:58,160 --> 01:09:03,490
because of the way i constructed it within epsilon large enough

1119
01:09:03,490 --> 01:09:05,820
regardless of where you start from

1120
01:09:06,020 --> 01:09:08,010
if you

1121
01:09:09,610 --> 01:09:10,790
long enough

1122
01:09:10,790 --> 01:09:13,500
the probability of staying of reaching y

1123
01:09:13,520 --> 01:09:15,240
is exactly this

1124
01:09:15,290 --> 01:09:16,790
first a conductor

1125
01:09:16,810 --> 01:09:18,690
berlin normalise it

1126
01:09:18,740 --> 01:09:21,490
i have one or was one

1127
01:09:23,710 --> 01:09:25,280
what i want to do

1128
01:09:25,310 --> 01:09:29,000
is to measure how two points are close

1129
01:09:29,030 --> 01:09:32,700
and as i've said before in original space

1130
01:09:32,930 --> 01:09:36,840
that is a difficult task can no in the original space in high dimensional space

1131
01:09:36,840 --> 01:09:38,880
i can know if two points are close

1132
01:09:38,910 --> 01:09:43,250
only very closely they're far apart i don't know how close they are

1133
01:09:44,990 --> 01:09:47,060
one way to measure closeness

1134
01:09:47,080 --> 01:09:49,440
is with respect to this random walk

1135
01:09:49,440 --> 01:09:50,820
so specifically

1136
01:09:50,840 --> 01:09:53,700
for any two points x not the next one

1137
01:09:53,700 --> 01:09:57,600
i can measure their closeness at some time t

1138
01:10:00,760 --> 01:10:05,350
by starting at x not in looking at the probability distribution at some later time

1139
01:10:05,350 --> 01:10:09,050
t so if this is actually not that's going to be some kind of about

1140
01:10:09,060 --> 01:10:10,360
here x not

1141
01:10:10,370 --> 01:10:11,690
making the same

1142
01:10:11,700 --> 01:10:12,990
o thing for x one

1143
01:10:13,000 --> 01:10:15,020
starting next one random walk

1144
01:10:15,950 --> 01:10:17,040
what is the

1145
01:10:17,050 --> 01:10:19,010
well because this was after time t

1146
01:10:19,020 --> 01:10:20,560
and see how these two were

1147
01:10:20,610 --> 01:10:22,430
are close to each other

1148
01:10:22,470 --> 01:10:24,240
that is a matter of

1149
01:10:24,250 --> 01:10:28,210
the dynamical proximity between two points with respect to this

1150
01:10:28,310 --> 01:10:30,100
random walk

1151
01:10:31,290 --> 01:10:34,620
and you can always so this is an l two norm but you can always

1152
01:10:34,620 --> 01:10:37,290
have a weighted l two norm

1153
01:10:38,350 --> 01:10:41,550
it turns out that if you take this weighted l two norm

1154
01:10:41,580 --> 01:10:42,800
where you

1155
01:10:42,870 --> 01:10:46,650
divide by people believe that this is the solution

1156
01:10:53,480 --> 01:10:55,710
norm has an intimate connection to the

1157
01:10:55,730 --> 01:10:57,780
you can vectors and again functions

1158
01:10:57,790 --> 01:10:59,290
as we have computed

1159
01:10:59,290 --> 01:11:01,480
and this is

1160
01:11:01,980 --> 01:11:06,640
spelled out in the following spectral theorem that the distance

1161
01:11:06,640 --> 01:11:10,990
the construction because the fourier transform is an isometry

1162
01:11:11,010 --> 01:11:14,680
minimizing zero to the energy in the time domain and the four domain is the

1163
01:11:15,850 --> 01:11:20,640
so what the minimum energy reconstruction does it sets to zeros the data that have

1164
01:11:20,640 --> 01:11:22,320
not seen

1165
01:11:22,350 --> 01:11:24,350
and so we construct messy

1166
01:11:24,370 --> 01:11:26,530
trigonometric polynomial

1167
01:11:26,550 --> 01:11:30,680
that has the frequency that i've seen and none of the other frequencies

1168
01:11:30,700 --> 01:11:33,910
and of course this message trigonometric polynomial which is here

1169
01:11:33,930 --> 01:11:35,430
look nothing like this

1170
01:11:35,450 --> 01:11:39,910
that has all frequencies

1171
01:11:39,930 --> 01:11:41,050
right now

1172
01:11:41,050 --> 01:11:43,430
well you cannot see this but

1173
01:11:43,600 --> 01:11:46,970
so this was my beautiful friend

1174
01:11:46,970 --> 01:11:52,390
and the phantom is not stars in the pixel domain but it's great explorers

1175
01:11:52,410 --> 01:11:56,910
so what you would do is you would minimize l one norm of what sparse

1176
01:11:56,970 --> 01:12:01,720
and what's passes the gradient so you minimize l one norm of the weight

1177
01:12:01,720 --> 01:12:04,510
that's because the total variation norm

1178
01:12:05,240 --> 01:12:09,660
and what if you do something else the filtered backprojection that people do in the

1179
01:12:09,660 --> 01:12:14,300
field to get this but if you minimize the total variation norm you have perfect

1180
01:12:14,300 --> 01:12:18,450
recovery and why because the gradient is extremely sparse

1181
01:12:18,470 --> 01:12:21,640
so one will get uses sparse solution

1182
01:12:21,660 --> 01:12:23,080
OK and that's another

1183
01:12:23,570 --> 01:12:30,350
time i got in trouble with reserve his university of wisconsin actually because i've is

1184
01:12:30,350 --> 01:12:32,950
i this radiologists and the data

1185
01:12:32,990 --> 01:12:38,070
about phantoms undersampled according to his polar geometry as in action you before

1186
01:12:38,070 --> 01:12:41,930
and i would return the results and the results they were always exact holds the

1187
01:12:41,930 --> 01:12:43,580
centre of this guy gave me

1188
01:12:43,680 --> 01:12:48,430
and at some point he said well manually completely fudging

1189
01:12:48,450 --> 01:12:50,240
in sciences in

1190
01:12:50,260 --> 01:12:54,280
what is the phone number of dean and so on and so forth

1191
01:12:55,450 --> 01:12:57,320
but then i gave them the code

1192
01:12:57,350 --> 01:13:00,910
people could see that it's actually

1193
01:13:00,930 --> 01:13:05,550
OK so of course all these experiments called for an explanation and explanation you've seen

1194
01:13:05,910 --> 01:13:09,100
this variation is is result which said that

1195
01:13:09,120 --> 01:13:13,200
for example thinking time and frequency is it's a theorem that is the number of

1196
01:13:13,200 --> 01:13:18,870
measurements essentially bigger than this one again then the recovery is complete exact

1197
01:13:20,680 --> 01:13:24,100
all right so this i don't really need to talk about because

1198
01:13:24,120 --> 01:13:28,030
well i've given you so much history in the first lecture about the use of

1199
01:13:28,030 --> 01:13:34,370
the one norm in geophysics and the early results in the mathematical signal processing community

1200
01:13:34,450 --> 01:13:38,350
which i will highlight a different set of results of the fact that we can

1201
01:13:38,350 --> 01:13:45,010
reconstruct sparse objects some undersampled frequency data has been known in the theory of computer

1202
01:13:45,010 --> 01:13:49,530
science at about the same time and there are some nice results by hand gilbert

1203
01:13:49,550 --> 01:13:55,600
and colleagues showings is how can this this can be done with completely different algorithms

1204
01:13:55,600 --> 01:13:59,910
even though the emphasis in this work is not at all on reconstruction from many

1205
01:13:59,910 --> 01:14:05,490
many sample data rather it's on speeding up things like fifty

1206
01:14:06,200 --> 01:14:08,280
so it's much more

1207
01:14:09,320 --> 01:14:13,370
it's not inspired by data analysis it inspired by

1208
01:14:13,410 --> 01:14:15,550
numerical linear algebra

1209
01:14:15,620 --> 01:14:18,660
OK so now compressed sensing what is it

1210
01:14:18,680 --> 01:14:24,050
OK alright so compressed sensing is trying to kind of resolve one of these

1211
01:14:25,490 --> 01:14:28,260
enormous down that we live with which is that

1212
01:14:28,490 --> 01:14:31,350
we live in a world that is a bit strange where we spend a lot

1213
01:14:31,350 --> 01:14:34,720
of time and effort and money acquiring data

1214
01:14:34,740 --> 01:14:38,120
when we know ahead of time that most of what we acquire we will end

1215
01:14:38,120 --> 01:14:39,820
up throwing it away

1216
01:14:39,820 --> 01:14:42,600
and a good example of this is my digital camera

1217
01:14:42,600 --> 01:14:46,030
my digital camera has lots of sensors

1218
01:14:46,050 --> 01:14:49,640
and so when i go in the backyard and i take a picture and i

1219
01:14:49,640 --> 01:14:51,260
don't from precedent

1220
01:14:51,300 --> 01:14:55,720
the rule five is about fifty megabytes and that's what i see

1221
01:14:55,740 --> 01:14:58,890
and then around japan two thousand

1222
01:14:58,910 --> 01:15:02,100
and it will compress down this picture two

1223
01:15:02,200 --> 01:15:05,850
a hundred hundredfold and so i would get a GP five of about one hundred

1224
01:15:05,850 --> 01:15:07,430
and fifty kilo right

1225
01:15:07,450 --> 01:15:12,620
so roughly speaking i through a ninety nine percent of the one i look at

1226
01:15:12,620 --> 01:15:16,890
the difference between these two pictures there's not much

1227
01:15:16,910 --> 01:15:18,390
and so

1228
01:15:18,390 --> 01:15:22,200
the the kind of the paradigm we live in is very strange which we we

1229
01:15:22,200 --> 01:15:25,970
spend a lot of time designing sensors and growing data

1230
01:15:25,970 --> 01:15:30,910
and the first thing we do have to require data is to throw most of

1231
01:15:30,930 --> 01:15:35,340
and so the question that we like to as it seems enormously wasteful

1232
01:15:35,350 --> 01:15:40,200
and i hope it's not stable for society it seems enormously wasteful another question that

1233
01:15:40,200 --> 01:15:43,010
you need to ask is is it possible to acquire the part that you will

1234
01:15:43,010 --> 01:15:45,120
not throw away

1235
01:15:46,010 --> 01:15:47,990
because really the kind of the

1236
01:15:47,990 --> 01:15:52,320
the way the world goes at the moment is we have very expensive sensors take

1237
01:15:52,340 --> 01:15:54,390
a lot of higher resolution data

1238
01:15:54,410 --> 01:15:58,990
but because we cannot is transmitter stores its data we need to compress it

1239
01:15:59,010 --> 01:16:00,620
so we compress them

1240
01:16:00,660 --> 01:16:03,530
so that we can transmit them store them

1241
01:16:03,550 --> 01:16:08,660
and we send them to receiver whose then compresses but in this compression stage

1242
01:16:08,680 --> 01:16:12,930
well essentially discard ninety nine percent of what we have measured of course

1243
01:16:13,470 --> 01:16:16,820
to be able to get this kind of pictures i need to be able to

1244
01:16:16,820 --> 01:16:20,930
look at the entire dataset to know what is it that i need to away

1245
01:16:20,950 --> 01:16:25,160
for example i need to compute its coefficient in the wavelet domain names and throw

1246
01:16:25,160 --> 01:16:27,580
away the little ones the big ones

1247
01:16:27,600 --> 01:16:32,850
all right so

1248
01:16:32,870 --> 01:16:38,260
will see that there is a way of doing this of arbitrary rethinking this picture

1249
01:16:38,260 --> 01:16:40,240
in in in a different way

1250
01:16:40,240 --> 01:16:44,790
and that is there that actually almost all

1251
01:16:44,790 --> 01:16:46,620
suggestions for

1252
01:16:46,640 --> 01:16:49,700
with these models

1253
01:16:49,720 --> 01:16:51,160
we discussed

1254
01:16:52,430 --> 01:16:55,640
one idea is to use the expansion is a

1255
01:16:55,660 --> 01:16:57,260
and somehow

1256
01:16:57,260 --> 01:16:59,220
music as an expression of the

1257
01:16:59,790 --> 01:17:02,330
around the world

1258
01:17:02,540 --> 01:17:03,910
this been explored

1259
01:17:05,120 --> 01:17:07,790
it's probably not so much

1260
01:17:07,810 --> 01:17:11,790
approximation to resolve his life

1261
01:17:11,790 --> 01:17:15,470
not as good

1262
01:17:15,490 --> 01:17:18,700
variational bound on the

1263
01:17:18,720 --> 01:17:21,010
much like

1264
01:17:21,140 --> 01:17:28,870
that i don't like it like this the

1265
01:17:30,620 --> 01:17:33,220
that and higher

1266
01:17:33,240 --> 01:17:37,620
and this is i about this

1267
01:17:38,850 --> 01:17:41,560
what defined

1268
01:17:48,390 --> 01:17:50,890
that dog get

1269
01:17:54,240 --> 01:17:55,640
the approximation

1270
01:17:55,660 --> 01:17:59,790
is actually turn things that

1271
01:17:59,810 --> 01:18:02,560
you can around the world

1272
01:18:02,580 --> 01:18:03,990
that's right

1273
01:18:04,010 --> 01:18:06,430
optimize processes

1274
01:18:06,450 --> 01:18:10,290
process and

1275
01:18:14,720 --> 01:18:16,490
one of the this is

1276
01:18:16,510 --> 01:18:19,270
to the front

1277
01:18:19,290 --> 01:18:23,530
and observations then the posterior is

1278
01:18:23,660 --> 01:18:25,850
as in right

1279
01:18:28,760 --> 01:18:31,060
to be a lot of work

1280
01:18:31,120 --> 01:18:34,580
so one can think of

1281
01:18:34,640 --> 01:18:38,060
the reduced form tried to

1282
01:18:40,430 --> 01:18:43,100
it's called to take action

1283
01:18:44,240 --> 01:18:46,310
which is why

1284
01:18:46,330 --> 01:18:47,870
and i realized that

1285
01:18:47,890 --> 01:18:50,160
i discovered that

1286
01:18:57,640 --> 01:18:58,200
all right

1287
01:18:58,220 --> 01:18:58,990
so now

1288
01:19:06,370 --> 01:19:07,760
much like

1289
01:19:08,040 --> 01:19:10,220
has a like the

1290
01:19:12,010 --> 01:19:13,080
in fact

1291
01:19:13,350 --> 01:19:19,470
well justified

1292
01:19:19,490 --> 01:19:22,580
it's hard

1293
01:19:22,850 --> 01:19:24,580
i want to find the

1294
01:19:30,640 --> 01:19:32,540
this is what must

1295
01:19:32,560 --> 01:19:36,930
you need to know

1296
01:19:36,970 --> 01:19:38,850
is that right use the

1297
01:19:40,600 --> 01:19:42,310
that means that every

1298
01:19:44,740 --> 01:19:47,240
that's not

1299
01:19:47,270 --> 01:19:49,640
my house

1300
01:19:49,640 --> 01:19:54,830
OK so what i want to

1301
01:19:55,430 --> 01:19:58,470
something of a

1302
01:19:58,490 --> 01:20:00,390
all size

1303
01:20:02,950 --> 01:20:06,810
i don't want to make sure is the site potentials

1304
01:20:06,830 --> 01:20:08,030
i going to have

1305
01:20:08,040 --> 01:20:11,790
now since we have been

1306
01:20:12,060 --> 01:20:14,950
and i

1307
01:20:17,870 --> 01:20:19,930
not so you know

1308
01:20:19,950 --> 01:20:22,510
it's also known

1309
01:20:22,510 --> 01:20:24,370
the approximation

1310
01:20:24,390 --> 01:20:26,830
the distribution over targets

1311
01:20:28,330 --> 01:20:31,580
the class label

1312
01:20:31,660 --> 01:20:34,390
probability that one by one

1313
01:20:38,790 --> 01:20:40,350
which is

1314
01:20:42,100 --> 01:20:43,120
by something

1315
01:20:43,870 --> 01:20:46,310
this is also to have

1316
01:20:48,080 --> 01:20:51,350
functional shape

1317
01:20:51,370 --> 01:20:52,970
but this is still somehow

1318
01:20:53,010 --> 01:20:55,560
because this one

1319
01:20:55,620 --> 01:20:56,510
so over

1320
01:20:56,540 --> 01:20:58,540
four different values of y

1321
01:20:58,560 --> 01:20:59,970
this thing you can do

1322
01:21:00,060 --> 01:21:09,790
we don't necessarily which is on the one that the sum of the two thousand

1323
01:21:09,810 --> 01:21:10,790
one is

1324
01:21:10,810 --> 01:21:12,060
not one

1325
01:21:15,370 --> 01:21:20,600
the main idea is that we

1326
01:21:20,640 --> 01:21:22,700
from i times

1327
01:21:22,720 --> 01:21:25,220
i normalized

1328
01:21:25,220 --> 01:21:27,350
such a long way to go into it is maybe see

1329
01:21:28,870 --> 01:21:31,280
target is so vision targets the prior

1330
01:21:32,750 --> 01:21:35,580
the likelihood that you cannot be computed but you know to

1331
01:21:36,320 --> 01:21:37,280
define it has

1332
01:21:37,750 --> 01:21:41,300
constructs can produce simulations from this likelihood

1333
01:21:43,660 --> 01:21:45,320
the trick is to

1334
01:21:45,820 --> 01:21:47,290
related to this very

1335
01:21:48,840 --> 01:21:49,470
the essential

1336
01:21:50,130 --> 01:21:52,830
properties that if you select from the prior

1337
01:21:53,230 --> 01:21:55,340
so the prime then you studio

1338
01:21:55,870 --> 01:21:56,670
from this model

1339
01:21:58,440 --> 01:21:59,710
and you wait until easy

1340
01:22:00,130 --> 01:22:01,370
is equal to one

1341
01:22:01,860 --> 01:22:02,790
the observed data

1342
01:22:05,120 --> 01:22:07,030
the time you get out of this movement

1343
01:22:08,040 --> 01:22:10,820
produces a a prime that is exactly

1344
01:22:11,230 --> 01:22:12,390
from the posterior distribution

1345
01:22:16,390 --> 01:22:16,910
is it true

1346
01:22:18,250 --> 01:22:20,080
just look at the distribution of

1347
01:22:21,590 --> 01:22:24,040
upcoming sit on it is that

1348
01:22:24,520 --> 01:22:27,890
prior times to accept the bridge to accept is for why

1349
01:22:28,310 --> 01:22:31,340
even today are so it's accept reject one one

1350
01:22:33,260 --> 01:22:33,710
it was

1351
01:22:36,430 --> 01:22:37,100
in nineteen

1352
01:22:38,080 --> 01:22:39,070
in nineteen eighty four by

1353
01:22:39,670 --> 01:22:42,240
then and also in nineteen eighty four

1354
01:22:43,380 --> 01:22:43,900
peter diggle

1355
01:22:47,820 --> 01:22:49,580
morning a conceptual way then

1356
01:22:50,240 --> 01:22:52,140
for the then in a practical way

1357
01:22:52,600 --> 01:22:56,810
but as the augmentation of what is the posterior distribution devices solution uses a prior

1358
01:22:57,690 --> 01:22:59,060
has a prediction for population

1359
01:22:59,620 --> 01:23:01,870
and in some cases weights

1360
01:23:05,580 --> 01:23:10,810
to the data that must mimic the traditional and so that's our spirits quite interesting because it gives

1361
01:23:11,490 --> 01:23:11,860
a fairly

1362
01:23:12,420 --> 01:23:18,650
intuitive approach to bayesian statistics but of course in practice it was only two nineteen ninety seven

1363
01:23:19,060 --> 01:23:20,980
that it became a practical tool

1364
01:23:21,710 --> 01:23:26,480
now is it practical well as i write it is not practical because wedding forces equal why

1365
01:23:27,290 --> 01:23:30,440
means at this is an even that must occur

1366
01:23:31,040 --> 01:23:32,290
at the human scale

1367
01:23:33,090 --> 01:23:33,990
you cannot wait

1368
01:23:34,940 --> 01:23:37,500
billions of years for is equal to one and therefore

1369
01:23:37,930 --> 01:23:39,130
you be see

1370
01:23:39,960 --> 01:23:43,260
is drifting away from this exact condition is the posture

1371
01:23:43,860 --> 01:23:46,880
but lowing form some tolerance around

1372
01:23:47,690 --> 01:23:47,960
so there

1373
01:23:49,080 --> 01:23:49,990
so we don't accept

1374
01:23:50,710 --> 01:23:54,090
only one is equal to other ones is closing after one

1375
01:23:55,120 --> 01:24:00,770
namely one distance between why and psi ops distance is less than a certain epsilon

1376
01:24:01,270 --> 01:24:02,690
and epsilon is tolerance

1377
01:24:03,140 --> 01:24:04,560
that you're ready to tolerate

1378
01:24:05,300 --> 01:24:06,320
and actually in practice

1379
01:24:06,880 --> 01:24:11,330
this is better and that you're forced to tolerate if you want to solve the problem

1380
01:24:12,090 --> 01:24:12,920
before you retire

1381
01:24:14,430 --> 01:24:16,560
now of course this produces an approximation

1382
01:24:17,230 --> 01:24:18,670
and this approximation

1383
01:24:19,090 --> 01:24:23,350
means actually estimating from the true pushchairs but from the convolution

1384
01:24:24,650 --> 01:24:25,820
this poster by

1385
01:24:26,220 --> 01:24:26,930
this tolerance

1386
01:24:27,840 --> 01:24:28,070
thank you

1387
01:24:29,210 --> 01:24:31,690
and that's that's the price you have to pay

1388
01:24:32,470 --> 01:24:34,000
more should pay four

1389
01:24:35,360 --> 01:24:38,280
selling approximately the problem

1390
01:24:39,310 --> 01:24:43,400
because the algorithm just you can write it in in a few lines you see from the data

1391
01:24:44,980 --> 01:24:47,290
as you from the fairy story used absolute

1392
01:24:48,300 --> 01:24:52,020
conditional on this value of the parameter and you wait until

1393
01:24:52,460 --> 01:24:53,190
xeon and why

1394
01:24:53,720 --> 01:24:59,020
closing of now there is another degree of approximation that is found in in most

1395
01:24:59,240 --> 01:25:01,060
cases namely that you cannot wait

1396
01:25:01,670 --> 01:25:04,830
four the whole data to be close to the whole to nail

1397
01:25:05,630 --> 01:25:06,590
and so forth

1398
01:25:07,140 --> 01:25:08,320
this proximity

1399
01:25:10,500 --> 01:25:11,180
to occur

1400
01:25:11,600 --> 01:25:13,030
in the reduced space

1401
01:25:14,040 --> 01:25:15,260
through the use of

1402
01:25:15,720 --> 01:25:18,660
statistics that are called summary statistics

1403
01:25:19,480 --> 01:25:22,650
which doesn't mean anything because they are statistics that is transforming the data

1404
01:25:23,730 --> 01:25:27,680
the body that are usually not sufficient so they are throwing away some part of

1405
01:25:27,690 --> 01:25:32,180
the information contained in the data but paradoxically they will explain minutes

1406
01:25:32,900 --> 01:25:35,470
this is unnecessary to do anything useful

1407
01:25:35,470 --> 01:25:40,090
really commonly used machine learning so it's it's used to the point where people often

1408
01:25:40,090 --> 01:25:43,960
won't mention that that's what this scoring on

1409
01:25:44,010 --> 01:25:47,710
i don't believe it's always the right thing for applications i gave an example of

1410
01:25:47,710 --> 01:25:53,510
reading text that is not true that you're going to necessarily just care about

1411
01:25:53,520 --> 01:25:57,300
whether you got the right answer on the check and you're going to also wanted

1412
01:25:57,320 --> 01:26:00,990
more complicated system that spits out don't know and that should be part of the

1413
01:26:00,990 --> 01:26:05,970
loss function how bad it is i don't know that's impossible to the k

1414
01:26:07,760 --> 01:26:11,350
something else you might do is achieve some sort of tolerance so i'm predicting the

1415
01:26:11,350 --> 01:26:15,690
strength of the concrete and i predict that it will have a tensile strength of

1416
01:26:15,690 --> 01:26:18,070
five and some units now

1417
01:26:18,080 --> 01:26:21,150
then they go away and build that concrete and i actually only had

1418
01:26:21,160 --> 01:26:23,090
tensile strength of four point five

1419
01:26:23,090 --> 01:26:24,590
are they allowed to see me

1420
01:26:24,650 --> 01:26:28,200
well presumably we had some sort of contract where i said

1421
01:26:28,980 --> 01:26:32,510
i'm going to predict the tensile strength and i think that i'm going to get

1422
01:26:32,510 --> 01:26:36,040
it correct within the bounds so one sort of loss might be and my within

1423
01:26:36,040 --> 01:26:37,850
the tolerance it i agree to

1424
01:26:37,860 --> 01:26:41,920
in which case that's OK otherwise are going to experience some loss

1425
01:26:41,970 --> 01:26:45,530
you have to be a bit careful with that sort of loss function say

1426
01:26:45,540 --> 01:26:49,400
i heard this story about consulting for

1427
01:26:49,410 --> 01:26:50,770
chemical plant

1428
01:26:51,860 --> 01:26:55,880
the law said that there was a strict limit on the amount of pollutants they

1429
01:26:55,880 --> 01:26:57,270
catch up into the river

1430
01:26:58,140 --> 01:26:59,770
the loss is

1431
01:26:59,820 --> 01:27:02,300
it's fine with legal as long as you

1432
01:27:02,350 --> 01:27:05,110
stay with this level of pollutants

1433
01:27:05,130 --> 01:27:11,220
otherwise it's illegal and that's bad we will or biting companies that's infinitely bad so

1434
01:27:11,220 --> 01:27:13,790
we going infinite loss so that never happened

1435
01:27:14,090 --> 01:27:18,030
and that's a terrible loss function to use because then what a computer system will

1436
01:27:18,600 --> 01:27:19,730
is it will

1437
01:27:19,760 --> 01:27:25,140
desperately trying stay legal and keep all the pollutants within the tolerance that

1438
01:27:25,150 --> 01:27:26,760
real systems are noisy

1439
01:27:26,770 --> 01:27:30,930
bad things happen maybe at some point you're going to exceed the legal threshold in

1440
01:27:30,930 --> 01:27:33,910
which case you experience infinite loss and after that it doesn't matter what you do

1441
01:27:33,910 --> 01:27:36,910
seem as well just dump all pollutants that you can into the river because it's

1442
01:27:36,910 --> 01:27:41,140
easier and so you need to sort of thing very carefully when you're constructing these

1443
01:27:41,140 --> 01:27:45,410
things and think realistically about about applications

1444
01:27:45,470 --> 01:27:49,020
having said all of that people don't necessarily do

1445
01:27:49,050 --> 01:27:52,960
the loss functions you'll see in papers are often much more

1446
01:27:53,020 --> 01:27:57,580
concerned with of a different symbol of a convex things that will make the code

1447
01:27:57,580 --> 01:28:03,010
easier to write the theory easier to do this algorithms easier to analyse say a

1448
01:28:03,030 --> 01:28:06,810
choosing loss functions is a compromise and

1449
01:28:06,820 --> 01:28:12,780
something that you will have to think about and return later to issue practical issues

1450
01:28:12,780 --> 01:28:15,800
to do with loss functions they you can sit down and come up with the

1451
01:28:15,810 --> 01:28:19,320
loss function and then it turns out to be not very good

1452
01:28:19,380 --> 01:28:22,930
and i'll give you examples of that later

1453
01:28:23,850 --> 01:28:25,420
in fact right now so

1454
01:28:25,430 --> 01:28:27,150
here it is

1455
01:28:27,200 --> 01:28:29,280
the regression problem we saw before

1456
01:28:29,330 --> 01:28:30,900
and here it is

1457
01:28:30,910 --> 01:28:32,960
another point on training set

1458
01:28:32,980 --> 01:28:38,930
so this is a very common situation you get outliers in data so someone's entering

1459
01:28:38,930 --> 01:28:43,270
these numbers in the table orange lemon orange lemon and then they were ham-fisted and

1460
01:28:43,270 --> 01:28:45,300
they put in one instead of minus one

1461
01:28:45,680 --> 01:28:49,540
or it just turns out that one of the items are free what happens to

1462
01:28:49,540 --> 01:28:51,400
be deformed and it's weird

1463
01:28:51,490 --> 01:28:54,910
and you don't want your whole learning system to be dominated by what turned out

1464
01:28:54,910 --> 01:28:58,530
to be a mistake what turned out to be something that was just a freak

1465
01:28:58,530 --> 01:29:03,070
so here we pull down the whole regression surface because of this one point

1466
01:29:03,130 --> 01:29:05,330
and maybe that's an unreasonable thing to do

1467
01:29:05,360 --> 01:29:09,910
so a least squares is often said to be not very robust to outliers and

1468
01:29:09,910 --> 01:29:13,290
what some people do with trying to identify these outliers and toss them out and

1469
01:29:13,290 --> 01:29:16,600
then we do the fit may be what you want to do is something other

1470
01:29:16,600 --> 01:29:17,770
than least squares

1471
01:29:17,780 --> 01:29:19,160
something that says

1472
01:29:19,170 --> 01:29:22,990
you know i i still want to sort of not ignore this happened but

1473
01:29:23,030 --> 01:29:25,740
i went to dominate everything i do

1474
01:29:25,750 --> 01:29:27,580
because of the

1475
01:29:29,240 --> 01:29:31,900
here's another example we saw before so a

1476
01:29:31,910 --> 01:29:34,120
this is the classification problem

1477
01:29:35,490 --> 01:29:40,500
the purple line is the result of fitting this data set by least squares

1478
01:29:42,290 --> 01:29:45,420
the weight vector associated with the purple line

1479
01:29:45,460 --> 01:29:51,940
and we predicting the response for given publication so we're computing the value which is

1480
01:29:51,940 --> 01:29:56,690
that which is w transpose x this thing is called the activation

1481
01:29:56,990 --> 01:30:01,600
if you want to do with and what we're trying to do is fit that

1482
01:30:02,620 --> 01:30:06,100
two are observed data which turns out to be plus one to minus one because

1483
01:30:06,100 --> 01:30:07,620
the classification problem

1484
01:30:07,700 --> 01:30:08,480
so now

1485
01:30:08,530 --> 01:30:12,230
as we move away from the plane at the plane that there

1486
01:30:12,270 --> 01:30:14,690
so it will get bigger it will be

1487
01:30:14,770 --> 01:30:17,850
one then it will be bigger than one so

1488
01:30:17,900 --> 01:30:21,260
a long way away from the plane we're going to get very large values and

1489
01:30:21,270 --> 01:30:25,530
the reason this curve is skewed because if it within the sensible place like the

1490
01:30:25,530 --> 01:30:29,260
green light all of these points would be very far away from

1491
01:30:29,300 --> 01:30:33,370
and then maybe very mismatched from the value of one we trying to get so

1492
01:30:33,380 --> 01:30:38,120
when the reason that using this quest classification turned out to be a bad idea

1493
01:30:38,230 --> 01:30:40,710
is these point look like outline

1494
01:30:41,390 --> 01:30:46,230
two this model

1495
01:30:47,180 --> 01:30:51,100
what we can do is look at the different loss functions that we could use

1496
01:30:51,100 --> 01:30:53,020
the classification instead of

1497
01:30:53,050 --> 01:30:54,950
instead of the square

1498
01:30:56,630 --> 01:31:01,770
here i've got said this this activation which is sort of a measure of how

1499
01:31:01,770 --> 01:31:05,920
far away from this decision plan and my find a very long way away

1500
01:31:05,930 --> 01:31:09,410
from it in one direction i get a very large positive values and that indicates

1501
01:31:09,410 --> 01:31:10,740
positive class

1502
01:31:10,780 --> 01:31:14,820
very negative values indicate very clearly on the other side so

1503
01:31:14,850 --> 01:31:19,170
system is going to very confident say things are in the negative class if that

1504
01:31:19,170 --> 01:31:20,510
is very negative

1505
01:31:21,230 --> 01:31:23,980
so here's an example of the loss function

1506
01:31:23,990 --> 01:31:26,740
if the true label is one

1507
01:31:27,640 --> 01:31:30,530
the black line shows the zero one loss

1508
01:31:30,540 --> 01:31:32,050
we have lots one

1509
01:31:32,060 --> 01:31:36,320
if z is negative because for all negative values we going to predict minus one

1510
01:31:36,380 --> 01:31:39,420
and the true label with one as soon as we get a positive value the

1511
01:31:39,420 --> 01:31:40,450
black curve

1512
01:31:40,460 --> 01:31:43,370
because they're so for any positive value

1513
01:31:43,410 --> 01:31:45,460
the losses there

1514
01:31:46,410 --> 01:31:50,620
the zero one loss is maybe a sensible thing to fit classification

1515
01:31:50,740 --> 01:31:55,770
that's what the sensible decision boundary could be derived from

1516
01:31:55,780 --> 01:31:58,900
the squared error is the screen which says

1517
01:31:58,940 --> 01:31:59,740
if you're

1518
01:31:59,800 --> 01:32:04,250
really really far from the decision boundary on the correct side of the decision boundary

1519
01:32:04,260 --> 01:32:07,880
then for some reason i'm going to experience loss because of that

1520
01:32:07,890 --> 01:32:10,550
which is a bit of a funny thing to do

1521
01:32:11,700 --> 01:32:16,290
it makes sense if we are fitting classification problem for the thing that we're minimising

1522
01:32:16,290 --> 01:32:20,830
to be monotonic function of this activation if we claiming that very positive value should

1523
01:32:20,830 --> 01:32:26,570
indicate positive labels them as we move really really far out and we strongly saying

1524
01:32:26,570 --> 01:32:31,710
that the true label is positive then we should definitely has are lost

1525
01:32:31,740 --> 01:32:33,460
so the other two curves

1526
01:32:34,190 --> 01:32:38,770
blue curve on the red curves show monotonic functions that we could use instead

1527
01:32:39,170 --> 01:32:44,460
the only thing is called the hinge loss and is used in something called support

1528
01:32:44,460 --> 01:32:47,890
and then we look at the situation at the end

1529
01:32:47,930 --> 01:32:49,950
during the second second

1530
01:32:49,990 --> 01:32:51,010
it is clear

1531
01:32:51,030 --> 01:32:53,250
and this is a straight line

1532
01:32:53,290 --> 01:32:56,180
that the velocity remains constant

1533
01:32:56,220 --> 01:32:59,180
and it remains minus six meters per second

1534
01:32:59,190 --> 01:33:01,750
that's exactly what it was at this point at the end

1535
01:33:01,840 --> 01:33:04,310
you can see it go six meters

1536
01:33:04,360 --> 01:33:06,260
plus three two minus three

1537
01:33:06,310 --> 01:33:07,750
in one second

1538
01:33:07,800 --> 01:33:09,320
so the velocity

1539
01:33:09,410 --> 01:33:11,170
his minus six

1540
01:33:11,180 --> 01:33:15,030
meters per second

1541
01:33:15,150 --> 01:33:17,490
the acceleration is therefore zero

1542
01:33:17,540 --> 01:33:23,420
you see that acceleration changes abruptly from minus six meters per second squared

1543
01:33:23,470 --> 01:33:25,340
two zero so

1544
01:33:25,400 --> 01:33:30,070
i can tell you what it is exactly at this moment in time

1545
01:33:30,200 --> 01:33:32,150
that's the situation during

1546
01:33:34,920 --> 01:33:37,780
second what is the situation at the end

1547
01:33:38,960 --> 01:33:40,310
of the second section

1548
01:33:41,620 --> 01:33:43,150
sect couldn't

1549
01:33:43,150 --> 01:33:48,030
at the end i know that x equals minus three

1550
01:33:48,080 --> 01:33:49,860
what is the velocity

1551
01:33:49,910 --> 01:33:51,920
i don't know

1552
01:33:51,960 --> 01:33:55,870
because it changes abruptly from minus six two zero so i don't know exactly what

1553
01:33:55,930 --> 01:33:57,470
is at that point it's

1554
01:33:57,520 --> 01:33:59,210
non physical things

1555
01:33:59,220 --> 01:34:01,340
very abrupt change

1556
01:34:05,070 --> 01:34:07,440
that's also very tricky things

1557
01:34:07,480 --> 01:34:09,160
because if the velocity

1558
01:34:09,180 --> 01:34:13,580
his minus sixty on this side of the two seconds

1559
01:34:13,640 --> 01:34:15,960
and he becomes zero

1560
01:34:16,010 --> 01:34:19,850
and if that happens it in a split second there must be a huge acceleration

1561
01:34:19,850 --> 01:34:22,990
just at that point which is non physical

1562
01:34:22,990 --> 01:34:25,310
so i would also put a question mark

1563
01:34:25,410 --> 01:34:29,000
at the a i don't know what it is

1564
01:34:29,000 --> 01:34:31,780
so they go to the thirty second

1565
01:34:31,800 --> 01:34:35,650
this part

1566
01:34:35,650 --> 01:34:37,760
let's first look

1567
01:34:37,810 --> 01:34:40,070
during the third

1568
01:34:41,520 --> 01:34:44,740
the object isn't going anywhere just sitting there

1569
01:34:44,740 --> 01:34:50,100
x remains minus three

1570
01:34:51,190 --> 01:34:52,990
the velocity

1571
01:34:53,000 --> 01:34:54,170
is zero

1572
01:34:54,200 --> 01:34:57,910
and is zero we can agree on that

1573
01:34:57,950 --> 01:35:01,620
what is the situation at the end

1574
01:35:01,710 --> 01:35:04,910
of the thirty seconds that means people three

1575
01:35:04,960 --> 01:35:07,330
well i know is that x minus three

1576
01:35:07,390 --> 01:35:09,920
that's non-negotiable

1577
01:35:09,930 --> 01:35:11,420
what the velocities

1578
01:35:11,430 --> 01:35:12,240
i don't know

1579
01:35:12,270 --> 01:35:14,690
because it's changing abruptly

1580
01:35:14,740 --> 01:35:15,870
one zero

1581
01:35:15,890 --> 01:35:18,860
a positive value sort of ill-defined

1582
01:35:18,920 --> 01:35:23,590
and the same is true for the acceleration of sudden change in velocity

1583
01:35:23,660 --> 01:35:26,480
that means there must be a huge acceleration

1584
01:35:27,260 --> 01:35:28,450
i no

1585
01:35:28,460 --> 01:35:29,790
ill defined

1586
01:35:29,820 --> 01:35:30,930
because this curve

1587
01:35:30,930 --> 01:35:34,290
is of course not very physical

1588
01:35:34,350 --> 01:35:36,300
let's now look

1589
01:35:36,440 --> 01:35:40,810
the last seconds four seconds

1590
01:35:40,860 --> 01:35:44,530
chris during

1591
01:35:45,260 --> 01:35:48,900
it's going from ninety three to play six in a straight line sort of velocity

1592
01:35:48,900 --> 01:35:50,260
is constant

1593
01:35:50,290 --> 01:35:54,000
is the velocity is constant there you can immediately conclude that is zero there is

1594
01:35:54,000 --> 01:35:55,590
no acceleration

1595
01:35:55,640 --> 01:35:59,430
and it goes ninety meters in a time span of one second

1596
01:35:59,480 --> 01:36:00,500
but it's now

1597
01:36:01,320 --> 01:36:05,400
plus nine meters per second

1598
01:36:05,480 --> 01:36:07,200
the object first

1599
01:36:07,210 --> 01:36:08,700
i went from

1600
01:36:08,740 --> 01:36:11,010
positive values of x

1601
01:36:11,060 --> 01:36:14,350
two zero into negative values for x doing all the time

1602
01:36:14,370 --> 01:36:15,810
the velocity was

1603
01:36:17,160 --> 01:36:19,450
sign convention and now

1604
01:36:19,520 --> 01:36:20,660
the velocity

1605
01:36:20,670 --> 01:36:22,660
it goes back to six

1606
01:36:22,740 --> 01:36:24,550
the velocity becomes plus nine

1607
01:36:26,090 --> 01:36:27,360
a second

1608
01:36:27,500 --> 01:36:30,600
what is the story at the end of the four seconds

1609
01:36:30,660 --> 01:36:34,050
well i can say is that actually because plastics

1610
01:36:34,070 --> 01:36:35,770
i don't know much more

1611
01:36:35,780 --> 01:36:37,410
i don't know what the velocities

1612
01:36:37,420 --> 01:36:44,760
neither do i know what the acceleration is the plot stops there any

1613
01:36:44,780 --> 01:36:46,750
now i would

1614
01:36:46,750 --> 01:36:49,660
i think that it is reasonable to ask the following question

1615
01:36:49,750 --> 01:36:55,030
what is the average velocity for instance between time zero and time for

1616
01:36:55,080 --> 01:36:57,080
average velocity

1617
01:36:57,090 --> 01:36:59,260
we define average velocity

1618
01:36:59,340 --> 01:37:01,040
as the position

1619
01:37:01,050 --> 01:37:01,930
at time

1620
01:37:01,930 --> 01:37:05,250
four seconds minus the position at time zero

1621
01:37:05,260 --> 01:37:06,700
divided by four

1622
01:37:06,710 --> 01:37:08,430
that is i definition

1623
01:37:08,440 --> 01:37:10,860
zero it is about six

1624
01:37:10,920 --> 01:37:12,760
at for applies six

1625
01:37:12,770 --> 01:37:14,500
so the upstairs is zero

1626
01:37:14,500 --> 01:37:19,360
the familiar with the bias variance dilemma in statistics its version of the same problem

1627
01:37:19,590 --> 01:37:24,500
cognitive scientist sometimes talk about constraints can you have learn has some hypothesis space and

1628
01:37:24,500 --> 01:37:27,400
there are some constraints that that tell you you

1629
01:37:27,410 --> 01:37:31,740
all logically possible hypotheses about what the word could mean maybe some

1630
01:37:31,750 --> 01:37:33,120
some are more

1631
01:37:33,150 --> 01:37:34,660
right than others

1632
01:37:34,680 --> 01:37:38,900
so we know that in some sense the former the answer but the questions that

1633
01:37:38,900 --> 01:37:42,340
we we want our are go beyond just saying abstract knowledge we want to know

1634
01:37:42,340 --> 01:37:46,230
how does some kind of prior knowledge abstract knowledge guide learning and inference from sparse

1635
01:37:46,230 --> 01:37:50,120
data what form does it take across different domains and tasks and how might that

1636
01:37:50,300 --> 01:37:54,990
technology itself be acquired particularly in machine learning

1637
01:37:55,000 --> 01:37:58,790
there's that you know there's there's an aesthetic value and some of the practical value

1638
01:37:58,790 --> 01:38:01,960
in wanting to have to wiring is little by hand as possible

1639
01:38:02,020 --> 01:38:05,400
now it's an open question how much of the brain is kind of wired by

1640
01:38:05,400 --> 01:38:08,670
hand maybe the invisible hand evolution but

1641
01:38:08,700 --> 01:38:14,230
it's it's quite likely that some significant parts of cognition are wired into supported by

1642
01:38:14,230 --> 01:38:17,650
things that are wired and so we don't necessarily want to say everything is learned

1643
01:38:18,780 --> 01:38:22,820
a lot of the deepest questions in the field these questions of nature versus nurture

1644
01:38:22,820 --> 01:38:28,450
empiricism versus nativism are debates about what's wired and what isn't and having computational tools

1645
01:38:28,450 --> 01:38:31,980
that can tell us how the most fundamental aspects knowledge might in principle be acquired

1646
01:38:32,220 --> 01:38:37,420
will give us useful ways to understand in reality whether and where

1647
01:38:37,460 --> 01:38:41,470
these these aspects of knowledge are in fact learned from data

1648
01:38:41,480 --> 01:38:45,620
and the kind of thing will talk about here in these two lectures are

1649
01:38:45,670 --> 01:38:49,530
using some ideas again you know most of these are somewhat familiar in machine learning

1650
01:38:49,530 --> 01:38:53,370
and i think you've seen oops i think you've seen versions of of some of

1651
01:38:53,370 --> 01:38:54,890
these already here

1652
01:38:54,910 --> 01:38:57,940
but i'm going to show you how to apply them to these kinds of problems

1653
01:38:57,940 --> 01:39:01,890
motivated by human learning and maybe mix them up in some interesting ways that are

1654
01:39:01,890 --> 01:39:06,440
not standard machine learning but i think we'll give us the potential for more interesting

1655
01:39:06,440 --> 01:39:11,500
human like machine learning approaches so roughly these ideas that will be talking about the

1656
01:39:11,500 --> 01:39:16,310
technical ideas i've organised in answer to these questions but really they all go together

1657
01:39:16,330 --> 01:39:20,930
interesting ways not just for convenience so the basic form

1658
01:39:20,940 --> 01:39:25,840
of understanding how abstract knowledge guide learning from sparse data will be formalising from a

1659
01:39:25,840 --> 01:39:30,820
bayesian point of view sort of more generally probabilistic inference point of view and and

1660
01:39:30,820 --> 01:39:35,330
thinking very strong in in terms of generative models so your your prior knowledge is

1661
01:39:35,330 --> 01:39:38,520
not just a bunch of numbers but it takes so takes the form of a

1662
01:39:38,670 --> 01:39:42,600
almost like a causal description of what's out there in the world that generates the

1663
01:39:42,600 --> 01:39:44,250
data that you're seeing

1664
01:39:44,720 --> 01:39:48,450
the the the kind of generative models will talk about some of them are going

1665
01:39:48,450 --> 01:39:50,540
to be the sort of things are familiar with

1666
01:39:50,610 --> 01:39:53,990
if for machine learning and most of what do today are going to stick to

1667
01:39:53,990 --> 01:39:59,460
that so you know things like representations like that multi dimensional vector spaces are graphical

1668
01:39:59,460 --> 01:40:02,840
models but to capture these more

1669
01:40:02,870 --> 01:40:08,840
cognitive parts of cognition higher-level intuitive theories regarding any more structured kinds of representations of

1670
01:40:08,840 --> 01:40:13,990
things like grammars logic schemas even even programs so we're going to be writing down

1671
01:40:13,990 --> 01:40:20,350
probabilities over logic or or or programs where those things will describe much more

1672
01:40:20,360 --> 01:40:25,790
powerfully structured generative models and from from again from a sort of historical point of

1673
01:40:25,790 --> 01:40:32,790
view these these more structured representations are often not certainly not associated with machine learning

1674
01:40:32,790 --> 01:40:34,700
or even the whole

1675
01:40:35,740 --> 01:40:40,230
a statistical approach to a i but more the kind of classical good old-fashioned AI

1676
01:40:40,230 --> 01:40:43,500
in the early days of AI where everything was about symbols in logic and all

1677
01:40:43,500 --> 01:40:47,980
that and what many people have come to realize is that if you like there

1678
01:40:47,980 --> 01:40:52,970
are these there's two classic areas of artificial intelligence and they had caught have corresponding

1679
01:40:52,970 --> 01:40:56,530
areas in cognitive science the symbolic

1680
01:40:56,550 --> 01:41:01,020
error and the statistical error and people have come to realize that those aren't don't

1681
01:41:01,020 --> 01:41:02,610
have to be defined as

1682
01:41:02,780 --> 01:41:07,930
competing conflicting worldviews but actually a lot of the the the interesting meat is to

1683
01:41:07,930 --> 01:41:10,490
be found in coming up with ways to combine those ideas and to be able

1684
01:41:10,490 --> 01:41:13,930
to do probabilistic inference over structured representations

1685
01:41:13,940 --> 01:41:19,080
and in even very sophisticated kinds of symbolic objects like computer programs

1686
01:41:19,100 --> 01:41:22,820
so there is for example and its last year some of us and bunch of

1687
01:41:22,820 --> 01:41:27,700
others organised a workshop on probabilistic programming which means many different things but it's it's

1688
01:41:27,700 --> 01:41:31,720
giving tools to try to do this sort of thing and it's it's it's increasingly

1689
01:41:31,730 --> 01:41:35,690
very active area of research on both human and machine sites

1690
01:41:35,700 --> 01:41:39,050
and then as far as trying to understand how these these this EM technology might

1691
01:41:39,050 --> 01:41:43,410
be acquired well if you like we're asking what the priors come from and within

1692
01:41:43,410 --> 01:41:48,300
bayesian statistics in certain very simple forms bayesian statisticians have long had an answer to

1693
01:41:48,300 --> 01:41:53,550
that they called hierarchical models hierarchical bayes and over the last few years in part

1694
01:41:53,550 --> 01:41:58,210
through the work of people like you y and zoubin and others and a lot

1695
01:41:58,210 --> 01:42:02,290
of people here organising this summer school these ideas have have come into machine learning

1696
01:42:02,290 --> 01:42:06,710
and in very powerful ways and i think you've already seen some of them but

1697
01:42:06,840 --> 01:42:11,570
you know the the cut the kind science version of hierarchical bayes what is sometimes

1698
01:42:11,570 --> 01:42:17,070
called learning to learn where you take the priors that the guide learning themselves are

1699
01:42:17,070 --> 01:42:20,150
the objects of learning so we'll see some of those ideas and will also be

1700
01:42:20,900 --> 01:42:24,390
a little bit of nonparametric bayes which i think you i was just to tell

1701
01:42:24,390 --> 01:42:27,640
you about this this is very important because

1702
01:42:27,660 --> 01:42:28,780
if you think about it

1703
01:42:28,790 --> 01:42:32,590
human learning where learning over entire lifetime and again you not just learning a little

1704
01:42:32,590 --> 01:42:37,570
bit of knowledge here the problems which motivate the nonparametric approach are

1705
01:42:37,700 --> 01:42:41,700
central here right you can just say that a model of some fixed finite complexity

1706
01:42:42,200 --> 01:42:44,630
and you know what that is in advance but you're going to have to have

1707
01:42:44,630 --> 01:42:49,100
ways to allow the effective complexity of the model to grow over whole lifetime trading

1708
01:42:49,100 --> 01:42:53,720
off the in occam's razor it's always it you know the the complexity and fit

1709
01:42:53,730 --> 01:42:58,750
just the sorts of things that are are very elegantly approach to nonparametric perspective

1710
01:42:58,770 --> 01:42:59,800
so we'll see

1711
01:42:59,810 --> 01:43:03,210
some of these five years over today in the next

1712
01:43:03,260 --> 01:43:09,380
OK so so we finished the introduction there will be kind of three ten kind

1713
01:43:09,600 --> 01:43:13,140
case study areas for probably

1714
01:43:13,150 --> 01:43:14,150
the next

1715
01:43:14,160 --> 01:43:16,010
twenty minutes or so i'll talk about

1716
01:43:16,040 --> 01:43:22,190
cognition as probabilistic inference this is just laying the groundwork basically showing how the basic

1717
01:43:22,190 --> 01:43:25,940
idea of bayesian inference and some very very simple kinds of generative models really really

1718
01:43:25,940 --> 01:43:29,340
simple you might even think trivial almost textbook statistics

1719
01:43:29,340 --> 01:43:34,090
a lot

1720
01:43:43,100 --> 01:43:47,670
and then you the

1721
01:43:53,240 --> 01:44:07,060
three of them on the top of it

1722
01:44:38,970 --> 01:44:43,020
you always

1723
01:45:00,690 --> 01:45:08,020
that is

1724
01:45:55,380 --> 01:46:02,090
i always

1725
01:46:12,800 --> 01:46:21,420
you want

1726
01:47:00,330 --> 01:47:05,260
why do we

1727
01:47:07,850 --> 01:47:11,460
well one

1728
01:47:31,030 --> 01:47:34,050
you really are

1729
01:47:40,010 --> 01:47:43,770
so i or

1730
01:48:16,400 --> 01:48:19,610
where i

1731
01:48:19,780 --> 01:48:26,990
by the time

1732
01:48:39,360 --> 01:48:41,670
i would like to

1733
01:48:47,930 --> 01:48:49,570
i t

1734
01:48:49,840 --> 01:48:55,690
i will be one

1735
01:49:42,040 --> 01:49:45,200
of the

1736
01:49:45,200 --> 01:49:48,360
dictionary that has to be stable

1737
01:49:48,390 --> 01:49:52,090
so these are the classical release about quality

1738
01:49:52,110 --> 01:49:57,690
if you change is going to have to change this is not too difficult to

1739
01:49:58,080 --> 01:50:01,940
the more difficult elements that you have to get

1740
01:50:01,980 --> 01:50:04,790
if that you have to be able to recover

1741
01:50:04,800 --> 01:50:08,790
the appropriate that's the appropriate features

1742
01:50:08,790 --> 01:50:12,300
for that you have to make sure that an element g

1743
01:50:12,310 --> 01:50:15,260
combination of elements of down

1744
01:50:15,260 --> 01:50:18,700
correlate in some sense that there is a dictionary

1745
01:50:18,720 --> 01:50:20,230
she's in love

1746
01:50:20,250 --> 01:50:24,260
then the dictionary which is not like

1747
01:50:24,850 --> 01:50:30,030
that you think that this be a kind of be different from individual elements of

1748
01:50:31,510 --> 01:50:33,190
that may not reach

1749
01:50:33,240 --> 01:50:37,360
so what are the kind of necessary conditions you have to make that work

1750
01:50:37,380 --> 01:50:41,530
you need to survive the condition if you kill five people with you there is

1751
01:50:41,530 --> 01:50:45,030
no way you'll be able to recover

1752
01:50:45,040 --> 01:50:50,780
you the here that these weights for shouldn't look alike too much

1753
01:50:51,710 --> 01:50:55,740
the way the lead and if they slightly translated with that they are basically the

1754
01:50:55,740 --> 01:50:59,770
same they look too much like you won't be able to distinguish

1755
01:51:01,370 --> 01:51:05,300
the set should not be too large because the set is too large

1756
01:51:05,320 --> 01:51:07,350
this you will never

1757
01:51:07,360 --> 01:51:13,200
so these are the basic conditions in order to be able to do that can

1758
01:51:13,240 --> 01:51:18,600
OK let's let's now go to the problem of video so in that case you

1759
01:51:18,600 --> 01:51:21,450
have an operator which subsamples the data

1760
01:51:21,470 --> 01:51:26,900
and the first idea that comes to mind is for example to do linear interpolation

1761
01:51:26,920 --> 01:51:30,550
and that's what you get when you go from here to here which is to

1762
01:51:30,550 --> 01:51:32,230
be explained interpolation

1763
01:51:33,140 --> 01:51:38,670
what immediately realizes that you could do better and why can you do resolution

1764
01:51:38,690 --> 01:51:43,010
because you have some kind of prior information about the fact that there are some

1765
01:51:43,010 --> 01:51:44,750
geometric regularities

1766
01:51:44,770 --> 01:51:47,030
for example the directions

1767
01:51:47,050 --> 01:51:49,870
well you should interpolate on

1768
01:51:50,090 --> 01:51:56,900
uniform neighbourhood you should interpolate to exploit this regularly along this direction

1769
01:51:56,920 --> 01:51:59,940
now the problem is that you don't know these directions so

1770
01:51:59,950 --> 01:52:01,190
you have to find

1771
01:52:02,040 --> 01:52:05,040
so that's the basic intuition behind that

1772
01:52:05,890 --> 01:52:09,980
if you look at this basic intuition in the fourier domain that's what you

1773
01:52:09,980 --> 01:52:14,420
on the left you have the original signal centre

1774
01:52:14,450 --> 01:52:17,200
so the fourier transform one line long

1775
01:52:18,270 --> 01:52:21,770
this is something you have your phone

1776
01:52:21,870 --> 01:52:28,150
population basically makes the projection over space which is the fixed base of low frequency

1777
01:52:28,170 --> 01:52:33,700
and what direction on interpolation will do is to that the space of frequency of

1778
01:52:33,700 --> 01:52:38,380
which are project you do that and boom you recover of course a much more

1779
01:52:38,380 --> 01:52:40,840
precise interpolation

1780
01:52:42,020 --> 01:52:45,360
the idea that we're going to do is the following

1781
01:52:45,390 --> 01:52:47,760
for this kind of problem

1782
01:52:47,760 --> 01:52:49,800
dictionary techniques

1783
01:52:51,090 --> 01:52:54,830
two they work or they don't work that's the question OK if you want to

1784
01:52:54,830 --> 01:52:58,250
try dictionary you may be used for example with

1785
01:52:58,250 --> 01:53:03,900
well let's said reasonable they provide sparse representation now the problems of with that is

1786
01:53:03,900 --> 01:53:07,060
that they have a very large or fine scale

1787
01:53:07,070 --> 01:53:09,230
you want to recover the fine scale

1788
01:53:09,240 --> 01:53:10,760
that may be

1789
01:53:12,100 --> 01:53:17,370
in other words you're going to be which is zero so the weight coefficients fine

1790
01:53:17,380 --> 01:53:19,030
scale idea

1791
01:53:19,350 --> 01:53:23,210
that's why a lot of work was done in the early two thousands to try

1792
01:53:23,210 --> 01:53:27,620
to build a different dictionary richard dictionary with geometrical structures

1793
01:53:27,640 --> 01:53:33,700
curvelet bandlets x like many trials have been done these are examples of bandlet so

1794
01:53:33,700 --> 01:53:39,520
how do you constructive and by linear combination of wavelengths to build a gated structures

1795
01:53:39,520 --> 01:53:44,890
so that these structures will take advantage of geometric regularity and will not die

1796
01:53:44,940 --> 01:53:47,670
issues of samples and that's how

1797
01:53:47,720 --> 01:53:50,520
we decided was that kind of tool to build

1798
01:53:50,540 --> 01:53:51,470
the company

1799
01:53:51,490 --> 01:53:56,380
so basically huge dictionary that

1800
01:53:56,440 --> 01:54:01,270
you have a theory which states it provides you a great sparsity presentation of any

1801
01:54:01,270 --> 01:54:04,810
piecewise regular cartoon image

1802
01:54:04,830 --> 01:54:08,030
if you go to venture capitalist and you can get money out of that because

1803
01:54:08,030 --> 01:54:11,060
luckily they've never done process for

1804
01:54:11,140 --> 01:54:14,120
and then you have to try to do it doesn't

1805
01:54:14,130 --> 01:54:19,370
doesn't work what does it mean doesn't it works very nicely on five six ten

1806
01:54:19,370 --> 01:54:24,450
images but when you need to have video that going to show four hours to

1807
01:54:24,450 --> 01:54:27,230
people who are in front of the screen to look whether

1808
01:54:27,230 --> 01:54:29,550
it's good or not it doesn't work

1809
01:54:29,560 --> 01:54:34,620
it doesn't work because in fact it doesn't provide sufficiently sparse representation

1810
01:54:34,630 --> 01:54:37,240
so that you will not be able to

1811
01:54:37,240 --> 01:54:40,750
to solve your inverse problems in a stable way

1812
01:54:41,650 --> 01:54:43,960
what do people do in practice

1813
01:54:43,960 --> 01:54:47,050
they is something but simple they tried to freely

1814
01:54:47,070 --> 01:54:49,200
try to grab all these

1815
01:54:49,210 --> 01:54:53,350
the degree of freedom which are these elements from the dictionary they basically have one

1816
01:54:53,350 --> 01:54:57,560
degree of freedom which is the direction of interpolation and we so this is equivalent

1817
01:54:57,560 --> 01:54:59,300
to projection in this space

1818
01:54:59,320 --> 01:55:01,610
so the idea is to say well

1819
01:55:01,630 --> 01:55:02,880
what can you do

1820
01:55:02,900 --> 01:55:06,570
if you have to solve this problem instead of having a dictionary

1821
01:55:06,590 --> 01:55:09,790
waveform with many many of these

1822
01:55:09,800 --> 01:55:13,790
let's structure the problem and just have a dictionary of vector space

1823
01:55:14,530 --> 01:55:17,670
so i'm going to work with the dictionary of vector space

1824
01:55:17,690 --> 01:55:21,340
and the only condition so this is the work which is

1825
01:55:21,520 --> 01:55:23,320
question you

1826
01:55:23,370 --> 01:55:24,730
his thesis an

1827
01:55:24,730 --> 01:55:29,010
the basic idea is you're going to build new vector space so that you're operator

1828
01:55:29,010 --> 01:55:33,060
the restriction of your operator to the image of these spaces is invertible

1829
01:55:33,770 --> 01:55:35,940
you want this in practice

1830
01:55:35,960 --> 01:55:39,760
and as you do the same thing that was the dictionary tried to decompose a

1831
01:55:39,760 --> 01:55:41,360
sparse representation

1832
01:55:41,390 --> 01:55:44,550
as a linear combination of elements in your vector space

1833
01:55:44,550 --> 01:55:47,980
but it's much easier vector space of

1834
01:55:48,000 --> 01:55:51,520
doing a combination of two vector spaces are very similar

1835
01:55:51,600 --> 01:55:53,010
you look at the data

1836
01:55:53,050 --> 01:55:54,540
and of course there is

1837
01:55:54,560 --> 01:56:00,190
combination of image of your elements of vector space by the operation to you

1838
01:56:00,260 --> 01:56:05,860
so you have fast this transform dictionary of vector space

1839
01:56:05,890 --> 01:56:08,150
so basically it's the same idea

1840
01:56:08,170 --> 01:56:10,960
you can out of that version

1841
01:56:10,970 --> 01:56:14,980
and what does it mean to hurt you belong to the vector space e to

1842
01:56:14,980 --> 01:56:15,710
the u

1843
01:56:15,730 --> 01:56:20,060
so by the inverse what is is bounded inverse doing

1844
01:56:20,070 --> 01:56:23,750
this is the direction of interpolation basically what i'm just saying

1845
01:56:23,760 --> 01:56:27,600
i found locally what's the appropriate direction i'm going to invert

1846
01:56:27,630 --> 01:56:31,980
the set something we a directional into polish

1847
01:56:31,990 --> 01:56:35,340
the reason why this is much more stable again is that

1848
01:56:35,360 --> 01:56:39,210
you have much fewer vector space and the number of elements

1849
01:56:39,210 --> 01:56:42,040
OK now how do construct vector

1850
01:56:42,060 --> 01:56:46,220
one way to do it which is simple is to begin from something that

1851
01:56:46,890 --> 01:56:52,820
reasonably sparse in this case where something here to work with vector space which are

1852
01:56:52,820 --> 01:56:54,590
built with blocks

1853
01:56:54,640 --> 01:57:00,610
of wavelet coefficients OK so i'm going to build here block b of wavelet coefficients

1854
01:57:00,630 --> 01:57:06,310
and within the space of log p of wavelet coefficients i recombine them to potentially

1855
01:57:06,330 --> 01:57:09,700
be correlated coefficient make it better than approximation

1856
01:57:09,700 --> 01:57:12,250
we use a new orthogonal basis

1857
01:57:12,380 --> 01:57:18,060
PCA basis which typically could be looks like it is it but the main point

1858
01:57:18,080 --> 01:57:19,100
is example

1859
01:57:20,970 --> 01:57:24,630
that last space which are basically

1860
01:57:24,650 --> 01:57:27,780
constructed by lots of ways this

1861
01:57:27,780 --> 01:57:31,220
we potentially in combination with these wavelengths two

1862
01:57:31,240 --> 01:57:34,780
as i said we correlates the crayfish

1863
01:57:34,800 --> 01:57:39,420
how many PCA coefficients are going to take going to adjust it so that my

1864
01:57:39,420 --> 01:57:42,760
operator you is invertible over myspace

1865
01:57:42,760 --> 01:57:44,910
is about using the

1866
01:57:44,950 --> 01:57:46,160
which was

1867
01:57:46,160 --> 01:57:48,950
o point in order to optimise your hyperparameters

1868
01:57:51,410 --> 01:57:54,990
i think this is a really important thing before i don't think even for human

1869
01:57:54,990 --> 01:57:59,740
vision or for computer vision will be able to do it with these friends energy

1870
01:57:59,740 --> 01:58:04,030
functions were to figure out how to make use of these hundreds of thousands of

1871
01:58:04,030 --> 01:58:08,850
numbers that we get any given visual input i just want to mention that the

1872
01:58:09,280 --> 01:58:12,590
c ninety nine percent of the work recently has been

1873
01:58:12,640 --> 01:58:14,820
in the training test paradigm

1874
01:58:15,120 --> 01:58:20,800
actually interacts with his thesis which was supervised by geoff hinton actually did empirical basis

1875
01:58:20,800 --> 01:58:22,680
for low level vision many years ago

1876
01:58:23,530 --> 01:58:25,180
and this

1877
01:58:25,200 --> 01:58:29,260
it's now become much more popular topic but i think it's time to revisit this

1878
01:58:29,260 --> 01:58:34,200
idea was new techniques many of which have been developed developed again here in hopes

1879
01:58:34,350 --> 01:58:39,140
for nonparametric inference i think we can go back and he was much more complicated

1880
01:58:39,430 --> 01:58:42,890
products energy functions using this framework

1881
01:58:42,910 --> 01:58:47,200
OK so in conclusion i convinced that despite the mind

1882
01:58:47,780 --> 01:58:53,740
the vision and humans is actually consistent with highly sophisticated energy minimisation

1883
01:58:55,430 --> 01:58:58,300
problems we can go to optimise the energy

1884
01:58:58,320 --> 01:59:00,120
despite the NP hardness

1885
01:59:00,140 --> 01:59:06,320
and you can get functions despite an exponential number of constraints but despite this progress

1886
01:59:06,320 --> 01:59:09,890
we have a lot of work to do to match the remarkable performance of low

1887
01:59:10,070 --> 01:59:15,640
vision in humans thank you very much

1888
01:59:35,320 --> 01:59:39,760
such services see that military regime by the computer vision or biological vision

1889
01:59:41,850 --> 01:59:48,800
one thing that strikes me is the computational neuroscientist is claiming that the human

1890
01:59:48,820 --> 01:59:55,760
vision is fantastic and it all these problems that text console with computer vision

1891
01:59:55,910 --> 01:59:58,990
and to me it seems that

1892
01:59:59,010 --> 02:00:00,660
so just naively that

1893
02:00:00,660 --> 02:00:05,430
human knowledge might not solve these these problems for two reasons why

1894
02:00:05,450 --> 02:00:08,700
we can do in fact just as we heard yesterday the continent track more than

1895
02:00:08,700 --> 02:00:10,740
four objects at the same time as moving so

1896
02:00:11,180 --> 02:00:16,430
really i don't think we're solving these problems really at the pixel level at which

1897
02:00:16,430 --> 02:00:18,910
this computer algorithms are trying to do so

1898
02:00:18,930 --> 02:00:24,820
o if i tend to think carefully about the image i might be able to

1899
02:00:24,820 --> 02:00:29,510
do pixel by pixel but usually i don't care about it i just go with

1900
02:00:29,530 --> 02:00:32,390
the with just eleven and second

1901
02:00:32,410 --> 02:00:37,470
if so i don't think it's the last major part that is solving it using

1902
02:00:37,470 --> 02:00:42,370
fantastic kind of visual models of what let's look like well what people look like

1903
02:00:42,370 --> 02:00:42,850
and all that

1904
02:00:43,120 --> 02:00:45,280
two to solve it just

1905
02:00:45,300 --> 02:00:51,600
just the way you use your model to so we don't have horses building all

1906
02:00:51,620 --> 02:00:55,490
other classes of objects and all the money is coming from highland division rather than

1907
02:00:56,050 --> 02:01:00,720
motion so i'm wondering whether you think these ideas might actually help to improve your

1908
02:01:01,010 --> 02:01:04,890
computer vision algorithms that have two points there

1909
02:01:04,890 --> 02:01:09,260
the second point i think it's about definition so i would define an algorithm that

1910
02:01:10,260 --> 02:01:12,120
description at the pixel level

1911
02:01:12,120 --> 02:01:17,200
the resolution as low level vision problem so it doesn't matter if you use knowledge

1912
02:01:17,220 --> 02:01:21,260
it's orthogonal to the issue of whether use knowledge not so

1913
02:01:21,280 --> 02:01:26,320
as far as i'm concerned you can think of of recognition field to optical flow

1914
02:01:26,720 --> 02:01:31,720
not only that's that's fine with me you can do without any that's that's very

1915
02:01:31,800 --> 02:01:36,740
question we do this we ask people to leave the pixel

1916
02:01:36,760 --> 02:01:41,280
in this experiment was talking about with the city charter capable of doing it in

1917
02:01:41,280 --> 02:01:45,760
the end that is you are capable of the velocity field can i ask you

1918
02:01:45,760 --> 02:01:48,930
to this pixel of correctly and you can say yes or no for every pixel

1919
02:01:49,370 --> 02:01:54,780
so it's true that may be that requires positive feedback loops but in the end

1920
02:01:54,870 --> 02:02:00,680
you have your visual system is capable of giving us these pixel accurate pixies at

1921
02:02:00,680 --> 02:02:07,410
what's right about that solving the global optimization problem since maybe a discussion about

1922
02:02:07,450 --> 02:02:11,470
in proceed simultaneously is an interesting one but

1923
02:02:11,490 --> 02:02:14,180
people come about three or four

1924
02:02:14,200 --> 02:02:18,820
simultaneously has about yesterday thanks

1925
02:02:18,850 --> 02:02:20,720
also question OK

1926
02:02:22,180 --> 02:02:23,700
thanks for you get

1927
02:02:23,720 --> 02:02:27,530
i was wondering whether you think that activision

1928
02:02:27,550 --> 02:02:33,760
good jump in in support the search for the energy function

1929
02:02:33,780 --> 02:02:37,300
should then we think he and seymour density of

1930
02:02:37,320 --> 02:02:40,820
how we should move in order to better perceive

1931
02:02:40,820 --> 02:02:43,700
and what is the right function

1932
02:02:43,720 --> 02:02:47,800
USS solution should look at the history

