1
00:00:00,000 --> 00:00:03,500
so it's like like the background knowledge to make mistakes like mistake bound there's no

2
00:00:03,500 --> 00:00:08,860
distribution assumption not surprisingly this is a harder problem right so anything to keep can

3
00:00:08,860 --> 00:00:13,050
be quickly learned can be mistake bound learned because whenever years you say i don't

4
00:00:13,050 --> 00:00:17,380
know you can just give us anything that turns anything mistake that you can learn

5
00:00:17,440 --> 00:00:19,690
the mistake bound model you can pack one

6
00:00:19,710 --> 00:00:23,550
but it's not the other way there's problems that you can mistake bound learn you

7
00:00:23,550 --> 00:00:27,610
can quickly learn and it's especially when they are taking advantage of the fact that

8
00:00:27,650 --> 00:00:31,210
you make a guess and if you're wrong you learn alot quick is not allowed

9
00:00:31,210 --> 00:00:34,360
to make that gas and so of an adversary can

10
00:00:34,380 --> 00:00:38,520
it's kind whittle away the hypothesis space and take a very very long time forcing

11
00:00:38,520 --> 00:00:42,550
it to make lots and lots of i don't knows

12
00:00:45,190 --> 00:00:50,590
i'm going to want to do is i'm going to present a bunch of examples

13
00:00:50,590 --> 00:00:54,570
of the easier with the examples how how i i just say it's not allowed

14
00:00:54,570 --> 00:00:55,630
to make mistakes

15
00:00:55,630 --> 00:00:57,170
it's as simple as that

16
00:00:57,230 --> 00:00:58,440
now have

17
00:00:58,500 --> 00:01:03,020
we we could do some examples but let's put it this is the way that's

18
00:01:03,030 --> 00:01:03,800
going to be

19
00:01:03,820 --> 00:01:06,710
here's why this very well suited to model learning so we're going to do

20
00:01:09,710 --> 00:01:13,380
have our exploration being driven by this known unknown distinction

21
00:01:13,400 --> 00:01:18,150
so anything in the world that our learner can't predict we're going to have to

22
00:01:18,150 --> 00:01:19,900
assume that something that's good

23
00:01:19,920 --> 00:01:22,840
and get the optimism in the face of uncertainty get to go out there and

24
00:01:22,840 --> 00:01:27,690
get the data once it gets there get that example find out the right answer

25
00:01:27,710 --> 00:01:31,110
and and it will not know that anymore it was yes

26
00:01:31,130 --> 00:01:32,710
i don't know that now

27
00:01:33,880 --> 00:01:37,630
so here's an example of something that we can quickly really simple

28
00:01:37,650 --> 00:01:41,480
we should all be able to get on board with which is learning the probability

29
00:01:41,480 --> 00:01:43,860
that a weighted coin is going to come up heads

30
00:01:43,880 --> 00:01:48,900
OK so of interest should we all know that if if we observe for n

31
00:01:48,920 --> 00:01:52,840
trials and we have x successes we can estimate the probability p the cartoon version

32
00:01:52,840 --> 00:01:54,920
of it is just x over and

33
00:01:54,940 --> 00:01:58,340
and what they have to down does it tells us

34
00:01:58,400 --> 00:02:02,840
the as a function of epsilon

35
00:02:02,860 --> 00:02:07,210
how likely it is that are estimate is going to be released the least epsilon

36
00:02:07,210 --> 00:02:08,650
close to the real answer

37
00:02:10,360 --> 00:02:11,520
i soon that you've

38
00:02:11,520 --> 00:02:13,690
the doing have been down earlier in the

39
00:02:13,710 --> 00:02:16,150
course the come up before you know

40
00:02:16,150 --> 00:02:17,670
interesting OK

41
00:02:18,380 --> 00:02:23,730
OK within them is also a little bit more about it and so

42
00:02:23,800 --> 00:02:29,800
suspenders concentration bound sort of thing so so so so what we're saying is

43
00:02:29,800 --> 00:02:32,880
no matter how big exit and more we can still get that kind of estimate

44
00:02:32,880 --> 00:02:36,550
right so i can flip the coin thirty times and it comes up heads five

45
00:02:36,550 --> 00:02:40,250
times so i could say well i'd estimate five hundred thirty is the probability becomes

46
00:02:40,250 --> 00:02:44,030
apparent the problem is that if we have the queen very much we should be

47
00:02:44,050 --> 00:02:47,920
all that certain that that they answer really is

48
00:02:49,070 --> 00:02:53,190
but as we get more and more data with high probability that the average we

49
00:02:53,190 --> 00:02:56,130
compute is really going to be very close to the real as the law of

50
00:02:56,130 --> 00:02:59,530
large numbers sort of thing so it to this just quantifies it says that if

51
00:02:59,530 --> 00:03:01,000
you've got a sample of size m

52
00:03:01,520 --> 00:03:04,670
and if you're interested in

53
00:03:04,730 --> 00:03:07,860
a variable that varies between

54
00:03:07,900 --> 00:03:12,750
being a right so the that's the range of values it can spit out

55
00:03:12,800 --> 00:03:17,250
o how how what's the probability that our estimate is going to be within a

56
00:03:17,250 --> 00:03:21,130
plant of that and it is this expression here you could take this and solve

57
00:03:21,130 --> 00:03:24,050
for what the probability is so

58
00:03:24,070 --> 00:03:27,400
sorry this is the probability you can take this and say if i'm saying i

59
00:03:27,400 --> 00:03:30,840
want to be able to shore one minus delta sure that i'm with abseil on

60
00:03:31,110 --> 00:03:33,960
you can solve for what i am what sample size is going to give you

61
00:03:33,960 --> 00:03:35,520
that assurance

62
00:03:35,530 --> 00:03:39,170
OK so what does this led to know is that one so if i want

63
00:03:39,170 --> 00:03:40,300
to be

64
00:03:40,300 --> 00:03:41,790
ninety five percent sure

65
00:03:41,800 --> 00:03:45,860
that i'm within point o one of the right probability

66
00:03:45,880 --> 00:03:48,710
we solve for and we say well here's how many examples you have to do

67
00:03:48,710 --> 00:03:50,230
to be that sure

68
00:03:50,230 --> 00:03:53,790
so that the quick algorithm quick quick learners good is going to say i don't

69
00:03:53,790 --> 00:03:59,340
know m times to gather that data and once it's gathered that data estimate with

70
00:03:59,340 --> 00:04:03,340
high probability is epsilon close and so we could just use that the estimate for

71
00:04:03,380 --> 00:04:05,290
then non

72
00:04:05,300 --> 00:04:07,820
OK want to say the function

73
00:04:07,840 --> 00:04:09,690
but i'm going to give you an we're going to play this says you're going

74
00:04:09,690 --> 00:04:11,270
to be a a quick learner

75
00:04:11,290 --> 00:04:14,960
we hope this discussed and so so that means that i give you the input

76
00:04:15,070 --> 00:04:17,840
and you have to give me the output and has to be correct

77
00:04:17,860 --> 00:04:20,090
or you can say i don't know

78
00:04:20,230 --> 00:04:24,170
but you can see forever because that would be cheating or so

79
00:04:24,210 --> 00:04:26,710
our first input one

80
00:04:27,710 --> 00:04:29,230
OK OK that's fair

81
00:04:29,550 --> 00:04:32,570
the output is one for that case can

82
00:04:32,590 --> 00:04:36,050
the next input is one

83
00:04:36,070 --> 00:04:37,400
i heard i don't know

84
00:04:37,420 --> 00:04:43,710
yes the output is one that's correct or the next input is zero

85
00:04:43,730 --> 00:04:46,710
i mean was one also turns out

86
00:04:46,730 --> 00:04:50,110
right next input is one

87
00:04:50,110 --> 00:04:52,270
good next input is zero

88
00:04:52,420 --> 00:04:55,790
we and we learn the function and you learn with no mistakes

89
00:04:55,800 --> 00:04:57,790
by the way congratulations

90
00:04:57,800 --> 00:05:01,750
all right so and how how many times you have sort of the ridiculous example

91
00:05:01,750 --> 00:05:05,750
but it turns out lots of lots of functions have elaboration that idea in

92
00:05:05,770 --> 00:05:09,900
so for this particular case how many mistakes not mistake sorry how many i don't

93
00:05:09,900 --> 00:05:11,770
knows did you use

94
00:05:11,770 --> 00:05:15,610
two and in fact is that bound for this class of functions

95
00:05:15,610 --> 00:05:16,930
x y

96
00:05:16,950 --> 00:05:18,360
c one

97
00:05:18,410 --> 00:05:20,050
o sin omega

98
00:05:20,070 --> 00:05:21,760
prior functions

99
00:05:21,800 --> 00:05:22,640
x two

100
00:05:22,640 --> 00:05:24,840
is c two

101
00:05:24,890 --> 00:05:26,660
cosine omega

102
00:05:26,700 --> 00:05:27,740
and x three

103
00:05:27,740 --> 00:05:29,510
this is three

104
00:05:29,530 --> 00:05:31,550
cosine omega

105
00:05:31,610 --> 00:05:33,530
i was looking for omega

106
00:05:33,550 --> 00:05:35,050
o now

107
00:05:36,390 --> 00:05:38,260
they go

108
00:05:38,300 --> 00:05:39,390
given by

109
00:05:39,430 --> 00:05:41,510
i'm telling you with omega is

110
00:05:41,530 --> 00:05:43,530
you're not going to negotiate that with

111
00:05:43,570 --> 00:05:47,660
we only solving for c one c two and c three and the steady state

112
00:05:47,660 --> 00:05:51,760
you will be able to do that because omega is non-negotiable you're going to get

113
00:05:51,760 --> 00:05:54,720
three equations with three unknowns c one

114
00:05:54,780 --> 00:05:55,620
he two

115
00:05:55,660 --> 00:05:57,340
he three

116
00:05:57,360 --> 00:06:00,030
you don't have to settle

117
00:06:00,070 --> 00:06:03,120
two only calculate the ratios of the you know

118
00:06:03,160 --> 00:06:04,820
going to get a real answer

119
00:06:04,840 --> 00:06:10,610
four c one c two and four c three which of course will depend all

120
00:06:10,660 --> 00:06:13,660
sure you know at

121
00:06:13,680 --> 00:06:18,570
we worry about phase angles here not only because there is no then

122
00:06:18,610 --> 00:06:20,220
and if there is no damping

123
00:06:20,360 --> 00:06:25,660
videoobject in phase out of it because it then began disease phase angles in between

124
00:06:25,660 --> 00:06:28,320
one hundred eighty degrees out of phase is a minus sign

125
00:06:28,360 --> 00:06:29,680
so we have our

126
00:06:29,760 --> 00:06:32,720
introduced on the it we face changes

127
00:06:32,760 --> 00:06:35,450
and zero base

128
00:06:35,470 --> 00:06:40,120
for that we have a plus and minus signs

129
00:06:40,950 --> 00:06:42,410
you are going to

130
00:06:42,510 --> 00:06:45,200
grind i did all the grinding

131
00:06:45,220 --> 00:06:48,090
every detail on the

132
00:06:48,090 --> 00:06:49,030
now i'm going to do

133
00:06:50,430 --> 00:06:52,950
however i want to make sure that if you

134
00:06:52,990 --> 00:06:54,490
go through that

135
00:06:55,390 --> 00:06:56,840
make grinding

136
00:06:56,880 --> 00:07:00,780
that you in the end up with the right

137
00:07:00,820 --> 00:07:04,470
solution so in that sense i'm going to help you little bit by giving you

138
00:07:04,510 --> 00:07:05,820
the d

139
00:07:05,820 --> 00:07:07,840
which is the the that we have here

140
00:07:07,880 --> 00:07:09,120
you have to

141
00:07:09,160 --> 00:07:10,200
bring me to

142
00:07:11,320 --> 00:07:12,430
so the

143
00:07:12,470 --> 00:07:16,180
it's going to be

144
00:07:16,220 --> 00:07:17,320
so we have to

145
00:07:17,360 --> 00:07:19,450
bye bye and you have to also

146
00:07:19,470 --> 00:07:23,240
omega as really

147
00:07:23,260 --> 00:07:25,590
OK of shorthand notation

148
00:07:25,640 --> 00:07:28,840
some of you may want to call it omega zero that's fine

149
00:07:28,860 --> 00:07:31,090
there's only one

150
00:07:31,140 --> 00:07:32,510
the only

151
00:07:32,510 --> 00:07:36,050
the brain an open and things but i still call it the nest remind you

152
00:07:36,050 --> 00:07:37,450
that it is

153
00:07:37,450 --> 00:07:40,140
the resonance frequency of the single

154
00:07:41,530 --> 00:07:43,010
then i d

155
00:07:43,070 --> 00:07:46,410
becomes miners omega squid

156
00:07:46,430 --> 00:07:52,340
always the result of the second derivative remember you only get miners omega square

157
00:07:52,410 --> 00:07:54,880
you get plus two

158
00:07:54,880 --> 00:07:56,570
omega as created

159
00:07:56,590 --> 00:08:01,740
you get the second column minus

160
00:08:01,740 --> 00:08:03,320
all we got was great

161
00:08:03,320 --> 00:08:05,800
and in the third column you get a zero

162
00:08:05,860 --> 00:08:07,410
no surprise

163
00:08:07,410 --> 00:08:10,450
to get a zero in the third column

164
00:08:10,450 --> 00:08:14,360
because the first differential equations

165
00:08:14,390 --> 00:08:17,360
has no connection with x-ray you know all

166
00:08:17,380 --> 00:08:18,090
and so

167
00:08:18,120 --> 00:08:19,880
you never see

168
00:08:19,910 --> 00:08:23,640
anything in the third column that will be zero

169
00:08:23,720 --> 00:08:27,490
but if you look at the second differential equation that has x one and x

170
00:08:27,490 --> 00:08:29,300
two and x three in

171
00:08:29,340 --> 00:08:31,880
so now you don't is zero

172
00:08:31,930 --> 00:08:34,390
so what you're going to see is minus

173
00:08:34,450 --> 00:08:36,380
omega as

174
00:08:36,430 --> 00:08:38,680
it's going to be a c one

175
00:08:38,720 --> 00:08:42,700
germany you get your mind is omega great

176
00:08:42,700 --> 00:08:47,010
i think it's plus two omega squid

177
00:08:47,030 --> 00:08:50,240
and your last hop was minus

178
00:08:50,260 --> 00:08:53,780
o mcguire's script

179
00:08:53,820 --> 00:08:55,860
number further differential equations

180
00:08:55,880 --> 00:08:57,890
there is no x one

181
00:08:58,950 --> 00:09:01,590
this zero here

182
00:09:01,660 --> 00:09:07,490
and then you get miners omega square

183
00:09:07,600 --> 00:09:09,160
and then you get

184
00:09:09,200 --> 00:09:11,910
is omega squid

185
00:09:13,360 --> 00:09:14,390
two omega

186
00:09:17,030 --> 00:09:19,090
and you have to take

187
00:09:20,890 --> 00:09:24,340
of this matrix that is d

188
00:09:24,390 --> 00:09:25,930
check to make sure

189
00:09:25,950 --> 00:09:29,450
i didn't slip up was minus signs when you get home

190
00:09:29,550 --> 00:09:34,030
you don't wonder why did you get that result

191
00:09:34,050 --> 00:09:35,300
and i think

192
00:09:35,340 --> 00:09:37,430
that looks good

193
00:09:37,430 --> 00:09:39,760
remember all those omega square

194
00:09:39,780 --> 00:09:43,030
always come from those second

195
00:09:43,090 --> 00:09:46,070
after the second derivative of cosine omega t

196
00:09:46,180 --> 00:09:48,320
always brings out the miners

197
00:09:48,380 --> 00:09:53,240
omega square so no surprise this is mine this is mine

198
00:09:53,260 --> 00:09:56,140
that's why

199
00:09:56,160 --> 00:10:03,890
so now we want to know would see one is

200
00:10:05,340 --> 00:10:07,300
the first column

201
00:10:07,340 --> 00:10:09,450
really reflect

202
00:10:09,450 --> 00:10:12,110
this actor

203
00:10:12,160 --> 00:10:13,660
the right side now

204
00:10:13,720 --> 00:10:16,910
it's going to be zero remember like the double pendulum

205
00:10:16,950 --> 00:10:19,280
so you're going to get here

206
00:10:19,340 --> 00:10:20,860
the first column

207
00:10:20,950 --> 00:10:23,180
it's going to get omega

208
00:10:25,680 --> 00:10:29,240
finds at the zero

209
00:10:29,280 --> 00:10:33,320
and they're going to get a zero and zero

210
00:10:33,390 --> 00:10:37,660
and then this column

211
00:10:40,570 --> 00:10:45,070
and this column

212
00:10:48,320 --> 00:10:49,300
that is the

213
00:10:51,550 --> 00:10:55,910
you divided by d

214
00:10:55,910 --> 00:10:56,880
that then

215
00:10:56,910 --> 00:10:59,840
is c one

216
00:10:59,890 --> 00:11:01,970
and of course

217
00:11:02,030 --> 00:11:05,760
i only go one step further out to sea to that becomes a little boring

218
00:11:07,140 --> 00:11:09,180
the two then

219
00:11:09,260 --> 00:11:11,320
you get

220
00:11:11,450 --> 00:11:14,120
this one

221
00:11:14,180 --> 00:11:16,340
and then the second column

222
00:11:16,390 --> 00:11:21,550
according to agreements rule is going to be omega as great

223
00:11:23,660 --> 00:11:25,260
zero zero

224
00:11:25,320 --> 00:11:26,880
and then

225
00:11:26,910 --> 00:11:31,660
thirty one

226
00:11:33,160 --> 00:11:38,240
and there you go and we divided by p of course

227
00:11:38,300 --> 00:11:41,740
and then you can write on c three

228
00:11:41,780 --> 00:11:43,760
your neurons

229
00:11:43,800 --> 00:11:51,430
i help you

230
00:11:51,490 --> 00:11:54,050
so when you do this

231
00:11:54,090 --> 00:11:56,450
you could if you wanted to

232
00:11:56,490 --> 00:12:00,280
first of all forty what we call the resonance frequency

233
00:12:00,340 --> 00:12:03,610
the resonance frequencies either one

234
00:12:03,640 --> 00:12:05,680
but i don't want to do that

235
00:12:07,070 --> 00:12:10,610
so you may want to put in the equals zero

236
00:12:10,660 --> 00:12:13,070
the made determinant equals zero

237
00:12:13,070 --> 00:12:16,700
which gives you the three resonance frequencies which really we would have called

238
00:12:16,740 --> 00:12:18,030
normal mode

239
00:12:18,070 --> 00:12:20,760
in case we not driving

240
00:12:20,840 --> 00:12:24,090
so for those of you work this out

241
00:12:24,110 --> 00:12:26,240
the lowest

242
00:12:26,240 --> 00:12:29,200
frequency resonance frequency which

243
00:12:29,220 --> 00:12:30,340
the normal mode

244
00:12:31,610 --> 00:12:34,240
minus the square root of two

245
00:12:34,240 --> 00:12:37,000
this direction has this value

246
00:12:37,070 --> 00:12:38,680
so let's write down now

247
00:12:38,680 --> 00:12:40,210
newton's second law

248
00:12:40,230 --> 00:12:42,460
in the x direction

249
00:12:42,480 --> 00:12:43,800
so we have t

250
00:12:43,820 --> 00:12:45,290
in the positive direction

251
00:12:46,790 --> 00:12:48,960
and one gene

252
00:12:48,970 --> 00:12:51,550
signed off on

253
00:12:52,800 --> 00:12:55,490
its maximum force minus

254
00:12:57,940 --> 00:12:59,090
and one

255
00:13:00,360 --> 00:13:02,110
the cosine of

256
00:13:02,210 --> 00:13:03,020
that now

257
00:13:03,020 --> 00:13:05,600
according to newton's law must be one

258
00:13:05,630 --> 00:13:10,380
times age if a is the acceleration uphill

259
00:13:10,430 --> 00:13:12,550
one equation with two unknowns

260
00:13:12,580 --> 00:13:13,880
you don't know eight

261
00:13:13,960 --> 00:13:17,450
and you don't know t

262
00:13:17,450 --> 00:13:20,550
or do you know t

263
00:13:20,560 --> 00:13:22,320
what is to

264
00:13:22,330 --> 00:13:25,390
what is the tension

265
00:13:25,430 --> 00:13:30,040
what is the tension when that thing is being accelerated of

266
00:13:30,950 --> 00:13:33,520
the courage to try

267
00:13:33,550 --> 00:13:35,350
i think two g

268
00:13:35,390 --> 00:13:38,180
you couldn't be more wrong

269
00:13:38,220 --> 00:13:40,900
it's now moving into being accelerated

270
00:13:41,010 --> 00:13:45,310
that means this object is going to be accelerated downward

271
00:13:45,330 --> 00:13:50,110
and if this force is the same as this can never accelerate out

272
00:13:50,160 --> 00:13:53,400
this team as get smaller

273
00:13:54,590 --> 00:14:01,530
an object in an elevator being accelerated down loses weight it's losing weight

274
00:14:01,560 --> 00:14:05,470
this object must be accelerated that we have to take that into account so the

275
00:14:05,470 --> 00:14:09,230
pension wanted to start accelerating will go down

276
00:14:09,250 --> 00:14:12,290
so i have the second equation for object number two

277
00:14:12,310 --> 00:14:15,050
i call this plus the action

278
00:14:15,070 --> 00:14:16,790
so for object number two

279
00:14:16,790 --> 00:14:18,290
i have two

280
00:14:19,660 --> 00:14:21,580
minus two

281
00:14:21,590 --> 00:14:23,630
he calls and two

282
00:14:23,640 --> 00:14:24,730
times age

283
00:14:24,730 --> 00:14:28,320
it is very important that you see that the tension will change

284
00:14:28,400 --> 00:14:32,450
now you have two equations with two unknowns

285
00:14:33,290 --> 00:14:35,140
i can solve

286
00:14:35,160 --> 00:14:36,100
very easy

287
00:14:36,110 --> 00:14:38,770
you just have to and i leave you with that

288
00:14:38,770 --> 00:14:40,480
i'll just give you the results

289
00:14:40,500 --> 00:14:41,610
i find

290
00:14:41,630 --> 00:14:43,640
that the acceleration

291
00:14:44,790 --> 00:14:48,270
the course plus think three point eight five

292
00:14:48,350 --> 00:14:49,650
it is correct

293
00:14:49,660 --> 00:14:52,140
what's the point eight five

294
00:14:52,190 --> 00:14:54,150
meters per second squared

295
00:14:54,230 --> 00:14:56,010
and i find that pension

296
00:14:57,080 --> 00:14:59,810
twelve point three

297
00:15:03,190 --> 00:15:05,440
i want to dwell on this a little bit

298
00:15:05,490 --> 00:15:08,400
i find forty acceleration of plus signs

299
00:15:08,460 --> 00:15:10,780
had i found a minus sign

300
00:15:10,790 --> 00:15:11,650
i would

301
00:15:11,660 --> 00:15:15,290
i'm sure would have made a mistake why is it mandatory that i find plus

302
00:15:16,580 --> 00:15:19,260
absolutely mandatory

303
00:15:19,290 --> 00:15:21,400
who wants to try that one

304
00:15:23,410 --> 00:15:30,020
you say

305
00:15:30,070 --> 00:15:34,370
you say well i would have said it slightly differently we know that acceleration is

306
00:15:34,370 --> 00:15:39,080
in the direction we derive that they're forty acceleration since i call that plus x

307
00:15:39,080 --> 00:15:44,020
direction that was my last time must come out plus so this comes out negative

308
00:15:44,030 --> 00:15:45,990
you made a mistake

309
00:15:46,000 --> 00:15:49,470
i also want this number to be less than

310
00:15:50,740 --> 00:15:54,760
if not i've made a mistake why does that number have to be less than

311
00:16:00,890 --> 00:16:03,620
this object is going down

312
00:16:03,630 --> 00:16:07,120
to put it the way we put it last time it lost weight is accelerating

313
00:16:07,120 --> 00:16:12,300
downwards this and TG which is twenty better wins it from p otherwise it would

314
00:16:12,300 --> 00:16:16,190
never be accelerated down so plus sign is the most

315
00:16:16,200 --> 00:16:17,000
and this is

316
00:16:17,010 --> 00:16:19,330
most and if you find

317
00:16:19,350 --> 00:16:22,620
not a plus sign but minus sign you have to go back to your calculation

318
00:16:22,630 --> 00:16:25,820
because you made a mistake

319
00:16:27,000 --> 00:16:28,880
we take the same situation

320
00:16:28,900 --> 00:16:33,280
i leave everything unchanged but i make the second mass and two

321
00:16:33,310 --> 00:16:37,720
make it o point four kilogrammes

322
00:16:37,980 --> 00:16:43,110
so now all the numbers remained the same that we have there

323
00:16:43,120 --> 00:16:45,550
accepted and two g

324
00:16:45,660 --> 00:16:48,560
now becomes four

325
00:16:48,610 --> 00:16:51,070
now i'm going to test again

326
00:16:51,120 --> 00:16:54,400
this and two g which is four

327
00:16:54,410 --> 00:16:56,180
is that larger

328
00:16:56,190 --> 00:16:57,870
then five

329
00:17:00,230 --> 00:17:02,210
the frictional force static

330
00:17:02,220 --> 00:17:05,040
four point three three

331
00:17:05,050 --> 00:17:06,650
the answer is no

332
00:17:06,710 --> 00:17:09,460
going to test for my second case

333
00:17:09,490 --> 00:17:10,700
is an two g

334
00:17:12,700 --> 00:17:13,980
then five

335
00:17:13,990 --> 00:17:17,070
minus four point three three

336
00:17:17,130 --> 00:17:19,560
the answer is no

337
00:17:19,620 --> 00:17:24,280
so what do we conclude

338
00:17:24,280 --> 00:17:25,980
what must be our conclusions

339
00:17:26,000 --> 00:17:30,780
condition one is not met condition two is not met

340
00:17:30,790 --> 00:17:32,130
conclusion is

341
00:17:33,280 --> 00:17:35,040
is zero object

342
00:17:35,700 --> 00:17:36,820
not to be

343
00:17:40,440 --> 00:17:41,840
and the frictional force

344
00:17:41,860 --> 00:17:47,090
it is going to just along the x direction so that the acceleration indeed is

345
00:17:48,310 --> 00:17:50,630
how does the frictional force to that

346
00:17:50,680 --> 00:17:52,480
this is that slope

347
00:17:52,580 --> 00:17:54,250
he was that object

348
00:17:54,300 --> 00:17:56,080
i only put in the forces

349
00:17:56,860 --> 00:18:00,050
x direction i don't bother about the y direction

350
00:18:00,060 --> 00:18:02,370
i know that there is an one g

351
00:18:02,390 --> 00:18:03,850
sign of

352
00:18:03,900 --> 00:18:07,050
and that one is five

353
00:18:07,060 --> 00:18:08,620
so we have here

354
00:18:08,630 --> 00:18:10,430
the component of gravity

355
00:18:10,470 --> 00:18:12,020
which is the m one

356
00:18:13,180 --> 00:18:14,210
time of

357
00:18:14,220 --> 00:18:16,110
and we know that is five

358
00:18:16,130 --> 00:18:17,740
we had there

359
00:18:17,780 --> 00:18:19,220
i also note

360
00:18:19,270 --> 00:18:22,290
that we have tension here

361
00:18:22,300 --> 00:18:23,430
and the tension

362
00:18:23,430 --> 00:18:27,950
i must be and two g because the object is not being accelerated we're back

363
00:18:27,950 --> 00:18:32,880
where we were number two is not being accelerated the tension is twenty

364
00:18:32,900 --> 00:18:35,360
sorry not twenty big ones i and two

365
00:18:35,500 --> 00:18:37,390
tension is

366
00:18:37,610 --> 00:18:40,150
g is is four

367
00:18:40,200 --> 00:18:44,210
five newtons downhill for newton's uphill

368
00:18:44,210 --> 00:18:46,000
somehow to separate

369
00:18:46,020 --> 00:18:50,330
the red points from blue point so i would say something like this might be

370
00:18:50,330 --> 00:18:53,830
pretty reasonable it's not exactly clear where there

371
00:18:54,770 --> 00:19:01,000
so let me see what happens so any idea what will happen

372
00:19:01,060 --> 00:19:03,210
with go like this

373
00:19:03,210 --> 00:19:07,640
what about this point

374
00:19:07,690 --> 00:19:09,670
so let me let me put

375
00:19:09,710 --> 00:19:14,520
so this is the government the regularisation parameter remember how much i way

376
00:19:14,540 --> 00:19:28,690
that norm thing so let me put axis thirty here

377
00:19:28,690 --> 00:19:30,020
OK go

378
00:19:30,040 --> 00:19:31,660
that's pretty good right

379
00:19:31,660 --> 00:19:34,710
that's more or less what you expect

380
00:19:34,770 --> 00:19:41,560
so what happens if i make this regularisation parameter much smaller

381
00:19:41,580 --> 00:19:47,750
the regularisation parameter is

382
00:19:48,370 --> 00:19:52,690
regularisation parameter is this guy is lambda so what happens when i make it much

383
00:19:52,690 --> 00:19:54,080
much smaller

384
00:19:54,100 --> 00:19:58,330
what happens when they make this escape much smaller is that i don't care about

385
00:19:58,330 --> 00:20:01,710
the norm i concur very complex ponderous but

386
00:20:01,730 --> 00:20:03,420
i want them to be fit

387
00:20:03,420 --> 00:20:07,790
there i want i want the thing to fit the data very well

388
00:20:07,790 --> 00:20:11,120
that's also known as overfitting

389
00:20:11,160 --> 00:20:14,890
so i want to fit the data very well and i don't care about the

390
00:20:14,890 --> 00:20:18,790
bombing so let me being let me make it

391
00:20:27,210 --> 00:20:38,560
it doesn't work

392
00:20:54,690 --> 00:20:56,710
so you see what happens

393
00:20:56,750 --> 00:21:02,120
basically the global bond somehow doesn't instill much here but what happens is that all

394
00:21:02,120 --> 00:21:03,310
of these guys

395
00:21:03,330 --> 00:21:09,270
they get their own little bottle so now you fitting every point basically if says

396
00:21:10,400 --> 00:21:14,190
that has to be correct so this is exactly what overfitting is

397
00:21:14,190 --> 00:21:18,420
using well this point is probably some sort of noise really hear everything is blue

398
00:21:18,420 --> 00:21:21,160
i o thing you want to classify this is to mister

399
00:21:21,210 --> 00:21:24,670
and here is the same thing and i don't know here maybe maybe not

400
00:21:24,810 --> 00:21:28,750
but basically what happens when you put this parameter to be quite low you start

401
00:21:28,750 --> 00:21:34,540
getting very complex boundary but every point is classified correctly from the data but there

402
00:21:34,540 --> 00:21:37,480
is no reason to think that when you get a new point p h one

403
00:21:39,230 --> 00:21:41,960
one beretta probably in the likelihood that

404
00:21:42,020 --> 00:21:48,320
so you're not doing very well here you doing much better with the hyperparameter gamma

405
00:21:48,320 --> 00:21:53,620
so here you allowing more complex functions and you get better fit to the existing

406
00:21:53,620 --> 00:22:02,420
data but you probably will get worse fit to new data points

407
00:22:02,460 --> 00:22:06,390
OK so this is very good at least squares and

408
00:22:06,420 --> 00:22:10,350
OK so another thing to emphasise here that of course this is a very standard

409
00:22:10,350 --> 00:22:15,350
matrix inversion problem and you can solve a lot of problems like this

410
00:22:15,350 --> 00:22:16,560
it's a

411
00:22:16,580 --> 00:22:21,030
it works very well in many problems you can use it for digit classification and

412
00:22:21,030 --> 00:22:26,000
all sorts of things they were very often but failed

413
00:22:37,890 --> 00:22:47,020
why is this a good classifier they and if

414
00:22:47,020 --> 00:22:53,710
the intuition is the following this thing

415
00:22:53,750 --> 00:22:57,350
this function which we call basic

416
00:22:57,390 --> 00:23:01,890
so this is going to produce a function which is pretty nice

417
00:23:02,040 --> 00:23:06,960
and yet fits the data well that's why it's a good classifier

418
00:23:07,000 --> 00:23:13,690
so somehow this penalty penalizes functions which change too much

419
00:23:13,690 --> 00:23:17,790
i don't know if it answers your question

420
00:23:17,810 --> 00:23:26,960
why is it the good classifier

421
00:23:27,020 --> 00:23:29,810
right of course it depends on land right

422
00:23:29,890 --> 00:23:34,060
but basically what you want you want to penalize functions was complicated behavior and this

423
00:23:34,060 --> 00:23:38,980
is what what does of course it's a very simplistic olga from and in general

424
00:23:39,020 --> 00:23:45,350
you often want to do something more sophisticated than that but it's a very simple

425
00:23:45,350 --> 00:23:48,670
algorithm and work quite well over

426
00:23:48,670 --> 00:23:50,290
a wide variety of

427
00:23:51,330 --> 00:23:57,390
but you mean you have to see the correct date also sits on the floor

428
00:23:57,830 --> 00:24:04,940
and this is actually very similar to support vector machines i was playing for commissions

429
00:24:05,080 --> 00:24:08,620
i in the kernel case in the second

430
00:24:09,710 --> 00:24:14,480
so what support vector machine so as you saw before support vector machines so now

431
00:24:14,480 --> 00:24:18,640
of course i am i in this more general setting so support vector machines and

432
00:24:18,710 --> 00:24:21,190
the following support vector machines

433
00:24:23,000 --> 00:24:25,890
instead of analyzing the square feet

434
00:24:25,940 --> 00:24:30,730
europe analyse this function and this is the following function if f basically has the

435
00:24:30,730 --> 00:24:32,460
right side

436
00:24:32,480 --> 00:24:35,980
and it's not least well used at least one

437
00:24:36,710 --> 00:24:41,580
this doesn't produce any penalties so suppose y is plus one then if f is

438
00:24:41,580 --> 00:24:45,500
bigger than one then this guy is going to be bigger than one one minus

439
00:24:45,500 --> 00:24:48,710
something which is bigger than one is going to be negative when i take the

440
00:24:48,710 --> 00:24:52,850
plus sign passed positive part of what is going to visit and plasters means take

441
00:24:53,170 --> 00:24:57,040
zero if it's less than zero and take the same thing if it's possible

442
00:24:57,100 --> 00:25:01,810
so four if f of x is bigger than one in why one that's fine

443
00:25:01,810 --> 00:25:05,980
by the same token if f of x is less than one and y if

444
00:25:05,980 --> 00:25:08,210
less than minus one and y

445
00:25:08,210 --> 00:25:12,580
i use minus one so why i can only have two well this year

446
00:25:12,600 --> 00:25:17,310
one minus that you can have more but that's the classification

447
00:25:17,420 --> 00:25:20,520
so i think

448
00:25:20,520 --> 00:25:25,960
then this is fine so basically if this doesn't analyse you if you have the

449
00:25:25,960 --> 00:25:28,660
correct what about this well you still have some

450
00:25:31,640 --> 00:25:36,810
so this is a loss function and this is

451
00:25:36,810 --> 00:25:41,310
they tried to board the plane and they told me my passport was valid it

452
00:25:41,310 --> 00:25:47,670
had suspicious river near the photograph so since incoming canberra they're worried about the safety

453
00:25:47,670 --> 00:25:48,580
kevin rudd

454
00:25:48,620 --> 00:25:53,150
and i thought i was in danger to and they

455
00:25:53,200 --> 00:25:55,990
meaning often get a new passport which took a few days

456
00:25:56,040 --> 00:25:57,810
so i'm here now

457
00:25:58,520 --> 00:26:03,520
my name's at manassas you've probably read from and

458
00:26:03,540 --> 00:26:05,600
you should garden

459
00:26:05,650 --> 00:26:09,050
all nodes which were an extra two sets

460
00:26:09,320 --> 00:26:10,900
in addition to

461
00:26:10,910 --> 00:26:15,160
the ones the printing about that they and

462
00:26:15,440 --> 00:26:17,230
o one told me that that

463
00:26:17,250 --> 00:26:19,140
they now give you any

464
00:26:19,150 --> 00:26:22,470
oh sorry there are two sets of nodes the the the the action and so

465
00:26:22,470 --> 00:26:24,760
i give you should we have the class

466
00:26:24,760 --> 00:26:28,970
they have all that i can see that

467
00:26:29,000 --> 00:26:32,750
so i didn't see that consensus

468
00:26:35,230 --> 00:26:38,790
disaster directly OK now

469
00:26:38,890 --> 00:26:43,370
i'm not gonna stick entirely to the nodes working together partly because you've learned a

470
00:26:43,370 --> 00:26:47,150
lot of a bit of natural deduction john slaney right

471
00:26:47,170 --> 00:26:51,260
so we're not going to to go over the old material can set a little

472
00:26:51,260 --> 00:26:52,730
bit to introduce

473
00:26:52,750 --> 00:26:58,690
these ideas and since would sort of a compact enough five hours to learn about

474
00:26:58,780 --> 00:27:04,220
non-classical logic is what i'd also like to touch on some things you have heard

475
00:27:04,220 --> 00:27:07,750
about the lectures and relate to what we're doing

476
00:27:08,080 --> 00:27:12,620
in here kitty but what i'm going to start out by the way

477
00:27:12,670 --> 00:27:15,790
something kind useful to me and i don't starve all my

478
00:27:15,840 --> 00:27:20,810
logic lectures how many of you are from sort of have a background in

479
00:27:22,970 --> 00:27:27,280
OK so three of and how many have a background in

480
00:27:30,690 --> 00:27:33,290
it's more and new science

481
00:27:33,500 --> 00:27:35,590
OK linguistics anybody

482
00:27:35,610 --> 00:27:42,620
now can talk a little bit about linguistics and last lecture but only very little

483
00:27:43,840 --> 00:27:46,470
but i often find that

484
00:27:46,530 --> 00:27:51,400
clusters are sometimes the philosophy student sometimes you want to learn about the mathematics because

485
00:27:51,400 --> 00:27:56,000
it's enough to pick up things didn't pick up during their undergraduate years and sometimes

486
00:27:56,000 --> 00:28:01,430
the computer scientist and the mathematicians one here but the false because that's something that

487
00:28:01,440 --> 00:28:05,180
so we're going to have a mixture of all of these things

488
00:28:10,620 --> 00:28:14,440
first of all i want one of the things i did that a little bit

489
00:28:14,440 --> 00:28:15,740
unusual for me

490
00:28:15,770 --> 00:28:21,810
is that took with called proof theoretic approach to this topic

491
00:28:21,820 --> 00:28:26,490
my teaching on this was not that unusual now but my teaching has evolved on

492
00:28:31,400 --> 00:28:33,410
when you're talking about

493
00:28:33,430 --> 00:28:40,940
language philosophers positions of language is so strict separation between syntax and semantics

494
00:28:40,990 --> 00:28:46,030
so syntaxes grammar and usually the theory proof

495
00:28:46,070 --> 00:28:51,530
ah so you formation rules language in it and you're deduction system is natural deduction

496
00:28:51,620 --> 00:28:56,090
sequent calculus hilbert style system whatever

497
00:28:56,120 --> 00:28:59,840
right you've learned about all those things are we done the sequent calculus

498
00:28:59,900 --> 00:29:04,870
OK great going to talk about that in the last few lectures i i mean

499
00:29:04,870 --> 00:29:10,820
i can introduce sequent calculus sequent style natural deduction systems to hybrid systems that greg

500
00:29:10,820 --> 00:29:14,820
restall someone else's often tartness like in this summer school

501
00:29:14,930 --> 00:29:16,250
has developed

502
00:29:18,690 --> 00:29:20,520
he and know i don't

503
00:29:21,210 --> 00:29:25,690
but anyway out of over that later

504
00:29:25,690 --> 00:29:28,000
so what we can do here

505
00:29:28,220 --> 00:29:33,410
sorry so usually talk about that is the separation from

506
00:29:33,470 --> 00:29:40,340
diffloss often like to talk about the proof theory and not all philosophers talk about

507
00:29:41,750 --> 00:29:47,930
um information rules over one some grammar and proof on one side and semantics on

508
00:29:47,930 --> 00:29:51,810
the other side and semantics especially with the theory of meaning in the theory of

509
00:29:52,690 --> 00:29:56,420
also very meaning it's what we understand only

510
00:29:56,430 --> 00:30:00,010
what these symbols really mean to us

511
00:30:00,020 --> 00:30:05,380
well psychologically kind of sort of psychological and that's a long story but i'm not

512
00:30:05,380 --> 00:30:09,380
going to go into that story too deeply because as in the philosophy of language

513
00:30:09,380 --> 00:30:13,930
course and also we just don't have that much time fascinating though it is something

514
00:30:13,930 --> 00:30:15,460
i'll be teaching a few weeks

515
00:30:15,510 --> 00:30:19,210
back in my home university

516
00:30:22,710 --> 00:30:24,940
well we're going to a a little bit more

517
00:30:25,000 --> 00:30:30,240
this notion of semantics

518
00:30:35,870 --> 00:30:38,000
what these symbols mean what their

519
00:30:38,020 --> 00:30:40,330
what they represent

520
00:30:40,390 --> 00:30:44,200
how we are supposed to understand

521
00:30:45,390 --> 00:30:47,430
four caps for example

522
00:30:47,480 --> 00:30:48,620
refers to

523
00:30:48,620 --> 00:30:50,160
a little fuzzy things

524
00:30:50,170 --> 00:30:53,600
and it sort of means well maybe

525
00:30:56,690 --> 00:30:58,100
of the genus

526
00:30:59,490 --> 00:31:01,200
species feline

527
00:31:03,420 --> 00:31:08,830
but what are the logical symbols mean and or not

528
00:31:11,100 --> 00:31:16,190
the traditional

529
00:31:16,490 --> 00:31:26,330
but it traditional answers given in terms of model theory will now tradition

530
00:31:26,360 --> 00:31:28,250
since the nineteen thirties

531
00:31:28,270 --> 00:31:32,620
and you've learned a lot about model theoretically one possible world semantics eleven model theory

532
00:31:32,620 --> 00:31:36,000
for first order logic regularly states so far

533
00:31:36,020 --> 00:31:43,120
in courses right what the model theory sometimes people called semantics formal semantics

534
00:31:43,170 --> 00:31:47,370
because it so closely tied with this notion of

535
00:31:48,120 --> 00:31:50,610
the various connectors mean

536
00:31:50,670 --> 00:31:52,330
but there is another tradition

537
00:31:52,350 --> 00:31:57,230
in philosophy that goes back as well the nineteen thirties that's philosophy the philosophy of

538
00:31:58,770 --> 00:32:02,190
good as well back to nineteen thirties

539
00:32:02,240 --> 00:32:07,330
and is these days gaining a little more

540
00:32:19,240 --> 00:32:20,640
it doesn't tend

541
00:32:21,490 --> 00:32:27,110
philosophy textbooks or mathematics textbooks or any are computer science textbooks of the computer scientist

542
00:32:27,110 --> 00:32:30,420
i find are a lot more open to these ideas

543
00:32:30,430 --> 00:32:32,510
because of this idea

544
00:32:32,520 --> 00:32:35,000
that you can treat program is approved

545
00:32:35,830 --> 00:32:38,250
this the the

546
00:32:38,260 --> 00:32:41,500
the programs as proof and the

547
00:32:41,550 --> 00:32:43,750
the curry howard isomorphism

548
00:32:43,810 --> 00:32:45,370
because of those

549
00:32:45,430 --> 00:32:49,560
notions that you have taught touched on here and the other summer school

550
00:32:49,580 --> 00:32:52,830
sometimes some sort very hard as morphisms

551
00:32:52,870 --> 00:32:56,660
now OK but you learn what some of you learn about that elsewhere in order

552
00:32:56,740 --> 00:33:00,330
to talk about it too much we might a little while later if we get

553
00:33:00,330 --> 00:33:06,240
some time we talk about the sequent calculus because of those those ideas computer scientists

554
00:33:06,250 --> 00:33:09,680
at least are little more open to this what is the idea

555
00:33:09,680 --> 00:33:12,410
over all values of y i

556
00:33:12,460 --> 00:33:13,510
and why i

557
00:33:13,560 --> 00:33:16,740
plus one of the probability

558
00:33:17,510 --> 00:33:19,050
why i

559
00:33:19,090 --> 00:33:20,620
when i plus one

560
00:33:20,640 --> 00:33:22,350
given x

561
00:33:23,670 --> 00:33:27,140
corresponding value for a while

562
00:33:27,150 --> 00:33:30,650
why i plus one

563
00:33:30,660 --> 00:33:31,590
OK a

564
00:33:31,690 --> 00:33:33,610
now these quantities

565
00:33:33,620 --> 00:33:38,990
are exactly what i'm getting so forward backward algorithm

566
00:33:40,100 --> 00:33:43,610
except that well in that case they would have two red dots next to each

567
00:33:44,680 --> 00:33:46,870
so with some overall those variables

568
00:33:46,880 --> 00:33:48,520
now we're all these

569
00:33:48,540 --> 00:33:53,990
and with a slate which i mean just after normalize give me the probability of

570
00:33:53,990 --> 00:33:57,280
y and y plus one

571
00:33:57,360 --> 00:34:00,470
OK so i compute those numbers

572
00:34:00,560 --> 00:34:03,040
plug them in here

573
00:34:03,060 --> 00:34:06,600
and that gives me this expectation

574
00:34:06,620 --> 00:34:08,930
well not not completely done yet

575
00:34:08,980 --> 00:34:10,890
because at the end of the day

576
00:34:10,940 --> 00:34:11,900
i will

577
00:34:11,950 --> 00:34:15,320
then we need to compute maybe the inner product between that

578
00:34:15,330 --> 00:34:17,920
and some theta

579
00:34:19,150 --> 00:34:27,820
therefore if i take an approach between fatah and the expected value

580
00:34:27,840 --> 00:34:28,920
that's been

581
00:34:30,700 --> 00:34:32,820
the sum over all the y i

582
00:34:32,840 --> 00:34:35,210
and why are plus one

583
00:34:37,340 --> 00:34:39,590
why i

584
00:34:39,600 --> 00:34:41,900
why i plus one

585
00:34:41,950 --> 00:34:44,150
and then here i can just put

586
00:34:44,200 --> 00:34:51,830
post is inappropriate

587
00:34:51,880 --> 00:34:55,050
and this is an expression we know how to compute

588
00:34:55,100 --> 00:34:57,310
the could

589
00:34:57,330 --> 00:35:00,170
that's just some

590
00:35:00,180 --> 00:35:04,750
expansion and we get the kernel directly

591
00:35:04,780 --> 00:35:07,880
the question at that point

592
00:35:07,890 --> 00:35:09,260
it's going

593
00:35:09,280 --> 00:35:14,500
two one

594
00:35:14,510 --> 00:35:18,030
OK so in

595
00:35:18,150 --> 00:35:21,480
well in fact about three four slides down the track

596
00:35:21,500 --> 00:35:24,560
i will be talking about in markov models

597
00:35:24,570 --> 00:35:28,710
no actually

598
00:35:28,760 --> 00:35:32,070
well that's

599
00:35:32,090 --> 00:35:33,560
somewhat related

600
00:35:38,380 --> 00:35:43,350
i mean the hidden markov SVM is in a way a precursor to the max

601
00:35:43,350 --> 00:35:46,050
margin markov network

602
00:35:46,060 --> 00:35:51,770
and just that they use a slightly different loss function all that

603
00:35:51,780 --> 00:35:53,820
i am not going to

604
00:35:53,830 --> 00:35:57,830
get into details about that this will be if you were to use the con

605
00:35:57,830 --> 00:36:01,980
considered different loss functions and not going to do it now

606
00:36:02,000 --> 00:36:05,450
if we have some time after the lectures

607
00:36:05,540 --> 00:36:12,300
i can briefly mention i would do that in the structured SVM setting

608
00:36:12,310 --> 00:36:15,880
i think it would confuse people if it is now

609
00:36:18,780 --> 00:36:21,350
was the

610
00:36:22,400 --> 00:36:24,650
it the country

611
00:36:28,560 --> 00:36:32,520
yes this this this and it's an exponential family

612
00:36:32,570 --> 00:36:35,430
the thing is if you don't have

613
00:36:35,480 --> 00:36:38,340
the nice factorizations assumption

614
00:36:38,350 --> 00:36:41,450
then it gets very very hard to the dynamic programming

615
00:36:41,590 --> 00:36:45,960
so for instance yes i could use the solid for my family

616
00:36:45,970 --> 00:36:46,860
and what

617
00:36:46,870 --> 00:36:48,140
get nice

618
00:36:48,150 --> 00:36:50,170
experiential like expansions

619
00:36:50,180 --> 00:36:53,290
and i couldn't do that and programming efficiently

620
00:36:53,310 --> 00:36:57,290
so yes in principle you can do it you get the convex programme

621
00:36:57,330 --> 00:37:00,350
but no you cannot find it efficiently

622
00:37:00,360 --> 00:37:04,290
at least i'm not aware of this we ever get such result i mean this

623
00:37:04,290 --> 00:37:06,850
will be one famous paper

624
00:37:07,670 --> 00:37:11,650
i wouldn't know at the moment how did that result we use

625
00:37:15,490 --> 00:37:20,930
well i am not assuming conjugate priors or anything like that yes you could replace

626
00:37:20,930 --> 00:37:25,520
well what what one of the problems of course first of all we have seen

627
00:37:25,530 --> 00:37:32,930
very clearly for example as statisticians have been shouting for many many decades association is

628
00:37:32,930 --> 00:37:37,480
not causation you can not least can not very simply

629
00:37:37,490 --> 00:37:42,030
interpret associations as having causal meaning

630
00:37:42,050 --> 00:37:44,200
so what are we going to do

631
00:37:44,220 --> 00:37:46,310
so traditionally statistics

632
00:37:46,900 --> 00:37:50,880
they do what i do with causes of a face as it is a hard

633
00:37:50,880 --> 00:37:53,340
problem when i'm going to go there

634
00:37:53,360 --> 00:37:59,850
OK touch that only deal with association correlation things is not completely true there's all

635
00:37:59,860 --> 00:38:03,040
theories experimental design which is one of the

636
00:38:03,050 --> 00:38:08,170
bright stars of twentieth century science thing

637
00:38:08,650 --> 00:38:14,510
which which does address causal issues in experimental studies how to arrange your interventions in

638
00:38:14,510 --> 00:38:17,300
order you can draw reliable causal conclusions

639
00:38:17,300 --> 00:38:23,700
which is great but it does only restrict itself to interventional studies randomisation and things

640
00:38:23,700 --> 00:38:24,520
like that

641
00:38:24,620 --> 00:38:29,500
and therefore it doesn't take is very far the really important problems of understanding data

642
00:38:29,500 --> 00:38:32,600
from observational studies

643
00:38:32,750 --> 00:38:37,840
so what we're going to do to make inferences about causation

644
00:38:37,920 --> 00:38:43,520
one way of thinking about the problem is that it's it's it's a really tricky

645
00:38:43,520 --> 00:38:45,300
problem of induction

646
00:38:46,200 --> 00:38:52,410
thinking about prediction the standard setting is that you've got some data

647
00:38:52,430 --> 00:38:56,490
a new learning something about the system that produced the data

648
00:38:56,520 --> 00:39:03,690
and you're thinking about using that to say what's going to happen in further examples

649
00:39:03,690 --> 00:39:06,590
generated by the same system so

650
00:39:06,630 --> 00:39:13,290
that the problem of induction from observed data to unobserved future cases and like every

651
00:39:13,290 --> 00:39:15,380
problem of induction is nontrivial

652
00:39:16,370 --> 00:39:18,510
at least your in the same

653
00:39:18,530 --> 00:39:24,990
setting the same regime your thinking about induction to further examples of the same system

654
00:39:25,690 --> 00:39:27,390
but with causality

655
00:39:27,470 --> 00:39:31,930
causal inference from observational data you moving two different systems

656
00:39:31,940 --> 00:39:37,340
few decades of from an on manipulated

657
00:39:37,360 --> 00:39:40,670
unobserved and kicked system if you like

658
00:39:40,680 --> 00:39:44,630
but your inferences to manipulated and kicked system

659
00:39:44,640 --> 00:39:51,100
so we're moving from one regime the observation regime two different regimes interventional regime the

660
00:39:51,100 --> 00:39:56,600
data is on the observation regime the question is about intervention regime we want to

661
00:39:56,600 --> 00:40:01,140
make an induction from data to the future but it's a very bold induction which

662
00:40:01,140 --> 00:40:06,180
has to skip across from one setting to another so we got

663
00:40:06,200 --> 00:40:12,810
think about how we might do and very important how we might justify

664
00:40:14,660 --> 00:40:17,440
now here's an interesting question i think

665
00:40:17,450 --> 00:40:19,520
until lot about

666
00:40:19,520 --> 00:40:23,080
well in implicitly about

667
00:40:23,100 --> 00:40:27,570
uncertain outcomes and we don't know don't have the patience can recover or not so

668
00:40:27,570 --> 00:40:29,030
clearly some

669
00:40:29,040 --> 00:40:33,550
framework involving probability is going to be pretty important to us

670
00:40:33,580 --> 00:40:37,490
somewhere some pro was going to be the is that enough

671
00:40:37,500 --> 00:40:41,880
or do we need in order to understand these questions of causality do we need

672
00:40:41,880 --> 00:40:44,540
to develop something novel

673
00:40:44,550 --> 00:40:47,140
a whole new foundational framework

674
00:40:47,170 --> 00:40:50,430
which will allow us to make sense of these questions and answers

675
00:40:50,590 --> 00:40:53,040
do we need a new formal framework many

676
00:40:53,050 --> 00:40:57,780
workers have decided the answer to that is yes unfortunately we haven't agreed on which

677
00:40:57,780 --> 00:41:00,910
new formal framework it is that we need

678
00:41:01,550 --> 00:41:05,300
my answer to that is not really we can pretty much model for what we

679
00:41:05,300 --> 00:41:07,130
got already

680
00:41:07,200 --> 00:41:09,470
so let's think about the various

681
00:41:09,490 --> 00:41:13,330
framework that have been suggested

682
00:41:16,550 --> 00:41:23,120
so at the very foundation level you know what are the fundamental building blocks

683
00:41:23,140 --> 00:41:25,720
that's you might want to use

684
00:41:25,740 --> 00:41:32,030
well the probability distributions maybe that's enough are trying to argue that is almost enough

685
00:41:32,100 --> 00:41:37,930
and we don't really need to go much further but there's a very strong

686
00:41:37,950 --> 00:41:43,660
enterprise lot of people who think that the way to think about causality is you've

687
00:41:43,660 --> 00:41:49,780
got to think about potential responses so i'll explain shortly

688
00:41:49,800 --> 00:41:51,220
what that's all about

689
00:41:51,220 --> 00:41:54,140
but it's something new which goes beyond

690
00:41:54,240 --> 00:41:59,370
the way we describe normal probability and association

691
00:42:01,220 --> 00:42:06,300
the functional relationships this is based on bit of physics and the the idea that

692
00:42:06,580 --> 00:42:12,800
if only had enough understanding of the causes of something there will be completely deterministic

693
00:42:13,950 --> 00:42:19,720
i would suggest you understand understand the relationship between smoking and lung cancer you've got

694
00:42:19,720 --> 00:42:24,070
to add just making all the other things that might also be involved in the

695
00:42:24,070 --> 00:42:25,600
environment and

696
00:42:25,620 --> 00:42:31,100
goodness knows what and then think of the functional relationship which actually outputs yes or

697
00:42:31,100 --> 00:42:36,890
no you will always get cancer which is also very common very very very popular

698
00:42:36,890 --> 00:42:42,180
i found odd because it's almost the antithesis of the statistical approach which says we

699
00:42:42,180 --> 00:42:47,840
have uncertainty and mo and it's we have to live with that model

700
00:42:47,850 --> 00:42:51,300
we don't we're not looking for certainty

701
00:42:51,350 --> 00:42:58,300
and there's something which i call extended conditional independence which is just related to a

702
00:42:58,300 --> 00:43:02,550
slight extension the idea of a simple probability modeling which again i will explain to

703
00:43:02,550 --> 00:43:04,890
you in some detail in fact

704
00:43:04,910 --> 00:43:09,370
so that we could say the fundamental building blocks and then there are various ways

705
00:43:09,370 --> 00:43:14,620
of body like imagine how these the child's constructors that you can put them together

706
00:43:14,620 --> 00:43:20,890
in various different ways of different structures which you can build on these things so

707
00:43:20,910 --> 00:43:26,740
structural equations that built on the idea of functional relationships and this is very this

708
00:43:26,740 --> 00:43:32,050
started out order by eighty years ago i think in in genetics was alright right

709
00:43:32,050 --> 00:43:38,280
to become very popular in in econometrics and many other areas the idea of a

710
00:43:38,280 --> 00:43:40,460
it might be that fraudsters

711
00:43:40,490 --> 00:43:44,190
taking the maximum amount from the camp and then you try to

712
00:43:44,200 --> 00:43:46,820
and then the bank will bring you up and say did you make the first

713
00:43:47,370 --> 00:43:49,060
or it could be the other way around

714
00:43:49,070 --> 00:43:54,190
excessively small transactions to test whether card has been blocked

715
00:43:54,190 --> 00:43:58,340
whether they have yet detected for example the car has been stolen

716
00:43:58,410 --> 00:44:02,150
small transactions go under the threshold of detection for example

717
00:44:02,160 --> 00:44:03,630
we can

718
00:44:03,660 --> 00:44:09,190
purchase of multiple small electrical items for example toasters and what machines and hairdryers because

719
00:44:09,190 --> 00:44:13,060
these can be easily sold on in car boot sales and so on

720
00:44:13,070 --> 00:44:16,070
all of those sort of examples of things which

721
00:44:16,340 --> 00:44:20,950
mean fraud but they are they might be suspicious they should perhaps be

722
00:44:20,950 --> 00:44:24,860
mixed in with other things and looked at

723
00:44:24,860 --> 00:44:28,570
i'm going to talk about will based methods in detail

724
00:44:28,580 --> 00:44:35,520
let me focus attention on the instead on supervised classification anomaly detection methods

725
00:44:35,530 --> 00:44:39,360
the basic principle you're all familiar with this i'm sure the basic principle of supervised

726
00:44:39,360 --> 00:44:44,570
classification methods is that you have enough context a set of known fraudulent and legitimate

727
00:44:46,320 --> 00:44:50,340
or counts with the descriptive variables reach the seventy two eighty

728
00:44:50,360 --> 00:44:53,860
variables for each transaction

729
00:44:53,870 --> 00:44:55,450
and our aim is to condense

730
00:44:55,490 --> 00:45:01,650
such a data set a retrospective transactions to rule enabling enabling us to correctly classify

731
00:45:01,660 --> 00:45:06,860
new transactions just using their descriptive information just using the seventy to eighty variables

732
00:45:06,870 --> 00:45:08,940
that's what they're trying to do

733
00:45:08,960 --> 00:45:09,990
and as you know

734
00:45:09,990 --> 00:45:12,810
there are a huge number of methods

735
00:45:12,830 --> 00:45:14,480
for these kinds of

736
00:45:14,480 --> 00:45:18,910
to protecting these problems developed by various different

737
00:45:18,920 --> 00:45:20,410
intellectual communities

738
00:45:20,410 --> 00:45:24,980
which are greater statistics and i've just listed some other methods and if your favorite

739
00:45:24,980 --> 00:45:29,440
method isn't there i apologize there are too many but i got

740
00:45:29,450 --> 00:45:31,870
the most important ones i think

741
00:45:31,870 --> 00:45:35,240
but there are others there are many variants of these and someone could fill all

742
00:45:35,240 --> 00:45:39,400
the slides in this presentation the the list of these

743
00:45:39,500 --> 00:45:43,210
OK so let's look at a particular example of this is from one of the

744
00:45:43,210 --> 00:45:44,820
banks which is

745
00:45:44,840 --> 00:45:47,410
collaborating with us

746
00:45:47,490 --> 00:45:49,920
still class k

747
00:45:50,870 --> 00:45:55,650
so in this particular example

748
00:45:55,660 --> 00:45:59,100
we've got one hundred seventy five million transactions which took place between the first of

749
00:46:00,650 --> 00:46:04,840
in two thousand five to thirty to november two thousand five

750
00:46:04,870 --> 00:46:07,740
sixteen point seventy million accounts

751
00:46:07,790 --> 00:46:10,100
six thousand of these

752
00:46:10,110 --> 00:46:12,080
seventeen million

753
00:46:12,120 --> 00:46:13,210
had fraud

754
00:46:13,240 --> 00:46:16,190
critical point of sale terminals

755
00:46:16,190 --> 00:46:20,190
we had between seventy and eighty in this particular case we had seventy six more

756
00:46:22,020 --> 00:46:27,100
per transaction most of them categorical some the continuous like amount of time

757
00:46:27,110 --> 00:46:30,240
but others merchant category code for example

758
00:46:30,270 --> 00:46:33,070
nature of the point of sale terminals

759
00:46:34,450 --> 00:46:36,670
it will be categorical

760
00:46:36,690 --> 00:46:40,410
in this particular is just an example of one of the analysis we did in

761
00:46:40,410 --> 00:46:46,960
this particular example we use rolling window activity records summarise transactions over the last seven

762
00:46:46,960 --> 00:46:51,370
days lost three days last one day zero here means we just looking at the

763
00:46:51,370 --> 00:46:55,020
last transaction by itself

764
00:46:55,830 --> 00:46:59,450
you take a stake in example you take the transactions over the last seven days

765
00:46:59,450 --> 00:47:03,950
each of the transactions which occurred in the last seven days had seventy six variables

766
00:47:03,950 --> 00:47:05,040
recording it

767
00:47:05,070 --> 00:47:09,080
maybe there are five transactions they got five times seventy six variables which need to

768
00:47:09,080 --> 00:47:14,580
be summarized in various ways process to extract useful descriptive information like this sort of

769
00:47:14,580 --> 00:47:19,560
variables i described when i was talking about the

770
00:47:19,570 --> 00:47:21,950
agency pattern

771
00:47:21,980 --> 00:47:23,200
when we did that

772
00:47:23,210 --> 00:47:27,660
we process to each of these activity records describe them in terms of eighty seven

773
00:47:29,070 --> 00:47:34,770
so we got eighty seven variables describing each activity record

774
00:47:35,240 --> 00:47:36,570
this particular study

775
00:47:36,650 --> 00:47:41,280
evaluated compared various different classification methods

776
00:47:41,280 --> 00:47:43,440
ranging from logistic the action

777
00:47:43,540 --> 00:47:46,730
social support vector machines random forests

778
00:47:46,730 --> 00:47:51,240
and so on

779
00:47:51,250 --> 00:47:53,440
two investigations were carried out

780
00:47:53,440 --> 00:47:55,500
this is quite an important point

781
00:47:55,530 --> 00:47:59,580
in this sort of work you often find people use the holdout sample they

782
00:47:59,600 --> 00:48:01,440
they randomly

783
00:48:01,450 --> 00:48:03,110
split data into two

784
00:48:03,130 --> 00:48:06,320
quite properly build the model on

785
00:48:06,320 --> 00:48:09,870
one of the parts and tested on the other we all know

786
00:48:09,870 --> 00:48:12,370
but that's very sensible thing do you don't want to build and test on the

787
00:48:14,160 --> 00:48:18,670
dataset because the optimistic results the optimistic bias you get so splitting in two is

788
00:48:18,690 --> 00:48:23,860
very sensible approach especially given that we got pretty large numbers

789
00:48:25,480 --> 00:48:27,880
in this particular problem one of the

790
00:48:27,940 --> 00:48:32,070
have made this clear one of the very important features is the things change over

791
00:48:33,500 --> 00:48:37,920
i've talked about faustus changing in reaction to anything you do

792
00:48:38,320 --> 00:48:41,120
but as we particularly aware the moment

793
00:48:41,130 --> 00:48:43,110
economic conditions can change

794
00:48:43,110 --> 00:48:47,100
we could be trundling along in very nice comfortable economic situation

795
00:48:47,120 --> 00:48:51,670
the sub-prime crisis could occur in the united states which turns everything into a downward

796
00:48:53,130 --> 00:48:56,850
all sorts of changes can occur new technology can come in to change things change

797
00:48:56,940 --> 00:48:58,820
in the system

798
00:48:58,840 --> 00:49:01,950
changes occurring all the time the so

799
00:49:01,980 --> 00:49:06,460
when you take a holdout sample when you take your retrospective data randomly speak split

800
00:49:06,530 --> 00:49:12,560
into two you are assuming that the few training test you are assuming that the

801
00:49:12,570 --> 00:49:14,950
distributions constant which is unrealistic

802
00:49:14,960 --> 00:49:17,190
so there is one

803
00:49:17,200 --> 00:49:18,740
i doubt about the

804
00:49:19,030 --> 00:49:24,940
how how accurately how originally you should interpret conclusions from this sort of analysis

805
00:49:24,980 --> 00:49:27,320
the second sort of

806
00:49:27,340 --> 00:49:31,170
one of the other analysis we did with this data set was we

807
00:49:31,190 --> 00:49:34,830
to some extent try to allow for that by training on the first three months

808
00:49:34,830 --> 00:49:37,280
of data and and testing it on them

809
00:49:37,290 --> 00:49:40,160
the final months of data

810
00:49:40,170 --> 00:49:41,940
that allows for

811
00:49:41,950 --> 00:49:44,160
to some extent changing

812
00:49:44,210 --> 00:49:48,070
conditions distributions and so it allows for population drift

813
00:49:48,070 --> 00:49:50,450
but call population drift

814
00:49:50,460 --> 00:49:55,980
well that's OK it illustrates how dramatically in one particular case

815
00:49:55,980 --> 00:49:58,770
when change occurred around thirtieth of october

816
00:49:58,820 --> 00:50:03,050
how things can how the predictive performance might degrade for example but of course the

817
00:50:03,050 --> 00:50:08,070
economy could change in any things could change in any number of other ways so

818
00:50:08,190 --> 00:50:13,310
anything here also should be interpreted cautiously with a pinch of salt

819
00:50:13,310 --> 00:50:18,450
and we consider the topic of sparse learning on matrices

820
00:50:18,480 --> 00:50:25,260
so in terms of features so series promising that we could get good results in

821
00:50:25,260 --> 00:50:28,010
terms of predictive performance if log b

822
00:50:28,060 --> 00:50:31,890
is at most equal to n but what happens when n equals one thousand

823
00:50:32,330 --> 00:50:35,260
so we say the same thing that you could do is

824
00:50:35,270 --> 00:50:37,250
exponential one thousand features

825
00:50:37,260 --> 00:50:41,310
it's kind of a lot if you want to to run algorithm with that now

826
00:50:41,320 --> 00:50:46,180
we see how we can use structure within the features to design algorithms can be

827
00:50:46,180 --> 00:50:51,370
run in polynomial time in an even though you you are implicitly considering exponentially many

828
00:50:51,370 --> 00:50:55,590
features as you could be evolving

829
00:50:55,620 --> 00:51:03,120
ah norms which are grouped with groups of a both which may overlap

830
00:51:03,170 --> 00:51:07,300
there is we a considerable source but into the matrices and this includes a lot

831
00:51:07,320 --> 00:51:11,090
for applications of machine learning multitask multiclass

832
00:51:11,120 --> 00:51:16,390
images completion image denoising topic models in them and that we must continue to types

833
00:51:16,460 --> 00:51:23,780
sparsity low rank what people often called sparse PCA or dictionary learning

834
00:51:23,800 --> 00:51:27,800
so this would be the outline of the tutorial the first part on the on

835
00:51:27,800 --> 00:51:29,890
the last so with algorithms and

836
00:51:30,520 --> 00:51:31,990
theoretical results

837
00:51:32,000 --> 00:51:38,450
and one part of the state of victoria as we consider structured sparse methods and

838
00:51:38,450 --> 00:51:41,230
finally sparse methods on matrices

839
00:51:41,250 --> 00:51:44,930
so before i start to go into the details

840
00:51:44,950 --> 00:51:47,990
i think of all the things that slice is going to be pretty pretty quick

841
00:51:47,990 --> 00:51:54,960
so i one on leads to sparse solutions and one simple way to to see

842
00:51:54,960 --> 00:51:57,590
that is to consider the constrained problem

843
00:51:57,610 --> 00:52:02,450
the first first throughout this talk i will either be normalized by the l one

844
00:52:02,450 --> 00:52:07,090
norm or constrained by the one norm and this is equivalent in the sense that

845
00:52:07,090 --> 00:52:12,050
for any team weighted constraint there is a long day for which you can be

846
00:52:12,050 --> 00:52:14,240
analyzed by lambda times norm of w

847
00:52:14,260 --> 00:52:16,410
OK so we go back and forth between those two

848
00:52:16,420 --> 00:52:21,390
and of course the mapping from london nineteen is not depends on data because as

849
00:52:21,390 --> 00:52:26,990
well as equivalent between bit inputs so why does everyone on mister to sparsity is

850
00:52:26,990 --> 00:52:31,730
here we have plotted the level sets of a quadratic function

851
00:52:31,760 --> 00:52:35,830
OK so good as to minimize so if you do unconstrained minimisation it will end

852
00:52:35,860 --> 00:52:39,780
up at the centre of of the ellipses and of course be constrained to the

853
00:52:40,380 --> 00:52:42,630
to anyone but

854
00:52:42,630 --> 00:52:46,100
and then and one ball is simply is quite that so you see that depending

855
00:52:46,100 --> 00:52:49,760
on where the quadratic function is located we get

856
00:52:49,780 --> 00:52:52,220
no zeros and then we get

857
00:52:52,240 --> 00:52:54,180
the optimum is obtained when

858
00:52:54,200 --> 00:53:00,930
the red curve touches the blue square you get no sparsity basis you have gone

859
00:53:00,930 --> 00:53:05,010
out in the one ball would be attracted to the corners as these are exactly

860
00:53:05,030 --> 00:53:06,850
the parts which we call to

861
00:53:06,870 --> 00:53:08,470
because this is a simple

862
00:53:08,490 --> 00:53:11,150
german confusion why one norm

863
00:53:11,220 --> 00:53:12,920
little sparsity

864
00:53:12,980 --> 00:53:18,890
because this is applied to be considered for the next half hour so

865
00:53:18,900 --> 00:53:24,060
so here we assume a linear model in model where we excise often referred to

866
00:53:24,060 --> 00:53:25,310
as covariates

867
00:53:25,330 --> 00:53:28,680
and why i responses so we will

868
00:53:28,980 --> 00:53:32,310
leave script capital why

869
00:53:32,360 --> 00:53:36,730
unspecified so can be either numbers of discrete values

870
00:53:36,740 --> 00:53:42,080
and so the group be considered is regularized we get arise problem we have is

871
00:53:42,080 --> 00:53:44,290
sort of an unknown latent class

872
00:53:44,570 --> 00:53:45,840
the regularizer

873
00:53:45,880 --> 00:53:50,610
so here i would not include a constant term b because it just makes life

874
00:53:50,610 --> 00:53:53,370
harder is this can be easily added

875
00:53:53,400 --> 00:54:00,310
and again you can be either using capitalised formulation like that are constrained formulation

876
00:54:00,340 --> 00:54:03,600
so when you have the square loss this has been used a lot in many

877
00:54:03,760 --> 00:54:08,040
communities in signal processing this is called so-called basis pursuit

878
00:54:08,060 --> 00:54:12,710
and the exact same machine learning it is called the lessor

879
00:54:12,730 --> 00:54:19,060
so i would start first by the more review of nonsmooth convex analysis a convex

880
00:54:20,290 --> 00:54:24,550
in a sense that one of is non differentiable these great some technicalities and i

881
00:54:24,550 --> 00:54:26,760
would spend fifteen minutes trying to

882
00:54:26,760 --> 00:54:29,230
did with those in the simplest possible way

883
00:54:29,250 --> 00:54:33,990
so we go over analysis which will how can we say that we are optimal

884
00:54:33,990 --> 00:54:39,410
and how we can optimize later so there's a nice sequence of books related to

885
00:54:39,410 --> 00:54:44,950
but the easy languages and somewhat of a hard languages CC saying finish really hard

886
00:54:44,990 --> 00:54:48,220
also somewhat surprising thing into german is very hard

887
00:54:48,280 --> 00:54:53,280
and what is really easy is a french spanish spanish portuguese

888
00:54:53,430 --> 00:54:56,970
jenny translating into english is pretty easy

889
00:54:57,970 --> 00:55:01,090
so that's interesting metrics so

890
00:55:01,110 --> 00:55:04,860
you might have noticed that local we have the european project for now

891
00:55:05,130 --> 00:55:06,410
you have

892
00:55:06,510 --> 00:55:10,910
put your matrix which happens by about this so why don't we just built the

893
00:55:10,930 --> 00:55:15,700
systems for european languages and he said because did for all official EU languages

894
00:55:15,760 --> 00:55:16,720
and we

895
00:55:16,780 --> 00:55:20,630
didn't completely realized that point that means if you do over twenty three languages you

896
00:55:20,630 --> 00:55:23,360
have to build about five hundred systems

897
00:55:23,360 --> 00:55:28,240
but OK cluster computing currently building these systems

898
00:55:28,800 --> 00:55:34,010
as part of the project we have this online evaluation so we have a website

899
00:55:34,010 --> 00:55:38,320
which might be down right now because the computer science department bar moving

900
00:55:38,340 --> 00:55:43,990
and all the computers get disconnected help that's going back up sometimes

901
00:55:44,010 --> 00:55:48,050
they actually have they can actually look at least the output of all these empty

902
00:55:48,070 --> 00:55:51,380
systems for some of the language that if multiple

903
00:55:51,430 --> 00:55:56,910
so participating in producing outputs you can concurrency see with the performance

904
00:55:56,930 --> 00:56:01,090
and if you build your own MT system you can upload their tickets automatically scored

905
00:56:01,610 --> 00:56:04,490
cross against the evidence

906
00:56:09,240 --> 00:56:13,240
and also you have the test set that is also a life sentence by sentence

907
00:56:14,070 --> 00:56:16,640
the eleven languages

908
00:56:16,680 --> 00:56:19,070
such that each

909
00:56:23,130 --> 00:56:31,110
yes well well that different

910
00:56:31,130 --> 00:56:33,760
so you have the same sentence

911
00:56:33,780 --> 00:56:39,590
translated across eleven languages and you have two thousand of those sentences it's it's apparel

912
00:56:39,630 --> 00:56:45,030
across the instances of course so here i am

913
00:56:45,050 --> 00:56:48,400
also not entirely believe the blue because because

914
00:56:48,400 --> 00:56:51,740
you compare against different references

915
00:56:51,760 --> 00:56:56,880
it's part of the reason why finishes harder is finished build gigantic words

916
00:56:57,740 --> 00:57:01,930
the finnish versus usually contain things like preposition

917
00:57:01,950 --> 00:57:05,130
and also this german disease of sticking words together

918
00:57:05,140 --> 00:57:07,590
so getting rid tried finish

919
00:57:07,630 --> 00:57:09,840
it's much harder than getting it would write english

920
00:57:09,860 --> 00:57:15,010
so getting rid finishes almost as good as getting three words writing in english

921
00:57:15,030 --> 00:57:17,780
so getting high blues course finishes much much

922
00:57:17,800 --> 00:57:19,280
part of because it

923
00:57:19,300 --> 00:57:22,320
you judged on how often you get a falcon right

924
00:57:24,260 --> 00:57:29,430
it's extremely difficult question posed to answer to basically have to answer the question is

925
00:57:29,430 --> 00:57:32,840
this spanish sentence as bad as this german

926
00:57:32,860 --> 00:57:38,430
it's very hard to get references and this is hard to get a handle on

927
00:57:40,030 --> 00:57:42,090
all the core of the columns

928
00:57:42,160 --> 00:57:45,860
well they have this compare against the same output language

929
00:57:46,680 --> 00:57:51,660
does it really mean translating into german is harder than translate into english

930
00:57:51,680 --> 00:57:54,220
it might be

931
00:57:54,220 --> 00:57:58,070
but in the rose i mean you because you always compared the same output and

932
00:57:58,090 --> 00:58:03,480
verasamy certainly the revelation manually if you translate from different source languages so they're blue

933
00:58:03,480 --> 00:58:05,510
losing to correlate

934
00:58:05,610 --> 00:58:07,780
will be different

935
00:58:21,130 --> 00:58:24,740
so the big problem the germans ones word order

936
00:58:25,110 --> 00:58:26,900
so typically the

937
00:58:26,910 --> 00:58:29,860
if you don't get very bright in the sentence it has a lot of knock-off

938
00:58:29,860 --> 00:58:35,860
affect everything else we just the language more likes to have build the sentence would

939
00:58:35,860 --> 00:58:38,900
often loosen its auxiliaries left and right

940
00:58:38,950 --> 00:58:41,570
so everything falls apart if you don't have the right to right play so that's

941
00:58:41,570 --> 00:58:45,820
one of the reasons rich morphology is another reason

942
00:58:45,840 --> 00:58:48,760
so if you look at the blue course finishes the religious mythology of all these

943
00:58:48,760 --> 00:58:51,400
languages the hardest thing which translate

944
00:58:52,700 --> 00:58:54,680
so those two

945
00:58:54,720 --> 00:59:00,160
otherwise german english are very related languages in terms of vocabulary the use of of

946
00:59:00,160 --> 00:59:03,840
the first look the same the kind of expressions you can use an english can

947
00:59:03,840 --> 00:59:05,050
almost always

948
00:59:05,070 --> 00:59:08,800
what can very often use the same expression in germany

949
00:59:09,410 --> 00:59:15,130
which is not the case into chinese english and so on

950
00:59:15,140 --> 00:59:17,740
so i like germany basically

951
00:59:17,760 --> 00:59:22,760
german economist pose the challenge of getting syntax right mt

952
00:59:22,780 --> 00:59:26,470
and in morphology otherwise it's easy language

953
00:59:26,490 --> 00:59:32,130
OK so that we can make you thing it's single class of languages by how

954
00:59:32,130 --> 00:59:35,740
well it translates it just takes the blue score say which two systems as close

955
00:59:35,740 --> 00:59:39,700
going put them together so it's kind of greedy clustering the first class of languages

956
00:59:39,700 --> 00:59:43,680
together very close to many clusters c one of the clusters of and if you

957
00:59:43,680 --> 00:59:45,590
do that well i think it

958
00:59:45,660 --> 00:59:47,220
i to tweak it a a little bit

959
00:59:47,240 --> 00:59:51,900
i mean comes down to the similarity metric ninety computers but you get the nice

960
00:59:51,900 --> 00:59:53,720
treaty of language pairs so

961
00:59:53,740 --> 00:59:58,590
this you know it's just language related to the portuguese spanish french italian i'm all

962
00:59:58,590 --> 01:00:00,700
related to the other ones

963
01:00:00,700 --> 01:00:04,660
you can of get the same cluster of the romantic and dramatic languages and the

964
01:00:04,660 --> 01:00:07,970
two in greek finnish

965
01:00:07,990 --> 01:00:15,180
so here's the thing is said about what is where is kind of already talked

966
01:00:15,180 --> 01:00:16,610
about this

967
01:00:17,410 --> 01:00:21,300
you see that some languages are easier to translate into that out of fifty translate

968
01:00:21,300 --> 01:00:25,820
if you if it ever all the scores of translating from german from english but

969
01:00:25,820 --> 01:00:27,860
english is slightly easier but not much

970
01:00:27,880 --> 01:00:29,400
twenty two point two

971
01:00:29,400 --> 01:00:30,720
twenty three point eight

972
01:00:30,780 --> 01:00:34,780
but if you look at this course into german into english and german is much

973
01:00:34,780 --> 01:00:36,760
much harder ten point

974
01:00:36,820 --> 01:00:39,030
different so there some languages that

975
01:00:39,030 --> 01:00:41,100
can you hear me

976
01:00:41,110 --> 01:00:45,750
right thank you for being here thanks for bringing here and try to make this

977
01:00:45,750 --> 01:00:51,010
interesting for you so i'm going to talk about markov logic a unifying language for

978
01:00:51,010 --> 01:00:55,700
information and knowledge management this is what that done at the university of washington with

979
01:00:55,700 --> 01:00:57,650
stanley kok daniel out

980
01:00:57,660 --> 01:01:02,410
four from paul matt richardson parag singla marc sumner and jue wang

981
01:01:02,520 --> 01:01:06,950
here's is a brief outline of my talk i will begin with a little bit

982
01:01:06,950 --> 01:01:08,150
of motivation

983
01:01:08,210 --> 01:01:13,050
and then some necessary background and then i'll get to the heart of things which

984
01:01:13,050 --> 01:01:18,140
is this new language called markov logic i'll talk a little about inference and learning

985
01:01:19,480 --> 01:01:23,540
markov logic and the software open source software that are implemented in

986
01:01:23,710 --> 01:01:28,000
and then i will talk about some of the things that we can do markov

987
01:01:28,010 --> 01:01:32,190
logic and conclude with a little bit of discussion

988
01:01:32,550 --> 01:01:34,340
so here's the motivation

989
01:01:34,390 --> 01:01:38,560
and here's a cartoon you know very very oversimplified cartoon

990
01:01:38,570 --> 01:01:44,370
of the state of information and knowledge management about twenty years ago

991
01:01:44,670 --> 01:01:48,280
you can think of information lying on this axis

992
01:01:48,670 --> 01:01:53,130
that because between structure on the one side and unstructured on the other

993
01:01:53,280 --> 01:01:57,520
twenty years ago what happened was that there were these things that were from we

994
01:01:57,530 --> 01:02:01,300
very much in the structured side like databases and knowledge bases

995
01:02:01,350 --> 01:02:05,900
and the language is that people use to deal with them like sql datalog in

996
01:02:06,220 --> 01:02:08,570
first order logic and its many variants

997
01:02:08,580 --> 01:02:11,480
this was the structure into the spectral

998
01:02:11,580 --> 01:02:15,760
and then there was the unstructured of the spectrum where you just had three text

999
01:02:15,780 --> 01:02:18,690
and of course there was information retrieval the

1000
01:02:18,700 --> 01:02:21,770
three text and there was an LP that dealt with free text

1001
01:02:21,850 --> 01:02:23,960
and this is where things stood

1002
01:02:24,480 --> 01:02:26,700
now fast-forward to today

1003
01:02:26,720 --> 01:02:28,850
what happened

1004
01:02:28,860 --> 01:02:30,530
it's really amazing

1005
01:02:31,150 --> 01:02:36,510
this spectrum between structured and unstructured has become completely populated

1006
01:02:36,680 --> 01:02:43,510
the combinations of structured and unstructured degrees of structure you know have explored there's of

1007
01:02:43,510 --> 01:02:46,360
course hypertext and you know html

1008
01:02:46,410 --> 01:02:51,450
but then there's also things somewhat more structure that protects like semistructured information for example

1009
01:02:51,450 --> 01:02:57,140
XML there's deep web right which is kind of like structured masquerading as an unstructured

1010
01:02:57,280 --> 01:03:02,360
the semantic web you know things like RDF and OWL web services so that we

1011
01:03:02,360 --> 01:03:07,370
have in this things like information extraction that go from structure to structure and there's

1012
01:03:07,380 --> 01:03:12,560
even things like ubiquitous computing instances that produce all this data that is kind of

1013
01:03:12,560 --> 01:03:13,140
much easier

1014
01:03:13,530 --> 01:03:15,110
people used to

1015
01:03:15,130 --> 01:03:18,640
and on the one hand this is very exciting but all these things that we

1016
01:03:18,650 --> 01:03:22,270
can do today that we couldn't do before but on the other hand you know

1017
01:03:22,270 --> 01:03:23,700
this is a nightmare

1018
01:03:25,070 --> 01:03:29,340
sure you know for software engineer i know applications program out there in the real

1019
01:03:29,340 --> 01:03:32,820
world you know there's ever more things that you need to learn

1020
01:03:33,100 --> 01:03:35,250
even more things that you need to understand

1021
01:03:35,270 --> 01:03:38,250
and you know the question is can make this a little bit better

1022
01:03:38,430 --> 01:03:42,580
and what's happened as you know the spectrum got populated is that

1023
01:03:42,660 --> 01:03:46,830
not surprisingly people from both ends tries to

1024
01:03:46,840 --> 01:03:50,290
and they still trying to a lot of what's going on today including at this

1025
01:03:50,290 --> 01:03:52,690
conference they tried to extend

1026
01:03:52,710 --> 01:03:56,400
there's technology towards the middle and towards the other side

1027
01:03:56,410 --> 01:03:59,570
right so people you know in databases and it

1028
01:03:59,580 --> 01:04:03,120
figure that this conference is a lot of the work that appears there is really

1029
01:04:03,120 --> 01:04:07,730
trying to do with data that is less structured than the standard database like XML

1030
01:04:07,730 --> 01:04:08,880
for example

1031
01:04:08,890 --> 01:04:15,210
and so forth likewise on the unstructured side right we have information retrieval that uses

1032
01:04:15,210 --> 01:04:18,730
things like the links in the HTML of course and so forth

1033
01:04:18,770 --> 01:04:24,280
but the question is can we do better than just this plethora of approaches and

1034
01:04:24,280 --> 01:04:28,050
instead of you know can we somehow actually meet in the middle here come up

1035
01:04:28,050 --> 01:04:31,280
with the language and you know a way of doing things that actually accommodates the

1036
01:04:31,280 --> 01:04:36,050
full spectrum and makes life easier for everybody again first of all i would argue

1037
01:04:36,050 --> 01:04:38,690
that we have to try to do that

1038
01:04:38,710 --> 01:04:42,450
and second of all i'm going to talk about here is a proposal in that

1039
01:04:42,450 --> 01:04:46,800
direction markov logic and you will see that you know even though it's a fairly

1040
01:04:46,910 --> 01:04:52,540
young idea the they has some notable successes to its credit of course this is

1041
01:04:52,550 --> 01:04:55,860
much more to do with people you will get interested in

1042
01:04:55,880 --> 01:04:58,940
developing some of these things and then using them and so forth

1043
01:04:59,720 --> 01:05:03,730
and by the way i was looking at the program conference and i was happy

1044
01:05:03,730 --> 01:05:09,300
to notice that the topic of almost every tutorial is directly related to something that

1045
01:05:09,300 --> 01:05:10,320
you can see here

1046
01:05:10,340 --> 01:05:15,840
which makes me believe that this is not the time has come to try something

1047
01:05:15,840 --> 01:05:17,390
like this

1048
01:05:18,130 --> 01:05:19,110
to summarize

1049
01:05:19,230 --> 01:05:24,580
what we need is languages can both handle structured and unstructured information in a very

1050
01:05:24,580 --> 01:05:26,460
efficient combination of the

1051
01:05:26,480 --> 01:05:30,750
but now of course as before with SQL and you know TFIDF and what not

1052
01:05:30,870 --> 01:05:35,510
it's it's not enough to have an ice language we need efficient algorithms for doing

1053
01:05:35,510 --> 01:05:39,230
inference in that language including all the tasks that people have in the past than

1054
01:05:39,230 --> 01:05:45,850
with IR NLP or knowledge bases are databases and you know for going between them

1055
01:05:46,670 --> 01:05:48,790
light and do

1056
01:05:48,790 --> 01:05:54,840
the that kind of prior information hunch that we have about nature about the large-scale

1057
01:05:54,840 --> 01:05:56,150
structure of nature

1058
01:05:56,160 --> 01:05:59,650
about the fundamental invariances in nature

1059
01:05:59,660 --> 01:06:04,680
and two and that and that just looking at the symmetries themselves

1060
01:06:04,690 --> 01:06:06,260
can lead to

1061
01:06:06,270 --> 01:06:10,720
really powerful statement about the nature of physical laws

1062
01:06:10,730 --> 01:06:14,440
so this was physics this is one of the main driving forces been physics and

1063
01:06:14,440 --> 01:06:20,870
say well throughout the twentieth century versus a starting in the nineteen thirties

1064
01:06:20,880 --> 01:06:26,190
the question i'm going to address the talk is how to do something similar

1065
01:06:26,240 --> 01:06:30,730
with data so when we don't have a physical systems going in and out of

1066
01:06:30,730 --> 01:06:35,530
these operations but when we're looking at the invariances of naturally occurring data

1067
01:06:35,550 --> 01:06:41,500
so we're going to be looking at various groups in but most we're going to

1068
01:06:41,500 --> 01:06:45,160
be looking both the discrete groups and continuous groups

1069
01:06:45,170 --> 01:06:48,650
we're going to be looking at a finite and infinite groups

1070
01:06:48,670 --> 01:06:51,660
one distinction that we're going to have to make

1071
01:06:51,700 --> 01:06:54,290
his between commutative and noncommutative groups

1072
01:06:54,300 --> 01:06:59,790
so this is this is the fundamental thing if the group operation is commutative but

1073
01:06:59,840 --> 01:07:02,710
if it doesn't matter which order chain together these

1074
01:07:02,820 --> 01:07:09,420
these operations and then the group is called a billion or commutative and that was

1075
01:07:09,420 --> 01:07:13,160
considered this signifies the underlying theories so this is going to be like a dividing

1076
01:07:13,880 --> 01:07:15,130
in our discussion

1077
01:07:15,960 --> 01:07:21,230
in particular generalizing ideas from that being around to non abelian around is going to

1078
01:07:21,230 --> 01:07:22,180
be really

1079
01:07:22,190 --> 01:07:24,960
exciting and it turns out that

1080
01:07:25,050 --> 01:07:30,670
many ideas do generalize but they become sort of somehow more supple and lead to

1081
01:07:30,670 --> 01:07:36,960
more insights and otherwise you might imagine OK so that just a couple of

1082
01:07:36,970 --> 01:07:43,230
examples of groups having started from an abstract definition by the simplest series of groups

1083
01:07:43,230 --> 01:07:48,050
that one can think of are the so called cyclic groups which essentially just the

1084
01:07:48,050 --> 01:07:52,280
integers modulo n to take integer zero ten minus one

1085
01:07:52,330 --> 01:07:58,430
and then you consider addition modulo n this structure which you can think of is

1086
01:07:58,440 --> 01:08:03,120
kind of these numbers sitting around the circle and adding and then you add the

1087
01:08:03,140 --> 01:08:07,640
angles corresponding to each number but you always come back because they are in the

1088
01:08:07,640 --> 01:08:12,730
so-called this sort of thing is called the cyclic group of order n

1089
01:08:12,770 --> 01:08:16,510
the smallest group which has slightly more

1090
01:08:16,560 --> 01:08:20,510
interesting structure is called the fear group

1091
01:08:20,560 --> 01:08:24,070
it has four elements and

1092
01:08:24,080 --> 01:08:27,460
in which you can cause the one i j and k

1093
01:08:28,890 --> 01:08:30,970
obeys this multiplication table

1094
01:08:30,990 --> 01:08:34,980
so the small groups on way to just capture the group is to explicitly right

1095
01:08:34,980 --> 01:08:39,020
down the multiplication table this is what i'm doing here so if you carefully look

1096
01:08:39,020 --> 01:08:43,590
at this you can see that one is the identity element any

1097
01:08:43,610 --> 01:08:46,570
element squared gives you back one

1098
01:08:46,620 --> 01:08:51,480
and if you multiply any element by any other element then you get

1099
01:08:52,230 --> 01:08:53,600
third element

1100
01:08:53,610 --> 01:08:58,100
if you carefully look at the structure discovered that this is identical

1101
01:08:58,890 --> 01:09:03,370
taking the direct product of just two cyclic groups of order two so this is

1102
01:09:03,370 --> 01:09:06,420
like moving around on the unit square

1103
01:09:06,700 --> 01:09:14,050
the first non commutative group in and if you can kind of trying to start

1104
01:09:14,050 --> 01:09:17,190
writing groups is the quaternion group

1105
01:09:17,200 --> 01:09:23,440
so this is first eight element in a slightly more complicated multiplication tables

1106
01:09:23,480 --> 01:09:27,030
i'm not sure can you see the i mean even for me it's kind of

1107
01:09:27,040 --> 01:09:30,620
in fact our

1108
01:09:30,630 --> 01:09:33,390
and here the

1109
01:09:33,400 --> 01:09:35,580
here the essential thing is that

1110
01:09:35,590 --> 01:09:43,640
the multiplication is kind of induced by this cyclic permutation of i j and k

1111
01:09:43,680 --> 01:09:48,200
so if you want to buy a giant j i by j infinity g by

1112
01:09:48,590 --> 01:09:51,030
that i k by a chain

1113
01:09:51,040 --> 01:09:55,800
but if you multiply the other way around then you get into the class k

1114
01:09:55,810 --> 01:10:00,970
he would get minus care so this is the first example of non commutative group

1115
01:10:02,860 --> 01:10:05,110
we started talking about that because he drove

1116
01:10:05,150 --> 01:10:09,360
the symmetry group of the icosahedron is quite famous is called the alternating group

1117
01:10:09,370 --> 01:10:13,370
o five letters are not removed in the second part of this work and going

1118
01:10:13,370 --> 01:10:14,310
to say

1119
01:10:14,320 --> 01:10:16,900
a where that comes from

1120
01:10:16,910 --> 01:10:21,600
for our purposes what if are going to turn out to be more important are

1121
01:10:21,600 --> 01:10:25,930
the so-called symmetric groups these are just the groups of permutations

1122
01:10:25,990 --> 01:10:27,710
and so

1123
01:10:27,730 --> 01:10:29,260
if you take an object

1124
01:10:29,270 --> 01:10:34,180
and you see all bijections of those an object or a set of set one

1125
01:10:34,190 --> 01:10:39,320
two n into one so all possible maps one ten to one to all possible

1126
01:10:39,320 --> 01:10:40,710
possible matchings

1127
01:10:40,720 --> 01:10:44,660
there and victoria all of them included this forms a group

1128
01:10:44,670 --> 01:10:50,290
right so if you've had to consider of people regard the permutation has an operation

1129
01:10:50,290 --> 01:10:53,950
on these numbers one two in a particular way in which you exchange the numbers

1130
01:10:53,950 --> 01:10:57,730
one two and then one of these operations you can followed by another this is

1131
01:10:57,730 --> 01:11:03,040
what gives you the group structure and the resulting group is called the symmetric group

1132
01:11:03,040 --> 01:11:07,390
and denoted as n where n is the number of objects that you are using

1133
01:11:07,440 --> 01:11:11,720
so it might seem like this is the canonical thing in

1134
01:11:11,730 --> 01:11:14,080
indeed many

1135
01:11:14,100 --> 01:11:22,670
many results in from the theory of finite groups have their simplest nontrivial case

1136
01:11:22,690 --> 01:11:28,660
for the symmetric group but even this simple thing just considering all permutations of something

1137
01:11:29,200 --> 01:11:31,610
reveals a surprisingly

1138
01:11:31,620 --> 01:11:35,270
deep and intricate structure this is what we're going to have to talk about and

1139
01:11:35,270 --> 01:11:37,200
the second in the second half of the

1140
01:11:37,220 --> 01:11:38,210
talk today

1141
01:11:40,670 --> 01:11:42,380
going back to commutative groups

1142
01:11:42,400 --> 01:11:43,980
just integers

1143
01:11:44,000 --> 01:11:45,790
again with respect to addition

1144
01:11:45,870 --> 01:11:47,480
again former group

1145
01:11:47,500 --> 01:11:49,940
this is slightly confusing i must warn you that

1146
01:11:49,940 --> 01:11:51,470
that evolve

1147
01:11:51,520 --> 01:11:53,260
over time

1148
01:11:53,860 --> 01:11:57,650
we need to be able to to incorporate new information information

1149
01:11:57,710 --> 01:12:01,360
process the new information that a

1150
01:12:01,530 --> 01:12:02,950
is available

1151
01:12:02,970 --> 01:12:06,700
and we need to be able to take

1152
01:12:06,720 --> 01:12:08,860
and we have to sign

1153
01:12:08,860 --> 01:12:16,420
in the cluster structure or in the data the is phenomenon that is

1154
01:12:16,480 --> 01:12:19,470
i'm going to present the knowledge

1155
01:12:19,480 --> 01:12:21,380
for the last three

1156
01:12:23,570 --> 01:12:26,340
if you that working method

1157
01:12:27,070 --> 01:12:31,140
there are a lot of of this form

1158
01:12:31,150 --> 01:12:32,850
last three

1159
01:12:32,900 --> 01:12:34,990
the lot examples

1160
01:12:35,010 --> 01:12:36,640
what i'm talking about

1161
01:12:36,710 --> 01:12:38,880
it's about clustering

1162
01:12:40,610 --> 01:12:41,830
the call

1163
01:12:41,850 --> 01:12:45,480
this is important because

1164
01:12:45,510 --> 01:12:47,420
in the

1165
01:12:47,460 --> 01:12:49,720
standard approach

1166
01:12:49,780 --> 01:12:53,830
i cluster variables are clustering examples

1167
01:12:55,870 --> 01:12:57,360
we only apply

1168
01:12:57,370 --> 01:13:00,960
the trust also played related to do what metrics

1169
01:13:01,230 --> 01:13:05,630
and i have an algorithm for clustering examples

1170
01:13:05,690 --> 01:13:07,080
if i want to

1171
01:13:07,090 --> 01:13:08,580
cluster variables

1172
01:13:08,590 --> 01:13:11,090
i apply a threshold to play

1173
01:13:11,140 --> 01:13:13,350
and that's the applied field

1174
01:13:13,390 --> 01:13:22,680
but now consider that this problem in data streams that

1175
01:13:22,790 --> 01:13:25,180
the trust also played

1176
01:13:25,240 --> 01:13:28,080
is the blocking the plate what does it mean

1177
01:13:28,080 --> 01:13:32,640
but it only on the first part put them

1178
01:13:32,700 --> 01:13:35,020
after processing all

1179
01:13:35,020 --> 01:13:37,320
the input that

1180
01:13:37,840 --> 01:13:39,790
and in the stream not

1181
01:13:39,810 --> 01:13:43,900
this is not possible because

1182
01:13:44,940 --> 01:13:47,200
the size of the stream is

1183
01:13:49,350 --> 01:13:52,110
in clustering clusters

1184
01:13:52,210 --> 01:13:55,950
variables in streaming set

1185
01:13:57,320 --> 01:13:58,860
new all

1186
01:13:58,920 --> 01:14:01,010
request of britain's

1187
01:14:01,850 --> 01:14:03,640
that it that

1188
01:14:03,700 --> 01:14:05,880
if i can not use

1189
01:14:05,930 --> 01:14:10,640
the threshold but

1190
01:14:12,350 --> 01:14:14,050
two thousand six

1191
01:14:14,100 --> 01:14:19,140
the reason that i was in the knowledge to collapse

1192
01:14:21,520 --> 01:14:24,400
variables in streaming

1193
01:14:24,420 --> 01:14:25,810
this is

1194
01:14:25,810 --> 01:14:27,730
incremental clustering

1195
01:14:27,790 --> 01:14:31,450
use of the simulate

1196
01:14:31,480 --> 01:14:34,440
CNN called collapsed

1197
01:14:34,600 --> 01:14:40,450
the sole reason at the end of the cold structure

1198
01:14:40,470 --> 01:14:43,620
the leaves of structured course form

1199
01:14:44,520 --> 01:14:45,900
cool hand

1200
01:14:53,920 --> 01:14:56,740
the all

1201
01:14:56,800 --> 01:14:58,860
can detect texts

1202
01:14:58,880 --> 01:15:00,590
and last structure

1203
01:15:00,590 --> 01:15:02,220
and the take

1204
01:15:02,610 --> 01:15:06,580
and the at all saints in the data that is needed

1205
01:15:08,810 --> 01:15:10,310
the basic idea

1206
01:15:14,140 --> 01:15:15,470
the evolution

1207
01:15:15,480 --> 01:15:22,260
of the diameter of clusters diameter the diameter of the cluster is the

1208
01:15:22,260 --> 01:15:25,310
the maximum distance between the

1209
01:15:25,360 --> 01:15:26,720
two variables

1210
01:15:28,920 --> 01:15:33,400
the data are used to collect

1211
01:15:34,520 --> 01:15:38,380
is that expand the structure

1212
01:15:38,390 --> 01:15:41,130
is applies when

1213
01:15:41,180 --> 01:15:43,620
we have continuous side

1214
01:15:43,670 --> 01:15:44,870
new thing

1215
01:15:44,910 --> 01:15:46,130
so we have

1216
01:15:46,190 --> 01:15:49,480
much more and detailed information

1217
01:15:51,730 --> 01:15:53,620
used because we have

1218
01:15:53,620 --> 01:15:56,400
the more detailed information

1219
01:15:56,420 --> 01:15:58,560
we can define

1220
01:15:58,610 --> 01:16:00,120
a given cluster

1221
01:16:00,190 --> 01:16:04,080
and the fact is a class is

1222
01:16:05,180 --> 01:16:07,840
in two two new clubs

1223
01:16:07,900 --> 01:16:11,110
so this is the first

1224
01:16:11,120 --> 01:16:12,980
the second of data

1225
01:16:16,100 --> 01:16:18,490
it applies

1226
01:16:18,520 --> 01:16:20,270
when f

1227
01:16:20,360 --> 01:16:21,620
we did that

1228
01:16:21,720 --> 01:16:25,200
the the data that we adults have now

1229
01:16:28,920 --> 01:16:31,450
does not respond to to what

1230
01:16:31,490 --> 01:16:33,690
has been accepted by

1231
01:16:33,740 --> 01:16:36,610
this means that something is changing

1232
01:16:36,620 --> 01:16:37,910
in the

1233
01:16:37,920 --> 01:16:39,130
in can

1234
01:16:39,130 --> 01:16:47,870
the of the hypothesis class which has t functions combined by by this quantity

1235
01:16:47,890 --> 01:16:51,980
then you're results which are more type it's just the simplest case

1236
01:16:54,390 --> 01:16:57,580
so let's try to put things together so

1237
01:16:57,630 --> 01:17:02,900
adaboost needs at most two log and overcome iterations

1238
01:17:02,930 --> 01:17:07,800
when we assume a certain property of the baseline OK so if the base learner

1239
01:17:07,800 --> 01:17:13,240
achieves an error rate of at least half minus gamma then the

1240
01:17:13,260 --> 01:17:17,940
number of iterations to achieve your training error is too low and overcome squared is

1241
01:17:17,940 --> 01:17:19,270
what we had yesterday

1242
01:17:19,280 --> 01:17:23,490
so let d be the VC dimension of the basic policies class

1243
01:17:23,570 --> 01:17:27,760
the VC dimension of the class of functions

1244
01:17:27,830 --> 01:17:32,140
which are the most can choose from after t iterations is just this one here

1245
01:17:32,140 --> 01:17:34,910
so it's this is

1246
01:17:34,960 --> 01:17:38,540
logan becomes clear overcome clear this is the number of iterations

1247
01:17:38,550 --> 01:17:41,410
so it's the times t

1248
01:17:41,490 --> 01:17:43,690
times lock

1249
01:17:43,710 --> 01:17:45,230
t right so and

1250
01:17:45,660 --> 01:17:50,850
can just write this out so in terms of and so this is essentially dcterms

1251
01:17:50,850 --> 01:17:52,790
log n time slots there

1252
01:17:52,840 --> 01:17:54,730
OK so this is an example

1253
01:17:54,800 --> 01:17:58,150
for instance when you have if you see the image of the basic classes class

1254
01:17:58,150 --> 01:17:59,690
which is two

1255
01:18:00,750 --> 01:18:05,870
then you can consider for instance the number of this this plot here the number

1256
01:18:05,870 --> 01:18:11,680
of examples go increases year and the CBC dimension of the combined forces class and

1257
01:18:11,680 --> 01:18:15,280
then this is this function so this is essentially

1258
01:18:15,290 --> 01:18:16,520
the function here

1259
01:18:16,610 --> 01:18:19,300
and as you see the number of any you compare the VC dimension with the

1260
01:18:19,300 --> 01:18:24,620
number of examples this is going to be discrepancy and the bound you've seen this

1261
01:18:24,630 --> 01:18:29,230
quotient but i mean this is fraction of the VC dimension and the number of

1262
01:18:29,230 --> 01:18:30,700
examples so

1263
01:18:30,760 --> 01:18:36,650
therefore when you have more examples it gets better

1264
01:18:39,640 --> 01:18:46,570
now everything together so adaboost generates consistent hypothesis after a small number of iterations so

1265
01:18:46,570 --> 01:18:50,000
then the training of the characteristic

1266
01:18:50,050 --> 01:18:54,830
so the VC dimension of consistent combined forces can be found by

1267
01:18:56,010 --> 01:19:00,230
OK because the because of the fast convergence

1268
01:19:00,310 --> 01:19:04,020
so from the BBC bound to the type probability

1269
01:19:04,070 --> 01:19:07,710
so the one minus that holds that the expected around

1270
01:19:07,830 --> 01:19:10,890
smaller than the empirical error which is zero

1271
01:19:10,900 --> 01:19:14,220
plus some terms which depends on the sea dimension

1272
01:19:14,230 --> 01:19:16,380
and this term

1273
01:19:16,400 --> 01:19:20,370
it's just this one here and if you know just let go in

1274
01:19:20,680 --> 01:19:25,040
two if increase and then this whole term goes to zero

1275
01:19:26,180 --> 01:19:27,990
so therefore the expected

1276
01:19:28,130 --> 01:19:29,650
goes to zero

1277
01:19:29,690 --> 01:19:31,160
so that

1278
01:19:31,170 --> 01:19:36,010
i mean the transition there is essentially zero which is the training error class something

1279
01:19:36,010 --> 01:19:38,760
which depends on

1280
01:19:38,860 --> 01:19:43,390
and if you choose n large enough then this this respond becomes small

1281
01:19:43,400 --> 01:19:51,170
OK so the properties of the big learner imply exponential convergence

1282
01:19:51,180 --> 01:19:56,690
the past exponentially fast convergence ensures a small VC dimension of the combined forces class

1283
01:19:56,700 --> 01:20:02,010
therefore we have a small deviation from the empirical risk

1284
01:20:02,020 --> 01:20:04,600
and therefore we can choose

1285
01:20:04,610 --> 01:20:06,810
any epsilon and delta

1286
01:20:06,850 --> 01:20:11,090
that for any epsilon delta there exist some n

1287
01:20:11,110 --> 01:20:13,100
such that the

1288
01:20:13,110 --> 01:20:16,850
the error smaller than epsilon with high probability

1289
01:20:16,860 --> 01:20:22,650
therefore any weak learner which has this property which implies that this first can be

1290
01:20:22,650 --> 01:20:27,250
boosted to achieve arbitrarily high energy accuracy

1291
01:20:27,270 --> 01:20:29,630
OK so therefore we can have shown

1292
01:20:29,630 --> 01:20:34,570
that week learning is essentially equivalent to strongly

1293
01:20:34,580 --> 01:20:37,770
so by using this posting

1294
01:20:37,850 --> 01:20:42,250
on the other hand it just means that we can learning is not much less

1295
01:20:42,630 --> 01:20:46,680
than ten strong learning so the assumption that we can was already quite strong

1296
01:20:46,700 --> 01:20:50,790
so to assume that the weak learners

1297
01:20:50,810 --> 01:20:55,790
always achieves an error which is better than random guessing it's quite a strong assumption

1298
01:20:55,820 --> 01:20:58,070
for any training

1299
01:20:58,090 --> 01:21:09,800
other questions were that this is first part of the transition

1300
01:21:10,010 --> 01:21:14,370
OK so

1301
01:21:14,410 --> 01:21:20,940
what happened was the experiments and observed something strange

1302
01:21:20,960 --> 01:21:26,130
so this is an experiment results so this is the number of iterations you

1303
01:21:26,130 --> 01:21:31,630
and this is the training error this is the expected error so the the test

1304
01:21:32,200 --> 01:21:37,070
and what they found after a few iterations of this on some letter dataset using

1305
01:21:37,070 --> 01:21:39,830
boosting with c four point five

1306
01:21:39,840 --> 01:21:45,240
and what i observed was well the training error was already zero after five iterations

1307
01:21:45,240 --> 01:21:47,150
or something

1308
01:21:47,210 --> 01:21:48,600
and test runs

1309
01:21:48,600 --> 01:21:51,770
let me go back to this example of having

1310
01:21:53,830 --> 01:21:59,680
gulshan clouds which can be represented as as having the potential of this form

1311
01:21:59,700 --> 01:22:01,040
in this case

1312
01:22:01,680 --> 01:22:04,450
what will happen is that the

1313
01:22:05,270 --> 01:22:07,060
that is the density

1314
01:22:09,240 --> 01:22:16,660
you can see there is a

1315
01:22:16,700 --> 01:22:18,160
can use this

1316
01:22:18,220 --> 01:22:27,270
so specifically let's let's take the case of two clusters i'm going to draw one-dimensional

1317
01:22:27,270 --> 01:22:30,700
example but it can be in seven dimensions

1318
01:22:30,720 --> 01:22:32,080
so this is my

1319
01:22:32,100 --> 01:22:33,640
a potential u

1320
01:22:33,640 --> 01:22:36,750
and they have two clusters which means that

1321
01:22:37,510 --> 01:22:42,140
steady state density look something like this there are many many points here many many

1322
01:22:42,140 --> 01:22:45,390
points here only very few points here

1323
01:22:46,970 --> 01:22:49,790
so this is the first you can function

1324
01:22:49,810 --> 01:22:51,040
of the forward

1325
01:22:53,790 --> 01:22:54,890
the first

1326
01:22:54,890 --> 01:22:59,120
when the function of the backward operator call this

1327
01:22:59,220 --> 01:23:01,850
she left in this one few right

1328
01:23:01,900 --> 01:23:04,600
the first thing in front of the backward operator

1329
01:23:04,640 --> 01:23:09,540
means take this about minus this part and divided by this part was this part

1330
01:23:09,580 --> 01:23:14,330
so if you do that mentally you get exactly something like

1331
01:23:14,350 --> 01:23:15,720
so this is

1332
01:23:15,770 --> 01:23:16,870
different there

1333
01:23:16,890 --> 01:23:19,620
balls that indeed all explanation why

1334
01:23:19,640 --> 01:23:21,290
we have to clusters

1335
01:23:21,290 --> 01:23:26,020
the first thing function it depends on the direction between the clusters are not

1336
01:23:27,470 --> 01:23:29,560
each cluster itself

1337
01:23:30,180 --> 01:23:32,020
this is the thing that you can prove

1338
01:23:32,080 --> 01:23:34,140
because you had this formula

1339
01:23:34,200 --> 01:23:38,770
so for example if if you had equal amount of points in each cluster fifty

1340
01:23:38,770 --> 01:23:40,290
fifty percent

1341
01:23:40,310 --> 01:23:42,740
you can actually show that if you look

1342
01:23:42,790 --> 01:23:48,100
unsupervised and supervised few unsupervised clustering official according to the sign

1343
01:23:48,100 --> 01:23:50,330
opps i one

1344
01:23:50,370 --> 01:23:53,870
as n goes to infinity and obtain a goes to zero europe you approach the

1345
01:23:54,700 --> 01:23:57,310
the optimal bayes error

1346
01:23:57,310 --> 01:23:59,890
so this is quite remarkable

1347
01:23:59,910 --> 01:24:02,330
and this this analysis

1348
01:24:02,330 --> 01:24:05,640
true regardless of the dimension way

1349
01:24:07,330 --> 01:24:08,470
so here's

1350
01:24:08,470 --> 01:24:10,250
an example

1351
01:24:10,270 --> 01:24:12,890
i had my two clusters here's my backward

1352
01:24:12,950 --> 01:24:16,600
first they can function which again depends only on the x direction and not in

1353
01:24:16,600 --> 01:24:18,020
the y direction

1354
01:24:19,200 --> 01:24:22,680
indeed in a way for this

1355
01:24:24,050 --> 01:24:28,140
i only need one coordinate to describe the data

1356
01:24:28,160 --> 01:24:30,470
or the long-term evolution of the data

1357
01:24:30,490 --> 01:24:34,330
so in the one i applaud the first let's say three functions

1358
01:24:34,330 --> 01:24:35,520
with the tool

1359
01:24:35,540 --> 01:24:40,290
next ones are simply functions of the first one they don't add additional information

1360
01:24:41,270 --> 01:24:47,010
the first one already captures the important long-term evolution of this system

1361
01:24:52,430 --> 01:24:55,200
now i will present a little bit

1362
01:24:55,270 --> 01:24:57,410
of an application that interested there

1363
01:24:57,430 --> 01:25:00,060
me mean and another people group material

1364
01:25:00,870 --> 01:25:01,850
can we use

1365
01:25:01,910 --> 01:25:06,180
this method to analyse stochastic dynamical systems

1366
01:25:06,200 --> 01:25:10,770
this is a very huge field and very important one

1367
01:25:10,790 --> 01:25:14,600
and so once again the basic framework is that you have

1368
01:25:14,660 --> 01:25:19,120
some kind of the dynamical system which is driven by a force field

1369
01:25:19,140 --> 01:25:21,020
plus noise

1370
01:25:22,510 --> 01:25:23,620
you want to study

1371
01:25:23,640 --> 01:25:26,580
how it evolves and what happens with it

1372
01:25:26,600 --> 01:25:30,390
so in principle the time evolution is the forward ones

1373
01:25:30,450 --> 01:25:34,200
is described by a forward fokker planck equation

1374
01:25:34,220 --> 01:25:37,180
this is the steady state we needed to see this

1375
01:25:37,220 --> 01:25:38,890
both factor

1376
01:25:39,950 --> 01:25:43,560
obviously if you want to understand how you approach is that the state units somehow

1377
01:25:43,560 --> 01:25:44,700
to compute

1378
01:25:44,770 --> 01:25:45,950
the next

1379
01:25:46,060 --> 01:25:49,540
you can vectors are you function of this operator

1380
01:25:51,330 --> 01:25:54,490
the main challenge here is that if you take any system which is more than

1381
01:25:54,490 --> 01:25:56,330
three or four dimensional

1382
01:25:57,560 --> 01:26:01,520
yes you you know that these are again function of this operator

1383
01:26:01,540 --> 01:26:04,740
but it's defined in high dimensional space and you can not

1384
01:26:04,750 --> 01:26:06,930
compute functions

1385
01:26:06,970 --> 01:26:10,680
directly because that's the cause of this out if you do

1386
01:26:11,010 --> 01:26:13,330
if you a great

1387
01:26:13,330 --> 01:26:14,580
on your

1388
01:26:14,600 --> 01:26:18,700
set the number of grid points is exponential with dimension

1389
01:26:18,770 --> 01:26:21,250
however you can easily simulate

1390
01:26:21,330 --> 01:26:22,700
random walk

1391
01:26:22,700 --> 01:26:24,470
according to that equation

1392
01:26:24,490 --> 01:26:25,390
and you can

1393
01:26:25,390 --> 01:26:29,850
generate millions and millions of data points on then that only then you need to

1394
01:26:37,220 --> 01:26:38,540
i want to apply the

1395
01:26:38,620 --> 01:26:41,560
diffusion maps but

1396
01:26:41,600 --> 01:26:43,520
there's one problem is that

1397
01:26:43,520 --> 01:26:47,100
indeed if you can imagine there was a lot of them had a factor of

1398
01:26:49,080 --> 01:26:51,390
so they can function of this

1399
01:26:51,390 --> 01:26:52,660
i need

1400
01:26:52,680 --> 01:26:55,540
so how to get rid of this factor of two this is a marathon of

1401
01:26:55,540 --> 01:26:57,200
technicality anyway

1402
01:26:57,220 --> 01:26:58,470
if you

1403
01:26:58,490 --> 01:27:02,450
but your matrix based on a slightly different kernels

1404
01:27:02,470 --> 01:27:06,510
you can show that as n goes to infinite dimension goes to zero

1405
01:27:06,520 --> 01:27:11,220
the converse to this without the additional factor of two so exactly capture

1406
01:27:11,250 --> 01:27:16,470
begin functions of this dynamical system

1407
01:27:21,620 --> 01:27:25,310
how much time they have

1408
01:27:25,350 --> 01:27:26,850
fifteen minutes

1409
01:27:26,870 --> 01:27:28,310
of it

1410
01:27:28,890 --> 01:27:31,560
so first i want to show

1411
01:27:31,560 --> 01:27:34,040
one two analytical examples

1412
01:27:34,140 --> 01:27:39,020
where i can compute things analytically without a

1413
01:27:42,330 --> 01:27:45,890
so the simplest one is is to take the case where

1414
01:27:45,910 --> 01:27:48,410
i have the potential which is the

1415
01:27:48,490 --> 01:27:50,370
harmonic or or

1416
01:27:51,970 --> 01:27:55,370
so this is me this means that they has essentially only one well

1417
01:27:55,390 --> 01:27:58,850
so in this case i can even compute the

1418
01:28:00,010 --> 01:28:04,240
what happens when n goes to infinity but with finite epsilon

1419
01:28:04,250 --> 01:28:07,890
if you do all the math what you get is that

1420
01:28:07,930 --> 01:28:12,890
they can values go like this so that was the parameter of the potential and

1421
01:28:12,890 --> 01:28:16,540
epsilon was my with so you have something which is smaller than one of our

1422
01:28:16,540 --> 01:28:17,720
of k

1423
01:28:18,620 --> 01:28:21,390
you can functions are

1424
01:28:21,390 --> 01:28:24,220
some polynomials of degree k times the

1425
01:28:24,220 --> 01:28:25,850
this gaussians

1426
01:28:26,830 --> 01:28:29,060
if they get on zero

1427
01:28:29,160 --> 01:28:31,270
what you get is that these

1428
01:28:31,330 --> 01:28:33,120
converge to k

1429
01:28:33,120 --> 01:28:34,290
and these

1430
01:28:34,310 --> 01:28:37,470
polynomials converge to the helmet polynomials

1431
01:28:39,640 --> 01:28:41,080
this case has been

1432
01:28:41,100 --> 01:28:43,700
extensively studied music

1433
01:28:43,720 --> 01:28:48,040
the studied physics immediately organizing and functions of something

1434
01:28:52,620 --> 01:28:55,510
suppose now i do something in two dimensions

1435
01:28:55,510 --> 01:28:57,020
so i have

1436
01:28:57,040 --> 01:28:59,640
how many potential to dimensions

1437
01:28:59,680 --> 01:29:04,490
x one square the place that one x the school of applied to

1438
01:29:04,520 --> 01:29:08,180
and then consider the case where tau one is much larger than tau two so

1439
01:29:08,180 --> 01:29:09,910
somehow in one

1440
01:29:09,930 --> 01:29:14,620
direction you're doing a slow motion and you know when you're doing it fast motion

1441
01:29:14,740 --> 01:29:16,560
what happens then

1442
01:29:16,580 --> 01:29:20,040
your potential can be written as

1443
01:29:20,040 --> 01:29:23,930
as follows and also the kernel can be written as follows

1444
01:29:24,020 --> 01:29:26,740
and therefore you have this kind of a

1445
01:29:26,750 --> 01:29:32,810
outer product made other product structure fully in again functions

1446
01:29:32,810 --> 01:29:33,670
tells me

1447
01:29:33,680 --> 01:29:35,450
so this doesn't have to be

1448
01:29:35,530 --> 01:29:39,780
probability but it's a compatibility function that tells me

1449
01:29:41,950 --> 01:29:43,690
how likely

1450
01:29:43,700 --> 01:29:44,730
these two

1451
01:29:44,750 --> 01:29:46,750
values are too

1452
01:29:46,760 --> 01:29:52,190
occur together so it's in this case i said it's more likely that they both

1453
01:29:53,730 --> 01:29:55,680
and it's even more likely

1454
01:29:55,870 --> 01:29:58,650
they're both not famous

1455
01:29:58,670 --> 01:30:01,090
and the way this

1456
01:30:01,790 --> 01:30:03,690
it defines the semantics

1457
01:30:03,760 --> 01:30:06,590
is again i'm just going to

1458
01:30:06,640 --> 01:30:09,830
for a particular instantiation

1459
01:30:15,640 --> 01:30:20,630
and then i need to normalize them to sum to one

1460
01:30:20,640 --> 01:30:22,150
and so

1461
01:30:23,960 --> 01:30:25,310
key advantage of

1462
01:30:25,320 --> 01:30:28,710
these approaches is you don't have to worry about it

1463
01:30:28,720 --> 01:30:30,680
basically city

1464
01:30:30,690 --> 01:30:36,280
the bad news is the fact that you need to deal with computing this

1465
01:30:36,300 --> 01:30:40,590
normalisation term

1466
01:30:41,770 --> 01:30:46,040
i think the reason that the markov networks are

1467
01:30:46,100 --> 01:30:49,660
can mean

1468
01:30:49,700 --> 01:30:54,550
particularly popular these days is a lot of the kinds of things we want to

1469
01:30:54,550 --> 01:30:55,540
model now

1470
01:30:55,590 --> 01:31:00,000
big social networks we don't have a good causal understanding point

1471
01:31:00,080 --> 01:31:04,340
so it makes sense to just model this kind of

1472
01:31:04,370 --> 01:31:09,640
mutual influence but not necessarily model it in a directed way we don't know who's

1473
01:31:09,640 --> 01:31:13,310
influencing who we just know and there's an influence

1474
01:31:16,180 --> 01:31:18,540
frame based directed

1475
01:31:18,550 --> 01:31:20,600
frame based undirected models

1476
01:31:23,300 --> 01:31:25,050
kind of

1477
01:31:25,050 --> 01:31:26,840
capturing these

1478
01:31:26,850 --> 01:31:30,200
symmetric non causal interactions

1479
01:31:31,420 --> 01:31:33,520
social nets i mentioned

1480
01:31:33,650 --> 01:31:35,690
and web pages

1481
01:31:35,750 --> 01:31:37,920
the categories of the link

1482
01:31:37,970 --> 01:31:40,140
pages are correlated

1483
01:31:40,150 --> 01:31:45,940
and you can have patterns there

1484
01:31:45,960 --> 01:31:47,670
involve multiple

1485
01:31:47,680 --> 01:31:49,310
sets of entities

1486
01:31:49,320 --> 01:31:51,300
and so

1487
01:31:51,320 --> 01:31:54,420
the way this works is a lot like

1488
01:31:54,440 --> 01:31:56,800
we have the schema before

1489
01:31:56,890 --> 01:32:00,280
and we have the relations

1490
01:32:00,300 --> 01:32:01,480
and now

1491
01:32:01,510 --> 01:32:03,570
we have

1492
01:32:04,590 --> 01:32:06,050
edge potentials

1493
01:32:12,800 --> 01:32:14,510
that are potentially

1494
01:32:14,520 --> 01:32:16,700
at the same venue

1495
01:32:16,720 --> 01:32:21,120
for example

1496
01:32:22,410 --> 01:32:25,450
just like with the directed case

1497
01:32:25,510 --> 01:32:31,850
an instantiated RMN defined semantics in terms of the ground

1498
01:32:31,910 --> 01:32:33,400
markov network

1499
01:32:33,410 --> 01:32:38,370
the variables are the attributes of all objects and the dependencies are determined by the

1500
01:32:41,410 --> 01:32:45,000
template structure

1501
01:32:45,050 --> 01:32:46,980
so for learning

1502
01:32:52,660 --> 01:32:54,970
our approach is

1503
01:32:55,000 --> 01:32:58,260
these are mainly the work of ben taskar

1504
01:32:58,300 --> 01:33:03,570
and he and his thesis for discriminative training there

1505
01:33:04,870 --> 01:33:11,140
max margin markov networks and special case associative markov networks and i'm going to go

1506
01:33:11,140 --> 01:33:14,790
into detail about them here but

1507
01:33:14,800 --> 01:33:15,710
i will show

1508
01:33:15,730 --> 01:33:17,770
kind of cartoon of them

1509
01:33:17,780 --> 01:33:19,620
and then

1510
01:33:19,640 --> 01:33:23,180
one of the kind of areas for wine is

1511
01:33:23,220 --> 01:33:29,230
is for collective classification where you doing collective classification is big

1512
01:33:29,280 --> 01:33:31,170
unstructured are

1513
01:33:31,180 --> 01:33:32,840
linked network

1514
01:33:33,350 --> 01:33:35,180
and they have shown to be

1515
01:33:35,240 --> 01:33:38,780
very effective

1516
01:33:39,570 --> 01:33:41,600
the idea is you have

1517
01:33:41,660 --> 01:33:46,950
training data from this you're going to learn the markov network you have new data

1518
01:33:46,950 --> 01:33:49,140
and the need to approximate inference

1519
01:33:49,190 --> 01:33:56,040
to get out the conclusion

1520
01:34:00,040 --> 01:34:02,420
this can be seen as

1521
01:34:02,470 --> 01:34:04,920
you're doing that estimation

1522
01:34:04,940 --> 01:34:07,340
where you're maximizing

1523
01:34:09,680 --> 01:34:11,470
to learn the parameters

1524
01:34:11,490 --> 01:34:13,630
and then for the classification

1525
01:34:13,640 --> 01:34:14,840
then you're doing

1526
01:34:14,850 --> 01:34:17,320
the actual argmax two

1527
01:34:17,370 --> 01:34:21,420
predict the labels

1528
01:34:22,980 --> 01:34:27,520
building on the approaches from

1529
01:34:27,580 --> 01:34:29,430
lafferty are you can

1530
01:34:29,430 --> 01:34:30,910
here the model

1531
01:34:30,930 --> 01:34:33,120
a predictive model of the system

1532
01:34:33,140 --> 01:34:36,270
that turns out to be

1533
01:34:37,460 --> 01:34:40,600
common for like models or or or equivalently a better way of saying it really

1534
01:34:40,600 --> 01:34:45,020
more accurate saying it would be a linear dynamical systems so what's a linear dynamical

1535
01:34:45,020 --> 01:34:46,790
system i should

1536
01:34:46,810 --> 01:34:50,750
i have a slight slide so linear dynamical system is just a system in which

1537
01:34:50,750 --> 01:34:53,020
the underlying underlying hidden state

1538
01:34:53,100 --> 01:34:55,450
presented by some vector x

1539
01:34:55,480 --> 01:35:00,830
and t plus one is the next time step state is some matrix a times

1540
01:35:00,830 --> 01:35:03,250
x t plus an noise

1541
01:35:03,450 --> 01:35:06,580
a noise vector

1542
01:35:06,640 --> 01:35:09,330
and you don't get to see the axis is that you get to see why

1543
01:35:09,330 --> 01:35:14,870
so y t is equal to some some matrix b times xt plus some other

1544
01:35:16,730 --> 01:35:26,750
it's a major modifications some matrix a times

1545
01:35:26,770 --> 01:35:31,810
now well the comfort part comes in because you don't get to see the state

1546
01:35:32,060 --> 01:35:35,160
and you get to see a sequence of observations and you're trying to estimate estimate

1547
01:35:35,160 --> 01:35:38,750
stated that that is what call but

1548
01:35:39,370 --> 01:35:43,430
about linear dynamical system that is a linear dynamical system so the point is counted

1549
01:35:43,430 --> 01:35:49,180
for very very widely used in engineering to very powerful process systems but very rare

1550
01:35:49,410 --> 01:35:54,450
so common filtering is the process for you given the dynamical system and you're trying

1551
01:35:54,450 --> 01:35:58,660
to estimate the state of the observations we try to learn

1552
01:35:58,710 --> 01:36:00,160
the model

1553
01:36:00,290 --> 01:36:03,120
dynamic model for

1554
01:36:03,160 --> 01:36:07,140
and here is something that i think we should get rid of somehow you know

1555
01:36:07,830 --> 01:36:09,160
so popular

1556
01:36:09,180 --> 01:36:10,210
so you agree

1557
01:36:10,230 --> 01:36:12,620
you research looked at least some people agree

1558
01:36:16,530 --> 01:36:20,350
although the case

1559
01:36:20,980 --> 01:36:24,390
this is the case where winnow no

1560
01:36:24,410 --> 01:36:26,120
so whole

1561
01:36:26,140 --> 01:36:31,230
but if you put in the matrix i think would so

1562
01:36:31,250 --> 01:36:34,790
and i

1563
01:36:34,790 --> 01:36:38,330
not really

1564
01:36:38,730 --> 01:36:40,910
we make something more difficult but

1565
01:36:40,930 --> 01:36:43,230
the but it's actually doing is equivalent

1566
01:36:43,250 --> 01:36:44,870
it is not

1567
01:36:44,960 --> 01:36:47,620
so you have a lot of problems

1568
01:36:47,640 --> 01:36:49,120
i would assume

1569
01:36:49,140 --> 01:36:50,500
that's my

1570
01:36:50,520 --> 01:36:55,080
future observations of

1571
01:36:57,250 --> 01:36:59,520
these operations conditional

1572
01:37:01,140 --> 01:37:02,520
furthermore more

1573
01:37:02,520 --> 01:37:04,620
the right of the system the

1574
01:37:04,640 --> 01:37:06,430
this is an

1575
01:37:08,330 --> 01:37:10,040
the by state

1576
01:37:10,060 --> 01:37:15,770
so that was the late so why you

1577
01:37:15,790 --> 01:37:25,640
what people call he was that is it is that have and the next generation

1578
01:37:25,660 --> 01:37:28,500
one the key the history

1579
01:37:29,100 --> 01:37:30,730
the only is

1580
01:37:30,730 --> 01:37:34,460
i mean i thirty t over already

1581
01:37:34,480 --> 01:37:35,540
so my

1582
01:37:35,560 --> 01:37:38,750
representation of state

1583
01:37:38,770 --> 01:37:43,890
it's going to be this need discovery

1584
01:37:43,910 --> 01:37:47,930
representational state the mean of the next generation

1585
01:37:47,950 --> 01:37:49,390
the corvette

1586
01:37:49,410 --> 01:37:52,660
this morning

1587
01:37:52,660 --> 01:37:56,890
just like say if you are interested i see

1588
01:37:56,910 --> 01:38:02,810
but i have not international but parameter one inauguration speech

1589
01:38:02,810 --> 01:38:07,850
i would extend that to the one-dimensional galaxy

1590
01:38:09,540 --> 01:38:10,980
if i do that

1591
01:38:11,350 --> 01:38:16,120
he the next observation conditions of life that information get you

1592
01:38:16,140 --> 01:38:18,370
and that's what about he

1593
01:38:19,960 --> 01:38:22,020
so key parameters this

1594
01:38:22,040 --> 01:38:24,810
my and the first observation

1595
01:38:24,830 --> 01:38:30,500
and that in the next iteration

1596
01:38:33,430 --> 01:38:35,750
then so i will follow

1597
01:38:35,770 --> 01:38:38,480
it's easier

1598
01:38:38,500 --> 01:38:40,040
right about it

1599
01:38:40,060 --> 01:38:43,810
the key the key problem which is basis for all

1600
01:38:45,680 --> 01:38:46,790
is this

1601
01:38:46,810 --> 01:38:48,060
the list

1602
01:38:48,080 --> 01:38:50,290
it is called

1603
01:38:50,310 --> 01:38:52,370
with a

1604
01:38:52,620 --> 01:38:58,910
well according to the next generation

1605
01:38:58,930 --> 01:39:03,460
so i'm going to call this is going

1606
01:39:04,100 --> 01:39:06,810
cost cost so

1607
01:39:06,960 --> 01:39:08,500
and i feel we

1608
01:39:08,500 --> 01:39:13,160
well that's what i'm saying is one point out that

1609
01:39:13,160 --> 01:39:14,490
OK so

1610
01:39:14,520 --> 01:39:18,140
i don't know of any way of getting a good on that so long day's

1611
01:39:18,150 --> 01:39:21,110
regularisation parameter so you know what solution

1612
01:39:22,390 --> 01:39:24,300
you can very important problem

1613
01:39:26,170 --> 01:39:32,580
so i that methods the new people use really methods that is very important is

1614
01:39:32,580 --> 01:39:38,590
that somebody methods you would select levels one after the other this forward selection

1615
01:39:38,600 --> 01:39:43,390
and forward backward you can allow yourself to allow yourself to be to go back

1616
01:39:44,360 --> 01:39:51,100
so first this is non convex methods can because it's just completely just try to

1617
01:39:51,100 --> 01:39:53,540
do to solve the combinatorial optimization

1618
01:39:53,550 --> 01:39:55,470
so it's hard to analyse

1619
01:39:55,480 --> 01:39:59,930
it's a lot simpler to implement OK thanks with a few lines of

1620
01:39:59,960 --> 01:40:04,170
you may have some problems of stability which i will talk about later but

1621
01:40:04,200 --> 01:40:07,290
you get the same types of results as we use it also

1622
01:40:07,300 --> 01:40:11,340
OK so if you take all the conditions i've talked about for the last because

1623
01:40:11,340 --> 01:40:16,670
it stands to be efficient with those was pretty much greater minnesota get the same

1624
01:40:16,670 --> 01:40:21,460
types of results and domains organised then a lot of good work that so

1625
01:40:21,470 --> 01:40:27,540
current results do not distinguish between the two methods and in sparseness using convex optimization

1626
01:40:27,590 --> 01:40:33,650
which is kind of waiting for all the people do that too sparse methods one

1627
01:40:33,650 --> 01:40:35,550
of which include myself

1628
01:40:35,580 --> 01:40:40,490
because you should always compared with the method is simple to implement and elephants works

1629
01:40:41,830 --> 01:40:47,580
and so based methods i want i want to go then right because you have

1630
01:40:47,580 --> 01:40:48,650
ways of doing

1631
01:40:48,660 --> 01:40:50,120
stuff bayesian way

1632
01:40:50,140 --> 01:40:54,160
and maybe one or if that

1633
01:40:54,220 --> 01:40:59,350
if you use consider that so this is the square norm

1634
01:40:59,360 --> 01:41:02,750
OK to close in one long so if you if you look at that if

1635
01:41:02,750 --> 01:41:05,040
you see that the probability of

1636
01:41:05,050 --> 01:41:06,520
garson likelihood

1637
01:41:06,530 --> 01:41:08,850
and this you can consider that does not

1638
01:41:08,870 --> 01:41:12,900
probability of going solo plus factorized laplace prior

1639
01:41:12,910 --> 01:41:16,390
so if you want to do MAP estimation with priority to get back to so

1640
01:41:16,730 --> 01:41:18,100
life is nice and easy

1641
01:41:19,730 --> 01:41:24,040
the posterior if you do like some kind of a fully bayesian approach it because

1642
01:41:24,250 --> 01:41:28,740
poster you have no zeros because the posterior distribution of w

1643
01:41:28,770 --> 01:41:35,040
even today has no zeros you don't get sparsity the posterior another but it's a

1644
01:41:35,040 --> 01:41:37,770
good thing or a bad thing but just keep in mind that you don't get

1645
01:41:37,790 --> 01:41:39,100
exact zeros

1646
01:41:39,120 --> 01:41:42,730
this has led people to consider a proxy to sparsity

1647
01:41:42,750 --> 01:41:47,150
OK right you don't get zeros do that you actually can

1648
01:41:47,160 --> 01:41:53,100
OK i really looking for is yours are or you are you wanting in fact

1649
01:41:53,360 --> 01:41:58,340
to use a heavy tailed distributions so people have used all types of beta distributions

1650
01:41:58,340 --> 01:41:59,650
when the sitting

1651
01:41:59,660 --> 01:42:03,560
and one thing i want to mention that there is a lot less theory than

1652
01:42:03,560 --> 01:42:07,650
frequentist method can you have seen very few papers fantasy

1653
01:42:08,110 --> 01:42:15,720
how those vision methods behave in high dimensions things nice is nice open problem domains

1654
01:42:15,720 --> 01:42:18,360
that you can compare them

1655
01:42:19,240 --> 01:42:25,980
we compare them in a very simple we do oppose small pose questions one compare

1656
01:42:25,980 --> 01:42:28,410
them very simple problem when k

1657
01:42:28,430 --> 01:42:34,020
OK is the support size for sixty four levels piece from two hundred fifty six

1658
01:42:34,160 --> 01:42:38,370
so she signal to noise may still wish with one

1659
01:42:38,390 --> 01:42:42,000
so here on the left i compare these methods of the oracle

1660
01:42:42,020 --> 01:42:46,000
so if you know you should know and the support sets of course this is

1661
01:42:46,000 --> 01:42:50,790
independent of the number of the band of p because you know the icon so

1662
01:42:50,850 --> 01:42:52,040
even if you have a lot

1663
01:42:52,100 --> 01:42:58,390
of the variables this does does not change in red this is greedy blues one

1664
01:42:58,400 --> 01:43:00,220
and lugosi two

1665
01:43:00,240 --> 01:43:02,590
OK so this is a very simple

1666
01:43:02,600 --> 01:43:04,840
you can get few

1667
01:43:04,890 --> 01:43:06,740
so you see that

1668
01:43:06,750 --> 01:43:11,330
if you assume that the point is sparse which is the case because genetic data

1669
01:43:11,330 --> 01:43:15,590
we scale it called for my eyeballs out of this is form

1670
01:43:15,600 --> 01:43:20,640
o two two fifty six so we have a few variables would behaves the same

1671
01:43:20,640 --> 01:43:23,530
as soon as you add more and more irrelevant variables

1672
01:43:23,550 --> 01:43:31,050
and two norm go we do a very quickly and anyone on go includes likely

1673
01:43:31,050 --> 01:43:36,600
now function of the people which is somewhat is discredited by that surrounds

1674
01:43:36,610 --> 01:43:42,140
and the greedy in fact we will work better at the beginning work late we're

1675
01:43:42,150 --> 01:43:46,390
OK so this is those types of behavior are not supported by any type of

1676
01:43:46,390 --> 01:43:53,070
theory OK i know of six months ago and the greedy methods at the beginning

1677
01:43:53,100 --> 01:43:54,330
of bars

1678
01:43:54,340 --> 01:43:59,550
meaning that given that is easy sitting he doesn't average was but what it does

1679
01:43:59,550 --> 01:44:02,070
it is less stable this maybe be also

1680
01:44:02,490 --> 01:44:06,170
it's one of the nice TV is with business so that it is a lot

1681
01:44:06,170 --> 01:44:07,770
more stable

1682
01:44:07,820 --> 01:44:12,630
and of course also if you want days data OK so you stuff is your

1683
01:44:12,630 --> 01:44:14,650
sparse problem wrote that x

1684
01:44:14,680 --> 01:44:19,120
then your problem is non sparse animal and the norm should be totally envi into

1685
01:44:19,120 --> 01:44:22,120
that because you nominated by and by rotations

1686
01:44:22,140 --> 01:44:23,900
so the blues

1687
01:44:23,920 --> 01:44:26,730
the black curve is not changing a lot

1688
01:44:26,730 --> 01:44:28,200
there is this

1689
01:44:28,260 --> 01:44:35,050
this complicated looking formula the condition formula which tells us that every infinitely divisible random

1690
01:44:35,050 --> 01:44:38,700
variable has a characteristic function

1691
01:44:38,720 --> 01:44:39,920
of this

1692
01:44:41,480 --> 01:44:43,520
well we've got parameters

1693
01:44:43,540 --> 01:44:48,540
or rather characteristics mu is negative is is real

1694
01:44:48,550 --> 01:44:51,540
sigma squared is nonnegative and new

1695
01:44:51,550 --> 01:44:52,910
it is a measure

1696
01:44:52,920 --> 01:44:54,800
on the real line

1697
01:44:54,810 --> 01:44:56,010
and not

1698
01:44:56,030 --> 01:44:57,640
chanting zero

1699
01:44:57,680 --> 01:45:02,880
which has an integrability condition which means that it may be a finite measure or

1700
01:45:02,880 --> 01:45:07,800
it maybe infinite but if it is then it's still integrate x squared

1701
01:45:07,810 --> 01:45:10,050
around the world

1702
01:45:10,060 --> 01:45:11,470
so this is

1703
01:45:11,480 --> 01:45:14,670
this is what that result tells us and

1704
01:45:14,690 --> 01:45:18,680
rather than looking at the characteristic functions we could equally well be looking at the

1705
01:45:18,680 --> 01:45:24,320
moment generating functions provided these exist and you get exactly the same

1706
01:45:24,330 --> 01:45:26,130
formula with all

1707
01:45:26,170 --> 01:45:30,790
i size replaced by gammas

1708
01:45:31,750 --> 01:45:36,090
so we've got that term here in red which is going to play a very

1709
01:45:36,090 --> 01:45:38,350
important role in constructing

1710
01:45:38,410 --> 01:45:41,140
navy processes

1711
01:45:42,480 --> 01:45:47,430
yes wikipedia is exactly

1712
01:45:47,440 --> 01:45:54,390
measuring the energy that x has meaning that acts sort of corresponds to the jump

1713
01:45:54,390 --> 01:45:56,810
sizes of of the the process

1714
01:45:56,830 --> 01:46:00,970
so new is a measure that will be related to jobs as we will as

1715
01:46:00,970 --> 01:46:02,520
we will see

1716
01:46:02,530 --> 01:46:04,670
but first let's look at this formula

1717
01:46:05,440 --> 01:46:08,590
sort of explain where things are coming from

1718
01:46:09,700 --> 01:46:12,230
if we just look at the first time

1719
01:46:12,260 --> 01:46:13,640
this is what we

1720
01:46:13,650 --> 01:46:17,760
recorded at the moment generating function of the normal distribution

1721
01:46:17,780 --> 01:46:19,940
so we are really looking at a

1722
01:46:19,950 --> 01:46:22,030
normal random variables

1723
01:46:22,820 --> 01:46:24,270
something else

1724
01:46:24,290 --> 01:46:27,510
now that's something else we have also seen

1725
01:46:27,520 --> 01:46:29,460
at least part of it

1726
01:46:29,480 --> 01:46:33,940
when we were looking at the moment generating function of the compound possible process

1727
01:46:33,960 --> 01:46:35,440
we were looking at

1728
01:46:35,450 --> 01:46:37,060
the distribution

1729
01:46:37,070 --> 01:46:40,170
of the jumps of the compound possible crisis

1730
01:46:40,190 --> 01:46:44,220
and we we had that the integral of e to the gamma x minus one

1731
01:46:44,300 --> 01:46:46,300
against that distribution

1732
01:46:46,350 --> 01:46:48,570
so again we we know

1733
01:46:48,580 --> 01:46:49,810
what what this

1734
01:46:49,820 --> 01:46:53,880
this meaning is and we make that identification of x

1735
01:46:53,890 --> 01:46:54,750
as the

1736
01:46:54,770 --> 01:46:56,560
jump size and then

1737
01:46:56,580 --> 01:47:04,280
and in this sense of the combined for some purposes but there's more

1738
01:47:05,320 --> 01:47:10,850
well that more is essentially something that we also understand

1739
01:47:12,100 --> 01:47:15,560
what is new is doing is just shifting

1740
01:47:16,550 --> 01:47:21,250
mu if you look at the moment generating function of of constant new

1741
01:47:21,300 --> 01:47:24,480
then you get exactly that time there's no randomness

1742
01:47:24,500 --> 01:47:28,020
and this is another such term which is a multiple

1743
01:47:28,030 --> 01:47:29,340
of gamma

1744
01:47:29,370 --> 01:47:36,140
that's what what characterizes this interpretation so we are really looking at something constant here

1745
01:47:36,160 --> 01:47:37,800
except that

1746
01:47:37,810 --> 01:47:38,870
the integral

1747
01:47:38,880 --> 01:47:41,210
as it stands

1748
01:47:41,220 --> 01:47:43,060
does not converge

1749
01:47:43,070 --> 01:47:48,050
separately for this part and for that part in general

1750
01:47:48,080 --> 01:47:49,140
so that's where

1751
01:47:49,160 --> 01:47:50,100
with this

1752
01:47:50,120 --> 01:47:53,900
term has the second interpretation not just as

1753
01:47:54,390 --> 01:47:56,040
as shifts

1754
01:47:56,050 --> 01:48:00,930
by a constant but as convergence generating time in this

1755
01:48:00,980 --> 01:48:02,320
in this interval

1756
01:48:02,330 --> 01:48:08,370
we need to carefully look at how that convergence translates into properties of the

1757
01:48:08,390 --> 01:48:10,810
of the levy process that we

1758
01:48:10,850 --> 01:48:16,030
going to construct associated with such an infinitely divisible distribution

1759
01:48:16,050 --> 01:48:17,310
so what i have

1760
01:48:18,090 --> 01:48:20,420
talk you through was to say

1761
01:48:20,420 --> 01:48:24,630
well a random variable infinitely divisible random variable is

1762
01:48:24,650 --> 01:48:28,270
the sum of a

1763
01:48:28,280 --> 01:48:31,100
normally distributed random variables

1764
01:48:31,170 --> 01:48:32,370
of a

1765
01:48:32,380 --> 01:48:35,320
compound poisson distributed random variable

1766
01:48:35,340 --> 01:48:37,690
well there were some problems with this

1767
01:48:37,710 --> 01:48:39,510
with this integrability

1768
01:48:39,520 --> 01:48:43,320
near zero if this is to be a distribution it has to be a finite

1769
01:48:43,320 --> 01:48:46,580
measure this measure in general can be infinite

1770
01:48:46,590 --> 01:48:49,870
but what we can do is restrict attention to

1771
01:48:52,240 --> 01:48:54,020
to the range of

1772
01:48:54,090 --> 01:48:58,530
x values outside the interval minus one one

1773
01:48:58,790 --> 01:49:04,920
this is the interpretation of these excesses jump sizes so we're looking at big jumps

1774
01:49:04,930 --> 01:49:09,520
and that c one here now in my representation is

1775
01:49:10,790 --> 01:49:13,850
coming from a compound process

1776
01:49:15,020 --> 01:49:16,780
jump distributions

1777
01:49:16,790 --> 01:49:17,890
so that nu

1778
01:49:17,900 --> 01:49:21,700
restricted to minus one one complement

1779
01:49:21,730 --> 01:49:23,670
so the the big jumps

1780
01:49:23,670 --> 01:49:25,170
i've been generated

1781
01:49:25,170 --> 01:49:27,070
and they're being put into

1782
01:49:27,080 --> 01:49:29,700
in to see one

1783
01:49:29,720 --> 01:49:30,760
that lambda

1784
01:49:30,820 --> 01:49:33,270
was the rate of fire on process

1785
01:49:33,280 --> 01:49:39,380
and the interpretation in our formula i mean the land that was just sitting at

1786
01:49:39,380 --> 01:49:44,240
the fact to this time so we just move it out of the

1787
01:49:44,250 --> 01:49:45,170
the new

1788
01:49:45,190 --> 01:49:46,840
when we normalize it

1789
01:49:46,850 --> 01:49:49,730
and really identify that compon across

1790
01:49:49,760 --> 01:49:51,120
component here

1791
01:49:51,950 --> 01:49:53,870
small jumps that we've

1792
01:49:53,880 --> 01:49:55,570
forgotten about here

1793
01:49:55,590 --> 01:49:56,700
they are the

1794
01:49:56,720 --> 01:49:59,270
the term that's most problematic here

1795
01:49:59,290 --> 01:50:02,260
but what we can do is not just

1796
01:50:02,310 --> 01:50:04,110
do this argument for

1797
01:50:04,130 --> 01:50:09,160
jumps greater than one but we can do that argument four times greater than epsilon

1798
01:50:09,180 --> 01:50:15,610
for any small epsilon and we can construct such a compound pass on process

1799
01:50:15,610 --> 01:50:17,370
i have now chosen here

1800
01:50:19,360 --> 01:50:24,650
of sizes between epsilon and one in absolute value

1801
01:50:24,660 --> 01:50:31,720
with distribution just as in this case the restriction of our measure and u two

1802
01:50:31,780 --> 01:50:33,140
my is

1803
01:50:33,160 --> 01:50:38,170
one epsilon minus epsilon and epsilon up to one

1804
01:50:38,310 --> 01:50:41,180
we've got the right associated which is the

1805
01:50:42,070 --> 01:50:43,260
the measure

1806
01:50:43,290 --> 01:50:45,120
evaluated on that

1807
01:50:46,840 --> 01:50:50,210
epsilon to one in absolute value

1808
01:50:50,280 --> 01:50:55,250
and for every epsilon this quantity makes sense but to construct

1809
01:50:56,750 --> 01:51:01,030
the random variable and one we need to let

1810
01:51:01,040 --> 01:51:03,060
epsilon tend to zero

1811
01:51:03,060 --> 01:51:07,420
and there's some technicalities in form of an l to limit this

1812
01:51:07,430 --> 01:51:09,740
can be made mathematically precise

1813
01:51:10,280 --> 01:51:12,450
OK so this was just too

1814
01:51:12,460 --> 01:51:17,790
to say that i infinitely divisible random variable has such a structure because we recognise

1815
01:51:17,810 --> 01:51:19,470
the ingredients

1816
01:51:20,350 --> 01:51:26,600
of course we can now use this to construct a levy process along the lines

1817
01:51:26,600 --> 01:51:27,480
that i've

1818
01:51:27,510 --> 01:51:33,620
i've explained on the previous slide to take a brownian motion to give us that

1819
01:51:33,620 --> 01:51:35,300
OK listen

1820
01:51:38,710 --> 01:51:41,350
when this room things that

1821
01:51:41,360 --> 01:51:45,890
you are a programmer

1822
01:51:47,430 --> 01:51:50,230
not so we OK

1823
01:51:50,230 --> 01:51:55,460
i'm a programmer and developer i have a pretty thin

1824
01:51:55,480 --> 01:51:58,090
academic background and

1825
01:51:58,140 --> 01:51:59,660
the difference between

1826
01:51:59,700 --> 01:52:03,060
developer and research that

1827
01:52:03,100 --> 01:52:05,340
researchers usually

1828
01:52:05,350 --> 01:52:07,410
is looking for a problem

1829
01:52:07,410 --> 01:52:08,850
have to solve

1830
01:52:10,280 --> 01:52:14,020
the developer is looking for solutions

1831
01:52:14,080 --> 01:52:19,800
and this car that i decided to

1832
01:52:19,840 --> 01:52:21,120
give you it

1833
01:52:21,170 --> 01:52:26,030
course about information retrieval from a developer point of view

1834
01:52:26,090 --> 01:52:31,150
it's not about the problems it's about solutions to maybe maybe small solutions but real

1835
01:52:31,150 --> 01:52:33,310
practical solutions that we

1836
01:52:33,330 --> 01:52:34,920
user that created

1837
01:52:34,950 --> 01:52:38,140
information retrieval systems

1838
01:52:39,780 --> 01:52:42,020
talking about information retrieval

1839
01:52:42,110 --> 01:52:43,500
we are all

1840
01:52:43,520 --> 01:52:46,690
very young here and we

1841
01:52:46,700 --> 01:52:51,840
you have things that information is something new

1842
01:52:51,860 --> 01:52:58,220
but i can assure you that information retrieval is more than four thousand years old

1843
01:52:58,220 --> 01:53:03,230
and in ancient egypt they already had all this stuff

1844
01:53:03,250 --> 01:53:08,330
they definitely had storage overhead tanks in paper form

1845
01:53:08,390 --> 01:53:10,160
they have indexes

1846
01:53:10,170 --> 01:53:13,220
i'm sure about this and is

1847
01:53:13,230 --> 01:53:20,140
archaeological results they actually in this information they put it into some being they judge

1848
01:53:20,280 --> 01:53:22,110
differently this judge

1849
01:53:22,170 --> 01:53:26,190
they had hyperlink because documented documents were

1850
01:53:27,420 --> 01:53:28,750
one document

1851
01:53:28,770 --> 01:53:32,970
usually referred to another one

1852
01:53:32,980 --> 01:53:38,440
then they had search engine it was library and

1853
01:53:38,490 --> 01:53:44,910
and library librarians the people in library they search engine for all humankind from one

1854
01:53:44,910 --> 01:53:46,440
thousand years

1855
01:53:46,500 --> 01:53:47,940
and they had

1856
01:53:47,940 --> 01:53:50,260
all this stuff also and they were

1857
01:53:50,310 --> 01:53:52,010
one hundred percent

1858
01:53:52,030 --> 01:53:55,760
but i like because if you have a huge library

1859
01:53:55,780 --> 01:54:01,260
you have a a lot of librarians in this library and they work in parallel

1860
01:54:01,280 --> 01:54:04,290
but everything is not everything but a lot of

1861
01:54:04,310 --> 01:54:08,600
that changed in in the middle of the previous century when

1862
01:54:09,090 --> 01:54:12,710
digital computers became much more popular

1863
01:54:12,750 --> 01:54:14,320
and beginning from

1864
01:54:14,340 --> 01:54:19,840
this period of time program people who are working with computers

1865
01:54:19,840 --> 01:54:25,900
started to play started playing some role is in the information retrieval

1866
01:54:26,780 --> 01:54:30,500
it's true that information retrieval system

1867
01:54:30,510 --> 01:54:34,810
developed in nineteen fifty are around this

1868
01:54:34,870 --> 01:54:38,660
and different from the current systems is that

1869
01:54:41,060 --> 01:54:42,780
problem oriented

1870
01:54:42,790 --> 01:54:49,540
for example in this slide is not very clear but this interface of lexis nexis

1871
01:54:49,540 --> 01:54:52,530
lexis nexis is legal

1872
01:54:52,540 --> 01:54:56,960
information retrieval system is a very popular one and more than that

1873
01:54:57,000 --> 01:54:58,900
if talking about

1874
01:54:58,940 --> 01:55:05,090
controlled collection collection that we know how many documents are in the collection and somebody

1875
01:55:05,090 --> 01:55:06,000
on this

1876
01:55:06,030 --> 01:55:13,380
so somebody it on this on this collection lexisnexis is still the biggest

1877
01:55:13,380 --> 01:55:15,750
information to the system in the world

1878
01:55:15,810 --> 01:55:20,280
and you can see the interface is not i believe it's not very clear but

1879
01:55:20,280 --> 01:55:25,530
it's very very problem oriented it's again interface for librarians

1880
01:55:25,540 --> 01:55:30,930
and again people who developed the system so they invented all this stuff they invented

1881
01:55:31,060 --> 01:55:35,060
in texas and all this stuff they didn't talk to librarians

1882
01:55:35,060 --> 01:55:41,400
because they so a smart enough and they can implement everything from scratch

1883
01:55:41,470 --> 01:55:43,220
that's what happens

1884
01:55:43,240 --> 01:55:46,750
next about year two thousand internet

1885
01:55:47,760 --> 01:55:50,060
b became popular

1886
01:55:50,110 --> 01:55:52,860
and internet search engines from

1887
01:55:54,880 --> 01:56:01,550
so i'm not popular type of sites that are doing some searching some search databases

1888
01:56:01,560 --> 01:56:04,620
suddenly became huge festival because of

1889
01:56:04,680 --> 01:56:10,700
very good canonical model of information of search engine internet search engine

1890
01:56:10,710 --> 01:56:12,120
and again

1891
01:56:12,130 --> 01:56:15,450
information retrieval became like

1892
01:56:15,460 --> 01:56:17,250
something very new

1893
01:56:17,310 --> 01:56:20,850
people again invent all this real

1894
01:56:20,870 --> 01:56:23,770
in indexes and all of this stuff

1895
01:56:25,310 --> 01:56:31,680
what i'm going to talk about this very basic things that structures that are behind

1896
01:56:31,680 --> 01:56:35,800
search engine or all the information retrieval system

1897
01:56:36,950 --> 01:56:39,670
i'll give you an example from librarians to

1898
01:56:39,690 --> 01:56:41,230
the modern systems

1899
01:56:41,260 --> 01:56:45,120
but only one exception i'm not going to talk about

1900
01:56:45,130 --> 01:56:48,310
a real implementation in any commercial systems

1901
01:56:48,320 --> 01:56:49,370
first of all

1902
01:56:49,380 --> 01:56:54,500
if i have worked for some of these systems i simply cannot do this

1903
01:56:54,500 --> 01:56:55,700
if i

