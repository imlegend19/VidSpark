1
00:00:00,000 --> 00:00:01,800
it's any type of activity we do

2
00:00:01,840 --> 00:00:04,140
it's really process so

3
00:00:04,150 --> 00:00:07,640
short periods of very intensive of that type of activity and the long periods of

4
00:00:07,640 --> 00:00:11,040
not much happening so if you think about the because it means that

5
00:00:11,420 --> 00:00:15,720
the short periods where i know the user it is very precisely because he or

6
00:00:15,720 --> 00:00:18,910
she make lots of phone calls and then there a long period that he or

7
00:00:18,910 --> 00:00:21,770
she doesn't make any functlon i have no idea where they are

8
00:00:23,370 --> 00:00:25,680
at the end of the day we do need to kind of deal with all

9
00:00:25,920 --> 00:00:31,080
limitations here i'm showing you is that the inter event time distribution for people who

10
00:00:31,080 --> 00:00:35,560
make phone calls and you can see it's fact distribution with the exponential cutoff

11
00:00:35,640 --> 00:00:39,550
and you know we and many others actually have worked on on how the model

12
00:00:39,550 --> 00:00:41,310
that it wasn't happening

13
00:00:41,380 --> 00:00:43,310
but they

14
00:00:43,320 --> 00:00:47,670
this very much affects our statistics but the question does it really matter and what

15
00:00:47,770 --> 00:00:50,550
of the things that we found is if we look at the user's motion it

16
00:00:50,550 --> 00:00:54,540
may not matter at all because we have such we have a very large number

17
00:00:54,540 --> 00:00:58,800
of users in our dataset we have also six to ten million people and then

18
00:00:59,820 --> 00:01:04,780
you always find individuals who who sent close enough such that you can collect sufficient

19
00:01:04,780 --> 00:01:08,420
number of them and you can order location very precisely

20
00:01:08,420 --> 00:01:09,400
at the end of the

21
00:01:09,410 --> 00:01:12,530
OK let's just look how is he doesn't look like so

22
00:01:12,540 --> 00:01:17,460
here we have a kind three hundred by two hundred kilometre region vertically two hundred

23
00:01:17,460 --> 00:01:19,010
may not be visible

24
00:01:19,020 --> 00:01:23,540
and here the trajectories of the trajectories of a couple of individuals and what you

25
00:01:23,540 --> 00:01:27,110
can see that some people seem to be moving along you know this guy here

26
00:01:27,200 --> 00:01:31,300
not moving too much you guys are going to much if you do a couple

27
00:01:31,300 --> 00:01:32,410
of the jumps

28
00:01:32,470 --> 00:01:38,420
and so it's a very heterogeneous process and the question is how you characterize how

29
00:01:38,420 --> 00:01:42,650
you described mathematically even though such a huge differences between what people do

30
00:01:44,370 --> 00:01:49,140
in order to do that what what we started to kind of fell on the

31
00:01:49,210 --> 00:01:54,260
shoulders of broken and others is to look at the jump size distribution simply what

32
00:01:54,300 --> 00:01:55,840
is the distance between

33
00:01:56,110 --> 00:01:58,420
two consecutive foreclose like this

34
00:01:58,430 --> 00:02:01,720
it was red one particular moment and later on admitted he or she made another

35
00:02:01,720 --> 00:02:05,540
court and the nice thing is that when we look at the distribution of the

36
00:02:05,580 --> 00:02:06,790
jump sizes

37
00:02:06,800 --> 00:02:10,950
then it seems to be following pretty much the same power law very close to

38
00:02:10,950 --> 00:02:14,190
the exponent the problem has found in the dollar belligerent

39
00:02:14,420 --> 00:02:16,290
and what is interesting

40
00:02:16,300 --> 00:02:19,650
it's because you know this is a fundamentally different process

41
00:02:19,660 --> 00:02:24,740
because you have to realize that that here we actually following the individuals just looking

42
00:02:24,740 --> 00:02:29,420
at the money and that will become very important in the second so the consistency

43
00:02:29,420 --> 00:02:33,260
between the two dissenters was very very encouraging so we see we somehow seeing the

44
00:02:33,260 --> 00:02:36,900
same process the same origin but really

45
00:02:36,920 --> 00:02:41,090
what is the mechanism for this world that is not clear because there are two

46
00:02:41,090 --> 00:02:44,080
very different explanations there's lots of it

47
00:02:44,090 --> 00:02:46,760
each user finds truncated leaving flight

48
00:02:46,800 --> 00:02:49,260
the other possibility is that

49
00:02:49,280 --> 00:02:54,590
none the user's follow flight but there are significant differences between the users of how

50
00:02:54,590 --> 00:02:59,270
they travel around so many people some people travel shorter distances most of the time

51
00:02:59,570 --> 00:03:05,880
of the most popular long distances and this population heterogeneity made some follow power law

52
00:03:06,190 --> 00:03:12,530
and made responsible for the process essentially that we see

53
00:03:12,590 --> 00:03:16,940
and in this part of the atmosphere we're here so we need to somehow distinguishes

54
00:03:16,940 --> 00:03:20,900
to processes and about germany not allowed to do this because the

55
00:03:21,630 --> 00:03:25,900
material is not telling us where the person is moving and we need to understand

56
00:03:25,900 --> 00:03:27,220
that every time the dollar

57
00:03:27,390 --> 00:03:33,670
somewhere it's really following some other process trajectories of the dollar bill measurement was going

58
00:03:33,670 --> 00:03:38,990
from person to person and sampling of many many individuals and so so it it

59
00:03:38,990 --> 00:03:42,870
was not possible to distinguish whether coming from one person to tell whether it's really

60
00:03:42,880 --> 00:03:46,250
kind of collective effect so we need to do that now and in order to

61
00:03:46,250 --> 00:03:50,090
do that what we did is for each user we define the centre of mass

62
00:03:50,090 --> 00:03:53,210
based on the trajectory and the radius of gyration and then go back to this

63
00:03:54,130 --> 00:03:57,970
and you can see this is the latest so generation disperse which looks very misleading

64
00:03:57,970 --> 00:04:02,960
because this person obviously makes a couple of travels very far away but the point

65
00:04:02,960 --> 00:04:06,690
is that most of the time is in this neighbourhood and these travels have a

66
00:04:06,690 --> 00:04:12,160
small statistical weight to the voting so therefore he is he's already generation is really

67
00:04:12,160 --> 00:04:16,920
dominated by the motion in its immediate neighborhood of home or work at the end

68
00:04:16,920 --> 00:04:20,830
of the day we can assign to each user particularly useful generation

69
00:04:20,840 --> 00:04:25,560
and the question we're asking really here is what is the difference between individuals that

70
00:04:25,660 --> 00:04:28,310
we find that there is such an issue for each of you for example in

71
00:04:28,310 --> 00:04:32,920
this room for everybody in the country how different with be i will similarly very

72
00:04:32,990 --> 00:04:34,310
different from each other

73
00:04:34,330 --> 00:04:38,730
and for us to see to what degree we need to see to what degree

74
00:04:38,730 --> 00:04:41,210
status generation is stable in time

75
00:04:41,300 --> 00:04:44,960
and for that we can group the users into different categories people seem to be

76
00:04:44,960 --> 00:04:49,130
travelling along people don't care so much look at the time dependence so this is

77
00:04:49,130 --> 00:04:50,920
how to use the generation of people who

78
00:04:51,390 --> 00:04:55,700
who was at the end you have less than a kilometre travelled looks and you

79
00:04:55,700 --> 00:04:59,600
see it seems to be saturated now why is this interesting

80
00:04:59,620 --> 00:05:04,050
because the saturation curve is a much slower dependence what you would expect this will

81
00:05:04,050 --> 00:05:08,650
disability fly because if you silly flight then we have a very precise prediction for

82
00:05:08,650 --> 00:05:12,860
and what is the previous generation should increase as the power law we don't see

83
00:05:12,860 --> 00:05:16,680
the problem behavior if anything it looks to be going to constant or maybe it's

84
00:05:16,680 --> 00:05:22,600
very slow increase and if we look more carefully turns out that the previous generation

85
00:05:22,610 --> 00:05:27,730
is really kind of growing approximately logarithmically in time so this is the linear local

86
00:05:28,070 --> 00:05:32,060
and you see that it's kind of roughly about the positive the logarithmic increase

87
00:05:32,070 --> 00:05:36,320
you could pretty much take the saturation process because at the end delivery becomes so

88
00:05:36,320 --> 00:05:40,460
slow but it's not it's kind of this is a very slow increase in time

89
00:05:40,520 --> 00:05:45,150
the point is that now we have a generation that fought like after all kind

90
00:05:45,150 --> 00:05:51,380
of flexible amounts observation more saturated not exactly then we can look at the distribution

91
00:05:51,550 --> 00:05:52,690
of these values

92
00:05:52,730 --> 00:05:56,760
and the nice thing is that you see yet again the same problem that we're

93
00:05:56,760 --> 00:05:57,980
seeing here

94
00:05:58,050 --> 00:06:03,110
that is what this is really kind of exploring how different individuals are and what

95
00:06:03,110 --> 00:06:06,810
you see is that most people have a very small radius of gyration one to

96
00:06:06,810 --> 00:06:11,990
ten kilometres if you never come to the table over hundreds of kilometres

97
00:06:12,050 --> 00:06:15,680
so this is indicating that really the origin of this part of the data

98
00:06:15,700 --> 00:06:19,080
the measurement was seeing and we're seeing them we look at the jump size distribution

99
00:06:19,080 --> 00:06:25,010
across the world population to high degree is rooted in the heterogeneity of the population

100
00:06:26,190 --> 00:06:31,430
the you know people people have typical region in which they move around and there

101
00:06:31,440 --> 00:06:36,310
are kind of distributed differences between different individuals so most people just move in the

102
00:06:36,310 --> 00:06:40,460
same neighborhood one two three kilometres if you have a regular travel over very long

103
00:06:41,660 --> 00:06:46,940
now the what we find here is slightly different from what we found earlier

104
00:06:47,590 --> 00:06:48,820
not so

105
00:06:48,860 --> 00:06:52,320
not significantly but actually is different doesn't make sense

106
00:06:52,330 --> 00:06:57,720
and to understand what this picture may so again showing this kind of trajectories and

107
00:06:57,720 --> 00:07:02,750
the this distribution really meant that it was kind of the fact that people is

108
00:07:02,750 --> 00:07:08,160
not so different from each other a few people moving this guy's pretty pictures suddenly

109
00:07:08,160 --> 00:07:13,500
moved here this guy moving at very small distances and these few big chance are

110
00:07:13,510 --> 00:07:17,680
the few travelers who really kind of had this very long radius of generation and

111
00:07:17,680 --> 00:07:22,520
they the ones who contributed to sleep but that but this because its population heterogeneity

112
00:07:22,520 --> 00:07:24,960
influence someone's breathing

113
00:07:24,970 --> 00:07:27,850
how you can control someone else's breathing

114
00:07:27,890 --> 00:07:30,620
i'm going to try and so i picked him up

115
00:07:31,080 --> 00:07:35,830
shock immunities stopping criteria wouldn't stop so so what did i

116
00:07:35,850 --> 00:07:37,610
held in very close

117
00:07:37,620 --> 00:07:38,930
to my chest

118
00:07:38,990 --> 00:07:41,420
and i started breathing

119
00:07:41,420 --> 00:07:44,670
at the same rate that he was crying

120
00:07:44,670 --> 00:07:50,720
so he was so i started breathing at the same rate and i had a

121
00:07:50,720 --> 00:07:56,340
very close we can feel me breathing at the same rate he was crying and

122
00:07:56,340 --> 00:08:06,630
slowly i slowed down

123
00:08:06,640 --> 00:08:10,730
and he slowed down and probably the same way that all of you just did

124
00:08:10,760 --> 00:08:16,680
i've done this in big groups and when i stopped breathing

125
00:08:16,930 --> 00:08:21,000
the whole route no one's breathing and some after you can start breathing you know

126
00:08:21,000 --> 00:08:22,630
it's OK to

127
00:08:22,730 --> 00:08:26,730
but but the important thing if you're going to do something like this if you're

128
00:08:26,730 --> 00:08:31,560
can lead people to for example your your sitting at home

129
00:08:31,560 --> 00:08:34,070
and i do not and your partners at home

130
00:08:34,120 --> 00:08:36,380
and you just had some great news

131
00:08:36,390 --> 00:08:40,650
i know you've got to pay rise or promotional or something and you run an

132
00:08:40,890 --> 00:08:43,530
annual really excited

133
00:08:43,560 --> 00:08:44,410
and you

134
00:08:44,420 --> 00:08:46,730
you get what you and your partner

135
00:08:46,760 --> 00:08:48,350
calm down

136
00:08:48,440 --> 00:08:53,350
calm down and you know i don't want to calm the how does that feel

137
00:08:53,390 --> 00:08:58,180
when you're really excited and some you know

138
00:08:58,240 --> 00:08:59,750
it's horrible isn't it

139
00:08:59,760 --> 00:09:03,430
just think why don't you understand how i feel what they should do

140
00:09:03,450 --> 00:09:06,180
if they want you to calm down you know what they should do as you

141
00:09:06,180 --> 00:09:10,810
running and into a guess what they should jump up and say well what happened

142
00:09:10,810 --> 00:09:15,420
and they should get on the same level as you first and then they should

143
00:09:15,420 --> 00:09:18,210
start slowly coming down and so will you

144
00:09:18,350 --> 00:09:22,280
you've got to match first if you want to influence someone

145
00:09:22,350 --> 00:09:27,490
match them first and then control and and come down and they will come down

146
00:09:27,530 --> 00:09:28,680
with you

147
00:09:28,710 --> 00:09:30,730
it doesn't work otherwise

148
00:09:30,920 --> 00:09:35,570
right i want to

149
00:09:35,620 --> 00:09:36,670
i want to

150
00:09:36,690 --> 00:09:38,600
demonstrate the power

151
00:09:40,280 --> 00:09:41,410
ron paul

152
00:09:41,420 --> 00:09:45,580
and the power of matching people

153
00:09:45,620 --> 00:09:50,460
because there another thing that richard bandler and john grinder found was this is natural

154
00:09:50,490 --> 00:09:52,480
it's totally natural

155
00:09:52,490 --> 00:09:53,920
human beings

156
00:09:53,920 --> 00:09:57,390
to start looking like each other and sounding like each other

157
00:09:57,410 --> 00:09:59,940
when they are connected to each other when there

158
00:09:59,960 --> 00:10:03,790
sort of in rapport wheels to start looking like each other

159
00:10:03,860 --> 00:10:08,140
and what they found is this is an actual trade if you have rapport

160
00:10:08,150 --> 00:10:09,420
you start

161
00:10:09,430 --> 00:10:11,430
looking and sounding the same

162
00:10:11,440 --> 00:10:14,240
and what they what they thought was hang on

163
00:10:14,290 --> 00:10:16,890
can we do it the other way round

164
00:10:16,920 --> 00:10:18,820
can we build rapport

165
00:10:22,400 --> 00:10:24,440
because the cause effect goes this way

166
00:10:24,540 --> 00:10:27,230
rapport causes matching

167
00:10:27,250 --> 00:10:29,470
visual and vocal and they asked

168
00:10:30,560 --> 00:10:34,020
visual and vocal matching cause rapport

169
00:10:34,080 --> 00:10:35,700
and what they found was

170
00:10:37,410 --> 00:10:39,470
you can manufacture rapport

171
00:10:39,530 --> 00:10:41,080
which is

172
00:10:41,090 --> 00:10:43,370
some people

173
00:10:43,390 --> 00:10:45,120
find very uncomfortable

174
00:10:45,210 --> 00:10:50,760
very uncomfortable with this idea of of manufacturing connection with another person but what you

175
00:10:50,760 --> 00:10:53,690
like you know it's true it works

176
00:10:53,750 --> 00:10:56,420
and so what we're going to do is i want to prove this to you

177
00:10:56,430 --> 00:11:00,310
and i want to prove how important it is to be able

178
00:11:00,340 --> 00:11:03,350
switches on when you need to switch it on so we can do very quick

179
00:11:04,990 --> 00:11:08,400
the first thing we need in this exercise you need to have a partner to

180
00:11:08,400 --> 00:11:12,120
work with so i would like you to find another person one person

181
00:11:12,270 --> 00:11:20,130
who you gonna do the exercise to just decided upon the accessed

182
00:11:20,160 --> 00:11:23,000
so you see you did did to do

183
00:11:23,020 --> 00:11:25,290
OK all three of them

184
00:11:25,320 --> 00:11:28,920
you're you're can someone come over here we've got one person

185
00:11:28,970 --> 00:11:30,250
three here

186
00:11:30,270 --> 00:11:33,580
because your straight if you come over here beta

187
00:11:33,580 --> 00:11:36,120
so if you work with the have it

188
00:11:39,270 --> 00:11:40,940
well why don't want to

189
00:11:40,940 --> 00:11:43,780
this essentially

190
00:11:43,830 --> 00:11:46,050
is equal to

191
00:11:48,960 --> 00:11:51,900
pseudoinverse of that set

192
00:11:51,960 --> 00:11:57,530
so n is equal to the pseudo inverse of distillation times the source matrix

193
00:11:57,580 --> 00:11:59,470
because the pseudoinverse

194
00:11:59,530 --> 00:12:05,630
so in essentially and the interesting thing here is that this and has a lot

195
00:12:05,630 --> 00:12:08,560
to go into the finals really important because

196
00:12:08,620 --> 00:12:12,500
if the reason i'm going on about find what is that when you because con

197
00:12:12,630 --> 00:12:16,590
so that mesh measure the face and things and really what that is that's a

198
00:12:16,590 --> 00:12:22,410
piecewise find a fine mesh so all those triangles are actually we treat those as

199
00:12:22,410 --> 00:12:26,930
the final what's and so if you want more complicated what we actually use this

200
00:12:26,930 --> 00:12:31,110
sort of case these little pieces of fine what together to build up a big

201
00:12:31,650 --> 00:12:35,350
and so it's kind of analogous to gas mixture models quite big gas in this

202
00:12:35,350 --> 00:12:39,490
kind of like your fundamental thing in machine learning that use if using a generative

203
00:12:39,490 --> 00:12:41,540
model you've got what asking

204
00:12:41,640 --> 00:12:46,760
gas distribution it's not because you need second order moments is the fundamental way

205
00:12:46,810 --> 00:12:52,640
to deal with the distribution and the final something similar in computer vision so basically

206
00:12:53,120 --> 00:12:56,510
i want to find can build up a lot more complicated what just from the

207
00:12:56,510 --> 00:13:00,600
so far obviously there are kind of war there are other

208
00:13:00,620 --> 00:13:04,000
how much what things like perspective and things like that but

209
00:13:04,040 --> 00:13:07,810
we were able later and on the interesting thing about this and

210
00:13:07,860 --> 00:13:08,690
is that

211
00:13:08,690 --> 00:13:11,640
it will have a functional forms and one

212
00:13:11,700 --> 00:13:13,850
and two

213
00:13:17,780 --> 00:13:19,220
two one

214
00:13:19,270 --> 00:13:20,800
one two

215
00:13:22,900 --> 00:13:24,650
one three

216
00:13:24,690 --> 00:13:28,580
two three belong here

217
00:13:28,640 --> 00:13:32,530
this is always the two d case zeros or one

218
00:13:33,050 --> 00:13:39,310
and important business e two is that i can treat this sort of clear this

219
00:13:42,470 --> 00:13:44,930
now write this

220
00:13:45,080 --> 00:13:48,850
and so on

221
00:13:48,990 --> 00:13:52,930
give you a microscope back

222
00:13:53,030 --> 00:13:56,260
all right this bigger

223
00:13:58,640 --> 00:14:02,330
i'm talking about the final this thing i think of one way of thinking of

224
00:14:02,330 --> 00:14:06,620
mapping one travel to another always think that is three points one travel to another

225
00:14:06,700 --> 00:14:07,450
and then

226
00:14:07,470 --> 00:14:09,680
i got that one

227
00:14:11,220 --> 00:14:12,760
keep stuff myself up here

228
00:14:12,780 --> 00:14:15,260
two one zero

229
00:14:15,310 --> 00:14:17,870
in one to two

230
00:14:17,930 --> 00:14:19,390
and one three

231
00:14:20,580 --> 00:14:21,660
two three

232
00:14:21,660 --> 00:14:23,160
zero one

233
00:14:23,220 --> 00:14:29,780
now the important thing another important thing that we deal with these what functions

234
00:14:29,830 --> 00:14:32,640
is the notion of identity

235
00:14:32,720 --> 00:14:36,010
and the notion of war this is that

236
00:14:36,060 --> 00:14:37,490
if all input x

237
00:14:37,580 --> 00:14:39,910
should be able to

238
00:14:39,950 --> 00:14:41,740
what function x

239
00:14:41,740 --> 00:14:45,990
given that my wall is the is zero

240
00:14:46,030 --> 00:14:48,470
so can i receive given that

241
00:14:50,310 --> 00:14:55,010
so to think about the translation in the example if i'm shifting if if p

242
00:14:55,060 --> 00:14:56,280
relates to

243
00:14:56,290 --> 00:14:59,030
translation x translation one

244
00:14:59,030 --> 00:15:01,870
this if this is zero zero effect

245
00:15:02,660 --> 00:15:07,260
x has to be able to the wall function given that my parameter here

246
00:15:07,370 --> 00:15:09,790
now can you can see a problem here

247
00:15:09,810 --> 00:15:13,930
if i come from it rises to find more in this way

248
00:15:13,950 --> 00:15:16,890
if i try if i try do that

249
00:15:16,910 --> 00:15:21,760
so essentially what i do when i'm trying to do define what is it

250
00:15:21,780 --> 00:15:26,930
i've got my source that my source functions and multiply this by n

251
00:15:26,950 --> 00:15:28,830
i get my destination

252
00:15:28,930 --> 00:15:30,390
so if

253
00:15:30,450 --> 00:15:33,260
if i said all these to zero

254
00:15:33,410 --> 00:15:35,530
multiply it by the source

255
00:15:35,580 --> 00:15:37,850
i'm going to get the destination

256
00:15:38,490 --> 00:15:41,600
so the other thing that we have to do when we're dealing with a funnel

257
00:15:41,600 --> 00:15:47,700
is essentially we represent slightly differently so we actually represents p

258
00:15:48,120 --> 00:15:51,760
and this is again going details with the final performance

259
00:15:51,760 --> 00:15:52,750
o ports

260
00:15:52,810 --> 00:15:55,360
the magnets

261
00:15:55,400 --> 00:15:58,230
another way of looking at is that this

262
00:15:58,260 --> 00:16:02,160
current loop is all by itself a little magnets will by the south pole is

263
00:16:02,180 --> 00:16:05,040
here and the north pole is there

264
00:16:05,090 --> 00:16:08,930
because is the direction of the magnetic dipole moment and the north pole attracts the

265
00:16:08,930 --> 00:16:12,470
south pole that's another way of looking at it and that's the reason why magnets

266
00:16:12,470 --> 00:16:16,390
attract each other by north and south poles attract each other and by north and

267
00:16:16,390 --> 00:16:20,960
north poles repel each other that's exactly the reason is the current that is flowing

268
00:16:21,140 --> 00:16:22,650
it is the world's force

269
00:16:22,710 --> 00:16:23,870
that causes

270
00:16:23,880 --> 00:16:26,860
the attraction or repelling force

271
00:16:26,900 --> 00:16:31,970
so paramagnetic material is attracted by a magnet essentially is that this field is not

272
00:16:33,450 --> 00:16:36,570
and diamagnetic material of course will be repelled

273
00:16:36,580 --> 00:16:37,690
it will be pushed

274
00:16:37,740 --> 00:16:43,360
away from the strong field because in paramagnetic and diamagnetic materials this current will be

275
00:16:43,360 --> 00:16:46,180
running in the opposite direction because it opposes

276
00:16:46,860 --> 00:16:48,110
external field

277
00:16:48,120 --> 00:16:52,270
west by magnetism supports it

278
00:16:52,330 --> 00:16:54,330
we have forum

279
00:16:54,370 --> 00:16:57,000
and the third form of magnetism

280
00:16:57,020 --> 00:16:58,640
actually the most interesting

281
00:16:58,650 --> 00:17:02,510
its ferromagnetism

282
00:17:02,520 --> 00:17:04,900
in the case of ferromagnetism

283
00:17:04,960 --> 00:17:07,370
we again have the evidence

284
00:17:07,440 --> 00:17:09,110
have themselves

285
00:17:09,150 --> 00:17:11,400
permanent dipole moments

286
00:17:11,480 --> 00:17:12,930
but now

287
00:17:12,980 --> 00:17:17,100
four very mysterious reasons which can only be understood with quantum mechanics

288
00:17:17,150 --> 00:17:19,200
there i domains

289
00:17:19,240 --> 00:17:23,000
which have the dimension of about a tenth of a millimeter maybe three tenths of

290
00:17:23,000 --> 00:17:23,870
a millimetre

291
00:17:23,960 --> 00:17:28,520
by the dipoles are a hundred percent aligned

292
00:17:28,530 --> 00:17:30,270
and these dipole

293
00:17:32,430 --> 00:17:34,250
which i one direction

294
00:17:34,350 --> 00:17:38,650
are uniformly distributed throughout the ferromagnetic materials

295
00:17:38,710 --> 00:17:40,530
so there may not be any

296
00:17:40,540 --> 00:17:42,700
nett magnetic field

297
00:17:42,710 --> 00:17:45,610
if i have here but try to make a sketch

298
00:17:45,660 --> 00:17:47,600
of those domains

299
00:17:47,750 --> 00:17:51,100
something like this then perhaps here

300
00:17:51,110 --> 00:17:52,980
all the dipole

301
00:17:52,980 --> 00:17:55,780
recall the hundred percent aligned in this direction

302
00:17:55,790 --> 00:17:57,260
but for instance here

303
00:17:57,310 --> 00:17:58,480
they all the

304
00:17:58,520 --> 00:18:00,180
aligned in this direction

305
00:18:00,280 --> 00:18:06,090
and the number of atoms involved in such a domain is typically ten to the

306
00:18:06,090 --> 00:18:09,150
seventeenth may be up to ten to the twenty one

307
00:18:10,870 --> 00:18:14,120
so if you doubt i apply an external fields

308
00:18:14,180 --> 00:18:15,700
these domains

309
00:18:15,720 --> 00:18:17,010
i will be force

310
00:18:17,070 --> 00:18:18,860
to go in the direction of the

311
00:18:18,910 --> 00:18:22,910
the magnetic field and of course the degree of success depends on the strength of

312
00:18:22,910 --> 00:18:25,990
the external field the strength of the vacuum field

313
00:18:26,120 --> 00:18:28,760
and all the temperature the lower the temperature

314
00:18:28,820 --> 00:18:32,270
the better it is because then there is less thermal agitation

315
00:18:32,320 --> 00:18:37,740
which of course at a certain random randomness the whole process

316
00:18:37,830 --> 00:18:41,860
so when i applied external field these domains as a whole

317
00:18:41,880 --> 00:18:44,980
can flip

318
00:18:44,990 --> 00:18:47,480
inside ferromagnetic materials

319
00:18:47,530 --> 00:18:51,010
the magnetic field can be thousands of times stronger

320
00:18:51,030 --> 00:18:53,200
that it is in the vacuum field

321
00:18:53,240 --> 00:18:55,020
and we will see some examples of that

322
00:18:56,680 --> 00:18:59,040
if you remove the external field

323
00:18:59,090 --> 00:19:00,730
in the case of

324
00:19:00,780 --> 00:19:02,130
so magnetism

325
00:19:02,130 --> 00:19:04,810
you have again complete chaos of the dipoles

326
00:19:04,830 --> 00:19:10,230
that's not necessarily the case was ferromagnetism some of those domains may stay aligned in

327
00:19:10,230 --> 00:19:13,700
the direction that the external field force forcing them

328
00:19:13,740 --> 00:19:19,020
if you're very carefully remove the external field undoubtedly some domains will flip back because

329
00:19:19,020 --> 00:19:22,730
of the temperature there's always thermal agitation some may remain

330
00:19:23,910 --> 00:19:25,190
and therefore

331
00:19:25,210 --> 00:19:29,630
the material once it has been exposed to an external magnetic field may have become

332
00:19:29,640 --> 00:19:31,790
permanently magnetic

333
00:19:31,850 --> 00:19:35,440
and the only way you can remove the permanent magnetism could be to ban on

334
00:19:35,440 --> 00:19:36,720
it with a hammer

335
00:19:36,750 --> 00:19:38,150
and then of course these

336
00:19:38,170 --> 00:19:39,490
domains were them

337
00:19:39,520 --> 00:19:41,300
get very nervous and then they will

338
00:19:41,310 --> 00:19:44,370
randomized themselves or you can hear them up

339
00:19:44,370 --> 00:19:45,740
and then you can also

340
00:19:45,790 --> 00:19:46,890
undo the

341
00:19:46,890 --> 00:19:52,410
the orientation of the domains the remains themselves will remain but then they average out

342
00:19:52,420 --> 00:19:56,500
not to produce any permanent magnetic field

343
00:19:56,560 --> 00:19:59,930
so for the same reason the ferromagnetism

344
00:19:59,980 --> 00:20:04,650
is pulled towards the strong field in case that we have a nonuniform magnetic field

345
00:20:04,660 --> 00:20:09,350
ferromagnetism of course will also be pulled towards the strong field except in the case

346
00:20:09,350 --> 00:20:12,510
of ferromagnetism the forces with which

347
00:20:13,870 --> 00:20:16,370
the material is pulled towards the magnet

348
00:20:16,380 --> 00:20:18,880
wayne larger than in case of

349
00:20:18,920 --> 00:20:20,790
paramagnetic materials

350
00:20:20,810 --> 00:20:22,630
if i take a paper clip

351
00:20:22,660 --> 00:20:25,750
you can do it at on you can hang a paper clip on the south

352
00:20:25,750 --> 00:20:28,740
pole of your magnet or the north pole of a magnet

353
00:20:28,780 --> 00:20:31,820
we all have gotten magnets in your body kits so you can try it at

354
00:20:32,530 --> 00:20:34,480
baker paperclips along

355
00:20:34,490 --> 00:20:37,140
the magnets the amount on which side you hang it

356
00:20:37,140 --> 00:20:39,530
ferrimagnetic material is always

357
00:20:39,560 --> 00:20:40,690
full towards

358
00:20:40,690 --> 00:20:42,910
the strong field

359
00:20:42,920 --> 00:20:45,420
if you have a few of those paperclips on their

360
00:20:45,430 --> 00:20:49,530
thank you very carefully and slowly removed

361
00:20:49,550 --> 00:20:51,190
those with them is a hammer yet

362
00:20:51,230 --> 00:20:55,660
you may actually notice that after you removed and that the paper clips themselves have

363
00:20:57,500 --> 00:21:00,760
you can actually try to hang them on each other make little change

364
00:21:00,810 --> 00:21:03,900
but drop them on the floor a few times in the neck mechanism neck visible

365
00:21:03,900 --> 00:21:04,790
go away

366
00:21:04,830 --> 00:21:07,900
so what you have written is that is that some of those domains

367
00:21:07,920 --> 00:21:09,740
remained aligned

368
00:21:09,750 --> 00:21:12,990
due to new external field

369
00:21:13,040 --> 00:21:16,740
mister magnetism there's no way that you can hang power

370
00:21:16,750 --> 00:21:21,640
magnetic material under most circumstances on the mac

371
00:21:21,680 --> 00:21:25,130
there's one exception i was show exceptional even today

372
00:21:25,130 --> 00:21:32,250
on the central path and then a very simple barrier method for general conic optimization is

373
00:21:32,250 --> 00:21:37,050
like this you start close to the central path you pick an X and a T that is

374
00:21:37,050 --> 00:21:40,690
sufficiently close so the Newton decrement is bounded by some beta for example one

375
00:21:40,690 --> 00:21:50,630
over eight and then at each iteration you make one Newton step on X with a

376
00:21:50,630 --> 00:21:56,830
step size one and then you update T but very slowly so you could take the

377
00:21:56,860 --> 00:22:03,230
current T and you multiply it with a constant close to one so that's called the short step

378
00:22:03,230 --> 00:22:09,650
barrier method  because at each iteration you stay in the neighborhood of the central path

379
00:22:09,650 --> 00:22:15,330
you keep X  so the reason why you make these tiny updates in T is

380
00:22:15,330 --> 00:22:21,850
to keep X after this Newton update inside the region of quadratic convergence so you make small

381
00:22:21,850 --> 00:22:27,270
updates in X small enough to keep X in the region of quadratic convergence and

382
00:22:27,270 --> 00:22:32,130
then you after one Newton step you update T by a small amount so it's a

383
00:22:32,130 --> 00:22:38,110
very simple method it's not very fast in practice but it's a practical method

384
00:22:38,110 --> 00:22:43,610
it can be implemented but you can show that the worst-case complexity the number

385
00:22:43,610 --> 00:22:50,390
of Newton systems you have to apply is bounded by a function like this that's

386
00:22:50,390 --> 00:22:55,270
exactly the same as for linear programming except that the parameter M who chose the number

387
00:22:55,270 --> 00:23:00,970
of inequalities in the LP is replaced by the barrier parameter theta for the cone

388
00:23:00,970 --> 00:23:08,570
that we use here so would be M for a semi-definite problem with M by M

389
00:23:08,570 --> 00:23:15,130
Newton matrix inequality and so on right so it's a very simple method but it has

390
00:23:15,130 --> 00:23:21,390
a polynomial worst-case complexity and again this bound with the square root of theta is actually the best-known bound

391
00:23:21,390 --> 00:23:29,450
for these types of problems it's not a very fast method and it's easy to

392
00:23:29,450 --> 00:23:33,730
see why it's not very fast because you make these very small and slow updates

393
00:23:33,730 --> 00:23:39,170
in T in practice  of course you would like to update T much faster and

394
00:23:39,170 --> 00:23:42,670
of course there's no need in practice to stay very close to the central path

395
00:23:42,680 --> 00:23:45,810
because you're only interested in the end point of the central path the optimal

396
00:23:45,810 --> 00:23:52,390
point of the LP so you can probably get there much faster by

397
00:23:52,390 --> 00:23:57,990
making larger steps and not constraining yourself to this region of quadratic convergence so that's

398
00:23:57,990 --> 00:24:02,370
one way to do this is toik instead of these short step methods you use

399
00:24:02,370 --> 00:24:06,930
a predictor corrector method so the idea is again very similar you try to

400
00:24:06,930 --> 00:24:13,270
follow the central path but  using a different mechanism as in these short step methods

401
00:24:13,270 --> 00:24:20,330
in a predictor corrector method you take at each iteration the tangent to the central path

402
00:24:20,490 --> 00:24:25,630
or if X is not on the central path an approximate tangent to the central path

403
00:24:25,650 --> 00:24:30,340
that's the predictor in the predictor step so you compute a tangent to the central path

404
00:24:30,340 --> 00:24:37,430
or an approximate tangent then you make a step in that direction you select a new

405
00:24:37,430 --> 00:24:41,810
value of T based on the step you make in that direction and then you

406
00:24:41,810 --> 00:24:46,380
recenter with the new T and that's a corrector step so you do one or more centering steps

407
00:24:46,380 --> 00:24:52,730
with the new value of T and then again you take this predictor step

408
00:24:52,730 --> 00:24:59,230
so the difference is here that using this mechanism you can increase T adaptively if the

409
00:24:59,230 --> 00:25:04,610
central path is not very nonlinear you can make larger updates in T then using

410
00:25:04,610 --> 00:25:08,990
a fixed update of this kind via than the barrier method so in  practice this is

411
00:25:08,990 --> 00:25:17,090
much more efficient and you can actually show that for some choices of these

412
00:25:17,130 --> 00:25:24,170
mak this predictor and corrector steps you get the same worst-case complexity so that's the

413
00:25:24,180 --> 00:25:30,650
section on barrier methods so they're  primal methods that try to follow the central path

414
00:25:30,710 --> 00:25:39,450
and you can use them for conic LPs  with general non pil  nonpolyhedral convex cones

415
00:25:39,450 --> 00:25:48,030
as long as you have  a normal barrier function for the cone so the

416
00:25:48,070 --> 00:25:54,790
last section is on symmetric primal dual methods so the barrier methods for and again

417
00:25:54,790 --> 00:26:01,850
this work was motivated by methods for linear programming so the interior point methods that

418
00:26:01,850 --> 00:26:07,070
are used for linear programming in practice are actually not the simple barrier methods that

419
00:26:07,070 --> 00:26:12,890
I started the previous section with so in practice for example in commercial interior point

420
00:26:12,890 --> 00:26:20,510
solvers people use primal dual methods instead of a primal barrier method and

421
00:26:20,510 --> 00:26:26,270
they're symmetric methods so they treat a primal and a dual  symmetrically so people

422
00:26:26,270 --> 00:26:32,650
have tried to extend these very successful primal dual methods for linear programming to conic

423
00:26:32,650 --> 00:26:42,270
optimization and then it turns out that the most elegant extensions of these methods

424
00:26:42,270 --> 00:26:50,230
are restricted to certain types of convex cones called symmetric cones that we'll define and

425
00:26:50,230 --> 00:26:56,830
then for these types of cones we can extend this primal dual methods so what's the purpose

426
00:26:56,830 --> 00:27:01,930
of a primal or how do primal dual methods compare with barrier methods so first

427
00:27:01,930 --> 00:27:08,550
there are some important similarities so they're both based on solving the central path

428
00:27:08,550 --> 00:27:16,190
to find a solution and they follow the same central path just using a different method

429
00:27:16,190 --> 00:27:20,060
also it's very important that per iteration the linear algebra you need in a

430
00:27:20,060 --> 00:27:24,590
primal dual method is the same as for a barrier method so in a barrier

431
00:27:24,590 --> 00:27:29,990
method in each iteration you had to solve a Newton system that's  determined by the barrier

432
00:27:29,990 --> 00:27:37,430
function that you use so here also the s system you have to solve at each

433
00:27:37,430 --> 00:27:42,370
iteration will have the same form as in a barrier method so that's also important so

434
00:27:42,370 --> 00:27:47,100
the cost per reiteration will be the same they follow the same central path but they

435
00:27:47,100 --> 00:27:50,610
try to do it in a more robust and faster way so you have you can

436
00:27:50,610 --> 00:27:57,530
arrive at a solution with fewer iterations so the advantages are that they're faster than

437
00:27:57,530 --> 00:28:01,300
we can easily solve white just checking

438
00:28:01,380 --> 00:28:04,900
well down constraint would be if it's not there will i just look at where

439
00:28:04,920 --> 00:28:07,800
the boundaries and i get the solution

440
00:28:07,850 --> 00:28:09,760
in general it's not quite that trivial

441
00:28:09,800 --> 00:28:15,620
but the nice thing is that convex optimization problems with convex constraints have a unique

442
00:28:15,630 --> 00:28:17,540
minimum value

443
00:28:17,570 --> 00:28:20,800
they don't necessarily have to have a unique solution

444
00:28:20,880 --> 00:28:23,370
but they have unique know that

445
00:28:23,380 --> 00:28:27,440
so why do they not necessarily have to have a unique

446
00:28:28,390 --> 00:28:30,930
well think of something that looks like a troll

447
00:28:30,940 --> 00:28:32,940
it goes down

448
00:28:32,950 --> 00:28:34,800
latin and then goes back up again

449
00:28:34,810 --> 00:28:36,560
there is a convex function

450
00:28:36,600 --> 00:28:41,130
however this convex function will have its minimizer anywhere on that straight line on the

451
00:28:42,840 --> 00:28:46,100
however it will have unique minimum value

452
00:28:46,500 --> 00:28:51,640
so what you might want to do is homework or

453
00:28:51,650 --> 00:28:54,000
tomorrow the beach whatever

454
00:28:54,010 --> 00:29:00,540
think about why convex function has unique number

455
00:29:00,580 --> 00:29:03,340
look at the proof states two or three line

456
00:29:03,400 --> 00:29:06,060
you will need the definition of convexity for that

457
00:29:06,100 --> 00:29:09,260
i assume that everybody here knows what is

458
00:29:09,270 --> 00:29:12,270
basically take the graph take two points

459
00:29:12,310 --> 00:29:14,090
but the straight line through it

460
00:29:14,140 --> 00:29:17,010
that straight line has to be above the function values

461
00:29:17,710 --> 00:29:19,570
of the function itself

462
00:29:23,090 --> 00:29:27,780
and the constraints obviously also convex is that they the nice thing if you have

463
00:29:27,780 --> 00:29:29,120
a convex set

464
00:29:29,190 --> 00:29:33,340
they take intersections between several convex it's the result is going to be a convex

465
00:29:34,930 --> 00:29:37,160
again something tomorrow for the beach

466
00:29:37,170 --> 00:29:40,420
you can do really nice drawings in the sand and this is going to be

467
00:29:40,420 --> 00:29:42,760
really useful if you want to work at

468
00:29:43,850 --> 00:29:46,400
yes try

469
00:29:50,580 --> 00:29:53,320
so what you need is that the constraints are convex

470
00:29:53,410 --> 00:29:55,880
and that the objective function is convex

471
00:29:55,900 --> 00:29:59,890
if one of the two doesn't happen you're in big trouble

472
00:29:59,900 --> 00:30:03,060
he this simple example

473
00:30:03,120 --> 00:30:06,090
so for instance if i were told to

474
00:30:06,150 --> 00:30:07,960
maximize this function

475
00:30:08,010 --> 00:30:09,910
i'll get real problems

476
00:30:09,920 --> 00:30:14,130
so let's assume this were a problem cannot quite we said that well but let's

477
00:30:14,130 --> 00:30:15,890
say i want to

478
00:30:15,900 --> 00:30:20,560
minimize the concave function rather than the convex one you can see that

479
00:30:20,570 --> 00:30:24,050
the minimum is going to be obtained at any of one of those four to

480
00:30:26,040 --> 00:30:30,040
so in general this type of probably NP hard

481
00:30:30,960 --> 00:30:36,770
little difference if i just change the sign

482
00:30:37,490 --> 00:30:39,350
so how do you solve

483
00:30:39,430 --> 00:30:43,430
we've got the objective function one half the square got this constraint

484
00:30:43,530 --> 00:30:48,340
we know that those constraints see i have to be listed here

485
00:30:48,420 --> 00:30:50,790
so now a recipe

486
00:30:50,800 --> 00:30:55,900
i cannot go into detail to prove why this recipe will actually do something useful

487
00:30:55,910 --> 00:31:01,290
but the what usually do is if you have a constrained optimization problem where

488
00:31:02,060 --> 00:31:06,380
vectors can be nonparametric on dimensional whatever it's a good idea to switch into the

489
00:31:07,300 --> 00:31:09,800
and then optimize the u

490
00:31:09,840 --> 00:31:11,050
and again

491
00:31:11,060 --> 00:31:12,390
this has been

492
00:31:12,400 --> 00:31:13,190
i would cite

493
00:31:13,200 --> 00:31:17,940
the key thing for the success of support vector machines that people solutions to dual

494
00:31:17,990 --> 00:31:19,160
the trouble is

495
00:31:19,190 --> 00:31:22,870
that people pretty much forgot that you could actually is going to solve this problem

496
00:31:22,870 --> 00:31:25,010
and the problems

497
00:31:25,040 --> 00:31:29,140
and so only over the past four to five years people have realized there are

498
00:31:29,140 --> 00:31:32,320
very efficient algorithm to solve these problems in the primal

499
00:31:32,380 --> 00:31:35,850
and that's pretty much what the state of the art in optimisation is now

500
00:31:36,060 --> 00:31:37,580
so things go

501
00:31:37,630 --> 00:31:41,430
back from one face to the other and sometimes notation and tricks

502
00:31:41,470 --> 00:31:43,990
are they useful to make progress

503
00:31:45,260 --> 00:31:48,570
if you see is that in iteration we think well why is he doing this

504
00:31:48,570 --> 00:31:50,130
can they do something else

505
00:31:50,200 --> 00:31:52,340
you probably on to something good

506
00:31:52,350 --> 00:31:53,790
either that or you

507
00:31:53,800 --> 00:31:57,650
i realize it's a completely daft idea not to do it but

508
00:31:57,660 --> 00:32:01,660
they have to ask why certain steps happen

509
00:32:01,670 --> 00:32:05,180
this is the only way high feel really make progress

510
00:32:06,560 --> 00:32:07,950
his prescription

511
00:32:08,010 --> 00:32:10,090
you take the primal objective function

512
00:32:10,150 --> 00:32:11,750
and you have to it

513
00:32:11,760 --> 00:32:14,960
lagrange multipliers time constraints

514
00:32:15,020 --> 00:32:18,450
so these are like virtual forces trying to ensure that this

515
00:32:18,520 --> 00:32:21,520
remains within its constraints

516
00:32:21,570 --> 00:32:23,460
and then

517
00:32:23,510 --> 00:32:25,170
you get the function that is

518
00:32:25,190 --> 00:32:26,740
a function of w

519
00:32:26,770 --> 00:32:27,800
and b

520
00:32:27,850 --> 00:32:30,750
and those lagrange multipliers

521
00:32:30,790 --> 00:32:34,630
and what you can prove is that this function eleven

522
00:32:34,680 --> 00:32:38,780
i will have a saddle point at the optimal solution

523
00:32:38,790 --> 00:32:40,100
so in other words

524
00:32:40,150 --> 00:32:44,300
it it will have a minimum with respect to w and b

525
00:32:44,310 --> 00:32:45,610
for optimal out

526
00:32:45,730 --> 00:32:52,460
will have a maximum with respect to alpha for optimal w and b

527
00:32:53,240 --> 00:32:54,480
what this means is

528
00:32:54,490 --> 00:32:57,290
if there is a saddle point where we can just go and take derivatives of

529
00:32:57,290 --> 00:33:00,030
l with respect to w and b

530
00:33:00,070 --> 00:33:01,430
so for that

531
00:33:01,430 --> 00:33:04,300
get rid of w b in the equation

532
00:33:04,360 --> 00:33:08,100
and just get an optimisation problem in the alps

533
00:33:09,040 --> 00:33:12,250
actually know this kind of stuff you've done it before it's just that you never

534
00:33:12,250 --> 00:33:13,910
thought of it this way

535
00:33:14,010 --> 00:33:17,950
let's say i want to minimize a function f of x without constraints

536
00:33:18,070 --> 00:33:22,580
one way of doing this is i could just go and computer relative if primal

537
00:33:22,580 --> 00:33:26,650
i don't understand this problem without any graphs and formal

538
00:33:27,470 --> 00:33:29,040
in equation

539
00:33:29,120 --> 00:33:32,160
so this is this is

540
00:33:32,200 --> 00:33:36,170
and how long and it's very very simple model

541
00:33:36,200 --> 00:33:41,080
but it shows what we usually see in in the cluster or announcing plans for

542
00:33:41,080 --> 00:33:42,610
biological computer

543
00:33:42,750 --> 00:33:45,190
the idea is that we can

544
00:33:45,200 --> 00:33:48,250
divide our program

545
00:33:48,260 --> 00:33:50,120
into two parts

546
00:33:50,160 --> 00:33:55,360
one is what that can be implemented completely in parallel

547
00:33:55,360 --> 00:33:56,950
and the

548
00:33:57,080 --> 00:34:01,030
is some part of the problem that can be paralyzed

549
00:34:01,050 --> 00:34:06,320
because it means something in addition to that the apply for example what is when

550
00:34:07,790 --> 00:34:11,620
and then we combine them into one in the extraction of what can be completely

551
00:34:11,620 --> 00:34:16,080
parallel combine into one index need synchronization without

552
00:34:16,080 --> 00:34:18,780
part of the of the system

553
00:34:19,570 --> 00:34:24,370
he is percentage of our program that can be done in parallel

554
00:34:24,410 --> 00:34:28,370
and is number of unique forest as

555
00:34:28,370 --> 00:34:33,290
you see what's going on and actually the graph this is extremely naive model that

556
00:34:33,290 --> 00:34:37,820
it doesn't take into account is not scalable resources such

557
00:34:37,830 --> 00:34:43,040
but if you look to israel classes you you can see our real parallel computation

558
00:34:43,120 --> 00:34:47,580
you can see that usually get something like this which is very close to the

559
00:34:50,230 --> 00:34:52,280
so is simple models

560
00:34:54,240 --> 00:34:59,830
and you can do we're after estimation of your program and using this equation

561
00:35:00,830 --> 00:35:02,550
rapidly but

562
00:35:02,630 --> 00:35:04,250
not so well

563
00:35:04,320 --> 00:35:06,410
predict how the system works

564
00:35:06,470 --> 00:35:09,610
and if you don't want to do this but want only

565
00:35:09,610 --> 00:35:12,010
to get some intuition about

566
00:35:12,030 --> 00:35:18,370
performance of your of the system in the class situation when you edit resources using

567
00:35:18,380 --> 00:35:21,360
cortisone to scaling

568
00:35:21,480 --> 00:35:22,630
you can

569
00:35:22,650 --> 00:35:25,300
three things that would be something like this

570
00:35:25,330 --> 00:35:27,290
so at the beginning

571
00:35:27,290 --> 00:35:30,970
usually increasing you linear

572
00:35:32,650 --> 00:35:34,010
pretty good

573
00:35:34,050 --> 00:35:39,620
then you see that it's some point when you can get an advantage of additional

574
00:35:39,670 --> 00:35:42,290
additional resources that to get

575
00:35:43,740 --> 00:35:49,160
the main idea of all development and what is the system architect should achieve is

576
00:35:49,160 --> 00:35:50,740
try to move

577
00:35:50,750 --> 00:35:53,450
these parties as much as possible

578
00:35:53,500 --> 00:35:58,330
and doing the and you can do this on only by reducing the number of

579
00:35:58,330 --> 00:36:00,870
not quite a lot of what

580
00:36:00,880 --> 00:36:05,860
he using part of your programs that can be paralyzed

581
00:36:06,500 --> 00:36:09,200
well it's very simple model

582
00:36:09,210 --> 00:36:15,570
and to be able to show you that it's the really important it dissertations

583
00:36:15,620 --> 00:36:18,860
i don't remember from what company his

584
00:36:18,910 --> 00:36:21,110
but it's attention from from

585
00:36:21,110 --> 00:36:23,820
current media media about

586
00:36:23,920 --> 00:36:28,870
well execution and the idea is that

587
00:36:28,920 --> 00:36:30,820
as i said at the beginning

588
00:36:30,820 --> 00:36:35,340
now we can to find a computer without the number of CPU

589
00:36:35,360 --> 00:36:39,550
and it's hard to find information retrieval task that

590
00:36:39,610 --> 00:36:42,360
actually more than one ball

591
00:36:42,380 --> 00:36:45,750
usually even if you are doing

592
00:36:46,470 --> 00:36:50,990
document processing for company so knowledge going to number of

593
00:36:51,630 --> 00:36:53,690
the program is unavoidable

594
00:36:53,700 --> 00:36:55,960
therefore for every election

595
00:36:56,000 --> 00:37:04,320
later then we'll talk about other algorithms to analyse also parallel versions of this

596
00:37:04,370 --> 00:37:08,500
and talking about parallel architectures online

597
00:37:08,540 --> 00:37:11,250
so and there are two approaches

598
00:37:11,300 --> 00:37:16,960
well architectures you have smaller architectural one computer number of CPU

599
00:37:17,010 --> 00:37:22,330
and we can and we have well architecture cluster of computers because they can implement

600
00:37:22,330 --> 00:37:26,870
in parallel other tasks they're doing other tasks in parallel

601
00:37:26,880 --> 00:37:31,490
in real life that well

602
00:37:31,550 --> 00:37:34,740
on one block when we are a very low one but

603
00:37:34,790 --> 00:37:39,590
it depends on the operating system usually operation system takes care

604
00:37:39,610 --> 00:37:41,150
about this

605
00:37:41,160 --> 00:37:46,250
you need only specify programming language however going to do it

606
00:37:46,290 --> 00:37:51,820
and in most of the modern persian system programming languages there are two approach

607
00:37:51,830 --> 00:37:57,290
the right of the red and processed the difference is that threat is for elocution

608
00:37:57,290 --> 00:37:58,530
in one program

609
00:37:58,550 --> 00:38:00,910
processes is different from

610
00:38:00,950 --> 00:38:04,630
well usually the rest broke it depends

611
00:38:04,650 --> 00:38:11,550
processes and was safe because they usually use installation of memory and when you're process

612
00:38:15,510 --> 00:38:16,820
that the q

613
00:38:16,820 --> 00:38:18,820
all other product

614
00:38:18,830 --> 00:38:22,950
but there is is not true because they all share the same memory usually

615
00:38:22,960 --> 00:38:24,840
problem is one

616
00:38:24,860 --> 00:38:27,650
he was all the threats of this problem

617
00:38:27,830 --> 00:38:33,410
it's a very short introduction when creating this when creating program we always have the

618
00:38:34,240 --> 00:38:35,300
they can create

619
00:38:35,330 --> 00:38:41,970
that program the doing everything we you have control about everything and everything is threatened

620
00:38:42,000 --> 00:38:43,030
it's good

621
00:38:43,050 --> 00:38:48,000
the disadvantage if if something goes wrong all this implication does

622
00:38:48,800 --> 00:38:51,300
or you can create a number of problems

623
00:38:51,330 --> 00:38:54,920
in real life he is doing a combination of this stuff

624
00:38:54,970 --> 00:38:57,000
if you're doing here is usually

625
00:38:57,160 --> 00:38:59,260
the and because

626
00:38:59,280 --> 00:39:05,750
for charge usually only read the memory and it's much less probable that you corrupted

627
00:39:05,750 --> 00:39:10,660
and q the process if you're doing in the studio creating a lot of indexes

628
00:39:10,660 --> 00:39:15,990
it's usually a number of threats because this memory separation can be useful for you

629
00:39:15,990 --> 00:39:19,900
in this case and you need to merge in any case result from different boxes

630
00:39:19,900 --> 00:39:22,500
and even

631
00:39:24,380 --> 00:39:30,030
the so from this point of view of every everything is pretty simple and one

632
00:39:30,030 --> 00:39:33,370
books and there is a lot of literature about it i even

633
00:39:33,370 --> 00:39:34,450
but you don't know

634
00:39:34,460 --> 00:39:38,830
what the what right for you if you have very little data and you have

635
00:39:38,830 --> 00:39:41,840
a very vague prior assumptions then

636
00:39:41,900 --> 00:39:44,190
you really don't know

637
00:39:44,210 --> 00:39:47,130
what the outcome is going to be that and that would be reflected

638
00:39:47,140 --> 00:39:49,900
in your here your posterior quite wide

639
00:39:49,910 --> 00:39:51,980
there was a it would

640
00:39:51,990 --> 00:39:54,310
it would have time you know close to fifty fifty

641
00:39:54,330 --> 00:39:58,880
the kind of thing that would be the right answer in that case

642
00:39:58,890 --> 00:40:00,300
that well

643
00:40:00,330 --> 00:40:02,200
in this case you don't even know

644
00:40:03,630 --> 00:40:05,590
the wrong answer is to try to

645
00:40:05,600 --> 00:40:08,740
he invented some procedure to give you a definite

646
00:40:10,330 --> 00:40:15,480
but there's something really wrong about that if you just get the answer

647
00:40:15,480 --> 00:40:18,520
the data doesn't really contain any information about that

648
00:40:18,570 --> 00:40:20,530
that was the top of the page

649
00:40:21,740 --> 00:40:23,680
you can you can solve it

650
00:40:23,910 --> 00:40:27,510
by doing that is statistics

651
00:41:19,570 --> 00:41:29,720
so the question relates to the the the fact whether

652
00:41:29,760 --> 00:41:34,160
even if you have a fairly strong prior doesn't in the limit of large amount

653
00:41:34,160 --> 00:41:35,190
of data

654
00:41:35,190 --> 00:41:37,850
so you actually get back the maximum likelihood estimate

655
00:41:37,910 --> 00:41:42,760
and it depends on exactly what the in some circumstances you do get convergence to

656
00:41:42,770 --> 00:41:46,510
the maximum likelihood estimate and in some circumstances you don't

657
00:41:46,530 --> 00:41:49,400
and depends bit

658
00:41:49,410 --> 00:41:52,740
i think it depends on the on the particulars of how the prior up and

659
00:41:52,740 --> 00:41:56,960
what the model

660
00:41:57,060 --> 00:42:10,430
it depends on a lot of detail so

661
00:42:11,040 --> 00:42:23,200
so there there are many simple class the model for example see that you actually

662
00:42:23,200 --> 00:42:25,780
get convergence the maximum likelihood estimate

663
00:42:25,840 --> 00:42:27,080
so let me

664
00:42:27,100 --> 00:42:29,240
let me know

665
00:42:29,260 --> 00:42:32,990
kind that that's postpone the

666
00:42:33,000 --> 00:42:35,830
discussion about how to get the prior

667
00:42:35,840 --> 00:42:38,510
until tomorrow because tomorrow we can discuss the model

668
00:42:38,530 --> 00:42:42,730
which is a has a lot more interesting properties we can actually specify

669
00:42:42,730 --> 00:42:47,500
things that make a lot more intuitive than you're only looking at

670
00:42:47,560 --> 00:42:51,380
at that controls or if you're looking at

671
00:42:51,380 --> 00:42:57,480
so for example like this which is removed from the actual natural context then

672
00:42:57,530 --> 00:43:01,550
it's all about it to worry about this kind of the problem but i promise

673
00:43:02,190 --> 00:43:06,990
that talk you beg me to stop talking about prior to tomorrow

674
00:43:29,050 --> 00:43:42,190
so the question is

675
00:43:42,810 --> 00:43:48,890
in even even in classical physics you're you're you're basing your modelling on assumptions about

676
00:43:48,990 --> 00:43:52,030
how not about the prior distribution because you don't have a prior distribution but now

677
00:43:52,050 --> 00:43:57,040
about the likelihood function that's entirely true for two sets of assumptions that coming here

678
00:43:57,100 --> 00:43:58,780
assumptions about the prior

679
00:43:58,790 --> 00:44:02,570
and no assumptions about the likelihood and this is also some somehow

680
00:44:02,630 --> 00:44:05,240
tone down a bit often because

681
00:44:05,240 --> 00:44:07,420
some people think

682
00:44:07,470 --> 00:44:12,710
classical statistics is being closer to an objective way of treating thing but of course

683
00:44:12,790 --> 00:44:13,950
only objective

684
00:44:13,960 --> 00:44:15,400
up to the point of

685
00:44:15,450 --> 00:44:18,970
well you are agree on what the likelihood function as the people

686
00:44:18,980 --> 00:44:21,480
focus a lot on arguing about what the prime

687
00:44:21,600 --> 00:44:24,740
but equally important you should argue about what like

688
00:44:24,820 --> 00:44:26,250
and things are not

689
00:44:26,270 --> 00:44:31,500
objective you can if you can agree on what i like about

690
00:44:31,510 --> 00:44:35,090
that's very very good

691
00:44:39,050 --> 00:44:42,200
i just spent the last five minutes of going through

692
00:44:42,210 --> 00:44:48,160
a couple of couple of words here i think are important to you realize that

693
00:44:48,160 --> 00:44:54,310
people use to mean different things which is which is very unfortunate

694
00:44:57,070 --> 00:45:05,140
OK so we a from the objective and on prior information but actually

695
00:45:05,140 --> 00:45:07,950
you know if just point out

696
00:45:07,960 --> 00:45:12,260
you can also think of classical getting objective because it depends on

697
00:45:12,320 --> 00:45:13,680
on what you're

698
00:45:13,710 --> 00:45:19,050
what you're assumptions are about the likelihood function how right

699
00:45:19,120 --> 00:45:20,430
what i like about

700
00:45:20,470 --> 00:45:23,010
right down to the conflict thinking things happen

701
00:45:23,070 --> 00:45:27,170
it's pretty clear what the likely to be of more complicated example

702
00:45:27,200 --> 00:45:32,810
it's not not not clear what like it

703
00:45:32,870 --> 00:45:39,730
something i that i only only in over here but i'll talk about more letters

704
00:45:39,740 --> 00:45:43,480
bayesian inference about the likelihood principle like

705
00:45:43,490 --> 00:45:44,870
but that

706
00:45:44,870 --> 00:45:49,560
the computer and only on the likelihood of of of the observation

707
00:45:49,620 --> 00:45:52,820
and the assumption that you're making it shouldn't be it should depend on

708
00:45:53,020 --> 00:45:56,490
on in particular should depend on observations have made

709
00:45:57,220 --> 00:45:59,210
possible outcomes

710
00:45:59,220 --> 00:46:00,700
that that you did not serve

711
00:46:00,710 --> 00:46:01,690
but that

712
00:46:01,940 --> 00:46:03,520
the result is not

713
00:46:03,580 --> 00:46:06,710
not allowed to depend on that and there's something wrong with analysis

714
00:46:06,770 --> 00:46:08,280
it depends on

715
00:46:08,330 --> 00:46:10,910
in a nodes in the classical

716
00:46:11,050 --> 00:46:15,780
testing your making statements about serving something or something more extreme you you making statements

717
00:46:16,700 --> 00:46:22,660
about other datasets as well like that that kind of thing doesn't obey the like

718
00:46:22,670 --> 00:46:25,240
i the more specific about later on

719
00:46:25,830 --> 00:46:30,200
the result of the debate from the posterior distribution

720
00:46:30,250 --> 00:46:35,510
and once you compute the posterior distribution then when you finished third mission tells you

721
00:46:35,510 --> 00:46:37,710
what is your new

722
00:46:37,760 --> 00:46:39,710
they of knowledge about

723
00:46:39,710 --> 00:46:42,520
you probably about how the world

724
00:46:44,170 --> 00:46:48,480
o thing i didn't go into in detail that how to use this posterior distribution

725
00:46:48,630 --> 00:46:49,720
we talk about

726
00:46:49,890 --> 00:46:53,630
a very simple example of how to make predictions about upcoming

727
00:46:53,640 --> 00:46:58,210
but to make decisions you can minimize the

728
00:46:58,210 --> 00:47:03,370
the expected loss the new term coming in here which is the loss function

729
00:47:03,380 --> 00:47:05,200
the loss function tells you

730
00:47:06,680 --> 00:47:08,550
how expensive is it

731
00:47:08,570 --> 00:47:13,690
to make a particular guess about the state of the art of various when something

732
00:47:13,690 --> 00:47:15,080
else happens to be true

733
00:47:15,090 --> 00:47:21,010
so you think in things like thing financial tries action but if you if you

734
00:47:21,010 --> 00:47:23,320
lost would be measured in

735
00:47:24,810 --> 00:47:27,230
in a medical setting

736
00:47:27,230 --> 00:47:29,170
last measured in terms of

737
00:47:29,190 --> 00:47:35,980
however people are feeling feeling or or the risk of them dying things like that

738
00:47:36,040 --> 00:47:40,790
but notice that the loss function is something that doesn't really enter into the into

739
00:47:40,790 --> 00:47:45,430
the now compute the posterior distributions and then you sort of doing this

740
00:47:45,470 --> 00:47:50,350
and then after you've done compute posterior and somebody can tell you know this is

741
00:47:50,350 --> 00:47:51,410
my loss function

742
00:47:51,460 --> 00:47:53,080
i actually care about you

743
00:47:53,230 --> 00:47:56,670
OK that's what you care about then you make

744
00:47:56,680 --> 00:47:58,100
this and the decision

745
00:47:59,640 --> 00:48:02,710
you're expected loss

746
00:48:02,720 --> 00:48:06,530
so this is the

747
00:48:06,650 --> 00:48:10,100
come back to the to the notion of the of cost function

748
00:48:10,100 --> 00:48:16,020
what i'm going to describe today and in the rademacher also it's slightly and satisfy

749
00:48:16,020 --> 00:48:21,750
all mentions something more about that a little later the results we had yesterday typically

750
00:48:21,750 --> 00:48:26,180
didn't have that square the square root of course makes the bounds much less tight

751
00:48:26,180 --> 00:48:30,410
you know if you had abandoned one hundred you take the square root and it's

752
00:48:30,410 --> 00:48:34,140
about one in ten which is seen point one inches

753
00:48:35,100 --> 00:48:41,750
but the flexibility and the of course when you're far away from you know rather

754
00:48:41,750 --> 00:48:44,350
larger errors then the the

755
00:48:44,370 --> 00:48:47,810
and the square root is not so significant

756
00:48:47,990 --> 00:48:53,350
OK so my first application of these ideas and this is like a warm-up

757
00:48:53,560 --> 00:48:55,220
rademacher complexity

758
00:48:55,250 --> 00:49:00,290
is this something referred to stability bounds

759
00:49:02,100 --> 00:49:06,220
but that night and i just recently published we stability to just mean sort of

760
00:49:06,220 --> 00:49:13,020
reliable estimations so it's slightly confusing with this user stability which is the technical term

761
00:49:13,040 --> 00:49:19,770
introduced by olivia was going on a PCFG and the

762
00:49:19,770 --> 00:49:22,970
used introduce very neat idea

763
00:49:22,970 --> 00:49:28,080
and it allows the fairly direct application of these concentration results so i think it's

764
00:49:28,080 --> 00:49:31,830
a nice way to start sort of lead into using the idea is to see

765
00:49:31,830 --> 00:49:34,100
how they used in

766
00:49:34,200 --> 00:49:39,790
example and then i will tackle the rademacher case which is which is more general

767
00:49:40,540 --> 00:49:47,180
so they introduced this idea of a uniformly stable algorithm so it's interesting that actually

768
00:49:47,180 --> 00:49:52,330
trying to bound the performance of a particular algorithm most generalisation mounds once we looked

769
00:49:52,330 --> 00:49:56,370
at it yesterday and the rademacher bounds are going to be

770
00:49:56,390 --> 00:50:01,120
so for apply for any function so sort of it doesn't matter what the algorithm

771
00:50:01,120 --> 00:50:06,680
does provide to get a good result then this will hold this is actually looking

772
00:50:06,680 --> 00:50:08,470
at the particular algorithm

773
00:50:08,680 --> 00:50:15,470
and it says is uniformly stable with it's with respect to a loss function l

774
00:50:15,470 --> 00:50:18,140
if this holds for all

775
00:50:18,180 --> 00:50:19,620
as for all

776
00:50:19,640 --> 00:50:21,560
training samples

777
00:50:21,580 --> 00:50:25,100
we apply the algorithm to the sample

778
00:50:25,140 --> 00:50:29,890
and we apply it to the sample with the ice come elements

779
00:50:29,930 --> 00:50:33,080
thrown away back slash i means that

780
00:50:33,120 --> 00:50:38,040
we remove the i th element so it's a bit like doing leave one out

781
00:50:38,310 --> 00:50:41,080
that's the idea here and

782
00:50:41,100 --> 00:50:45,330
what we have to cheque is that if we apply

783
00:50:45,390 --> 00:50:50,270
the result of that function to new example

784
00:50:50,290 --> 00:50:57,750
the loss associated with that and the loss associated with this when we apply after

785
00:50:57,750 --> 00:51:00,680
removing that example is

786
00:51:00,680 --> 00:51:01,950
for all

787
00:51:01,950 --> 00:51:05,160
test examples less or equal to beta

788
00:51:05,200 --> 00:51:10,270
so it's a fairly strong requirement but surprisingly many

789
00:51:12,000 --> 00:51:15,750
and even the SVM in the case of the one norm SVM you can get

790
00:51:15,810 --> 00:51:22,410
bound on this peter i'll show you later so that this is actually you know

791
00:51:22,430 --> 00:51:28,750
remarkable that is so applicable it does make sense in some respects and that you

792
00:51:28,750 --> 00:51:33,970
know you wouldn't want things to change too much just by removing one example

793
00:51:36,140 --> 00:51:39,580
of course if you just take a hard margin SVM this doesn't apply you can

794
00:51:39,580 --> 00:51:40,580
have quite

795
00:51:40,580 --> 00:51:42,390
strong changes in

796
00:51:42,430 --> 00:51:44,410
the output OK

797
00:51:44,430 --> 00:51:49,930
so this is this is the assumption i'm i'm simplifying down what they've done they

798
00:51:49,930 --> 00:51:54,580
have actually done quite a number of different variations on this definition in the paper

799
00:51:55,250 --> 00:52:00,720
if you're interested you could follow and read that i think i put references to

800
00:52:00,720 --> 00:52:05,250
all of these papers have made use of in the in the back and many

801
00:52:05,250 --> 00:52:06,930
others as well

802
00:52:06,950 --> 00:52:07,890
OK so

803
00:52:07,890 --> 00:52:10,200
how do we approach this using this

804
00:52:10,220 --> 00:52:13,430
concentration inequality strategy

805
00:52:15,580 --> 00:52:20,450
what we're looking at is trying to estimate the difference between the

806
00:52:22,910 --> 00:52:24,750
the expected loss

807
00:52:24,770 --> 00:52:26,040
of the

808
00:52:28,640 --> 00:52:32,060
this is a this is the function returned by the algorithm

809
00:52:32,080 --> 00:52:34,580
this is the loss and its expectation

810
00:52:34,600 --> 00:52:35,490
under the

811
00:52:35,490 --> 00:52:37,140
underlying distribution

812
00:52:37,330 --> 00:52:39,540
and this is its empirical loss

813
00:52:39,560 --> 00:52:41,580
this is the loss on the training set

814
00:52:41,580 --> 00:52:43,910
i'm using this notation

815
00:52:43,970 --> 00:52:49,370
to denote the expectation on the training set so it just means the average of

816
00:52:49,370 --> 00:52:54,830
the training set losses if you see that from now on it always means that

817
00:52:54,870 --> 00:52:58,970
k case just work out the loss on the training set and then take the

818
00:52:58,970 --> 00:53:00,620
average of that

819
00:53:01,810 --> 00:53:06,910
this is the sort of what we can tell from the training set this is

820
00:53:06,910 --> 00:53:08,520
what we want to know about

821
00:53:08,540 --> 00:53:12,140
you know this is the true performance on on unseen data

822
00:53:12,180 --> 00:53:15,680
and so the idea that is the difference between those two

823
00:53:16,370 --> 00:53:20,060
so we're going to go for it you know just try and

824
00:53:20,100 --> 00:53:25,950
get bound on that and we're going to diamonds theorem to this DNS quantity

825
00:53:25,950 --> 00:53:30,370
but we're going to look first at how much this can change as a result

826
00:53:30,370 --> 00:53:32,060
of the substitution

827
00:53:32,100 --> 00:53:33,830
one example

828
00:53:33,850 --> 00:53:37,340
i'm going to look at how much this can change from the substitution of a

829
00:53:37,340 --> 00:53:38,500
single example

830
00:53:38,540 --> 00:53:39,370
so this

831
00:53:39,450 --> 00:53:43,000
we're going to first street this as

832
00:53:43,020 --> 00:53:45,410
so we're treating this is i f of s

833
00:53:45,430 --> 00:53:46,490
right in

834
00:53:46,500 --> 00:53:47,850
the diamonds there

835
00:53:47,850 --> 00:53:51,200
but we're going to see how much this can change first and then much this

836
00:53:51,200 --> 00:53:54,850
can change and then add the two together for the total amount of change so

837
00:53:54,850 --> 00:53:56,850
it is of course one

838
00:53:56,910 --> 00:54:00,970
maybe i'm jumping too quickly but if you want to apply madame is theorem the

839
00:54:00,970 --> 00:54:04,040
critical thing is to get about on this quantity here

840
00:54:04,040 --> 00:54:10,350
one year then you see in the position that variations for the other know

841
00:54:10,440 --> 00:54:16,710
and what we expected to have this two thousand ten that the genome sequencing only

842
00:54:16,710 --> 00:54:18,920
like about how you

843
00:54:18,930 --> 00:54:25,020
so again how is expected to live get it but that would give us a

844
00:54:25,020 --> 00:54:25,940
lot of

845
00:54:26,000 --> 00:54:30,190
i mean much much more than just imagine of the human genome and you can

846
00:54:30,360 --> 00:54:33,430
everybody can get so

847
00:54:33,430 --> 00:54:37,350
so this is like a big basis and the support on one CD with something

848
00:54:37,370 --> 00:54:42,160
and then you can fill up hard to find

849
00:54:44,150 --> 00:54:47,020
OK so typically typically b

850
00:54:47,040 --> 00:54:52,550
popular with the population data recorded deviation from the the reference

851
00:54:52,570 --> 00:54:55,720
so one typical

852
00:54:55,750 --> 00:55:00,820
we will present a single nucleotide polymorphism where only one i j

853
00:55:03,490 --> 00:55:07,780
but also decisions and deviation

854
00:55:07,830 --> 00:55:13,360
so one thing which complicates things is that you have two copies of the DNA

855
00:55:13,360 --> 00:55:14,630
and you don't necessarily know

856
00:55:14,680 --> 00:55:16,340
which copy that one

857
00:55:16,390 --> 00:55:18,180
so this

858
00:55:18,190 --> 00:55:20,780
i mean it's just making the analysis book

859
00:55:20,800 --> 00:55:24,610
when sequencing and you don't know which is which copy of it

860
00:55:24,720 --> 00:55:29,010
but maybe one important part

861
00:55:29,040 --> 00:55:35,890
please biologically what you but so what is the identification of haplotypes what is a

862
00:55:36,730 --> 00:55:41,610
so what you see here is that you have different phenotypes different genomic sequence

863
00:55:41,610 --> 00:55:45,370
and you see the variation here with the first be

864
00:55:45,380 --> 00:55:47,470
always t here

865
00:55:47,520 --> 00:55:49,770
the second we have always see

866
00:55:49,780 --> 00:55:54,040
the next one you see the same kind of change

867
00:55:54,260 --> 00:55:56,080
so you see variations

868
00:55:56,200 --> 00:55:57,600
the same

869
00:55:57,620 --> 00:56:00,190
and it usually happens when these

870
00:56:00,220 --> 00:56:02,750
polymorphism next to each other

871
00:56:02,760 --> 00:56:09,300
OK because it's unlikely that two we want this to happen or are separated by

872
00:56:12,500 --> 00:56:19,200
so so mutation and there other what like the one here

873
00:56:19,260 --> 00:56:21,470
then they show different

874
00:56:22,090 --> 00:56:27,930
and whenever she several what business here together is called the haplotype

875
00:56:27,990 --> 00:56:32,190
the thing about this haplotype is you only need to identify one of these twenty

876
00:56:32,190 --> 00:56:36,000
one in order to understand the full structure

877
00:56:37,190 --> 00:56:39,170
the haplotypes

878
00:56:39,180 --> 00:56:40,940
OK in you're

879
00:56:40,960 --> 00:56:43,380
in expression

880
00:56:43,380 --> 00:56:47,100
so if you can

881
00:56:52,880 --> 00:56:57,230
so you know the synapse are linked so usually appear together

882
00:56:57,260 --> 00:56:58,080
i mean

883
00:56:58,090 --> 00:57:04,420
so that they are close to each other it's unlikely that separated by way

884
00:57:04,430 --> 00:57:05,660
by crossover

885
00:57:05,710 --> 00:57:12,340
by recombination but i think that a very combinations so in there way it's more

886
00:57:12,340 --> 00:57:14,120
likely to break

887
00:57:14,150 --> 00:57:19,050
so whenever you have very close to each other and likely

888
00:57:21,890 --> 00:57:28,440
and what actually see years you see difference NIPS on a and then you see

889
00:57:31,130 --> 00:57:36,300
these red i mean this this final here shows you that these are correlated in

890
00:57:36,300 --> 00:57:38,640
different people i want to compute

891
00:57:38,690 --> 00:57:45,480
OK so that means the high correlation and why no correlation once and it separates

892
00:57:45,480 --> 00:57:47,820
the upper block on this one

893
00:57:49,870 --> 00:57:53,780
but it's

894
00:57:55,620 --> 00:57:57,290
so expression

895
00:57:58,640 --> 00:58:06,730
so the most prominent way of measuring gene expression by my very own article

896
00:58:06,790 --> 00:58:09,540
the metric my

897
00:58:09,590 --> 00:58:13,200
so the idea is that you have a glass plate and you put on these

898
00:58:13,200 --> 00:58:15,970
glass plates small sequence

899
00:58:15,990 --> 00:58:20,690
i mean sequences of twenty twenty five one sixteen hyperlink

900
00:58:20,690 --> 00:58:27,690
the sequences are complementary perspectives regions what research and on a piece of the team

901
00:58:27,690 --> 00:58:29,340
of DNA

902
00:58:29,340 --> 00:58:31,110
so if

903
00:58:31,160 --> 00:58:33,780
we are now using on a simple

904
00:58:34,110 --> 00:58:38,190
which we took all of the cell so we can

905
00:58:38,240 --> 00:58:40,430
we take them on a

906
00:58:40,500 --> 00:58:41,660
i would i

907
00:58:41,670 --> 00:58:43,360
to the

908
00:58:43,390 --> 00:58:45,880
troops on on the page

909
00:58:45,890 --> 00:58:53,270
and the label them on an NEC whenever they habitation has happened then we see

910
00:58:53,330 --> 00:59:00,480
light sport position if you know its position we have a bot which are independent

911
00:59:00,480 --> 00:59:04,360
which means that on a level

912
00:59:04,380 --> 00:59:06,540
we have been from the cells

913
00:59:06,710 --> 00:59:13,050
so what you get is with this kind of picture so we got me

914
00:59:13,370 --> 00:59:20,540
the idea expressed in the red thing but this is for comparison of two

915
00:59:20,600 --> 00:59:26,140
but there are many nontrivial level analysis tasks so you had this big place and

916
00:59:26,160 --> 00:59:32,580
there might be a million different products and things happen like a lot of what

917
00:59:32,580 --> 00:59:40,750
wanted after we are what this gradient hybridisation i mean concentration was on the true

918
00:59:40,760 --> 00:59:45,340
remove all these different out and you can use a lot of machine learning just

919
00:59:45,340 --> 00:59:53,480
OK guess is important just to recap we heard this morning because you've relaunched

920
00:59:54,740 --> 00:59:57,660
we talk about

921
00:59:57,720 --> 00:59:59,310
general models

922
00:59:59,430 --> 01:00:06,340
and i said the general model you need to define

923
01:00:11,640 --> 01:00:18,360
we can and i rise by offering here as well

924
01:00:19,790 --> 01:00:22,110
how to specify problem solving

925
01:00:22,110 --> 01:00:24,220
for general public

926
01:00:24,260 --> 01:00:28,510
OK as our

927
01:00:29,290 --> 01:00:36,680
OK i had the transition function in a market

928
01:00:36,710 --> 01:00:41,750
markov decision process was something to think about the transition function

929
01:00:41,760 --> 01:00:44,970
right which is

930
01:00:45,040 --> 01:00:48,890
which is the mark of assumptions camps

931
01:00:48,930 --> 01:00:53,060
the really you too much

932
01:00:53,070 --> 01:00:54,950
the activation function

933
01:00:56,420 --> 01:00:59,170
reward function gamma is what

934
01:00:59,180 --> 01:01:00,560
discount factor

935
01:01:00,570 --> 01:01:04,170
it is horizon wasn't able previous slide

936
01:01:04,180 --> 01:01:08,170
OK these are general problems which is the phi gamma with this by asian you

937
01:01:08,170 --> 01:01:13,280
also have the objective that you're trying to optimise the is discounted

938
01:01:13,320 --> 01:01:17,370
and you can start to say what are possible the solutions this problem of properties

939
01:01:17,370 --> 01:01:19,170
to have and so on

940
01:01:19,870 --> 01:01:23,560
because only six hours sitting in the chair model i went with

941
01:01:23,620 --> 01:01:26,570
popular model of the MDP

942
01:01:28,760 --> 01:01:30,780
what i need an MDP

943
01:01:31,760 --> 01:01:35,870
so if we what i dropped from here i don't need

944
01:01:35,930 --> 01:01:38,070
don't states

945
01:01:44,140 --> 01:01:50,610
in the end specifically markovian

946
01:01:50,650 --> 01:01:53,960
an easy they often the average function

947
01:01:54,000 --> 01:01:58,210
now because again this is the observation there are exactly the states is the bijection

948
01:01:58,220 --> 01:02:00,870
so you don't need observation orientation function

949
01:02:02,960 --> 01:02:08,000
and that the different you know how you should act what you reward for a

950
01:02:08,000 --> 01:02:11,630
course you need to know what your position criterion is

951
01:02:11,650 --> 01:02:16,100
so we said that the thing that we want to optimize

952
01:02:16,220 --> 01:02:18,810
OK so we define the policy right

953
01:02:18,810 --> 01:02:21,340
the mapping from states

954
01:02:21,380 --> 01:02:23,400
the action

955
01:02:23,410 --> 01:02:26,410
forty MDP we know that

956
01:02:26,500 --> 01:02:31,720
what's one property of the optimal policy

957
01:02:31,800 --> 01:02:35,810
pi star for an MDP

958
01:02:41,960 --> 01:02:44,960
so we know that there exists

959
01:02:45,000 --> 01:02:48,990
a price to such that for all states

960
01:02:49,030 --> 01:02:50,180
and all

961
01:02:50,210 --> 01:02:51,870
other policies

962
01:02:53,460 --> 01:02:55,550
the pie star

963
01:02:55,650 --> 01:02:59,840
the values you get underpriced of bns

964
01:02:59,850 --> 01:03:02,710
it's going to equal to

965
01:03:02,770 --> 01:03:07,090
the of one of us the of what here

966
01:03:07,740 --> 01:03:11,780
right for all the calls

967
01:03:13,720 --> 01:03:15,900
pi star

968
01:03:16,400 --> 01:03:18,400
and we know that exists

969
01:03:18,410 --> 01:03:21,970
and then i've got a nice property at the policy

970
01:03:22,030 --> 01:03:26,180
it's also deterministic

971
01:03:26,190 --> 01:03:30,030
OK as really nice

972
01:03:30,030 --> 01:03:33,880
this assumes that the function is a fixed probability distribution

973
01:03:33,930 --> 01:03:36,800
the transition function is not some person i can screw you

974
01:03:36,850 --> 01:03:40,410
OK that can change there will be non stationary

975
01:03:40,460 --> 01:03:44,000
this has to be stationary has markovian

976
01:03:44,050 --> 01:03:45,030
you don't know it

977
01:03:45,180 --> 01:03:49,210
the i mean what what are the weavers learning day we don't need no t

978
01:03:49,220 --> 01:03:51,780
but after properties

979
01:03:51,780 --> 01:03:56,530
so the

980
01:03:56,990 --> 01:03:59,600
sition means that i made the changes

981
01:03:59,620 --> 01:04:06,350
upon some suitable way apply which is not consistent so actually closest cluster markovian

982
01:04:06,360 --> 01:04:08,260
uh in terms of

983
01:04:08,300 --> 01:04:12,170
but so so for example if you

984
01:04:13,220 --> 01:04:16,130
if you're playing a poet this which strategies

985
01:04:16,170 --> 01:04:22,030
switch strategies on you their policy-making markovian but they might just change is simple

986
01:04:22,040 --> 01:04:24,280
i just do their internal state

987
01:04:24,280 --> 01:04:25,650
but but the

988
01:04:25,660 --> 01:04:28,210
the point is that this is fixed

989
01:04:28,260 --> 01:04:31,150
fixed probability distribution never changes

990
01:04:35,400 --> 01:04:36,290
so is this

991
01:04:36,300 --> 01:04:40,540
pastor satorras take we define

992
01:04:42,050 --> 01:04:45,460
the value of some of the pious

993
01:04:45,510 --> 01:04:47,920
as an expectation

994
01:04:53,900 --> 01:04:58,410
notations but that it was just was used in the future

995
01:05:00,780 --> 01:05:03,770
the expectation the following some policy pi

996
01:05:03,780 --> 01:05:06,840
given that we started

997
01:05:06,900 --> 01:05:10,400
in the initial state

998
01:05:10,460 --> 01:05:13,280
which possess

999
01:05:16,980 --> 01:05:20,860
we want to look at the expected sum of future this kind rewards so so

1000
01:05:20,860 --> 01:05:23,340
several times

1001
01:05:23,390 --> 01:05:28,390
out to infinite eight for doing it horizon articles infinity

1002
01:05:28,400 --> 01:05:31,450
and as i dropped the baby them sitting in

1003
01:05:38,380 --> 01:05:39,820
OK area

1004
01:05:44,420 --> 01:05:45,190
OK one

1005
01:05:45,200 --> 01:05:52,220
this the or to the data samples the executioner sample rewards the reward at time

1006
01:05:53,660 --> 01:05:57,450
and what

1007
01:05:57,480 --> 01:06:03,740
came into power to write this carry one

1008
01:06:03,760 --> 01:06:05,980
OK the defined policies

1009
01:06:05,990 --> 01:06:07,400
well defined

1010
01:06:07,460 --> 01:06:11,400
with the value function is OK now i got all the ingredients

1011
01:06:11,410 --> 01:06:14,210
i know there exists some policy part i for which

1012
01:06:15,480 --> 01:06:18,260
every state some examples by star

1013
01:06:18,270 --> 01:06:22,960
which which gives you the optimal value from all possible states NASA policy that's your

1014
01:06:22,960 --> 01:06:27,450
goal in in independent and certainly or we first learn the goal is to find

1015
01:06:27,460 --> 01:06:28,480
out why stars

1016
01:06:31,020 --> 01:06:34,650
now there are two reasons i justifying pi start

1017
01:06:34,650 --> 01:06:37,720
to model based algorithms

1018
01:06:37,780 --> 01:06:46,450
now start

1019
01:06:49,090 --> 01:06:52,950
cover value iteration which go by many names

1020
01:06:52,990 --> 01:07:01,610
also referred to as dynamic programming

1021
01:07:13,040 --> 01:07:15,710
relied on the bellman backup which

1022
01:07:15,720 --> 01:07:18,690
you quickly

1023
01:07:21,150 --> 01:07:23,480
it is this so

1024
01:07:23,480 --> 01:07:26,400
so long kernel methods

1025
01:07:26,410 --> 01:07:30,090
in fact what i to do the second there was

1026
01:07:30,110 --> 01:07:33,990
in the first bit actually did quadratic programming OK was

1027
01:07:34,010 --> 01:07:37,020
complextype problems

1028
01:07:37,030 --> 01:07:38,990
but i wanted to show in the second battle

1029
01:07:39,000 --> 01:07:44,550
in fact kernel based methods is bigger than this quadratic programming in fact i can

1030
01:07:44,550 --> 01:07:49,830
do linear programming and i can do

1031
01:07:49,850 --> 01:07:51,530
non linear optimisation

1032
01:07:51,550 --> 01:07:58,660
and also not really restricted to classification regression i can consider other types of problems

1033
01:07:58,750 --> 01:08:02,310
for example novelty detection OK

1034
01:08:02,350 --> 01:08:06,090
it might be for a system where

1035
01:08:06,100 --> 01:08:09,890
perhaps and try to classify objects and my classifier

1036
01:08:09,900 --> 01:08:13,990
perhaps is about classifier works OK but chemists certain things

1037
01:08:14,000 --> 01:08:21,030
why not detectors may still actually indicate the whatever it is is a novel

1038
01:08:21,040 --> 01:08:25,230
OK so i want to show that in fact the broader class of kernel based

1039
01:08:26,060 --> 01:08:27,140
and also

1040
01:08:27,180 --> 01:08:29,870
cover a few points really cover last time

1041
01:08:29,880 --> 01:08:34,410
for example had actually find the kernel parameter things like that

1042
01:08:35,050 --> 01:08:39,740
in the second lecture we show that the standard QP SVM is only one optimisation

1043
01:08:39,740 --> 01:08:46,370
approach we can we can do linear programming nonlinear programming semi definite programming it's OK

1044
01:08:47,070 --> 01:08:50,230
because of the drug in the whole of optimisation theory

1045
01:08:50,250 --> 01:08:56,290
right so we show that there are further tasks beyond classification classification regression

1046
01:08:56,300 --> 01:09:01,610
which can be performed using kernel based methods

1047
01:09:01,630 --> 01:09:03,120
twenty two

1048
01:09:09,350 --> 01:09:14,090
right comment on different learning algorithms and strategies for model selection

1049
01:09:14,100 --> 01:09:17,950
to find the culprit and and and also

1050
01:09:17,970 --> 01:09:19,330
another big

1051
01:09:19,340 --> 01:09:25,520
the really big cluster kernel based methods is i can actually use composite kernels give

1052
01:09:25,520 --> 01:09:29,580
you sample of this i think right in my previous talk i actually had some

1053
01:09:29,580 --> 01:09:35,510
material on network inference which i said that actually dropped because of time and network

1054
01:09:35,510 --> 01:09:37,500
inference i wanted to predict

1055
01:09:37,520 --> 01:09:40,950
the functional relationships in gene between genes OK

1056
01:09:40,970 --> 01:09:45,030
and perhaps a link to a plus one no link will be and minus one

1057
01:09:45,050 --> 01:09:49,480
now i could use expression data for that many people do however if i want

1058
01:09:49,480 --> 01:09:53,230
to build a classifier to my network inference which is sort optimal

1059
01:09:53,290 --> 01:09:55,660
i would use all data which is available

1060
01:09:56,360 --> 01:10:00,300
and what if i want to build a classifier uses multiple types of data

1061
01:10:00,370 --> 01:10:05,600
then that transmit additional composite kernel and data fusion again something i can do kernel

1062
01:10:05,600 --> 01:10:10,470
based methods that that have a problem with other techniques OK so it could cover

1063
01:10:10,470 --> 01:10:16,120
all these sort of offshoots of the story of kernel based methods as a linear

1064
01:10:16,120 --> 01:10:23,210
programming will continue programming nonlinear programming and non detection applications and other games

1065
01:10:23,580 --> 01:10:29,450
the problems giving up to now been passive learning for example in my previous lecture

1066
01:10:29,450 --> 01:10:32,310
taught about will stewart and how to use an SVM

1067
01:10:32,750 --> 01:10:38,410
to build the binary classifier to predict relapse non relapse that in all instances i

1068
01:10:38,410 --> 01:10:39,040
give you up to now

1069
01:10:39,520 --> 01:10:42,170
been passive learning in other words the classifier

1070
01:10:42,220 --> 01:10:50,420
i give you get the classifier datapoints learns these and tries to classify new data

1071
01:10:51,430 --> 01:10:56,330
active learning is the learning algorithm actually poses queries

1072
01:10:56,350 --> 01:11:01,070
to the oracle information source so as to will the rule much more efficiently

1073
01:11:01,730 --> 01:11:05,710
and it's been like me and trying to teach you some rule like arithmetic

1074
01:11:06,090 --> 01:11:10,050
so two plus two equals four three three to six and you could just sit

1075
01:11:10,050 --> 01:11:15,410
that i do this because the student actually just saw this sort these examples and

1076
01:11:15,410 --> 01:11:20,290
hope to generalize active learning which is called

1077
01:11:20,310 --> 01:11:25,330
which is the smart student i would say something two plus two equals four

1078
01:11:25,350 --> 01:11:28,990
and then you are OK well that's the case was the answer

1079
01:11:29,000 --> 01:11:36,420
two three percent by carefully posing your questions you can maximize information transfer OK to

1080
01:11:36,420 --> 01:11:44,740
active learning and something where SVM useful also in the second part of touch here

1081
01:11:44,740 --> 01:11:50,560
and they're quite alot on what was formerly my research areas for example in dublin

1082
01:11:50,670 --> 01:11:55,140
that give you was cooked up with christine bennett and the active learning was cooked

1083
01:11:55,140 --> 01:11:58,050
up with another and alex smola

1084
01:11:58,060 --> 01:12:03,120
right so also talk about radio SVM model selection different types of kernels and learning

1085
01:12:03,120 --> 01:12:07,930
with composite kernels but we able to cover all this right so i did this

1086
01:12:07,930 --> 01:12:10,950
quadratic programming type approach

1087
01:12:10,960 --> 01:12:15,450
but it's worth noting that there is also a linear programming approach which is very

1088
01:12:15,450 --> 01:12:24,710
similar popularized by christine burns and all the magnets are in michigan teaching

1089
01:12:24,730 --> 01:12:35,030
this was constant and its some one and it's sort champion

1090
01:12:35,450 --> 01:12:42,670
kernelizable so i can do a linear programming approach to show it to the second

1091
01:12:42,710 --> 01:12:44,410
but in fact

1092
01:12:44,420 --> 01:12:46,000
there's a huge number

1093
01:12:46,830 --> 01:12:54,480
and i otherwise i can tell you a kind of neural network algorithms such as

1094
01:12:54,480 --> 01:12:56,590
the perceptron mean over

1095
01:12:56,600 --> 01:12:58,450
and the adatron

1096
01:12:58,460 --> 01:13:04,650
OK these particular algorithms going back to the interface neural networks the data appears in

1097
01:13:04,650 --> 01:13:09,310
the form of the dot product scalar product so just like a scene just

1098
01:13:09,320 --> 01:13:12,200
i can take that dot product replaced by kernel

1099
01:13:12,210 --> 01:13:14,930
so i can kernelized algorithms such as the

1100
01:13:14,940 --> 01:13:17,340
perceptron winnow for control

1101
01:13:17,350 --> 01:13:19,810
if you look back on the constitution

1102
01:13:19,830 --> 01:13:26,210
you can see the kernel adatron somebody else did very early on and is relatively

1103
01:13:27,120 --> 01:13:33,770
time kernelized algorithm occurs in about forty lines much easier than the straightforward SVM still

1104
01:13:33,780 --> 01:13:40,020
quadratic programming and it'll handle non linearly separable datasets OK you're also take things that

1105
01:13:40,020 --> 01:13:44,250
if all of the variables and then states this is an m squared table i

1106
01:13:44,250 --> 01:13:45,570
have to work with

1107
01:13:45,590 --> 01:13:50,940
and so on can something up this table that's in m squared operation

1108
01:13:50,970 --> 01:13:54,460
and then i've got it sort of an order an operation to make another and

1109
01:13:54,460 --> 01:13:58,160
sweat operation to multiply this in here so all of these steps of order m

1110
01:13:58,160 --> 01:14:02,120
squared and the number of them is just linear in the length of the chain

1111
01:14:02,120 --> 01:14:05,210
so this looks way better than working with the joint distribution

1112
01:14:06,950 --> 01:14:10,710
so what's happening we started here at the beginning of the chain we

1113
01:14:10,720 --> 01:14:13,380
some the first variable

1114
01:14:13,380 --> 01:14:16,660
and then we multiply by the next potential some the next variables

1115
01:14:16,680 --> 01:14:20,340
we can take all of these things together we can give them

1116
01:14:20,810 --> 01:14:24,930
so we finish with this will left that i thing that we're not

1117
01:14:24,950 --> 01:14:28,180
marginalizing we get the whole thing and name

1118
01:14:29,760 --> 01:14:34,120
the function of x i will call it the alpha message

1119
01:14:34,130 --> 01:14:36,600
the messages going forwards along the chain

1120
01:14:36,630 --> 01:14:37,920
and then

1121
01:14:39,120 --> 01:14:41,070
this lot here when i

1122
01:14:41,080 --> 01:14:44,920
when i've some that exi plus one all the way up to x l

1123
01:14:45,140 --> 01:14:48,770
for the function of x i again which we call beta message

1124
01:14:48,780 --> 01:14:52,630
and that sort of started the the far into the chain so we're willing to

1125
01:14:52,630 --> 01:14:57,330
what now is the recursive algorithm for computing the marginals

1126
01:14:57,630 --> 01:15:02,380
so the marginal is just the product of the alpha message in the beta message

1127
01:15:02,380 --> 01:15:03,670
if we go back to here

1128
01:15:03,680 --> 01:15:07,290
we just had this much the products of this slot times this

1129
01:15:10,870 --> 01:15:11,760
but the

1130
01:15:11,770 --> 01:15:17,110
message themselves can be found recursively so the alpha message no x i

1131
01:15:17,120 --> 01:15:22,510
was obtained by summing over the previous node x i minus one of the clique

1132
01:15:22,510 --> 01:15:24,740
potential times message that

1133
01:15:24,770 --> 01:15:28,290
the alpha message that arrive from node x i minus one

1134
01:15:28,310 --> 01:15:30,190
so there's just recursive

1135
01:15:30,560 --> 01:15:35,120
computation of that alpha message in this case is going forwards along the chain we

1136
01:15:35,120 --> 01:15:37,440
start the message x i minus one

1137
01:15:37,460 --> 01:15:40,730
we apply this operation in the message site

1138
01:15:40,850 --> 01:15:45,270
then suddenly here for the beta message we start with that site plus one

1139
01:15:45,320 --> 01:15:50,310
and we were going backwards and we get the message x i

1140
01:15:50,330 --> 01:15:53,650
so i got one message going forward along the chain one message going backward along

1141
01:15:53,650 --> 01:15:57,190
the chain they meet select side we multiply them together

1142
01:15:58,410 --> 01:16:01,120
in order to get the marginal the only other thing we do is in general

1143
01:16:01,120 --> 01:16:07,620
will have this normalisation constant but finding the normalisation constant very easy because this is

1144
01:16:07,620 --> 01:16:12,540
just an n dimensional vector so we have an order operation in order to normalise

1145
01:16:12,540 --> 01:16:16,210
it so that we get the required marginal

1146
01:16:20,120 --> 01:16:21,640
so we have an algorithm

1147
01:16:21,670 --> 01:16:23,970
which has cost is not

1148
01:16:23,980 --> 01:16:28,350
and raised to the power l is not exponential in the length of the chain

1149
01:16:28,350 --> 01:16:31,610
is something like hell times m squared

1150
01:16:31,720 --> 01:16:34,950
o operations each which will and squared is now linear the length of the chain

1151
01:16:34,950 --> 01:16:36,530
rather than exponential

1152
01:16:36,730 --> 01:16:40,460
and turns out to be not one of them

1153
01:16:40,700 --> 01:16:44,460
and it's pretty easy i think you can you can see

1154
01:16:44,550 --> 01:16:46,100
to extend

1155
01:16:46,190 --> 01:16:48,050
that algorithm two

1156
01:16:48,060 --> 01:16:52,170
any graph which has a tree structure graph which there are no loops

1157
01:16:52,310 --> 01:16:56,160
you can see why

1158
01:16:56,760 --> 01:17:01,900
the problem with this is going to be the phi phi added one extra meaning

1159
01:17:01,900 --> 01:17:04,220
from here all the way around so there

1160
01:17:10,960 --> 01:17:15,170
this clique potential involving x one and x two also involves itself so i can

1161
01:17:15,170 --> 01:17:18,670
no longer just to the south asian over x one i can't get the process

1162
01:17:19,680 --> 01:17:26,330
and inference in that graph is actually extremely hard

1163
01:17:26,370 --> 01:17:29,370
so if we have a tree structured graphs have no loops

1164
01:17:29,390 --> 01:17:34,140
then we can just write down the recursive algorithm the kind of just described and

1165
01:17:34,140 --> 01:17:35,640
it takes the following form

1166
01:17:35,650 --> 01:17:38,240
so at each node we take

1167
01:17:38,260 --> 01:17:40,100
all incoming messages

1168
01:17:40,990 --> 01:17:43,710
let's take a little piece of the tree structured graphs here

1169
01:17:43,720 --> 01:17:45,110
it has three neighbours

1170
01:17:45,120 --> 01:17:47,740
and then we messages coming in

1171
01:17:47,760 --> 01:17:52,120
which are the partial summations from here on out with all the rest of the

1172
01:17:52,120 --> 01:17:53,190
graph here

1173
01:17:53,210 --> 01:17:57,730
there's another message arriving here which is obtained by summing over recursively something of everything

1174
01:17:57,730 --> 01:18:00,410
in all the graph which connects to here

1175
01:18:00,420 --> 01:18:04,190
so multiply those two potentials together together with the

1176
01:18:04,210 --> 01:18:05,840
local any local

1177
01:18:06,360 --> 01:18:11,830
potential from that no difference is if this is connected to an observed node and

1178
01:18:11,830 --> 01:18:14,550
we then some at node a site

1179
01:18:14,570 --> 01:18:16,520
and that gives an outgoing message

1180
01:18:16,540 --> 01:18:18,600
there should be a function of the

1181
01:18:18,620 --> 01:18:20,930
very about it's connected to here

1182
01:18:21,030 --> 01:18:26,560
so it's just a simple generalization of

1183
01:18:26,570 --> 01:18:31,530
all the algorithm that we derive the chain case

1184
01:18:31,540 --> 01:18:36,740
so what's going to happen is there will be a message flowing across every link

1185
01:18:38,400 --> 01:18:40,470
in general

1186
01:18:40,490 --> 01:18:51,880
we need to do something slightly more than that if we had a

1187
01:18:51,960 --> 01:18:53,940
OK so

1188
01:18:54,100 --> 01:18:58,850
this was an algorithm for computing the marginal at this particular note no in general

1189
01:18:58,850 --> 01:19:03,190
if we want the marginal let's say several nodes or even all of the nodes

1190
01:19:03,210 --> 01:19:05,430
it would be very wasteful

1191
01:19:05,530 --> 01:19:08,590
stupid wasteful to run the algorithm

1192
01:19:08,590 --> 01:19:11,010
every time the fresh for every single

1193
01:19:11,020 --> 01:19:13,950
no so to get the marginal for this node have to propagate the message all

1194
01:19:13,950 --> 01:19:17,460
the way from the beginning to that no but again this node after probably the

1195
01:19:17,460 --> 01:19:20,850
same message all way from the beginning i don't want to do that work

1196
01:19:20,870 --> 01:19:25,320
over and over again and the solutions very easy if you take one message and

1197
01:19:25,320 --> 01:19:26,810
frequency of

1198
01:19:26,820 --> 01:19:29,250
my hand i don't know

1199
01:19:29,330 --> 01:19:31,250
very far

1200
01:19:31,250 --> 01:19:32,440
go fast

1201
01:19:37,290 --> 01:19:39,100
our copy is

1202
01:19:39,120 --> 01:19:41,100
this is the

1203
01:19:41,120 --> 01:19:43,190
first of all

1204
01:19:43,230 --> 01:19:45,620
like this week

1205
01:19:45,650 --> 01:19:50,760
my hand is really not moving very much look look at my hands almost moving

1206
01:19:50,770 --> 01:19:51,930
and you see that

1207
01:19:51,950 --> 01:19:53,310
picture on the board

1208
01:19:53,340 --> 01:19:54,850
first of all

1209
01:19:54,860 --> 01:20:01,200
so now we have the fast

1210
01:20:01,240 --> 01:20:03,550
and it starts laughing again

1211
01:20:13,080 --> 01:20:21,810
when two fast that i want one

1212
01:20:21,990 --> 01:20:24,190
yeah yeah yeah yeah

1213
01:20:25,040 --> 01:20:26,200
the second

1214
01:20:26,210 --> 01:20:28,740
second harmonic

1215
01:20:28,780 --> 01:20:29,820
very happy

1216
01:20:29,840 --> 01:20:32,310
my hand is not very much

1217
01:20:32,390 --> 01:20:34,600
second harmonic

1218
01:20:34,640 --> 01:20:37,220
let's not go for the first

1219
01:20:37,220 --> 01:20:38,760
o to fast

1220
01:20:38,770 --> 01:20:40,180
he does nothing

1221
01:20:40,230 --> 01:20:42,620
a little faster does nothing

1222
01:20:42,780 --> 01:20:44,620
o faster

1223
01:20:44,630 --> 01:20:45,870
a little faster

1224
01:20:45,880 --> 01:20:47,600
it is

1225
01:20:47,620 --> 01:20:51,980
the knowledge

1226
01:20:54,980 --> 01:20:56,280
two points in the middle

1227
01:20:57,600 --> 01:21:00,030
not quite in the middle

1228
01:21:00,060 --> 01:21:02,750
two point of this free standing still

1229
01:21:02,800 --> 01:21:06,000
and of course joseph hands is also standing still

1230
01:21:06,000 --> 01:21:14,890
and now i try to go to from all i can possibly generate

1231
01:21:14,940 --> 01:21:16,770
you can't

1232
01:21:16,780 --> 01:21:19,190
you tell me then later how many

1233
01:21:19,230 --> 01:21:22,210
of the mountains and valleys you saw

1234
01:21:22,220 --> 01:21:24,710
if you see one point that is not moving

1235
01:21:24,730 --> 01:21:25,930
that is already the

1236
01:21:25,940 --> 01:21:27,030
second of all

1237
01:21:27,050 --> 01:21:32,840
the two point moving the moment so if you count four points

1238
01:21:32,880 --> 01:21:36,480
but i can do better than

1239
01:21:36,870 --> 01:21:39,280
i'm trying to push

1240
01:21:39,300 --> 01:21:41,120
you must this resonance

1241
01:21:41,200 --> 01:21:45,450
when the system

1242
01:21:46,370 --> 01:21:48,550
i lost

1243
01:21:48,650 --> 01:21:50,970
i love you

1244
01:21:52,010 --> 01:21:54,690
i got one

1245
01:21:54,770 --> 01:22:02,420
well for someone

1246
01:22:02,420 --> 01:22:05,550
they come on the UIUC

1247
01:22:05,780 --> 01:22:08,740
what you said

1248
01:22:09,350 --> 01:22:11,630
joseph unites all thirteen right

1249
01:22:11,640 --> 01:22:12,800
in some

1250
01:22:15,360 --> 01:22:16,930
is also known as

1251
01:22:17,490 --> 01:22:20,270
all unreliable

1252
01:22:20,280 --> 01:22:24,750
OK so you see

1253
01:22:24,760 --> 01:22:26,010
that's the way

1254
01:22:26,020 --> 01:22:28,340
that strings behave

1255
01:22:28,400 --> 01:22:31,530
and that's exactly the way to strings behave

1256
01:22:31,530 --> 01:22:34,520
of musical instruments

1257
01:22:34,540 --> 01:22:37,460
here is the string

1258
01:22:37,500 --> 01:22:40,040
that we can also

1259
01:22:40,060 --> 01:22:41,680
also late

1260
01:22:42,790 --> 01:22:46,740
by hand but we can we have a little more here that can make this

1261
01:22:46,740 --> 01:22:49,130
part go up and down

1262
01:22:49,140 --> 01:22:55,420
and the idea is we going to put it for you in the third harmonic

1263
01:22:55,470 --> 01:22:59,780
and the frequency so high in the third harmonic that you can really not see

1264
01:22:59,790 --> 01:23:01,100
that this is up

1265
01:23:01,120 --> 01:23:02,610
that this is down

1266
01:23:02,620 --> 01:23:06,010
goes too fast you could with the others being not here

1267
01:23:06,050 --> 01:23:08,930
and so then we are going to struggle with with strobe light

1268
01:23:08,930 --> 01:23:10,770
to make you see the

1269
01:23:10,780 --> 01:23:18,120
string more or less dense field though not quite but almost

1270
01:23:18,180 --> 01:23:22,330
literacy whether we can get into this homologous very delicate we may have to change

1271
01:23:22,330 --> 01:23:25,340
the frequency of pretty good

1272
01:23:25,450 --> 01:23:28,330
thirty he appointed is standing still

1273
01:23:28,390 --> 01:23:30,790
two stand still third harmonic

1274
01:23:30,800 --> 01:23:34,810
so the resonance now we call this address it's at its natural frequency

1275
01:23:34,820 --> 01:23:36,620
it's driven there

1276
01:23:36,640 --> 01:23:39,510
just the right frequency

1277
01:23:39,520 --> 01:23:43,590
he goes too fast you cannot see it stands you stand there the fact that

1278
01:23:43,590 --> 01:23:45,830
i going to put a strobe light on it

1279
01:23:45,890 --> 01:23:47,740
and the strobe light

1280
01:23:47,780 --> 01:23:51,920
throws light on it goes on and off on off on off on off with

1281
01:23:51,970 --> 01:23:54,210
about the same frequency

1282
01:23:54,260 --> 01:23:59,020
about the same we cannot exactly get the same so you're going to see now

1283
01:24:00,540 --> 01:24:04,380
the road will stand still or slowly move that's only

1284
01:24:04,390 --> 01:24:05,200
of course

1285
01:24:05,210 --> 01:24:08,510
optical illusion and reality goes like this

1286
01:24:08,520 --> 01:24:11,260
but it makes it we make it look standstill

1287
01:24:11,480 --> 01:24:12,560
the way that we

1288
01:24:12,610 --> 01:24:14,870
cast light on OK

1289
01:24:14,880 --> 01:24:16,630
if you're ready it to michael's

1290
01:24:16,710 --> 01:24:20,530
let's see

1291
01:24:20,540 --> 01:24:24,380
two and that of their

1292
01:24:24,400 --> 01:24:30,480
so i have a green light strobe on

1293
01:24:30,520 --> 01:24:31,600
where is the

1294
01:24:31,630 --> 01:24:37,290
order lecture one neuron

1295
01:24:37,340 --> 01:24:40,170
OK look at that so you see the green

1296
01:24:40,170 --> 01:24:42,620
the green

1297
01:24:45,090 --> 01:24:46,500
is that a

1298
01:24:46,500 --> 01:24:48,970
frequency very close to

1299
01:24:48,980 --> 01:24:51,820
the way that the rope is moving and so it looks like

1300
01:24:51,870 --> 01:24:55,740
standing still if i i can try to do a little better but it's too

1301
01:24:57,240 --> 01:25:05,250
to try to perhaps

1302
01:25:05,270 --> 01:25:15,340
no i went in the wrong direction

1303
01:25:15,370 --> 01:25:18,780
it's not so important that i hated exactly as long as you get the idea

1304
01:25:20,730 --> 01:25:24,970
what i can also do now i can turn it into a work of art

1305
01:25:25,080 --> 01:25:27,860
and i can put on the second stroke scope

1306
01:25:27,870 --> 01:25:29,290
in red

1307
01:25:29,340 --> 01:25:31,780
which i fleshed twice as fast

1308
01:25:31,860 --> 01:25:34,590
so it illuminates the role

1309
01:25:34,650 --> 01:25:36,200
to one

1310
01:25:36,210 --> 01:25:39,780
complete oscillation so you see the robot devil in red

1311
01:25:39,800 --> 01:25:43,100
and single ingredient

1312
01:25:43,190 --> 01:25:48,340
let's do that now

1313
01:25:48,370 --> 01:25:52,580
i have to

1314
01:25:52,600 --> 01:25:56,390
do the best i can

1315
01:25:56,400 --> 01:25:59,270
five ninety that becomes leaven eighty

1316
01:25:59,280 --> 01:26:08,390
to be very close now

1317
01:26:08,400 --> 01:26:12,990
the very close

1318
01:26:13,000 --> 01:26:16,450
she twice in red is he'd want green

1319
01:26:16,470 --> 01:26:20,670
nice to have a human

1320
01:26:26,130 --> 01:26:31,110
sure because we cannot exactly

1321
01:26:31,170 --> 01:26:32,110
he that there

1322
01:26:32,140 --> 01:26:35,250
it's is so therefore you get the phenomenon

1323
01:26:35,260 --> 01:26:38,860
very slow

1324
01:26:38,930 --> 01:26:43,040
we play with of the green and the red almost see exactly

1325
01:26:43,050 --> 01:26:45,040
they coincide

1326
01:26:45,040 --> 01:26:47,610
so i can change that very easily by

1327
01:26:47,610 --> 01:26:50,420
if we do this in the feature space

1328
01:26:50,440 --> 01:26:52,190
it will be equivalent

1329
01:26:52,210 --> 01:26:54,260
two modified directly

1330
01:26:54,280 --> 01:26:58,050
the gram matrix of the value of the canon metric

1331
01:27:02,530 --> 01:27:04,170
OK so

1332
01:27:04,170 --> 01:27:07,300
you know that you are so

1333
01:27:07,320 --> 01:27:12,820
so i was not completely i don't agree completely with what you see on wednesday

1334
01:27:12,860 --> 01:27:15,360
so seriously

1335
01:27:15,380 --> 01:27:19,610
overfitting with adaboost several papers actually that

1336
01:27:19,670 --> 01:27:25,920
and you must we need to take care of this parameters in order to avoid

1337
01:27:25,920 --> 01:27:32,030
overfitting and even in some cases you must change your loss function

1338
01:27:32,050 --> 01:27:37,240
it is the same age as of matching in order to avoid overfitting

1339
01:27:37,260 --> 01:27:40,760
you have kind of sea was just

1340
01:27:40,780 --> 01:27:45,550
there are several reasons instance peter back true

1341
01:27:50,400 --> 01:27:51,820
we're going to do this

1342
01:27:51,840 --> 01:27:54,110
and so fights for that

1343
01:27:54,130 --> 01:27:57,550
it's going to be a little bit more difficult when we're going to look at

1344
01:27:57,550 --> 01:28:00,740
what we have we cannot work directly with this five

1345
01:28:00,780 --> 01:28:01,990
of y

1346
01:28:02,010 --> 01:28:05,780
because we don't have a we have no implicit function

1347
01:28:05,800 --> 01:28:09,670
so we are going to use can and cannot do so

1348
01:28:09,670 --> 01:28:10,940
at each time

1349
01:28:10,960 --> 01:28:14,110
how problem which is defined by

1350
01:28:14,150 --> 01:28:17,860
some training input data and some output

1351
01:28:17,860 --> 01:28:20,510
a gram metrics

1352
01:28:20,530 --> 01:28:24,860
at each time we're going to change run metric

1353
01:28:24,860 --> 01:28:25,820
in order to

1354
01:28:25,840 --> 01:28:27,070
to capture

1355
01:28:30,630 --> 01:28:34,110
it was not predicted well i said previous

1356
01:28:34,150 --> 01:28:37,670
their combination

1357
01:28:37,760 --> 01:28:39,860
OK so one by

1358
01:28:39,900 --> 01:28:42,380
if you want to do this

1359
01:28:42,400 --> 01:28:44,110
in the general case

1360
01:28:44,240 --> 01:28:45,190
you have

1361
01:28:45,470 --> 01:28:50,630
to consider basis function but is research

1362
01:28:50,650 --> 01:28:52,550
in the following from

1363
01:28:52,570 --> 01:28:55,820
it's all in combination

1364
01:28:55,860 --> 01:29:00,710
the function in combination with the weights that depend of x

1365
01:29:00,800 --> 01:29:04,070
and so as a m and

1366
01:29:04,090 --> 01:29:06,300
he was key to solution

1367
01:29:06,320 --> 01:29:08,320
in terms of these

1368
01:29:08,320 --> 01:29:09,920
so the residual

1369
01:29:09,940 --> 01:29:12,760
only the future output space

1370
01:29:12,800 --> 01:29:14,510
OK it's

1371
01:29:14,530 --> 01:29:16,690
is it the cases

1372
01:29:17,210 --> 01:29:18,530
the moment where

1373
01:29:19,710 --> 01:29:24,170
i don't want to let people

1374
01:29:24,190 --> 01:29:25,420
OK so remember

1375
01:29:25,420 --> 01:29:28,380
adaboost gradient boosting regression

1376
01:29:28,400 --> 01:29:29,760
you change

1377
01:29:29,760 --> 01:29:31,150
your outputs

1378
01:29:31,170 --> 01:29:35,840
each time you want to make relation and visitors

1379
01:29:35,860 --> 01:29:37,300
in state of

1380
01:29:37,320 --> 01:29:38,960
the true output so

1381
01:29:38,970 --> 01:29:43,400
during all the gradient boosting change your output what to this

1382
01:29:43,440 --> 01:29:47,590
in our setting is the same but in the feature space

1383
01:29:47,610 --> 01:29:49,090
and of course

1384
01:29:49,110 --> 01:29:50,030
we have no

1385
01:29:50,900 --> 01:29:53,170
two the implicit function theorem

1386
01:29:53,190 --> 01:29:55,530
so we're going to do this

1387
01:29:55,590 --> 01:29:57,030
with one metrics

1388
01:29:57,050 --> 01:29:58,570
can an actress

1389
01:29:58,590 --> 01:30:01,130
moreover if we want this to be

1390
01:30:01,150 --> 01:30:03,610
particle physics

1391
01:30:03,690 --> 01:30:05,470
we have

1392
01:30:05,490 --> 01:30:07,880
if we have this

1393
01:30:07,920 --> 01:30:09,440
kind of functions

1394
01:30:09,460 --> 01:30:11,470
directly of five

1395
01:30:11,490 --> 01:30:17,510
i wish i might sedaris then i can go follows is learning

1396
01:30:17,570 --> 01:30:19,190
it's possible to do it

1397
01:30:19,190 --> 01:30:19,990
to do it

1398
01:30:20,010 --> 01:30:21,940
of course the trees

1399
01:30:21,960 --> 01:30:24,030
wishing tree that i would

1400
01:30:24,030 --> 01:30:27,170
one and just keep things the computation

1401
01:30:27,210 --> 01:30:32,130
so we have two stage in this is that with the learning stage

1402
01:30:32,150 --> 01:30:34,860
so in the london stage well

1403
01:30:34,880 --> 01:30:39,380
it's exactly the same as previously but in some computation three

1404
01:30:39,400 --> 01:30:45,800
two or three pages of competition but just linear algebra this year are very complex

1405
01:30:45,820 --> 01:30:52,940
each time you want to add some basis rigorous you're going to change from metrics

1406
01:30:52,990 --> 01:30:57,550
in such a way that you take into account the fact that previously you eat

1407
01:30:57,550 --> 01:31:00,780
some and interfaces to predict

1408
01:31:00,820 --> 01:31:02,740
that is sum output

1409
01:31:02,820 --> 01:31:06,440
so you you you are just two

1410
01:31:06,470 --> 01:31:08,820
to make the computation not

1411
01:31:08,840 --> 01:31:13,280
in that case the output feature space that we scanners

1412
01:31:14,190 --> 01:31:16,800
it is equivalent to do this competition research

1413
01:31:18,630 --> 01:31:21,400
you apply your proof and basilan

1414
01:31:21,420 --> 01:31:24,780
which is able to do we can analyse the spaces

1415
01:31:24,820 --> 01:31:26,190
you get

1416
01:31:26,240 --> 01:31:28,460
in fact this kind of functions

1417
01:31:28,570 --> 01:31:32,280
and then you can go on

1418
01:31:32,300 --> 01:31:33,940
and you can be tree

1419
01:31:35,360 --> 01:31:39,130
so so far you have these

1420
01:31:39,150 --> 01:31:40,320
you have been doing

1421
01:31:40,340 --> 01:31:42,360
but you are not able to predict

1422
01:31:42,370 --> 01:31:47,070
so can you see why we are not able to predict

1423
01:31:47,090 --> 01:31:50,040
in fact all the competition and in

1424
01:31:50,050 --> 01:31:52,750
was of course and

1425
01:31:52,790 --> 01:31:53,820
as for one

1426
01:31:53,830 --> 01:31:58,950
i was in feature space but all my functions in terms

1427
01:32:02,250 --> 01:32:03,870
thank you

1428
01:32:03,900 --> 01:32:06,840
in terms of the so the residuals

1429
01:32:06,870 --> 01:32:08,780
and it's not at all what i want

1430
01:32:08,800 --> 01:32:10,570
i won't

1431
01:32:10,580 --> 01:32:12,400
at least my my

1432
01:32:12,910 --> 01:32:14,530
function in them

1433
01:32:16,200 --> 01:32:17,940
so true phi

1434
01:32:17,990 --> 01:32:18,750
so if i

1435
01:32:18,830 --> 01:32:20,280
of y i

1436
01:32:20,940 --> 01:32:24,130
so this is possible i want to use the

1437
01:32:24,170 --> 01:32:27,530
competition but you can recursively get

1438
01:32:27,540 --> 01:32:28,500
get this

1439
01:32:28,540 --> 01:32:35,360
and you can rewrite your output always in the feature space by this combination

1440
01:32:35,360 --> 01:32:39,740
and then i'm going to talk about the classic cars

1441
01:32:39,760 --> 01:32:41,940
well maybe you can say that already

1442
01:32:41,950 --> 01:32:47,240
learning our gardens the monte carlo temperature friends and

1443
01:32:47,250 --> 01:32:50,860
and some new staff as well

1444
01:32:50,940 --> 01:32:55,800
so the learning problem solving learning problems increases the reaction that the and the sum

1445
01:32:55,800 --> 01:32:58,930
on all or you can send it sounds because of

1446
01:32:58,970 --> 01:33:05,150
the the and the like the next state and reward

1447
01:33:05,170 --> 01:33:10,150
in the learning problem you're just given this bunch of samples adar

1448
01:33:10,230 --> 01:33:16,200
you've got this samples by interacting with the environment or the just give

1449
01:33:16,260 --> 01:33:20,320
and you want to you have to learn a good policy and

1450
01:33:20,360 --> 01:33:24,350
then the question shifts a little bit so you can consider t

1451
01:33:24,360 --> 01:33:26,570
usually these two problems

1452
01:33:26,590 --> 01:33:29,520
are the problem has two sides

1453
01:33:29,580 --> 01:33:34,460
so one is computational complexity so you want to have an arrogant and that can

1454
01:33:34,460 --> 01:33:37,290
be executed and then it will finish

1455
01:33:37,390 --> 01:33:39,120
in your lifetime

1456
01:33:39,200 --> 01:33:43,510
so that's one problem the other problem is that you want to have in our

1457
01:33:44,470 --> 01:33:50,550
that's able to extract as much information from the samples that is just possible

1458
01:33:50,590 --> 01:33:59,220
so you want to have in other words a simple efficient learning

1459
01:33:59,740 --> 01:34:06,220
OK and you could have two different cause so one goal is that once you

1460
01:34:06,430 --> 01:34:09,820
gutter sound but just want to learn a good policy

1461
01:34:09,890 --> 01:34:13,640
and maybe you're interacting with the system but you don't care about the actual costs

1462
01:34:14,630 --> 01:34:15,760
during learning

1463
01:34:15,770 --> 01:34:19,380
we you want to learn an optimal way are very

1464
01:34:19,400 --> 01:34:20,970
plus they are very

1465
01:34:20,970 --> 01:34:22,100
she played

1466
01:34:22,120 --> 01:34:24,540
so you care about your losses

1467
01:34:24,550 --> 01:34:30,330
why are interacting with the system so the second problem is inherently online

1468
01:34:30,340 --> 01:34:33,590
the first problem could be offline

1469
01:34:33,610 --> 01:34:36,300
OK so

1470
01:34:36,350 --> 01:34:39,170
first we are going to consider learning

1471
01:34:39,190 --> 01:34:42,960
optimally are learning the fast lane

1472
01:34:44,970 --> 01:34:46,880
they're not to

1473
01:34:46,940 --> 01:34:51,040
too many designers that we can scale to

1474
01:34:51,090 --> 01:34:54,020
be aware and BP's for

1475
01:34:54,040 --> 01:34:55,560
for learning

1476
01:34:55,570 --> 01:34:57,990
in fast

1477
01:34:59,440 --> 01:35:00,640
we are

1478
01:35:00,670 --> 01:35:04,560
we will need to to look at the singular cases and we're going to look

1479
01:35:04,560 --> 01:35:08,820
at the very famous problem in the bandit problem

1480
01:35:08,860 --> 01:35:14,420
and inspired by the many problems are going to look at very briefly at an

1481
01:35:15,640 --> 01:35:19,810
the reason target and tries to solve finite mdps and

1482
01:35:19,890 --> 01:35:21,710
optimal manner are

1483
01:35:21,880 --> 01:35:25,630
just very efficiently

1484
01:35:25,690 --> 01:35:29,850
so i guess i'm going to skip this slide because this just

1485
01:35:29,870 --> 01:35:36,000
explains the difference between a supervised learning and reinforcement learning but that's

1486
01:35:36,090 --> 01:35:38,570
might already be clear to you

1487
01:35:39,840 --> 01:35:44,560
reinforcement learning is more general than supervised learning for i

1488
01:35:44,590 --> 01:35:48,530
supervised learning problems reinforcement learning problems but

1489
01:35:48,550 --> 01:35:52,840
but on the other hand

1490
01:35:53,030 --> 01:35:57,880
reinforcement learning problems are because i believe the more and limited feedback there due to

1491
01:35:57,880 --> 01:36:02,550
see if i just more difficult than those supervised learning problems

1492
01:36:02,560 --> 01:36:07,650
like in the mathematical society

1493
01:36:07,710 --> 01:36:12,890
so learning of optimality so that's that's both explore exploit

1494
01:36:12,900 --> 01:36:17,590
and so as i said we're going to look into bandit problems

1495
01:36:17,600 --> 01:36:22,180
and then into the problem of learning and the kids

1496
01:36:23,070 --> 01:36:27,790
imagine that you have two herbs to treatment for some

1497
01:36:29,280 --> 01:36:31,320
and you don't know

1498
01:36:31,330 --> 01:36:34,860
which one of the two is more efficient

1499
01:36:34,930 --> 01:36:37,340
and you want to test them out

1500
01:36:37,380 --> 01:36:39,850
how the these

1501
01:36:39,860 --> 01:36:45,360
so it's a ensure that we have this two treatments based on the success successful

1502
01:36:45,360 --> 01:36:46,800
with this

1503
01:36:48,160 --> 01:36:50,850
of course the the goal is to

1504
01:36:50,900 --> 01:36:55,480
well maybe this illnesses left and

1505
01:36:55,480 --> 01:36:59,490
either you to cure a patient or you lose the patient and of course your

1506
01:36:59,490 --> 01:37:00,460
goal is

1507
01:37:00,490 --> 01:37:03,990
not to lose too many patients by trying to figure out

1508
01:37:04,010 --> 01:37:05,920
which treatment to use

1509
01:37:06,740 --> 01:37:08,690
so that makes sense

1510
01:37:08,710 --> 01:37:10,710
the question is

1511
01:37:10,760 --> 01:37:16,090
imagine that you are in the situation where you already experimented with both of the

1512
01:37:16,090 --> 01:37:18,810
treatments quite a few times

1513
01:37:19,850 --> 01:37:27,370
to see that treatment and all riders herb is somebody better than the other two

1514
01:37:27,400 --> 01:37:29,360
and what the

1515
01:37:29,410 --> 01:37:30,840
use that

1516
01:37:31,960 --> 01:37:34,020
use the other

1517
01:37:35,610 --> 01:37:42,710
you saying

1518
01:37:42,730 --> 01:37:46,270
well i believe prior should play any role here so

1519
01:37:46,280 --> 01:37:49,590
if i'm trying to sell and are given to

1520
01:37:49,640 --> 01:37:52,010
clinical trial company then

1521
01:37:52,160 --> 01:37:56,620
was prior should second is the prior of the from c company or is the

1522
01:37:56,650 --> 01:37:58,850
prior of the patients are

1523
01:37:58,870 --> 01:38:04,450
was prodded should be no prior

1524
01:38:04,460 --> 01:38:05,810
so what to do

1525
01:38:07,160 --> 01:38:08,270
do you choose

1526
01:38:08,270 --> 01:38:10,210
o would go was the treatment

1527
01:38:10,270 --> 01:38:13,880
that looks bad

1528
01:38:14,170 --> 01:38:18,310
OK that makes sense right so because it works better in the past it should

1529
01:38:18,310 --> 01:38:20,170
work but in the future

1530
01:38:20,190 --> 01:38:23,910
goal the treatment that just not better

1531
01:38:23,960 --> 01:38:24,860
the first one

1532
01:38:25,360 --> 01:38:26,860
no one OK

1533
01:38:39,120 --> 01:38:43,370
so that's maybe a different problem so in that problem OK

1534
01:38:43,370 --> 01:38:44,830
member canada was down here

1535
01:38:45,970 --> 01:38:47,870
and we predicted it was what was here

1536
01:38:48,580 --> 01:38:49,740
there's they update

1537
01:38:50,930 --> 01:38:52,010
just got one here

1538
01:38:52,780 --> 01:38:54,660
gothenburg which was way up here

1539
01:38:55,410 --> 01:38:56,560
so it's an empirical

1540
01:38:57,120 --> 01:38:58,240
evidence the shrinkage

1541
01:38:58,950 --> 01:39:01,060
for those astute among you may have

1542
01:39:03,240 --> 01:39:04,180
that we lost are

1543
01:39:05,160 --> 01:39:05,640
in a sense

1544
01:39:07,510 --> 01:39:08,580
supposed to be funny but

1545
01:39:10,140 --> 01:39:12,220
so we lost our ass where they go

1546
01:39:13,950 --> 01:39:14,530
it went

1547
01:39:15,370 --> 01:39:17,370
way after this site so

1548
01:39:17,870 --> 01:39:20,640
this is stockholm and it turned out in stockholm

1549
01:39:21,200 --> 01:39:24,280
there are many more deaths in the control group

1550
01:39:24,890 --> 01:39:27,180
breast cancer deaths then you could have predicted

1551
01:39:27,910 --> 01:39:30,680
so it's like you know regression to the mean but then

1552
01:39:31,120 --> 01:39:31,990
there's wayne gretzky

1553
01:39:33,010 --> 01:39:36,370
udacity cresta the main stock and didn't progress me

1554
01:39:39,510 --> 01:39:43,180
on the two thousand nine this thing about brawling over mammography

1555
01:39:44,030 --> 01:39:45,760
the task force employed

1556
01:39:48,080 --> 01:39:51,700
anne randomized controlled trials to make their recommendation

1557
01:39:52,180 --> 01:39:55,660
and the recommendation was the same as the recommendation we made

1558
01:39:56,660 --> 01:39:57,800
twelve years earlier

1559
01:39:58,310 --> 01:40:00,810
this modelling including one bayesian model

1560
01:40:01,830 --> 01:40:03,180
and andy anderson model

1561
01:40:04,890 --> 01:40:07,120
andy tell you about the at

1562
01:40:08,310 --> 01:40:11,430
on the reactions to the task force recommendation

1563
01:40:12,390 --> 01:40:13,540
at the time said

1564
01:40:14,830 --> 01:40:15,580
new guidelines

1565
01:40:16,160 --> 01:40:17,260
draw opposition

1566
01:40:18,620 --> 01:40:19,970
sparked heated debate

1567
01:40:23,220 --> 01:40:27,140
globe says breast cancer screening advice appended

1568
01:40:27,600 --> 01:40:31,450
and there are lots charges at the time and if you remember death squads

1569
01:40:32,080 --> 01:40:35,180
so the charges with this as an obama death squad

1570
01:40:35,990 --> 01:40:38,410
keeping women from getting these saving

1571
01:40:41,780 --> 01:40:46,010
so these are the models that they used was francis

1572
01:40:46,930 --> 01:40:48,830
and cystatin is based on the following

1573
01:40:49,300 --> 01:40:51,260
this is the trends in breast cancer

1574
01:40:52,160 --> 01:40:55,810
mortality over time it's actually a broken down by race

1575
01:40:56,700 --> 01:41:00,350
but you see the whites which is that be the biggest group

1576
01:41:01,010 --> 01:41:02,870
the drop is substantial

1577
01:41:03,410 --> 01:41:05,910
similarly new came drop between

1578
01:41:06,370 --> 01:41:07,200
in nineteen ninety

1579
01:41:07,810 --> 01:41:10,780
and two thousand seven was thirty six percent

1580
01:41:11,780 --> 01:41:13,080
so what's causing them

1581
01:41:13,830 --> 01:41:18,600
andy screening people said this is screening mammography because screening came in

1582
01:41:19,310 --> 01:41:20,140
in this period

1583
01:41:21,470 --> 01:41:23,910
body treatment people said this is treatment

1584
01:41:25,310 --> 01:41:26,870
because screen treatment

1585
01:41:27,760 --> 01:41:29,030
got very much better

1586
01:41:30,120 --> 01:41:30,910
o including

1587
01:41:31,970 --> 01:41:35,950
two marks and including chemotherapy in the same period

1588
01:41:36,700 --> 01:41:39,560
so cystatin was a group of seven model is

1589
01:41:40,140 --> 01:41:42,540
funded by the and then see eye to

1590
01:41:43,370 --> 01:41:44,830
model this process

1591
01:41:47,850 --> 01:41:49,350
and we publish our results

1592
01:41:49,800 --> 01:41:51,030
in the new england journal

1593
01:41:53,540 --> 01:41:54,410
in my favourite

1594
01:41:56,560 --> 01:42:02,430
the headline was from sea and and statistical blitz helps spin down mammography benefits

1595
01:42:03,850 --> 01:42:05,370
the new york times editorial

1596
01:42:06,370 --> 01:42:12,120
these are some of the data we used e everybody opened up there are balls data

1597
01:42:12,830 --> 01:42:15,990
the city seedy and see i various other groups

1598
01:42:17,060 --> 01:42:19,950
unfortunately we don't have in the united states

1599
01:42:20,620 --> 01:42:21,300
uh i

1600
01:42:21,300 --> 01:42:27,120
database like they have in scandinavian countries where you can follow the same women over time

1601
01:42:27,890 --> 01:42:31,720
to see what happens you know how they were treated i how long they lived

1602
01:42:31,970 --> 01:42:34,080
we had only cross sectional data

1603
01:42:34,490 --> 01:42:39,660
so we knew who got you know what the proportion of women were the cut marks we knew

1604
01:42:40,530 --> 01:42:44,640
you know what their age is wear we knew that screen how frequently they get

1605
01:42:44,640 --> 01:42:47,680
screened et cetera but we know the two things together

1606
01:42:50,700 --> 01:42:51,890
on this was eight

1607
01:42:52,530 --> 01:42:56,430
i had to work hard to get my co authors to understand

1608
01:42:56,970 --> 01:42:59,370
and to agree to put this in the paper

1609
01:43:00,330 --> 01:43:01,810
this was eh

1610
01:43:03,470 --> 01:43:04,140
a kernel

1611
01:43:04,930 --> 01:43:06,560
estimates estimation

1612
01:43:07,180 --> 01:43:11,700
forty major things that we are addressing this is the reduction in

1613
01:43:12,140 --> 01:43:14,160
breast cancer mortality due screening

1614
01:43:14,600 --> 01:43:17,260
this is the reduction due to adjuvant treatment

1615
01:43:18,450 --> 01:43:21,740
andy we had individual models that were

1616
01:43:22,140 --> 01:43:24,600
spread over the board actually they are different

1617
01:43:25,390 --> 01:43:26,330
this is model him

1618
01:43:27,370 --> 01:43:30,060
framed in this is rochester assessments

1619
01:43:32,010 --> 01:43:34,080
georgetown stanford dana-farber

1620
01:43:36,530 --> 01:43:38,600
everyone except model him

1621
01:43:39,370 --> 01:43:40,510
as a point estimate

1622
01:43:41,410 --> 01:43:44,760
and i wanted to convey the variability in the modelling

1623
01:43:45,280 --> 01:43:47,280
by taking this rather grows

1624
01:43:49,350 --> 01:43:54,220
estimate it shows kind of the negative trend which is sort of expected because

1625
01:43:55,260 --> 01:43:59,490
if something is not do screening that there's something else maybe it's treatment

1626
01:44:04,100 --> 01:44:06,010
i did the same thing for modeling

1627
01:44:06,680 --> 01:44:10,680
because model hamas really many models we do lots of simulations

1628
01:44:11,410 --> 01:44:13,220
and we if we if we don't match

1629
01:44:13,890 --> 01:44:18,990
mortality then we rejected if we do then we accept that we accept the parameters

1630
01:44:18,990 --> 01:44:20,740
that we used to the simulation

1631
01:44:21,140 --> 01:44:24,530
so use using bayes rule and in the interesting

1632
01:44:25,010 --> 01:44:25,830
not typical

1633
01:44:26,490 --> 01:44:29,830
what you can simply write down the likelihood function for example

1634
01:44:29,830 --> 01:44:32,020
is it inventing these rules

1635
01:44:32,040 --> 01:44:36,090
because what we need is all the sound all logic

1636
01:44:36,110 --> 01:44:37,290
and complete

1637
01:44:37,370 --> 01:44:38,860
and terminate

1638
01:44:38,880 --> 01:44:40,870
so there are various

1639
01:44:41,180 --> 01:44:43,150
ways of getting this

1640
01:44:43,270 --> 01:44:45,180
and i just thought i

1641
01:44:45,230 --> 01:44:51,420
i'll tell you a a little bit more about different systems also we

1642
01:44:51,430 --> 01:44:53,210
we haven't finished completeness

1643
01:44:53,220 --> 01:44:54,730
if there is no

1644
01:44:54,750 --> 01:44:58,880
closedk k tableau for y OK so we're putting the icing on the cake now

1645
01:44:59,880 --> 01:45:01,310
so this was soundness

1646
01:45:01,360 --> 01:45:03,040
so what's completeness

1647
01:45:03,050 --> 01:45:07,510
if there is no closedk tableau for y then y is satisfiable

1648
01:45:07,590 --> 01:45:10,520
now closed

1649
01:45:11,770 --> 01:45:12,830
fall four

1650
01:45:16,240 --> 01:45:18,510
he said

1651
01:45:18,530 --> 01:45:20,480
OK so

1652
01:45:20,510 --> 01:45:21,810
y twelve

1653
01:45:21,820 --> 01:45:24,540
this is not a a blog during its construction

1654
01:45:24,550 --> 01:45:27,570
you get a model graph or hintikka said

1655
01:45:27,590 --> 01:45:30,160
you get something like w

1656
01:45:30,160 --> 01:45:31,930
list and

1657
01:45:32,010 --> 01:45:35,980
where each one here is downward saturated hintikka sets

1658
01:45:35,990 --> 01:45:40,520
there is a relation between them if these guys hold and you just like this

1659
01:45:40,520 --> 01:45:43,470
valuation take it's course

1660
01:45:43,520 --> 01:45:46,720
if the atoms in a make it true the is not in eighty you make

1661
01:45:46,720 --> 01:45:47,780
it false

1662
01:45:47,800 --> 01:45:51,770
crank handle and it'll work for larger formulae

1663
01:45:54,320 --> 01:45:58,950
what's the final corollary of in this special case when you let y equal to

1664
01:45:58,950 --> 01:46:00,590
not five

1665
01:46:00,680 --> 01:46:04,710
i suppose you could be some particular not then it says

1666
01:46:06,040 --> 01:46:11,100
there he is not closed

1667
01:46:11,270 --> 01:46:15,260
what four not far

1668
01:46:16,790 --> 01:46:20,010
not five

1669
01:46:20,630 --> 01:46:21,550
he is

1670
01:46:21,660 --> 01:46:26,330
k satisfiable

1671
01:46:26,340 --> 01:46:33,250
what does that mean that means that there exists some model there there exists a

1672
01:46:33,250 --> 01:46:37,170
world which makes not quite true

1673
01:46:37,170 --> 01:46:40,910
now what i want of the tell

1674
01:46:46,630 --> 01:46:49,600
if there is a closedk tableau for not five

1675
01:46:49,610 --> 01:46:51,220
ten five valid

1676
01:46:51,220 --> 01:46:53,840
if there is no closedk tableau not five

1677
01:46:53,870 --> 01:46:55,480
then phi is not valid

1678
01:46:55,500 --> 01:46:59,900
wise final value because it is model which makes it falls

1679
01:46:59,910 --> 01:47:09,310
so this is a decision procedure for validity

1680
01:47:09,330 --> 01:47:11,650
OK what about logical consequence

1681
01:47:11,650 --> 01:47:15,910
remember what we wanted and then calculate was phased introduction

1682
01:47:15,920 --> 01:47:17,960
from some set of assumptions

1683
01:47:18,010 --> 01:47:20,910
so suppose we

1684
01:47:24,260 --> 01:47:27,250
he a lot of what we do

1685
01:47:27,260 --> 01:47:28,240
from gamma

1686
01:47:28,250 --> 01:47:32,900
if there is closedk tableau form gamma semicolon not

1687
01:47:33,010 --> 01:47:36,980
now what i mean by that is that all

1688
01:47:38,600 --> 01:47:44,080
we want completeness what we want to say is is there is no

1689
01:47:44,130 --> 01:47:46,310
close to teddlie

1690
01:47:47,010 --> 01:47:48,970
there is some model

1691
01:47:48,980 --> 01:47:53,360
and some world that w makes

1692
01:47:53,380 --> 01:47:56,180
all the things in gamma trophy

1693
01:47:56,180 --> 01:47:59,670
so i not some world there is a moral

1694
01:47:59,720 --> 01:48:04,130
such that the model forces all the things in gamma and

1695
01:48:04,190 --> 01:48:06,350
the model forces

1696
01:48:06,370 --> 01:48:08,080
does not force

1697
01:48:08,100 --> 01:48:11,860
what is construction gave us

1698
01:48:11,920 --> 01:48:13,300
right i was

1699
01:48:15,180 --> 01:48:17,930
but how do we know

1700
01:48:17,950 --> 01:48:20,340
that we still got this camera

1701
01:48:20,370 --> 01:48:23,810
OK this is an example there

1702
01:48:23,830 --> 01:48:28,000
so the problem is in the k will you start off with here

1703
01:48:28,010 --> 01:48:32,000
and i suppose xbox p one

1704
01:48:32,010 --> 01:48:33,980
so gamma is this guy

1705
01:48:34,000 --> 01:48:35,570
five is the sky

1706
01:48:35,600 --> 01:48:37,940
we negate five so we get these

1707
01:48:37,950 --> 01:48:40,680
but in negation normal form of these

1708
01:48:40,690 --> 01:48:43,660
now we do and diamond jump get this

1709
01:48:43,740 --> 01:48:46,080
so these constructions is

1710
01:48:46,110 --> 01:48:50,770
he is saturated world which has diamonds

1711
01:48:50,780 --> 01:48:53,950
boxes there i need and some other guy

1712
01:48:53,950 --> 01:48:58,830
it's like how we're doing a jump to the successor world down here which has

1713
01:49:00,010 --> 01:49:04,880
formula without diamond all the axis come along with there aren't any voxel nearly and

1714
01:49:04,880 --> 01:49:07,650
is it gets thrown away

1715
01:49:07,700 --> 01:49:11,150
now i don't mean know all that this guy

1716
01:49:11,190 --> 01:49:15,240
as gamma in because we put it in the beginning just stay there

1717
01:49:15,290 --> 01:49:18,120
but when we jumped we lost again

1718
01:49:18,130 --> 01:49:21,110
right this guy doesn't make a w node

1719
01:49:21,200 --> 01:49:24,430
this guy making all truth

1720
01:49:25,270 --> 01:49:29,650
we've got this but we haven't got this

1721
01:49:29,670 --> 01:49:34,380
so we've got this time we don't want to be one of the negation

1722
01:49:34,400 --> 01:49:36,980
so what you do is a simple trees

1723
01:49:36,980 --> 01:49:39,340
of the weak classifiers themselves

1724
01:49:40,710 --> 01:49:44,130
so actually when we come back to that question OK because i'm going to say

1725
01:49:44,130 --> 01:49:46,420
something about that and then

1726
01:49:46,920 --> 01:49:52,690
we're using the same weak learning algorithm

1727
01:49:52,710 --> 01:49:54,860
but it's different weak classifiers

1728
01:49:54,880 --> 01:50:00,150
so it using the same algorithm to generate a different decision tree and every round

1729
01:50:02,480 --> 01:50:12,770
this example they don't this example the air raids

1730
01:50:12,840 --> 01:50:17,580
using decision trees often converge to something around thirty forty percent

1731
01:50:17,590 --> 01:50:21,210
as they do in this example which means the alpert's are converging to a positive

1732
01:50:21,980 --> 01:50:24,190
not necessarily converging but there

1733
01:50:24,190 --> 01:50:27,440
intending to stay around positive numbers

1734
01:50:36,150 --> 01:50:41,840
well it's way you can think it is discarding them it's more like it's more

1735
01:50:41,840 --> 01:50:47,040
like the entire ensemble the entire combination is converging to something is what seems to

1736
01:50:47,040 --> 01:50:48,690
be happening

1737
01:50:52,500 --> 01:50:54,170
five of

1738
01:51:04,060 --> 01:51:05,560
the district ellis

1739
01:51:05,630 --> 01:51:10,250
right so how do have to go on the training area zero so we're talking

1740
01:51:10,400 --> 01:51:14,500
here about the training area of the combined classifier so

1741
01:51:14,520 --> 01:51:17,630
the individual weak classifiers each have

1742
01:51:17,650 --> 01:51:21,580
positive area which is usually like thirty forty percent

1743
01:51:21,630 --> 01:51:25,400
OK so you can just even though the training of the combined classifier zero

1744
01:51:25,420 --> 01:51:30,790
you can continue with boosting is we can continue to reweight examples generate weak classifiers

1745
01:51:30,790 --> 01:51:32,130
and so on

1746
01:51:32,130 --> 01:51:34,810
that question

1747
01:51:34,840 --> 01:51:40,420
OK good so hopefully you're all intrigued by this apparent paradox this apparent contradiction of

1748
01:51:40,420 --> 01:51:43,460
occam's razor

1749
01:51:52,040 --> 01:51:58,130
a very

1750
01:52:00,860 --> 01:52:03,190
well so so

1751
01:52:03,210 --> 01:52:04,360
OK so

1752
01:52:04,380 --> 01:52:08,650
i'm assuming throughout this talk when talking about generalisation error

1753
01:52:08,650 --> 01:52:12,610
that the training data is coming from the same distribution which is generated in the

1754
01:52:12,630 --> 01:52:13,630
test data

1755
01:52:13,630 --> 01:52:15,940
because otherwise there's really no hope

1756
01:52:15,960 --> 01:52:18,060
saying anything theoretical

1757
01:52:18,710 --> 01:52:23,980
so i guess i don't have a clear intuition of that but i mean at

1758
01:52:23,980 --> 01:52:28,610
this point terms of expecting in terms of expecting this is not what we expect

1759
01:52:28,610 --> 01:52:29,840
at this point

1760
01:52:29,860 --> 01:52:33,860
right so we don't even understand this picture let alone a picture where we're

1761
01:52:33,880 --> 01:52:36,110
the current situation you describe

1762
01:52:36,790 --> 01:52:46,840
so the same as for the correct features available to them

1763
01:52:46,860 --> 01:52:50,560
we were

1764
01:52:50,770 --> 01:52:54,840
we were so weak classifiers we are see the same

1765
01:52:54,900 --> 01:52:56,710
so we have

1766
01:52:56,790 --> 01:53:01,920
overlaps well i i haven't looked at these decision trees but i would expect that

1767
01:53:01,920 --> 01:53:05,250
there would be a lot of overlap between them i mean because we're talking about

1768
01:53:05,250 --> 01:53:06,250
two million

1769
01:53:06,250 --> 01:53:08,290
decision tree nodes so

1770
01:53:08,310 --> 01:53:11,270
you i would expect there would be a lot of overlap between them

1771
01:53:11,290 --> 01:53:17,580
it's not very surprising

1772
01:53:20,150 --> 01:53:25,540
i mean there's computing completely different functions the features are overlapping but the functions are

1773
01:53:25,540 --> 01:53:28,580
completely different in every round

1774
01:53:37,540 --> 01:53:48,540
how do we choose the features for c four point five which is flooding

1775
01:53:58,710 --> 01:54:05,400
which be OK so we're just using c four point five is the black box

1776
01:54:05,440 --> 01:54:08,730
so we're just throwing in all the features that we've been given

1777
01:54:08,750 --> 01:54:11,630
i was just letting c four point five doing with things are just using it

1778
01:54:11,630 --> 01:54:14,230
as a straight black box

1779
01:54:15,190 --> 01:54:20,460
OK we're just running out of boosting exactly the way described

1780
01:54:20,460 --> 01:54:23,080
OK right so we've got this

1781
01:54:23,090 --> 01:54:26,130
this apparent contradiction with occam's razor

1782
01:54:26,170 --> 01:54:30,340
so how can we explain this lack of overfitting

1783
01:54:31,000 --> 01:54:33,610
this is a different story that

1784
01:54:33,610 --> 01:54:37,980
we told this is work with your friend peter bartlett we suddenly

1785
01:54:38,000 --> 01:54:39,610
so the key idea of this

1786
01:54:39,630 --> 01:54:44,000
the area this story is that the training year which we've been looking at so

1787
01:54:44,000 --> 01:54:47,130
far is only telling us part of the story

1788
01:54:47,130 --> 01:54:52,610
it's only measuring which of the training examples are right or wrong

1789
01:54:52,630 --> 01:54:54,520
so instead what we want to do

1790
01:54:54,560 --> 01:54:59,690
is we also want to consider the confidences of those classifications

1791
01:54:59,710 --> 01:55:05,150
so to high level the way the story is going is that with more rounds

1792
01:55:05,150 --> 01:55:10,730
of boosting we're increasing the confidences of the training examples so nothing is happening the

1793
01:55:10,730 --> 01:55:13,090
training area which is only measuring whether

1794
01:55:13,110 --> 01:55:15,630
the predictions are right or wrong

1795
01:55:15,630 --> 01:55:19,610
but the confidences with more rounds of boosting are increasing

1796
01:55:19,690 --> 01:55:23,090
and those confidences those increases in confidence

1797
01:55:23,130 --> 01:55:26,940
are translating into better performance on the test set

1798
01:55:27,000 --> 01:55:29,190
so that the high-level argument

1799
01:55:29,210 --> 01:55:30,730
that we're putting forward

1800
01:55:30,780 --> 01:55:35,130
now to make an argument like that means something we need to have some definition

1801
01:55:35,170 --> 01:55:36,670
of confidence

1802
01:55:36,690 --> 01:55:40,250
so we measure the confidence of our prediction

1803
01:55:41,380 --> 01:55:43,000
this idea of the margin

1804
01:55:44,360 --> 01:55:48,770
we've seen i think you've seen a lot of over the last several days

1805
01:55:48,770 --> 01:55:52,150
but the notion of margin is a little bit different for boosting

1806
01:55:52,170 --> 01:55:56,130
then it is for SVM so let me talk about that here

1807
01:55:56,150 --> 01:55:58,880
so remember that the final combined classifier

1808
01:55:58,880 --> 01:56:02,860
is just the weighted majority vote of weak classifiers

1809
01:56:02,880 --> 01:56:04,790
it's not about

1810
01:56:04,810 --> 01:56:07,250
so if you want to measure the confidence

1811
01:56:07,250 --> 01:56:11,380
in the outcome of an election in actual election

1812
01:56:11,380 --> 01:56:13,020
when you look at is

1813
01:56:13,040 --> 01:56:14,860
the margin you look at

1814
01:56:14,880 --> 01:56:17,790
the difference between the number of people voting for the

1815
01:56:17,790 --> 01:56:21,440
right candidates and the number of people voting for the wrong candidate

1816
01:56:22,710 --> 01:56:27,980
so you can decide which candidates are right or wrong for yourself of course but

1817
01:56:28,000 --> 01:56:32,110
so we can measure confidence in the same way using margin the margin is the

1818
01:56:32,110 --> 01:56:35,610
strength of the vote of those weak classifiers

1819
01:56:35,630 --> 01:56:39,920
so just the fraction really this should be the weight in fractions

1820
01:56:39,940 --> 01:56:43,540
of the weak classifiers voting for the correct label

1821
01:56:43,580 --> 01:56:45,880
minus the weighted fraction

1822
01:56:45,880 --> 01:56:48,170
voting for the incorrect label

1823
01:56:48,170 --> 01:56:50,420
and that's the definition of margin

1824
01:56:50,440 --> 01:56:56,460
so margin under this definition is a real number between minus one in class one

1825
01:56:56,520 --> 01:57:02,090
and if the margin is very close to zero and that's the low confidence predictions

1826
01:57:02,110 --> 01:57:06,840
if it's a positive number then that means that the weighted majority vote

1827
01:57:06,880 --> 01:57:11,210
a weighted majority voting for the correct label so that means the combine classifiers in

1828
01:57:11,210 --> 01:57:13,310
the correct predictions

1829
01:57:13,360 --> 01:57:15,110
it's a negative number

1830
01:57:15,190 --> 01:57:19,630
then that means a majority voting for the incorrect label

1831
01:57:19,630 --> 01:57:24,130
which means that this combined classifiers can an incorrect predictions

1832
01:57:24,150 --> 01:57:25,980
and if the number is

1833
01:57:25,980 --> 01:57:28,420
close to minus one or plus one

1834
01:57:28,500 --> 01:57:33,500
that means that very high confidence prediction which might be correct or incorrect

1835
01:57:34,790 --> 01:57:37,000
OK so what is the evidence

1836
01:57:37,060 --> 01:57:39,590
the margins have anything to do

1837
01:57:39,610 --> 01:57:44,090
with better performance on the test set yes questions

1838
01:57:45,320 --> 01:57:54,480
the weighted sum of the weak classifiers crafted by the

1839
01:57:55,750 --> 01:58:01,500
so to actually be y times backs in the notation i gave earlier

1840
01:58:04,500 --> 01:58:05,730
OK so

1841
01:58:05,730 --> 01:58:09,360
so there are two forms of evidence that we consider one is empirical evidence and

1842
01:58:09,360 --> 01:58:13,610
the others theoretical evidence so let me start with the empirical evidence

1843
01:58:13,610 --> 01:58:15,710
about the happen right so for example

1844
01:58:16,160 --> 01:58:18,320
on that one had here

1845
01:58:18,320 --> 01:58:21,620
this is something that we're trying to figure out what had trainer

1846
01:58:21,640 --> 01:58:27,370
as an attempt to approximate generalizations so the notation convention is usually the thing the

1847
01:58:27,380 --> 01:58:31,610
hat on top of things we're using test all the quantities

1848
01:58:31,680 --> 01:58:35,640
at h had this the hypothesis output by the learning algorithm to try to estimate

1849
01:58:35,640 --> 01:58:39,010
what the functions from x to y x y

1850
01:58:40,070 --> 01:58:46,590
this actually prove something about work on empirical risk minimization will do well this is

1851
01:58:46,600 --> 01:58:55,490
giving us little generalisation which is one here

1852
01:59:01,820 --> 01:59:07,800
in order to prove first learning theory result i'm going to state two lemmas are

1853
01:59:07,870 --> 01:59:11,270
the first is the union bound

1854
01:59:16,030 --> 01:59:28,600
which is the following that that's a one in a they don't want to see

1855
01:59:28,600 --> 01:59:33,470
events i mean you events this is of probabilistic event either happens or not

1856
01:59:33,490 --> 01:59:38,160
on and these are not necessarily independent

1857
01:59:38,190 --> 01:59:49,220
so there's some joint distribution over the events a one through a k and maybe

1858
01:59:49,220 --> 01:59:52,450
they are independent maybe not no assumption that

1859
01:59:54,570 --> 02:00:00,020
on the probability of a one

1860
02:00:00,040 --> 02:00:03,170
o eight two or not

1861
02:00:03,200 --> 02:00:08,080
how to write this as the unions involved

1862
02:00:08,090 --> 02:00:14,300
i had this just means just set notation but probably just before it's probably have

1863
02:00:14,320 --> 02:00:18,050
and one of these events occurring in a one or a two or up to

1864
02:00:18,050 --> 02:00:20,020
eight this is

1865
02:00:20,100 --> 02:00:23,380
this could probably close

1866
02:00:23,710 --> 02:00:30,200
her with her

1867
02:00:33,360 --> 02:00:38,940
the intuition behind this is just you know and not should be seen ben diagrams

1868
02:00:39,420 --> 02:00:43,240
the pictures are probably before we have and when about to do maybe a little

1869
02:00:43,240 --> 02:00:48,380
cryptic so just ignore that just about you haven't seen before we have seen it

1870
02:00:48,390 --> 02:00:51,460
before then this is really

1871
02:00:51,510 --> 02:00:54,220
you know the

1872
02:00:54,240 --> 02:01:00,650
this is really very probably a one union a union three is that have a

1873
02:01:00,650 --> 02:01:05,930
one of the two

1874
02:01:06,090 --> 02:01:11,380
this the right so that the total loss in the union of these three things

1875
02:01:11,380 --> 02:01:15,550
is the to the sum of the masses individuals is not very surprising

1876
02:01:15,940 --> 02:01:20,670
it turns out that depending on how you define a axioms of probability this is

1877
02:01:20,670 --> 02:01:23,990
this is actually one of the axioms of probability theory so failed

1878
02:01:24,000 --> 02:01:26,710
i want to try to prove this is the

1879
02:01:26,740 --> 02:01:27,600
is usually

1880
02:01:27,630 --> 02:01:33,370
written as as as an axiom says six sigma additivity of probability measures this is

1881
02:01:33,370 --> 02:01:35,800
what is sometimes called the flow

1882
02:01:42,040 --> 02:01:46,060
in learning theory is commonly called the union bound such as

1883
02:01:46,070 --> 02:01:51,690
on november i need hardly have been followed

1884
02:01:54,400 --> 02:02:02,130
making i want i want actually prove disastrous fate which is this one of dizzy

1885
02:02:07,080 --> 02:02:08,120
i i d

1886
02:02:08,140 --> 02:02:15,290
on the random variables with mean five

1887
02:02:15,390 --> 02:02:26,180
so this is the only one is equal to phi

1888
02:02:33,600 --> 02:02:38,870
let's say you observe an IID bernouilli random variables want to estimate the mean

1889
02:02:38,890 --> 02:02:44,150
so then defined by hat and this is game that notation no convention i had

1890
02:02:44,150 --> 02:02:50,870
needs and to attempt to is our estimator is an estimator solidifying it had to

1891
02:02:50,870 --> 02:02:56,060
be one over the other one says this

1892
02:02:56,070 --> 02:03:01,500
our intent the mean he's been using random variables by by so average

1893
02:03:04,070 --> 02:03:06,390
and let's any gamma

1894
02:03:06,420 --> 02:03:12,750
on prefix

1895
02:03:16,380 --> 02:03:19,000
the whole thing inequality on

1896
02:03:19,050 --> 02:03:21,010
is that

1897
02:03:21,080 --> 02:03:32,530
the probability your estimate of five

1898
02:03:32,550 --> 02:03:36,310
is more than gamma away from the true value of phi

1899
02:03:36,330 --> 02:03:41,300
the this is bounded by two two gamma

1900
02:03:42,030 --> 02:03:43,810
just some pictures

1901
02:03:43,860 --> 02:03:51,670
so this theorem holds this period this lemma the have been qualities is just the

1902
02:03:51,670 --> 02:03:53,600
statement that this holds true

1903
02:03:53,610 --> 02:03:57,980
but then we now draw cartoons to describe some of the intuition behind the test

1904
02:04:00,350 --> 02:04:05,090
the same exact this is a real number line from zero to one and so

1905
02:04:05,090 --> 02:04:08,030
if i mean if you have any random variables

1906
02:04:09,360 --> 02:04:15,490
you remember for you know whatever some undergraduate probability or statistics across the central limit

1907
02:04:15,490 --> 02:04:18,200
theorem this says that we have a shallow things together

1908
02:04:18,220 --> 02:04:20,460
you tend to get the calcium distribution

1909
02:04:20,580 --> 02:04:26,500
and so on when you toss a coin with bias by we observed is linear

1910
02:04:26,500 --> 02:04:27,680
in the variables

1911
02:04:27,700 --> 02:04:32,870
on any average day then you know the probability distribution

1912
02:04:32,950 --> 02:04:34,590
if i had

1913
02:04:34,660 --> 02:04:44,310
right well rafi peer gaussianity

1914
02:04:44,360 --> 02:04:47,820
it turns out to be a series of was actually the

1915
02:04:47,830 --> 02:04:53,260
cumulative distribution function if i have a conversion of the gaussians the technically phi had

1916
02:04:53,300 --> 02:04:57,930
only taken to the streets set the values on because these factions one of ramses

1917
02:04:58,130 --> 02:05:03,850
doesn't really evidence the but just cartoon figure is the conversion of theta c

1918
02:05:05,180 --> 02:05:09,450
so what have been called he says is that if you think about the gamma

1919
02:05:09,470 --> 02:05:15,080
that's one interval gamma i guess in the interval gamma

1920
02:05:15,100 --> 02:05:19,020
the same that the probability mass the hills

1921
02:05:19,030 --> 02:05:21,500
in other words the probability that

1922
02:05:21,550 --> 02:05:26,580
my value if i had to small then gamma away from the true value

1923
02:05:26,590 --> 02:05:29,580
the total mass

1924
02:05:30,300 --> 02:05:33,090
that year

1925
02:05:33,110 --> 02:05:37,510
tell the probability mass in these tales is at most two

1926
02:05:37,510 --> 02:05:41,060
e to make down the grid

1927
02:05:41,080 --> 02:05:42,990
that's what they have to equal

1928
02:05:43,010 --> 02:05:45,790
so we can read that this is this is just the right hand side of

1929
02:05:45,790 --> 02:05:47,920
the pond to you to to ask

1930
02:05:48,080 --> 02:05:52,080
so bounds the probability that you make a mistake in estimating the

1931
02:05:52,090 --> 02:05:53,680
random variable

1932
02:05:53,710 --> 02:05:58,590
and the

1933
02:05:58,600 --> 02:05:59,670
cool thing

1934
02:05:59,690 --> 02:06:06,280
about this bound is thinking behind these boundaries that form the easiest explanation the in

1935
02:06:06,290 --> 02:06:09,360
to so it says if fixed value gamma

1936
02:06:09,420 --> 02:06:13,120
as you increase the size of the training set is it has to climb one

