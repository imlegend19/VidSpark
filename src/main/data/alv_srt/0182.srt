1
00:00:00,000 --> 00:00:03,670
maybe that's not so into intuitively

2
00:00:03,760 --> 00:00:09,290
you can work that out for yourself and ada one problem

3
00:00:09,330 --> 00:00:11,040
what it means is

4
00:00:11,060 --> 00:00:17,020
that you can define this as actually equals zero ignore gravity completely

5
00:00:17,080 --> 00:00:19,480
and set of differential equation

6
00:00:19,500 --> 00:00:21,330
as if there was no gravity

7
00:00:21,400 --> 00:00:25,750
this is actually full zero so you off over a distance AX

8
00:00:25,770 --> 00:00:27,710
from the equilibrium position

9
00:00:27,760 --> 00:00:31,890
we only allow for and bring forth minus kx

10
00:00:31,930 --> 00:00:33,630
and everything works

11
00:00:33,630 --> 00:00:35,210
and of course you should be able to

12
00:00:35,230 --> 00:00:42,020
to prove that that is correct

13
00:00:42,110 --> 00:00:46,040
every spring oscillate in simple harmonic fashion

14
00:00:46,080 --> 00:00:47,740
and we have such as spring here

15
00:00:47,760 --> 00:00:49,200
because if you can do

16
00:00:49,210 --> 00:00:51,360
we favor and get it up here

17
00:00:51,400 --> 00:00:53,750
then i should be able to demonstrate

18
00:00:53,810 --> 00:00:55,770
that's a

19
00:00:55,830 --> 00:00:59,800
uniform circular motion

20
00:01:01,580 --> 00:01:03,090
on the wall

21
00:01:03,130 --> 00:01:06,220
called shadow projection

22
00:01:06,240 --> 00:01:08,940
you should be able thank you more calls

23
00:01:08,990 --> 00:01:11,980
should be able to

24
00:01:12,070 --> 00:01:13,550
have the same motion

25
00:01:13,560 --> 00:01:14,950
as much grain

26
00:01:15,010 --> 00:01:16,900
provided of course

27
00:01:16,940 --> 00:01:19,140
that's very very carefully

28
00:01:19,140 --> 00:01:22,440
make the period of oscillation of this brain

29
00:01:22,500 --> 00:01:24,840
exactly the same as the time

30
00:01:24,840 --> 00:01:28,300
for this object to go around

31
00:01:28,360 --> 00:01:31,300
we don't shadow projected on their

32
00:01:31,350 --> 00:01:35,100
and then i will even try to these this one

33
00:01:35,170 --> 00:01:38,810
it's very difficult at the same time that this one is here

34
00:01:38,860 --> 00:01:40,410
and what you will see that

35
00:01:40,440 --> 00:01:42,150
you will see the uniform

36
00:01:42,150 --> 00:01:44,730
circular motion projected

37
00:01:44,750 --> 00:01:48,180
becomes the simple harmonic motion and you'll see this point

38
00:01:48,230 --> 00:01:50,680
simple one

39
00:01:50,810 --> 00:01:53,460
so i will try to do that in

40
00:01:53,540 --> 00:02:01,720
shadow projection will make it to the doctor

41
00:02:01,740 --> 00:02:03,980
and for that i need some

42
00:02:03,990 --> 00:02:07,710
light here

43
00:02:14,600 --> 00:02:18,680
half OK

44
00:02:18,710 --> 00:02:22,490
and we also want to the so you see the spring and there you see

45
00:02:22,490 --> 00:02:24,500
this object

46
00:02:24,550 --> 00:02:28,100
which is rotating in a circle

47
00:02:28,150 --> 00:02:32,060
but you think it's as simple harmonic motion that's of course my objection

48
00:02:32,070 --> 00:02:34,460
my objective

49
00:02:34,520 --> 00:02:35,340
so now

50
00:02:35,370 --> 00:02:37,960
that is difficult i will have to block

51
00:02:38,050 --> 00:02:39,660
you for a few seconds

52
00:02:39,680 --> 00:02:42,610
i will try now to realize this

53
00:02:42,650 --> 00:02:43,720
the same time

54
00:02:43,740 --> 00:02:47,220
and also at the same amplitude

55
00:02:50,470 --> 00:02:56,620
wasn't my best day was it

56
00:03:05,900 --> 00:03:09,160
this is perhaps the best i can do today

57
00:03:10,290 --> 00:03:13,860
don't go exactly next to each other but you see have the same period and

58
00:03:13,860 --> 00:03:15,430
they both are present

59
00:03:15,450 --> 00:03:17,830
simple harmonic oscillation this spring

60
00:03:17,830 --> 00:03:20,160
because we just calculated that

61
00:03:20,200 --> 00:03:22,410
and the projection of the uniform

62
00:03:22,440 --> 00:03:33,310
circular motion

63
00:03:34,740 --> 00:03:36,830
if we

64
00:03:36,860 --> 00:03:39,910
return to the spring and maybe we should remove this

65
00:03:39,950 --> 00:03:43,180
if we return to the spring

66
00:03:43,200 --> 00:03:46,560
then we have the period it for scoring system

67
00:03:46,570 --> 00:03:48,050
which is two pi

68
00:03:48,060 --> 00:03:50,390
times square root of m k

69
00:03:51,550 --> 00:03:53,710
and i want to bring this

70
00:03:53,810 --> 00:03:57,160
two attest to quantitative test

71
00:03:57,160 --> 00:04:00,120
how accurate this is

72
00:04:00,180 --> 00:04:02,260
i'm going to double the mass

73
00:04:02,270 --> 00:04:04,310
i'm going to hang on that spring

74
00:04:04,310 --> 00:04:06,400
going to measure the period

75
00:04:06,450 --> 00:04:09,600
and then i want

76
00:04:09,640 --> 00:04:13,040
the mass which is twice as high i want that period

77
00:04:14,410 --> 00:04:16,580
the square root of two times higher

78
00:04:16,700 --> 00:04:19,740
that's what this equation predict

79
00:04:19,800 --> 00:04:23,350
now whenever you want to do the measurement in physics

80
00:04:23,400 --> 00:04:26,800
well you want to compare numbers you have a certain

81
00:04:26,850 --> 00:04:28,400
goal in mind

82
00:04:28,410 --> 00:04:33,650
measurement without the uncertainty in the measurement is completely meaningless

83
00:04:33,650 --> 00:04:35,120
you must know

84
00:04:35,160 --> 00:04:39,310
the accuracy of your measurement

85
00:04:39,340 --> 00:04:41,150
o and one

86
00:04:41,170 --> 00:04:44,160
five hundred

87
00:04:44,160 --> 00:04:46,860
the bottom line is o point two grams

88
00:04:48,900 --> 00:04:51,080
it's thousand

89
00:04:51,160 --> 00:04:55,140
plus and minus o point two grams that's the best we can do

90
00:04:55,160 --> 00:04:58,110
that's an extremely small error

91
00:04:58,120 --> 00:05:02,550
this is an error of only o point o four percent

92
00:05:02,550 --> 00:05:10,150
categorize different anomaly detection techniques according to different criteria

93
00:05:13,280 --> 00:05:15,920
let's start from the left

94
00:05:17,110 --> 00:05:18,590
we have this

95
00:05:18,600 --> 00:05:21,020
point anomaly detection many times to

96
00:05:21,050 --> 00:05:22,860
basically find

97
00:05:22,950 --> 00:05:25,940
individual data records as anomalous and basically

98
00:05:25,950 --> 00:05:30,650
most of the policies for dealing with here also tried to convert them into this

99
00:05:30,650 --> 00:05:32,780
problem because it's much easier to do

100
00:05:33,640 --> 00:05:36,030
you have this classification based techniques

101
00:05:36,080 --> 00:05:38,300
then you basically have

102
00:05:38,310 --> 00:05:41,480
both normal and abnormal data available

103
00:05:41,500 --> 00:05:44,580
you can sort nearest neighbor based techniques

104
00:05:44,660 --> 00:05:50,450
and then you try to detect anomalies based computing the distances between data records

105
00:05:50,550 --> 00:05:54,310
clustering basic things are quite similar to this

106
00:05:54,510 --> 00:05:57,190
you have different statistical techniques which obeys

107
00:05:57,220 --> 00:05:59,500
it's don't parametric and nonparametric

108
00:05:59,510 --> 00:06:03,340
well statistical statistical approaches

109
00:06:03,420 --> 00:06:08,660
and you have some other techniques which include information theory based spectral composition some people

110
00:06:08,660 --> 00:06:13,480
try different visualization techniques and so on and of course we the national this collect

111
00:06:13,480 --> 00:06:16,110
contextual in collective action

112
00:06:16,120 --> 00:06:18,010
if you have real

113
00:06:18,070 --> 00:06:22,950
time system you have to have an online anomaly detection and sometimes

114
00:06:23,050 --> 00:06:25,160
the data the data is

115
00:06:25,180 --> 00:06:26,960
dispersed along

116
00:06:27,930 --> 00:06:29,530
physical locations

117
00:06:29,550 --> 00:06:32,530
you have to deal with distributed anomaly detection

118
00:06:32,590 --> 00:06:34,880
sometimes you can not merge the data

119
00:06:36,590 --> 00:06:40,410
have to deal with you know how to make this global anomalies don't we don't

120
00:06:40,570 --> 00:06:43,860
know what talk briefly about this idea

121
00:06:43,920 --> 00:06:47,660
any questions so far

122
00:06:47,680 --> 00:06:53,690
OK good

123
00:06:55,030 --> 00:06:59,250
i'll start with explaining briefly the classification basically

124
00:06:59,270 --> 00:07:02,210
as i mentioned earlier

125
00:07:02,260 --> 00:07:05,230
the main idea is to build a classification

126
00:07:05,310 --> 00:07:07,970
it model for both normal

127
00:07:08,750 --> 00:07:12,920
anomalous behavior so basically you assuming here that you have

128
00:07:12,940 --> 00:07:17,470
lower the labels from both normal and anomalous behavior available

129
00:07:17,520 --> 00:07:20,210
so of

130
00:07:21,470 --> 00:07:25,160
what does it use here must be able to handle this kind of skewed

131
00:07:25,180 --> 00:07:27,010
the data distribution

132
00:07:27,070 --> 00:07:28,200
for example

133
00:07:28,220 --> 00:07:30,150
if you have ninety nine percent of

134
00:07:30,160 --> 00:07:33,260
data the correspond to normal and only one percent correspond

135
00:07:33,320 --> 00:07:36,400
two anomalies using simple decision tree may not give you

136
00:07:36,480 --> 00:07:40,250
very good results because using these techniques

137
00:07:41,730 --> 00:07:43,330
designed for

138
00:07:45,700 --> 00:07:49,920
so in this case we are dealing with two types of approaches supervised and semi

139
00:07:49,920 --> 00:07:52,640
supervised classification techniques

140
00:07:52,640 --> 00:07:53,750
in supervised

141
00:07:53,880 --> 00:07:55,800
basically solving bought

142
00:07:55,810 --> 00:07:58,990
labels available both from normal and

143
00:07:59,930 --> 00:08:01,000
and you have to

144
00:08:02,980 --> 00:08:05,660
costs are different types of classifiers

145
00:08:07,550 --> 00:08:09,720
between normal and anomalous

146
00:08:09,780 --> 00:08:14,340
and finally in the summer surprised you're assuming there is only a label from the

147
00:08:14,340 --> 00:08:16,120
normal behavior

148
00:08:16,120 --> 00:08:21,570
available and then using some kind of one class model

149
00:08:21,650 --> 00:08:23,220
define what is normal

150
00:08:23,270 --> 00:08:27,100
and then to detect deviations from this long beach area

151
00:08:29,060 --> 00:08:30,850
four numbers so

152
00:08:30,890 --> 00:08:34,870
basically you get the data from the normally you're trying to construct the model that

153
00:08:34,870 --> 00:08:37,430
describes this more will be shown here

154
00:08:37,450 --> 00:08:42,100
and then using this the model to go to the new data everything that fits

155
00:08:42,100 --> 00:08:45,570
this model would correspond to only have everything else

156
00:08:45,620 --> 00:08:49,740
should be anomaly

157
00:08:57,840 --> 00:09:00,760
so obvious advantages of these techniques are the

158
00:09:00,850 --> 00:09:05,940
one can be easy to understand because you know exactly what is normal it is

159
00:09:07,970 --> 00:09:10,240
since we have both normal and anomaly

160
00:09:10,250 --> 00:09:13,660
you can achieve pretty high accuracy in detecting this event

161
00:09:16,260 --> 00:09:24,830
in semi supervised techniques also the normal behaviour since you know what is normal can

162
00:09:24,830 --> 00:09:25,830
be actually

163
00:09:25,840 --> 00:09:30,670
very well learned and you can use this description of all bishops to the fact

164
00:09:30,670 --> 00:09:32,260
future anomaly

165
00:09:33,830 --> 00:09:38,340
kind of disadvantages include in both cases you require

166
00:09:38,440 --> 00:09:39,660
the labels

167
00:09:39,710 --> 00:09:44,140
in supervised case require both labels from normal anomaly as i was surprised acquired labels

168
00:09:44,140 --> 00:09:45,370
from test

169
00:09:45,410 --> 00:09:50,930
and in real life applications this is sometimes not visible because the labelled data

170
00:09:51,010 --> 00:09:51,920
let's say you have

171
00:09:51,930 --> 00:09:54,930
millions of data records in it some time

172
00:09:57,590 --> 00:10:02,150
the drawback of surprise discussion take techniques can usually

173
00:10:02,180 --> 00:10:05,030
they cannot detect this emerging new

174
00:10:05,050 --> 00:10:08,470
anomalous because they know what is normal anomaly and everything

175
00:10:08,490 --> 00:10:13,150
outside the range they just belonging to normal anomaly depending on the closeness

176
00:10:13,150 --> 00:10:19,390
we are not building characteristic descriptions but we are building discriminating descriptions with which we

177
00:10:19,420 --> 00:10:26,050
want to select such attributes which discriminate the best between the classes

178
00:10:26,050 --> 00:10:31,620
so rule set representation we have a set of rules

179
00:10:31,640 --> 00:10:34,660
it is the disjunctive set of conjunctive rules

180
00:10:34,810 --> 00:10:38,150
in the form if condition then class

181
00:10:38,170 --> 00:10:42,260
we can also write it differently like and

182
00:10:42,260 --> 00:10:45,840
plus if condition or conditions if class

183
00:10:45,850 --> 00:10:48,520
sometimes to use this

184
00:10:49,550 --> 00:10:54,090
arrow representation sometimes we say if then we have seen

185
00:10:54,100 --> 00:10:55,240
let's see if

186
00:10:55,240 --> 00:10:59,280
of course on communities normal then played tennis yes

187
00:10:59,290 --> 00:11:01,320
so that's the class

188
00:11:01,390 --> 00:11:06,190
these are the attribute values which in the condition of attribute values

189
00:11:06,600 --> 00:11:10,840
it represents the condition part of the rule and the conclusion part of the rule

190
00:11:11,110 --> 00:11:13,310
is the class

191
00:11:13,310 --> 00:11:17,490
and this is the set of tools for a particular class

192
00:11:17,590 --> 00:11:26,600
this is the simplified rule representation actually we can have if

193
00:11:27,710 --> 00:11:31,650
in the form of a conjunction of attribute value then

194
00:11:31,700 --> 00:11:32,610
a certain

195
00:11:33,880 --> 00:11:38,200
i not of the set of all the possible classes so it is not necessary

196
00:11:39,350 --> 00:11:42,730
for these conditions only the

197
00:11:42,770 --> 00:11:44,620
these conditions would cover

198
00:11:44,680 --> 00:11:47,050
just examples of one class

199
00:11:47,060 --> 00:11:48,840
they may cover

200
00:11:48,860 --> 00:11:54,110
examples of different classes then the majority class will be just assigned to the

201
00:11:54,120 --> 00:11:56,100
conclusion part of the rule

202
00:11:56,110 --> 00:12:01,910
and here we would say what what what what what is the distribution of classes

203
00:12:01,910 --> 00:12:06,320
what are the other classes for which the rule holds

204
00:12:06,320 --> 00:12:10,610
and then finally when once we have a set of rules this rule one rule

205
00:12:10,620 --> 00:12:15,210
two rule three will have a set of all these rules and in addition we

206
00:12:15,210 --> 00:12:19,930
may put a default rule next to it by default rule is

207
00:12:19,980 --> 00:12:24,540
and it in order to improve the classification accuracy of the set of rules

208
00:12:24,600 --> 00:12:27,420
so let's again look at the data

209
00:12:27,450 --> 00:12:29,860
this is again the customer data

210
00:12:29,880 --> 00:12:32,100
and we could say well

211
00:12:32,110 --> 00:12:34,530
we could use the following rules

212
00:12:34,540 --> 00:12:37,710
if income is larger than so and so much then

213
00:12:37,740 --> 00:12:40,580
the person is the big spender

214
00:12:41,650 --> 00:12:48,120
there's another rule for big spender and there are three rules for not big spenders

215
00:12:48,120 --> 00:12:52,300
here and in addition we can add the default rules

216
00:12:52,300 --> 00:12:54,520
which is as big spender yes

217
00:12:54,530 --> 00:13:00,590
so this defaults rule simply says that the majority of the examples

218
00:13:00,640 --> 00:13:02,310
which have

219
00:13:02,570 --> 00:13:06,270
in the set of instances has

220
00:13:06,830 --> 00:13:08,230
the following class

221
00:13:08,250 --> 00:13:09,510
a third

222
00:13:09,520 --> 00:13:13,840
in some approaches the default rule just says

223
00:13:13,900 --> 00:13:16,520
what is the majority class

224
00:13:16,520 --> 00:13:21,630
in the set of examples which have not been covered by the rules

225
00:13:21,690 --> 00:13:25,600
explicit rules above the default rule

226
00:13:26,360 --> 00:13:30,940
when using the rules for classification it can occur that none of the rules would

227
00:13:30,940 --> 00:13:35,850
cover the new instance which we want to classify therefore in order that every instance

228
00:13:35,850 --> 00:13:39,990
is covered we have to add a default rule

229
00:13:39,990 --> 00:13:45,010
so the what does it mean that the instances covered

230
00:13:45,020 --> 00:13:51,150
that is an instance or an example which we would like to classify that satisfies

231
00:13:51,150 --> 00:13:55,600
the conditions of the rule then the instance is covered by the rule

232
00:13:55,630 --> 00:13:57,840
if none of the a

233
00:13:57,840 --> 00:14:02,300
none of the rules covers an instance then the default rule applies

234
00:14:02,300 --> 00:14:08,390
and this provides us with the classification into the majority class

235
00:14:08,410 --> 00:14:11,150
for the conductance example

236
00:14:11,160 --> 00:14:14,380
we look at the different representations

237
00:14:14,390 --> 00:14:18,910
we have again rules like production is reduced

238
00:14:18,950 --> 00:14:21,380
then contact lens no

239
00:14:23,370 --> 00:14:25,790
here we have the class distribution

240
00:14:25,790 --> 00:14:28,610
over the set of training examples

241
00:14:28,640 --> 00:14:33,480
zero examples of class soft lenses zero examples of

242
00:14:33,510 --> 00:14:36,920
plus hard lenses and and twelve examples

243
00:14:36,990 --> 00:14:40,820
of class no lenses in the training set

244
00:14:41,910 --> 00:14:43,100
however here

245
00:14:43,120 --> 00:14:44,710
this rule

246
00:14:44,740 --> 00:14:47,580
is not such a clear clean rule

247
00:14:47,690 --> 00:14:51,450
although the conclusions soft lenses actually

248
00:14:51,460 --> 00:14:53,270
this rule satisfies

249
00:14:53,280 --> 00:14:59,930
five instances of class soft zero instances of class heart and one instance of class

250
00:15:01,380 --> 00:15:05,870
here you could or to say that well before in order to

251
00:15:05,900 --> 00:15:07,170
i have a rule which is

252
00:15:07,180 --> 00:15:07,870
i mean

253
00:15:07,880 --> 00:15:12,370
which doesn't cover which only covers instances of a particular class

254
00:15:12,380 --> 00:15:14,120
then we need to be

255
00:15:14,350 --> 00:15:19,300
another attribute value added to the conjunction of attribute values

256
00:15:19,330 --> 00:15:22,670
but such rules could overfitting the data

257
00:15:22,690 --> 00:15:28,490
therefore the learning algorithm has decided not to add one additional condition

258
00:15:28,520 --> 00:15:33,480
to the rule but rather to leave a probabilistic distribution of classes in the conclusion

259
00:15:33,840 --> 00:15:35,840
and assigned the majority class

260
00:15:35,870 --> 00:15:39,960
so you will see that in these rules can be

261
00:15:40,040 --> 00:15:41,010
i used

262
00:15:41,080 --> 00:15:43,070
as a classifier either

263
00:15:43,150 --> 00:15:45,130
it's hard rules like

264
00:15:45,150 --> 00:15:47,100
assigning this class

265
00:15:47,150 --> 00:15:52,100
if the condition is satisfied or as probabilistic rules so in this case

266
00:15:52,120 --> 00:15:58,290
we would have zero probability of class heart and five divided by six probability four

267
00:15:58,290 --> 00:16:02,030
class after and one divided by six probability of class

268
00:16:03,180 --> 00:16:05,080
so the rules can be

269
00:16:05,080 --> 00:16:12,680
either probabilistic or hard so most of rule learning algorithms deal with probabilistic rules and

270
00:16:12,690 --> 00:16:14,960
with probabilistic classification

271
00:16:15,020 --> 00:16:22,080
even one example could be classified by different rules into different classes again probabilistic classification

272
00:16:22,200 --> 00:16:23,650
would be used

273
00:16:23,680 --> 00:16:30,120
so this is in the case when we have an unordered rules and we could

274
00:16:30,120 --> 00:16:31,680
have on the other hand

275
00:16:31,700 --> 00:16:37,920
ordered list of rules that will be shown later so in on ordered rule sets

276
00:16:37,920 --> 00:16:40,920
which was the case in this the first examples

277
00:16:40,940 --> 00:16:42,480
the rules

278
00:16:42,490 --> 00:16:46,280
classifica nditions is led by first determining the class

279
00:16:46,300 --> 00:16:49,170
and then the conditions which are put

280
00:16:49,190 --> 00:16:51,250
into the body

281
00:16:54,160 --> 00:17:00,930
in this sense i think you could say well their order because they are consecutively

282
00:17:00,930 --> 00:17:04,750
and if it is different names to different things there is called that way of

283
00:17:04,750 --> 00:17:07,040
stating the unique name assumption in how

284
00:17:07,200 --> 00:17:10,140
because that you make the same assumption that there is no such thing for the

285
00:17:10,140 --> 00:17:15,450
closed world assumption it was debated in the working group because clearly people wanted but

286
00:17:15,470 --> 00:17:20,430
there's no it was not well understood enough we should do it properly

287
00:17:21,470 --> 00:17:23,830
so you write should be my

288
00:17:26,450 --> 00:17:28,500
you have some simple piece of all

289
00:17:28,520 --> 00:17:32,160
you just described communities mentioned class class professors

290
00:17:32,270 --> 00:17:36,890
and the descendants of this classes saying this is class with that name

291
00:17:37,290 --> 00:17:41,600
and it's a partial definition and is also closely

292
00:17:42,140 --> 00:17:45,140
so this is not everything we know about associate professors because it's part of the

293
00:17:45,220 --> 00:17:49,640
nation but what we do know is that associate professors are subclasses of academic staff

294
00:17:49,640 --> 00:17:56,310
members a very simple subclass of statement and we could say that i associate professor

295
00:17:56,310 --> 00:17:58,620
assistant professor disjointclasses

296
00:17:58,640 --> 00:18:01,450
i the same for professors and associate professors

297
00:18:01,470 --> 00:18:10,580
they don't overlap and we can also give complete definition is particularly simple definition namely

298
00:18:10,620 --> 00:18:15,120
faculty the class faculty is completely defined by saying that the government is saying these

299
00:18:15,120 --> 00:18:16,680
two are equal

300
00:18:16,700 --> 00:18:18,180
last night

301
00:18:18,560 --> 00:18:20,870
so this is all very efficient

302
00:18:20,890 --> 00:18:27,370
this is the abstract syntax is that yes and there's a bit of software but

303
00:18:27,370 --> 00:18:30,250
i have it's of you can run locally or because of you can run over

304
00:18:30,250 --> 00:18:36,020
the web that translates in and out of these in the RDF XML syntax

305
00:18:36,120 --> 00:18:38,500
here's some more interesting cases so for example

306
00:18:38,930 --> 00:18:45,350
we have a property a has arranged nonnegative integers which is reasonable for age and

307
00:18:45,560 --> 00:18:48,450
these data types are stolen from the XML schema

308
00:18:48,470 --> 00:18:53,750
they and so we we have the property is still by

309
00:18:53,790 --> 00:19:00,270
and it's domain is cause the range academic staff members and subpropertyof involves

310
00:19:00,330 --> 00:19:04,850
so you can be involved in the course and being involved as a teacher

311
00:19:04,910 --> 00:19:06,250
both court

312
00:19:06,450 --> 00:19:11,750
and you could say well teachers is the inverse property of told by

313
00:19:11,940 --> 00:19:16,180
the property and of course the domain and range

314
00:19:16,200 --> 00:19:17,040
in the

315
00:19:17,270 --> 00:19:21,750
you could say that two properties are equivalent what your lectures in which he teaches

316
00:19:21,750 --> 00:19:23,580
maybe there could be properties

317
00:19:24,600 --> 00:19:29,700
you could say that the same great as is transitive and symmetric property

318
00:19:29,950 --> 00:19:37,160
which arrange the main student two students can have the same gradient as the transition

319
00:19:37,160 --> 00:19:41,430
this is the kind of notation and you could imagine just typed message or writing

320
00:19:41,450 --> 00:19:43,270
on the blackboard

321
00:19:43,700 --> 00:19:46,060
the sort of like

322
00:19:46,080 --> 00:19:46,890
it's not

323
00:19:47,540 --> 00:19:50,770
whatever came simple enough

324
00:19:50,870 --> 00:19:52,680
some people say list like that

325
00:19:52,720 --> 00:19:58,870
OK so it is that we talked about classes we don't know

326
00:19:58,910 --> 00:20:00,470
visual cells

327
00:20:00,490 --> 00:20:04,470
this is a particularly simple case and this is the name of the individual and

328
00:20:04,470 --> 00:20:07,120
the individual is an instance of the class lecture

329
00:20:07,140 --> 00:20:09,660
and this was in the idea way of spelling instance of

330
00:20:09,790 --> 00:20:16,000
he is more interesting case where this individual is is not only instance of the

331
00:20:16,000 --> 00:20:21,200
class academicstaffmember but for the property age it has the value thirty nine

332
00:20:21,370 --> 00:20:25,350
and it's important to be data typed literals so it's not just the string theory

333
00:20:25,370 --> 00:20:29,810
nine but this is just the string thirty nine properties and in

334
00:20:29,870 --> 00:20:36,620
OK you could say that is to advise functional so you have at most one

335
00:20:36,620 --> 00:20:39,560
teacher for every course

336
00:20:39,580 --> 00:20:42,370
and if you say that this individual

337
00:20:42,390 --> 00:20:44,890
these course

338
00:20:44,910 --> 00:20:46,370
and it just

339
00:20:47,000 --> 00:20:51,250
for course and it stored by this individual and installed by this individual and we

340
00:20:51,250 --> 00:20:55,620
know that told by the functional properties and therefore to individuals must be the same

341
00:20:57,040 --> 00:20:58,470
by inference

342
00:20:58,750 --> 00:21:04,060
OK if we nevertheless say that these two are different individuals which would that we

343
00:21:04,060 --> 00:21:05,580
have an inconsistency

344
00:21:05,600 --> 00:21:09,870
so then if i've written all this stuff and i've written something

345
00:21:09,870 --> 00:21:11,200
all of it can't be true

346
00:21:11,270 --> 00:21:13,310
this is detectable

347
00:21:13,330 --> 00:21:17,870
and LDL is complete in the sense that if you write down something that's inconsistent

348
00:21:17,870 --> 00:21:19,430
the readers will find

349
00:21:19,680 --> 00:21:23,330
and this is the kullback way of state was should not just saying to the

350
00:21:23,430 --> 00:21:28,720
individuals different but the whole bunch of individuals there here there could be many more

351
00:21:28,770 --> 00:21:33,350
useful individuals your entire domain in one big different individual statement

352
00:21:33,390 --> 00:21:34,770
he said there are different

353
00:21:34,870 --> 00:21:36,560
making unique name assumption

354
00:21:36,600 --> 00:21:39,810
and they you're back database level

355
00:21:39,870 --> 00:21:43,410
so here some more interesting class definitions

356
00:21:44,700 --> 00:21:48,160
this is the ideal university were first year courses

357
00:21:48,180 --> 00:21:51,270
this restriction that they are told by

358
00:21:54,230 --> 00:21:58,370
taught by all the values from this law must be of the class professor

359
00:21:58,370 --> 00:22:05,560
and is also strange university because of the the any math courses and in the

360
00:22:05,560 --> 00:22:11,450
classical mathematics courses on mathematics course as this reaction is my property that

361
00:22:11,520 --> 00:22:15,410
it is the value of this individuals all the mathematics courses are taught by the

362
00:22:15,410 --> 00:22:16,700
same person

363
00:22:22,910 --> 00:22:25,810
this is saying that academic staff members

364
00:22:28,200 --> 00:22:31,680
teach some undergraduate courses you could almost

365
00:22:32,000 --> 00:22:38,350
it is literally and this is that courses are taught by at least one person

366
00:22:38,370 --> 00:22:40,720
minimal cardinality one

367
00:22:40,770 --> 00:22:43,410
so you must have value for

368
00:22:43,470 --> 00:22:44,660
is thought by

369
00:22:45,720 --> 00:22:49,230
and you can of course combine these things so you can say well department the

370
00:22:49,230 --> 00:22:54,750
class department is partially defined by the fact that is minimally ten members maximally thirty

371
00:22:56,370 --> 00:22:58,680
and reason

372
00:22:59,220 --> 00:23:06,140
partial means that these are some of the defining properties but not all of them

373
00:23:07,430 --> 00:23:11,230
you are i think that this is a subclass of the class implicitly defined by

374
00:23:11,230 --> 00:23:12,220
the convention

375
00:23:12,560 --> 00:23:19,720
if you put complete here they you can still add further causes but these clauses

376
00:23:19,730 --> 00:23:20,870
they must then

377
00:23:20,890 --> 00:23:27,290
already implied by your definition and if that's not the case you because it's so

378
00:23:27,310 --> 00:23:30,580
this the default is that you give partial definitions

379
00:23:30,600 --> 00:23:33,520
OK and if you want to four difference to be closed

380
00:23:33,520 --> 00:23:34,910
it's a complete

381
00:23:34,930 --> 00:23:40,100
and nobody else can change the information they can add further clauses for these tools

382
00:23:40,140 --> 00:23:44,140
can only be because it already applied the the original definition

383
00:23:44,140 --> 00:23:46,120
and if it's not the case and they don't something

384
00:23:46,120 --> 00:23:48,170
OK so the

385
00:23:48,190 --> 00:23:56,650
giving five parts is kind of general introduction to statistical machine translation and also the

386
00:23:56,650 --> 00:24:02,530
issue of evaluation for machine translation which is somewhat open issues

387
00:24:02,540 --> 00:24:05,190
and about how to you the decoding

388
00:24:05,210 --> 00:24:10,500
with phrase based models how do you learn phrase based models and then

389
00:24:11,000 --> 00:24:15,410
all we have time for that two more research she kind of things to work

390
00:24:15,410 --> 00:24:21,900
that we currently do in edinburgh on factored translation models and this kind of training

391
00:24:23,770 --> 00:24:25,980
so i think the machine translation as

392
00:24:26,030 --> 00:24:27,510
it's very easy to use

393
00:24:27,520 --> 00:24:28,950
i understood problems

394
00:24:28,960 --> 00:24:32,600
so you get things like this and you try to make sense of it try

395
00:24:32,660 --> 00:24:35,590
to figure out what does that mean i have no idea what that means i

396
00:24:35,590 --> 00:24:38,220
could that have been done at some point

397
00:24:38,240 --> 00:24:40,140
still would explain it to me

398
00:24:40,160 --> 00:24:45,540
so so this is one of the oldest problems in artificial intelligence and artificial intelligence

399
00:24:45,540 --> 00:24:49,980
research came up as a as a topic how can we build intelligent machines

400
00:24:49,990 --> 00:24:53,440
machine translation was considered one of the problems

401
00:24:53,820 --> 00:24:56,230
to tackle besides playing chess

402
00:24:56,280 --> 00:24:59,120
and to do really well

403
00:24:59,130 --> 00:25:04,360
the general assumption is that you need to do all the hard things in artificial

404
00:25:04,360 --> 00:25:05,840
intelligence you need to

405
00:25:05,860 --> 00:25:07,440
no one world knowledge

406
00:25:07,450 --> 00:25:13,060
you know what semantics about what things mean what what what does everything because they

407
00:25:13,080 --> 00:25:17,120
always some translation cases where you really need to know what you're talking about

408
00:25:17,140 --> 00:25:20,260
we try to do something much simpler

409
00:25:20,280 --> 00:25:23,600
because we don't know how to do all these things but we try to get

410
00:25:23,600 --> 00:25:27,970
as far as you want so we would be doing is inspired by the story

411
00:25:27,970 --> 00:25:31,480
of the rosetta stone that you might have heard about

412
00:25:31,490 --> 00:25:35,910
so the egyptian language was a long time in the mystery now we know what

413
00:25:35,910 --> 00:25:37,480
it meant

414
00:25:37,490 --> 00:25:44,180
they had all these funny symbols of i is some camels what i know

415
00:25:44,200 --> 00:25:49,240
and it probably was some some of that but nobody could encode

416
00:25:49,250 --> 00:25:54,860
until seventeen ninety nine someone found in the desert the stone that had one part

417
00:25:54,860 --> 00:26:00,840
egyptian one part cream greek and other parts of language but

418
00:26:00,890 --> 00:26:05,070
and that enable them to people to decode

419
00:26:05,110 --> 00:26:08,360
the egyptian alphabet because you just look at the egyptian you could look at the

420
00:26:08,360 --> 00:26:13,890
corresponding greek you so all you have another greek word occurs and that different currents

421
00:26:13,900 --> 00:26:17,430
and if you do that for awhile you can really in the world the words

422
00:26:17,430 --> 00:26:19,730
suggestion i

423
00:26:20,470 --> 00:26:21,500
what we are

424
00:26:21,510 --> 00:26:25,010
in the simulation of right now is that we have a similar situation where the

425
00:26:25,010 --> 00:26:28,210
computer can discover automatically how to translate

426
00:26:28,260 --> 00:26:33,650
so we have now available to us a lot of texts in electronic form

427
00:26:33,690 --> 00:26:36,230
thanks to the internet and all the other answers

428
00:26:36,250 --> 00:26:38,470
that computers can analyse

429
00:26:40,180 --> 00:26:45,390
so for some language pairs we have on the order of hundreds of millions of

430
00:26:45,390 --> 00:26:47,500
words translated

431
00:26:47,520 --> 00:26:52,410
even before like spanish english not hard to get hundreds of millions of words and

432
00:26:52,410 --> 00:26:55,140
to give you some idea of what that is so if you if you just

433
00:26:55,190 --> 00:26:58,800
prior book and the book store that has maybe a few hundred thousand words in

434
00:26:59,640 --> 00:27:04,720
so maybe it's a good guess that ordinary educated person writes about ten thousand words

435
00:27:04,720 --> 00:27:08,870
a day series maybe three and half million words year

436
00:27:08,880 --> 00:27:13,170
and annuity had million words in lifetime so the computers

437
00:27:13,200 --> 00:27:15,140
have access to

438
00:27:15,150 --> 00:27:21,150
translated texts in the volume that is comparable then you able to read lifetime

439
00:27:21,160 --> 00:27:23,630
and if you look the modelling world text

440
00:27:23,680 --> 00:27:28,650
computers have access to much much more data than you ever be able to read

441
00:27:28,700 --> 00:27:30,840
so if you try to read the internet

442
00:27:31,260 --> 00:27:33,680
it will keep you busy i think that number

443
00:27:33,700 --> 00:27:38,060
some thirty thousand years if you read takes on this of

444
00:27:38,080 --> 00:27:40,550
that's how long it's gonna take so

445
00:27:40,560 --> 00:27:42,960
i mean it's somewhat stunning that

446
00:27:43,030 --> 00:27:47,840
yes we learn language and you probably smart development language because we learn language in

447
00:27:47,840 --> 00:27:52,180
a context where things are pointed out to suppose that this is you know table

448
00:27:52,490 --> 00:27:56,340
this is a screen and then we can learn things that way

449
00:27:57,440 --> 00:28:00,140
in terms of raw language that we are confronted with

450
00:28:00,240 --> 00:28:04,700
computers actually have a huge advantage of second process much much much more

451
00:28:04,710 --> 00:28:07,210
text and be able to do

452
00:28:07,220 --> 00:28:10,300
OK so the

453
00:28:10,310 --> 00:28:12,920
statistical machine translation models are talking about

454
00:28:12,930 --> 00:28:17,120
and just give you a quick rundown of basic principles

455
00:28:17,140 --> 00:28:19,930
so we have two different models

456
00:28:19,940 --> 00:28:24,200
one is the language model or also have been speech recognition and one of the

457
00:28:24,200 --> 00:28:29,010
translation model language model is there general

458
00:28:29,060 --> 00:28:32,930
storytelling here is to translate from a foreign language into english

459
00:28:32,940 --> 00:28:36,570
so the language model is there to figure out what is good english what is

460
00:28:36,570 --> 00:28:39,330
fluent english what makes sense in english

461
00:28:39,380 --> 00:28:41,320
and the translation model

462
00:28:41,360 --> 00:28:43,840
the purpose is the more obvious

463
00:28:43,910 --> 00:28:47,710
you have to find out some correspondence between for instance in english sentence

464
00:28:47,730 --> 00:28:53,720
so two mathematical models one tells you these two sentences to the match up

465
00:28:53,730 --> 00:28:57,650
another model cells who is the english actually good english

466
00:28:57,700 --> 00:29:01,550
because you want to have a translation only only

467
00:29:01,560 --> 00:29:06,410
adequately represents the input you also want translation that

468
00:29:07,250 --> 00:29:09,420
fluently and the output lacked

469
00:29:09,460 --> 00:29:13,100
so given these two mathematical models you could take

470
00:29:13,110 --> 00:29:15,670
any two words sentences

471
00:29:15,680 --> 00:29:17,230
and measure

472
00:29:17,240 --> 00:29:19,110
is the output of english

473
00:29:19,120 --> 00:29:23,050
and to test the input and the output correspond

474
00:29:23,060 --> 00:29:29,730
and so that the small problem remaining is finding the best sentence for a given

475
00:29:29,730 --> 00:29:33,030
input that has the highest probability

476
00:29:33,050 --> 00:29:35,100
in terms of taxation moral

477
00:29:35,150 --> 00:29:40,980
and the language model so probability world pretty much everything is possible you could take

478
00:29:40,980 --> 00:29:44,740
any possible in sentence as possible translation for

479
00:29:44,840 --> 00:29:46,820
so spanish input sentence

480
00:29:46,860 --> 00:29:50,580
and all these models should give you some probabilities for it

481
00:29:50,670 --> 00:29:55,500
so but you can list up all spanish sentences after being little bit smarter finding

482
00:29:55,530 --> 00:29:57,920
good space

483
00:29:57,940 --> 00:30:01,980
so i talked a lot about the translation model and going on and i don't

484
00:30:02,100 --> 00:30:04,040
much about language models

485
00:30:04,930 --> 00:30:07,070
OK so another slide from here

486
00:30:09,400 --> 00:30:11,050
so this is

487
00:30:12,800 --> 00:30:14,810
someone called quiet

488
00:30:14,820 --> 00:30:20,150
triangle list of our parent better so you have

489
00:30:20,170 --> 00:30:24,670
kind of the process how maybe humans actually do translation

490
00:30:24,680 --> 00:30:28,400
so when the translated for instance into english

491
00:30:28,420 --> 00:30:32,960
we read the foreign words we know something about the formal grammar we have

492
00:30:32,980 --> 00:30:37,820
we know what the words mean you've kind of now a concept of of the

493
00:30:37,820 --> 00:30:41,240
meaning and ultimately in our head is the meaning of the sentence

494
00:30:41,330 --> 00:30:43,500
and given the meaning of the sentence

495
00:30:43,510 --> 00:30:48,030
we can then go around find what the writing of concepts with the right words

496
00:30:48,040 --> 00:30:50,630
what is the right syntax have to put that in

497
00:30:50,640 --> 00:30:55,150
and ultimately whatever set by the paper

498
00:30:55,170 --> 00:30:59,790
all the models that currently exist

499
00:30:59,800 --> 00:31:03,390
ignore the entire top part of the permanent

500
00:31:03,400 --> 00:31:07,690
so very simple models take the form would shuffle them around put it produced english

501
00:31:10,550 --> 00:31:13,860
once he moved to more free space you can move up a little bit in

502
00:31:13,870 --> 00:31:15,150
the parameters

503
00:31:15,160 --> 00:31:19,610
because he realized that some birds grouped together and have meaning

504
00:31:19,630 --> 00:31:24,040
that is independent of the individual words and a lot of effort is currently in

505
00:31:24,440 --> 00:31:28,600
machine translation research and i don't have much about it is to move further up

506
00:31:28,750 --> 00:31:31,120
to use syntax and grammar

507
00:31:31,170 --> 00:31:32,800
to build better models

508
00:31:32,810 --> 00:31:36,340
all the work on this event is very much driven by

509
00:31:36,800 --> 00:31:38,350
what works now

510
00:31:38,370 --> 00:31:41,280
can you build a system and show me that better

511
00:31:41,280 --> 00:31:45,820
OK well i

512
00:31:46,370 --> 00:31:51,000
so this the first of its about the processes

513
00:31:51,240 --> 00:31:52,820
the method

514
00:31:52,850 --> 00:31:54,850
and the ways we

515
00:31:54,880 --> 00:31:57,850
build and maintain the kernel today

516
00:31:57,870 --> 00:32:03,920
and look at some of the things we would be doing in the future

517
00:32:03,940 --> 00:32:06,280
so this is what used to have a long time ago

518
00:32:06,760 --> 00:32:11,800
new delhi the capital which is the table it on the floppy disk

519
00:32:11,850 --> 00:32:15,290
was actually bigger and when it's not fitting on one floppy disk because this is

520
00:32:15,290 --> 00:32:20,300
clearly maintainable would be impossible to deal with more than one megabyte

521
00:32:20,330 --> 00:32:25,660
had people and carried around in the most popular

522
00:32:25,680 --> 00:32:28,580
everybody packages to this guy called this

523
00:32:28,770 --> 00:32:31,260
new everybody by name

524
00:32:31,270 --> 00:32:32,450
i read every page

525
00:32:32,480 --> 00:32:34,400
commented on every email

526
00:32:34,600 --> 00:32:38,610
strangely enough we don't do it that way anyway

527
00:32:41,660 --> 00:32:44,190
after what became obvious

528
00:32:44,220 --> 00:32:47,900
you can't actually just feed one

529
00:32:47,920 --> 00:32:53,970
so system if all were a small number of maintaining particular part

530
00:32:54,000 --> 00:32:55,870
collected packages

531
00:32:55,890 --> 00:33:00,560
check checked the centre leg is still commented on everything

532
00:33:00,580 --> 00:33:06,370
various things happen to support the project again got a lot larger switches

533
00:33:06,390 --> 00:33:10,120
in nineteen ninety five nineteen ninety seven

534
00:33:10,360 --> 00:33:12,250
let's have children

535
00:33:12,360 --> 00:33:20,500
dramatically reduced his ability to spend large amounts of time on the project

536
00:33:20,530 --> 00:33:22,860
there's lots of other problems with

537
00:33:22,860 --> 00:33:24,300
the way it used to be done

538
00:33:24,370 --> 00:33:29,200
if loners went on holiday per week there were no updates for

539
00:33:29,250 --> 00:33:32,360
and that means you could see if you actually got tree

540
00:33:32,420 --> 00:33:35,110
what else everybody else have been doing

541
00:33:35,140 --> 00:33:36,730
there was no way of

542
00:33:36,750 --> 00:33:39,660
taking your particular changes

543
00:33:39,670 --> 00:33:45,140
fixing clashes between change everything stop

544
00:33:45,160 --> 00:33:48,470
and you have one central coordinator

545
00:33:48,470 --> 00:33:52,540
which has its advantages and disadvantages

546
00:33:52,690 --> 00:33:56,030
various people played with CBS the

547
00:33:56,070 --> 00:33:58,440
the current rationing system

548
00:33:58,440 --> 00:34:01,740
policy vs is the reason to system where

549
00:34:01,750 --> 00:34:04,020
single person

550
00:34:04,380 --> 00:34:06,870
look at all of the categories in

551
00:34:07,710 --> 00:34:11,910
actually your sending these horrible it's not going in the end

552
00:34:11,940 --> 00:34:14,900
so there is no system editorial control

553
00:34:15,000 --> 00:34:19,600
there's no way for people to sell to the good from the bad

554
00:34:19,620 --> 00:34:24,320
so we start with this one central coordinator must maintain

555
00:34:24,370 --> 00:34:27,540
another problem is change long because you want history

556
00:34:27,560 --> 00:34:31,500
why would the change in nineteen ninety four a

557
00:34:31,530 --> 00:34:36,470
it might be important in when you're looking at making huge change

558
00:34:36,560 --> 00:34:40,340
so how do you manage change logs available and edited

559
00:34:40,400 --> 00:34:45,400
and usually wrong very complete

560
00:34:45,430 --> 00:34:49,750
one of these is immediately tells you need something better

561
00:34:49,780 --> 00:34:54,970
so we need something that could be tracked all of things that are going on

562
00:34:54,970 --> 00:35:03,030
and the first thing that came along with the some arguments in the community doing

563
00:35:03,030 --> 00:35:06,280
things properly and this this does not scale

564
00:35:06,290 --> 00:35:11,340
someone called lion void created a non-free project called bit people

565
00:35:11,350 --> 00:35:16,060
most was refused to use because it is not

566
00:35:16,250 --> 00:35:21,680
various people did use a got less and less free over time

567
00:35:21,750 --> 00:35:25,290
so and each gel hopefully reverse engineered

568
00:35:25,540 --> 00:35:31,270
was much complaining loudly of people that trade secrets breaking licenses

569
00:35:31,280 --> 00:35:33,720
whereas in fact the individual reverse engineer

570
00:35:33,780 --> 00:35:35,660
he tell it to the

571
00:35:36,680 --> 00:35:40,180
we got to command line like problems that help

572
00:35:40,190 --> 00:35:43,960
which point they keep the server from help take part in with the list of

573
00:35:43,960 --> 00:35:47,280
all of the command understood

574
00:35:47,280 --> 00:35:50,850
there was a bit more simple and my

575
00:35:51,470 --> 00:35:53,990
larry bull had because they

576
00:35:54,000 --> 00:35:57,390
ten try to take his coat off with it

577
00:35:57,400 --> 00:36:00,090
this debate in the world

578
00:36:00,100 --> 00:36:04,090
so let's spend a couple of weeks writing a replacement of the first version of

579
00:36:04,110 --> 00:36:06,720
was called get

580
00:36:06,740 --> 00:36:14,090
get is very different something like CVS get is truly distributed version control system

581
00:36:14,100 --> 00:36:19,780
so everybody who is using it as their own complete source tree the machine

582
00:36:19,860 --> 00:36:22,210
you can make changes to this

583
00:36:22,220 --> 00:36:24,840
with change love removed

584
00:36:24,850 --> 00:36:27,280
merge changes from other people

585
00:36:27,290 --> 00:36:30,530
and apply your changes up and so on

586
00:36:31,140 --> 00:36:34,970
instead of having an idea whether it's the same as repository

587
00:36:34,990 --> 00:36:39,240
is a single central point with is the defining point

588
00:36:39,270 --> 00:36:43,840
you can have multiple trees and you can push data between these trees said that

589
00:36:44,760 --> 00:36:50,520
and that means that maintains complaint code together with change log

590
00:36:51,700 --> 00:36:52,730
develop the code

591
00:36:53,900 --> 00:36:57,150
take patches from other contributors

592
00:36:57,160 --> 00:37:00,900
and then push the law in one easy to manage show to let us

593
00:37:00,920 --> 00:37:02,380
the margin

594
00:37:02,520 --> 00:37:04,220
change to merge

595
00:37:04,410 --> 00:37:05,990
it is merged

596
00:37:06,010 --> 00:37:09,450
any conflicts can be resolved

597
00:37:09,490 --> 00:37:13,410
so that the first kind of real tend to use the technology things

598
00:37:13,430 --> 00:37:18,610
getting from

599
00:37:18,630 --> 00:37:20,850
today's model looks something like this

600
00:37:20,880 --> 00:37:26,840
possibly have yet depending on your particular distribution

601
00:37:26,850 --> 00:37:29,520
so it's still so far

602
00:37:29,530 --> 00:37:33,920
get close to have full URL if you can get all the rest of world

603
00:37:35,660 --> 00:37:38,840
you will get a table

604
00:37:38,970 --> 00:37:41,450
sort of like that table you get

605
00:37:41,510 --> 00:37:44,130
get archived with that of

606
00:37:45,590 --> 00:37:48,470
CD into it part two

607
00:37:48,470 --> 00:37:51,500
variables now called

608
00:37:51,510 --> 00:37:53,610
why is the observations

609
00:37:53,620 --> 00:38:00,970
and sets of parameters they correspond to observed and non observed parameters and now introduce

610
00:38:00,980 --> 00:38:03,080
these axes in the case of the

611
00:38:03,530 --> 00:38:08,060
i see a it was the sources these are things we do not observe they

612
00:38:08,060 --> 00:38:12,850
are the latent variables and the model is we think

613
00:38:12,870 --> 00:38:15,910
the total probability of what we observe

614
00:38:16,890 --> 00:38:22,490
composed in such a way that is the probability of the the unobserved variables

615
00:38:22,560 --> 00:38:28,380
and this is the conditional probability of the observation given

616
00:38:28,380 --> 00:38:30,630
so latent ones so this again

617
00:38:30,640 --> 00:38:32,870
it's just the way of writing

618
00:38:34,100 --> 00:38:36,150
joint distribution

619
00:38:37,880 --> 00:38:38,830
so i have

620
00:38:38,880 --> 00:38:42,800
forgetting about the parameters per second

621
00:38:42,990 --> 00:38:46,960
this is just the way of writing that he

622
00:38:47,110 --> 00:38:50,980
y and x jointly observing

623
00:38:50,990 --> 00:38:56,400
in this structure usually i have an idea if i know what the latent variables

624
00:38:56,400 --> 00:38:57,750
are i know

625
00:38:57,790 --> 00:38:58,600
how the

626
00:38:58,610 --> 00:39:02,960
the observations are rated so in the ICA model if i know what the sources

627
00:39:03,660 --> 00:39:09,100
then i believe the wise are just a linear operation on the sources maybe add

628
00:39:09,160 --> 00:39:12,860
bit of noise and this takes this into account and i have some ideas how

629
00:39:12,870 --> 00:39:15,060
the latent are generated

630
00:39:15,420 --> 00:39:19,770
right but unfortunately we don't observe x so what we have to do is we

631
00:39:19,770 --> 00:39:21,240
have to get p y

632
00:39:21,260 --> 00:39:24,130
and that means i have to some

633
00:39:24,800 --> 00:39:27,490
over all possible values of the axis

634
00:39:27,500 --> 00:39:34,510
or in case it continues thing after integrating over x

635
00:39:34,560 --> 00:39:35,920
i have to do

636
00:39:35,980 --> 00:39:38,220
the summation over x and that's what

637
00:39:38,310 --> 00:39:39,720
i wrote to you

638
00:39:40,380 --> 00:39:41,640
and that makes a bit

639
00:39:41,850 --> 00:39:44,500
the problem

640
00:39:44,510 --> 00:39:47,400
on the other hand

641
00:39:51,610 --> 00:39:56,410
no no not that well there might be some parameters that i have to tune

642
00:39:57,260 --> 00:40:01,530
and this probability and maybe there are some parameters that i have to tune in

643
00:40:01,530 --> 00:40:06,120
this probably it just means both of them can have parameters which i want to

644
00:40:06,120 --> 00:40:08,750
tune using maximum likelihood

645
00:40:08,760 --> 00:40:12,100
but what want

646
00:40:15,560 --> 00:40:21,580
well i mean if you want to make a set of

647
00:40:21,650 --> 00:40:22,260
i mean

648
00:40:23,720 --> 00:40:30,900
i mean the theta x does not depend on x maybe it's the wrong it's

649
00:40:30,900 --> 00:40:36,870
the wrong way of writing the text simply means it's those parameters data that appear

650
00:40:36,870 --> 00:40:41,630
in the extensive so it shouldn't be considered a function of x

651
00:40:41,640 --> 00:40:45,960
so it's just those are the ones that are here and those are the ones

652
00:40:45,960 --> 00:40:48,640
i mean i didn't want to give them the same nameplate and then it would

653
00:40:48,640 --> 00:40:52,280
just mean o is this the same fate as that now just there's

654
00:40:52,340 --> 00:40:55,650
something does here something else there

655
00:41:00,990 --> 00:41:03,440
so the the most

656
00:41:04,330 --> 00:41:06,050
the most simple

657
00:41:06,060 --> 00:41:09,810
and well known case is a mixture of gaussians

658
00:41:09,830 --> 00:41:11,510
so we know how gaussians

659
00:41:11,550 --> 00:41:14,030
and then gauss in data look like

660
00:41:14,580 --> 00:41:18,260
but what happens if we have more structure in data so this is like a

661
00:41:18,260 --> 00:41:19,490
histogram of

662
00:41:19,560 --> 00:41:23,710
data that have sort of two humps and he was one of

663
00:41:23,730 --> 00:41:29,410
you can't sort of model this by a single gaussians any ideas of is introducing

664
00:41:29,410 --> 00:41:32,580
a mixture of gaussians so

665
00:41:35,140 --> 00:41:38,740
that's the way of writing these things

666
00:41:38,750 --> 00:41:42,020
so what i have is

667
00:41:42,030 --> 00:41:43,280
i have

668
00:41:43,380 --> 00:41:45,040
couple of gaussians

669
00:41:45,170 --> 00:41:46,650
this is a

670
00:41:46,940 --> 00:41:48,640
in density

671
00:41:48,650 --> 00:41:51,370
and for each

672
00:41:51,380 --> 00:41:55,770
what i call the component i have the mean and the variance

673
00:41:55,790 --> 00:41:57,900
they bases on this picture

674
00:41:57,960 --> 00:42:03,780
there's things like you know were yes the number of hops right so you would

675
00:42:03,780 --> 00:42:09,340
have this one actually it seems like it has one main is another means so

676
00:42:09,550 --> 00:42:12,310
here and this is sort of easily

677
00:42:12,380 --> 00:42:18,140
you can easily see what what happens in then mix them together in such a

678
00:42:18,140 --> 00:42:24,690
way that again you have a normalized probability density so you mix them with

679
00:42:26,940 --> 00:42:32,190
weights that sum up to one because if this sums up to one

680
00:42:32,350 --> 00:42:36,980
and then you see these there are nonnegative and sum up to one then you

681
00:42:36,980 --> 00:42:38,830
see that this is valid

682
00:42:41,890 --> 00:42:48,130
the total likelihood again you would sort of take all your observations y is independent

683
00:42:48,130 --> 00:42:51,390
and so you have this is the model so

684
00:42:51,400 --> 00:42:56,260
what you have now is we observe y but you don't know from which goes

685
00:42:56,410 --> 00:42:58,000
it comes actually so

686
00:42:59,060 --> 00:43:01,080
the number

687
00:43:01,090 --> 00:43:03,030
the k is

688
00:43:03,040 --> 00:43:06,370
the hidden variables the discrete variables

689
00:43:08,630 --> 00:43:11,230
so it is clear i mean before we had to move

690
00:43:11,600 --> 00:43:17,560
we had these axes and x corresponds now to the variable k

691
00:43:18,550 --> 00:43:19,910
we would like to do

692
00:43:19,940 --> 00:43:22,760
maximum likelihood and maybe i just go

693
00:43:22,760 --> 00:43:26,890
correspond to an output false zero so we can see it in this situation

694
00:43:26,900 --> 00:43:31,270
we cannot draw line in separate perfectly all the positive on one

695
00:43:31,280 --> 00:43:34,710
side and negative on the other f i was to draw decision boundary

696
00:43:34,720 --> 00:43:38,000
that's linear is actually kind of famous example it was shown

697
00:43:38,560 --> 00:43:42,900
with use in sorry the in the perceptrons book to show that a single

698
00:43:42,910 --> 00:43:47,410
neuron has some limitations however much like don't describe

699
00:43:47,420 --> 00:43:50,950
me i can transform my input space into another representation

700
00:43:51,160 --> 00:43:54,710
and then get a problem that simpler in this case can be separated

701
00:43:54,720 --> 00:43:57,360
by a single neuron by a linear decision boundary

702
00:43:57,750 --> 00:44:02,200
so what i'm showing here on this side is what if i take my input

703
00:44:02,210 --> 00:44:06,330
space and instead now two dimensions with which i'm going to

704
00:44:06,340 --> 00:44:10,630
draw my inputs my examples the first was going to be the n over

705
00:44:10,640 --> 00:44:15,290
the negative of the first dimension and x to and the second

706
00:44:15,290 --> 00:44:18,050
dimensions going to be the end of the first mention the negative

707
00:44:18,050 --> 00:44:23,960
of x to so in this case this example here zero zero

708
00:44:24,130 --> 00:44:29,550
would have an and for both both this and this and b zero

709
00:44:29,770 --> 00:44:32,400
so what actually map to this point here

710
00:44:32,650 --> 00:44:36,480
and actually this point also if we run through the

711
00:44:37,260 --> 00:44:40,320
calculation of both and here will also

712
00:44:40,580 --> 00:44:45,300
wrap into the same point and we can go through the derivations and

713
00:44:45,300 --> 00:44:48,650
and find out that for both these points they also live essentially

714
00:44:48,650 --> 00:44:53,940
the same place so in this case i can actually drawn now line between

715
00:44:53,950 --> 00:44:57,750
the positive negatives and the thing to notice here is that the

716
00:44:57,760 --> 00:45:01,840
function i've use to now draw my data differently

717
00:45:02,040 --> 00:45:06,150
are these to functions that i've shown before i could actually

718
00:45:06,160 --> 00:45:10,420
represent with a single neuron so what this example suggests is

719
00:45:10,430 --> 00:45:15,020
that if i first compute a few set of neurons and i have some output

720
00:45:15,020 --> 00:45:17,880
neuron that is connected to these intermediate neurons

721
00:45:17,880 --> 00:45:21,120
and now i can start modelling nonlinear decision boundaries

722
00:45:21,370 --> 00:45:23,590
a in this case i can actually perform

723
00:45:24,320 --> 00:45:28,030
model this this function here the x or function

724
00:45:28,890 --> 00:45:32,570
as that is at the core like why we are interested in neural nets

725
00:45:32,580 --> 00:45:35,710
one neural nets interesting is that the allows to go beyond

726
00:45:35,890 --> 00:45:39,490
linear classifiers and actually model non-linear decision boundaries

727
00:45:41,190 --> 00:45:42,220
any questions far

728
00:45:44,530 --> 00:45:48,660
right so that brings us to multilayer neural net

729
00:45:49,920 --> 00:45:52,750
i'm to start simply with a single hidden layer

730
00:45:52,990 --> 00:45:57,280
neural network so in this case what this means is that f between

731
00:45:57,280 --> 00:46:00,290
the input in the output i'm going to have a layer of what i call

732
00:46:00,290 --> 00:46:03,900
hidden units each of these units all like artificial neurons

733
00:46:03,910 --> 00:46:07,080
that is they are connected with the previous layer the input

734
00:46:07,340 --> 00:46:10,430
and perform a linear transformation

735
00:46:10,990 --> 00:46:14,230
of the input and then apply some activation function

736
00:46:14,600 --> 00:46:19,120
ok so you're seeing here essentially a

737
00:46:19,520 --> 00:46:22,050
just like a different notation in the context of

738
00:46:22,260 --> 00:46:26,360
one hidden layer neural net but we see the same for for the pre-activation

739
00:46:26,370 --> 00:46:31,270
it's a bias so this notation i'm i'm using the exponent to index

740
00:46:31,280 --> 00:46:34,740
the layer so this is the one because this bias

741
00:46:34,920 --> 00:46:40,010
for the units the first hidden layer and then this index eyes to

742
00:46:40,020 --> 00:46:43,620
indicate that this is for this is the bias what the i with layer

743
00:46:43,930 --> 00:46:46,580
sorry i f unit in the first hidden layer

744
00:46:47,470 --> 00:46:51,470
and i'm going to take that bias to get my pre-activation loss summing

745
00:46:51,480 --> 00:46:55,710
over the contribution of all the input units so all units in the

746
00:46:55,720 --> 00:47:00,320
previous layer weighted by the corresponding weight of the connection

747
00:47:00,330 --> 00:47:04,330
between the g with input and the i f hidden unit one for which

748
00:47:04,340 --> 00:47:07,040
i'm showing the computation of pre-activation

749
00:47:07,570 --> 00:47:12,040
and again here this matrix w which contains all the weights between

750
00:47:12,050 --> 00:47:16,100
all pairs of hidden unit and input unit i'm i'm storing them into

751
00:47:16,110 --> 00:47:20,620
this matrix w one that i'm using exponent to index to refer

752
00:47:20,630 --> 00:47:25,620
again to the first hidden layer and i can actually write the computation

753
00:47:25,630 --> 00:47:29,190
of all pre-activation of all units in the first hidden layer

754
00:47:29,430 --> 00:47:34,540
in vector form so i can just take the bias vector b one

755
00:47:35,140 --> 00:47:40,300
and i'm as you've seen here what this operation response to effectively

756
00:47:40,310 --> 00:47:44,550
is to take the vector x and multiply it by the row the highest

757
00:47:44,560 --> 00:47:50,150
rho of my matrix w so if i just take the product of the x

758
00:47:50,440 --> 00:47:54,240
multiplying on the right with my matrix w well in the doing is

759
00:47:54,240 --> 00:47:57,590
for each row i'm going to do a dot product between x and the corresponding

760
00:47:57,590 --> 00:48:01,080
row weights which means that i get all the contributions to the

761
00:48:01,090 --> 00:48:05,020
pre-activation for all hidden units so to sum up

762
00:48:05,450 --> 00:48:09,250
my pre-activation is just this vector-valued

763
00:48:10,070 --> 00:48:13,760
linear transformation on my previous layer input layer x

764
00:48:14,720 --> 00:48:18,760
and then as usual neural for regular units

765
00:48:19,540 --> 00:48:22,630
i get my hidden layer activation by applying my

766
00:48:22,860 --> 00:48:27,250
choice of activation function g over my vector applying it element-wise

767
00:48:28,230 --> 00:48:30,620
and then finally to get my output put imagine

768
00:48:30,790 --> 00:48:33,560
performing binary classification so the output would

769
00:48:33,570 --> 00:48:36,980
like neuron between zero and one that gives you the probability

770
00:48:36,990 --> 00:48:41,530
belong to the positive class well i'm again going to take a bias

771
00:48:41,540 --> 00:48:43,820
so b two now because i'm at the second

772
00:48:44,130 --> 00:48:48,270
layers which is the output layer and going to multiply by some

773
00:48:48,280 --> 00:48:51,970
weight vector connecting my output unit with all the hidden units

774
00:48:51,980 --> 00:48:57,020
that i'm noting w to multiply by the activation of the first hidden

775
00:48:57,030 --> 00:49:00,020
layer which is h one or here i should probably have a

776
00:49:00,180 --> 00:49:05,410
one here so that's for single layer neural network

777
00:49:06,820 --> 00:49:12,400
the questions on the notation for anything related to the slide

778
00:49:15,650 --> 00:49:21,860
so i case describe the case of binary classification more generally

779
00:49:21,860 --> 00:49:23,730
more commonly would be interested in

780
00:49:23,730 --> 00:49:27,650
classifying to multiple class performing multi-class classification

781
00:49:28,360 --> 00:49:32,100
so in this case what it means is that an input my belong to one

782
00:49:32,110 --> 00:49:37,540
out of c capital c different classes and so if i wanted to do

783
00:49:37,540 --> 00:49:41,730
independent subspaces multidimensional components

784
00:49:41,740 --> 00:49:45,760
but then well that's

785
00:49:45,770 --> 00:49:49,020
the first starting point but then the question is that

786
00:49:49,040 --> 00:49:55,270
well what kind of dependencies can we have inside the subspaces

787
00:49:55,290 --> 00:50:01,650
one nice thing i see is that when we say that components are independent then

788
00:50:01,670 --> 00:50:06,960
all that's that is remains to be determined about the component is there a one-dimensional

789
00:50:07,060 --> 00:50:11,190
this probability distribution function or the probability distribution functions

790
00:50:11,310 --> 00:50:15,710
but they are one-dimensional so that kind of with rather limited they are not too

791
00:50:15,710 --> 00:50:20,290
difficult to estimate but if we say that we have n dimensional

792
00:50:20,560 --> 00:50:25,650
let's say OK dimension well it's OK if we have k dimensional subspaces

793
00:50:25,700 --> 00:50:31,040
and we can have any kind of dependencies inside the subspaces then the complexity of

794
00:50:31,040 --> 00:50:37,580
the model completely explodes because we have k dimensional probability densities and well that's extremely

795
00:50:37,580 --> 00:50:43,150
that those are very difficult to model or estimate in engine

796
00:50:43,190 --> 00:50:44,350
so what

797
00:50:44,370 --> 00:50:52,720
one approach that small is motivated by these applications ICA the feature extraction

798
00:50:52,730 --> 00:50:59,040
this is a one one two one o approach uses the idea of universe is

799
00:50:59,040 --> 00:51:02,970
based on the idea of invariant feature subspaces by michael

800
00:51:03,890 --> 00:51:07,190
so what gonna cities that's well linear filters

801
00:51:07,210 --> 00:51:11,700
necessarily lack any kind of invariances well you would think that for example if an

802
00:51:11,700 --> 00:51:12,800
input is

803
00:51:12,870 --> 00:51:15,550
minus x instead of x

804
00:51:15,560 --> 00:51:21,590
then the output of a linear filter will necessarily changes sign so you wouldn't have

805
00:51:21,590 --> 00:51:25,770
a come cost have even this kind of simple

806
00:51:25,780 --> 00:51:27,310
a simple

807
00:51:27,550 --> 00:51:29,930
invariance with respect to policy

808
00:51:30,030 --> 00:51:32,970
so what propose is that's a very

809
00:51:33,770 --> 00:51:40,840
mathematical framework to model dependencies is two subspaces and in particular that looking at the

810
00:51:40,840 --> 00:51:46,260
moment of of projections onto the subspace so we have in the end we what

811
00:51:46,270 --> 00:51:52,440
we have is some WIC that spans some kind of a invariant feature subspaces and

812
00:51:52,440 --> 00:51:59,270
then we will get the linear projections and take the sum of squares of things

813
00:51:59,280 --> 00:52:01,600
well this is certainly not

814
00:52:01,650 --> 00:52:06,730
i mean this idea doesn't really come out of the this kind of model for

815
00:52:06,730 --> 00:52:12,970
invaders have been used for a long time in in subspace classifiers for example that

816
00:52:12,970 --> 00:52:19,220
the idea is that you present said one class of objects by subspace and you

817
00:52:19,220 --> 00:52:26,800
then you project onto subspace to see whether whether you are in the subspace

818
00:52:26,810 --> 00:52:29,080
so now if we combine these two ideas

819
00:52:29,340 --> 00:52:37,820
multidimensional independent components an invariant subspaces what you get is what we call independent subspace

820
00:52:38,960 --> 00:52:42,640
the idea is that well this invariant subspace

821
00:52:43,860 --> 00:52:47,470
tells us how we can actually define in a meaningful way

822
00:52:47,490 --> 00:52:53,940
and and a very simple and very simple way but if the distributions inside the

823
00:52:53,940 --> 00:53:01,950
subspaces we can just say that these distributions inside subspaces hospital symmetric which means that

824
00:53:01,950 --> 00:53:05,980
they depend only on the norm of the projection that is only on the value

825
00:53:05,980 --> 00:53:11,380
of invariant features that we have actually simplify the matter very much because now inside

826
00:53:11,380 --> 00:53:14,840
each subspace is we only need to be on the only

827
00:53:14,850 --> 00:53:19,230
freedom left is the distribution of that no

828
00:53:19,240 --> 00:53:25,640
which is you know again the one-dimensional probability density function and not more complicated than

829
00:53:25,740 --> 00:53:32,690
the one dimensional PDF seen basic ICA

830
00:53:33,710 --> 00:53:34,040
so in

831
00:53:34,440 --> 00:53:38,750
first two well if we not talk about this thing from with the point of

832
00:53:38,750 --> 00:53:43,310
image processing and computer vision well of course people have their figured out many different

833
00:53:43,310 --> 00:53:51,620
kinds of invariant features features in invariant with respect translation of of scale or anything

834
00:53:51,620 --> 00:53:53,740
but here the point is that we are not

835
00:53:53,760 --> 00:53:56,570
specifying which

836
00:53:56,620 --> 00:53:59,000
in what way these features should be

837
00:53:59,010 --> 00:54:04,320
invariant we are just saying that OK let us we have this kind of subspaces

838
00:54:04,650 --> 00:54:09,700
that can be estimated from the data and when we estimate then we will probably

839
00:54:09,700 --> 00:54:15,460
get some kind of independent sorry some kind of invariances but those invariances will be

840
00:54:15,520 --> 00:54:19,710
determined by the data itself and not by us

841
00:54:19,730 --> 00:54:26,740
so this just a simple graphic description of how we then compute in this model

842
00:54:26,760 --> 00:54:31,810
so we have this WIC just as in the basic ICA basic ICA case we

843
00:54:33,150 --> 00:54:34,880
linear dot products to get

844
00:54:34,900 --> 00:54:37,550
some components

845
00:54:37,570 --> 00:54:42,410
but now the point is that instead of processing each component separately

846
00:54:42,420 --> 00:54:48,110
to compute the PDF or whatever we now have now put them into groups for

847
00:54:48,110 --> 00:54:50,720
example here groups of all components

848
00:54:50,730 --> 00:54:53,140
that the PDF is

849
00:54:53,180 --> 00:54:57,450
is a function of of of this of taking the squares of each of these

850
00:54:57,450 --> 00:55:02,740
components and then in some inside the school of subspaces

851
00:55:02,750 --> 00:55:07,200
well here i show the square root which is so if you want to compute

852
00:55:07,220 --> 00:55:11,590
the itself instead of the square on but that doesn't really matter

853
00:55:11,640 --> 00:55:18,810
so this what is so here we get then the value of the invariant feature

854
00:55:18,830 --> 00:55:20,380
so what happens

855
00:55:20,390 --> 00:55:23,910
when we have apply this image data

856
00:55:23,920 --> 00:55:29,570
what we see is the emergence of the of so-called complex cell properties so these

857
00:55:31,380 --> 00:55:38,000
feature stops this subspaces we'll have properties similar to so-called complex cells in the visual

858
00:55:39,340 --> 00:55:43,340
well it doesn't matter if you don't know about complexity but well basically in the

859
00:55:43,340 --> 00:55:49,340
primary visual cortex basically people distinguish two types of sensors simple cells and complex as

860
00:55:49,370 --> 00:55:51,120
simple cells

861
00:55:51,130 --> 00:55:54,070
usually more by some kind of a linear

862
00:55:54,200 --> 00:56:01,420
linear models or at least models that are very close to linear models and that

863
00:56:01,440 --> 00:56:07,500
can be estimated by linear systems identification as complex cells are then something more complicated

864
00:56:07,500 --> 00:56:11,370
something very clearly not

865
00:56:11,390 --> 00:56:12,820
one of the

866
00:56:12,820 --> 00:56:24,870
OK let's get started

867
00:56:24,890 --> 00:56:29,710
so hello everybody and we'll come back to that part of the machine learning in

868
00:56:29,710 --> 00:56:32,140
bioinformatics lecture

869
00:56:32,190 --> 00:56:36,840
i'd like to start with a brief repetition of really have what we had last

870
00:56:36,840 --> 00:56:41,030
time so i think i lost a few people there and you have some questions

871
00:56:41,030 --> 00:56:44,750
just ask during

872
00:56:44,770 --> 00:56:51,460
OK so i would like to mainly started many talk about bioinformatics i guess it's

873
00:56:51,480 --> 00:56:58,750
all kind of you know then i started talking about string kernels and yes yesterday

874
00:56:58,750 --> 00:57:02,260
i was talking about large scale data structures and introduce a

875
00:57:02,270 --> 00:57:07,500
in the last time i talked about the structure of learning

876
00:57:07,550 --> 00:57:08,690
OK so

877
00:57:08,700 --> 00:57:13,830
i mean he talked about being kind of we had different data types in bioinformatics

878
00:57:13,840 --> 00:57:18,540
but i mean you talked about string so and here in some ways of defining

879
00:57:18,540 --> 00:57:19,610
a string

880
00:57:21,520 --> 00:57:25,700
one way is to define the feature space explicitly and then computers to this kind

881
00:57:25,700 --> 00:57:30,100
of product and that i showed you a race of how to do this efficiently

882
00:57:30,100 --> 00:57:33,330
using string indexing structure

883
00:57:33,400 --> 00:57:39,520
another way was to derive from the generative model like hidden markov model markov chain

884
00:57:39,780 --> 00:57:45,650
these things that i've shown you like the fish account of a simple probabilistic model

885
00:57:45,650 --> 00:57:51,720
is actually equivalent to the great degree which i have described

886
00:57:51,740 --> 00:57:56,060
and you can also derive from a similarity measure this coverage which is called the

887
00:57:56,060 --> 00:58:03,980
local alignment kernel which is something that every program to compute alignments that defines

888
00:58:05,780 --> 00:58:06,720
OK so

889
00:58:06,800 --> 00:58:13,180
so i talked a little bit about these string data structure the string indexing data

890
00:58:13,180 --> 00:58:17,250
structure so you cannot i mean you can speed up a little bit of a

891
00:58:17,250 --> 00:58:22,030
single kernel computation so i mean but it cannot be much faster than linear in

892
00:58:22,030 --> 00:58:23,090
to see

893
00:58:23,140 --> 00:58:27,650
so what you can be part that when you compute linear combinations of kernels and

894
00:58:27,650 --> 00:58:31,760
you can be actually faster than the sum of the length of the

895
00:58:31,770 --> 00:58:37,650
OK and the idea here was that the

896
00:58:37,660 --> 00:58:45,110
exploit the feature map phi x is sparse only few elements not

897
00:58:45,130 --> 00:58:48,970
and also a linear combination of the features of this feature is going to be

898
00:58:49,880 --> 00:58:52,100
OK and the sparse vectors

899
00:58:52,110 --> 00:58:53,820
we can represent either by

900
00:58:53,840 --> 00:58:54,950
explicit maps

901
00:58:54,970 --> 00:59:01,050
sorted lists or suffix trees tried binary fission

902
00:59:01,070 --> 00:59:04,360
and then we can do operations on these spot feature

903
00:59:04,380 --> 00:59:10,690
and this being accommodations to be used when we compute the output of an SCM

904
00:59:11,550 --> 00:59:15,380
for instance we have a set of n support vectors and you would like to

905
00:59:15,380 --> 00:59:19,730
compute the output of the whole test set this test set can be pretty large

906
00:59:19,730 --> 00:59:26,070
like genomic scale you can have like the human genome sometimes billion data points i

907
00:59:26,070 --> 00:59:27,450
would like to invite you

908
00:59:27,460 --> 00:59:30,190
and they really speak

909
00:59:30,210 --> 00:59:35,430
OK and here a data structures really help

910
00:59:35,750 --> 00:59:38,610
also talked about so

911
00:59:39,390 --> 00:59:42,000
this is but this is

912
00:59:43,070 --> 00:59:47,030
so i also talked about the diffusion

913
00:59:47,040 --> 00:59:51,960
so that least three ways of integrating data so that you typically don't have you

914
00:59:51,960 --> 00:59:57,780
typically have more than one kind of information points when you're classifying genes you might

915
00:59:57,780 --> 01:00:02,850
have expression data might sequence and other information in the question is how do you

916
01:00:02,850 --> 01:00:04,600
combine these two things

917
01:00:05,820 --> 01:00:07,190
so one way would be

918
01:00:07,200 --> 01:00:09,330
two integrated early

919
01:00:09,340 --> 01:00:11,050
so you simply take features

920
01:00:11,090 --> 01:00:16,290
hence the feature vectors concatenate them and then use one can and come up with

921
01:00:16,410 --> 01:00:17,630
a classifier

922
01:00:17,670 --> 01:00:22,330
another way would be intermediate integration so you have

923
01:00:22,350 --> 01:00:28,800
one can metrics are one to features you drive because each feature set

924
01:00:28,840 --> 01:00:32,070
then these two come up with this

925
01:00:32,090 --> 01:00:37,610
so what you have seen yesterday in article of the highest talk with essentially late

926
01:00:37,610 --> 01:00:41,540
integration so he took a different features that came up

927
01:00:41,560 --> 01:00:46,710
with many kernels and discriminant function and at the very end he combine several discriminant

928
01:00:46,710 --> 01:00:50,540
functions to come up with a multiclass classifier

929
01:00:50,590 --> 01:00:54,540
and in some cases this might be preferable other cases

930
01:00:54,860 --> 01:00:58,690
one typically the intermediate integration is that time

931
01:01:07,440 --> 01:01:11,950
CV here

932
01:01:11,990 --> 01:01:17,690
so for instance you could i mean that there's some work where you can exponentiated

933
01:01:17,950 --> 01:01:21,440
these take

934
01:01:21,530 --> 01:01:25,080
you take the sum of the kernel and then

935
01:01:25,120 --> 01:01:26,320
i did

936
01:01:26,840 --> 01:01:33,560
so so you can exponentiated these kernels that at the right to take the long

937
01:01:33,560 --> 01:01:37,260
back so that's another way life

938
01:01:37,280 --> 01:01:38,310
we want

939
01:01:44,650 --> 01:01:51,150
know there was some by

940
01:01:51,170 --> 01:01:56,960
according to do so he worked with exponentiating kernels and

941
01:01:56,960 --> 01:01:58,870
on the internet

942
01:01:58,890 --> 01:02:02,250
but here will only

943
01:02:02,260 --> 01:02:04,520
give proof of one

944
01:02:04,530 --> 01:02:08,360
of of the

945
01:02:08,450 --> 01:02:10,020
of the five live

946
01:02:10,230 --> 01:02:12,420
steps right

947
01:02:12,430 --> 01:02:15,040
so we're trying to prove that

948
01:02:15,990 --> 01:02:20,700
g is connected and there is exactly one path between any two vertices so i

949
01:02:20,700 --> 01:02:22,040
have two indices

950
01:02:23,030 --> 01:02:28,830
the and i know that there is exactly one path between u and v

951
01:02:28,890 --> 01:02:30,330
then i would like to

952
01:02:30,340 --> 01:02:32,020
prove that

953
01:02:32,020 --> 01:02:33,510
it is connected

954
01:02:33,530 --> 01:02:36,480
but the removal of any edge disconnected OK

955
01:02:37,260 --> 01:02:40,770
if you if the first condition is true

956
01:02:40,940 --> 01:02:42,300
it's connected

957
01:02:42,320 --> 01:02:44,190
then this part is true too

958
01:02:44,270 --> 01:02:47,830
it is connected to there's nothing to it we just have to prove that if

959
01:02:47,830 --> 01:02:49,590
we remove any action

960
01:02:50,480 --> 01:02:51,930
the graph

961
01:02:51,970 --> 01:02:53,970
i will be disconnected

962
01:02:54,810 --> 01:02:59,000
so let's take in any

963
01:02:59,040 --> 01:03:02,950
from let's say x and y

964
01:03:02,990 --> 01:03:04,460
it's an arbitrary

965
01:03:09,530 --> 01:03:12,530
if that is if if this one is true

966
01:03:12,550 --> 01:03:16,570
this means there is exactly one

967
01:03:16,580 --> 01:03:20,520
path between x and y and that part

968
01:03:20,530 --> 01:03:22,110
it's just the edge

969
01:03:28,270 --> 01:03:30,800
if i remove the edge e

970
01:03:30,830 --> 01:03:33,260
there is no pattern between x four

971
01:03:33,270 --> 01:03:38,130
so there must be in different components

972
01:03:38,190 --> 01:03:39,810
connected component

973
01:03:39,950 --> 01:03:44,260
so the graph is not connected to the present

974
01:03:44,270 --> 01:03:48,230
and here the proofs that i'm not going to vote go

975
01:03:48,240 --> 01:03:50,940
two because i would like to cover it be more

976
01:03:52,180 --> 01:03:54,320
the only question

977
01:04:02,830 --> 01:04:05,280
there may be more than one wall

978
01:04:05,300 --> 01:04:07,300
let me give you

979
01:04:07,350 --> 01:04:08,810
one example

980
01:04:09,590 --> 01:04:11,300
i have

981
01:04:13,000 --> 01:04:13,760
that's a

982
01:04:14,840 --> 01:04:16,070
from x

983
01:04:16,080 --> 01:04:17,970
two y

984
01:04:17,970 --> 01:04:21,950
there is only one path that would be let's see

985
01:04:21,970 --> 01:04:23,770
that would be w so

986
01:04:26,720 --> 01:04:30,760
w that is the path between x and y

987
01:04:30,770 --> 01:04:32,770
unique but here is

988
01:04:32,820 --> 01:04:33,710
a what

989
01:04:37,430 --> 01:04:39,600
the y

990
01:04:39,760 --> 01:04:40,730
so what

991
01:04:40,780 --> 01:04:43,530
which goes like this

992
01:04:43,540 --> 01:04:45,860
so then we one more than one one

993
01:04:52,060 --> 01:04:53,550
if we if

994
01:04:53,610 --> 01:04:57,770
and it's true if you start walking then just

995
01:04:58,680 --> 01:05:01,860
so part is notable it is

996
01:05:02,550 --> 01:05:06,720
and walk it maybe you may walk with wander around them

997
01:05:19,260 --> 01:05:21,440
the but that is very important

998
01:05:21,450 --> 01:05:28,940
consequence to this theorem that the stated above and through just one-fifth of the of

999
01:05:30,160 --> 01:05:31,530
and that is

1000
01:05:35,340 --> 01:05:38,370
contains at least one vertex of valence one

1001
01:05:38,500 --> 01:05:39,990
here we have to be

1002
01:05:41,230 --> 01:05:45,250
all graphs that we consider here are finite

1003
01:05:49,580 --> 01:05:52,470
because for instance for infinite graphs

1004
01:05:52,520 --> 01:05:55,020
this is not true

1005
01:05:55,030 --> 01:05:57,770
let me give you an example of an infinite graph

1006
01:05:57,790 --> 01:05:59,950
which this is not true

1007
01:06:00,030 --> 01:06:02,480
and if you need three

1008
01:06:02,530 --> 01:06:10,480
so that's an example of an infinite tree

1009
01:06:10,480 --> 01:06:13,550
there is exactly one

1010
01:06:13,570 --> 01:06:16,040
path between any two vertices

1011
01:06:16,090 --> 01:06:17,060
so that's three

1012
01:06:18,080 --> 01:06:20,000
but these three has no

1013
01:06:20,000 --> 01:06:24,720
a vertex of valence one each vertex valence two

1014
01:06:26,340 --> 01:06:30,010
so one has to be careful

1015
01:06:34,810 --> 01:06:37,980
here's the proof

1016
01:06:37,990 --> 01:06:41,190
suppose g is a graph

1017
01:06:41,250 --> 01:06:42,800
maybe three right

1018
01:06:42,860 --> 01:06:46,070
in which every vertex has valency at least two

1019
01:06:46,080 --> 01:06:49,800
then by the handshaking lemma

1020
01:06:49,820 --> 01:06:50,980
we have

1021
01:06:51,180 --> 01:06:54,520
the number of edges is one over two

1022
01:06:54,530 --> 01:06:56,370
time to some of the valence

1023
01:06:56,380 --> 01:06:57,990
of our graph

1024
01:06:58,020 --> 01:07:02,000
but remember the valence of each vertex is at least two

1025
01:07:02,010 --> 01:07:03,300
so this is greater

1026
01:07:03,940 --> 01:07:05,560
we have three of g

1027
01:07:05,580 --> 01:07:08,240
valence is so this must be

1028
01:07:08,330 --> 01:07:10,510
later equal to one-half

1029
01:07:10,520 --> 01:07:12,510
times is one half from here

1030
01:07:12,600 --> 01:07:14,630
view of g comes from

1031
01:07:14,640 --> 01:07:16,890
blg times two comes from this

1032
01:07:16,940 --> 01:07:21,930
now these two into castle and to get g so what do we get

1033
01:07:21,980 --> 01:07:25,980
we get that the of g is greater or equal to the of g

1034
01:07:25,990 --> 01:07:27,330
on the other hand

1035
01:07:27,350 --> 01:07:28,700
if g is three

1036
01:07:28,710 --> 01:07:30,990
then he of g is less than

1037
01:07:31,090 --> 01:07:33,960
energy because you g is just

1038
01:07:33,970 --> 01:07:35,480
view of g minus one

1039
01:07:35,520 --> 01:07:36,600
as we

1040
01:07:36,620 --> 01:07:41,030
we've seen in the in the in the previous year so that's a contradiction so

1041
01:07:41,030 --> 01:07:45,060
if you have such a graph then g is definitely not true

1042
01:07:45,070 --> 01:07:46,170
and you see

1043
01:07:46,190 --> 01:07:48,690
the this handshaking lemma

1044
01:07:48,740 --> 01:07:49,720
holds for

1045
01:07:49,740 --> 01:07:52,470
we're fine finite graphs called for

1046
01:07:52,480 --> 01:07:53,850
this is called for

1047
01:07:56,960 --> 01:08:01,940
of vertices of valence one have special name

1048
01:08:01,950 --> 01:08:03,570
there are called leaves

1049
01:08:03,620 --> 01:08:10,380
what is the spanning tree

1050
01:08:10,410 --> 01:08:14,980
a spanning tree is a spanning subgraph is three

1051
01:08:15,030 --> 01:08:17,290
and it's very important

1052
01:08:17,310 --> 01:08:21,340
that a finite graph

1053
01:08:23,270 --> 01:08:28,250
if and only if it contains a spanning tree

1054
01:08:30,780 --> 01:08:34,790
here again we have to say that the graph is finite

1055
01:08:34,800 --> 01:08:37,530
because for is for infinite graphs

1056
01:08:37,580 --> 01:08:40,040
the existence of a spanning tree

1057
01:08:40,050 --> 01:08:44,250
is equivalent to the axiom of choice

1058
01:08:44,260 --> 01:08:46,760
so that's

1059
01:08:46,770 --> 01:08:48,390
you cannot prove it

1060
01:08:48,410 --> 01:08:53,270
you just have to assume it by my actions

1061
01:08:53,290 --> 01:08:57,730
but for finite graphs it's easy right

1062
01:08:57,770 --> 01:09:00,370
if graph is connected

1063
01:09:03,450 --> 01:09:06,330
it must contain a spanning tree

1064
01:09:06,350 --> 01:09:11,020
so we have a finite graph

1065
01:09:11,030 --> 01:09:16,100
it is connected

1066
01:09:16,120 --> 01:09:21,030
if it's not three then by that theorem it must have at least one cycle

1067
01:09:22,530 --> 01:09:23,860
the cycle

1068
01:09:23,890 --> 01:09:26,560
any edge on the cycle removing it

1069
01:09:26,580 --> 01:09:30,680
and the graph stays connected

1070
01:09:32,800 --> 01:09:34,160
keep doing that

1071
01:09:34,190 --> 01:09:39,300
because the graph is finite there are only finitely many

1072
01:09:39,300 --> 01:09:43,260
so we have various tools

1073
01:09:43,270 --> 01:09:46,300
we have for example

1074
01:09:46,320 --> 01:09:49,440
exactly the sort of unfortunate

1075
01:09:50,070 --> 01:09:53,690
were sort of tracking bug

1076
01:09:53,700 --> 01:09:59,390
it has a user interface is about ten years old web design

1077
01:09:59,400 --> 01:10:00,700
it has

1078
01:10:00,710 --> 01:10:05,680
its environment is completely non user-friendly uses very technical language

1079
01:10:05,720 --> 01:10:10,220
it's very hard to localize only speaks english

1080
01:10:10,250 --> 01:10:11,430
very slow

1081
01:10:11,440 --> 01:10:13,820
if you have large numbers above

1082
01:10:13,840 --> 01:10:18,770
the biggest problem of all the above the love is villa is just good enough

1083
01:10:18,770 --> 01:10:21,700
nobody has written replace

1084
01:10:21,860 --> 01:10:26,780
but it was a little bit worse find somebody would have replaced with software

1085
01:10:26,800 --> 01:10:30,610
but it sure yesterday

1086
01:10:30,640 --> 01:10:33,420
there are lots of things we can do with exelon

1087
01:10:33,440 --> 01:10:38,250
one of which is that if for example somebody followed by the door

1088
01:10:38,260 --> 01:10:40,510
which turns out to be kind of

1089
01:10:40,560 --> 01:10:44,150
somebody else while the volume and returned that same kind of

1090
01:10:44,150 --> 01:10:48,140
we can't merge this back together coming close

1091
01:10:48,190 --> 01:10:50,800
we can't link them with the kernel

1092
01:10:50,810 --> 01:10:57,640
so we never pushing but ran between distributions finding common things between distribution

1093
01:10:57,720 --> 01:11:00,900
we've got no way of actually working out

1094
01:11:01,050 --> 01:11:04,510
dear passing about on from one place to another

1095
01:11:04,530 --> 01:11:07,270
re-entering it using all of the

1096
01:11:07,280 --> 01:11:08,810
cc lists

1097
01:11:08,830 --> 01:11:11,860
a lot of messing around

1098
01:11:11,870 --> 01:11:15,490
we have a regression tracking system is point new

1099
01:11:15,490 --> 01:11:19,750
and the idea is to these to find things where somebody said this used to

1100
01:11:19,750 --> 01:11:22,100
work stop words

1101
01:11:22,120 --> 01:11:25,920
because of the often very easy to debug because if you know when it stopped

1102
01:11:27,140 --> 01:11:30,320
you only have to look at a small amount to see why

1103
01:11:31,860 --> 01:11:33,510
we have a lot of time

1104
01:11:33,550 --> 01:11:38,120
usually checking you regression which turns out to be hard

1105
01:11:38,140 --> 01:11:44,330
somebody said you still have traded to the new release now doesn't

1106
01:11:44,350 --> 01:11:50,380
after much investigation you discover the CPU fan filed the same day

1107
01:11:50,400 --> 01:11:55,740
so in fact that it was a hard problem

1108
01:11:55,740 --> 01:11:58,300
the motion tracking system is very bad

1109
01:11:58,320 --> 01:12:02,320
it has a superb user interface is called rafale

1110
01:12:02,330 --> 01:12:03,970
initially financed that

1111
01:12:03,980 --> 01:12:05,330
we so badly

1112
01:12:05,350 --> 01:12:11,660
the human body is human regression tracking system we may automate in the future if

1113
01:12:11,730 --> 01:12:14,860
doesn't scale

1114
01:12:14,910 --> 01:12:18,330
but for the for the moment it's a one-man show

1115
01:12:18,370 --> 01:12:19,920
it's very very useful

1116
01:12:19,940 --> 01:12:22,220
he's also tracking patches west

1117
01:12:22,230 --> 01:12:26,610
sometimes people to actually fix the regression and again this

1118
01:12:26,770 --> 01:12:30,650
is actually saying this one is needed to look at this is a regression that

1119
01:12:30,650 --> 01:12:34,260
thank you got

1120
01:12:34,260 --> 01:12:38,450
arjen and event is playing with the idea of statistical tracking

1121
01:12:38,460 --> 01:12:39,910
it is the

1122
01:12:39,940 --> 01:12:43,140
o thing you get the kernel when something goes horribly wrong

1123
01:12:43,170 --> 01:12:44,400
and that's a

1124
01:12:44,410 --> 01:12:46,650
a lot of data registers

1125
01:12:46,670 --> 01:12:49,160
all traces

1126
01:12:49,170 --> 01:12:52,560
and i just wondered since we've got a lot more users that

1127
01:12:52,570 --> 01:12:56,130
we should not enough people we can use this

1128
01:12:57,060 --> 01:13:01,230
if we set things which distribution should tools

1129
01:13:01,240 --> 01:13:06,240
which send it seems to me central site which are now in the light door

1130
01:13:06,260 --> 01:13:10,170
i believe the latest about two hours

1131
01:13:10,200 --> 01:13:14,820
you should be able to look at all of these things find common

1132
01:13:14,840 --> 01:13:20,250
and the idea that we would be able to start the same thing thing

1133
01:13:20,250 --> 01:13:22,440
in the most obvious ontologies

1134
01:13:22,450 --> 01:13:26,050
the most common but occurring is this one

1135
01:13:26,070 --> 01:13:30,160
it is really really useful in terms of what is that we should think what

1136
01:13:30,270 --> 01:13:30,550
the most

1137
01:13:32,700 --> 01:13:33,780
well it

1138
01:13:33,800 --> 01:13:38,750
to a certain extent it's probably the one most people see

1139
01:13:38,760 --> 01:13:43,880
the second thing is to do with not really done some very large companies with

1140
01:13:43,880 --> 01:13:48,060
huge numbers of service have done in

1141
01:13:48,070 --> 01:13:51,560
it's the condensed ask questions saying

1142
01:13:51,570 --> 01:13:56,040
all machines the crash what is common about

1143
01:13:56,050 --> 01:13:58,950
so you start the statistical correlation

1144
01:13:58,960 --> 01:14:00,040
for example

1145
01:14:00,050 --> 01:14:06,250
does everybody you see this particular this problem western discourse each out

1146
01:14:06,320 --> 01:14:08,940
maybe they all have the same distance from

1147
01:14:08,940 --> 01:14:11,320
maybe this problem certain age

1148
01:14:12,460 --> 01:14:15,530
so you can start to actually look at look at the data and ask questions

1149
01:14:15,530 --> 01:14:18,710
on very much more scientific fashion

1150
01:14:18,760 --> 01:14:22,470
and in theory you can actually do but not the analysis of

1151
01:14:22,490 --> 01:14:26,470
so you can do things ninety five percent certain p ten so you

1152
01:14:26,500 --> 01:14:28,940
statistical data sets

1153
01:14:28,950 --> 01:14:31,350
ninety percent to ninety five percent certain p

1154
01:14:31,380 --> 01:14:34,440
the this is linked having this hard

1155
01:14:34,440 --> 01:14:38,070
it is very very useful for debugging

1156
01:14:38,100 --> 01:14:40,350
and the great thing about this

1157
01:14:40,360 --> 01:14:41,060
it's the

1158
01:14:41,090 --> 01:14:42,690
statistical stuff

1159
01:14:42,700 --> 01:14:46,280
doesn't need the end induced to do anything complicated

1160
01:14:46,300 --> 01:14:52,310
on their machine in some sense the and the support of all

1161
01:14:52,400 --> 01:14:54,620
software to deal with

1162
01:14:54,630 --> 01:14:58,320
you know why they have to do any idea push about saying

1163
01:14:58,370 --> 01:15:01,790
when it comes saying your machine crash previously

1164
01:15:01,800 --> 01:15:03,200
i have recovered the

1165
01:15:03,210 --> 01:15:04,910
the error of

1166
01:15:04,960 --> 01:15:06,670
and i think it too

1167
01:15:06,690 --> 01:15:08,560
four analysis yes no

1168
01:15:09,950 --> 01:15:11,530
so it's very scale

1169
01:15:11,530 --> 01:15:19,460
because we don't teach everybody had to use

1170
01:15:19,470 --> 01:15:20,390
one i think

1171
01:15:20,530 --> 01:15:22,650
people do a binary search

1172
01:15:22,740 --> 01:15:27,450
two six twenty four to six twenty five doesn't

1173
01:15:27,460 --> 01:15:30,010
what's the first thing you do is developed

1174
01:15:30,030 --> 01:15:32,280
one the first the things you say

1175
01:15:32,300 --> 01:15:34,360
what's the middle two six twenty four

1176
01:15:34,370 --> 01:15:35,970
release candidate for

1177
01:15:38,410 --> 01:15:39,760
release candidate two

1178
01:15:40,610 --> 01:15:42,610
release candidate three no

1179
01:15:43,290 --> 01:15:46,810
you know if you can't figure out why something has changed

1180
01:15:46,830 --> 01:15:51,710
you don't know there is a new narrative to small

1181
01:15:51,710 --> 01:15:57,370
you can look through that changes i want to change abruptly

1182
01:15:57,390 --> 01:16:00,600
it was developed but not all do

1183
01:16:00,620 --> 01:16:05,830
you need a lot of CPU power the very no longer problems

1184
01:16:05,880 --> 01:16:08,780
you need a lot of memory memory is not a problem

1185
01:16:08,820 --> 01:16:13,490
in a lot of this this is definitely not a problem

1186
01:16:13,530 --> 01:16:16,000
so it's easy to do

1187
01:16:16,010 --> 01:16:19,030
the problem with is that is

1188
01:16:19,030 --> 01:16:20,970
i mean a calculation here

1189
01:16:21,030 --> 01:16:25,360
so i'm going to do it with a six hundred thirty three

1190
01:16:28,700 --> 01:16:31,610
laser light

1191
01:16:31,610 --> 01:16:33,570
and this

1192
01:16:33,570 --> 01:16:37,590
so the being of that laser is about three millimeters and then we have this

1193
01:16:40,300 --> 01:16:43,320
this is the opening this is the

1194
01:16:43,320 --> 01:16:49,950
and we can make the

1195
01:16:50,010 --> 01:16:52,430
so we we already made predictions

1196
01:16:54,680 --> 01:16:56,360
then i mentioned

1197
01:16:56,360 --> 01:16:59,430
if this

1198
01:16:59,470 --> 01:17:02,510
if the opening is only a tenth of a millimeter

1199
01:17:02,530 --> 01:17:06,140
you would expect that the central maximum on that screen which is about three metres

1200
01:17:06,140 --> 01:17:10,390
away that's why i chose to three meters that central maximum will then be two

1201
01:17:10,390 --> 01:17:11,680
centimetres wide

1202
01:17:11,740 --> 01:17:13,880
but i can make it we wider

1203
01:17:13,890 --> 01:17:15,360
because i can make d

1204
01:17:15,410 --> 01:17:22,110
way smaller than tens of millions

1205
01:17:22,110 --> 01:17:25,550
this is it

1206
01:17:27,200 --> 01:17:28,930
all right so what you see now

1207
01:17:28,950 --> 01:17:30,110
is that the

1208
01:17:30,110 --> 01:17:31,950
so that is

1209
01:17:32,030 --> 01:17:34,410
very large very open

1210
01:17:34,410 --> 01:17:37,300
i don't know maybe millimeter or so

1211
01:17:37,300 --> 01:17:40,320
and i'm going to

1212
01:17:43,010 --> 01:17:46,530
now but at one point here so that you can this is a nice moment

1213
01:17:48,160 --> 01:17:52,640
you see that central maximum she are powerful and overwhelming that is in terms of

1214
01:17:52,640 --> 01:17:53,760
its brightness

1215
01:17:53,780 --> 01:17:58,760
we understand now why because this crazy function sin beta divided by the square

1216
01:17:58,780 --> 01:17:59,970
we already know

1217
01:17:59,990 --> 01:18:02,700
it is here although i would say five centimetres

1218
01:18:02,740 --> 01:18:07,160
it's already now this thread with must be less than the tens of millions because

1219
01:18:07,160 --> 01:18:10,720
there will be two centimetres if it were tens of millions and i hope you

1220
01:18:10,720 --> 01:18:15,120
you can see this being distinctly those dark location

1221
01:18:15,190 --> 01:18:17,220
you see a lot of them

1222
01:18:17,410 --> 01:18:21,320
keep in mind that the mean maximum next brought maximum

1223
01:18:21,380 --> 01:18:23,530
is only four and a half percent and get

1224
01:18:23,550 --> 01:18:25,160
smaller and smaller

1225
01:18:25,160 --> 01:18:27,660
hello and low as you go further away

1226
01:18:27,660 --> 01:18:28,590
so now

1227
01:18:28,610 --> 01:18:32,570
and i go way beyond one tens of millions smaller

1228
01:18:32,570 --> 01:18:35,820
we're a small and i'll keep in mind when i make the slept with smaller

1229
01:18:36,050 --> 01:18:38,640
and less likely to go through i can help that

1230
01:18:38,660 --> 01:18:42,340
so the whole image will become fainter that's the price i pay

1231
01:18:42,360 --> 01:18:44,300
four leading less light

1232
01:18:44,300 --> 01:18:45,880
but what i'd blame is

1233
01:18:45,880 --> 01:18:51,220
surely the absurdity that the centre maximum gets wider and wider and wider as i

1234
01:18:52,050 --> 01:18:54,740
the senate

1235
01:18:54,780 --> 01:18:59,780
smaller and smaller and smaller so this is no narrower and narrower and narrower and

1236
01:18:59,780 --> 01:19:05,860
narrower in the center maximum that you see there is almost full

1237
01:19:05,910 --> 01:19:07,820
so the centre

1238
01:19:07,890 --> 01:19:11,760
the opening of my slits must now be something like

1239
01:19:11,820 --> 01:19:15,860
the only ten microns are so extreme that is very highly

1240
01:19:16,260 --> 01:19:17,530
highly accurate

1241
01:19:17,640 --> 01:19:20,700
the device whereby we have

1242
01:19:20,700 --> 01:19:23,030
the option of making this liquid

1243
01:19:23,070 --> 01:19:24,800
the smallest

1244
01:19:24,930 --> 01:19:26,950
michael using this is the

1245
01:19:27,010 --> 01:19:29,530
the result you get

1246
01:19:29,590 --> 01:19:30,640
i will now

1247
01:19:30,660 --> 01:19:32,050
make this list

1248
01:19:32,070 --> 01:19:36,160
open open open open up here we have the point

1249
01:19:36,180 --> 01:19:38,610
i make the centre point about two centimeters

1250
01:19:38,620 --> 01:19:41,800
this liquid which is about now is about content

1251
01:19:41,800 --> 01:19:48,070
of a millimeter

1252
01:19:48,110 --> 01:19:51,010
all right

1253
01:19:51,030 --> 01:19:52,620
i now have to make

1254
01:19:52,680 --> 01:19:54,590
an important confession

1255
01:19:54,610 --> 01:19:56,180
about the gradients

1256
01:19:58,510 --> 01:19:59,820
some of you

1257
01:19:59,860 --> 01:20:01,930
we're very observant

1258
01:20:01,970 --> 01:20:03,950
you may have noticed

1259
01:20:04,120 --> 01:20:05,930
the maximum

1260
01:20:05,970 --> 01:20:10,570
one of the greatest that showed also when i did experiment with my own

1261
01:20:10,590 --> 01:20:11,700
one of all

1262
01:20:12,820 --> 01:20:14,280
the same right

1263
01:20:14,390 --> 01:20:15,700
what's the difference

1264
01:20:15,740 --> 01:20:20,110
no one asked me about it and i was hoping that no one has

1265
01:20:20,120 --> 01:20:21,910
and the reason for that is

1266
01:20:21,910 --> 01:20:24,700
each one of those groups

1267
01:20:24,700 --> 01:20:27,430
has a finite size opening

1268
01:20:27,510 --> 01:20:30,340
and each one of those openings ac

1269
01:20:30,390 --> 01:20:31,240
this way

1270
01:20:31,280 --> 01:20:35,590
they cause diffraction recall that the fraction right answers semantics

1271
01:20:35,640 --> 01:20:38,680
and so superimposed on the greening

1272
01:20:40,490 --> 01:20:44,610
this causes diffraction and the net result than this

1273
01:20:44,610 --> 01:20:45,610
but you get

1274
01:20:45,660 --> 01:20:48,490
the product of the two

1275
01:20:48,530 --> 01:20:50,700
so if i am n here now

1276
01:20:50,720 --> 01:20:53,800
no i'm removing this debate and of course this is now

1277
01:20:53,840 --> 01:20:57,280
four grading but debate is defined this wages d

1278
01:20:57,280 --> 01:21:00,660
it is the opening of the school of ukraine

1279
01:21:00,680 --> 01:21:02,550
and the

1280
01:21:02,610 --> 01:21:05,010
is the separation between the grading

1281
01:21:05,030 --> 01:21:06,880
then i can write down here

1282
01:21:06,930 --> 01:21:08,360
the sign

1283
01:21:08,410 --> 01:21:10,360
of and delta

1284
01:21:10,360 --> 01:21:13,350
at the histogram of its values of its components

1285
01:21:13,550 --> 01:21:17,500
i can get this kind of girls in shape histogram

1286
01:21:17,520 --> 01:21:21,490
and when i look at the end one solution it's very different again the kind

1287
01:21:21,490 --> 01:21:25,870
of the reserve values of x seems to be roughly uniform except that there is

1288
01:21:25,870 --> 01:21:32,030
an enormous peak here which indicates that a lot of these x components are actually

1289
01:21:34,040 --> 01:21:39,460
OK so we see in fundamental difference about to which problem we look at between

1290
01:21:39,470 --> 01:21:44,110
the l one solution until

1291
01:21:45,250 --> 01:21:46,380
all right so

1292
01:21:46,400 --> 01:21:52,090
in this lecture is we can be very interested in solving underdetermined system of equations

1293
01:21:52,090 --> 01:21:56,040
of course we all know that if i have fewer equations than unknowns

1294
01:21:56,060 --> 01:21:58,020
i can not

1295
01:21:58,040 --> 01:22:00,680
so underdetermined system of equations

1296
01:22:00,690 --> 01:22:06,120
but we're gonna be nevertheless concerned with is situations in which i have fewer equations

1297
01:22:06,120 --> 01:22:08,040
than unknowns

1298
01:22:08,060 --> 01:22:12,160
and of course we're going to make some assumptions about the unknown so that the

1299
01:22:12,170 --> 01:22:15,340
solution to this problem at least has some meaning

1300
01:22:15,350 --> 01:22:19,910
and so we can assume that the unknown vector x that we interested in recovering

1301
01:22:19,910 --> 01:22:22,750
from unfortunately too few equations is sparse

1302
01:22:22,780 --> 01:22:27,480
in the sense that most of its coefficients may be zero

1303
01:22:27,830 --> 01:22:29,810
and what we learned during his lecture

1304
01:22:29,880 --> 01:22:34,180
is if we have an underdetermined system of equations to solve and is arise

1305
01:22:34,210 --> 01:22:38,770
in a countless number of applications and if the right hand side happen to be

1306
01:22:38,770 --> 01:22:43,350
sparse approximately sparse then what we learn is that in one

1307
01:22:43,580 --> 01:22:50,920
minimisation often returns the correct answer and i'll show you some applications and this phenomenon

1308
01:22:50,920 --> 01:22:54,490
really has thousands of applications

1309
01:22:54,500 --> 01:22:55,940
OK so

1310
01:22:55,980 --> 01:23:02,810
there are many problems in which we we have unfortunately fewer equations than unknowns but

1311
01:23:03,240 --> 01:23:08,740
the premise that what we're looking for is far reading radically changes the problem and

1312
01:23:08,740 --> 01:23:11,650
makes the search for solutions feasible

1313
01:23:11,750 --> 01:23:16,990
so i'll show you two examples and we'll see many more examples during this

1314
01:23:18,890 --> 01:23:21,680
one example which is extremely modern is

1315
01:23:21,710 --> 01:23:24,430
something has to do with gene expression arrays

1316
01:23:24,450 --> 01:23:26,800
so in gene expression array of course

1317
01:23:26,980 --> 01:23:32,180
are other people who are not familiar with gene expression arrays

1318
01:23:32,190 --> 01:23:33,370
all right

1319
01:23:33,390 --> 01:23:37,210
OK so in gene expression arrays the idea is that you put

1320
01:23:37,300 --> 01:23:39,080
genes on the chip

1321
01:23:39,090 --> 01:23:41,700
and you're gonna measures amount of

1322
01:23:42,020 --> 01:23:43,480
protein that the

1323
01:23:44,530 --> 01:23:51,160
OK so what we're doing is we're measuring the change of expression levels in in

1324
01:23:51,160 --> 01:23:54,830
lots of genes simultaneously because we all put them on the chip that looks like

1325
01:23:54,830 --> 01:23:59,140
this we have little dots you put the genes in there and then you measuring

1326
01:23:59,500 --> 01:24:01,140
the expression level

1327
01:24:01,240 --> 01:24:05,470
and the idea is you have to pay a person who is healthy person we

1328
01:24:05,490 --> 01:24:11,860
sick and you're trying to measure the difference in expressions of several genes compared between

1329
01:24:12,210 --> 01:24:15,820
a and healthy patient and sick anticipation

1330
01:24:15,830 --> 01:24:20,650
OK so the idea of a gene expression arrays to quantitatively measure the expression of

1331
01:24:20,690 --> 01:24:23,750
tens of thousands of genes simultaneously

1332
01:24:23,770 --> 01:24:28,350
and so you create picture that look like this where if you have a green

1333
01:24:28,350 --> 01:24:31,900
dot i forgot in which way it goes but it means that the gene is

1334
01:24:31,900 --> 01:24:35,900
overexpressed in if you have already done it's underexpressed and if you have a black

1335
01:24:35,900 --> 01:24:38,550
dot is about the same

1336
01:24:41,230 --> 01:24:45,510
what you have is because there is no easy you have observations about data of

1337
01:24:45,510 --> 01:24:48,220
interest y of the form x players e

1338
01:24:48,260 --> 01:24:52,770
why is the measured data as this is what you measuring and despite number i

1339
01:24:52,990 --> 01:24:56,930
x i is the true change in expression level four gene the gene that you

1340
01:24:56,930 --> 01:24:59,440
put at spot number i in the eyes

1341
01:24:59,460 --> 01:25:03,370
it is a stochastic error which has to do with things that you cannot control

1342
01:25:03,390 --> 01:25:09,750
so we have data like this but in genomics applications

1343
01:25:09,760 --> 01:25:11,250
it's very

1344
01:25:11,390 --> 01:25:16,010
common to assume that if we're looking at the disease of interest only a few

1345
01:25:16,010 --> 01:25:21,000
genes will be actually responsible for this disease and what that means it means that

1346
01:25:21,000 --> 01:25:25,550
the unknown the things i'm looking for is actually exhibit extremely sparse

1347
01:25:25,560 --> 01:25:27,760
such analysis would expect that

1348
01:25:27,770 --> 01:25:32,190
you know the number of genes coding for a certain disease will be at most

1349
01:25:32,190 --> 01:25:37,010
in the hundreds and so you expect on a hundred locations point is vector x

1350
01:25:37,530 --> 01:25:42,690
this thirty thousand dimensional vector x you would expect that only a few hundred at

1351
01:25:42,690 --> 01:25:46,730
most would be non-zero

1352
01:25:46,750 --> 01:25:52,480
OK so this sparsity easier the knowledge of the priory knowledge that what you're looking

1353
01:25:52,480 --> 01:25:53,840
for is sparse

1354
01:25:53,850 --> 01:25:56,930
really informs the methods of analysis

1355
01:25:56,950 --> 01:25:59,390
of statistical analysis

1356
01:25:59,680 --> 01:26:03,850
another example is again taken from genetics

1357
01:26:03,870 --> 01:26:09,850
it's an example of sparse signals in genetics where for example there's a quantitative trait

1358
01:26:10,140 --> 01:26:11,950
that you're interested in

1359
01:26:12,080 --> 01:26:18,120
in the knowledge in in studying for example cholesterol level

1360
01:26:18,800 --> 01:26:19,680
and so

1361
01:26:19,690 --> 01:26:25,820
so there is this quantity is is variable y which is a cholesterol level

1362
01:26:25,830 --> 01:26:30,990
so you would like to read relate cholesterol level with is eugene attacked

1363
01:26:31,000 --> 01:26:36,130
OK so with the genetic makeup of a patient

1364
01:26:36,150 --> 01:26:41,340
all right so what people do in the field and in genetics they're relate the

1365
01:26:41,340 --> 01:26:42,750
cholesterol level which is why

1366
01:26:43,120 --> 01:26:48,530
the model the linear model of the form y is equal to EX plus

1367
01:26:48,530 --> 01:26:48,980
but two

1368
01:26:49,110 --> 01:26:54,000
people in the same way the sorts of people do i have

1369
01:26:55,230 --> 01:26:57,590
the other is

1370
01:26:57,630 --> 01:26:58,650
one more

1371
01:26:58,900 --> 01:27:00,300
what we're doing

1372
01:27:03,130 --> 01:27:03,980
and when

1373
01:27:04,000 --> 01:27:05,460
there is about one

1374
01:27:05,550 --> 01:27:06,500
so far

1375
01:27:07,880 --> 01:27:12,920
it seems like using bases his

1376
01:27:21,380 --> 01:27:24,210
one zero

1377
01:27:24,230 --> 01:27:25,380
well is

1378
01:27:25,460 --> 01:27:26,920
we don't want to

1379
01:27:28,010 --> 01:27:29,130
how can

1380
01:27:29,170 --> 01:27:30,300
what we do

1381
01:27:30,550 --> 01:27:31,900
how we think

1382
01:27:31,900 --> 01:27:36,610
and of course we're biological and we learn things and and

1383
01:27:36,630 --> 01:27:42,530
we were born with the brain but now it has complicated computational processes and those

1384
01:27:42,530 --> 01:27:43,440
are the ones

1385
01:27:43,490 --> 01:27:45,160
that i'm interested in

1386
01:27:45,160 --> 01:27:53,730
so four

1387
01:27:53,740 --> 01:28:03,290
or you could say that anything

1388
01:28:03,330 --> 01:28:07,540
that's like saying there isn't really a table is just tiny part

1389
01:28:07,550 --> 01:28:12,300
but it doesn't matter at this highway that's what i meant by insulation the higher

1390
01:28:12,300 --> 01:28:17,860
levels are probably much more like a computer then like heart

1391
01:28:17,870 --> 01:28:19,270
a hard has the

1392
01:28:19,540 --> 01:28:25,130
interest intial connection of muscle fibres it's all one thing there's no not a great

1393
01:28:25,130 --> 01:28:32,270
deal of separation in the brain there are separate functions wonderfully isolated from one another

1394
01:28:32,280 --> 01:28:35,130
most parts don't interact unless

1395
01:28:35,340 --> 01:28:38,250
there's a good reason for them to

1396
01:28:41,490 --> 01:28:43,670
computer screen

1397
01:28:43,690 --> 01:28:46,710
no one knows what how dreaming works

1398
01:28:46,790 --> 01:28:49,440
and the standard

1399
01:28:49,450 --> 01:28:51,200
there are three or four theories

1400
01:28:51,200 --> 01:28:54,230
one is that dreaming is like

1401
01:28:54,250 --> 01:28:57,860
garbage collection in last

1402
01:28:58,290 --> 01:29:00,010
another is that it's

1403
01:29:02,090 --> 01:29:06,240
it's a great and nice theory which was that

1404
01:29:06,240 --> 01:29:09,190
a lot of ideas share some neurons

1405
01:29:09,200 --> 01:29:15,550
and by alternating least stimulating them you kill the connections that are relevant but he

1406
01:29:15,550 --> 01:29:17,480
didn't have a clear idea

1407
01:29:17,490 --> 01:29:18,880
how that works

1408
01:29:18,900 --> 01:29:22,700
but i wouldn't want to answer any questions about sleep because no one

1409
01:29:22,740 --> 01:29:24,730
understands it

1410
01:29:24,950 --> 01:29:28,730
incidentally you probably has ten different functions

1411
01:29:30,110 --> 01:29:34,900
because almost all animals are either nocturnal or diurnal

1412
01:29:34,910 --> 01:29:38,700
it takes two brains to survive in the

1413
01:29:38,710 --> 01:29:42,530
when is light and when it starkers there's different kinds of predators

1414
01:29:42,540 --> 01:29:45,590
so almost all animals have opted to

1415
01:29:45,610 --> 01:29:48,610
to disappear in

1416
01:29:48,620 --> 01:29:50,500
either in the day or the night

1417
01:29:50,510 --> 01:29:55,050
but once you have a mechanism like that at the evolution uses it for ten

1418
01:29:55,050 --> 01:29:56,160
other things

1419
01:29:58,270 --> 01:30:02,070
the muscular like

1420
01:30:06,580 --> 01:30:10,210
they like to

1421
01:30:10,210 --> 01:30:20,320
which is

1422
01:30:20,320 --> 01:30:23,450
this is

1423
01:30:24,680 --> 01:30:27,820
the whole

1424
01:31:13,730 --> 01:31:17,440
one these

1425
01:32:21,600 --> 01:32:25,570
and then we

1426
01:32:34,200 --> 01:32:37,450
it is

1427
01:32:44,780 --> 01:32:50,970
so this is

1428
01:33:11,980 --> 01:33:19,130
you know

1429
01:34:33,670 --> 01:34:40,230
and on the

1430
01:34:40,240 --> 01:34:45,370
you can see

1431
01:35:12,120 --> 01:35:15,080
one of those

1432
01:35:31,340 --> 01:35:42,390
the same

1433
01:36:03,590 --> 01:36:11,390
this station

1434
01:36:11,450 --> 01:36:17,350
are the concept

1435
01:37:28,200 --> 01:37:29,700
she is

1436
01:37:33,860 --> 01:37:41,240
what you see here

1437
01:37:41,270 --> 01:37:47,520
most of the time

1438
01:37:47,530 --> 01:37:51,080
the case

1439
01:38:01,000 --> 01:38:06,940
o point

1440
01:38:31,010 --> 01:38:35,350
what we

1441
01:38:35,370 --> 01:38:41,220
this was

1442
01:38:41,280 --> 01:38:43,450
so this

1443
01:38:46,170 --> 01:38:50,760
also see that

1444
01:38:50,760 --> 01:38:53,910
he said

1445
01:38:56,080 --> 01:39:01,820
we also want to do

1446
01:39:01,840 --> 01:39:06,310
so let's see what

1447
01:39:08,890 --> 01:39:19,610
we all we

1448
01:39:19,670 --> 01:39:25,610
his son

1449
01:39:27,760 --> 01:39:29,390
and the

1450
01:39:29,410 --> 01:39:31,590
this so called

1451
01:39:35,100 --> 01:39:38,610
some of the brain

1452
01:39:38,630 --> 01:39:40,260
it's true

1453
01:39:44,340 --> 01:39:51,320
we've seen that he knows the difference

1454
01:39:51,340 --> 01:39:53,610
some the

1455
01:40:01,820 --> 01:40:04,970
so the easy way

1456
01:40:06,720 --> 01:40:08,940
so strong

1457
01:40:08,960 --> 01:40:10,530
in this area

1458
01:40:10,530 --> 01:40:12,050
trying to

1459
01:40:13,610 --> 01:40:18,390
see want

1460
01:40:19,710 --> 01:40:21,050
the consists

1461
01:40:21,050 --> 01:40:23,130
we want

1462
01:40:23,160 --> 01:40:24,800
is always

1463
01:40:31,230 --> 01:40:33,510
it's not all

1464
01:40:46,280 --> 01:40:49,940
all right

1465
01:41:06,790 --> 01:41:10,200
let's see what

1466
01:41:12,600 --> 01:41:17,120
you're you're

1467
01:41:17,150 --> 01:41:21,010
thank you

1468
01:41:25,450 --> 01:41:27,960
all right

1469
01:41:32,850 --> 01:41:35,400
i mean it's

1470
01:41:38,200 --> 01:41:46,390
right so little

1471
01:41:46,660 --> 01:41:51,160
all rest

1472
01:41:51,210 --> 01:41:53,080
and you it

1473
01:41:53,280 --> 01:41:58,040
so what you say what

1474
01:42:03,400 --> 01:42:05,310
o two

1475
01:42:13,110 --> 01:42:20,460
and what i show you

1476
01:42:20,470 --> 01:42:21,930
it remains

1477
01:42:35,650 --> 01:42:40,410
he said he was

1478
01:42:42,860 --> 01:42:49,700
the students of the challenge

1479
01:42:49,730 --> 01:42:52,300
it's a

1480
01:42:52,310 --> 01:42:56,140
real job

1481
01:43:00,370 --> 01:43:01,720
so just

1482
01:43:01,920 --> 01:43:04,530
being with

1483
01:43:04,590 --> 01:43:06,290
o two

1484
01:43:13,560 --> 01:43:19,080
one one one

1485
01:43:21,180 --> 01:43:22,410
the majority

1486
01:43:25,450 --> 01:43:27,160
due to set

1487
01:43:44,770 --> 01:43:49,560
we show you

1488
01:43:51,900 --> 01:43:54,310
so that

1489
01:44:00,170 --> 01:44:05,120
so and so the problem is

1490
01:44:08,560 --> 01:44:20,530
the first role was always

1491
01:44:20,530 --> 01:44:24,400
to do

1492
01:44:28,560 --> 01:44:32,880
so many years

1493
01:44:37,130 --> 01:44:41,180
and so

1494
01:44:43,620 --> 01:44:45,010
it should be

1495
01:44:49,920 --> 01:44:50,600
it is

1496
01:44:56,720 --> 01:44:59,130
from one

1497
01:44:59,150 --> 01:45:01,840
it's a

1498
01:45:04,080 --> 01:45:05,500
we know

1499
01:45:05,500 --> 01:45:12,580
and the implementation of details if you try to apply this

1500
01:45:12,580 --> 01:45:17,140
and start off with the required total thomas level aiming for

1501
01:45:17,160 --> 01:45:21,580
it generally takes an age to get started so there is a kind of adopted

1502
01:45:21,590 --> 01:45:26,380
tolerance schemes we start off with large values slowly decrease the value we were interested

1503
01:45:26,380 --> 01:45:29,420
in the one

1504
01:45:29,430 --> 01:45:35,560
there's also an approximate sequential importance sampling algorithm for by system itself and should be

1505
01:45:35,560 --> 01:45:40,710
a few problems up to point out a couple of weeks ago by robert christian

1506
01:45:41,330 --> 01:45:43,450
and company

1507
01:45:44,990 --> 01:45:46,570
well this atoms

1508
01:45:46,580 --> 01:45:53,000
so far so good and but there are problems still very slow to converge

1509
01:45:53,240 --> 01:45:56,260
we've got this acceptance rate here in this acceptance that here

1510
01:45:56,280 --> 01:45:58,570
when we make very slowly

1511
01:45:58,680 --> 01:46:01,300
and so on

1512
01:46:02,950 --> 01:46:05,420
often we have more help available to

1513
01:46:05,430 --> 01:46:11,880
i'll illustrate this with reference to the branching process problems in the moment

1514
01:46:11,910 --> 01:46:17,390
but we've got a parameter theta we can split it into several blocks you can

1515
01:46:18,040 --> 01:46:20,120
arbitrarily many

1516
01:46:20,130 --> 01:46:25,120
the gibbs sampler concentrated then which follows we draw the first value the first block

1517
01:46:25,130 --> 01:46:28,080
the towards the conditional density given in

1518
01:46:28,200 --> 01:46:29,120
the block

1519
01:46:29,140 --> 01:46:30,480
and so on the data

1520
01:46:31,560 --> 01:46:37,220
that often turns out that although we don't know the complete likelihood we can calculate

1521
01:46:37,220 --> 01:46:41,070
part likelihood that we know some of these parameters and then we can calculate sort

1522
01:46:41,070 --> 01:46:43,060
of these conditional distributions

1523
01:46:43,200 --> 01:46:47,660
to pay for that we know this conditional to the distribution of theta one given

1524
01:46:47,660 --> 01:46:50,840
p two data we don't know this one

1525
01:46:51,010 --> 01:46:55,320
so we could then do a ABC within

1526
01:46:55,330 --> 01:47:01,730
it's kind of template on ABC one metropolis approximate probability hastings including gibbs

1527
01:47:01,740 --> 01:47:03,610
thirty the following

1528
01:47:03,620 --> 01:47:07,110
we are first block from it's time to get close to the city

1529
01:47:07,130 --> 01:47:09,800
this of course is an acceptance rate of one

1530
01:47:09,810 --> 01:47:14,250
and the second block we do ABC to withdraw from its parameter we see data

1531
01:47:14,460 --> 01:47:18,480
which have to fit the data is good

1532
01:47:20,540 --> 01:47:24,570
so this way we got this automatic acceptance here

1533
01:47:24,610 --> 01:47:28,800
we the ABC here we can explore the

1534
01:47:28,850 --> 01:47:31,720
their posterior space quicker than one of the b

1535
01:47:31,930 --> 01:47:33,880
the case

1536
01:47:36,050 --> 01:47:40,880
yes because you want to draw from it

1537
01:47:41,810 --> 01:47:47,120
if you're doing the metropolis hastings version of it

1538
01:47:47,130 --> 01:47:49,440
so from the previous slide then

1539
01:47:49,460 --> 01:47:53,470
you want OK you can you do it you got acceptance and and then you

1540
01:47:53,470 --> 01:47:56,170
would calculate that if you get to take a step one

1541
01:47:56,210 --> 01:48:05,080
OK so back to the examples before we call this this is a branching process

1542
01:48:05,110 --> 01:48:10,070
generating the data in some sense and the red bits of the discrete observations we

1543
01:48:11,720 --> 01:48:15,030
so it turns out that if we condition on known history

1544
01:48:15,080 --> 01:48:20,550
all the three parameters that the distribution of the sampling points are often

1545
01:48:20,570 --> 01:48:24,760
tractable because the slightly might be be passed on

1546
01:48:24,810 --> 01:48:29,460
mutations along the branch lengths so by the fossil finds of

1547
01:48:29,460 --> 01:48:31,540
so i can

1548
01:48:31,620 --> 01:48:36,040
so concentrating on the

1549
01:48:36,100 --> 01:48:37,880
climate evidence from from moment

1550
01:48:37,890 --> 01:48:44,240
call what we're trying to estimate was that at this point here how far back

1551
01:48:44,240 --> 01:48:45,730
in the past that we have to go

1552
01:48:45,740 --> 01:48:50,380
to find the divergence time of the primates when they first evolved

1553
01:48:50,410 --> 01:48:54,940
the data we have all the fossil finds that we know of the way painful

1554
01:48:55,110 --> 01:48:59,410
discovered fossil finds linear all they get is a snapshot wanted we reconstruct something about

1555
01:49:04,550 --> 01:49:09,580
you also need to think about how to choose a good summary statistics

1556
01:49:09,600 --> 01:49:14,290
and if you only have a an unknown the problem we have to do is

1557
01:49:14,290 --> 01:49:15,220
in in general

1558
01:49:15,280 --> 01:49:19,690
so what we want is somewhat summary summarizing apple as

1559
01:49:19,710 --> 01:49:22,720
and some ideas about what they say

1560
01:49:22,730 --> 01:49:28,450
which is sensitive to changes in the parameter but both variations in data

1561
01:49:28,510 --> 01:49:29,960
so we call this is

1562
01:49:30,930 --> 01:49:36,710
a stochastic model so over here this is two dimensional data the green my output

1563
01:49:36,710 --> 01:49:41,130
from a particular parameter value this from another parameter value can so we get a

1564
01:49:41,130 --> 01:49:45,180
cloud data for each by the premise that we want to some that actually going

1565
01:49:45,180 --> 01:49:46,080
to separate these two

1566
01:49:47,790 --> 01:49:49,710
it promises continuous

1567
01:49:49,710 --> 01:49:51,080
but what if you

1568
01:49:51,090 --> 01:49:56,640
so obviously the location column emails and there is a good starting point

1569
01:49:56,670 --> 01:49:58,640
but in general is

1570
01:49:58,700 --> 01:50:02,910
nontrivial how to come up with a good summaries takes away the intuition it seems

1571
01:50:02,910 --> 01:50:06,500
about the problem is dealing with

1572
01:50:06,550 --> 01:50:13,130
and they injured results well so

1573
01:50:13,140 --> 01:50:17,920
and been found that sometimes you can add information to summary so increases summary adding

1574
01:50:17,920 --> 01:50:19,530
more information you expect to die

1575
01:50:19,550 --> 01:50:25,440
new inference an example that happens everything falls down him and that's probably happening because

1576
01:50:25,450 --> 01:50:29,460
you your model wrong all the wrong yes

1577
01:50:29,580 --> 01:50:33,170
and what might not to fit the model one summary of one of the summary

1578
01:50:33,170 --> 01:50:35,360
statement about the same time is difficult

1579
01:50:35,390 --> 01:50:37,480
the battery to model some kind of model

1580
01:50:37,490 --> 01:50:39,460
discrepancy also can have

1581
01:50:39,460 --> 01:50:41,670
model error there that

1582
01:50:41,710 --> 01:50:43,790
but what we would like ideally

1583
01:50:43,810 --> 01:50:47,720
it's some kind of systematic approach for finding good summary

1584
01:50:47,780 --> 01:50:51,230
one of the most recent data

1585
01:50:52,370 --> 01:50:55,170
so talking about primates

1586
01:50:56,400 --> 01:51:00,420
the observed number of fossils perhaps i should have said

1587
01:51:00,950 --> 01:51:04,520
the the picture isn't actually simple

1588
01:51:04,570 --> 01:51:10,470
in that time is divided geological epochs and we can't they fossils that precisely the

1589
01:51:10,520 --> 01:51:13,680
we can do is count the number of fossils found in the my scene the

1590
01:51:13,680 --> 01:51:16,720
number we found that you see and and so on

1591
01:51:16,730 --> 01:51:20,470
so data is actually discrete number counts the number of primate species we know from

1592
01:51:23,000 --> 01:51:28,050
so the next one of the metrics we tried was the following measure the distance

1593
01:51:28,090 --> 01:51:30,500
the simulated data real data

1594
01:51:30,560 --> 01:51:33,530
the nice thing about

1595
01:51:33,610 --> 01:51:38,050
the ABC methodology is that you want your simulators may comes you want

1596
01:51:38,110 --> 01:51:41,530
you can try many different metrics you want you have to revert everything you people

1597
01:51:41,530 --> 01:51:42,670
people your data

1598
01:51:45,780 --> 01:51:52,020
OK so what we this a went wrong and the the tracer MCMC output and

1599
01:51:52,020 --> 01:51:52,850
this here is the

1600
01:51:53,390 --> 01:51:56,970
prediction of the number of modern primate species

1601
01:51:57,020 --> 01:51:58,340
so we we know

1602
01:51:58,810 --> 01:52:03,150
the number of modern primate species about three hundred fifty where the models predict predicting

1603
01:52:03,150 --> 01:52:05,610
about fifty

1604
01:52:05,640 --> 01:52:11,220
well this is very simple to solve using ABC methodology all we do is change

1605
01:52:11,220 --> 01:52:14,880
the metric we have determined the metric which takes account

1606
01:52:15,970 --> 01:52:19,260
the predicted modern population size

1607
01:52:19,300 --> 01:52:25,440
so we know that these three hundred seventy six modern species roughly in and so

1608
01:52:25,440 --> 01:52:29,580
what we do we compare estimated output with three seven six days for why we

1609
01:52:29,580 --> 01:52:31,630
penalize the model

1610
01:52:31,760 --> 01:52:35,920
and then this gives approximation from this posterior distribution so the posterior distribution of the

1611
01:52:35,920 --> 01:52:37,400
parameters given the data

1612
01:52:37,410 --> 01:52:40,280
the value three seven six

1613
01:52:40,280 --> 01:52:41,700
i release it

1614
01:52:41,740 --> 01:52:45,400
from a point that is at least two and a half times the radius

1615
01:52:45,440 --> 01:52:47,020
of this circle

1616
01:52:47,040 --> 01:52:51,970
above the zero level if i do any law lower it will not make it

1617
01:52:52,100 --> 01:52:55,770
think about this that is something that you could not have just easily

1618
01:52:55,780 --> 01:52:57,780
predicted it's very

1619
01:52:57,790 --> 01:53:00,280
strong result but it's not something that

1620
01:53:00,330 --> 01:53:05,340
you say intuitively oh yes of course it follows immediately from the conservation of mechanical

1621
01:53:07,110 --> 01:53:10,740
so if i release it that that two and a half radius point by the

1622
01:53:10,740 --> 01:53:12,730
way somewhere here

1623
01:53:12,790 --> 01:53:14,710
so if i release this object

1624
01:53:14,730 --> 01:53:16,990
way below that

1625
01:53:17,040 --> 01:53:18,650
it will not make this point

1626
01:53:18,700 --> 01:53:19,850
let's do that

1627
01:53:19,860 --> 01:53:23,920
you see didn't make it go to the higher

1628
01:53:23,980 --> 01:53:25,010
didn't make it

1629
01:53:25,020 --> 01:53:27,920
go higher to make it

1630
01:53:27,960 --> 01:53:29,170
a little higher

1631
01:53:29,180 --> 01:53:31,620
still didn't make it

1632
01:53:32,520 --> 01:53:35,330
i go to the two and half mark

1633
01:53:35,330 --> 01:53:38,880
and now to make it

1634
01:53:38,950 --> 01:53:41,190
two and a half times the radius

1635
01:53:41,200 --> 01:53:45,990
conservation of mechanical energy tells you that that is the minimum it takes to just

1636
01:53:45,990 --> 01:53:47,710
go through that point

1637
01:53:47,800 --> 01:53:50,860
of course if if there were no loss of energy at all

1638
01:53:50,860 --> 01:53:54,100
there were no mechanical energy loss that means if there were no friction

1639
01:53:54,120 --> 01:53:56,510
and if i were to release it at this point

1640
01:53:56,530 --> 01:53:59,160
it would have to make it back to this point again

1641
01:53:59,160 --> 01:54:00,120
with zero

1642
01:54:00,130 --> 01:54:03,200
kinetic energy but the that one that's not the case there is always a little

1643
01:54:03,200 --> 01:54:05,180
bit of friction with the

1644
01:54:05,200 --> 01:54:07,750
track one thing and also of course with air

1645
01:54:07,770 --> 01:54:10,700
so if i release it all the way here

1646
01:54:10,720 --> 01:54:12,920
you would not expect it that it

1647
01:54:12,960 --> 01:54:16,350
bounds of all the way to here will probably stop somewhere there may not even

1648
01:54:16,350 --> 01:54:18,410
make it to the and we can try that

1649
01:54:18,420 --> 01:54:23,230
made somewhere to hear a little lower than that level

1650
01:54:23,280 --> 01:54:24,440
because there is some

1651
01:54:24,440 --> 01:54:25,890
friction that

1652
01:54:25,940 --> 01:54:29,360
an avoidable

1653
01:54:29,370 --> 01:54:30,240
all right

1654
01:54:30,260 --> 01:54:32,080
this is the classic one

1655
01:54:32,080 --> 01:54:35,430
many exams where this problem is being given i will give it to you this

1656
01:54:35,430 --> 01:54:39,130
time but it's a classic when you see it on the general exams of for

1657
01:54:40,020 --> 01:54:44,890
and it's simply a matter of conservation of mechanical energy

1658
01:54:44,900 --> 01:54:49,620
that's not a good situation whereby a and b are so far apart

1659
01:54:49,640 --> 01:54:54,560
that the gravitational acceleration is no longer a constant and so you can no longer

1660
01:54:54,560 --> 01:54:55,640
simply say

1661
01:54:55,680 --> 01:55:00,940
that the difference in potential energy between point b and point a is simply and

1662
01:55:02,940 --> 01:55:05,800
so now we are dealing with

1663
01:55:05,840 --> 01:55:07,580
very important

1664
01:55:07,590 --> 01:55:09,360
concepts and that is

1665
01:55:09,440 --> 01:55:11,740
the gravitational force

1666
01:55:11,750 --> 01:55:15,440
you can think of the earth

1667
01:55:15,440 --> 01:55:17,440
acting on

1668
01:55:18,460 --> 01:55:20,110
or you can think of the song

1669
01:55:21,430 --> 01:55:22,940
on a planet

1670
01:55:22,970 --> 01:55:24,600
whatever you prefer

1671
01:55:24,600 --> 01:55:28,110
but that's what i want to deal with when the distances and

1672
01:55:28,150 --> 01:55:29,550
very large

1673
01:55:29,580 --> 01:55:30,890
let me first

1674
01:55:30,910 --> 01:55:35,770
give you the formal definition of gravitational potential energy

1675
01:55:35,830 --> 01:55:38,170
the formal definition is

1676
01:55:38,170 --> 01:55:42,620
that the gravitational potential energy at a point p

1677
01:55:42,630 --> 01:55:45,930
it is the work that i will tell you wouldn't have to do to bring

1678
01:55:45,930 --> 01:55:47,600
that mass

1679
01:55:47,610 --> 01:55:49,010
from infinity

1680
01:55:49,030 --> 01:55:51,290
to that point p

1681
01:55:51,370 --> 01:55:55,310
now you may say that's very strange that in physics there are definitions

1682
01:55:55,330 --> 01:55:58,730
which were waterloo in comes in well we can change it to gravity

1683
01:55:58,750 --> 01:56:00,520
because my forces always

1684
01:56:00,530 --> 01:56:04,850
the same forces gravity was a minus sign so is also minus the work that

1685
01:56:04,850 --> 01:56:05,930
gravity does

1686
01:56:05,970 --> 01:56:10,150
when the object moves from infinity to that point p i just like to think

1687
01:56:10,150 --> 01:56:13,610
of it it's easier for me to think of it as the work that i

1688
01:56:14,970 --> 01:56:16,420
so if

1689
01:56:16,420 --> 01:56:17,680
we apply that

1690
01:56:20,010 --> 01:56:21,440
we first have to know

1691
01:56:21,440 --> 01:56:24,380
what is the gravitational force

1692
01:56:24,490 --> 01:56:27,600
this is an object capital and then you can think of this as being the

1693
01:56:27,600 --> 01:56:29,830
earth if you want to

1694
01:56:30,680 --> 01:56:33,300
there is he an object little and

1695
01:56:33,360 --> 01:56:34,450
then you have to know

1696
01:56:34,510 --> 01:56:37,860
what the forces are between the two and this now is the

1697
01:56:37,940 --> 01:56:39,130
is newton's

1698
01:56:39,160 --> 01:56:41,500
universal law of gravity

1699
01:56:41,560 --> 01:56:43,500
which he postulated

1700
01:56:51,500 --> 01:56:55,760
he says the force

1701
01:56:55,810 --> 01:56:59,210
that little and experiences

1702
01:56:59,230 --> 01:57:00,150
this force

1703
01:57:00,170 --> 01:57:06,090
equals i'll put little and here in the capital and you so it is little

1704
01:57:06,090 --> 01:57:10,710
an experiences that forces due to the presence of capital and

1705
01:57:10,730 --> 01:57:12,500
he calls little

1706
01:57:12,540 --> 01:57:16,850
times capital and find a constant which newton in his there is no no yet

1707
01:57:16,860 --> 01:57:18,250
what the value was

1708
01:57:18,300 --> 01:57:19,250
divided by

1709
01:57:19,260 --> 01:57:20,470
are squared

1710
01:57:20,480 --> 01:57:22,060
if are

1711
01:57:22,080 --> 01:57:23,610
is the distance

1712
01:57:23,660 --> 01:57:26,760
between the two

1713
01:57:26,780 --> 01:57:30,370
this object since newton's third law holds

1714
01:57:30,370 --> 01:57:32,530
actually equals minus reaction

1715
01:57:32,590 --> 01:57:33,990
this force

1716
01:57:34,060 --> 01:57:35,080
which is

1717
01:57:35,160 --> 01:57:40,060
o indicated as capital little and it is the force that this one experiences due

1718
01:57:40,060 --> 01:57:41,760
to the presence of this one

1719
01:57:41,830 --> 01:57:45,660
it is exactly the same in magnitude but opposite in

1720
01:57:45,710 --> 01:57:48,370
direction and that is the universal law

1721
01:57:50,810 --> 01:57:52,520
gravity is always

1722
01:57:54,060 --> 01:57:58,410
gravity sucks that's the way to think of it always attracts there's no such thing

1723
01:57:58,410 --> 01:58:00,480
as a repelling forces

1724
01:58:00,480 --> 01:58:02,910
the gravitational constant g

1725
01:58:02,980 --> 01:58:07,180
is an extremely low number six point six seven

1726
01:58:07,230 --> 01:58:09,310
times ten to the minus eleven

1727
01:58:09,330 --> 01:58:10,540
in our

1728
01:58:10,560 --> 01:58:14,710
as i units which is newton's gram meters per kilogram or something like that

1729
01:58:14,770 --> 01:58:17,000
that's an extremely low number

1730
01:58:17,020 --> 01:58:19,120
it means that if i have

1731
01:58:19,160 --> 01:58:23,910
two objects which are each one kilogram

1732
01:58:23,930 --> 01:58:25,660
which are

1733
01:58:25,660 --> 01:58:29,430
about one meter apart which i have now here about one meter

1734
01:58:29,540 --> 01:58:31,500
that the force

1735
01:58:31,540 --> 01:58:33,580
which they attract each other

1736
01:58:33,640 --> 01:58:37,290
is only six point six seven times ten to the minus eleven newton's

1737
01:58:37,290 --> 01:58:39,730
it is an extremely small

1738
01:58:43,600 --> 01:58:46,620
this water of

1739
01:58:46,730 --> 01:58:50,330
and i am here this is my mass

1740
01:58:50,350 --> 01:58:53,330
when i experience

1741
01:58:53,330 --> 01:58:54,540
the force

1742
01:58:54,560 --> 01:58:59,290
which is given by this equation this would be than the mass of the

1743
01:58:59,290 --> 01:59:00,640
of the earth

1744
01:59:00,660 --> 01:59:03,790
now because i made

1745
01:59:03,890 --> 01:59:05,910
so if i need you experience

1746
01:59:05,930 --> 01:59:08,270
the gravitational acceleration

1747
01:59:08,310 --> 01:59:11,180
and the gravitational acceleration of the experience

1748
01:59:11,210 --> 01:59:12,580
it is therefore given

1749
01:59:12,620 --> 01:59:16,020
by ng divided by are squared

1750
01:59:16,020 --> 01:59:21,030
you could abstractly think of being associated with nodes in the graph so

1751
01:59:21,110 --> 01:59:25,370
one is to predict the category of an object based

1752
01:59:25,380 --> 01:59:30,330
obviously on its attributes but also based on

1753
01:59:30,370 --> 01:59:35,410
it's like maybe the number of links the degrees something about the structure

1754
01:59:35,420 --> 01:59:38,470
attributes of linked objects

1755
01:59:41,240 --> 01:59:42,950
objects that are linked to

1756
01:59:42,980 --> 01:59:44,120
objects that are

1757
01:59:44,120 --> 01:59:46,000
link to and so on

1758
01:59:47,360 --> 01:59:48,650
so there's rich

1759
01:59:48,670 --> 01:59:55,150
kind of feature construction the potentially has to happen in order to get these attributes

1760
01:59:55,150 --> 01:59:58,050
the you're going to use for making predictions

1761
02:00:00,040 --> 02:00:01,800
and so that's like

1762
02:00:01,850 --> 02:00:03,130
the simplest

1763
02:00:03,140 --> 02:00:04,940
kind of application

1764
02:00:07,880 --> 02:00:09,600
as are all type task

1765
02:00:09,610 --> 02:00:12,070
another one that's

1766
02:00:12,120 --> 02:00:19,690
obviously very closely related is object type prediction so

1767
02:00:19,700 --> 02:00:24,470
here again you want to make use of attributes of linked and the link objects

1768
02:00:24,480 --> 02:00:29,250
but maybe it's that actually the object is the different types of might have a

1769
02:00:29,250 --> 02:00:33,390
different structure so if you think trying to

1770
02:00:34,340 --> 02:00:40,280
the type of a publication whether conference papers journal or workshops each of those will

1771
02:00:40,280 --> 02:00:42,010
have different attribute

1772
02:00:42,020 --> 02:00:45,660
so if for example you're doing information extraction

1773
02:00:45,720 --> 02:00:49,570
and i figured out that the journal then i should try and get the volume

1774
02:00:49,570 --> 02:00:52,780
number and the issue number

1775
02:00:52,840 --> 02:00:54,220
and so on

1776
02:00:54,240 --> 02:01:00,770
verses if i figured out the conference proceedings then i need other kinds of information

1777
02:01:00,770 --> 02:01:04,720
so but still the actual label

1778
02:01:05,750 --> 02:01:07,920
object type is very much like

1779
02:01:08,010 --> 02:01:11,600
classification object so

1780
02:01:11,630 --> 02:01:16,860
this is the simplest set up and now one of the things that interesting

1781
02:01:16,880 --> 02:01:19,770
in the case of statistical relational learning

1782
02:01:19,810 --> 02:01:22,930
is the fact that you're not going to predict

1783
02:01:22,930 --> 02:01:24,760
these things in isolation

1784
02:01:24,780 --> 02:01:26,340
but you have a whole

1785
02:01:26,350 --> 02:01:28,550
a collection of objects

1786
02:01:28,590 --> 02:01:30,300
there are unlabelled

1787
02:01:30,350 --> 02:01:34,820
you have to optimize over the labels of all of the objects that one so

1788
02:01:34,820 --> 02:01:41,030
you're not doing one shot classification but you need to do this kind of collective

1789
02:01:41,070 --> 02:01:46,730
classification this happens in the number of the all problems that you need to do

1790
02:01:46,730 --> 02:01:48,260
a kind of joint

1791
02:01:48,270 --> 02:01:50,480
optimisation two

1792
02:01:50,480 --> 02:01:54,580
solve the interesting inference problems

1793
02:01:54,600 --> 02:01:56,950
so the link prediction

1794
02:01:56,990 --> 02:01:59,700
again he has

1795
02:01:59,740 --> 02:02:02,060
can be associated with the kind of edge

1796
02:02:02,140 --> 02:02:06,820
the task so one is as simple as

1797
02:02:06,830 --> 02:02:08,760
you know that analogy

1798
02:02:08,770 --> 02:02:13,160
putting a label on an object is putting a label on lake so i know

1799
02:02:13,160 --> 02:02:14,000
these two

1800
02:02:14,000 --> 02:02:19,040
people are related by the friends did they go to school together you know

1801
02:02:19,070 --> 02:02:23,810
what the relationship

1802
02:02:23,830 --> 02:02:26,090
a second type is just

1803
02:02:26,130 --> 02:02:30,360
predicting the existence of the link so in the first case i know there's a

1804
02:02:30,360 --> 02:02:35,060
link i'm trying to put a label on that actually is a much easier problem

1805
02:02:35,060 --> 02:02:36,450
to do well

1806
02:02:36,460 --> 02:02:41,300
because you conditioning on the existence of the link verses

1807
02:02:41,330 --> 02:02:45,340
predicting whether between two arbitrary

1808
02:02:45,390 --> 02:02:49,760
they are authors whether or not they published a paper together that can be harder

1809
02:02:51,970 --> 02:02:53,750
to predict but it's one

1810
02:02:53,880 --> 02:02:55,730
people want to do all the time

1811
02:02:55,770 --> 02:02:59,920
you know you're trying to figure out how do these two people know each other

1812
02:02:59,960 --> 02:03:03,540
and then

1813
02:03:03,600 --> 02:03:05,140
a third one that

1814
02:03:05,140 --> 02:03:10,740
and i been useful in a number of situations is

1815
02:03:10,750 --> 02:03:12,590
predicting the the number of links

1816
02:03:14,310 --> 02:03:18,680
predicting citation counts for example and it's interesting to

1817
02:03:18,810 --> 02:03:20,640
keep in mind that this really

1818
02:03:20,650 --> 02:03:22,920
is a different task

1819
02:03:22,930 --> 02:03:25,840
in different inference task then

1820
02:03:25,900 --> 02:03:31,850
doing the link existence prediction now if you have a model that predicts link existence

1821
02:03:31,850 --> 02:03:34,590
obviously you could just some over

1822
02:03:34,610 --> 02:03:36,440
all possible length

1823
02:03:36,550 --> 02:03:40,740
but if you know that you don't have to actually

1824
02:03:40,800 --> 02:03:43,650
distinguish between

1825
02:03:43,690 --> 02:03:45,800
you know who someone

1826
02:03:45,860 --> 02:03:51,010
publishers with her salon in may be better to build the model that directly just

1827
02:03:51,010 --> 02:03:53,150
as the cardinality estimation

1828
02:03:53,190 --> 02:03:58,250
you know that's the model that potentially at a much higher level of abstraction he

1829
02:03:58,250 --> 02:03:59,610
may be able to

1830
02:03:59,690 --> 02:04:02,110
get a more robust

1831
02:04:02,200 --> 02:04:03,520
and estimate

1832
02:04:03,700 --> 02:04:06,660
this and so you should keep in mind when

1833
02:04:06,660 --> 02:04:09,930
you're doing this what you're goal is

1834
02:04:11,440 --> 02:04:13,070
these are

1835
02:04:13,100 --> 02:04:13,990
the edge

1836
02:04:14,000 --> 02:04:16,190
kind of task

