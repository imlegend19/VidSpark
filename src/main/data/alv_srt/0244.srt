1
00:00:00,000 --> 00:00:04,420
and actually they have going on

2
00:00:05,820 --> 00:00:07,540
o technique

3
00:00:08,860 --> 00:00:10,380
they have like

4
00:00:12,790 --> 00:00:13,990
but the

5
00:00:14,000 --> 00:00:16,290
when they right to

6
00:00:19,390 --> 00:00:20,770
on election

7
00:00:20,880 --> 00:00:22,920
for five

8
00:00:22,940 --> 00:00:29,690
the change in the world but they

9
00:00:32,570 --> 00:00:35,480
we should look like this for all possible

10
00:00:41,360 --> 00:00:45,720
but the reality

11
00:00:46,730 --> 00:00:48,460
and we are looking

12
00:00:48,510 --> 00:00:50,030
this picture

13
00:00:51,390 --> 00:00:56,150
we all try to distinguish moment and

14
00:00:56,170 --> 00:00:59,350
changes the picture always

15
00:00:59,370 --> 00:01:01,590
which for so if

16
00:01:01,760 --> 00:01:06,070
i can tell you want this to be true

17
00:01:08,800 --> 00:01:14,630
the home going with the

18
00:01:14,750 --> 00:01:19,060
it's actually and

19
00:01:20,220 --> 00:01:21,420
this paper

20
00:01:22,330 --> 00:01:25,180
when i was in the first time

21
00:01:26,290 --> 00:01:28,670
about the second part understand this

22
00:01:28,690 --> 00:01:30,220
and if i could

23
00:01:30,240 --> 00:01:32,260
like this

24
00:01:33,220 --> 00:01:34,820
complete what

25
00:01:34,850 --> 00:01:37,510
because this

26
00:01:48,110 --> 00:01:51,550
well intentioned

27
00:01:51,680 --> 00:01:56,390
what you see on the left

28
00:02:01,690 --> 00:02:04,300
it's gone north with

29
00:02:04,350 --> 00:02:09,260
the thing is i think all of these circles here but there are no actually

30
00:02:09,260 --> 00:02:10,670
on line

31
00:02:10,690 --> 00:02:12,460
so we're trying to

32
00:02:12,650 --> 00:02:16,870
i think because it featured two additional

33
00:02:16,890 --> 00:02:19,710
just to add additional part of the city

34
00:02:19,790 --> 00:02:22,780
well she

35
00:02:22,790 --> 00:02:26,470
and that was that

36
00:02:26,480 --> 00:02:27,720
which also

37
00:02:27,770 --> 00:02:32,690
four of the fact that have a lot of assumptions there is lot

38
00:02:32,710 --> 00:02:35,090
the culture

39
00:02:35,120 --> 00:02:39,180
environmental factors affect the way we things

40
00:02:39,380 --> 00:02:43,080
if i describe the picture

41
00:02:43,100 --> 00:02:49,200
how to describe

42
00:02:53,360 --> 00:02:55,780
from top to bottom from the bottom

43
00:02:55,840 --> 00:02:59,030
because i read from left to right

44
00:02:59,240 --> 00:03:01,920
actually when they

45
00:03:01,920 --> 00:03:04,900
how we are

46
00:03:04,920 --> 00:03:11,770
which is derived from right to left the all the way from top to bottom

47
00:03:20,980 --> 00:03:24,730
now mentioned our company based two of

48
00:03:24,740 --> 00:03:27,760
you should know about your

49
00:03:27,780 --> 00:03:35,950
cultural learnings about your personal experiences to retrieve images that reflect your vision

50
00:03:36,040 --> 00:03:38,760
and it's not so

51
00:03:39,080 --> 00:03:41,450
nobody can do that

52
00:03:41,530 --> 00:03:45,250
another interesting is called

53
00:03:45,260 --> 00:03:47,230
these lines

54
00:03:47,250 --> 00:03:49,740
actually of equal length

55
00:03:49,760 --> 00:03:51,490
but also seem

56
00:03:51,500 --> 00:03:54,170
but this one is this one

57
00:03:54,180 --> 00:03:55,490
i solution

58
00:03:55,500 --> 00:03:56,400
but i

59
00:03:56,420 --> 00:03:57,730
i have seen

60
00:03:57,750 --> 00:03:59,460
there was

61
00:03:59,500 --> 00:04:03,020
some information about now i want

62
00:04:05,070 --> 00:04:06,550
for me

63
00:04:08,050 --> 00:04:10,740
so the probability

64
00:04:10,760 --> 00:04:14,220
i think does

65
00:04:15,330 --> 00:04:18,590
actually a local solution the one

66
00:04:18,600 --> 00:04:20,880
i removed from

67
00:04:26,590 --> 00:04:31,110
this looks like the left side of the building

68
00:04:31,130 --> 00:04:35,300
and this one looks like inside the building

69
00:04:35,980 --> 00:04:37,900
we can

70
00:04:37,920 --> 00:04:43,070
if we are able to see the anything like this one it means that have

71
00:04:43,090 --> 00:04:45,130
o thing and you should see it

72
00:04:45,320 --> 00:04:46,980
the small one

73
00:04:47,030 --> 00:04:48,840
and if we had

74
00:04:48,900 --> 00:04:55,460
usually these walls so much we and that there should be

75
00:05:00,500 --> 00:05:02,210
a bit more serious

76
00:05:02,230 --> 00:05:06,090
actually our visual perception is

77
00:05:06,110 --> 00:05:09,750
really sophisticated

78
00:05:11,380 --> 00:05:14,980
we have someone else

79
00:05:15,000 --> 00:05:21,860
learn about how i more general books i actually think

80
00:05:22,000 --> 00:05:28,320
can we do about our but i just don't have to be able to

81
00:05:29,670 --> 00:05:33,520
we're becoming levels density

82
00:05:34,420 --> 00:05:36,000
although the

83
00:05:36,190 --> 00:05:41,050
so we can see what is happening in his life

84
00:05:42,520 --> 00:05:43,920
we can

85
00:05:44,210 --> 00:05:46,150
the same moment the time

86
00:05:46,210 --> 00:05:49,500
so i that to

87
00:05:49,500 --> 00:05:51,000
so can

88
00:05:51,090 --> 00:05:54,070
this is always

89
00:05:59,880 --> 00:06:00,710
i don't know

90
00:06:00,730 --> 00:06:06,210
does the average level of intensity which we see no

91
00:06:06,500 --> 00:06:11,940
another thing that is not the subject to write the first

92
00:06:13,980 --> 00:06:19,210
the actual line so if you will

93
00:06:19,250 --> 00:06:22,690
incremental amount of light

94
00:06:22,710 --> 00:06:24,650
the perceived

95
00:06:26,480 --> 00:06:32,800
although the logarithmic function

96
00:06:35,480 --> 00:06:37,480
even if we actually created

97
00:06:37,530 --> 00:06:42,210
all of labels which we can

98
00:06:42,230 --> 00:06:50,400
one moment of time we can see a much smaller range of

99
00:06:50,460 --> 00:06:54,020
and so on

100
00:06:54,050 --> 00:06:57,670
we can show

101
00:07:04,190 --> 00:07:08,150
just sort of like

102
00:07:09,150 --> 00:07:13,980
in the beginning but after same time can distinguish

103
00:07:13,980 --> 00:07:16,750
on average a book to

104
00:07:16,750 --> 00:07:20,750
and so sounds like

105
00:07:20,770 --> 00:07:23,570
how people understand

106
00:07:23,590 --> 00:07:25,480
you are a

107
00:07:26,300 --> 00:07:29,320
and he is on a function from the

108
00:07:29,320 --> 00:07:33,990
so i suppose this may not be what you estimate suppose you have i tell

109
00:07:33,990 --> 00:07:38,870
you had of termites decide just going to use a thousand samples

110
00:07:39,470 --> 00:07:42,950
well maybe we should make it nicer number like nine hundred

111
00:07:42,970 --> 00:07:47,300
so let's take three hundred samples and you can be very active and aggressive it's

112
00:07:47,300 --> 00:07:48,390
lacking which

113
00:07:49,870 --> 00:07:55,100
examples you'd like to get labeled from those first three hundred then the second group

114
00:07:55,100 --> 00:07:57,220
of three hundred you just going to take them

115
00:07:57,240 --> 00:08:05,340
randomly chosen randomly selected examples for labeling television two hypotheses one a one h tilde

116
00:08:05,760 --> 00:08:10,910
and then with the final third of now you just wrote say OK well h

117
00:08:10,910 --> 00:08:13,930
and h told agree on lots of areas

118
00:08:13,950 --> 00:08:16,530
but they disagree in a certain region they have two

119
00:08:16,530 --> 00:08:21,530
linear separators so you're just sample in that region where they disagree neutral state in

120
00:08:21,530 --> 00:08:22,780
that region

121
00:08:22,800 --> 00:08:26,780
who's better who's better in the area where they disagree and then you like that

122
00:08:26,780 --> 00:08:27,950
one finally

123
00:08:27,970 --> 00:08:31,870
so use a final three the thirty of your budget that way so you have

124
00:08:31,870 --> 00:08:36,030
to decide ahead of time in the set-up how many total samples you used is

125
00:08:36,030 --> 00:08:38,050
that they tend to the sick

126
00:08:42,070 --> 00:08:43,350
another question

127
00:08:43,350 --> 00:08:52,240
i mean

128
00:08:52,260 --> 00:08:57,780
according to me depends on the algorithm in certain algorithm certainly would have

129
00:08:57,780 --> 00:09:02,030
that capability if you have no noise in your your hypothesis is in your set

130
00:09:02,140 --> 00:09:09,160
using something like al gore this generalized binary search and you have adequately rich set

131
00:09:09,160 --> 00:09:13,800
of hypotheses you can actually serve to make some distinction among eral pull cool air

132
00:09:13,800 --> 00:09:16,120
hypothesis space then

133
00:09:16,140 --> 00:09:19,600
these algorithms will eventually find the correct one

134
00:09:19,720 --> 00:09:23,280
but i mean the one of the problems with those algorithms is

135
00:09:23,800 --> 00:09:29,240
sometimes it's not that easy in a given application to implement exactly that type of

136
00:09:29,240 --> 00:09:35,760
process because maintaining this version space it could be a huge object that's you computationally

137
00:09:35,760 --> 00:09:38,800
prohibitive to actually construct and maintain and so

138
00:09:38,820 --> 00:09:40,280
a lot of

139
00:09:40,300 --> 00:09:42,140
practical algorithms

140
00:09:42,200 --> 00:09:46,740
could almost be viewed as approximations to these procedures and then

141
00:09:46,760 --> 00:09:48,530
it's not so clear

142
00:09:48,530 --> 00:09:52,820
whether or not though there always work so

143
00:09:52,820 --> 00:09:58,530
so let me just have to go through these ideas so active learning for classification

144
00:09:58,570 --> 00:10:03,440
and this is heart disease example one thing that you might do is you you

145
00:10:03,440 --> 00:10:05,510
say OK i some data here's my

146
00:10:05,530 --> 00:10:08,680
best linear classifier given the data i have so far

147
00:10:08,840 --> 00:10:10,340
now here another

148
00:10:10,390 --> 00:10:14,740
set of unlabeled examples i could ask for labels for and the typical thing that

149
00:10:14,740 --> 00:10:15,740
you might see

150
00:10:15,760 --> 00:10:19,350
which again is sort of like an approximation but we've been talking about those other

151
00:10:19,780 --> 00:10:24,080
you might identify a point that seems to be you have the most uncertainty about

152
00:10:24,080 --> 00:10:30,800
one way to measure uncertainty very simple heuristic would be well points their real close

153
00:10:30,800 --> 00:10:34,240
to the decision boundary that's where i'm not so sure because i once i'm saying

154
00:10:34,240 --> 00:10:37,050
it should be read and on the other side and saying it's blue so let's

155
00:10:37,350 --> 00:10:41,510
try to query a point that is closest to the decision boundary that's one way

156
00:10:41,510 --> 00:10:42,990
of the people

157
00:10:43,430 --> 00:10:44,680
we try to

158
00:10:44,700 --> 00:10:49,490
update linear classifiers and we get label for that point and that would allow us

159
00:10:49,490 --> 00:10:54,070
to refine our notion of the decision boundary so the idea to to keep in

160
00:10:54,070 --> 00:10:55,120
mind in this

161
00:10:55,140 --> 00:10:59,870
classification cases that active learning is served narrowing down

162
00:10:59,890 --> 00:11:04,490
the precise location of the best decision boundary

163
00:11:04,800 --> 00:11:07,180
and this is an example

164
00:11:07,240 --> 00:11:09,990
that illustrates this

165
00:11:10,030 --> 00:11:12,720
which is motivated by sensor networking problem so

166
00:11:12,740 --> 00:11:16,490
suppose that we had a sensor network so each of these black nodes is the

167
00:11:16,490 --> 00:11:19,620
sensor network deployed over some area

168
00:11:19,620 --> 00:11:25,010
and there are two environmental conditions the blue condition in the right conditions and we

169
00:11:25,010 --> 00:11:29,280
don't know that but are sensors can report whether they measuring random measuring blows to

170
00:11:29,280 --> 00:11:34,100
think of again is this oil spill or contaminant or something and let's suppose our

171
00:11:34,120 --> 00:11:39,410
hypothesis is that the the boundary between these two is just a linear boundary to

172
00:11:39,410 --> 00:11:43,390
keep things simple so we have a hundred sensors in this example that would give

173
00:11:43,390 --> 00:11:49,740
us a total of about ninety nine hundred different possible linear separations of the sensors

174
00:11:49,840 --> 00:11:55,680
and if you run an algorithm like this splitting algorithm is generalized binary search algorithm

175
00:11:55,950 --> 00:11:57,390
these are the points

176
00:11:57,410 --> 00:12:02,600
that the algorithm would select query and you can see as it goes along it

177
00:12:02,600 --> 00:12:08,740
starts to begin to focus on those examples nearest to what turns out to be

178
00:12:08,740 --> 00:12:11,510
the decision boundary in this case so it doesn't know

179
00:12:11,530 --> 00:12:15,450
that this white line is the boundary but it starts

180
00:12:15,660 --> 00:12:19,720
comment on that automatically so is doing a lot

181
00:12:19,720 --> 00:12:28,460
emerged after

182
00:12:28,500 --> 00:12:33,290
and this time we started on solving differential equations

183
00:12:33,710 --> 00:12:36,700
this is the 3rd lecture of the

184
00:12:36,770 --> 00:12:43,040
term I have yet solve single differential equation in this class of well there will

185
00:12:43,040 --> 00:12:50,560
be rectified from now until the end of of the term or so once you

186
00:12:50,560 --> 00:12:57,270
learn separation of variables which is the most elementary method there is the single I

187
00:12:57,270 --> 00:13:00,880
think the single most important equation

188
00:13:01,440 --> 00:13:06,850
is the 1 that's called first order linear equation but also because it occurs frequently

189
00:13:06,850 --> 00:13:09,390
in models because it's all

190
00:13:12,640 --> 00:13:14,850
I think that's a lot of the

191
00:13:15,610 --> 00:13:20,530
if you drop the course that today you'll still have learned the most important methods

192
00:13:20,530 --> 00:13:26,160
separation of variables and first order linear equations so what is such an equation works

193
00:13:26,160 --> 00:13:30,610
quite well I'll write it in that there are several ways of writing it but

194
00:13:30,610 --> 00:13:35,950
I think the most basic conditions and I'm going to use acts as the independent

195
00:13:35,950 --> 00:13:42,250
variable because that's where your book does but in the applications it's often the time

196
00:13:42,250 --> 00:13:47,130
that the independent variable and also try to to give you examples which show that

197
00:13:47,130 --> 00:13:53,310
half of the equation looks like there's a problem of finding some function of X

198
00:13:53,750 --> 00:14:00,870
times y prime plus some other function of X times Y is equal to yet

199
00:14:00,870 --> 00:14:07,720
another function of X of obviously the X doesn't have the same status here that

200
00:14:07,720 --> 00:14:12,270
why does the why is extremely limited how it can appear in the equations for

201
00:14:12,270 --> 00:14:19,110
the x can be pretty much arbitrary like in those places so that's the equation

202
00:14:19,110 --> 00:14:22,910
we're talking about and and I'll put up

203
00:14:23,200 --> 00:14:27,420
this is the 1st version of it and will call them now

204
00:14:27,470 --> 00:14:34,210
why is that called the linear equations the word linear is a very heavily used

205
00:14:34,220 --> 00:14:39,690
word in mathematics science and engineering for the moment he

206
00:14:39,940 --> 00:14:46,170
the best simple answer is because it's a linear

207
00:14:46,190 --> 00:14:47,870
In variety

208
00:14:48,130 --> 00:14:53,280
and why prime variables y and y primal white prime is not a variable well

209
00:14:53,870 --> 00:14:58,260
you'll learn in a certain sense it helps us think of it as 1 not

210
00:14:58,260 --> 00:15:02,100
right now perhaps but think of it as a linear

211
00:15:02,910 --> 00:15:10,040
analogy the most closely analogous thing would be the equation of acquire a linear equation

212
00:15:10,040 --> 00:15:14,560
that looked like a real linear equation the kind you studied in high school which

213
00:15:14,660 --> 00:15:22,660
would look like this you have 2 variables and ideas constant coefficients and you'll see

214
00:15:23,310 --> 00:15:28,150
that's linear equation and that's the sense in which this is linear it's linear and

215
00:15:28,150 --> 00:15:34,480
why prime and which of the analogs of the variables Y 1 and y 2

216
00:15:35,390 --> 00:15:45,390
a little bit of terminology of C is equal to 0 it's called homogeneous

217
00:15:45,510 --> 00:15:50,790
the same way this equation is called homogeneous as you know from 1802 if the

218
00:15:50,790 --> 00:15:57,590
right hand side is 0 sociolects I should write here but I won't that's called

219
00:15:57,590 --> 00:16:06,670
homogeneous now this is of a common form the equation but it's not what's called

220
00:16:06,680 --> 00:16:16,810
standard the standard form for the equation and since visibility of prime courses to primary

221
00:16:17,010 --> 00:16:23,390
course of confusion which is probably completely correct but a prime source of confusion as

222
00:16:23,390 --> 00:16:28,110
well as that of the standard

223
00:16:28,130 --> 00:16:40,120
linear form and no underlying linear is the 1st coefficient of Y prime is taken

224
00:16:40,120 --> 00:16:45,830
to be 1 so if I divide you can always convert that to a standard

225
00:16:45,830 --> 00:16:50,120
form by simply dividing through by it and if I do that the equation will

226
00:16:50,120 --> 00:16:56,860
look like why prime plus no it's common to not call be anymore the coefficient

227
00:16:56,870 --> 00:17:02,420
because really be already and therefore it's common adopted yet and a letter for it

228
00:17:02,420 --> 00:17:08,600
and the standard 1 that many people used to speak about the right hand side

229
00:17:08,600 --> 00:17:12,990
we need a new letter for that to it's C overage but will call

230
00:17:13,950 --> 00:17:25,150
Cuba so I talk about the standard linear form for for linear first-order equation is

231
00:17:25,450 --> 00:17:31,590
absolutely that that I'm talking about now you immediately see there's a potential for confusion

232
00:17:31,590 --> 00:17:40,100
here because what I call the standard form for a first-order equations so I'm not

233
00:17:41,730 --> 00:17:47,250
the standard first-order forms

234
00:17:47,270 --> 00:17:52,950
it was what would that be

235
00:17:53,010 --> 00:17:57,860
well it would be why prime equals and everything else on the left hand side

236
00:17:58,280 --> 00:18:04,120
so it would be white prime and now if I turn this into the standard

237
00:18:04,120 --> 00:18:05,670
i'm afraid

238
00:18:05,940 --> 00:18:09,170
graph cuts for image segmentation

239
00:18:09,190 --> 00:18:11,610
so the image segmentation is

240
00:18:11,670 --> 00:18:14,300
that's a very important issue in

241
00:18:14,320 --> 00:18:16,260
computer vision

242
00:18:16,270 --> 00:18:18,200
given an image

243
00:18:18,250 --> 00:18:21,070
how do i break it up into bits have some

244
00:18:24,200 --> 00:18:27,100
so really the application of graph cuts

245
00:18:27,170 --> 00:18:29,140
in this area stems

246
00:18:29,190 --> 00:18:34,730
boykov and jolly really but there have been people working on this somerfield tools results

247
00:18:34,780 --> 00:18:36,710
later on

248
00:18:36,810 --> 00:18:39,270
the question is how do you set this up

249
00:18:39,320 --> 00:18:41,400
as a markov random field

250
00:18:43,110 --> 00:18:47,970
when i get out of serial or show the markov random finding the

251
00:18:48,810 --> 00:18:53,200
what you're trying to do with markov random field is the probability distribution but you're

252
00:18:53,220 --> 00:18:55,410
trying to find the most probable

253
00:18:55,420 --> 00:18:59,350
state to find most probable given that

254
00:18:59,570 --> 00:19:03,210
it's turns out that's equivalent to just energy

255
00:19:03,260 --> 00:19:07,720
minimisation problem as we'll see how do we how do we define

256
00:19:07,810 --> 00:19:12,150
this energy minimisation so

257
00:19:12,200 --> 00:19:15,150
here's an example i guess original paper

258
00:19:18,810 --> 00:19:19,660
in fact

259
00:19:19,670 --> 00:19:22,200
this was always in these

260
00:19:22,240 --> 00:19:23,210
graph have

261
00:19:24,260 --> 00:19:25,200
doing this

262
00:19:25,260 --> 00:19:28,300
you have two types of energy

263
00:19:28,310 --> 00:19:31,200
very similar to what i showed the hysteria as well

264
00:19:31,210 --> 00:19:34,050
you have an energy which is just pixel by pixel

265
00:19:34,070 --> 00:19:37,420
so OK you take you take a picture typically this is the way you do

266
00:19:37,420 --> 00:19:39,860
things you take a picture

267
00:19:42,320 --> 00:19:44,200
like here

268
00:19:45,570 --> 00:19:54,650
the lights which dim down from the rules in one

269
00:19:54,670 --> 00:19:56,100
OK well i guess you see

270
00:19:57,920 --> 00:20:00,070
you take a picture typically

271
00:20:00,080 --> 00:20:03,700
and often there is little bit of interaction here because you want to know what

272
00:20:03,700 --> 00:20:05,810
you want to segment right so

273
00:20:06,060 --> 00:20:08,280
person will come and sort of

274
00:20:08,340 --> 00:20:10,690
do i

275
00:20:10,730 --> 00:20:13,650
you can click on a few points or draw line

276
00:20:14,550 --> 00:20:18,050
so the foreground object which in this case the bird

277
00:20:19,310 --> 00:20:22,460
take some points on the background object

278
00:20:23,360 --> 00:20:24,610
so far as to what i

279
00:20:24,630 --> 00:20:27,310
however these are samples

280
00:20:28,790 --> 00:20:33,440
the foreground and background so there you have some samples of what i consider to

281
00:20:33,440 --> 00:20:38,460
be foreground i want to say without sample what i consider background

282
00:20:38,470 --> 00:20:40,730
one of the wanted segments

283
00:20:40,740 --> 00:20:41,960
and those give you

284
00:20:41,970 --> 00:20:43,360
an empirical

285
00:20:46,820 --> 00:20:49,330
and so you can then

286
00:20:49,340 --> 00:20:51,130
user sort of two

287
00:20:51,150 --> 00:20:52,830
two clusters

288
00:20:53,790 --> 00:20:56,140
i know whatever you like the

289
00:20:56,170 --> 00:21:00,590
support vector machine something i try work out given the pixel which class

290
00:21:01,560 --> 00:21:03,780
fall into that gives us away

291
00:21:03,820 --> 00:21:05,550
of giving the label

292
00:21:05,590 --> 00:21:07,390
to every pixel OK

293
00:21:09,540 --> 00:21:11,500
what you use

294
00:21:11,550 --> 00:21:12,440
i said

295
00:21:12,440 --> 00:21:17,310
you you you take samples what you take samples of probably not sufficient just take

296
00:21:17,310 --> 00:21:18,580
the colour

297
00:21:18,630 --> 00:21:23,000
that pixel might take the color neighborhood gets some texture descriptors

298
00:21:23,030 --> 00:21:25,540
i things work out based on

299
00:21:25,550 --> 00:21:28,230
based on the small neighborhood what pixel

300
00:21:28,230 --> 00:21:34,150
each what which class each should belong to so the two classes here foreground

301
00:21:35,190 --> 00:21:36,470
we want to label

302
00:21:36,480 --> 00:21:38,970
before rounding the good ones

303
00:21:38,990 --> 00:21:40,500
a little background

304
00:21:41,480 --> 00:21:45,270
OK so that gives you the so-called

305
00:21:45,280 --> 00:21:48,960
the pixel by pixel you returned cost function

306
00:21:48,970 --> 00:21:50,990
which are the unitary like likelihood

307
00:21:51,040 --> 00:21:55,720
first on the right in this expression

308
00:21:57,160 --> 00:21:58,700
you also have

309
00:22:04,630 --> 00:22:06,580
as i

310
00:22:06,620 --> 00:22:09,240
shouldn't slide the beginning

311
00:22:14,590 --> 00:22:20,670
as an image

312
00:22:20,740 --> 00:22:22,920
each of these circles

313
00:22:22,980 --> 00:22:27,520
represents the pixel

314
00:22:27,530 --> 00:22:29,240
the track

315
00:22:29,280 --> 00:22:31,780
and each of these i have

316
00:22:34,620 --> 00:22:36,980
d stands about

317
00:22:37,020 --> 00:22:39,960
this is the given this this pixel x

318
00:22:39,970 --> 00:22:42,870
what is the cost of putting that in one class was the cost of putting

319
00:22:42,870 --> 00:22:44,070
in class one

320
00:22:44,130 --> 00:22:46,590
what the cost putting class europe

321
00:22:46,600 --> 00:22:49,080
but you also have these terms

322
00:22:52,350 --> 00:22:54,640
pixel up and down left and right

323
00:22:54,760 --> 00:22:59,740
for each of those each of those years sign in terms of

324
00:23:01,110 --> 00:23:03,740
depends on the pair

325
00:23:03,740 --> 00:23:08,740
so so for instance this is the function of

326
00:23:08,810 --> 00:23:09,950
x one

327
00:23:10,000 --> 00:23:11,450
x two

328
00:23:11,460 --> 00:23:15,290
this x one this is in the set zero and one

329
00:23:15,300 --> 00:23:16,610
this is in the set

330
00:23:16,630 --> 00:23:18,030
zero and one

331
00:23:18,030 --> 00:23:19,110
in other words

332
00:23:19,140 --> 00:23:21,010
what is the cost

333
00:23:21,050 --> 00:23:22,070
the signing

334
00:23:22,120 --> 00:23:24,330
this pixel in the foreground

335
00:23:24,340 --> 00:23:28,250
and this piece foreground also cost finding this piece in the foreground

336
00:23:28,310 --> 00:23:30,540
this pixel background

337
00:23:33,570 --> 00:23:36,600
there are various things you can do here one of them is just sort of

338
00:23:36,600 --> 00:23:37,900
say up front

339
00:23:39,950 --> 00:23:41,460
pixels that

340
00:23:41,470 --> 00:23:42,900
next to each other

341
00:23:43,000 --> 00:23:44,310
belong together

342
00:23:46,470 --> 00:23:48,070
that's very rough

343
00:23:48,090 --> 00:23:50,970
but fairly useful

344
00:23:50,980 --> 00:23:54,730
criterion so you signed cost

345
00:23:54,810 --> 00:23:56,450
the cost

346
00:23:56,490 --> 00:23:58,490
having zero one

347
00:23:58,530 --> 00:24:00,050
some value

348
00:24:01,700 --> 00:24:02,990
the cost

349
00:24:03,040 --> 00:24:05,780
this is the pixel x i x j

350
00:24:05,790 --> 00:24:07,080
the cost of having it

351
00:24:07,100 --> 00:24:08,470
one zero

352
00:24:08,470 --> 00:24:10,940
it's also value but the cost

353
00:24:10,990 --> 00:24:13,200
of having them zero and zero

354
00:24:13,280 --> 00:24:14,560
no cost

355
00:24:14,580 --> 00:24:16,050
assigned that

356
00:24:16,110 --> 00:24:19,440
one one so in other words

357
00:24:20,490 --> 00:24:21,600
i assign

358
00:24:22,660 --> 00:24:25,010
pixels to the same class

359
00:24:25,010 --> 00:24:26,360
and can cost

360
00:24:27,570 --> 00:24:31,000
i person card so that i think it is really worthwhile

361
00:24:31,000 --> 00:24:35,290
assigned different classes right so this is known as

362
00:24:36,410 --> 00:24:39,630
the potts model

363
00:24:39,640 --> 00:24:41,510
the potts model

364
00:24:43,390 --> 00:24:49,040
markov random field is something which comes from the dynamics of that

365
00:24:49,990 --> 00:24:52,590
o thing called the ising model which is similar to

366
00:24:52,610 --> 00:24:55,940
but maybe this is the ising model in fact in this particular case

367
00:24:58,620 --> 00:25:03,090
that's one type to doesn't actually look at the just sort of say OK things

368
00:25:03,090 --> 00:25:04,950
which are

369
00:25:04,990 --> 00:25:07,890
next to each other probably not the same class typically

370
00:25:07,910 --> 00:25:10,090
you can also

371
00:25:10,120 --> 00:25:14,100
have terms like the middle one so the right hand when we talk about the

372
00:25:14,100 --> 00:25:15,840
middle one sort of

373
00:25:15,850 --> 00:25:19,250
is are adopted driven

374
00:25:19,250 --> 00:25:24,540
hello i'm from his university it's actually work with my peers otherwise professors cell

375
00:25:25,250 --> 00:25:30,120
and today i'm going to talk about sailing ships studio factory in depth images

376
00:25:31,890 --> 00:25:35,130
after hatching about the most fundamental problems in computer vision

377
00:25:35,740 --> 00:25:38,650
it has is like give you like this for example

378
00:25:39,250 --> 00:25:43,090
we want a computer to recognize objects and the categories automatically

379
00:25:43,520 --> 00:25:45,870
so in this case is me and etiquette

380
00:25:47,120 --> 00:25:53,180
and over the past decade they're all comedians actually work with corey's our detection especially

381
00:25:53,190 --> 00:25:57,740
the current state of the art which used amazing people earning which is considered as

382
00:25:57,740 --> 00:26:00,200
well the top ten greatest breakthrough in science

383
00:26:00,980 --> 00:26:01,820
so just to see

384
00:26:02,230 --> 00:26:05,440
how far we are from solving this problem let's do a little experiment

385
00:26:05,850 --> 00:26:08,490
which are called the album only dataset

386
00:26:09,510 --> 00:26:15,070
well take the sales are or fee from professor young and tested on the proper

387
00:26:15,510 --> 00:26:20,010
they have proposed focus group and see how well it performs the off-the-shelf

388
00:26:21,210 --> 00:26:22,490
so here is the input image

389
00:26:22,970 --> 00:26:25,380
you can see inside the bounding box is clearly a char

390
00:26:26,220 --> 00:26:33,210
and the if we think this image patch told off and here is the five most likely categories within predict

391
00:26:33,850 --> 00:26:37,680
so you can see thousands many managed to financials and then you cable

392
00:26:39,240 --> 00:26:40,150
let's try another one

393
00:26:41,060 --> 00:26:46,040
this this chair is recognised support as well as well is not too bad

394
00:26:46,550 --> 00:26:49,600
at least sounds make making crack that's both of them kind okay

395
00:26:51,010 --> 00:26:52,170
so let's have another try

396
00:26:52,820 --> 00:26:56,630
well this this attack is recognised as a special type of dog

397
00:26:57,860 --> 00:27:00,280
so please tell me what is wrong here

398
00:27:01,630 --> 00:27:02,220
absolutely no

399
00:27:02,700 --> 00:27:05,130
most of these algorithms actually now you mention it

400
00:27:05,670 --> 00:27:07,240
higher forty years ago

401
00:27:07,700 --> 00:27:12,680
image now also adults otherwise is at the basis as otherwise has already told us

402
00:27:12,720 --> 00:27:18,040
at this temple techniques we're not going to work for cinema analyzes for many reasons

403
00:27:18,570 --> 00:27:19,110
for example

404
00:27:19,770 --> 00:27:20,770
larger variations

405
00:27:21,440 --> 00:27:22,280
will point difference

406
00:27:24,740 --> 00:27:26,460
carter like my is room

407
00:27:27,310 --> 00:27:29,480
and no cushion cos there's a little cute dog

408
00:27:30,200 --> 00:27:33,690
and all the problems make already had to really really difficult

409
00:27:34,070 --> 00:27:38,600
and that's we know all the deep learning godfather such as you really super genius

410
00:27:38,900 --> 00:27:42,170
and professor can figure out how the brain works for many many fans

411
00:27:42,690 --> 00:27:48,200
and you can now figure out how to solve this problem how appropiate is still like myself can do anything

412
00:27:49,580 --> 00:27:54,590
so probably is the thing about the album because i thinking about another level you could

413
00:27:55,780 --> 00:27:57,210
so maybe that's not help

414
00:27:58,060 --> 00:28:03,970
absolutely no people care a lot more about playing games compared to going to the more of something has a

415
00:28:04,440 --> 00:28:09,970
and that the market companies designs a lot of local citizens such as microsoft connect

416
00:28:10,740 --> 00:28:15,230
and they have all citizens are you know was a lot of places in computer vision tasks

417
00:28:15,750 --> 00:28:16,210
for example

418
00:28:16,760 --> 00:28:18,600
the city reconstruction is the confusion

419
00:28:19,840 --> 00:28:21,110
you can image inference

420
00:28:21,780 --> 00:28:26,150
and popular human pose estimation which enable you to catch the for use body

421
00:28:27,360 --> 00:28:31,870
so today i'm going to introduce initiatives a studio detector in depth images

422
00:28:32,350 --> 00:28:35,000
so you put a lot of our system will be a single

423
00:28:35,650 --> 00:28:40,590
can that's map and the output will be a set of city bombing boxes with object labels

424
00:28:42,020 --> 00:28:48,710
by using that's or detector can significantly outperforms the current state of state-of-the-art algorithms based on colour image

425
00:28:49,140 --> 00:28:52,330
and achieve about one percent improvement on the average precision

426
00:28:53,720 --> 00:28:59,750
so now i'm going to talk about our first and then apply some analysis about the way this doesn't work

427
00:29:01,210 --> 00:29:06,020
so given on category for example tower in this case where the first collect set

428
00:29:06,920 --> 00:29:08,630
city can models from the internet

429
00:29:09,650 --> 00:29:12,410
and therefore each of the can city can model

430
00:29:12,810 --> 00:29:15,450
well rendered that's map from different viewpoints

431
00:29:16,350 --> 00:29:21,210
just like this we continue this until color the typical will over object

432
00:29:22,450 --> 00:29:27,630
and therefore each that's not projects to city point and compute on features and this

433
00:29:27,630 --> 00:29:32,040
really point clouds and use this as the only positive examples and come out with

434
00:29:32,050 --> 00:29:37,140
an active from labeled can that's my we try a linear example acid with hard

435
00:29:38,850 --> 00:29:41,540
so now i'm going to talk about how we actually compute the features and is

436
00:29:42,450 --> 00:29:44,720
if you still remember to case like the hough

437
00:29:45,380 --> 00:29:50,140
because the image into cells and we compute the features for each cell and concatenate

438
00:29:50,260 --> 00:29:53,060
to one feature vector which corresponding to the maximum weight

439
00:29:53,790 --> 00:29:54,790
and this is is really

440
00:29:55,250 --> 00:29:59,740
because the city space industry cells and compute the features we just is out and

441
00:29:59,740 --> 00:30:03,580
can kind featured twenty vector and use them to learn the weight

442
00:30:04,890 --> 00:30:10,490
and in order to capture swedish shape information with these unforeseeable features like supplying high density

443
00:30:11,330 --> 00:30:12,020
surface normal

444
00:30:12,640 --> 00:30:15,210
surface shape and the truncated signed distance function

445
00:30:15,800 --> 00:30:18,940
and the comments whatsoever features a sort of unofficial vector

446
00:30:19,470 --> 00:30:21,160
and use it to train unless lesbian

447
00:30:22,480 --> 00:30:27,740
and the think we're sliding window is ready to you the score for each window

448
00:30:28,020 --> 00:30:30,920
and use the example that use the training data streams

449
00:30:31,530 --> 00:30:35,320
and we use the physical size of a typical territories that the size of a

450
00:30:35,320 --> 00:30:39,380
sliding window so that we don't need to do the multiscale scanning like twenty case

451
00:30:40,690 --> 00:30:41,640
so for each location

452
00:30:42,050 --> 00:30:45,800
all the examples as we tell whether it is a town not if all them

453
00:30:45,930 --> 00:30:47,520
know that is probably not here

454
00:30:48,140 --> 00:30:51,280
and we continue to do this cover all the city space

455
00:30:53,380 --> 00:30:56,590
and some of the example and so yes it is probably actually

456
00:30:57,200 --> 00:31:02,980
and sometimes multiple detector get for the same location then we use non maxima suppression began the final detection score

457
00:31:04,240 --> 00:31:08,230
here is the final detection result with the corresponding summer we used

458
00:31:09,820 --> 00:31:10,980
here's another result

459
00:31:11,490 --> 00:31:16,670
on the left is the current that's image and on the right is a playing how is this city models

460
00:31:17,900 --> 00:31:22,330
o system all use that that's actually includes and carries only hope is registered with the data

461
00:31:22,930 --> 00:31:27,550
and then you can see that in this case we use tentative unfolding tell with

462
00:31:27,800 --> 00:31:30,510
which demonstrate the ability to generalize four model

463
00:31:32,390 --> 00:31:35,750
here's another example is the better with the bat and the chair detected

464
00:31:40,680 --> 00:31:43,340
yet another one you can see that the chair on the right

465
00:31:44,040 --> 00:31:47,070
it's correctly detected but actually with the wrong orientation

466
00:31:50,410 --> 00:31:52,750
we were all systems on the problem i

467
00:31:53,730 --> 00:31:57,880
the following categories and twenty that's over the cable

468
00:31:58,320 --> 00:32:06,130
and across different categories also detectors significantly outperforms the deviant and on various datasets and argentina haskell we'll see

469
00:32:07,440 --> 00:32:11,010
and just to give you some intuition why we are better now as shown side-by-side

470
00:32:11,010 --> 00:32:15,440
comparison with over said that's this shapes and the color-based if you

471
00:32:16,210 --> 00:32:21,100
but in this case was about getting correct but artifact complexity bounding box

472
00:32:22,420 --> 00:32:26,120
and in this case that if you fail to recognise the chair because it's too dark

473
00:32:26,540 --> 00:32:30,560
however by using that's we are not we not influenced by the lighting conditions

474
00:32:31,930 --> 00:32:37,090
and is it in this example that if you have a false positive with iran size of bounding box

475
00:32:37,540 --> 00:32:41,520
however because we use the physical size of a chapter so this is also a

476
00:32:41,520 --> 00:32:44,480
this is the integral in going from a to b

477
00:32:44,590 --> 00:32:47,210
f of the or

478
00:32:47,220 --> 00:32:50,550
and the only term that i have is the one

479
00:32:50,560 --> 00:32:52,130
that deals with

480
00:32:52,140 --> 00:32:54,130
the y direction the other terms

481
00:32:54,180 --> 00:32:56,340
if nothing in it so it is the integral

482
00:32:56,390 --> 00:32:58,980
in going from a to b

483
00:32:58,990 --> 00:33:00,210
if f y

484
00:33:00,340 --> 00:33:03,160
the y

485
00:33:03,210 --> 00:33:03,920
and that

486
00:33:03,950 --> 00:33:05,710
equals minus and g

487
00:33:05,710 --> 00:33:06,740
as we have the

488
00:33:07,300 --> 00:33:08,350
minus and g

489
00:33:11,160 --> 00:33:13,170
while b

490
00:33:13,310 --> 00:33:16,720
might violate sort of these miners and g

491
00:33:16,830 --> 00:33:19,000
times age

492
00:33:19,020 --> 00:33:20,460
what you see here

493
00:33:21,190 --> 00:33:24,760
it is completely independent of the past that i have chosen

494
00:33:24,810 --> 00:33:26,920
it doesn't matter how i move

495
00:33:26,930 --> 00:33:28,490
the only thing that matters

496
00:33:28,520 --> 00:33:31,870
is the difference in height between point a and point b

497
00:33:31,930 --> 00:33:33,090
it could be

498
00:33:33,100 --> 00:33:37,340
larger than zero if b is about eight could be smaller than zero

499
00:33:37,340 --> 00:33:40,620
if b is below a it could be equal to zero if b has the

500
00:33:42,090 --> 00:33:43,620
as a

501
00:33:43,630 --> 00:33:48,380
whenever the work that is done by the force is independent of it passes only

502
00:33:48,380 --> 00:33:55,840
determined by the starting point and end point that force is called a conservative force

503
00:33:55,880 --> 00:34:00,090
it's a very important concept in physics i repeated whenever the work that is done

504
00:34:00,090 --> 00:34:01,300
by a force

505
00:34:01,310 --> 00:34:03,320
in going from one point to another

506
00:34:03,410 --> 00:34:07,620
it is independent of the pass is only determined by the starting point in the

507
00:34:07,620 --> 00:34:16,160
end point we call that a conservative forces gravity is a conservative force very clear

508
00:34:16,210 --> 00:34:17,510
i suppose

509
00:34:20,270 --> 00:34:21,450
do the work

510
00:34:21,460 --> 00:34:22,600
i go from

511
00:34:22,660 --> 00:34:23,790
a to b

512
00:34:23,800 --> 00:34:25,740
in some very strange way

513
00:34:25,750 --> 00:34:27,040
and is very clear

514
00:34:27,090 --> 00:34:28,350
it to work

515
00:34:28,410 --> 00:34:32,270
i would have done would be plus

516
00:34:32,340 --> 00:34:34,410
and you because my force of course

517
00:34:34,410 --> 00:34:36,440
it's exactly in the opposite direction

518
00:34:37,840 --> 00:34:40,250
so whatever gravity is doing

519
00:34:40,320 --> 00:34:41,540
positive work

520
00:34:41,550 --> 00:34:43,440
i would be doing negative work

521
00:34:43,450 --> 00:34:45,840
i hold in my hand when i'm doing

522
00:34:45,890 --> 00:34:47,760
positive work gravity

523
00:34:47,840 --> 00:34:48,920
is doing

524
00:34:48,980 --> 00:34:51,590
negative work

525
00:34:54,400 --> 00:34:57,640
again i'm going to concentrate now on the case where we deal with

526
00:35:00,010 --> 00:35:01,990
when there's only gravity

527
00:35:02,740 --> 00:35:04,300
we have the

528
00:35:04,310 --> 00:35:05,980
minus and gh

529
00:35:07,620 --> 00:35:08,460
is to work

530
00:35:08,470 --> 00:35:11,420
done going from

531
00:35:11,420 --> 00:35:12,910
a to b

532
00:35:13,860 --> 00:35:15,940
miners and g

533
00:35:15,990 --> 00:35:18,350
times y b

534
00:35:18,350 --> 00:35:20,270
my widely

535
00:35:20,290 --> 00:35:22,400
and that now is the kinetic energy

536
00:35:22,450 --> 00:35:24,620
at point being

537
00:35:24,630 --> 00:35:26,580
minus the kinetic energy

538
00:35:26,600 --> 00:35:32,000
o point eight is it to work energy theorem

539
00:35:32,020 --> 00:35:35,290
look closely here i can rearrange this and i can bring the bees on one

540
00:35:36,050 --> 00:35:38,320
i can we be eight alongside

541
00:35:38,370 --> 00:35:39,480
i don't get

542
00:35:39,500 --> 00:35:40,770
and g

543
00:35:40,790 --> 00:35:42,980
times y of b

544
00:35:43,910 --> 00:35:47,200
the kinetic energy at point b

545
00:35:47,210 --> 00:35:48,910
calls and g

546
00:35:48,990 --> 00:35:54,100
times y a plus the kinetic energy one

547
00:35:54,110 --> 00:35:56,710
and this is truly amazing results

548
00:35:56,730 --> 00:36:00,520
we call

549
00:36:00,560 --> 00:36:02,130
and g y

550
00:36:02,230 --> 00:36:05,060
we give the name

551
00:36:05,120 --> 00:36:07,190
and we call that

552
00:36:08,970 --> 00:36:11,810
potential energy

553
00:36:11,870 --> 00:36:17,060
often we write with that p e we write for that you

554
00:36:17,140 --> 00:36:18,810
what you're seeing here is

555
00:36:18,820 --> 00:36:20,460
that the sum

556
00:36:21,370 --> 00:36:23,900
potential energy at point b

557
00:36:24,010 --> 00:36:25,630
and the kinetic energy

558
00:36:25,670 --> 00:36:26,710
at point

559
00:36:26,750 --> 00:36:27,970
is the same

560
00:36:27,970 --> 00:36:33,190
has the potential energy a kinetic energy at point a

561
00:36:33,190 --> 00:36:37,130
one can be converted into the other and can be converted back kinetic energy can

562
00:36:37,130 --> 00:36:38,410
be converted back to

563
00:36:38,430 --> 00:36:43,070
potential energy and potential energy can be converted back but

564
00:36:43,160 --> 00:36:46,620
the some of them which we call mechanical energy

565
00:36:46,630 --> 00:36:48,610
this conserved

566
00:36:48,620 --> 00:36:51,470
mechanical energy is only conserved

567
00:36:51,510 --> 00:36:52,780
if the force

568
00:36:52,790 --> 00:36:54,930
is a conservative force

569
00:36:54,960 --> 00:36:59,250
it's extremely useful we'll use it many times which you have to be very careful

570
00:36:59,250 --> 00:37:00,870
and standing around

571
00:37:00,920 --> 00:37:02,420
so you have

572
00:37:02,520 --> 00:37:09,500
these which decrease

573
00:37:10,330 --> 00:37:13,770
can you have the number of regular support vectors which increases

574
00:37:13,790 --> 00:37:15,290
in this area

575
00:37:15,310 --> 00:37:18,290
so all here we have the deep you know what

576
00:37:18,330 --> 00:37:19,790
a performance dip

577
00:37:19,830 --> 00:37:23,770
and is the deep in terms of the number of support vectors

578
00:37:23,810 --> 00:37:27,620
so she tuned c

579
00:37:27,650 --> 00:37:34,380
actually what you see here is taken we set

580
00:37:34,400 --> 00:37:36,150
a sequel one hundred

581
00:37:36,250 --> 00:37:40,960
something this which is quite quite large

582
00:37:47,690 --> 00:37:54,120
so here it is

583
00:38:00,380 --> 00:38:28,460
because you have the number of features

584
00:38:29,620 --> 00:38:33,420
i did i did we could also with the number of training documents

585
00:38:39,670 --> 00:38:41,120
so it is recorded

586
00:38:41,120 --> 00:38:44,940
now have so many viewpoints do i to six some

587
00:38:44,980 --> 00:38:55,810
if you varies in both training documents

588
00:38:55,870 --> 00:39:00,080
well i expect because you number is the number of features and of documents of

589
00:39:00,080 --> 00:39:05,100
training documents so i expect the same but and i suspect the same sensing

590
00:39:05,120 --> 00:39:09,690
then what i see when i take is pictures with it is in this case

591
00:39:09,730 --> 00:39:14,290
where we have to the number of support vectors is taken here

592
00:39:14,310 --> 00:39:17,330
here force equal one hundred so you have

593
00:39:17,350 --> 00:39:19,730
train set of two thousand

594
00:39:20,580 --> 00:39:22,400
one i the same

595
00:39:22,440 --> 00:39:27,270
but in the just to show is that the deep in terms of sparsity of

596
00:39:27,270 --> 00:39:28,830
the solution we move

597
00:39:28,850 --> 00:39:32,980
together we set the number of documents

598
00:39:32,980 --> 00:39:35,000
why i don't know

599
00:39:35,020 --> 00:39:38,480
obviously the resulting together because

600
00:39:38,500 --> 00:39:42,670
he has the maximum number of features

601
00:39:42,710 --> 00:39:45,140
given the twenty two documents

602
00:39:45,190 --> 00:39:46,500
so you have about

603
00:39:46,500 --> 00:39:49,400
seven hundred different features

604
00:39:49,440 --> 00:39:51,600
in twenty two documents and then it

605
00:39:51,620 --> 00:39:54,770
it increases

606
00:39:54,790 --> 00:39:57,600
united together smallest

607
00:39:57,900 --> 00:39:59,400
the maximum number of

608
00:40:01,060 --> 00:40:05,600
so the size of the bubble fourteen increase and you

609
00:40:05,600 --> 00:40:10,520
so increase of few transit

610
00:40:29,640 --> 00:40:33,460
f one measure

611
00:40:33,500 --> 00:40:41,940
and if it gives out sparsity from user in good results

612
00:40:42,000 --> 00:40:44,170
why is the gap between

613
00:40:44,170 --> 00:40:49,540
my problem is the gap between regular techniques and

614
00:40:49,540 --> 00:40:52,060
and SVM is not so huge

615
00:40:52,080 --> 00:40:55,870
thank you here it is

616
00:40:55,920 --> 00:40:58,870
thank you your one nearest neighbor

617
00:40:58,920 --> 00:41:04,730
and then you have as yet made by is and can research more more labels

618
00:41:04,750 --> 00:41:07,830
and then the gain is

619
00:41:07,880 --> 00:41:10,960
when you just have twenty or ten percent again

620
00:41:10,960 --> 00:41:14,060
is not a huge

621
00:41:14,080 --> 00:41:16,690
thing to sparsity

622
00:41:17,870 --> 00:41:24,020
so how you use it sees more sparse will be a new solution

623
00:41:28,690 --> 00:41:34,310
that's what i see so as i will be seeing more sparse when the solution

624
00:41:34,310 --> 00:41:38,040
but it's not like fifty percent less it's like a few percent less

625
00:41:41,710 --> 00:41:46,980
the solution here

626
00:41:46,980 --> 00:41:50,460
i'm more sparse than the ones

627
00:41:50,500 --> 00:41:54,630
so obviously if you had to choose between two solution was the same performance i

628
00:41:54,630 --> 00:41:57,730
which is the one which is more sparse because they in practice it will go

629
00:41:57,730 --> 00:42:00,620
much faster to classify new documents

630
00:42:00,670 --> 00:42:05,100
it's also nice to have more sparse solution representing the concept

631
00:42:05,190 --> 00:42:16,380
so is higher see you get more sparse solutions

632
00:42:16,400 --> 00:42:17,770
thank you

633
00:42:17,790 --> 00:42:22,000
what is your question before because they couldn't get the

634
00:42:22,020 --> 00:42:25,520
correct use

635
00:42:49,730 --> 00:42:55,060
it is it's it's

636
00:42:55,080 --> 00:42:59,880
the last time

637
00:43:12,650 --> 00:43:16,350
so monitor the linear separability of the data

638
00:43:22,560 --> 00:43:26,790
i had to try but i still because that's what i do now

639
00:43:26,810 --> 00:43:29,100
and i'm not confident with it

640
00:43:29,230 --> 00:43:31,150
what i see here

641
00:43:31,190 --> 00:43:33,230
i was trying to explain it

642
00:43:33,270 --> 00:43:34,480
at this moment

643
00:43:34,500 --> 00:43:38,190
for me is the performance struck between

644
00:43:38,650 --> 00:43:41,480
more sparse solution here

645
00:43:41,580 --> 00:43:46,310
with high c a smaller one

646
00:43:46,350 --> 00:43:48,600
solution was this one see value

647
00:43:50,560 --> 00:43:52,310
it comes from there

648
00:43:52,310 --> 00:43:54,190
from outlier points

649
00:43:54,190 --> 00:43:56,650
which might be misclassified

650
00:43:56,670 --> 00:43:59,770
and to suppose is i i d

651
00:43:59,830 --> 00:44:05,420
we have a famous the classification task and twenty newsgroup which is about point that

652
00:44:06,500 --> 00:44:09,270
against took point religion

653
00:44:09,270 --> 00:44:11,620
so let's see you again

654
00:44:11,690 --> 00:44:13,140
let's look at this picture

655
00:44:13,160 --> 00:44:22,960
in this picture margin error would be appointed that some lies inside this region could

656
00:44:22,960 --> 00:44:25,620
be on the wrong side of it could be just a little bit in the

657
00:44:25,620 --> 00:44:29,600
margin until on the correct side so as soon as the distance to this

658
00:44:29,620 --> 00:44:33,160
line here is nonzero we call it the marginal

659
00:44:33,210 --> 00:44:35,520
one can actually show

660
00:44:35,540 --> 00:44:40,270
quite straightforwardly using the KKT conditions

661
00:44:41,120 --> 00:44:46,690
all margin errors will be support vectors all margin errors will have another five that's

662
00:44:46,690 --> 00:44:51,040
non-zero in fact they would have another five it's as large as possible

663
00:44:51,060 --> 00:44:57,190
so they the of five will increase until it is the football which

664
00:44:57,210 --> 00:44:59,350
again this sort of

665
00:45:00,960 --> 00:45:06,000
it makes intuitive sense from this kind of argument here

666
00:45:06,000 --> 00:45:13,100
so back to the previous slide

667
00:45:14,460 --> 00:45:15,730
margin errors

668
00:45:15,790 --> 00:45:20,850
having non-zero i support vectors having non-zero five but i told you all margin errors

669
00:45:20,850 --> 00:45:22,580
are support vectors

670
00:45:23,020 --> 00:45:27,810
however the country is not true not all support vector to be mounted to support

671
00:45:27,810 --> 00:45:31,230
that this could also lie exactly on the edge of the margin

672
00:45:31,230 --> 00:45:32,790
and then they would not be marginal

673
00:45:33,540 --> 00:45:37,310
and there's a lot so there is little difference between these two sets and those

674
00:45:37,310 --> 00:45:39,910
are the points that lie exactly on the margin

675
00:45:40,230 --> 00:45:42,890
now it turns out one can prove that

676
00:45:42,910 --> 00:45:44,310
thank you

677
00:45:45,480 --> 00:45:47,100
this is

678
00:45:47,120 --> 00:45:50,710
but what about the case so the question is what is the intuition about the

679
00:45:50,710 --> 00:45:51,960
mountain on

680
00:45:52,410 --> 00:45:53,980
so in a way

681
00:45:54,350 --> 00:46:00,080
let's go back to this picture

682
00:46:00,210 --> 00:46:03,140
so the margin is still

683
00:46:03,210 --> 00:46:07,940
this distance but the difference is not at some points by the market if you

684
00:46:07,940 --> 00:46:12,750
want to the marginal so you no longer trying to separate all

685
00:46:12,770 --> 00:46:17,270
points with maximum margin but you're saying i'm going to do some trade i will

686
00:46:17,580 --> 00:46:23,830
try to have a large margin but i will all viewpoints don't that are alive

687
00:46:23,940 --> 00:46:26,390
to fly inside this model even on the wrong side

688
00:46:26,440 --> 00:46:31,100
trade is in the in in the sea has been characterized by c

689
00:46:31,120 --> 00:46:35,960
which is some kind of creative constant that we don't really know how to interpret

690
00:46:35,960 --> 00:46:41,170
other than saying it's the upper bound on the outside the maximum influence we will

691
00:46:41,170 --> 00:46:43,140
give to one point

692
00:46:43,170 --> 00:46:46,980
or in the new SVM the trade is characterized by new

693
00:46:46,980 --> 00:46:48,560
in a way which

694
00:46:48,650 --> 00:46:50,560
they now

695
00:46:57,020 --> 00:46:58,500
here we go

696
00:46:58,540 --> 00:47:04,310
so in the new SVM one can prove that the solution of the optimisation problem

697
00:47:05,580 --> 00:47:07,870
satisfies the following proposition

698
00:47:07,890 --> 00:47:12,040
new is an upper bound on the fraction of points

699
00:47:12,060 --> 00:47:13,790
that all margin errors

700
00:47:13,810 --> 00:47:17,520
and it's a lower bound on the fraction of points are support vectors

701
00:47:17,580 --> 00:47:20,640
and remember the difference between these two sets

702
00:47:20,640 --> 00:47:23,960
support vector modern rules where exactly the points that lie

703
00:47:23,960 --> 00:47:27,580
at the edge of the margin so that exactly satisfy

704
00:47:27,600 --> 00:47:29,000
the constraints

705
00:47:29,480 --> 00:47:31,500
as equalities

706
00:47:31,500 --> 00:47:35,290
with value of ci equal to zero

707
00:47:35,310 --> 00:47:40,660
so that's the difference between these two sets and one can show that under certain

708
00:47:40,660 --> 00:47:42,310
benign conditions

709
00:47:42,330 --> 00:47:43,140
on the

710
00:47:43,660 --> 00:47:46,100
the kernel and on the

711
00:47:46,140 --> 00:47:48,980
a distribution generating the data

712
00:47:51,640 --> 00:47:58,750
these two inequalities will become equality so asymptotically the fraction of points lie exactly

713
00:47:58,960 --> 00:48:03,520
on the edge of the margin will be negligible

714
00:48:03,520 --> 00:48:04,930
the sea

715
00:48:05,720 --> 00:48:07,930
let's say i want to change

716
00:48:07,950 --> 00:48:11,260
so as a as a as a run as a write-in opposition out of on

717
00:48:11,270 --> 00:48:16,850
on have to respect the constraint that some of i alpha i y i

718
00:48:16,910 --> 00:48:18,600
but can

719
00:48:18,700 --> 00:48:24,660
so this is the only constraint and i didn't happen was talking about corners

720
00:48:27,770 --> 00:48:31,260
suppose they might change just alpha one

721
00:48:32,080 --> 00:48:34,450
then i know the alpha one

722
00:48:34,450 --> 00:48:38,640
must be equal to sum i was two to n

723
00:48:38,640 --> 00:48:40,220
alpha i

724
00:48:40,270 --> 00:48:44,260
why i divided by y one

725
00:48:44,910 --> 00:48:49,260
and so alpha one can actually be written exactly as a function of alpha two

726
00:48:49,260 --> 00:48:51,470
alpha three and so on graph

727
00:48:51,490 --> 00:48:54,700
and so if i hold alpha two alpha three of four

728
00:48:54,740 --> 00:48:59,620
throughout thanks to the country alpha one because of was the the final radius these

729
00:49:00,740 --> 00:49:04,640
whereas in contrast if i choose to change alpha one alpha two at the same

730
00:49:05,520 --> 00:49:09,740
then on i saw the constraints i know alpha one alpha two

731
00:49:09,770 --> 00:49:11,740
must must must satisfy

732
00:49:11,760 --> 00:49:14,580
yes that linear constraints

733
00:49:14,600 --> 00:49:18,740
but and this is where you can change alpha alpha one you also change alpha

734
00:49:18,760 --> 00:49:21,790
two according to make sure to satisfy the constraint

735
00:49:21,810 --> 00:49:23,240
the mix

736
00:49:27,910 --> 00:49:32,990
so zeta was defined on its own to find

737
00:49:33,140 --> 00:49:38,330
the back to support so on each iteration i have in some sense the parameters

738
00:49:38,330 --> 00:49:40,370
alpha one alpha two and and so on

739
00:49:40,410 --> 00:49:43,740
and i want to change alpha one alpha two say

740
00:49:43,790 --> 00:49:47,160
so from the previous iteration on you know

741
00:49:47,180 --> 00:49:51,640
let's say i have not violated constraints so that holds true and so on just

742
00:49:51,640 --> 00:49:56,020
defining zeta to be equal to this because after one y one was two y

743
00:49:56,020 --> 00:50:01,540
two must be equal to minus some from i was reading that some just defining

744
00:50:01,660 --> 00:50:05,290
so to find this to be easy to to

745
00:50:18,370 --> 00:50:20,490
so this is on

746
00:50:20,520 --> 00:50:25,180
so in every iteration you choose maybe a different pair of alpha to update on

747
00:50:25,370 --> 00:50:29,180
the way you do this is something i don't talk about the also it's a

748
00:50:29,180 --> 00:50:33,720
couple were more about that but the basic outline of the algorithm is on on

749
00:50:33,720 --> 00:50:37,790
every iteration of the algorithm you're going to select some alpha i alpha j to

750
00:50:37,790 --> 00:50:42,040
update public life from the sport graceful XML file alpha j to update

751
00:50:42,060 --> 00:50:46,790
by some heuristic and they use the procedure just described to

752
00:50:46,790 --> 00:50:49,760
actually update alpha i alpha j

753
00:50:50,160 --> 00:50:54,370
so this is what actually just about was the procedure

754
00:50:54,390 --> 00:50:58,930
to optimize w respect alpha phi alpha j i didn't actually talk about the heuristic

755
00:50:58,930 --> 00:51:03,560
for choosing alpha i alpha j

756
00:51:05,080 --> 00:51:07,620
well the option

757
00:51:07,620 --> 00:51:10,930
o and w is

758
00:51:10,950 --> 00:51:16,660
this where i to be done about functions previously about this

759
00:51:16,680 --> 00:51:21,540
you know this is all this is about solving the

760
00:51:21,540 --> 00:51:23,950
and that

761
00:51:24,000 --> 00:51:32,240
so this is about solving the optimisation problem for years and so this is the

762
00:51:32,240 --> 00:51:37,720
objective function we had the best that's the about

763
00:51:37,810 --> 00:51:39,680
this was which

764
00:51:44,950 --> 00:51:46,120
change you

765
00:51:46,140 --> 00:51:49,890
one of the one chain works

766
00:51:53,760 --> 00:51:56,970
this is what are

767
00:51:58,830 --> 00:52:02,930
let me the chancellor said differently so on

768
00:52:02,970 --> 00:52:06,080
what we're trying to do is we're trying to optimize the objective function to be

769
00:52:06,080 --> 00:52:07,240
available for

770
00:52:07,270 --> 00:52:11,490
so the metric approaches that we care about is whether w alpha is getting better

771
00:52:11,490 --> 00:52:12,830
at every iteration

772
00:52:13,950 --> 00:52:19,040
and so what is true according to sense and for is on every iteration that

773
00:52:19,180 --> 00:52:23,770
of alpha can only increase it may stay the same increase the content words are

774
00:52:23,990 --> 00:52:30,100
as true that eventually the office converges some values true that intervening iterations the alpha

775
00:52:30,220 --> 00:52:34,620
made one of the alphas moved further away and coast further closer to the final

776
00:52:34,620 --> 00:52:38,600
value on that there will be some will be more about what really care about

777
00:52:38,620 --> 00:52:43,120
is the w about is getting better every time which which which is true

778
00:52:45,560 --> 00:52:50,000
six couple common words on

779
00:52:50,020 --> 00:52:55,830
some of the phi wrap up on this one is that other diseases tourist she

780
00:52:55,970 --> 00:52:58,970
just run has original algorithm talked about the heuristic

781
00:52:58,990 --> 00:53:03,550
choosing which values for choosing which pair alpha i alpha j to obtain it was

782
00:53:03,550 --> 00:53:06,910
a very complicated here it is is on is one of the things that sort

783
00:53:06,910 --> 00:53:11,760
of not conceptually complicated is very complicated to explain your work so i will talk

784
00:53:11,760 --> 00:53:12,660
about that here

785
00:53:13,080 --> 00:53:17,140
if you want to learn about it you know can look out

786
00:53:17,160 --> 00:53:19,040
john that paper

787
00:53:19,100 --> 00:53:24,890
on the SMO algorithm is that the heuristic is pretty easy to read

788
00:53:24,910 --> 00:53:30,810
and later on served for the world also posting a handle on the problem on

789
00:53:30,810 --> 00:53:36,070
the course home page on with sort of simplified version of the service the in

790
00:53:36,230 --> 00:53:39,220
the news problems

791
00:53:39,230 --> 00:53:42,440
so you can see some of the courses meetings for more details

792
00:53:43,320 --> 00:53:47,330
one of thing that in talk about was on how to update the parameter be

793
00:53:47,330 --> 00:53:50,020
right so this solving all alphas

794
00:53:50,030 --> 00:53:54,690
this also the alphas is lost get w getting it until was hard to compute

795
00:53:54,690 --> 00:53:58,810
the parameter b and it turns out that again not actually not very difficult to

796
00:53:58,990 --> 00:54:02,530
all that you read about yourself with the with the with the notes that will

797
00:54:03,890 --> 00:54:06,910
that they will post along with the next problem

798
00:54:11,180 --> 00:54:14,640
the last two to interact with to this lecture we want to do is just

799
00:54:14,640 --> 00:54:23,270
tell you briefly about a couple of examples of of applications of so

800
00:54:23,280 --> 00:54:26,530
last time

801
00:54:26,530 --> 00:54:29,780
address piece you person addressed piece e eight by saying

802
00:54:30,390 --> 00:54:33,200
and this also works for this he everything on

803
00:54:34,330 --> 00:54:39,380
that's why so many periodicities in engineering are just applying piece e eight dataset on

804
00:54:40,000 --> 00:54:44,680
so the person addressed by saying okay if i'm progressing from one region to another

805
00:54:44,690 --> 00:54:46,730
time by regressing from your height to weight

806
00:54:47,550 --> 00:54:49,100
i get different regression line

807
00:54:50,980 --> 00:54:52,750
predicting your height given your weight

808
00:54:53,250 --> 00:54:55,260
and if i predict your weight given your knowledge

809
00:54:57,570 --> 00:54:58,950
he said that's clearly strange

810
00:55:01,080 --> 00:55:04,160
should be fit data if we want the regression line to be the same

811
00:55:05,500 --> 00:55:05,890
the answer

812
00:55:07,090 --> 00:55:07,540
peace yet

813
00:55:08,550 --> 00:55:10,630
you'll find the direction of maximum variance again

814
00:55:11,060 --> 00:55:15,890
so that's how personal address that and it was later discovered that the question you

815
00:55:15,890 --> 00:55:19,760
answered that question is the same thing as the of the hotelling's question by telling

816
00:55:19,910 --> 00:55:25,130
was looking appreciate in the sense of factor analysis model where space and was just

817
00:55:25,130 --> 00:55:28,930
answering a different question so i think very much if you should think about

818
00:55:29,480 --> 00:55:32,770
people missus multiple algorithms in one because is just about to show

819
00:55:33,730 --> 00:55:36,310
peace is also this distance matching algorithm

820
00:55:36,750 --> 00:55:42,850
which one like classical multidimensional scaling which underpins a low dimensionality reduction work in statistics

821
00:55:46,020 --> 00:55:46,250
we go

822
00:55:47,510 --> 00:55:52,630
so the proof that this is a nice little lagrange multiplier based proof where you say

823
00:55:53,070 --> 00:55:54,440
that's fine direction

824
00:55:54,840 --> 00:55:56,300
in the data center data

825
00:55:56,680 --> 00:55:58,680
rotation vector here this is the

826
00:55:59,730 --> 00:56:03,790
i is constrained to be length one so it's giving us the direction of data

827
00:56:04,070 --> 00:56:06,640
that gives as the largest variance so

828
00:56:07,270 --> 00:56:11,580
i one is that maximize variance why times are one

829
00:56:13,400 --> 00:56:16,890
now we can rewrite the variance in terms of the sample covariance and that's the

830
00:56:16,890 --> 00:56:19,830
typical trait down here which is if we look at the variance

831
00:56:20,920 --> 00:56:22,880
it's computed in this way the sum

832
00:56:23,280 --> 00:56:25,830
all the values in the new dataset transpose

833
00:56:27,010 --> 00:56:27,410
so this is

834
00:56:28,000 --> 00:56:32,140
so each value in the new dataset multiplied by itself which i will always write

835
00:56:32,220 --> 00:56:33,400
as a vector in the product

836
00:56:34,470 --> 00:56:39,190
i can say the vector product and i conflict this guy around because its transpose

837
00:56:39,510 --> 00:56:40,450
and write it in this way

838
00:56:41,120 --> 00:56:42,870
the rotation times one over the

839
00:56:43,810 --> 00:56:49,250
in the why transpose why perhaps which is basically the sample covariance times the rotation

840
00:56:49,250 --> 00:56:52,650
on the other side so this variance that we're trying to maximize can be written

841
00:56:54,600 --> 00:56:56,110
sample covariance vector

842
00:56:56,550 --> 00:56:59,170
so we want to maximize the subject to the constraint

843
00:57:00,410 --> 00:57:02,280
and that's the grande multiplier problem

844
00:57:02,780 --> 00:57:05,840
and again we just by gradient methods so if we take the

845
00:57:06,560 --> 00:57:10,860
i mean matrix-vector derivatives for me is the mainstay of machine learning so i'm assuming

846
00:57:11,110 --> 00:57:13,980
you know that if you don't know that it doesn't matter but you know you

847
00:57:13,980 --> 00:57:16,340
really need get on top for them to do all this linear algebra stuff

848
00:57:16,810 --> 00:57:17,780
so be it

849
00:57:18,910 --> 00:57:21,900
the grungy and has this form this is the ground multiply

850
00:57:22,770 --> 00:57:23,820
dealing with the constraint

851
00:57:25,310 --> 00:57:30,820
taking the derivative we get to sample covariance times are one minus two lambda one times are one

852
00:57:31,200 --> 00:57:35,630
and we can rearrange this to in this format has the form and i can value problem

853
00:57:36,500 --> 00:57:40,050
that's the first principal component the direction the largest variance

854
00:57:41,970 --> 00:57:44,420
we've sort shown that actual variance is

855
00:57:45,000 --> 00:57:48,010
and i can value problem with the result is the solution to like value problem

856
00:57:48,010 --> 00:57:49,920
we don't know which i can value it is

857
00:57:50,540 --> 00:57:55,320
but multiply both sides this by aaron transpose premultiplying it

858
00:57:56,070 --> 00:57:56,630
tells us

859
00:57:57,520 --> 00:58:02,760
the lambda one is equal to aaron transpose this so what so lambda one turns

860
00:58:02,760 --> 00:58:06,480
out to be the variance we extract that's why withdrawing earlier

861
00:58:08,450 --> 00:58:10,940
without explaining why drawn in which is always good

862
00:58:12,080 --> 00:58:14,540
in these things it the variance the direction

863
00:58:15,170 --> 00:58:18,810
the largest variance turned out to be the one that's all one in this notation

864
00:58:19,340 --> 00:58:23,870
and the variance we extract is given by lambda one di died value there

865
00:58:24,550 --> 00:58:25,650
yeah so that's what

866
00:58:26,810 --> 00:58:27,810
this proof is showing

867
00:58:30,540 --> 00:58:32,090
and that's the first principal component

868
00:58:33,850 --> 00:58:37,140
the maximum variance is therefore necessarily the maximum igon value

869
00:58:37,700 --> 00:58:38,380
so the principle

870
00:58:38,990 --> 00:58:42,010
i can vector i tend to think of that's the first principal component

871
00:58:42,450 --> 00:58:46,770
to the next principal component you need to impose the next direction should be orthogonal

872
00:58:47,370 --> 00:58:51,340
so that you get the rotation right otherwise it's not a rotation matrix are extracting

873
00:58:52,220 --> 00:58:55,250
and you apply the same thing again with this look around know one of those

874
00:58:55,250 --> 00:58:59,750
magical mass things happens that you even though you've got this extra term here

875
00:59:00,440 --> 00:59:00,940
if you know

876
00:59:01,330 --> 00:59:03,270
premultiply this whole thing by

877
00:59:03,890 --> 00:59:05,940
i transpose again

878
00:59:08,590 --> 00:59:10,110
that's same trick we did find out the

879
00:59:10,730 --> 00:59:19,080
variance what premultiplying at shows is the i transpose times archaic because they're different indexes is zero

880
00:59:20,750 --> 00:59:22,780
um but also on my

881
00:59:23,400 --> 00:59:27,170
transpose esskay archaic is also zero

882
00:59:27,650 --> 00:59:28,640
and that's because

883
00:59:30,560 --> 00:59:34,870
this guy is a previous i can vector we've extracted and substituting in

884
00:59:36,030 --> 00:59:38,930
go through this too quickly i'm aware about substituting in this guy

885
00:59:39,330 --> 00:59:41,870
shows that if you take place this is like

886
00:59:42,510 --> 00:59:44,580
if people are two times this yeah

887
00:59:46,030 --> 00:59:47,150
there are two times less

888
00:59:48,580 --> 00:59:52,980
must be equal to because we've already said i want satisfies this must be equal

889
00:59:52,980 --> 00:59:55,150
to lambda one hour to transpose all

890
00:59:56,300 --> 00:59:59,010
which this constraint is forcing to be zero

891
00:59:59,770 --> 01:00:01,240
so basically premultiplying by

892
01:00:04,570 --> 01:00:06,490
eliminates and eliminates that's

893
01:00:06,830 --> 01:00:08,050
and then just extracts

894
01:00:08,450 --> 01:00:12,500
the element this which is equal to all right so gamma i times one is

895
01:00:12,500 --> 01:00:15,750
equal to zero which implies that gamma i zero so

896
01:00:17,280 --> 01:00:22,790
la grande multiplier constraint that just disappears if satisfied even without using it that's what it means

897
01:00:24,730 --> 01:00:25,980
hands once step

898
01:00:26,360 --> 01:00:27,250
things disappeared

899
01:00:28,780 --> 01:00:30,530
it is rewriting the same thing yeah

900
01:00:31,110 --> 01:00:35,330
you get for the k'th component is also a solution if the eigen value problem

901
01:00:35,540 --> 01:00:40,660
so the principal components are just be i can vectors of the covariance matrix but

902
01:00:40,660 --> 01:00:45,560
in this case we look for them by looking for directions of maximum variance which

903
01:00:45,680 --> 01:00:48,060
indeed also minimizes the subjective

904
01:00:48,680 --> 01:00:51,790
all looking at these distances in the latent space

905
01:00:53,570 --> 01:00:55,910
so we can extract as many principal components that we like

906
01:00:56,780 --> 01:00:59,850
but this is more the traditional way of looking at it rather than me

907
01:01:00,780 --> 01:01:02,990
probabilistic piece you anyway i did beginning

908
01:01:04,150 --> 01:01:07,200
a little bit about trigger because what i said is that we were gonna be

909
01:01:07,200 --> 01:01:09,910
talking about non linear dimensionality reduction

910
01:01:11,350 --> 01:01:12,950
and then i've solve the problem

911
01:01:13,330 --> 01:01:14,920
by just applying a rotation

912
01:01:15,410 --> 01:01:19,040
now the last time i checked is a rotation isn't a non linear

913
01:01:19,620 --> 01:01:21,170
dimensionality reduction

914
01:01:22,300 --> 01:01:23,610
what i've been proving here

915
01:01:26,440 --> 01:01:30,170
that if you restrict yourself to rotations and scalings

916
01:01:32,230 --> 01:01:35,030
then piece e eight is the minimizer of this

917
01:01:38,500 --> 01:01:40,150
the problem with putting animations in your flight

918
01:01:40,800 --> 01:01:42,090
this subjective here

919
01:01:44,180 --> 01:01:48,370
if you restrict yourself to rotation and scaling that's a linear operation

920
01:01:50,600 --> 01:01:52,550
that's the minimiser this objective

921
01:01:53,160 --> 01:01:55,220
so that's actually known as classical

922
01:01:55,990 --> 01:02:01,130
multidimensional scaling but there's one other odd aspect to it that i skipped over okay

923
01:02:01,130 --> 01:02:03,930
so the results of applying the digits data festival

924
01:02:06,020 --> 01:02:06,530
this is to

925
01:02:07,020 --> 01:02:09,000
dimensions so this is the what you saw

926
01:02:09,530 --> 01:02:10,040
the beginning

927
01:02:10,040 --> 01:02:13,560
and then of course the current has reached the maximum value

928
01:02:13,570 --> 01:02:16,120
which you can find ways on the floor

929
01:02:16,200 --> 01:02:20,550
because the self inductance itself has no reason to think of the self inductance has

930
01:02:20,550 --> 01:02:22,920
made of superconducting materials

931
01:02:22,940 --> 01:02:25,000
no resistance

932
01:02:25,010 --> 01:02:27,870
and so without knowing much about physics

933
01:02:27,920 --> 01:02:30,870
you can make a lot

934
01:02:32,060 --> 01:02:33,960
the current

935
01:02:34,010 --> 01:02:37,370
that is going to flow as a function of time

936
01:02:37,370 --> 01:02:39,940
you start out with zero

937
01:02:39,990 --> 01:02:42,060
and then ultimately

938
01:02:42,080 --> 01:02:45,750
if you wait long enough you reach maximum current

939
01:02:45,760 --> 01:02:46,910
which is given by

940
01:02:46,930 --> 01:02:49,300
law which is simply

941
01:02:49,310 --> 01:02:50,860
divided by r

942
01:02:51,130 --> 01:02:53,730
you slowly

943
01:02:53,770 --> 01:02:56,380
approach that value

944
01:02:56,450 --> 01:02:59,980
and how slowly depends on the value of the self inductance

945
01:03:00,020 --> 01:03:03,270
if the self inductances very high

946
01:03:03,320 --> 01:03:06,010
by climate like this is a high

947
01:03:06,020 --> 01:03:07,480
value for l

948
01:03:07,580 --> 01:03:09,330
the self inductances very

949
01:03:10,730 --> 01:03:12,770
it is a low value for l

950
01:03:12,800 --> 01:03:16,100
if the self inductance was zero

951
01:03:16,140 --> 01:03:18,750
come up instantaneously

952
01:03:18,800 --> 01:03:20,480
but i just convinced you

953
01:03:20,560 --> 01:03:24,240
there is no such thing as zero self and there's always something finite no matter

954
01:03:24,240 --> 01:03:25,700
how small

955
01:03:25,750 --> 01:03:28,000
so this is qualitatively

956
01:03:28,060 --> 01:03:30,170
what you would expect if you

957
01:03:30,180 --> 01:03:31,890
use your stomach

958
01:03:31,940 --> 01:03:35,490
and if you don't use your brains yet there's nothing wrong with using your stomach

959
01:03:37,550 --> 01:03:39,180
now i want to do

960
01:03:39,180 --> 01:03:41,320
this in a more civilized way

961
01:03:41,350 --> 01:03:42,950
and i want to use

962
01:03:43,040 --> 01:03:45,480
my brains

963
01:03:45,560 --> 01:03:48,940
and when i use my brains i have to set up an equation

964
01:03:48,990 --> 01:03:50,920
for the search

965
01:03:50,930 --> 01:03:53,380
and if you read your book

966
01:03:53,410 --> 01:03:54,690
you will find that

967
01:03:54,730 --> 01:03:56,180
mister john coding

968
01:03:56,190 --> 01:03:59,200
tells you to use case cells lutrol

969
01:03:59,320 --> 01:04:03,530
mister john cole he doesn't understand for this law and he's not the only one

970
01:04:03,560 --> 01:04:07,590
almost every college book that you read on physics

971
01:04:07,600 --> 01:04:08,980
i do this wrong

972
01:04:09,030 --> 01:04:13,550
they advise you to use it the rule which says that the closed loop integral

973
01:04:13,550 --> 01:04:17,040
around the circuit is zero that of course is other nonsense

974
01:04:17,080 --> 01:04:18,470
how can it be zero

975
01:04:18,530 --> 01:04:20,780
because it is changing magnetic flux

976
01:04:20,790 --> 01:04:24,860
and so it can only be minus the phi e

977
01:04:24,920 --> 01:04:25,990
i advise you

978
01:04:26,010 --> 01:04:28,090
go to the aid of two websites

979
01:04:28,140 --> 01:04:29,520
and download

980
01:04:29,560 --> 01:04:31,320
lectures supplement that

981
01:04:31,380 --> 01:04:33,230
you will find in which i

982
01:04:33,260 --> 01:04:34,430
address this issue

983
01:04:34,470 --> 01:04:37,060
and it's very hard

984
01:04:37,070 --> 01:04:42,730
so the closed loop in the role of the doctor l

985
01:04:42,840 --> 01:04:45,420
if you go around the circuit

986
01:04:45,460 --> 01:04:48,060
is not zero

987
01:04:48,100 --> 01:04:50,460
is minus the phi beta

988
01:04:50,520 --> 01:04:52,320
five days long

989
01:04:52,340 --> 01:04:54,560
so is mine is l

990
01:04:56,510 --> 01:05:03,210
so we have to go around the circuit and we have to apply for this

991
01:05:03,950 --> 01:05:04,860
and knots

992
01:05:04,890 --> 01:05:05,950
he of

993
01:05:06,010 --> 01:05:07,440
literally thousands

994
01:05:07,450 --> 01:05:10,560
applied here

995
01:05:10,560 --> 01:05:14,990
this is the site of the battle and the minus side so electric field in

996
01:05:14,990 --> 01:05:17,350
the batteries in this direction

997
01:05:17,480 --> 01:05:20,230
electric field in the self inductances zero

998
01:05:20,260 --> 01:05:25,580
because the self inductance has no resistance superconducting material

999
01:05:25,720 --> 01:05:27,480
so the electric field in

1000
01:05:29,570 --> 01:05:32,480
if the current is in this direction which will be

1001
01:05:33,970 --> 01:05:36,960
electric field in the resistor

1002
01:05:37,080 --> 01:05:39,760
will be in this direction

1003
01:05:39,770 --> 01:05:41,950
now i am equipped

1004
01:05:42,050 --> 01:05:44,660
right down the closed loop integral

1005
01:05:44,670 --> 01:05:47,430
of the adult the l

1006
01:05:47,480 --> 01:05:49,950
i start here

1007
01:05:50,060 --> 01:05:53,010
and i always go in the direction of the current

1008
01:05:53,030 --> 01:05:55,270
and i advise you to do the same

1009
01:05:55,280 --> 01:05:57,400
i don't care that you guess

1010
01:05:57,440 --> 01:06:00,320
the wrong direction for the current that's fine

1011
01:06:00,330 --> 01:06:03,020
later minus signs will

1012
01:06:03,030 --> 01:06:03,970
correct that

1013
01:06:03,980 --> 01:06:06,790
they will tell you that you really guess the wrong direction

1014
01:06:06,830 --> 01:06:08,060
but always

1015
01:06:08,060 --> 01:06:10,810
go around to look in the direction of the current

1016
01:06:10,820 --> 01:06:12,990
because then the EMF

1017
01:06:13,060 --> 01:06:15,110
it's always minus LVI

1018
01:06:15,130 --> 01:06:17,960
if you go in directions of post to the current

1019
01:06:18,020 --> 01:06:19,890
this plus the identity

1020
01:06:19,930 --> 01:06:22,060
that could become confusing

1021
01:06:22,080 --> 01:06:25,430
so i always go in the direction of the current

1022
01:06:25,440 --> 01:06:26,390
and so

1023
01:06:26,400 --> 01:06:27,480
i first

1024
01:06:27,490 --> 01:06:29,560
for the self inductance

1025
01:06:29,640 --> 01:06:33,780
there is no electric field in the self inductance so don't only going from here

1026
01:06:33,790 --> 01:06:37,950
to here is zero this is where the books are wrong

1027
01:06:37,970 --> 01:06:39,810
it is zero

1028
01:06:40,460 --> 01:06:43,930
i go through the resistor so now i get plus

1029
01:06:43,940 --> 01:06:46,170
i r

1030
01:06:46,560 --> 01:06:48,760
and the other in the same direction

1031
01:06:48,770 --> 01:06:50,920
this law tells me is i or

1032
01:06:50,930 --> 01:06:52,150
in the battery

1033
01:06:52,160 --> 01:06:56,610
i go against the electric field so i get minus

1034
01:06:57,820 --> 01:07:00,410
that now equals minus l

1035
01:07:00,440 --> 01:07:02,470
the idea

1036
01:07:02,480 --> 01:07:03,660
this is

1037
01:07:03,670 --> 01:07:05,390
the only sane

1038
01:07:05,430 --> 01:07:07,410
and the only correct way

1039
01:07:08,320 --> 01:07:09,760
five these laws

1040
01:07:09,770 --> 01:07:10,650
in this

1041
01:07:13,420 --> 01:07:15,830
you can write it a little differently

1042
01:07:15,850 --> 01:07:18,490
which may give you some insight

1043
01:07:18,510 --> 01:07:21,810
for instance you could write

1044
01:07:24,600 --> 01:07:26,510
i can bring v in l

1045
01:07:26,520 --> 01:07:29,830
and l to one side so i can write down the

1046
01:07:29,920 --> 01:07:33,300
miners LVI pp

1047
01:07:33,310 --> 01:07:36,150
because i r

1048
01:07:36,200 --> 01:07:39,310
this the same equation when you look

1049
01:07:39,350 --> 01:07:42,560
and the nice thing about writing it this way

1050
01:07:42,650 --> 01:07:45,930
is that since the ITT is positive here

1051
01:07:45,940 --> 01:07:49,240
it's growing in time

1052
01:07:49,260 --> 01:07:52,630
the induced EMF which is this value

1053
01:07:52,680 --> 01:07:54,760
notice it's always opposing

1054
01:07:54,830 --> 01:07:59,700
the voltage of my battery and that's with manslaughter is all about

1055
01:07:59,710 --> 01:08:03,080
it's not until the ITT has become zero

1056
01:08:03,130 --> 01:08:07,660
that vehicles i are and that happens of course if you wait long enough

1057
01:08:07,700 --> 01:08:11,100
so we have to solve the differential equation

1058
01:08:11,190 --> 01:08:12,880
and one is often done

1059
01:08:12,880 --> 01:08:16,450
that to tom minka he invented EP and he can talk a lot about situations

1060
01:08:16,450 --> 01:08:21,840
in which but i know he's going to discuss the that that issue in great

1061
01:08:21,840 --> 01:08:26,260
length as well because it's in some situations yes multimodality people not work well in

1062
01:08:26,260 --> 01:08:31,130
use variational message passing instead to the whole suite of these possible algorithms you need

1063
01:08:31,130 --> 01:08:33,930
to choose the right one so leave tom to talk about that because that's really

1064
01:08:34,030 --> 01:08:36,740
is this lecture

1065
01:08:36,760 --> 01:08:38,280
no questions

1066
01:08:38,300 --> 01:08:42,910
all right so i the question what about still evolving through time so the other

1067
01:08:42,910 --> 01:08:47,360
thing that can happen so what described so far is we assume that everybody has

1068
01:08:47,360 --> 01:08:50,780
a fixed but unknown scale what happens of course is that you play lots of

1069
01:08:50,780 --> 01:08:56,450
chess your skill improves so again we can capture that by just drawing my graph

1070
01:08:56,490 --> 01:08:57,950
so here's a graph

1071
01:08:57,990 --> 01:09:02,340
which says players one and two have met and they had again and got a

1072
01:09:02,340 --> 01:09:06,630
game outcome and later they're going to meet and have another game meantime on this

1073
01:09:06,630 --> 01:09:11,510
scale may have evolved has a very simple model just as the skill is just

1074
01:09:11,510 --> 01:09:13,610
undergoing a sort of gas diffusion

1075
01:09:13,650 --> 01:09:18,470
so says the scale the next time is gas distribution centered on the scale of

1076
01:09:18,470 --> 01:09:20,930
the previous time

1077
01:09:21,970 --> 01:09:23,320
and then off go

1078
01:09:32,590 --> 01:09:38,780
sure well if you want to skip that distribution then by all means you go

1079
01:09:38,780 --> 01:09:42,430
ahead and put in a skewed distribution that's that's great

1080
01:09:42,470 --> 01:09:43,720
if you believe

1081
01:09:43,740 --> 01:09:46,950
if you believe skills should increase steadily with time

1082
01:09:46,990 --> 01:09:48,700
i don't know my skills are definitely

1083
01:09:48,740 --> 01:09:51,010
the decreased over the years

1084
01:09:51,680 --> 01:09:54,010
at age age should be in area

1085
01:09:57,760 --> 01:10:01,220
OK and

1086
01:10:01,240 --> 01:10:05,010
you don't want to mention that this is that this isn't just an academic study

1087
01:10:05,380 --> 01:10:09,740
but it's also been used on on line

1088
01:10:09,760 --> 01:10:15,470
four well since since two thousand five which processing these figures are quite out of

1089
01:10:15,470 --> 01:10:20,590
date actually so really processing millions of game outcomes but they worldwide twenty four hours

1090
01:10:20,590 --> 01:10:24,110
a day seven days a week so this is represents an enormous application

1091
01:10:24,490 --> 01:10:28,090
of bayesian methods are very large scale application of bayesian methods which i don't think

1092
01:10:28,090 --> 01:10:32,550
we could have contemplated doing you know ten years ago we didn't have this machinery

1093
01:10:32,550 --> 01:10:36,760
available to do this fast large-scale inference is one of the reasons why i think

1094
01:10:36,760 --> 01:10:40,570
this is a very exciting time for machine learning because as the you know the

1095
01:10:40,570 --> 01:10:44,550
quantity of data in the world is doubling every nine months is it or something

1096
01:10:44,550 --> 01:10:48,320
so we live in a world with its increasingly data rich and now we have

1097
01:10:48,320 --> 01:10:53,300
tools to do inference on very large data sets are exciting juncture

1098
01:10:53,300 --> 01:10:58,340
and actually i mentioned also mention just to prove this was sort of one-off flash-in-the-pan

1099
01:10:58,340 --> 01:11:05,130
there's no another application again is exactly the same kind of tools this time with

1100
01:11:05,150 --> 01:11:11,670
sixty advertisements in in search you're not google well actually there are many realizes there

1101
01:11:11,670 --> 01:11:13,880
are other search engines are not quite as popular

1102
01:11:13,900 --> 01:11:19,470
so the company based in seattle has one of them and just like google is

1103
01:11:19,470 --> 01:11:23,220
trying to model paying it with adverts so the problem is to pick pick the

1104
01:11:23,220 --> 01:11:28,320
right and that's you don't about these with adverts but irrelevant because that just knowing

1105
01:11:28,590 --> 01:11:31,340
you want to show you see on this is you can the ones which the

1106
01:11:31,340 --> 01:11:36,170
user actually clicks on so there is a very important inference problem to work out

1107
01:11:36,170 --> 01:11:37,800
which are to show

1108
01:11:37,820 --> 01:11:41,120
we showed that you should be shown to the user and again that you know

1109
01:11:41,120 --> 01:11:48,220
a real time online planet scale applications involving huge datasets and again also went

1110
01:11:48,260 --> 01:11:52,130
like a few months ago and now all of the

1111
01:11:52,180 --> 01:11:54,900
all of the advertisements served up by

1112
01:11:54,910 --> 01:12:00,110
by this particular company on search engine or all

1113
01:12:00,130 --> 01:12:02,340
chosen using this method

1114
01:12:04,950 --> 01:12:11,050
performance improvement over the previous method was was enormous that's been hugely beneficial

1115
01:12:11,300 --> 01:12:13,820
very practical sense

1116
01:12:13,820 --> 01:12:18,930
o thing ought to mention art schools of thought about this this wonderful framework of

1117
01:12:18,930 --> 01:12:24,680
graphical models and fast efficient local message passing algorithms so that you mentioned the work

1118
01:12:24,680 --> 01:12:28,760
of john winn this was his phd work and this was a system called five

1119
01:12:28,800 --> 01:12:35,880
so this is an academic study rather than a fully functional toolkit what he did

1120
01:12:35,880 --> 01:12:37,340
was to say well

1121
01:12:37,360 --> 01:12:41,860
let's really embrace this graphical framework and let's build an inference engine which start by

1122
01:12:41,860 --> 01:12:43,590
allowing you to draw a graph

1123
01:12:43,610 --> 01:12:46,220
so what you get is a sort of drawing package

1124
01:12:46,240 --> 01:12:50,970
and by pointing and clicking you can construct the graph and this is some

1125
01:12:51,010 --> 01:12:57,110
some kind of mixture of factor analyzers some bayesian mixture of factor analyzers each of

1126
01:12:57,110 --> 01:13:01,840
these nodes are very well these things can see these plates here this will be

1127
01:13:01,840 --> 01:13:05,690
are assumes this noise is zero what is not a hundred percent clear i think

1128
01:13:05,690 --> 01:13:11,000
what he's assuming about the noise and this proved about eighty people assuming noise machine

1129
01:13:11,000 --> 01:13:14,960
coming out machine learning is perfect but if you assume

1130
01:13:15,080 --> 01:13:18,330
your your data is linearly related to the

1131
01:13:18,360 --> 01:13:23,700
unobserved factors plus the mean plus some noise as we saw regression

1132
01:13:23,710 --> 01:13:26,260
so this multioutput regression in fact

1133
01:13:26,270 --> 01:13:29,740
and we've got similar noise thing again the noise is very

1134
01:13:29,780 --> 01:13:32,440
so this is the thing that's going on we've got some

1135
01:13:32,480 --> 01:13:37,000
two dimensional factor space this function mapping to what we observe this three-dimensional could be

1136
01:13:37,000 --> 01:13:38,310
much higher dimensional

1137
01:13:39,020 --> 01:13:44,570
what we can show is that we can write down a likelihood again as we

1138
01:13:44,570 --> 01:13:45,860
could before

1139
01:13:45,900 --> 01:13:48,920
and then i can write down the full likelihood of the data just by making

1140
01:13:48,920 --> 01:13:50,780
independence assumptions

1141
01:13:50,880 --> 01:13:56,500
and then using a prior over x this time instead of w

1142
01:13:56,520 --> 01:13:58,280
i can compute

1143
01:13:58,330 --> 01:13:59,760
the joint distribution

1144
01:13:59,770 --> 01:14:04,880
for why the marginal distribution for y and it has its gaussians with the meaning

1145
01:14:04,880 --> 01:14:06,190
given by mu

1146
01:14:06,200 --> 01:14:10,610
and this particular form of covariance this is why i said that

1147
01:14:10,620 --> 01:14:14,760
like factors they will a w factors and if you do the other way round

1148
01:14:14,760 --> 01:14:18,440
actions appear as factors in the actual

1149
01:14:18,480 --> 01:14:25,250
mathematical sense so ashamed he decided to call the components so it's a covariance

1150
01:14:25,300 --> 01:14:29,940
if the calcium the mean and the covariance this covariance has this particular fall which

1151
01:14:29,940 --> 01:14:35,770
is low rainfall and it captures the thing is the dimensionality reduction and basically chris

1152
01:14:35,770 --> 01:14:37,960
bishop and mike tipping proved

1153
01:14:37,990 --> 01:14:40,620
you could solve this problem

1154
01:14:40,630 --> 01:14:45,080
by looking at the eigen vectors of the sample covariance matrix so that i was

1155
01:14:45,080 --> 01:14:48,130
talking about the next two things together but

1156
01:14:48,140 --> 01:14:51,520
tipping and bishop is really close to the original paper apart from the addition of

1157
01:14:51,520 --> 01:14:56,390
the noise but you can show that things are solving this w by computing the

1158
01:14:56,390 --> 01:14:59,760
eigen values and i can vectors of the covariance matrix which is just what we

1159
01:14:59,760 --> 01:15:00,930
know as PCA

1160
01:15:00,940 --> 01:15:06,260
so a probabilistic interpretation has a slight advantage

1161
01:15:06,310 --> 01:15:12,500
of dealing with missing data so is famous artificial the pseudo real generated type dataset

1162
01:15:12,500 --> 01:15:13,950
called the oil data

1163
01:15:13,960 --> 01:15:17,070
and the three different regimes of flow in this data then you have the sense

1164
01:15:17,260 --> 01:15:22,650
i think there's twelve them in total which are measuring density across an oil pipeline

1165
01:15:23,320 --> 01:15:26,760
was it either flow flows all mixed together or in close in this layer way

1166
01:15:26,760 --> 01:15:31,390
or this annual away soil water and the gas and the sensors measuring density and

1167
01:15:31,390 --> 01:15:35,440
all you know about the senses right so you can in principle component analysis on

1168
01:15:35,440 --> 01:15:39,810
the original data and that's what you see these different flow regimes so this is

1169
01:15:39,810 --> 01:15:43,690
a different homogeneous flow regimes in one of these islam and many others and you

1170
01:15:43,700 --> 01:15:47,020
know which are much more similar to each other what's interesting if you've done the

1171
01:15:47,140 --> 01:15:51,070
probabilistic approaches where you could decide doing is throwaway data you can get rid of

1172
01:15:52,330 --> 01:15:53,920
use missing data

1173
01:15:53,930 --> 01:15:58,140
and you can do the same principal component analysis so what you actually get a

1174
01:15:58,140 --> 01:16:02,430
similar looking things as you throw away more data they start looking less similar

1175
01:16:02,440 --> 01:16:05,010
so that's the only thirty percent of the data

1176
01:16:05,070 --> 01:16:09,450
and that fifty percent of the data being thrown away it's probabilistic approach helps you

1177
01:16:09,460 --> 01:16:11,370
to do that and you can think of that

1178
01:16:11,380 --> 01:16:15,120
during that in the way of the EM algorithms that's one advantage of a probabilistic

1179
01:16:15,120 --> 01:16:19,760
interpretation of PCA which you don't have any original page

1180
01:16:19,760 --> 01:16:23,780
OK so that was just a slight sense the factor analysis is pretty much the

1181
01:16:23,780 --> 01:16:26,480
same model but with the diagonal boy

1182
01:16:26,500 --> 01:16:30,260
different noise for each output

1183
01:16:30,300 --> 01:16:34,040
so you can think of these these these methods is a viable method is very

1184
01:16:34,040 --> 01:16:35,380
closely related

1185
01:16:35,390 --> 01:16:39,800
so following the example of trying to leave time for that

1186
01:16:39,900 --> 01:16:42,060
questions and discussion

1187
01:16:42,120 --> 01:16:45,310
what i talked about is probabilistic interpretation

1188
01:16:47,540 --> 01:16:51,720
the learning has these are functions as negative log likelihoods

1189
01:16:51,760 --> 01:16:52,960
contrasting to learn

1190
01:16:52,970 --> 01:16:56,030
loss functions the expected losses you're trying to deal with

1191
01:16:56,490 --> 01:16:58,760
what then was talking about

1192
01:16:58,790 --> 01:17:03,810
so the bayes the controversy in the bayesian approach is treating parameters of random variables

1193
01:17:03,920 --> 01:17:06,490
is nothing controversial about using bayes rule

1194
01:17:06,500 --> 01:17:12,540
learning proceeds through this combination of prior and likelihood and then you can even do

1195
01:17:12,540 --> 01:17:16,830
it sequentially one day prior to posterior and the use your posterior for the prior

1196
01:17:16,830 --> 01:17:19,380
for the next eight point

1197
01:17:19,540 --> 01:17:23,160
the latent variables makes gaussians r

1198
01:17:23,170 --> 01:17:28,150
not bayesian but they're using bayes rule so that probabilistic methods in learning as well but that

1199
01:17:28,430 --> 01:17:30,150
they're not bayesian because

1200
01:17:30,160 --> 01:17:35,830
you're only placing prior distributions over things that everyone except to stochastic variable

1201
01:17:35,830 --> 01:17:37,460
these problems

1202
01:17:37,480 --> 01:17:40,650
as an optimisation problem

1203
01:17:40,660 --> 01:17:42,420
and then how could we

1204
01:17:42,430 --> 01:17:44,950
could we could and then

1205
01:17:46,320 --> 01:17:49,630
build system according to its principles

1206
01:17:49,650 --> 01:17:50,940
so i'm going do

1207
01:17:50,970 --> 01:17:52,180
is going through

1208
01:17:52,190 --> 01:17:53,600
a sort of the torah

1209
01:17:53,600 --> 01:17:55,020
and commentary on this

1210
01:17:55,040 --> 01:18:00,320
of how we're going about posing to these problems from a bayesian perspective

1211
01:18:00,340 --> 01:18:02,440
as an optimisation problem

1212
01:18:02,460 --> 01:18:06,910
and some of us now is in the last five years has become

1213
01:18:06,920 --> 01:18:08,810
more generally standards

1214
01:18:08,820 --> 01:18:10,300
in the computer vision

1215
01:18:10,310 --> 01:18:13,870
literature not not all computer vision people

1216
01:18:13,890 --> 01:18:18,050
actually to this day use a bayesian perspective

1217
01:18:18,070 --> 01:18:24,800
know people also will accept some of the assumptions were making here

1218
01:18:24,800 --> 01:18:29,220
for example one of the assumptions we make in this first part of our system

1219
01:18:29,240 --> 01:18:32,220
is that image segmentation

1220
01:18:32,240 --> 01:18:35,710
or shall we say in general the clustering problem

1221
01:18:35,730 --> 01:18:39,980
is your place that's not rocket science to say that i mean we all know

1222
01:18:40,950 --> 01:18:46,390
but what was saying in particular is terms like minimizing some energy function may have

1223
01:18:46,390 --> 01:18:48,730
no relevance to perception

1224
01:18:48,740 --> 01:18:53,350
i have no relevance to me my metadata to minimize some function who says that

1225
01:18:53,350 --> 01:18:57,220
functions way representative of what we do

1226
01:18:57,270 --> 01:19:01,540
we saw yesterday morning of course there is some in places where there is quite

1227
01:19:01,570 --> 01:19:07,930
nice intersection between some optimisation criteria like maximizing entropy

1228
01:19:07,940 --> 01:19:10,730
and the responses of cells that was interesting

1229
01:19:10,750 --> 01:19:13,420
but that's not necessarily always the case

1230
01:19:13,680 --> 01:19:17,210
minimizing a function of some forms that you think of

1231
01:19:17,250 --> 01:19:21,050
is is necessary right so we take a supervised learning approach

1232
01:19:21,050 --> 01:19:25,940
one of the main one actually fit how humans perform doing this task

1233
01:19:25,970 --> 01:19:29,070
this is what you

1234
01:19:29,070 --> 01:19:30,160
in other words

1235
01:19:30,190 --> 01:19:34,460
contrary to many people we say in segmentation is not done

1236
01:19:34,470 --> 01:19:36,230
without supervision

1237
01:19:36,240 --> 01:19:37,930
that is to ask the process

1238
01:19:37,940 --> 01:19:43,570
even the visual process in humans of being able to encode the same as the

1239
01:19:43,600 --> 01:19:48,420
single consisting of parts and relationship doesn't stop

1240
01:19:48,440 --> 01:19:54,120
after segmentation after feature extraction the knowledge plays the role even in the very early

1241
01:19:55,190 --> 01:19:58,020
in fact we neurophysiology indicates that

1242
01:19:58,320 --> 01:20:00,520
that's story in a way

1243
01:20:01,920 --> 01:20:06,810
neurophysiology the visual cortex not like number conversation

1244
01:20:07,650 --> 01:20:12,460
what we do is we pose the problem of image segmentation

1245
01:20:12,500 --> 01:20:15,460
and annotation as a single proper

1246
01:20:15,480 --> 01:20:20,390
and what's the single problem problem the problem is i want to be able to

1247
01:20:20,440 --> 01:20:21,830
two label

1248
01:20:21,830 --> 01:20:23,170
a pixel

1249
01:20:23,170 --> 01:20:26,890
as belonging to a certain class

1250
01:20:26,920 --> 01:20:31,960
like for example shadow ground aspen birch whatever

1251
01:20:32,000 --> 01:20:35,730
and that the invasion of that pixel label

1252
01:20:35,770 --> 01:20:40,060
better be dependent on its neighbours

1253
01:20:40,080 --> 01:20:44,940
how is a markov random field so that the random variable this pixel now

1254
01:20:44,940 --> 01:20:48,560
is there any failed but it is labels the panels neighbours

1255
01:20:48,580 --> 01:20:52,640
but also what determines my my my neighbors as aspen

1256
01:20:52,690 --> 01:20:54,000
what does my neighbour

1257
01:20:54,020 --> 01:20:57,560
but of course what i observed pixel level observed that pixel

1258
01:20:57,580 --> 01:21:01,460
red green and blue values in the next few weeks you could observe the whole

1259
01:21:02,620 --> 01:21:04,890
because they're getting this hyperspectral cameras

1260
01:21:04,940 --> 01:21:09,030
so you observe a lot of things that will so to sort things observation and

1261
01:21:09,030 --> 01:21:09,980
you have

1262
01:21:11,150 --> 01:21:14,390
a dependency between labels

1263
01:21:14,410 --> 01:21:19,560
and nothing goes right through all this work this two ways two components for inference

1264
01:21:19,630 --> 01:21:21,000
observation model

1265
01:21:21,060 --> 01:21:22,750
and the click

1266
01:21:22,850 --> 01:21:26,310
the the neighborhood dependencies

1267
01:21:26,330 --> 01:21:30,870
well how to you get a handle on my tractable anyway the first part how

1268
01:21:30,870 --> 01:21:36,890
to even get initial labeled pixels well done some pretty simple stuff which i think

1269
01:21:36,890 --> 01:21:37,670
is now

1270
01:21:37,730 --> 01:21:41,460
it depends on the application but i think of conditional

1271
01:21:41,480 --> 01:21:44,040
gas mixture models are

1272
01:21:46,080 --> 01:21:48,640
really perform reasonably well

1273
01:21:48,650 --> 01:21:52,120
we could well have used to support vector machine as well

1274
01:21:52,250 --> 01:21:56,600
the reason why like as mixture models is because they have to propel the conversation

1275
01:21:57,460 --> 01:22:02,040
i like having where i can understand the rules

1276
01:22:02,060 --> 01:22:05,830
in some sort of form some sort of explicit form

1277
01:22:05,870 --> 01:22:10,940
so in this work what we've done is used i class conditional gas mixture model

1278
01:22:10,960 --> 01:22:12,270
to get initial

1279
01:22:12,330 --> 01:22:17,560
to get initial labelling of all pixels for class

1280
01:22:17,560 --> 01:22:22,230
how do we do that we use supervised so expert photographer also yes that's an

1281
01:22:22,230 --> 01:22:25,310
aspen that to show that number of

1282
01:22:25,330 --> 01:22:30,690
and in that multi dimensional the colour space

1283
01:22:30,750 --> 01:22:32,420
the colour space

1284
01:22:32,490 --> 01:22:35,910
i have many places in the colour space

1285
01:22:35,920 --> 01:22:37,560
the correspond to

1286
01:22:38,120 --> 01:22:39,810
example as the

1287
01:22:39,870 --> 01:22:42,080
was many different regions

1288
01:22:42,140 --> 01:22:46,250
of the kind of people have different colour distribution and in many different regions of

1289
01:22:46,250 --> 01:22:49,000
the attribute space correspond to ground

1290
01:22:49,000 --> 01:22:50,770
we can assume that this one

1291
01:22:50,810 --> 01:22:55,420
cluster in be space and that's why i think you you were many classifiers don't

1292
01:22:56,170 --> 01:22:57,790
this is one

1293
01:22:59,370 --> 01:23:00,960
of evidence for him

1294
01:23:01,000 --> 01:23:04,920
that is because you have many sources of the same class

1295
01:23:04,940 --> 01:23:06,750
and of course what you're trying to do

1296
01:23:06,830 --> 01:23:09,420
is to derive a non-linear decision

1297
01:23:09,480 --> 01:23:11,350
that will tell you

1298
01:23:11,370 --> 01:23:12,650
at runtime

1299
01:23:12,670 --> 01:23:15,920
is this pixel belong to this class and a

1300
01:23:15,960 --> 01:23:20,460
so in the in the class conditional gasnet models of course use the best of

1301
01:23:20,460 --> 01:23:21,270
the best

1302
01:23:21,290 --> 01:23:24,870
you simply the nearest nearest neighbour mixture

1303
01:23:24,870 --> 01:23:27,580
as the the the main page

1304
01:23:28,250 --> 01:23:32,390
in the thing you build up your classifiers by con ditional trying

1305
01:23:32,440 --> 01:23:39,940
the different classes you fit mixture models we use bowman's MDL version of gas mixtures

1306
01:23:39,960 --> 01:23:45,140
is some criteria for deriving the number of fixtures and then for each class you

1307
01:23:45,140 --> 01:23:48,000
do that so because when you classify

1308
01:23:48,000 --> 01:23:52,840
you have you have classified simply give you the probability of class given the mixture

1309
01:23:52,840 --> 01:23:54,770
models the prior to this

1310
01:23:54,790 --> 01:23:55,920
and it's act

1311
01:23:56,750 --> 01:24:00,750
so it gives the way of of getting initial classification

1312
01:24:00,750 --> 01:24:04,980
but more important than that what it does is give us away the funding for

1313
01:24:04,980 --> 01:24:07,690
every pixel a probability vector

1314
01:24:07,710 --> 01:24:09,310
probability vector

1315
01:24:09,330 --> 01:24:12,060
so the input images but green blue

1316
01:24:12,060 --> 01:24:17,190
the output is false distance initial values is now probability that picture which is the

1317
01:24:17,190 --> 01:24:20,790
probability of being aspen birch ground or whatever

1318
01:24:21,910 --> 01:24:24,790
but i'm not going to make the hard decision about what it is now

1319
01:24:24,850 --> 01:24:27,410
because academics and now something else

1320
01:24:27,410 --> 01:24:29,350
and so on is something else

1321
01:24:29,370 --> 01:24:31,290
the something else is

1322
01:24:31,310 --> 01:24:34,600
now a richer structure

1323
01:24:34,600 --> 01:24:37,690
i said i mean going to something else was

1324
01:24:37,710 --> 01:24:39,710
my neighbourhood pixels by

1325
01:24:39,730 --> 01:24:42,730
and that was the traditional hough like of interface

1326
01:24:42,730 --> 01:24:47,710
rate of the universe and how it changes with time is determined by the energy

1327
01:24:47,710 --> 01:24:54,310
and pressure content of the universe so today dark energy if it exists is the

1328
01:24:54,310 --> 01:25:01,510
dominant energy content of the universe so in some sense it's driving the expansion of

1329
01:25:01,510 --> 01:25:13,850
the universe so on this important issue it's time to take sides we can't hide

1330
01:25:13,850 --> 01:25:20,390
from the data the data and the consistency of lambda CDM is overwhelming it's too

1331
01:25:20,390 --> 01:25:27,350
good to ignore and again it's not just supernovae it's all these other indicators also

1332
01:25:27,350 --> 01:25:30,900
and what we know is that what we know what we do know is that

1333
01:25:30,930 --> 01:25:38,110
H of Z is not given by the Einstein-de Sitter result so what the only thing

1334
01:25:38,110 --> 01:25:46,550
that we can say is that this equation does not work that G zero zero calculated

1335
01:25:46,550 --> 01:25:52,770
on the basis of the Robertson Walker metric is not equal to T zero zero

1336
01:25:52,770 --> 01:25:58,650
where the stress energy tensor is only matter so there are two things we

1337
01:25:58,650 --> 01:26:04,370
can do we only know an equation that doesn't work we can modify the right

1338
01:26:04,370 --> 01:26:11,190
hand side of Einstein's equations by adding a Delta T zero zero and we can

1339
01:26:11,190 --> 01:26:17,210
add something that's a constant to the right hand side that is just a cosmo

1340
01:26:17,210 --> 01:26:24,590
illogical constant I put just in quotes because it's a remarkable value it's a remarkable

1341
01:26:24,590 --> 01:26:31,230
result or you could add something that's not constant that has some dynamics and the

1342
01:26:31,230 --> 01:26:37,370
usual assumption is that the dynamics which has a change in time is driven by

1343
01:26:37,370 --> 01:26:44,830
a scalar field or you one might say you can modify the left side of the

1344
01:26:44,830 --> 01:26:51,810
Einstein equations and you can say that this the observations are the first indication that

1345
01:26:51,810 --> 01:26:58,860
gravity is something beyond Einstein's theory of gravity so you can add branes extra dimensions

1346
01:26:58,860 --> 01:27:05,190
strings I'll talk about models you'll understand what F of R means in a little while or

1347
01:27:05,220 --> 01:27:10,250
you can take the crazy approach that is just the Einstein equation and in fact

1348
01:27:10,250 --> 01:27:16,790
the friedmann homogeneous isotropic model is not a good description of our universe and

1349
01:27:16,790 --> 01:27:22,550
there's a back reaction on in in homogeneities that's adding another term to the left hand side

1350
01:27:22,550 --> 01:27:29,450
of the equation I'll talk about these possibilities so let's look at the right side

1351
01:27:29,450 --> 01:27:37,870
of Einstein's equations and ask what theoretical tools do we have to add something to

1352
01:27:37,870 --> 01:27:44,730
the right hand side of Einstein's equation now I think that I'd look out I see most

1353
01:27:44,730 --> 01:27:51,650
of you experimentalists so you might laugh when a theorist talks about tools right you

1354
01:27:51,650 --> 01:27:56,810
say well what does a theorist know about tools they only use a pencil a paper

1355
01:27:56,880 --> 01:28:03,590
and an eraser and if they're really good they don't need an eraser but in fact I

1356
01:28:03,590 --> 01:28:09,450
know a lot about tools the reason I know a lot about tools is in

1357
01:28:09,450 --> 01:28:17,850
my life I have owned two cars a British sports car and a fiat so for

1358
01:28:17,860 --> 01:28:22,970
twenty-five years I owned one of these cars and I had to work on the

1359
01:28:22,980 --> 01:28:30,290
damn things every week so I know a lot about tools and what I learned

1360
01:28:30,290 --> 01:28:36,950
after twenty years of using tools is that you really only need two tools there

1361
01:28:36,950 --> 01:28:43,690
are only two indispensable tools with two tools you can fix anything and the

1362
01:28:43,690 --> 01:28:59,010
two tools are are duct tape and WD forty and here's how you decide what to

1363
01:28:59,010 --> 01:29:09,010
use if something moves and it shouldn't you use duct tape I was very

1364
01:29:09,010 --> 01:29:17,210
relieved on my tour of atlas yesterday to see duct tape being used it keeps it from moving right otherwise

1365
01:29:17,210 --> 01:29:26,080
the detector would move around and if something doesn't move and it should you squirt it

1366
01:29:26,080 --> 01:29:34,830
with WD forty those two tools can fix anything now there are theoretical analogs to

1367
01:29:34,830 --> 01:29:43,760
duct tape and WD forty the theoretical analog to duct tape is the anthropic principle

1368
01:29:43,760 --> 01:29:51,950
or if you speak the language of string its called now the landscape the theoretical

1369
01:29:51,950 --> 01:29:58,490
analog to WD forty is scalar fields when you ever you want something to move

1370
01:29:58,490 --> 01:30:05,970
a little dynamics you squirt it with the scalar field in the language of dark energy the

1371
01:30:05,970 --> 01:30:12,850
scalar field is known as quintessence so let me talk about at the anthropic principle explanation

1372
01:30:12,850 --> 01:30:22,230
anthropic landscape duct tape or faith-based explanation of the cosmoillogical constant now this actually sounds much

1373
01:30:22,230 --> 01:30:27,630
better with a British accent but I can't do that I'm sorry the idea is

1374
01:30:27,630 --> 01:30:32,090
that we seen there are many sources of vacuum energy all of them tremendous

1375
01:30:32,090 --> 01:30:41,980
some positive and some negative perhaps why would they cancel to give something cancel

1376
01:30:41,980 --> 01:30:49,450
to a hundred and nineteen decimal places well the idea is that string theory has many and the

1377
01:30:49,450 --> 01:30:57,350
number that people will bandy about is ten to the five hundred vacua and the the

1378
01:30:57,350 --> 01:31:04,010
cancellations are different in all of these vacua some of these vacua correspond to cancellations

1379
01:31:04,010 --> 01:31:11,410
that lead lea yield a small cosmoillogical constant now to have such cancellations you might

1380
01:31:11,410 --> 01:31:20,690
expect that this cancellation would be exponentially uncommon but there are exponentially many vacua and the vacua

1381
01:31:20,690 --> 01:31:27,970
where the cancellation is almost exact or preferredbecause more common values of

1382
01:31:27,970 --> 01:31:38,710
a cosmological constant lea result in an inhospitable universe therefore string theory neatly explains the observations

1383
01:31:38,710 --> 01:31:43,730
by saying there's so many possible vacua we can't predict it so one of them must

1384
01:31:43,730 --> 01:31:47,990
have a value of the cosmoillogical constant of ten to the minus thirty grams per

1385
01:31:47,990 --> 01:31:55,450
cubic centimeter so you see like duct tape it's not an elegant solution to

1386
01:31:55,450 --> 01:32:04,210
the problem and it's probably not permanent so the other solution to the right hand side

1387
01:32:04,210 --> 01:32:11,270
is quintessence the WD forty approach that we're gonna squirt a scalar field and the idea is

1388
01:32:11,270 --> 01:32:17,890
that there are many possible contributions to the cosmological constant we have no theory that

1389
01:32:17,890 --> 01:32:22,110
this is just works

1390
01:32:22,150 --> 01:32:24,800
OK we can decide if

1391
01:32:24,870 --> 01:32:26,320
the question

1392
01:32:33,310 --> 01:32:35,570
it was just a

1393
01:32:35,580 --> 01:32:38,820
well with this is to this day

1394
01:32:38,840 --> 01:32:47,700
once you're OK so

1395
01:32:47,780 --> 01:32:50,130
this is the this thing here

1396
01:32:50,180 --> 01:32:55,100
is exactly that quantity

1397
01:32:55,150 --> 01:32:59,430
so so this thing is the kernel of the distribution but in fact is actually

1398
01:32:59,430 --> 01:33:03,840
also kernel between pairs as you can see

1399
01:33:03,870 --> 01:33:06,660
suppose is great

1400
01:33:06,670 --> 01:33:10,420
and obviously you would estimate this by just replacing these

1401
01:33:10,440 --> 01:33:14,690
by empirical averages so you just have double

1402
01:33:22,220 --> 01:33:25,040
ah yes we did

1403
01:33:28,590 --> 01:33:34,260
but actually sometimes of us can works slightly better so where is one of the

1404
01:33:34,260 --> 01:33:37,380
other i mean it doesn't make a huge difference but sometimes

1405
01:33:37,400 --> 01:33:41,300
but that's kind of work slightly better

1406
01:33:42,370 --> 01:33:45,380
the problem is attribute matching so that's what i told you before

1407
01:33:45,430 --> 01:33:49,990
so you have to companies which merge and you want to measure databases

1408
01:33:50,850 --> 01:33:54,100
this tool here will be just a tiny

1409
01:33:54,100 --> 01:33:57,870
ingredient in a big database merging two

1410
01:33:57,890 --> 01:34:01,650
so you have lots of other generating systems around

1411
01:34:01,670 --> 01:34:02,550
he one

1412
01:34:02,600 --> 01:34:06,130
to find the corresponding attributes between two thousand six

1413
01:34:06,150 --> 01:34:10,080
so what you do is you only use the distributions of random variables

1414
01:34:10,130 --> 01:34:13,840
just check whether those same

1415
01:34:14,930 --> 01:34:17,780
you want to match different sets of states of it

1416
01:34:17,840 --> 01:34:21,120
there is it's no good if you just taking that well they're all dates you

1417
01:34:21,120 --> 01:34:24,870
might want to have want to check whether they're evenly spaced and all that

1418
01:34:24,950 --> 01:34:27,160
you might want to match names

1419
01:34:27,180 --> 01:34:30,650
are but we can actually merged database at all

1420
01:34:30,670 --> 01:34:34,490
so the idea is well you just measure the distance between distributions of the coordinates

1421
01:34:34,510 --> 01:34:37,650
you find the best match

1422
01:34:38,370 --> 01:34:40,730
i'm going to to show you a little bit more

1423
01:34:40,750 --> 01:34:43,200
now with metric

1424
01:34:44,440 --> 01:34:46,130
different datasets and

1425
01:34:46,230 --> 01:34:51,110
well it works OK and many other tests are not too bad either

1426
01:34:52,130 --> 01:34:55,070
this is basically checking well how often

1427
01:34:55,120 --> 01:34:55,940
if the

1428
01:34:55,980 --> 01:35:00,560
if the attributes of the same does indeed say yeah that's OK and if they're

1429
01:35:00,580 --> 01:35:02,150
not the same how often does it say

1430
01:35:02,180 --> 01:35:04,030
it's still be OK to merge

1431
01:35:04,050 --> 01:35:07,770
it doesn't do an absolutely stellar job i mean

1432
01:35:07,820 --> 01:35:11,860
it still works better than the other but it's not really great

1433
01:35:12,670 --> 01:35:14,520
in some cases

1434
01:35:14,630 --> 01:35:17,880
uniform convergence this is actually not so bad

1435
01:35:19,110 --> 01:35:19,990
OK well

1436
01:35:20,220 --> 01:35:22,330
it's kind of reasonably

1437
01:35:22,380 --> 01:35:25,430
in this case it fails miserably

1438
01:35:25,440 --> 01:35:26,310
actually there

1439
01:35:26,360 --> 01:35:28,700
i think is above

1440
01:35:28,760 --> 01:35:30,290
it doesn't make any sense

1441
01:35:31,920 --> 01:35:34,550
the unbiased OK well

1442
01:35:34,570 --> 01:35:38,240
that's twenty five books and papers

1443
01:35:42,150 --> 01:35:45,420
there's something else that we can do so let's say i have got two

1444
01:35:46,730 --> 01:35:49,040
they all have ten attributes

1445
01:35:49,050 --> 01:35:52,990
i want to find the base pairing of the ten datasets of the one

1446
01:35:53,400 --> 01:35:56,550
ten ten attributes of the one that is it to the ten attributes of the

1447
01:35:56,550 --> 01:35:58,840
other parties

1448
01:35:58,890 --> 01:36:04,320
so in that case i know that i cannot assign one attribute to multiple instances

1449
01:36:04,320 --> 01:36:06,090
never it

1450
01:36:09,060 --> 01:36:12,270
well what i could do is i could just say well the cost of assigning

1451
01:36:13,620 --> 01:36:17,780
does it one me i to attribute j

1452
01:36:17,800 --> 01:36:22,710
yeah that's it just given by some CIJ

1453
01:36:22,720 --> 01:36:26,410
and the overall cost for matching one data set to the other will be just

1454
01:36:26,410 --> 01:36:27,670
the sum over

1455
01:36:27,710 --> 01:36:30,520
and this is another permutation

1456
01:36:30,580 --> 01:36:32,520
so in which permitted or the

1457
01:36:32,580 --> 01:36:36,950
entries will be near the opposite of the term CIJ

1458
01:36:36,980 --> 01:36:43,700
so in that case they want to minimize the cross this out to minimize instead

1459
01:36:43,710 --> 01:36:49,170
o the space of all the permutation matrices pi

1460
01:36:49,670 --> 01:36:53,240
so that sounds like a combinatorial optimization problem

1461
01:36:53,300 --> 01:36:54,590
the matching problems

1462
01:36:54,750 --> 01:36:58,480
but the good thing is that this is something that people have solved in the

1463
01:36:59,440 --> 01:37:02,420
the so-called linear assignment problem

1464
01:37:02,460 --> 01:37:04,060
and it turns out that you can

1465
01:37:04,080 --> 01:37:06,940
find the linear programming relaxation of this

1466
01:37:06,980 --> 01:37:11,110
and thereby solve the problem exactly

1467
01:37:11,140 --> 01:37:16,430
so obviously this problem here

1468
01:37:16,450 --> 01:37:20,590
so in all the right but for the moment

1469
01:37:22,110 --> 01:37:25,130
a relaxation of the upper problem

1470
01:37:25,140 --> 01:37:27,620
so the supply

1471
01:37:27,630 --> 01:37:30,620
i would just have zeros and ones

1472
01:37:30,740 --> 01:37:33,940
of course the only the rules some has to be one and the columns and

1473
01:37:33,940 --> 01:37:37,430
has to be one because i'm on a matching one term from one side to

1474
01:37:37,430 --> 01:37:38,420
the other

1475
01:37:38,430 --> 01:37:40,760
likewise only one term from the other side

1476
01:37:40,810 --> 01:37:42,510
so the first one

1477
01:37:42,520 --> 01:37:47,020
so that's basically a constraint on the first of the constraint on the second of

1478
01:37:47,070 --> 01:37:51,380
all distilled into spider have to be critical in zero

1479
01:37:51,420 --> 01:37:55,240
and if i were to impose the phi j's can only be zero one

1480
01:37:55,330 --> 01:38:00,690
and this will be formally called into the upper problem

1481
01:38:00,730 --> 01:38:05,030
OK any questions at that point at the moment so i've just done reformulation of

1482
01:38:12,660 --> 01:38:17,740
what would happen if i were to drop this

1483
01:38:18,660 --> 01:38:21,660
i'm basically relaxing my constraints

1484
01:38:21,670 --> 01:38:28,050
so therefore relax my constraints my solution can only get better

1485
01:38:30,020 --> 01:38:31,290
so let's

1486
01:38:31,320 --> 01:38:36,470
assume i just relax this constraint in which case it becomes a linear program

1487
01:38:36,490 --> 01:38:38,890
this is all linear constraints

1488
01:38:38,900 --> 01:38:41,360
and that's all it

1489
01:38:41,410 --> 01:38:44,360
and then it turns out that my solution point

1490
01:38:44,410 --> 01:38:47,140
has only zero one entries

1491
01:38:47,190 --> 01:38:48,650
what does it mean

1492
01:38:48,700 --> 01:38:51,450
what it means that i have found a solution

1493
01:38:51,490 --> 01:38:53,570
which is optimal

1494
01:38:53,620 --> 01:38:58,820
in the relaxed case but also in the for the constrained case

1495
01:38:58,840 --> 01:39:02,460
so it means that i have found a solution which is also optimal the more

1496
01:39:02,460 --> 01:39:06,120
tightly constrained case

1497
01:39:06,770 --> 01:39:09,530
and then you can start asking well is this coincidence or

1498
01:39:09,780 --> 01:39:15,130
can i always assume that i will find such a c one solution

1499
01:39:15,150 --> 01:39:17,060
and it turns out that for

1500
01:39:17,120 --> 01:39:19,420
this specific problem here

1501
01:39:19,430 --> 01:39:21,950
i will always find a solution

1502
01:39:21,970 --> 01:39:27,120
that will just consist of zeros and ones

1503
01:39:27,150 --> 01:39:29,420
like magic

1504
01:39:29,440 --> 01:39:30,890
well why is this so

1505
01:39:30,900 --> 01:39:35,000
well if you look at those constraints if you write them out it turns out

1506
01:39:35,000 --> 01:39:39,800
that those constraints can be viewed as a union modular constraint

1507
01:39:39,830 --> 01:39:43,820
and the model of constraints are particularly nice because if you take a submatrix of

1508
01:39:43,830 --> 01:39:49,230
the big matrix in inverted it'll turn out it will just having two interchanges

1509
01:39:49,230 --> 01:39:52,310
and if the inverse of a matrix is only integer

1510
01:39:52,330 --> 01:39:55,080
my right hand side contains only integers

1511
01:39:55,130 --> 01:39:59,610
then the solution of a linear system will also be only integers

1512
01:39:59,690 --> 01:40:03,590
and unimodular matrices do exactly that

1513
01:40:03,650 --> 01:40:06,000
this is one such case

1514
01:40:06,030 --> 01:40:07,930
this is what could among found

1515
01:40:07,930 --> 01:40:11,580
and they came up with the hungarian marriage there

1516
01:40:11,640 --> 01:40:17,430
so gay marriage theorem is basically in hungarian guys in hungarian women and they all

1517
01:40:17,430 --> 01:40:20,420
would like to get married in fact they will have to get married

1518
01:40:20,430 --> 01:40:23,220
but they don't all like each other equally well

1519
01:40:23,270 --> 01:40:29,240
and overall you want to find some assignment that will make everybody maximally happy

1520
01:40:29,240 --> 01:40:32,750
first i differentiate with respect to x o i get white squares

1521
01:40:32,770 --> 01:40:37,780
then i differentiate with respect to the other factory the y squared factor and i

1522
01:40:37,780 --> 01:40:42,030
get next time two y y prime

1523
01:40:42,080 --> 01:40:44,150
and then zero gives me zero so

1524
01:40:44,180 --> 01:40:45,670
one zero

1525
01:40:48,810 --> 01:40:55,070
right so there is the implicit differentiation step

1526
01:40:55,120 --> 01:40:56,510
and now

1527
01:40:56,560 --> 01:41:01,160
i just want to solve for y prime so i'm going to factor out for

1528
01:41:01,160 --> 01:41:03,180
y q and

1529
01:41:03,250 --> 01:41:08,760
o plus two x y that's the factor on why prime

1530
01:41:08,820 --> 01:41:13,510
and i'm going to put the white squares on the other side minus y squared

1531
01:41:13,510 --> 01:41:16,330
over here

1532
01:41:16,350 --> 01:41:18,980
and so the formula for y prime

1533
01:41:21,720 --> 01:41:28,120
the nice why squared divided by four like you don't want to fly

1534
01:41:28,160 --> 01:41:29,520
so that's the

1535
01:41:29,550 --> 01:41:30,920
the formula

1536
01:41:30,970 --> 01:41:33,600
the solution

1537
01:41:33,720 --> 01:41:40,030
the slope

1538
01:41:40,180 --> 01:41:44,000
give questions

1539
01:41:49,240 --> 01:41:52,620
so the question is for the why would we have to sort of put in

1540
01:41:52,620 --> 01:41:54,770
what we saw before an explicit

1541
01:41:54,780 --> 01:41:59,860
equation and the answer is absolutely yes that's exactly the point so this is not

1542
01:41:59,860 --> 01:42:00,910
a complete

1543
01:42:00,910 --> 01:42:02,850
the solution to all problems

1544
01:42:04,040 --> 01:42:08,750
we started with an implicit equation we differentiated and we got in the and also

1545
01:42:08,750 --> 01:42:12,860
an implicit equation doesn't tell us what y is a function of x we have

1546
01:42:12,860 --> 01:42:16,110
to go back to this formula

1547
01:42:16,110 --> 01:42:21,080
to get the formula for four x so for example let me give you

1548
01:42:21,140 --> 01:42:23,470
an example here so this is

1549
01:42:23,480 --> 01:42:24,710
this is the

1550
01:42:24,760 --> 01:42:29,410
hides a degree of complexity of the problem but it's a degree of complexity that

1551
01:42:29,410 --> 01:42:31,280
we must live with

1552
01:42:31,320 --> 01:42:33,530
here so for example

1553
01:42:35,990 --> 01:42:37,720
x equals one

1554
01:42:37,740 --> 01:42:39,890
you can see

1555
01:42:39,920 --> 01:42:43,120
that's why someone saw

1556
01:42:43,140 --> 01:42:45,240
that happens to be

1557
01:42:45,300 --> 01:42:50,470
so it's also one of the four x y where minus two zero that's why

1558
01:42:50,480 --> 01:42:53,350
i picked the two actually so it would be one plus one minus two equals

1559
01:42:54,160 --> 01:42:57,610
i just wanted to have a convenient solution there pull out of my head at

1560
01:42:57,610 --> 01:42:58,660
this point

1561
01:42:58,710 --> 01:43:02,810
so i did that and so we now know that when x equals one y

1562
01:43:02,810 --> 01:43:04,920
equals one so at

1563
01:43:04,930 --> 01:43:07,280
one one along the curve

1564
01:43:09,410 --> 01:43:12,260
the slope

1565
01:43:12,440 --> 01:43:18,980
is equal to one well i have to plug in here minus one squared divided

1566
01:43:19,650 --> 01:43:21,780
four times one cubed

1567
01:43:22,620 --> 01:43:25,240
two kinds one times one

1568
01:43:25,320 --> 01:43:28,900
that's just plugging in that formula over there which turns out to be minus one

1569
01:43:32,020 --> 01:43:36,810
right so i can get it on the other hand

1570
01:43:45,310 --> 01:43:48,100
we're stuck

1571
01:43:54,400 --> 01:43:56,430
this formula star

1572
01:43:56,480 --> 01:44:01,060
to find more

1573
01:44:08,110 --> 01:44:12,460
so let me let me just make two points about this which are just philosophical

1574
01:44:12,460 --> 01:44:14,290
point for you right now

1575
01:44:14,310 --> 01:44:17,170
the first is

1576
01:44:17,230 --> 01:44:20,550
one i promise you at the beginning of this class that we're going to be

1577
01:44:20,550 --> 01:44:24,400
able to differentiate any function you know

1578
01:44:24,490 --> 01:44:29,210
i meant it very literally what i meant is if you know the function

1579
01:44:29,220 --> 01:44:32,060
will be able to give a formula for the derivative you don't know how to

1580
01:44:32,060 --> 01:44:36,430
find the function you have a lot of trouble finding the derivative so we didn't

1581
01:44:36,430 --> 01:44:38,060
make any promises

1582
01:44:38,220 --> 01:44:40,950
but if you can't find the function you will be able to find the derivative

1583
01:44:40,960 --> 01:44:46,970
by some magic that will never happen and however complex the function is a root

1584
01:44:46,980 --> 01:44:52,900
of fourth degree polynomial is are can be pretty complicated function of the coefficients

1585
01:44:52,940 --> 01:44:57,060
we're stuck with this degree of complexity in the problem

1586
01:44:57,080 --> 01:45:01,960
but the big advantage of this method notice is that although we had to find

1587
01:45:01,960 --> 01:45:04,380
star we had to find its formal star

1588
01:45:04,380 --> 01:45:07,600
or and there are many other ways of doing these things numerically by the way

1589
01:45:07,610 --> 01:45:09,070
which will learn later

1590
01:45:09,130 --> 01:45:12,410
so there's a good method for doing it numerically

1591
01:45:12,420 --> 01:45:16,030
although we had to find that we never had differentiated

1592
01:45:16,050 --> 01:45:20,390
we had a fast way of getting the slope so we need to know what

1593
01:45:20,390 --> 01:45:25,170
x and y were but why primary got by an algebraic formula in terms of

1594
01:45:25,230 --> 01:45:27,550
the values here

1595
01:45:27,570 --> 01:45:30,170
so this is very fast forgetting the slope

1596
01:45:30,210 --> 01:45:33,650
once you know the the point

1597
01:45:34,710 --> 01:45:39,700
what's in parentheses are used this is well let's see if i can manage is

1598
01:45:39,700 --> 01:45:40,970
this this little

1599
01:45:40,990 --> 01:45:45,460
is this the parentheses you're talking about

1600
01:45:46,490 --> 01:45:48,190
this is say

1601
01:45:48,200 --> 01:45:53,960
well so maybe i should put commas around it was as a y

1602
01:45:54,050 --> 01:45:56,150
come come

1603
01:45:57,640 --> 01:45:59,100
well here was at

1604
01:45:59,110 --> 01:46:01,460
at x equals one

1605
01:46:01,500 --> 01:46:06,130
and here's but at say i'm just throwing out of value here

1606
01:46:06,210 --> 01:46:10,470
any other value actually there's one value my favorite value well this is easy to

1607
01:46:10,470 --> 01:46:15,530
evaluate right lexical zero i can i can do it there

1608
01:46:15,660 --> 01:46:18,540
that's the that maybe the only one which is

1609
01:46:18,550 --> 01:46:23,070
these are in use

1610
01:46:32,040 --> 01:46:35,740
other questions

1611
01:46:36,480 --> 01:46:40,570
now now we have to do something more here so i claimed to you that

1612
01:46:40,570 --> 01:46:41,990
that we

1613
01:46:42,050 --> 01:46:46,310
we can differentiate all the functions we know but really we can learn a tremendous

1614
01:46:46,310 --> 01:46:49,740
about about about some functions which are really hard to get at

1615
01:46:49,740 --> 01:46:52,640
i mean even though you've already seen this you still have to kind of figure

1616
01:46:52,650 --> 01:46:57,880
out each word piece by piece because you can use the larger structures of of

1617
01:46:57,880 --> 01:47:02,670
semantics the underlying meaning of the five this is expressing as well as the syntax

1618
01:47:02,880 --> 01:47:03,680
the rules about how

1619
01:47:04,120 --> 01:47:09,820
words go together in linear sequence to produce complex meaning

1620
01:47:09,830 --> 01:47:13,450
and i think this this helps to show you why we need if we're going

1621
01:47:13,450 --> 01:47:17,450
to you know we we want to apply statistical approaches to understanding passing and language

1622
01:47:17,450 --> 01:47:21,970
learning but they need to be sensitive to a lot more than current natural language

1623
01:47:21,970 --> 01:47:25,830
processing approaches are they need to be sensitive to the structure of syntax and they

1624
01:47:25,830 --> 01:47:28,810
need to be struck sensitive to meaning

1625
01:47:29,790 --> 01:47:32,940
sure why not

1626
01:47:35,350 --> 01:47:42,090
well sure that shannon said i mean a lot of what generated the early controversy

1627
01:47:42,090 --> 01:47:47,430
about statistics and language with chomsky was i i take it reading shannon's first paper

1628
01:47:47,460 --> 01:47:50,560
information theory which he showed how much you could do really cool you could do

1629
01:47:50,560 --> 01:47:54,520
with you know just on grounds for graphs right i mean everybody if you haven't

1630
01:47:54,520 --> 01:47:59,030
read shannon's classic paper on information they go back and read it he basically introduced

1631
01:47:59,030 --> 01:48:02,140
that idea that he didn't have the massive data that google had but i think

1632
01:48:02,140 --> 01:48:06,730
everybody would agree you have all the data in the world as we now have

1633
01:48:06,960 --> 01:48:08,670
that could work

1634
01:48:08,690 --> 01:48:11,290
on the this

1635
01:48:11,290 --> 01:48:17,360
yes i'm arguing that the basic technology of some of the information theory and finding

1636
01:48:17,400 --> 01:48:22,930
clusters that has been with that underlies a lot of machine learning since shannon can

1637
01:48:22,930 --> 01:48:27,980
solve this problem but that is not enough to solve this problem because it doesn't

1638
01:48:27,980 --> 01:48:31,990
capture meaning and it doesn't capture the rules by which meaning is projected into linear

1639
01:48:31,990 --> 01:48:36,230
sequences of words that would be syntax semantics and syntax that's not to say that

1640
01:48:36,230 --> 01:48:40,240
statistics information zero relevant that we need as as i think it was just need

1641
01:48:40,240 --> 01:48:44,710
to understand how those ideas for learning and inference operate over the kinds of representations

1642
01:48:44,710 --> 01:48:50,110
of meaning and syntactic structure that linguists have proposed and i think google would agree

1643
01:48:50,110 --> 01:48:53,240
that's why they're not really trying to solve this problem and that's why they kind

1644
01:48:53,240 --> 01:48:57,500
of laugh off start-up switch which claimed to threaten them like for example powerset was

1645
01:48:57,830 --> 01:49:00,750
hot start in the internet world for a little while say well we're going to

1646
01:49:00,750 --> 01:49:04,720
use actual now you know syntax for natural language processing we're going to be google

1647
01:49:04,720 --> 01:49:09,430
in natural language search and go to step how to the right when then i

1648
01:49:09,430 --> 01:49:12,660
think microsoft bought them for some what in that business is a paltry sum of

1649
01:49:12,660 --> 01:49:14,890
one hundred million or something y

1650
01:49:14,940 --> 01:49:20,230
well because google said we we just don't have the technology yet to solve that

1651
01:49:20,230 --> 01:49:23,000
problem so we're you know we're not even i'm going to try to do that

1652
01:49:23,220 --> 01:49:25,310
we're going to solve the problems we know how to do the ones that were

1653
01:49:27,140 --> 01:49:28,430
i would say

1654
01:49:28,450 --> 01:49:31,020
we don't have that we don't have the technology to solve in the we google

1655
01:49:31,020 --> 01:49:35,480
wants to sort which is you know very high accuracy instantaneous across the entire world

1656
01:49:35,850 --> 01:49:39,550
but for me while i'm excited about this coming together again in these fields is

1657
01:49:39,550 --> 01:49:42,140
that i do think we have these kinds of ideas and i want to try

1658
01:49:42,150 --> 01:49:45,690
to sketch out this is just as much it's not it is not a google

1659
01:49:45,690 --> 01:49:51,400
level engineering challenge it's more of basic scientific research in reverse engineering challenge

1660
01:49:51,410 --> 01:49:56,130
there are of course many other inductive problems in language and big focus which i

1661
01:49:56,130 --> 01:49:58,950
want to say much more about now but we'll definitely see this later on summer

1662
01:49:58,950 --> 01:50:04,480
schools acquisition so there's not just using your knowledge of language to pass sentence but

1663
01:50:04,490 --> 01:50:08,420
learning and knowledge of language how to learn the rules of syntax well that's that's

1664
01:50:08,420 --> 01:50:12,860
that's another place where there's a long long been study of how the mind goes

1665
01:50:12,860 --> 01:50:15,540
beyond the data that's given and

1666
01:50:15,550 --> 01:50:17,520
we hear a lot about that

1667
01:50:17,530 --> 01:50:19,880
just one last set of examples

1668
01:50:19,950 --> 01:50:24,380
which i'll start off with thinking from science and then moved to to children's cognitive

1669
01:50:24,380 --> 01:50:30,590
development the problem of representing and constructing a theory of the world so i think

1670
01:50:30,590 --> 01:50:35,110
about what might be the greatest moments of human learning about the greatest accomplishments of

1671
01:50:35,110 --> 01:50:38,930
wins when scientists or whatever you want to call them natural philosophers are just final

1672
01:50:38,940 --> 01:50:45,460
philosophers have have have come to fundamentally new and fundamentally more correct insights about how

1673
01:50:45,460 --> 01:50:49,230
the world works by in some sense looking at data observations it's so how did

1674
01:50:49,230 --> 01:50:54,520
darwin come to his theory of natural selection by looking at the finches on galapagos

1675
01:50:54,540 --> 01:50:58,900
or how did newton come his universal law of gravitation by looking at the orbits

1676
01:50:58,900 --> 01:51:02,960
of the planets and maybe some data about the moon and maybe a few apples

1677
01:51:02,980 --> 01:51:08,190
how did mendel glimpse the basics of genetics in his experiments with p parts was

1678
01:51:08,190 --> 01:51:12,250
based on finding clusters are correlations in masses of high dimensional data

1679
01:51:12,270 --> 01:51:13,480
it doesn't seem like

1680
01:51:13,560 --> 01:51:17,330
it was based on finding some kind of causal

1681
01:51:17,370 --> 01:51:22,540
model positing something about the the underlying but unseen causal structure of the world and

1682
01:51:22,540 --> 01:51:27,190
CNN comparing different kinds of causal theories and seeing which seems to best explain the

1683
01:51:28,230 --> 01:51:31,150
another way to think of it is urged to pose this challenge is to go

1684
01:51:31,150 --> 01:51:34,620
back to what i think that was talking about a fundamental challenge for any kind

1685
01:51:34,620 --> 01:51:39,170
of association all learning association of learning or statistical learning is

1686
01:51:39,190 --> 01:51:43,580
if you like the discovery of new concepts or new dimensions over which read the

1687
01:51:43,580 --> 01:51:47,850
statistics or the associations have to be computed so when we're talking about doing dimensionality

1688
01:51:47,850 --> 01:51:50,080
you always have a quadratic function whose only

1689
01:51:50,100 --> 01:51:51,600
exercise two

1690
01:51:51,620 --> 01:51:54,570
the weight

1691
01:51:54,620 --> 01:51:56,210
represents the graphs

1692
01:51:56,360 --> 01:51:58,190
represent could in that for

1693
01:51:58,200 --> 01:52:03,110
will see before you have many adverse effects

1694
01:52:03,110 --> 01:52:05,940
is linear terms class

1695
01:52:05,990 --> 01:52:07,880
these terms we have

1696
01:52:07,880 --> 01:52:09,660
OK i j x i

1697
01:52:09,700 --> 01:52:10,870
so far

1698
01:52:11,950 --> 01:52:14,160
right always do that by

1699
01:52:14,170 --> 01:52:16,890
so we have terms like excite xj

1700
01:52:16,980 --> 01:52:18,930
just replace x by

1701
01:52:18,970 --> 01:52:21,270
one minus six five our multiply

1702
01:52:21,270 --> 01:52:26,390
right you get in this form

1703
01:52:26,450 --> 01:52:27,750
i mean that

1704
01:52:27,800 --> 01:52:29,800
what do we do

1705
01:52:31,040 --> 01:52:33,590
we draw a graph

1706
01:52:33,800 --> 01:52:36,840
px i

1707
01:52:39,020 --> 01:52:40,840
is zero

1708
01:52:40,840 --> 01:52:43,840
this one is this is the key

1709
01:52:44,040 --> 01:52:45,710
the key construction

1710
01:52:47,290 --> 01:52:49,220
i take it take an edge

1711
01:52:49,240 --> 01:52:54,180
from excited xj going to put weight on what is that age me

1712
01:52:54,190 --> 01:52:55,930
what do you think of you think of

1713
01:52:55,970 --> 01:52:59,640
the head of this our main xj

1714
01:52:59,650 --> 01:53:01,130
the tail of the our

1715
01:53:01,140 --> 01:53:02,970
is excited by

1716
01:53:02,980 --> 01:53:06,110
because the tail you put a bar across

1717
01:53:06,110 --> 01:53:07,620
and i take this to

1718
01:53:07,630 --> 01:53:08,660
i i j

1719
01:53:08,680 --> 01:53:09,850
i may

1720
01:53:09,900 --> 01:53:11,790
put that value

1721
01:53:11,820 --> 01:53:14,020
on the page

1722
01:53:16,380 --> 01:53:19,380
what about a linear term so OK this so

1723
01:53:19,680 --> 01:53:21,840
all these for all the

1724
01:53:21,900 --> 01:53:24,150
six k

1725
01:53:24,340 --> 01:53:26,800
have a

1726
01:53:26,800 --> 01:53:29,110
i j k weight HAK

1727
01:53:29,120 --> 01:53:30,440
being here OK

1728
01:53:30,460 --> 01:53:33,990
what about the new maybe the term five

1729
01:53:34,040 --> 01:53:35,840
x i

1730
01:53:46,280 --> 01:53:49,600
if x i and x k

1731
01:53:49,790 --> 01:53:53,380
bring in the in the in the markov friends the neighbours in the markov random

1732
01:53:53,380 --> 01:53:55,270
field and there will be such

1733
01:53:55,480 --> 01:53:57,690
certainly there can potentially be

1734
01:53:57,750 --> 01:54:01,360
just between

1735
01:54:01,370 --> 01:54:04,910
no i mean typically is not an order left to right typically if this is

1736
01:54:04,910 --> 01:54:06,380
an image

1737
01:54:06,390 --> 01:54:10,770
you've got them into two-dimensional re-negotiate here one here

1738
01:54:12,500 --> 01:54:16,140
you can then they'll be edges here the edges

1739
01:54:16,240 --> 01:54:17,610
only between the

1740
01:54:17,700 --> 01:54:21,000
the neighbouring terms like that right

1741
01:54:21,020 --> 01:54:29,020
now officially on july

1742
01:54:34,910 --> 01:54:44,320
in the lot for like minded

1743
01:54:44,360 --> 01:54:45,540
if only

1744
01:54:45,590 --> 01:54:47,640
with graph cuts you

1745
01:54:47,770 --> 01:54:52,360
with the ordered edges

1746
01:54:52,360 --> 01:54:55,310
one is the special cluster here i guess

1747
01:54:55,350 --> 01:55:01,890
when adding spectral clustering here

1748
01:55:01,910 --> 01:55:04,650
well i'm sure you can could that form that

1749
01:55:04,660 --> 01:55:06,860
typically in the general case

1750
01:55:06,860 --> 01:55:10,310
you've got directed edges that's for sure

1751
01:55:26,520 --> 01:55:28,790
i mean from the point of view of

1752
01:55:28,880 --> 01:55:29,900
the flow

1753
01:55:29,920 --> 01:55:31,730
this very different very

1754
01:55:31,820 --> 01:55:34,350
big difference between edge going here

1755
01:55:34,360 --> 01:55:39,600
with five hundred and it's going abstraction five one means you can push things in

1756
01:55:39,600 --> 01:55:40,850
that direction

1757
01:55:40,920 --> 01:55:44,450
five using the world's first

1758
01:55:44,470 --> 01:55:47,390
so that the directed edges

1759
01:55:47,420 --> 01:55:49,470
from directed edges

1760
01:55:49,640 --> 01:55:53,770
what do we do with linear terms five exi

1761
01:55:53,840 --> 01:55:57,250
we can think of that if you like is being five

1762
01:55:57,270 --> 01:55:58,890
zero by

1763
01:55:58,950 --> 01:56:00,810
x y zero by

1764
01:56:02,150 --> 01:56:04,790
right so by my

1765
01:56:05,940 --> 01:56:06,960
that would be

1766
01:56:06,970 --> 01:56:08,380
five there

1767
01:56:08,440 --> 01:56:11,710
excise the head of the arrow zero tails

1768
01:56:11,720 --> 01:56:15,540
right so that be five there will be five x y

1769
01:56:15,540 --> 01:56:18,130
and so any

1770
01:56:19,070 --> 01:56:24,160
an element like that of an edge here three that will be for instance three

1771
01:56:24,180 --> 01:56:25,300
times one

1772
01:56:25,300 --> 01:56:26,780
times x y

1773
01:56:27,420 --> 01:56:28,600
in other words

1774
01:56:29,430 --> 01:56:30,110
x i

1775
01:56:30,230 --> 01:56:33,550
so any function like this

1776
01:56:33,700 --> 01:56:36,240
one that you

1777
01:56:36,250 --> 01:56:40,030
the edges weights correspond one-to-one with edges

1778
01:56:40,060 --> 01:56:41,560
weights on the graph

1779
01:56:41,640 --> 01:56:44,230
now the point what is the point

1780
01:56:45,060 --> 01:56:46,390
of this

1781
01:56:46,500 --> 01:56:48,020
the point is

1782
01:56:48,120 --> 01:56:49,650
if for instance

1783
01:56:49,710 --> 01:56:51,880
i now partition

1784
01:56:51,960 --> 01:56:55,200
and this comes up to zero

1785
01:56:55,250 --> 01:56:58,000
and this comes to be one

1786
01:56:58,050 --> 01:56:59,500
and that means

1787
01:56:59,520 --> 01:57:02,430
the x equals zero

1788
01:57:04,100 --> 01:57:05,340
equals one

1789
01:57:08,150 --> 01:57:11,070
so that means in fact that exi power

1790
01:57:12,620 --> 01:57:15,070
equals one

1791
01:57:15,170 --> 01:57:16,540
so this

1792
01:57:19,110 --> 01:57:21,750
that's one so this contribute

1793
01:57:21,810 --> 01:57:24,400
i i j to the way that is exactly

1794
01:57:24,420 --> 01:57:25,310
the edge

1795
01:57:25,320 --> 01:57:27,460
which is which is cut words

1796
01:57:27,520 --> 01:57:29,500
some of the edges chicago

1797
01:57:29,550 --> 01:57:31,340
these are exactly

1798
01:57:32,030 --> 01:57:33,720
value function

1799
01:57:35,020 --> 01:57:37,490
with this construction

1800
01:57:37,550 --> 01:57:39,750
how various it just to make partition

1801
01:57:39,770 --> 01:57:43,950
and just as edges which go from zero to one correspond to non-zero terms here

1802
01:57:45,030 --> 01:57:47,300
which have the corresponding weights

1803
01:57:48,920 --> 01:57:49,910
in this way

1804
01:57:49,920 --> 01:57:51,540
inside ref

1805
01:57:51,540 --> 01:57:52,850
which she gives you

1806
01:57:52,900 --> 01:57:55,580
the function so for instance

1807
01:57:55,660 --> 01:57:58,160
as well as a here

1808
01:58:00,750 --> 01:58:04,480
well there is no need to look at the example on the on the right

1809
01:58:04,480 --> 01:58:05,490
that gives you

1810
01:58:05,500 --> 01:58:07,630
he's your example of something

1811
01:58:08,250 --> 01:58:09,880
complex function

1812
01:58:09,880 --> 01:58:12,590
i'm going to break it by saying that well

1813
01:58:12,600 --> 01:58:14,920
i require that all the points here

1814
01:58:14,930 --> 01:58:17,110
the value of the linear function

1815
01:58:17,130 --> 01:58:21,700
less people than one one and here greater recall than one

1816
01:58:21,770 --> 01:58:23,150
so there's nothing

1817
01:58:23,200 --> 01:58:25,600
magic about two things one

1818
01:58:25,610 --> 01:58:29,420
as a matter of fact we can actually make this venerable of the optimisation problem

1819
01:58:29,420 --> 01:58:30,600
later as well

1820
01:58:30,650 --> 01:58:32,220
and then you will get

1821
01:58:32,230 --> 01:58:36,710
svm optimisation with a new trick which unfortunately won't be able to cover

1822
01:58:36,730 --> 01:58:39,130
in this talk just because of lack of time

1823
01:58:40,920 --> 01:58:52,840
basically i mean by the lookup or looks look for something called the new trick

1824
01:58:53,560 --> 01:58:55,490
when i wrote a paper on that

1825
01:58:55,500 --> 01:58:59,160
i think nineteen ninety eight

1826
01:58:59,170 --> 01:59:02,950
for two thousand and we up to the neural computation

1827
01:59:02,970 --> 01:59:07,440
and you'll find all sorts of ways how to adjust the way

1828
01:59:11,830 --> 01:59:16,100
now that we assume that well i can actually go and try to compute these

1829
01:59:17,340 --> 01:59:20,760
well without loss of childcare can assume that has to be at least one point

1830
01:59:20,760 --> 01:59:22,820
here and there which

1831
01:59:22,900 --> 01:59:27,380
o which the WTA but the obtained the value of one on one one

1832
01:59:27,420 --> 01:59:30,760
well why not because if it were not the case i could just extend the

1833
01:59:30,760 --> 01:59:34,720
margin until i hit one of the points of the other side

1834
01:59:34,770 --> 01:59:39,340
so therefore there must be at least one point to which the constraint is active

1835
01:59:40,280 --> 01:59:43,960
now if we do that well we it there must be some point it's one

1836
01:59:44,030 --> 01:59:46,300
whatever they want with these ones

1837
01:59:46,360 --> 01:59:51,230
o point x o which relates to put these minus one

1838
01:59:51,280 --> 01:59:56,500
taking the difference between the equations gives us w x one one x two

1839
01:59:56,550 --> 01:59:58,490
it was it

1840
01:59:58,540 --> 02:00:01,650
well something very nice happen he has gone away

1841
02:00:01,700 --> 02:00:05,080
we don't know what he really might be anyway so getting rid of it is

1842
02:00:07,710 --> 02:00:09,090
we therefore no

1843
02:00:09,100 --> 02:00:12,730
that the inner product between this connection here

1844
02:00:12,740 --> 02:00:14,950
and the normal vector to the hyperplane

1845
02:00:14,960 --> 02:00:17,180
unlike normal vector

1846
02:00:18,490 --> 02:00:19,900
it's the

1847
02:00:20,150 --> 02:00:22,190
now we almost

1848
02:00:23,010 --> 02:00:25,290
although now need to do is i just need to

1849
02:00:25,300 --> 02:00:29,740
we restrict this vector w unit link i need to replace w

1850
02:00:29,810 --> 02:00:32,090
but you want to by normal

1851
02:00:32,130 --> 02:00:36,410
take an approach that with x two one i think one

1852
02:00:36,420 --> 02:00:38,340
so in other words and predicting

1853
02:00:38,980 --> 02:00:40,250
o line here

1854
02:00:40,260 --> 02:00:42,910
on to the normal

1855
02:00:42,950 --> 02:00:45,310
that the product within tell me how

1856
02:00:45,320 --> 02:00:46,370
the two

1857
02:00:46,380 --> 02:00:48,840
trials are

1858
02:00:50,350 --> 02:00:52,560
if i do this

1859
02:00:52,570 --> 02:00:56,950
just you want everything but norm of w and find out that two of the

1860
02:00:56,950 --> 02:00:58,030
norm of w

1861
02:00:58,100 --> 02:01:00,750
is the the weight of the margin

1862
02:01:01,950 --> 02:01:03,030
so now

1863
02:01:03,040 --> 02:01:04,250
i can actually just

1864
02:01:04,270 --> 02:01:09,160
really take this problem and optimize away and i would get something that's not totally

1865
02:01:10,940 --> 02:01:12,070
so in other words

1866
02:01:12,080 --> 02:01:15,870
what i can do is i can require that for all the points which line

1867
02:01:15,870 --> 02:01:17,190
plus minus one

1868
02:01:17,300 --> 02:01:18,710
that that makes

1869
02:01:18,720 --> 02:01:19,600
but the

1870
02:01:19,630 --> 02:01:21,590
is less people than one one

1871
02:01:21,640 --> 02:01:24,160
for all the points which are in the positive class

1872
02:01:24,230 --> 02:01:25,790
and it well

1873
02:01:25,800 --> 02:01:27,450
great people than one

1874
02:01:27,500 --> 02:01:31,440
and i'm going to maximize two over the normal w

1875
02:01:31,510 --> 02:01:36,560
the question here

1876
02:01:36,610 --> 02:01:39,890
that is the geometry clear

1877
02:01:39,980 --> 02:01:42,720
what i'm projecting

1878
02:01:42,740 --> 02:01:44,340
onto this normal vector

1879
02:01:44,350 --> 02:01:46,460
this guy here that w

1880
02:01:46,480 --> 02:01:48,180
w is orthogonal

1881
02:01:48,200 --> 02:01:49,750
to the hyperplane just

1882
02:01:49,800 --> 02:01:52,000
as you define a hyperplane

1883
02:01:53,850 --> 02:01:55,240
it's normal vector

1884
02:01:55,330 --> 02:01:57,170
and this offset from the origin

1885
02:01:57,210 --> 02:01:58,630
they also be

1886
02:01:58,640 --> 02:02:01,170
the normal vector w

1887
02:02:01,180 --> 02:02:05,710
and i'm projecting this line which unfortunately i cannot really reach

1888
02:02:05,720 --> 02:02:09,100
in the following up onto

1889
02:02:09,140 --> 02:02:11,190
the small picture

1890
02:02:13,250 --> 02:02:17,290
this will give me constrained optimization problem and i could

1891
02:02:17,330 --> 02:02:20,140
just leave it at that the problem is

1892
02:02:20,190 --> 02:02:24,890
but this in an objective function to be maximized is particularly nasty

1893
02:02:24,940 --> 02:02:28,630
and we got the normal rather than the squared norm we have to do likewise

1894
02:02:28,650 --> 02:02:33,120
we need to maximise its that's not the knife optimisation problem

1895
02:02:33,230 --> 02:02:37,800
now we can actually turn it into a very nice convex optimisation problem that was

1896
02:02:37,810 --> 02:02:41,650
in way the genius of thinking germany is in the sixties to realize that you

1897
02:02:41,650 --> 02:02:43,670
could do it

1898
02:02:43,710 --> 02:02:49,120
and the way they did this by realising well rather than maximizing revenue over

1899
02:02:49,130 --> 02:02:50,430
two or w

1900
02:02:50,440 --> 02:02:53,040
you could minimize w with the right

1901
02:02:54,130 --> 02:02:56,990
the minute quantities so this is all white

1902
02:02:58,030 --> 02:03:02,130
OK this at least gives a convex optimisation problem

1903
02:03:02,140 --> 02:03:03,600
along with the kind of function

1904
02:03:03,620 --> 02:03:05,990
so that should be all right

1905
02:03:06,000 --> 02:03:07,090
well don't know

1906
02:03:07,100 --> 02:03:11,380
you know what the quality of it

1907
02:03:12,470 --> 02:03:13,830
most hands are around

1908
02:03:13,840 --> 02:03:16,600
not all of it quickly draw word

1909
02:03:17,680 --> 02:03:21,640
which is a piece of chalk

1910
02:03:21,690 --> 02:03:24,080
the draw

1911
02:03:24,370 --> 02:03:26,230
as a

1912
02:03:26,690 --> 02:03:35,380
so the convex function

1913
02:03:36,030 --> 02:03:39,980
in in are would simply be one where you take the points

1914
02:03:39,990 --> 02:03:44,660
take the line between them and the function values below that in general what you

1915
02:03:44,660 --> 02:03:46,660
will get is that

1916
02:03:48,060 --> 02:03:50,900
let's call the x x prime

1917
02:03:51,050 --> 02:03:53,480
let's assume that there is some land

1918
02:03:53,490 --> 02:03:56,340
and the said between zero and one

1919
02:03:56,460 --> 02:03:57,970
i have

1920
02:03:58,390 --> 02:04:00,090
lambda ix

1921
02:04:00,920 --> 02:04:02,890
one one lambda

1922
02:04:04,440 --> 02:04:06,440
it's less equal

1923
02:04:06,520 --> 02:04:08,040
then land

1924
02:04:08,110 --> 02:04:09,830
it will fix

1925
02:04:10,630 --> 02:04:13,120
one one lambda

1926
02:04:13,170 --> 02:04:16,060
at that point

1927
02:04:16,110 --> 02:04:19,950
now convex functions have a a whole bunch of really nice properties

1928
02:04:20,050 --> 02:04:23,130
by the way it's called strictly convex if this is in

1929
02:04:23,180 --> 02:04:24,710
if this is the strict

1930
02:04:24,760 --> 02:04:29,160
inequality for all and not being here and why

1931
02:04:29,810 --> 02:04:33,900
these functions have said the really nice properties amongst others

1932
02:04:33,950 --> 02:04:36,430
but if you want to minimize them

1933
02:04:36,480 --> 02:04:39,460
they all have a unique minimum value

1934
02:04:39,480 --> 02:04:44,210
provided that the domain in which i'm optimizing on its

1935
02:04:44,260 --> 02:04:46,080
the main is convex

1936
02:04:46,290 --> 02:04:48,280
it went on the main

1937
02:04:48,360 --> 02:04:51,370
if i can take any two points from line

1938
02:04:51,460 --> 02:04:52,410
like it

1939
02:04:52,420 --> 02:04:53,870
the next prime

1940
02:04:53,890 --> 02:04:55,740
and i draw a line between them

1941
02:04:55,780 --> 02:04:59,270
and the timeline is contained within the mine

1942
02:05:01,660 --> 02:05:07,640
to show that quite a constrained convex minimization problem has a unique minimum value

1943
02:05:07,730 --> 02:05:11,320
he will just go inside by contradiction

1944
02:05:11,320 --> 02:05:15,170
only with would say well let's assume that there are two local minima

1945
02:05:15,190 --> 02:05:18,440
x and x run by

1946
02:05:18,480 --> 02:05:19,480
and then you go

1947
02:05:19,490 --> 02:05:23,930
and look at the interpolation between the two points

1948
02:05:24,090 --> 02:05:27,880
that the interpolation between those two points after being inside the

1949
02:05:27,890 --> 02:05:30,940
it has to be a valid optimisation

1950
02:05:30,950 --> 02:05:34,170
secondly i know just like convexity

1951
02:05:34,180 --> 02:05:38,120
but any point on that interpolation is less equal the

1952
02:05:38,120 --> 02:05:41,170
a linear combination of the values of the two local minima

1953
02:05:42,120 --> 02:05:45,880
also less people than the smaller the

1954
02:05:47,040 --> 02:05:49,140
any of the points on that line

1955
02:05:49,210 --> 02:05:50,980
i will also be omitted

1956
02:05:51,030 --> 02:05:53,790
OK so get the contradiction

1957
02:05:53,840 --> 02:05:55,160
however no

1958
02:05:55,220 --> 02:05:59,410
that i just i was very careful talking about minimum value

1959
02:05:59,460 --> 02:06:01,260
not minimize

1960
02:06:01,260 --> 02:06:03,890
this is a convex function

1961
02:06:03,950 --> 02:06:06,590
and anyway here

1962
02:06:06,590 --> 02:06:09,790
i'm at a minimizer

1963
02:06:09,900 --> 02:06:15,480
it has a unique value but we end up i don't know

1964
02:06:15,530 --> 02:06:20,290
but this is a convex optimisation one one

1965
02:06:20,310 --> 02:06:23,520
leave out the bag is going to do a lot more of that

1966
02:06:23,870 --> 02:06:25,940
later in the week

1967
02:06:26,050 --> 02:06:29,090
so you have to believe again a couple of things

1968
02:06:29,110 --> 02:06:32,390
on face value

1969
02:06:32,480 --> 02:06:35,940
until he shows up and then he will teach a lot more about this should

1970
02:06:35,940 --> 02:06:38,440
help you understand what's going on here

1971
02:06:39,210 --> 02:06:41,320
so what we have is

1972
02:06:41,560 --> 02:06:43,470
a bunch of linear constraints

1973
02:06:43,530 --> 02:06:45,560
and those linear constraints

