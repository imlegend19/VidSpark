1
00:00:00,000 --> 00:00:03,880
all the weights the one

2
00:00:03,970 --> 00:00:06,070
now at every time step

3
00:00:06,130 --> 00:00:09,680
we observe our feature

4
00:00:09,690 --> 00:00:10,980
and then

5
00:00:10,990 --> 00:00:15,240
for every action will determine the probability

6
00:00:15,250 --> 00:00:19,280
of choosing that action in this time timestamp so this is where

7
00:00:19,300 --> 00:00:23,720
where forum our probability distribution over actions

8
00:00:23,760 --> 00:00:25,870
that we will use so

9
00:00:25,890 --> 00:00:27,280
each action

10
00:00:27,300 --> 00:00:30,310
i will have some minimum probability

11
00:00:30,320 --> 00:00:34,980
it was which it will get chosen so that distribute some

12
00:00:36,010 --> 00:00:37,870
probability mass

13
00:00:37,960 --> 00:00:44,430
but put it in every action and this is the remaining probability mass

14
00:00:44,440 --> 00:00:46,210
after with itself

15
00:00:47,550 --> 00:00:51,920
and this is how the remaining probability mass is distributed

16
00:00:51,960 --> 00:00:56,100
over the actions you just some the weights

17
00:00:56,130 --> 00:01:03,500
of the policies predicting that action and you normalized by the total sum of weights

18
00:01:03,500 --> 00:01:09,730
so to determine the probability of choosing action a is just some of the weights

19
00:01:09,750 --> 00:01:12,500
of all the policies predicting that action

20
00:01:12,510 --> 00:01:17,200
on this axis

21
00:01:17,210 --> 00:01:23,200
and you just normalize but it's it's it's pretty natural you choose action

22
00:01:23,240 --> 00:01:25,510
proportionally to

23
00:01:25,590 --> 00:01:29,150
the the weight the total weight

24
00:01:29,210 --> 00:01:36,760
on policies choose in that action

25
00:01:36,770 --> 00:01:40,450
but there is some probability mass that you put the site

26
00:01:40,460 --> 00:01:44,520
and distribute randomly over all the actions

27
00:01:44,570 --> 00:01:50,490
so that every action there is some minimum probability with which each action is chosen

28
00:01:50,540 --> 00:01:53,040
and this technicalities needed for

29
00:01:53,100 --> 00:01:58,420
the proof to go through so you need some minimum probability that

30
00:01:58,460 --> 00:02:00,230
every action gets

31
00:02:01,230 --> 00:02:06,380
and it turns out that the optimal value of this minimum probability skills was something

32
00:02:06,380 --> 00:02:11,420
like one over root key

33
00:02:11,490 --> 00:02:12,560
and there's

34
00:02:12,630 --> 00:02:18,540
this log of the number of policies again

35
00:02:18,600 --> 00:02:25,150
OK so what did you formed your distribution of actions and you draw an action

36
00:02:25,170 --> 00:02:26,810
on that distribution

37
00:02:26,830 --> 00:02:29,300
you observe the war it

38
00:02:29,350 --> 00:02:32,850
and now you update the weights

39
00:02:32,900 --> 00:02:34,720
so that they can be used

40
00:02:34,720 --> 00:02:39,780
in the next round to be the probability of actions and you do that by

41
00:02:39,780 --> 00:02:41,270
four men

42
00:02:41,280 --> 00:02:45,220
an unbiased estimate of the war

43
00:02:46,120 --> 00:02:48,680
if to update the weight

44
00:02:48,680 --> 00:02:51,610
i have a given policy pi

45
00:02:51,640 --> 00:02:53,390
if the policy

46
00:02:53,400 --> 00:02:57,170
doesn't agree with the the action that was actually taken

47
00:02:57,240 --> 00:03:02,310
then you do not have the the weight the weight remains the same

48
00:03:02,340 --> 00:03:04,770
if you do that

49
00:03:04,810 --> 00:03:08,930
if pi agrees with the action taken then you multiply

50
00:03:09,000 --> 00:03:11,990
the the weight by

51
00:03:12,030 --> 00:03:15,110
an exponential function of

52
00:03:16,120 --> 00:03:17,220
this is the

53
00:03:17,230 --> 00:03:21,540
estimate of the world trade to divide the reward by the probability of taking that

54
00:03:21,540 --> 00:03:23,490
action where

55
00:03:23,550 --> 00:03:27,010
the probability comes from here

56
00:03:27,010 --> 00:03:32,270
right and we multiply by the this we can think about is the learning rate

57
00:03:35,440 --> 00:03:37,510
and the importance of

58
00:03:37,580 --> 00:03:40,420
multiplying by the exponential is that

59
00:03:40,420 --> 00:03:42,400
you can say

60
00:03:42,460 --> 00:03:45,900
when you multiply the exponentials you get some

61
00:03:46,740 --> 00:03:50,300
in the exponent over the rounds when you

62
00:03:50,360 --> 00:03:55,290
look at the some of you can look at the

63
00:03:55,480 --> 00:04:01,540
some of these quantities so with around

64
00:04:01,550 --> 00:04:19,540
when you multiply

65
00:04:19,580 --> 00:04:23,430
so this algorithm gives there are no assumptions about

66
00:04:23,430 --> 00:04:27,470
about how axis and are is a generated

67
00:04:27,520 --> 00:04:33,580
so they can be adversarially chosen

68
00:04:33,630 --> 00:04:37,060
and you can still compete in a very strong sense

69
00:04:37,070 --> 00:04:40,420
so if you really good at will be will scale as

70
00:04:40,420 --> 00:04:42,180
square root of t

71
00:04:42,190 --> 00:04:44,420
where is your time horizon

72
00:04:44,500 --> 00:04:47,480
and you don't need to know t here

73
00:04:47,500 --> 00:04:49,770
so this is a very good of

74
00:04:49,820 --> 00:04:53,820
it sounds

75
00:04:53,840 --> 00:05:01,750
right so that's the theorem associated with that

76
00:05:01,750 --> 00:05:09,920
and there's a matching lower bound

77
00:05:10,210 --> 00:05:13,050
and so this is the bound

78
00:05:13,070 --> 00:05:16,400
on the expected regret

79
00:05:16,400 --> 00:05:18,250
and it can be modified

80
00:05:18,270 --> 00:05:23,170
on their way to get the the algorithm can be modified to give a high

81
00:05:23,170 --> 00:05:25,910
the vic's over px i just turned

82
00:05:25,920 --> 00:05:31,110
the p and q upside down and again in the minus sign

83
00:05:31,130 --> 00:05:36,030
so if you want to let me call this object

84
00:05:37,640 --> 00:05:40,310
so it's a function of x

85
00:05:40,320 --> 00:05:42,770
it's a new random variable y

86
00:05:42,870 --> 00:05:44,690
so what i have is

87
00:05:44,700 --> 00:05:48,410
expectation of the negative logarithm of y

88
00:05:48,720 --> 00:05:53,850
the negative logarithm is something logarithm looks like this is the negative logarithm looks like

89
00:05:53,850 --> 00:05:55,130
that so it's com

90
00:05:56,220 --> 00:05:58,930
this guy is a convex function of y

91
00:05:58,950 --> 00:06:01,670
this will be greater equal to

92
00:06:01,720 --> 00:06:04,220
minus log

93
00:06:04,280 --> 00:06:06,000
expectation of y

94
00:06:06,010 --> 00:06:10,660
which is the same as minus log

95
00:06:10,680 --> 00:06:12,240
some over x

96
00:06:12,260 --> 00:06:16,970
p of x which was why again it was q over p

97
00:06:16,980 --> 00:06:21,860
so we got

98
00:06:21,900 --> 00:06:27,120
two pieces here we cancel it we get some over x q of x q

99
00:06:27,120 --> 00:06:33,520
is also probability so it sums up to one so it's equal to minus log

100
00:06:33,540 --> 00:06:36,200
some q of x x

101
00:06:36,200 --> 00:06:40,960
this is equal to one one log of one is zero so we get a

102
00:06:40,960 --> 00:06:44,790
zero is i've just shown greater or equal to zero so this is the only

103
00:06:44,790 --> 00:06:47,530
thing that i need for the moment for the proof

104
00:06:47,900 --> 00:06:50,990
so what i construct now is the following things

105
00:06:51,070 --> 00:06:53,140
so i construct

106
00:06:55,180 --> 00:06:57,290
the following object

107
00:06:57,310 --> 00:07:00,390
so i just take any q of x

108
00:07:00,400 --> 00:07:03,460
and i consider the p

109
00:07:03,470 --> 00:07:07,600
the probability of the hidden variables given the observation y

110
00:07:07,720 --> 00:07:12,470
so now i introduces the definition of p of x

111
00:07:13,510 --> 00:07:16,020
p of x given y is

112
00:07:16,040 --> 00:07:19,490
p of y common x

113
00:07:20,250 --> 00:07:23,950
divided by

114
00:07:25,520 --> 00:07:27,600
of y

115
00:07:28,260 --> 00:07:31,950
right that's the definition of conditional

116
00:07:31,950 --> 00:07:36,680
so if i put this into here and rearrange this

117
00:07:36,690 --> 00:07:39,430
i get this inequality

118
00:07:39,450 --> 00:07:42,130
so i just have to plug this in

119
00:07:44,260 --> 00:07:48,180
right turn this the other way around

120
00:07:48,190 --> 00:07:50,980
and again and minus

121
00:07:51,010 --> 00:07:53,460
this gets here

122
00:07:53,460 --> 00:07:55,550
and was just

123
00:07:55,570 --> 00:07:57,410
insert this

124
00:07:57,460 --> 00:08:01,550
into this inequality you get a new inequality

125
00:08:03,230 --> 00:08:06,890
for fixed data the right is actually

126
00:08:07,360 --> 00:08:12,490
the biggest value for q

127
00:08:12,510 --> 00:08:14,480
it is actually this

128
00:08:14,490 --> 00:08:17,810
because if q is equal to p i get zero here and there is no

129
00:08:17,810 --> 00:08:23,430
longer an inequality anymore i to the best you can have is the one where

130
00:08:23,430 --> 00:08:26,220
q is equal to this

131
00:08:27,480 --> 00:08:28,860
let me see

132
00:08:31,080 --> 00:08:38,150
so what we're not doing that so the first thing was in the e step

133
00:08:38,160 --> 00:08:40,040
of the

134
00:08:40,600 --> 00:08:43,020
go back to

135
00:08:43,060 --> 00:08:46,940
to the EM algorithm here

136
00:08:46,940 --> 00:08:48,450
first of all

137
00:08:48,460 --> 00:08:54,120
what i need in the in algorithm is this quantity is the probability of the

138
00:08:54,120 --> 00:08:58,310
heaven hidden variables given y and the previous parameters

139
00:08:58,420 --> 00:09:03,630
how do i get that out of an optimisation procedure i get it out of

140
00:09:03,630 --> 00:09:05,800
an optimisation procedure

141
00:09:05,820 --> 00:09:12,430
when i form this quantity foreign this quantity look here

142
00:09:12,440 --> 00:09:15,890
this quantity and ask what is

143
00:09:15,910 --> 00:09:21,560
the key of x which makes this object maximum

144
00:09:23,330 --> 00:09:26,290
and the object that makes this thing and maximum

145
00:09:27,130 --> 00:09:28,510
precisely this

146
00:09:28,540 --> 00:09:33,360
so i take this p of x which is the maximizer of that

147
00:09:33,450 --> 00:09:35,660
in the second step

148
00:09:35,680 --> 00:09:39,210
i'll take this

149
00:09:41,160 --> 00:09:47,110
so q of x

150
00:09:47,130 --> 00:09:54,500
so the best q of x is p t of x given y and the

151
00:09:54,500 --> 00:09:59,510
previous parameter so this is qt called this qt

152
00:10:01,870 --> 00:10:05,950
in the next step if

153
00:10:05,970 --> 00:10:08,510
so if i rewrite this quantity

154
00:10:08,680 --> 00:10:11,200
it's a bit

155
00:10:11,230 --> 00:10:16,280
messy so i have a q log q that this part of it

156
00:10:16,290 --> 00:10:18,030
and i have a q

157
00:10:18,950 --> 00:10:21,940
log p and that is precisely

158
00:10:21,950 --> 00:10:23,500
the definitions

159
00:10:26,100 --> 00:10:27,210
after this

160
00:10:27,220 --> 00:10:30,790
this thing

161
00:10:30,810 --> 00:10:33,170
so what i have

162
00:10:33,210 --> 00:10:38,110
it can be DEM algorithm can be viewed as doing the following things

163
00:10:38,190 --> 00:10:40,060
well just

164
00:10:40,190 --> 00:10:43,690
find q which maximizes this one

165
00:10:43,740 --> 00:10:47,440
and then i find the parameters theta

166
00:10:48,280 --> 00:10:50,220
maximize is this one

167
00:10:51,810 --> 00:10:54,400
it seems like this is

168
00:10:54,400 --> 00:10:59,310
this object is the one that is always maximized so i take an old data

169
00:11:00,050 --> 00:11:02,720
maximizes with respect to q

170
00:11:03,150 --> 00:11:04,680
and if i have

171
00:11:04,710 --> 00:11:10,380
maximized with respect to q then i keep this t free this this data free

172
00:11:10,380 --> 00:11:13,860
and replies it again by by one

173
00:11:13,870 --> 00:11:19,120
that that maximizes the sick and say this is the object of relevance i optimise

174
00:11:20,400 --> 00:11:25,810
for fixed that even i keep q fixed and then i optimize theta against life

175
00:11:25,810 --> 00:11:28,060
cycle between the two things

176
00:11:28,210 --> 00:11:31,390
right so

177
00:11:31,440 --> 00:11:36,030
then i can show that actually in this way

178
00:11:39,710 --> 00:11:44,270
decreasing the log likelihood

179
00:11:44,300 --> 00:11:47,320
but i'm always going in the right direction

180
00:11:48,810 --> 00:11:51,920
so let me see

181
00:11:51,930 --> 00:11:53,370
do we have that

182
00:11:53,550 --> 00:12:00,130
right so

183
00:12:00,140 --> 00:12:02,520
we start from this

184
00:12:02,610 --> 00:12:04,750
we identify

185
00:12:04,810 --> 00:12:06,720
this debate

186
00:12:07,150 --> 00:12:09,260
is that

187
00:12:09,280 --> 00:12:11,710
and so we get a abound

188
00:12:13,520 --> 00:12:17,020
so of this is what we actually

189
00:12:17,110 --> 00:12:20,210
i would like to maximize so we can show

190
00:12:20,390 --> 00:12:24,400
this for anything it is this period equal to that

191
00:12:24,550 --> 00:12:26,280
we also know

192
00:12:27,580 --> 00:12:33,150
if i said a to be equal to the old parameter i get an the

193
00:12:33,150 --> 00:12:35,610
quality here

194
00:12:35,650 --> 00:12:38,100
so that follows from

195
00:12:43,190 --> 00:12:46,040
this line here

196
00:12:46,100 --> 00:12:47,700
that if

197
00:12:47,750 --> 00:12:50,110
if i put in qt year

198
00:12:50,140 --> 00:12:52,150
then i would get

199
00:12:52,200 --> 00:12:54,270
and quality

200
00:12:54,290 --> 00:12:56,220
so what i have

201
00:12:56,260 --> 00:12:59,070
it is these two things

202
00:12:59,070 --> 00:13:02,550
one of the many many articles

203
00:13:43,690 --> 00:13:47,820
here we

204
00:13:53,700 --> 00:13:56,070
in the

205
00:14:15,310 --> 00:14:18,500
we to do

206
00:14:38,320 --> 00:14:43,210
running gag on

207
00:16:15,160 --> 00:16:33,770
thank you

208
00:17:01,750 --> 00:17:07,110
in here

209
00:17:27,750 --> 00:17:28,610
this used

210
00:17:39,070 --> 00:17:44,310
that kind of thing

211
00:18:19,550 --> 00:18:27,100
and why

212
00:18:27,180 --> 00:18:29,810
we i

213
00:18:34,560 --> 00:18:37,190
if you

214
00:18:41,640 --> 00:18:43,670
you are

215
00:19:03,250 --> 00:19:07,940
you are

216
00:19:07,970 --> 00:19:12,450
that are

217
00:19:20,030 --> 00:19:28,870
i are

218
00:19:28,870 --> 00:19:32,770
the point is the pair xy

219
00:19:35,490 --> 00:19:38,400
there is a joint distribution over x and you

220
00:19:38,460 --> 00:19:41,860
which has the following conditionals

221
00:19:42,770 --> 00:19:47,170
the position x the height is just uniform between zero and the height of the

222
00:19:47,170 --> 00:19:51,260
case that this conditional distribution here

223
00:19:51,270 --> 00:19:54,180
and then given the height

224
00:19:54,200 --> 00:19:55,430
the conditional

225
00:19:55,440 --> 00:20:00,500
distribution over is well we've got to be underneath the car

226
00:20:00,550 --> 00:20:06,000
but otherwise the distribution is uniform these two conditional distributions very simple

227
00:20:06,040 --> 00:20:07,280
so in principle

228
00:20:07,290 --> 00:20:10,780
you could maybe imagine gibbs sampling from these distributions in

229
00:20:10,840 --> 00:20:12,560
you take a position

230
00:20:12,610 --> 00:20:13,430
and you

231
00:20:13,430 --> 00:20:14,690
randomly drawn height

232
00:20:14,700 --> 00:20:16,070
and then given height

233
00:20:16,090 --> 00:20:17,940
you randomly drawn position

234
00:20:17,950 --> 00:20:21,540
and then you randomly drawn height and in position and then any high

235
00:20:21,590 --> 00:20:24,830
and like that you would wonder slowly around underneath the car

236
00:20:27,550 --> 00:20:30,020
what is achieved

237
00:20:30,030 --> 00:20:35,960
the question is exactly how we going to implement these updates

238
00:20:35,970 --> 00:20:38,010
let's first see it on

239
00:20:38,060 --> 00:20:41,210
you know metalcap so often we know that

240
00:20:41,210 --> 00:20:46,610
probability distributions that we need to sample from the in the model that sounds like

241
00:20:46,750 --> 00:20:51,690
like an easy problem but remember that we might just be updating one variable

242
00:20:51,690 --> 00:20:55,130
as part of the system and conditional on everything else one variable often has been

243
00:20:55,430 --> 00:20:57,020
you might all distribution

244
00:20:57,030 --> 00:21:01,480
so this would just be one update of a markov chain

245
00:21:03,190 --> 00:21:06,390
the update where we sample height is easy because we can evaluate the height of

246
00:21:06,400 --> 00:21:10,270
the curve so we just or uniform number between there and the height

247
00:21:10,280 --> 00:21:13,330
the update to the position is harder

248
00:21:13,340 --> 00:21:16,450
because it's difficult to analytically workout

249
00:21:17,570 --> 00:21:20,880
this lies underneath the curve is

250
00:21:21,970 --> 00:21:24,920
what you can do for you mail system is sample

251
00:21:25,920 --> 00:21:28,900
the line underneath the curve by rejection sampling

252
00:21:29,700 --> 00:21:31,960
you sample from the bigger distribution

253
00:21:32,010 --> 00:21:36,310
uniform distribution over bracket which you could find some sort of bisections that which is

254
00:21:36,310 --> 00:21:39,850
guaranteed to think out outside the extent the curve

255
00:21:39,880 --> 00:21:44,640
the sample from that and if you happen to stumble outside the region where you

256
00:21:44,640 --> 00:21:46,200
just get sample

257
00:21:46,230 --> 00:21:48,880
you are also able to shrink bracket which is the detail

258
00:21:48,970 --> 00:21:52,400
and the sample again until you get a new point into the web projection something

259
00:21:52,400 --> 00:21:55,710
to me from here to here on the curve

260
00:21:58,280 --> 00:21:59,960
compare this to

261
00:21:59,980 --> 00:22:03,350
based metropolis sampling gibson

262
00:22:03,380 --> 00:22:07,030
trouble is something we had to take a step with so that we knew

263
00:22:07,030 --> 00:22:09,980
how far we should propose moving it was nice but which here

264
00:22:09,990 --> 00:22:14,960
we've automatically sort of considered the whole width of the distribution

265
00:22:15,040 --> 00:22:19,890
it's something didn't have that we could also sample from this conditional distribution but we

266
00:22:19,890 --> 00:22:23,800
had we would have to have analytically worked out that conditional distribution or constructed the

267
00:22:23,820 --> 00:22:24,780
sample for

268
00:22:25,650 --> 00:22:29,500
all we have to deal with evaluate the curve up to a constant

269
00:22:29,550 --> 00:22:33,250
so this is again something where you can write to really generic routinely to

270
00:22:33,260 --> 00:22:35,550
give me a function handle it evaluates you're

271
00:22:35,580 --> 00:22:38,750
thank you for your probability up to constants and i'll just

272
00:22:50,810 --> 00:22:55,500
question the question is how can we construct the rejection from will sample from this

273
00:22:55,500 --> 00:22:57,640
bracket so it's important to understand

274
00:22:57,680 --> 00:23:00,830
we i see them

275
00:23:00,850 --> 00:23:08,680
that you have selected point thank you say

276
00:23:08,710 --> 00:23:13,320
what you need to do is constructed bracket with the ends are above the curve

277
00:23:13,340 --> 00:23:17,170
so you start off with some brackets around the point they it's not wide enough

278
00:23:17,170 --> 00:23:20,820
certainly from here to here and you notice at the ends of your putative bracket

279
00:23:20,820 --> 00:23:24,050
are underneath the cabin you could do that but just evaluating the cat at this

280
00:23:24,050 --> 00:23:25,370
particular point

281
00:23:25,390 --> 00:23:27,570
so then you just double the size

282
00:23:27,650 --> 00:23:31,240
and then you evaluate the endpoints again and you keep doubling the size the bracket

283
00:23:32,170 --> 00:23:37,370
it six outside that's one algorithm you can imagine many more sophisticated way the search

284
00:23:37,420 --> 00:23:40,710
but is this thank you very much that just involves

285
00:23:40,720 --> 00:23:46,620
evaluating the function at point

286
00:23:46,640 --> 00:23:50,360
that doesn't work if the distributions multimodal of course a

287
00:23:50,370 --> 00:23:53,200
here's an example of constructing the bracket where

288
00:23:53,200 --> 00:23:57,740
we started with a brackets around that point which we slow down randomly just we

289
00:23:57,750 --> 00:24:02,840
picked somewhere and we put them randomly around point and then this and the bracket

290
00:24:02,860 --> 00:24:04,800
the left hand wasn't

291
00:24:04,850 --> 00:24:08,490
above the curve and so we extended the length of the bracket

292
00:24:08,490 --> 00:24:11,960
and it's not because it's the back again now it is so i stopped

293
00:24:12,010 --> 00:24:16,100
so now i've constructed bracket which doesn't cover the whole region that we need something

294
00:24:16,130 --> 00:24:20,680
that we want to sample from the gibbs sampling because we've ignored this region here

295
00:24:20,700 --> 00:24:23,770
what's needed it turns out that

296
00:24:23,780 --> 00:24:25,630
you can do this anyway

297
00:24:25,670 --> 00:24:27,210
and it's a valid

298
00:24:27,900 --> 00:24:31,290
this is the chain which isn't good to get some work is not able to

299
00:24:31,290 --> 00:24:32,280
move over here

300
00:24:32,300 --> 00:24:36,040
but it still is the stationary distribution environment and say

301
00:24:36,060 --> 00:24:41,060
the introduction of a single height meant that we've just got a bunch of simple

302
00:24:41,060 --> 00:24:44,150
rules that allow us to make decisions about moving around

303
00:24:44,180 --> 00:24:48,790
without having to do any computation

304
00:24:49,640 --> 00:24:51,130
there is that we're here

305
00:24:51,150 --> 00:24:54,490
so width of the bracket but i had to pick before that i will just

306
00:24:54,490 --> 00:24:57,390
do something exponentially grace and

307
00:24:57,440 --> 00:24:59,400
that will be cheap so a

308
00:24:59,430 --> 00:25:02,860
you could worry that i've now had to evaluate the function

309
00:25:02,920 --> 00:25:06,000
at several places that could be costly

310
00:25:07,010 --> 00:25:13,000
on the other hand in order to extend the length that were looking at this

311
00:25:13,000 --> 00:25:15,750
is the linear cost so far bracket

312
00:25:16,750 --> 00:25:24,310
it is sort of the typical length scale of functions was out and we have

313
00:25:24,310 --> 00:25:28,960
a bracketed with the mass then in order to expand bracket out so that it

314
00:25:28,980 --> 00:25:34,960
covers length l l over sigma when we compare that to the diffusion on top

315
00:25:34,960 --> 00:25:36,690
and that was the square

316
00:25:38,170 --> 00:25:42,300
thanks it if you get that size wrong in place are not going to matter

317
00:25:42,320 --> 00:25:44,340
nearly as much as it does metropolis

318
00:25:44,360 --> 00:25:47,550
this is a much more sort of robust algorithm for just routine news if you

319
00:25:47,550 --> 00:25:48,670
don't want to

320
00:25:48,800 --> 00:25:51,780
we too careful about setting up three parameters

321
00:25:51,880 --> 00:25:56,450
when i noticed that when i run the practical on wednesday for if few if

322
00:25:56,450 --> 00:26:00,300
you want to do it would probably encourage you to use the variant like something

323
00:26:00,300 --> 00:26:04,480
because it was probably just be a lot easier to get working on anything else

324
00:26:07,460 --> 00:26:08,300
is there

325
00:26:15,820 --> 00:26:17,980
is one of the

326
00:26:17,980 --> 00:26:19,220
we can go on so

327
00:26:19,230 --> 00:26:25,470
here we see a very funny phenomenon this figure is again doesn't work but then

328
00:26:25,740 --> 00:26:26,530
the next

329
00:26:26,620 --> 00:26:31,410
on the next slides almost identical figures somehow starts to where can i checked that

330
00:26:31,410 --> 00:26:36,530
all figures after this quite seems to be quite OK i don't understand what was

331
00:26:36,600 --> 00:26:42,020
for this reason for the first sixteen slice of it's like wrong

332
00:26:42,040 --> 00:26:43,180
sixteen is

333
00:26:43,200 --> 00:26:47,890
the power of four that must have something to do with OK but so this

334
00:26:47,890 --> 00:26:51,780
is just this is not a simple numerical illustration of fact

335
00:26:51,800 --> 00:26:52,770
so we have

336
00:26:54,260 --> 00:27:01,550
data distributed according to a uniform distribution so the independent components are uniformly distributed and

337
00:27:01,550 --> 00:27:03,040
then we have here

338
00:27:03,050 --> 00:27:04,820
mixed data

339
00:27:04,840 --> 00:27:09,660
that has been widened so this is a scatter plot of the exterior and some

340
00:27:09,660 --> 00:27:12,630
others and all this circles here denotes the unit sphere

341
00:27:12,650 --> 00:27:17,130
and these the line here this is the idea of this line here is to

342
00:27:17,130 --> 00:27:22,100
show that well now we have this w that we are trying to optimize and

343
00:27:22,130 --> 00:27:24,820
w can take any value on the unit sphere

344
00:27:24,880 --> 00:27:28,050
which in this case simply means that can you know

345
00:27:30,480 --> 00:27:35,190
which means that we can the capabilities as far west as the function of these

346
00:27:35,190 --> 00:27:37,050
of the angle of w

347
00:27:37,290 --> 00:27:40,560
and we get this simple plot so ketosis

348
00:27:40,940 --> 00:27:44,390
goes down and up and go up and down now point to note is that

349
00:27:44,740 --> 00:27:47,570
since a uniform distribution has

350
00:27:47,760 --> 00:27:54,950
negative kurtosis all these mixtures also have negative kurtosis and when maximizing the absolute value

351
00:27:54,950 --> 00:27:58,740
of kurtosis we actually maximizing it here

352
00:27:58,780 --> 00:28:01,510
on these two points

353
00:28:01,520 --> 00:28:03,350
and how well it is not

354
00:28:03,400 --> 00:28:10,630
not very obvious but well the point is that these two quantities to anglesey correspond

355
00:28:10,630 --> 00:28:16,070
to the directions of the independent components that is when w is either pointing in

356
00:28:16,070 --> 00:28:18,540
this direction

357
00:28:18,560 --> 00:28:21,050
all in this direction

358
00:28:21,070 --> 00:28:23,460
that is the directions of

359
00:28:23,490 --> 00:28:25,340
given by this

360
00:28:25,380 --> 00:28:29,110
square box

361
00:28:29,130 --> 00:28:29,930
OK so

362
00:28:29,940 --> 00:28:36,590
then this leads us to obey the basic ICA estimation procedure so first we have

363
00:28:36,610 --> 00:28:40,380
the data x and we whiten the data well i did the whitened data by

364
00:28:40,380 --> 00:28:43,560
by by z for some sort of summary

365
00:28:43,580 --> 00:28:46,820
and now what we do is some kind of is in the best of the

366
00:28:46,820 --> 00:28:52,600
basic case we can do kind of recursive estimation or iterative estimation of these i

367
00:28:52,600 --> 00:28:57,420
mean i mean deflationary estimation and some people call it of the independent components so

368
00:28:57,420 --> 00:29:02,560
let's be iteration come one then we take a random vector victor w

369
00:29:02,630 --> 00:29:07,820
and then we use some kind of maximisation procedure for example in the simplest case

370
00:29:07,950 --> 00:29:12,660
gradient procedure which is rather simple to derive we can we can rather easily compute

371
00:29:12,660 --> 00:29:14,570
the gradient of kurtosis

372
00:29:14,590 --> 00:29:17,620
so so we can maxmin maximize the case the

373
00:29:17,860 --> 00:29:24,040
nongaussianity using some method under the constraint that the norm of w equals one

374
00:29:24,050 --> 00:29:31,320
because for whitened data this constraints unit constant of unit norm equals the constraint of

375
00:29:31,330 --> 00:29:33,160
unit makes

376
00:29:33,760 --> 00:29:38,760
and so what we have competed one of those those components that we increment iteration

377
00:29:38,760 --> 00:29:42,670
count by one and go back here we take another random vector and we do

378
00:29:42,680 --> 00:29:47,530
just the same thing but now this time we constrain the new vector so that

379
00:29:47,600 --> 00:29:48,690
is always

380
00:29:49,360 --> 00:29:55,190
also going so those vectors that have been previously found because of these because the

381
00:29:55,200 --> 00:30:01,380
mixing matrix a is orthogonal and and there is an optimum these W's will eventually

382
00:30:01,380 --> 00:30:07,910
converge to the right to the columns of the mixing matrix

383
00:30:07,930 --> 00:30:12,660
and so we do this in times square when we have an independent components and

384
00:30:12,660 --> 00:30:14,450
dimensions so we

385
00:30:14,460 --> 00:30:18,110
we assume here that number of the dimension of the data is equal to the

386
00:30:18,110 --> 00:30:19,840
number of independent components

387
00:30:19,890 --> 00:30:23,480
that's what we have been saying all the time for simplicity so we do this

388
00:30:23,480 --> 00:30:27,600
and times and then gets the independent components it's basically

389
00:30:29,300 --> 00:30:36,800
because there is there is there are many different variants of this basic method well

390
00:30:36,800 --> 00:30:41,290
the first thing is that we don't need to like first compute one independent component

391
00:30:41,290 --> 00:30:46,870
in another one and another one we can like it somehow maximize in parallel have

392
00:30:46,900 --> 00:30:54,530
maximize nongaussianity is of of combinations given by many different w eyes and then keep

393
00:30:54,530 --> 00:30:57,930
them you just thank you expect inspectors also

394
00:30:57,950 --> 00:31:01,850
in some manner

395
00:31:01,870 --> 00:31:05,870
OK so now you basically know how to to do i see a

396
00:31:05,890 --> 00:31:08,090
you just need to compute the gradient

397
00:31:08,110 --> 00:31:10,600
of kurtosis that's the main thing

398
00:31:10,610 --> 00:31:14,320
and then you can do this

399
00:31:14,330 --> 00:31:18,320
OK but that is certainly not the end of the story

400
00:31:18,340 --> 00:31:24,230
this is really the basic way of estimating ICA also one one that is well

401
00:31:24,530 --> 00:31:27,190
mathematical simple to analyse and therefore

402
00:31:27,200 --> 00:31:30,350
it is easy to understand

403
00:31:30,360 --> 00:31:31,560
it is

404
00:31:31,570 --> 00:31:37,510
well this is it's not that this is something that is never used in practice

405
00:31:37,510 --> 00:31:41,270
with the scale the methods that are really the same that similar to what i

406
00:31:41,270 --> 00:31:46,750
just showed that i used and have been used with success but they are not

407
00:31:46,750 --> 00:31:48,760
the optimal methods for many

408
00:31:48,800 --> 00:31:55,210
for many situations well classical problem with kurtosis and any of these higher order cumulants

409
00:31:55,210 --> 00:31:56,950
least be

410
00:31:56,960 --> 00:32:00,620
OK so the column player can get the loss of at most p and the

411
00:32:00,620 --> 00:32:02,970
row player can get lost spent most the

412
00:32:02,990 --> 00:32:06,640
and the column player can always force lost about least p

413
00:32:06,660 --> 00:32:10,850
so that's saying that p star is optimal for the row player

414
00:32:10,860 --> 00:32:13,220
thank you star is optimal for the

415
00:32:13,230 --> 00:32:17,200
max player for the column player

416
00:32:17,220 --> 00:32:18,280
OK so

417
00:32:18,290 --> 00:32:22,270
OK so so far i haven't said anything about boosting

418
00:32:22,290 --> 00:32:25,790
but boosting like i said it turns out to be a game and what we're

419
00:32:25,790 --> 00:32:31,100
going to see is that boosting its the very notion of boosting is very closely

420
00:32:31,100 --> 00:32:36,580
connected to the minimax theorem so it's just intimately connected to game theory

421
00:32:36,600 --> 00:32:40,350
so what we're going to do is we're going to set up a matrix game

422
00:32:40,360 --> 00:32:44,200
and that matrix is the game that's being played

423
00:32:44,210 --> 00:32:47,810
when an algorithm like adaboost is run

424
00:32:47,990 --> 00:32:51,730
so the booster remember there are two players the boost the weak learners

425
00:32:51,760 --> 00:32:55,290
so the booster will be identified with the row player of this game

426
00:32:55,300 --> 00:33:00,160
the weak learner is identified with the column player

427
00:33:00,220 --> 00:33:03,050
and now we can construct this matrix

428
00:33:03,600 --> 00:33:05,410
so first of all

429
00:33:05,410 --> 00:33:06,710
for most of this

430
00:33:06,970 --> 00:33:08,930
part of the tutorial

431
00:33:08,960 --> 00:33:13,640
i'm going to imagine that we can list all the possible weak classifiers

432
00:33:13,650 --> 00:33:18,640
all of them so this is not just the weak classifiers which are actually returned

433
00:33:18,640 --> 00:33:22,830
by the weak learning algorithm on a particular run of the algorithm this is the

434
00:33:22,830 --> 00:33:25,790
space of all possible weak classifiers

435
00:33:25,810 --> 00:33:28,670
so for using say c four point five

436
00:33:28,700 --> 00:33:30,410
as our weak learning algorithm

437
00:33:30,430 --> 00:33:31,330
this would be

438
00:33:31,370 --> 00:33:36,270
the space of all possible decision trees all decision trees

439
00:33:37,070 --> 00:33:41,380
this is a very very large set of

440
00:33:42,990 --> 00:33:48,040
in general it will typically be in well not typically better will often be infinite

441
00:33:48,060 --> 00:33:50,820
here i'm assuming that

442
00:33:50,840 --> 00:33:52,610
it's finite

443
00:33:52,620 --> 00:33:55,000
a finite but extremely large

444
00:33:55,030 --> 00:34:00,380
OK so g one through g and is the space of all possible weak classifiers

445
00:34:00,390 --> 00:34:04,510
and now we can describe the matrix which is the matrix game which is being

446
00:34:04,510 --> 00:34:06,580
played by these two players

447
00:34:06,610 --> 00:34:08,370
so the rose

448
00:34:08,410 --> 00:34:11,780
will be identified with training examples

449
00:34:11,790 --> 00:34:14,360
so there and training examples

450
00:34:14,370 --> 00:34:15,660
in the i th row

451
00:34:15,670 --> 00:34:19,890
is identified with the i th training example

452
00:34:20,080 --> 00:34:24,930
the columns of this matrix are identified with these weak classifiers

453
00:34:25,000 --> 00:34:31,700
so there an an weak classifiers in this entire space again and is gigantic number

454
00:34:31,700 --> 00:34:33,530
gigantic but finite

455
00:34:33,570 --> 00:34:35,680
so this matrix is huge

456
00:34:35,700 --> 00:34:38,860
has one column for each week classifiers

457
00:34:39,010 --> 00:34:43,080
the j th column corresponds to the j weak classifier

458
00:34:43,110 --> 00:34:44,660
now we need to say

459
00:34:44,670 --> 00:34:46,830
with the values of this matrix are

460
00:34:46,840 --> 00:34:50,840
so these values are just something very simple and natural

461
00:34:50,900 --> 00:34:54,320
so remember rose corresponds to an example

462
00:34:54,330 --> 00:34:57,520
in column j corresponds to weak classifiers

463
00:34:57,560 --> 00:35:02,450
so we'll just put a one for that entry of the matrix if example i

464
00:35:02,450 --> 00:35:06,330
is correctly classified by the g weak classifiers

465
00:35:06,340 --> 00:35:09,000
and we'll put is zero otherwise

466
00:35:09,010 --> 00:35:12,330
so this is just a huge giant encoding

467
00:35:12,390 --> 00:35:15,240
of which we classifiers in our space

468
00:35:15,260 --> 00:35:20,050
are correct and which examples it's just specifying which exhibit which

469
00:35:20,070 --> 00:35:23,530
weak classifiers are correct and which examples

470
00:35:23,580 --> 00:35:26,020
when i claiming is that

471
00:35:26,030 --> 00:35:30,400
the boosting process is actually playing this game

472
00:35:32,940 --> 00:35:35,640
so before we get there

473
00:35:35,660 --> 00:35:38,700
what happens if we take the minimax theorem

474
00:35:38,760 --> 00:35:44,510
and we apply it to this particular game the minmax theorem applies to any matrix

475
00:35:44,550 --> 00:35:49,080
any game so what happens if we applied to this game

476
00:35:49,110 --> 00:35:54,340
well we get something which is very closely related to boosting

477
00:35:54,370 --> 00:35:58,080
so let's go back to the weak learning assumption or the gamma weak learning assumption

478
00:35:58,080 --> 00:36:00,450
which is the starting point for boosting

479
00:36:00,460 --> 00:36:06,290
gamma weak learning assumption says that for every distribution over examples

480
00:36:06,290 --> 00:36:09,920
there exists a weak classifiers

481
00:36:10,010 --> 00:36:14,790
so that the accuracy that we classifiers at least have close gamma so in other

482
00:36:14,790 --> 00:36:18,700
words if you choose an example of random according to p

483
00:36:18,750 --> 00:36:23,200
the probability that you get an example that's correctly classified by gj is at least

484
00:36:23,200 --> 00:36:24,540
the hapless gamma

485
00:36:24,550 --> 00:36:28,460
that's what the weak learning assumption says it says the weak learning algorithm for any

486
00:36:28,460 --> 00:36:33,000
distribution is able to find a weak classifier with that property

487
00:36:33,010 --> 00:36:36,010
with accuracy at least hapless camera

488
00:36:36,030 --> 00:36:38,530
so what does that mean in terms of this game

489
00:36:40,040 --> 00:36:44,420
the fact that it's for all distribution gets replaced by millions were taking the men

490
00:36:44,430 --> 00:36:46,330
over all distributions

491
00:36:46,360 --> 00:36:50,770
over training examples are operated rows of this matrix

492
00:36:50,780 --> 00:36:55,090
it's saying for any distribution there exists a g j

493
00:36:55,110 --> 00:36:59,020
so if we take the max over all of the weak classifiers j

494
00:37:00,160 --> 00:37:01,900
this probability

495
00:37:01,930 --> 00:37:06,650
but in terms of this matrix that probability can just be written as pj with

496
00:37:06,650 --> 00:37:11,770
this means expected value when i is chosen according to p

497
00:37:11,780 --> 00:37:14,570
and which is the column j

498
00:37:14,630 --> 00:37:18,410
OK so this is a max the loss that will be suffered by the role

499
00:37:18,410 --> 00:37:21,450
players at least at least gamma

500
00:37:21,460 --> 00:37:24,410
it turns out that taking the max over

501
00:37:24,420 --> 00:37:27,690
rose is the same as taking the max over rho

502
00:37:27,700 --> 00:37:32,870
distributions so this is equal to the the west

503
00:37:32,880 --> 00:37:36,240
just because this thing is linear

504
00:37:36,250 --> 00:37:43,360
and now we can apply the minimax theorem minmax theorem says minimax equals maximum

505
00:37:43,370 --> 00:37:49,440
so saying that maximin is at least half plus gamma

506
00:37:49,460 --> 00:37:54,860
but again this man over wrote distributions is the same as taking the men over

507
00:37:54,860 --> 00:37:57,010
individual rows

508
00:37:57,030 --> 00:38:02,510
so we get this the max over column distributions of the men over individual rows

509
00:38:02,710 --> 00:38:03,740
of the loss

510
00:38:03,740 --> 00:38:08,020
if you haven't seen it yet

511
00:38:08,030 --> 00:38:10,440
so now want a

512
00:38:10,540 --> 00:38:13,300
straightforward example

513
00:38:13,310 --> 00:38:16,800
whereby we

514
00:38:18,300 --> 00:38:20,670
a certain

515
00:38:20,680 --> 00:38:23,090
dependent on x

516
00:38:23,130 --> 00:38:25,150
i give you is given

517
00:38:27,670 --> 00:38:31,330
is ten to fifteen

518
00:38:31,390 --> 00:38:33,600
times x

519
00:38:33,690 --> 00:38:34,900
so that

520
00:38:34,910 --> 00:38:36,290
is a given

521
00:38:36,330 --> 00:38:38,720
and this holds

522
00:38:38,780 --> 00:38:41,190
between x equals zero

523
00:38:42,180 --> 00:38:44,430
they tend to minus two meters

524
00:38:44,470 --> 00:38:49,280
so it holds over the space of one centimetre

525
00:38:49,350 --> 00:38:53,840
sort of potential changes linearly with this

526
00:38:53,920 --> 00:38:56,710
what now is the electric fields

527
00:38:56,770 --> 00:38:58,230
in that space

528
00:38:59,480 --> 00:39:02,360
electric field to go back to my

529
00:39:02,380 --> 00:39:05,960
description there

530
00:39:06,020 --> 00:39:09,030
there is only one component xn direction

531
00:39:09,140 --> 00:39:12,020
so the first derivative becomes minus

532
00:39:12,060 --> 00:39:14,080
ten to the fifth

533
00:39:14,170 --> 00:39:15,930
times x rules

534
00:39:15,940 --> 00:39:18,460
and the other is zero

535
00:39:18,690 --> 00:39:21,770
y zero

536
00:39:21,790 --> 00:39:25,010
and these are

537
00:39:25,020 --> 00:39:27,040
so you may say well book nice

538
00:39:27,490 --> 00:39:30,430
mathematics but we don't see any physics

539
00:39:30,510 --> 00:39:33,640
is more physical than you think

540
00:39:33,650 --> 00:39:36,950
imagine that i have here played

541
00:39:37,010 --> 00:39:39,240
which is charged

542
00:39:39,300 --> 00:39:42,100
the positive charge

543
00:39:42,180 --> 00:39:45,550
the play is at location x

544
00:39:45,580 --> 00:39:47,980
eleven out here

545
00:39:48,050 --> 00:39:51,520
say at location zero i call this place a

546
00:39:51,560 --> 00:39:53,480
and i call this play be

547
00:39:53,490 --> 00:39:55,570
this place is charged

548
00:40:00,980 --> 00:40:03,930
going to do that

549
00:40:03,930 --> 00:40:07,010
so i can put the electric field inside here

550
00:40:07,130 --> 00:40:08,940
according to the

551
00:40:10,720 --> 00:40:13,150
minus ten to the fifth

552
00:40:13,190 --> 00:40:18,090
and it is in the direction of mining x rules so x roof

553
00:40:18,150 --> 00:40:21,670
using this direction

554
00:40:21,690 --> 00:40:25,240
electric field in the opposite direction

555
00:40:25,300 --> 00:40:27,920
and it's the same everywhere

556
00:40:27,960 --> 00:40:29,470
and that's very physical

557
00:40:29,480 --> 00:40:31,550
we discuss that

558
00:40:31,560 --> 00:40:34,840
when we discuss the electric field near

559
00:40:34,900 --> 00:40:40,360
very large grains that the electric field inside was a constant remember

560
00:40:40,380 --> 00:40:42,710
and the electric field inside

561
00:40:43,740 --> 00:40:48,130
sigma divided by epsilon zero if sigma is the surface charge density on each of

562
00:40:48,130 --> 00:40:49,980
these plates

563
00:40:50,030 --> 00:40:52,850
and we argue that the electric field outside

564
00:40:52,860 --> 00:40:54,050
the above zero

565
00:40:54,060 --> 00:40:59,290
and that the electric field outside it was about to extremely physical there exactly what

566
00:40:59,290 --> 00:41:00,790
you see here

567
00:41:00,890 --> 00:41:02,180
electric field

568
00:41:02,190 --> 00:41:06,770
minus ten first time six or the magnitude of the electric field here

569
00:41:06,810 --> 00:41:10,490
the magnitude is ten

570
00:41:12,670 --> 00:41:17,600
from either

571
00:41:17,630 --> 00:41:20,360
well now is the potential difference well

572
00:41:20,380 --> 00:41:22,130
v eight

573
00:41:22,180 --> 00:41:25,140
minus b

574
00:41:25,190 --> 00:41:26,710
nine b

575
00:41:26,870 --> 00:41:29,760
this integral

576
00:41:29,820 --> 00:41:32,360
going from a to b

577
00:41:32,380 --> 00:41:34,470
one of the dot

578
00:41:36,190 --> 00:41:38,270
so i go from here to here

579
00:41:38,280 --> 00:41:42,270
i write down for DL i write down the acts of course

580
00:41:42,380 --> 00:41:45,030
i call that the x direction

581
00:41:45,110 --> 00:41:48,170
so i will write down the dot

582
00:41:52,630 --> 00:41:55,340
and so this is

583
00:41:55,380 --> 00:41:59,550
minus ten to fifteen

584
00:41:59,620 --> 00:42:03,680
i'm integral in going from a to b

585
00:42:03,690 --> 00:42:06,550
of x roof

586
00:42:10,320 --> 00:42:14,610
look scary but it is trivial x rules is the unit vector in this direction

587
00:42:14,650 --> 00:42:17,880
and vx is little vector x in this direction

588
00:42:17,930 --> 00:42:19,870
so labels in the same direction

589
00:42:19,880 --> 00:42:20,970
the cosine

590
00:42:20,990 --> 00:42:23,290
the angle between the two is one

591
00:42:23,300 --> 00:42:24,830
so i can forget about

592
00:42:24,890 --> 00:42:27,510
vector i can forget about the dot

593
00:42:27,570 --> 00:42:29,300
so this becomes

594
00:42:30,870 --> 00:42:33,080
ten to fifteen

595
00:42:33,090 --> 00:42:36,380
i was integral in going from a to b

596
00:42:36,430 --> 00:42:40,090
of the x

597
00:42:40,150 --> 00:42:41,550
that is trivial

598
00:42:41,610 --> 00:42:45,680
that is minus ten to the fifth

599
00:42:45,690 --> 00:42:47,320
times the location

600
00:42:47,330 --> 00:42:49,960
i have to do the integral between a and b

601
00:42:49,990 --> 00:42:53,190
so i get new x three

602
00:42:53,190 --> 00:42:59,890
line is activated

603
00:42:59,910 --> 00:43:04,130
and if this is ten to the minus two meters

604
00:43:04,140 --> 00:43:06,190
we go from here

605
00:43:06,210 --> 00:43:10,740
two years one centimetre i must multiply this by ten to the minus two

606
00:43:10,760 --> 00:43:16,010
so i get this is minus one thousand volts

607
00:43:16,070 --> 00:43:17,920
so a

608
00:43:17,950 --> 00:43:21,150
thousand fold lower than b

609
00:43:21,190 --> 00:43:22,920
that's what it means

610
00:43:22,980 --> 00:43:25,990
that's something that's very physical

611
00:43:26,100 --> 00:43:28,960
notice that if you go from left to right

612
00:43:29,020 --> 00:43:32,000
that the potential grows linearly

613
00:43:32,080 --> 00:43:34,660
this is lower than that

614
00:43:34,670 --> 00:43:36,880
and if you in your head

615
00:43:38,930 --> 00:43:41,340
like these parallel to the plane

616
00:43:41,340 --> 00:43:44,330
each one of those planes will be people potential state

617
00:43:44,380 --> 00:43:46,380
everywhere have the same potential

618
00:43:46,430 --> 00:43:48,820
and gradually when you move it up

619
00:43:48,830 --> 00:43:50,210
your potential

620
00:43:51,530 --> 00:43:54,690
notice the electric field goes from plus to minus

621
00:43:54,730 --> 00:43:56,370
the opposite direction

622
00:43:56,370 --> 00:43:58,800
that's always the reason that's behind that

623
00:43:58,820 --> 00:44:02,340
the minus sign

624
00:44:02,380 --> 00:44:05,920
clearly i'm always breaking choice where i

625
00:44:05,970 --> 00:44:10,500
two is my zero potential we discuss that last time you don't always have to

626
00:44:10,550 --> 00:44:12,540
two is infinitely far zero

627
00:44:12,550 --> 00:44:14,150
so i could choose

628
00:44:15,200 --> 00:44:17,640
arbitrarily to be zero potential

629
00:44:17,640 --> 00:44:20,380
this would then be full of thousands

630
00:44:20,430 --> 00:44:22,260
so you then find that

631
00:44:22,320 --> 00:44:25,190
the potential v

632
00:44:25,280 --> 00:44:27,520
then simply tend to defeat

633
00:44:27,520 --> 00:44:32,400
was something not very clear from lost from the last set of lectures have you

634
00:44:32,400 --> 00:44:36,180
got any questions

635
00:44:37,370 --> 00:44:38,740
everything clear

636
00:44:38,800 --> 00:44:45,820
they still recall what's elements trick is and how cross validation works and all that

637
00:44:47,380 --> 00:44:51,020
good who can tell me an example of a good kernel

638
00:44:54,590 --> 00:44:55,720
garcia kernel

639
00:44:56,800 --> 00:44:59,360
you can tell you that the one hand

640
00:45:01,690 --> 00:45:09,040
OK can somebody telling example of something that is not the kernel

641
00:45:09,070 --> 00:45:12,750
but it's symmetric

642
00:45:12,750 --> 00:45:24,750
well a function it looks like the kind of but isn't

643
00:45:24,780 --> 00:45:30,080
OK achieving new to let

644
00:45:32,100 --> 00:45:34,630
what we're going to do today is to look at

645
00:45:34,680 --> 00:45:37,820
essentially convex optimization for methods

646
00:45:37,830 --> 00:45:39,410
for inference

647
00:45:39,460 --> 00:45:44,250
that's basically what did yesterday's we started with a very simple estimators we just take

648
00:45:45,580 --> 00:45:49,790
and they moved on to estimate which more or less smooth out the doctor

649
00:45:49,800 --> 00:45:52,210
these are the parzen windows estimators

650
00:45:52,220 --> 00:45:57,740
and then we saw very simple stochastic gradient descent optimisation procedure called the perceptron

651
00:45:57,750 --> 00:46:01,220
which will give you a linear separators

652
00:46:01,270 --> 00:46:03,610
and we analyse starting quite a bit of detail

653
00:46:03,660 --> 00:46:06,290
it's very simple south me can find

654
00:46:06,360 --> 00:46:08,470
what we're going to do today is

655
00:46:08,490 --> 00:46:12,320
well look at support vector classification

656
00:46:13,900 --> 00:46:19,000
and regression quantile regression of fiction the new trick so all sorts of

657
00:46:19,050 --> 00:46:22,500
why is how you can do support vectors that's kind of standard fare

658
00:46:22,580 --> 00:46:26,190
this stuff kind of spans the area from

659
00:46:26,240 --> 00:46:28,100
nineteen ninety five

660
00:46:28,140 --> 00:46:33,100
until about two thousand five the quantile regression will be about their novelty detection with

661
00:46:33,140 --> 00:46:34,570
the abrupt

662
00:46:36,460 --> 00:46:40,270
ninety nine or two thousand like what's new trick

663
00:46:41,280 --> 00:46:42,930
the last lecture

664
00:46:43,010 --> 00:46:44,640
we look at some

665
00:46:44,650 --> 00:46:50,330
a slightly more cutting-edge stuff sequence annotation web page ranking path planning

666
00:46:50,340 --> 00:46:52,580
and just in general optimisation

667
00:46:52,580 --> 00:46:55,490
and how you go and implement some things

668
00:46:57,610 --> 00:46:58,860
this one

669
00:46:59,520 --> 00:47:01,340
the challenging

670
00:47:01,390 --> 00:47:04,730
there should be reasonably easy going

671
00:47:04,770 --> 00:47:08,310
that's it if you have questions at any time please let me know

672
00:47:08,360 --> 00:47:10,680
i know this stuff i've written the slide

673
00:47:10,730 --> 00:47:13,010
i want to teach you how it works so

674
00:47:13,050 --> 00:47:16,180
if you don't interrupt me analysis and you know it

675
00:47:16,310 --> 00:47:18,740
so i support vector

676
00:47:18,830 --> 00:47:22,670
well what will do is will first look at just the standard SVM

677
00:47:22,760 --> 00:47:26,420
and then the optimisation problem a few properties of that

678
00:47:26,460 --> 00:47:31,050
and we compute what's called the dual optimisation problem

679
00:47:32,430 --> 00:47:35,080
this thing is

680
00:47:35,090 --> 00:47:40,330
it to my

681
00:47:46,860 --> 00:47:47,670
this is

682
00:47:47,710 --> 00:47:52,210
an example the query and yesterday we have two sets we want to separate them

683
00:47:54,150 --> 00:47:55,170
what we

684
00:47:55,240 --> 00:47:58,050
i want to do is we want to estimate the y given x thing in

685
00:48:00,420 --> 00:48:04,080
well instead of just doing this we could try to find the function f of

686
00:48:04,080 --> 00:48:05,590
x this task

687
00:48:05,640 --> 00:48:09,050
so and already talked about that in the morning when he was talking about logistic

688
00:48:10,150 --> 00:48:13,360
so anyway if you understood what nanda was talking about four

689
00:48:13,360 --> 00:48:14,760
logistic regression

690
00:48:14,770 --> 00:48:18,020
this will be a walk in the park is it's much easier

691
00:48:18,020 --> 00:48:22,020
if it doesn't make sense to maybe after that people will also understand and

692
00:48:23,980 --> 00:48:25,360
it's useful

693
00:48:25,420 --> 00:48:29,390
anyway so

694
00:48:29,520 --> 00:48:32,870
OK the neck and i can just fast-forward

695
00:48:37,590 --> 00:48:40,960
let's say i got those states and i want to separate them

696
00:48:40,980 --> 00:48:44,270
and any of those lines will do the trick

697
00:48:44,310 --> 00:48:46,810
now the question is

698
00:48:46,940 --> 00:48:50,250
who thinks the blue line is the base

699
00:48:51,720 --> 00:48:55,790
who thinks the black line is the base

700
00:48:55,840 --> 00:48:58,330
who thinks the red line is the best

701
00:49:00,770 --> 00:49:02,750
given that a lot of people say

702
00:49:04,710 --> 00:49:08,120
the the black line baseball why

703
00:49:11,290 --> 00:49:14,760
why did you vote for the black then must have thought something

704
00:49:17,940 --> 00:49:21,460
i mean i didn't write very well i mean actually to the

705
00:49:21,470 --> 00:49:25,010
gone down here somewhere

706
00:49:25,060 --> 00:49:29,920
and then they not going to ask you why you voted for the red one

707
00:49:29,950 --> 00:49:31,820
so and

708
00:49:33,040 --> 00:49:38,340
so that intuition to find a hyperplane that has the largest distance from both states

709
00:49:38,370 --> 00:49:41,320
that's pretty much what we're going to pursue

710
00:49:41,370 --> 00:49:44,880
and we'll see that this actually corresponds to the notion of scale

711
00:49:45,010 --> 00:49:49,410
as in the simplest function the one of the largest separating function

712
00:49:49,570 --> 00:49:51,640
the smoothest

713
00:49:51,690 --> 00:49:55,880
linear function they all amount to the same thing and we'll see why this is

714
00:49:59,320 --> 00:50:02,020
that's the one to rule them all

715
00:50:02,100 --> 00:50:04,710
maximum separation from both sides

716
00:50:04,760 --> 00:50:05,620
one you

717
00:50:05,640 --> 00:50:08,570
what you could do it you could run a bunch of perceptrons

718
00:50:08,600 --> 00:50:11,960
at the end of the day you just have perceptron solutions

719
00:50:12,000 --> 00:50:15,020
and get fairly close to this

720
00:50:15,080 --> 00:50:20,090
this might be actually computationally very efficient way of finding

721
00:50:21,520 --> 00:50:24,590
people have actually explore that as much as they should

722
00:50:24,870 --> 00:50:27,080
so how do we get this line

723
00:50:27,140 --> 00:50:28,250
so that's

724
00:50:28,260 --> 00:50:32,140
the picture is probably as old as support vectors are so

725
00:50:32,150 --> 00:50:34,310
courtesy of an actual copy

726
00:50:34,340 --> 00:50:38,010
when i was i think quite nostalgic value you see the point isn't so nice

727
00:50:38,010 --> 00:50:39,020
anymore but anyway

728
00:50:39,460 --> 00:50:41,000
so these are

729
00:50:41,030 --> 00:50:42,510
a few points

730
00:50:42,520 --> 00:50:45,630
and these are a bunch of squares and i want to separate the search from

731
00:50:46,520 --> 00:50:48,210
well done OK

732
00:50:48,220 --> 00:50:49,920
now how would i do this

733
00:50:49,960 --> 00:50:53,750
well case i find some separating hyperplane

734
00:50:53,830 --> 00:50:55,870
and if i want that both

735
00:50:55,880 --> 00:51:00,040
six have equal distance from it well i can just make parallel transport of the

736
00:51:00,060 --> 00:51:02,030
hyperplane to all the sites

737
00:51:02,100 --> 00:51:05,910
and then just measure that this

738
00:51:06,830 --> 00:51:09,770
everybody knows that i can describe a hyperplane

739
00:51:09,820 --> 00:51:14,540
so it's normal equation in other words i denote the normal vector by w

740
00:51:14,540 --> 00:51:17,710
written about ninety six by enough

741
00:51:17,730 --> 00:51:21,790
and i think with some contributions from chris bishop's book about it really nice way

742
00:51:21,790 --> 00:51:25,680
of playing with lots of certainly all the machine learning algorithms in to state the

743
00:51:25,680 --> 00:51:28,740
i ninety six and i think it's got some updated stuff and i think this

744
00:51:28,750 --> 00:51:30,160
new version coming out

745
00:51:30,170 --> 00:51:33,250
so there is some great hours playing with these things you can look at how

746
00:51:33,270 --> 00:51:35,750
these demos being done so

747
00:51:35,770 --> 00:51:37,610
it can be very similar

748
00:51:37,630 --> 00:51:40,470
two the demo i did with k means

749
00:51:40,490 --> 00:51:42,270
we've got two clusters

750
00:51:42,310 --> 00:51:46,490
and this is i data here so when someone what happens here

751
00:51:46,520 --> 00:51:47,590
we can do

752
00:51:47,600 --> 00:51:50,730
so what you're going to see is the red cross and the blue cluster

753
00:51:50,810 --> 00:51:54,260
eight point instead of being allocated immediately to red or blue

754
00:51:54,310 --> 00:51:58,550
he believes he belongs partially to write impartially blue so we're going to see that

755
00:51:58,620 --> 00:52:01,850
visualizes the combination of red and blue in each of these guys and that's the

756
00:52:03,060 --> 00:52:06,760
so you can see the collars changing the guys and then you can see the

757
00:52:07,050 --> 00:52:11,230
senses of the gaussians and the covariance is moving so we see the collars changing

758
00:52:11,230 --> 00:52:15,880
that e that when you see the sense the gaussians moving that's the m step

759
00:52:19,060 --> 00:52:22,850
you can see that if i click on the right place

760
00:52:24,060 --> 00:52:28,480
so his initial configuration of the mixture model is configured and can you see that

761
00:52:30,130 --> 00:52:31,650
i can see really clear

762
00:52:31,680 --> 00:52:36,710
the lights at the front somewhere

763
00:52:36,760 --> 00:52:40,920
it's annoying because the blue circle where red circle that

764
00:52:40,930 --> 00:52:42,710
and weights

765
00:52:42,760 --> 00:52:44,420
i know

766
00:52:44,430 --> 00:52:58,730
a real-time map

767
00:53:08,320 --> 00:53:14,810
OK that's clear of so you got red component and the blue component here

768
00:53:14,920 --> 00:53:20,960
then you know to responsibilities so initially because the blue rinse these guys o thing

769
00:53:21,040 --> 00:53:24,510
i must belong to him and those guys think almost belong to him and actually

770
00:53:24,510 --> 00:53:27,260
know really confident about it even

771
00:53:27,280 --> 00:53:31,000
you know they're not close to any of senses the posterior says are extremely confident

772
00:53:31,010 --> 00:53:35,690
about where they are but then we update the means and variances so that's the

773
00:53:35,690 --> 00:53:38,230
first thing means and variances they move

774
00:53:38,280 --> 00:53:41,940
this was a really bad initialisation just to show lots of steps and then the

775
00:53:42,950 --> 00:53:45,880
the responsibility now the

776
00:53:45,950 --> 00:53:49,750
confused they belong to and then using some iterations

777
00:53:49,790 --> 00:53:54,780
so slowly they become clear about who belongs to what responsibility start going to the

778
00:53:54,780 --> 00:53:58,480
correct things that we would have seen very quickly actually with the k means clustering

779
00:53:58,480 --> 00:54:00,820
k means clustering via a lot faster this

780
00:54:00,860 --> 00:54:05,070
this is fully probabilistic it's soft and extensions of this model

781
00:54:05,080 --> 00:54:08,790
which have to reach process is sitting on top allow you to do infinite

782
00:54:08,790 --> 00:54:13,350
components and decide on the number of components are using

783
00:54:13,510 --> 00:54:17,580
with lots of other bayesian treatment of the problem so this is not bayesian because

784
00:54:17,580 --> 00:54:21,200
we just using bayes rule to update something that we believe the stochastic model is

785
00:54:21,200 --> 00:54:23,040
a sparse selection of ask

786
00:54:23,320 --> 00:54:27,880
and then we assign some data sample given that

787
00:54:27,890 --> 00:54:32,110
OK so i'm going to very quickly speed through my dimensionality reduction stuff

788
00:54:35,730 --> 00:54:38,200
so you have to mention this quote

789
00:54:38,280 --> 00:54:43,920
and then i'll mention the results meant so i love it so it's always a

790
00:54:43,920 --> 00:54:47,200
must get to a certain age when you actually start bothering to read the papers

791
00:54:47,220 --> 00:54:51,980
that you keep referencing four different methods

792
00:54:52,000 --> 00:54:56,860
so tell nineteen thirty three is person first call these things principal components people credited

793
00:54:56,860 --> 00:55:00,790
pierce but to my mind reading the two papers and this is the one i

794
00:55:00,790 --> 00:55:04,330
preferred because it is much more in line with my image what principal components it

795
00:55:04,570 --> 00:55:08,380
is also inspired by psychology because it comes i can't remember the name of the

796
00:55:08,380 --> 00:55:10,910
guy who was responsible for factor analysis

797
00:55:10,920 --> 00:55:19,820
you know just there's lord someone or another was is some really famous social scientists

798
00:55:19,830 --> 00:55:24,200
yeah something first and the will of the people working on factor analysis around this

799
00:55:25,040 --> 00:55:27,260
it was an important technique for you

800
00:55:27,260 --> 00:55:32,540
people looking IQ and they developed these really interesting methods and hotelling was actually said

801
00:55:32,540 --> 00:55:36,480
well mathematician and statistician who was talking to social scientists is what you want to

802
00:55:36,490 --> 00:55:40,410
do that i think we should do it like this and he actually derives PCA

803
00:55:40,410 --> 00:55:45,800
through factor analysis type idea that you've got a bunch of variables you observe and

804
00:55:45,800 --> 00:55:48,860
he even has this is the function so this is a relation is a function

805
00:55:48,860 --> 00:55:54,160
of some unobserved variables so this might be your IQ or whatever these factors are

806
00:55:54,240 --> 00:55:59,420
then he says so this is called source of much confusion in future however to

807
00:55:59,420 --> 00:56:03,980
the perspective case these ideas outside the in the computer uses of according to the

808
00:56:03,980 --> 00:56:08,970
word factor in mathematics and we better call axis components of the complex depicted by

809
00:56:08,970 --> 00:56:13,310
the test so he actually saying these are factors so is factor analysis in the

810
00:56:13,310 --> 00:56:19,050
original derivation of what he's saying principal component analysis and actually really annoying me

811
00:56:19,060 --> 00:56:23,340
the way the factors appear in the covariance function they do appear as factors in

812
00:56:23,340 --> 00:56:27,190
some sense so you could have called fact aside almost think of these factors but

813
00:56:27,190 --> 00:56:29,520
we start with the ten principal component analysis

814
00:56:29,530 --> 00:56:32,400
because the he invented the message here so

815
00:56:32,410 --> 00:56:36,090
he has a slightly different approach to how you fit in this model

816
00:56:36,140 --> 00:56:40,000
and it's slightly different from what we would think in the modern context so these

817
00:56:40,000 --> 00:56:41,730
prestigious honour

818
00:56:43,520 --> 00:56:45,670
these are two important theorems

819
00:56:45,690 --> 00:56:49,690
or anything that was something that

820
00:56:52,210 --> 00:56:54,190
given a class of form

821
00:56:54,210 --> 00:56:57,480
the ones on some

822
00:56:57,480 --> 00:57:02,130
we can show that is and why

823
00:57:02,170 --> 00:57:06,400
now completeness says that a class of four

824
00:57:06,420 --> 00:57:09,040
that is unsatisfied

825
00:57:09,360 --> 00:57:14,590
you want the understand the to show

826
00:57:14,610 --> 00:57:15,820
don't need

827
00:57:22,210 --> 00:57:29,090
i also like to mention briefly satisfied testing

828
00:57:29,110 --> 00:57:33,210
so the satisfiability problem

829
00:57:33,750 --> 00:57:36,750
quite interesting because it is the first

830
00:57:36,790 --> 00:57:39,380
the saw NP complete

831
00:57:39,690 --> 00:57:42,090
probably no

832
00:57:42,190 --> 00:57:46,690
just says that

833
00:57:46,690 --> 00:57:49,940
it has a polynomial time solution do it

834
00:57:50,060 --> 00:57:52,980
i want to

835
00:57:55,900 --> 00:58:00,210
the biggest questions computer science is

836
00:58:00,360 --> 00:58:04,130
this equation

837
00:58:04,230 --> 00:58:06,380
is the class

838
00:58:07,340 --> 00:58:09,690
due to

839
00:58:09,710 --> 00:58:10,960
people and

840
00:58:10,980 --> 00:58:13,540
i think

841
00:58:18,840 --> 00:58:21,750
this is this is is one

842
00:58:29,460 --> 00:58:31,580
given that you and p

843
00:58:35,980 --> 00:58:37,480
the existing

844
00:58:37,480 --> 00:58:42,400
satisfiability so they should be like used

845
00:58:45,630 --> 00:58:48,230
more than red round line

846
00:58:49,540 --> 00:58:53,110
one of the

847
00:58:53,170 --> 00:58:57,230
one the algorithm is a decision which

848
00:58:57,250 --> 00:59:01,770
the decision to choose which you make

849
00:59:01,770 --> 00:59:04,480
signed through two

850
00:59:04,500 --> 00:59:06,270
propositional variables

851
00:59:06,270 --> 00:59:09,880
and essentially is just the point

852
00:59:10,880 --> 00:59:13,900
one set of the

853
00:59:13,900 --> 00:59:14,940
so they can

854
00:59:14,940 --> 00:59:18,000
so to learn from past mistakes

855
00:59:19,630 --> 00:59:21,540
the next slide

856
00:59:21,730 --> 00:59:26,360
but i just want to point you to this website

857
00:59:26,360 --> 00:59:30,480
sat competition of this is a huge industry no

858
00:59:30,540 --> 00:59:31,790
after you have

859
00:59:31,810 --> 00:59:34,610
this competition successful

860
00:59:34,630 --> 00:59:36,420
i have

861
00:59:36,540 --> 00:59:40,690
huge collections benchmarked problems

862
00:59:40,820 --> 00:59:43,730
and so on

863
00:59:46,730 --> 00:59:48,840
most of the settlers

864
00:59:48,860 --> 00:59:53,210
this group called p of

865
00:59:57,360 --> 00:59:59,230
his partner

866
01:00:02,250 --> 01:00:04,500
so i guess that

867
01:00:04,730 --> 01:00:07,540
this dataset was not played

868
01:00:07,560 --> 01:00:11,230
try to analyse the brain

869
01:00:11,290 --> 01:00:15,020
the technique of this conflict analysis

870
01:00:15,110 --> 01:00:20,090
here is one example of the technique

871
01:00:20,150 --> 01:00:23,650
so if you can see source

872
01:00:24,710 --> 01:00:26,880
quite surprising

873
01:00:26,960 --> 01:00:30,420
they are often used as target

874
01:00:31,040 --> 01:00:35,810
salford people the problems with

875
01:00:37,630 --> 01:00:42,170
it is a set of

876
01:00:42,210 --> 01:00:45,040
OK now i come to

877
01:00:48,040 --> 01:00:51,790
representation of the form of

878
01:00:51,810 --> 01:00:57,170
as i said can basically represents and we want

879
01:00:57,250 --> 01:00:58,730
in the truth

880
01:00:58,730 --> 01:01:02,840
this is all possible truth and come

881
01:01:02,900 --> 01:01:07,080
it's expensive exponentially in terms of

882
01:01:07,090 --> 01:01:07,940
so a

883
01:01:09,440 --> 01:01:10,940
people are looking

884
01:01:10,960 --> 01:01:13,040
for representation

885
01:01:13,090 --> 01:01:16,310
and the reason why

886
01:01:16,320 --> 01:01:19,130
we can represent because

887
01:01:19,130 --> 01:01:22,230
in certain areas such more attractive

888
01:01:22,270 --> 01:01:26,860
represents the transition as property

889
01:01:26,940 --> 01:01:28,960
and they work

890
01:01:28,980 --> 01:01:33,540
well first you want representation that is the smallest

891
01:01:33,730 --> 01:01:36,340
it takes less time

892
01:01:42,590 --> 01:01:46,900
the representation because graph

893
01:01:47,210 --> 01:01:49,270
this is a form

894
01:01:49,320 --> 01:01:50,520
the most useful

895
01:01:50,610 --> 01:01:52,810
this is central point

896
01:01:52,810 --> 01:01:57,120
everything here is linear and l is of fixed matrix i can bring it out

897
01:01:57,120 --> 01:01:59,350
this is l

898
01:01:59,390 --> 01:02:00,830
bring out on the

899
01:02:01,830 --> 01:02:04,600
the expected value of

900
01:02:04,600 --> 01:02:07,200
he transposed

901
01:02:08,080 --> 01:02:11,790
l transferred to see that i just the usual parentheses

902
01:02:11,830 --> 01:02:16,790
that's l e e transpose looks like a crossword puzzle

903
01:02:16,810 --> 01:02:17,850
l prince

904
01:02:18,890 --> 01:02:24,720
on saying as i can bring the only l transpose out of the expectation because

905
01:02:24,740 --> 01:02:28,510
when i multiply these values by probabilities

906
01:02:28,560 --> 01:02:31,080
l is fixed

907
01:02:31,140 --> 01:02:32,370
now let's say

908
01:02:32,390 --> 01:02:34,010
OK and what's this

909
01:02:35,470 --> 01:02:36,530
that sigma

910
01:02:36,580 --> 01:02:38,030
that sigma b

911
01:02:38,080 --> 01:02:39,850
so this is

912
01:02:39,870 --> 01:02:41,140
sigma b

913
01:02:53,720 --> 01:02:55,530
it's are

914
01:02:55,530 --> 01:02:59,620
you know if we can figure out what this is

915
01:02:59,680 --> 01:03:02,890
we know what hell is now this is where i feel

916
01:03:03,100 --> 01:03:07,050
like a bit of maths a typical bit of magic in these formulas is going

917
01:03:07,050 --> 01:03:08,620
to happen now

918
01:03:08,680 --> 01:03:10,010
i'm gonna

919
01:03:10,030 --> 01:03:13,100
but in this awful expression for l

920
01:03:13,140 --> 01:03:16,010
and see what i get

921
01:03:16,050 --> 01:03:21,490
what i'm doing now is totally standard may be familiar to all of you but

922
01:03:21,490 --> 01:03:23,780
i think about it take this last step

923
01:03:23,790 --> 01:03:25,450
can i just say OK

924
01:03:25,450 --> 01:03:28,490
i'm claiming that this is the right l to choose

925
01:03:28,530 --> 01:03:29,760
well gauss

926
01:03:29,780 --> 01:03:31,430
so it was sold

927
01:03:31,470 --> 01:03:33,160
i was going to argue with again

928
01:03:33,160 --> 01:03:34,240
OK so

929
01:03:34,260 --> 01:03:39,410
there is no he he says that the right to take is saying is the

930
01:03:39,410 --> 01:03:41,580
right see the take is is

931
01:03:41,600 --> 01:03:43,560
sigma in

932
01:03:43,560 --> 01:03:45,280
OK so can i

933
01:03:45,330 --> 01:03:48,930
but that in here could you do that multiplication and you know what we're going

934
01:03:48,930 --> 01:03:50,310
to get

935
01:03:50,330 --> 01:03:51,890
make a little room

936
01:03:51,910 --> 01:03:54,470
do it is a little long

937
01:03:58,060 --> 01:03:59,620
OK here comes l

938
01:03:59,790 --> 01:04:01,450
so i have

939
01:04:01,490 --> 01:04:04,050
a transposing

940
01:04:04,060 --> 01:04:06,010
inverse a

941
01:04:06,060 --> 01:04:07,810
all inverse

942
01:04:07,850 --> 01:04:13,720
times a transposing members that was that's the l finally

943
01:04:13,780 --> 01:04:16,050
OK now comes the sigma

944
01:04:16,060 --> 01:04:18,760
sigma others straight segments can be

945
01:04:18,790 --> 01:04:20,450
time sigma

946
01:04:20,580 --> 01:04:23,030
now comes any more room

947
01:04:23,080 --> 01:04:25,180
four l transpose OK

948
01:04:25,260 --> 01:04:31,410
this formula will finish before the borders

949
01:04:32,410 --> 01:04:34,200
that was l

950
01:04:34,240 --> 01:04:36,330
and what self transpose

951
01:04:36,330 --> 01:04:40,490
sigma by the way of course is symmetric

952
01:04:40,510 --> 01:04:43,530
sigma of course is symmetric

953
01:04:43,550 --> 01:04:47,560
so when i transpose i don't have to worry about transposing sigma a of course

954
01:04:47,560 --> 01:04:48,970
i do have to

955
01:04:48,990 --> 01:04:55,290
and this combination is symmetric so i transposing that no no that's OK so now

956
01:04:55,290 --> 01:04:59,560
i want to transpose this whole thing in the opposite order so i transpose that

957
01:04:59,700 --> 01:05:01,120
just sigma inverse

958
01:05:01,180 --> 01:05:10,850
suppose that a transpose that that symmetric so it doesn't change transposing inverse inverse and

959
01:05:10,910 --> 01:05:16,200
what have i got

960
01:05:16,240 --> 01:05:18,720
so this is following all the rules

961
01:05:18,740 --> 01:05:23,470
and notice well there's the same inverse sigma identity in the middle there

962
01:05:23,510 --> 01:05:29,450
and then there an a transposing the inverse a which i'll just put parentheses around

963
01:05:29,470 --> 01:05:30,830
i call attention

964
01:05:30,830 --> 01:05:32,120
and what do i have

965
01:05:32,160 --> 01:05:34,160
i've got this inverse

966
01:05:34,200 --> 01:05:35,430
times this thing

967
01:05:35,430 --> 01:05:37,470
times inverse again so

968
01:05:37,530 --> 01:05:43,030
so this is not all this and left me just with the

969
01:05:43,030 --> 01:05:45,220
it's alright if i just

970
01:05:45,220 --> 01:05:47,970
that's the answer what a beautiful answer

971
01:05:48,030 --> 01:05:49,120
that the

972
01:05:49,180 --> 01:05:51,490
the right choice of

973
01:05:52,780 --> 01:05:57,330
the right choice of those are the right choice of the weighting matrix

974
01:05:58,930 --> 01:06:01,740
a beautiful answer for the

975
01:06:01,760 --> 01:06:03,490
variance in the

976
01:06:03,510 --> 01:06:05,260
in the in the you have

977
01:06:05,280 --> 01:06:07,580
and the beautiful answer is this

978
01:06:07,640 --> 01:06:09,550
is the inverse is p

979
01:06:09,560 --> 01:06:13,180
is our peak

980
01:06:13,220 --> 01:06:15,180
that's the crucial matrix

981
01:06:15,220 --> 01:06:19,510
in fact that's like the most important thing is output

982
01:06:19,510 --> 01:06:22,470
so that the most important output is that

983
01:06:24,490 --> 01:06:28,200
you had which we finally identified as

984
01:06:29,870 --> 01:06:35,740
i think you had we finally identified as our matrix p

985
01:06:35,780 --> 01:06:38,470
is that right

986
01:06:38,470 --> 01:06:39,330
let's see

987
01:06:39,370 --> 01:06:40,330
i didn't

988
01:06:40,350 --> 01:06:43,810
yes sorry i didn't do this was wrong does

989
01:06:43,810 --> 01:06:46,050
four to get four race

990
01:06:46,100 --> 01:06:48,450
i met peter be

991
01:06:48,470 --> 01:06:54,950
exactly this matrix that we're sorry crucial matrix for us he is

992
01:06:54,970 --> 01:06:56,620
it is

993
01:06:56,810 --> 01:07:01,410
transpose sigma b version

994
01:07:01,430 --> 01:07:04,830
all inverse that's

995
01:07:04,870 --> 01:07:05,970
that's the key

996
01:07:05,970 --> 01:07:10,100
that's the beautiful matrix and the whole subject and

997
01:07:10,180 --> 01:07:11,890
i can of saying

998
01:07:11,930 --> 01:07:17,560
this is this is the inverse of our stiffness matrix those early lectures

999
01:07:17,580 --> 01:07:19,580
we're about

1000
01:07:19,620 --> 01:07:23,410
this matrix right and its inverse

1001
01:07:23,470 --> 01:07:25,950
and now we're meeting

1002
01:07:25,970 --> 01:07:28,390
as the central output

1003
01:07:28,390 --> 01:07:31,060
that the inverse of the stiffness matrix

1004
01:07:31,140 --> 01:07:35,870
as the central output sigma you've that it's the variance of the

1005
01:07:35,910 --> 01:07:39,830
error in our estimated parameters

1006
01:07:39,830 --> 01:07:42,290
it's it's fantastic

1007
01:07:43,560 --> 01:07:46,180
well there is one more central

1008
01:07:46,200 --> 01:07:47,740
o point

1009
01:07:47,830 --> 01:07:52,100
you remember what was special about those matrices in

1010
01:07:52,140 --> 01:07:53,470
beginning of this

1011
01:07:53,470 --> 01:07:56,260
a course of lectures

1012
01:07:56,280 --> 01:07:59,330
they were tried diag

1013
01:07:59,390 --> 01:08:02,010
they were tried diagonal and that

1014
01:08:02,060 --> 01:08:06,660
and that came for the line of springs because

1015
01:08:06,680 --> 01:08:14,370
each frame is just connected to one brain below and one above

1016
01:08:14,390 --> 01:08:17,010
but springs here we're not connected to

1017
01:08:17,010 --> 01:08:18,470
springs ten

1018
01:08:18,490 --> 01:08:22,010
down or we wouldn't have had right diagonal matrices

1019
01:08:22,030 --> 01:08:26,260
all i want to say is that the most important word that's not on the

1020
01:08:27,470 --> 01:08:29,200
is try diag

1021
01:08:30,140 --> 01:08:33,830
it's that i have to qualify it

1022
01:08:33,910 --> 01:08:35,870
for the for those

1023
01:08:35,890 --> 01:08:37,850
first lectures it was

1024
01:08:37,870 --> 01:08:41,720
normal try diagonal matrices

1025
01:08:42,490 --> 01:08:45,850
we're going to have walked right

1026
01:08:45,850 --> 01:08:50,930
that's the central thing that makes the whole kalman filter process work

1027
01:08:50,930 --> 01:08:52,510
is that there is that the

1028
01:08:52,530 --> 01:08:54,600
he matrix in there

1029
01:08:54,640 --> 01:08:58,240
this matrix in the in the giant process

1030
01:08:58,240 --> 01:09:03,640
is blocked right diagonal that i cannot overemphasize that's the key

1031
01:09:03,640 --> 01:09:06,740
this is no i in no way to insurers

1032
01:09:06,760 --> 01:09:12,220
the community to come up first consistent vocabulary because well i can decide instead of

1033
01:09:12,220 --> 01:09:17,860
looking up if there is maybe location property of darius lives and property i can

1034
01:09:17,860 --> 01:09:21,160
just create a new property here that's OK that's

1035
01:09:21,170 --> 01:09:25,120
that's what happens in which is all the time if you look at the categorization

1036
01:09:25,120 --> 01:09:31,280
wikipedia we see that there were numerous categories created and still there that actually expressed

1037
01:09:31,280 --> 01:09:32,740
in the same thing

1038
01:09:32,770 --> 01:09:36,920
and the we the idea is that after a while

1039
01:09:36,920 --> 01:09:37,990
other users

1040
01:09:38,020 --> 01:09:45,720
so called gardeners come along and we try to re conciliator this kind of information

1041
01:09:45,730 --> 01:09:50,800
this works in an open environment that has enough users

1042
01:09:50,810 --> 01:09:55,190
it will not work if you try this out in a company are in a

1043
01:09:55,190 --> 01:09:57,620
small group of maybe ten people

1044
01:09:59,030 --> 01:10:03,870
if no dedicated gardener you cannot expect to begin to clean up itself without any

1045
01:10:04,610 --> 01:10:09,710
wikipedia can do that because they have potentially six billion users

1046
01:10:09,730 --> 01:10:13,810
and even if only a small

1047
01:10:13,820 --> 01:10:19,580
small small part of them dedicated themselves to cleaning up the wiki it still enough

1048
01:10:19,580 --> 01:10:21,020
to do it

1049
01:10:21,210 --> 01:10:26,050
if you're trying to use semantic wiki for your company are if new group are

1050
01:10:26,380 --> 01:10:28,110
smaller system

1051
01:10:28,120 --> 01:10:31,920
you need to assign someone to do this cleanup

1052
01:10:31,940 --> 01:10:35,630
they can have a lot of tools to help them to clean up so they

1053
01:10:35,630 --> 01:10:39,300
don't have to go manually for the weekend or this kind of stuff

1054
01:10:39,310 --> 01:10:42,340
but you can they can rise to in order to clean them up there a

1055
01:10:42,340 --> 01:10:46,530
lot of what's already written that can have this kind of towards the special pages

1056
01:10:46,530 --> 01:10:49,720
in the wiki that can help with cleaning up this kind of stuff so they

1057
01:10:49,720 --> 01:10:52,150
have a lot of help this still have to do it

1058
01:10:53,140 --> 01:10:55,130
look everyone becomes

1059
01:10:55,190 --> 01:11:01,840
shared by themselves but everybody who built ontology several stakeholders knows that you have at

1060
01:11:01,860 --> 01:11:03,700
one point two

1061
01:11:03,720 --> 01:11:10,360
conciliator because different opinions

1062
01:11:10,370 --> 01:11:14,470
so i think actually this is one of the hardest part in wrecking semantic wiki

1063
01:11:15,250 --> 01:11:22,180
but no way to technical means it so to social things happening

1064
01:11:22,220 --> 01:11:24,360
so we started to talk about that

1065
01:11:24,500 --> 01:11:29,210
and the collaborative ontology if in this case if it just hierarchy

1066
01:11:30,370 --> 01:11:33,570
for example this is the page about two trust

1067
01:11:33,590 --> 01:11:38,840
so what people people who attacked is being true to category to to there is

1068
01:11:38,840 --> 01:11:41,110
a description of human readable

1069
01:11:41,130 --> 01:11:45,150
about two just saying it uses the person that gives the cost of the tree

1070
01:11:45,150 --> 01:11:48,610
of similar events and they are in

1071
01:11:48,710 --> 01:11:51,190
this example two which inside the system

1072
01:11:52,860 --> 01:11:57,260
other things we can do is say OK tutors all belong to to category person

1073
01:11:57,280 --> 01:11:58,610
so this is the

1074
01:11:58,630 --> 01:12:03,170
in this case up subclass class of relation as we know it from rdfs

1075
01:12:03,190 --> 01:12:08,440
and this is the now knows that everyone was in the category twitter also indicated

1076
01:12:11,400 --> 01:12:17,320
the human readable interpretation is what's important to the community because that's where fit the

1077
01:12:17,320 --> 01:12:20,860
look up OK what do i mean if this term and how can i reuse

1078
01:12:21,650 --> 01:12:25,320
if they disagree with the can i going changes

1079
01:12:25,340 --> 01:12:29,360
when the can go on the discussion page every page on the wiki peter property

1080
01:12:29,360 --> 01:12:34,720
page and on instance page category page has discussion pages you can subscribe to get

1081
01:12:34,720 --> 01:12:39,630
informed if to get changed and other things so you can discuss

1082
01:12:40,550 --> 01:12:45,820
properties within the community and come up with a common vocabulary for this fact

1083
01:12:46,960 --> 01:12:51,590
category pages are also the page is spoken and that information like this

1084
01:12:51,590 --> 01:12:58,380
concert i'm talking here about actually refers is it's the same thing as the

1085
01:13:00,210 --> 01:13:04,380
term for tudor or whatever to keep that's where you can use external your eyes

1086
01:13:04,380 --> 01:13:10,360
as well

1087
01:13:13,820 --> 01:13:15,650
what i'd like to do later

1088
01:13:15,670 --> 01:13:20,740
i'm always kicking this things are what i tried to come up later the mu

1089
01:13:20,740 --> 01:13:24,030
is to come up with a vocabulary

1090
01:13:24,050 --> 01:13:28,650
where we can describe the group here we can describe the terms that we want

1091
01:13:28,650 --> 01:13:33,620
to use to describe the group which properties you want to use which categories want

1092
01:13:33,620 --> 01:13:36,860
to use and how to present them befriend the

1093
01:13:36,880 --> 01:13:38,940
we keep

1094
01:13:38,960 --> 01:13:42,150
well we can then do is just browse to begin to see the information is

1095
01:13:42,150 --> 01:13:47,860
see how it's connected this is basically are simplification of graph you and only you

1096
01:13:47,860 --> 01:13:52,360
can we get something much nicer than that area forget about show that later so

1097
01:13:52,360 --> 01:13:58,320
basically this means that incoming links so korean topics has body was found

1098
01:13:58,340 --> 01:14:04,280
whatever that means that was found polytechnic college is in the city was found and

1099
01:14:04,480 --> 01:14:09,650
was kind of information this is outgoing grand opening was on your dialect

1100
01:14:09,670 --> 01:14:12,360
wrong some dialects and so on

1101
01:14:12,380 --> 01:14:16,210
you see the examples actually recreate for isoleucine you see last year

1102
01:14:16,210 --> 01:14:18,360
which was in the percent

1103
01:14:18,360 --> 01:14:20,650
i'm sorry i didn't have time to update it

1104
01:14:20,720 --> 01:14:25,260
so when we have all this information to wikipedia

1105
01:14:25,280 --> 01:14:30,320
why should actually use of about entering the data i have just added into the

1106
01:14:30,320 --> 01:14:33,260
basic idea is that you can also query

1107
01:14:33,260 --> 01:14:35,610
the information you have inside

1108
01:14:36,150 --> 01:14:37,490
writing queries

1109
01:14:37,510 --> 01:14:44,130
looks very much like writing annotations basically if you want to know what the conferences

1110
01:14:44,150 --> 01:14:47,070
you just type in the same thing to do would have type four

1111
01:14:47,630 --> 01:14:49,940
taking a page is being conference

1112
01:14:49,990 --> 01:14:52,280
and then you can can actually that's OK

1113
01:14:52,300 --> 01:14:55,110
which pages conference

1114
01:14:55,130 --> 01:14:58,880
and then you can ask dedicated so in this case the answer can see a

1115
01:14:58,880 --> 01:15:02,990
little bit down here is saying OK to triple-a i two thousand seven was located

1116
01:15:02,990 --> 01:15:07,030
in vancouver to play at two thousand eight in chicago and so on to the

1117
01:15:07,030 --> 01:15:10,210
ottawa conference happened

1118
01:15:11,240 --> 01:15:14,510
it it consists of query atoms

1119
01:15:14,530 --> 01:15:18,530
once this category is basically asking for the time

1120
01:15:18,550 --> 01:15:20,530
hierarchies are

1121
01:15:20,590 --> 01:15:26,240
in this case take into account

1122
01:15:26,260 --> 01:15:29,440
you can put content conditions on the properties

1123
01:15:29,760 --> 01:15:33,340
which means for example located in tenerife

1124
01:15:34,320 --> 01:15:35,610
you can also

1125
01:15:35,650 --> 01:15:40,280
but these conditions with if it's if the red you actually be order you can

1126
01:15:40,800 --> 01:15:44,740
ask for anything bigger or smaller than a certain value so for all the people

1127
01:15:44,740 --> 01:15:51,320
that are bigger than one seventeen rec semantic web community for example are all papers

1128
01:15:51,320 --> 01:15:53,220
starting with the letter l

1129
01:15:53,240 --> 01:15:54,260
but like this

1130
01:15:54,440 --> 01:15:57,670
you can ask for

1131
01:15:57,690 --> 01:16:02,050
things that have value on certain property at all this

1132
01:16:02,050 --> 01:16:02,990
but in class

1133
01:16:02,990 --> 01:16:06,170
or you can print print statements saying that OK

1134
01:16:06,170 --> 01:16:09,320
as i said what change

1135
01:16:15,900 --> 01:16:17,050
in the war

1136
01:16:18,280 --> 01:16:19,490
this is

1137
01:16:24,440 --> 01:16:26,780
organizational forms part

1138
01:16:26,800 --> 01:16:29,530
so i think

1139
01:16:30,690 --> 01:16:36,010
the answer to this so one model that gives the densification is

1140
01:16:36,030 --> 01:16:39,900
is is is basically you seem like a community hierarchy

1141
01:16:39,920 --> 01:16:42,510
right so you assume that i know you have some

1142
01:16:42,510 --> 01:16:43,840
people here

1143
01:16:43,860 --> 01:16:47,070
but then joined into departments

1144
01:16:48,170 --> 01:16:49,630
OK here are two more

1145
01:16:49,820 --> 01:16:53,340
right and then these these people are again

1146
01:16:53,360 --> 01:16:55,710
all right i had this whole just

1147
01:16:55,760 --> 01:16:59,780
and that's going community hierarchy right and then if i assume

1148
01:16:59,820 --> 01:17:03,240
that the probability of the nodes linking

1149
01:17:03,710 --> 01:17:08,900
so let's call it f of h is like some constant

1150
01:17:08,920 --> 01:17:14,070
two minus h where h is the tree distance so this list of nodes that

1151
01:17:14,170 --> 01:17:19,820
distance one these distance to write it on many levels in the hierarchy to go

1152
01:17:19,860 --> 01:17:20,550
to meet

1153
01:17:20,570 --> 01:17:24,070
so basically so if you assume that people from the same

1154
01:17:24,110 --> 01:17:29,300
same communities tend to talk more than people from five communities that if you just

1155
01:17:29,420 --> 01:17:32,610
assume this this will give you network to densify

1156
01:17:32,610 --> 01:17:37,590
so the larger the community that the larger the discrete hierarchy the more people you

1157
01:17:37,590 --> 01:17:41,820
have the more the the the more edges just get so

1158
01:17:41,840 --> 01:17:46,210
to some power in this power depends on this constant

1159
01:17:46,240 --> 01:17:50,130
right so basically that's how hard it is to cross communities so if this is

1160
01:17:50,130 --> 01:17:52,070
this constant

1161
01:17:52,090 --> 01:17:55,550
on one spectrum of the core of of the

1162
01:17:55,570 --> 01:18:00,940
this constant we basically say that the there is no hierarchy everyone can talk to

1163
01:18:00,940 --> 01:18:05,070
everyone else and everyone but everyone else and you get a quadratic growth

1164
01:18:05,090 --> 01:18:06,550
on the other side

1165
01:18:06,570 --> 01:18:10,740
you can just get people just talk inside their own community and it's too hard

1166
01:18:10,780 --> 01:18:13,190
perform cross cross-community links

1167
01:18:13,190 --> 01:18:18,820
and so this type of process will give the sification where the where the exponent

1168
01:18:18,820 --> 01:18:21,260
will be a function of this but parameters

1169
01:18:22,960 --> 01:18:27,860
that's all

1170
01:18:28,480 --> 01:18:30,260
it interesting to see

1171
01:18:30,280 --> 01:18:37,110
but should should be in the web graph

1172
01:18:37,130 --> 01:18:37,990
we have

1173
01:18:39,550 --> 01:18:45,650
no i can tell you that you want

1174
01:18:50,150 --> 01:18:55,490
two which

1175
01:18:55,510 --> 01:19:00,670
the c

1176
01:19:00,670 --> 01:19:05,440
because of fast solution

1177
01:19:05,460 --> 01:19:08,210
so we should

1178
01:19:16,170 --> 01:19:18,440
now you two

1179
01:19:18,460 --> 01:19:22,420
exact solution it seems to

1180
01:19:22,420 --> 01:19:23,690
we use

1181
01:19:30,820 --> 01:19:32,630
one of the nation's

1182
01:19:35,530 --> 01:19:39,740
it was my identification and the changing their so

1183
01:19:39,760 --> 01:19:42,840
maybe i should have a slide

1184
01:19:45,320 --> 01:19:49,840
OK so i imagine this is your current social networking now you can't you can't

1185
01:19:49,990 --> 01:19:53,010
part so right that

1186
01:19:53,030 --> 01:19:56,940
that's your right you come to the party and you have someone who introduces you're

1187
01:19:56,960 --> 01:19:58,990
right now the idea is that

1188
01:19:59,200 --> 01:20:03,410
because small forest five so the idea is you start fire here this person the

1189
01:20:03,540 --> 01:20:08,690
men first met the bottom going into introduce you are right so with some probability

1190
01:20:08,690 --> 01:20:12,800
they will introduce you here and you will will become friends with this guy

1191
01:20:12,830 --> 01:20:16,530
and then this guy was going introduce you to other people right so

1192
01:20:17,660 --> 01:20:18,900
they introduced they

1193
01:20:19,000 --> 01:20:21,920
they did not introduce you to that person or you did not create a link

1194
01:20:21,950 --> 01:20:25,300
to that person but you created to this person

1195
01:20:26,240 --> 01:20:29,550
this is what happens right so basically you can say that

1196
01:20:29,570 --> 01:20:31,760
you add a new node based by

1197
01:20:31,780 --> 01:20:33,840
by starting

1198
01:20:33,860 --> 01:20:34,910
some kind of

1199
01:20:35,400 --> 01:20:39,760
why propagation of five propagation from one of the existing nodes and then you and

1200
01:20:39,760 --> 01:20:43,370
then you linked all the nodes that that so i think this goes into the

1201
01:20:43,460 --> 01:20:46,700
what was saying and if you do this

1202
01:20:46,710 --> 01:20:49,040
then the graphs you are getting

1203
01:20:49,080 --> 01:20:52,580
are so here is the number of nodes and the number of edges so we

1204
01:20:52,580 --> 01:20:54,160
get graphs that densify

1205
01:20:54,210 --> 01:20:57,580
and they also get the ship ever shrinking diameter

1206
01:20:57,620 --> 01:21:07,980
capture the

1207
01:21:17,870 --> 01:21:21,500
i don't know which

1208
01:21:21,510 --> 01:21:26,450
which is to say that

1209
01:21:26,450 --> 01:21:30,250
so in other words just put all formulas one she we have an optimal control

1210
01:21:30,250 --> 01:21:32,300
solutions which is of this form

1211
01:21:32,300 --> 01:21:35,760
where we can now we can write it as a path integral

1212
01:21:35,780 --> 01:21:38,810
we can take this to also to the pastoral where s is not the past

1213
01:21:38,810 --> 01:21:40,330
contribution plus and terms

1214
01:21:40,390 --> 01:21:42,240
and so

1215
01:21:42,320 --> 01:21:44,180
we can write this whole thing

1216
01:21:44,450 --> 01:21:49,300
as a free energy because j is minds local size so we just get exactly

1217
01:21:49,300 --> 01:21:54,890
free energy contribution where we you normally in free energy for some over exponent of

1218
01:21:54,890 --> 01:21:55,780
the energy

1219
01:21:55,780 --> 01:22:01,160
here we get this sum of the exponents of of an action and also divided

1220
01:22:01,160 --> 01:22:04,000
by new which is later of temperature

1221
01:22:04,020 --> 01:22:06,580
so if this is the partition so

1222
01:22:06,590 --> 01:22:09,730
the corresponding probability distribution

1223
01:22:09,740 --> 01:22:11,410
is this quantity

1224
01:22:11,430 --> 01:22:12,690
right so this is the

1225
01:22:12,700 --> 01:22:13,990
the normalizer

1226
01:22:14,000 --> 01:22:17,740
and the probability distribution is just the exponent which in the past that you can

1227
01:22:17,740 --> 01:22:21,080
think of conditional on the current position where you are

1228
01:22:21,190 --> 01:22:25,400
so you are here you can consider very many paths each of them has a

1229
01:22:25,400 --> 01:22:27,530
certain cost as

1230
01:22:27,540 --> 01:22:31,350
which takes into account the past contribution in the end cost and then you have

1231
01:22:31,350 --> 01:22:35,540
to then this is the probability of all these patterns

1232
01:22:35,620 --> 01:22:39,160
so far

1233
01:22:39,200 --> 01:22:42,010
to just put put together give a little it's while in

1234
01:22:44,380 --> 01:22:48,200
you know

1235
01:22:48,330 --> 01:22:50,110
and that

1236
01:22:50,140 --> 01:22:54,160
this is just

1237
01:22:54,170 --> 01:23:02,830
the probability will arise here once i

1238
01:23:04,860 --> 01:23:17,050
so this is an example using you'll see the right so

1239
01:23:17,100 --> 01:23:19,640
now we have such a

1240
01:23:19,700 --> 01:23:22,990
the last thing i want to talk about is how you can actually use this

1241
01:23:23,000 --> 01:23:28,070
to solve for efficiently some very intractable problem and this is the case of coordination

1242
01:23:28,070 --> 01:23:32,660
of of the number of agents so now we have not one of these processes

1243
01:23:32,660 --> 01:23:37,430
but we have an of them which are agents and they each have independent dynamics

1244
01:23:37,430 --> 01:23:39,110
which is given by f l five

1245
01:23:39,130 --> 01:23:43,230
and they have independent controls which are you alpha and have independent noise images kci

1246
01:23:44,100 --> 01:23:49,200
but their actions should be coordinated in the sense that they have to jointly solve

1247
01:23:49,200 --> 01:23:53,200
some task which is which is measured at a future time so for instance they

1248
01:23:53,200 --> 01:23:56,890
have to extinguish number of fires and they have to all run around two all

1249
01:23:56,890 --> 01:24:00,470
run to the model to the same fire extinguisher one of these fires would that

1250
01:24:00,520 --> 01:24:05,020
sort of cornish pasty that in time there is an equal distribution of agents over

1251
01:24:05,020 --> 01:24:10,110
over five years so we have an and cost which depends on the and position

1252
01:24:10,110 --> 01:24:15,190
of all the agents and we assume that these ages positions are restricted to these

1253
01:24:15,190 --> 01:24:19,240
and to any of these fire locations for each other agent

1254
01:24:19,240 --> 01:24:23,040
and position is in the case in an infinite cost for any of the other

1255
01:24:23,040 --> 01:24:25,320
and configuration

1256
01:24:25,330 --> 01:24:27,590
so then we get exactly the same formula

1257
01:24:27,610 --> 01:24:29,100
we get that

1258
01:24:29,150 --> 01:24:33,340
this partition sum is now some of the integral over all the

1259
01:24:33,590 --> 01:24:37,890
h positions the dynamics to go from x to y

1260
01:24:37,930 --> 01:24:44,040
factorizes as the forward diffusion of each of these agents to move from x alpha

1261
01:24:44,040 --> 01:24:47,910
two while find a completely independent and so you just get the product of these

1262
01:24:47,910 --> 01:24:52,540
of these independent agent dynamics and then the whole thing is penalized by an and

1263
01:24:52,540 --> 01:24:56,320
cost where all these and costs are coupled through his phi

1264
01:24:56,370 --> 01:24:57,660
and so

1265
01:24:57,660 --> 01:25:00,720
we can write this as a

1266
01:25:00,740 --> 01:25:05,820
since the end cost are restricted to these look discrete locations we can which are

1267
01:25:05,850 --> 01:25:10,510
fires we can restrict this we can try to some this integral is the sum

1268
01:25:10,510 --> 01:25:15,430
and we get here sort of energy expression where we put his role in the

1269
01:25:15,490 --> 01:25:16,600
in the exponent

1270
01:25:16,780 --> 01:25:20,950
and so we have we defined this PY

1271
01:25:20,970 --> 01:25:24,680
as this probability distributions and with before

1272
01:25:25,730 --> 01:25:27,390
then the optimal control

1273
01:25:27,410 --> 01:25:29,680
which is the gradient of j

1274
01:25:29,680 --> 01:25:33,410
which is the gradient of the log of site it becomes

1275
01:25:33,820 --> 01:25:38,450
this kind of an expectation that we have to compute so we have

1276
01:25:38,450 --> 01:25:40,350
to take the gradient of

1277
01:25:41,950 --> 01:25:45,930
we have to compute dynamics for each of the agents take the gradient with respect

1278
01:25:45,930 --> 01:25:49,100
to the initial position of that agency

1279
01:25:49,120 --> 01:25:51,220
and that gives us some formula

1280
01:25:51,220 --> 01:25:54,140
and that is OK we can do that but then we have to take the

1281
01:25:54,140 --> 01:26:00,280
expectation value with respect to this probability distribution which is really a probability distribution over

1282
01:26:00,280 --> 01:26:01,180
all the

1283
01:26:01,180 --> 01:26:04,950
and positions of all the agents and the entire

1284
01:26:04,950 --> 01:26:06,850
right so we now see

1285
01:26:06,870 --> 01:26:13,700
that is coordination task has become a graphical model inference task because if we think

1286
01:26:14,300 --> 01:26:17,260
if we think of this probability distribution

1287
01:26:17,320 --> 01:26:19,700
as a graphical model which is basically is

1288
01:26:21,700 --> 01:26:29,320
then taking to computing this expectation value is just depends on the single very also

1289
01:26:29,320 --> 01:26:30,850
like computing marginal

1290
01:26:30,870 --> 01:26:32,820
four y alpha

1291
01:26:32,820 --> 01:26:38,870
in in the high dimensional state state vector y going from one to two and

1292
01:26:38,890 --> 01:26:40,370
so which is the marginal

1293
01:26:40,450 --> 01:26:42,660
and for computation

1294
01:26:43,780 --> 01:26:48,200
the city of CO two to to to solve this is then you compute the

1295
01:26:49,390 --> 01:26:53,780
and the longer if for each agent to move to each target so each agent

1296
01:26:53,780 --> 01:26:59,280
is an agent alpha is the location x alpha where currently is compute for each

1297
01:26:59,280 --> 01:27:00,720
of the target location

1298
01:27:00,740 --> 01:27:03,160
what what the cost is in the absence

1299
01:27:03,180 --> 01:27:05,870
of the of the control

1300
01:27:05,890 --> 01:27:09,640
and of course is going to be but this factorized basically problem of all the

1301
01:27:09,640 --> 01:27:12,680
agents of you to do for each separately can still be difficult but you can

1302
01:27:12,680 --> 01:27:17,050
do a possible can be is made using monte carlo sampling or something maybe variational

1303
01:27:18,370 --> 01:27:20,430
and then once we have these terms

1304
01:27:20,450 --> 01:27:27,260
we have to do we have to do a graphical model inference in that couples

1305
01:27:27,260 --> 01:27:29,870
all these these other all these

1306
01:27:29,890 --> 01:27:31,780
to to basically put it in here

1307
01:27:31,800 --> 01:27:36,740
and then do this graph will inference in that case and you can do to

1308
01:27:36,740 --> 01:27:39,430
maybe exact the graphical model is small

1309
01:27:39,530 --> 01:27:42,930
you can use BP or mean field approximation

1310
01:27:42,970 --> 01:27:44,490
so in a simple

1311
01:27:44,700 --> 01:27:48,660
example or the state space is one-dimensional and we have an agent's

1312
01:27:48,890 --> 01:27:53,470
and we have no dynamics then this conditional distribution is just the gaussians

1313
01:27:53,490 --> 01:27:56,910
and in the end cost just as well i want all the part of the

1314
01:27:57,050 --> 01:27:59,490
fires to have the same number of agents

1315
01:27:59,510 --> 01:28:02,700
then we get this kind of a quadratic form

1316
01:28:02,850 --> 01:28:06,800
and then we can compute the optimal control and you can control is now given

1317
01:28:06,800 --> 01:28:09,180
by the expectation value of y

1318
01:28:09,200 --> 01:28:13,180
t t in this graphical model minus the current position

1319
01:28:14,300 --> 01:28:18,640
this thing is the is now the intractable quantity that we have to compute it

1320
01:28:18,640 --> 01:28:20,240
is the expected value

1321
01:28:21,890 --> 01:28:25,200
at the future for agent alpha in the context of what all the other people

1322
01:28:25,200 --> 01:28:29,760
are doing so to summary of all this uncertainty of all the list of all

1323
01:28:29,760 --> 01:28:33,530
the other i'd like to see it here is simulations

1324
01:28:33,550 --> 01:28:35,090
so here is the beginning of time

1325
01:28:35,140 --> 01:28:38,910
the end of time here is five fires

1326
01:28:38,930 --> 01:28:44,120
and here is a bunch of agents and they start to some locations use what

1327
01:28:44,120 --> 01:28:46,890
i've plotted here is the expected

1328
01:28:47,140 --> 01:28:51,720
target location of each of these agents as a function of time so these blue

1329
01:28:51,720 --> 01:28:57,870
agent reasons things initially he should go to to the target located zero

1330
01:28:57,890 --> 01:29:01,490
and then at some point something happens because the other agents are doing something and

1331
01:29:01,490 --> 01:29:05,370
he said well known that actually that i go to target minus two

1332
01:29:05,390 --> 01:29:08,620
and and then one she for so once you have york

1333
01:29:08,640 --> 01:29:10,950
s you expect expected target location

1334
01:29:10,950 --> 01:29:15,030
data jointly with the parameter theta

1335
01:29:15,070 --> 01:29:17,640
but what i'm interested in is now

1336
01:29:17,700 --> 01:29:21,470
i observe the actual sequence data

1337
01:29:22,280 --> 01:29:25,280
do i know about the parameters theta

1338
01:29:25,300 --> 01:29:30,210
what is my gaining knowledge what is my change in in ignorance what this might

1339
01:29:30,210 --> 01:29:31,760
change in

1340
01:29:33,220 --> 01:29:38,720
i don't know what is what what's my change in how much information do i

1341
01:29:38,720 --> 01:29:42,550
gained what's my how does my knowledge about say the change if i have seen

1342
01:29:43,740 --> 01:29:46,910
good so bayesians would use

1343
01:29:46,930 --> 01:29:49,640
this result

1344
01:29:49,640 --> 01:29:51,410
so by definition

1345
01:29:51,410 --> 01:29:53,530
if i

1346
01:29:53,550 --> 01:29:57,870
compute this i get the conditional

1347
01:29:57,890 --> 01:30:04,680
distribution of the data given the this is called the subject is called the posterior

1348
01:30:06,550 --> 01:30:09,660
the posterior density

1349
01:30:12,510 --> 01:30:15,140
of data given

1350
01:30:17,050 --> 01:30:18,370
the data

1351
01:30:19,470 --> 01:30:21,720
but the density

1352
01:30:21,760 --> 01:30:26,200
the answer of the bayesian is not i think this data is is is is

1353
01:30:26,200 --> 01:30:28,010
the most likely thing

1354
01:30:28,090 --> 01:30:31,950
the bayesian comes up with the density over theta

1355
01:30:31,970 --> 01:30:37,390
saying which they does are more likely to be responsible

1356
01:30:37,410 --> 01:30:40,800
and this incorporates two things the prior knowledge

1357
01:30:40,820 --> 01:30:45,910
which is this prior and this term is actually the likelihood because it's the probability

1358
01:30:45,910 --> 01:30:47,780
of the data given

1359
01:30:47,800 --> 01:30:53,240
the parameters of the bayesian takes the likelihood multiplies by the prior to this guy

1360
01:30:53,240 --> 01:30:55,300
is just the normalizer

1361
01:30:55,340 --> 01:30:58,430
it makes things normalized to one

1362
01:30:58,430 --> 01:31:03,050
so can turns out of this things comes out to be

1363
01:31:03,100 --> 01:31:05,490
just the normalizing integral right

1364
01:31:05,570 --> 01:31:06,570
because this

1365
01:31:06,590 --> 01:31:08,490
for the

1366
01:31:10,490 --> 01:31:13,470
tossing experiment this density

1367
01:31:13,490 --> 01:31:18,260
this parameter theta is between zero and one so if i integrated

1368
01:31:18,260 --> 01:31:24,740
from zero to one this whole thing should be one that means this normalizer

1369
01:31:24,800 --> 01:31:27,030
must then be the integral

1370
01:31:27,090 --> 01:31:30,970
right so this is the posterior

1371
01:31:30,990 --> 01:31:32,160
let's see

1372
01:31:32,200 --> 01:31:33,640
for the specific

1373
01:31:33,840 --> 01:31:36,990
coin tossing experiment

1374
01:31:37,010 --> 01:31:38,930
i just tried this out

1375
01:31:41,120 --> 01:31:44,140
did this for a couple of

1376
01:31:45,320 --> 01:31:47,530
for ten fifty and one hundred

1377
01:31:48,530 --> 01:31:49,860
so you see

1378
01:31:49,860 --> 01:31:51,530
this is the posterior

1379
01:31:53,090 --> 01:31:59,720
talk about yes so first of all this is the posterior so the bayesians if

1380
01:31:59,720 --> 01:32:03,260
they forced to do certain things they would say this is the answer right i

1381
01:32:03,260 --> 01:32:05,990
mean this is my knowledge about

1382
01:32:06,340 --> 01:32:08,680
about what i have after i've seen

1383
01:32:08,700 --> 01:32:12,870
three data points so this is my knowledge

1384
01:32:12,910 --> 01:32:17,860
before my knowledge was like that everything is equally probable i've seen three data points

1385
01:32:18,160 --> 01:32:19,780
so this is why i get

1386
01:32:19,800 --> 01:32:24,910
so to make right so still says you know it's more likely that the right

1387
01:32:24,910 --> 01:32:30,740
parameter came from here but it's fairly uncertain and i'm actually can say how uncertain

1388
01:32:30,740 --> 01:32:33,370
this is relative to my prior beliefs

1389
01:32:33,800 --> 01:32:37,100
now if you see more and more data points than

1390
01:32:37,280 --> 01:32:41,760
you see here becoming like this becoming like that and this was a hundred data

1391
01:32:41,760 --> 01:32:44,590
points is actually looks like the gaussians density

1392
01:32:46,620 --> 01:32:47,820
well you see it's

1393
01:32:47,840 --> 01:32:52,140
the true parameter was actually point seven so it seems like it's fairly

1394
01:32:52,160 --> 01:32:53,660
well centered about

1395
01:32:53,680 --> 01:32:55,470
o point seven

1396
01:32:55,490 --> 01:33:00,450
and fine so what you could try and do and say well if you really

1397
01:33:00,450 --> 01:33:04,390
forced give me an answer i want to know what your parameter is please tell

1398
01:33:04,390 --> 01:33:07,300
me what's what's the right OK so i can say is

1399
01:33:09,370 --> 01:33:10,910
you could even say

1400
01:33:10,910 --> 01:33:15,430
the map well this is the one that maximizes this posterior

1401
01:33:15,450 --> 01:33:21,170
so it's OK this but then what it doesn't sort of incorporate the shape right

1402
01:33:21,170 --> 01:33:24,010
because the maximum should not be the mean and so on

1403
01:33:24,100 --> 01:33:29,920
another popular thing is just the posterior mean and the posterior mean you take your

1404
01:33:29,920 --> 01:33:33,910
posterior and calculate the mean by doing an integral

1405
01:33:33,970 --> 01:33:39,220
so what you would get it's actually the funny thing is you don't get the

1406
01:33:39,220 --> 01:33:42,800
the probability that the bayes estimate

1407
01:33:44,010 --> 01:33:48,840
the probability of having one is not in one over n

1408
01:33:49,370 --> 01:33:52,890
but it's in one plus one over and plus to

1409
01:33:53,140 --> 01:33:57,780
and so it looks like bayesian estimate looks slightly different depends on the prior because

1410
01:33:57,780 --> 01:34:00,280
of prior knowledge that i had

1411
01:34:00,320 --> 01:34:05,180
and sort of this is always a useful thing for things to regularize i mean

1412
01:34:05,200 --> 01:34:09,090
in some cases if you say well look i mean i don't have many data

1413
01:34:09,090 --> 01:34:11,530
and in some cases some

1414
01:34:11,550 --> 01:34:15,100
maybe you don't have to call but you have something they can have multiple outputs

1415
01:34:15,620 --> 01:34:20,120
and some of the things have actually zeros do you really believe that that then

1416
01:34:20,120 --> 01:34:24,870
the estimate is zero if this thing doesn't even a short sequence of data you

1417
01:34:24,870 --> 01:34:27,240
thing doesn't really occur

1418
01:34:27,320 --> 01:34:32,930
so maybe you do you do you use this thing for making some other predictions

1419
01:34:33,180 --> 01:34:37,050
and is zero is really a very strong assumptions

1420
01:34:37,070 --> 01:34:41,120
no this doesn't occur the probability for this to happen is really zero

1421
01:34:41,140 --> 01:34:44,240
well if you have a prior belief at least

1422
01:34:44,260 --> 01:34:49,240
that's something non-zero and still there is the bayes method would give you

1423
01:34:49,660 --> 01:34:51,550
kind of something

1424
01:34:51,550 --> 01:34:55,910
even if it's a very small count that comes out and you have the yes

1425
01:34:57,970 --> 01:35:04,030
well it's going well there's this the question of what is and what is the

1426
01:35:04,030 --> 01:35:07,570
bayesian ignorance so you have to

1427
01:35:07,620 --> 01:35:10,820
even though is the theory about what's the proper way of saying i don't know

1428
01:35:10,820 --> 01:35:16,590
in bayesian terms it's not quite the flat prior which i can't explain at the

1429
01:35:16,590 --> 01:35:19,570
moment i mean

1430
01:35:21,280 --> 01:35:25,390
so there's is different ways of saying i don't know but

1431
01:35:25,390 --> 01:35:29,370
and you can also learn priors from the data that's a bit fishy sound a

1432
01:35:29,370 --> 01:35:33,640
bit fishy but priors speak have also parameters in which you can

1433
01:35:33,660 --> 01:35:37,510
so sort to adapt to the data which i will demonstrate in the moment the

1434
01:35:37,530 --> 01:35:41,820
least you should see that the bayesian answer may not be equal to the maximum

1435
01:35:41,820 --> 01:35:45,090
likelihood and but in one case it's the same

1436
01:35:45,200 --> 01:35:47,430
if you just stick to the

1437
01:35:47,990 --> 01:35:50,030
to the map the most

1438
01:35:50,070 --> 01:35:52,470
probable bayesian value

1439
01:35:52,510 --> 01:35:55,860
if PFA to is equal to a constant

1440
01:35:55,870 --> 01:35:58,820
like in this case i set it equal to one

1441
01:35:59,180 --> 01:36:04,570
so if this is the one then the posterior depends on theta only via the

1442
01:36:06,470 --> 01:36:11,090
so if i just look what is the most probable value for data

1443
01:36:11,230 --> 01:36:13,530
with the flat prior

1444
01:36:13,570 --> 01:36:18,010
it is the one that maximizes this it maximizes the likelihood

1445
01:36:18,070 --> 01:36:24,780
so the bayesian MAP estimation with the flat priors equal to maximum likelihood

1446
01:36:24,800 --> 01:36:31,090
so you see somehow this has got something those are sort of related but

1447
01:36:31,100 --> 01:36:33,760
right there are differences is if there are more

1448
01:36:33,760 --> 01:36:36,810
it basically can be proven

1449
01:36:36,820 --> 01:36:43,450
in the following way you basically convert the problem of cameron two approximation of the

1450
01:36:43,450 --> 01:36:45,840
sample into a classification problem

1451
01:36:45,850 --> 01:36:48,080
by adding one extra dimensions

1452
01:36:48,120 --> 01:36:51,680
and you can actually say you know if it's within gamma two

1453
01:36:51,700 --> 01:36:57,050
a positive example of it's not you know it's correctly classified as within government two

1454
01:36:57,070 --> 01:36:58,310
above and below

1455
01:36:58,320 --> 01:37:01,440
so we make two examples from each original example

1456
01:37:02,700 --> 01:37:07,540
the original weight vector actually solve that classification problem

1457
01:37:07,580 --> 01:37:09,780
with margin gamma two

1458
01:37:09,790 --> 01:37:12,110
so you apply the perceptron algorithm

1459
01:37:12,130 --> 01:37:14,110
and the perceptron algorithm

1460
01:37:14,140 --> 01:37:15,940
always adds in

1461
01:37:15,950 --> 01:37:19,100
the training examples into the weight vector

1462
01:37:19,100 --> 01:37:21,120
and we know by the

1463
01:37:21,180 --> 01:37:26,280
the perceptron convergence theorem that the number of updates makes is at most this number

1464
01:37:26,280 --> 01:37:29,110
and so the sum it will happen firstly

1465
01:37:29,120 --> 01:37:33,290
a discrete you know during presentations

1466
01:37:33,420 --> 01:37:36,450
in other words with integer values here

1467
01:37:36,540 --> 01:37:40,360
and furthermore the some of those will be correspond to the number of updates that

1468
01:37:40,360 --> 01:37:43,930
were made and that will be less than a task on gamma square

1469
01:37:43,990 --> 01:37:48,150
so that we know that by just running the perceptron algorithm for any weight vector

1470
01:37:48,150 --> 01:37:49,780
will find some

1471
01:37:49,780 --> 01:37:51,540
the weight vector in this class

1472
01:37:51,550 --> 01:37:53,410
that is a good approximation

1473
01:37:53,470 --> 01:37:56,910
of the of that original weight vector

1474
01:37:56,920 --> 01:37:58,890
so it's a very neat trick

1475
01:37:58,910 --> 01:38:03,220
and it actually gives you a bound for the size of the covered by

1476
01:38:03,260 --> 01:38:07,540
just counting the size of that set which you can do by sort of combinatorial

1477
01:38:07,540 --> 01:38:09,480
arguments basically putting

1478
01:38:09,530 --> 01:38:12,870
you know a certain number of letters in a certain number of pigeon holes in

1479
01:38:12,870 --> 01:38:14,800
this how many ways you can do that

1480
01:38:15,110 --> 01:38:19,610
so it's it's sort of standard combinatorial question and you can actually banned

1481
01:38:19,620 --> 01:38:20,610
this number

1482
01:38:21,390 --> 01:38:25,530
the log of this covering number by this quantity here k where k

1483
01:38:25,600 --> 01:38:29,370
is this value eight r-squared on gamma square

1484
01:38:29,380 --> 01:38:33,650
so it's i just sort of fell you know that's probably easier than just spouting

1485
01:38:33,650 --> 01:38:37,810
general talks about covering numbers

1486
01:38:37,880 --> 01:38:42,130
so i think it

1487
01:38:42,200 --> 01:38:50,040
the no because you can update more than once on the same example

1488
01:38:50,040 --> 01:38:55,490
but it will be the case that all the positive points always have plus something

1489
01:38:55,490 --> 01:38:58,420
and all the negative minus so you could reduce

1490
01:38:58,470 --> 01:39:00,550
but it will be

1491
01:39:00,680 --> 01:39:02,970
in some instructions

1492
01:39:04,450 --> 01:39:07,840
so it so there's a little bit more detail that you need to you know

1493
01:39:07,840 --> 01:39:11,740
why the normalized by that number here z and so on a little bit more

1494
01:39:11,950 --> 01:39:13,540
you basically that's the idea

1495
01:39:15,290 --> 01:39:18,770
OK so now we can put it all together with the statistical result once got

1496
01:39:18,770 --> 01:39:23,010
that idea of recovery in mind the same sort of outline of proof applies here

1497
01:39:23,010 --> 01:39:25,940
we are we've got the error zero

1498
01:39:26,530 --> 01:39:27,420
true error

1499
01:39:28,710 --> 01:39:31,560
and a large margin

1500
01:39:31,570 --> 01:39:34,910
and we move to the double sample trick same thing

1501
01:39:35,560 --> 01:39:39,960
we retain the large margin on the first half of those of the data

1502
01:39:40,620 --> 01:39:45,920
and we just now moved from having a true error that belong to a second-half

1503
01:39:45,920 --> 01:39:48,130
sample error one on two

1504
01:39:48,180 --> 01:39:51,580
and the factor two in here that's exactly the same as before

1505
01:39:51,590 --> 01:39:52,940
and now we

1506
01:39:52,960 --> 01:39:54,720
we're going to

1507
01:39:54,730 --> 01:39:58,070
do this by replacing this sort of

1508
01:39:58,110 --> 01:40:03,700
there exists over all of the functions by the functions in the in the cupboard

1509
01:40:03,720 --> 01:40:06,180
and as i suggested before

1510
01:40:07,810 --> 01:40:11,920
in the cover can be found which is within gamma on two of this function

1511
01:40:11,920 --> 01:40:13,680
may be messing us up here

1512
01:40:13,790 --> 01:40:18,290
and that will mean that it has a margin least gamma on two on this

1513
01:40:18,300 --> 01:40:19,690
half of the sample

1514
01:40:19,710 --> 01:40:23,210
and the misclassified points of margin less than gamma on two

1515
01:40:23,650 --> 01:40:26,070
and that's what that symbol means here

1516
01:40:27,230 --> 01:40:28,380
on their

1517
01:40:28,390 --> 01:40:29,930
the second half of the sample

1518
01:40:29,940 --> 01:40:31,880
and so this is again

1519
01:40:31,880 --> 01:40:32,540
you know

1520
01:40:32,880 --> 01:40:37,450
from the symmetrisation viewpoint a strange situation because you seem to have

1521
01:40:37,470 --> 01:40:40,910
lots of good margins everything with a good margin on this side and some things

1522
01:40:40,910 --> 01:40:46,170
with less margin on the side and again symmetrisation argument says that's shouldn't happen

1523
01:40:46,220 --> 01:40:49,870
and you get to the my sets on over two coming in

1524
01:40:49,940 --> 01:40:52,070
and setting that equal to delta

1525
01:40:52,110 --> 01:40:56,290
and converting into a bound on eps along

1526
01:40:56,300 --> 01:40:58,540
you arrive at this

1527
01:40:58,590 --> 01:41:00,400
expression here

1528
01:41:00,420 --> 01:41:06,360
we're now the logo of the covering numbers has replaced VC dimension expression that we

1529
01:41:06,360 --> 01:41:08,140
had in the previous

1530
01:41:08,180 --> 01:41:11,310
results OK

1531
01:41:11,310 --> 01:41:15,540
we would need to treat how to handle different gammas

1532
01:41:15,690 --> 01:41:20,650
but we can do that by applying their we can really apply structural risk minimisation

1533
01:41:20,650 --> 01:41:22,320
over the choice of gamma

1534
01:41:22,330 --> 01:41:26,130
and we can therefore make this bound to hold for

1535
01:41:26,180 --> 01:41:27,180
for any

1536
01:41:27,190 --> 01:41:30,360
the observed value of gamma

1537
01:41:30,370 --> 01:41:33,930
so now what does this tell us well we just need to work out what

1538
01:41:33,930 --> 01:41:35,860
this is but we've already done that

1539
01:41:35,870 --> 01:41:37,430
and so we can plug

1540
01:41:37,460 --> 01:41:38,620
that in

1541
01:41:39,190 --> 01:41:41,270
now in general

1542
01:41:41,960 --> 01:41:45,970
you need to you know for general function classes you need to have a method

1543
01:41:45,970 --> 01:41:49,930
of computing this i what i did was just for linear function classes or come

1544
01:41:49,930 --> 01:41:52,620
back to that in a minute which is what we need of course for support

1545
01:41:52,620 --> 01:41:53,620
vector machines

1546
01:41:53,640 --> 01:41:54,950
but in general

1547
01:41:54,960 --> 01:41:59,240
you would have to have the methods of computing this and there are n now

1548
01:41:59,240 --> 01:42:02,770
analogues of the VC dimension

1549
01:42:02,780 --> 01:42:06,540
but allow you to do this so well in the VC dimension we had sours

1550
01:42:06,540 --> 01:42:08,520
lemma bounding the growth function

1551
01:42:08,570 --> 01:42:12,130
and there is an equivalent of what's called the fat shattering dimension

1552
01:42:12,190 --> 01:42:16,850
and the result due to allow on how which bound the gamma growth function which

1553
01:42:16,850 --> 01:42:22,140
is what i want that log that covering numbers so that covering number is referred

1554
01:42:22,790 --> 01:42:26,280
so you have a more general framework that applies in

1555
01:42:26,330 --> 01:42:29,070
in you know two two general classes

1556
01:42:29,340 --> 01:42:35,300
that may not be linear function classes provided they have finite fat shattering dimension

1557
01:42:35,350 --> 01:42:36,270
which is

1558
01:42:36,280 --> 01:42:37,740
again a sort of

1559
01:42:37,800 --> 01:42:41,910
analogue of the VC dimension that takes into account the margin which the shattering takes

1560
01:42:42,740 --> 01:42:45,020
then all of this goes through

1561
01:42:45,040 --> 01:42:49,660
this result i should warn is is is pretty horrendous to the proof of this

1562
01:42:49,660 --> 01:42:51,150
is is

1563
01:42:51,210 --> 01:42:56,320
scary OK and the bound is very weak

1564
01:42:56,330 --> 01:43:00,880
OK so that's that that's the general case but in the case of SVM is

1565
01:43:00,880 --> 01:43:04,980
we can just plugin found that i i got from that explicit construction

1566
01:43:05,040 --> 01:43:07,120
and here is what we end up with

1567
01:43:07,120 --> 01:43:08,390
so fixing

1568
01:43:08,410 --> 01:43:18,990
during rotation

1569
01:43:19,040 --> 01:43:27,910
it takes o one time

1570
01:43:30,680 --> 01:43:33,080
so the total insert times

1571
01:43:33,080 --> 01:43:40,600
is lot

1572
01:43:40,660 --> 01:43:50,060
so once i figured out that this is the right information because we don't know

1573
01:43:50,060 --> 01:43:53,120
what we're using this information for you but

1574
01:43:53,180 --> 01:43:58,330
once i know that's the information showing you that it works in the insert delete

1575
01:43:58,410 --> 01:44:01,290
continue working order log time is easy

1576
01:44:01,290 --> 01:44:03,100
and i do believe actually a a little bit

1577
01:44:03,950 --> 01:44:10,100
this is similar

1578
01:44:11,450 --> 01:44:14,810
and then we go through and you find something

1579
01:44:14,870 --> 01:44:17,100
they have to go through the whole business of

1580
01:44:17,310 --> 01:44:19,040
swapping have

1581
01:44:20,310 --> 01:44:26,220
you know if it's an internal node you gets swamped with its successor predecessor

1582
01:44:26,270 --> 01:44:28,750
OK and so there's a bunch of other

1583
01:44:28,750 --> 01:44:32,100
things that have to be dealt with but it's all stuff where you can update

1584
01:44:32,100 --> 01:44:33,450
the information

1585
01:44:33,470 --> 01:44:38,680
using this thing is also essentially local changes when you're updating information

1586
01:44:39,520 --> 01:44:43,060
you can do essentially only on a path from the

1587
01:44:43,120 --> 01:44:45,850
from the root and most of the trees never

1588
01:44:46,120 --> 01:44:47,370
it dealt with

1589
01:44:47,370 --> 01:44:50,540
OK so i'll leave that to you folks to to work out also in the

1590
01:44:50,540 --> 01:44:51,830
book if you

1591
01:44:54,230 --> 01:44:56,060
OK that's good exercise

1592
01:44:56,200 --> 01:45:01,470
OK so for so any questions about the first three steps

1593
01:45:04,310 --> 01:45:06,970
four step is no operations

1594
01:45:07,020 --> 01:45:15,140
OK so

1595
01:45:15,180 --> 01:45:18,310
let's see if i could do that

1596
01:45:28,470 --> 01:45:43,750
so are also tries to find

1597
01:45:54,140 --> 01:45:55,790
the interval i

1598
01:45:55,810 --> 01:45:58,200
OK so i here's an internal

1599
01:45:58,200 --> 01:46:00,410
they get to court that's

1600
01:46:00,450 --> 01:46:05,560
in this rather writing recursively were write it as a sort of every recursive already

1601
01:46:05,560 --> 01:46:08,290
with the while loop

1602
01:46:11,350 --> 01:46:14,500
equally can write recursively the other one that we wrote we could have written as

1603
01:46:14,500 --> 01:46:15,870
well as well

1604
01:46:15,890 --> 01:46:17,910
and i had the recursive call

1605
01:46:17,910 --> 01:46:20,930
so here we are going to be to start x get through

1606
01:47:08,520 --> 01:47:17,430
the current later

1607
01:47:19,390 --> 01:48:11,770
that's the co

1608
01:48:11,790 --> 01:48:14,910
so let's just see how it works

1609
01:48:14,990 --> 01:48:16,950
let's search for the interval

1610
01:48:16,970 --> 01:48:23,180
the search for the interval

1611
01:48:23,200 --> 01:48:29,000
fourteen sixteen

1612
01:48:29,200 --> 01:48:35,890
fourteen sixteen in history

1613
01:48:35,910 --> 01:48:38,730
OK so let's see

1614
01:48:38,750 --> 01:48:41,830
so says exc starts at the root

1615
01:48:42,970 --> 01:48:47,580
and now while it's not nailed was not now because the route

1616
01:48:47,600 --> 01:48:50,470
and what is this doing

1617
01:48:50,500 --> 01:49:06,450
so we tell me what that code does

1618
01:49:09,350 --> 01:49:15,580
this is well so this doing give me

1619
01:49:15,640 --> 01:49:19,560
this is just saying something between i and into vex

1620
01:49:19,600 --> 01:49:23,890
was it seems x the interval stored index

1621
01:49:23,910 --> 01:49:39,040
OK what is this testing for

1622
01:49:44,330 --> 01:49:54,000
this is testing for

1623
01:49:54,000 --> 01:49:55,540
is question

1624
01:49:55,560 --> 01:50:06,230
or below so

1625
01:50:06,290 --> 01:50:09,520
and you just simple words simple words

1626
01:50:09,520 --> 01:50:11,430
test for overlaps

1627
01:50:11,470 --> 01:50:15,230
in particular test whether they

1628
01:50:15,290 --> 01:50:20,950
do were down

1629
01:50:20,970 --> 01:50:24,290
you don't have to tell don't do

1630
01:50:24,290 --> 01:50:27,680
test whether they so if i get to this point what i know about i

1631
01:50:27,680 --> 01:50:29,620
and two

1632
01:50:33,950 --> 01:50:35,660
and i

1633
01:50:35,680 --> 01:50:40,200
so in the backs

1634
01:50:40,220 --> 01:50:41,660
i don't

1635
01:50:43,620 --> 01:50:50,270
OK they don't overlap because

1636
01:50:50,330 --> 01:50:52,350
the high of one

1637
01:50:52,390 --> 01:50:54,720
is smaller than the lower the other

1638
01:50:54,770 --> 01:50:57,990
hi one is small and lonely other they don't overlap that way

1639
01:50:57,990 --> 01:51:03,330
they overlap the other way know because then because we're testing also whether

1640
01:51:03,370 --> 01:51:05,370
below the one is higher

1641
01:51:05,390 --> 01:51:07,600
it's bigger than the higher the other

1642
01:51:07,620 --> 01:51:11,370
so this thing is either like this or like this

1643
01:51:12,700 --> 01:51:14,680
right this is testing

1644
01:51:14,720 --> 01:51:16,600
not overlap

1645
01:51:17,520 --> 01:51:19,370
that makes it simpler so

1646
01:51:19,370 --> 01:51:25,700
what i'm searching for fourteen sixteen i checked here and i say do they overlap

1647
01:51:25,750 --> 01:51:29,850
and the answer is now can understand it without having to go through all that

1648
01:51:29,870 --> 01:51:33,910
arithmetic calculations now they don't overlap

1649
01:51:34,950 --> 01:51:40,810
if they did overlap i found what i want and what's going to happen

1650
01:51:40,830 --> 01:51:45,450
when drop out of the while loop and just return x

1651
01:51:45,470 --> 01:51:48,310
it's our turn something that overlaps that's my goal

1652
01:51:48,310 --> 01:51:49,600
so here says

1653
01:51:49,700 --> 01:51:51,930
they don't overlap

1654
01:51:52,520 --> 01:51:57,770
so then i say well if left of x is not now

1655
01:51:57,810 --> 01:52:01,600
they know got left child

1656
01:52:02,620 --> 01:52:03,890
and little by

1657
01:52:03,890 --> 01:52:06,930
it is less or equal to m of left backs

1658
01:52:07,000 --> 01:52:08,000
then we go

1659
01:52:08,040 --> 01:52:09,180
left so

1660
01:52:09,200 --> 01:52:12,600
what happens in this case if i'm searching for fourteen sixteen

1661
01:52:12,660 --> 01:52:15,930
is below by less frequent and left of x

1662
01:52:22,000 --> 01:52:23,500
and i'm searching

1663
01:52:23,540 --> 01:52:25,200
and is it

1664
01:52:25,200 --> 01:52:26,140
less than

1665
01:52:26,140 --> 01:52:27,470
m of

1666
01:52:27,620 --> 01:52:30,390
less than eighteen

1667
01:52:31,540 --> 01:52:35,720
so therefore what do i do

1668
01:52:35,730 --> 01:52:36,770
go left

1669
01:52:36,830 --> 01:52:40,220
make xpointer this guy

1670
01:52:40,250 --> 01:52:43,370
now i checked is it overlap

1671
01:52:44,870 --> 01:52:47,490
OK i take a look at the left guy

1672
01:52:49,680 --> 01:52:51,520
so is a

1673
01:52:51,580 --> 01:52:55,100
i compare it with

1674
01:52:55,930 --> 01:52:58,370
fourteen right

1675
01:52:58,470 --> 01:53:00,180
and is it

1676
01:53:00,230 --> 01:53:04,180
is it lower knows why go right

1677
01:53:05,350 --> 01:53:09,270
and now i discovered that i have an overlap here and its overlaps

1678
01:53:09,310 --> 01:53:14,000
returns then the fifteen eighteen

1679
01:53:14,040 --> 01:53:16,910
as an overlapping

1680
01:53:18,140 --> 01:53:20,680
OK if i were searching for

1681
01:53:20,950 --> 01:53:28,770
twelve fourteen

1682
01:53:28,770 --> 01:53:30,980
because the amount of energy that you can

1683
01:53:30,990 --> 01:53:34,330
then there is frightening four hundred and fifty

1684
01:53:34,380 --> 01:53:39,330
and my power supply here

1685
01:53:39,370 --> 01:53:40,950
that would deliver

1686
01:53:42,830 --> 01:53:45,570
three thousand balls that this is the

1687
01:53:45,580 --> 01:53:48,420
the voltage of the power supply is about thirty

1688
01:53:48,460 --> 01:53:50,430
eight hundred fold

1689
01:53:50,480 --> 01:53:55,470
and so now now the idea is that i'm going to charge the capacitor

1690
01:53:55,480 --> 01:53:59,790
always have to be very slow and careful that i don't make mistakes

1691
01:53:59,830 --> 01:54:01,150
because this is really

1692
01:54:01,220 --> 01:54:03,530
device that could be lethal if you

1693
01:54:03,550 --> 01:54:05,010
not careful

1694
01:54:05,020 --> 01:54:07,190
so i think we OK

1695
01:54:07,360 --> 01:54:11,040
the moment that i'm going to charge the capacitor the reading there will show you

1696
01:54:11,040 --> 01:54:13,660
the potential difference over these

1697
01:54:14,980 --> 01:54:17,080
and i will take a long time for that

1698
01:54:17,100 --> 01:54:19,920
to go up to three thousand people

1699
01:54:20,000 --> 01:54:22,410
so i think i'm ready to go

1700
01:54:22,500 --> 01:54:24,180
and i'm going to charge

1701
01:54:26,670 --> 01:54:30,180
so you see now that the potential difference over the plates

1702
01:54:30,270 --> 01:54:32,560
is very low is is zero

1703
01:54:32,610 --> 01:54:36,300
but if you wait just a few seconds you will see very slowly

1704
01:54:36,320 --> 01:54:38,980
that is charging up

1705
01:54:38,990 --> 01:54:41,060
and fifteen minutes from now

1706
01:54:41,100 --> 01:54:42,850
we will be very close to the

1707
01:54:42,890 --> 01:54:45,040
three thousand four mark

1708
01:54:45,060 --> 01:54:47,750
and then we will return to this so we leave it on

1709
01:54:48,470 --> 01:54:49,140
for now

1710
01:54:50,080 --> 01:54:52,990
it is charged

1711
01:54:53,040 --> 01:54:55,610
the idea of a photo

1712
01:54:56,990 --> 01:54:59,390
is that your charges the capacitor

1713
01:54:59,400 --> 01:55:01,490
and that to discharge

1714
01:55:01,550 --> 01:55:04,460
over the light source

1715
01:55:04,470 --> 01:55:06,710
so the idea being

1716
01:55:06,710 --> 01:55:10,030
that you have a capacitor

1717
01:55:10,050 --> 01:55:12,510
raise some of this

1718
01:55:12,520 --> 01:55:14,780
and then we charge the capacitor

1719
01:55:14,830 --> 01:55:17,720
for a certain amount of energy in there

1720
01:55:17,720 --> 01:55:18,960
and and then

1721
01:55:18,960 --> 01:55:21,620
we dump all the energy

1722
01:55:21,660 --> 01:55:25,230
in a bold so use the capacitor

1723
01:55:25,250 --> 01:55:27,370
we're going to charge it up

1724
01:55:27,370 --> 01:55:28,880
switch here

1725
01:55:28,890 --> 01:55:30,680
and here is the light bulb

1726
01:55:30,730 --> 01:55:32,900
and when we throw to switch

1727
01:55:32,900 --> 01:55:35,210
then all the energy will

1728
01:55:36,150 --> 01:55:40,080
going to light bulb is positive charge and negative charge

1729
01:55:40,110 --> 01:55:41,640
currently start to flow

1730
01:55:41,650 --> 01:55:42,770
and you will see

1731
01:55:42,790 --> 01:55:45,510
a flash of light

1732
01:55:45,520 --> 01:55:46,960
i have here

1733
01:55:46,980 --> 01:55:50,660
the capacitance of a thousand might prefer

1734
01:55:50,660 --> 01:55:52,060
so c

1735
01:55:55,680 --> 01:55:59,120
i'm going to put the potential difference over the past

1736
01:55:59,140 --> 01:56:01,330
one hundred

1737
01:56:01,480 --> 01:56:03,020
which then gives me

1738
01:56:04,080 --> 01:56:08,160
energy of one half the square

1739
01:56:08,270 --> 01:56:11,770
which is five due

1740
01:56:11,870 --> 01:56:14,640
in fact this is not just one capacity

1741
01:56:14,660 --> 01:56:17,870
these are twelve capacitors which i walked up in such a way

1742
01:56:18,290 --> 01:56:22,430
the twelve capacitors of eighty micro for each

1743
01:56:22,480 --> 01:56:25,600
combined capacity of one thousand

1744
01:56:25,640 --> 01:56:27,790
micro for

1745
01:56:27,830 --> 01:56:28,620
and so

1746
01:56:28,620 --> 01:56:30,180
i'm going to charge it up

1747
01:56:30,230 --> 01:56:31,180
and then

1748
01:56:31,180 --> 01:56:32,230
i'm going to

1749
01:56:32,270 --> 01:56:34,000
this charge the capacitor

1750
01:56:34,020 --> 01:56:35,080
for light

1751
01:56:35,100 --> 01:56:37,040
and then you will be able to see

1752
01:56:37,100 --> 01:56:40,580
some lights perhaps depending upon how much energy

1753
01:56:40,620 --> 01:56:43,370
we don't fool there

1754
01:56:43,410 --> 01:56:46,730
concentrate now on this slide built

1755
01:56:47,250 --> 01:56:48,600
a hundred

1756
01:56:48,640 --> 01:56:51,810
you should see here to use it

1757
01:56:51,810 --> 01:56:54,060
seven hundred volts now

1758
01:56:54,100 --> 01:56:57,290
and i'm not going to charge at the moment that i charge you will see

1759
01:56:57,290 --> 01:57:00,430
the voltage over the capacitor

1760
01:57:00,480 --> 01:57:03,790
and so it takes a while for it can charge up so goes down to

1761
01:57:03,790 --> 01:57:07,520
zero and then slowly come back two hundred may take five to ten seconds you

1762
01:57:08,540 --> 01:57:11,000
there we go

1763
01:57:11,000 --> 01:57:12,750
big only five or six

1764
01:57:12,790 --> 01:57:13,890
so now we have

1765
01:57:13,910 --> 01:57:17,450
on the vault so we have five drove stored in there

1766
01:57:18,310 --> 01:57:20,040
i'm going to discharge that now

1767
01:57:20,040 --> 01:57:21,730
over this light well

1768
01:57:21,750 --> 01:57:22,520
you already

1769
01:57:23,680 --> 01:57:25,310
one zero

1770
01:57:25,370 --> 01:57:27,310
a little bit of light

1771
01:57:27,430 --> 01:57:30,040
i can tell you disappointed

1772
01:57:30,080 --> 01:57:31,910
it's not very exciting

1773
01:57:31,930 --> 01:57:35,180
it's not really my style is

1774
01:57:35,220 --> 01:57:37,540
well what we can do

1775
01:57:37,580 --> 01:57:40,430
we can increase the voltage a little bit

1776
01:57:40,520 --> 01:57:43,250
we could go to two hundred and fifty four

1777
01:57:43,390 --> 01:57:44,540
in which case

1778
01:57:44,540 --> 01:57:48,680
since it goes with the square would have six times more energy than we have

1779
01:57:48,680 --> 01:57:50,140
many tools

1780
01:57:50,200 --> 01:57:53,270
let's see whether that's a little bit more exciting

1781
01:57:53,270 --> 01:57:54,890
so now i have to

1782
01:57:54,950 --> 01:57:58,660
back up the voltage to two hundred fifty falls now you see the power supply

1783
01:58:00,270 --> 01:58:02,580
four hundred fifty volts

1784
01:58:02,580 --> 01:58:08,100
getting there we don't have boy looking on but for two hundred fifty volts

1785
01:58:08,120 --> 01:58:11,230
and now i can charge up again will take a little longer

1786
01:58:11,250 --> 01:58:12,520
so you see the

1787
01:58:12,520 --> 01:58:14,350
voltage over the capacitor

1788
01:58:14,350 --> 01:58:16,310
four hundred seventy

1789
01:58:16,330 --> 01:58:17,500
two hundred

1790
01:58:17,560 --> 01:58:19,910
two fifty that we are

1791
01:58:19,930 --> 01:58:21,290
now we can see

1792
01:58:21,370 --> 01:58:24,210
where do we get a little bit more light so you go from five do

1793
01:58:24,210 --> 01:58:26,120
now to thirty two

1794
01:58:27,370 --> 01:58:29,220
one zero

1795
01:58:29,230 --> 01:58:31,080
now we're getting somewhere

1796
01:58:31,100 --> 01:58:32,980
now you really see how

1797
01:58:33,000 --> 01:58:33,850
a photo

1798
01:58:36,270 --> 01:58:38,500
now we all of course have

1799
01:58:38,560 --> 01:58:42,040
this drafted destructive instincts

1800
01:58:42,060 --> 01:58:47,700
and so you wonder

1801
01:58:48,850 --> 01:58:52,450
here's your thinking the same thing that i

1802
01:58:52,450 --> 01:58:56,040
so we try three hundred forty volts and see whether the

1803
01:58:57,730 --> 01:59:01,180
maybe explode

1804
01:59:01,250 --> 01:59:07,660
the idea for the supply condolences that let's go all the way

1805
01:59:07,680 --> 01:59:10,140
three hundred thirty seven vols OK

1806
01:59:10,160 --> 01:59:13,720
so that would mean that we have fifty joules roughly

1807
01:59:13,720 --> 01:59:15,620
is larger

1808
01:59:15,670 --> 01:59:19,780
so we have this model it's essentially the model we've seen before

1809
01:59:20,240 --> 01:59:26,110
plus noise analysis and novelty is that you know how to noise term which you

1810
01:59:26,120 --> 01:59:27,150
i assume

1811
01:59:27,170 --> 01:59:30,530
later on that is stochastic but for the moment we don't need

1812
01:59:30,650 --> 01:59:36,430
all right so in statistics typically the way people approaching are modelling the ways statistical

1813
01:59:36,430 --> 01:59:40,440
theory has gone for many many years is to assume that we have a fixed

1814
01:59:40,440 --> 01:59:46,060
number of variables that we have an enormous number of observations going to infinity

1815
01:59:46,090 --> 01:59:50,650
that's a classical set up the number of parameters number of variables p is a

1816
01:59:50,650 --> 01:59:54,910
fixed number of observations goes to infinity so we have this store matrix

1817
01:59:55,000 --> 02:00:00,170
and we developed a lot of asymptotics explaining what happens when the number of variables

1818
02:00:00,170 --> 02:00:01,570
goes to infinity

1819
02:00:01,590 --> 02:00:06,530
but right now you've seen already many examples were systems even in those setting on

1820
02:00:06,530 --> 02:00:10,770
to determine where n is about the same size as p of p

1821
02:00:10,790 --> 02:00:14,900
the number of columns of course is much greater than and that's where the field

1822
02:00:14,910 --> 02:00:17,250
is going at the moment

1823
02:00:17,280 --> 02:00:21,800
this is called high dimensionality in the field of statistics where the number of variables

1824
02:00:21,800 --> 02:00:24,900
greatly exceeds the number of

1825
02:00:24,910 --> 02:00:29,770
and this has a lot of applications compressed sensing can genomics we've seen

1826
02:00:30,020 --> 02:00:34,070
applications where we have few patients but thousands of genes

1827
02:00:34,110 --> 02:00:36,650
tens of thousands of genes that we need to

1828
02:00:37,500 --> 02:00:44,170
in chemometrics which have been involved with is in biomedical imaging and then after conversion

1829
02:00:44,210 --> 02:00:48,040
the number of times was this matrix is of this form and not super told

1830
02:00:48,040 --> 02:00:51,350
matrix is actually becoming the norm

1831
02:00:51,360 --> 02:00:53,240
in many applied fields

1832
02:00:53,980 --> 02:00:55,980
so we have

1833
02:00:55,990 --> 02:00:59,040
this problem we're now we have

1834
02:00:59,090 --> 02:01:03,490
to solve to try to recover x not only from fewer measurements but also no

1835
02:01:03,500 --> 02:01:06,770
easy and so we can assume at the moment that the noise may not be

1836
02:01:06,770 --> 02:01:11,170
stochastic but it's bound in l two that to speak like an engineer the noise

1837
02:01:11,170 --> 02:01:13,540
power is

1838
02:01:13,560 --> 02:01:15,520
OK so i'm going to have

1839
02:01:15,540 --> 02:01:19,680
my system that i wish to solve where

1840
02:01:22,540 --> 02:01:28,680
where y which is equal to EX sixteen houses underdetermined system but now we have

1841
02:01:28,690 --> 02:01:32,520
on top of that not only is this is this matrix is not invertible but

1842
02:01:32,520 --> 02:01:36,050
on top of these equations are not exactly reliable

1843
02:01:38,150 --> 02:01:39,320
so now

1844
02:01:39,340 --> 02:01:43,280
two kind of sort of this we can use the lasso which we discussed earlier

1845
02:01:43,300 --> 02:01:47,120
which we can relax the constraints and we're going to say well

1846
02:01:47,140 --> 02:01:50,330
right now i'm not going to enforce equality constraints what i'm going to do is

1847
02:01:50,330 --> 02:01:52,540
if i know the noise level

1848
02:01:52,550 --> 02:01:54,250
it is less than epsilon

1849
02:01:54,420 --> 02:01:55,210
one is

1850
02:01:55,250 --> 02:01:59,670
find that solution has minimum and one norman is consistent with the data and now

1851
02:01:59,700 --> 02:02:02,920
consistent with the data means that the reconstruction

1852
02:02:02,920 --> 02:02:04,950
needs to be within the noise

1853
02:02:06,290 --> 02:02:10,670
OK so that is in english among all objects consistent with the data

1854
02:02:10,670 --> 02:02:15,510
pick that was minimum and one or more consistency now means in this relaxed way

1855
02:02:15,540 --> 02:02:21,800
and that's essentially the procedure proposed by centers on signs in eighty three and by

1856
02:02:23,070 --> 02:02:25,900
in nineteen six

1857
02:02:25,930 --> 02:02:27,310
OK so

1858
02:02:27,330 --> 02:02:28,330
we would

1859
02:02:30,800 --> 02:02:35,830
reconstruction which is within the noise level to make sure that for example

1860
02:02:36,010 --> 02:02:39,410
that the true facts which i'm looking for is actually

1861
02:02:39,420 --> 02:02:41,150
in the physical sense

1862
02:02:41,170 --> 02:02:43,810
that is the basic constraints

1863
02:02:45,040 --> 02:02:50,000
all right so so what are the main results when we have possibly have an

1864
02:02:50,250 --> 02:02:58,290
adversarial noise we observe y equals ex-players e words easy is bounded as we've seen

1865
02:02:58,320 --> 02:03:02,670
and then we have this result that is

1866
02:03:02,680 --> 02:03:05,510
the to myself just among the turnstile

1867
02:03:05,530 --> 02:03:08,040
that said nothing goes wrong

1868
02:03:08,050 --> 02:03:13,750
OK so if we assume the same conditions is restricted isometry property that we need

1869
02:03:13,750 --> 02:03:16,390
even for less recovery

1870
02:03:16,410 --> 02:03:19,660
then when you solve this optimisation problem

1871
02:03:19,700 --> 02:03:23,840
then you guaranteed a good error and good air means here means what it means

1872
02:03:23,840 --> 02:03:28,820
that the distance between the reconstruction and the truth is the error that you would

1873
02:03:28,820 --> 02:03:30,680
get in the noiseless case

1874
02:03:30,740 --> 02:03:36,090
it's actually getting the largest component of x and so on plus term that now

1875
02:03:36,100 --> 02:03:39,670
so when you see we got i mean there's a numerical cost

1876
02:03:39,690 --> 02:03:44,680
plus it turned absolutely which is simply proportional to the past

1877
02:03:44,690 --> 02:03:45,960
to the nose

1878
02:03:46,020 --> 02:03:48,960
so this result is very reassuring it so that

1879
02:03:49,010 --> 02:03:52,910
well you have to feel question you asked me to invert matrix that is not

1880
02:03:54,040 --> 02:03:58,450
it looks completely down team because you know most of the singular values of a

1881
02:03:58,720 --> 02:04:03,740
zero but yet one excels in this relaxed and one problem

1882
02:04:03,750 --> 02:04:07,200
what is guaranteed is that you don't do that and the way you look how

1883
02:04:07,200 --> 02:04:11,620
well you do you get the accuracy you get if you had no no use

1884
02:04:12,800 --> 02:04:16,650
the term simply proportional to the notice that

1885
02:04:17,260 --> 02:04:18,630
and so

1886
02:04:18,640 --> 02:04:22,290
there's no blow up i mean things don't go to infinity or you know the

1887
02:04:22,290 --> 02:04:25,270
error bound is really well controlled

1888
02:04:25,290 --> 02:04:29,530
the recovery error is on the same order as the observation area and as the

1889
02:04:29,530 --> 02:04:33,300
noise level increases then the error

1890
02:04:33,440 --> 02:04:34,960
degrees nicely

1891
02:04:34,990 --> 02:04:38,870
and so simply proportional to the amount of noise that you

1892
02:04:39,900 --> 02:04:41,120
OK so that's

1893
02:04:41,140 --> 02:04:44,620
the reassuring because it said that is the knowledge level is small and the object

1894
02:04:44,630 --> 02:04:46,060
is sparse for example

1895
02:04:46,080 --> 02:04:50,830
and these terms disappear and you have an error which is just proportional to the

1896
02:04:50,830 --> 02:04:53,050
notion of

1897
02:04:53,100 --> 02:04:58,710
so why is this happening well i'm going to draw a picture

1898
02:04:58,740 --> 02:05:02,260
which is essentially the picture you've seen why is it that not only i can

1899
02:05:02,260 --> 02:05:08,200
invert non invertible matrices but i can do this in the presence of knows because

1900
02:05:08,200 --> 02:05:12,060
of kind of geometry again well let's look at

1901
02:05:12,080 --> 02:05:16,580
our exit sitting there are now i make measurements

1902
02:05:16,590 --> 02:05:20,040
but now i don't know that x is on this line because it's measurements have

1903
02:05:20,040 --> 02:05:21,450
been corrupted by

1904
02:05:21,460 --> 02:05:23,300
by notes

1905
02:05:23,310 --> 02:05:26,070
so what do i know about where x is

1906
02:05:26,120 --> 02:05:29,470
well if i had no knowledge that we need and we know that he would

1907
02:05:29,470 --> 02:05:31,310
leave on this line

1908
02:05:32,840 --> 02:05:36,070
but there is no is so what do i know

1909
02:05:36,120 --> 02:05:40,380
well i know that the one applies is a matrix

1910
02:05:40,400 --> 02:05:43,080
that a x minus the reconstruction

1911
02:05:43,100 --> 02:05:45,410
well by the triangular inequality

1912
02:05:45,440 --> 02:05:49,810
is less than a x minus one plus one minus VX

1913
02:05:49,830 --> 02:05:54,040
but because x is she's evil for the optimisation programme this is less than the

1914
02:05:54,040 --> 02:05:56,500
this is the talk about learning sum-product networks discriminatively

1915
02:05:57,080 --> 02:06:00,520
i'm rob gens this is joint work with pedro domingos university washington

1916
02:06:02,050 --> 02:06:06,080
sum-product networks are a new deep probabilistic architecture with tractable inference

1917
02:06:07,190 --> 02:06:11,200
you might not be familiar the SPN so give you a brief guarantor of a sample SPN

1918
02:06:11,790 --> 02:06:12,840
over some image pixels

1919
02:06:13,410 --> 02:06:14,500
so at the bottom we have heard

1920
02:06:14,830 --> 02:06:16,620
these distributions which model the pixels

1921
02:06:17,330 --> 02:06:20,310
and the products of this distributions would be considered image patch features

1922
02:06:21,100 --> 02:06:22,550
which which we have small and large

1923
02:06:23,380 --> 02:06:27,670
and then sums over those products would be considered mixture models over image patch features

1924
02:06:28,670 --> 02:06:31,990
and the products over those sums could be considered part decompositions

1925
02:06:32,450 --> 02:06:34,530
which you've one like this or one like this

1926
02:06:35,510 --> 02:06:39,360
and then finally are some node can be considered as pooling function

1927
02:06:39,960 --> 02:06:41,240
over party compositions

1928
02:06:42,120 --> 02:06:45,440
and this whole structure could be repeated the features that mixtures the

1929
02:06:45,930 --> 02:06:48,200
party compositions as many layers as you want

1930
02:06:48,760 --> 02:06:52,330
so as you can see SPNs are quite expressive despite the tractability

1931
02:06:53,840 --> 02:06:56,780
so in this talk i'm first going to motivate the use the sum-product networks

1932
02:06:57,360 --> 02:06:58,720
then i'm going to review how they work

1933
02:06:59,360 --> 02:07:03,440
then i'm gonna show you how to add feature functions to experience and training discriminatively

1934
02:07:04,150 --> 02:07:08,880
and finally I'll show some experiments where sum-product networks outperform support vector machines and deep models

1935
02:07:09,300 --> 02:07:10,730
and image classification benchmarks

1936
02:07:11,540 --> 02:07:12,810
but first i with some motivation

1937
02:07:15,140 --> 02:07:19,560
many is graphical models because they allow us to compactly represent the joint distribution over many variables

1938
02:07:21,520 --> 02:07:23,220
inference in graphical models is intractable

1939
02:07:23,750 --> 02:07:27,230
which often forces us to reduce the expressivity over model often with low treewidth

1940
02:07:28,190 --> 02:07:31,850
although many approximate inference schemes have been proposed role of difficulties

1941
02:07:32,770 --> 02:07:36,390
in contrast sum-product networks perform best exact inference

1942
02:07:36,890 --> 02:07:37,770
and high treewidth models

1943
02:07:41,010 --> 02:07:45,110
deep architectures use layers of hidden variables to increase the representational power

1944
02:07:45,630 --> 02:07:51,220
however when we add layers dependent variables graphical models this compound we already intractable inference problem

1945
02:07:52,340 --> 02:07:54,250
sum-product networks can be viewed as a model

1946
02:07:54,830 --> 02:07:58,410
the has full probabilistic semantics and tractable inference over many layers

1947
02:08:01,510 --> 02:08:05,690
up until now we've only seen generative training of sum-product networks however for many applications

1948
02:08:06,010 --> 02:08:08,410
we find that discriminative learning is performed better

1949
02:08:09,160 --> 02:08:10,100
this is our contribution

1950
02:08:11,010 --> 02:08:13,510
with conditional random fields we can use flexible feature functions

1951
02:08:14,510 --> 02:08:17,020
and yet the conditional partition function remains intractable

1952
02:08:18,310 --> 02:08:22,470
in this talk we're going to combine the advantages of SPN with those conditional models

1953
02:08:23,190 --> 02:08:26,600
namely flexible feature functions with exact inference over high treewidth

1954
02:08:32,210 --> 02:08:33,350
what is an SPN

1955
02:08:34,360 --> 02:08:36,000
let's define SPNs recursively

1956
02:08:39,940 --> 02:08:41,290
a univariate distributions is an SPN

1957
02:08:42,340 --> 02:08:45,150
you can pick your favorite discrete or continuous distribution

1958
02:08:45,830 --> 02:08:49,130
but for the sake of this example we'll defined Xs being a multinomial

1959
02:08:50,960 --> 02:08:53,250
one variables not that interesting so next definition

1960
02:08:54,340 --> 02:08:54,890
which is that

1961
02:08:55,920 --> 02:08:59,350
a product of SPNs over disjoint variables is also an SPN

1962
02:09:00,000 --> 02:09:01,500
so just part of another variable Y

1963
02:09:02,290 --> 02:09:03,840
which can be any distribution but

1964
02:09:04,520 --> 02:09:06,580
but in this example we also have a multinomial

1965
02:09:07,910 --> 02:09:12,000
now you might notice that the product two variables is just independent so last definition

1966
02:09:16,870 --> 02:09:21,250
which is that a weighted sum of the SPNs over the same variables is also an SPN

1967
02:09:22,310 --> 02:09:25,120
so now we just brought another distribution over these same two variables

1968
02:09:25,520 --> 02:09:28,230
and formed a weighted sum now now establishes dependencies

1969
02:09:29,070 --> 02:09:32,340
so these are some node sends out a hidden variable just like in a mixture model

1970
02:09:34,450 --> 02:09:38,200
so what we have now is sum-product networks are really directed acyclic graph

1971
02:09:38,720 --> 02:09:40,580
which univariate distributions at the leaves

1972
02:09:40,990 --> 02:09:42,840
and some some product nodes internal nodes

1973
02:09:44,510 --> 02:09:47,680
we can also use multivariate distributions at the least but you'd have to be able

1974
02:09:47,680 --> 02:09:51,290
to ensure that you can compute the partition functions and modes in closed form

1975
02:09:54,010 --> 02:09:57,730
and the beauty this definition is that it's recursive so we can build up layers and layers

1976
02:09:58,350 --> 02:10:02,200
more and more and more structured and yet are always remaining tractable

1977
02:10:05,370 --> 02:10:06,980
the key property spans

1978
02:10:07,560 --> 02:10:09,670
is that's all marginals are computable in linear time

1979
02:10:10,400 --> 02:10:11,250
it's easy to see why

1980
02:10:12,570 --> 02:10:16,580
because the partition function is the product node over disjoint sets of variables is just

1981
02:10:16,580 --> 02:10:18,800
the product of the partition functions the children

1982
02:10:20,230 --> 02:10:23,160
and the partition function some node over the same set of variables

1983
02:10:23,630 --> 02:10:26,370
this is the weighted sum of the partition functions its children

1984
02:10:27,410 --> 02:10:28,030
a marginal

1985
02:10:28,590 --> 02:10:30,620
it's just the partition function and nasty and

1986
02:10:31,140 --> 02:10:35,140
well you replace those degree distributions with seven over evidence variables

1987
02:10:35,790 --> 02:10:36,990
with delta functions

1988
02:10:37,520 --> 02:10:38,740
where the probability that's

1989
02:10:39,550 --> 02:10:40,220
so for example

1990
02:10:40,860 --> 02:10:43,320
if we are curious that the probability that x is zero in this model

