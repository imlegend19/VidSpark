1
00:00:00,000 --> 00:00:04,890
it's clear we cannot derive an upper bound of of two means converge to each

2
00:00:04,890 --> 00:00:09,680
other because each of these quantities will convert the patient so that both close to

3
00:00:09,680 --> 00:00:13,230
this they also have to be close to each other and we have some kind

4
00:00:13,230 --> 00:00:17,120
of triangle inequality we lose a little bit over here so we get a factor

5
00:00:17,120 --> 00:00:18,770
of two and we get

6
00:00:18,790 --> 00:00:21,330
epsilon over two minutes of silence

7
00:00:21,890 --> 00:00:23,410
basically the same thing

8
00:00:25,290 --> 00:00:27,750
so if we translate this back into machine learning

9
00:00:29,790 --> 00:00:33,020
does the probability of obtaining m samples

10
00:00:33,040 --> 00:00:38,140
so we obtain training set where the training error and test error differ by more

11
00:00:38,140 --> 00:00:41,520
than epsilon is founded by this point

12
00:00:41,540 --> 00:00:47,160
now or i already mentioned at some point this refers to one fixed function f

13
00:00:47,180 --> 00:00:48,830
so why is this the case

14
00:00:48,850 --> 00:00:53,190
so what if we chose the function quickly interested in so we choose the function

15
00:00:53,190 --> 00:00:55,620
giving us the minimum training

16
00:00:55,640 --> 00:00:59,870
after all that's what we want to know is that function going well problem is

17
00:00:59,870 --> 00:01:04,040
this function has looked at the training so choosing the function has a minimum training

18
00:01:04,080 --> 00:01:06,100
we first have to look at the training data

19
00:01:06,120 --> 00:01:10,410
so the sum of this function knows something about all the training examples

20
00:01:10,420 --> 00:01:13,290
and then unfortunately

21
00:01:13,310 --> 00:01:15,310
even though

22
00:01:15,600 --> 00:01:20,640
even though these guys are still independent x i y i functions i mean

23
00:01:20,660 --> 00:01:22,890
depends on all of them jointly

24
00:01:22,890 --> 00:01:26,480
and in this case i so no longer independent

25
00:01:26,480 --> 00:01:28,480
and the telephone doesn't apply

26
00:01:29,160 --> 00:01:31,180
that's the whole problem so that

27
00:01:31,190 --> 00:01:32,270
what we have to

28
00:01:32,290 --> 00:01:38,290
deal with it now

29
00:01:38,310 --> 00:01:40,830
now here's something not true

30
00:01:42,000 --> 00:01:43,810
it's actually not

31
00:01:43,810 --> 00:01:49,770
not a full one direction is easy the other one is highly nontrivial but i've

32
00:01:49,770 --> 00:01:51,230
tried to motivate

33
00:01:51,310 --> 00:01:52,640
before this picture

34
00:01:52,660 --> 00:01:54,330
it turns out that

35
00:01:54,330 --> 00:01:56,410
it's necessary sufficient

36
00:01:57,230 --> 00:01:59,350
so the type of consistency

37
00:01:59,410 --> 00:02:02,250
consistency member questions

38
00:02:02,270 --> 00:02:05,850
do we get the best result in the limit of infinite dimension data points

39
00:02:05,870 --> 00:02:12,250
for this is necessary and sufficient to have a type of uniform convergence

40
00:02:12,270 --> 00:02:14,140
of training error

41
00:02:14,160 --> 00:02:17,580
two tests are often here risk

42
00:02:17,600 --> 00:02:22,490
over the whole function so in this picture from before over this whole x axis

43
00:02:22,490 --> 00:02:24,710
of all functions

44
00:02:25,250 --> 00:02:30,890
if we have this uniform convergence and we have consistency and vice versa

45
00:02:31,730 --> 00:02:33,890
of course if you look at these things

46
00:02:33,890 --> 00:02:37,690
the reason this is a little bit hard to cheque for a learning machine

47
00:02:37,710 --> 00:02:41,690
and that's why actually we would like to have properties of class of functions which

48
00:02:41,690 --> 00:02:46,980
tell us something about when the uniform convergence will take place not that's

49
00:02:47,000 --> 00:02:49,270
capacity measures

50
00:02:51,060 --> 00:02:56,290
and this will show up automatically if we try to search about

51
00:02:57,940 --> 00:03:01,980
let's take a closer look at this quantity so remember this is what we were

52
00:03:01,980 --> 00:03:03,160
interested in

53
00:03:03,160 --> 00:03:07,230
so this is not the deviation between

54
00:03:07,290 --> 00:03:10,350
training error and test error

55
00:03:10,350 --> 00:03:15,080
the primitive these deviations larger time but not now is uniformly so this is the

56
00:03:15,080 --> 00:03:19,780
supremum have never seen the supremum just think of it as the maximum this is

57
00:03:20,690 --> 00:03:24,500
so the deviation that we get for the worst possible function and the worst i

58
00:03:24,500 --> 00:03:28,750
mean function train test error are as different as possible

59
00:03:28,770 --> 00:03:30,890
so in a way to function well

60
00:03:30,980 --> 00:03:35,100
measuring the training error mislead us a lot about what the test

61
00:03:35,120 --> 00:03:38,100
it's always function so we're interested in

62
00:03:38,140 --> 00:03:41,330
what's the probability that for the world's functions

63
00:03:41,330 --> 00:03:44,210
we are misled by more than epsilon

64
00:03:44,210 --> 00:03:47,620
so if we can prove that even for the west function we are signing close

65
00:03:47,620 --> 00:03:49,080
with high probability

66
00:03:49,100 --> 00:03:51,750
then we find because then we don't care about

67
00:03:51,770 --> 00:03:54,940
which one is the worst one would get just going to use the one minimising

68
00:03:54,940 --> 00:03:57,140
the training error and we know we're going to find

69
00:03:59,690 --> 00:04:02,060
so first of all it is probably the simplest case

70
00:04:02,080 --> 00:04:06,020
function class contains only one function in that case we can just cross out the

71
00:04:06,020 --> 00:04:09,250
supremum maximum

72
00:04:09,270 --> 00:04:12,330
so we are left with the statement that we had on the previous slide

73
00:04:12,390 --> 00:04:14,980
in the channel but does the job for

74
00:04:14,980 --> 00:04:16,910
that is the telephone

75
00:04:16,910 --> 00:04:17,870
it tells us

76
00:04:17,890 --> 00:04:20,710
what's going to happen

77
00:04:20,770 --> 00:04:25,810
just as an aside one application of this policy even if you're you're facing the

78
00:04:25,810 --> 00:04:30,190
sea europe but not completely orthodox phase three o bayesian who still

79
00:04:30,210 --> 00:04:33,230
evaluates things on the test set

80
00:04:33,230 --> 00:04:37,100
it's a real place you should be doing is usually just putting all your prior

81
00:04:37,100 --> 00:04:41,370
knowledge training the data then you don't you don't have to be anything but let's

82
00:04:41,370 --> 00:04:47,350
say you're a reasonable place like lawrence

83
00:04:49,560 --> 00:04:53,540
so you have a thousand test examples and you want to know no i've missed

84
00:04:53,560 --> 00:04:57,350
my my around this type of the examples how likely is it that i'm close

85
00:04:57,350 --> 00:05:01,790
to the true are but i would be measuring five million examples of ten million

86
00:05:02,620 --> 00:05:06,310
then also this poem tells you this

87
00:05:06,330 --> 00:05:08,040
because tells you

88
00:05:08,060 --> 00:05:12,210
so now we have one fixed function it's now so we have chosen the function

89
00:05:12,210 --> 00:05:16,660
that minimizes the training error all that maximizes the evidence or whatever criteria you want

90
00:05:16,660 --> 00:05:21,270
to use you want expansion now you're looking at test set

91
00:05:21,270 --> 00:05:23,620
so this function has nothing to do with test set

92
00:05:23,620 --> 00:05:29,230
and then again these losses i on the test points the independent if field test

93
00:05:29,390 --> 00:05:34,460
is IID test examples are drawn i i d from the some distribution

94
00:05:34,480 --> 00:05:38,460
and then this tells you how far is is the empirical quantity

95
00:05:38,460 --> 00:05:41,250
all my averaged over test set

96
00:05:41,270 --> 00:05:47,330
from the real test error that would be measuring infinity and the test examples

97
00:05:47,350 --> 00:05:49,290
and this tells you that

98
00:05:49,290 --> 00:05:51,660
it's good to have more examples because

99
00:05:51,680 --> 00:05:54,890
the difference would go exponentially with good on exponential

100
00:05:54,920 --> 00:05:57,100
so that's nice

101
00:05:57,120 --> 00:06:02,410
so that's but now let's go to the slightly more complicated case where we have

102
00:06:02,410 --> 00:06:03,480
a function with

103
00:06:03,580 --> 00:06:08,100
several stories function class with several functions to do it

104
00:06:08,100 --> 00:06:09,640
two very slowly

105
00:06:09,680 --> 00:06:12,330
we start to functions so the

106
00:06:12,330 --> 00:06:13,690
plant will be

107
00:06:13,790 --> 00:06:18,500
two functions and functions we can use something called the union bound

108
00:06:18,560 --> 00:06:24,230
and then the nontrivial step will be from an functions to infinitely many functions so

109
00:06:24,230 --> 00:06:29,380
got a couple of protons those electrons are more tightly bound the less flexible so

110
00:06:29,410 --> 00:06:36,850
number of electrons and also the size for equal numbers of electrons a larger energy

111
00:06:36,850 --> 00:06:40,850
will be softer and switch you're than smaller entities

112
00:06:40,920 --> 00:06:46,100
so polarizabilities scales with electron number and with

113
00:06:47,400 --> 00:06:49,820
so we reason that helium is going to be

114
00:06:50,080 --> 00:06:57,600
so low polarizabilities words argon will be high polarizability because we have a lot of

115
00:06:57,600 --> 00:07:02,690
electrons they have to buy necessity you know what the the filling sequences the more

116
00:07:02,690 --> 00:07:06,690
electrons you have those last electrons the farther out so the positive charge in the

117
00:07:06,690 --> 00:07:11,140
nucleus has a weaker hold on and so therefore there while they're bounded they're a

118
00:07:11,140 --> 00:07:17,860
little bit more to frisky another sit near the place move around right so let's

119
00:07:17,860 --> 00:07:23,660
get back to homonuclear species this is induced dipole induced dipole that's the 3rd type

120
00:07:23,660 --> 00:07:36,400
of interaction induced dipole induced dipoles induced by call this a really fascinating wanted proposal

121
00:07:36,400 --> 00:07:39,250
people for a long time from

122
00:07:39,460 --> 00:07:44,460
and its dominant its dominant in non polar molecules

123
00:07:44,470 --> 00:07:53,100
dominant in non polar molecules 100 dominant nonpolar everything's was done dominant non polar species

124
00:07:53,100 --> 00:07:59,580
atoms and molecules in our here's some examples of years Oregon

125
00:08:00,560 --> 00:08:06,930
the you look the periodic table argon has a melting point of 84 kelvin and

126
00:08:06,930 --> 00:08:09,980
has a boiling point of the 87 kelvin

127
00:08:09,990 --> 00:08:14,190
I told you whenever you see a solid you have to conclude that it's held

128
00:08:14,190 --> 00:08:20,510
together by some force so let's imagine we're at liquid nitrogen temperature we take some

129
00:08:20,510 --> 00:08:26,190
are gone out of argon gas cylinder and we followed through and through and that

130
00:08:26,190 --> 00:08:31,400
too was immersed in liquid nitrogen liquid nitrogen 77 Calvin was below 84

131
00:08:31,970 --> 00:08:35,060
so the arrogance solidified

132
00:08:35,060 --> 00:08:37,400
it's going to deposit on the walls of the tube

133
00:08:37,690 --> 00:08:40,320
and after while have a lot of race

134
00:08:40,400 --> 00:08:47,160
some set minimum look at Argonne based what blinds are going too hard

135
00:08:47,220 --> 00:08:54,160
well it's not ionic it's not primary covalent

136
00:08:54,210 --> 00:08:57,320
it's not dipole-dipole

137
00:08:57,430 --> 00:08:59,490
they're all the same

138
00:08:59,530 --> 00:09:03,380
what could possibly blind argon

139
00:09:03,380 --> 00:09:05,490
but suggest

140
00:09:05,510 --> 00:09:08,990
the adhesion because it's it's it's sticking wide

141
00:09:11,120 --> 00:09:16,580
all the answer was given to us by the German scientist by the name of

142
00:09:16,580 --> 00:09:19,600
fritz london Fritz London

143
00:09:19,980 --> 00:09:21,500
in London

144
00:09:21,560 --> 00:09:28,140
he said I think I can explain this he went look back over here dipole-dipole

145
00:09:28,160 --> 00:09:29,120
said you know

146
00:09:29,230 --> 00:09:32,360
the we've already come to the conclusion that

147
00:09:32,820 --> 00:09:37,180
electrons are emotions electrons are not stationary

148
00:09:37,360 --> 00:09:41,320
so he says a managing if we could zoom in here with the themto 2nd

149
00:09:41,320 --> 00:09:48,320
camera and take a snapshot instead of having a uniform distribution of electrons suppose that

150
00:09:48,480 --> 00:09:52,140
at this instant in time there is a slight

151
00:09:52,140 --> 00:09:59,530
in excess of electron density here at around 2 o'clock and therefore

152
00:09:59,660 --> 00:10:08,290
the conservation of charge dictates that there must be a slight electron deficiency that they

153
00:10:09,950 --> 00:10:13,640
this is the case what's the consequence for the adjacent are gone

154
00:10:13,670 --> 00:10:15,490
what this adjacent are gone

155
00:10:15,530 --> 00:10:21,490
says if this is electron deficient it's going to pull electrons in this argon and

156
00:10:21,490 --> 00:10:24,900
make the bottom right here electrons

157
00:10:25,840 --> 00:10:32,250
leaving the top left electron-deficient which then causes but she told her and he told

158
00:10:32,250 --> 00:10:35,320
him etc etc

159
00:10:35,340 --> 00:10:40,340
so what is the result here the result is we have dipoles

160
00:10:40,380 --> 00:10:47,600
time-averaged it's nonpolar but it's always dipolar and there's a tiny tiny type

161
00:10:48,620 --> 00:10:54,790
so he called this time fluctuating at that time fluctuating dipole

162
00:10:54,860 --> 00:11:02,670
and its present in all systems in all systems even a sodium ion has electron

163
00:11:02,670 --> 00:11:04,730
cloud that's in motion

164
00:11:04,750 --> 00:11:08,710
and we don't see this in sodium because the action of this in sodium is

165
00:11:08,710 --> 00:11:13,770
dwarfed by the coelomic force which is a thousand times stronger but this is the

166
00:11:13,770 --> 00:11:15,360
only game in town

167
00:11:15,360 --> 00:11:18,770
when you're at 18 77 kelvins

168
00:11:18,790 --> 00:11:23,620
there's nothing left so this is dominant and the forest that comes out of this

169
00:11:23,620 --> 00:11:28,600
this attractive force is being turned the London dispersion force

170
00:11:29,010 --> 00:11:36,640
the London dispersion force in a London dispersion force is a is a very weak

171
00:11:36,640 --> 00:11:43,930
force and it's as you would expect it's proportional to the polarizability right elements that

172
00:11:43,930 --> 00:11:45,990
have being

173
00:11:46,010 --> 00:11:51,320
electron clouds that weakly held are going to be more mobile than small

174
00:11:52,320 --> 00:11:58,480
atoms that have small electron clouds tightly held so that London dispersion

175
00:11:58,490 --> 00:12:04,290
the London dispersion energy is goes as the square of polarize ability and as you

176
00:12:04,330 --> 00:12:09,030
expect it's a very weak force analogous to the repulsive force right we're talking about

177
00:12:09,030 --> 00:12:12,430
electron electron repulsion wanted you see that before

178
00:12:12,470 --> 00:12:17,690
we saw that in the cool the derivation of the the with the Madeleine cost

179
00:12:17,690 --> 00:12:22,080
and that's the born exponent of very high number because this is very weak force

180
00:12:22,370 --> 00:12:27,250
solve if this is the inter species separation this goes as part of the states

181
00:12:27,280 --> 00:12:32,380
which makes sense it's sort of born like

182
00:12:37,880 --> 00:12:41,300
this is actually got another name

183
00:12:41,320 --> 00:12:42,690
you know

184
00:12:42,690 --> 00:12:44,480
to be high

185
00:12:44,520 --> 00:12:45,210
when you

186
00:12:45,210 --> 00:12:47,520
this stripping of of

187
00:12:47,570 --> 00:12:50,540
words but the problem is that

188
00:12:50,540 --> 00:12:53,210
you might make and also a new

189
00:12:53,230 --> 00:12:56,110
remove words because you build some

190
00:12:56,790 --> 00:13:03,610
or perhaps another three and a system for compressed compression of your sentence because we

191
00:13:03,610 --> 00:13:06,110
call this sentence compression

192
00:13:07,270 --> 00:13:14,290
do make this and also this affects the classification accuracy

193
00:13:14,290 --> 00:13:15,790
OK so

194
00:13:15,790 --> 00:13:17,420
i had this one

195
00:13:18,230 --> 00:13:21,790
so i'm going to another

196
00:13:21,790 --> 00:13:26,590
part of this is cause

197
00:13:27,590 --> 00:13:28,900
i think

198
00:13:37,210 --> 00:13:38,710
you did

199
00:13:38,710 --> 00:13:41,290
at the end of each chapter

200
00:13:43,130 --> 00:13:48,920
and you can always write me something is amiss there

201
00:13:49,020 --> 00:14:12,920
you these

202
00:14:12,940 --> 00:14:29,040
well we use the to dynamic programming for training

203
00:14:29,070 --> 00:14:31,750
that's the open source system don't

204
00:14:31,810 --> 00:14:36,270
the techniques that i discovered you are that is that there

205
00:14:36,290 --> 00:14:38,710
very very general so

206
00:14:38,730 --> 00:14:46,440
you you have opened for

207
00:14:46,460 --> 00:14:54,500
i i i don't really know speech speech group

208
00:14:55,980 --> 00:14:57,790
part of speech

209
00:14:57,810 --> 00:14:59,150
and so

210
00:14:59,690 --> 00:15:03,480
i was speech transcription

211
00:15:11,400 --> 00:15:15,630
and so

212
00:15:15,670 --> 00:15:19,900
i will compile a list of all

213
00:15:19,920 --> 00:15:23,250
of URL where you can also sort

214
00:15:23,250 --> 00:15:24,610
the name at this

215
00:15:24,630 --> 00:15:26,020
to the dynamics

216
00:15:26,040 --> 00:15:28,880
for every one is infinite

217
00:15:30,770 --> 00:15:34,590
but i do this next week when i'm back in my

218
00:15:34,610 --> 00:15:39,820
because i have all this information in some some files in my office

219
00:15:39,840 --> 00:15:40,840
i mean that's

220
00:15:40,880 --> 00:15:47,730
serious much quicker to consign that

221
00:16:15,570 --> 00:16:21,210
in research

222
00:16:21,230 --> 00:16:22,650
in our research

223
00:16:22,670 --> 00:16:23,840
yes we do

224
00:16:23,860 --> 00:16:25,960
i come back later for one

225
00:16:25,980 --> 00:16:29,550
slides of our research

226
00:16:33,050 --> 00:16:34,710
a typical example

227
00:16:35,440 --> 00:16:39,770
well it's so hard to do this to go back and forth otherwise i'll show

228
00:16:39,790 --> 00:16:41,750
you immunity it's like

229
00:16:41,770 --> 00:16:45,130
if you look at the end my last slide

230
00:16:46,860 --> 00:16:48,810
maria sharapova

231
00:16:48,820 --> 00:16:55,710
cases that i don't know some some business associates is saved when she won the

232
00:16:58,690 --> 00:17:01,190
detecting the sentence

233
00:17:01,190 --> 00:17:02,650
the type

234
00:17:02,690 --> 00:17:04,480
the predicates are

235
00:17:04,500 --> 00:17:05,710
the for

236
00:17:05,750 --> 00:17:08,630
that expresses a certain action

237
00:17:08,650 --> 00:17:10,790
and you detect that active

238
00:17:10,790 --> 00:17:13,810
who is doing that action

239
00:17:13,810 --> 00:17:16,130
you detect also

240
00:17:16,170 --> 00:17:22,860
some maybe some object manipulation here so what is the patient of that action

241
00:17:22,880 --> 00:17:26,550
and you detect where it happened maybe in melbourne or

242
00:17:26,570 --> 00:17:31,920
park or something and when it happens so these are difficult task so this is

243
00:17:31,920 --> 00:17:33,820
semantic role labeling

244
00:17:33,840 --> 00:17:35,130
in the sentence

245
00:17:35,150 --> 00:17:37,250
and so the

246
00:17:37,290 --> 00:17:40,090
they are difficult task a difficult task

247
00:17:40,190 --> 00:17:43,750
well you could use such conditions such a linear chain

248
00:17:43,750 --> 00:17:47,070
conditional random field

249
00:17:47,090 --> 00:17:49,520
so that's one example

250
00:17:49,540 --> 00:17:54,170
i would some other examples later

251
00:17:57,460 --> 00:17:59,940
first of all that

252
00:18:00,120 --> 00:18:04,570
well i don't

253
00:18:04,570 --> 00:18:09,070
if some information about semantic topic models

254
00:18:09,070 --> 00:18:10,880
because i think they are

255
00:18:12,090 --> 00:18:18,130
for information purposes and for the winter

256
00:18:18,150 --> 00:18:24,290
later you get some very simple i think very interesting research path

257
00:18:24,310 --> 00:18:25,690
so the

258
00:18:25,860 --> 00:18:29,000
it's for the school's football

259
00:18:29,020 --> 00:18:31,090
so we currently

260
00:18:32,690 --> 00:18:40,210
you can also use these models to limited amount it some amount of similar division

261
00:18:40,230 --> 00:18:42,480
these models

262
00:18:42,480 --> 00:18:44,340
domain is empty right

263
00:18:44,360 --> 00:18:47,750
but the domain must not be empty as i said before this is some some

264
00:18:47,750 --> 00:18:50,000
presupposition here so

265
00:18:52,460 --> 00:18:57,290
can use an inference of this is that the whole set of axioms specified using

266
00:18:57,290 --> 00:18:58,480
some editor

267
00:18:58,500 --> 00:19:02,400
doesn't make sense at all if the tee box satisfiable

268
00:19:02,590 --> 00:19:07,880
and also concept subsumption is an important service

269
00:19:07,900 --> 00:19:13,560
if you are familiar with other logic few real very easily identify that a UCQ

270
00:19:13,560 --> 00:19:20,400
is the fragment of first-order logic concepts correspond to unitary predicates roles are binary predicates

271
00:19:20,560 --> 00:19:26,710
the concept descriptions can be seen as first order logic formula with one free variable

272
00:19:26,710 --> 00:19:28,040
look at this one here

273
00:19:28,090 --> 00:19:33,500
so for all white holes that had of x y implies department of y and

274
00:19:33,500 --> 00:19:36,590
x is the free variables here

275
00:19:36,610 --> 00:19:39,940
pretty much the same using in in this example

276
00:19:41,150 --> 00:19:45,340
concept describes the formula with one free variable and the GCI

277
00:19:45,340 --> 00:19:47,980
that i have introduced before

278
00:19:48,000 --> 00:19:52,580
are a formula without free variables so-called sentences look

279
00:19:53,770 --> 00:19:58,130
DCI corresponds to this formula here

280
00:19:58,150 --> 00:20:02,790
no free variables any you could argue that so why should i deal with description

281
00:20:02,790 --> 00:20:04,060
logics at all

282
00:20:04,840 --> 00:20:06,630
the point here is that for this

283
00:20:06,710 --> 00:20:10,650
specific fragment of first-order logic which has a very

284
00:20:10,670 --> 00:20:17,130
intuitive and convenient syntax that allows you to grasp the set semantics very easily

285
00:20:17,150 --> 00:20:24,230
decidability can be proven and also complexity results of the decision problems are available and

286
00:20:24,230 --> 00:20:28,090
practical implementations for than actually solving the is

287
00:20:28,810 --> 00:20:31,730
the decision problems can can be

288
00:20:31,750 --> 00:20:34,150
downloaded so this is the major

289
00:20:37,560 --> 00:20:40,980
considering specific fragments

290
00:20:47,520 --> 00:20:50,360
and what using ontology means

291
00:20:50,420 --> 00:20:53,360
coming up with a set of

292
00:20:55,520 --> 00:20:56,540
i have the

293
00:20:56,560 --> 00:21:01,270
i have repeated the a set of axioms here that we have discussed before and

294
00:21:02,110 --> 00:21:03,230
once we have

295
00:21:03,250 --> 00:21:08,610
we use the machine to check out whether they make sense

296
00:21:08,630 --> 00:21:12,750
in this case it's pretty obvious that the tee box is satisfiable

297
00:21:12,880 --> 00:21:20,060
then it might be interesting to find out whether there are implicit subsumption relationships

298
00:21:20,080 --> 00:21:22,130
or whether there are

299
00:21:22,150 --> 00:21:25,360
concepts that are not satisfiable

300
00:21:25,960 --> 00:21:27,580
one is interested

301
00:21:27,590 --> 00:21:29,770
in an overview about

302
00:21:29,790 --> 00:21:31,250
the whole

303
00:21:31,290 --> 00:21:41,080
ontology and so one is usually interested in finding the set of concept names

304
00:21:41,130 --> 00:21:42,310
that are

305
00:21:42,320 --> 00:21:44,250
more general than one

306
00:21:44,250 --> 00:21:46,810
concept name might focus on

307
00:21:46,820 --> 00:21:52,790
so the idea is that one takes every concept name that is mentioned in the

308
00:21:52,840 --> 00:21:53,920
t box

309
00:21:53,920 --> 00:21:59,080
or in the signature if you want and find out the most specific concept names

310
00:21:59,080 --> 00:22:01,440
that subsume this

311
00:22:01,480 --> 00:22:07,230
concept name that we focus on the the common subsumer is here are called parents

312
00:22:07,460 --> 00:22:13,310
you might also define the children in the most general concept names which are subsumed

313
00:22:13,310 --> 00:22:15,710
by the concept name and

314
00:22:16,320 --> 00:22:19,540
the reason is usually supported service that

315
00:22:19,540 --> 00:22:24,980
in that implements an efficient algorithms to find all the parents and children for every

316
00:22:24,980 --> 00:22:30,170
concept name that is mentioned in the t box

317
00:22:30,190 --> 00:22:35,110
and this is this whole process is known as classifying the tee box or is

318
00:22:35,110 --> 00:22:40,920
classification process if you have a look at the tee box here so we identify

319
00:22:40,940 --> 00:22:46,400
person and professor fullprofessor student

320
00:22:46,520 --> 00:22:47,520
on the grid

321
00:22:51,210 --> 00:22:53,770
department right and

322
00:22:53,810 --> 00:22:56,650
you from from looking at the

323
00:22:56,670 --> 00:22:59,880
DCI stem from considering the semantics

324
00:22:59,900 --> 00:23:02,400
we find some apparent

325
00:23:02,420 --> 00:23:10,380
concept subsumption relationship or apparent relationships the professor

326
00:23:10,400 --> 00:23:11,460
if this up

327
00:23:11,480 --> 00:23:16,880
the concept of person and even child of person full professor the child

328
00:23:19,810 --> 00:23:23,770
this is pretty obvious from considering the extent to most people are just fine with

329
00:23:23,770 --> 00:23:28,880
this but i would like to argue that is important to use the reasoning services

330
00:23:29,170 --> 00:23:34,250
to find implicit subsumption relationships and i mean

331
00:23:34,340 --> 00:23:37,060
given all the obvious stuff here

332
00:23:41,820 --> 00:23:45,210
for you it's not a big problem to me

333
00:23:45,210 --> 00:23:46,380
see that

334
00:23:46,400 --> 00:23:48,710
are chair here is

335
00:23:48,730 --> 00:23:51,630
a person might if we focus on this

336
00:23:51,650 --> 00:23:52,790
relation here

337
00:23:52,960 --> 00:24:00,790
we can press the button and get a print out of this

338
00:24:00,790 --> 00:24:06,770
child parent relationship the graph structure is usually known as the taxonomy

339
00:24:06,820 --> 00:24:09,310
it can be shown on the screen

340
00:24:09,360 --> 00:24:11,670
either with

341
00:24:11,670 --> 00:24:16,020
four that use or using some graph display you know this thing

342
00:24:16,880 --> 00:24:18,840
we should probably and

343
00:24:18,880 --> 00:24:21,480
some GCI year

344
00:24:23,110 --> 00:24:25,060
it becomes apparent that

345
00:24:25,060 --> 00:24:25,920
if we

346
00:24:25,940 --> 00:24:26,920
and this

347
00:24:26,940 --> 00:24:33,610
looks tricky i will explain in the second there there implicit subsumption relationships introduced at

348
00:24:35,270 --> 00:24:37,310
of which you might not be aware of

349
00:24:37,320 --> 00:24:43,080
so look what we're specifying here is that this is the set right

350
00:24:43,080 --> 00:24:46,900
and this is the set of all the things of the world that are on

351
00:24:46,900 --> 00:24:48,880
the left-hand side of

352
00:24:48,900 --> 00:24:52,290
head off

353
00:24:52,320 --> 00:24:58,290
you remember we describe this set of objects using the complex concept descriptions and we

354
00:24:58,290 --> 00:25:01,880
say this is the set of objects acts

355
00:25:01,900 --> 00:25:06,270
for which there exists a partner on the right-hand side as in it in a

356
00:25:06,270 --> 00:25:09,840
table of the head of relations

357
00:25:09,860 --> 00:25:14,750
we don't impose any severe restrictions on the right-hand side

358
00:25:14,770 --> 00:25:16,060
we just say

359
00:25:16,060 --> 00:25:17,720
we just

360
00:25:17,720 --> 00:25:22,720
here the factorisation applied by conditional independence in these

361
00:25:22,740 --> 00:25:25,430
joint probability distribution

362
00:25:25,510 --> 00:25:27,520
so here we have the joint

363
00:25:27,560 --> 00:25:29,870
we make no assumptions

364
00:25:30,790 --> 00:25:32,870
no conditional independence

365
00:25:33,010 --> 00:25:36,850
here we do make conditional independence assumption

366
00:25:36,850 --> 00:25:39,140
that condition independence assumption we saw

367
00:25:40,640 --> 00:25:44,290
these factorizations in particular

368
00:25:44,290 --> 00:25:49,330
so now let's do the same thing let's some on x one and x three

369
00:25:49,370 --> 00:25:54,220
this quantity but now this quantity is written this way

370
00:25:56,970 --> 00:26:00,430
since this quantity is written that way

371
00:26:00,450 --> 00:26:03,240
there is a marked tree we can use

372
00:26:03,240 --> 00:26:07,120
to make computations more efficient

373
00:26:07,200 --> 00:26:08,580
what's this

374
00:26:08,600 --> 00:26:11,160
well the smart trick is very simple

375
00:26:11,200 --> 00:26:15,560
we realize that we are some on two variables x one and x three

376
00:26:18,640 --> 00:26:22,370
and we see that some of these factors

377
00:26:22,430 --> 00:26:25,970
don't depend on x y

378
00:26:25,990 --> 00:26:29,720
for example these two factors will depend on x one

379
00:26:29,810 --> 00:26:33,870
some of these factors don't depend on next three for example the first factor does

380
00:26:33,870 --> 00:26:36,700
not depend on x

381
00:26:36,760 --> 00:26:39,200
so some factors are constant

382
00:26:39,240 --> 00:26:42,540
with regard to some some advice

383
00:26:45,280 --> 00:26:47,580
hopefully take advantage of

384
00:26:50,290 --> 00:26:55,700
let's just make this summation let's open this summation x one x three into two

385
00:26:55,700 --> 00:26:59,560
explicit summations and x three and x one

386
00:26:59,620 --> 00:27:01,290
let's school

387
00:27:01,350 --> 00:27:07,490
behind affects one here everything that does not mean does not depend on x one

388
00:27:07,510 --> 00:27:12,200
because that's not constant for the summation

389
00:27:12,450 --> 00:27:16,240
mathematically what i'm doing what i'm doing this

390
00:27:17,290 --> 00:27:22,180
he's using what we call the stream with what

391
00:27:23,470 --> 00:27:24,870
i'm taking

392
00:27:24,890 --> 00:27:27,760
called factor

393
00:27:27,790 --> 00:27:30,160
in the summer

394
00:27:30,180 --> 00:27:36,100
pulling its outside

395
00:27:36,140 --> 00:27:41,040
if i have a common factor

396
00:27:41,060 --> 00:27:42,120
in the summer

397
00:27:42,120 --> 00:27:44,810
the common factor is a

398
00:27:44,810 --> 00:27:46,280
how can i write it

399
00:27:46,290 --> 00:27:48,260
well this is equal

400
00:27:48,260 --> 00:27:51,200
two eight times be policy

401
00:27:53,600 --> 00:27:56,290
that's exactly what i'm doing here

402
00:27:56,470 --> 00:28:00,830
in the summer over x one i have a common factor which is the use

403
00:28:05,020 --> 00:28:06,950
if i keep like these

404
00:28:07,890 --> 00:28:09,850
we have three

405
00:28:09,870 --> 00:28:14,350
o operations

406
00:28:14,490 --> 00:28:19,180
i have on multiple occasions he another multiplication you have

407
00:28:19,290 --> 00:28:27,620
if i like these i have to average

408
00:28:28,740 --> 00:28:32,510
and if you want to see a wonderful paper that part of all these

409
00:28:32,580 --> 00:28:35,220
look at something called

410
00:28:35,280 --> 00:28:41,100
the generalized distributive law

411
00:28:41,120 --> 00:29:08,200
just google for

412
00:29:08,200 --> 00:29:10,870
the generalized distributive law

413
00:29:10,910 --> 00:29:13,040
the paper to basically talk

414
00:29:13,080 --> 00:29:15,930
basically describes all these operations

415
00:29:15,950 --> 00:29:20,010
as simply and generalized application of the distributive law

416
00:29:20,060 --> 00:29:21,520
the key idea is that

417
00:29:21,540 --> 00:29:25,580
these quantities here is equal to user one here

418
00:29:25,600 --> 00:29:29,870
but if you do these operations you have

419
00:29:29,910 --> 00:29:31,700
cost of two

420
00:29:31,720 --> 00:29:35,410
this operation have a cost of three

421
00:29:35,430 --> 00:29:38,810
so when we have a course of three you just fall off

422
00:29:39,760 --> 00:29:42,290
first the songs and then multiply

423
00:29:42,310 --> 00:29:44,720
that's exactly what's going on

424
00:29:44,720 --> 00:29:47,120
that simple fact

425
00:29:47,140 --> 00:29:51,580
i will show up immediately here

426
00:29:51,620 --> 00:29:53,350
what's the result of this

427
00:29:54,740 --> 00:29:56,060
well first

428
00:29:56,080 --> 00:29:57,680
these summation here

429
00:29:57,700 --> 00:30:00,330
on which it is

430
00:30:00,390 --> 00:30:04,120
just the some

431
00:30:04,180 --> 00:30:12,560
some x one of p of x one given x two

432
00:30:14,520 --> 00:30:17,240
well this this has to be one right

433
00:30:17,280 --> 00:30:19,680
and regardless of x two

434
00:30:19,740 --> 00:30:23,390
conditional probabilities approach right

435
00:30:26,140 --> 00:30:29,260
these objects is one

436
00:30:29,260 --> 00:30:32,870
the current index on this

437
00:30:32,870 --> 00:30:35,170
and then you would switch over to

438
00:30:35,190 --> 00:30:39,370
new index by tapping into the free have this

439
00:30:39,420 --> 00:30:41,150
new wide field and

440
00:30:41,210 --> 00:30:46,140
when you copy the new index back to be faster happiness

441
00:30:46,150 --> 00:30:47,490
you do a deal

442
00:30:47,500 --> 00:30:48,960
the original first copy

443
00:30:48,980 --> 00:30:53,480
and then now would actually free up some disk space that you could use for

444
00:30:53,480 --> 00:30:56,420
building various performance improving data structures in the middle

445
00:30:56,740 --> 00:30:59,570
one of the things we built was something we call the pair cache which is

446
00:30:59,570 --> 00:31:02,870
basically if you look at commonly co occurring query terms

447
00:31:02,910 --> 00:31:09,150
we would offline precompute pre intersected pairs of posting lists for these commonly co occurring

448
00:31:10,310 --> 00:31:14,650
i don't have to be phrase turn things have to be terms that occur frequently

449
00:31:14,650 --> 00:31:16,070
increased together

450
00:31:16,090 --> 00:31:20,260
so we're not building a phrase index it's just you know barcelona and restaurants occurred

451
00:31:20,260 --> 00:31:23,700
in the same document

452
00:31:23,880 --> 00:31:29,960
by two thousand we refined our hardware designer but we had

453
00:31:29,970 --> 00:31:32,410
decided cases are OK

454
00:31:32,800 --> 00:31:37,430
and we managed to get all the connectors on the front of the computers

455
00:31:38,020 --> 00:31:42,450
it was quite painful to reach around the back field corporate

456
00:31:42,510 --> 00:31:44,170
the ethernet cable

457
00:31:44,190 --> 00:31:49,670
and we were still not only are in data centers we were leasing space in

458
00:31:49,680 --> 00:31:53,880
hosting facilities and they charged by the square foot which was an excellent

459
00:31:53,880 --> 00:31:54,870
the of

460
00:31:54,920 --> 00:31:58,230
so we would try to pack as many computers as we possibly could use much

461
00:31:58,230 --> 00:32:04,900
electricity as we possibly could into the number square-foot scruffy buried licenses

462
00:32:05,330 --> 00:32:07,320
so that was good

463
00:32:07,350 --> 00:32:08,200
they a

464
00:32:08,220 --> 00:32:10,810
sometimes need a little extra help with their cooling

465
00:32:10,810 --> 00:32:12,610
we were purchased fans for them

466
00:32:14,590 --> 00:32:18,150
as a consequence we also got fairly good moving out of bankruptcy data centers

467
00:32:18,180 --> 00:32:19,820
and moving into new ones

468
00:32:19,820 --> 00:32:23,320
and so

469
00:32:23,370 --> 00:32:26,040
the nice thing is

470
00:32:26,060 --> 00:32:30,000
actually too hard to move into a new data centre have other acts all cable

471
00:32:30,000 --> 00:32:31,040
operator a

472
00:32:31,590 --> 00:32:35,290
and then you basically have to we'll interaction then connect together all the tracks which

473
00:32:35,290 --> 00:32:40,900
is to central router and you're ready to go

474
00:32:40,980 --> 00:32:44,620
so in the period of nineteen ninety two thousand one roughly

475
00:32:44,630 --> 00:32:45,770
we were

476
00:32:45,820 --> 00:32:50,720
simultaneously having huge traffic increases and huge increases in index size

477
00:32:50,750 --> 00:32:56,100
so our index size when roughly by factor twenty and at the same time as

478
00:32:56,100 --> 00:32:58,220
we were having roughly

479
00:32:58,230 --> 00:33:02,890
can you know ten fifteen twenty percent growth per month increase traffic

480
00:33:03,080 --> 00:33:07,970
and we are also out signing deals for example we find search

481
00:33:08,430 --> 00:33:10,940
deal with yahoo to provide their search service

482
00:33:10,950 --> 00:33:13,910
and that doubled traffic overnight in the same time we were actually

483
00:33:13,970 --> 00:33:15,840
doubling our index size

484
00:33:15,840 --> 00:33:17,120
the previous few days

485
00:33:17,130 --> 00:33:19,390
it's quite exciting

486
00:33:19,410 --> 00:33:24,020
and the performance of the index of system is what is really important

487
00:33:24,040 --> 00:33:28,770
is it in when you're dealing with this sort of thing we're continuously buying more

488
00:33:28,820 --> 00:33:32,930
more hardware and releasing more and more data center space but we basically needed ten

489
00:33:32,950 --> 00:33:37,700
to thirty percent software improvements every month in order to sort of sweet by our

490
00:33:38,860 --> 00:33:41,650
otherwise we would have no end up

491
00:33:41,690 --> 00:33:43,410
in a bad state

492
00:33:45,620 --> 00:33:47,490
so the nice thing about this basic

493
00:33:47,730 --> 00:33:54,070
structure now move doctor after the side is another interesting from from the standpoint but

494
00:33:54,070 --> 00:33:56,650
i think is that it's fairly easy to

495
00:33:56,900 --> 00:34:00,410
had capacity new

496
00:34:00,410 --> 00:34:03,110
you can grow your indexed by adding more in charge

497
00:34:03,130 --> 00:34:04,620
and then you can

498
00:34:04,640 --> 00:34:08,250
more replicas that capacity

499
00:34:08,270 --> 00:34:11,730
and if you're very you're next slide that morning charge

500
00:34:11,740 --> 00:34:13,490
and more charges

501
00:34:13,540 --> 00:34:18,240
and more replicas for capacity and more dangerous and eventually end up a lot of

502
00:34:18,240 --> 00:34:21,050
machines and one charge replicas

503
00:34:21,100 --> 00:34:23,190
but it's a fairly

504
00:34:23,230 --> 00:34:28,600
reasonable process to do that as you do you don't try to do this in

505
00:34:28,600 --> 00:34:32,640
the middle of indexes sort of say OK that the next month's index we're going

506
00:34:32,640 --> 00:34:35,090
to do seventeen jar instead of ten

507
00:34:37,270 --> 00:34:41,080
so one of the things one of the reasons you have to keep that in

508
00:34:41,080 --> 00:34:45,320
charge as you increase the index size is that you want to have query response

509
00:34:45,320 --> 00:34:46,200
times the

510
00:34:46,300 --> 00:34:50,800
reasonably low and if you make sure too large well it might not fit on

511
00:34:50,970 --> 00:34:53,440
the local disk severe machines or

512
00:34:54,130 --> 00:34:57,560
your response time we get too large and so you need to sort of increasing

513
00:34:57,560 --> 00:35:02,950
numbers charts in linearly as you're increasing during exercise

514
00:35:04,970 --> 00:35:08,690
it's also possible to get pretty big performance improvements which we did by

515
00:35:08,700 --> 00:35:13,790
writing around disk scheduling system that was aware of what pending on that particular server

516
00:35:13,850 --> 00:35:17,060
and prioritizing reads in a reasonable way

517
00:35:17,070 --> 00:35:21,000
we also focused the firm on improving our index encoding which is what i mean

518
00:35:21,060 --> 00:35:23,240
by now

519
00:35:23,260 --> 00:35:24,990
so the original encoding

520
00:35:25,000 --> 00:35:31,490
done as part of the prototype stanford was pretty simple it was actually fairly simple

521
00:35:31,490 --> 00:35:35,450
byte aligned format basically you have

522
00:35:35,570 --> 00:35:39,900
the document identifier plus the number of hits including thirty two bit five but for

523
00:35:39,900 --> 00:35:44,480
the number of hits and twenty seven the ducati many sixteen bit for each word

524
00:35:45,490 --> 00:35:49,670
for each occurrence of that particular word that encoded by the position of the word

525
00:35:49,710 --> 00:35:53,130
as a sequence number within the document

526
00:35:53,170 --> 00:35:54,350
and some

527
00:35:54,370 --> 00:35:56,580
a few bits for attributes of

528
00:35:56,600 --> 00:35:59,480
the words like was it is in the the title

529
00:35:59,500 --> 00:36:02,420
was it in a big fighter little font

530
00:36:02,860 --> 00:36:03,930
and so on

531
00:36:05,320 --> 00:36:09,990
and for very long posting lists you eventually want skip tables into

532
00:36:10,010 --> 00:36:14,390
the data structure so that if a you have the word of or something if

533
00:36:14,390 --> 00:36:16,790
you have a table on the side that tells you

534
00:36:16,790 --> 00:36:17,900
we docherty

535
00:36:17,910 --> 00:36:20,320
a million is that of set

536
00:36:20,340 --> 00:36:21,320
you know

537
00:36:22,020 --> 00:36:25,720
two billion two billion in year posting list

538
00:36:27,770 --> 00:36:33,150
by the line format was pretty simple decoders every compact and it actually required lots

539
00:36:33,150 --> 00:36:36,490
of space so we work for fair amount of our

540
00:36:36,540 --> 00:36:38,910
including data structures

541
00:36:38,920 --> 00:36:43,130
and give you a brief background on

542
00:36:43,130 --> 00:36:44,570
grace encoding techniques

543
00:36:44,580 --> 00:36:50,860
because the be useful to understand the encoding format we used basically there's several different

544
00:36:50,860 --> 00:36:53,650
bit level encodings you can imagine using which are going to be slower than by

545
00:36:53,670 --> 00:36:59,230
level encodings buildings are things like you encoding re two in the number and you

546
00:36:59,230 --> 00:37:04,010
have an ones followed by zero gamma you're going to encode some prefix and unitary

547
00:37:04,420 --> 00:37:07,150
indicates the length of the number and that number of bits

548
00:37:08,700 --> 00:37:11,390
which is a special form of golomb codes

549
00:37:11,390 --> 00:37:15,990
are you you basically have a number and you have a base

550
00:37:16,110 --> 00:37:19,370
during encode that number and and right because the base is always going to be

551
00:37:19,370 --> 00:37:20,650
part of two

552
00:37:20,670 --> 00:37:21,950
so for example

553
00:37:21,960 --> 00:37:24,210
you encode

554
00:37:24,230 --> 00:37:26,630
and divided by what their bases

555
00:37:26,630 --> 00:37:31,740
three then you can and to encode and divided by two to the third engineering

556
00:37:31,740 --> 00:37:33,750
and then and model

557
00:37:35,190 --> 00:37:36,140
three that's

558
00:37:37,590 --> 00:37:39,430
and preserving

559
00:37:41,740 --> 00:37:42,840
before we

560
00:37:42,850 --> 00:37:47,270
used for a while was basically a block based index format so

561
00:37:47,290 --> 00:37:51,180
the nice property that had so we're going to have skip table and then it's

562
00:37:51,180 --> 00:37:52,690
going to a sequence of blocks

563
00:37:52,690 --> 00:37:54,250
welcome to

564
00:37:54,260 --> 00:37:57,310
the first course happy to have chris bishop here

565
00:37:57,320 --> 00:37:59,880
i flew in yesterday with his own playing

566
00:37:59,900 --> 00:38:07,470
he is assistant director of microsoft research lab in cambridge UK before that he

567
00:38:08,450 --> 00:38:14,980
quite a substantial academic career but now he's directing the efforts in machine learning and

568
00:38:14,980 --> 00:38:16,170
computer vision

569
00:38:16,180 --> 00:38:19,990
at microsoft research labs so we're very happy to have him here is one of

570
00:38:19,990 --> 00:38:23,460
the world experts on graphical models and bayesian methods

571
00:38:23,900 --> 00:38:27,450
before we start with this course we would like to do a little

572
00:38:29,040 --> 00:38:33,140
it turns out that it would be inconvenient for him to give the last two

573
00:38:33,140 --> 00:38:37,510
hours on monday afternoons so if you have your schedule handy i suggest you just

574
00:38:37,510 --> 00:38:39,120
take it out briefly

575
00:38:39,140 --> 00:38:43,790
because we have two options how to deal with this issue

576
00:38:43,810 --> 00:38:45,790
so we propose to

577
00:38:45,800 --> 00:38:52,000
cancel the lectures bishop seven to eight on monday the twenty eighth of september

578
00:38:52,020 --> 00:38:55,000
and we have two possibilities where we could

579
00:38:55,510 --> 00:39:01,650
catch up with this time that we're losing number one would be to have ascended

580
00:39:01,650 --> 00:39:06,260
special saturday afternoon not on a saturday afternoon session

581
00:39:06,300 --> 00:39:12,480
you probably mean that chris teachers hours in the morning and we would have a

582
00:39:13,110 --> 00:39:14,130
two and three

583
00:39:14,140 --> 00:39:17,980
in the afternoon the other so that we eat into our

584
00:39:18,020 --> 00:39:24,100
precious saturday afternoon times that's why we thought of another solution to which he's into

585
00:39:24,920 --> 00:39:30,960
lunchtime but not not really i guess the only it into the post-lunch nap time

586
00:39:30,970 --> 00:39:36,610
and that would be to have two hours on both thursday and friday so one

587
00:39:36,610 --> 00:39:41,930
on thursday one on friday both between two fifteen three

588
00:39:41,940 --> 00:39:46,360
so it would still finish in time for the people taking the practical sessions and

589
00:39:46,390 --> 00:39:50,210
the ones who are not a practical sessions could have been that between three and

590
00:39:50,960 --> 00:40:00,860
so she we take votes who would prefer to have some extra work on saturday

591
00:40:02,960 --> 00:40:04,590
this is differ everything

592
00:40:04,600 --> 00:40:07,480
all these times referred to this week

593
00:40:07,490 --> 00:40:09,970
so who is in favor saturday

594
00:40:09,980 --> 00:40:26,450
it's not only five i would say who is in favor of the day of

595
00:40:28,910 --> 00:40:31,210
looks like more

596
00:40:31,250 --> 00:40:35,880
that's a about forty forty plus

597
00:40:35,890 --> 00:40:37,490
OK so

598
00:40:52,060 --> 00:40:57,720
well it would mean that people who

599
00:40:57,740 --> 00:41:00,230
go to the practical session on

600
00:41:00,240 --> 00:41:02,370
monday afternoon will skip

601
00:41:02,380 --> 00:41:07,350
which lectures so is complicated optimisation problem we we tried to

602
00:41:07,370 --> 00:41:12,120
if so we we put the courses bishop seven and eight in the US men

603
00:41:12,130 --> 00:41:16,190
everybody can go to number one through six and only a few people would miss

604
00:41:16,190 --> 00:41:20,110
seven and eight so if we put a good edits two and three in the

605
00:41:20,110 --> 00:41:27,310
afternoon then those people mister in three wouldn't really profit from his fourth lecture so

606
00:41:27,690 --> 00:41:30,560
i would prefer not to do that one

607
00:41:34,830 --> 00:41:38,430
both in group one

608
00:41:46,570 --> 00:41:49,450
it was amazing

609
00:41:49,460 --> 00:41:52,140
is going on here

610
00:41:52,250 --> 00:41:55,330
i mean take a quick look

611
00:41:55,350 --> 00:42:02,810
OK i'll try to double check with him but

612
00:42:02,860 --> 00:42:07,230
and we'll see if that works will do that otherwise would go for option number

613
00:42:07,230 --> 00:42:08,380
two and

614
00:42:08,390 --> 00:42:09,750
don't find out soon

615
00:42:09,760 --> 00:42:14,230
OK so i don't take more time away from chris thank you

616
00:42:14,240 --> 00:42:16,340
thank you very much indeed

617
00:42:16,420 --> 00:42:20,830
OK so i'm going to talk about two

618
00:42:20,840 --> 00:42:24,780
topic which me is represented the most interesting

619
00:42:24,800 --> 00:42:27,220
developments in the field of machine learning

620
00:42:27,630 --> 00:42:29,900
in the last decade or so

621
00:42:29,910 --> 00:42:32,530
they are distinct topics but they actually go

622
00:42:32,540 --> 00:42:38,620
rather nicely together so you'll see many links between these two topics topics being graphical

623
00:42:42,580 --> 00:42:45,070
approximation schemes

624
00:42:45,090 --> 00:42:49,970
which come the general heading of variational methods for lack of time i shall

625
00:42:49,990 --> 00:42:51,610
focusing on one

626
00:42:51,630 --> 00:42:53,190
in particular

627
00:42:53,470 --> 00:42:55,750
very popular kind of variational

628
00:42:59,170 --> 00:43:03,510
so i understand from the organisers that the audience is very heterogeneous and i can

629
00:43:03,510 --> 00:43:06,270
see a few experts sitting in the audience but i know a lot of people

630
00:43:06,320 --> 00:43:08,830
are in the first year phd

631
00:43:08,840 --> 00:43:14,290
so you have an insoluble problem i can't keep everybody happy the whole time what

632
00:43:14,290 --> 00:43:19,090
i'm going to do something for the first few lectures is issues things fairly basic

633
00:43:19,860 --> 00:43:24,200
that is i'm going to assume that you know nothing about graphical models or variational

634
00:43:24,200 --> 00:43:27,590
inference and just start from the beginning i shall assume

635
00:43:27,610 --> 00:43:29,330
you understand the

636
00:43:29,340 --> 00:43:31,560
the sum and product rule of probability

637
00:43:31,800 --> 00:43:36,000
if you don't then you probably lasted very five you

638
00:43:36,010 --> 00:43:43,260
OK let me just give you some roadmap where we're going

639
00:43:43,270 --> 00:43:43,810
so i'm going to

640
00:43:43,830 --> 00:43:45,190
talk about two

641
00:43:45,200 --> 00:43:48,800
the two main kinds of graphical model

642
00:43:48,810 --> 00:43:50,310
directed graphs

643
00:43:50,330 --> 00:43:53,260
and undirected graphs

644
00:43:53,270 --> 00:43:55,850
these graphs do their pectoral

645
00:43:55,850 --> 00:44:00,880
that's important we really need that our algorithm can be phrased as a decision tree

646
00:44:00,890 --> 00:44:03,620
and in particular we now from this transformation

647
00:44:03,640 --> 00:44:08,160
that all comparison sorts can be represented as a decision tree but there are some

648
00:44:08,160 --> 00:44:12,050
sorting algorithms which cannot be represented as a decision tree and we will turn to

649
00:44:12,050 --> 00:44:17,640
that momentarily before we get there

650
00:44:17,650 --> 00:44:22,730
i phrase this theorem as a lower bound on decision tree sorting but of course

651
00:44:22,730 --> 00:44:26,610
we also get a lower bound on comparison sorting

652
00:44:26,620 --> 00:44:28,550
and in particular tells us

653
00:44:28,560 --> 00:44:32,770
that merge sort and heapsort

654
00:44:32,780 --> 00:44:42,510
are asymptotically optimal their dependence on and in terms of asymptotic notation

655
00:44:42,520 --> 00:44:44,500
so ignoring constant factors

656
00:44:44,510 --> 00:44:46,760
these algorithms are often

657
00:44:46,810 --> 00:44:52,300
in terms of growth of n

658
00:44:52,320 --> 00:44:55,810
but this is only in the comparison model

659
00:44:55,960 --> 00:44:59,840
so for among comparison sorting algorithms

660
00:45:00,600 --> 00:45:01,680
which these are

661
00:45:01,680 --> 00:45:03,650
asymptotically optimal use

662
00:45:03,730 --> 00:45:06,770
the minimum number of comparisons up to constant factors

663
00:45:06,780 --> 00:45:08,540
in fact the whole running time is

664
00:45:08,550 --> 00:45:12,290
dominated by the number of comparisons sulfate and organ

665
00:45:12,340 --> 00:45:14,790
so this is good news

666
00:45:14,810 --> 00:45:19,450
and i should probably mention a little bit what happens with randomized algorithms what i've

667
00:45:19,450 --> 00:45:20,640
described here

668
00:45:20,660 --> 00:45:24,550
really only applies in some sense to deterministic algorithms

669
00:45:24,550 --> 00:45:25,820
any one

670
00:45:25,830 --> 00:45:27,290
so anyone seems

671
00:45:27,290 --> 00:45:28,790
what would change

672
00:45:28,790 --> 00:45:36,910
with randomized diagrams or wherever assume that i have deterministic comparison sort

673
00:45:36,930 --> 00:45:38,040
this is a bit subtle

674
00:45:38,050 --> 00:45:40,950
and i only noticed it reading the notes this morning

675
00:45:54,030 --> 00:45:56,480
i give you hands is over here

676
00:45:56,490 --> 00:46:09,050
the right hand side of the world

677
00:46:17,360 --> 00:46:19,940
five deterministic algorithm

678
00:46:19,950 --> 00:46:23,240
they are does is completely determined at each step

679
00:46:25,980 --> 00:46:29,900
as long as i know all the comparisons that are made up to some points

680
00:46:29,900 --> 00:46:36,140
it's determine what that algorithm will do

681
00:46:36,160 --> 00:46:38,380
but if i have a randomized algorithm

682
00:46:38,400 --> 00:46:42,630
it also depends on the outcomes of some quite flips

683
00:46:42,640 --> 00:46:52,180
so any suggestions what breaks over here

684
00:46:52,200 --> 00:46:55,290
there's more than one tree exact

685
00:46:55,310 --> 00:46:59,140
so we have this assumption that we only have one tree for each and in

686
00:46:59,140 --> 00:47:03,160
fact what we get is the probability distribution over trees

687
00:47:03,180 --> 00:47:06,780
for each value of and if you take all the possible

688
00:47:06,800 --> 00:47:10,650
executions of that out all the instruction traces one now

689
00:47:10,660 --> 00:47:14,710
in addition to branching comparisons we also suppression whether coin flip came out heads or

690
00:47:15,900 --> 00:47:21,200
however we're generating random numbers came up with some value between one and

691
00:47:21,210 --> 00:47:26,310
so we get a probability distribution over trees this lower bound still applies though

692
00:47:26,330 --> 00:47:29,850
because no matter what tree we get i i don't really care i get at

693
00:47:29,850 --> 00:47:32,090
least one tree for each

694
00:47:32,100 --> 00:47:33,110
and this

695
00:47:33,120 --> 00:47:37,730
proof applies to every tree some at no matter what tree you get it has

696
00:47:37,730 --> 00:47:40,210
to have if it's the correct tree

697
00:47:40,220 --> 00:47:41,570
it has to have high

698
00:47:41,590 --> 00:47:46,140
omega and log n so even this lower bound applies even for randomized algorithms can

699
00:47:46,140 --> 00:47:48,450
get better and log n

700
00:47:48,450 --> 00:47:52,850
because no matter what really comes up with no matter how those concepts come out

701
00:47:55,220 --> 00:48:00,580
this argument still applies every tree that comes out has to be correct

702
00:48:00,600 --> 00:48:02,460
so this is really

703
00:48:02,460 --> 00:48:04,660
at least one tree

704
00:48:04,680 --> 00:48:14,820
and that that will now work so we also get the fact that randomized quicksort

705
00:48:14,930 --> 00:48:20,060
is asymptotically optimal in expectation

706
00:48:26,290 --> 00:48:34,100
but in order to say the randomized quicksort is asymptotically optimal

707
00:48:34,100 --> 00:48:36,630
now we've got a normal distribution

708
00:48:36,680 --> 00:48:39,220
and if if the

709
00:48:39,290 --> 00:48:43,340
observation that you actually see falls outside of this gate let's say plus and minus

710
00:48:43,340 --> 00:48:46,770
two sigma points on the normal distribution is reasonable to

711
00:48:50,130 --> 00:48:55,010
everything does so now we can see the same fine track running with this validation

712
00:48:55,010 --> 00:48:59,720
gate now because the validation gate is the distributed thing because we are making many

713
00:49:00,650 --> 00:49:05,580
and along normals along this curve and so we can define

714
00:49:06,320 --> 00:49:09,470
the expected range of observations any point on the curve so

715
00:49:09,490 --> 00:49:12,620
the whole thing could be depicted as the tube

716
00:49:12,670 --> 00:49:15,390
with the mean of this

717
00:49:15,400 --> 00:49:21,870
i mean that i mean prediction on the solid line and the range of observations

718
00:49:21,920 --> 00:49:24,630
depicted along the student so what you see here

719
00:49:24,650 --> 00:49:26,940
is that the lower part of the hand

720
00:49:26,960 --> 00:49:30,060
has just fallen outside the validation gate

721
00:49:30,070 --> 00:49:33,280
so now the observation process is going to fail around here

722
00:49:33,300 --> 00:49:38,440
and because the observation process fails no observations made around here

723
00:49:38,460 --> 00:49:43,610
the only half of the kalman filter as it were operates over those

724
00:49:44,870 --> 00:49:48,600
simplifying things like because obviously

725
00:49:48,610 --> 00:49:50,790
you know we're not shopping the curves up into account

726
00:49:50,880 --> 00:49:56,360
bits exactly but in this region the effect of observations and felt so all that

727
00:49:56,360 --> 00:49:59,860
happens is that the variance builds up from successive

728
00:49:59,900 --> 00:50:05,780
predictions and the range gates for validation gate fans and fans until of course you

729
00:50:05,780 --> 00:50:10,020
reach a point like this one were sufficiently fact that now

730
00:50:12,470 --> 00:50:15,400
features again and everything will recover

731
00:50:15,410 --> 00:50:18,400
and once we start measuring here then

732
00:50:18,420 --> 00:50:23,100
that kind of punctures the validation gate you always hear the hiss there

733
00:50:23,110 --> 00:50:29,910
coming out as the validation gate measurements made again validation gate collapses and actually

734
00:50:29,920 --> 00:50:34,710
you know it's very well known that when all observations are present and validation gate

735
00:50:34,810 --> 00:50:38,980
and switching any of them off then you will reach a steady state where the

736
00:50:40,690 --> 00:50:43,840
as depicted by this validation the window

737
00:50:43,850 --> 00:50:47,940
collapses to a steady state won't collapse to zero it will collapse to the point

738
00:50:47,940 --> 00:50:52,500
where there's a balance between the

739
00:50:52,510 --> 00:50:59,800
influence of observations and the uncertainty as a result of repeated predictions

740
00:50:59,810 --> 00:51:02,310
OK so now the last thing i want to do

741
00:51:02,320 --> 00:51:05,950
in this section is just revisit the idea of

742
00:51:08,150 --> 00:51:10,540
learning the dynamical model was used in

743
00:51:11,510 --> 00:51:14,160
and so a reasonable

744
00:51:14,180 --> 00:51:17,490
let's think about this just in one dimension so i just think of point tracking

745
00:51:17,500 --> 00:51:22,500
point along the line for a moment and so x x event is just

746
00:51:23,200 --> 00:51:25,620
the real number

747
00:51:25,630 --> 00:51:28,070
and let's say that the

748
00:51:28,130 --> 00:51:31,810
process that we want to predict has this

749
00:51:31,810 --> 00:51:38,810
for me it's it's as the discrete form of simple harmonic oscillator second order equation

750
00:51:38,840 --> 00:51:43,480
that this wn is a series of independent random noise

751
00:51:44,120 --> 00:51:53,250
yes random random noise variance in there should be b here

752
00:51:53,260 --> 00:51:55,570
so that we are actually coupling this

753
00:51:55,600 --> 00:51:59,970
a series of random noise variables in via some game b which want to discover

754
00:51:59,980 --> 00:52:03,370
so now the the whole thing is to observe

755
00:52:06,090 --> 00:52:12,080
sample training emotion and

756
00:52:12,100 --> 00:52:13,680
from the training motion

757
00:52:13,700 --> 00:52:15,640
to learn

758
00:52:15,700 --> 00:52:20,290
the parameters a north b one and b which describe this kind of pendulum in

759
00:52:20,780 --> 00:52:24,520
in the breeze

760
00:52:24,520 --> 00:52:26,810
again this can all be done

761
00:52:26,830 --> 00:52:31,400
pretty much conventionally if we write down the maximum likelihood principle

762
00:52:31,970 --> 00:52:37,180
it actually boils down to minimizing the sum of the squares of the errors of

763
00:52:37,840 --> 00:52:40,400
of the predictions

764
00:52:40,480 --> 00:52:44,150
that you make over particular training data set

765
00:52:44,170 --> 00:52:48,150
so in other words given the internet plus one points in the training data i

766
00:52:48,150 --> 00:52:53,610
would be able to predict and plus two using this formula are neglecting from moment

767
00:52:53,620 --> 00:52:54,640
the random

768
00:52:54,660 --> 00:52:59,070
noise variable so now i just measure the square error in the prediction and sum

769
00:52:59,080 --> 00:53:02,990
them up and the maximum likelihood estimator turns out to be equivalent to doing this

770
00:53:02,990 --> 00:53:04,470
least squares computation

771
00:53:04,480 --> 00:53:09,530
and when you actually boil it all down and solve these equations it turns out

772
00:53:09,560 --> 00:53:14,260
you've got some simultaneous equations in we for a not in a one in which

773
00:53:14,260 --> 00:53:19,450
the coefficients of these autocorrelations where you take the entire training sequence and a copy

774
00:53:19,450 --> 00:53:23,760
of itself and you shift the copy and multiply them together and add up and

775
00:53:23,780 --> 00:53:29,930
so those are the discrete autocorrelations and these equations are very similar to what are

776
00:53:29,930 --> 00:53:35,010
called the yule walker equations which are absolutely standard in in a

777
00:53:35,100 --> 00:53:40,070
learning for a operas there's a slight slight variations here to do with

778
00:53:40,080 --> 00:53:44,530
dealing with the boundaries the start and the end of the training sequence problem

779
00:53:44,540 --> 00:53:48,240
and similarly you can go back and reconstruct the errors

780
00:53:48,250 --> 00:53:53,540
and use that to estimate the this coefficient should have been here multiplying the noise

781
00:53:53,560 --> 00:53:57,920
the whole thing works very similarly in multiple dimensions so in the state space that

782
00:53:57,920 --> 00:54:02,920
we have six dimensions or whatever it is for capital x we have a very

783
00:54:02,920 --> 00:54:05,600
similar setup just shown

784
00:54:05,620 --> 00:54:07,340
first order form of the

785
00:54:08,470 --> 00:54:12,760
in other words with the history of only one time step

786
00:54:12,760 --> 00:54:16,670
i could put more time steps in on the other hand with state space representations

787
00:54:16,670 --> 00:54:20,120
anyway this is how you

788
00:54:20,120 --> 00:54:26,980
and you win

789
00:54:27,210 --> 00:54:31,390
optimized for this position

790
00:54:31,480 --> 00:54:38,340
so i was actually hoping to quickly summarize what we did yesterday but it would

791
00:54:38,340 --> 00:54:40,450
mean that i have to walk back there

792
00:54:40,490 --> 00:54:45,260
which could be a bit of risk and we don't have any anyway some values

793
00:54:45,260 --> 00:54:46,610
this thing here

794
00:54:46,650 --> 00:54:54,160
so yesterday i was trying to do

795
00:54:54,190 --> 00:54:58,410
introduce the basic ideas of kernels and feature spaces

796
00:55:01,150 --> 00:55:08,090
it's not

797
00:55:08,110 --> 00:55:11,180
too much that you have to remember from this lecture

798
00:55:11,230 --> 00:55:13,060
four today but

799
00:55:13,070 --> 00:55:15,800
what you should remember is slow

800
00:55:15,820 --> 00:55:19,740
definitions of what is a positive definite kernel

801
00:55:20,050 --> 00:55:26,070
so a positive definite kernel is a symmetric function kx x x prime is the

802
00:55:26,070 --> 00:55:27,930
same scale

803
00:55:27,980 --> 00:55:31,540
x y and x with the property that

804
00:55:31,560 --> 00:55:38,100
whatever coefficients AI AJ used

805
00:55:38,110 --> 00:55:40,060
this quantity here

806
00:55:40,200 --> 00:55:45,730
and whatever point x i x j this quantity is nonnegative

807
00:55:47,050 --> 00:56:00,250
when you read this

808
00:56:00,290 --> 00:56:03,080
yes shoppers gets

809
00:56:11,520 --> 00:56:18,010
OK so and if we have a positive definite kernels if we have a function

810
00:56:18,080 --> 00:56:19,960
that satisfies

811
00:56:19,980 --> 00:56:22,090
this condition fall

812
00:56:22,160 --> 00:56:29,320
a i j x i x j then positive kernel and then we actually constructed

813
00:56:29,320 --> 00:56:33,580
a feature space such that this kernel computes the dot product so we construct

814
00:56:34,110 --> 00:56:38,680
constructing a mapping phi such that for all x x prime

815
00:56:38,690 --> 00:56:43,740
this equation here holds true

816
00:56:43,750 --> 00:56:47,580
and if you remember the mapping phi

817
00:56:47,590 --> 00:56:52,650
but this is the mapping phi takes point x maps it into the function k

818
00:56:52,650 --> 00:56:54,620
of dot com i x

819
00:56:54,670 --> 00:56:57,020
so this function

820
00:56:57,690 --> 00:57:01,600
again this is the function which maps the point x y

821
00:57:01,620 --> 00:57:03,190
in two

822
00:57:03,210 --> 00:57:06,070
k of ex-prime come x

823
00:57:06,080 --> 00:57:08,780
OK so this dot is just a shorthand notation for

824
00:57:09,340 --> 00:57:12,130
this function here

825
00:57:12,210 --> 00:57:16,630
and then so one way to think of it was that we have some points

826
00:57:16,790 --> 00:57:18,850
and they are mapped into

827
00:57:18,870 --> 00:57:20,980
i'm sitting on this point

828
00:57:21,000 --> 00:57:22,210
and then we we

829
00:57:22,340 --> 00:57:24,800
find some way of taking the dot product

830
00:57:24,810 --> 00:57:29,570
in this space of functions in the space of bumps and we define the dot

831
00:57:29,570 --> 00:57:33,680
product such that actually this equation here was true in

832
00:57:33,700 --> 00:57:38,940
also such that it is the product was positive definite strictly positive definite and and

833
00:57:38,940 --> 00:57:42,780
so on so now we know if we have such a kernel function

834
00:57:42,830 --> 00:57:45,940
it's actually the same as the dot product in another space

835
00:57:45,960 --> 00:57:51,280
in other words whenever we have and i agree that can be carried out in

836
00:57:51,280 --> 00:57:57,500
terms of dot products we can substitute kernel function wherever we had the product before

837
00:57:57,510 --> 00:58:01,720
provided that we only need to compute the product between points from the input domain

838
00:58:01,880 --> 00:58:05,120
mapped into that i dimensional space

839
00:58:05,140 --> 00:58:09,520
so we we substitute the kernel for the dot product in this way we implicitly

840
00:58:09,520 --> 00:58:14,590
carry out the algorithm in terms of the phi of x rather than the axis

841
00:58:14,610 --> 00:58:19,540
in that feature space can be very high dimensional in the case of the polynomial

842
00:58:19,540 --> 00:58:24,590
kernel that should do it was i mean in a small toy example with ten

843
00:58:24,590 --> 00:58:29,240
to the power of ten already or in a small example and follicles in kernel

844
00:58:29,270 --> 00:58:34,190
it's actually infinite dimensional and so it's a very high dimensional space where we normally

845
00:58:34,190 --> 00:58:38,410
wouldn't want to compute the products but using kernels we do it

846
00:58:41,770 --> 00:58:48,710
slightly more sophisticated algorithm that doesn't slightly more sophisticated than this house and windows-based classifier

847
00:58:48,710 --> 00:58:50,880
of yesterday

848
00:58:50,960 --> 00:58:55,860
so if you remember mister the this sense of class classifier

849
00:58:55,880 --> 00:59:00,200
we we had two classes we compute the mean of each class and then checked

850
00:59:00,200 --> 00:59:05,150
with the test points closer to the one may close to the other ones and

851
00:59:05,170 --> 00:59:07,790
once we formulate this in terms of kernels

852
00:59:07,930 --> 00:59:12,340
the decision function was basically an expansion in terms of kernels

853
00:59:12,350 --> 00:59:14,150
in some constants

854
00:59:14,240 --> 00:59:18,230
the expansion in terms of kernels was the following it was one expansion

855
00:59:18,240 --> 00:59:22,300
well over one part of it was an expansion to the positive points

856
00:59:22,330 --> 00:59:26,620
in one was expansion in terms of the negative points and the weights were constant

857
00:59:26,660 --> 00:59:32,050
within the two classes for all the positive points and weight one over m one

858
00:59:32,260 --> 00:59:36,380
all the negative points weight one over n two where n one and n two

859
00:59:36,380 --> 00:59:39,090
with the sizes of these two classes

860
00:59:39,110 --> 00:59:42,100
so the numbers of positive and negative points respectively

861
00:59:42,160 --> 00:59:43,350
so no

862
00:59:43,420 --> 00:59:45,570
we will construct another classifiers

863
00:59:45,780 --> 00:59:50,670
which will also induce a separating hyperplane in the feature space it may be a

864
00:59:50,670 --> 00:59:52,210
slightly smarter ones

865
00:59:52,310 --> 00:59:56,550
and then it will be a hyperplane that leads to the large margin of separation

866
00:59:56,550 --> 00:59:58,600
between the two classes

867
00:59:58,610 --> 01:00:02,310
so again the idea is map the points into this feature space we construct a

868
01:00:02,310 --> 01:00:04,880
separating hyperplane

869
01:00:08,330 --> 01:00:15,020
OK so you have missed much yet but

870
01:00:15,090 --> 01:00:20,050
but i had the picture i was thinking that you it

871
01:00:20,290 --> 01:00:23,610
so let's see what they have the same picture in mind

872
01:00:23,660 --> 01:00:25,560
when you me talking

873
01:00:25,570 --> 01:00:28,840
close your eyes and imagine it

874
01:00:35,640 --> 01:00:38,800
so here's are separating hyperplane

875
01:00:41,550 --> 01:00:43,670
so what's the separating hyperplane

876
01:00:43,730 --> 01:00:46,310
separating hyperplane is

877
01:00:46,320 --> 01:00:51,020
characterized by

878
01:00:51,030 --> 01:00:52,760
a normal vector w

879
01:00:52,790 --> 01:00:54,960
and an offset b

880
01:00:54,970 --> 01:01:01,920
so the geometric location of the hyperplane is characterized by this linear equation

881
01:01:03,570 --> 01:01:09,940
previous separating the truth the property that this function here takes positive values on the

882
01:01:09,940 --> 01:01:11,550
one side of the hyperplane

883
01:01:11,740 --> 01:01:17,420
negative values on the other ones

884
01:01:17,430 --> 01:01:23,290
let's call an optimal separating hyperplane a hyperplane with

885
01:01:23,340 --> 01:01:25,670
because of these change the set

886
01:01:25,720 --> 01:01:31,670
i played with the property that the distance to the closest point is maximized

887
01:01:31,690 --> 01:01:33,540
subject to the constraint have

888
01:01:33,550 --> 01:01:35,340
which i mean that

889
01:01:35,360 --> 01:01:39,200
all the positive points and one class of the negative ones i the other so

890
01:01:40,230 --> 01:01:45,580
class should be separated correctly but in addition we went the distance of the closest

891
01:01:45,580 --> 01:01:48,110
point to the hyperplane to be maximized

892
01:01:48,130 --> 01:01:54,500
one way to find the optimal hyperplane is to compute the convex hull

893
01:01:54,540 --> 01:01:57,080
of the two datasets

894
01:01:57,130 --> 01:02:00,970
the set of points can be computed as convex combinations

895
01:02:01,250 --> 01:02:04,640
if you want to think of it the set of all points that sort of

896
01:02:04,640 --> 01:02:06,350
like in between

897
01:02:07,630 --> 01:02:11,540
roughly speaking

898
01:02:11,560 --> 01:02:14,030
so we can compute the convex hull

899
01:02:14,310 --> 01:02:16,520
to find the shortest connection

900
01:02:16,530 --> 01:02:21,680
between these two convex hulls then this direction of the connection gives us the direction

901
01:02:21,680 --> 01:02:25,570
now well treated from the outside

902
01:02:25,570 --> 01:02:29,290
but essentially the process happens on inside

903
01:02:29,320 --> 01:02:34,360
the dynamics of change happens on the inside

904
01:02:36,570 --> 01:02:38,270
you get for example

905
01:02:38,330 --> 01:02:40,600
gases which are more stable

906
01:02:41,210 --> 01:02:45,280
highly disorganized states

907
01:02:45,290 --> 01:02:47,700
and it takes a huge push

908
01:02:47,720 --> 01:02:50,690
and this is work done by one three it takes a huge push from the

909
01:02:50,690 --> 01:02:53,230
outside to get them to organize

910
01:02:53,310 --> 01:02:54,810
into some sort of pattern

911
01:02:56,120 --> 01:02:58,620
this is a highly unstable structure

912
01:02:58,640 --> 01:02:59,850
this happens

913
01:02:59,860 --> 01:03:03,250
and more easy for the system the system we can

914
01:03:03,600 --> 01:03:07,570
disorganised state

915
01:03:07,620 --> 01:03:13,700
this is something of interest for us virtual organizations

916
01:03:13,820 --> 01:03:19,250
and i will maintain that possibly when you're talking about faster and faster

917
01:03:19,310 --> 01:03:24,940
first building trust and transient very very transient organisation dealing with a system is much

918
01:03:24,940 --> 01:03:25,920
like a

919
01:03:25,980 --> 01:03:30,710
it is more stable and we seem to have decided disorganised state in other words

920
01:03:30,750 --> 01:03:31,840
in one sense

921
01:03:31,930 --> 01:03:33,450
most disconnected

922
01:03:35,990 --> 01:03:38,340
the potential has the potential to

923
01:03:38,340 --> 01:03:42,720
organised into particular form given the perturbation from the outside

924
01:03:42,950 --> 01:03:44,540
so for example

925
01:03:45,590 --> 01:03:50,180
there is the opportunity to do theoretical research

926
01:03:52,610 --> 01:03:55,580
general disconnected from each other

927
01:03:55,710 --> 01:03:57,190
that's for the inputs

928
01:03:57,350 --> 01:04:00,910
and so organised it has

929
01:04:01,030 --> 01:04:02,770
four for two days

930
01:04:02,840 --> 01:04:05,660
as processes that actually merged results

931
01:04:05,790 --> 01:04:13,690
and it is most likely most in this state organised in organised

932
01:04:13,730 --> 01:04:16,850
that's an example

933
01:04:20,830 --> 01:04:26,140
now called the architecture of course in the internal possibly in coastal states and their

934
01:04:26,140 --> 01:04:30,140
interconnections that's what we mean by organizations organisation

935
01:04:30,160 --> 01:04:33,250
the causal architecture of the process

936
01:04:33,290 --> 01:04:37,870
internal repository causal states and their interconnections

937
01:04:39,960 --> 01:04:45,390
to figure out what the cause of architecture the complicated so i'm not sure we

938
01:04:45,390 --> 01:04:47,940
didn't do that for the social sciences

939
01:04:47,960 --> 01:04:50,330
this into these kinds of systems and that's

940
01:04:50,350 --> 01:04:54,000
where we we really need machine and

941
01:05:00,750 --> 01:05:04,480
the organised system to explore exploit the two

942
01:05:04,480 --> 01:05:09,020
lawrence like well it's quite easy to maintain the constraints

943
01:05:09,020 --> 01:05:12,140
and you're run around and you can see here

944
01:05:12,640 --> 01:05:13,710
and this is

945
01:05:13,810 --> 01:05:16,290
run around inside constrained space

946
01:05:16,310 --> 01:05:22,140
and does all the pattern creation knowledge production and digital

947
01:05:22,190 --> 01:05:30,500
i exploit explore system it actually relax the constraints of its system and it explores

948
01:05:30,500 --> 01:05:34,020
the of this constraint so actually

949
01:05:34,040 --> 01:05:37,660
the notion interplay is more important here

950
01:05:37,690 --> 01:05:39,540
and it able to do right

951
01:05:39,600 --> 01:05:43,910
generally be able to do more computational power

952
01:05:51,140 --> 01:05:57,250
national coordination is really important is that it's constantly rewiring model always

953
01:05:57,270 --> 01:05:58,580
and stable

954
01:05:58,600 --> 01:06:02,520
relationships of this that can be used

955
01:06:02,540 --> 01:06:06,270
functional relationship where you have time to work to do in the lab

956
01:06:06,440 --> 01:06:07,430
the system

957
01:06:08,810 --> 01:06:12,580
tasks functions are we want to solve the coordination

958
01:06:12,600 --> 01:06:16,500
three wise it's

959
01:06:17,040 --> 01:06:21,770
multiple equilibria may have no regular behaviour and they require no for error in order

960
01:06:21,810 --> 01:06:24,140
to generate more

961
01:06:24,230 --> 01:06:27,060
so this is one of the things that his

962
01:06:28,540 --> 01:06:32,730
this is a lot of us is actually does produced

963
01:06:32,770 --> 01:06:36,580
it does make mistakes and errors in one of the things that we're talking about

964
01:06:36,580 --> 01:06:40,560
but this is what they don't tell us anything really OK because even importance sampling

965
01:06:40,560 --> 01:06:43,560
satisfied such that tries OK

966
01:06:43,580 --> 01:06:47,600
so he didn't tell us specifically on many particles you would need to us so

967
01:06:47,600 --> 01:06:52,750
as to approximate the joint the joint essentially target distribution properly so this is kind

968
01:06:52,750 --> 01:06:56,290
of result which actually not really informative you just tell you if we have to

969
01:06:56,290 --> 01:07:00,560
be able to use in a particle basically you might be able to you can

970
01:07:00,560 --> 01:07:03,750
do with commercial trucks so that's not quite

971
01:07:06,440 --> 01:07:09,020
what kind of positive result can we obtain

972
01:07:09,040 --> 01:07:11,730
well and so i'm actually

973
01:07:11,810 --> 01:07:14,190
some kind of mixing assumptions

974
01:07:14,210 --> 01:07:16,440
and that the

975
01:07:16,460 --> 01:07:21,640
so mixing assumption on set in the context of state space model some mixing assumption

976
01:07:21,640 --> 01:07:23,750
on essentially the tall

977
01:07:23,770 --> 01:07:28,000
o two more food you're trying to approximate the then what you can obtain you

978
01:07:28,000 --> 01:07:31,920
can obtain results that are uniform in time convulsed with that

979
01:07:32,350 --> 01:07:33,480
you that

980
01:07:33,540 --> 01:07:38,830
if you look just at the coral marginal distribution

981
01:07:38,850 --> 01:07:43,400
robin tries to you look at pioneer instead of looking at the approximation by electronics

982
01:07:43,400 --> 01:07:45,980
works and you look at its marginal distribution

983
01:07:46,000 --> 01:07:49,500
so in the context of state space model p of x and y what went

984
01:07:49,500 --> 01:07:55,000
in order to compare that to small to count approximation is particle approximation then you

985
01:07:55,000 --> 01:07:56,230
can show

986
01:07:56,250 --> 01:07:59,080
the some basic you're you i just mentioned

987
01:07:59,080 --> 01:08:01,670
that essentially the l

988
01:08:01,690 --> 01:08:06,060
you obtain approximation you your day doesn't increase over time

989
01:08:07,810 --> 01:08:09,580
this is essentially

990
01:08:09,600 --> 01:08:16,000
some very positive about particle method you might not be indeed able to approximate properly

991
01:08:16,000 --> 01:08:19,640
the joint distribution you cannot expect to be able to do that

992
01:08:19,650 --> 01:08:25,150
but under regularity assumptions if you're only basically look at the approximation of the marginal

993
01:08:25,150 --> 01:08:31,310
distribution then basically you what you want to calorimeter on very good properties you don't

994
01:08:31,310 --> 01:08:35,250
have any accumulation of urls over the time you have like a uniform entire convergence

995
01:08:35,250 --> 01:08:41,520
balls so this is why essentially all discovery techniques became popular because such was

996
01:08:41,520 --> 01:08:42,770
if you know

997
01:08:44,310 --> 01:08:50,400
and essentially the violence the relative alliance of normalizing constant simulate

998
01:08:50,420 --> 01:08:52,830
then we have seen actually

999
01:08:52,850 --> 01:08:58,880
that if you were dealing with sequential importance sampling on sampling typically you have

1000
01:08:58,900 --> 01:09:04,440
so the bones via that increases exponentially with time index when i'm telling you

1001
01:09:04,930 --> 01:09:11,460
when you do resampling under similar you right assumptions then basically the volatility violence essentially

1002
01:09:11,460 --> 01:09:15,900
only increases linearly with the time index this is much better we've gone from exponentially

1003
01:09:15,900 --> 01:09:18,350
but really not about

1004
01:09:18,370 --> 01:09:23,980
so you that assumption

1005
01:09:24,000 --> 01:09:26,310
so such that essentially

1006
01:09:26,330 --> 01:09:29,120
if you initialise c

1007
01:09:29,140 --> 01:09:32,690
i said look at the filter so these guys

1008
01:09:35,710 --> 01:09:40,230
so i'm going to specify what's going on

1009
01:09:40,310 --> 01:09:46,460
so assume you have the state space model OK

1010
01:09:47,080 --> 01:09:52,230
says you look at what's happened when the initial state is distributed according x not

1011
01:09:52,380 --> 01:09:54,040
be continue new

1012
01:09:54,060 --> 01:09:54,810
all o

1013
01:09:54,830 --> 01:09:57,540
is distributed according to of emotional new

1014
01:09:57,560 --> 01:10:00,710
what you all to one that basically

1015
01:10:00,730 --> 01:10:04,880
as j increases the different ways between these two

1016
01:10:04,900 --> 01:10:11,120
distribution into polarisation in the second are geometrically fast time indexed so some are you

1017
01:10:11,140 --> 01:10:13,620
all the assumptions as you

1018
01:10:13,670 --> 01:10:19,020
what happened in the past is actually going to be forgive forgot exponentially fast

1019
01:10:19,040 --> 01:10:23,850
so that's that's what he tells you get

1020
01:10:23,960 --> 01:10:27,850
you also have summaries that's essentially he's

1021
01:10:27,870 --> 01:10:29,670
you try to obtain

1022
01:10:29,670 --> 01:10:32,850
so OK we don't have a good approximation

1023
01:10:32,870 --> 01:10:37,920
of the joint distribution and i told you k is not very good now

1024
01:10:37,940 --> 01:10:41,730
let's say that two approximation of the joint distribution

1025
01:10:41,750 --> 01:10:47,020
o two sample from it's OK to look at the distribution of the past then

1026
01:10:47,020 --> 01:10:48,400
one can show

1027
01:10:48,420 --> 01:10:52,350
this is the result by the model was the result is that under limiting assumption

1028
01:10:52,460 --> 01:10:57,770
essentially the distribution of the positive sample only differ from the true

1029
01:10:57,830 --> 01:11:02,790
essentially distribution by a ball that increases only knowledge of the time in that what

1030
01:11:02,790 --> 01:11:07,270
is one small this is called it is much better than when you were doing

1031
01:11:07,270 --> 01:11:14,480
basically a single tree pollen something where the ball typically increasing expand exponentially fast really

1032
01:11:14,480 --> 01:11:18,420
positive results about particle methods

1033
01:11:18,440 --> 01:11:23,640
OK so like for example that i'm going to back to the example

1034
01:11:23,690 --> 01:11:26,310
we are describing

1035
01:11:26,330 --> 01:11:29,540
at the very beginning for importance sampling

1036
01:11:29,560 --> 01:11:32,230
so let's consider case for example when

1037
01:11:32,290 --> 01:11:35,980
the target distribution factorizes OK

1038
01:11:36,000 --> 01:11:40,850
the UN policy distribution so you have the target distribution is the product of unit

1039
01:11:41,350 --> 01:11:46,350
a unit normal distribution and impartial distribution is the product of the normal distribution of

1040
01:11:46,350 --> 01:11:48,310
violence in the square

1041
01:11:48,330 --> 01:11:51,460
if you look at the related violence

1042
01:11:53,520 --> 01:11:58,420
the importance sampling estimator it blows up exponentially fast the time index

1043
01:11:58,420 --> 01:12:00,710
if you look at the center to violence

1044
01:12:00,730 --> 01:12:04,690
in particles basically the

1045
01:12:04,750 --> 01:12:09,310
smc estimate sequential monte carlo estimate what you see that basically the smaller

1046
01:12:09,330 --> 01:12:14,880
time index which was basically the exponent is just inform it's linear increase so we

1047
01:12:14,880 --> 01:12:17,380
moved from essentially

1048
01:12:19,170 --> 01:12:20,960
two linear so

1049
01:12:20,980 --> 01:12:26,330
whereas two other related violence of ten minus two full sequential importance sampling you would

1050
01:12:26,330 --> 01:12:30,960
these are met with very little empirical support

1051
01:12:31,060 --> 01:12:36,960
and the claim that psychoanalysis proves itself by being by its tremendous success in curing

1052
01:12:36,960 --> 01:12:40,540
mental illness is also almost certainly not true

1053
01:12:40,590 --> 01:12:45,790
for most maybe not all the most psychological disorders there are quicker and more reliable

1054
01:12:45,790 --> 01:12:51,170
treatments than psychoanalysis and there is considerable controversy as to whether

1055
01:12:51,270 --> 01:12:56,210
the tony soprano method inside where you get this insight and this discovery all now

1056
01:12:56,210 --> 01:12:59,880
i know makes any real difference

1057
01:12:59,960 --> 01:13:05,460
in alleviating symptoms such as anxiety disorders or depression

1058
01:13:05,880 --> 01:13:09,380
this is why

1059
01:13:09,400 --> 01:13:11,790
the sort of sort of sticker shock

1060
01:13:11,810 --> 01:13:15,790
when people go to university psychology department what say look

1061
01:13:15,840 --> 01:13:20,290
hey where's so an insect how they take classes on freud

1062
01:13:20,340 --> 01:13:22,310
your expert on freud

1063
01:13:22,360 --> 01:13:28,790
and the true this freudian psychoanalysis is almost never study inside psychology departments

1064
01:13:29,480 --> 01:13:30,590
it's not at

1065
01:13:30,610 --> 01:13:35,210
the cognitive or developmental side not the clinical side there are some exceptions

1066
01:13:35,270 --> 01:13:40,230
but for the most part even the people who do study fried within psychology departments

1067
01:13:40,250 --> 01:13:41,540
do so critically

1068
01:13:41,560 --> 01:13:44,980
not very few of them would see themselves as psycho

1069
01:13:45,020 --> 01:13:51,210
and practitioner or as the freudian psychologists freud lives on

1070
01:13:51,230 --> 01:13:56,060
both in the clinical setting in in university but freud at yale for instance is

1071
01:13:56,060 --> 01:13:59,940
much more likely to be found in the history department of literature department then in

1072
01:13:59,960 --> 01:14:03,790
psychology department and this is typical enough

1073
01:14:07,090 --> 01:14:11,630
despite all of this sort of sour things i just said about

1074
01:14:11,650 --> 01:14:14,690
the big idea then important

1075
01:14:14,710 --> 01:14:18,190
of the dynamic unconscious remains intact

1076
01:14:18,210 --> 01:14:23,630
we'll go over and over and over again different case studies where some really interesting

1077
01:14:23,630 --> 01:14:25,340
aspects of mental like

1078
01:14:27,630 --> 01:14:29,610
to be unconscious

1079
01:14:30,360 --> 01:14:33,360
there's one question i'm not going to skip over this

1080
01:14:33,420 --> 01:14:35,670
for reasons of time

1081
01:14:36,380 --> 01:14:37,770
and just go

1082
01:14:37,790 --> 01:14:39,710
to some examples

1083
01:14:39,770 --> 01:14:43,070
of the unconscious in modern psychology

1084
01:14:44,460 --> 01:14:46,170
here's a simple example

1085
01:14:46,190 --> 01:14:47,460
of the unconscious

1086
01:14:47,460 --> 01:14:49,000
in modern psychology

1087
01:14:53,330 --> 01:14:57,380
so new year sentence like

1088
01:14:57,400 --> 01:14:59,840
john thinks the bill likes him

1089
01:14:59,880 --> 01:15:02,040
in a fraction of a second

1090
01:15:02,040 --> 01:15:03,590
you realize

1091
01:15:03,610 --> 01:15:06,290
this means that john things to build

1092
01:15:06,310 --> 01:15:07,710
likes johnny

1093
01:15:07,730 --> 01:15:10,060
if you are descendants groups

1094
01:15:10,090 --> 01:15:12,310
i don't think the bill like himself

1095
01:15:12,360 --> 01:15:14,230
in a fraction of a second

1096
01:15:14,250 --> 01:15:15,270
you would think

1097
01:15:15,290 --> 01:15:17,920
that means john thanks to bill expert

1098
01:15:17,940 --> 01:15:21,270
and as we will get to when we get the lecture language this is not

1099
01:15:21,290 --> 01:15:24,150
conscious you don't know how you do this you don't even know what you are

1100
01:15:24,150 --> 01:15:25,020
doing this

1101
01:15:25,040 --> 01:15:26,500
we do it quickly

1102
01:15:26,570 --> 01:15:28,000
and instinctively

1103
01:15:28,040 --> 01:15:30,480
so much of our day-to-day life

1104
01:15:30,500 --> 01:15:36,360
can be done unconsciously they're different activities you can do

1105
01:15:36,420 --> 01:15:42,070
driving chewing gum shoelace tying where if you're good enough at the expert enough fact

1106
01:15:42,070 --> 01:15:44,900
that you know you're doing

1107
01:15:44,980 --> 01:15:47,650
i was at a party a few years ago a friend of mine and we

1108
01:15:47,650 --> 01:15:51,960
ran out of food so is it just go pick up some food

1109
01:15:52,020 --> 01:15:54,150
an hour later he was gonna

1110
01:15:54,170 --> 01:15:59,540
still gonna be around the corner and we call them up on the cell phone

1111
01:15:59,540 --> 01:16:00,960
and he said oh

1112
01:16:00,980 --> 01:16:04,920
i got on the highway approach to work

1113
01:16:05,860 --> 01:16:08,380
he works like an hour when would

1114
01:16:08,400 --> 01:16:11,810
and and the some versions of these things happened

1115
01:16:11,810 --> 01:16:18,360
all the time

1116
01:16:18,380 --> 01:16:20,980
maybe more surprising

1117
01:16:20,980 --> 01:16:25,460
freud's inside that our likes and dislikes

1118
01:16:25,500 --> 01:16:28,810
are due to factors that

1119
01:16:28,810 --> 01:16:34,070
we're not necessarily conscious has a lot of empirical support a lot of empirical support

1120
01:16:34,130 --> 01:16:37,830
from research into social psychology for example

1121
01:16:37,840 --> 01:16:41,270
so here's here's one finding in social psychology

1122
01:16:43,310 --> 01:16:46,060
if somebody goes through a terrible

1123
01:16:47,980 --> 01:16:50,000
to get into a club

1124
01:16:50,020 --> 01:16:53,210
though like the club more

1125
01:16:53,210 --> 01:16:58,150
you might think they like unless the people do terrible things but actually hazing is

1126
01:16:59,230 --> 01:17:01,980
but are remarkably successful tool

1127
01:17:01,980 --> 01:17:07,140
and you see that in this subject specific also that already very high correlation between

1128
01:17:07,770 --> 01:17:13,870
alpha power plants and vote for heart rates the subject the

1129
01:17:15,620 --> 01:17:20,760
the correlation is even higher so that the hard-headed variations are traditionally generally are highly

1130
01:17:20,760 --> 01:17:26,400
correlated to bolt so that means that if you do and experiments where you also

1131
01:17:26,400 --> 01:17:34,540
modulate the the heartbeat for instance by presenting emotional saliva Protestant libraries has an influence

1132
01:17:34,540 --> 01:17:39,270
on the heart rituals against responses well from all the parts of the brain and

1133
01:17:39,270 --> 01:17:49,160
principle of these subject now that if you but what about the bits tool to

1134
01:17:49,160 --> 01:17:57,760
quantify the amount of of correlation defined over different subjects and different probabilities between heart

1135
01:17:57,760 --> 01:18:00,960
rate and Adolfo balance

1136
01:18:02,460 --> 01:18:08,140
but rather that see that for some subjects almost the whole brain surgery

1137
01:18:08,200 --> 01:18:15,520
90 % of fraction of the brain is correlated to the alpha band and in

1138
01:18:15,520 --> 01:18:20,980
some subjects the subjects there is hardly any coalition structure is very very large inter-subject

1139
01:18:20,980 --> 01:18:26,750
variations toward this kind of correlations

1140
01:18:32,300 --> 01:18:38,360
what's there would would now like to present some results on the data-driven analysis and

1141
01:18:38,360 --> 01:18:41,760
there are based on the fact that the that the model different approach that the

1142
01:18:41,800 --> 01:18:47,920
artificial some limitations on is that we have applied the regression model each voxel separately

1143
01:18:47,920 --> 01:18:50,340
instead of on the whole brain

1144
01:18:51,600 --> 01:18:57,940
I only have evidence all frequencies and the cost to get to clean up the

1145
01:18:58,890 --> 01:19:05,310
variations and we have not used any of you have not made use of the

1146
01:19:05,310 --> 01:19:07,680
spatial distribution of the alpha frequencies

1147
01:19:09,320 --> 01:19:12,200
now as far as the 1st question is who

1148
01:19:13,420 --> 01:19:21,440
concerned we we looked at the try to find out whether variations we find in

1149
01:19:21,440 --> 01:19:25,820
different subject that have to do something with the algorithms or whether it has something

1150
01:19:25,820 --> 01:19:31,860
to do with the original signals and for that we look at the the the

1151
01:19:31,860 --> 01:19:36,340
the the clustering on the on the role of bold signal so without applying any

1152
01:19:36,340 --> 01:19:42,680
model if we knew we removed all the nuisance effects of emotion affects all the

1153
01:19:42,680 --> 01:19:51,160
trends and all the heart effects and GC these projects are also applied to mobile

1154
01:19:51,230 --> 01:19:54,160
the bolt signals and at the defined distance measure

1155
01:19:54,940 --> 01:19:59,920
and this is the distance measure was used for clustering analysis and then we compare

1156
01:19:59,920 --> 01:20:06,800
compared different subjects have see if we if we apply this this data driven clustering

1157
01:20:06,800 --> 01:20:12,900
approach to durable signals and we see of if we if go to about

1158
01:20:14,000 --> 01:20:21,140
8 clusters and we see among subjects that you have a patrol and appraisal of

1159
01:20:21,140 --> 01:20:28,890
cluster foreseeable points also belongs to a cluster until seceded said that although there is

1160
01:20:28,910 --> 01:20:34,540
not the information in the clustering of the algorithm regarding the connectedness of these components

1161
01:20:34,540 --> 01:20:43,310
you used to underpin mostly connected components and if you now correlated these components to

1162
01:20:43,310 --> 01:20:48,580
the alphabet that all these components they appear to to quality the operator so in

1163
01:20:48,580 --> 01:20:51,580
this sense if you 1st clustering and

1164
01:20:51,810 --> 01:20:58,870
computer every topic clusters that the amount of correlation it can be demonstrated in all

1165
01:20:58,870 --> 01:21:04,640
the subjects and if you look at the sort of clusters and they seem to

1166
01:21:04,640 --> 01:21:10,250
be related to the to the default network so here's the spot and this part

1167
01:21:10,250 --> 01:21:15,480
there are connected to at least produced assembled signals

1168
01:21:15,840 --> 01:21:19,200
as well

1169
01:21:20,020 --> 01:21:26,780
what they have done knowledge that I have not had only used acceptable parts of

1170
01:21:26,780 --> 01:21:31,520
the of the alpha band variation if I look at the complete pattern of the

1171
01:21:31,520 --> 01:21:36,340
alphabet uh and he as carriers of 10 born to 16 hertz

1172
01:21:37,180 --> 01:21:41,320
about this subject and I see that the that the the lot of patterns they're

1173
01:21:41,320 --> 01:21:48,310
they're very in time sometimes it's nothing is present sometimes volunteer Doppler happen although this

1174
01:21:48,330 --> 01:21:55,160
is justice power of a power sometimes as he directly due to the left and

1175
01:21:55,160 --> 01:22:00,320
sometimes it's more to right there's a lot of things going on and is 600

1176
01:22:00,320 --> 01:22:05,340
pictures and is only for 1 of the 1 part of the of the alphabet

1177
01:22:05,340 --> 01:22:12,140
and you should look at the frequency 1 security Tyrol origin completely different purpose and

1178
01:22:12,140 --> 01:22:18,400
all these deceased graphs to police the state of the brain is related to the

1179
01:22:18,400 --> 01:22:25,900
office of them and they could be correlated the bold enough to do that we

1180
01:22:26,320 --> 01:22:33,460
look at these kind of patterns and now clustering and

1181
01:22:33,480 --> 01:22:36,770
algorithm was applied to these different

1182
01:22:37,230 --> 01:22:43,860
patterns of alpha rhythm and uh moments maps of the EEG and then at the

1183
01:22:43,860 --> 01:22:47,400
theater that's probably get 1 group without any

1184
01:22:48,530 --> 01:22:53,020
this was 1 group this was a group and these groups were the same kind

1185
01:22:53,020 --> 01:22:59,450
of distributed here again not the average of all these groups and we can all

1186
01:22:59,450 --> 01:23:06,620
subjects to find this kind of patents so there was 1 nonspecific cluster there was

1187
01:23:06,620 --> 01:23:07,460
no overlap model

1188
01:23:08,460 --> 01:23:15,700
there was 1 cluster that was pretty variety of was dominating activity lower frequency

1189
01:23:16,920 --> 01:23:22,400
there was 1 cluster where it was dominated at the higher frequencies and it was

1190
01:23:22,400 --> 01:23:30,000
1 clustering gospel the middle of the frequency so once specific and low and high

1191
01:23:30,000 --> 01:23:33,930
called for in terms of estimable quantities which might or might not be useful for

1192
01:23:33,930 --> 01:23:35,720
your purposes

1193
01:23:35,750 --> 01:23:40,430
that's quite a the complex bit of convex analysis that goes behind the bounds quite

1194
01:23:40,430 --> 01:23:44,390
need the important things you can get them out exactly typically

1195
01:23:44,620 --> 01:23:48,080
but you can do in general is this

1196
01:23:48,080 --> 01:23:52,560
i have now eliminated the error from x to y and the hypotheses

1197
01:23:52,600 --> 01:23:56,770
what does it mean if there's anything that picture the error vector y is missing

1198
01:23:56,830 --> 01:24:01,140
well one thing it makes y independent of f of x

1199
01:24:01,180 --> 01:24:03,520
that's what is represented in the new picture

1200
01:24:03,540 --> 01:24:07,810
when y is independent of x means what i do two x isn't going to

1201
01:24:07,810 --> 01:24:13,430
change what the distribution of y so basically that so there's no causal effect

1202
01:24:13,500 --> 01:24:17,870
so the hypothesis of no causal effect which is clearly very interesting hypotheses

1203
01:24:17,890 --> 01:24:22,790
it's kind of represented by this picture as a kind of because although this is

1204
01:24:22,790 --> 01:24:25,930
a sufficient condition for that it's not necessary

1205
01:24:25,950 --> 01:24:29,410
so there's a bit of fuzziness going on here but never mind about that and

1206
01:24:29,410 --> 01:24:32,490
also if i can eliminate that image

1207
01:24:32,540 --> 01:24:37,970
another property holds which is that in the into the observation regime

1208
01:24:38,020 --> 01:24:44,330
on any regime but only in the observation raising wise independent of z

1209
01:24:44,350 --> 01:24:48,350
and that's something i can tell

1210
01:24:48,450 --> 01:24:51,850
so if i test it i find y is not independent of z

1211
01:24:51,870 --> 01:24:55,680
and i believe my original picture on the previous page

1212
01:24:55,700 --> 01:24:59,850
and i find y is not independent of z observationally then i can believe that

1213
01:24:59,850 --> 01:25:02,490
and that our companies

1214
01:25:02,500 --> 01:25:06,490
that means that there is a causal effect so i can do hypothesis

1215
01:25:06,660 --> 01:25:10,200
and it just uses the instrument and the response

1216
01:25:10,200 --> 01:25:13,870
so great things we can do and although none of this theory is really new

1217
01:25:14,220 --> 01:25:19,680
i think just the use of the DAG models to represent the story and this

1218
01:25:20,080 --> 01:25:22,910
is very useful for number reason first of all

1219
01:25:22,930 --> 01:25:30,000
it's absolutely explicit what being assumed put those intervention indicators and then use the tag

1220
01:25:31,160 --> 01:25:36,950
so the bad apply very so you're telling that story translated words really and see

1221
01:25:36,970 --> 01:25:40,930
what it says about what you're assuming about what is reasonable

1222
01:25:41,020 --> 01:25:46,520
the say does not depend on the regime which marginal and conditional distributions which changed

1223
01:25:46,520 --> 01:25:51,230
so you tell the story very explicitly

1224
01:25:51,250 --> 01:25:54,990
put that in context think about your problem is that of valid story doesn't make

1225
01:25:54,990 --> 01:25:59,330
sense could you convince the down so that it was true

1226
01:25:59,370 --> 01:26:03,790
what do we can you the diagram one so the diagram you know one of

1227
01:26:03,790 --> 01:26:07,640
the things with you can analyse it can also some conclusions work formulae

1228
01:26:07,660 --> 01:26:12,350
four for understanding how to compute effects and things like that

1229
01:26:12,370 --> 01:26:14,080
i have a very good example

1230
01:26:14,100 --> 01:26:18,180
instrumental variables mendelian randomisation

1231
01:26:18,220 --> 01:26:24,540
so the question does lot of observations studies suggesting that low serum cholesterol increases the

1232
01:26:24,540 --> 01:26:26,490
risk of all kinds of cancers

1233
01:26:30,350 --> 01:26:35,580
you can actually easily go in and fix somebody serum cholesterol

1234
01:26:35,620 --> 01:26:44,490
but his is very very clever idea because there is a particular gene APOE gene

1235
01:26:44,500 --> 01:26:46,220
which actually an instrument

1236
01:26:47,080 --> 01:26:51,580
it turns out the people who are born with a particular value this gene

1237
01:26:51,640 --> 01:26:56,310
naturally have very low serum cholesterol

1238
01:26:57,630 --> 01:27:05,080
what these confounders here things like your diet smoking sort of common causes of who

1239
01:27:05,080 --> 01:27:09,830
knows it might affect both serum cholesterol and cancer there might be a hidden chamber

1240
01:27:09,830 --> 01:27:14,040
which actually having observed have been able to observe the that might and might be

1241
01:27:14,040 --> 01:27:18,620
reverse causation the tumour might be lower your cholesterol rather than the other way around

1242
01:27:18,660 --> 01:27:21,950
so all sorts of things could be problems and if we don't observe w i

1243
01:27:21,970 --> 01:27:23,350
don't know what's going on

1244
01:27:23,370 --> 01:27:28,930
but z is it reasonable to think that the conditional independencies whole well z is

1245
01:27:28,930 --> 01:27:31,660
your ali elements is assigned birth

1246
01:27:31,680 --> 01:27:32,990
and it's pretty

1247
01:27:32,990 --> 01:27:33,890
easy to

1248
01:27:33,930 --> 01:27:36,930
convince yourself that is independent of whether or not you later going to have a

1249
01:27:36,930 --> 01:27:38,230
hidden gem

1250
01:27:38,250 --> 01:27:43,970
so it's independent and is in typically it's hard to argue bit harder that is

1251
01:27:43,970 --> 01:27:48,970
independent of diet and smoking habits of things because part of family environment and things

1252
01:27:48,970 --> 01:27:53,390
like that but but but you could argue that the conditions hold quite recently so

1253
01:27:53,410 --> 01:27:58,160
you can use it as an instrument it's common dearly randomisation is because men those

1254
01:27:58,200 --> 01:28:02,830
laws for the allocation of this gene tierney what's going on

1255
01:28:02,830 --> 01:28:08,870
it does affect low serum serum cholesterol and so for example you could check to

1256
01:28:10,350 --> 01:28:18,180
the incidence of cancer varies with the value gene you have if it does

1257
01:28:18,180 --> 01:28:20,750
that suggests there is a causal link

1258
01:28:20,790 --> 01:28:22,410
it doesn't

1259
01:28:22,430 --> 01:28:24,970
and it's not clear but maybe there's

1260
01:28:25,020 --> 01:28:26,580
measuring it is very hard

1261
01:28:26,580 --> 01:28:32,890
but you can actually do hypothesis test set is there or not

1262
01:28:32,910 --> 01:28:36,810
and this is great because we have a story to tell in context

1263
01:28:36,890 --> 01:28:42,180
we know the story we understand the genetics and is because we understand the genetics

1264
01:28:42,180 --> 01:28:45,790
and we can make plausible arguments to justify that particular picture

1265
01:28:46,250 --> 01:28:50,390
that we can use so we can't just say oh here is picture believe me

1266
01:28:50,390 --> 01:28:54,890
that works i can say here's the page and this is why it works

1267
01:28:55,020 --> 01:28:58,870
and now i know that people to go to work for me great can manipulate

1268
01:28:58,870 --> 01:29:03,770
it i can do all kinds of statistical analysis and get out some causal conclusions

1269
01:29:03,810 --> 01:29:09,870
because i justified the picture which involves both stochastic variables and most importantly

1270
01:29:09,890 --> 01:29:12,230
the intervention variables question

1271
01:29:12,250 --> 01:29:14,660
sorry which

1272
01:29:15,950 --> 01:29:18,430
that's problem

1273
01:29:18,430 --> 01:29:20,500
vision or

1274
01:29:23,370 --> 01:29:25,160
in addition

1275
01:29:29,500 --> 01:29:35,870
OK well that is if i have these interventions will be hypothetical because i but

1276
01:29:35,870 --> 01:29:40,640
i'm interested in thinking about hypothetical intervention where would go in and try and set

1277
01:29:40,640 --> 01:29:45,200
some somebody's cholesterol see whether i would if i could raise your cholesterol or by

1278
01:29:45,200 --> 01:29:48,290
various means fitting wakes perhaps

1279
01:29:48,500 --> 01:29:52,060
then maybe i could increase you

1280
01:29:52,080 --> 01:29:57,040
decrease the chance of cancer so i i want to know what would happen in

1281
01:29:57,040 --> 01:30:01,220
such hypothetical interventions but the gene the gene is already there

1282
01:30:01,230 --> 01:30:03,330
so there's no way i can so it's

1283
01:30:03,390 --> 01:30:08,700
is completely independent of what i choose to do it here

1284
01:30:08,720 --> 01:30:12,930
the three d

1285
01:30:20,060 --> 01:30:21,350
what you

1286
01:30:24,470 --> 01:30:26,250
absolutely is the here

1287
01:30:26,250 --> 01:30:30,150
so the question was illegal about just replaces it with it because they want is

1288
01:30:30,150 --> 01:30:33,620
sometimes it is it is independent of w

1289
01:30:33,700 --> 01:30:37,160
that's not true of x

1290
01:30:37,290 --> 01:30:41,870
ideally would like this link to be strong stronger this link the belly of the

1291
01:30:41,870 --> 01:30:46,770
beast only to retain an independent spirit award

1292
01:30:46,830 --> 01:30:49,680
i think we'll get this

1293
01:30:49,720 --> 01:30:57,620
OK so we argue in some cases in which we can identify causal effect from

1294
01:30:57,620 --> 01:31:02,540
purely observational data if you could only make the right assumptions and this a very

1295
01:31:03,200 --> 01:31:08,850
theory but developed by poco do calculus so imagine software problem and it's got these

1296
01:31:08,850 --> 01:31:12,580
to what i call the main of stochastic

1297
01:31:12,600 --> 01:31:17,910
variables as well as intervention variables and we specified that the appropriate

1298
01:31:17,930 --> 01:31:24,250
extended conditional independence properties possibly by demented dagger pearlian DAG will possibly does by now

1299
01:31:24,250 --> 01:31:28,790
to write a list of algebraic formulae it doesn't matter

1300
01:31:28,810 --> 01:31:34,120
and again we might have some unobservable variables in which case we call you

1301
01:31:34,120 --> 01:31:37,470
as of variables v so we get to get off

1302
01:31:37,490 --> 01:31:41,230
two problems with that data first of all it's only observational

1303
01:31:41,250 --> 01:31:44,230
secondly it's incomplete to the good news

1304
01:31:44,250 --> 01:31:47,200
we just got the observation distribution over these

1305
01:31:47,220 --> 01:31:48,930
and we're interested in

1306
01:31:48,970 --> 01:31:53,660
what can we say about cause and effect which is defined in terms of intervention

1307
01:31:53,660 --> 01:31:56,870
it is not so much that he is using you know

1308
01:31:56,920 --> 01:32:01,730
infinite or nonconstructive methods but is very very aware of it ten is very sensitive

1309
01:32:01,740 --> 01:32:06,910
to what is almost apologetic and he brings that makes that a central topic of

1310
01:32:10,430 --> 01:32:17,290
OK so having

1311
01:32:17,370 --> 01:32:23,820
made the case that the kernel was working firmly and squarely in the hilbert but

1312
01:32:23,820 --> 01:32:30,980
finitary metamathematical tradition known to try to make the case that that

1313
01:32:30,990 --> 01:32:32,570
he was at the same time

1314
01:32:32,580 --> 01:32:35,210
curiously at odds with that tradition

1315
01:32:35,220 --> 01:32:39,250
and in some way chafed against hope influence

1316
01:32:40,880 --> 01:32:48,300
in order to help clarify the relationship between a girl and her but i like

1317
01:32:48,300 --> 01:32:56,080
to start by discussing the relationship of cooperate with one of his predecessors namely kronecker

1318
01:32:56,090 --> 01:33:00,960
so help us early work in algebraic geometry and algebraic number theory was strongly influenced

1319
01:33:00,960 --> 01:33:06,200
by that of kronecker but as is well known but was very critical of chronic

1320
01:33:06,300 --> 01:33:08,860
methodological prescriptions for mathematics

1321
01:33:09,170 --> 01:33:13,020
and in fact it was programme can be seen as an attempt to do battle

1322
01:33:13,020 --> 01:33:21,740
with chroniclers own terms and the situation was was was very colourfully described by violence

1323
01:33:21,740 --> 01:33:23,330
obituary hopper

1324
01:33:23,350 --> 01:33:26,610
and the copy past so you know people indulge me and let me read it

1325
01:33:27,850 --> 01:33:29,470
so while rates

1326
01:33:29,490 --> 01:33:34,500
when one inquires into the dominant influences acting upon hilbert in his formative years

1327
01:33:34,510 --> 01:33:39,390
one is puzzled by the peculiarly ambivalent character his relationship to kronecker

1328
01:33:39,480 --> 01:33:42,500
dependent on him he rebels against

1329
01:33:42,500 --> 01:33:47,130
chronicles work is undoubtedly of paramount importance for hilbert in his algebraic period

1330
01:33:47,180 --> 01:33:51,040
but the old gentleman in berlin so it seemed to help used his power and

1331
01:33:51,040 --> 01:33:57,430
authority to stretch mathematics upon the procrustes the embedding arbitrary philosophical principles and to suppress

1332
01:33:57,430 --> 01:34:00,200
such developments as the not conform

1333
01:34:00,250 --> 01:34:04,700
a late echo of this old feud is the polemic against prowess intuitionism with which

1334
01:34:04,700 --> 01:34:09,540
the sexagenarian hilbert opens his first article and nobody knew the mathematik

1335
01:34:09,600 --> 01:34:15,850
help would slashing blows are aimed chroniclers ghost whom he sees rising from the grave

1336
01:34:15,920 --> 01:34:20,780
but inescapable ambivalence even here while he fights him he follows

1337
01:34:20,840 --> 01:34:28,780
reasoning along strictly intuitionistic lines is found necessary by him to safeguard non intuitionistic mathematics

1338
01:34:28,790 --> 01:34:32,340
so it it's very very colorful i really like the image of the

1339
01:34:32,400 --> 01:34:36,040
albert slashing blows and and chronicles ghost

1340
01:34:36,070 --> 01:34:42,430
rising from the sounds like a third-rate hard or something like that

1341
01:34:43,420 --> 01:34:48,170
so the relationship between gilbert girl and but it's not nearly as

1342
01:34:48,180 --> 01:34:49,420
as dramatic

1343
01:34:49,430 --> 01:34:54,120
so i've characterize girls work being firmly in the tradition at albert established and much

1344
01:34:54,120 --> 01:35:00,580
but in fact devoted to answering questions that albert himself posed in that regard girl

1345
01:35:00,600 --> 01:35:04,460
gives credit where it's due in does not in any way tonight opus influence or

1346
01:35:04,460 --> 01:35:06,820
play down the importance of his contribution

1347
01:35:06,840 --> 01:35:11,600
so in fact his nineteen thirty one paper and the incompleteness theorems ends with a

1348
01:35:11,600 --> 01:35:16,850
remarkably charitable an optimistic assessment with regard to its effect on

1349
01:35:16,980 --> 01:35:21,760
o bridge program so he writes i wish to note expressly that the second incompleteness

1350
01:35:21,760 --> 01:35:25,570
theorem you know for the formal systems that just considered that these dams do not

1351
01:35:25,570 --> 01:35:28,340
contradict helpers formalise the viewpoint

1352
01:35:28,500 --> 01:35:33,020
for this viewpoint presupposes only the existence of a consistency proof in which nothing but

1353
01:35:33,020 --> 01:35:37,660
finitary means of proof is used and it's conceivable that there exist finitary proofs that

1354
01:35:37,660 --> 01:35:43,130
cannot be expressed in the relevant formalisms so good holding out optimism that we might

1355
01:35:43,130 --> 01:35:44,010
be able to

1356
01:35:44,020 --> 01:35:51,150
developed compelling finitary principles and the man as you go beyond for example set theoretic

1357
01:35:52,800 --> 01:35:57,790
within a few years however he seems to have abandoned this view so for example

1358
01:35:57,790 --> 01:36:01,070
the lecture and social seminar in nineteen thirty eight

1359
01:36:01,070 --> 01:36:05,830
he's much more critical of attempts to salvage her in herbert's original plan to establish

1360
01:36:05,830 --> 01:36:11,270
the consistency of mathematics so commenting on and gets since proof of the consistency of

1361
01:36:11,270 --> 01:36:17,380
arithmetic arithmetic using transfinite induction up to epsilon not says the following this is the

1362
01:36:17,390 --> 01:36:18,890
second quote there

1363
01:36:18,910 --> 01:36:22,480
so i would like to remark by the way the against to give a proof

1364
01:36:22,490 --> 01:36:26,050
of this rule of inference to note again and try to give a heuristic argument

1365
01:36:26,360 --> 01:36:29,780
as to why we should accept transfinite induction that's why not

1366
01:36:29,880 --> 01:36:33,640
and even said that this was the essential part of his consistency proof

1367
01:36:33,660 --> 01:36:36,610
in reality is not in the true it's not a matter of proof at all

1368
01:36:36,610 --> 01:36:38,170
but an appeal to evidence

1369
01:36:38,180 --> 01:36:41,860
i think it makes more sense to formulate an axiom precisely and to say that

1370
01:36:41,860 --> 01:36:43,830
is just not further reducible

1371
01:36:43,890 --> 01:36:49,150
but here again the drive of hope its pupils to derive something from nothing stands

1372
01:36:49,890 --> 01:36:55,630
so that's a much more critical assessment notices that is not criticizing hilbert is criticizing

1373
01:36:55,630 --> 01:36:59,360
help pupils right but he's he's he's

1374
01:36:59,530 --> 01:37:07,730
you directly criticizing attempts to sort of formalized health help programme as it was originally

1375
01:37:08,950 --> 01:37:16,520
in later years one finds him generally very critical of planets methodology for example in

1376
01:37:16,520 --> 01:37:19,380
a letter he wrote to how long in nineteen sixty seven

1377
01:37:19,420 --> 01:37:23,410
he blamed the failure of failure of school to extract the completeness theorem from his

1378
01:37:23,420 --> 01:37:27,630
results of nineteen twenty three he blamed the failure on the intellectual climate at that

1379
01:37:27,630 --> 01:37:33,120
time so did his schooling came very close to two proving the completeness theorem feed

1380
01:37:33,130 --> 01:37:37,440
only just thought to state essentially the pieces were all there

1381
01:37:37,450 --> 01:37:41,870
and his what goals as he says the blindness or prejudice whatever you may call

1382
01:37:41,870 --> 01:37:46,620
it logicians is indeed surprising but i think the explanation is not hard to find

1383
01:37:46,630 --> 01:37:51,810
it lies in a widespread lack at that time of the required epistemological attitude towards

1384
01:37:51,810 --> 01:37:55,030
metamathematics and toward non finitary

1385
01:37:55,030 --> 01:37:59,150
so addition they just missing because they don't have the right attitude

1386
01:37:59,170 --> 01:38:03,220
and he goes on to say non finitary reasoning in mathematics was widely considered to

1387
01:38:03,220 --> 01:38:07,230
be meaningful only to the extent to which can be interpreted are justified in terms

1388
01:38:07,230 --> 01:38:14,110
of finitary metamathematics so there is again trying to i think these faces roughly characterizing

1389
01:38:14,110 --> 01:38:17,690
helper tradition and then he says you know that this for the most part has

1390
01:38:17,690 --> 01:38:23,440
turned out to be impossible in consequence of my results and subsequent subsequent work

1391
01:38:24,110 --> 01:38:29,170
in his remarks we went out of his way so despite the fact that

1392
01:38:29,230 --> 01:38:33,950
as i have said almost all of his proofs in logic or explicitly explicitly finitary

1393
01:38:33,950 --> 01:38:36,790
it comments to how long good one out of his way to emphasise the

1394
01:38:37,290 --> 01:38:42,570
quote the objective is to conception of mathematics and metamathematics in general and of transfinite

1395
01:38:42,570 --> 01:38:46,720
reasoning in particular was fundamental to my other work in logic

1396
01:38:46,740 --> 01:38:51,430
OK so of course by representing france finding methods with an explicit formal systems you

1397
01:38:51,430 --> 01:38:55,720
could always able to make use of such reasoning while maintaining the planetary significance but

1398
01:38:55,720 --> 01:39:00,120
it's interesting that later in his career what is playing up are the transfer methods

1399
01:39:00,120 --> 01:39:02,260
and sought to downplay the

1400
01:39:02,260 --> 01:39:05,960
the importance of the finitary metamathematics stance

1401
01:39:05,980 --> 01:39:12,260
let's is sort of a few

1402
01:39:12,270 --> 01:39:17,170
yes a few months later in a follow-up letter your your repeated the claim

1403
01:39:17,230 --> 01:39:21,520
so this is again this is the correspondence between kernel and how why so few

1404
01:39:21,520 --> 01:39:25,970
months later he repeated the claim that would have been practically impossible delivers results without

1405
01:39:26,190 --> 01:39:28,960
and i'm just an objective this conception

1406
01:39:28,960 --> 01:39:33,310
going down w two starting in nineteen going down to so i go

1407
01:39:33,990 --> 01:39:35,560
four-and-a-half before it's zero

1408
01:39:36,230 --> 01:39:40,920
w four starting every starting to accelerate to so i can one

1409
01:39:41,420 --> 01:39:43,730
so when x to y equals one here

1410
01:39:45,240 --> 01:39:51,510
w formulas are owners and that's the first one day zero so w four should be my leaving variable

1411
01:39:52,000 --> 01:39:53,630
if x two is my entering variable

1412
01:39:54,850 --> 01:39:58,810
how do that just a second let's look over here i could have chosen this woman i'm not going to

1413
01:39:59,530 --> 01:40:05,300
if i did this one i would have only these to look at so the fore and three here

1414
01:40:06,940 --> 01:40:11,080
we're at five going down the radio format which is zero after one and a quarter

1415
01:40:11,500 --> 01:40:14,810
after x four gets up to one recorder quarter this and i'm going down every

1416
01:40:14,840 --> 01:40:19,130
three so x four cruelty-free befor w two because there are so i would pick

1417
01:40:19,200 --> 01:40:20,710
x forty by entering variable

1418
01:40:21,180 --> 01:40:26,670
then w one be my leaving variables was in this one or r this one scientific this one

1419
01:40:30,020 --> 01:40:30,660
the algebra

1420
01:40:33,660 --> 01:40:35,080
i think you get all the

1421
01:40:35,150 --> 01:40:37,280
do we just described without actually doing it

1422
01:40:37,740 --> 01:40:43,340
i want put x two on the left so i would just take this equation here right next to

1423
01:40:43,810 --> 01:40:48,210
and the last hundred to x to the side bring w four outside divided by two

1424
01:40:50,030 --> 01:40:52,860
rearranges that equation but then i have to eliminate x to

1425
01:40:53,500 --> 01:40:55,270
from all these equations up here

1426
01:40:56,080 --> 01:40:56,650
by doing

1427
01:40:57,870 --> 01:40:59,410
gaussian elimination type

1428
01:41:00,220 --> 01:41:03,590
steps here for example to develop next to

1429
01:41:04,030 --> 01:41:05,310
the first this role here

1430
01:41:06,210 --> 01:41:09,850
i have a plus two here in the minus to here so if i took

1431
01:41:09,900 --> 01:41:14,950
this role and andw w forward to it so w one plus w four r

1432
01:41:15,050 --> 01:41:16,380
and at all these things that will be

1433
01:41:17,910 --> 01:41:19,140
plus two is seven

1434
01:41:21,150 --> 01:41:21,880
minus three

1435
01:41:22,300 --> 01:41:23,540
plus minus one

1436
01:41:25,030 --> 01:41:26,110
this goes to zero now

1437
01:41:26,670 --> 01:41:29,250
which is good i want to get rid of x two in this equation

1438
01:41:29,980 --> 01:41:34,700
but plus w four also attracted before but then in this slide

1439
01:41:35,190 --> 01:41:39,680
and similarly go across here and i get i update all these coefficients which very simple

1440
01:41:41,240 --> 01:41:42,140
just taking

1441
01:41:42,760 --> 01:41:48,310
this coefficients make it the sign robert something like this coefficient gets updated by taking

1442
01:41:49,280 --> 01:41:51,700
this time this the divided by that's

1443
01:41:53,830 --> 01:41:55,120
something very simple like that

1444
01:41:55,680 --> 01:41:58,490
but anyway because arithmetic is very funny this little

1445
01:41:58,870 --> 01:42:03,730
java application that doesn't of forming so when i click this button it doesn't arithmetic

1446
01:42:05,080 --> 01:42:08,930
i believe that's the seven and that's the minus one that i mentioned when i added w

1447
01:42:10,220 --> 01:42:13,800
fore announced attracted over there's minus w four appearing there are

1448
01:42:14,270 --> 01:42:20,140
divided by the two and that's why we see that one-half anyway the pivot told us all the arithmetic correctly

1449
01:42:20,650 --> 01:42:23,500
the objective function was zero to start with and now it's too

1450
01:42:23,990 --> 01:42:24,690
it's going up

1451
01:42:25,880 --> 01:42:26,810
we're trying to maximize

1452
01:42:27,440 --> 01:42:30,860
end although not optimal yet because this new dictionary

1453
01:42:32,800 --> 01:42:36,820
the solution associated with it is not optimal because

1454
01:42:37,260 --> 01:42:40,320
x forums a positive coefficient and if i

1455
01:42:40,900 --> 01:42:42,260
increase x four from

1456
01:42:43,490 --> 01:42:45,740
i can make further improvements in the objective function

1457
01:42:47,790 --> 01:42:50,510
that's what i wanna do next so i do the same thing that i just

1458
01:42:50,510 --> 01:42:52,750
did i look through here i have

1459
01:42:54,400 --> 01:42:57,930
don't look the ones are going down so there's a minus three and minus four r and

1460
01:42:58,370 --> 01:42:59,690
these don't have to worry about

1461
01:43:00,140 --> 01:43:04,510
so seven going rate one hundred three seven going down donor fore

1462
01:43:05,050 --> 01:43:10,460
that's the one that will have zero first so w two is my leaving variable effects force my entering variable

1463
01:43:11,010 --> 01:43:12,590
so i just click on this little button here

1464
01:43:12,990 --> 01:43:13,840
and it doesn't have it

1465
01:43:14,270 --> 01:43:14,870
and i've done

1466
01:43:19,010 --> 01:43:19,910
you can always check

1467
01:43:20,730 --> 01:43:21,570
if you're right

1468
01:43:26,000 --> 01:43:26,560
o i

1469
01:43:26,840 --> 01:43:27,890
my volume is too low

1470
01:43:32,440 --> 01:43:34,800
if you so the bounded the biologist for making

1471
01:43:35,260 --> 01:43:38,180
assertions if you make an incorrect assertion like power so this is

1472
01:43:40,680 --> 01:43:42,910
i get bart simpson none bart simpson

1473
01:43:45,380 --> 01:43:46,120
calling me a loser

1474
01:43:51,130 --> 01:43:52,000
somebody else's play

1475
01:43:53,060 --> 01:43:53,560
it was me

1476
01:43:55,770 --> 01:43:56,050
all right

1477
01:43:59,580 --> 01:44:01,540
there is the simplex methods

1478
01:44:04,040 --> 01:44:06,960
there are some issues that we have to discuss with you yes

1479
01:44:07,690 --> 01:44:09,270
there are some issues we have discussed

1480
01:44:11,850 --> 01:44:15,370
some problems are unbounded the answer is infinity

1481
01:44:16,290 --> 01:44:16,580
and how

1482
01:44:16,990 --> 01:44:19,240
how hard to detect with the simplex methods

1483
01:44:20,070 --> 01:44:21,130
here's an example

1484
01:44:21,530 --> 01:44:23,360
that's illustrates what can go

1485
01:44:25,790 --> 01:44:26,240
so here's

1486
01:44:26,840 --> 01:44:27,930
this is a dictionary

1487
01:44:29,390 --> 01:44:33,560
so we could take either-or x one to be the entering variable or x three

1488
01:44:37,160 --> 01:44:37,540
if we

1489
01:44:38,490 --> 01:44:38,950
pick pickaxe

1490
01:44:41,730 --> 01:44:42,710
we could do it here

1491
01:44:43,130 --> 01:44:46,140
there's been no problem i have a look at the to the three and the

1492
01:44:46,140 --> 01:44:48,990
foreign figure out which we believe are wide pay the fine

1493
01:44:51,350 --> 01:44:54,540
i'm greedy i go with the one with the largest coefficients so i pick x

1494
01:44:54,540 --> 01:44:56,130
one to be my entering variable

1495
01:44:56,630 --> 01:45:00,810
and here in this case being really really pays off because of i look down this column here

1496
01:45:01,870 --> 01:45:06,420
w one going up iterative five w two is going up iterators are w three

1497
01:45:06,420 --> 01:45:09,680
is not changing w four going up w five is going up

1498
01:45:10,350 --> 01:45:12,250
there's nothing blocking x one

1499
01:45:13,060 --> 01:45:14,410
from going all the way to infinity

1500
01:45:15,240 --> 01:45:16,440
and so the problems unbounded

1501
01:45:18,260 --> 01:45:21,850
end and that's how the simplex methods detects unbound this

1502
01:45:24,300 --> 01:45:27,760
so that's one thing that can go wrong and that's why there's a little button on the tool

1503
01:45:33,620 --> 01:45:34,380
i'm gonna show you

1504
01:45:38,700 --> 01:45:41,400
different algorithms for the simplex methods over the next couple hours

1505
01:45:42,230 --> 01:45:43,830
i one show you the first one

1506
01:45:44,590 --> 01:45:46,260
so i haven't shown complete thing because

1507
01:45:46,710 --> 01:45:48,790
i should what i should do assume that work

1508
01:45:49,200 --> 01:45:50,350
feasible the start with

1509
01:45:51,050 --> 01:45:56,560
and i went from being feasible today after that's really only half of what we

1510
01:45:56,560 --> 01:45:58,520
have to do is to find a feasible solution

1511
01:45:59,010 --> 01:45:59,650
the star with

1512
01:46:00,100 --> 01:46:00,810
and so on

1513
01:46:01,300 --> 01:46:05,860
that's called on initialisation how do we get started with the first feasible solution

1514
01:46:07,320 --> 01:46:12,170
so here's a problem where i have some negative values on the right hand side

1515
01:46:14,250 --> 01:46:19,210
end what i'm gonna do about them so these three methods and show you'd today

1516
01:46:21,460 --> 01:46:28,410
various ways of addressing the initialization problem or start out with the one show now because it's the simplest

1517
01:46:29,840 --> 01:46:31,980
quick and easy to explain but as we

1518
01:46:32,410 --> 01:46:34,590
move along a so you ones i think are better

1519
01:46:35,900 --> 01:46:39,890
but this is a very simple thing to do modify this problem just temporarily

1520
01:46:40,500 --> 01:46:43,260
by subtracting a new variable which are called x zero

1521
01:46:43,260 --> 01:46:47,910
decided so do an experiment this year house so

1522
01:46:48,170 --> 01:46:53,060
what i'm going to run the panel is initially foster on i have some questions

1523
01:46:53,080 --> 01:46:53,930
we are

1524
01:46:54,160 --> 01:46:55,630
asked panelists

1525
01:46:56,020 --> 01:46:59,180
out that they would like to solicit questions from you

1526
01:46:59,670 --> 01:47:01,010
we're going to do is that

1527
01:47:02,310 --> 01:47:03,970
technology mediation

1528
01:47:04,360 --> 01:47:05,780
so if we could have the

1529
01:47:06,190 --> 01:47:10,870
screen on you'll see yeah the page it's going to be displayed

1530
01:47:11,130 --> 01:47:14,090
throughout so append own

1531
01:47:14,450 --> 01:47:18,430
if she goes smooth on your way but i will device

1532
01:47:18,710 --> 01:47:21,610
kdd thirteen panel dot back chand dot

1533
01:47:22,870 --> 01:47:26,980
please by the way if you have a system with three d

1534
01:47:27,200 --> 01:47:29,200
please use the three jiri

1535
01:47:29,390 --> 01:47:33,910
because there are very limited number of ip addresses available in

1536
01:47:34,140 --> 01:47:37,630
in this room so you can get your ip address pleased that force

1537
01:47:38,300 --> 01:47:43,230
when you go so the landing page for the link to set up the top there

1538
01:47:43,470 --> 01:47:45,440
you you'll see one veins

1539
01:47:45,590 --> 01:47:47,740
so you click on to the panel events

1540
01:47:47,970 --> 01:47:50,850
that will take future what's displayed they are

1541
01:47:51,250 --> 01:47:56,790
at the top most popular questions foster i have ceded the system with

1542
01:47:56,970 --> 01:48:02,370
a large number of questions so if internet falls of the company the clean nobody can

1543
01:48:03,890 --> 01:48:04,960
one problem

1544
01:48:06,320 --> 01:48:12,770
you i going to lead soon provides some sort of identifies asking for identify and optional

1545
01:48:14,100 --> 01:48:18,310
once you feel that even you'll see this you can ask a question up the top

1546
01:48:18,730 --> 01:48:20,630
so post questions up the top

1547
01:48:23,830 --> 01:48:28,110
in the non questions wrong isla top and some other get displayed yeah

1548
01:48:28,270 --> 01:48:31,500
if you scroll down please scroll down just say

1549
01:48:31,820 --> 01:48:35,160
all questions they in down the bottom

1550
01:48:35,380 --> 01:48:39,020
and if you click on that apple down arrows on the side

1551
01:48:39,220 --> 01:48:43,570
you can put a the for or against questions and the most popular ones will

1552
01:48:43,910 --> 01:48:45,410
wrong rise to the top

1553
01:48:45,770 --> 01:48:48,200
so without further ado

1554
01:48:48,630 --> 01:48:53,930
let's get launched and i'm going to get the first start question to asama

1555
01:48:54,170 --> 01:48:56,770
so some you're very successful

1556
01:48:56,970 --> 01:48:58,040
research for

1557
01:48:58,230 --> 01:49:02,800
your also very successful s microsoft in industry

1558
01:49:02,960 --> 01:49:05,470
what are the first night you leave all of s

1559
01:49:05,810 --> 01:49:07,630
in order to launch first startup

1560
01:49:10,510 --> 01:49:13,760
i don't know if you ask my family at that time they would have said

1561
01:49:13,780 --> 01:49:15,600
these crazy lost

1562
01:49:16,470 --> 01:49:19,460
know the real driver is

1563
01:49:21,620 --> 01:49:23,520
the biggest factor is probably curiosity

1564
01:49:26,020 --> 01:49:30,120
have you know seeing startups happened wondering what the world is about if you're just doing

1565
01:49:30,140 --> 01:49:32,180
research just pure data science

1566
01:49:33,240 --> 01:49:34,140
but then the second

1567
01:49:34,380 --> 01:49:36,570
and third factors which are just as important

1568
01:49:38,540 --> 01:49:39,540
number to

1569
01:49:39,880 --> 01:49:43,530
is you really want to see impact of your work

1570
01:49:44,140 --> 01:49:46,730
you really want to seal works spread faster

1571
01:49:46,950 --> 01:49:49,720
and you always get to a degree where it could only have

1572
01:49:50,060 --> 01:49:54,630
so much influence from a platform whatever that got from as microsoft with a wonderful huge

1573
01:49:55,640 --> 01:49:56,910
but it has its limitations

1574
01:49:58,180 --> 01:50:00,190
and of course the third one is

1575
01:50:00,530 --> 01:50:01,770
unlimited upside

1576
01:50:02,570 --> 01:50:03,470
right so

1577
01:50:03,860 --> 01:50:07,040
if if you really do a good job if you really have a product that

1578
01:50:07,690 --> 01:50:09,090
people want out there

1579
01:50:09,480 --> 01:50:15,360
then this guy's element and there's nothing nothing as as exciting as that

1580
01:50:17,330 --> 01:50:20,360
so what the biggest surprise says

1581
01:50:24,450 --> 01:50:29,120
well i mean now now that i've got a few startups it's no longer surprise but at the time

1582
01:50:30,730 --> 01:50:33,400
look nothing nothing goes like you plan

1583
01:50:33,990 --> 01:50:36,370
it's always going to need more money

1584
01:50:36,870 --> 01:50:38,090
take longer time

1585
01:50:40,350 --> 01:50:45,080
you go through so or i went through some very very very dark times

1586
01:50:45,540 --> 01:50:46,630
you know you have to

1587
01:50:47,420 --> 01:50:51,590
know every good company has to go through a cycle where you have to lay off employees new

1588
01:50:51,610 --> 01:50:53,820
you face as zero cash balance and

1589
01:50:54,300 --> 01:50:56,230
the world looks really dark and then

1590
01:50:56,490 --> 01:50:58,960
somehow things work out and everything

1591
01:50:59,260 --> 01:51:00,270
comes together

1592
01:51:00,500 --> 01:51:02,120
through sheer will power

1593
01:51:03,680 --> 01:51:05,000
but i think

1594
01:51:05,420 --> 01:51:09,280
you know the biggest surprise now that i would say having done if you start ups

1595
01:51:09,440 --> 01:51:10,480
still surprises me

1596
01:51:10,930 --> 01:51:13,060
is no matter how how many times you do it

1597
01:51:13,410 --> 01:51:15,370
and how many times by a lecture

1598
01:51:15,760 --> 01:51:19,720
about you know every month i give a lecture about sixty entrepreneurs

1599
01:51:20,140 --> 01:51:21,970
and one of the lectures is about

1600
01:51:22,730 --> 01:51:25,190
mistakes to avoid when you do start up

1601
01:51:26,360 --> 01:51:28,060
and i'll just share with everyone

1602
01:51:28,740 --> 01:51:29,860
with every startup

1603
01:51:30,210 --> 01:51:36,070
and up doing the same mistakes over and over so f that's surprise which is down

1604
01:51:36,090 --> 01:51:37,270
both that question

1605
01:51:40,570 --> 01:51:44,880
so i have a question for or that sort of along the same lines i

1606
01:51:44,890 --> 01:51:48,920
think we know that there are a lot of academics in you know in the audience in your

1607
01:51:49,280 --> 01:51:52,650
you had a nice cushy job as a professor and

1608
01:51:53,180 --> 01:51:57,080
you know doing your research know why why the world would you

1609
01:51:57,310 --> 01:52:00,740
go off and do this what does it add to your life as an academic

1610
01:52:02,560 --> 01:52:04,390
be involved in founding

1611
01:52:05,430 --> 01:52:06,340
it's a lot of work

1612
01:52:08,040 --> 01:52:14,860
so let me quickly give to answers i think one is that in academia

1613
01:52:15,100 --> 01:52:18,170
once we've figured out how to survive we realized that we want

1614
01:52:18,350 --> 01:52:19,330
or have an impact

1615
01:52:19,480 --> 01:52:24,410
and engines like google scholar for you to back from the size here

1616
01:52:24,590 --> 01:52:28,170
is one way to quantify just how little impact we're having right

1617
01:52:28,420 --> 01:52:33,190
some my papers are cited maybe a few times here and there but fear one maybe not

1618
01:52:33,210 --> 01:52:36,930
at all and then when you click on the citation actually see what they said it becomes clear

1619
01:52:37,100 --> 01:52:38,540
that i read the paper

1620
01:52:39,740 --> 01:52:44,620
yeah like put paper on exit list papers out exit through my if i was lucky

1621
01:52:45,290 --> 01:52:48,550
so for me i felt like

1622
01:52:48,730 --> 01:52:53,310
how to have an impact teaching as a research as a but but found that

1623
01:52:53,490 --> 01:52:58,800
creating something that people love the first thing created where i have that kind of

1624
01:52:59,150 --> 01:53:04,940
relationship with with customers was better crawler or search anybody physically remember better crawler

1625
01:53:06,300 --> 01:53:08,610
yeah i think yeah

1626
01:53:08,770 --> 01:53:13,390
most of your to this is you know before for the was google such

1627
01:53:15,060 --> 01:53:18,640
so i think one way to have an impact was was a big deal for me

1628
01:53:18,750 --> 01:53:21,100
and then doing it for a while i realized that

1629
01:53:21,240 --> 01:53:25,540
academia as kind of like playing ridges it's

1630
01:53:25,670 --> 01:53:28,710
it's cerebral it's it's challenging it's fun

1631
01:53:28,910 --> 01:53:31,760
but startups is like playing poker red

1632
01:53:31,950 --> 01:53:34,640
or more is satisfaction more stake

1633
01:53:34,910 --> 01:53:37,010
like some was saying although

1634
01:53:37,330 --> 01:53:41,700
i don't have ever believed in a limited upside but there is severally higher

1635
01:53:41,720 --> 01:53:42,640
higher stakes

1636
01:53:42,780 --> 01:53:46,420
side actually love the ability to to do both

1637
01:53:47,150 --> 01:53:48,280
and you feel like

1638
01:53:49,190 --> 01:53:52,310
there from the perspective academic there are

1639
01:53:52,770 --> 01:53:54,710
particular costs

1640
01:53:54,860 --> 01:53:58,770
but ought to be taken into account for somebody just jump into doing a

1641
01:53:59,790 --> 01:54:05,220
as you're asking is their cost to the user professor to start so

1642
01:54:05,430 --> 01:54:07,640
often explain how

1643
01:54:07,640 --> 01:54:11,240
well done some research a very in

1644
01:54:11,390 --> 01:54:15,510
how to make suggestions on how to do this link prediction

1645
01:54:15,560 --> 01:54:19,350
and back when you're in a way i we looked at a number of the

1646
01:54:19,350 --> 01:54:22,930
social networks that they had and we look at you know what's the probability of

1647
01:54:22,930 --> 01:54:27,140
friendship as a function of distance so

1648
01:54:27,190 --> 01:54:30,390
now it is about how we found is that most of the new edges defined

1649
01:54:30,600 --> 01:54:33,740
for independent friends people there are two hops away

1650
01:54:33,740 --> 01:54:38,810
so all of these graphs showing how many friendships were created as a function of

1651
01:54:38,810 --> 01:54:40,760
distance so this

1652
01:54:40,790 --> 01:54:43,220
x x is now these is the number of hops

1653
01:54:43,240 --> 01:54:47,100
the two people were part before they are connected by the set

1654
01:54:47,560 --> 01:54:51,830
so little to see but all these only what similar log scales

1655
01:54:51,870 --> 01:54:56,350
so there's something like an order of magnitude between two thousand three so

1656
01:54:57,100 --> 01:55:03,140
by far the majority of french going to people their friends of friends

1657
01:55:03,140 --> 01:55:04,510
and this

1658
01:55:04,530 --> 01:55:07,950
becomes even more dramatic when normalized to the fact that there are many many more

1659
01:55:07,950 --> 01:55:12,220
people three hops away into absolutely right you have something like a hundred thirty square

1660
01:55:12,220 --> 01:55:14,240
friends of friends and then another factor of one

1661
01:55:15,080 --> 01:55:18,450
one for that and

1662
01:55:18,470 --> 01:55:23,450
of course there is overlap and that approximately and so

1663
01:55:23,450 --> 01:55:29,350
this is good from the point of view because already under very something like seventeen

1664
01:55:30,310 --> 01:55:34,160
and so if you had to go any further to generate you cannot you get

1665
01:55:34,160 --> 01:55:36,350
into an astronomical numbers

1666
01:55:36,370 --> 01:55:43,890
so there's only one and all the in this book in or not too surprisingly

1667
01:55:43,890 --> 01:55:45,390
it follows the same

1668
01:55:45,410 --> 01:55:51,260
it's on work as in these other social networks by far the most of the

1669
01:55:51,260 --> 01:55:54,240
edges are planning two friends of friends

1670
01:55:56,470 --> 01:56:02,010
that brings us to this so idea problems which is here we have this source

1671
01:56:02,010 --> 01:56:07,390
notes and we have all that person's friends and then we have all the friends

1672
01:56:07,390 --> 01:56:11,100
of friends and would like to do is figure out what

1673
01:56:11,120 --> 01:56:16,310
his friends of friends and the actual average is something like forty thousand enough to

1674
01:56:16,310 --> 01:56:19,680
leave it as an exercise to think why it's so much larger than hundred thirty

1675
01:56:21,830 --> 01:56:26,850
it's all of those forty thousand friends of friends which ones are my most likely

1676
01:56:27,570 --> 01:56:30,870
become friends with the weight on the front but

1677
01:56:30,910 --> 01:56:36,160
and so the challenge is one of scale something out you know how to do

1678
01:56:36,160 --> 01:56:39,660
this for five million users times forty thousand friends of friends

1679
01:56:39,660 --> 01:56:44,810
and then also the machine learning challenges now you know what the right demographic in

1680
01:56:44,810 --> 01:56:46,160
our features are

1681
01:56:46,180 --> 01:56:47,720
to do this problem

1682
01:56:47,740 --> 01:56:55,580
so if you to do something simple so with a good thing to do would

1683
01:56:56,100 --> 01:56:59,850
or just to look at the number of friends in common sense

1684
01:56:59,870 --> 01:57:03,720
make sense since this is not the intuition that if you have many friends in

1685
01:57:03,720 --> 01:57:06,290
common with the person you probably know them somehow

1686
01:57:06,350 --> 01:57:11,660
the the first is true and so this is a graph showing the probability of

1687
01:57:11,660 --> 01:57:15,700
friendship as a function the mutual friend and then the

1688
01:57:16,810 --> 01:57:22,140
because of very quickly again it's on log log scale this time but see you

1689
01:57:22,140 --> 01:57:26,100
can see that going from one to ten friends going up by

1690
01:57:26,140 --> 01:57:30,720
in order to magnitude or so and the time you get two hundred fans your

1691
01:57:30,740 --> 01:57:32,850
over a thousand times more likely to

1692
01:57:32,850 --> 01:57:37,470
and finally make fun connection and

1693
01:57:37,490 --> 01:57:39,430
so that's a good start but we can actually

1694
01:57:39,450 --> 01:57:44,390
do much better by building more sophisticated features and doing a lot of machine learning

1695
01:57:44,410 --> 01:57:46,350
so just to give one example

1696
01:57:46,430 --> 01:57:51,930
and in the end i think there's a lot of them in this genre

1697
01:57:53,180 --> 01:57:54,010
so here's

1698
01:57:54,080 --> 01:58:01,640
this is this user's and use this to find a friend and there's some people

1699
01:58:01,640 --> 01:58:03,490
some in the media is

1700
01:58:03,540 --> 01:58:05,100
switch to make

1701
01:58:05,120 --> 01:58:11,120
two different and so all these edges have some annotations on the right and in

1702
01:58:11,120 --> 01:58:13,510
particular we think about the ages

1703
01:58:13,540 --> 01:58:15,200
and so

1704
01:58:15,260 --> 01:58:20,640
this is a serious ongoing fourteen years old this is very new it's only available

1705
01:58:20,640 --> 01:58:24,290
and would like to do is use the information in the new edges are somehow

1706
01:58:24,290 --> 01:58:25,990
they receive higher weight

1707
01:58:26,120 --> 01:58:29,120
so if you think about why this might be

1708
01:58:29,180 --> 01:58:33,290
well you have your friends from a few years ago in high school that you've

1709
01:58:33,290 --> 01:58:37,330
never seen sensor nets are still in your friend list but it's very unlikely that

1710
01:58:37,330 --> 01:58:41,310
you're going to be friend become friends with one of your old high school friends

1711
01:58:41,350 --> 01:58:45,030
new friends for instance use are living somewhere far away

1712
01:58:45,040 --> 01:58:48,680
and so it turns out that the time stamps and it is very important for

1713
01:58:48,680 --> 01:58:49,850
doing the job

1714
01:58:49,890 --> 01:58:53,330
and so we can sort of the features that take into account

1715
01:58:53,410 --> 01:58:56,680
by using these delta so this is one example where we sum over all the

1716
01:58:56,680 --> 01:58:58,990
intermediate so some of these two people

1717
01:58:59,040 --> 01:59:03,100
and we come up with some formulas that you know decreasing with time so in

1718
01:59:03,100 --> 01:59:04,830
this case we just multiply

1719
01:59:04,870 --> 01:59:07,850
these two times together and inverse them

1720
01:59:07,870 --> 01:59:10,950
and then maybe think

1721
01:59:11,010 --> 01:59:13,560
think some intuition from pagerank where

1722
01:59:13,560 --> 01:59:17,240
every node to distributes its influence equally

1723
01:59:17,260 --> 01:59:22,430
and if an intermediate node has five thousand friends you should be able to us

1724
01:59:22,850 --> 01:59:25,620
give people influenced all those five thousand people

1725
01:59:25,620 --> 01:59:28,140
so we'll do something like divided by

1726
01:59:28,140 --> 01:59:29,490
the person's degree

1727
01:59:29,540 --> 01:59:33,930
and it turned out in practice for whatever reason about buried degrees

1728
01:59:38,930 --> 01:59:41,580
i don't want to give the impression that this is

1729
01:59:41,760 --> 01:59:45,850
sailing all of but this is just sort of should give you a flavor of

1730
01:59:45,850 --> 01:59:49,310
the features that we can build with this rich

1731
01:59:49,310 --> 01:59:51,140
does that we have

1732
01:59:54,470 --> 01:59:58,080
let me give you a high-level system overview of how all the fences that things

1733
01:59:58,080 --> 01:59:59,640
are made based

1734
01:59:59,660 --> 02:00:04,760
so the first step is the you know this very user and it comes in

1735
02:00:04,760 --> 02:00:09,180
the system which goes in does all this from the friends figures out who does

1736
02:00:09,180 --> 02:00:12,530
forty thousand people are there are potential candidates

1737
02:00:12,580 --> 02:00:17,040
generative under features were so simple things like the number of mutual friends

1738
02:00:17,060 --> 02:00:22,660
what complicated things like this sort of time weighted mutual friends that talked about

1739
02:00:22,850 --> 02:00:27,700
but then also demographic features so things like the age of both of the other

1740
02:00:27,700 --> 02:00:28,740
sources are in and

1741
02:00:28,890 --> 02:00:31,390
the ages of all people cannot

1742
02:00:31,390 --> 02:00:34,200
and things like that

1743
02:00:34,260 --> 02:00:36,580
that goes into some machine learning system

1744
02:00:36,620 --> 02:00:41,030
and for this we just use sort of relatively simple off the shelf

1745
02:00:41,030 --> 02:00:42,810
approach to by

1746
02:00:42,830 --> 02:00:43,890
decision trees

1747
02:00:43,910 --> 02:00:47,450
and that produces some predictions for all those people so all of the friends of

1748
02:00:47,450 --> 02:00:49,580
friends get some support

1749
02:00:49,600 --> 02:00:53,220
now of course we want to show you the same two people every time you

1750
02:00:53,220 --> 02:00:54,470
can say

1751
02:00:54,490 --> 02:00:56,470
so we need to do something

1752
02:00:56,490 --> 02:01:00,470
you know and every impression every home page view to show you the best

1753
02:01:00,510 --> 02:01:03,390
pair suggestions and

1754
02:01:03,410 --> 02:01:08,240
the first two steps are relatively expensive so we don't do them now on every

1755
02:01:08,240 --> 02:01:13,540
page load i said well there is cash so the top fifty and then when

1756
02:01:13,540 --> 02:01:18,310
you can say well we ran all those fifty people using something much simpler machine

1757
02:01:18,310 --> 02:01:19,330
learning model

1758
02:01:19,350 --> 02:01:24,620
and that gives us the real time series prediction where we combine these stories with

1759
02:01:24,830 --> 02:01:28,890
for instance the impression counter so how many times you've seen to suggestions on the

1760
02:01:28,890 --> 02:01:32,510
last day

1761
02:01:32,560 --> 02:01:35,330
OK so i mean for almost the entire

1762
02:01:35,330 --> 02:01:38,020
the lights

1763
02:01:38,450 --> 02:01:40,120
is being done

1764
02:01:40,120 --> 02:01:41,640
that means that

1765
02:01:43,410 --> 02:01:45,180
i know

1766
02:01:45,200 --> 02:01:50,830
he sort of also if will know what you see is what you see that

1767
02:01:50,890 --> 02:01:53,730
there is data mining machine learning operations

1768
02:01:55,810 --> 02:01:58,680
elevation is one them

1769
02:01:58,700 --> 02:02:03,060
even distributed across all the last but we don't know yet

1770
02:02:03,080 --> 02:02:04,660
by by

1771
02:02:05,200 --> 02:02:07,220
time is running around

1772
02:02:07,230 --> 02:02:09,310
this is a very

1773
02:02:09,330 --> 02:02:12,250
after the fall of this

1774
02:02:12,270 --> 02:02:15,600
and the situation this is you know the and

1775
02:02:15,640 --> 02:02:18,830
this is almost anything stabilize

1776
02:02:19,000 --> 02:02:21,700
nine five in amsterdam

1777
02:02:21,700 --> 02:02:24,520
and this is the of of the data mining

1778
02:02:24,560 --> 02:02:30,000
you want to learn a lot about what belongs to what are the points race

1779
02:02:30,020 --> 02:02:35,790
so one of us so you can see this kind hard for me to stop

1780
02:02:37,600 --> 02:02:39,160
are going to

1781
02:02:39,180 --> 02:02:44,460
so much so that's a good times all the other things

1782
02:02:45,230 --> 02:02:47,620
actually this accuracy was

1783
02:02:47,640 --> 02:02:54,980
other easy for example the of reason you say you know this activity is going

1784
02:02:55,000 --> 02:02:57,680
to say

1785
02:02:57,770 --> 02:03:03,040
and you can look at it and you also see this

1786
02:03:03,060 --> 02:03:05,450
you can get a pretty high

1787
02:03:06,580 --> 02:03:08,160
my name's

1788
02:03:08,160 --> 02:03:11,950
well i want to show you here is if you are not covered by the

1789
02:03:12,950 --> 02:03:15,830
see a lot something red

1790
02:03:15,930 --> 02:03:19,020
i believe that we are producing using these messages

1791
02:03:19,040 --> 02:03:20,020
but a few

1792
02:03:20,080 --> 02:03:21,620
can we decide to it

1793
02:03:21,620 --> 02:03:25,220
i really enjoyed reading this blah

1794
02:03:25,230 --> 02:03:26,100
they are

1795
02:03:26,120 --> 02:03:27,910
whereas red one the

1796
02:03:27,930 --> 02:03:30,700
ninety three is on the third column

1797
02:03:30,720 --> 02:03:32,020
this is what is

1798
02:03:32,040 --> 02:03:33,620
and so on

1799
02:03:33,700 --> 02:03:36,140
OK i see you are

1800
02:03:38,020 --> 02:03:40,450
in ICML each time

1801
02:03:40,460 --> 02:03:45,120
so i go down all these concepts of

1802
02:03:46,450 --> 02:03:47,520
what about the

1803
02:03:48,060 --> 02:03:50,640
the first one is the first one he

1804
02:03:50,660 --> 02:03:52,480
i think the

1805
02:03:55,040 --> 02:03:57,460
which one that's

1806
02:03:57,480 --> 02:04:00,000
sigmod vldb

1807
02:04:00,020 --> 02:04:03,410
these are the conference you can see

1808
02:04:03,700 --> 02:04:09,930
as i already so high that was furious after reaching again as you can see

1809
02:04:09,960 --> 02:04:15,730
here it is the only way to mark that don't worry which ones when i

1810
02:04:15,730 --> 02:04:17,500
was young

1811
02:04:17,500 --> 02:04:19,410
you it which you don't

1812
02:04:20,000 --> 02:04:25,000
it's hard to say it i have to say it's like i conference which it

1813
02:04:25,000 --> 02:04:27,250
asserts that makes sense

1814
02:04:27,290 --> 02:04:31,330
hello everybody only going over on five

1815
02:04:31,350 --> 02:04:33,200
the high winds i

1816
02:04:33,200 --> 02:04:40,000
four followed by but there is also some data-mining something on it

1817
02:04:40,020 --> 02:04:46,180
it's almost everywhere estate so that's why CIKM conference is not sure which suffer loss

1818
02:04:46,830 --> 02:04:49,230
but it's sort of like

1819
02:04:49,250 --> 02:04:50,950
people need to raise

1820
02:04:50,960 --> 02:04:52,180
but he said

1821
02:04:52,500 --> 02:04:56,750
can you know which ones they found the this one day it's

1822
02:04:56,750 --> 02:04:59,410
b memory bank everything

1823
02:04:59,410 --> 02:05:03,640
we write the words we conferences we ran

1824
02:05:04,220 --> 02:05:10,230
what are the ones that come from the viewers so database database system they for

1825
02:05:10,230 --> 02:05:13,700
a system for the management of the relational

