1
00:00:00,000 --> 00:00:01,750
so this is very rare

2
00:00:01,830 --> 00:00:02,900
the computer

3
00:00:02,920 --> 00:00:09,860
OK if i was really and coffee and to get a job to trying to

4
00:00:09,860 --> 00:00:10,490
do this

5
00:00:10,880 --> 00:00:16,940
i don't want to model this model is factorizations are distribution here to this

6
00:00:17,370 --> 00:00:20,700
specify what is

7
00:00:20,760 --> 00:00:24,820
one sometimes makes you parents of all this

8
00:00:24,820 --> 00:00:31,840
as and with is is the sum over x two four x the size of

9
00:00:31,840 --> 00:00:37,640
the right side over x two x one x five here right here parameters

10
00:00:37,650 --> 00:00:39,340
he is this

11
00:00:39,360 --> 00:00:41,500
OK that's door to

12
00:00:41,520 --> 00:00:43,060
it's right star

13
00:00:43,070 --> 00:00:44,620
how can you say work

14
00:00:44,630 --> 00:00:47,310
you can say word by saying

15
00:00:47,320 --> 00:00:50,430
this term some exciting

16
00:00:51,150 --> 00:00:56,980
these you depending on the size of the question is all the way

17
00:00:57,000 --> 00:00:58,750
in the here

18
00:00:58,750 --> 00:01:01,940
and now i'm just going to do this calculation first

19
00:01:01,940 --> 00:01:07,460
compute this five whatever i get all stored in some interval research and

20
00:01:07,480 --> 00:01:08,310
push this

21
00:01:08,320 --> 00:01:15,380
although it here and this intermediate is an assignment on each one of these much

22
00:01:15,400 --> 00:01:19,450
were back from and i nineteen eighty

23
00:01:19,460 --> 00:01:26,020
so if you look at the slide and is not totally clear we see something

24
00:01:26,160 --> 00:01:29,220
if you see this history

25
00:01:29,240 --> 00:01:32,230
the problem is going be be this treaty

26
00:01:32,490 --> 00:01:36,190
these are unclear on this

27
00:01:36,200 --> 00:01:37,730
this is just the distributed

28
00:01:40,370 --> 00:01:41,670
that's the idea

29
00:01:41,680 --> 00:01:46,040
we exploit the factorisation of the joint distribution to save

30
00:01:46,050 --> 00:01:50,680
four by doing intermediate solutions in this

31
00:01:51,930 --> 00:01:57,580
this is a very good and you could always just write down the equations

32
00:01:57,600 --> 00:01:58,860
you have one

33
00:01:58,890 --> 00:02:03,450
four about how you're going to see how do it you can solve inference problem

34
00:02:03,450 --> 00:02:05,040
by writing down the distribution

35
00:02:05,450 --> 00:02:11,830
rearranging solutions for in writing computer but what you don't know what we're going to

36
00:02:11,830 --> 00:02:14,490
be for the future

37
00:02:14,520 --> 00:02:16,370
so please consider

38
00:02:16,760 --> 00:02:19,710
so i can just tell you anything about it

39
00:02:19,730 --> 00:02:26,340
o model was anything else is query and still

40
00:02:26,350 --> 00:02:27,730
well all

41
00:02:27,770 --> 00:02:29,100
OK so computer

42
00:02:29,140 --> 00:02:31,560
only has to answer query

43
00:02:31,570 --> 00:02:34,310
observing x six

44
00:02:34,320 --> 00:02:35,830
one could do

45
00:02:35,850 --> 00:02:39,720
right wrong once but it has to be able to answer any queries like the

46
00:02:43,960 --> 00:02:49,520
computer program inside the actors so that's what we're doing

47
00:02:51,660 --> 00:02:55,980
just as bad before i remember the

48
00:02:56,050 --> 00:02:58,810
general different problem involves

49
00:02:58,820 --> 00:03:03,020
all special cases of all the things that want do one

50
00:03:03,060 --> 00:03:07,770
so you know this then you can just

51
00:03:09,690 --> 00:03:13,440
it's so you have very complicated distribution you want know

52
00:03:13,440 --> 00:03:18,490
the marginal probability is known as the distribution is inference

53
00:03:18,510 --> 00:03:22,300
with no opposition and with that query

54
00:03:22,560 --> 00:03:25,400
this is something that all

55
00:03:26,660 --> 00:03:28,460
so i don't know

56
00:03:28,470 --> 00:03:31,230
parents you can read the article directly

57
00:03:31,260 --> 00:03:33,330
graph of were

58
00:03:34,300 --> 00:03:39,680
and in my direction model you can ignore all the the nodes that are downstream

59
00:03:39,680 --> 00:03:41,430
from the query

60
00:03:41,440 --> 00:03:42,590
so long as no

61
00:03:42,610 --> 00:03:49,220
they're just marginalize on only are graph before so that we have to see

62
00:03:49,280 --> 00:03:51,020
and then

63
00:03:51,180 --> 00:03:53,520
also directed models you know

64
00:03:53,530 --> 00:04:00,860
service must evaluate one because the conditional probabilities so so this distribution px x four

65
00:04:01,210 --> 00:04:02,050
x two

66
00:04:02,090 --> 00:04:08,060
when the sun x four i know one because the conditional solution rather than asking

67
00:04:08,060 --> 00:04:10,230
my computer is

68
00:04:10,260 --> 00:04:16,930
one just right is coded says if there's some conditional distribution over the first

69
00:04:16,940 --> 00:04:18,510
that's one

70
00:04:18,520 --> 00:04:19,440
so this

71
00:04:21,400 --> 00:04:24,210
but before we can write a general

72
00:04:24,220 --> 00:04:33,440
inference for directed graphs we need to know is the first few call explaining away

73
00:04:34,140 --> 00:04:36,920
it uses correlation to me

74
00:04:36,930 --> 00:04:39,510
the inverse problems

75
00:04:39,760 --> 00:04:45,990
so remember we're trying to figure out which ones are coupled based on some sort

76
00:04:47,010 --> 00:04:50,990
you might think could work at this size

77
00:04:51,020 --> 00:04:56,090
the graph because the explaining away nodes

78
00:04:56,340 --> 00:04:57,530
controlled by

79
00:04:57,570 --> 00:05:00,060
observations which were originally

80
00:05:00,070 --> 00:05:02,150
the graph so in order to

81
00:05:02,160 --> 00:05:03,960
here is we

82
00:05:03,970 --> 00:05:06,300
the g

83
00:05:07,100 --> 00:05:08,680
some of the more

84
00:05:08,680 --> 00:05:12,120
i first compute things coming from the

85
00:05:14,870 --> 00:05:16,320
and i come from here

86
00:05:16,330 --> 00:05:20,210
if i want to compute not query for x in minus one

87
00:05:20,220 --> 00:05:24,250
four exactly the same argument i need to multiply these

88
00:05:24,280 --> 00:05:28,930
message by the message that's coming from there to here

89
00:05:35,580 --> 00:05:40,250
but only these message and i haven't computed these methods compute x

90
00:05:40,260 --> 00:05:42,120
so what we do is

91
00:05:42,170 --> 00:05:46,620
if i want to compute every query the p of every xn

92
00:05:46,670 --> 00:05:48,930
what's a smart way to do do it

93
00:05:48,940 --> 00:05:52,780
well i just compute all the messages

94
00:05:52,800 --> 00:05:57,300
then whenever i have to compute p of xn

95
00:05:57,340 --> 00:06:00,110
i just checked

96
00:06:00,160 --> 00:06:04,930
which are the messages that they need to want

97
00:06:04,970 --> 00:06:09,480
so in order to compute a single p of x

98
00:06:09,490 --> 00:06:10,750
i have one cost

99
00:06:10,760 --> 00:06:14,820
in article in order to have access to compute all p of xn

100
00:06:14,870 --> 00:06:17,950
it's just twice the course

101
00:06:17,950 --> 00:06:20,700
because i just need to compute all the message

102
00:06:20,750 --> 00:06:22,400
and then they can query

103
00:06:25,920 --> 00:06:29,340
it's called belief propagation these immediately translates to

104
00:06:30,440 --> 00:06:33,070
because actually the three quite is

105
00:06:33,090 --> 00:06:36,160
well you why only that the reason why the change here

106
00:06:36,230 --> 00:06:39,470
it is because at some point this thing and

107
00:06:39,480 --> 00:06:43,670
and then you can start to compute it

108
00:06:43,720 --> 00:06:48,220
because from the perspective of the elimination of with them

109
00:06:48,250 --> 00:06:52,570
what's really happening is that you're always something

110
00:06:52,610 --> 00:06:54,520
on the leaf nodes

111
00:06:54,570 --> 00:06:57,110
and then you eliminate that leaves

112
00:06:57,200 --> 00:07:01,140
you can imagine that you were actually only some leaves just your

113
00:07:01,150 --> 00:07:03,740
three string

114
00:07:06,440 --> 00:07:10,710
every possible some that you do you do only you don't know anything unless we

115
00:07:10,710 --> 00:07:14,800
choose the elimination order that comes from the leaves

116
00:07:14,850 --> 00:07:18,930
these were eliminated by definition the next one will be only already OK

117
00:07:18,960 --> 00:07:20,050
and then you keep on going

118
00:07:20,080 --> 00:07:22,250
that's why you need

119
00:07:22,250 --> 00:07:28,590
a change or three according to general case

120
00:07:28,760 --> 00:07:36,510
i was

121
00:07:52,970 --> 00:08:00,430
no they don't agree they don't

122
00:08:00,480 --> 00:08:04,500
what is important to understand is that we have two steps

123
00:08:04,510 --> 00:08:08,120
one step one step is exactly what you said we need to start from the

124
00:08:10,030 --> 00:08:11,930
and propagate

125
00:08:11,970 --> 00:08:15,420
these messages and next master will depend on christmas

126
00:08:15,430 --> 00:08:16,750
and back

127
00:08:16,770 --> 00:08:20,280
then you have compiled the entire set of mass

128
00:08:20,330 --> 00:08:22,890
then it's just the query

129
00:08:22,890 --> 00:08:26,710
what's p of x and just multiplying incoming message

130
00:08:26,840 --> 00:08:29,270
that's school we completely correct so

131
00:08:29,280 --> 00:08:33,850
at compile time basically you need to run this protocol starting from the leaf

132
00:08:33,910 --> 00:08:38,160
propagate everything and then propagate that

133
00:08:39,410 --> 00:08:44,140
you can start for example next one xn propagate xn then propagate back to x

134
00:08:44,140 --> 00:08:46,410
one xn

135
00:08:46,410 --> 00:08:47,200
we can

136
00:08:47,260 --> 00:08:51,570
into this choose new route three

137
00:08:51,590 --> 00:08:53,750
so this called belief propagation

138
00:08:53,800 --> 00:08:55,740
it's really nothing more

139
00:08:55,750 --> 00:08:59,500
what's more again

140
00:08:59,540 --> 00:09:03,660
bars requirements OK so let's study the storage requirements here

141
00:09:03,660 --> 00:09:09,250
well basically authorities limited to small tables you think think that table that these functions

142
00:09:09,250 --> 00:09:09,990
are just

143
00:09:10,070 --> 00:09:11,030
you have

144
00:09:12,920 --> 00:09:15,980
a function which is

145
00:09:15,980 --> 00:09:20,780
for example a real valued function of some discrete five

146
00:09:20,800 --> 00:09:23,480
so you have some state space here

147
00:09:23,510 --> 00:09:24,460
let's call

148
00:09:24,460 --> 00:09:26,020
the size of these

149
00:09:26,040 --> 00:09:29,590
is as for example or state space

150
00:09:30,330 --> 00:09:33,060
obviously you need to explore

151
00:09:33,070 --> 00:09:35,020
s square numbers

152
00:09:35,070 --> 00:09:36,220
for these

153
00:09:36,220 --> 00:09:38,170
quantities here

154
00:09:38,260 --> 00:09:40,240
for every one of these quantities

155
00:09:40,280 --> 00:09:42,750
you have in mind is one of them

156
00:09:42,760 --> 00:09:44,050
because you have

157
00:09:44,060 --> 00:09:46,020
n minus one

158
00:09:47,120 --> 00:09:48,900
in the tree

159
00:09:49,010 --> 00:09:54,440
so the complexity of supporting all these things is in one of times

160
00:09:54,450 --> 00:09:56,760
and square

161
00:09:56,810 --> 00:09:58,390
is because it's the three

162
00:09:58,390 --> 00:10:02,670
what you see before we finish today is a generalisation of these are

163
00:10:02,720 --> 00:10:04,370
for arbitrary graphs

164
00:10:04,390 --> 00:10:06,330
that square can become cubic

165
00:10:06,350 --> 00:10:07,940
and so on

166
00:10:10,560 --> 00:10:14,540
it is a combination

167
00:10:14,750 --> 00:10:17,030
for example one ratio

168
00:10:17,050 --> 00:10:18,240
it's a

169
00:10:19,920 --> 00:10:24,250
so all

170
00:10:26,830 --> 00:10:30,750
one of these

171
00:10:36,740 --> 00:10:40,000
that's something like that

172
00:10:40,040 --> 00:10:42,940
the the we really

173
00:10:45,160 --> 00:10:47,760
repeated computations here is by

174
00:10:47,780 --> 00:10:51,010
is by writing the right political

175
00:10:51,010 --> 00:10:54,010
so you just run the right protocol

176
00:10:54,020 --> 00:10:56,260
and so that the query time

177
00:10:56,280 --> 00:11:01,610
you just do very simple query

178
00:11:02,750 --> 00:11:04,960
let's proceed because there's throughout

179
00:11:06,070 --> 00:11:08,230
so basically that's what i just said

180
00:11:08,240 --> 00:11:11,960
so the computational complexity we have

181
00:11:12,010 --> 00:11:15,930
we have two times in messages to be computed

182
00:11:15,940 --> 00:11:20,190
because you have in mind of the mass in one direction and minus one messages

183
00:11:20,210 --> 00:11:23,330
in the other direction

184
00:11:24,060 --> 00:11:25,960
and also obviously you can

185
00:11:25,960 --> 00:11:28,930
if you just defining trivial messages from the leaves

186
00:11:28,950 --> 00:11:29,790
as one

187
00:11:29,810 --> 00:11:32,170
then you can write the general equation

188
00:11:32,200 --> 00:11:34,320
the the more nice way

189
00:11:35,100 --> 00:11:38,280
really help to run the of

190
00:11:38,290 --> 00:11:42,780
but that's just in the official journal is just short

191
00:11:42,830 --> 00:11:46,810
and you can run belief propagation in the tree as well is exactly the same

192
00:11:46,810 --> 00:11:47,920
is just that

193
00:11:47,960 --> 00:11:49,090
whenever u

194
00:11:49,110 --> 00:11:52,180
come whenever you have an incoming message

195
00:11:52,220 --> 00:11:53,690
you need to to take the product

196
00:11:53,740 --> 00:11:57,710
of all incoming messages in order to send the next message

197
00:11:57,850 --> 00:12:03,520
so what happened

198
00:12:03,530 --> 00:12:07,460
message passing equation so the message that you sent from j

199
00:12:07,460 --> 00:12:11,280
talked about the process basics and we've seen how to can be used in regression

200
00:12:12,000 --> 00:12:12,360
so now

201
00:12:13,340 --> 00:12:16,210
i wanna go slightly beyond slightly beyond the basics

202
00:12:17,920 --> 00:12:20,100
so i this is one thing this is one thing that i want to say

203
00:12:20,600 --> 00:12:22,110
but during during the break

204
00:12:22,540 --> 00:12:26,220
is there for more details on this this is a book that was written by by

205
00:12:26,830 --> 00:12:28,080
by carl chris williams

206
00:12:28,610 --> 00:12:32,370
gas processes for machine learning and a lot of what we've talked about so far

207
00:12:32,700 --> 00:12:36,460
and it's just it's just within the first five chapters so this is this is

208
00:12:36,460 --> 00:12:37,370
available online

209
00:12:38,590 --> 00:12:43,010
and in any more details that you want a lot stuff we talked about should be should be found here

210
00:12:45,400 --> 00:12:48,720
okay so what's so what's next we've looked at the basics now let's revisit the

211
00:12:48,720 --> 00:12:52,030
model and see what can be what we can mess around with

212
00:12:53,090 --> 00:12:53,350
and so

213
00:12:53,890 --> 00:12:55,080
i want to use that's it

214
00:12:55,690 --> 00:12:59,720
in a very specific way states to see what we can mess around with because i wanna talk about why

215
00:13:00,410 --> 00:13:04,460
normally when we present what's next and guassian processes we talk about gas prices for

216
00:13:04,460 --> 00:13:06,230
classification we talk about different kernels

217
00:13:06,650 --> 00:13:11,010
and it's and it's it's not always clear why why we're doing at and so on

218
00:13:11,060 --> 00:13:14,170
so when we talk about is we developed this nice model

219
00:13:15,290 --> 00:13:16,890
and we see how it can be used for regression

220
00:13:17,330 --> 00:13:20,640
but now we start to pick different pieces of this we can see how it

221
00:13:20,640 --> 00:13:24,040
can be used for some different problems are reduced in different markets and different modeling

222
00:13:25,190 --> 00:13:28,560
okay so the first thing we might wanna mess around with in this model is

223
00:13:28,740 --> 00:13:30,520
very high is the model hyperparameters

224
00:13:30,960 --> 00:13:34,350
right what is that's model selection that's what we just talked about so there is one option

225
00:13:36,950 --> 00:13:37,620
the second option

226
00:13:38,780 --> 00:13:41,260
is we can mess around with the functional form of care for

227
00:13:42,540 --> 00:13:43,790
this kernel choice here

228
00:13:44,350 --> 00:13:47,550
we said that this is a canonical kernel choice squared exponential but there are plenty

229
00:13:47,550 --> 00:13:48,780
of others and we'll talk about those

230
00:13:49,250 --> 00:13:49,640
just now

231
00:13:51,750 --> 00:13:54,850
another thing we might wanna mess with this the cheapy itself the cheapy prior

232
00:13:55,670 --> 00:13:57,140
we're not going to talk about that here today

233
00:13:57,810 --> 00:14:02,320
because we're trying to talk about calcium processes but what should be aware that you

234
00:14:02,320 --> 00:14:07,370
can imagine having switching calcium processes or some other comp some other complicated system here

235
00:14:07,370 --> 00:14:09,010
than than just the basic cheapy

236
00:14:10,850 --> 00:14:11,510
and the fourth

237
00:14:12,050 --> 00:14:15,150
is the data distribution itself so what the data we receive

238
00:14:15,550 --> 00:14:19,580
what the data that we measure is not these nice continuous real valued variables but

239
00:14:19,580 --> 00:14:23,440
something different like a class label or something like this well then we can still

240
00:14:23,440 --> 00:14:26,540
use the same dupuy context we just have to change the likelihood function

241
00:14:27,680 --> 00:14:32,320
and we and we'll talk about that's something that some of the problems that there is a likelihood choices

242
00:14:33,690 --> 00:14:35,340
okay so let's start with kernel choices

243
00:14:41,060 --> 00:14:45,730
we talked about how the guassian process gives us this nice probability distribution over functions

244
00:14:47,900 --> 00:14:51,440
what the kernel is doing is it's helping us determine what kind of what kind

245
00:14:51,440 --> 00:14:52,640
of functions that we get from there

246
00:14:53,130 --> 00:14:56,760
there is this this this prior what sort of what type of functions we were

247
00:14:56,760 --> 00:14:59,360
drawing out this so the squared exponential kernel

248
00:14:59,840 --> 00:15:02,970
is the canonical kernel fore trying smooth

249
00:15:03,580 --> 00:15:06,170
dry smooth functions functions from a cheap e

250
00:15:06,310 --> 00:15:11,220
it's infinitely differentiable and we've seen these four draws ad nauseam at this point

251
00:15:12,820 --> 00:15:16,250
so another kernel that we might use is the rational quadratic

252
00:15:17,260 --> 00:15:20,170
i put this up here to show that that this is just the different this

253
00:15:20,170 --> 00:15:22,180
is just the different functional form that will give us some

254
00:15:22,850 --> 00:15:24,150
another interesting draws from a

255
00:15:24,970 --> 00:15:25,670
it's actually

256
00:15:26,350 --> 00:15:30,760
i don't don't worry about this too much but you see this term here is a squared exponential

257
00:15:31,950 --> 00:15:32,480
and in fact

258
00:15:32,880 --> 00:15:35,520
what this rational quadratic is actually

259
00:15:37,250 --> 00:15:38,650
of gamma weighted

260
00:15:39,510 --> 00:15:43,400
length scales so that the z here is is a gamma distribution over z

261
00:15:43,880 --> 00:15:44,880
hands this is

262
00:15:45,910 --> 00:15:47,900
this will be one one over the length scale

263
00:15:48,990 --> 00:15:52,080
in these in this in this question is also what this is doing is giving us

264
00:15:53,190 --> 00:15:54,700
a weighted sum of

265
00:15:55,400 --> 00:15:56,560
squared exponentials if you will

266
00:15:59,060 --> 00:16:01,670
and here four draws from that's and what you can see is there

267
00:16:02,420 --> 00:16:08,250
for a particular choice here there's some longer-range correlations then we might have seen in the squared exponential drop

268
00:16:08,510 --> 00:16:11,120
but also some some shorter wiggles as well

269
00:16:11,510 --> 00:16:12,700
so what this sick

270
00:16:13,180 --> 00:16:17,710
what is the point out is this just with a different form the kernel function we seem to be getting

271
00:16:18,330 --> 00:16:20,750
we're still getting nice smooth continuous draws

272
00:16:21,310 --> 00:16:24,810
but they have different different smoothness properties in different long-range properties

273
00:16:27,020 --> 00:16:31,100
one the questions i was asked fours about a periodic kernel so

274
00:16:31,670 --> 00:16:35,970
we've we've been talking about kernels that give us that give us these functions over time

275
00:16:36,410 --> 00:16:39,790
but what about what about a periodic function and so what you see here

276
00:16:40,310 --> 00:16:40,890
is that's

277
00:16:41,850 --> 00:16:43,820
we've we've added what looks like

278
00:16:45,050 --> 00:16:48,470
you know and this this this exponential term we just put it put a sign in here

279
00:16:49,680 --> 00:16:50,470
and what this gives us

280
00:16:50,880 --> 00:16:55,530
when we take draws from in the same in the same way so again we're just evaluating the kernel matrix

281
00:16:56,280 --> 00:16:59,210
and we're taking draws from guassian so here four draws from there

282
00:16:59,780 --> 00:17:02,910
and what you see here is that within a particular period

283
00:17:03,490 --> 00:17:06,470
this just looks like a draw from something like a squared exponential

284
00:17:07,690 --> 00:17:10,930
but now there's this extra property that periodic over time this is

285
00:17:11,950 --> 00:17:15,820
this is this is this cute but also this is something that is very useful fore

286
00:17:16,850 --> 00:17:18,210
let's say the heart rate example

287
00:17:19,080 --> 00:17:23,620
four r measuring weather patterns are something like this this is something that is actually that that's actually useful

288
00:17:27,450 --> 00:17:31,420
it up to now all the kernels even talking about are stationary kernel so so

289
00:17:31,810 --> 00:17:32,770
those of us who take taken

290
00:17:33,840 --> 00:17:38,550
some course in stochastic processes things like this remember you know about this notion of

291
00:17:38,550 --> 00:17:43,770
stationary kernels which is we've been talking about kernel functions as a function of two

292
00:17:43,770 --> 00:17:47,810
arguments but in reality all the functions that we've looked at so far are really

293
00:17:47,810 --> 00:17:49,150
on the other

294
00:17:49,190 --> 00:17:51,440
mention is the probabilistic language use

295
00:17:51,460 --> 00:17:54,940
so you could use something like bayesian networks markov networks like we just saw

296
00:17:54,960 --> 00:17:58,900
you could use some like for example probabilistic context free grammars or you know some

297
00:17:59,440 --> 00:18:00,560
more limited

298
00:18:00,580 --> 00:18:04,730
languages like say you know naive bayes which is a special case of these nodes

299
00:18:04,730 --> 00:18:06,400
and so on

300
00:18:06,420 --> 00:18:10,380
another dimension is the type of learning that you can do in this approach

301
00:18:10,540 --> 00:18:15,920
so the generative versus discriminative like just seeing the structure parameters you could have knowledge

302
00:18:15,920 --> 00:18:20,520
rich knowledge poor learning in principle you could do any of these types of learning

303
00:18:20,540 --> 00:18:22,150
with any approach

304
00:18:22,170 --> 00:18:26,940
in practice the not always available sometimes there's no particular reason for that sometimes there

305
00:18:26,960 --> 00:18:28,370
is a reason

306
00:18:28,380 --> 00:18:31,060
and then there's also the type of inference that you do

307
00:18:31,100 --> 00:18:35,210
so you can do any he versus marginal inference as we just saw

308
00:18:35,290 --> 00:18:40,690
another important dimensions along which is approach is very is how to handle ground

309
00:18:40,710 --> 00:18:42,850
you could ground that everything fully

310
00:18:42,940 --> 00:18:47,210
you could grant things partially that's actually the key idea not a small construction or

311
00:18:47,210 --> 00:18:49,020
you could even do lifted inference

312
00:18:49,040 --> 00:18:50,150
so again this

313
00:18:50,150 --> 00:18:53,600
this is another important dimension to to think about

314
00:18:53,670 --> 00:18:56,940
so let's start with the earliest example

315
00:18:56,940 --> 00:19:02,620
which is knowledge based model construction and actually encompasses quite a variety of approaches

316
00:19:02,620 --> 00:19:08,690
here the logical languages horn clauses and the probabilistic languages bayes nets

317
00:19:08,710 --> 00:19:09,940
and the basic idea

318
00:19:09,960 --> 00:19:12,960
on how to combine logic and probability here's is the following

319
00:19:12,960 --> 00:19:18,310
every ground atoms so i have a first order the world so i have relations

320
00:19:18,310 --> 00:19:22,400
i have predicted i have audits et cetera and i can construct patterns in the

321
00:19:22,400 --> 00:19:24,540
way that we just saw

322
00:19:24,540 --> 00:19:28,440
every possible run that is going to be a positive potential knows

323
00:19:28,440 --> 00:19:29,830
innovation network

324
00:19:29,850 --> 00:19:33,850
so so i have network over boolean variables each of which is one the termites

325
00:19:33,850 --> 00:19:35,420
in france and abroad

326
00:19:36,040 --> 00:19:40,940
and then what's going to happen is that i'm going to construct a bayesian network

327
00:19:40,940 --> 00:19:45,420
from my first order knowledge base that's what is called knowledge based model construction knowledge

328
00:19:45,420 --> 00:19:49,210
base is the more knowledge based on the model is the vision network

329
00:19:49,250 --> 00:19:52,810
and the way i'm going to start vision of what is the following

330
00:19:52,830 --> 00:19:55,830
the head of the clause is going to be the che child

331
00:19:55,850 --> 00:19:59,060
the body of the clause is going to be the turbulence

332
00:19:59,940 --> 00:20:01,370
and i i have

333
00:20:01,370 --> 00:20:06,250
and now that i have more than one clause with the same here

334
00:20:06,250 --> 00:20:09,460
i need some kind of combination function

335
00:20:09,480 --> 00:20:12,350
to combine the predictions from the various clauses

336
00:20:12,870 --> 00:20:14,790
so all the atoms

337
00:20:14,810 --> 00:20:16,400
in all the clauses

338
00:20:16,420 --> 00:20:17,480
have this

339
00:20:17,500 --> 00:20:19,960
as their heads are going be parents

340
00:20:19,980 --> 00:20:21,210
of the nodes

341
00:20:21,830 --> 00:20:26,120
each clause is the is a deterministic conjunction and now what i need is some

342
00:20:26,120 --> 00:20:27,440
kind of way

343
00:20:27,460 --> 00:20:32,560
to combine with the different classes and this combination function can be something like logistic

344
00:20:33,520 --> 00:20:34,830
or noisy or

345
00:20:34,850 --> 00:20:38,480
noisy or is the most commonly used one it basically says that the child is

346
00:20:39,370 --> 00:20:42,810
if one of the rules says it's true

347
00:20:42,830 --> 00:20:47,070
but that does affect the probability for each one there's also often some probably they

348
00:20:47,070 --> 00:20:51,100
might be true even if all of the current clauses

349
00:20:52,290 --> 00:20:55,880
so what happens is that when i have a query

350
00:20:55,900 --> 00:20:58,440
what i do is i do partial ground

351
00:20:58,460 --> 00:21:04,170
meaning i construct just the network that i need to answer the question

352
00:21:04,190 --> 00:21:08,650
which is good basically going to be composed of the prologue prove trees of the

353
00:21:08,650 --> 00:21:13,870
query node or nodes and all the evidence nodes you can prove that network containing

354
00:21:14,750 --> 00:21:18,510
has all the information that you need to the right probability i e you will

355
00:21:18,510 --> 00:21:22,850
give the same answers as if you for the whole network with all possible moves

356
00:21:22,850 --> 00:21:25,940
and of course a lot of the time it's going to be much much smaller

357
00:21:25,960 --> 00:21:30,870
OK so we do ground basically by doing all prologue

358
00:21:30,900 --> 00:21:35,940
and and then we can do something like per belief propagation over these networks or

359
00:21:35,960 --> 00:21:39,330
it could be another probabilistic inference method but that is you know at this point

360
00:21:39,330 --> 00:21:43,690
i just have a vision that was so i can use anything belief propagation MCMC

361
00:21:43,690 --> 00:21:44,730
et cetera

362
00:21:44,790 --> 00:21:48,830
so to learn the structure of these networks we can use ILP

363
00:21:48,830 --> 00:21:52,350
you can you could learn a lot like this just by running any of the

364
00:21:52,350 --> 00:21:56,440
system taking its output and saying it's the model not course you also need to

365
00:21:56,440 --> 00:22:00,690
learn the parameters to learn the parameters often use something like the

366
00:22:00,750 --> 00:22:05,020
and the reason is something like this that's typically you will not have observed a

367
00:22:05,020 --> 00:22:06,730
lot of the intermediates

368
00:22:06,730 --> 00:22:12,150
predicates in your inference is that everything you can impose for otherwise you need to

369
00:22:12,150 --> 00:22:13,620
do something like you

370
00:22:13,620 --> 00:22:18,920
they typically people do not have resources such as the is for closing pure ILP

371
00:22:19,020 --> 00:22:22,920
and then you just the parameters using here which is actually not quite the right

372
00:22:22,920 --> 00:22:26,230
thing to do if you think about it you actually like to learn at each

373
00:22:26,230 --> 00:22:31,980
point learning the clauses using your ultimate like with the objective function

374
00:22:31,980 --> 00:22:34,000
so that's what this model construction

375
00:22:34,020 --> 00:22:38,870
the oldest and probably most very family of these approaches

376
00:22:38,900 --> 00:22:40,100
here's another one

377
00:22:40,100 --> 00:22:44,920
stochastic logic programs here the logical languages again when clauses

378
00:22:45,020 --> 00:22:48,810
the probabilistic language is not probabilistic context free grammars

379
00:22:48,830 --> 00:22:53,370
and for stochastic logic programs can just be seen as listing PCFG to the first

380
00:22:53,370 --> 00:22:57,270
order level by turning production rules into horn clauses

381
00:22:57,520 --> 00:23:01,040
so here's what we do is we take the prolog program

382
00:23:01,060 --> 00:23:04,420
and we just attach a probability to each class

383
00:23:04,520 --> 00:23:05,980
with the restriction

384
00:23:06,040 --> 00:23:10,880
that the sum of the probabilities of course with the same has to be one

385
00:23:10,900 --> 00:23:12,670
and the semantics of this is that

386
00:23:12,690 --> 00:23:16,400
they that had is produced as one of its bodies we just don't know which

387
00:23:16,400 --> 00:23:18,650
but we have a probability distribution over them

388
00:23:18,870 --> 00:23:24,600
so unless we really gives you is a probability distribution over prolog prove trees

389
00:23:24,600 --> 00:23:27,310
so if then you want to do inference then we have to do is you

390
00:23:27,310 --> 00:23:31,330
just look at the proof start of whatever could you have to have used should

391
00:23:31,330 --> 00:23:32,640
have to do all proves

392
00:23:32,750 --> 00:23:36,620
but problem can do this for you what has to happen is that you need

393
00:23:36,620 --> 00:23:38,420
to keep track of the probabilities

394
00:23:38,420 --> 00:23:41,850
meaning as you go down the tree need to multiply the probabilities and then you

395
00:23:41,850 --> 00:23:44,710
need to add the probabilities of all of all the trees

396
00:23:44,900 --> 00:23:48,210
and this this give the probability of you create

397
00:23:48,300 --> 00:23:52,210
they again to do learning we can use ILP to the structure learning

398
00:23:52,270 --> 00:23:58,040
to do to learn parameters we can learn with a small record which is

399
00:23:58,060 --> 00:23:58,560
there year

400
00:23:58,600 --> 00:24:01,580
it's to be what's called just

401
00:24:01,640 --> 00:24:03,810
what i mean by just o

402
00:24:03,830 --> 00:24:05,900
not all succeed

403
00:24:06,730 --> 00:24:10,900
but some of the prospect succeed by this scheme also has some probability

404
00:24:10,900 --> 00:24:13,770
so what i need to do is to renormalize

405
00:24:13,810 --> 00:24:17,310
the partition function in slps is not one

406
00:24:17,360 --> 00:24:21,730
like vision networks so need to to adjust my you are not willing to take

407
00:24:21,790 --> 00:24:25,750
into account e i need to normalize to subtract out

408
00:24:26,350 --> 00:24:30,060
the probability mass of the precipitate

409
00:24:30,060 --> 00:24:31,480
here's another

410
00:24:31,480 --> 00:24:33,850
probabilistic relational models

411
00:24:33,870 --> 00:24:37,140
a lot of work was done in this over the last several years

412
00:24:37,150 --> 00:24:39,850
here the logical languages frame systems

413
00:24:39,870 --> 00:24:44,770
you have classes of objects and for each class of objects you have a canonical

414
00:24:44,770 --> 00:24:48,750
description of the arctic meaning the attributes of the objects of that class and the

415
00:24:48,750 --> 00:24:52,150
relations of the objects of that class to other classes

416
00:24:52,270 --> 00:24:56,060
the probabilistic language is once again bayesian networks

417
00:24:56,080 --> 00:24:57,730
and the idea is that

418
00:24:57,730 --> 00:25:00,750
we don't have differing systems where for each class

419
00:25:00,750 --> 00:25:01,600
we have

420
00:25:01,620 --> 00:25:04,140
a bayesian network that describes

421
00:25:04,150 --> 00:25:09,000
the joint distribution of the attributes of that of the objects of that class

422
00:25:09,100 --> 00:25:13,790
and this is the relational model because the attributes of an object in addition to

423
00:25:14,020 --> 00:25:18,140
extending in an attribute of an object in addition to being able to depend on

424
00:25:18,140 --> 00:25:19,620
multi CPU

425
00:25:19,640 --> 00:25:24,500
computer was very rare at the and the

426
00:25:26,730 --> 00:25:30,850
so you need you need to to use some error to get into a situation

427
00:25:30,850 --> 00:25:33,580
when you run a number of CPU

428
00:25:34,630 --> 00:25:39,240
i think that have much more mild CPU

429
00:25:39,250 --> 00:25:42,230
machine that ordinary one

430
00:25:43,100 --> 00:25:45,500
in the future i think that

431
00:25:45,610 --> 00:25:50,610
usually you have got to be used in every country

432
00:25:50,620 --> 00:25:52,340
but we start from

433
00:25:52,380 --> 00:25:53,620
from memory

434
00:25:53,630 --> 00:25:57,080
the good thing here is that

435
00:25:58,690 --> 00:26:04,320
nearly all hardware that we can use

436
00:26:04,690 --> 00:26:08,650
it has now the same memory hierarchy

437
00:26:09,370 --> 00:26:11,650
in this little

438
00:26:11,660 --> 00:26:15,200
in ordinary workstation

439
00:26:15,220 --> 00:26:19,830
or in the data centre is specially designed to our

440
00:26:19,890 --> 00:26:21,100
it seem

441
00:26:21,110 --> 00:26:24,240
the same hierarchy of memory

442
00:26:24,250 --> 00:26:27,860
the most cave commemorate is CPU register

443
00:26:27,900 --> 00:26:33,380
how many to come when you does seem to have

444
00:26:33,390 --> 00:26:40,610
does anybody know

445
00:26:40,620 --> 00:26:50,100
what does it mean basic

446
00:26:50,120 --> 00:26:56,270
then of course the case common

447
00:26:56,820 --> 00:27:04,120
three candidates the

448
00:27:04,170 --> 00:27:05,420
it's not so

449
00:27:05,450 --> 00:27:07,990
there is that there is not is stop

450
00:27:08,000 --> 00:27:11,750
you can you can to answer this question exactly because

451
00:27:11,800 --> 00:27:13,350
there are

452
00:27:13,370 --> 00:27:18,550
i think that's right now in production there are about sixteen models and all of

453
00:27:18,550 --> 00:27:20,120
them have

454
00:27:20,160 --> 00:27:23,460
different number of y

455
00:27:23,500 --> 00:27:25,250
i hope it's not really

456
00:27:25,280 --> 00:27:29,090
now it's usually called like super

457
00:27:29,130 --> 00:27:36,170
random access memory or process random access memory and the idea is that something in

458
00:27:37,420 --> 00:27:38,860
which is the

459
00:27:39,000 --> 00:27:44,780
link wired together with arithmetic or logical you need and all other parts of the

460
00:27:46,530 --> 00:27:49,360
what we know about the memory that we have

461
00:27:49,400 --> 00:27:52,120
a small amount of this memory on the

462
00:27:52,130 --> 00:27:54,140
congress white

463
00:27:54,210 --> 00:27:56,120
but it's extremely fast

464
00:27:56,120 --> 00:27:59,580
it's processes are running on you go by

465
00:27:59,590 --> 00:28:05,000
so it's it's not a second story this memory is not less than on the

466
00:28:05,010 --> 00:28:07,020
second is very fine

467
00:28:07,830 --> 00:28:09,220
all one

468
00:28:09,260 --> 00:28:12,050
then all modern process so that they

469
00:28:12,080 --> 00:28:14,220
have CPU cache

470
00:28:14,230 --> 00:28:19,350
CPU cache is fast memory that is also detached this process

471
00:28:19,360 --> 00:28:21,600
but i think that usually

472
00:28:21,700 --> 00:28:24,650
people don't know anything about CPU cache

473
00:28:24,660 --> 00:28:27,640
only only what we know about it is that

474
00:28:27,700 --> 00:28:30,710
for unknown reasons people who are doing it

475
00:28:30,720 --> 00:28:32,880
computer hardware market

476
00:28:32,940 --> 00:28:35,590
for this number one box

477
00:28:35,630 --> 00:28:38,470
so you still have to with

478
00:28:38,480 --> 00:28:45,160
two megabytes of cash four megabytes of passion and so on and so forth

479
00:28:45,170 --> 00:28:49,550
this is an important number for all purposes and i think that it's not so

480
00:28:49,550 --> 00:28:53,590
important for ordinary people who are blind

481
00:28:53,630 --> 00:28:58,460
laptop for office applications because it's not so good

482
00:28:58,540 --> 00:29:02,000
so this this number is not so important for them

483
00:29:02,110 --> 00:29:07,600
but again was good with this CPU cache because his why leslie two

484
00:29:07,610 --> 00:29:08,620
two there

485
00:29:08,620 --> 00:29:11,080
central process you need

486
00:29:11,090 --> 00:29:12,970
it's pretty fast

487
00:29:12,990 --> 00:29:16,610
and you can always take abou that describe this

488
00:29:16,640 --> 00:29:18,390
hardware and say

489
00:29:18,400 --> 00:29:20,250
that you need

490
00:29:20,300 --> 00:29:24,740
for example half of second to get data from the cache

491
00:29:24,760 --> 00:29:28,500
you can always predict how when operations you need to get the data

492
00:29:28,580 --> 00:29:31,740
so talking about complexity when we talk about when we

493
00:29:31,740 --> 00:29:34,330
describe the number of operations

494
00:29:34,370 --> 00:29:36,990
for these two types of memory

495
00:29:37,030 --> 00:29:39,200
you always can

496
00:29:40,630 --> 00:29:42,410
how long will take

497
00:29:42,420 --> 00:29:45,120
computational this

498
00:29:45,120 --> 00:29:47,480
if you have such a good collection

499
00:29:47,490 --> 00:29:48,790
that actually

500
00:29:48,820 --> 00:29:49,860
fits into

501
00:29:49,870 --> 00:29:51,530
two megabytes of

502
00:29:51,580 --> 00:29:54,800
to be married imagine the situation

503
00:29:55,710 --> 00:29:57,250
you can

504
00:29:57,260 --> 00:30:01,230
precisely calculate how many times

505
00:30:01,240 --> 00:30:04,970
will take colocalisation indexing of the collection

506
00:30:05,010 --> 00:30:06,320
because you know

507
00:30:08,100 --> 00:30:11,370
for all other types of memory true

508
00:30:11,380 --> 00:30:15,250
it it could be surprised but when you are accessing two

509
00:30:15,270 --> 00:30:17,070
random access memory in your

510
00:30:19,020 --> 00:30:22,350
you see different numbers

511
00:30:22,450 --> 00:30:26,570
you cannot predict in advance how long it will take to get quite from around

512
00:30:26,570 --> 00:30:27,950
the memory

513
00:30:27,970 --> 00:30:29,730
when going on

514
00:30:29,770 --> 00:30:31,640
the right number three

515
00:30:31,700 --> 00:30:36,190
the reason CPU cache because CPU cache actually

516
00:30:36,200 --> 00:30:42,510
campaign lots of random access memory it can simply

517
00:30:42,510 --> 00:30:47,370
because it is real-time right here and it's one of the principles of self organizing

518
00:30:48,660 --> 00:30:50,950
recall the first slide was

519
00:30:50,970 --> 00:30:52,890
let's look at the idea that

520
00:30:52,930 --> 00:30:56,010
this is our spontaneous

521
00:30:56,050 --> 00:31:00,620
in real time dynamically driven organization

522
00:31:00,680 --> 00:31:04,450
so we don't

523
00:31:04,620 --> 00:31:11,780
at that activity happens state is kind of like an active and there's whole world

524
00:31:11,800 --> 00:31:14,350
knowledge and we use that word

525
00:31:14,370 --> 00:31:16,050
in the state so that

526
00:31:16,090 --> 00:31:17,870
that that

527
00:31:17,890 --> 00:31:23,140
what is not in memory states does

528
00:31:23,490 --> 00:31:27,120
and yet

529
00:31:27,180 --> 00:31:28,390
then there

530
00:31:28,410 --> 00:31:29,680
the you've got

531
00:31:29,700 --> 00:31:30,950
another direction

532
00:31:30,970 --> 00:31:32,720
but you don't remember you

533
00:31:33,800 --> 00:31:35,490
i to

534
00:31:35,550 --> 00:31:37,910
the second thing you need

535
00:31:37,950 --> 00:31:39,050
the they

536
00:31:40,510 --> 00:31:41,450
the direction

537
00:31:41,470 --> 00:31:43,750
the more you know

538
00:31:44,350 --> 00:31:47,180
five minutes

539
00:31:47,220 --> 00:31:48,680
they don't

540
00:31:48,720 --> 00:31:51,300
this is actually a

541
00:31:57,470 --> 00:32:01,240
you the fact that is we

542
00:32:02,780 --> 00:32:05,950
yes that's exactly the problem

543
00:32:06,010 --> 00:32:11,300
is that generally OK there's two ways basically that we know about the ones from

544
00:32:11,300 --> 00:32:15,140
inside just from interacting so for example you could

545
00:32:15,160 --> 00:32:18,160
because the person sometimes you have life events

546
00:32:20,100 --> 00:32:25,910
completely reorganized the way of the world where you are i injury for example physical

547
00:32:25,930 --> 00:32:29,660
injury he completely reorganized the way that i actually know

548
00:32:29,740 --> 00:32:31,050
that's the next

549
00:32:31,070 --> 00:32:36,100
motivation but if required adaptation from the inside for me to be able to sustain

550
00:32:36,100 --> 00:32:38,180
the organization OK

551
00:32:38,240 --> 00:32:45,450
right from the more concrete they have full neurons their interaction with the dancing

552
00:32:47,140 --> 00:32:53,120
well there are going

553
00:32:53,240 --> 00:32:57,100
so this is a very beautiful neurons

554
00:32:57,100 --> 00:33:01,070
the interacting particle that your

555
00:33:04,640 --> 00:33:09,050
well not to

556
00:33:09,070 --> 00:33:15,050
you are the one is should be looking at the current affairs programme where should

557
00:33:15,070 --> 00:33:17,890
we modeling things in the world neurons

558
00:33:17,890 --> 00:33:23,460
in things inside the head or do we do new kinds of which is happening

559
00:33:23,460 --> 00:33:24,510
which is

560
00:33:24,530 --> 00:33:28,420
what's happening outside is as important and the is teenagers

561
00:33:28,510 --> 00:33:30,570
red should be

562
00:33:30,590 --> 00:33:33,870
so why can't the neuron structure

563
00:33:35,390 --> 00:33:40,780
all i have a lot to me

564
00:33:41,030 --> 00:33:43,950
the structure of sentence

565
00:33:45,200 --> 00:33:50,390
there are people have different ideas and reviews for this to decide which of the

566
00:33:50,390 --> 00:33:53,890
modelling ideas or to play with in order to look

567
00:33:53,890 --> 00:34:00,590
and social mission that is common to see confirmation connections modelling

568
00:34:00,600 --> 00:34:07,120
we have to develop connections model of collective cognition

569
00:34:07,220 --> 00:34:08,800
by bias now back to

570
00:34:08,830 --> 00:34:11,180
we need

571
00:34:12,280 --> 00:34:13,680
a group of people

572
00:34:13,700 --> 00:34:17,570
could not necessarily have a purpose from within

573
00:34:17,600 --> 00:34:19,800
but immediate purpose that is

574
00:34:19,930 --> 00:34:22,760
imposed from the outside or results

575
00:34:22,850 --> 00:34:26,490
from from the so-called outside

576
00:34:26,490 --> 00:34:29,920
in a similar way i have obviously talked about super advanced things but this of

577
00:34:29,920 --> 00:34:34,670
all things and i want to talk about just introduce a couple of the concepts

578
00:34:34,670 --> 00:34:36,510
of unsupervised learning

579
00:34:37,380 --> 00:34:39,170
in unsupervised learning

580
00:34:39,180 --> 00:34:43,060
i'm going to split into just two areas for the moment which i think you

581
00:34:43,060 --> 00:34:47,490
could certainly do more but i'd like to mention briefly the idea of clustering

582
00:34:47,500 --> 00:34:51,440
and dimensionality reduction so we'll start with clustering and i just want to show the

583
00:34:51,440 --> 00:34:54,180
simple k means clustering algorithms

584
00:34:54,210 --> 00:34:55,470
well basically

585
00:34:55,490 --> 00:34:59,590
you don't have now you've got your data and you want to categorize it without

586
00:35:01,060 --> 00:35:04,040
you don't have any labels or anything like that so the way to deal with

587
00:35:04,040 --> 00:35:10,300
that is one example is the cluster and humans doing this all the time so

588
00:35:10,310 --> 00:35:11,560
for example we

589
00:35:11,590 --> 00:35:17,620
class two different animal species into different groups so you think we say it before

590
00:35:17,620 --> 00:35:21,610
we knew the genetic different we say all those those rule frogs they share similar

591
00:35:21,610 --> 00:35:27,310
characteristics we can get confused with things like duck-billed platypus if it doesn't fit into

592
00:35:27,310 --> 00:35:31,210
one group or another by classification

593
00:35:31,240 --> 00:35:35,810
so we like to do discrete categorisation thing so

594
00:35:35,860 --> 00:35:39,800
at the moment because it's election day in the UK a different political parties i

595
00:35:39,800 --> 00:35:44,190
would think it's very funny that you you are allowed to have for example in

596
00:35:44,190 --> 00:35:50,250
britain any one more than one of three political different political opinions because those are

597
00:35:50,300 --> 00:35:52,420
the only three for on the states you only have two

598
00:35:52,860 --> 00:35:58,100
one of two political opinions bits of the clustering of many different opinions people talk

599
00:35:58,100 --> 00:36:01,110
about even being on the right to left so i think when people talk about

600
00:36:01,110 --> 00:36:04,630
being on the right to the left the talking about i dimensionality reduction yes so

601
00:36:04,660 --> 00:36:09,210
your further to the right or your further to the left politically that there seems

602
00:36:09,220 --> 00:36:13,000
to be more to politics in the one-dimensional dimensionality reduction but if you then go

603
00:36:13,000 --> 00:36:17,510
beyond the dimensionality reduction you say your left-wing socialists so then that

604
00:36:17,540 --> 00:36:20,790
clustering you're right wing your

605
00:36:20,800 --> 00:36:22,800
conservative so

606
00:36:22,900 --> 00:36:26,800
that's another sort of classification so you know we obviously like to these things even

607
00:36:26,800 --> 00:36:32,610
when underlying people actually generally probably have a continuum of opinions and very little different

608
00:36:32,620 --> 00:36:36,060
ways so

609
00:36:36,090 --> 00:36:40,210
the idea of clustering to some allocation to the groups and much harder but perhaps

610
00:36:40,210 --> 00:36:43,790
more interesting the number of different groups i will be talking about that this be

611
00:36:43,790 --> 00:36:46,120
showing this is simple

612
00:36:46,290 --> 00:36:49,100
k means clustering so

613
00:36:49,110 --> 00:36:53,690
for example we have a set k cluster centers so in k means clustering what

614
00:36:53,690 --> 00:36:58,190
we require is a set of k cluster centers on an assignment of data to

615
00:36:58,190 --> 00:37:02,650
each of the cluster centers so what we do there is we can initialize perhaps

616
00:37:02,650 --> 00:37:06,670
each cluster centres as the data points so we could start we have to some

617
00:37:06,710 --> 00:37:11,300
starting idea about cluster centers and one initialisation would be just to take cluster centers

618
00:37:11,300 --> 00:37:15,680
the data points and assign each data point to the nearest cluster centre then we

619
00:37:15,680 --> 00:37:20,170
can update each cluster centre by setting to the mean of the assign datapoints so

620
00:37:20,170 --> 00:37:23,980
here is objective that we're minimising their so

621
00:37:23,990 --> 00:37:27,590
if we were if is cluster centers and we have k of them

622
00:37:27,600 --> 00:37:30,110
then we got why is data

623
00:37:30,130 --> 00:37:33,560
we basically look at all the data that is allocated to one of the cluster

624
00:37:33,560 --> 00:37:36,720
centers and we compute the say the squared

625
00:37:36,730 --> 00:37:40,910
distance between across the center and the data is allocated to it we some all

626
00:37:40,920 --> 00:37:41,860
of that

627
00:37:41,870 --> 00:37:45,810
and that's basic model of what's going on in clustering so we're minimizing the sum

628
00:37:45,810 --> 00:37:49,610
of euclidean squared distances between points and their associated centres

629
00:37:49,620 --> 00:37:53,630
so this minimum is guaranteed to be globally unique to the other stuff we doing

630
00:37:53,630 --> 00:37:58,960
so far you're actually looking for global and unique minimum but this is i think

631
00:37:58,960 --> 00:38:02,960
this is much more interesting case is worrying i think if your memories global and

632
00:38:02,960 --> 00:38:06,390
unique it says that you may be doing something too simple although lots of people

633
00:38:06,470 --> 00:38:11,480
disagree with that so the objective is non convex optimisation problem but the algorithm works

634
00:38:11,480 --> 00:38:14,220
very well in in lots of cases some data

635
00:38:14,230 --> 00:38:15,980
i've artificially generated

636
00:38:15,990 --> 00:38:18,250
and it's sort of an

637
00:38:18,260 --> 00:38:21,180
well we might see it was actually that was

638
00:38:21,220 --> 00:38:23,100
potentially three clusters that

639
00:38:23,140 --> 00:38:28,570
and the way the algorithm works if we initialize cluster centers by sampling randomly from

640
00:38:28,570 --> 00:38:31,630
the data we happen to sample two sensors

641
00:38:31,650 --> 00:38:34,990
two data points in one cluster one data point in another

642
00:38:35,040 --> 00:38:36,610
we allocate

643
00:38:37,260 --> 00:38:42,040
data points to nearest cluster centre so here we've got three all these blue

644
00:38:42,060 --> 00:38:43,680
the nearest cluster centre

645
00:38:43,710 --> 00:38:44,880
is this one here

646
00:38:44,890 --> 00:38:46,610
so already we've got

647
00:38:46,730 --> 00:38:47,460
sort of

648
00:38:47,480 --> 00:38:49,190
identify cluster here

649
00:38:49,200 --> 00:38:50,750
the red ones

650
00:38:50,760 --> 00:38:54,360
all these part of these guys down here allocated to this one part of the

651
00:38:54,360 --> 00:39:00,290
relegated here then we update by setting and cost centre to be the mean

652
00:39:00,360 --> 00:39:05,830
of the points currently existing and that moves and then and then we reallocate so

653
00:39:05,830 --> 00:39:08,490
now we've got to guide in this cluster allocated to here

654
00:39:08,500 --> 00:39:11,870
but the rest relegated here we do again reallocate

655
00:39:11,930 --> 00:39:16,260
and we do it again and now none of the allocations changing we converge so

656
00:39:16,260 --> 00:39:21,610
that's k means clustering which is perhaps the simplest clustering algorithm you could envisage it's

657
00:39:21,610 --> 00:39:27,230
got several limitations in the distance is being measured just in terms of euclidean distance

658
00:39:27,230 --> 00:39:32,400
of this perceptually if we had

659
00:39:37,230 --> 00:39:38,370
an example

660
00:39:38,390 --> 00:39:41,510
where we had a bunch of data in the middle here and this is the

661
00:39:41,510 --> 00:39:46,100
famous example people have used to demonstrate other algorithms and then you've got a bunch

662
00:39:46,100 --> 00:39:47,610
of data entering outside here

663
00:39:48,020 --> 00:39:52,230
this algorithm would work because there's no cluster centre

664
00:39:52,270 --> 00:39:55,830
that all these guys are closer to here

665
00:39:55,840 --> 00:39:59,430
then they would be to hear so what you end up doing there is if

666
00:39:59,430 --> 00:40:02,900
you wanted to cluster them also these were in different cluster to v these you

667
00:40:02,900 --> 00:40:07,850
would have to have cluster centers going all the way around

668
00:40:07,890 --> 00:40:11,850
and i and i think very impressive a again i remember being very impressed when

669
00:40:11,850 --> 00:40:13,750
it comes that came out was

670
00:40:13,760 --> 00:40:18,780
papers on spectral clustering which do allow you to to clusterings like this and what

671
00:40:18,780 --> 00:40:21,280
they do is clever transformations on the data

672
00:40:21,310 --> 00:40:25,650
again using sort basis function like approaches and they do k means clustering in a

673
00:40:25,650 --> 00:40:28,780
different space one of the popular algorithms does that

674
00:40:28,880 --> 00:40:33,080
where you can actually cluster these things that are connected so that shows one of

675
00:40:33,080 --> 00:40:38,190
the things that's interesting about what you might think is your impression what clustering is

676
00:40:38,190 --> 00:40:44,220
of how things should cluster and one algorithm two when you find an objective function

677
00:40:44,230 --> 00:40:50,820
so it's perhaps dangerous you define an objective function like this

678
00:40:50,840 --> 00:40:52,820
and it seems reasonable

679
00:40:52,840 --> 00:40:58,500
but the way functions isn't necessarily how you might expect clustering to function because really

680
00:40:58,500 --> 00:41:00,250
what goes on i think

681
00:41:00,270 --> 00:41:04,440
clustering you're not just looking at how close things are but you're looking at whether

682
00:41:04,440 --> 00:41:07,730
there gaps so when you look clustering you probably

683
00:41:07,730 --> 00:41:10,660
using these are all the data points are close to each other on this outer

684
00:41:10,660 --> 00:41:14,280
ring here so they belong together and then there's a gap to these data points

685
00:41:14,280 --> 00:41:18,850
here see also interested in ensuring that the gap between these different points where the

686
00:41:18,850 --> 00:41:23,950
algorithm doesn't doesn't do that so you know it's an example where the mathematical formalism

687
00:41:23,950 --> 00:41:25,260
is slightly wrong

688
00:41:25,280 --> 00:41:28,500
spectral clustering is away

689
00:41:28,510 --> 00:41:29,570
out of that

690
00:41:30,620 --> 00:41:34,640
one way of trying to select the number of clusters a lot of people have

691
00:41:34,640 --> 00:41:38,710
been looking at is say things like the recently processes

692
00:41:38,750 --> 00:41:43,100
which are probabilistic approaches to clustering which try and consider

693
00:41:43,110 --> 00:41:49,530
prior distributions to say that the could be infinite classes is i mean i won't

694
00:41:49,530 --> 00:41:51,510
i go backwards and forwards forever

695
00:41:51,530 --> 00:41:53,650
if i want to generate a completely unbiased sample

696
00:41:53,670 --> 00:41:56,130
that's definitely a boltzmann machine, and it's tedious

697
00:41:56,150 --> 00:41:57,800
once i've done that

698
00:41:57,820 --> 00:42:01,630
i then take this binary state that i've got here and go chunk chunk

699
00:42:01,670 --> 00:42:04,710
and that's an unbiased sample from the composite model

700
00:42:04,760 --> 00:42:08,490
so the composite model is not a big boltzmann machine

701
00:42:08,530 --> 00:42:12,070
the composite model is

702
00:42:12,090 --> 00:42:13,590
a restricted boltzmann machine

703
00:42:13,590 --> 00:42:17,730
and then deep belief net stuff as many layers as you like

704
00:42:17,750 --> 00:42:21,670
so what we've got now is a way churning out lots of layers of deep belief

705
00:42:21,670 --> 00:42:22,820
net stuff

706
00:42:22,880 --> 00:42:26,920
by learning this undirected stuff in the other direction

707
00:42:26,920 --> 00:42:29,590
it's kind of, not the kind of thing you'd have thought of doing.

708
00:42:29,650 --> 00:42:33,400
we just started doing this and then realized what we were doing later

709
00:42:33,670 --> 00:42:37,920
you wouldn't have thought of learning lots of layers of

710
00:42:37,960 --> 00:42:39,920
directed stuff

711
00:42:39,980 --> 00:42:51,490
in the wrong direction using undirected stuff you'll see later why that's a good idea

712
00:42:52,880 --> 00:42:58,150
i'm now going to go into some slightly complicated math but then we're gonna have

713
00:42:58,150 --> 00:42:59,440
a nice example

714
00:42:59,940 --> 00:43:06,610
i'm gonna to try and justify what the greedy learning is up to, and why it works

715
00:43:06,610 --> 00:43:09,510
why it can learn a layer at a time like this

716
00:43:09,570 --> 00:43:13,650
first i need to get some, sort of, properties of factorial distributions straight.

717
00:43:14,440 --> 00:43:16,420
given the visible units

718
00:43:16,460 --> 00:43:21,280
the distribution across the hidden units is factorial, that is, they're independent,

719
00:43:21,340 --> 00:43:24,460
the hidden units, given the visible units.

720
00:43:24,480 --> 00:43:26,730
the posterior of the hiddens

721
00:43:26,780 --> 00:43:32,550
but if i now add up those posterior distributions, or rather, average them over many different data

722
00:43:33,780 --> 00:43:37,030
what i get is something that isn't factorial

723
00:43:37,050 --> 00:43:39,490
just like when i take a mixture of gaussians, i get something that isn't a

724
00:43:40,940 --> 00:43:45,320
here when i average these factorial distributions i get something non-factorial

725
00:43:45,360 --> 00:43:49,170
i'll call that thing that i get the aggregated posterior

726
00:43:49,190 --> 00:43:52,150
it's the posterior distribution over the hidden units,

727
00:43:52,210 --> 00:44:00,170
aggregated over all the data vectors.

728
00:44:00,230 --> 00:44:04,670
here's one picture of what greedy learning is doing

729
00:44:04,800 --> 00:44:06,820
we're learning some weights,

730
00:44:06,820 --> 00:44:09,570
that connect hidden units to visible units

731
00:44:09,610 --> 00:44:13,070
and these weights are doing two things for us

732
00:44:13,090 --> 00:44:15,130
the weights definitely define,

733
00:44:15,170 --> 00:44:16,880
given a hidden vector

734
00:44:16,940 --> 00:44:20,280
what the probability is of turning on each of the visible units.

735
00:44:20,280 --> 00:44:23,610
so given a distribution over hidden vectors,

736
00:44:23,610 --> 00:44:26,960
they define a distribution over visible vectors.

737
00:44:26,980 --> 00:44:30,670
in particular given the aggregated posterior distribution

738
00:44:30,710 --> 00:44:33,030
they define some distribution for the data

739
00:44:33,090 --> 00:44:35,460
which won't be quite right.

740
00:44:38,050 --> 00:44:40,360
so this is a kind of parametric model that,

741
00:44:40,440 --> 00:44:42,250
given this aggregated posterior,

742
00:44:44,800 --> 00:44:47,570
a distribution for the data.

743
00:44:47,730 --> 00:44:50,050
the very same weights

744
00:44:50,110 --> 00:44:53,820
also define a prior over the hidden things

745
00:44:53,840 --> 00:44:58,820
so there's two things that are different. one is the aggregated posterior

746
00:44:58,820 --> 00:45:01,280
that you get by putting in data

747
00:45:01,280 --> 00:45:04,210
and looking at the states of the hidden units

748
00:45:04,210 --> 00:45:07,980
a separate thing is the prior that you have for this,

749
00:45:07,990 --> 00:45:10,650
defined by these very same weights

750
00:45:10,650 --> 00:45:14,570
and the way the prior is defined is that, suppose i took a boltzmann machine with these weights

751
00:45:14,570 --> 00:45:18,050
here, and just went backwards and forwards for a long time and then sampled this

752
00:45:18,090 --> 00:45:20,490
that would be a sample from this prior.

753
00:45:20,530 --> 00:45:21,940
notice i didn't mention data when i did that

754
00:45:21,960 --> 00:45:25,300
that is just what the model would like to put here

755
00:45:25,340 --> 00:45:30,320
if it's a perfect model, of course, it'll be the same as this aggregated posterior

756
00:45:31,210 --> 00:45:33,940
these weights are playing two roles

757
00:45:33,980 --> 00:45:36,480
if we view it in a directed kind of way

758
00:45:36,530 --> 00:45:38,550
they're defining

759
00:45:38,610 --> 00:45:41,170
a prior over this stuff

760
00:45:41,230 --> 00:45:42,730
and they're defining

761
00:45:42,780 --> 00:45:44,420
the probability of this

762
00:45:44,440 --> 00:45:46,150
given these states

763
00:45:46,190 --> 00:45:48,570
so we could generate from the model by going backwards and forwards for a long

764
00:45:48,570 --> 00:45:50,570
time then sampling from the prior, and then

765
00:45:50,610 --> 00:45:54,440
doing one small sampling here

766
00:45:54,530 --> 00:45:57,980
and the trick of the deep learning is going to be to say

767
00:45:58,010 --> 00:46:00,380
we're gonna keep this bit

768
00:46:00,460 --> 00:46:03,610
we're gonna allow the W's to keep defining that

769
00:46:03,630 --> 00:46:06,460
but for this bit, we're gonna replace them by something else. we're gonna take this

770
00:46:06,460 --> 00:46:10,110
term, throw it away, and replace it by something better

771
00:46:10,110 --> 00:46:12,820
which is what we're going to get when we treat this as data and learn another boltzmann machine

772
00:46:12,820 --> 00:46:15,710
with different weights up here.

773
00:46:16,610 --> 00:46:18,880
this is tough stuff to get the hang of

774
00:46:19,650 --> 00:46:22,610
it's going to get easier later.

775
00:46:22,630 --> 00:46:26,360
so a way of saying that in an equation is that the probability of a visible vector

776
00:46:26,380 --> 00:46:28,630
is the sum over all hidden states

777
00:46:28,730 --> 00:46:32,980
of the probability of generating that hidden vector from the model

778
00:46:33,030 --> 00:46:37,210
times the probability of the visible vector given the hidden vector

779
00:46:37,250 --> 00:46:40,920
so that defines the probability of a visible vector. and if you've already learned the weights

780
00:46:40,920 --> 00:46:42,980
that define both these terms

781
00:46:42,990 --> 00:46:46,440
then what you could do is leave this term alone and go off and improve that term, and you'll

782
00:46:46,460 --> 00:46:48,530
get a better model for the data

783
00:46:48,590 --> 00:46:56,280
and that's the trick of deep learning.

784
00:46:56,340 --> 00:47:00,780
just to sort of keep you straight about what's factorial and what's not:

785
00:47:00,800 --> 00:47:02,690
in a directed belief net,

786
00:47:02,710 --> 00:47:05,820
like the earthquake example,

787
00:47:05,820 --> 00:47:13,130
a bit

788
00:47:13,160 --> 00:47:15,270
OK let's get started now

789
00:47:15,280 --> 00:47:21,500
i'm assuming i'm assuming that a UN to recitation yesterday

790
00:47:21,520 --> 00:47:26,890
the that even if you didn't you know how to separate variables and you know

791
00:47:27,210 --> 00:47:29,520
how to construct simple models

792
00:47:29,550 --> 00:47:35,660
of sort physical problems with differential equations possibly even sell them

793
00:47:35,660 --> 00:47:38,770
so you should learn that either in high school

794
00:47:38,770 --> 00:47:40,480
four eighteen o one

795
00:47:44,700 --> 00:47:48,670
so i'm going to start from that point assume you know that i'm not going

796
00:47:48,670 --> 00:47:53,240
to tell you what differential equations or what modelling is if you

797
00:47:53,330 --> 00:47:58,410
still uncertain about those things the book has a very long and good explanation of

798
00:47:58,410 --> 00:48:00,620
it just be that stuff

799
00:48:02,120 --> 00:48:05,870
we're talking about

800
00:48:05,880 --> 00:48:07,220
first order

801
00:48:07,260 --> 00:48:12,940
all these of the

802
00:48:12,990 --> 00:48:18,600
all use three two akron two acronyms these ordinary differential equations i think all of

803
00:48:19,460 --> 00:48:20,650
knows that were

804
00:48:20,680 --> 00:48:22,120
taking a course i

805
00:48:22,160 --> 00:48:26,040
so we're talk about first-order odis

806
00:48:30,370 --> 00:48:33,260
which in standard form are written

807
00:48:33,300 --> 00:48:34,910
you isolate

808
00:48:36,200 --> 00:48:39,940
derivative of y with respect to let x let's say

809
00:48:39,960 --> 00:48:45,290
on the left-hand side and on the right-hand side you write everything else

810
00:48:45,290 --> 00:48:47,930
you can't always do this very well

811
00:48:47,940 --> 00:48:53,790
but for today i'm going to assume that it has been done and it's doable

812
00:48:53,800 --> 00:48:58,740
so for example some of the ones that will be considered either today or in

813
00:48:58,740 --> 00:49:00,130
the problems that

814
00:49:02,510 --> 00:49:06,440
things like

815
00:49:07,480 --> 00:49:10,630
why prior equals x over y it's pretty simple

816
00:49:10,680 --> 00:49:15,720
the problems that also problems that has why prime equals

817
00:49:16,580 --> 00:49:22,920
r move

818
00:49:23,720 --> 00:49:26,640
minus y squared

819
00:49:26,760 --> 00:49:29,290
and it also has

820
00:49:29,310 --> 00:49:32,230
why prime equals

821
00:49:32,290 --> 00:49:34,070
why minus x squared

822
00:49:36,500 --> 00:49:37,720
there are others too

823
00:49:40,250 --> 00:49:43,950
you look at this this of course you can solve by separating variables

824
00:49:44,010 --> 00:49:45,580
so this is solvable

825
00:49:46,630 --> 00:49:48,630
this one

826
00:49:49,570 --> 00:49:54,730
in neither of these can you separate variables and they look extremely similar

827
00:49:54,750 --> 00:50:00,160
but they are extremely disimilar are the most dissimilar about them

828
00:50:00,320 --> 00:50:03,160
is this one

829
00:50:03,170 --> 00:50:08,470
is easily solvable and you learn you don't know already next time next friday and

830
00:50:08,510 --> 00:50:09,780
solve this one

831
00:50:09,790 --> 00:50:12,200
this one which looks almost the same

832
00:50:12,220 --> 00:50:17,610
is unsolvable in a certain sense namely there are no elementary functions which you can

833
00:50:17,610 --> 00:50:22,780
write down which will prove give a solution of adequate differential equation

834
00:50:22,820 --> 00:50:25,130
so right away one conference

835
00:50:25,190 --> 00:50:31,190
the most significant factor even for the simplest possible differential equations those which only involve

836
00:50:31,200 --> 00:50:36,940
the first derivative it's possible to write an extremely looking got simple guys up with

837
00:50:36,940 --> 00:50:40,360
this one up in blue to indicate is there that

838
00:50:40,380 --> 00:50:43,100
o three

839
00:50:43,230 --> 00:50:48,910
i mean that you know not really bad

840
00:50:51,600 --> 00:50:58,220
it's not solvable in the ordinary sense in which you think of the equation equation

841
00:50:58,220 --> 00:50:59,480
is solvable

842
00:50:59,660 --> 00:51:04,290
and since those equations are the rule rather than the exception

843
00:51:04,310 --> 00:51:06,610
i'm going about this first day

844
00:51:06,630 --> 00:51:09,600
two not solving a single differential equation

845
00:51:09,630 --> 00:51:12,310
but indicating to you what you do

846
00:51:12,320 --> 00:51:15,890
when you meet a blue equation like that what you do

847
00:51:15,910 --> 00:51:17,250
with it

848
00:51:17,260 --> 00:51:21,640
so this first is going to be devoted to geometric ways of looking at differential

849
00:51:24,510 --> 00:51:28,180
numerical at the very end all talk a little bit about numerical

850
00:51:28,210 --> 00:51:32,880
ways and you will work on both of those for the first problem set

851
00:51:34,170 --> 00:51:39,550
what are geometric view of differential equations

852
00:51:46,630 --> 00:51:52,800
it's something that contrasted with the usual procedures by which you solve things you don't

853
00:51:52,800 --> 00:51:57,430
find elementary functions which sell them a call that the analytic method

854
00:51:57,470 --> 00:52:00,070
so on the one hand we have the knowledge

855
00:52:00,130 --> 00:52:02,940
the got talent ideas

856
00:52:02,950 --> 00:52:07,250
in which you write down explicitly the equation

857
00:52:07,260 --> 00:52:09,580
why primary equals f of that's why

858
00:52:09,610 --> 00:52:13,770
and if you look for certain functions which are called it's solutions now

859
00:52:14,150 --> 00:52:16,240
so far there is the only e

860
00:52:16,260 --> 00:52:21,260
and why one of x notice i don't use a separate letter IOU's g or

861
00:52:21,260 --> 00:52:26,130
h or something like that the solution because the letters multiply so quickly that is

862
00:52:26,610 --> 00:52:29,010
like gravity multiply this is of

863
00:52:29,090 --> 00:52:32,970
then after a while if you keep using different letters for each new idea you

864
00:52:32,970 --> 00:52:35,120
can figure out what

865
00:52:35,900 --> 00:52:38,310
i'll use y one means

866
00:52:38,320 --> 00:52:44,740
it's a particular solution a solution of this differential equation of course the differential equation

867
00:52:44,750 --> 00:52:46,260
has many solutions

868
00:52:46,280 --> 00:52:49,900
containing an arbitrary constant so so-called this silly

869
00:52:50,950 --> 00:52:52,510
the geometric view

870
00:52:58,430 --> 00:53:01,700
the geometric guy that corresponds to this

871
00:53:01,710 --> 00:53:08,530
version of writing equation is something called the direction field

872
00:53:08,530 --> 00:53:11,190
and it's not normally--this relates to the very first slide--there's not normally an

873
00:53:11,190 --> 00:53:13,850
issue of, there's so much noise you couldn't tell what this was by looking at

874
00:53:13,850 --> 00:53:16,420
that. in vision, you look at this and you know what this is.

875
00:53:16,480 --> 00:53:19,030
occasionally you're wrong, but you pretty much know what this is

876
00:53:19,540 --> 00:53:23,030
so if you want to get from here to here

877
00:53:23,130 --> 00:53:25,740
trying to learn your way directly like that,

878
00:53:25,760 --> 00:53:29,680
whilst ignoring the fact that was really going on, is crazy

879
00:53:29,690 --> 00:53:33,770
in particular because this is a very low bandwidth path

880
00:53:33,810 --> 00:53:37,180
you'd be much better off just inverting this first

881
00:53:37,190 --> 00:53:38,950
and then

882
00:53:39,010 --> 00:53:42,650
learning this simpler mapping--this mapping, this mapping's gotta be simpler

883
00:53:42,690 --> 00:53:45,530
than this mapping, right? if it happens like that

884
00:53:47,000 --> 00:53:49,060
once you've got this, then learn that.

885
00:53:49,240 --> 00:53:51,040
i'm sort of labouring this point

886
00:53:52,230 --> 00:53:59,920
if you buy this picture, then unsupervised pre training makes sense. you're finding this.

887
00:53:59,940 --> 00:54:02,830
OK i want to talk a little bit about modeling real values because so far, it's

888
00:54:02,830 --> 00:54:05,650
all been binary values, sometimes treated as

889
00:54:05,680 --> 00:54:07,550
real valued probabilities

890
00:54:10,550 --> 00:54:11,940
so for

891
00:54:11,950 --> 00:54:16,730
mnist digits, you can treat intermediate values like probabilities/ you can think of them

892
00:54:16,730 --> 00:54:19,480
as the probability that pixel had ink or something like that

893
00:54:19,490 --> 00:54:22,280
but it doesn't work for real images

894
00:54:22,290 --> 00:54:24,100
so in a real image

895
00:54:24,150 --> 00:54:28,320
then, the intensity of a pixel is almost exactly the average of its neighbours almost

896
00:54:30,080 --> 00:54:32,650
and you can't get that sort of exact

897
00:54:32,660 --> 00:54:36,280
you know, you can't get an exact 0.7 and not 0.69

898
00:54:36,320 --> 00:54:40,620
out of these logistic units. you need something else

899
00:54:41,450 --> 00:54:43,220
yee-whye teh and i

900
00:54:43,290 --> 00:54:46,240
tried using integer valued variables

901
00:54:46,260 --> 00:54:49,050
which you can make quite easily by saying i've got n identical copies of a

902
00:54:49,050 --> 00:54:50,190
binary unit

903
00:54:50,220 --> 00:54:52,770
so now

904
00:54:58,330 --> 00:55:01,660
i can get myself integer values. it's still hard to get them very precise because

905
00:55:01,660 --> 00:55:03,900
the noise grows with the value

906
00:55:03,900 --> 00:55:08,670
in fact, if the binary unit has the probability p of being on,

907
00:55:08,680 --> 00:55:11,400
which is the logistic of the total input it gets,

908
00:55:11,410 --> 00:55:14,670
then you get a number that has--

909
00:55:15,200 --> 00:55:17,650
your real value has this mean

910
00:55:17,750 --> 00:55:20,790
and this variance. so in the middle there's quite a lot of variance

911
00:55:20,830 --> 00:55:25,930
so we got--we managed to deal with some real valued images like that

912
00:55:25,950 --> 00:55:27,650
matching faces for example

913
00:55:27,660 --> 00:55:29,380
yee-whye did a very nice job of it

914
00:55:29,420 --> 00:55:31,810
but that was mainly to do with the fact that yee-whye does a good

915
00:55:31,810 --> 00:55:35,190
job of everything, even if he's working with a not very good model like this

916
00:55:39,130 --> 00:55:40,540
a better way to model

917
00:55:40,580 --> 00:55:42,530
integer values

918
00:55:42,550 --> 00:55:48,300
you take our binary unit and you replicate it many times, but each time you replicate it, you shift it

919
00:55:48,310 --> 00:55:53,140
that is, you give it biases. they all have the same shared bias that's learned,

920
00:55:53,150 --> 00:55:55,360
but they're shifted like this

921
00:55:55,370 --> 00:55:58,750
there's a reason for the 0.5's

922
00:55:58,780 --> 00:56:01,550
so this is what they look like

923
00:56:01,560 --> 00:56:04,060
and here is the total input. so down here

924
00:56:04,070 --> 00:56:06,070
the above output's zero

925
00:56:06,130 --> 00:56:09,720
and what we're gonna do is just take the sum of all their outputs

926
00:56:09,740 --> 00:56:12,130
so if you ask what does this curve look like when i add up all

927
00:56:15,280 --> 00:56:17,300
to within the thickness of the line,

928
00:56:17,310 --> 00:56:19,760
it's this curve here.

929
00:56:19,770 --> 00:56:22,880
it's very accurately this

930
00:56:22,890 --> 00:56:25,130
if you take an infinite number of these

931
00:56:29,250 --> 00:56:30,240
this curve

932
00:56:30,250 --> 00:56:32,480
can be made out of

933
00:56:32,520 --> 00:56:33,570
these guys

934
00:56:33,580 --> 00:56:35,990
and because you've got a learning algorithm for these guys

935
00:56:36,000 --> 00:56:40,150
i mean i can always put some constraint on sharing weights and things, and the learning algorithm still works

936
00:56:40,150 --> 00:56:41,990
so i now have to learn these guys

937
00:56:44,050 --> 00:56:49,320
i know a way of learning these--this, this thing with integer noise

938
00:56:49,350 --> 00:56:52,970
and what i've done that i'm happy, that contrast divergence learning will work for this

939
00:56:52,970 --> 00:56:56,670
thing with the appropriate integer noise

940
00:56:56,680 --> 00:56:58,760
and therefore

941
00:56:59,370 --> 00:57:02,820
i then throw away the theory and say, i'm gonna learn with this thing

942
00:57:02,880 --> 00:57:07,150
and have some gaussian noise that's got the same variance as the appropriate integer noise

943
00:57:07,240 --> 00:57:09,830
and so i end up with this kind of unit

944
00:57:09,850 --> 00:57:13,020
it's--the output is the max of zero

945
00:57:13,030 --> 00:57:15,910
and the input plus some gaussian noise

946
00:57:15,920 --> 00:57:18,120
whose value is--whose

947
00:57:18,190 --> 00:57:19,930
standard deviation

948
00:57:19,940 --> 00:57:22,810
whose variance is the logistic of

949
00:57:22,820 --> 00:57:24,330
the total input

950
00:57:24,350 --> 00:57:28,280
and that's roughly right and these guys work very nicely

951
00:57:28,290 --> 00:57:34,720
and you train them just by--i can now have these rectified linear units instead

952
00:57:34,720 --> 00:57:35,990
of binary units

953
00:57:36,010 --> 00:57:39,270
so i can deal with positive real valued stuff here

954
00:57:39,280 --> 00:57:42,530
especially if lots of the values are zero. it's very happy with that.

955
00:57:42,580 --> 00:57:45,830
and use the same units here

956
00:57:45,900 --> 00:57:49,350
and i can train it up with contrastive divergence just the same way as i did before

957
00:57:49,370 --> 00:57:52,530
it's very important that when i choose states for these guys

958
00:57:52,570 --> 00:57:54,500
i stick in the sampling noise

959
00:57:54,520 --> 00:57:57,420
i don't have to stick in the sampling noise when i choose these states or when i

960
00:57:57,430 --> 00:57:58,550
choose these states.

961
00:57:58,560 --> 00:58:01,610
sampling noise here doesn't do anything, except add noise.

962
00:58:02,400 --> 00:58:05,160
it stops you transmitting real values accurately

963
00:58:05,170 --> 00:58:08,250
which regularizes things lot.

964
00:58:08,290 --> 00:58:10,870
so i can learn with those.

965
00:58:10,880 --> 00:58:14,570
so here's some experiments that i did with units like that. unfortunately,

966
00:58:14,730 --> 00:58:17,220
the units weren't exactly like that

967
00:58:17,550 --> 00:58:20,150
they were very like that but not exactly like that

968
00:58:20,190 --> 00:58:24,410
because i hadn't yet figured all this out when i did the experiments. but i'll tell you the results

969
00:58:25,530 --> 00:58:28,770
and hopefully the experiments are only going to get better by doing it right.

970
00:58:28,770 --> 00:58:32,680
in some way three is the father of y two and could write for the

971
00:58:32,680 --> 00:58:35,270
chain as long as i want to just made an example with the chain of

972
00:58:35,270 --> 00:58:36,920
length three now

973
00:58:36,970 --> 00:58:40,570
that's after this we again to our system we have in there

974
00:58:40,570 --> 00:58:43,340
let's see what we get

975
00:58:43,380 --> 00:58:46,190
again we get the same three doctors

976
00:58:46,210 --> 00:58:51,040
namely johnny cantoni and why do we get johnny cantoni i mean the reason is

977
00:58:51,040 --> 00:58:52,670
essentially the same

978
00:58:52,670 --> 00:58:57,180
we are not asking for explicitly giving the father of this person we just asking

979
00:58:57,180 --> 00:59:00,840
for the existence of a chain of ancestors

980
00:59:00,850 --> 00:59:03,140
and since we know that the

981
00:59:03,160 --> 00:59:08,830
each person has a father and the range of the hasfather property is again a

982
00:59:09,680 --> 00:59:12,570
the father will again be a present the following will again have a father and

983
00:59:12,570 --> 00:59:13,690
so on

984
00:59:13,710 --> 00:59:17,630
so however long we make this chain we know that have

985
00:59:17,630 --> 00:59:19,040
the instance of person

986
00:59:19,070 --> 00:59:22,260
will be in the answer to such a query

987
00:59:23,450 --> 00:59:27,080
the existence of the father is implied by

988
00:59:27,250 --> 00:59:30,560
the constraints in our ontology

989
00:59:31,790 --> 00:59:34,680
now let's make it slightly different query sorry

990
00:59:35,650 --> 00:59:36,910
suddenly changed

991
00:59:36,960 --> 00:59:38,560
it allows

992
00:59:41,010 --> 00:59:42,750
it is again very similar to the earth

993
00:59:42,770 --> 00:59:46,770
to the previous one in the previous line what the difference now

994
00:59:46,780 --> 00:59:50,250
the difference is that now we ask asking explicitly also for

995
00:59:52,430 --> 00:59:56,230
the president is at the end of the chain so for their ancestors

996
00:59:56,250 --> 00:59:58,610
for the grand grandfather of

997
00:59:58,610 --> 01:00:04,530
of x because we are not asking for expert for the pair x y three

998
01:00:04,560 --> 01:00:07,310
let's see what we get now is an answer

999
01:00:07,390 --> 01:00:11,990
OK we don't get and the answer the number fifty properties zero so if we

1000
01:00:11,990 --> 01:00:13,930
ask explicitly

1001
01:00:14,960 --> 01:00:19,380
there the individual it here then the system is not able any more to exploit

1002
01:00:19,680 --> 01:00:24,040
the information in ontology because it doesn't have sufficient information

1003
01:00:24,940 --> 01:00:29,260
this is the information is not sufficient to say that anyone

1004
01:00:29,270 --> 01:00:33,010
o has sought to extract some

1005
01:00:33,060 --> 01:00:37,600
that are related to multiple chaining of the hasfather relations

1006
01:00:37,650 --> 01:00:38,380
we can

1007
01:00:38,530 --> 01:00:52,310
this is an issue but i mean this this is the sum of the game

1008
01:00:52,310 --> 01:00:55,340
the one place one if one wants the name

1009
01:00:55,510 --> 01:00:58,420
so if one is interested in the name

1010
01:00:59,420 --> 01:01:02,540
put the variability it means he is interested in and that's why i asked for

1011
01:01:02,540 --> 01:01:05,530
the name if you are not interested in the name that he would about the

1012
01:01:05,530 --> 01:01:06,430
first query

1013
01:01:06,450 --> 01:01:10,120
well you know that you will i mean this person will have some chain of

1014
01:01:10,120 --> 01:01:14,810
ancestors but you're not interested in buddhism has that and so you don't return because

1015
01:01:14,810 --> 01:01:19,240
i mean return invented name returning no name is not to there's not much of

1016
01:01:19,240 --> 01:01:25,360
a difference to some degree so it's actually just to queries reflect different information needs

1017
01:01:25,380 --> 01:01:26,660
that persons

1018
01:01:26,660 --> 01:01:29,880
all users may have when they deposed these queries here they are interested in the

1019
01:01:29,880 --> 01:01:32,130
name of the twenty ask for it

1020
01:01:32,140 --> 01:01:34,910
and the system may or may not have enough information to

1021
01:01:34,960 --> 01:01:37,900
provides answer here they not interested in the name and that's why i did not

1022
01:01:37,900 --> 01:01:41,420
know that so the messages you can take it from here is not whether that's

1023
01:01:41,420 --> 01:01:45,490
too much i mean if you ask only the information that you really need because

1024
01:01:45,490 --> 01:01:49,420
if you ask too much then you may not get also what you want to

1025
01:01:49,420 --> 01:01:53,490
compare and we had in concrete examples we had this experience where

1026
01:01:53,510 --> 01:01:58,660
i mean users west session posing queries which are all too much information in this

1027
01:01:58,720 --> 01:02:03,560
didn't allow the system to infer information that could actually provide would have been useful

1028
01:02:03,560 --> 01:02:05,680
for users

1029
01:02:08,680 --> 01:02:10,870
now let's look at the final example

1030
01:02:10,900 --> 01:02:19,230
in which which is a bit more complicated so how much time do i have

1031
01:02:20,200 --> 01:02:21,590
fifty minutes

1032
01:02:21,640 --> 01:02:23,670
fifteen OK

1033
01:02:23,690 --> 01:02:27,920
so i guess i might keep this example also

1034
01:02:27,970 --> 01:02:30,690
this example shows a case where

1035
01:02:30,700 --> 01:02:35,900
you have to all the system in order to provide an answer has to do

1036
01:02:35,900 --> 01:02:39,790
with what is called reasoning by cases so it has to analyse various possibilities that

1037
01:02:39,790 --> 01:02:41,370
depend on the data and this

1038
01:02:41,950 --> 01:02:45,520
complex thing to do and the fact that this comes

1039
01:02:45,540 --> 01:02:52,630
along is because here in our ontology reuse essentially disjunctive information we say that professors

1040
01:02:52,630 --> 01:02:56,920
are partitioned into st professor and full professors and this leads to this problematic issue

1041
01:02:56,920 --> 01:02:59,070
of reasoning by cases again

1042
01:02:59,100 --> 01:03:02,040
i keep this for time reasons

1043
01:03:02,070 --> 01:03:03,410
now a

1044
01:03:03,450 --> 01:03:06,180
so just to summarise what we were saying

1045
01:03:06,190 --> 01:03:11,100
now in this in this setting where we want to answer queries on large amount

1046
01:03:11,110 --> 01:03:15,000
of data in the presence of incomplete information we want to take into account at

1047
01:03:15,000 --> 01:03:17,630
runtime the constraints the reason at runtime

1048
01:03:17,670 --> 01:03:20,730
and we want to answer complex database like queries

1049
01:03:20,750 --> 01:03:25,080
also should is this is what happens if we want to deal with multiple information

1050
01:03:25,080 --> 01:03:28,130
sources used you in the context of information integration

1051
01:03:28,200 --> 01:03:31,070
this situation typically arises

1052
01:03:31,180 --> 01:03:35,430
just to connect with same before when i was talking about answering queries we want

1053
01:03:35,430 --> 01:03:40,000
to reason so the technically the notion that you are interested in what's interesting is

1054
01:03:40,180 --> 01:03:47,360
the so-called certain answers to a query which i intuitively the answers over all possible

1055
01:03:47,360 --> 01:03:49,760
interpretations that are models of our

1056
01:03:49,790 --> 01:03:51,180
ontology can

1057
01:03:51,200 --> 01:03:54,470
this is similar to the kind of reasoning that we did for

1058
01:03:54,490 --> 01:03:59,260
when where and when we are asking instances of classes

1059
01:03:59,270 --> 01:04:01,170
except that now we have is

1060
01:04:01,210 --> 01:04:05,700
but it's the same notion so we want or copies of constance that

1061
01:04:06,750 --> 01:04:10,620
in the answer to the query for every possible interpretation is a model of our

1062
01:04:11,900 --> 01:04:15,070
and these are called the certain answers

1063
01:04:15,080 --> 01:04:19,370
and what we want to do is want to get the certain answers by process

1064
01:04:19,370 --> 01:04:23,370
of inference so in general this process of instance required to take to create a

1065
01:04:23,410 --> 01:04:25,770
t-bone steak our data a box

1066
01:04:25,830 --> 01:04:30,760
do some inference all this information and produced set the certain answers

1067
01:04:30,790 --> 01:04:34,960
but the point is that if you want to deal with data efficiently

1068
01:04:34,980 --> 01:04:40,220
then we especially need in this largely inference process need to separate the contribution comes

1069
01:04:40,220 --> 01:04:44,310
from the data from the contribution that comes from the korean from the tee box

1070
01:04:44,380 --> 01:04:50,480
so what we're interested in is in the process of query answering by rewriting what

1071
01:04:50,480 --> 01:04:52,430
does this mean that look at this so

1072
01:04:52,450 --> 01:04:57,410
separating the contribution means you satiated for for some computational

1073
01:04:57,420 --> 01:04:58,840
over the query

1074
01:04:58,840 --> 01:05:05,380
and the box and produce essentially new query that incorporate the information coming from the

1075
01:05:05,380 --> 01:05:07,260
tee box this is called

1076
01:05:07,320 --> 01:05:12,720
perfect rewriting of our initial increase the perfect rewriting cooperate the information in the books

1077
01:05:12,730 --> 01:05:13,990
in the new query

1078
01:05:15,150 --> 01:05:19,840
you take this nuclear and and say over our a box by viewing the able

1079
01:05:19,840 --> 01:05:24,320
to just it was database few as if it was complete data

1080
01:05:25,040 --> 01:05:30,090
we want this to produce than the certain answers which are defined in the larger

1081
01:05:30,090 --> 01:05:34,600
this is the first thing that i that that i need

1082
01:05:34,640 --> 01:05:38,700
the other thing is that there must be a deterministic relation

1083
01:05:40,000 --> 01:05:42,810
my belief in a certain proposition

1084
01:05:42,870 --> 01:05:44,920
and i believe in its negation

1085
01:05:46,250 --> 01:05:49,590
so if i sort of say

1086
01:05:49,630 --> 01:05:51,840
sort of running out of examples here

1087
01:05:52,070 --> 01:05:57,270
imagine that k

1088
01:05:59,000 --> 01:06:01,250
OK let me just be vague so

1089
01:06:01,250 --> 01:06:05,050
if i if i think that if i sort of if i'm convinced that a

1090
01:06:05,050 --> 01:06:09,800
proposition is false this is false then there must be a mathematical way for me

1091
01:06:10,690 --> 01:06:15,400
to know what my belief is indeed being true OK and this relation must be

1092
01:06:15,410 --> 01:06:19,300
deterministic OK

1093
01:06:19,400 --> 01:06:23,480
so the third one

1094
01:06:23,600 --> 01:06:28,310
the first one one is a bit complicated i really don't know where to go

1095
01:06:30,100 --> 01:06:34,020
i need to be able to

1096
01:06:34,150 --> 01:06:36,650
if i want to know how much i believe i want to be the joint

1097
01:06:37,070 --> 01:06:40,790
like earlier we had this joint distribution p of x come out why OK

1098
01:06:40,820 --> 01:06:45,050
how much do i simultaneously believe into given propositions

1099
01:06:45,100 --> 01:06:48,410
and today being sunny and me being one eighty OK

1100
01:06:49,260 --> 01:06:50,510
i must be

1101
01:06:50,510 --> 01:06:52,320
there must be way

1102
01:06:52,320 --> 01:06:53,620
that that i can

1103
01:06:53,620 --> 01:06:58,490
we construct this information from these two quantities here roughly so

1104
01:06:58,510 --> 01:07:02,700
from the conditional beliefs so i see sunny and now how much do i believe

1105
01:07:02,700 --> 01:07:04,510
in my heart could be

1106
01:07:04,680 --> 01:07:06,680
but then i need to also use

1107
01:07:06,700 --> 01:07:09,650
actually what my belief in the in

1108
01:07:09,660 --> 01:07:12,760
in the something being out is

1109
01:07:12,810 --> 01:07:16,850
the consequence of these reactions if you satisfy them all if you if you need

1110
01:07:16,850 --> 01:07:18,900
to satisfy the more

1111
01:07:20,870 --> 01:07:26,020
then basically this is another book has anyone read james book

1112
01:07:28,090 --> 01:07:30,510
not that many people but it's

1113
01:07:30,510 --> 01:07:32,150
it's a beautiful book really

1114
01:07:32,230 --> 01:07:33,710
he is

1115
01:07:33,930 --> 01:07:39,140
a viral and bayesian his his is there already

1116
01:07:39,150 --> 01:07:40,930
he died old

1117
01:07:40,940 --> 01:07:46,420
but his left a lot of followers shall we say so so that the big

1118
01:07:46,420 --> 01:07:50,930
thing is that if these belief functions have to satisfy all of this then they

1119
01:07:50,930 --> 01:08:02,750
must satisfy the rules of probability theory including bayes rule

1120
01:08:06,460 --> 01:08:07,770
so how could we

1121
01:08:07,780 --> 01:08:09,400
how can we apply this to

1122
01:08:09,420 --> 01:08:12,530
two modelling situation

1123
01:08:12,550 --> 01:08:16,740
so imagine the following imagine we had a certain dataset

1124
01:08:16,750 --> 01:08:19,620
imagine we had some models to choose from

1125
01:08:19,680 --> 01:08:23,920
and then given the model i also have to choose some values for the parameters

1126
01:08:23,920 --> 01:08:27,620
OK so there's quite a few uncertain quantities laying around

1127
01:08:27,710 --> 01:08:31,330
all right now the interesting thing is that

1128
01:08:31,370 --> 01:08:35,680
the two quantities that come to mind one of them is

1129
01:08:35,680 --> 01:08:40,210
the prior beliefs so what the mean is before i see the data before even

1130
01:08:40,210 --> 01:08:41,310
see the data

1131
01:08:41,400 --> 01:08:42,460
how all

1132
01:08:42,470 --> 01:08:44,150
you know

1133
01:08:44,190 --> 01:08:48,770
what's my prior belief among complexity with the user neural network with ten billion hidden

1134
01:08:50,250 --> 01:08:54,770
o or one with three OK internally in my mind i have some sort of

1135
01:08:54,770 --> 01:08:56,740
a predisposition

1136
01:08:56,770 --> 01:09:01,560
you know it might be very vague prior distribution over over the complexity but i

1137
01:09:01,560 --> 01:09:06,460
certainly would probably not use one with with ten billion

1138
01:09:07,180 --> 01:09:09,990
very differently

1139
01:09:10,000 --> 01:09:13,190
but there was a function

1140
01:09:22,740 --> 01:09:28,310
so we should just one minus

1141
01:09:28,330 --> 01:09:30,390
that's what it

1142
01:09:30,400 --> 01:09:36,050
so the the axiom was a lot more abstract just said that that necessarily if

1143
01:09:36,050 --> 01:09:39,370
i if i have is basically because

1144
01:09:39,390 --> 01:09:44,340
proposition and it being falls must cover all the space of hypotheses and therefore there

1145
01:09:44,340 --> 01:09:49,680
must be a deterministic relation between the two

1146
01:09:49,700 --> 01:09:53,030
OK i want to have a model one said was of picked my neural net

1147
01:09:54,430 --> 01:09:56,310
a thousand hidden units let's say

1148
01:09:56,330 --> 01:10:00,280
now i have to select also some values for the weights right

1149
01:10:00,410 --> 01:10:03,560
and from my experience i know that

1150
01:10:03,570 --> 01:10:06,830
if the values get too large and stuff like that you know

1151
01:10:06,880 --> 01:10:09,060
i may overfit some gonna

1152
01:10:09,160 --> 01:10:12,030
i'm going to sort of i i suppose you know all of us

1153
01:10:12,110 --> 01:10:17,490
do that in one way or another right you can either add a sort of

1154
01:10:17,530 --> 01:10:21,430
squared norm penalty to the weight spread due to your cost function you have you

1155
01:10:21,430 --> 01:10:24,500
know the sum of squared weights or anything that sort of pushes away to be

1156
01:10:24,500 --> 01:10:27,580
small and to get sort of smooth functions and stuff like that we all have

1157
01:10:27,580 --> 01:10:33,120
our assumptions about smoothness regularity and so on

1158
01:10:33,120 --> 01:10:34,290
and then the third

1159
01:10:34,300 --> 01:10:36,780
the third quantity is

1160
01:10:36,790 --> 01:10:38,800
i want to have a model

1161
01:10:38,810 --> 01:10:42,240
and once you have picked value for the parameters

1162
01:10:43,560 --> 01:10:47,140
what kind of what kind of data can be small

1163
01:10:49,030 --> 01:10:51,680
all right and i can also express that

1164
01:10:51,680 --> 01:10:55,380
so you might say oh wait a moment if i have my neural net and

1165
01:10:55,390 --> 01:10:58,550
i have chosen the values of the parameters and everything is fixed

1166
01:10:59,290 --> 01:11:03,830
there's no uncertainty here right well no i mean there is still some uncertainty that

1167
01:11:03,830 --> 01:11:07,880
the trivial one is just noise right you might assume that there is some observation

1168
01:11:08,760 --> 01:11:10,500
so you might you might think that

1169
01:11:10,530 --> 01:11:14,240
even if a choice of model and the choice of parameters already gives you sort

1170
01:11:14,240 --> 01:11:16,740
of the the the the underlying function

1171
01:11:16,750 --> 01:11:19,580
you know that if you were to observed data

1172
01:11:19,600 --> 01:11:22,050
you know chances are they will come with a bit of noise

1173
01:11:22,060 --> 01:11:25,620
right so you still have some uncertainty here and you will be able to express

1174
01:11:25,620 --> 01:11:27,970
this as as a distribution

1175
01:11:27,990 --> 01:11:29,380
as well

1176
01:11:30,310 --> 01:11:33,610
so what can i do now with all these distributions

1177
01:11:33,680 --> 01:11:39,070
isabelle mentioned earlier that i i d

1178
01:11:39,080 --> 01:11:43,350
this is a thing of the past now we're we're over that

1179
01:11:43,360 --> 01:11:47,950
it's it's sort of true so

1180
01:11:48,000 --> 01:11:51,990
very often you see this use of this thing here is called the likelihood so

1181
01:11:51,990 --> 01:11:57,870
here x has been transformed into a d for for for the data OK

1182
01:11:57,930 --> 01:12:00,540
if they were independent you could sort of

1183
01:12:00,580 --> 01:12:02,930
you can sort of factorizes but there is no

1184
01:12:02,940 --> 01:12:06,970
there's no compelling reason there's absolutely no reason why why you need

1185
01:12:07,050 --> 01:12:09,070
the the i i d

1186
01:12:09,080 --> 01:12:10,930
on this slide OK

1187
01:12:13,050 --> 01:12:17,850
one interesting quantity in in sort of interest in the sort of the bayesian setting

1188
01:12:17,860 --> 01:12:21,560
what happens is the following and i'll show you show you later a couple of

1189
01:12:21,560 --> 01:12:26,300
figures try to illustrate his let's take let's take the the neural network example again

1190
01:12:26,380 --> 01:12:29,240
i have some sort of idea of what my

1191
01:12:29,280 --> 01:12:32,170
values of all the values of my weights should be

1192
01:12:32,180 --> 01:12:36,050
so what i can do is i can give them a probability distribution and that's

1193
01:12:36,050 --> 01:12:37,880
have five equals two

1194
01:12:37,890 --> 01:12:40,710
so if you just want to not

1195
01:12:40,710 --> 01:12:44,580
and you can also measure there are now two of them to get h squared

1196
01:12:44,610 --> 01:12:46,610
another exponent here

1197
01:12:46,660 --> 01:12:49,350
so obviously smaller we make smash size

1198
01:12:49,370 --> 01:12:53,650
so close to the point j two together to better we have

1199
01:12:53,720 --> 01:12:55,800
get to get to the continuous solution

1200
01:12:57,810 --> 01:13:02,870
it's also a lot of adaptive strategy which can automatically

1201
01:13:02,910 --> 01:13:06,770
choose how far we have to refine the grid if you this adaptive grid cells

1202
01:13:06,830 --> 01:13:09,760
not of code alzey how to

1203
01:13:09,760 --> 01:13:13,010
where to put the grid points and had two two

1204
01:13:13,030 --> 01:13:15,840
get to a good solution

1205
01:13:15,860 --> 01:13:20,870
these two dimensional results in similar results in higher dimensions but most of the series

1206
01:13:20,870 --> 01:13:22,180
for the five months of this

1207
01:13:22,570 --> 01:13:24,820
two and three dimensions because no way really

1208
01:13:24,840 --> 01:13:29,310
could do them in high dimensions before

1209
01:13:29,310 --> 01:13:31,500
OK so

1210
01:13:35,810 --> 01:13:38,860
a bit of new movies so that you can see what people are doing was

1211
01:13:44,080 --> 01:13:47,610
this is just the ball which dropped into water

1212
01:13:47,680 --> 01:13:51,560
and difficult thing here is to

1213
01:13:51,570 --> 01:13:54,880
to actually be able to

1214
01:13:56,290 --> 01:13:59,990
things that were there was that they have little splash is coming out and this

1215
01:13:59,990 --> 01:14:05,390
can kind of this holds that's in america very difficult to track set

1216
01:14:05,410 --> 01:14:08,740
what else do we have

1217
01:14:10,370 --> 01:14:14,310
the same when we have

1218
01:14:14,310 --> 01:14:21,540
stops this one

1219
01:14:21,570 --> 01:14:25,630
don't ever

1220
01:14:25,650 --> 01:14:28,100
so here we have adjustable

1221
01:14:28,110 --> 01:14:32,880
coming into the water and is the difficulty is that we have to do this

1222
01:14:32,910 --> 01:14:34,240
affix subject

1223
01:14:34,250 --> 01:14:35,630
fixed objects

1224
01:14:35,700 --> 01:14:37,930
and be able to track

1225
01:14:38,130 --> 01:14:43,860
as easy conditions on the boundary and it's all moving that's difficult to tracks

1226
01:14:44,390 --> 01:14:47,110
two of nowadays

1227
01:14:47,130 --> 01:14:50,960
and some people working with hollywood for this kind of things to make better movies

1228
01:14:50,970 --> 01:14:55,120
and we have i

1229
01:14:55,140 --> 01:14:58,530
you to

1230
01:15:01,380 --> 01:15:05,360
we can have some fluid looking over boundaries and finding the thing up a little

1231
01:15:05,360 --> 01:15:13,040
bit of fluid goes down here and this is just move it just for that

1232
01:15:13,050 --> 01:15:21,490
and one last time and

1233
01:15:25,580 --> 01:15:28,330
and one is more

1234
01:15:28,350 --> 01:15:31,020
not one of my more

1235
01:15:31,060 --> 01:15:32,570
sick of fluid

1236
01:15:32,590 --> 01:15:42,790
that's how it starts moving

1237
01:15:45,120 --> 01:15:48,270
we you can kind of see second of

1238
01:16:08,290 --> 01:16:12,640
so that's was very short course on finite element have you have a basic understanding

1239
01:16:12,640 --> 01:16:14,360
what what's going on

1240
01:16:14,380 --> 01:16:21,890
at some some names no driven what is notable for its

1241
01:16:21,910 --> 01:16:26,270
the set of being done in three dimensions two and three dimensions that are not

1242
01:16:26,270 --> 01:16:28,700
sparse grids which is one way

1243
01:16:28,710 --> 01:16:29,360
two years

1244
01:16:29,420 --> 01:16:30,920
go into

1245
01:16:30,930 --> 01:16:34,780
one way to extend this grid kind of approaches into higher dimensions

1246
01:16:34,980 --> 01:16:39,420
re one of the basic ideas has to do with the what kind of pages

1247
01:16:39,420 --> 01:16:41,890
using so before i read this

1248
01:16:41,920 --> 01:16:44,560
the basis functions as we won

1249
01:16:44,560 --> 01:16:45,300
we too

1250
01:16:45,340 --> 01:16:46,610
and we three

1251
01:16:46,660 --> 01:16:48,240
and obviously can

1252
01:16:48,250 --> 01:16:53,750
represents function so the combination of these functions these functions can represent commission six from

1253
01:16:53,830 --> 01:16:57,730
so we want to include between one and be week

1254
01:16:59,970 --> 01:17:02,630
there's a different approach called direct basis

1255
01:17:03,640 --> 01:17:08,100
i was already first discovered in nineteen nine by problem

1256
01:17:08,110 --> 01:17:10,960
but some picked up demonstrators

1257
01:17:11,020 --> 01:17:12,740
and there we have

1258
01:17:12,800 --> 01:17:19,820
it's similar to what it's the same as the wavelet decomposition method you heard so

1259
01:17:19,850 --> 01:17:24,010
we have now this space with three the same space as before but we can

1260
01:17:24,010 --> 01:17:28,070
write it as this w three plus w two

1261
01:17:28,070 --> 01:17:29,280
we know that

1262
01:17:30,260 --> 01:17:31,240
so i

1263
01:17:31,260 --> 01:17:33,030
is less than or equal to five

1264
01:17:33,890 --> 01:17:34,820
so i

1265
01:17:34,930 --> 01:17:37,300
this article to cite as well

1266
01:17:37,570 --> 01:17:41,030
so this the last prop that sorry the third property just tells us that one

1267
01:17:41,030 --> 01:17:44,010
of the one of the two opposite inclusions also

1268
01:17:44,140 --> 01:17:48,140
and i'll tell you what i mean what effect that has

1269
01:17:48,280 --> 01:17:51,780
OK the last two properties just tell us fairly explicitly that

1270
01:17:53,760 --> 01:17:58,570
non beliefs minimally entrenched

1271
01:17:58,620 --> 01:18:02,510
and so anything that's not in case mainly entrenched in the last one tells us

1272
01:18:02,510 --> 01:18:06,140
that the tautologies max trench

1273
01:18:06,160 --> 01:18:08,680
OK now what this gives us structure

1274
01:18:08,680 --> 01:18:11,160
which is often called the total preorder

1275
01:18:11,200 --> 01:18:13,720
so essentially what is the rank

1276
01:18:13,760 --> 01:18:16,260
sort of levels or a series of works

1277
01:18:16,260 --> 01:18:18,850
so these are the beliefs he

1278
01:18:18,930 --> 01:18:23,080
and most importantly ranked then these ones in these ones and this is about the

1279
01:18:23,100 --> 01:18:25,140
sort of separate ranks form

1280
01:18:25,160 --> 01:18:29,120
now you can prove some nice properties of this form is probably the most important

1281
01:18:29,120 --> 01:18:30,680
one is the following

1282
01:18:30,720 --> 01:18:33,970
that if you take this rank at any level

1283
01:18:34,010 --> 01:18:36,140
and you can't

1284
01:18:36,180 --> 01:18:40,510
and you look at all the formulas that are in here

1285
01:18:40,570 --> 01:18:42,430
let's suppose this form is

1286
01:18:42,490 --> 01:18:44,080
gathered together in the same game

1287
01:18:44,180 --> 01:18:45,180
and that's it

1288
01:18:45,220 --> 01:18:48,430
is deductively closed

1289
01:18:48,470 --> 01:18:53,550
so if cannot anyway all of the consequences of those beliefs are also in that

1290
01:18:58,620 --> 01:19:02,350
OK now the two properties i want to have a a look at just before

1291
01:19:02,350 --> 01:19:04,490
the ones at the bottom six and seven

1292
01:19:05,800 --> 01:19:09,570
that number three property essentially tells us that if we look at the conjunction of

1293
01:19:09,570 --> 01:19:11,390
phi psi

1294
01:19:11,410 --> 01:19:13,300
so if we have two formulas

1295
01:19:13,350 --> 01:19:14,870
so if i here

1296
01:19:16,230 --> 01:19:19,970
then a conjunction is at the same level as the minimum of the two

1297
01:19:20,010 --> 01:19:23,800
so find sites

1298
01:19:23,870 --> 01:19:28,470
the last one just says that with the destruction of the two

1299
01:19:28,550 --> 01:19:31,410
is either going to be here

1300
01:19:31,430 --> 01:19:33,240
or it's going to be he e

1301
01:19:33,350 --> 01:19:35,180
it's going to be some with

1302
01:19:35,180 --> 01:19:37,100
we don't know what

1303
01:19:37,280 --> 01:19:44,660
OK so that's what is your

1304
01:19:44,660 --> 01:19:48,200
a similar picture to the one given on board

1305
01:19:48,240 --> 01:19:51,990
the any questions open to us is where this disjunction is an that has a

1306
01:19:51,990 --> 01:19:58,220
very important function is change

1307
01:19:58,300 --> 01:20:03,200
let's look at this in conditions c-minus condition is perhaps the most important condition

1308
01:20:03,260 --> 01:20:07,890
it tells you how to construct the function k minus five

1309
01:20:08,450 --> 01:20:13,760
from this ordering

1310
01:20:13,850 --> 01:20:20,120
and it says well some formula size is going to be in k minus five

1311
01:20:20,160 --> 01:20:24,700
exactly under the following conditions firstly site has to be believed it has to be

1312
01:20:24,700 --> 01:20:25,600
in my

1313
01:20:25,620 --> 01:20:30,410
belief that someone

1314
01:20:33,600 --> 01:20:36,280
then either one of two conditions hold

1315
01:20:36,280 --> 01:20:39,810
i mean if i was the tautology in which case i can do anything i

1316
01:20:39,810 --> 01:20:44,820
can give anything up country firewire so in that case i do not

1317
01:20:44,840 --> 01:20:47,600
that's the sort of trivial case

1318
01:20:47,720 --> 01:20:51,740
in other cases they use this condition with the five is less than five or

1319
01:20:51,740 --> 01:20:54,280
signed to determine whether

1320
01:20:54,300 --> 01:20:55,990
i should give up some

1321
01:20:56,030 --> 01:20:58,450
OK so let's see what happens what

1322
01:20:58,550 --> 01:21:02,970
what this guy condition is doing

1323
01:21:03,100 --> 01:21:05,510
here i've got to form find site

1324
01:21:05,580 --> 01:21:10,010
i want to give up fight and size less important five

1325
01:21:10,070 --> 01:21:15,180
so if i took my rationality criteria said look i should give up less important

1326
01:21:15,180 --> 01:21:17,950
stuff i've more important stuff

1327
01:21:17,950 --> 01:21:19,800
that's ownership indicates that

1328
01:21:19,820 --> 01:21:22,870
i'm going to get rid of size well

1329
01:21:22,930 --> 01:21:25,970
does that seem like a rational thing to do

1330
01:21:26,030 --> 01:21:29,910
would you give us if i told you about five and there's something less important

1331
01:21:29,930 --> 01:21:33,100
here which always giving

1332
01:21:33,120 --> 01:21:36,070
you know what if i you know what if site has nothing to do with

1333
01:21:36,070 --> 01:21:39,490
five so i talked about sure talks about cheese

1334
01:21:39,510 --> 01:21:41,910
you know there's no real reason to

1335
01:21:41,930 --> 01:21:45,450
always give up side just because i happen to believe

1336
01:21:45,490 --> 01:21:46,990
in less

1337
01:21:50,640 --> 01:21:53,910
you know when i was outside before believed it was sunny

1338
01:21:53,950 --> 01:21:58,350
but hearing in this sort rumblings outside you want to contract beliefs

1339
01:21:59,030 --> 01:22:00,820
you know

1340
01:22:02,470 --> 01:22:06,660
perhaps i have a strong belief about like much weaker belief about my car still

1341
01:22:06,660 --> 01:22:08,740
being in the car park when i get back

1342
01:22:08,740 --> 01:22:11,410
should i give my belief in the car park just because i want to give

1343
01:22:11,410 --> 01:22:13,990
up my belief in the fact that some else

1344
01:22:14,620 --> 01:22:18,550
now the key comes down to the hat how this condition is used and that's

1345
01:22:18,550 --> 01:22:20,200
two freeways sparsity

1346
01:22:20,220 --> 01:22:24,530
that we draw the dictionary elements for action now which is the basis on base

1347
01:22:24,530 --> 01:22:27,630
moment can which can be a normal distribution

1348
01:22:27,650 --> 01:22:33,130
and we associate each dictionary element which is a pi k indicating the probability for

1349
01:22:33,130 --> 01:22:35,530
this dictionary to be selected in the data

1350
01:22:35,550 --> 01:22:37,890
and with pike k draw from data

1351
01:22:37,910 --> 01:22:43,440
so we would rather pay to acquire coefficients the ikea which case into the i

1352
01:22:43,470 --> 01:22:45,790
i d from below deepika

1353
01:22:45,800 --> 01:22:49,270
so by construction of rotation would be sparse

1354
01:22:49,290 --> 01:22:53,940
however assume that the interoperability which means they have by no binomial coefficient is how

1355
01:22:53,940 --> 01:22:59,030
to restrict so end up with a another reputation that the choice of the weights

1356
01:22:59,030 --> 01:23:02,990
associate with the from the normal so actually is

1357
01:23:03,010 --> 01:23:08,900
actual weights is the element wise product between the binary indicators e i understood as

1358
01:23:08,900 --> 01:23:13,880
i we and we have always the model exercise equal to be w plus most

1359
01:23:13,880 --> 01:23:17,840
these dictionary that i is the coefficient which

1360
01:23:17,890 --> 01:23:24,150
has real when you also imposed that many of these questions can be exactly zero

1361
01:23:24,200 --> 01:23:26,130
based on this

1362
01:23:26,170 --> 01:23:30,040
the condition we can compute hierarchical model only it

1363
01:23:30,170 --> 01:23:33,990
here i'm not going to discuss in detail this hara model but i want to

1364
01:23:33,990 --> 01:23:40,200
i want to mention that we only put long informative priors on on this on

1365
01:23:40,600 --> 01:23:45,150
this parameters and we we can get we can have the data itself to you

1366
01:23:45,150 --> 01:23:47,290
for what's the properties sparsity level of

1367
01:23:47,300 --> 01:23:48,730
and the laws will rise

1368
01:23:48,740 --> 01:23:50,160
i mean the data

1369
01:23:50,170 --> 01:23:55,480
and in case that the use cases we only observe a subset of the

1370
01:23:55,500 --> 01:23:59,590
data which means such an image we only observe a subset of pixels

1371
01:23:59,610 --> 01:24:04,930
and all only have a project to compress the moment they instead of directly observing

1372
01:24:04,990 --> 01:24:11,460
data we are observing y equals two sigma XI where sigma is someone in matrix

1373
01:24:12,490 --> 01:24:15,470
we end we a full likelihood model

1374
01:24:15,490 --> 01:24:21,560
the first term here is the likelihood and all the rest of the priors we

1375
01:24:21,560 --> 01:24:27,020
cannot directly calculate the posterior which parameter so we you can use but we can

1376
01:24:27,020 --> 01:24:32,030
use gibbs sampling or variational bayes one the first most the two can calculate the

1377
01:24:32,030 --> 01:24:33,960
conditional posterior experiment

1378
01:24:34,920 --> 01:24:40,190
the detail i'm not going to discuss the detail which can be found the decision

1379
01:24:41,150 --> 01:24:45,740
one mentions i'll all give some first show with our compost coordinates and that has

1380
01:24:45,760 --> 01:24:48,220
no computational complexity

1381
01:24:48,230 --> 01:24:52,900
and was always the dictionary was based both process priors

1382
01:24:52,910 --> 01:24:55,470
has a different perspective compared to

1383
01:24:55,480 --> 01:25:01,650
member the case with addiction investors however you free capital look as this updating equations

1384
01:25:01,980 --> 01:25:07,100
we can find the actually closely related however the have have the difference in the

1385
01:25:07,100 --> 01:25:08,440
level exporting

1386
01:25:08,460 --> 01:25:13,110
prisoner but any information and we can show that our dictionary elements that has the

1387
01:25:13,110 --> 01:25:17,110
highest level exploiting prison input information

1388
01:25:17,470 --> 01:25:19,850
and is also natural to use all

1389
01:25:19,870 --> 01:25:22,590
model two two sequential learning

1390
01:25:22,610 --> 01:25:23,550
since we

1391
01:25:23,570 --> 01:25:30,410
i mean we really nonparametric bayesian framework and so you instead of directly of calculating

1392
01:25:30,410 --> 01:25:34,640
the post your dictionary conditional on the whole dataset we can partition the

1393
01:25:34,650 --> 01:25:40,120
how dataset integer partitions and we first calculate the posterior dictionary on the first to

1394
01:25:40,130 --> 01:25:43,880
be partitioned you are and then use this post as the prior

1395
01:25:43,960 --> 01:25:47,860
to calculate the posterior dictionary based on the first

1396
01:25:47,940 --> 01:25:52,440
they pageant you on the second part indeed to do distinguish only until we're taking

1397
01:25:52,440 --> 01:25:55,950
into all the data into consideration

1398
01:25:55,970 --> 01:25:59,030
so in summary of noise around

1399
01:25:59,050 --> 01:26:02,740
the model relies arrangements bosnian not know

1400
01:26:02,750 --> 01:26:07,360
and the dictionary size is automatically inferred the training data lot required l model which

1401
01:26:07,360 --> 01:26:11,760
we can directly below the dictionary and the construction data based on noisy and incomplete

1402
01:26:13,960 --> 01:26:18,680
o it's some works i can has its own unique sparse representation which renders much

1403
01:26:18,680 --> 01:26:23,510
more flexibility than enforcing the saying it's possible for all the

1404
01:26:23,560 --> 01:26:28,140
the conditions and a single model here can be applied

1405
01:26:28,160 --> 01:26:32,810
four moments grayscale RGB or hyperspectral imaging denied any painting

1406
01:26:32,830 --> 01:26:35,640
first i was sure that image denoising result

1407
01:26:36,430 --> 01:26:37,360
here there

1408
01:26:38,140 --> 01:26:39,660
we can show that if

1409
01:26:39,670 --> 01:26:41,000
last level

1410
01:26:41,010 --> 01:26:46,540
and the july slowly the saying that case really perform our borders however if this

1411
01:26:46,540 --> 01:26:47,760
is the last level

1412
01:26:47,800 --> 01:26:52,200
is the higher than the true lower than the two possible cases really would tend

1413
01:26:52,220 --> 01:26:55,580
to either always moves in the image or not giving

1414
01:26:55,600 --> 01:26:56,460
you know

1415
01:26:56,490 --> 01:26:59,800
nice reduction is also interesting to look at a dictionary

1416
01:26:59,820 --> 01:27:04,240
in the right column because see the high loss level smaller size the the dictionary

1417
01:27:04,740 --> 01:27:09,110
would be inferred which makes sense because the highest possible level some there's less information

1418
01:27:09,110 --> 01:27:11,680
we can extract from image

1419
01:27:11,700 --> 01:27:16,390
and here we give inpainting results on RGB image on the first of all we

1420
01:27:16,390 --> 01:27:21,160
have is when you take eighty percent pixels this obviously image based on this cropped

1421
01:27:21,260 --> 01:27:24,390
image only with reconstructed images is

1422
01:27:24,410 --> 01:27:26,970
and we take a fifty percent because of

1423
01:27:26,980 --> 01:27:30,950
and the reconstructed image region which looks almost the same

1424
01:27:30,970 --> 01:27:33,220
and here is another example for to me

1425
01:27:33,240 --> 01:27:37,000
RGB image inpainting we have eighty percent pixels missing

1426
01:27:38,020 --> 01:27:42,600
this is our restored image this the original image and this is probably image and

1427
01:27:42,650 --> 01:27:44,510
can see the coverage in the past

1428
01:27:44,520 --> 01:27:48,130
this one point six a corresponding one iteration

1429
01:27:48,250 --> 01:27:53,180
and we can also do text remove this in your learning image this television image

1430
01:27:53,180 --> 01:27:54,920
that's the state sequence rule

1431
01:27:54,930 --> 01:27:56,260
and you would say

1432
01:27:56,260 --> 01:27:59,870
you know basically like this in the temporal lobe

1433
01:27:59,890 --> 01:28:02,660
city of linear temporal logic in

1434
01:28:02,720 --> 01:28:07,670
OK and what you want to bring his right up until the temporal logic axioms

1435
01:28:07,680 --> 01:28:11,800
and your progression plan now actually

1436
01:28:11,800 --> 01:28:16,730
looking at it is generating the proposition three it's looking at its library

1437
01:28:16,750 --> 01:28:21,750
i linear temporal logic formulas and seeing if any of these branches of violating any

1438
01:28:21,750 --> 01:28:22,970
of these rules

1439
01:28:23,010 --> 01:28:28,290
to do that again interesting thing ones have been keeping backup incremental satisfaction of all

1440
01:28:29,000 --> 01:28:34,250
search control the state sequence words and any time one of the rules is violated

1441
01:28:34,300 --> 01:28:37,050
you know you can that match

1442
01:28:38,120 --> 01:28:42,750
again this a nice that the technical achievement in terms of writing that algorithm is

1443
01:28:42,750 --> 01:28:48,250
how to do incrementally evaluation of LTL formula just the same a inmates dn

1444
01:28:48,350 --> 01:28:53,410
that's networks how to keep track of incremental passes phd

1445
01:28:53,420 --> 01:28:57,310
now how the language itself what look

1446
01:28:58,140 --> 01:29:02,380
and you think that you know blocks my question is can you unite these for

1447
01:29:03,420 --> 01:29:09,610
OK this is essentially says good of those that do not violate the conditions

1448
01:29:09,660 --> 01:29:12,880
OK this is simply a simple some problem

1449
01:29:12,930 --> 01:29:13,910
it says that

1450
01:29:13,960 --> 01:29:15,050
as you can see

1451
01:29:15,070 --> 01:29:16,000
and then

1452
01:29:16,000 --> 01:29:18,860
keep growing would tell us a white that

1453
01:29:18,920 --> 01:29:23,150
this reminds me of woody allen's speech to my speech to graduate students with c

1454
01:29:23,180 --> 01:29:25,330
is to do all the good things that were all the bad things and come

1455
01:29:25,330 --> 01:29:29,380
almost six o'clock you know that's the general idea so this is essentially a new

1456
01:29:29,380 --> 01:29:31,270
in blocks one this

1457
01:29:31,280 --> 01:29:32,860
is what you really trying to do

1458
01:29:32,910 --> 01:29:34,080
you do

1459
01:29:34,090 --> 01:29:37,130
come up with the best plan

1460
01:29:37,150 --> 01:29:40,830
the question is how many of you very smart people here could write

1461
01:29:40,840 --> 01:29:45,010
not necessarily right now go home and look at these and without cheating by to

1462
01:29:45,010 --> 01:29:50,130
see if you can like is really silly can but you know seems like that

1463
01:29:50,140 --> 01:29:53,330
so the heart of the so these are the whole that

1464
01:29:53,330 --> 01:29:55,280
you know the guy that humans

1465
01:29:55,330 --> 01:29:59,380
who wrote the knowledge was able to fight so my heart is completely of two

1466
01:29:59,400 --> 01:30:03,730
barker's by as with things like this apparently and the guy must have been hard

1467
01:30:03,750 --> 01:30:07,680
to deal with other kids dealing with lots were but anyway he was able to

1468
01:30:07,680 --> 01:30:08,570
write this stuff

1469
01:30:08,570 --> 01:30:13,390
and and of course that actually interesting part of the land program itself is to

1470
01:30:13,390 --> 01:30:18,410
incrementally and effectively devalued the truth often want to solve the problem of your learning

1471
01:30:18,580 --> 01:30:20,530
what you want to do is learn these things

1472
01:30:20,570 --> 01:30:23,940
these are some of the most effective ones now if you don't like as good

1473
01:30:23,990 --> 01:30:28,370
one as this relates somewhat less interesting ones it will still work is just not

1474
01:30:28,370 --> 01:30:32,110
you're not going to be able to to be working in blocks world

1475
01:30:32,790 --> 01:30:35,100
gets us to the interesting

1476
01:30:35,140 --> 01:30:40,220
question and know sort of pixels to learning aspects of this one thing which is

1477
01:30:40,220 --> 01:30:45,930
in the KB planning there were actually two competitions knowledge base planner and you know

1478
01:30:45,980 --> 01:30:50,440
what also separately kept an international planning competition

1479
01:30:50,470 --> 01:30:53,910
two years i think the thousand and two thousand two they had knowledge base planning

1480
01:30:54,600 --> 01:30:58,930
and in fact the two best planners which has been in the first and second

1481
01:30:58,930 --> 01:31:05,140
second first shop which is the one i showed you tonight high-level language planning for

1482
01:31:05,150 --> 01:31:10,770
the the and the plan which are right beside the main knowledge about the sequences

1483
01:31:10,770 --> 01:31:12,190
of blocks

1484
01:31:12,210 --> 01:31:16,310
so now the question of course is it's really hard to figure out i mean

1485
01:31:16,360 --> 01:31:22,250
one great lesson from KB planning is it's really what the learning distance

1486
01:31:22,260 --> 01:31:25,070
it's really what learning this stuff because if you can learn it you can do

1487
01:31:25,070 --> 01:31:26,800
just as well as the

1488
01:31:26,840 --> 01:31:31,670
but if you don't get into this learning angle then the lessons of

1489
01:31:31,690 --> 01:31:37,030
like you you know i mean how you was for him back plan my version

1490
01:31:37,030 --> 01:31:43,720
of using biomarkers planet is you know click the planet is planning multiplication instance of

1491
01:31:43,720 --> 01:31:46,830
for him because there's no way i can write the kind of knowledge that you

1492
01:31:46,830 --> 01:31:50,150
can and similarly shop lots of applications

1493
01:31:50,220 --> 01:31:52,540
the instance of being in now

1494
01:31:52,590 --> 01:31:55,350
so that you can write these things for me the point i'm trying to make

1495
01:31:55,350 --> 01:31:59,630
i mean you know deported kill me if you this but the thing to make

1496
01:31:59,630 --> 01:32:05,240
it is the really comparing the campaigning for him to in one type of knowledge

1497
01:32:05,240 --> 01:32:06,670
to the type of knowledge

1498
01:32:06,680 --> 01:32:09,510
and what is the right thing that you're comparing if you're doing you want them

1499
01:32:09,510 --> 01:32:13,480
merged with i been all that all of these guys can be defeated by somebody

1500
01:32:13,480 --> 01:32:16,590
will write a c program you know spent six six months of his life i'd

1501
01:32:16,590 --> 01:32:19,800
like to see program for that particular domain and that's the only thing they do

1502
01:32:19,800 --> 01:32:22,490
is they have no other interest in it

1503
01:32:22,490 --> 01:32:24,690
OK so i think in some sense

1504
01:32:24,710 --> 01:32:28,750
planning as a separate track fell by the way because people didn't know what to

1505
01:32:28,750 --> 01:32:29,940
make out of the

1506
01:32:30,000 --> 01:32:33,130
results but for this come here it's a great opportunity

1507
01:32:33,150 --> 01:32:34,110
you can learn

1508
01:32:35,120 --> 01:32:39,480
makes for him like these kinds of things is studying psychoanalysis of them we could

1509
01:32:39,480 --> 01:32:42,540
just learn this kind of knowledge and that's radically

1510
01:32:42,550 --> 01:32:46,910
OK so the question is that maybe we should just learn

1511
01:32:48,250 --> 01:32:50,500
it's finally get systems

1512
01:32:50,520 --> 01:32:54,170
the customisation part

1513
01:32:55,550 --> 01:32:59,880
you but i started by saying don't have time and customization done research can be

1514
01:32:59,880 --> 01:33:04,190
given by the humans and we look at what they're doing and then of the

1515
01:33:04,650 --> 01:33:06,540
i'm going to look at

1516
01:33:07,000 --> 01:33:10,430
how it is on a mission again

1517
01:33:13,170 --> 01:33:15,530
so this is my view

1518
01:33:17,010 --> 01:33:18,580
that area

1519
01:33:18,640 --> 01:33:21,840
in two different pieces of learning these kinds of knowledge

1520
01:33:21,850 --> 01:33:27,590
looks complicated network explain to you what do so first at this point remember that

1521
01:33:27,640 --> 01:33:31,600
in the very beginning i said for planning you can learn such control you can

1522
01:33:31,600 --> 01:33:34,330
learn domain physics you can learn

1523
01:33:35,340 --> 01:33:39,820
OK now we're are only talking about such control so we are the big ones

1524
01:33:39,830 --> 01:33:41,020
in the big one

1525
01:33:41,020 --> 01:33:45,510
now i'm going to be i'm saying if i want to learn such control what

1526
01:33:45,510 --> 01:33:49,010
sorts of approaches have been taken what the what

1527
01:33:49,020 --> 01:33:50,990
had i could accomplish gets

1528
01:33:51,000 --> 01:33:54,340
so there no at the top level you would say you could improve on existing

1529
01:33:55,970 --> 01:33:59,800
it seems like it is a little thing especially planning people rebuilding of planets and

1530
01:33:59,800 --> 01:34:03,280
you would be very sad somebody just this your community is not needed we can

1531
01:34:03,280 --> 01:34:04,350
just learn everything

1532
01:34:04,350 --> 01:34:06,460
here it is it's

1533
01:34:06,520 --> 01:34:08,670
italian heavy metals arsenic

1534
01:34:08,680 --> 01:34:14,870
the aircraft

1535
01:34:14,880 --> 01:34:18,920
OK questions

1536
01:34:18,930 --> 01:34:22,690
all right so we've described the system

1537
01:34:22,720 --> 01:34:28,350
with these properties now these properties come in two flavours have extensive properties

1538
01:34:28,350 --> 01:34:31,890
and intensive property

1539
01:34:33,460 --> 01:34:37,630
extensive properties of the ones that scale with the size of the system double the

1540
01:34:37,630 --> 01:34:38,580
system there

1541
01:34:38,590 --> 01:34:40,350
double and in there

1542
01:34:40,360 --> 01:34:41,480
numerical number

1543
01:34:41,500 --> 01:34:42,580
for instance

1544
01:34:43,230 --> 01:34:45,370
to double the alignment

1545
01:34:46,130 --> 01:34:48,160
double for that's of use

1546
01:34:48,170 --> 01:34:49,070
the mass

1547
01:34:49,080 --> 01:34:53,020
if you double the amount of stuff the will double

1548
01:34:53,050 --> 01:34:59,690
o intensive properties don't care about the scale of the system

1549
01:34:59,690 --> 01:35:00,610
two double

1550
01:35:00,800 --> 01:35:02,910
everything in the system

1551
01:35:02,960 --> 01:35:05,300
the temperature is not change not double

1552
01:35:05,350 --> 01:35:06,960
temperature stays the same

1553
01:35:07,020 --> 01:35:11,320
the temperature is an intensive and you can make intensive properties out of the extensive

1554
01:35:11,320 --> 01:35:14,130
properties by dividing by

1555
01:35:14,190 --> 01:35:16,990
the number of moles and this

1556
01:35:17,010 --> 01:35:20,040
so i can make a quantity that are called bar

1557
01:35:20,090 --> 01:35:22,270
which is the molar volume

1558
01:35:22,320 --> 01:35:26,680
the volume of one more of the component my sister

1559
01:35:26,700 --> 01:35:31,390
and that becomes an intensive quantity

1560
01:35:31,520 --> 01:35:33,650
volume which is an intensified

1561
01:35:35,070 --> 01:35:36,650
finds from all

1562
01:35:38,130 --> 01:35:42,830
of of the stuff

1563
01:35:44,180 --> 01:35:49,030
so as i mentioned termite dynamics is the science of equilibrium systems

1564
01:35:53,700 --> 01:35:59,210
so we need a new meaning it is also describes the evolution of one equilibrium

1565
01:35:59,220 --> 01:36:02,010
to another equally how to go from one to the other

1566
01:36:02,050 --> 01:36:06,600
and so the set of properties that describes the system

1567
01:36:06,600 --> 01:36:09,220
in equilibrium and doesn't change

1568
01:36:09,240 --> 01:36:10,430
so these

1569
01:36:10,430 --> 01:36:15,930
on changing properties that describe the state of colorado instead of the system

1570
01:36:15,940 --> 01:36:18,460
markov state variables

1571
01:36:18,690 --> 01:36:25,440
state variables

1572
01:36:25,460 --> 01:36:27,860
this crime

1573
01:36:27,980 --> 01:36:29,850
equilibrium is state

1574
01:36:29,860 --> 01:36:34,580
and they don't care about how the the states got to words

1575
01:36:34,620 --> 01:36:36,460
they don't care about the history

1576
01:36:36,520 --> 01:36:38,420
of the state just

1577
01:36:38,420 --> 01:36:40,760
i know that if you have water

1578
01:36:40,780 --> 01:36:43,540
zero degrees celsius with

1579
01:36:43,550 --> 01:36:44,930
i it

1580
01:36:44,960 --> 01:36:49,000
that you can define it as the heterogeneous system with a certain

1581
01:36:49,010 --> 01:36:54,570
certain density for the water certain density for the highest etcetera etcetera

1582
01:36:54,700 --> 01:36:57,210
doesn't care how you got there

1583
01:36:57,230 --> 01:37:00,710
we're going to find other properties that do care about the history of the system

1584
01:37:00,710 --> 01:37:01,940
like work

1585
01:37:01,990 --> 01:37:06,200
you put a system or he put in the system some other variables

1586
01:37:06,250 --> 01:37:11,050
but you can use those to define the equilibrium state

1587
01:37:11,130 --> 01:37:15,190
no use if a very independent of history

1588
01:37:15,210 --> 01:37:19,110
and it turns out that for one component system one twenty meaning one kind of

1589
01:37:19,110 --> 01:37:22,300
molecule the system

1590
01:37:22,480 --> 01:37:24,550
all that you need to know

1591
01:37:24,600 --> 01:37:26,570
to describe the system

1592
01:37:26,620 --> 01:37:29,350
is the number of moles

1593
01:37:29,400 --> 01:37:32,620
one performance system

1594
01:37:32,630 --> 01:37:38,030
and to describe one phase in that system one component

1595
01:37:38,080 --> 01:37:40,250
homogeneous systems need

1596
01:37:42,360 --> 01:37:45,010
and two variables

1597
01:37:45,070 --> 01:37:50,140
for instance the pressure and the temperature

1598
01:37:50,190 --> 01:37:53,220
for the volume and pressure

1599
01:37:53,280 --> 01:37:56,040
we have the number of moles into

1600
01:37:56,090 --> 01:37:58,110
intensive variables

1601
01:37:58,150 --> 01:38:02,580
then you know everything there is to know about the

1602
01:38:02,590 --> 01:38:05,380
by the cliff instead of that system

1603
01:38:06,130 --> 01:38:07,460
hundreds of

1604
01:38:08,460 --> 01:38:10,030
quantities that you can

1605
01:38:10,040 --> 01:38:13,230
calculate and measure that are interesting and important properties

1606
01:38:13,240 --> 01:38:17,660
and all you need is just a few variables to get everything out

1607
01:38:17,660 --> 01:38:21,950
and that's really the power thermodynamics is that it takes so little information

1608
01:38:22,040 --> 01:38:23,750
to get so much

1609
01:38:23,800 --> 01:38:25,170
information out

1610
01:38:25,230 --> 01:38:26,700
so little data

1611
01:38:26,750 --> 01:38:28,330
to get a lot of

1612
01:38:28,370 --> 01:38:33,570
predictive information

1613
01:38:33,570 --> 01:38:41,370
and a function that has small energy on this graph with a function that's constructed

1614
01:38:41,370 --> 01:38:44,340
from lambda one and lambda two or

1615
01:38:44,360 --> 01:38:47,020
the corresponding phi one phi two along

1616
01:38:51,060 --> 01:38:54,470
OK so

1617
01:38:54,670 --> 01:38:59,690
remember that's only part of the great we want small graph energy but we also

1618
01:38:59,690 --> 01:39:05,620
want your function to fit your labelled training data well

1619
01:39:05,640 --> 01:39:08,730
so this will happen if

1620
01:39:08,730 --> 01:39:09,740
your function

1621
01:39:09,760 --> 01:39:14,590
it is constructed using only those two

1622
01:39:14,610 --> 01:39:19,530
bases and then if you're label happens to be such that

1623
01:39:19,530 --> 01:39:21,030
for example positive

1624
01:39:21,050 --> 01:39:28,190
labels happens on this side and negative labels is that part of the graph

1625
01:39:28,190 --> 01:39:29,390
then you will have

1626
01:39:29,420 --> 01:39:33,350
in objective function that's extremely small

1627
01:39:33,360 --> 01:39:36,970
it's the minimizer

1628
01:39:46,030 --> 01:39:49,620
remember the assumption assumption really is

1629
01:39:49,640 --> 01:39:51,470
you are

1630
01:39:51,480 --> 01:39:54,260
target function the truth

1631
01:39:54,280 --> 01:39:58,600
said the data fit to defeat the graph where it's smooth on the graph

1632
01:39:58,620 --> 01:40:03,620
let's look at the example when this assumption is not true

1633
01:40:04,210 --> 01:40:10,190
so you have seen many times the two moons dataset i believe where the data

1634
01:40:10,190 --> 01:40:14,250
points are distributed kind of like this where you have to

1635
01:40:15,270 --> 01:40:21,460
and there connected with each room but disconnected and if you have positive on what

1636
01:40:21,470 --> 01:40:27,510
they want one the negative labels on the other one it's very much like the

1637
01:40:27,520 --> 01:40:28,980
change there

1638
01:40:28,990 --> 01:40:33,480
but let me now turn this two moons into inclining one so they actually overlap

1639
01:40:36,770 --> 01:40:39,320
and i draw samples from it then

1640
01:40:39,340 --> 01:40:43,820
the the circles are going to be some samples if not you create

1641
01:40:43,860 --> 01:40:50,290
a graph that's how people typically created nearest neighbour graph or absent graph

1642
01:40:50,310 --> 01:40:54,130
we will have trouble because now the two

1643
01:40:54,140 --> 01:40:57,070
parts will be strongly connected here

1644
01:40:57,090 --> 01:40:59,210
OK and if you

1645
01:40:59,240 --> 01:41:00,510
i have

1646
01:41:00,530 --> 01:41:03,010
two labelled points

1647
01:41:03,010 --> 01:41:09,130
here and here and you do any of this stuff well then not surprisingly

1648
01:41:09,230 --> 01:41:11,700
it's not going to do very well

1649
01:41:11,720 --> 01:41:17,100
so this is the case where the graph assumption is wrong

1650
01:41:17,210 --> 01:41:23,180
later on we will see one way to

1651
01:41:23,180 --> 01:41:25,860
help create a better crop

1652
01:41:26,860 --> 01:41:37,050
so i'm going to stop here and see if there are questions

1653
01:41:37,070 --> 01:41:39,530
not let's move on to

1654
01:41:39,730 --> 01:41:42,110
the final

1655
01:41:42,170 --> 01:41:47,740
family of algorithms called s three VM stands for semi supervised

1656
01:41:47,760 --> 01:41:50,460
support vector machines

1657
01:41:50,670 --> 01:41:55,440
now a brief review of support vector machines

1658
01:41:55,460 --> 01:41:58,010
this is only with labelled data

1659
01:41:58,030 --> 01:42:03,250
so if you have three positive three negative labels points and you want to find

1660
01:42:03,340 --> 01:42:10,060
the decision boundary SVM finds a linear decision boundary that separates the two classes with

1661
01:42:10,060 --> 01:42:15,910
the largest margin that means i defined the cart with the largest gap in the

1662
01:42:15,910 --> 01:42:18,850
gap is represented by dashed lines

1663
01:42:18,860 --> 01:42:20,640
OK so this is the

1664
01:42:20,690 --> 01:42:25,950
now if you also have unlabelled data points

1665
01:42:27,920 --> 01:42:32,430
it's the same labelled data point

1666
01:42:32,480 --> 01:42:35,180
but the green dots are unlabelled

1667
01:42:35,230 --> 01:42:37,100
then the previous cut

1668
01:42:37,130 --> 01:42:38,490
you may not be good

1669
01:42:40,280 --> 01:42:45,040
this decision boundary which cuts through this unlabelled data

1670
01:42:45,050 --> 01:42:48,550
why not good well if you believe that this

1671
01:42:48,560 --> 01:42:54,030
a whole lot looks like a whole lot is very dense and probably should

1672
01:42:54,030 --> 01:42:59,520
come from the same class then you want to avoid cutting through

1673
01:42:59,530 --> 01:43:01,180
that's labeled region

1674
01:43:01,190 --> 01:43:03,970
instead now the

1675
01:43:03,980 --> 01:43:05,180
the largest

1676
01:43:05,200 --> 01:43:07,010
margin linear

1677
01:43:07,030 --> 01:43:09,670
foundry is perhaps this

1678
01:43:09,690 --> 01:43:11,760
notice it still separates

1679
01:43:11,770 --> 01:43:16,510
the labelled data point but on the unlabelled data points

1680
01:43:16,530 --> 01:43:21,300
you're going to have a gap of this size

1681
01:43:21,320 --> 01:43:26,740
and that's the idea behind s three VM

1682
01:43:26,740 --> 01:43:29,170
you just want to separate

1683
01:43:29,290 --> 01:43:35,490
classes but as well as the unlabelled data somehow so you get the largest margin

1684
01:43:35,500 --> 01:43:39,470
OK so how do we do with this

1685
01:43:39,490 --> 01:43:44,850
now we need to review a little bit what is behind SPM

1686
01:43:44,930 --> 01:43:48,190
and this is the soft margin SVM

1687
01:43:48,200 --> 01:43:50,270
let me explain that

1688
01:43:51,010 --> 01:43:52,950
you have

1689
01:43:53,140 --> 01:43:56,410
classifier which i didn't right here as

1690
01:43:57,380 --> 01:44:02,430
which is HX plus b so edges some

1691
01:44:02,430 --> 01:44:03,990
so there's the

1692
01:44:04,010 --> 01:44:05,950
the bias term but

1693
01:44:05,970 --> 01:44:09,350
that's tracks plus b is a linear function of x

1694
01:44:11,080 --> 01:44:14,160
you have a linear function here and

1695
01:44:14,180 --> 01:44:16,310
you want to say that

1696
01:44:16,330 --> 01:44:19,680
this linear function is going to produce

1697
01:44:19,680 --> 01:44:24,220
this is the correct sign for my label y

1698
01:44:25,620 --> 01:44:26,720
i want

1699
01:44:26,740 --> 01:44:27,790
i hx

1700
01:44:27,810 --> 01:44:29,040
plus b

1701
01:44:29,060 --> 01:44:32,310
to be positive if y is

1702
01:44:32,330 --> 01:44:35,930
in the positive class and i want this to be negative if y is in

1703
01:44:35,930 --> 01:44:37,310
the negative class

1704
01:44:38,910 --> 01:44:40,740
i want

1705
01:44:40,760 --> 01:44:44,330
y times this to be always possible

1706
01:44:44,370 --> 01:44:45,580
that's my hope

1707
01:44:45,600 --> 01:44:49,560
and and furthermore not only do i want them to be on

1708
01:44:49,600 --> 01:44:52,030
to make the correct

1709
01:44:52,040 --> 01:44:54,370
classification i want to make

1710
01:44:54,370 --> 01:44:58,060
the classification with a very strong confidence

1711
01:44:58,080 --> 01:45:02,510
so instead of requiring this could be bigger than zero i wanted to be bigger

1712
01:45:02,510 --> 01:45:03,680
than one

1713
01:45:03,700 --> 01:45:07,640
so they have to be are sort of on the

1714
01:45:07,660 --> 01:45:10,790
so far away from the decision boundary

1715
01:45:10,810 --> 01:45:12,850
so that's what i wanted

1716
01:45:12,870 --> 01:45:15,530
but i also want to

1717
01:45:16,080 --> 01:45:21,950
take into consideration that some data points may not be separable in my case

1718
01:45:23,220 --> 01:45:28,310
it's it may not be possible for every data point to satisfy

1719
01:45:30,040 --> 01:45:31,910
without without this term

1720
01:45:31,910 --> 01:45:36,640
so i may not be able to separate every data points

1721
01:45:36,660 --> 01:45:38,330
four enough

1722
01:45:38,350 --> 01:45:42,180
so i need to relax it the way i relax it is by saying that

1723
01:45:42,560 --> 01:45:45,910
i'm going to introduce the slack variables because i

1724
01:45:46,060 --> 01:45:49,600
such that now i make this

1725
01:45:49,620 --> 01:45:52,370
so concise positive so i make this

1726
01:45:52,390 --> 01:45:56,220
inequality easier to satisfy right i can make the site

1727
01:45:56,240 --> 01:46:00,330
big so that this term is close to zero or you might even make it

1728
01:46:00,330 --> 01:46:03,100
bigger than one so that i will allow

1729
01:46:03,120 --> 01:46:06,370
this point to be on the wrong side of the decision boundary

1730
01:46:07,760 --> 01:46:09,310
i want to also

1731
01:46:09,310 --> 01:46:12,920
if we knew something about the probability we should be able to

1732
01:46:12,960 --> 01:46:18,500
use this information and that's exactly what prizes for

1733
01:46:20,810 --> 01:46:22,150
the problem is here

1734
01:46:22,180 --> 01:46:25,030
only if we sample many times get the problem is right

1735
01:46:25,060 --> 01:46:26,620
a good rule of thumb

1736
01:46:26,630 --> 01:46:29,370
this is in general actually quite handy

1737
01:46:29,410 --> 01:46:32,280
you need unless you do some symmetry

1738
01:46:32,370 --> 01:46:37,120
about ten times as many observations as you have to mention

1739
01:46:37,130 --> 01:46:41,320
you might say about the microarrays you know you've got maybe a hundred thousand

1740
01:46:41,340 --> 01:46:43,110
measurements and you've got only

1741
01:46:43,360 --> 01:46:46,920
two hundred three patients well this is very i see you have to do something

1742
01:46:46,920 --> 01:46:48,080
really fancy

1743
01:46:48,140 --> 01:46:49,510
but in general

1744
01:46:49,520 --> 01:46:51,150
if you've got ten times

1745
01:46:51,160 --> 01:46:54,370
as many observations as you've got parameters

1746
01:46:54,380 --> 01:46:57,570
yes a

1747
01:46:59,330 --> 01:47:00,670
what can you do

1748
01:47:00,680 --> 01:47:04,920
well one way of introducing processes what's called conjugate priors

1749
01:47:04,980 --> 01:47:08,740
and i'm not going to explain in a lot of detail here but what they

1750
01:47:09,660 --> 01:47:11,430
correspond to it

1751
01:47:11,440 --> 01:47:15,650
it's like assuming that they have additional fake stuff

1752
01:47:15,690 --> 01:47:17,940
so for instance if i

1753
01:47:18,730 --> 01:47:20,880
well some game of monopoly or

1754
01:47:21,000 --> 01:47:24,390
but even with the frame and it's also dies right

1755
01:47:25,340 --> 01:47:28,790
so usually i would not assume that my friend will cheat on me so i

1756
01:47:28,790 --> 01:47:30,160
don't assume that the

1757
01:47:30,240 --> 01:47:31,820
by state

1758
01:47:31,870 --> 01:47:35,600
so it's like assuming that one in addition to all the time so they passed

1759
01:47:35,600 --> 01:47:37,980
my dies in that game and might have maybe

1760
01:47:38,370 --> 01:47:39,480
played another

1761
01:47:39,490 --> 01:47:41,590
ten games with the guy before

1762
01:47:41,610 --> 01:47:45,820
and know he hasn't read them in the past so i can essentially atmospheric extra

1763
01:47:49,060 --> 01:47:51,700
so what i was basically do is i would take

1764
01:47:51,760 --> 01:47:54,820
the actual occurrences of an event i

1765
01:47:54,880 --> 01:47:57,170
and that my fake accounts to it

1766
01:47:57,180 --> 01:48:03,060
and the normalized by the number of trials plus all the extra fake counts the

1767
01:48:03,110 --> 01:48:06,770
this is something that in natural language processing people have actually done

1768
01:48:06,810 --> 01:48:08,460
it's called the possible

1769
01:48:08,510 --> 01:48:13,240
what they do here do there is a just and i want to it

1770
01:48:13,260 --> 01:48:18,130
it's called the possible because the going came up with a course without of course

1771
01:48:18,130 --> 01:48:20,710
the notion of conjugate priors but

1772
01:48:20,730 --> 01:48:21,690
none the less

1773
01:48:21,700 --> 01:48:25,020
well done fifty years ago

1774
01:48:25,060 --> 01:48:28,910
the very special case of just anyone

1775
01:48:28,920 --> 01:48:32,910
now what happens if we do this

1776
01:48:32,930 --> 01:48:35,270
look at that

1777
01:48:36,830 --> 01:48:39,620
here by the way there is

1778
01:48:40,730 --> 01:48:42,160
error here that b

1779
01:48:42,210 --> 01:48:43,410
o point one five

1780
01:48:43,450 --> 01:48:44,790
so what i do

1781
01:48:45,110 --> 01:48:48,700
these are my six different outcomes these are the actual

1782
01:48:48,780 --> 01:48:50,990
right like to estimate will give me

1783
01:48:51,000 --> 01:48:53,900
the probability is that

1784
01:48:53,990 --> 01:48:55,930
max estimate

1785
01:48:55,950 --> 01:48:58,640
when my total number of occurrences six

1786
01:48:58,680 --> 01:49:02,180
i will give my point one five point seven and so on here

1787
01:49:02,190 --> 01:49:07,180
so what you can see is it moves things more towards the uniform distribution

1788
01:49:07,190 --> 01:49:09,500
of everywhere one six

1789
01:49:09,540 --> 01:49:12,380
if i have one hundred additional pseudocount

1790
01:49:13,210 --> 01:49:18,400
it's actually very close to one sixth of

1791
01:49:30,590 --> 01:49:35,150
absolutely and i'll be showing you some pictures on exactly that after so what you

1792
01:49:35,230 --> 01:49:38,330
immediately covered is the downside of this approach

1793
01:49:38,420 --> 01:49:42,490
so the upside is we managed to get away with the local started in the

1794
01:49:42,490 --> 01:49:45,640
in the likely case with advice OK

1795
01:49:45,670 --> 01:49:49,860
in the unlikely case whether sustained it'll take us a lot longer to pick things

1796
01:49:51,780 --> 01:49:56,900
so what you describe is a little bit like the situation we're really golgi

1797
01:49:56,940 --> 01:49:59,930
goes to some gambling den where they're going to cheat on him

1798
01:49:59,980 --> 01:50:03,030
and then he will lose a lot of money until he realizes what had their

1799
01:50:03,050 --> 01:50:06,140
cheating on me and not just unlucky

1800
01:50:06,190 --> 01:50:09,530
so what you should be doing therefore you know if you may be

1801
01:50:09,540 --> 01:50:10,700
go to some

1802
01:50:10,710 --> 01:50:13,820
officially sanctioned co because you know well you can assume

1803
01:50:13,910 --> 01:50:15,770
i everything fair

1804
01:50:15,780 --> 01:50:18,190
like improv is a large prior like this

1805
01:50:18,250 --> 01:50:22,780
fact some somewhat any place they should be pro using something like this think because

1806
01:50:22,780 --> 01:50:26,060
they cannot trust it very much

1807
01:50:26,150 --> 01:50:29,820
so what it really means is

1808
01:50:29,870 --> 01:50:33,670
that's a strong prior well in this case brings us closer to

1809
01:50:33,750 --> 01:50:36,450
in this case the uniform distribution but in general

1810
01:50:36,460 --> 01:50:39,770
well the rest is most likely

1811
01:50:39,770 --> 01:50:44,630
if we have a problem for instance what do you want to recognise only the

1812
01:50:44,630 --> 01:50:48,730
numbers that were said to form you have only the numbers so you don't need

1813
01:50:48,730 --> 01:50:51,860
to go through four you will directly model the words

1814
01:50:51,920 --> 01:50:54,420
which only numbers

1815
01:50:54,880 --> 01:51:00,190
but in the general case you will pass true phoneme so we're going to define

1816
01:51:00,210 --> 01:51:03,130
the phoneme model so in hmm

1817
01:51:03,150 --> 01:51:07,750
that is going to be too we hope to be able to generate all the

1818
01:51:07,800 --> 01:51:09,360
the sequences

1819
01:51:09,480 --> 01:51:12,270
a rather data corresponding to

1820
01:51:12,270 --> 01:51:14,570
to the corresponding phonemes

1821
01:51:15,800 --> 01:51:18,710
and this is my HMM and look at the

1822
01:51:18,730 --> 01:51:20,300
it has the particle are

1823
01:51:20,320 --> 01:51:22,530
relation is what we call it

1824
01:51:22,590 --> 01:51:24,820
left to right model

1825
01:51:24,840 --> 01:51:27,940
basically saying that well this time going by

1826
01:51:27,940 --> 01:51:33,030
and where you're here you can either stay given the amount of time of goal

1827
01:51:34,000 --> 01:51:35,840
eventually go out

1828
01:51:35,860 --> 01:51:37,670
and because we

1829
01:51:37,780 --> 01:51:42,800
we make this assumption after twenty years of modelling that the fourteen goes through a

1830
01:51:42,800 --> 01:51:44,400
chunk of small

1831
01:51:46,460 --> 01:51:52,380
steps which follow the same distribution

1832
01:51:52,400 --> 01:51:53,360
so now

1833
01:51:53,380 --> 01:51:54,690
if i want to

1834
01:51:54,710 --> 01:51:58,210
to model a given some words

1835
01:51:58,500 --> 01:52:00,960
let's suppose the word is cat

1836
01:52:01,010 --> 01:52:03,570
if i have the phoneme c

1837
01:52:03,570 --> 01:52:05,550
defining a infinity

1838
01:52:05,570 --> 01:52:10,820
let's suppose i decided that my language to have these phonemes well before the word

1839
01:52:10,820 --> 01:52:13,110
count simple concatenation

1840
01:52:13,130 --> 01:52:14,820
of the very phonemes

1841
01:52:14,840 --> 01:52:19,570
and that creates a new HMM which is the concatenation of three hmm

1842
01:52:19,570 --> 01:52:25,050
where i just need to find these values these connectors which is quite easy to

1843
01:52:25,530 --> 01:52:27,530
and i now have the word

1844
01:52:29,280 --> 01:52:33,880
and then i can do that for sentences so is if you have a given

1845
01:52:33,880 --> 01:52:39,440
sentence you tell me the sentences cats don't like dogs

1846
01:52:39,460 --> 01:52:43,210
this is a sequence of words and hence the sequence of words on a sequence

1847
01:52:43,210 --> 01:52:48,770
of full of of sequence of phonemes so it's a sequence of phonemes and for

1848
01:52:48,770 --> 01:52:51,570
each one you have an HMM so i can create

1849
01:52:52,460 --> 01:52:58,130
sequence of nature which is going to be one single HMM and that's hmm

1850
01:52:58,210 --> 01:53:00,690
represents one sentence

1851
01:53:00,730 --> 01:53:03,000
cats don't like dogs

1852
01:53:03,050 --> 01:53:07,550
now what i want is to find the parameters of these these combine hmm that

1853
01:53:07,550 --> 01:53:13,860
maximize the likelihood of having generated that sentence

1854
01:53:13,860 --> 01:53:16,750
and i can do that for all sentences

1855
01:53:16,800 --> 01:53:20,980
and that's only the training part now

1856
01:53:21,030 --> 01:53:25,500
when a new sentence arrive i want to know what was pronounced and that's where

1857
01:53:25,510 --> 01:53:28,070
the viterbi algorithm

1858
01:53:28,110 --> 01:53:32,650
becomes important because now what i want to know is what was used to a

1859
01:53:32,670 --> 01:53:34,150
sequence of

1860
01:53:34,250 --> 01:53:37,230
for me that was pronounced

1861
01:53:37,610 --> 01:53:39,980
for given to sentence

1862
01:53:40,000 --> 01:53:41,690
so what i will do is that

1863
01:53:41,710 --> 01:53:47,920
to simplify we supposed to exemplify will have created a very big hmn

1864
01:53:47,940 --> 01:53:51,090
of all the possible words so

1865
01:53:51,090 --> 01:53:52,690
this is the following

1866
01:53:52,750 --> 01:53:55,210
a sequence of phoneme corresponding to one word

1867
01:53:55,230 --> 01:53:59,070
then you have a sequence of phoneme corresponding to another word and all the words

1868
01:53:59,070 --> 01:54:01,440
here of my language

1869
01:54:01,460 --> 01:54:06,400
but remember that there is copies so the number of parameters is not so big

1870
01:54:06,400 --> 01:54:08,440
because you may have

1871
01:54:08,480 --> 01:54:13,130
well you may have an a here in a year in a here all the

1872
01:54:13,130 --> 01:54:17,500
same it's just that you join them but the number of parameters is related to

1873
01:54:17,510 --> 01:54:21,440
the number of twenty on the number of workers project

1874
01:54:21,460 --> 01:54:26,150
you create a very very big HMM which has a number of words by the

1875
01:54:26,150 --> 01:54:27,420
number of warnings

1876
01:54:27,420 --> 01:54:33,070
there were times number of states performing times the number of parameters state

1877
01:54:33,110 --> 01:54:38,980
and you are going to search for a given set that's the best buy that

1878
01:54:38,980 --> 01:54:42,690
would go a long that you seem to connect back so

1879
01:54:42,710 --> 01:54:47,940
this sentence is a sequence of words or your question post to that here

1880
01:54:47,960 --> 01:54:49,460
generate the word

1881
01:54:49,480 --> 01:54:50,550
go back

1882
01:54:50,550 --> 01:54:52,980
generate in other words the back

1883
01:54:53,000 --> 01:54:58,360
narrative work until if you're going to search for the best part wrong this

1884
01:54:58,380 --> 01:54:59,670
connected graph

1885
01:54:59,690 --> 01:55:01,150
with loops

1886
01:55:01,170 --> 01:55:02,320
it doesn't matter

1887
01:55:02,320 --> 01:55:04,380
because all these loops can be

1888
01:55:04,400 --> 01:55:06,070
unfolded in time

1889
01:55:06,090 --> 01:55:10,070
and you're going to use this viterbi algorithm to find out of course this is

1890
01:55:10,150 --> 01:55:12,280
not really efficient so we have

1891
01:55:12,300 --> 01:55:17,800
lots of more efficient going to do that but i'm not going to talk about

1892
01:55:17,800 --> 01:55:19,210
the this is the main

1893
01:55:19,230 --> 01:55:20,070
i d

1894
01:55:20,110 --> 01:55:22,190
of how you do speech recognition

1895
01:55:22,250 --> 01:55:24,550
so that's very

1896
01:55:26,820 --> 01:55:27,820
of course

1897
01:55:27,840 --> 01:55:32,150
doing being able to do that is one thing when you do research you like

1898
01:55:32,150 --> 01:55:34,500
to know how good is your model

1899
01:55:34,530 --> 01:55:37,570
so you need to have a kind of loss function

1900
01:55:37,570 --> 01:55:42,960
and and in speech recognition what most people use is what they call the word

1901
01:55:42,960 --> 01:55:45,750
error rate so the you have model the

1902
01:55:45,770 --> 01:55:48,230
if view this is sequence of words

1903
01:55:48,280 --> 01:55:52,340
and if you want to compare that sequence of words with the correct sequence of

1904
01:55:52,340 --> 01:55:55,460
words but the correct sequence of words may have more

1905
01:55:55,480 --> 01:55:57,270
or less words

1906
01:55:57,320 --> 01:56:00,480
and it's not so simple to compute the

1907
01:56:00,480 --> 01:56:06,860
the distance between two sequences of words so they use the technique based on

1908
01:56:07,000 --> 01:56:13,250
on something which is used to compare two sequences of strings the strings of various

1909
01:56:14,300 --> 01:56:17,750
which is called the edit distance has also many other names

1910
01:56:19,650 --> 01:56:23,250
in the end with that technique you can compete with the the word error rate

1911
01:56:23,270 --> 01:56:25,400
which is going to be the number of

1912
01:56:25,400 --> 01:56:30,460
of words that you have inserted that we're not in the original sequence

1913
01:56:30,480 --> 01:56:33,840
plus the number of words that you forgot to say

1914
01:56:33,840 --> 01:56:35,650
plus the number of words that you

1915
01:56:35,670 --> 01:56:37,570
substitute with another

1916
01:56:37,610 --> 01:56:40,150
all the possible errors that you may have

1917
01:56:40,190 --> 01:56:45,050
divided by the true number of words which were expected that's going to give you

1918
01:56:45,550 --> 01:56:49,920
there were direct which you have to be careful it's not

1919
01:56:50,550 --> 01:56:54,090
nine nice measure because it it doesn't it's the bounded

1920
01:56:54,250 --> 01:56:57,190
it is bounded more than one in fact so

1921
01:56:57,190 --> 01:57:00,860
you have to be careful with that

1922
01:57:01,500 --> 01:57:04,440
we're now in the last part of the talk

1923
01:57:04,460 --> 01:57:08,320
which is now we can talk about with similar process

1924
01:57:08,320 --> 01:57:12,880
at least at

1925
01:57:16,610 --> 01:57:21,650
we talked about the germans and in moved in so it remains are very handy

1926
01:57:21,670 --> 01:57:22,710
to model

1927
01:57:22,730 --> 01:57:26,960
one single sequence of information

1928
01:57:27,010 --> 01:57:33,090
on the other hand in multimodal processing you often have several sequences of information going

1929
01:57:33,090 --> 01:57:37,320
true at the same time for instance you may have the device

1930
01:57:37,400 --> 01:57:39,900
in the image of someone talking like now

1931
01:57:39,900 --> 01:57:45,770
you may want to model these two streams jointly may have more than two you

