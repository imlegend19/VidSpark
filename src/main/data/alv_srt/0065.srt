1
00:00:00,000 --> 00:00:03,000
the nice structure and then if you went to the third dimension you would see

2
00:00:04,140 --> 00:00:08,120
the structure in the wrong then you don't quite see in this two dimensions

3
00:00:08,640 --> 00:00:09,670
the fact that these um

4
00:00:10,760 --> 00:00:12,020
angle around is different here

5
00:00:13,850 --> 00:00:18,290
as i say go for large neighborhoods because these graphs are disconnected and that's meant

6
00:00:18,290 --> 00:00:22,000
that this is what pushed back the green are relying on the red

7
00:00:23,190 --> 00:00:26,330
although you can see some nice structure and that's section there

8
00:00:26,850 --> 00:00:27,670
that you don't see in

9
00:00:28,470 --> 00:00:33,900
peace yet but this is meant we had together these large neighborhoods to ensure these portions the manifold connected

10
00:00:34,450 --> 00:00:37,090
i mean i can be a problem obviously with these neighborhood graphs

11
00:00:40,080 --> 00:00:42,840
gene expression again i don't really see anything much and at one

12
00:00:45,430 --> 00:00:47,390
i'm really close the scottish now

13
00:00:48,510 --> 00:00:52,030
and i sound like our secretaries in sheffield apparently as well

14
00:00:53,640 --> 00:00:54,740
so again you see

15
00:00:56,500 --> 00:00:59,420
some separation but this is weird because sarah

16
00:01:00,430 --> 00:01:03,370
one appears to students in the speech group has something moved into

17
00:01:04,060 --> 00:01:04,390
the man

18
00:01:05,250 --> 00:01:07,440
so i'm not quite sure the embedding very good

19
00:01:09,050 --> 00:01:12,070
in fact i'm kind of skeptical about whether

20
00:01:12,590 --> 00:01:14,490
the question i can that's is a good thing to do

21
00:01:15,590 --> 00:01:18,870
in many cases i tell you what i think it's good is if you've got

22
00:01:18,920 --> 00:01:23,720
enormous amounts of data because the igon value problem you do is a sparse igon

23
00:01:23,720 --> 00:01:26,130
value problem computationally that's very quick solve

24
00:01:28,110 --> 00:01:32,610
but it's very few data points often gives really bad embeddings and that's because it's

25
00:01:32,610 --> 00:01:37,080
not putting any effort into thinking about what these adjacency weight should be and i'll

26
00:01:37,080 --> 00:01:39,300
come back to that later when i tried unify

27
00:01:39,970 --> 00:01:40,690
the perspective

28
00:01:41,880 --> 00:01:44,030
it's fast because it doesn't do anything

29
00:01:45,930 --> 00:01:47,220
and that's not a bad thing

30
00:01:48,030 --> 00:01:49,050
if you think about

31
00:01:49,550 --> 00:01:50,300
page rank

32
00:01:50,890 --> 00:01:52,630
it's basically laplacian i can maps

33
00:01:53,210 --> 00:01:56,060
google algorithm so you can do this on web scale data

34
00:01:56,930 --> 00:01:59,710
then adjacency just the linkage between um

35
00:02:00,390 --> 00:02:01,250
different web pages

36
00:02:01,930 --> 00:02:04,820
very very similar so you can do this on an almost data and if you

37
00:02:04,820 --> 00:02:08,860
go for almost data it turns out it doesn't matter that you don't parameterized

38
00:02:09,460 --> 00:02:10,280
the uh

39
00:02:10,350 --> 00:02:13,480
graph very much but if on the data sets are used here

40
00:02:14,280 --> 00:02:18,300
me particularly this one where there's only a few data very high-dimensional but few data

41
00:02:18,670 --> 00:02:22,110
it's not doing a great job so i would tend to think of using the

42
00:02:22,130 --> 00:02:27,190
question might maximum going very very large data um because certainly but that you can't

43
00:02:27,190 --> 00:02:27,880
do isomap

44
00:02:28,380 --> 00:02:30,330
which i think is a really good uh

45
00:02:31,020 --> 00:02:35,150
basic thing to so if you see the results the mean has some really nice

46
00:02:35,150 --> 00:02:39,600
results and things like that value dataset and uses the semi supervised learning so we

47
00:02:39,600 --> 00:02:42,850
can if only really work in domains that i haven't shown here

48
00:02:46,020 --> 00:02:47,350
locally linear embeddings

49
00:02:49,780 --> 00:02:53,270
you know i i really hate this not locally linear embeddings are like that but

50
00:02:53,570 --> 00:02:56,810
what i hate about spectral methods is what i'm doing now and i'm gonna try

51
00:02:56,810 --> 00:02:57,820
and correct the mean it

52
00:02:58,380 --> 00:03:00,370
it's what i do is i stand up when i say

53
00:03:00,840 --> 00:03:02,910
i has not a way of doing dimensionality reduction

54
00:03:03,440 --> 00:03:08,730
imagine everything's living and the diffusion equation or everything's locally linear all this and this

55
00:03:08,730 --> 00:03:11,230
is there and you can modern think about makes sense

56
00:03:12,510 --> 00:03:13,770
if you try and teach that's

57
00:03:14,080 --> 00:03:18,860
to undergraduates which i think must be the final aim any the research doing being

58
00:03:18,860 --> 00:03:22,000
able to incorporate incorporated into an undergraduate course you have to do

59
00:03:23,040 --> 00:03:26,920
three lectures on each methods in order for them to understand what's going on there

60
00:03:28,370 --> 00:03:31,100
it's completely different motivations and you get a flavor

61
00:03:31,590 --> 00:03:33,690
well this is i'm going through because now

62
00:03:34,530 --> 00:03:39,300
this is a new way to do dimensionality reduction which says let's approximate and nonlinear

63
00:03:39,300 --> 00:03:42,280
manifold by small linear patches and it's a really nice idea

64
00:03:44,060 --> 00:03:48,510
assume distance between data points is small relative to the curvature so if you look

65
00:03:48,510 --> 00:03:52,050
in your neighborhood that we've been talking about before and assume that is linear in

66
00:03:52,050 --> 00:03:54,660
the neighborhood the curvature is moving quickly

67
00:03:55,680 --> 00:04:00,570
then you find a set of linear regression weights for this locally linear patch and that's this equation here

68
00:04:01,400 --> 00:04:03,700
you try and reconstruct a point from its neighbors

69
00:04:04,730 --> 00:04:05,610
by this weighted

70
00:04:06,170 --> 00:04:09,970
weighting of its neighbours so you've got a point why i and you try and

71
00:04:10,120 --> 00:04:14,590
reconstruct its value by weighting neighbours about point with a quadratic error function

72
00:04:19,730 --> 00:04:21,030
object if it is

73
00:04:21,570 --> 00:04:26,230
invariant to rotation and rescaling view data but is not invariant to translation so there's

74
00:04:26,230 --> 00:04:28,220
something we're gonna much do to the subjective

75
00:04:29,010 --> 00:04:29,900
i end

76
00:04:31,600 --> 00:04:38,160
sam roweis and lawrence saul proposes an motivated well if you want to if we modify with translation

77
00:04:38,540 --> 00:04:43,700
so adding mutual hawai i's so this is minus why jason a minus mu there

78
00:04:44,200 --> 00:04:48,000
we add any object if we get this form and not changes the value of

79
00:04:48,000 --> 00:04:52,150
the objective so in order to fix the if you constrain the sum to one

80
00:04:54,290 --> 00:04:55,490
the sum of the weights to one

81
00:04:56,110 --> 00:05:00,400
that's a minus mu and not councils without new yeah so that's an important thing

82
00:05:00,400 --> 00:05:02,860
is not just regression this would just be regression

83
00:05:04,170 --> 00:05:07,750
in the data space rather than in the feature points space and minimize the subjective

84
00:05:07,750 --> 00:05:12,490
but this is regression with can extract constraint that the sum of w must be

85
00:05:12,490 --> 00:05:13,160
equal to one

86
00:05:16,340 --> 00:05:20,310
you see these constraints popping up everywhere right the classy and transpose one

87
00:05:21,080 --> 00:05:23,470
so that's gonna be meaningful data it's got another

88
00:05:23,950 --> 00:05:27,050
but you can see here in this in the way we the subjective here

89
00:05:29,260 --> 00:05:30,590
now the idea is to then

90
00:05:31,000 --> 00:05:31,890
these w's

91
00:05:33,350 --> 00:05:38,630
breach data point breeze neighborhood and then you look across all the neighborhoods

92
00:05:39,330 --> 00:05:45,330
and you start with a new dataset will cortex and you force acts to be a low dimensional dataset

93
00:05:46,480 --> 00:05:50,620
and he was actually low dimensional data you some across all the neighbours you get

94
00:05:50,620 --> 00:05:54,470
this which is the same objective function that is really expressed in a different way

95
00:05:54,470 --> 00:05:56,210
regression from

96
00:05:56,240 --> 00:06:03,690
now is just very very simple mathematical inequality which i call the basic inequality which

97
00:06:03,690 --> 00:06:07,500
reads as follows and then we try to read off from the basic inequality what

98
00:06:07,530 --> 00:06:08,920
can see

99
00:06:08,920 --> 00:06:16,200
the basic inequality here is kind of prediction error exp hat minus expletives zero how

100
00:06:16,200 --> 00:06:19,650
well can i predict the true underlying regression surface

101
00:06:19,740 --> 00:06:23,100
you scale it is one hour and that's not important so you take euclidean norm

102
00:06:24,280 --> 00:06:29,260
then there is actually an additional term is less or equal than something

103
00:06:29,270 --> 00:06:31,920
i will discuss this here below

104
00:06:31,930 --> 00:06:35,590
and here comes this other terminal at that time the l one norm of the

105
00:06:35,590 --> 00:06:37,840
true underlying parameter

106
00:06:37,850 --> 00:06:42,350
OK this is kind of really the spirit of the whole as the game you

107
00:06:42,350 --> 00:06:48,270
say OK if my truth is sparse if they the zero is sparse

108
00:06:48,300 --> 00:06:51,700
then it should work and what means inspires sparse

109
00:06:51,710 --> 00:06:57,540
actually reduced the last typically means sparse respectively l one or because this is the

110
00:06:57,540 --> 00:07:00,160
normal to the regularisation

111
00:07:00,170 --> 00:07:02,700
OK so what you have in mind is actually

112
00:07:02,750 --> 00:07:06,770
they zero one norm is not that large spas

113
00:07:06,780 --> 00:07:09,510
so this term is kind of small

114
00:07:09,530 --> 00:07:10,980
because you believe

115
00:07:10,990 --> 00:07:17,030
that year truce sparse if it's not sparse hopelessly lost you can not do statistical

116
00:07:17,030 --> 00:07:20,280
inference in a non sparse problem

117
00:07:20,290 --> 00:07:24,030
OK so the basic inequality is really trivial i just want to give you

118
00:07:24,440 --> 00:07:28,090
an idea how trivial it is so first of all you just write down the

119
00:07:28,090 --> 00:07:31,230
different using the definition of the last so you know

120
00:07:31,250 --> 00:07:37,700
it minimizes discrete aeropostale one or so whatever the data happens always smaller or equal

121
00:07:37,720 --> 00:07:44,230
to use any other value fate now the other value of pages spectral one debate

122
00:07:44,240 --> 00:07:47,130
OK and then i just this term

123
00:07:47,130 --> 00:07:52,340
this is made up y x beta zero plus eight phelan you pick it apart

124
00:07:52,340 --> 00:07:57,240
you rewrite this term you have here prediction error in comes the ceylon

125
00:07:57,260 --> 00:08:01,590
and then comes this additional crossair to through very easy to do

126
00:08:01,590 --> 00:08:07,140
and then on the right hand side we write if there exists long just you

127
00:08:07,160 --> 00:08:10,700
squared two norm and then you plug it in

128
00:08:10,760 --> 00:08:12,030
this can solve

129
00:08:12,050 --> 00:08:16,460
and you end up with this basic inequality very simple just uses the definition of

130
00:08:16,500 --> 00:08:18,180
the class

131
00:08:20,150 --> 00:08:24,080
if you want to do some theory about linear models and i mean all other

132
00:08:24,080 --> 00:08:30,540
models generalized linear models additive models multitask models it's always the same game going on

133
00:08:31,270 --> 00:08:35,670
so we need to say something about this to this crossed

134
00:08:35,700 --> 00:08:38,470
and the other one just sparsity will do it for us

135
00:08:38,500 --> 00:08:40,790
so this is the main thing

136
00:08:40,800 --> 00:08:43,470
let's just do it in the linear model

137
00:08:43,490 --> 00:08:48,840
here is a very simple but actually quite sharp inequality so here is my

138
00:08:48,850 --> 00:08:53,180
cross terms and this is trivial but this is just a

139
00:08:53,400 --> 00:08:57,030
triangle inequality so you can bounded

140
00:08:57,060 --> 00:09:00,840
you just take that part and you always wanted to do something with the l

141
00:09:00,840 --> 00:09:03,370
one norm because last is a one

142
00:09:03,380 --> 00:09:06,110
guided methodology

143
00:09:06,130 --> 00:09:10,280
so here you have the one norm this will be hopefully small if it had

144
00:09:10,290 --> 00:09:14,100
to do a good job and then you have to use other to two times

145
00:09:14,110 --> 00:09:15,150
the max

146
00:09:15,160 --> 00:09:18,430
over these variables which come from its transpose sticks

147
00:09:18,440 --> 00:09:20,440
it's very easy to write down

148
00:09:20,460 --> 00:09:22,000
OK now comes the trick

149
00:09:22,010 --> 00:09:25,930
and i mean all irritations at least what i see do this trick

150
00:09:25,990 --> 00:09:26,980
he say

151
00:09:27,760 --> 00:09:33,680
let's take the probabilistic part decided separate the probabilistic part the right stuff

152
00:09:33,680 --> 00:09:37,430
from the more analytical part which i will discuss

153
00:09:37,440 --> 00:09:40,330
so what you do is you say OK

154
00:09:40,330 --> 00:09:43,610
here this guy let's just look on this set

155
00:09:43,620 --> 00:09:45,730
of events that

156
00:09:45,760 --> 00:09:47,980
kind of maximum here is small

157
00:09:47,990 --> 00:09:49,660
let's look at the set

158
00:09:49,760 --> 00:09:54,360
two times the max here of this is literally for in a lambda zero and

159
00:09:54,360 --> 00:09:58,130
lambda is sometimes called the noise level of the problems

160
00:09:58,140 --> 00:10:02,380
OK this is the probabilistic part of the problem then you know this article is

161
00:10:02,520 --> 00:10:03,690
lambda zero

162
00:10:03,740 --> 00:10:08,110
so ontologies is really two why not going into the details

163
00:10:08,120 --> 00:10:11,460
and then you can come up with this on this table

164
00:10:11,490 --> 00:10:16,160
which will have high probability travel just argue you have this inequality here so you

165
00:10:16,160 --> 00:10:23,280
have your prediction error scale even something more than here's something and again if they

166
00:10:23,280 --> 00:10:25,890
to parts with respect to the one norm

167
00:10:25,900 --> 00:10:28,280
it is a small number

168
00:10:28,310 --> 00:10:33,170
so in a way this is just

169
00:10:33,180 --> 00:10:37,460
the train arrived just take it apart and now i need to say something about

170
00:10:37,460 --> 00:10:39,780
what is the probability forty six

171
00:10:39,800 --> 00:10:42,500
and this is really how current theories going

172
00:10:42,510 --> 00:10:46,730
so i need to say something how should we choose to land here

173
00:10:46,750 --> 00:10:50,330
if you look for land the beginning to land zero so i need to say

174
00:10:50,330 --> 00:10:53,330
something about how we choose to learn the zero

175
00:10:53,340 --> 00:10:57,760
and i want to say something about the probability forty six times

176
00:10:57,780 --> 00:10:59,640
OK to

177
00:10:59,670 --> 00:11:04,730
heuristics is choose lambda as small as possible right and you make this error term

178
00:11:04,730 --> 00:11:10,530
as small as possible but there is a trade here lambda shouldn't be too small

179
00:11:10,580 --> 00:11:14,390
if i take a very small don't have to build large into lambda zero effect

180
00:11:14,390 --> 00:11:15,720
pick lambda zero

181
00:11:15,720 --> 00:11:20,510
now four of how they look like

182
00:11:20,590 --> 00:11:22,260
so what

183
00:11:22,320 --> 00:11:23,490
about the same

184
00:11:23,510 --> 00:11:27,300
so that they have a problem with the help of one of k

185
00:11:27,340 --> 00:11:29,090
and the parameters here

186
00:11:29,120 --> 00:11:32,640
has to be posted but didn't need not sum to one this is how people

187
00:11:36,120 --> 00:11:38,970
when the output of one

188
00:11:39,490 --> 00:11:40,890
the dirichlet

189
00:11:41,030 --> 00:11:43,100
distribution with all

190
00:11:43,120 --> 00:11:45,200
with the parameters of one

191
00:11:45,220 --> 00:11:47,100
simply if you a

192
00:11:47,120 --> 00:11:50,510
uniform distribution over the probability simplex

193
00:11:50,510 --> 00:11:52,570
so this is in the case

194
00:11:52,620 --> 00:11:54,950
when he goes tree

195
00:11:55,010 --> 00:11:59,820
and you've probably is fact each vertex corresponds to one

196
00:11:59,820 --> 00:12:01,720
and if i

197
00:12:01,910 --> 00:12:02,930
each and

198
00:12:02,990 --> 00:12:06,450
each point within the triangle corresponds to

199
00:12:06,510 --> 00:12:10,600
it's a pi factor and this i that they would have

200
00:12:10,660 --> 00:12:13,120
value which is a lot

201
00:12:13,160 --> 00:12:14,890
o or one

202
00:12:14,910 --> 00:12:19,860
so it is the one who is three i think that would mean that i

203
00:12:19,860 --> 00:12:20,970
want is pretty big

204
00:12:20,990 --> 00:12:23,780
but i when i feel pretty small

205
00:12:24,090 --> 00:12:28,680
when the parameters of the tree they

206
00:12:28,680 --> 00:12:30,410
all of the one

207
00:12:30,410 --> 00:12:34,300
that we can all get a little bump in the center

208
00:12:34,340 --> 00:12:35,660
and the like

209
00:12:35,660 --> 00:12:39,070
so these are the very david cameron who

210
00:12:39,070 --> 00:12:41,510
this is the duration of the parameter five

211
00:12:41,550 --> 00:12:43,640
you can see that the but

212
00:12:43,660 --> 00:12:46,700
more concentrated around

213
00:12:47,240 --> 00:12:48,800
around the the

214
00:12:49,090 --> 00:12:53,510
the size of the parameter corresponds to how concentrated its

215
00:12:53,530 --> 00:12:56,390
it's the the distribution

216
00:12:56,410 --> 00:12:58,070
when you have a problem

217
00:12:58,070 --> 00:13:00,620
and this is why i i who

218
00:13:01,140 --> 00:13:05,220
the bomb help shift around this simplex

219
00:13:07,490 --> 00:13:10,530
and if the permit is less than one

220
00:13:11,550 --> 00:13:12,450
so that

221
00:13:12,490 --> 00:13:13,740
instead of

222
00:13:13,760 --> 00:13:15,860
now concentrating around a little boy

223
00:13:15,910 --> 00:13:17,970
all the math counts rates

224
00:13:17,990 --> 00:13:20,680
and the cluster around the but this is

225
00:13:24,030 --> 00:13:29,100
so that's what the distribution of the black

226
00:13:32,120 --> 00:13:33,280
so this

227
00:13:35,140 --> 00:13:38,890
interesting properties of dirichlet distributions which is useful

228
00:13:38,930 --> 00:13:39,990
in terms of

229
00:13:40,010 --> 00:13:41,780
i dirichlet process

230
00:13:41,820 --> 00:13:45,090
the first one that's what i call an agglomerative

231
00:13:45,140 --> 00:13:47,640
property of dirichlet distributions

232
00:13:47,700 --> 00:13:51,180
let's say that i want your i e

233
00:13:51,240 --> 00:13:55,570
is drawn from one iteration with the help of one of our key

234
00:13:55,620 --> 00:13:56,470
and you pick

235
00:13:56,490 --> 00:13:59,220
one and i two and add them together

236
00:13:59,220 --> 00:14:01,370
this factor

237
00:14:01,390 --> 00:14:06,570
which belongs to the k minus one dimensional probability simplex

238
00:14:06,590 --> 00:14:07,450
the thing

239
00:14:07,510 --> 00:14:11,030
is it is also a dirichlet distribution

240
00:14:11,300 --> 00:14:13,360
but with the parameters

241
00:14:13,360 --> 00:14:18,260
out of one of the added together

242
00:14:18,280 --> 00:14:20,530
and generally if we have

243
00:14:20,530 --> 00:14:23,840
i want to i j if you think the partition of

244
00:14:24,720 --> 00:14:26,990
the should

245
00:14:27,030 --> 00:14:29,530
is partition of one k

246
00:14:29,550 --> 00:14:31,590
then we start

247
00:14:32,340 --> 00:14:35,090
for each but that it l

248
00:14:35,140 --> 00:14:36,470
i one

249
00:14:36,530 --> 00:14:40,200
the sum all the entries of high i within that subset

250
00:14:41,390 --> 00:14:43,840
this thing belong to that

251
00:14:43,890 --> 00:14:46,550
j dimensional probability simplex

252
00:14:46,590 --> 00:14:47,860
and this thing

253
00:14:48,180 --> 00:14:53,030
we also dirichlet distributed with the corresponding parameters some together

254
00:14:53,200 --> 00:14:55,570
it is an important property of

255
00:14:55,650 --> 00:14:58,930
patricia distributions which would be using them

256
00:14:59,070 --> 00:15:02,870
so the contents of the agglomerative property is one

257
00:15:03,070 --> 00:15:05,760
is that semantic property so

258
00:15:05,760 --> 00:15:10,360
in the case of the agglomerative property you pick who components of our

259
00:15:10,370 --> 00:15:14,030
of all back up high and we add them together in the case of the

260
00:15:14,030 --> 00:15:17,450
estimated with within one component of the vector and we into

261
00:15:18,340 --> 00:15:19,510
and the way we split

262
00:15:20,590 --> 00:15:22,220
is as follows so

263
00:15:22,220 --> 00:15:27,990
again we assume that i want to i is usually distributed with the parameter

264
00:15:28,220 --> 00:15:30,570
independently the

265
00:15:30,600 --> 00:15:33,530
how long how cool from a dirichlet

266
00:15:33,570 --> 00:15:36,280
from a two dimensional grid

267
00:15:36,340 --> 00:15:38,090
with parameters given by

268
00:15:38,090 --> 00:15:39,670
two tablespoons

269
00:15:39,750 --> 00:15:41,120
his hundred and eighty

270
00:15:42,420 --> 00:15:44,100
i needed to know more

271
00:15:44,210 --> 00:15:45,020
for this

272
00:15:46,460 --> 00:15:49,460
and so i had to do my own homework on it

273
00:15:49,510 --> 00:15:52,210
the least you see here

274
00:15:52,270 --> 00:15:54,770
what this character can do for you

275
00:15:54,780 --> 00:16:00,880
so you hundred eighty calories two people's tablespoons of put tablespoons very low fat

276
00:16:00,930 --> 00:16:02,660
and the rest may interest you

277
00:16:02,710 --> 00:16:04,330
before you use it

278
00:16:04,420 --> 00:16:06,140
i had no c one

279
00:16:06,150 --> 00:16:08,330
which i calculated and measured it

280
00:16:08,350 --> 00:16:09,520
in fact

281
00:16:09,620 --> 00:16:13,030
what kind of demonstration i will be doing today you can

282
00:16:13,080 --> 00:16:16,250
the right but it's very strongly temperature dependent

283
00:16:16,300 --> 00:16:19,150
could be different yesterday from today

284
00:16:19,200 --> 00:16:22,160
i measured c two two reasonable accuracy

285
00:16:22,170 --> 00:16:24,040
notice that the density

286
00:16:24,080 --> 00:16:25,270
of the

287
00:16:26,420 --> 00:16:30,340
in terms of kilograms per cubic metre is very close to see two i mentioned

288
00:16:31,960 --> 00:16:34,670
very close not exactly but very close

289
00:16:34,770 --> 00:16:39,180
the steel ball bearings have a density of about seventy eight hundred

290
00:16:40,340 --> 00:16:43,830
cubic meter and i'm going to drop in that carries zero

291
00:16:43,900 --> 00:16:45,700
four ball bearings

292
00:16:45,740 --> 00:16:49,450
and they have diameters of an a challenge

293
00:16:49,500 --> 00:16:51,730
o five thirty seconds three sixteen

294
00:16:52,640 --> 00:16:55,970
a quarter of an inch

295
00:16:56,030 --> 00:16:57,800
and what i calculated

296
00:16:57,850 --> 00:16:59,340
it was the

297
00:16:59,390 --> 00:17:02,680
terminal velocity as a function of radius

298
00:17:02,690 --> 00:17:05,450
all these ball bearings

299
00:17:05,530 --> 00:17:09,030
all of this is on the web

300
00:17:09,110 --> 00:17:12,740
so what you see here it is a logarithmic plot this is the log scale

301
00:17:12,740 --> 00:17:15,210
and this is a log scale you see this being

302
00:17:15,230 --> 00:17:18,870
and he was you the is in meters of the ball bearings

303
00:17:18,960 --> 00:17:20,290
and this

304
00:17:20,320 --> 00:17:23,050
is my solution to equation number one

305
00:17:23,070 --> 00:17:27,830
when i substitute various values of in there i get the terminal

306
00:17:27,840 --> 00:17:30,600
speed like this

307
00:17:30,640 --> 00:17:32,310
and this is the critical

308
00:17:32,360 --> 00:17:35,650
speed which is as one of our relationship

309
00:17:35,730 --> 00:17:37,740
if you look at these black dots here

310
00:17:37,760 --> 00:17:42,650
then the terminal speed is ten times larger year

311
00:17:42,700 --> 00:17:44,730
then the critical speed

312
00:17:44,770 --> 00:17:47,940
and so i noticed that when you are at speeds above that

313
00:17:47,990 --> 00:17:49,030
the during

314
00:17:49,080 --> 00:17:51,430
exclusively in domain two

315
00:17:51,440 --> 00:17:52,920
and your terminal

316
00:17:52,930 --> 00:17:57,160
speed is proportional to the square root of the radius of the ball bearings

317
00:17:57,890 --> 00:18:01,400
black dot is a factor of ten below the critical speed

318
00:18:01,490 --> 00:18:05,260
so you see when you at low speeds when you work here

319
00:18:06,110 --> 00:18:09,630
you see that you fall into exclusively in two main one

320
00:18:09,640 --> 00:18:14,130
and you see that the terminal speed is proportional to our square this slope here

321
00:18:14,850 --> 00:18:16,080
plus two

322
00:18:16,090 --> 00:18:18,290
this diagram in this slope here

323
00:18:18,330 --> 00:18:20,100
this plus one half

324
00:18:20,150 --> 00:18:22,800
are ball bearings all year

325
00:18:22,850 --> 00:18:27,490
and so we are exclusively operating in regime one with viscous terms

326
00:18:29,060 --> 00:18:32,460
now you could say well what is the meaning of this critical

327
00:18:32,470 --> 00:18:36,250
speedy here if they never reach that speed any out

328
00:18:37,220 --> 00:18:41,670
this critical speed for small ball bearings would be some hundred meters per second that's

329
00:18:41,670 --> 00:18:43,640
about two hundred miles per hour

330
00:18:43,660 --> 00:18:45,990
there's nothing wrong with injecting

331
00:18:46,000 --> 00:18:48,860
a ball-bearing with four hundred miles per hour into

332
00:18:48,870 --> 00:18:50,680
this era in which case

333
00:18:50,720 --> 00:18:53,970
if you inject it with four hundred miles per hour you will be above the

334
00:18:55,290 --> 00:18:56,400
speed and so

335
00:18:56,430 --> 00:18:57,960
for a short while

336
00:18:58,010 --> 00:18:59,380
what emotion b

337
00:18:59,460 --> 00:19:00,920
controlled by

338
00:19:00,970 --> 00:19:03,930
the pressure term but of course when gravity takes over

339
00:19:03,940 --> 00:19:05,690
you ultimately end up

340
00:19:05,700 --> 00:19:07,440
in regione

341
00:19:07,520 --> 00:19:11,530
but that's the meaning of the critical velocity if you could give the ball bearings

342
00:19:11,550 --> 00:19:13,400
such a high speed

343
00:19:14,160 --> 00:19:15,880
the two terms i

344
00:19:15,890 --> 00:19:18,500
equal that's all it means

345
00:19:18,520 --> 00:19:21,240
very well now we are going to

346
00:19:21,260 --> 00:19:24,120
look at the the various

347
00:19:24,170 --> 00:19:27,000
ball bearings various sizes

348
00:19:27,980 --> 00:19:31,520
i'll show you how we do the experiment you will shortly see

349
00:19:31,540 --> 00:19:34,670
a on the screen there

350
00:19:34,690 --> 00:19:36,870
seven marks

351
00:19:36,880 --> 00:19:39,540
which are one centimeter apart

352
00:19:39,550 --> 00:19:43,530
there in the liquid one two three four five six seven

353
00:19:43,600 --> 00:19:44,990
here is the liquid

354
00:19:45,040 --> 00:19:47,770
and the ball bearings drop from above

355
00:19:47,790 --> 00:19:51,310
when it reaches this line i will start my time

356
00:19:51,420 --> 00:19:56,540
and when it crosses one two three four when it crosses this line

357
00:19:56,550 --> 00:19:58,250
i will stop my time

358
00:19:58,790 --> 00:20:01,530
each mark is about one centimeter apart

359
00:20:01,540 --> 00:20:02,780
so this

360
00:20:02,850 --> 00:20:06,520
is a journey of about four centimeters

361
00:20:06,620 --> 00:20:09,580
and we'll measure the time it takes to go from here

362
00:20:10,280 --> 00:20:12,150
here and the terminal

363
00:20:13,590 --> 00:20:14,350
is given

364
00:20:14,360 --> 00:20:17,450
clearly regime once you see the terminal velocity

365
00:20:20,780 --> 00:20:23,720
not at the time that it will take

366
00:20:23,730 --> 00:20:26,600
is of course this this and let me call it

367
00:20:28,530 --> 00:20:30,250
ball bearings travel

368
00:20:30,290 --> 00:20:31,800
divided by t

369
00:20:31,850 --> 00:20:33,570
terminal velocity

370
00:20:33,580 --> 00:20:36,950
and that is proportional to jue in which she was

371
00:20:37,000 --> 00:20:39,710
by one of our scripts

372
00:20:39,760 --> 00:20:41,880
now i will give you the

373
00:20:41,890 --> 00:20:45,550
not the radio but i'll give you the diameters of these ball bearings that's the

374
00:20:46,410 --> 00:20:47,750
they come

375
00:20:47,800 --> 00:20:50,120
so we're going to get a list here

376
00:20:50,140 --> 00:20:52,740
of the diameters

377
00:20:52,750 --> 00:20:55,810
of the barbarians and the amateurs is in inches

378
00:20:55,820 --> 00:20:59,300
i smallest one has diameter of and a two minutes

379
00:20:59,320 --> 00:21:00,750
that i have

380
00:21:00,800 --> 00:21:02,750
o five thirty seconds

381
00:21:02,760 --> 00:21:04,510
i have three six things

382
00:21:05,140 --> 00:21:07,690
all the things come inches that's one of those things

383
00:21:07,730 --> 00:21:13,150
and i have one quarter inch diameter

384
00:21:13,200 --> 00:21:18,270
if i plot here plot if i give you here the diameter in terms of

385
00:21:18,280 --> 00:21:20,480
one thirty seconds of an end

386
00:21:20,560 --> 00:21:22,130
and this is for

387
00:21:22,140 --> 00:21:23,840
five six

388
00:21:23,880 --> 00:21:24,730
and eight

389
00:21:27,620 --> 00:21:31,270
if the time that it takes is proportional to one of our school and it

390
00:21:31,270 --> 00:21:37,300
will also be proportional to one over the square score that's the same

391
00:21:37,320 --> 00:21:39,110
what i'm going to plot

392
00:21:39,130 --> 00:21:40,300
is not

393
00:21:40,360 --> 00:21:43,690
one of the squares but to get some nice numbers

394
00:21:43,730 --> 00:21:45,300
i'm going to plot for you

395
00:21:46,130 --> 00:21:48,430
hundreds of these grids

396
00:21:48,430 --> 00:21:52,500
well by d is then in these units one thirty second of an inch

397
00:21:52,550 --> 00:21:56,000
and that gives me some nice numbers then i get to six point two five

398
00:21:56,800 --> 00:21:58,520
i get the four point

399
00:21:58,530 --> 00:22:02,480
zero zero you can see a hundred divided by twenty five

400
00:22:02,500 --> 00:22:03,950
is exactly four

401
00:22:03,960 --> 00:22:05,260
i get two point

402
00:22:05,280 --> 00:22:10,150
seven eight here and my last number is one point five six

403
00:22:10,180 --> 00:22:12,290
and now i'm going to time

404
00:22:12,350 --> 00:22:15,140
and my timing uncertainty

405
00:22:15,150 --> 00:22:18,350
it is of course dictated by my reaction time

406
00:22:18,410 --> 00:22:22,230
that should be at least o point one second

407
00:22:22,270 --> 00:22:25,690
however you will see when i reach the quarter inch

408
00:22:25,690 --> 00:22:30,230
ball bearings that goes so fast that my ever could well be tens of the

409
00:22:30,240 --> 00:22:32,480
second goes in the flesh

410
00:22:33,950 --> 00:22:36,210
i would allow you to tens of the second

411
00:22:36,220 --> 00:22:41,100
and here i really don't know maybe one one-tenth so maybe two-tenths of a second

412
00:22:41,100 --> 00:22:45,290
gives you a big negative term low energy that's good is happy network

413
00:22:45,300 --> 00:22:47,190
so happy networks are ones where

414
00:22:47,220 --> 00:22:50,940
the binary is an on the positive weights between

415
00:22:50,960 --> 00:22:54,680
one nice property of this is if you differentiate this energy with the rest of

416
00:22:54,680 --> 00:22:58,080
the negative and she would like to wait you just get this product of states

417
00:22:58,820 --> 00:23:02,800
so the jury is very easy to manipulate the energies

418
00:23:04,140 --> 00:23:05,640
by changing the weights

419
00:23:05,720 --> 00:23:09,100
those are the derivatives

420
00:23:09,110 --> 00:23:13,290
and that

421
00:23:13,330 --> 00:23:16,030
and we want to do is

422
00:23:16,620 --> 00:23:20,950
change the way to change the energies to change the probabilities so that the log

423
00:23:20,950 --> 00:23:23,900
probability of the data is high

424
00:23:23,920 --> 00:23:26,890
so for joint configurations

425
00:23:26,930 --> 00:23:29,040
the visible hidden that

426
00:23:29,080 --> 00:23:34,970
the probability of the joint configurations for e minus the energy if

427
00:23:35,030 --> 00:23:37,290
you run the net in the right way

428
00:23:37,300 --> 00:23:40,290
so if you take one of these nets

429
00:23:40,340 --> 00:23:42,550
and i just go backwards and forwards

430
00:23:42,910 --> 00:23:46,860
updating all these units in parallel and updating all these units in parallel

431
00:23:46,910 --> 00:23:50,190
using the the majestic to pick binary states

432
00:23:50,320 --> 00:23:55,950
then if i don't have this week what's called equilibrium distribution or stationary distribution and

433
00:23:55,950 --> 00:23:57,790
in that stationary distribution

434
00:23:57,810 --> 00:24:02,250
it will be sampling

435
00:24:02,310 --> 00:24:06,430
during vectors in proportion to e to the minus energy

436
00:24:06,480 --> 00:24:08,950
so learned you ones will be some a lot

437
00:24:15,940 --> 00:24:17,550
so the probability of

438
00:24:17,610 --> 00:24:20,040
during that will be e to the minus its energy

439
00:24:20,100 --> 00:24:21,540
normalized by

440
00:24:21,550 --> 00:24:26,420
the same quantity over all all possible pairs of the visible and hidden that

441
00:24:26,450 --> 00:24:28,740
and if you want the probability visible that

442
00:24:28,750 --> 00:24:31,850
just marginalize out age some lateral h

443
00:24:31,850 --> 00:24:32,700
get that

444
00:24:32,720 --> 00:24:35,160
so rose to learning we just need to

445
00:24:35,210 --> 00:24:38,800
take logs and take derivatives and now we can just all the weights to change

446
00:24:38,830 --> 00:24:43,270
and is the appropriate way

447
00:24:43,280 --> 00:24:45,460
and when you do that

448
00:24:45,480 --> 00:24:47,840
you very simple learning rule

449
00:24:47,840 --> 00:24:49,720
so what's nice about both machines

450
00:24:49,920 --> 00:24:55,520
this stuff was only discovered a few years later by people doing maxent

451
00:24:57,950 --> 00:25:00,950
run this markov chain where you start with some data

452
00:25:00,960 --> 00:25:03,770
you activate the hidden units use

453
00:25:03,800 --> 00:25:07,130
reactivated the pixels from the hidden units and you go

454
00:25:07,180 --> 00:25:09,770
exports until you reach equilibria

455
00:25:09,910 --> 00:25:11,890
you could take a long time

456
00:25:11,940 --> 00:25:15,470
and you're getting fantasies from the model that is the kind of thing the model

457
00:25:15,470 --> 00:25:18,890
believes being generated and this completely forgotten where it started

458
00:25:18,900 --> 00:25:20,810
that's the definition of

459
00:25:21,280 --> 00:25:25,780
and when you show your data

460
00:25:25,790 --> 00:25:28,490
you measure the correlation between

461
00:25:28,550 --> 00:25:30,700
the binary states

462
00:25:30,720 --> 00:25:35,230
six thousand buyers states feature detectors visible units and units

463
00:25:35,400 --> 00:25:40,620
that zero here means when you showing your data times zero in this markov chain

464
00:25:40,670 --> 00:25:45,580
and the angle brackets just physics for the expected value of

465
00:25:45,620 --> 00:25:48,100
i don't like using because i can't even manage

466
00:25:49,690 --> 00:25:51,890
you do this measure the same statistic

467
00:25:51,900 --> 00:25:55,390
when the model is sampled from the things he believes in

468
00:25:55,450 --> 00:25:58,020
and hey presto the learning algorithm is

469
00:25:58,030 --> 00:26:01,080
if you want to maximize the log probabilities well that

470
00:26:01,090 --> 00:26:05,320
measure the statistical the visible that the prime here

471
00:26:05,330 --> 00:26:08,990
and these angle brackets are kind of average over the noise in these guys

472
00:26:09,050 --> 00:26:12,480
and the same statistical on the models of the program

473
00:26:12,540 --> 00:26:13,620
that is

474
00:26:13,630 --> 00:26:16,020
the derivative of the log prob

475
00:26:16,020 --> 00:26:17,770
so we got a very simple and will

476
00:26:17,780 --> 00:26:20,690
and its local it only depends on

477
00:26:20,710 --> 00:26:24,090
the behaviour of the two units of the way connects

478
00:26:24,090 --> 00:26:27,370
which is quite surprising because the derivatives here

479
00:26:27,400 --> 00:26:30,050
depends only on the weights in the network

480
00:26:30,100 --> 00:26:33,920
but its dependence on all the other weights in the network shows up this difference

481
00:26:33,920 --> 00:26:37,740
of correlations so the process of settling to equilibrium

482
00:26:37,800 --> 00:26:40,300
it's kind of propagating information around the net

483
00:26:40,310 --> 00:26:44,450
in order to produce these statistics and those statistics are telling you

484
00:26:44,500 --> 00:26:46,550
the telling this weight w i j

485
00:26:46,560 --> 00:26:49,190
everything you need to know about all the other weights

486
00:26:49,210 --> 00:26:53,370
so is in backpropagation explicitly by the chain rule going backwards

487
00:26:53,400 --> 00:26:55,220
this just runs this

488
00:26:55,230 --> 00:27:01,950
also gibbs sampling and that somehow gets the information to statistics

489
00:27:04,900 --> 00:27:08,590
this rule we figured out the general case in the

490
00:27:08,630 --> 00:27:09,840
early eighties

491
00:27:09,840 --> 00:27:12,560
and then i thought this kind of that was very interesting because it's only got

492
00:27:12,560 --> 00:27:14,740
one hidden layer with no connections between them

493
00:27:14,800 --> 00:27:17,420
this just the sort of boring special case

494
00:27:17,520 --> 00:27:22,840
many years later i maybe got million times as fast

495
00:27:22,890 --> 00:27:26,870
so the way making a million times faster service

496
00:27:29,150 --> 00:27:31,880
instead of running along change in equilibrium

497
00:27:31,890 --> 00:27:33,220
one hundred steps

498
00:27:33,250 --> 00:27:35,280
you just go on and off again

499
00:27:35,420 --> 00:27:38,450
that is not actually quite the right algorithm but it works quite nicely

500
00:27:38,570 --> 00:27:40,840
there's a post writing maybe today

501
00:27:40,850 --> 00:27:44,340
tony conditions under which it doesn't work there's all sorts of conditions you make this

502
00:27:44,340 --> 00:27:45,790
graph but what

503
00:27:45,800 --> 00:27:49,120
when i say

504
00:27:49,130 --> 00:27:51,730
so you got you come down you go up against the new measure the difference

505
00:27:51,730 --> 00:27:53,270
in these statistics

506
00:27:53,310 --> 00:27:56,970
and sort of what you're seeing is what the statistics are with data

507
00:27:57,040 --> 00:27:59,970
and you seem sort of the direction in which is heading is it goes towards

508
00:27:59,970 --> 00:28:03,960
equilibrium and how this this beginning to change the model gets to have a say

509
00:28:03,960 --> 00:28:05,960
about what should be going on

510
00:28:05,980 --> 00:28:09,290
and that difference is enough to allow you to do learning

511
00:28:09,290 --> 00:28:13,620
so and we'll see later a much better justification for using this

512
00:28:13,630 --> 00:28:15,620
because you more insight into it

513
00:28:15,670 --> 00:28:18,610
and this is very justified when the weights are quite small

514
00:28:18,640 --> 00:28:23,200
but we can substitute the learning allows infinity here

515
00:28:23,240 --> 00:28:26,150
for this little c to go up down

516
00:28:26,160 --> 00:28:30,590
so now that goes a million times faster reasoning is a million times as fast

517
00:28:30,680 --> 00:28:33,700
is because it's a hundred times less computation

518
00:28:33,720 --> 00:28:36,050
and it took me seventeen years to think of it

519
00:28:36,050 --> 00:28:40,140
and in the seventeen years computers got ten thousand times faster

520
00:28:40,210 --> 00:28:45,470
it was very important to take a long time to think of it

521
00:28:46,340 --> 00:28:49,670
so i'm going show you a little example

522
00:28:49,690 --> 00:28:53,100
where you take some little hundred digits

523
00:28:54,900 --> 00:28:56,990
you have only fifty feature detector neurons

524
00:28:57,000 --> 00:28:59,180
you start off with all random weights

525
00:28:59,190 --> 00:29:03,900
right click the detectors you reconstruct the pixels activity sectors again

526
00:29:04,040 --> 00:29:05,580
and you can think of it is

527
00:29:05,580 --> 00:29:06,960
with the data there

528
00:29:07,030 --> 00:29:10,090
every time a is active common with slightly

529
00:29:10,130 --> 00:29:12,680
when you've got reconstructions

530
00:29:12,690 --> 00:29:18,350
every time a pair of the feature vector pixel active you going decrement weights slightly

531
00:29:18,360 --> 00:29:20,300
one way of thinking of it is

532
00:29:20,310 --> 00:29:23,120
i think it gives some insight into what the algorithm that two

533
00:29:24,630 --> 00:29:26,180
the reconstructions

534
00:29:26,200 --> 00:29:30,910
will typically be things that the network is how here where is the reality

535
00:29:30,950 --> 00:29:34,530
the show some reality it represents the reality

536
00:29:34,570 --> 00:29:37,150
then you ask you to reconstruct the reality

537
00:29:37,170 --> 00:29:42,140
and the reconstructed with the sort of surprising corners not off the reconstruct something smoother

538
00:29:42,160 --> 00:29:43,270
more regularly

539
00:29:43,280 --> 00:29:45,560
it's something that prefers to believe

540
00:29:45,720 --> 00:29:48,360
on the training of community consists saying this

541
00:29:48,390 --> 00:29:50,500
take the stuff you'd like to believe

542
00:29:50,530 --> 00:29:52,380
i don't believe it

543
00:29:52,480 --> 00:29:55,690
take the stuff is actually there and you believe it

544
00:29:55,740 --> 00:30:00,360
george bush used to run the other algorithms

545
00:30:07,390 --> 00:30:10,830
what i'm showing you here is the weights the little network like this learns we

546
00:30:10,850 --> 00:30:13,120
show lots of images to use

547
00:30:13,130 --> 00:30:16,910
and so each of these big squares is the a feature vector

548
00:30:16,930 --> 00:30:21,000
and the show you the strengths of the two hundred fifty six connections to the

549
00:30:21,000 --> 00:30:24,390
they are discarded transcription

550
00:30:24,420 --> 00:30:26,610
and this

551
00:30:26,640 --> 00:30:34,490
RNA messenger RNA they can be translated to proteins so when i put an arrow

552
00:30:34,490 --> 00:30:40,120
from the june one to produce one i'm just saying that one gene produces one

553
00:30:41,160 --> 00:30:46,790
and you will see that it's not the direct harold you must first produce

554
00:30:46,830 --> 00:30:53,980
messenger RNA and then this is our messenger RNA is introduced into a pretty

555
00:30:54,370 --> 00:30:56,750
and the genes can

556
00:30:56,760 --> 00:30:59,180
for several point eight

557
00:30:59,930 --> 00:31:01,200
once you have these

558
00:31:01,230 --> 00:31:03,850
protein these proteins can

559
00:31:03,860 --> 00:31:05,620
in fact do many

560
00:31:05,860 --> 00:31:08,750
it can be fine sense here

561
00:31:08,770 --> 00:31:09,750
i'm not designed

562
00:31:09,770 --> 00:31:11,980
specific what they called design

563
00:31:11,990 --> 00:31:14,060
which is going to catalyze

564
00:31:14,080 --> 00:31:17,550
some bioc biochemical reaction

565
00:31:17,560 --> 00:31:19,320
with w one

566
00:31:19,330 --> 00:31:23,990
and with the body that that's transforming into metabolite to

567
00:31:24,100 --> 00:31:25,250
i would like to

568
00:31:25,270 --> 00:31:29,880
by this chemical reactions but also pretty

569
00:31:29,890 --> 00:31:31,870
o point but things can be

570
00:31:31,960 --> 00:31:38,750
can interact with another protein physical interaction and and from the and also called a

571
00:31:38,750 --> 00:31:40,640
complex complex put

572
00:31:40,670 --> 00:31:44,640
and contains our so

573
00:31:44,670 --> 00:31:47,990
very active in a different kind of

574
00:31:48,160 --> 00:31:50,430
activities in the sense

575
00:31:50,440 --> 00:31:51,690
but here

576
00:31:51,800 --> 00:31:54,300
we have we have only

577
00:31:54,310 --> 00:31:58,190
as interactions between molecule between proteins

578
00:31:59,420 --> 00:32:04,460
and was the w h

579
00:32:05,740 --> 00:32:09,060
so what's interesting here

580
00:32:09,080 --> 00:32:13,520
in fact when the seller has to respond to some input signal

581
00:32:13,550 --> 00:32:18,440
for instance the scene and maintenance in which says

582
00:32:19,920 --> 00:32:22,610
i put quotes here which saved now

583
00:32:22,620 --> 00:32:24,410
make differenciation

584
00:32:24,420 --> 00:32:28,370
so so as to respond and as two

585
00:32:28,380 --> 00:32:34,070
how to control in the way that it responds and noise is controlled

586
00:32:34,100 --> 00:32:37,020
is implemented by a network

587
00:32:37,040 --> 00:32:40,810
in fact the network at the level of

588
00:32:40,830 --> 00:32:42,810
gene regulatory network

589
00:32:42,830 --> 00:32:49,980
so genes in fact interact and make possible the production of pertains

590
00:32:50,040 --> 00:32:53,860
but there some proteins interact physically

591
00:32:53,910 --> 00:32:54,880
and this

592
00:32:54,900 --> 00:32:57,310
matrix is called

593
00:32:57,320 --> 00:32:59,670
protein protein interaction network

594
00:32:59,680 --> 00:33:01,910
and of course the

595
00:33:01,920 --> 00:33:05,350
seven hours of live in you

596
00:33:05,440 --> 00:33:12,410
live in being is is alive and their metabolism and they are biochemical reactions this

597
00:33:12,410 --> 00:33:14,540
level which is sensitive

598
00:33:14,560 --> 00:33:16,350
metabolic network

599
00:33:16,440 --> 00:33:20,180
so in fact we have another network which is called signalling

600
00:33:20,180 --> 00:33:22,410
the best way of signaling networks

601
00:33:22,420 --> 00:33:30,060
it's usually the network which involves for instance hormones and different kind of

602
00:33:30,070 --> 00:33:35,060
internet formations that are going to

603
00:33:35,060 --> 00:33:36,460
two BBC news

604
00:33:37,430 --> 00:33:38,990
two proteins

605
00:33:39,020 --> 00:33:41,180
in general all eventually two

606
00:33:41,190 --> 00:33:42,180
thirty two

607
00:33:42,180 --> 00:33:43,110
two genes

608
00:33:43,130 --> 00:33:46,060
but here i'm just going to talk about

609
00:33:46,080 --> 00:33:48,300
gene regulatory networks

610
00:33:48,310 --> 00:33:54,950
protein protein interaction network and we want to go into detail for metabolic networks but

611
00:33:54,950 --> 00:34:01,150
it's also very very important in bioinformatics in fact you should know how to engineer

612
00:34:01,160 --> 00:34:02,540
the metabolism

613
00:34:02,560 --> 00:34:09,330
of the bacteria for instance you can use if you want to to to

614
00:34:09,350 --> 00:34:16,180
to process since your garbage is all you can really engineers some bacteria of all

615
00:34:16,180 --> 00:34:19,570
the small micro organisms to do something for you

616
00:34:19,700 --> 00:34:21,920
so it's really important

617
00:34:21,940 --> 00:34:22,560
to be

618
00:34:22,630 --> 00:34:29,810
two no one twenty to understand to manage with all these three levels

619
00:34:29,850 --> 00:34:31,970
from another point of view

620
00:34:32,400 --> 00:34:34,440
if you know how

621
00:34:34,440 --> 00:34:36,920
the same react

622
00:34:36,920 --> 00:34:43,250
so this is one way to continue when you're in QA and with this automaton

623
00:34:43,270 --> 00:34:45,180
is deterministic

624
00:34:50,560 --> 00:34:54,380
it that it's not a singleton animals set

625
00:34:56,450 --> 00:34:58,950
and yet it has the notion of

626
00:35:01,270 --> 00:35:04,970
nondeterministic nondeterminism so

627
00:35:09,160 --> 00:35:11,450
gonna play with different ones

628
00:35:13,750 --> 00:35:15,590
but we are interested in

629
00:35:15,660 --> 00:35:20,630
given one computer to declare this computation is successful

630
00:35:26,340 --> 00:35:29,620
i would say that this computation is successful

631
00:35:29,830 --> 00:35:36,860
it took several successful it's branch of this country

632
00:35:39,480 --> 00:35:41,730
i'm gonna give you what authority

633
00:35:41,890 --> 00:35:46,390
property the tree automata because it's the tree automata

634
00:35:48,740 --> 00:35:53,400
this is done the definition of what is x what is a successful run

635
00:35:55,930 --> 00:35:57,340
and you put all

636
00:35:59,690 --> 00:36:00,840
i mean

637
00:36:00,850 --> 00:36:01,660
two two

638
00:36:01,690 --> 00:36:06,440
two your quite successful will have different kinds of treatments will have

639
00:36:08,840 --> 00:36:12,370
the moment that whatever and it's always a matter of

640
00:36:14,910 --> 00:36:16,310
looking for

641
00:36:18,780 --> 00:36:20,680
so parity

642
00:36:20,690 --> 00:36:21,840
so i come back

643
00:36:21,870 --> 00:36:22,920
the issue

644
00:36:25,300 --> 00:36:29,840
that's the way you branch a new form a tree like can very e

645
00:36:32,270 --> 00:36:33,750
something which

646
00:36:33,770 --> 00:36:34,940
kind of

647
00:36:35,260 --> 00:36:38,840
as the number two in the states

648
00:36:44,540 --> 00:36:45,990
OK so state

649
00:36:50,870 --> 00:36:53,450
finance state finance set

650
00:36:53,570 --> 00:36:59,490
that's the way i will leave when they find so this is given OK specifi

651
00:36:59,490 --> 00:37:02,210
definition of around

652
00:37:07,480 --> 00:37:08,390
on the

653
00:37:10,740 --> 00:37:16,090
on the example just the formal definition here it doesn't matter

654
00:37:18,440 --> 00:37:19,680
this is what you

655
00:37:33,360 --> 00:37:37,760
about three which causes which is called one zero

656
00:37:42,300 --> 00:37:44,990
i have a in the input

657
00:37:45,150 --> 00:37:46,330
and i have QA

658
00:37:46,500 --> 00:37:49,190
that was my formula

659
00:37:51,590 --> 00:37:54,430
so what i choose

660
00:37:56,760 --> 00:37:58,800
in my in my previous here

661
00:37:58,850 --> 00:38:00,250
i look at it

662
00:38:02,600 --> 00:38:05,160
in my input rate

663
00:38:05,180 --> 00:38:08,200
was the way and this is a

664
00:38:10,640 --> 00:38:12,970
in my examples

665
00:38:15,380 --> 00:38:18,370
in the set

666
00:38:18,410 --> 00:38:23,270
number ten our example had only once one l

667
00:38:28,040 --> 00:38:32,840
because it was deterministic but in general you may have different

668
00:38:35,440 --> 00:38:37,800
so i to pick up and

669
00:38:38,220 --> 00:38:39,840
down in computation

670
00:38:42,060 --> 00:38:44,180
that some

671
00:38:44,560 --> 00:38:49,150
o my lord

672
00:38:49,180 --> 00:38:51,510
and what should be the this

673
00:38:55,870 --> 00:38:58,130
is around if it's away

674
00:39:00,350 --> 00:39:03,820
with states

675
00:39:04,340 --> 00:39:09,030
one of the problems for binary tree

676
00:39:13,670 --> 00:39:14,930
so that

677
00:39:14,970 --> 00:39:20,950
the labeling is locally called ondition relation tells

678
00:39:23,180 --> 00:39:26,440
around finite automata is the sequence

679
00:39:28,440 --> 00:39:31,450
each position

680
00:39:33,410 --> 00:39:39,250
from one state to the next one of functional relation that is given for to

681
00:39:41,470 --> 00:39:44,450
this is exactly justify that is the computation

682
00:39:46,480 --> 00:39:50,880
you have to put some label the full state here

683
00:39:50,880 --> 00:39:53,320
OK alright

684
00:39:53,340 --> 00:39:56,440
what i'd like to do i got a lot of questions afterwards yesterday about continuous

685
00:39:56,440 --> 00:39:59,320
space models of i just that was

686
00:39:59,360 --> 00:40:02,490
i'm done with the thing now moving on to different topics in different topic is

687
00:40:03,400 --> 00:40:07,400
is well it turns out computation kind of matters which may be is not so

688
00:40:09,190 --> 00:40:12,860
you could say the planning is just the convex so what we can do at

689
00:40:12,860 --> 00:40:17,630
this point we can learn very sophisticated models of potentially very very big world

690
00:40:17,650 --> 00:40:21,970
environment but we can't act in necessarily must we can do the planning you could

691
00:40:21,970 --> 00:40:26,090
say plan is just a computational problem is not learning problem but

692
00:40:26,110 --> 00:40:31,210
we can now learn much larger environments then we can plan

693
00:40:31,970 --> 00:40:36,380
that's not good so something we'd have to do something or these models that we

694
00:40:36,380 --> 00:40:40,990
spend so much time learning are useless we can plan

695
00:40:43,470 --> 00:40:46,090
so let me tell you about a trick that p a couple people have used

696
00:40:46,150 --> 00:40:49,490
i've never seen described quite this way before but i think it's a useful way

697
00:40:49,490 --> 00:40:53,280
to look at so yesterday i told you about three classes of reinforcement learning algorithm

698
00:40:53,380 --> 00:40:56,240
policy search value function based

699
00:40:56,260 --> 00:41:01,280
learning and model based learning so what's happening in this case is here we we

700
00:41:01,280 --> 00:41:03,070
search for policy directly

701
00:41:03,130 --> 00:41:07,220
by interacting with the environment here we interact with the environment and learn value function

702
00:41:07,220 --> 00:41:11,630
and the policy is generated from that computationally here we interact with the environment to

703
00:41:11,630 --> 00:41:17,150
learn the transitions and rewards and computationally create value function and the policy from that

704
00:41:17,170 --> 00:41:20,090
OK so that sort of seems to cover everything

705
00:41:20,110 --> 00:41:21,990
but here's the thing to notice

706
00:41:23,570 --> 00:41:29,190
once we have learned transition and reward function o in this computation here's is what's

707
00:41:29,190 --> 00:41:32,720
taking too long so what can we do to make this computation cheaper

708
00:41:32,720 --> 00:41:36,300
well one thing to notice is that this transition in the and the reward function

709
00:41:36,300 --> 00:41:41,220
here constitute like a little many environment like a little copy of the world inside

710
00:41:41,220 --> 00:41:42,840
the agents had

711
00:41:42,900 --> 00:41:45,970
and so what can you do when you've got a world that you can interact

712
00:41:45,970 --> 00:41:47,670
with you can do

713
00:41:47,670 --> 00:41:50,010
reinforcement learning

714
00:41:50,940 --> 00:41:54,720
so here's an approach an odd approach or possibly out approach but it turns out

715
00:41:54,720 --> 00:41:55,300
to be

716
00:41:55,380 --> 00:41:59,940
extremely insightful policy search inside model based methods so

717
00:41:59,970 --> 00:42:04,420
this out of boxes when interact with the world to learn the model

718
00:42:04,420 --> 00:42:06,550
transition model and reward function

719
00:42:06,550 --> 00:42:12,260
then inside the agent's head it's going to interact with its model to learn the

720
00:42:12,260 --> 00:42:17,130
policy so it's doing policy search reinforcement learning all in

721
00:42:17,150 --> 00:42:18,550
mental space

722
00:42:18,550 --> 00:42:21,260
OK to come up with a policy which they can can then run in the

723
00:42:21,260 --> 00:42:23,050
real world

724
00:42:23,070 --> 00:42:27,690
why is agood well some of these policies search methods are actually pretty efficient

725
00:42:27,720 --> 00:42:31,340
computationally even if they're not great in terms of the data we can get all

726
00:42:31,340 --> 00:42:34,510
the data we want now just by querying this model it's this their first day

727
00:42:34,530 --> 00:42:36,320
and night so

728
00:42:36,340 --> 00:42:39,400
so there's one thought and the related that we can go up another level and

729
00:42:40,010 --> 00:42:41,650
OK well we can

730
00:42:41,650 --> 00:42:45,630
user experience to learn transition and reward functions and then

731
00:42:45,690 --> 00:42:49,980
pretending that world is the real world learning value function from and use that i

732
00:42:49,980 --> 00:42:52,170
function generative model

733
00:42:52,920 --> 00:42:59,670
these ideas are actually surprisingly helpful because all the crazy approximations that people have come

734
00:42:59,670 --> 00:43:02,240
up with for doing value function

735
00:43:02,260 --> 00:43:06,340
reinforcement learning and doing policy search we can use them in the context of the

736
00:43:06,340 --> 00:43:07,510
model based methods

737
00:43:10,720 --> 00:43:14,130
all these things are trying to sell the bellman equation but but

738
00:43:14,150 --> 00:43:18,110
this one the what i meant by this picture is just literally trying to solve

739
00:43:18,110 --> 00:43:23,720
the bellman equation strictly computationally but one way of approximating that is by throwing reinforcement

740
00:43:23,720 --> 00:43:27,960
learning tools at just in simulation so it is trying to silence trying to solve

741
00:43:27,960 --> 00:43:32,570
the model these all these guys are trying to solve the model but the particular

742
00:43:32,570 --> 00:43:37,850
techniques that are being used in these boxes are themselves describable as reinforcement learning methods

743
00:43:37,900 --> 00:43:41,610
which just not learning from real data the learning from the model

744
00:43:41,670 --> 00:43:45,310
well there are many

745
00:43:45,310 --> 00:43:51,490
policy search will in this in this box here yes so it's basically you propose

746
00:43:51,490 --> 00:43:54,560
a policy in your head then you run it in your head and if it's

747
00:43:54,560 --> 00:43:57,560
good then you run in the real world if it's not then you change in

748
00:43:57,560 --> 00:43:59,810
your head and then rewrite it in your head to see if you can improve

749
00:43:59,810 --> 00:44:03,890
it so you can do various kinds of hill climbing gradient ascent or other clever

750
00:44:03,890 --> 00:44:08,710
techniques to come up with ways of improving this policy relative to the model

751
00:44:08,720 --> 00:44:11,210
right and then what you have that what you think it's good enough for the

752
00:44:11,210 --> 00:44:14,220
for the model then you release it into the real world

753
00:44:14,270 --> 00:44:15,430
so no additional

754
00:44:15,430 --> 00:44:20,980
real world experience was needed for the tuning of that policy so here's what i

755
00:44:20,980 --> 00:44:24,560
would say is one of the most successful examples of this particular approach which is

756
00:44:24,570 --> 00:44:32,350
autonomous helicopter flight where the outer approaches this model based reinforcement learning in particular

757
00:44:32,400 --> 00:44:35,310
in get all in their two thousand three work i think they've done a lot

758
00:44:35,310 --> 00:44:36,860
of other things with the same idea

759
00:44:36,920 --> 00:44:41,880
the first parameter the model space are trying to control helicopter that's going around three

760
00:44:42,060 --> 00:44:45,840
and they have they know alot about helicopters they just don't know everything all the

761
00:44:45,890 --> 00:44:49,990
detailed numbers but they know that this rotor effects

762
00:44:50,010 --> 00:44:53,850
will call it thrust OK so they have a lot of information about how the

763
00:44:53,850 --> 00:44:56,320
state variable should affect other state variables

764
00:44:56,390 --> 00:45:00,600
they basically had like almost like a DBN like they always have a

765
00:45:00,630 --> 00:45:04,210
and influence diagram but they can exit the different variables to each other but they

766
00:45:04,210 --> 00:45:07,010
don't have all the numbers killed and so they created this

767
00:45:07,050 --> 00:45:12,220
parameterized model and then use experience in the real world to set the parameters

768
00:45:12,240 --> 00:45:16,550
OK so the learning of the model basically fills in the details creating a complete

769
00:45:16,550 --> 00:45:18,480
simulation models

770
00:45:18,520 --> 00:45:21,880
the end up their particular work eighteen actually use

771
00:45:21,920 --> 00:45:25,520
because they didn't have the policy at the head of person fly the helicopter but

772
00:45:25,520 --> 00:45:27,820
not in a way that it was being copied just in a way that would

773
00:45:27,820 --> 00:45:31,780
take it through the state space of it so so that there was enough data

774
00:45:31,800 --> 00:45:32,720
to tune

775
00:45:32,730 --> 00:45:36,800
the model so that it would accurately reflect how the helicopter really works

776
00:45:36,880 --> 00:45:41,560
the resulting model very high dimensional space and action space so to solve it instead

777
00:45:41,560 --> 00:45:45,180
of trying to solve it analytically or with

778
00:45:45,180 --> 00:45:50,420
that can come out from another vocabulary that can come of mexican wherever you want

779
00:45:50,990 --> 00:45:54,170
and then you can say OK for all the cities i want to use these

780
00:45:54,180 --> 00:45:55,750
external URI

781
00:45:55,770 --> 00:45:59,440
and i want to link that to that one in that case the RDF export

782
00:45:59,460 --> 00:46:00,740
everything in the wiki

783
00:46:00,780 --> 00:46:02,650
will be able to say OK

784
00:46:03,770 --> 00:46:05,850
sun is the city

785
00:46:08,880 --> 00:46:12,310
this vocabulary for example

786
00:46:12,370 --> 00:46:15,980
so can reduce external your eyes with in the wiki

787
00:46:16,000 --> 00:46:21,740
you very limited in what you can create basically you can only create hierarchies of

788
00:46:21,740 --> 00:46:24,520
properties and hierarchies of categories it's basically

789
00:46:24,540 --> 00:46:29,640
so nothing more of the ontology and that

790
00:46:29,650 --> 00:46:35,890
o great OK

791
00:46:35,900 --> 00:46:40,390
the wiki already has number of data is available inside

792
00:46:44,180 --> 00:46:51,730
the missing numbers already for populations for example it also has other your eyes go

793
00:46:51,730 --> 00:46:54,170
for annotations

794
00:46:54,190 --> 00:46:59,370
just as an RDF which means that we don't have semantic impact on the OWL

795
00:47:01,050 --> 00:47:06,100
and which may be useful for example to add some metadata about the concept

796
00:47:06,120 --> 00:47:11,470
but annotations are rarely used the semantic media because because most users just don't have

797
00:47:11,470 --> 00:47:12,530
the need for

798
00:47:14,330 --> 00:47:18,670
it is for well for dates for example a see my birthday is on february

799
00:47:18,670 --> 00:47:23,680
twenty seven nineteen seventy eight it has its using a lot of cleverness in the

800
00:47:23,680 --> 00:47:27,920
background to try to get the data out of it it has the back

801
00:47:27,940 --> 00:47:35,050
because it's based on the mice equal data implementation you can't have stated goal before

802
00:47:35,050 --> 00:47:39,330
nineteen ten or something like that so which

803
00:47:39,350 --> 00:47:43,170
it has to be repaired at some point we have actually

804
00:47:43,180 --> 00:47:45,380
because got the patch

805
00:47:45,390 --> 00:47:50,080
for this problem from the most funny of source actually take

806
00:47:50,090 --> 00:47:53,110
there was a community that created the wiki for

807
00:47:53,120 --> 00:47:54,570
creation of some

808
00:47:54,580 --> 00:47:58,360
i don't know if you know what creationism is the belief that god created the

809
00:47:58,360 --> 00:48:00,530
world five-and-a-half thousand years ago

810
00:48:00,550 --> 00:48:06,410
and before that there was nothing and all the things like dinosaurs or

811
00:48:06,670 --> 00:48:12,180
geological features that we think about than that basically about box in science

812
00:48:12,210 --> 00:48:16,220
because you know the world is only five thousand years ago but big and they

813
00:48:16,220 --> 00:48:17,240
had the wiki

814
00:48:17,260 --> 00:48:22,580
three percent in knowledge about creationism and ID and they noticed about the need to

815
00:48:22,580 --> 00:48:25,980
go further back in time to nineteen ten

816
00:48:26,020 --> 00:48:30,790
not too much further back but still so they created the patch for their own

817
00:48:31,510 --> 00:48:37,040
to allow dates back up to five and a half thousand years ago

818
00:48:37,080 --> 00:48:40,430
and then we have to think can make few years more to it so that

819
00:48:40,430 --> 00:48:43,400
they actually created the patch for the whole sixty four

820
00:48:43,400 --> 00:48:45,650
basically forever back in time

821
00:48:45,700 --> 00:48:50,730
and we're still working on implementing the system but it's just nice to see that

822
00:48:50,760 --> 00:48:55,450
sometimes those kind of come in

823
00:48:55,460 --> 00:49:01,240
email is one data type to be used enumeration so you can say that a

824
00:49:02,760 --> 00:49:07,510
property may use should only the following two or three or four values or whatever

825
00:49:07,530 --> 00:49:10,730
just like in our enumeration

826
00:49:10,750 --> 00:49:12,560
the geographic coordinates

827
00:49:12,590 --> 00:49:17,460
lots of questions on numbers we already have pages and other pages and we can

828
00:49:17,460 --> 00:49:18,950
basically identities

829
00:49:18,950 --> 00:49:21,700
strange just

830
00:49:21,720 --> 00:49:25,040
some characters temperature

831
00:49:25,090 --> 00:49:34,260
this implemented because the translation from says to farenheit is not no inflation

832
00:49:34,260 --> 00:49:37,350
text which is a long string for you have

833
00:49:37,360 --> 00:49:41,360
so this is that we don't have extra features for

834
00:49:41,370 --> 00:49:47,760
length order dimension like weights and so on this because this is implemented in the

835
00:49:48,680 --> 00:49:53,560
datatype you can actually tell

836
00:49:54,430 --> 00:49:58,770
it has to be this often number you and it has a unit

837
00:49:58,800 --> 00:50:01,450
and then you can also define within the wiki

838
00:50:01,490 --> 00:50:07,480
that different units can be converted into each other for certain conversion factor

839
00:50:07,530 --> 00:50:12,390
we have the temperature data type extra implemented because it's not what you can do

840
00:50:14,010 --> 00:50:20,060
fahrenheit with kelvin celsius but for most of the other units out there you can

841
00:50:20,060 --> 00:50:28,270
actually just useful in order to become conversion and therefore you can define whatever

842
00:50:28,290 --> 00:50:32,370
unit conversions you like in your wiki

843
00:50:32,420 --> 00:50:33,940
also any new

844
00:50:33,960 --> 00:50:39,750
datatypes is pretty easy if you can you if you are willing to programmed in

845
00:50:41,070 --> 00:50:47,440
so all these other conversions or the conversions two you just on weak interface don't

846
00:50:47,440 --> 00:50:51,310
have to be a problem for that if you want to add further datatypes you

847
00:50:51,310 --> 00:50:54,510
have to be a programmer and that the mean ph

848
00:50:54,520 --> 00:50:59,930
is at different levels that we encounter she m

849
00:50:59,930 --> 00:51:04,640
one of the nice things the whole conversion factors was that there are quite a

850
00:51:04,640 --> 00:51:07,310
number of spend now that are

851
00:51:07,330 --> 00:51:09,240
dedicated to

852
00:51:09,240 --> 00:51:13,950
fantasy worlds of science fiction worlds but they have partially their own

853
00:51:14,240 --> 00:51:21,700
measurement systems so they're not using inches and all meters or whatever but they've invented

854
00:51:21,700 --> 00:51:25,990
the complete measurement system and they can basically had

855
00:51:26,010 --> 00:51:32,560
those conversions to the system and have the biggest building intelligent with it and

856
00:51:32,580 --> 00:51:40,750
even translated two meters and what kind of stuff

857
00:51:45,980 --> 00:51:49,810
in right

858
00:51:49,870 --> 00:51:53,940
exactly in semantic media wiki we made the this

859
00:51:54,070 --> 00:51:57,020
decision and this restriction

860
00:51:57,050 --> 00:52:01,060
that the subject has always to be the page

861
00:52:01,070 --> 00:52:04,210
so you cannot talk about anything else

862
00:52:04,260 --> 00:52:06,910
and it was on page but about was on

863
00:52:06,920 --> 00:52:09,950
that has several reasons

864
00:52:09,950 --> 00:52:14,170
minus one minus one plus one plus one for instance

865
00:52:17,920 --> 00:52:20,470
so do plus one here is

866
00:52:21,810 --> 00:52:25,410
his three we can shatter set of size three we consider any set of size

867
00:52:26,610 --> 00:52:27,730
i can't get all

868
00:52:27,750 --> 00:52:29,510
two to four sixteen

869
00:52:32,050 --> 00:52:35,960
another way of looking at that and here are written in the right way with

870
00:52:35,970 --> 00:52:39,010
with the additional offset parameter theta

871
00:52:39,030 --> 00:52:42,520
is we can directly look at

872
00:52:42,540 --> 00:52:44,980
how many points we can shatter by

873
00:52:45,000 --> 00:52:52,180
by this class of linear threshold functions and you know i want to step through

874
00:52:52,180 --> 00:52:55,350
the proof but it's easy to see that you can show

875
00:52:56,660 --> 00:53:01,890
you can show the points if and only if these vectors of x one minus

876
00:53:01,890 --> 00:53:05,600
one extreme response on this means the concatenation of the vector x one with that

877
00:53:05,610 --> 00:53:10,890
the last component is minus one these that these n vectors are linearly independent

878
00:53:10,940 --> 00:53:15,620
right if they are linearly independent then we can not just get the science right

879
00:53:15,640 --> 00:53:20,570
we can actually get these values to anything we what rogers by inverting this

880
00:53:20,570 --> 00:53:23,980
this matrix if

881
00:53:27,120 --> 00:53:30,990
you know we can certainly shatter set that is linearly independent and you can show

882
00:53:30,990 --> 00:53:32,190
that if there

883
00:53:33,190 --> 00:53:38,080
but if if we can shatter set in asserting independent

884
00:53:38,110 --> 00:53:42,690
but of course that means we can find a sequence of m equal to

885
00:53:42,690 --> 00:53:46,110
the dimension plus one that are linearly independent

886
00:53:46,860 --> 00:53:51,780
and we can now and we can find separate any bigger

887
00:53:51,800 --> 00:53:53,160
so so

888
00:53:53,200 --> 00:53:56,810
n can be any bigger than d plus one

889
00:53:56,830 --> 00:54:03,570
two allows to shut and we can find a basis of that dimension

890
00:54:03,590 --> 00:54:05,930
of this form

891
00:54:05,930 --> 00:54:08,970
right is the direct proof that we can shatter set of

892
00:54:08,980 --> 00:54:12,360
size t plus one but the VC dimension is equal to t plus one in

893
00:54:12,360 --> 00:54:14,240
this case

894
00:54:14,250 --> 00:54:17,000
OK so here is the

895
00:54:17,030 --> 00:54:19,820
so we've seen that if

896
00:54:19,840 --> 00:54:23,630
if m is smaller than the VC dimension

897
00:54:23,630 --> 00:54:26,750
by by definition the growth function is to the end

898
00:54:26,780 --> 00:54:29,950
so it's exponential in m when m

899
00:54:31,270 --> 00:54:35,670
bigger than the VC dimension we get a polynomial rate of growth

900
00:54:35,670 --> 00:54:40,740
of the growth function and this is called cells lemon

901
00:54:40,750 --> 00:54:42,690
OK so

902
00:54:46,280 --> 00:54:49,170
or sometimes called the south sherlock

903
00:54:49,680 --> 00:54:52,510
so here's the VC dimension is not given d

904
00:54:52,530 --> 00:54:54,680
the growth function and

905
00:54:54,690 --> 00:54:57,580
is no more than the sum of them choose

906
00:54:57,640 --> 00:54:58,670
and two xi

907
00:54:58,680 --> 00:55:03,750
so somewhere in two zero up to m tuesday

908
00:55:10,510 --> 00:55:11,270
let me

909
00:55:11,280 --> 00:55:12,250
let me

910
00:55:12,260 --> 00:55:15,140
it's a little bit about why

911
00:55:15,140 --> 00:55:19,970
why this is true

912
00:55:20,000 --> 00:55:22,250
right so i guess in the in the

913
00:55:22,250 --> 00:55:25,990
the original proof that we saw in using this sort of the result we're working

914
00:55:25,990 --> 00:55:30,170
backwards the growth function looks like this you know we have inequality here

915
00:55:30,220 --> 00:55:34,770
the linear threshold functions and you know as a result we know how big n

916
00:55:34,770 --> 00:55:38,570
has to be before we drop below to the end that tells the VC dimension

917
00:55:38,820 --> 00:55:41,920
sounds lemma tells us that in general we can go the other way around that

918
00:55:41,930 --> 00:55:45,530
that when we know the VC dimension is less than some quantity that tells us

919
00:55:45,530 --> 00:55:46,290
that the

920
00:55:46,320 --> 00:55:49,330
the growth function grows no more than

921
00:55:50,920 --> 00:55:53,020
in m with exponent

922
00:55:53,040 --> 00:55:55,770
the VC dimension

923
00:55:55,820 --> 00:55:57,960
OK so why is it true there

924
00:55:58,010 --> 00:56:00,170
yeah i know of

925
00:56:00,220 --> 00:56:00,900
i guess

926
00:56:01,560 --> 00:56:05,290
a quite distinct groups of of cells level i want to show you the the

927
00:56:06,660 --> 00:56:11,210
so this is the shifting the shifting proof

928
00:56:13,660 --> 00:56:16,990
all right at the table your memory i showed you an example of this this

929
00:56:16,990 --> 00:56:19,520
table for the case of a

930
00:56:19,540 --> 00:56:23,050
linear threshold functions on the real line

931
00:56:23,070 --> 00:56:28,230
right this is a table of values of the functions take on a particular sequence

932
00:56:28,230 --> 00:56:29,510
x one through x m

933
00:56:29,520 --> 00:56:33,540
right this is the restrictions of functions so that sequence down here we have distinct

934
00:56:33,540 --> 00:56:39,010
functions each row corresponds to a function from class but gives the distinct signs sequence

935
00:56:39,060 --> 00:56:41,920
right at the next one is minus one if it x twos plus one and

936
00:56:41,920 --> 00:56:43,490
so on

937
00:56:43,530 --> 00:56:45,980
OK so here in this example

938
00:56:45,980 --> 00:56:47,240
there are

939
00:56:47,290 --> 00:56:48,890
just five

940
00:56:48,890 --> 00:56:52,800
distinct rows to the table so the cardinality of the set of restrictions is is

941
00:56:52,810 --> 00:56:55,730
five we counting rows in the table

942
00:56:55,740 --> 00:56:57,400
OK and the idea of the

943
00:56:57,540 --> 00:57:01,420
of cells liver all we're doing is saying if the VC dimension is no more

944
00:57:01,420 --> 00:57:02,740
than d

945
00:57:02,750 --> 00:57:05,930
the number of the rows the number of rows in the table

946
00:57:05,940 --> 00:57:07,520
can't be too large

947
00:57:07,580 --> 00:57:12,460
OK so we're working to see that the number of rows is not too big

948
00:57:12,490 --> 00:57:18,720
and the idea is to change this table into a form that that doesn't increase

949
00:57:18,720 --> 00:57:20,210
the VC dimension

950
00:57:20,220 --> 00:57:23,360
but it's easy to count the rose

951
00:57:23,430 --> 00:57:27,360
right doesn't doesn't increase in summation doesn't change the number rose but it's easy to

952
00:57:27,360 --> 00:57:31,250
counter the rows have been that within the transformed table

953
00:57:31,330 --> 00:57:34,280
OK and the transformation is as follows so

954
00:57:34,340 --> 00:57:37,490
we work column by column

955
00:57:37,530 --> 00:57:39,190
for each column

956
00:57:39,200 --> 00:57:42,230
right we change every plus one minus

957
00:57:42,240 --> 00:57:46,270
if that leaves the row that isn't already in the table so we shifting down

958
00:57:46,270 --> 00:57:51,260
from plus one to minus one everywhere that we can that the doesn't eliminate arrived

959
00:57:51,260 --> 00:57:55,260
from the table doesn't leave us with the right it's already in the table

960
00:57:55,290 --> 00:58:01,020
so for instance if we if we start with the first column take this plus

961
00:58:01,030 --> 00:58:04,930
we can shifted down to minus because we'd be left with minus minus minus plus

962
00:58:04,930 --> 00:58:07,480
plus and we don't have anything like that

963
00:58:07,890 --> 00:58:11,190
we don't have any any rather like that in the table this one is closest

964
00:58:11,190 --> 00:58:13,230
but it's ends

965
00:58:13,240 --> 00:58:17,380
OK then we can take this one and then shifted down to minus plus plus

966
00:58:17,380 --> 00:58:19,290
minus plus because there's nothing

967
00:58:19,320 --> 00:58:20,330
like that in the

968
00:58:20,350 --> 00:58:21,990
in the table

969
00:58:23,210 --> 00:58:26,480
right if this had been a plus then we could and should that one down

970
00:58:26,510 --> 00:58:29,520
because we would be left with the road was identical

971
00:58:30,550 --> 00:58:36,520
so let's just do that shifting down column by column ensuring always that we only

972
00:58:37,490 --> 00:58:41,630
when they when they left with the distinct row in the table

973
00:58:41,670 --> 00:58:47,330
right so here's the transformed table in that case so you know we saw we

974
00:58:47,330 --> 00:58:51,360
started first column we can shift everything down when we get the second column we're

975
00:58:51,360 --> 00:58:54,630
left with the first let's think about that when we get the second column well

976
00:58:54,630 --> 00:58:57,490
if we have to leave this is a plus

977
00:58:57,510 --> 00:59:00,580
right why is that well in the first column we should do that minor so

978
00:59:00,580 --> 00:59:04,320
we had minus minus minus plus plus and we get to the second column we

979
00:59:04,320 --> 00:59:08,080
can change that to minus minus minus plus plus because there's already

980
00:59:08,100 --> 00:59:09,760
row in the table k

981
00:59:09,780 --> 00:59:12,880
so we end up with a table it looks like this

982
00:59:12,880 --> 00:59:15,760
all right so

983
00:59:15,810 --> 00:59:18,320
what can we say about this table

984
00:59:18,330 --> 00:59:20,750
well one observation is that if we ever see

985
00:59:20,750 --> 00:59:23,690
a bunch of classes in road

986
00:59:23,690 --> 00:59:27,450
what do we know

987
00:59:27,470 --> 00:59:29,700
but if i see two classes like this

988
00:59:29,720 --> 00:59:32,810
i know that somewhere else in the table i can change one of those classes

989
00:59:32,820 --> 00:59:35,190
two months and i have to have the road

990
00:59:35,210 --> 00:59:38,690
or i can change this plus two months and i have to have the right

991
00:59:38,740 --> 00:59:40,950
right i can change both of them to minus

992
00:59:40,960 --> 00:59:43,180
you have to have the right

993
00:59:43,200 --> 00:59:48,020
OK so everything so we have this closed below property rights that if we if

994
00:59:48,020 --> 00:59:51,250
we were to shift anything down from here

995
00:59:51,260 --> 00:59:55,460
OK we would get a row that's already in the table

996
00:59:56,390 --> 00:59:59,860
so i didn't mention you know your way through all the columns and you keep

997
00:59:59,860 --> 01:00:02,600
on going until you end up with this table you know with the fixed point

998
01:00:02,600 --> 01:00:05,510
of this transformation OK no no no

999
01:00:05,530 --> 01:00:08,490
no more shifting can be done

1000
01:00:08,490 --> 01:00:13,000
and this is the the table you end up with after after that full process

1001
01:00:13,050 --> 01:00:17,490
so close that it may mean that you need to work through the columns more

1002
01:00:17,490 --> 01:00:19,540
than more than once

1003
01:00:19,610 --> 01:00:22,690
OK so

1004
01:00:22,690 --> 01:00:25,580
closer to be equivalent using an equivalence

1005
01:00:25,770 --> 01:00:29,190
now this atom has to be a predicate

1006
01:00:29,330 --> 01:00:33,210
and the arguments of that predicate should be the variables and the constants that appear

1007
01:00:33,210 --> 01:00:34,460
in the formula

1008
01:00:35,190 --> 01:00:38,730
because i want to add them to have one grounding for each one of the

1009
01:00:39,980 --> 01:00:42,910
and the way to get this make sure that have the same arguments on both

1010
01:00:43,850 --> 01:00:48,020
other than that the conversion is still the same

1011
01:00:48,150 --> 01:00:51,180
now we get to the weighted model counting procedure

1012
01:00:51,230 --> 01:00:54,830
what changes here two things the first thing is that we're going to have a

1013
01:00:54,830 --> 01:00:59,850
new argument there were passing down the recursive calls to weighted model counting and this

1014
01:00:59,850 --> 01:01:03,260
argument is the set of substitution constraints

1015
01:01:03,280 --> 01:01:07,770
again this is just as in first-order theorem proving right is either substitutions i had

1016
01:01:07,770 --> 01:01:11,320
the constra i have this as a as i do unifications

1017
01:01:11,370 --> 01:01:14,730
i have the substitutions that made this unifications go through

1018
01:01:14,730 --> 01:01:15,090
and made the

1019
01:01:15,450 --> 01:01:20,330
resolutions possible they accumulate as i progressed the inference the same thing is going to

1020
01:01:20,330 --> 01:01:21,400
happen here

1021
01:01:21,440 --> 01:01:24,350
i'm going to accumulate substitution constraints

1022
01:01:24,460 --> 01:01:28,960
a substitution constraints applied to acquire just means that i'm only going to consider the

1023
01:01:28,960 --> 01:01:32,920
groundings of that clause there are consistent with the substitution

1024
01:01:32,970 --> 01:01:35,600
so if i have something like friends x y

1025
01:01:35,620 --> 01:01:40,230
and i've replaced x y and now i'm only interested in france and the y

1026
01:01:40,240 --> 01:01:42,270
OK this is what it does

1027
01:01:42,380 --> 01:01:45,230
and as i mentioned before we need to consider

1028
01:01:45,250 --> 01:01:51,200
and substitutions so we're going to have four types sufficient constraints variables equal to constant

1029
01:01:51,200 --> 01:01:53,270
variable not equal to a constant

1030
01:01:53,490 --> 01:01:58,050
very equal to variable variable not equal to variable because all these cases need to

1031
01:01:58,050 --> 01:01:59,500
be handled differently

1032
01:01:59,540 --> 01:02:04,660
and in fact i have a much richer and more compact languages substitution constraints this

1033
01:02:04,660 --> 01:02:08,090
is just the minimum language that allows us to do everything that we do in

1034
01:02:08,100 --> 01:02:11,990
resolution theorem proving and probabilistic inference

1035
01:02:12,000 --> 01:02:15,010
and and then the final and most important changes that now we need to go

1036
01:02:15,010 --> 01:02:19,310
to war WMC with them and lift each of the steps

1037
01:02:19,420 --> 01:02:21,270
so let's see how we do that

1038
01:02:21,290 --> 01:02:23,250
however the base case

1039
01:02:23,260 --> 01:02:26,190
the base is joyfully simple to lift

1040
01:02:26,220 --> 01:02:29,760
because remember all that we have in the base case was the product of this

1041
01:02:29,760 --> 01:02:31,590
over all the atoms

1042
01:02:31,670 --> 01:02:35,750
notice that now this is only over all the things that are allowed by the

1043
01:02:35,750 --> 01:02:38,460
substitution that i'm carrying around right

1044
01:02:38,500 --> 01:02:40,240
i'm carrying a substitutionary

1045
01:02:40,260 --> 01:02:43,520
so now i have all the things that are consistent with the substitution and all

1046
01:02:43,520 --> 01:02:44,820
i have to do

1047
01:02:44,880 --> 01:02:47,660
is instead of being being multiplication

1048
01:02:47,660 --> 01:02:49,310
for each at

1049
01:02:49,980 --> 01:02:53,200
i just raised this to the number of rounds of the

1050
01:02:53,210 --> 01:02:55,790
allowed by the constitution

1051
01:02:55,820 --> 01:03:00,250
notice that this this very simple and very innocent but it's a fabulous gain in

1052
01:03:01,410 --> 01:03:04,570
because the number of runs of the method can easily be in the millions or

1053
01:03:05,810 --> 01:03:09,340
and if we weren't doing lifted inference so basically have to do a million or

1054
01:03:09,350 --> 01:03:11,480
billion multiplications

1055
01:03:11,480 --> 01:03:15,030
and now instead of being able multiplications i just risen to power

1056
01:03:15,160 --> 01:03:18,570
so we want to do this if we can

1057
01:03:18,590 --> 01:03:23,550
again if the CNF is unsatisfiable then returned zero obviously that part didn't change the

1058
01:03:23,550 --> 01:03:26,630
next part is the decomposition step

1059
01:03:26,650 --> 01:03:29,910
now i need to look for u lifted decomposition

1060
01:03:29,930 --> 01:03:31,820
what is it to decomposition

1061
01:03:31,880 --> 01:03:36,780
it's a decomposition of the set of clauses into remedies are first order clauses now

1062
01:03:36,820 --> 01:03:38,740
and the condition now is that it

1063
01:03:38,760 --> 01:03:42,500
the two the let's say that the composer CNF into two sub seeing as it

1064
01:03:42,500 --> 01:03:46,880
has to be the case that no grounding of one atom in here appears in

1065
01:03:48,130 --> 01:03:52,800
right if this to CNS she even one chronicle one at another independent like can

1066
01:03:53,820 --> 01:03:58,120
but we can check the condition and if that condition applies again i can just

1067
01:03:58,120 --> 01:04:00,500
decompose this into the products

1068
01:04:00,510 --> 01:04:06,320
of the individual a class counts and again i can take each one of those

1069
01:04:06,320 --> 01:04:10,260
in just raise it to its number of rounds again i'm getting another exponential reduction

1070
01:04:10,260 --> 01:04:13,720
in one here but if i have many many many different copies of the same

1071
01:04:13,720 --> 01:04:17,650
class i don't need to keep multiplying it just need to compute the value once

1072
01:04:17,660 --> 01:04:19,500
and raised to the corresponding

1073
01:04:19,520 --> 01:04:23,380
finally of course in the worst case i have to split

1074
01:04:23,500 --> 01:04:26,300
and this is actually the most complicated step

1075
01:04:26,370 --> 01:04:29,230
what does it mean to split on the first order at

1076
01:04:29,270 --> 01:04:32,520
i suppose that i split on the atom friends and x

1077
01:04:33,220 --> 01:04:36,690
what this means is that i'm actually splitting on all elements of the

1078
01:04:36,740 --> 01:04:39,760
and going on friends and and and friends and above bob and friends and the

1079
01:04:40,840 --> 01:04:46,260
so naively splitting on the first part of the atom actually creates an exponential number

1080
01:04:46,260 --> 01:04:47,310
of branch

1081
01:04:47,370 --> 01:04:50,750
and that's bad news we don't want to do that if we can avoid it

1082
01:04:50,760 --> 01:04:54,810
it turns out that usually we can avoid it for the following reasons

1083
01:04:54,850 --> 01:05:00,410
is that it doesn't actually matter which particular people and is friends with

1084
01:05:00,410 --> 01:05:04,650
what actually matters is how many she's friends with and how many she's not

1085
01:05:04,700 --> 01:05:07,530
so what i want to do is i want to count those cases in fact

1086
01:05:07,530 --> 01:05:09,070
they may not even all

1087
01:05:09,100 --> 01:05:13,420
depending on what my current knowledge bases so now what i have is the sum

1088
01:05:15,800 --> 01:05:19,460
of true groundings of the late so i just i took an exponential number of

1089
01:05:19,460 --> 01:05:22,060
splits and it became a linear number of splits

1090
01:05:22,330 --> 01:05:26,240
and then what i do before i send this overall

1091
01:05:27,410 --> 01:05:28,600
like this

1092
01:05:28,680 --> 01:05:32,540
and for each of those what why have i have a recursive call to tell

1093
01:05:33,700 --> 01:05:35,750
i with the CNF

1094
01:05:35,810 --> 01:05:39,480
simplified by setting this set into the corresponding value

1095
01:05:40,430 --> 01:05:45,120
and with the with the additional substitutions that i used to unify the set with

1096
01:05:45,130 --> 01:05:47,970
the CNN whatever those might be

1097
01:05:47,990 --> 01:05:50,300
so this is the recursive call

1098
01:05:50,320 --> 01:05:53,650
notice that the most enlightened part however occurs every year

1099
01:05:54,250 --> 01:05:56,710
this is the recursive call so i think that the one i'm going to get

1100
01:05:56,710 --> 01:06:00,400
is a bunch of these sums right with some of the products from their own

1101
01:06:00,400 --> 01:06:03,180
i notice what's happening here right

1102
01:06:03,240 --> 01:06:04,590
what i have here

1103
01:06:04,610 --> 01:06:05,810
at the end of the day

1104
01:06:05,820 --> 01:06:07,600
it's just the polynomial

1105
01:06:07,610 --> 01:06:10,290
in the weights of the formulas

1106
01:06:10,310 --> 01:06:13,270
that's that's all it is it's appalling

1107
01:06:13,290 --> 01:06:17,450
and so this is the minimum you buy and as i do this right so

1108
01:06:17,450 --> 01:06:20,780
this is the number of true groundings is the number of false surroundings as i

1109
01:06:20,780 --> 01:06:24,120
do this for more and more atoms right i'm just going to get a monomial

1110
01:06:24,120 --> 01:06:27,000
with more and more of them at the end of the day the weight of

1111
01:06:27,000 --> 01:06:28,260
the monomials

1112
01:06:28,280 --> 01:06:32,220
it is just the number of worlds in which that combination of members of true

1113
01:06:32,220 --> 01:06:34,240
and false of the formulas

1114
01:06:34,280 --> 01:06:38,340
even though this is complicated at some level it's very to intuitive

1115
01:06:38,350 --> 01:06:42,720
probabilistic inference is computing the multilinear function

1116
01:06:42,750 --> 01:06:45,930
lifted inference computing the polynomial

1117
01:06:45,950 --> 01:06:49,960
of arbitrary degree in fact it was could be very high

1118
01:06:49,980 --> 01:06:53,550
if you think about it you know doing ground inference is a lot like doing

1119
01:06:54,590 --> 01:06:57,200
while not being allowed to use powers

1120
01:06:57,200 --> 01:07:01,930
imagining outerwear feedbacks to the ten you have to multiplied by itself n times

1121
01:07:01,940 --> 01:07:04,910
well if you have access to the million you have to multiply exploits of a

1122
01:07:04,910 --> 01:07:05,820
million times

1123
01:07:05,880 --> 01:07:10,080
that's what differences with lifted inference we just you know raise it you know to

1124
01:07:10,730 --> 01:07:12,690
whatever it is and keep going

1125
01:07:12,710 --> 01:07:17,320
so this is how we have this is the basic schema for how we do

1126
01:07:17,950 --> 01:07:22,500
model counting and therefore probabilistic theorem proving suffice it to say that you know this

1127
01:07:22,500 --> 01:07:27,580
game as i just showed here you know isn't that efficient there's there's a bunch

1128
01:07:27,590 --> 01:07:31,460
i mean this is already a lot more efficient than the in-ground difference but it

1129
01:07:31,460 --> 01:07:34,860
can be made a lot more efficient using several things

1130
01:07:35,270 --> 01:07:39,670
many of them are just standard things that we can inherit from either improving or

1131
01:07:39,670 --> 01:07:45,440
different samples of random information that was supposedly your random number generator basically

1132
01:07:45,460 --> 01:07:51,750
that are

1133
01:07:51,770 --> 01:07:53,790
on each trial

1134
01:07:53,810 --> 01:07:57,040
and then this they did that within the things a story and they have to

1135
01:07:57,040 --> 01:08:01,020
do it as little delay and so on and so ten fifteen seconds later they

1136
01:08:01,020 --> 01:08:04,670
have to make the next decision

1137
01:08:04,710 --> 01:08:10,580
although you may but the the the subject here don't do that

1138
01:08:10,960 --> 01:08:14,990
i'll talk about that in more detail in the next study because the next it's

1139
01:08:14,990 --> 01:08:16,150
even more crucial

1140
01:08:16,270 --> 01:08:20,940
and in the in the next study we had left and right button responses and

1141
01:08:20,960 --> 01:08:22,500
you can basically look at

1142
01:08:22,540 --> 01:08:29,190
certain parameters of stochasticity of sequential sequential dependence you can for example look at autocorrelation

1143
01:08:29,360 --> 01:08:33,830
you can look at the distribution of phase durations is another one you can look

1144
01:08:33,830 --> 01:08:39,420
at and it's we chose subjects to be

1145
01:08:40,230 --> 01:08:44,440
as close to random number generator as possible so they came in first

1146
01:08:44,440 --> 01:08:47,770
we didn't tell them they have to be random we didn't tell them that they

1147
01:08:47,770 --> 01:08:53,790
had to produce a random sequence we didn't tell them that they had to match

1148
01:08:53,810 --> 01:08:58,360
we told them simply that they could randomly choose the point that they are in

1149
01:08:58,360 --> 01:09:03,860
the next experiment what what they freely choose which button which button to press

1150
01:09:03,880 --> 01:09:10,400
and we chose all subjects are roughly spontaneously that if you think every person has

1151
01:09:10,420 --> 01:09:15,900
narrator which of those of

1152
01:09:15,900 --> 01:09:22,310
i don't show any sequential dependence so we we pre selected subjects for this this

1153
01:09:26,860 --> 01:09:33,560
you know you got the support that point so looked into this she's and supplement

1154
01:09:33,560 --> 01:09:37,610
of this paper we look into this and you can't seventy percent from history

1155
01:09:37,920 --> 01:09:41,460
you can do just about fifty percent from the history this would look at first

1156
01:09:41,460 --> 01:09:46,250
order correlation for example you can show you can train a classifier on sequences of

1157
01:09:46,250 --> 01:09:52,860
its own so many previous trials and things like that and you can buy classifiers

1158
01:09:52,880 --> 01:09:57,980
will show you in the next talk of in the next section of talk show

1159
01:09:57,980 --> 01:09:59,380
you how this

1160
01:09:59,440 --> 01:10:03,650
developed in time and you can also see how it increases with the distance from

1161
01:10:03,650 --> 01:10:07,730
the previous trial and this increase with the distance from the previous trials suggest is

1162
01:10:07,730 --> 01:10:12,380
not some overlap of information from the previous trial that you're picking up this room

1163
01:10:12,380 --> 01:10:16,170
is a very important point where we took particular care to look into this point

1164
01:10:16,170 --> 01:10:17,900
in the paper

1165
01:10:20,000 --> 01:10:26,520
by doing so

1166
01:10:34,730 --> 01:10:41,250
it a

1167
01:10:41,270 --> 01:10:52,670
you mean if you take the whole brain as as high as input

1168
01:11:04,360 --> 01:11:07,880
he says that's not what we're doing here

1169
01:11:07,940 --> 01:11:11,910
i've shown you a single study here where we're taking this will take in the

1170
01:11:11,910 --> 01:11:16,690
region of interest in the visual system what we do there is we choose voxels

1171
01:11:16,710 --> 01:11:18,630
based on two criteria

1172
01:11:18,670 --> 01:11:22,130
a that they're in a particular brain area for example b one b two b

1173
01:11:23,040 --> 01:11:26,210
and b are activated by the stimulus

1174
01:11:26,360 --> 01:11:29,560
independent independent of

1175
01:11:29,580 --> 01:11:31,860
no independent of

1176
01:11:31,880 --> 01:11:35,920
independent of the question whether it prefers once the most of the others to despite

1177
01:11:35,980 --> 01:11:38,560
look where other voxel stimulus response

1178
01:11:38,580 --> 01:11:43,360
without introducing any specific information that would allow you to discriminate

1179
01:11:43,380 --> 01:11:46,770
of course you don't have any leakage or pre selection that kind of allows you

1180
01:11:46,770 --> 01:11:52,230
to tripoli around this and if you run it on noise stimuli you see that

1181
01:11:52,270 --> 01:11:59,080
the the disk the accuracies are chance level and you get a distribution around now

1182
01:11:59,100 --> 01:12:03,380
or chance you get the distribution around chance level but we don't use regions of

1183
01:12:03,380 --> 01:12:04,400
interest here

1184
01:12:04,460 --> 01:12:08,980
this is based on a on the searchlight approach to map the information across the

1185
01:12:08,980 --> 01:12:13,610
whole range and then you do a correction for the fact that you're correcting cross

1186
01:12:13,610 --> 01:12:17,600
large number of you doing this across large number of brain areas

1187
01:12:20,610 --> 01:12:31,410
this is the output we what you get is the output of this is a

1188
01:12:32,230 --> 01:12:33,440
tells you

1189
01:12:33,480 --> 01:12:38,380
based on specific statistical criterion where there is information

1190
01:12:38,440 --> 01:12:42,440
above chance and you get home and it's like you can i mean this is

1191
01:12:42,440 --> 01:12:45,750
this is not a three d rendered man but it could show three d man

1192
01:12:45,790 --> 01:12:50,360
rendered map we can see exactly parts of the brain but called the information it

1193
01:12:50,400 --> 01:12:53,270
is not biased in any way by regions of interest

1194
01:12:53,290 --> 01:12:56,980
and that would not be valid and the reason it wouldn't be valid is that

1195
01:12:56,980 --> 01:13:02,360
the region of interest would have to come from something specific from some specific reasoning

1196
01:13:02,750 --> 01:13:06,940
and the reasoning would be it has shown overall activity for example and that's not

1197
01:13:06,940 --> 01:13:14,310
that easy because the overall activity does not necessarily going hand-in-hand with the decodable information

1198
01:13:14,310 --> 01:13:16,610
so they can be independent from each other

1199
01:13:16,610 --> 01:13:22,700
then you know the difference between function f in RKHS the FOV of its brain

1200
01:13:22,700 --> 01:13:29,190
is probably smaller than the number of times the distance between the senate prime but

1201
01:13:29,190 --> 01:13:31,680
the distance induced by the kernel

1202
01:13:31,690 --> 01:13:35,460
so let's assume that we focus on the notion of small norman

1203
01:13:35,540 --> 01:13:40,050
can we say small number of this thing is small then you see that one

1204
01:13:40,050 --> 01:13:43,200
of the consequences that at least one of its prime

1205
01:13:43,250 --> 01:13:47,410
can be very large as soon as it is close to prime

1206
01:13:47,420 --> 01:13:50,370
in terms of the metric induced by the count

1207
01:13:50,990 --> 01:13:55,150
so this is something we call smooth smoothness in the sense that

1208
01:13:55,160 --> 01:13:59,700
we're not very too much between similar points and here the similarity measure the in

1209
01:13:59,720 --> 01:14:00,970
terms of

1210
01:14:01,000 --> 01:14:02,210
can i used

1211
01:14:03,670 --> 01:14:06,040
so this means in particular that you you might want to to remember

1212
01:14:06,240 --> 01:14:07,440
these simple people

1213
01:14:09,100 --> 01:14:11,420
having a small on the dark ages

1214
01:14:11,450 --> 01:14:14,840
implies sort function with a small number of features

1215
01:14:14,840 --> 01:14:20,000
in it is a function that cannot be too much between points and that's when

1216
01:14:20,000 --> 01:14:24,100
you can tell whether it is the geometric finer points so it can be very

1217
01:14:24,100 --> 01:14:30,600
to merge with respect to to the geometry defined by the kind on the point

1218
01:14:30,610 --> 01:14:32,850
but if you example of that

1219
01:14:33,200 --> 01:14:38,170
well so let's start with the simplest can only not victoria's

1220
01:14:38,180 --> 01:14:41,740
remember that when you you shall space

1221
01:14:41,780 --> 01:14:46,940
these are are and is based on torus the simplest and what the known for

1222
01:14:46,990 --> 01:14:49,940
can we describe the RKHS and an ontology

1223
01:14:49,950 --> 01:14:52,200
and after that the answer is yes

1224
01:14:52,220 --> 01:14:57,230
so it is very important because go back to the definition bit of the RKHS

1225
01:14:57,240 --> 01:14:59,840
the typical function k chase

1226
01:15:01,140 --> 01:15:04,510
like the of the phi kappa psi ix

1227
01:15:04,540 --> 01:15:07,740
so if you have a fairly simply side that effects

1228
01:15:07,940 --> 01:15:11,100
you that when you some of phi exciting piece

1229
01:15:11,550 --> 01:15:17,000
it's intuitive definition of ix OK so called the new the some of the size

1230
01:15:17,100 --> 01:15:19,920
at the annual of these would be something like that

1231
01:15:20,750 --> 01:15:23,510
so far the RKHS of the linear kernel

1232
01:15:23,540 --> 01:15:26,040
is the set of null functions

1233
01:15:26,090 --> 01:15:34,090
so you can summarize by saying that an explosion RKHS as the value that is

1234
01:15:34,090 --> 01:15:40,090
and furthermore one month of competition can easily show that in fact one of these

1235
01:15:40,090 --> 01:15:41,730
is written as the body is

1236
01:15:42,180 --> 01:15:46,140
what is the norm in a RKHS of these functions so the solution now is

1237
01:15:46,140 --> 01:15:48,170
seen as one point of the RKHS

1238
01:15:48,190 --> 01:15:52,490
well known distortion in fact is exactly equal to the euclidean norm of w

1239
01:15:52,500 --> 01:15:55,540
the body was seen as a victory so show that you can just go back

1240
01:15:55,540 --> 01:16:00,180
to the previous definition of the norm and your directly

1241
01:16:00,180 --> 01:16:04,370
so let's see let's look at this picture for instance suppose that piece is just

1242
01:16:04,370 --> 01:16:07,240
the real line so you are you making the enoch nl

1243
01:16:07,260 --> 01:16:08,770
on the real numbers

1244
01:16:08,780 --> 01:16:13,100
what are the functions in the RKHS i said these are the not functions so

1245
01:16:13,100 --> 01:16:14,860
we have three not functions

1246
01:16:14,870 --> 01:16:17,390
so the horizontal axis is case

1247
01:16:17,410 --> 01:16:19,730
OK and the vertical axis is evolving

1248
01:16:19,760 --> 01:16:24,080
april three that this question to the functions f of the

1249
01:16:24,080 --> 01:16:28,800
two weeks in red for the people you know and visible

1250
01:16:28,820 --> 01:16:31,130
these are two and green

1251
01:16:31,140 --> 01:16:36,100
these are three examples of functions in the RKHS of these individual on the RKHS

1252
01:16:36,110 --> 01:16:39,490
and what the known this function what the numbers to the number of the of

1253
01:16:39,490 --> 01:16:41,920
the use of this is exactly the slope here

1254
01:16:41,970 --> 01:16:45,190
so the red one is normal to the blue one of one and the green

1255
01:16:45,190 --> 01:16:48,780
minimum of twenty five and what the smoothness assumption

1256
01:16:48,790 --> 01:16:53,880
well remember that this was the first to do that when the number small the

1257
01:16:53,920 --> 01:16:58,570
the those very too much with respect to the metric induced by the kernel so

1258
01:16:58,570 --> 01:17:03,160
here the kind of it doesn't do any change in the space is the metric

1259
01:17:03,160 --> 01:17:07,700
of the canons just instances and this thing is that if the numbers small

1260
01:17:07,740 --> 01:17:10,200
then the function as a small so

1261
01:17:10,220 --> 01:17:14,760
OK that's the direct intervention of the archbishop

1262
01:17:14,780 --> 01:17:18,470
now you can when the can not another thing is sometimes a bit more tricky

1263
01:17:18,470 --> 01:17:23,800
here we just summarize non result i not provide because it's a bit technical but

1264
01:17:23,800 --> 01:17:29,490
orthonormal columns that in this case here this factorisation that we're looking for in this

1265
01:17:29,490 --> 01:17:36,450
PLSA approach is not really one way let's say the columns here are orthonormal

1266
01:17:36,470 --> 01:17:39,870
or are the rose here but rather

1267
01:17:39,880 --> 01:17:45,190
we have you know very different types of constraints namely that all the entries are

1268
01:17:45,190 --> 01:17:50,440
nonnegative in all these numbers that show up in these matrices are probabilities so they

1269
01:17:50,440 --> 01:17:51,340
better be

1270
01:17:51,360 --> 01:17:57,530
nonnegative otherwise you know it it's doesn't it doesn't correspond to the semantics of the

1271
01:17:57,540 --> 01:18:04,540
probabilistic model and also we have certain normalisation constraints but the normalisation constraint are not

1272
01:18:04,540 --> 01:18:09,730
such that the two norm is one but rather that the one norm of let's

1273
01:18:09,730 --> 01:18:10,910
say these

1274
01:18:11,350 --> 01:18:16,220
here are these here sorry no in that formulation is actually the one norm of

1275
01:18:16,220 --> 01:18:20,990
the columns here and the rose here that has to sum to one

1276
01:18:21,000 --> 01:18:25,500
right and it again because of the normalisation that's induced by the fact that we

1277
01:18:25,500 --> 01:18:28,200
are talking about probabilities here

1278
01:18:28,250 --> 01:18:34,710
OK so this has also been investigated under the name nonnegative matrix factorizations by leniency

1279
01:18:35,860 --> 01:18:37,600
and others and actually

1280
01:18:37,660 --> 01:18:43,080
this gives us a model in this nonnegative matrix factorisation under certain

1281
01:18:43,100 --> 01:18:46,200
conditions are actually equivalent to one another

1282
01:18:46,220 --> 01:18:49,130
OK so

1283
01:18:49,150 --> 01:18:54,520
so now how do we actually learn such a model how do we fit the

1284
01:18:54,540 --> 01:18:56,080
the model based on data

1285
01:18:56,090 --> 01:19:02,440
we can in the simplest case just to look like lihood estimation OK so we

1286
01:19:02,440 --> 01:19:06,940
have all parameters theta and pi and we're given that say just the count the

1287
01:19:06,940 --> 01:19:08,920
count matrix n

1288
01:19:08,940 --> 01:19:13,130
i mean if we write and what the log likelihood is actually the sum over

1289
01:19:13,130 --> 01:19:15,390
all document word pairs

1290
01:19:15,400 --> 01:19:18,190
and then we have the number of times that of course of course is that

1291
01:19:18,190 --> 01:19:19,470
the euro then

1292
01:19:19,490 --> 01:19:24,370
you know effectively that determine the sum does not show up so

1293
01:19:24,390 --> 01:19:27,710
you know there's some although it looks really big because it goes over all pairs

1294
01:19:27,710 --> 01:19:32,820
only goes over the past that actually have non-zero occurrence frequency

1295
01:19:32,840 --> 01:19:35,820
and then here and now we have the log probability

1296
01:19:35,830 --> 01:19:37,190
o five

1297
01:19:37,200 --> 01:19:42,160
that word given the document as predicted by our model and this is exactly this

1298
01:19:42,160 --> 01:19:47,120
mixture formed right so the log of the probability says the sum of his e

1299
01:19:47,180 --> 01:19:48,220
and then

1300
01:19:48,240 --> 01:19:51,610
here w is e times gives you given d

1301
01:19:51,630 --> 01:19:59,600
OK so that's what we try to maximize with respect to the parameters

1302
01:19:59,620 --> 01:20:01,610
pi and theta

1303
01:20:02,640 --> 01:20:05,910
of course you know this is a

1304
01:20:05,920 --> 01:20:09,350
you know this is a non convex problem is had to be expected if we

1305
01:20:09,350 --> 01:20:12,470
deal with with mixture problems here

1306
01:20:12,520 --> 01:20:17,590
and what we can do the following kind of standard procedure

1307
01:20:17,610 --> 01:20:23,220
in dealing with latent variable models is performed in expectation maximisation

1308
01:20:23,230 --> 01:20:30,080
or use the expectation maximisation algorithm to perform maximum likelihood estimation and

1309
01:20:30,090 --> 01:20:34,640
and if we do that and i actually give the derivation of this algorithm later

1310
01:20:34,640 --> 01:20:39,430
but let me show you what is argued amounts to it's also very intuitive what

1311
01:20:39,430 --> 01:20:40,200
we get

1312
01:20:40,220 --> 01:20:45,410
in the EMI with them we alternate between e step and m step and in

1313
01:20:45,410 --> 01:20:52,290
the e step what we do is we compute the posterior probability of this concept

1314
01:20:52,290 --> 01:20:57,550
variable given a particular occurrence of a particular word in a specific document OK so

1315
01:20:57,550 --> 01:21:01,190
for every word that occurs

1316
01:21:01,210 --> 01:21:05,840
we would compute that probability is the probability that word occurrence is related to a

1317
01:21:05,840 --> 01:21:08,970
particular concepts and how is that

1318
01:21:08,980 --> 01:21:13,190
you know what is that the posterior probability were just by bayes rule we can

1319
01:21:13,190 --> 01:21:15,660
write it as p of the given d

1320
01:21:15,720 --> 01:21:20,450
OK so basically dropping the w and then p of w given

1321
01:21:20,460 --> 01:21:25,350
comedy actually but but because of the independence assumptions made in this model

1322
01:21:25,370 --> 01:21:29,130
we can drop the users just p of w given c and then it has

1323
01:21:29,130 --> 01:21:30,480
to be normalized

1324
01:21:30,500 --> 01:21:34,670
appropriately so that it sums to one if we sum over all possible disease

1325
01:21:36,150 --> 01:21:41,420
so we can think of that as the probability that a particular occurrence is explained

1326
01:21:41,460 --> 01:21:45,250
why this concept c and you can see that of course if the document is

1327
01:21:45,250 --> 01:21:50,110
more likely to be related to the concept the probability will be higher as is

1328
01:21:50,130 --> 01:21:55,110
if the word we're talking about here actually has a high probability to express this

1329
01:21:55,110 --> 01:21:59,890
particular concept right so that it's is also very intuitive and then in the m

1330
01:22:02,200 --> 01:22:05,620
you know so we in the e step we compute the this these red guys

1331
01:22:05,620 --> 01:22:07,120
here based on the blue

1332
01:22:07,120 --> 01:22:12,270
dynamics which in coming from physics this is a very powerful i

1333
01:22:12,290 --> 01:22:13,850
really powerful idea

1334
01:22:13,870 --> 01:22:16,960
because i was used to make the ball move basically

1335
01:22:16,960 --> 01:22:18,900
in the the disorder in the

1336
01:22:19,710 --> 01:22:24,830
on the generalisation been oppose basically per cent in the literature

1337
01:22:24,870 --> 01:22:31,290
in particular basically there're some paper by mark girolami on user click food they apologized

1338
01:22:31,480 --> 01:22:34,460
we mystical riemannian manifold ncnc

1339
01:22:34,480 --> 01:22:36,880
we're essentially what you're let me has done

1340
01:22:36,900 --> 01:22:40,420
it bad he's been please propose an extension

1341
01:22:40,420 --> 01:22:43,350
of these techniques where on all

1342
01:22:44,940 --> 01:22:50,750
the velocity instead of being distributed according to the normal distribution of corvallis independent of

1343
01:22:50,750 --> 01:22:55,770
x it's using the geometry of the target distribution pi x you're interested in so

1344
01:22:55,770 --> 01:23:00,730
as to essentially make this thing dependent on x on exploit all come up we

1345
01:23:00,790 --> 01:23:07,600
basically i reason which actually exploit are much better at exploring the uh

1346
01:23:07,620 --> 01:23:11,790
target distribution so there's a lot of the charge in subject

1347
01:23:11,810 --> 01:23:14,810
four so there we sense

1348
01:23:14,850 --> 01:23:19,860
of the surface story of stone by the mediterranean multiculturism our for yourself to sell

1349
01:23:19,860 --> 01:23:24,290
the paper recently developed by so this is not the kind of song about

1350
01:23:24,310 --> 01:23:26,540
techniques in multicolored

1351
01:23:28,290 --> 01:23:30,000
the last

1352
01:23:30,020 --> 01:23:32,440
techniques discuss

1353
01:23:32,460 --> 01:23:37,600
he's something you less sophisticated that sort of slice sampling

1354
01:23:37,690 --> 01:23:39,810
well i mean to an MCMC things

1355
01:23:39,870 --> 01:23:45,690
which basically you can understand even after having been introduced to MCMC only non often

1356
01:23:46,460 --> 01:23:48,380
it's called palestine period

1357
01:23:48,400 --> 01:23:51,290
OK so proud on

1358
01:23:51,310 --> 01:23:52,870
he's basically

1359
01:23:52,870 --> 01:23:53,920
what you cannot do

1360
01:23:53,940 --> 01:24:01,080
is that once more on the on the notion of basically using aleksei volleyball on

1361
01:24:01,080 --> 01:24:06,310
the CI target distribution that was introduced by jerome thompson in the nineties on this

1362
01:24:06,310 --> 01:24:09,880
has become extremely popular in physics

1363
01:24:10,940 --> 01:24:13,690
let's start with an example so

1364
01:24:13,710 --> 01:24:17,270
basically you interest in the prior distribution pi x

1365
01:24:17,420 --> 01:24:21,560
OK on this distribution might be

1366
01:24:21,580 --> 01:24:26,770
to be high dimensional on my during modelled on is very difficult to come up

1367
01:24:26,770 --> 01:24:31,150
with good MCMC algorithm to sample from OK

1368
01:24:31,170 --> 01:24:34,830
so what you're going to do you going to do something which is a bit

1369
01:24:34,880 --> 01:24:37,000
which is the cornerstone of the calloway

1370
01:24:37,040 --> 01:24:38,350
say well OK

1371
01:24:38,370 --> 01:24:44,730
it might be very difficult to sample from the original target distribution pi x OK

1372
01:24:46,370 --> 01:24:50,190
he might be it it's always much easier

1373
01:24:50,210 --> 01:24:53,170
to some poor from the temperature version of of

1374
01:24:53,210 --> 01:24:55,020
by of x that is

1375
01:24:55,040 --> 01:24:57,900
basically sample from the distribution

1376
01:24:57,920 --> 01:25:03,100
i think that has been raised to an exponent which is below one OK on

1377
01:25:03,120 --> 01:25:06,380
renormalized OK so why is it the case was simply

1378
01:25:06,400 --> 01:25:09,080
so i've got a really warfare

1379
01:25:10,100 --> 01:25:12,620
so assume basically

1380
01:25:12,670 --> 01:25:17,600
you have a bimodal target distribution that correspond to pi that's OK

1381
01:25:17,620 --> 01:25:19,900
so that corresponds to five four one

1382
01:25:19,920 --> 01:25:22,000
he's basically you look

1383
01:25:22,020 --> 01:25:27,830
at the same target distribution pi x rays to pull if i already normalised

1384
01:25:27,870 --> 01:25:31,690
as essentially phi goes to zero is going to flatten

1385
01:25:31,710 --> 01:25:34,670
completely the initial target distribution

1386
01:25:35,440 --> 01:25:37,120
then essentially

1387
01:25:37,140 --> 01:25:41,350
so the distance between the two initial node here is going to come to vanish

1388
01:25:41,350 --> 01:25:46,100
as you make the gamma defy the exponent five goes to zero and then you

1389
01:25:46,100 --> 01:25:49,480
have solution is much easier to sample four actually

1390
01:25:49,500 --> 01:25:51,940
OK so that's still not idea

1391
01:25:53,040 --> 01:25:57,620
prior to this idea the something you or you differently here you from is the

1392
01:25:57,620 --> 01:26:02,960
real side of instead of comparing that is the raising the target distributions to be

1393
01:26:03,270 --> 01:26:08,830
to between zero and one what you can do instead is closely you can raise

1394
01:26:08,830 --> 01:26:10,440
it so poor that is

1395
01:26:10,460 --> 01:26:14,730
superior to one and you know that as you expand and phi k goes to

1396
01:26:15,810 --> 01:26:22,330
then this resulting distribution is gonna concentrate itself on the set of global maximum of

1397
01:26:22,330 --> 01:26:27,770
of of by this is simulated annealing OK so whereas and healing what you do

1398
01:26:27,770 --> 01:26:33,040
is your the initial target distribution to exponent which are large because it means that

1399
01:26:33,040 --> 01:26:37,960
you concentrate the target distribution are on the set of global maximum OK

1400
01:26:37,980 --> 01:26:43,940
during the review process nor is it basically sorry you're raise it to exponent which

1401
01:26:43,940 --> 01:26:48,920
are belong to flatten it as much as possible because sampling from such a flat

1402
01:26:48,920 --> 01:26:51,560
distribution is much much easier

1403
01:26:51,580 --> 01:26:55,400
OK so that's the idea of going to use it quite quite a lot of

1404
01:26:55,400 --> 01:26:56,900
time later

1405
01:26:57,150 --> 01:27:01,150
particle methods that is used quite a lot as well so now what to do

1406
01:27:01,150 --> 01:27:05,940
OK so that's not neither side but all use this idea so as to build

1407
01:27:05,940 --> 01:27:07,750
efficient markov chain monte carlo

1408
01:27:07,770 --> 01:27:11,210
well what you can do is not going to say

1409
01:27:11,290 --> 01:27:14,730
is trying to sample only formed by x

1410
01:27:14,750 --> 01:27:17,520
which correspond to you in my case two

1411
01:27:17,520 --> 01:27:23,980
five p capital p we have introduced this kind of p minus one artificial distribution

1412
01:27:24,060 --> 01:27:27,350
you know you're going to be the market share extended space

1413
01:27:30,150 --> 01:27:35,040
well known you external the markov chain what does it targets

1414
01:27:35,060 --> 01:27:40,370
basically the following in violent solution which is basically the point out of the temperature

1415
01:27:40,370 --> 01:27:42,710
target from k one to p

1416
01:27:43,830 --> 01:27:47,060
so that's basically the idea is to build a markov chain is going to be

1417
01:27:47,060 --> 01:27:51,440
the nc shane so as to sample from this going on to do this kind

1418
01:27:51,440 --> 01:27:52,380
of things

1419
01:27:52,380 --> 01:27:55,900
hello hello

1420
01:27:56,110 --> 01:28:03,270
you can tell everybody hear me with my

1421
01:28:04,190 --> 01:28:09,810
so our first like to thank the organisers were giving me the opportunity to tell

1422
01:28:09,810 --> 01:28:13,830
you about the area that working on my name is the white a mother gatsby

1423
01:28:13,830 --> 01:28:19,740
unit this itself in london and be telling you about an introduction to bayesian nonparametric

1424
01:28:22,630 --> 01:28:23,680
i think so

1425
01:28:23,700 --> 01:28:28,350
the next week peter orbanz will also tell you about some foundational

1426
01:28:28,360 --> 01:28:34,440
stuff on bayesian nonparametrics as well so this lecture will become more introductory stereo giving

1427
01:28:34,500 --> 01:28:38,320
you a bit of an idea of how of what bayesian nonparametric models

1428
01:28:38,470 --> 01:28:41,500
and then after peter orbanz lectures out a bit more

1429
01:28:41,510 --> 01:28:44,000
more advanced models

1430
01:28:44,180 --> 01:28:45,460
on thursday

1431
01:28:45,470 --> 01:28:51,690
so here is the outline for today's talk first tell you about

1432
01:28:51,720 --> 01:28:54,890
some examples of parametric models that you

1433
01:28:54,900 --> 01:28:59,130
you put have probably seen final some in earlier parts of the

1434
01:28:59,190 --> 01:29:00,440
this summer school

1435
01:29:00,460 --> 01:29:01,430
and and then

1436
01:29:01,440 --> 01:29:07,160
you feel a bit of introduction of his nonparametrics and as an example outcome go

1437
01:29:07,160 --> 01:29:08,940
true in a bit more detail

1438
01:29:08,960 --> 01:29:12,580
particle model called infinite mixture model

1439
01:29:12,600 --> 01:29:14,970
OK and then will introduce them in

1440
01:29:14,990 --> 01:29:17,880
part of this talk which is the dirichlet process

1441
01:29:21,300 --> 01:29:24,270
so some examples of parametric models

1442
01:29:24,270 --> 01:29:27,400
so i think you guys have seen this carl rasmussen's

1443
01:29:27,410 --> 01:29:31,650
talk right so i suppose they want to do supervised learning and we have to

1444
01:29:31,650 --> 01:29:34,520
learn a function from some input space x

1445
01:29:34,540 --> 01:29:38,240
to an output space y so for example the function might look like this

1446
01:29:38,270 --> 01:29:41,490
and we are given a bunch of training data so

1447
01:29:41,520 --> 01:29:46,520
x one is here and by one is the high point for example in the

1448
01:29:46,520 --> 01:29:50,190
next two and y two and so on and then we fit a function which

1449
01:29:50,190 --> 01:29:51,320
goes true

1450
01:29:51,410 --> 01:29:53,520
this set of life

1451
01:29:53,540 --> 01:29:58,190
so the typical parametric way of doing things will be

1452
01:29:58,240 --> 01:30:02,730
let's start with a set of basis functions right so this is a set of

1453
01:30:02,730 --> 01:30:08,010
functions which can which describes the basic shapes from which we can be of functions

1454
01:30:08,870 --> 01:30:12,390
so then we say that we parameterized function f

1455
01:30:12,410 --> 01:30:20,780
of x uses the parameters w some linear combination of the basis functions and here

1456
01:30:20,780 --> 01:30:23,850
and of course the parents w and

1457
01:30:23,850 --> 01:30:28,410
if just doing something very simple we might say well defined as a set of

1458
01:30:29,760 --> 01:30:31,120
such that our

1459
01:30:31,130 --> 01:30:36,280
training output y i is as close as possible to the function evaluated at the

1460
01:30:36,280 --> 01:30:37,640
training input

1461
01:30:37,660 --> 01:30:39,010
x i

1462
01:30:39,040 --> 01:30:42,780
and of course we use the squared norm because it's the simplest one and want

1463
01:30:42,780 --> 01:30:44,030
to minimize

1464
01:30:44,250 --> 01:30:46,720
this quantity over the training set

1465
01:30:46,970 --> 01:30:52,640
so this very simple but will be bayesian in this lecture so

1466
01:30:53,190 --> 01:30:59,310
this is following calls footsteps so instead i will rephrase this using a probabilistic model

1467
01:30:59,320 --> 01:31:04,440
in particular what would say is that our output y i given our input x

1468
01:31:04,440 --> 01:31:06,440
y and the parameters w

1469
01:31:06,440 --> 01:31:08,420
will be simply

1470
01:31:08,440 --> 01:31:14,380
was it will have the mean given by the function evaluated at x i

1471
01:31:14,470 --> 01:31:15,760
plus some

1472
01:31:17,310 --> 01:31:21,560
absolutely i which is drawn from some girls in distribution

1473
01:31:21,600 --> 01:31:27,040
and then i will also place the prior on properties so wk will be some

1474
01:31:27,040 --> 01:31:31,880
not some constant will because in distributed with zero mean and some

1475
01:31:32,040 --> 01:31:34,290
the variance tempo square

1476
01:31:34,350 --> 01:31:40,100
and what we want to do is compute the posterior as if in conflict you

1477
01:31:40,100 --> 01:31:44,840
OK so let's look at the form of the function that

1478
01:31:44,920 --> 01:31:48,030
the thing is a linear combination of

1479
01:31:48,760 --> 01:31:50,390
basis functions

1480
01:31:50,440 --> 01:31:51,750
so firstly

1481
01:31:51,760 --> 01:31:55,530
what basis functions should we use right so there's this kind of a number of

1482
01:31:55,530 --> 01:32:01,420
questions that we can ask ourselves whether this is a good model firstly what basis

1483
01:32:01,420 --> 01:32:05,320
functions to use you know you could be using the same gaussian shaped

1484
01:32:05,350 --> 01:32:10,320
basis functions that and that basically gives us the assumption that

1485
01:32:10,370 --> 01:32:16,440
all functions as small we can use things like that are kind triangular shaped and

1486
01:32:16,440 --> 01:32:21,500
then that might give us piecewise linear sort of functions for example

1487
01:32:21,530 --> 01:32:25,990
the second question is how many basis functions to reduce so

1488
01:32:26,000 --> 01:32:30,080
the it in the sense this number of basis functions kate house is the complexity

1489
01:32:30,080 --> 01:32:31,370
class of our

1490
01:32:33,320 --> 01:32:34,480
so far

1491
01:32:34,500 --> 01:32:37,810
class of functions that we approximating the true function with

1492
01:32:40,630 --> 01:32:43,530
do we actually have the belief that

1493
01:32:43,560 --> 01:32:45,730
the true function f star

1494
01:32:45,880 --> 01:32:47,440
can be expressed

1495
01:32:48,060 --> 01:32:50,430
a linear combination of basis functions

1496
01:32:50,480 --> 01:32:51,830
for some

1497
01:32:51,860 --> 01:32:54,000
a set of parameters w

1498
01:32:54,020 --> 01:32:56,300
so this is typically unlikely because

1499
01:32:56,320 --> 01:33:02,000
as we know from carl's lecture the typical the true function is typically can very

1500
01:33:02,000 --> 01:33:08,130
complicated and very hard to approximate anyway so this is very unlikely that we can

1501
01:33:08,130 --> 01:33:12,760
come up with a finite set of basis functions that can approximate out to function

1502
01:33:12,760 --> 01:33:14,210
without knowing it

1503
01:33:14,430 --> 01:33:17,590
without knowing the true function to begin with

1504
01:33:17,610 --> 01:33:25,380
and finally do we even believe that the nice processes goes what

1505
01:33:25,400 --> 01:33:28,980
what if we don't actually believe that the nice process goes what can we do

1506
01:33:28,980 --> 01:33:29,880
about it

1507
01:33:29,900 --> 01:33:33,370
so there no questions about so

1508
01:33:33,380 --> 01:33:38,070
we can starting out with this parametric family of functions and these are questions about

1509
01:33:38,130 --> 01:33:41,740
whether we want to go beyond this particular parametric family

1510
01:33:41,750 --> 01:33:42,980
and of course

1511
01:33:43,010 --> 01:33:47,060
because in process is a way of going beyond a parametric family to a non

1512
01:33:47,060 --> 01:33:49,130
parametric family

1513
01:33:49,140 --> 01:33:53,690
no promontory family of functions

1514
01:33:53,690 --> 01:33:55,330
so here's another example

1515
01:33:55,340 --> 01:33:58,050
so you want to do that the estimation

1516
01:33:58,180 --> 01:34:04,890
so this is an unsupervised learning task now and we are

1517
01:34:04,900 --> 01:34:08,670
yes we suppose that some true underlying density f star

1518
01:34:08,690 --> 01:34:13,250
this this and this and from which we do have obtained i i d samples

1519
01:34:13,250 --> 01:34:17,810
from so this this forms of training set x i so the bots here comes

1520
01:34:17,810 --> 01:34:20,400
the i i d samples from after

1521
01:34:20,440 --> 01:34:25,150
and what we want to do is to somehow recover the underlying density here after

1522
01:34:25,170 --> 01:34:27,620
OK from the training set

1523
01:34:27,630 --> 01:34:31,760
again the simplest thing we might want to do is to assume some sort of

1524
01:34:31,760 --> 01:34:38,150
exponential family distribution system for example something like cows so this is the because in

1525
01:34:38,150 --> 01:34:41,030
parameterized by its mean and its covariance

1526
01:34:41,060 --> 01:34:45,060
he has a normalisation term here and then

1527
01:34:45,070 --> 01:34:46,190
here is the

1528
01:34:50,130 --> 01:34:52,900
everybody knows this thing anyway so

1529
01:34:52,920 --> 01:34:56,180
so what's the problem with the girls firstly is union model right

1530
01:34:57,480 --> 01:35:01,370
for something like this way multimodal you would you would definitely not be able to

1531
01:35:01,370 --> 01:35:05,380
fit the data so you just give it a single bump into still trip

1532
01:35:05,560 --> 01:35:10,630
secondly the shape is quite restrictive right so in high dimensions we know that

1533
01:35:10,650 --> 01:35:13,150
the shiva because basically looks like

1534
01:35:13,170 --> 01:35:16,680
like pancake of bubble bobble simply

1535
01:35:16,690 --> 01:35:21,190
are about the people

1536
01:35:21,190 --> 01:35:25,340
and of course it was it has things like like pretty elect tales basically as

1537
01:35:25,340 --> 01:35:30,680
x goes to infinity then the problem the probability of x goes down really quickly

1538
01:35:32,760 --> 01:35:37,520
instead what we might want to do is to use a mixture model right so

1539
01:35:37,760 --> 01:35:40,210
we will promote

1540
01:35:40,230 --> 01:35:41,940
the density f of x

1541
01:35:41,950 --> 01:35:46,330
using a mixture of k different colours in for example

1542
01:35:46,330 --> 01:35:49,000
this lecture is going to be a little bit different from the first one what

1543
01:35:49,000 --> 01:35:50,410
i wanted this lecture

1544
01:35:50,420 --> 01:35:53,510
its cover several topics

1545
01:35:53,540 --> 01:35:59,240
which a fairly elementary level in the sense for the important building blocks for some

1546
01:35:59,240 --> 01:36:02,130
of the other things that you learn about in this summer school so

1547
01:36:03,370 --> 01:36:08,920
while of in themselves they have somewhat limited applicability there are

1548
01:36:08,940 --> 01:36:15,720
very important foundational ideas for things like graphical models approximate inference techniques and so on

1549
01:36:16,330 --> 01:36:22,210
before i start electrodes with dimension one thing very briefly yesterday i talked about my

1550
01:36:22,210 --> 01:36:24,740
sort of personal vision of the third generation of

1551
01:36:24,800 --> 01:36:30,730
machine learning based on graphical models and approximate inference techniques and one of the examples

1552
01:36:30,730 --> 01:36:36,550
i used was a medical example looking for gene environment fractions in in childhood asthma

1553
01:36:37,630 --> 01:36:40,700
this actually is an area in the area of

1554
01:36:40,910 --> 01:36:42,670
by medicine healthcare

1555
01:36:42,670 --> 01:36:46,820
it is one which i think is is one of the prime target areas this

1556
01:36:46,840 --> 01:36:50,910
third generation machine intelligence and it's one that i think we very exciting over the

1557
01:36:50,910 --> 01:36:55,500
next ten years and this is one of the reasons why so it's widely agreed

1558
01:36:55,500 --> 01:36:59,780
that we're at the beginning of the personal healthcare revolution people differ as to what

1559
01:36:59,780 --> 01:37:04,410
form that will take but what are the ingredients is the advent of electronic health

1560
01:37:05,490 --> 01:37:10,330
in the UK because of its national health service is actually relatively to other countries

1561
01:37:10,330 --> 01:37:14,020
quite far advanced least in in in primary health care

1562
01:37:14,030 --> 01:37:16,660
you know your GP or first line position

1563
01:37:16,660 --> 01:37:20,610
we already have access to data set about ten million people have been collected over

1564
01:37:20,610 --> 01:37:24,490
the last twelve years that's pretty big data set really with the UK's and i

1565
01:37:24,490 --> 01:37:29,440
think most countries is fully integrated electronic health records so from cradle to grave or

1566
01:37:29,440 --> 01:37:35,860
your personal information is is available to not only two two physicians to experts and

1567
01:37:35,860 --> 01:37:41,330
people who travel abroad and so on and together that represents represents a big opportunity

1568
01:37:41,330 --> 01:37:46,660
obviously for data mining and in particular for personalized healthcare so one ingredient

1569
01:37:46,670 --> 01:37:49,020
and the UK spending about fourteen

1570
01:37:49,050 --> 01:37:52,640
a billion pounds of taxpayers' money has made some progress towards this

1571
01:37:52,660 --> 01:37:58,830
it's called connecting for health in the UK and most most countries have similar programs

1572
01:37:58,950 --> 01:38:03,230
the interesting thing is personal genomic so today you can go to companies such as

1573
01:38:03,230 --> 01:38:07,460
the ones listed here and then the drop of blood some saliva on measure about

1574
01:38:07,820 --> 01:38:13,120
a few hundred thousand single nucleotide polymorphisms are genetic variations and i'll come back and

1575
01:38:13,120 --> 01:38:17,430
i'll tell you that you've got such and such percentage increase probability of getting cancer

1576
01:38:17,430 --> 01:38:20,490
or you're much less likely to have a heart attack on the average and so

1577
01:38:20,490 --> 01:38:25,170
on all all very primitive i think it's fair to say this stage for a

1578
01:38:25,240 --> 01:38:29,270
glimpse of what might be to come and the glimpse really i think is this

1579
01:38:29,270 --> 01:38:31,040
that the the cost

1580
01:38:34,430 --> 01:38:39,330
the human genome is is plummeting is on an exponential curve but we be the

1581
01:38:39,330 --> 01:38:44,010
halving time is not of eighteen months like most laws more like six months

1582
01:38:44,020 --> 01:38:48,910
and so while the first the human genome project cost several billion dollars was a

1583
01:38:48,910 --> 01:38:54,740
planet wide collaboration there is now the next prize for technology in which the recurring

1584
01:38:54,740 --> 01:38:58,180
costs will be just ten thousand dollars to sequence the entire human genome most people

1585
01:38:58,180 --> 01:39:02,340
expect that price became very soon the national institute of health in the US senate

1586
01:39:02,340 --> 01:39:07,680
target of twenty fourteen four one thousand dollar the human genome so sometimes the next

1587
01:39:07,680 --> 01:39:12,110
few years for the less the price of laptop you have your entire genome sequenced

1588
01:39:12,110 --> 01:39:13,830
and the question is so what

1589
01:39:13,830 --> 01:39:16,650
OK in that

1590
01:39:16,670 --> 01:39:20,570
we've got this long history of season and is what does it mean question that's

1591
01:39:20,570 --> 01:39:23,710
really that's really the question so by looking at

1592
01:39:25,050 --> 01:39:31,800
the phenotype physiological environmental data from health records combining it with the

1593
01:39:32,320 --> 01:39:38,510
the what's said to be an explosion in genomic data there's an opportunity to

1594
01:39:38,520 --> 01:39:43,740
to realize this personal healthcare revolution to tailor drugs to individuals and in all sorts

1595
01:39:43,740 --> 01:39:48,240
of things with huge benefits financial benefits and benefits in terms of the quality of

1596
01:39:48,240 --> 01:39:51,840
healthcare people receive but the heart of it of course in statistics we're not going

1597
01:39:51,840 --> 01:39:55,710
to build first principle simulations of the human body that would be the ultimate way

1598
01:39:55,710 --> 01:39:59,120
of doing this that's not gonna happen probably for many centuries in the mean time

1599
01:39:59,120 --> 01:40:02,790
we can look at statistics of large data sets so this is actually quite a

1600
01:40:02,790 --> 01:40:05,890
big this the reason i'm mentioning all of this is because this is one of

1601
01:40:05,890 --> 01:40:11,660
the big growth areas for the lab here in cambridge we're offering scholarships internships people

1602
01:40:11,660 --> 01:40:16,390
partly through the phd for three months and postdoctoral fellowships if anybody's interested in that

1603
01:40:16,390 --> 01:40:20,140
instead of sharing that vision and working with us on that please come and find

1604
01:40:20,140 --> 01:40:22,820
me at some point during the summer school

1605
01:40:23,070 --> 01:40:26,510
all right so on with the tutorial so i said i was going to cover

1606
01:40:26,510 --> 01:40:31,400
a bunch of the basic but very important topics to do with with bayesian inference

1607
01:40:31,450 --> 01:40:35,950
and i thought i'd start with a little bit of the discussion about why

1608
01:40:35,960 --> 01:40:39,710
why bother with all these probabilities at all in the first place

1609
01:40:41,910 --> 01:40:43,320
let's consider

1610
01:40:43,480 --> 01:40:47,340
an example is hypothetical example puts the numbers in the these numbers are just pulled

1611
01:40:47,340 --> 01:40:52,340
out of thin air but so so just take them as initiative not not representative

1612
01:40:52,380 --> 01:40:53,730
of the real world

1613
01:40:53,760 --> 01:40:59,470
let's imagine we want to build a medical screening system to undertake people emotionally healthy

1614
01:40:59,520 --> 01:41:03,830
we want to screen for for some kind of cancer say let's say it image

1615
01:41:04,940 --> 01:41:09,480
and one of these images in fact depicts a pre-cancerous condition and the others is

1616
01:41:09,480 --> 01:41:12,840
normal in this case it's quite easy to tell apart in other cases it's quite

1617
01:41:14,120 --> 01:41:16,700
so what we have is an image

1618
01:41:16,730 --> 01:41:18,850
vector x that we collect

1619
01:41:18,870 --> 01:41:25,130
from from from the population in general and we want to classify these as either

1620
01:41:25,130 --> 01:41:27,230
cancer or precancerous

1621
01:41:27,250 --> 01:41:32,370
or normal so that's the goal to classification problem

1622
01:41:32,390 --> 01:41:35,580
and so effectively that that's the decision we need to

1623
01:41:35,580 --> 01:41:38,650
take an image as input and the end of the day we need to make

1624
01:41:38,650 --> 01:41:42,010
a decision either to sell that person that's fine come back in three years for

1625
01:41:42,010 --> 01:41:47,250
your next screening or to say something not quite right here would like to do

1626
01:41:47,250 --> 01:41:48,560
some more tests

1627
01:41:48,580 --> 01:41:51,380
OK this the two sort of possible outcomes

1628
01:41:51,390 --> 01:41:55,450
so you could just build some sort of machine which sucks in images the inputs

1629
01:41:55,450 --> 01:42:00,520
and spits out cancer normal the output of this sort of decision engine we certainly

1630
01:42:00,520 --> 01:42:02,950
do that could be very useful

1631
01:42:02,970 --> 01:42:08,140
we could in two steps we could go via probabilities we can compute

1632
01:42:08,140 --> 01:42:09,770
the posterior probability

1633
01:42:09,780 --> 01:42:14,600
that's a vector x belongs to the council the normal class and subsequent subsequently we

1634
01:42:14,600 --> 01:42:19,020
could use the probabilities in the decision steps to make that decision

1635
01:42:19,030 --> 01:42:23,650
and seems appealing because the probability is of course tell us whether we very confident

1636
01:42:23,650 --> 01:42:28,250
or other machine is very confident that this person is normal or whether i think

1637
01:42:28,250 --> 01:42:34,840
something about this course this is a course for non-scientists that portion of the the

1638
01:42:34,920 --> 01:42:41,050
enrollment policies is not a suggestion i really don't want science majors in this class

1639
01:42:41,050 --> 01:42:44,510
if you are a science major i'm going to notice because that's one of the

1640
01:42:44,510 --> 01:42:49,650
things that appears on the class list which are major is so don't don't take

1641
01:42:49,650 --> 01:42:55,140
the course of you're science major let me point out that are the freshman don't

1642
01:42:55,140 --> 01:43:00,090
have a major so it doesn't matter if you intend to be a science major

1643
01:43:00,090 --> 01:43:04,880
if you're if you're freshman if you are science major i recommend astro to ten

1644
01:43:05,020 --> 01:43:08,950
which is being given this term i have a little hand out all the different

1645
01:43:08,950 --> 01:43:14,210
introductory astronomy courses at the front of the room if you're interested

1646
01:43:14,230 --> 01:43:15,200
let's say

1647
01:43:16,520 --> 01:43:18,370
it is also true

1648
01:43:18,390 --> 01:43:25,160
that this course is kind intended for non-science majors who have a certain basic high

1649
01:43:25,160 --> 01:43:26,310
school level

1650
01:43:26,430 --> 01:43:33,280
comfort with sort of tenth grade science and math if you're extremely phobic about these

1651
01:43:33,280 --> 01:43:35,090
kinds of things i would say

1652
01:43:35,130 --> 01:43:41,490
astronomy one twenty while has a similar level of math has a so much shallower

1653
01:43:41,490 --> 01:43:45,650
learning curve and is somewhat deeper safety net so if you're the kind of person

1654
01:43:45,650 --> 01:43:49,930
who breaks into a sweat when somebody writes down an equal sign check out one

1655
01:43:52,450 --> 01:43:54,290
let's see

1656
01:43:54,310 --> 01:43:58,440
but that's not the biggest difference between this class and one twenty i think the

1657
01:43:58,440 --> 01:44:01,980
biggest difference is what the classes trying to do

1658
01:44:02,050 --> 01:44:06,690
astronomy one twenty and also one ten and another courses in our department and elsewhere

1659
01:44:06,690 --> 01:44:12,910
in the university are basically survey courses most introductory science courses are survey courses that

1660
01:44:12,910 --> 01:44:18,910
cover a fairly wide subject matter this isn't that what this course is supposed to

1661
01:44:18,910 --> 01:44:25,650
do is we're going to talk about three particular topics in a very considerable detail

1662
01:44:25,670 --> 01:44:32,670
enough detail so that by the end of our discussion you'll understand what's going on

1663
01:44:32,670 --> 01:44:38,010
in current research in this topic and by current i don't need this decade i

1664
01:44:38,010 --> 01:44:45,280
mean this week astronomy is currently in this stage of very rapid advancement and one

1665
01:44:45,280 --> 01:44:48,800
of the things that happened every time i've taught this course in the past is

1666
01:44:48,800 --> 01:44:52,990
that at some point during the semester someone will publish some piece of research which

1667
01:44:52,990 --> 01:44:58,520
changes some aspect of the curriculum coming waving some paper and everything will be changed

1668
01:44:59,130 --> 01:45:02,680
and i can guarantee that of course because i can't predict the future but it

1669
01:45:02,690 --> 01:45:06,240
happened every time in the past so we're really are trying to get you all

1670
01:45:06,240 --> 01:45:10,000
the way out to the frontiers of the subject

1671
01:45:10,010 --> 01:45:14,940
i think this is actually a better approach for non-science majors because after all we

1672
01:45:14,940 --> 01:45:19,060
live in the internet age if you want to find out a bunch of facts

1673
01:45:19,060 --> 01:45:23,840
about some scientific topic you can go online to go to wikipedia or wherever look

1674
01:45:23,840 --> 01:45:29,760
these facts up that's not a big problem the problem comes when there are two

1675
01:45:29,760 --> 01:45:35,120
sets of facts which directly contradict each other this happens quite frequently in

1676
01:45:35,680 --> 01:45:42,640
in scientific topics these days particularly those with a kind of political or moral overtones

1677
01:45:42,640 --> 01:45:46,190
and you get you get facts that directly contradict each other what you're supposed to

1678
01:45:46,190 --> 01:45:48,520
do about that

1679
01:45:48,530 --> 01:45:53,780
what i'm hoping is that by talking about situations in which the fact at the

1680
01:45:53,780 --> 01:46:00,940
moment really are known yet you can develop some skills in interpreting these kind of

1681
01:46:00,940 --> 01:46:03,090
contradictory facts for yourself

1682
01:46:03,140 --> 01:46:06,940
if you don't do that then the only alternative is to listen to the experts

1683
01:46:06,940 --> 01:46:13,440
argue with each other and vote for whoever argues the loudest or looks the best

1684
01:46:13,440 --> 01:46:18,110
when they're doing it or has a degree from harvard or whatever it is and

1685
01:46:18,300 --> 01:46:22,410
you guys can do better than that and so the hope is that by

1686
01:46:22,840 --> 01:46:29,500
practising this kind of scale of evaluating science when the answer is not fully understood

1687
01:46:29,540 --> 01:46:34,640
that you can develop skills that will stand you in good stead when you run

1688
01:46:34,640 --> 01:46:40,940
into scientific controversies in a political context or legal context or just as ordinary citizens

1689
01:46:40,940 --> 01:46:43,090
in the course of your lives

1690
01:46:43,100 --> 01:46:51,200
it also happens that the three particular topics i think of some some real interest

1691
01:46:51,200 --> 01:46:57,080
and importance in themselves and i'll get to the three topics again in just a

1692
01:46:57,080 --> 01:47:00,640
moment here but let me point out that this kind of approach has the downside

1693
01:47:00,650 --> 01:47:05,050
to it and this has been pointed out repeatedly on course evaluations because we're dealing

1694
01:47:05,050 --> 01:47:10,210
with stuff which ultimately the answers are not yet understood there's no textbooks

1695
01:47:10,260 --> 01:47:13,800
there can be a textbook we have figured out what to put in the textbooks

1696
01:47:13,800 --> 01:47:18,160
you and the problem for with that is that that makes the lecture is very

1697
01:47:18,160 --> 01:47:21,420
important because that's the only information you're going to get there's a whole bunch of

1698
01:47:21,420 --> 01:47:24,540
on line readings and stuff but they tend to have a point of view

1699
01:47:24,550 --> 01:47:28,450
and so it's really the lectures that are the basis of course

1700
01:47:28,460 --> 01:47:32,220
the problem with that is that i've chosen to give this course at the ungodly

1701
01:47:32,220 --> 01:47:35,650
early hour of nine thirty in the morning and you guys are going to have

1702
01:47:35,650 --> 01:47:40,170
to show up and so here's the deal maker deal with you

1703
01:47:40,200 --> 01:47:44,870
your job is to get to class by nine thirty in the morning my job

1704
01:47:44,870 --> 01:47:47,000
is to keep you awake once here

1705
01:47:47,050 --> 01:47:54,290
OK so if we both succeed in cooperating in this and will probably will probably

1706
01:47:54,290 --> 01:47:59,940
be OK but but seriously if you're if you're anticipating regular difficulties in getting to

1707
01:47:59,940 --> 01:48:03,890
class this is not actually a great class to take because there's no backup in

1708
01:48:03,890 --> 01:48:06,100
the form of the textbook

1709
01:48:06,210 --> 01:48:13,560
alright the particular topics that are under discussion i listed them here in green

1710
01:48:13,610 --> 01:48:18,360
the first of them are extrasolar planets by which i mean planets around stars other

1711
01:48:18,360 --> 01:48:22,890
than the sun it's well known that there are many many of these planets all

1712
01:48:22,900 --> 01:48:27,070
you have to do is watch star trek or something like that and you you'll

1713
01:48:27,070 --> 01:48:31,700
find many many examples and this has been a staple of science fiction for quite

1714
01:48:31,700 --> 01:48:33,210
a long time

1715
01:48:33,220 --> 01:48:39,450
oddly enough until ten years ago there was absolutely no evidence for this we assumed

1716
01:48:39,450 --> 01:48:42,960
that because the stars the normal starring there many other stars of the sun is

1717
01:48:42,960 --> 01:48:46,090
the normal starring there are many other stars like the sun that there must be

1718
01:48:46,090 --> 01:48:51,450
many planets of the same kind as the planet and also the solar system circling

1719
01:48:51,450 --> 01:48:55,570
around all these other stars but until nineteen ninety five there was not one bit

1720
01:48:55,570 --> 01:48:57,590
of evidence to support that idea

1721
01:48:57,690 --> 01:49:02,510
since nineteen ninety five this has become a huge growth industry and research we now

1722
01:49:02,510 --> 01:49:07,420
know of hundreds literally hundreds of planet all of them discovered in the last ten

1723
01:49:07,420 --> 01:49:11,730
years and so this is the situation in which what ten years ago was science

1724
01:49:11,730 --> 01:49:16,070
no no we have to what what we what we tried was like maybe taking

1725
01:49:16,090 --> 01:49:17,860
different amounts of time

1726
01:49:17,880 --> 01:49:18,840
trying to see

1727
01:49:18,840 --> 01:49:22,810
how hard that changes and that that didn't change much but we didn't specifically look

1728
01:49:22,810 --> 01:49:25,250
at how it evolves over time

1729
01:49:25,670 --> 01:49:29,360
so i don't have an answer for that so the only the only thing is

1730
01:49:29,360 --> 01:49:32,150
that we find that for for many times

1731
01:49:32,210 --> 01:49:35,730
especially like for example for being in the exponent is not

1732
01:49:35,770 --> 01:49:39,460
uniform over all the degree of danger so for example in

1733
01:49:39,460 --> 01:49:43,190
link in your you get this like sort of three three three

1734
01:49:43,210 --> 01:49:47,310
types of behaviour if you have a small degree no you are very sticky

1735
01:49:47,320 --> 01:49:50,570
then you and i mean the range you sort of

1736
01:49:50,590 --> 01:49:54,750
average to continue for a high degree node again you are suppressed

1737
01:49:54,770 --> 01:49:58,360
so we found is that for example especially thinking that sort of three ranges of

1738
01:49:58,380 --> 01:50:01,670
the same while in the rest of the networks it seems you speaking at the

1739
01:50:01,670 --> 01:50:04,750
same rate regardless of what you

1740
01:50:08,360 --> 01:50:12,690
we're dealing with one

1741
01:50:13,000 --> 01:50:18,170
he was

1742
01:50:24,210 --> 01:50:29,670
so so we were talking to them actually turns out there are people who put

1743
01:50:29,670 --> 01:50:32,940
email addresses in the profile and these are people who just want to have as

1744
01:50:32,940 --> 01:50:36,770
most things as possible and i think one thing that the person has forty plus

1745
01:50:36,770 --> 01:50:38,000
thousand things

1746
01:50:38,840 --> 01:50:42,980
that the conjecture is that these people are these people sort of the people that

1747
01:50:42,980 --> 01:50:46,290
are very sticky at the people who just getting things and doing nothing else and

1748
01:50:46,290 --> 01:50:48,940
they actually put an email that you can even wider

1749
01:50:48,960 --> 01:50:54,320
and then the low degree not guys again sticky and talking to them was because

1750
01:50:54,320 --> 01:50:58,250
they trying to send the more more reminders about what is going on around the

1751
01:50:58,250 --> 01:51:00,400
network to create more edges

1752
01:51:00,420 --> 01:51:04,270
so this is sort of two reasons why they are there are more stick it

1753
01:51:04,270 --> 01:51:06,400
on a very limited range

1754
01:51:10,900 --> 01:51:13,840
here are

1755
01:51:13,900 --> 01:51:17,000
o for which

1756
01:51:17,270 --> 01:51:21,310
you believe that

1757
01:51:39,170 --> 01:51:40,130
well we

1758
01:51:52,110 --> 01:51:57,860
that could be the exponential basically explanation could i think that's a good point actually

1759
01:51:57,860 --> 01:52:00,110
i think that what is what is

1760
01:52:00,170 --> 01:52:04,190
going going to sort of some kind of random surfer type model where like maker

1761
01:52:04,190 --> 01:52:06,840
hopping decided could not and continue

1762
01:52:06,840 --> 01:52:11,110
and just because high degree nodes have sort of more that's fascinating to them

1763
01:52:11,150 --> 01:52:16,000
that that could that that that sort of the mechanism for reference that could be

1764
01:52:16,000 --> 01:52:18,020
the mechanism for preferential attachment

1765
01:52:18,020 --> 01:52:21,840
but you are more likely to keep on the other hand what is also could

1766
01:52:21,840 --> 01:52:24,070
be a problem in our data right is the

1767
01:52:24,070 --> 01:52:28,460
the the natural way to close this is by trying to see all these of

1768
01:52:28,460 --> 01:52:31,710
your friends created this new french and then you go through them and we can

1769
01:52:31,710 --> 01:52:35,190
see that it is exactly creating triangles

1770
01:52:35,210 --> 01:52:36,050
so that's

1771
01:52:36,070 --> 01:52:37,090
maybe another

1772
01:52:37,090 --> 01:52:40,840
another reason why i they is not too realistic

1773
01:52:45,500 --> 01:52:49,460
very well

1774
01:52:49,480 --> 01:52:51,460
you could actually

1775
01:52:51,480 --> 01:52:54,170
they a three

1776
01:52:58,170 --> 01:52:59,670
the a

1777
01:53:03,900 --> 01:53:05,440
but was

1778
01:53:06,400 --> 01:53:07,500
they offer

1779
01:53:07,500 --> 01:53:09,480
well there

1780
01:53:10,670 --> 01:53:16,070
they were they actually

1781
01:53:16,090 --> 01:53:20,360
i think this is an excellent point the reason so to be able to i

1782
01:53:20,360 --> 01:53:23,110
think to be able to verify we would have to go

1783
01:53:23,170 --> 01:53:24,550
beyond two hops

1784
01:53:25,610 --> 01:53:29,190
we could go beyond two hubs because the number of possible that's increases so much

1785
01:53:29,210 --> 01:53:31,980
that you can't compute any more

1786
01:53:32,000 --> 01:53:34,730
so that's the reason why sort of one of the reasons why we are so

1787
01:53:34,730 --> 01:53:38,480
focused on trying to see just because we were able to compute the likely

1788
01:53:38,820 --> 01:53:43,630
but i think that's that's a great point and actually we should be

1789
01:54:27,250 --> 01:54:33,020
so that is that if you just look at i know the number how does

1790
01:54:33,040 --> 01:54:37,440
the k core structure so course so basically you like feeling of the network by

1791
01:54:37,750 --> 01:54:41,650
removing clothing he also wrote the removal of the degree one nodes of degree two

1792
01:54:41,650 --> 01:54:43,440
thousand see hundred workforce

1793
01:54:43,500 --> 01:54:44,460
and i

1794
01:54:44,500 --> 01:54:49,400
if we don't release if the community says small then we should see sort

1795
01:54:49,400 --> 01:54:53,230
one component and then as as we keep deleting things at some point in time

1796
01:54:53,270 --> 01:54:57,170
see a lot of them and then we would be just left one by one

1797
01:54:57,290 --> 01:55:01,130
the question is how these pieces are attached to write and i don't know if

1798
01:55:01,150 --> 01:55:03,270
it is some kind of threshold or something

1799
01:55:03,710 --> 01:55:07,630
because it it's not clear you know is it like to high degree nodes

1800
01:55:07,630 --> 01:55:09,670
having this in the lead like

1801
01:55:09,690 --> 01:55:14,110
two low degree nodes having these being bridges between the two communities right

1802
01:55:24,750 --> 01:55:30,400
four four

1803
01:55:36,980 --> 01:55:40,610
four the

1804
01:55:41,150 --> 01:55:51,460
she the OK that's a great that's a great point because what we want science

1805
01:55:51,460 --> 01:55:56,130
subject right what he wanted to do was this and this is likely remove the

1806
01:55:56,130 --> 01:56:00,940
risk strategies which is like the moved see the components are connected by a single

1807
01:56:00,940 --> 01:56:05,900
so this is why when you people perform this simple substitution of the two for

1808
01:56:05,900 --> 01:56:07,160
the one norm

1809
01:56:07,190 --> 01:56:10,580
we're going to have some of the weights which i exactly zero and this

1810
01:56:10,650 --> 01:56:14,260
scheme performs of feature selection in that

1811
01:56:14,260 --> 01:56:15,270
since has been

1812
01:56:15,290 --> 01:56:18,970
very popular recently people just

1813
01:56:18,980 --> 01:56:21,820
just use that because some

1814
01:56:21,820 --> 01:56:22,970
now i mean

1815
01:56:22,980 --> 01:56:23,940
people like

1816
01:56:25,120 --> 01:56:26,970
but to decision problems that are well

1817
01:56:31,050 --> 01:56:35,720
now let's go back to the zero norm problem

1818
01:56:35,760 --> 01:56:41,550
minimizing the one is kind of surrogates to problem we we see that because of

1819
01:56:41,560 --> 01:56:43,820
his diamond

1820
01:56:43,850 --> 01:56:45,550
the shape

1821
01:56:45,570 --> 01:56:49,910
objective we're going to have some ways are going to be equal to zero

1822
01:56:49,940 --> 01:56:57,390
but all real objective was to actually minimize the number of nonzero weights

1823
01:56:57,400 --> 01:57:02,220
and that's what's called the the zero and zero is nothing but the

1824
01:57:02,820 --> 01:57:06,680
number of nonzero weights

1825
01:57:06,690 --> 01:57:10,730
as we said before it's and st objective to optimize

1826
01:57:10,740 --> 01:57:13,150
so we can replace it by some

1827
01:57:13,160 --> 01:57:16,470
more well-behaved functions

1828
01:57:16,470 --> 01:57:21,610
and wasn't collaborators have devised a scheme to minimize the objective

1829
01:57:21,660 --> 01:57:26,400
and they show that actually boils down to very simple algorithm

1830
01:57:26,410 --> 01:57:28,650
you start with some sigma

1831
01:57:28,650 --> 01:57:30,470
that all scaling factors

1832
01:57:30,550 --> 01:57:32,530
and they all equal to one

1833
01:57:32,570 --> 01:57:35,610
then compute the solution of your SVM

1834
01:57:35,640 --> 01:57:39,700
on some datasets

1835
01:57:39,730 --> 01:57:42,860
and then what you're going to do is going to define scaling factors that are

1836
01:57:42,980 --> 01:57:45,140
actually values of your

1837
01:57:45,570 --> 01:57:48,190
of w star

1838
01:57:48,320 --> 01:57:53,190
and you re-evaluate your scaling factor at each step

1839
01:57:53,190 --> 01:57:57,140
by multiplying them by the w star

1840
01:57:57,140 --> 01:58:01,660
in absolute values and very easy what you do is that you're going to risk

1841
01:58:01,830 --> 01:58:04,140
your input at each iteration

1842
01:58:04,150 --> 01:58:06,230
but how important they are

1843
01:58:06,280 --> 01:58:07,440
to the predictor

1844
01:58:07,450 --> 01:58:10,070
using the absolute value of the weights

1845
01:58:10,120 --> 01:58:12,720
and you have some kind of an exponential decay

1846
01:58:12,760 --> 01:58:16,050
of those least important features

1847
01:58:18,070 --> 01:58:23,350
still linear SVM but it can be generalized to the nonlinear is here

1848
01:58:23,350 --> 01:58:27,440
the in a similar way what you do for

1849
01:58:27,470 --> 01:58:29,770
recursive feature elimination

1850
01:58:29,850 --> 01:58:33,720
in that case you don't choose the absolute value of the w

1851
01:58:34,640 --> 01:58:38,220
but you scaling factors that correspond to

1852
01:58:38,230 --> 01:58:40,270
the derivative of

1853
01:58:40,350 --> 01:58:41,900
you're cost functions

1854
01:58:41,930 --> 01:58:43,260
with respect

1855
01:58:43,280 --> 01:58:46,700
two the variables

1856
01:58:46,740 --> 01:58:48,240
and this can be

1857
01:58:48,290 --> 01:58:55,470
efficiently optimize efficiently approximated if you assume that the support vectors don't change

1858
01:58:57,570 --> 01:59:02,020
two consecutive steps that is when you remove the future that you're interested in if

1859
01:59:02,020 --> 01:59:06,270
you assume that this for the global change you can easily estimate that

1860
01:59:06,290 --> 01:59:08,610
but actually

1861
01:59:08,680 --> 01:59:13,700
find the original papers the authors only dealt with

1862
01:59:13,730 --> 01:59:15,320
the linear SVM

1863
01:59:15,330 --> 01:59:17,390
and this is you know

1864
01:59:18,070 --> 01:59:25,890
completely straightforward to implement and and a very efficient method

1865
01:59:25,890 --> 01:59:30,370
so in summary for embedded methods

1866
01:59:30,400 --> 01:59:34,850
and then this is a good inspiration to design new feature selection algorithms for you

1867
01:59:34,850 --> 01:59:39,310
know you're all algorithms so for example if you have like you know how to

1868
01:59:39,330 --> 01:59:43,480
do it in some question how can we do feature selection for nonstationary process so

1869
01:59:43,510 --> 01:59:47,900
how can we do feature selection for this so that other algorithms what the answer

1870
01:59:47,900 --> 01:59:50,600
would be that know tried

1871
01:59:50,610 --> 01:59:55,470
try methods of scaling factors applied to massive scanning factors in the process of of

1872
01:59:56,260 --> 02:00:00,560
this is this is one simple way of doing things

1873
02:00:00,620 --> 02:00:05,310
but this is not too far from wrapper techniques and can be extended to multiclass

1874
02:00:05,310 --> 02:00:10,890
into regression but they have the advantage that they faster

1875
02:00:10,900 --> 02:00:16,070
then the proper techniques you don't have to do with what as many trainings of

1876
02:00:16,070 --> 02:00:20,010
of classifiers or predictors

1877
02:00:20,020 --> 02:00:23,320
so in other words we're talking about algorithms and

1878
02:00:23,320 --> 02:00:28,440
technical things we're going to go into more philosophical things going to talk about the

1879
02:00:28,440 --> 02:00:31,160
problem of causality

1880
02:00:31,180 --> 02:00:33,160
so far

1881
02:00:33,440 --> 02:00:36,550
just said that you know feature selection

1882
02:00:36,570 --> 02:00:41,560
deals with removing features to improve or least degrade prediction of y

1883
02:00:41,570 --> 02:00:43,020
regardless of how

1884
02:00:43,030 --> 02:00:47,730
these are pictures all wired to the target

1885
02:00:47,740 --> 02:00:52,830
and i've given you these two examples as the justification why one should use multivariate

1886
02:00:52,830 --> 02:00:57,180
feature selection not just univariate feature selection

1887
02:00:57,190 --> 02:01:02,640
number to tell you that everything i told you so far was long

1888
02:01:02,700 --> 02:01:10,350
and actually is very dangerous to do multivariate feature selection

1889
02:01:10,490 --> 02:01:13,470
this is an example from real life data

1890
02:01:13,480 --> 02:01:17,470
in which we have discovered when features

1891
02:01:17,520 --> 02:01:20,070
and that was very

1892
02:01:20,150 --> 02:01:22,530
so discriminant by itself

1893
02:01:22,550 --> 02:01:24,190
and then the second feature

1894
02:01:24,220 --> 02:01:25,350
that helped

1895
02:01:25,370 --> 02:01:29,900
the first feature but by itself was not this at all

1896
02:01:29,910 --> 02:01:33,310
this example is an example of mass spectrometry

1897
02:01:33,320 --> 02:01:39,440
in which all the points in the spectrum were used as control

1898
02:01:40,410 --> 02:01:46,830
the opportunity here is the value of the teacher

1899
02:01:46,850 --> 02:01:49,680
so for example at this position here

1900
02:01:49,700 --> 02:01:53,120
we have overlaid on the spectrum of one class which are the rights that trying

1901
02:01:53,230 --> 02:01:56,900
to speak of the other class the green spectrum so you can see the examples

1902
02:01:56,900 --> 02:02:01,270
of the red class are well separated from the examples of the great class using

1903
02:02:01,270 --> 02:02:03,220
this particular feature x one

1904
02:02:03,230 --> 02:02:07,060
this would be that feature in the scatterplot

1905
02:02:09,230 --> 02:02:11,010
there was another hx two

1906
02:02:11,020 --> 02:02:14,270
they had been extracted by or feature selection algorithm

