1
00:00:00,000 --> 00:00:02,690
then you have to know what it means to just

2
00:00:02,710 --> 00:00:04,380
talk about people

3
00:00:04,420 --> 00:00:08,320
and you say my income is three db lower during

4
00:00:08,340 --> 00:00:10,840
and that makes him very happy

5
00:00:10,840 --> 00:00:13,980
OK so

6
00:00:17,650 --> 00:00:22,650
that's all we need for this simple example we now want to go into

7
00:00:22,650 --> 00:00:25,460
more complicated things

8
00:00:25,480 --> 00:00:28,650
OK pulse amplitude modulation

9
00:00:28,690 --> 00:00:30,480
this is one of the

10
00:00:30,500 --> 00:00:32,530
major ways

11
00:00:32,610 --> 00:00:39,000
of turning bits into signals and then turning the signals and the waveforms again

12
00:00:39,050 --> 00:00:44,820
again i'm doing this first not because it's the most important schemes talk about

13
00:00:44,840 --> 00:00:48,590
but because we want to understand these things one by one

14
00:00:48,610 --> 00:00:54,300
and we understand p a and all talk about QAM and you'll understand that and

15
00:00:54,300 --> 00:00:59,380
then we want to look at other other variations of these things

16
00:00:59,400 --> 00:01:03,050
so the signals in PAAM are one-dimensional

17
00:01:03,070 --> 00:01:05,170
OK the consolation

18
00:01:05,230 --> 00:01:08,130
the only thing that can be sense one-dimensional

19
00:01:08,150 --> 00:01:09,920
one real dimension

20
00:01:09,940 --> 00:01:12,360
is the set of real numbers

21
00:01:14,270 --> 00:01:16,510
so you're going to modulate

22
00:01:16,570 --> 00:01:18,980
these are real numbers

23
00:01:19,000 --> 00:01:22,750
now we're going to talk about later how do you find this function here we

24
00:01:22,750 --> 00:01:26,480
are just going to take these real numbers which are coming in to the transmitter

25
00:01:26,480 --> 00:01:29,210
one by one out of the digital encoder

26
00:01:29,230 --> 00:01:34,480
you take these numbers you're gonna view them as being multiplied by the late impulses

27
00:01:34,480 --> 00:01:36,380
and then passed through a filter

28
00:01:36,400 --> 00:01:39,480
and the filter responses just

29
00:01:39,530 --> 00:01:41,500
impulse responses p of ten

30
00:01:41,530 --> 00:01:46,550
in other words what we're doing is saying we don't like the sinc function

31
00:01:46,630 --> 00:01:48,980
therefore the simplest thing to do

32
00:01:49,040 --> 00:01:52,500
this replaced the same function by something we do like

33
00:01:52,550 --> 00:01:58,230
so we're going to replace the same function by some filter characteristic which we like

34
00:01:58,510 --> 00:02:01,380
and that's the way to modify

35
00:02:01,420 --> 00:02:03,400
the previous example

36
00:02:03,400 --> 00:02:07,770
into something that makes better sense so we're doing two things here we're talking about

37
00:02:07,770 --> 00:02:14,000
BAM one we're talking about generalizing this binary signal set

38
00:02:14,000 --> 00:02:14,820
and two

39
00:02:14,820 --> 00:02:17,900
we're talking about generalizing the same function

40
00:02:17,900 --> 00:02:21,900
and to some arbitrary impulse response

41
00:02:21,920 --> 00:02:25,420
OK so standard PIM signal set

42
00:02:25,440 --> 00:02:30,270
users equally spaced signals symmetric around zero

43
00:02:30,690 --> 00:02:34,980
and if you look at the picture makes it clear what this means

44
00:02:35,210 --> 00:02:40,480
it's the same thing we were using all along we are talking about quantisation

45
00:02:40,530 --> 00:02:43,550
if you're looking at one dimensional quantization

46
00:02:43,570 --> 00:02:46,030
that's a very natural way

47
00:02:46,070 --> 00:02:48,980
to choose the representation points

48
00:02:49,000 --> 00:02:54,840
here we're doing everything in the opposite way so we're starting out with these points

49
00:02:54,900 --> 00:03:01,400
well we're starting out with a symbols and turning them into a signals

50
00:03:01,420 --> 00:03:05,250
and when you take a symbols and turn them into eight signals

51
00:03:05,270 --> 00:03:07,300
perfectly natural thing to do

52
00:03:07,320 --> 00:03:08,230
is to

53
00:03:08,610 --> 00:03:10,960
make each of these signals

54
00:03:11,010 --> 00:03:15,710
equally spaced from each other and centered on the origin

55
00:03:15,820 --> 00:03:17,230
it was

56
00:03:17,250 --> 00:03:18,480
now when the

57
00:03:18,500 --> 00:03:20,900
this is not something we have to see later

58
00:03:20,940 --> 00:03:30,130
i think you can see that that these symbols or are used with equal probability

59
00:03:30,130 --> 00:03:31,530
and you're trying to

60
00:03:31,550 --> 00:03:33,960
reduce the amount of energy

61
00:03:34,010 --> 00:03:36,340
that the signals use

62
00:03:36,460 --> 00:03:41,640
which will then go through and to reduce the amount of energy in the wave

63
00:03:41,640 --> 00:03:43,980
forms that were transmitting

64
00:03:44,130 --> 00:03:49,190
it's nice to have them centered around zero

65
00:03:49,190 --> 00:03:53,570
because they have the means that mean is just going to contribute directly

66
00:03:53,590 --> 00:03:55,280
to the expected

67
00:03:55,300 --> 00:04:00,750
i mean square value of the signal you're using so so why not centred around

68
00:04:01,900 --> 00:04:08,610
later on we'll see many reasons for that centring around zero but for now we're

69
00:04:08,610 --> 00:04:12,610
not going to worry about any of those and we're going to senator round zero

70
00:04:12,710 --> 00:04:16,690
OK and the other thing is why do you want them to be equally spaced

71
00:04:16,690 --> 00:04:20,030
well i'll talk about the injustice

72
00:04:24,440 --> 00:04:25,820
the signal energy

73
00:04:25,820 --> 00:04:30,180
some high level knowledge here some hypothesis space of grammars this is what something was

74
00:04:30,180 --> 00:04:35,160
called universal grammar or any in any computational linguistic tries to to grammar induction believes

75
00:04:35,160 --> 00:04:40,160
in some form of universal grammar some hypothesis space of possible grammars and then in

76
00:04:40,160 --> 00:04:43,840
this case learning the grammar is bayesian inference up at this level which requires working

77
00:04:44,070 --> 00:04:48,210
up through multiple levels of not directly observable information in order to figure out what

78
00:04:48,210 --> 00:04:53,500
these rules are so that's the idea of hierarchical bayesian inference here over multiple levels

79
00:04:53,500 --> 00:04:58,020
with increasingly structured abstract representations as you move up in the hierarchy and the y

80
00:04:58,020 --> 00:05:02,870
i don't know anybody in natural language processing tools implemented exactly all these levels of

81
00:05:02,870 --> 00:05:06,540
various people implement various pieces of this hierarchy and as a whole this picture i

82
00:05:06,540 --> 00:05:10,050
think captures you know resembles view of the state of the art of how we

83
00:05:10,050 --> 00:05:15,020
think something like the integrated problem of passing and language acquisition might be able to

84
00:05:16,290 --> 00:05:19,890
similar kinds of ideas are being explored in vision like this you again this sort

85
00:05:19,890 --> 00:05:23,890
of a long history of thinking about visual scene interpretation as a kind of passing

86
00:05:23,890 --> 00:05:26,820
and if you like the work but rather willsky and others that i should the

87
00:05:26,820 --> 00:05:31,310
beginning it can be seen as a kind of scene passing but there's a complimentary

88
00:05:31,310 --> 00:05:36,980
direction this the more sort of hierarchical structured probabilistic approach which is being developed by

89
00:05:36,980 --> 00:05:41,500
people like sanctioned due in our new orleans two demon which is as more based

90
00:05:41,500 --> 00:05:44,910
on the analogy to language when you talk about grammars for scenes in those grammars

91
00:05:44,910 --> 00:05:50,270
give you priors over structural descriptions are parts of an image and this approach seems

92
00:05:50,270 --> 00:05:54,540
to have a lot more promising these for representation for trying to describe how we

93
00:05:54,540 --> 00:05:58,540
can see three d structure from two d image of course it's also a lock-up

94
00:05:58,540 --> 00:06:02,520
computationally very difficult it's very hard to do efficient inference in some kind of a

95
00:06:02,520 --> 00:06:05,280
scene grammar like this so one of the things we're interested in our group and

96
00:06:05,280 --> 00:06:08,890
others are two is how do you combine these two kinds of approaches to try

97
00:06:08,890 --> 00:06:13,920
to get the representational power of this approach the efficient inference of these capacities of

98
00:06:13,920 --> 00:06:14,950
these bottom-up

99
00:06:15,200 --> 00:06:17,280
object class detectors for example

100
00:06:17,290 --> 00:06:24,240
so these are are all things like that

101
00:06:26,870 --> 00:06:31,500
for the

102
00:06:35,790 --> 00:06:38,320
you all in learning

103
00:06:39,330 --> 00:06:43,380
and that's the key question those these sources

104
00:06:43,400 --> 00:06:45,020
so here is

105
00:06:45,040 --> 00:06:46,770
right so what's

106
00:06:47,340 --> 00:06:50,930
right so so i would to talk more about this later on in the in

107
00:06:50,930 --> 00:06:55,730
the summer school but there's many tradeoffs the world that we're all familiar with the

108
00:06:55,740 --> 00:07:00,160
the more structured sophisticated your brain richer representations are the harder it is to do

109
00:07:00,160 --> 00:07:03,540
efficient inference but also the harder it is to have two to learn everything i

110
00:07:03,540 --> 00:07:06,340
have any idea what is the right hypothesis space for learning that could be effectively

111
00:07:06,340 --> 00:07:11,520
searched and to learn this kind of knowledge and certainly some some ideas which have

112
00:07:11,520 --> 00:07:14,160
evolved over the last few decades

113
00:07:14,180 --> 00:07:17,180
our appreciation for well

114
00:07:17,240 --> 00:07:21,960
what kinds of problems that seem intractable might be solved if you just wait long

115
00:07:21,960 --> 00:07:25,070
enough either way long enough for your computer to cycle long enough for those clever

116
00:07:25,070 --> 00:07:26,390
guys int'l two

117
00:07:26,500 --> 00:07:31,580
to build a yet another order of magnitude faster computing power or for those clever

118
00:07:31,820 --> 00:07:37,520
people into beginning and sheffield place to come up with clever mathematical formulations and i

119
00:07:37,520 --> 00:07:41,400
think there are there's there's a lot of progress has been made to the point

120
00:07:41,400 --> 00:07:44,300
that on both in terms of mass and

121
00:07:44,320 --> 00:07:48,760
map of optimisation the and the the sort of hardware and software of

122
00:07:48,770 --> 00:07:50,120
of computing power

123
00:07:50,140 --> 00:07:54,860
that make problems that would seem completely ridiculous you know a couple of decades ago

124
00:07:55,650 --> 00:07:58,690
standard and other ones which would seem so

125
00:07:58,760 --> 00:08:02,250
ridiculous that you would never even have talked about them for fear being laughed off

126
00:08:02,250 --> 00:08:05,310
the stage now this i can talk about them on the stage and i you

127
00:08:05,310 --> 00:08:08,510
know i can't point to something which works and says oh yeah we can do

128
00:08:08,510 --> 00:08:09,540
that efficiently

129
00:08:09,560 --> 00:08:13,110
but it's this is a legitimate goal to work towards

130
00:08:13,130 --> 00:08:14,530
and i do think that

131
00:08:14,540 --> 00:08:19,910
you know when you actually get into it you can have legitimate debates in good

132
00:08:19,910 --> 00:08:23,180
debates about what is likely to be learned or innate

133
00:08:23,190 --> 00:08:28,130
what kind of how power how rich or unstructured representations are likely to be based

134
00:08:28,130 --> 00:08:31,570
on thinking about what's computationally possible but just as

135
00:08:31,580 --> 00:08:35,690
we don't really know very much about what's neurally plausible we also don't really know

136
00:08:35,750 --> 00:08:37,540
what what are the kind of

137
00:08:37,590 --> 00:08:40,810
i mean what's neurally plausible say in terms of representation we don't know what's neurally

138
00:08:40,810 --> 00:08:44,640
plausible in terms of the limits of computation exactly and we want to say well

139
00:08:44,640 --> 00:08:47,760
this is how the brain works

140
00:08:47,780 --> 00:08:51,280
it's hard to it's hard to say well this is the conversation which which seems

141
00:08:51,280 --> 00:08:55,190
intractable based on our current singing computer science and that's probably not likely to be

142
00:08:55,190 --> 00:08:57,360
done in the brain

143
00:08:57,370 --> 00:09:00,580
as far as we know the brain and i i tend to believe not only

144
00:09:00,590 --> 00:09:06,040
the brain represents rich structures that is far more powerful computationally than we usually invoke

145
00:09:06,040 --> 00:09:11,310
as a constraint in practical machine learning on what's algorithm willing to live

146
00:09:11,320 --> 00:09:16,540
now these sets of these ideas have been i think fairly well well one not

147
00:09:16,540 --> 00:09:20,370
fairly well worked out but at least there are being the starting to be explored

148
00:09:20,370 --> 00:09:24,700
by number of groups in language and vision and in some of our work we

149
00:09:24,700 --> 00:09:27,770
emphasise more kind of higher-level cognitive problems

150
00:09:27,780 --> 00:09:31,400
but we're also interested in the vision ones like for example word learning so i

151
00:09:31,410 --> 00:09:34,490
mentioned before and will show a little bit of this later on the class how

152
00:09:34,490 --> 00:09:37,580
can you think about say learning the meaning of the word from a few examples

153
00:09:37,580 --> 00:09:41,230
as also inference in this kind of a hierarchical bayesian model with various kinds of

154
00:09:41,230 --> 00:09:46,060
structured representations or causal learning so again that's the problem which has a long history

155
00:09:46,060 --> 00:09:50,890
in machine learning whether it's learning just associations are now learning the structure of graphical

156
00:09:50,890 --> 00:09:54,300
models and we'll see how that can be described as a kind of bayesian inference

157
00:09:54,300 --> 00:09:58,090
and also have more abstract calls knowledge the kind that allows a person to make

158
00:09:58,320 --> 00:10:02,740
a causal inference from what seems like way too limited data can also be described

159
00:10:02,740 --> 00:10:06,420
as something like a hierarchical bayesian computation

160
00:10:07,020 --> 00:10:11,190
and lastly i will have too much time for this in this class with something

161
00:10:11,190 --> 00:10:15,210
that we're very interested in them happy to talk about it with people here and

162
00:10:15,210 --> 00:10:19,280
actually some of the students from MIT who who are attending the school

163
00:10:19,370 --> 00:10:22,710
i have been quite interested in even worked on models of the sort of thing

164
00:10:23,080 --> 00:10:24,540
so going to say

165
00:10:24,540 --> 00:10:30,100
and so that's how you compute easy i transpose easy as you transpose and substituting

166
00:10:30,100 --> 00:10:31,990
back into the formula

167
00:10:32,010 --> 00:10:35,120
and you would then have your m step update

168
00:10:35,130 --> 00:10:38,280
to the printer matrix longer

169
00:10:39,730 --> 00:10:43,930
and also one the point on this derivation is that it turns out it's partly

170
00:10:43,930 --> 00:10:50,220
because the name ian our expectation maximisation of one common mistake for the amount of

171
00:10:50,500 --> 00:10:51,220
it is

172
00:10:51,270 --> 00:10:54,150
in the e step on someone to

173
00:10:54,160 --> 00:10:57,190
take expectation of the random variables e

174
00:10:57,200 --> 00:11:01,030
and then the m step just the expected value everywhere you see it

175
00:11:01,110 --> 00:11:06,180
so in particular one common mistake in deriving the factor analysis is to look at

176
00:11:06,180 --> 00:11:09,380
this and say look as easy transfers

177
00:11:09,490 --> 00:11:20,820
that's just for the expected value under the q distribution and you approach that

178
00:11:20,840 --> 00:11:24,650
new qx i times musee arguments i transposed

179
00:11:24,700 --> 00:11:31,110
into that expectation and this is this is incorrect there and been incorrect derivation of

180
00:11:31,860 --> 00:11:33,160
because some

181
00:11:33,170 --> 00:11:36,340
it's missing this all term frequency i think

182
00:11:36,390 --> 00:11:40,980
so so one common misconception VEM is that in the east and just compute the

183
00:11:40,980 --> 00:11:45,910
expected value of a random variable and the m step the expected value and it

184
00:11:45,910 --> 00:11:49,090
turns out some our own that turns out to be the right thing to do

185
00:11:49,090 --> 00:11:52,960
in the mixture of gaussians and mixtures naive bayes models that will actually be the

186
00:11:52,960 --> 00:11:53,990
right answer

187
00:11:54,050 --> 00:11:58,320
but in general the album is more complicated than just taking the expected values of

188
00:11:58,320 --> 00:12:03,220
the random variables pretending that they were so observed that the expected value

189
00:12:03,230 --> 00:12:07,860
OK so i'll go for this just illustrate that step as well

190
00:12:09,160 --> 00:12:11,430
so just to summarise the we

191
00:12:11,630 --> 00:12:14,550
the three key thing to keep in mind that that they came up with this

192
00:12:14,560 --> 00:12:19,870
derivation were one for the east step we had a continuous scouting random variables so

193
00:12:19,880 --> 00:12:21,430
compute e step

194
00:12:21,440 --> 00:12:25,810
we actually compute the mean and covariance of the council of the distribution q i

195
00:12:26,660 --> 00:12:30,200
the second thing that came up was on the m step

196
00:12:30,210 --> 00:12:32,140
when you see these integrals

197
00:12:32,290 --> 00:12:36,770
sometimes if expect the interpret as the expectation the rest of the map becomes much

198
00:12:37,800 --> 00:12:39,980
and the final thing was

199
00:12:39,990 --> 00:12:42,270
again in the m step on

200
00:12:42,280 --> 00:12:42,940
you know

201
00:12:42,970 --> 00:12:47,850
the album is derived by a certain maximisation problem that we thought is not necessarily

202
00:12:47,850 --> 00:12:51,550
just the expected value of xei everywhere

203
00:12:54,590 --> 00:12:55,430
the c

204
00:12:56,190 --> 00:13:01,130
that is the kind of natural way too many equations on and and and even

205
00:13:01,130 --> 00:13:04,820
doing this i was skipping many steps single selection analysis c

206
00:13:04,870 --> 00:13:09,110
you know the all the that's all the derivations of the stethoscope like how you

207
00:13:09,110 --> 00:13:13,700
actually take care to respect matrix slander on how to compute the updates to the

208
00:13:13,700 --> 00:13:18,420
other parameters as well the new emphasizes

209
00:13:20,460 --> 00:13:22,200
and so

210
00:13:22,210 --> 00:13:24,350
that's the factor analysis out

211
00:13:28,650 --> 00:13:31,650
in the step right or

212
00:13:33,110 --> 00:13:39,690
the second is parameters in the first ten years of his life

213
00:13:39,740 --> 00:13:43,590
but it seems to me that q is as low

214
00:13:43,610 --> 00:13:44,860
as well

215
00:13:45,130 --> 00:13:46,940
this is the right

216
00:13:46,960 --> 00:13:50,400
this is questions raised to you

217
00:13:50,440 --> 00:13:55,700
it doesn't q i have parameters and so on

218
00:13:55,720 --> 00:13:59,760
so in the end however on QI is

219
00:13:59,800 --> 00:14:05,280
that is is there any and all of them sometimes PSV i may have parameters

220
00:14:05,340 --> 00:14:09,590
the URI of the i may never have any parameters on

221
00:14:09,590 --> 00:14:14,840
in this was a case of factor analysis your eye doesn't parameters on in other

222
00:14:14,840 --> 00:14:17,360
examples and mixture of gaussians model say

223
00:14:17,380 --> 00:14:22,350
xiao was the multinomial random variable is an example of the i is you to

224
00:14:23,610 --> 00:14:26,490
he was you i will never have any parameters

225
00:14:26,530 --> 00:14:30,900
and in particular two i i was going to be

226
00:14:30,900 --> 00:14:38,380
this is the scouts in distribution right is no doubt will be given by me

227
00:14:40,670 --> 00:14:46,280
covariance you know singer

228
00:14:46,470 --> 00:14:48,320
and so

229
00:14:49,360 --> 00:14:56,010
it's true that mu and sigma made themselves have depended on the values of the

230
00:14:56,010 --> 00:14:59,970
parameters i had in the previous generation

231
00:15:00,010 --> 00:15:03,970
but on the way to think about you is going to take the parameters from

232
00:15:03,970 --> 00:15:06,380
the previous iteration of the algorithm

233
00:15:06,400 --> 00:15:10,880
and use that compute what QI is your i

234
00:15:10,960 --> 00:15:14,170
that's that's that's the e seventy algorithm

235
00:15:14,420 --> 00:15:20,050
and i myself computer what you lives years then this is a fixed distribution and

236
00:15:20,090 --> 00:15:23,550
use these things values for mu and sigma

237
00:15:23,550 --> 00:15:26,320
me deal with costs ten dollars to come from u to you

238
00:15:26,370 --> 00:15:30,640
and these are the nodes and this is the network and now the minimum spanning

239
00:15:31,270 --> 00:15:34,150
is the theory that connects all the nodes

240
00:15:34,150 --> 00:15:39,550
but we start minimum weight the total number of is and you you see in

241
00:15:39,550 --> 00:15:43,680
this one and this is very easy to implement if you the network in order

242
00:15:43,690 --> 00:15:47,910
to find the minimum spanning tree the algorithm is very trivial you start to remove

243
00:15:47,910 --> 00:15:53,250
the highest aggregate cost of the link for example here and from that is to

244
00:15:53,260 --> 00:15:58,540
go to learn one layer one o one bits condition if you remove node you

245
00:15:58,540 --> 00:16:00,440
don't remove this link i mean if you

246
00:16:00,450 --> 00:16:03,120
remove the in the middle weeks

247
00:16:03,150 --> 00:16:06,620
you don't remove have them can but you continue in this way you finally get

248
00:16:06,620 --> 00:16:06,980
the three

249
00:16:07,400 --> 00:16:10,400
which is called the minimum standards of the algorithm is very simple to get going

250
00:16:10,400 --> 00:16:13,590
on but in this case there is the minimum tree

251
00:16:13,600 --> 00:16:18,250
the is when it is it's a part of the network that is very important

252
00:16:18,250 --> 00:16:20,790
for traffic for transportation

253
00:16:20,810 --> 00:16:25,170
because of the goes and the minimum of the place for minimum course and this

254
00:16:25,170 --> 00:16:30,260
is very widely used to optimize traffic flow design operation of communication network

255
00:16:30,310 --> 00:16:31,580
so what do we know

256
00:16:31,600 --> 00:16:35,950
for learning about that on this minimum minimum spanning tree

257
00:16:35,970 --> 00:16:39,870
so first of all we know before they told you if you don't

258
00:16:39,910 --> 00:16:44,050
eight the minimum distance without considering the weight

259
00:16:44,050 --> 00:16:47,420
the minimum distance grows like log again if it's get free

260
00:16:47,440 --> 00:16:51,150
if it's not scale free if it goes like looking

261
00:16:51,170 --> 00:16:52,630
this is when you

262
00:16:52,630 --> 00:16:56,590
i don't consider the now the question what has been you can see the debate

263
00:16:56,630 --> 00:16:59,590
so one can solve using missiles form

264
00:16:59,610 --> 00:17:03,240
statistical physics julie and find it you

265
00:17:03,260 --> 00:17:06,890
use the weight and you want to find the optimal way you can see the

266
00:17:06,890 --> 00:17:07,730
optimal bay

267
00:17:07,830 --> 00:17:11,040
becomes much longer instead of going from a to b

268
00:17:11,040 --> 00:17:15,400
to go to the links here and it cost fifty eight you go for me

269
00:17:15,500 --> 00:17:19,920
to you and will cost much less because twenty nine only you see the sum

270
00:17:19,920 --> 00:17:22,130
of these numbers and this

271
00:17:22,140 --> 00:17:25,950
the past the minimum spanning trees that have been global and to become longer than

272
00:17:26,040 --> 00:17:30,090
the land they begin to see but still they have relatively short it there will

273
00:17:30,100 --> 00:17:31,180
flow again

274
00:17:31,210 --> 00:17:35,580
however if you take lambda greater than three unable to show any and you make

275
00:17:35,610 --> 00:17:39,610
the same analysis to minimize vanity and you find the links of the past that

276
00:17:39,610 --> 00:17:43,580
you have to go on the minimum spanning tree which is important for for dynamics

277
00:17:43,590 --> 00:17:44,580
of population

278
00:17:44,970 --> 00:17:49,670
this becomes very large into to the one so instead of login so this slogan

279
00:17:49,670 --> 00:17:54,740
platforms to once into the ones and possible to look at this you see also

280
00:17:54,750 --> 00:18:00,400
be useful the importance was given the other important because they still can still small

281
00:18:00,400 --> 00:18:01,660
world even if you

282
00:18:01,670 --> 00:18:05,700
this all the time and you have a and you want to find a the

283
00:18:05,720 --> 00:18:08,590
shortest path to the optimal path to find these

284
00:18:10,070 --> 00:18:13,840
this is important but i want to use this

285
00:18:14,770 --> 00:18:17,600
one can use it in the following the first of all let me show you

286
00:18:17,620 --> 00:18:22,380
are would be taken big network NBC of the minimum spanning tree looks this is

287
00:18:22,380 --> 00:18:23,870
the minimum spanning tree

288
00:18:23,870 --> 00:18:28,660
and a because it is the minimum spanning tree inventory includes ultimately reaching everywhere in

289
00:18:28,700 --> 00:18:29,690
the network

290
00:18:29,720 --> 00:18:34,220
but if you if you have two in if you want to improve now the

291
00:18:34,230 --> 00:18:39,400
network is a big the we have to use to import all the links you

292
00:18:39,400 --> 00:18:42,050
but to be this is the order of n

293
00:18:42,070 --> 00:18:44,040
again very big systems

294
00:18:44,050 --> 00:18:48,830
we don't want to spend so much money in improving the system is not definitive

295
00:18:48,830 --> 00:18:51,830
because we know something is you remember connects all the nodes

296
00:18:51,840 --> 00:18:53,680
so what we do we use

297
00:18:53,710 --> 00:18:55,590
concepts form percolation

298
00:18:55,600 --> 00:19:00,170
and we instead of looking good manners when he would look on a subset of

299
00:19:00,170 --> 00:19:04,300
them in spanning tree which we call which is called the infinite percolating cluster this

300
00:19:04,300 --> 00:19:10,120
is xk subset of this cluster together minimum they infinite according cluster is also very

301
00:19:10,120 --> 00:19:13,270
simple but now we know that in general and for

302
00:19:13,300 --> 00:19:16,810
a kind of this form of systems c

303
00:19:17,240 --> 00:19:19,200
it is one of the key zero

304
00:19:19,220 --> 00:19:23,170
minus one because is the case the KL member carries the degree

305
00:19:23,180 --> 00:19:25,360
and this is the average of the

306
00:19:25,360 --> 00:19:26,630
because of the degree

307
00:19:26,630 --> 00:19:28,900
you don't get

308
00:19:29,040 --> 00:19:33,790
so we have to understand that they have to understand all of the social factors

309
00:19:33,790 --> 00:19:37,110
play into predicting where we're going

310
00:19:39,720 --> 00:19:44,770
these change the interesting demonstration against the

311
00:19:44,790 --> 00:19:47,240
seven years ago

312
00:19:47,270 --> 00:19:49,160
i would not have expected

313
00:19:49,170 --> 00:19:52,440
there would be no money coming from the business world

314
00:19:53,920 --> 00:19:56,300
small amounts of memory

315
00:19:56,330 --> 00:20:01,870
these are the people who set a bit slow with another four gigs

316
00:20:01,900 --> 00:20:06,100
but it turns out all sorts of different things to play this

317
00:20:06,170 --> 00:20:11,350
firstly we have lot now shipping linux on embedded devices

318
00:20:11,350 --> 00:20:15,040
a few years ago i was calling common

319
00:20:15,060 --> 00:20:18,370
all of these embedded devices the memory constraint

320
00:20:18,380 --> 00:20:22,980
these people are you still think the all megabytes is a lot of that

321
00:20:23,050 --> 00:20:26,360
even more so for what's on the flash

322
00:20:26,370 --> 00:20:31,290
so you coming from distributions which if you install everything

323
00:20:31,290 --> 00:20:34,410
these they take two of three d points hard the

324
00:20:34,460 --> 00:20:38,460
well you never the point that it's not just the methods of definition of the

325
00:20:38,460 --> 00:20:43,970
door so usage you could never have to use all of the trials

326
00:20:44,050 --> 00:20:45,470
is systems where

327
00:20:45,480 --> 00:20:49,310
the test we can begin to fall megabyte flash

328
00:20:49,340 --> 00:20:53,470
because there again the following words like the flash

329
00:20:53,480 --> 00:20:57,060
might be the difference between the product being practical or profitable

330
00:20:57,720 --> 00:21:03,420
so we have a better people are desperately trying to become small time which is

331
00:21:04,720 --> 00:21:08,110
because that helps everybody else

332
00:21:08,170 --> 00:21:13,420
the second reason people and now wanting small which is virtualization

333
00:21:13,420 --> 00:21:17,290
because the historical model the desktop is

334
00:21:21,580 --> 00:21:26,900
web servers getting a bit like will buy another computer

335
00:21:26,910 --> 00:21:31,540
with virtualization the attitude is it's no longer

336
00:21:31,550 --> 00:21:32,420
o my

337
00:21:32,430 --> 00:21:37,050
and in any we'll use the only well-known authors use

338
00:21:37,280 --> 00:21:42,420
CPA UIUC i did well to process who cares

339
00:21:42,430 --> 00:21:49,050
basically every time something one virtual machine is wasting resources of every other virtual machine

340
00:21:49,110 --> 00:21:52,650
that's very visible of things like three ninety nine

341
00:21:53,280 --> 00:21:56,290
that is if you know the results

342
00:21:56,290 --> 00:21:59,560
very fast world another

343
00:21:59,590 --> 00:22:06,210
so the graph starting to realize that wasting the CPU is a bad idea

344
00:22:07,170 --> 00:22:11,980
another big factor how would being driven by miles of better people

345
00:22:12,040 --> 00:22:14,730
businesses while the latter

346
00:22:17,380 --> 00:22:23,720
lot it's actually really really annoying when your battery goes flat trying to do something

347
00:22:23,730 --> 00:22:27,230
so everybody was batteries last forever

348
00:22:27,280 --> 00:22:34,090
how is a big factor for example the people trying to do really really big

349
00:22:34,110 --> 00:22:35,850
data centers

350
00:22:35,850 --> 00:22:38,460
o really worried about power

351
00:22:38,470 --> 00:22:43,430
you know the the scale of one large company we switched over some of the

352
00:22:43,430 --> 00:22:46,390
data set to more efficient PC hardware

353
00:22:46,410 --> 00:22:52,230
start using virtualization power-saving is over a megawatt

354
00:22:52,240 --> 00:22:56,300
the same in which they almost imagine how

355
00:22:56,310 --> 00:23:02,700
the final thing plays into this is that the energy is getting more and more

356
00:23:04,140 --> 00:23:05,820
got some more and more key

357
00:23:05,830 --> 00:23:08,300
o things like carbon credits

358
00:23:08,320 --> 00:23:14,760
the green always of relation

359
00:23:14,780 --> 00:23:18,050
real time is another key part of our common future

360
00:23:18,060 --> 00:23:20,630
large numbers of people in real time

361
00:23:21,940 --> 00:23:24,060
used to be the for all

362
00:23:24,070 --> 00:23:26,190
the typical its user base

363
00:23:26,200 --> 00:23:27,850
interest in real time

364
00:23:29,000 --> 00:23:33,070
does my MP three player skin

365
00:23:33,750 --> 00:23:37,950
there are small number of people using it in a better world the majority of

366
00:23:37,950 --> 00:23:39,590
the users was

367
00:23:39,600 --> 00:23:44,730
i don't know maybe MP three to get one the one DVD smoothly one the

368
00:23:44,730 --> 00:23:46,430
windows around

369
00:23:46,440 --> 00:23:51,770
i don't think that receiving emails soundscape always felt this

370
00:23:53,020 --> 00:23:57,800
today i believe that uses there are people asking questions like can i mean

371
00:23:58,060 --> 00:24:01,020
timing requirements for mobile phones

372
00:24:01,030 --> 00:24:02,810
people building

373
00:24:02,830 --> 00:24:07,090
like piece machinery by someone who are interesting questions like that

374
00:24:07,110 --> 00:24:10,370
if my machine guns that control will it stop before

375
00:24:10,380 --> 00:24:14,070
after it goes through the wall

376
00:24:14,080 --> 00:24:16,610
it turns out the big banks

377
00:24:16,620 --> 00:24:21,410
are interested in this because one of the problems we have a moment

378
00:24:21,440 --> 00:24:24,630
is that if you run several jobs on learning machine

379
00:24:24,690 --> 00:24:27,370
you repeat few times

380
00:24:27,390 --> 00:24:30,650
they don't tend to all finnish time

381
00:24:30,760 --> 00:24:32,640
very efficient probably

382
00:24:32,650 --> 00:24:35,230
do the job better

383
00:24:35,240 --> 00:24:37,230
public can be quite there

384
00:24:37,230 --> 00:24:39,370
so depending on how this

385
00:24:40,480 --> 00:24:42,260
scheduling gets orders

386
00:24:42,330 --> 00:24:46,570
pending on who happens to win the argument that fits in memory

387
00:24:46,590 --> 00:24:51,390
you might start three jobs of the definitions same first step

388
00:24:51,400 --> 00:24:54,700
one of the finishes way before the second so

389
00:24:54,710 --> 00:24:59,000
and this caused problems some application

390
00:24:59,060 --> 00:25:02,400
because there are lots of of solving applications

391
00:25:02,430 --> 00:25:04,270
where we want to do

392
00:25:04,340 --> 00:25:05,500
is right

393
00:25:05,510 --> 00:25:07,370
a set of jobs

394
00:25:07,370 --> 00:25:13,110
so therefore if you then go and compute well PH domain

395
00:25:14,040 --> 00:25:19,060
y given x theta so basically trying to hidden markov models discriminatively

396
00:25:19,080 --> 00:25:22,520
you get this serious part of it

397
00:25:22,580 --> 00:25:26,290
well what's the upshot you can take it mark model implementation

398
00:25:26,310 --> 00:25:32,270
he trained discriminatively you get this year you don't need to record everything from scratch

399
00:25:33,150 --> 00:25:38,380
now to see whether we can make things a little bit more fancy

400
00:25:38,400 --> 00:25:41,600
so far we've been looking at the change

401
00:25:41,600 --> 00:25:44,020
basically where one variable

402
00:25:44,040 --> 00:25:49,270
just depends on its neighbour which then independent on its neighbour and so on

403
00:25:49,290 --> 00:25:51,870
now what i did wrong here is a tree

404
00:25:51,880 --> 00:25:54,210
i mean you can pick it up in any

405
00:25:55,420 --> 00:25:56,730
and hang out of there

406
00:25:57,040 --> 00:26:00,080
it will be actually OK

407
00:26:00,480 --> 00:26:04,790
now the question is can i repeat the same reasoning that they have so far

408
00:26:04,790 --> 00:26:08,480
if my graphical models the tree

409
00:26:08,540 --> 00:26:10,630
well the maximal cliques in the tree

410
00:26:10,690 --> 00:26:11,940
i just the neighbours

411
00:26:14,630 --> 00:26:21,020
we have seen with sort of junction trees before

412
00:26:22,630 --> 00:26:26,560
never heard of junction trees before

413
00:26:26,630 --> 00:26:29,420
who could explain to me what the junction trees

414
00:26:29,440 --> 00:26:33,830
you can

415
00:26:34,940 --> 00:26:42,100
OK so i will go through them reasonably smoothly

416
00:26:45,370 --> 00:26:48,250
most of the time

417
00:26:48,310 --> 00:26:56,770
so basically the idea

418
00:26:56,790 --> 00:26:58,060
that allowed us

419
00:26:58,080 --> 00:26:59,480
to treat

420
00:26:59,500 --> 00:27:00,460
a chain

421
00:27:00,480 --> 00:27:03,150
efficiently what to say well

422
00:27:03,150 --> 00:27:04,790
every random variable

423
00:27:04,790 --> 00:27:08,710
only influences the next one and therefore if i have to some or all the

424
00:27:08,710 --> 00:27:11,900
random variables i can do this in

425
00:27:11,920 --> 00:27:13,250
recursive manner

426
00:27:13,270 --> 00:27:15,940
but just peeling off from the end

427
00:27:15,960 --> 00:27:18,480
summing up everything had been

428
00:27:18,670 --> 00:27:20,080
doing the same

429
00:27:20,100 --> 00:27:22,690
i propagation backwards again

430
00:27:22,710 --> 00:27:27,540
so that gives gamers the forward backward algorithm

431
00:27:29,480 --> 00:27:31,020
if i have

432
00:27:31,060 --> 00:27:34,560
this model here

433
00:27:34,580 --> 00:27:36,420
and i have some function in

434
00:27:36,420 --> 00:27:41,980
which couples these terms these two terms and these terms

435
00:27:42,000 --> 00:27:46,580
then well if i had managed to sum over all these

436
00:27:46,630 --> 00:27:50,600
all the values here already

437
00:27:50,650 --> 00:27:52,870
and i only get an expression with this

438
00:27:52,880 --> 00:27:54,310
is some function

439
00:27:54,330 --> 00:27:57,460
of whatever value this right no takes

440
00:27:57,520 --> 00:28:00,230
and do the same thing here

441
00:28:00,250 --> 00:28:05,150
and i can eliminate this notice well from my reasoning by just taking the product

442
00:28:05,150 --> 00:28:10,960
whatever comes in here whatever comes in from here times the potential function in there

443
00:28:10,980 --> 00:28:15,650
and then seen this on to the next note

444
00:28:15,730 --> 00:28:20,940
this would then allow me to perform the summation over all those variables here

445
00:28:20,940 --> 00:28:24,650
first of all in nicely decentralised fashion

446
00:28:24,670 --> 00:28:29,420
because i can do this part independently from that part

447
00:28:29,440 --> 00:28:30,880
and i just

448
00:28:30,920 --> 00:28:31,630
need to

449
00:28:31,650 --> 00:28:35,870
seems rather simple messages along i don't actually need to know what's happening in here

450
00:28:36,330 --> 00:28:38,290
just to perform this operation

451
00:28:38,340 --> 00:28:42,880
all i need is just a message telling me well depending on

452
00:28:42,920 --> 00:28:44,830
the right value

453
00:28:45,790 --> 00:28:48,270
how this message in here will look like

454
00:28:48,290 --> 00:28:52,250
so if this can only take two different values only need to send into numbers

455
00:28:52,250 --> 00:28:53,290
from here

456
00:28:53,350 --> 00:28:58,440
eight received two numbers from there regardless of how big this system is

457
00:28:58,460 --> 00:29:00,330
and the former summation here

458
00:29:00,350 --> 00:29:03,730
or something that couples this with

459
00:29:05,170 --> 00:29:10,000
and pass on another two numbers

460
00:29:10,060 --> 00:29:11,480
then here

461
00:29:13,080 --> 00:29:14,920
i will have to pass on

462
00:29:15,230 --> 00:29:17,980
OK he gets a bit tricky

463
00:29:18,020 --> 00:29:22,850
because i can just say well i'm going to pass on the summation over these

464
00:29:22,850 --> 00:29:27,310
past whatever the green state is over here and over there without thinking very much

465
00:29:29,110 --> 00:29:30,650
what allowed us to do

466
00:29:30,670 --> 00:29:32,710
the operations in the right now at war

467
00:29:32,900 --> 00:29:36,960
but i had received all the messages but one

468
00:29:36,980 --> 00:29:42,150
miss out on that link where hadn't received the message here

469
00:29:42,170 --> 00:29:45,980
so how can i make progress at the green

470
00:29:46,750 --> 00:29:49,900
forest received rates message

471
00:29:49,920 --> 00:29:53,730
but i haven't and say i want to go down here

472
00:29:53,770 --> 00:29:57,850
well what i will need is we the message from up there

473
00:29:57,880 --> 00:30:02,480
so i do the same operations for police we know i get the message that

474
00:30:02,480 --> 00:30:04,650
pass it on

475
00:30:04,670 --> 00:30:05,810
here again

476
00:30:05,940 --> 00:30:08,130
well if i want to move up there

477
00:30:08,130 --> 00:30:13,670
need to get this message some adopt passes through

478
00:30:13,670 --> 00:30:17,080
so now this gives us an algorithm to very simple one

479
00:30:17,100 --> 00:30:19,420
it's called the message passing algorithm

480
00:30:19,480 --> 00:30:21,080
he goes as follows

481
00:30:21,170 --> 00:30:24,170
in order to send a message

482
00:30:24,170 --> 00:30:25,900
it was two pi

483
00:30:25,960 --> 00:30:28,110
divided by omega

484
00:30:28,150 --> 00:30:33,300
fortunately for me very shortly afterwards i raise this from the black poured so you

485
00:30:33,300 --> 00:30:39,570
won't see it for very long in any case this is what it should be

486
00:30:39,610 --> 00:30:40,800
and therefore

487
00:30:40,800 --> 00:30:44,320
i'm going to put in here

488
00:30:44,380 --> 00:30:48,940
zero crossings to guide myself so that i don't make the mistake

489
00:30:49,010 --> 00:30:51,010
would you often see

490
00:30:51,010 --> 00:30:57,230
people think that this really is slowly getting shorter in time that is not true

491
00:30:57,280 --> 00:31:00,420
it is the amplitude that decays away

492
00:31:00,440 --> 00:31:02,360
and so if we know

493
00:31:02,400 --> 00:31:05,740
try to

494
00:31:05,840 --> 00:31:07,190
put in

495
00:31:07,240 --> 00:31:08,880
he also nations

496
00:31:08,920 --> 00:31:11,380
and this is what's going to happen

497
00:31:11,380 --> 00:31:14,090
so you see that the oscillation dies away

498
00:31:14,130 --> 00:31:14,990
in time

499
00:31:14,990 --> 00:31:15,840
but the

500
00:31:15,840 --> 00:31:16,960
a period t

501
00:31:16,990 --> 00:31:18,240
is uniquely

502
00:31:19,340 --> 00:31:21,860
and that depends on how much

503
00:31:21,960 --> 00:31:26,920
friction there is

504
00:31:26,970 --> 00:31:31,530
it is not uncommon to introduce

505
00:31:31,570 --> 00:31:33,550
the quality factor

506
00:31:36,260 --> 00:31:38,050
that is high

507
00:31:38,110 --> 00:31:40,150
if there is little damping

508
00:31:40,190 --> 00:31:41,900
and that is low

509
00:31:41,940 --> 00:31:43,460
if there is

510
00:31:43,510 --> 00:31:45,490
a lot of them

511
00:31:45,510 --> 00:31:52,300
and that q which is dimensionless is omega zero divided by gamma

512
00:31:52,320 --> 00:31:55,030
you see immediately if gamma is high

513
00:31:55,030 --> 00:31:56,110
thank you is low

514
00:31:56,110 --> 00:32:00,130
low quality also later

515
00:32:00,190 --> 00:32:02,090
if you introduce that

516
00:32:02,130 --> 00:32:05,530
you can go back to your omega query equation

517
00:32:05,570 --> 00:32:09,300
and then omega becomes omega zero squared

518
00:32:09,300 --> 00:32:13,630
times one minus one divided by four q script

519
00:32:13,710 --> 00:32:16,010
is this any different from that

520
00:32:16,710 --> 00:32:21,610
so is a different way of writing it because you introduce this

521
00:32:21,670 --> 00:32:24,960
what it tells you is that if q

522
00:32:25,010 --> 00:32:27,780
is about ten

523
00:32:27,800 --> 00:32:31,690
and i bet you accuse are much higher for these pendulums

524
00:32:31,730 --> 00:32:35,350
then you have you one divided by four hundred it is one quarter of a

525
00:32:36,570 --> 00:32:40,210
but says omega is the square root of that

526
00:32:40,260 --> 00:32:43,970
it's only one it's over percent so for q of ten

527
00:32:44,030 --> 00:32:45,510
only guy

528
00:32:45,570 --> 00:32:51,010
there is only one it's percent lower than omega zero

529
00:32:51,090 --> 00:32:54,860
even if you make the clue as well as two

530
00:32:54,920 --> 00:33:00,880
the frequencies only off by about three point two percent

531
00:33:00,940 --> 00:33:03,090
so i wanted to appreciate that

532
00:33:03,150 --> 00:33:06,490
most of the time but not always

533
00:33:06,550 --> 00:33:15,960
is only got very close to omega zero

534
00:33:16,030 --> 00:33:19,730
we can look at the case

535
00:33:19,880 --> 00:33:22,820
in a different way

536
00:33:22,960 --> 00:33:27,380
we cannot do that here

537
00:33:27,380 --> 00:33:31,780
i'll do that he only sent aboard because we don't need that

538
00:33:31,820 --> 00:33:33,320
any more

539
00:33:33,340 --> 00:33:39,730
instead of saying i have to wait to overcome my seconds

540
00:33:39,740 --> 00:33:45,170
forty amplitude to go down by a factor of e

541
00:33:45,190 --> 00:33:46,920
i can ask myself

542
00:33:46,940 --> 00:33:51,170
how many oscillations do i have to wait

543
00:33:51,170 --> 00:33:53,470
forty amplitude to go down

544
00:33:55,920 --> 00:34:00,380
how many oscillations

545
00:34:01,590 --> 00:34:03,400
and also relations

546
00:34:03,420 --> 00:34:05,740
i will take this long

547
00:34:05,780 --> 00:34:07,380
to being the period

548
00:34:07,400 --> 00:34:08,900
of one oscillation

549
00:34:08,900 --> 00:34:12,300
but for reasonable values of q

550
00:34:12,340 --> 00:34:17,460
TNT zero and the same like only going on because there are closely the same

551
00:34:17,470 --> 00:34:19,170
so i can write for this

552
00:34:19,170 --> 00:34:22,760
this is approximately and times to zero

553
00:34:22,780 --> 00:34:25,800
that's approximately and times two buy

554
00:34:25,820 --> 00:34:28,670
divided by omega zero

555
00:34:28,860 --> 00:34:36,190
i can now substitute this time in this team

556
00:34:36,210 --> 00:34:40,740
and only concentrate on the decay portion that early part

557
00:34:40,800 --> 00:34:43,380
what i find then that a

558
00:34:43,400 --> 00:34:45,380
after an oscillations

559
00:34:45,400 --> 00:34:50,900
is a times e to the minus come over to t

560
00:34:51,010 --> 00:34:56,460
to the miners come over two times this time after and also nations and

561
00:34:56,530 --> 00:34:58,300
divided by two pi

562
00:34:58,320 --> 00:35:01,920
divided by omega zero

563
00:35:01,960 --> 00:35:04,840
you lose two

564
00:35:04,900 --> 00:35:08,280
but only always zero divided by dumais q

565
00:35:08,280 --> 00:35:09,670
so now

566
00:35:09,710 --> 00:35:11,230
you have an informed

567
00:35:11,230 --> 00:35:12,650
which is minus

568
00:35:13,570 --> 00:35:14,710
times by

569
00:35:14,740 --> 00:35:16,340
over q

570
00:35:16,420 --> 00:35:20,420
is this any different from what we had before

571
00:35:22,840 --> 00:35:24,420
here we say

572
00:35:24,490 --> 00:35:25,780
i have to wait

573
00:35:25,800 --> 00:35:30,530
to overcome my second forty amplitude to go down by a factor of e

574
00:35:30,530 --> 00:35:31,940
we say

575
00:35:31,990 --> 00:35:35,900
if n is killed divided by pi

576
00:35:35,940 --> 00:35:38,190
then the amplitude goes down

577
00:35:38,210 --> 00:35:41,840
by one of each so in one case i ask myself

578
00:35:41,860 --> 00:35:44,010
how many seconds do i have to wait

579
00:35:44,030 --> 00:35:48,710
in your other case i ask myself how many oscillations do i have to wait

580
00:35:48,760 --> 00:35:50,170
and so if q

581
00:35:50,240 --> 00:35:51,840
is ten

582
00:35:51,940 --> 00:35:56,590
it tells you that you have to wait about three oscillations roughly

583
00:35:56,610 --> 00:35:58,730
forty amplitude to go down

584
00:35:58,780 --> 00:36:02,970
by a factor of e

585
00:36:02,990 --> 00:36:04,710
and if q is one hundred

586
00:36:04,710 --> 00:36:06,780
you have to wait more like thirty two

587
00:36:08,090 --> 00:36:09,860
forty amplitude to go down

588
00:36:09,920 --> 00:36:13,210
by a factor of e

589
00:36:13,230 --> 00:36:14,570
eighteen minutes

590
00:36:16,150 --> 00:36:18,550
now you can start taking notes again

591
00:36:20,690 --> 00:36:22,090
i have here

592
00:36:24,610 --> 00:36:28,010
and the pendulums have about the same length

593
00:36:28,010 --> 00:36:29,670
and the objects

594
00:36:29,670 --> 00:36:33,920
i have about the same radius

595
00:36:33,960 --> 00:36:35,280
that means the

596
00:36:35,320 --> 00:36:37,590
the b

597
00:36:37,610 --> 00:36:41,710
which is this coefficient

598
00:36:41,760 --> 00:36:43,630
in front of the

599
00:36:43,690 --> 00:36:48,590
velocity is about the same

600
00:36:48,690 --> 00:36:50,130
the gunman

601
00:36:50,150 --> 00:36:53,010
it's not the same because if they have the same b

602
00:36:53,050 --> 00:36:54,170
but if the mass

603
00:36:54,190 --> 00:36:58,050
of the two object is very different this is styrofoam

604
00:36:58,070 --> 00:36:59,610
and this is unbelievable

605
00:36:59,610 --> 00:37:02,300
there's a huge difference in government

606
00:37:02,320 --> 00:37:05,760
and since the period of the two pendulums is very closely the same they have

607
00:37:05,780 --> 00:37:07,280
the same length

608
00:37:07,280 --> 00:37:13,280
now is how can we specify a joint distribution over phi and theta because that if

609
00:37:13,280 --> 00:37:18,640
we can specify this joint distribution then that's basically tells us we how we can

610
00:37:18,640 --> 00:37:23,940
construct a G that's a draw from our Dirichlet process and there're kind of two ways

611
00:37:23,940 --> 00:37:30,160
of doing this one  is called stick breaking construction and one's called the Poisson Dirichlet distribution the

612
00:37:30,160 --> 00:37:35,800
stick breaking construction is yet another metaphor for the Dirichlet process and it goes

613
00:37:35,800 --> 00:37:42,220
as follows so we have a stick of length one okay so that's black line here

614
00:37:42,220 --> 00:37:49,800
and we're gonna iteratively take the stick and break it into two parts so in the

615
00:37:49,800 --> 00:37:53,200
first step we're gonna take this stick and we're gonna it break into the blue part

616
00:37:53,200 --> 00:37:57,900
and the red part okay and the red part here the length we're gonna call phi

617
00:37:57,900 --> 00:38:06,000
star one we're gonna take the blue part and we're gonna take this

618
00:38:06,300 --> 00:38:10,650
shortened stick and break into to halves again that and that part is gonna

619
00:38:10,650 --> 00:38:15,480
call be called phi star two and then we're gonna be we're gonna take

620
00:38:15,480 --> 00:38:20,900
this break off a piece phi star three phi star four and so forth and

621
00:38:20,900 --> 00:38:24,820
because we started off with a stick of lenght  1 the total sum of the

622
00:38:24,820 --> 00:38:29,180
phis has to be one and of course the the length of the sticks cannot

623
00:38:29,180 --> 00:38:36,860
be negative so each of the phis are positive so the stick breaking construction is

624
00:38:36,870 --> 00:38:42,160
given by this this iterative form here where at each point the breakpoint is gonna

625
00:38:42,160 --> 00:38:49,200
be distributed according to a beta distribution with a parameter of one and alpha and that gives

626
00:38:49,200 --> 00:38:54,660
us our sequence of phis for our sequence of theta we're simply gonna draw each

627
00:38:54,660 --> 00:39:00,820
of them IID from our H distribution okay so that the thetas are gonna be drawn IID

628
00:39:00,820 --> 00:39:08,260
from H the phis are gonna constructed gonna be constructed according to this stick breaking construction

629
00:39:08,260 --> 00:39:14,800
and if we construct G as this infinite sum of the of of this atoms

630
00:39:14,800 --> 00:39:20,180
then we can show that this G which we should be a random probability measure

631
00:39:20,180 --> 00:39:27,860
will be drawn from the Dirichlet process okay notice here that the phis are actually decreasing

632
00:39:27,860 --> 00:39:32,020
on average but not strictly decreasing so on average phi one will be larger than

633
00:39:32,020 --> 00:39:35,860
phi two  which will be larger than phi three and so forth but is not necessary the case

634
00:39:35,860 --> 00:39:42,580
that any particular instantiation phi one will be larger than phi two okay it's also

635
00:39:42,580 --> 00:39:49,480
sometimes called the GEM distribution the Griffiths-Engen-McCloskey the other thing

636
00:39:49,650 --> 00:39:54,660
which are described here previously the Poisson Dirichlet distribution this gives strictly

637
00:39:54,660 --> 00:40:01,740
decreasing ordering of our sticks okay but unfortunately this is actually not a countr not a

638
00:40:01,740 --> 00:40:07,920
computationally tractable distribution so people haven't really used it in practice ok it's kind of a way of

639
00:40:07,920 --> 00:40:13,880
thinking about it but it's not really used in practice oaky so again we can also derive

640
00:40:13,880 --> 00:40:20,380
a stick breaking construction using the finite mixture model so the reason why why we

641
00:40:20,380 --> 00:40:25,240
take the K goes to infinity limit of this model that this model can in

642
00:40:25,240 --> 00:40:31,760
a sense breaks down is because we basically assume a symmetric Dirichlet prior on our

643
00:40:31,760 --> 00:40:39,040
mixing proportions right so that's that basically says that every makes every component

644
00:40:39,140 --> 00:40:45,420
has a priori equal probability under that under our prior of being assigned a particular

645
00:40:45,420 --> 00:40:55,340
data point okay so so that actually makes the so basically what we have

646
00:40:55,340 --> 00:40:59,500
is a uniform distribution on one  to K as K goes to infinity and of

647
00:40:59,500 --> 00:41:07,120
course that will break that doesn't actually approach any any  valid distribution on the

648
00:41:07,120 --> 00:41:12,280
natural numbers as K goes to infinity there's no uniform distribution over the over the

649
00:41:12,280 --> 00:41:19,680
natural numbers so the idea to get to the stick breaking construction is that we

650
00:41:19,680 --> 00:41:28,240
can permute the mixture components under our finite mixture model such that the the first

651
00:41:28,240 --> 00:41:36,540
clusters in our list of clusters will on average have higher probability of being assigned

652
00:41:36,540 --> 00:41:41,020
data items okay so this kind of breaks the symmetry of the of the

653
00:41:41,050 --> 00:41:46,760
symmetric Dirichlet and then we can take the limit as K goes to infinity and

654
00:41:46,760 --> 00:41:53,220
this breaking of the symmetry is basically by reordering  the the mixing portions the the

655
00:41:53,220 --> 00:42:02,420
clusters such that the mixing proportion are kind of either they are strictly decreasing okay or

656
00:42:02,420 --> 00:42:07,300
the are kind of stochastically decreasing in the case of where we reorder them

657
00:42:07,310 --> 00:42:11,700
such that they are of strictly decreasing order then we that gives us the Poisson

658
00:42:11,700 --> 00:42:16,550
the chan the firing rates change with the with the variations in what in our

659
00:42:17,740 --> 00:42:22,070
what the the stimulus were received. so this is well known in neuroscience.

660
00:42:22,100 --> 00:42:27,550
it is also well known in neuroscience, and there are here I point here to some some references on

661
00:42:28,240 --> 00:42:33,300
on the journal of the psychology of music and another on also in nature neuroscience

662
00:42:33,300 --> 00:42:37,630
that music elicits feelings and emotions.

663
00:42:37,650 --> 00:42:42,940
and the more expressive is the music, the stronger are these emotions and these feelings.

664
00:42:42,950 --> 00:42:47,690
so we like expressive musics because we li we we like feeling emotions and feelings

665
00:42:47,690 --> 00:42:48,730
because we feel alive

666
00:42:49,300 --> 00:42:56,670
when we have emotions and we feel feelings, right. this is start very early, you know, even sometimes some

667
00:42:56,670 --> 00:43:00,680
people say before we are born but what is sure is that

668
00:43:00,690 --> 00:43:06,420
very soon after we are born, mothers they use this what is called motherese type

669
00:43:06,430 --> 00:43:07,820
of language.

670
00:43:07,880 --> 00:43:13,040
they sing and talk to infants using this cooing tone and have a higher pitch

671
00:43:13,040 --> 00:43:14,250
so in a different way

672
00:43:14,280 --> 00:43:15,550
than when

673
00:43:15,560 --> 00:43:16,970
they talk to adults.

674
00:43:16,990 --> 00:43:21,550
and this is and this is well known now is a paper about another paper

675
00:43:21,550 --> 00:43:30,670
in nature that helps these prelinguistic infants children to regulate to regulate emotional states.

676
00:43:30,680 --> 00:43:36,920
so for these reasons I think expressive  music is a extremely important subject and indeed

677
00:43:36,920 --> 00:43:42,990
it has been approached not only from the neuroscience and the psychology point of view and

678
00:43:42,990 --> 00:43:46,790
in music perception point of view but also from the point of view of AI.

679
00:43:46,790 --> 00:43:50,240
and here I picked up a few

680
00:43:50,240 --> 00:43:51,000
a few

681
00:43:52,550 --> 00:43:55,790
a few early work on

682
00:43:55,820 --> 00:43:58,290
on rendering expressive music.

683
00:43:58,360 --> 00:44:03,280
the first one is maybe is the one of if it's not the first one

684
00:44:03,280 --> 00:44:06,530
one of the very early ones by Margaret Johnson.

685
00:44:06,540 --> 00:44:11,030
she used rules, a rule based system so a production system

686
00:44:11,110 --> 00:44:14,940
an expert system to figure out to infer

687
00:44:14,950 --> 00:44:19,780
the dar duration and the articulation of the notes playing

688
00:44:20,160 --> 00:44:22,410
fugues from the well tempered clavier.

689
00:44:22,450 --> 00:44:25,040
so let's have a let's listen libi a bit

690
00:44:25,050 --> 00:44:26,800
to that also.

691
00:44:39,810 --> 00:44:41,300
here is not bad

692
00:44:41,300 --> 00:44:48,170
but still it's far from being being perfect. and one of the reasons is that

693
00:44:48,170 --> 00:44:52,850
it is a midi rendering, is not is not sound file, is not wave, right. that

694
00:44:52,850 --> 00:44:57,820
we'll see later the difference on that. that puts a lot of limitations for many years

695
00:44:57,890 --> 00:45:01,300
people working on computer music and expressive performances

696
00:45:01,310 --> 00:45:05,210
they were limited to to this midi type of rendering

697
00:45:05,220 --> 00:45:08,560
that is is much much much too limit

698
00:45:08,580 --> 00:45:09,770
too limited.

699
00:45:09,790 --> 00:45:15,940
another work late later on is the director music just that they use rules but

700
00:45:16,690 --> 00:45:19,050
neural networks to fine-tune

701
00:45:19,050 --> 00:45:22,830
the rules also to even two to learn the rules

702
00:45:23,620 --> 00:45:25,330
let's see that which is

703
00:45:25,340 --> 00:45:27,880
a little bit better

704
00:45:27,900 --> 00:45:33,290
well a a a

705
00:45:44,000 --> 00:45:46,510
well i to do

706
00:45:47,050 --> 00:45:51,390
with three expressive

707
00:45:51,450 --> 00:45:54,720
created on set based on the

708
00:45:57,190 --> 00:45:58,630
an article

709
00:45:58,650 --> 00:46:03,430
i what is in the case of johnson was all was dealing only with the

710
00:46:03,430 --> 00:46:09,040
duration of the knowledge articulation goes to this works by pressing the well that's a

711
00:46:09,040 --> 00:46:10,370
third dimension

712
00:46:11,070 --> 00:46:12,880
it's certainly much better

713
00:46:12,920 --> 00:46:14,630
still it is maybe

714
00:46:16,820 --> 00:46:20,380
the last one in one dimension and and i want you also to listen to

715
00:46:20,380 --> 00:46:25,950
some some example is one which is going on since nineteen ninety nine leaves why

716
00:46:25,950 --> 00:46:27,870
again had been more and his group

717
00:46:27,910 --> 00:46:31,620
they are probably right now one of the

718
00:46:31,640 --> 00:46:38,880
the top music technology music computer music look in the world there are impressive work

719
00:46:39,540 --> 00:46:42,570
using data mining and other machine learning techniques

720
00:46:42,620 --> 00:46:46,720
to discover the discovery rules instead of having rules

721
00:46:46,730 --> 00:46:52,880
even is the system discovered rules by the trade being trained by way of tens

722
00:46:52,880 --> 00:46:59,540
of thousands of notes being played by high quality performance unique global centre for grampian

723
00:46:59,630 --> 00:47:07,410
control programme fiona which is centralized and they can capture very very subtle pull station

724
00:47:07,410 --> 00:47:11,180
in the piano so it's very sophisticated to system

725
00:47:11,190 --> 00:47:15,930
and they get extremely interesting results c

726
00:47:15,930 --> 00:47:17,160
two songs

727
00:47:17,170 --> 00:47:20,640
the solution to some of the

728
00:47:41,880 --> 00:47:47,530
so here we are using

729
00:47:47,620 --> 00:47:49,180
all right

730
00:47:50,550 --> 00:47:52,950
but if you mention

731
00:47:53,040 --> 00:47:59,250
city of usually also have no right

732
00:47:59,320 --> 00:48:05,460
which is already quite

733
00:48:05,460 --> 00:48:07,830
the system used

734
00:48:07,850 --> 00:48:09,830
bit human in the loop

735
00:48:09,940 --> 00:48:14,560
you want to suggest the most probable categories

736
00:48:14,580 --> 00:48:15,350
and then

737
00:48:15,370 --> 00:48:19,940
human selected between them that's why we call is more important than precision

738
00:48:20,480 --> 00:48:26,710
and what is the we also have one or more just

739
00:48:26,710 --> 00:48:30,310
one of the few results

740
00:48:30,350 --> 00:48:34,620
OK so that was naive bayes classifiers

741
00:48:35,100 --> 00:48:40,060
later after the work was published people found out that you know SVM performs well

742
00:48:40,060 --> 00:48:44,330
for document categorisation so here is

743
00:48:44,330 --> 00:48:45,710
some other work

744
00:48:45,770 --> 00:48:50,410
which used writers two thousand datasets sample out of that

745
00:48:51,750 --> 00:48:53,460
and the results i will show

746
00:48:53,480 --> 00:49:00,270
the average over sixteen categories out of four hundred three categories

747
00:49:00,910 --> 00:49:04,910
and the feature selection measures compared here

748
00:49:04,910 --> 00:49:10,370
some of them are scruffy simple filtering scoring as the previous one and some of

749
00:49:10,370 --> 00:49:12,600
them are those embedded they set

750
00:49:12,620 --> 00:49:15,000
taking weights from normal

751
00:49:15,000 --> 00:49:17,690
induced by the model

752
00:49:17,690 --> 00:49:19,330
so here in the graph

753
00:49:19,330 --> 00:49:21,960
this is the number of features

754
00:49:22,000 --> 00:49:24,620
and there is an average f one

755
00:49:24,660 --> 00:49:27,370
on the test data

756
00:49:28,560 --> 00:49:31,250
we want to cause to maximize one

757
00:49:31,270 --> 00:49:35,120
and this is the performance of different feature scoring measures the information gain this on

758
00:49:35,120 --> 00:49:36,180
the previous

759
00:49:37,350 --> 00:49:42,120
here it peaks at the beginning but then dropped drastically actually below

760
00:49:42,140 --> 00:49:43,910
using all the features

761
00:49:45,370 --> 00:49:46,790
speaking here again

762
00:49:46,850 --> 00:49:48,960
they've bayesian classifier

763
00:49:49,230 --> 00:49:51,410
by the two measures

764
00:49:51,460 --> 00:49:53,890
that are proposed in this work

765
00:49:53,890 --> 00:49:56,040
using this normal

766
00:49:56,100 --> 00:50:01,100
way to in the normal SVM or weights in the perceptron

767
00:50:01,100 --> 00:50:03,330
as feature scoring measure

768
00:50:03,330 --> 00:50:05,140
and then based on that

769
00:50:05,140 --> 00:50:08,830
the features for sorting the naive bayes classifier was wrong

770
00:50:08,830 --> 00:50:10,410
so they do help

771
00:50:10,480 --> 00:50:12,270
a bayesian classifier

772
00:50:12,310 --> 00:50:13,560
much more

773
00:50:14,350 --> 00:50:18,980
the other two measures that we saw previously

774
00:50:19,060 --> 00:50:23,850
so and what if the perceptron is used as classifier

775
00:50:23,850 --> 00:50:27,500
again the same for feature selection measures

776
00:50:27,540 --> 00:50:31,830
and you can see here again the perceptron SVM

777
00:50:31,850 --> 00:50:34,540
performed the best but actually

778
00:50:34,540 --> 00:50:38,850
the overall performance of perceptron is not really improved

779
00:50:38,910 --> 00:50:42,620
and if one is around zero points fifty five

780
00:50:42,640 --> 00:50:45,750
and it was similar to the previous slide

781
00:50:45,790 --> 00:50:48,790
so the previous slides we got improvement of naive bayes

782
00:50:48,810 --> 00:50:54,080
but not more than what is achieved here using all the features

783
00:50:54,100 --> 00:50:56,500
and if we use linear SVM

784
00:50:56,830 --> 00:50:58,850
what i mentioned previously

785
00:50:58,890 --> 00:51:02,190
and the performance is better than zero points having

786
00:51:02,190 --> 00:51:05,290
the same data set the same setting

787
00:51:05,290 --> 00:51:07,750
svm normal feature selection

788
00:51:07,810 --> 00:51:09,180
have the most

789
00:51:09,180 --> 00:51:14,100
but doesn't really help much the overall performance there is just a tiny gem here

790
00:51:15,000 --> 00:51:17,890
we can't really say is significant

791
00:51:17,940 --> 00:51:20,290
so based on this i would still say

792
00:51:20,290 --> 00:51:22,040
using SVM

793
00:51:22,060 --> 00:51:23,580
except for

794
00:51:24,680 --> 00:51:27,830
by normal separation they don't have graphs here

795
00:51:27,850 --> 00:51:32,960
but the publication was his vision of machine learning research two thousand three there was

796
00:51:33,120 --> 00:51:37,000
a special issue actually of feature selection and dimensionality reduction

797
00:51:37,020 --> 00:51:39,160
which i highly recommend it's really

798
00:51:39,180 --> 00:51:41,980
a lot of very good papers in there

799
00:51:43,210 --> 00:51:45,440
so my last slide

800
00:51:45,500 --> 00:51:47,790
was my

801
00:51:48,210 --> 00:51:51,730
so one one of the interesting thing is

802
00:51:51,770 --> 00:51:55,190
one observation that was published

803
00:51:55,210 --> 00:51:57,140
and feature subset selection

804
00:51:57,160 --> 00:52:00,910
we select some features and discard the rest

805
00:52:00,910 --> 00:52:04,520
but does discard features can help

806
00:52:04,560 --> 00:52:06,810
in some situations

807
00:52:06,810 --> 00:52:11,230
i'm john merriman and this is history two o two

808
00:52:12,060 --> 00:52:17,610
and so on here every monday and wednesday ten thirty two eleven twenty and way

809
00:52:17,800 --> 00:52:21,760
courses that that these are all really major themes and go over this little bit

810
00:52:21,770 --> 00:52:25,670
on the talk about some of the themes are and i kind lecture on things

811
00:52:25,670 --> 00:52:28,050
that i think the complements

812
00:52:28,070 --> 00:52:30,380
what you're doing let me give an example

813
00:52:30,400 --> 00:52:37,110
when i talk about imperialism the new imperialism why is that europe basically took over

814
00:52:37,120 --> 00:52:38,820
the entire world between

815
00:52:38,830 --> 00:52:40,980
of the eighteen eighties and in nineteen

816
00:52:40,990 --> 00:52:47,870
fourteen i you can read the chapter in history modern europe which had fun writing

817
00:52:47,870 --> 00:52:49,980
but i sure in the boy scouts

818
00:52:49,990 --> 00:52:52,900
and i often say that a lecture the boy scouts because i was thrown out

819
00:52:52,900 --> 00:52:54,270
of the boy scouts input them

820
00:52:54,320 --> 00:52:58,470
portland oregon when i was a kid because i didn't manage to accumulate a single

821
00:52:58,470 --> 00:53:02,550
badge and was totally useless after four seasons and

822
00:53:02,640 --> 00:53:06,740
but that's not what i do it because to understand the new imperialism one has

823
00:53:09,150 --> 00:53:15,030
took over essentially all of africa the places were totally uncharted suddenly became a highly

824
00:53:15,030 --> 00:53:16,760
contested between british

825
00:53:16,780 --> 00:53:25,080
french and german and italian concurs that one has to understand the cultural imperialism and

826
00:53:25,080 --> 00:53:27,140
the order of the boy scouts

827
00:53:27,150 --> 00:53:28,660
in britain

828
00:53:28,680 --> 00:53:30,270
has a lot to do with that

829
00:53:30,320 --> 00:53:33,990
why generations of british using their counterparts in germany in

830
00:53:34,110 --> 00:53:37,880
and even australians you in other places

831
00:53:37,900 --> 00:53:41,680
i began to think it was important people look at a map and a schoolhouse

832
00:53:41,960 --> 00:53:47,090
that they had the color red for britain increasingly taking over the map of europe

833
00:53:47,090 --> 00:53:50,310
and lots of other places i mean of of

834
00:53:50,360 --> 00:53:54,250
asia and africa and lots of other places as well as or instead of at

835
00:53:54,250 --> 00:53:57,140
the very beginning that lecture also looked at three things really ought to know about

836
00:53:57,140 --> 00:54:02,840
the new imperialism why why they did this then i talk about the boy scouts

837
00:54:02,840 --> 00:54:07,770
so that those two things will fit together when i talk about world war one

838
00:54:07,820 --> 00:54:12,600
two lectures my friend and colleague j winners and one of them on the great

839
00:54:12,600 --> 00:54:18,560
war and modern memory i instead of finding the entire war and i i think

840
00:54:18,560 --> 00:54:24,260
quite sporty chapter on that in the book all talk about about trench warfare and

841
00:54:24,270 --> 00:54:28,890
you see a film called has glorious and early a kubrick film about the mutinies

842
00:54:28,890 --> 00:54:29,930
in nineteen

843
00:54:29,950 --> 00:54:35,310
in nineteen seventeen and i'll talk about the news in nineteen seventeen people decide enough

844
00:54:35,310 --> 00:54:40,360
is enough in other time for nothing we want to go over the top

845
00:54:40,410 --> 00:54:43,550
so which is the say that it's important to come to lecture

846
00:54:43,570 --> 00:54:47,600
and it's important the sections i cut back on the reading is to use about

847
00:54:47,600 --> 00:54:51,090
four more books and i use now but it's better to concentrate on what you're

848
00:54:51,090 --> 00:54:54,930
doing the books are a history of modern europe

849
00:54:54,980 --> 00:54:58,620
which is basically second edition which i wrote for people like you

850
00:54:58,630 --> 00:55:04,670
and then you read persian letters not all that that would be rather lengthy lengthy

851
00:55:04,670 --> 00:55:05,700
they are still

852
00:55:05,710 --> 00:55:10,690
you read excerpts in personal letters and wanted to know talks about in weight talks

853
00:55:10,690 --> 00:55:15,190
about relations between west and east and it really phenomenal

854
00:55:15,230 --> 00:55:19,640
moment in the history of the alignment

855
00:55:19,660 --> 00:55:23,290
then you have a pause where you basically just reading me

856
00:55:23,420 --> 00:55:28,210
for better for worse hope for better in so you get to a meal as

857
00:55:28,980 --> 00:55:31,720
great novel jermyn out

858
00:55:31,840 --> 00:55:37,180
which is which is the classic lower was the first naturalist

859
00:55:37,350 --> 00:55:41,930
o naturalist novelist are at least in france

860
00:55:41,970 --> 00:55:43,980
and when he wrote to me now

861
00:55:43,990 --> 00:55:48,880
germantown means by adding the of trees but he means the putting of people being

862
00:55:48,880 --> 00:55:50,440
aware of themselves as

863
00:55:50,450 --> 00:55:55,420
as workers he went down the mines in the north of france in the as

864
00:55:55,450 --> 00:55:59,790
in one of the heroes of the book is a woman called that spain who

865
00:55:59,790 --> 00:56:04,350
was fifteen years old but is seen alive life being fifteen years old and was

866
00:56:04,350 --> 00:56:08,020
allow rule wrote the larger now

867
00:56:08,090 --> 00:56:13,620
he went down into the minds to look at fifteen year-old young woman barely older

868
00:56:13,620 --> 00:56:17,600
than girls working in the mines twelve hours a day

869
00:56:17,610 --> 00:56:21,230
so is the book that just resounds with reality and this really kind of an

870
00:56:21,230 --> 00:56:26,200
amazing amazing amazing but anything like that i hope you will

871
00:56:26,220 --> 00:56:28,670
and then home mister the butcher

872
00:56:28,690 --> 00:56:36,170
hale is about accusations of ritual murder in germantown is about about the second german

873
00:56:36,170 --> 00:56:45,020
reich and it's about antisemitism in a small place with bigger consequences george orwell i

874
00:56:45,020 --> 00:56:46,530
went off to fight

875
00:56:46,580 --> 00:56:48,960
the good fight in spain

876
00:56:48,980 --> 00:56:50,880
during the spanish civil war

877
00:56:50,890 --> 00:56:53,700
where was sort of a dry run

878
00:56:53,750 --> 00:57:01,430
four even more horrible war and even more horrible fascist and it's about his engagement

879
00:57:01,430 --> 00:57:07,460
in disillusionment in the spanish republican forces

880
00:57:07,480 --> 00:57:10,350
the loyalist forces and about

881
00:57:10,370 --> 00:57:12,870
tensions in the duplicity of

882
00:57:12,880 --> 00:57:19,380
stones folks undercutting the trustees and undercutting the anarchists and is one of those classics

883
00:57:19,380 --> 00:57:25,020
is a classic for very good reason it's really marvelous read and finally there's ordinary

884
00:57:26,150 --> 00:57:30,350
i go to poland in the last couple years four five times for

885
00:57:30,370 --> 00:57:37,370
for various reasons and i never enough and i went to auschwitz a year ago

886
00:57:37,370 --> 00:57:41,190
and some you probably then there and it you're going through this the

887
00:57:41,200 --> 00:57:42,870
horror that all

888
00:57:42,890 --> 00:57:44,380
and if saying

889
00:57:44,400 --> 00:57:49,020
you're seeing empty suitcases for people's names on them that people don't exist anymore interesting

890
00:57:49,020 --> 00:57:52,970
baby shoes and things like that we think who could have done

891
00:57:52,980 --> 00:57:55,020
who could have gone out

892
00:57:55,040 --> 00:58:02,160
and simply in assembly line wakefield people or infielder on large which is was a

893
00:58:02,160 --> 00:58:05,020
large industrial town still is in poland

894
00:58:05,040 --> 00:58:09,570
simply gone out and one the brains out of mothers babies grandmothers anyway they found

895
00:58:09,570 --> 00:58:10,900
who could have done

896
00:58:10,910 --> 00:58:14,910
well the answer the chris brown has is ordinary man

897
00:58:14,930 --> 00:58:19,880
and he had a quite brilliant idea of looking at a german

898
00:58:19,900 --> 00:58:23,300
you know essentially policemen from hard work

899
00:58:23,310 --> 00:58:27,040
the top the port town of harm called important hands before

900
00:58:27,130 --> 00:58:31,470
and he follows them from the lives of ordinary people

901
00:58:31,520 --> 00:58:32,820
in two

902
00:58:32,830 --> 00:58:35,100
the killing fields there was nothing worse than that

903
00:58:36,010 --> 00:58:41,590
and it's also short terminal along with these other ones are short and it's also

904
00:58:41,630 --> 00:58:45,980
is gripping it's quite quite amazing so

905
00:58:46,280 --> 00:58:50,110
those are the books i think the history money i hope it's fun to read

906
00:58:50,130 --> 00:58:56,030
thank you and enjoy that and the lectures kind of see the themes speaker

907
00:58:56,080 --> 00:59:02,880
for themselves sections to everybody liked wednesday night sessions

908
00:59:03,110 --> 00:59:07,660
one of my colleagues has only once in eight sections are and we won increased

909
00:59:07,870 --> 00:59:12,500
that because sometimes you don't find a large audience and even friday morning and thirty

910
00:59:12,500 --> 00:59:19,780
slots abandon that cannot we're going to have to seven to eight and then third

911
00:59:19,990 --> 00:59:21,080
one thirty

912
00:59:21,090 --> 00:59:23,980
PM on thursday

913
00:59:24,000 --> 00:59:25,570
at two thirty eight

914
00:59:26,630 --> 00:59:29,720
and i don't know when we starting sections that sometimes we don't do it on

915
00:59:29,720 --> 00:59:34,390
the second week it depends on what what is this wednesday so i don't know

916
00:59:34,390 --> 00:59:39,710
maybe start next week maybe we want who knows but they will happen and and

917
00:59:39,720 --> 00:59:43,410
there's also a short forty paper assignment

918
00:59:43,410 --> 00:59:47,700
so what

919
00:59:47,850 --> 00:59:57,060
will be

920
01:00:01,160 --> 01:00:06,660
one hundred four

921
01:00:06,820 --> 01:00:08,970
well i

922
01:00:08,990 --> 01:00:14,320
he is the problem

923
01:00:14,600 --> 01:00:20,160
this is not

924
01:00:22,280 --> 01:00:25,280
is a

925
01:00:35,800 --> 01:00:42,180
which is what

926
01:00:42,200 --> 01:00:48,700
we also use it

927
01:00:48,720 --> 01:00:54,200
it would be nice to

928
01:00:54,330 --> 01:01:02,030
all right that

929
01:01:02,100 --> 01:01:05,200
it is

930
01:01:23,530 --> 01:01:29,760
exactly the

931
01:01:29,780 --> 01:01:35,010
relations with

932
01:01:35,160 --> 01:01:41,370
and you can use

933
01:01:50,890 --> 01:01:54,680
the way

934
01:02:07,300 --> 01:02:14,700
this is the

935
01:02:22,140 --> 01:02:32,930
i was sure

936
01:02:55,450 --> 01:02:59,970
it is the a

937
01:03:07,220 --> 01:03:12,280
well said

938
01:03:20,300 --> 01:03:22,700
we do

939
01:03:31,330 --> 01:03:36,640
what really

940
01:03:50,760 --> 01:03:53,410
we should

941
01:03:53,450 --> 01:04:03,280
you see you want to use one

942
01:04:03,300 --> 01:04:09,410
so we

943
01:04:14,580 --> 01:04:23,300
it's a shame

944
01:04:24,060 --> 01:04:28,990
it also shows

945
01:04:57,870 --> 01:05:04,550
she what

946
01:05:07,620 --> 01:05:13,450
then they

947
01:05:13,600 --> 01:05:18,720
very attractive option

948
01:05:18,720 --> 01:05:23,410
running it on five million data points may say we generated one thousand clusters and

949
01:05:23,420 --> 01:05:27,320
then we tried to analysis to analysis clusters

950
01:05:27,330 --> 01:05:31,090
OK so then you can already see those two reasons might be very different from

951
01:05:31,090 --> 01:05:34,070
each other so you really want to achieve different things as those

952
01:05:34,110 --> 01:05:35,500
those goals

953
01:05:35,510 --> 01:05:40,350
but some of what is called clustering that's already part of the constitution

954
01:05:41,030 --> 01:05:46,330
so yeah i just want to start off with some pictures before we

955
01:05:46,380 --> 01:05:50,630
going to i was the this gene expression data and maybe many of you have

956
01:05:50,630 --> 01:05:54,780
already seen it so you and so on the left side you see something like

957
01:05:55,770 --> 01:05:57,260
so gene expression data is

958
01:05:57,700 --> 01:06:01,780
is something for bioinformatics where you want to show you yes you measure the certain

959
01:06:01,780 --> 01:06:05,380
genes are expressed in cell or not and you do this because you want to

960
01:06:05,380 --> 01:06:09,330
find out i don't know which genetic reasons that might be the certain so there's

961
01:06:09,340 --> 01:06:16,770
something or whatever another biologists c so the raw data get something actually already preprocessed

962
01:06:16,770 --> 01:06:19,270
process that gets like matrix

963
01:06:19,270 --> 01:06:21,810
which contains the genes in the in the

964
01:06:21,840 --> 01:06:25,520
caught in the rules in different conditions the columns and all you might want to

965
01:06:25,520 --> 01:06:28,220
figure out the certain types of so

966
01:06:29,270 --> 01:06:31,380
there are certain genes which act active in the cells

967
01:06:31,420 --> 01:06:34,770
this cluster the data ideally into

968
01:06:34,830 --> 01:06:38,970
i believe that the best results something useful you get something like this where you

969
01:06:39,200 --> 01:06:43,660
don't have all expressed genes on top and the expressed genes on the bottom

970
01:06:43,710 --> 01:06:46,530
and then you can get this to a biologist and you can

971
01:06:46,540 --> 01:06:50,770
i tell you whether this might have some meaning

972
01:06:52,750 --> 01:06:57,500
so another area which i find quite fascinating is network analysis so

973
01:06:57,520 --> 01:06:59,440
there are many datasets which

974
01:06:59,450 --> 01:07:04,320
i'm not just numbers like gene expression in the end numbers vector but there many

975
01:07:04,320 --> 01:07:07,230
datasets are in form of a cross directly so if you want to i don't

976
01:07:07,230 --> 01:07:09,830
know you look at the internet into one to do

977
01:07:09,840 --> 01:07:11,010
two tools

978
01:07:11,020 --> 01:07:14,250
two data mining and some web pages they always have the structure of the graph

979
01:07:14,250 --> 01:07:16,590
because if the links between the web pages

980
01:07:18,860 --> 01:07:22,330
and maybe not you want to figure out which pages are related to each other

981
01:07:23,760 --> 01:07:28,830
is this is an example for social network the you it's from

982
01:07:28,830 --> 01:07:34,190
so company they collected who right similar to which other persons company and you just

983
01:07:34,190 --> 01:07:36,970
make a graph of the didn't maybe you want to know i don't know which

984
01:07:36,970 --> 01:07:37,650
are the

985
01:07:37,660 --> 01:07:41,160
two groups of people who communicate with each other which books which would more communicate

986
01:07:41,400 --> 01:07:43,630
with each other but actually they don't

987
01:07:44,570 --> 01:07:46,830
you might want to find clusters in such a graph

988
01:07:47,220 --> 01:07:54,090
OK an example which i many people of you're working on this image segmentation

989
01:07:54,100 --> 01:07:56,110
but the goal is to beat

990
01:07:56,140 --> 01:07:59,410
i mean what you want to do essentially is you are given an image and

991
01:07:59,410 --> 01:08:03,660
you want to do a very rough segmentation which just don't segments background from foreground

992
01:08:03,670 --> 01:08:07,460
something detect very very large structure in your data set and then you might go

993
01:08:07,460 --> 01:08:11,640
in the DB

994
01:08:11,790 --> 01:08:14,760
and the last example i want to show

995
01:08:14,770 --> 01:08:18,940
this hierarchical clustering so the ones we shall show before it always

996
01:08:18,950 --> 01:08:23,170
the dataset and then you say you want to find out of five clusters

997
01:08:23,200 --> 01:08:25,220
here we have something where we

998
01:08:25,660 --> 01:08:28,660
not only want to find like a certain number of justice but we want to

999
01:08:28,660 --> 01:08:30,450
find a hierarchy of clusters

1000
01:08:30,460 --> 01:08:34,070
the example here still from some paper is

1001
01:08:34,110 --> 01:08:35,270
you want to

1002
01:08:35,330 --> 01:08:38,520
you want to classify mammals in different groups

1003
01:08:38,570 --> 01:08:43,210
and what do they get to such an evolutionary tree which may be so set

1004
01:08:43,270 --> 01:08:46,950
i don't know a rat and mouse are very much related to each other there

1005
01:08:47,330 --> 01:08:49,980
cluster there's much cluster

1006
01:08:50,010 --> 01:08:53,670
but then in a very high scale also related to primates

1007
01:08:53,720 --> 01:08:57,420
but so does the distance between the primates to the rats

1008
01:08:57,450 --> 01:09:00,280
this is pretty large so its or its

1009
01:09:00,310 --> 01:09:03,220
it's a hierarchy so so

1010
01:09:03,230 --> 01:09:06,230
all of the animals and then you can split them into different groups

1011
01:09:06,280 --> 01:09:10,610
that's call rocky clustering

1012
01:09:13,090 --> 01:09:15,640
so and that's the part i want to skip

1013
01:09:15,710 --> 01:09:26,070
because i just want to start with some

1014
01:09:26,080 --> 01:09:30,480
OK so the i want to start with now is called spectral clustering it's and

1015
01:09:30,480 --> 01:09:32,600
i was

1016
01:09:32,620 --> 01:09:36,290
which does not right just rings so you tell you to give me five clusters

1017
01:09:36,290 --> 01:09:39,960
and then hopefully gets to FA or it will give you five classes and whether

1018
01:09:40,090 --> 01:09:43,420
useful or not we will see

1019
01:09:43,460 --> 01:09:46,250
so i want to go into detail it is either them and

1020
01:09:46,280 --> 01:09:48,830
before we do this i want to give a rough overview you that in no

1021
01:09:48,830 --> 01:09:52,070
way in which step we are so

1022
01:09:52,080 --> 01:09:52,850
it's very

1023
01:09:52,890 --> 01:09:57,580
rough idea of spectral clustering is the following year OK we start this data points

1024
01:09:58,450 --> 01:10:00,980
and we assume that similarities

1025
01:10:01,110 --> 01:10:04,730
similarity means so that those as i j

1026
01:10:04,750 --> 01:10:09,370
so similarity means the similarity between two points is high objects very close to each

1027
01:10:09,370 --> 01:10:13,930
other if they were related to each other so the higher the value more

1028
01:10:13,950 --> 01:10:17,350
the more i believe those points in the same cluster

1029
01:10:17,370 --> 01:10:21,310
now what i do is i want to represent the state in in in in

1030
01:10:21,840 --> 01:10:25,460
form but i don't have caught it so i don't assume that those points on

1031
01:10:25,500 --> 01:10:30,110
our something like this it can be anything just any similarity function

1032
01:10:30,120 --> 01:10:34,070
and then an abstract way of representing data will be used to build some similarity

1033
01:10:34,070 --> 01:10:36,890
graph and you will see different ways of doing that

1034
01:10:38,850 --> 01:10:42,620
so what we do is we just to each vertex of of this graph represents

1035
01:10:42,630 --> 01:10:44,120
the data points

1036
01:10:44,290 --> 01:10:49,840
then we connect vertices which are similar to each other so we could say

1037
01:10:49,850 --> 01:10:53,410
and then you put an edge between between the two largest so you if i

1038
01:10:53,410 --> 01:10:56,040
don't know point one point two and they similar to each other so we put

1039
01:10:56,040 --> 01:10:57,320
an edge

1040
01:10:57,330 --> 01:11:01,720
additionally what do is we wait that's by the similarities of points are very similar

1041
01:11:01,730 --> 01:11:04,950
to each other you could like all this

1042
01:11:04,980 --> 01:11:09,120
but it's not use this shortage so that they related to each other that close

1043
01:11:09,190 --> 01:11:11,830
and it carries the weight as i j

1044
01:11:11,870 --> 01:11:17,120
here they and they may be for positive similarity but it's so human similarity zero

1045
01:11:17,120 --> 01:11:21,170
point nine and here we have point one suggests is a graphic way of showing

1046
01:11:21,200 --> 01:11:24,110
their similarity of the tools

1047
01:11:24,160 --> 01:11:25,220
and then

1048
01:11:25,280 --> 01:11:28,600
OK now we have this graph and goal of clustering is then

1049
01:11:28,620 --> 01:11:29,550
so this

1050
01:11:29,570 --> 01:11:34,180
to find the a cut in this graph such that

1051
01:11:34,200 --> 01:11:39,660
such that the resulting things are clusters and define how this can be done

1052
01:11:39,710 --> 01:11:42,330
so that's that's the basic schemes

1053
01:11:42,330 --> 01:11:46,780
just to give them back of your mind with

1054
01:11:47,460 --> 01:11:49,820
because now we need to start some notation

1055
01:11:49,820 --> 01:11:51,300
of the other

1056
01:11:52,180 --> 01:11:54,450
i don't know ten to fifteen

1057
01:11:55,390 --> 01:11:59,890
known now that we know all of the other if you think thirty or forty

1058
01:11:59,890 --> 01:12:05,990
thousand structures and the the base of increases is increasing so you have a huge

1059
01:12:06,430 --> 01:12:10,550
projects that aim at getting the three d structures

1060
01:12:10,570 --> 01:12:13,450
of all the proteins of an organism

1061
01:12:13,470 --> 01:12:17,050
and and so more and more so this is what you feel differences but this

1062
01:12:17,050 --> 01:12:20,340
is the the fitting we need to care about we start to have databases of

1063
01:12:21,280 --> 01:12:24,740
OK so we have the resources there's of thousands of researchers at the start to

1064
01:12:24,740 --> 01:12:27,620
be able to manipulate them to compare them to do the right thing to do

1065
01:12:27,620 --> 01:12:29,010
mission on them

1066
01:12:29,070 --> 01:12:33,050
so the fifty we input minutes for three d structures

1067
01:12:33,300 --> 01:12:38,080
on top of that we have network so i i showed you the iteration put

1068
01:12:38,080 --> 01:12:41,660
in the interaction network there are other ways that we have so these things are

1069
01:12:41,660 --> 01:12:47,240
input kids i think that are created by these technologies that were given kind of

1070
01:12:47,240 --> 01:12:51,160
them on the web and need to process them but we need to design first

1071
01:12:51,160 --> 01:12:56,930
problems and and reasons that should be able to process sequential structure courses throughout and

1072
01:12:56,930 --> 01:13:01,280
then you of course the time series of oppression it is many things but so

1073
01:13:01,280 --> 01:13:04,140
there is the there is still the on the data

1074
01:13:04,140 --> 01:13:06,620
that's for sure

1075
01:13:06,640 --> 01:13:12,410
one the expectations this is a very notable so why do we care about all

1076
01:13:12,430 --> 01:13:17,030
that in fact what the bigger targets of these industries so here the goal is

1077
01:13:17,030 --> 01:13:22,370
not only to have a good start enzyme for instance or to to make money

1078
01:13:22,370 --> 01:13:28,070
by amazing french series here the big expectations for first

1079
01:13:28,240 --> 01:13:33,840
in biology as a science thanks to this new that should be able to better

1080
01:13:33,840 --> 01:13:39,930
understand biology and there is a big industry towns and cities pressure images of course

1081
01:13:40,050 --> 01:13:43,530
so the idea of course we hope that things so all that would be able

1082
01:13:43,530 --> 01:13:47,870
to understand the structure of the functions of all the proteins et cetera that we're

1083
01:13:47,910 --> 01:13:52,740
able to to international recognition so how does it work to be

1084
01:13:52,820 --> 01:13:55,950
there are many things we don't know we in fact we we know almost nothing

1085
01:13:55,950 --> 01:14:01,680
about bot towards and sympathy with noise how to design a single organism

1086
01:14:01,700 --> 01:14:03,140
that does the function

1087
01:14:03,140 --> 01:14:06,800
here there is a very hard find this called synthetic biology

1088
01:14:06,800 --> 01:14:10,030
that a OK i want to design a small but we have to solve this

1089
01:14:10,030 --> 01:14:16,030
problem like the pollution problem or whatever and if i understand how

1090
01:14:16,300 --> 01:14:20,090
so how i can design about these

1091
01:14:20,100 --> 01:14:25,740
this involves of course understanding know-it-alls then this can have many applications

1092
01:14:25,740 --> 01:14:30,450
OK images and of course there are many applications that are the focus of

1093
01:14:30,470 --> 01:14:36,200
possession of this first things signature we hope to be able to understand many diseases

1094
01:14:36,200 --> 01:14:41,280
of the molecular level sold locally looking at the global thing i point out that

1095
01:14:41,280 --> 01:14:45,720
this is a start but now we can thanks to microarrays they face to match

1096
01:14:45,740 --> 01:14:50,140
between history we can look at the article details of how can serve rules how

1097
01:14:50,280 --> 01:14:56,260
they efficient over the virus takes place happens etcetera so the hope is that by

1098
01:14:56,570 --> 01:15:00,620
molecule are aspects of this we have better understanding and therefore better ideas on how

1099
01:15:00,620 --> 01:15:02,220
to solve that

1100
01:15:02,260 --> 01:15:08,140
there is probably something which is not so difficult i mean that some successes the

1101
01:15:08,140 --> 01:15:11,390
which is about early diagnosis and prognosis of diseases

1102
01:15:11,410 --> 01:15:16,840
so before you have you have some physical before you have some something called the

1103
01:15:16,840 --> 01:15:20,240
table from the table things at the macro level

1104
01:15:20,640 --> 01:15:23,680
of the face of it this is there are many changes at the molecular level

1105
01:15:23,830 --> 01:15:26,640
for instance when you have a concern that differ in their kids

1106
01:15:27,010 --> 01:15:31,970
things have started to change in your yourselves several years ago

1107
01:15:31,970 --> 01:15:33,370
OK so all

1108
01:15:33,370 --> 01:15:38,680
o thing because because we have the ability now this approach to process

1109
01:15:38,680 --> 01:15:42,660
so sort of information the molecular level we hope to be able to do early

1110
01:15:42,660 --> 01:15:50,240
diagnosis and early produces whole how would what be able to show this is thanks

1111
01:15:50,240 --> 01:15:56,780
to a biomarkers and things to do section of molecular aspects of it and this

1112
01:15:56,930 --> 01:15:58,240
also a lot of of

1113
01:15:58,910 --> 01:16:02,990
for instance when you want you can so one of the most difficult occasion of

1114
01:16:02,990 --> 01:16:08,240
and is in personal cancer is given different types of tumor u u

1115
01:16:08,260 --> 01:16:12,910
it is the moniker you obtain composition of its tomorrow by the effect of expression

1116
01:16:12,910 --> 01:16:15,950
of the genes and they need to design a classifier you

1117
01:16:15,950 --> 01:16:20,820
as you and whatever to predict whether when i give you my simple they can

1118
01:16:20,820 --> 01:16:24,240
pretty what type of commodities and therefore what is the best treatment for it can

1119
01:16:25,280 --> 01:16:29,820
of course there still some hope to design new drugs and to have some personalized

1120
01:16:29,820 --> 01:16:34,510
medicine that for the weight

1121
01:16:34,660 --> 01:16:40,530
OK so i will almost certainly this information but then you don't want to spend

1122
01:16:40,530 --> 01:16:43,070
too much time on it

1123
01:16:43,090 --> 01:16:46,930
if question is OK what do we do in bioinformatics if you got away from

1124
01:16:46,930 --> 01:16:51,510
this conference what types of paper written what other things it's very hard to have

1125
01:16:51,510 --> 01:16:55,340
a global teacher because you see that things are going very fast you have new

1126
01:16:55,340 --> 01:17:00,450
data every year and this new program every year

1127
01:17:00,450 --> 01:17:05,070
but i just want to summarize some aspects of the things that have received deserved

1128
01:17:05,090 --> 01:17:07,120
a lot of attention recently

1129
01:17:07,640 --> 01:17:13,200
and this is this is a ten years ago first you have to feel soul

1130
01:17:13,220 --> 01:17:17,910
obviously one of the the thirty first thing to be solved was you know that

1131
01:17:17,950 --> 01:17:19,450
we had this is

1132
01:17:19,450 --> 01:17:21,520
the blue component and ten percent of the

1133
01:17:21,580 --> 01:17:25,120
so i went to the inputs ten percent the blue component

1134
01:17:25,200 --> 01:17:28,320
so first of all pick one of the components with probability given by the mixing

1135
01:17:28,320 --> 01:17:30,990
coefficients and then you call your local

1136
01:17:31,000 --> 01:17:38,310
that function which generates from the corresponding guassian with its mean and its variance

1137
01:17:38,360 --> 01:17:40,180
so two stage process

1138
01:17:40,250 --> 01:17:42,010
that generates the data points

1139
01:17:42,020 --> 01:17:44,980
so the first time you might be the red components and then from the red

1140
01:17:45,230 --> 01:17:47,270
c and you have to sample the state point here

1141
01:17:47,710 --> 01:17:51,270
the next time you take the blue component and the happen to choose that data

1142
01:17:51,270 --> 01:17:56,900
points and so on

1143
01:17:56,910 --> 01:17:58,720
now we generated the data

1144
01:17:58,730 --> 01:18:00,630
in that way

1145
01:18:04,190 --> 01:18:05,770
we will actually be able to

1146
01:18:05,780 --> 01:18:09,710
colour in the data points so if i ran my when i run my

1147
01:18:09,760 --> 01:18:12,420
it's about like generate this data set

1148
01:18:12,500 --> 01:18:16,730
i can color in the data points according to which of the components

1149
01:18:16,740 --> 01:18:18,820
actually generated the data point

1150
01:18:18,840 --> 01:18:23,300
which component was chosen

1151
01:18:23,310 --> 01:18:27,620
in practice we want to solve the inverse problem the was given the data we

1152
01:18:27,620 --> 01:18:30,800
want to find the parameters of the mixture model

1153
01:18:33,480 --> 01:18:37,280
that will turn out that if we knew the colours of the data points is

1154
01:18:37,280 --> 01:18:38,710
you'll see in a moment

1155
01:18:38,760 --> 01:18:39,710
they're in

1156
01:18:39,720 --> 01:18:43,130
maximum likelihood would be trivial it would simply say

1157
01:18:44,030 --> 01:18:47,660
the corresponding guassian to the corresponding

1158
01:18:47,670 --> 01:18:49,120
in other words we fit

1159
01:18:49,140 --> 01:18:52,280
the red component to the right data the green and the green bay to the

1160
01:18:52,280 --> 01:18:57,020
blue components the blue data using the standard maximum likelihood solution for single galaxy and

1161
01:18:57,300 --> 01:19:00,170
it's pretty obvious but will prove that in a minute

1162
01:19:00,280 --> 01:19:04,810
the problem is that we don't know the colours the colours are latent variables the

1163
01:19:04,810 --> 01:19:06,800
things which are not available to us

1164
01:19:06,810 --> 01:19:09,660
in other words when i give you a data set and ask you to fit

1165
01:19:09,660 --> 01:19:13,880
a mixture of gaussians you don't see this what you see is this

1166
01:19:13,890 --> 01:19:17,490
so you see on college data point you don't have the colours so the colours

1167
01:19:17,490 --> 01:19:22,540
are hidden variables or latent variables

1168
01:19:22,590 --> 01:19:25,410
so we can formalise that mathematically

1169
01:19:25,450 --> 01:19:27,230
as follows

1170
01:19:27,290 --> 01:19:32,370
so these latent variables are are binary latent variables

1171
01:19:33,640 --> 01:19:35,070
for every

1172
01:19:35,090 --> 01:19:38,240
for every data point we have

1173
01:19:39,280 --> 01:19:44,500
is a set of binary variables one for every component so zk

1174
01:19:44,510 --> 01:19:48,620
k runs from one up to capitol k the number of components in the mixture

1175
01:19:48,670 --> 01:19:51,540
each element of z is either zero or one

1176
01:19:51,550 --> 01:19:55,790
and in fact all of them are set to zero except the particular one which

1177
01:19:55,790 --> 01:19:58,320
picks out the corresponding component

1178
01:19:58,340 --> 01:20:02,600
OK so he would be an example

1179
01:20:04,350 --> 01:20:09,430
this this would be any equals one to n equals five here's an example

1180
01:20:09,450 --> 01:20:11,840
of the latent variable matrix

1181
01:20:11,860 --> 01:20:14,030
four five data points

1182
01:20:14,050 --> 01:20:17,460
in which we have a mixture of three gaussians which might be the red green

1183
01:20:17,460 --> 01:20:19,180
and blue components

1184
01:20:19,200 --> 01:20:23,540
so what this says is that data point number one is coloured red data point

1185
01:20:23,540 --> 01:20:24,810
number two is blue

1186
01:20:24,840 --> 01:20:27,760
data point number three is green and so on

1187
01:20:27,780 --> 01:20:31,350
so you see that in every row of this matrix

1188
01:20:31,390 --> 01:20:33,000
all the entries is zero

1189
01:20:33,020 --> 01:20:36,810
except one of them which is set to one so that picks out

1190
01:20:36,850 --> 01:20:39,770
the component was generated the data point

1191
01:20:39,780 --> 01:20:43,410
so when i plot this when i put the corresponding matrix of data

1192
01:20:43,460 --> 01:20:48,080
i can colour the first component red in colour second component blue and so on

1193
01:20:48,390 --> 01:20:49,430
what i would say is this

1194
01:20:49,830 --> 01:20:53,610
so if i knew the latent variables i could draw this picture

1195
01:20:53,630 --> 01:20:55,740
obviously the latent so i can't

1196
01:20:55,750 --> 01:20:59,100
after all that picture instead

1197
01:21:06,200 --> 01:21:07,970
we can write down

1198
01:21:07,980 --> 01:21:11,880
a probabilistic model which is the joint probability distribution

1199
01:21:11,890 --> 01:21:13,150
for the

1200
01:21:13,160 --> 01:21:17,840
latent variables and the observed variables that crossed windsor this little piece of graph here

1201
01:21:17,840 --> 01:21:20,520
so for the moment just consider one data point

1202
01:21:20,590 --> 01:21:23,170
x one observation x

1203
01:21:23,180 --> 01:21:28,090
and z is the vector corresponding to one of those rows of the matrix so

1204
01:21:28,120 --> 01:21:28,950
this is

1205
01:21:30,300 --> 01:21:33,150
so z here my notation here is the z

1206
01:21:33,170 --> 01:21:35,380
is a little

1207
01:21:35,760 --> 01:21:41,730
it's just this little vectors zero zero one or whatever

1208
01:21:41,750 --> 01:21:44,230
and this is going to be a little graphical model

1209
01:21:44,280 --> 01:21:47,340
so the joint distribution

1210
01:21:47,350 --> 01:21:49,620
p of x and z

1211
01:21:49,670 --> 01:21:53,390
will be here said given its parents around today

1212
01:21:53,400 --> 01:21:57,830
here that's given its parents which is z

1213
01:21:57,850 --> 01:22:01,190
so i need to write down the marginal for z and the conditional for x

1214
01:22:01,190 --> 01:22:04,560
given z and then i've specified the joint distribution

1215
01:22:04,610 --> 01:22:08,530
was that is just a little indicator variables appear given z

1216
01:22:08,580 --> 01:22:09,980
if i know

1217
01:22:09,990 --> 01:22:15,300
which components generated the data points and is just the guassian with the corresponding and

1218
01:22:15,370 --> 01:22:19,930
and variance so p that's given z i can just write is the product of

1219
01:22:19,930 --> 01:22:25,020
all components of that guassian raised the powers zk k

1220
01:22:25,070 --> 01:22:27,430
because all of these numbers is zero

1221
01:22:27,450 --> 01:22:30,480
except one of them which is one

1222
01:22:30,490 --> 01:22:34,360
and that it's not the corresponding guassian

1223
01:22:34,410 --> 01:22:38,180
and similarly for the prior probability of the

1224
01:22:38,190 --> 01:22:44,010
of said remember when i generated this from matlab i picked the component with probability

1225
01:22:44,030 --> 01:22:44,900
pi k

1226
01:22:44,910 --> 01:22:48,840
OK component probability pi k so again z

1227
01:22:48,890 --> 01:22:52,240
i could just write is the product of pi to the powers that because all

1228
01:22:52,240 --> 01:22:54,830
the z zero except one of them say zj j

1229
01:22:55,350 --> 01:22:58,560
and that picks out the mixing coefficients pi j

1230
01:22:58,570 --> 01:23:03,110
so if i multiply this by this i get the joint distribution

1231
01:23:03,130 --> 01:23:06,390
these are both exponential family distributions life

1232
01:23:06,500 --> 01:23:09,570
i've expanded instead of just writing down

1233
01:23:09,580 --> 01:23:13,840
the distribution of x i've expanded my space by introducing latent variables and written down

1234
01:23:13,880 --> 01:23:15,340
joint distribution

1235
01:23:15,350 --> 01:23:18,230
over the observed and the hidden variables

1236
01:23:18,300 --> 01:23:21,930
that distribution is expressed by directed graph

1237
01:23:21,970 --> 01:23:25,040
in which the conditional distributions are from the exponential family

1238
01:23:25,080 --> 01:23:30,670
very simple example but we can apply the same idea much more complex models

1239
01:23:30,680 --> 01:23:34,640
now i can ask the following question i ask what the marginal of x one

1240
01:23:34,640 --> 01:23:35,870
p of x

1241
01:23:35,880 --> 01:23:40,640
what you get p of x

1242
01:23:40,640 --> 01:23:41,010
the math

1243
01:23:51,040 --> 01:23:55,000
o using huffman codes in this dumb way i could try and be a bit smarter you could say

1244
01:23:55,640 --> 01:23:58,290
let's encode blocks of characters together

1245
01:23:58,900 --> 01:24:03,630
and so you could take encounters time and say i can kill of this problem

1246
01:24:03,780 --> 01:24:06,720
one simple by itself being very probable by

1247
01:24:07,180 --> 01:24:08,230
having blocks say

1248
01:24:08,810 --> 01:24:12,380
six characters time or maybe sixteen characters time

1249
01:24:12,940 --> 01:24:14,440
and analysis all possible

1250
01:24:15,110 --> 01:24:16,730
blocks of sixteen characters

1251
01:24:17,140 --> 01:24:19,990
and ask the audience what the probability and all

1252
01:24:21,790 --> 01:24:23,620
fifty to the past sixteen

1253
01:24:24,940 --> 01:24:29,060
sequences of length sixteen is assuming alphabet size of size fifty

1254
01:24:30,290 --> 01:24:32,840
and then we can use the huffman algorithm to encode the blocks

1255
01:24:33,620 --> 01:24:34,450
well you could say

1256
01:24:35,780 --> 01:24:40,110
i think in terms of runs we could say how many runs and one other

1257
01:24:40,120 --> 01:24:44,010
and here we could ask the audience to predict how long do you think you'll

1258
01:24:44,010 --> 01:24:46,090
get ones fall before you get it wrong

1259
01:24:47,150 --> 01:24:51,020
and how can i ask you for bets on the number of how long the run is

1260
01:24:51,600 --> 01:24:53,780
and i give an integer that was

1261
01:24:54,400 --> 01:24:56,250
zero and it was five get a zero

1262
01:24:56,720 --> 01:24:59,220
five and then there was a around like three

1263
01:24:59,750 --> 01:25:01,400
and then there were three guesses and so forth

1264
01:25:01,900 --> 01:25:06,030
so we can encode into a different alphabet where we talk about run lengths

1265
01:25:06,880 --> 01:25:10,090
and then use the huffman algorithm and then if you if we can still use of

1266
01:25:10,880 --> 01:25:11,390
but maybe

1267
01:25:12,850 --> 01:25:14,390
we're finding is a little bit complicated

1268
01:25:14,790 --> 01:25:17,780
and maybe we'd like to move on and say that there's a better way than

1269
01:25:17,780 --> 01:25:19,950
simple codes and that's the message of this lecture

1270
01:25:24,090 --> 01:25:27,740
if you were to do what i just described the for example including blocks the

1271
01:25:27,750 --> 01:25:32,090
characters you'd have to compute lots more joint probability if we got up to this

1272
01:25:32,890 --> 01:25:34,050
i would've been saying q

1273
01:25:34,720 --> 01:25:39,440
okay i e e now tell me what you think that the next five characters

1274
01:25:39,440 --> 01:25:42,790
might be just the next five years ago i guess again and again and again

1275
01:25:43,020 --> 01:25:47,710
and i be implicitly running through a great long list of possibilities for the next

1276
01:25:47,710 --> 01:25:48,460
five characters

1277
01:25:49,560 --> 01:25:51,120
which you would have to actually compute

1278
01:25:51,600 --> 01:25:56,470
even though only one of those five character sequences actually don't happen so you'd have

1279
01:25:56,470 --> 01:25:58,490
to think about many more possibilities

1280
01:25:59,490 --> 01:26:00,990
so in summary

1281
01:26:02,090 --> 01:26:04,220
what we would like is any solution

1282
01:26:05,000 --> 01:26:05,830
data compression

1283
01:26:06,320 --> 01:26:08,000
where we can use

1284
01:26:08,790 --> 01:26:09,950
context dependent

1285
01:26:11,370 --> 01:26:12,890
probabilistic models

1286
01:26:13,390 --> 01:26:16,930
so we want to allow dependent want to allow for adaptation

1287
01:26:17,750 --> 01:26:18,720
andy we want

1288
01:26:19,290 --> 01:26:22,440
to be able to handle very concentrated probability distributions

1289
01:26:23,290 --> 01:26:24,510
so here's the summary

1290
01:26:28,280 --> 01:26:29,950
we want a methods that can handle

1291
01:26:46,970 --> 01:26:47,800
so we want

1292
01:26:53,190 --> 01:26:53,930
anne d

1293
01:26:56,040 --> 01:26:57,080
we want to get rid

1294
01:26:59,760 --> 01:27:02,590
all the symbol code gap between our land age

1295
01:27:05,480 --> 01:27:08,690
by handling strong certain distributions well

1296
01:27:13,050 --> 01:27:17,760
about tell you how to solve this problem but first let's just think it's something

1297
01:27:17,760 --> 01:27:19,610
we can do with this board before we wanted of

1298
01:27:20,450 --> 01:27:21,470
here's the game is to play

1299
01:27:22,450 --> 01:27:25,550
i'm gonna write down the green integers on a piece a paper

1300
01:27:27,400 --> 01:27:30,210
so how many guesses that did for the first character

1301
01:27:33,150 --> 01:27:35,270
five for the next five for the next three

1302
01:27:35,660 --> 01:27:36,790
one one one

1303
01:27:40,730 --> 01:27:42,730
one one one one and so on

1304
01:27:45,140 --> 01:27:47,260
well i could write down those entities on a piece of paper

1305
01:27:49,600 --> 01:27:49,870
and now

1306
01:27:52,100 --> 01:27:53,030
these integers

1307
01:27:53,440 --> 01:27:56,070
contain enough information to reconstruct the headline

1308
01:27:56,950 --> 01:27:57,400
sort of

1309
01:27:58,320 --> 01:28:03,130
imagine that's through this small here there's another lecture theatre identical to this one

1310
01:28:03,630 --> 01:28:05,870
identical in the sense that it's got a lecture in the front

1311
01:28:06,660 --> 01:28:09,600
here's my identical twin and all the view in the lecture theatre

1312
01:28:10,070 --> 01:28:14,850
are also there in the form of identical twins with exactly the same thoughts beliefs and predictions

1313
01:28:15,250 --> 01:28:16,120
and we post this

1314
01:28:17,870 --> 01:28:19,100
into another lecture theatre

1315
01:28:20,140 --> 01:28:23,130
and i come in and i said we just talk about symbol codes and now

1316
01:28:23,130 --> 01:28:25,950
we're gonna play a guessing game and i'm going to tell you a headline that

1317
01:28:25,950 --> 01:28:26,540
i've got written

1318
01:28:27,090 --> 01:28:29,340
thank you very much on this is a paper here

1319
01:28:31,560 --> 01:28:37,260
over the headlines and that it uses this alphabet and please start getting and now you make guesses

1320
01:28:38,050 --> 01:28:38,450
you say

1321
01:28:39,720 --> 01:28:40,260
and i say no

1322
01:28:40,950 --> 01:28:45,510
as a user may and i don't know is that this site and then when

1323
01:28:45,510 --> 01:28:48,930
we get the fifth guess cause has five on this with babies someone says them

1324
01:28:48,930 --> 01:28:52,150
and i said yeah that's what it says it is okay and we get them

1325
01:28:52,500 --> 01:28:53,550
and then we keep on going

1326
01:28:55,820 --> 01:28:59,970
stopping you whenever you get the right number and because we perfect identical twins

1327
01:29:00,460 --> 01:29:01,410
the next lecture

1328
01:29:03,090 --> 01:29:06,490
we would end up writing down exactly the same set of letters on the board

1329
01:29:06,590 --> 01:29:08,580
we go through the same reasoning process

1330
01:29:09,030 --> 01:29:10,060
and we would end up with a headline

1331
01:29:12,730 --> 01:29:14,130
so that's a fun idea

1332
01:29:14,900 --> 01:29:18,440
that we couldn't quite implement because we have all got identical twins

1333
01:29:18,990 --> 01:29:24,940
but it's the the whole it's the heart of all state-of-the-art compression algorithms that you're

1334
01:29:24,950 --> 01:29:31,290
range have identical twins the two ends and those identical twins run forest this context

1335
01:29:31,290 --> 01:29:36,060
dependent adaptive probabilistic modeling in software yes question

1336
01:29:42,650 --> 01:29:49,710
the question is are we assuming that it's a deterministic universe identical twins would exactly replicators yes

1337
01:29:50,230 --> 01:29:54,800
for the purposes of today the universe is actually deterministic and your identical twins would

1338
01:29:54,800 --> 01:29:56,150
behave exactly the same as you

1339
01:29:56,710 --> 01:29:58,300
that's the analogy and i'm assuming

1340
01:29:59,820 --> 01:30:04,970
four this to work in real life we need to ensure that we have to the deterministic computers so

1341
01:30:05,620 --> 01:30:10,850
be adaptive probabilistic model that i'm talking about will be a computer program that tries

1342
01:30:10,850 --> 01:30:12,860
to emulate an expert audience like you

1343
01:30:13,620 --> 01:30:14,870
hand we will have

1344
01:30:15,290 --> 01:30:18,060
one copy about program at our encoding

1345
01:30:18,500 --> 01:30:24,060
computer will have an exactly identical deterministic copy on another computer that will work in

1346
01:30:24,060 --> 01:30:28,010
exactly the same way so that we can replicate your guesses in your predictions

1347
01:30:29,800 --> 01:30:36,740
okay so how do we do about well first i'm going to introduce the encoding methods called arithmatic coding

1348
01:30:40,450 --> 01:30:41,330
introduce it to you

1349
01:30:41,970 --> 01:30:43,190
using several

1350
01:30:43,580 --> 01:30:44,800
toy examples

1351
01:30:45,950 --> 01:30:48,660
one of which will be the you the whole identical twin

1352
01:30:51,110 --> 01:30:52,550
that will start with slightly simpler

1353
01:31:05,570 --> 01:31:07,500
so let's give ourselves three tasks

1354
01:31:09,800 --> 01:31:12,840
these are tasks that are simpler than the general

1355
01:31:14,150 --> 01:31:16,120
desire that we have on the board over here

1356
01:31:17,110 --> 01:31:19,530
this implies that they have to be the essence

1357
01:31:20,350 --> 01:31:21,820
of the problems are grappling with

1358
01:31:22,540 --> 01:31:24,370
the first example is going to be bent coin

1359
01:31:25,840 --> 01:31:28,270
and we might have a bent coin with probabilities

1360
01:31:28,730 --> 01:31:29,340
they be

1361
01:31:30,900 --> 01:31:32,580
equal to i don't know point nine nine

1362
01:31:33,310 --> 01:31:34,190
o point zero one

1363
01:31:38,770 --> 01:31:41,760
so we already know that symbol codes struggle with this we have to do

1364
01:31:42,180 --> 01:31:45,500
something clunky with symbol codes to get any compression

1365
01:31:46,070 --> 01:31:46,790
for the bent coin

1366
01:31:46,790 --> 01:31:49,270
this is a database of

1367
01:31:49,280 --> 01:31:52,540
real objects now, as opposed to handwritten digits

1368
01:31:52,570 --> 01:31:55,000
they're toys bought in the USA

1369
01:31:56,500 --> 01:31:57,640
there's animals,

1370
01:31:58,600 --> 01:32:01,060
planes, trucks, and cars

1371
01:32:01,100 --> 01:32:05,390
there's five different objects each of which is seen from many different viewpoints under many

1372
01:32:05,390 --> 01:32:06,840
different lighting conditions

1373
01:32:06,990 --> 01:32:12,630
and then the test set is five more objects not the same objects so rigid

1374
01:32:12,630 --> 01:32:14,490
geometry is not gonna help you here

1375
01:32:14,550 --> 01:32:18,210
five more objects of the same class. so in particular

1376
01:32:18,270 --> 01:32:23,540
one of the test animals is a stegosaurus, so you have to gen--you have to realize that a stegosaurus is a lion,

1377
01:32:23,550 --> 01:32:24,650
not a plane

1378
01:32:25,770 --> 01:32:30,690
because the toys were bought in america,

1379
01:32:30,700 --> 01:32:33,100
every single person is holding a weapon

1380
01:32:33,120 --> 01:32:35,950
so the concept weapon would have done equally well.

1381
01:32:35,960 --> 01:32:44,210
in the data,

1382
01:32:44,280 --> 01:32:48,810
that is, in the norb database, there's two ninety six by ninety six images

1383
01:32:48,890 --> 01:32:50,300
so stereo pairs

1384
01:32:50,310 --> 01:32:51,750
with a centred object

1385
01:32:51,760 --> 01:32:54,840
the edges are mainly blank because it was designed for convolutional neural nets

1386
01:32:54,990 --> 01:32:57,170
and the background is uniform and bright

1387
01:32:57,170 --> 01:33:00,840
so i was in a hurry. i did this just before i left to come here,

1388
01:33:00,850 --> 01:33:02,080
in a few days,

1389
01:33:02,090 --> 01:33:05,270
so i simplified the data by: i first throw away one image

1390
01:33:05,300 --> 01:33:08,180
and then i only use the middle part of the image that was left and then i

1391
01:33:08,180 --> 01:33:13,460
downsampled it in the crudest way by averaging four pixels together

1392
01:33:13,520 --> 01:33:15,550
and then i looked at the histogram of the intensities

1393
01:33:15,570 --> 01:33:18,300
which looks like this, where this is the bright background

1394
01:33:18,350 --> 01:33:20,390
and i threw away all this stuff

1395
01:33:20,390 --> 01:33:24,240
and call all the background zero, and then just measure intensity this way

1396
01:33:24,250 --> 01:33:27,940
so now it fits the rectified linear nicely because got a lot of zeros

1397
01:33:28,030 --> 01:33:30,740
and then some fairly uniform values above zero

1398
01:33:30,740 --> 01:33:32,600
but limited

1399
01:33:33,200 --> 01:33:37,570
so that was the data i used

1400
01:33:37,620 --> 01:33:40,580
and i showed you lots of pictures of it. you can still tell what--what

1401
01:33:40,630 --> 01:33:42,000
the things are pretty well.

1402
01:33:42,010 --> 01:33:43,960
there's a few cases where

1403
01:33:43,970 --> 01:33:49,570
the bright pixels you throw are really important, and you can't tell any more what things are.

1404
01:33:49,590 --> 01:33:51,460
and here's the results i got

1405
01:33:51,490 --> 01:33:55,500
so this is now on images of real objects, as opposed to mnist digits.

1406
01:33:55,510 --> 01:33:58,220
if you take the full norb

1407
01:33:58,270 --> 01:34:00,600
database with

1408
01:34:00,630 --> 01:34:02,190
the full resolution images

1409
01:34:02,220 --> 01:34:06,190
and do logistic regression on the pixels, you get 20% error

1410
01:34:06,240 --> 01:34:08,550
if you train a gaussian svm,

1411
01:34:08,560 --> 01:34:12,090
or rather, leon bottou trains a gaussian svm, and he's pretty good at training them, you get

1412
01:34:12,230 --> 01:34:13,720
eleven point six percent

1413
01:34:13,790 --> 01:34:18,190
a convolutional neural net trained in le cun's group worked out really good, 6%, but this

1414
01:34:18,190 --> 01:34:22,960
thing has knowledge about geometry built into it. it knows which pixels inare near which other pixels

1415
01:34:23,000 --> 01:34:28,220
that's the best published results so far, although we've now got 5.3%

1416
01:34:28,220 --> 01:34:31,970
with another method that we haven't published yet.

1417
01:34:31,980 --> 01:34:33,980
on my little reduced norb,

1418
01:34:34,000 --> 01:34:38,030
if you do logistic regression on the raw pixels, you get 30% error

1419
01:34:38,080 --> 01:34:42,050
so looking at this and looking at this it's obvious you take all these numbers

1420
01:34:42,050 --> 01:34:45,790
and multiply by two-thirds, right? because that's what turns this into this. and that's obviously

1421
01:34:45,790 --> 01:34:47,600
what you lost by going down to this

1422
01:34:47,620 --> 01:34:52,040
so you should mentally multiply these by two-thirds.

1423
01:34:52,600 --> 01:34:55,550
if you do one hidden layer of a thousand units,

1424
01:34:55,550 --> 01:34:59,050
train greedily, and never fine-tuned, just greedy trained

1425
01:34:59,070 --> 01:35:01,510
and then you do a logistic regression,

1426
01:35:01,510 --> 01:35:02,980
you get fifteen percent.

1427
01:35:02,990 --> 01:35:04,640
if you do two hidden layers,

1428
01:35:04,700 --> 01:35:07,790
another hidden layer of a thousand units, you get down to 10%

1429
01:35:08,480 --> 01:35:12,430
i have to admit there were various different experiments went on and this was the best

1430
01:35:14,580 --> 01:35:18,940
but that's already better than a support vector machine on the full resolution data

1431
01:35:18,980 --> 01:35:21,970
now you might argue that, well, maybe i'm doing a lot of regularisation here

1432
01:35:22,130 --> 01:35:25,000
but that doesn't sort of fit with this very well

1433
01:35:25,070 --> 01:35:29,550
well, maybe it does, 'cause there aren't many parameters there.

1434
01:35:30,300 --> 01:35:33,610
this is just to demonstrate those kinds of units work nicely for

1435
01:35:33,630 --> 01:35:34,980
real valued data

1436
01:35:35,070 --> 01:35:38,130
and that greedy learning with no fine tuning

1437
01:35:38,200 --> 01:35:41,790
is already getting good features

1438
01:35:41,860 --> 01:35:48,450
if you were to back propagated, you'd now presumably do significantly better.

1439
01:35:48,450 --> 01:35:50,460
these are some of the features it learned

1440
01:35:50,500 --> 01:35:52,540
the first hidden layer of features

1441
01:35:52,550 --> 01:35:56,390
you can't see them very well here. there's little local sort of gabor-ish things

1442
01:35:56,430 --> 01:35:58,790
and there's some much bigger features like this guy that you might be able to

1443
01:35:58,790 --> 01:36:02,580
see, which is the sort of edge of a car

1444
01:36:02,710 --> 01:36:05,550
so with binary units, a feature--the most a feature can do is come

1445
01:36:05,550 --> 01:36:07,230
on and say hey

1446
01:36:07,230 --> 01:36:08,280
i'm i'm here

1447
01:36:08,290 --> 01:36:09,440
this feature

1448
01:36:09,440 --> 01:36:12,450
if you show the car that matches that

1449
01:36:12,490 --> 01:36:16,110
it won't just come on, it'll come on with a high level and say, hey hey i'm really really

1450
01:36:17,270 --> 01:36:20,620
so that's a bit like a template match or a partial template match

1451
01:36:21,190 --> 01:36:26,820
so you can have some units that are very informative

1452
01:36:26,880 --> 01:36:31,040
OK so that's one way of doing real values. here's a more standard way of doing real values

1453
01:36:31,080 --> 01:36:34,760
that's been used quite a lot

1454
01:36:35,500 --> 01:36:39,940
this is the value of a pixel, a real valued pixel let's say

1455
01:36:39,990 --> 01:36:41,930
and this is the

1456
01:36:44,410 --> 01:36:47,940
and what we're gonna do is have a parabolic containment function

1457
01:36:47,980 --> 01:36:51,820
so i have some bias for the pixel that says i'm minimum energy when i'm here

1458
01:36:51,880 --> 01:36:53,340
at this bias level

1459
01:36:53,390 --> 01:36:59,310
my energy goes up quadratically as i go away from there

1460
01:36:59,360 --> 01:37:02,660
and then the effect of the hidden units

1461
01:37:02,710 --> 01:37:06,810
the top down input that this pixel is receiving from the hidden units

1462
01:37:06,830 --> 01:37:10,210
is going to correspond to an energy gradient

1463
01:37:10,210 --> 01:37:11,910
because you have a term like this

1464
01:37:12,880 --> 01:37:16,640
take its derivative with respect to v i,

1465
01:37:16,690 --> 01:37:21,280
set this to one, take the derivative with respect to v i, and you get this product, right?

1466
01:37:23,120 --> 01:37:24,390
looks like this

1467
01:37:28,150 --> 01:37:33,240
so the minimum energy state for this pixel is where these two gradients balance

1468
01:37:33,240 --> 01:37:38,150
so as this term gets bigger it'll move out linearly, because the gradient of this thing increases

1469
01:37:39,910 --> 01:37:41,370
so essentially

1470
01:37:41,390 --> 01:37:44,830
you put this parabolic containment function on each pixel

1471
01:37:44,870 --> 01:37:48,290
and that guarantees things won't go crazy

1472
01:37:48,300 --> 01:37:52,490
and now you can write down an energy function. so this is the normal energy function

1473
01:37:53,320 --> 01:37:54,760
visible and hidden units

1474
01:37:54,760 --> 01:38:00,250
except that i scaled the visible activities by the standard deviation of this--

1475
01:38:00,260 --> 01:38:05,350
of the gaussian whose negative log is this parabola

1476
01:38:05,400 --> 01:38:08,660
because this is all in the log domain

1477
01:38:08,710 --> 01:38:09,980
and then

1478
01:38:10,090 --> 01:38:12,960
those terms to do with biases on the hiddens as usual

1479
01:38:12,960 --> 01:38:16,210
and there's this containment function which looks like that

1480
01:38:16,280 --> 01:38:17,910
so that's an energy function

1481
01:38:17,930 --> 01:38:19,580
and you can

1482
01:38:19,600 --> 01:38:23,700
run your markov chain just like before. you can activate the hidden--activating the hidden units is

1483
01:38:23,700 --> 01:38:26,380
just the same as before you just use real values times the weights.

1484
01:38:26,440 --> 01:38:28,330
then reconstructing

1485
01:38:28,360 --> 01:38:31,180
you use this picture. you get a top down input which is the sum of these

1486
01:38:34,280 --> 01:38:36,790
you can just set this to the energy minimum

1487
01:38:36,850 --> 01:38:38,890
and you can add some noise if you like

1488
01:38:38,950 --> 01:38:42,140
if you need--if you want to learn the variances, you factor out the noise. if

1489
01:38:42,140 --> 01:38:43,800
you just set all these to one

1490
01:38:43,970 --> 01:38:46,770
you can be deterministic when you reconstruct

1491
01:38:46,790 --> 01:38:48,430
and that works very nicely

1492
01:38:48,450 --> 01:38:51,850
it's slow. you need to use a learning rate that's like a hundred times slower

1493
01:38:51,850 --> 01:38:54,970
than for binary units which many people don't understand

1494
01:38:55,140 --> 01:38:59,470
so many people have published papers saying this doesn't work and we got lousy results and blah blah blah. actually you just

1495
01:38:59,470 --> 01:39:01,300
use much a more smaller learning rate

1496
01:39:01,320 --> 01:39:03,260
so here's a student of mine

1497
01:39:03,310 --> 01:39:06,600
who's very determined called alex krizhevsky

1498
01:39:07,800 --> 01:39:10,370
trained a hundred thousand

1499
01:39:10,380 --> 01:39:11,930
binary filters

1500
01:39:11,930 --> 01:39:19,620
so the idea was similar to reconstruct this organisation hierarchy from social network data

1501
01:39:19,620 --> 01:39:25,260
and so basically we were having we had communication records

1502
01:39:26,200 --> 01:39:33,220
at the time roughly seven hundred seventy just people and so probability that would be

1503
01:39:33,220 --> 01:39:38,370
roughly were able to reconstruct the organisational structure so what we did

1504
01:39:38,370 --> 01:39:42,320
so the whole approach was roughly five steps

1505
01:39:42,340 --> 01:39:45,320
first we started with the minister lotfi seven show

1506
01:39:45,340 --> 01:39:50,160
one example of this law applies lot lines from log files in the next slide

1507
01:39:50,160 --> 01:39:51,070
then b

1508
01:39:51,070 --> 01:39:56,550
clean this we got email transactions with section was written a sense person b so

1509
01:39:56,550 --> 01:40:00,720
we had just who sent email to whom and at what time does not know

1510
01:40:02,200 --> 01:40:07,840
out of this we construct the graph and this graph OK was transformed into sparse

1511
01:40:07,840 --> 01:40:09,820
matrix and then we we

1512
01:40:09,870 --> 01:40:13,470
they can assist institutional taxonomy ontology

1513
01:40:13,490 --> 01:40:22,780
now the data which we used so it should be used the data from email

1514
01:40:22,780 --> 01:40:28,370
spam filter software so somehow spam was already removed by the software itself and since

1515
01:40:28,370 --> 01:40:31,490
we using the log files directly from this suffering was

1516
01:40:31,510 --> 01:40:37,910
pretty easy and in doing so we we had information assets owned by sender and

1517
01:40:37,950 --> 01:40:45,070
receiver send this is an example of successful email section was about john davies

1518
01:40:45,090 --> 01:40:46,950
the correctly spelled

1519
01:40:47,030 --> 01:40:57,370
this time it was automatically generated to me at a certain point

1520
01:40:57,390 --> 01:41:00,300
maybe if you can there but you can check

1521
01:41:03,360 --> 01:41:05,200
we had

1522
01:41:06,930 --> 01:41:09,720
something like this up

1523
01:41:09,720 --> 01:41:12,260
the metadata for

1524
01:41:12,280 --> 01:41:16,030
from september fifteen two thousand three

1525
01:41:16,050 --> 01:41:18,590
march two thousand five roughly two

1526
01:41:18,620 --> 01:41:20,410
four four year in

1527
01:41:21,010 --> 01:41:25,780
initially we had just to give you a feeling for initially had on was thinking

1528
01:41:25,780 --> 01:41:27,030
about the data

1529
01:41:27,070 --> 01:41:34,640
after filtering out successful human sections it remained just five hundred sixty four megabytes and

1530
01:41:34,680 --> 01:41:41,200
these are often included two point seven successfully email section for further processing

1531
01:41:43,100 --> 01:41:45,510
so the whole dataset

1532
01:41:45,680 --> 01:41:49,340
references to forty five thousand email addresses

1533
01:41:49,390 --> 01:41:58,390
so this spammers of course joking and after cleaning

1534
01:41:58,450 --> 01:42:03,530
cleaning means that we removed all the email addresses which appeared less than ten times

1535
01:42:03,530 --> 01:42:04,680
so this was

1536
01:42:04,700 --> 01:42:11,720
good heuristic to remove all everything what was not really useful it remains seventeen thousand

1537
01:42:11,720 --> 01:42:18,310
email addresses of each seven hundred seventy million addresses were internal ones and so you

1538
01:42:18,890 --> 01:42:20,010
see this

1539
01:42:20,030 --> 01:42:26,300
almost eight hundred people communicated with roughly sixteen thousand other people so is

1540
01:42:26,340 --> 01:42:31,910
graphic communication records of our institution probably other institutions will have the same thing same

1541
01:42:31,910 --> 01:42:35,030
style of proportion

1542
01:42:35,930 --> 01:42:42,410
OK and so what got out of this so then we transform this graph into

1543
01:42:42,430 --> 01:42:43,990
the the for

1544
01:42:43,990 --> 01:42:50,390
some kind of analytic representation and we did clustering on top of it and what

1545
01:42:50,390 --> 01:42:52,070
it came out so

1546
01:42:54,220 --> 01:42:57,950
so this graph shows the set of clusters

1547
01:42:58,220 --> 01:43:03,890
hierarchical clusters and we can actually see that the structure of our institute so all

1548
01:43:03,890 --> 01:43:06,450
the email addresses related to

1549
01:43:06,950 --> 01:43:08,660
department of physics

1550
01:43:08,680 --> 01:43:12,700
would be kind of more closely connected then let's say connected to

1551
01:43:12,720 --> 01:43:14,890
maybe chemistry are two

1552
01:43:14,910 --> 01:43:19,450
electronic departments which would be one computer high-tech department

1553
01:43:19,550 --> 01:43:20,910
and within

1554
01:43:20,930 --> 01:43:26,140
well i think departments you would have obvious difference between different

1555
01:43:26,390 --> 01:43:32,450
systems controlled a i conferences communications and even AI we have a couple of department

1556
01:43:32,450 --> 01:43:33,260
store groups

1557
01:43:33,320 --> 01:43:37,220
we would see subgroup so basically this graph really

1558
01:43:37,220 --> 01:43:43,430
if we perform e simple not too complex

1559
01:43:43,700 --> 01:43:45,930
clustering on top of it then we

1560
01:43:45,930 --> 01:43:46,800
you can see

1561
01:43:48,160 --> 01:43:50,390
how this gets

1562
01:43:50,490 --> 01:43:52,220
split in into two

1563
01:43:52,240 --> 01:43:54,820
organizational structure of the institution

1564
01:43:54,840 --> 01:43:58,760
so another visualisation of the graph would be the following

1565
01:43:58,800 --> 01:44:03,620
so maybe you can see this this is this two d visualization there

1566
01:44:05,140 --> 01:44:09,530
a couple of mountains here we have three months this one this one and

1567
01:44:09,570 --> 01:44:11,590
this one and if the

1568
01:44:11,600 --> 01:44:12,890
number so each

1569
01:44:12,910 --> 01:44:16,720
i here would be one person in if we have more than area then we

1570
01:44:16,720 --> 01:44:18,240
would get

1571
01:44:18,430 --> 01:44:23,010
mountain and of higher mountain and we would nicely see

1572
01:44:23,620 --> 01:44:30,800
scientific sets of departments and administration in the middle which is connecting point connecting point

1573
01:44:31,450 --> 01:44:33,450
other people

1574
01:44:34,820 --> 01:44:39,340
this is one example which is a little bit related cool

1575
01:44:39,370 --> 01:44:43,530
x two and so on

1576
01:44:51,260 --> 01:44:54,410
the graph from the document

1577
01:44:54,490 --> 01:44:57,890
email logs from the middle so this means that

1578
01:44:57,930 --> 01:44:59,950
people from

1579
01:44:59,970 --> 01:45:05,030
chemistry would communicate with the administration of physical laws of physics would also think that

1580
01:45:05,390 --> 01:45:10,410
there is no content involved here

1581
01:45:11,710 --> 01:45:17,200
in this log file they were just set the recipient

1582
01:45:17,280 --> 01:45:21,950
i don't have any additional features

1583
01:45:21,950 --> 01:45:25,160
so i just a few more slides on this MSN

1584
01:45:25,160 --> 01:45:26,590
but is this would be

1585
01:45:28,450 --> 01:45:32,570
for relaxation at the end so this is just simulation which i won't go into

1586
01:45:32,570 --> 01:45:33,970
much into this

1587
01:45:33,970 --> 01:45:40,070
how complex clusters can come out and so this would be classed departments and clusters

1588
01:45:40,070 --> 01:45:41,590
and we can see that

1589
01:45:41,600 --> 01:45:46,260
basically a clustering was able to isolate very well

1590
01:45:46,300 --> 01:45:50,120
the existing departments

1591
01:45:50,140 --> 01:45:56,300
OK so the last topic is analysis of MSN messenger communication network services were done

1592
01:45:56,300 --> 01:46:01,220
by unesco it's last year when he was interned there

1593
01:46:03,870 --> 01:46:06,720
what kind of data that he get

1594
01:46:07,490 --> 01:46:11,360
for every conversation on MSN messenger

1595
01:46:11,370 --> 01:46:17,530
he had a list of users specific participated in the conversation and in principle there

1596
01:46:17,530 --> 01:46:21,300
could go somewhere people people on the conversation but

1597
01:46:21,360 --> 01:46:26,820
this is mostly they are just doing what preach conversation we have and for each

1598
01:46:26,820 --> 01:46:29,320
so if see all familiar

1599
01:46:29,340 --> 01:46:30,780
wz p

1600
01:46:30,800 --> 01:46:36,280
so we now instantiate w therapy for non rigid non rigid shape

1601
01:46:36,420 --> 01:46:39,370
it's just this main shapes s

1602
01:46:40,370 --> 01:46:43,550
a linear combination of these basis

1603
01:46:43,570 --> 01:46:45,320
well we're we pk

1604
01:46:45,320 --> 01:46:48,070
it is the thing i'm trying to solve

1605
01:46:48,090 --> 01:46:51,120
and i have the shape parameters here

1606
01:46:51,160 --> 01:46:56,390
now i artificially i can artificially insert

1607
01:46:56,410 --> 01:47:01,840
similarity is similarity transform here quite easily so at my talk and non rigid shape

1608
01:47:01,840 --> 01:47:03,830
but you can incorporate

1609
01:47:05,700 --> 01:47:07,590
so you can actually have

1610
01:47:07,620 --> 01:47:10,520
and i can think that's represent translation

1611
01:47:10,770 --> 01:47:12,200
in x and y

1612
01:47:12,230 --> 01:47:14,730
scale and rotation in

1613
01:47:15,120 --> 01:47:18,800
so that adds an extra four argument is that and that's what's quite simple to

1614
01:47:19,950 --> 01:47:22,410
so you can just to think of it as a linear

1615
01:47:22,420 --> 01:47:25,660
this is the linear model against the linear shape model

1616
01:47:25,740 --> 01:47:26,550
and so

1617
01:47:26,550 --> 01:47:31,270
so you so we have the construction of appearance model

1618
01:47:31,320 --> 01:47:35,150
and exactly what i'm showing that sort of god

1619
01:47:35,160 --> 01:47:39,710
the actual image on the source source image i will to the main shape so

1620
01:47:39,710 --> 01:47:44,380
rather than removing just the similarity although

1621
01:47:44,510 --> 01:47:49,870
richard what sorry i actually what this to the main shape surface s and that's

1622
01:47:49,870 --> 01:47:54,820
what i did PCA PCA on that appearance not on the non rigid shape

1623
01:47:57,990 --> 01:48:02,680
so but to shameless textures and then you can output that so

1624
01:48:02,730 --> 01:48:04,540
it's like an example of the

1625
01:48:04,550 --> 01:48:15,350
this is that it can become very well

1626
01:48:15,370 --> 01:48:19,570
this is a little bit of illumination change their

1627
01:48:19,570 --> 01:48:21,680
map opens kind of guy

1628
01:48:21,820 --> 01:48:22,710
coming out there

1629
01:48:22,730 --> 01:48:26,770
but essentially that's that's the variation in the shape

1630
01:48:26,790 --> 01:48:31,090
and and then again very very similar so actually the functional form of of the

1631
01:48:31,090 --> 01:48:35,850
shape and appearance models are exactly the same with PCA on appearance with PCA and

1632
01:48:38,040 --> 01:48:40,600
unable to develop my parents are all

1633
01:48:40,610 --> 01:48:45,080
and the template is that the mean appearance plus

1634
01:48:45,230 --> 01:48:47,190
variations in the shape

1635
01:48:47,200 --> 01:48:50,510
so it's exactly the same in microarray experiments only

1636
01:48:50,510 --> 01:48:55,890
so that's how i close the problem in concurrency from the previous the previous lots

1637
01:48:55,890 --> 01:48:58,360
of galaxies that have been going over

1638
01:48:58,380 --> 01:49:05,230
that it's quite simple then actually sold land and people

1639
01:49:05,280 --> 01:49:08,700
so this is the full idea of what's going on

1640
01:49:08,750 --> 01:49:14,660
we click on hollywood points thousands if not millions of points with process analysis so

1641
01:49:14,670 --> 01:49:18,580
that we know that we're really worrying about non rigid shape

1642
01:49:18,600 --> 01:49:21,510
we then PCA on the non rigid shape and

1643
01:49:21,530 --> 01:49:25,420
they're kind of more advanced techniques then the pedia now you can do things so

1644
01:49:25,820 --> 01:49:27,410
obviously cover

1645
01:49:27,470 --> 01:49:28,780
and we do this

1646
01:49:28,790 --> 01:49:33,860
a probabilistic PCA interpretation of this so kind can regularize for how much your face

1647
01:49:33,860 --> 01:49:35,730
can vary the things that so

1648
01:49:35,750 --> 01:49:36,530
let's say

1649
01:49:36,580 --> 01:49:40,030
i want to make sure that my face variations stays within possum minus three standard

1650
01:49:40,030 --> 01:49:42,380
deviations of original shape

1651
01:49:42,560 --> 01:49:47,890
and similarly by the PCA on my appearance models and again so this is the

1652
01:49:47,910 --> 01:49:51,140
linear shape model but the magic comes with

1653
01:49:51,190 --> 01:49:58,320
putting the two together and how do i fit

1654
01:50:03,410 --> 01:50:06,100
so when i have a

1655
01:50:06,360 --> 01:50:09,880
incoming power

1656
01:50:10,100 --> 01:50:14,670
so when i want to instantiate model so i've got the appearance here

1657
01:50:16,570 --> 01:50:18,470
and i know what my

1658
01:50:18,480 --> 01:50:23,450
so values so a lot of the first argued that the organ value of god

1659
01:50:23,730 --> 01:50:27,750
three thousand five hundred fifty nine three one to megaton fifty six

1660
01:50:27,760 --> 01:50:30,320
unable to put together the shape

1661
01:50:30,320 --> 01:50:35,430
the smallest and i determine our rate of convergence in the norm anyway OK

1662
01:50:39,250 --> 01:50:45,370
so one thing that happens in this case then this is we are looking at

1663
01:50:45,380 --> 01:50:47,500
so our matrix are semi definite

1664
01:50:47,520 --> 01:50:52,790
right positive semi definite definition simply says that for all x not equal to zero

1665
01:50:52,790 --> 01:50:55,930
august i don't need that here x transpose a x is greater than equal to

1666
01:50:56,910 --> 01:51:02,430
OK so i standard definition that makes all the analysis works here is that we're

1667
01:51:02,430 --> 01:51:03,950
interested in the support

1668
01:51:03,990 --> 01:51:05,790
of a by b

1669
01:51:05,790 --> 01:51:09,620
and this is just simply i want to minimize

1670
01:51:09,630 --> 01:51:14,850
i want the minimum lambda such that lambda b-minus positive definite

1671
01:51:14,900 --> 01:51:20,220
so what this means in terms of electrical circuits it says that i have two

1672
01:51:20,220 --> 01:51:23,120
electrical circuits one a and one b

1673
01:51:23,130 --> 01:51:26,470
and what i want to know is how many copies of the electrical circuit the

1674
01:51:26,470 --> 01:51:30,760
do i need in order to be able to carry as much current for all

1675
01:51:30,760 --> 01:51:32,470
experiments is a

1676
01:51:34,000 --> 01:51:37,800
good on the condition number here just the product of these things

1677
01:51:37,800 --> 01:51:41,720
so what ends up happening now is from electrical point of view what we're going

1678
01:51:41,720 --> 01:51:43,310
to do the viewer two

1679
01:51:43,320 --> 01:51:45,340
graph electrical circuits

1680
01:51:45,410 --> 01:51:48,190
and what we're going to want to know is how do we then

1681
01:51:49,750 --> 01:51:54,260
a might be that support b by a and that's going to determine our own

1682
01:51:56,260 --> 01:51:59,890
good and so how do you do that classic thing here is is using what

1683
01:51:59,890 --> 01:52:01,770
are called path embedding

1684
01:52:01,890 --> 01:52:04,510
so suppose we have two graphs

1685
01:52:04,540 --> 01:52:07,760
with vertices v gmbh

1686
01:52:07,820 --> 01:52:12,900
the path embedding is simply a map from the edges of g

1687
01:52:12,910 --> 01:52:17,870
to pass in one of them but passive h

1688
01:52:17,920 --> 01:52:22,340
such that the big the path begins at the end of the j

1689
01:52:23,290 --> 01:52:26,480
so let's just a small example here

1690
01:52:26,500 --> 01:52:32,600
so i suppose in this example now we have two graphs one is a complete

1691
01:52:33,770 --> 01:52:36,750
and the other graph is therefore cycle

1692
01:52:36,770 --> 01:52:40,830
so suppose we have our four cycle here

1693
01:52:40,840 --> 01:52:42,770
in a dream

1694
01:52:42,800 --> 01:52:47,490
and then the red graph here is actually a k four

1695
01:52:47,520 --> 01:52:52,650
so there's a natural path embedding namely this red ed will simply map mapped to

1696
01:52:52,650 --> 01:52:56,960
the screen in this red and will map to the screen edge this one here

1697
01:52:56,960 --> 01:53:00,830
to to here but this essentially here doesn't exist

1698
01:53:00,830 --> 01:53:04,730
in the original four cycle so why don't we just map that the path to

1699
01:53:04,730 --> 01:53:07,580
use this screen and then the screen

1700
01:53:07,630 --> 01:53:09,390
at the same time when we map

1701
01:53:09,400 --> 01:53:14,610
this redpath here to these two bring edges so the net effect now

1702
01:53:18,290 --> 01:53:19,330
so i guess

1703
01:53:19,390 --> 01:53:21,040
slide here so

1704
01:53:21,050 --> 01:53:24,980
so in the in this example then what happens

1705
01:53:25,050 --> 01:53:30,100
is the critical thing that makes all this analysis work then is the following

1706
01:53:30,110 --> 01:53:31,620
and that is

1707
01:53:31,640 --> 01:53:33,650
in this example

1708
01:53:33,690 --> 01:53:36,660
there's going to be congestion

1709
01:53:36,750 --> 01:53:40,790
have so the congestion in this case then it's going to be what

1710
01:53:42,680 --> 01:53:47,810
this enzyme is used by three pass straight namely the search

1711
01:53:47,820 --> 01:53:51,070
this engine genus right in the original k four

1712
01:53:51,090 --> 01:53:55,100
so the congestion here and that seems to be the worst right this has congestion two

1713
01:53:56,970 --> 01:54:00,580
and to so the top the worst congestion is three

1714
01:54:00,590 --> 01:54:02,900
the dilation here

1715
01:54:02,910 --> 01:54:04,950
in this case well

1716
01:54:04,970 --> 01:54:09,810
this red edges rap one and the same length so it's not dilated all right

1717
01:54:09,850 --> 01:54:14,160
this said here this red one is mapped a path of length two

1718
01:54:14,180 --> 01:54:16,540
so the dilation is two

1719
01:54:16,550 --> 01:54:20,640
so a natural thing then this is that the congestion time dilation in this case

1720
01:54:20,640 --> 01:54:22,770
is six

1721
01:54:22,800 --> 01:54:24,930
so what i want to claim then

1722
01:54:25,010 --> 01:54:27,830
and the set here in this slide

1723
01:54:27,830 --> 01:54:30,460
it's just check see

1724
01:54:33,180 --> 01:54:38,630
now let's go back

1725
01:54:38,800 --> 01:54:45,980
so if i go back so i guess that's missing here so any other slide

1726
01:54:46,060 --> 01:54:49,880
so this is the so so in this case here than a standard there and

1727
01:54:49,880 --> 01:54:53,310
then it it's is not hard to show is that the support then of k

1728
01:54:54,880 --> 01:54:58,590
by the cycle of size four than it is

1729
01:54:59,930 --> 01:55:01,330
o is bounded by

1730
01:55:05,480 --> 01:55:07,660
and so so therefore

1731
01:55:07,680 --> 01:55:10,240
and on the other hand clearly the support

1732
01:55:10,250 --> 01:55:11,570
c four

1733
01:55:11,570 --> 01:55:16,150
by k four sentences subgraph just to the path embedding is unit one is less

1734
01:55:16,150 --> 01:55:17,430
than or equal to one

1735
01:55:17,450 --> 01:55:19,980
so this says that the condition number

1736
01:55:19,980 --> 01:55:23,460
the class for k four

1737
01:55:23,480 --> 01:55:25,920
and the class c and c four

1738
01:55:25,940 --> 01:55:28,830
that is less than or equal to six

1739
01:55:30,540 --> 01:55:33,250
c is the cycle of length four

1740
01:55:33,270 --> 01:55:34,460
OK thank you

1741
01:55:34,480 --> 01:55:39,310
OK so so that says and is that in this particular case

1742
01:55:39,390 --> 01:55:43,770
that if we precondition k four with a four cycle

1743
01:55:43,790 --> 01:55:47,310
we expect to have to do about six iterations for better if we did the

1744
01:55:47,310 --> 01:55:48,500
naive method

1745
01:55:48,520 --> 01:55:51,920
and if we did conjugate gradient then we need to do

1746
01:55:51,940 --> 01:55:55,730
square root of six iterations but of course is the constants and the right

1747
01:55:57,120 --> 01:56:02,790
so this is the technology than that used to be able to analyse one quality

1748
01:56:04,580 --> 01:56:06,120
precondition to another

1749
01:56:10,390 --> 01:56:15,210
so i guess what i'd like to do actually is just talk about historically what's

1750
01:56:15,210 --> 01:56:17,810
happened in terms of solvers

1751
01:56:17,830 --> 01:56:20,000
for the

1752
01:56:20,040 --> 01:56:23,350
the plane or case still classic place for which

1753
01:56:23,370 --> 01:56:26,000
elisa theoreticians have cut their teeth

1754
01:56:26,000 --> 01:56:27,080
and so

1755
01:56:27,100 --> 01:56:29,670
if i go back to linear so to say well

1756
01:56:29,670 --> 01:56:34,020
prior to the fifties nothing was known as you could say that run time with

