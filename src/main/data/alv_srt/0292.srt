1
00:00:00,000 --> 00:00:02,560
but this omega is the same as that one

2
00:00:03,250 --> 00:00:07,000
of course otherwise it would be in normal mode we're going to search for the

3
00:00:07,000 --> 00:00:08,650
normal modes

4
00:00:08,670 --> 00:00:11,380
and in the normal mode

5
00:00:11,440 --> 00:00:12,710
the frequencies

6
00:00:12,750 --> 00:00:16,960
must be the same of all objects whether you have five or six or two

7
00:00:16,960 --> 00:00:18,360
or three

8
00:00:18,420 --> 00:00:20,020
but the ratios

9
00:00:21,170 --> 00:00:24,670
that's a different issue however there in phase i and phase

10
00:00:25,540 --> 00:00:27,730
the ratios can be negative

11
00:00:27,770 --> 00:00:29,060
on fifth

12
00:00:29,110 --> 00:00:33,420
but you never have any phase angles other than one hundred eighty degrees or zero

13
00:00:33,420 --> 00:00:36,480
because we have no damping we've taken them

14
00:00:36,500 --> 00:00:37,800
so it is essential

15
00:00:37,820 --> 00:00:40,710
when you substitute that newton's second law

16
00:00:40,730 --> 00:00:43,110
these omega psi the same because they

17
00:00:44,190 --> 00:00:46,790
if you the

18
00:00:46,800 --> 00:00:48,770
normal modes

19
00:00:48,770 --> 00:00:55,630
OK you're ready

20
00:00:55,650 --> 00:00:59,460
i'm going to offset the first one

21
00:00:59,500 --> 00:01:01,420
of distance

22
00:01:01,480 --> 00:01:02,710
x y

23
00:01:04,610 --> 00:01:06,590
i don't have mass and

24
00:01:06,630 --> 00:01:07,840
they have length l

25
00:01:07,860 --> 00:01:09,670
to remind you

26
00:01:09,670 --> 00:01:12,940
and we are going to have omega zero screening

27
00:01:12,980 --> 00:01:15,020
because g over l

28
00:01:15,130 --> 00:01:17,290
and omega s

29
00:01:17,300 --> 00:01:20,170
because scale

30
00:01:20,230 --> 00:01:23,590
this one

31
00:01:23,610 --> 00:01:25,210
i offset

32
00:01:25,230 --> 00:01:28,130
over the distance x to notice

33
00:01:28,170 --> 00:01:34,980
you don't have to do that always offset them in the same direction

34
00:01:35,020 --> 00:01:37,000
and then there's this spring

35
00:01:37,040 --> 00:01:39,820
that connect them

36
00:01:39,860 --> 00:01:41,460
i to make it very

37
00:01:41,480 --> 00:01:43,650
in the spring otherwise

38
00:01:43,690 --> 00:01:47,880
the picture becomes a little bit too complicated

39
00:01:48,000 --> 00:01:52,040
that this angle b theta one

40
00:01:52,090 --> 00:01:55,110
and this angle be theta two

41
00:01:55,170 --> 00:01:57,360
so we have here

42
00:01:58,020 --> 00:02:02,880
you and g

43
00:02:02,920 --> 00:02:04,840
reviewed the pension

44
00:02:04,840 --> 00:02:07,480
roughly ng for small angles

45
00:02:07,480 --> 00:02:09,210
and we have the attention

46
00:02:09,230 --> 00:02:11,750
which is roughly and g

47
00:02:11,790 --> 00:02:16,500
but to make the way i drew it but that's not so important now

48
00:02:18,520 --> 00:02:21,360
there is another force that acts on both

49
00:02:21,400 --> 00:02:23,710
and which forces

50
00:02:23,750 --> 00:02:25,940
that's spring force

51
00:02:27,770 --> 00:02:28,940
do we agree

52
00:02:29,000 --> 00:02:33,630
what the magnitude of the spring forces

53
00:02:33,670 --> 00:02:39,570
the magnitude and we'll argue about the direction

54
00:02:39,630 --> 00:02:41,690
we call magnitude

55
00:02:41,770 --> 00:02:52,440
look very closely offset one by x two and i offset order by x one

56
00:02:52,440 --> 00:02:56,230
i was asking you a question you can answer quiz question

57
00:02:58,630 --> 00:03:04,340
the magnitude of that force

58
00:03:06,360 --> 00:03:08,110
and x two

59
00:03:08,290 --> 00:03:10,190
is x one

60
00:03:10,190 --> 00:03:12,980
non-negotiable right

61
00:03:13,040 --> 00:03:16,230
now if x two is larger than x

62
00:03:16,270 --> 00:03:19,590
we agree that the spring force is in this direction

63
00:03:19,610 --> 00:03:27,590
and we agree then that this spring force most this direction

64
00:03:27,710 --> 00:03:30,170
x two is large the next

65
00:03:30,190 --> 00:03:34,730
well that means i can leave everything the way it is now

66
00:03:34,730 --> 00:03:37,980
as long as i give this a minus sign it is the sign of OK

67
00:03:37,980 --> 00:03:40,210
because if x two

68
00:03:40,270 --> 00:03:41,630
smaller than x one

69
00:03:41,770 --> 00:03:43,500
automatically for both

70
00:03:43,520 --> 00:03:46,210
my older rise fine

71
00:03:46,360 --> 00:03:49,270
therefore we

72
00:03:49,270 --> 00:03:52,980
in my head i can think of x two being larger than x one set

73
00:03:52,980 --> 00:03:56,650
of the difference equations and i no longer have to worry

74
00:03:56,710 --> 00:03:58,860
about the fact that maybe x two

75
00:03:58,920 --> 00:04:00,860
at certain moment in time is not

76
00:04:00,920 --> 00:04:03,540
larger than x one

77
00:04:03,540 --> 00:04:08,020
so now i set up the differential equations nx one double dot

78
00:04:08,040 --> 00:04:12,570
but as this one

79
00:04:12,590 --> 00:04:15,400
let's first dependent

80
00:04:15,480 --> 00:04:17,980
there's minus

81
00:04:18,000 --> 00:04:21,610
t times the sign of state i one and to use and

82
00:04:21,610 --> 00:04:24,210
so is minus and g

83
00:04:24,250 --> 00:04:25,560
time x one

84
00:04:25,570 --> 00:04:27,650
divided by l

85
00:04:27,690 --> 00:04:31,420
we agreed is that horizontal component here

86
00:04:31,420 --> 00:04:36,670
the horizontal component of these driving it back to equilibrium minus sign

87
00:04:37,420 --> 00:04:40,860
as the spring force is driving it away from equilibrium

88
00:04:40,920 --> 00:04:42,230
so it's going to get

89
00:04:46,060 --> 00:04:47,230
x two

90
00:04:48,150 --> 00:04:50,380
x one

91
00:04:50,480 --> 00:04:52,440
look at this

92
00:04:54,270 --> 00:04:56,690
very important now

93
00:04:56,770 --> 00:04:59,040
very important

94
00:04:59,130 --> 00:05:00,710
and the next one

95
00:05:00,770 --> 00:05:02,570
annex two double dot

96
00:05:02,570 --> 00:05:06,130
because of them

97
00:05:06,130 --> 00:05:07,310
the objects

98
00:05:07,320 --> 00:05:09,480
it's the grouper is empty

99
00:05:09,540 --> 00:05:11,350
the object is not heavy

100
00:05:11,420 --> 00:05:13,940
and i'm standing to the object

101
00:05:13,960 --> 00:05:17,070
now i might say well i think i can pick up the

102
00:05:17,070 --> 00:05:18,950
objective all those things are true

103
00:05:19,030 --> 00:05:21,640
and the other not glued to the take

104
00:05:21,670 --> 00:05:24,600
OK i can pick out the objective all of those things are true

105
00:05:24,660 --> 00:05:28,400
and the things like two degrees is going to get some sleep

106
00:05:28,410 --> 00:05:31,690
i can pick up an object if all of those things true you can think

107
00:05:31,690 --> 00:05:34,170
of a million sort of qualifications to

108
00:05:34,180 --> 00:05:37,690
when it's possible to pick up an object

109
00:05:37,730 --> 00:05:42,560
so the qualification problem is this problem of trying to figure out all the conditions

110
00:05:42,560 --> 00:05:45,220
under which you could possibly be able to pick

111
00:05:45,260 --> 00:05:46,620
the object

112
00:05:47,870 --> 00:05:51,660
four which it's possible to perform that action and that in itself might be quite

113
00:05:53,470 --> 00:05:56,480
OK now the ramification problem so

114
00:05:56,490 --> 00:06:00,820
which we usually distinguish between the direct effects of actions of the direct effects of

115
00:06:01,270 --> 00:06:04,920
action and i go over to the lights which were it might be

116
00:06:04,920 --> 00:06:06,760
and i flicked the switch

117
00:06:06,780 --> 00:06:10,470
so the direct effect of the action is that changes the position of the switch

118
00:06:10,470 --> 00:06:12,360
from being down to y

119
00:06:13,500 --> 00:06:15,020
but there are another

120
00:06:15,030 --> 00:06:20,450
what we might call indirect effects or ramifications differences lights my camera

121
00:06:21,000 --> 00:06:24,000
you know the white board the screen might come down

122
00:06:24,890 --> 00:06:29,510
the core ramification problem is this problem of not just working out what the direct

123
00:06:29,510 --> 00:06:32,950
effects of the action of the what all the ramifications are in direct effects of

124
00:06:36,190 --> 00:06:38,930
and they can be quite difficult

125
00:06:38,930 --> 00:06:42,390
OK now back to the frame problem how can to solve so before we solve

126
00:06:42,410 --> 00:06:46,310
let's figure out what would qualify as the solution

127
00:06:47,530 --> 00:06:56,320
what we got is described as system in these terms as initial state preconditions effects

128
00:06:56,570 --> 00:07:01,170
what we want is a way of taking that cranking handle spinning out these frame

129
00:07:01,170 --> 00:07:05,640
axioms automatically rather than us having to come up with we need an additional assumption

130
00:07:05,640 --> 00:07:10,320
that assumption is that if the effects of the action say nothing about that particular

131
00:07:11,190 --> 00:07:18,420
are you know that particular point sorry then that particular flight should retain its current

132
00:07:18,460 --> 00:07:19,790
OK now

133
00:07:19,830 --> 00:07:23,290
what we want is we want scheme that's going to be concise and the reason

134
00:07:23,290 --> 00:07:26,540
we want that is that what we need these axioms

135
00:07:26,560 --> 00:07:28,670
we have to write them down

136
00:07:28,720 --> 00:07:34,240
they're not automatically given to us not in any logical sense deductive sense from the

137
00:07:34,240 --> 00:07:36,280
actions so far

138
00:07:36,290 --> 00:07:40,560
furthermore if we able to generate them automatically rather than having me write down

139
00:07:40,600 --> 00:07:45,180
then this list chance making error

140
00:07:45,240 --> 00:07:50,090
furthermore if i'd make a slight change to the effects again you actually remember there

141
00:07:50,090 --> 00:07:53,890
was another effective action had to write down so that he

142
00:07:53,930 --> 00:07:57,450
and i can read compiled this thing and get my new

143
00:07:59,680 --> 00:08:02,270
and the last method which i haven't put down there is if you like me

144
00:08:02,280 --> 00:08:03,430
really lazy

145
00:08:03,430 --> 00:08:05,250
then you don't want to see there trying to

146
00:08:05,310 --> 00:08:08,620
figure out all the things that don't change when actions

147
00:08:08,740 --> 00:08:16,940
so we better to some automatic way of doing

148
00:08:17,360 --> 00:08:20,790
i can get come to understand a few other bits of terminology and so on

149
00:08:20,790 --> 00:08:22,930
before i get to

150
00:08:22,970 --> 00:08:25,320
looking at the particular solution the frame problem

151
00:08:25,410 --> 00:08:27,710
now we usually distinguish two

152
00:08:27,770 --> 00:08:30,250
reasoning problems in the situation calculus

153
00:08:30,260 --> 00:08:34,090
the first is this problem here is that if i take all of the actions

154
00:08:34,130 --> 00:08:36,480
he all my description of

155
00:08:36,540 --> 00:08:39,630
you know the state of the physics of the domain

156
00:08:39,640 --> 00:08:41,480
which are called chemotherapy

157
00:08:41,540 --> 00:08:46,710
i should be able to work out whether a particular floor for particular formula

158
00:08:46,720 --> 00:08:49,980
is true after performing a sequence of actions they want

159
00:08:50,030 --> 00:08:53,440
and that's called the prediction problem just figuring out what's true

160
00:08:53,450 --> 00:08:57,150
after you put down after you perform sequence of actions

161
00:08:57,150 --> 00:08:59,510
given some description of the

162
00:08:59,590 --> 00:09:02,400
now the same

163
00:09:02,440 --> 00:09:06,210
now the other one is well it's OK to be able to talk about what

164
00:09:06,210 --> 00:09:09,270
might be true after performing these actions but we also have to reason about whether

165
00:09:09,270 --> 00:09:10,450
it's actually

166
00:09:10,470 --> 00:09:15,290
possible to perform that sequence maybe there is some action that i can perform that

167
00:09:15,380 --> 00:09:19,540
called the legality problems and essentially what i will do it will take

168
00:09:19,630 --> 00:09:22,030
the initial situation to be legal

169
00:09:22,150 --> 00:09:27,930
and in any other situation is going to be legal precisely when the current situation

170
00:09:27,930 --> 00:09:28,730
is legal

171
00:09:28,830 --> 00:09:32,780
and it's possible to perform a particular action in that situation the next will be

172
00:09:32,780 --> 00:09:34,580
legal as well

173
00:09:34,660 --> 00:09:35,690
again we need to

174
00:09:35,720 --> 00:09:40,100
the reason about about the projection problem about what's true in particular

175
00:09:40,210 --> 00:09:43,280
situation after performing these actions

176
00:09:44,290 --> 00:09:47,460
we need to be able to also work it would be able to determine whether

177
00:09:47,460 --> 00:09:53,330
it's actually possible to execute a sequence of actions

178
00:09:53,430 --> 00:09:55,980
all right let's try and work out how to

179
00:09:57,630 --> 00:10:00,140
had had automatically create these

180
00:10:03,750 --> 00:10:07,240
OK so let's have a look at this example above you were given some positive

181
00:10:07,240 --> 00:10:08,580
effects for four

182
00:10:08,580 --> 00:10:12,620
so if something is fragile then x this object x is going to be broken

183
00:10:12,620 --> 00:10:15,350
up the robot does the drop action

184
00:10:15,370 --> 00:10:18,870
if this upon the the next to the object x

185
00:10:19,230 --> 00:10:23,980
then x is going to be broken after the to explore the action

186
00:10:23,980 --> 00:10:26,890
so most of these examples of violence for other

187
00:10:26,910 --> 00:10:30,290
if they're not usually knowledge

188
00:10:30,290 --> 00:10:32,660
OK so what we can do is we're just going to rewrite them in this

189
00:10:32,660 --> 00:10:37,010
particular form and the particular form is fairly straightforward what we want is basically all

190
00:10:37,660 --> 00:10:41,790
the antecedent sort of stuff on the left-hand side and on the right-hand side one

191
00:10:41,810 --> 00:10:43,560
particular flaw where

192
00:10:43,600 --> 00:10:46,890
talking about which is broken in this case so on the right-hand side of the

193
00:10:46,890 --> 00:10:49,770
implication which have drawn is the horseshoe

194
00:10:50,080 --> 00:10:54,850
i've got broken sort of isolated the flow that we're talking about broken on the

195
00:10:54,850 --> 00:10:57,230
right side of the implication

196
00:10:57,230 --> 00:11:00,190
and all of danny's stuck all the other stuff on left

197
00:11:01,660 --> 00:11:06,520
great self managed to isolate the conditions under which broke the country

198
00:11:06,560 --> 00:11:09,020
and that is that if there is the robot

199
00:11:09,040 --> 00:11:10,930
the action is the dropped

200
00:11:10,930 --> 00:11:11,930
the robot

201
00:11:11,940 --> 00:11:13,230
drop six

202
00:11:13,250 --> 00:11:19,040
and x is fragile or there's a bomb the explosive these next x then

203
00:11:19,100 --> 00:11:20,960
breaking against

204
00:11:21,040 --> 00:11:26,460
and then a similar thing for the negative effect so forgot negative effect acting like

205
00:11:26,690 --> 00:11:34,080
x becomes not broken read the robot performs repair action on of one isolate

206
00:11:34,080 --> 00:11:37,080
the following on the right-hand side of an implication

207
00:11:37,120 --> 00:11:39,310
and so i have this

208
00:11:39,370 --> 00:11:43,250
rereading formula which is logically equivalent which is says it the robot

209
00:11:43,270 --> 00:11:47,080
in the robot does repair action anything when you break

210
00:11:47,100 --> 00:11:51,080
now i have to write down that essentially i have formulas of this form

211
00:11:51,100 --> 00:11:51,890
i've got

212
00:11:51,910 --> 00:11:57,710
so if i can just so got this form piece of which makes it remains

213
00:11:57,710 --> 00:12:01,580
the conditions under which the flow becomes true positive

214
00:12:01,660 --> 00:12:06,690
and then i got this in some form which is which are essentially the conditions

215
00:12:06,770 --> 00:12:09,460
under which this can this flow will become falls

216
00:12:09,540 --> 00:12:12,270
so isolated the conditions under which

217
00:12:14,060 --> 00:12:15,310
become true

218
00:12:15,310 --> 00:12:19,120
can use to interpret the data we see so for example the taxonomic tree over

219
00:12:19,120 --> 00:12:22,000
animals which we might expect that the sum

220
00:12:22,040 --> 00:12:25,730
you know this new feature is somehow going to be for example smooth with respect

221
00:12:25,730 --> 00:12:29,020
to this taxonomy tree things nearby in the tree are likely to have the same

222
00:12:29,140 --> 00:12:31,350
values of these features and we're going to

223
00:12:31,400 --> 00:12:33,060
we're going to but by defining

224
00:12:33,560 --> 00:12:38,060
prior on the data given structure probabilistic model of what data we expect to see

225
00:12:38,060 --> 00:12:42,370
given a certain say structure will be able to use this to

226
00:12:42,370 --> 00:12:46,690
give us appropriate inductive biases for generalizing this new property for figuring out which other

227
00:12:46,690 --> 00:12:51,270
things have this property and we can also work backwards from all the observed features

228
00:12:51,270 --> 00:12:55,940
that we see to infer the tree structure of course we have to have some

229
00:12:55,940 --> 00:12:58,770
kind of higher knowledge for example we have to know we're looking for tree in

230
00:12:58,770 --> 00:13:02,830
order to infer tree like this so will also specify what we call the level

231
00:13:02,830 --> 00:13:07,650
of structural form which specifies the kind of relational system learning with and will even

232
00:13:07,650 --> 00:13:11,190
extend the hierarchical bayesian model to to make inferences at this level to be able

233
00:13:11,190 --> 00:13:12,730
to infer that say

234
00:13:12,730 --> 00:13:15,920
one domain has the tree structure for the domain has a different kind of structure

235
00:13:15,980 --> 00:13:22,060
and that's going to be important because not every domain represent properties is tree structure

236
00:13:22,190 --> 00:13:23,690
right so

237
00:13:23,690 --> 00:13:26,980
the starting work up from the bottom here how do we define the probabilistic model

238
00:13:26,980 --> 00:13:29,350
of the data given structure

239
00:13:30,900 --> 00:13:34,810
the intuition is like i said is that we want this prior which in a

240
00:13:34,810 --> 00:13:37,870
sense is going to it's going to be flexible it's going to give any way

241
00:13:37,870 --> 00:13:42,000
of labeling the objects here in the labeling of black or grey anyway of labelling

242
00:13:42,000 --> 00:13:45,830
objects some prior probability but ones which are smooth with respect to the to the

243
00:13:45,830 --> 00:13:50,710
graph structure should have higher prior probability this captures the basic idea of generalization by

244
00:13:50,710 --> 00:13:56,190
similarity where structure determine similarity and to do this we're going to use the word

245
00:13:56,190 --> 00:14:00,000
build on some nice work of you lafferty and ghahramani which probably most people here

246
00:14:00,000 --> 00:14:04,210
are familiar with so i want to go into detail on the basic idea is

247
00:14:04,210 --> 00:14:10,640
to define the gaussians field a set of over the graph a set of continuous

248
00:14:10,670 --> 00:14:14,960
real valued quality quantities at each node of the graph reflecting sort of how much

249
00:14:15,250 --> 00:14:19,980
that features associated with that point of the graph and then

250
00:14:21,290 --> 00:14:25,540
we use the the structure of the graph to parameterize the covariance of the scouting

251
00:14:25,540 --> 00:14:26,730
field so how

252
00:14:26,780 --> 00:14:29,370
feature associations very

253
00:14:29,420 --> 00:14:33,210
between a as you go across the graph the idea the intuition is that points

254
00:14:33,210 --> 00:14:36,940
which are closer in the graph should have higher covariance you can think of that

255
00:14:36,940 --> 00:14:40,380
also is like saying that this this feature sort of there is like a like

256
00:14:40,380 --> 00:14:44,880
a smooth random walk or diffusion process over the graph so for example here's one

257
00:14:44,880 --> 00:14:49,790
sample from this gas field here's another one the math behind it as again many

258
00:14:49,790 --> 00:14:53,120
of you are probably familiar with is just we take the

259
00:14:53,310 --> 00:14:55,150
a regularized version

260
00:14:55,170 --> 00:15:01,560
of the graph posse in an inverse that give the covariance of this gaussianprocess

261
00:15:01,580 --> 00:15:02,900
for the kernel you like

262
00:15:02,920 --> 00:15:07,060
and then if we want to turn this into a distribution on feature extensions are

263
00:15:07,060 --> 00:15:11,210
ways of labeling objects according to whether they have a binary feature not and we

264
00:15:11,210 --> 00:15:14,580
can just threshold this continuous association

265
00:15:16,150 --> 00:15:20,230
no no no in in order to use this distribution we want to do two

266
00:15:20,230 --> 00:15:22,810
things we want to understand how we can learn the right

267
00:15:22,880 --> 00:15:26,770
structured representation of the domain and how we can use it to make inferences from

268
00:15:26,770 --> 00:15:31,100
very few examples of the new property and with the answer to those two questions

269
00:15:31,100 --> 00:15:34,790
is basically the same so to infer the right tree structure we assume that each

270
00:15:34,790 --> 00:15:39,230
of the observed features are generated from this guassian process over that

271
00:15:39,290 --> 00:15:42,560
over the tree and then we just find the tree that best fits the observed

272
00:15:42,560 --> 00:15:46,880
features so you can think of it as a kind of bayesian hierarchical clustering i

273
00:15:46,880 --> 00:15:51,900
mean yet another bayesian hierarchical clustering we find the tree that you know on average

274
00:15:51,900 --> 00:15:55,810
makes the observed features most likely under that smooth process

275
00:15:55,850 --> 00:16:00,830
but the the method isn't just restricted to trees we can use other kinds of

276
00:16:00,830 --> 00:16:03,650
inductive biases so for example here's the tree

277
00:16:03,690 --> 00:16:08,000
here's the tree that the best tree that was found for this some fifty

278
00:16:08,000 --> 00:16:11,980
animal eighty feature set but we can also

279
00:16:12,020 --> 00:16:15,190
try different kind of inductive bias if you like a different kind of structure for

280
00:16:15,190 --> 00:16:19,440
example we can search for the best two-dimensional embedding we're now again we have some

281
00:16:19,440 --> 00:16:25,190
way to determine covariance based on proximity in this two-dimensional space and it's it's you

282
00:16:25,190 --> 00:16:28,310
can think of it more less is the limiting case of the graph based approach

283
00:16:28,310 --> 00:16:30,900
when you take when you take imagine take the

284
00:16:30,920 --> 00:16:31,880
mesh size

285
00:16:31,920 --> 00:16:33,420
two zero

286
00:16:33,470 --> 00:16:36,810
and we can see well which of these kinds of you both of these are

287
00:16:36,810 --> 00:16:42,000
are structured representations of the domain that makes sense of the observed feature matrix they

288
00:16:42,000 --> 00:16:46,330
both kind of compression or dimensionality reduction we can ask which provide the best inductive

289
00:16:46,330 --> 00:16:52,810
bias for generalizing new properties so that we now to assume that this new property

290
00:16:52,850 --> 00:16:56,940
it is the complete extension of this new property which we don't observe but we

291
00:16:56,960 --> 00:17:01,520
only observe a few samples the complete extension is a random draw from that same

292
00:17:01,520 --> 00:17:04,980
gaussianprocess over the graph and we're going to use that as the prior for how

293
00:17:04,980 --> 00:17:06,370
to fill in

294
00:17:06,380 --> 00:17:07,690
the unobserved

295
00:17:09,170 --> 00:17:12,420
and here's the result on two classic data sets

296
00:17:12,420 --> 00:17:17,310
so what i'm planning here are there's the two datasets are the two columns here

297
00:17:18,480 --> 00:17:24,270
each point data point corresponds to different inductive generalization different one of these arguments the

298
00:17:24,270 --> 00:17:28,960
ones over here are have forces as the conclusion so basically we're on a very

299
00:17:29,310 --> 00:17:30,440
very the

300
00:17:30,480 --> 00:17:35,250
the objects which are the examples so for example this point is

301
00:17:35,290 --> 00:17:39,120
is the arguments given cows elephants have property how likely is it that horses have

302
00:17:39,120 --> 00:17:44,440
the same property and other points here correspond to different sets of examples over here

303
00:17:44,500 --> 00:17:48,140
the conclusion is a general one that the question is how likely is it that

304
00:17:48,140 --> 00:17:51,850
all mammals have property given three examples of things which have the property

305
00:17:51,880 --> 00:17:56,870
and the x axis of the model predictions and the y axis are human judgments

306
00:17:56,870 --> 00:17:59,710
so what you see here is a case where you have a model that are

307
00:17:59,710 --> 00:18:04,100
highly correlated with people's judgments generalizations of the model things are strong like this one

308
00:18:04,100 --> 00:18:06,750
are also ones that people think are strong

309
00:18:06,750 --> 00:18:09,640
and once and they will also agree on which ones are weak

310
00:18:09,650 --> 00:18:15,100
and these two plots here correspond to the predictions of this gaussianprocess defined over a

311
00:18:15,100 --> 00:18:19,710
tree structure the ones here correspond to the predictions of the gaussianprocess defined over two-dimensional

312
00:18:19,710 --> 00:18:23,770
space and what you can see is a substantial difference the tree structured model predicts

313
00:18:23,770 --> 00:18:29,770
people's judgments significantly better than the two dimensional one learned from the same feature matrix

314
00:18:29,790 --> 00:18:31,040
now we can

315
00:18:31,060 --> 00:18:32,650
we can explore other kinds of

316
00:18:32,670 --> 00:18:36,460
inductive biases in a sense what we what i would argue here's a tree structured

317
00:18:36,460 --> 00:18:41,350
model is the correct or moral more and more the correct inductive bias for biological

318
00:18:41,350 --> 00:18:45,750
species and properties worth the two dimensional spaces isn't really the right what you know

319
00:18:45,750 --> 00:18:50,670
if you think about evolution and mutation and selection are you going to get structures

320
00:18:50,670 --> 00:18:55,100
of species and distributions of properties that more let's have the distribution and people in

321
00:18:55,100 --> 00:18:58,460
some sense it seems might be picking up on that

322
00:18:58,460 --> 00:19:02,120
some plausible sort of articulatory position

323
00:19:03,950 --> 00:19:11,130
and we can report dickinson show that they could get a well the claim that

324
00:19:11,130 --> 00:19:15,940
they could get a c and increase in performance on some relatively simple

325
00:19:17,490 --> 00:19:23,900
tasks with the problem in making these kinds of comparisons though is that you know

326
00:19:23,950 --> 00:19:28,530
when we make comparisons with our existing HMM systems as we change the parameters as

327
00:19:28,530 --> 00:19:29,930
we had parameters

328
00:19:29,980 --> 00:19:33,850
we often can we can often do better so it's very difficult to make these

329
00:19:33,850 --> 00:19:39,420
kinds of a-b comparisons because you have a whole different set of parameters because the

330
00:19:39,420 --> 00:19:43,510
topology of your agents it is different

331
00:19:43,730 --> 00:19:45,640
any other questions about

332
00:19:49,130 --> 00:19:51,600
i mentioned another example where

333
00:19:51,630 --> 00:19:54,610
we can use

334
00:19:54,630 --> 00:19:57,130
we can modify

335
00:19:57,140 --> 00:19:59,640
our lexicon

336
00:20:00,250 --> 00:20:05,660
incorporating instead of representing the dictionary entries

337
00:20:05,700 --> 00:20:09,340
as sequences of phonemes

338
00:20:09,660 --> 00:20:15,240
you know like you i got the word and i would generally in my phonemic

339
00:20:15,240 --> 00:20:18,410
dictionary expand that in terms of the sequence of phonemes

340
00:20:18,450 --> 00:20:25,150
instead of doing that represent my lexical entries in terms of the bundles of distinctive

341
00:20:28,920 --> 00:20:30,140
i still have

342
00:20:30,150 --> 00:20:32,930
more or less a segmental representation

343
00:20:34,890 --> 00:20:43,080
mister professor stephens in defining his landmark based technique he would have word is represented

344
00:20:43,080 --> 00:20:45,750
in the lexicon according to this kind of model

345
00:20:45,800 --> 00:20:46,990
and i think that's what

346
00:20:47,000 --> 00:20:52,230
that's what motivated the this kind of thing

347
00:20:54,460 --> 00:20:55,180
and so

348
00:20:55,200 --> 00:20:58,430
we're still have the canonical dictionary

349
00:20:58,520 --> 00:21:05,250
but we have now expanded the words into sequences of

350
00:21:07,110 --> 00:21:08,910
distinctive features

351
00:21:08,930 --> 00:21:13,070
and so the issue though is is how do we how do we decode with

352
00:21:15,380 --> 00:21:17,890
i just really redrawn

353
00:21:17,900 --> 00:21:21,730
this table here at the top of the slide

354
00:21:23,300 --> 00:21:27,710
there's sort of two models that are that are represented here one

355
00:21:27,740 --> 00:21:32,320
a a model of a certainty of of uncertainty

356
00:21:32,340 --> 00:21:34,960
as to how the difference

357
00:21:34,990 --> 00:21:38,600
a distinctive features here are synchronized

358
00:21:38,640 --> 00:21:40,910
another model of uncertainty

359
00:21:41,630 --> 00:21:45,660
the actual for the actual observations

360
00:21:45,700 --> 00:21:47,620
a particular feature

361
00:21:47,660 --> 00:21:50,490
four for a given frame so how

362
00:21:50,530 --> 00:21:52,170
how likely it might be

363
00:21:52,180 --> 00:21:57,580
that we substitute one feature for another in the actual realization

364
00:21:57,610 --> 00:22:01,630
of this word as we speak right so get the base for dictionary

365
00:22:02,120 --> 00:22:04,330
with that that we've

366
00:22:04,820 --> 00:22:11,240
this has all the same old problems that we discussed earlier with regular canonical dictionary

367
00:22:11,240 --> 00:22:15,750
is that when we actually speak these words are going to be pronounced differently and

368
00:22:15,750 --> 00:22:19,240
how do we deal with that when the idea here is to actually have a

369
00:22:20,300 --> 00:22:23,240
that of uncertainty how

370
00:22:23,260 --> 00:22:29,130
how likely is it that we might substitute one feature for another when we actually

371
00:22:29,130 --> 00:22:30,400
utter the word

372
00:22:32,280 --> 00:22:36,130
does it work well we had some sort of sequence index here

373
00:22:36,150 --> 00:22:38,530
and for for each for each

374
00:22:39,260 --> 00:22:44,880
each feature i have there's a little example here where

375
00:22:44,880 --> 00:22:46,200
i have the manner

376
00:22:46,200 --> 00:22:48,630
and the place distinctive features

377
00:22:48,650 --> 00:22:57,470
i had this feature index incriminating from increasing values as we progress

378
00:22:58,110 --> 00:23:02,590
through the through decoding this word so and i see

379
00:23:02,590 --> 00:23:10,430
that by having so differing feature index for these two features that describing the level

380
00:23:10,450 --> 00:23:12,110
of a secret

381
00:23:12,800 --> 00:23:13,510
and so

382
00:23:13,920 --> 00:23:18,860
if these things are perfectly synchronized be sort of chopping through here in exactly the

383
00:23:18,860 --> 00:23:22,400
same rate these guys will have exactly the same indices

384
00:23:22,720 --> 00:23:27,970
as i go through and everything is fine if allowing for some level of unseen

385
00:23:28,590 --> 00:23:32,900
and secondly i might have something that looks more like this different indexes for the

386
00:23:32,900 --> 00:23:38,830
different features and apply some sort of model or perhaps some sort of penalty

387
00:23:38,930 --> 00:23:43,970
that will describe the sort of the level of a security that i decided it

388
00:23:43,970 --> 00:23:46,720
was acceptable for these features

389
00:23:48,740 --> 00:23:56,450
that's one aspect of this model the other again is we're again and removing in

390
00:23:56,450 --> 00:23:57,650
time here

391
00:23:57,680 --> 00:24:04,740
you know when left to right is i'm is i'm decoding feature frames in the

392
00:24:05,590 --> 00:24:06,260
and so

393
00:24:06,300 --> 00:24:13,490
the other other model i might have is the the probability of substituting one feature

394
00:24:13,490 --> 00:24:20,990
for another so for example here i'm assuming that there is some underlying baseform feature

395
00:24:20,990 --> 00:24:23,200
here that might be of OWL

396
00:24:24,470 --> 00:24:26,180
and so on

397
00:24:26,180 --> 00:24:33,380
and then the the actual observed feature i get when i actually dakota

398
00:24:33,510 --> 00:24:37,010
the entrance my system might be something different

399
00:24:37,050 --> 00:24:43,300
so the model that i will have trained for the probability of substituting some observed

400
00:24:43,300 --> 00:24:46,570
feature for the for

401
00:24:46,610 --> 00:24:52,360
given that a particular one actually was in the base from dictionary so this is

402
00:24:52,360 --> 00:24:56,610
an attempt to try and to try and undo

403
00:24:56,650 --> 00:25:03,030
they these problems associated with base was canonical baseform dictionary

404
00:25:20,470 --> 00:25:25,800
sure you need for this particular one in or OK i think i suppose i

405
00:25:25,800 --> 00:25:31,470
was undue i have a sequence of words and i'm going to coding obviously i

406
00:25:31,570 --> 00:25:36,070
have an entire search space but let's pick up after that search space and i

407
00:25:36,070 --> 00:25:39,590
just have a sequence of words it's been hypothesized i'm trying to

408
00:25:39,610 --> 00:25:44,970
come up with the the the goodness of that sequence with respect to this model

409
00:25:45,010 --> 00:25:51,260
in that case i would i could tacked together the lexical representations for

410
00:25:51,280 --> 00:25:53,590
the all the words in the string

411
00:25:53,590 --> 00:25:54,760
right so

412
00:25:54,780 --> 00:25:58,380
then the you know the and so the notion of

413
00:25:58,400 --> 00:25:59,780
of how

414
00:25:59,800 --> 00:26:04,680
when i call articulate two words for example

415
00:26:04,720 --> 00:26:06,360
the notion of of

416
00:26:06,360 --> 00:26:09,190
for this set the stage for the classes my accent

417
00:26:09,250 --> 00:26:11,840
i will try and improve the handwriting there's not much i can do about the

418
00:26:11,840 --> 00:26:16,560
next to the stage

419
00:26:16,580 --> 00:26:19,910
all right so once again if both put out for the my packet to be

420
00:26:19,910 --> 00:26:24,600
minus what would be we both could be possible to get plus

421
00:26:24,630 --> 00:26:27,350
if i put out my pair

422
00:26:27,360 --> 00:26:29,930
would be then she gets c

423
00:26:29,970 --> 00:26:32,400
and i put these and she puts alpha

424
00:26:32,470 --> 00:26:34,210
she gets

425
00:26:34,220 --> 00:26:37,280
right that's when i have all the information that was on the sheet of paper

426
00:26:37,280 --> 00:26:39,820
just added

427
00:26:39,880 --> 00:26:43,790
now there is another way of organizing this is standard in game theory to get

428
00:26:43,790 --> 00:26:45,360
used to it now the first

429
00:26:45,420 --> 00:26:49,650
rather than drawing two different tables like this one do

430
00:26:49,660 --> 00:26:52,180
i'm gonna take the second table

431
00:26:52,250 --> 00:26:55,690
and superimpose it on top of the first

432
00:26:55,700 --> 00:26:58,220
so we do that we'll see i mean

433
00:26:58,230 --> 00:26:59,360
i'm going to do

434
00:26:59,380 --> 00:27:01,940
his drawing larger table

435
00:27:01,950 --> 00:27:06,500
the same basic structure i'm choosing alpha and beta on the roads

436
00:27:06,510 --> 00:27:10,100
my hair is choosing alpha and beta

437
00:27:10,130 --> 00:27:11,550
on the columns

438
00:27:11,560 --> 00:27:13,980
but now i'm going to put both grades in

439
00:27:14,000 --> 00:27:16,970
so the ones on the diagonal

440
00:27:17,050 --> 00:27:21,790
both get b-minus we both choose alpha

441
00:27:21,800 --> 00:27:23,680
we both get people as

442
00:27:23,690 --> 00:27:25,860
if we both should be

443
00:27:25,890 --> 00:27:31,530
but if i choose alpha my patches beats i get a

444
00:27:31,550 --> 00:27:33,530
she gets sick

445
00:27:33,580 --> 00:27:35,840
devices be and she chooses

446
00:27:35,910 --> 00:27:37,550
and it's mean you get to see

447
00:27:37,570 --> 00:27:38,510
and it's hard

448
00:27:38,520 --> 00:27:39,190
it gets

449
00:27:39,510 --> 00:27:42,760
so as i did here

450
00:27:42,780 --> 00:27:46,710
the first grade corresponds to the role plan

451
00:27:46,720 --> 00:27:47,930
me in this case

452
00:27:47,940 --> 00:27:51,810
the second grade each box corresponds to the column player

453
00:27:51,860 --> 00:27:53,090
my parents case

454
00:27:53,110 --> 00:27:55,190
so this is a nice succinct way

455
00:27:55,210 --> 00:27:57,630
recording was in the previous two tables

456
00:27:57,640 --> 00:28:00,920
this is an outcome matrix this tells us everything

457
00:28:00,930 --> 00:28:05,130
that was in the game

458
00:28:05,140 --> 00:28:07,130
it's matrix

459
00:28:09,670 --> 00:28:13,540
so now seems a good time to start talking about what people did so

460
00:28:13,550 --> 00:28:17,040
this is a show of hands how many of you

461
00:28:17,070 --> 00:28:19,150
it shows how far

462
00:28:19,190 --> 00:28:21,820
the hands up so that you can catch that so just to be able to

463
00:28:21,820 --> 00:28:23,700
see how OK

464
00:28:23,760 --> 00:28:24,470
all right

465
00:28:24,500 --> 00:28:27,350
and how we view shows pizza

466
00:28:27,360 --> 00:28:30,940
there's far more to keep the weight and the need is OK

467
00:28:31,040 --> 00:28:34,090
the beta here OK so long looks like a lot of going to find out

468
00:28:34,220 --> 00:28:38,270
the capital of the alpha betas let me try and find out the reasons why

469
00:28:38,270 --> 00:28:42,310
people chose to let me and the alpha is up again

470
00:28:43,040 --> 00:28:44,980
the woman in red

471
00:28:46,800 --> 00:28:49,750
i can get mike to the UK we are

472
00:28:49,770 --> 00:28:53,190
you know on the run from the FBI we can also won why it uses

473
00:28:53,190 --> 00:28:59,720
alpha right so why do you choose alpha

474
00:28:59,750 --> 00:29:09,680
so so he wrote at least squares run part was going do respond to that

475
00:29:09,680 --> 00:29:13,410
and the other reasons for choosing alpha around the world that we can get the

476
00:29:13,410 --> 00:29:15,420
the woman

477
00:29:15,480 --> 00:29:18,500
the density divided by the just like

478
00:29:18,550 --> 00:29:26,970
the together

479
00:29:26,970 --> 00:29:31,970
so it's it's name the second your name was

480
00:29:31,980 --> 00:29:34,160
caught in your name was

481
00:29:34,220 --> 00:29:37,830
there is so slightly different reasons same choice alpha

482
00:29:37,890 --> 00:29:40,060
clarity reasons

483
00:29:40,110 --> 00:29:43,490
the early stages said no matter what the other person does

484
00:29:43,490 --> 00:29:47,000
she reckons she get to get a better grade if you chose alpha to hold

485
00:29:47,000 --> 00:29:50,810
political come back to its parent pluralism come back to paris and the second is

486
00:29:50,810 --> 00:29:54,040
talk to the beat is the second let me just emphasise this stage there are

487
00:29:54,040 --> 00:29:58,170
no wrong answers later on the class become questions have right now there's no run

488
00:29:58,170 --> 00:30:02,480
to optimize for that inference procedure

489
00:30:02,560 --> 00:30:06,000
and then when we do that

490
00:30:07,350 --> 00:30:11,890
we could get this kind of results

491
00:30:11,920 --> 00:30:13,020
so this

492
00:30:13,040 --> 00:30:15,390
here is this line in

493
00:30:15,390 --> 00:30:19,870
all we just as well we don't have the world knowledge and

494
00:30:19,960 --> 00:30:24,730
it's not doing too well and then if we have these two relations that contained

495
00:30:24,730 --> 00:30:26,250
by the location

496
00:30:26,270 --> 00:30:32,810
we get better results and here we constructed of simulation actually so human could disambiguate

497
00:30:32,810 --> 00:30:37,330
any of the sentences so that's why you can get almost zero

498
00:30:37,480 --> 00:30:39,870
sent here when you have

499
00:30:39,890 --> 00:30:41,160
the world knowledge

500
00:30:41,170 --> 00:30:43,270
because if have less well knowledge

501
00:30:43,270 --> 00:30:45,190
you don't do well in

502
00:30:46,520 --> 00:30:50,160
a more realistic setting you probably don't even have some of the world knowledge and

503
00:30:50,160 --> 00:30:54,120
all of the right so then you'll probably be more in this domain

504
00:30:54,140 --> 00:30:55,310
of the middle

505
00:31:00,170 --> 00:31:03,120
we can also look at the

506
00:31:03,270 --> 00:31:10,170
representations we learned for concepts just like in the previous talks and again you get

507
00:31:11,350 --> 00:31:18,190
kind of a semantic similarity here for example it learns that female

508
00:31:18,190 --> 00:31:21,370
concepts are most similar to other

509
00:31:23,790 --> 00:31:26,750
and so on

510
00:31:27,870 --> 00:31:30,250
so that's it for

511
00:31:31,020 --> 00:31:37,040
part of the talk and we try to show a simple but general framework for

512
00:31:37,230 --> 00:31:40,270
language grounding based on the task of concept labeling

513
00:31:40,290 --> 00:31:47,020
and we try to make it scalable model that didn't use handcrafted features all rules

514
00:31:47,020 --> 00:31:50,980
by learning hidden representations

515
00:31:51,100 --> 00:31:53,750
and the real girl which

516
00:31:54,310 --> 00:31:58,500
more like an artificial intelligence goal would be to train a learner living in a

517
00:31:58,500 --> 00:32:02,900
computer game world to learn language from scratch

518
00:32:04,080 --> 00:32:10,160
with other characters in that world like i think this would be really cool

519
00:32:11,060 --> 00:32:16,620
we know we're not there yet which is actually of final conclusion for the whole

520
00:33:34,170 --> 00:33:39,120
the graphical model

521
00:34:09,330 --> 00:34:20,350
what you see is what

522
00:35:22,270 --> 00:35:27,350
to the prior on probability

523
00:35:27,370 --> 00:35:29,190
the reason for parameters which is

524
00:35:41,040 --> 00:35:54,480
so it's really hard to his questions

525
00:35:59,710 --> 00:36:07,230
right away

526
00:36:07,440 --> 00:36:11,730
yes yes

527
00:36:12,420 --> 00:36:19,620
image is politically core something like this would what

528
00:36:19,750 --> 00:36:27,440
you might

529
00:36:28,210 --> 00:36:32,310
i mean you may be important in the case of a one

530
00:36:32,310 --> 00:36:41,140
well what the unsupervised task actually if you consider that as a tasking set-up where

531
00:36:41,140 --> 00:36:47,350
you trained supervised and unsupervised actually unsupervised is mostly trained before

532
00:36:47,350 --> 00:36:50,990
so if you of this then you can extend this to the limit in a

533
00:36:50,990 --> 00:36:54,810
way where you have a continuous distribution

534
00:36:54,820 --> 00:36:57,300
but OK so it's

535
00:36:57,340 --> 00:36:58,910
in general most

536
00:37:00,040 --> 00:37:04,330
convex function you and encounter are lower semicontinuous so you don't have to worry too

537
00:37:04,330 --> 00:37:05,630
much about that

538
00:37:05,650 --> 00:37:07,900
i markov inequality

539
00:37:08,260 --> 00:37:12,480
anyone wants to

540
00:37:12,490 --> 00:37:15,610
give the idea

541
00:37:15,710 --> 00:37:20,260
did anyone single i mean find find the trick

542
00:37:21,420 --> 00:37:22,640
OK maybe

543
00:37:22,650 --> 00:37:23,390
OK so

544
00:37:23,400 --> 00:37:25,120
maybe that wasn't enough

545
00:37:25,140 --> 00:37:29,400
and maybe you are too shy

546
00:37:29,410 --> 00:37:32,790
OK so

547
00:37:32,870 --> 00:37:34,810
as i said we start from that

548
00:37:34,820 --> 00:37:38,160
and we start the other way around so we start from the right

549
00:37:38,820 --> 00:37:39,910
and and go

550
00:37:39,920 --> 00:37:41,900
to the to the left

551
00:37:45,470 --> 00:37:47,800
so what we write first is that x

552
00:37:47,820 --> 00:37:52,230
because x is larger than zero

553
00:37:55,280 --> 00:37:58,900
x is larger than c

554
00:38:01,400 --> 00:38:03,700
is larger than

555
00:38:03,730 --> 00:38:05,700
t times the indicator

556
00:38:05,720 --> 00:38:07,800
that x is larger than the

557
00:38:07,840 --> 00:38:09,310
it's a bit

558
00:38:09,360 --> 00:38:13,590
stupid to write this but

559
00:38:13,640 --> 00:38:15,770
that's how it works so

560
00:38:15,810 --> 00:38:19,950
what actually this is this is always true because either

561
00:38:20,050 --> 00:38:24,370
x is less than t and the indicator function is equal to zero

562
00:38:24,390 --> 00:38:28,260
and because we know that x is larger than zero this is true or

563
00:38:28,330 --> 00:38:32,350
x is larger than t and indicator is equal to one

564
00:38:32,970 --> 00:38:35,970
we we obtain x larger than t

565
00:38:36,550 --> 00:38:40,010
i mean we don't see much by saying this but this is enough to prove

566
00:38:40,050 --> 00:38:44,470
this result because now we take the expectation of both sides

567
00:38:45,080 --> 00:38:51,580
the fixation of x on this side and the expectation of the indicator

568
00:38:51,660 --> 00:38:53,470
is the probability

569
00:38:53,470 --> 00:38:58,470
so we take we obtain that the expectation of x is larger than c times

570
00:38:58,470 --> 00:38:59,490
the probability

571
00:38:59,550 --> 00:39:01,220
x larger than t

572
00:39:01,240 --> 00:39:04,030
OK this is nothing but this statement

573
00:39:06,740 --> 00:39:08,470
for the other ones

574
00:39:08,490 --> 00:39:11,140
it's the same idea

575
00:39:11,140 --> 00:39:12,620
you apply actually

576
00:39:12,620 --> 00:39:13,720
so these are

577
00:39:13,720 --> 00:39:17,780
championship in general consequences of markov inequality

578
00:39:19,200 --> 00:39:26,260
you take before applying markov you take a certain function inside the probability so here

579
00:39:27,010 --> 00:39:28,570
what you do is

580
00:39:28,620 --> 00:39:30,330
you replace the probability

581
00:39:30,330 --> 00:39:32,300
so let's write it here

582
00:39:32,410 --> 00:39:41,550
so because

583
00:39:41,600 --> 00:39:42,660
this quantity

584
00:39:42,890 --> 00:39:46,740
it is nonnegative

585
00:39:46,760 --> 00:39:49,330
you can

586
00:39:49,600 --> 00:39:50,370
i mean it

587
00:39:50,390 --> 00:39:52,370
easy to see that

588
00:39:52,430 --> 00:39:54,530
the probability that this quantity

589
00:39:54,570 --> 00:39:57,600
is larger than something

590
00:39:57,640 --> 00:40:01,620
it is the same as the probability that the square of this quantity is larger

591
00:40:01,620 --> 00:40:05,640
than the square of the something

592
00:40:05,660 --> 00:40:11,100
causes people not be true if we if we don't take the absolute values and

593
00:40:11,100 --> 00:40:13,030
and and

594
00:40:13,780 --> 00:40:19,930
there is an equality inequality between those two events because they are

595
00:40:19,950 --> 00:40:25,350
i mean every time this is true this is true and evidences prove this system

596
00:40:25,390 --> 00:40:27,570
and you can use actually

597
00:40:27,580 --> 00:40:28,640
so the fact that

598
00:40:28,660 --> 00:40:32,910
one event is equivalent to the other event implies that the probability of the same

599
00:40:32,910 --> 00:40:37,680
and this is a consequence if you want of

600
00:40:37,720 --> 00:40:38,640
this one

601
00:40:39,970 --> 00:40:44,410
here the road goes into this in both directions then you use

602
00:40:44,450 --> 00:40:50,390
both statements and you get the equality i

603
00:40:53,760 --> 00:40:55,830
take the square and here

604
00:40:55,850 --> 00:41:00,510
when you apply markov inequality the expected value of the square here

605
00:41:00,580 --> 00:41:03,720
is nothing but the variance of the of the bible x

606
00:41:03,760 --> 00:41:06,260
that's why you think the variance on the right

607
00:41:06,390 --> 00:41:09,550
and for general if you do the same trick

608
00:41:10,240 --> 00:41:11,850
so you start

609
00:41:14,910 --> 00:41:17,120
x amount of energy

610
00:41:17,120 --> 00:41:19,940
this book introduction to algorithms

611
00:41:20,590 --> 00:41:25,550
MIT students can get any any of the local bookstores including you might who is

612
00:41:25,550 --> 00:41:27,750
also new online service

613
00:41:27,760 --> 00:41:31,520
that provides the

614
00:41:31,620 --> 00:41:35,890
textbooks you can also get a discount if you buy find at the MIT

615
00:41:36,470 --> 00:41:38,900
press bookstore

616
00:41:38,910 --> 00:41:44,440
there's a coupon in the MIT student telephone directory

617
00:41:44,450 --> 00:41:48,280
four discount on MIT press books and you can use that to purchase this book

618
00:41:48,280 --> 00:41:50,210
in discount

619
00:41:50,250 --> 00:41:58,290
you course website this is the course website links to the stellar website which is

620
00:41:58,290 --> 00:42:00,720
we're actually everything will be kept

621
00:42:00,730 --> 00:42:05,800
and estimate students have their own website

622
00:42:05,850 --> 00:42:12,170
a lot some students find this course particularly challenging

623
00:42:12,260 --> 00:42:17,150
so each we'll have extra help we will post

624
00:42:17,200 --> 00:42:21,100
weekly office hours on the course website for the TA's

625
00:42:21,120 --> 00:42:25,370
and then as an experiment this term we're going to offer homework labs for this

626
00:42:26,620 --> 00:42:31,210
so our homework lab is is it's a place in the time you can go

627
00:42:32,490 --> 00:42:34,430
other people in the course will go

628
00:42:34,440 --> 00:42:35,720
to do homework

629
00:42:35,770 --> 00:42:36,920
and there will be

630
00:42:36,930 --> 00:42:41,480
o typically two TA's who staff the lab

631
00:42:41,530 --> 00:42:45,040
so as you're working on your homework you can get help from the TA's if

632
00:42:45,040 --> 00:42:46,140
you need it

633
00:42:46,150 --> 00:42:50,380
generally a place in czech we reschedule those and be on the

634
00:42:50,880 --> 00:42:56,340
course calendar for where it is and when it is that the the we help

635
00:42:56,340 --> 00:42:58,980
but usually sunday's two to four PM

636
00:42:59,230 --> 00:43:01,060
or else it will be in

637
00:43:01,480 --> 00:43:04,740
some people i think the first one is an evening right

638
00:43:04,750 --> 00:43:09,870
a year to win the homework is due your best bet is trying do the

639
00:43:09,870 --> 00:43:11,960
homework in advance of

640
00:43:12,010 --> 00:43:13,280
you homework lab

641
00:43:13,300 --> 00:43:14,380
but then

642
00:43:14,390 --> 00:43:17,700
if you want extra help if you want to talk over your solutions with people

643
00:43:17,700 --> 00:43:21,790
causes we'll talk about problems that you can solve in collaboration with other people in

644
00:43:21,790 --> 00:43:23,310
the class

645
00:43:23,930 --> 00:43:30,350
in addition there are several peer assistance programs

646
00:43:30,360 --> 00:43:38,110
and those usually get all also the office of minority education has a an assistance

647
00:43:38,110 --> 00:43:42,840
program and those usually get booked up pretty quickly so if you're interested in those

648
00:43:42,850 --> 00:43:44,140
good idea too

649
00:43:44,150 --> 00:43:48,190
make an appointment to get there and get

650
00:43:48,200 --> 00:43:50,870
get help soon

651
00:43:50,910 --> 00:43:54,220
OK so homework clubs i hope a lot of people try that out we've never

652
00:43:54,220 --> 00:43:57,530
done this i don't know of any other course to other people have no of

653
00:43:57,530 --> 00:44:00,580
course is at MIT that done this

654
00:44:02,180 --> 00:44:04,200
six o one one day OK

655
00:44:06,810 --> 00:44:09,480
and was successful in that class

656
00:44:09,490 --> 00:44:11,430
never went OK

657
00:44:13,950 --> 00:44:16,160
so we'll see

658
00:44:16,180 --> 00:44:16,950
well say

659
00:44:16,970 --> 00:44:21,170
OK if it if it's not paying off then will just return to or near

660
00:44:21,190 --> 00:44:23,560
office hours for those TA's but

661
00:44:23,670 --> 00:44:29,570
but i think that for some students i think that's that's a good opportunity

662
00:44:29,620 --> 00:44:33,380
if you wish to be registered in this course you must sign up on the

663
00:44:33,380 --> 00:44:34,790
course web page

664
00:44:34,810 --> 00:44:37,500
so that's requirement one must be done today

665
00:44:37,560 --> 00:44:47,230
we will find it difficult to pass the course if you're not in the class

666
00:44:48,210 --> 00:44:52,270
and you should notify at you if you decide to drop so that we can

667
00:44:52,270 --> 00:44:57,570
get you often stop the mailings stop the spam OK

668
00:44:57,580 --> 00:45:01,990
and you should register today before seven PM

669
00:45:02,040 --> 00:45:07,100
and then we're an email your recitation assignment you before noon tomorrow

670
00:45:07,710 --> 00:45:14,060
and if you don't inform received this information by thursday noon please send us email

671
00:45:14,080 --> 00:45:17,760
OK saying to the core staff generally not to me individually

672
00:45:18,240 --> 00:45:21,820
saying that you didn't receive your recitation assignment

673
00:45:22,770 --> 00:45:26,170
so if you have received it by thursday noon

674
00:45:26,190 --> 00:45:29,550
you know if you want to know why i think that generally i think are

675
00:45:29,550 --> 00:45:32,470
interesting about tonight writer this by tomorrow morning

676
00:45:34,490 --> 00:45:36,970
students don't have to worry about this

677
00:45:36,990 --> 00:45:40,590
problem sets with nine problem sets that we

678
00:45:40,640 --> 00:45:43,240
the project will be assigned during the semester

679
00:45:43,250 --> 00:45:47,780
a couple things about problems at home works will generally be accepted

680
00:45:47,830 --> 00:45:55,390
if you are extenuating stands circumstances you should make prior arrangements with your recitation instructor

681
00:45:55,460 --> 00:45:59,250
that almost all the administrative staff you shouldn't come to me and say can i

682
00:45:59,250 --> 00:46:02,990
had in something like this should be talking to your recitation instructor

683
00:46:05,840 --> 00:46:08,000
you can read the other things about the

684
00:46:08,010 --> 00:46:13,420
the form there are both are but let me just mention that there exercises that

685
00:46:13,420 --> 00:46:15,560
should be solved but not hindered in

686
00:46:15,570 --> 00:46:19,380
as well to give you drill on the material i highly recommend you doing the

687
00:46:19,380 --> 00:46:24,510
exercises they both test your understanding of the material and exercises have this way of

688
00:46:24,510 --> 00:46:29,050
finding themselves on quizzes

689
00:46:30,590 --> 00:46:34,540
they are often asked to describe algorithms and here's little outline of what you can

690
00:46:34,540 --> 00:46:38,370
use to describe an algorithm

691
00:46:39,200 --> 00:46:43,700
the grading policy is something that somehow i cover and always every term is at

692
00:46:43,700 --> 00:46:46,710
least a couple of students who

693
00:46:46,750 --> 00:46:48,400
pretend like i never

694
00:46:48,410 --> 00:46:50,160
show them this OK

695
00:46:51,900 --> 00:46:59,170
if you skip problems it has a non-linear effect on your great

696
00:47:01,960 --> 00:47:05,380
OK so you know skip any problems

697
00:47:05,400 --> 00:47:07,920
no effect on your great OK

698
00:47:07,970 --> 00:47:12,910
i've escape one problem and hundreds of letters right we can handle that

699
00:47:12,920 --> 00:47:19,340
OK but two problems it's attempt as you see by the time you've skipped like

700
00:47:20,290 --> 00:47:25,100
letter grades it's already a third of five problems is not problem sets by the

701
00:47:25,910 --> 00:47:27,530
this is problems

702
00:47:27,580 --> 00:47:31,660
OK you're down the third of letter grades

703
00:47:32,610 --> 00:47:38,400
and if you don't do nine or more so that's typically about three to four

704
00:47:38,400 --> 00:47:39,890
problem sets

705
00:47:39,940 --> 00:47:42,990
OK you don't pass the class

706
00:47:43,010 --> 00:47:45,850
OK so i always have some students coming to the end the say i didn't

707
00:47:45,850 --> 00:47:47,600
do any my problems

708
00:47:47,610 --> 00:47:52,570
OK just passed me because i did OK on exams answer no

709
00:47:52,590 --> 00:47:54,960
OK very simple answer questions

710
00:47:54,970 --> 00:47:56,440
set it up front

711
00:47:57,070 --> 00:48:00,070
so the problem sets are an integral part of the

712
00:48:00,130 --> 00:48:06,180
the course collaboration policy this is extremely important so everybody pay attention if you sleep

713
00:48:06,770 --> 00:48:08,030
wake up

714
00:48:09,190 --> 00:48:13,650
like that's going to wake anybody

715
00:48:15,760 --> 00:48:17,570
so the goal of homework

716
00:48:17,580 --> 00:48:20,830
professor domains in my philosophy is that the goal of homework is to help you

717
00:48:20,830 --> 00:48:22,750
learn the material

718
00:48:22,760 --> 00:48:26,710
one way of helping to learn is not just be stuck and unable to solve

719
00:48:27,640 --> 00:48:31,790
because then you are in no better shape when you examine all around

720
00:48:31,810 --> 00:48:35,110
OK which is where we're actually evaluating u

721
00:48:35,110 --> 00:48:37,550
it's not the kind of

722
00:48:37,650 --> 00:48:41,170
legal break type of theorem which tells you you can take all these of these

723
00:48:41,170 --> 00:48:47,010
individual marginals and piece them together into for like lower dimensional measures piecing together into

724
00:48:47,050 --> 00:48:48,740
high dimensional one

725
00:48:49,990 --> 00:48:51,260
what it is is

726
00:48:51,340 --> 00:48:54,720
it's this regularity here

727
00:48:54,720 --> 00:48:57,070
from my point of view this is the regularities here

728
00:48:57,090 --> 00:48:59,630
and kind of kind of the intuition

729
00:48:59,680 --> 00:49:03,280
but that might be helpful is if you think for example of

730
00:49:03,300 --> 00:49:06,300
of continuous functions on the real line

731
00:49:06,300 --> 00:49:09,530
so in order to define a function on the real line you have to specify

732
00:49:09,530 --> 00:49:11,360
its value at every single point

733
00:49:11,380 --> 00:49:12,170
so the

734
00:49:12,180 --> 00:49:16,050
in general function on the real line has an uncountable number of degrees of freedom

735
00:49:16,050 --> 00:49:18,510
one for each point in r

736
00:49:18,510 --> 00:49:23,380
now if you restrict your find a set of functions to the continuous functions

737
00:49:23,400 --> 00:49:27,630
a continuous function is completely defined if you know its values on a dense subset

738
00:49:27,630 --> 00:49:30,550
so far but for example if you know all the values of the function on

739
00:49:31,280 --> 00:49:34,550
on the rational numbers then it's already completely defined

740
00:49:34,610 --> 00:49:38,780
because the the the the the condition that it's that's continues

741
00:49:38,800 --> 00:49:43,180
oppose so much smoothness on it that effectively reduces the number of degrees of freedom

742
00:49:43,470 --> 00:49:47,430
the effective number of degrees of freedom is much smaller than the uncountable set

743
00:49:47,450 --> 00:49:50,780
and you can go even further so theorem that most people you will be familiar

744
00:49:50,780 --> 00:49:55,530
with this decision nyquist sampling theorem which says if you have a bandlimited function

745
00:49:55,530 --> 00:49:56,680
then it's actually

746
00:49:56,700 --> 00:50:01,030
then then you can sample of finite number of times within its period until

747
00:50:01,070 --> 00:50:03,150
and that completely defines the function

748
00:50:03,180 --> 00:50:06,880
so it's bandlimited then actually find the values of a finite number of points if

749
00:50:06,880 --> 00:50:11,170
you choose the right way to find number of points completely

750
00:50:12,110 --> 00:50:15,590
in the series is somewhat similar to what this says is

751
00:50:15,970 --> 00:50:20,650
this the this measure here is the set function that maps sets to numbers

752
00:50:20,670 --> 00:50:24,430
but it's not an arbitrary set function but is the probability measure and we had

753
00:50:24,430 --> 00:50:28,570
this number of conditions in the definition of a probability measure that

754
00:50:28,630 --> 00:50:29,590
that we

755
00:50:29,590 --> 00:50:32,630
o request require office that function in order for it to be called the probability

756
00:50:32,630 --> 00:50:35,470
measure and with this says is

757
00:50:35,530 --> 00:50:38,400
one way to interpret this is to say that

758
00:50:38,400 --> 00:50:41,760
the marginals

759
00:50:41,780 --> 00:50:42,990
the marginals

760
00:50:42,990 --> 00:50:45,110
that we that we started with

761
00:50:45,130 --> 00:50:49,260
define the value of this measure on sets that look like this so

762
00:50:49,300 --> 00:50:52,990
the the the marginals of finite dimensional so the set down here

763
00:50:53,050 --> 00:50:56,030
this this overall space would be infinite dimensional

764
00:50:56,090 --> 00:50:59,240
but we have something that is non-trivial shape

765
00:50:59,240 --> 00:51:01,700
only a finite number of dimensions

766
00:51:02,930 --> 00:51:06,740
and all other dimensions of experiments in the kind of

767
00:51:08,700 --> 00:51:11,030
what the theorem says is

768
00:51:11,050 --> 00:51:12,760
if we know if we know

769
00:51:12,780 --> 00:51:14,300
the value of the measure

770
00:51:14,320 --> 00:51:17,450
of this measure mu e on all sides of this shape

771
00:51:17,490 --> 00:51:20,950
then we actually know it on all sides and i think

772
00:51:21,010 --> 00:51:23,320
because the measures so regularly

773
00:51:23,360 --> 00:51:27,760
it would not work for an arbitrary set function it has to be probability

774
00:51:28,010 --> 00:51:31,800
that's regularity results

775
00:51:33,340 --> 00:51:36,650
and in the in the jargon of probability theory this kind of matter that is

776
00:51:36,650 --> 00:51:40,110
defined by the theorem is called the projective limit

777
00:51:40,150 --> 00:51:42,510
of this family of march

778
00:51:42,530 --> 00:51:52,070
projective limit

779
00:51:52,150 --> 00:51:56,170
so now i really have to hurry up

780
00:51:58,780 --> 00:52:01,200
quickly let's let's quickly run through the

781
00:52:01,220 --> 00:52:04,530
through the construction of the gaussian process again just two

782
00:52:04,570 --> 00:52:06,570
to see what individual components me

783
00:52:06,570 --> 00:52:08,860
if we if we apply that to an example

784
00:52:08,910 --> 00:52:11,590
and then i will tell you what's wrong with this kind of

785
00:52:14,570 --> 00:52:17,150
gaussianprocess process construction we will choose

786
00:52:17,150 --> 00:52:20,720
this this excess space which was that are

787
00:52:20,720 --> 00:52:25,040
here in the form in later life

788
00:52:33,760 --> 00:52:43,150
decided not to like in better

789
00:52:43,390 --> 00:52:48,210
look at

790
00:52:48,220 --> 00:52:54,480
the better

791
00:52:54,500 --> 00:52:56,940
OK so let's go back to the

792
00:52:59,290 --> 00:53:01,040
so you choose to stay

793
00:53:01,150 --> 00:53:02,970
state one

794
00:53:03,020 --> 00:53:07,150
with probability

795
00:53:07,210 --> 00:53:09,330
o point one

796
00:53:09,390 --> 00:53:12,190
good as one with probability point

797
00:53:15,380 --> 00:53:22,380
probability point one

798
00:53:22,430 --> 00:53:24,910
you get are still

799
00:53:24,930 --> 00:53:28,710
with every change deterministically go

800
00:53:28,770 --> 00:53:31,050
the probability of one point l

801
00:53:32,520 --> 00:53:33,660
s two

802
00:53:39,610 --> 00:53:45,210
now imagine that i told you you could you could achieve

803
00:53:45,260 --> 00:53:47,490
some value

804
00:53:47,540 --> 00:53:51,130
you could see some value from being in state s one you could achieve some

805
00:53:51,130 --> 00:53:55,690
value in being in state s two

806
00:53:57,540 --> 00:54:00,050
now how do you do

807
00:54:00,100 --> 00:54:02,180
i hope have have

808
00:54:02,290 --> 00:54:04,850
how do you have to achieve

809
00:54:05,010 --> 00:54:09,660
that's OK was given here but all that the math all work now

810
00:54:09,710 --> 00:54:12,100
in this example

811
00:54:12,210 --> 00:54:15,040
OK so the first thing you say is

812
00:54:15,070 --> 00:54:16,430
i could stay

813
00:54:16,480 --> 00:54:17,960
if i stay

814
00:54:17,960 --> 00:54:20,080
with point nine probability

815
00:54:20,080 --> 00:54:21,990
that's not this branch

816
00:54:22,150 --> 00:54:24,280
with point nine probability

817
00:54:24,390 --> 00:54:27,110
i can get the this one

818
00:54:27,120 --> 00:54:30,990
they were singing achieve at least that value for me as well

819
00:54:31,550 --> 00:54:33,420
and one point one probability

820
00:54:33,510 --> 00:54:36,110
i get what

821
00:54:36,370 --> 00:54:44,480
one of the transition to have to get the best two

822
00:54:44,490 --> 00:54:49,980
the point one times what

823
00:54:50,020 --> 00:54:56,860
are one thing to say

824
00:54:59,890 --> 00:55:04,150
so point nine acted as one of the best one

825
00:55:04,210 --> 00:55:09,680
four point one public invested in getting what

826
00:55:09,700 --> 00:55:14,370
he has to

827
00:55:14,420 --> 00:55:16,800
look at

828
00:55:17,610 --> 00:55:19,700
here is is much simpler

829
00:55:19,710 --> 00:55:22,230
the probability one i get asked to

830
00:55:22,240 --> 00:55:24,990
and i get the of

831
00:55:25,030 --> 00:55:29,610
so for each branch

832
00:55:29,700 --> 00:55:34,730
for each branch stay or change i know the expected value you find these numbers

833
00:55:34,770 --> 00:55:36,950
some numbers

834
00:55:37,050 --> 00:55:39,300
then what action

835
00:55:39,300 --> 00:55:43,550
OK you you know the numbers from which had previously

836
00:55:43,590 --> 00:55:46,740
imagine to denote these numbers

837
00:55:46,800 --> 00:55:50,990
if you have to choose between stay change mass one

838
00:55:51,000 --> 00:55:52,710
not what the numbers are

839
00:55:52,780 --> 00:56:00,140
how how would you choose the best action

840
00:56:04,400 --> 00:56:10,800
OK i see you are linear algebra by that that would last year

841
00:56:10,890 --> 00:56:18,020
a bit more explicit forty nine BS one was point one vs two

842
00:56:18,050 --> 00:56:22,330
is greater than

843
00:56:22,390 --> 00:56:26,780
the s two

844
00:56:31,060 --> 00:56:35,430
do what

845
00:56:35,470 --> 00:56:37,840
a course day

846
00:56:37,870 --> 00:56:41,370
and if it's not true

847
00:56:41,430 --> 00:56:43,290
then do what

848
00:56:43,440 --> 00:56:48,980
change OK

849
00:56:48,990 --> 00:56:51,590
so if you know you can see some value in end

850
00:56:51,660 --> 00:56:53,780
i don't know how i know that

851
00:56:53,800 --> 00:56:58,060
what will get minute you can derive the policy that what she believes that value

852
00:56:58,160 --> 00:57:00,010
OK and here's exactly

853
00:57:00,030 --> 00:57:05,640
so you have so we assume that we i didn't show

854
00:57:05,660 --> 00:57:07,050
we were here

855
00:57:07,110 --> 00:57:10,930
but that so when we have reward to for staying

856
00:57:10,980 --> 00:57:13,940
and we had the war ten for change

857
00:57:13,990 --> 00:57:17,030
so this is slightly different should be too

858
00:57:18,690 --> 00:57:22,850
ten plus

859
00:57:22,860 --> 00:57:24,860
but still

860
00:57:28,370 --> 00:57:31,240
there that's correct so

861
00:57:32,120 --> 00:57:37,450
actually it which maximizes the reward that you get a plus b

862
00:57:38,730 --> 00:57:40,620
value again the next

863
00:57:40,670 --> 00:57:43,500
OK so what i drug here

864
00:57:44,550 --> 00:57:51,980
yes you exactly

865
00:57:52,070 --> 00:57:57,940
right so that there are there this existence theorem that says there is a policy

866
00:57:57,990 --> 00:58:00,130
only present the current state that

867
00:58:01,680 --> 00:58:04,120
that is optimal there exists a policy

868
00:58:04,180 --> 00:58:05,640
i have properties

869
00:58:05,660 --> 00:58:13,980
so we only this assertion is based policies

870
00:58:14,030 --> 00:58:16,430
yeah exactly

871
00:58:18,430 --> 00:58:21,570
so i mean i actually like an answer themselves why

872
00:58:21,590 --> 00:58:25,750
did did did receive policy suffice

873
00:58:25,760 --> 00:58:28,070
because if if i had one action that could be

874
00:58:28,090 --> 00:58:32,280
ten expectation well let's say a course stay

875
00:58:32,320 --> 00:58:36,390
how expectation sorry i'm building giving notation

876
00:58:38,600 --> 00:58:39,970
feature value of

877
00:58:42,050 --> 00:58:47,110
and a course change had expected future value

878
00:58:47,110 --> 00:58:50,750
doesn't really make a difference seventy percent

879
00:58:50,860 --> 00:58:51,970
on the same

880
00:58:51,980 --> 00:58:55,490
now the basic technique

881
00:59:00,610 --> 00:59:04,740
so that the weighted by the case went down to eighty five

882
00:59:04,860 --> 00:59:10,610
because the rate i mean cannot identify it would be much more training data to

883
00:59:10,610 --> 00:59:14,020
identify that certain motifs has appeared

884
00:59:14,040 --> 00:59:18,040
but you don't about this picture of the ship

885
00:59:18,100 --> 00:59:20,090
this should comment

886
00:59:20,100 --> 00:59:23,060
let's use the ship

887
00:59:23,070 --> 00:59:26,380
to specify how much of the kernel country

888
00:59:27,600 --> 00:59:30,490
larouche by ten

889
00:59:32,740 --> 00:59:37,020
i can be larger than the very simple

890
00:59:37,030 --> 00:59:43,840
i should add

891
00:59:43,890 --> 00:59:47,220
but it's little long but

892
00:59:47,330 --> 00:59:49,940
only goes up to ninety two

893
00:59:51,640 --> 00:59:55,040
and i think when you increase the further

894
00:59:55,060 --> 00:59:56,280
like what

895
01:00:02,940 --> 01:00:08,980
maybe you can play around with it these scripts which used here you can download

896
01:00:08,980 --> 01:00:15,890
from on the wiki page what you need is toolbox of visual box link

897
01:00:15,940 --> 01:00:18,420
the developing the link on the on the

898
01:00:20,090 --> 01:00:25,610
should we both the org and that you can download from open source

899
01:00:25,990 --> 01:00:28,120
just compiled

900
01:00:30,880 --> 01:00:41,340
we estimate that i mean well the heuristic i mean what we do is we

901
01:00:41,970 --> 01:00:48,190
this kind of progress so we some remaining in accuracy in SVM like

902
01:00:48,580 --> 01:00:50,470
so if we

903
01:00:50,510 --> 01:00:52,420
we have certain assumptions how

904
01:00:52,440 --> 01:00:54,500
o that is related to the training time

905
01:00:55,890 --> 01:00:58,220
i mean it's usually not very

906
01:00:58,230 --> 01:00:59,110
but but

907
01:01:01,100 --> 01:01:07,910
other of questions for example work for the truth about what

908
01:01:14,140 --> 01:01:21,120
OK so

909
01:01:21,240 --> 01:01:27,870
i mean we talked about mining based on the generation of feature space where we

910
01:01:27,870 --> 01:01:31,780
think about which kind of features we would like to do it

911
01:01:31,790 --> 01:01:35,600
once we find that you can simply take the kind

912
01:01:35,610 --> 01:01:44,100
the band for the spectral mismatch mismatch twenty of these things the position with of

913
01:01:44,100 --> 01:01:45,820
the other side

914
01:01:45,840 --> 01:01:49,810
based on physical chemical properties

915
01:01:49,870 --> 01:01:55,340
now i would like to describe another kind of is called the fisher that uses

916
01:01:55,650 --> 01:01:57,520
probabilistic model

917
01:01:57,610 --> 01:02:01,410
the basis and generate from the probabilistic model

918
01:02:01,760 --> 01:02:03,240
and and find

919
01:02:03,250 --> 01:02:07,920
well actually defined feature which are then used to define find

920
01:02:08,000 --> 01:02:14,820
OK so probabilistic models for probabilistic modeling of violence because quite a bit older actually

921
01:02:14,820 --> 01:02:20,490
than than most machine learning technique markov models one markov models of one example hidden

922
01:02:20,490 --> 01:02:27,720
markov models frequently used in computational biology but also stochastic context free

923
01:02:27,740 --> 01:02:31,370
what is the point in a hidden markov model

924
01:02:31,480 --> 01:02:37,030
there some start date and end date and then you have other state and they

925
01:02:38,870 --> 01:02:40,100
when the match

926
01:02:40,110 --> 01:02:43,460
one version with additional offices in motor

927
01:02:43,520 --> 01:02:46,500
OK so this is like model

928
01:02:46,520 --> 01:02:50,390
like a lot of what sequence which should exhibit a certain

929
01:02:50,410 --> 01:02:53,750
and depending on the motor

930
01:02:53,820 --> 01:02:58,330
but in the motives they are set by the emission probability

931
01:02:58,340 --> 01:03:02,340
OK so important part

932
01:03:02,870 --> 01:03:04,970
for parametric model

933
01:03:04,980 --> 01:03:07,130
family of distributions

934
01:03:07,140 --> 01:03:10,920
usually like this to have some parameter of some

935
01:03:10,940 --> 01:03:13,470
from space maybe in parameters

936
01:03:13,510 --> 01:03:19,590
and then you would to the distribution is parameterized by theta

937
01:03:19,700 --> 01:03:28,730
OK a simple example is a position specific scoring matrices is very frequently used in

938
01:03:28,890 --> 01:03:30,590
about it

939
01:03:30,600 --> 01:03:34,790
and it essentially identical to your order markov chain

940
01:03:34,900 --> 01:03:39,000
so the idea is that we have a sequence of this explain

941
01:03:39,010 --> 01:03:40,410
and then we have

942
01:03:40,660 --> 01:03:46,210
a bit of link with before then we have a metric which is four times

943
01:03:46,210 --> 01:03:47,870
the length of the sea

944
01:03:47,880 --> 01:03:53,130
so and we compute the probability of the secret by taking the

945
01:03:53,150 --> 01:03:55,670
by using the

946
01:03:55,700 --> 01:04:02,650
he index them and so this one role point eight one b one party party

947
01:04:03,100 --> 01:04:07,860
and if the person is in a we use the

948
01:04:07,870 --> 01:04:12,880
and i like to have magic from a metric

949
01:04:12,940 --> 01:04:15,820
and for every position

950
01:04:15,820 --> 01:04:20,240
but it doesn't have are are being the number of random samples you may

951
01:04:22,890 --> 01:04:26,510
points the gap between the bucket of six draws from it

952
01:04:28,610 --> 01:04:29,950
so that shows that really

953
01:04:33,130 --> 01:04:33,840
so what i've done

954
01:04:34,630 --> 01:04:35,680
here is drawing

955
01:04:37,010 --> 01:04:37,910
fifty samples

956
01:04:38,610 --> 01:04:40,450
from the green distribution

957
01:04:41,030 --> 01:04:43,450
for every one of them given that coordinates

958
01:04:44,550 --> 01:04:46,430
this uniformly distributed between

959
01:04:46,890 --> 01:04:47,840
the horizontal axis

960
01:04:50,240 --> 01:04:52,990
it means that all those points shown in blue yellow

961
01:04:53,610 --> 01:04:56,070
those points are uniformly distributed in the area

962
01:04:56,630 --> 01:04:57,430
and the degree

963
01:04:58,550 --> 01:04:59,590
that's what this construction

964
01:05:00,360 --> 01:05:00,970
one step

965
01:05:02,410 --> 01:05:05,090
get to a point is scored x u

966
01:05:05,880 --> 01:05:07,060
uniform and the green

967
01:05:07,970 --> 01:05:11,320
many uses and you cut out all ones that have a low

968
01:05:11,890 --> 01:05:13,160
just on the the red curves

969
01:05:14,050 --> 01:05:17,430
those points must be uniformly distributed in the area of the right

970
01:05:18,140 --> 01:05:20,090
so you can imagine railway all yellow ones

971
01:05:20,570 --> 01:05:21,530
one you think

972
01:05:22,010 --> 01:05:24,160
exactly from registration

973
01:05:25,200 --> 01:05:25,410
you know

974
01:05:27,220 --> 01:05:28,260
that's rejection sampling

975
01:05:30,010 --> 01:05:33,010
and it's absolutely crucial that must be true in situ

976
01:05:33,510 --> 01:05:36,890
is bigger than or equal to be star because

977
01:05:38,590 --> 01:05:40,010
it's not true as shown by

978
01:05:40,760 --> 01:05:41,320
this example

979
01:05:41,820 --> 01:05:45,910
and if you draw points and pretend that you're doing rejection sampling you will reject

980
01:05:45,930 --> 01:05:51,370
the yellow points to the points now the distribution q actually drawing from some strange

981
01:05:51,370 --> 01:05:54,550
distribution that is the red shirt or something like that

982
01:05:55,110 --> 01:05:55,530
in you

983
01:05:56,820 --> 01:06:01,140
and so it's crucial that you must be an oracle start

984
01:06:01,640 --> 01:06:05,570
and that these are is complicated distribution is a little bit of understand

985
01:06:06,260 --> 01:06:07,510
it may be a very difficult

986
01:06:09,200 --> 01:06:11,360
that you've got a few which this is true

987
01:06:13,490 --> 01:06:17,280
it's not just that you have to you know that is true for some something to happen

988
01:06:17,490 --> 01:06:20,300
say that it is true for a particular you need sea

989
01:06:20,740 --> 01:06:22,760
the algorithm to you

990
01:06:23,340 --> 01:06:24,320
but i see it

991
01:06:24,950 --> 01:06:25,950
get uniform

992
01:06:34,640 --> 01:06:37,720
okay so the comment is it's very easy to check

993
01:06:38,490 --> 01:06:39,680
whether see

994
01:06:40,640 --> 01:06:43,930
is bigger and start with the places where you actually true

995
01:06:44,410 --> 01:06:45,910
points and that's absolutely true

996
01:06:46,390 --> 01:06:47,110
so you could

997
01:06:50,280 --> 01:06:50,800
not only

998
01:06:52,320 --> 01:06:55,530
drawing you could also see if you had one

999
01:06:55,990 --> 01:06:56,610
then would

1000
01:07:00,140 --> 01:07:02,780
sorry if you equal see times due

1001
01:07:03,700 --> 01:07:06,220
would still be less than so you can very easily checked

1002
01:07:07,300 --> 01:07:08,160
true that is true error

1003
01:07:09,160 --> 01:07:09,570
thank you

1004
01:07:09,890 --> 01:07:14,720
so yes you can check that's not satisfactory on column that you really want theorem

1005
01:07:14,720 --> 01:07:17,550
that says if you run this algorithm or long enough

1006
01:07:18,680 --> 01:07:19,450
get the right answer

1007
01:07:20,200 --> 01:07:22,220
so just checking is good enough

1008
01:07:24,110 --> 01:07:27,890
okay and here again is the estimate of the expected value

1009
01:07:29,450 --> 01:07:31,610
all the methods we saw earlier with using

1010
01:07:33,240 --> 01:07:34,180
importance sampling

1011
01:07:35,090 --> 01:07:39,510
and then in green answer all rejection sampling which is

1012
01:07:40,110 --> 01:07:41,970
growing from the correct distribution

1013
01:07:42,450 --> 01:07:43,110
and you can see it

1014
01:07:43,370 --> 01:07:46,970
so settling down as well in the same way as the other three

1015
01:07:50,450 --> 01:07:51,910
the other any questions about

1016
01:07:53,010 --> 01:07:54,180
importance sampling or

1017
01:07:55,450 --> 01:07:56,320
rejection sampling

1018
01:08:02,760 --> 01:08:07,930
all assembled into the assumption is that q is a distribution that we can draw from so

1019
01:08:08,610 --> 01:08:08,860
and they

1020
01:08:09,450 --> 01:08:11,180
real space it might be a guassian question

1021
01:08:12,070 --> 01:08:14,840
or it might be a mixture of gaussians you've invested in some way

1022
01:08:15,930 --> 01:08:20,280
in any case this means the sort of thing you could use that to

1023
01:08:20,840 --> 01:08:21,360
it might be

1024
01:08:21,840 --> 01:08:28,390
the distribution where each state end things in the magnet just flip so biased coin

1025
01:08:28,550 --> 01:08:31,030
some sort of assignment or the

1026
01:08:31,220 --> 01:08:31,720
points of

1027
01:08:32,050 --> 01:08:33,220
independently of each other

1028
01:08:33,640 --> 01:08:35,110
and that's a very easy distribution

1029
01:08:36,030 --> 01:08:37,780
that's the sort of human use

1030
01:08:39,630 --> 01:08:42,640
distribution that doesn't have strong correlations it

1031
01:08:43,280 --> 01:08:46,410
what it does have correlations in it among variables

1032
01:08:47,200 --> 01:08:51,220
simple correlation simple enough that you can still solve the problem drawing from

1033
01:08:53,180 --> 01:08:54,800
thank you

1034
01:08:55,490 --> 01:08:59,590
so rejection sampling is wonderful usually you can't do it

1035
01:09:00,090 --> 01:09:01,640
and you will be two reasons why

1036
01:09:02,410 --> 01:09:09,530
is that you can't think about few such that's you can it because you don't understand it well enough

1037
01:09:10,820 --> 01:09:12,700
or if you can come up with such

1038
01:09:15,320 --> 01:09:17,220
which you have proof this property

1039
01:09:18,140 --> 01:09:26,640
it may be so much higher than that you end up rejecting every single point just run never point

1040
01:09:27,200 --> 01:09:29,340
because you're only in this area

1041
01:09:29,950 --> 01:09:32,160
four agreeing thing that so much higher

1042
01:09:32,630 --> 01:09:32,990
and the red

1043
01:09:35,610 --> 01:09:40,490
so that's the sad truth about rejecting something that unless you're very lucky or very clever

1044
01:09:41,220 --> 01:09:43,610
you won't actually the age set

1045
01:09:43,860 --> 01:09:45,820
he rejection sampling a real problem

1046
01:09:47,640 --> 01:09:50,990
in one dimensional problems like the ones we have on the screen now

1047
01:09:51,510 --> 01:09:55,450
you can realistically do rejection sampling and one approach to

1048
01:09:56,510 --> 01:09:58,050
the methods are about to discuss

1049
01:09:59,340 --> 01:10:00,140
which i metropolis

1050
01:10:00,140 --> 01:10:02,830
i had been all but forgotten

1051
01:10:02,840 --> 01:10:04,880
students around princeton

1052
01:10:04,890 --> 01:10:09,390
i knew him only as the phantom five hall

1053
01:10:10,030 --> 01:10:16,130
silent goes like figure who scroll mysterious messages

1054
01:10:16,150 --> 01:10:21,390
on the blackboards of princeton's math and physics buildings

1055
01:10:21,400 --> 01:10:22,960
and outside

1056
01:10:22,970 --> 01:10:24,790
a small circle

1057
01:10:24,810 --> 01:10:26,970
of mathematicians

1058
01:10:26,980 --> 01:10:29,510
even people like me

1059
01:10:30,210 --> 01:10:36,400
i had heard about his work simply assumed he was dead

1060
01:10:36,410 --> 01:10:38,130
so naturally

1061
01:10:38,140 --> 01:10:39,080
i was

1062
01:10:39,080 --> 01:10:44,210
very intrigued to learn that she was alive

1063
01:10:44,220 --> 01:10:46,790
he seemed to be well

1064
01:10:46,800 --> 01:10:51,370
and he might now win the nobel prize semidefi phone calls

1065
01:10:51,390 --> 01:10:52,590
to me

1066
01:10:52,610 --> 01:10:53,750
the notion

1067
01:10:53,760 --> 01:10:55,680
that someone

1068
01:10:55,690 --> 01:10:58,460
it's someone who had been lost

1069
01:10:58,510 --> 01:11:00,570
for so long

1070
01:11:00,630 --> 01:11:02,700
could be found again

1071
01:11:02,710 --> 01:11:05,630
it's someone who had fallen

1072
01:11:05,660 --> 01:11:08,810
as far as john nash had fallen

1073
01:11:08,830 --> 01:11:10,970
could come back

1074
01:11:10,990 --> 01:11:12,050
to me

1075
01:11:12,060 --> 01:11:17,210
it seems extraordinary it sounded like a greek myths

1076
01:11:18,330 --> 01:11:23,460
fairy tale

1077
01:11:23,490 --> 01:11:26,250
extraordinary lives

1078
01:11:26,310 --> 01:11:29,070
i have this unique

1079
01:11:29,120 --> 01:11:30,700
unique our

1080
01:11:30,710 --> 01:11:32,170
john nash

1081
01:11:32,180 --> 01:11:33,830
who someone

1082
01:11:36,370 --> 01:11:40,140
thinking thinking all always thinking

1083
01:11:40,200 --> 01:11:41,490
is a genius

1084
01:11:41,500 --> 01:11:44,990
most of us are not most of our lives

1085
01:11:45,040 --> 01:11:50,080
don't contain the elements of genius madness

1086
01:11:50,110 --> 01:11:54,870
reawakening and nobel yet paradoxically

1087
01:11:54,880 --> 01:11:56,950
his trials

1088
01:11:56,960 --> 01:11:59,640
and triumphs

1089
01:11:59,670 --> 01:12:01,920
the trials and triumphs of this

1090
01:12:01,930 --> 01:12:10,210
very unique individual have nonetheless resonated with millions and millions of people

1091
01:12:10,210 --> 01:12:14,060
around the world

1092
01:12:14,090 --> 01:12:19,430
i want because because many of you in this room mathematicians i just want to

1093
01:12:19,430 --> 01:12:21,050
read you a letter

1094
01:12:21,070 --> 01:12:24,580
john to illustrate this point the john nash

1095
01:12:24,660 --> 01:12:27,530
got a few weeks ago

1096
01:12:27,570 --> 01:12:29,690
dear mister nash

1097
01:12:31,360 --> 01:12:33,210
i am nine years old

1098
01:12:33,250 --> 01:12:36,050
my name is at still so

1099
01:12:36,090 --> 01:12:37,690
i am a girl

1100
01:12:37,790 --> 01:12:40,140
i really admire you

1101
01:12:40,170 --> 01:12:43,350
you are my are all el al model

1102
01:12:43,410 --> 01:12:45,450
for a lot of things

1103
01:12:45,460 --> 01:12:51,610
i think the smartest person who ever lived i really wish to be like you

1104
01:12:51,670 --> 01:12:55,890
i would love to study maths

1105
01:12:55,910 --> 01:12:57,420
the only problem

1106
01:12:57,460 --> 01:12:58,660
is that

1107
01:12:58,700 --> 01:13:01,810
i'm not very good at math

1108
01:13:01,840 --> 01:13:03,800
i can do it

1109
01:13:03,840 --> 01:13:05,320
i like it

1110
01:13:05,330 --> 01:13:08,930
i'm just not very good at it

1111
01:13:08,970 --> 01:13:12,350
was that what it was like when you were a kid

1112
01:13:12,360 --> 01:13:15,260
have you always been good at math

1113
01:13:15,290 --> 01:13:18,200
please write back love ali

1114
01:13:19,230 --> 01:13:21,330
i love your name

1115
01:13:22,890 --> 01:13:28,730
act one of john nash is true

1116
01:13:28,790 --> 01:13:33,160
it is a classic coming-of-age story

1117
01:13:33,170 --> 01:13:35,640
john nash was born in

1118
01:13:35,670 --> 01:13:37,760
blue field west virginia

1119
01:13:37,820 --> 01:13:40,040
in the blue ridge mountains

1120
01:13:40,080 --> 01:13:42,960
he was born on the eve of the great depression

1121
01:13:43,000 --> 01:13:46,800
he was a peculiar

1122
01:13:46,810 --> 01:13:52,130
solitary intellectually precocious little boy

1123
01:13:52,160 --> 01:13:54,580
other children nicknamed him

1124
01:13:54,600 --> 01:13:57,260
but brains

1125
01:13:57,280 --> 01:13:58,700
i had ideas

1126
01:13:58,710 --> 01:14:00,250
he said later

1127
01:14:00,310 --> 01:14:03,430
i had ideas but they were sort of buggy

1128
01:14:03,450 --> 01:14:05,540
we're not perfectly sound

1129
01:14:05,630 --> 01:14:10,660
at ten john nash was doing rather sophisticated

1130
01:14:10,710 --> 01:14:12,740
chemical experiments

1131
01:14:12,860 --> 01:14:15,780
and tricking other children

1132
01:14:15,790 --> 01:14:19,340
by shocking them with electrical currents

1133
01:14:19,360 --> 01:14:21,710
at fifteen he was

1134
01:14:21,710 --> 01:14:23,540
this common sense

1135
01:14:24,670 --> 01:14:25,900
what is

1136
01:14:25,920 --> 01:14:30,780
the cyc ontology are subsite knowledge base so this would be kind of synonyms cyc

1137
01:14:30,780 --> 01:14:33,370
knowledge base cyc ontology

1138
01:14:34,070 --> 01:14:36,610
this is

1139
01:14:36,660 --> 01:14:42,110
o an ontology you can see these in many different ways but one possible visualisation

1140
01:14:42,110 --> 01:14:42,990
this would be

1141
01:14:43,000 --> 01:14:44,680
following like three

1142
01:14:44,710 --> 01:14:47,560
of concepts and these three

1143
01:14:49,450 --> 01:14:51,610
three hundred thousand concepts which is

1144
01:14:51,670 --> 01:14:53,310
a lot

1145
01:14:53,330 --> 01:15:01,500
and these concepts are connected weights fifteen thousand relationships if you remember before his wordnet

1146
01:15:01,500 --> 01:15:05,670
was roughly hundred thousand concepts connected with weights

1147
01:15:05,730 --> 01:15:10,560
twenty six relationships and it's already very rich while cyc

1148
01:15:10,570 --> 01:15:12,520
because numbers might be

1149
01:15:12,570 --> 01:15:14,220
and so

1150
01:15:15,340 --> 01:15:17,730
this concepts and relationships are

1151
01:15:17,800 --> 01:15:22,660
the weights more than three three four million

1152
01:15:23,620 --> 01:15:26,030
actually effects which are connected

1153
01:15:26,070 --> 01:15:27,370
into the

1154
01:15:27,400 --> 01:15:29,740
on the bottom of the

1155
01:15:29,770 --> 01:15:31,900
this conception three

1156
01:15:32,130 --> 01:15:37,010
we'll see examples of this

1157
01:15:37,020 --> 01:15:43,860
so the whole ontology is expressed in first-order and higher-order logics then it has a

1158
01:15:43,870 --> 01:15:50,890
special mechanism for so called contextual logic and so on so it's pretty strong

1159
01:15:50,950 --> 01:15:56,850
mechanism but the main formalism is logic

1160
01:15:56,910 --> 01:16:03,110
places all mean let's say so at the top we have a concept think

1161
01:16:03,120 --> 01:16:08,830
which so everything is sub concept of thing and then

1162
01:16:08,850 --> 01:16:13,010
we have the space part-time by imparting thinks

1163
01:16:14,030 --> 01:16:18,480
part of living things are also human beings and it only human beings because different

1164
01:16:18,480 --> 01:16:22,900
aspects of human beings and so on so this is a structured routine very detailed

1165
01:16:23,170 --> 01:16:28,120
way i will show some of these course concepts and later

1166
01:16:31,410 --> 01:16:34,530
so what is roughly the structure of the cyc ontology

1167
01:16:36,660 --> 01:16:39,550
on the top we have the so-called upper ontology

1168
01:16:39,550 --> 01:16:44,300
and these are really abstract concepts which she would correspond to a philosophical

1169
01:16:44,300 --> 01:16:52,810
notions how to organize how to organize the word tends want and relationships between different

1170
01:16:53,120 --> 01:16:54,600
things and so on

1171
01:16:54,680 --> 01:17:00,070
then below this we have something which they called courtier is like space causality and

1172
01:17:00,080 --> 01:17:05,580
so on and then below this we have more domain specific stuff

1173
01:17:05,600 --> 01:17:08,930
on the bottom effects which are

1174
01:17:08,950 --> 01:17:10,850
the actual facts of life

1175
01:17:10,870 --> 01:17:16,760
instances which are connected either to the domain specific or core theories

1176
01:17:16,780 --> 01:17:18,830
as a

1177
01:17:19,070 --> 01:17:21,490
in upper ontology would be things like thing

1178
01:17:22,050 --> 01:17:27,570
what is individual temporal think events and so on while that's a courtier is would

1179
01:17:27,570 --> 01:17:30,280
already have rules like this

1180
01:17:30,310 --> 01:17:32,700
for all events a and b

1181
01:17:32,700 --> 01:17:34,890
if a causes b

1182
01:17:35,010 --> 01:17:38,660
implies a precedes b for instance this rule which

1183
01:17:39,450 --> 01:17:40,970
it's through four

1184
01:17:41,300 --> 01:17:43,160
any situation in the

1185
01:17:43,220 --> 01:17:45,240
it's normal this

1186
01:17:45,260 --> 01:17:46,970
which we know

1187
01:17:48,910 --> 01:17:51,680
if you go to some other

1188
01:17:51,720 --> 01:17:55,780
mystic stories then maybe this wouldn't be true anymore

1189
01:17:55,780 --> 01:17:58,580
that's why this is quite common sense

1190
01:17:59,800 --> 01:18:04,780
they they also have this mechanism for contextualizing knowledge and let's say that try to

1191
01:18:04,780 --> 01:18:06,030
model also

1192
01:18:06,050 --> 01:18:10,080
greek mythology than a lot of these things don't hold any more about it there's

1193
01:18:10,080 --> 01:18:12,800
a mechanism how to escape from these rules as well

1194
01:18:14,100 --> 01:18:17,740
and domain specific theory would be

1195
01:18:17,760 --> 01:18:22,140
so like this for any mammal m and any anthrax bacteria eight

1196
01:18:22,200 --> 01:18:26,350
if m is exposed to is a

1197
01:18:26,370 --> 01:18:31,120
this causes m to be infected by a reference this is a rule which was

1198
01:18:31,140 --> 01:18:35,390
he this domain specific theories

1199
01:18:35,430 --> 01:18:40,470
and the actual fact of life would be that john is a person infected by

1200
01:18:41,740 --> 01:18:45,260
and by having all this upper level knowledge we would know that john is a

1201
01:18:45,260 --> 01:18:46,620
person we know

1202
01:18:46,640 --> 01:18:52,240
very well the person can get infected it's an animal and so on and what

1203
01:18:52,240 --> 01:18:55,680
it means being affected and you know what the sentence and so on because all

1204
01:18:55,700 --> 01:19:01,140
this of all these upper-level levels of knowledge

1205
01:19:04,530 --> 01:19:06,760
OK and this is an example of how

1206
01:19:06,780 --> 01:19:11,800
this ontologies from psychiatry slating

1207
01:19:11,800 --> 01:19:13,660
text knowledge into

1208
01:19:13,680 --> 01:19:15,310
first order logic

1209
01:19:15,330 --> 01:19:20,180
so if we have a sentence like this so there a couple of examples from

1210
01:19:20,200 --> 01:19:22,640
this there is the main because in the last

1211
01:19:22,640 --> 01:19:25,180
yes you can imagine that they got some funding

1212
01:19:25,220 --> 01:19:27,680
this kind of the domain

1213
01:19:27,780 --> 01:19:31,830
so there's groups are capable of directing assassinations so this is

1214
01:19:32,010 --> 01:19:35,970
so into the so-called cyc least like

1215
01:19:35,990 --> 01:19:37,700
language but you can

1216
01:19:37,800 --> 01:19:40,490
basically first order logic

1217
01:19:40,550 --> 01:19:44,010
so if some x some group

1218
01:19:44,030 --> 01:19:49,800
it's a terrorist group this implies that they are behavior that this group is

1219
01:19:49,850 --> 01:19:56,640
which are capable assistant something direct cage so they basically these concepts from ontology

1220
01:19:56,660 --> 01:20:02,510
and relationships and this kind of rule this kind of centres gets translated in this

1221
01:20:02,510 --> 01:20:05,160
kind of rule and then later on it can be used

1222
01:20:05,160 --> 01:20:07,050
when doing increasing

1223
01:20:07,200 --> 01:20:09,510
or this sentence

1224
01:20:09,510 --> 01:20:15,080
for those who considers an agent an enemy that agent with the world that the

1225
01:20:15,080 --> 01:20:16,780
in that this

1226
01:20:16,930 --> 01:20:20,490
gets translated into such a state

1227
01:20:20,510 --> 01:20:24,580
this is mainly done by hand i mean especially translation of this kind of

1228
01:20:25,580 --> 01:20:28,280
well let's see the actual effects

1229
01:20:28,570 --> 01:20:34,280
which can be fired from that in news stories

1230
01:20:34,280 --> 01:20:38,330
OK this is based it then

1231
01:20:38,330 --> 01:20:40,550
said in this kind of people

1232
01:20:40,640 --> 01:20:46,490
based basedinregion alqaida of constant OK that's based on region of ghana in afghanistan are

1233
01:20:46,490 --> 01:20:49,470
the concepts in

1234
01:20:49,470 --> 01:20:51,140
in the database

1235
01:20:51,140 --> 01:20:55,530
and so one way how they want to go through the

1236
01:20:55,660 --> 01:20:57,180
comment news stories

1237
01:20:57,200 --> 01:21:02,990
then this major effects are getting translated in such logic form and then the system

1238
01:21:02,990 --> 01:21:07,720
is able to to reason with minutes later on i will show some examples

1239
01:21:07,720 --> 01:21:11,140
so this is an

1240
01:21:11,160 --> 01:21:12,550
another view how

1241
01:21:12,550 --> 01:21:14,070
concepts in

1242
01:21:14,070 --> 01:21:19,970
psychiatry presented as a concept for psychoanalysts

1243
01:21:19,990 --> 01:21:27,970
the several lexical representations like psychoanalyst psychoanalysts and so on and this is no hierarchy

1244
01:21:27,990 --> 01:21:30,070
all of these concepts so

1245
01:21:30,080 --> 01:21:32,030
so i find it is

1246
01:21:32,030 --> 01:21:38,220
sub concept of medical care professionals the further on health professions professional adult and professional

1247
01:21:38,220 --> 01:21:39,030
market this is one way

1248
01:21:39,470 --> 01:21:43,530
how we can abstract cycle and this another way how to

1249
01:21:43,550 --> 01:21:53,800
generalize this concept is this is psychologists scientist researcher presented patient person homo sapiens and

1250
01:21:53,850 --> 01:21:58,350
the logic of species and on the here we are pretty high up in the

1251
01:21:58,350 --> 01:22:02,720
in the hierarchy already so this is just one simple snapshot from the

1252
01:22:02,760 --> 01:22:04,390
from the database

1253
01:22:04,410 --> 01:22:09,440
and if you know that somebody other freud's psychoanalyst then we know immediately that it's

1254
01:22:09,440 --> 01:22:12,200
also homo sapiens and whatever holds for

1255
01:22:12,600 --> 01:22:19,830
homosapiens holds also for right the article is reasoning engine can can use this information

1256
01:22:19,830 --> 01:22:21,340
a factor of of two

1257
01:22:21,350 --> 01:22:25,420
we multiply in the correct way in the summation right and we get

1258
01:22:25,440 --> 01:22:29,350
a vector over four things right we call factor

1259
01:22:29,360 --> 01:22:32,660
that is the function basically if i signs the number

1260
01:22:32,710 --> 01:22:34,230
two each possibility

1261
01:22:34,240 --> 01:22:35,620
right this

1262
01:22:35,640 --> 01:22:37,340
factor doesn't need to have some two

1263
01:22:37,370 --> 01:22:41,900
set of probability doesn't necessarily sum to one in this case actually

1264
01:22:43,150 --> 01:22:45,410
if you know one

1265
01:22:45,460 --> 01:22:48,570
in this case it won't

1266
01:22:48,570 --> 01:22:52,590
but it's just nonnegative numbers right

1267
01:22:52,610 --> 01:22:57,030
so in the combined these two pieces and then further we just keep doing that

1268
01:22:57,030 --> 01:22:59,100
the inexorable push it in

1269
01:22:59,680 --> 01:23:02,790
until you can no longer do that

1270
01:23:02,790 --> 01:23:05,550
so here we also did was

1271
01:23:05,580 --> 01:23:08,110
switch the switch the variables

1272
01:23:08,140 --> 01:23:09,840
it's fine to write

1273
01:23:09,850 --> 01:23:16,280
so now we have this missionary and finally we have this factor to be

1274
01:23:16,300 --> 01:23:19,640
that's only ever be and what way that

1275
01:23:19,700 --> 01:23:25,440
it that we can't can't calculate and and you will calculate how many iterations this

1276
01:23:25,440 --> 01:23:30,670
took as opposed to how many iterations this to commonly classes and multiplies

1277
01:23:30,690 --> 01:23:33,190
yet huge wins from doing

1278
01:23:33,210 --> 01:23:36,000
from doing this because what you're doing is instead of

1279
01:23:36,010 --> 01:23:41,360
instead of doing this but this was for this year doing one multiply in one

1280
01:23:41,360 --> 01:23:43,900
edition instead of a whole bunch

1281
01:23:43,920 --> 01:23:45,770
OK so that's the that's the key

1282
01:23:45,770 --> 01:23:51,850
the idea that they are from these both variable elimination and

1283
01:23:51,860 --> 01:23:54,010
the junction tree algorithm

1284
01:23:55,000 --> 01:23:56,880
so we actually got into

1285
01:23:56,900 --> 01:24:02,410
graphical models yet this is just conditional independence assumptions and

1286
01:24:02,430 --> 01:24:06,250
how to help us do inference right so we don't say anything about graphical models

1287
01:24:06,250 --> 01:24:08,990
but you know we're going there so the

1288
01:24:09,010 --> 01:24:11,450
the variable elimination OK and so on

1289
01:24:11,520 --> 01:24:15,390
if you think about how long does it take this in general right it's done

1290
01:24:16,760 --> 01:24:21,350
so the size of these things we generated go along these things are fixed

1291
01:24:21,390 --> 01:24:27,160
right so we need to worry about is the size of these factors that we

1292
01:24:27,160 --> 01:24:28,730
can be carrying around right

1293
01:24:31,290 --> 01:24:36,710
basically that is the size of the largest intermediate factor

1294
01:24:36,730 --> 01:24:41,480
and we picked elimination order in

1295
01:24:41,560 --> 01:24:46,160
some ran away tell you what i did what i said i just picked one

1296
01:24:46,180 --> 01:24:51,410
different elimination order is going to produce different types of different sizes of these

1297
01:24:51,420 --> 01:24:54,830
intermediate factors if i chose to eliminate

1298
01:24:55,630 --> 01:24:57,330
the first

1299
01:24:57,340 --> 01:24:59,580
right this would be very unfortunate choice

1300
01:25:02,400 --> 01:25:10,760
t would get a factor well someone to produce with factor that includes and and

1301
01:25:10,770 --> 01:25:14,850
well and

1302
01:25:14,950 --> 01:25:20,020
ian be OK so i would have factor three things because if i eliminate a

1303
01:25:20,020 --> 01:25:22,770
i get to multiply those guys

1304
01:25:22,780 --> 01:25:25,560
this guy the guy everything that includes a

1305
01:25:25,640 --> 01:25:31,300
OK so then i get something that's over EB right through variables so b

1306
01:25:31,310 --> 01:25:33,580
a size eight right

1307
01:25:33,600 --> 01:25:35,320
these guys are also still

1308
01:25:35,330 --> 01:25:39,800
so more generally if you think you know the wrong order i do get some

1309
01:25:39,800 --> 01:25:41,810
factors are very large

1310
01:25:41,820 --> 01:25:47,390
and you picking the up one is NP hard but there are good heuristics for

1311
01:25:47,390 --> 01:25:49,440
picking reasonable it's so

1312
01:25:52,230 --> 01:25:54,610
if you wanted to get right

1313
01:25:54,630 --> 01:25:56,140
you know

1314
01:25:56,160 --> 01:25:57,840
basically grow

1315
01:25:57,950 --> 01:26:01,110
exponential with the number of variables inside

1316
01:26:04,910 --> 01:26:10,430
so we have haven't said what they look like but in graphical models

1317
01:26:10,450 --> 01:26:13,190
this these independencies in

1318
01:26:13,260 --> 01:26:18,850
these properties of know how big how expensive this this inference is going to be

1319
01:26:18,870 --> 01:26:22,090
it becomes obvious from the graphical structure

1320
01:26:22,130 --> 01:26:23,760
right so

1321
01:26:23,760 --> 01:26:26,280
but it conveys exactly what we need

1322
01:26:28,760 --> 01:26:31,460
right so so so what i'm going to what i'm what i'm getting at here

1323
01:26:31,460 --> 01:26:34,600
is that gas and process can be used to specify

1324
01:26:34,610 --> 01:26:36,640
a distribution over functions

1325
01:26:37,370 --> 01:26:38,780
and if we want to make

1326
01:26:38,790 --> 01:26:40,920
probabilistic inference about functions

1327
01:26:40,960 --> 01:26:45,160
then we better have a way to write down distributions of all these objects because

1328
01:26:45,160 --> 01:26:48,880
if you can do that then we can apply probability theory

1329
01:26:49,450 --> 01:26:51,740
and the gas process is exactly

1330
01:26:51,800 --> 01:26:54,840
the object that we use to to handle

1331
01:26:54,840 --> 01:27:00,460
distribution over functions to specify them and to manipulate

1332
01:27:00,470 --> 01:27:04,300
OK so the former definition here is because the process is a collection of random

1333
01:27:05,140 --> 01:27:08,310
any finite number of which have gotten distributions

1334
01:27:08,410 --> 01:27:13,800
mathematically there are a number of technicalities involved in having infinite collections of things

1335
01:27:13,840 --> 01:27:18,560
so if you strip mathematician and you might wonder you know is this is this

1336
01:27:18,560 --> 01:27:22,320
really well defined i'm not strict in general care about these things

1337
01:27:23,100 --> 01:27:24,510
but but that's why

1338
01:27:24,540 --> 01:27:27,960
so that's the mathematicians definition here right so they

1339
01:27:27,970 --> 01:27:32,090
try to avoid saying anything about infinity it's just a place to collect collection but

1340
01:27:32,090 --> 01:27:36,810
only any finite number of which have consistent gaussian which i haven't said anything about

1341
01:27:36,850 --> 01:27:38,820
things that might be

1342
01:27:40,680 --> 01:27:45,610
all right but we don't we don't actually we only need to worry about a

1343
01:27:45,640 --> 01:27:49,180
thing nothing will think that will happen here

1344
01:27:49,190 --> 01:27:51,930
so just like calcium distribution here

1345
01:27:51,940 --> 01:27:54,500
it's not specified fully specified by

1346
01:27:54,510 --> 01:27:55,710
i mean that

1347
01:27:55,790 --> 01:27:58,440
and the covariance matrix

1348
01:27:58,470 --> 01:27:59,720
i guess in process

1349
01:27:59,730 --> 01:28:00,670
it's just

1350
01:28:00,680 --> 01:28:06,130
specified in terms of the infinitely long versions of that so instead of having a

1351
01:28:06,130 --> 01:28:10,460
mean vector and have the gas process has and has an infinitely long

1352
01:28:10,510 --> 01:28:11,610
i mean that right

1353
01:28:11,620 --> 01:28:12,630
an infinite

1354
01:28:12,690 --> 01:28:15,090
the long vectors are functions

1355
01:28:15,140 --> 01:28:15,940
OK so

1356
01:28:15,980 --> 01:28:18,390
the gas has i mean function

1357
01:28:18,450 --> 01:28:23,240
and similarly it doesn't have the covariance matrix but has a covariance

1358
01:28:23,310 --> 01:28:28,080
matrix here which is an infinite by infinite dimensional matrix

1359
01:28:28,840 --> 01:28:32,830
that's not that's function with two arguments

1360
01:28:34,570 --> 01:28:36,780
so formally this way we can write down

1361
01:28:36,800 --> 01:28:39,670
something like this

1362
01:28:39,710 --> 01:28:40,710
all right

1363
01:28:40,730 --> 01:28:41,810
so now

1364
01:28:44,360 --> 01:28:46,890
at this point you might be a little worried because

1365
01:28:47,350 --> 01:28:50,160
you can actually write down long vector anywhere

1366
01:28:50,200 --> 01:28:54,010
so if this machinery that i'm going to develop would require you to write that

1367
01:28:54,010 --> 01:28:55,500
down your computer for example

1368
01:28:55,560 --> 01:28:57,060
then you know already that

1369
01:28:57,070 --> 01:29:01,380
you know you can do it exactly this to make things like very

1370
01:29:02,380 --> 01:29:05,690
turns out the things don't get very

1371
01:29:05,730 --> 01:29:09,580
and the reason why things will will will turn out OK

1372
01:29:09,580 --> 01:29:11,590
is what's known as the

1373
01:29:11,610 --> 01:29:14,450
what i call the marginalisation property

1374
01:29:15,080 --> 01:29:19,830
so basically it has to do with the fact that the marginal distribution of the

1375
01:29:19,830 --> 01:29:24,720
gaussians joint council is again accounts so if we if we had something

1376
01:29:24,780 --> 01:29:26,030
which is jointly

1377
01:29:26,500 --> 01:29:31,190
jointly has a gas distribution if we now marginalise out the distribution of y

1378
01:29:31,230 --> 01:29:33,260
then we get back the distribution of x

1379
01:29:33,280 --> 01:29:38,700
this is joint distribution was calcium than the distribution of x also began

1380
01:29:38,710 --> 01:29:43,710
and both x and y could be vectors in this case

1381
01:29:44,360 --> 01:29:45,220
so now

1382
01:29:45,720 --> 01:29:50,960
so let's pretend that we now have a full gaussianprocess

1383
01:29:50,970 --> 01:29:52,050
right so we have

1384
01:29:52,060 --> 01:29:55,930
we have an infinite and infinitely large

1385
01:29:56,070 --> 01:29:58,250
long vector here right

1386
01:29:58,960 --> 01:30:01,130
the joint distribution over

1387
01:30:01,140 --> 01:30:03,230
let's say we had

1388
01:30:03,240 --> 01:30:07,430
so we had a number of of points are actually interested in

1389
01:30:07,440 --> 01:30:09,170
and they had all the other points

1390
01:30:09,180 --> 01:30:10,250
why here

1391
01:30:11,210 --> 01:30:14,760
now if you write down the joint distribution of this that have a mean where

1392
01:30:14,780 --> 01:30:18,580
the with the mean of the x-factor will be a and the mean of

1393
01:30:18,610 --> 01:30:19,990
why that to be

1394
01:30:20,000 --> 01:30:24,620
the the covariances between axis will be a and the cross covariances will begin by

1395
01:30:26,310 --> 01:30:28,220
so we're where this thing here

1396
01:30:28,230 --> 01:30:32,450
if this is the gas process and this thing here will be will be infinitely

1397
01:30:32,450 --> 01:30:36,230
large matrix is symmetric

1398
01:30:36,240 --> 01:30:39,460
but the nice thing here is that the marginal distribution for x is just going

1399
01:30:39,460 --> 01:30:40,170
to be

1400
01:30:40,180 --> 01:30:44,610
and all girls in distribution with mean and covariance matrix capital

1401
01:30:44,960 --> 01:30:46,610
so it means

1402
01:30:46,670 --> 01:30:50,660
if you if you ask finite dimensional questions

1403
01:30:51,820 --> 01:30:53,810
this infinite dimensional objects

1404
01:30:53,850 --> 01:30:57,690
then you get if you get some get finite dimensional right

1405
01:30:57,710 --> 01:31:00,240
the finite dimensional and so doesn't

1406
01:31:00,300 --> 01:31:03,910
depend on all these things are going on all the all the corners of this

1407
01:31:03,920 --> 01:31:06,250
very very long time

1408
01:31:06,250 --> 01:31:13,820
joint distributions using a set of local probability relationships are local distributions that are specified

1409
01:31:13,820 --> 01:31:16,020
actually on the graph

1410
01:31:16,040 --> 01:31:21,280
and the way we manipulate these graphs is we draw one node in the graph

1411
01:31:21,280 --> 01:31:23,920
for every random variables in our problem

1412
01:31:23,930 --> 01:31:30,430
and then edges between those nodes tell us something about the conditional independence assumptions and

1413
01:31:30,430 --> 01:31:36,080
actually the way these things work is relatively intuitive i can just draw you a

1414
01:31:36,200 --> 01:31:37,640
quick picture here

1415
01:31:39,510 --> 01:31:41,210
i can show you

1416
01:31:41,220 --> 01:31:43,950
this work so let's say there are four

1417
01:31:43,970 --> 01:31:47,300
variables in our domain x one

1418
01:31:47,340 --> 01:31:48,520
x two

1419
01:31:48,530 --> 01:31:50,820
x three and x four

1420
01:31:50,830 --> 01:31:57,130
in directed style of graphical models when you draw a picture like this what you're

1421
01:31:57,130 --> 01:31:59,030
really saying is

1422
01:31:59,050 --> 01:32:05,060
you're telling me how you want to factorize the joint distribution across x one through

1423
01:32:05,060 --> 01:32:10,890
x four by using this picture and intuitively if you have some variable

1424
01:32:10,900 --> 01:32:14,520
that's the parents of another variable in the graph

1425
01:32:14,540 --> 01:32:17,150
then the child variables

1426
01:32:17,200 --> 01:32:22,320
can decide its distribution by looking only at its parents so in this picture here

1427
01:32:22,330 --> 01:32:26,850
that i drew the idea is that first we choose a value for x one

1428
01:32:26,870 --> 01:32:31,270
and then given the i four x one we choose a value for x two

1429
01:32:31,350 --> 01:32:36,100
x three x three needs to only consulted parent x one and x two needs

1430
01:32:36,110 --> 01:32:38,850
only consulted parents which is also one

1431
01:32:38,860 --> 01:32:42,710
and then given x two and x three which is about four x four x

1432
01:32:42,710 --> 01:32:47,390
one needs to only consulted which is so this picture

1433
01:32:47,410 --> 01:32:52,830
is a way of specifying a factorisation of the joint distribution which is the following

1434
01:32:54,530 --> 01:33:03,020
so this is just the joint distribution between all those things and

1435
01:33:03,040 --> 01:33:06,170
this picture specifies the following factorisation

1436
01:33:06,180 --> 01:33:08,770
p px x one times

1437
01:33:08,820 --> 01:33:11,770
p of x two given x one

1438
01:33:11,790 --> 01:33:14,430
the next three given x one

1439
01:33:14,440 --> 01:33:17,410
and then p of x four different two

1440
01:33:17,460 --> 01:33:27,300
so this picture is the quote which communicates to you how i choose to factorize

1441
01:33:27,300 --> 01:33:29,540
this joint distribution

1442
01:33:30,180 --> 01:33:37,570
and the code for the complete independence factorizations is just to have no errors in

1443
01:33:37,570 --> 01:33:39,090
your graph

1444
01:33:40,070 --> 01:33:42,040
so if i restored heroes

1445
01:33:42,050 --> 01:33:45,370
then that would be my way of telling you that i wanted to factorize the

1446
01:33:45,370 --> 01:33:49,980
joint distribution not like this but just like this one

1447
01:33:50,000 --> 01:33:51,580
and people two

1448
01:33:51,740 --> 01:33:54,180
times t three

1449
01:33:54,360 --> 01:33:57,560
the next

1450
01:34:03,070 --> 01:34:05,410
there's also a cold

1451
01:34:05,420 --> 01:34:08,730
four specifying no assumptions at all

1452
01:34:08,750 --> 01:34:12,510
in year distribution in other words that you don't want to make any factorisation assumptions

1453
01:34:12,510 --> 01:34:16,790
you want to represent the joint as the most general possible joint distribution that you

1454
01:34:17,840 --> 01:34:21,720
because they want to take a guess at what the code looks like

1455
01:34:21,730 --> 01:34:24,340
i can help you out by drawing the picture

1456
01:34:24,360 --> 01:34:25,450
like this

1457
01:34:25,520 --> 01:34:29,970
you can also achieve nothing by looking ahead announce if your

1458
01:34:29,990 --> 01:34:31,890
we should do that

1459
01:34:33,900 --> 01:34:36,860
so what is the

1460
01:34:36,870 --> 01:34:39,880
how do you represent a completely general distribution

1461
01:34:39,890 --> 01:34:42,030
well you know that by the law

1462
01:34:42,040 --> 01:34:47,300
of conditional probability you can always write this distribution no matter what the distribution is

1463
01:34:47,590 --> 01:34:49,670
you can always write it

1464
01:34:49,720 --> 01:34:55,880
in a change so by saying it's marginal over x one and then some conditional

1465
01:34:55,890 --> 01:35:00,260
x two given x one x three given everything that comes before

1466
01:35:00,310 --> 01:35:02,190
x two and x one

1467
01:35:02,210 --> 01:35:03,920
and x four

1468
01:35:03,930 --> 01:35:09,520
given everything that came before x three x two

1469
01:35:09,560 --> 01:35:14,270
so what i wrote this equation i didn't make any assumptions about this distribution

1470
01:35:14,280 --> 01:35:20,610
unlike the previous equations derived which were strong factorizations assumptions here any distribution the joint

1471
01:35:20,610 --> 01:35:24,490
distribution can always be written this way by the law of conditional probability

1472
01:35:24,510 --> 01:35:29,410
so the graph corresponds to this is just the graph in which each node has

1473
01:35:29,410 --> 01:35:32,760
all the nodes to its left is its parents

1474
01:35:39,450 --> 01:35:40,620
and so on

1475
01:35:40,630 --> 01:35:42,490
OK so

1476
01:35:42,510 --> 01:35:45,880
by drawing these pictures i can communicate to you

1477
01:35:45,890 --> 01:35:49,670
what assumptions i want to make about the joint distribution all the way from the

1478
01:35:49,670 --> 01:35:55,920
completely factorized distribution which is a graph with no edges to be fully general arbitrary

1479
01:35:55,920 --> 01:35:59,620
degree distribution which is a graph that has many many edges

1480
01:36:00,740 --> 01:36:06,620
so these graphical models are also known as bayes nets or belief nets for bayesian

1481
01:36:06,620 --> 01:36:08,320
belief nets or

1482
01:36:08,430 --> 01:36:10,260
depending on exactly

1483
01:36:10,280 --> 01:36:11,900
you know how

1484
01:36:11,920 --> 01:36:17,180
what historical figure looking at and the the history here is that judea pearl was

1485
01:36:17,180 --> 01:36:22,950
the statistician at UCLA really deserves most of the credit for bringing these graphical models

1486
01:36:22,950 --> 01:36:29,210
into artificial intelligence and later machine learning he wrote a book called probabilistic inference and

1487
01:36:29,210 --> 01:36:35,000
intelligent systems in which he goes to the exactly the probabilistic approach to IR and

1488
01:36:35,000 --> 01:36:38,620
why you need joint distributions in these graphical models which at the time he wrote

1489
01:36:38,630 --> 01:36:42,870
the book were virtually unknown and lots of people ridiculed him for writing this sort

1490
01:36:42,870 --> 01:36:47,030
of byzantium book about this weird area of graph theory the obviously has nothing to

1491
01:36:47,030 --> 01:36:52,610
do with and now graphical models like dominated huge part of statistical

1492
01:36:52,630 --> 01:36:54,510
artificial intelligence

1493
01:36:54,530 --> 01:36:58,700
there's a lot of credit due to judea pearl's writing that very seminal book at

1494
01:36:58,700 --> 01:37:03,540
the time in which you those ideas were not talk today is also

1495
01:37:03,590 --> 01:37:10,290
unhappily the father of the journalist any pearl was killed in the middle east six

1496
01:37:10,290 --> 01:37:14,320
or seven years ago so if you do that think that's the same thing

1497
01:37:14,370 --> 01:37:16,810
there's a question here

1498
01:37:21,400 --> 01:37:26,910
exactly that's exactly right so everyone here the question the question was well you can

1499
01:37:26,910 --> 01:37:30,760
write this factorisation by ordering the variables in any way you want

1500
01:37:30,780 --> 01:37:31,650
and so

1501
01:37:31,690 --> 01:37:35,230
does it mean that there could be multiple graphs which are equivalent and the answer

1502
01:37:35,230 --> 01:37:36,320
is yes

1503
01:37:36,340 --> 01:37:41,030
any graph specifies the family of distributions based on the assumption is that the graph

1504
01:37:41,030 --> 01:37:46,030
encodes but there might be several different graphs which all encode the same set of

1505
01:37:46,030 --> 01:37:49,220
assumptions and thus specified same families an excellent

1506
01:37:51,580 --> 01:37:55,090
OK so there are two kinds of graphical models here i just told you about

1507
01:37:55,100 --> 01:37:58,890
the directed kind there's also an undirected kind of model

1508
01:37:58,900 --> 01:38:01,400
in the undirected graphical model

1509
01:38:02,010 --> 01:38:09,790
assumptions are a little bit different assumptions are that the distribution is specified as the

1510
01:38:09,810 --> 01:38:15,190
product of potentials that live on the cliques of the graph

1511
01:38:15,210 --> 01:38:18,560
there's a lot of words in that sentence so what you say it again

1512
01:38:18,570 --> 01:38:21,810
a little bit more slowly in undirected models

1513
01:38:21,870 --> 01:38:24,380
what you do is you look at the graph

1514
01:38:24,390 --> 01:38:29,260
which is the same thing as in directed models that code for me to communicate

1515
01:38:29,260 --> 01:38:33,570
to you what factorisation i want to make this distribution so x

1516
01:38:33,570 --> 01:38:36,720
one that has the greatest variety of things

1517
01:38:36,740 --> 01:38:42,680
OK so let me just mention some applications just to give you a flavour of

1518
01:38:42,680 --> 01:38:43,990
what can be done

1519
01:38:44,320 --> 01:38:48,800
with probabilistic logical models and within moments in particular

1520
01:38:48,800 --> 01:38:50,910
so as christian already mentioned

1521
01:38:50,930 --> 01:38:55,220
srl in general animals in particular have been used for a whole slew of things

1522
01:38:56,090 --> 01:38:58,950
in just about every domain that you care to imagine

1523
01:38:58,950 --> 01:39:00,950
the one that i'm going to focus on here

1524
01:39:00,970 --> 01:39:03,430
is information extraction

1525
01:39:03,430 --> 01:39:07,530
so again this is the example that that christian had our goal is to first

1526
01:39:07,530 --> 01:39:09,990
of all segment these citations

1527
01:39:09,990 --> 01:39:12,150
in two authors titles and venues

1528
01:39:12,170 --> 01:39:14,800
and then you know do the into resolution

1529
01:39:14,820 --> 01:39:20,570
right information extraction has two basic steps segmentation and into resolution into resolution is where

1530
01:39:20,570 --> 01:39:23,880
you realize that crossing was the same as saying that he

1531
01:39:23,930 --> 01:39:27,740
and tripoli as it's the same thing as proceedings of the twenty first national conference

1532
01:39:27,740 --> 01:39:28,530
on AI

1533
01:39:28,570 --> 01:39:31,740
notice that this is not easy to do right because the strings look completely different

1534
01:39:31,740 --> 01:39:34,780
in this case but we're going to be able to do this

1535
01:39:34,800 --> 01:39:37,610
and then you also realize that this will papers the same thing as the whole

1536
01:39:38,860 --> 01:39:41,720
and now the state of the art

1537
01:39:41,720 --> 01:39:44,130
in into resolution

1538
01:39:44,130 --> 01:39:47,340
is to use an HMM or CRF for segmentation

1539
01:39:47,360 --> 01:39:52,220
by using it to assign each token to a field so author author author title

1540
01:39:52,220 --> 01:39:54,360
title venue evaluates

1541
01:39:54,380 --> 01:39:59,030
and then into resolution is done by using a classifier like logistic regression naive bayes

1542
01:39:59,450 --> 01:40:04,220
to predict for every pair fields or citations whether the same or not

1543
01:40:04,280 --> 01:40:09,030
and then finally need the transitive closure step right because if if you're classifier says

1544
01:40:09,030 --> 01:40:12,150
that is the same as b and is the same as c

1545
01:40:12,200 --> 01:40:14,030
that is the same as

1546
01:40:14,030 --> 01:40:19,260
now if you want to build your own information extraction system like people typically the

1547
01:40:19,260 --> 01:40:22,050
industry these days using a language like c java

1548
01:40:22,090 --> 01:40:26,050
you probably run into the tens of thousands of lines of code and taking weeks

1549
01:40:26,050 --> 01:40:28,030
to do in the body

1550
01:40:28,030 --> 01:40:32,110
in markov logic it's just seven form

1551
01:40:32,130 --> 01:40:35,170
it takes you know half an hour to write them down and actually the seven

1552
01:40:35,170 --> 01:40:36,720
from fit on this slide

1553
01:40:36,800 --> 01:40:40,820
and that's what i'm going to show next

1554
01:40:40,840 --> 01:40:42,840
so first of all you can

1555
01:40:42,860 --> 01:40:46,470
so this is actually what you would inputting in your mln file that you then

1556
01:40:46,470 --> 01:40:47,340
you know

1557
01:40:47,360 --> 01:40:49,880
put into or commit to learn weights and do inference

1558
01:40:49,910 --> 01:40:52,280
you can have optionally e

1559
01:40:52,320 --> 01:40:54,450
type definitions

1560
01:40:54,470 --> 01:40:57,720
i'm putting them here for clarity but usually can just further from the that's so

1561
01:40:57,720 --> 01:40:59,380
that we have to do this part

1562
01:40:59,400 --> 01:41:03,010
so going have talk and field citation in position

1563
01:41:03,070 --> 01:41:06,990
tokens are the words that appear fields are in this case five thousand anybody could

1564
01:41:06,990 --> 01:41:10,090
be city organization person et cetera

1565
01:41:10,110 --> 01:41:13,630
and all citations and positions are the obvious things

1566
01:41:13,650 --> 01:41:17,030
and then this predicate next thing need to find the predicates

1567
01:41:17,680 --> 01:41:21,970
so this predicate tokeniser evidence predicates is that this talk and appears in this position

1568
01:41:22,070 --> 01:41:23,610
the citation

1569
01:41:23,630 --> 01:41:26,570
and then these predicates of the query predicates

1570
01:41:26,570 --> 01:41:29,320
the infield predicate there's the segmentation

1571
01:41:29,340 --> 01:41:32,910
it says that this position in the citations for of this field like for example

1572
01:41:32,910 --> 01:41:37,070
the first position of the first citation is part of the off field

1573
01:41:37,130 --> 01:41:39,430
and that these two billion to resolution

1574
01:41:39,450 --> 01:41:45,410
the same field as the resolution for fields and same citation doesn't for citation

1575
01:41:45,430 --> 01:41:48,630
so here is the ML and that as the whole work

1576
01:41:48,650 --> 01:41:51,030
seven formulas as advertised

1577
01:41:51,030 --> 01:41:53,930
the first three formulas implement the CRF

1578
01:41:53,950 --> 01:41:58,450
or or or hmm if you trained discriminatively

1579
01:41:58,510 --> 01:42:00,930
the first formula

1580
01:42:00,950 --> 01:42:07,300
effectively implement effective actually precisely performance the observation matrix of the HMM

1581
01:42:07,320 --> 01:42:14,110
so the convention here is that all free variables are universally quantified as in prolog

1582
01:42:14,170 --> 01:42:16,050
and this little plus

1583
01:42:16,070 --> 01:42:21,700
it means that we want alchemy to learn weights for every grounding of this formula

1584
01:42:22,050 --> 01:42:25,300
for every one of these variables

1585
01:42:25,300 --> 01:42:28,880
now we have pluses on both the token in the field

1586
01:42:28,930 --> 01:42:33,410
so what learning weights for this formula does is matrix of weights of token by

1587
01:42:34,380 --> 01:42:39,150
which is the correlation between tokens in fields like conferences like appearing title smith is

1588
01:42:39,150 --> 01:42:41,630
likely to appear in a moments for

1589
01:42:41,650 --> 01:42:44,400
and then you know the other forms of basically just below know this one is

1590
01:42:44,400 --> 01:42:45,820
the transition matrix

1591
01:42:45,860 --> 01:42:48,530
and this one is a little trick that allows us to not have you know

1592
01:42:48,570 --> 01:42:50,150
special state four

1593
01:42:50,150 --> 01:42:54,300
you know something that doesn't there's not part of any of the field but the

1594
01:42:54,300 --> 01:42:58,380
bottom line here is that you can set up an HMM or CRF in markov

1595
01:42:58,380 --> 01:43:01,200
logic with just two very short

1596
01:43:01,200 --> 01:43:03,360
now this parts into resolution

1597
01:43:03,380 --> 01:43:06,840
the first formula predicts whether two things are in the same field

1598
01:43:06,950 --> 01:43:11,490
you can think of this is like a similarity comparison saying that if these two

1599
01:43:12,470 --> 01:43:13,800
if this

1600
01:43:13,820 --> 01:43:16,780
appears in both of these fields that makes them more likely to be the same

1601
01:43:16,950 --> 01:43:20,680
and i'm going to learn what for how informative the token is so this is

1602
01:43:20,720 --> 01:43:22,490
similarity comparison it's also

1603
01:43:23,430 --> 01:43:28,450
a logistic regression where the predicted variables saying field and these

1604
01:43:28,450 --> 01:43:29,530
which is lambda

1605
01:43:29,530 --> 01:43:33,570
then we take all the strings of length one that's just individual entities then all

1606
01:43:33,580 --> 01:43:37,600
the strings of length two then all the strings thanks three and so on so

1607
01:43:37,630 --> 01:43:42,030
makes a nice order in which we do have the strings that arrive more or

1608
01:43:42,030 --> 01:43:43,580
less like with like

1609
01:43:43,590 --> 01:43:47,770
so with the hierarchical order for just got to letters in order alphabet which are

1610
01:43:47,770 --> 01:43:52,150
a and b we would find is for strings for this order the strings at

1611
01:43:53,100 --> 01:43:55,150
so got one string of length

1612
01:43:55,170 --> 01:43:59,750
one two strings of length sorry one string of length zero two strings of length

1613
01:43:59,750 --> 01:44:02,150
one for strings of length two

1614
01:44:02,200 --> 01:44:06,610
eight of length three and so on

1615
01:44:06,660 --> 01:44:09,930
so just for the sake of an example

1616
01:44:10,040 --> 01:44:13,380
if we got a b and c with a smaller than b and smaller than

1617
01:44:14,080 --> 01:44:19,000
for the lexicographic order we'd have a baby which would be before a b

1618
01:44:19,020 --> 01:44:21,250
but if you took the length lex order

1619
01:44:21,260 --> 01:44:24,810
you have it the opposite way round because one is of length two and that

1620
01:44:24,810 --> 01:44:27,780
one is of length three seems easy

1621
01:44:27,830 --> 01:44:33,100
and carefully if you're looking at the prefix oil well maybe is neither a prefix

1622
01:44:33,100 --> 01:44:39,070
of a a b nor is a a b prefix if a

1623
01:44:39,080 --> 01:44:44,270
OK so this is one of the ideas of topology to put order over things

1624
01:44:44,270 --> 01:44:48,940
the second idea typically into apologies to distances things because you've got a set of

1625
01:44:48,940 --> 01:44:52,570
strings or a set of objects and you wanted to put some sort of the

1626
01:44:52,570 --> 01:44:58,420
distance of this we're hearing this morning mentioned the case of the k nearest neighbours

1627
01:44:58,710 --> 01:45:01,100
all these techniques

1628
01:45:01,120 --> 01:45:06,870
typically in pattern recognition all based on having distances so what we know now about

1629
01:45:06,870 --> 01:45:12,020
distances and how do we build distances on typically strings

1630
01:45:12,100 --> 01:45:15,550
so the first thing is what is the problem while the problem is to build

1631
01:45:15,550 --> 01:45:17,100
some sort of

1632
01:45:17,120 --> 01:45:20,070
a function which given a class of objects

1633
01:45:20,100 --> 01:45:23,600
we're going to talk about strings but at the same time there are distances on

1634
01:45:23,600 --> 01:45:28,140
graphs there distances on trees you can have distances on anything you're trying to represent

1635
01:45:28,390 --> 01:45:33,760
so these objects under all representations of these objects and you want to functions such

1636
01:45:33,760 --> 01:45:38,360
that the closer x and y are one to each other the smaller is the

1637
01:45:38,360 --> 01:45:43,120
smaller is the distance between the two that is what a distance is

1638
01:45:43,960 --> 01:45:45,600
i come to to the second

1639
01:45:45,610 --> 01:45:47,040
but then

1640
01:45:47,040 --> 01:45:52,080
apart from saying that apart from saying that you've got this class of objects representations

1641
01:45:52,200 --> 01:45:56,790
and just something that is close to the smaller the distance when you like have

1642
01:45:56,790 --> 01:46:02,180
some mathematical properties of the mathematical properties of these things here and they define what

1643
01:46:02,180 --> 01:46:03,950
is called a metric

1644
01:46:04,010 --> 01:46:06,150
OK now

1645
01:46:06,180 --> 01:46:08,200
when you read

1646
01:46:08,210 --> 01:46:09,990
books and articles

1647
01:46:09,990 --> 01:46:15,020
the confusion about the word distances is really very complete right some people are going

1648
01:46:15,020 --> 01:46:19,610
to call this an something that does have the properties and some people don't call

1649
01:46:19,610 --> 01:46:23,360
it like that some people use distance with all sorts of things are not so

1650
01:46:23,360 --> 01:46:24,890
it's very important to know

1651
01:46:24,920 --> 01:46:28,050
if we do have the properties in which when we do have or not why

1652
01:46:28,520 --> 01:46:30,280
for algorithmic reasons

1653
01:46:30,320 --> 01:46:33,760
again because you will not be able to use the same algorithms if you've got

1654
01:46:33,760 --> 01:46:37,670
certain properties about this distance using or not

1655
01:46:37,690 --> 01:46:42,430
so just let's look at what other the properties that are usually imposed one of

1656
01:46:42,430 --> 01:46:46,200
them is saying to the distance between x and x is zero

1657
01:46:46,210 --> 01:46:47,350
fair enough

1658
01:46:47,410 --> 01:46:50,690
the second one is to say that this between x and y

1659
01:46:50,720 --> 01:46:54,800
is the distance between y and x o saying the symmetrical you can think about

1660
01:46:54,800 --> 01:46:58,520
distance if you've done already machine learning you realize that quite a few of them

1661
01:46:58,520 --> 01:47:03,350
do not already comply with this being symmetrical is not always that easy

1662
01:47:03,360 --> 01:47:09,150
third typically that is in every case is it has that is that the of

1663
01:47:09,160 --> 01:47:13,960
x and y is positive so you want to distance to be positive having negative

1664
01:47:13,960 --> 01:47:15,540
distances and from

1665
01:47:15,550 --> 01:47:20,190
even if there are cases where people are doing distances where the result is not

1666
01:47:20,250 --> 01:47:25,430
a number or to complex number or it's something completely different for because that's what

1667
01:47:25,430 --> 01:47:28,820
they like and there are two other ones which

1668
01:47:28,830 --> 01:47:33,030
reasonable are important one of them is to say that whenever got the distance between

1669
01:47:33,030 --> 01:47:38,360
x and y is zero then it's because and only because x equals y that

1670
01:47:38,410 --> 01:47:43,200
is very important because that means that when we got that property it means every

1671
01:47:43,200 --> 01:47:48,280
time we are testing the distance we can actually at the same time trying to

1672
01:47:48,280 --> 01:47:50,240
test for the

1673
01:47:50,250 --> 01:47:55,240
isomorphism of the structure of for the quality of the structure of the equivalent of

1674
01:47:55,240 --> 01:47:56,030
the structure

1675
01:47:56,350 --> 01:48:01,690
so in problems where whatever you're manipulating finding out if the two things of same

1676
01:48:01,690 --> 01:48:03,990
take graphs for example

1677
01:48:04,010 --> 01:48:08,910
OK if you do distance in graphs to try and see if two graphs are

1678
01:48:08,950 --> 01:48:13,680
whatever the distances between them if you've got this property here

1679
01:48:13,700 --> 01:48:18,870
df x y equals you implies x equals y what does it mean it means

1680
01:48:18,870 --> 01:48:24,450
that what you're computing will help you solve the graph isomorphism problem

1681
01:48:24,450 --> 01:48:29,840
who knows what the graph isomorphism problem is

1682
01:48:31,300 --> 01:48:34,470
but it it's basically you take one graph

1683
01:48:34,510 --> 01:48:38,490
and you just shove everything around you move things i mean they and you rename

1684
01:48:38,490 --> 01:48:42,620
the labels and you've got to graph that the that you tried see is the

1685
01:48:42,620 --> 01:48:46,530
the same graph you know just where you move things around or not

1686
01:48:46,550 --> 01:48:50,600
is it just a question of representing in space the same object

1687
01:48:50,620 --> 01:48:53,990
so if it is the same object then that means that we have got the

1688
01:48:53,990 --> 01:49:00,070
graph isomorphism with they are isomorphic if you don't it means that they mean that

1689
01:49:00,070 --> 01:49:03,870
they are not isomorphic so this happy all the time when you are trying to

1690
01:49:03,870 --> 01:49:08,700
do are represented in images by graphs where people are trying to think is the

1691
01:49:08,700 --> 01:49:12,450
same image this this by saying is this the same graph is this

1692
01:49:12,470 --> 01:49:17,100
now this is what you call this is another untractable problem it's not quite NP

1693
01:49:18,280 --> 01:49:21,430
right but it's regarded to be

1694
01:49:21,600 --> 01:49:26,180
hard in the sense there's is no polynomial algorithm either to solve the problem

1695
01:49:26,180 --> 01:49:28,680
at least people believe there isn't one

1696
01:49:28,680 --> 01:49:31,840
OK so this means what it means that if you are thinking of doing nice

1697
01:49:31,840 --> 01:49:36,450
distances that have all these sort of features but at the same time you're lovely

1698
01:49:36,450 --> 01:49:41,930
little distance is solving some problems with this supposed to be

1699
01:49:41,990 --> 01:49:45,530
difficult then it means that something wrong somewhere

1700
01:49:45,550 --> 01:49:50,680
probably all those are your algorithm is exponential all your thinking you're doing something that

1701
01:49:50,680 --> 01:49:52,990
you're not doing

1702
01:49:53,010 --> 01:49:56,720
so that's why i know a little bit what we're doing about this is important

1703
01:49:56,990 --> 01:50:02,070
this the last equation is known as the triangle inequality which is

1704
01:50:02,120 --> 01:50:07,550
typical right we know that if the distance between x y and added to the

1705
01:50:07,550 --> 01:50:12,120
distance of y z should be larger than the distance between x and z this

1706
01:50:12,120 --> 01:50:15,550
is not always the case in a lot of distances that are used in practice

1707
01:50:15,550 --> 01:50:18,180
typically in bioinformatics

1708
01:50:18,180 --> 01:50:21,890
but why is it useful to have it

1709
01:50:21,910 --> 01:50:25,950
however can think OK you know it might want is all we may not want

1710
01:50:25,950 --> 01:50:30,510
it is just because we want to do mathematics no for algorithmic reasons when you

1711
01:50:30,510 --> 01:50:37,280
do have it you can actually you use it the triangular in equivalence to avoid

1712
01:50:37,280 --> 01:50:42,070
for example searchengin hold whole chunks of the data space

1713
01:50:42,090 --> 01:50:44,910
because you can use it to say well you know i know this is already

1714
01:50:44,910 --> 01:50:48,530
more than whatever threshold i was looking for so there's no point in looking at

1715
01:50:48,530 --> 01:50:51,620
these because i will be even further

1716
01:50:51,640 --> 01:50:57,760
OK so be able to use those properties are related to doing things like that

1717
01:50:57,760 --> 01:51:01,210
what and that edge is an edge of the shortest path from s to v

1718
01:51:02,770 --> 01:51:04,020
what do we know

1719
01:51:09,190 --> 01:51:13,130
do you know about the i becomes the correct value delta s comedy i this

1720
01:51:13,130 --> 01:51:15,260
was called correctness lemma

1721
01:51:15,290 --> 01:51:16,250
last time

1722
01:51:16,260 --> 01:51:20,040
one of the things we proved about algorithm but it was really just the fact

1723
01:51:20,130 --> 01:51:22,420
about relaxation

1724
01:51:25,950 --> 01:51:27,050
and it was a pretty

1725
01:51:27,050 --> 01:51:28,980
simple proof

1726
01:51:29,000 --> 01:51:39,280
it comes from the fact

1727
01:51:39,300 --> 01:51:44,270
we we know the shortest path weight is this certainly DVI was at least this

1728
01:51:46,150 --> 01:51:51,060
and let's suppose is greater otherwise we were done we know DVI minus one is

1729
01:51:51,060 --> 01:51:52,080
set to this

1730
01:51:52,090 --> 01:51:56,660
so this is exactly the condition is being checked in the relaxation step and they

1731
01:51:56,660 --> 01:51:59,290
will actually this the DVI

1732
01:51:59,310 --> 01:52:01,070
well you will be greater than this

1733
01:52:01,080 --> 01:52:02,090
but suppose

1734
01:52:02,100 --> 01:52:05,910
and then we'll set equal to this and that's exactly the SPI

1735
01:52:06,020 --> 01:52:10,540
when we relax that edge we've got set it to the right values so so

1736
01:52:10,550 --> 01:52:13,620
the this is the end of the proof round very simple

1737
01:52:14,290 --> 01:52:19,510
the point is you you look at your shortest path here it is

1738
01:52:19,530 --> 01:52:25,520
and if we some there's no negative weight cycles this has the correct value initially

1739
01:52:25,590 --> 01:52:28,090
the idea of s is going to be zero

1740
01:52:28,100 --> 01:52:30,800
after the first round you've gotta relax is an edge

1741
01:52:30,810 --> 01:52:34,380
and then you get the right value for the vertex after the second round you

1742
01:52:34,380 --> 01:52:37,950
got to get you gotta relax the essential to achieve the right value the value

1743
01:52:37,950 --> 01:52:39,590
for this vertex and so on

1744
01:52:39,620 --> 01:52:43,510
so no matter which shortest path you take you can apply this analysis and you

1745
01:52:43,510 --> 01:52:45,290
know that by

1746
01:52:45,330 --> 01:52:47,510
if that is the length of the path

1747
01:52:47,570 --> 01:52:50,940
is here we stand it was k edges

1748
01:52:52,280 --> 01:52:58,650
then after k rounds you've got to be done

1749
01:52:58,660 --> 01:53:01,160
so this not actually in the per

1750
01:53:01,180 --> 01:53:06,070
so this means after ten rounds

1751
01:53:06,090 --> 01:53:14,200
we have the right answer for k which is

1752
01:53:15,590 --> 01:53:25,370
so the only question is how they could KB better be the right answer

1753
01:53:25,440 --> 01:53:28,000
and most the minus one

1754
01:53:28,120 --> 01:53:32,050
the claim by the the algorithm that you only need to do the minus one

1755
01:53:32,050 --> 01:53:35,730
steps and indeed the number of edges in a simple

1756
01:53:36,510 --> 01:53:41,620
in the graph is at most the number of vertices minus one

1757
01:53:41,650 --> 01:53:44,850
OK is it must be minus one

1758
01:53:46,790 --> 01:53:47,890
please simple

1759
01:53:47,910 --> 01:53:59,170
so that's why we have to assume that wasn't just any shortest path had simple

1760
01:53:59,170 --> 01:54:04,390
one didn't repeatedly vertices so there must be vertices in the past of the minus

1761
01:54:04,390 --> 01:54:06,690
one edges in the past

1762
01:54:08,350 --> 01:54:10,280
and that's all there is the number four

1763
01:54:10,280 --> 01:54:11,990
it's pretty simple

1764
01:54:12,010 --> 01:54:16,520
incorrectness of course we're using a lot of elements that we had last time

1765
01:54:16,530 --> 01:54:19,450
which makes it easier

1766
01:54:19,480 --> 01:54:30,400
OK a consequence of this theorem this proved is that

1767
01:54:30,430 --> 01:54:35,540
if the bellman ford fails to converge

1768
01:54:35,740 --> 01:54:43,850
and that's what the algorithm is tracking is whether this relaxation

1769
01:54:43,900 --> 01:54:46,080
still requires work

1770
01:54:46,080 --> 01:54:49,860
after this the minus one step further and the album is run another round of

1771
01:54:49,860 --> 01:54:55,820
heathrow and see whether anything changes also the argument fails to converge

1772
01:54:55,830 --> 01:54:58,330
after the virus one steps

1773
01:54:59,690 --> 01:55:06,590
then there has to be

1774
01:55:06,600 --> 01:55:10,030
a negative weight cycle

1775
01:55:10,050 --> 01:55:23,110
OK this is just the contrapositive what we proved we prove that you assume there

1776
01:55:23,110 --> 01:55:27,770
is no negative weight cycle then we know the DNS is is zero and then

1777
01:55:27,770 --> 01:55:30,920
all this argument as you've got to converge after the minus one rounds can be

1778
01:55:30,920 --> 01:55:34,360
anything left to do once you reach the shortest path weights going monotonically you can

1779
01:55:34,360 --> 01:55:35,420
never hit the bottom

