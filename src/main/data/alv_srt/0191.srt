1
00:00:00,000 --> 00:00:01,980
he was

2
00:00:03,560 --> 00:00:06,090
OK so

3
00:00:06,150 --> 00:00:12,930
all o

4
00:00:15,700 --> 00:00:19,200
the reason why

5
00:00:19,330 --> 00:00:24,040
nineteen for six

6
00:00:31,530 --> 00:00:34,610
which would be useful to

7
00:00:36,110 --> 00:00:37,870
this one

8
00:00:37,900 --> 00:00:48,080
we want to

9
00:01:14,320 --> 00:01:19,810
so the result

10
00:01:19,910 --> 00:01:24,500
thank you for

11
00:01:30,990 --> 00:01:35,210
it has struggled with

12
00:01:41,820 --> 00:01:43,690
two that

13
00:01:43,750 --> 00:01:46,700
for every one of these two

14
00:01:48,710 --> 00:01:50,250
right is

15
00:01:58,860 --> 00:02:02,480
that's the way love

16
00:02:04,870 --> 00:02:08,550
it possible to with it

17
00:02:08,560 --> 00:02:11,000
other notable

18
00:02:15,380 --> 00:02:20,500
so what do you do distribution see the

19
00:02:20,510 --> 00:02:22,690
he is

20
00:02:25,990 --> 00:02:26,900
this the

21
00:02:28,440 --> 00:02:31,150
so what

22
00:02:31,160 --> 00:02:34,900
the problem is that

23
00:02:37,520 --> 00:02:40,990
actually it

24
00:02:48,180 --> 00:02:50,540
there is

25
00:02:51,380 --> 00:02:55,690
i want to to be optimized

26
00:02:56,350 --> 00:02:57,170
so what

27
00:02:57,170 --> 00:03:02,000
the all right

28
00:03:05,770 --> 00:03:09,460
so what you have here

29
00:03:22,840 --> 00:03:26,520
so this is just a

30
00:03:26,680 --> 00:03:35,230
you do that

31
00:03:35,250 --> 00:03:40,010
three of

32
00:03:40,010 --> 00:03:44,290
so what do we do

33
00:03:48,930 --> 00:03:52,870
he also writes

34
00:03:52,920 --> 00:03:54,560
did the

35
00:03:56,580 --> 00:03:59,070
she wants

36
00:04:01,250 --> 00:04:03,250
do that

37
00:04:15,330 --> 00:04:16,510
and that

38
00:04:19,980 --> 00:04:25,000
you to

39
00:04:34,310 --> 00:04:42,820
so that we can do

40
00:04:42,840 --> 00:04:46,630
there are just

41
00:04:49,380 --> 00:04:52,480
since we use

42
00:04:52,480 --> 00:04:58,120
some mobile devices and so on interact with whoever happens to be new neighborhood these

43
00:04:58,190 --> 00:05:03,850
all use cases generated from industrial membership of our group and they say for these

44
00:05:03,850 --> 00:05:08,180
kind of use cases idea schema expressivity is simply not enough

45
00:05:08,230 --> 00:05:09,620
there are things

46
00:05:09,750 --> 00:05:11,090
you cannot do that way

47
00:05:11,790 --> 00:05:18,980
so that was the driver for definition of new language the web ontology language

48
00:05:19,000 --> 00:05:25,090
the web ontology language of voices just horrible acronyms so i was much better and

49
00:05:25,090 --> 00:05:28,100
also if any of you read winnie-the-pooh

50
00:05:28,130 --> 00:05:30,110
OK then you know the story

51
00:05:30,120 --> 00:05:35,540
so i only know it partially i think out loud

52
00:05:35,600 --> 00:05:41,410
actually is somewhere in the stories OWL spells his name will

53
00:05:42,560 --> 00:05:49,150
so what we found that the OK we can do that too brightly call out

54
00:05:49,280 --> 00:05:53,590
and of course all of these requirements is quite a long list i had this

55
00:05:53,590 --> 00:05:57,990
little list all staff and say saying was this long list of requirements of use

56
00:05:58,860 --> 00:06:04,190
and then it's very difficult if you define what language does it all they have

57
00:06:04,190 --> 00:06:08,310
some people that want some of it so you can't really agree on any single

58
00:06:08,310 --> 00:06:13,960
language that that keep everybody happy so instead of defining what language you define ceiling

59
00:06:13,960 --> 00:06:19,400
which is not allowed to say it you define what language with the layers right

60
00:06:19,400 --> 00:06:21,140
now there was in job he

61
00:06:21,190 --> 00:06:27,350
this single language with the different layers of expressiveness the idea of OWL lite in

62
00:06:27,400 --> 00:06:33,820
because of transatlantic cooperation collaboration we have to spell it wrong way the idea idea

63
00:06:33,820 --> 00:06:39,050
between by light is that it's simple classification hierarchy in the style of ideas schema

64
00:06:39,410 --> 00:06:44,360
but with some simple constraints some of the constraints that you cannot say in in

65
00:06:44,840 --> 00:06:48,120
schema so for example the domain property

66
00:06:48,130 --> 00:06:54,040
formulated as a constrained everything must be from this type and if it's not then

67
00:06:54,040 --> 00:06:56,910
it's the constraint that can be said in our

68
00:06:56,990 --> 00:07:00,120
but it's very simple language

69
00:07:00,180 --> 00:07:04,900
was the idea that OWL DL nielsen for description logics and that's because

70
00:07:05,000 --> 00:07:08,550
it is based on description logic which

71
00:07:08,560 --> 00:07:14,710
exactly balances on the point between maximal expressiveness while maintaining tractability so there people who

72
00:07:14,860 --> 00:07:17,000
been doing homework for last ten

73
00:07:17,080 --> 00:07:21,730
fifteen years maybe even longer to exactly met the complexity landscape of how much you

74
00:07:21,730 --> 00:07:25,830
can experts say in logic while maintaining

75
00:07:25,930 --> 00:07:30,620
reasonable tractability and OWL DL is poised exactly on so if you have a little

76
00:07:30,620 --> 00:07:33,610
bit more to its complexity goes

77
00:07:34,450 --> 00:07:40,010
and there's well understood formalizations for this kind of like six

78
00:07:40,010 --> 00:07:44,960
so you can just think of OWL DL as a a well known description logic

79
00:07:44,960 --> 00:07:46,460
was an XML syntax

80
00:07:46,580 --> 00:07:48,730
that's that's really all there is

81
00:07:48,820 --> 00:07:55,540
well in this if you mean that there is an there's an inference

82
00:07:56,350 --> 00:08:01,210
the definition of prove rules and also implementation proved that are complete strangers

83
00:08:01,380 --> 00:08:02,580
and that's

84
00:08:02,860 --> 00:08:05,310
what you meant

85
00:08:06,640 --> 00:08:11,590
well if people say completely so different things so yes you can do complete inference

86
00:08:11,750 --> 00:08:17,100
in OWL and strictly speaking the theoretical complexity is bad news but the practical complexity

87
00:08:17,100 --> 00:08:20,020
for realistic cases tool

88
00:08:20,020 --> 00:08:24,990
and but this is not enough for some people some people for in particular wanted

89
00:08:24,990 --> 00:08:27,280
this circular metal

90
00:08:27,360 --> 00:08:31,230
what your question was was a thing is that you have classes that can be

91
00:08:31,230 --> 00:08:35,030
themselves the value of the property or classes that can be a member of other

92
00:08:35,030 --> 00:08:39,380
classes and these people want that kind of

93
00:08:39,410 --> 00:08:45,630
certainly metamodel ndx expressions of OWL and so for that we all full

94
00:08:45,690 --> 00:08:47,200
where you

95
00:08:47,290 --> 00:08:55,660
basically you can do anything you want to use that ability in humans even decidability

96
00:08:56,200 --> 00:08:59,770
it's not even the case that you cannot compute it efficiently you can compute it

97
00:09:00,460 --> 00:09:04,560
it's not a matter to be clever enough there's no the universe that there exists

98
00:09:04,560 --> 00:09:11,960
an algorithm will to complete reasoning for full also means that the standard formal semantics

99
00:09:11,960 --> 00:09:15,540
that we use for yellow lights no longer apply have to do some non standard

100
00:09:15,540 --> 00:09:21,390
formalisation which was developed more or less for the purposes of OWL full and well

101
00:09:21,390 --> 00:09:25,510
you have all the syntactic freedom of RDF said you could you want i mean

102
00:09:25,630 --> 00:09:29,200
you could do everything you want to stay with this technical innovation rules of RDF

103
00:09:29,670 --> 00:09:32,690
including applying the language to itself

104
00:09:32,800 --> 00:09:37,940
so you can say that the subclassof relationship has maxcardinality two

105
00:09:37,960 --> 00:09:40,990
we saying that every class is most subclasses

106
00:09:41,060 --> 00:09:45,080
that's really applying the language to its own constructions

107
00:09:45,160 --> 00:09:49,680
so that already shows you that you can forget about the fisheries with those kind

108
00:09:49,690 --> 00:09:50,660
of language

109
00:09:50,760 --> 00:09:57,060
and i think it's complexity something like p space

110
00:10:09,350 --> 00:10:15,850
so it's not quite higher order because the number of predicates elements still countable and

111
00:10:16,190 --> 00:10:17,430
if to have

112
00:10:17,450 --> 00:10:22,620
over countable number credit brokers dealing with before it becomes really order but it you

113
00:10:22,620 --> 00:10:25,510
can add more or less predicates as variables

114
00:10:25,640 --> 00:10:27,180
that is

115
00:10:28,900 --> 00:10:32,230
let's do this year but this is like a very rich you could

116
00:10:32,230 --> 00:10:36,000
we have things like it really useful for right

117
00:10:36,060 --> 00:10:41,160
yes it is really just because it's useful i seem to convince in this case

118
00:10:41,160 --> 00:10:43,080
is the one limit the classes

119
00:10:43,090 --> 00:10:49,370
so we you have classes of classes often is problem when you model particular domain

120
00:10:49,400 --> 00:10:53,440
whether you something is an instance of model something is the a class

121
00:10:53,460 --> 00:10:57,580
so we had this debate within the only case i think so we have the

122
00:10:57,580 --> 00:11:02,910
class of all boeing airplanes clearly class right now is seven four seven is a

123
00:11:03,930 --> 00:11:06,670
of all the planes or is it just a particular

124
00:11:06,680 --> 00:11:10,450
type of of airplane an instance of the class but of course a particular seven

125
00:11:10,450 --> 00:11:14,360
four seven on the on on the platform tarmac somewhere that's an instance of that

126
00:11:14,360 --> 00:11:18,890
class of seven four seven so depending from which viewpoint you look at the main

127
00:11:18,890 --> 00:11:23,320
something is either class or instance also what happens is that two people with the

128
00:11:23,320 --> 00:11:27,590
same domain one of the model is class one of model is an instance you

129
00:11:27,590 --> 00:11:29,410
want to say that these two articles

130
00:11:29,450 --> 00:11:32,510
but now the you can do it because you have to decide

131
00:11:32,520 --> 00:11:37,640
idea class for instance between the inequality between two would across the boundary

132
00:11:37,660 --> 00:11:39,110
so there i

133
00:11:39,120 --> 00:11:44,140
realistic use cases i see increase schreiber two a mapping between the art and architecture

134
00:11:44,140 --> 00:11:48,460
desires in wordnet and

135
00:11:48,480 --> 00:11:52,350
which is about the village useful thing to do and it in a very elegant

136
00:11:52,350 --> 00:11:57,490
way by using class construction get around this site

137
00:11:57,510 --> 00:12:02,130
it just means that you can implement reasons for but you could not guarantee these

138
00:12:02,130 --> 00:12:04,000
reasons give you all the

139
00:12:04,020 --> 00:12:05,330
that's the price you pay

140
00:12:05,460 --> 00:12:09,110
it doesn't mean it's impossible to reason with just impossible to complete

141
00:12:09,170 --> 00:12:14,020
if you don't care about computers and people like that hayes said it was very

142
00:12:14,040 --> 00:12:18,770
a great advocate all the same who can make this because you the data that

143
00:12:18,790 --> 00:12:22,670
you start with a single did you start with incomplete in the first place

144
00:12:22,690 --> 00:12:27,080
if you start with reasoning about stuff on the web already

145
00:12:27,090 --> 00:12:31,380
so you can that your reasoning a little bit more incompleteness so he says that

146
00:12:31,400 --> 00:12:32,520
all completeness

147
00:12:32,520 --> 00:12:35,000
above all the others area

148
00:12:35,110 --> 00:12:39,770
but then he comes with his medical applications is now if i want to argue

149
00:12:39,770 --> 00:12:44,360
that there is not the particular driver really want know is there

150
00:12:44,380 --> 00:12:48,290
in one those you know because i'm used to be my reasoning

151
00:12:48,310 --> 00:12:51,480
there's really genuine case for you to believe

152
00:12:51,480 --> 00:12:54,390
and got a in the long sequence

153
00:12:54,450 --> 00:12:57,110
algorithm says sorry you can't match this

154
00:12:57,140 --> 00:13:01,210
so moment what i'm going to do is i'm going to look here and say

155
00:13:01,210 --> 00:13:03,810
OK failed where do i go back to

156
00:13:03,860 --> 00:13:08,560
sorry i can't help you you can't done enough you haven't got anything that you

157
00:13:08,560 --> 00:13:11,910
can keep for further so you've got to come back to the same place and

158
00:13:11,910 --> 00:13:15,950
you're going to find yourself doing sorry here this column missing which is just saying

159
00:13:15,950 --> 00:13:19,530
compare this a with a you have just seen

160
00:13:19,570 --> 00:13:23,980
and you advance one compare the b with the next a same as before it

161
00:13:23,980 --> 00:13:29,320
says sorry the b and they have different you've got come back to to start

162
00:13:29,430 --> 00:13:33,700
so can the which is here with the first

163
00:13:33,710 --> 00:13:38,630
then the b with the BCI everything's OK bnb that's alright they match to be

164
00:13:38,630 --> 00:13:41,310
which is here and you can notice we've got the big the beginning of the

165
00:13:41,310 --> 00:13:42,750
fragment a b

166
00:13:42,760 --> 00:13:45,310
so the b compares with b

167
00:13:45,340 --> 00:13:49,910
then you advanced version a from the little sequence and the sea

168
00:13:51,720 --> 00:13:55,930
but what happens there was is going to say no it doesn't work but again

169
00:13:55,930 --> 00:13:59,800
i'm sorry just the fact that got to see here doesn't help us we can't

170
00:13:59,880 --> 00:14:03,860
say exploit this to be able to gain something so we could go back to

171
00:14:03,860 --> 00:14:04,800
the start

172
00:14:04,820 --> 00:14:06,820
so let's get to the interesting case

173
00:14:06,840 --> 00:14:12,020
so then we compare the a with the authority a that we've got here with

174
00:14:12,020 --> 00:14:17,590
this c then the a for a with the a

175
00:14:17,610 --> 00:14:21,680
the be with the CE let's let's get to a nice place with a b

176
00:14:21,680 --> 00:14:26,610
would be so as to be the case here the third which is here with

177
00:14:27,580 --> 00:14:30,610
that is here

178
00:14:30,630 --> 00:14:33,160
see with this c

179
00:14:33,160 --> 00:14:36,430
and we've got all string and i fact my example isn't that useful because i

180
00:14:36,430 --> 00:14:40,670
haven't sorry see the only place i did was here a b c

181
00:14:40,680 --> 00:14:42,060
i would have been able

182
00:14:42,090 --> 00:14:44,030
to just say what position

183
00:14:44,040 --> 00:14:47,790
OK i'm sorry it's not it's not an easy one to explain to people in

184
00:14:47,790 --> 00:14:51,090
string spend spend about three was not explaining this

185
00:14:51,110 --> 00:14:53,380
now try to do it in ten doesn't work

186
00:14:53,430 --> 00:14:55,390
OK so

187
00:14:55,770 --> 00:15:00,660
just a quick conclusion about about these business of looking for patterns in strings looking

188
00:15:00,660 --> 00:15:06,730
in patterns for strings is an open area of research where people are actually trying

189
00:15:06,750 --> 00:15:11,570
still today to gain as much as possible but basically on the constants

190
00:15:11,630 --> 00:15:15,850
you can't going on much more than on the concensus to try and really do

191
00:15:15,850 --> 00:15:17,520
as fast as possible

192
00:15:17,530 --> 00:15:21,450
these looking for patterns in strings

193
00:15:21,450 --> 00:15:26,970
so just the word the complexity of KMP instead of being as she might have

194
00:15:27,630 --> 00:15:29,410
the the

195
00:15:29,410 --> 00:15:34,770
multiplication of the complexity of of the sorry the length of the first string by

196
00:15:34,770 --> 00:15:38,560
the length of the second-string thanks to this sort of tricks we've managed to have

197
00:15:38,560 --> 00:15:42,840
a complexity which is the length of the first string plus length the second feature

198
00:15:42,860 --> 00:15:45,350
OK plus because you basically only

199
00:15:45,360 --> 00:15:50,330
you never going backwards which is what's interesting it

200
00:15:50,340 --> 00:15:55,130
OK so we've got the strings and now what we'd like to do is to

201
00:15:55,130 --> 00:16:00,020
be able to put in order over a the set of strings all over is

202
00:16:00,020 --> 00:16:02,640
a set of strings is given a set of strings and you wanted to be

203
00:16:02,640 --> 00:16:06,260
able to go through that sort of strings in a particular ways what sort of

204
00:16:06,260 --> 00:16:08,120
order relations and we have

205
00:16:08,220 --> 00:16:12,020
so the first thing is when to suppose we do have an alphabetical order as

206
00:16:12,030 --> 00:16:15,360
basic so that in alphabetical order over the

207
00:16:16,610 --> 00:16:20,240
you've got that networking events from that

208
00:16:20,240 --> 00:16:22,430
so once the alphabetical order

209
00:16:22,440 --> 00:16:26,200
the first order and you can do on strings is the prefix for three prefix

210
00:16:26,200 --> 00:16:29,780
or interest is to say that one string is

211
00:16:29,800 --> 00:16:32,450
prefix office second string

212
00:16:32,450 --> 00:16:36,700
one string is prefix of the country this is not a total ordering i didn't

213
00:16:36,700 --> 00:16:40,140
say we only got to do to flooring but this is a nice ordering can

214
00:16:40,140 --> 00:16:45,530
be used in certain cases easy second one which is the well-known order is the

215
00:16:45,530 --> 00:16:48,340
lexicographic order this is the dictionary order

216
00:16:48,360 --> 00:16:53,050
the dictionary order is you've got enough of a particular order on the symbols so

217
00:16:53,050 --> 00:16:57,240
you can then put whatever set singles was so set of strings you have you

218
00:16:57,240 --> 00:16:59,510
can put them in a dictionary

219
00:16:59,540 --> 00:17:01,110
the dictionary might be in

220
00:17:01,170 --> 00:17:05,830
so during the cancer say that either one string is the prefix of the other

221
00:17:05,950 --> 00:17:10,950
right all they share up to a certain place a common prefix

222
00:17:10,970 --> 00:17:13,970
and then if i look at the next letter with the next letter is going

223
00:17:13,970 --> 00:17:18,640
to be the difference one just for the alphabetical order

224
00:17:18,660 --> 00:17:24,010
OK there are problems with the lexicographic order they rarely problems the problems are that

225
00:17:24,010 --> 00:17:25,020
you can't do

226
00:17:25,050 --> 00:17:27,690
recursion based on that thematic lord

227
00:17:27,760 --> 00:17:29,670
basically if you look at certain

228
00:17:29,680 --> 00:17:33,200
cases i can if i decide that is before b

229
00:17:33,210 --> 00:17:36,250
the real problem is that in between a and b

230
00:17:36,270 --> 00:17:40,580
we would like to be able to get their reasonably quickly there's actually an infinity

231
00:17:40,580 --> 00:17:42,050
of strings

232
00:17:42,070 --> 00:17:43,760
for the alphabetical order

233
00:17:43,810 --> 00:17:45,720
because i can do a

234
00:17:48,740 --> 00:17:50,250
a b

235
00:17:50,270 --> 00:17:54,360
and so on all these strings

236
00:17:54,380 --> 00:17:55,780
or all

237
00:17:55,850 --> 00:17:57,700
smaller than b

238
00:17:57,710 --> 00:18:02,300
that makes it not nice ordering to work when we working with strings even if

239
00:18:02,300 --> 00:18:05,990
it's the natural wonders of one we're using when we're looking into addiction but it

240
00:18:05,990 --> 00:18:08,620
actually we do not have

241
00:18:08,640 --> 00:18:11,930
an infinity of strings so it's not a problem but if we do have a

242
00:18:11,930 --> 00:18:14,680
possible infinity of strings that with got

243
00:18:14,690 --> 00:18:18,790
so the next one that is mostly used when people are working on saying grammatical

244
00:18:18,790 --> 00:18:22,800
inference because that is my topic but you use it more generally when you're working

245
00:18:22,800 --> 00:18:24,820
on strings is

246
00:18:24,840 --> 00:18:28,470
ah the length lex order

247
00:18:28,470 --> 00:18:31,510
the idea is to say we want to avoid this but we do like the

248
00:18:31,510 --> 00:18:36,600
dictionary order so we just make sure that we start by ordering with the victory

249
00:18:36,660 --> 00:18:40,620
all strings of length zero there's just one length of the string of length zero

250
00:18:40,620 --> 00:18:44,260
so we basically recognised the

251
00:18:44,310 --> 00:18:46,930
the remaining variance as the sum

252
00:18:46,930 --> 00:18:49,370
this is the variance of the k column this is the variance in the cape

253
00:18:49,370 --> 00:18:53,720
colony so two sides and time the best case columns and some other variants of

254
00:18:53,720 --> 00:18:55,440
the k column

255
00:18:55,450 --> 00:18:57,040
so we basically get the results

256
00:18:57,060 --> 00:18:59,220
the the error is equal to two

257
00:18:59,240 --> 00:19:01,150
times and squared

258
00:19:01,750 --> 00:19:08,160
times the sum of the variances of discarded columns so these are discarded columns

259
00:19:08,200 --> 00:19:10,660
so if we want to minimize its variance

260
00:19:12,010 --> 00:19:14,550
so if you want to minimize the error

261
00:19:14,570 --> 00:19:15,960
we should minimize

262
00:19:15,970 --> 00:19:18,650
the variance of the columns we discard

263
00:19:19,980 --> 00:19:22,760
is that makes sense

264
00:19:22,780 --> 00:19:27,150
geometrically it makes no sense and ability put so that later

265
00:19:27,190 --> 00:19:31,460
so basically this little simple purpose saying if we're doing feature extraction in such a

266
00:19:31,460 --> 00:19:34,640
way that we're trying to throw away features from data in such a way that

267
00:19:34,640 --> 00:19:37,310
we're trying to preserve this distance matrix

268
00:19:37,330 --> 00:19:41,510
the way we should do that is discard directions of least variance

269
00:19:41,520 --> 00:19:44,420
or retain directions of most variance

270
00:19:51,850 --> 00:19:54,600
so how does that work in practice one

271
00:19:56,250 --> 00:19:58,390
we like to do two dimensional

272
00:19:59,260 --> 00:20:01,590
representations of our data

273
00:20:01,610 --> 00:20:05,870
why do we like to the two-dimensional representations about data because we can show them

274
00:20:05,870 --> 00:20:07,170
on slides

275
00:20:07,180 --> 00:20:11,710
so if we do the two-dimensional representation about data so we retain the two largest

276
00:20:11,710 --> 00:20:13,310
variance columns

277
00:20:13,330 --> 00:20:16,960
from this data and then we look at a distance matrix

278
00:20:16,990 --> 00:20:20,900
representation of data that's what we see

279
00:20:20,950 --> 00:20:22,790
so it's trying to represent

280
00:20:24,600 --> 00:20:28,230
so it's not doing particularly good job i mean you can see it's getting the

281
00:20:29,350 --> 00:20:30,960
dominance a little bit

282
00:20:30,970 --> 00:20:34,460
but not really because it's the flattening all the way across that if we go

283
00:20:34,460 --> 00:20:38,410
to ten dimensions sort of things start to improve it

284
00:20:38,460 --> 00:20:40,050
but still

285
00:20:40,080 --> 00:20:43,260
it's not clear that the maxima is really one hundred eighty in fact this one's

286
00:20:43,260 --> 00:20:47,140
gone a bit of minimum quite close to one hundred eighty for ten dimensions isn't

287
00:20:47,240 --> 00:20:49,210
really helping

288
00:20:49,250 --> 00:20:50,330
we go to

289
00:20:50,350 --> 00:20:53,420
a hundred dimensions

290
00:20:54,160 --> 00:20:57,010
now you can start seeing perhaps this minimum

291
00:20:57,070 --> 00:21:01,990
about one hundred eighty you don't see this structure though this grading structure particularly well

292
00:21:02,660 --> 00:21:07,050
rotation of each ninety degrees so some of the finer structure isn't really that

293
00:21:07,140 --> 00:21:11,010
if you go to a thousand dimensions

294
00:21:11,010 --> 00:21:14,660
so we started off with the four thousand ninety six dimensional dataset

295
00:21:14,710 --> 00:21:15,640
and we have

296
00:21:15,680 --> 00:21:17,770
governor three courses mentions

297
00:21:17,830 --> 00:21:20,400
is actually a particularly great dimension reduction

298
00:21:20,410 --> 00:21:23,600
now you really start to see the structure of things seen in about one thousand

299
00:21:23,600 --> 00:21:26,340
dimensions if you're going to do feature selection

300
00:21:26,360 --> 00:21:33,160
to extract these guys and retain retained the quality of this in point distance matrix

301
00:21:33,200 --> 00:21:37,080
so in some sense you could say it's not a great algorithm the

302
00:21:37,100 --> 00:21:38,980
there you go

303
00:21:39,680 --> 00:21:42,000
so what we basically doing here

304
00:21:43,090 --> 00:21:44,520
this is now

305
00:21:44,560 --> 00:21:47,710
how high dimensional dataset it's got two dimensions and

306
00:21:47,750 --> 00:21:50,330
and what we're going to do is we're going to try and reduce it to

307
00:21:50,330 --> 00:21:52,340
one dimensional dataset

308
00:21:52,360 --> 00:21:56,200
so so i can show you how that productions taking place

309
00:21:56,200 --> 00:22:00,330
so what this variance maximum variance is saying is that we should look the direction

310
00:22:00,330 --> 00:22:05,940
of maximum variance along this direction experience looks like a standard deviation about six variance

311
00:22:05,940 --> 00:22:07,410
around thirty six

312
00:22:07,430 --> 00:22:09,890
around this direction it looks to be more like

313
00:22:09,940 --> 00:22:11,400
so there

314
00:22:13,000 --> 00:22:15,030
the standard deviation sixteen

315
00:22:15,050 --> 00:22:19,690
so here we should be retained in this direction and discarding this direction according to

316
00:22:19,690 --> 00:22:21,450
the algorithm we just described

317
00:22:21,490 --> 00:22:26,200
so what does that involve doing that involves projecting all these data points

318
00:22:26,210 --> 00:22:30,200
o onto the x axis and replacing original data

319
00:22:30,210 --> 00:22:32,280
with that

320
00:22:32,300 --> 00:22:34,670
so all structure with throwing away their

321
00:22:34,670 --> 00:22:36,770
between all these points immediately

322
00:22:36,810 --> 00:22:40,830
great isn't so that's the algorithm

323
00:22:42,230 --> 00:22:45,850
there's a bit of a more sensible algorithm that is very obvious

324
00:22:45,890 --> 00:22:48,340
if you think about one thing

325
00:22:50,180 --> 00:22:53,740
preserves interpoint distances i can rotate

326
00:22:53,790 --> 00:22:55,460
this dataset

327
00:22:55,460 --> 00:22:58,140
all the interpoint distances stay the same

328
00:22:58,150 --> 00:23:00,480
so if i make a rotation

329
00:23:00,560 --> 00:23:01,960
i'm doing nothing

330
00:23:01,970 --> 00:23:05,750
to lose any i can be make any rotation like

331
00:23:05,760 --> 00:23:08,680
so what should i do well it seems to make sense to try to take

332
00:23:08,680 --> 00:23:11,530
the data

333
00:23:11,540 --> 00:23:14,070
such that when i project

334
00:23:14,140 --> 00:23:15,440
onto the

335
00:23:15,440 --> 00:23:16,900
x axis

336
00:23:16,910 --> 00:23:19,230
i'm losing a lot less information

337
00:23:19,240 --> 00:23:23,910
so look at the size of these projections here

338
00:23:23,960 --> 00:23:26,400
this is the size of the projection

339
00:23:28,610 --> 00:23:33,120
and actually in this example there are quite strongly correlated you could envisage examples where

340
00:23:33,120 --> 00:23:35,150
it was even true

341
00:23:35,210 --> 00:23:40,200
well you've got a sort of less variance in that direction and more and sort

342
00:23:40,200 --> 00:23:42,250
stronger correlations

343
00:23:42,270 --> 00:23:43,820
and this becomes even true

344
00:23:43,870 --> 00:23:47,290
in fact this can be very true for the digital data the artificial data we

345
00:23:49,980 --> 00:23:51,680
sort of improved idea

346
00:23:51,690 --> 00:23:54,210
is to do this rotation first

347
00:23:55,180 --> 00:23:57,420
so we rotate

348
00:23:57,470 --> 00:24:01,560
to align data so the experience in the directions of maximum variance are aligned with

349
00:24:01,570 --> 00:24:06,730
the axes and then we extract these interpoint distances

350
00:24:07,910 --> 00:24:11,980
so we need the rotation that will minimise residual error

351
00:24:11,990 --> 00:24:15,690
we already derived an algorithm

352
00:24:15,740 --> 00:24:18,340
for discarding directions

353
00:24:18,400 --> 00:24:22,270
and it said that we should discard direction with maximum variance

354
00:24:22,340 --> 00:24:27,520
the error is then given by the sum of residual variances

355
00:24:27,560 --> 00:24:32,060
rotations of the data matrix do not effect this analysis at all

356
00:24:33,510 --> 00:24:37,360
those distances all remain the same

357
00:24:38,580 --> 00:24:40,680
what this means is we should rotate

358
00:24:40,680 --> 00:24:44,180
to find the directions of maximum variance

359
00:24:44,230 --> 00:24:48,140
we're so rotate so that the maximum variance directions are aligned with the axes and

360
00:24:48,140 --> 00:24:50,860
then discard the lower variance direction

361
00:24:50,920 --> 00:24:55,210
now you might recognise algorithm and will come back to that

362
00:24:55,320 --> 00:24:58,590
but if you do that this is what you get

363
00:24:58,600 --> 00:25:02,510
so on the left this is distances reconstructed from two dimensions

364
00:25:02,590 --> 00:25:04,300
of the data set

365
00:25:04,350 --> 00:25:09,970
remember the two dimensional example before

366
00:25:10,010 --> 00:25:13,060
well that

367
00:25:14,650 --> 00:25:17,820
i two dimensional example is already showing

368
00:25:17,870 --> 00:25:22,010
we've got a sort of minimum one hundred eighty to one hundred dimensions before we

369
00:25:22,010 --> 00:25:23,410
saw that structure

370
00:25:23,410 --> 00:25:27,580
in the example we're just doing feature selection

371
00:25:29,530 --> 00:25:32,790
this is distances reconstructed with ten dimensions

372
00:25:33,520 --> 00:25:36,210
you're seeing the fine structure in the grids

373
00:25:36,210 --> 00:25:37,820
with only ten dimensions

374
00:25:37,840 --> 00:25:38,840
it took

375
00:25:38,850 --> 00:25:40,920
a thousand dimensions

376
00:25:40,970 --> 00:25:44,420
to see that before

377
00:25:44,470 --> 00:25:47,620
if we go two hundred dimensions

378
00:25:47,620 --> 00:25:52,680
low ph writes about ph three first forecast benzoate gas so

379
00:25:52,710 --> 00:25:58,510
what's going to happen is this virion will respond in a defensive manner

380
00:25:58,530 --> 00:26:00,370
there's there's the principle

381
00:26:00,370 --> 00:26:03,870
that's in chemistry that is analogous to newton's third law

382
00:26:03,880 --> 00:26:04,530
you know

383
00:26:04,550 --> 00:26:06,270
action reaction

384
00:26:06,320 --> 00:26:08,810
it's called alicia tell you principle

385
00:26:08,820 --> 00:26:11,610
alicia telling principle

386
00:26:11,620 --> 00:26:12,990
after the french

387
00:26:13,020 --> 00:26:15,620
scientist who enunciated

388
00:26:15,620 --> 00:26:17,430
should tell you principle

389
00:26:17,470 --> 00:26:21,440
and it and it talks about the restoring force

390
00:26:21,440 --> 00:26:23,420
the restoring force

391
00:26:23,470 --> 00:26:25,790
exerted by chemical system

392
00:26:25,800 --> 00:26:29,480
sort of force exerted by

393
00:26:29,620 --> 00:26:31,960
chemical system

394
00:26:31,960 --> 00:26:35,500
and what is this restoring force designed to do

395
00:26:35,550 --> 00:26:37,430
it's designed to

396
00:26:37,430 --> 00:26:40,510
minimize the impact of any perturbation

397
00:26:40,630 --> 00:26:42,660
designed to minimize

398
00:26:45,770 --> 00:26:49,650
the effect of any perturbation

399
00:26:49,660 --> 00:26:53,510
by perturbation i mean out of physical preservation i mean chemical shift any kind of

400
00:26:53,510 --> 00:26:55,140
chemical perturbation

401
00:26:59,600 --> 00:27:01,050
let's say we have the

402
00:27:01,070 --> 00:27:06,710
z iron sitting happily in neutral solution and suddenly the ph

403
00:27:08,860 --> 00:27:11,180
suddenly the ph drops

404
00:27:12,100 --> 00:27:14,940
how can the virion respond

405
00:27:14,960 --> 00:27:16,890
what do i do to write two

406
00:27:16,900 --> 00:27:21,150
bring the ph back to where it was before the disturbance

407
00:27:21,220 --> 00:27:24,670
what is the manifestation of ph it's prototype

408
00:27:24,720 --> 00:27:25,930
so somehow

409
00:27:25,940 --> 00:27:29,740
the proton concentration suddenly one way up

410
00:27:29,770 --> 00:27:32,200
driving the ph way down

411
00:27:32,250 --> 00:27:34,630
annunziata iron sitting here

412
00:27:34,640 --> 00:27:36,650
is there are right here

413
00:27:36,730 --> 00:27:42,050
we consider in do to minimize the impact this flood of protons

414
00:27:42,100 --> 00:27:46,310
well of the two possibilities how do you neutralise something you either flood this with

415
00:27:47,480 --> 00:27:50,540
start flooding this with hydroxyl

416
00:27:50,600 --> 00:27:52,460
in order to neutralize

417
00:27:52,500 --> 00:27:56,180
i will consider i and i see no hydroxyl so that's no good

418
00:27:56,220 --> 00:27:58,220
but what could around

419
00:27:58,260 --> 00:28:01,120
there's a carboxylic acid and

420
00:28:01,160 --> 00:28:03,440
an attachment site for protons

421
00:28:03,490 --> 00:28:05,290
so what's the i can do

422
00:28:05,300 --> 00:28:07,200
is to gobble up

423
00:28:07,910 --> 00:28:09,540
excess proton

424
00:28:09,570 --> 00:28:10,870
in order to

425
00:28:10,870 --> 00:28:13,670
bring the ph back to where it was before

426
00:28:13,680 --> 00:28:15,990
so let's write a reaction

427
00:28:16,050 --> 00:28:19,320
it triggers this response which is

428
00:28:20,800 --> 00:28:21,980
we know

429
00:28:22,210 --> 00:28:33,720
when i put the

430
00:28:33,730 --> 00:28:36,980
but the carboxylic acid over to the right here

431
00:28:36,990 --> 00:28:39,720
so now this is from the solution

432
00:28:39,730 --> 00:28:43,790
this is from the solution is trying to bring the ph back to where it

433
00:28:43,790 --> 00:28:44,900
was before

434
00:28:46,150 --> 00:28:47,700
the proton

435
00:28:47,710 --> 00:28:51,590
and attaching the proton to this carboxylic acid and

436
00:28:51,630 --> 00:29:02,440
so what's happened out its veteran

437
00:29:02,490 --> 00:29:06,320
this and the negative and has been capped so better i'm now

438
00:29:06,400 --> 00:29:08,900
is a net positive

439
00:29:08,940 --> 00:29:14,970
it's not positive now over here it's not neutral

440
00:29:14,970 --> 00:29:18,800
and it's gobbling up protons from the solution

441
00:29:18,840 --> 00:29:21,800
so this is really an acid base reaction

442
00:29:21,820 --> 00:29:23,740
for which i can write k

443
00:29:23,760 --> 00:29:29,860
the acid dissociation constant and you know i'm getting tired writing all these characters our

444
00:29:31,090 --> 00:29:34,320
reformat this so i'm going to say this is

445
00:29:34,360 --> 00:29:37,570
the virion is going to represent as

446
00:29:37,590 --> 00:29:40,110
proton plus the rest

447
00:29:40,150 --> 00:29:42,300
this is proton

448
00:29:42,320 --> 00:29:44,630
and what's this thing here this is now

449
00:29:44,670 --> 00:29:46,010
there's been an iron

450
00:29:46,030 --> 00:29:51,670
the which proton has attached i've attached the proton to the net neutral species k

451
00:29:51,690 --> 00:29:53,780
i can rewrite this reaction in

452
00:29:53,780 --> 00:29:59,440
the more compact notation so a captures all of this other stuff minus the

453
00:30:02,920 --> 00:30:05,510
so i can write the k for that reaction

454
00:30:05,530 --> 00:30:07,570
OK one will equal

455
00:30:07,590 --> 00:30:09,670
concentration of proton

456
00:30:09,670 --> 00:30:12,150
times the concentration of

457
00:30:12,170 --> 00:30:16,970
neutrals virion over the concentration of this

458
00:30:16,990 --> 00:30:23,340
protein it's better on the iron onto which we've glommed this

459
00:30:23,360 --> 00:30:25,400
proton from the

460
00:30:25,400 --> 00:30:28,440
solution and we're going to take a page out of sorensen's book and will take

461
00:30:28,440 --> 00:30:30,650
the log of both sides

462
00:30:30,690 --> 00:30:32,550
and then we can say that

463
00:30:32,590 --> 00:30:34,050
pk one

464
00:30:34,050 --> 00:30:35,490
is equal to

465
00:30:35,510 --> 00:30:36,820
he h

466
00:30:39,260 --> 00:30:43,920
but i'm writing ten here but you know in engineering if you see ILOG it's

467
00:30:43,920 --> 00:30:47,880
typically log base ten i know some of the math classes

468
00:30:47,880 --> 00:30:49,130
they use the

469
00:30:49,170 --> 00:30:51,820
elegy to represent natural log but

470
00:30:51,920 --> 00:30:54,550
it's uncommon engineering

471
00:30:54,590 --> 00:30:56,300
right so here's the

472
00:30:56,300 --> 00:30:58,820
here's the sorenson version of the equation

473
00:30:58,840 --> 00:31:02,920
and what this tells us is that at fifty percent

474
00:31:02,940 --> 00:31:09,900
fifty percent association words when fifty percent of the z orion has gobbled up

475
00:31:09,940 --> 00:31:14,800
the proton to generate this protein eighteen species

476
00:31:14,840 --> 00:31:17,090
this ratio is equal

477
00:31:17,130 --> 00:31:18,440
fifty percent

478
00:31:18,440 --> 00:31:22,240
z orion on protein eight fifty percent protein eight is of the log of one

479
00:31:22,240 --> 00:31:26,610
is zero and that's the phd gives us the value of the

480
00:31:26,670 --> 00:31:29,260
of the acid dissociation constant

481
00:31:29,280 --> 00:31:31,740
fifty percent of

482
00:31:37,340 --> 00:31:39,760
spiteri and we have

483
00:31:39,760 --> 00:31:41,300
the concentration

484
00:31:41,360 --> 00:31:42,970
of the sphere

485
00:31:42,970 --> 00:31:46,430
the motivation

486
00:31:47,200 --> 00:31:52,410
research point of view is that there's a great deal of interest on

487
00:31:52,820 --> 00:31:56,570
something called deep belief networks and you're going to get three hours of lectures on

488
00:31:56,600 --> 00:32:00,150
the belief networks by geoff hinton but let me quote him

489
00:32:00,260 --> 00:32:01,860
even though he's not here

490
00:32:02,520 --> 00:32:04,100
this is from scholarpedia

491
00:32:04,100 --> 00:32:09,690
deep belief nets are probabilistic generative models that are composed of multiple layers of stochastic

492
00:32:09,690 --> 00:32:13,380
latent variables you should hopefully understand what that means

493
00:32:13,390 --> 00:32:15,520
and i can draw pictures as well

494
00:32:15,530 --> 00:32:21,450
the latent variables typically have binary values and are often called hidden units or feature

495
00:32:21,450 --> 00:32:26,590
detectors so just imagine multiple layers of binary variables

496
00:32:27,350 --> 00:32:31,840
the top two layers of undirected symmetric connections OK so the top two is an

497
00:32:31,840 --> 00:32:34,780
undirected graphical model of some kind

498
00:32:34,810 --> 00:32:39,680
the lower layers receive top-down directed connections from the layer below

499
00:32:39,690 --> 00:32:43,810
OK so the the top looks like an undirected graph and the lower layers look

500
00:32:43,810 --> 00:32:46,600
like a directed graph that layer

501
00:32:49,060 --> 00:32:53,170
the states of the units in the lowest layer represent a data vector so the

502
00:32:53,170 --> 00:32:57,110
data appears at the bottom and all of these layers

503
00:32:57,130 --> 00:33:01,390
on top of the data are hidden layers that are meant to explain

504
00:33:01,440 --> 00:33:02,480
what the data

505
00:33:04,800 --> 00:33:05,980
so that's it

506
00:33:06,010 --> 00:33:11,930
a quote from jeff of what people eat that is and

507
00:33:11,980 --> 00:33:13,510
here's what we're going to do

508
00:33:14,240 --> 00:33:17,940
there's been a huge amount of interest in this area there are lots of papers

509
00:33:17,940 --> 00:33:25,060
on deep belief nets but people have addressed some interesting questions in particular they have

510
00:33:25,060 --> 00:33:30,140
to address questions of for example how deep should this network be given some data

511
00:33:30,440 --> 00:33:35,440
do we need one or three or seven layers to model the data

512
00:33:35,450 --> 00:33:40,230
how wide should each layer be doing the ten hidden units per layer or a

513
00:33:40,230 --> 00:33:43,140
hundred or thousand for example

514
00:33:43,190 --> 00:33:48,230
and and also what sorts of units should we have should we have binary units

515
00:33:48,230 --> 00:33:53,930
continuous units etcetera so the definition had usually binary units or something like that but

516
00:33:53,930 --> 00:33:57,900
we could have other kinds of units can the model learn this stuff

517
00:33:57,910 --> 00:33:59,810
from the data

518
00:33:59,940 --> 00:34:03,310
so the goal of what i'm going to describe this to learn the structure of

519
00:34:03,350 --> 00:34:04,730
the network

520
00:34:04,770 --> 00:34:07,020
and the approach is going to be

521
00:34:07,030 --> 00:34:09,980
to have a nonparametric bayesian methods

522
00:34:09,980 --> 00:34:15,600
the learned structure of a layered directed deep belief network from data so i'm going

523
00:34:15,600 --> 00:34:20,530
to ignore this sort of definition that had undirected connections the popular and i'm just

524
00:34:20,530 --> 00:34:22,720
going to assume is directed all the way down

525
00:34:25,150 --> 00:34:27,530
the model might look like this

526
00:34:27,570 --> 00:34:33,090
the data are at the bottom it's directed acyclic graph with layers

527
00:34:33,100 --> 00:34:37,690
and each other some variable number of layers in each layer as some

528
00:34:37,730 --> 00:34:40,280
variable width

529
00:34:41,650 --> 00:34:45,440
how many layers should there be how wide should each layer b

530
00:34:45,490 --> 00:34:46,890
all right so

531
00:34:46,890 --> 00:34:50,880
what we want to do is we want to do inference over these quantities the

532
00:34:50,880 --> 00:34:54,800
number of layers and the width of the layers so the way we're going to

533
00:34:54,810 --> 00:34:59,270
do this is we're going to think of hidden variables

534
00:35:00,730 --> 00:35:02,800
the structure of the graph

535
00:35:02,810 --> 00:35:04,180
so we're going to say

536
00:35:04,190 --> 00:35:09,410
z i j and horsey i j and keep a happy

537
00:35:09,470 --> 00:35:12,240
is equal to one

538
00:35:12,260 --> 00:35:14,110
means that j

539
00:35:14,140 --> 00:35:17,640
is a parent of node i

540
00:35:17,650 --> 00:35:18,770
in layer m

541
00:35:18,770 --> 00:35:22,640
that is node j is the parent node i'm layer and so these are binary

542
00:35:22,640 --> 00:35:27,150
variables representing the connections that appear in this

543
00:35:30,380 --> 00:35:37,410
in particular if we specify a sequence of binary matrices one for each layer

544
00:35:37,410 --> 00:35:53,940
all right so

545
00:35:54,040 --> 00:35:55,430
start again

546
00:35:55,440 --> 00:35:57,840
so just to

547
00:35:57,930 --> 00:36:00,380
to review

548
00:36:00,400 --> 00:36:01,680
we're looking at

549
00:36:02,990 --> 00:36:04,600
general approach

550
00:36:04,640 --> 00:36:06,280
optimizing the

551
00:36:06,300 --> 00:36:11,290
by risk the expectation of five y times f of x

552
00:36:11,310 --> 00:36:16,760
and asking the question when does so supposing we had complete information about the probability

553
00:36:17,980 --> 00:36:25,600
or you could leave work in the population case when does minimisation of the

554
00:36:25,610 --> 00:36:27,260
expectation of a

555
00:36:27,290 --> 00:36:32,340
phi correspond to minimisation of the risk

556
00:36:32,390 --> 00:36:40,040
so we have these definitions motivated by by considering the conditional expectation of five y

557
00:36:40,050 --> 00:36:41,690
times f of x

558
00:36:41,700 --> 00:36:44,040
when we fix the single x

559
00:36:44,050 --> 00:36:49,250
and if we think about the conditional probability of x

560
00:36:49,410 --> 00:36:51,540
having some value you

561
00:36:51,560 --> 00:36:53,430
and we minimize over

562
00:36:53,440 --> 00:36:56,820
all values that f of x might take

563
00:36:56,830 --> 00:37:01,480
the conditional expectation of five y times x and that's

564
00:37:03,350 --> 00:37:07,110
if we compare that to

565
00:37:07,330 --> 00:37:09,520
h minus the leader

566
00:37:09,540 --> 00:37:14,650
eight months later is the minimum of the same quantity subject to the constraint that

567
00:37:14,670 --> 00:37:16,050
the value

568
00:37:16,090 --> 00:37:20,770
the argument here that the alpha that corresponds to the set of x fixed point

569
00:37:22,730 --> 00:37:25,880
has the wrong sign sign is different from

570
00:37:25,890 --> 00:37:29,910
the sign of the minus one

571
00:37:29,960 --> 00:37:33,890
so we compare these two certainly a cheerleader should be

572
00:37:33,900 --> 00:37:37,820
the the optimum should be better than this this constraint up

573
00:37:37,830 --> 00:37:41,320
what we like is that it's strictly better

574
00:37:41,340 --> 00:37:42,140
right so

575
00:37:42,150 --> 00:37:43,780
if we look

576
00:37:43,800 --> 00:37:46,180
and focus on single x

577
00:37:46,210 --> 00:37:48,320
and we certainly but i hope that

578
00:37:49,120 --> 00:37:55,040
optimisation the constrained to have the wrong sign will lead to the evalu

579
00:37:55,060 --> 00:37:57,150
then we may not constrained

580
00:37:57,170 --> 00:37:58,590
because otherwise

581
00:37:58,600 --> 00:38:01,540
we could get the optimal value

582
00:38:02,260 --> 00:38:07,570
without getting the right side and then for classification would be in trouble right for

583
00:38:07,570 --> 00:38:10,630
the risk indicator of going the same route would be in trouble

584
00:38:10,650 --> 00:38:14,820
OK so we call this classification calibrated if we have strict inequality

585
00:38:14,830 --> 00:38:18,980
inequality for all conditional probability is not equal to one

586
00:38:19,090 --> 00:38:21,170
because that's a point wise

587
00:38:21,190 --> 00:38:24,810
kind of a condition

588
00:38:24,960 --> 00:38:30,960
the last definition before i tell you the result

589
00:38:30,980 --> 00:38:32,610
and i'm

590
00:38:32,630 --> 00:38:36,930
i'm standing the definition just for convex phi actually everything that i have to say

591
00:38:36,930 --> 00:38:39,660
here is is true more generally

592
00:38:39,680 --> 00:38:44,500
we just is more complicated for non convex five it's in the whole motivation here

593
00:38:44,500 --> 00:38:51,200
is to use a convex surrogate for this this cost function you know it's

594
00:38:51,220 --> 00:38:52,970
it's probably

595
00:38:53,290 --> 00:38:57,490
reasonable to look at the specific case where phi is is convex it it's a

596
00:38:57,490 --> 00:38:58,730
little simpler

597
00:39:01,370 --> 00:39:02,950
so given a convex

598
00:39:03,550 --> 00:39:05,720
we can define this this function

599
00:39:05,730 --> 00:39:06,560
so i

600
00:39:06,570 --> 00:39:11,600
so at theta it's five zero

601
00:39:12,840 --> 00:39:15,880
h one plus the driver two

602
00:39:15,900 --> 00:39:17,460
right now

603
00:39:17,470 --> 00:39:22,720
we could equivalently written this as h minus one plus the driver two

604
00:39:23,210 --> 00:39:24,540
right to the

605
00:39:24,550 --> 00:39:30,660
five zero is for convex phi we can always achieve the optimal meant

606
00:39:30,670 --> 00:39:32,750
five zero

607
00:39:36,560 --> 00:39:38,610
all right so let's look at this example

608
00:39:39,150 --> 00:39:44,870
you you recall this is the

609
00:39:44,880 --> 00:39:49,130
this is the shape of a church when we looked at that truncated quadratic

610
00:39:50,790 --> 00:39:52,650
it has this entropy like

611
00:39:56,010 --> 00:40:00,710
sia theta as we move away from the half this thing is is symmetric about

612
00:40:02,770 --> 00:40:04,340
i guess that's obvious from the

613
00:40:04,350 --> 00:40:09,330
from the definition here we can change it to to one minus eta we're optimizing

614
00:40:09,330 --> 00:40:12,120
over the old alpha so we haven't changed anything

615
00:40:12,320 --> 00:40:15,720
right so these things are symmetric about half so as you move away from the

616
00:40:15,720 --> 00:40:17,510
half we're looking at how does

617
00:40:18,410 --> 00:40:22,710
differ from five zero five zero is the value at either equals half up here

618
00:40:22,720 --> 00:40:26,690
which is one in this case so what and how h decreases as we move

619
00:40:26,690 --> 00:40:28,120
away from

620
00:40:29,310 --> 00:40:31,980
the maximal value and that this function

621
00:40:32,030 --> 00:40:34,530
in red a sign

622
00:40:36,210 --> 00:40:39,410
all right so this we can equivalently think of it as as i say the

623
00:40:39,410 --> 00:40:43,250
difference between a ge minus at some value and h at that value

624
00:40:45,160 --> 00:40:49,560
you know the gap right we want h minus to be strictly bigger than a

625
00:40:49,560 --> 00:40:53,900
ge this function is is looking at the gap between the two in the non

626
00:40:53,900 --> 00:40:57,510
convex case we have to do something else to make this a convex function it

627
00:40:57,510 --> 00:41:00,930
turns out you will see later on we with the property and that's the the

628
00:41:00,930 --> 00:41:04,810
right thing to do but as i say you know there's a little

629
00:41:04,850 --> 00:41:08,540
a little bit of extra extra work but all of the results that that are

630
00:41:08,540 --> 00:41:12,490
present go through in the in the case of arbitrary

631
00:41:12,540 --> 00:41:16,850
five bounded from below

632
00:41:16,940 --> 00:41:19,560
all right so

633
00:41:19,580 --> 00:41:23,720
this is the main theorem so any questions about the definitions

634
00:41:23,730 --> 00:41:26,790
so this these quantities you should all be clear

635
00:41:28,510 --> 00:41:30,880
OK so so he is the

636
00:41:30,900 --> 00:41:38,470
the main results about the relationship between risk and virus it's actually relationship between

637
00:41:38,470 --> 00:41:41,120
so this is a unique name

638
00:41:41,170 --> 00:41:43,340
the idea is that different rest

639
00:41:45,190 --> 00:41:52,580
but there involved in the same potential function which is usually so i given that

640
00:41:52,720 --> 00:41:56,660
so many of these two x two x five x was

641
00:41:56,700 --> 00:42:00,230
graph that user x sixty two

642
00:42:00,260 --> 00:42:06,150
OK that's the way because they appear argument conditional access

643
00:42:06,160 --> 00:42:09,730
so the first thing to do the right thing to

644
00:42:10,870 --> 00:42:13,810
parents and

645
00:42:13,830 --> 00:42:20,490
after the draft directive on a section on the rest of so this not formalisation

646
00:42:20,490 --> 00:42:25,020
because his parents very happy here

647
00:42:25,070 --> 00:42:26,670
i mean

648
00:42:26,690 --> 00:42:28,870
that's what i call

649
00:42:31,590 --> 00:42:33,990
that's just take

650
00:42:34,050 --> 00:42:36,870
explain this is really about that

651
00:42:36,900 --> 00:42:37,840
o course

652
00:42:37,860 --> 00:42:39,310
when we were beautiful

653
00:42:39,310 --> 00:42:44,050
not forget and sees that might come up very different situations so for

654
00:42:44,310 --> 00:42:49,790
in addition and put in all the links of his car power

655
00:42:55,320 --> 00:43:03,400
so i'm trying to teach you and general procedure which is correct and also notion

656
00:43:03,400 --> 00:43:05,230
of multiple queries

657
00:43:07,140 --> 00:43:08,220
queries per

658
00:43:08,230 --> 00:43:14,340
during learning with latent variables in the end and also when you call to infer

659
00:43:14,760 --> 00:43:19,540
something about the world so these queries was

660
00:43:19,550 --> 00:43:22,560
the in different

661
00:43:22,570 --> 00:43:23,940
that's exactly area

662
00:43:23,940 --> 00:43:27,470
first learning and also

663
00:43:27,490 --> 00:43:34,680
right so that means for the tree structured graphical models

664
00:43:34,690 --> 00:43:40,260
the first of all there no clue about changes

665
00:43:40,280 --> 00:43:42,980
by the whole

666
00:43:44,180 --> 00:43:47,170
the question models are also trees

667
00:43:47,310 --> 00:43:51,430
exact inference of tree is the basis for

668
00:43:51,440 --> 00:43:56,050
which is also the general exact inference problems already

669
00:43:56,060 --> 00:44:02,210
that was the basis for many algorithms which we're going to to

670
00:44:02,230 --> 00:44:05,850
that's very small trees

671
00:44:05,910 --> 00:44:07,310
and more

672
00:44:07,330 --> 00:44:11,490
in the case of trees because there's no explaining away trees can be very simple

673
00:44:11,990 --> 00:44:14,660
for example so it's

674
00:44:15,300 --> 00:44:19,230
in the last year graph one

675
00:44:19,260 --> 00:44:20,710
makes problem

676
00:44:20,740 --> 00:44:24,310
so just just to remind you you know this graph here

677
00:44:25,670 --> 00:44:27,890
this graph here is not a tree

678
00:44:27,910 --> 00:44:30,170
because this has more than one

679
00:44:31,430 --> 00:44:39,160
this graph is still singly connected to but they are for example not to this

680
00:44:39,160 --> 00:44:40,970
is simple

681
00:44:41,060 --> 00:44:45,420
OK so

682
00:44:45,440 --> 00:44:48,810
now in the first two general strategy

683
00:44:48,820 --> 00:44:50,160
for surfaces

684
00:44:50,220 --> 00:44:53,320
on trees and what is the

685
00:44:53,440 --> 00:44:55,240
these trees

686
00:44:55,260 --> 00:44:57,110
work towards node

687
00:44:57,400 --> 00:44:59,020
so we consider

688
00:44:59,450 --> 00:45:02,440
so some nodes in the tree

689
00:45:03,290 --> 00:45:05,840
which has the highest

690
00:45:07,410 --> 00:45:12,470
he died in the middle of the inference procedure may seem obscure to you

691
00:45:12,490 --> 00:45:13,740
because i didn't talk about

692
00:45:13,760 --> 00:45:18,120
clearly are the sons are converge onto writing

693
00:45:18,140 --> 00:45:22,070
i think that but i just mentioned we have

694
00:45:22,090 --> 00:45:23,430
that thousands the nodes

695
00:45:23,440 --> 00:45:25,190
no i j is one

696
00:45:25,280 --> 00:45:29,190
we're eventually you have to know why is that

697
00:45:29,210 --> 00:45:32,730
and this is the symmetry of

698
00:45:33,980 --> 00:45:36,190
when we said j

699
00:45:36,190 --> 00:45:42,590
one of the variables excluded here actors create something of the

700
00:45:42,610 --> 00:45:45,660
so that is the case here

701
00:45:46,600 --> 00:45:53,520
we're these variables one by one and we the same variable we're left with is

702
00:45:54,720 --> 00:45:57,040
conditional distribution

703
00:45:57,050 --> 00:46:01,200
and that there are just so you have an example when we say that x

704
00:46:01,200 --> 00:46:04,070
five from quite

705
00:46:04,480 --> 00:46:05,650
got some

706
00:46:07,070 --> 00:46:12,700
which depend on everything that was to on x three and x

707
00:46:14,390 --> 00:46:17,320
six next so

708
00:46:17,390 --> 00:46:22,270
when we said we want to use one only seven j

709
00:46:22,340 --> 00:46:26,710
military what factors could be created

710
00:46:26,790 --> 00:46:28,140
and tree

711
00:46:28,170 --> 00:46:32,310
we can actually vary from state so that know

712
00:46:32,390 --> 00:46:37,130
subtrees below j when say well i mean for history

713
00:46:37,160 --> 00:46:43,390
now the nose in search of these because there are already some day if we

714
00:46:43,390 --> 00:46:45,280
proceed from the leaves to root

715
00:46:45,290 --> 00:46:46,610
once you get to j

716
00:46:46,640 --> 00:46:49,460
everything is true that already

717
00:46:49,460 --> 00:46:55,630
so in order the situation is that he always showed before

718
00:46:55,660 --> 00:46:59,240
parents by the time we get to do everything in the

719
00:46:59,240 --> 00:47:01,860
gardens so there's no way to any

720
00:47:01,880 --> 00:47:04,450
factors involved no labels

721
00:47:04,460 --> 00:47:06,070
and i think

722
00:47:06,090 --> 00:47:14,970
if you have any other subtrees since some over a dozen interactive all trees

723
00:47:14,970 --> 00:47:17,620
he has to treat so there's no

724
00:47:17,660 --> 00:47:22,790
interaction between the particles so only the only

725
00:47:22,790 --> 00:47:25,880
back to create the day

726
00:47:25,900 --> 00:47:29,310
how do we know which is unique heritage

727
00:47:29,320 --> 00:47:32,270
after years of conditional distributions

728
00:47:32,310 --> 00:47:33,310
p j

729
00:47:34,210 --> 00:47:35,860
so here's

730
00:47:37,090 --> 00:47:38,240
this is true

731
00:47:39,400 --> 00:47:46,480
there's true here some some three years later is a tree which she figures

732
00:47:46,480 --> 00:47:53,070
we are almost ready to begin so welcome to the third

733
00:47:53,100 --> 00:47:56,670
tutorial review session of this

734
00:47:56,680 --> 00:47:59,350
workshop in summer school

735
00:48:00,540 --> 00:48:01,940
as you know we

736
00:48:01,940 --> 00:48:07,980
i have these sort of tutorial reviews running all through the school for ten days

737
00:48:07,980 --> 00:48:08,710
or so

738
00:48:10,550 --> 00:48:17,840
what we're going to talk about today is is

739
00:48:18,370 --> 00:48:25,260
about geometric methods and manifold learning so what michelle belkin and

740
00:48:25,280 --> 00:48:26,390
i'm going to do

741
00:48:26,420 --> 00:48:37,180
is split of presentation between the two of us and hopefully what people would be

742
00:48:37,180 --> 00:48:40,710
able to communicate to you is

743
00:48:40,750 --> 00:48:42,950
some idea

744
00:48:44,600 --> 00:48:47,650
class of methods that

745
00:48:48,030 --> 00:48:52,600
i have received some attention over the last decade or so

746
00:48:52,760 --> 00:48:59,640
and the perspective on machine learning that seems to have emerged over the last decade

747
00:48:59,650 --> 00:49:01,290
two and it's very

748
00:49:01,870 --> 00:49:08,860
sort of geometrically oriented point of view with which one can approach questions in that

749
00:49:08,870 --> 00:49:11,210
recognition and machine learning

750
00:49:11,250 --> 00:49:16,200
o and sometimes even in numerical computation

751
00:49:16,670 --> 00:49:22,870
and hopefully you will get a sense of the way in which ideas from

752
00:49:22,890 --> 00:49:25,260
probability and statistics

753
00:49:25,260 --> 00:49:26,980
ideas from

754
00:49:27,280 --> 00:49:29,030
computer science

755
00:49:29,060 --> 00:49:32,360
algorithms and especially graph theory

756
00:49:32,570 --> 00:49:34,340
and ideas from

757
00:49:36,460 --> 00:49:38,170
geometry and topology

758
00:49:38,200 --> 00:49:42,310
come together in some sort of a natural way

759
00:49:42,340 --> 00:49:44,930
in this setting

760
00:49:46,430 --> 00:49:49,140
the logistics of this is like this

761
00:49:49,150 --> 00:49:52,620
i speak for about

762
00:49:52,640 --> 00:49:57,220
and our no more than an hour and fifteen minutes after which we

763
00:49:58,320 --> 00:50:02,420
they can break for about fifteen minutes may show with

764
00:50:02,610 --> 00:50:06,770
then speak for about an hour after that

765
00:50:06,790 --> 00:50:09,660
maybe an hour and fifteen minutes

766
00:50:09,700 --> 00:50:11,700
will take a second break

767
00:50:11,700 --> 00:50:13,640
and then

768
00:50:13,670 --> 00:50:17,580
we'll have a closing session for about half an hour to wrap up odds and

769
00:50:17,580 --> 00:50:20,140
ends and things of that sort

770
00:50:24,260 --> 00:50:28,380
the context

771
00:50:28,390 --> 00:50:30,860
in which all of this

772
00:50:30,880 --> 00:50:32,850
really developed

773
00:50:32,910 --> 00:50:38,130
is really the context of data analysis and machine learning problems in in very very

774
00:50:38,130 --> 00:50:38,950
high dimensions

775
00:50:39,420 --> 00:50:43,770
so increasingly we live in a world in which we are faced all the time

776
00:50:43,770 --> 00:50:50,170
by learning problems in very very high dimensional spaces in genetics and neuroscience in image

777
00:50:50,170 --> 00:50:54,520
analysis piece analysis or other problems that we might have worked on

778
00:50:58,360 --> 00:51:04,330
the basic and fundamental sort of question that faces all of us is to try

779
00:51:04,330 --> 00:51:08,600
and understand under what circumstances it might be possible

780
00:51:08,640 --> 00:51:14,610
to make effective inferences in high dimensional spaces without running into

781
00:51:14,640 --> 00:51:17,820
the so-called curse of dimensionality

782
00:51:17,830 --> 00:51:19,720
OK so

783
00:51:19,730 --> 00:51:22,640
as far as i see it

784
00:51:22,830 --> 00:51:26,920
people have course known about the curse of dimensionality for a long time

785
00:51:28,660 --> 00:51:30,540
there are at least

786
00:51:30,540 --> 00:51:33,160
three different points of view

787
00:51:33,540 --> 00:51:38,420
from which one can approach the central issue

788
00:51:38,440 --> 00:51:42,070
the first and the most classical of these

789
00:51:42,080 --> 00:51:45,160
is the point of view of smoothness

790
00:51:45,200 --> 00:51:47,390
so that we clarify

791
00:51:49,020 --> 00:51:51,420
it has been known for a long time

792
00:51:52,700 --> 00:51:55,520
suppose you're trying to learn the function

793
00:51:55,570 --> 00:51:59,350
and it's a function of several variables

794
00:52:03,790 --> 00:52:05,100
has say

795
00:52:05,630 --> 00:52:08,380
s derivatives

796
00:52:09,290 --> 00:52:10,920
so it belongs to

797
00:52:10,980 --> 00:52:14,660
what is called the sobolev space h HSA

798
00:52:14,670 --> 00:52:16,090
so it's a function

799
00:52:16,100 --> 00:52:17,330
maybe actually used

800
00:52:17,350 --> 00:52:26,630
border little bit

801
00:52:27,090 --> 00:52:34,250
actually show you are trying to use this hand

802
00:52:34,280 --> 00:52:37,480
OK maybe i'll use this might maybe this is that OK

803
00:52:37,600 --> 00:52:41,660
so i have a function

804
00:52:41,690 --> 00:52:44,080
from are the

805
00:52:44,120 --> 00:52:45,440
two are

806
00:52:45,460 --> 00:52:49,590
so it's a function of the variables and it has its derivatives and l tools

807
00:52:49,590 --> 00:52:51,460
so f

808
00:52:51,500 --> 00:52:53,180
belongs to h s

809
00:52:54,030 --> 00:52:58,870
something like this then the rates of convergence looked like this this is the number

810
00:52:58,870 --> 00:53:01,850
of examples you need to learn a function

811
00:53:03,410 --> 00:53:04,810
and i think

812
00:53:04,860 --> 00:53:06,670
it's actually something

813
00:53:06,690 --> 00:53:11,100
like this to us plus the something like OK not as would be but approximately

814
00:53:11,100 --> 00:53:12,040
is already

815
00:53:14,380 --> 00:53:16,100
if you look at that rate

816
00:53:17,930 --> 00:53:21,060
you see that if s the smoothness

817
00:53:21,150 --> 00:53:24,900
is of order d

818
00:53:25,870 --> 00:53:30,100
the rate does not depend upon the dimension

819
00:53:30,100 --> 00:53:31,490
and therefore

820
00:53:31,500 --> 00:53:34,730
there is no cause of dimensionality

821
00:53:34,790 --> 00:53:38,660
OK so this is of course known for a long time and so people realize

822
00:53:38,670 --> 00:53:42,810
that if the function you're trying to learn a function of several variables in a

823
00:53:42,810 --> 00:53:47,850
function in high dimensional spaces but it's very small and the smoothness

824
00:53:47,920 --> 00:53:50,250
is in the particular sense

825
00:53:50,290 --> 00:53:55,030
then there is no curse of dimensionality and this insight on this observation has an

826
00:53:55,030 --> 00:53:57,170
algorithmic reflection

827
00:53:58,160 --> 00:53:59,990
so based on this insight

828
00:54:00,020 --> 00:54:03,530
people have realized for a long time that perhaps it's a good idea

829
00:54:03,690 --> 00:54:08,080
to try and learn smooth functions perhaps it's try it's a good idea to try

830
00:54:08,540 --> 00:54:12,580
and fit the smoothest function that you can do the data

831
00:54:12,580 --> 00:54:16,490
any other set of questions or requests

832
00:54:21,460 --> 00:54:23,190
it's more about this

833
00:54:24,480 --> 00:54:27,370
the fundamental from the classifier

834
00:54:27,370 --> 00:54:30,030
things where this one zero instead

835
00:54:30,030 --> 00:54:36,870
that in here

836
00:54:46,190 --> 00:54:47,270
all o

837
00:54:49,970 --> 00:55:01,710
the part of the because i'm i'm surprisingly maybe

838
00:55:01,720 --> 00:55:05,160
from what i sound like i'm not trained as a statistician

839
00:55:05,170 --> 00:55:10,830
and i if you pick up any classic textbook in statistics they'll tell you about

840
00:55:10,830 --> 00:55:12,200
frequentist theory

841
00:55:12,430 --> 00:55:15,240
there are different kinds of frequentist theory as well

842
00:55:15,250 --> 00:55:19,630
and i think it would do you really poor job if i try to explain

843
00:55:20,920 --> 00:55:24,990
OK so

844
00:55:26,710 --> 00:55:30,740
all talk about the classification problem

845
00:55:34,810 --> 00:55:38,980
let me just show you a particular picture

846
00:55:39,020 --> 00:55:43,190
OK let's let's let's deal with this problem because i think

847
00:55:44,130 --> 00:55:48,640
the feature selection problem sort of interestingly related to classification

848
00:55:52,500 --> 00:55:54,220
but still this picture first

849
00:55:59,410 --> 00:56:05,010
in the machine learning community people have talked about two different

850
00:56:05,060 --> 00:56:08,300
approaches are two different methodologies

851
00:56:08,320 --> 00:56:14,030
generative models and discriminative models so you'll hear those terms being used a lot

852
00:56:14,670 --> 00:56:19,350
imagine that you are trying to do classification with some input x

853
00:56:19,400 --> 00:56:23,580
some classes y so why could be possible minus one

854
00:56:24,560 --> 00:56:26,670
generative model

855
00:56:26,700 --> 00:56:29,320
is a model for the

856
00:56:29,740 --> 00:56:35,780
probability it's basically a model for all of the variables in your data sets so

857
00:56:35,800 --> 00:56:37,120
it involves

858
00:56:37,130 --> 00:56:41,980
a model with the prior probabilities of the two classes like for example you might

859
00:56:41,980 --> 00:56:46,120
believe that one class is much more frequent than the other class of prairie right

860
00:56:46,530 --> 00:56:49,950
p of why is the probability of the probabilities of these two classes which you

861
00:56:49,950 --> 00:56:51,710
can learn from data

862
00:56:51,720 --> 00:56:55,470
and then the other component is the class conditional density

863
00:56:55,520 --> 00:56:57,750
so the probability of of

864
00:56:59,850 --> 00:57:02,710
inputs x given the class label

865
00:57:02,770 --> 00:57:08,260
OK so it's a classic example of that imagine doing classification i think

866
00:57:08,470 --> 00:57:13,480
each of my classes is modeled by calcium OK

867
00:57:13,520 --> 00:57:17,340
so class plus one is modelled by gauss in class minus one is modelled by

868
00:57:17,340 --> 00:57:18,910
gaussians OK

869
00:57:18,920 --> 00:57:23,620
then in a generative model what i would do is i would model the data

870
00:57:23,620 --> 00:57:27,540
in class one with the calcium all the data in class minus one with the

871
00:57:27,540 --> 00:57:32,610
gaussians and then i would apply bayes rule to try to compute the optimal decision

872
00:57:32,610 --> 00:57:34,980
boundary which is

873
00:57:34,990 --> 00:57:40,410
given by the point at which probability of y equals plus one given x is

874
00:57:40,490 --> 00:57:45,640
greater than point five say that's or equal to point five percent of calls him

875
00:57:45,750 --> 00:57:49,450
OK that method is really bad right

876
00:57:49,460 --> 00:57:52,460
so why is it really that this is one of the places where

877
00:57:52,510 --> 00:57:55,800
you know being a little bayesian can be very dangerous

878
00:57:55,810 --> 00:57:58,300
the reason is really bad is that

879
00:57:59,430 --> 00:58:03,140
you know the probability of data given class one

880
00:58:03,150 --> 00:58:04,690
might be

881
00:58:04,730 --> 00:58:08,570
you know completely nongaussian there might be some complicated shape

882
00:58:08,610 --> 00:58:13,730
and by making the strong assumptions about the class conditional density

883
00:58:14,020 --> 00:58:15,730
you can get really screwed

884
00:58:15,780 --> 00:58:21,080
so naive bayes models for example i hate that term naive bayes because

885
00:58:21,090 --> 00:58:24,370
it's first of all the not even trained in bayesian manner

886
00:58:24,380 --> 00:58:26,010
and second of all

887
00:58:26,710 --> 00:58:28,740
you know

888
00:58:28,750 --> 00:58:32,970
there are there really i mean if because naive models

889
00:58:33,160 --> 00:58:37,560
so naive bayes models essentially what they do is they say

890
00:58:37,610 --> 00:58:39,780
the class conditional density

891
00:58:39,790 --> 00:58:42,700
here the probability of x given y

892
00:58:42,750 --> 00:58:44,790
it involves

893
00:58:44,840 --> 00:58:49,980
it assumes that all of your your input variables are independent given y

894
00:58:50,060 --> 00:58:53,620
OK that's what the naive bayes model and that's a really terrible model you get

895
00:58:53,620 --> 00:58:57,170
really terrible results most of the time unless you're

896
00:58:57,190 --> 00:59:00,030
input features really are independent given the class

897
00:59:01,310 --> 00:59:07,030
generative models basically half the model of the data both the axis and the y

898
00:59:09,930 --> 00:59:12,770
discriminative models

899
00:59:12,990 --> 00:59:18,680
try to directly models the conditional distribution of y given x

900
00:59:18,730 --> 00:59:22,920
in other words they don't have to necessarily model the input distribution the distribution of

901
00:59:22,920 --> 00:59:24,340
the axes

902
00:59:24,350 --> 00:59:28,750
OK so for example you're you're directly trying to model the

903
00:59:28,800 --> 00:59:30,600
class boundary

904
00:59:30,650 --> 00:59:35,160
you know or you could be directly modeling the last boundary the set of points

905
00:59:35,160 --> 00:59:36,270
for which

906
00:59:38,270 --> 00:59:41,200
if you believe that your classification

907
00:59:41,790 --> 00:59:43,890
in two one classes

908
00:59:43,900 --> 00:59:45,980
the same as a classification to the other

909
00:59:46,080 --> 00:59:48,870
this is most easily

910
00:59:48,880 --> 00:59:52,310
the seeing if we just look at one of these pictures OK now i know

911
00:59:52,310 --> 00:59:56,700
that it's very hard to see see thing grey lines here

912
00:59:56,710 --> 00:59:57,920
OK good

913
00:59:59,490 --> 01:00:02,970
here's here's the bayesian approach to linear classification

914
01:00:04,490 --> 01:00:06,480
before observing data

915
01:00:06,530 --> 01:00:09,290
what i'm going to do is i'm going to believe that

916
01:00:09,340 --> 01:00:11,360
the class boundary could be

917
01:00:11,370 --> 01:00:15,270
any line in the plane so could be this line it could be this line

918
01:00:15,450 --> 01:00:17,770
to be this line et cetera

919
01:00:20,200 --> 01:00:22,740
after i observe the data

920
01:00:22,750 --> 01:00:26,150
so first OK before observing the data i'm going to believe that it could be

921
01:00:26,150 --> 01:00:28,300
any lines and i'm going to sign

922
01:00:28,330 --> 01:00:31,870
a uniform probability to every possible direction of the line

923
01:00:32,500 --> 01:00:37,150
so the same probability for any possible direction line now after i observe the data

924
01:00:37,150 --> 01:00:38,830
and you can

925
01:00:38,890 --> 01:00:40,060
exploit it

926
01:00:40,070 --> 01:00:43,860
without having to do all the things that the system do is not having to

927
01:00:43,860 --> 01:00:47,120
find collect put together installed

928
01:00:47,150 --> 01:00:52,090
process manipulates all the semantic information on the web

929
01:00:52,360 --> 01:00:54,820
so most probably the most famous

930
01:00:54,830 --> 01:00:58,060
of the system is integer developing very gallery

931
01:00:58,170 --> 01:01:04,680
originally the simple very simple entity lookup service this thing you give it to see

932
01:01:04,680 --> 01:01:06,280
where i have entity

933
01:01:06,320 --> 01:01:11,310
it tells you in which RDF documents on the web this entity is mentioned always

934
01:01:11,310 --> 01:01:12,960
used always described

935
01:01:13,000 --> 01:01:15,470
you can do the same with keywords

936
01:01:15,530 --> 01:01:22,410
again simply use the keyword you semantic document contains entity about a given keywords and

937
01:01:22,410 --> 01:01:28,860
now you can in addition to model structured queries i simple example is

938
01:01:28,880 --> 01:01:32,360
if you use this one it's a bit more complicated because it's use what is

939
01:01:32,360 --> 01:01:33,990
called content negotiation

940
01:01:34,030 --> 01:01:35,740
meaning that you have to say

941
01:01:35,750 --> 01:01:38,230
in the end of the request like

942
01:01:38,240 --> 01:01:42,700
type of thing you expect so here they i say i want to RDF

943
01:01:44,840 --> 01:01:46,600
and i want

944
01:01:46,670 --> 01:01:50,950
always the RDF documents on the web that talks about much that can this talk

945
01:01:50,950 --> 01:01:52,220
is not about me

946
01:01:52,240 --> 01:01:54,890
r and then it give me

947
01:01:54,930 --> 01:01:56,520
in RDF representation

948
01:01:56,550 --> 01:02:01,280
saying that you are a number of results result one

949
01:02:01,300 --> 01:02:02,080
it is

950
01:02:02,490 --> 01:02:04,310
semantic entity

951
01:02:04,330 --> 01:02:08,410
corresponding to match that in is the web page

952
01:02:08,410 --> 01:02:13,660
describing in the semantic web that hogg semantic wiki this is very simple but when

953
01:02:13,660 --> 01:02:17,390
you know that they RDF data in this place

954
01:02:17,940 --> 01:02:23,890
but you can use and you can and that actually concern and is relevant to

955
01:02:24,950 --> 01:02:26,530
this particular entity

956
01:02:27,080 --> 01:02:28,010
which is me

957
01:02:28,840 --> 01:02:30,200
you can do

958
01:02:30,200 --> 01:02:34,540
a bit more structured query you can have for example for documents

959
01:02:34,560 --> 01:02:38,390
containing entities that for full name that like

960
01:02:38,400 --> 01:02:42,500
so in that case you will find myself to find basically all you can ask

961
01:02:42,500 --> 01:02:44,170
for much like

962
01:02:44,190 --> 01:02:45,620
as an entity

963
01:02:45,640 --> 01:02:50,930
being of the class person so used to evade the city and organizational anything that

964
01:02:50,930 --> 01:02:55,490
is has the same name you won't get you want you get person and in

965
01:02:55,490 --> 01:02:58,870
any case it returned it gives you the precise location

966
01:02:58,960 --> 01:03:04,870
of the semantic data for document containing all the that describe what you asked for

967
01:03:06,300 --> 01:03:09,220
there is one very simple but very useful application

968
01:03:09,220 --> 01:03:14,050
of that which is the multiple again for the drupal

969
01:03:14,070 --> 01:03:17,530
CMS is the idea of modes

970
01:03:17,550 --> 01:03:22,460
it is me it means meaning of the term is that it can link tag

971
01:03:22,460 --> 01:03:25,510
used to annotate any resource

972
01:03:26,080 --> 01:03:31,080
four pictures of articles or anything to your eyes

973
01:03:31,100 --> 01:03:37,780
two implying that if you look to your right you specify what is the meaning

974
01:03:38,180 --> 01:03:42,000
disambiguated you say my talk about paris

975
01:03:42,080 --> 01:03:43,330
you say

976
01:03:43,420 --> 01:03:48,230
you can link back paris two very similar name meaning that paris would become

977
01:03:48,230 --> 01:03:53,600
the city and that are essential to know anything like this very very simple

978
01:03:53,620 --> 01:03:55,030
plugin for drupal

979
01:03:55,060 --> 01:03:57,380
when which you can use mode two

980
01:03:57,420 --> 01:04:00,440
whatever you do it tagging it we got to see in each

981
01:04:00,450 --> 01:04:05,990
and try new documents that content concerning tag then you can choose a i actually

982
01:04:05,990 --> 01:04:11,780
link your taxes arrive and then you have semantic applications for your content

983
01:04:12,620 --> 01:04:13,480
this is

984
01:04:13,490 --> 01:04:15,340
or pretty nice one thing

985
01:04:15,500 --> 01:04:16,270
that is

986
01:04:16,310 --> 01:04:21,970
and i think we need to put it on the new zealand national semantic document

987
01:04:23,040 --> 01:04:25,920
what you want to do with it if you want to exploit it if you

988
01:04:25,920 --> 01:04:30,970
want to create a few simply to explore it you have to do it yourself

989
01:04:31,040 --> 01:04:33,140
so watson

990
01:04:33,150 --> 01:04:38,100
it is based on similar ideas because the first we call it the semantic web

991
01:04:38,300 --> 01:04:42,490
gateway for the semantic web as it includes all

992
01:04:42,510 --> 01:04:46,270
the future you need to find and exploit

993
01:04:46,270 --> 01:04:52,890
unless i'm getting old

994
01:04:52,900 --> 01:04:58,080
it better be the same inconsequential like

995
01:04:58,140 --> 01:05:01,800
OK axiom system it's a

996
01:05:03,920 --> 01:05:06,280
and the definition of

997
01:05:09,400 --> 01:05:11,960
this is what we want to prove

998
01:05:14,710 --> 01:05:17,830
to show this relation

999
01:05:18,780 --> 01:05:21,700
it's the same thing as this

1000
01:05:23,910 --> 01:05:31,490
on the first of the formula

1001
01:05:32,660 --> 01:05:40,350
first i'm going to simplify the job of it

1002
01:05:48,100 --> 01:05:49,380
before the first

1003
01:05:49,390 --> 01:05:52,710
zero flee

1004
01:05:52,720 --> 01:05:54,770
i'm going to

1005
01:05:54,830 --> 01:05:57,270
observe that this is a two way

1006
01:05:57,280 --> 01:06:00,630
proof so by conditional

1007
01:06:00,690 --> 01:06:05,390
and it's always a lot easier than the

1008
01:06:06,710 --> 01:06:11,000
first of all if

1009
01:06:11,140 --> 01:06:13,910
there is a derivation

1010
01:06:14,100 --> 01:06:18,860
of a from gamma then

1011
01:06:21,160 --> 01:06:23,130
in the example

1012
01:06:23,130 --> 01:06:25,170
and by gamma

1013
01:06:25,220 --> 01:06:26,220
that's cool

1014
01:06:29,570 --> 01:06:31,380
of the soundness of this

1015
01:06:31,750 --> 01:06:32,720
and it is

1016
01:06:32,770 --> 01:06:34,110
this deduction

1017
01:06:34,160 --> 01:06:37,380
deducibility relation

1018
01:06:37,390 --> 01:06:42,410
there's everything it says is correct

1019
01:06:42,460 --> 01:06:45,020
the converse direction

1020
01:06:45,190 --> 01:06:54,410
is completeness

1021
01:07:01,290 --> 01:07:02,290
that is to

1022
01:07:02,470 --> 01:07:06,450
is it but it doesn't leave anything out

1023
01:07:06,500 --> 01:07:08,700
if anything really is

1024
01:07:08,720 --> 01:07:11,870
entailed by giving them

1025
01:07:11,880 --> 01:07:17,610
soundness is much easier to fix than can

1026
01:07:17,660 --> 01:07:23,690
so i'm going to knock off sound at first

1027
01:07:23,700 --> 01:07:27,190
concentrate on the real thing

1028
01:07:27,220 --> 01:07:29,310
how do we prove

1029
01:07:29,350 --> 01:07:30,570
but this

1030
01:07:30,600 --> 01:07:32,410
deducibility relation

1031
01:07:32,450 --> 01:07:36,100
is sound relative to the semantics

1032
01:07:36,200 --> 01:07:37,950
we're all

1033
01:07:38,040 --> 01:07:43,250
of course we do it by induction on the length of the derivation here

1034
01:07:43,280 --> 01:07:46,280
the shot repetition we always say

1035
01:07:46,580 --> 01:07:49,150
the shortest derivation of a from gamma

1036
01:07:49,290 --> 01:07:54,350
and that's all it indicates is that a case where we have the derivation of

1037
01:07:54,350 --> 01:07:57,100
length one

1038
01:07:57,220 --> 01:07:59,540
that the that's the proof

1039
01:07:59,560 --> 01:08:03,720
and then there's the case where it comes

1040
01:08:03,760 --> 01:08:06,500
by the mid from somebody

1041
01:08:07,810 --> 01:08:10,440
if b then c

1042
01:08:10,530 --> 01:08:13,720
and i mean function hypothesis we know

1043
01:08:15,850 --> 01:08:19,680
the is indeed help get

1044
01:08:21,310 --> 01:08:22,880
so is

1045
01:08:22,880 --> 01:08:24,450
if b and c

1046
01:08:24,850 --> 01:08:28,090
but then

1047
01:08:28,130 --> 01:08:30,600
it's obvious

1048
01:08:30,650 --> 01:08:32,470
the c is also

1049
01:08:32,530 --> 01:08:36,000
let's look at the truth table for b and b then c which i didn't

1050
01:08:36,000 --> 01:08:39,070
actually spell out because you know it so well

1051
01:08:41,090 --> 01:08:42,950
i mean

1052
01:08:42,970 --> 01:08:43,850
take any

1053
01:08:43,940 --> 01:08:45,820
interpretation the

1054
01:08:45,850 --> 01:08:49,910
and valuation satisfies everything in gamma

1055
01:08:49,930 --> 01:08:52,350
it satisfies this

1056
01:08:52,410 --> 01:08:55,620
it just buys this

1057
01:08:55,630 --> 01:08:57,980
OK so the definition of

1058
01:09:00,370 --> 01:09:04,670
is such that is obviously going to satisfy c

1059
01:09:05,590 --> 01:09:10,060
implication satisfied if and only if

1060
01:09:10,110 --> 01:09:15,200
c is satisfied albeit not that is that is to in

1061
01:09:15,230 --> 01:09:19,340
so the induction step for once is going to be trivial all the work goes

1062
01:09:19,430 --> 01:09:23,560
i got proof of length one

1063
01:09:24,810 --> 01:09:27,900
how could be very well

1064
01:09:27,960 --> 01:09:30,630
could be a member of gamma

1065
01:09:30,630 --> 01:09:34,900
now what if i don't know what selection bias is present selection bias may be

1066
01:09:36,020 --> 01:09:37,500
and they have

1067
01:09:37,670 --> 01:09:39,960
example in people with hypertension

1068
01:09:40,030 --> 01:09:43,280
well work and this is kind of tricky to understand

1069
01:09:43,320 --> 01:09:46,300
and it's tricky for me to think that if

1070
01:09:46,360 --> 01:09:48,880
continues trying to say

1071
01:09:48,880 --> 01:09:55,920
the causal DAG could not be the one in a way i could have this

1072
01:09:55,920 --> 01:09:57,570
causal that

1073
01:09:57,610 --> 01:09:58,800
because we

1074
01:09:58,820 --> 01:10:01,530
but what i mean by the square root note by the way

1075
01:10:01,550 --> 01:10:03,000
it means that this is

1076
01:10:03,030 --> 01:10:05,280
you know that that's instantiated

1077
01:10:05,340 --> 01:10:08,360
it's not an observer noted for selection node

1078
01:10:08,420 --> 01:10:10,710
it means for all

1079
01:10:10,730 --> 01:10:12,820
and it is of the population

1080
01:10:12,860 --> 01:10:16,820
this node is instantiated the same value

1081
01:10:17,050 --> 01:10:19,110
right so

1082
01:10:19,110 --> 01:10:20,360
i'm saying

1083
01:10:20,360 --> 01:10:22,320
that causes us

1084
01:10:22,340 --> 01:10:23,900
the causes

1085
01:10:23,920 --> 01:10:28,090
and everybody in the population has the same value of s

1086
01:10:28,090 --> 01:10:30,800
could this be what i'm observing

1087
01:10:30,840 --> 01:10:32,750
but i think it could be

1088
01:10:34,920 --> 01:10:38,150
if everybody was instantiated

1089
01:10:40,190 --> 01:10:42,590
remember the discounting

1090
01:10:43,190 --> 01:10:48,340
this node is instantiated these two are rendered deep under

1091
01:10:48,340 --> 01:10:51,110
which means that actually easy would be dependent

1092
01:10:51,130 --> 01:10:54,530
all right that's the for sale goes that way now

1093
01:10:54,630 --> 01:10:57,210
i mean x and y are independent

1094
01:10:57,210 --> 01:11:03,320
it's actually dependent on cx will also be dependent on these parents

1095
01:11:04,030 --> 01:11:06,730
selection bias could not be here

1096
01:11:06,760 --> 01:11:09,460
could be like this

1097
01:11:09,500 --> 01:11:15,750
because we wouldn't have this observed dependency again because that's instantiations so these guys independent

1098
01:11:15,800 --> 01:11:17,780
and because it's sale goes this way

1099
01:11:17,780 --> 01:11:21,440
that's would also be dependent on y

1100
01:11:21,500 --> 01:11:24,690
the causal DAG could be the one in b

1101
01:11:24,730 --> 01:11:28,030
could selection bias up you're going like that

1102
01:11:28,090 --> 01:11:32,170
because now i don't have this problem i instantiate the

1103
01:11:32,190 --> 01:11:33,250
for everybody

1104
01:11:33,260 --> 01:11:36,300
which makes axon http and

1105
01:11:36,420 --> 01:11:40,440
since these arrows both go this way and the head-to-head meeting

1106
01:11:40,500 --> 01:11:43,420
x h and y are independent

1107
01:11:43,440 --> 01:11:47,110
so by the d separation argument that means that some three

1108
01:11:49,690 --> 01:11:52,610
so this could be what's going

1109
01:11:52,670 --> 01:11:57,630
i could get get is pretty complicated like so what's going on that observing x

1110
01:11:57,630 --> 01:11:58,880
y z and w

1111
01:11:59,050 --> 01:12:02,650
and in the background the a hidden costs of as the

1112
01:12:03,650 --> 01:12:07,780
the selection bias going and with his very blocks but they could be what's going

1113
01:12:07,780 --> 01:12:12,590
on right it's it's complicated but it could be was going out

1114
01:12:12,670 --> 01:12:14,110
so i can

1115
01:12:14,110 --> 01:12:20,750
i can have this

1116
01:12:20,800 --> 01:12:23,750
now could selection bias here

1117
01:12:23,800 --> 01:12:27,570
this is the edge that i learned before the has because

1118
01:12:27,590 --> 01:12:29,980
you could this be selection by

1119
01:12:30,190 --> 01:12:34,590
no i can't because you instantiate this note

1120
01:12:34,650 --> 01:12:39,570
that's actually the same is again and and basically and largely appealing to intuition here

1121
01:12:39,570 --> 01:12:43,550
because it proved formally is much more work but

1122
01:12:43,630 --> 01:12:49,070
by instantiate this node that not much different than instantiating this one

1123
01:12:49,090 --> 01:12:55,900
member by noses value ranges actually y dependent because of discounting

1124
01:12:55,930 --> 01:12:59,690
i do the same thing these child in the same way because because it's child

1125
01:12:59,690 --> 01:13:01,050
depends on

1126
01:13:01,090 --> 01:13:01,750
and c

1127
01:13:01,760 --> 01:13:02,730
so by now

1128
01:13:02,780 --> 01:13:04,090
this value

1129
01:13:04,110 --> 01:13:06,730
i still discounting going

1130
01:13:06,780 --> 01:13:10,420
it's not much different than instantiating the itself

1131
01:13:10,440 --> 01:13:11,800
so i can tell

1132
01:13:11,840 --> 01:13:18,460
function bias like this going and

1133
01:13:18,530 --> 01:13:21,480
i couldn't tell the one to be even

1134
01:13:21,530 --> 01:13:24,280
and that's because of different problem

1135
01:13:25,650 --> 01:13:29,150
this node is instantiated

1136
01:13:29,170 --> 01:13:30,590
this way

1137
01:13:30,610 --> 01:13:34,300
h and w are independent

1138
01:13:35,860 --> 01:13:38,550
because these arrows both go and

1139
01:13:38,550 --> 01:13:39,960
h axis

1140
01:13:43,170 --> 01:13:49,570
so the independent would between w one x w one x being independent

1141
01:13:49,610 --> 01:13:52,690
not independent conditional on c which is

1142
01:13:52,860 --> 01:13:55,250
i was

1143
01:13:55,300 --> 01:13:58,730
observed that would have observed that and i'm not observing this

1144
01:13:58,750 --> 01:14:01,150
like the bottom line is you can never

1145
01:14:01,170 --> 01:14:03,110
turn this heroine

1146
01:14:03,150 --> 01:14:07,430
so i can tell us and i can't find this arrow going and so i

1147
01:14:07,430 --> 01:14:09,710
cannot either situation

1148
01:14:09,710 --> 01:14:12,630
i can't the selection variable without hindrance

1149
01:14:12,630 --> 01:14:15,940
because i can't ever with the hidden cause

1150
01:14:15,960 --> 01:14:17,750
two different reasons

1151
01:14:17,800 --> 01:14:22,530
one would be a violation of x and y be independent the other being

1152
01:14:22,570 --> 01:14:26,400
it would make w independent of x

1153
01:14:26,460 --> 01:14:28,340
so back i can

1154
01:14:28,360 --> 01:14:35,130
i can of this i can't that

1155
01:14:35,150 --> 01:14:39,170
i told you i can't decide really care about that

1156
01:14:39,210 --> 01:14:41,360
i could have done

1157
01:14:41,400 --> 01:14:45,210
so say anything about selection bias that i could have

1158
01:14:45,280 --> 01:14:50,050
has he pointing the w

1159
01:14:50,110 --> 01:14:53,110
this is the only one of these four possibilities i can have

1160
01:14:53,130 --> 01:14:55,020
so the bottom line here

1161
01:14:55,030 --> 01:14:55,980
is that

1162
01:14:55,980 --> 01:14:59,190
once to make the causal embedded faithfulness assumption

1163
01:14:59,190 --> 01:15:02,230
the thing is that when have a parametric model

1164
01:15:02,250 --> 01:15:04,400
when the parameter estimation d

1165
01:15:04,410 --> 01:15:05,940
then there is the way

1166
01:15:05,960 --> 01:15:11,330
to make sure that mentions these are the initial parameters using something called something called

1167
01:15:11,330 --> 01:15:13,380
the future is called victoria

1168
01:15:13,390 --> 01:15:16,150
this is the gradient of the log that you have

1169
01:15:16,160 --> 01:15:17,770
one of the of

1170
01:15:17,780 --> 01:15:19,140
it is he

1171
01:15:19,150 --> 01:15:21,670
these heuristic sequence

1172
01:15:21,680 --> 01:15:26,380
and is that so you have your from model we assume that you are estimated

1173
01:15:26,380 --> 01:15:27,250
one parameter

1174
01:15:29,460 --> 01:15:31,910
a zero here

1175
01:15:32,130 --> 01:15:38,300
that is optimized for you that question is suppose you get a sequence cannot subsequently

1176
01:15:38,300 --> 01:15:39,200
to victoria

1177
01:15:39,210 --> 01:15:42,230
what do we need to compute the gradient down

1178
01:15:42,240 --> 01:15:47,830
on the second of sequence with the model one of the famous made

1179
01:15:47,850 --> 01:15:52,360
so here is a great time so by definition of victoria

1180
01:15:52,400 --> 01:15:55,900
o mentioned the mission of that which is the which is the same for all

1181
01:15:58,280 --> 01:16:02,580
and what is it or you can make you can already so there is a

1182
01:16:02,580 --> 01:16:06,660
small tree here which are to the typically you can see that three-dimensional data this

1183
01:16:06,660 --> 01:16:10,210
is away from some street to victoria and then you can use the victoria as

1184
01:16:10,210 --> 01:16:12,400
input to SVM

1185
01:16:12,510 --> 01:16:17,170
so what do we do that well the justification for this problem would involve

1186
01:16:17,180 --> 01:16:22,920
detail that we should not i don't want to enter into now because

1187
01:16:23,010 --> 01:16:26,770
but in fact what you you see is what you get when you take the

1188
01:16:26,770 --> 01:16:34,320
gradient of what we try to use some that the parameters which is how much

1189
01:16:34,320 --> 01:16:37,330
each parameter inference is that the your

1190
01:16:37,360 --> 01:16:38,630
o points

1191
01:16:38,640 --> 01:16:40,450
OK i just sequence

1192
01:16:40,480 --> 01:16:42,150
this is given permission

1193
01:16:42,170 --> 01:16:44,660
try to use your model

1194
01:16:44,710 --> 01:16:52,820
two strikes the inference of it for you use all of your of different interests

1195
01:16:53,250 --> 01:16:54,530
and in fact the

1196
01:16:54,850 --> 01:16:56,520
is that you don't

1197
01:16:56,530 --> 01:17:03,320
in fact for the participation of these two interesting justification that that company should all

1198
01:17:03,360 --> 01:17:08,800
the first one is that in instead of taking the in the UK take strange

1199
01:17:08,800 --> 01:17:09,860
thing here

1200
01:17:09,870 --> 01:17:14,650
which is the normalized so instead of taking the inner product between the features going

1201
01:17:14,650 --> 01:17:17,250
to assume that the metric induced by the

1202
01:17:17,290 --> 01:17:22,650
fisher information matrix is twenty days but if you do that then you something that

1203
01:17:22,690 --> 01:17:24,760
that becomes independent

1204
01:17:24,770 --> 01:17:27,360
i found this argument

1205
01:17:27,370 --> 01:17:28,770
so what is that

1206
01:17:28,780 --> 01:17:30,990
here you've model

1207
01:17:31,010 --> 01:17:31,650
that's fine

1208
01:17:31,660 --> 01:17:33,320
set of distributions

1209
01:17:33,330 --> 01:17:34,640
it depends on the top

1210
01:17:34,650 --> 01:17:35,360
but you can use

1211
01:17:35,370 --> 01:17:39,930
that's right the same set of distributions with different parameter

1212
01:17:39,940 --> 01:17:44,360
noted that try to be friends well it turns out that if you apply like

1213
01:17:44,410 --> 01:17:48,440
you can here doesn't depend on the way you are which was any your

1214
01:17:48,460 --> 01:17:54,990
OK so it's because of the fact that the that says anything using the current

1215
01:17:56,410 --> 01:18:00,530
and more importantly always it's a bit tricky to prove it has been shown so

1216
01:18:00,550 --> 01:18:03,270
when the canal i was presented that

1217
01:18:03,280 --> 01:18:07,410
if you assume that your your fish can alter the model

1218
01:18:08,650 --> 01:18:11,550
the bible class

1219
01:18:11,610 --> 01:18:15,300
then you should be using SVM with gaussian kernel

1220
01:18:15,330 --> 01:18:19,920
should be at least as good as using something called the MAP estimate so i

1221
01:18:19,920 --> 01:18:23,820
will not until to the set of this but roughly speaking i'm not sure is

1222
01:18:24,530 --> 01:18:26,800
doesn't hypothesis which are usually not

1223
01:18:26,810 --> 01:18:32,660
satisfied particular occasions but is the beginning of the justification of why should use that

1224
01:18:32,670 --> 01:18:35,520
OK so this is the view that if use it in a few hours

1225
01:18:35,640 --> 01:18:38,670
i was that told to because then

1226
01:18:38,690 --> 01:18:42,030
the shock and it's good because we do as well as what we do

1227
01:18:42,040 --> 01:18:48,030
if you use the generative model the navy navy mean by making the classes in

1228
01:18:48,040 --> 01:18:50,750
the necessary probability of the class

1229
01:18:50,770 --> 01:18:55,270
OK i will not say more about that in practice it has been used because

1230
01:18:55,270 --> 01:18:57,630
in practice

1231
01:18:57,910 --> 01:19:03,160
the question is you can see that it turns out that most promising models using

1232
01:19:03,320 --> 01:19:07,500
but it is a graphical models for many others like it and you can easily

1233
01:19:07,500 --> 01:19:09,400
compute the score vector

1234
01:19:09,440 --> 01:19:11,190
i just called victoria

1235
01:19:11,210 --> 01:19:12,540
and so on

1236
01:19:12,600 --> 01:19:16,250
so it's and its some more efficient and i mean

1237
01:19:16,340 --> 01:19:21,030
editions the complete this you're doing estimation graphical models

1238
01:19:21,040 --> 01:19:26,060
but still it was historically the first one is that you use the graphical processing

1239
01:19:26,810 --> 01:19:32,450
you know the represent the stream by definition of data

1240
01:19:32,900 --> 01:19:35,270
now i will present the second

1241
01:19:35,290 --> 01:19:36,780
general class

1242
01:19:36,800 --> 01:19:38,570
of camels

1243
01:19:38,570 --> 01:19:46,030
l discussions on the use of basically graphical models for structural pattern recognition

1244
01:19:48,540 --> 01:19:52,500
since it's the last sort of lectures i don't want to be too

1245
01:19:52,520 --> 01:19:53,680
two technical

1246
01:19:53,700 --> 01:19:57,400
i want to give some motivation more and give you some examples of promised yesterday

1247
01:19:57,400 --> 01:19:59,050
i how use this stuff in

1248
01:19:59,060 --> 01:20:03,020
solving some quite difficult image understanding problems

1249
01:20:04,070 --> 01:20:09,410
again all do the same as it did yesterday today my mind go straight through

1250
01:20:09,650 --> 01:20:11,840
but not to host

1251
01:20:15,420 --> 01:20:18,370
so i want to do is remind you where we were yesterday

1252
01:20:19,920 --> 01:20:21,400
where we

1253
01:20:21,410 --> 01:20:26,620
ah yes here

1254
01:20:31,450 --> 01:20:33,120
so yesterday

1255
01:20:34,510 --> 01:20:37,450
i i been discussion on

1256
01:20:37,490 --> 01:20:42,440
what you might call we call the graph matching problem processing in structural pattern recognition

1257
01:20:42,440 --> 01:20:44,910
is the pattern recognition problem in vision

1258
01:20:44,980 --> 01:20:47,840
and that is the problem of matching features

1259
01:20:47,850 --> 01:20:53,160
a small set of features often to a very very large typically features causing

1260
01:20:53,200 --> 01:21:00,400
and we discussed various of modern approaches to that in terms of spectral methods misplaced

1261
01:21:00,400 --> 01:21:04,630
methods methods in bayesian bayesian network methods

1262
01:21:04,650 --> 01:21:06,310
and in talk yesterday

1263
01:21:06,850 --> 01:21:11,040
the game emphasizes the continuum between exact

1264
01:21:11,090 --> 01:21:17,240
the exact models and exact inference through to inexact models and exact inference and being

1265
01:21:17,240 --> 01:21:18,900
able to choose the right

1266
01:21:18,920 --> 01:21:24,840
the inside to choose the right position on that continuum to really get things all

1267
01:21:24,860 --> 01:21:30,240
functioning in useful particularly in our area with lots of data

1268
01:21:30,250 --> 01:21:32,240
and yesterday we focus on

1269
01:21:32,250 --> 01:21:38,840
three to illustrate the things on points at MAX matching and really situations also with

1270
01:21:39,430 --> 01:21:42,020
that will really not attributed

1271
01:21:42,030 --> 01:21:46,340
that is not just point so to speak of an additional unlabeled graphs

1272
01:21:46,360 --> 01:21:51,030
with adjacency matrices matrices and the like

1273
01:21:51,040 --> 01:21:55,540
well the training of the shared his vision is not like that when we

1274
01:21:55,570 --> 01:22:00,700
when we solve problems like matching ten places seems to think is we observe the

1275
01:22:00,700 --> 01:22:07,810
segmented typically from segmentation always tough to processes have attributes like orientation making of length

1276
01:22:07,810 --> 01:22:10,390
they have size colour

1277
01:22:10,410 --> 01:22:13,110
he has said yesterday and so

1278
01:22:13,130 --> 01:22:17,090
and so do the binary attributes not just distance

1279
01:22:17,140 --> 01:22:21,940
they can have very very very many different types of attributes so then the problem

1280
01:22:21,940 --> 01:22:26,280
really generalizes up to an attributed graph matching problem

1281
01:22:27,410 --> 01:22:33,020
so yesterday in the last a paradigm we we spoke about we spoke about graph

1282
01:22:33,020 --> 01:22:37,120
itself being conceived of as a markov random field

1283
01:22:37,130 --> 01:22:41,700
where each of the nodes with the random variable

1284
01:22:41,720 --> 01:22:43,990
because have the number of states

1285
01:22:44,010 --> 01:22:46,870
the discrete random variable and those states happen to be

1286
01:22:46,880 --> 01:22:50,630
the way we propose that the state of the same

1287
01:22:50,650 --> 01:22:55,830
and we pose a problem of graph matching deriving the optimal state for each node

1288
01:22:55,830 --> 01:23:00,040
which of course different by definition give you a solution to the problem or

1289
01:23:00,060 --> 01:23:02,910
template matching right

1290
01:23:02,920 --> 01:23:08,220
and we went through the junction tree algorithm has is one of the things we've

1291
01:23:08,220 --> 01:23:12,750
been looking at and compared to other other approximate like

1292
01:23:12,770 --> 01:23:16,100
the station based at

1293
01:23:16,110 --> 01:23:23,590
the markov property of the random field came from defining some sort of dependencies of

1294
01:23:23,590 --> 01:23:25,510
some vertices on his

1295
01:23:25,530 --> 01:23:29,740
and that this is typically the some sort of local local connectivity

1296
01:23:29,790 --> 01:23:33,680
it came about for example by triangulation call

1297
01:23:33,710 --> 01:23:36,060
world congress

1298
01:23:36,070 --> 01:23:39,230
and you saw how we then took those

1299
01:23:41,860 --> 01:23:43,450
those cliques

1300
01:23:43,460 --> 01:23:48,220
and then decompose the potential function is fixed pairwise which is the traditional thing to

1301
01:23:48,220 --> 01:23:52,980
do but we also noted yesterday that's not necessarily the only thing you could do

1302
01:23:53,000 --> 01:23:56,860
and all often depend on the type of invariance is the type of the order

1303
01:23:56,860 --> 01:24:01,480
statistics are important and those sorts of things

1304
01:24:02,800 --> 01:24:05,010
an extension to

1305
01:24:05,020 --> 01:24:09,470
that type of formulation for inferences

1306
01:24:09,530 --> 01:24:11,960
is really a problem too

1307
01:24:12,050 --> 01:24:15,330
attributed graph matching so what we simply do

1308
01:24:15,340 --> 01:24:17,530
is actually have

1309
01:24:17,550 --> 01:24:19,530
i hidden markov random field

1310
01:24:19,540 --> 01:24:25,060
and hidden markov random field is really a generalization of the a hidden markov model

1311
01:24:25,070 --> 01:24:27,140
so for every study

1312
01:24:27,210 --> 01:24:29,950
i have a series of observations or

1313
01:24:29,960 --> 01:24:32,600
i have a state dependent

1314
01:24:32,610 --> 01:24:34,880
the observation probability density function

1315
01:24:34,900 --> 01:24:39,160
like in hidden markov models i might have certain calling which has a certain probability

1316
01:24:39,160 --> 01:24:41,640
density being by

1317
01:24:41,720 --> 01:24:46,730
same thing with this problem for every every every state every time a feature for

1318
01:24:46,730 --> 01:24:52,340
example the distance different distribution of attributes for example the the

1319
01:24:52,350 --> 01:24:57,280
the iris may have a different colour distribution then for example a certain type of

1320
01:24:57,490 --> 01:24:59,960
pigmented skin

1321
01:25:00,010 --> 01:25:03,030
but the problem is still the same

1322
01:25:03,550 --> 01:25:07,540
i still want to infer the hidden states

1323
01:25:07,560 --> 01:25:10,030
now the hidden states using the

1324
01:25:10,060 --> 01:25:13,810
terminology of hidden markov models from two things

1325
01:25:13,830 --> 01:25:15,710
an observation model

1326
01:25:15,730 --> 01:25:19,600
and the markov random field model

1327
01:25:19,620 --> 01:25:24,490
just like in hidden markov models have memory model in the hidden markov model like

1328
01:25:24,490 --> 01:25:26,300
which is the markov chain

1329
01:25:26,320 --> 01:25:29,670
i have an observation model which are bring together

1330
01:25:29,710 --> 01:25:30,960
by that

1331
01:25:32,390 --> 01:25:38,070
in the dynamic programming algorithm all the that were shouting for estimation

1332
01:25:39,120 --> 01:25:40,430
the thing is now

1333
01:25:40,950 --> 01:25:42,810
hidden markov random fields

1334
01:25:42,820 --> 01:25:46,490
unless i create an inexact model

1335
01:25:46,500 --> 01:25:51,400
i'm not going to be able do exact inference because this is terribly difficult problem

1336
01:25:51,570 --> 01:25:52,900
these huge

1337
01:25:52,920 --> 01:25:56,670
so for hidden markov random fields

1338
01:25:56,690 --> 01:25:57,820
you have the same

1339
01:25:57,830 --> 01:26:03,530
you have the same you have the problem the problem of organ intractable model

1340
01:26:04,250 --> 01:26:05,670
in our case

1341
01:26:05,690 --> 01:26:08,170
the hidden markov random field i'm going to talk about

1342
01:26:08,170 --> 01:26:12,380
and similarly the trial will have patients that will be most likely to inform them so

1343
01:26:12,380 --> 01:26:15,390
this is actually quite revolutionary.

1344
01:26:15,440 --> 01:26:19,900
and then one step up from this the entire melanoma community is

1345
01:26:19,920 --> 01:26:23,590
is simply one node and cancer commons.

1346
01:26:23,640 --> 01:26:28,070
and so we have similar communities that we're planning to build for other cancers and

1347
01:26:28,070 --> 01:26:29,820
these will be connected

1348
01:26:29,860 --> 01:26:38,530
and the interesting thing is that there are many, many opportunities already emerging

1349
01:26:38,570 --> 01:26:43,630
to be able to demonstrate the the cross learnings between these communities. so for example there's

1350
01:26:43,690 --> 01:26:45,170
a drug called gleevec,

1351
01:26:45,190 --> 01:26:50,680
which was a wonder drug for a certain type of leukaemia, chronic myelogenous leukemia, it

1352
01:26:50,680 --> 01:26:55,730
basically made that disease something one could live with for a very long time.

1353
01:26:55,750 --> 01:27:02,240
it turned out that three percent of melanomas expressed a mutation that that drug happened to hit.

1354
01:27:02,240 --> 01:27:03,510
who would have guessed?

1355
01:27:03,520 --> 01:27:08,380
similarly the drug that I talked about the BRAF inhibitor in that I was talking about

1356
01:27:08,380 --> 01:27:13,310
earlier for melanoma may be applicable to the seven percent of

1357
01:27:13,480 --> 01:27:16,720
colorectal cancers that express BRAF mutations.

1358
01:27:16,760 --> 01:27:20,760
we don't know that yet and there are many, many mutations that are shared across

1359
01:27:20,760 --> 01:27:22,180
the solid tumors

1360
01:27:22,600 --> 01:27:25,170
in the GI tract and and along.

1361
01:27:25,190 --> 01:27:30,350
so this is an opportunity to do the the kind of cross learning that requires

1362
01:27:30,350 --> 01:27:34,540
the kind of thinking and applications that we can do in this community.

1363
01:27:34,540 --> 01:27:37,080
so that's cancer commons

1364
01:27:37,110 --> 01:27:43,220
and I would now like to talk about the AI opportunities and and challenges. so reframing

1365
01:27:43,220 --> 01:27:47,350
things from an AI perspective what we're talking about is a gigantic search

1366
01:27:47,360 --> 01:27:50,530
over the opportunity space of targets and leads

1367
01:27:50,570 --> 01:27:56,410
and that search is guided by knowledge, a lot of knowledge about cancer biology, about

1368
01:27:56,410 --> 01:28:00,560
drugs, about clinical response,  tests, essays, all of those things.

1369
01:28:00,570 --> 01:28:03,320
the problem is that the knowledge is

1370
01:28:03,400 --> 01:28:07,410
largely uncertain still, there's large gaps in the knowledge

1371
01:28:07,430 --> 01:28:10,070
incomplete, uncertain,

1372
01:28:10,090 --> 01:28:12,040
much of it is no doubt wrong.

1373
01:28:12,050 --> 01:28:14,180
and so what we have to do

1374
01:28:14,250 --> 01:28:16,740
is use the experiments on patients,

1375
01:28:16,760 --> 01:28:21,530
ethical experiments remember 'cause every patient has to get the best treatment for them personally

1376
01:28:21,640 --> 01:28:26,900
but to gradually start to fill in in these gaps by doing adaptive planning

1377
01:28:26,920 --> 01:28:30,550
to be able to look at the questions that we're trying to answer and then

1378
01:28:30,550 --> 01:28:32,080
go back and try to

1379
01:28:32,130 --> 01:28:39,380
use patients in an ethical way to run these experiments, capture learnings, use the learnings to

1380
01:28:39,380 --> 01:28:42,580
infer what we can do about the causal nature

1381
01:28:42,600 --> 01:28:47,380
of cancer and the mechanisms of action of the drugs and then generalize from each patient

1382
01:28:47,390 --> 01:28:51,780
to many other patients in order to be able to drive the field forward.

1383
01:28:51,820 --> 01:28:57,180
now, of course, there's  much too big a gap between patients and the knowledge ball over

1384
01:28:57,180 --> 01:29:01,350
there, so we've gotta have intermediate levels of learning and of course we've talked

1385
01:29:01,360 --> 01:29:03,300
about one so far,

1386
01:29:03,320 --> 01:29:04,970
which is the reference model,

1387
01:29:04,990 --> 01:29:06,870
which is a very nice crisp

1388
01:29:08,440 --> 01:29:13,560
problem where the goal is to be able to continually improve predictions.

1389
01:29:13,570 --> 01:29:16,620
but there are many, many, many other levels of knowledge and these are some of

1390
01:29:16,620 --> 01:29:20,150
the levels that are involved in the collabrx  one process. for those of you

1391
01:29:20,150 --> 01:29:25,320
in the back pathway analysis mechanism of ana analysis what do these pathways have to do with

1392
01:29:25,320 --> 01:29:31,110
particular known  cancer mechanisms, using that to identify targets, drugs, picking clinical trials and so

1393
01:29:31,110 --> 01:29:35,550
forth and we've got learning to do on every one of these levels

1394
01:29:35,570 --> 01:29:36,580
every day

1395
01:29:36,600 --> 01:29:38,320
with every patient.

1396
01:29:38,330 --> 01:29:41,720
so the generalisation is basically that each level

1397
01:29:41,770 --> 01:29:43,630
generates hypotheses

1398
01:29:43,660 --> 01:29:46,080
that can be then used

1399
01:29:46,150 --> 01:29:50,520
that  can then be tested at the next level down as is traditional in the hierarchical learning

1400
01:29:50,520 --> 01:29:55,110
system and I think that the hierarchical base is very nice way to formulate this

1401
01:29:55,920 --> 01:30:00,560
because all the knowledge is probabilistic at every single level.

1402
01:30:00,570 --> 01:30:06,330
so in the in the time remaining I'm going to focus on some specific challenges

1403
01:30:06,340 --> 01:30:14,270
for and opportunities and challenges for this community in three areas knowledge management, hierarchical planning

1404
01:30:14,270 --> 01:30:16,690
and learning and generalisation.

1405
01:30:16,710 --> 01:30:21,360
first in terms of knowledge management I've already said that cancer,

1406
01:30:21,380 --> 01:30:25,400
curing cancer, treating cancer involves a lot of knowledge

1407
01:30:25,420 --> 01:30:30,880
and these are there's a very small fraction of the knowledge that's available just

1408
01:30:30,880 --> 01:30:34,160
on the internet and I wanna give you a scale an idea for

1409
01:30:34,160 --> 01:30:39,040
the scale of this knowledge, pubmed which is a major resource

1410
01:30:39,040 --> 01:30:41,440
from the national library of medicine

1411
01:30:41,470 --> 01:30:47,240
has twenty million abstracts of  medical literature in it,  about three million of those are

1412
01:30:47,240 --> 01:30:52,400
specifically about cancer but the answers don't necessarily come from that the answers may come

1413
01:30:52,400 --> 01:30:58,140
from for example literature on metabolism because cancer screws up all kinds of processes within

1414
01:30:58,140 --> 01:31:03,980
cells and if they  may provide the clue about how to do interdiction. the gene expression omnibus

1415
01:31:03,980 --> 01:31:09,110
is a public database of gene expression data there's four hundred fifty thousand

1416
01:31:09,130 --> 01:31:13,170
experiments that are documented there,

1417
01:31:13,190 --> 01:31:16,910
gene expression and sometimes the clinical annotation that goes with them

1418
01:31:16,920 --> 01:31:21,590
that was last time I looked a few days ago, this grows by hundreds

1419
01:31:21,590 --> 01:31:23,380
of experiments daily.

1420
01:31:23,910 --> 01:31:28,820
clinical trials dot gov has nearly a hundred thousand trials

1421
01:31:28,820 --> 01:31:31,940
ongoing at any moment in time

1422
01:31:31,970 --> 01:31:35,890
and about forty percent of those are cancer.

1423
01:31:35,910 --> 01:31:41,150
and asco  is where the cancer trials, american society of clinical oncology, it's a  conference much

1424
01:31:41,150 --> 01:31:43,090
like this one just larger

1425
01:31:43,100 --> 01:31:46,830
where the trials get reported each year

1426
01:31:46,870 --> 01:31:50,710
and this is truly the frontlines and I wanna give you a report from

1427
01:31:50,710 --> 01:31:54,550
the frontlines of asco, a month ago I went to asco twenty ten,

1428
01:31:54,560 --> 01:31:57,870
which is in chicago

1429
01:31:57,900 --> 01:32:01,570
there are thirty thousand people at that conference the plenary hall

1430
01:32:01,580 --> 01:32:05,920
is at least as long as this one and at least as wide, it's the

1431
01:32:05,920 --> 01:32:07,770
biggest room I've ever seen

1432
01:32:07,810 --> 01:32:11,010
it's it's not like a sports stadium where you can actually see the players in the

1433
01:32:11,010 --> 01:32:14,270
back of the room you can see a pin point of the guy up in the front

1434
01:32:14,270 --> 01:32:18,750
giving the  talk and then hundreds of screens scattered through the room so you you gotta

1435
01:32:18,780 --> 01:32:24,160
have a visceral understanding, this is a  twenty ring circus four thousand abstracts are presented

1436
01:32:24,330 --> 01:32:25,660
in four days

1437
01:32:25,680 --> 01:32:28,560
of which about six of them make the plenary.

1438
01:32:28,750 --> 01:32:32,170
so those are the those are the ones that are considered practice changing and those are the ones you

1439
01:32:32,170 --> 01:32:34,240
can read about in  the new york times.

1440
01:32:34,290 --> 01:32:38,610
but there are five hundred or a thousand others that might be life saving

1441
01:32:38,640 --> 01:32:41,850
for some patient, if there was only a way to be able to get that

1442
01:32:41,850 --> 01:32:47,290
information and put it in front of the patient and doc the right time.

1443
01:32:47,290 --> 01:32:51,280
so I'll ask my question again, do you think a little AI can help?

1444
01:32:51,300 --> 01:32:53,330
here's another issue that comes up,

1445
01:32:53,350 --> 01:32:57,070
we talked about a few slides ago I said there was this drug for

1446
01:32:57,070 --> 01:32:59,530
leukemia that helped three percent of of

1447
01:32:59,580 --> 01:33:05,650
melanoma patients. it took nearly two decades to be able to take that knowledge

1448
01:33:05,670 --> 01:33:07,110
and transfer it

1449
01:33:07,120 --> 01:33:10,630
and and be able to connect the dots to be able to figure that out

1450
01:33:10,640 --> 01:33:12,150
so the the result

1451
01:33:12,190 --> 01:33:18,490
that said gleevec has major response for melanoma patients who express a C kit

1452
01:33:18,530 --> 01:33:22,980
mutation was published I believe in nineteen ninety eight by in two thousand and eight by

1453
01:33:22,980 --> 01:33:26,540
a friend of mine Stephen Hodi.  but in nineteen ninety one

1454
01:33:26,540 --> 01:33:31,410
i'm told it's very it's very neat stuff and

1455
01:33:34,410 --> 01:33:38,380
they they have journal paper impressed with their where are applying this this are not

1456
01:33:38,380 --> 01:33:43,030
impressed in preparation where they are applying this to some some various kinds of real

1457
01:33:43,030 --> 01:33:46,510
world datasets but i don't know i don't have any results on that you or

1458
01:33:46,510 --> 01:33:52,830
i don't have any of the results but these these ideas of hierarchical models really

1459
01:33:52,830 --> 01:33:55,700
do show up lot in cognitive science and what i want this is what i

1460
01:33:55,700 --> 01:33:59,790
want to talk about progress the last fifteen minutes is ways that

1461
01:33:59,790 --> 01:34:02,210
but models which which find

1462
01:34:02,230 --> 01:34:07,860
some kind of abstract relational structure in dataset like some kind of tree structured hierarchy

1463
01:34:07,910 --> 01:34:11,920
have been really useful in getting us to think about the deep learning problems for

1464
01:34:12,620 --> 01:34:19,880
not deep learning well actually conceptually deep and also hierarchically deep learning problems for humans

1465
01:34:20,610 --> 01:34:24,040
so the way we actually model this thing which keeps showing you to motivate things

1466
01:34:24,040 --> 01:34:27,370
that never actually showed your model of these data but what we do basically inspired

1467
01:34:27,370 --> 01:34:33,660
by both traditional semantic networks and these results finding class hierarchical tree structured clusters of

1468
01:34:33,660 --> 01:34:38,280
object categories in the brain is basically do what kind of hierarchical clustering and you

1469
01:34:38,280 --> 01:34:42,410
can just i mean you can model these things pretty well just doing

1470
01:34:42,420 --> 01:34:45,410
generic are clustering but

1471
01:34:45,410 --> 01:34:48,870
for reasons that will show you in a second we prefer approach a kind of

1472
01:34:48,870 --> 01:34:54,370
bayesian hierarchical clustering where objects essentially get grouped into a tree structure based on how

1473
01:34:54,370 --> 01:34:59,230
similar they are and then nodes of this tree correspond to candidate word meanings so

1474
01:34:59,230 --> 01:35:02,860
for example this two for could refer to that this sort of that nodes anything

1475
01:35:02,860 --> 01:35:07,250
under it and just by seeing how few labeled examples clustering the tree you're able

1476
01:35:07,250 --> 01:35:11,610
to generalize the word in a very powerful way and get good inductive bias that

1477
01:35:11,610 --> 01:35:12,620
comes from the

1478
01:35:12,620 --> 01:35:15,790
from the structure of the tree where the rule of the unlabelled data is to

1479
01:35:15,790 --> 01:35:19,040
tell you what the tree structures and once you have the tree structure that not

1480
01:35:19,040 --> 01:35:22,330
very hard to figure out from a few labeled examples where is the best point

1481
01:35:22,330 --> 01:35:24,240
to generalize in that tree

1482
01:35:24,290 --> 01:35:27,580
and this is just some data which i will not show you but the model

1483
01:35:27,580 --> 01:35:30,910
fits the data well but what i said we think of this from a kind

1484
01:35:30,910 --> 01:35:34,330
of a hierarchical bayesian perspective this is the sort of picture we have in mind

1485
01:35:34,410 --> 01:35:35,250
there are some

1486
01:35:35,250 --> 01:35:38,080
features which you observe projects this could be

1487
01:35:38,130 --> 01:35:40,940
the you know each of these could be a neuron ninety that we can measure

1488
01:35:40,950 --> 01:35:45,620
the results from and then applying some of those tree structured clustering models to that

1489
01:35:45,620 --> 01:35:49,230
data would give us something like the stream so the idea is in a sense

1490
01:35:49,230 --> 01:35:54,450
which is where we have a generative model of how the features of objects are

1491
01:35:54,450 --> 01:35:59,000
are distributed over some tree structure then we can do bayesian inference in this model

1492
01:35:59,000 --> 01:36:00,080
to figure out

1493
01:36:00,080 --> 01:36:03,040
the poster over trees or maybe just to find the best tree

1494
01:36:03,160 --> 01:36:07,740
and use that to set expectations for new features like like the word label too

1495
01:36:09,190 --> 01:36:11,750
but the reason why we want to think about this from hierarchical bayes point of

1496
01:36:11,750 --> 01:36:14,370
view is we want to consider the fact that

1497
01:36:14,420 --> 01:36:18,110
not only what we now know what's the right restructuring have to learn that from

1498
01:36:18,110 --> 01:36:21,040
the unlabelled data but we might not know that we should be looking for tree

1499
01:36:21,040 --> 01:36:21,790
at all

1500
01:36:21,840 --> 01:36:26,800
if you look more generally across cognition you see a lot of concepts get organize

1501
01:36:26,830 --> 01:36:30,900
into things like trees but there are also other ways to organize concepts and we

1502
01:36:30,900 --> 01:36:34,540
want to be able to discover for example that in this domain of of

1503
01:36:34,550 --> 01:36:38,120
i thought a tree structure is appropriate but in some other domain another structure might

1504
01:36:38,120 --> 01:36:42,800
be appropriate so a picture like this where might consider different forms of structures you

1505
01:36:43,330 --> 01:36:48,000
a lot of which i talked about here like spaces trees if you're interested in

1506
01:36:48,000 --> 01:36:49,940
social relations like who

1507
01:36:49,990 --> 01:36:54,010
advisers who you might be thinking of something sometimes more like the dominance ordering or

1508
01:36:54,040 --> 01:36:57,510
partial order you know how how do you how are we able to discover these

1509
01:36:57,510 --> 01:37:02,370
different forms of structure in data without necessarily knowing that in advanced

1510
01:37:05,630 --> 01:37:10,500
here's just a little bit of motivating motivating intuition b

1511
01:37:11,250 --> 01:37:15,000
you know what i guess one possible response to this is to say well don't

1512
01:37:15,000 --> 01:37:17,080
explicitly try to identify

1513
01:37:17,080 --> 01:37:22,330
these forms structure but have some kind of the network that will just implicitly

1514
01:37:22,370 --> 01:37:26,540
i find but in sort of with enough with enough data to knowledge a completely

1515
01:37:26,540 --> 01:37:31,780
generic representational structure big set of weights to specialize in this sort of direction and

1516
01:37:31,780 --> 01:37:36,120
that's the direction and within cognitive science some people have been usefully pursuing that approach

1517
01:37:36,120 --> 01:37:42,130
like tim rogers and jamie callan have an interesting book basically applying to deep autoencoders

1518
01:37:42,130 --> 01:37:44,820
to these kinds of cognitive tasks

1519
01:37:44,820 --> 01:37:48,790
but we think there's good reason to consider a more structured alternative where you explicitly

1520
01:37:48,790 --> 01:37:54,470
try to identify structure of the right forum for example knowing that the animals

1521
01:37:54,510 --> 01:37:58,840
organised some kind of tree structure makes it much easier to be able to

1522
01:37:58,940 --> 01:38:03,190
generalize from just one example of a new species here's the does anyone know what

1523
01:38:03,190 --> 01:38:04,880
this thing is

1524
01:38:04,940 --> 01:38:09,580
it's do or wildebeest right but even if you didn't know that you can see

1525
01:38:09,610 --> 01:38:13,070
just based on a quick glance at it similar looks some sort of similar to

1526
01:38:13,070 --> 01:38:15,420
these things and not to those things in sync and this plug it right there

1527
01:38:15,420 --> 01:38:19,400
into the tree without having to consider all possible ways it could be related to

1528
01:38:19,410 --> 01:38:21,380
all sorts of other species

1529
01:38:21,400 --> 01:38:25,950
here's another example showing the value of this higher level structural form for inductive bias

1530
01:38:26,000 --> 01:38:31,470
against the first you particularly want american missile test here abstract knowledge of american geography

1531
01:38:31,470 --> 01:38:36,580
and culture so here's here's a few american cities let me tell you about the

1532
01:38:36,580 --> 01:38:37,970
mystery city

1533
01:38:38,000 --> 01:38:42,610
which has just a few properties so it has an average annual temperature of sixty

1534
01:38:42,610 --> 01:38:45,170
six degrees fahrenheit or in nineteen celsius

1535
01:38:45,200 --> 01:38:48,410
in two thousand four voted sixty percent for george bush

1536
01:38:48,470 --> 01:38:51,860
and the most popular foods are fried and barbecue

1537
01:38:51,870 --> 01:38:55,540
so where is the city

1538
01:38:55,550 --> 01:38:58,540
well understood just how and when i get to it

1539
01:38:58,650 --> 01:39:03,940
now let's

1540
01:39:03,950 --> 01:39:05,410
let's do MCMC

1541
01:39:05,410 --> 01:39:07,050
metropolis hastings with

1542
01:39:07,090 --> 01:39:08,750
in proposals

1543
01:39:09,380 --> 01:39:10,330
it discussion

1544
01:39:10,670 --> 01:39:14,450
OK but somewhere over here right

1545
01:39:14,460 --> 01:39:20,530
i was using an hamiltonian MCMC so you may be somewhere around right now again

1546
01:39:20,530 --> 01:39:23,280
you don't have to know that much you you may be thinking of a particular

1547
01:39:23,280 --> 01:39:25,450
city in mind

1548
01:39:25,490 --> 01:39:26,710
or you may not

1549
01:39:26,740 --> 01:39:30,800
but you know enough about how some of these features tend to co vary across

1550
01:39:30,800 --> 01:39:34,660
the two dimensional map of united states that from just a couple of data points

1551
01:39:34,660 --> 01:39:38,830
you can figure out of a lot about what i'm talking about

1552
01:39:38,870 --> 01:39:40,700
now we can also do

1553
01:39:40,920 --> 01:39:42,700
well actually charles

1554
01:39:42,710 --> 01:39:45,750
charles kemp and i've done a lot of experiments which i don't really have time

1555
01:39:45,750 --> 01:39:49,740
to tell you anything about except the take-home message from these experiments is you give

1556
01:39:49,740 --> 01:39:53,460
people various has just like what i was doing you tell them about a new

1557
01:39:53,460 --> 01:39:58,170
property or new object from in front just a couple of observations you ask them

1558
01:39:58,190 --> 01:40:01,170
to make predictions about what other properties of that is going to have or what

1559
01:40:01,170 --> 01:40:05,510
other objects are going to have this new property and depending on the the the

1560
01:40:05,510 --> 01:40:10,300
the domain you know if you give people biological species their predictions are best captured

1561
01:40:10,840 --> 01:40:15,750
tree structured models if you give people predictions about features of cities the best captured

1562
01:40:15,750 --> 01:40:20,910
image patch here what uses the characteristics socail which is the scale peak in the

1563
01:40:20,910 --> 01:40:26,280
scale the main and dominant gradient orientation different ways of estimating actually the scale and

1564
01:40:26,280 --> 01:40:29,890
rotation characteristics of the patch and then

1565
01:40:29,930 --> 01:40:31,680
if you have a matching

1566
01:40:31,710 --> 01:40:36,930
pair right so for example you can see these two image patches are similar so

1567
01:40:36,930 --> 01:40:39,090
they have similar descriptions they will match

1568
01:40:39,100 --> 01:40:44,410
for the ones for the matching descriptor you also have the size of the region

1569
01:40:44,410 --> 01:40:49,170
and the dominant rotation angle right so once you have the spare what you can

1570
01:40:49,770 --> 01:40:53,180
you can tell you determine the scale change between the two patches

1571
01:40:53,190 --> 01:40:58,550
and the rotation change between the two pi two patches will content change of

1572
01:40:58,570 --> 01:41:00,260
much two half

1573
01:41:00,270 --> 01:41:02,900
and rotation angle of twenty degrees

1574
01:41:03,000 --> 01:41:04,420
so each

1575
01:41:06,600 --> 01:41:11,140
descriptor pair is out in the scale and angle difference

1576
01:41:11,230 --> 01:41:13,270
and the assumption we make here

1577
01:41:13,280 --> 01:41:18,980
which is not which is not exact but it's approximation is that the global image

1578
01:41:18,980 --> 01:41:21,760
and rotation changes are roughly consistent

1579
01:41:21,800 --> 01:41:25,560
so it's it's holds exactly if you have a scale change between the images so

1580
01:41:25,560 --> 01:41:29,640
the image patches they will have the same scale change between the matching image patches

1581
01:41:29,640 --> 01:41:33,470
with the scared him to go and between them it doesn't hold if you for

1582
01:41:33,470 --> 01:41:38,010
example the viewpoint change there will be only approximately correct but in case of image

1583
01:41:38,010 --> 01:41:41,010
rotation in which case and is exactly correct

1584
01:41:41,230 --> 01:41:45,310
in case of perspective viewpoint changes rough approximation

1585
01:41:45,340 --> 01:41:50,380
OK and for example for the orientation consistency and so on

1586
01:41:50,420 --> 01:41:54,560
on the left you have query image on the right the image which has been

1587
01:41:54,560 --> 01:41:58,260
stored in the database and you can see for each point

1588
01:41:58,270 --> 01:42:00,040
we show the rotation

1589
01:42:00,050 --> 01:42:04,540
the angle in the rotation difference between the patches right and you can see here

1590
01:42:04,560 --> 01:42:08,680
most of them are consistent but if you look closely there are few outliers for

1591
01:42:10,440 --> 01:42:13,380
this one up here in the top right which is much larger than the same

1592
01:42:14,930 --> 01:42:20,170
and if you quantized this orientation differences you can see here

1593
01:42:20,190 --> 01:42:21,650
there's is the maximum

1594
01:42:21,660 --> 01:42:25,850
and this is actually the rotation angle between two images so if you look here

1595
01:42:25,870 --> 01:42:28,690
this is the maximum for this pair of images

1596
01:42:28,750 --> 01:42:34,060
if you look you it's pay pile for the approximately rotation angle of forty five

1597
01:42:34,070 --> 01:42:37,180
degrees between two images is found correctly

1598
01:42:39,060 --> 01:42:41,440
example for the scale consistency

1599
01:42:41,440 --> 01:42:42,700
you can see here

1600
01:42:42,710 --> 01:42:45,580
two images to different images of the ivory tower

1601
01:42:45,680 --> 01:42:48,090
yes approximately scale change of

1602
01:42:48,180 --> 01:42:51,770
one half between the two of them and you can see here we show the

1603
01:42:51,770 --> 01:42:53,850
matching matching regions

1604
01:42:54,680 --> 01:42:58,950
you take the difference of the characteristic scale and you can see here

1605
01:42:58,960 --> 01:43:01,270
that find the p q

1606
01:43:01,290 --> 01:43:07,010
at approximately one-half right this allows to fill the matches not only on the description

1607
01:43:07,360 --> 01:43:09,670
but also on the geometric consistency

1608
01:43:09,710 --> 01:43:14,830
and so if you keep this this histograms in mind

1609
01:43:14,880 --> 01:43:19,680
these are the histograms was for the voting so voting is not only performed for

1610
01:43:19,680 --> 01:43:23,720
image but it's also performed was angle and scale different

1611
01:43:23,760 --> 01:43:29,960
we've seen we have observed that these steps stops the independence of the rotation and

1612
01:43:29,960 --> 01:43:33,250
scale which is of course between two images independent

1613
01:43:33,290 --> 01:43:34,720
then obtain

1614
01:43:34,730 --> 01:43:39,730
the score for all quantities is quite hard enough scale difference for each image

1615
01:43:39,740 --> 01:43:41,480
and the final score

1616
01:43:41,510 --> 01:43:45,770
then filled in for each of these two parameters we take the in between them

1617
01:43:45,820 --> 01:43:48,600
OK so this allows us

1618
01:43:48,610 --> 01:43:53,980
two of ten matches that agrees with the main difference in orientation scale

1619
01:43:54,010 --> 01:43:57,250
this will be taking into account for the final score so if we have many

1620
01:43:57,250 --> 01:44:01,330
matches but none of them agrees into in the scale of the orientation that we

1621
01:44:01,330 --> 01:44:02,630
be also out

1622
01:44:02,680 --> 01:44:04,150
on the other hand if they agree

1623
01:44:04,210 --> 01:44:08,310
keep peak which is much higher in the end we are voting score might but

1624
01:44:08,310 --> 01:44:09,630
is much higher

1625
01:44:09,650 --> 01:44:10,510
and what

1626
01:44:10,930 --> 01:44:13,280
an interesting question and can i ask

1627
01:44:13,330 --> 01:44:14,840
it's to so

1628
01:44:14,850 --> 01:44:18,450
again further improvement by using the full geometric ranking or not

1629
01:44:18,490 --> 01:44:24,000
so ideally i would say yes to show this now spend results ideally the answer

1630
01:44:24,000 --> 01:44:28,920
would be yes because for now we just have done very weak geometric verification however

1631
01:44:28,920 --> 01:44:33,110
if we put the full geometric model on top of that we obtain

1632
01:44:33,130 --> 01:44:35,160
additional verification

1633
01:44:35,170 --> 01:44:39,860
and then something which is interesting as well actually if you look

1634
01:44:39,880 --> 01:44:44,260
at the the and the differences from images

1635
01:44:44,290 --> 01:44:49,040
we can observe that what would you think anyways for natural collection of natural images

1636
01:44:49,120 --> 01:44:54,400
most of features that take the same angle then there's the peak at high

1637
01:44:54,410 --> 01:44:55,960
and i which

1638
01:44:55,980 --> 01:45:00,760
so we can use the prior for providing our and the different which shows has

1639
01:45:00,760 --> 01:45:03,040
been shown to further improve the results

1640
01:45:03,160 --> 01:45:09,690
and following the experimental evaluation if created dataset for evaluation

1641
01:45:09,700 --> 01:45:15,010
which contains two thousand images five hundred query images nine hundred ninety one annotated true

1642
01:45:17,590 --> 01:45:21,490
one million described attractor images which have obtained from flickr

1643
01:45:21,500 --> 01:45:26,690
if constructed the vocabulary of an independent dataset also obtained from flickr

1644
01:45:26,730 --> 01:45:32,620
and our search system is almost real time showing them all in a minute

1645
01:45:32,720 --> 01:45:35,220
so how do we have made our results

1646
01:45:35,230 --> 01:45:40,470
those outside of based on mean average precision which means the bigger the better right

1647
01:45:40,480 --> 01:45:41,970
here we know

1648
01:45:42,020 --> 01:45:47,580
no no precision and one will the very high precision so basically decision is the

1649
01:45:47,580 --> 01:45:51,470
precision recall curve so if you have what we call one precision one that's the

1650
01:45:51,470 --> 01:45:54,040
best curve we can obtain

1651
01:45:54,060 --> 01:46:01,020
the average precision recall curves a few examples of the dataset

1652
01:46:01,120 --> 01:46:01,940
and then

1653
01:46:01,950 --> 01:46:06,670
a few images image sequences which on dataset so on the left is the query

1654
01:46:06,670 --> 01:46:08,470
images and

1655
01:46:08,490 --> 01:46:10,800
the right it's all the images which have

1656
01:46:10,820 --> 01:46:15,900
if have stored in this manner images which would like to retrieve

1657
01:46:15,900 --> 01:46:17,150
another example

1658
01:46:17,170 --> 01:46:19,100
poster from venice

1659
01:46:19,160 --> 01:46:23,960
and a few examples of the distractor images from flickr

1660
01:46:23,980 --> 01:46:28,430
and the evaluation for our holiday dataset

1661
01:46:28,450 --> 01:46:31,760
and law we show the baseline

1662
01:46:31,770 --> 01:46:34,870
so just obtain the bag of visual words

1663
01:46:34,870 --> 01:46:40,920
OK great pleasure to be here thanks for the invitation this talk i this seems

1664
01:46:40,930 --> 01:46:44,560
about time image and vision and geometry and so on

1665
01:46:44,570 --> 01:46:47,450
and how you told to is inspired by

1666
01:46:47,470 --> 01:46:52,220
geometry and two dimensional signals images and so on

1667
01:46:52,240 --> 01:46:55,950
it's so for a long time to be a one-dimensional talk i apologize because most

1668
01:46:55,950 --> 01:46:58,450
of the solutions we master are actually

1669
01:46:58,490 --> 01:47:02,190
can be explained in one dimension so easily and then we'll go back and actually

1670
01:47:02,190 --> 01:47:06,340
talk also about superresolution is very anderson application

1671
01:47:06,350 --> 01:47:09,820
OK so i i hope at some point to convince you that i'm not in

1672
01:47:09,820 --> 01:47:11,380
the wrong auditorium

1673
01:47:11,390 --> 01:47:17,690
OK thanks to the organizers to the sponsors they also first before anything else say

1674
01:47:17,690 --> 01:47:23,300
a set this is work joint work with terrible blair graduated from polytechnic year in

1675
01:47:23,310 --> 01:47:28,950
the colonial hockey two phd students and richard i PM until know what worked on

1676
01:47:28,950 --> 01:47:31,990
this while being phd student at EPFL

1677
01:47:35,810 --> 01:47:40,840
o thing and thirty five minutes so till eleven when tried to tell you

1678
01:47:40,930 --> 01:47:45,650
he's going back to very old result which is shown sampling

1679
01:47:45,650 --> 01:47:52,480
which is essentially the underpinning theorem for everything we do in individual processing in you

1680
01:47:52,480 --> 01:47:59,120
normal comedians on and and then also question the validity of random sampling for images

1681
01:47:59,120 --> 01:48:02,700
in the article about some motivation for the work described

1682
01:48:02,850 --> 01:48:07,540
and from there i will talk about signals which are sparse in some domain

1683
01:48:08,150 --> 01:48:09,320
but not in

1684
01:48:09,380 --> 01:48:14,790
is certainly not in fourier domain therefore they don't actually fit the shaman framework for

1685
01:48:14,790 --> 01:48:22,320
example diracs infinitely sharp objects edges and so on and define classes of signals that

1686
01:48:22,320 --> 01:48:23,560
are kind

1687
01:48:23,570 --> 01:48:26,480
which generalizes bandlimited functions

1688
01:48:26,480 --> 01:48:29,920
and for which are going to pose the question of sampling

1689
01:48:31,400 --> 01:48:35,710
that's what i'm going to do well in in section three one shows that actually

1690
01:48:35,860 --> 01:48:41,450
even though these signals are of infinite bandwidth and not in shift invariant subspaces i

1691
01:48:41,450 --> 01:48:46,660
have to insist better on manifolds can be sampled at least certain classes we can

1692
01:48:46,660 --> 01:48:51,350
explain very precisely how to sample them and we'll do this and actually pull the

1693
01:48:51,350 --> 01:48:56,340
sampling theorem exactly on on some type of signals and then from there on we

1694
01:48:56,340 --> 01:48:59,890
want to show that this is actually robust in the case of noise so it's

1695
01:48:59,890 --> 01:49:05,130
not just the paper and pencil result and is time limits all tell you a

1696
01:49:05,130 --> 01:49:10,200
little bit about the connection between this work and compressed sensing

1697
01:49:10,200 --> 01:49:15,260
and i'll conclude by talking a little bit about applications and and also an example

1698
01:49:15,260 --> 01:49:17,940
of technology transfer offices

1699
01:49:17,950 --> 01:49:21,510
of these mathematical results to actual applications

1700
01:49:22,600 --> 01:49:26,040
so the set up is extremely simple but i want to insist on the set

1701
01:49:26,040 --> 01:49:27,230
of it

1702
01:49:27,250 --> 01:49:31,420
we are we live in continuous time OK so my world is continuous time physical

1703
01:49:31,420 --> 01:49:34,790
world and i want to search the world

1704
01:49:34,800 --> 01:49:39,080
and top of the world i have sampling kernel it's an observation kernel it's still

1705
01:49:39,450 --> 01:49:43,570
in the digital photography camera or it's

1706
01:49:43,580 --> 01:49:48,570
you know it's the lowpass filter on the input of from a to d converters

1707
01:49:48,570 --> 01:49:52,160
for dual processing for example and san

1708
01:49:52,200 --> 01:49:56,320
i have an approximation here that's typically projection

1709
01:49:56,330 --> 01:50:03,440
and i take a uniform sample the two things that are keys it's continuous time

1710
01:50:03,450 --> 01:50:09,070
OK so it's t storyline and its uniform sampling and that's how sampling is done

1711
01:50:09,070 --> 01:50:11,040
in ninety nine point nine

1712
01:50:11,050 --> 01:50:15,100
percent of objects i know of in the digital world

1713
01:50:15,140 --> 01:50:19,300
not to say a hundred percent because it can always be an exception but that's

1714
01:50:19,300 --> 01:50:22,980
not how we acquire the real world so here's the real war here's the digital

1715
01:50:22,980 --> 01:50:28,030
world and the question is completely obvious is when you have a one-to-one mapping between

1716
01:50:28,030 --> 01:50:35,730
objects here from the continents world to sample set of sequences right and it's a

1717
01:50:35,820 --> 01:50:39,070
it's a very obvious question since we use this all the time

1718
01:50:39,080 --> 01:50:43,320
and the astonishing thing is that this set here is

1719
01:50:43,330 --> 01:50:46,360
here is only partially described as far as i know

1720
01:50:46,370 --> 01:50:48,240
so we'll extend that set a little bit

1721
01:50:48,250 --> 01:50:52,660
and the other thing i want to say is you don't always have the choice

1722
01:50:53,540 --> 01:50:57,040
about sampling the observation kernel so for example

1723
01:50:57,050 --> 01:51:01,270
this is the european southern space observatory

1724
01:51:01,290 --> 01:51:04,290
when you go there and you want to do the job signal processing people say

1725
01:51:04,290 --> 01:51:09,540
oh by the way here is sampling currently fifty million telescopes have seven of them

1726
01:51:09,740 --> 01:51:13,280
because so it's rather expensive operation so it comes and say

1727
01:51:13,330 --> 01:51:16,040
you know what i want to change the sampling current i would like to have

1728
01:51:16,040 --> 01:51:20,120
a pseudorandom sequence you know people sort of put your yourself things the desert

1729
01:51:20,140 --> 01:51:25,120
never talk to you again so that's really like this and then it also happens

1730
01:51:25,120 --> 01:51:30,690
that is sampling rate is often not your choice is given by technology by prior

1731
01:51:30,690 --> 01:51:32,960
design and on or simply by

1732
01:51:33,100 --> 01:51:37,660
constraints from the physics of what rate you can actually sample of joe wideband signals

1733
01:51:37,660 --> 01:51:42,950
which have ten gigahertz if you want to sample them at nyquist shannon bandwidth it

1734
01:51:42,950 --> 01:51:46,960
will be twenty gigahertz which is a little bit pushing the technology

1735
01:51:47,600 --> 01:51:53,220
so the classic result i don't want to recall it here the statement from the

1736
01:51:53,220 --> 01:51:58,920
nineteen forty eight paper is actually was reinvented proved many times or earlier shannon but

1737
01:51:58,920 --> 01:52:03,090
it's actually is the projection of the input space onto the space of bandlimited signals

1738
01:52:03,090 --> 01:52:07,620
and applying what formula or or something like this you can show there is a

1739
01:52:07,620 --> 01:52:14,240
one-to-one map you can approximate x in its bandlimited or small version

1740
01:52:14,250 --> 01:52:19,660
now what i call the nightmare for signal processing or image processing person is you

1741
01:52:19,660 --> 01:52:24,030
have an object that is very small that comes from the space of bandlimited function

1742
01:52:24,140 --> 01:52:29,250
and sends the devil comes and puts one singularity onto this signal so f of

1743
01:52:29,620 --> 01:52:34,160
are you let say x of t is is from the band limited space

1744
01:52:34,220 --> 01:52:38,230
then the devil makes new function f of the which has just two unknowns the

1745
01:52:38,230 --> 01:52:43,560
location and the high office of continuity i

1746
01:52:43,590 --> 01:52:48,400
the fourier transform is bandlimited anymore and there is no sampling theorem for nine

1747
01:52:48,420 --> 01:52:52,580
OK unless you're happy to sort of small out this edge which then you know

1748
01:52:52,580 --> 01:52:58,630
is equal to the integral from 0 to 1 of the Hessian of ex post

1749
01:52:59,310 --> 01:53:01,970
times minus X

1750
01:53:07,400 --> 01:53:16,900
times minus X dt this matrix that is of the vector

1751
01:53:17,560 --> 01:53:19,740
why is this statement true by the way

1752
01:53:20,040 --> 01:53:21,980
OK so

1753
01:53:21,990 --> 01:53:25,620
this statement is true just think of them is defined

1754
01:53:26,320 --> 01:53:33,740
the function of 1 variable T V of t to be F of X

1755
01:53:33,760 --> 01:53:39,010
which are the gradient of x plus seek times e minus X

1756
01:53:39,390 --> 01:53:46,380
so everybody see this OK then whom I up to now

1757
01:53:47,740 --> 01:53:49,860
you said lot

1758
01:53:49,870 --> 01:53:51,850
higher rates

1759
01:53:51,910 --> 01:53:54,680
right OK so high what is the of 0

1760
01:53:58,360 --> 01:54:09,980
it was really and about gradient of X and the question what is the 1

1761
01:54:10,080 --> 01:54:20,800
that you and I and what is the product of the derivative of this guy

1762
01:54:20,810 --> 01:54:27,040
with respect to the noun don't think that is known for is means you of

1763
01:54:27,040 --> 01:54:29,200
is the derivative of the entire vector

1764
01:54:29,960 --> 01:54:33,280
you know

1765
01:54:39,440 --> 01:54:51,580
if you go to the elevator was to the money that's

1766
01:54:51,860 --> 01:54:56,820
times consumers

1767
01:54:57,080 --> 01:54:59,140
In order to estimate

1768
01:55:02,060 --> 01:55:10,440
the Hessian evaluated here

1769
01:55:10,560 --> 01:55:16,920
and so now the fundamental theorem of calculus I know that you want find of

1770
01:55:16,920 --> 01:55:24,740
0 is equal to the integral is from 0 what the prior to the 1st

1771
01:55:25,600 --> 01:55:35,360
that's on the fundamental theorem can can now 1 is gradient methods the fear of

1772
01:55:35,360 --> 01:55:37,480
zeros gradient method next

1773
01:55:37,800 --> 01:55:42,140
and this is the integral from zero one the price of dt

1774
01:55:42,900 --> 01:55:50,710
that's the case analysis to prove it and I invite you to help me is

1775
01:55:50,720 --> 01:55:57,140
any make errors in

1776
01:56:03,340 --> 01:56:14,200
OK so here goes

1777
01:56:15,540 --> 01:56:20,420
so next month is

1778
01:56:21,220 --> 01:56:31,520
the exons my interest XML is a star is disability extends his X month the

1779
01:56:31,530 --> 01:56:37,600
Hessian inverse times the gradient

1780
01:56:37,740 --> 01:56:48,380
that's easy enough just rewrite this a little differently optimize articles because this plus Hessian

1781
01:56:48,380 --> 01:56:58,170
number and the gradient that kind of land that why is this true him up

1782
01:56:59,310 --> 01:57:00,850
you know

1783
01:57:01,080 --> 01:57:03,350
here in

1784
01:57:03,660 --> 01:57:14,740
what it actually is what introduce more rapid and introduces because it's equals 0 OK

1785
01:57:15,040 --> 01:57:16,040
so I

1786
01:57:16,210 --> 01:57:25,080
that is on the right is this is excellent that's the heart of the Hessian

1787
01:57:25,540 --> 01:57:29,560
inverse another book that formula

1788
01:57:29,780 --> 01:57:32,380
just great

1789
01:57:32,420 --> 01:57:35,740
this formula here from Proposition 2

1790
01:57:36,020 --> 01:57:42,170
the gradient of a star minus the gradient method that is going to be the

1791
01:57:42,170 --> 01:57:46,780
integral from 0 to 1 of the Hessian

1792
01:57:47,210 --> 01:57:52,400
X plus the storm

1793
01:57:53,160 --> 01:57:55,450
this guy

1794
01:57:55,580 --> 01:58:00,500
time based on

1795
01:58:00,900 --> 01:58:10,820
everybody with me but that is not a rewrite everything which has front

1796
01:58:11,050 --> 01:58:25,340
and this is a very passionate explicity times this phenomenon called agent

1797
01:58:25,390 --> 01:58:36,420
that's what I so would do it from set of this and I'm up to

1798
01:58:36,920 --> 01:58:40,760
you name is we've Nicholas would lead to an analysis

1799
01:58:42,200 --> 01:58:44,820
1 of the

