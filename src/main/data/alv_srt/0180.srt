1
00:00:00,000 --> 00:00:05,210
t two g for the octahedral case

2
00:00:06,550 --> 00:00:08,600
the difference in energy

3
00:00:08,610 --> 00:00:12,730
between these sets of orbitals

4
00:00:15,460 --> 00:00:16,050
is this

5
00:00:16,060 --> 00:00:21,110
sign here sort of the delta zero sign

6
00:00:21,140 --> 00:00:27,230
and actually o and that after he trolled was up to huge role

7
00:00:27,240 --> 00:00:29,910
and it's the crystal

8
00:00:35,870 --> 00:00:45,680
so it's how much the ligands where the field so this

9
00:00:45,690 --> 00:00:49,370
theoretical case up here there's no splitting going on

10
00:00:49,390 --> 00:00:50,810
you bring in

11
00:00:50,820 --> 00:00:56,530
the ligands and they causes splitting because some orbital to go up in energy in

12
00:00:56,530 --> 00:00:57,910
some to go down

13
00:00:57,940 --> 00:01:02,070
and the distance between them to the degree of splitting

14
00:01:02,110 --> 00:01:07,450
is this octahedral crystal field splitting energy

15
00:01:07,460 --> 00:01:12,030
now the overall energy of the system is maintained

16
00:01:12,060 --> 00:01:15,540
so if these go up in these go down

17
00:01:15,550 --> 00:01:19,900
to get everything to be equal

18
00:01:29,620 --> 00:01:32,810
so these parts some of these should go up

19
00:01:32,810 --> 00:01:35,180
trying to draw the straight from this line

20
00:01:35,200 --> 00:01:37,780
these were going to go up in energy

21
00:01:37,780 --> 00:01:39,970
five plus two

22
00:01:39,990 --> 00:01:43,350
the octahedral crystal field splitting energy

23
00:01:43,360 --> 00:01:46,940
and these three orbitals are going to go down

24
00:01:46,990 --> 00:01:48,430
in energy

25
00:01:48,520 --> 00:01:50,280
will be stabilized

26
00:01:50,290 --> 00:01:53,890
by minus two for this

27
00:01:53,910 --> 00:01:59,400
the octahedral crystal field splitting energy so two orbitals are going up so it's by

28
00:02:00,630 --> 00:02:05,110
three orbitals are going down so it's by two this because

29
00:02:05,110 --> 00:02:10,340
it's going to have to equal out so you're not for creating a different system

30
00:02:10,340 --> 00:02:15,950
here the energy is is going to be conserved the overall energy is maintained so

31
00:02:15,950 --> 00:02:19,060
torvalds up to thirty three orbitals down

32
00:02:19,350 --> 00:02:23,390
too fast

33
00:02:25,290 --> 00:02:28,400
so that's really it's a very simple idea

34
00:02:28,470 --> 00:02:30,070
instead of the

35
00:02:31,470 --> 00:02:33,600
we're all the ligands are everywhere

36
00:02:33,610 --> 00:02:37,910
when you put them in octahedral geometry it that some of the the orbitals differently

37
00:02:37,910 --> 00:02:42,610
than others some inner orbitals go up in energy some go down the overall energy

38
00:02:42,620 --> 00:02:44,500
it is maintained

39
00:02:44,500 --> 00:02:47,360
so let's look at an example now

40
00:02:47,390 --> 00:02:51,610
and figure out how to put electrons into the system

41
00:02:51,650 --> 00:02:54,330
so let's look at the case where we have

42
00:02:56,160 --> 00:02:59,490
nh three three

43
00:02:59,530 --> 00:03:01,690
and we are

44
00:03:03,040 --> 00:03:08,020
so first we need to figure out the oxidation number of chromium

45
00:03:08,030 --> 00:03:10,340
having everything in the graph here

46
00:03:10,350 --> 00:03:13,100
i mean that you have six ligands total

47
00:03:13,110 --> 00:03:17,920
three NH three ligand three b are ligands or from i'd like it

48
00:03:17,930 --> 00:03:21,310
and those are all in one bracket there's no number in the corner of the

49
00:03:21,310 --> 00:03:24,110
brackets so that means the overall charge

50
00:03:24,120 --> 00:03:27,110
in the system has to to be equal to zero

51
00:03:27,150 --> 00:03:32,460
so what will be the oxidation number of grammy

52
00:03:32,510 --> 00:03:34,770
should be minus one

53
00:03:34,800 --> 00:03:36,460
three minus one

54
00:03:36,520 --> 00:03:38,290
what about NH three

55
00:03:38,310 --> 00:03:39,850
as a unit

56
00:03:41,540 --> 00:03:43,950
so three times zero

57
00:03:43,970 --> 00:03:48,570
and what is that we view with the the oxidation number of chromium

58
00:03:52,320 --> 00:03:59,600
so we have prostrate so now we can figure out d electron count

59
00:03:59,610 --> 00:04:05,450
so we need to look up the group number

60
00:04:10,020 --> 00:04:12,830
the group numbers what

61
00:04:12,840 --> 00:04:20,410
can you see it up there

62
00:04:20,440 --> 00:04:24,460
most people seem to be saying six

63
00:04:24,470 --> 00:04:26,750
OK so that's right

64
00:04:26,760 --> 00:04:29,000
so x minus three

65
00:04:29,020 --> 00:04:34,100
it's great so this is the gene three system

66
00:04:34,150 --> 00:04:40,510
so that means we're going to put in free electrons

67
00:04:40,550 --> 00:04:46,150
so where would you i mean to put those electrons down here up here

68
00:04:48,660 --> 00:04:51,150
what if i do this

69
00:04:51,170 --> 00:04:52,810
would that be good

70
00:04:54,870 --> 00:04:58,410
so we we are coming back to some of the role of the learned earlier

71
00:04:58,410 --> 00:04:59,700
in the semester

72
00:04:59,750 --> 00:05:02,100
so first you want to put them in the lowest

73
00:05:02,140 --> 00:05:03,850
energy orbitals

74
00:05:03,860 --> 00:05:07,690
you don't want to ever have won with two parallel spins because that would give

75
00:05:07,690 --> 00:05:09,970
you the same four quantum numbers

76
00:05:10,070 --> 00:05:11,600
so that's not good

77
00:05:11,610 --> 00:05:16,030
and you also have a choice want to put them in the same

78
00:05:16,080 --> 00:05:21,130
energy put them in single eight before you start to pair so those are all

79
00:05:21,130 --> 00:05:23,960
things that you talked about earlier in the system

80
00:05:24,020 --> 00:05:25,680
earlier in the semester

81
00:05:25,700 --> 00:05:27,840
so we put our three

82
00:05:27,850 --> 00:05:31,450
electrons in one to each orbital

83
00:05:31,470 --> 00:05:32,680
all the spin

84
00:05:34,280 --> 00:05:40,490
so we have a lot principal pauli exclusion and also huns rule all at play

85
00:05:40,490 --> 00:05:44,190
here with how we put in the electrons

86
00:05:44,210 --> 00:05:46,050
so now we can

87
00:05:46,050 --> 00:05:47,930
we can

88
00:05:47,960 --> 00:05:53,290
talk about the d and electron configuration

89
00:05:53,320 --> 00:05:55,900
and maybe i can rate over here

90
00:05:56,750 --> 00:06:02,340
that's just the way of expressing how many electrons are down here

91
00:06:07,450 --> 00:06:09,380
electron configuration

92
00:06:09,390 --> 00:06:11,910
so that would just be able to

93
00:06:11,960 --> 00:06:13,680
t two g

94
00:06:13,700 --> 00:06:15,270
so the three

95
00:06:15,280 --> 00:06:17,690
so you list the name

96
00:06:17,770 --> 00:06:22,290
of these sets of orbitals and that's really why these have labels because it makes

97
00:06:22,290 --> 00:06:27,490
your life easier instead of listing all the orbitals each time so there are three

98
00:06:27,500 --> 00:06:30,370
in this lower energy levels

99
00:06:30,420 --> 00:06:32,700
now we can also talk about

100
00:06:33,140 --> 00:06:40,190
crystal field stabilisation energy

101
00:06:40,240 --> 00:06:42,110
crystal field

102
00:06:45,350 --> 00:06:47,740
you don't get this confused with

103
00:06:47,790 --> 00:06:50,350
crystal field splitting energy

104
00:06:50,490 --> 00:06:53,630
crystal field stabilisation energy

105
00:06:53,640 --> 00:06:56,170
and it's abbreviated c

106
00:06:59,190 --> 00:07:02,110
and that's really just thinking about how

107
00:07:02,150 --> 00:07:04,870
much to stabilised by

108
00:07:04,880 --> 00:07:06,310
this splitting

109
00:07:06,310 --> 00:07:09,730
he was the first one to record the and had very very few neurons i'm

110
00:07:09,730 --> 00:07:11,540
not sure we can rest

111
00:07:11,580 --> 00:07:13,420
shared with those data

112
00:07:14,020 --> 00:07:20,150
i don't think wolfram schultz wolfram schultz is kind of the expert of recorded dopamine

113
00:07:20,150 --> 00:07:24,770
neurons in monkeys and i don't think he's ever done similar stimulus reward

114
00:07:24,810 --> 00:07:28,100
you might also ask what about stimulus reward reward

115
00:07:28,150 --> 00:07:31,920
you would want to stimulus to hide of twice as much prediction are nothing to

116
00:07:31,920 --> 00:07:35,170
the rewards so there are still things to be done

117
00:07:36,080 --> 00:07:40,600
people have put this theory to stringent tests not only the ones that we saw

118
00:07:41,370 --> 00:07:46,020
with different probabilities in different magnitudes but also

119
00:07:46,040 --> 00:07:50,000
one hundred buyer all winter and it is they said well

120
00:07:50,040 --> 00:07:54,440
we know that if we have regular temporal difference learning

121
00:07:54,440 --> 00:07:57,480
the value at each point in time should be

122
00:07:58,770 --> 00:08:02,980
and exponentially weighted sum of all the previous rewards

123
00:08:03,730 --> 00:08:07,440
you can work that out out of TD learning and we won't work right now

124
00:08:07,440 --> 00:08:12,370
but you can check it later the last reward have the highest influence on the

125
00:08:12,370 --> 00:08:17,020
critical value then the one before that less influenced less according to

126
00:08:17,080 --> 00:08:22,020
the learning rate so there are exponentially discounted because of the learning great

127
00:08:22,060 --> 00:08:26,450
so the question is could we see that in dopamine neurons and what they did

128
00:08:26,450 --> 00:08:28,210
is they gave

129
00:08:28,330 --> 00:08:33,730
the monkeys different amounts of reward in in each trial and recorded the prediction error

130
00:08:33,730 --> 00:08:34,090
the the

131
00:08:34,540 --> 00:08:36,890
dopamine signal any child

132
00:08:38,920 --> 00:08:42,890
fit a model

133
00:08:42,910 --> 00:08:46,770
well let me be very clear now we're looking at the prediction are sort of

134
00:08:46,770 --> 00:08:51,290
prediction should be reward minus value right so the last word should have

135
00:08:51,310 --> 00:08:53,310
o weighting of one

136
00:08:53,330 --> 00:08:56,540
minus the values of mine this kind of

137
00:08:59,600 --> 00:09:03,960
and so they took the prediction regression on all the previous on the

138
00:09:03,960 --> 00:09:05,440
last ten

139
00:09:07,410 --> 00:09:09,370
the magnitudes of the last ten

140
00:09:09,390 --> 00:09:11,960
rewards and indeed found

141
00:09:11,980 --> 00:09:17,810
these points the black which fit amazingly well to the red line which is the

142
00:09:17,810 --> 00:09:19,460
prediction of the model

143
00:09:19,480 --> 00:09:23,100
with the concept learning great

144
00:09:23,140 --> 00:09:25,230
and in the same study they

145
00:09:25,230 --> 00:09:27,060
they could also

146
00:09:27,060 --> 00:09:30,850
there kind of a slightly different analysis where every trial

147
00:09:30,870 --> 00:09:34,980
and the model predicted what the prediction error should be after fitting the learning rate

148
00:09:34,980 --> 00:09:37,100
to the specific monkey

149
00:09:37,150 --> 00:09:41,670
and the measured firing great is on this axis and you see that basically

150
00:09:41,670 --> 00:09:44,460
a straight line as you could want from noisy

151
00:09:44,850 --> 00:09:46,750
biological measurements

152
00:09:46,790 --> 00:09:50,540
after the prediction error of minus o point one then it kind of

153
00:09:50,560 --> 00:09:52,640
it flattens out that's

154
00:09:52,690 --> 00:09:57,890
that's an issue that we're still that people are still grappling with the thing is

155
00:09:57,940 --> 00:10:03,170
prediction errors if dopamine firing is the prediction error it there can be negative firing

156
00:10:03,170 --> 00:10:05,750
it can only stop firing completely

157
00:10:05,750 --> 00:10:09,940
right so at some point this has to level out and something else maybe has

158
00:10:11,500 --> 00:10:17,230
helping representing negative prediction errors are hypothesis is that in other neuromodulators does this

159
00:10:17,230 --> 00:10:21,750
or that the length of the gap in firing that passes from hand by the

160
00:10:21,750 --> 00:10:23,560
length of of the gas

161
00:10:23,560 --> 00:10:28,350
represents how big this prediction are the negative prediction errors but least for positive prediction

162
00:10:28,350 --> 00:10:29,540
errors are so nice

163
00:10:29,560 --> 00:10:34,210
linear relationship between what the model says prediction should be and how much the neuron

164
00:10:40,520 --> 00:10:42,750
so the idea is the dopamine

165
00:10:43,870 --> 00:10:45,060
prediction error

166
00:10:45,080 --> 00:10:48,920
and we know the prediction errors have a role in reinforcement learning prediction errors are

167
00:10:49,730 --> 00:10:51,140
learning right

168
00:10:51,830 --> 00:10:54,190
first of all as he said

169
00:10:54,230 --> 00:10:58,140
dopamine project mainly to the basal ganglia basal ganglia

170
00:10:58,140 --> 00:10:59,310
are these

171
00:10:59,310 --> 00:11:05,170
complex structures on both sides kind of inside not in the cortex inside your brain

172
00:11:05,190 --> 00:11:08,440
if you go kind of from here to its the middle

173
00:11:12,410 --> 00:11:16,310
and what we know about what do mean does there is

174
00:11:18,480 --> 00:11:24,790
corticostriatal synapses show dopamine dependent plasticity so this is quite a mouthful but

175
00:11:24,980 --> 00:11:26,710
right i have

176
00:11:27,390 --> 00:11:30,210
this is the neuron in the straight

177
00:11:30,230 --> 00:11:34,190
and they get inputs from two directions from two areas one is from the cortex

178
00:11:34,190 --> 00:11:36,390
from from outside the

179
00:11:36,440 --> 00:11:42,520
frontal area of the cortex mostly so this is this for coming to this snaps

180
00:11:42,520 --> 00:11:46,690
and on the other hand it gets don't need reference kind of hard the synapse

181
00:11:46,690 --> 00:11:50,170
is called the neck of this five this is the head of the five hundred

182
00:11:50,170 --> 00:11:53,580
index of the spines of each and every snaps

183
00:11:53,620 --> 00:11:55,310
and it turns out

184
00:11:55,330 --> 00:11:58,770
but if you are

185
00:12:01,290 --> 00:12:06,640
if you excite the neurons here so you invoke activity in these neurons and in

186
00:12:06,640 --> 00:12:09,120
a way that causes this neuron to fire

187
00:12:09,150 --> 00:12:14,810
and measure how that affects the strength of the thing that's usually LTP long-term potentiation

188
00:12:15,140 --> 00:12:20,690
and hebbian learning prescribes that fire together wire together if this virus and this virus

189
00:12:20,690 --> 00:12:23,140
their connection strength

190
00:12:23,140 --> 00:12:25,890
but it turns out that in this in these

191
00:12:25,910 --> 00:12:28,870
cortex two straight synapses

192
00:12:28,890 --> 00:12:32,310
only if there's still around that happens so here you see

193
00:12:32,330 --> 00:12:33,830
these measurements

194
00:12:33,850 --> 00:12:34,770
i don't mean

195
00:12:34,810 --> 00:12:36,730
was around and so

196
00:12:36,790 --> 00:12:41,790
the height of the activation of this neuron became higher after

197
00:12:41,810 --> 00:12:44,940
carrying their activity their activity

198
00:12:44,980 --> 00:12:49,770
in the bottom here there was no dopamine around and actually even became lower

199
00:12:49,770 --> 00:12:51,190
so the idea is

200
00:12:51,210 --> 00:12:55,020
that's not not regular hebbian learning here but three factor learning rule you need to

201
00:12:55,020 --> 00:12:58,620
have both the presynaptic neuron and the postsynaptic neuron

202
00:12:58,750 --> 00:13:03,290
and dopamine around for learning which is exactly what we wanted we wanted learning contingent

203
00:13:03,290 --> 00:13:07,210
on prediction errors if there's the prediction error you should learn and if there's no

204
00:13:07,210 --> 00:13:14,460
prediction there you don't need to learn

205
00:13:15,940 --> 00:13:18,640
so so far

206
00:13:18,650 --> 00:13:23,480
what we have is the conditioning can be viewed as prediction learning

207
00:13:23,540 --> 00:13:26,080
and we already have

208
00:13:26,120 --> 00:13:30,710
an algorithm for how to do that given by reinforcement learning which is temporal difference

209
00:13:32,480 --> 00:13:37,390
now we have a neural implementation of that we have temporal difference areas in dopamine

210
00:13:37,410 --> 00:13:40,520
that affect learning in corticostriatal synapses

211
00:13:40,540 --> 00:13:46,480
in the basal ganglia

212
00:13:46,540 --> 00:13:50,940
this is very very very exciting for neuroscience because we want to understand what does

213
00:13:50,940 --> 00:13:55,000
it mean does not only do we have an answer here but we have

214
00:13:55,060 --> 00:14:00,520
a normative theory for why dopamine firing is the way is it's not only telling

215
00:14:00,520 --> 00:14:02,960
us that the prediction error tells us why

216
00:14:03,020 --> 00:14:07,150
what would we want this to be this way one prediction errors for optimal learning

217
00:14:07,150 --> 00:14:12,600
of values that predict future rewards so it's it's much more than just describing the

218
00:14:12,600 --> 00:14:13,730
function here

219
00:14:13,750 --> 00:14:15,940
and this is

220
00:14:15,960 --> 00:14:17,850
also an example of how

221
00:14:17,870 --> 00:14:21,190
once we have a computational model of learning that allows us to look in the

222
00:14:21,210 --> 00:14:25,120
brain for hidden variables that the model postulates OK

223
00:14:25,170 --> 00:14:29,850
prediction error is basically hidden variables you can see it people's behavior or in the

224
00:14:29,850 --> 00:14:33,270
inputs from the world it's something that the model

225
00:14:33,270 --> 00:14:37,640
suggested and here we found it in the brain and we'll see other examples

226
00:14:37,640 --> 00:14:40,330
and this was yale university press

227
00:14:40,350 --> 00:14:44,520
and these were three lectures he gave at yale university in nineteen thirty two

228
00:14:45,550 --> 00:14:49,220
the open world and if the subtitles says metaphysics

229
00:14:49,240 --> 00:14:50,530
of science

230
00:14:50,540 --> 00:14:52,920
you can see this is a book on philosophy

231
00:14:52,930 --> 00:14:54,940
of science and mathematics

232
00:14:54,950 --> 00:14:58,400
and this book is not as well known but i think it's an even better

233
00:14:58,400 --> 00:14:59,840
book in some ways

234
00:14:59,910 --> 00:15:04,530
there's a book of her violin nineteen forty nine which is much better known

235
00:15:04,640 --> 00:15:09,290
which was originally published in german i won't try to say in german but in

236
00:15:09,290 --> 00:15:11,050
english it was the philosophy

237
00:15:11,100 --> 00:15:16,960
of mathematics and natural science and it was published by princeton university press

238
00:15:16,980 --> 00:15:19,780
in nineteen forty nine

239
00:15:20,030 --> 00:15:24,210
that's a very well known book of his own philosophy but the open world is

240
00:15:24,210 --> 00:15:25,630
not as well known

241
00:15:25,710 --> 00:15:29,010
and in this book i discovered

242
00:15:29,020 --> 00:15:30,630
one day

243
00:15:30,660 --> 00:15:34,150
a very interesting idea

244
00:15:34,160 --> 00:15:37,090
which i'm going to write down to you now

245
00:15:37,120 --> 00:15:40,880
and i'm going to say it two ways of saying it

246
00:15:40,890 --> 00:15:42,550
the way

247
00:15:42,560 --> 00:15:47,160
hermann vials of say the way in your language

248
00:15:47,240 --> 00:15:51,260
the way from involves that it is

249
00:15:51,280 --> 00:15:53,760
if a rule

250
00:15:53,800 --> 00:15:57,790
can be talking about a scientific law is the law can be

251
00:15:57,840 --> 00:16:00,880
arbitrarily complicated

252
00:16:00,900 --> 00:16:04,880
arbitrarily complex

253
00:16:04,900 --> 00:16:06,820
then the

254
00:16:08,650 --> 00:16:10,580
of law

255
00:16:10,630 --> 00:16:12,390
becomes vacuous

256
00:16:12,410 --> 00:16:17,360
when all or

257
00:16:17,370 --> 00:16:21,290
because there's always along

258
00:16:22,200 --> 00:16:24,320
it is always

259
00:16:24,340 --> 00:16:25,870
the law

260
00:16:25,970 --> 00:16:29,710
now in your language the way to say this would be

261
00:16:29,770 --> 00:16:33,190
if a pattern can be arbitrarily complicated

262
00:16:33,230 --> 00:16:39,350
then the concept of pattern pattern becomes meaningless because there's always pattern

263
00:16:39,420 --> 00:16:43,710
that's the same idea

264
00:16:43,800 --> 00:16:48,520
and herman viola

265
00:16:48,540 --> 00:16:52,290
cool says that this idea comes from

266
00:16:52,310 --> 00:16:56,070
of lightness

267
00:16:56,080 --> 00:16:58,250
in sixteen eighty six

268
00:16:58,300 --> 00:17:01,170
from the the metaphysics

269
00:17:01,210 --> 00:17:03,020
discourse on metaphysics

270
00:17:03,040 --> 00:17:04,180
he scored

271
00:17:07,830 --> 00:17:12,530
and in fact if you go there you find it i think it's

272
00:17:12,550 --> 00:17:16,490
it's paragraphs five and six of the discourse the metaphysics

273
00:17:16,520 --> 00:17:17,700
there's a link

274
00:17:17,720 --> 00:17:22,270
from my home page from my website to the detect national france where you get

275
00:17:22,270 --> 00:17:23,900
this book for free

276
00:17:23,930 --> 00:17:25,260
in france

277
00:17:25,330 --> 00:17:28,400
i also have a link to an english translation

278
00:17:28,460 --> 00:17:31,470
the discrete metaphysics is a small

279
00:17:31,490 --> 00:17:34,970
outline that live in its wrote

280
00:17:35,060 --> 00:17:36,620
in sixteen eighty six

281
00:17:36,630 --> 00:17:38,400
he was consulting

282
00:17:38,790 --> 00:17:41,120
on how to pump water out of

283
00:17:41,220 --> 00:17:42,720
silver mines

284
00:17:42,760 --> 00:17:46,180
and there was a big snowstorm many had free moment so he wrote on some

285
00:17:46,180 --> 00:17:47,440
of his ideas

286
00:17:47,490 --> 00:17:48,540
and then he

287
00:17:48,550 --> 00:17:51,940
he sent a just a summary of the summary

288
00:17:52,010 --> 00:17:53,520
in the letter

289
00:17:53,540 --> 00:17:58,220
two a fugitive french philosophical are are no more nodes

290
00:17:58,270 --> 00:18:04,320
who was a fugitive because he was afraid that qatar's didn't like his ideas

291
00:18:04,340 --> 00:18:12,770
and and are no are node will only recommend this gentleman a are

292
00:18:12,780 --> 00:18:16,030
the inference i always say the grand are known

293
00:18:16,040 --> 00:18:21,470
he had a great reputation as of peoplesoft and arnold was so horrified that these

294
00:18:21,470 --> 00:18:24,690
ideas of like he said that they were heretical

295
00:18:24,770 --> 00:18:28,000
but like it's never

296
00:18:28,020 --> 00:18:33,960
never never sent arnold the the complete the the the complete works

297
00:18:33,970 --> 00:18:35,240
and it stayed in

298
00:18:35,260 --> 00:18:38,680
enlighten it's is among languages papers

299
00:18:38,690 --> 00:18:43,810
and was discovered i think it was something like eighteen fifty more than a century

300
00:18:43,810 --> 00:18:45,840
after alignment as deaths

301
00:18:45,880 --> 00:18:51,870
now like this is like this is personal papers were preserved by good fortune they

302
00:18:51,870 --> 00:18:56,230
were preserved much of what is best work is either in letters to people

303
00:18:56,290 --> 00:19:00,370
or is notes to himself when never seen by anyone else

304
00:19:00,400 --> 00:19:03,760
and the reason all these papers are fortunately preserved

305
00:19:03,770 --> 00:19:10,020
it is because there is the state library of hanover because like this is really

306
00:19:10,080 --> 00:19:16,020
a day job is my job was being an incredible mind doing philosophy mathematics all

307
00:19:16,020 --> 00:19:17,180
kinds of things

308
00:19:17,230 --> 00:19:20,160
but his day job was

309
00:19:20,210 --> 00:19:25,270
advisor to the duke of hanover another government responsibilities of that sort so his papers

310
00:19:25,270 --> 00:19:28,040
were most of his papers deals with affairs of state

311
00:19:28,050 --> 00:19:33,250
and there are preserved in the hanover state library but scattered among these papers dealing

312
00:19:33,250 --> 00:19:37,450
with affairs of state which no one cares about any more are masterpieces

313
00:19:37,580 --> 00:19:40,060
dealing with every conceivable subject

314
00:19:41,070 --> 00:19:44,530
among them was the disk with them at the end of his

315
00:19:44,540 --> 00:19:46,640
OK so

316
00:19:49,230 --> 00:19:50,220
let me

317
00:19:50,230 --> 00:19:53,490
try to explain this idea of a little

318
00:19:55,850 --> 00:19:57,440
and to do that

319
00:19:57,450 --> 00:19:59,310
what i wanna do

320
00:19:59,320 --> 00:20:01,990
it is

321
00:20:02,000 --> 00:20:05,740
give you a toy model of the following question so i think the key question

322
00:20:05,740 --> 00:20:08,250
is whether a pattern spurious real

323
00:20:08,420 --> 00:20:13,230
another way to put that is when i tell you how live said by the

324
00:20:13,800 --> 00:20:16,740
which is different from the way remember also that the way let me put it

325
00:20:16,770 --> 00:20:17,960
is like this

326
00:20:17,970 --> 00:20:22,200
what said is take a piece of paper and just sort of scatter spots at

327
00:20:22,230 --> 00:20:23,970
the close your eyes and

328
00:20:24,010 --> 00:20:26,110
throwing spots of at

329
00:20:27,180 --> 00:20:30,740
like i said no matter where these points are

330
00:20:30,780 --> 00:20:34,630
this finite set of points there will always be a mathematical equation passing through those

331
00:20:37,410 --> 00:20:41,740
the existence of a mathematical equation can not enable you to distinguish between a set

332
00:20:41,740 --> 00:20:45,020
of points that is lawless that was done at random by

333
00:20:45,070 --> 00:20:48,740
close your eyes and black ink against the piece of paper and a set of

334
00:20:48,740 --> 00:20:49,780
points that

335
00:20:49,830 --> 00:20:51,300
the following along

336
00:20:51,340 --> 00:20:55,260
so how can you tell the difference well what light it says if the equation

337
00:20:55,260 --> 00:20:57,390
is simple

338
00:20:57,420 --> 00:20:59,710
then then that's the law

339
00:20:59,710 --> 00:21:09,320
the search engine

340
00:21:09,390 --> 00:21:11,000
looking into some

341
00:21:11,030 --> 00:21:15,680
related questions because i want to show you how to process

342
00:21:15,720 --> 00:21:18,080
two types of information

343
00:21:18,670 --> 00:21:23,960
it is very important in many information retrieval and language processing tasks

344
00:21:24,040 --> 00:21:32,070
this is probabilistic language models and link graph

345
00:21:32,070 --> 00:21:33,820
and also i want to

346
00:21:33,830 --> 00:21:37,170
i apologize because

347
00:21:37,180 --> 00:21:43,200
i modified my presentation and it's a bit different from

348
00:21:43,210 --> 00:21:45,370
the version that you have in new

349
00:21:45,420 --> 00:21:47,810
text books

350
00:21:49,010 --> 00:21:53,100
beginning from language models before discussion what is

351
00:21:53,230 --> 00:21:55,920
link the language models and

352
00:21:55,980 --> 00:21:59,530
what we can do that i want to give you

353
00:21:59,540 --> 00:22:01,650
activation example

354
00:22:01,750 --> 00:22:04,150
imagine that again for all

355
00:22:04,170 --> 00:22:07,590
the search engine we decided to add

356
00:22:07,600 --> 00:22:10,870
search suggestions i'm talking about

357
00:22:11,590 --> 00:22:13,010
decided to

358
00:22:13,030 --> 00:22:17,700
make suggestions about to write a feeling of the query

359
00:22:18,420 --> 00:22:21,290
user submitted to the system

360
00:22:22,870 --> 00:22:24,010
our users

361
00:22:24,030 --> 00:22:25,860
i submitted this to work

362
00:22:25,870 --> 00:22:28,120
britain's the

363
00:22:28,140 --> 00:22:29,000
this is

364
00:22:29,040 --> 00:22:30,730
actually very common

365
00:22:31,950 --> 00:22:35,450
there is a page on one popular

366
00:22:35,480 --> 00:22:37,450
search engine

367
00:22:37,980 --> 00:22:41,060
about about misspellings in in the lab

368
00:22:41,540 --> 00:22:44,130
and on this page you can see

369
00:22:44,170 --> 00:22:46,330
it's usually stuff space so

370
00:22:46,380 --> 00:22:49,230
the name of this well known singers

371
00:22:51,170 --> 00:22:55,260
this is not the most three where may be number four

372
00:22:55,260 --> 00:22:57,520
number three

373
00:22:58,970 --> 00:23:03,070
if they are well educated programmers developers

374
00:23:03,100 --> 00:23:05,130
how we can select right

375
00:23:05,140 --> 00:23:08,160
variants of this work

376
00:23:08,230 --> 00:23:10,230
because we have

377
00:23:10,290 --> 00:23:11,700
good programmers

378
00:23:11,740 --> 00:23:14,950
you know that how we can calculate

379
00:23:14,990 --> 00:23:17,630
similarity between streams

380
00:23:17,640 --> 00:23:19,160
and usually

381
00:23:19,200 --> 00:23:21,390
we are using living

382
00:23:21,420 --> 00:23:23,800
distance this time distance

383
00:23:23,890 --> 00:23:25,350
american pronunciation

384
00:23:25,360 --> 00:23:31,570
basically the number of edit operations that hundreds

385
00:23:31,570 --> 00:23:34,380
one string to another string

386
00:23:35,240 --> 00:23:38,040
our misspelled the name of this

387
00:23:38,050 --> 00:23:41,630
famous goal

388
00:23:41,670 --> 00:23:44,440
it can be converted into

389
00:23:44,480 --> 00:23:46,670
alright spelled word

390
00:23:46,690 --> 00:23:49,100
the first is

391
00:23:49,130 --> 00:23:51,230
and in this case you need to

392
00:23:51,240 --> 00:23:53,070
edit operations

393
00:23:53,130 --> 00:23:54,670
it's all of

394
00:23:54,690 --> 00:23:57,130
for example we can remove y

395
00:23:57,170 --> 00:23:59,480
and i

396
00:24:02,140 --> 00:24:04,890
it can be can where we can convert

397
00:24:04,890 --> 00:24:07,730
also this to edit operations

398
00:24:11,010 --> 00:24:12,420
first name

399
00:24:14,290 --> 00:24:15,600
OK so

400
00:24:17,640 --> 00:24:20,110
talking about number of edit operations

401
00:24:20,140 --> 00:24:21,380
it doesn't help here

402
00:24:21,390 --> 00:24:24,290
they have the same result

403
00:24:24,300 --> 00:24:27,760
OK so in this case what we can do if you remember from all the

404
00:24:27,760 --> 00:24:30,290
second lecture we can get

405
00:24:30,320 --> 00:24:34,480
huge training set at all different misspellings

406
00:24:34,510 --> 00:24:39,920
and from these training set generate different rules how people

407
00:24:40,860 --> 00:24:41,720
how people

408
00:24:41,820 --> 00:24:44,010
this type of work

409
00:24:44,020 --> 00:24:50,690
and using this huge set calculate probability of every modification it's like

410
00:24:50,740 --> 00:24:53,750
eleven signed but with probability

411
00:24:53,800 --> 00:24:55,630
and i

412
00:24:55,680 --> 00:25:00,300
i one hundred percent sure that in this case

413
00:25:00,360 --> 00:25:02,550
when ification to breed

414
00:25:02,560 --> 00:25:05,940
it be much more probable

415
00:25:05,990 --> 00:25:07,690
simply because

416
00:25:07,730 --> 00:25:13,980
a one i is pretty common

417
00:25:14,140 --> 00:25:19,170
three ground in english language and a i and is also pretty common and based

418
00:25:19,170 --> 00:25:22,110
in the simplest case because just say well be right

419
00:25:22,160 --> 00:25:28,120
you know we can do like maximisation standard statistical inference procedures

420
00:25:28,140 --> 00:25:31,810
we write down the likelihood of the log likelihood function

421
00:25:32,120 --> 00:25:37,050
so it's just the sum over the counts and then so the observed frequencies and

422
00:25:37,100 --> 00:25:38,600
the log of

423
00:25:38,610 --> 00:25:44,190
the probability of war of the observed word given the document and that

424
00:25:44,200 --> 00:25:47,790
you know it's just this mixture something

425
00:25:47,880 --> 00:25:50,240
so we can try to maximize that

426
00:25:52,080 --> 00:25:56,040
of course the problem is with these you know with these types of mixture models

427
00:25:56,040 --> 00:25:59,330
you know you might have seen this before you know you get a non convex

428
00:25:59,330 --> 00:26:04,440
optimisation problem and you know that we have to resort to resort to

429
00:26:04,480 --> 00:26:09,140
approximations doing that OK and so on

430
00:26:09,210 --> 00:26:10,380
i will

431
00:26:11,940 --> 00:26:13,620
go over the

432
00:26:15,000 --> 00:26:20,370
some of the details here the argument that's used again we will use the expectation

433
00:26:20,370 --> 00:26:26,310
maximisation algorithm so familiar with that you just take a moment write the model OK

434
00:26:26,310 --> 00:26:30,050
but i would do this here even if you are familiar with it might be

435
00:26:30,050 --> 00:26:31,060
illuminating to see

436
00:26:31,660 --> 00:26:37,180
what the solution really is so in the expectation maximisation algorithm

437
00:26:37,840 --> 00:26:42,660
we are alternate to step one step the so-called e step where we compute the

438
00:26:42,660 --> 00:26:44,680
posterior probability

439
00:26:44,680 --> 00:26:45,700
of the

440
00:26:45,710 --> 00:26:50,500
later the unobserved variables OK what are the unobserved variables here we're basically what our

441
00:26:50,500 --> 00:26:55,570
model assumes is that every occurrence of the term in the document is associated with

442
00:26:55,570 --> 00:27:00,120
a particular concept think of it as the hidden cause you know why is that

443
00:27:00,120 --> 00:27:04,880
term occurring in the document well because the document is with a particular concept OK

444
00:27:05,800 --> 00:27:09,630
so the question is if we see w in the context of the document d

445
00:27:09,630 --> 00:27:13,930
what's the probability that that is somehow has to do with the particular concept c

446
00:27:14,430 --> 00:27:19,390
and well proportional to the probability that the document deals with that concept and the

447
00:27:19,390 --> 00:27:26,020
probability times the probability that the word w expresses the concept can then just smaller

448
00:27:27,120 --> 00:27:31,460
and then these posterior probabilities are used to

449
00:27:31,520 --> 00:27:36,450
we estimate the parameters OK so you you have estimates of the parameters on the

450
00:27:36,450 --> 00:27:40,470
right-hand side to compute the posterior probabilities and those go to the right-hand side and

451
00:27:40,470 --> 00:27:45,350
you compute new estimates for your parameters and the way you do that is also

452
00:27:45,350 --> 00:27:46,490
very trivial

453
00:27:46,510 --> 00:27:50,810
you know for instance here right what's the probability that the word expresses the concept

454
00:27:50,810 --> 00:27:54,660
well you look at all occurrences of the word and then you look at the

455
00:27:54,660 --> 00:27:59,290
posterior probability for each occurrence of the word whether it expressed particular concept or not

456
00:27:59,300 --> 00:28:00,560
just that this

457
00:28:00,580 --> 00:28:01,850
and then just knowledge

458
00:28:01,940 --> 00:28:08,410
right so these equations are really trivial right so you know there's basically nothing else

459
00:28:08,410 --> 00:28:11,680
but how often is term associated with the concept

460
00:28:11,690 --> 00:28:13,840
i mean of course these probabilities

461
00:28:13,850 --> 00:28:18,400
how often might not be a cold but rather some action number and on the

462
00:28:18,400 --> 00:28:22,560
right-hand side we have you know how often is document associated

463
00:28:22,600 --> 00:28:26,180
and now we can just you know reiterated two

464
00:28:26,210 --> 00:28:30,740
and you know that's what the EMI with which do

465
00:28:30,770 --> 00:28:35,890
so for those of you who are not familiar with the model with i also

466
00:28:35,890 --> 00:28:37,010
have a little

467
00:28:39,520 --> 00:28:41,880
on on how more formally

468
00:28:41,920 --> 00:28:47,330
you could actually write design OK and then we show you

469
00:28:47,780 --> 00:28:52,800
just the flavour of how you how you typically do this so

470
00:28:53,290 --> 00:28:58,650
you know our log likelihood function is some some overall observations and here actually i

471
00:28:58,720 --> 00:29:03,690
assume that have a document word occurrence and i just have an index are

472
00:29:03,730 --> 00:29:07,430
OK just has to do to be able to deal with multiplicities of the same

473
00:29:07,430 --> 00:29:11,850
term occurring in the document can we just number them by OK every occurrence of

474
00:29:11,850 --> 00:29:13,780
a term and the numbers are

475
00:29:13,890 --> 00:29:16,340
and what we can do actually

476
00:29:16,360 --> 00:29:23,760
it is to come up with q permit lower bound on the log likelihood in

477
00:29:23,760 --> 00:29:24,810
which in the following way

478
00:29:26,570 --> 00:29:27,340
where we

479
00:29:27,360 --> 00:29:31,730
you know this q are fuzzy or q is is not probability distribution over the

480
00:29:31,730 --> 00:29:35,160
concepts and for every

481
00:29:36,440 --> 00:29:40,310
you know for every observation we can make a choice for that distribution

482
00:29:41,530 --> 00:29:45,030
and then we will we will actually get lower bound on the log likelihood now

483
00:29:45,030 --> 00:29:47,640
why sisters-in-law bound on the log likelihood

484
00:29:50,520 --> 00:29:52,520
easily follows from

485
00:29:52,530 --> 00:29:54,510
jensen's inequality

486
00:29:56,070 --> 00:29:58,900
you know basically the log likelihood function

487
00:29:58,900 --> 00:30:02,530
signal as we would have gotten if we whiten the data

488
00:30:02,550 --> 00:30:06,440
but without spending the time that are required to whiten the data

489
00:30:06,440 --> 00:30:08,900
so all the hidden units are going to see

490
00:30:09,010 --> 00:30:11,180
is the underlying data

491
00:30:11,240 --> 00:30:13,440
but the learning signal connections

492
00:30:13,510 --> 00:30:15,400
is going to be the same as they would have got

493
00:30:15,470 --> 00:30:19,780
if we if we want we stopped one the data before we did this

494
00:30:19,800 --> 00:30:22,450
so how does that work well

495
00:30:22,510 --> 00:30:29,030
imagine pixel that can be correctly reconstructed by these pairwise interactions

496
00:30:29,070 --> 00:30:31,450
we go out we come down again

497
00:30:31,470 --> 00:30:34,760
it's correctly reconstructed anyway by the pairwise interactions

498
00:30:34,780 --> 00:30:38,670
so for example it doesn't need any top-down input

499
00:30:38,720 --> 00:30:42,470
OK we get the right anyway it's the same in the data reconstruction so the

500
00:30:42,510 --> 00:30:44,880
learning signal for it

501
00:30:44,900 --> 00:30:48,280
but if the pairwise interactions get it wrong

502
00:30:48,340 --> 00:30:52,240
then we get a big thing as well as an edge

503
00:30:52,280 --> 00:30:54,590
so the learning signal you get from CD

504
00:30:54,610 --> 00:30:57,550
it is like the learning signal you get away data

505
00:30:57,550 --> 00:30:59,280
you don't have to wait to do the right thing

506
00:30:59,300 --> 00:31:04,260
or rather the time it takes you to to reconstruct is sort of the time

507
00:31:04,260 --> 00:31:07,470
it takes to do the two would take you to the widening

508
00:31:07,510 --> 00:31:11,590
but your inferences going straight through already you're ready inferring things in higher layers while

509
00:31:11,590 --> 00:31:13,740
doing that reconstruction

510
00:31:13,760 --> 00:31:17,880
and the point of the reconstruction is not to get a better model of the

511
00:31:18,670 --> 00:31:21,070
it is to get the worst model of the world

512
00:31:21,090 --> 00:31:23,610
they give you better learning signal

513
00:31:24,920 --> 00:31:28,860
that's just a theory

514
00:31:31,650 --> 00:31:35,090
so so far what we've been doing is just

515
00:31:35,180 --> 00:31:37,820
learning to

516
00:31:37,860 --> 00:31:39,320
change the

517
00:31:39,320 --> 00:31:41,900
the top down input to hidden units

518
00:31:41,920 --> 00:31:46,760
to visible units but not actually modulate the national connections we assume that fixed

519
00:31:48,940 --> 00:31:51,550
what would really like to do is be able to

520
00:31:51,570 --> 00:31:55,920
what left connections so i can tell soldiers to kind of stand to arms length

521
00:31:55,920 --> 00:31:57,570
apart when i can

522
00:31:57,630 --> 00:31:59,440
obviously generate one like that

523
00:31:59,450 --> 00:32:02,700
we don't want to fix tomorrow i would like to be able to much more

524
00:32:02,720 --> 00:32:06,670
and in fact the the general principle here

525
00:32:06,670 --> 00:32:09,840
which is if you want to build the hierarchical systems

526
00:32:09,880 --> 00:32:13,280
things one level can tell things that have low exactly what they ought to be

527
00:32:16,510 --> 00:32:18,970
they can tell things at the level below

528
00:32:18,990 --> 00:32:21,650
well they ought to be trying to achieve in some sense

529
00:32:21,670 --> 00:32:25,590
that's a much better way to do abstraction a big management hierarchy the guys at

530
00:32:25,590 --> 00:32:29,470
the top level shouldn't be telling you

531
00:32:29,510 --> 00:32:32,050
you know where to put your hand on the desk so you can we should

532
00:32:32,050 --> 00:32:35,130
quickly so you can fill in the next fought this should be telling you that

533
00:32:35,130 --> 00:32:39,320
they should be paying you of fulfilling all forms quickly

534
00:32:39,340 --> 00:32:41,550
and then you figure out how to do that

535
00:32:41,700 --> 00:32:44,170
well i think it's the same in more enormous any hierarchical system you don't want

536
00:32:44,170 --> 00:32:45,200
to micro-manage

537
00:32:45,490 --> 00:32:49,740
and so what you want is wonderful should create the objective function for the level

538
00:32:50,340 --> 00:32:53,860
and then the level of to the optimisation

539
00:32:53,880 --> 00:32:55,220
so for example

540
00:32:55,220 --> 00:32:59,530
if you're a faculty member you should explain to your students what the true objective

541
00:32:59,530 --> 00:33:03,620
function is that is the number of papers with my name on really counts and

542
00:33:03,620 --> 00:33:06,860
then the graduate students optimise the very good luck

543
00:33:12,200 --> 00:33:15,050
so long time ago terrace and ask you pointed out that

544
00:33:15,070 --> 00:33:18,920
this is just a sort of quadratic energy function there's no reason we should make

545
00:33:18,920 --> 00:33:23,490
it cubic i think you can probably generalize beyond that

546
00:33:23,530 --> 00:33:24,800
this will be good enough for us

547
00:33:24,820 --> 00:33:30,550
and the learning algorithm will go through you just have to measure the sufficient statistics

548
00:33:30,550 --> 00:33:32,590
and i triples of

549
00:33:33,630 --> 00:33:36,820
so i can think of unique as acting as a switch

550
00:33:36,820 --> 00:33:40,090
so if k was the hidden units and this was too visible units when k

551
00:33:40,090 --> 00:33:41,260
is all

552
00:33:41,300 --> 00:33:44,510
we have the weight w i j k between these two visible units in the

553
00:33:44,510 --> 00:33:46,280
case of we don't

554
00:33:46,280 --> 00:33:49,440
OK so k can switch in this way

555
00:33:53,630 --> 00:33:57,530
and you can actually view it causes symmetric you can view i switch on wide-ranging

556
00:33:57,530 --> 00:34:01,180
k two

557
00:34:01,200 --> 00:34:06,780
so you could use higher levels machines like that for example modelling image transformations

558
00:34:08,880 --> 00:34:11,050
is this pixel goes there

559
00:34:11,090 --> 00:34:12,760
and this pixel goes there

560
00:34:12,780 --> 00:34:14,760
that's the consistent translation

561
00:34:14,780 --> 00:34:19,720
so we could have if we had units represented translations this screen unit could say

562
00:34:19,740 --> 00:34:22,450
i've got a big weight here and i got a big weight here

563
00:34:22,470 --> 00:34:24,650
so when i see this pair

564
00:34:24,650 --> 00:34:26,320
both of them

565
00:34:26,340 --> 00:34:30,610
this pair times out weight causes me to get lots of input

566
00:34:30,630 --> 00:34:33,880
and this pattern that would cause me to get more input with several of these

567
00:34:33,880 --> 00:34:37,470
overcome i bias my negative bias and i turn on saying i believe in this

568
00:34:39,240 --> 00:34:42,280
OK so that's how you make some the detector translation

569
00:34:42,340 --> 00:34:44,900
it can also generate translation

570
00:34:44,950 --> 00:34:48,670
by you give this and you get this little is that

571
00:34:48,760 --> 00:34:51,900
OK and we can learn using the

572
00:34:51,950 --> 00:34:53,300
CD learning

573
00:34:54,030 --> 00:34:55,970
but the problem with this is

574
00:34:55,990 --> 00:35:00,910
we've got lots and lots of these parameters because that we have one parameter for

575
00:35:00,910 --> 00:35:01,970
each pair

576
00:35:01,990 --> 00:35:07,660
of these times the number of hidden units we

577
00:35:07,680 --> 00:35:11,180
so more recently we tried factorizing

578
00:35:11,200 --> 00:35:14,820
this is basic argument is if you think about translations of images

579
00:35:14,830 --> 00:35:21,400
you don't really need enough degrees of freedom to represent arbitrary permutations translations on my

580
00:35:23,160 --> 00:35:28,490
so let's suppose we need less parameters maybe we can factorize useless parameters

581
00:35:28,510 --> 00:35:31,290
so we'll factorized this

582
00:35:31,290 --> 00:35:32,490
in this way

583
00:35:32,500 --> 00:35:35,210
it looks was initially we're going to say we can model this

584
00:35:35,220 --> 00:35:39,370
this this is the weight tends around american minimum attendance

585
00:35:39,370 --> 00:35:41,000
as the sum

586
00:35:41,050 --> 00:35:43,010
a whole bunch of factors

587
00:35:43,030 --> 00:35:49,250
in practice and each factor is a three way outer product of one dimensional vectors

588
00:35:50,860 --> 00:35:53,740
and when you add all these up

589
00:35:53,750 --> 00:35:57,000
if you add up enough of them you can model any turns

590
00:35:57,000 --> 00:36:00,180
but if you have less of them you can model tensors that have nice regular

591
00:36:00,180 --> 00:36:02,830
structure and that's what we want

592
00:36:02,840 --> 00:36:09,050
so it is these guys only have two indices so we only have quadratically many

593
00:36:09,050 --> 00:36:15,740
of these and three times quadratic is much better than cubic

594
00:36:15,750 --> 00:36:20,340
you can think of his my three way to protect the product of the WIF

595
00:36:20,500 --> 00:36:23,660
the wj from the whf and that me tensor

596
00:36:23,680 --> 00:36:26,070
that's a rank one tensor

597
00:36:26,080 --> 00:36:29,900
but one add up enough i can get tens of any running

598
00:36:29,900 --> 00:36:32,680
and we can think of

599
00:36:32,700 --> 00:36:34,920
one of these guys the hidden ones

600
00:36:34,950 --> 00:36:38,330
as specifying the covariance matrix

601
00:36:38,340 --> 00:36:42,540
sorry an inverse covariance matrix here

602
00:36:42,550 --> 00:36:46,400
this is the energy function the pairwise energy function between the eyes and the joes

603
00:36:46,460 --> 00:36:48,400
the inverse covariance matrix

604
00:36:48,400 --> 00:36:50,700
so by taking

605
00:36:51,370 --> 00:36:56,710
which combinations of hidden units we can synthesize inverse covariance

606
00:36:56,710 --> 00:36:58,110
now i'm

607
00:36:58,120 --> 00:37:01,800
now the camera is moving off the cycle can be moving in between

608
00:37:01,850 --> 00:37:05,560
the two cameras and sticky encyclopedia if they use

609
00:37:05,610 --> 00:37:07,930
and when you're just like in the other guys

610
00:37:08,310 --> 00:37:11,020
looking straight at you

611
00:37:11,020 --> 00:37:13,010
a renowned computer scientist from

612
00:37:13,090 --> 00:37:14,360
a heinrich

613
00:37:14,410 --> 00:37:17,100
and it's nice when he looks at using this

614
00:37:17,100 --> 00:37:21,490
OK so one application for this in teleconferencing supposing you got you know

615
00:37:21,510 --> 00:37:25,370
super laptop with you you can get now with the camera very

616
00:37:25,390 --> 00:37:26,610
and you are

617
00:37:26,610 --> 00:37:31,780
teleconferencing girlfriend has the same that top pretty good except an annoying thing is that

618
00:37:31,780 --> 00:37:32,840
you don't quite

619
00:37:32,970 --> 00:37:36,700
he doesn't look at you you don't look at it's not being rude it's that

620
00:37:36,700 --> 00:37:37,900
it is looking at the

621
00:37:37,910 --> 00:37:41,260
you know picture viewing the screen you can't looking at the camera competent both

622
00:37:41,260 --> 00:37:43,690
and you know short drilling a hole

623
00:37:43,800 --> 00:37:48,290
in the middle of your expensive black top and poking camera through

624
00:37:48,600 --> 00:37:52,190
you know what we can do about this well here is one possible technology

625
00:37:52,220 --> 00:37:54,360
forward which is that you mount the camera

626
00:37:54,360 --> 00:37:57,790
on the two sides of the laptop or maybe at the top and the bottom

627
00:37:57,960 --> 00:38:01,480
usually shorter and now synthesized

628
00:38:01,570 --> 00:38:03,460
the view in between

629
00:38:03,480 --> 00:38:04,580
and actually

630
00:38:04,580 --> 00:38:07,080
we can simply cite the cycle continues that would be

631
00:38:07,090 --> 00:38:09,980
you know in directly in between

632
00:38:10,070 --> 00:38:12,630
but we can also synthesize

633
00:38:13,690 --> 00:38:15,320
you know if you anywhere

634
00:38:15,370 --> 00:38:18,850
along the line joining the cameras that would be good if you want to you

635
00:38:18,850 --> 00:38:20,540
know you've got a window

636
00:38:20,580 --> 00:38:25,230
based computing system you move the window around that has the person talking in the

637
00:38:25,290 --> 00:38:30,340
more general but can do whatever the window not

638
00:38:30,350 --> 00:38:31,100
OK so

639
00:38:31,110 --> 00:38:35,060
just a few minutes more on stereo matching problem then will be nice btw tomorrow

640
00:38:35,060 --> 00:38:39,450
or methods for solving the optimisation problem

641
00:38:40,180 --> 00:38:42,020
so so far we talked about

642
00:38:42,090 --> 00:38:45,530
using dynamic programming to solve this problem

643
00:38:45,540 --> 00:38:47,810
along with two epipolar lines

644
00:38:47,830 --> 00:38:51,630
and you know that sounds quite good except the result you get when you do

645
00:38:53,260 --> 00:38:58,840
not always what you would wish also here is the disparity map derived from this

646
00:38:58,840 --> 00:39:00,590
pair of images of

647
00:39:00,830 --> 00:39:02,280
the pentagon

648
00:39:02,300 --> 00:39:04,590
you know the pentagonal

649
00:39:04,640 --> 00:39:06,740
this is a a very popular in the

650
00:39:06,780 --> 00:39:08,970
very for some reason

651
00:39:08,970 --> 00:39:11,280
i think because they paid for the work

652
00:39:11,680 --> 00:39:17,180
and you see that there is a sort of quite frequently artifacts were all aligned

653
00:39:19,950 --> 00:39:24,570
you know something failed minutes it work most of the time because most you see

654
00:39:25,320 --> 00:39:26,890
looking reasonably

655
00:39:26,910 --> 00:39:31,820
plausible but every now and again this the castro's catastrophic failure the entire

656
00:39:31,910 --> 00:39:35,050
a small part in this place backwards

657
00:39:35,090 --> 00:39:39,360
and and so now that the whole the whole thing is

658
00:39:39,470 --> 00:39:40,390
the ship

659
00:39:40,430 --> 00:39:44,700
and it will not be very difficult to stop this happening only prior

660
00:39:44,740 --> 00:39:48,260
on good disparities within were not restricted to

661
00:39:48,280 --> 00:39:52,800
what happens on line but also to counter the neighbouring lines

662
00:39:52,860 --> 00:39:57,450
and so we to do something about that and you know this point we might

663
00:39:57,530 --> 00:40:02,970
you start to get very creative about inventing priors the work in two dimensions but

664
00:40:02,970 --> 00:40:07,570
you should feel bad about that because what's going to happen to the programme

665
00:40:07,590 --> 00:40:11,300
yes it's going to be totally missed messed up because once you get two dimensions

666
00:40:11,300 --> 00:40:14,240
there is no natural ordering of the data from the dynamic programming is not going

667
00:40:14,240 --> 00:40:19,720
to work and because we might try some sickening hybrids where you kind of moving

668
00:40:19,720 --> 00:40:23,550
raster scan order down the image solving with no programming is you go and you

669
00:40:23,550 --> 00:40:28,360
get a least half the constraints because you take the line we already solved and

670
00:40:29,220 --> 00:40:33,820
use the line you take it as if it was correct problems still being inferred

671
00:40:33,820 --> 00:40:39,110
and then use that inference rigid constraints the next line of work they might do

672
00:40:39,110 --> 00:40:42,910
something a bit but you know you're for professional you're not going to put up

673
00:40:42,910 --> 00:40:44,410
with that kind of thing

674
00:40:44,470 --> 00:40:46,910
all right

675
00:40:46,930 --> 00:40:49,860
not even when it nearly in

676
00:40:50,610 --> 00:40:51,390
you know

677
00:40:51,390 --> 00:40:53,470
we really would like to have some

678
00:40:53,490 --> 00:40:58,360
the constraints between neighboring epipolar lines and you know is what the constraint what to

679
00:40:58,360 --> 00:41:03,870
you have

680
00:42:20,270 --> 00:42:22,120
in fact

681
00:44:04,430 --> 00:44:08,360
and that is a problem

682
00:44:26,740 --> 00:44:32,180
are more

683
00:44:32,200 --> 00:44:39,790
in fact

684
00:44:52,800 --> 00:44:54,970
each of

685
00:45:52,320 --> 00:45:56,190
there he

686
00:46:09,780 --> 00:46:14,410
he is

687
00:46:14,430 --> 00:46:19,410
you can

688
00:46:25,610 --> 00:46:30,590
they are as

689
00:46:30,890 --> 00:46:35,930
four years later

690
00:46:35,960 --> 00:46:37,640
it is

691
00:47:05,660 --> 00:47:10,170
well in that case for

692
00:47:10,190 --> 00:47:12,760
it inverts

693
00:47:34,510 --> 00:47:39,420
what we do

694
00:47:44,580 --> 00:47:47,300
all the

695
00:47:51,820 --> 00:47:59,030
that means that if

696
00:47:59,030 --> 00:48:00,570
but when you constructed back

697
00:48:01,180 --> 00:48:03,810
u you don't suffering reconstruction sorry

698
00:48:05,950 --> 00:48:10,340
right obviously if you're reconstruction error is zero then you have a perfect in color right

699
00:48:10,970 --> 00:48:15,100
and in particular if the number of hidden layers the number of hidden features here

700
00:48:15,570 --> 00:48:17,800
is much smaller than the number of inputs

701
00:48:18,120 --> 00:48:20,030
dimensions and you have perfect reconstruction

702
00:48:20,690 --> 00:48:23,970
and you have perfect compression of the data right you can also view

703
00:48:24,470 --> 00:48:26,640
these kinds of models trying to compress the data

704
00:48:29,520 --> 00:48:32,350
now the interesting thing about these models is that's

705
00:48:33,080 --> 00:48:34,020
if the hidden

706
00:48:35,160 --> 00:48:36,900
as well as the output layer lean years

707
00:48:39,780 --> 00:48:42,650
and you're trying to minimize we reconstruction here

708
00:48:43,230 --> 00:48:45,620
then it turns out that you can recover piece you

709
00:48:46,490 --> 00:48:48,470
you can recover principal component analysis

710
00:48:49,260 --> 00:48:55,010
so if you look at these key hidden units they will span the same space as the first principal components

711
00:48:55,560 --> 00:48:59,340
so it's kinda interesting if you basically make this model bilinear when you recurrent piece

712
00:48:59,350 --> 00:49:03,350
you many of you probably heard about you see it's like a one of these

713
00:49:03,400 --> 00:49:05,530
heaviest use tools

714
00:49:06,080 --> 00:49:06,930
a fourth

715
00:49:07,320 --> 00:49:10,520
you know across many many different domains data compression there are

716
00:49:13,680 --> 00:49:19,550
so you can view autoencoders is non linear extension of see which can be much more powerful than piece you

717
00:49:21,620 --> 00:49:26,830
if your data is binary then obviously you can also is a decoder in decoder

718
00:49:26,830 --> 00:49:29,400
stage in your also uses a nonlinear

719
00:49:30,380 --> 00:49:31,160
decoding function

720
00:49:31,990 --> 00:49:33,130
in this case sigmoid function

721
00:49:34,250 --> 00:49:36,070
you know this is has become very

722
00:49:36,790 --> 00:49:42,570
also successfully used models and it actually relates to something that's called restricted boltzmann machines

723
00:49:45,270 --> 00:49:47,710
relates to both machines and graphical models

724
00:49:50,050 --> 00:49:55,400
so one of these key components in a lot of these models is that you wanna get sparse representations

725
00:49:57,320 --> 00:49:59,140
that turns out to be you

726
00:49:59,470 --> 00:50:03,770
a very useful thing and empirically people found that's you know if you wanna get

727
00:50:03,770 --> 00:50:06,830
good features u you wanna put some kind of sparsity on on

728
00:50:07,510 --> 00:50:11,880
on the latent representations and here you if even if you're trying to discover binary

729
00:50:11,880 --> 00:50:13,680
features you'd like them to be sparse

730
00:50:15,780 --> 00:50:16,560
and that's exactly

731
00:50:17,400 --> 00:50:18,880
sort of the model that was done by

732
00:50:19,330 --> 00:50:21,710
and why you're group of a few years ago

733
00:50:22,550 --> 00:50:27,280
we're again trying to minimize every decoder a function that attempting to minimize the reconstruction

734
00:50:27,280 --> 00:50:31,180
error so this is this is the reconstruction error

735
00:50:31,590 --> 00:50:35,290
this is the penalty on these latent features you want these features to be sparse

736
00:50:35,290 --> 00:50:37,510
so you put one penalty it's a common thing to do

737
00:50:38,080 --> 00:50:39,540
and this has been courting function

738
00:50:41,380 --> 00:50:46,780
it turns out that's been coding function is also a very useful to students to have in particular test time

739
00:50:47,920 --> 00:50:49,020
it's a very useful

740
00:50:50,420 --> 00:50:52,730
an explicit encoding function because the test time

741
00:50:53,270 --> 00:50:56,020
when i show you a new image show you a new web page

742
00:50:56,460 --> 00:51:00,120
you'd like to get the latent representations very quickly right

743
00:51:02,380 --> 00:51:05,640
and an explicit according including function allows you to do that

744
00:51:07,090 --> 00:51:11,400
now one of the things that people realized you know five or six years ago

745
00:51:11,400 --> 00:51:13,330
is that well he can actually stack these things

746
00:51:14,690 --> 00:51:15,490
so you can learn

747
00:51:18,160 --> 00:51:18,320
you know

748
00:51:18,960 --> 00:51:19,800
first layer features

749
00:51:20,590 --> 00:51:25,150
then you can repeat the operation can say well can actually take these latent features and then call them again

750
00:51:25,880 --> 00:51:30,900
and so forth and maybe at the top level i'm gonna have my classification model

751
00:51:31,100 --> 00:51:32,970
or whatever you want use them fore

752
00:51:34,100 --> 00:51:38,200
come now when you look at the stacked autoencoders you know you might ask why

753
00:51:38,490 --> 00:51:41,330
people have that salting caldas existed in the last twenty years

754
00:51:41,770 --> 00:51:42,850
but only five

755
00:51:43,310 --> 00:51:46,060
four five years ago people figured out that you can actually stack them

756
00:51:46,830 --> 00:51:49,950
the question is why people haven't thought of this thing before r

757
00:51:50,430 --> 00:51:51,190
this a natural thing

758
00:51:51,640 --> 00:51:53,700
um why people have tried doing that's

759
00:51:54,850 --> 00:51:57,910
and it turns out that you know if you look at the stacking here it's

760
00:51:57,910 --> 00:52:02,280
not obvious whether it's a hack or whether there is some deep mathematical insight

761
00:52:03,470 --> 00:52:04,600
behind these kinds of models

762
00:52:05,230 --> 00:52:06,210
and i'm gonna show you'll

763
00:52:08,260 --> 00:52:10,690
that's taking makes sense in terms of actually

764
00:52:11,490 --> 00:52:13,700
optimizing a certain kind of objective function

765
00:52:14,190 --> 00:52:15,480
so it's not it's not a hack

766
00:52:17,200 --> 00:52:20,770
this is known as a greedy layerwise pre-training or greedy layerwise

767
00:52:21,230 --> 00:52:22,690
learning of of the features

768
00:52:23,770 --> 00:52:27,440
now at the test time you can remove the decoder if you're just interested in

769
00:52:27,580 --> 00:52:31,450
recognizing what's going on in images as well speech signal web pages

770
00:52:32,010 --> 00:52:33,170
and just in code

771
00:52:33,760 --> 00:52:37,320
given an input tax you just go through multiple layers of representations and then u

772
00:52:37,950 --> 00:52:40,010
you can use it for four for classifying things

773
00:52:42,550 --> 00:52:43,100
in general

774
00:52:47,420 --> 00:52:52,830
you know you can use standard am in colder and that's sort of equivalent to

775
00:52:52,830 --> 00:52:56,660
a neural network model a neural network architecture can use convolutional which is

776
00:52:56,660 --> 00:52:57,850
of this line

777
00:52:59,410 --> 00:53:00,660
OK so

778
00:53:00,730 --> 00:53:04,890
we all know what a galaxy distribution is a you know what accounting processes probably

779
00:53:04,910 --> 00:53:09,890
right so that's how you go from the finite dimensional world international world for function

780
00:53:09,940 --> 00:53:13,560
the you do that you just say from many international world of functions i take

781
00:53:13,560 --> 00:53:15,060
a finite number of points

782
00:53:15,060 --> 00:53:17,560
and i asked them to have a girls in distribution

783
00:53:17,600 --> 00:53:19,850
i could never find a collection of points

784
00:53:19,850 --> 00:53:21,960
five by core theorems

785
00:53:21,960 --> 00:53:25,060
i put a measure on function

786
00:53:25,140 --> 00:53:27,160
public health process

787
00:53:27,160 --> 00:53:30,960
we can do that same procedure all kinds of other distribution of the gaseous like

788
00:53:30,960 --> 00:53:34,540
and so the dirichlet distribution is financial for distribution and i can ask to put

789
00:53:34,540 --> 00:53:38,750
a distribution in different order of objects by taking finite dimensional

790
00:53:38,890 --> 00:53:41,480
sample over here as it

791
00:53:41,660 --> 00:53:44,140
consistency something else in case

792
00:53:44,160 --> 00:53:48,410
i have to have these points in the single so measure five overlapping points to

793
00:53:48,410 --> 00:53:50,890
the same destination of points

794
00:53:51,770 --> 00:53:56,020
well you it is richly distribution also get an called the dirichlet process so the

795
00:53:56,020 --> 00:53:59,430
way that works is that i think some underlying space called that omega

796
00:53:59,430 --> 00:54:02,480
and i thought now into a finer partition

797
00:54:02,500 --> 00:54:06,120
and again points now getting cell of the partition

798
00:54:06,140 --> 00:54:10,670
and for each of those cells i assign a measure that cells not is the

799
00:54:10,670 --> 00:54:13,060
underlying measure on this space

800
00:54:13,080 --> 00:54:17,620
and not a one is the measure signed a one hundred not and so on

801
00:54:17,790 --> 00:54:24,390
to cells so do not have a at the wonder measure citing these cells

802
00:54:24,430 --> 00:54:28,890
and if i plug in i get scaling factor caused directly probabilities can you are

803
00:54:28,940 --> 00:54:31,620
parameters can be positive and from the one

804
00:54:31,670 --> 00:54:35,580
then i can plug these numbers into a dirichlet distribution

805
00:54:35,660 --> 00:54:39,790
and i give myself a random sample of numbers that sum to one

806
00:54:39,830 --> 00:54:42,640
if i do that again i'll get another set of numbers that sum to one

807
00:54:42,640 --> 00:54:46,170
of these is the underlying parameters for these fixed partition

808
00:54:46,410 --> 00:54:50,460
like in the gas in case i think the point i have a alpha point

809
00:54:50,480 --> 00:54:53,730
now i can do that for all partitions

810
00:54:53,750 --> 00:54:57,600
and i get consistency i use the same cell here multiple partitions would get the

811
00:54:57,600 --> 00:54:59,770
same problem find itself

812
00:54:59,790 --> 00:55:04,190
and you do that by the dirichlet distribution by aggregating cells in the dirichlet distribution

813
00:55:04,250 --> 00:55:08,710
i get pulled directly probably which is the sum of the hyperparameters for those cell

814
00:55:08,730 --> 00:55:09,710
the work

815
00:55:09,710 --> 00:55:11,810
so you get the commodore vaccine holding

816
00:55:11,830 --> 00:55:16,620
and therefore there must exist an underlying process on on what now

817
00:55:16,640 --> 00:55:18,850
well on measure

818
00:55:18,850 --> 00:55:20,660
on measures

819
00:55:20,690 --> 00:55:24,060
functions in more but i measure because the measures

820
00:55:24,060 --> 00:55:29,460
OK so that hopefully give you a flavour of the of the definition and the

821
00:55:29,460 --> 00:55:34,080
fascinating fact about this is that is now a distribution on measure what sample i

822
00:55:34,080 --> 00:55:35,100
get a measure

823
00:55:35,100 --> 00:55:37,960
that measure turned out to be an infinite sum of atoms

824
00:55:37,980 --> 00:55:41,250
it's the screen probably one no matter what not

825
00:55:41,270 --> 00:55:43,540
give you a little flavor that's interesting

826
00:55:43,560 --> 00:55:47,580
a little flavor why that occurs think about the poster process if i sample t

827
00:55:47,640 --> 00:55:51,140
from a dirichlet process i got one of these measures and then sample theta from

828
00:55:51,850 --> 00:55:54,370
what the poster prof

829
00:55:54,390 --> 00:55:57,830
well for a fixed picture partition

830
00:55:57,850 --> 00:56:00,460
one of the data fell in one of the cell

831
00:56:00,480 --> 00:56:04,810
so by the usual the risley updates on on the financial world

832
00:56:04,830 --> 00:56:07,390
the exponent of that

833
00:56:07,410 --> 00:56:10,330
cell one by one and all the other thing

834
00:56:10,440 --> 00:56:13,500
the usual dirichlet multinomial update

835
00:56:13,500 --> 00:56:18,710
but that must be true for any partition therefore by for myself to be really

836
00:56:18,710 --> 00:56:20,890
really small around that data

837
00:56:21,020 --> 00:56:24,910
always been export going by one more health small that sellers

838
00:56:25,000 --> 00:56:28,660
would suggest adam nelson there that data

839
00:56:28,660 --> 00:56:33,750
so that the posterior process of the same gene up extra data

840
00:56:33,790 --> 00:56:35,460
now back to doing this

841
00:56:35,480 --> 00:56:37,810
eventually reveal all of the

842
00:56:37,830 --> 00:56:41,040
and while the final for infinite sum of atoms

843
00:56:41,060 --> 00:56:45,980
that's what you produced and from that argument was around already in the seventies and

844
00:56:45,980 --> 00:56:49,390
i must say not very many people understood it was in probability theory and no

845
00:56:49,410 --> 00:56:52,290
statisticians really grab hold of anything with

846
00:56:52,310 --> 00:56:56,480
OK if there hard paper story but you know i think from this discussion hope

847
00:56:56,790 --> 00:56:58,660
read them and they want

848
00:56:58,660 --> 00:57:02,980
but in the nineties finally someone actually prove the theorem which is much easier to

849
00:57:04,250 --> 00:57:09,040
and and give you construct procedures the common works the problem is that it's an

850
00:57:09,040 --> 00:57:11,410
existence theorem always right no construction

851
00:57:11,430 --> 00:57:13,560
but the romans showed that

852
00:57:13,640 --> 00:57:18,160
g this object that we have is mysterious on the previous slides actually has a

853
00:57:18,160 --> 00:57:20,770
representation as an infinite sum of and

854
00:57:20,830 --> 00:57:22,560
this is logarithmic anymore

855
00:57:22,580 --> 00:57:27,520
and it's the sum over pi i don't think it was but iraqi delta theta

856
00:57:27,730 --> 00:57:31,710
look and the locations are coming from the not so this red thing here is

857
00:57:31,730 --> 00:57:34,890
not so let's say the gaussians for simplicity

858
00:57:34,910 --> 00:57:39,270
and then i sample atoms from you know that blue things

859
00:57:39,270 --> 00:57:41,460
and i get an infinite sum of atoms

860
00:57:41,480 --> 00:57:45,250
with certain weights in these weights are what are called stick breaking weights and where

861
00:57:45,250 --> 00:57:47,730
they come from is product of beta

862
00:57:47,790 --> 00:57:52,350
distributions samples from data i was stick the goes from zero to one

863
00:57:52,410 --> 00:57:56,560
break up the first piece in the stack under beta distribution is called beta one

864
00:57:56,620 --> 00:57:59,930
the second piece is the remaining part of the stick breaking up another bayonet that's

865
00:57:59,930 --> 00:58:01,790
been to get these links here

866
00:58:01,810 --> 00:58:04,100
and that's what these parts are

867
00:58:04,180 --> 00:58:08,410
right so if i did that once get a bunch of atoms

868
00:58:08,410 --> 00:58:11,960
and i'll get about to pi values get and put them all together i get

869
00:58:11,960 --> 00:58:15,460
a big g that's what the blue thing it's like a picket fence

870
00:58:15,520 --> 00:58:19,160
by doing this again will obviously get different atoms and i get different pi values

871
00:58:19,160 --> 00:58:24,620
linear motion evolution equation that is the state that phi x

872
00:58:24,760 --> 00:58:31,910
a simple linear combination of the state at time exposure time them the wall

873
00:58:31,960 --> 00:58:35,090
plus some kind of ocean this

874
00:58:35,180 --> 00:58:37,610
with the appropriate metric

875
00:58:37,710 --> 00:58:38,670
so the

876
00:58:38,680 --> 00:58:41,650
something time it's relevant in this context

877
00:58:41,660 --> 00:58:44,900
all these things are just markov process

878
00:58:44,920 --> 00:58:47,240
because what i can write

879
00:58:47,250 --> 00:58:49,680
so this is the punisher and from

880
00:58:49,700 --> 00:58:55,130
experiments what makes a gaussian distribution of arguement xk

881
00:58:55,150 --> 00:58:57,550
i mean x minus one

882
00:58:57,560 --> 00:59:03,420
on compliant even that sigma that is the the the distance

883
00:59:03,470 --> 00:59:07,180
OK so this is very popular

884
00:59:07,810 --> 00:59:13,220
bowling talking on this is ue the learning some applications in computer vision

885
00:59:14,260 --> 00:59:15,790
another the type of

886
00:59:15,800 --> 00:59:18,560
addition of the markov process

887
00:59:18,610 --> 00:59:23,510
OK for example the very as you interested in speech signal

888
00:59:23,560 --> 00:59:25,810
very very basic

889
00:59:25,830 --> 00:59:27,740
models of speech

890
00:59:27,750 --> 00:59:30,430
i don't see the learning basically

891
00:59:30,480 --> 00:59:35,500
this page and they are also this is what i mean is simply to say

892
00:59:35,500 --> 00:59:42,840
that the page time testing the switching time is simply a linear combination

893
00:59:42,860 --> 00:59:45,670
of the previous value

894
00:59:45,940 --> 00:59:47,330
the speech signal

895
00:59:47,340 --> 00:59:50,720
at time n minus one minus the

896
00:59:50,820 --> 00:59:52,500
some notion of

897
00:59:52,930 --> 00:59:56,030
these things are obviously

898
00:59:56,050 --> 00:59:57,660
can or return

899
00:59:57,670 --> 00:59:59,460
as a markov process

900
00:59:59,520 --> 01:00:03,290
what you need to do what you need to do to do this just the

901
01:00:03,290 --> 01:00:07,250
state you k which is pretty x

902
01:00:07,300 --> 01:00:10,490
it's a minor point you might as well be used back

903
01:00:10,510 --> 01:00:16,910
in the same picture the value of the node on the end of the day

904
01:00:17,110 --> 01:00:19,440
on your rewrite everything

905
01:00:19,490 --> 01:00:22,610
you see that will be extended process

906
01:00:22,660 --> 01:00:25,430
then you have also only

907
01:00:25,480 --> 01:00:31,200
gaussian evolution equation which is very similar to to the politics of the new except

908
01:00:31,200 --> 01:00:32,460
that now

909
01:00:32,470 --> 01:00:38,720
the metric of the metric are different but nothing

910
01:00:38,730 --> 01:00:41,170
nothing only

911
01:00:41,220 --> 01:00:44,090
the prime it's still a markov models

912
01:00:45,880 --> 01:00:48,320
so could be a bit more fun

913
01:00:48,340 --> 01:00:51,960
the call says if doing it you know it's very good

914
01:00:51,970 --> 01:00:53,580
because in particular

915
01:00:53,590 --> 01:00:57,970
which can be non stationary or anything like this

916
01:00:57,990 --> 01:01:00,860
the three of them

917
01:01:00,990 --> 01:01:05,020
and on the surface

918
01:01:05,030 --> 01:01:07,660
you say well is fitting or

919
01:01:07,710 --> 01:01:10,330
markov also for person they are

920
01:01:10,340 --> 01:01:14,780
but well already and they are in this because it's not stationary maybe it could

921
01:01:14,920 --> 01:01:19,410
be more fancy and try to put some kind of non stationary component in it

922
01:01:19,490 --> 01:01:22,840
so for example you could say well it's already and they are the kind of

923
01:01:22,840 --> 01:01:24,200
time varying

924
01:01:24,210 --> 01:01:28,230
so in this case what you do you could try to put also

925
01:01:28,290 --> 01:01:33,370
a kind of varying evolution of the occupation

926
01:01:33,380 --> 01:01:35,400
so this thing well

927
01:01:35,410 --> 01:01:38,310
that defines once more on the show

928
01:01:38,990 --> 01:01:42,110
evolution equation for the the diffusion

929
01:01:42,130 --> 01:01:47,410
on is that everything together you with the time the opposition of the speech signal

930
01:01:47,500 --> 01:01:53,340
then these things can also be written again as the market also we kind of

931
01:01:53,340 --> 01:01:55,240
false evolution equation

932
01:01:55,280 --> 01:01:58,830
doesn't matter if the market

933
01:01:58,880 --> 01:02:00,360
this is the last example

934
01:02:00,370 --> 01:02:02,650
w with that any more

935
01:02:02,860 --> 01:02:07,500
i'm quite interested in all the financial economic points

936
01:02:07,510 --> 01:02:13,500
on in all application and we working with continuous time process

937
01:02:13,510 --> 01:02:17,080
for example it is very popular modelling financial econometrics

938
01:02:17,100 --> 01:02:21,330
which is the that it's the model is very simple idea

939
01:02:21,350 --> 01:02:23,660
we're essentially to try to

940
01:02:23,700 --> 01:02:26,750
model the dynamics of an asset price st

941
01:02:26,760 --> 01:02:31,860
using a stochastic differential equation there

942
01:02:31,870 --> 01:02:35,340
like an the infinitesimal increments xt

943
01:02:35,370 --> 01:02:42,320
is equal to unity californian company kind of job also live process

944
01:02:42,330 --> 01:02:45,410
a very popular type of model

945
01:02:45,420 --> 01:02:48,580
all these things have tried to work with and our exactly

946
01:02:48,620 --> 01:02:52,980
when you try to do inference is that you can for continuous time called so

947
01:02:52,980 --> 01:02:54,160
the most basic

948
01:02:54,170 --> 01:02:58,360
i think people are doing when they want to basically do

949
01:02:58,450 --> 01:03:01,400
particle will people think those models

950
01:03:01,670 --> 01:03:04,800
the first thing to do is the discrete discretized

951
01:03:06,680 --> 01:03:10,860
the discrete time model simple scheme

952
01:03:10,880 --> 01:03:13,230
you just keep approximate the

953
01:03:13,290 --> 01:03:16,000
very related by some kind of a finite

954
01:03:16,050 --> 01:03:19,130
finite differences are not due to model

955
01:03:19,140 --> 01:03:22,030
which one will where can be right

956
01:03:22,050 --> 01:03:23,630
and the simple

957
01:03:24,220 --> 01:03:27,220
the origin of the simple markov also

958
01:03:27,270 --> 01:03:28,650
on here

959
01:03:29,440 --> 01:03:30,820
this is it on

960
01:03:30,830 --> 01:03:35,420
there are many many if you look at basically the literature there are many continuous

961
01:03:35,420 --> 01:03:41,580
time models of this one is the this is for example about net

962
01:03:41,630 --> 01:03:43,530
if you look at the the model for

963
01:03:43,550 --> 01:03:47,420
the dynamics of population dynamics although small enough

964
01:03:47,440 --> 01:03:49,660
you by it

965
01:03:50,470 --> 01:03:56,610
well when you were willing parties discretised them on it becomes this important time up

966
01:03:56,650 --> 01:04:02,740
all the techniques cannot disperse all those parts of the sequential monte carlo techniques are

967
01:04:02,770 --> 01:04:06,480
actually been successfully used in this context

968
01:04:06,520 --> 01:04:12,650
so this is that this is a lot so now this was the

969
01:04:12,660 --> 01:04:14,520
basic model

970
01:04:14,570 --> 01:04:16,390
for the process

971
01:04:16,470 --> 01:04:18,820
of interest in the second

972
01:04:20,200 --> 01:04:22,670
of the target could be basically

973
01:04:22,680 --> 01:04:26,220
the local activity of an asset or whatever

974
01:04:26,230 --> 01:04:27,900
a little bit of an asset

975
01:04:29,970 --> 01:04:33,120
i don't think this markup office model

976
01:04:33,130 --> 01:04:36,180
the prior distribution of the process

977
01:04:36,190 --> 01:04:37,930
but i don't have access to it

978
01:04:38,930 --> 01:04:40,320
i don't have access to it

979
01:04:40,350 --> 01:04:43,680
so what i assume in something very simple

980
01:04:43,830 --> 01:04:46,840
we generalize that later on

981
01:04:46,880 --> 01:04:48,460
i will assume that

982
01:04:48,480 --> 01:04:50,060
xk might be

983
01:04:50,080 --> 01:04:52,300
he does also

984
01:04:52,310 --> 01:04:54,160
access funds

985
01:04:55,110 --> 01:04:59,300
to another related process y is also

986
01:04:59,300 --> 01:05:01,980
the discrete time markov process

987
01:05:01,990 --> 01:05:04,030
and in particular

988
01:05:04,040 --> 01:05:07,100
i will make the following statistical assumptions

989
01:05:07,150 --> 01:05:09,530
i would assume that

990
01:05:09,530 --> 01:05:12,090
if i knew the policies xk

991
01:05:12,120 --> 01:05:14,490
i don't know if i knew it

992
01:05:14,490 --> 01:05:17,320
then the salvation y a

993
01:05:17,340 --> 01:05:19,900
with respect to be independent

994
01:05:19,910 --> 01:05:22,340
o marginally distributed

995
01:05:23,560 --> 01:05:26,820
two why we don't see e g

996
01:05:26,990 --> 01:05:28,310
is this is

997
01:05:28,310 --> 01:05:31,480
so there's a few things in the system that are a bit different from what

998
01:05:31,520 --> 01:05:34,460
is often done in terms of sort of structure from motion

999
01:05:35,030 --> 01:05:39,490
pipelines and so on so we're not necessarily detecting all the features we can see in every image

1000
01:05:40,250 --> 01:05:42,020
we're detecting features when we need to

1001
01:05:42,480 --> 01:05:46,020
so this green box flashing around especially hunting for new features

1002
01:05:46,440 --> 01:05:48,740
in spaces that aren't already well covered

1003
01:05:49,340 --> 01:05:50,400
by existing features

1004
01:05:50,810 --> 01:05:51,770
so you see that

1005
01:05:52,520 --> 01:05:55,780
worked with initializing new features has new itself

1006
01:05:56,650 --> 01:06:00,700
this income interview and once new features appear within tracking them

1007
01:06:01,430 --> 01:06:01,980
very stable

1008
01:06:04,350 --> 01:06:07,630
so this is a little bit more on how the tracking work so that's also

1009
01:06:07,630 --> 01:06:10,130
not done bottom-up it's done in a very predictive right

1010
01:06:10,610 --> 01:06:11,560
so in every frame

1011
01:06:12,170 --> 01:06:12,930
we have a

1012
01:06:13,430 --> 01:06:15,120
the current estimate where the camera is

1013
01:06:16,290 --> 01:06:19,890
current estimate of where this the features are in the world so the yes men

1014
01:06:19,920 --> 01:06:22,890
where the camera is now especially when was on the last frame

1015
01:06:23,360 --> 01:06:25,380
plus some kind of motion prediction

1016
01:06:25,910 --> 01:06:31,250
with an extra uncertainty added to it to account for the fact that you know has uncertain dynamics

1017
01:06:32,460 --> 01:06:35,450
enables us to basically predict which features should be visible

1018
01:06:36,050 --> 01:06:37,420
but also where image

1019
01:06:37,990 --> 01:06:38,390
should they

1020
01:06:38,890 --> 01:06:40,440
you are we most likely to find them

1021
01:06:40,970 --> 01:06:45,360
so basically dynamically calculating search region feature these features

1022
01:06:46,010 --> 01:06:46,690
one each frame

1023
01:06:47,220 --> 01:06:48,080
and then we basically

1024
01:06:48,600 --> 01:06:53,090
trying to match each those features by correlation search just within these small

1025
01:06:53,910 --> 01:06:55,660
prediction regions only on each frame

1026
01:06:56,340 --> 01:06:58,840
and that's one of the things in the early days that made the system

1027
01:06:59,460 --> 01:07:01,800
fast and unable to work in real time

1028
01:07:02,600 --> 01:07:04,910
so this then shows the output of the system

1029
01:07:06,720 --> 01:07:09,260
running in real time at thirty frames per second here

1030
01:07:09,660 --> 01:07:11,650
this thing flying around is an estimated

1031
01:07:12,200 --> 01:07:14,250
three eighty position of the camera so

1032
01:07:14,870 --> 01:07:17,250
six degrees of freedom for position rotation

1033
01:07:17,890 --> 01:07:20,160
displayed here is this kind of flying graphic

1034
01:07:20,750 --> 01:07:23,830
and then each one of these things here is one of the landmarks in the scene

1035
01:07:24,300 --> 01:07:29,770
so you see that has a texture patch associated with it which is kind of its unique

1036
01:07:31,540 --> 01:07:33,560
and here we now see a three d e

1037
01:07:34,130 --> 01:07:36,210
ellipse representing uncertainty

1038
01:07:36,690 --> 01:07:37,800
regions so you see that when

1039
01:07:38,480 --> 01:07:42,870
new features are initialize they appear in the map with big uncertainty particularly in the depth coordinate

1040
01:07:43,340 --> 01:07:45,380
as we make more and more measurements the uncertainty

1041
01:07:46,030 --> 01:07:46,760
shrinks down

1042
01:07:47,910 --> 01:07:52,330
so at demo of what we could do with this system is is some augmented reality

1043
01:07:52,780 --> 01:07:54,870
so here we basically going to put some virtual

1044
01:07:55,470 --> 01:07:57,390
objects into a real video

1045
01:07:57,850 --> 01:08:01,570
so this is straightforward if you've got a system which is giving you

1046
01:08:01,970 --> 01:08:02,720
in real time

1047
01:08:03,180 --> 01:08:06,270
six-degree-of-freedom position for the camera on every frame

1048
01:08:06,710 --> 01:08:07,320
all you do is

1049
01:08:07,710 --> 01:08:09,240
seven positioned opengeo

1050
01:08:09,980 --> 01:08:13,270
and so you draw me a shelf has this from position

1051
01:08:13,810 --> 01:08:15,040
stick it on top the real

1052
01:08:16,390 --> 01:08:18,400
and if you could the quality of your camera

1053
01:08:19,030 --> 01:08:23,060
emotional state estimation is good then they shall should look like they're really stuck

1054
01:08:23,760 --> 01:08:24,480
so the real world

1055
01:08:25,460 --> 01:08:27,680
and we've got some good news here for adjusting their

1056
01:08:28,700 --> 01:08:29,620
positions in the world

1057
01:08:30,280 --> 01:08:32,520
so you can see this data the results are

1058
01:08:33,260 --> 01:08:36,940
quite good the shelved certainly stayed pretty much in the right place you you can see the some

1059
01:08:37,430 --> 01:08:38,980
just a particularly when this some

1060
01:08:39,880 --> 01:08:41,250
inclusion of some of the features

1061
01:08:44,250 --> 01:08:45,480
but it's not bad

1062
01:08:47,200 --> 01:08:49,100
so just a shows a couple of either

1063
01:08:51,850 --> 01:08:54,180
applications of exactly the same algorithm

1064
01:08:54,620 --> 01:08:57,140
so this is an application to a humanoid robot in

1065
01:08:57,830 --> 01:09:00,170
lab in japan risk collaborating with

1066
01:09:00,780 --> 01:09:01,940
so this is just a

1067
01:09:02,600 --> 01:09:04,680
sequence the robot walked around in a circle

1068
01:09:05,940 --> 01:09:06,680
this guy's just

1069
01:09:07,220 --> 01:09:09,650
this is just a cradled catching case it falls over

1070
01:09:10,500 --> 01:09:13,660
the robot basically what random the circle and there's a camera and its head

1071
01:09:14,260 --> 01:09:14,930
that we were running

1072
01:09:15,730 --> 01:09:16,420
one slam on

1073
01:09:17,690 --> 01:09:20,380
um so this is the view from the cameras it works

1074
01:09:21,050 --> 01:09:22,430
and this is the estimated

1075
01:09:23,780 --> 01:09:25,870
camera trajectory and map features from

1076
01:09:26,730 --> 01:09:27,570
from one this land

1077
01:09:28,140 --> 01:09:30,280
so you can see that we will crawl around in a circle

1078
01:09:31,950 --> 01:09:33,010
building a map features

1079
01:09:37,690 --> 01:09:39,530
yeah but basically the same kind of behavior

1080
01:09:40,080 --> 01:09:44,120
this video is particularly interesting because it shows the loop closure so the robot comes all the way around

1081
01:09:44,630 --> 01:09:45,910
there's a point where suddenly these

1082
01:09:46,890 --> 01:09:51,770
uncertainty snap from being quite big suddenly getting much smaller again and that's happened when

1083
01:09:51,770 --> 01:09:53,220
the robot comes all the way around the loop

1084
01:09:53,680 --> 01:09:54,970
and is able to we observe

1085
01:09:55,440 --> 01:10:00,080
things that it's all very early in its trajectory so those are landmarks have a very accurate

1086
01:10:00,570 --> 01:10:02,400
positions in the world because it's all them

1087
01:10:02,840 --> 01:10:05,160
close to the time when it defined its coordinate frame

1088
01:10:05,680 --> 01:10:09,230
at the start of slam so once it can every registers with is

1089
01:10:09,900 --> 01:10:15,300
the other features that it's initialize more recently get pulled around with it and not so much better

1090
01:10:16,470 --> 01:10:17,590
estimated positions

1091
01:10:18,380 --> 01:10:19,520
so that's kind of classic

1092
01:10:20,200 --> 01:10:21,210
slam behavior

1093
01:10:22,310 --> 01:10:22,810
is another

1094
01:10:24,120 --> 01:10:27,810
demo this is something i did with one quarter mile who is now a

1095
01:10:28,440 --> 01:10:31,660
at the university bristow and you probably still have the running

1096
01:10:32,420 --> 01:10:33,270
obviously next year

1097
01:10:33,890 --> 01:10:36,060
but he built back in his speech to this

1098
01:10:36,960 --> 01:10:40,560
really cool a wearable camera system that he called a wearable robot

1099
01:10:41,350 --> 01:10:42,160
but had a

1100
01:10:43,130 --> 01:10:44,490
cameron basically a servo

1101
01:10:45,340 --> 01:10:46,390
actuator platform

1102
01:10:47,330 --> 01:10:49,500
and then so someone was wearing the collar

1103
01:10:50,110 --> 01:10:51,210
moving around in the scene

1104
01:10:52,050 --> 01:10:53,140
and we could do things like

1105
01:10:54,230 --> 01:10:58,800
tell the camera stay fixated on one particular object as the person is moving around

1106
01:10:59,330 --> 01:11:01,580
and then decatur fixate on another object

1107
01:11:02,440 --> 01:11:03,340
and stay looking back

1108
01:11:05,600 --> 01:11:09,590
so all enabled by real time position estimation from from slam

1109
01:11:10,350 --> 01:11:12,250
so since those days we we've

1110
01:11:12,810 --> 01:11:13,710
grown quite a lot in

1111
01:11:14,100 --> 01:11:16,320
the sophistication and what we can do in real time

1112
01:11:17,110 --> 01:11:19,940
visual slam there's there's been a number of things kind of going on

1113
01:11:20,460 --> 01:11:21,910
around the world in more general

1114
01:11:23,420 --> 01:11:27,600
research and a lot this research has not necessarily been in computer vision it's happening in

1115
01:11:28,010 --> 01:11:31,400
robotics where people are building robot mapping systems using completely

1116
01:11:31,940 --> 01:11:37,960
different sorts of sensors like laser range finders but still the general character of these mapping problems is very much

1117
01:11:38,510 --> 01:11:43,030
shared so for instance we now have a pretty good understanding of how to make really large scale

1118
01:11:43,670 --> 01:11:44,570
mapping systems

1119
01:11:45,060 --> 01:11:46,700
because basically if you can make

1120
01:11:47,330 --> 01:11:51,040
something that can make a good map locally similar to what you've already seen

1121
01:11:51,780 --> 01:11:54,560
you can then as you start exploring large areas

1122
01:11:54,980 --> 01:11:56,350
just basically make a chain

1123
01:11:56,920 --> 01:12:00,230
these local maps start building one map when it gets too big

1124
01:12:00,700 --> 01:12:02,190
pocket start a new one

1125
01:12:02,640 --> 01:12:04,230
remember the transformation between

1126
01:12:04,810 --> 01:12:06,580
you get a chain of connected mats

1127
01:12:07,160 --> 01:12:08,350
and then if you have a

1128
01:12:08,400 --> 01:12:11,070
another component this is often enabled by

1129
01:12:11,730 --> 01:12:13,020
and image retrieval type

1130
01:12:13,520 --> 01:12:19,030
vision system that can basically say hang on are described an image that looks really similar to what i grabbed

1131
01:12:19,910 --> 01:12:20,890
you know twenty minutes ago

1132
01:12:21,380 --> 01:12:23,320
i think i'm back in the same place i've been before

1133
01:12:24,000 --> 01:12:26,640
then even if there is some drift that's occurred

1134
01:12:27,060 --> 01:12:27,600
in these

1135
01:12:28,290 --> 01:12:31,910
in your chain of local maps which is inevitable because they'll always be a little

1136
01:12:31,910 --> 01:12:33,530
bit of error in each local map

1137
01:12:34,230 --> 01:12:38,120
then you can correct it you can say at this place actually joins up this

1138
01:12:39,070 --> 01:12:40,420
enforce the constraints

1139
01:12:40,900 --> 01:12:43,000
and do some kind of optimization over my whole

1140
01:12:44,580 --> 01:12:48,670
and you know that this image retrieval part can now be quite fast and robust

1141
01:12:49,340 --> 01:12:54,520
the correction of the market can also be quite fast and robust these days and you can get back to

1142
01:12:55,260 --> 01:12:56,540
globally consistent maps

1143
01:12:56,540 --> 01:13:00,990
to go beyond just people are connected people are not connected

1144
01:13:01,040 --> 01:13:05,920
and basically the you will be

1145
01:13:05,930 --> 01:13:06,990
that the

1146
01:13:08,560 --> 01:13:12,720
the links that will be looking at will we just i'm i'm connected to someone

1147
01:13:12,720 --> 01:13:16,990
not connected to someone but we will try to to go beyond sort of capture

1148
01:13:17,020 --> 01:13:20,520
let's the sentiment standards of links and so on

1149
01:13:21,900 --> 01:13:29,130
the the

1150
01:13:29,140 --> 01:13:30,090
OK good

1151
01:13:30,450 --> 01:13:34,760
so here is series of of things the things that we look and so the

1152
01:13:34,760 --> 01:13:37,260
first thing i will talk about is

1153
01:13:38,380 --> 01:13:43,630
one possible way how to sort of something that we did together in collaboration facebook

1154
01:13:43,630 --> 01:13:47,870
about how how to predict links or how do you recommend friends social network and

1155
01:13:47,880 --> 01:13:52,590
how to do this in machine learning framework and then we go beyond not just

1156
01:13:52,590 --> 01:13:56,810
saying what people you may know what you actually start thinking about how can what

1157
01:13:56,870 --> 01:14:01,770
you what you think of people so so what your friends what your enemies and

1158
01:14:01,770 --> 01:14:03,010
then you sort of been

1159
01:14:03,570 --> 01:14:07,540
one is the same how how do people evaluate others people how do people evaluate

1160
01:14:07,540 --> 01:14:12,070
content the they see what kind of mechanisms could be used to just see so

1161
01:14:12,200 --> 01:14:16,600
based on the audience that appears on the page how they become much like page

1162
01:14:16,600 --> 01:14:18,620
OK so these are some of the

1163
01:14:18,650 --> 01:14:20,850
three or four parts to the door

1164
01:14:20,870 --> 01:14:24,270
the second so here here and here are the things right so now i want

1165
01:14:24,270 --> 01:14:25,900
to focus on interactions

1166
01:14:25,930 --> 01:14:29,730
the first thing i will be talking about it will be a big part about

1167
01:14:29,730 --> 01:14:33,370
how do i recommend links in networks and then i will show you what that

1168
01:14:33,400 --> 01:14:39,070
that the strength estimate the strength of their tie strength of the connection on facebook

1169
01:14:39,090 --> 01:14:40,040
then we will go

1170
01:14:40,060 --> 01:14:43,700
the same not just require the people i know but what do i think of

1171
01:14:43,700 --> 01:14:47,560
them are my friends are my enemies of my photos and then we start seeing

1172
01:14:47,560 --> 01:14:52,150
about how do people evaluate others other people how they evaluate the content created by

1173
01:14:52,150 --> 01:14:55,880
people and so on OK so this is this is the place and we start

1174
01:14:55,880 --> 01:15:00,320
with the first talk about how become an example networks of how do i recommend

1175
01:15:00,350 --> 01:15:04,180
your friends to create OK so the second will be the following so the way

1176
01:15:04,180 --> 01:15:08,260
we do this is that we will be able to take a graph and we

1177
01:15:08,260 --> 01:15:10,010
will take a graph

1178
01:15:10,320 --> 01:15:16,130
the sort of music some outside sometimes it is your right and say OK what

1179
01:15:16,130 --> 01:15:19,920
edits are going to happen in this graph let's say the next month of our

1180
01:15:19,920 --> 01:15:24,540
idea will be to produce ranked list of links that that not getting the present

1181
01:15:24,540 --> 01:15:27,700
graph and that are predicted to appear in the

1182
01:15:27,700 --> 01:15:31,610
in the future snapshot of the right so you will be i'm taking the face

1183
01:15:31,660 --> 01:15:33,420
the graph of facebook today

1184
01:15:33,420 --> 01:15:38,510
i want to generate a list of new possibilities that will emerge interface will grow

1185
01:15:38,540 --> 01:15:42,580
in the next one OK so that's that's set and then of course how do

1186
01:15:42,580 --> 01:15:47,070
i measure success the way i made myself is to say OK how many of

1187
01:15:47,070 --> 01:15:52,830
the edges i predict that really appeared in the this future best right and the

1188
01:15:52,830 --> 01:15:57,290
way you can capture these kind of performances to say OK what is my precision

1189
01:15:57,480 --> 01:16:00,980
on the top pop right so if i say here are the most likely no

1190
01:16:01,160 --> 01:16:04,660
edges that i think would happen in the future how many of them you get

1191
01:16:04,670 --> 01:16:09,510
correct so that that is one possible way to evaluate and this is obviously a

1192
01:16:09,570 --> 01:16:16,070
tradition passed a prediction task was formalized this way by a little david lieberman john

1193
01:16:16,070 --> 01:16:19,390
kleinberg back in two thousand three and the the

1194
01:16:19,640 --> 01:16:25,210
facebook example sort of using moderate use this sort of is based is based on

1195
01:16:25,210 --> 01:16:26,690
is OK so

1196
01:16:26,700 --> 01:16:31,700
the idea is for example the following so so you can take in this case

1197
01:16:31,700 --> 01:16:37,920
collaboration networks from from different areas of physics and you have about these networks have

1198
01:16:37,920 --> 01:16:39,580
a few thousand of authors

1199
01:16:39,600 --> 01:16:45,350
and then of thousands of collaborations and now you can take for example

1200
01:16:45,410 --> 01:16:50,830
displayed timing to have no

1201
01:16:50,850 --> 01:16:56,260
one such split time in house and say OK what the new collaborations that will

1202
01:16:56,290 --> 01:17:01,380
happen happen in the future right so the idea is that you only you only

1203
01:17:01,380 --> 01:17:05,950
have to you want to create is ranked list of edges in these networks and

1204
01:17:05,950 --> 01:17:08,540
the way you can do this is basically the for example may be for every

1205
01:17:08,540 --> 01:17:10,480
pair of nodes is somehow

1206
01:17:10,510 --> 01:17:15,690
compute the proximity or distance and then during this paris by the by the proximity

1207
01:17:15,850 --> 01:17:19,790
top one on the bottom the other on the part of the the ones that

1208
01:17:19,810 --> 01:17:23,090
are far apart at the bottom and then the the nodes of the best of

1209
01:17:23,100 --> 01:17:26,330
nodes and problem is are predictions right so for example you could say i had

1210
01:17:26,540 --> 01:17:30,210
one way to link to engage us would be by the distance in the graph

1211
01:17:30,210 --> 01:17:34,090
right sort of this closer are the in the graph the more likely that just

1212
01:17:34,090 --> 01:17:39,060
to appear right that's maybe not so good predictor right another possible way to to

1213
01:17:39,060 --> 01:17:43,500
quantify the proximity between a pair of nodes to say how many common neighbors right

1214
01:17:43,500 --> 01:17:48,090
so the idea would be a better people that have lots of common friends together

1215
01:17:48,100 --> 01:17:52,030
i believe that they are more likely to create edges among themselves right instead of

1216
01:17:52,040 --> 01:17:56,440
sort of common neighbors i was shown here you can do that jakarta similarity between

1217
01:17:56,440 --> 01:18:00,190
what my friends what are your friends and was just got similarity between these two

1218
01:18:00,190 --> 01:18:02,900
sets there is

1219
01:18:02,940 --> 01:18:07,330
there is a method that will turn out sort of an unsupervised method that turn

1220
01:18:07,340 --> 01:18:11,850
out to work best is something that that that is referred to as a damage

1221
01:18:11,850 --> 01:18:14,230
on that score when sort of i go

1222
01:18:14,240 --> 01:18:18,360
i goal all the common friends a pair of nodes has then i take and

1223
01:18:18,360 --> 01:18:22,210
a some instead of just counting them which would be the common neighbours i think

1224
01:18:22,210 --> 01:18:26,730
one over the log of the degree of that common friend we have together so

1225
01:18:26,730 --> 01:18:30,640
the higher degree of the guy the less last week it will add to the

1226
01:18:30,640 --> 01:18:35,410
sum i could for example say the probability of two nodes connecting is proportional to

1227
01:18:35,960 --> 01:18:39,850
the product of of the degrees of the end points and so on and so

1228
01:18:40,540 --> 01:18:43,680
i have so basically for every pair of nodes in the network i could go

1229
01:18:43,680 --> 01:18:47,680
computer the score and now with the rank nodes by the score and predict the

1230
01:18:47,680 --> 01:18:51,250
node with the highest score on the top and here is how well this would

1231
01:18:51,250 --> 01:18:57,790
work so what i'm showing you here is results from the paper by even when

1232
01:18:57,800 --> 01:19:03,290
playing with these are different methods this is the this is the score higher is

1233
01:19:03,290 --> 01:19:10,670
better and this is the relative performance versus making random predictions right so basically the

1234
01:19:10,670 --> 01:19:15,370
if the charge doesn't change the charged surface density doesn't change and so the electric

1235
01:19:15,370 --> 01:19:17,550
field inside remains constant

1236
01:19:17,550 --> 01:19:19,260
so exactly what we did there

1237
01:19:19,280 --> 01:19:21,110
and now i'm going to

1238
01:19:21,120 --> 01:19:23,200
move them further apart

1239
01:19:23,220 --> 01:19:24,560
therefore i'm going to

1240
01:19:24,560 --> 01:19:26,330
make the larger

1241
01:19:26,340 --> 01:19:27,720
and that can only happen

1242
01:19:27,890 --> 01:19:31,030
the potential difference between the plates increase

1243
01:19:31,070 --> 01:19:33,380
i will start off with thousand balls

1244
01:19:33,390 --> 01:19:35,500
by these one millimeter

1245
01:19:35,580 --> 01:19:39,220
and then i will open this gap of ten years and then i have the

1246
01:19:39,240 --> 01:19:40,690
potential difference

1247
01:19:41,880 --> 01:19:43,890
ten thousand volts

1248
01:19:43,900 --> 01:19:45,260
but since the

1249
01:19:46,780 --> 01:19:48,280
in the capacitor

1250
01:19:48,300 --> 01:19:49,370
is one half

1251
01:19:49,390 --> 01:19:51,930
a few times the potential difference

1252
01:19:51,950 --> 01:19:54,370
this is the same as this delta

1253
01:19:54,370 --> 01:19:56,970
and if q is not changing but if i go

1254
01:19:57,060 --> 01:19:58,480
from the from

1255
01:19:58,510 --> 01:20:01,750
one thousand volts ten thousand fold is very clear

1256
01:20:01,760 --> 01:20:06,510
i have done work i have increased the electrostatic potential energy

1257
01:20:06,580 --> 01:20:08,650
and this is what i want to show you

1258
01:20:08,670 --> 01:20:11,160
we're going to have that there

1259
01:20:11,170 --> 01:20:13,550
so i changed my

1260
01:20:13,590 --> 01:20:17,730
television and i'll have to change the lights so little bit so you

1261
01:20:17,790 --> 01:20:19,670
i can see that

1262
01:20:20,960 --> 01:20:23,740
and this one or this one of

1263
01:20:26,150 --> 01:20:27,970
wait for the light to settle

1264
01:20:27,990 --> 01:20:32,150
and we want also the

1265
01:20:32,200 --> 01:20:34,480
currently is on the one on the right there

1266
01:20:34,580 --> 01:20:36,010
is the

1267
01:20:36,160 --> 01:20:38,130
and these the current meter

1268
01:20:38,180 --> 01:20:39,800
and you see here

1269
01:20:39,840 --> 01:20:41,580
these two plates

1270
01:20:41,740 --> 01:20:44,390
separated now by about one millimeter

1271
01:20:44,420 --> 01:20:45,830
i have here

1272
01:20:45,840 --> 01:20:48,170
a very thin sheets

1273
01:20:48,230 --> 01:20:53,200
transparency which i can move in between to make sure they don't make contact

1274
01:20:53,310 --> 01:20:55,210
here is my power supply

1275
01:20:55,260 --> 01:20:56,710
and i have there

1276
01:20:56,760 --> 01:20:59,110
this the propeller type thing which is

1277
01:20:59,110 --> 01:21:01,140
some kind of the voltmeter

1278
01:21:01,190 --> 01:21:04,040
and if going to move in this direction

1279
01:21:04,040 --> 01:21:07,890
that means that the voltage between the plates

1280
01:21:08,040 --> 01:21:10,260
so i'm going to charge now

1281
01:21:10,280 --> 01:21:11,960
the potential difference of

1282
01:21:12,050 --> 01:21:17,340
thousand and as i do that you will see a very short search here

1283
01:21:17,390 --> 01:21:20,290
on this and here

1284
01:21:20,340 --> 01:21:23,780
it's not very spectacular but at least you can see for the first time in

1285
01:21:23,780 --> 01:21:26,550
your life the charge is actually flowing

1286
01:21:26,600 --> 01:21:28,290
from my power supply

1287
01:21:28,300 --> 01:21:30,920
onto the plates newspapers

1288
01:21:30,940 --> 01:21:31,820
that's it

1289
01:21:31,820 --> 01:21:35,150
only the current as long as the charge is flowing

1290
01:21:35,210 --> 01:21:36,980
let me first say that

1291
01:21:37,030 --> 01:21:39,220
look at the there

1292
01:21:39,230 --> 01:21:42,000
three two one zero

1293
01:21:42,050 --> 01:21:43,630
that's all it took

1294
01:21:43,710 --> 01:21:45,600
the charge this place

1295
01:21:45,640 --> 01:21:47,570
now fully charged

1296
01:21:47,580 --> 01:21:49,230
thousands all different

1297
01:21:49,250 --> 01:21:51,340
and now as i'm going to

1298
01:21:51,340 --> 01:21:53,110
increase the gap

1299
01:21:53,120 --> 01:21:54,620
there no reason for any

1300
01:21:54,670 --> 01:21:58,960
charge to go away from the plates so the media will not do much

1301
01:21:59,010 --> 01:22:00,530
probably nothing

1302
01:22:00,540 --> 01:22:05,060
but you're going to see this propeller which indicates the potential difference between the plates

1303
01:22:05,210 --> 01:22:07,380
going to see it move because

1304
01:22:07,390 --> 01:22:11,950
i'm doing all this work i'm going from one millimeter to ten millimeters i'm creating

1305
01:22:11,950 --> 01:22:17,330
always electric fields and it's hard work pays off in terms of increasing the potential

1306
01:22:17,330 --> 01:22:18,780
one thousand volts

1307
01:22:18,850 --> 01:22:20,420
ten thousand volts

1308
01:22:20,480 --> 01:22:22,780
so there you go

1309
01:22:22,870 --> 01:22:24,310
two millimeters now

1310
01:22:24,330 --> 01:22:26,590
look at the voltmeter is going

1311
01:22:26,750 --> 01:22:29,500
three millimetres i always hard work

1312
01:22:29,740 --> 01:22:32,380
doing nothing for many many years

1313
01:22:32,380 --> 01:22:34,580
i'm creating electric fields

1314
01:22:34,590 --> 01:22:36,240
we should be proud of me

1315
01:22:36,260 --> 01:22:38,850
i'm creating an electric field look at that

1316
01:22:38,860 --> 01:22:43,800
electric field remains constant between the plates because the charge is trapped charge can go

1317
01:22:44,850 --> 01:22:46,750
and now it's seven millimetres

1318
01:22:46,820 --> 01:22:48,680
seven thousand volts

1319
01:22:48,740 --> 01:22:51,150
eight thousand votes

1320
01:22:51,160 --> 01:22:56,100
i mean nine millimeters nine thousand both number and needed there's nothing no charges

1321
01:22:56,100 --> 01:22:59,630
following to replace no charges flowing from the plates

1322
01:22:59,640 --> 01:23:01,180
i'm now at ten millimetres

1323
01:23:01,200 --> 01:23:02,730
and now i have created

1324
01:23:02,810 --> 01:23:06,660
huge volume electric field and the potential difference

1325
01:23:06,820 --> 01:23:09,180
ten times larger than it was before

1326
01:23:09,200 --> 01:23:10,300
and so you see

1327
01:23:10,300 --> 01:23:11,400
i indeed

1328
01:23:11,420 --> 01:23:17,790
have done work you see it here in front of your own eyes

1329
01:23:17,790 --> 01:23:19,550
all right let

1330
01:23:19,680 --> 01:23:22,120
this down and i'll take the

1331
01:23:22,280 --> 01:23:25,070
lights back up

1332
01:23:25,110 --> 01:23:27,440
and we go back

1333
01:23:27,470 --> 01:23:35,260
two normal

1334
01:23:35,320 --> 01:23:36,550
i have here

1335
01:23:36,610 --> 01:23:39,740
eight hundred michael for capacitor

1336
01:23:39,850 --> 01:23:41,710
dangerous baby

1337
01:23:41,740 --> 01:23:45,540
and we can charge that up to three thousand volts

1338
01:23:45,570 --> 01:23:46,840
and when we do that

1339
01:23:46,850 --> 01:23:48,230
we get three

1340
01:23:48,290 --> 01:23:50,170
tens of the coulomb of charge

1341
01:23:50,240 --> 01:23:50,990
on that

1342
01:23:53,280 --> 01:23:56,570
so the

1343
01:23:56,790 --> 01:23:59,880
it is the number

1344
01:23:59,890 --> 01:24:02,060
so it is one hundred

1345
01:24:04,390 --> 01:24:08,280
going to put the potential difference over three thousand balls

1346
01:24:08,330 --> 01:24:10,850
gives charge q

1347
01:24:10,960 --> 01:24:13,210
o point three coulomb

1348
01:24:13,260 --> 01:24:14,540
and that means

1349
01:24:14,580 --> 01:24:16,650
one half the square

1350
01:24:16,710 --> 01:24:19,680
which is the energy that is stored in the capacitor

1351
01:24:19,760 --> 01:24:22,330
is four hundred and fifty two

1352
01:24:22,380 --> 01:24:27,210
this will take fifteen minutes

1353
01:24:27,230 --> 01:24:29,650
so i'm going to charge it now

1354
01:24:29,670 --> 01:24:34,800
because at the end of lecture i need a charged capacitor for demonstration

1355
01:24:34,840 --> 01:24:36,790
so i can show you there

1356
01:24:38,080 --> 01:24:41,340
potential difference over the capacitor

1357
01:24:41,390 --> 01:24:45,390
which will slowly change

1358
01:24:45,470 --> 01:24:48,190
and will keep an eye on it during the lecture

1359
01:24:48,200 --> 01:24:52,290
and then by the time it fully charged we will have reached the end of

1360
01:24:52,290 --> 01:24:53,170
the lecture

1361
01:24:53,220 --> 01:24:54,780
and then we can

1362
01:24:56,340 --> 01:24:58,810
so here's is than this

1363
01:25:00,140 --> 01:25:02,670
one hundred microphone what i call it the monster

1364
01:25:02,670 --> 01:25:04,560
the complexity of the

1365
01:25:04,600 --> 01:25:09,540
examples that were actually observing this is coming from the empirical rademacher complexity

1366
01:25:09,560 --> 01:25:11,180
component we use

1367
01:25:11,200 --> 01:25:14,920
and is measured by the trace of the kernel matrix so that's another nice

1368
01:25:14,950 --> 01:25:16,750
property that we seem to be

1369
01:25:17,280 --> 01:25:19,880
so getting for free in this case

1370
01:25:23,260 --> 01:25:25,230
i think

1371
01:25:25,240 --> 01:25:26,400
you know there are no

1372
01:25:26,420 --> 01:25:33,450
other factors the only thing that's disappointing compared to say the VC proof

1373
01:25:33,540 --> 01:25:39,410
is that so maybe the large margin covering number proof is that we have a

1374
01:25:39,410 --> 01:25:41,560
square in fact coming in

1375
01:25:41,570 --> 01:25:45,140
and as i say that really seems to come from the way the rademacher said

1376
01:25:45,140 --> 01:25:47,920
that doesn't seem to be an easy way to get around

1377
01:25:48,320 --> 01:25:53,560
we are handling the case where we have soft margins because we've got the size

1378
01:25:53,560 --> 01:25:55,730
which we didn't handle directly

1379
01:25:55,750 --> 01:25:58,070
in the covering number case

1380
01:25:58,120 --> 01:26:00,290
though i should mention that

1381
01:26:00,310 --> 01:26:02,150
there is a technique

1382
01:26:02,170 --> 01:26:07,130
extending that previous covering number analysis to the soft margin case

1383
01:26:07,150 --> 01:26:10,550
so you can actually do it using those those results

1384
01:26:10,560 --> 01:26:12,240
and still

1385
01:26:12,260 --> 01:26:16,070
keep out of the square and territory so in some respects

1386
01:26:16,080 --> 01:26:17,540
i i have a sort of

1387
01:26:17,550 --> 01:26:19,330
slight preference for the

1388
01:26:19,340 --> 01:26:22,410
previous results as far as the SVM is concerned

1389
01:26:22,480 --> 01:26:26,730
having said that i think the best bounds for the SVM the new PAC bayesian

1390
01:26:26,730 --> 01:26:28,280
bounds that

1391
01:26:28,330 --> 01:26:30,620
i think you will probably

1392
01:26:30,630 --> 01:26:32,310
due to his

1393
01:26:32,320 --> 01:26:35,320
this section of these lectures

1394
01:26:35,330 --> 01:26:39,830
it simplifies even further in the case of gassing kernels because the

1395
01:26:40,670 --> 01:26:41,970
the kernel of

1396
01:26:41,990 --> 01:26:45,610
o point with itself in the gas kernel is always one

1397
01:26:45,620 --> 01:26:47,390
so this is just through them

1398
01:26:47,410 --> 01:26:49,590
so you get for of ten gamma

1399
01:26:49,770 --> 01:26:52,620
this is the form of the case

1400
01:26:55,600 --> 01:26:59,000
hopefully that gives you the flavour of one applications

1401
01:26:59,010 --> 01:27:00,470
o of the band

1402
01:27:00,490 --> 01:27:05,390
the trick is in the case of linear functions the bounding the actual function class

1403
01:27:05,390 --> 01:27:07,880
itself linear functions is really quite

1404
01:27:07,890 --> 01:27:10,100
needs and straightforward

1405
01:27:10,120 --> 01:27:13,370
then the technicalities coming when you try and

1406
01:27:13,390 --> 01:27:17,620
you know translate that into the actual application that you're interested in

1407
01:27:17,620 --> 01:27:19,830
so in order to sort of

1408
01:27:19,840 --> 01:27:22,970
i that i thought it would be good to look at

1409
01:27:22,990 --> 01:27:27,300
one of the case in point of applying this and so i'd like to spend

1410
01:27:27,300 --> 01:27:28,540
the rest of the

1411
01:27:28,570 --> 01:27:31,090
time we have which is

1412
01:27:31,130 --> 01:27:32,610
roughly half an hour

1413
01:27:32,840 --> 01:27:34,700
just looking at kernel PCA

1414
01:27:34,710 --> 01:27:38,610
to give you a flavor of of another application hopefully get

1415
01:27:38,820 --> 01:27:42,800
you know sort of the idea that this is applicable technique and you could think

1416
01:27:42,800 --> 01:27:44,920
of applying in other areas as well

1417
01:27:44,950 --> 01:27:48,810
OK so how does kernel PCA work

1418
01:27:48,810 --> 01:27:51,050
what we do is

1419
01:27:51,070 --> 01:27:56,360
we we can actually compute the projection of a new point into the space spanned

1420
01:27:56,360 --> 01:27:59,490
by the i find back to the correlation matrix

1421
01:27:59,520 --> 01:28:05,080
this is the correlation matrix in the kernel defined feature space i'm assuming here five

1422
01:28:05,100 --> 01:28:07,930
projection into the kernel defined feature space

1423
01:28:08,430 --> 01:28:14,030
and we'd like to compute the if the projection on the i th i but

1424
01:28:14,040 --> 01:28:15,210
we can do that by this

1425
01:28:15,770 --> 01:28:22,540
computation that doesn't involve actually computing directly in the in the feature space

1426
01:28:22,550 --> 01:28:26,860
it involves the eigen vectors of the kernel matrix

1427
01:28:26,880 --> 01:28:31,280
and the idea value of that i corresponding eigen values

1428
01:28:31,290 --> 01:28:35,300
so we need to do i can analysis of the kernel matrix

1429
01:28:35,310 --> 01:28:39,620
and we then can get a dual representation in terms of that i can value

1430
01:28:39,760 --> 01:28:41,270
and i can but

1431
01:28:41,290 --> 01:28:45,180
so this computation gives us the projection of this new point

1432
01:28:45,210 --> 01:28:46,440
on to the

1433
01:28:46,450 --> 01:28:49,470
i th

1434
01:28:49,510 --> 01:28:52,370
i can vector in the feature space

1435
01:28:52,380 --> 01:28:53,920
and the idea values

1436
01:28:53,940 --> 01:28:59,400
corresponds to this is also the i value of that i can be in the

1437
01:28:59,410 --> 01:29:00,620
in the

1438
01:29:00,630 --> 01:29:05,430
i cannot find feature space so this is the idea the kernel PCA trick essentially

1439
01:29:05,940 --> 01:29:07,040
sort of

1440
01:29:07,040 --> 01:29:11,290
compute do the analysis in in the dual space in the

1441
01:29:11,300 --> 01:29:15,900
in terms of the kernel matrix and then compute projections as if you're working in

1442
01:29:15,900 --> 01:29:19,180
this feature space so i won't go into more detail on the

1443
01:29:19,240 --> 01:29:22,610
on the algorithm itself i'm sure that that's been covered

1444
01:29:22,650 --> 01:29:24,580
elsewhere in this

1445
01:29:24,620 --> 01:29:26,950
in the summer school

1446
01:29:28,570 --> 01:29:34,060
this implies that we can perform PCA in a kernel defined feature space

1447
01:29:34,780 --> 01:29:39,460
in the normal orthonormal basis given by the eigen vectors of this

1448
01:29:41,940 --> 01:29:50,600
normally PCA is applied to low dimensional feature spaces so normally i would imagine doing

1449
01:29:50,600 --> 01:29:54,800
it in this sort of thing you given explicit factor and say i want to

1450
01:29:54,850 --> 01:29:57,160
project from hundred dimensions in

1451
01:29:57,180 --> 01:29:58,640
five or something

1452
01:30:00,430 --> 01:30:05,210
suddenly applying in these very high dimensional feature spaces raises the question whether that's a

1453
01:30:05,210 --> 01:30:06,730
sensible thing to do

1454
01:30:06,740 --> 01:30:10,020
if there is any statistical reliability to the

1455
01:30:10,020 --> 01:30:12,370
actual future directions that you

1456
01:30:13,820 --> 01:30:18,560
so this comes down to be like a question for statistical

1457
01:30:18,580 --> 01:30:20,550
learning theory to analyse

1458
01:30:20,570 --> 01:30:25,310
to try to estimate to what extent that those future directions are sensible in terms

1459
01:30:25,310 --> 01:30:27,840
of capturing the data so that is essentially what

1460
01:30:27,880 --> 01:30:30,060
this application of

1461
01:30:30,130 --> 01:30:33,860
of rademacher complexity is attempting to achieve is attempting to try and

1462
01:30:33,880 --> 01:30:35,070
analyse that

1463
01:30:36,770 --> 01:30:42,470
so it should hopefully be able to highlight the critical elements that affect the quality

1464
01:30:42,470 --> 01:30:44,030
of the kernel PCA

1465
01:30:45,240 --> 01:30:49,120
and so what we can do is considered can perform PCA on a randomly drawn

1466
01:30:49,130 --> 01:30:50,360
set s

1467
01:30:50,360 --> 01:30:53,960
of size m in the feature space defined by the kernel kappa

1468
01:30:53,990 --> 01:30:57,430
we're going to project new data onto the space to be that

1469
01:30:57,510 --> 01:31:02,950
spanned by the first k remaining the largest k i can vectors corresponding to the

1470
01:31:02,950 --> 01:31:06,830
largest island is in in that features

1471
01:31:08,770 --> 01:31:13,850
and what we would like to be able to us to the following it is

1472
01:31:13,850 --> 01:31:15,830
the result which says that

1473
01:31:15,840 --> 01:31:21,560
what we're interested in here is trying to bound how well that features that

1474
01:31:21,730 --> 01:31:27,620
space have those projections are likely to capture new data so we're going to get

1475
01:31:27,630 --> 01:31:28,850
new data

1476
01:31:28,860 --> 01:31:31,580
we're gonna projected into that feature space

1477
01:31:31,680 --> 01:31:34,440
we learn from the samples

1478
01:31:35,120 --> 01:31:36,880
what we'd like to be able to say

1479
01:31:36,910 --> 01:31:41,480
is that that feature spaces that that subspaces are sensible

1480
01:31:41,530 --> 01:31:45,710
space to use the new data and that means we like to be able to

1481
01:31:45,710 --> 01:31:50,170
say that new data will have a significant projection in that space

1482
01:31:50,200 --> 01:31:52,040
therefore that it is orthogonal

1483
01:31:52,050 --> 01:31:53,670
projection into these

1484
01:31:53,690 --> 01:31:57,070
orthogonal space is relatively small

1485
01:31:57,070 --> 01:31:59,380
and what we've got this right hand side

1486
01:31:59,390 --> 01:32:04,830
this notation means the projection into the orthogonal that is the orthogonal complement of the

1487
01:32:05,600 --> 01:32:08,480
the hands of a randomly drawn

1488
01:32:08,510 --> 01:32:11,870
you know this is the expectation of that projection square

1489
01:32:11,890 --> 01:32:15,670
so this is the amount of information that is lost

1490
01:32:15,690 --> 01:32:20,350
if you like when we observe the new data point and start working with it

1491
01:32:20,360 --> 01:32:21,880
in the in this

1492
01:32:21,900 --> 01:32:26,020
projection space we lost information about it by projecting it now

1493
01:32:26,020 --> 01:32:31,000
project was started in nineteen eighty eight he there was the international project because i

1494
01:32:31,020 --> 01:32:34,540
think for me and also involving six countries

1495
01:32:34,560 --> 01:32:42,290
it was located there was competition between the initiate even private initiative and at the

1496
01:32:43,750 --> 01:32:48,580
in two thousand three it was officially consider the stability

1497
01:32:48,900 --> 01:32:53,100
so there some preliminary results published in two thousand one the web address of of

1498
01:32:53,100 --> 01:32:57,730
the human genome in two thousand three it was the final image and also the

1499
01:32:57,730 --> 01:33:00,350
official find that there are some technical terms here

1500
01:33:00,350 --> 01:33:05,330
what's called the final genomics when it's solved some accuracy in some this is the

1501
01:33:05,350 --> 01:33:06,500
so according to

1502
01:33:06,500 --> 01:33:12,290
the the standards of civility technology was finished in two thousand three and so on

1503
01:33:12,810 --> 01:33:17,770
sources distances the two thousand one five down all the draft of the human genome

1504
01:33:17,770 --> 01:33:20,420
which is you can download the sequence of letters

1505
01:33:22,270 --> 01:33:25,020
of DNA of the of the

1506
01:33:25,020 --> 01:33:30,170
after getting k so that the human genome eye focus a bit on the human

1507
01:33:30,170 --> 01:33:34,380
genome because it used to the about but also many other organisms that have been

1508
01:33:34,380 --> 01:33:41,040
sequenced single organism so you have longer genomes have been sequenced but overall the what

1509
01:33:41,040 --> 01:33:42,900
we remain in the history of

1510
01:33:42,900 --> 01:33:48,690
missus this denmark papers that is and the first thing dimension

1511
01:33:48,750 --> 01:33:51,770
so the question was whether or not we have to do so we can solve

1512
01:33:51,770 --> 01:33:57,200
everything we can cure people can find all the g is protein structure so the

1513
01:33:57,200 --> 01:34:02,330
thing that went very fast obviously that there was a boom in the field so

1514
01:34:02,330 --> 01:34:07,190
this is what the feel of bioinformatics really was expecting because because it's about from

1515
01:34:07,190 --> 01:34:09,980
it is roughly speaking refers to using

1516
01:34:10,000 --> 01:34:16,540
informatics technologies so conscious and sentient technology mathematics is to try to understand the processes

1517
01:34:16,600 --> 01:34:24,620
seeing is a computer this requires string analyses requires signal processing centre

1518
01:34:24,620 --> 01:34:28,890
and things went very fast so we started before of course but something before this

1519
01:34:28,890 --> 01:34:35,540
started earlier but the number of people voted by twenty five increase lots thousand

1520
01:34:35,560 --> 01:34:41,150
so the things that we saw for many successful applications of machine learning very only

1521
01:34:41,150 --> 01:34:43,230
for instance

1522
01:34:44,440 --> 01:34:47,580
so we some feature graphical models

1523
01:34:47,580 --> 01:34:52,400
remember all those sentiments are are welded these were like you know applications to find

1524
01:34:52,400 --> 01:34:58,020
genes in india is remembered DNA is a long strand of six billion liters you

1525
01:34:58,020 --> 01:35:02,940
know that you think it encodes the actual parts correspond to proteins

1526
01:35:02,960 --> 01:35:08,460
that could be and they and there were people the organisms based on HMS and

1527
01:35:08,500 --> 01:35:15,850
become models two given DNA sequences speech processing text processing that they were able to

1528
01:35:15,870 --> 01:35:19,440
it was quite electricity to detect the genes in the in the genomes of

1529
01:35:19,460 --> 01:35:21,500
there was an automatic ways to

1530
01:35:21,520 --> 01:35:24,400
to find the genes stop referring to his

1531
01:35:24,420 --> 01:35:27,500
actually write some good live

1532
01:35:27,620 --> 01:35:33,360
and there was some first finding that was that would disappointing for researchers first discovered

1533
01:35:33,360 --> 01:35:36,190
that a few years ago we thought they were

1534
01:35:36,270 --> 01:35:41,150
the there were many of the protein so one question was how many genes those

1535
01:35:41,150 --> 01:35:44,710
human contain it was an open question ten years ago

1536
01:35:44,810 --> 01:35:49,810
people speculated about one hundred thousand perhaps and each time a new draft came in

1537
01:35:50,040 --> 01:35:56,230
the shape of the know the the decreased so i think nowadays estimated field twenty

1538
01:35:56,230 --> 01:35:57,770
five thousand

1539
01:35:57,770 --> 01:36:00,580
so that's also number twenty nine

1540
01:36:00,650 --> 01:36:02,400
if you take the

1541
01:36:02,420 --> 01:36:04,040
cool the human genome

1542
01:36:04,060 --> 01:36:09,210
which one of the function of which is to encode proteins there are fifteen twenty

1543
01:36:09,210 --> 01:36:12,210
five thousand positions which are called genes

1544
01:36:12,640 --> 01:36:15,060
that encode proteins

1545
01:36:15,080 --> 01:36:19,170
in fact it was a disappointing for for many people because it's not

1546
01:36:19,170 --> 01:36:22,730
it's not so much compared to to other organisms just take more than a slightly

1547
01:36:23,310 --> 01:36:26,620
more than a simple is equal to

1548
01:36:26,640 --> 01:36:28,620
two monkeys

1549
01:36:28,960 --> 01:36:34,730
so that's what we discover and so something more that we discovered that five these

1550
01:36:34,810 --> 01:36:38,500
these twenty five genes that are possible in a on the cover

1551
01:36:38,500 --> 01:36:41,370
of the order of of one percent of the gene

1552
01:36:41,370 --> 01:36:45,770
so currently we don't know exactly what the remaining so there are other parts which

1553
01:36:45,770 --> 01:36:50,830
are not genes but we know functions but in the current season operators

1554
01:36:50,830 --> 01:36:55,880
there's a limit of ninety seven percent of the genome that is considered as

1555
01:36:55,900 --> 01:37:01,000
j DNA just we don't know what it is worth ninety some people think that

1556
01:37:01,040 --> 01:37:05,000
it is used this is just that evolution doesn't care about having been genomes so

1557
01:37:05,020 --> 01:37:08,770
we cannot begin until only one of two percent of it is useful

1558
01:37:08,770 --> 01:37:13,850
but this is of course speculation with an idea of where what it is

1559
01:37:16,480 --> 01:37:23,270
he is also a the centre of terminology there is a debate here about twenty

1560
01:37:23,270 --> 01:37:25,370
five thousand genes which is

1561
01:37:26,140 --> 01:37:28,690
roughly speaking twenty five thousand

1562
01:37:28,690 --> 01:37:32,770
places in DNA that codes for protein now when you go from

1563
01:37:32,790 --> 01:37:36,730
the province had the simple just that one

1564
01:37:36,750 --> 01:37:43,020
DNA a gene is transcribed into one RNA and one protein but in fact there

1565
01:37:43,040 --> 01:37:45,500
are many processes that that means that one

1566
01:37:45,580 --> 01:37:51,480
jean DNA can respond to several proteins but that things called alternative splicing that says

1567
01:37:51,480 --> 01:37:54,780
that you're not allowed to take all the gene so when you out there and

1568
01:37:54,780 --> 01:37:59,500
a process can be cut extended there are so we see is good is going

1569
01:38:00,270 --> 01:38:01,880
we were come said

1570
01:38:02,080 --> 01:38:03,960
what is the genes

1571
01:38:06,620 --> 01:38:08,370
OK so

1572
01:38:08,440 --> 01:38:10,000
so this was the genome

1573
01:38:10,040 --> 01:38:13,230
and that was a big motivation for

1574
01:38:13,250 --> 01:38:17,000
it but in fact something even bigger was hidden behind the

1575
01:38:17,000 --> 01:38:17,790
which is

1576
01:38:17,790 --> 01:38:22,520
sensible set of new technologies and there were some of the countries with the

1577
01:38:22,960 --> 01:38:28,730
advances so they were taken committees and there were things that were made possible the

1578
01:38:28,730 --> 01:38:35,150
genome sequencing projects and these features show you just some of the new technologies that

1579
01:38:35,190 --> 01:38:41,730
that have been developed for recently so in the last ten years for sure and

1580
01:38:41,770 --> 01:38:47,000
that would be very useful for bioinformatics because of technologies to say that you know

1581
01:38:47,020 --> 01:38:48,500
the capacity to two

1582
01:38:48,540 --> 01:38:50,920
generate huge amounts of data

1583
01:38:50,980 --> 01:38:53,560
and this data with with the starting point of

1584
01:38:53,580 --> 01:38:55,870
research in bioinformatics

1585
01:38:55,880 --> 01:38:56,790
so this is

1586
01:38:57,080 --> 01:39:01,270
sequence are you know this is what they can when they a sell you obtain

1587
01:39:01,390 --> 01:39:05,500
your ten is just such that for the human genome it was

1588
01:39:05,520 --> 01:39:10,500
thirteen years initiate even because it would be in the first thing basically the sequence

1589
01:39:10,500 --> 01:39:11,940
and not fall

1590
01:39:11,940 --> 01:39:14,100
the other four hundred thousand dollars

1591
01:39:15,230 --> 01:39:21,080
it is very some companies of the world are because the countries evidence so now

1592
01:39:21,080 --> 01:39:23,920
this is doing continue consequence of cats

1593
01:39:23,940 --> 01:39:25,190
if you want is very

1594
01:39:25,210 --> 01:39:27,500
ricci is to

1595
01:39:27,520 --> 01:39:30,960
so it is becoming a major technology

1596
01:39:30,960 --> 01:39:33,330
now all all these features

1597
01:39:33,560 --> 01:39:36,000
technologies like this

1598
01:39:36,020 --> 01:39:37,670
now we know how to

1599
01:39:37,690 --> 01:39:42,170
two is to sequence who enhanced and we also because we have sequence of the

1600
01:39:42,170 --> 01:39:46,370
genome with some of the positions in the DNA that vary between individuals

1601
01:39:46,400 --> 01:39:50,650
so no this is possible to take many individuals many people like you and me

1602
01:39:50,870 --> 01:39:55,580
and to focus on the other positive our between g humans who twelve you know

1603
01:39:55,600 --> 01:39:57,020
your genome

1604
01:39:57,040 --> 01:39:59,520
i think i like what are your letters

1605
01:39:59,520 --> 01:40:00,960
in the position that

1606
01:40:00,960 --> 01:40:03,330
humans and this is a unique identifier for

1607
01:40:03,330 --> 01:40:06,800
is the set of all probability dist

1608
01:40:06,850 --> 01:40:08,700
well both statements

1609
01:40:12,160 --> 01:40:15,770
in one particular probability distribution where those

1610
01:40:15,790 --> 01:40:18,920
statements are true is a particular element

1611
01:40:18,970 --> 01:40:21,290
the particle instance

1612
01:40:21,320 --> 01:40:23,290
of graphical models

1613
01:40:25,880 --> 01:40:29,720
it's important to have that in mind because in the context of learning when we

1614
01:40:29,720 --> 01:40:33,810
are actually parliament rise in these people by some people

1615
01:40:33,850 --> 01:40:36,270
and we are doing learning or very theta

1616
01:40:36,330 --> 01:40:37,320
according to

1617
01:40:37,390 --> 01:40:39,310
we are changing actually

1618
01:40:39,360 --> 01:40:42,560
itself the model right

1619
01:40:42,570 --> 01:40:44,290
when we look

1620
01:40:44,480 --> 01:40:50,790
we are trying to find reach particle model instance of that class of graphs

1621
01:40:50,840 --> 01:40:54,270
these things things that gives us for example

1622
01:40:54,330 --> 01:41:00,910
much like for the data like

1623
01:41:01,000 --> 01:41:03,990
so these are graphical model

1624
01:41:04,000 --> 01:41:05,210
and again

1625
01:41:05,220 --> 01:41:09,640
the questions will be interesting estimating parameters for the models

1626
01:41:09,680 --> 01:41:14,660
computing probabilities of things in the model of particle outcomes of particular

1627
01:41:17,840 --> 01:41:23,340
i find particularly interesting realizations for example the MAP assignment map assigns knocking out that

1628
01:41:23,340 --> 01:41:27,260
just the configuration with maximal probability

1629
01:41:30,660 --> 01:41:32,660
and in order to manipulate

1630
01:41:32,660 --> 01:41:37,260
such a probabilistic model we need

1631
01:41:37,310 --> 01:41:39,950
we need to know about the base basically

1632
01:41:40,010 --> 01:41:44,860
we have p of x which is of probability distribution right

1633
01:41:44,900 --> 01:41:48,980
not to compute things with p of x we need to know the structure of

1634
01:41:49,730 --> 01:41:52,350
your facts to represent

1635
01:41:53,660 --> 01:41:55,850
with the representation of facts

1636
01:41:55,940 --> 01:41:57,590
we saw if we have

1637
01:41:59,640 --> 01:42:03,890
what the representation the representations of position over two

1638
01:42:03,900 --> 01:42:06,650
small functions of two via

1639
01:42:07,560 --> 01:42:11,120
if we have conditional independence

1640
01:42:11,130 --> 01:42:12,640
we will see some

1641
01:42:12,690 --> 01:42:16,330
what the representation will be when have an arbitrary number of

1642
01:42:16,370 --> 01:42:20,130
and now we also need to find ways of computing

1643
01:42:20,180 --> 01:42:24,990
things efficiently with these representation p of x the

1644
01:42:25,000 --> 01:42:30,180
right because that the critical thing we saw that if p of x is not

1645
01:42:30,220 --> 01:42:32,660
simplified in anyway

1646
01:42:32,690 --> 01:42:35,800
if you just assume we have the original your facts

1647
01:42:35,800 --> 01:42:41,090
we all the variables and we have we don't have any structure on probability

1648
01:42:41,100 --> 01:42:43,560
we want to compute p of x i

1649
01:42:43,610 --> 01:42:48,860
we need to sum over all the instances where x i hope

1650
01:42:48,910 --> 01:42:53,100
that's an exponential sums not feasible so basically

1651
01:42:53,100 --> 01:43:01,160
we will actually need find new to find ways of computing efficiently without our graphical

1652
01:43:01,160 --> 01:43:04,190
so these are the questions that we need

1653
01:43:06,830 --> 01:43:08,730
so let's start

1654
01:43:08,750 --> 01:43:12,000
but the rest is questioned by thinking about some exercise

1655
01:43:12,020 --> 01:43:19,270
in question so far we can continue

1656
01:43:19,320 --> 01:43:22,080
let's think about an exercise

1657
01:43:32,420 --> 01:43:35,580
assume you have been provided the initial everybody

1658
01:43:36,770 --> 01:43:41,040
let's assume that this particular conditional independence statements

1659
01:43:43,830 --> 01:43:44,940
this particular

1660
01:43:44,960 --> 01:43:48,640
x one independent think forgiven actually

1661
01:43:58,020 --> 01:44:00,770
what you can do

1662
01:44:00,810 --> 01:44:03,140
you can

1663
01:44:05,500 --> 01:44:09,060
as the factorized form these problems but people

1664
01:44:09,120 --> 01:44:11,170
conditional independence

1665
01:44:11,210 --> 01:44:12,310
which is given

1666
01:44:12,310 --> 01:44:14,190
by these

1667
01:44:14,250 --> 01:44:16,640
is one of the ways to represent this particular

1668
01:44:20,400 --> 01:44:23,870
this thing is independent of this thing given this thing here

1669
01:44:23,870 --> 01:44:28,580
you're just suppress the second thing here have p of x one x three

1670
01:44:28,790 --> 01:44:33,120
p of x one and p of extreme conditions on x next sort

1671
01:44:33,140 --> 01:44:36,190
this is just another break algebraic

1672
01:44:36,250 --> 01:44:38,850
description of this concept

1673
01:44:38,940 --> 01:44:42,750
according to the definition of conditional independence that this often previously

1674
01:44:45,460 --> 01:44:47,500
let's expand here

1675
01:44:47,560 --> 01:44:51,600
these expression of p of x one two and three

1676
01:44:52,640 --> 01:44:53,870
what we have

1677
01:44:55,210 --> 01:44:57,000
i have basically the following

1678
01:44:57,210 --> 01:45:01,190
p of x one given x two

1679
01:45:01,250 --> 01:45:03,170
p of x one three

1680
01:45:03,190 --> 01:45:05,080
p of x

1681
01:45:05,160 --> 01:45:07,520
well we really needed to see the

1682
01:45:07,600 --> 01:45:10,480
i probably sleep type is OK

1683
01:45:11,100 --> 01:45:14,040
this is based this will be just a few of exterior

1684
01:45:19,960 --> 01:45:24,670
i i wanted it now because this is latex and this compiler have to comply

1685
01:45:24,980 --> 01:45:28,580
so this should be p of x o

1686
01:45:28,640 --> 01:45:31,040
and then immediately just the disease

1687
01:45:31,060 --> 01:45:32,540
these first

1688
01:45:32,600 --> 01:45:37,420
it is given by the

1689
01:45:37,480 --> 01:45:39,900
and then you will think these factorizations

1690
01:45:41,540 --> 01:45:43,370
good so

1691
01:45:43,390 --> 01:45:47,460
immediately we see that we have here some type of factorizations

1692
01:45:47,460 --> 01:45:52,390
but has arise from a given conditional independence states

1693
01:45:52,540 --> 01:45:55,500
well we start to see that these structures

1694
01:46:00,790 --> 01:46:03,250
the sample space

1695
01:46:03,310 --> 01:46:04,940
is such that

1696
01:46:04,940 --> 01:46:07,500
the joint probability distribution

1697
01:46:07,540 --> 01:46:11,460
he's factorized into some particular function

1698
01:46:11,580 --> 01:46:14,810
so that's the structure that's the structure continued to exploit

1699
01:46:14,830 --> 01:46:19,520
we have some factorizations here so now when we are going to compute or marginal

1700
01:46:19,830 --> 01:46:24,730
for example when we are with some these functions over x one or x two

1701
01:46:24,730 --> 01:46:28,330
and x three compute that some that was exponential

1702
01:46:28,370 --> 01:46:31,750
if we didn't have any simplification

1703
01:46:33,920 --> 01:46:35,870
actually have apparently

1704
01:46:35,890 --> 01:46:37,870
to take advantage of the fact

1705
01:46:38,580 --> 01:46:40,640
we have a factorized form

1706
01:46:40,690 --> 01:46:42,640
for these objects

1707
01:46:42,690 --> 01:46:45,710
in order to compute those things more efficient

1708
01:46:45,710 --> 01:46:48,330
and that's the basic of exact inference in

1709
01:46:48,350 --> 01:46:51,600
graphical models

1710
01:46:53,560 --> 01:46:54,750
sorry for that

1711
01:46:54,750 --> 01:46:58,020
and to just flowed through graph to see when

1712
01:46:58,040 --> 01:46:59,770
functions the same thing

1713
01:46:59,790 --> 01:47:05,000
well we're almost a mincut maxflow thing here the observations supposing you've got a graph

1714
01:47:05,000 --> 01:47:09,930
with positive weights with positive weight in which there is no positive flow from fourth

1715
01:47:11,040 --> 01:47:13,270
they got a graph is no fly

1716
01:47:16,180 --> 01:47:21,910
what what is that name

1717
01:47:21,960 --> 01:47:25,620
one zero sum center nodes here and i can get

1718
01:47:25,640 --> 01:47:29,060
see on the top one on the bottom

1719
01:47:29,060 --> 01:47:33,460
but there are some nodes i can get to from zero

1720
01:47:33,500 --> 01:47:37,710
and there are some nodes i can call those b one

1721
01:47:37,750 --> 01:47:42,930
i'm not saying get to one there's something get zero call is easier

1722
01:47:43,020 --> 01:47:45,580
now that means there's no edge

1723
01:47:45,600 --> 01:47:49,000
going across there with positive way

1724
01:47:49,120 --> 01:47:54,290
because otherwise i could get from zero through part positive way

1725
01:47:54,330 --> 01:47:55,790
the zero

1726
01:47:55,830 --> 01:47:57,750
means said

1727
01:47:57,890 --> 01:48:01,580
the zero the all the nodes can be reached from zero and it has a

1728
01:48:01,600 --> 01:48:06,620
nonzero weight with positive weight and he's the one rank that means there is no

1729
01:48:07,750 --> 01:48:10,680
going across here positive way otherwise

1730
01:48:10,730 --> 01:48:15,680
can't the definition that node being easier not the one and hence

1731
01:48:15,730 --> 01:48:16,660
the sum

1732
01:48:16,680 --> 01:48:17,770
of all

1733
01:48:17,810 --> 01:48:22,000
the edges which go from the zero to the one

1734
01:48:22,040 --> 01:48:23,890
they have zero weight

1735
01:48:23,980 --> 01:48:26,690
so the costs of that partition

1736
01:48:29,160 --> 01:48:36,000
and since it since has cost zero must be the minimum cost partition because

1737
01:48:36,020 --> 01:48:40,250
got positive weights there cannot be a partition of the graph which has less

1738
01:48:40,270 --> 01:48:41,690
this way so

1739
01:48:42,370 --> 01:48:43,810
my graph

1740
01:48:43,850 --> 01:48:48,600
has this property there's no path from zero to one defining the

1741
01:48:48,620 --> 01:48:50,810
the best partition is trivial

1742
01:48:52,120 --> 01:48:55,210
find the ones you can get from zero and that is the best

1743
01:48:55,250 --> 01:48:57,620
partition has zero cost right

1744
01:48:57,620 --> 01:48:59,430
so what about

1745
01:48:59,500 --> 01:49:03,250
that's the proof so what good is that well

1746
01:49:03,250 --> 01:49:05,830
if you want to find the minimum cost partition

1747
01:49:05,830 --> 01:49:08,020
you start out the graph

1748
01:49:08,060 --> 01:49:10,000
pushing flows

1749
01:49:10,000 --> 01:49:11,690
from zero to one

1750
01:49:12,600 --> 01:49:15,160
no more

1751
01:49:15,230 --> 01:49:17,000
no more is possible

1752
01:49:17,040 --> 01:49:20,960
so i i find afloat

1753
01:49:20,980 --> 01:49:23,910
which goes from zero to one

1754
01:49:23,960 --> 01:49:25,210
by reweight

1755
01:49:25,210 --> 01:49:28,350
the graph according to that flow

1756
01:49:29,660 --> 01:49:33,460
my previous theorem said that represents the same function

1757
01:49:34,040 --> 01:49:37,460
usually one except the constant of flow right

1758
01:49:38,790 --> 01:49:42,290
it doesn't matter whether i try to find the minimum partition on the original graph

1759
01:49:42,290 --> 01:49:45,460
all the final graph families others will do it on the final graph which trivial

1760
01:49:45,480 --> 01:49:47,100
so my algorithm is

1761
01:49:47,120 --> 01:49:49,660
bush flow until i can't push and more

1762
01:49:49,660 --> 01:49:50,950
then label

1763
01:49:50,960 --> 01:49:51,750
according to

1764
01:49:51,830 --> 01:49:54,450
as you get different zero

1765
01:49:54,500 --> 01:49:57,370
it's easier one

1766
01:49:57,430 --> 01:50:01,180
and that will be the minimum value of the original function

1767
01:50:01,190 --> 01:50:09,270
so here's an example of this max flow algorithms

1768
01:50:09,290 --> 01:50:11,250
taking the taking graph

1769
01:50:11,250 --> 01:50:13,290
on the left

1770
01:50:13,290 --> 01:50:14,680
which has the a weight

1771
01:50:14,690 --> 01:50:17,500
now look at some

1772
01:50:17,540 --> 01:50:20,890
the red arrows i can obviously pushed two units there

1773
01:50:20,890 --> 01:50:23,350
six two six push

1774
01:50:23,410 --> 01:50:24,830
two units afloat

1775
01:50:24,850 --> 01:50:27,520
along that that's six comes for

1776
01:50:27,620 --> 01:50:30,190
two become zero remember have to

1777
01:50:31,520 --> 01:50:33,460
backward graph with a zero

1778
01:50:33,520 --> 01:50:37,080
and the a six so now i have a graph

1779
01:50:37,120 --> 01:50:38,950
with small weight

1780
01:50:38,960 --> 01:50:40,930
so let's try and

1781
01:50:40,960 --> 01:50:43,810
find flow in that the right hand one

1782
01:50:43,830 --> 01:50:49,160
i that three four what pushed three through that they get

1783
01:50:49,230 --> 01:50:51,790
two after finally in the middle

1784
01:50:51,830 --> 01:50:55,960
the more complex more ground left across the

1785
01:50:56,000 --> 01:51:00,160
with one minute and i get to that graph on the right now you see

1786
01:51:01,080 --> 01:51:02,540
graph zero

1787
01:51:02,660 --> 01:51:05,640
there's no way i can follow those errors

1788
01:51:05,730 --> 01:51:07,540
and get to one

1789
01:51:08,600 --> 01:51:10,120
so i cannot

1790
01:51:10,160 --> 01:51:11,980
do any more

1791
01:51:12,120 --> 01:51:13,640
pushing more

1792
01:51:13,640 --> 01:51:17,040
so let's see what the nodes

1793
01:51:17,060 --> 01:51:21,350
by following errors which can get i can get two x one from zero to

1794
01:51:21,350 --> 01:51:24,330
take the arrow cross there to get to x two

1795
01:51:24,390 --> 01:51:28,430
but i can't get out of extreme errors going direction so

1796
01:51:28,480 --> 01:51:33,080
my solutions x one x two equals zero and x three one

1797
01:51:33,160 --> 01:51:38,250
that is the minimum cost of the function represented by the graph

1798
01:51:38,250 --> 01:51:40,870
on the left the same function

1799
01:51:40,890 --> 01:51:43,180
on the right in seven months

1800
01:51:43,210 --> 01:51:45,500
so here's a summary

1801
01:51:45,540 --> 01:51:51,560
all quadratic pseudo boolean functions can be expressed graphs right the function values decrease cost

1802
01:51:51,560 --> 01:51:53,060
corresponding partition

1803
01:51:53,100 --> 01:51:54,460
show that

1804
01:51:54,520 --> 01:51:57,410
four way to pass the the min cut in the graph is equal to the

1805
01:51:57,410 --> 01:52:01,180
maximum permissible flow

1806
01:52:01,230 --> 01:52:04,160
and you can be parametrized graph reweighted

1807
01:52:04,160 --> 01:52:07,760
and the message that f three cents x four is

1808
01:52:07,800 --> 01:52:12,050
according to again this rule is the product of the messages that got in times

1809
01:52:12,050 --> 01:52:15,410
the function itself summing up all the other variables

1810
01:52:15,470 --> 01:52:20,470
so is the product of this message and this message that it got in

1811
01:52:21,600 --> 01:52:23,280
times the function

1812
01:52:23,300 --> 01:52:26,890
its own function f three which is a function of the variables x two and

1813
01:52:26,890 --> 01:52:27,760
x four

1814
01:52:27,870 --> 01:52:30,820
and then summing up all the other variables

1815
01:52:30,990 --> 01:52:34,080
that is something i x two

1816
01:52:34,140 --> 01:52:38,370
so this is the final message that f three cents x four

1817
01:52:41,390 --> 01:52:43,410
prove yourself

1818
01:52:43,470 --> 01:52:46,050
that if you take this

1819
01:52:46,050 --> 01:52:46,850
and you

1820
01:52:46,870 --> 01:52:51,340
normalize it to sum to one over the values of x four

1821
01:52:51,350 --> 01:52:54,850
that will be the probability distribution over x four

1822
01:52:54,890 --> 01:52:57,390
represented by the factor graph

1823
01:53:01,010 --> 01:53:03,680
now what about if we observe some evidence

1824
01:53:03,680 --> 01:53:09,120
let's say we observed that x one takes on the value a

1825
01:53:09,120 --> 01:53:10,970
you know this a

1826
01:53:10,970 --> 01:53:14,010
x one is the binary variables a is zero

1827
01:53:14,030 --> 01:53:15,320
or something like that

1828
01:53:15,320 --> 01:53:18,970
now how does this change those probabilities well

1829
01:53:20,370 --> 01:53:24,910
you initialise all messages to be one but then when you observe x one takes

1830
01:53:24,910 --> 01:53:26,780
on the value a

1831
01:53:26,930 --> 01:53:32,070
then the message that x one sensor f one

1832
01:53:32,910 --> 01:53:36,720
a delta function on the value x y equals a

1833
01:53:39,120 --> 01:53:41,030
x one is the binary

1834
01:53:41,050 --> 01:53:44,120
valuable taking on the value zero and one

1835
01:53:44,140 --> 01:53:47,890
instead of sending the vector of all ones the messages

1836
01:53:47,930 --> 01:53:49,510
one zero

1837
01:53:51,240 --> 01:53:54,850
that's just too

1838
01:53:54,930 --> 01:53:56,600
to represent the fact that

1839
01:53:56,620 --> 01:54:00,820
x one takes on the value zero

1840
01:54:00,870 --> 01:54:03,100
and then everything else

1841
01:54:03,410 --> 01:54:05,680
falls through

1842
01:54:05,740 --> 01:54:10,190
in exactly the same way but because this is the delta function on x one

1843
01:54:10,190 --> 01:54:12,890
taking on the value a

1844
01:54:12,910 --> 01:54:14,300
when you

1845
01:54:14,300 --> 01:54:17,300
he said the message from f one x two

1846
01:54:18,760 --> 01:54:21,600
product of that message has the factor here

1847
01:54:21,660 --> 01:54:22,840
is just

1848
01:54:22,850 --> 01:54:25,800
the function f one

1849
01:54:25,800 --> 01:54:29,010
as a function of x two but with the value x one

1850
01:54:29,010 --> 01:54:32,100
set to a

1851
01:54:32,160 --> 01:54:36,620
all this falls through and this is what you get is the message after reset

1852
01:54:36,620 --> 01:54:41,800
x four and again once normalized this as a function of x four

1853
01:54:41,840 --> 01:54:44,410
you will be able to confirm that this is

1854
01:54:44,430 --> 01:54:48,970
the probability of x four given x y equals a

1855
01:54:50,570 --> 01:54:53,160
so any questions about this

1856
01:54:53,160 --> 01:54:56,700
i'm sure this is pretty fast and it's quite hard to follow all of this

1857
01:54:57,010 --> 01:55:00,010
but i want to give you if you have encountered this

1858
01:55:00,070 --> 01:55:03,410
before i want to give you a sense for the algorithms

1859
01:55:03,430 --> 01:55:06,910
we're sending messages on these graphs and how

1860
01:55:06,910 --> 01:55:09,850
they can correspond to computing probabilities

1861
01:55:10,070 --> 01:55:13,300
on these graphs

1862
01:55:14,450 --> 01:55:17,120
any questions

1863
01:55:18,850 --> 01:55:21,070
everybody is ready for lunch

1864
01:55:21,080 --> 01:55:24,220
all right

1865
01:55:27,870 --> 01:55:32,950
so i talked about factor graphs and directed graphs in the abstract

1866
01:55:33,780 --> 01:55:38,280
you actually see them all over the place in the machine learning and pattern recognition

1867
01:55:39,320 --> 01:55:43,160
in fact a lot of models that are very familiar

1868
01:55:43,180 --> 01:55:44,050
two as

1869
01:55:44,070 --> 01:55:50,490
like hidden markov models and linear gaussians state space models can be very

1870
01:55:50,530 --> 01:55:52,620
nicely represented as

1871
01:55:52,620 --> 01:55:58,070
directed graphs are factor graphs so here is the directed graph for

1872
01:55:58,160 --> 01:56:02,200
a hidden markov model which is also the same

1873
01:56:03,530 --> 01:56:07,800
four state space model common filter model

1874
01:56:07,910 --> 01:56:12,700
you have a bunch of hidden state variables and you have a bunch of observations

1875
01:56:12,700 --> 01:56:15,970
in your modelling time series of these observations

1876
01:56:16,050 --> 01:56:22,450
shaded in these nodes represent the you've observed these guys and these guys are hidden

1877
01:56:22,450 --> 01:56:26,340
connection to do so will have on the fact that is a function only on

1878
01:56:26,340 --> 01:56:28,740
the bottom of the

1879
01:56:28,890 --> 01:56:32,870
now let's suppose that want to compute the matching

1880
01:56:33,340 --> 01:56:35,070
the distribution of a

1881
01:56:35,090 --> 01:56:40,950
well this is the sum over all of the remaining viable of the joint distribution

1882
01:56:40,950 --> 01:56:45,070
now i can write down my factorizations

1883
01:56:45,120 --> 01:56:47,930
there the factor graph

1884
01:56:47,950 --> 01:56:52,950
and performed the same mission and doing that i realized that we need to perform

1885
01:56:52,950 --> 01:56:54,640
to the power of three

1886
01:56:54,700 --> 01:56:56,590
some nations that means

1887
01:56:56,610 --> 01:57:02,760
number of submission which is this potentially in the number of viable

1888
01:57:02,820 --> 01:57:05,180
i have to some of

1889
01:57:05,200 --> 01:57:09,700
now though we can think of a bit more clever way of doing that so

1890
01:57:09,720 --> 01:57:10,740
if we

1891
01:57:10,740 --> 01:57:12,300
look for example

1892
01:57:12,300 --> 01:57:16,990
this first that it does not depend on the them means that also the second

1893
01:57:16,990 --> 01:57:20,010
term doesn't depend on the them is that we can push

1894
01:57:20,030 --> 01:57:21,910
this estimation

1895
01:57:21,910 --> 01:57:23,350
here before

1896
01:57:24,570 --> 01:57:26,220
after three fact

1897
01:57:26,240 --> 01:57:29,700
and similarly for the estimation so i will end up there

1898
01:57:29,720 --> 01:57:31,780
i can distribute

1899
01:57:31,800 --> 01:57:36,050
push this summation further and i feel if i look

1900
01:57:36,070 --> 01:57:40,470
i have to compute here to some of the the one we said despite over

1901
01:57:40,510 --> 01:57:45,890
to some plus two something you just something that means that we have two times

1902
01:57:45,890 --> 01:57:47,450
three some missions

1903
01:57:47,470 --> 01:57:50,990
and that means that the number of mission which is non linear in the

1904
01:57:51,010 --> 01:57:54,640
and the number of viable to some

1905
01:57:54,660 --> 01:57:58,350
so we we gain something if we get only two in this case but

1906
01:57:58,410 --> 01:58:01,010
you can imagine that if you have a big graphs

1907
01:58:01,030 --> 01:58:03,090
you can save a lot of computations

1908
01:58:03,120 --> 01:58:07,610
well we can see this procedure the kind of message passing

1909
01:58:07,800 --> 01:58:09,700
so what

1910
01:58:09,760 --> 01:58:11,870
we can define an

1911
01:58:11,940 --> 01:58:19,570
this there was some excerpts from the bible the to the valuable c

1912
01:58:19,590 --> 01:58:22,200
and on this before from here then

1913
01:58:22,220 --> 01:58:24,740
so i i can define

1914
01:58:26,320 --> 01:58:30,910
as an aside from the bible it c to the right it would be

1915
01:58:30,930 --> 01:58:33,200
and finally this

1916
01:58:33,220 --> 01:58:34,410
as the methods

1917
01:58:34,430 --> 01:58:36,280
from b to a so

1918
01:58:36,280 --> 01:58:39,340
basically what i can do this us

1919
01:58:39,370 --> 01:58:41,970
passing messages

1920
01:58:42,050 --> 01:58:44,570
from the body will be up

1921
01:58:44,590 --> 01:58:49,280
to the reliable eight

1922
01:58:49,370 --> 01:58:52,820
now if you think about him to compute the

1923
01:58:52,890 --> 01:58:56,320
join this to the margin of c instead

1924
01:58:56,320 --> 01:58:58,910
we will have to pass messages in

1925
01:58:59,140 --> 01:59:03,220
names in bold direction coming coming from this

1926
01:59:03,240 --> 01:59:08,550
so from all the factor that lead to see in this case there are two

1927
01:59:08,570 --> 01:59:11,590
the fact that i think this is to pass messages

1928
01:59:11,640 --> 01:59:14,220
from the end of messages i can

1929
01:59:14,240 --> 01:59:16,110
i can see that

1930
01:59:16,120 --> 01:59:18,890
in a similar way

1931
01:59:21,010 --> 01:59:24,640
in the more complicated cases in which i have bunching three

1932
01:59:24,660 --> 01:59:28,910
the minister for example you factor is a function of three nodes

1933
01:59:28,970 --> 01:59:34,660
things get a bit more complicated than i have is not sufficient to define available

1934
01:59:34,660 --> 01:59:41,030
to viral messages that has befallen to define the fact that the bible message and

1935
01:59:41,030 --> 01:59:43,300
divided would factor measurements

1936
01:59:46,530 --> 01:59:47,740
the cl

1937
01:59:47,740 --> 01:59:50,680
you define these kind of messages

1938
01:59:52,510 --> 01:59:55,160
let's look at the bible to factor such

1939
01:59:55,180 --> 01:59:57,610
if a viable here

1940
01:59:57,620 --> 01:59:59,990
i want to compute

1941
02:00:00,030 --> 02:00:02,470
the most to from the bottom of the

1942
02:00:02,470 --> 02:00:05,010
to the fact that as well

1943
02:00:05,010 --> 02:00:07,800
in order to do that

1944
02:00:07,800 --> 02:00:09,930
i have to multiply

1945
02:00:09,950 --> 02:00:11,990
the i

1946
02:00:12,050 --> 02:00:15,930
coming the messages from the front door

1947
02:00:16,530 --> 02:00:17,850
which are

1948
02:00:17,870 --> 02:00:22,260
the link with the body will be so f one vessel into the

1949
02:00:22,300 --> 02:00:26,010
two answering to be able to compute the message from one two

1950
02:00:26,090 --> 02:00:29,990
the the message from two to be multiply them

1951
02:00:29,990 --> 02:00:35,320
and then i can send i o thing the message from b two

1952
02:00:35,370 --> 02:00:38,930
is that clear

1953
02:00:40,780 --> 02:00:42,030
if i want

1954
02:00:42,030 --> 02:00:44,720
two cents

1955
02:00:44,760 --> 02:00:46,870
and this that from

1956
02:00:46,870 --> 02:00:50,180
fact or to everybody so from that too

1957
02:00:51,640 --> 02:00:54,180
i will have to gain

1958
02:00:54,410 --> 02:00:57,800
compute the messages from

1959
02:00:57,820 --> 02:01:00,510
in this case we want to have

1960
02:01:00,550 --> 02:01:02,120
two two

1961
02:01:02,160 --> 02:01:04,570
so all the messages

1962
02:01:04,660 --> 02:01:09,430
from the all the valuable to define exact

1963
02:01:09,470 --> 02:01:12,570
obviously the the masses from the two

1964
02:01:12,620 --> 02:01:14,680
more to apply

1965
02:01:14,700 --> 02:01:18,620
multiply by then this the that's with a

1966
02:01:18,640 --> 02:01:20,910
i i right here

1967
02:01:20,910 --> 02:01:22,300
then multiply

1968
02:01:22,320 --> 02:01:24,160
these two by

1969
02:01:24,180 --> 02:01:26,010
by the fact

1970
02:01:26,030 --> 02:01:27,740
and then some over

1971
02:01:27,740 --> 02:01:31,990
the two rival here

1972
02:01:32,030 --> 02:01:33,700
and you can see that what

1973
02:01:34,970 --> 02:01:39,180
we have someone here is exactly the same you can take a look at that

1974
02:01:39,180 --> 02:01:40,680
more closely

1975
02:01:41,510 --> 02:01:43,450
released in the slide

1976
02:01:43,470 --> 02:01:47,160
and if you want to compute the marginal as we said we are two

1977
02:01:47,220 --> 02:01:48,510
i just think the

1978
02:01:48,510 --> 02:01:53,300
the product of all factor valuable message

1979
02:01:53,300 --> 02:01:55,350
for the fact that link to this

1980
02:01:58,280 --> 02:02:04,390
now let's give a little bit of an example of inference in hidden markov model

1981
02:02:04,570 --> 02:02:06,890
which is a very popular models

