1
00:00:00,000 --> 00:00:03,890
this is what we want to bound again the offline algorithm

2
00:00:08,800 --> 00:00:11,940
if you just want to minimize this it puts you things at that the average

3
00:00:11,940 --> 00:00:15,210
but in a more fancy one if if you want to compute an offline output

4
00:00:15,210 --> 00:00:16,110
and now

5
00:00:16,150 --> 00:00:18,590
what you would want to do always

6
00:00:18,630 --> 00:00:20,420
you would also

7
00:00:20,440 --> 00:00:25,310
maybe at some kind of prior or some kind of finish divergence to some initial

8
00:00:25,310 --> 00:00:26,850
weight vector

9
00:00:26,860 --> 00:00:29,260
and if you minimize this

10
00:00:29,260 --> 00:00:33,440
this is not the bregman divergences ricci is this is the

11
00:00:33,470 --> 00:00:38,940
cumulant function associated with the gaussians which is quadratic the simplest case

12
00:00:38,960 --> 00:00:40,880
then you can define

13
00:00:40,920 --> 00:00:43,110
can minimize

14
00:00:43,120 --> 00:00:45,260
this term

15
00:00:45,270 --> 00:00:52,070
and what you get

16
00:00:52,130 --> 00:00:55,840
see i didn't it didn't do the form it OK

17
00:00:55,870 --> 00:01:01,810
except in third

18
00:01:01,860 --> 00:01:07,550
OK so this is how you would motivate the

19
00:01:07,550 --> 00:01:09,380
the batch algorithm

20
00:01:09,410 --> 00:01:12,190
the online algorithm you'd motivated

21
00:01:13,100 --> 00:01:15,860
taking a divergence

22
00:01:17,420 --> 00:01:21,910
also the initial parameter but only the loss

23
00:01:21,960 --> 00:01:25,620
so far into it so far

24
00:01:25,630 --> 00:01:27,100
and these different

25
00:01:27,100 --> 00:01:31,240
this weightings of the initial distribution this one

26
00:01:31,250 --> 00:01:32,530
the first is this one

27
00:01:32,560 --> 00:01:34,600
might be different

28
00:01:40,770 --> 00:01:45,610
an alternate version of motivation of the online algorithms you take the divergence to the

29
00:01:47,550 --> 00:01:48,920
and then just the loss

30
00:01:48,940 --> 00:01:52,180
the current loss occurred at the end

31
00:01:52,200 --> 00:01:56,560
and while your parameters set this way

32
00:01:56,570 --> 00:02:01,500
and if you do this kind of updates and compute this minimum you get the

33
00:02:01,500 --> 00:02:03,440
following the offline algorithm

34
00:02:06,430 --> 00:02:08,950
puts a certain weight on the initial

35
00:02:09,000 --> 00:02:10,360
and then

36
00:02:10,420 --> 00:02:14,300
some of the instances and divides by

37
00:02:14,300 --> 00:02:19,520
this trade off parameter plus the number of trials a number of trials number of

38
00:02:20,940 --> 00:02:27,750
the online algorithm has a very similar form except these weights might be different

39
00:02:27,760 --> 00:02:30,600
you can also

40
00:02:31,080 --> 00:02:34,690
derive this

41
00:02:34,750 --> 00:02:39,070
OK express this in terms of the previous so this is the previous one minus

42
00:02:39,070 --> 00:02:43,520
eight top t plus one comes this with this is sort of the gradient of

43
00:02:43,520 --> 00:02:44,520
the loss

44
00:02:44,570 --> 00:02:45,720
and this has

45
00:02:45,780 --> 00:02:49,140
one of the t type of behavior

46
00:02:49,170 --> 00:02:50,670
you can also see

47
00:02:53,650 --> 00:02:55,620
this update here

48
00:02:55,680 --> 00:02:58,640
can be written in terms of link function

49
00:02:58,660 --> 00:03:02,930
you remember when we derive think that bregman divergences is always

50
00:03:02,960 --> 00:03:07,370
you talk about idea you went to another domain

51
00:03:07,390 --> 00:03:10,760
this is not going to be the expectation expectation domain

52
00:03:10,860 --> 00:03:13,770
you add in the gradient and you go back

53
00:03:13,800 --> 00:03:15,170
so this update

54
00:03:15,190 --> 00:03:17,560
you can write this way

55
00:03:17,600 --> 00:03:24,260
and it's from it's similar to what we used before

56
00:03:28,090 --> 00:03:29,700
the online algorithm has

57
00:03:29,720 --> 00:03:31,970
the freedom to choose

58
00:03:31,990 --> 00:03:34,010
this additional

59
00:03:34,070 --> 00:03:35,530
trade parameter

60
00:03:35,570 --> 00:03:39,590
which could be different from the initial trade for the trade off parameter of the

61
00:03:39,590 --> 00:03:42,450
batch algorithm

62
00:03:42,490 --> 00:03:46,530
if the two are the same the online algorithm is kind of an incremental offline

63
00:03:50,180 --> 00:03:51,760
sometimes it's good

64
00:03:52,720 --> 00:03:57,370
make the align parameter the online parameter a little bit bigger

65
00:03:57,390 --> 00:04:00,490
OK so this corresponds to

66
00:04:00,550 --> 00:04:02,840
the following situation

67
00:04:02,840 --> 00:04:03,450
if you

68
00:04:03,450 --> 00:04:07,510
plug in everything and you do it for gaussians and so forth this other reasonable

69
00:04:10,910 --> 00:04:12,510
the plane line

70
00:04:12,530 --> 00:04:14,660
algorithm would be

71
00:04:14,660 --> 00:04:18,200
you want to set this parameter to zero and you just sort of two maximum

72
00:04:19,180 --> 00:04:22,160
if t examples you divide by t

73
00:04:22,160 --> 00:04:24,780
the online algorithm on trial t

74
00:04:24,820 --> 00:04:27,590
has seen t minus one example

75
00:04:28,370 --> 00:04:33,590
it turns out by setting the these these eight parameters

76
00:04:33,640 --> 00:04:36,890
a little bit differently a better way to set it is

77
00:04:36,910 --> 00:04:38,530
to add here plus one

78
00:04:38,590 --> 00:04:44,010
so the number of examples plus one it's it's like regularizing it by adding initially

79
00:04:44,010 --> 00:04:46,780
example point zero

80
00:04:46,800 --> 00:04:52,240
we call this the forward algorithm because it estimates it doesn't just

81
00:04:52,260 --> 00:04:58,760
compute the maximum likelihood estimates it but by putting another point zero

82
00:04:58,820 --> 00:05:01,430
before it before it

83
00:05:01,450 --> 00:05:03,570
compute its current parameter

84
00:05:03,590 --> 00:05:05,890
it guess is that the parameter is

85
00:05:05,930 --> 00:05:10,870
actually had zero and that includes that into the estimate and that gets you plus

86
00:05:12,140 --> 00:05:17,510
imagine here plus zero which doesn't show up

87
00:05:18,800 --> 00:05:21,050
can do the same thing

88
00:05:21,070 --> 00:05:23,680
for the newly

89
00:05:23,740 --> 00:05:27,470
maximum likelihood again is the average but

90
00:05:30,390 --> 00:05:32,680
fancy online algorithm would be

91
00:05:32,700 --> 00:05:37,860
something like custom of trust him off i don't know it's two russians

92
00:05:37,870 --> 00:05:41,100
which put a half count here

93
00:05:41,140 --> 00:05:43,740
put an additional example at the half

94
00:05:43,740 --> 00:05:47,820
and then divide by the number of examples plus one

95
00:05:47,860 --> 00:05:50,530
and that sort of a regularized version

96
00:05:50,570 --> 00:05:54,970
love plus estimate is another case where have one here

97
00:05:54,990 --> 00:05:58,530
and repeating this because

98
00:05:58,530 --> 00:06:05,510
you don't use the joint decision trees use again my mind for a zero

99
00:06:05,880 --> 00:06:07,270
seventy five

100
00:06:07,280 --> 00:06:09,260
the wrong one so a zero

101
00:06:09,260 --> 00:06:10,050
this one

102
00:06:10,560 --> 00:06:15,520
five of the six negative and here we have five of this form and we

103
00:06:15,520 --> 00:06:16,580
need that

104
00:06:16,630 --> 00:06:20,530
well my i think and simply multiply these

105
00:06:20,550 --> 00:06:22,910
the marginal

106
00:06:24,530 --> 00:06:28,730
likelihood ratios to obtain an estimate of the joint

107
00:06:29,810 --> 00:06:34,970
like ratio one prior to the posterior

108
00:06:34,990 --> 00:06:39,740
it's by no

109
00:06:41,960 --> 00:06:44,410
and to visualize the right to get

110
00:06:44,420 --> 00:06:51,520
and you can transform these probability these ratios the probability is as before can

111
00:06:52,120 --> 00:06:54,560
right and as you see

112
00:06:54,580 --> 00:07:01,700
nine days may one while the mistakes because the models in this case are misleading

113
00:07:01,700 --> 00:07:03,440
but the model a

114
00:07:03,510 --> 00:07:10,050
one more of the next year and again all of the next years multiply those

115
00:07:10,050 --> 00:07:15,530
you get something like twice as many of the next year but in fact

116
00:07:15,550 --> 00:07:21,730
there are more than simple so that they form a ranking which we all know

117
00:07:21,770 --> 00:07:25,430
what to do that you can visualize wrong space

118
00:07:25,430 --> 00:07:27,030
why is this

119
00:07:30,140 --> 00:07:35,690
again we have this only sixteen points to work with some of the ROC curve

120
00:07:35,820 --> 00:07:38,560
and i think it has to go through these points

121
00:07:40,790 --> 00:07:48,630
and blue actually indicate the marginals because the market decision stumps and again they must

122
00:07:48,630 --> 00:07:49,850
be somewhere

123
00:07:49,870 --> 00:07:53,260
among those sixty points so actually

124
00:07:53,270 --> 00:07:54,970
i think this is me

125
00:07:54,970 --> 00:07:55,980
well zero

126
00:07:55,990 --> 00:07:56,440
this is a

127
00:07:56,490 --> 00:07:58,870
a one

128
00:07:58,890 --> 00:07:59,710
this a

129
00:07:59,720 --> 00:08:01,470
here and b

130
00:08:04,740 --> 00:08:06,100
OK so basically

131
00:08:06,130 --> 00:08:10,980
nine baseball all my face has four blue vectors and price

132
00:08:12,920 --> 00:08:17,770
you will end up in ROC space and cyberspace makes mistakes

133
00:08:17,880 --> 00:08:19,710
so we have the game

134
00:08:19,720 --> 00:08:24,840
marginal probabilities you can do anything about that but i want have is we have

135
00:08:24,840 --> 00:08:28,440
access to truth joint probability which you know

136
00:08:28,450 --> 00:08:30,800
so we have this ranking

137
00:08:31,090 --> 00:08:35,800
but that's something think this ranking

138
00:08:37,390 --> 00:08:38,850
why simple

139
00:08:39,890 --> 00:08:43,840
she that become the left and right children of the truth

140
00:08:43,860 --> 00:08:48,370
so it's too long for free

141
00:08:48,390 --> 00:08:55,210
if i just small values of a around in both cases i get three four

142
00:08:55,210 --> 00:08:59,530
now this is what i call the lexicographic ranking

143
00:08:59,560 --> 00:09:04,720
a simple left to right or of the tree which

144
00:09:04,730 --> 00:09:07,950
if we could be before a and we always will be

145
00:09:07,980 --> 00:09:10,690
zero for the one and

146
00:09:10,700 --> 00:09:12,420
a one

147
00:09:12,430 --> 00:09:18,570
before a people there so why do i call lexicographic ranking well

148
00:09:18,640 --> 00:09:25,600
think about that the brain is what do you do find people for

149
00:09:25,780 --> 00:09:27,020
for instance

150
00:09:27,690 --> 00:09:34,500
you want to know whether by people flocked to understand what the stuff from the

151
00:09:34,510 --> 00:09:36,640
before know right

152
00:09:37,380 --> 00:09:38,510
if you want to know

153
00:09:40,290 --> 00:09:47,460
a lot of people that we see that one of the many applications of my

154
00:09:47,530 --> 00:09:50,860
suffering in england

155
00:09:50,890 --> 00:09:53,030
then you use standard two

156
00:09:53,040 --> 00:09:56,260
names until you find the first position where they differ

157
00:09:56,270 --> 00:10:01,950
and then you say if a because before

158
00:10:01,990 --> 00:10:08,450
and we know what comes after that right so lexicographic ranking is basically finding the

159
00:10:08,450 --> 00:10:13,630
first point where two things differently and use that point is the criterion which store

160
00:10:13,640 --> 00:10:17,220
the addresses and that's exactly what we're doing here

161
00:10:17,930 --> 00:10:20,590
if we want to compare two examples

162
00:10:20,620 --> 00:10:21,270
if they

163
00:10:21,380 --> 00:10:23,570
for p

164
00:10:23,590 --> 00:10:27,750
i just need to know which one of the people zero i know that one

165
00:10:29,740 --> 00:10:34,530
if they are equal for the goalkeeper in the tree

166
00:10:34,550 --> 00:10:37,570
of course in practice you wouldn't do the mandatory

167
00:10:37,640 --> 00:10:41,730
exponential space that this is this is the different

168
00:10:41,740 --> 00:10:43,010
in practice

169
00:10:43,030 --> 00:10:44,810
we use the odds ratio

170
00:10:44,830 --> 00:10:53,230
two towards the attributes because they basically gives us the the fact lexicographic ranking has

171
00:10:53,230 --> 00:10:55,850
strictly stronger lines the naive bayes

172
00:10:55,860 --> 00:10:59,890
so this is very quick

173
00:10:59,910 --> 00:11:03,460
the o racial can be understood as yet another splitting

174
00:11:03,510 --> 00:11:07,500
they have quite nice nice properties

175
00:11:07,520 --> 00:11:08,500
so basically i

176
00:11:08,520 --> 00:11:10,370
ratio is

177
00:11:10,470 --> 00:11:15,840
people by and which is the film that divided by

178
00:11:15,860 --> 00:11:21,080
the ratio of gold rush and one of the reasons why is so much like

179
00:11:21,080 --> 00:11:24,120
something like information gain for instance the odds ratio

180
00:11:24,520 --> 00:11:27,820
at the closed form if you want to draw knights

181
00:11:27,860 --> 00:11:28,990
red lines

182
00:11:28,990 --> 00:11:31,280
japanese man rocks

183
00:11:31,370 --> 00:11:34,690
so one way to understand the

184
00:11:34,700 --> 00:11:36,800
if you look at this

185
00:11:36,990 --> 00:11:40,910
ratio p

186
00:11:40,910 --> 00:11:44,480
and what is and is something like

187
00:11:44,500 --> 00:11:45,510
the area

188
00:11:45,750 --> 00:11:47,640
this guy here

189
00:11:47,660 --> 00:11:51,090
and the other one is the area that there

190
00:11:52,190 --> 00:11:53,120
o point

191
00:11:53,210 --> 00:11:56,770
this point of ratio five

192
00:11:56,770 --> 00:11:59,610
because of this i five because that want

193
00:11:59,640 --> 00:12:02,230
and if i go along this

194
00:12:02,270 --> 00:12:05,720
one of key problem as an argument

195
00:12:06,600 --> 00:12:08,080
he met some

196
00:12:08,220 --> 00:12:10,270
in what i'm about to show you

197
00:12:10,280 --> 00:12:14,340
we actually use the odds ratio also is this is true

198
00:12:14,350 --> 00:12:15,760
but it did

199
00:12:17,810 --> 00:12:22,700
now comes the moment of truth for enough to decide to give them all

200
00:12:22,700 --> 00:12:29,230
to my students and then at the bar had spent long

201
00:12:29,250 --> 00:12:30,090
and also

202
00:12:30,110 --> 00:12:35,030
the first were to produce is it's still in beta which means and it can

203
00:12:35,090 --> 00:12:38,470
be something else

204
00:12:39,160 --> 00:12:43,270
i just want to visualize some of these ideas here

205
00:12:43,280 --> 00:12:47,230
the problem is the problem if you're old enough

206
00:12:47,270 --> 00:12:48,890
to understand how many

207
00:12:48,930 --> 00:12:51,690
you even though there

208
00:12:51,710 --> 00:12:54,510
that's not to say too much about the

209
00:13:00,570 --> 00:13:02,370
so house

210
00:13:04,330 --> 00:13:06,200
it's a

211
00:13:09,690 --> 00:13:11,410
actually incorporates

212
00:13:11,430 --> 00:13:16,050
some of the best classifiers of web

213
00:13:16,080 --> 00:13:19,870
so let me just show you what it does i can open

214
00:13:21,210 --> 00:13:25,000
data files diabetes

215
00:13:25,000 --> 00:13:26,570
if you know this is

216
00:13:27,070 --> 00:13:32,150
if even if this increases decreases slower than the really what you know pieces of

217
00:13:32,150 --> 00:13:37,110
luggage for practical purposes basically constant so the idea is that even if i i

218
00:13:37,110 --> 00:13:42,050
even if they can lead to large networks my cluster size remains unchanged and my

219
00:13:42,050 --> 00:13:43,360
cluster sizes

220
00:13:43,420 --> 00:13:47,920
you know the medium medium class size sixty in there and if i take the

221
00:13:47,920 --> 00:13:50,300
average i think i get hundred nine

222
00:13:50,320 --> 00:13:51,790
so the point is

223
00:13:51,790 --> 00:13:55,020
even if i take you know huge networks of best of millions of nodes

224
00:13:55,030 --> 00:13:56,710
the best clustering

225
00:13:56,730 --> 00:14:00,300
are of the same size meaning around hundred

226
00:14:00,320 --> 00:14:02,090
so that's the first so

227
00:14:02,130 --> 00:14:04,710
just don't get because

228
00:14:05,000 --> 00:14:08,070
and if i go look and you know what they want

229
00:14:08,110 --> 00:14:12,530
what are the last what what he described as likely would look like this is

230
00:14:12,530 --> 00:14:14,570
how like that so this edge

231
00:14:14,590 --> 00:14:16,150
between red and green

232
00:14:16,210 --> 00:14:20,380
is the edge cutting and you can see that these things are

233
00:14:20,380 --> 00:14:23,670
launched this as they are richer than trees

234
00:14:23,670 --> 00:14:25,050
so they are

235
00:14:25,110 --> 00:14:28,050
as i've shown the next slide actually there are bigger

236
00:14:28,070 --> 00:14:30,710
and then what we you expect or

237
00:14:30,730 --> 00:14:33,000
what you could ask if you know this

238
00:14:33,020 --> 00:14:37,190
this is the painting of the just the result of course this is a big

239
00:14:37,290 --> 00:14:42,020
but the idea is real networks as sparse right every degrees of super-low so we

240
00:14:42,020 --> 00:14:47,730
naturally expect something like this to just by chance because frankly because it's

241
00:14:47,790 --> 00:14:51,800
and if i plot the distribution of you know this because sizes

242
00:14:51,920 --> 00:14:55,630
this is the real network and this is the network with the same degree distribution

243
00:14:55,630 --> 00:14:59,750
so basically a random graph with the same degree sequence right so i keep the

244
00:14:59,750 --> 00:15:03,980
density and keep the degree sequence i'm just randomizing the edges so

245
00:15:04,000 --> 00:15:09,000
and you can see i get much smaller and much fewer of disease which means

246
00:15:09,000 --> 00:15:11,710
that this is actually real

247
00:15:11,800 --> 00:15:14,150
OK so that's that's the next iteration

248
00:15:14,210 --> 00:15:19,050
this small pieces on the edge of the network that do what you would expect

249
00:15:19,050 --> 00:15:21,070
chance from the

250
00:15:21,070 --> 00:15:24,520
OK so this is one

251
00:15:25,360 --> 00:15:27,750
so then you could say oh you know

252
00:15:27,770 --> 00:15:32,590
if this these things on the edge of the network are really causing trouble what

253
00:15:32,820 --> 00:15:34,320
if you just going to remove

254
00:15:34,380 --> 00:15:38,980
right so what i'm showing you here is that this is now i think opinions

255
00:15:38,980 --> 00:15:43,820
on i would like to read these plots were shown before and the green now

256
00:15:43,840 --> 00:15:47,840
these plots were shown that i removed is green set up the network and i

257
00:15:47,840 --> 00:15:50,270
only considered

258
00:15:50,290 --> 00:15:54,710
the yellow let's say take this yellow clouds sort of these networks where in the

259
00:15:55,570 --> 00:15:57,400
and what's your situation

260
00:15:57,480 --> 00:16:00,690
observation is that nothing happens right i

261
00:16:00,730 --> 00:16:05,070
at least one years to get pretty much the same the same shape of of

262
00:16:05,110 --> 00:16:09,190
the network community profile the only difference is that you know now the green is

263
00:16:09,190 --> 00:16:14,670
a shifted a bit upwards and a be a bit to the right

264
00:16:14,710 --> 00:16:18,340
and what does this tell me this tells me two things first is that

265
00:16:18,340 --> 00:16:22,150
when i remove this risk is connected by a single edge now

266
00:16:22,150 --> 00:16:26,150
what is causing me the travel what is being managed by is the discussion or

267
00:16:26,170 --> 00:16:29,610
this more components are connected to the network the two edges

268
00:16:29,630 --> 00:16:30,420
right so

269
00:16:31,020 --> 00:16:31,980
the two edge

270
00:16:32,000 --> 00:16:33,300
connected whiskers

271
00:16:33,320 --> 00:16:38,360
i'm responsible for this but this is this is what remains or explanation for this

272
00:16:38,360 --> 00:16:40,250
but remains as it was

273
00:16:40,450 --> 00:16:44,210
so what this tells me about the structure of the networks it basically says the

274
00:16:44,210 --> 00:16:48,690
networks this mess the core periphery structure that have this this is on the edge

275
00:16:48,730 --> 00:16:51,380
the i can very easily cut away and then i have this

276
00:16:52,190 --> 00:16:55,530
on you like dance and then supported the you know if i give away the

277
00:16:55,530 --> 00:16:58,420
outer layer i get to se in millions of

278
00:16:58,570 --> 00:17:02,590
so if i move the periphery the core itself will always be decomposable to the

279
00:17:03,440 --> 00:17:05,460
so that's why

280
00:17:05,500 --> 00:17:08,800
and here's the picture just schematic right

281
00:17:08,860 --> 00:17:11,520
if i got away is small pieces

282
00:17:11,530 --> 00:17:16,270
right i'm getting this part of the of the network community profile plots and once

283
00:17:16,270 --> 00:17:19,570
they want to get wants to go beyond that size scale of hundred and i

284
00:17:19,570 --> 00:17:24,460
want to be immediately says i got into these denser and denser part of the

285
00:17:24,460 --> 00:17:27,270
network and might not getting worse and worse

286
00:17:27,900 --> 00:17:32,150
and just to be clear what the result is that the network core contains around

287
00:17:32,150 --> 00:17:37,090
sixty percent of the nodes and eighty percent of the right so there is significant

288
00:17:37,130 --> 00:17:40,030
forty percent of the nodes this

289
00:17:40,050 --> 00:17:42,300
well let's call the lemma one for you

290
00:17:42,670 --> 00:17:46,590
so there is still a lot of nodes

291
00:17:47,230 --> 00:17:49,530
that's the point

292
00:17:49,550 --> 00:17:54,750
now i want to get a bit more technical about how we got the network

293
00:17:55,690 --> 00:17:59,150
so far i really i just told you what kind of algorithm i'm using to

294
00:17:59,150 --> 00:18:02,250
find is to find now i want to get a bit more

295
00:18:02,250 --> 00:18:06,520
into detail what kind of set different clustering algorithms give

296
00:18:06,840 --> 00:18:10,210
so the first thing we can say is what if we allow cuts like that

297
00:18:10,210 --> 00:18:14,320
right for example you come to me and say gimme gimme cluster on a node

298
00:18:14,360 --> 00:18:19,460
i say you know his clustering night no write these nodes i cut wages and

299
00:18:19,460 --> 00:18:21,940
you know i get whatever inside that's my

300
00:18:21,960 --> 00:18:24,880
so this is the first question is you know what if i love

301
00:18:24,900 --> 00:18:27,070
cuts in some the

302
00:18:27,110 --> 00:18:30,360
at least you know from the last point if you don't make much sense right

303
00:18:30,380 --> 00:18:33,340
if i if i want to now go and you know

304
00:18:33,360 --> 00:18:36,900
call is a single community that that's not what i want to do

305
00:18:37,520 --> 00:18:42,380
and then the second heuristic i using it's very so i call this a bag

306
00:18:42,380 --> 00:18:45,110
of whiskers heuristic and all i see is

307
00:18:45,150 --> 00:18:47,420
i identify scores

308
00:18:47,460 --> 00:18:50,980
i mean i put them in the back and then when you come to me

309
00:18:50,980 --> 00:18:56,300
and say you know community on seven other whatever i just take the appropriate to

310
00:18:56,300 --> 00:18:59,940
discus throw them in the back and say here's here's the cluster right here is

311
00:18:59,940 --> 00:19:03,960
that you ask for a particular number of nodes here is the class that number

312
00:19:05,770 --> 00:19:08,150
now the question is how these things work

313
00:19:08,190 --> 00:19:10,480
so this is a bit busy but

314
00:19:10,570 --> 00:19:14,750
i don't know so again you can see that because we had before

315
00:19:14,750 --> 00:19:17,500
this is the number of nodes in the cluster which is the quality of the

316
00:19:17,520 --> 00:19:19,150
cluster of better

317
00:19:20,420 --> 00:19:21,550
we start up here

318
00:19:21,570 --> 00:19:24,860
if we just minimum around hundred and goes out

319
00:19:24,900 --> 00:19:28,000
and then each graph here is a different person

320
00:19:28,320 --> 00:19:30,550
so that is the one we saw before

321
00:19:30,590 --> 00:19:34,320
right and you can see that there are clustering methods that even better clusters than

322
00:19:34,320 --> 00:19:36,360
the red the then the red

323
00:19:36,380 --> 00:19:37,570
so what

324
00:19:37,570 --> 00:19:39,390
and therefore

325
00:19:39,400 --> 00:19:41,080
if an object sinks

326
00:19:41,140 --> 00:19:43,650
and the density of the object is larger

327
00:19:43,660 --> 00:19:46,320
and the density of the fluid

328
00:19:46,360 --> 00:19:50,870
and the amazing thing is that this is completely independent of the dimensions of the

329
00:19:51,860 --> 00:19:53,700
the only thing that matters

330
00:19:54,580 --> 00:19:59,110
the density

331
00:19:59,190 --> 00:20:02,560
if you take a pebble is thrown in the water it sinks because the density

332
00:20:02,560 --> 00:20:05,640
of the petal is higher than water with a piece of wood

333
00:20:05,720 --> 00:20:09,890
it has a density lower than water you throw the water flow independent of the

334
00:20:12,240 --> 00:20:14,580
but i don't think so it floats

335
00:20:14,650 --> 00:20:17,210
the buoyant force is always identical to the weight

336
00:20:17,340 --> 00:20:20,900
of the displaced fluid

337
00:20:20,940 --> 00:20:23,150
and this brings up one of my

338
00:20:24,340 --> 00:20:27,780
questions that i have for you would i want you to think about

339
00:20:27,790 --> 00:20:31,180
and if you have a full understanding of archimedes principle

340
00:20:31,210 --> 00:20:33,060
you will be able to answer

341
00:20:33,140 --> 00:20:36,140
so concentrate on them going to present you with

342
00:20:36,150 --> 00:20:38,580
i am in the swimming pool

343
00:20:38,620 --> 00:20:42,090
and i'm in the boat

344
00:20:42,100 --> 00:20:43,810
swimming pool

345
00:20:43,860 --> 00:20:45,430
here is the boat

346
00:20:45,470 --> 00:20:47,400
and i'm sitting in the boat

347
00:20:47,410 --> 00:20:51,200
and i have a rocket in my boat

348
00:20:51,290 --> 00:20:52,690
i'm sitting in a swimming pool

349
00:20:52,700 --> 00:20:55,080
nice rock in my book

350
00:20:55,150 --> 00:20:56,680
i'm like

351
00:20:56,720 --> 00:20:58,190
the one line

352
00:20:58,200 --> 00:20:59,870
of the swimming pool

353
00:20:59,940 --> 00:21:02,840
very carefully

354
00:21:02,850 --> 00:21:04,400
i take the rock

355
00:21:04,490 --> 00:21:07,560
and i throw it overboard

356
00:21:07,600 --> 00:21:09,990
well the waterline go up

357
00:21:10,050 --> 00:21:12,300
well the waterline go down

358
00:21:12,310 --> 00:21:13,570
well maybe

359
00:21:13,590 --> 00:21:16,200
what will stay the same

360
00:21:16,290 --> 00:21:19,590
use your intuition don't mind being wrong

361
00:21:19,640 --> 00:21:22,170
home you have some time to think about it i'm sure you'll come up with

362
00:21:22,170 --> 00:21:23,320
the right answer

363
00:21:23,330 --> 00:21:26,610
o things at the one line will go up

364
00:21:26,700 --> 00:21:30,550
swimming pool with things that the one that will go down

365
00:21:30,590 --> 00:21:35,570
the things that make no difference at the waterline stays the same

366
00:21:35,700 --> 00:21:37,100
is OK

367
00:21:39,010 --> 00:21:40,490
four line will change

368
00:21:40,520 --> 00:21:42,820
but you figure it out

369
00:21:42,820 --> 00:21:48,340
OK you apply archimedes principle and you'll get the answer

370
00:21:48,400 --> 00:21:50,220
i want to talk about stability

371
00:21:50,290 --> 00:21:56,470
particularly its ability of ships which is a very important thing they float

372
00:21:56,520 --> 00:22:00,060
suppose i have an object here it is floating in water

373
00:22:00,070 --> 00:22:01,890
the water line

374
00:22:01,910 --> 00:22:06,990
and that he be the centre of mass of that object

375
00:22:07,050 --> 00:22:10,440
i could be way off centre could be nice work be bowlers could be rocks

376
00:22:10,440 --> 00:22:11,540
in there

377
00:22:11,550 --> 00:22:15,680
right doesn't have to be uniform density centre of mass could be off the scent

378
00:22:15,770 --> 00:22:18,150
of the geometric center

379
00:22:18,160 --> 00:22:19,420
so it is object

380
00:22:19,440 --> 00:22:20,950
has a certain mass

381
00:22:20,960 --> 00:22:23,890
and this is the gravitational force

382
00:22:23,940 --> 00:22:25,190
but now look at the

383
00:22:25,210 --> 00:22:28,180
centre of mass

384
00:22:30,640 --> 00:22:31,950
that's clearly

385
00:22:31,990 --> 00:22:33,120
four years

386
00:22:33,190 --> 00:22:34,470
somewhere here

387
00:22:34,480 --> 00:22:35,940
the displaced fluid

388
00:22:35,950 --> 00:22:37,140
that is

389
00:22:37,190 --> 00:22:39,290
where the wind forces

390
00:22:40,840 --> 00:22:42,900
so now what you have

391
00:22:42,940 --> 00:22:46,740
we have the talk on this object relative to any point that you choose doesn't

392
00:22:46,740 --> 00:22:48,170
matter where you pick a point

393
00:22:48,210 --> 00:22:49,350
the talk

394
00:22:49,360 --> 00:22:53,060
and so was going to happen this object is clearly going to rotate

395
00:22:53,100 --> 00:22:54,340
in this direction

396
00:22:54,400 --> 00:22:56,530
and the talk will only be zero

397
00:22:56,570 --> 00:22:57,940
when the buoyant force

398
00:22:57,950 --> 00:23:00,610
and the gravitational force i one line

399
00:23:00,670 --> 00:23:02,100
and two become zero

400
00:23:02,110 --> 00:23:03,070
and then

401
00:23:03,120 --> 00:23:05,440
it is completely happy

402
00:23:05,560 --> 00:23:08,830
now there are two ways that you can get them on one line

403
00:23:08,840 --> 00:23:10,520
we discussed earlier

404
00:23:10,530 --> 00:23:12,360
in different context

405
00:23:13,600 --> 00:23:14,760
i have the

406
00:23:14,770 --> 00:23:17,480
centre of mass of the object below

407
00:23:17,540 --> 00:23:19,370
the centre of mass of the

408
00:23:19,430 --> 00:23:23,390
this place fluid or above in both cases would be on one line

409
00:23:23,430 --> 00:23:27,540
however in one case there would be stable equilibrium in the other

410
00:23:27,610 --> 00:23:30,230
that would not be stable equilibrium

411
00:23:30,260 --> 00:23:33,410
i have here an object

412
00:23:33,450 --> 00:23:35,140
which has its centre of mass

413
00:23:35,150 --> 00:23:36,450
very low

414
00:23:36,510 --> 00:23:37,690
you can tell that

415
00:23:37,700 --> 00:23:39,210
no way of knowing

416
00:23:39,270 --> 00:23:40,830
all you know is that the

417
00:23:40,890 --> 00:23:44,290
the weight of the displaced fluid that you see here

418
00:23:44,300 --> 00:23:45,850
it's the same

419
00:23:45,910 --> 00:23:46,700
as the

420
00:23:46,730 --> 00:23:49,670
weight of the object that's all you know

421
00:23:49,710 --> 00:23:53,140
if i took this object

422
00:23:53,150 --> 00:23:54,980
and and i tell the little

423
00:23:55,090 --> 00:23:57,430
with the centre of mass very low

424
00:23:57,510 --> 00:24:00,430
so here's an

425
00:24:00,440 --> 00:24:02,320
and here's some underwater line

426
00:24:03,980 --> 00:24:07,930
the centre of mass of the displaced fluid is somewhere here

427
00:24:07,960 --> 00:24:09,190
fp is here

428
00:24:09,190 --> 00:24:10,190
we force

429
00:24:10,200 --> 00:24:11,640
you can see what's going to happen

430
00:24:11,650 --> 00:24:15,360
it's going to rotate to the right it's a restoring torque

431
00:24:15,420 --> 00:24:16,150
and so

432
00:24:16,170 --> 00:24:17,590
it's completely stable

433
00:24:17,600 --> 00:24:19,860
i can wobble back and forth

434
00:24:19,890 --> 00:24:22,300
and in stable

435
00:24:22,340 --> 00:24:24,070
if i would turn it over

436
00:24:24,180 --> 00:24:25,800
that is not stable

437
00:24:25,870 --> 00:24:26,950
because now

438
00:24:26,970 --> 00:24:28,990
i would have the centre of mass

439
00:24:29,010 --> 00:24:31,260
somewhere here high up

440
00:24:31,410 --> 00:24:35,720
now i have g

441
00:24:35,730 --> 00:24:37,980
and the centre of the buoyant force

442
00:24:37,980 --> 00:24:40,880
OK let's get started it's so

443
00:24:40,940 --> 00:24:46,110
now i introduced you the basics of kernel methods

444
00:24:46,120 --> 00:24:48,630
actually you can

445
00:24:48,660 --> 00:24:52,280
you could leave now

446
00:24:52,300 --> 00:24:55,680
because i think you know everything if

447
00:24:55,780 --> 00:24:59,370
well you know now is is sufficient if you want to to to do kernel

448
00:24:59,370 --> 00:25:04,550
methods you know i support vector machines you know things about kernels and if you

449
00:25:04,550 --> 00:25:09,430
just want to do the applications or if you want to localize your own algorithm

450
00:25:09,590 --> 00:25:11,700
i i think you you already have

451
00:25:11,710 --> 00:25:14,070
everything to do that

452
00:25:14,080 --> 00:25:19,140
one thing i want to show you now is actually i want to show you

453
00:25:19,150 --> 00:25:23,430
two things and probably won't have time to show you the last thing of the

454
00:25:23,610 --> 00:25:25,530
of the of my talk about

455
00:25:25,530 --> 00:25:29,290
you have everything on the on the printouts and then if you have questions you

456
00:25:29,290 --> 00:25:31,710
we can discuss any points

457
00:25:31,760 --> 00:25:34,680
if you want so the first thing that i want to talk to you about

458
00:25:34,680 --> 00:25:42,420
is the i want to show you examples of kernels for structured data because

459
00:25:42,920 --> 00:25:46,730
one of the the very nice things about kernels

460
00:25:46,790 --> 00:25:51,390
as i said to you before is that you can use them to do classification

461
00:25:51,390 --> 00:25:58,950
for that data that are structured that have information on on their structure that you

462
00:25:58,950 --> 00:26:05,560
cannot have when you just work with vectors so this was very in advertising thing

463
00:26:05,560 --> 00:26:07,870
about kernel methods

464
00:26:07,870 --> 00:26:10,730
saying that now you have all the algorithms

465
00:26:10,750 --> 00:26:14,960
on one side and all you have to care about is having a good kernel

466
00:26:14,960 --> 00:26:19,150
for the application that you want to to to address

467
00:26:21,010 --> 00:26:24,400
i'll show you a specific kernels for sequences

468
00:26:24,400 --> 00:26:30,060
and graphs of course there exists many many many different kernels you can build your

469
00:26:30,060 --> 00:26:30,900
own kernel

470
00:26:30,980 --> 00:26:35,820
but i still have to do is the specific kernels that i'm going to show

471
00:26:35,820 --> 00:26:42,890
you because in some sense they contain everything that you may need to to know

472
00:26:42,890 --> 00:26:45,530
if you want to build your own code

473
00:26:52,160 --> 00:26:58,700
why talking about kernels for structured data because if you want to tackle the real

474
00:26:58,700 --> 00:26:59,730
life applications

475
00:27:00,260 --> 00:27:05,390
you may have to work with

476
00:27:05,390 --> 00:27:12,940
so structured data it's very it's very rare when you have a attribute vector data

477
00:27:12,980 --> 00:27:17,670
it's an usually if you have that then means that somehow you've left you've lost

478
00:27:17,790 --> 00:27:21,700
some information on on the data you're going to work with

479
00:27:24,230 --> 00:27:25,640
the the

480
00:27:25,660 --> 00:27:31,070
again a very nice thing about currency that you may want to tackle problems with

481
00:27:31,070 --> 00:27:34,350
structured data and try to

482
00:27:34,440 --> 00:27:40,230
preserve as much information as possible on the on the thing the objects that you

483
00:27:40,230 --> 00:27:41,220
work on

484
00:27:41,300 --> 00:27:47,420
and for instance here you have you have DNA DNA strain

485
00:27:47,440 --> 00:27:56,550
we then the different i remember how it's called english but ATC in g

486
00:27:56,570 --> 00:27:59,360
and here you have molecules

487
00:27:59,410 --> 00:28:05,690
so i'm going to talk about a bit about molecules because those are

488
00:28:05,700 --> 00:28:07,450
a typical example of of

489
00:28:07,480 --> 00:28:11,570
of graph that you may encounter and and they

490
00:28:12,600 --> 00:28:18,860
molecules to pose questions about how we can do virtual screening meaning that you want

491
00:28:18,860 --> 00:28:22,950
to have

492
00:28:23,010 --> 00:28:29,410
ultimate process to directly identify what

493
00:28:29,420 --> 00:28:36,230
molecules can help you sure these kind of disease are these are the kind of

494
00:28:36,230 --> 00:28:38,600
these is so it's very

495
00:28:38,610 --> 00:28:45,600
i mean it's i mean it's very easy to synthesize and to build molecules in

496
00:28:45,600 --> 00:28:50,690
instance what they called in silico studies to model molecules in in the computer is

497
00:28:50,690 --> 00:28:52,610
just generating graphs

498
00:28:52,630 --> 00:28:55,180
it's just using a computer so

499
00:28:55,210 --> 00:28:58,300
among all the billions of

500
00:28:58,360 --> 00:29:04,340
molecule that you have sometimes you may want to identify some of those molecules that

501
00:29:04,850 --> 00:29:06,110
that may have

502
00:29:06,160 --> 00:29:08,490
desired properties

503
00:29:09,120 --> 00:29:16,090
and it's very very very important for pharmaceutical companies

504
00:29:17,980 --> 00:29:24,130
this is a very rough draft scale on on the state of the art

505
00:29:24,150 --> 00:29:30,670
on classification problems if you have structured data and unstructured data

506
00:29:31,480 --> 00:29:34,970
basically in machine learning etc

507
00:29:34,980 --> 00:29:36,940
usually very easy

508
00:29:36,990 --> 00:29:42,020
where i put quotes here because it depends on the problem and the application that

509
00:29:42,060 --> 00:29:46,480
you have plenty of methods that you can use way if you want to to

510
00:29:46,490 --> 00:29:50,050
do classification with

511
00:29:50,060 --> 00:29:52,100
victoria vectors

512
00:29:52,100 --> 00:29:55,670
this is something that you know you know you've you've known from high school

513
00:29:56,800 --> 00:29:59,980
and there are many many many things that you can use

514
00:29:59,980 --> 00:30:05,500
and then you have sequences sequences are a bit too richer than x because the

515
00:30:05,500 --> 00:30:12,600
position of of of of elements that describe the sequences with respect to each other

516
00:30:12,600 --> 00:30:14,420
are very important

517
00:30:14,440 --> 00:30:19,770
and but in end there are methods to deal with sequences but they are not

518
00:30:19,770 --> 00:30:25,090
as plentiful as they are for for vector data

519
00:30:25,100 --> 00:30:27,190
and then

520
00:30:27,230 --> 00:30:30,630
you have graph data

521
00:30:31,290 --> 00:30:33,770
that are very hard to handle

522
00:30:33,780 --> 00:30:36,040
because you

523
00:30:36,050 --> 00:30:38,020
easily end up with

524
00:30:38,030 --> 00:30:48,300
np hard problems because comparing graphs trying to to identify award because identify common substructures

525
00:30:48,310 --> 00:30:56,410
and things like that is very computationally demanding if you have no hypothesis and assumptions

526
00:30:56,410 --> 00:30:58,680
and the kind of graphs you're working on

527
00:30:58,730 --> 00:31:04,530
and this is the reason why there's is very few things on the classification and

528
00:31:04,530 --> 00:31:11,880
graph classifications and at his there is no state of the art methods that or

529
00:31:11,880 --> 00:31:16,610
or propose a method that says OK if you want to do graph classification i'm

530
00:31:16,610 --> 00:31:19,990
going to use this method because i know that is what i have to do

531
00:31:20,160 --> 00:31:24,420
there is no such thing for for graph data

532
00:31:24,470 --> 00:31:31,000
and so here again we see that there are very few methods to to to

533
00:31:31,000 --> 00:31:33,030
tackle problems involving graphs

534
00:31:33,030 --> 00:31:39,120
the phone content provided by MIT opencourseware under creative commons license

535
00:31:39,140 --> 00:31:48,550
additional information about our relations and MIT opencourseware in general is available OCW MIT you

536
00:31:48,740 --> 00:31:50,660
all right so we have a few more

537
00:31:50,670 --> 00:31:54,590
powerpoint things before i'm going to attempt using the board

538
00:31:56,650 --> 00:32:01,980
again we're in the transition metal unit and today we're going to introduce something called

539
00:32:01,980 --> 00:32:03,630
crystal field theory

540
00:32:03,640 --> 00:32:08,970
and this is in chapter sixteen in your book

541
00:32:08,990 --> 00:32:12,500
so let me just tell you about two different theories and again chemistry is an

542
00:32:12,500 --> 00:32:14,710
experimental science we collect data

543
00:32:15,110 --> 00:32:19,060
and then we try to come up with theories that explain the data so the

544
00:32:19,060 --> 00:32:24,070
theories are sort of a hundred percent summer more simple and some are more complicated

545
00:32:24,080 --> 00:32:26,380
to try to explain what we observe

546
00:32:26,500 --> 00:32:32,020
and some of these theories of other pretty simple approximations of what's really going on

547
00:32:32,040 --> 00:32:36,570
the pretty good job of explaining the things that we are observing

548
00:32:36,590 --> 00:32:38,300
all right so there are two

549
00:32:38,310 --> 00:32:42,930
there are two different kinds of theories that you hear about with transition metals you

550
00:32:42,930 --> 00:32:46,350
hear about crystal field theory and ligand field theory

551
00:32:46,370 --> 00:32:53,190
and so these were again developed to explain certain types of properties of coordination complexes

552
00:32:53,190 --> 00:32:58,130
of transition metals so there are observed data and then people try to think about

553
00:32:58,130 --> 00:33:04,000
how you could rationalize what was observed

554
00:33:04,010 --> 00:33:07,580
so the basic idea behind the theory

555
00:33:07,640 --> 00:33:12,580
is that if you have your metal ion with its given oxidation number

556
00:33:12,620 --> 00:33:14,930
and it's in the centre of the core nations

557
00:33:16,200 --> 00:33:21,090
the energy levels of the d orbitals for that particular metal

558
00:33:21,100 --> 00:33:26,050
we're going to be alternate when ligands are bound compared to sort of free

559
00:33:27,600 --> 00:33:33,020
so the binding of ligands around the metal where there are those d orbitals associated

560
00:33:33,020 --> 00:33:40,840
with that metal is going to have an effect and creates special properties

561
00:33:40,880 --> 00:33:45,440
so crystal field theory which is what we're going to be talking about is based

562
00:33:45,440 --> 00:33:47,810
on an ionic description

563
00:33:47,830 --> 00:33:53,420
basically you're going to be considering ligands as negative point charges and negative charge and

564
00:33:53,420 --> 00:33:58,290
you're going to ask the question is that negative charge pointed toward the orbital it

565
00:33:58,310 --> 00:34:02,240
so that'll be repulsive and that's going to happen that

566
00:34:03,390 --> 00:34:08,880
ligand field theory includes covalent as well as ionic description

567
00:34:08,960 --> 00:34:14,890
so it is a more powerful it theory and it it can describe things that

568
00:34:14,920 --> 00:34:19,080
little bit better but it that we don't have enough time to cover it here

569
00:34:19,480 --> 00:34:23,690
and if you're interested in transition metals you probably don't want to take another chemistry

570
00:34:23,690 --> 00:34:28,970
course called five o three which is in organic chemistry so at this point we're

571
00:34:28,970 --> 00:34:34,480
just going to be doing the more simple theory which just considers the ionic description

572
00:34:34,490 --> 00:34:37,960
but it does pretty well you can explain some things with it but you should

573
00:34:37,960 --> 00:34:38,970
be aware

574
00:34:39,010 --> 00:34:43,360
but this is not the sort of the the mos that you can do a

575
00:34:43,360 --> 00:34:47,070
little bit better with another theory so you should just be aware that this is

576
00:34:47,070 --> 00:34:52,930
really the most simplistic kind of of representation or theory to explain some of the

577
00:34:54,630 --> 00:34:57,420
coordination complexes

578
00:34:58,130 --> 00:35:00,490
so all of these series

579
00:35:00,510 --> 00:35:05,600
they require you to think in three dimensions about the orbitals because they're all about

580
00:35:05,600 --> 00:35:09,080
the interactions with ligands either covalently or

581
00:35:09,120 --> 00:35:14,240
ionic interactions but they are all about the interactions of ligands with those d orbitals

582
00:35:14,290 --> 00:35:17,400
so we i mentioned last time you need to know the shapes of the d

583
00:35:17,400 --> 00:35:22,960
orbitals and you need to be able to draw them to some limited liability

584
00:35:22,970 --> 00:35:26,930
so we went through the names of the d orbitals last time let's just review

585
00:35:26,930 --> 00:35:30,270
it now because we're going to be talking about this more today

586
00:35:30,380 --> 00:35:33,970
so what is this guy here

587
00:35:34,230 --> 00:35:36,040
he z squared

588
00:35:36,060 --> 00:35:41,500
and here's our reference frame so these up here why along here x is coming

589
00:35:41,510 --> 00:35:42,950
out towards you

590
00:35:43,000 --> 00:35:46,380
so what about this one

591
00:35:46,850 --> 00:35:49,900
d x squared minus y squared

592
00:35:50,020 --> 00:35:53,080
down here with that one

593
00:35:53,120 --> 00:36:00,300
dx y so we're from the amplitudes are forty five degrees off the axis in

594
00:36:00,340 --> 00:36:03,330
the x y plane

595
00:36:03,340 --> 00:36:07,720
over here we have the why these an our along the

596
00:36:07,730 --> 00:36:12,300
and the last one is

597
00:36:12,650 --> 00:36:16,530
x the over here

598
00:36:16,580 --> 00:36:20,710
right so those are five d orbitals that will be talking about a lot in

599
00:36:20,720 --> 00:36:22,940
the next two lectures

600
00:36:22,950 --> 00:36:27,640
all right let me just show you this is all it's really three-dimensional so let's

601
00:36:27,640 --> 00:36:33,360
take a look at the three-dimensional image of these are of these d orbitals

602
00:36:33,380 --> 00:36:35,050
so this is a

603
00:36:35,060 --> 00:36:38,560
square this is in your hand out of the movie

604
00:36:38,570 --> 00:36:46,440
but you can forget that little better sense of the three dimensions here

605
00:36:46,460 --> 00:36:47,310
right so moving

606
00:36:47,320 --> 00:36:50,780
first rotate the axes russia why does that

607
00:36:50,820 --> 00:36:55,910
and then it's going to rotate the idea orbitals so you can see the donut

608
00:36:55,920 --> 00:36:57,860
in the middle with the whole

609
00:36:57,880 --> 00:37:01,270
and the maximum amplitude are long

610
00:37:02,190 --> 00:37:05,850
the axis here this is is square

611
00:37:05,890 --> 00:37:08,730
and it's right along the orbitals right along

612
00:37:08,740 --> 00:37:12,260
this the axis which is important

613
00:37:12,270 --> 00:37:14,640
so now let's take a look at the

614
00:37:14,660 --> 00:37:17,080
x squared minus twice square

615
00:37:18,390 --> 00:37:20,850
and the important point here

616
00:37:20,870 --> 00:37:26,050
is that the orbitals are directly on axis and the axes are

617
00:37:26,050 --> 00:37:30,650
can generalize that slightly i can say that if i have a bunch of nonnegative

618
00:37:30,650 --> 00:37:33,840
parameters lambda i which sum to one

619
00:37:33,850 --> 00:37:36,400
like probabilities if you like

620
00:37:36,420 --> 00:37:38,180
and the someone i

621
00:37:40,550 --> 00:37:42,800
lambda i times

622
00:37:42,810 --> 00:37:45,350
this convex function which this case is z

623
00:37:45,360 --> 00:37:47,150
i'm sure that i

624
00:37:47,470 --> 00:37:50,650
is less than or equal to the log

625
00:37:50,710 --> 00:37:56,810
of the someone i lambda i said i

626
00:37:56,860 --> 00:37:58,600
so that's just saying that the

627
00:37:58,620 --> 00:38:02,620
the linear combination of the logs which is the point here

628
00:38:02,630 --> 00:38:04,620
it lies on or below

629
00:38:04,670 --> 00:38:08,040
the log of the linear combination which is this point here just by saying that

630
00:38:08,150 --> 00:38:10,780
convex function

631
00:38:10,860 --> 00:38:15,480
and then if you allow me the liberty of replacing the sum by an integration

632
00:38:15,540 --> 00:38:17,090
weighted now by

633
00:38:17,100 --> 00:38:19,600
the probability density nonnegative

634
00:38:19,650 --> 00:38:21,930
function which integrates to one

635
00:38:22,940 --> 00:38:27,210
so we have the following property that the integral of q

636
00:38:27,250 --> 00:38:29,260
log of p over q

637
00:38:29,310 --> 00:38:32,400
minus that in z

638
00:38:32,450 --> 00:38:35,590
so this is a convex combination of the log

639
00:38:35,690 --> 00:38:37,440
the function

640
00:38:38,840 --> 00:38:43,400
i can apply this inequality the minus signs of the inequality is reversed there's no

641
00:38:43,410 --> 00:38:46,110
greater than or equal to minus

642
00:38:47,780 --> 00:38:49,310
of the integral

643
00:38:49,350 --> 00:38:53,920
of q times the function which is here the q

644
00:38:55,700 --> 00:39:00,230
now the two cues will cancelled

645
00:39:01,420 --> 00:39:03,180
entropy is one

646
00:39:03,230 --> 00:39:06,530
so on the right-hand side have minus the log one

647
00:39:06,540 --> 00:39:07,910
which is zero

648
00:39:07,960 --> 00:39:09,640
so the

649
00:39:09,650 --> 00:39:13,250
the KL divergence is greater than or equal to zero

650
00:39:13,480 --> 00:39:16,940
the result

651
00:39:16,950 --> 00:39:19,470
that was one property the KL divergence

652
00:39:19,490 --> 00:39:24,830
so another property the KL divergence another property the KL divergence is the

653
00:39:25,380 --> 00:39:30,210
his the definition

654
00:39:30,220 --> 00:39:36,060
recall that if if he i said he was set equal to

655
00:39:36,100 --> 00:39:39,230
he said

656
00:39:39,280 --> 00:39:42,850
this is a q which is one of the longer one is zero the average

657
00:39:42,910 --> 00:39:45,670
zero is zero so

658
00:39:45,680 --> 00:39:48,670
here equals p implies the KL

659
00:39:48,720 --> 00:39:51,240
vanishes vanishes when

660
00:39:51,250 --> 00:39:55,390
q is equal to p

661
00:39:55,450 --> 00:39:58,180
that's the only situation in which vanishes

662
00:39:58,230 --> 00:39:59,480
so we can

663
00:39:59,490 --> 00:40:05,330
we can show that as follows

664
00:40:05,350 --> 00:40:08,470
and i apologize if the mathematicians in the audience to find the proof less than

665
00:40:08,470 --> 00:40:12,890
rigorous but at least it's not intuitive

666
00:40:12,930 --> 00:40:18,440
derivation so that's the that's the KL divergence let's look at a stationary point

667
00:40:18,490 --> 00:40:19,690
so will will

668
00:40:19,900 --> 00:40:22,450
stationary points with respect to q z

669
00:40:22,990 --> 00:40:27,240
maybe this is a good excuse to explain why all this is called variational because

670
00:40:27,250 --> 00:40:29,250
recall that q of z

671
00:40:29,280 --> 00:40:30,620
is actually a function

672
00:40:30,640 --> 00:40:33,570
so this thing this KL divergence

673
00:40:34,650 --> 00:40:36,110
is the function now

674
00:40:37,010 --> 00:40:41,290
the function or rather the probability distribution qz

675
00:40:42,150 --> 00:40:46,110
o functionality something we you take function input output the number

676
00:40:46,130 --> 00:40:49,640
seven function takes the numbers input and output number

677
00:40:49,720 --> 00:40:51,700
in calculus

678
00:40:51,740 --> 00:40:53,490
we differentiate functions

679
00:40:53,510 --> 00:40:57,020
and in the calculus of variations we differentiate function functionalities

680
00:40:57,030 --> 00:40:59,890
so a typical problem in the calculus of variations

681
00:40:59,930 --> 00:41:03,220
would be the following i got a point a and point b

682
00:41:03,230 --> 00:41:06,490
i want to find the shortest path between a and b

683
00:41:06,530 --> 00:41:09,590
the path is defined by some curve

684
00:41:09,650 --> 00:41:14,290
the distance to affect curve is s of x

685
00:41:14,300 --> 00:41:17,110
and the distance the length along the path

686
00:41:17,160 --> 00:41:19,590
is the functionality function as of

687
00:41:19,680 --> 00:41:21,690
this effects

688
00:41:21,700 --> 00:41:23,560
in other words

689
00:41:23,580 --> 00:41:26,470
for a given function of x this is just the number

690
00:41:26,480 --> 00:41:31,820
and in countless variations we differentiating respect to functions not just to a finite set

691
00:41:31,820 --> 00:41:32,800
of numbers

692
00:41:32,820 --> 00:41:35,350
looking for stationary point usually point

693
00:41:35,390 --> 00:41:37,860
is easily shown to be a straight line

694
00:41:37,880 --> 00:41:40,830
i want to point

695
00:41:40,840 --> 00:41:42,360
to do the same thing over here

696
00:41:42,980 --> 00:41:47,530
differentiate this with respect to the function QZ so what we're doing

697
00:41:47,580 --> 00:41:48,820
it is we are

698
00:41:48,840 --> 00:41:52,220
exploring all possible functions qz

699
00:41:52,230 --> 00:42:00,300
finding stationary points with respect to all possible choices for q of said

700
00:42:00,310 --> 00:42:02,860
one thing we must do of course is to

701
00:42:03,580 --> 00:42:05,950
it's all functions qz such that

702
00:42:05,970 --> 00:42:08,800
subject to the normalization condition

703
00:42:08,850 --> 00:42:11,330
so we need to add electrons multiplier

704
00:42:11,360 --> 00:42:13,260
which is q

705
00:42:13,310 --> 00:42:15,510
the interval of q dz

706
00:42:15,520 --> 00:42:17,670
is equal to one

707
00:42:17,700 --> 00:42:23,600
and now we can differentiate with respect to q

708
00:42:23,610 --> 00:42:25,510
and so we end up with

709
00:42:25,540 --> 00:42:29,690
the following with the functional derivative to zero

710
00:42:29,740 --> 00:42:31,700
so there's is a q here

711
00:42:31,770 --> 00:42:36,030
so we can differentiate if we differentiate back you will get longer

712
00:42:36,030 --> 00:42:39,270
he z minus log

713
00:42:41,900 --> 00:42:44,440
there's another hu here longer q

714
00:42:44,450 --> 00:42:48,590
to the derivative of the log is one of the few that will cancel this

715
00:42:48,590 --> 00:42:51,820
user will be left with a minus one

716
00:42:51,820 --> 00:42:53,520
the final q here

717
00:42:53,540 --> 00:42:56,320
his first music

718
00:42:56,330 --> 00:43:00,240
and if i exponentially as it says that qz

719
00:43:00,300 --> 00:43:03,130
is some constant times p of z

720
00:43:03,390 --> 00:43:08,720
but q and p have to integrate to one so the constant must be one

721
00:43:08,720 --> 00:43:11,080
and so he said

722
00:43:11,170 --> 00:43:13,910
equals p of z

723
00:43:13,910 --> 00:43:18,690
so so we left that out but that's a good point that we've where we

724
00:43:18,690 --> 00:43:23,570
have a very simplified model only trying to relate sort of the what we might

725
00:43:23,570 --> 00:43:28,190
consider sort of the instantaneous positions articulators to the acoustic output

726
00:43:28,200 --> 00:43:32,960
so it is simplified in that respect so what i'm trying to do is just

727
00:43:32,960 --> 00:43:38,310
make the relationship between two the shape of the vocal tract and the acoustics that

728
00:43:38,310 --> 00:43:39,120
come out

729
00:43:39,170 --> 00:43:43,650
and this is about the time that all the computer scientists get up and run

730
00:43:43,650 --> 00:43:47,750
the course right where i didn't sign up for circuit theory but the point is

731
00:43:47,750 --> 00:43:53,580
that we can very easily use this electrical model where all these electrical components are

732
00:43:53,580 --> 00:43:59,820
defined in terms of of these sort of acoustic quantities to derive the wave equation

733
00:43:59,860 --> 00:44:06,040
and actually we can do that by just writing circuit equations here and letting these

734
00:44:06,480 --> 00:44:12,250
distances go to infinitesimal quantities and we get these differential equations and then a we

735
00:44:12,250 --> 00:44:15,900
can plug in boundary conditions i can have a simple

736
00:44:15,910 --> 00:44:20,610
vocal tract up there at the time where we have the

737
00:44:20,850 --> 00:44:23,530
volume velocity into the glottis

738
00:44:23,540 --> 00:44:27,900
coming out of the let's i want to know the transfer function that from the

739
00:44:27,900 --> 00:44:32,870
input to the output i can assume a certain form of the solution to the

740
00:44:32,870 --> 00:44:37,450
wave equation plugin boundary conditions i could get i can groups how can i could

741
00:44:37,570 --> 00:44:38,490
get this

742
00:44:40,110 --> 00:44:44,570
self for the transfer function and i hope i get something i i get a

743
00:44:44,690 --> 00:44:46,530
a expression for the

744
00:44:47,170 --> 00:44:52,450
transfer function of the vocal tract which is in terms of a propagation constant and

745
00:44:52,450 --> 00:44:55,490
what's important here is the length of this too

746
00:44:55,530 --> 00:45:01,620
this led to the two something that depends generally on the individual i might have

747
00:45:01,620 --> 00:45:04,480
a longer or shorter vocal tract than you do

748
00:45:04,480 --> 00:45:07,140
it's something it speakers speaker specific

749
00:45:09,170 --> 00:45:15,480
and again rushing and i could go through here for a typical value of

750
00:45:15,500 --> 00:45:18,150
link seventeen and a half centimetre

751
00:45:18,160 --> 00:45:20,230
acoustic tube

752
00:45:20,240 --> 00:45:29,600
corresponds to a typical male saying i will get performance structure here that is roughly

753
00:45:30,050 --> 00:45:36,390
a sitting here in five hundred since equally spaced at one thousand hertz centers i'm

754
00:45:36,490 --> 00:45:40,830
making certain assumptions that i can make the band big smaller

755
00:45:41,580 --> 00:45:45,560
the the message here is that for a this simple model

756
00:45:45,570 --> 00:45:52,110
i can have formant frequencies that have that they have certain forms frequencies and the

757
00:45:52,110 --> 00:45:58,450
second bullet here is that these formant frequencies are inversely proportional to the two blank

758
00:45:58,450 --> 00:46:00,910
with the tube length is dependent on the speaker

759
00:46:00,950 --> 00:46:04,040
so in this implies that the effects of

760
00:46:04,070 --> 00:46:06,370
speaker dependent variability

761
00:46:06,400 --> 00:46:08,620
can be reduced simply

762
00:46:10,230 --> 00:46:18,440
normalizing along the frequency axis so in other words i had this physiologically plausible transformation

763
00:46:18,450 --> 00:46:21,190
i can perform

764
00:46:21,240 --> 00:46:28,890
normalizing a normalizing frequency axis that has this direct correlates in the in the articulatory

765
00:46:29,780 --> 00:46:30,890
and so

766
00:46:30,940 --> 00:46:35,120
this has led to an people have been doing this for years has has led

767
00:46:35,750 --> 00:46:39,020
the use of of frequency warping

768
00:46:39,030 --> 00:46:43,190
as as a speaker normalisation approach in in ASR

769
00:46:43,200 --> 00:46:44,700
and the

770
00:46:45,730 --> 00:46:51,240
one the way this is performed as well but it was will basically what's the

771
00:46:51,240 --> 00:46:58,070
frequency axis over a relatively small range were compressed by around ten to twenty percent

772
00:46:58,080 --> 00:46:59,810
will generate the

773
00:46:59,810 --> 00:47:01,500
actual acoustic

774
00:47:02,080 --> 00:47:06,850
the actual acoustic features associated with these worked utterances

775
00:47:06,890 --> 00:47:09,360
and and then we to choose the best warping

776
00:47:09,360 --> 00:47:12,250
as being that those

777
00:47:12,410 --> 00:47:15,820
is corresponding to those worked features

778
00:47:15,870 --> 00:47:21,250
that somehow maximize the likelihood of our utterance with respect to the model that we

779
00:47:21,250 --> 00:47:22,960
use in speech recognition

780
00:47:23,770 --> 00:47:28,430
this is the end result is is you could also use some training to give

781
00:47:28,430 --> 00:47:32,700
you more sort of compact models but the end result here is

782
00:47:34,020 --> 00:47:35,500
we were able to

783
00:47:35,660 --> 00:47:38,990
get a really significant increase increase in performance

784
00:47:39,020 --> 00:47:41,600
with a very small amount of data

785
00:47:41,650 --> 00:47:46,140
the amount of data that would require a to get this similar increase in performance

786
00:47:46,140 --> 00:47:52,180
by more traditional adaptation algorithms that don't really have any sort of physical correlates with

787
00:47:52,460 --> 00:47:57,740
the amount of data for that which is maybe almost an order of magnitude more

788
00:47:58,790 --> 00:48:01,770
this is the sort of the message here is is it for

789
00:48:01,780 --> 00:48:07,000
even in in the sort of traditional algorithms that were implementing in our traditional ASR

790
00:48:07,000 --> 00:48:14,060
systems when we have a sort of of physical justification to them

791
00:48:14,070 --> 00:48:16,120
this week there can be

792
00:48:16,120 --> 00:48:20,400
lies outside the boundary which is also

793
00:48:20,400 --> 00:48:22,310
five hundred years

794
00:48:22,330 --> 00:48:24,790
the one we don't need to practice

795
00:48:24,810 --> 00:48:31,580
i changed the problem because this between our web site will be pushed

796
00:48:37,540 --> 00:48:42,580
so called from the the twenty one

797
00:48:42,690 --> 00:48:44,000
what we

798
00:48:44,020 --> 00:48:46,190
because of that when

799
00:48:47,170 --> 00:48:49,730
the people

800
00:48:52,500 --> 00:48:55,310
the selection

801
00:48:55,330 --> 00:48:57,040
first of all the time

802
00:48:57,190 --> 00:48:58,830
to try to

803
00:48:58,850 --> 00:49:00,580
so that that's where

804
00:49:00,600 --> 00:49:02,900
so the bot

805
00:49:02,920 --> 00:49:04,540
the small

806
00:49:04,560 --> 00:49:05,960
because it

807
00:49:05,980 --> 00:49:06,980
i don't

808
00:49:06,980 --> 00:49:09,980
the about distribution of what state

809
00:49:15,210 --> 00:49:15,920
we can

810
00:49:16,230 --> 00:49:19,880
it will provide

811
00:49:19,900 --> 00:49:24,500
the way the way so i have point in ways

812
00:49:24,500 --> 00:49:27,400
which are distributed equally

813
00:49:27,420 --> 00:49:29,190
there are no

814
00:49:29,210 --> 00:49:31,630
some query

815
00:49:32,000 --> 00:49:39,830
don't have something like this is the

816
00:49:39,900 --> 00:49:41,150
the case

817
00:49:41,230 --> 00:49:44,400
it's clear that this was a great

818
00:49:44,420 --> 00:49:48,150
the small amount of white the along the boundary

819
00:49:49,630 --> 00:49:55,980
the smallest distance to practice before three

820
00:49:56,900 --> 00:49:59,710
so first of all all

821
00:49:59,730 --> 00:50:01,960
it also like such

822
00:50:01,980 --> 00:50:04,170
partition of

823
00:50:04,190 --> 00:50:07,130
but perhaps the smaller

824
00:50:07,190 --> 00:50:12,130
and we can mention that actually should select one

825
00:50:12,150 --> 00:50:13,250
one of the space

826
00:50:13,310 --> 00:50:16,810
because it was the first step toward

827
00:50:16,850 --> 00:50:19,630
about it will have like this one

828
00:50:20,830 --> 00:50:24,150
it was selected by for

829
00:50:24,170 --> 00:50:25,770
on the

830
00:50:25,790 --> 00:50:27,310
also managed

831
00:50:28,330 --> 00:50:33,290
it will be like this it's possible to follow the links to

832
00:50:33,310 --> 00:50:35,440
already smaller than this one

833
00:50:35,460 --> 00:50:39,610
but this was one of the first one which is like

834
00:50:39,810 --> 00:50:44,020
and the kind of place you will have smaller power

835
00:50:48,870 --> 00:50:53,440
and our approach

836
00:50:53,460 --> 00:50:55,250
so to try

837
00:50:55,270 --> 00:50:59,750
partition by what to

838
00:51:01,210 --> 00:51:04,080
i do we have one

839
00:51:04,100 --> 00:51:08,210
in other quality and

840
00:51:08,270 --> 00:51:09,480
we want to

841
00:51:09,480 --> 00:51:13,830
the ranges of british actually

842
00:51:13,880 --> 00:51:18,560
so it really well multiply that is

843
00:51:18,580 --> 00:51:21,290
and our

844
00:51:23,370 --> 00:51:26,060
you might end

845
00:51:26,110 --> 00:51:29,650
tools like that for

846
00:51:33,080 --> 00:51:33,960
i mean

847
00:51:35,150 --> 00:51:37,250
one romantic what we need

848
00:51:37,250 --> 00:51:41,790
from one twenty two hours i space so

849
00:51:41,790 --> 00:51:44,580
select the best rating

850
00:51:48,610 --> 00:51:51,880
many had not just by the way

851
00:51:52,630 --> 00:51:57,210
they how that first when the point

852
00:51:59,210 --> 00:52:00,400
by buyer

853
00:52:00,420 --> 00:52:01,330
for example

854
00:52:01,500 --> 00:52:05,210
these points be managed

855
00:52:05,230 --> 00:52:06,870
and we

856
00:52:08,330 --> 00:52:11,270
the standard deviation of

857
00:52:12,600 --> 00:52:14,270
OK well this

858
00:52:14,290 --> 00:52:16,060
for every

859
00:52:16,080 --> 00:52:22,350
this image so this is the one that want to say

860
00:52:24,330 --> 00:52:25,830
and was that

861
00:52:25,850 --> 00:52:29,060
that would put them up there

862
00:52:29,060 --> 00:52:30,730
what does it mean

863
00:52:30,730 --> 00:52:33,900
the division is

864
00:52:33,940 --> 00:52:38,850
we want to select one change the according space so which are

865
00:52:38,870 --> 00:52:44,440
why from all at once and the bigger than that

866
00:52:46,940 --> 00:52:49,960
big distribution

867
00:52:57,130 --> 00:53:01,150
we should actually from all these points that one

868
00:53:02,810 --> 00:53:04,600
we have a

869
00:53:04,600 --> 00:53:10,060
when you have something like here and division the like

870
00:53:10,080 --> 00:53:11,190
from the disease

871
00:53:11,230 --> 00:53:12,310
and if take

872
00:53:12,330 --> 00:53:15,480
this one so will be

873
00:53:15,480 --> 00:53:21,230
problems with the standard deviation is much smaller because all

874
00:53:21,650 --> 00:53:26,000
close disease one of them is one

875
00:53:26,080 --> 00:53:28,690
o point which

876
00:53:28,710 --> 00:53:32,850
class one

877
00:53:38,040 --> 00:53:39,400
like one range

878
00:53:39,760 --> 00:53:41,080
the device

879
00:53:42,250 --> 00:53:44,830
done owing to lack of data

880
00:53:48,770 --> 00:53:51,060
beginning of the first

881
00:53:52,290 --> 00:53:55,100
and this is the right

882
00:53:56,270 --> 00:53:58,110
three three areas

883
00:53:58,130 --> 00:54:00,420
and here we are

884
00:54:00,480 --> 00:54:01,900
well also

885
00:54:01,960 --> 00:54:04,040
no no what

886
00:54:04,690 --> 00:54:09,080
also they were ones i was that good quality

887
00:54:09,830 --> 00:54:11,420
i mean these

888
00:54:11,440 --> 00:54:17,810
it was selected this weighted start with

889
00:54:17,850 --> 00:54:22,810
well first of all that these

890
00:54:22,830 --> 00:54:25,940
and this the first issue like that

891
00:54:28,110 --> 00:54:29,440
the partition

892
00:54:30,310 --> 00:54:34,440
it should that next the point which is the game

893
00:54:34,460 --> 00:54:38,520
the has longest

894
00:54:39,560 --> 00:54:41,560
so like this but

895
00:54:41,710 --> 00:54:44,520
this is the

896
00:54:44,540 --> 00:54:45,940
and and we

897
00:54:45,960 --> 00:54:50,790
and the the partition according to this point you right

898
00:54:50,850 --> 00:54:52,940
and this

899
00:54:57,370 --> 00:54:59,480
when the point

900
00:55:12,750 --> 00:55:14,350
the problem

901
00:55:14,350 --> 00:55:15,730
OK so very

902
00:55:15,730 --> 00:55:18,590
so the elementary example of bayesian learning

903
00:55:18,600 --> 00:55:22,520
the talk was called introduction to beijing learning but it does illustrate a lot of

904
00:55:23,090 --> 00:55:29,190
a lot of the key ideas i think any questions about that

905
00:55:29,200 --> 00:55:32,600
so if you can

906
00:55:33,860 --> 00:55:43,340
the reason we just images go back and take a look at that so

907
00:55:44,320 --> 00:55:45,940
so a model

908
00:55:45,980 --> 00:55:47,270
is that

909
00:55:47,270 --> 00:55:51,560
for any value of x the value of t is drawn from gas distribution which

910
00:55:51,560 --> 00:55:54,350
is centered on some function y x and w

911
00:55:54,350 --> 00:55:58,120
and we've chosen function which is linear in w so this

912
00:55:59,520 --> 00:56:00,300
for t

913
00:56:00,310 --> 00:56:01,190
it is

914
00:56:01,400 --> 00:56:04,030
against his mean is

915
00:56:04,090 --> 00:56:07,180
a linear function of w so

916
00:56:07,200 --> 00:56:09,620
so it's

917
00:56:09,650 --> 00:56:13,480
so p c

918
00:56:24,770 --> 00:56:27,640
it is of the form

919
00:56:29,100 --> 00:56:32,410
so with respect to all of these parameters w

920
00:56:32,430 --> 00:56:35,820
it has this functional form is the same as that of the gaussians is e

921
00:56:35,820 --> 00:56:39,610
to the minus the quadratic function of w

922
00:56:39,840 --> 00:56:44,300
so the density distribution because of the distribution and the w but it has that

923
00:56:44,310 --> 00:56:46,570
form the conjugate prior

924
00:56:46,950 --> 00:56:48,900
is guassian so there's

925
00:56:48,980 --> 00:56:51,720
i have chosen the conjugate priors

926
00:56:51,730 --> 00:56:57,600
because i've chosen the conjugate prior constant country prior is that priors gaston because its

927
00:56:57,600 --> 00:57:01,050
conjugate on observe some data remains guassian

928
00:57:01,140 --> 00:57:03,450
i support data is still guassian

929
00:57:03,480 --> 00:57:04,440
and that's

930
00:57:05,860 --> 00:57:10,860
and has simple sufficient statistics i just need to compute

931
00:57:10,890 --> 00:57:13,940
means and covariances of those phi

932
00:57:14,940 --> 00:57:15,840
and that

933
00:57:15,840 --> 00:57:18,860
vector in this case is just the two-dimensional

934
00:57:18,880 --> 00:57:23,060
that china two by two matrix and about how many observations i compute you just

935
00:57:23,070 --> 00:57:26,470
have to update the finite dimensional set of sufficient statistics

936
00:57:26,490 --> 00:57:28,100
so those are the two sort of

937
00:57:28,110 --> 00:57:30,820
key things was only

938
00:57:38,190 --> 00:57:49,100
oh sorry sequential i just yes the order doesn't matter so

939
00:57:49,110 --> 00:57:53,930
it for these independent observations i can absorb them in any order just the products

940
00:57:53,940 --> 00:57:57,740
so it doesn't matter whether i take the product of the likelihood functions for the

941
00:57:57,740 --> 00:58:02,150
observations and get that big like function multiplied by the prior p the posterior whether

942
00:58:02,150 --> 00:58:05,800
i start with the prior lives on one data point i absorb another

943
00:58:06,010 --> 00:58:07,160
get same answer

944
00:58:08,740 --> 00:58:15,150
sort of

945
00:58:15,230 --> 00:58:17,360
is because we have the same

946
00:58:17,390 --> 00:58:20,530
i don't think

947
00:58:21,930 --> 00:58:26,620
it's because your computing the product of n likelihood functions with the prior if you

948
00:58:26,620 --> 00:58:30,230
multiply you know and plus-one things together as much if you first of all multiply

949
00:58:30,230 --> 00:58:33,410
subset them together and the later multiply and the other thing is the sort of

950
00:58:33,410 --> 00:58:35,900
commutative multiplication

951
00:58:37,270 --> 00:58:39,140
what's going on here

952
00:58:39,140 --> 00:58:42,970
the school moved

953
00:58:48,770 --> 00:58:53,220
on the

954
00:58:54,410 --> 00:58:58,430
so really

955
00:58:58,660 --> 00:59:03,810
these are sensor data points so we're

956
00:59:03,850 --> 00:59:10,060
for example if if the temperature is above thirty centigrade you don't observe that value

957
00:59:10,060 --> 00:59:12,990
so that's example data points not being missing at random

958
00:59:13,010 --> 00:59:14,990
so you

959
00:59:15,450 --> 00:59:19,350
so the missing data course are very important

960
00:59:19,350 --> 00:59:23,990
aspect of many practical applications is often assumed is convenient to do so the data

961
00:59:23,990 --> 00:59:28,240
is missing at random so somebody just randomly removes subset of data has to be

962
00:59:28,240 --> 00:59:33,340
very easy to deal with because you just marginalize that is unobserved variables if the

963
00:59:33,740 --> 00:59:37,400
missing this depends for example on the value of the data points and you have

964
00:59:37,440 --> 00:59:42,270
no missing at random and you could i don't have experience with this but you

965
00:59:42,270 --> 00:59:45,900
can in principle model this within the context of the graphical models you can model

966
00:59:46,010 --> 00:59:48,010
the missing this process

967
00:59:48,030 --> 00:59:51,150
and it will be missing this random it is something that you can but you

968
00:59:51,150 --> 00:59:54,510
can model and then do inference on that more complex models

969
00:59:54,850 --> 01:00:00,740
three other classical methods

970
01:00:00,760 --> 01:00:15,010
requirements the the relationships in the priors in this century

971
01:00:15,400 --> 01:00:19,590
what can you try to make a connection since entering the choice of prior use

972
01:00:47,490 --> 01:00:50,100
i'm not sure it's necessary the prior i think it's the process by which the

973
01:00:50,100 --> 01:00:53,060
observations get censored but you want to model

974
01:00:53,160 --> 01:00:54,400
not necessarily the

975
01:00:54,410 --> 01:00:58,380
but you write down the generative process of how these data points came to be

976
01:00:58,380 --> 01:01:00,100
and how they got censored

977
01:01:00,100 --> 01:01:01,940
if you can write down that model

978
01:01:01,940 --> 01:01:03,760
and then you in good shape

979
01:01:04,240 --> 01:01:08,990
should we this is quite detailed as a fascinating topic should just take it offline

980
01:01:09,010 --> 01:01:13,530
getting quite short on time and this one final topic very keen to please mention

981
01:01:13,550 --> 01:01:14,970
briefly so us

982
01:01:14,980 --> 01:01:18,890
if that's the case of the other points of clarification

983
01:01:18,900 --> 01:01:19,900
but have said

984
01:01:21,740 --> 01:01:26,090
it is but

985
01:01:26,350 --> 01:01:32,480
you sometimes have to keep

986
01:01:32,800 --> 01:01:35,480
sometimes you have to keep track of things that may be the number of data

987
01:01:36,360 --> 01:01:39,650
you've observed so for instance

988
01:01:39,700 --> 01:01:43,020
you know the gas you just keep the mean and covariance of the data use

989
01:01:43,200 --> 01:01:47,570
so far been absorbed one more data point it matters whether you you know kept

990
01:01:48,840 --> 01:01:50,480
and this is

991
01:01:50,530 --> 01:01:53,310
as the name the school auxiliary

992
01:01:53,310 --> 01:01:56,970
what what they called

993
01:01:56,980 --> 01:01:59,880
you know the the statisticians have a name for these so this is not is

994
01:01:59,880 --> 01:02:04,060
not formally called policticians to stick to just link you know something that the number

995
01:02:04,070 --> 01:02:05,190
of data points

996
01:02:05,200 --> 01:02:08,690
so you don't have to keep track about as well that's also justifies just the

997
01:02:10,220 --> 01:02:13,020
so doesn't the key point is the that amount stuff you have to store is

998
01:02:13,020 --> 01:02:17,160
not growing as the dataset grows is finite

999
01:02:18,520 --> 01:02:23,240
but for the likelihood functions can those ages don't matter because you're you're interested its

1000
01:02:23,240 --> 01:02:24,880
functional dependence

1001
01:02:24,900 --> 01:02:26,120
on eta

1002
01:02:26,120 --> 01:02:29,770
minimize the problem of all want to maximize

1003
01:02:33,230 --> 01:02:35,950
let's see some

1004
01:02:36,030 --> 01:02:40,380
introduce a bit more the notation for the classification

1005
01:02:40,390 --> 01:02:41,880
we have find

1006
01:02:41,890 --> 01:02:46,110
there are in the dimensional space

1007
01:02:46,120 --> 01:02:49,490
two each data point to label is associated

1008
01:02:49,570 --> 01:02:52,320
but this time the label can be done on two

1009
01:02:52,340 --> 01:02:53,310
can on

1010
01:02:53,320 --> 01:02:55,150
plus one minus one

1011
01:02:55,160 --> 01:02:59,010
we are looking for additional functions

1012
01:02:59,590 --> 01:03:01,840
i put to

1013
01:03:03,280 --> 01:03:05,770
by the way the following

1014
01:03:05,790 --> 01:03:11,770
with a slight abuse of notation just use that as follows

1015
01:03:11,790 --> 01:03:14,130
including by

1016
01:03:14,470 --> 01:03:17,290
do it but i'm going to fix

1017
01:03:18,340 --> 01:03:25,180
this is the formulation for fisher discriminant analysis or in discriminant analysis

1018
01:03:25,200 --> 01:03:26,510
and you can see that

1019
01:03:26,520 --> 01:03:31,090
it is simply changing the set of the label and we are solving the same

1020
01:03:31,110 --> 01:03:37,660
problem and and sort being different task so from regression to classification

1021
01:03:37,680 --> 01:03:39,940
we have we can solve exactly

1022
01:03:39,960 --> 01:03:45,780
in fact what the fisher discriminant analysis is doing

1023
01:03:45,880 --> 01:03:51,560
is maximized the function that represents the difference between the mean

1024
01:03:52,730 --> 01:03:57,370
two classes normalized by a measure the

1025
01:03:57,510 --> 01:04:00,400
represent the spread of the two

1026
01:04:00,420 --> 01:04:02,410
of the data

1027
01:04:04,360 --> 01:04:11,990
this is the typical correlation that is minimized by maximizing fisher's discriminant analysis so maximize

1028
01:04:12,680 --> 01:04:17,870
the distance between the mean of the two classes normalized by the sum of standard

1029
01:04:19,060 --> 01:04:22,450
we can rewrite to be more of that

1030
01:04:22,460 --> 01:04:25,940
w by considering what happened

1031
01:04:25,950 --> 01:04:27,510
well known

1032
01:04:28,760 --> 01:04:35,780
between between the metrics and within class scatter matrix

1033
01:04:37,700 --> 01:04:43,120
the this is the formulation of the problem of fisher discriminant analysis

1034
01:04:43,760 --> 01:04:47,720
we can only for these also will arrive before

1035
01:04:47,740 --> 01:04:53,890
and this solution is the same of course but what we see before

1036
01:04:53,920 --> 01:04:59,270
later we see an interpretation of fisher discriminant analysis

1037
01:05:00,030 --> 01:05:05,920
i would talk now about some again by this problem

1038
01:05:06,050 --> 01:05:08,170
well known

1039
01:05:08,190 --> 01:05:12,340
principal component analysis can be cast by problem

1040
01:05:12,360 --> 01:05:18,610
or the problem that i want to find the relationship between the data sets

1041
01:05:18,630 --> 01:05:26,280
OK so again by the problems for example by combining analysis on the data the

1042
01:05:26,530 --> 01:05:28,400
each summer

1043
01:05:29,270 --> 01:05:35,670
easy to think about problem fashion square the the problem of canonical correlation i can

1044
01:05:35,720 --> 01:05:37,400
the above problem

1045
01:05:37,420 --> 01:05:41,060
for all of them economic

1046
01:05:41,090 --> 01:05:47,360
and we can stand or to this technique tool finding relationship between more than two

1047
01:05:47,360 --> 01:05:54,260
data sources and we will see the way canonical correlation analysis

1048
01:05:54,270 --> 01:05:58,860
it is the general form of an eigenvalue problem

1049
01:05:58,870 --> 01:06:05,240
this is not because we have a nice properties on a problem like that

1050
01:06:05,270 --> 01:06:11,700
first of all that for many methodists begin value in a vector v

1051
01:06:11,720 --> 01:06:15,140
then also that you have to

1052
01:06:15,160 --> 01:06:16,710
and the

1053
01:06:16,730 --> 01:06:21,330
good news is also if we are able to ask what problem and then again

1054
01:06:21,330 --> 01:06:27,980
by the problem you have to compute because of a lot of studies of technique

1055
01:06:28,090 --> 01:06:32,300
can solve the fact that using these problems

1056
01:06:32,310 --> 01:06:35,950
so let's talk about the most famous line that is

1057
01:06:35,960 --> 01:06:38,140
think by combining that

1058
01:06:38,150 --> 01:06:41,300
in first component analysis we want to find

1059
01:06:42,930 --> 01:06:45,870
of large variance in the data

1060
01:06:45,890 --> 01:06:48,470
and we are maximizing

1061
01:06:49,020 --> 01:06:57,050
this a function that represent the divide of the projection of the data set on

1062
01:06:58,240 --> 01:06:59,700
and again

1063
01:06:59,720 --> 01:07:01,770
and this is the type the function

1064
01:07:01,840 --> 01:07:06,060
and we can impose their capacity control

1065
01:07:06,110 --> 01:07:12,200
just recall the framework introduced by now look imposing the normal that is equal to

1066
01:07:14,250 --> 01:07:18,440
how do we solve problems again we can compute the ground

1067
01:07:18,440 --> 01:07:19,480
and so on

1068
01:07:19,640 --> 01:07:21,710
following a by so

1069
01:07:21,720 --> 01:07:24,270
as you can see that the problem

1070
01:07:24,620 --> 01:07:30,700
of this form

1071
01:07:30,800 --> 01:07:37,960
another way to formulate principal component analysis is used by the shell

1072
01:07:37,960 --> 01:07:46,020
and the math except for all that he can't covariance matrix that is positive semidefinite

1073
01:07:46,970 --> 01:07:48,460
symmetric matrix

1074
01:07:49,990 --> 01:07:56,930
the solution of this problem is given by the las vegas the again vector corresponding

1075
01:07:56,950 --> 01:07:59,040
to the largest i

1076
01:07:59,060 --> 01:08:01,200
the value of the ratio

1077
01:08:01,210 --> 01:08:04,800
they correspond to the largest value

1078
01:08:04,830 --> 01:08:14,400
if we started to play the data then we get the other

1079
01:08:14,400 --> 01:08:22,910
five combining and this is usually used for dimensionality reduction would need to approximate how

1080
01:08:22,910 --> 01:08:26,090
about making a low dimensional subspace

1081
01:08:26,110 --> 01:08:32,870
so from k two d with a much much smaller than d

1082
01:08:32,890 --> 01:08:35,770
why do we need dimensionality reduction

1083
01:08:35,790 --> 01:08:41,140
for many things for some high for visualisation but was is imagine that we want

1084
01:08:41,140 --> 01:08:46,890
to be able to do that in two d two dimensional three dimension

1085
01:08:47,070 --> 01:08:53,290
we can use dimensionality reduction from the impression when we need to store

1086
01:08:53,320 --> 01:08:55,830
the amount of that like images

1087
01:08:55,840 --> 01:08:59,670
we can use dimensionality reduction for example

1088
01:08:59,680 --> 01:09:02,770
forty nine because

1089
01:09:02,800 --> 01:09:06,910
we remove small noise action somehow

1090
01:09:07,150 --> 01:09:09,300
dimensionality reduction

1091
01:09:09,360 --> 01:09:14,530
very often imply information loss but we have no guarantee that PCA

1092
01:09:14,580 --> 01:09:20,410
what information as possible i say not always because

1093
01:09:20,410 --> 01:09:25,440
let's assume that i for example one feature that is constant for all the data

1094
01:09:25,440 --> 01:09:26,710
points that

1095
01:09:26,760 --> 01:09:30,820
if we project our back to the

1096
01:09:30,840 --> 01:09:32,480
dimension d minus one

1097
01:09:32,510 --> 01:09:37,530
we don't use the information but this is in general the case we missing information

1098
01:09:37,550 --> 01:09:40,210
so is how do we

1099
01:09:40,220 --> 01:09:47,130
project to our about dimensional space how we get these compact representation of our by

1100
01:09:47,130 --> 01:09:50,950
considering the begin that correspond to the

1101
01:09:50,960 --> 01:09:53,340
the largest eigen values

1102
01:09:53,370 --> 01:10:01,400
send it usually relies on many of going back to the

1103
01:10:01,610 --> 01:10:04,540
he managed to components that we keep

1104
01:10:04,560 --> 01:10:11,610
was studied by the spectrum so we study divide and explained by

1105
01:10:11,610 --> 01:10:13,030
all the components

1106
01:10:13,050 --> 01:10:20,010
and this is an example of taking value spectrum we we can for example use

1107
01:10:20,030 --> 01:10:21,860
this rule to choose k

1108
01:10:21,870 --> 01:10:26,280
well actually it into some of the first k

1109
01:10:26,280 --> 01:10:30,890
we can deal with this partial occlusion because we still recognise parts perhaps on this

1110
01:10:30,890 --> 01:10:34,030
side of the car even inside the because including OK

1111
01:10:34,050 --> 01:10:37,340
this is sort of some of the reasons these parts methods have become popular

1112
01:10:40,760 --> 01:10:45,480
in this tutorial these things to concentrate on going to go through a set of

1113
01:10:45,510 --> 01:10:52,110
models applications papers that people have developed over the last two three years and was

1114
01:10:52,120 --> 01:10:54,460
going to be in common is going to be some parts

1115
01:10:54,470 --> 01:10:57,440
is going to be a structure term

1116
01:10:57,450 --> 01:11:01,510
and maybe some learning and some recognition OK what would differ

1117
01:11:01,540 --> 01:11:06,190
is how these parts are defined how they learnt the same with the structure of

1118
01:11:06,200 --> 01:11:07,350
the different models

1119
01:11:07,360 --> 01:11:12,830
sometimes it will be very loose model of the structure just saying something one part

1120
01:11:12,830 --> 01:11:16,240
is to the left of another part of it could be very tight models like

1121
01:11:16,240 --> 01:11:19,600
everything is modeled by a calcium

1122
01:11:19,610 --> 01:11:24,750
learning methods differ the order in which the parts of structural and whatever

1123
01:11:24,770 --> 01:11:30,450
and to course what's important in terms of relevance as well as the complexity how

1124
01:11:30,490 --> 01:11:32,400
how long does this actually takes to run

1125
01:11:32,410 --> 01:11:35,830
and to learn

1126
01:11:35,880 --> 01:11:39,420
right so these are not going to cover the start of with

1127
01:11:39,880 --> 01:11:45,940
papers methods that first of all of the parts and then at the structure

1128
01:11:45,960 --> 01:11:48,880
gloss on top of that the case parts are important

1129
01:11:48,890 --> 01:11:50,950
now comes to contrast that with

1130
01:11:50,970 --> 01:11:52,570
methods which

1131
01:11:52,580 --> 01:11:56,900
start really the structure of the structure is the primary thing

1132
01:11:57,600 --> 01:12:00,110
i'll take a fair bit although i'm going to go on

1133
01:12:00,120 --> 01:12:05,670
class of models where the parts and structures are fairly equal and in fact they

1134
01:12:05,720 --> 01:12:08,550
learned simultaneously it's not you know one and the other

1135
01:12:08,610 --> 01:12:12,870
now we work that we've been doing on what's called the constellation model

1136
01:12:12,880 --> 01:12:15,910
and finally i end up with some

1137
01:12:15,930 --> 01:12:18,230
summary is state-of-the-art

1138
01:12:18,240 --> 01:12:19,340
they on two

1139
01:12:19,440 --> 01:12:23,050
pascal challenge

1140
01:12:23,060 --> 01:12:27,630
OK so this is starting off with models were the first step is to learn

1141
01:12:30,020 --> 01:12:32,830
this is the basic idea is somewhere in the image

1142
01:12:33,340 --> 01:12:36,760
and you want to home in on the bits of the image interesting

1143
01:12:36,810 --> 01:12:37,970
and the way to do that

1144
01:12:38,250 --> 01:12:40,790
tradition is to run an interest point detector

1145
01:12:40,800 --> 01:12:44,840
over the image that throws away the bits of the image like this which have

1146
01:12:44,840 --> 01:12:46,340
nothing of interest in them

1147
01:12:46,460 --> 01:12:48,690
then it finds that the text the image

1148
01:12:48,700 --> 01:12:54,860
so the best interest operators people use common ones are the ones by harris offers

1149
01:12:54,880 --> 01:12:59,080
finds interest operator and around each of these

1150
01:12:59,120 --> 01:13:03,950
interest points you grab a region the size of the region is the parameter can

1151
01:13:03,950 --> 01:13:07,370
vary so here showing ten by ten regions six hundred dimensional

1152
01:13:09,410 --> 01:13:11,250
each of these

1153
01:13:11,300 --> 01:13:13,330
interest points find the region

1154
01:13:13,430 --> 01:13:16,570
and this is done over the whole training set maybe face images

1155
01:13:16,600 --> 01:13:19,140
that's the data was gathered

1156
01:13:19,190 --> 01:13:20,360
and then

1157
01:13:20,600 --> 01:13:24,590
the status cluster to some similarity measure between

1158
01:13:25,000 --> 01:13:27,350
the image regions

1159
01:13:27,410 --> 01:13:30,350
the together i'm sure you've heard lot about clustering yesterday so

1160
01:13:30,360 --> 01:13:32,580
you've got the idea has cluster

1161
01:13:32,590 --> 01:13:33,840
this type of data

1162
01:13:33,850 --> 01:13:37,810
and perhaps will come out is the cluster for the i here

1163
01:13:37,830 --> 01:13:39,760
and across for the mouth here

1164
01:13:40,600 --> 01:13:45,630
and because these parts of these regions occur across all the training data the foreground

1165
01:13:45,630 --> 01:13:50,070
regions that these big clusters the hope will be the bigger clusters

1166
01:13:50,080 --> 01:13:52,460
are the ones that you want

1167
01:13:52,580 --> 01:13:56,690
so each of these large clusters defined apart

1168
01:13:56,740 --> 01:13:58,150
and then we go back to the

1169
01:13:58,190 --> 01:14:01,980
here's an example by the way the sum of the parts that are found

1170
01:14:02,030 --> 01:14:03,620
now we go back to the image

1171
01:14:05,210 --> 01:14:06,830
find interest points

1172
01:14:06,850 --> 01:14:08,660
and then each of the regions

1173
01:14:12,210 --> 01:14:14,390
these interest points we we can see if we want to

1174
01:14:14,410 --> 01:14:17,650
decide if there's apart there are not that we know about so say want to

1175
01:14:17,650 --> 01:14:20,850
find if this file this mouth in this image

1176
01:14:20,870 --> 01:14:21,630
we compare

1177
01:14:22,330 --> 01:14:26,090
image region to each of the regions around this in it interest points and we

1178
01:14:26,100 --> 01:14:29,060
find that they fit quite well in these two positions

1179
01:14:29,070 --> 01:14:30,120
OK and

1180
01:14:30,380 --> 01:14:33,090
it can be that we find the best fit all we find a set of

1181
01:14:33,100 --> 01:14:35,900
parts of it will be without within defined

1182
01:14:35,910 --> 01:14:38,030
the parts

1183
01:14:39,070 --> 01:14:40,210
for this

1184
01:14:40,220 --> 01:14:42,390
in particular image

1185
01:14:43,730 --> 01:14:48,200
that's the schematic for all of the methods in the first part of the talk

1186
01:14:48,220 --> 01:14:52,620
this clustering step of this gathering from text from various training images

1187
01:14:53,120 --> 01:14:55,140
and that defines

1188
01:14:55,150 --> 01:14:56,300
and the parts

1189
01:14:56,320 --> 01:15:01,670
not going to particular method run through that's the general introduction to the particular talk

1190
01:15:01,670 --> 01:15:03,950
about is one given by lee

1191
01:15:03,970 --> 01:15:04,990
and she'll

1192
01:15:05,030 --> 01:15:07,230
over the last couple of years

1193
01:15:07,280 --> 01:15:09,630
and this is exactly takes this

1194
01:15:09,650 --> 01:15:13,640
approach is because we have a set of training data for cars

1195
01:15:13,660 --> 01:15:17,230
find interest points find parts

1196
01:15:17,810 --> 01:15:22,510
so in detail what they use is model cars actually

1197
01:15:23,490 --> 01:15:25,860
clustering produces

1198
01:15:25,880 --> 01:15:29,980
so people across just the interest points produce these candidate regions are going to be

1199
01:15:31,270 --> 01:15:33,310
let me just say

1200
01:15:33,920 --> 01:15:35,020
from my point of view

1201
01:15:35,030 --> 01:15:36,210
these images

1202
01:15:36,470 --> 01:15:39,340
training from our good the segmented images even though

1203
01:15:39,930 --> 01:15:44,800
they haven't explicitly segmented because these images and you can see the background is so

1204
01:15:44,800 --> 01:15:48,420
simple effectively they know what the foreground object is

1205
01:15:48,550 --> 01:15:51,460
and so it's making a little bit easier

1206
01:15:51,480 --> 01:15:53,220
these are clustered

1207
01:15:53,240 --> 01:15:55,070
and what you're seeing here

1208
01:15:55,730 --> 01:15:59,820
the various clusters that they found find so here

1209
01:16:00,990 --> 01:16:02,600
the the will of the car

1210
01:16:02,620 --> 01:16:04,270
most represented

1211
01:16:04,280 --> 01:16:06,510
and then maybe this is

1212
01:16:06,560 --> 01:16:10,670
part of the car sector you get salient parts the car coming out by the

1213
01:16:13,000 --> 01:16:17,760
clustering is carried out using this measure of similarity

1214
01:16:17,770 --> 01:16:20,090
which is the normalized cross correlation

1215
01:16:20,650 --> 01:16:23,230
and then term for the size of the part

1216
01:16:23,290 --> 01:16:25,390
because that's a measure of similarity

1217
01:16:25,400 --> 01:16:27,850
for clustering and also for determining

1218
01:16:27,920 --> 01:16:30,590
the similarity between this part

1219
01:16:30,610 --> 01:16:33,300
and the image

1220
01:16:34,930 --> 01:16:37,540
so now as before i can ask you questions

1221
01:16:37,550 --> 01:16:39,500
so why is it you think they use

1222
01:16:39,530 --> 01:16:41,770
normalized cross correlation

1223
01:16:41,790 --> 01:16:50,490
why do use that rather than some other measure difference

1224
01:16:54,890 --> 01:17:02,670
yes in fact actually in these methods they do allow the two pixel adjustments

1225
01:17:02,720 --> 01:17:09,760
but that's that's that's translational shift is another type of shift they want to be

1226
01:17:09,780 --> 01:17:12,700
invariant something else please

1227
01:17:16,860 --> 01:17:21,730
thank you very much like yes so that's the answer by using normalized cross correlation

1228
01:17:21,730 --> 01:17:23,400
if there's a fine transformation

1229
01:17:23,790 --> 01:17:27,350
of the intensities of in that region they are invariant to it

1230
01:17:27,360 --> 01:17:30,640
OK so it's the reason you'll see all the way through you'll see that

1231
01:17:30,650 --> 01:17:36,970
in this first section people use the researchers used this normalized cross correlation very good

1232
01:17:37,140 --> 01:17:41,360
so here we are with we've got all parts

1233
01:17:41,960 --> 01:17:45,050
it would take an image and you want to see if that's got current or

1234
01:17:45,050 --> 01:17:46,520
not that's the question

1235
01:17:46,580 --> 01:17:48,300
find interest points

1236
01:17:48,530 --> 01:17:51,270
fifty candidate parts and now

1237
01:17:51,310 --> 01:17:54,260
this is where we bring in the idea of structure so far we had no

1238
01:17:54,350 --> 01:17:56,730
the notion of structure

1239
01:17:57,410 --> 01:17:58,600
the way they

1240
01:17:58,600 --> 01:18:02,600
in this case the largest lower bound is the maximum absolute value of the entries

1241
01:18:02,600 --> 01:18:03,980
of p

1242
01:18:04,060 --> 01:18:09,960
and if you minimize trying to over x y something equivalent to this

1243
01:18:09,980 --> 01:18:17,370
so that's a few examples for linear programming

1244
01:18:17,560 --> 01:18:19,180
so the next

1245
01:18:19,210 --> 01:18:22,100
extension and this quadratic programming and that's

1246
01:18:22,120 --> 01:18:23,330
something else

1247
01:18:23,370 --> 01:18:27,930
back to the fifties so here we keep the same types of constraints linear inequalities

1248
01:18:29,020 --> 01:18:34,850
and we take in convex quadratic cost function of x

1249
01:18:34,910 --> 01:18:40,750
so p is a symmetric matrix and this was the surface with the of this

1250
01:18:40,770 --> 01:18:42,460
quadratic function

1251
01:18:42,460 --> 01:18:46,080
so the consensus is still polyhedral as in the case

1252
01:18:46,100 --> 01:18:51,020
but the level curves of if have changed their now

1253
01:18:51,040 --> 01:18:54,830
if is a quadratic function so these are ellipses

1254
01:18:54,850 --> 01:18:58,990
and then depending on where the midpoint in the centre of these ellipses or the

1255
01:18:59,250 --> 01:19:01,160
unconstrained minimum of the

1256
01:19:01,180 --> 01:19:03,010
objective function is the

1257
01:19:03,600 --> 01:19:06,730
find an optimum that can be on the boundary

1258
01:19:06,750 --> 01:19:09,310
i can go on the corner points or it can be in the interior of

1259
01:19:09,310 --> 01:19:12,000
the fort hood and that's the constraints

1260
01:19:12,040 --> 01:19:16,040
so that's the difference with linear programming

1261
01:19:16,060 --> 01:19:19,660
so very

1262
01:19:19,680 --> 01:19:21,290
example that actually

1263
01:19:21,310 --> 01:19:27,500
motivated the development of quadratic programming in the fifties is a linear programme it's today

1264
01:19:27,500 --> 01:19:32,750
what people call linear programming with uncertainty robust linear programs

1265
01:19:32,750 --> 01:19:36,890
so this this comes up in investments

1266
01:19:36,910 --> 01:19:41,730
science so this is the basic markovits support for your problem

1267
01:19:41,730 --> 01:19:44,790
so because we have an LP

1268
01:19:44,810 --> 01:19:49,930
we assume that data and the constraints are exactly known so g and h are

1269
01:19:49,930 --> 01:19:55,750
exactly given but we assume that c is uncertain objective is

1270
01:19:55,810 --> 01:19:59,390
so animal it as a random variable

1271
01:19:59,410 --> 01:20:06,020
then obviously that makes it i suppose x objective function and random variable

1272
01:20:06,040 --> 01:20:10,370
and then we have to define what we mean by minimizing random variable x

1273
01:20:10,370 --> 01:20:12,370
random variable

1274
01:20:12,390 --> 01:20:14,210
that depends on x

1275
01:20:14,270 --> 01:20:19,210
well if c has mean sea by covariance sigma

1276
01:20:19,250 --> 01:20:24,940
then you have the trail between the expected value of the test was x just

1277
01:20:25,160 --> 01:20:26,790
see part i suppose x

1278
01:20:26,810 --> 01:20:28,080
and the variance

1279
01:20:28,100 --> 01:20:29,710
accessible semantics

1280
01:20:29,730 --> 01:20:32,160
so an investment that would be the expected

1281
01:20:33,140 --> 01:20:35,790
the minimizer we would minimize expected loss

1282
01:20:35,810 --> 01:20:36,960
so the

1283
01:20:37,290 --> 01:20:39,370
the mean will be the expected loss

1284
01:20:39,390 --> 01:20:43,640
and the variance would be the risk on the portfolio

1285
01:20:43,660 --> 01:20:47,850
and there is a trail between these two by just minimising

1286
01:20:47,870 --> 01:20:53,100
the expected value of the transpose x we might have a solution to the high

1287
01:20:54,290 --> 01:20:57,710
that's very sensitive to changes in sea

1288
01:20:57,730 --> 01:21:01,060
so i to model this is to look at the linear or convex combination of

1289
01:21:01,080 --> 01:21:03,640
it it's a combination of the two objectives

1290
01:21:03,700 --> 01:21:05,430
so suppose we minimize the

1291
01:21:05,440 --> 01:21:09,430
a combination of a weighted sum of the expected

1292
01:21:09,500 --> 01:21:14,080
after value expected objective function and a variance

1293
01:21:14,100 --> 01:21:15,750
accessible semantics

1294
01:21:15,750 --> 01:21:21,460
it's amazing parameter that expresses our risk aversion to commas zero

1295
01:21:21,500 --> 01:21:25,850
then we're don't care about the risk of the variance in the solution

1296
01:21:25,980 --> 01:21:28,850
if we are very are higher

1297
01:21:28,910 --> 01:21:32,620
risk averse to which is the larger value of come on put higher penalty on

1298
01:21:32,640 --> 01:21:34,560
high variance

1299
01:21:34,560 --> 01:21:38,680
but for positive any positive value of x this is a convex quadratic function of

1300
01:21:39,350 --> 01:21:42,330
so this is a quadratic programming problem

1301
01:21:42,350 --> 01:21:47,270
it's one of the first applications of quadratic programming

1302
01:21:47,350 --> 01:21:50,810
in the fifties

1303
01:21:50,910 --> 01:21:56,160
another famous one this just to continue this linear discrimination example from

1304
01:21:56,480 --> 01:21:58,330
linear programming

1305
01:21:58,350 --> 01:22:02,430
so here is the solution if you have two sets of points that are separable

1306
01:22:02,430 --> 01:22:04,310
by these two hyperplanes

1307
01:22:04,350 --> 01:22:10,480
then we can also try to find that in general there are multiple supporting separating

1308
01:22:10,480 --> 01:22:16,270
hyperplanes can try to find the one that maximizes the distance between these two sets

1309
01:22:16,270 --> 01:22:18,490
questions can

1310
01:22:18,500 --> 01:22:24,730
the first person to disperse the

1311
01:22:24,830 --> 01:22:29,140
this is rules something that it's also

1312
01:22:29,160 --> 01:22:35,600
from what i can see that you

1313
01:22:35,620 --> 01:22:39,270
why did factor graph you don't you can do everything by with the big page

1314
01:22:39,350 --> 01:22:43,910
mathematics right so the advantage of the graphs i think the ones i described earlier

1315
01:22:44,680 --> 01:22:49,520
well defined graph generalises in designing the model instruction model and helping us to set

1316
01:22:49,520 --> 01:22:51,790
out the calculation so yes i mean you

1317
01:22:51,930 --> 01:22:52,830
you know

1318
01:22:52,830 --> 01:22:54,490
you could just

1319
01:22:54,520 --> 01:22:56,700
manipulate the mathematics

1320
01:22:56,700 --> 01:22:58,350
but by

1321
01:22:59,700 --> 01:23:04,430
expressing is graph and expresses local message passing it just makes a heck of a

1322
01:23:04,430 --> 01:23:10,080
lot easier to understand the calculations to derive them to set might code in principle

1323
01:23:10,080 --> 01:23:13,680
you don't even need to draw the diagram

1324
01:23:13,720 --> 01:23:16,910
there's a wonderful story i think i can tell this because the very very smart

1325
01:23:16,910 --> 01:23:20,450
guy called steffen lauritzen who wrote a very famous book called graphical models it's quite

1326
01:23:20,450 --> 01:23:24,450
a theoretical book and some of the local jewish go also works in this field

1327
01:23:24,700 --> 01:23:31,330
was sent a draft of this this book from stephanie draft the this book on

1328
01:23:31,330 --> 01:23:36,450
graphical models and so he read through the draft back to stephan steffens by theoretical

1329
01:23:36,450 --> 01:23:40,950
sort of and he said you know stuff and this is a great book on

1330
01:23:40,950 --> 01:23:44,290
graphical model is fantastic is really going to be a classic there's one thing you

1331
01:23:44,290 --> 01:23:47,600
could do that would improve it make it easier for the reader wishes to add

1332
01:23:47,600 --> 01:23:51,430
a few pictures

1333
01:23:51,500 --> 01:23:57,850
so you don't need the grasp but for the for for me and i think

1334
01:23:57,850 --> 01:24:00,970
for many people enormously beneficial

1335
01:24:01,060 --> 01:24:04,680
in this country

1336
01:24:09,540 --> 01:24:16,520
yes you can still include directional lighting factor graphs you can discuss this

1337
01:24:16,540 --> 01:24:19,000
as you can so

1338
01:24:19,020 --> 01:24:23,850
underlying of course if if underlying this was the director graph that directionality is still

1339
01:24:26,790 --> 01:24:28,270
nothing here

1340
01:24:28,270 --> 01:24:33,410
specifies anything about any underlying directionality those so this is this is quite general so

1341
01:24:36,200 --> 01:24:41,890
so i guess i guess what you've lost actually is the following if you have

1342
01:24:41,890 --> 01:24:46,040
two nodes the points of the school head-to-head nodes these units pointing this node

1343
01:24:46,080 --> 01:24:51,100
then you have the conditional independence property these guys are independent when you observe this

1344
01:24:51,100 --> 01:24:55,290
to become dependent explaining away and you get get you lose that we just expresses

1345
01:24:55,290 --> 01:24:58,680
the fact that it's on the factor graph you can't see that the property of

1346
01:24:58,700 --> 01:25:00,600
the original cryptochrome across

1347
01:25:03,850 --> 01:25:04,770
the other

1348
01:25:07,290 --> 01:25:12,640
OK so you have the property here is this was all exact

1349
01:25:13,390 --> 01:25:16,970
we've actually done here if inference which is not only linear in the size of

1350
01:25:16,970 --> 01:25:21,790
the graph that result in efficient but a couple of other but is also exact

1351
01:25:21,850 --> 01:25:25,290
and also if you think about it for a moment may just to go back

1352
01:25:25,290 --> 01:25:27,640
to the original graph

1353
01:25:28,140 --> 01:25:29,750
to compute this

1354
01:25:29,770 --> 01:25:33,520
marginal we sent messages all the way in from the outside

1355
01:25:33,580 --> 01:25:35,000
compute the marginal

1356
01:25:35,020 --> 01:25:38,390
if not he said well i want this marginal you do the same thing you

1357
01:25:38,390 --> 01:25:43,450
send messages in from the outside to that node but actually all of those messages

1358
01:25:43,450 --> 01:25:44,160
will be

1359
01:25:44,200 --> 01:25:48,310
messages are already computed and in fact if you

1360
01:25:48,330 --> 01:25:51,250
let's say you pick a node and you send the message out

1361
01:25:51,290 --> 01:25:52,330
call that the route

1362
01:25:52,330 --> 01:25:55,080
so the message at all these comments and the measures and leads back to the

1363
01:25:55,080 --> 01:25:57,200
roots of each link is how to have

1364
01:25:57,290 --> 01:26:00,640
every possible message been sent once in each direction across each link

1365
01:26:00,680 --> 01:26:05,560
then you have all the messages you need to compute all marginals on every variable

1366
01:26:05,580 --> 01:26:08,620
and if you want the marginal factor you just take the product of the incoming

1367
01:26:08,620 --> 01:26:10,040
messages to

1368
01:26:10,040 --> 01:26:14,100
so its exact and by just using twice as much computation you can get all

1369
01:26:14,100 --> 01:26:16,750
the modules in the graph

1370
01:26:17,790 --> 01:26:19,970
wonderful stuff

1371
01:26:20,000 --> 01:26:26,410
it does depend on the graph being treated that that discussion breaks down the road

1372
01:26:27,680 --> 01:26:30,600
and what to do if the graph is not a tree well you can just

1373
01:26:30,600 --> 01:26:33,250
apply the message passing anyway the called

1374
01:26:33,270 --> 01:26:35,120
loopy belief propagation

1375
01:26:35,200 --> 01:26:37,750
and it's rather ad hoc

1376
01:26:38,560 --> 01:26:41,120
but it turns out that it works extremely well

1377
01:26:41,140 --> 01:26:43,060
and it also turns out that

1378
01:26:43,080 --> 01:26:45,200
it's equivalent some other techniques

1379
01:26:45,370 --> 01:26:47,220
david be talking about this in your

1380
01:26:47,250 --> 01:26:49,930
they will tell you a lot more about why this is actually turns out to

1381
01:26:49,930 --> 01:26:51,580
be really really good thing to do

1382
01:26:51,580 --> 01:26:58,790
think can go wrong with the marginalizations might not be tractable there we just had

1383
01:26:58,850 --> 01:27:02,370
little some asians but if you have continuous variables we need to do integrations we

1384
01:27:02,370 --> 01:27:05,580
may not be able to do those in closed form so what can we do

1385
01:27:05,580 --> 01:27:08,850
if the marginalizations an attractor

1386
01:27:09,870 --> 01:27:15,100
in general that we've got some complicated distribution and we want to compute marginals and

1387
01:27:15,100 --> 01:27:19,640
to make predictions using the distribution is not tractable we do about it

1388
01:27:19,720 --> 01:27:25,950
one thing about it is monte carlo draw samples from the distribution and we have

1389
01:27:26,450 --> 01:27:27,750
in very can give two

1390
01:27:27,770 --> 01:27:31,430
o talked about monte carlo techniques today and tomorrow i think

1391
01:27:31,450 --> 01:27:34,330
and what colour settings so wonderful for

1392
01:27:34,350 --> 01:27:40,430
couple reasons first of all the extremely generally compliant very very wide wide-ranging distributions also

1393
01:27:40,430 --> 01:27:43,820
in some sense that they're exactly they would be exact if you had an infinitely

1394
01:27:43,820 --> 01:27:45,720
powerful computer

1395
01:27:48,520 --> 01:27:50,490
the main limitation i think is that

1396
01:27:50,500 --> 01:27:54,730
they can be computationally costly so scaling up to large data sets can can prove

1397
01:27:54,730 --> 01:27:56,220
to be problematic

1398
01:27:56,250 --> 01:27:58,540
one scholar methods have a lot of advantages

1399
01:27:58,540 --> 01:28:03,790
i think i'm quite excited about is of complementary set of techniques which are based

1400
01:28:04,370 --> 01:28:09,910
analytical approximations and make use of local message passing on graphs of illustrate that schematically

1401
01:28:09,910 --> 01:28:12,990
here in monte-carlo we draw samples

1402
01:28:13,000 --> 01:28:18,990
which is distributed according to the true posterior distribution in these other techniques we approximate

1403
01:28:19,000 --> 01:28:23,830
the the true distribution in red with some family of simple distributions shown schematically in

1404
01:28:23,830 --> 01:28:28,810
green and then we explore that family of distributions to find the member which is

1405
01:28:28,810 --> 01:28:32,990
the best representation of the true distribution in some sense we have to define what

1406
01:28:32,990 --> 01:28:35,390
we mean by the best or the closest match

1407
01:28:35,390 --> 01:28:40,620
there's a whole bunch of these variational message passing loopy belief propagation already mentioned expectation

1408
01:28:40,620 --> 01:28:44,430
propagation and many others very active field of research

1409
01:28:45,810 --> 01:28:50,330
a disadvantage here is that we can only ever find an approximation so if we

1410
01:28:50,330 --> 01:28:54,200
for approximating this complicated distribution by cassie and we can only ever find against in

1411
01:28:55,660 --> 01:28:59,120
on the other hand it can be very efficient and can scale to large datasets

1412
01:28:59,120 --> 01:29:02,750
are show you in a minute and tom minka is going to give two

1413
01:29:02,770 --> 01:29:05,600
o lectures on that topic

1414
01:29:05,620 --> 01:29:10,830
so give you an illustration

1415
01:29:10,850 --> 01:29:15,200
of this framework in action so this is the problem of

1416
01:29:16,770 --> 01:29:18,100
and we can think of this

1417
01:29:18,290 --> 01:29:22,040
and in terms of concrete example think of chess imagine that we're all members of

1418
01:29:22,040 --> 01:29:25,910
the chess club and we were going to play games of chess against each other

1419
01:29:25,970 --> 01:29:29,140
we get together in pairs we play a game of chess and one person wins

1420
01:29:29,140 --> 01:29:31,660
or the other person wins or to draw

1421
01:29:31,700 --> 01:29:37,780
and from all those little pairwise results we want to drop the league table not

1422
01:29:37,780 --> 01:29:41,410
to but everybody in the room into the big league example from from

1423
01:29:41,490 --> 01:29:43,620
the best and the worst playing chess

1424
01:29:43,620 --> 01:29:47,850
so it's a partial ranking problem we have lots of partial rankings one person is

1425
01:29:47,850 --> 01:29:49,200
nationality couple

1426
01:29:49,230 --> 01:29:51,760
that says that my name is elizabeth coppock

1427
01:29:52,240 --> 01:29:53,200
so that will

1428
01:29:55,170 --> 01:29:57,580
mapping between the string concept

1429
01:29:59,350 --> 01:30:02,250
most of the politicians

1430
01:30:03,760 --> 01:30:07,860
indirectly related words to concepts

1431
01:30:07,960 --> 01:30:10,560
extreme so you have

1432
01:30:10,660 --> 01:30:13,730
dog theword

1433
01:30:13,750 --> 01:30:16,100
which can have multiple forms

1434
01:30:16,120 --> 01:30:18,540
dog dogs

1435
01:30:18,550 --> 01:30:19,950
the singular and the plural

1436
01:30:23,080 --> 01:30:24,540
now a mass nouns

1437
01:30:27,060 --> 01:30:29,850
i would have a mass number assertion

1438
01:30:31,290 --> 01:30:32,530
what are the words

1439
01:30:33,150 --> 01:30:35,200
the string water

1440
01:30:35,260 --> 01:30:38,800
and then those words

1441
01:30:38,840 --> 01:30:40,330
are related to

1442
01:30:40,340 --> 01:30:43,820
concepts so the denotation of dog theword is

1443
01:30:43,850 --> 01:30:45,620
first dark

1444
01:30:45,650 --> 01:30:53,110
this gives you a level of abstraction that allows you to

1445
01:30:53,120 --> 01:30:55,160
now both dog and dogs

1446
01:30:55,170 --> 01:30:57,360
to the concept doc

1447
01:30:57,370 --> 01:30:58,540
or you know

1448
01:30:58,550 --> 01:31:00,820
sing and sang

1449
01:31:00,830 --> 01:31:02,860
to the concept of singing

1450
01:31:08,870 --> 01:31:13,240
you one concept in the word

1451
01:31:13,250 --> 01:31:14,950
it's a one word

1452
01:31:14,950 --> 01:31:16,460
saying the word

1453
01:31:16,510 --> 01:31:18,190
that's an individual in the KB

1454
01:31:18,200 --> 01:31:20,220
and it's related to

1455
01:31:26,320 --> 01:31:28,350
and and well

1456
01:31:28,410 --> 01:31:30,580
because there's really only

1457
01:31:30,670 --> 01:31:33,180
one semantic relationship involved

1458
01:31:33,240 --> 01:31:34,920
a single word

1459
01:31:34,940 --> 01:31:40,090
that it sort of represents the root which is the meaningful elements in

1460
01:31:40,150 --> 01:31:42,860
in each of these strings

1461
01:31:52,950 --> 01:31:54,420
because that was

1462
01:31:54,440 --> 01:31:56,220
the mean

1463
01:31:56,230 --> 01:31:58,970
that would capture the fact that there's really only one

1464
01:31:59,000 --> 01:32:03,000
semantic relation is the relation between the roots and

1465
01:32:03,060 --> 01:32:04,550
the concept

1466
01:32:04,580 --> 01:32:06,830
and then the room is related to these

1467
01:32:06,920 --> 01:32:10,030
constraints immediately

1468
01:32:10,130 --> 01:32:16,760
it means it

1469
01:32:16,870 --> 01:32:18,300
you could infer

1470
01:32:18,310 --> 01:32:22,420
a direct relationship between of the strings and the concept

1471
01:32:22,820 --> 01:32:29,460
but if it's there's only one relation involved it should only be stated once

1472
01:32:31,230 --> 01:32:37,160
OK so

1473
01:32:37,550 --> 01:32:44,380
all talk about four different kinds of semantic predicates by semantic predicates i mean

1474
01:32:44,440 --> 01:32:46,790
things that really words too

1475
01:32:48,130 --> 01:32:55,730
words are linguistically things to semantic things OK so i talked about namestring notation

1476
01:32:58,090 --> 01:33:03,290
in mainstream really from concept denotation of the word to concept

1477
01:33:03,370 --> 01:33:06,950
and then you also have some transfer ticket

1478
01:33:06,960 --> 01:33:10,340
like verbsemtrans which really

1479
01:33:11,710 --> 01:33:13,740
two semantic templates

1480
01:33:13,760 --> 01:33:19,650
which is a kind of abstract descriptions of events

1481
01:33:22,990 --> 01:33:25,930
the word will be related to the semantic template

1482
01:33:27,260 --> 01:33:29,210
eating events

1483
01:33:29,210 --> 01:33:31,120
where the subject

1484
01:33:31,140 --> 01:33:32,560
argument is

1485
01:33:32,620 --> 01:33:34,140
plays the role of the eater

1486
01:33:34,170 --> 01:33:34,850
and the

1487
01:33:34,890 --> 01:33:37,310
object argument place thing

1488
01:33:38,740 --> 01:33:40,170
and you

1489
01:33:40,180 --> 01:33:43,640
keywords in there for the subject and the object

1490
01:33:43,690 --> 01:33:45,710
keep the roles that they play

1491
01:33:45,730 --> 01:33:46,950
and then

1492
01:33:46,960 --> 01:33:50,370
for generations gentemplate

1493
01:33:50,370 --> 01:33:53,100
and then we also have asserttemplate

1494
01:33:53,160 --> 01:33:55,930
which is

1495
01:33:56,400 --> 01:34:05,780
i won't talk about asserttemplate one much like gentemplate it relates to sentence template concepts

1496
01:34:05,900 --> 01:34:08,700
sort of an abstract description of the sentence

1497
01:34:10,510 --> 01:34:11,510
you can

1498
01:34:11,600 --> 01:34:14,200
interpret that as a relation

1499
01:34:14,210 --> 01:34:17,230
with gentemplate it is given relation

1500
01:34:17,240 --> 01:34:18,170
how do you

1501
01:34:18,170 --> 01:34:20,050
realise that

1502
01:34:20,530 --> 01:34:24,000
sentence you could use to

1503
01:34:24,160 --> 01:34:26,000
realize a given predicate

1504
01:34:32,710 --> 01:34:34,300
it seems strange

1505
01:34:34,460 --> 01:34:35,640
already covered

1506
01:34:35,680 --> 01:34:38,450
this example shows that

1507
01:34:38,460 --> 01:34:40,590
the same constant can have

1508
01:34:40,670 --> 01:34:44,450
two different names know mark twain

1509
01:34:44,470 --> 01:34:46,780
senior women's

1510
01:34:47,140 --> 01:34:51,470
you could use

1511
01:34:51,470 --> 01:34:53,910
have question structure

1512
01:34:53,910 --> 01:34:59,220
so indeed that's when we're done defects is the output for every vertex is going

1513
01:34:59,220 --> 01:35:01,930
to give us the shortest path from s to that vertex

1514
01:35:01,950 --> 01:35:06,260
along the way it's going to be some estimated distance from a vertex an improvement

1515
01:35:06,270 --> 01:35:08,660
over time this is an entity

1516
01:35:08,660 --> 01:35:11,350
so initially we know that the distance

1517
01:35:11,380 --> 01:35:12,250
we now

1518
01:35:12,740 --> 01:35:16,940
the distance from s to s is zero arrested that to be our estimates going

1519
01:35:16,940 --> 01:35:20,910
to be accurate everything else are discussed set to infinity as we know may not

1520
01:35:20,910 --> 01:35:21,920
be connected

1521
01:35:21,930 --> 01:35:23,710
the beginning we don't know much

1522
01:35:23,730 --> 01:35:27,420
as initially is going to be infinity immediately we're going to have less to big

1523
01:35:27,950 --> 01:35:31,930
and then the interesting careers q

1524
01:35:31,940 --> 01:35:36,580
which is going to consist of initially all the vertices in the graph

1525
01:35:36,580 --> 01:35:40,840
and it's going to not just be q with the letter suggests is going to

1526
01:35:40,840 --> 01:35:48,200
be a priority queue so it's going to maintain and in particular the vertex the

1527
01:35:48,200 --> 01:35:50,820
has the smallest distance estimate

1528
01:35:50,840 --> 01:35:53,440
so this is the priority queue

1529
01:35:53,470 --> 01:35:58,920
this is really an abuse of notation for a data structure

1530
01:35:58,920 --> 01:36:06,480
OK so this can be he whatever the vertices are key

1531
01:36:06,500 --> 01:36:09,270
on d

1532
01:36:09,300 --> 01:36:10,670
distance estimate

1533
01:36:10,670 --> 01:36:14,670
so in particular as well have the this is going to be i mean keep

1534
01:36:14,690 --> 01:36:19,360
as will be the guy who is the minimum everyone else has the same key

1535
01:36:20,280 --> 01:36:24,680
and we're going to repeatedly extract the minimum element from this cube and do other

1536
01:36:27,350 --> 01:36:31,750
OK so that's this is initialisation

1537
01:36:32,430 --> 01:36:36,430
i'm going to call the initialisation because well

1538
01:36:36,450 --> 01:36:40,110
it's pretty simple thing it takes linear time nothing fancy going on

1539
01:36:40,110 --> 01:36:42,070
the heart of the other

1540
01:36:42,090 --> 01:36:45,170
it is all in

1541
01:36:45,180 --> 01:36:46,580
six science

1542
01:36:53,140 --> 01:36:57,080
so we're so this is not really a step that the means the first step

1543
01:36:57,080 --> 01:36:58,300
here that we need to do

1544
01:36:58,320 --> 01:37:02,170
it is we take the vertex whose distance estimate is minimal so that

1545
01:37:02,500 --> 01:37:06,910
among all the vertices not yet as currently as is that dq has everyone in

1546
01:37:06,910 --> 01:37:09,120
general you will have everyone except as

1547
01:37:09,130 --> 01:37:12,900
so we'll take the vertex u there has the minimum key

1548
01:37:12,920 --> 01:37:15,580
in the priority queue

1549
01:37:15,600 --> 01:37:17,180
six extract the man

1550
01:37:17,230 --> 01:37:20,210
from q

1551
01:37:30,600 --> 01:37:42,080
and little u two s

1552
01:37:42,780 --> 01:37:44,480
claim that that is now

1553
01:37:44,500 --> 01:37:47,420
and that's exactly what we're saying here that s

1554
01:37:47,430 --> 01:37:51,900
but as the vertex has the minimum distance estimate and now we need to update

1555
01:37:51,900 --> 01:37:56,340
the distances we're looking at each adjacent vertex

1556
01:37:56,390 --> 01:37:57,490
are you should be

1557
01:37:58,500 --> 01:38:01,650
adjacency list for you

1558
01:38:01,710 --> 01:38:08,130
we look at

1559
01:38:08,140 --> 01:38:11,550
few distances

1560
01:38:11,570 --> 01:38:29,340
so that's the algorithm

1561
01:38:29,350 --> 01:38:31,070
or less

1562
01:38:31,080 --> 01:38:34,970
this is the key

1563
01:38:35,040 --> 01:38:38,080
should define a little bit of what's going on here

1564
01:38:38,510 --> 01:38:43,230
we talked mainly about undirected graphs last time were thinking about directed graphs and the

1565
01:38:43,230 --> 01:38:46,980
adjacency list for you here this is going to mean give me all the vertices

1566
01:38:46,980 --> 01:38:48,220
for which there

1567
01:38:48,240 --> 01:38:50,190
and it's an edge from u to v

1568
01:38:50,200 --> 01:38:54,740
so this is the outgoing adjacency was not the incoming adjacency list

1569
01:38:54,760 --> 01:38:59,710
wonder if you actually was everything directed graphs here we're only gonna care about those

1570
01:38:59,710 --> 01:39:03,580
ones so for every edge u v what this is saying

1571
01:39:03,580 --> 01:39:06,920
we're going to compare the current estimate for v

1572
01:39:06,930 --> 01:39:08,100
and this

1573
01:39:08,110 --> 01:39:10,350
canada estimate which is

1574
01:39:10,370 --> 01:39:13,370
which intuitively means you go from s to u

1575
01:39:13,390 --> 01:39:14,520
that's the view

1576
01:39:14,550 --> 01:39:17,980
because we now know that's the right answer this this in fact

1577
01:39:18,000 --> 01:39:18,990
he calls

1578
01:39:19,000 --> 01:39:25,660
we hope something arguments correct there should be the shortest path

1579
01:39:25,670 --> 01:39:29,660
way from st you because we just attitude as whenever we had something as it

1580
01:39:29,660 --> 01:39:32,020
should have the right values

1581
01:39:32,070 --> 01:39:34,860
so this is we could say well you take the shortest path from s to

1582
01:39:34,860 --> 01:39:38,780
you and then you follow the edge from u to v that has weight wv

1583
01:39:38,780 --> 01:39:42,010
that's one possible path from

1584
01:39:42,040 --> 01:39:44,440
s two v

1585
01:39:44,460 --> 01:39:47,770
and that's the shorter path than the one we currently have in our estimate this

1586
01:39:47,770 --> 01:39:51,780
is smaller than that then we should update the estimate to be that some because

1587
01:39:51,780 --> 01:39:53,980
that's the better so

1588
01:39:54,000 --> 01:39:57,250
i added to our database of past so to speak

1589
01:39:57,270 --> 01:39:58,790
a very intuitive

1590
01:40:00,090 --> 01:40:02,430
clearly shouldn't do anything

1591
01:40:04,090 --> 01:40:06,020
in this should be has the

1592
01:40:06,050 --> 01:40:10,440
it makes sense will prove that in the moment that's the first part of correctness

1593
01:40:10,690 --> 01:40:13,710
never screws up and the tricky part is to show that it finds all the

1594
01:40:13,710 --> 01:40:15,030
powers we care about

1595
01:40:15,040 --> 01:40:17,970
this is called the relaxation steps

1596
01:40:18,160 --> 01:40:24,680
relaxation always

1597
01:40:24,690 --> 01:40:28,080
difficult technique to teach to MIT students

1598
01:40:28,190 --> 01:40:30,260
and come very naturally but

1599
01:40:30,290 --> 01:40:35,330
OK so very simple operation it it comes from

1600
01:40:35,330 --> 01:40:38,030
optimisation terminology programming

1601
01:40:38,050 --> 01:40:41,360
terminology so to speak and

1602
01:40:41,360 --> 01:40:45,320
can anyone tell me that this this inequality look familiar at all

1603
01:40:45,340 --> 01:40:48,540
what kind of especially when you start writing it this way

1604
01:40:48,590 --> 01:40:51,030
you say well the shortest path from s to v

1605
01:40:51,050 --> 01:40:52,700
on the shortest path from

1606
01:40:52,700 --> 01:40:56,240
st u and some edge from u to v

1607
01:40:56,260 --> 01:40:59,400
that look like anything we've seen in fact it was on this board but i

1608
01:40:59,400 --> 01:41:00,490
just arrest

1609
01:41:00,510 --> 01:41:05,860
triangle inequality so this is trying to make the triangle inequality true certainly the shortest

1610
01:41:05,860 --> 01:41:09,490
path from s to be should be less than or equal to not this greater

1611
01:41:09,490 --> 01:41:14,170
than the shortest path from s to u plus some you know whatever path

1612
01:41:14,170 --> 01:41:15,630
from u to v

1613
01:41:15,650 --> 01:41:18,530
this is the shortest path should be most

1614
01:41:18,550 --> 01:41:20,740
so this is sort of a somewhat more general

1615
01:41:20,800 --> 01:41:23,030
triangle inequality

1616
01:41:23,070 --> 01:41:26,360
and we want certainly it should be true

1617
01:41:26,380 --> 01:41:29,740
so it's not really we fix it it to have greater than we make it

1618
01:41:29,740 --> 01:41:32,820
equal but i want to make it less than because that's not always true

1619
01:41:32,820 --> 01:41:36,240
OK but certainly should be less than or equal to so this is fixing the

1620
01:41:36,240 --> 01:41:39,740
triangle inequality it's trying to make that constrain true

1621
01:41:39,900 --> 01:41:43,550
and optimisation that's called relax relaxing the constraint

1622
01:41:43,550 --> 01:41:47,700
so we're sort of relaxing the triangle inequality here over and over and over and

1623
01:41:47,700 --> 01:41:52,320
by the end which level all the shows past that's the clay very simple algorithm

1624
01:41:52,380 --> 01:41:53,970
let's try it on the graph

1625
01:41:53,990 --> 01:41:58,610
and that should make it more intuitive way working and the rest electoral proving that

1626
01:41:58,610 --> 01:42:00,130
it works

1627
01:42:08,150 --> 01:42:18,340
so let's take it should mention one other thing here sorry

1628
01:42:18,360 --> 01:42:20,840
that whenever we change

1629
01:42:20,860 --> 01:42:24,610
this is changing the key of b in the priority queue

1630
01:42:24,630 --> 01:42:28,220
so implicitly what's happening here in this assignment

1631
01:42:28,220 --> 01:42:29,590
it's getting a bit

1632
01:42:29,590 --> 01:42:32,630
is it decrease key operation

1633
01:42:32,700 --> 01:42:39,610
OK which we talk briefly about

1634
01:42:39,630 --> 01:42:43,470
the last class in the context of the minimum spanning trees were also decreasing the

1635
01:42:43,470 --> 01:42:47,190
key point is we're changing the key we're changing the key of one element in

1636
01:42:47,190 --> 01:42:51,880
this relaxation step in the priority queue so that you know now becomes the minimum

1637
01:42:51,880 --> 01:42:53,550
we should extracted here

1638
01:42:53,570 --> 01:42:59,740
and we're only ever decreasing he's because we're always replacing larger eyes with small amounts

1639
01:42:59,740 --> 01:43:03,830
this is the state of the art general method that involves a lot of hand

1640
01:43:03,830 --> 01:43:06,660
tuning and taking parameter

1641
01:43:06,680 --> 01:43:10,330
this is the result of phd thesis

1642
01:43:10,620 --> 01:43:12,310
a lot of work to do that

1643
01:43:13,830 --> 01:43:17,370
by the fact that we that a lot less time

1644
01:43:17,390 --> 01:43:19,160
this is kind of of

1645
01:43:19,510 --> 01:43:23,910
upper bound baseline this she did this is actually

1646
01:43:23,930 --> 01:43:25,620
if you know the structure

1647
01:43:25,680 --> 01:43:27,580
and do the alignment

1648
01:43:28,220 --> 01:43:30,450
this is the best that you

1649
01:43:30,510 --> 01:43:32,740
there is still some room for improvement

1650
01:43:32,780 --> 01:43:37,080
but actually one

1651
01:43:37,080 --> 01:43:38,870
all right

1652
01:43:38,870 --> 01:43:40,970
more recently the second two

1653
01:43:44,180 --> 01:43:46,700
going for bioinformatics knowledge information people

1654
01:43:46,700 --> 01:43:50,950
but it stays the same thing you just have to specify these three things

1655
01:43:50,970 --> 01:43:56,160
you have to specify what the representation with the loss function

1656
01:43:56,180 --> 01:43:59,530
so here's the problem in information retrieval

1657
01:43:59,580 --> 01:44:03,120
so you we have to present results

1658
01:44:03,120 --> 01:44:07,580
people in this room probably going to be happy

1659
01:44:07,640 --> 01:44:09,490
but the rest of the world is not

1660
01:44:11,010 --> 01:44:14,100
all the people who are actually looking for

1661
01:44:14,160 --> 01:44:18,830
stock quote on servicemaster company which has

1662
01:44:18,850 --> 01:44:20,800
stocktickersymbol st

1663
01:44:20,850 --> 01:44:22,930
i'm not going to be happy used

1664
01:44:22,970 --> 01:44:28,450
everybody is looking for the french magazine as the is not

1665
01:44:28,470 --> 01:44:34,330
all students of that tremendous on my dad knows what i find that not going

1666
01:44:40,470 --> 01:44:44,470
people in the information retrieval have recognised is

1667
01:44:44,550 --> 01:44:46,680
the kind of putting

1668
01:44:46,700 --> 01:44:52,550
on x into one basket just looking at one interpretation of the is not strategy

1669
01:44:52,550 --> 01:44:53,760
in particular

1670
01:44:53,850 --> 01:44:57,100
kind of user happiness is a submodular function

1671
01:44:57,660 --> 01:45:03,220
users are not going to be happy all you don't give them any relevant results

1672
01:45:04,050 --> 01:45:05,990
they actually almost

1673
01:45:06,030 --> 01:45:09,030
they're actually quite happy if you just give them one

1674
01:45:09,080 --> 01:45:14,080
so better ranking to present in this case would actually be something like the right

1675
01:45:15,050 --> 01:45:16,780
people with looking for

1676
01:45:16,810 --> 01:45:20,180
machine learning methods are going to be happy people looking for the dog

1677
01:45:20,410 --> 01:45:22,370
because overlapping

1678
01:45:22,470 --> 01:45:24,240
boca fans happy

1679
01:45:25,850 --> 01:45:28,450
but to be able to do this we have to

1680
01:45:28,470 --> 01:45:32,580
he able to model dependencies between

1681
01:45:33,640 --> 01:45:38,050
in particular we have to recognise the model that once you presented one machine learning

1682
01:45:39,010 --> 01:45:42,600
we don't want you want to build another results for

1683
01:45:42,700 --> 01:45:47,310
in this way become structured prediction problem we have to model based

1684
01:45:47,330 --> 01:45:49,410
so how are we going to do that

1685
01:45:49,490 --> 01:45:50,850
so formally

1686
01:45:51,240 --> 01:45:55,200
only the problem of given act

1687
01:45:55,260 --> 01:45:58,300
which is the set of results may be

1688
01:45:58,350 --> 01:46:00,970
the top one hundred results that are current search engine

1689
01:46:01,890 --> 01:46:03,580
and y is

1690
01:46:03,580 --> 01:46:08,600
the subset that we want to predict in the top ten

1691
01:46:08,620 --> 01:46:11,950
so the mapping from the set to itself

1692
01:46:12,030 --> 01:46:14,510
actually want to to ranking

1693
01:46:17,010 --> 01:46:20,510
so how are we going to represent the problem well going to follow the idea

1694
01:46:21,600 --> 01:46:24,490
because microsoft called essential pages

1695
01:46:24,530 --> 01:46:26,850
we say OK topic diversity

1696
01:46:26,850 --> 01:46:29,700
is the same as were

1697
01:46:29,850 --> 01:46:32,830
if you think about this diagram here

1698
01:46:32,890 --> 01:46:37,490
these are all the words that are occurring in the one of the with current

1699
01:46:37,550 --> 01:46:39,120
these are the documents

1700
01:46:39,300 --> 01:46:41,740
let's say the one hundred documents

1701
01:46:41,870 --> 01:46:42,950
this result

1702
01:46:43,740 --> 01:46:46,700
the way that i want to pick the subset is

1703
01:46:47,620 --> 01:46:49,720
to maximize the

1704
01:46:49,760 --> 01:46:53,160
total number of work that have myself

1705
01:46:53,220 --> 01:46:56,350
i want to take a set of documents that kind of covers almost all of

1706
01:46:56,370 --> 01:46:57,580
the words

1707
01:46:57,580 --> 01:47:01,260
that occur in poor results

1708
01:47:02,120 --> 01:47:07,180
this is the max coverage problem is NP hard but they're good approximation for example

1709
01:47:07,180 --> 01:47:10,140
the simple greedy approximation it's this one

1710
01:47:11,050 --> 01:47:16,830
one wraps along one over e approximation because someone

1711
01:47:18,490 --> 01:47:20,950
dv over the last several

1712
01:47:21,010 --> 01:47:24,030
works probably is expected to start

1713
01:47:25,800 --> 01:47:30,530
it is the largest area covered most work

1714
01:47:31,330 --> 01:47:33,950
you take the document that

1715
01:47:33,950 --> 01:47:37,430
if you subtract the words that already covered covered most of the work that would

1716
01:47:37,430 --> 01:47:40,100
be too here

1717
01:47:41,390 --> 01:47:44,200
you know which of the next document covers the

1718
01:47:44,220 --> 01:47:46,990
kind of gives the most marginal benefits

1719
01:47:47,030 --> 01:47:48,830
before is actually bigger but

1720
01:47:48,850 --> 01:47:53,100
most of the work already covered e three

1721
01:47:53,140 --> 01:47:57,370
and then before and this way you cover all of almost all of the work

1722
01:47:57,410 --> 01:47:58,430
that would be

1723
01:48:02,580 --> 01:48:09,620
but as you sound like to put it in a corner of the world

1724
01:48:09,620 --> 01:48:13,120
is not all words are equally important

1725
01:48:13,640 --> 01:48:17,430
what we actually want to learn in this problem here is

1726
01:48:17,450 --> 01:48:20,470
what are the right way

1727
01:48:20,580 --> 01:48:23,760
get to each individual words so important words

1728
01:48:23,810 --> 01:48:29,180
the important covers get highway unimportant what should go away

1729
01:48:29,200 --> 01:48:35,280
and again with the model that has a linear function

1730
01:48:35,300 --> 01:48:37,740
and we can put in features like

1731
01:48:38,290 --> 01:48:41,470
you know

1732
01:48:41,490 --> 01:48:44,780
of mister woodcock occur in the results that

1733
01:48:44,800 --> 01:48:48,490
how often does occur at least in the title of the results that we can

1734
01:48:48,490 --> 01:48:52,580
come up with a whole set of features again because into the feature vectors

1735
01:48:54,680 --> 01:48:59,120
let me skip the details actually use songs targets ICML

1736
01:48:59,180 --> 01:49:01,070
two tuesday

1737
01:49:01,640 --> 01:49:03,780
i to that

1738
01:49:03,780 --> 01:49:04,970
so we've picked

1739
01:49:04,970 --> 01:49:07,030
now representation with

1740
01:49:07,030 --> 01:49:10,890
come up toward the argmax just we

1741
01:49:11,060 --> 01:49:14,850
is the loss function that they're going to use

1742
01:49:14,850 --> 01:49:15,970
we want

1743
01:49:17,870 --> 01:49:20,910
the kind of popular topic we have

1744
01:49:20,950 --> 01:49:27,600
i was also one two three four five subtopics and subtopic because the documents in

1745
01:49:27,600 --> 01:49:28,390
this one

1746
01:49:30,030 --> 01:49:33,780
and our loss function is following what we want to make sure we cover the

1747
01:49:33,780 --> 01:49:35,220
popular topics

1748
01:49:35,240 --> 01:49:37,160
the loss is just going to be

1749
01:49:37,470 --> 01:49:41,870
it's a forty one d ten

1750
01:49:42,330 --> 01:49:45,260
he won covers this subtopic t ten

1751
01:49:45,280 --> 01:49:46,800
however this subtopic

1752
01:49:46,800 --> 01:49:49,510
uncovered these we hear less

1753
01:49:49,570 --> 01:49:51,470
all are of you well

1754
01:49:51,510 --> 01:49:53,070
that's very low

1755
01:49:54,700 --> 01:49:55,390
if we

1756
01:49:55,410 --> 01:49:57,530
predicted d two and seven

1757
01:49:57,530 --> 01:49:58,830
these two here

1758
01:49:58,890 --> 01:50:06,990
then we will cover only these two documents you all of which i know

1759
01:50:07,050 --> 01:50:10,830
and again the separation oracle that we need to training and actually is

1760
01:50:10,850 --> 01:50:15,620
again the same structure to the next problem so we can use the same

1761
01:50:18,550 --> 01:50:21,740
so we just plug this into the general cutting plane learning algorithm

1762
01:50:22,760 --> 01:50:25,100
and here are the results

1763
01:50:25,140 --> 01:50:26,280
this is for

1764
01:50:26,300 --> 01:50:29,030
it's kind of hard to get there for this because

1765
01:50:29,080 --> 01:50:30,530
the learning problem that people have

1766
01:50:30,660 --> 01:50:35,390
look but there is data from the track interactive track

1767
01:50:35,810 --> 01:50:41,030
we're documents actually the in labeled subtopics

1768
01:50:41,070 --> 01:50:45,510
and so

1769
01:50:45,510 --> 01:50:51,390
you can think of the machine

1770
01:50:51,410 --> 01:50:51,940
and the

1771
01:50:59,760 --> 01:51:05,000
so you

1772
01:51:55,400 --> 01:52:01,880
you think

1773
01:52:02,090 --> 01:52:05,890
of course want

1774
01:52:19,100 --> 01:52:22,810
we have one or o

1775
01:52:22,850 --> 01:52:25,160
well of

1776
01:52:25,190 --> 01:52:35,130
so for me

1777
01:52:43,660 --> 01:52:45,910
you know who

1778
01:53:03,140 --> 01:53:06,870
i don't know

1779
01:53:07,020 --> 01:53:09,900
what we want

1780
01:53:20,220 --> 01:53:23,050
this is

1781
01:53:29,040 --> 01:53:30,860
she was

1782
01:53:35,480 --> 01:53:41,410
he was

1783
01:53:48,210 --> 01:53:51,210
so what

1784
01:54:00,300 --> 01:54:03,280
that is

1785
01:54:22,040 --> 01:54:29,840
four forty

1786
01:54:45,070 --> 01:54:53,280
well what i like about you when you

1787
01:54:53,290 --> 01:54:58,430
but i would like to

1788
01:55:19,410 --> 01:55:25,900
o or one that

1789
01:55:51,910 --> 01:55:54,060
all right

1790
01:55:54,060 --> 01:55:55,730
and the distinguished by

1791
01:55:55,900 --> 01:56:00,470
a formal semantics OK typically model theoretic of us a bit about what i mean

1792
01:56:00,470 --> 01:56:01,780
by that later

1793
01:56:01,860 --> 01:56:03,620
there fragments of first-order

1794
01:56:03,640 --> 01:56:07,560
and the closely related to propositional modal in dynamic logic

1795
01:56:07,730 --> 01:56:11,310
and one of the kind of key things about is that in general we have

1796
01:56:11,320 --> 01:56:14,760
the provision of inference services OK so we have some kind of sound and complete

1797
01:56:14,760 --> 01:56:16,920
decision procedures for key

1798
01:56:16,930 --> 01:56:23,740
inference problems and there are no a number of implemented systems of these things

1799
01:56:23,770 --> 01:56:25,310
so there's been

1800
01:56:25,330 --> 01:56:27,430
kind of a body of research

1801
01:56:27,440 --> 01:56:29,230
in this area has led to the

1802
01:56:29,250 --> 01:56:35,190
optimisation the reasoning procedures and implementations of system

1803
01:56:36,010 --> 01:56:38,630
essentially in DL

1804
01:56:38,640 --> 01:56:42,660
we have we apply this model theoretic semantics because we have the notion of interpretations

1805
01:56:42,660 --> 01:56:45,670
which are kind of mappings we have a domain of discourse the things that we're

1806
01:56:45,670 --> 01:56:47,020
going to be talking about

1807
01:56:47,120 --> 01:56:50,540
and then we map classes to sets of objects in this domain and we map

1808
01:56:50,540 --> 01:56:54,830
properties to sets set of pairs of objects individuals not to individuals

1809
01:56:54,880 --> 01:56:58,400
and then there are a bunch of rules that describe how we interpret constructs in

1810
01:56:59,720 --> 01:57:04,440
OK and they tell us when an interpretation is kind of respecting the statements that

1811
01:57:04,440 --> 01:57:06,230
we make with ontology

1812
01:57:06,290 --> 01:57:08,770
OK most will see a little bit more about that later

1813
01:57:08,790 --> 01:57:10,420
but the key thing to

1814
01:57:10,650 --> 01:57:14,400
remember really i think this is in some ways this is perhaps the the one

1815
01:57:14,400 --> 01:57:18,450
thing that i want to take home when you're thinking about about how is that

1816
01:57:18,470 --> 01:57:20,160
class description

1817
01:57:20,210 --> 01:57:25,000
OK so if i have some class description is essentially a characterisation of the individuals

1818
01:57:25,000 --> 01:57:29,370
that are members of that class was telling me about the things that are members

1819
01:57:29,370 --> 01:57:32,680
of that class

1820
01:57:32,690 --> 01:57:37,310
so you made you may see talk of various different species of owl

1821
01:57:37,330 --> 01:57:40,320
OK so there this notion of what we call layering and how

1822
01:57:40,420 --> 01:57:43,760
so there are three species in the in the al

1823
01:57:44,660 --> 01:57:49,010
recommendations and these are OWL full OWL OWL DL and OWL lite

1824
01:57:49,070 --> 01:57:53,280
OK we have what we call syntactic layering to say effectively they use the same

1825
01:57:53,280 --> 01:57:58,970
vocabulary OK so every OWL lite ontology is is lbl ontology is an alpha ontology

1826
01:57:58,980 --> 01:58:02,400
and there's also a notion of semantic layering which is the

1827
01:58:02,410 --> 01:58:04,560
but the semantic should agree

1828
01:58:04,580 --> 01:58:09,180
within the if i have a light ontology the inferences that i can draw upon

1829
01:58:09,190 --> 01:58:13,670
the OWL lite semantics should correspond with those that i'm using OWL DL semantics and

1830
01:58:13,670 --> 01:58:18,930
using l full semantics cases the the layering is kind of nicely behaved as it

1831
01:58:21,450 --> 01:58:23,030
so in know full

1832
01:58:23,050 --> 01:58:26,860
essentially what we have in full is

1833
01:58:27,530 --> 01:58:33,010
rdf with some additional vocabulary case we have RDF plus some extra vocabulary that likely

1834
01:58:33,010 --> 01:58:34,070
to talk about

1835
01:58:34,150 --> 01:58:40,860
my classes and properties and i've made cannot specify some kind of additional constraints markov

1836
01:58:40,970 --> 01:58:44,090
so there is no restriction on the way in which we use the vocabulary he

1837
01:58:44,090 --> 01:58:47,860
essentially kind of anything you can do in RDF you can do an awful

1838
01:58:48,740 --> 01:58:53,890
and in order to cope with this an RDF style model theory OK which which

1839
01:58:53,890 --> 01:58:58,720
allows you to kind of interpret southall ontologies are going to concentrate a bit more

1840
01:58:58,740 --> 01:59:03,030
there on the fragment we all the species known as DL

1841
01:59:03,590 --> 01:59:06,720
now within DL what we have here now is we have

1842
01:59:06,720 --> 01:59:11,390
some restrictions on the way in which the vocabulary can be used and this is

1843
01:59:11,390 --> 01:59:16,570
this is primarily to ensure that this then corresponds to the the language is that

1844
01:59:16,980 --> 01:59:23,130
these description logics such as first-order logic OK so for example we can do things

1845
01:59:23,130 --> 01:59:27,680
like restrictions on the way in which we can the kind of statement so we

1846
01:59:27,680 --> 01:59:32,270
can make it so we can alter the underlying ontology vocabulary itself

1847
01:59:32,290 --> 01:59:35,890
OK so i can't say things about the built-in vocabulary

1848
01:59:35,920 --> 01:59:40,120
so that's kind of so there is a notion that one needs to separate out

1849
01:59:40,330 --> 01:59:44,730
the the resources we talk about the language and resources which talk about the content

1850
01:59:44,730 --> 01:59:49,460
of my ontology can we can we can make statements about the language with LDL

1851
01:59:49,550 --> 01:59:53,640
there are restrictions on the kind of metamodelling that we can do

1852
01:59:53,920 --> 01:59:58,770
and the kind of characterisation of what constitutes ontology is defined by

1853
01:59:58,830 --> 02:00:02,980
by an abstract syntax which then allows that allows me to get back into RDF

1854
02:00:02,980 --> 02:00:06,910
graphs so i can characterize the the kinds of graphs that sit within this lbl

1855
02:00:09,340 --> 02:00:12,710
there's is a standard DL first-order model theory provided

1856
02:00:12,750 --> 02:00:16,960
and the and the claim benefit then is that this means that apply well defined

1857
02:00:16,960 --> 02:00:21,430
semantics we can understand the formal properties of the language and we can apply some

1858
02:00:21,430 --> 02:00:25,030
kind of known reasoning algorithms and use some implemented systems here

1859
02:00:25,050 --> 02:00:30,550
there's also a species called OWL lite which is kind of similar to DLP with

1860
02:00:30,550 --> 02:00:34,830
fewer constructs so some of the things we can talk about explicit negation or union

1861
02:00:34,830 --> 02:00:35,720
and the reverse space

1862
02:00:37,730 --> 02:00:38,320
and before

1863
02:00:40,160 --> 02:00:41,930
i did the shuffling into the card at

1864
02:00:43,250 --> 02:00:45,430
the probability of the front face being

1865
02:00:47,300 --> 02:00:50,960
white or black and reverse space being white or black

1866
02:00:51,460 --> 02:00:53,720
the probability of this hair

1867
02:00:56,230 --> 02:00:57,820
being white and white with one

1868
02:00:58,880 --> 02:00:59,740
the probability that being

1869
02:01:00,150 --> 02:01:01,550
brown brown was one third

1870
02:01:02,460 --> 02:01:03,230
the probability of

1871
02:01:04,680 --> 02:01:06,850
other two things the third because you don't like

1872
02:01:07,650 --> 02:01:10,510
the carbon black why can't just coming along

1873
02:01:10,960 --> 02:01:11,380
and this

1874
02:01:11,810 --> 02:01:12,510
fifty fifty

1875
02:01:12,960 --> 02:01:13,590
which was held

1876
02:01:14,860 --> 02:01:15,930
so the state there

1877
02:01:16,390 --> 02:01:17,110
and i think there

1878
02:01:17,890 --> 02:01:20,670
that's is the joint probability of everything

1879
02:01:21,670 --> 02:01:24,420
and now with this strong probability we can

1880
02:01:29,550 --> 02:01:31,590
data and anything else you want to throw in

1881
02:01:35,090 --> 02:01:38,380
and data is what given what given is more in this world

1882
02:01:39,030 --> 02:01:40,650
today the white came up

1883
02:01:41,300 --> 02:01:43,480
so we condition on the front being white

1884
02:01:47,280 --> 02:01:49,320
and we can then look at the probability

1885
02:01:49,920 --> 02:01:50,960
be reverse

1886
02:01:55,710 --> 02:01:59,340
white for example by on ising this probability distribution

1887
02:01:59,940 --> 02:02:00,650
and we get to this

1888
02:02:04,980 --> 02:02:09,530
this may not convince people who still really think fifty fifty fifty fifty how can

1889
02:02:09,530 --> 02:02:11,590
you say it's two-thirds that's rubbish

1890
02:02:12,230 --> 02:02:13,840
and so i've got one final argument

1891
02:02:14,750 --> 02:02:15,780
that i hope will help

1892
02:02:17,960 --> 02:02:22,650
and then once this argument has been accepted maybe will compel you to agree that

1893
02:02:22,650 --> 02:02:24,510
it's a good idea is right and probably everything

1894
02:02:26,630 --> 02:02:30,320
so here's the final argument and it's not that the reverse was indeed white

1895
02:02:30,730 --> 02:02:32,650
that's happened to be true but it didn't have to be

1896
02:02:33,760 --> 02:02:34,630
the argument goes like this

1897
02:02:35,210 --> 02:02:38,030
imagine we play this game lots of times and instead asking

1898
02:02:38,480 --> 02:02:42,820
what's the probability of the faces what i i always asking the question what's the

1899
02:02:42,820 --> 02:02:45,960
probability that the face is the same as the one you're seeing now

1900
02:02:46,900 --> 02:02:47,630
okay so

1901
02:02:50,110 --> 02:02:55,340
this one saying this one and the same this one and not the same so to that the time

1902
02:02:55,780 --> 02:02:58,110
the face will be the same on the back as it is on the front

1903
02:02:58,900 --> 02:02:59,710
so when you see a white

1904
02:03:00,150 --> 02:03:02,010
is to this chance that back is right

1905
02:03:02,690 --> 02:03:05,070
and when you see around the city firsthand the back is brown

1906
02:03:06,960 --> 02:03:07,460
so that's

1907
02:03:07,940 --> 02:03:13,070
not based on probability theory and i think it's a fairly compelling argument the correct answer is two-thirds

1908
02:03:13,760 --> 02:03:14,900
hopefully that's convinced you

1909
02:03:15,340 --> 02:03:22,110
that we should use probability theory because if you can't actually reliably solve this exercise involving just

1910
02:03:22,750 --> 02:03:28,750
one random variable which is which card another which is which way up when to run burial

1911
02:03:29,320 --> 02:03:31,170
problem if a smart audience if

1912
02:03:31,730 --> 02:03:33,070
graduates from cambridge

1913
02:03:34,420 --> 02:03:38,980
can't reliably solve this problem so i i know it's fine got the answer

1914
02:03:40,150 --> 02:03:41,250
that's really shows you

1915
02:03:41,880 --> 02:03:43,480
yeah inference is a little bit tricky

1916
02:03:43,860 --> 02:03:48,210
inference isn't totally straightforward humans are often very good inference but if you want to

1917
02:03:48,210 --> 02:03:49,050
be sure you get it right

1918
02:03:49,690 --> 02:03:53,230
use probability theory because probability theory will always the right answer

1919
02:03:54,110 --> 02:03:54,510
i okay

1920
02:03:59,400 --> 02:04:02,760
all right so let's now talk some more about noisy channels

1921
02:04:05,490 --> 02:04:06,840
and actually let me just

1922
02:04:07,480 --> 02:04:11,650
i remind myself a kind of let's talk through through this work where we're heading

1923
02:04:11,650 --> 02:04:14,300
is we're going talk about a bit more about inference

1924
02:04:15,380 --> 02:04:18,010
and we are also going to information measures for noisy channels

1925
02:04:18,940 --> 02:04:22,460
classic noisy channel is a binary symmetric channel at the top right

1926
02:04:24,320 --> 02:04:25,420
is obscured by any

1927
02:04:26,760 --> 02:04:27,420
while a cycle

