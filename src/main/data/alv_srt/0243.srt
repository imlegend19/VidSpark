1
00:00:00,000 --> 00:00:05,790
so we had to to get a sequence of one of the variables each

2
00:00:05,800 --> 00:00:06,920
there is

3
00:00:06,920 --> 00:00:10,610
independent and identically distributed according to g

4
00:00:10,620 --> 00:00:15,540
then we can ask what is the marginal distribution of this sequence of random variables

5
00:00:15,550 --> 00:00:17,440
when we integrate out g

6
00:00:17,650 --> 00:00:20,800
so this

7
00:00:20,820 --> 00:00:22,850
it turns out to so

8
00:00:22,940 --> 00:00:26,790
most of the ways in which the dirichlet process is used in practice is

9
00:00:26,860 --> 00:00:30,410
basically the way it interacts with the rest of the

10
00:00:30,420 --> 00:00:33,400
i'm of the model is that we simply draw samples from g

11
00:00:33,480 --> 00:00:37,550
so if we know what's the marginal distribution of samples drawn from g where g

12
00:00:37,550 --> 00:00:42,790
is interviewed that we can know all about g that mean

13
00:00:42,800 --> 00:00:44,750
so that we can only

14
00:00:44,780 --> 00:00:48,460
the second thing is that

15
00:00:48,480 --> 00:00:57,140
since g is this infinite sum of point masses then there is positive probability that

16
00:00:57,140 --> 00:01:00,490
is a positive probability that sets of drawers so

17
00:01:01,420 --> 00:01:03,880
sets of

18
00:01:03,910 --> 00:01:05,320
the draws from g

19
00:01:05,370 --> 00:01:07,360
can take on exactly the same value

20
00:01:07,420 --> 00:01:12,900
so the probability that theta one is going take on value test one is going

21
00:01:12,920 --> 00:01:13,930
to be pi one

22
00:01:13,960 --> 00:01:18,920
and the probability that theta two is going to take on value of theta start

23
00:01:18,930 --> 00:01:22,670
to is also going to be the test that one is going to be pi

24
00:01:22,670 --> 00:01:26,610
one as well so that's a positive probability that theta one and theta two can

25
00:01:26,610 --> 00:01:28,130
take on the same value

26
00:01:32,280 --> 00:01:34,360
one the question we could ask is

27
00:01:35,940 --> 00:01:39,440
if we put sets of that take on the same value

28
00:01:39,470 --> 00:01:44,050
together into a cluster that how does this cluster structure look like

29
00:01:44,800 --> 00:01:46,760
over as sequence

30
00:01:50,060 --> 00:01:55,470
so so those i can represent patients of dirichlet processes which will get quickly before

31
00:01:55,470 --> 00:01:58,600
we move on to different processes

32
00:02:01,090 --> 00:02:02,440
of treaties

33
00:02:02,990 --> 00:02:04,610
i can ask it this slide here

34
00:02:04,620 --> 00:02:06,970
come back to this later way

35
00:02:09,710 --> 00:02:14,070
the first representation where we describe the joint distribution of

36
00:02:14,090 --> 00:02:16,900
the pies and the theta stars

37
00:02:16,920 --> 00:02:20,420
it's called a stick breaking construction

38
00:02:22,360 --> 00:02:24,800
this actually gives us a constructive

39
00:02:24,820 --> 00:02:27,720
a construction for all all the data

40
00:02:28,040 --> 00:02:33,650
and all the other data stars and all the pies together and goes as follows

41
00:02:33,660 --> 00:02:37,430
the state has going to be drawn i i d from the

42
00:02:38,400 --> 00:02:41,110
one of the parameters of or perhaps a shepherd

43
00:02:41,170 --> 00:02:43,540
should introduced the parameters

44
00:02:43,550 --> 00:02:45,230
in case a

45
00:02:45,240 --> 00:02:49,480
she introduced to find the right

46
00:02:50,360 --> 00:02:52,360
over here

47
00:02:52,590 --> 00:03:00,430
i've described to the the dirichlet process in terms of this base measure in london

48
00:03:00,990 --> 00:03:06,420
and what we can do is to normalize lambda so we're going to express lumber

49
00:03:06,440 --> 00:03:09,310
as the product between the scalar alpha that

50
00:03:09,340 --> 00:03:10,330
a oppose

51
00:03:10,340 --> 00:03:13,860
and a probability measure h

52
00:03:13,860 --> 00:03:16,520
so alpha is simply the total mass of lambda

53
00:03:16,530 --> 00:03:18,730
and she's just lumber divided by

54
00:03:18,730 --> 00:03:20,630
its total mass of

55
00:03:20,650 --> 00:03:27,140
and usually people call alpha here the strength parameter or the concentration parameter

56
00:03:27,150 --> 00:03:32,610
and it acts like some sort of inverse variance for the dirichlet process

57
00:03:32,620 --> 00:03:34,250
while this

58
00:03:34,270 --> 00:03:41,060
probability measure h is usually called the base distribution since it is the probability measure

59
00:03:41,110 --> 00:03:44,850
and this turns out to be the mean of the dirichlet process

60
00:03:44,860 --> 00:03:49,070
so yes so if we

61
00:03:49,080 --> 00:03:53,400
look at any particular subset of all probability space e

62
00:03:53,460 --> 00:03:58,130
there is pretty easy to work to see that the expectation of g of a

63
00:03:58,130 --> 00:04:00,360
to g is so this is a

64
00:04:00,540 --> 00:04:04,640
a random variable because g is random so we fixed a but you during the

65
00:04:04,860 --> 00:04:09,140
soldier of is still random and we can look at it its expectation so that

66
00:04:09,140 --> 00:04:11,610
this expectation is simply

67
00:04:11,620 --> 00:04:14,480
h of a where h other is simply the mass

68
00:04:14,490 --> 00:04:18,000
assigned to the subset a by the base distribution h

69
00:04:18,020 --> 00:04:23,320
and we can also look at it's variance then the variance is going to be

70
00:04:23,510 --> 00:04:25,730
some numerator which is

71
00:04:25,780 --> 00:04:28,220
of times one message of

72
00:04:28,230 --> 00:04:32,850
divided by alpha plus one so that's why we see that alpha is the inverse

73
00:04:32,850 --> 00:04:35,610
variance right so the larger alpha is

74
00:04:35,660 --> 00:04:38,750
the smaller the variance and vice versa

75
00:04:38,760 --> 00:04:42,380
so coming back to the stick breaking construction

76
00:04:43,760 --> 00:04:48,600
here's construction for g so at first we're gonna

77
00:04:48,640 --> 00:04:52,030
take all the items and we're going to draw the i i d from our

78
00:04:52,030 --> 00:04:53,960
base distribution h

79
00:04:53,980 --> 00:04:57,290
so h is the distribution we can draw from the so we can draw infinite

80
00:04:58,260 --> 00:04:59,240
from h

81
00:04:59,410 --> 00:05:01,650
and then

82
00:05:05,080 --> 00:05:10,940
this sequence of weights is drawn according to the following way so imagine that we're

83
00:05:10,940 --> 00:05:13,280
going to start with this thing of length one

84
00:05:13,340 --> 00:05:15,830
we're gonna break at some point

85
00:05:17,700 --> 00:05:18,890
we're going to take this

86
00:05:18,950 --> 00:05:21,990
the length of this stick

87
00:05:22,000 --> 00:05:23,360
to be pi one

88
00:05:23,360 --> 00:05:27,080
and then we're going to because so we're going to take

89
00:05:27,100 --> 00:05:30,110
the rest of the thing now gonna break at some point

90
00:05:30,120 --> 00:05:32,960
and then the length of this thing is going to be pi two

91
00:05:32,980 --> 00:05:35,160
and then we're going to take the rest here

92
00:05:35,180 --> 00:05:39,860
get some point and we're going to get poetry pay-for-play five and so on and

93
00:05:39,860 --> 00:05:43,750
because we started off with the stick with the state of length one the total

94
00:05:43,750 --> 00:05:47,810
length of all the spice pi one plus pi two plays poetry and so on

95
00:05:48,060 --> 00:05:49,350
the sum to one

96
00:05:50,450 --> 00:05:56,720
and the locations of the breakpoints here simply drawn from from from beta distribution so

97
00:05:56,730 --> 00:05:57,790
beta one alpha

98
00:05:59,710 --> 00:06:03,460
so this gives us a construction for both

99
00:06:03,500 --> 00:06:05,440
pi and theta

100
00:06:05,450 --> 00:06:10,260
and from past data we can construct a our

101
00:06:10,280 --> 00:06:13,410
our drawn from a dirichlet process g

102
00:06:13,440 --> 00:06:18,950
this is also sometimes known as the GM distribution so the distribution pi sometimes also

103
00:06:18,980 --> 00:06:21,440
called GM distribution we can write it as

104
00:06:21,450 --> 00:06:25,700
prior to the GM of sometimes people use pi

105
00:06:25,740 --> 00:06:28,390
drawn from the stick alpha

106
00:06:28,450 --> 00:06:32,740
there's also another notation that people in machine learning have used

107
00:06:32,760 --> 00:06:36,960
this is actually the initials of

108
00:06:38,040 --> 00:06:40,010
three of the tree

109
00:06:40,020 --> 00:06:42,190
i guess they probably

110
00:06:43,200 --> 00:06:47,540
i think richard's hanging mcclosky

111
00:06:52,950 --> 00:06:53,820
this is a

112
00:06:53,830 --> 00:07:00,020
this is a very direct construction of dirichlet processes and in fact

113
00:07:00,970 --> 00:07:02,710
peter was talking about how

114
00:07:03,220 --> 00:07:05,650
that's kind of difficulties with using

115
00:07:05,660 --> 00:07:11,240
i come across the consistency theorem to construct this random probability measure we can construct

116
00:07:11,240 --> 00:07:12,970
a random function that

117
00:07:13,090 --> 00:07:15,970
i can look like a probability measures but we can

118
00:07:15,980 --> 00:07:19,570
proof that the probability measure

119
00:07:19,590 --> 00:07:23,650
but what's the theorem and it in his nineteen ninety four pp is that started

120
00:07:23,650 --> 00:07:25,120
off with this construction

121
00:07:25,270 --> 00:07:29,150
and proof that this construction satisfies all the problems

122
00:07:29,200 --> 00:07:31,990
all the properties of the dirichlet process

123
00:07:32,000 --> 00:07:36,110
and it's obviously a any draw from is going to be a probability measure is

124
00:07:36,110 --> 00:07:39,580
actually a proper random probability measure

125
00:07:39,600 --> 00:07:42,860
and this proves that

126
00:07:42,890 --> 00:07:44,980
i dirichlet processes

127
00:07:45,110 --> 00:07:48,110
does exist

128
00:07:48,970 --> 00:07:49,760
OK so

129
00:07:50,500 --> 00:07:51,360
the other

130
00:07:51,370 --> 00:07:56,070
a way of looking at a dirichlet processes is using is what is called the

131
00:07:56,070 --> 00:07:58,150
polya urn scheme so in this case

132
00:07:58,160 --> 00:08:01,000
we look at the sequence of draws from

133
00:08:01,040 --> 00:08:02,790
i i d draws from g

134
00:08:02,840 --> 00:08:06,370
so each state has to be drawn from this distribution g

135
00:08:06,370 --> 00:08:09,640
thank you very much for inviting me here

136
00:08:09,660 --> 00:08:15,940
so there was works of yesterday on robust high dimensional statistics and we had a

137
00:08:15,940 --> 00:08:17,410
last minute swarm

138
00:08:17,410 --> 00:08:21,660
so for those of you that were there this this talk parts of this talk

139
00:08:21,710 --> 00:08:29,050
we're presented there so my apologies and plan so i'm going to tell you

140
00:08:29,100 --> 00:08:33,870
a story about similar to the last not just in the in the sense that

141
00:08:34,100 --> 00:08:37,420
we have some corruption and we want to think about how to remove it but

142
00:08:37,420 --> 00:08:42,040
it's in it's it's in a much simpler problem principal component analysis

143
00:08:42,070 --> 00:08:48,250
so there's going to be two problems at all little

144
00:08:48,270 --> 00:08:52,590
be able to talk about today if only briefly the first one is is looking

145
00:08:52,590 --> 00:08:55,960
at the most basic one of the most basic statistical

146
00:08:57,490 --> 00:09:01,270
that that you can use it's been very widely applied in a wide variety of

147
00:09:01,320 --> 00:09:06,000
fields principal component analysis except i'm going to be interested in the case where we

148
00:09:06,000 --> 00:09:10,650
have outliers a lot of our lives not just a few but a constant fraction

149
00:09:10,760 --> 00:09:16,180
five percent one percent twenty percent in these outliers are give you a precise model

150
00:09:16,180 --> 00:09:21,950
later for for the set up here but there are completely arbitrary so i struggled

151
00:09:21,950 --> 00:09:22,230
with my

152
00:09:22,680 --> 00:09:23,840
with my

153
00:09:23,850 --> 00:09:29,090
co-authors here on what word to use because we say outliers it means different things

154
00:09:29,090 --> 00:09:33,600
to different people so we're not assuming that these have high variance or low variance

155
00:09:33,810 --> 00:09:38,680
they're from a different distribution we're going to assume that these outliers are generated in

156
00:09:38,680 --> 00:09:44,040
a completely arbitrary manner possibly even by a malicious adversary possibly by an adversary that

157
00:09:44,060 --> 00:09:47,750
seen the other points and knows the algorithm that we're going to use so so

158
00:09:47,760 --> 00:09:49,950
quite powerful not limited anyway

159
00:09:50,010 --> 00:09:53,340
and then of course the number of outliers

160
00:09:53,400 --> 00:09:58,370
so then i'm going to look at PCA again but look at a different application

161
00:09:58,400 --> 00:10:02,510
show how short of this problem in fact is

162
00:10:02,540 --> 00:10:07,870
kin is going to be the driver behind solving the collaborative filtering problem where you

163
00:10:07,870 --> 00:10:13,030
have manipulators of the easiest way to describe collaborative filtering is is referenced by now

164
00:10:13,030 --> 00:10:19,250
famous netflix problem so netflix has movies end users and the very interested in giving

165
00:10:22,280 --> 00:10:28,340
to users based on very partial observation of movies if seen in rated about what

166
00:10:28,560 --> 00:10:32,900
is they think the netflix thinks that you're going to like and so the question

167
00:10:32,900 --> 00:10:37,870
of course is recommender systems are everywhere amazon uses this

168
00:10:40,120 --> 00:10:42,220
relies on this

169
00:10:43,120 --> 00:10:46,280
i'm interested in this in in this setting where you have a lot of

170
00:10:46,300 --> 00:10:48,460
people that are trying to manipulate the system

171
00:10:48,500 --> 00:10:51,520
so people that are given bogus ratings because they want to be able to give

172
00:10:51,840 --> 00:10:57,000
one one restaurant better recommendation another they want amazon to say

173
00:10:57,400 --> 00:11:00,970
if you look at this point you're going to like this other book or netflix

174
00:11:00,990 --> 00:11:03,740
queues so these are the two problems that i'm going to look at it turns

175
00:11:03,740 --> 00:11:06,560
out that there other quite related

176
00:11:06,580 --> 00:11:13,150
and there's there's an underlying theme that ties these together and that's the regime that

177
00:11:13,150 --> 00:11:15,090
that is particularly interesting

178
00:11:15,120 --> 00:11:17,930
and the regime is challenging

179
00:11:17,970 --> 00:11:22,400
is that the high dimensional data so this is this is featured prominently in all

180
00:11:22,400 --> 00:11:23,900
all of that so many posters

181
00:11:24,300 --> 00:11:27,440
that talked about this but let me just briefly go go over to what is

182
00:11:27,680 --> 00:11:32,050
what is the high dimensional regime as opposed to what i call the classical regime

183
00:11:32,180 --> 00:11:38,240
so the high dimensional regime is the setting where the dimensionality

184
00:11:38,270 --> 00:11:42,680
is approximately equal to the number of observations so if you think about the basic

185
00:11:42,680 --> 00:11:46,990
results you know about in statistics and probability even even the most simple things like

186
00:11:46,990 --> 00:11:48,630
law of large numbers

187
00:11:48,650 --> 00:11:52,180
you fix the dimension and you the number of points go to infinity

188
00:11:52,190 --> 00:11:55,550
so that's not all what i'm what i'm interested in hearing interested in the case

189
00:11:55,880 --> 00:11:57,460
where you have

190
00:11:57,470 --> 00:12:01,840
maybe ten thousand points and maybe there fifty thousand dimensions

191
00:12:01,860 --> 00:12:05,300
or maybe five hundred points and maybe there in five hundred dimensions so that that's

192
00:12:05,300 --> 00:12:06,930
is the that's the regime

193
00:12:06,940 --> 00:12:10,280
can we can obtain scaling results in this regime is well except that you just

194
00:12:10,280 --> 00:12:13,710
scale those two numbers together

195
00:12:13,720 --> 00:12:17,620
so there's many reasons why you why you care about why you might care about

196
00:12:17,620 --> 00:12:25,200
this primarily application driven many examples from from biology and bioinformatics falls exactly into this

197
00:12:25,200 --> 00:12:29,450
picture indeed any situation where you can look in

198
00:12:29,480 --> 00:12:34,640
we're all you can do is is looking more in higher resolution at that something

199
00:12:35,010 --> 00:12:37,430
you immediately fall into the high dimensional data so

200
00:12:37,810 --> 00:12:39,370
for example

201
00:12:39,420 --> 00:12:43,990
think about a patient with a rare disease now

202
00:12:44,000 --> 00:12:49,890
OK so there's thousands of patients have some rare that some rare cancer it's

203
00:12:49,910 --> 00:12:53,520
at one point we had very we had very limited amount of data we could

204
00:12:53,520 --> 00:12:58,930
collect from patients like this but now you can sequence their DNA you can you

205
00:12:58,930 --> 00:13:03,000
can look at proteins you can look at protein structure so while the number of

206
00:13:03,000 --> 00:13:08,140
patients we hope is not increasing or not increasing faster than the population

207
00:13:08,180 --> 00:13:10,930
the description of one of these data points

208
00:13:10,970 --> 00:13:12,580
is now in the

209
00:13:12,620 --> 00:13:16,930
many thousands millions or or or maybe or maybe more so this is why the

210
00:13:16,930 --> 00:13:22,630
high dimensional data is is really prevalent in many different applications one particular application that

211
00:13:22,630 --> 00:13:23,690
he was

212
00:14:14,800 --> 00:14:18,030
the school

213
00:14:22,460 --> 00:14:26,050
so that

214
00:14:26,090 --> 00:14:28,220
in one to one

215
00:14:32,500 --> 00:14:34,530
so i

216
00:14:38,740 --> 00:14:41,130
so that

217
00:14:41,150 --> 00:14:43,940
so what

218
00:15:05,670 --> 00:15:11,620
this is a

219
00:15:26,390 --> 00:15:28,720
one to

220
00:15:46,470 --> 00:15:52,880
the moment

221
00:16:41,210 --> 00:16:46,740
i must be

222
00:17:13,110 --> 00:17:18,900
first one

223
00:17:21,560 --> 00:17:22,970
so this

224
00:17:46,370 --> 00:17:50,080
so what

225
00:18:01,300 --> 00:18:03,150
he so that

226
00:18:18,290 --> 00:18:22,210
that's what this

227
00:18:28,530 --> 00:18:33,960
one is

228
00:18:33,980 --> 00:18:37,300
it's hard to see

229
00:18:43,690 --> 00:18:46,830
in the world

230
00:18:54,240 --> 00:18:56,240
this was

231
00:18:57,500 --> 00:19:00,750
that is

232
00:19:12,610 --> 00:19:18,220
and so this is an instance of and that got me

233
00:19:18,640 --> 00:19:22,700
that's what i

234
00:19:27,100 --> 00:19:29,570
that is for all so

235
00:19:29,590 --> 00:19:30,960
the story

236
00:19:30,970 --> 00:19:35,330
the problem of

237
00:19:42,120 --> 00:19:44,170
we've gone

238
00:19:44,190 --> 00:19:48,070
the cells that line

239
00:20:27,320 --> 00:20:28,310
so it

240
00:20:35,370 --> 00:20:39,200
that's one show

241
00:20:39,200 --> 00:20:40,720
for that

242
00:20:46,740 --> 00:20:48,030
so what

243
00:20:48,030 --> 00:20:53,540
suppose that the question is do you want to take an assistant implement ontology in

244
00:20:53,540 --> 00:20:57,970
order to enhance it and implement it again so if the answer is not so

245
00:20:57,970 --> 00:20:59,350
you don't need to do

246
00:20:59,450 --> 00:21:04,320
engineering of this is the ontology and because of that you don't like the engineering

247
00:21:04,420 --> 00:21:08,880
activity on the on the table so these kind of questions and this kind of

248
00:21:09,170 --> 00:21:14,540
table can be used to select which one idea activities that you need for developing

249
00:21:17,500 --> 00:21:23,280
after that based on the life cycle more than that you've already selected and based

250
00:21:23,280 --> 00:21:26,320
on the application of the activities that you have

251
00:21:26,340 --> 00:21:27,620
to make in the

252
00:21:27,630 --> 00:21:30,760
in the previous table you can

253
00:21:30,770 --> 00:21:35,000
great the ontology life cycle for your

254
00:21:35,010 --> 00:21:39,330
four your ontology and it is important to mention that we can have the ontology

255
00:21:39,330 --> 00:21:43,080
life cycle for the same of so here you can see that

256
00:21:43,110 --> 00:21:49,500
from the requirements specification we go to the sign implementation maintenance or in that case

257
00:21:49,570 --> 00:21:51,810
from the requirements definition we

258
00:21:51,830 --> 00:21:56,590
do we have a use system resources we do some kind of modelling implementation and

259
00:21:56,590 --> 00:22:02,830
maintenance or here we do we use reengineering the same model implementation

260
00:22:02,850 --> 00:22:05,010
i mean these depends of the

261
00:22:05,010 --> 00:22:07,340
d activities that

262
00:22:07,360 --> 00:22:12,850
you have already selected him prove in the previous state

263
00:22:12,870 --> 00:22:13,880
so now i

264
00:22:15,420 --> 00:22:19,740
we know the activities we know about the life cycles more that we know how

265
00:22:19,740 --> 00:22:25,840
about how to select the ontologies for our application and the next step is where

266
00:22:25,840 --> 00:22:29,870
can i find ontologies with the goal of reusing them

267
00:22:29,960 --> 00:22:36,530
so and that case we talk about the ontology metadata vocabulary about and also about

268
00:22:38,640 --> 00:22:40,030
so for the

269
00:22:40,030 --> 00:22:44,480
ontology the ontology metadata vocabulary is on an ontology

270
00:22:44,490 --> 00:22:49,640
that allows us to describe to provide metadata for

271
00:22:49,690 --> 00:22:51,820
the ontology is that we

272
00:22:51,840 --> 00:22:53,890
i have already been so there

273
00:22:53,900 --> 00:22:58,550
we can find information about the name of the ontology the number of classes in

274
00:22:58,570 --> 00:22:59,510
in know

275
00:22:59,530 --> 00:23:04,420
ontology the number of properties in which are linked to the ontologies implemented who is

276
00:23:04,420 --> 00:23:09,940
the author so this is the information that appear in the ontology metadata vocabulary and

277
00:23:11,790 --> 00:23:20,140
we have several ontology registries where we can find information about these ontologies like tony

278
00:23:20,140 --> 00:23:21,020
that or

279
00:23:21,050 --> 00:23:22,300
or what

280
00:23:22,320 --> 00:23:27,020
so how we can search ontologies in that case you using honest so in that

281
00:23:27,020 --> 00:23:33,840
case you will you will get an application in which you you can introduce here

282
00:23:34,770 --> 00:23:35,680
the by the

283
00:23:35,690 --> 00:23:36,900
the the the the the

284
00:23:36,970 --> 00:23:39,900
the some parameters for instance i am looking for

285
00:23:40,000 --> 00:23:45,150
an ontology everything in in RDFS for instance or retaining now

286
00:23:45,210 --> 00:23:48,770
in the domain of travelling and so on and so forth

287
00:23:49,990 --> 00:23:55,960
filling in these so you can get into trouble in domain for instance all these

288
00:23:55,960 --> 00:23:57,700
ontologies which are

289
00:24:01,260 --> 00:24:05,280
for in that case if you select one of the structure ontology is what you

290
00:24:05,280 --> 00:24:08,620
get is some kind of RDF called with the

291
00:24:08,640 --> 00:24:16,720
information about these these ontologies with a to the ontology so clicking there because you

292
00:24:16,720 --> 00:24:18,460
can just download the

293
00:24:19,200 --> 00:24:22,250
the ontology of the other systems and especially watson

294
00:24:22,260 --> 00:24:23,300
which is the new

295
00:24:23,330 --> 00:24:28,790
and your application built for searching on the web you can even though we import

296
00:24:28,790 --> 00:24:34,110
some pieces of the ontologies if use the neon toolkit in by

297
00:24:34,810 --> 00:24:35,990
so now

298
00:24:36,000 --> 00:24:37,560
you have already

299
00:24:37,590 --> 00:24:42,590
you have the ontologies you know about the process and

300
00:24:42,600 --> 00:24:47,490
what thing going two percent now is an example of how we have built ontologies

301
00:24:47,500 --> 00:24:50,320
and deployment mediators marketplace

302
00:24:50,370 --> 00:24:54,150
OK case in order to provide us feedback of how to use or this is

303
00:24:54,170 --> 00:24:55,250
stuff before

304
00:24:55,290 --> 00:25:00,290
four billion application so in that case what we have is a person

305
00:25:00,300 --> 00:25:05,840
these persons that introduced her CV

306
00:25:05,850 --> 00:25:11,250
the city is for one two and it's one of employment services which she which

307
00:25:11,250 --> 00:25:13,630
are in different countries so

308
00:25:13,640 --> 00:25:19,570
these in that case they should be these in the information on the different the

309
00:25:19,570 --> 00:25:21,570
so that the basis

310
00:25:21,620 --> 00:25:25,670
the different companies use it is is is localized in the it is written in

311
00:25:25,670 --> 00:25:31,060
different mediums this in the different kind in in the different primate centres do some

312
00:25:31,060 --> 00:25:31,750
kind of

313
00:25:31,790 --> 00:25:38,580
mapping between the city of the job first the different employment service provides so if

314
00:25:38,580 --> 00:25:42,370
you so much i mean if there is eligible for for for the person then

315
00:25:42,510 --> 00:25:48,210
an aggregated offer is sent to the to the first so in that case we

316
00:25:48,210 --> 00:25:50,190
are working on with one

317
00:25:50,580 --> 00:25:56,830
context and at the same time different companies have different databases and different schemas for

318
00:25:56,830 --> 00:26:04,040
storing the information so they be in the domain of this application is really simple

319
00:26:04,070 --> 00:26:08,230
i need to more than usual for CV so

320
00:26:08,490 --> 00:26:10,300
class of people

321
00:26:10,330 --> 00:26:14,760
so there are two ways of building these these this ontology first one

322
00:26:14,790 --> 00:26:20,370
is trying to build a center using a centralized approach we can be to just

323
00:26:20,370 --> 00:26:27,050
one centralized ontology one reference ontologies and to provide different types of mappings between the

324
00:26:27,050 --> 00:26:33,970
ontologies and the different is key mass of the different employment services

325
00:26:33,990 --> 00:26:35,100
the other

326
00:26:35,130 --> 00:26:38,020
approaches to use the federated network of

327
00:26:38,020 --> 00:26:40,100
he transitions are particular action

328
00:26:40,120 --> 00:26:41,670
in a particular state right

329
00:26:41,680 --> 00:26:43,420
it is many times

330
00:26:43,590 --> 00:26:48,170
then you're estimates of the probability of the next is going to converge to the

331
00:26:49,170 --> 00:26:51,220
and these estimates converge

332
00:26:51,230 --> 00:26:52,480
very closely

333
00:26:52,480 --> 00:26:54,670
then he dynamic programme

334
00:26:54,730 --> 00:26:56,270
run on

335
00:26:56,310 --> 00:26:58,270
these two mdps will be

336
00:26:58,290 --> 00:27:00,500
almost the same

337
00:27:00,520 --> 00:27:09,020
in terms of the value of the next next summer rewards in return

338
00:27:09,120 --> 00:27:12,930
and this is the explore exploit lemma

339
00:27:12,980 --> 00:27:15,870
explore exploit lemma says that

340
00:27:15,930 --> 00:27:18,880
the value of the programming

341
00:27:19,670 --> 00:27:22,910
none mdps of knowledge

342
00:27:22,960 --> 00:27:25,080
but two times

343
00:27:25,160 --> 00:27:29,490
divide programming and in piece about on average is greater than

344
00:27:30,460 --> 00:27:35,790
the best you can do in the real world

345
00:27:35,890 --> 00:27:36,670
OK so

346
00:27:36,690 --> 00:27:40,570
these are the two fundamental and one of them says

347
00:27:40,620 --> 00:27:45,690
the internal data structure of the down to to converge to the truth

348
00:27:45,730 --> 00:27:47,710
cannot converge well enough that we can

349
00:27:47,730 --> 00:27:49,060
estimate things

350
00:27:51,140 --> 00:27:54,540
then they explore exploit them

351
00:27:54,560 --> 00:27:56,940
this is something about

352
00:27:57,440 --> 00:28:00,560
our beliefs

353
00:28:01,350 --> 00:28:02,650
how well we can

354
00:28:05,350 --> 00:28:06,710
o o

355
00:28:06,770 --> 00:28:08,930
the sooner this is

356
00:28:08,940 --> 00:28:10,350
the probability

357
00:28:10,440 --> 00:28:13,650
you can actually reach an unknown state

358
00:28:13,660 --> 00:28:15,750
because of the way we define things

359
00:28:15,830 --> 00:28:17,100
and this is

360
00:28:17,190 --> 00:28:20,120
what are maximum effect summary words is

361
00:28:21,790 --> 00:28:22,980
none of h

362
00:28:24,350 --> 00:28:27,100
in the real world

363
00:28:27,620 --> 00:28:30,330
this sum is created and

364
00:28:30,370 --> 00:28:32,790
the best the optimal policy can do

365
00:28:32,830 --> 00:28:36,460
in the real world

366
00:28:36,540 --> 00:28:39,080
OK so the way we use these dilemmas

367
00:28:39,080 --> 00:28:40,310
if we say

368
00:28:40,410 --> 00:28:43,520
we choose in

369
00:28:43,580 --> 00:28:46,560
it's that the simulation and by pretty hard

370
00:28:46,980 --> 00:28:48,580
so in particular

371
00:28:50,750 --> 00:28:52,020
one over poly

372
00:28:52,040 --> 00:28:54,600
it is very small

373
00:28:54,620 --> 00:28:57,230
and then

374
00:28:57,250 --> 00:29:00,080
for exploit lemma

375
00:29:00,120 --> 00:29:02,970
so is implied the difference between

376
00:29:02,980 --> 00:29:05,270
how well we can do in the real world

377
00:29:05,330 --> 00:29:06,660
and how well we can do

378
00:29:06,680 --> 00:29:09,110
in region

379
00:29:11,350 --> 00:29:13,250
all right so this was the the

380
00:29:13,310 --> 00:29:18,210
the by the if this difference is large

381
00:29:18,600 --> 00:29:21,210
however we can do in the real world

382
00:29:21,210 --> 00:29:22,790
and how well we can do

383
00:29:23,500 --> 00:29:26,640
the notion of a ge MDP forever

384
00:29:26,680 --> 00:29:29,380
because of the simulation and this

385
00:29:29,410 --> 00:29:30,710
is also the same

386
00:29:30,730 --> 00:29:34,000
as p none of age right

387
00:29:34,020 --> 00:29:36,660
so the in how we can do in the real world

388
00:29:36,790 --> 00:29:39,850
and how well they can do in the known of HMT

389
00:29:39,910 --> 00:29:41,480
it's good enough on

390
00:29:41,520 --> 00:29:43,460
as you imply

391
00:29:44,790 --> 00:29:45,930
the probability

392
00:29:48,180 --> 00:29:49,430
it's going to be greater than

393
00:29:49,440 --> 00:29:54,230
some reasonable quantity

394
00:29:54,250 --> 00:29:56,160
so the two possibilities

395
00:29:56,170 --> 00:29:58,550
e there

396
00:29:58,560 --> 00:30:00,600
the inequality goes this way

397
00:30:00,640 --> 00:30:03,040
in which case were done because acting

398
00:30:03,060 --> 00:30:06,310
according to innovate and if you want to be near optimal

399
00:30:06,360 --> 00:30:08,720
or the inequality goes this way

400
00:30:08,730 --> 00:30:17,790
in which case the probability of exploration has to be recently large

401
00:30:17,910 --> 00:30:22,790
and of course it's probably a good exploration is reasonably large

402
00:30:22,790 --> 00:30:24,620
well because

403
00:30:24,640 --> 00:30:26,080
we can only

404
00:30:26,110 --> 00:30:28,010
successfully explore

405
00:30:28,020 --> 00:30:30,430
something like this many times

406
00:30:30,460 --> 00:30:33,060
so the the the

407
00:30:33,080 --> 00:30:37,210
this is a good exploration is we can have some state where there's some action

408
00:30:37,330 --> 00:30:40,660
we have executed less than n times

409
00:30:44,920 --> 00:30:46,200
we can some state

410
00:30:46,690 --> 00:30:52,350
so there are times that we can and cannot state whether action is executed lesson

411
00:30:52,350 --> 00:30:54,850
in times is something like

412
00:30:54,850 --> 00:30:57,970
the second be elaborated in various ways

413
00:31:00,230 --> 00:31:07,790
it's simply this object would have to be positive

414
00:31:07,800 --> 00:31:09,870
mister here

415
00:31:13,720 --> 00:31:16,390
the interstate actually

416
00:31:16,590 --> 00:31:21,750
not actually stated that have a function alpha k

417
00:31:21,780 --> 00:31:24,090
plus b if it is positive

418
00:31:24,170 --> 00:31:28,320
then it's inside it is negative outside

419
00:31:28,330 --> 00:31:35,120
well it's easiest to see from here the

420
00:31:35,130 --> 00:31:37,420
there's bound because back in

421
00:31:37,430 --> 00:31:38,600
if i saw

422
00:31:38,650 --> 00:31:41,430
the hyperplane in in feature space on the data

423
00:31:41,540 --> 00:31:46,520
these points actually lying back in in the input may actually on the page

424
00:31:46,540 --> 00:31:50,860
this is guys on inside and but i do have a function which i don't

425
00:31:50,860 --> 00:31:56,570
see it stated but it's basically when the function is positive or zero then OK

426
00:31:56,570 --> 00:31:57,200
where it

427
00:31:57,340 --> 00:32:00,850
negative and outside

428
00:32:00,860 --> 00:32:04,440
so dense in the state of the art

429
00:32:04,500 --> 00:32:07,300
right so other things

430
00:32:07,580 --> 00:32:12,300
this is going to push it down with alex smola and now lives in order

431
00:32:12,440 --> 00:32:14,110
see who

432
00:32:14,120 --> 00:32:17,030
because god so passive learning

433
00:32:17,410 --> 00:32:21,800
personnel in the learning machine receives examples so not sure those other things you can

434
00:32:21,800 --> 00:32:23,680
do for active learning

435
00:32:24,500 --> 00:32:29,340
why learning and indeed also on the show that the

436
00:32:29,350 --> 00:32:34,440
svm framework as well set up for active learning so passive learning the learning machine

437
00:32:34,440 --> 00:32:39,780
receives examples learns these and attempts to generalize to new instances active learning the learning

438
00:32:39,780 --> 00:32:42,360
machine poses queries or questions to the oracle

439
00:32:42,580 --> 00:32:48,990
so as to maximize returns she actually useful examples of this surely

440
00:32:49,020 --> 00:32:53,330
now there are several types of active learning

441
00:32:54,180 --> 00:33:00,010
the algorithm selects unlabeled examples for the human expert to label result we have this

442
00:33:00,960 --> 00:33:03,790
family of handwritten characters

443
00:33:03,810 --> 00:33:05,750
and what you would wish

444
00:33:06,130 --> 00:33:12,830
what do you have these two hundred sixty thousand handwritten characters and some of so

445
00:33:12,830 --> 00:33:19,190
let's go around and label always what i'm saying is the learning machine could have

446
00:33:19,190 --> 00:33:21,890
all these unlabeled instances and ask OK

447
00:33:21,900 --> 00:33:24,000
what is this handwritten character

448
00:33:24,020 --> 00:33:25,500
presents a new one

449
00:33:25,800 --> 00:33:31,730
what is this handwritten characters well so this is called membership queries algorithm selects unlabeled

450
00:33:31,730 --> 00:33:35,030
examples there a second type of

451
00:33:35,050 --> 00:33:37,250
query which is creating queries

452
00:33:39,260 --> 00:33:44,810
this can be meaningless so context so for handwritten characters

453
00:33:44,860 --> 00:33:48,970
i can obviously do the thing i've got a whole lot of handwritten characters and

454
00:33:48,980 --> 00:33:53,310
the mountain can pick this one and say what what does one

455
00:33:53,340 --> 00:33:57,760
and what is this it's a what's it has to be but if i offended

456
00:33:57,760 --> 00:34:01,930
handwritten character can easily invent every character just makes no sense OK

457
00:34:01,940 --> 00:34:07,500
so depending on the context so these makes sense it's not it's membership queries which

458
00:34:07,500 --> 00:34:09,360
and the talk about now

459
00:34:10,040 --> 00:34:12,190
i'm going skip some of this

460
00:34:12,200 --> 00:34:14,710
and just get the essential points

461
00:34:14,730 --> 00:34:16,560
support vector machines

462
00:34:16,580 --> 00:34:19,400
they were set up in this context show you why

463
00:34:21,430 --> 00:34:23,600
one thing i noted earlier on

464
00:34:23,610 --> 00:34:26,810
is these points here than on support vectors

465
00:34:26,840 --> 00:34:31,130
never appear in the decision function and support vectors are

466
00:34:31,150 --> 00:34:36,130
in fact i therefore don't really need to know the labels of the don't support

467
00:34:36,130 --> 00:34:37,380
vectors the

468
00:34:37,610 --> 00:34:42,400
sort of an important for me because they were ultimately into the decision function so

469
00:34:43,390 --> 00:34:44,600
what i could do

470
00:34:44,620 --> 00:34:50,590
it is actually try and run a heuristic work on labelled data

471
00:34:50,600 --> 00:34:55,000
and i was simply trying to ask the labels if i can of what going

472
00:34:55,000 --> 00:34:57,100
to be the support vectors OK

473
00:34:57,120 --> 00:35:00,550
so for the SVM scenarios will set up for

474
00:35:00,580 --> 00:35:02,120
membership query

475
00:35:02,130 --> 00:35:05,950
in fact we may as well therefore what is the best point to query the

476
00:35:05,950 --> 00:35:12,280
unlabeled instances and mike algorithm can pick these what it turned out was the slides

477
00:35:12,280 --> 00:35:15,310
called consi ways but basically

478
00:35:15,620 --> 00:35:22,150
if let t plus the labeled instances where vast labelled square

479
00:35:22,240 --> 00:35:24,050
the negative

480
00:35:24,240 --> 00:35:29,660
so i've already asked the labels of these these red blobs are data points but

481
00:35:29,660 --> 00:35:34,050
i don't know what the label is and i met office label OK it turns

482
00:35:34,050 --> 00:35:38,810
out to prove the theorem that in fact this goes back a bit to what

483
00:35:38,830 --> 00:35:43,240
in actually did in nineteen nineties and the contacts but it turns out that data

484
00:35:43,240 --> 00:35:48,340
points which is closest to the current hyperplane is the best point query in fact

485
00:35:48,350 --> 00:35:53,510
so makes sense because it's probably going to be maximally ambiguous

486
00:35:53,540 --> 00:35:58,550
these guys here probably going to be negative and these are going to be positive

487
00:35:58,550 --> 00:36:02,650
whereas this ones which and the marginal band and particularly those one to close to

488
00:36:02,650 --> 00:36:08,910
the current hyperplane probably maximum ambiguous i do make the game from last president query

489
00:36:10,590 --> 00:36:16,500
if the query faster label this one it was going to be negative supposedly negative

490
00:36:16,580 --> 00:36:19,330
then this is going to have to waste my time

491
00:36:19,340 --> 00:36:22,460
is going to be positive one something off is going to happen because it's going

492
00:36:22,460 --> 00:36:26,850
to be hyperplane around a lot so it doesn't look too good

493
00:36:26,870 --> 00:36:28,930
well what i imagine what happened here

494
00:36:28,940 --> 00:36:31,230
is it possible is going to be a positive

495
00:36:31,250 --> 00:36:35,440
my heart i might move that y can be negative news that way but relatively

496
00:36:35,440 --> 00:36:37,330
little effects and

497
00:36:37,390 --> 00:36:40,510
it's so consistent with what's been going on for

498
00:36:40,530 --> 00:36:43,510
just show you an instance of the learning

499
00:36:43,530 --> 00:36:47,210
let me just to come up with the damn rule which is the majority rule

500
00:36:47,210 --> 00:36:51,740
which i created a string of zeroes and ones if i one zero then the

501
00:36:51,740 --> 00:36:55,860
targets one more zeros and ones that the target is zero

502
00:36:55,860 --> 00:36:58,120
OK now that is the rule

503
00:36:58,130 --> 00:37:00,190
and i have to make most of learning

504
00:37:00,200 --> 00:37:02,810
one is i check my SVM

505
00:37:02,870 --> 00:37:09,130
just these samples and it passively learned examples i give it does this then this

506
00:37:09,130 --> 00:37:11,220
is the curve it follows the k

507
00:37:11,250 --> 00:37:13,550
and as even more samples

508
00:37:13,560 --> 00:37:17,190
then the test error drops until eventually gets to zero

509
00:37:17,200 --> 00:37:22,540
alternatively my learning machine could pick a string of his choosing

510
00:37:22,560 --> 00:37:26,280
all string perhaps in the back of a lot of these strings they can ask

511
00:37:27,030 --> 00:37:28,500
what's the answer

512
00:37:28,510 --> 00:37:30,860
two the strings

513
00:37:30,860 --> 00:37:32,550
generalized clause

514
00:37:32,560 --> 00:37:36,280
which has exactly one formula on the positive side nothing on the negative side were

515
00:37:36,280 --> 00:37:38,530
just asserting out

516
00:37:39,580 --> 00:37:42,310
now at every step

517
00:37:42,320 --> 00:37:44,330
we've got to set

518
00:37:44,340 --> 00:37:46,310
generally have several

519
00:37:46,380 --> 00:37:51,680
such clauses which may have left side right side positive and negative

520
00:37:53,810 --> 00:37:56,010
and we're going to apply

521
00:37:56,030 --> 00:37:59,240
one of the set of rules to simplify

522
00:37:59,250 --> 00:38:02,190
and the rules are very straightforward

523
00:38:02,200 --> 00:38:04,090
OK there come in

524
00:38:04,210 --> 00:38:07,130
are there sort of like quite a lot of them because every connective can appear

525
00:38:07,140 --> 00:38:10,500
on the positive side or on the negative side

526
00:38:10,590 --> 00:38:12,030
but wherever

527
00:38:12,050 --> 00:38:15,750
there is a formula with some

528
00:38:15,760 --> 00:38:19,090
connected and it is going to have a main connective and there will be some

529
00:38:19,090 --> 00:38:20,450
rule to simplify it

530
00:38:20,460 --> 00:38:22,040
for example

531
00:38:22,090 --> 00:38:24,670
if we have at any point

532
00:38:24,850 --> 00:38:31,750
if we have at any point and negated formulas are not be

533
00:38:31,790 --> 00:38:35,870
on the positive side

534
00:38:36,500 --> 00:38:39,150
we've got general clause here

535
00:38:39,230 --> 00:38:42,210
and this copy

536
00:38:42,240 --> 00:38:46,740
with the not be in it

537
00:38:46,790 --> 00:38:50,390
OK well let's let's just be a little bit more

538
00:38:50,460 --> 00:38:52,850
formal about this

539
00:38:53,090 --> 00:38:58,760
he is some people prime

540
00:38:58,800 --> 00:39:00,710
together with

541
00:39:00,720 --> 00:39:02,190
use a comma

542
00:39:02,240 --> 00:39:05,000
not be

543
00:39:06,240 --> 00:39:08,220
the negative

544
00:39:08,240 --> 00:39:11,480
stuff maybe anything

545
00:39:11,490 --> 00:39:14,030
we can transform that

546
00:39:14,040 --> 00:39:15,300
without changing the

547
00:39:15,320 --> 00:39:16,800
without changing the meaning

548
00:39:16,810 --> 00:39:20,200
intuitively obviously we can transform back

549
00:39:20,250 --> 00:39:22,220
in two well we're just leave

550
00:39:22,300 --> 00:39:24,320
other stuff on the positive side

551
00:39:24,330 --> 00:39:26,010
and will be

552
00:39:26,090 --> 00:39:29,670
on the negative side

553
00:39:29,710 --> 00:39:32,000
OK we'll just have to be is one more

554
00:39:32,010 --> 00:39:36,020
negative sky and we got rid of the connective

555
00:39:36,030 --> 00:39:37,560
OK here there was

556
00:39:37,610 --> 00:39:42,140
and negative formula on the positive side here the inside that is on the negative

557
00:39:43,430 --> 00:39:46,260
and it's the same thing the other way as well

558
00:39:46,350 --> 00:39:48,770
OK if we had not be over here

559
00:39:48,840 --> 00:39:49,960
we could fly

560
00:39:49,970 --> 00:39:54,060
and for the be over here

561
00:39:54,070 --> 00:39:55,820
and i

562
00:39:55,830 --> 00:40:00,510
and we can do the same thing with other connectives also

563
00:40:00,560 --> 00:40:02,310
if we have

564
00:40:02,380 --> 00:40:13,350
here you that there's some stuff i mean it's this is

565
00:40:15,040 --> 00:40:17,000
when i write the prime

566
00:40:17,010 --> 00:40:18,040
not be

567
00:40:18,050 --> 00:40:20,540
i mean the prime union

568
00:40:20,550 --> 00:40:22,350
not be

569
00:40:22,360 --> 00:40:24,400
OK and this

570
00:40:24,410 --> 00:40:28,010
this is one of the one of the general clauses the maybe bunch up OK

571
00:40:28,010 --> 00:40:31,390
you leave everything else same

572
00:40:31,760 --> 00:40:35,430
you can do the same

573
00:40:35,440 --> 00:40:38,660
four other connectives for example for

574
00:40:42,430 --> 00:40:46,790
suppose you have b and c

575
00:40:46,870 --> 00:40:50,650
on the positive side

576
00:40:50,660 --> 00:40:52,670
so this is

577
00:41:03,120 --> 00:41:04,950
and the negative

578
00:41:05,990 --> 00:41:09,040
then we what we're doing is we're sort of

579
00:41:09,060 --> 00:41:11,250
we're asserting the positive stuff

580
00:41:11,290 --> 00:41:19,860
and denying the negative stuff well asserted conjunction is the same as asserting the condoms

581
00:41:21,590 --> 00:41:23,350
what happens to this

582
00:41:23,410 --> 00:41:25,750
is it turns into well

583
00:41:25,840 --> 00:41:29,750
we're setting the condoms but the whole thing is a sort of distraction and you

584
00:41:29,750 --> 00:41:36,830
know it so this is going to turn into two disjunctions

585
00:41:36,840 --> 00:41:40,760
one would be

586
00:41:42,110 --> 00:41:44,160
one with

587
00:41:49,460 --> 00:41:51,650
all right we're asserting this

588
00:41:51,710 --> 00:41:55,300
disjunction and we're setting this distance

589
00:41:56,300 --> 00:41:58,010
it's so

590
00:41:58,030 --> 00:42:01,590
so there the and is it is here as it were

591
00:42:02,310 --> 00:42:07,850
the connective disappeared here

592
00:42:07,860 --> 00:42:12,690
and similar things happen on the

593
00:42:12,700 --> 00:42:15,910
well do you think happens on the negative side

594
00:42:15,960 --> 00:42:19,200
if we have

595
00:42:19,250 --> 00:42:21,450
this IP

596
00:42:22,550 --> 00:42:25,450
and some in private together with

597
00:42:25,450 --> 00:42:30,370
b and c

598
00:42:30,390 --> 00:42:33,000
we can transform that keep

599
00:42:41,080 --> 00:42:42,530
and we just

600
00:42:42,540 --> 00:42:45,510
but in the two

601
00:42:45,510 --> 00:42:46,640
OK so

602
00:42:46,680 --> 00:42:47,700
the thing

603
00:42:47,720 --> 00:42:53,330
willing MCMC i want to simulate essentially i want to obtain samples

604
00:42:53,390 --> 00:42:56,480
distributed according to the target distribution pi OK

605
00:42:56,500 --> 00:43:00,980
essentially what i'm going to do it does the MCMC philosophy

606
00:43:00,990 --> 00:43:02,810
i'm essentially

607
00:43:03,600 --> 00:43:08,080
building the markov chain which is such that asymptotically

608
00:43:08,100 --> 00:43:13,480
in the number of iterations by markov change i have some paul approximate is distributed

609
00:43:13,480 --> 00:43:18,120
according to pi to do this i need to basically build markov process which admits

610
00:43:18,120 --> 00:43:23,340
basically markov kernel which any bias involved distribution on a generic way to do this

611
00:43:23,700 --> 00:43:26,430
i told you was metropolis hastings were essentially

612
00:43:26,830 --> 00:43:32,650
you propose come the day basically according to proposal distribution that is potentially dependent on

613
00:43:32,650 --> 00:43:36,510
the current state of the markov chain on then you accept or reject it could

614
00:43:36,510 --> 00:43:41,270
this probability i five OK which is given by the ratio target time to raise

615
00:43:41,290 --> 00:43:42,730
your proposal

616
00:43:42,760 --> 00:43:48,370
it's a very very generic categories on we've seen that basically these guys

617
00:43:48,440 --> 00:43:50,630
is a

618
00:43:50,660 --> 00:43:53,220
it generates whatever being too

619
00:43:53,240 --> 00:43:57,210
markov can we do i think the distribution so

620
00:43:57,210 --> 00:44:01,130
does that as required i only know to know the target up to a normalizing

621
00:44:01,130 --> 00:44:04,940
constant because it only appears basically we

622
00:44:05,270 --> 00:44:06,930
in the algorithm

623
00:44:07,100 --> 00:44:12,460
in the race still gamma of the current candidates gamma proposal divided by the cold

624
00:44:12,600 --> 00:44:15,740
value so this is i don't need to know what exactly only have to what

625
00:44:15,960 --> 00:44:17,290
constant that's quite

626
00:44:18,180 --> 00:44:20,040
on you can be

627
00:44:20,050 --> 00:44:22,020
x four major OK

628
00:44:22,040 --> 00:44:25,620
as long as you can see from this guy

629
00:44:25,630 --> 00:44:27,100
that's OK

630
00:44:27,130 --> 00:44:30,680
basically you can basically are evaluated

631
00:44:30,710 --> 00:44:35,710
simulate unevaluated then it's fine so you can crazy thing actually in the metropolis hastings

632
00:44:35,710 --> 00:44:39,770
algorithm says basically on the current state

633
00:44:39,790 --> 00:44:41,650
basically of

634
00:44:42,240 --> 00:44:44,180
chromosome maps x

635
00:44:44,190 --> 00:44:47,600
then say you could do something like one

636
00:44:47,770 --> 00:44:54,040
say look cool optimisation algorithms like gradient type algorithms to find the real cool

637
00:44:54,060 --> 00:44:56,450
maximum of the

638
00:44:56,480 --> 00:44:58,410
called the target distribution

639
00:44:58,470 --> 00:45:02,850
on then propose the candidate some further on this local maxima

640
00:45:02,860 --> 00:45:04,980
that's valid you can do that

641
00:45:05,010 --> 00:45:10,350
as long as deterministic algorithm to the local optimisation technique uses deterministic that's fine you

642
00:45:10,350 --> 00:45:11,760
can all districts

643
00:45:11,860 --> 00:45:14,850
you could do actually think even more complicated actually

644
00:45:14,850 --> 00:45:16,440
yeah like value

645
00:45:16,450 --> 00:45:19,060
the most his x

646
00:45:19,070 --> 00:45:22,350
you could run actually is stochastic algorithm OK

647
00:45:22,380 --> 00:45:26,190
that's kind of stochastic optimization algorithm that finds

648
00:45:26,250 --> 00:45:29,510
look cool map maximum of the target

649
00:45:29,530 --> 00:45:31,980
use ontologies footballer on that

650
00:45:31,980 --> 00:45:36,200
it's a bit more complicated argument that you can show this is valid actually so

651
00:45:36,220 --> 00:45:38,980
you can be proposing the very complicated way

652
00:45:44,760 --> 00:45:46,760
you would initialize yo

653
00:45:46,780 --> 00:45:50,010
you're basically you're optimisation

654
00:45:51,090 --> 00:45:54,640
positive at according you sample that it

655
00:45:56,570 --> 00:45:58,880
you can do that

656
00:45:58,890 --> 00:46:02,910
so this is this is valid

657
00:46:02,950 --> 00:46:10,530
so i was saying you use that to build

658
00:46:10,570 --> 00:46:15,750
the the proposal on then you would or on this this this a local you

659
00:46:15,750 --> 00:46:18,510
need to get the company in in this case

660
00:46:19,750 --> 00:46:24,190
but you can use the stochastic so i resented it is still valid the argument

661
00:46:24,190 --> 00:46:29,060
is a little bit more complex is called a quantum proposal mental processing that's from

662
00:46:29,060 --> 00:46:34,200
quite a few use in the literature refer you to pay for these eichengreen peter

663
00:46:34,340 --> 00:46:35,890
the spigot to mortar

664
00:46:36,690 --> 00:46:41,040
so it is much more freedom that summer essentially with the gibbs sampler

665
00:46:42,060 --> 00:46:44,360
because where in the gibbs sampling

666
00:46:44,380 --> 00:46:50,440
essentially all only sample the candidate compared scale according to its conditional distribution given the

667
00:46:50,440 --> 00:46:52,810
remaining components OK

668
00:46:52,820 --> 00:46:57,860
so you need also to be able to sample from the conditional distribution we've discovered

669
00:46:58,090 --> 00:47:00,560
processing algorithm you don't need that

670
00:47:00,570 --> 00:47:03,280
OK this year we don't need that

671
00:47:03,290 --> 00:47:05,490
OK let's good so let's consider

672
00:47:05,620 --> 00:47:09,540
instead of continuing very content like complicated stuff for for us to start let's try

673
00:47:09,540 --> 00:47:12,420
to kind consider a very simple

674
00:47:12,430 --> 00:47:16,540
simple case so the first thing could be say well i would like to be

675
00:47:16,550 --> 00:47:17,540
the proposal

676
00:47:17,620 --> 00:47:20,410
which is independent proposals so it's a bit

677
00:47:20,420 --> 00:47:25,000
very similar to rejection sampling essentially forget the concept of markov chain triple but the

678
00:47:25,000 --> 00:47:29,270
county according to density qx is called independent proposals

679
00:47:29,280 --> 00:47:32,100
in this case i was seductively employability

680
00:47:32,110 --> 00:47:36,660
well this is the ratio of the target summary the proposal get simplified obviously because

681
00:47:37,050 --> 00:47:41,600
the public independent of point anywhere on obviously this is the rate of the normalized

682
00:47:41,600 --> 00:47:45,780
target on you can write it as the ratio of normalized proposal why did i

683
00:47:45,780 --> 00:47:49,280
write in this way is just to make you should show you that essentially in

684
00:47:49,280 --> 00:47:55,410
this case the image will be thing acceptance probability essentially you have completed that are

685
00:47:55,410 --> 00:47:58,710
very very similar to what appears in rejection sampling

686
00:47:58,980 --> 00:48:06,350
so it is very similar so compared to rejection sampling i don't this requirement that

687
00:48:06,350 --> 00:48:07,530
i need

688
00:48:07,540 --> 00:48:10,370
two have that gamma of the q

689
00:48:10,390 --> 00:48:11,740
these upper bounded

690
00:48:11,750 --> 00:48:14,810
over the wall space it that is actually

691
00:48:14,840 --> 00:48:17,220
bush and i said with an aneurysm

692
00:48:17,230 --> 00:48:20,030
whatever being she is going to have essentially

693
00:48:20,040 --> 00:48:24,900
the white in the distribution while at the same point independent proposal you support

694
00:48:24,930 --> 00:48:29,370
of pies including the support of q but that this condition on the tails okay

695
00:48:29,370 --> 00:48:30,930
so that seems to be quite good

696
00:48:31,700 --> 00:48:33,100
so one

697
00:48:33,160 --> 00:48:37,990
delete looks so much like image processing and the rejection sampling algorithm you know you

698
00:48:37,990 --> 00:48:41,160
might want to assess whether what's going to be the performance of the cycle reason

699
00:48:41,160 --> 00:48:45,080
for different proposals OK so let's going to target

700
00:48:45,100 --> 00:48:50,970
x OK are going to be some proposal which are essentially two case once we

701
00:48:50,980 --> 00:48:55,220
basically independent proposal which have thicker tails

702
00:48:55,230 --> 00:49:00,240
OK so it's we just want tell cannot tales in which case essentially private URI

703
00:49:00,410 --> 00:49:04,970
is basically unbounded OK in which case you could apply

704
00:49:04,980 --> 00:49:10,620
the rejection sampling but human you will be able to apply basically the middle possessing

705
00:49:10,910 --> 00:49:16,950
of case where essentially she was fricatives OK in which case both metropolis hastings on

706
00:49:17,100 --> 00:49:21,920
basically rejection sampling could be used so here is what happens when basically

707
00:49:21,970 --> 00:49:27,000
you're trying to sample from simple univariate basically gaussian distribution

708
00:49:27,020 --> 00:49:29,840
on you use a proposal distribution

709
00:49:29,840 --> 00:49:37,120
Dirichlet distribution if we reorder them stochastically according to this process which is called size

710
00:49:37,130 --> 00:49:42,740
biased permutation then we get a stick breaking construction okay and this process basically

711
00:49:42,740 --> 00:49:47,260
goes as follows so what we're gonna first decide which cluster is gonna be our

712
00:49:47,260 --> 00:49:52,260
first cluster and we're gonna pick cluster K to be the first cluster with probability

713
00:49:52,260 --> 00:49:57,720
phi K okay so that will be our first cluster and we remove that cluster from

714
00:49:57,890 --> 00:50:03,960
our list of clusters normalize our mixing proportions and repeat this so that we pick a

715
00:50:03,960 --> 00:50:08,580
second cluster and the third cluster and so forth and we see that of course

716
00:50:08,580 --> 00:50:14,620
this is this will give us a stochastic ordering where the phis are not

717
00:50:14,620 --> 00:50:23,950
but they are on average decreasing okay and then after you've done this reordering we can now

718
00:50:23,950 --> 00:50:31,340
take K to go to infinity and that gives us a stick breaking construction so

719
00:50:31,340 --> 00:50:35,720
the nice thing with the stick breaking construction as a representation of the Dirichlet process

720
00:50:35,720 --> 00:50:43,840
is that is actually very easy to work with and it's very easy to generalize

721
00:50:43,840 --> 00:50:48,160
so I won't be go ing into details but I'll come back to to the

722
00:50:48,160 --> 00:50:53,080
mixture models now and look at the different representations particularly the Chinese restaurant process

723
00:50:53,080 --> 00:50:57,820
and the stick breaking construction and  the reason for that is that the reason

724
00:50:57,820 --> 00:51:02,840
I can't people always talk about the different representations of the Dirichlet processes is because it

725
00:51:02,840 --> 00:51:08,600
turns out that the different representations tend to lead to different algorithms with for doing

726
00:51:08,600 --> 00:51:16,540
inference in the Dirichlet process with different properties okay so what is a Dirichlet process mixture

727
00:51:16,540 --> 00:51:20,940
model it's a  defined in in the following way so as I said before G

728
00:51:20,960 --> 00:51:26,140
is gonna be D P distributed and then theta I is gonna be are

729
00:51:26,140 --> 00:51:31,550
gonna be IID draws from G and basically we can think of theta I as

730
00:51:31,550 --> 00:51:39,960
the parameter of the cluster that data item I belongs to and so if theta I is the

731
00:51:39,960 --> 00:51:46,060
parameter then data item I will be generated from a distribution parameterized by theta I okay

732
00:51:46,380 --> 00:51:52,800
so this gives us our mixture model which is a DP mixture model so now

733
00:51:52,800 --> 00:51:59,140
we can look at two things one one way is are the CRP and one way is the

734
00:51:59,140 --> 00:52:06,680
stick breaking construction in the case of the CRP basically we say that

735
00:52:06,700 --> 00:52:12,000
we know that the thetas are gonna be clustered according to the distinct values so we

736
00:52:12,000 --> 00:52:16,260
can integrate our G and simply look at the partitioning of the clust of the

737
00:52:16,260 --> 00:52:21,390
data items induced by the clustering of the thetas and that is described by a

738
00:52:21,390 --> 00:52:30,520
Chinese restaurant process so our partitioning is gonna have a prior given by the CRP

739
00:52:30,520 --> 00:52:38,420
for each cluster in our partition we'll have a prior distribution given by H and

740
00:52:38,420 --> 00:52:46,860
then every data item XI is gonna be drawn from distribution parameterized by theta

741
00:52:46,860 --> 00:52:54,480
star C where C is the unit cluster that data item I belongs to in the partition row

742
00:52:54,480 --> 00:52:58,940
so here's a graphical model the nice thing with the CRP representation is that

743
00:52:58,940 --> 00:53:04,020
it makes explicit that this is a clustering model is a model for clustering the

744
00:53:04,020 --> 00:53:10,080
data items into  different clusters okay and also another nice thing is that

745
00:53:10,080 --> 00:53:15,620
using the CRP  prior for row kind of obvious that need to limit the number of

746
00:53:15,620 --> 00:53:24,880
clusters to a certain fixed number like K or something so this kind of leads us to what

747
00:53:24,900 --> 00:53:33,010
are called marginal Markov chain Monte Carlo sampling is marginal in the sense that

748
00:53:33,010 --> 00:53:39,540
we basically marginalize out G and we work only with the random partition that's induced on

749
00:53:39,540 --> 00:53:46,240
the dataset and basically the the infinite limit of the Gibbs sampler that we

750
00:53:46,240 --> 00:53:54,720
just derived earlier is the  is the Gibbs sampler for is this marginal MCMC

751
00:53:54,720 --> 00:54:07,410
sampler right where basically let me see yeah so the probability that data item I is gonna

752
00:54:07,410 --> 00:54:15,320
belong to this cluster give in called row I is gonna be proportional to

753
00:54:15,320 --> 00:54:22,920
this will be proportional to a conditional prior of over clusters that data I

754
00:54:22,930 --> 00:54:28,400
can belong to and the conditional likelihood of data item XI given that it

755
00:54:28,400 --> 00:54:34,360
belongs to cluster row I and this is kind of as we just derived it

756
00:54:34,400 --> 00:54:40,240
as in the infinite limit of the finite mixture model case the tricky bit of this

757
00:54:40,240 --> 00:54:44,960
this marginal samplers is in terms of dealing with new clusters

758
00:54:44,980 --> 00:54:51,500
when we actually have to introduce new clusters and so there's this

759
00:54:51,500 --> 00:54:59,020
nice paper by Radford Neal in about two thousand kind that kind of surveyed a number of marginal samplers

760
00:54:59,020 --> 00:55:08,860
for this so yes sorry row is basically the way customers are assigned

761
00:55:08,860 --> 00:55:17,860
to tables however this the whole seating arrangement of the of the restaurants

762
00:55:17,860 --> 00:55:22,410
here's what the result looks like you have to do this on these six demonstrations we just looked at

763
00:55:23,820 --> 00:55:28,060
so we have a new helicopter in picture the white one that's the one that's been recovered

764
00:55:28,520 --> 00:55:32,090
trajectory that we think about any intent on the pile what he was trying to fly

765
00:55:32,850 --> 00:55:34,040
flying is nose trajectory

766
00:55:37,220 --> 00:55:39,490
butcher but this white trajectory you'll see is

767
00:55:40,110 --> 00:55:41,650
a cleaner version of the other ones

768
00:55:43,640 --> 00:55:47,500
we also see is that all the time line so a replying things here

769
00:55:47,990 --> 00:55:50,470
if he timeline version of these over the trajectories

770
00:55:52,340 --> 00:55:53,350
look is a little more

771
00:55:57,310 --> 00:55:57,710
look cut

772
00:55:58,320 --> 00:55:59,260
other demonstrations

773
00:56:00,070 --> 00:56:01,290
this is double loop sequence

774
00:56:02,630 --> 00:56:02,800
we have

775
00:56:03,210 --> 00:56:04,450
three demonstration shown here

776
00:56:05,040 --> 00:56:06,030
and this is what you get out

777
00:56:08,110 --> 00:56:10,250
bring this process the white out the trajectory

778
00:56:11,250 --> 00:56:14,100
so you get a much cleaner circles than any other demonstrations

779
00:56:14,600 --> 00:56:16,760
the king in the purple one was likely

780
00:56:17,170 --> 00:56:23,220
due to imperfect sensory measurements it's alive the helicopter actually flew acting but all those things get california's noise

781
00:56:23,670 --> 00:56:23,930
and get

782
00:56:24,350 --> 00:56:25,880
smoothed out in this denoising step

783
00:56:26,930 --> 00:56:29,880
the black one is now a retired judge if a w

784
00:56:31,980 --> 00:56:34,690
the next step to be able to control is defined dynamics model

785
00:56:35,710 --> 00:56:38,840
to get standard helicopter dynamics of the might look something like this

786
00:56:39,220 --> 00:56:39,550
these are

787
00:56:39,970 --> 00:56:42,270
rigid body models for the helicopter so

788
00:56:43,200 --> 00:56:45,210
catching everything about the aerodynamics

789
00:56:45,670 --> 00:56:47,000
in two forces then

790
00:56:47,430 --> 00:56:48,500
result on the helicopter

791
00:56:50,490 --> 00:56:51,930
may not be familiar with the notation

792
00:56:52,510 --> 00:56:54,760
but i think look at here is the capital sees

793
00:56:55,400 --> 00:56:57,800
the capital sees are the free parameters in this model

794
00:56:58,450 --> 00:57:00,600
everything else you gotta get from data collection

795
00:57:01,910 --> 00:57:05,130
look at the capital see are factors they appear linearly in these equations

796
00:57:05,720 --> 00:57:08,750
so you can run in linear regression defined those from there

797
00:57:09,740 --> 00:57:13,680
and then if you didn't you collected data here you dot is the derivative of

798
00:57:13,690 --> 00:57:15,610
the forward velocity vi don't do it

799
00:57:16,200 --> 00:57:18,910
sideways lost in w the derivative on the vertical velocity

800
00:57:19,760 --> 00:57:22,640
feature are angular rates the dollar version of the derivatives

801
00:57:23,770 --> 00:57:25,670
collected data from a linear regression

802
00:57:26,590 --> 00:57:29,250
and then design a controller based on the simulation model

803
00:57:30,150 --> 00:57:31,200
you can have a helicopter

804
00:57:32,890 --> 00:57:36,130
but if you try to do the same thing for his dynamic maneuvers doesn't work

805
00:57:37,210 --> 00:57:40,900
let's look a little more careful about how could model you get from this dynamic maneuvers

806
00:57:42,240 --> 00:57:43,500
here the horizontal axis

807
00:57:43,910 --> 00:57:44,400
it's time

808
00:57:45,320 --> 00:57:46,160
very boxes

809
00:57:46,690 --> 00:57:50,150
these are the model makes in predicting acceleration of the helicopter

810
00:57:51,990 --> 00:57:53,220
u have your data

811
00:57:54,240 --> 00:57:58,130
based on their at the current time you predict the acceleration helicopter experience

812
00:57:58,730 --> 00:58:02,930
they need do look at the data and then at the next time so that the actual acceleration was

813
00:58:03,320 --> 00:58:04,030
you make a difference

814
00:58:06,670 --> 00:58:08,160
is in meters per second squared

815
00:58:09,980 --> 00:58:13,580
gravity is about ten meters per second squared we see here they you get up to

816
00:58:14,170 --> 00:58:14,910
three cheese

817
00:58:15,410 --> 00:58:17,900
of error in predicting actual relation helicopter

818
00:58:18,370 --> 00:58:21,620
that's huge right that's being three times gravity in

819
00:58:22,040 --> 00:58:25,360
anticipating what's going to happen makes it really hard to control helicopter

820
00:58:26,040 --> 00:58:28,710
fact makes it impossible to do based on the mall five at

821
00:58:30,780 --> 00:58:31,730
if you look more carefully

822
00:58:32,270 --> 00:58:34,090
at all these demonstration trajectories

823
00:58:36,170 --> 00:58:37,080
here's what they look like

824
00:58:40,250 --> 00:58:42,890
same thing actually regime error as a function of time

825
00:58:43,280 --> 00:58:44,390
and here's what they look like

826
00:58:44,970 --> 00:58:46,830
after running the algorithm that extracts the

827
00:58:47,540 --> 00:58:49,870
intent the pilot which this time alignment

828
00:58:51,940 --> 00:58:53,670
so we see here is three j air

829
00:58:54,390 --> 00:58:55,530
yes it's an area in the model

830
00:58:56,080 --> 00:58:59,250
but is very consistent it's not noise it's not stochasticity

831
00:58:59,730 --> 00:59:03,040
in the down the helicopter it's something and if you do the same thing over

832
00:59:03,040 --> 00:59:04,920
and over and over you got over and over

833
00:59:05,380 --> 00:59:07,900
get this additional three g's pushing on the helicopter

834
00:59:08,850 --> 00:59:10,080
and you can anticipate this

835
00:59:10,710 --> 00:59:11,380
so we should

836
00:59:13,500 --> 00:59:16,880
so the key observation here the white helicopter dynamics is really complicated

837
00:59:18,540 --> 00:59:19,190
it turns out

838
00:59:20,210 --> 00:59:21,450
well not deterministic

839
00:59:22,510 --> 00:59:24,010
it is fairly repeatable

840
00:59:24,740 --> 00:59:29,460
and that's probably why how white house can learn the fly helicopter so well after a lot training

841
00:59:30,810 --> 00:59:33,050
recovered the patterns the helicopter dynamics

842
00:59:33,490 --> 00:59:37,360
you know how it works is not a big surprise them is just very complicated

843
00:59:37,360 --> 00:59:39,740
but after a while the muscle memory has built up

844
00:59:40,280 --> 00:59:42,730
the recognition of what will happen and be able to anticipate

845
00:59:43,650 --> 00:59:45,010
so we wanted do is somehow

846
00:59:46,590 --> 00:59:47,290
this variability

847
00:59:49,910 --> 00:59:51,660
what models things like air flow

848
00:59:52,210 --> 00:59:56,020
around the helicopter that's part the true state but it's not in our state space model

849
00:59:56,520 --> 00:59:57,430
actuator delays

850
00:59:58,650 --> 00:59:59,550
flapping of the blades

851
01:00:01,220 --> 01:00:03,250
so it does say given the repeatability

852
01:00:03,750 --> 01:00:08,500
we're going to still stay away from modeling flow is very expensive for similar flow

853
01:00:09,420 --> 01:00:10,250
and we want to learn

854
01:00:10,990 --> 01:00:14,230
laws that are specific to the trajectory we're trying to execute

855
01:00:15,150 --> 01:00:18,880
and the idea here is rather than running a standard linear regression defined e vectors

856
01:00:19,680 --> 01:00:22,060
the weighted linear regression so you get the data

857
01:00:22,620 --> 01:00:24,690
for a particular slice of trajectory

858
01:00:25,830 --> 01:00:32,080
and weighted data closest to current time most highly and there further away from the current time less

859
01:00:33,220 --> 01:00:37,740
what happens then you want a model is fine tuned to we're currently in this trajectory

860
01:00:39,180 --> 01:00:42,310
if you look at the model capture the entire pattern that we saw

861
01:00:42,860 --> 01:00:46,310
was captured by the global model that was trying to one one set of coefficients

862
01:00:47,020 --> 01:00:49,620
you will learn was very specific rejected trying to fly

863
01:00:50,420 --> 01:00:51,070
and that's

864
01:00:51,620 --> 01:00:54,010
this i think they do it this way you don't want the fly

865
01:00:54,690 --> 01:00:56,580
just arbitrary things you learn fly

866
01:00:57,290 --> 01:00:59,220
the your has been demonstrated to you

867
01:01:01,160 --> 01:01:03,000
let's take a look at how well this works

868
01:01:04,370 --> 01:01:09,280
experimental setup we had is one where we have cameras on the ground looking up at the helicopter

869
01:01:09,870 --> 01:01:11,070
the last track position

870
01:01:11,840 --> 01:01:14,750
our pilot can track orientation and position with his eyes

871
01:01:15,980 --> 01:01:18,120
cameras we have visual process and we have

872
01:01:18,120 --> 01:01:23,630
one zero

873
01:01:23,690 --> 01:01:28,200
look this one is standing still

874
01:01:28,280 --> 01:01:35,400
but this one is standing still

875
01:01:35,410 --> 01:01:36,380
that's been

876
01:01:36,380 --> 01:01:40,200
you see that the energy is transferred from one to the other

877
01:01:40,260 --> 01:01:42,080
and that is a

878
01:01:42,090 --> 01:01:45,060
the phenomenon that follows immediately from this

879
01:01:45,070 --> 01:01:46,590
what would happen

880
01:01:46,600 --> 01:01:49,540
if i moved the

881
01:01:49,590 --> 01:01:52,770
spring up suppose i moved spring

882
01:01:55,290 --> 01:01:57,800
higher halfway

883
01:01:57,880 --> 01:01:59,910
anyone without

884
01:01:59,930 --> 01:02:02,790
looking towards the blackboard

885
01:02:02,860 --> 01:02:05,970
but you sure intuition

886
01:02:06,010 --> 01:02:07,900
what would happen

887
01:02:07,910 --> 01:02:14,160
with the same phenomenon happened

888
01:02:16,670 --> 01:02:20,360
this been one

889
01:02:20,380 --> 01:02:24,130
so what would say phenomena happening

890
01:02:24,960 --> 01:02:26,260
but what is the

891
01:02:26,310 --> 01:02:28,870
so it

892
01:02:29,210 --> 01:02:36,400
now let's look at the blackboard

893
01:02:36,450 --> 01:02:37,530
i really think

894
01:02:38,590 --> 01:02:40,100
the certain

895
01:02:40,150 --> 01:02:44,690
separation right you make that separation small you make the effect of the spring small

896
01:02:45,040 --> 01:02:48,450
so the only good mines becomes even closer to the omega plus

897
01:02:48,520 --> 01:02:50,370
so to be period will increase

898
01:02:50,390 --> 01:02:51,610
take longer

899
01:02:52,630 --> 01:02:57,550
i want to stop let's find that

900
01:02:57,610 --> 01:03:03,790
so we're going to move this up

901
01:03:03,800 --> 01:03:05,140
i put it roughly

902
01:03:11,740 --> 01:03:13,680
OK ready for this

903
01:03:13,700 --> 01:03:16,020
so we doing it again

904
01:03:16,060 --> 01:03:17,910
and then you will see the same phenomena

905
01:03:17,930 --> 01:03:20,020
accept it will take longer

906
01:03:20,020 --> 01:03:23,550
for the first one to come to

907
01:03:23,560 --> 01:03:24,330
so i have

908
01:03:24,380 --> 01:03:37,080
decrease to cooperate

909
01:03:37,130 --> 01:03:42,520
it's still swinging happily

910
01:03:42,520 --> 01:03:45,370
is beginning to change its mind

911
01:03:46,970 --> 01:03:48,690
now standing still

912
01:03:48,700 --> 01:03:50,330
the longer

913
01:03:54,210 --> 01:04:00,130
now to something mind-boggling something that you really want to see

914
01:04:00,140 --> 01:04:05,960
suppose i bring this all the way to the top

915
01:04:06,050 --> 01:04:08,840
that's an interesting because then omega minus

916
01:04:08,890 --> 01:04:12,310
is exactly the same as only got plus

917
01:04:12,320 --> 01:04:16,100
because look this term goes away

918
01:04:20,650 --> 01:04:22,130
it's thomas s

919
01:04:22,140 --> 01:04:24,650
cutting out the spring there's no spring anymore

920
01:04:24,790 --> 01:04:27,560
will not happen if i start one year

921
01:04:27,600 --> 01:04:34,620
three two one zero

922
01:04:36,410 --> 01:04:43,090
one will string and the other ones

923
01:04:43,120 --> 01:04:45,210
and that's shocking

924
01:04:45,230 --> 01:04:47,840
it's shocking you have two pendulums

925
01:04:47,880 --> 01:04:49,860
and they're not even connected anymore

926
01:04:49,880 --> 01:04:51,880
you start this one straight

927
01:04:51,890 --> 01:04:56,040
and the other one will never start to sway is an amazing four thirty thousand

928
01:04:56,040 --> 01:04:56,740
dollars duration

929
01:04:57,180 --> 01:05:00,390
something fantastic

930
01:05:00,460 --> 01:05:03,360
except that the beta

931
01:05:03,400 --> 01:05:05,610
it is infinitely more

932
01:05:06,620 --> 01:05:08,680
the patient

933
01:05:08,700 --> 01:05:12,260
so let's the for

934
01:05:12,290 --> 01:05:15,340
so let's demonstrate

935
01:05:15,380 --> 01:05:20,340
so there we go

936
01:05:23,240 --> 01:05:24,610
six work

937
01:05:24,620 --> 01:05:29,220
look at this equation

938
01:05:29,270 --> 01:05:31,880
when only minus is omega plus

939
01:05:31,900 --> 01:05:35,390
this story is always zero

940
01:05:35,400 --> 01:05:39,900
so you see x two remain zero producing

941
01:05:39,910 --> 01:05:45,330
and in omega minus is omega plus this cosine term is always plus one

942
01:05:45,330 --> 01:05:48,770
hahaha that's what you see

943
01:05:48,820 --> 01:05:55,470
isn't it amazing the power physics

944
01:05:55,530 --> 01:05:58,370
so what is truly remarkable

945
01:05:58,380 --> 01:06:01,680
that we can describe

946
01:06:01,690 --> 01:06:04,300
for any initial conditions

947
01:06:04,340 --> 01:06:06,060
the motion in terms of

948
01:06:06,070 --> 01:06:08,470
the linear superposition

949
01:06:08,510 --> 01:06:11,150
of the two normal modes

950
01:06:11,190 --> 01:06:14,610
so what region they look like an impossibility

951
01:06:14,610 --> 01:06:16,130
when i started

952
01:06:16,210 --> 01:06:19,620
the first thirty seconds of my lecture was that demonstration

953
01:06:19,620 --> 01:06:22,060
under these conditions so we

954
01:06:22,120 --> 01:06:25,940
it gets left out so this is something that that you'll see

955
01:06:26,650 --> 01:06:30,290
many times

956
01:06:30,320 --> 01:06:31,460
all right so

957
01:06:31,470 --> 01:06:34,790
w with the equilibrium constant

958
01:06:34,800 --> 01:06:39,210
and it's the product of hydro near my and times hydroxide ion

959
01:06:39,290 --> 01:06:43,510
it's always going to be one point zero times ten to the minus fourteen

960
01:06:43,520 --> 01:06:47,550
at room temperature

961
01:06:52,030 --> 01:06:53,020
we're going to

962
01:06:53,040 --> 01:06:55,090
look at

963
01:06:55,090 --> 01:06:58,350
definitions of ph and p o h

964
01:06:58,370 --> 01:07:01,830
and come back to this KW

965
01:07:01,880 --> 01:07:02,650
think of

966
01:07:02,760 --> 01:07:06,760
that person here

967
01:07:06,770 --> 01:07:15,050
so ph

968
01:07:15,070 --> 01:07:18,620
PH equals minus log

969
01:07:18,640 --> 01:07:21,300
of hydrogen ion

970
01:07:25,350 --> 01:07:27,850
o h

971
01:07:27,860 --> 01:07:31,190
he o h equals minus log

972
01:07:31,530 --> 01:07:35,720
the hydroxide ion concentration

973
01:07:35,770 --> 01:07:38,660
we just saw that w

974
01:07:40,590 --> 01:07:44,420
the hydrogen ion concentration

975
01:07:44,430 --> 01:07:48,300
times the hydroxide ion concentration

976
01:07:48,340 --> 01:07:49,230
so now

977
01:07:49,270 --> 01:07:51,970
if we take this expression

978
01:07:51,980 --> 01:07:58,640
and we put logs on both sides and minus signs on both sides

979
01:07:58,680 --> 01:08:01,070
we can come up with the minus log

980
01:08:01,170 --> 01:08:02,850
the w

981
01:08:02,860 --> 01:08:05,710
minus log

982
01:08:05,730 --> 01:08:09,400
hydrogen ion concentration

983
01:08:12,990 --> 01:08:15,580
ion concentration

984
01:08:16,660 --> 01:08:18,750
i find that p

985
01:08:25,160 --> 01:08:28,460
he o h

986
01:08:28,520 --> 01:08:30,480
and that can equal

987
01:08:32,000 --> 01:08:34,510
o point zero zero

988
01:08:35,020 --> 01:08:38,390
room temperature

989
01:08:38,430 --> 01:08:40,980
so you see there is a relationship

990
01:08:42,220 --> 01:08:47,880
pkw ph np o wage this derived from this equation

991
01:08:47,880 --> 01:08:53,400
from k w b equal to the hydroxide times the height

992
01:08:53,440 --> 01:08:55,350
ion concentrations

993
01:08:55,360 --> 01:08:59,920
these are all useful things to remember in doing the problems because if you're if

994
01:08:59,920 --> 01:09:02,480
you're given appear waking ask for appear

995
01:09:03,430 --> 01:09:10,400
room temperature you can use those equations to enter confer

996
01:09:10,410 --> 01:09:15,040
it also turns out to be important in terms of thinking about the strength

997
01:09:15,090 --> 01:09:20,990
of acids and bases

998
01:09:22,090 --> 01:09:23,880
let's look at

999
01:09:23,890 --> 01:09:26,420
the strength of acids and bases

1000
01:09:26,470 --> 01:09:29,660
in terms of phd relationship between ph

1001
01:09:29,690 --> 01:09:32,480
and as it some basis

1002
01:09:32,500 --> 01:09:34,870
so the ph of pure water

1003
01:09:34,900 --> 01:09:36,940
should be seven

1004
01:09:36,960 --> 01:09:40,520
and that's neutral seven neutral ph

1005
01:09:40,600 --> 01:09:45,830
and as the solution is anything less than a ph of seven

1006
01:09:45,870 --> 01:09:48,580
a basic solution is anything greater

1007
01:09:50,450 --> 01:09:52,050
and the EPA

1008
01:09:52,060 --> 01:09:54,040
defines corrosive

1009
01:09:54,050 --> 01:09:57,930
substances that yield ph is lower than three

1010
01:09:57,940 --> 01:10:04,200
or higher than twelve point five

1011
01:10:04,200 --> 01:10:10,020
represented by the DSM which is the matrix of the signal box derivative

1012
01:10:10,050 --> 01:10:17,930
should i really don't what the has them the hessian matrix is

1013
01:10:24,350 --> 01:10:27,020
thirty is symmetric

1014
01:10:27,090 --> 01:10:30,510
in the way the

1015
01:11:06,200 --> 01:11:08,730
sense of vectors x one

1016
01:11:08,740 --> 01:11:11,970
it is the

1017
01:11:11,990 --> 01:11:15,860
two value in r

1018
01:11:16,890 --> 01:11:20,080
the gradient of s

1019
01:11:20,140 --> 01:11:23,350
in the point x

1020
01:11:23,380 --> 01:11:30,230
is the vector that is made out of the partial derivative of x

1021
01:11:30,240 --> 01:11:33,400
of f

1022
01:11:33,440 --> 01:11:35,550
at point x

1023
01:11:37,960 --> 01:11:40,260
which means that

1024
01:11:40,300 --> 01:11:43,440
the x two

1025
01:11:45,350 --> 01:11:52,480
this vector has the coordinates one for each of the valuables

1026
01:11:52,490 --> 01:11:54,310
x one two x p

1027
01:11:56,050 --> 01:11:59,340
and this notation means that you consider

1028
01:11:59,350 --> 01:12:04,970
as for as a function of only one the first of all ball x one

1029
01:12:04,980 --> 01:12:07,820
you do you consider the function g

1030
01:12:07,830 --> 01:12:13,340
you fix x two

1031
01:12:13,360 --> 01:12:14,780
two eggs the

1032
01:12:14,790 --> 01:12:17,460
you could consider the function g that

1033
01:12:17,510 --> 01:12:25,490
to real number associates f x with x two and x the fixed

1034
01:12:25,520 --> 01:12:29,140
and that's a function of one variable that you can differentiate

1035
01:12:30,010 --> 01:12:35,680
and the partial derivative is the derivative of of this one dimensional function

1036
01:12:35,790 --> 01:12:38,780
now ninety is symmetric

1037
01:12:38,800 --> 01:12:42,970
if you're going to differentiate twice then you may

1038
01:12:43,030 --> 01:12:45,950
i want to differentiate twice

1039
01:12:46,010 --> 01:12:48,660
with the against the the same

1040
01:12:48,670 --> 01:12:53,230
the same valuable x one or you may want to differentiate and once with a

1041
01:12:53,350 --> 01:12:57,680
according to x one and the second time according to x two right so you

1042
01:12:57,680 --> 01:13:01,720
get you for which is the matrix of all these fulfilled

1043
01:13:02,260 --> 01:13:04,690
second order derivatives

1044
01:13:04,700 --> 01:13:08,080
think of that

1045
01:13:17,060 --> 01:13:19,230
so it's the matrix

1046
01:13:20,160 --> 01:13:21,640
we've done the end

1047
01:13:21,650 --> 01:13:24,540
they are going on the

1048
01:13:24,560 --> 01:13:29,940
the function that is twice differentiated into valuable

1049
01:13:39,840 --> 01:13:46,230
and you see take an

1050
01:13:46,240 --> 01:13:50,290
no rule i and column j

1051
01:13:50,310 --> 01:13:59,630
then you differentiate once according to x i and the second term according to x

1052
01:14:00,690 --> 01:14:02,760
and so this matrix

1053
01:14:02,770 --> 01:14:05,690
as nice

1054
01:14:05,700 --> 01:14:08,440
as an easy form is actually symmetric

1055
01:14:08,450 --> 01:14:12,320
you can verify you can actually see that you can

1056
01:14:12,330 --> 01:14:15,060
it's actually true that tissue differentiate from

1057
01:14:15,070 --> 01:14:19,050
with respect to x y and then existed then it's the same thing is that

1058
01:14:19,050 --> 01:14:24,640
it isn't as if you differentiate with respect to a then x i

1059
01:14:24,680 --> 01:14:27,840
so this matrix is a symmetric matrix

1060
01:14:28,390 --> 01:14:35,300
and it happens that if you have a local minimizer

1061
01:14:35,310 --> 01:14:38,310
then this matrix is semidefinite was

1062
01:14:41,770 --> 01:14:47,270
alright what happens is common for convex functions

1063
01:14:47,320 --> 01:14:49,820
so i figured that

1064
01:14:54,970 --> 01:14:59,240
so let before i go back to to convex functions here are the questions that

1065
01:14:59,240 --> 01:15:01,020
i haven't mentioned so far

1066
01:15:01,030 --> 01:15:08,170
one of you want to find the global minimizer but you want to rex restrict

1067
01:15:08,210 --> 01:15:11,710
the constraints x two living in

1068
01:15:11,730 --> 01:15:13,720
in an interview for example q

1069
01:15:13,730 --> 01:15:18,410
so if you have a construct of constrained optimisation problem how would you know that

1070
01:15:18,410 --> 01:15:21,660
whereas with very little additional wisdom

1071
01:15:21,670 --> 01:15:25,190
most mutations could be at least plausible

1072
01:15:26,970 --> 01:15:31,720
you could reduce the number of fatalities by factors of OCT aliens

1073
01:15:31,730 --> 01:15:33,930
by just a few heuristics

1074
01:15:33,950 --> 01:15:37,280
so it's so easy to improve evolution is not funny

1075
01:15:39,020 --> 01:15:41,990
of skip this one

1076
01:15:42,180 --> 01:15:46,910
but you know i was generally the trouble with evolution

1077
01:15:46,970 --> 01:15:50,290
is the genes operate on various levels

1078
01:15:51,100 --> 01:15:57,140
very few genes actually control particular details of particular structures

1079
01:15:57,180 --> 01:16:01,810
which means that in order to get an improvement in particular structure

1080
01:16:01,810 --> 01:16:03,330
you have to wait for

1081
01:16:04,600 --> 01:16:06,790
a series of coincidences

1082
01:16:06,810 --> 01:16:08,350
they give

1083
01:16:08,370 --> 01:16:11,240
which would never get rewarded all

1084
01:16:11,280 --> 01:16:16,260
unless they happen to have side effects that also promoted survival for

1085
01:16:16,580 --> 01:16:18,450
entirely different reasons

1086
01:16:18,470 --> 01:16:22,200
but the worst part of evolution is that it only remembers

1087
01:16:22,240 --> 01:16:23,660
what succeeded

1088
01:16:23,720 --> 01:16:25,990
and it doesn't keep records

1089
01:16:26,010 --> 01:16:29,950
so can only learn to avoid

1090
01:16:30,830 --> 01:16:32,620
five or ten thousand

1091
01:16:32,640 --> 01:16:34,830
serious mistakes

1092
01:16:34,870 --> 01:16:36,950
it has no way to remember

1093
01:16:37,020 --> 01:16:38,930
these genomes are size

1094
01:16:38,950 --> 01:16:43,470
we have no way to remember a million less common mistakes

1095
01:16:43,490 --> 01:16:48,910
however every culture teaches children very large numbers of common mistakes and that's why

1096
01:16:49,390 --> 01:16:51,160
so many

1097
01:16:51,220 --> 01:16:53,640
fairy tales are popular

1098
01:16:53,700 --> 01:16:56,600
o thing the stories we tell their children

1099
01:16:56,660 --> 01:17:01,060
are generally about things that you shouldn't do because

1100
01:17:01,350 --> 01:17:03,450
it will kill you

1101
01:17:03,470 --> 01:17:07,780
there's a psychologist named neuron

1102
01:17:07,870 --> 01:17:09,720
o believes that

1103
01:17:09,760 --> 01:17:13,950
so many cultures have the same legends that they must be

1104
01:17:15,240 --> 01:17:16,930
inherited or

1105
01:17:17,180 --> 01:17:21,560
placed in some mysterious racial unconscious

1106
01:17:21,560 --> 01:17:25,010
but the fact is that if you look at the legends that is

1107
01:17:25,020 --> 01:17:26,680
talking about

1108
01:17:27,040 --> 01:17:29,490
they involve

1109
01:17:29,510 --> 01:17:32,100
little stories about social

1110
01:17:32,120 --> 01:17:35,510
wonders that are liable to get you killed

1111
01:17:36,330 --> 01:17:39,890
every one of these popular legends

1112
01:17:40,040 --> 01:17:44,040
represent stories that are going to happen in any culture that has a lot of

1113
01:17:44,040 --> 01:17:46,290
people crowded together

1114
01:17:47,370 --> 01:17:50,430
we'll get inherited not as genes but as

1115
01:17:50,430 --> 01:17:51,930
means as

1116
01:17:51,950 --> 01:17:54,520
propagated beliefs in that structure

1117
01:17:54,580 --> 01:17:57,260
so the popularity of the on

1118
01:17:57,260 --> 01:17:59,120
it is very strange

1119
01:17:59,120 --> 01:18:02,140
because he doesn't have a plausible theory

1120
01:18:02,180 --> 01:18:04,520
of this very real phenomenon

1121
01:18:05,490 --> 01:18:09,640
the point is that evolution cannot remember a large number of

1122
01:18:09,640 --> 01:18:12,930
of slightly useful facts those will die out

1123
01:18:14,490 --> 01:18:18,510
and you'll only remember the big ones

1124
01:18:18,740 --> 01:18:24,580
another problem is this

1125
01:18:24,640 --> 01:18:26,850
when somebody builds a

1126
01:18:26,850 --> 01:18:28,660
computer program

1127
01:18:28,700 --> 01:18:32,890
that's going to manipulate some kind of data such as common sense

1128
01:18:32,970 --> 01:18:35,410
knowledge of a certain form

1129
01:18:35,450 --> 01:18:37,640
one of the first things you do is say

1130
01:18:37,640 --> 01:18:40,350
how am i going to represent that knowledge

1131
01:18:40,350 --> 01:18:46,470
should represented as sentences in natural language with syntactic parsing tree

1132
01:18:46,520 --> 01:18:50,430
sure i recognise it as a relational database

1133
01:18:51,450 --> 01:18:57,350
should i start the information as numerical coefficients in a neural network

1134
01:18:57,370 --> 01:19:02,470
or statistical probabilities in a hidden markov model or whatever

1135
01:19:03,260 --> 01:19:06,780
i used to hear students arguing about this

1136
01:19:06,790 --> 01:19:09,910
and the typical conversation would be

1137
01:19:09,910 --> 01:19:12,390
the best thing is to use logic because

1138
01:19:12,410 --> 01:19:17,430
logic is perfectly unambiguous and the rules of inference are perfectly clear

1139
01:19:17,450 --> 01:19:21,950
and it never makes any mistakes

1140
01:19:22,290 --> 01:19:23,810
and then somebody else

1141
01:19:23,850 --> 01:19:25,240
i will say yes but

1142
01:19:25,260 --> 01:19:28,560
you can represent most ordinary things in

1143
01:19:28,600 --> 01:19:30,410
predicate calculus

1144
01:19:30,430 --> 01:19:31,970
why not

1145
01:19:32,010 --> 01:19:34,200
what is predicate calculus

1146
01:19:34,220 --> 01:19:37,490
it's a language that has only three adjectives

1147
01:19:37,510 --> 01:19:41,370
sometimes always and never

1148
01:19:41,390 --> 01:19:43,830
you can say sort of or

1149
01:19:43,850 --> 01:19:46,080
this is similar to that

1150
01:19:46,100 --> 01:19:49,700
and nobody has ever successfully

1151
01:19:49,700 --> 01:19:53,890
done any kind of significant reasoning by analogy

1152
01:19:53,910 --> 01:19:56,720
using formal logic

1153
01:19:56,740 --> 01:20:00,330
so it's good for certain kinds of mathematics and other things

1154
01:20:00,350 --> 01:20:03,540
jolly good for certain kinds of business procedures

1155
01:20:03,580 --> 01:20:07,830
but there is no good for common sense thinking just too rigid

1156
01:20:10,660 --> 01:20:13,490
so we said why not use neural networks

1157
01:20:13,510 --> 01:20:16,350
and this would be my complaint

1158
01:20:16,370 --> 01:20:19,740
neural networks are actually more rigid than logic

1159
01:20:19,780 --> 01:20:22,760
because they don't have any adjectives at all

1160
01:20:22,790 --> 01:20:25,510
everything is represent or rather one

1161
01:20:25,540 --> 01:20:28,450
everything is represented as

1162
01:20:28,510 --> 01:20:30,410
as the number

1163
01:20:30,410 --> 01:20:32,550
with respect to minds

1164
01:20:32,640 --> 01:20:34,810
so here is where the root cars

1165
01:20:34,870 --> 01:20:37,350
definition comes into the picture of the fine

1166
01:20:37,360 --> 01:20:38,520
this guy here

1167
01:20:38,540 --> 01:20:40,750
this going to cause

1168
01:20:40,770 --> 01:20:42,020
so intuitively

1169
01:20:42,040 --> 01:20:44,010
what we are doing is just too

1170
01:20:44,360 --> 01:20:45,490
to six to see

1171
01:20:45,930 --> 01:20:51,960
are how similar is i on the average with respect to all other

1172
01:20:52,020 --> 01:20:54,310
elements in

1173
01:20:54,360 --> 01:20:56,160
except years

1174
01:20:57,040 --> 01:20:59,920
we are waiting in the sound with the red wings

1175
01:21:00,020 --> 01:21:03,140
OK once we have numbers then we can define

1176
01:21:03,240 --> 01:21:08,370
the total sum we can define the total weight of this of this set by

1177
01:21:08,370 --> 01:21:11,680
just some in all the contribution

1178
01:21:11,700 --> 01:21:14,610
now let me give you a very intuitive

1179
01:21:14,770 --> 01:21:17,710
interpretation of this number

1180
01:21:19,150 --> 01:21:23,410
it turns out that the sign of the site's importance

1181
01:21:23,420 --> 01:21:26,440
before defining the notion of dominance

1182
01:21:26,490 --> 01:21:30,110
let's try and see what happens in this two simple cases

1183
01:21:30,120 --> 01:21:32,190
consider this graph here

1184
01:21:32,210 --> 01:21:35,850
we have this three that is two three and four

1185
01:21:35,860 --> 01:21:41,840
they are highly similar to each other in fact the similarities is constant

1186
01:21:41,890 --> 01:21:43,810
what happens if i try

1187
01:21:44,940 --> 01:21:46,350
this here

1188
01:21:46,360 --> 01:21:48,640
this set to two three four

1189
01:21:48,690 --> 01:21:53,460
well intuitively i expect that the overall similarity this new set

1190
01:21:53,560 --> 01:21:58,250
sixteen one two three four will decrease because imagine something to the set which is

1191
01:21:58,250 --> 01:21:59,490
less similar

1192
01:21:59,540 --> 01:22:02,150
with respect to the intelligence

1193
01:22:02,190 --> 01:22:04,490
so this is reflected by the fact that

1194
01:22:04,530 --> 01:22:05,980
w one

1195
01:22:06,000 --> 01:22:09,740
with respect to one two three four is less than c

1196
01:22:10,540 --> 01:22:14,150
if i compare this number here and this number is less than zero what i

1197
01:22:14,150 --> 01:22:15,150
can conclude

1198
01:22:15,150 --> 01:22:19,550
this this is the basic intuition is that if i try and

1199
01:22:19,670 --> 01:22:21,640
one to the set

1200
01:22:21,660 --> 01:22:27,220
then i will decrease the internal similar to of the set

1201
01:22:27,600 --> 01:22:32,110
let's consider this other examples here we have six seven eight

1202
01:22:32,150 --> 01:22:36,780
they are highly scene they are seen which i can see higher of course because

1203
01:22:36,780 --> 01:22:38,950
there is another verdicts

1204
01:22:39,020 --> 01:22:40,870
five which is more similar

1205
01:22:40,870 --> 01:22:43,370
but if we just look at six seven eight

1206
01:22:43,400 --> 01:22:44,770
four mcclure coherent group

1207
01:22:44,870 --> 01:22:46,370
police at that scale

1208
01:22:46,380 --> 01:22:50,450
so what happens if i had this but it's here to the set i would

1209
01:22:50,450 --> 01:22:53,280
expect that you're similarity will increase

1210
01:22:53,340 --> 01:22:56,840
and this is reflected by the fact that w five

1211
01:22:56,850 --> 01:23:01,950
the the for the weight assigned to disguise here with respect to what is greater

1212
01:23:01,950 --> 01:23:02,960
than zero

1213
01:23:02,980 --> 01:23:04,760
OK so the size

1214
01:23:05,430 --> 01:23:11,810
this ways there's an interesting things about what happens when i try and that was

1215
01:23:11,880 --> 01:23:13,330
two groups of data

1216
01:23:13,370 --> 01:23:16,860
so this leads us to the main definition of dominance

1217
01:23:16,900 --> 01:23:20,380
we are in the symmetric case right now we are assuming

1218
01:23:20,390 --> 01:23:23,790
it's similar to semantic and they are nonnegative

1219
01:23:24,450 --> 01:23:25,780
we define

1220
01:23:25,940 --> 01:23:30,900
a subset of edges the the dominant set if these two conditions are satisfied we

1221
01:23:30,900 --> 01:23:32,160
require that

1222
01:23:32,190 --> 01:23:33,190
for all

1223
01:23:33,200 --> 01:23:36,600
in total there this world i in as

1224
01:23:36,640 --> 01:23:40,330
w s r is greater than zero which means that

1225
01:23:40,340 --> 01:23:42,510
this set as high

1226
01:23:42,530 --> 01:23:44,590
the internal coherence

1227
01:23:44,590 --> 01:23:48,390
but we know that turn on which it is not the only thing here

1228
01:23:48,410 --> 01:23:50,780
we need to do to care about that

1229
01:23:50,830 --> 01:23:56,030
we also we also need an external criteria so required for all i know along

1230
01:23:56,080 --> 01:23:57,900
with external to us

1231
01:23:57,910 --> 01:24:02,690
this number is less than zero which means that if i try to

1232
01:24:02,750 --> 01:24:05,740
i said i would agree

1233
01:24:05,760 --> 01:24:13,230
so in one sense a dominant set is the maximally coherent set of maximum in

1234
01:24:13,230 --> 01:24:16,240
the sense that as soon as i try to add something else

1235
01:24:16,290 --> 01:24:18,250
your personal

1236
01:24:18,270 --> 01:24:23,510
so this is our basic definition

1237
01:24:23,520 --> 01:24:26,990
no we formalize the notion of the cluster

1238
01:24:28,740 --> 01:24:33,920
so from now on our notion of a cluster coincide the dominant sets here is

1239
01:24:33,920 --> 01:24:35,420
a numerical example

1240
01:24:35,450 --> 01:24:39,450
here the set one two three is dominant and you can see

1241
01:24:39,520 --> 01:24:43,350
internally it's high coherence coherence and then outside

1242
01:24:43,390 --> 01:24:48,940
that is outside less similar to the

1243
01:24:48,990 --> 01:24:51,280
now let's

1244
01:24:51,330 --> 01:24:53,400
start from the beginning we

1245
01:24:53,450 --> 01:24:58,370
we started from the case where the magic of similarities with zero one

1246
01:24:58,390 --> 01:24:59,400
was my

1247
01:24:59,460 --> 01:25:02,030
and now we have introduced the notion

1248
01:25:02,040 --> 01:25:05,380
in that case in the binary case we we so that

1249
01:25:05,420 --> 01:25:08,990
the notion of the class actually is the notion of the maximal clique

1250
01:25:09,030 --> 01:25:13,180
now we are in the way the case and we introduce the notion of dominance

1251
01:25:13,180 --> 01:25:18,340
that and now the national question what is the relationship between the maximum clique and

1252
01:25:19,640 --> 01:25:22,290
or another way around if i have

1253
01:25:22,350 --> 01:25:24,840
an unweighted graph where the

1254
01:25:24,860 --> 01:25:30,310
edges are weighted by one zero then what is the dominant set so the answer

1255
01:25:30,390 --> 01:25:32,930
is that brazil one metastasis

1256
01:25:32,940 --> 01:25:36,760
dominant sets coincide with strictly maximum

1257
01:25:36,870 --> 01:25:38,500
OK actually

1258
01:25:38,580 --> 01:25:43,100
notice that here we have a strictly maximum clique if instead of eleven

1259
01:25:43,140 --> 01:25:46,870
less than zero here we have less than or equal to zero

1260
01:25:46,920 --> 01:25:50,950
so we relax this this condition then we would have maximal clique but as i

1261
01:25:50,950 --> 01:25:55,960
said before maximal cliques are a less stable in the sense that they can throw

1262
01:25:55,960 --> 01:26:00,240
away about x and the another one to get the next clue when we are

1263
01:26:00,240 --> 01:26:02,050
interested in stable solutions

1264
01:26:02,780 --> 01:26:04,410
so that's the reason why here

1265
01:26:04,410 --> 01:26:06,750
we have to stick to less than

1266
01:26:06,770 --> 01:26:14,690
the last thing we want

1267
01:26:14,700 --> 01:26:15,600
this one

1268
01:26:15,600 --> 01:26:21,250
OK this condition this is

1269
01:26:21,300 --> 01:26:25,360
now is the second condition in this is which makes this if i give you

1270
01:26:25,400 --> 01:26:27,710
a cluster i expect that every

1271
01:26:27,800 --> 01:26:31,660
subset of this cluster will be internal career

1272
01:26:31,660 --> 01:26:33,790
so in this sense i mean

1273
01:26:33,810 --> 01:26:39,260
the intuition behind this is just if

1274
01:26:41,380 --> 01:26:45,180
if you doubt that you wanted to gain you want to have this

1275
01:26:45,190 --> 01:26:47,260
this one correspondence

1276
01:26:47,270 --> 01:26:54,330
you will have a weaker notion which is that in one to one correspondence

1277
01:26:54,330 --> 01:26:55,450
OK now

1278
01:26:59,510 --> 01:27:03,330
now we have find the notion of the dominant set now let's put ourselves from

1279
01:27:03,520 --> 01:27:07,560
a computational perspective is how can i find the answer

1280
01:27:07,620 --> 01:27:10,770
and then we maybe we can ask of partition

1281
01:27:10,800 --> 01:27:13,970
a set of data into dimensions

1282
01:27:13,990 --> 01:27:15,250
OK in order to

1283
01:27:15,270 --> 01:27:16,840
find dominant set

1284
01:27:17,160 --> 01:27:22,160
actually we could for example we could derive agreed younger

1285
01:27:22,170 --> 01:27:27,750
for example finding maxima maxima with the maximal clique in a graph that i can

1286
01:27:27,750 --> 01:27:32,140
do a straightforward argument i just think about it just order

1287
01:27:32,190 --> 01:27:33,220
the vertices

1288
01:27:33,280 --> 01:27:37,350
and the graph by degree just just take the highest degree nodes and a

1289
01:27:37,350 --> 01:27:47,740
there is a oscillation itself

1290
01:27:47,990 --> 01:27:50,660
so that's what

1291
01:27:50,660 --> 01:27:51,930
that's right

1292
01:27:56,220 --> 01:27:59,660
so far all of

1293
01:27:59,910 --> 01:28:05,740
the goal is to these are the largest least squares problems

1294
01:28:08,990 --> 01:28:13,720
this is the forward stagewise optimisation

1295
01:28:13,740 --> 01:28:15,550
all these things

1296
01:28:15,570 --> 01:28:17,600
so can see how well

1297
01:28:17,620 --> 01:28:20,180
we will be

1298
01:28:20,320 --> 01:28:27,240
generation has to be because i was

1299
01:28:27,240 --> 01:28:29,550
so the problem is that you

1300
01:28:29,970 --> 01:28:31,370
thank you

1301
01:28:34,600 --> 01:28:36,280
that's that's a good thing

1302
01:28:39,280 --> 01:28:47,700
and this is what i propose is really a whole question that is currently just

1303
01:28:47,850 --> 01:28:49,600
right from the

1304
01:28:49,680 --> 01:28:59,120
the discrete loss function when they're not on this one because of the content be

1305
01:28:59,140 --> 01:29:00,450
and so

1306
01:29:00,490 --> 01:29:08,640
that's a question that is the only to the minimisation of structure for the expectation

1307
01:29:08,640 --> 01:29:10,570
of one

1308
01:29:10,600 --> 01:29:12,260
five years

1309
01:29:12,280 --> 01:29:14,680
does that mean that there is

1310
01:29:14,740 --> 01:29:18,570
what one of

1311
01:29:18,590 --> 01:29:23,800
so that we could special

1312
01:29:23,820 --> 01:29:25,660
and we

1313
01:29:25,680 --> 01:29:29,820
and how did you this

1314
01:29:30,760 --> 01:29:34,120
using this

1315
01:29:34,200 --> 01:29:36,320
for the last time

1316
01:29:38,030 --> 01:29:45,090
i mean this is something that

1317
01:29:45,160 --> 01:29:48,590
what is the probability distribution we have

1318
01:29:48,600 --> 01:29:57,850
as we know it it's like be the right off the bat is

1319
01:29:57,870 --> 01:30:02,350
it is possible

1320
01:30:02,570 --> 01:30:05,010
it was

1321
01:30:05,030 --> 01:30:08,180
in final question

1322
01:30:08,200 --> 01:30:09,990
see the relationship

1323
01:30:11,780 --> 01:30:16,570
the first annual

1324
01:30:22,760 --> 01:30:25,600
twenty two

1325
01:30:52,700 --> 01:30:58,030
it is

1326
01:31:11,330 --> 01:31:16,010
so let's look at the

1327
01:31:18,180 --> 01:31:29,330
right so that the whole of on these sorts of questions

1328
01:31:32,390 --> 01:31:38,800
the use of images is very

1329
01:31:39,050 --> 01:31:43,160
for instance there is

1330
01:31:47,100 --> 01:31:48,720
it is

1331
01:31:48,740 --> 01:31:54,450
one of the things that come from

1332
01:31:54,470 --> 01:31:56,100
all right

1333
01:31:56,200 --> 01:31:58,720
also has some

1334
01:31:58,780 --> 01:32:01,990
so this is also

1335
01:32:02,010 --> 01:32:05,430
it is easy to find

1336
01:32:07,850 --> 01:32:10,300
years as

1337
01:32:10,430 --> 01:32:12,260
the problem

1338
01:32:14,950 --> 01:32:17,200
also say

1339
01:32:17,200 --> 01:32:21,030
also is

1340
01:32:23,350 --> 01:32:28,720
characterizing the two tools to support

1341
01:32:30,930 --> 01:32:38,120
well characterised by loss which minimizes

1342
01:32:41,970 --> 01:32:48,050
the first thing i

1343
01:32:48,070 --> 01:32:49,950
the position of already

1344
01:32:52,280 --> 01:32:54,910
the reason to exist

1345
01:33:00,510 --> 01:33:04,220
we just the citation

1346
01:33:07,120 --> 01:33:13,030
so using the results of the research is part of

1347
01:33:13,570 --> 01:33:14,990
the threshold

1348
01:33:15,530 --> 01:33:22,090
that is different from one that will denote by s

1349
01:33:23,620 --> 01:33:29,240
so is the thing here is that almost all of this is that

1350
01:33:29,260 --> 01:33:32,330
if you click on probability distribution

1351
01:33:32,350 --> 01:33:33,910
right just choose

1352
01:33:35,600 --> 01:33:39,820
to label the first one is why

1353
01:33:43,070 --> 01:33:47,090
it is that they are more is different

1354
01:33:47,100 --> 01:33:48,370
the the problem

1355
01:33:48,620 --> 01:33:52,200
so class

1356
01:33:52,240 --> 01:33:55,350
and with this one is four

1357
01:33:55,370 --> 01:33:56,760
this cost function

1358
01:33:58,180 --> 01:34:03,910
it was easy to specify one

1359
01:34:10,350 --> 01:34:12,780
in response to that

1360
01:34:12,780 --> 01:34:15,910
it's a big role

1361
01:34:15,930 --> 01:34:17,590
measuring the

1362
01:34:17,600 --> 01:34:20,160
of course that can

1363
01:34:20,780 --> 01:34:22,200
he the smallest

1364
01:34:22,220 --> 01:34:24,390
recently she

1365
01:34:25,030 --> 01:34:28,200
it uses of distribution

1366
01:34:30,850 --> 01:34:35,390
so the station using

1367
01:34:35,410 --> 01:34:41,470
the conditional probability of y is one particular case

1368
01:34:41,490 --> 01:34:45,550
all the next

1369
01:34:45,570 --> 01:34:49,450
and that is

1370
01:34:49,450 --> 01:34:54,140
parameter five that because she

1371
01:34:54,160 --> 01:34:58,970
the last year the

1372
01:35:03,320 --> 01:35:07,350
it turns out also find

1373
01:35:07,370 --> 01:35:08,550
the optimal

1374
01:35:08,570 --> 01:35:15,850
more real to me like i was looking for

1375
01:35:15,870 --> 01:35:23,640
so so crucial decomposition what we do

1376
01:35:23,660 --> 01:35:28,970
he's right this river is the station is that this

1377
01:35:29,050 --> 01:35:33,010
so think

1378
01:35:33,120 --> 01:35:40,460
right so this expectation here we can write an expectation rates of conditional expectations of

1379
01:35:40,600 --> 01:35:43,850
phi one and that's

1380
01:35:43,850 --> 01:35:48,800
horizontal that it doesn't make a difference for the uniform data whether it's cluster

1381
01:35:48,810 --> 01:35:51,810
this how many clusters are recited some somewhat correct for this

1382
01:35:51,820 --> 01:35:57,050
and that's the idea what the gap statistic actually does

1383
01:35:57,120 --> 01:35:58,550
the trick is now

1384
01:35:58,550 --> 01:36:01,820
of course it's not so easy you can just say i divide by the number

1385
01:36:01,820 --> 01:36:05,060
of clusters or i've divided by the square root of the the number of clusters

1386
01:36:05,810 --> 01:36:08,200
it is not very clear how exactly

1387
01:36:08,240 --> 01:36:11,950
the influence of the number of clusters is aircraft so it's not the functional dependency

1388
01:36:11,970 --> 01:36:13,810
which is obvious

1389
01:36:13,820 --> 01:36:18,330
and what they get statistic analysis

1390
01:36:18,480 --> 01:36:20,620
we try to generate

1391
01:36:20,680 --> 01:36:22,190
essentially we do

1392
01:36:22,200 --> 01:36:24,170
in the previous slide

1393
01:36:24,180 --> 01:36:26,880
so assume that given the data set

1394
01:36:26,890 --> 01:36:30,810
we know the curve decreases like this now what we do is we generate another

1395
01:36:30,810 --> 01:36:32,450
second data set

1396
01:36:32,480 --> 01:36:35,180
we test which live in the same domain

1397
01:36:35,190 --> 01:36:38,930
but which is the data set which doesn't contain any cluster don't

1398
01:36:38,980 --> 01:36:42,810
and then we say OK now we look into this curve

1399
01:36:42,830 --> 01:36:47,580
divided by the square distance because here in this uniform case it shows us

1400
01:36:47,680 --> 01:36:50,820
how the number k how this curve decreases

1401
01:36:50,870 --> 01:36:55,720
independently of structure and then we just say can be used for normalization

1402
01:36:55,750 --> 01:36:59,200
and then hopefully what happened here is that this curve

1403
01:36:59,250 --> 01:37:02,180
still decreases if i

1404
01:37:02,210 --> 01:37:06,780
say two or three clusters but it increases if i want to take ten cluster

1405
01:37:06,790 --> 01:37:09,200
just because i divided out this effect

1406
01:37:11,050 --> 01:37:16,930
and that's what is what is it's called the gap statistics o

1407
01:37:16,960 --> 01:37:18,510
what you do is you run

1408
01:37:18,900 --> 01:37:22,340
with different numbers of k in the original data

1409
01:37:22,410 --> 01:37:24,340
and run it on this uniform data

1410
01:37:24,480 --> 01:37:28,360
and you take to the ratio between the two

1411
01:37:28,460 --> 01:37:31,220
if you want you can also take a look at it but it doesn't change

1412
01:37:32,610 --> 01:37:35,450
and then you say OK i minimize this quantity

1413
01:37:35,550 --> 01:37:39,040
the minimum gives me the number of k

1414
01:37:39,730 --> 01:37:44,810
and i think that's one of the method which is mostly used for selecting the

1415
01:37:44,810 --> 01:37:46,820
number of clusters

1416
01:37:46,830 --> 01:37:50,430
and there's one step which is not so trivial actually

1417
01:37:50,500 --> 01:37:53,390
the step of generating this uniform distribution

1418
01:37:55,480 --> 01:37:58,050
the data we here

1419
01:37:58,070 --> 01:38:01,150
to generate such a uniform distribution you could just say OK

1420
01:38:01,160 --> 01:38:03,880
look at the space in which my pillars

1421
01:38:03,900 --> 01:38:10,100
and i generate a uniform distribution center from a uniform distribution on this

1422
01:38:10,110 --> 01:38:12,680
on the same domain somehow

1423
01:38:12,680 --> 01:38:13,530
and of course

1424
01:38:13,540 --> 01:38:17,550
if you were not to be for example you can do that but if we

1425
01:38:17,550 --> 01:38:22,510
have data which is hundred dimensions it you can just it's very difficult to sample

1426
01:38:22,510 --> 01:38:26,090
or i mean of course you could try to sample from a hundred dimensional units

1427
01:38:26,280 --> 01:38:28,180
you but somehow it

1428
01:38:28,280 --> 01:38:32,660
it is very likely that the data you get this or the this reference distribution

1429
01:38:32,660 --> 01:38:35,840
you get in this case has nothing to do with the data so maybe data

1430
01:38:36,260 --> 01:38:41,200
submanifold in those hundred dimensional space and it actually only has three dimensions but if

1431
01:38:41,200 --> 01:38:44,000
the centre from this hundred dimensional unit cube

1432
01:38:44,010 --> 01:38:45,940
that probably won't be a good idea

1433
01:38:47,010 --> 01:38:49,350
so another approach which is often taking is

1434
01:38:49,360 --> 01:38:54,170
he just scramble your data so what you do is you have

1435
01:38:54,200 --> 01:38:55,110
you take

1436
01:38:55,140 --> 01:39:01,070
your data matrix

1437
01:39:01,080 --> 01:39:02,250
so you have

1438
01:39:03,330 --> 01:39:04,750
data point

1439
01:39:05,560 --> 01:39:09,790
o point one song to feature one feature two

1440
01:39:09,800 --> 01:39:12,860
so dave is described in such metrics

1441
01:39:12,920 --> 01:39:14,540
and what can i do with you

1442
01:39:14,550 --> 01:39:16,840
take each column in this matrix

1443
01:39:16,850 --> 01:39:19,330
and two randomly permuted

1444
01:39:19,350 --> 01:39:21,010
what happens is that

1445
01:39:21,100 --> 01:39:25,680
feature tool if you only look at future to the distribution of theta two is

1446
01:39:25,680 --> 01:39:26,950
still the same

1447
01:39:28,000 --> 01:39:31,940
in the data after scrambling but some of the features randomly distributed to some data

1448
01:39:33,150 --> 01:39:36,530
by this way so you do this for each column independently of each column to

1449
01:39:36,530 --> 01:39:46,760
OK so now that carries some colours and patterns and some basic definitions i'm going

1450
01:39:46,760 --> 01:39:49,150
to go and to into a little bit of detail and some of the tools

1451
01:39:49,150 --> 01:39:50,060
that we have

1452
01:39:50,100 --> 01:39:56,410
for graph mining

1453
01:39:56,420 --> 01:40:00,920
so some of the tools to look at is a matrix decomposition direct application that

1454
01:40:00,920 --> 01:40:02,780
principal component analysis

1455
01:40:02,780 --> 01:40:08,610
look at some random walks and ranking of algorithms and then co clustering and self

1456
01:40:08,610 --> 01:40:14,120
similarity into plots

1457
01:40:16,050 --> 01:40:19,870
so what one example of the matrices will want to work with as the adjacency

1458
01:40:19,870 --> 01:40:22,190
matrix but another common wires

1459
01:40:24,580 --> 01:40:27,260
documents and terms for clustering

1460
01:40:27,270 --> 01:40:31,620
so in this matrix here will have a set of papers and each of them

1461
01:40:31,620 --> 01:40:37,260
has never turned associated with people once it mentions the term data thirteen times

1462
01:40:37,730 --> 01:40:40,050
it and so on

1463
01:40:40,060 --> 01:40:44,150
so we want to be able to find patterns in groups and concepts from this

1464
01:40:44,150 --> 01:40:48,500
from this matrix

1465
01:40:48,560 --> 01:40:53,310
so in singular value decomposition we would take the document term matrix and decompose it

1466
01:40:54,220 --> 01:40:58,830
into three matrices the theorem is you then you can do this with any matrix

1467
01:41:00,530 --> 01:41:02,760
we want to be able to identify

1468
01:41:02,780 --> 01:41:07,840
CS concepts and medical concept because will have it's pretty obvious we update information retrieval

1469
01:41:08,220 --> 01:41:11,470
and then bring and lung is probably a medical concept so we want to be

1470
01:41:11,470 --> 01:41:12,890
able to

1471
01:41:12,920 --> 01:41:16,360
b b we all want to be able to tell which papers are which papers

1472
01:41:16,360 --> 01:41:20,590
go where

1473
01:41:20,590 --> 01:41:22,750
so for instance are

1474
01:41:22,860 --> 01:41:28,340
the the u eight u matrix is the document concept similarity matrix and we have

1475
01:41:28,340 --> 01:41:29,440
to be

1476
01:41:29,450 --> 01:41:37,700
the first paper has cs score of one point one eight

1477
01:41:37,780 --> 01:41:41,610
and then the sigma indicates the string to string of all the concepts that were

1478
01:41:41,610 --> 01:41:45,690
found with this decomposition so we see concepts

1479
01:41:45,740 --> 01:41:50,960
c is concept strength nine point six four

1480
01:41:51,030 --> 01:41:52,020
and then

1481
01:41:52,030 --> 01:41:56,290
this is the transpose gives the term concept similarity that is

1482
01:41:56,310 --> 01:41:58,280
how how strongly

1483
01:41:58,370 --> 01:42:00,810
certain terms

1484
01:42:00,810 --> 01:42:05,340
maps to concept

1485
01:42:06,720 --> 01:42:10,280
but the term data maps to CS concept

1486
01:42:10,290 --> 01:42:14,250
with the score point five a

1487
01:42:14,440 --> 01:42:18,660
so interpretation of this we could think of multiplying a with itself together

1488
01:42:18,720 --> 01:42:20,000
term similarity

1489
01:42:20,020 --> 01:42:21,530
term term similarity

1490
01:42:21,560 --> 01:42:26,310
or document to document similarity

1491
01:42:26,320 --> 01:42:29,000
and we may refer to this decomposition

1492
01:42:29,000 --> 01:42:31,820
with having you is the left singular vectors

1493
01:42:31,870 --> 01:42:36,710
sigma singular values and the transported as the right singular vectors of what we might

1494
01:42:36,710 --> 01:42:37,710
refer to

1495
01:42:37,720 --> 01:42:42,660
we write for these terms a little later

1496
01:42:42,680 --> 01:42:46,470
so what if we wanted to extend extend these

1497
01:42:46,500 --> 01:42:49,540
this decomposition to identify concepts over time

1498
01:42:49,560 --> 01:42:53,770
so to that we use the user tensor and that's the that's and in general

1499
01:42:53,770 --> 01:43:00,190
generalisation make me matrix so here we have three dimensional generalisation of matrix we take

1500
01:43:01,060 --> 01:43:02,690
what you want to take arthur's

1501
01:43:02,690 --> 01:43:05,120
o over time

1502
01:43:05,190 --> 01:43:06,820
over different conferences

1503
01:43:06,840 --> 01:43:08,990
and then look then now

1504
01:43:09,030 --> 01:43:11,180
the terms for each each

1505
01:43:11,190 --> 01:43:21,960
other and conference

1506
01:43:22,040 --> 01:43:27,630
so there's a tensor decomposition that corresponds to this

1507
01:43:27,940 --> 01:43:31,910
we have the author by keyword by conference tensor are here

1508
01:43:31,930 --> 01:43:32,810
and then

1509
01:43:32,820 --> 01:43:36,040
decomposing it would give us after by other groups

1510
01:43:36,100 --> 01:43:42,560
conference conference by conference groups and keyword by keyword groups

1511
01:43:42,620 --> 01:43:45,370
and then we would have a core interaction tensor

1512
01:43:47,370 --> 01:43:49,060
explains how the

1513
01:43:49,060 --> 01:43:49,930
how the

1514
01:43:49,940 --> 01:43:52,850
after groups the conference groups in the keyword groups

1515
01:43:52,860 --> 01:43:55,580
interact with each other

1516
01:43:55,590 --> 01:44:00,570
and this is discussed in detail in another tutorial by jamming sound and lyrical the

1517
01:44:00,580 --> 01:44:03,580
and it can be find this address

1518
01:44:03,590 --> 01:44:07,220
and that over a lot a lot of tensor methods and some of the applications

1519
01:44:07,220 --> 01:44:12,410
there that i'm not able to cover in this manner time

1520
01:44:14,270 --> 01:44:20,450
direct application of speedy is principal component analysis it's a method of dimensionality reduction

1521
01:44:20,600 --> 01:44:25,030
so in trivial case might have two dimensions we have we want to measure occurrences

1522
01:44:25,030 --> 01:44:28,390
of data compared to occurrences of the word information

1523
01:44:28,420 --> 01:44:33,280
so we know that these are often correlated with the weekend

1524
01:44:33,330 --> 01:44:35,710
projected onto the spectre he

1525
01:44:35,720 --> 01:44:38,470
factor here

1526
01:44:38,530 --> 01:44:41,660
and then we have for any point we will be able to the and map

1527
01:44:41,660 --> 01:44:50,470
it onto the one dimensional space

1528
01:44:50,530 --> 01:44:53,780
so i'm getting from SVD principal component analysis

1529
01:44:53,830 --> 01:44:55,170
we would take

1530
01:44:55,180 --> 01:44:59,230
however however k number dimensions we want to protect all the data on two

1531
01:44:59,240 --> 01:45:00,350
and then

1532
01:45:00,360 --> 01:45:03,340
take the first got the first k columns u

1533
01:45:03,410 --> 01:45:05,410
in the first

1534
01:45:05,420 --> 01:45:08,590
the first k entries sigma

1535
01:45:08,590 --> 01:45:15,380
and multiply together and that would be our principal component

1536
01:45:15,390 --> 01:45:19,590
in our going to go over some

1537
01:45:19,600 --> 01:45:26,590
random walks and ranking other algorithms which which may somebody's matrix methods

1538
01:45:26,600 --> 01:45:30,770
so the first first of all introduces climate algorithm hits

1539
01:45:30,780 --> 01:45:34,570
and the problem definition is that given a web graph that we have

1540
01:45:36,280 --> 01:45:41,110
and a query such as bicycles or singular value decomposition is some term we want

1541
01:45:41,970 --> 01:45:45,150
we want to look at the bottom and we want to find the most authoritative

1542
01:45:45,150 --> 01:45:47,980
webpages for this web query

1543
01:45:47,990 --> 01:45:55,340
the details can be found in quite kleinberg soda soda nineteen ninety eight

1544
01:45:55,410 --> 01:45:56,520
so the

1545
01:45:56,570 --> 01:46:00,360
the the way the algorithm works is that were given some graph

1546
01:46:00,380 --> 01:46:03,720
and the first step

1547
01:46:03,740 --> 01:46:07,680
is to find all pages containing the query terms

1548
01:46:07,770 --> 01:46:12,570
so we might find these three pages that have these terms in them

1549
01:46:12,600 --> 01:46:16,400
and then extend the next step is to expand by one move forward and one

1550
01:46:16,400 --> 01:46:19,240
backward and so these are going to be all the nodes that we want to

1551
01:46:19,240 --> 01:46:20,880
be able to rank

1552
01:46:20,910 --> 01:46:26,160
according to the to their authority on a certain subject

1553
01:46:26,180 --> 01:46:28,080
and so on the on the resulting

1554
01:46:28,090 --> 01:46:31,780
no that we take down will get a high score higher authority score to know

1555
01:46:31,780 --> 01:46:34,420
that have many important notes pointing to them

1556
01:46:34,450 --> 01:46:37,230
and then we'll give a high importance score happiness score

1557
01:46:37,260 --> 01:46:40,380
to know that point to the authorities

1558
01:46:40,380 --> 01:46:41,940
plus one half

1559
01:46:41,990 --> 01:46:42,930
and two

1560
01:46:43,450 --> 01:46:45,670
two prime square

1561
01:46:45,680 --> 01:46:47,770
two equations with two unknowns

1562
01:46:47,800 --> 01:46:50,520
and in principle you can solve for v one prime

1563
01:46:50,570 --> 01:46:52,140
and for the two prime

1564
01:46:52,160 --> 01:46:54,620
except that it could be time consuming

1565
01:46:54,680 --> 01:46:58,690
and so on exams what is normally done when you get a problem like that

1566
01:46:58,700 --> 01:47:02,970
number you get the problem by year and one thousand eight and two

1567
01:47:03,020 --> 01:47:07,040
or and one is much much larger than n two

1568
01:47:07,180 --> 01:47:08,600
or in one

1569
01:47:08,610 --> 01:47:11,040
is not much smaller than two

1570
01:47:11,080 --> 01:47:12,360
like banging a

1571
01:47:12,390 --> 01:47:17,390
basketball into a ping-pong ball or being able to basque

1572
01:47:17,410 --> 01:47:19,180
i will do it very

1573
01:47:19,250 --> 01:47:22,690
a simple example whereby i will take now for you

1574
01:47:22,700 --> 01:47:24,330
in one equals and two

1575
01:47:24,590 --> 01:47:27,450
will call that

1576
01:47:27,470 --> 01:47:29,660
and i will even simplified problem

1577
01:47:29,680 --> 01:47:34,160
by making the two zero so the second object is standing still

1578
01:47:34,240 --> 01:47:36,160
one dimensional collision

1579
01:47:36,890 --> 01:47:40,240
it's that object very special case

1580
01:47:40,310 --> 01:47:43,430
so to conservation of momentum now

1581
01:47:43,490 --> 01:47:45,140
becomes much simpler

1582
01:47:45,190 --> 01:47:46,750
and time be one

1583
01:47:46,750 --> 01:47:48,680
there is no v two

1584
01:47:49,930 --> 01:47:51,470
the one prime

1585
01:47:51,540 --> 01:47:52,480
class and

1586
01:47:52,560 --> 01:47:54,250
the two prime

1587
01:47:54,370 --> 01:47:58,490
conservation of kinetic energy

1588
01:47:58,500 --> 01:47:59,580
one half

1589
01:48:01,800 --> 01:48:02,950
ones created

1590
01:48:05,670 --> 01:48:06,830
one half

1591
01:48:07,940 --> 01:48:09,730
the one prime squared

1592
01:48:09,740 --> 01:48:11,760
plus one half

1593
01:48:12,620 --> 01:48:13,500
two crime

1594
01:48:16,270 --> 01:48:19,470
notice that now i lose my and which is very convenient

1595
01:48:19,480 --> 01:48:22,350
i lose my one half and even

1596
01:48:22,380 --> 01:48:25,260
this is very easy to solve

1597
01:48:25,350 --> 01:48:28,520
if you square this equation

1598
01:48:28,540 --> 01:48:31,860
you get something look very similar to this if you square it

1599
01:48:31,870 --> 01:48:33,910
you could be one square

1600
01:48:35,310 --> 01:48:37,490
he one prime

1601
01:48:37,490 --> 01:48:40,130
plus the two prime squared

1602
01:48:40,180 --> 01:48:41,630
was two

1603
01:48:41,720 --> 01:48:43,370
he one prime

1604
01:48:43,550 --> 01:48:44,950
the prime

1605
01:48:45,040 --> 01:48:49,910
and compare this equation with this equation almost identical except for this term

1606
01:48:49,920 --> 01:48:52,510
so this term must be zero

1607
01:48:52,520 --> 01:48:55,440
we in order to be two prime is not zero

1608
01:48:55,560 --> 01:48:56,550
the kids

1609
01:48:56,580 --> 01:48:58,540
so it will go forward

1610
01:48:58,580 --> 01:49:00,320
so what that means that

1611
01:49:00,330 --> 01:49:03,200
one prime equals zero

1612
01:49:03,250 --> 01:49:05,390
and if v one prime equals zero

1613
01:49:05,410 --> 01:49:08,330
you see that FT two prime equals one

1614
01:49:08,380 --> 01:49:10,200
and this is that classic case

1615
01:49:16,020 --> 01:49:17,350
this one

1616
01:49:17,370 --> 01:49:19,380
has no speed

1617
01:49:19,410 --> 01:49:22,160
it's the basis of the velocity field one

1618
01:49:22,180 --> 01:49:23,730
they have the the same as

1619
01:49:23,820 --> 01:49:27,850
after the collision this one stands still and this one takes over the speed famous

1620
01:49:27,850 --> 01:49:29,330
newton's cradle

1621
01:49:29,370 --> 01:49:30,660
you see it often

1622
01:49:30,670 --> 01:49:32,080
we spend limbs

1623
01:49:32,100 --> 01:49:33,220
it is the logo

1624
01:49:33,240 --> 01:49:37,000
on the a to one home page and i showed you a demonstration here

1625
01:49:37,060 --> 01:49:41,300
when we discuss that elections

1626
01:49:41,360 --> 01:49:42,050
all right

1627
01:49:42,060 --> 01:49:43,420
let's move on

1628
01:49:43,430 --> 01:49:45,260
and that's something now

1629
01:49:46,990 --> 01:49:48,140
the works

1630
01:49:48,190 --> 01:49:50,600
and the momentum

1631
01:49:52,970 --> 01:49:54,290
and let's go

1632
01:49:54,320 --> 01:49:56,870
discussed at atwood machine

1633
01:49:57,040 --> 01:49:59,000
i atwood machine

1634
01:49:59,010 --> 01:50:02,180
clever device that allows you to measure

1635
01:50:02,260 --> 01:50:07,700
a reasonable degree of accuracy the gravitational acceleration

1636
01:50:07,830 --> 01:50:09,250
is supporting

1637
01:50:09,390 --> 01:50:12,920
forty mass and radius are

1638
01:50:12,930 --> 01:50:15,250
it's solid

1639
01:50:15,480 --> 01:50:18,040
solid disk

1640
01:50:19,310 --> 01:50:22,120
friction is about point p

1641
01:50:22,120 --> 01:50:23,270
really is or

1642
01:50:23,360 --> 01:50:27,060
and there's a rope here

1643
01:50:27,060 --> 01:50:29,020
new massless

1644
01:50:29,080 --> 01:50:30,460
we ignore them as

1645
01:50:30,470 --> 01:50:32,590
mass and two is here

1646
01:50:32,670 --> 01:50:34,750
mass and one this year

1647
01:50:34,970 --> 01:50:37,240
let's assume that two

1648
01:50:37,250 --> 01:50:38,400
the larger

1649
01:50:38,410 --> 01:50:39,750
and one

1650
01:50:39,760 --> 01:50:42,470
this will be accelerated in this direction

1651
01:50:42,490 --> 01:50:44,850
this will be accelerated this direction

1652
01:50:44,870 --> 01:50:46,850
this will start to rotate

1653
01:50:46,880 --> 01:50:48,850
the angular velocity omega

1654
01:50:50,000 --> 01:50:51,120
will be

1655
01:50:52,740 --> 01:50:55,050
of time

1656
01:50:55,070 --> 01:50:55,800
and now

1657
01:50:55,820 --> 01:50:58,570
the first thing we want to do is to make up

1658
01:50:58,620 --> 01:51:00,660
free body diagrams

1659
01:51:00,670 --> 01:51:03,110
everybody diagrams for this one

1660
01:51:03,140 --> 01:51:05,100
it's easy

1661
01:51:05,110 --> 01:51:07,020
and one g

1662
01:51:09,860 --> 01:51:12,090
the one up

1663
01:51:12,130 --> 01:51:13,520
this one

1664
01:51:13,580 --> 01:51:15,350
we have

1665
01:51:15,490 --> 01:51:17,850
two g down

1666
01:51:17,850 --> 01:51:21,430
are kind of cool is three other types of learning we haven't

1667
01:51:21,450 --> 01:51:24,800
mentioned reinforcement learning

1668
01:51:24,850 --> 01:51:29,350
it is more of a kind of artificial intelligence typesetting where you've got an agent

1669
01:51:29,350 --> 01:51:29,700
and some

1670
01:51:30,080 --> 01:51:33,910
world and trying to maximize rewards of some kind

1671
01:51:33,910 --> 01:51:36,080
and this very questions but

1672
01:51:36,580 --> 01:51:38,800
if your

1673
01:51:38,820 --> 01:51:42,740
if you're some kind of entity in some kind of world how much time she

1674
01:51:42,760 --> 01:51:47,760
spent exploring that world and how much time he spent exploiting the resources that you've

1675
01:51:48,490 --> 01:51:49,930
already found

1676
01:51:50,160 --> 01:51:53,410
so i think that's got some interesting things to say about

1677
01:51:54,100 --> 01:51:56,450
learning good behaviour patterns

1678
01:51:57,050 --> 01:51:58,350
transfer learning is

1679
01:51:58,370 --> 01:52:03,600
also very interesting i think it's the idea that different tasks might be related in

1680
01:52:03,600 --> 01:52:04,510
some way

1681
01:52:05,510 --> 01:52:08,140
you might be wanting to classification tasks

1682
01:52:08,160 --> 01:52:10,330
perhaps your learning

1683
01:52:10,350 --> 01:52:13,200
actually learning diagnose malaria from

1684
01:52:13,830 --> 01:52:19,780
from microscopic all images and you might have another test career trying to diagnose tuberculosis

1685
01:52:19,780 --> 01:52:23,720
the same types images and the

1686
01:52:23,740 --> 01:52:27,320
the things that you are doing both of them i have some overlap you know

1687
01:52:27,320 --> 01:52:31,510
there might be some underlying representation which makes it easy to solve but things and

1688
01:52:31,510 --> 01:52:36,050
if you've got both datasets you combine them in some way then perhaps that helps

1689
01:52:36,050 --> 01:52:38,100
to even better

1690
01:52:38,680 --> 01:52:44,640
i think also transfer learning is pretty interesting in the in the eyes sense

1691
01:52:44,660 --> 01:52:45,800
in is

1692
01:52:45,820 --> 01:52:48,160
something that people seem to do really well

1693
01:52:48,200 --> 01:52:49,720
you know

1694
01:52:49,740 --> 01:52:51,530
you may be

1695
01:52:51,550 --> 01:52:56,620
you may try to learn to ride a motorbike and perhaps you already know how

1696
01:52:56,620 --> 01:53:00,220
to ride a bicycle you had drive a car and somehow those skills can have

1697
01:53:00,220 --> 01:53:03,060
combined to to help you pick up the

1698
01:53:03,140 --> 01:53:05,830
the scale of by driving quickly you know this kind of

1699
01:53:05,830 --> 01:53:07,830
o thing that we can

1700
01:53:07,850 --> 01:53:08,890
we can

1701
01:53:08,910 --> 01:53:12,680
transfer the the representations that we've learned from one

1702
01:53:12,700 --> 01:53:14,620
from one problem and the

1703
01:53:14,640 --> 01:53:15,640
the kinds of

1704
01:53:15,660 --> 01:53:19,450
the decision making processes we used to another one that's a very interesting field

1705
01:53:20,350 --> 01:53:21,800
active learning

1706
01:53:21,820 --> 01:53:24,330
is the case where

1707
01:53:25,850 --> 01:53:27,450
you have

1708
01:53:27,450 --> 01:53:29,720
data which you actually have to pay for

1709
01:53:29,780 --> 01:53:34,950
and you can pay for data at different points in your input space

1710
01:53:34,970 --> 01:53:36,580
and you want to

1711
01:53:36,600 --> 01:53:38,430
get the points which give you

1712
01:53:38,450 --> 01:53:39,950
the most valuable

1713
01:53:39,950 --> 01:53:44,510
so this is like if you've got some budget for collecting data which you actually

1714
01:53:44,510 --> 01:53:51,600
almost always literally have where should you devote budget to collecting things

1715
01:53:51,620 --> 01:53:55,970
so this is for example i talked about crop disease

1716
01:53:56,010 --> 01:53:59,680
mapping so we've got exactly that situation

1717
01:53:59,680 --> 01:54:02,470
if you want to survey we've got some model

1718
01:54:02,470 --> 01:54:04,660
in space and time of where the

1719
01:54:04,660 --> 01:54:06,350
density of diseases

1720
01:54:08,470 --> 01:54:12,260
we want to know as much as possible about by where the disease

1721
01:54:12,280 --> 01:54:15,390
is this particular places were interested in

1722
01:54:16,470 --> 01:54:17,930
we're going to have

1723
01:54:17,950 --> 01:54:21,320
we want to collect data from the points that were not certain about

1724
01:54:21,910 --> 01:54:28,680
the deep space of input where that maximally informative about

1725
01:54:28,780 --> 01:54:29,870
about the

1726
01:54:29,890 --> 01:54:31,740
the quantity of interest

1727
01:54:31,780 --> 01:54:35,560
so we might look again at the error bars in our model we places that

1728
01:54:35,700 --> 01:54:39,260
if we've already had you know a thousand samples from

1729
01:54:39,350 --> 01:54:44,080
this point beside across that we might be more interested in a region where we're

1730
01:54:44,080 --> 01:54:46,220
not example

1731
01:54:49,350 --> 01:54:53,120
that's about it i've got one that's why just to wrap up with some comments

1732
01:54:53,330 --> 01:54:56,470
kind of some practical

1733
01:54:56,530 --> 01:54:58,120
it's called that

1734
01:55:01,530 --> 01:55:04,620
first my three practical tips visualize things whenever you can

1735
01:55:04,660 --> 01:55:10,080
you continue supporting tools try projecting down to low dimensions you can see

1736
01:55:10,100 --> 01:55:12,970
if it's a classification where the decision boundary is

1737
01:55:12,970 --> 01:55:19,410
you can see the relationship between the variables space three really big health

1738
01:55:19,410 --> 01:55:23,450
if you can incorporate background knowledge should by no

1739
01:55:24,490 --> 01:55:28,470
aspects of the input are more important than others think about the speech where we

1740
01:55:28,470 --> 01:55:32,550
took the raw data and we used to background knowledge of

1741
01:55:32,550 --> 01:55:35,300
different frequencies

1742
01:55:35,320 --> 01:55:39,430
being generated as part of the speech process you know we might already know that

1743
01:55:39,430 --> 01:55:41,780
kind of representation is more and more useful

1744
01:55:41,830 --> 01:55:43,870
any time you can reason about the

1745
01:55:43,890 --> 01:55:48,370
domain knowledge you have the skin has going to make it make it better

1746
01:55:48,370 --> 01:55:50,620
and you can do that either by

1747
01:55:50,660 --> 01:55:56,410
either by changing the representation of your data for example you might it by changing

1748
01:55:57,030 --> 01:56:01,030
the prior if using some kind of bayesian approach

1749
01:56:01,280 --> 01:56:06,800
good features beat fancy models if you find some clever representation of data

1750
01:56:06,830 --> 01:56:09,510
often the better than finding some

1751
01:56:09,510 --> 01:56:12,160
really complex state-of-the-art

1752
01:56:12,180 --> 01:56:15,470
a classifier or regression or one

1753
01:56:15,490 --> 01:56:21,740
here's quite interesting survey quite recent of a bunch of machine learning projects and how

1754
01:56:21,740 --> 01:56:25,950
much time people spend on the different stages so this can give you some

1755
01:56:25,970 --> 01:56:29,530
some sort of idea of what you're probably going to get involved in if you

1756
01:56:29,530 --> 01:56:32,820
if you decide to embark on one of these projects

1757
01:56:32,830 --> 01:56:37,470
and it's all about the data basically collecting the data and

1758
01:56:37,510 --> 01:56:39,680
dealing with missing values

1759
01:56:42,430 --> 01:56:48,260
getting you get someone's messy database and filtering and outliers and whatnot that something which

1760
01:56:48,260 --> 01:56:50,890
takes like half the size of the project

1761
01:56:50,890 --> 01:56:51,890
and actually

1762
01:56:51,890 --> 01:56:56,050
you know what we've talked about changing representation

1763
01:56:56,080 --> 01:56:57,870
learning the model of the data

1764
01:56:57,890 --> 01:56:59,470
you might think

1765
01:56:59,490 --> 01:57:01,820
when you first comes the

1766
01:57:01,890 --> 01:57:06,010
the field of this is going to be words for all about you you formulate

1767
01:57:06,010 --> 01:57:09,780
some model many learned from the data in your is not machine learning is well

1768
01:57:09,780 --> 01:57:12,760
actually it's only ten percent of what you're

1769
01:57:12,780 --> 01:57:16,850
actually doing if you have one of these projects in practice have found this to

1770
01:57:16,850 --> 01:57:19,140
be about the right

1771
01:57:19,160 --> 01:57:21,050
there's a whole bunch of

