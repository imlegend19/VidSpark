1
00:00:00,000 --> 00:00:07,660
so that's that's correct much a part of the feature function is to indicate

2
00:00:07,720 --> 00:00:10,290
which y values have higher probability

3
00:00:10,340 --> 00:00:15,380
and if someone is intrinsically have higher probability then you can model that was the

4
00:00:15,380 --> 00:00:17,950
feature function doesn't depend on x

5
00:00:17,970 --> 00:00:21,950
but if you have a feature function only depend on access

6
00:00:22,610 --> 00:00:26,340
what what would actually happen is that

7
00:00:26,370 --> 00:00:32,450
mathematically would cancel from the numerator and denominator because you have some after actually have

8
00:00:32,460 --> 00:00:34,190
corresponding wj

9
00:00:34,240 --> 00:00:35,580
so far

10
00:00:35,630 --> 00:00:38,700
for every different y here

11
00:00:38,730 --> 00:00:40,910
you at the same contribution

12
00:00:40,920 --> 00:00:41,990
up here

13
00:00:42,010 --> 00:00:49,190
and so for itself so you could divide through so that that contribution to the

14
00:00:49,200 --> 00:00:55,200
councillors from the numerator denominator and i wouldn't have any influence on the relative probabilities

15
00:00:55,200 --> 00:00:56,860
of the different wise

16
00:00:58,950 --> 00:01:00,310
it's the d

17
00:01:05,110 --> 00:01:06,440
let me

18
00:01:06,640 --> 00:01:09,160
ask another question so

19
00:01:12,970 --> 00:01:15,170
they to a set

20
00:01:15,220 --> 00:01:18,390
so after a for j equals one

21
00:01:19,340 --> 00:01:21,160
j equals

22
00:01:21,860 --> 00:01:24,500
so and the

23
00:01:24,500 --> 00:01:27,630
if i want to talk about the dimensionality of the problem here the dimension as

24
00:01:27,640 --> 00:01:29,450
the number of feature functions

25
00:01:29,470 --> 00:01:35,330
and d is fixed

26
00:01:36,550 --> 00:01:39,940
more often

27
00:01:39,970 --> 00:01:42,030
the length

28
00:01:43,610 --> 00:01:46,080
and the length

29
00:01:47,270 --> 00:01:50,090
is not fixed

30
00:01:51,000 --> 00:01:55,050
axes of x's words and y is waiting for a hyphen

31
00:01:55,140 --> 00:01:56,800
or axes

32
00:01:56,830 --> 00:01:59,770
so sentences and y is part of speech

33
00:02:01,680 --> 00:02:04,670
so one of the more

34
00:02:04,710 --> 00:02:06,920
one of the machine learning issues that

35
00:02:06,920 --> 00:02:09,800
you know feature funk resolve

36
00:02:10,800 --> 00:02:17,680
how to apply supervised learning to objects that don't have a fixed length fixed size

37
00:02:20,200 --> 00:02:23,200
you can have a fixed number of feature functions

38
00:02:23,200 --> 00:02:24,520
even if the

39
00:02:24,520 --> 00:02:27,700
the length of the axes otherwise is not fixed

40
00:02:29,930 --> 00:02:31,820
we can have so

41
00:02:31,860 --> 00:02:35,140
then we can have a special

42
00:02:47,300 --> 00:02:49,830
but what i wrote up here

43
00:02:49,830 --> 00:02:52,040
so f j of x y

44
00:02:52,050 --> 00:02:53,540
it is the sum

45
00:02:53,550 --> 00:02:55,890
over the length of ax

46
00:02:56,740 --> 00:02:58,480
little fj

47
00:02:58,480 --> 00:02:59,740
and then

48
00:02:59,760 --> 00:03:03,980
two neighbouring values in the

49
00:03:03,990 --> 00:03:05,580
label sequence

50
00:03:05,580 --> 00:03:08,850
and then the input x for

51
00:03:08,890 --> 00:03:10,480
and then the

52
00:03:10,490 --> 00:03:12,610
position i

53
00:03:24,930 --> 00:03:32,670
well you would be essentially unchanged expanding by one so in fact

54
00:03:36,170 --> 00:03:42,920
maybe something that's closer to what i would actually do in practice and maybe some

55
00:03:43,800 --> 00:03:46,820
i equals zero to ten plus one

56
00:03:46,830 --> 00:03:49,330
where n is the

57
00:03:49,360 --> 00:03:51,550
length of x

58
00:03:58,830 --> 00:04:00,420
i corps

59
00:04:00,460 --> 00:04:02,200
one to n plus one

60
00:04:02,240 --> 00:04:06,390
fj of why i minus one y i

61
00:04:06,400 --> 00:04:08,970
x y where

62
00:04:09,010 --> 00:04:10,360
i define

63
00:04:15,960 --> 00:04:18,750
as a start

64
00:04:18,760 --> 00:04:23,500
and why and plus one is and

65
00:04:23,550 --> 00:04:26,910
and so

66
00:04:26,920 --> 00:04:30,570
so far come

67
00:04:31,380 --> 00:04:32,220
when i

68
00:04:32,230 --> 00:04:37,040
thinking about trying to predict the output sequence y

69
00:04:37,050 --> 00:04:40,070
it can be computationally convenient

70
00:04:40,100 --> 00:04:45,420
too much to just say well why zero is always start and y n plus

71
00:04:45,420 --> 00:04:46,820
one is always and

72
00:04:46,870 --> 00:04:51,980
and then what i want to predict is whatever comes in between

73
00:05:00,490 --> 00:05:04,320
so the

74
00:05:04,370 --> 00:05:06,790
the the big restriction here

75
00:05:06,800 --> 00:05:14,900
the significant restriction

76
00:05:14,980 --> 00:05:18,240
is that each fj

77
00:05:18,250 --> 00:05:20,500
i can only

78
00:05:20,550 --> 00:05:27,750
examine two adjacent

79
00:05:31,150 --> 00:05:33,930
why i minus one

80
00:05:35,000 --> 00:05:36,480
why i

81
00:05:43,910 --> 00:05:47,420
month and so i have a

82
00:05:47,440 --> 00:05:53,730
the terminology is going to use as a high level feature function

83
00:05:57,650 --> 00:05:58,650
and then

84
00:05:58,650 --> 00:06:03,440
so sixteen to nine that doesn't seem like a big deal but if it hadn't

85
00:06:03,440 --> 00:06:06,980
been for if i could draw you know hundred we would have gone into the

86
00:06:06,980 --> 00:06:10,800
hundred down to something like four times one hundred

87
00:06:10,940 --> 00:06:13,550
as you go from two to one hundred more than the number of atoms in

88
00:06:13,550 --> 00:06:16,340
the universe to roughly four hundred numbers

89
00:06:16,360 --> 00:06:18,110
and that's that's quite reasonable

90
00:06:18,130 --> 00:06:20,180
and if you think about

91
00:06:20,180 --> 00:06:22,800
you know if you're trying to get one of these models to data four hundred

92
00:06:22,800 --> 00:06:24,370
parameters is

93
00:06:24,440 --> 00:06:26,740
not unreasonable worse two to one hundred

94
00:06:26,970 --> 00:06:30,910
well datasets are big these days but not that big

95
00:06:30,930 --> 00:06:35,780
right so it's very important that kind of compression in terms of the parameterisation of

96
00:06:35,780 --> 00:06:37,710
the distribution

97
00:06:37,810 --> 00:06:44,640
is anyone bothered by this factorisation

98
00:06:44,660 --> 00:06:53,030
is there an inconsistency between what i wrote and what state it up there

99
00:07:00,230 --> 00:07:04,600
here i was only using maximal cliques

100
00:07:04,640 --> 00:07:08,770
right there i sort of sets over all cliques

101
00:07:08,780 --> 00:07:13,000
right so the additional cliques that we left over the nodes the nodes are cliques

102
00:07:13,170 --> 00:07:17,840
but is it wrong whatever it here

103
00:07:18,060 --> 00:07:24,530
yes absorb

104
00:07:24,530 --> 00:07:28,950
you're a man of few words but correct words which is good right so you

105
00:07:28,950 --> 00:07:30,290
can always absorbed these things so

106
00:07:30,730 --> 00:07:34,240
we haven't so this is unique if you sort of string parameters out of it

107
00:07:34,240 --> 00:07:40,540
you can get unique factorizations the the way we written these factorizations are unique but

108
00:07:40,540 --> 00:07:43,820
that's not what is actually important right now what's important is that the factorisation is

109
00:07:43,820 --> 00:07:46,090
very compact

110
00:07:46,150 --> 00:07:51,340
OK so

111
00:07:51,570 --> 00:07:55,960
an important result in this area that i'll come back to later actually says you

112
00:07:55,960 --> 00:07:57,700
might be wondering OK so

113
00:07:57,710 --> 00:08:01,330
high-level just recapping we said grasp probability

114
00:08:01,350 --> 00:08:05,350
we said OK i can look at vertex cutset and i can look at conditional

115
00:08:05,350 --> 00:08:10,030
independence or i can look at cliques and i can look at factorizations

116
00:08:11,200 --> 00:08:16,780
you might say well why did i introduced both of them

117
00:08:16,830 --> 00:08:21,790
what interest both because they are actually equivalent this this is a theorem known as

118
00:08:21,790 --> 00:08:24,960
the hammersley clifford theorem it doesn't matter which way you go

119
00:08:24,990 --> 00:08:28,380
if you start with a fixed graph and you say that you want to factorize

120
00:08:28,380 --> 00:08:33,450
across the cliques that's the same as you want saying that all these conditional independence

121
00:08:33,450 --> 00:08:34,840
statements hold

122
00:08:34,860 --> 00:08:42,940
so the markov chain is a particular example of a right the markov chain you

123
00:08:42,940 --> 00:08:46,430
know when you build markov chains you don't actually typically write them like this you

124
00:08:46,430 --> 00:08:51,490
typically write them is an initial distribution the conditional distribution x two given x one

125
00:08:51,490 --> 00:08:53,110
x three given x two

126
00:08:53,180 --> 00:08:57,040
but that's a special case of this kind of factorizations

127
00:08:57,070 --> 00:09:00,810
so that's one way to understand markov chains the other ways in terms of the

128
00:09:00,810 --> 00:09:04,700
conditioning on present breaks the graph into two

129
00:09:04,810 --> 00:09:07,470
right so for markov chains we can sort of see the

130
00:09:07,540 --> 00:09:11,720
which doesn't matter where we start we end up with the same answer but hammersley

131
00:09:11,720 --> 00:09:15,180
clifford says actually for a general graph that that's also true

132
00:09:15,250 --> 00:09:18,520
and that's not obvious

133
00:09:18,540 --> 00:09:22,500
one direction is not hard although the not hard direction with your in the other

134
00:09:22,500 --> 00:09:24,650
direction is actually quite difficult

135
00:09:24,690 --> 00:09:30,490
not quite difficult but it it's more

136
00:09:30,500 --> 00:09:31,520
OK so

137
00:09:31,530 --> 00:09:34,310
all come back to that in a minute let me just start giving you some

138
00:09:36,080 --> 00:09:39,970
this is an example that we've i've already does mention the markov chain

139
00:09:40,140 --> 00:09:44,640
what i'm doing here is actually implementing it to make it what's known as a

140
00:09:44,640 --> 00:09:47,030
hidden markov model

141
00:09:47,040 --> 00:09:55,690
so hidden markov model you have both these variables x that are evolving over time

142
00:09:55,890 --> 00:09:57,610
and you also have

143
00:09:58,700 --> 00:10:03,450
variables hanging off the axes these are wise these are shaded in because that's typically

144
00:10:03,450 --> 00:10:05,060
what you observe

145
00:10:05,080 --> 00:10:10,800
right so that actually the visit big success of of markov models are one of

146
00:10:10,800 --> 00:10:18,020
the really big successes is in automatic speech recognition which i personally don't like because

147
00:10:18,020 --> 00:10:21,620
it's part of the reason that you know whenever you call a phone number for

148
00:10:21,680 --> 00:10:25,800
a company you get a computer these days you don't get customer service

149
00:10:25,810 --> 00:10:28,100
right but speech recognition

150
00:10:28,110 --> 00:10:34,850
you know speech models is way speech evolves least humans can be modelled very natural

151
00:10:34,860 --> 00:10:39,810
is a markov process to sort of the sequential dependence in the way that people

152
00:10:39,810 --> 00:10:45,830
are forming phonemes and the problem of speech recognition is exactly it's well modeled by

153
00:10:45,830 --> 00:10:50,040
exactly this graph of course along graph so these are the sort of hidden phonemes

154
00:10:50,300 --> 00:10:55,000
the sort of units of speech that you're trying to infer and the shaded circles

155
00:10:55,000 --> 00:10:58,300
those are other the random variables that's what you observe so if you're speaking over

156
00:10:58,300 --> 00:11:01,760
the phone you're getting some sort of noisy version of what the person is really

157
00:11:01,760 --> 00:11:06,370
saying or things like you know people might not be fluent speakers if i speak

158
00:11:06,370 --> 00:11:12,100
french there be some certain amount of noise we say for the speech recognition devices

159
00:11:12,100 --> 00:11:13,430
and so on

160
00:11:14,800 --> 00:11:20,050
so it's an instance of this is what the computer services noisy instance what is

161
00:11:20,050 --> 00:11:23,230
trying to infer is the actual phonemes and then it would pass them to get

162
00:11:23,230 --> 00:11:27,180
words and what is exploiting is that there's a lot of temporal dependence in the

163
00:11:27,180 --> 00:11:33,450
sequence of all words phonemes at all levels of language there's tons of temple dependent

164
00:11:33,730 --> 00:11:38,230
so really the successive of automatic speech recognition is an instance of this

165
00:11:38,230 --> 00:11:42,240
and it's an instance of some algorithms talk about tomorrow

166
00:11:42,260 --> 00:11:45,440
some of these message passing algorithms

167
00:11:45,470 --> 00:11:50,880
so markov models are great but

168
00:11:50,890 --> 00:11:53,440
there's many things you want to do in practice that are

169
00:11:53,440 --> 00:11:54,790
more complicated

170
00:11:54,870 --> 00:12:00,630
what various people have done an interesting sort of extension of speech recognition is if

171
00:12:00,630 --> 00:12:04,550
you have a video that has both audio tracks and it has speaking

172
00:12:04,600 --> 00:12:08,680
let's assume that multiple people are speaking and

173
00:12:08,720 --> 00:12:14,160
hence multiples people's lips are moving or maybe it's slightly different times if you want

174
00:12:14,220 --> 00:12:19,030
to source all the different names for the cocktail party problem

175
00:12:19,140 --> 00:12:21,190
you have many people speaking

176
00:12:21,190 --> 00:12:25,840
and you'd like to separate the audio track into multiple tracks to get each person's

177
00:12:26,750 --> 00:12:30,370
one way you can do that better is if you have a video

178
00:12:30,430 --> 00:12:35,940
and then you can somehow use the synchrony between my lips moving informing certain things

179
00:12:35,940 --> 00:12:37,990
and what's been said on the audio

180
00:12:38,260 --> 00:12:42,290
so now what you have is you have two temporal models you have to markov

181
00:12:42,290 --> 00:12:44,990
chains you have the video evolving over time

182
00:12:45,040 --> 00:12:47,550
you have the audio evolving over time

183
00:12:47,680 --> 00:12:52,410
so you'd like to model both the temporal dependencies would also like to synchronize

184
00:12:52,470 --> 00:12:55,600
if you'd like to synchronize across streams

185
00:12:55,660 --> 00:12:59,300
you might actually have multiple cameras if you're lucky you might have camera one camera

186
00:12:59,310 --> 00:13:00,840
two audio one

187
00:13:00,890 --> 00:13:04,450
and you have a whole coupled set of markov chains

188
00:13:04,450 --> 00:13:06,370
so this is a much richer model

189
00:13:06,390 --> 00:13:08,430
and we're going to see that

190
00:13:08,430 --> 00:13:13,440
is maybe and so we'll have something to do with the entropy and shannon information content and that's the thing

191
00:13:14,090 --> 00:13:14,520
and that would

192
00:13:14,990 --> 00:13:17,040
again reinforced the claims we've been

193
00:13:18,840 --> 00:13:19,930
and then the practical question

194
00:13:20,440 --> 00:13:25,850
is taste of theory i just wanna know how to make a good one how to make good symbol codes

195
00:13:30,270 --> 00:13:32,100
and when we say good or optimal

196
00:13:33,900 --> 00:13:35,410
we need to define what we mean by

197
00:13:41,380 --> 00:13:42,280
so let's do at

198
00:13:47,110 --> 00:13:50,820
what do we mean by optimal well after compression so we want a short

199
00:13:52,330 --> 00:13:55,730
link there are some really that's our main objective function

200
00:13:56,680 --> 00:13:57,490
so there may be others

201
00:13:59,170 --> 00:14:00,900
so the find the expected length

202
00:14:06,560 --> 00:14:07,810
or at ensemble

203
00:14:11,300 --> 00:14:12,740
on the ensemble being

204
00:14:13,130 --> 00:14:15,370
the alphabet with all its probabilities

205
00:14:18,200 --> 00:14:21,470
well defined it can be out of three at x is

206
00:14:21,940 --> 00:14:24,480
some overall your elements in the alphabet

207
00:14:25,340 --> 00:14:26,920
and multiply the probability of

208
00:14:28,110 --> 00:14:28,690
the outcome

209
00:14:29,350 --> 00:14:32,720
by the length that you have assigned with all i

210
00:14:33,250 --> 00:14:34,240
is the length

211
00:14:35,360 --> 00:14:36,470
in bits of

212
00:14:39,870 --> 00:14:42,010
the code that you've assigned to be i

213
00:14:43,160 --> 00:14:47,780
haven't okay nothing profound going on here and just defining the expected length

214
00:14:48,800 --> 00:14:50,040
and it's probably a good idea to

215
00:14:50,590 --> 00:14:52,260
immediately use an example

216
00:15:04,600 --> 00:15:06,360
so let's give ourselves an alphabet

217
00:15:07,000 --> 00:15:08,560
now what's going to be eh

218
00:15:12,550 --> 00:15:14,110
and let's have a probability distribution

219
00:15:20,270 --> 00:15:20,840
one eight

220
00:15:22,110 --> 00:15:22,650
this is a toy

221
00:15:23,330 --> 00:15:24,530
probability distribution with

222
00:15:25,110 --> 00:15:26,850
on even probabilities

223
00:15:27,560 --> 00:15:29,250
one thing we might want to know about this

224
00:15:30,760 --> 00:15:34,810
ensemble is what is its entropy and the answer is half-time model

225
00:15:35,900 --> 00:15:36,980
plus a quarter times to

226
00:15:38,250 --> 00:15:38,910
less than eight

227
00:15:39,960 --> 00:15:41,060
three thousand eight

228
00:15:44,230 --> 00:15:49,710
these one two three and three being respectively the logspace to of this this this and this

229
00:15:53,030 --> 00:15:53,850
and that's is

230
00:15:55,430 --> 00:15:56,010
one intercourse

231
00:16:04,910 --> 00:16:05,650
give ourselves

232
00:16:06,180 --> 00:16:10,470
symbol code and then try and make it better and we need pin down what

233
00:16:10,470 --> 00:16:13,380
the rules of the game are actually going going to be here

234
00:16:16,530 --> 00:16:18,030
so what do we want

235
00:16:18,850 --> 00:16:20,270
let's let's define it

236
00:16:21,380 --> 00:16:21,980
the rules

237
00:16:25,010 --> 00:16:26,720
first and then look at some examples

238
00:16:30,240 --> 00:16:34,520
the first rule is it had better work when someone code this and then someone

239
00:16:34,520 --> 00:16:38,320
else and decodes the better get back what we first put in so what comes

240
00:16:38,320 --> 00:16:40,260
up must be what comes in

241
00:16:42,830 --> 00:16:44,590
we wanted to decode

242
00:16:48,630 --> 00:16:52,770
we wanted to guarantee to decode correctly not just have a ninety nine point nine

243
00:16:52,770 --> 00:16:55,850
percent chance of working which is already quite good in our

244
00:16:56,300 --> 00:16:58,150
a slightly silly approach last time

245
00:16:59,070 --> 00:17:01,470
we wanted to always decode correctly

246
00:17:05,890 --> 00:17:10,190
let's just finer the rules and then say a bit more precisely what we what we mean by

247
00:17:10,600 --> 00:17:10,960
by this

248
00:17:11,680 --> 00:17:15,460
ideally we'd like it to be easy to decode as well

249
00:17:16,050 --> 00:17:17,050
so we don't have

250
00:17:17,690 --> 00:17:23,410
the recipient having just spent hours staring at the codebook trying to figure out where

251
00:17:23,410 --> 00:17:28,860
the punctuation where the missing punctuation marks go in these in this received string

252
00:17:30,680 --> 00:17:33,160
so easy to decode would be nice to have

253
00:17:34,220 --> 00:17:37,070
but is not essential if if we can get a big win

254
00:17:37,490 --> 00:17:40,280
on this final objective which is small expected length

255
00:17:42,740 --> 00:17:43,610
small el

256
00:17:48,650 --> 00:17:54,140
let's just be precise about what we mean by it's got the code right we want the code to be

257
00:17:54,750 --> 00:17:56,630
uniquely decodable

258
00:18:01,600 --> 00:18:07,290
so when you receive an encoded string there should be no ambiguity about what was said so

259
00:18:12,380 --> 00:18:13,610
string x

260
00:18:14,820 --> 00:18:16,640
hand any other string why

261
00:18:17,880 --> 00:18:18,880
such that's

262
00:18:20,130 --> 00:18:21,490
x is not the same as one

263
00:18:23,710 --> 00:18:24,870
we want the encoding

264
00:18:25,540 --> 00:18:26,420
of x

265
00:18:27,250 --> 00:18:27,940
not equal

266
00:18:31,490 --> 00:18:33,830
so that's the definition of unique decodability

267
00:18:47,590 --> 00:18:50,010
andy just check my notes section is clear

268
00:18:51,620 --> 00:18:56,730
the is the way we encode a particular element alphabet and then have see all

269
00:18:56,730 --> 00:18:59,260
the string like that's that's just a concatenation

270
00:18:59,710 --> 00:19:00,710
of thanks from

271
00:19:01,480 --> 00:19:01,910
thanks to

272
00:19:04,710 --> 00:19:07,710
so i'm overloading the symbol see to me

273
00:19:08,640 --> 00:19:12,780
the way to encode a single symbol or it could be the encoding of an entire string

274
00:19:15,330 --> 00:19:17,590
okay we've got ourselves an ensemble let's define

275
00:19:19,120 --> 00:19:21,470
some simple codes and have a think about these

276
00:19:21,930 --> 00:19:25,170
object is what it means is say it's gonna be uniquely decodable what is

277
00:19:25,540 --> 00:19:29,140
what does that imply how do we get a small expected length

278
00:19:29,930 --> 00:19:31,440
is this a good start for english

279
00:19:31,440 --> 00:19:32,460
to something

280
00:19:32,480 --> 00:19:33,930
and almost

281
00:19:33,940 --> 00:19:37,860
o also of data may contain huge amount of noise

282
00:19:37,870 --> 00:19:42,390
and if you want to detect anomalies in the noise data this is quite different

283
00:19:42,390 --> 00:19:45,070
and difficult because

284
00:19:46,750 --> 00:19:49,490
sometimes a regular

285
00:19:49,500 --> 00:19:50,630
the values

286
00:19:51,670 --> 00:19:52,980
chapman into

287
00:19:52,990 --> 00:19:57,460
in the noisy environment this is especially true for time series data and temporal

288
00:19:58,430 --> 00:20:02,630
when you're trying to detect anomalous subsequences so for example if an animal

289
00:20:02,670 --> 00:20:04,940
anomalous subsequence is

290
00:20:06,300 --> 00:20:09,070
for any reason the different than anything else

291
00:20:09,080 --> 00:20:13,060
if you have the noisy data single individual instances may not be

292
00:20:13,590 --> 00:20:17,240
maybe in the perfect range of so few examples it would be

293
00:20:17,260 --> 00:20:19,000
clear to what

294
00:20:19,020 --> 00:20:23,070
so also in the world data

295
00:20:23,090 --> 00:20:27,680
normal behavior keeps evolving so for example if you create the model

296
00:20:27,690 --> 00:20:32,820
when you want to detect anomalies this behavior is changing over time so you the

297
00:20:32,820 --> 00:20:36,140
model should be updated as so the question is

298
00:20:36,170 --> 00:20:40,130
how frequently you need to be the model and you know how you do this

299
00:20:40,130 --> 00:20:43,970
one because the computational efficiency is very important

300
00:20:43,980 --> 00:20:47,700
because if you distance data coming at enormous speed

301
00:20:47,750 --> 00:20:50,330
so you have to figure out how fast into this

302
00:20:50,340 --> 00:20:52,430
and of course

303
00:20:52,490 --> 00:20:55,450
if you have thousands of features in your application

304
00:20:55,670 --> 00:21:00,260
all these thousand features important which features are important for anomaly detection this is not

305
00:21:00,260 --> 00:21:04,280
the classical feature selection problem like in every data mining problem definition you have the

306
00:21:04,280 --> 00:21:05,640
class label

307
00:21:05,660 --> 00:21:10,230
here we usually do not have the class label in trying to identify what

308
00:21:10,270 --> 00:21:13,230
relevant features is not that is not easy

309
00:21:13,240 --> 00:21:16,280
so different people are doing a different kind of

310
00:21:16,290 --> 00:21:19,660
feature projection and so on in order to protect the

311
00:21:19,670 --> 00:21:23,090
among in different dimensions

312
00:21:23,100 --> 00:21:28,130
so there are several aspects of anomaly detection people try to cover here

313
00:21:28,340 --> 00:21:32,560
first what kind of the data we had the nature of input data what kind

314
00:21:33,200 --> 00:21:35,460
labels do have

315
00:21:35,470 --> 00:21:36,730
type of anomalies

316
00:21:36,740 --> 00:21:38,870
but i'll alex this means

317
00:21:38,910 --> 00:21:43,040
output what kind of output the animal protection of data is provided to

318
00:21:43,050 --> 00:21:46,520
and also how you do evaluation of anomaly detection techniques

319
00:21:46,530 --> 00:21:56,650
so let's assume that you have some data most of the data is a

320
00:21:56,660 --> 00:22:01,640
it's coming into the form of the univariate or multivariate and it is the only

321
00:22:01,640 --> 00:22:03,550
way they usually have

322
00:22:03,580 --> 00:22:06,230
just one one available which is

323
00:22:06,270 --> 00:22:08,620
not very often the case

324
00:22:08,630 --> 00:22:11,180
and if you have a multivariate

325
00:22:12,030 --> 00:22:16,840
future of maybe tens hundreds sometimes thousands of variables

326
00:22:16,920 --> 00:22:22,330
and you have to find anomalies in this dimensional space

327
00:22:22,350 --> 00:22:25,540
so the question is

328
00:22:25,550 --> 00:22:26,770
if you have

329
00:22:26,780 --> 00:22:32,020
anomalies that appear in only one feature that is find but for example

330
00:22:32,080 --> 00:22:37,090
if anomalies not obvious in any of the individual features

331
00:22:37,100 --> 00:22:39,990
how to find these anomalous and was the right

332
00:22:40,060 --> 00:22:42,430
the size of the itemsets difficult

333
00:22:42,700 --> 00:22:48,590
so also you may deal with different kind of

334
00:22:48,600 --> 00:22:52,370
the type of activity also familiar with all of this from the

335
00:22:52,410 --> 00:22:55,260
money perspective you may have binary attributes

336
00:22:55,260 --> 00:23:01,950
you may have categorical attributes continuous and hybrid for example if you're dealing with

337
00:23:02,000 --> 00:23:03,710
network intrusion detection

338
00:23:03,720 --> 00:23:08,630
you may end up using source IP destination IP addresses and sometimes it's not clear

339
00:23:08,630 --> 00:23:10,740
how to convert this IP

340
00:23:10,750 --> 00:23:15,040
and i think this is into the future that you can use in your calculation

341
00:23:15,060 --> 00:23:24,510
OK so we also have different data types you may dealing with sequential data for

342
00:23:24,510 --> 00:23:25,950
example stock

343
00:23:26,600 --> 00:23:29,260
market data collecting from

344
00:23:29,280 --> 00:23:32,260
some engines your car lane

345
00:23:32,290 --> 00:23:36,310
also you may have the dynamics data like this is

346
00:23:36,330 --> 00:23:38,950
genes data you may have to

347
00:23:39,000 --> 00:23:40,390
data from the traffic

348
00:23:40,410 --> 00:23:43,900
know how certain carson moving someone to the accident accidents

349
00:23:44,780 --> 00:23:47,000
if you have heard sounds data

350
00:23:47,040 --> 00:23:48,910
if you want

351
00:23:48,920 --> 00:23:53,630
you have basically spatial and temporal data because you have two dimensions

352
00:23:53,630 --> 00:23:57,850
also this is some data from from the ports all across the US and sometimes

353
00:23:57,850 --> 00:24:02,100
it's important to understand the location as well as other attributes to see that sometimes

354
00:24:02,100 --> 00:24:06,140
you have to make sure all these attributes plus spatial and temporal domain and you

355
00:24:06,140 --> 00:24:10,510
have to deal with all these issues in order to detect anomalies

356
00:24:10,870 --> 00:24:14,500
according to the data labels that are available to you

357
00:24:14,520 --> 00:24:20,510
you may have seen situations when we talking about supervised anomaly detection in this case

358
00:24:20,620 --> 00:24:22,380
the labels for both normal

359
00:24:22,390 --> 00:24:27,410
and anomalies are available to you so in this case you just supplying some classification

360
00:24:27,550 --> 00:24:34,470
techniques in order to detect both normal and anomaly core for whatever

361
00:24:34,480 --> 00:24:39,630
since this is usually imbalanced datasets usually have to tweak

362
00:24:39,650 --> 00:24:44,160
classification of interest to work on this kind of the problems different people were doing

363
00:24:44,160 --> 00:24:47,560
different things and i'll have a small section

364
00:24:48,700 --> 00:24:53,490
also if you can labels available only from the normal may be talking about some

365
00:24:53,490 --> 00:24:59,600
unsupervised anomaly detection techniques in this case usually build some data-mining the model to represent

366
00:24:59,600 --> 00:25:00,990
this normal behavior

367
00:25:01,030 --> 00:25:04,920
and then you are trying to detect deviations from that normal behavior

368
00:25:04,920 --> 00:25:08,860
and then i i can look at aragon i look at your target and then

369
00:25:08,860 --> 00:25:12,840
i i tried to come up is an example that screws up here again

370
00:25:12,850 --> 00:25:16,430
right so even the class and the is is really huge

371
00:25:16,490 --> 00:25:20,490
then maybe i will be able to pick some such an MDP by the class

372
00:25:20,490 --> 00:25:22,650
of family business pretty small

373
00:25:22,730 --> 00:25:26,810
the and surely maybe if if the classifier and if it just contains the single

374
00:25:26,810 --> 00:25:27,820
and EP

375
00:25:27,890 --> 00:25:29,520
then found

376
00:25:29,540 --> 00:25:30,900
there is nothing single

377
00:25:30,920 --> 00:25:32,390
i could write so

378
00:25:32,400 --> 00:25:34,870
because there are good could be very

379
00:25:35,570 --> 00:25:38,130
that you need to that single and EP

380
00:25:38,130 --> 00:25:42,170
and then i cannot come come up with a counterexample right so we knew that

381
00:25:42,840 --> 00:25:47,890
a largish class fmdps and the question is how large the class family has to

382
00:25:49,480 --> 00:25:53,190
and it turns out to be that this is really the best class of mdps

383
00:25:53,260 --> 00:25:54,920
in my opinion at least

384
00:25:56,690 --> 00:25:58,900
the transition probabilities

385
00:25:58,930 --> 00:25:59,930
could be

386
00:25:59,940 --> 00:26:03,420
any uniformly lipschitz transition

387
00:26:03,490 --> 00:26:08,040
probably is what do we mean by saying you know but small so the idea

388
00:26:08,040 --> 00:26:09,680
of small spaces that

389
00:26:10,160 --> 00:26:16,180
closings resulting closings c find that states which are close by

390
00:26:16,200 --> 00:26:22,730
and i'm taking the same actions then there is something transition probabilities of sorry that

391
00:26:22,740 --> 00:26:24,120
is acting

392
00:26:24,130 --> 00:26:27,850
probability distributions are going to be close to each other so that

393
00:26:27,870 --> 00:26:28,810
the idea

394
00:26:29,530 --> 00:26:34,010
and the idea is that you have this live has property that if these two

395
00:26:34,010 --> 00:26:38,410
states i'm getting closer to each other than the probability distributions are getting closer to

396
00:26:38,410 --> 00:26:39,750
each other

397
00:26:39,890 --> 00:26:43,220
OK so that's one constraint on the class family piece

398
00:26:43,280 --> 00:26:45,530
the constraint is is

399
00:26:45,540 --> 00:26:47,820
she showing this class

400
00:26:47,870 --> 00:26:50,000
by quite a big stand

401
00:26:50,060 --> 00:26:55,800
so that constraints is that the transition probabilities and the rewards are uniformly bounded

402
00:26:55,820 --> 00:26:59,040
so why is this a very strong constraints

403
00:26:59,060 --> 00:27:03,300
it's a very strong constraint because if you take the gaussian

404
00:27:03,360 --> 00:27:06,460
and you increase the dimensionality

405
00:27:06,490 --> 00:27:07,970
and you keep the

406
00:27:10,730 --> 00:27:12,240
very and speaks

407
00:27:12,250 --> 00:27:16,620
there and the height of the gaussian is going to grow to infinity with n

408
00:27:17,500 --> 00:27:18,820
so this means

409
00:27:18,820 --> 00:27:21,390
that if you have a uniformly bounded

410
00:27:21,420 --> 00:27:25,980
transition probs if you have a uniform bound the transition probabilities

411
00:27:26,030 --> 00:27:32,080
then you're rotating gold is gaussian zeroing out noise in the transitions which we would

412
00:27:32,080 --> 00:27:36,780
have to fix various you have to as you increase the dimensionality

413
00:27:36,800 --> 00:27:40,870
you have to increase the benefits you have to increase the variance

414
00:27:40,890 --> 00:27:42,590
so your problems become

415
00:27:42,590 --> 00:27:47,820
more and more noisy in chennai noise should be helpful in this class of problems

416
00:27:47,830 --> 00:27:53,970
why because well in the limit if your problem is very very noisy

417
00:27:54,920 --> 00:27:57,530
you have no control right

418
00:27:57,530 --> 00:27:58,860
so in the limit

419
00:27:58,880 --> 00:28:02,740
if i if the noise is so large that no matter what action i am

420
00:28:02,740 --> 00:28:07,160
taking i'm pretty much everywhere in the state space so that the

421
00:28:07,180 --> 00:28:08,510
the limit for

422
00:28:08,560 --> 00:28:10,110
a large noise

423
00:28:10,120 --> 00:28:13,610
that i don't have any control them was the value function

424
00:28:13,630 --> 00:28:19,280
it will be constant everywhere so it's very small so it's very easy to learn

425
00:28:19,330 --> 00:28:23,650
OK so large noise generally have in this problems

426
00:28:23,660 --> 00:28:26,300
when you want to learn a value function

427
00:28:26,330 --> 00:28:30,250
and the this condition he laments that

428
00:28:30,310 --> 00:28:34,260
so it seems that you cannot have large noise in the

429
00:28:36,720 --> 00:28:39,100
this seems like

430
00:28:39,100 --> 00:28:40,580
that's right

431
00:28:40,630 --> 00:28:45,900
so we should start thinking about five functions than because this and just say that

432
00:28:45,990 --> 00:28:50,600
if you want to estimate value functions for what is reasonable cost family is so

433
00:28:50,600 --> 00:28:53,150
you have smallest control problems

434
00:28:53,170 --> 00:28:58,550
the noise is pretty large and the control problem and yet you are not able

435
00:28:58,560 --> 00:29:01,190
to come up with an efficient target

436
00:29:02,000 --> 00:29:05,000
maybe there's not much overlap

437
00:29:05,050 --> 00:29:12,550
so we should go to the beach because the sun just came out

438
00:29:12,600 --> 00:29:17,180
anyway so it turns out that you can do a few things so monte carlo

439
00:29:17,180 --> 00:29:19,600
concert risky game

440
00:29:21,340 --> 00:29:26,280
actually the game is going to be that that we have this negative result

441
00:29:26,330 --> 00:29:30,410
and it you have to do that the conditions some all you have to change

442
00:29:30,600 --> 00:29:35,300
the problem otherwise value there is no chance of a full forthcoming coming this by

443
00:29:35,320 --> 00:29:39,160
them right so the first thing that we are trying to do is that we

444
00:29:39,160 --> 00:29:40,260
say that

445
00:29:40,290 --> 00:29:45,050
well maybe it's enough computing of a function just forget about the phi functions

446
00:29:45,070 --> 00:29:47,140
just come up with good actions

447
00:29:49,140 --> 00:29:51,890
that will be the next thing that that we try to do

448
00:29:51,900 --> 00:29:55,150
and so these given was proposed by

449
00:29:55,170 --> 00:29:58,640
currents and his colleagues in two thousand two

450
00:29:58,860 --> 00:30:01,130
i guess this newspaper

451
00:30:01,150 --> 00:30:05,370
so the idea is is very very simple and it's classic so if if you

452
00:30:05,370 --> 00:30:08,510
forget about all the stochastic stop

453
00:30:08,570 --> 00:30:11,710
and you face the planning problem one the

454
00:30:11,780 --> 00:30:14,970
european the search tree right

455
00:30:15,030 --> 00:30:19,610
two latin each and you have to choose a good action so classically it would

456
00:30:20,830 --> 00:30:23,140
so you've given state

457
00:30:23,160 --> 00:30:25,030
and you want to choose a good action

458
00:30:25,040 --> 00:30:28,910
but you don't know what the action is so you try all of them

459
00:30:28,930 --> 00:30:31,170
if you look at that lead

460
00:30:31,260 --> 00:30:33,220
and from those states

461
00:30:34,390 --> 00:30:37,820
what do you do you have to try these actions until you get to the

462
00:30:38,700 --> 00:30:43,160
or until in this case we are considering this content problems

463
00:30:43,210 --> 00:30:47,040
and so it turns out that if you do this this content just think about

464
00:30:47,040 --> 00:30:47,970
it like

465
00:30:48,070 --> 00:30:52,630
so we had this discounted sum

466
00:30:53,460 --> 00:30:59,730
some forgot to the prior p times the rewards if the rewards are bonded

467
00:30:59,740 --> 00:31:01,930
i can try this summer

468
00:31:01,940 --> 00:31:04,310
after a certain number of turns right

469
00:31:04,310 --> 00:31:06,900
i want change the values too much

470
00:31:06,950 --> 00:31:08,830
why because the tail

471
00:31:08,840 --> 00:31:13,640
if i start to some from capital t to infinity

472
00:31:13,690 --> 00:31:17,870
and this guy said bonded let's say by one

473
00:31:18,750 --> 00:31:20,360
in absolute value

474
00:31:20,450 --> 00:31:23,450
then this whole thing is going to be

475
00:31:23,460 --> 00:31:24,600
born the

476
00:31:24,630 --> 00:31:28,210
by going on to the part of captain that he right so i can just

477
00:31:28,210 --> 00:31:29,620
put it out

478
00:31:29,640 --> 00:31:34,860
otherwise this semantics some sounds to one over one minus common for this whole thing

479
00:31:34,860 --> 00:31:36,300
is only by

480
00:31:36,350 --> 00:31:41,130
government to the power of that if i start to some after cavity

481
00:31:41,170 --> 00:31:42,190
five just

482
00:31:42,200 --> 00:31:43,360
for about

483
00:31:43,360 --> 00:31:45,590
and missing after

484
00:31:45,640 --> 00:31:47,680
a certain amount of time

485
00:31:47,700 --> 00:31:51,120
because this is going to be a very small number so if i want to

486
00:31:51,120 --> 00:31:52,760
compare the values

487
00:31:52,810 --> 00:31:55,900
so we want to compute the values of the actions if you want to compare

488
00:31:56,140 --> 00:31:58,630
compare the values of two actions

489
00:31:58,650 --> 00:32:02,170
you don't have to build this free after infinity

490
00:32:02,200 --> 00:32:04,630
so you can stop after a lot

491
00:32:05,930 --> 00:32:09,430
so you don't need a superposition so just want to come up with a good

492
00:32:09,430 --> 00:32:11,050
enough action

493
00:32:11,060 --> 00:32:12,860
and so this is the the

494
00:32:12,870 --> 00:32:15,330
effective for rice and so this is

495
00:32:15,390 --> 00:32:21,480
if you are shooting at selection accuracy that and k i is born and the

496
00:32:21,480 --> 00:32:27,220
today i'm going to be following up on the lectures on graphical models just to

497
00:32:27,220 --> 00:32:31,700
recap what we did last time i comfort

498
00:32:31,750 --> 00:32:33,680
the basics of

499
00:32:33,720 --> 00:32:41,340
factor graphs undirected graphs and directed graphs what they are how they represent conditional independence

500
00:32:41,340 --> 00:32:43,130
and then i talked about

501
00:32:43,140 --> 00:32:45,100
inference algorithms

502
00:32:45,120 --> 00:32:49,870
namely belief propagation factor graph propagation and a little bit

503
00:32:49,890 --> 00:32:54,980
about the junction tree algorithm elimination other ways of doing inference

504
00:32:56,680 --> 00:33:01,060
today we're going to do is we're going to be talking about

505
00:33:03,020 --> 00:33:04,600
graphs from data

506
00:33:04,610 --> 00:33:10,310
in particular in the talk first about learning the parameters of the graph from data

507
00:33:10,350 --> 00:33:13,150
and then we're going to be talking about learning the structure of a graph from

508
00:33:13,150 --> 00:33:16,150
data and they're all

509
00:33:16,160 --> 00:33:20,210
in the second half of give you a little bit more of a

510
00:33:20,230 --> 00:33:26,170
sort of research flavor question in graphical models and try to relate it to other

511
00:33:26,170 --> 00:33:28,150
lecturers at the summer school

512
00:33:28,160 --> 00:33:32,490
so you can hopefully see relationships between a lot of these different topics

513
00:33:32,550 --> 00:33:34,900
so let's just dive right in

514
00:33:34,920 --> 00:33:39,790
to learning the parameters of a graph from data

515
00:33:39,800 --> 00:33:45,610
and let's start out with directed graphical models like this graph here

516
00:33:45,650 --> 00:33:51,390
i'll talk about undirected graphical models later undirected graphs are harder in some way so

517
00:33:51,510 --> 00:33:53,300
i leave that for later

518
00:33:53,320 --> 00:34:00,660
so here's the a directed graph over four variables and it corresponds to the following

519
00:34:00,660 --> 00:34:05,460
factorisation of the joint distribution over these four variables and this should by now be

520
00:34:05,820 --> 00:34:10,610
incredibly familiar to you because most electors have used this kind of thing

521
00:34:10,630 --> 00:34:14,560
so what do i mean by the parameters of this model

522
00:34:14,600 --> 00:34:17,810
so let's look at this graph in this factors station

523
00:34:17,820 --> 00:34:18,990
it doesn't

524
00:34:19,040 --> 00:34:24,070
define the probability distribution over these variables just to say that there are factored in

525
00:34:24,070 --> 00:34:27,120
this way right we need to actually

526
00:34:27,550 --> 00:34:31,670
consider each of these terms and define

527
00:34:31,690 --> 00:34:37,920
of what that term actually is how would you implement something that would generate values

528
00:34:37,950 --> 00:34:41,350
x two given values of x one

529
00:34:41,370 --> 00:34:47,280
and for simplicity i'm going to start out by talking about discrete variables

530
00:34:47,300 --> 00:34:49,350
so assume that each variable

531
00:34:49,360 --> 00:34:54,740
x is discrete and can take on k i values so we don't need to

532
00:34:54,740 --> 00:34:56,850
all take the same number of values

533
00:34:56,890 --> 00:34:57,950
for example

534
00:34:58,710 --> 00:35:01,940
in particular let's look at one of the variables

535
00:35:01,960 --> 00:35:04,950
next to it depends on x one

536
00:35:04,960 --> 00:35:09,090
so there are some parameter that relates x two to x one

537
00:35:09,310 --> 00:35:13,970
if x one is a binary variable x two can take on

538
00:35:13,980 --> 00:35:15,920
three values

539
00:35:15,930 --> 00:35:19,840
then the relationship between x one and x two is given by this to by

540
00:35:19,840 --> 00:35:22,110
three table

541
00:35:22,130 --> 00:35:24,210
and i

542
00:35:24,810 --> 00:35:27,180
rows of the table represent

543
00:35:27,190 --> 00:35:30,290
possible values of x two given x one

544
00:35:30,340 --> 00:35:33,950
and because this is the probability distribution over x two given

545
00:35:33,960 --> 00:35:35,590
the value of x one

546
00:35:35,600 --> 00:35:38,390
these rows sum to one

547
00:35:38,410 --> 00:35:40,140
OK everybody happy with that

548
00:35:42,510 --> 00:35:48,150
it's just an example of a particular case here in general the parameters of this

549
00:35:49,350 --> 00:35:52,690
it can be represented as four tables

550
00:35:52,710 --> 00:35:56,300
one for each variable given its parents

551
00:35:57,820 --> 00:36:02,080
they one is the table four x one given its parents well x one doesn't

552
00:36:02,080 --> 00:36:06,220
have any parents north and something so

553
00:36:06,330 --> 00:36:10,110
it has only k one

554
00:36:10,170 --> 00:36:14,120
entries it's one by k one table

555
00:36:14,140 --> 00:36:19,210
they did two has k one by k two entries et cetera

556
00:36:19,230 --> 00:36:22,780
and these tables are called conditional probability tables

557
00:36:22,790 --> 00:36:25,650
and they have the following semantics

558
00:36:26,330 --> 00:36:31,720
theta one k

559
00:36:31,770 --> 00:36:33,390
for this valuable here

560
00:36:33,400 --> 00:36:37,800
is the probability that x one takes on the value k

561
00:36:37,820 --> 00:36:39,450
more generally

562
00:36:39,470 --> 00:36:43,430
we have a variable given its parents

563
00:36:44,240 --> 00:36:45,560
theta two

564
00:36:45,570 --> 00:36:47,820
OK OK prime

565
00:36:47,830 --> 00:36:52,220
it is the probability that x who takes on the value k prime

566
00:36:52,240 --> 00:36:56,430
given that x one its parent took on the value k

567
00:36:56,480 --> 00:36:59,390
all right this is all very straightforward so i just want to lay out the

568
00:36:59,390 --> 00:37:03,590
semantics make sure everybody is happy to ask me questions for sure

569
00:37:03,640 --> 00:37:05,750
if you're not happy

570
00:37:05,770 --> 00:37:10,090
now this this is a nice simple picture but imagine

571
00:37:10,130 --> 00:37:12,510
we had a variable

572
00:37:13,550 --> 00:37:18,210
it had more than one parent so let's it had two parents

573
00:37:18,230 --> 00:37:19,320
for example

574
00:37:19,330 --> 00:37:25,780
if the node has m parents then there are two ways of representing the conditional

575
00:37:25,780 --> 00:37:28,820
probability tables and they're completely equivalent

576
00:37:28,870 --> 00:37:35,520
one of them is to say that table is represented by an employee's one-dimensional table

577
00:37:35,570 --> 00:37:39,740
one for each of the parents one of those images for each of the parents

578
00:37:39,740 --> 00:37:43,440
and no obviously one dimension for the child and it's over the child mention the

579
00:37:43,450 --> 00:37:45,610
things that the sum to one

580
00:37:46,360 --> 00:37:49,100
so in an impulse one-dimensional table

581
00:37:50,860 --> 00:37:54,420
something that we could do is we could collapse

582
00:37:54,440 --> 00:38:00,100
all the possible states of the parents into one sort of superstate

583
00:38:00,190 --> 00:38:04,790
and just represent things as a two dimensional table

584
00:38:04,810 --> 00:38:10,970
r where the number of super states of all the parents is just the product

585
00:38:10,970 --> 00:38:15,950
over all the parents of the number of possible states that that parents can take

586
00:38:16,920 --> 00:38:21,920
so obviously set of three by three by three table we can represented as in

587
00:38:21,920 --> 00:38:23,930
nine by treatable

588
00:38:23,980 --> 00:38:25,720
OK that's going to be

589
00:38:27,160 --> 00:38:28,290
because then

590
00:38:28,300 --> 00:38:34,040
the number of indices we have on these matrices is it can be variable so

591
00:38:34,040 --> 00:38:37,030
you know our life with just get annoying if we have to have a variable

592
00:38:37,030 --> 00:38:39,690
the fact that

593
00:38:39,840 --> 00:38:42,900
don't know how much

594
00:38:57,090 --> 00:39:02,110
i have not seen

595
00:39:03,310 --> 00:39:06,250
that's line

596
00:39:06,300 --> 00:39:10,140
at the show

597
00:39:20,920 --> 00:39:27,050
and for completion

598
00:39:27,180 --> 00:39:31,440
the thing to do

599
00:39:35,780 --> 00:39:41,190
it's actually from

600
00:39:43,090 --> 00:39:46,580
i'm not sure

601
00:39:50,530 --> 00:39:58,780
i thought

602
00:39:58,790 --> 00:40:04,010
thank you for us

603
00:40:19,460 --> 00:40:22,910
the time

604
00:40:34,100 --> 00:40:39,500
so the question

605
00:40:46,970 --> 00:40:50,420
i ran

606
00:40:57,410 --> 00:41:01,630
i don't

607
00:41:15,080 --> 00:41:17,220
it's a

608
00:41:23,200 --> 00:41:25,800
so that was

609
00:41:28,890 --> 00:41:30,700
so to clarify

610
00:41:30,720 --> 00:41:39,340
that change

611
00:41:39,350 --> 00:41:41,760
and the

612
00:41:48,100 --> 00:41:52,610
i mean that's

613
00:41:54,390 --> 00:42:03,410
you can

614
00:42:08,630 --> 00:42:13,790
so what

615
00:42:13,940 --> 00:42:18,700
that's a very fine

616
00:42:18,790 --> 00:42:24,130
project which i think it's time

617
00:42:33,640 --> 00:42:36,280
in fact

618
00:42:36,280 --> 00:42:39,260
the row space what's in that

619
00:42:40,470 --> 00:42:44,310
it's all combinations of the road

620
00:42:44,320 --> 00:42:45,650
that's natural

621
00:42:45,720 --> 00:42:49,460
we wanna space so we have to take all combinations and we start with the

622
00:42:49,460 --> 00:42:51,780
rows so the roses

623
00:42:51,840 --> 00:42:54,400
spans the rose

624
00:42:54,550 --> 00:42:58,260
on the road the basis for the row space

625
00:42:58,310 --> 00:43:01,280
maybe so maybe not

626
00:43:01,320 --> 00:43:05,750
the rows are a basis for the row space when they're independent but if they're

627
00:43:05,760 --> 00:43:09,950
dependent as in the

628
00:43:11,550 --> 00:43:13,160
my area from last time

629
00:43:13,690 --> 00:43:19,140
they're not those 3 rows are not a basis the row space would would only

630
00:43:19,140 --> 00:43:24,190
be two-dimensional I only need to rose from bases so the row space now what's

631
00:43:24,990 --> 00:43:28,730
it's all combinations of the rows and then

632
00:43:28,750 --> 00:43:32,690
combinations of the rows of X

633
00:43:33,270 --> 00:43:38,670
but I don't like working with row vectors all my vectors have been column vectors

634
00:43:38,670 --> 00:43:41,790
I'd like to stay with column vectors

635
00:43:41,810 --> 00:43:45,280
how can I get a column vectors

636
00:43:45,320 --> 00:43:48,890
out of these rows I transpose the matrix

637
00:43:48,900 --> 00:43:53,190
so if that's OK with you I'm going to transpose the matrix and I'm going

638
00:43:53,190 --> 00:43:54,780
to say all

639
00:43:56,810 --> 00:43:58,150
of the

640
00:44:01,100 --> 00:44:02,820
the follows

641
00:44:02,890 --> 00:44:07,070
a big transport

642
00:44:07,100 --> 00:44:08,690
and that allows me

643
00:44:08,990 --> 00:44:12,150
the you is the convenient notation

644
00:44:12,160 --> 00:44:16,140
the column space of a transpose

645
00:44:19,370 --> 00:44:24,020
nothing you know mathematics went on there we just got some

646
00:44:24,070 --> 00:44:27,200
vectors that were lying down the standard

647
00:44:28,520 --> 00:44:34,840
it means that we can use the column space of A transpose that's telling me

648
00:44:34,840 --> 00:44:37,870
in a nice matrix notation

649
00:44:37,930 --> 00:44:39,020
what the row space

650
00:44:40,690 --> 00:44:44,310
OK and finally

651
00:44:44,320 --> 00:44:46,720
is another null space

652
00:44:46,760 --> 00:44:52,250
the 4th vector the for fundamental space will be

653
00:44:52,280 --> 00:44:57,660
the null space of A transpose before the sky is the null space

654
00:44:57,690 --> 00:45:08,200
of a Trent and of course my notation is the end of a transfer

655
00:45:08,260 --> 00:45:11,950
that's the null space of A transpose

656
00:45:12,010 --> 00:45:15,700
there we don't have a perfect name

657
00:45:15,770 --> 00:45:20,310
for this space as us as a connecting with a

658
00:45:20,320 --> 00:45:25,310
but our usual name is the left null space and I'll show you why in

659
00:45:25,890 --> 00:45:29,550
so often I call this the

660
00:45:29,570 --> 00:45:35,160
just write that were the left null space

661
00:45:42,510 --> 00:45:46,580
so this way we have the row space of a and we switch it to

662
00:45:46,580 --> 00:45:48,930
the column space of A transpose

663
00:45:49,160 --> 00:45:52,320
so we have this space of

664
00:45:52,370 --> 00:45:57,890
guys that I call left null space of A but the good notation is it's

665
00:45:57,890 --> 00:46:05,070
the null space of A transpose OK those therefore space

666
00:46:05,100 --> 00:46:07,340
where are space

667
00:46:07,360 --> 00:46:10,770
what what big space so they can work

668
00:46:10,820 --> 00:46:14,030
when a is there might

669
00:46:19,860 --> 00:46:24,460
in that case the null space of

670
00:46:24,480 --> 00:46:28,690
what's in the null space of A vectors within component

671
00:46:28,700 --> 00:46:34,290
solutions to a x equals 0 so the null space of A is N. R.

672
00:46:36,910 --> 00:46:40,310
within the column space of well columns

673
00:46:40,320 --> 00:46:43,340
how many components of those columns has

674
00:46:43,360 --> 00:46:49,310
so this column space is inputs

675
00:46:49,340 --> 00:46:51,720
and M

676
00:46:51,770 --> 00:46:57,280
what about the column space of A transpose which are adjusted disguised way of saying the rows of

677
00:46:57,280 --> 00:47:01,170
range here's one this is amplitude versus time

678
00:47:01,180 --> 00:47:03,890
and the second one in phase

679
00:47:03,970 --> 00:47:06,410
so i'm going to drive directly beneath

680
00:47:06,420 --> 00:47:11,970
and so the crests line up the troughs lineup so if i take great one

681
00:47:11,970 --> 00:47:13,540
plus rate two

682
00:47:13,620 --> 00:47:15,910
i'm going to add them and i'll get

683
00:47:19,610 --> 00:47:22,410
the additive sum of amplitudes

684
00:47:22,410 --> 00:47:23,930
because are in phase

685
00:47:23,950 --> 00:47:25,740
so this is called

686
00:47:25,780 --> 00:47:27,240
in phase

687
00:47:27,260 --> 00:47:34,170
radiation something or the other term applied to this is constructive interference

688
00:47:38,110 --> 00:47:43,020
that's the one extreme to the other extreme is

689
00:47:43,160 --> 00:47:48,040
totally out of phase so in that case re one of draws i did before

690
00:47:48,090 --> 00:47:49,260
but rate two

691
00:47:49,270 --> 00:47:50,640
all draw

692
00:47:50,660 --> 00:47:52,930
symmetrically opposite

693
00:47:52,950 --> 00:47:56,920
so that in re two

694
00:47:57,090 --> 00:48:01,620
the crest of re to align with the trough of re one

695
00:48:01,670 --> 00:48:05,970
the trough of great to align with the crest of re one and for argument's

696
00:48:05,970 --> 00:48:10,290
sake let's say that these are equal in amplitude so if i have to

697
00:48:10,300 --> 00:48:16,780
raise equal in amplitude and they interfere with the perfectly out of phase

698
00:48:16,800 --> 00:48:23,210
if i had these two signals what i will get is

699
00:48:24,430 --> 00:48:26,480
they will cancel

700
00:48:26,540 --> 00:48:28,220
complete cancellation

701
00:48:28,230 --> 00:48:29,870
so this is called

702
00:48:29,890 --> 00:48:32,400
d struck interference

703
00:48:32,410 --> 00:48:39,540
destructive interference

704
00:48:39,540 --> 00:48:42,600
and those are the only two principles we need

705
00:48:42,660 --> 00:48:44,010
and we can describe

706
00:48:44,020 --> 00:48:45,110
and very

707
00:48:45,120 --> 00:48:48,850
a reasonable level what's going on inside

708
00:48:50,040 --> 00:48:53,290
which is being irradiated by

709
00:48:53,350 --> 00:48:58,090
electromagnetic radiation in the x region of the spectrum is on the uses these two

710
00:48:59,270 --> 00:49:02,280
so let's not go to a simple

711
00:49:02,280 --> 00:49:03,650
model here of

712
00:49:06,090 --> 00:49:09,700
world rawson here

713
00:49:09,710 --> 00:49:11,400
there's the talk

714
00:49:12,700 --> 00:49:14,680
let's go down to second

715
00:49:14,780 --> 00:49:16,410
make this cubic

716
00:49:17,170 --> 00:49:19,280
so here's two planes of atoms

717
00:49:19,310 --> 00:49:23,760
and let's say i've got incident light coming in at some angle

718
00:49:24,640 --> 00:49:28,920
here's some incident light coming in at some angle theta

719
00:49:31,330 --> 00:49:34,720
so one re i'm showing striking this at

720
00:49:34,740 --> 00:49:39,700
and if i observe the laws of specular reflection this will reflect from the mayor

721
00:49:39,720 --> 00:49:41,910
at the identical angle

722
00:49:41,930 --> 00:49:46,600
so the angle of reflection equals the angle of incidence and let's put a secondary

723
00:49:46,640 --> 00:49:50,490
school to secondary coming down here

724
00:49:50,540 --> 00:49:53,370
and it to world reflecting the same

725
00:49:53,420 --> 00:49:56,330
angle and come out like so

726
00:49:56,490 --> 00:49:59,770
others self on somebody's leaving the room

727
00:49:59,780 --> 00:50:01,370
let's go

728
00:50:01,410 --> 00:50:09,450
i want the cell phones off no compromises

729
00:50:09,470 --> 00:50:12,040
charles respect for classmates

730
00:50:12,040 --> 00:50:13,560
this route

731
00:50:13,580 --> 00:50:19,810
disrespectful to me it's disrespectful to the class

732
00:50:19,860 --> 00:50:23,930
don't get

733
00:50:24,020 --> 00:50:27,890
that happens on wednesday i'll tear your paper in front of your eyes and give

734
00:50:27,890 --> 00:50:29,380
you a zero

735
00:50:29,390 --> 00:50:36,780
mark my words

736
00:50:39,150 --> 00:50:43,380
i want to look at what happens when i have light coming in

737
00:50:43,430 --> 00:50:49,060
at a particular angle and furthermore i want to specify that i have are in

738
00:50:49,060 --> 00:50:57,690
phase radiation in space radiation which is also termed coherence so i have in face

739
00:50:57,690 --> 00:51:03,690
incident coherent radiation and furthermore i wanna monochromatic

740
00:51:03,750 --> 00:51:08,930
monochromatic so these are x-rays that are coherent and monochromatic monochromatic means

741
00:51:09,060 --> 00:51:12,750
a single wavelength a single waverly

742
00:51:12,790 --> 00:51:16,960
so here's the incident radiation on the left

743
00:51:16,980 --> 00:51:22,280
this is the incident radiation on the left and this is the reflected radiation on

744
00:51:22,280 --> 00:51:24,460
the right

745
00:51:24,520 --> 00:51:28,520
and what do i want to see here i want to see what are the

746
00:51:28,520 --> 00:51:31,940
conditions to give me constructive interference

747
00:51:31,950 --> 00:51:36,660
because i don't want destructive interference for destructive interference i will see anything

748
00:51:37,540 --> 00:51:42,000
let's mark this by showing some kind of away form do is show

749
00:51:42,020 --> 00:51:44,540
just the first sign so i

750
00:51:44,580 --> 00:51:48,140
i'll show the first signs site and then i'll show the same

751
00:51:48,180 --> 00:51:53,760
on the second ray so to indicate is that i have in phase radiation coming

752
00:51:53,780 --> 00:51:56,300
down from the left

753
00:51:56,320 --> 00:52:01,270
and then over here i want to indicate that i have in phase radiation leaving

754
00:52:01,280 --> 00:52:04,940
so again all indicate the sinusoids in the same

755
00:52:07,140 --> 00:52:08,360
there's an issue here

756
00:52:08,370 --> 00:52:09,120
you can see

757
00:52:09,130 --> 00:52:10,310
re one

758
00:52:10,320 --> 00:52:12,750
travels a shorter distance

759
00:52:12,760 --> 00:52:15,780
then ray to re two has to go farther

760
00:52:15,790 --> 00:52:20,130
so i can actually calculate how far that is if i drop and normal

761
00:52:20,150 --> 00:52:21,290
from here

762
00:52:21,300 --> 00:52:22,770
normal from here

763
00:52:22,800 --> 00:52:24,920
i can label this a

764
00:52:25,780 --> 00:52:27,000
and c

765
00:52:27,080 --> 00:52:29,310
we can see that rate two

766
00:52:29,320 --> 00:52:34,830
as the travel far distances a-b BC

767
00:52:34,910 --> 00:52:38,280
what's the condition of a plus b c

768
00:52:38,280 --> 00:52:40,210
to ensure that these two

769
00:52:40,230 --> 00:52:42,050
are in phase

770
00:52:42,060 --> 00:52:43,370
they must equal

771
00:52:43,380 --> 00:52:48,760
some path length that is a whole number of wavelengths otherwise i'll get some amount

772
00:52:49,340 --> 00:52:51,270
destructive interference

773
00:52:51,280 --> 00:52:55,390
and and of course is a whole number i don't care one two and so

774
00:52:55,390 --> 00:53:00,360
on and actually we can call this is the order of the reflection

775
00:53:00,370 --> 00:53:04,820
this is the order of the reflection

776
00:53:04,840 --> 00:53:10,580
and if i go through the math we know what theta is what's this distance

777
00:53:10,580 --> 00:53:14,110
between successive planes if i'm showing you

778
00:53:14,130 --> 00:53:18,570
adjacent hkl planes then this distance here is the

779
00:53:19,030 --> 00:53:24,620
as we defined it before this is the distance between adjacent hkl planes and if

780
00:53:24,620 --> 00:53:29,490
i go through the trigonometry here and lambda will equal two times d

781
00:53:29,540 --> 00:53:31,690
so i

782
00:53:31,790 --> 00:53:34,140
of the two

783
00:53:34,230 --> 00:53:39,490
got the angle theta we know the distance between successive planes if n lambda equal

784
00:53:39,490 --> 00:53:45,200
all right last time we talk exclusively about

785
00:53:45,240 --> 00:53:47,420
completely inelastic collisions

786
00:53:47,460 --> 00:53:49,040
today i will talk about

787
00:53:49,050 --> 00:53:50,420
collisions in more

788
00:53:52,400 --> 00:53:55,760
let's take a one-dimensional case

789
00:53:55,810 --> 00:53:57,470
we have here

790
00:53:59,330 --> 00:54:00,560
we have here

791
00:54:00,580 --> 00:54:01,350
and two

792
00:54:03,110 --> 00:54:08,970
to make life a little easier to to make the two zero

793
00:54:08,990 --> 00:54:10,610
and this

794
00:54:11,620 --> 00:54:15,150
has velocity v one

795
00:54:15,190 --> 00:54:17,690
after the collision

796
00:54:17,690 --> 00:54:19,340
and two

797
00:54:20,370 --> 00:54:22,050
a velocity

798
00:54:22,110 --> 00:54:24,070
the two prime

799
00:54:25,890 --> 00:54:27,750
and what

800
00:54:27,760 --> 00:54:29,820
that have a velocity

801
00:54:29,850 --> 00:54:31,590
he one prime

802
00:54:31,650 --> 00:54:33,650
i don't even know what changes direction

803
00:54:33,650 --> 00:54:35,610
one it is in that direction

804
00:54:35,620 --> 00:54:38,620
you was see that you one is possible

805
00:54:38,680 --> 00:54:41,540
to find the one prime and to find

806
00:54:41,540 --> 00:54:45,770
the two prime it's clear that you're no need to equations

807
00:54:45,820 --> 00:54:46,610
and if

808
00:54:46,630 --> 00:54:48,910
there is no net external force

809
00:54:48,930 --> 00:54:50,800
on the system as a whole

810
00:54:50,820 --> 00:54:52,710
during the collisions then

811
00:54:52,710 --> 00:54:54,830
momentum is conserved

812
00:54:54,880 --> 00:54:56,760
so you can write down

813
00:54:57,790 --> 00:54:58,930
the one

814
00:54:58,930 --> 00:55:00,220
must be

815
00:55:00,240 --> 00:55:01,220
and one

816
00:55:01,270 --> 00:55:03,130
he one prime

817
00:55:05,600 --> 00:55:06,520
two prime

818
00:55:06,570 --> 00:55:10,710
now you may want to put errors over there to indicate that these vectors but

819
00:55:10,710 --> 00:55:13,630
since is a one-dimensional case you can leave the errors off

820
00:55:13,650 --> 00:55:18,180
and the signs will then automatically take care of the direction if you call this

821
00:55:19,410 --> 00:55:21,130
then if you get a minus sign

822
00:55:21,160 --> 00:55:21,880
you know

823
00:55:22,610 --> 00:55:26,020
the velocities in the opposite direction

824
00:55:26,040 --> 00:55:27,290
for now

825
00:55:30,800 --> 00:55:32,570
the the second equation

826
00:55:32,580 --> 00:55:33,850
nine physics

827
00:55:33,880 --> 00:55:35,990
we do believe very strongly

828
00:55:37,460 --> 00:55:38,880
conservation of energy

829
00:55:38,900 --> 00:55:42,710
not necessarily in the conservation of kinetic energy actually have seen last time you can

830
00:55:42,710 --> 00:55:44,270
destroy kinetic energy

831
00:55:44,330 --> 00:55:48,300
but somehow we believe that if you destroy energy it must come out in some

832
00:55:48,300 --> 00:55:49,250
other form

833
00:55:49,370 --> 00:55:52,080
you cannot create energy out of nothing

834
00:55:52,090 --> 00:55:56,430
and in the case of the completely inelastic collisions that we've seen last time

835
00:55:56,430 --> 00:55:59,640
we lost kinetic energy which was converted to heat

836
00:55:59,680 --> 00:56:04,730
there was internal friction when the carrack ploughed into each other labels internal friction no

837
00:56:04,730 --> 00:56:09,420
external friction and that took out kinetic energy

838
00:56:09,430 --> 00:56:10,430
and so

839
00:56:11,490 --> 00:56:13,680
its most general form

840
00:56:13,680 --> 00:56:14,990
you can write down

841
00:56:15,020 --> 00:56:17,110
that the kinetic energy before

842
00:56:18,830 --> 00:56:20,330
some number q

843
00:56:21,200 --> 00:56:24,250
the kinetic energy after the collision and if you know q

844
00:56:24,330 --> 00:56:26,270
then you have the second equation

845
00:56:26,270 --> 00:56:28,520
and then you can solve for v one prime

846
00:56:28,550 --> 00:56:30,520
and for two prime

847
00:56:31,460 --> 00:56:34,520
q is larger than zero

848
00:56:34,550 --> 00:56:39,960
then you can gain kinetic energy that is possible we did that last time

849
00:56:39,960 --> 00:56:40,770
we had

850
00:56:40,780 --> 00:56:43,710
two cars which were connected by his points

851
00:56:43,750 --> 00:56:45,400
and we going to wire

852
00:56:45,430 --> 00:56:49,730
and each went off in the opposite direction there was no kinetic energy before if

853
00:56:49,730 --> 00:56:53,250
you want to call it the collision but there was kinetic energy afterwards that was

854
00:56:53,250 --> 00:56:58,080
the potential energy of the spring that was converted into kinetic energy so q can

855
00:56:58,080 --> 00:56:59,490
be larger than zero

856
00:56:59,500 --> 00:57:02,990
we call that a superb

857
00:57:02,990 --> 00:57:04,430
elastic collision

858
00:57:04,460 --> 00:57:05,800
could be an explosion

859
00:57:05,810 --> 00:57:07,800
that's a super elastic collision

860
00:57:07,890 --> 00:57:09,580
and then there is the possibility

861
00:57:10,490 --> 00:57:13,960
equals zero very special case we will deal with that today

862
00:57:13,980 --> 00:57:15,670
and we call that

863
00:57:15,710 --> 00:57:17,770
elastic collisions

864
00:57:17,810 --> 00:57:24,550
i will often call it a completely elastic collision which is really not necessary elastic

865
00:57:24,550 --> 00:57:27,300
itself already means q is zero

866
00:57:27,310 --> 00:57:29,840
and then there is a case of which we have seen

867
00:57:29,860 --> 00:57:31,750
several examples last time

868
00:57:31,770 --> 00:57:35,280
of inelastic collisions when you lose kinetic energy

869
00:57:35,300 --> 00:57:36,590
so this is an

870
00:57:38,280 --> 00:57:40,800
the last condition

871
00:57:40,870 --> 00:57:42,800
and so if you know that q is

872
00:57:42,800 --> 00:57:44,200
then you

873
00:57:44,240 --> 00:57:45,990
can solve these equations

874
00:57:45,990 --> 00:57:49,710
whenever q is less than zero whenever you lose kinetic energy

875
00:57:49,750 --> 00:57:51,900
the loss in general goes into

876
00:57:54,240 --> 00:57:56,170
now i want to

877
00:57:56,180 --> 00:57:59,050
continuing case whereby completely

878
00:57:59,050 --> 00:58:02,530
elastic collision

879
00:58:02,590 --> 00:58:04,490
so q is zero

880
00:58:04,500 --> 00:58:07,610
momentum is conserved because there was no net external force

881
00:58:07,640 --> 00:58:10,820
so now kinetic energy is also conserved

882
00:58:10,840 --> 00:58:12,380
so i can write down now

883
00:58:12,390 --> 00:58:14,070
one half

884
00:58:14,090 --> 00:58:14,940
and one

885
00:58:14,950 --> 00:58:18,450
the ones created was the kinetic energy before the collision

886
00:58:18,490 --> 00:58:19,280
must be

887
00:58:19,290 --> 00:58:20,550
the kinetic energy

888
00:58:20,560 --> 00:58:23,340
after the collision one half and one

889
00:58:23,380 --> 00:58:26,840
the one primes career plus one half

890
00:58:29,510 --> 00:58:31,050
prior square

891
00:58:31,060 --> 00:58:33,280
this is my creation number one

892
00:58:33,300 --> 00:58:35,050
and this is my creation of number the two

893
00:58:35,060 --> 00:58:39,250
and they can be solved you can solve them their souls in your book i

894
00:58:39,250 --> 00:58:43,110
will simply give you the results because the results are very interesting to play we

895
00:58:43,180 --> 00:58:45,550
thought we would be doing today

896
00:58:45,610 --> 00:58:48,050
three one prime

897
00:58:48,060 --> 00:58:49,060
will be

898
00:58:49,070 --> 00:58:51,100
and one minus two

899
00:58:51,140 --> 00:58:53,530
divided by and one

900
00:58:53,570 --> 00:58:55,540
plus and two

901
00:58:55,590 --> 00:58:56,380
time times

902
00:58:56,390 --> 00:58:59,030
v one

903
00:58:59,050 --> 00:59:00,840
and the two prime

904
00:59:00,860 --> 00:59:02,740
will be two and one

905
00:59:02,790 --> 00:59:06,110
divided by and one class and two

906
00:59:07,930 --> 00:59:10,950
fifty one

907
00:59:10,970 --> 00:59:13,660
the first thing that you already right away

908
00:59:14,380 --> 00:59:18,230
that the two prime is always in the same direction as the one that's completely

909
00:59:20,480 --> 00:59:23,220
the second object was standing still remember

910
00:59:23,250 --> 00:59:28,030
so if you plough something into the second object they obviously continue in that direction

911
00:59:28,030 --> 00:59:28,990
that's clear

912
00:59:29,060 --> 00:59:32,300
so you see you can never have assigned reversal here

913
00:59:32,310 --> 00:59:34,630
he however you can have assigned we first

914
00:59:34,680 --> 00:59:38,800
if you want the ping-pong ball for billions all the ping-pong ball will come back

915
00:59:38,840 --> 00:59:40,760
and this one becomes negative

916
00:59:40,800 --> 00:59:45,190
whereas if you plough believable into a ping-pong ball it will go forward and so

917
00:59:45,190 --> 00:59:48,500
this can both be negative and can be positive

918
00:59:48,590 --> 00:59:52,640
depending upon whether the upstairs is negative or positive

919
00:59:52,670 --> 00:59:54,110
so this is the result

920
00:59:54,120 --> 00:59:56,510
which holds on three conditions

921
00:59:57,230 --> 00:59:59,850
the kinetic energy is conserved so q is zero

922
00:59:59,860 --> 01:00:02,200
that momentum is conserved and

923
01:00:02,220 --> 01:00:04,640
that the two before the collision

924
01:00:04,700 --> 01:00:07,200
equals zero

925
01:00:07,370 --> 01:00:09,950
let's look at three interesting

926
01:00:09,980 --> 01:00:13,540
cases whereby we go to extremes

927
01:00:13,550 --> 01:00:15,240
let's first take the case

928
01:00:16,160 --> 01:00:20,300
and one is much much larger than n two

929
01:00:20,300 --> 01:00:22,300
and one is much much larger

930
01:00:22,310 --> 01:00:23,300
then and two

931
01:00:23,300 --> 01:00:26,610
another way of thinking about that is that that

932
01:00:30,680 --> 01:00:32,950
extreme case the limiting case

933
01:00:33,010 --> 01:00:35,790
so it's like having a bowling ball

934
01:00:35,840 --> 01:00:36,930
that you

935
01:00:36,970 --> 01:00:39,100
collide was ping pong ball

936
01:00:39,110 --> 01:00:41,300
if you look at that equation

937
01:00:41,310 --> 01:00:43,920
when and two goes to zero

938
01:00:44,040 --> 01:00:46,620
this is zero this is zero

939
01:00:46,680 --> 01:00:48,920
notice that v one prime

940
01:00:48,930 --> 01:00:51,280
equals VY

941
01:00:51,970 --> 01:00:54,300
that is completely intuitive

942
01:00:54,320 --> 01:00:58,160
if the bowling ball collides with the ping-pong ball the building wasn't even see the

943
01:00:58,160 --> 01:01:01,500
ping-pong ball it continues its role as if nothing happened

944
01:01:01,550 --> 01:01:02,970
that's exactly what you see

945
01:01:02,980 --> 01:01:04,160
after the collision

946
01:01:04,160 --> 01:01:08,680
is an implemented in the lab and in things like this and in

947
01:01:08,690 --> 01:01:11,180
we tried different from in FSL

948
01:01:11,540 --> 01:01:16,310
with the e we wanted to to and makes it into independent time courses

949
01:01:16,370 --> 01:01:21,830
and the fmri just into independent maps with associated time courses so we doing temporal

950
01:01:21,830 --> 01:01:24,450
ICA here and spatial there

951
01:01:24,510 --> 01:01:28,260
we can do that either separately or all together

952
01:01:28,360 --> 01:01:32,190
this just we do that separately then we can do it on single trial level

953
01:01:32,510 --> 01:01:36,130
with fmri that this has been presented in the previous talk is what you what

954
01:01:36,130 --> 01:01:40,200
you do and if you makes the time series by voxels into

955
01:01:40,220 --> 01:01:41,950
maps by time courses

956
01:01:43,150 --> 01:01:49,190
indeed you get time courses by special topographies with a single file weights within them

957
01:01:49,210 --> 01:01:55,690
i tend to

958
01:01:55,690 --> 01:01:58,910
if you're my own bias with the data that i'm working with so

959
01:01:58,930 --> 01:02:01,960
what i wanted to do is to do the whole thing for the whole population

960
01:02:01,960 --> 01:02:03,200
at once

961
01:02:03,230 --> 01:02:06,850
i really like to push just one but not many buttons

962
01:02:06,850 --> 01:02:11,130
because it simplifies detection of group independent components

963
01:02:11,150 --> 01:02:15,950
you get a single set of i sees the distant that projected into this into

964
01:02:15,950 --> 01:02:18,770
the individual data and they get back to that

965
01:02:18,780 --> 01:02:20,610
there's no additional clustering

966
01:02:20,610 --> 01:02:24,430
i mean what what you can do is to individual ICA's on your easy or

967
01:02:24,680 --> 01:02:29,270
data and then have some kind of clustering idea and start clustering

968
01:02:29,290 --> 01:02:30,430
there is

969
01:02:30,430 --> 01:02:35,300
an additional complexity in that clustering algorithm choice of closer to detect or when you

970
01:02:35,300 --> 01:02:37,150
do that visually there is bias

971
01:02:37,160 --> 01:02:39,240
so how do you treat that in your

972
01:02:39,330 --> 01:02:40,940
group inference later

973
01:02:41,050 --> 01:02:43,950
that's the that's difficult thing for me

974
01:02:44,490 --> 01:02:48,680
it on all it's easier to keep track of things you already dealing with two

975
01:02:48,680 --> 01:02:50,640
very big things

976
01:02:50,690 --> 01:02:51,870
to make that

977
01:02:51,910 --> 01:02:54,150
and you have twenty subjects twenty fold

978
01:02:54,160 --> 01:03:00,040
problem is probably unnecessary for me i don't have these big computers very small computers

979
01:03:01,530 --> 01:03:05,770
the idea that this is a line in in two papers that in the special

980
01:03:06,420 --> 01:03:11,630
in international journal of psychophysiology preprints if you will if you want i can send

981
01:03:11,630 --> 01:03:12,380
them you

982
01:03:12,540 --> 01:03:14,030
right after the talk

983
01:03:14,060 --> 01:03:19,390
so let's put that into the graphic again

984
01:03:19,400 --> 01:03:22,700
well we want to do is the group ICA of the data

985
01:03:22,730 --> 01:03:26,490
not just use ICA for architecture it could correction and then get back to you

986
01:03:26,490 --> 01:03:27,430
i want to do

987
01:03:27,460 --> 01:03:29,690
group ICA this step

988
01:03:29,690 --> 01:03:34,410
get the canonical components within our population see whether that correlates with

989
01:03:34,450 --> 01:03:36,860
the group i my data

990
01:03:37,010 --> 01:03:41,110
that idea has been around since two thousand one with windscale came out with his

991
01:03:41,150 --> 01:03:46,090
group inference for functional neuroimaging toolbox that's the action from gift can also called just

992
01:03:46,090 --> 01:03:50,950
called give al

993
01:03:51,000 --> 01:03:54,930
and we can do that as i said pele or in joint just squeeze them

994
01:03:54,930 --> 01:03:56,620
together into one big space

995
01:03:56,630 --> 01:03:59,750
and then we can do an additional trick which is to start off with the

996
01:03:59,760 --> 01:04:01,230
group ICA for the fmri

997
01:04:01,250 --> 01:04:03,410
take the

998
01:04:03,420 --> 01:04:07,940
time series here from the independent components and deconvolved

999
01:04:07,950 --> 01:04:12,300
deconvolve the traffic out of this time course and then use the HRF estimate to

1000
01:04:12,300 --> 01:04:16,650
get back to the parametric modulation of single trials

1001
01:04:16,660 --> 01:04:19,140
ideally before the convolution happened

1002
01:04:19,160 --> 01:04:23,300
that's been going if you're doing SPM analysis or

1003
01:04:23,400 --> 01:04:26,880
DNA analysis you going the whole thing backwards

1004
01:04:26,890 --> 01:04:29,810
you start out with the result in the go back to the primitive modulation

1005
01:04:29,870 --> 01:04:33,510
which makes sense if you want to predict the EEG with many sources in the

1006
01:04:33,510 --> 01:04:36,870
from right rather than predict fmri with sources from the easy

1007
01:04:36,890 --> 01:04:39,300
depends a bit on how you

1008
01:04:39,330 --> 01:04:42,350
one thing it work

1009
01:04:42,610 --> 01:04:48,520
first off we did simulation just check whether or or not

1010
01:04:49,750 --> 01:04:50,700
the group ICA

1011
01:04:50,740 --> 01:04:52,500
i think that's

1012
01:04:52,620 --> 01:04:57,200
i'm getting to that but basically the the description was given by the court

1013
01:04:57,300 --> 01:04:58,840
in the previous talk

1014
01:04:58,850 --> 01:05:01,450
i'm getting better

1015
01:05:01,460 --> 01:05:06,950
so we're doing this saying this ICA will explain why it right away

1016
01:05:06,960 --> 01:05:08,890
we put sources

1017
01:05:09,720 --> 01:05:12,880
which sources is are defined as

1018
01:05:12,910 --> 01:05:14,730
time courses over trials

1019
01:05:14,750 --> 01:05:16,380
spatial distributions

1020
01:05:16,390 --> 01:05:19,560
e responses and EEG topography is

1021
01:05:19,580 --> 01:05:22,690
in one big bucket or secondly if you will

1022
01:05:22,700 --> 01:05:24,440
and we mix them together

1023
01:05:24,450 --> 01:05:28,230
and on a single trial you don't see anything of the sort which he

1024
01:05:28,250 --> 01:05:33,000
some blogs and some noise here you could physiological noise onto this mixture

1025
01:05:33,010 --> 01:05:37,190
and some some technical noise some some gaussian noise and all kinds of things that

1026
01:05:37,190 --> 01:05:41,790
we could think of our existing in real data and then we just use this

1027
01:05:41,790 --> 01:05:43,170
group implementations

1028
01:05:43,180 --> 01:05:46,300
to get back to the sources

1029
01:05:46,330 --> 01:05:50,230
well we don't catch is easy fast transients

1030
01:05:50,290 --> 01:05:56,540
whether or not these are very relevant for event related potentials is a different question

1031
01:05:56,550 --> 01:06:01,590
so basically just saying that this whole framework technically looks OK

1032
01:06:01,670 --> 01:06:06,960
this is the group ICA works

1033
01:06:06,990 --> 01:06:08,890
you basically

1034
01:06:08,900 --> 01:06:12,880
starts at this point we have some kind of event related

1035
01:06:13,830 --> 01:06:18,650
that makes it easier to get your head around it which is then implemented in

1036
01:06:18,650 --> 01:06:26,110
sub in subjects basically which have consistent or relatively consistent activation maps and time courses

1037
01:06:26,120 --> 01:06:27,770
which are then mixed by

1038
01:06:28,200 --> 01:06:30,470
individual mixing mattresses

1039
01:06:31,580 --> 01:06:34,030
the the signal that you with the tag

1040
01:06:34,060 --> 01:06:35,620
so that you would require

1041
01:06:35,620 --> 01:06:39,060
in the from my side in new volumes and on the aegean there in a

1042
01:06:39,060 --> 01:06:42,130
single trials with then do is

1043
01:06:42,140 --> 01:06:43,710
typical preprocessing of

1044
01:06:43,750 --> 01:06:45,720
from my data for group study so we

1045
01:06:46,890 --> 01:06:52,230
put the brains the individual brains into a common space we feel that we normalize

1046
01:06:52,350 --> 01:06:53,630
we smooth

1047
01:06:53,630 --> 01:06:58,970
and the easy part we doing artifact reduction with with individual ICA some powerful drain

1048
01:06:59,240 --> 01:07:00,950
and parking

1049
01:07:02,860 --> 01:07:08,510
then we can do is to group the grouping step then we concatenate

1050
01:07:08,540 --> 01:07:12,380
these these data after we reduce the so we take the

1051
01:07:12,430 --> 01:07:13,940
the fmri

1052
01:07:13,990 --> 01:07:16,880
images by

1053
01:07:16,930 --> 01:07:20,030
by time points and you reduce it with the PCA two

1054
01:07:20,050 --> 01:07:22,150
over the temporal dimension

1055
01:07:22,150 --> 01:07:25,990
so we defined we define graphical models now what i'm going to talk about his

1056
01:07:25,990 --> 01:07:28,200
inference and propagation algorithms

1057
01:07:28,250 --> 01:07:29,840
so this is going to be

1058
01:07:31,420 --> 01:07:32,960
probably a bit heavy

1059
01:07:33,000 --> 01:07:35,440
four lecture but you know

1060
01:07:35,460 --> 01:07:39,290
i just want to cover the main inference and propagation algorithms

1061
01:07:39,310 --> 01:07:40,470
so the you

1062
01:07:40,480 --> 01:07:42,900
at the moment in the slide

1063
01:07:46,350 --> 01:07:49,580
let's think about the problem of inferring consider the following

1064
01:07:49,600 --> 01:07:57,600
directed graph which represents this factorisation of the joint probability over the five variables

1065
01:07:59,240 --> 01:08:02,130
the inference problem i'm going to think about

1066
01:08:02,180 --> 01:08:08,110
as the problem of evaluating the probability distribution over some set of variables

1067
01:08:08,160 --> 01:08:11,080
given the values of another set of variables

1068
01:08:11,090 --> 01:08:15,700
while something out integrating all the variables that are not interesting to us

1069
01:08:15,750 --> 01:08:21,010
so for example we have a graphical model which is an expert system of some

1070
01:08:21,300 --> 01:08:24,710
big nuclear power plant with ten thousand variables

1071
01:08:24,720 --> 01:08:30,170
we observe a whole bunch of readings of different sensors that we might have one

1072
01:08:30,170 --> 01:08:31,810
big variable in the middle

1073
01:08:31,860 --> 01:08:35,480
of the graph that represents the probability that we're having

1074
01:08:35,870 --> 01:08:37,950
meltdown of the core

1075
01:08:37,970 --> 01:08:41,320
and we might want to run our inference algorithm every once in awhile to to

1076
01:08:41,330 --> 01:08:44,770
check with the idea that is

1077
01:08:45,820 --> 01:08:49,100
how do we compute

1078
01:08:50,460 --> 01:08:54,480
let's consider for example how do we compute the probability of

1079
01:08:54,490 --> 01:08:56,110
the variable age

1080
01:08:56,130 --> 01:09:00,510
given that we observe the value of the variable c

1081
01:09:00,560 --> 01:09:04,900
so imagine for now that the variables are binary

1082
01:09:04,920 --> 01:09:08,140
and let's just naively tried to compute that so

1083
01:09:08,450 --> 01:09:12,940
we want to compute the probability of a given c

1084
01:09:12,950 --> 01:09:18,500
so let's start out by computing the probability of a and c taking on the

1085
01:09:18,500 --> 01:09:20,060
value the

1086
01:09:20,590 --> 01:09:25,960
well the graph has a total of five variables

1087
01:09:25,970 --> 01:09:30,160
we have to the variables here to compute that we take the joint distribution and

1088
01:09:30,160 --> 01:09:34,120
we sum over all settings of the the other three variables

1089
01:09:34,130 --> 01:09:35,550
OK so that

1090
01:09:35,560 --> 01:09:38,280
eight summations

1091
01:09:38,290 --> 01:09:41,710
because this is two times two times two but we have to do that for

1092
01:09:41,710 --> 01:09:46,160
each of the two possible values of the variable a is well so that the

1093
01:09:46,160 --> 01:09:48,960
total of sixteen operations

1094
01:09:54,150 --> 01:09:59,090
now to complete the denominator that we want to compute the probability that takes on

1095
01:09:59,090 --> 01:10:03,220
the values c so we take this every summer over the two values of a

1096
01:10:03,220 --> 01:10:07,520
that's another two operations and then for each of the two values of a we

1097
01:10:07,520 --> 01:10:08,740
divide up

1098
01:10:08,990 --> 01:10:13,330
this guy by that guy and that's another two operations so we managed to compute

1099
01:10:13,330 --> 01:10:19,510
everything in twenty operations which isn't bad or five variables but it gets really horrendous

1100
01:10:21,190 --> 01:10:23,340
summing over all the variables here

1101
01:10:23,350 --> 01:10:26,880
when you have n binary variables would involve summing over

1102
01:10:27,040 --> 01:10:31,860
two to the end possible configurations of all the variables so essentially what we're trying

1103
01:10:31,870 --> 01:10:36,750
to do is we're trying to do inference which we can do simply by enumeration

1104
01:10:37,280 --> 01:10:39,300
we're trying to do it efficiently

1105
01:10:39,520 --> 01:10:42,270
the key idea

1106
01:10:42,730 --> 01:10:45,660
so here's the slightly more efficient method

1107
01:10:45,680 --> 01:10:51,770
we can take that joint probability distribution and that's on and we can take some

1108
01:10:51,780 --> 01:10:54,380
of the terms in the farm

1109
01:10:55,690 --> 01:11:00,490
so for example the some of the variable e well all these guys

1110
01:11:00,550 --> 01:11:02,350
don't depend on the

1111
01:11:02,360 --> 01:11:06,740
only this guy depends on the ability to think that the mission over the all

1112
01:11:06,740 --> 01:11:10,650
the way up here and then later notice that the probability

1113
01:11:10,720 --> 01:11:15,070
the other variable when we sum over all values equals one

1114
01:11:15,090 --> 01:11:19,120
and this gets eliminated that gets eliminated and if we did

1115
01:11:19,340 --> 01:11:21,430
things a little more cleverly

1116
01:11:21,450 --> 01:11:23,390
we could do

1117
01:11:23,590 --> 01:11:29,780
the first step in for operations that sixteen so this is just another toy example

1118
01:11:29,920 --> 01:11:32,950
just to make you think about the fact that the order in which you do

1119
01:11:32,950 --> 01:11:41,500
the operations and using the conditional independence relationships can save you a lot of applications

1120
01:11:41,510 --> 01:11:46,920
so what i'll talk about in this lecture our belief propagation methods

1121
01:11:46,930 --> 01:11:53,440
and these methods use the conditional independence relationships in the graph to do efficient inference

1122
01:11:53,490 --> 01:11:57,620
and for graphs which are singly connected which i'm about to define

1123
01:11:57,830 --> 01:12:02,990
we can get exponential gains in efficiency over the naive method

1124
01:12:07,620 --> 01:12:13,360
so let's talk about belief propagation in singly connected undirected graph

1125
01:12:13,880 --> 01:12:21,730
so directed exactly graph is singly connected if its underlying undirected graph is a tree

1126
01:12:22,690 --> 01:12:25,130
another way to think about that is

1127
01:12:25,650 --> 01:12:27,620
if there's only one

1128
01:12:27,630 --> 01:12:31,120
i have between any two nodes

1129
01:12:31,170 --> 01:12:34,610
so the graph on this side is singly connected

1130
01:12:34,620 --> 01:12:37,480
because they can know there's only one

1131
01:12:37,760 --> 01:12:41,540
have between this graph is not fully connected

1132
01:12:41,540 --> 01:12:45,700
so we incorporate this in the objective function and then we have

1133
01:12:45,750 --> 01:12:49,850
usual magic and it can be very easy to see

1134
01:12:49,950 --> 01:12:51,610
but this means that

1135
01:12:52,520 --> 01:12:53,840
we are

1136
01:12:53,850 --> 01:12:56,700
we start the above the origin

1137
01:12:56,710 --> 01:12:59,490
we start from some way

1138
01:12:59,500 --> 01:13:01,530
w t minus one

1139
01:13:01,930 --> 01:13:05,160
and then we have some

1140
01:13:05,230 --> 01:13:08,220
some x being

1141
01:13:08,240 --> 01:13:11,820
on which we don't actually have a large enough margin

1142
01:13:11,870 --> 01:13:15,280
OK i suppose that x the for for simplicity

1143
01:13:15,330 --> 01:13:17,690
if x is a unit normal

1144
01:13:19,730 --> 01:13:20,510
but now

1145
01:13:20,520 --> 01:13:24,610
we want we want to achieve the margin of

1146
01:13:24,620 --> 01:13:27,980
a one of the next to the origin

1147
01:13:28,020 --> 01:13:31,650
so now we take the perpendicular here

1148
01:13:31,720 --> 01:13:34,780
and we make the projection

1149
01:13:34,790 --> 01:13:35,590
which is

1150
01:13:35,600 --> 01:13:37,000
the meaning of this

1151
01:13:38,460 --> 01:13:41,590
of that and one on two

1152
01:13:44,150 --> 01:13:47,970
which defines the basis of the solution

1153
01:13:48,050 --> 01:13:49,420
but the solution

1154
01:13:49,440 --> 01:13:52,080
that achievement in exactly one

1155
01:13:52,130 --> 01:13:53,400
on the

1156
01:13:53,410 --> 01:13:55,640
on the example

1157
01:13:55,850 --> 01:13:58,080
so we predict that the here

1158
01:13:58,130 --> 01:14:00,410
and this projection is our

1159
01:14:01,250 --> 01:14:07,230
we predict that the human mind to protect our new updated weight

1160
01:14:07,260 --> 01:14:13,050
so we have a single linear constraint passed solution we project our solution onto the

1161
01:14:13,050 --> 01:14:14,810
single that drink

1162
01:14:14,860 --> 01:14:17,010
and we obtain the neuron

1163
01:14:17,100 --> 01:14:18,260
the one

1164
01:14:18,270 --> 01:14:20,370
very simple and we very

1165
01:14:20,530 --> 01:14:23,000
four so it's easy

1166
01:14:23,030 --> 01:14:27,430
we can run these diagram and these are great is

1167
01:14:27,450 --> 01:14:31,040
so is the version of

1168
01:14:33,360 --> 01:14:37,110
they have additional properties

1169
01:14:37,160 --> 01:14:39,660
of course

1170
01:14:39,670 --> 01:14:41,170
of course i mean

1171
01:14:41,190 --> 01:14:46,170
here we are making and very aggressive update that

1172
01:14:47,300 --> 01:14:49,530
our mining more

1173
01:14:50,770 --> 01:14:53,080
in the same way

1174
01:14:55,940 --> 01:15:01,140
like variable to the SVM objective function in order to what counts the

1175
01:15:01,180 --> 01:15:05,580
four datasets that are not linearly separable

1176
01:15:05,640 --> 01:15:08,020
we might want in this set up

1177
01:15:08,030 --> 01:15:10,830
two and a similar feature

1178
01:15:10,880 --> 01:15:17,770
well boy the that in the case of a stream of these highly non notable

1179
01:15:18,150 --> 01:15:21,560
these are just too much aggressive updates

1180
01:15:21,580 --> 01:15:22,650
are going through

1181
01:15:22,700 --> 01:15:24,880
because of them

1182
01:15:24,920 --> 01:15:29,870
so we learn that we see in a moment the variance of the updated take

1183
01:15:29,880 --> 01:15:31,690
into account the

1184
01:15:33,880 --> 01:15:38,140
the if you introduce slack variables for out

1185
01:15:39,730 --> 01:15:46,460
is texting kidding amount equal to one the next or the next example

1186
01:15:46,520 --> 01:15:48,860
and the

1187
01:15:50,070 --> 01:15:52,590
closeness the possible

1188
01:15:52,600 --> 01:15:57,830
so we use the slack variables for exactly balance things that you know that could

1189
01:15:57,830 --> 01:16:03,230
be better within with nonseparable streams and then we'll see how to analyse

1190
01:16:04,370 --> 01:16:06,850
this is the other up

1191
01:16:06,900 --> 01:16:07,720
first of all

1192
01:16:08,350 --> 01:16:09,310
we see

1193
01:16:09,310 --> 01:16:15,000
combination of things in between so if you want you can condition on certain evidence

1194
01:16:15,000 --> 01:16:19,940
variables and then you can get the posterior over some query node given the evidence

1195
01:16:19,940 --> 01:16:23,160
or you can just use it to compute the marginal

1196
01:16:23,180 --> 01:16:24,800
other big distribution

1197
01:16:24,800 --> 01:16:27,870
so if i give you a joint distribution for that

1198
01:16:27,960 --> 01:16:31,000
evidence i just give you a joint distribution

1199
01:16:34,040 --> 01:16:36,120
over several random variables

1200
01:16:36,550 --> 01:16:41,600
and i ask you what is the original

1201
01:16:41,640 --> 01:16:46,110
p x twelve flights

1202
01:16:47,260 --> 01:16:51,180
so that's a perfectly valid question it doesn't involve observing evidence

1203
01:16:51,210 --> 01:16:56,760
but it's just a question about this joint distribution and elimination algorithm can be used

1204
01:16:57,000 --> 01:17:01,610
to answer that question what do we do we just don't introduce any evidence potentials

1205
01:17:01,980 --> 01:17:05,430
we just some of everything except x twelve OK

1206
01:17:05,660 --> 01:17:11,870
so i think of elimination is just a way of trying to some efficiently variables

1207
01:17:11,870 --> 01:17:16,110
except our single query variable which we want to be left with the and

1208
01:17:17,410 --> 01:17:25,030
so there are some tricks that we can use to save some time elimination in

1209
01:17:25,030 --> 01:17:26,560
directed models

1210
01:17:26,600 --> 01:17:27,810
we can ignore

1211
01:17:27,820 --> 01:17:30,870
any nodes that are downstream of the

1212
01:17:30,890 --> 01:17:32,350
query node

1213
01:17:32,360 --> 01:17:36,030
and do not contain any evidence

1214
01:17:37,180 --> 01:17:40,570
so if you have a a graphical model

1215
01:17:40,620 --> 01:17:44,410
that has some structure like this

1216
01:17:50,960 --> 01:17:53,270
and let's say this number is observed

1217
01:17:53,280 --> 01:17:57,430
and this node is observed and this is the query node here that we're trying

1218
01:17:57,430 --> 01:18:00,560
to gather information about

1219
01:18:00,570 --> 01:18:03,610
so these nodes down here

1220
01:18:03,630 --> 01:18:06,190
in the aftermath model the any effect

1221
01:18:06,200 --> 01:18:10,280
because there's no evidence there and not part of the query system so we know

1222
01:18:10,280 --> 01:18:14,040
that if we sum the just going to contribute eventually factor of unity

1223
01:18:14,450 --> 01:18:17,430
so we can save myself some work rather than

1224
01:18:17,460 --> 01:18:21,700
laboriously doing the sums only to find out that they come to one we can

1225
01:18:21,700 --> 01:18:26,660
just erase them structurally from the graphical model at the very beginning and we can

1226
01:18:26,660 --> 01:18:29,450
do inference on everything that's left

1227
01:18:29,460 --> 01:18:34,310
so it's only a few observe some evidence or if you're part of the query

1228
01:18:34,310 --> 01:18:38,040
that you're going to affect the model otherwise we can just train the graphical model

1229
01:18:38,070 --> 01:18:43,050
from the leaves back towards the query and evidence nodes so this is actually the

1230
01:18:43,050 --> 01:18:48,820
model we would run the elimination algorithm on would introduce evidence potentials for these two

1231
01:18:48,820 --> 01:18:50,030
and then some

1232
01:18:50,040 --> 01:18:54,310
these three notes we would get some function over this renormalized and that would be

1233
01:18:54,310 --> 01:18:59,970
the answer

1234
01:19:00,580 --> 01:19:07,560
so this is just the idea that

1235
01:19:07,690 --> 01:19:11,840
if we know that this sum is is supposed to be unity then

1236
01:19:11,970 --> 01:19:15,630
we shouldn't do it and i would just like to point out you can use

1237
01:19:15,630 --> 01:19:20,600
this trick in undirected models because there's this partition function and you don't know

1238
01:19:20,600 --> 01:19:25,170
how that set of nodes will contribute to the partition function the normalizer and so

1239
01:19:25,170 --> 01:19:27,560
we cannot we can't get away with that

1240
01:19:29,950 --> 01:19:30,960
and there

1241
01:19:30,970 --> 01:19:34,780
let me just go a little bit more into the details of this of this

1242
01:19:34,780 --> 01:19:38,370
algorithm but always keep in mind

1243
01:19:38,380 --> 01:19:43,070
this equation which is really the crux of what we're doing right we're trying to

1244
01:19:43,070 --> 01:19:44,700
efficiently so

1245
01:19:44,720 --> 01:19:48,340
over joint settings of these variables by

1246
01:19:48,340 --> 01:19:53,390
taking advantage of the distributive property and pushing the sums as far into the product

1247
01:19:53,420 --> 01:19:54,820
as we can

1248
01:19:54,820 --> 01:20:04,220
so the nomination is really just a sort of graphical algorithm you can think of

1249
01:20:04,220 --> 01:20:10,160
it as a graphical algorithm that takes each node in the graph does an operation

1250
01:20:10,160 --> 01:20:14,870
which allows you to remove it and because of that operation it might introduce new

1251
01:20:14,870 --> 01:20:16,780
edges in the graph

1252
01:20:16,780 --> 01:20:21,020
OK so i'm going to try to convince you that by just looking at the

1253
01:20:21,020 --> 01:20:23,380
simple explaining away example

1254
01:20:23,410 --> 01:20:28,910
that we had before so imagine you had a

1255
01:20:28,950 --> 01:20:31,650
let me just to raise this

1256
01:20:31,660 --> 01:20:36,760
i intentionally kept in reserve over here and somehow all three of them are bacteria

1257
01:20:39,300 --> 01:20:41,870
so imagine you had a graphical model

1258
01:20:41,900 --> 01:20:44,110
that looks like this

1259
01:20:44,130 --> 01:20:50,790
now if we observe this note

1260
01:20:50,800 --> 01:20:55,590
we know explaining away that once you observe these nodes are

1261
01:20:55,660 --> 01:20:58,530
distributions over these two students become coupled

1262
01:20:59,010 --> 01:21:03,600
so i think this is a graph algorithm in which you start with a graphical

1263
01:21:03,600 --> 01:21:07,460
model and you need to know to get the new graphical model human node in

1264
01:21:07,470 --> 01:21:11,910
each stage of the graphical model which represents the belief about what's left

1265
01:21:12,090 --> 01:21:16,330
then we kind of know once we say that this notion

1266
01:21:16,380 --> 01:21:19,910
there has to be some coupling between these two notes here

1267
01:21:19,930 --> 01:21:25,440
we have to introduce some dependency on these two nodes which indicates that because of

1268
01:21:25,500 --> 01:21:28,440
explaining away in fact we we

1269
01:21:28,480 --> 01:21:29,790
i have some

1270
01:21:29,840 --> 01:21:34,390
correlation our beliefs so there no indication algorithm

1271
01:21:34,410 --> 01:21:40,100
the operation which removes each node from the graph but that might introduce new edges

1272
01:21:40,390 --> 01:21:41,570
into the graph

1273
01:21:41,680 --> 01:21:46,670
so more precisely what happens is for each node in the ordering that we pick

1274
01:21:47,060 --> 01:21:52,420
you connect the neighbors of x y and then you remove exiled from the graph

1275
01:21:52,420 --> 01:21:55,750
which algebraically corresponds to something in

1276
01:21:55,750 --> 01:21:56,980
use that one

1277
01:21:57,020 --> 01:22:04,020
the police arrive can give all the information about the noise and then you can

1278
01:22:04,020 --> 01:22:07,690
figure out the mystery observation up to time t

1279
01:22:07,710 --> 01:22:11,040
and then the police can

1280
01:22:11,040 --> 01:22:12,350
figure out what

1281
01:22:12,370 --> 01:22:13,890
the probable

1282
01:22:13,980 --> 01:22:18,520
position for each the burglar at each time step

1283
01:22:18,540 --> 01:22:23,940
all the police can do is also to compute the most likely path

1284
01:22:23,940 --> 01:22:26,750
the most likely sequence of positions

1285
01:22:26,770 --> 01:22:30,100
for each time step which is what i show in there

1286
01:22:30,120 --> 01:22:32,440
in the fourth line here

1287
01:22:32,460 --> 01:22:37,080
then again maybe i effect i'm sure there will be the

1288
01:22:37,100 --> 01:22:40,520
in this case is seeking work quite well

1289
01:22:40,580 --> 01:22:44,120
it depends a lot on the problem there are cases in which my work

1290
01:22:44,230 --> 01:22:48,350
that's why

1291
01:22:48,580 --> 01:22:53,310
i give you know another example of modelling with an item

1292
01:22:53,370 --> 01:22:56,790
suppose that we have established thing

1293
01:22:56,810 --> 01:22:59,750
stipe is that as the tendency to

1294
01:23:01,120 --> 01:23:06,020
it the core of key on a all neighbouring key on the keyboard

1295
01:23:06,020 --> 01:23:09,580
and based on the type of secrecy one thing for that

1296
01:23:10,330 --> 01:23:12,560
what is the most likely war

1297
01:23:12,770 --> 01:23:14,830
this sequence correspond to

1298
01:23:14,870 --> 01:23:19,960
and again we can represent that would be the marker modelling which is the

1299
01:23:20,670 --> 01:23:23,080
is the intent electorate

1300
01:23:23,080 --> 01:23:25,580
at time t

1301
01:23:26,390 --> 01:23:34,000
represent the letter that was actually type at time t

1302
01:23:34,020 --> 01:23:36,250
as the transition before

1303
01:23:36,250 --> 01:23:43,600
the the bible we can use just on studies of mathematics of the of the

1304
01:23:43,600 --> 01:23:49,080
english language so probably don't see that about

1305
01:23:49,120 --> 01:23:50,440
for example

1306
01:23:50,440 --> 01:23:52,980
is very highly probable that

1307
01:23:53,060 --> 01:23:59,500
the letter you will follow the letter q and what has indicated here

1308
01:23:59,560 --> 01:24:01,210
like all of

1309
01:24:01,230 --> 01:24:02,210
and you also

1310
01:24:02,230 --> 01:24:03,620
i assume that

1311
01:24:04,920 --> 01:24:08,520
typist will probably type d

1312
01:24:08,580 --> 01:24:09,850
according to p

1313
01:24:09,870 --> 01:24:12,480
with high probability as well

1314
01:24:12,500 --> 01:24:13,270
he show

1315
01:24:16,100 --> 01:24:20,920
but the is the probability that the word types some of the

1316
01:24:20,980 --> 01:24:24,600
the key in the wiki

1317
01:24:24,620 --> 01:24:27,330
so again you can do

1318
01:24:27,330 --> 01:24:29,710
a kind of there b

1319
01:24:29,730 --> 01:24:31,810
according to and

1320
01:24:31,830 --> 01:24:35,330
given that the type is type with this

1321
01:24:35,480 --> 01:24:40,460
sequence here what is the most likely wore this correspond to what you can use

1322
01:24:40,500 --> 01:24:44,640
the more set the most likely than sequences

1323
01:24:44,660 --> 01:24:52,080
using said kind of terrorism and disregard the will those that don't correspond to the

1324
01:24:53,100 --> 01:24:58,580
sentence in the english dictionary and then take the most proper english war as has

1325
01:24:58,850 --> 01:24:59,770
the same

1326
01:25:01,120 --> 01:25:06,160
and anybody can tell what the answer can be

1327
01:25:07,190 --> 01:25:10,020
who said let me

1328
01:25:13,600 --> 01:25:15,770
OK so i still have

1329
01:25:15,850 --> 01:25:18,000
o thing twenty minutes or

1330
01:25:19,020 --> 01:25:30,960
so so far i have described

1331
01:25:30,980 --> 01:25:35,520
several types of graphical models and how to do inference with that

1332
01:25:35,540 --> 01:25:39,870
how to understand independence but i have always assumed that

1333
01:25:40,830 --> 01:25:45,120
we knew the table entries in our distribution so

1334
01:25:45,120 --> 01:25:46,120
for example

1335
01:25:46,140 --> 01:25:51,690
in the case of the action man i assume that i knew the

1336
01:25:51,710 --> 01:25:54,390
that we knew the transition emissions

1337
01:25:54,420 --> 01:25:57,900
probability now i want to discuss the little bit

1338
01:25:57,920 --> 01:26:05,190
OWL two to less stable and and therefore suppose that we have a model of

1339
01:26:05,210 --> 01:26:07,290
random variable x

1340
01:26:09,140 --> 01:26:11,750
the pair which depends on the

1341
01:26:11,790 --> 01:26:16,210
unknown parameters theta

1342
01:26:16,230 --> 01:26:20,020
and from the set of observation i want

1343
01:26:20,640 --> 01:26:24,310
for something about sitcom

1344
01:26:24,310 --> 01:26:32,330
well i guess you assume that these observations are independent the identically distributed according to

1345
01:26:32,330 --> 01:26:33,440
this model

1346
01:26:33,460 --> 01:26:37,940
and the forty second season of the joint distribution of all our

1347
01:26:37,940 --> 01:26:40,230
observation given

1348
01:26:40,270 --> 01:26:42,370
but i think that i can

1349
01:26:42,420 --> 01:26:47,190
right it does the product of simple groups

1350
01:26:47,310 --> 01:26:49,670
and they can also

1351
01:26:49,750 --> 01:26:53,770
introduce the prior distribution of prior preference of mine

1352
01:26:53,790 --> 01:26:55,350
for me to see

1353
01:26:55,370 --> 01:27:02,620
and then as with bayes rule as we have seen i can infer the posterior

1354
01:27:02,620 --> 01:27:03,600
of theta

1355
01:27:03,640 --> 01:27:07,390
he's given my observations

1356
01:27:07,400 --> 01:27:13,000
and i can view this as an extension of i believe that to simply considering

1357
01:27:13,020 --> 01:27:19,750
an additional note my belief that which has lead to all the observations

1358
01:27:19,810 --> 01:27:26,520
and you can also use more confrontation during which

1359
01:27:26,560 --> 01:27:27,890
i have just

1360
01:27:27,900 --> 01:27:31,660
one statue the giving from p two

1361
01:27:31,660 --> 01:27:34,600
one the site and they have played

1362
01:27:34,660 --> 01:27:39,810
i think that indicated that the addition of the same structure

1363
01:27:39,830 --> 01:27:45,100
and balance

1364
01:27:45,100 --> 01:27:50,660
if you want to summarize the our posterior to very popular way of doing it

1365
01:27:50,670 --> 01:27:54,140
is the maximum of posterior in metal

1366
01:27:54,160 --> 01:27:56,310
which simply compute the model

1367
01:27:56,330 --> 01:27:58,730
of the posterior

1368
01:27:58,830 --> 01:28:03,540
all the maximum likelihood method and you talk a lot about yesterday

1369
01:28:03,540 --> 01:28:05,350
it wouldn't start running on your data sets

1370
01:28:06,680 --> 01:28:11,730
they have been generalized to capture correlations new topics coming in all dumping going away

1371
01:28:13,090 --> 01:28:16,360
they have been applied to other types of classification problems

1372
01:28:16,790 --> 01:28:19,130
just you know when to apply naive bayes

1373
01:28:19,750 --> 01:28:22,900
they have been applied to build cluster ensemble something so that's art

1374
01:28:23,360 --> 01:28:28,430
and of late if you look at the machine learning literature there's quite a bit of discussion on nonparametric priors

1375
01:28:28,940 --> 01:28:33,760
so i didn't get into what kinds of graphs are maintained or what is mixed memberships

1376
01:28:34,150 --> 01:28:38,680
usually these additional distribution is the most basic one like people are sort of

1377
01:28:39,150 --> 01:28:43,840
the beginning of the nonparametric statistics literature from the seventies and eighties

1378
01:28:44,360 --> 01:28:48,170
leg usually processes and pitman yor processes and so on and somehow they making this

1379
01:28:48,170 --> 01:28:49,800
work they are they are being used in

1380
01:28:50,470 --> 01:28:57,680
you know image segmentation computer vision application text analysis applications computational biology applications it's a very rich class of models

1381
01:28:59,850 --> 01:29:04,020
and being used in many many domains i just showed you an example of text and that's okay

1382
01:29:04,720 --> 01:29:09,050
so so so the last problem discussed in the context of graphical models

1383
01:29:11,000 --> 01:29:13,360
the canonical recommendation system problem

1384
01:29:14,050 --> 01:29:18,900
right and again in this problem graphical models have been just very very successful

1385
01:29:20,960 --> 01:29:25,010
this is the netflix amazon problem right here's a bunch of movies my interviews and

1386
01:29:25,050 --> 01:29:28,900
they have created a bunch of things and then but most of this matrix is

1387
01:29:28,900 --> 01:29:33,080
empty and you're trying to fill out the matrix are trying to predict how weather

1388
01:29:33,550 --> 01:29:34,640
sort of somebody lights

1389
01:29:35,050 --> 01:29:39,470
a particular movie are not may have some information about the movie are the user

1390
01:29:40,380 --> 01:29:44,640
now it turns out that this problem of course shows up in a recommendation system e-commerce

1391
01:29:45,180 --> 01:29:50,430
it also forms the basis of many other problems so this is how to advertise

1392
01:29:50,640 --> 01:29:54,800
this advertisments on the web so these are the web pages and these are advertisments

1393
01:29:54,800 --> 01:29:56,010
and u measuring five

1394
01:29:56,510 --> 01:29:59,470
advertisment for this particular product on this web page

1395
01:29:59,900 --> 01:30:03,260
how many people click on back and based on the idea of trying to guess

1396
01:30:03,260 --> 01:30:06,900
where are actually places that's right so so that was more people click on it

1397
01:30:07,390 --> 01:30:08,650
and we make more money

1398
01:30:09,970 --> 01:30:11,760
and again you will have some side-information

1399
01:30:12,250 --> 01:30:17,040
the problem i am particularly walking on is actually a problem in forest ecology so

1400
01:30:17,480 --> 01:30:18,010
we have

1401
01:30:18,710 --> 01:30:22,010
i'm working on some of the largest database of

1402
01:30:23,170 --> 01:30:28,420
plant information that has been put together color try deep you can take it out it is the single largest

1403
01:30:28,860 --> 01:30:34,140
information on plants that has ever been put together and working with studied and peter

1404
01:30:34,140 --> 01:30:36,590
record what are excellent bicycle ideas

1405
01:30:37,220 --> 01:30:41,930
this has information about six thousand species millions of observations individuals

1406
01:30:42,380 --> 01:30:48,590
and fragmentary observations like leaf nitrogen content phosphorylated specifically very early size and so on and so forth

1407
01:30:49,150 --> 01:30:50,510
about opposing traits

1408
01:30:51,510 --> 01:30:54,020
when somebody went into the tree and started measuring

1409
01:30:55,150 --> 01:31:01,550
they not measure our house and thinks it's just not possible right so so so this matrix is mostly empty

1410
01:31:02,110 --> 01:31:03,920
and then we are trying to fill it out to sea

1411
01:31:04,800 --> 01:31:06,300
what happens in just a sense of

1412
01:31:07,260 --> 01:31:11,140
you know only nitrogen i'm just saying that i have information and some other plants

1413
01:31:11,140 --> 01:31:15,250
if you actually want to know where the plants this is about those plants right

1414
01:31:15,720 --> 01:31:17,140
we know there are more and more trees

1415
01:31:18,100 --> 01:31:21,470
in on the planet right so we say we going to try to fill out

1416
01:31:22,180 --> 01:31:25,820
the rest of the trees are and this is sort of the british and we

1417
01:31:25,820 --> 01:31:29,340
have in terms of what we have and we have something like this for each

1418
01:31:31,000 --> 01:31:35,060
very ambitious and just starting off but i'm going to show you glimpse of sort

1419
01:31:35,060 --> 01:31:36,930
of the method some other methods we have

1420
01:31:37,390 --> 01:31:38,980
which have been successful in

1421
01:31:39,500 --> 01:31:44,470
netflix recommendations system by problems we are not trying to take it to a more scientific problem

1422
01:31:44,960 --> 01:31:48,720
which is you know this this particular are probably forced like okay

1423
01:31:50,170 --> 01:31:51,300
a very brief overview

1424
01:31:54,210 --> 01:31:58,430
so matrix factorization forms the backbone of this class of methods

1425
01:31:58,840 --> 01:32:01,080
right matrix factorization has been around for a while

1426
01:32:02,550 --> 01:32:06,960
we can take the ratings matrix and somehow ignored the missing values and try to

1427
01:32:06,960 --> 01:32:08,630
come up with a single value decomposition

1428
01:32:09,810 --> 01:32:14,140
the problem with this is that if you know the size of some of these metrices

1429
01:32:14,610 --> 01:32:16,900
there there are probably ten million by

1430
01:32:17,540 --> 01:32:21,170
one hundred thousand matrix right if you're trying to do and is really

1431
01:32:22,100 --> 01:32:25,090
on matlab from the best to you because it's not going to finish

1432
01:32:25,820 --> 01:32:26,140
right so

1433
01:32:27,470 --> 01:32:30,000
so ideally we would like to do something like

1434
01:32:30,650 --> 01:32:32,510
it's just it's just not possible to do it

1435
01:32:33,190 --> 01:32:37,640
given the size of this matrix and also the fact that many of them are actually missing so

1436
01:32:38,220 --> 01:32:40,890
what people came up it and this was sort of partly

1437
01:32:41,300 --> 01:32:45,360
rejuvenated during the netflix challenge and many of these methods actually were shown to work

1438
01:32:45,360 --> 01:32:47,020
pretty well during the netflix challenge

1439
01:32:47,590 --> 01:32:48,900
and this is one of those methods

1440
01:32:50,050 --> 01:32:52,390
by slow then reverse publishing two thousand nine

1441
01:32:53,250 --> 01:32:55,790
so they came up with a graphical model for doing this

1442
01:32:57,080 --> 01:32:58,390
right and this is the graphical model

1443
01:32:58,390 --> 01:33:04,140
now isomap is something different why some what you can think of is think every data point you have

1444
01:33:04,930 --> 01:33:06,280
has being a city

1445
01:33:07,450 --> 01:33:08,550
in a high dimensional space

1446
01:33:10,260 --> 01:33:11,780
then build roads

1447
01:33:12,320 --> 01:33:17,760
for each of those data points to its neighboring cities so you define a neighborhood some way

1448
01:33:18,220 --> 01:33:19,840
okay nearest neighbours is very common

1449
01:33:20,280 --> 01:33:20,910
so you say

1450
01:33:21,740 --> 01:33:22,700
if i look at the

1451
01:33:23,160 --> 01:33:27,990
seven nearest neighbours i connect them by roads straight roads like the romans yeah

1452
01:33:28,860 --> 01:33:31,590
so roman roads between all these neighbouring cities

1453
01:33:32,740 --> 01:33:33,180
and then

1454
01:33:33,600 --> 01:33:33,910
you say

1455
01:33:34,300 --> 01:33:37,070
let me compute the road distance between every city

1456
01:33:37,600 --> 01:33:38,700
june now data points

1457
01:33:39,320 --> 01:33:42,680
and compute distance matrix from a really elegant simple idea

1458
01:33:43,070 --> 01:33:43,620
well because

1459
01:33:44,140 --> 01:33:48,470
you can't go between on neighbours directly you have to travel along the straight roads

1460
01:33:50,220 --> 01:33:52,030
between the different cities right

1461
01:33:52,600 --> 01:33:55,620
now that's an approximation for sort of manifold distance so

1462
01:33:57,240 --> 01:33:58,050
you should never

1463
01:33:58,470 --> 01:34:01,240
do anything on the swiss roll by the way this is just an illustration

1464
01:34:03,010 --> 01:34:05,010
the swiss roll is not a real data set

1465
01:34:05,450 --> 01:34:09,780
no one ever comes u with a thousand data points in three dimensions and says

1466
01:34:10,410 --> 01:34:12,640
please reduce the dimensionality of this data

1467
01:34:13,720 --> 01:34:15,430
it's extremely unrealistic

1468
01:34:15,930 --> 01:34:20,490
i don't blame them for publishing this in the first place illustrative example but we

1469
01:34:20,490 --> 01:34:22,180
used to be something called is all problem

1470
01:34:23,090 --> 01:34:27,700
you had is all problem not interesting one person's heard this or problem

1471
01:34:28,280 --> 01:34:33,240
but there's all proper with like a classification problem in machine learning in which the

1472
01:34:34,180 --> 01:34:38,160
uh all the all the gents will remember very well which is that like

1473
01:34:39,010 --> 01:34:40,720
let's see i probably can't even do it right

1474
01:34:43,090 --> 01:34:44,180
so this is like is all

1475
01:34:45,970 --> 01:34:48,340
and the point is you can't linearly separate

1476
01:34:49,490 --> 01:34:52,390
no linear classifier will give you the right answer

1477
01:34:53,070 --> 01:34:54,050
challenging isn't it

1478
01:34:56,550 --> 01:35:00,950
now someone uses the problems to illustrate the issue with linear classifiers yeah

1479
01:35:01,800 --> 01:35:06,870
and then it became like every paper had to show how the non linear classifiers solves all problems

1480
01:35:07,760 --> 01:35:13,370
it was never a serious problem with an illustration of a point which is a valid point illustrate

1481
01:35:15,720 --> 01:35:17,200
that's the same with the swiss roll

1482
01:35:18,140 --> 01:35:23,140
it's not a massive it's funny in if you go into like applied pure mass talks

1483
01:35:23,700 --> 01:35:26,280
they'll say and now we're gonna go to an application of this

1484
01:35:26,890 --> 01:35:29,720
spectral stop which is this important swiss roll

1485
01:35:30,410 --> 01:35:34,450
the machine learning is very much study because people need to unravel it

1486
01:35:38,550 --> 01:35:42,600
it's not a real dataset and actually unless you're doing something you wanna show something

1487
01:35:42,600 --> 01:35:46,240
like that when you get holes in it some aspects illustrative effect which is also

1488
01:35:46,240 --> 01:35:47,620
been useful fine but

1489
01:35:48,200 --> 01:35:49,510
please try and do the

1490
01:35:50,360 --> 01:35:52,450
the dimensionality reduction on real data

1491
01:35:53,010 --> 01:35:57,240
and you know i almost switch i don't even read the section on swiss roll but it's useful

1492
01:35:57,860 --> 01:36:01,160
for the first papers that use which was twelve years ago so it's like this

1493
01:36:01,160 --> 01:36:04,870
or problem is useful and we use the teacher about all problem as well when

1494
01:36:04,870 --> 01:36:08,600
we talk about classification and it's still useful as a concept you know that it's

1495
01:36:08,600 --> 01:36:12,220
a nonlinear manifold you can see it's two dimensional and is useful for showing this

1496
01:36:12,220 --> 01:36:16,840
idea in the cities the road network connecting all these cities which are the data

1497
01:36:16,840 --> 01:36:20,260
points together and what you're really interested in is the distance

1498
01:36:21,680 --> 01:36:22,820
along this plane you

1499
01:36:23,820 --> 01:36:27,220
so that's why the distance along the surface if you that you are interested in

1500
01:36:27,640 --> 01:36:29,800
when you drive around europe all over the mountains

1501
01:36:30,800 --> 01:36:34,120
that's what you are interested in you can't quite computer see approximated by

1502
01:36:35,280 --> 01:36:40,340
these roads connecting all the data points together now even in this very well samples

1503
01:36:41,030 --> 01:36:45,490
manifold is extremely well sample considers in three dimensions and there's a bunch of data

1504
01:36:45,490 --> 01:36:47,720
here look at the big holes appear in

1505
01:36:48,180 --> 01:36:53,320
the road network back and have some fun effects but this is i mean this i think my favourite

1506
01:36:53,820 --> 01:36:59,590
a the spectral dimensionality reduction methods because it's so simple and it works it works quite nicely so

1507
01:37:00,660 --> 01:37:03,090
just to illustrate what goes on what can go wrong in

1508
01:37:04,120 --> 01:37:08,410
so construct nearest neighbors for each data point so here is a manifold i generated

1509
01:37:08,410 --> 01:37:12,140
are used a gaussian process a generator it hopefully you can see what's shape is

1510
01:37:12,140 --> 01:37:13,010
even though it's three

1511
01:37:13,870 --> 01:37:18,410
is in sort of instituting manifold living in three so some species like this

1512
01:37:20,870 --> 01:37:25,340
because i generated in the latent space i know the neighborhood in the latent space so i can draw it

1513
01:37:26,620 --> 01:37:29,840
and that's really what you're interested so you wanna know the distance between these two

1514
01:37:29,840 --> 01:37:34,090
point but we're not allowing more than four neighbours so the approximation to this is

1515
01:37:34,090 --> 01:37:36,930
to go along this neighbours neighbours neighbours this state yeah

1516
01:37:37,780 --> 01:37:39,410
very nice and you can compute

1517
01:37:40,990 --> 01:37:45,840
and there's these funky algorithms floyds and something else for computing these things fast

1518
01:37:46,390 --> 01:37:51,620
relatively fast rate and you get the distance between these two guys and you do classical multidimensional scaling

1519
01:37:53,530 --> 01:37:55,240
so that's works really well

1520
01:37:55,240 --> 01:37:59,430
well there

1521
01:37:59,490 --> 01:38:01,660
this is because the

1522
01:38:01,680 --> 01:38:05,890
the company

1523
01:38:07,960 --> 01:38:10,160
the group

1524
01:38:34,550 --> 01:38:37,740
first of all let me thank you very much for this opportunity

1525
01:38:38,530 --> 01:38:45,020
a couple of things i'd like to clarify just as we will along i have

1526
01:38:45,020 --> 01:38:49,150
been in this business for quite a long time and not around and specialist but

1527
01:38:49,850 --> 01:38:54,860
being kind of talk for a long time that it's important i find to the

1528
01:38:54,860 --> 01:38:57,710
start to this will be a very informal

1529
01:38:57,890 --> 01:39:00,560
so i want to keep the informal

1530
01:39:00,610 --> 01:39:02,410
most of you

1531
01:39:02,460 --> 01:39:08,130
english is certainly at at least your second language is not perhaps even years unforced

1532
01:39:08,170 --> 01:39:11,630
i don't even speak english i a natural scott

1533
01:39:11,640 --> 01:39:13,820
so i may have

1534
01:39:13,840 --> 01:39:15,720
very slight accent

1535
01:39:17,990 --> 01:39:21,170
if you have any difficulty with anything i say

1536
01:39:21,190 --> 01:39:25,320
and if i get excited i tend to speak a little bit quickly

1537
01:39:25,340 --> 01:39:27,970
stop me and ask me questions

1538
01:39:28,090 --> 01:39:32,820
again as we're going through if something that comes up and you want to ask

1539
01:39:32,820 --> 01:39:38,230
a few questions stop we and ask will try keep the the questioning as we

1540
01:39:38,230 --> 01:39:43,450
go through short because of what of time potentially at the end to cover

1541
01:39:45,630 --> 01:39:49,330
professor jesse said i would like to cover

1542
01:39:50,320 --> 01:39:55,490
applications initially a little bit of chatter about reddish equipment we are

1543
01:39:55,500 --> 01:40:02,760
and then go to some applications with the AFM with some tears applications mainly using

1544
01:40:03,010 --> 01:40:07,480
basically in nano field because it come into the nano field and some stuff on

1545
01:40:07,480 --> 01:40:11,920
scanning electron microscopy have loads and loads and loads of material

1546
01:40:11,960 --> 01:40:13,530
but we don't have

1547
01:40:13,640 --> 01:40:17,420
we have limited time so i to take questions

1548
01:40:18,120 --> 01:40:21,790
as i say stop me and ask

1549
01:40:21,810 --> 01:40:22,640
o i thing

1550
01:40:22,650 --> 01:40:26,260
what is very brief cover what is roman

1551
01:40:26,270 --> 01:40:30,470
it is important to realize that rahman is not new technique from is a very

1552
01:40:30,470 --> 01:40:32,180
very old technique

1553
01:40:32,250 --> 01:40:36,430
i only really in the past twenty years the roman has become much much more

1554
01:40:37,020 --> 01:40:38,800
a user technique

1555
01:40:38,810 --> 01:40:43,710
this is because of the advent of lasers and computers and CCD chips which is

1556
01:40:43,710 --> 01:40:47,390
allowed the the actor collection of perspective difficult

1557
01:40:47,410 --> 01:40:50,870
i spent a lot of time in india so this slide is a well known

1558
01:40:50,920 --> 01:40:55,190
CV raman did the work in original in nineteen twenty eight

1559
01:40:55,210 --> 01:40:58,070
and you know that long time ago

1560
01:40:58,880 --> 01:41:05,050
the spectrometer the he used in his actually still working i original spectrometer and at

1561
01:41:05,050 --> 01:41:07,690
the moment six of them and they actually all

1562
01:41:07,720 --> 01:41:08,820
still function

1563
01:41:10,310 --> 01:41:12,830
ramon still goes on

1564
01:41:12,840 --> 01:41:18,090
i think quickly light scattering technique impinge on the sample

1565
01:41:18,100 --> 01:41:19,540
light scatters off

1566
01:41:19,560 --> 01:41:21,880
and this is the important part

1567
01:41:21,900 --> 01:41:25,030
place less than one part in ten million

1568
01:41:25,060 --> 01:41:26,420
is romany

1569
01:41:27,780 --> 01:41:30,490
so getting that right out from the laser light

1570
01:41:30,640 --> 01:41:32,550
actually the difficult part

1571
01:41:32,630 --> 01:41:38,000
traditionally this was done using diffraction gratings and you lost more signals

1572
01:41:38,030 --> 01:41:39,660
when you actually had

1573
01:41:39,670 --> 01:41:43,750
with the advent of a holographic not filters about fifteen twenty years ago

1574
01:41:44,500 --> 01:41:49,430
things improved dramatically not so much in the fact that the the raman was better

1575
01:41:49,470 --> 01:41:52,340
but the ability to collect farming was actually simpler

1576
01:41:52,350 --> 01:41:55,390
and it took a lot of the expertise

1577
01:41:55,410 --> 01:42:00,930
i emphasise the idea of using raman instruments like drum to be used by multiple

1578
01:42:00,930 --> 01:42:05,710
users rather than optical physicist and that's that's a good way to go

1579
01:42:05,720 --> 01:42:07,810
why obama

1580
01:42:07,860 --> 01:42:10,390
first though

1581
01:42:10,410 --> 01:42:12,250
it is extremely sensitive

1582
01:42:12,260 --> 01:42:15,840
i think amphetamine and methamphetamine very

1583
01:42:15,860 --> 01:42:19,740
very closely associated chemical the roman data

1584
01:42:19,810 --> 01:42:25,400
it is phenomenally difference each region in the CCD imaging and in the fingerprint raman

1585
01:42:25,400 --> 01:42:28,200
region which is around here

1586
01:42:28,220 --> 01:42:31,640
roman typically any

1587
01:42:31,660 --> 01:42:37,610
change in the chemical stock took a toll crystalline with this additional waste material you

1588
01:42:37,610 --> 01:42:42,590
will get change in raman spectra simple idea if something goes from a high data

1589
01:42:42,590 --> 01:42:45,190
to him high then you will get

1590
01:42:45,200 --> 01:42:47,330
a change in the raman spectrum

1591
01:42:47,380 --> 01:42:50,170
and these are very subtle changes

1592
01:42:50,350 --> 01:42:55,270
the problem is that people find is that the the automatically assume but we can

1593
01:42:55,270 --> 01:42:56,920
do all this with FTIR

1594
01:42:56,930 --> 01:42:58,810
a lot of can

1595
01:42:58,820 --> 01:43:00,270
but with one

1596
01:43:00,280 --> 01:43:02,380
you can do it in more samples

1597
01:43:02,560 --> 01:43:08,540
you can do everything from what weight samples are going to be looking at

1598
01:43:08,550 --> 01:43:15,330
moist materials reaction vessels this type of thing high-pressure sales these types of things from

1599
01:43:15,330 --> 01:43:16,930
and can run in work

1600
01:43:17,120 --> 01:43:20,450
so here but i think it's important to see the difference

1601
01:43:20,470 --> 01:43:21,970
and the chemical structures

1602
01:43:22,010 --> 01:43:23,850
is very very obvious

1603
01:43:23,870 --> 01:43:25,630
and in roman data

1604
01:43:25,680 --> 01:43:28,520
so the highly sensitive things

1605
01:43:29,350 --> 01:43:31,390
that's where we are now

1606
01:43:31,390 --> 01:43:33,810
one extra multiplication

1607
01:43:33,830 --> 01:43:37,890
compute the inner product by this in in a very high dimensional space

1608
01:43:37,910 --> 01:43:41,500
so in that example of the

1609
01:43:41,520 --> 01:43:43,700
of the

1610
01:43:43,830 --> 01:43:46,850
two images around six hundred pixels

1611
01:43:46,870 --> 01:43:51,600
rather than computing at one hundred eighty thousand dimensional vector and another one hundred eighty

1612
01:43:51,600 --> 01:43:55,520
thousand dimensional vector and multiplying into the components together and something

1613
01:43:55,540 --> 01:44:01,200
we actually take a six hundred dimensional vector not most by the corresponding entry in

1614
01:44:01,220 --> 01:44:04,290
the other six hundred dimensional vector sum them up

1615
01:44:04,310 --> 01:44:09,750
and one extra operation which is square the result

1616
01:44:09,750 --> 01:44:15,040
so this is clearly a being shortcut no really serious shortcuts and effectively we can

1617
01:44:15,040 --> 01:44:16,640
apply this algorithm

1618
01:44:16,720 --> 01:44:19,500
as i said you know these all you have to do is

1619
01:44:19,520 --> 01:44:22,060
completely inappropriate here and square

1620
01:44:22,080 --> 01:44:26,770
all the sorry the inner product in the input space and square and you've got

1621
01:44:27,450 --> 01:44:31,500
entries in this optimisation problem you solve it you've got the alphas and i want

1622
01:44:31,500 --> 01:44:34,270
to evaluate a new point you can do the same thing

1623
01:44:35,060 --> 01:44:41,450
using this evaluation here competing approach in input space and square some with the corresponding

1624
01:44:41,450 --> 01:44:42,640
alpha rise

1625
01:44:42,660 --> 01:44:45,560
and you have draw

1626
01:44:49,180 --> 01:44:51,640
OK so

1627
01:44:51,680 --> 01:44:54,250
so that that's just what i've said

1628
01:44:54,270 --> 01:44:58,390
he competed at six hundred dimensional inner product and then square the result

1629
01:44:58,410 --> 01:45:04,490
you can even work in infinite dimensional space in fact because this kernel here

1630
01:45:04,490 --> 01:45:07,750
it corresponds to an infinite dimensional space so you can

1631
01:45:07,770 --> 01:45:10,250
with this finite computation

1632
01:45:10,250 --> 01:45:16,750
finite optimisation problem you actually are working in infinite dimensional space

1633
01:45:16,770 --> 01:45:20,560
here is very strange feeling

1634
01:45:21,520 --> 01:45:23,310
you might say well OK

1635
01:45:23,330 --> 01:45:27,080
i i can think of lots of functions that will be nice and you know

1636
01:45:27,080 --> 01:45:28,410
it's kind of functions

1637
01:45:28,790 --> 01:45:33,930
effectively all i need to do is find some way of comparing two inputs

1638
01:45:35,080 --> 01:45:37,560
you know that would be a potential kernel function

1639
01:45:37,560 --> 01:45:40,040
there is a restriction however the

1640
01:45:40,250 --> 01:45:41,970
arises from

1641
01:45:41,970 --> 01:45:43,560
the assumption that the

1642
01:45:43,580 --> 01:45:48,310
the kernel function does correspond to an inner product in some feature space and they

1643
01:45:48,310 --> 01:45:51,790
corresponds to if we take in general

1644
01:45:51,790 --> 01:45:55,060
some of this type will be to being any

1645
01:45:55,080 --> 01:45:56,140
real valued

1646
01:45:56,160 --> 01:45:57,790
and he realizes here

1647
01:45:57,810 --> 01:46:00,080
we can write the following

1648
01:46:00,890 --> 01:46:03,390
substitute the kernel function for the

1649
01:46:03,390 --> 01:46:06,970
in the product that is and then bring these

1650
01:46:07,220 --> 01:46:11,700
beta inside the inner product and we end up with a norm here because this

1651
01:46:11,700 --> 01:46:15,430
is just the son peter i five a sign that the same values and this

1652
01:46:15,430 --> 01:46:18,950
is just the norm squared of this vector and so must be because they are

1653
01:46:18,950 --> 01:46:20,160
equal to zero

1654
01:46:20,180 --> 01:46:21,330
so this

1655
01:46:21,350 --> 01:46:24,850
is known as the positive semi definite property

1656
01:46:24,870 --> 01:46:27,600
if you think of this as the matrix

1657
01:46:27,620 --> 01:46:29,870
then this is the

1658
01:46:29,910 --> 01:46:33,990
shows this matrix has nonnegative eigen values

1659
01:46:34,000 --> 01:46:37,500
again i don't want to go into any details but this is the restriction

1660
01:46:37,520 --> 01:46:39,990
that must hold for any training set

1661
01:46:40,000 --> 01:46:44,640
but this is the only restriction i mean it provided a kernel function satisfies this

1662
01:46:45,950 --> 01:46:52,310
and obviously symmetric you know so you should if you swap components shouldn't change value

1663
01:46:53,330 --> 01:46:54,220
it is

1664
01:46:54,250 --> 01:46:58,770
a kernel function in the sense that it corresponds to some projection into a feature

1665
01:46:59,790 --> 01:47:00,720
so now

1666
01:47:01,160 --> 01:47:05,370
rather than having to worry about feature spaces and defining feature spaces

1667
01:47:05,390 --> 01:47:10,810
we have to worry about kernels defining ways of comparing factors i would argue that

1668
01:47:10,810 --> 01:47:16,120
actually isn't it might sound like a worse situation but i think actually it's

1669
01:47:16,140 --> 01:47:20,370
often easier to get somebody with domain expertise to talk about how to compare two

1670
01:47:23,200 --> 01:47:26,250
get to suggest some feature representations

1671
01:47:26,270 --> 01:47:30,790
it's not always the case you there are sometimes natural feature representations that arise and

1672
01:47:30,790 --> 01:47:33,390
again we may be able to use them to define kernels

1673
01:47:33,410 --> 01:47:38,540
but but quite frequently you know a domain expert knows about his data those when

1674
01:47:38,540 --> 01:47:39,930
two things are similar

1675
01:47:39,950 --> 01:47:44,680
but may not know what features of those things make them similar or dissimilar

1676
01:47:44,700 --> 01:47:46,310
so it's

1677
01:47:46,350 --> 01:47:50,450
but certainly in applying this approach

1678
01:47:50,470 --> 01:47:54,700
you know defining the kernel function is going to be a critical component

1679
01:47:54,830 --> 01:47:58,060
in the rollout out of the technology

1680
01:47:58,080 --> 01:48:00,220
so what have we achieved

1681
01:48:00,250 --> 01:48:06,290
we can replace the problem of neural network architectures by kernel definition so whereas in

1682
01:48:06,290 --> 01:48:07,600
the neural network

1683
01:48:08,660 --> 01:48:13,750
UB be saying OK got to you know choose an architecture choose how many hidden

1684
01:48:13,750 --> 01:48:16,000
nodes or whatever it is

1685
01:48:16,040 --> 01:48:18,620
we've now

1686
01:48:18,660 --> 01:48:22,890
arrived the situation where our problem is to define a kernel function

1687
01:48:23,750 --> 01:48:28,040
as i have suggested that possibly arguably more natural

1688
01:48:28,040 --> 01:48:34,410
but the restriction that i described on that function is natural in this sort of

1689
01:48:34,410 --> 01:48:37,910
positivity positive semi definite restriction is a bit weird

1690
01:48:37,950 --> 01:48:40,930
so you know some researchers looked at

1691
01:48:40,950 --> 01:48:45,160
trying to get around that are not required positive semidefiniteness

1692
01:48:46,890 --> 01:48:49,680
it's certainly not a silver bullet because

1693
01:48:49,700 --> 01:48:53,250
you know as with any learning as i suggested you know it's important to get

1694
01:48:53,270 --> 01:48:55,620
fit with the data you know if you

1695
01:48:55,640 --> 01:48:56,990
if you have a problem

1696
01:48:57,000 --> 01:49:02,000
and you just roll out your favourite kernel maybe there is to fit between the

1697
01:49:02,000 --> 01:49:03,470
problem that kernel

1698
01:49:03,530 --> 01:49:08,950
so you may have to design the cover for the problem particularly if you have

1699
01:49:09,000 --> 01:49:11,770
you know more unusual type of data

1700
01:49:11,790 --> 01:49:17,060
but the other sort of nice property that we're somehow manage to arrive at without

1701
01:49:17,060 --> 01:49:19,080
really intending to is this

1702
01:49:19,100 --> 01:49:20,600
the property here

1703
01:49:21,120 --> 01:49:26,870
you know typically linear learning can only be applied if you have a vector representation

1704
01:49:26,870 --> 01:49:30,770
of your data i mean that's exactly what a linear function is a linear weighting

1705
01:49:30,770 --> 01:49:32,080
of those different

1706
01:49:32,100 --> 01:49:33,620
one of the feature vector

1707
01:49:33,660 --> 01:49:35,810
but now

1708
01:49:36,500 --> 01:49:40,200
however i the situation where we can potentially define kernel

1709
01:49:40,220 --> 01:49:45,100
for non vectorial data so we might have a no strings or graph for

1710
01:49:45,680 --> 01:49:47,910
in know some complex data structure

1711
01:49:47,930 --> 01:49:51,640
all we have to do is to define a function that takes two objects of

1712
01:49:51,640 --> 01:49:52,850
this type

1713
01:49:52,870 --> 01:49:57,290
somehow compares them and arrived with a single number

1714
01:49:57,310 --> 01:50:00,080
and that satisfies the property that i described

1715
01:50:00,100 --> 01:50:01,250
and we can apply

1716
01:50:01,310 --> 01:50:03,790
linear learning machines on that data

1717
01:50:03,810 --> 01:50:07,370
so suddenly sort of open up the potential of

1718
01:50:07,370 --> 01:50:12,330
have done or or or or bought as recommendations to the other two this customer

1719
01:50:12,980 --> 01:50:18,080
and OK let's just move on here so

1720
01:50:18,090 --> 01:50:22,790
so the advantages of collaborative filtering is then you know we can apply then even

1721
01:50:22,790 --> 01:50:28,090
if the content is not easily analyse like in the multimedia something we can also

1722
01:50:28,090 --> 01:50:32,310
filter items based on quality and taste and not just content but that's also of

1723
01:50:32,310 --> 01:50:37,470
course as we've seen before in the web context that's that's important and they're already

1724
01:50:37,610 --> 01:50:42,910
be relied on human annotations namely the links to help us and here again you

1725
01:50:42,910 --> 01:50:49,170
know we could we could exploit the the things we observe about user preference and

1726
01:50:49,170 --> 01:50:54,700
then also you know it might help help us actually defined serendipitous recommendations so sometimes

1727
01:50:54,700 --> 01:50:59,370
making recommendations for things were based on the content is not very clear that there

1728
01:50:59,410 --> 01:51:03,960
even related OK but that's a lot of people who buy a and b a

1729
01:51:03,960 --> 01:51:07,030
and b based on the content seem to have nothing to do with one another

1730
01:51:07,030 --> 01:51:11,070
but still in my opinion might be an interesting connection so we can do these

1731
01:51:11,070 --> 01:51:14,260
types of things in this context

1732
01:51:14,280 --> 01:51:17,190
OK so

1733
01:51:17,230 --> 01:51:23,630
so so the the standard approach that has been developed i mean this collaborative filtering

1734
01:51:23,670 --> 01:51:30,140
technique basically that came up in the early-mid nineties and then in the late nineties

1735
01:51:30,140 --> 01:51:34,820
was also commercialized and the basic idea is really of what has been done there

1736
01:51:34,820 --> 01:51:39,700
are is the use of nearest neighbour type of approaches so the idea is well

1737
01:51:39,700 --> 01:51:44,620
first you compute a similarity function between the user i and the user came

1738
01:51:44,660 --> 01:51:48,970
right how do you do that it a popular way to appears on correlation coefficient

1739
01:51:48,970 --> 01:51:52,270
and it works like this you look at all the

1740
01:51:52,280 --> 01:51:55,560
items that have been rated by

1741
01:51:55,580 --> 01:51:59,960
user i and j so here we actually assume i should have said that we

1742
01:52:00,020 --> 01:52:05,370
we have ratings available for a subset of items that user has rated a particular

1743
01:52:05,370 --> 01:52:10,720
set of items think of about five five-star scale OK so we look at

1744
01:52:10,760 --> 01:52:14,110
the items that have been rated by both

1745
01:52:14,120 --> 01:52:17,860
and we compare the rating of an item by user i

1746
01:52:17,890 --> 01:52:21,870
normalized by the mean actually so we subtract the mean for user i and we

1747
01:52:21,870 --> 01:52:23,990
look at the rating of user

1748
01:52:24,000 --> 01:52:28,010
OK on that item j subtracting the mean

1749
01:52:28,200 --> 01:52:31,190
for that user OK so we we take

1750
01:52:31,240 --> 01:52:34,920
that inner product if you like and then we

1751
01:52:34,930 --> 01:52:39,530
the normalized by the standard deviations of the ratings of user i and k that

1752
01:52:39,530 --> 01:52:45,000
product OK so that the correlation coefficient usually which just some overall dimensions here

1753
01:52:45,010 --> 01:52:49,290
but since you know they'll they'll of missing values you only some of those that

1754
01:52:49,290 --> 01:52:52,620
are in common right you can otherwise you can really compare

1755
01:52:52,630 --> 01:52:56,710
by the correlation between two users you can only compare it on the things that

1756
01:52:57,370 --> 01:53:01,350
separated in common setting well and then you get the similarity

1757
01:53:03,220 --> 01:53:08,370
and then you do some type of nearest neighbour regression where you say well the

1758
01:53:08,370 --> 01:53:12,160
predicted rating for user i on item j

1759
01:53:12,610 --> 01:53:14,140
is that they

1760
01:53:14,150 --> 01:53:18,380
you take the mean rating of user i and then relative to the mean rating

1761
01:53:18,380 --> 01:53:20,520
you look at all the

1762
01:53:20,530 --> 01:53:23,040
a certain set of nearest neighbors typically

1763
01:53:23,050 --> 01:53:27,190
of user i the nearest neighbours being the ones with the highest similarity

1764
01:53:27,210 --> 01:53:30,750
OK you take the similarity as the weight

1765
01:53:30,770 --> 01:53:32,930
and then you look at

1766
01:53:32,980 --> 01:53:37,340
the difference between the rating of that user k that we're looking at here

1767
01:53:37,440 --> 01:53:41,210
the rating on j relative to

1768
01:53:41,220 --> 01:53:45,760
the average rating of that users so saying that well does that use rate that

1769
01:53:45,760 --> 01:53:50,270
particular item higher-than-average or lower than average and if so by what amount and these

1770
01:53:50,270 --> 01:53:56,230
increments we wait with the similarity and then we normalized by the sum of the

1771
01:53:58,670 --> 01:54:02,370
one of the l one norm if you like of these

1772
01:54:02,390 --> 01:54:05,910
adjusting the absolute values of the similarities and that is

1773
01:54:05,940 --> 01:54:10,450
kind of the prediction we make it so we say prediction that we make about

1774
01:54:10,450 --> 01:54:15,640
the rating of user i on an unseen item j is the average

1775
01:54:15,660 --> 01:54:21,020
rating for that user plus an increment and increment is just the weighted sum of

1776
01:54:21,020 --> 01:54:23,770
increments from similar users

1777
01:54:23,780 --> 01:54:29,710
so so this has been is a very simple idea and has been very popular

1778
01:54:29,730 --> 01:54:34,330
in terms of learning it doesn't really require any learning because you know the memory

1779
01:54:34,330 --> 01:54:38,820
based learning people call it right you just store all the information and then once

1780
01:54:38,820 --> 01:54:42,340
you want to make prediction just go into database and computed right but there is

1781
01:54:42,340 --> 01:54:43,340
no offline

1782
01:54:43,350 --> 01:54:44,750
imputation yes

1783
01:54:46,630 --> 01:54:49,340
i this

1784
01:54:53,540 --> 01:54:55,710
so this year

1785
01:55:00,420 --> 01:55:03,160
if you look like

1786
01:55:09,460 --> 01:55:12,480
so the question so this is it for

1787
01:55:12,500 --> 01:55:19,000
right for the so the simple cases where you really have to great right there

1788
01:55:19,540 --> 01:55:24,200
the situation where you say well i why you think of the binary by writing

1789
01:55:24,450 --> 01:55:29,440
things that people don't buy don't like saying things like like

1790
01:55:29,450 --> 01:55:30,700
you know that

1791
01:55:30,710 --> 01:55:33,460
might not be a descent try

1792
01:55:33,470 --> 01:55:37,630
but you know basically

1793
01:55:37,850 --> 01:55:43,620
so so in this setting if you want to move to towards more interesting things

1794
01:55:43,640 --> 01:55:47,830
it becomes a little bit unclear how actually do that

1795
01:55:47,850 --> 01:55:51,230
so this is why i think actually that methods

1796
01:55:51,250 --> 01:55:53,680
for instance that use more probabilistic

1797
01:55:53,700 --> 01:55:57,070
approach right where you could actually model the types of things

1798
01:55:57,140 --> 01:56:00,890
but you observe and related to the types of things that you really interested in

1799
01:56:00,890 --> 01:56:05,850
the response variable makes more sense here

1800
01:56:05,870 --> 01:56:07,730
so you have to rely

1801
01:56:07,750 --> 01:56:11,820
on that or you have to find you know basically if there's some other way

1802
01:56:11,820 --> 01:56:15,530
that you can come up with a similarity measure right you could still use

1803
01:56:15,580 --> 01:56:17,330
some formulas like that

1804
01:56:17,340 --> 01:56:21,480
perhaps but if you have right but this still as well this still assumes that

1805
01:56:21,480 --> 01:56:24,560
we are dealing with great

1806
01:56:24,900 --> 01:56:28,580
so it's somewhat limited

1807
01:56:28,590 --> 01:56:31,480
OK let me

1808
01:56:31,490 --> 01:56:34,690
so let me present now another way that might be more

1809
01:56:34,700 --> 01:56:40,310
along the lines of what you've learned here in the last two weeks

1810
01:56:40,360 --> 01:56:44,320
and that's we now can we also do this in a discriminative way so what

1811
01:56:44,330 --> 01:56:46,230
we like to learn really

1812
01:56:46,230 --> 01:56:47,400
it is the a function

1813
01:56:47,420 --> 01:56:52,770
from user item pairs into let's say in this case of ratings right the numbers

1814
01:56:52,770 --> 01:56:55,170
from zero to two are

1815
01:56:55,220 --> 01:57:00,590
you know referring to the number of stars or something like that find score

1816
01:57:00,640 --> 01:57:04,360
so you know for each user and each item we would like to sign such

1817
01:57:04,360 --> 01:57:05,520
a number

1818
01:57:05,530 --> 01:57:12,580
there are some problems so one is how to interpret actually the ratings

1819
01:57:12,610 --> 01:57:17,060
we could just take them as you know absolute numbers but but often that's really

1820
01:57:17,060 --> 01:57:20,260
not what we want rather what we often one is we only want to give

1821
01:57:20,260 --> 01:57:24,490
them the meaning on an ordinal scale meaning you know if we have a rating

1822
01:57:24,490 --> 01:57:28,270
and we have another rating that is higher than we'd like to say OK well

1823
01:57:28,270 --> 01:57:33,430
this is preferred over the other but the the numerical value of the difference

1824
01:57:34,210 --> 01:57:36,010
we don't really want to

1825
01:57:36,070 --> 01:57:39,030
perhaps interpret that you know that

1826
01:57:39,110 --> 01:57:42,460
it's not clear that has meaning so it's a different whether we talk about an

1827
01:57:42,460 --> 01:57:47,310
absolute scale ordinal scale so in order to deal with that we can do is

1828
01:57:47,310 --> 01:57:49,370
we can use ordinal regression

1829
01:57:49,420 --> 01:57:53,620
techniques so we treat it as an ordinal regression problem and the second challenge is

1830
01:57:53,620 --> 01:57:56,110
well if we do that if we want to learn the mapping how do we

1831
01:57:56,110 --> 01:57:58,010
actually represent

1832
01:57:58,040 --> 01:57:59,270
things over here

1833
01:57:59,280 --> 01:58:05,460
right particular user item pair can just take a number that is the number of

1834
01:58:05,460 --> 01:58:09,190
that user and the number just some identifier for the item i wouldn't be able

1835
01:58:09,190 --> 01:58:13,790
to learn the mapping i need something that characterizes users and items right

1836
01:58:13,810 --> 01:58:18,270
OK and so here's the first of all the idea of how you can do

