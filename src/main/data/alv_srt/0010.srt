1
00:00:00,000 --> 00:00:03,330
also help me choose the correct number of clusters

2
00:00:03,350 --> 00:00:06,060
so it looks like a very promising tool

3
00:00:06,080 --> 00:00:06,890
we view

4
00:00:07,080 --> 00:00:11,620
stability is a measure of fit between the probability distribution and a clustering model

5
00:00:13,720 --> 00:00:19,410
but it fails when the clustering model is not aligned with the input data

6
00:00:19,430 --> 00:00:23,600
and so we we therefore can use it as a model selection tool and that's

7
00:00:23,600 --> 00:00:27,290
the way it's being used is about selection tools you want to select the right

8
00:00:27,290 --> 00:00:30,750
model the right algorithm the right number of k

9
00:00:31,200 --> 00:00:33,140
for your data

10
00:00:33,160 --> 00:00:37,520
you estimate the instability of your algorithm on your data

11
00:00:37,560 --> 00:00:42,910
and you choose the algorithm all parameters that maximize the stability

12
00:00:42,930 --> 00:00:47,100
that's the way it's being used as a model selection tool

13
00:00:47,120 --> 00:00:49,540
and now we try

14
00:00:49,560 --> 00:00:52,520
to prove that really works

15
00:00:55,020 --> 00:00:59,750
we looked at two different algorithm basic on incentive-based like k

16
00:00:59,770 --> 00:01:04,250
i mean the linkage based algorithms and we look at the very simple set of

17
00:01:04,250 --> 00:01:10,410
that think this a synthetic state of the mixture of gaussians all

18
00:01:10,430 --> 00:01:12,950
mixture of tree because

19
00:01:12,950 --> 00:01:19,540
a this uniform distribution and we could prove that in all those cases

20
00:01:19,560 --> 00:01:23,060
those two types of these types of that

21
00:01:23,080 --> 00:01:30,220
we could prove that in all these cases stability correctly detected the right word and

22
00:01:30,220 --> 00:01:31,910
the right number of clusters

23
00:01:32,120 --> 00:01:34,930
this is the proof now it's not the

24
00:01:34,970 --> 00:01:38,660
you could but in order to prove it we need to restrict our attention to

25
00:01:38,660 --> 00:01:40,830
very specific distributions you can

26
00:01:40,830 --> 00:01:42,370
very difficult to prove

27
00:01:42,390 --> 00:01:47,640
that would be the correct number of clusters for every distribution but was the nation's

28
00:01:47,660 --> 00:01:52,470
and we also have experimental results show that was very nicely so here for example

29
00:01:52,810 --> 00:01:57,700
i want doing here is i have my distribution is this swiss roll with three

30
00:01:57,700 --> 00:02:02,600
different components so there is chocolate vanilla and

31
00:02:02,620 --> 00:02:04,120
something j

32
00:02:04,180 --> 00:02:06,350
three different companies in your swiss roll

33
00:02:06,370 --> 00:02:10,020
and you want to be able to claim and the and and doing a single

34
00:02:10,020 --> 00:02:12,600
linkage clustering and my parameter

35
00:02:12,640 --> 00:02:15,850
is when do i stop matching points

36
00:02:15,870 --> 00:02:18,560
so if i choose the distance

37
00:02:18,580 --> 00:02:23,370
for matching points too large then get this kind of

38
00:02:23,450 --> 00:02:26,290
picture where it connects points across

39
00:02:26,310 --> 00:02:28,000
different layers

40
00:02:28,160 --> 00:02:31,290
if i choose my distances

41
00:02:31,310 --> 00:02:32,080
to be a

42
00:02:32,080 --> 00:02:36,790
my my threshold to be point fifteen i get is very nice but fissioning into

43
00:02:36,790 --> 00:02:37,870
three rules

44
00:02:37,890 --> 00:02:39,500
if i choose my

45
00:02:39,560 --> 00:02:41,560
threshold to be too small

46
00:02:42,580 --> 00:02:44,910
factions into small

47
00:02:45,930 --> 00:02:47,930
and the question is so we know

48
00:02:47,950 --> 00:02:53,580
the correct for this is that the correct value of threshold should be point fifteen

49
00:02:53,580 --> 00:02:56,660
can stability detected for me

50
00:02:57,830 --> 00:02:59,720
we have this very nice picture well

51
00:02:59,790 --> 00:03:04,770
here i draw the instability with different threshold values

52
00:03:04,810 --> 00:03:09,140
and here in order to know whether i got the correct thing or not i

53
00:03:09,160 --> 00:03:14,540
what i draw here is the percentage of points that fall into the third largest

54
00:03:15,640 --> 00:03:18,220
so if you have ten clusters are just looking at the

55
00:03:18,470 --> 00:03:22,180
and what percentage of four of points fall

56
00:03:22,270 --> 00:03:28,430
and because we know that it was generated by three roads

57
00:03:28,450 --> 00:03:30,140
we want wanted to sound

58
00:03:30,160 --> 00:03:33,080
largest cluster will contain a sort of the point

59
00:03:33,220 --> 00:03:38,040
and if we compare the graph of instability with different values and

60
00:03:38,100 --> 00:03:42,910
the size of the largest cluster we see that the instability beautiful it attacks the

61
00:03:42,910 --> 00:03:48,200
values to give you the correct clustering police

62
00:03:48,200 --> 00:03:54,600
this implies this this empirical but we can also prove that it does so in

63
00:03:54,700 --> 00:03:57,890
the situation was

64
00:03:59,080 --> 00:04:01,640
two o five the situation was very nice

65
00:04:01,660 --> 00:04:05,450
we could formally we formally define the notion of stability

66
00:04:05,450 --> 00:04:13,060
we could argue that necessary property of any clustering method we could prove that really

67
00:04:13,060 --> 00:04:20,060
works for those very simple distributions like mixture of gaussians swiss roll

68
00:04:22,140 --> 00:04:27,100
we so we could prove that it works very nicely and also very nicely in

69
00:04:27,980 --> 00:04:31,180
so everybody was happy and we said

70
00:04:31,180 --> 00:04:37,950
in the beginning of two after the beginning of january to the sixth or you

71
00:04:37,950 --> 00:04:42,850
can look for domain was my student we set to prove you know to put

72
00:04:42,850 --> 00:04:48,200
the final nail into the structure to prove that stability really does what you want

73
00:04:48,200 --> 00:04:50,790
because they have all the possible evidence

74
00:04:53,430 --> 00:04:56,060
it's a question of how do

75
00:04:56,080 --> 00:05:01,120
from what is the correct job but i don't know about that

76
00:05:01,120 --> 00:05:02,040
but then

77
00:05:02,040 --> 00:05:05,060
we started working on this and

78
00:05:05,080 --> 00:05:06,620
we started seeing some

79
00:05:06,720 --> 00:05:08,060
things which are not so

80
00:05:08,080 --> 00:05:10,120
satisfactory so consider for example

81
00:05:10,140 --> 00:05:11,430
these two

82
00:05:11,620 --> 00:05:16,180
datasets and assume that you have some k bigger than one

83
00:05:16,200 --> 00:05:20,540
what happened here will be stable and not just take a random sample want to

84
00:05:20,540 --> 00:05:22,350
cluster into three sets

85
00:05:22,370 --> 00:05:28,310
then another random sample clustering trees that that's my generating distribution would be stable we

86
00:05:28,390 --> 00:05:30,930
shift all the time

87
00:05:32,580 --> 00:05:36,240
the shift over the time right but what happened here

88
00:05:36,250 --> 00:05:38,310
if made that

89
00:05:38,310 --> 00:05:39,980
it will be stable

90
00:05:40,000 --> 00:05:43,240
but it is a very small change of my that

91
00:05:43,290 --> 00:05:46,540
if i take the position into three sets

92
00:05:46,560 --> 00:05:49,000
eventually what you get is just

93
00:05:49,040 --> 00:05:52,750
three sets like this but here was unstable now

94
00:05:53,470 --> 00:05:56,640
and so the the number of correct number of

95
00:05:56,640 --> 00:05:58,350
clusters is three

96
00:05:58,430 --> 00:06:02,390
you cannot really argue that the correct number three here

97
00:06:02,410 --> 00:06:05,100
not you but stability doesn't detect it

98
00:06:05,120 --> 00:06:09,830
so we started seeing those fun examples and then we could turn them into a

99
00:06:11,240 --> 00:06:17,140
so the bottom line of our analysis which lasted more than a year

100
00:06:17,140 --> 00:06:19,610
in fact only amplitudes at the same

101
00:06:19,620 --> 00:06:22,190
that means the total effect

102
00:06:22,210 --> 00:06:24,440
that you get some of the two

103
00:06:24,490 --> 00:06:26,280
pictorial some of the two

104
00:06:26,300 --> 00:06:30,630
is then going to be too easier realizing call it easier

105
00:06:30,640 --> 00:06:34,120
and then i get the cosine of half the songs

106
00:06:34,210 --> 00:06:35,570
i get the cosine

107
00:06:35,610 --> 00:06:37,090
of omega t

108
00:06:37,140 --> 00:06:39,620
minus delta divided by two

109
00:06:39,640 --> 00:06:41,790
the cosine of half the difference

110
00:06:41,800 --> 00:06:47,620
that becomes the cosine of delta phi phi

111
00:06:47,640 --> 00:06:50,460
now you see why the light intensity

112
00:06:50,500 --> 00:06:53,600
is proportional to the cosine square

113
00:06:53,650 --> 00:06:55,350
delta all two

114
00:06:55,390 --> 00:06:57,780
because when you calculate

115
00:06:57,790 --> 00:07:02,690
the mean value of the point is that you have to cross we

116
00:07:03,500 --> 00:07:07,330
b also has this and also has this

117
00:07:07,440 --> 00:07:11,390
so you get the square of this term and the square of this

118
00:07:11,430 --> 00:07:14,440
but the average of the square of is one

119
00:07:14,460 --> 00:07:15,860
get that for now

120
00:07:15,860 --> 00:07:18,740
so you see is going to be proportional

121
00:07:18,760 --> 00:07:20,310
the square

122
00:07:21,390 --> 00:07:23,050
of two

123
00:07:23,130 --> 00:07:25,510
and so this function that you're seeing

124
00:07:25,530 --> 00:07:28,630
delta is limited proportional to science data

125
00:07:28,710 --> 00:07:30,300
this graph that you

126
00:07:30,350 --> 00:07:38,930
is a cosine square curve

127
00:07:38,950 --> 00:07:41,160
the experiment and show you

128
00:07:41,180 --> 00:07:43,660
has a

129
00:07:43,710 --> 00:07:45,900
separation of the

130
00:07:45,900 --> 00:07:47,700
which is one quarter

131
00:07:47,750 --> 00:07:49,740
my wavelength six hundred ten

132
00:07:49,750 --> 00:07:52,730
thirty three nanometers which is very much the same

133
00:07:52,740 --> 00:07:53,890
and the distance

134
00:07:53,890 --> 00:07:56,730
is about five metres

135
00:07:56,780 --> 00:07:59,100
so the way that

136
00:07:59,140 --> 00:08:02,500
we have this arrangement is that two slits

137
00:08:05,140 --> 00:08:08,790
like this so this separation is one point four millimetre

138
00:08:08,830 --> 00:08:10,900
and then we put over there

139
00:08:10,920 --> 00:08:12,750
a laser beam

140
00:08:12,810 --> 00:08:15,990
and then the result is what you're going to see there

141
00:08:16,040 --> 00:08:16,960
and so

142
00:08:18,210 --> 00:08:21,160
i think about

143
00:08:22,820 --> 00:08:25,670
you see there in order for you to see

144
00:08:25,690 --> 00:08:27,180
i have to make it

145
00:08:39,500 --> 00:08:42,370
when you're up here on the screen

146
00:08:42,380 --> 00:08:47,130
you will see areas which i distinctly dark

147
00:08:47,140 --> 00:08:49,000
you see areas where by

148
00:08:49,960 --> 00:08:51,500
the light in other words

149
00:08:51,550 --> 00:08:54,400
you see the pattern that we just calculated you see

150
00:08:54,410 --> 00:08:56,150
constructive interference

151
00:08:56,160 --> 00:08:59,140
and you see destructive interference

152
00:08:59,220 --> 00:09:01,700
you're looking at an experiment

153
00:09:01,760 --> 00:09:04,970
it is historically of great importance

154
00:09:04,980 --> 00:09:08,810
it is one of the most mysterious and puzzling issues

155
00:09:08,810 --> 00:09:09,710
in all

156
00:09:09,730 --> 00:09:12,180
of physics

157
00:09:13,310 --> 00:09:15,580
interference patterns

158
00:09:15,620 --> 00:09:18,430
it was first shown by young eighteen or what

159
00:09:18,450 --> 00:09:21,170
it's clearly convincing evidence that lights

160
00:09:21,210 --> 00:09:23,670
is away from home

161
00:09:25,130 --> 00:09:27,180
twenty century physics

162
00:09:27,240 --> 00:09:31,150
has shown that light comes in the form of individual photons

163
00:09:31,160 --> 00:09:33,530
with well defined energies

164
00:09:33,530 --> 00:09:34,700
and photons

165
00:09:34,780 --> 00:09:35,870
and behavior

166
00:09:35,870 --> 00:09:39,940
like bullets they have momentum they use radiation pressure

167
00:09:40,040 --> 00:09:43,380
and they can be localized in space

168
00:09:43,420 --> 00:09:47,640
we can detect individual photons in a way that we detect individual

169
00:09:50,210 --> 00:09:52,930
like particles

170
00:09:52,990 --> 00:09:59,330
however the interference that you're staring at on the screen it can only be explained

171
00:09:59,400 --> 00:10:02,070
if we assume that each photon

172
00:10:02,080 --> 00:10:05,090
i went through both slits

173
00:10:05,190 --> 00:10:08,110
but how on earth can one particle

174
00:10:09,080 --> 00:10:11,370
through both slits

175
00:10:11,410 --> 00:10:12,630
it will have to be

176
00:10:12,630 --> 00:10:13,930
one slit

177
00:10:13,990 --> 00:10:15,590
or the other

178
00:10:15,650 --> 00:10:18,030
and here lies the great mystery

179
00:10:19,350 --> 00:10:21,420
is both

180
00:10:21,500 --> 00:10:24,130
it is always if you want to be away

181
00:10:24,200 --> 00:10:28,130
and is the particle if you wanted to be a particle

182
00:10:28,200 --> 00:10:29,570
if you manage

183
00:10:29,580 --> 00:10:32,530
to determine for every photo

184
00:10:32,660 --> 00:10:35,100
which of the two slits it went

185
00:10:35,110 --> 00:10:36,640
and that is possible

186
00:10:36,710 --> 00:10:40,490
do it low light intensity you can rename determined

187
00:10:40,540 --> 00:10:44,740
through which of the two slits each photon went then you will not see any

188
00:10:44,740 --> 00:10:47,140
interference anymore

189
00:10:47,240 --> 00:10:50,440
the pattern on the screen will be like that all the tomatoes you get to

190
00:10:52,460 --> 00:10:54,750
so the moment that you establish

191
00:10:54,800 --> 00:10:57,740
the particle character of the lights

192
00:10:57,810 --> 00:10:59,410
by determining

193
00:10:59,480 --> 00:11:04,140
through which slit it when you have destroyed its weight character

194
00:11:04,190 --> 00:11:05,190
and it will not

195
00:11:05,200 --> 00:11:07,350
behave like we will behave

196
00:11:07,400 --> 00:11:09,680
like particles

197
00:11:09,740 --> 00:11:15,300
however as long as you do not determine through which slit the photon went

198
00:11:15,300 --> 00:11:18,000
does as long as it remains your guess

199
00:11:18,090 --> 00:11:21,010
so which one it when lights were revealed to you

200
00:11:21,090 --> 00:11:22,480
it's wave character

201
00:11:22,540 --> 00:11:24,760
and you will see interference

202
00:11:24,810 --> 00:11:26,570
the choice is yours

203
00:11:26,640 --> 00:11:29,830
but you cannot both ways

204
00:11:29,900 --> 00:11:31,660
so you can now

205
00:11:31,780 --> 00:11:34,060
that's something that had a huge impact

206
00:11:34,080 --> 00:11:35,280
on physics

207
00:11:35,330 --> 00:11:38,180
clearly at the time that this experiment was done

208
00:11:38,200 --> 00:11:39,590
it was accepted

209
00:11:42,220 --> 00:11:43,490
now look at it

210
00:11:43,510 --> 00:11:48,290
in very different ways

211
00:11:48,330 --> 00:11:50,380
i want to show you slides

212
00:11:50,430 --> 00:11:51,560
all of this

213
00:11:51,570 --> 00:11:56,250
what we call double slit interference in white light we did it in red light

214
00:11:56,250 --> 00:12:00,010
that's the easiest to very sharply defined next

215
00:12:00,040 --> 00:12:02,620
and the reason why you get so sharply defined

216
00:12:02,800 --> 00:12:05,530
because you don't have the interference of the other class

217
00:12:05,630 --> 00:12:07,900
but of course if you do it with white light

218
00:12:07,920 --> 00:12:09,650
then you will see the light

219
00:12:09,700 --> 00:12:12,850
it has its own spacing in the red light has its own spaces

220
00:12:12,900 --> 00:12:15,570
and the net result of course is that you

221
00:12:15,580 --> 00:12:17,580
c is somewhat different

222
00:12:17,630 --> 00:12:20,490
and that also here

223
00:12:20,510 --> 00:12:24,800
the dark for that upper

224
00:12:25,740 --> 00:12:28,400
is that the double slit interference

225
00:12:29,070 --> 00:12:30,670
blue light

226
00:12:30,720 --> 00:12:32,600
you see clearly dark

227
00:12:32,620 --> 00:12:34,890
o lines which

228
00:12:34,980 --> 00:12:37,370
the destructive interference locations

229
00:12:37,410 --> 00:12:41,680
and you see as i already indicated on the blackboard separation of the red is

230
00:12:41,680 --> 00:12:44,190
larger wavelength and

231
00:12:44,240 --> 00:12:46,250
and if you then do with white light

232
00:12:46,310 --> 00:12:49,320
you see something like this more difficult to

233
00:12:49,400 --> 00:12:51,440
actually see the maximum

234
00:12:51,490 --> 00:12:56,860
and the minimum

235
00:12:56,970 --> 00:12:58,910
now what you can do

236
00:12:58,930 --> 00:13:02,080
with light if you can

237
00:13:02,170 --> 00:13:05,360
two light was light into darkness

238
00:13:05,420 --> 00:13:07,900
then obviously you should also be able

239
00:13:09,720 --> 00:13:12,160
for some time

240
00:13:12,210 --> 00:13:15,720
it's just a matter of scaling up the whole experiment

241
00:13:15,730 --> 00:13:17,670
and that's what we have

242
00:13:17,680 --> 00:13:20,510
so that for you here

243
00:13:20,700 --> 00:13:23,180
we have two loudspeakers

244
00:13:23,200 --> 00:13:26,150
so only equations that you have here

245
00:13:29,310 --> 00:13:31,670
except that we can

246
00:13:31,730 --> 00:13:33,700
so are d is no longer

247
00:13:33,750 --> 00:13:37,480
according the media but are is no one and a half metres

248
00:13:37,490 --> 00:13:39,360
so here is the source of sound

249
00:13:39,400 --> 00:13:41,510
and was the source of

250
00:13:41,590 --> 00:13:42,480
and they

251
00:13:42,500 --> 00:13:45,970
i said by the same electronics

252
00:13:45,970 --> 00:13:47,670
in the appendix

253
00:13:47,690 --> 00:13:50,510
and the properties of trees that i want to read

254
00:13:50,550 --> 00:13:54,650
OK because you actually prove that kind of thing rather than it just being

255
00:13:54,670 --> 00:13:57,510
obvious which is OK

256
00:13:57,530 --> 00:14:00,150
OK so we know that then

257
00:14:00,170 --> 00:14:05,570
two is partitioned

258
00:14:13,690 --> 00:14:18,050
call them t one

259
00:14:18,070 --> 00:14:20,130
thirty two

260
00:14:20,170 --> 00:14:23,960
so here's one seven three

261
00:14:23,960 --> 00:14:28,320
and here's another tree partition

262
00:14:28,320 --> 00:14:31,740
no matter what age i picked there will be two subtrees

263
00:14:31,800 --> 00:14:34,320
this partition into even if the subtree

264
00:14:34,340 --> 00:14:38,880
it is trivial symmetry for example just has a single node and no edges

265
00:14:45,570 --> 00:14:54,490
so theorem proof

266
00:14:56,920 --> 00:14:59,960
a property of optimal substructure

267
00:15:07,820 --> 00:15:09,170
t one

268
00:15:11,570 --> 00:15:13,550
now spanning tree

269
00:15:13,690 --> 00:15:16,130
for the graph

270
00:15:16,130 --> 00:15:17,420
g one

271
00:15:21,840 --> 00:15:28,170
the subgraph

272
00:15:40,610 --> 00:15:44,880
in t one

273
00:15:44,920 --> 00:15:48,470
that is

274
00:15:48,490 --> 00:15:51,490
v one is just the vertices in t one

275
00:15:51,510 --> 00:15:56,240
this is what it means to be induced

276
00:15:56,260 --> 00:16:02,940
OK so the one is the vertices in t one

277
00:16:02,990 --> 00:16:05,090
so in this picture

278
00:16:05,110 --> 00:16:08,190
i believe what is t one

279
00:16:08,220 --> 00:16:09,760
fifty two

280
00:16:09,780 --> 00:16:12,780
in this picture

281
00:16:12,800 --> 00:16:16,220
these are the vertices of t one so that's the one

282
00:16:18,110 --> 00:16:24,440
and the one is the set of

283
00:16:24,440 --> 00:16:28,960
pairs of vertices x and y

284
00:16:28,970 --> 00:16:29,720
that are

285
00:16:29,720 --> 00:16:33,490
the edges there in e one such that

286
00:16:33,510 --> 00:16:35,090
both x and y

287
00:16:35,110 --> 00:16:37,650
long to be one

288
00:16:37,670 --> 00:16:45,170
OK so i'm showing the edges of g here

289
00:16:45,220 --> 00:16:46,400
but basically

290
00:16:46,420 --> 00:16:50,630
given an edge one from here to here that would be in one

291
00:16:50,700 --> 00:16:53,490
if one from here to here it wouldn't

292
00:16:53,490 --> 00:16:57,090
everyone from here to here it would not

293
00:16:57,670 --> 00:17:02,860
so the vertices of the graph subgraph induced by the vertices of t one or

294
00:17:02,860 --> 00:17:04,530
just those

295
00:17:05,300 --> 00:17:09,280
connect up things in v one

296
00:17:09,940 --> 00:17:12,800
and similarly for t two

297
00:17:18,360 --> 00:17:26,090
the theorem says that if i look at

298
00:17:26,130 --> 00:17:27,880
just the edges

299
00:17:27,900 --> 00:17:32,550
within the graph here

300
00:17:32,570 --> 00:17:33,840
g one

301
00:17:33,900 --> 00:17:39,400
those are induced by these vertices t one is in fact the minimum spanning tree

302
00:17:39,440 --> 00:17:42,780
four that subgraph

303
00:17:42,780 --> 00:17:47,050
that's what the theorem says

304
00:17:47,070 --> 00:17:50,550
OK if i look it over here conversly

305
00:17:54,820 --> 00:17:55,880
if i look at this

306
00:17:55,900 --> 00:17:58,340
the set of edges that are induced

307
00:17:58,380 --> 00:17:59,800
by this is

308
00:17:59,820 --> 00:18:02,240
the set of vertices to vertices in t two

309
00:18:02,260 --> 00:18:08,200
in fact he two is the minimum spanning tree on that subgraph

310
00:18:10,360 --> 00:18:12,490
we can even do it over here

311
00:18:12,510 --> 00:18:15,940
if i took a look for example at these

312
00:18:17,670 --> 00:18:21,740
let's see what should we do if we cut out some edge like

313
00:18:21,740 --> 00:18:24,840
five say we cut out five

314
00:18:25,990 --> 00:18:29,380
i cut out age five then t one would be

315
00:18:30,300 --> 00:18:32,800
four vertices here

316
00:18:32,820 --> 00:18:36,530
and the point is that if i look at the subgraph induced on that that's

317
00:18:36,550 --> 00:18:38,570
these pages here

318
00:18:38,590 --> 00:18:42,280
in fact the six eight three are all

319
00:18:42,320 --> 00:18:45,010
edges in the minimum spanning tree

320
00:18:45,030 --> 00:18:48,150
for that subgraph

321
00:18:49,670 --> 00:18:53,030
so that's what the theorem says so let's prove

322
00:19:02,320 --> 00:19:07,630
OK so what technique many is to prove it

323
00:19:07,630 --> 00:19:15,990
OK we learn this technique last time

324
00:19:16,010 --> 00:19:20,440
and hand

325
00:19:20,530 --> 00:19:28,260
something you do in your text editor all the time

326
00:19:28,260 --> 00:19:34,050
and each crosses one person one researcher in the network network has roughly two hundred

327
00:19:34,070 --> 00:19:35,940
fifty researchers

328
00:19:36,010 --> 00:19:42,320
and it's so the idea of this reservation was to visualize

329
00:19:42,320 --> 00:19:43,460
to visualize

330
00:19:43,490 --> 00:19:47,610
all the people within the network in such a way that people which are working

331
00:19:47,610 --> 00:19:51,400
in a similar and that they would be closer on the feature in the area

332
00:19:51,420 --> 00:19:56,150
which are kind of desert so what would be represented as topics

333
00:19:56,170 --> 00:20:02,630
and visualize them in three d then would see this geographical map

334
00:20:02,650 --> 00:20:05,570
if we assign

335
00:20:05,570 --> 00:20:09,210
assigned names to

336
00:20:09,210 --> 00:20:11,780
to the people

337
00:20:17,670 --> 00:20:22,420
and reduces down just the most productive people so this would be

338
00:20:22,490 --> 00:20:24,030
based on the number of

339
00:20:24,050 --> 00:20:29,920
publications in the database so each person here is presented by the abstracts of the

340
00:20:29,920 --> 00:20:37,070
papers which he could contribute it's person contributed to the project samy bengio so he

341
00:20:37,070 --> 00:20:41,150
was very productive so he he was the most active and then if he refuses

342
00:20:42,920 --> 00:20:47,400
some other people would start appearing so appearing so this would be the most

343
00:20:47,420 --> 00:20:48,380
product two

344
00:20:48,400 --> 00:20:51,670
people seeing the metric

345
00:20:52,300 --> 00:20:59,820
i'm certainly not the most productive but not last

346
00:20:59,880 --> 00:21:06,130
OK so this would be the people in asia

347
00:21:06,190 --> 00:21:08,760
the content of the people that

348
00:21:08,800 --> 00:21:09,990
the next

349
00:21:10,420 --> 00:21:15,280
element which we have a picture of so-called

350
00:21:15,300 --> 00:21:16,570
common words

351
00:21:19,440 --> 00:21:20,650
OK we saw the but

352
00:21:20,760 --> 00:21:25,170
seven people appear in certain parts of the map but we are also interested in

353
00:21:25,170 --> 00:21:30,610
what this part of the math actually talks about what the topic of this

354
00:21:30,780 --> 00:21:33,400
it was the topic of the documents which lie

355
00:21:33,400 --> 00:21:37,090
in that particular part of the man

356
00:21:37,670 --> 00:21:38,840
here this

357
00:21:38,860 --> 00:21:41,110
why would with

358
00:21:41,150 --> 00:21:43,550
would be the most basic keywords

359
00:21:43,550 --> 00:21:46,070
from the documents which are

360
00:21:46,090 --> 00:21:48,070
kind of

361
00:21:48,070 --> 00:21:49,940
positioned in that part of the

362
00:21:49,960 --> 00:21:52,050
so here we can see

363
00:21:52,670 --> 00:21:54,900
roughly that

364
00:21:54,920 --> 00:21:55,860
in this part

365
00:21:56,090 --> 00:22:02,490
this would be documents related to document natural language processing web mining text mining can

366
00:22:02,490 --> 00:22:05,320
so on this would be documents from

367
00:22:05,380 --> 00:22:15,300
multimedia mining division and so on and this will be dewey and and and

368
00:22:15,320 --> 00:22:21,710
analysis of that here we have one small additional element which helps us to see

369
00:22:21,710 --> 00:22:23,400
this article as well

370
00:22:30,090 --> 00:22:33,920
the average the position the circle we get the most typical

371
00:22:33,920 --> 00:22:36,090
keywords from that area so

372
00:22:37,090 --> 00:22:41,510
so here we see that the area of their so described with this was like

373
00:22:41,510 --> 00:22:43,800
images object features

374
00:22:43,800 --> 00:22:50,780
segmentation it's also this would be typical keywords for the analysis of multimedia data

375
00:22:51,280 --> 00:22:55,780
computer vision and so on while this would be more like text mining semantic web

376
00:22:55,780 --> 00:22:58,110
and so on and this would be the theory

377
00:22:59,240 --> 00:23:03,900
so this is another element which helps us observing this document collection

378
00:23:05,710 --> 00:23:09,260
if we add the name sense

379
00:23:09,300 --> 00:23:10,150
check this

380
00:23:10,170 --> 00:23:11,940
document in

381
00:23:11,980 --> 00:23:18,880
three d

382
00:23:18,940 --> 00:23:23,460
it is not safe this environment so this is not the whole landscape

383
00:23:23,510 --> 00:23:24,590
three d

384
00:23:24,590 --> 00:23:38,110
threat to fly a little bit into the

385
00:23:38,990 --> 00:23:43,480
you OK see basically this is the same as we saw before

386
00:23:43,490 --> 00:23:45,150
except that now we can

387
00:23:45,170 --> 00:23:49,150
see the whole thing can be closed

388
00:23:51,090 --> 00:23:53,280
five percent

389
00:24:03,610 --> 00:24:11,280
the top is

390
00:24:22,130 --> 00:24:26,220
why should be somewhere here

391
00:24:30,840 --> 00:24:34,260
here are so

392
00:24:34,420 --> 00:24:39,590
and this would be the worst which would become

393
00:24:41,530 --> 00:24:46,460
so we can it was always important whenever showing these maps

394
00:24:46,800 --> 00:24:52,940
these guys who is in the top of the hill because this is been you're

395
00:24:52,960 --> 00:24:54,690
the guy

396
00:24:54,690 --> 00:25:02,820
this is the view from text or the

397
00:25:05,530 --> 00:25:06,780
this would be

398
00:25:10,880 --> 00:25:14,760
OK so this this is kind of

399
00:25:15,150 --> 00:25:18,920
fun at least i mean to to see

400
00:25:18,990 --> 00:25:26,070
this was among media it's also here

401
00:25:27,740 --> 00:25:31,630
so this is kind of

402
00:25:31,630 --> 00:25:34,360
visualisation of

403
00:25:41,070 --> 00:25:45,150
we can also in you of course see

404
00:25:45,210 --> 00:25:48,530
this would be this text and visual arts like this

405
00:25:48,820 --> 00:25:55,360
so this is really solution of static static database why this could be interesting not

406
00:25:55,980 --> 00:25:58,690
by itself is already may be interesting to see

407
00:25:58,710 --> 00:26:04,010
what's the structure of certain text corpus but as a component or some model which

408
00:26:04,010 --> 00:26:05,880
is accompanied by some other functions

409
00:26:07,400 --> 00:26:12,050
analyzing text corpus or a

410
00:26:12,070 --> 00:26:20,610
let's say for later on we will see this ontology learning model then so in

411
00:26:20,610 --> 00:26:26,150
combination with some other functionality this can get very useful just because it shows that

412
00:26:26,510 --> 00:26:28,780
this high level structure of a lot of data

413
00:26:28,980 --> 00:26:31,320
and this one single image

414
00:26:32,920 --> 00:26:36,990
especially if you are not familiar with the content of that particular

415
00:26:37,030 --> 00:26:40,240
cork was and this can be very helpful

416
00:26:40,340 --> 00:26:45,440
now i will show in the same

417
00:26:49,480 --> 00:26:57,050
how the text can be visualized also through time

418
00:27:03,090 --> 00:27:05,340
so this is no visualization of

419
00:27:05,420 --> 00:27:08,400
three hundred thousand news

420
00:27:08,460 --> 00:27:10,070
stories new stories

421
00:27:10,090 --> 00:27:11,550
which corresponds to

422
00:27:12,940 --> 00:27:14,880
new stories from

423
00:27:14,880 --> 00:27:18,530
you know it is tempered by choosing data this is hard to read here but

424
00:27:18,530 --> 00:27:22,620
as you point nine o point eight six and you run this template the EM

425
00:27:22,620 --> 00:27:26,890
algorithm you see kind of curse because like this right you can see that convergence

426
00:27:26,890 --> 00:27:27,970
is slower

427
00:27:27,990 --> 00:27:32,630
so it takes longer until it actually picks up but then you can see that

428
00:27:32,670 --> 00:27:37,180
you you know that no overfitting takes place at least here in the in the

429
00:27:37,180 --> 00:27:42,190
sixty iterations shown and also that what you reaching the end is better than what

430
00:27:42,190 --> 00:27:45,120
you would reach for instance here he would do early stopping or something like that

431
00:27:45,120 --> 00:27:48,940
right you know what you could do here to regularize this is basically just stop

432
00:27:48,940 --> 00:27:50,830
at that point you know it would be

433
00:27:50,850 --> 00:27:53,550
if you could hit exactly that point that would be the best you can do

434
00:27:53,560 --> 00:27:57,660
but you know but with this trick you can actually get models that are a

435
00:27:57,660 --> 00:27:59,900
little bit better than that

436
00:27:59,920 --> 00:28:05,600
OK so this is not a very principled approach i would show more principled approach

437
00:28:05,600 --> 00:28:08,820
if you know how how to do that

438
00:28:08,820 --> 00:28:13,830
in the next couple of slides OK but so so this

439
00:28:13,840 --> 00:28:17,830
this much about this probabilistic latent semantic analysis models

440
00:28:17,850 --> 00:28:20,250
another i'd like to

441
00:28:20,270 --> 00:28:22,520
talk about an extension

442
00:28:22,530 --> 00:28:27,050
that has been called latent dirichlet allocation LDA

443
00:28:27,730 --> 00:28:31,900
was proposed by david blyth and rating and michael jordan

444
00:28:31,940 --> 00:28:39,660
and and that's basically an improvement of killers a or generalisation in certain bayesian sense

445
00:28:39,660 --> 00:28:41,810
and in LDA

446
00:28:42,110 --> 00:28:48,140
it works as follows it's actually it's a generative model for documents

447
00:28:48,210 --> 00:28:50,650
OK so what we do is we generate

448
00:28:50,650 --> 00:28:53,480
documents we ignore the sequence of the words

449
00:28:53,500 --> 00:28:55,620
so we don't really

450
00:28:55,630 --> 00:28:59,960
but so what we generate basically is is bag of words not documents in terms

451
00:28:59,960 --> 00:29:03,830
of sentence structure and things like that so what we do is this is the

452
00:29:03,830 --> 00:29:05,960
following we first sample

453
00:29:06,060 --> 00:29:08,960
the length of the documents which is called the capital and

454
00:29:09,000 --> 00:29:11,300
according to that's a simpler song

455
00:29:11,390 --> 00:29:13,990
some rate parameter excited

456
00:29:14,030 --> 00:29:16,230
and then we choose

457
00:29:16,960 --> 00:29:19,890
four that document a particular

458
00:29:19,890 --> 00:29:21,650
the mixing proportions

459
00:29:22,660 --> 00:29:25,180
according to some dirichlet distributions

460
00:29:25,390 --> 00:29:28,800
and we have hyperparameters of here

461
00:29:28,800 --> 00:29:33,280
and then once we have these mixing proportions we now choose for each of the

462
00:29:33,280 --> 00:29:36,890
words that we want to generate OK we decided we want to generate and of

463
00:29:36,890 --> 00:29:39,360
the market so the loop over n

464
00:29:39,380 --> 00:29:46,080
we independently choose a topic according to a multinomial distribution with these pies are now

465
00:29:46,130 --> 00:29:48,690
the probabilities of you you know picking

466
00:29:49,270 --> 00:29:54,260
you know what the spinal specifies is the probability of a particular concept

467
00:29:54,320 --> 00:29:58,060
given our document right and what we do here is now is we sample a

468
00:29:58,060 --> 00:30:00,700
concept actually from that distribution

469
00:30:00,730 --> 00:30:06,990
and then given that we've sampled the topic we know sample the particular word

470
00:30:08,520 --> 00:30:10,990
from that is from a distribution

471
00:30:11,580 --> 00:30:15,940
that corresponds to what we've seen before the probability of generating a particular word given

472
00:30:15,940 --> 00:30:17,870
a particular concept

473
00:30:17,920 --> 00:30:19,820
right so

474
00:30:19,850 --> 00:30:20,990
so this is

475
00:30:21,050 --> 00:30:22,180
the way

476
00:30:22,250 --> 00:30:25,870
and the a works notes that the main difference is

477
00:30:25,920 --> 00:30:27,610
that we think of

478
00:30:27,610 --> 00:30:30,390
these mixing proportions here

479
00:30:30,440 --> 00:30:33,800
as another random variable

480
00:30:33,800 --> 00:30:39,420
that's how the distribution immediately distribution whereas in the pillars a model we thought of

481
00:30:39,420 --> 00:30:43,570
it actually is a set of parameters e for each document we would basically have

482
00:30:43,570 --> 00:30:48,240
a set of parameters that would be these mixing proportions we just estimated and here

483
00:30:48,240 --> 00:30:52,250
we just make it part of the model that also allows us to you know

484
00:30:52,250 --> 00:30:56,000
this is a random variable we can basically also integrate out these variables OK we

485
00:30:56,000 --> 00:30:59,700
have additional latent variables we can integrate them yes

486
00:31:03,270 --> 00:31:05,930
we have already

487
00:31:11,420 --> 00:31:13,920
so we will

488
00:31:13,930 --> 00:31:15,760
you know we get the following

489
00:31:15,800 --> 00:31:17,250
model we

490
00:31:17,260 --> 00:31:23,120
i have a probability over these these high vector these mixing proportions which is unknown

491
00:31:23,150 --> 00:31:27,470
the latent concepts now thought of as a vector of n concept variables for a

492
00:31:27,470 --> 00:31:30,440
particular document and then actually observed

493
00:31:30,490 --> 00:31:35,190
word frequencies of the observed word for this document

494
00:31:35,210 --> 00:31:38,550
and we can then write down to the following model year

495
00:31:38,550 --> 00:31:43,510
so the probability for pi just depends on the hyperparameters that additional distribution right can

496
00:31:43,510 --> 00:31:44,870
be written like this

497
00:31:44,890 --> 00:31:47,670
a bunch of gamma functions over there and then just

498
00:31:47,690 --> 00:31:55,370
you know these probabilities raised to the power of five minus one of hyperparameters

499
00:31:55,370 --> 00:32:02,220
and even here we just generated independently the the words that belong to this document

500
00:32:02,260 --> 00:32:03,980
so we sample

501
00:32:04,000 --> 00:32:07,000
the concept according to that distribution

502
00:32:07,020 --> 00:32:10,930
and then we sample the word for each particular concert according to that

503
00:32:10,950 --> 00:32:13,000
and what we can do now is

504
00:32:13,480 --> 00:32:15,870
for instance

505
00:32:15,930 --> 00:32:22,400
we can define now distribution over documents model by these words vectors right remember document

506
00:32:22,400 --> 00:32:26,980
is now we think of as being a vector of word counts

507
00:32:26,990 --> 00:32:29,660
and we can do that by taking this

508
00:32:29,670 --> 00:32:32,660
and by integrating out the

509
00:32:34,120 --> 00:32:37,160
latent variables these would be the highest

510
00:32:37,180 --> 00:32:39,160
and then by summing out

511
00:32:41,070 --> 00:32:44,630
the discrete latent variables these would be the concept

512
00:32:44,630 --> 00:32:48,680
you can do to the independence of these we can actually pull is in here

513
00:32:48,750 --> 00:32:50,610
and and some over here

514
00:32:50,630 --> 00:32:52,380
for each individual

515
00:32:52,380 --> 00:32:57,430
so we get these empirical distributions for player one and player two

516
00:32:57,450 --> 00:33:00,960
what i regret things says is that look

517
00:33:00,970 --> 00:33:03,080
how well i've done

518
00:33:03,160 --> 00:33:06,070
in the sequence is at least as well

519
00:33:06,090 --> 00:33:10,430
i don't list as well this is correct as the best they could have done

520
00:33:10,740 --> 00:33:12,660
from playing any single action

521
00:33:12,680 --> 00:33:17,780
in hindsight almost at least as well minus some small termites decreasing with the length

522
00:33:17,780 --> 00:33:21,040
of the sequence in fact and doing almost as well as i could have done

523
00:33:21,040 --> 00:33:23,040
within a mixed strategy

524
00:33:23,060 --> 00:33:27,020
because if you compare itself to some opponents playing some fixed strategy

525
00:33:27,070 --> 00:33:31,120
and you're look regret from playing any single strategy in the mixed strategy

526
00:33:31,130 --> 00:33:32,380
it's the same

527
00:33:33,060 --> 00:33:34,590
because you know

528
00:33:34,600 --> 00:33:37,250
if i if i know what is going to do at least distribution i got

529
00:33:37,250 --> 00:33:40,720
some best response and then i got a bunch of

530
00:33:41,210 --> 00:33:43,770
or maybe multiple best responses but

531
00:33:43,780 --> 00:33:46,390
i might as well just choose the best response to single and i don't have

532
00:33:46,400 --> 00:33:47,890
to randomize

533
00:33:47,910 --> 00:33:48,810
but anyway

534
00:33:48,830 --> 00:33:50,680
what we're doing at least as well

535
00:33:50,690 --> 00:33:53,240
as the best

536
00:33:53,710 --> 00:33:55,350
mixed strategy response

537
00:33:56,690 --> 00:33:58,210
in particular

538
00:33:58,230 --> 00:34:03,040
what this means is that while the opponent is probably

539
00:34:03,050 --> 00:34:08,140
this term here the point is probably best off choosing these babies according to

540
00:34:08,150 --> 00:34:10,090
the right rule at least

541
00:34:10,100 --> 00:34:13,600
to to screw over senses zero something he's going to choose them to minimize the

542
00:34:13,600 --> 00:34:16,130
payoff to do that he knows that

543
00:34:16,320 --> 00:34:21,460
released for this inequality to estimate minimize this inequality

544
00:34:21,500 --> 00:34:26,640
the worst case would be where beta is the distribution mu that minimizes

545
00:34:26,650 --> 00:34:31,940
player one's payoff if player two we're just trying to speak

546
00:34:31,950 --> 00:34:35,370
we know we get at least as much

547
00:34:35,420 --> 00:34:38,470
for the data which is bigger than if

548
00:34:38,520 --> 00:34:43,160
we do just shows the distribution it's going to minimize that

549
00:34:45,080 --> 00:34:49,600
and similarly for the second player you get the same thing except passing negative the

550
00:34:49,630 --> 00:34:52,000
opposite of each of these so

551
00:34:52,010 --> 00:34:56,920
when you negate this and do it in the opposite order so this new you

552
00:34:56,920 --> 00:34:57,880
get the same

553
00:34:57,890 --> 00:34:59,250
same thing here

554
00:34:59,270 --> 00:35:04,070
but in the opposite order and you get that

555
00:35:04,090 --> 00:35:08,120
the payoff to player two is most this

556
00:35:08,140 --> 00:35:12,180
and this is exactly the opposite these two terms are equal here

557
00:35:12,190 --> 00:35:15,360
so this is exactly the opposite of

558
00:35:15,410 --> 00:35:17,890
what we had before

559
00:35:19,580 --> 00:35:20,430
right so

560
00:35:20,540 --> 00:35:24,670
we have the maximum we knew from before the going first is disadvantage and we

561
00:35:24,680 --> 00:35:28,830
see these two things are equal because this holds for any enemy i mean

562
00:35:28,840 --> 00:35:34,130
so we could imagine conceptually playing this game for a very long time and then

563
00:35:34,150 --> 00:35:40,740
show that these two things are arbitrarily close together and therefore there must be equal

564
00:35:40,790 --> 00:35:44,140
so not only is this a proof of minimax so it's it's kind of strategy

565
00:35:44,600 --> 00:35:46,650
for playing repeated games

566
00:35:46,660 --> 00:35:50,110
using this weighted majority and is pretty reasonable

567
00:35:50,130 --> 00:35:55,300
i think OK so basically the weighted majority gives very simple proof of the minimax

568
00:35:56,480 --> 00:36:00,340
in which essentially says this for every game zero sum game is the value of

569
00:36:00,340 --> 00:36:05,120
the game and you can guarantee yourself in this value by playing the weighted majority

570
00:36:05,160 --> 00:36:07,390
in fact is in the weighted majority

571
00:36:07,400 --> 00:36:12,590
you get regret going to zero regardless of upon so if your opponent

572
00:36:12,630 --> 00:36:14,480
is playing really stupidly

573
00:36:14,500 --> 00:36:17,260
you might even be able to really tapped capitalize

574
00:36:17,310 --> 00:36:20,800
on you might be able to be needed for playing tic-tac-toe this algorithm if find

575
00:36:20,800 --> 00:36:22,660
that strategy

576
00:36:22,670 --> 00:36:26,260
so i think it's a very

577
00:36:26,280 --> 00:36:30,510
clean theorem for the zero-sum game game case

578
00:36:30,530 --> 00:36:32,110
what's going on in the

579
00:36:32,120 --> 00:36:35,290
general sum games where things don't add up to zero

580
00:36:37,950 --> 00:36:41,150
there's a lot more murky there is still a lot of research going into what

581
00:36:41,150 --> 00:36:45,700
what should happen what does happen what should go on the learning in this case

582
00:36:45,710 --> 00:36:48,800
so in general game like take a look at this game

583
00:36:48,850 --> 00:36:50,130
coordination games

584
00:36:50,160 --> 00:36:54,820
we we really want choose a or b like let's say

585
00:36:54,840 --> 00:36:59,040
let's say my friend and i like to both play pool

586
00:36:59,060 --> 00:37:02,350
or you know good baseball game we prefer to go to a baseball game or

587
00:37:02,360 --> 00:37:03,160
play pool

588
00:37:03,160 --> 00:37:05,040
reaching choose but if one of us

589
00:37:05,920 --> 00:37:08,270
to play pool and the other one is the baseball game

590
00:37:08,300 --> 00:37:09,710
we're not very happy to one

591
00:37:09,710 --> 00:37:12,500
coordinate and she's the same thing

592
00:37:12,530 --> 00:37:15,010
and for this kind of game there actually three

593
00:37:15,030 --> 00:37:18,430
for this particular game actually three equilibria

594
00:37:18,440 --> 00:37:19,860
one of them would be

595
00:37:20,820 --> 00:37:23,110
choose a relevant both to be

596
00:37:23,110 --> 00:37:27,800
and the third being mixed strategy i think probably one-third two-thirds

597
00:37:27,860 --> 00:37:32,150
where we choose where we mix between a and b k

598
00:37:32,160 --> 00:37:36,620
in general these games can have very many equilibria

599
00:37:36,630 --> 00:37:37,960
it's kind of interesting

600
00:37:37,970 --> 00:37:41,430
as long as not to generate i think they number of equilibria

601
00:37:43,180 --> 00:37:45,400
there's a lot of it's very hard factors

602
00:37:45,420 --> 00:37:47,610
nobody knows how to find an equilibrium

603
00:37:47,630 --> 00:37:48,880
one of these games

604
00:37:48,900 --> 00:37:52,790
officially nobody knows how to find the nash equilibrium of

605
00:37:52,820 --> 00:37:54,810
a two player game

606
00:37:54,830 --> 00:37:58,820
represented in matrix in terms of efficiency when they think about it is polynomial time

607
00:37:58,820 --> 00:38:01,190
algorithms that big open problem

608
00:38:01,210 --> 00:38:06,030
in the theoretical theory literature one of the criticisms actually the big criticisms of nash

609
00:38:06,030 --> 00:38:07,550
equilibrium is

610
00:38:07,600 --> 00:38:12,660
if it's such a natural idea and you really think that explains how people play

611
00:38:12,680 --> 00:38:14,260
you can find one

612
00:38:14,280 --> 00:38:17,240
in the game then how natural is that

613
00:38:17,240 --> 00:38:21,280
in proc

614
00:42:35,410 --> 00:42:38,890
you know

615
00:44:06,990 --> 00:44:09,320
all parties

616
00:45:01,210 --> 00:45:09,590
long beach

617
00:45:09,590 --> 00:45:14,420
gang member of the system of many particles could be connected with springs

618
00:45:14,520 --> 00:45:19,130
could be chemical explosions going on the could flow into each other they break each

619
00:45:19,130 --> 00:45:20,220
other up

620
00:45:20,240 --> 00:45:22,390
and you know momentum will not change

621
00:45:22,390 --> 00:45:27,100
if there is no net external to work on that system because all the internal

622
00:45:27,100 --> 00:45:29,820
torques taught cancel out because actions

623
00:45:29,830 --> 00:45:34,510
equals minus reaction

624
00:45:34,530 --> 00:45:35,490
so if we know

625
00:45:35,500 --> 00:45:39,600
compare conservation of angular momentum with conservation of momentum

626
00:45:39,600 --> 00:45:40,660
and in the case

627
00:45:40,670 --> 00:45:45,220
of the conservation of momentum remember when we had a system of objects

628
00:45:45,230 --> 00:45:48,270
in the absence of external force

629
00:45:48,280 --> 00:45:49,940
on the system as a whole

630
00:45:49,950 --> 00:45:51,660
net external force

631
00:45:51,660 --> 00:45:53,330
momentum was conserved

632
00:45:53,380 --> 00:45:54,340
now we have

633
00:45:54,350 --> 00:45:57,690
a system of particles in the absence of the net

634
00:45:57,710 --> 00:45:59,710
external torque

635
00:45:59,760 --> 00:46:04,780
angular momentum is conserved

636
00:46:04,790 --> 00:46:05,710
in the case

637
00:46:05,720 --> 00:46:06,710
of the

638
00:46:06,810 --> 00:46:08,700
i scale is the light

639
00:46:08,750 --> 00:46:11,130
when you pull your arms in

640
00:46:11,170 --> 00:46:13,210
the moment of inertia goes down

641
00:46:13,260 --> 00:46:16,170
and so you frequency goes up

642
00:46:16,180 --> 00:46:18,560
when the star strings

643
00:46:18,690 --> 00:46:20,790
radius goes down

644
00:46:20,810 --> 00:46:22,320
its moment of inertia

645
00:46:22,340 --> 00:46:23,270
goes down

646
00:46:23,290 --> 00:46:24,530
and therefore

647
00:46:24,580 --> 00:46:26,210
it's angular velocity

648
00:46:26,240 --> 00:46:27,740
must go up

649
00:46:27,750 --> 00:46:29,010
moment of inertia

650
00:46:29,030 --> 00:46:32,100
because we are squid

651
00:46:32,170 --> 00:46:36,960
what determines the size of a star

652
00:46:37,010 --> 00:46:39,310
this is a the

653
00:46:39,310 --> 00:46:42,870
inside the star is inference nuclear furnace

654
00:46:43,950 --> 00:46:45,750
fusion is going on

655
00:46:45,830 --> 00:46:47,390
that produces

656
00:46:48,190 --> 00:46:49,840
and pressure

657
00:46:49,880 --> 00:46:52,650
which wanted to expand the start

658
00:46:52,700 --> 00:46:55,360
on the other hand there is gravity

659
00:46:55,390 --> 00:46:56,670
which is sorry

660
00:46:56,730 --> 00:46:58,140
you can do that

661
00:46:58,140 --> 00:46:59,380
i want to

662
00:46:59,430 --> 00:47:00,750
all together

663
00:47:00,810 --> 00:47:03,900
in fact gravity would like to collapse

664
00:47:03,930 --> 00:47:06,670
nature finds a balance between

665
00:47:06,730 --> 00:47:09,380
the gravity and this pressure due to

666
00:47:09,460 --> 00:47:11,540
and the nuclear furnace

667
00:47:11,610 --> 00:47:13,170
now comes the time

668
00:47:13,180 --> 00:47:16,560
that the nuclear furnace has been completely consume

669
00:47:16,640 --> 00:47:20,170
for that takes an additional five billion years

670
00:47:20,180 --> 00:47:24,160
the sun has already been burning nuclear fuel for five billion years

671
00:47:24,170 --> 00:47:25,680
there's no five billion

672
00:47:27,150 --> 00:47:30,360
and once the nuclear fuel has been consumed

673
00:47:30,510 --> 00:47:33,380
there are three and products

674
00:47:33,420 --> 00:47:34,410
of the

675
00:47:34,520 --> 00:47:38,130
dead star that's left over

676
00:47:38,130 --> 00:47:40,810
and these three and products

677
00:47:40,820 --> 00:47:41,880
the following

678
00:47:41,900 --> 00:47:44,110
number one

679
00:47:44,160 --> 00:47:48,000
it's called a white dwarf

680
00:47:48,020 --> 00:47:51,380
it has a radius approximately the same as the rest

681
00:47:51,420 --> 00:47:53,640
ten thousand kilometres

682
00:47:53,690 --> 00:47:56,930
and the mass of the white dwarf there's a whole range of them

683
00:47:56,960 --> 00:48:01,090
but the typical number say is half the mass of the sun

684
00:48:01,130 --> 00:48:03,520
that's one possible and problem

685
00:48:03,640 --> 00:48:06,020
this will be the fate of our sun by the way

686
00:48:06,080 --> 00:48:09,710
the density of such an object is quite high ten two do

687
00:48:10,740 --> 00:48:12,880
be roughly ten two to six

688
00:48:13,810 --> 00:48:16,120
cubic centimeter

689
00:48:16,170 --> 00:48:18,150
another possibility is

690
00:48:18,190 --> 00:48:22,240
that you end up with a neutron star

691
00:48:22,310 --> 00:48:26,620
neutron star has a radius of about ten kilometres

692
00:48:28,520 --> 00:48:30,360
it has a mass

693
00:48:30,400 --> 00:48:31,620
of roughly

694
00:48:31,670 --> 00:48:34,610
one point five times the mass of the sun

695
00:48:34,630 --> 00:48:35,970
and its density

696
00:48:36,000 --> 00:48:39,090
it's about ten to fourteen grams per cubic centimeter

697
00:48:39,090 --> 00:48:40,420
which is even higher

698
00:48:41,130 --> 00:48:43,830
the density of nuclei

699
00:48:43,840 --> 00:48:45,960
and then there is the possibility

700
00:48:46,020 --> 00:48:47,880
which is even more bizarre

701
00:48:47,960 --> 00:48:49,810
thank you and up

702
00:48:49,810 --> 00:48:51,430
with the black hole

703
00:48:51,480 --> 00:48:54,070
i will not talk about black holes today but i will get back to that

704
00:48:54,870 --> 00:48:56,370
in in one

705
00:48:56,380 --> 00:48:57,990
and the black hole

706
00:48:58,010 --> 00:48:59,710
for all practical purposes

707
00:48:59,720 --> 00:49:01,520
has no size at all

708
00:49:01,570 --> 00:49:02,910
the mass of the black

709
00:49:03,660 --> 00:49:05,350
must be larger than

710
00:49:05,360 --> 00:49:07,250
we think three solar masses

711
00:49:07,270 --> 00:49:08,750
and so the density

712
00:49:08,880 --> 00:49:13,560
it is infinitely high

713
00:49:13,610 --> 00:49:17,280
whether you end up to be white dwarf neutron star or black hole depends on

714
00:49:18,120 --> 00:49:19,720
mass of the of the

715
00:49:19,740 --> 00:49:22,170
the progenitor star that collapses

716
00:49:22,170 --> 00:49:25,260
when the fuel in nuclear fools garden

717
00:49:25,270 --> 00:49:28,560
and in order to form a neutron star you would have to start off

718
00:49:28,600 --> 00:49:32,340
with the star of probably at least ten solar masses maybe even more

719
00:49:32,430 --> 00:49:33,670
so our sun will not

720
00:49:33,700 --> 00:49:37,430
become a neutron star but our sun will ultimately become

721
00:49:37,460 --> 00:49:39,370
a white dwarf

722
00:49:39,490 --> 00:49:42,900
i would be reasonable question to ask why do you end up only with these

723
00:49:42,900 --> 00:49:45,970
three possibilities why is there nothing in between

724
00:49:45,980 --> 00:49:50,430
note that is a huge difference from ten thousand kilometres ten kilometres is there nothing

725
00:49:50,430 --> 00:49:51,660
in between

726
00:49:51,700 --> 00:49:55,150
and the answer to that lies in quantum mechanics which is not part of this

727
00:49:55,150 --> 00:49:57,440
course but you'll see that in eight o five

728
00:49:57,450 --> 00:49:59,260
why only these two

729
00:49:59,350 --> 00:50:03,380
and then if you get into general relativity then you understand why

730
00:50:03,390 --> 00:50:05,020
there is then this third

731
00:50:05,060 --> 00:50:08,010
very bizarre possibilities

732
00:50:08,030 --> 00:50:10,580
when the star collapses

733
00:50:10,620 --> 00:50:12,180
two things happen

734
00:50:12,200 --> 00:50:13,220
first of all

735
00:50:13,270 --> 00:50:16,610
there is a huge amount of gravitational potential energy

736
00:50:16,640 --> 00:50:21,010
that is released in the form of kinetic energy

737
00:50:21,060 --> 00:50:25,740
the stuff full then we call gravitational collapse and the gravitational potential energy converts to

738
00:50:25,740 --> 00:50:28,070
kinetic energy and that ultimately

739
00:50:28,090 --> 00:50:29,290
converts to heat

740
00:50:29,310 --> 00:50:30,740
and to radiation

741
00:50:30,760 --> 00:50:32,530
if i take an object here

742
00:50:32,650 --> 00:50:33,760
these of chuck

743
00:50:33,850 --> 00:50:36,170
and i dropped that

744
00:50:36,260 --> 00:50:38,810
that you can call gravitational collapse

745
00:50:38,850 --> 00:50:42,000
gravitational potential energy is converted to kinetic energy

746
00:50:42,060 --> 00:50:43,310
and ultimately

747
00:50:43,320 --> 00:50:44,170
it goes

748
00:50:44,200 --> 00:50:45,640
two heat

749
00:50:45,650 --> 00:50:47,670
we're talking about star

750
00:50:47,690 --> 00:50:48,430
which is

751
00:50:50,610 --> 00:50:52,780
and the amount of

752
00:50:52,830 --> 00:50:55,590
gravitational potential energy that become available

753
00:50:55,600 --> 00:50:57,480
are enormous

754
00:50:57,590 --> 00:50:58,620
in vision

755
00:50:58,670 --> 00:51:01,040
so this huge amount of energy released

756
00:51:01,040 --> 00:51:06,620
the trick will be that the empirical risks the training error only refers to M

757
00:51:07,870 --> 00:51:14,830
and all functions take the values output values plus minus one so our functions on these

758
00:51:14,830 --> 00:51:21,330
M points can take at most two to the part of M values so on M points

759
00:51:21,330 --> 00:51:29,390
the function class has effectively maximum size of two to the power of M

760
00:51:29,430 --> 00:51:34,220
oh there's a question

761
00:51:34,370 --> 00:51:47,700
so this these are different sets they could they could overlap they don't have

762
00:51:47,700 --> 00:51:54,000
to be disjoint I guess that's what you to mean it's likely before

763
00:51:54,580 --> 00:51:57,350
so if they

764
00:51:57,500 --> 00:51:58,930
if they are not disjoint

765
00:51:58,950 --> 00:52:03,660
then this quantity here is nonzero which means when we are doing this upper bound we are losing

766
00:52:03,660 --> 00:52:09,490
something if they were disjoint then we would be losing less than our

767
00:52:09,490 --> 00:52:13,870
bound and then would be sharper so we might loose a lot if

768
00:52:13,870 --> 00:52:16,790
these events overlap

769
00:52:25,620 --> 00:52:31,640
so I mean through this jointness of events is something that's similar to what you mean by

770
00:52:31,640 --> 00:52:37,310
independence right independence is a pro property of a probability measures if we just look at the

771
00:52:37,310 --> 00:52:45,000
events we would normally be talking about whether the are disjoint or mutually exclusive so

772
00:52:45,000 --> 00:52:53,100
okay so we use a trick to reduce the infinite case to the finite case and

773
00:52:53,100 --> 00:52:59,810
this is the so-called symmetrization lemma Vapnik and Chervonenkis and

774
00:52:59,830 --> 00:53:04,700
this I'm not gonna prove this but I think you will believe me when you hear it this lemma says

775
00:53:04,700 --> 00:53:07,060
that the probability

776
00:53:07,060 --> 00:53:10,370
that training error and test error

777
00:53:10,580 --> 00:53:19,310
differ by model upsilon can be upper bounded by the twice the probability that two

778
00:53:19,310 --> 00:53:21,350
different training errors

779
00:53:21,410 --> 00:53:24,540
differ from each ofther by model upsilon over two

780
00:53:25,160 --> 00:53:28,220
so if if these two

781
00:53:28,220 --> 00:53:34,660
things if by drawing two sets of training points if you you if by doing

782
00:53:34,660 --> 00:53:38,080
this you cannot get

783
00:53:38,100 --> 00:53:43,140
training errors that are very different from each other then we can also guarantee that

784
00:53:43,140 --> 00:53:51,330
the training error and the test error cannot be very different it's again some kind of triangle inequality type argument

785
00:53:51,330 --> 00:53:55,890
so which means now we need a training set of twice the size

786
00:53:55,910 --> 00:54:00,020
and maybe maybe we you ignore this down here now

787
00:54:00,120 --> 00:54:02,620
this is just for details if you want to look at it afterwards

788
00:54:02,620 --> 00:54:06,600
again but basically means rather than

789
00:54:06,640 --> 00:54:13,000
having this quantity here which is the this integral over the whole probability distribution that we don't know

790
00:54:13,000 --> 00:54:20,000
if we are only interested in upper bounding this quantity we can first upper bound it by

791
00:54:20,000 --> 00:54:24,550
this thing here and the nice thing about this is that this is a term that only involves

792
00:54:24,550 --> 00:54:28,370
M points this is a term that only involves N points so if we can

793
00:54:28,370 --> 00:54:33,770
somehow upper bound this guys by using some tricks that rely on the function class being

794
00:54:33,770 --> 00:54:38,470
effectively finite we can do it over here because this these quantities referred to

795
00:54:38,470 --> 00:54:44,930
finitely many points and as I mentioned before in that case the function classes are effectively finite

796
00:54:44,980 --> 00:54:50,750
so what we are therefore interested in is the maximum size of our function class

797
00:54:50,750 --> 00:54:57,720
on two M points and this is sometimes called the shattering coefficient we denote it by

798
00:54:57,720 --> 00:55:04,520
this calligraphic N so this can be also generalized to the case of real valued outputs

799
00:55:04,520 --> 00:55:11,080
functions and people look at things like covering them was in that case but we only look at classification here and

800
00:55:11,220 --> 00:55:17,040
this shattering coefficient and let me remind you what it means so the size of a

801
00:55:17,040 --> 00:55:23,160
function class plus minus one valued function class on two M points is

802
00:55:23,160 --> 00:55:26,350
the maximum number of different outputs

803
00:55:26,480 --> 00:55:31,020
that the function class can generate on two M points

804
00:55:31,100 --> 00:55:35,500
we will consider two functions that are that give the same outputs on

805
00:55:35,500 --> 00:55:39,520
our two M point we will consider them equivalent there's no way for us to

806
00:55:39,520 --> 00:55:41,020
distinguish them and therefore

807
00:55:41,560 --> 00:55:47,100
we are only interested in how many such functions they are and

808
00:55:47,160 --> 00:55:51,220
if you think of the output vectors so the output vectors are of plus minus

809
00:55:51,220 --> 00:55:53,520
one plus one whenever we have

810
00:55:54,080 --> 00:55:59,580
correctly separated the points they are minus one if we have done it the

811
00:55:59,580 --> 00:56:05,080
other way round so if we have assigned a point to the other class sorry

812
00:56:05,080 --> 00:56:07,080
the plus one or

813
00:56:07,100 --> 00:56:10,870
no actually this that's true they are plus one or minus one so I was not sorry I

814
00:56:10,870 --> 00:56:14,200
have to take back what I just said so I could also do this in terms

815
00:56:14,200 --> 00:56:18,390
of the loss functions but it's not done like this here so these are just plus one minus one

816
00:56:18,390 --> 00:56:22,930
how many output vectors can we generate them to M points in other words

817
00:56:22,950 --> 00:56:27,790
how many how many different ways can we ways can we classify the points

818
00:56:27,850 --> 00:56:29,720
in two classes

819
00:56:29,940 --> 00:56:34,290
and of course each point could go either in the one class or the other

820
00:56:34,290 --> 00:56:40,680
one and there's an upper bound at this quantity which is two to the part of two M

821
00:56:40,680 --> 00:56:43,720
and if this upper bound is actually pertained

822
00:56:44,230 --> 00:56:49,060
then the function class is said to shatter the two M points it means that we can realize

823
00:56:49,060 --> 00:56:55,560
all possible class assignments using functions from our class so for instance the function class has

824
00:56:55,560 --> 00:57:02,680
maximum richness or maximum capacity on sets of points of size two M

825
00:57:02,720 --> 00:57:05,750
so now we put everything together

826
00:57:05,770 --> 00:57:11,040
to get our bound so we first use the symmetrization

827
00:57:11,040 --> 00:57:16,850
so this symmetrization lemma to get rid of this test error this quantity that depends

828
00:57:16,850 --> 00:57:23,700
on the unknown probability measure that's integral over this probability measure to get two quantities that are both

829
00:57:23,810 --> 00:57:26,230
only depending on M points

830
00:57:28,640 --> 00:57:29,620
we then

831
00:57:29,660 --> 00:57:35,370
say and I'll be cheating a tiny little bit here and if you want to know details I

832
00:57:35,370 --> 00:57:39,180
can either give you something to read or I can tell you afterwards but I don't want

833
00:57:39,180 --> 00:57:42,950
to go into great detail here

834
00:57:43,000 --> 00:57:48,480
then I'm basically saying well on my my function class on these two M points

835
00:57:48,500 --> 00:57:55,600
has a size calligraphic N this shattering coefficient so that means I have a calligraph

836
00:57:55,600 --> 00:57:57,230
I can

837
00:57:57,270 --> 00:58:03,230
effectively the size of function class is this thing here so I pick functions F one through F

838
00:58:03,250 --> 00:58:10,640
sub N and rewrite this as a union of all these events then I apply the

839
00:58:10,660 --> 00:58:15,060
union bounds so each of these things

840
00:58:15,060 --> 00:58:19,230
stick breaking construction it counts splits those two apart so in the stick breaking construction

841
00:58:19,360 --> 00:58:23,910
count very easy to see right so the values that is assigned to the clusters

842
00:58:23,910 --> 00:58:27,800
is going to be the state that is going to be independent from the price

843
00:58:27,800 --> 00:58:28,560
and the

844
00:58:28,580 --> 00:58:33,210
and so we can treat those two parts separately and then we can now infer

845
00:58:33,210 --> 00:58:35,230
is rhythms turns out to be more efficient

846
00:58:35,260 --> 00:58:39,470
yes if you change one of the state state status is going to change the

847
00:58:40,420 --> 00:58:45,030
of all the state assigned to that trust

848
00:58:47,630 --> 00:58:50,610
should move on so

849
00:58:50,620 --> 00:58:56,940
another interesting property of the dirichlet process is that it turns out that posteriors of

850
00:58:56,940 --> 00:58:58,880
dirichlet processes also

851
00:58:58,900 --> 00:59:03,480
dirichlet processes under the the following scheme so if g is drawn from a dirichlet

852
00:59:03,480 --> 00:59:06,530
process and theta one two theta and

853
00:59:06,540 --> 00:59:08,610
drawn i i d from g

854
00:59:08,670 --> 00:59:10,310
then the posterior g

855
00:59:10,310 --> 00:59:12,630
given theta one theta and

856
00:59:12,650 --> 00:59:14,650
conditioning on one

857
00:59:14,660 --> 00:59:15,890
the is

858
00:59:15,900 --> 00:59:21,310
going to be a dirichlet process as well with the following update the concentration parameter

859
00:59:21,310 --> 00:59:22,900
and base distribution

860
00:59:23,690 --> 00:59:28,120
not that this base distribution is in fact exactly the same as

861
00:59:29,640 --> 00:59:34,540
and this is

862
00:59:36,660 --> 00:59:38,010
except that

863
00:59:38,040 --> 00:59:44,020
instead and minus one half plus one as and in here is basically the original

864
00:59:44,020 --> 00:59:47,080
base distribution mixed with

865
00:59:47,090 --> 00:59:52,500
a bunch of delta functions one in each of the values of theta one theta

866
00:59:52,510 --> 00:59:58,660
OK so referring to

867
00:59:58,670 --> 01:00:01,000
one of the question that was asked

868
01:00:04,230 --> 01:00:10,320
if we look at the polya urn scheme right so we have this conditional distributions

869
01:00:10,320 --> 01:00:14,010
that we can now write down so this is in fact a density

870
01:00:14,030 --> 01:00:17,900
so is actually the joint density of theta one two theta

871
01:00:18,020 --> 01:00:19,620
and we can

872
01:00:19,630 --> 01:00:23,810
and we can write this down as basically the

873
01:00:24,230 --> 01:00:28,730
the probability of theta one times probability of theta two given theta one probability of

874
01:00:28,750 --> 01:00:31,020
the tree given theta one and two

875
01:00:31,060 --> 01:00:32,250
and we can

876
01:00:32,260 --> 01:00:35,370
and the probabilities are basically given by this

877
01:00:35,420 --> 01:00:39,350
because this conditional distributions and if you

878
01:00:39,360 --> 01:00:43,110
right down the probability then it turns out that it caused to form which looks

879
01:00:43,110 --> 01:00:44,360
like the following

880
01:00:44,370 --> 01:00:49,980
one of the things the world

881
01:00:55,400 --> 01:00:59,290
he is actually not as the

882
01:00:59,290 --> 01:01:03,170
o thing

883
01:01:03,200 --> 01:01:08,880
OK i look and this very naive something which is right now either the probability

884
01:01:08,880 --> 01:01:16,190
of the densities in life multiply them together and you can work so i guess

885
01:01:16,190 --> 01:01:17,160
you can

886
01:01:17,210 --> 01:01:21,570
think of this as the density on on it probably on on the union space

887
01:01:21,590 --> 01:01:23,210
where each

888
01:01:23,230 --> 01:01:24,660
of the

889
01:01:25,130 --> 01:01:29,250
each of the spaces in this union space corresponds to having a different number of

890
01:01:31,520 --> 01:01:36,260
anyway sluggish easily see is that

891
01:01:36,260 --> 01:01:38,870
this probability is

892
01:01:38,900 --> 01:01:42,330
independence of the ordering in which

893
01:01:42,330 --> 01:01:43,190
in which

894
01:01:43,230 --> 01:01:45,540
the theta

895
01:01:45,550 --> 01:01:47,120
in which the fate has come in

896
01:01:47,150 --> 01:01:51,380
so if we had reordered the fate test so for example

897
01:01:51,400 --> 01:01:55,010
in this case is they're calling this data one of this code the three four

898
01:01:55,010 --> 01:01:58,100
and according to one

899
01:01:58,120 --> 01:02:01,020
and maybe according to the two ends the tree

900
01:02:01,030 --> 01:02:04,200
then the probability that we have down here

901
01:02:04,230 --> 01:02:07,050
of this permuted sequence

902
01:02:07,060 --> 01:02:10,870
it's still exactly the same because it actually doesn't depend on the ordering of the

903
01:02:11,670 --> 01:02:17,070
so this problem probably this property is called exchangeability to say is that the joint

904
01:02:17,070 --> 01:02:23,100
distribution is invariant to permutations of theta one two theta

905
01:02:23,110 --> 01:02:27,310
and that's that's a pretty

906
01:02:27,320 --> 01:02:28,900
that's a really fundamental

907
01:02:31,370 --> 01:02:33,750
actually held by definitely

908
01:02:33,900 --> 01:02:37,120
actually definitely didn't actually proved the most

909
01:02:37,170 --> 01:02:43,940
most general result by people typically refer to it as the sister

910
01:02:43,950 --> 01:02:48,870
it just says that if you have a joint distribution is exchangeable

911
01:02:48,940 --> 01:02:53,680
then there must be a random probability measure which makes them independent

912
01:02:53,690 --> 01:02:55,790
OK so

913
01:02:57,140 --> 01:02:58,810
so i'll actually stayed

914
01:02:59,720 --> 01:03:03,510
in the case of the polya urn scheme then this random probability measure turns out

915
01:03:03,520 --> 01:03:06,370
to be the dirichlet process

916
01:03:06,370 --> 01:03:08,940
so here's the statement theorem

917
01:03:09,340 --> 01:03:14,110
so if we have a distribution on i want to see and

918
01:03:14,210 --> 01:03:18,110
that is invariant to permutations what this is therefore any permutation

919
01:03:19,620 --> 01:03:24,530
this probability is going to be equal to the probability where we have permitted the

920
01:03:24,530 --> 01:03:27,100
indices of data

921
01:03:27,130 --> 01:03:30,570
and this has to be true for every n

922
01:03:30,820 --> 01:03:34,250
for every positive integer n and if that's the case then we say that the

923
01:03:34,250 --> 01:03:36,520
sequence is infinitely exchangeable

924
01:03:36,640 --> 01:03:40,530
and if a sequence is infinitely exchangeable

925
01:03:41,330 --> 01:03:45,860
in this case here know that the only thing we require is that

926
01:03:45,890 --> 01:03:47,410
that that

927
01:03:47,430 --> 01:03:51,290
the distribution is invariant to permutations it doesn't say that the state has has to

928
01:03:51,290 --> 01:03:53,020
be independent

929
01:03:53,050 --> 01:03:56,200
in fact in the case of the polya urn scheme to think that are not

930
01:03:56,200 --> 01:04:03,400
independent but what the theorem states that there exists a latent we can imagine

931
01:04:03,460 --> 01:04:10,160
there is a random infinite dimensional parameter in this case g

932
01:04:10,180 --> 01:04:11,620
such that

933
01:04:11,640 --> 01:04:16,610
we can write the joint distribution of theta one and theta and

934
01:04:17,540 --> 01:04:19,580
the marginal distribution

935
01:04:19,600 --> 01:04:22,060
where we have a prior on g

936
01:04:23,630 --> 01:04:26,750
each of the things ties are independent given g

937
01:04:26,770 --> 01:04:29,500
so this is the probability of data i

938
01:04:29,540 --> 01:04:30,370
given g

939
01:04:30,380 --> 01:04:32,670
and we can do without g

940
01:04:32,670 --> 01:04:35,520
so we can always write a joint distribution here

941
01:04:36,460 --> 01:04:38,940
where the fetus may not be independent

942
01:04:39,000 --> 01:04:40,710
but there are infinitely exchangeable

943
01:04:40,870 --> 01:04:42,170
as this

944
01:04:42,190 --> 01:04:44,420
the marginal distribution of

945
01:04:44,420 --> 01:04:51,090
OK so and poultry eggs from the kunzman in canada france

946
01:04:51,130 --> 01:04:54,040
i'm going to talk about

947
01:04:54,050 --> 01:04:55,560
machine learning and vision

948
01:04:55,580 --> 01:04:57,040
in the first lecture will

949
01:04:57,060 --> 01:05:02,130
almost entirely the low-level features and the visual signals so when talk very much about

950
01:05:02,130 --> 01:05:04,600
things like object recognition until the second lecture

951
01:05:04,900 --> 01:05:06,970
that's kind of deliberate because

952
01:05:06,970 --> 01:05:10,100
machine and people who start to use vision

953
01:05:10,120 --> 01:05:12,330
often don't have the basics of vision

954
01:05:12,350 --> 01:05:13,460
nineteen ten two

955
01:05:13,470 --> 01:05:18,470
take an image turn into a list of pixels apply kernel to doesn't work very

956
01:05:19,220 --> 01:05:23,900
and the reason is that there's a lot of technology vision itself so i think

957
01:05:23,900 --> 01:05:27,650
it's probably bit of machine learning people to get an overview of what the technologies

958
01:05:27,650 --> 01:05:28,680
and vision

959
01:05:28,720 --> 01:05:31,750
seeing some of the object recognition things

960
01:05:31,760 --> 01:05:35,520
OK if you have any questions stop me interrupt that's fine

961
01:05:35,540 --> 01:05:38,020
OK so the two lectures

962
01:05:38,040 --> 01:05:40,640
all start talking about the visual signal

963
01:05:40,650 --> 01:05:45,220
so this is from human vision something about the statistics of natural images

964
01:05:45,400 --> 01:05:49,780
now talk about low-level features in the next lecture we'll talk about things like object

965
01:05:50,620 --> 01:05:55,940
prominent different order from this image classification and indexing things like that

966
01:05:55,980 --> 01:05:59,840
OK so why is visual recognition hard

967
01:06:00,340 --> 01:06:04,000
if you start doing defined it's really quite hard task we've made a lot of

968
01:06:04,000 --> 01:06:10,150
progress in recent years so hard one thing is very close international natural object classes

969
01:06:10,150 --> 01:06:12,750
so see some examples in the second

970
01:06:12,760 --> 01:06:16,940
the second thing and all the fixed you have a problem

971
01:06:18,280 --> 01:06:19,420
images so

972
01:06:19,440 --> 01:06:23,700
lighting variations contrast variation shadowing specularities

973
01:06:23,800 --> 01:06:27,980
then there's a whole lot of things to do with genetic variability so

974
01:06:28,470 --> 01:06:33,560
translations and rotations within the image scaling and translations when the objects move and it's

975
01:06:33,800 --> 01:06:36,890
also rotations and it's and perspective effects

976
01:06:37,970 --> 01:06:41,640
the three d world is a complex place and when you created into an image

977
01:06:41,670 --> 01:06:43,500
things getting more complex

978
01:06:43,570 --> 01:06:45,750
and in this

979
01:06:45,750 --> 01:06:50,000
carter inclusion so basically anything in the image that we don't want to detect we

980
01:06:50,000 --> 01:06:51,440
don't want to recognise

981
01:06:51,450 --> 01:06:53,510
is going to be a problem for us

982
01:06:53,510 --> 01:06:55,640
this is common for the signals

983
01:06:55,660 --> 01:06:57,390
the signal processing tasks

984
01:06:57,390 --> 01:07:03,680
and the context is extremely important in vision c need to somehow get some of

985
01:07:04,450 --> 01:07:07,390
context from this class that's bothering you

986
01:07:07,410 --> 01:07:10,790
use for the detection task but you don't really want to be distracted by too

987
01:07:10,790 --> 01:07:16,670
much in the other reason that visual recognition is how just seems so damned good

988
01:07:16,710 --> 01:07:19,020
that they don't realize how difficult it is

989
01:07:20,550 --> 01:07:22,300
if you start looking at

990
01:07:22,300 --> 01:07:25,130
images in detail pixels in detail

991
01:07:25,510 --> 01:07:27,960
you find the

992
01:07:28,010 --> 01:07:30,510
you don't understand why you are going to to start working

993
01:07:30,640 --> 01:07:34,830
if you look at the image little piece of pixel the algorithm looking at you

994
01:07:34,830 --> 01:07:38,670
don't understand what it doesn't you see it in a bigger context so

995
01:07:38,680 --> 01:07:42,910
humans have this amazing ability to kind of integrating integrate the information they see that

996
01:07:42,910 --> 01:07:43,950
the whole image

997
01:07:44,070 --> 01:07:46,600
which we currently don't really have

998
01:07:46,860 --> 01:07:49,570
and we're working on but it's hard to do

999
01:07:49,580 --> 01:07:52,700
so there are some reasons why visual recognition is hard

1000
01:07:53,550 --> 01:07:59,850
so natural object classes faces you can see got changes imposed got sunglasses

1001
01:07:59,860 --> 01:08:01,380
changes in lighting

1002
01:08:01,390 --> 01:08:05,860
inclusions and parenting modern telephones

1003
01:08:05,920 --> 01:08:10,640
anything can happen visual class dogs

1004
01:08:12,510 --> 01:08:13,830
it's difficult

1005
01:08:13,890 --> 01:08:17,420
and you have a lot of knowledge about the class to say what the images

1006
01:08:17,420 --> 01:08:18,480
have in common

1007
01:08:18,540 --> 01:08:23,230
have some things in common but it's not very obvious and then across light years

1008
01:08:23,260 --> 01:08:25,950
which is kinda

1009
01:08:25,960 --> 01:08:30,330
structural or functional classes the class defined by the thing having a surface that you

1010
01:08:30,330 --> 01:08:31,390
can sit on

1011
01:08:31,460 --> 01:08:34,070
it's not really visual class

1012
01:08:34,830 --> 01:08:38,760
we can probably make detectors which is the work for many churches but we never

1013
01:08:38,760 --> 01:08:42,020
going to make it to take two weeks for all chairs into we understand the

1014
01:08:42,020 --> 01:08:45,680
way the human environment works what kinds of services are allowed to be set on

1015
01:08:45,680 --> 01:08:46,890
things like this

1016
01:08:46,980 --> 01:08:51,080
so there's a lot of kind of domain information it's non visual

1017
01:08:51,110 --> 01:08:54,580
we have to do have to put into the system if we really want to

1018
01:08:55,180 --> 01:08:56,360
but probably

1019
01:08:57,830 --> 01:09:00,670
so given that small physical things

1020
01:09:00,750 --> 01:09:04,300
the effects of lighting so here are some images

1021
01:09:04,300 --> 01:09:10,430
the same face seen from the same viewpoint anything was changing the lighting

1022
01:09:10,440 --> 01:09:14,620
as a human you probably think OK let's face i've no problem seeing it

1023
01:09:14,750 --> 01:09:18,380
i can email you recognised it's the same person but if you actually look at

1024
01:09:18,380 --> 01:09:19,620
the pixel level

1025
01:09:20,670 --> 01:09:21,750
you find the

1026
01:09:21,770 --> 01:09:25,770
you know the value of a pixel one position just changes completely there's not really

1027
01:09:25,770 --> 01:09:30,230
anything that's in the raw image that will tell you what the thing is so

1028
01:09:30,230 --> 01:09:35,000
you don't have to do some kind of preprocessing to understand

1029
01:09:35,020 --> 01:09:40,120
what's going on with the lighting before you can start doing recognition similarly specularities these

1030
01:09:40,130 --> 01:09:45,630
bright spots on the sun glasses because the light they're very bright saturated skin here

1031
01:09:45,670 --> 01:09:49,280
cameras often saturate you don't see it if you look at it because you say

1032
01:09:49,300 --> 01:09:53,400
used is human but it tends to throw off image processing routines

1033
01:09:54,270 --> 01:09:57,780
may not be able to tell what it is it's two parts with lighting

1034
01:09:57,800 --> 01:09:59,800
they can

1035
01:09:59,800 --> 01:10:03,650
something here or with the graphical models the semantics

1036
01:10:03,660 --> 01:10:06,210
you know it too finely lunch

1037
01:10:06,220 --> 01:10:07,710
on at the beach

1038
01:10:07,760 --> 01:10:16,280
on the side bring is we go with another thing i'm making edits on the

1039
01:10:16,280 --> 01:10:20,040
slides and either the couple's lives plus yesterday

1040
01:10:20,510 --> 01:10:24,170
so i'm going to make the new set of slides available

1041
01:10:24,180 --> 01:10:26,710
at the end of this

1042
01:10:27,450 --> 01:10:32,130
actually during the meeting already copied

1043
01:10:32,140 --> 01:10:33,600
OK so

1044
01:10:33,610 --> 01:10:36,110
let's go to the case of expectation

1045
01:10:36,130 --> 01:10:39,560
we have proposed that some not unknown x

1046
01:10:39,560 --> 01:10:41,060
we have some data

1047
01:10:41,100 --> 01:10:44,320
we have been able to to learn the first let's assume we have estimated the

1048
01:10:44,320 --> 01:10:46,280
posterior distribution

1049
01:10:46,290 --> 01:10:49,020
of the unknown given the data so far

1050
01:10:49,060 --> 01:10:54,050
and what we're interested in computing the expectation of functions

1051
01:10:54,220 --> 01:10:57,780
f of x could be just x in which case we just want the mean

1052
01:10:57,780 --> 01:10:59,930
of the posterior

1053
01:10:59,930 --> 01:11:03,890
or if you're interested in the various we would have x minus the mean

1054
01:11:03,910 --> 01:11:05,380
and so on so

1055
01:11:05,390 --> 01:11:09,390
that is f of x to be something that allows to get any properties of

1056
01:11:09,390 --> 01:11:12,560
this distribution

1057
01:11:12,560 --> 01:11:15,430
so this is how we do it this is how we do

1058
01:11:15,460 --> 01:11:17,300
among the carla for these problems

1059
01:11:17,300 --> 01:11:18,950
we simulate

1060
01:11:18,960 --> 01:11:20,780
a bunch of points

1061
01:11:20,790 --> 01:11:23,060
from this distribution

1062
01:11:23,140 --> 01:11:27,260
and also and explain how it fact that's the whole point of this these lectures

1063
01:11:27,260 --> 01:11:29,600
had that simulation

1064
01:11:29,660 --> 01:11:32,430
but let's assume we have found simulation

1065
01:11:32,460 --> 01:11:37,340
so you have a bunch of points simulate sort of where this higher probability you've

1066
01:11:37,340 --> 01:11:39,130
generator these things

1067
01:11:39,180 --> 01:11:41,520
so it's kind of think of this as the inverse

1068
01:11:41,530 --> 01:11:45,060
in learning we have these data points and we try to come up with this

1069
01:11:45,060 --> 01:11:48,380
kernel density estimate that's what

1070
01:11:48,390 --> 01:11:50,170
we were doing yesterday

1071
01:11:50,180 --> 01:11:53,020
now we're doing the opposite we have the distribution

1072
01:11:53,050 --> 01:11:55,190
we generated data

1073
01:11:55,950 --> 01:12:01,450
in fact and you could use this to publish papers because they generated data and

1074
01:12:01,450 --> 01:12:04,100
then you say that this is the solution to it

1075
01:12:04,160 --> 01:12:07,340
and it will get a very good fit

1076
01:12:07,400 --> 01:12:10,970
it was in the paper that that called

1077
01:12:10,980 --> 01:12:14,150
they they to set selection

1078
01:12:14,250 --> 01:12:19,730
so basically and you come up with the model generated data from the model and

1079
01:12:19,730 --> 01:12:22,390
then you reconstruct the model from that date

1080
01:12:23,710 --> 01:12:27,840
o thing is that is lot of truth that a lot of people to separated

1081
01:12:27,890 --> 01:12:29,810
with the model in the reconstruc

1082
01:12:29,820 --> 01:12:36,200
the data with the same argument only works very well surprise

1083
01:12:37,340 --> 01:12:38,330
once we

1084
01:12:38,350 --> 01:12:40,430
sample these points

1085
01:12:40,450 --> 01:12:43,140
we've been them in some sort of way

1086
01:12:43,150 --> 01:12:47,040
for simplicity let's say that we doing an histogram bin

1087
01:12:48,380 --> 01:12:53,180
and that then approximation gives as an approximation

1088
01:12:54,230 --> 01:12:57,280
this is the posterior distribution

1089
01:12:57,300 --> 01:13:01,830
OK so we have an approximation of the effects given date in green

1090
01:13:04,590 --> 01:13:06,130
in reality

1091
01:13:06,140 --> 01:13:09,930
we don't need to construct this approximation to solve an integral

1092
01:13:09,980 --> 01:13:13,130
that's that's kind of it's very very important point

1093
01:13:13,140 --> 01:13:19,000
because constructing kernel density estimation in very high dimensions the not gonna make any sense

1094
01:13:19,040 --> 01:13:22,180
all we want to sample to computers

1095
01:13:22,180 --> 01:13:26,400
again if this guy is replaced by a delta function

1096
01:13:26,420 --> 01:13:30,680
so if we care about the mean motion is used to replace phi delta function

1097
01:13:30,680 --> 01:13:31,790
after me

1098
01:13:31,820 --> 01:13:33,590
then you only need one sample two

1099
01:13:33,680 --> 01:13:35,170
do this

1100
01:13:35,220 --> 01:13:36,560
we care about

1101
01:13:36,600 --> 01:13:39,060
this guy the estimate

1102
01:13:39,110 --> 01:13:43,060
we don't care so much about thanksgiving feast and this we really care about

1103
01:13:43,070 --> 01:13:49,280
but in general we focus on the estimate

1104
01:13:50,330 --> 01:13:53,710
so basically what we do is the same thing

1105
01:13:54,110 --> 01:13:55,950
this basically

1106
01:13:56,020 --> 01:14:00,670
you can think of this depends just delta function you could replace the centre of

1107
01:14:00,670 --> 01:14:03,440
the ben by delta function

1108
01:14:03,450 --> 01:14:07,820
and the delta function

1109
01:14:10,030 --> 01:14:11,380
can the colour

1110
01:14:15,330 --> 01:14:17,920
there's the delta function here of size two

1111
01:14:17,960 --> 01:14:20,590
delta function of size four

1112
01:14:20,600 --> 01:14:25,790
maybe delta function of size three and one of size two

1113
01:14:25,800 --> 01:14:28,170
and then one of size one

1114
01:14:28,250 --> 01:14:30,190
so you're replacing

1115
01:14:30,200 --> 01:14:33,560
the distribution by the the representation

1116
01:14:33,570 --> 01:14:36,500
and then again by the properties of the delta function

1117
01:14:36,580 --> 01:14:39,940
when you replace all these functions here

1118
01:14:40,000 --> 01:14:44,070
all you get is the function evaluated at each of those delta functions

1119
01:14:44,130 --> 01:14:46,980
so the monte carlo estimate than

1120
01:14:47,040 --> 01:14:50,320
this thing will be replaced just by the sum

1121
01:14:50,380 --> 01:14:52,020
of i call one

1122
01:14:52,030 --> 01:14:53,560
two and

1123
01:14:53,600 --> 01:14:55,060
one over n

1124
01:14:55,060 --> 01:14:56,810
of f of

1125
01:15:02,020 --> 01:15:19,430
that's monte carlo integration

1126
01:15:19,430 --> 01:15:23,060
i'm going to it here because i mean that that's the very much the most

1127
01:15:23,060 --> 01:15:24,470
important thing

1128
01:15:24,530 --> 01:15:27,290
so we have on equal

1129
01:15:27,460 --> 01:15:29,890
suppose we want to mean

1130
01:15:29,900 --> 01:15:31,500
let's say a new

1131
01:15:31,610 --> 01:15:33,180
call x

1132
01:15:33,190 --> 01:15:34,930
p of x given data

1133
01:15:40,230 --> 01:15:45,010
what we do is we construct an approximation of the effects given data

1134
01:15:45,020 --> 01:15:46,440
which will be equal

1135
01:15:46,460 --> 01:15:49,230
his it's a bunch of delta function

1136
01:15:49,290 --> 01:15:53,930
we normalize this so that they sum to one because we have probability that's why

1137
01:15:53,930 --> 01:15:55,810
i have one over n

1138
01:15:55,820 --> 01:15:58,530
and it dealt

1139
01:15:58,530 --> 01:16:02,280
and the one below there can kill this one so i can always identify two

1140
01:16:02,280 --> 01:16:03,900
pairs with kill each other

1141
01:16:03,920 --> 01:16:05,570
that means dark

1142
01:16:05,610 --> 01:16:11,190
so this must be a criterion for destructive interference that may not be the only

1143
01:16:11,190 --> 01:16:15,530
angles for which there is that structure but i want to convince you that there

1144
01:16:15,530 --> 01:16:20,420
is at least one for which is you know

1145
01:16:20,490 --> 01:16:22,360
i will now introduce

1146
01:16:22,420 --> 01:16:24,840
the phase angle bay

1147
01:16:24,860 --> 01:16:29,400
which is the phase difference between source one and source two

1148
01:16:30,240 --> 01:16:31,590
this is the

1149
01:16:31,590 --> 01:16:36,740
one side of the split in the middle it's not the phase angle between two

1150
01:16:36,740 --> 01:16:41,860
neighbouring sources because the neighbouring sources touch each other as an infinite number of point

1151
01:16:41,860 --> 01:16:44,780
sources to continuous working source

1152
01:16:44,800 --> 01:16:47,150
so it is the phase angle between the

1153
01:16:47,190 --> 01:16:51,690
edges of the slit in the centre of the that's the way i define data

1154
01:16:51,710 --> 01:16:53,650
so my better now

1155
01:16:54,800 --> 01:16:56,190
two by

1156
01:16:56,210 --> 01:16:57,780
divided by land

1157
01:16:59,210 --> 01:17:00,840
one of the

1158
01:17:00,900 --> 01:17:02,340
i'm sign

1159
01:17:04,240 --> 01:17:05,210
so you see

1160
01:17:05,210 --> 01:17:08,460
one of each of the two so you can

1161
01:17:09,570 --> 01:17:11,010
divided by lambda

1162
01:17:11,250 --> 01:17:12,610
sign of

1163
01:17:12,610 --> 01:17:18,840
no i will not the right for you as i did

1164
01:17:18,860 --> 01:17:23,760
so precisely for gratings i will not the rifle you what the light intensity is

1165
01:17:23,760 --> 01:17:25,110
a function of angle

1166
01:17:25,210 --> 01:17:26,570
again it is a matter of

1167
01:17:26,610 --> 01:17:28,210
adding vectors

1168
01:17:28,260 --> 01:17:31,420
but you can look it up in the back of your parents down in section

1169
01:17:31,440 --> 01:17:32,510
eight seven

1170
01:17:32,510 --> 01:17:34,590
so i will only give you the answer

1171
01:17:34,610 --> 01:17:35,940
i did it for grading

1172
01:17:35,960 --> 01:17:38,860
you do this one

1173
01:17:38,860 --> 01:17:40,840
and you can show now that

1174
01:17:40,880 --> 01:17:44,440
the intensity of the light as a function of that

1175
01:17:44,460 --> 01:17:46,820
base angle better

1176
01:17:46,940 --> 01:17:48,710
is i zero

1177
01:17:48,760 --> 01:17:51,240
times the sign of data

1178
01:17:51,280 --> 01:17:52,920
divided by data

1179
01:17:52,960 --> 01:17:56,070
and of course no surprise that you get a square there because that has to

1180
01:17:56,090 --> 01:17:59,570
do with the poynting vector

1181
01:17:59,610 --> 01:18:02,300
and this function is very different from

1182
01:18:02,300 --> 01:18:04,340
a great function

1183
01:18:04,400 --> 01:18:06,230
before i plant

1184
01:18:06,240 --> 01:18:07,860
let me

1185
01:18:07,900 --> 01:18:12,610
make a few calculations of the human center board so you can still compare

1186
01:18:12,610 --> 01:18:14,240
with this

1187
01:18:16,420 --> 01:18:17,530
so i'm going to

1188
01:18:17,530 --> 01:18:19,860
right on your side of data is

1189
01:18:21,170 --> 01:18:24,150
and then he might call from data

1190
01:18:24,170 --> 01:18:27,030
and then you come the sign of data

1191
01:18:27,030 --> 01:18:29,820
you come see intensity i

1192
01:18:29,840 --> 01:18:33,880
the first thing to sign of data

1193
01:18:33,960 --> 01:18:35,800
is zero

1194
01:18:35,820 --> 01:18:38,280
one of the sign of data is zero

1195
01:18:38,280 --> 01:18:40,740
it's immediately obvious that data is

1196
01:18:40,800 --> 01:18:43,760
it's also obvious sign of data is zero

1197
01:18:43,760 --> 01:18:46,860
you get zero divided by zero use low low-powered

1198
01:18:46,880 --> 01:18:49,400
and this ratio becomes one

1199
01:18:49,460 --> 01:18:51,340
now the light intensity

1200
01:18:51,340 --> 01:18:53,300
is i zero

1201
01:18:53,320 --> 01:18:56,840
well that's not so surprised that you have a lot of light

1202
01:18:56,880 --> 01:18:59,490
because of course if you have an opening year

1203
01:18:59,570 --> 01:19:03,150
you shine light through with you expect to see right on the wall there in

1204
01:19:03,150 --> 01:19:06,130
the middle of the street you expect to see a lot light so that's not

1205
01:19:06,170 --> 01:19:07,230
so surprising

1206
01:19:07,240 --> 01:19:11,010
this by the way is the way we we define eyes

1207
01:19:11,050 --> 01:19:14,360
so i zero is defined as the maximum that you will see

1208
01:19:14,420 --> 01:19:16,210
when you so to speak look

1209
01:19:16,260 --> 01:19:18,880
straightforward and an angle theta here

1210
01:19:18,880 --> 01:19:21,670
that's the way we define i zero

1211
01:19:21,690 --> 01:19:23,170
so now let's take

1212
01:19:23,190 --> 01:19:25,650
sin theta is land divided by

1213
01:19:25,760 --> 01:19:31,610
so now data is

1214
01:19:31,670 --> 01:19:34,110
fourteen years

1215
01:19:34,130 --> 01:19:35,260
sin theta

1216
01:19:35,260 --> 01:19:38,450
is land that divided by the way this so we you see that there is

1217
01:19:40,110 --> 01:19:41,900
so design of data

1218
01:19:41,940 --> 01:19:44,630
is no zero

1219
01:19:44,630 --> 01:19:48,360
and therefore he invented is zero

1220
01:19:48,380 --> 01:19:49,880
because the upstairs

1221
01:19:49,900 --> 01:19:51,090
is zero

1222
01:19:51,150 --> 01:19:53,760
but the downstairs is not zero

1223
01:19:57,180 --> 01:20:00,440
OK if you take this halfway take this half

1224
01:20:00,460 --> 01:20:03,590
you get the sign of saying i love that divided by d i when he

1225
01:20:03,590 --> 01:20:06,170
predicted that you would have that structure

1226
01:20:06,210 --> 01:20:10,320
destructive interference that's exactly this case of course

1227
01:20:10,380 --> 01:20:12,090
and now we have two lambda

1228
01:20:12,110 --> 01:20:13,920
divided by d

1229
01:20:13,920 --> 01:20:17,510
give me a baton of two pi

1230
01:20:17,550 --> 01:20:19,840
the game me again zero here

1231
01:20:19,940 --> 01:20:23,440
there is no zero so they are more locations in space

1232
01:20:23,470 --> 01:20:26,010
that is complete are not just one

1233
01:20:26,050 --> 01:20:33,490
in fact there is an infinite number of them you can go on like this

1234
01:20:33,550 --> 01:20:36,530
i will first plot the curve and then we'll discuss it in a little bit

1235
01:20:36,530 --> 01:20:38,820
more detail

1236
01:20:38,840 --> 01:20:41,090
and i will only plot

1237
01:20:41,110 --> 01:20:43,340
in terms of sin theta

1238
01:20:43,380 --> 01:20:46,510
because that an angle that i can relate to

1239
01:20:46,670 --> 01:20:52,840
i can tell my mother about data i can tell me more about phase angle

1240
01:20:52,860 --> 01:20:58,760
but i can tell you about theta one this is ten degrees twenty degrees thirty

1241
01:20:58,780 --> 01:21:03,570
degrees that's a real angle in my laboratory that is that a

1242
01:21:03,670 --> 01:21:08,960
then i can relate to always put things in terms of science a

1243
01:21:09,090 --> 01:21:13,150
so here is my zero

1244
01:21:13,210 --> 01:21:15,460
and that this be

1245
01:21:15,490 --> 01:21:18,280
let that divided by the capital b

1246
01:21:18,280 --> 01:21:20,550
that is the with

1247
01:21:20,550 --> 01:21:22,110
this is to like that

1248
01:21:22,190 --> 01:21:23,510
divided by p

1249
01:21:23,960 --> 01:21:25,490
here three lambda

1250
01:21:25,510 --> 01:21:26,990
divided by the

1251
01:21:26,990 --> 01:21:32,340
on the other side minus land that divided by the and so on

1252
01:21:32,340 --> 01:21:33,740
one more

1253
01:21:34,630 --> 01:21:35,570
so now

1254
01:21:35,590 --> 01:21:39,490
the curve that you seem not so obvious

1255
01:21:39,550 --> 01:21:43,530
when you plotted you'll see that you get maximum which we define to be i

1256
01:21:43,530 --> 01:21:46,590
zero in central mexico

1257
01:21:46,630 --> 01:21:50,340
and then you get you for zero

1258
01:21:50,340 --> 01:21:52,040
we train directly

1259
01:21:52,050 --> 01:21:54,550
on the japanese english

1260
01:21:57,250 --> 01:21:59,680
we can actually do better than that

1261
01:21:59,690 --> 01:22:04,550
a little bit and we don't actually have to do any machine translation tool so

1262
01:22:04,550 --> 01:22:06,100
it's stuck

1263
01:22:06,120 --> 01:22:08,580
implicit in the models

1264
01:22:09,150 --> 01:22:16,860
we can look inside w likely to call him representation in the same way resulted

1265
01:22:16,860 --> 01:22:20,470
in his talk and you get some similar kind of things so you

1266
01:22:20,660 --> 01:22:25,190
in the embedding space you find key in this case the cat and the sources

1267
01:22:25,190 --> 01:22:28,980
close to mccartney and so on

1268
01:22:29,000 --> 01:22:32,290
and that's the end of that part

1269
01:22:32,310 --> 01:22:34,850
of the talk we

1270
01:22:34,860 --> 01:22:40,740
showed this kind of models can be powerful supervised models for

1271
01:22:40,750 --> 01:22:42,530
for document ranking

1272
01:22:42,550 --> 01:22:44,800
and the efficient and scalable

1273
01:22:44,830 --> 01:22:51,060
and we should have nonlinearities that we could add to improve accuracy would actually be

1274
01:22:51,070 --> 01:22:53,000
nice to do is make

1275
01:22:53,000 --> 01:22:59,530
these these modules one and two here even more non-linear maybe put like the word

1276
01:22:59,530 --> 01:23:01,080
order into their of

1277
01:23:01,140 --> 01:23:07,390
work with bag of words about here we have have done

1278
01:23:09,110 --> 01:23:10,570
and that brings me

1279
01:23:11,440 --> 01:23:12,820
a glass of water

1280
01:23:12,840 --> 01:23:15,790
and powerful

1281
01:23:15,910 --> 01:23:19,310
so now

1282
01:23:19,410 --> 01:23:21,620
we've looked at

1283
01:23:21,640 --> 01:23:27,360
we've looked to tagging tasks which are not reflect retrieval music understand

1284
01:23:27,810 --> 01:23:29,340
text tasks

1285
01:23:29,450 --> 01:23:34,330
you want to do something a bit more semantic

1286
01:23:34,350 --> 01:23:35,880
we use

1287
01:23:35,890 --> 01:23:39,540
one way to look at what happens if

1288
01:23:39,550 --> 01:23:45,140
you're trying to learn language within the world so your yours you're doing situated learning

1289
01:23:48,120 --> 01:23:51,860
you know all these tasks like part of speech

1290
01:23:51,870 --> 01:23:54,070
tagging chunking

1291
01:23:55,500 --> 01:24:00,200
and machine translation semantic role labeling there

1292
01:24:00,680 --> 01:24:04,710
subtasks like syntactic or semantic sub tasks

1293
01:24:13,380 --> 01:24:21,980
this image invaded my talk and the these things don't actually you know try

1294
01:24:21,980 --> 01:24:27,020
situated learning within the world where humans do we understand language

1295
01:24:27,040 --> 01:24:31,000
often because it has a deep connection to the world we're in

1296
01:24:31,020 --> 01:24:33,960
you know it's often about that world

1297
01:24:33,970 --> 01:24:36,570
if you have sentences like

1298
01:24:36,870 --> 01:24:40,580
he passed the exam john went to the banque or john saw bill in the

1299
01:24:40,580 --> 01:24:44,290
park with his telescope you can

1300
01:24:45,260 --> 01:24:51,040
disambiguate sentences with your knowledge about the world if you happen to know that belies

1301
01:24:51,050 --> 01:24:54,940
the telescope and fred took an exam last week and john is in the countryside

1302
01:24:54,940 --> 01:25:00,220
not in the city then you know you can disambiguating words like bank here

1303
01:25:00,230 --> 01:25:05,000
so it might be riverbank and he might be free

1304
01:25:05,020 --> 01:25:06,590
for example

1305
01:25:06,620 --> 01:25:11,460
and we want to look at how can a computer do that almost specifically how

1306
01:25:11,460 --> 01:25:15,430
can it can be to learn to do that and can we learn

1307
01:25:17,870 --> 01:25:21,690
some representations of the world and of text

1308
01:25:23,090 --> 01:25:26,390
in order to make two to solve this problem

1309
01:25:28,180 --> 01:25:29,870
we looked at

1310
01:25:30,680 --> 01:25:34,180
a kind of simulated situated environment

1311
01:25:34,190 --> 01:25:41,200
by looking at a kind of computer gaming type situation so his the picture seems

1312
01:25:41,200 --> 01:25:44,970
so that means all the coefficients in the objective have the correct sign

1313
01:25:45,450 --> 01:25:46,060
would be negative

1314
01:25:47,030 --> 01:25:49,570
but but some other coefficients in the right hand side

1315
01:25:50,110 --> 01:25:51,290
we have the wrong side

1316
01:25:52,980 --> 01:25:56,700
the dual problem looks like this so the coefficients appear

1317
01:25:57,300 --> 01:25:59,440
have the right so you have the wrong sign that's okay

1318
01:26:00,050 --> 01:26:02,370
but all these have the right side so i can apply either

1319
01:26:02,800 --> 01:26:06,480
the simplex methods that already shown you to this problem

1320
01:26:07,570 --> 01:26:12,030
but do it thinking about it up here and we do that up here instead down here

1321
01:26:14,130 --> 01:26:15,580
so here's a summary

1322
01:26:19,210 --> 01:26:19,800
what we do

1323
01:26:21,070 --> 01:26:24,300
we choose an index i that's negative

1324
01:26:25,500 --> 01:26:26,600
first baeijs negative

1325
01:26:28,790 --> 01:26:31,280
the variable why is the entering variable

1326
01:26:31,860 --> 01:26:32,710
and the dual side

1327
01:26:33,520 --> 01:26:37,670
the variable w i will be deleting area very well on me primal side

1328
01:26:40,060 --> 01:26:41,090
we do exactly what we

1329
01:26:41,760 --> 01:26:43,900
what had before we looked at why

1330
01:26:44,400 --> 01:26:45,380
i increase

1331
01:26:46,070 --> 01:26:50,870
holding things are we figure out which is this these diseases are first exactly that

1332
01:26:51,870 --> 01:26:54,990
exactly what we've been doing all i just want to get on the dual side

1333
01:26:55,830 --> 01:26:57,300
we figure out which are these

1334
01:26:57,830 --> 01:26:59,220
variables must leave

1335
01:27:00,410 --> 01:27:01,750
by doing those ratios that i was

1336
01:27:02,860 --> 01:27:04,940
during mental arithmetic before r

1337
01:27:05,850 --> 01:27:11,020
so in the dual problem why is the entering variable and ceejay the leaving variable

1338
01:27:11,660 --> 01:27:19,070
but on the primal side we just to the corresponding to that word w i is the leaving variable

1339
01:27:20,080 --> 01:27:24,530
he i appeared in the column here and here the road here this w as the leaving variable

1340
01:27:24,940 --> 01:27:26,880
and x jay-z the entering variable

1341
01:27:27,780 --> 01:27:31,490
and after the pivot the primal and dual are so they are transpose eachother

1342
01:27:32,150 --> 01:27:34,700
and so we can continue in all in this way

1343
01:27:35,170 --> 01:27:39,920
and improving the dual problem until the dual reaches of melody and we've solved the problem

1344
01:27:41,570 --> 01:27:44,100
we solve the primal problem by solving the dual problem

1345
01:27:48,980 --> 01:27:50,640
so here is just go through an example

1346
01:27:51,070 --> 01:27:52,980
so if we have a primal problem that looks like this

1347
01:27:53,760 --> 01:27:54,710
with some negatives

1348
01:27:55,130 --> 01:27:59,710
we have that the negative transpose over here so five would be the entering variable

1349
01:28:01,950 --> 01:28:04,860
we look down here we see the three in the two here

1350
01:28:05,560 --> 01:28:10,060
this is hundred three six so this is easy to is the leaving variable

1351
01:28:11,000 --> 01:28:14,660
so the three as the pivot element so the minus three would be the pivotal

1352
01:28:14,660 --> 01:28:18,670
element here i just do their here so i can do that by looking i

1353
01:28:18,670 --> 01:28:22,780
i don't have to write this down the course because it's just the negative transpose

1354
01:28:23,190 --> 01:28:27,400
i can do this entire thought process here i look for the most negative that's

1355
01:28:27,400 --> 01:28:31,830
the minus five here i look across this role and in my mind and thinking

1356
01:28:31,830 --> 01:28:35,790
about the dualism looking here is saying okay look and the dual things can be

1357
01:28:35,790 --> 01:28:39,300
something starting foregoing is zero at a rate of three

1358
01:28:39,750 --> 01:28:42,280
there's something starting at six going to zero to

1359
01:28:42,790 --> 01:28:49,860
this is the one is gonna firstly nowadays can arrangement already shows ignoring minus signs and figuring out which one

1360
01:28:50,560 --> 01:28:51,320
it is the

1361
01:28:52,190 --> 01:28:53,490
entering variable in this case

1362
01:28:53,910 --> 01:28:54,280
and so

1363
01:28:55,690 --> 01:28:57,380
so we have the w two and x two

1364
01:28:59,070 --> 01:28:59,700
guess the pivot

1365
01:29:00,700 --> 01:29:02,200
and we do that if it's correct

1366
01:29:05,500 --> 01:29:07,350
but do one more pit so i picked the minus

1367
01:29:07,760 --> 01:29:08,830
six and a third

1368
01:29:09,450 --> 01:29:10,470
i look across here

1369
01:29:11,110 --> 01:29:15,830
the see so it was primal pivot cells looking for positive numbers and these text files

1370
01:29:16,220 --> 01:29:19,570
now i'm looking for negative numbers in the text fields because i think about the

1371
01:29:19,570 --> 01:29:21,870
negative transpose this will be the only candidate

1372
01:29:22,290 --> 01:29:22,590
and so

1373
01:29:23,300 --> 01:29:24,780
x four must be entering

1374
01:29:29,950 --> 01:29:32,710
so i do that's the on

1375
01:29:44,140 --> 01:29:45,050
we would say

1376
01:29:53,780 --> 01:29:54,800
that's just summarizing the role

1377
01:29:55,950 --> 01:29:57,640
and so we do that and we get a here

1378
01:30:00,400 --> 01:30:02,070
now we look at this one end

1379
01:30:04,360 --> 01:30:05,780
this variable must

1380
01:30:06,420 --> 01:30:10,520
leaving variable across here for positive numbers w one must be the

1381
01:30:10,990 --> 01:30:11,630
entering very

1382
01:30:14,610 --> 01:30:15,350
and more optimal

1383
01:30:19,830 --> 01:30:20,510
that's wrong

1384
01:30:22,110 --> 01:30:23,570
oh yes i did say wrong

1385
01:30:24,920 --> 01:30:25,530
my mistake

1386
01:30:26,560 --> 01:30:28,020
with thinking about these

1387
01:30:29,350 --> 01:30:33,890
do also looking at negative numbers and you're not positive numbers so i'm looking at these two

1388
01:30:34,490 --> 01:30:38,980
the the mental calculation five iterate over half to already tense

1389
01:30:39,420 --> 01:30:42,880
yeah i guess this is the one that wins so it's x one x two

1390
01:30:43,670 --> 01:30:44,490
so i click on

1391
01:30:45,340 --> 01:30:47,990
that's what do the pivot around moment and i end up

1392
01:30:48,570 --> 01:30:51,490
with an optimal dictionary so i just solved

1393
01:30:52,070 --> 01:30:52,660
the problem

1394
01:30:53,230 --> 01:30:55,810
that you didn't know an easy way to sell before r

1395
01:30:56,120 --> 01:30:57,740
by thinking about the dual problem

1396
01:31:03,810 --> 01:31:07,060
does to algorithms by themselves the primal simplex methods

1397
01:31:07,720 --> 01:31:08,800
which i should do first

1398
01:31:09,370 --> 01:31:11,550
and the dual simplex methods which i just showed do

1399
01:31:13,390 --> 01:31:16,860
are constrained in the primal simplex methods only works

1400
01:31:17,890 --> 01:31:18,850
if your primal feasible

1401
01:31:19,700 --> 01:31:21,340
the dual simplex method only works

1402
01:31:21,880 --> 01:31:22,980
if a dual feasible

1403
01:31:24,800 --> 01:31:25,900
here's the problem more

1404
01:31:26,920 --> 01:31:28,010
primal feasible

1405
01:31:28,450 --> 01:31:29,580
and do feasible

1406
01:31:30,740 --> 01:31:33,310
and by the way this is yet another version and my little tool here

1407
01:31:33,310 --> 01:31:37,370
cases and in some ways this can help you want

1408
01:31:37,480 --> 01:31:45,370
this model actually as articulate collaborators showed can be made also exactly supersymmetric and consistent

1409
01:31:45,390 --> 01:31:48,790
OK now

1410
01:31:48,830 --> 01:31:50,640
it's also easy to get

1411
01:31:50,660 --> 01:31:56,660
the family multiplicity free all you need is to have as many intersections families and

1412
01:31:56,660 --> 01:32:03,180
this comes out very naturally therefore these models against him to capture first of all

1413
01:32:03,210 --> 01:32:05,830
relatively straightforward you of course

1414
01:32:05,960 --> 01:32:09,270
beverly is often the case and when you work with them

1415
01:32:09,290 --> 01:32:14,270
or was something doesn't seem to work but one does get nicely grand unified theories

1416
01:32:14,270 --> 01:32:20,480
sort mister systems by thinking about this construction some have been discussed at length by

1417
01:32:20,480 --> 01:32:25,390
many people and here is a review on there are many more

1418
01:32:25,430 --> 01:32:29,890
let's think as i did for the heterotic string let's think about the effective action

1419
01:32:29,890 --> 01:32:36,040
in all these theories in four dimensions you see things will work very very differently

1420
01:32:36,060 --> 01:32:41,210
for the einstein action the thing will work in the same way because you basically

1421
01:32:41,210 --> 01:32:47,200
the graviton leaves and then dimensions therefore it sees the volume of the compact six

1422
01:32:47,200 --> 01:32:54,440
dimensions and effective for dimensionality gravitational coupling will be given by the same expressions as

1423
01:32:56,100 --> 01:33:01,460
but for the young theory things can work very differently because they're young meets theory

1424
01:33:01,460 --> 01:33:03,870
leaves no one is localized

1425
01:33:03,890 --> 01:33:10,770
brains share hypersurface is compact space therefore it only uses a part of this volume

1426
01:33:10,770 --> 01:33:15,750
namely the part of the volume that is all wrapped by these brains

1427
01:33:15,770 --> 01:33:20,710
if they are the brains that's v minus three if you have CD three brains

1428
01:33:20,710 --> 01:33:23,870
that to minimize OK this is just one

1429
01:33:24,870 --> 01:33:31,500
therefore you get a different relation for the young men's captain constant

1430
01:33:31,520 --> 01:33:37,460
in this theory is contradicted the heterotic string gauge couplings need not unify actually and

1431
01:33:38,120 --> 01:33:43,060
in the sense that the robot can because they do seem to unify unifying nature

1432
01:33:43,080 --> 01:33:46,910
you don't have to unify it can of course there is no no-go theorem i

1433
01:33:46,910 --> 01:33:52,060
showed you before a realisation refers five what they should

1434
01:33:52,080 --> 01:33:57,330
on the other hand because there's no universal relation tying this thing scale to the

1435
01:33:57,330 --> 01:34:03,310
planck scale there is much more freedom to play around with scales before pretty much

1436
01:34:03,310 --> 01:34:09,710
factor having a grand unified theory very very high up to the sixteen and seventeen

1437
01:34:09,730 --> 01:34:12,830
and then you know the experimental consequences

1438
01:34:12,830 --> 01:34:17,250
i would have to be looked at by the in the gravitational sector i'll say

1439
01:34:18,040 --> 01:34:18,540
you mean

1440
01:34:18,560 --> 01:34:24,560
something about it or indirectly through proton decay or other such signatures

1441
01:34:24,560 --> 01:34:29,790
here seems this case and not tied any more you can play around

1442
01:34:30,480 --> 01:34:32,390
for instance you see

1443
01:34:32,390 --> 01:34:37,480
that the black scale depends very sensitively on the volume of the compact space which

1444
01:34:37,480 --> 01:34:38,810
is transverse

1445
01:34:38,870 --> 01:34:43,930
the standard model brain namely the part of the volume which is the ratio of

1446
01:34:43,960 --> 01:34:48,560
six dimensional volume the p humanist three-dimensional volume

1447
01:34:48,660 --> 01:34:52,560
therefore one can consider very exotic possibilities

1448
01:34:52,580 --> 01:34:59,910
for instance if the standard model lives on a seven brane one hence blank scale

1449
01:35:00,020 --> 01:35:03,580
effective for the management of thanksgiving given by these

1450
01:35:03,580 --> 01:35:08,000
and this is the extreme possibility of taking more than one of things happening

1451
01:35:08,020 --> 01:35:14,440
but taking this transverse volume two-dimensional transverse volume to be in the micro or almost

1452
01:35:14,460 --> 01:35:15,660
immediately metric

1453
01:35:15,680 --> 01:35:16,790
the region

1454
01:35:16,790 --> 01:35:20,060
and this the scale to be very close to the TV and this is the

1455
01:35:20,060 --> 01:35:27,850
sixth in situation that is discussed sometimes one wonders about records in the LHC so

1456
01:35:27,850 --> 01:35:28,790
this is

1457
01:35:28,890 --> 01:35:35,370
priority not ruled out by the more the model building rules of orientifold vacua

1458
01:35:35,410 --> 01:35:40,810
no such extreme scenarios the weakness of gravity is due to the fact that gravitational

1459
01:35:40,810 --> 01:35:45,250
flux spreads out this extra dimensional space

1460
01:35:45,270 --> 01:35:47,140
and therefore that's why

1461
01:35:47,160 --> 01:35:49,500
the lines are less dense and

1462
01:35:49,500 --> 01:35:53,410
the forces we can

1463
01:35:54,890 --> 01:35:57,210
you may say that web

1464
01:35:57,230 --> 01:36:01,850
here there is at least something very nice and actually in the beginning it was

1465
01:36:01,870 --> 01:36:06,140
i felt so is you know have brought down the string scale to the TV

1466
01:36:06,140 --> 01:36:11,660
besides the fact that you LHC people will see all sorts of nice stuff next

1467
01:36:11,660 --> 01:36:16,060
year we have also made sort of the gauge hierarchy problem we don't need to

1468
01:36:16,060 --> 01:36:17,410
worry anymore

1469
01:36:17,430 --> 01:36:18,790
about why

1470
01:36:18,850 --> 01:36:24,480
the electroweak scale is still lower than the planck scale well the planck scale is

1471
01:36:24,480 --> 01:36:28,200
not fundamental what is fundamental the string scale that's

1472
01:36:28,250 --> 01:36:32,390
if UDVT of course that's a red herring with having to solve the problem

1473
01:36:32,430 --> 01:36:34,790
because we now need to to explain

1474
01:36:34,810 --> 01:36:39,960
an equivalent question why the hell is the volume of these transverse spatial large what

1475
01:36:39,960 --> 01:36:41,000
sets it

1476
01:36:41,020 --> 01:36:44,390
why it's not of the order of the string scale which would have been more

1477
01:36:47,850 --> 01:36:52,040
this brings me to the more general question which is actually

1478
01:36:52,040 --> 01:36:52,680
you know

1479
01:36:52,700 --> 01:36:58,430
the game theory a gradient over them

1480
01:36:58,440 --> 01:37:00,150
OK good so

1481
01:37:00,300 --> 01:37:04,750
learning theory in the eighties and nineties this problem came back came back by problem

1482
01:37:04,750 --> 01:37:08,880
called the problem of combining expert advice where we're really gonna now think of and

1483
01:37:08,980 --> 01:37:13,290
a very large number room and we have a lot of prediction rules so these

1484
01:37:13,290 --> 01:37:17,530
choices are now a bunch of different prediction rules we might be using it might

1485
01:37:17,530 --> 01:37:20,980
be a whole space of a whole class of different possible

1486
01:37:20,990 --> 01:37:25,010
classifiers prediction rules we might be using and we'd like to do nearly as well

1487
01:37:25,010 --> 01:37:27,040
as the best of these predictions

1488
01:37:27,090 --> 01:37:31,040
so our job to do nearly as well as the best function in that class

1489
01:37:31,140 --> 01:37:35,460
it's very nice algorithm littlestone warmuth in the late eighties showed how

1490
01:37:35,480 --> 01:37:39,340
you can get a cost which does nearly as well as the best function only

1491
01:37:39,340 --> 01:37:43,890
by one possible factor was an additive term was this type of log the number

1492
01:37:43,890 --> 01:37:48,790
of rules of repertoire if you said that on that about fifty two things out

1493
01:37:48,790 --> 01:37:54,550
it gives you regret like the previous but logan over where logan is this current

1494
01:37:54,740 --> 01:37:59,180
to the number of time steps regret and that's one of the DNA wrapped on

1495
01:37:59,190 --> 01:38:04,030
squared is like login that's great if the number of choices you have a large

1496
01:38:05,020 --> 01:38:08,050
a lot of different possible ways to pick from because

1497
01:38:12,450 --> 01:38:16,140
very much faster so that means that for that doesn't hurt you too much to

1498
01:38:16,140 --> 01:38:20,250
throw a lot more options into them

1499
01:38:20,310 --> 01:38:23,240
i don't really know because

1500
01:38:23,250 --> 01:38:24,780
fans know

1501
01:38:24,790 --> 01:38:26,350
to the thirtieth

1502
01:38:26,370 --> 01:38:27,850
logan is much better

1503
01:38:35,010 --> 01:38:38,670
yes that's right that

1504
01:38:38,810 --> 01:38:40,490
and i sure which is which can

1505
01:38:40,610 --> 01:38:42,180
his was not anyway

1506
01:38:45,300 --> 01:38:47,330
it's also

1507
01:38:50,460 --> 01:38:52,730
i went to this conference

1508
01:38:53,750 --> 01:38:59,580
yes that's esther so so they work also had this kind

1509
01:38:59,590 --> 01:39:04,410
and since then there's been a lot of other work the now optimal the function

1510
01:39:04,410 --> 01:39:08,140
then has been a lot of work getting exact constants king's second order terms in

1511
01:39:08,140 --> 01:39:10,240
various other things

1512
01:39:10,260 --> 01:39:12,300
but we're not going to worry about

1513
01:39:12,350 --> 01:39:15,750
now this is on the non bandit model this is on the model where you

1514
01:39:15,750 --> 01:39:20,870
get full feedback so you when you drive when you're done you find out from

1515
01:39:20,870 --> 01:39:25,810
your friends how long each other options and it's actually very natural in this context

1516
01:39:25,980 --> 01:39:30,140
because when you try to make prediction if you find out the right and you

1517
01:39:30,140 --> 01:39:33,040
can figure out how well you would have done had he chosen one of the

1518
01:39:33,040 --> 01:39:34,090
other predictions

1519
01:39:34,190 --> 01:39:37,590
in the bandit model where you have been choices and you only get feedback from

1520
01:39:37,590 --> 01:39:41,160
the choice you took you have to is try everything i mean you know maybe

1521
01:39:41,160 --> 01:39:46,920
there's some option after just have tried got things together with the the cost of

1522
01:39:46,940 --> 01:39:48,490
this extra factor

1523
01:39:48,590 --> 01:39:52,490
OK this is kind of as a quick history and i want to do it

1524
01:39:54,090 --> 01:39:58,940
is look at a particular problem in this series of the problem of combining expert

1525
01:40:08,400 --> 01:40:09,900
so imagine

1526
01:40:10,040 --> 01:40:13,810
whatever is and we'd like to predict the stock

1527
01:40:13,830 --> 01:40:15,700
i mean i still do

1528
01:40:15,760 --> 01:40:21,540
make some money so we're going to do is we're going to ask an experts

1529
01:40:21,540 --> 01:40:25,730
for their opinion i think the markets go up or down

1530
01:40:25,800 --> 01:40:30,290
so these are a bit different prediction rules can be actually people that could be

1531
01:40:30,290 --> 01:40:34,920
prediction rules that could be different statistics that we're looking at different you know economic

1532
01:40:34,920 --> 01:40:38,990
indicators whatever and each one is suggesting the market up down up and we want

1533
01:40:38,990 --> 01:40:40,520
somehow combine

1534
01:40:41,420 --> 01:40:46,140
and we'd like to use the advice of all these different indicators were people prediction

1535
01:40:46,160 --> 01:40:50,250
somehow make our own production OK so for instance you know maybe on the one

1536
01:40:50,660 --> 01:40:52,320
is very expert

1537
01:40:52,330 --> 01:40:58,420
they are down the property neighbours dog weighing scale and then we make a prediction

1538
01:40:58,480 --> 01:41:02,400
and then we find out what happened to the market

1539
01:41:02,460 --> 01:41:07,030
the next day down up down the road

1540
01:41:07,030 --> 01:41:09,940
i think it's my neighbour's dog always right

1541
01:41:10,090 --> 01:41:12,000
i got

1542
01:41:12,040 --> 01:41:15,260
the only two examples

1543
01:41:15,310 --> 01:41:17,700
i think it's quite good indicators

1544
01:41:17,940 --> 01:41:22,600
OK this is that the game to go to work to get this information organised

1545
01:41:22,600 --> 01:41:26,570
and make our own prediction and then we get the right and wrong and we

1546
01:41:26,570 --> 01:41:28,310
get you know

1547
01:41:28,320 --> 01:41:33,870
we get mistake we get it wrong the mistake right

1548
01:41:33,890 --> 01:41:36,280
OK our job is to not make too many mistakes

1549
01:41:36,280 --> 01:41:40,770
this i mean i assume that this conclusion is not true

1550
01:41:40,780 --> 01:41:45,180
but all the other things are true and negative contradiction

1551
01:41:45,180 --> 01:41:50,680
so assuming that these two sets are not the same means that there's atleast one

1552
01:41:50,680 --> 01:41:54,220
element which is in one sense but not in the subset

1553
01:41:54,340 --> 01:41:57,780
for simplicity on assume that x one

1554
01:41:57,800 --> 01:42:01,120
so x one is a member of the set x qx one is not in

1555
01:42:01,120 --> 01:42:03,010
the same way

1556
01:42:03,020 --> 01:42:09,280
OK so then of first move over this some to the left hand side so

1557
01:42:09,280 --> 01:42:12,020
i can subtract this one from this

1558
01:42:12,020 --> 01:42:16,330
so what do i get well actually the two things i moved this

1559
01:42:16,350 --> 01:42:22,340
to this site and then it might happen first excluded that these excess repeat otherwise

1560
01:42:22,340 --> 01:42:26,390
repeat but some otherwise might be identical to some of the axis i haven't excluded

1561
01:42:26,390 --> 01:42:30,380
that so if i move this thing to the other side i want to make

1562
01:42:30,380 --> 01:42:35,990
it nice again and make it a sum over pairwise distinct points and if it

1563
01:42:35,990 --> 01:42:39,520
turns out that some otherwise i could some of the excess i can just update

1564
01:42:39,520 --> 01:42:41,230
the coefficient corresponding

1565
01:42:42,180 --> 01:42:46,050
the only thing i know is that

1566
01:42:46,080 --> 01:42:49,030
x one is not in set y so the coefficient of x one is not

1567
01:42:49,030 --> 01:42:54,660
going to change all might change arbitrary arbitrarily about the coefficient of x one is

1568
01:42:54,660 --> 01:42:58,570
not going to change so i moved this thing over i can make it a

1569
01:42:58,570 --> 01:43:00,810
sum of pairwise distinct points

1570
01:43:00,820 --> 01:43:02,540
well i know that

1571
01:43:02,560 --> 01:43:06,520
first coefficient i one is equal

1572
01:43:06,530 --> 01:43:11,760
two the coefficient that had already full of one that is by assumption non-zero so

1573
01:43:11,760 --> 01:43:15,280
i was assuming all countries on zero

1574
01:43:15,520 --> 01:43:17,350
and all other coefficients

1575
01:43:17,410 --> 01:43:20,320
might be might be anything don't care

1576
01:43:20,330 --> 01:43:25,510
so then now i have this equality here i know that first cooperation is nonzero

1577
01:43:26,460 --> 01:43:33,050
dot product in the is encoded space with the same thing same expression again

1578
01:43:33,140 --> 01:43:38,700
so you remember was the dot product between kernel and the kernel

1579
01:43:38,720 --> 01:43:42,590
it's exactly kernel evaluated at these two points

1580
01:43:42,610 --> 01:43:45,510
so the product between this and this

1581
01:43:45,520 --> 01:43:48,210
usually this equation here

1582
01:43:48,210 --> 01:43:50,830
what do i know i know that the first go

1583
01:43:51,440 --> 01:43:55,680
number one is nonzero other grammars might be zero but anyway overall the effect of

1584
01:43:55,680 --> 01:43:57,610
grammars is not the zero vector

1585
01:43:57,640 --> 01:44:02,980
so i know this equality here is true with vector which is not the zero

1586
01:44:02,980 --> 01:44:08,820
vector hence the kernel is not strictly positive definite because that's exactly what the stipulation

1587
01:44:08,820 --> 01:44:12,070
of strictly positive definite this excludes

1588
01:44:12,100 --> 01:44:13,950
so our our assumptions

1589
01:44:13,950 --> 01:44:16,410
the kernel is strictly positive definite

1590
01:44:16,420 --> 01:44:17,450
cannot hold true

1591
01:44:19,130 --> 01:44:24,180
or contradiction is worked out and we know that the two sets so that this

1592
01:44:24,180 --> 01:44:29,430
thing cannot happen that two sets have to be identical

1593
01:44:29,440 --> 01:44:31,290
so that's

1594
01:44:31,320 --> 01:44:34,160
kind of interesting and it turns out

1595
01:44:34,200 --> 01:44:35,190
one can

1596
01:44:35,200 --> 01:44:37,720
make it more general because at the end of the day we are

1597
01:44:38,390 --> 01:44:44,010
actually it's hard to say but in statistics one is often interested not just statements

1598
01:44:44,010 --> 01:44:48,050
about finite datasets but also statements about distributions

1599
01:44:48,220 --> 01:44:51,600
the whole body distributions can we

1600
01:44:51,610 --> 01:44:56,520
can we generalise this mapping so there's nothing seems to be kind of interesting things

1601
01:44:56,520 --> 01:44:58,410
on mapping to recap

1602
01:44:58,440 --> 01:45:02,080
takes a set of points maps it into a single point and that single point

1603
01:45:02,080 --> 01:45:04,510
still remembers all properties

1604
01:45:04,520 --> 01:45:07,670
of the set of points

1605
01:45:07,680 --> 01:45:11,770
that's quite nice also because if you imagine just as side note this these points

1606
01:45:11,770 --> 01:45:17,380
that could be anything they could be strings could be vectors they could be texts

1607
01:45:17,530 --> 01:45:21,280
could be images as long as you can define a kernel function on these points

1608
01:45:21,300 --> 01:45:23,750
you you'll find so

1609
01:45:23,750 --> 01:45:27,690
we have set of general points we turn it into a vector of factors are

1610
01:45:27,690 --> 01:45:31,530
nice because we can do linear algebra with them and this vector

1611
01:45:31,590 --> 01:45:32,730
subject to

1612
01:45:32,800 --> 01:45:36,350
the conditions on the kernel function this vector knows everything

1613
01:45:36,420 --> 01:45:39,920
about that point to the victors and as representation

1614
01:45:39,930 --> 01:45:44,090
you can do it our department and you have lost in information

1615
01:45:44,110 --> 01:45:50,520
OK so

1616
01:45:50,520 --> 01:45:52,060
c so

1617
01:45:52,080 --> 01:45:56,750
so the mapping that we've been talking about all let's think of it as a

1618
01:45:56,750 --> 01:46:01,830
mapping on and called the mean mapping is mapping takes a set of points and

1619
01:46:01,830 --> 01:46:03,190
it computes the means

1620
01:46:03,210 --> 01:46:05,340
in the reproducing kernel hilbert space

1621
01:46:05,360 --> 01:46:08,520
so let's see what kind of properties this mapping has

1622
01:46:08,570 --> 01:46:10,850
first properties

1623
01:46:11,040 --> 01:46:13,200
if we take that mapping

1624
01:46:13,200 --> 01:46:17,610
of the set of points apply to this point and we take the dot product

1625
01:46:17,610 --> 01:46:19,940
with an arbitrary function of of space

1626
01:46:19,950 --> 01:46:24,250
so i'll just substitute this in here to take the product with the function of

1627
01:46:24,250 --> 01:46:25,320
remember the

1628
01:46:25,340 --> 01:46:29,040
kernel represents point evaluation of function

1629
01:46:29,060 --> 01:46:32,180
so the product between the content x i

1630
01:46:32,250 --> 01:46:37,710
the function exactly give us gives us the value of the function x i

1631
01:46:37,720 --> 01:46:44,710
so the dot product between these average of kernels just gives us the average of

1632
01:46:44,720 --> 01:46:49,010
function on the set of points so in a way this if the kernel represents

1633
01:46:49,010 --> 01:46:51,550
point evaluation then the

1634
01:46:51,600 --> 01:46:54,880
the mean of a set of points in the feature space this represents

1635
01:46:54,930 --> 01:46:59,740
evaluating the mean of a function over a set of points

1636
01:46:59,760 --> 01:47:02,810
so that's one thing the other thing is

1637
01:47:03,990 --> 01:47:06,530
take two sets of points x and y

1638
01:47:06,550 --> 01:47:09,750
met them into the feature space and then take the difference vector

1639
01:47:09,760 --> 01:47:11,920
so if we do that

1640
01:47:11,940 --> 01:47:16,520
one thing one way to write down the difference vector

1641
01:47:16,570 --> 01:47:22,070
is essentially the cushy schwartz inequality again

1642
01:47:22,100 --> 01:47:24,480
is the largest value that we can get

1643
01:47:24,480 --> 01:47:27,290
by taking dot products between the vectors

1644
01:47:28,140 --> 01:47:33,380
arbitrary unit vectors of factors that have maximal length unit

1645
01:47:33,390 --> 01:47:36,230
so i guess you can imagine if you have vector and you take all sorts

1646
01:47:36,230 --> 01:47:40,540
of products with vectors of length one the largest you can get is the length

1647
01:47:40,540 --> 01:47:44,300
of the vector itself you get exactly the vector is parallel to the first one

1648
01:47:44,590 --> 01:47:49,810
is just one way of rewriting this link here of course not and it's much

1649
01:47:49,810 --> 01:47:54,840
more complicated because it looks like this is complicated optimisation problem over this unit ball

1650
01:47:54,860 --> 01:48:00,950
in our potentially infinite dimensional space but it's the same thing but let's rewrite this

1651
01:48:00,950 --> 01:48:04,890
one using this inequality is this equality that we derived before

1652
01:48:05,070 --> 01:48:10,060
here we have is the products between means and functions they so just gives us

1653
01:48:10,550 --> 01:48:12,260
mean values of functions

1654
01:48:12,280 --> 01:48:17,010
so what is this thing here this thing here looks for

1655
01:48:17,020 --> 01:48:19,700
functions so you're looking for

1656
01:48:19,710 --> 01:48:20,850
the function

1657
01:48:20,870 --> 01:48:26,560
a function which has maximum length one so much the the maximum norm one which

1658
01:48:28,270 --> 01:48:30,530
this sets of points x and y

1659
01:48:30,550 --> 01:48:34,500
so we have two samples x and y and now we're looking for a function

1660
01:48:34,500 --> 01:48:38,340
which will on average take different values and x

1661
01:48:38,390 --> 01:48:40,910
and why

1662
01:48:40,950 --> 01:48:42,810
so finding the function

1663
01:48:42,830 --> 01:48:48,270
which gives you which best distinguishes the two samples x and y is the same

1664
01:48:48,270 --> 01:48:52,820
as computing the distance between the means to us

1665
01:48:52,830 --> 01:48:56,320
so this is also kind of interesting and we'll come back to that in a

1666
01:48:58,520 --> 01:49:02,880
so we can also look if we solve this optimisation problem five we found that

1667
01:49:02,880 --> 01:49:06,410
function best distinguishing the samples we can look how this function

1668
01:49:06,420 --> 01:49:08,040
how this function looks

1669
01:49:08,060 --> 01:49:14,210
for a given dataset surprising we can say we sample data points from a girl

1670
01:49:14,300 --> 01:49:18,420
in plus pleasant distribution so let's say x is sampled from goes in y is

1671
01:49:18,420 --> 01:49:21,220
the numbers in the streets of we end up level by level and an upper

1672
01:49:22,200 --> 01:49:25,390
that's going to miss the answer to this is the total

1673
01:49:25,410 --> 01:49:27,950
computed level by level was the

1674
01:49:27,970 --> 01:49:30,340
he went to compute it usually gives you

1675
01:49:30,340 --> 01:49:32,650
nice answers like geometric answers

1676
01:49:32,930 --> 01:49:38,590
so we have one time games plus five sixteen times square and if we believe

1677
01:49:38,590 --> 01:49:39,990
in fate

1678
01:49:40,010 --> 01:49:45,430
and we see this three number recurrence we know that we have the right answer

1679
01:49:45,430 --> 01:49:46,990
in general is going to be

1680
01:49:46,990 --> 01:49:51,130
in five the power k over sixteen to the power cables we hope

1681
01:49:51,240 --> 01:49:55,740
and so on and you know it keeps going it doesn't go on infinitely but

1682
01:49:55,740 --> 01:49:59,990
let's just assume it goes on infinitely that will be an upper bound

1683
01:49:59,990 --> 01:50:01,910
because on forever

1684
01:50:01,930 --> 01:50:04,820
this is all times and square

1685
01:50:04,840 --> 01:50:06,490
OK now

1686
01:50:06,510 --> 01:50:09,840
if you're in no one thing about geometric series you should know

1687
01:50:09,860 --> 01:50:14,200
the one class have lost according of the sum of the powers of two

1688
01:50:14,220 --> 01:50:19,070
you get

1689
01:50:20,110 --> 01:50:25,010
OK were computer scientists began at least the binary case this is like writing zero

1690
01:50:25,010 --> 01:50:29,220
point one one one one one one one in binary actually one point one one

1691
01:50:29,220 --> 01:50:35,220
one one and one one one one forever is the same as one the

1692
01:50:35,260 --> 01:50:39,070
this is even smaller we five sixty that's less than half the number is growing

1693
01:50:39,070 --> 01:50:42,530
each time so this is even less than two

1694
01:50:44,410 --> 01:50:48,130
if you want there's a nifty formula for solving the general

1695
01:50:48,150 --> 01:50:51,780
she metric series but all we need is that it's the constant so this is

1696
01:50:51,930 --> 01:50:53,720
order and square

1697
01:50:53,840 --> 01:50:57,070
it's also make and square

1698
01:50:57,090 --> 01:51:00,630
pretty obvious system against great because the top thing is square

1699
01:51:00,680 --> 01:51:04,680
so there's are lower bound and square and we have it within

1700
01:51:04,700 --> 01:51:06,180
factor two

1701
01:51:06,180 --> 01:51:08,280
pretty good you actually get better factor here

1702
01:51:08,300 --> 01:51:12,320
so that's recursion tree method so little shaky here because we have these dot dot

1703
01:51:12,320 --> 01:51:13,590
dot and we just

1704
01:51:13,610 --> 01:51:18,410
i believe that geometric turns most of times geometric problem here i would define check

1705
01:51:18,410 --> 01:51:19,590
it with the

1706
01:51:19,610 --> 01:51:24,430
with the substitution because it's not obvious to me it's going to be geometric in

1707
01:51:24,430 --> 01:51:26,470
the cases look at in the moment

1708
01:51:26,490 --> 01:51:28,010
it will be much clearer

1709
01:51:28,130 --> 01:51:37,070
so clear that we can state theorem that everything is working fine

1710
01:51:37,090 --> 01:51:40,050
OK and still time

1711
01:51:40,070 --> 01:51:47,280
so that was recursion trees

1712
01:51:47,300 --> 01:51:49,910
one more method we're going to talk about

1713
01:51:49,950 --> 01:51:54,360
and you can essentially think of it as an application of the recursion tree method

1714
01:51:54,410 --> 01:51:55,150
but it's

1715
01:51:55,160 --> 01:51:56,590
made more precise

1716
01:52:01,780 --> 01:52:05,450
this is an actual

1717
01:52:05,490 --> 01:52:11,740
theorem was recursion tree better if the data does obvious better check

1718
01:52:11,760 --> 01:52:17,660
the sad part about the master method is it's it's pretty restrictive only applies

1719
01:52:17,720 --> 01:52:22,180
two particular family recurrences

1720
01:52:24,220 --> 01:52:32,220
so it should be here then

1721
01:52:32,240 --> 01:52:36,680
equals a some constant eight times t and over b

1722
01:52:36,700 --> 01:52:39,660
plus some function of n

1723
01:52:39,800 --> 01:52:42,090
we call f

1724
01:52:42,090 --> 01:52:44,320
yes i call f

1725
01:52:44,340 --> 01:52:48,340
so in particular will not cover the occurrence i just solve because i was requesting

1726
01:52:48,340 --> 01:52:52,910
on two different problems of different sizes here every problem you recurse on should be

1727
01:52:52,910 --> 01:52:57,300
of the same size a some problems i think this is the recursive algorithm a

1728
01:52:57,300 --> 01:53:01,680
subproblems each of them is of size and over b so the total cost will

1729
01:53:01,680 --> 01:53:04,910
be less than a f and non recursive work

1730
01:53:04,910 --> 01:53:09,930
few constraints a should be at least one

1731
01:53:09,950 --> 01:53:14,340
at least one recursion b should be strictly greater than one you better make problem

1732
01:53:14,360 --> 01:53:16,130
smaller else is going to be

1733
01:53:17,410 --> 01:53:22,860
and f f should have some nice property

1734
01:53:22,910 --> 01:53:26,280
f of n should be

1735
01:53:26,550 --> 01:53:30,820
asymptotically positive

1736
01:53:30,860 --> 01:53:39,820
how many people are asymptotically positive means

1737
01:53:39,840 --> 01:53:44,840
no one can even read the textbook that's OK i haven't read either but until

1738
01:53:45,650 --> 01:53:50,490
and then he noticed and

1739
01:53:50,510 --> 01:53:57,720
what what you think asymptotically positive means that we can do a little bit better

1740
01:54:02,300 --> 01:54:05,570
his means from large enough for venizelos

1741
01:54:06,650 --> 01:54:08,260
this means

1742
01:54:08,280 --> 01:54:10,820
is greater than zero

1743
01:54:10,840 --> 01:54:14,660
four and every some and not for some constant

1744
01:54:14,720 --> 01:54:18,320
and so eventually should be positive i mean we don't care about whether it's a

1745
01:54:18,320 --> 01:54:20,070
good one for n equals one

1746
01:54:20,090 --> 01:54:21,680
not a big deal

1747
01:54:21,700 --> 01:54:28,070
what effect the answer because we only care about the asymptotics of that

1748
01:54:28,160 --> 01:54:35,260
good so the master method you given the recurrence of this form it tells you

1749
01:54:35,260 --> 01:54:36,360
the answer

1750
01:54:36,360 --> 01:54:39,680
that's the great thing about the master method the annoying thing about the master method

1751
01:54:39,680 --> 01:54:43,050
is it has three cases it's about long u

1752
01:54:43,780 --> 01:54:46,570
it takes a little bit longer to memorize than all the others because the others

1753
01:54:46,570 --> 01:54:49,160
are just ideas here we need to actually remember

1754
01:54:49,160 --> 01:54:50,880
two things

1755
01:54:50,890 --> 01:54:54,660
some of the state here

1756
01:54:54,680 --> 01:54:59,550
but not quite there is one very simple idea which is we're gonna compare this

1757
01:54:59,550 --> 01:55:01,630
non recursive work for them

1758
01:55:02,430 --> 01:55:04,450
a very particular function

1759
01:55:04,450 --> 01:55:06,700
and log base b of

1760
01:55:06,880 --> 01:55:10,470
KY and to lock base b event

1761
01:55:10,490 --> 01:55:11,360
you'll see

1762
01:55:12,610 --> 01:55:17,990
turns out it is the number of leaves in the recursion tree will

1763
01:55:18,030 --> 01:55:20,150
that's the foreshadowing

1764
01:55:20,740 --> 01:55:23,720
it's either less equal or bigger

1765
01:55:23,740 --> 01:55:26,930
and here we care about asymptotics and this is all we have to be a

1766
01:55:26,930 --> 01:55:31,590
little bit more precise about less equal bigger you might think well means willow big

1767
01:55:31,590 --> 01:55:33,510
data or little america

1768
01:55:33,660 --> 01:55:36,950
it would be nice if the theorem held for all those cases but it is

1769
01:55:36,950 --> 01:55:39,110
a bit some gaps

1770
01:55:39,130 --> 01:55:45,630
so let's start with case one is one is when f is smaller and not

1771
01:55:45,630 --> 01:55:47,200
just that's willow

1772
01:55:49,110 --> 01:55:53,740
it's actually quite a bit smaller it's going to be polynomially smaller than and log

1773
01:55:53,740 --> 01:55:55,840
base b

1774
01:55:55,930 --> 01:56:06,840
so for some positive and so on

1775
01:56:06,840 --> 01:56:11,340
the running time should be this and to this constant always be a minus that

1776
01:56:11,340 --> 01:56:16,390
you could assume that the annotation does not necessarily depend on the position in the

1777
01:56:16,390 --> 01:56:18,190
text where you are

1778
01:56:18,200 --> 01:56:20,170
this may not always be a very good idea

1779
01:56:20,220 --> 01:56:22,860
prince if you read the newspaper article

1780
01:56:22,870 --> 01:56:24,310
there are set up in such a way

1781
01:56:24,320 --> 01:56:27,300
the first few phrases contain the most important bit

1782
01:56:27,350 --> 01:56:31,680
and then the information content goes down and goes more into the background

1783
01:56:31,690 --> 01:56:32,900
so therefore

1784
01:56:32,920 --> 01:56:35,440
stationary model when you're dealing with text

1785
01:56:35,450 --> 01:56:38,740
may or may not be a very good idea

1786
01:56:38,750 --> 01:56:40,760
but to make our life easier

1787
01:56:40,780 --> 01:56:44,350
and to avoid spilling equations all over that slide

1788
01:56:44,400 --> 01:56:46,060
which is going to make that assumption

1789
01:56:48,380 --> 01:56:50,380
g of data given x

1790
01:56:50,430 --> 01:56:51,830
is this big

1791
01:56:51,840 --> 01:56:53,490
mister down here

1792
01:56:54,730 --> 01:56:57,500
so when we had almost the same expression yesterday

1793
01:56:57,520 --> 01:57:01,590
just the ones were missing and they were just axis here

1794
01:57:04,050 --> 01:57:05,990
again well i can write this

1795
01:57:06,090 --> 01:57:07,510
this is just some

1796
01:57:07,520 --> 01:57:11,440
number which depends on what in what he plus one for the sake of simplicity

1797
01:57:11,440 --> 01:57:16,570
let's assume that those wise just see one valued random variables

1798
01:57:16,580 --> 01:57:18,700
so i just need to store

1799
01:57:18,710 --> 01:57:21,370
four numbers for each of those terms

1800
01:57:22,560 --> 01:57:27,350
i want to some of the first of all of this

1801
01:57:27,400 --> 01:57:28,240
and just

1802
01:57:28,260 --> 01:57:29,180
as yesterday

1803
01:57:29,410 --> 01:57:32,830
i can push that summation in

1804
01:57:32,880 --> 01:57:35,750
actually if you look closely you'll see that's like

1805
01:57:35,800 --> 01:57:39,240
one very long multiplication of matrices

1806
01:57:39,250 --> 01:57:41,000
so that the two by two matrix

1807
01:57:41,010 --> 01:57:44,430
you multiply by the next time to the next one extended in

1808
01:57:44,460 --> 01:57:45,640
and so on

1809
01:57:45,650 --> 01:57:48,250
in the end you just summarily entries

1810
01:57:48,540 --> 01:57:51,520
that's really what's written there

1811
01:57:53,320 --> 01:57:54,120
and then

1812
01:57:54,170 --> 01:57:56,490
we just use dynamic programming or

1813
01:57:56,540 --> 01:58:02,980
in this specific case you would be much better off just using matrix multiplication

1814
01:58:04,140 --> 01:58:07,530
and then you get your data connection

1815
01:58:07,620 --> 01:58:09,310
likewise we can do

1816
01:58:09,360 --> 01:58:10,730
all those

1817
01:58:10,740 --> 01:58:13,600
partial sums by the forward backward algorithm

1818
01:58:13,720 --> 01:58:16,390
the same pictures yesterday

1819
01:58:16,400 --> 01:58:18,560
just did not operate on the wise

1820
01:58:18,610 --> 01:58:21,300
yesterday were or are operating on the axes

1821
01:58:21,350 --> 01:58:25,610
so there's no real conceptual difference here

1822
01:58:27,170 --> 01:58:30,350
the questions at this point because i mean what are they doing now is just

1823
01:58:30,350 --> 01:58:38,300
reviewing procedures we had yesterday and show you that the very same tricks apply today

1824
01:58:38,310 --> 01:58:42,680
everything here good

1825
01:58:43,730 --> 01:58:45,540
in the can do

1826
01:58:45,550 --> 01:58:47,640
message passing and all that now

1827
01:58:47,650 --> 01:58:49,340
the only thing that

1828
01:58:49,500 --> 01:58:53,040
change is a little bit is the objective function

1829
01:58:54,140 --> 01:58:59,390
yesterday we didn't have to y here and those you they just didn't economics

1830
01:59:00,680 --> 01:59:03,650
now we need to minimise that's OK so

1831
01:59:03,660 --> 01:59:07,320
we need to get the first derivative to banish first derivative of this is

1832
01:59:07,400 --> 01:59:10,950
you seem to just minus for fixed on y i

1833
01:59:10,960 --> 01:59:12,900
some are all of those

1834
01:59:13,860 --> 01:59:16,400
these are the conditional expectations

1835
01:59:16,450 --> 01:59:18,590
that's all comes from the log partition function

1836
01:59:18,610 --> 01:59:22,660
so in other words its expected value of the joint feature map here

1837
01:59:22,710 --> 01:59:26,660
given that particular sequence that i want to detect

1838
01:59:26,670 --> 01:59:29,500
and then this is just a small to keep status

1839
01:59:29,590 --> 01:59:33,410
this is just the regularizer the keep status more

1840
01:59:34,220 --> 01:59:36,840
now how to compute those things

1841
01:59:36,850 --> 01:59:41,100
after all that some joint feature map and what

1842
01:59:41,140 --> 01:59:44,880
you know i mean it's not easy to compute

1843
01:59:44,890 --> 01:59:50,260
but what we know is we know the conditional probabilities p of why i

1844
01:59:50,300 --> 01:59:52,330
because excise already known

1845
01:59:52,420 --> 01:59:55,420
given the entire sequence

1846
01:59:56,140 --> 01:59:57,650
so we have to do is

1847
01:59:57,670 --> 02:00:01,210
in the forward backward algorithm

1848
02:00:01,220 --> 02:00:05,680
just get those individual probabilities here

1849
02:00:05,690 --> 02:00:07,780
and then we apply those in

1850
02:00:07,830 --> 02:00:10,010
and come can compute expectation expectations

1851
02:00:10,020 --> 02:00:13,630
let me write it out in detail

1852
02:00:22,700 --> 02:00:25,540
i want to compute expected value

1853
02:00:32,290 --> 02:00:34,260
and why i

1854
02:00:34,280 --> 02:00:35,710
why i plus one

1855
02:00:35,760 --> 02:00:38,070
so with some slight abuse of notation

1856
02:00:38,090 --> 02:00:40,300
i'm dropping

1857
02:00:40,340 --> 02:00:42,170
the summation here

1858
02:00:42,180 --> 02:00:43,870
and and i will now go

1859
02:00:43,880 --> 02:00:46,580
all the points in the sequence

1860
02:00:49,080 --> 02:00:51,730
i want to get this

1861
02:00:51,780 --> 02:00:54,300
expectation given

1862
02:00:54,330 --> 02:00:57,710
sequence x

1863
02:00:57,730 --> 02:00:59,800
well this is nothing else

1864
02:00:59,820 --> 02:01:01,520
but the sun

