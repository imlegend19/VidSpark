1
00:00:00,000 --> 00:00:02,430
as sort of a is an intermediate step two

2
00:00:02,440 --> 00:00:04,260
doing then something with this

3
00:00:04,690 --> 00:00:08,440
understanding so for instance your data points

4
00:00:08,470 --> 00:00:11,980
the plane or in high dimensional space and the clusters

5
00:00:12,960 --> 00:00:15,630
and why is he had these clusters

6
00:00:15,640 --> 00:00:18,320
and you want either input maybe

7
00:00:18,400 --> 00:00:22,760
where the clusters of the number of clusters shape classes

8
00:00:24,040 --> 00:00:26,160
related density estimation

9
00:00:26,200 --> 00:00:30,390
OK and then this reinforcement learning which is in between supervised and unsupervised learning

10
00:00:31,000 --> 00:00:35,900
it's usually used in the agent framework an agent like a robot

11
00:00:35,910 --> 00:00:39,820
ect and observe and occasionally gets reward for his

12
00:00:41,440 --> 00:00:42,690
which means

13
00:00:42,790 --> 00:00:46,550
positive remark if it does something good in the negative remarks punishment if it does

14
00:00:46,550 --> 00:00:51,650
something that robert walking around falling down the staircase is negatively what kind of power

15
00:00:51,650 --> 00:00:56,410
block getting which positive or if you play games if you win the game with

16
00:00:56,410 --> 00:00:59,620
eighteen wins game gets rewarded game to get

17
00:00:59,670 --> 00:01:03,280
punish and from this very scars feedback

18
00:01:03,290 --> 00:01:05,050
they should learn and

19
00:01:05,060 --> 00:01:07,520
the field of reinforcement

20
00:01:07,560 --> 00:01:09,070
and the others

21
00:01:09,100 --> 00:01:11,870
which is sort of in between senior supervised learning

22
00:01:12,280 --> 00:01:14,380
quite fashionable in recent days

23
00:01:14,390 --> 00:01:20,370
something in between supervised and unsupervised learning part of the data is labelled active learning

24
00:01:20,370 --> 00:01:24,780
sort of a limited form of reinforcement learning

25
00:01:25,640 --> 00:01:27,590
what is supervised learning

26
00:01:29,900 --> 00:01:32,810
you're in for a rule in newly is

27
00:01:32,880 --> 00:01:35,520
use this rule for

28
00:01:35,530 --> 00:01:37,690
predicting future output

29
00:01:37,780 --> 00:01:39,100
from inputs

30
00:01:39,510 --> 00:01:43,890
or you can use it for knowledge extraction to adjust to understand what is going

31
00:01:44,970 --> 00:01:49,700
or for compressing the data

32
00:01:49,810 --> 00:01:53,130
or outlier detection other application

33
00:01:53,140 --> 00:01:53,910
so here

34
00:01:53,930 --> 00:01:55,070
so i said

35
00:01:55,130 --> 00:01:59,270
supervised learning is see the classification or regression that's roughly true and he a classification

36
00:02:02,060 --> 00:02:07,640
another credit card problem your new customer and you want to evaluate them

37
00:02:07,640 --> 00:02:12,280
where the customers rolls having a credit card which type of credit card and you

38
00:02:12,280 --> 00:02:13,610
have to

39
00:02:13,650 --> 00:02:16,270
point of information given in common

40
00:02:16,290 --> 00:02:18,750
and every savings of the customer

41
00:02:19,950 --> 00:02:23,520
if the customer has a high income and a lot of savings this it's a

42
00:02:23,520 --> 00:02:25,860
low-risk customers

43
00:02:26,250 --> 00:02:29,560
but if you know the income is lower the savings are lowering of the even

44
00:02:29,590 --> 00:02:34,320
are all negative side then it's a high-risk customers

45
00:02:34,350 --> 00:02:38,910
and you could classify so you have some data points here from past experience these

46
00:02:40,400 --> 00:02:44,260
o would grow through the credit card in these customers were fine

47
00:02:44,300 --> 00:02:45,860
and then you could

48
00:02:45,900 --> 00:02:49,630
think of OK maybe this is the boundary here

49
00:02:49,640 --> 00:02:54,550
which classifies them between low risk and high risk and in this case the discriminant

50
00:02:54,550 --> 00:02:59,540
with differences if income is high and savings high and low risk high risk

51
00:02:59,560 --> 00:03:01,700
so that's the classification who

52
00:03:04,250 --> 00:03:06,290
and regression is

53
00:03:06,300 --> 00:03:09,160
they have some input

54
00:03:10,360 --> 00:03:11,690
for instance the age

55
00:03:11,750 --> 00:03:13,640
of a house

56
00:03:14,860 --> 00:03:17,240
he is the average price of this house

57
00:03:17,260 --> 00:03:19,400
they in a given suburb and you want to

58
00:03:19,510 --> 00:03:22,380
now we have a house with this age have no data point and you want

59
00:03:23,740 --> 00:03:26,870
determine what is a reasonable price

60
00:03:26,950 --> 00:03:28,680
so here's data points

61
00:03:29,140 --> 00:03:33,860
they look like maybe they're lying on straight line so you make a straight line

62
00:03:33,860 --> 00:03:36,140
regression which minimizes some error

63
00:03:36,140 --> 00:03:37,270
and then you can

64
00:03:37,390 --> 00:03:38,770
determine the price for

65
00:03:38,880 --> 00:03:40,220
so this ge

66
00:03:40,400 --> 00:03:43,700
both are class both supervised learning problems

67
00:03:45,140 --> 00:03:46,820
OK unsupervised

68
00:03:47,340 --> 00:03:49,240
we have no output

69
00:03:49,860 --> 00:03:53,380
i want to learn what normally happens

70
00:03:53,610 --> 00:03:55,520
for instance clustering

71
00:03:56,760 --> 00:03:59,030
i have an example later

72
00:04:00,770 --> 00:04:03,700
OK so in reinforcement learning

73
00:04:03,780 --> 00:04:05,140
as already mentioned

74
00:04:06,490 --> 00:04:11,880
i want to learn a policy of an agent which acts with environment

75
00:04:11,920 --> 00:04:13,220
so you want to

76
00:04:13,230 --> 00:04:18,350
determine a sequence of outputs which is good in the sense of maximizing reward

77
00:04:18,400 --> 00:04:20,720
if no supervisor which tells

78
00:04:20,730 --> 00:04:22,500
the agent

79
00:04:23,020 --> 00:04:24,740
what to do

80
00:04:24,770 --> 00:04:28,190
what would have been the right action but only the action is

81
00:04:28,790 --> 00:04:33,440
good or bad and also the reward can be delayed and that makes a huge

82
00:04:33,440 --> 00:04:35,470
difference in the design

83
00:04:35,670 --> 00:04:38,270
i rhythms and analysis of the problem

84
00:04:38,340 --> 00:04:41,050
because you have the credit assignment problem

85
00:04:43,860 --> 00:04:49,860
should be above comes later and you run into the exploration versus exploitation problem

86
00:04:50,620 --> 00:04:53,170
they can always has to consider should

87
00:04:54,440 --> 00:04:58,950
what i know and maximize greedily my reward in the next time instant

88
00:04:59,180 --> 00:05:02,470
should the learner a little bit more

89
00:05:02,520 --> 00:05:04,270
walk around

90
00:05:04,310 --> 00:05:06,390
i explores some possibilities

91
00:05:07,570 --> 00:05:09,040
if the agent knows

92
00:05:09,050 --> 00:05:10,690
this doesn't need to

93
00:05:10,750 --> 00:05:13,090
good reverse immediately but

94
00:05:13,110 --> 00:05:14,740
learning means that

95
00:05:14,750 --> 00:05:16,920
may lead to higher revenue in the long run

96
00:05:16,930 --> 00:05:18,340
i mean you all sitting here

97
00:05:18,350 --> 00:05:19,980
learning something

98
00:05:19,990 --> 00:05:23,690
some of you probably in the hope of getting better which later good jobs

99
00:05:23,750 --> 00:05:25,610
learning some more intense

100
00:05:25,620 --> 00:05:27,470
catching up with

101
00:05:27,630 --> 00:05:30,510
most years quotation

102
00:05:30,520 --> 00:05:41,860
i mean some of you learn because they have found in learning but

103
00:05:49,860 --> 00:05:53,750
it's quite interesting i mean why do children like to play they like to play

104
00:05:53,750 --> 00:05:58,680
because they to build it in a good thing because it maximizes long-term reward and

105
00:05:59,780 --> 00:06:01,650
most of you would like to learn

106
00:06:01,670 --> 00:06:02,890
because of

107
00:06:02,900 --> 00:06:07,350
genetic or biological reasons because it's good for survival

108
00:06:07,440 --> 00:06:11,130
or in former times go for survival in the long run and nowadays it's maybe

109
00:06:11,140 --> 00:06:13,420
could force could celery

110
00:06:13,500 --> 00:06:17,480
OK so so some more

111
00:06:17,860 --> 00:06:21,140
ways you can classify machine learning

112
00:06:21,970 --> 00:06:30,450
OK with the slide out of all the slice of the blue and black has

113
00:06:30,450 --> 00:06:31,450
no meaning

114
00:06:33,630 --> 00:06:40,100
OK the blue means that the scope the typical scope of my lecture

115
00:06:40,140 --> 00:06:43,420
my research and not

116
00:06:43,430 --> 00:06:44,920
of this one here

117
00:06:44,930 --> 00:06:46,320
OK so

118
00:06:46,340 --> 00:06:49,660
there's the

119
00:06:49,670 --> 00:06:57,900
the classification between the statistical and machine learning approach to intelligent data analysis and sort

120
00:06:57,900 --> 00:07:02,300
of the good old fashioned by isoco finding good old-fashioned AI i

121
00:07:02,430 --> 00:07:07,790
based approach which is more knowledge-based logic orientated inferring

122
00:07:07,800 --> 00:07:10,140
classic rules and so on

123
00:07:10,140 --> 00:07:11,900
is not zero

124
00:07:11,910 --> 00:07:13,340
so let's have this charge

125
00:07:14,690 --> 00:07:17,050
that move in this direction

126
00:07:17,110 --> 00:07:18,100
and so

127
00:07:18,110 --> 00:07:20,000
there is no current

128
00:07:20,030 --> 00:07:23,050
that is angle theta between them

129
00:07:23,130 --> 00:07:26,140
they are going to be important because it's across product

130
00:07:26,190 --> 00:07:27,950
between velocity and b

131
00:07:27,960 --> 00:07:32,790
these the sign of the data comes in later

132
00:07:32,840 --> 00:07:34,020
you will say

133
00:07:34,020 --> 00:07:35,520
i hope you'll say

134
00:07:35,530 --> 00:07:37,930
well listen then this is ridiculous

135
00:07:37,940 --> 00:07:41,050
the positive charges don't move through wires

136
00:07:41,070 --> 00:07:42,570
it the electrons

137
00:07:42,580 --> 00:07:44,570
the move one vein

138
00:07:44,660 --> 00:07:46,440
responsible for the current

139
00:07:46,440 --> 00:07:48,860
and electrons have negative charge

140
00:07:48,880 --> 00:07:50,880
and they go in this direction

141
00:07:52,480 --> 00:07:53,840
perfectly fine

142
00:07:54,850 --> 00:07:57,770
the negative charge going in this direction

143
00:07:57,840 --> 00:08:02,210
it is mathematically exactly the same as the positive charge going in that direction

144
00:08:02,210 --> 00:08:04,060
in both cases do we agree

145
00:08:04,080 --> 00:08:06,050
the current is in this direction

146
00:08:06,050 --> 00:08:07,720
i have preferred

147
00:08:07,720 --> 00:08:09,360
for mathematical reasons

148
00:08:09,470 --> 00:08:11,340
take a plus b q charge

149
00:08:11,390 --> 00:08:12,860
going in this direction

150
00:08:12,870 --> 00:08:19,250
rather than thinking mind is dq chart that goes with this philosophy that

151
00:08:19,250 --> 00:08:20,450
there is no

152
00:08:20,460 --> 00:08:21,730
difference at all

153
00:08:21,740 --> 00:08:23,500
in the outcome

154
00:08:23,550 --> 00:08:24,790
as you will see

155
00:08:24,800 --> 00:08:26,340
so in this charge

156
00:08:26,350 --> 00:08:27,730
there is a force

157
00:08:32,180 --> 00:08:35,280
this magnetic force

158
00:08:35,290 --> 00:08:37,740
and that is the charge

159
00:08:39,230 --> 00:08:40,290
that equation

160
00:08:40,300 --> 00:08:42,610
thanks me

161
00:08:43,520 --> 00:08:45,410
the ones that drift velocity

162
00:08:46,330 --> 00:08:47,210
and here

163
00:08:47,300 --> 00:08:52,280
is the magnetic field at this location

164
00:08:52,330 --> 00:08:54,720
the current through the wire

165
00:08:54,760 --> 00:08:57,510
everywhere the wire must dq dt

166
00:08:57,600 --> 00:09:01,140
because that's the definition of current

167
00:09:01,190 --> 00:09:03,580
how many calls per second

168
00:09:03,600 --> 00:09:05,720
because there's always security

169
00:09:05,760 --> 00:09:07,940
so i can also write this

170
00:09:07,980 --> 00:09:09,870
as i dt

171
00:09:15,290 --> 00:09:21,880
but i remember a to one

172
00:09:25,300 --> 00:09:27,950
that is p times a

173
00:09:27,990 --> 00:09:30,040
time is the distance

174
00:09:30,050 --> 00:09:31,980
and i call the distance

175
00:09:33,730 --> 00:09:36,490
it's the distance along the wire

176
00:09:36,540 --> 00:09:41,590
put the distance in here now because i don't want to clutter up my

177
00:09:41,740 --> 00:09:44,400
my drawing

178
00:09:44,440 --> 00:09:46,540
so this charge in time

179
00:09:46,550 --> 00:09:47,870
the team

180
00:09:47,900 --> 00:09:51,120
moved over that distance vector

181
00:09:51,130 --> 00:09:52,480
a one

182
00:09:52,490 --> 00:09:54,290
so i can write down for this

183
00:09:54,300 --> 00:09:56,960
product i can write down dl

184
00:09:57,000 --> 00:09:59,770
so i can also write down that

185
00:09:59,790 --> 00:10:01,330
the f of the

186
00:10:14,470 --> 00:10:18,210
well this is telling you

187
00:10:18,210 --> 00:10:20,340
this is the force

188
00:10:21,840 --> 00:10:26,070
over a small segment of the y which has remained dl

189
00:10:26,140 --> 00:10:27,800
i is the current through wire

190
00:10:27,810 --> 00:10:33,760
and b is the local magnetic field at that location the that's what it means

191
00:10:33,820 --> 00:10:36,700
and if you want to know the entire force on the wire

192
00:10:36,780 --> 00:10:40,370
you have to do the integral along the whole

193
00:10:40,430 --> 00:10:42,050
so you have to

194
00:10:42,080 --> 00:10:43,540
one integral

195
00:10:43,540 --> 00:10:47,430
along the entire wire and every portion vl after the

196
00:10:47,440 --> 00:10:48,590
what this

197
00:10:48,610 --> 00:10:50,050
and you get them

198
00:10:50,050 --> 00:10:51,040
a force

199
00:10:51,040 --> 00:10:54,510
which is the vector you have to have those factors victoria

200
00:10:54,560 --> 00:10:56,190
could be paying next

201
00:10:56,230 --> 00:11:01,770
but that's the basic idea

202
00:11:03,590 --> 00:11:06,510
i want to calculate what the force was

203
00:11:06,570 --> 00:11:08,470
on this wire roughly

204
00:11:08,520 --> 00:11:18,060
when we ran three hundred and PD through their

205
00:11:18,120 --> 00:11:22,680
and i make geometry so simple that we can execute that integral

206
00:11:23,000 --> 00:11:24,770
the wire

207
00:11:24,820 --> 00:11:26,890
we had current running through you

208
00:11:26,900 --> 00:11:30,590
which was three hundred and PD is roughly

209
00:11:30,600 --> 00:11:33,570
and we have a magnetic field

210
00:11:33,620 --> 00:11:36,290
it was right in the gap there

211
00:11:36,300 --> 00:11:38,020
the magnetic fields

212
00:11:41,090 --> 00:11:42,880
and that was

213
00:11:42,950 --> 00:11:44,580
ten of the first one

214
00:11:48,450 --> 00:11:51,750
that magnetic field was only operating here

215
00:11:51,760 --> 00:11:53,820
it wasn't operating there

216
00:11:53,820 --> 00:11:56,800
where the target will be the solution and you know the truth is a special

217
00:11:56,800 --> 00:12:01,010
case of junction tree which can be efficiently implemented is arithmetic circuits

218
00:12:01,030 --> 00:12:02,860
that's what happened

219
00:12:02,880 --> 00:12:07,170
so the language features actually dictate what's in the langevin solved

220
00:12:07,210 --> 00:12:11,960
that should actually be happen invisible to the user the destination so gets picked if

221
00:12:11,960 --> 00:12:15,510
you want to say no i really want to use algorithm for this second set

222
00:12:15,510 --> 00:12:16,840
of parameters to do that

223
00:12:16,880 --> 00:12:19,010
but representing things in this way

224
00:12:19,030 --> 00:12:22,460
does not imply that with maxwell into the

225
00:12:22,480 --> 00:12:25,670
max maxwalksat happens to be the thing that we have implemented a

226
00:12:25,730 --> 00:12:29,400
we actually have something else which is a graph cut algorithm which are all these

227
00:12:29,400 --> 00:12:34,550
problems is enough we have an incompetent or can one incorporate is a general purpose

228
00:12:34,550 --> 00:12:36,880
exact inference of

229
00:12:36,880 --> 00:12:41,360
so that's that's a very good question and you know it's important to realize that

230
00:12:41,360 --> 00:12:44,510
all i'm talking about here is the language in which you can represent things and

231
00:12:44,510 --> 00:12:46,590
you can write complex models in this way

232
00:12:46,840 --> 00:12:50,650
do inference that depends on the organs have under the hood

233
00:12:50,670 --> 00:12:51,460
OK so

234
00:12:51,480 --> 00:12:54,000
let's look at an even more ambitious examples

235
00:12:54,010 --> 00:12:57,030
let's try to do information extraction

236
00:12:57,030 --> 00:13:01,900
so information extraction is of course very important problem these days is the problem of

237
00:13:01,900 --> 00:13:04,270
extracting database from text

238
00:13:04,300 --> 00:13:10,030
also some sources like web pages or XML or spreadsheet and whatnot

239
00:13:10,030 --> 00:13:13,490
and of course the canonical example of this is that it have a lot of

240
00:13:13,490 --> 00:13:19,300
research is expecting editors of publications from citation lists the citeseer problem not in the

241
00:13:19,300 --> 00:13:25,570
internet example that i gave before i assumed that the data was already pre segmented

242
00:13:25,570 --> 00:13:30,170
into database records and all i need to do is figure out which records correspond

243
00:13:30,190 --> 00:13:31,610
to the same paper

244
00:13:31,630 --> 00:13:35,230
here i'm going to go all the way to start with the raw text as

245
00:13:35,230 --> 00:13:36,130
the input

246
00:13:36,150 --> 00:13:39,690
and i'm going to do that by combining two of these models that we just

247
00:13:39,690 --> 00:13:41,960
saw before so one of the

248
00:13:42,010 --> 00:13:44,170
it's going to be into resolutions

249
00:13:44,170 --> 00:13:46,360
the other thing that we need to do segmentation

250
00:13:46,440 --> 00:13:49,530
we need to go to the input string of each citation and figure out what

251
00:13:49,530 --> 00:13:52,480
is the author whereas the title was the venue

252
00:13:52,500 --> 00:13:54,860
and for that we can use an HMM

253
00:13:54,880 --> 00:13:59,050
or more precisely than is the CRF because it's going to be trained discriminatively but

254
00:13:59,050 --> 00:14:03,650
this is a typical people police for segmentation is to use an HMM or CRF

255
00:14:03,670 --> 00:14:09,530
to label each position is being you know title author venue or whatever feels interested

256
00:14:10,030 --> 00:14:14,590
it could be you know city person organization et cetera

257
00:14:14,590 --> 00:14:16,480
so what can we do this

258
00:14:16,500 --> 00:14:19,670
well here's the complete ML

259
00:14:19,690 --> 00:14:21,980
that does information extraction for you

260
00:14:22,000 --> 00:14:23,300
it contains

261
00:14:23,320 --> 00:14:26,730
four predicates and seventy four

262
00:14:26,730 --> 00:14:30,260
i should think about it this is quite remarkable because if you want to wrote

263
00:14:30,260 --> 00:14:35,650
in information extraction system you know using java c plus plus it probably wanted to

264
00:14:35,670 --> 00:14:39,840
thousands tens of thousands of lines of code and politic you weeks to do

265
00:14:39,860 --> 00:14:43,880
in alchemy you know just need to write these seven short formulas that offered on

266
00:14:43,880 --> 00:14:44,840
this slide

267
00:14:44,860 --> 00:14:47,320
so what are the formalism as predicates well

268
00:14:47,340 --> 00:14:52,250
all i did was go together the HMM and into resolution approach that we saw

269
00:14:52,250 --> 00:14:55,590
before so we have to come now

270
00:14:55,590 --> 00:14:59,360
it's going to be predicted good position and the citation is argument so this is

271
00:14:59,420 --> 00:15:01,960
this just this is going to be the evidence

272
00:15:01,980 --> 00:15:06,030
the input it just says what's the kind of each position in the input

273
00:15:06,050 --> 00:15:08,130
and then in the field

274
00:15:08,150 --> 00:15:11,820
so that this position is in this field in the citations so this is what's

275
00:15:11,820 --> 00:15:13,800
going do the segmentation

276
00:15:13,800 --> 00:15:18,210
so this could say that position one in citation two is in field

277
00:15:18,230 --> 00:15:23,210
and then same year the same citation are into resolution predicates that we saw before

278
00:15:24,440 --> 00:15:25,070
and now

279
00:15:25,090 --> 00:15:28,210
here's the HMM segmentation it says

280
00:15:28,230 --> 00:15:30,980
here's the observation matrix if you will it says there

281
00:15:30,980 --> 00:15:32,070
this talk

282
00:15:32,110 --> 00:15:35,710
is indicative of this you so this is going to learn the correlation matrix between

283
00:15:35,710 --> 00:15:41,860
tokens and fields like for example conferences very indicative of a new so is journal

284
00:15:41,870 --> 00:15:44,450
if if positions

285
00:15:44,490 --> 00:15:48,330
i is in the field after them so its position i plus one

286
00:15:48,340 --> 00:15:52,040
notice that here i don't have time anymore

287
00:15:52,040 --> 00:15:54,770
because for four

288
00:15:54,780 --> 00:16:00,130
for segmentation it's actually not that important to model the complete transition matrix between all

289
00:16:00,130 --> 00:16:04,270
feel types all you really want to model is the phenomenon that you tend to

290
00:16:04,270 --> 00:16:07,530
stay in the same feel time if and the title you probably going to be

291
00:16:07,530 --> 00:16:10,950
in the title for a few more tokens before he switched to see that

292
00:16:11,000 --> 00:16:15,050
so to do that you just leave the formula in this form you learn fewer

293
00:16:15,050 --> 00:16:19,820
parameters and naturally you know generalize better because there's this true parameters to learn so

294
00:16:19,820 --> 00:16:22,330
that's what i'm doing here

295
00:16:22,480 --> 00:16:26,490
and and here's another subtlety

296
00:16:26,510 --> 00:16:27,940
in general

297
00:16:27,950 --> 00:16:31,690
the position when he might not be any of the fields and the way people

298
00:16:31,690 --> 00:16:35,620
typically do this is by introducing this pseudo field called the

299
00:16:35,670 --> 00:16:39,770
but the is is because now you the correlation between various things another and of

300
00:16:39,770 --> 00:16:43,450
course it's a list of a bunch of things a lot of very well anyone

301
00:16:43,460 --> 00:16:45,620
something quite nice well here

302
00:16:45,660 --> 00:16:49,580
we we can allow we don't need to explicitly introduced another type all we need

303
00:16:49,580 --> 00:16:54,770
to do is relax the restrictions that the skills be mutually exclusive and exhaustive

304
00:16:54,770 --> 00:16:58,420
let me thank you for this wonderful talk

305
00:16:58,440 --> 00:17:05,040
in professor chomsky here is that the problem is provided in shows the monash elbassuoni

306
00:17:05,060 --> 00:17:06,190
the largo b

307
00:17:06,210 --> 00:17:11,770
so that is the economic another chance at what i felt pressure and you not

308
00:17:11,770 --> 00:17:14,080
be similarly lists alibi set

309
00:17:14,100 --> 00:17:17,890
it's the easter island usually you publish on started

310
00:17:18,010 --> 00:17:23,580
studied this enzyme microphone you need so that familiar that could go to the museum

311
00:17:23,580 --> 00:17:25,390
of critical reviews per channel

312
00:17:25,410 --> 00:17:29,340
you my colleague of so much then capability there

313
00:17:30,090 --> 00:17:32,480
not be solid mechanics in

314
00:17:32,490 --> 00:17:36,380
this this a good evening is actually is needed to separate the process so that

315
00:17:36,470 --> 00:17:37,750
is not shown here

316
00:17:37,760 --> 00:17:40,070
it's all to the as was this

317
00:17:40,080 --> 00:17:42,240
it was among the started to

318
00:17:42,250 --> 00:17:50,660
but it still needs saying that within so that only as the microphone

319
00:17:50,680 --> 00:17:57,650
the question is going to be

320
00:17:57,670 --> 00:18:01,430
the reason i really

321
00:18:03,510 --> 00:18:05,940
the only thing preventing

322
00:18:05,960 --> 00:18:08,500
the cook blood pressure in in the sentence

323
00:18:12,830 --> 00:18:14,920
this one

324
00:18:14,930 --> 00:18:17,120
OK you liberal

325
00:18:17,140 --> 00:18:21,640
professor chomsky

326
00:18:21,650 --> 00:18:23,330
you may not be aware

327
00:18:23,350 --> 00:18:24,640
that we have

328
00:18:24,650 --> 00:18:27,330
two US embassies

329
00:18:27,340 --> 00:18:28,960
in slovenia

330
00:18:28,980 --> 00:18:32,930
one of the first one in slovenia

331
00:18:32,970 --> 00:18:36,940
it is located at the very centre of ljubljana

332
00:18:36,950 --> 00:18:44,670
with an official figure in the person of US ambassador why other american embassy quote

333
00:18:47,410 --> 00:18:49,320
books are because

334
00:18:50,440 --> 00:18:53,350
all over slovenia is in the society

335
00:18:53,380 --> 00:18:55,980
libraries and private sessions

336
00:18:56,040 --> 00:18:58,570
of american authors like you know

337
00:18:59,250 --> 00:19:00,910
it's a

338
00:19:00,920 --> 00:19:02,580
citizens on that

339
00:19:02,600 --> 00:19:05,550
norman mailer and many others

340
00:19:05,610 --> 00:19:07,570
which of these

341
00:19:07,610 --> 00:19:11,060
representations of america is chomsky

342
00:19:12,920 --> 00:19:15,740
and indeed the representative of the base

343
00:19:17,850 --> 00:19:23,810
well actually

344
00:19:23,820 --> 00:19:29,740
the first comment until i heard the rest reminded me of

345
00:19:29,760 --> 00:19:32,510
something that i was told by an old

346
00:19:32,530 --> 00:19:36,330
friends and close friend who was the

347
00:19:38,030 --> 00:19:40,810
middle east correspondent for

348
00:19:42,870 --> 00:19:47,800
television in the united states you've probably seen him on television

349
00:19:47,810 --> 00:19:52,300
he once told me that ABC had three

350
00:19:52,310 --> 00:19:54,800
offices in

351
00:19:56,160 --> 00:19:58,220
one was in japan

352
00:19:58,240 --> 00:20:06,800
one was in tel aviv and one was in jerusalem address the issue of the

353
00:20:06,820 --> 00:20:09,130
as for the two american way

354
00:20:09,190 --> 00:20:14,280
this were you know that's really something for you to decide that

355
00:20:14,290 --> 00:20:18,170
if you look closely you'll find

356
00:20:20,080 --> 00:20:26,770
documented the distinction that a briefly mentioned

357
00:20:26,780 --> 00:20:30,330
mainly there's a very sharp divide

358
00:20:31,760 --> 00:20:34,000
american public opinion

359
00:20:34,010 --> 00:20:34,950
and the

360
00:20:35,430 --> 00:20:37,230
consensus of the two

361
00:20:37,240 --> 00:20:38,590
political parties

362
00:20:38,610 --> 00:20:40,120
the media

363
00:20:40,170 --> 00:20:44,210
and the intellectual classes quite generally

364
00:20:44,260 --> 00:20:47,020
on issue after issue

365
00:20:47,030 --> 00:20:48,470
i mentioned if few

366
00:20:48,490 --> 00:20:51,620
major issues on which there are sharply divided

367
00:20:51,640 --> 00:20:54,810
but it extends quite generally

368
00:20:54,830 --> 00:20:57,370
just to and it's not reported

369
00:20:57,670 --> 00:21:02,620
i mentioned that these major studies were not reported right just prior to to the

370
00:21:02,630 --> 00:21:06,710
us election which illustrates i think the recognition

371
00:21:06,760 --> 00:21:11,180
on the part of editors that it wouldn't be a good idea to let people

372
00:21:12,090 --> 00:21:15,190
so that others share their opinions

373
00:21:15,210 --> 00:21:22,280
people should feel isolated and helpless and there's plenty of more evidence about this

374
00:21:22,300 --> 00:21:27,400
for example the federal budget government budget was announced

375
00:21:27,420 --> 00:21:31,190
in february months ago

376
00:21:31,200 --> 00:21:33,740
and shortly after it

377
00:21:33,760 --> 00:21:41,940
the most important institution in the world that monitors public opinion and academic based in

378
00:21:42,000 --> 00:21:47,690
tution in the united states carried out an in-depth study of

379
00:21:47,710 --> 00:21:52,490
public attitudes towards what the budget ought to be

380
00:21:52,510 --> 00:21:59,110
and it turns out that attitudes are diametrically opposed to the budget just about every

381
00:22:00,390 --> 00:22:03,190
so large majority of the population

382
00:22:03,250 --> 00:22:10,140
i felt that there should be cut back in military spending huge substantial increase in

383
00:22:10,140 --> 00:22:11,740
social spending

384
00:22:11,750 --> 00:22:16,600
more spending on support for the united nations

385
00:22:16,720 --> 00:22:19,120
more spending on foreign aid

386
00:22:19,540 --> 00:22:21,580
continues like this

387
00:22:21,600 --> 00:22:27,370
exactly the opposite of the budget that was approved by congress in just about every

388
00:22:27,370 --> 00:22:28,920
major respect

389
00:22:28,970 --> 00:22:31,380
well which is the real america

390
00:22:31,400 --> 00:22:38,820
the complicated story let's take for example the war in iraq invasion of iraq as

391
00:22:38,820 --> 00:22:44,160
in this example to illustrate the complexity of answering the question

392
00:22:44,450 --> 00:22:49,090
about seventy five percent of the population

393
00:22:49,110 --> 00:22:52,470
i think it's you with enron two

394
00:22:52,490 --> 00:22:54,220
invade iran

395
00:22:55,540 --> 00:22:59,320
iraq was developing weapons of mass destruction

396
00:22:59,370 --> 00:23:02,060
to use against the united states

397
00:23:02,080 --> 00:23:06,610
and it was cooperating with al-qaida two

398
00:23:06,740 --> 00:23:09,180
organised terrorist actions

399
00:23:09,210 --> 00:23:11,650
using weapons of mass destruction

400
00:23:11,690 --> 00:23:17,070
against the united states is similar to the september eleven bombing

401
00:23:17,080 --> 00:23:19,260
so that seventy five percent

402
00:23:19,300 --> 00:23:21,270
who say the words wrong

403
00:23:22,240 --> 00:23:23,850
that was the case

404
00:23:23,910 --> 00:23:30,210
even the government has long ago conceded that there wasn't any truth to this claims

405
00:23:30,220 --> 00:23:32,960
nevertheless fifty percent of the public

406
00:23:32,970 --> 00:23:38,760
roughly still support war looks like a contradiction

407
00:23:38,770 --> 00:23:40,350
until you

408
00:23:40,360 --> 00:23:45,160
i realize that fifty percent of the public still believes this claims

409
00:23:46,090 --> 00:23:51,130
government media propaganda system is sufficiently powerful

410
00:23:51,140 --> 00:23:54,780
so the people continue to believe the the claims

411
00:23:54,780 --> 00:23:58,940
we can look at the unlabelled part of distribution to really strong

412
00:23:59,190 --> 00:24:00,630
some probability

413
00:24:00,660 --> 00:24:03,340
there exist two hypotheses in

414
00:24:03,400 --> 00:24:04,670
capital h

415
00:24:04,700 --> 00:24:09,460
which there's a disagreement

416
00:24:09,520 --> 00:24:11,410
this is the key e

417
00:24:11,450 --> 00:24:14,150
the key difference between

418
00:24:14,250 --> 00:24:17,220
this active learning and cow

419
00:24:17,280 --> 00:24:21,090
is in the definition of h

420
00:24:21,110 --> 00:24:23,180
h is going to be defined as

421
00:24:23,190 --> 00:24:24,650
this is the boxes

422
00:24:24,660 --> 00:24:26,810
you might eventually converge

423
00:24:26,850 --> 00:24:32,200
rather than set of policies which are consistent with all labels you've seen so far

424
00:24:36,000 --> 00:24:38,700
that is the sort of the key difference

425
00:24:38,820 --> 00:24:45,750
so one biggest the it's the bounds and the bound arithmetic which is going to

426
00:24:45,770 --> 00:24:49,410
control you know the number of one might eventually converge to this

427
00:25:08,860 --> 00:25:12,220
that's a very simple question

428
00:25:12,230 --> 00:25:15,830
i don't know the answer i think is is the answer

429
00:25:26,130 --> 00:25:28,610
we're not really using the line here

430
00:25:35,540 --> 00:25:39,840
the valve is going to return

431
00:25:39,860 --> 00:25:44,130
an epsilon optimal hypothesis with probability one minus delta

432
00:25:44,150 --> 00:25:48,710
and so we need to what the problem that this recursion process america's best going

433
00:25:48,710 --> 00:25:50,240
to terminate at at some point

434
00:25:50,270 --> 00:25:52,370
two new termination condition

435
00:25:52,540 --> 00:25:55,650
to termination condition is going to be that the

436
00:25:55,670 --> 00:25:57,560
the difference between

437
00:25:57,570 --> 00:26:03,680
your upper bound your small of around and small slow around in times the disagreement

438
00:26:03,690 --> 00:26:06,230
is this this person and so on

439
00:26:10,460 --> 00:26:12,600
OK so so these are the basic

440
00:26:14,230 --> 00:26:16,350
and this is the are so

441
00:26:16,380 --> 00:26:21,530
if it is inherited you want to fit in any set of classifiers

442
00:26:21,540 --> 00:26:27,080
you can check initially to see that

443
00:26:27,090 --> 00:26:29,130
you not done

444
00:26:29,230 --> 00:26:32,450
and then we have this inner loop here

445
00:26:32,460 --> 00:26:34,210
which is going to say

446
00:26:34,230 --> 00:26:36,020
given to labelled examples

447
00:26:37,030 --> 00:26:41,130
weekend decreases as the path space

448
00:26:41,160 --> 00:26:43,600
so we start with no labelled examples

449
00:26:43,620 --> 00:26:45,370
and we start with

450
00:26:45,380 --> 00:26:47,820
so we have our current hypothesis space

451
00:26:47,940 --> 00:26:52,700
we have the new one they going to create after we you're a recursion that

452
00:26:54,930 --> 00:26:55,840
we're going to

453
00:26:55,910 --> 00:26:57,440
finished the recursion step

454
00:26:57,450 --> 00:27:00,080
when the size of disagreement region

455
00:27:01,500 --> 00:27:11,850
or inequalities around way

456
00:27:11,860 --> 00:27:12,900
two what this

457
00:27:12,910 --> 00:27:15,940
and on the right way

458
00:27:16,630 --> 00:27:23,290
right so h primal will are store for the old one

459
00:27:23,310 --> 00:27:24,670
and we want to

460
00:27:24,750 --> 00:27:26,950
recurse by factor of two

461
00:27:26,950 --> 00:27:33,240
one side the disagreement region to decrease for factor of two

462
00:27:33,270 --> 00:27:34,830
OK so we have to cheque to

463
00:27:34,830 --> 00:27:40,530
it's possible we reach termination condition inside the loop so it so we are done

464
00:27:40,840 --> 00:27:43,860
if not

465
00:27:44,630 --> 00:27:47,180
we're going to double the number of

466
00:27:47,200 --> 00:27:51,490
unlabelled examples on which there is disagreement

467
00:27:51,520 --> 00:27:54,170
so we're we're going to take a bunch of unlabelled examples that could be from

468
00:27:54,170 --> 00:27:55,520
the stream from pool

469
00:27:55,610 --> 00:28:00,680
i which there's disagreement for a current about the space age

470
00:28:00,690 --> 00:28:03,860
and are going to ask label for these examples

471
00:28:03,920 --> 00:28:05,330
they're going to

472
00:28:06,330 --> 00:28:09,250
are upper bounds and lower bounds

473
00:28:10,160 --> 00:28:11,480
what what

474
00:28:11,500 --> 00:28:12,930
new reduced

475
00:28:13,750 --> 00:28:17,810
of hypotheses it's prime we have

476
00:28:17,850 --> 00:28:19,320
or keep doing this

477
00:28:20,590 --> 00:28:25,840
we managed to have the size of the disagreement region

478
00:28:27,750 --> 00:28:28,620
so this is just

479
00:28:28,630 --> 00:28:30,530
this is saying

480
00:28:30,740 --> 00:28:35,370
so the complex way because we don't know in advance how many labels

481
00:28:35,400 --> 00:28:36,430
we need

482
00:28:36,490 --> 00:28:38,730
we ask what we label enough

483
00:28:39,680 --> 00:28:41,470
on current disagreement region

484
00:28:41,490 --> 00:28:45,070
to have the size of the beam reached

485
00:28:45,080 --> 00:28:48,610
and there and then we have this recursion step here

486
00:28:49,830 --> 00:28:53,700
and at the very end after done we just returns the past this inner meaning

487
00:28:53,700 --> 00:28:55,220
ten six

488
00:29:06,950 --> 00:29:09,730
that is a good way

489
00:29:10,050 --> 00:29:15,000
that's really good

490
00:29:15,520 --> 00:29:17,340
there's always turns out

491
00:29:17,350 --> 00:29:21,420
but you're right that that's why it's not being used here

492
00:29:21,450 --> 00:29:24,430
you have questions

493
00:29:32,180 --> 00:29:35,150
so there's various things you can prove about this

494
00:29:35,200 --> 00:29:37,590
some of them are pretty straightforward

495
00:29:37,730 --> 00:29:39,410
the claim is that

496
00:29:39,430 --> 00:29:45,410
very easily passes and for any distribution including ones with arbitrary noise between the x

497
00:29:45,410 --> 00:29:46,960
and y

498
00:29:47,020 --> 00:29:50,070
for it was at someone

499
00:29:50,080 --> 00:29:53,950
with very high probability you can make this be anything you want

500
00:29:53,960 --> 00:30:01,670
even to return and have some of more classifier

501
00:30:01,710 --> 00:30:05,700
OK so the that's says that

502
00:30:06,620 --> 00:30:12,840
because in supervised learning you have learning algorithms PAC learning algorithms which have these kinds

503
00:30:12,840 --> 00:30:14,100
of guaranteed although

504
00:30:14,180 --> 00:30:20,100
so it is if you know your computation you can you can get cancer guarantees

505
00:30:20,570 --> 00:30:23,570
generic and so

506
00:30:23,580 --> 00:30:28,720
so maybe the numbers was acquired one being something like the VC dimension divided by

507
00:30:28,730 --> 00:30:30,430
epsilon square

508
00:30:30,440 --> 00:30:32,340
or something like that

509
00:30:32,350 --> 00:30:35,300
so this is the a safety guarantees

510
00:30:35,320 --> 00:30:37,570
this is another safety guarantee you can say

511
00:30:37,580 --> 00:30:38,460
for every

512
00:30:38,480 --> 00:30:40,620
the hypotheses very distribution

513
00:30:40,630 --> 00:30:44,030
the number of labelled samples is not much worse than in what you get for

514
00:30:44,030 --> 00:30:47,360
supervised learning

515
00:30:47,380 --> 00:30:50,950
and the can probably see whether that's true mean in the worst case you never

516
00:30:52,290 --> 00:30:55,950
and if you never occurs you just using the standard upper and lower bounds for

517
00:30:55,950 --> 00:30:57,330
supervised learning

518
00:30:57,380 --> 00:31:02,820
and then the termination condition is about things which you would get with supervised learning

519
00:31:02,830 --> 00:31:04,080
OK so now

520
00:31:04,090 --> 00:31:08,230
i would like to do better so the claim is

521
00:31:08,240 --> 00:31:13,350
for all hypothesis spaces for all distributions if you have disagreement coefficient of theta

522
00:31:13,370 --> 00:31:14,210
and some

523
00:31:14,220 --> 00:31:15,770
VC dimension

524
00:31:17,240 --> 00:31:19,170
now we're going to have independence

525
00:31:19,190 --> 00:31:20,630
new parameter

526
00:31:20,650 --> 00:31:23,570
which is no

527
00:31:23,570 --> 00:31:25,650
so this is this

528
00:31:25,670 --> 00:31:28,610
the minimum error hypothesis

529
00:31:28,630 --> 00:31:32,190
in you said about six

530
00:31:32,190 --> 00:31:34,370
the core the the the colors

531
00:31:34,380 --> 00:31:39,490
a walk at their own pace so to speak and then the smaller the wavelength

532
00:31:41,200 --> 00:31:45,220
close it will be to to zero order and then the spacing between first and

533
00:31:45,220 --> 00:31:49,540
second will also be smaller than in the case of the long wavelength in this

534
00:31:49,540 --> 00:31:51,260
case red

535
00:31:51,290 --> 00:31:53,060
and this is something that i also

536
00:31:53,070 --> 00:31:55,500
i want to demonstrate to you it is not so easy

537
00:31:55,520 --> 00:31:56,920
you get a very strong

538
00:31:56,930 --> 00:31:59,410
a powerful source of white light

539
00:31:59,450 --> 00:32:01,360
i'm using for this a

540
00:32:01,370 --> 00:32:04,050
a reflection grating you can also

541
00:32:04,070 --> 00:32:08,700
use gradient in reflection you take metal and groove metal on metal

542
00:32:08,720 --> 00:32:10,920
and you get a reflection which we will

543
00:32:10,960 --> 00:32:12,800
have there on the wall

544
00:32:12,810 --> 00:32:14,550
this reflection grating

545
00:32:14,560 --> 00:32:17,360
has a spacing in which is four times smaller

546
00:32:17,390 --> 00:32:19,420
then the one we have here

547
00:32:19,460 --> 00:32:21,560
it's only two and a half

548
00:32:21,640 --> 00:32:25,690
microns so the angles they don't one one the three and a half degrees but

549
00:32:25,690 --> 00:32:27,580
it will be four times larger

550
00:32:27,700 --> 00:32:31,310
the main purpose why why i want to show you this is i use white

551
00:32:31,310 --> 00:32:34,270
light that zero order is white

552
00:32:34,340 --> 00:32:38,190
and then we'll see also of course the first and the second order

553
00:32:38,200 --> 00:32:39,380
if we have

554
00:32:39,420 --> 00:32:42,700
good eyes because of the the whole thing is not so

555
00:32:42,720 --> 00:32:44,570
so very bright

556
00:32:44,630 --> 00:32:47,510
make sure that i have the

557
00:32:52,160 --> 00:32:56,120
so your eyes may have to adjust a little bit to the darkness

558
00:32:56,160 --> 00:33:00,440
if x is not overpowering because light source is not very bright

559
00:33:00,500 --> 00:33:01,440
what you see

560
00:33:02,970 --> 00:33:07,360
here's my laser pointer this is the zero to maximum

561
00:33:07,370 --> 00:33:11,390
and there are the maximum is very wide

562
00:33:11,400 --> 00:33:13,400
the reason is the divergence of the

563
00:33:13,410 --> 00:33:14,620
white light beam

564
00:33:14,630 --> 00:33:18,740
it's not this factor of and again again much less

565
00:33:18,790 --> 00:33:20,660
i can turn on also

566
00:33:20,690 --> 00:33:21,700
these are

567
00:33:21,710 --> 00:33:23,720
which i was at the same time

568
00:33:23,810 --> 00:33:25,700
you'll see that the red laser

569
00:33:26,530 --> 00:33:28,610
give its own

570
00:33:28,830 --> 00:33:31,960
then they come to the zero order of the red laser is of course also

571
00:33:31,960 --> 00:33:34,020
you all colors a year

572
00:33:34,030 --> 00:33:36,940
and then you see here

573
00:33:36,970 --> 00:33:39,310
first order of the red

574
00:33:39,320 --> 00:33:43,410
a large angle with these very small you see in the first order of the

575
00:33:43,410 --> 00:33:45,610
red second order of the red

576
00:33:45,660 --> 00:33:47,860
you see the first order of the blue

577
00:33:47,870 --> 00:33:50,740
first order of the blue you see a big difference

578
00:33:50,790 --> 00:33:52,250
between the separation

579
00:33:52,260 --> 00:33:53,200
of the red

580
00:33:53,220 --> 00:33:55,790
and the separation from the zero order

581
00:33:55,800 --> 00:33:59,420
and the blue big difference

582
00:33:59,420 --> 00:34:00,910
there's actually

583
00:34:00,930 --> 00:34:02,480
a much better way

584
00:34:02,520 --> 00:34:05,150
that i can make you see all this

585
00:34:05,150 --> 00:34:06,600
and that is

586
00:34:06,650 --> 00:34:10,560
if i ask you which i think i'm going to do now

587
00:34:10,570 --> 00:34:14,110
two usual grading this hold all the four seconds before you get your

588
00:34:14,160 --> 00:34:16,780
your on gratings out

589
00:34:16,810 --> 00:34:18,210
our equations

590
00:34:18,250 --> 00:34:20,940
that we have the right so far

591
00:34:20,990 --> 00:34:22,580
only hold

592
00:34:22,620 --> 00:34:26,920
if we look very far away

593
00:34:26,960 --> 00:34:30,880
these angles of fatah only through if you go very far away

594
00:34:30,890 --> 00:34:33,150
for reasons that we discussed last time

595
00:34:33,160 --> 00:34:36,210
because the surfaces of maxima hyperbolas

596
00:34:36,230 --> 00:34:40,990
and is only the angle theta is only an approximation if you very far away

597
00:34:41,000 --> 00:34:43,660
we can however

598
00:34:43,670 --> 00:34:46,960
do something very clever we can use the lens

599
00:34:46,970 --> 00:34:48,720
and if we have elements

600
00:34:48,770 --> 00:34:52,990
we can bring in it's very close without disturbing the angles

601
00:34:53,000 --> 00:34:55,560
this is degrading

602
00:34:55,600 --> 00:34:57,710
the number of sources that i have

603
00:34:57,770 --> 00:35:00,940
and so the light comes in this direction

604
00:35:00,990 --> 00:35:06,770
and if i put here lens

605
00:35:06,820 --> 00:35:09,160
this is the screen

606
00:35:09,210 --> 00:35:11,940
the focal point of the lens

607
00:35:11,950 --> 00:35:14,050
and if

608
00:35:14,050 --> 00:35:17,170
the angle theta

609
00:35:17,240 --> 00:35:19,160
if the angle theta

610
00:35:19,210 --> 00:35:20,330
for which

611
00:35:20,380 --> 00:35:21,900
i would have expected

612
00:35:21,950 --> 00:35:23,610
in this direction

613
00:35:23,650 --> 00:35:27,820
my first order maximum and you of course my zero or the maximum

614
00:35:27,830 --> 00:35:30,360
then the lens will not change that angle

615
00:35:30,410 --> 00:35:33,630
we never discussed lenses so it may not be so obvious to you

616
00:35:33,750 --> 00:35:36,980
the ends will always maintain integrity

617
00:35:37,020 --> 00:35:38,390
of angles

618
00:35:40,240 --> 00:35:45,090
the angle theta that we derive here is the correct angle but of course in

619
00:35:45,090 --> 00:35:46,430
terms of x

620
00:35:46,470 --> 00:35:50,470
that's enormously reduced if this distance is very small

621
00:35:50,510 --> 00:35:53,250
so when we have the option that we don't have to

622
00:35:53,260 --> 00:35:56,920
allow for very large distances like now ten meters

623
00:35:56,970 --> 00:35:59,470
and so your eye is the perfect tool

624
00:35:59,520 --> 00:36:02,270
to use for that because you have lens in your

625
00:36:02,280 --> 00:36:05,280
in UAI that's the whole idea here and so i want you to get your

626
00:36:05,280 --> 00:36:06,970
gratings out

627
00:36:06,980 --> 00:36:10,460
and i want you to hold the gratings in front of your eyes

628
00:36:10,470 --> 00:36:13,810
and manipulate the gratings a little bit

629
00:36:13,880 --> 00:36:16,030
so to you get the lines vertical

630
00:36:16,090 --> 00:36:17,400
you will easily

631
00:36:17,420 --> 00:36:18,740
he able to do that

632
00:36:18,750 --> 00:36:23,120
this is your light source

633
00:36:23,160 --> 00:36:24,100
o lines

634
00:36:24,100 --> 00:36:27,310
you're gratings a thousand lines per millimeter

635
00:36:27,360 --> 00:36:31,640
that means the spacing of your grading is one micron

636
00:36:31,690 --> 00:36:33,760
one micron is

637
00:36:33,770 --> 00:36:35,640
ten times smaller

638
00:36:35,640 --> 00:36:40,160
and this number solely angles are you in your case

639
00:36:40,240 --> 00:36:44,310
the way larger than what we have there i will make it completely dark

640
00:36:44,310 --> 00:36:45,580
and then i want you to

641
00:36:45,600 --> 00:36:46,630
i rotate

642
00:36:49,160 --> 00:36:53,360
creating such that you get this backtrack on either side on the left and on

643
00:36:53,360 --> 00:36:55,110
the right that means you're

644
00:36:55,130 --> 00:36:56,300
groups and then

645
00:36:56,310 --> 00:37:00,330
in vertical direction

646
00:37:00,380 --> 00:37:01,890
and what you see now

647
00:37:01,900 --> 00:37:05,460
when better than what i could show you my previous demonstration

648
00:37:05,480 --> 00:37:08,660
you see the zero order is the lamp itself

649
00:37:08,660 --> 00:37:13,680
all the colors right at the center that's the land that user and maximum

650
00:37:13,780 --> 00:37:15,100
and then you see

651
00:37:15,100 --> 00:37:16,400
if you go to the right

652
00:37:16,410 --> 00:37:21,610
you see the blue coming in beautifully first because at the smallest wavelength

653
00:37:21,680 --> 00:37:24,920
going further to the right you see the first order red

654
00:37:24,920 --> 00:37:27,460
with the acceleration

655
00:37:29,080 --> 00:37:33,140
now comes the fact that we discuss their that nothing goes out in the direction

656
00:37:33,140 --> 00:37:34,960
of the acceleration

657
00:37:36,580 --> 00:37:40,620
and go out in this direction so no electric field is produced in this direction

658
00:37:40,640 --> 00:37:43,020
and the maximum is produced in this direction

659
00:37:43,020 --> 00:37:45,390
and somewhere in between here

660
00:37:45,440 --> 00:37:46,790
that is there

661
00:37:46,890 --> 00:37:48,690
reflected by

662
00:37:48,690 --> 00:37:50,350
the sign of data

663
00:37:50,410 --> 00:37:52,350
a is zero

664
00:37:52,390 --> 00:37:54,060
nothing goes down

665
00:37:54,060 --> 00:37:59,140
if data ninety degrees you get the maximum that's the whole plane perpendicular to the

666
00:37:59,210 --> 00:38:01,250
oscillating charges whole plane

667
00:38:01,290 --> 00:38:04,370
maximum electric field

668
00:38:04,520 --> 00:38:06,020
and then

669
00:38:06,020 --> 00:38:06,920
that may

670
00:38:06,940 --> 00:38:09,160
not be so obvious to you now

671
00:38:09,190 --> 00:38:13,080
it's also inversely proportional to the distance are

672
00:38:13,120 --> 00:38:14,190
double are

673
00:38:14,210 --> 00:38:16,350
electric field strength goes down

674
00:38:18,500 --> 00:38:21,190
a factor of two

675
00:38:21,250 --> 00:38:24,420
and the poynting vector

676
00:38:24,440 --> 00:38:26,660
which is the product of a and b

677
00:38:26,660 --> 00:38:28,870
he is or proportional to be

678
00:38:28,890 --> 00:38:30,640
so the poynting vector then

679
00:38:30,660 --> 00:38:31,790
is proportional

680
00:38:31,790 --> 00:38:33,290
to q square

681
00:38:33,290 --> 00:38:36,580
proportional to the square proportional

682
00:38:36,580 --> 00:38:39,560
designed square of data

683
00:38:39,580 --> 00:38:41,790
and inversely proportional

684
00:38:41,810 --> 00:38:43,120
we are square

685
00:38:43,160 --> 00:38:48,180
that is obvious that it has to be inversely proportional which are square

686
00:38:48,190 --> 00:38:51,120
because if you go if you ever sphere

687
00:38:51,160 --> 00:38:53,730
radiation goes out

688
00:38:53,730 --> 00:38:56,730
and you twice as far away

689
00:38:56,770 --> 00:38:58,060
then you know

690
00:38:58,080 --> 00:39:01,390
that the area of the sphere is four times larger

691
00:39:01,390 --> 00:39:05,710
and so the amount of energy per square meter must be four times slower so

692
00:39:05,710 --> 00:39:11,060
the poynting vector must fall off as one of our greatest conservation of energy

693
00:39:11,080 --> 00:39:14,230
and if you accept the fact that the poynting vector falls was one of our

694
00:39:14,230 --> 00:39:18,710
square then the vector must always one are

695
00:39:18,710 --> 00:39:22,660
because the poynting vector is the product of and b

696
00:39:22,680 --> 00:39:24,920
bnb must both fall of

697
00:39:24,940 --> 00:39:30,790
as one of or

698
00:39:30,870 --> 00:39:35,270
i want to show you

699
00:39:35,330 --> 00:39:37,190
a picture that may help

700
00:39:39,210 --> 00:39:40,660
the radiation

701
00:39:40,660 --> 00:39:47,160
electric field is

702
00:39:47,210 --> 00:39:49,940
oriented relative to a

703
00:39:49,980 --> 00:39:52,910
charge that we are accelerating in the middle here

704
00:39:52,960 --> 00:39:56,000
the accelerated charge up and down

705
00:39:56,040 --> 00:39:57,580
frequency omega

706
00:39:57,640 --> 00:39:59,500
whatever radiation you want to make

707
00:39:59,500 --> 00:40:04,100
fine with me you can make only as large as you want

708
00:40:04,190 --> 00:40:08,500
one thing i as these these waves here these snakes

709
00:40:08,540 --> 00:40:14,790
represent electromagnetic radiation and nothing is going on in this direction we understand why

710
00:40:14,790 --> 00:40:17,180
the maximum is going out in the plane

711
00:40:18,440 --> 00:40:23,250
to this direction that is this this this and this

712
00:40:23,310 --> 00:40:28,770
and notice that the vector that you receive when you're here

713
00:40:28,830 --> 00:40:31,390
is perpendicular to are this are

714
00:40:31,410 --> 00:40:33,370
and this perpendicular

715
00:40:33,410 --> 00:40:36,270
and notice is what i said that a

716
00:40:37,370 --> 00:40:38,330
and the

717
00:40:38,330 --> 00:40:40,500
in one place

718
00:40:40,520 --> 00:40:42,770
eight which is the acceleration

719
00:40:43,980 --> 00:40:46,270
and he e in one place

720
00:40:48,250 --> 00:40:50,420
i e e i in one place

721
00:40:52,500 --> 00:40:54,580
i e i in one place

722
00:40:54,640 --> 00:40:57,080
so you can always determine the direction

723
00:40:57,100 --> 00:40:59,250
of the oscillating

724
00:40:59,270 --> 00:41:01,060
electric fields

725
00:41:01,060 --> 00:41:05,390
and if you go at an angle fade out which is neither zero which here

726
00:41:05,390 --> 00:41:09,390
nor ninety degrees which is here here and here but if you have something in

727
00:41:10,600 --> 00:41:14,660
and notice that the effect here is drawn a little smaller

728
00:41:14,680 --> 00:41:16,960
that that sign state here

729
00:41:17,710 --> 00:41:22,330
and consequently the poynting vector in that direction

730
00:41:22,370 --> 00:41:26,790
will be smaller than in this direction and in this direction the poynting vector

731
00:41:26,790 --> 00:41:28,120
will be

732
00:41:31,100 --> 00:41:35,310
we call this radiation where you here or there are there are there we call

733
00:41:35,310 --> 00:41:37,620
the linearly polarized radiation

734
00:41:37,690 --> 00:41:40,290
for the simple reason that the electric field

735
00:41:40,310 --> 00:41:43,420
it is oscillating in one direction

736
00:41:43,440 --> 00:41:45,480
it's linear

737
00:41:45,500 --> 00:41:47,690
and i'm going to produce for you

738
00:41:47,710 --> 00:41:50,080
linearly polarized radiation

739
00:41:50,080 --> 00:41:54,250
and i have two demonstrations for that

740
00:41:54,270 --> 00:41:58,180
so i get the lights back on again let's discuss the

741
00:42:00,830 --> 00:42:02,850
i have here

742
00:42:07,830 --> 00:42:10,370
which transmits at

743
00:42:10,420 --> 00:42:14,410
ten gigahertz

744
00:42:14,410 --> 00:42:17,480
that's a wavelength of three centimetres

745
00:42:17,500 --> 00:42:20,420
you wouldn't call the radio you would call the radio that's just a matter of

746
00:42:23,690 --> 00:42:29,060
and we have also a receiver we have the transmitter and we have a receiver

747
00:42:29,180 --> 00:42:31,920
the transmitter is here

748
00:42:32,020 --> 00:42:34,770
and the receiver is here

749
00:42:34,850 --> 00:42:38,230
to give you a three-dimensional picture

750
00:42:38,230 --> 00:42:41,790
this is my coordinate system this is coming straight out of the blackboard to you

751
00:42:41,810 --> 00:42:43,940
that's what i meant by this

752
00:42:43,960 --> 00:42:45,120
then the

753
00:42:47,000 --> 00:42:48,250
it is aimed like so

754
00:42:48,270 --> 00:42:49,870
just like this

755
00:42:50,000 --> 00:42:53,330
current is going to be oscillating like this

756
00:42:53,480 --> 00:42:55,600
and the receiver

757
00:42:55,620 --> 00:42:58,520
i think of this is my radio which is receiving

758
00:42:58,560 --> 00:43:00,500
it's here

759
00:43:00,500 --> 00:43:05,040
and the intent of the radio is also in this direction

760
00:43:05,080 --> 00:43:06,080
and so

761
00:43:06,100 --> 00:43:07,560
the electric field

762
00:43:07,580 --> 00:43:11,540
which you see here which comes from these oscillating charges

763
00:43:11,580 --> 00:43:14,600
which also made by the way over here the frequency

764
00:43:14,660 --> 00:43:17,560
ten billion times per second

765
00:43:17,580 --> 00:43:18,960
the electric field

766
00:43:18,980 --> 00:43:23,810
is perpendicular to are these are

767
00:43:23,810 --> 00:43:25,810
and the electric field

768
00:43:25,830 --> 00:43:27,910
are a and one plane

769
00:43:27,920 --> 00:43:29,790
so that means

770
00:43:29,790 --> 00:43:34,660
that the electric field that right here is also leading like so

771
00:43:34,710 --> 00:43:39,480
and therefore this on ten this receiver is very happy the radiation comes in exactly

772
00:43:39,480 --> 00:43:40,690
in the right way

773
00:43:40,730 --> 00:43:43,730
and so it will receive it

774
00:43:43,770 --> 00:43:49,750
we have modulated this signal with an audio signal amplitude modulation as we discussed earlier

775
00:43:49,770 --> 00:43:51,890
but you also use radios

776
00:43:51,890 --> 00:43:55,870
and we modulated was approximately one kilometres audio signal

777
00:43:55,870 --> 00:43:59,960
i will make you listen to the audio signal you can tell them

778
00:44:00,000 --> 00:44:03,690
this receiver is indeed receiving the three centimeter radar

779
00:44:03,710 --> 00:44:07,540
from this transmitter that's what we will first two

780
00:44:07,560 --> 00:44:09,250
well this turn on the

781
00:44:10,580 --> 00:44:14,750
new year one

782
00:44:19,910 --> 00:44:26,560
and received by the time straight line like this

783
00:44:26,690 --> 00:44:29,960
administrators really goes from here

784
00:44:29,980 --> 00:44:31,520
i put my hands in between

785
00:44:31,560 --> 00:44:33,540
my hands up to three centimeters

786
00:44:33,560 --> 00:44:35,120
it's not there anymore

787
00:44:35,140 --> 00:44:37,620
it's not there anymore

788
00:44:37,620 --> 00:44:38,470
really optimizing

789
00:44:39,020 --> 00:44:40,870
the controller learn dynamics model

790
00:44:41,490 --> 00:44:43,370
through usually ten twenty iterations

791
00:44:43,790 --> 00:44:46,990
you get to a point where it fully and the flight entire trajectory

792
00:44:48,280 --> 00:44:51,960
is something called a learning control it's where you know you do the same task

793
00:44:52,310 --> 00:44:56,130
as we use that knowledge to find your controller through real world trials

794
00:44:57,820 --> 00:45:01,080
so go back to step forward after around this loop about

795
00:45:01,710 --> 00:45:02,570
ten twenty times

796
00:45:03,220 --> 00:45:04,290
busy for about an hour

797
00:45:04,720 --> 00:45:06,080
and use the top result you get

798
00:45:11,420 --> 00:45:13,080
what you see here is completely autonomous

799
00:45:13,620 --> 00:45:14,540
starting on the ground

800
00:45:14,940 --> 00:45:16,120
helicopter flips over

801
00:45:20,790 --> 00:45:24,860
and it shows that this particular airshow show was i stand it would demonstrate all

802
00:45:25,020 --> 00:45:29,140
past capabilities in the first five sections and everything after that's what people couldn't do

803
00:45:30,190 --> 00:45:31,390
so however was done before

804
00:45:32,680 --> 00:45:35,570
this was done before it's a profile by absolute

805
00:45:36,130 --> 00:45:38,850
centralized before starting with done before

806
00:45:39,980 --> 00:45:41,880
now on these manner somebody was able to do

807
00:45:45,070 --> 00:45:45,680
it looks

808
00:45:50,610 --> 00:45:51,160
it was a great

809
00:45:51,760 --> 00:45:52,690
here which at the top

810
00:45:53,370 --> 00:45:57,150
stopped and that's the way generation where you climb rather than burning

811
00:45:57,620 --> 00:45:59,020
your kinetic energy the fraction

812
00:46:00,930 --> 00:46:03,840
can fast circles these helicopters are

813
00:46:05,470 --> 00:46:13,310
about their blatter's eight hundred millimeters long almost immediately for single label itself goes up to fifty miles per hour

814
00:46:13,940 --> 00:46:16,590
so this thing is quite pretty fast and coming down

815
00:46:18,810 --> 00:46:21,800
and this is all the hardest manoeuvres wired is the hardest manoeuvres

816
00:46:22,300 --> 00:46:23,230
when stay in place

817
00:46:23,780 --> 00:46:26,560
helicopters working in areas just pushing around

818
00:46:27,010 --> 00:46:29,940
so it's much harder to predict what the influence on the air flow is

819
00:46:30,340 --> 00:46:31,200
on the helicopter

820
00:46:42,430 --> 00:46:43,180
and then we do this we

821
00:46:43,260 --> 00:46:48,050
done that you haven't seen this particular video are autorotation landing which i mentioned earlier it's where r u

822
00:46:48,810 --> 00:46:49,920
let's say your engine dies

823
00:46:51,680 --> 00:46:53,780
you still might come down safely what you do is

824
00:46:54,560 --> 00:46:59,820
your helicopter blades will set you back some exactly like it when turbine but you can think about the way

825
00:47:00,310 --> 00:47:01,920
and absorb here also energy

826
00:47:02,370 --> 00:47:05,280
in two plates spinning energy which there will be burned on diffraction

827
00:47:06,140 --> 00:47:08,310
and so you can still land relatively safely

828
00:47:09,600 --> 00:47:14,900
usually in a relatively high water will be broken the people in the helicopter will survive it's well done

829
00:47:16,800 --> 00:47:20,770
now the thing is something called cows it's worth you're essentially flip around the place

830
00:47:21,160 --> 00:47:22,610
well spinning the tail around

831
00:47:24,120 --> 00:47:27,490
the sum of the minimum that we've done that when this take part this figure

832
00:47:27,640 --> 00:47:31,170
show but it cannot completely entire repertoire our pilot had

833
00:47:33,780 --> 00:47:36,860
one question might ask yourselves okay you maneuvers step

834
00:47:37,310 --> 00:47:38,530
extra powers can do but

835
00:47:39,180 --> 00:47:44,570
maybe you're doing very nicely you're just kind doing them they look good but they weren't well done

836
00:47:45,880 --> 00:47:47,540
let's take a look at how well done they work

837
00:47:48,120 --> 00:47:50,510
so remember white is what we're trying to fly

838
00:47:51,050 --> 00:47:51,510
and black

839
00:47:52,050 --> 00:47:54,270
it's how because we were so lucky showing

840
00:47:54,740 --> 00:47:56,230
where you actually was

841
00:47:57,520 --> 00:47:59,560
this is a graph actually play with this replay

842
00:48:00,380 --> 00:48:04,590
recordings position orientation and the helicopter is actual flight they were replaying here

843
00:48:05,120 --> 00:48:08,010
shown that were within a metre of where we were asked to be

844
00:48:08,760 --> 00:48:13,950
this is very high accuracy even for people who spend their lives on fine-tuning helicopter hover controllers

845
00:48:14,560 --> 00:48:16,730
they get one meter accuracy that can be pretty happy

846
00:48:21,050 --> 00:48:22,970
okay so fly exactly where we want to fly

847
00:48:26,840 --> 00:48:28,380
how to teach helicopter to fly

848
00:48:29,060 --> 00:48:30,120
by mean extra powers

849
00:48:30,470 --> 00:48:31,260
machine learning are and

850
00:48:31,970 --> 00:48:33,710
so should the court probabilistic inference

851
00:48:34,190 --> 00:48:34,880
then we combined

852
00:48:35,470 --> 00:48:38,470
with the optimal control together and defined maneuvers

853
00:48:40,130 --> 00:48:42,690
well the point that they change anything control side

854
00:48:43,090 --> 00:48:43,580
we just

855
00:48:44,330 --> 00:48:46,030
will read a lot of control papers

856
00:48:46,700 --> 00:48:50,800
tried a lot of things and conversion of control stick just make me down

857
00:48:51,200 --> 00:48:52,800
would work best for our problem

858
00:48:53,530 --> 00:48:58,460
and although was or indeed learning the specific dynamics models forties trajectories

859
00:48:58,910 --> 00:49:03,170
made this work and exactly how a digital-only control which is a little different from

860
00:49:03,170 --> 00:49:04,320
what other people have done before

861
00:49:04,320 --> 00:49:08,180
but in general they could be more than one normal

862
00:49:08,220 --> 00:49:10,630
process usually make sure

863
00:49:10,650 --> 00:49:15,390
number processes and only one

864
00:49:15,730 --> 00:49:19,190
and the third and most

865
00:49:19,250 --> 00:49:21,330
sorry of outliers may

866
00:49:21,350 --> 00:49:24,800
represent different classes of generating mechanisms

867
00:49:24,820 --> 00:49:30,710
so the outliers the set of all points may be very large

868
00:49:32,390 --> 00:49:33,480
the basic model

869
00:49:33,510 --> 00:49:38,160
usually assume that outliers are observations and not

870
00:49:38,170 --> 00:49:40,330
i said all that's not very large

871
00:49:40,350 --> 00:49:42,780
so these three

872
00:49:42,810 --> 00:49:44,520
basic points here

873
00:49:44,540 --> 00:49:49,680
should illustrate the basic intuition behind the statistical

874
00:49:49,690 --> 00:49:54,720
approach by balkans is very limited

875
00:49:54,730 --> 00:49:56,600
and you need more

876
00:49:59,180 --> 00:50:02,430
techniques to

877
00:50:02,650 --> 00:50:04,450
conquer nowadays data

878
00:50:04,470 --> 00:50:09,300
so the consequence is that a lot of models and approaches have evolved in the

879
00:50:09,340 --> 00:50:10,550
past years

880
00:50:10,670 --> 00:50:14,600
in order to exceed limitations

881
00:50:14,660 --> 00:50:17,260
and yes i think not

882
00:50:17,270 --> 00:50:20,510
that's easy to keep track of of the solutions so one

883
00:50:20,530 --> 00:50:22,500
m of this tutorial courses

884
00:50:22,520 --> 00:50:23,880
try to give you

885
00:50:23,900 --> 00:50:26,010
some of you know this

886
00:50:26,020 --> 00:50:31,330
and what's very important is if you look at specific outlier detection

887
00:50:31,340 --> 00:50:34,010
models and approaches is

888
00:50:35,110 --> 00:50:36,680
each of them usually

889
00:50:36,710 --> 00:50:39,560
involves sometimes you

890
00:50:39,580 --> 00:50:41,060
but very typical

891
00:50:41,070 --> 00:50:43,670
assumptions and restrictions and

892
00:50:43,710 --> 00:50:47,270
very often these assumptions and restrictions are hidden

893
00:50:47,280 --> 00:50:50,420
are not explicitly discussed in papers

894
00:50:50,430 --> 00:50:54,580
but usually each paper for each approach

895
00:50:54,640 --> 00:50:55,920
has one

896
00:50:55,930 --> 00:50:57,300
at least one

897
00:50:59,480 --> 00:51:02,000
you have to keep this in mind when we

898
00:51:02,430 --> 00:51:07,180
discuss different different approaches to action

899
00:51:07,200 --> 00:51:11,280
so there are generally some general application scenarios

900
00:51:11,300 --> 00:51:14,250
first of all we have supervised scenario

901
00:51:14,270 --> 00:51:17,890
because in some applications you just training data

902
00:51:18,020 --> 00:51:20,930
o of normal and abnormal data

903
00:51:20,960 --> 00:51:23,960
and in this case you can try to train the classifier

904
00:51:25,410 --> 00:51:27,020
this is a supervised task

905
00:51:27,040 --> 00:51:32,410
usually this is an unbalanced classification problem

906
00:51:32,440 --> 00:51:34,610
because you usually have a lot more

907
00:51:34,630 --> 00:51:38,830
normal observations the training data that

908
00:51:38,840 --> 00:51:41,100
OK you have to

909
00:51:41,190 --> 00:51:43,320
the second scenario seems to provide

910
00:51:43,380 --> 00:51:47,280
so in this case usually have just training data for one class

911
00:51:48,860 --> 00:51:50,500
normal distribution or

912
00:51:50,510 --> 00:51:52,380
the abnormal

913
00:51:52,400 --> 00:51:54,520
data points

914
00:51:54,530 --> 00:51:56,470
and you have to learn again

915
00:51:58,130 --> 00:52:00,110
rules that bias

916
00:52:00,170 --> 00:52:01,820
to do the job

917
00:52:01,830 --> 00:52:06,560
and finally we have them so unsupervised scenario where we don't have any training data

918
00:52:07,670 --> 00:52:12,730
and in most applications this case i don't think and

919
00:52:12,750 --> 00:52:16,010
in this tutorial we focus on the unsupervised scenario

920
00:52:16,030 --> 00:52:18,910
so you won't find in this

921
00:52:18,930 --> 00:52:24,420
the tree nothing about supervised things like to julian

922
00:52:28,650 --> 00:52:32,570
if we are talking about unsupervised learning unsupervised data mining then

923
00:52:32,610 --> 00:52:33,510
maybe you

924
00:52:33,520 --> 00:52:35,650
just have a question that

925
00:52:36,520 --> 00:52:37,260
is it

926
00:52:37,280 --> 00:52:40,100
is also detection very related to

927
00:52:40,130 --> 00:52:41,260
and similar to

928
00:52:41,290 --> 00:52:44,970
clustering so our allies not just decide do

929
00:52:45,030 --> 00:52:46,300
clustering algorithms

930
00:52:48,560 --> 00:52:53,230
true on the first side maybe because many clustering algorithms

931
00:52:53,320 --> 00:52:58,080
not assign all points of course but can come from noise of the

932
00:52:59,250 --> 00:53:03,560
the option that applying one of those are in

933
00:53:03,610 --> 00:53:05,940
retreat noise it as outliers

934
00:53:05,950 --> 00:53:11,830
but there are some problems with this because clustering algorithms are usually optimise find clusters

935
00:53:11,830 --> 00:53:12,570
and not

936
00:53:12,590 --> 00:53:14,070
find out so

937
00:53:15,000 --> 00:53:17,560
it's more orthogonal than

938
00:53:17,710 --> 00:53:20,530
it's related of course but it's these two

939
00:53:20,580 --> 00:53:24,500
problems of detection clustering are called

940
00:53:24,510 --> 00:53:28,420
and that means that the accuracy of all the detection depends on how good the

941
00:53:28,420 --> 00:53:31,860
clustering captures the structure of the cluster

942
00:53:31,880 --> 00:53:33,520
and not how good it

943
00:53:33,570 --> 00:53:36,300
he then defies of

944
00:53:36,300 --> 00:53:38,300
and again a set of

945
00:53:38,300 --> 00:53:41,720
vlachos in nuclear physics

946
00:53:43,130 --> 00:53:44,540
i'm going to

947
00:53:45,110 --> 00:53:51,560
give you some examples of recent studies that are made nowadays so

948
00:53:51,600 --> 00:53:52,800
the samples

949
00:53:52,810 --> 00:53:55,780
experimental and theoretical studies

950
00:53:55,800 --> 00:53:57,670
so they are made only

951
00:53:57,690 --> 00:53:59,510
all of the world so

952
00:53:59,530 --> 00:54:06,170
in many different places and some are made here is that these are facilities

953
00:54:06,180 --> 00:54:12,130
so i went first to describe the figure of merit of the present approaches theoretical

954
00:54:13,600 --> 00:54:18,560
because of the new computer were very high now we can make some systematics and

955
00:54:18,560 --> 00:54:20,540
really challenge the theories

956
00:54:20,560 --> 00:54:25,280
and to compare extensive comparisons between theory and experiment

957
00:54:25,290 --> 00:54:30,240
so then if we have something which is close to experiments we can hope to

958
00:54:30,240 --> 00:54:34,640
make some predictions for all these exotic nuclei

959
00:54:35,730 --> 00:54:39,570
i will talk quite a long time about exotic nuclei

960
00:54:39,590 --> 00:54:45,100
and this is there are really many many experiments were related to these

961
00:54:47,570 --> 00:54:52,540
i will talk about the isomers so isomers are metastable states

962
00:54:52,560 --> 00:54:55,880
they can appear in different class region and

963
00:54:55,900 --> 00:54:57,090
there really

964
00:54:57,120 --> 00:54:59,480
had to go down to structure

965
00:55:00,240 --> 00:55:01,590
of the nuclei

966
00:55:01,600 --> 00:55:06,260
we talk about shape coexistence search for super heavy elements

967
00:55:06,260 --> 00:55:11,010
and then i was in this way is hope to test fundamental symmetries

968
00:55:11,030 --> 00:55:13,670
for example of the weak interaction

969
00:55:13,680 --> 00:55:16,880
so today is really devoted to class structure

970
00:55:16,900 --> 00:55:20,900
so tomorrow we we talk about the reaction in the vicinity but sufficient to to

971
00:55:20,900 --> 00:55:22,900
their structure

972
00:55:22,930 --> 00:55:27,950
so first figure of merit of the present approaches so this is one example

973
00:55:27,960 --> 00:55:32,540
so this has been a panelist mean field based approach is so the beyond mean

974
00:55:32,540 --> 00:55:35,180
field but still the main assumption has been

975
00:55:35,200 --> 00:55:39,070
that means so you have a comparison between very

976
00:55:39,090 --> 00:55:42,980
in experiments for the first two plastic citation

977
00:55:43,000 --> 00:55:46,850
the energy of this exhibition in even even nuclei

978
00:55:48,570 --> 00:55:50,980
there are five hundred fifty seven

979
00:55:51,020 --> 00:55:53,970
experimentally observed to press levels

980
00:55:53,980 --> 00:55:57,350
and we have calculated all except sixteen

981
00:55:57,360 --> 00:55:59,330
these like one

982
00:55:59,350 --> 00:56:04,170
so the you know why we have eliminated this light elements

983
00:56:04,190 --> 00:56:09,300
so the answer is the mean field approximation which is not valid for human birth

984
00:56:10,230 --> 00:56:11,640
nuclear sorry

985
00:56:11,660 --> 00:56:13,950
because when you're doing so it's an average

986
00:56:13,980 --> 00:56:16,850
you are a very cheap over all the particles

987
00:56:16,860 --> 00:56:21,660
so you need to to be valid to have huge number of nucleons of particles

988
00:56:21,670 --> 00:56:23,040
so that's where we

989
00:56:23,050 --> 00:56:24,860
that's why we don't trust

990
00:56:24,920 --> 00:56:28,540
calculation for very small systems

991
00:56:29,580 --> 00:56:33,890
so that's this is the energy experiments for perfect calculation it would have been the

992
00:56:33,910 --> 00:56:39,080
straight line here and you see that we have some scatter point

993
00:56:39,100 --> 00:56:42,860
so in more detail as you see that this is this is the same feature

994
00:56:43,010 --> 00:56:49,600
is energy spans three orders of magnitude from twenty kb to three mb

995
00:56:50,320 --> 00:56:55,250
it's really a challenge for any theory to represent two want to reproduce feature that

996
00:56:55,250 --> 00:56:59,080
are so different from one place to the other one

997
00:56:59,100 --> 00:57:04,250
so you see here the following states we have a very good agreement

998
00:57:04,670 --> 00:57:07,130
we have the experiments

999
00:57:07,140 --> 00:57:11,630
if we look in more detail we see that its for strongly deformed nuclei

1000
00:57:11,640 --> 00:57:13,670
so it means that these

1001
00:57:13,950 --> 00:57:18,800
approach works well for strongly deformed nuclei

1002
00:57:18,820 --> 00:57:23,790
here it's not so good and it appears in fact for semi magic nuclei

1003
00:57:23,830 --> 00:57:27,300
magic nuclei stanley magic light nuclei

1004
00:57:27,320 --> 00:57:32,450
so we can explain why we don't have such an agreement

1005
00:57:32,470 --> 00:57:37,000
it because as i told you yesterday there are different ways of exciting and you

1006
00:57:37,000 --> 00:57:39,670
can use either can be collective like

1007
00:57:39,690 --> 00:57:42,950
rotational it's different vibrational

1008
00:57:43,010 --> 00:57:47,480
or it can be of individual excitations from moving from one show to the other

1009
00:57:48,480 --> 00:57:51,620
this mechanism is not allowed here

1010
00:57:51,630 --> 00:57:52,950
and here

1011
00:57:52,950 --> 00:57:58,940
it should have a role in so you should have cooking between these intrinsic excitations

1012
00:57:59,010 --> 00:58:01,450
and collective motion of nuclei

1013
00:58:01,460 --> 00:58:04,490
and that's why we don't have such a good agreement

1014
00:58:04,500 --> 00:58:08,200
is because of these that has been here

1015
00:58:08,260 --> 00:58:10,510
and here there are six

1016
00:58:10,520 --> 00:58:17,720
bad nuclei where we are completely and in fact it's for some neutron deficient later

1017
00:58:17,740 --> 00:58:19,810
mercury isotopes

1018
00:58:19,820 --> 00:58:24,260
and we are now lies in that it seems that is because of shape coexistence

1019
00:58:24,260 --> 00:58:26,710
so we don't have the right information

1020
00:58:26,740 --> 00:58:29,860
of these nuclei and

1021
00:58:29,880 --> 00:58:33,010
since the energy of the two places is

1022
00:58:33,020 --> 00:58:35,000
related to these deformations

1023
00:58:35,010 --> 00:58:37,800
if we don't find the good information at all

1024
00:58:37,820 --> 00:58:39,620
we are completely so

1025
00:58:39,660 --> 00:58:41,570
a lot of work is no

1026
00:58:41,620 --> 00:58:48,380
that they can to improve this and all to understand better this nuclei

1027
00:58:49,260 --> 00:58:52,180
we have also calculated quadrupole moments

1028
00:58:52,200 --> 00:58:57,950
of these different to close states differently so this is also very experiments

1029
00:58:57,960 --> 00:59:01,320
so there are not so many experimental data

1030
00:59:01,330 --> 00:59:02,960
this is there some

1031
00:59:02,970 --> 00:59:06,490
and you see that we are quite close to the experiment

1032
00:59:06,500 --> 00:59:10,710
and even better we can make some predictions because in this case

1033
00:59:10,720 --> 00:59:13,680
some displays the dominant thanks to

1034
00:59:13,700 --> 00:59:16,950
the sign of the moment was not known so we don't know if it was

1035
00:59:16,950 --> 00:59:18,530
tried to relate

1036
00:59:18,560 --> 00:59:23,950
so what we have done that we have the them twice positive information first and

1037
00:59:23,960 --> 00:59:25,340
they get minus the

1038
00:59:25,370 --> 00:59:27,860
and it appears that for agreement very

1039
00:59:27,900 --> 00:59:31,380
we can predict that this nicolai are prolate

1040
00:59:31,400 --> 00:59:34,970
with a negative quadrupole moment

1041
00:59:34,990 --> 00:59:40,110
to summarize all these which has been published in different papers again that the series

1042
00:59:40,110 --> 00:59:42,050
appears to be reliable

1043
00:59:42,060 --> 00:59:47,020
and that some predictions can be made in exotic nuclei

1044
00:59:47,030 --> 00:59:51,030
so now this is the subject except technique

1045
00:59:51,060 --> 00:59:56,060
there are different topics that can be studied with exotic nuclei for example neutron and

1046
00:59:56,060 --> 01:00:00,270
proton drip lines had to a new shared structure

1047
01:00:00,270 --> 01:00:04,560
anybody want to try

1048
01:00:04,580 --> 01:00:06,320
come on

1049
01:00:06,340 --> 01:00:09,450
yes what you say

1050
01:00:09,470 --> 01:00:16,290
two people holding hands pressed together very good anybody have different readings

1051
01:00:16,330 --> 01:00:19,120
yes and that yes

1052
01:00:21,020 --> 01:00:23,660
dancing bears

1053
01:00:27,250 --> 01:00:30,210
OK gotta right here named

1054
01:00:30,280 --> 01:00:33,770
reported it helps to know that that thing is that i think is very good

1055
01:00:33,810 --> 01:00:37,290
anybody else one other yes

1056
01:00:37,310 --> 01:00:39,390
a man in a as

1057
01:00:40,180 --> 01:00:41,770
it turns out

1058
01:00:41,790 --> 01:00:45,690
that there are right answers and wrong answers or shock tests

1059
01:00:48,770 --> 01:00:52,060
according to the test this is from a real rush shock tests

1060
01:00:52,080 --> 01:00:55,210
it is important to see the bodies two human figures

1061
01:00:55,230 --> 01:00:57,230
usually females are clients

1062
01:00:57,270 --> 01:00:58,790
good work over there

1063
01:00:58,810 --> 01:01:03,040
if you don't it's seen as the sine of problems relating to people

1064
01:01:03,140 --> 01:01:08,920
if you want to go for cave friends or butterfly or vagina that's also okay

1065
01:01:12,890 --> 01:01:15,520
the raw shark test is

1066
01:01:15,530 --> 01:01:18,290
transcendental useless

1067
01:01:18,350 --> 01:01:23,390
it is when it has been studied and explored and it is as useless as

1068
01:01:24,470 --> 01:01:27,860
rolling dice it is users is leaves

1069
01:01:27,880 --> 01:01:29,740
none the less

1070
01:01:29,760 --> 01:01:30,880
people love it

1071
01:01:30,890 --> 01:01:35,090
and it's use all over the place and it is used for example in child

1072
01:01:35,090 --> 01:01:36,630
custody cases

1073
01:01:36,640 --> 01:01:40,860
if if you've broken up with your partner and you guys are quarreling over who

1074
01:01:40,860 --> 01:01:46,160
gets to keep the kids you might find yourself in shrinks office looking at this

1075
01:01:46,200 --> 01:01:47,700
and in fact this is why

1076
01:01:47,740 --> 01:01:51,670
they end up on the web their services there are people have been kind enough

1077
01:01:51,670 --> 01:01:52,870
to put on the web

1078
01:01:52,880 --> 01:01:54,380
using plots

1079
01:01:54,390 --> 01:01:56,210
including the right answers

1080
01:01:56,220 --> 01:01:57,090
to them

1081
01:01:57,210 --> 01:02:00,350
but they are worthless as psychological measures

1082
01:02:00,370 --> 01:02:02,620
can we do better well

1083
01:02:02,630 --> 01:02:03,980
we probably can

1084
01:02:06,730 --> 01:02:12,140
the study where he went through the dictionary and took all of the traits that

1085
01:02:12,140 --> 01:02:18,060
he believed to be related to personality and he got eighteen thousand

1086
01:02:18,080 --> 01:02:20,190
but what was interesting was

1087
01:02:20,270 --> 01:02:25,510
they were necessarily independent rates so the traits like friendly

1088
01:02:25,560 --> 01:02:27,160
so see about

1089
01:02:27,180 --> 01:02:29,830
while komi warm-hearted

1090
01:02:29,850 --> 01:02:32,410
seem to all tap the same thing

1091
01:02:32,430 --> 01:02:33,900
so could tell

1092
01:02:33,920 --> 01:02:36,460
and many others try to narrow it down

1093
01:02:36,510 --> 01:02:39,700
try to to ask the question in how many ways

1094
01:02:39,720 --> 01:02:43,560
people's personalities different from one another

1095
01:02:43,570 --> 01:02:48,590
how many parameters of different you need how how many numbers

1096
01:02:48,610 --> 01:02:53,670
the can i give you that was narrowly wins and say what personality you are

1097
01:02:54,040 --> 01:02:57,730
one approach was from icing

1098
01:02:57,750 --> 01:03:00,540
who claimed or just two

1099
01:03:00,550 --> 01:03:04,550
you could be somewhere in the scale of introverted extroverted

1100
01:03:04,560 --> 01:03:08,370
it somewhere on the scale of neurotic unstable

1101
01:03:08,390 --> 01:03:11,150
and these are basically two types of traits with two

1102
01:03:11,160 --> 01:03:14,460
settings each there are basically four types of people

1103
01:03:14,470 --> 01:03:19,770
later on he added another trade trade which he described as psychotic system versus non

1104
01:03:19,770 --> 01:03:24,880
cicadas is and the cruelty meant whether your aggressive are empathetic

1105
01:03:24,910 --> 01:03:29,850
and three traits with two settings each giving you a types of people

1106
01:03:32,610 --> 01:03:33,680
later on

1107
01:03:33,700 --> 01:03:34,990
i can tell

1108
01:03:35,010 --> 01:03:37,590
drop down to sixteen factors

1109
01:03:37,600 --> 01:03:42,860
so the sixteen personality factors are sixteen ways people with different

1110
01:03:42,880 --> 01:03:46,680
and so if i ask you to describe your roommate

1111
01:03:46,690 --> 01:03:51,990
along the sixteen dimensions you should be able to do so

1112
01:03:52,000 --> 01:03:55,000
more recently people have come to the conclusion

1113
01:03:55,850 --> 01:03:57,970
two or three is too few

1114
01:03:57,980 --> 01:03:59,950
but sixteen might be too many

1115
01:03:59,990 --> 01:04:07,400
and there's psychological consensus on what's been known as the big five

1116
01:04:07,420 --> 01:04:12,020
and the big five personality factors

1117
01:04:12,030 --> 01:04:13,600
are these

1118
01:04:13,610 --> 01:04:18,280
and what this means is when we talk about each other

1119
01:04:18,290 --> 01:04:20,060
and use additives

1120
01:04:20,070 --> 01:04:21,320
the claim is

1121
01:04:21,360 --> 01:04:25,580
we could do so in thousands of different ways but deep down we're talking about

1122
01:04:25,580 --> 01:04:29,960
one of these five dimensions this means that when the psychological

1123
01:04:29,980 --> 01:04:32,930
test measures something about somebody

1124
01:04:32,940 --> 01:04:37,750
about her personality if it's a good test is measuring one of these five things

1125
01:04:37,770 --> 01:04:41,830
and that means that as people interacting with one another in the world these are

1126
01:04:41,860 --> 01:04:48,670
five things that we're interested so one of them is an erotic verses stable

1127
01:04:51,120 --> 01:04:53,070
is somebody sort of nineteen

1128
01:04:53,080 --> 01:04:56,640
and in wearing or they com

1129
01:04:56,660 --> 01:04:59,310
x river introvert

1130
01:04:59,840 --> 01:05:04,250
open to experience versus close to experience

1131
01:05:05,410 --> 01:05:08,850
which is a courteous friendly versus non agreeable

1132
01:05:08,860 --> 01:05:10,370
rude selfish

1133
01:05:10,370 --> 01:05:14,840
the value of the appropriate each mistake

1134
01:05:14,850 --> 01:05:18,540
then i can

1135
01:05:18,550 --> 01:05:22,180
capello lower bound on the image page

1136
01:05:22,200 --> 01:05:28,110
OK or even equivalently i can find a lower bound on increase of these difference

1137
01:05:28,280 --> 01:05:30,130
is the same thing

1138
01:05:30,140 --> 01:05:32,600
then i can use the

1139
01:05:32,980 --> 01:05:38,200
one the number of mistakes made by you in terms of the

1140
01:05:38,240 --> 01:05:41,490
object SVM objective

1141
01:05:41,650 --> 01:05:43,080
so this is the

1142
01:05:43,090 --> 01:05:44,680
the clear

1143
01:05:49,990 --> 01:05:52,010
but this is actually easy

1144
01:05:52,020 --> 01:05:59,960
this is actually needed that

1145
01:05:59,980 --> 01:06:03,020
but it was quickly

1146
01:06:03,030 --> 01:06:05,940
in that we just

1147
01:06:05,950 --> 01:06:11,420
so we are interested in by number of things not not the loss of the

1148
01:06:11,420 --> 01:06:14,710
square of the interval in two

1149
01:06:14,720 --> 01:06:17,050
but directly number of mistakes

1150
01:06:17,070 --> 01:06:21,580
so we get a slightly weaker commitment for you you one

1151
01:06:22,570 --> 01:06:23,460
because it is

1152
01:06:23,470 --> 01:06:26,350
angelo describing losses is always nicer

1153
01:06:26,430 --> 01:06:27,330
the with

1154
01:06:33,660 --> 01:06:36,200
we claim

1155
01:06:36,210 --> 01:06:41,150
that whenever he makes mistakes

1156
01:06:42,610 --> 01:06:44,260
that the man of one

1157
01:06:44,270 --> 01:06:46,410
x being

1158
01:06:46,420 --> 01:06:48,740
one of the around

1159
01:06:48,760 --> 01:06:49,930
this implies

1160
01:06:50,050 --> 01:06:54,760
but the value of the dual

1161
01:06:54,770 --> 01:06:58,620
of the game

1162
01:06:58,640 --> 01:07:00,680
when p is chosen

1163
01:07:00,690 --> 01:07:02,420
the optimal values

1164
01:07:02,430 --> 01:07:03,490
at the start

1165
01:07:03,500 --> 01:07:06,150
he was one of four

1166
01:07:06,190 --> 01:07:09,550
is the least

1167
01:07:09,560 --> 01:07:11,660
but these are

1168
01:07:11,680 --> 01:07:15,530
times the

1169
01:07:15,550 --> 01:07:21,210
all that the minus one

1170
01:07:28,550 --> 01:07:31,090
so remember that out the star

1171
01:07:32,560 --> 01:07:34,210
now that i'm not sure how

1172
01:07:34,220 --> 01:07:35,270
this can be

1173
01:07:35,800 --> 01:07:38,770
we know that everything is bigger than one

1174
01:07:39,040 --> 01:07:43,240
and we know what it is but it so it

1175
01:07:43,250 --> 01:07:47,240
once again these people get the mistake bound

1176
01:07:47,260 --> 01:07:50,470
just because a lot the the one

1177
01:07:50,490 --> 01:07:54,620
but the main thing that we know that this is the same

1178
01:07:54,670 --> 01:07:59,300
and then let me take healthy because i'm assuming for simplicity of all the

1179
01:07:59,320 --> 01:08:01,510
writing this many

1180
01:08:01,520 --> 01:08:06,040
that i'm assuming that this is the normalized for just for the sake of simplicity

1181
01:08:06,090 --> 01:08:07,930
and he is at the

1182
01:08:07,940 --> 01:08:10,750
o that the human one

1183
01:08:10,760 --> 01:08:14,510
so lot of the of the guy that we are going to update

1184
01:08:17,560 --> 01:08:19,970
OK two cases

1185
01:08:19,990 --> 01:08:22,050
this is the mean

1186
01:08:22,100 --> 01:08:26,310
i the mean we have to look case one

1187
01:08:26,370 --> 01:08:28,720
i mean is actually by l

1188
01:08:28,740 --> 01:08:36,100
i mean is achieved by LP so the loss of the minus feel that determines

1189
01:08:36,100 --> 01:08:37,640
what's more c

1190
01:08:39,340 --> 01:08:41,740
so now we compute the

1191
01:08:42,180 --> 01:08:44,940
of his are

1192
01:08:45,030 --> 01:08:48,500
we you have written

1193
01:08:48,520 --> 01:08:49,730
on your notes

1194
01:08:51,380 --> 01:08:52,610
every year

1195
01:08:52,630 --> 01:08:55,510
so check should be equal to minus

1196
01:08:57,910 --> 01:09:00,670
which was the first negative but there

1197
01:09:03,230 --> 01:09:06,460
quit because they were the output feedback

1198
01:09:06,480 --> 01:09:09,480
we can expect

1199
01:09:09,500 --> 01:09:11,320
which is

1200
01:09:16,090 --> 01:09:17,880
which is exactly

1201
01:09:17,900 --> 01:09:23,460
but these are have been because of what he is the case

1202
01:09:24,440 --> 01:09:25,590
after the star

1203
01:09:25,610 --> 01:09:29,030
is that because it's

1204
01:09:30,790 --> 01:09:35,730
half of our everyday and i have

1205
01:09:35,780 --> 01:09:38,750
a second case

1206
01:09:38,980 --> 01:09:42,880
this is more the LP

1207
01:09:42,890 --> 01:09:48,070
then the the eighteenth of all the things that are

1208
01:09:49,710 --> 01:09:54,960
we have minus serious growth high

1209
01:09:55,990 --> 01:09:57,930
laugh at sea

1210
01:10:00,920 --> 01:10:02,370
now i know

1211
01:10:03,920 --> 01:10:07,380
c is more entities b and c

1212
01:10:07,400 --> 01:10:11,440
and i know that is positive so i can be

1213
01:10:11,450 --> 01:10:13,220
i can make this

1214
01:10:13,260 --> 01:10:18,000
smaller by replacing one of these is seen year

1215
01:10:20,020 --> 01:10:21,600
one minus the

1216
01:10:21,610 --> 01:10:22,660
l p

1217
01:10:23,220 --> 01:10:24,610
plus a

1218
01:10:24,660 --> 01:10:26,590
l in equal

1219
01:10:31,280 --> 01:10:34,300
which is equal to the game

1220
01:10:34,320 --> 01:10:37,650
how is our

1221
01:10:38,260 --> 01:10:42,030
and and and and they both case

1222
01:10:42,100 --> 01:10:45,540
for what is that we have this

1223
01:10:45,550 --> 01:10:47,440
it inequality here

1224
01:10:49,250 --> 01:10:52,360
we can now what this is about one

1225
01:10:52,420 --> 01:10:55,270
i write it here because something

1226
01:10:55,330 --> 01:10:56,750
i have

1227
01:10:58,580 --> 01:11:01,170
so what we know here we can write

1228
01:11:02,700 --> 01:11:09,090
WP minus one is the indicator function of the one making a mistake that he

1229
01:11:11,460 --> 01:11:12,590
this is

1230
01:11:12,610 --> 01:11:14,550
find mean

1231
01:11:15,880 --> 01:11:18,900
see common ones

1232
01:11:19,760 --> 01:11:22,580
this is that most

1233
01:11:24,020 --> 01:11:25,830
what is the most that

1234
01:11:25,840 --> 01:11:27,070
and that

1235
01:11:27,860 --> 01:11:30,410
of WP minus one

1236
01:11:30,420 --> 01:11:32,170
why because that mistakes

1237
01:11:32,180 --> 01:11:35,770
this guy bigger than one and this is an indicator function

1238
01:11:35,790 --> 01:11:39,980
times we

1239
01:11:39,980 --> 01:11:41,650
the extra row and column of cake

1240
01:11:43,730 --> 01:11:48,880
that's why it's difficult to embed new data points in all these algorithms they actually

1241
01:11:48,880 --> 01:11:52,860
tend to work with a fixed number they point however it is trivial to introduce

1242
01:11:52,900 --> 01:11:53,760
new data feature

1243
01:11:55,630 --> 01:11:59,630
so that's something we're not used to thinking about doing because normally we won't parametric

1244
01:11:59,630 --> 01:12:04,300
models i theta model so i'm going to classify all you people here as

1245
01:12:05,400 --> 01:12:05,880
i don't know

1246
01:12:07,730 --> 01:12:08,840
what i classifier was

1247
01:12:09,820 --> 01:12:12,380
something classifier used as as as the

1248
01:12:13,510 --> 01:12:15,150
of friends or enemies now

1249
01:12:15,550 --> 01:12:18,570
and every time i see one if you like classifier as a friend or enemy

1250
01:12:18,570 --> 01:12:21,710
of data about u i know what you're likes and dislikes are yeah

1251
01:12:22,880 --> 01:12:25,670
but what happens if i find a new big data about

1252
01:12:26,510 --> 01:12:29,320
what do i have to do in the standard parametric modeling framework

1253
01:12:30,570 --> 01:12:32,320
i have to learn a new model

1254
01:12:33,570 --> 01:12:38,090
so if you wanted say if i'm asking questions and deciding whether i like or dislike u

1255
01:12:38,690 --> 01:12:39,230
and i say

1256
01:12:39,990 --> 01:12:43,360
do you like or new feature

1257
01:12:43,860 --> 01:12:48,820
how to accommodate within a model which is a classic independently across data points i can't

1258
01:12:49,400 --> 01:12:50,630
not without refitting the model

1259
01:12:51,650 --> 01:12:55,420
but in this model i can trivially just as you can introduce new data so

1260
01:12:55,510 --> 01:12:57,280
that fixed number of people in this room

1261
01:12:58,820 --> 01:13:01,690
this is the right way of doing it someone else walks in the room

1262
01:13:02,280 --> 01:13:04,260
and i have problems i have to rethink this model

1263
01:13:05,670 --> 01:13:09,010
but model every time ask a new question i have to refit the model

1264
01:13:11,250 --> 01:13:13,190
the point in i mean the lesson from this bit is

1265
01:13:14,110 --> 01:13:17,380
just because you what all things in a certain way doesn't have to be the

1266
01:13:17,380 --> 01:13:18,880
way you think about things in the future

1267
01:13:19,260 --> 01:13:23,400
because large computers and so on and so forth mean that things are slightly different

1268
01:13:23,420 --> 01:13:27,510
this is always from the classical way statisticians will teach you things and i think

1269
01:13:27,510 --> 01:13:31,400
it's much more interesting to think in these weights consequence model definition

1270
01:13:32,010 --> 01:13:37,250
there are problems by doing models way as we just described but there are also things things become easier

1271
01:13:39,230 --> 01:13:42,630
so what's going on here another point about this is

1272
01:13:43,090 --> 01:13:46,940
the covariance interpretation we were looking at before about how we get the distances from

1273
01:13:46,940 --> 01:13:52,090
these models what we now know is this is a new kernel matrix is the

1274
01:13:52,130 --> 01:13:54,360
inverse about classy we saw before

1275
01:13:55,320 --> 01:13:59,050
so in all these models what we were doing with canopy see and view is

1276
01:13:59,050 --> 01:14:01,010
we are doing and i can value decomposition on kauai

1277
01:14:01,570 --> 01:14:03,190
taking the largest eigen values

1278
01:14:06,340 --> 01:14:12,210
we're seeing that little plasticine is inverse okay plus a diagonal constant that doesn't affect the i can vectors

1279
01:14:14,340 --> 01:14:16,460
so what that's saying is

1280
01:14:19,550 --> 01:14:22,730
me skip because i can talk about and realize how about the next

1281
01:14:25,210 --> 01:14:25,590
and also

1282
01:14:25,990 --> 01:14:26,690
short time

1283
01:14:28,440 --> 01:14:30,630
the i can decomposition and the covariance is this

1284
01:14:32,900 --> 01:14:35,210
the i can decomposition the laplacian is fact

1285
01:14:38,460 --> 01:14:39,750
so how these related

1286
01:14:40,590 --> 01:14:41,920
the i can vectors are the same

1287
01:14:42,690 --> 01:14:44,760
but the igon values are one over each other

1288
01:14:45,760 --> 01:14:47,260
and there's also this annoying little guy

1289
01:14:48,420 --> 01:14:49,670
he doesn't affect the i vectors

1290
01:14:50,420 --> 01:14:51,840
he has to be there for the mass

1291
01:14:53,690 --> 01:14:55,070
principal eigen values okay

1292
01:14:55,690 --> 01:14:58,010
are the smallest igon values for all

1293
01:14:59,960 --> 01:15:01,340
so in other words

1294
01:15:02,280 --> 01:15:07,090
when you're doing the smallest igon values at the laplacian igon maps and locally linear embedding

1295
01:15:07,710 --> 01:15:11,960
you're just doing the same thing you're doing kernel pieces maximum variance unfolding

1296
01:15:12,440 --> 01:15:14,070
but you're operating on the glacier

1297
01:15:14,690 --> 01:15:17,070
why is operating on passing a good idea

1298
01:15:23,440 --> 01:15:24,650
where they come from where

1299
01:15:26,630 --> 01:15:27,780
you need to compute the inverse

1300
01:15:28,420 --> 01:15:31,170
actually don't really invested in igon value problem anyway

1301
01:15:31,920 --> 01:15:35,690
because was you i can values the universe is just a matter of inverting

1302
01:15:36,300 --> 01:15:37,250
he igon values

1303
01:15:38,670 --> 01:15:40,190
you don't need to compute

1304
01:15:41,210 --> 01:15:42,130
like about problem

1305
01:15:43,340 --> 01:15:47,840
so because the power methods and so on and so forth i can vectors and this easy to extract

1306
01:15:48,860 --> 01:15:50,900
i convicts on this slow to extract

1307
01:15:51,670 --> 01:15:53,820
so you should if you can operate on this

1308
01:15:55,760 --> 01:15:59,710
and what this i mean i haven't what is through maximum variance unfolding what the

1309
01:15:59,710 --> 01:16:03,990
implications are that it would be really nice if you could maximum variance unfolding never

1310
01:16:03,990 --> 01:16:08,590
specify capability that's why slow you gotta specify the full matrix kay

1311
01:16:09,010 --> 01:16:10,860
which is actually depend on some sparse

1312
01:16:11,340 --> 01:16:12,590
not network underneath

1313
01:16:13,170 --> 01:16:15,250
so nice algorithm it it's slows the result

1314
01:16:16,400 --> 01:16:21,800
isomap doesn't quite fit into this framework doesn't because isomap we never actually that they

1315
01:16:21,840 --> 01:16:24,880
whenever define the probabilistic model we don't look a little plastic

1316
01:16:25,880 --> 01:16:30,860
what goes on in isomap is your feeling in you can work out what implicit kay was

1317
01:16:31,400 --> 01:16:32,380
you're not filling in

1318
01:16:33,340 --> 01:16:34,550
the elements okay by

1319
01:16:35,300 --> 01:16:39,940
maximum likelihood or anything as i'm suggesting hear just putting them by shortest distance on a graph

1320
01:16:40,990 --> 01:16:44,300
it turns out have short on time so you have the time to go into

1321
01:16:44,320 --> 01:16:50,130
the details on this but the distances that are associated with these models are effectively

1322
01:16:50,130 --> 01:16:54,920
random walks on the graph going across all possible routes and the sum across all

1323
01:16:54,920 --> 01:16:58,590
diffusion distances on graphs so in one case

1324
01:16:59,010 --> 01:16:59,260
for the

1325
01:16:59,820 --> 01:17:00,630
last in models

1326
01:17:01,300 --> 01:17:08,380
what you the distances they are implied are the distances associated with summing across all possible routes in the graph

1327
01:17:08,380 --> 01:17:12,100
point is born up here which is then

1328
01:17:12,150 --> 01:17:16,350
splits into two critical points and then t is equal to five

1329
01:17:16,400 --> 01:17:21,670
the first critical point which came from way back merges with one of these two

1330
01:17:21,670 --> 01:17:23,850
and the other one survives and goes on

1331
01:17:23,870 --> 01:17:30,500
for the first question that follows the situation of the critical points of the function

1332
01:17:30,500 --> 01:17:34,000
f as time goes on depending on time

1333
01:17:34,210 --> 01:17:37,270
one it again

1334
01:17:37,270 --> 01:17:41,040
here it goes

1335
01:17:41,040 --> 01:17:43,690
OK so this is a very very simple example

1336
01:17:43,710 --> 01:17:45,880
just to illustrate

1337
01:17:45,900 --> 01:17:47,560
what for

1338
01:17:47,730 --> 01:17:52,560
talking about here

1339
01:17:52,580 --> 01:18:01,350
so this parameter all this algorithm was actually applied to serious computer science application

1340
01:18:01,350 --> 01:18:07,250
taking part of it so this is this was used to design an algorithm for

1341
01:18:07,250 --> 01:18:12,350
teaching a robot the concept of occlusion so robot with an onboard camera

1342
01:18:12,460 --> 01:18:19,730
was or was taking pictures of simple scene which if i understand correctly consisted of

1343
01:18:20,690 --> 01:18:24,650
coloured balls and and then as the robot is moving around these two boss at

1344
01:18:24,650 --> 01:18:28,230
certain stages you see both of them and then eventually one of them will be

1345
01:18:28,230 --> 01:18:32,190
covered by the other one and now the analysis of the critical points on the

1346
01:18:32,190 --> 01:18:37,580
images or give give you information about the song clues and this was used to

1347
01:18:37,580 --> 01:18:41,710
be devised an algorithm for teaching a robot the concept

1348
01:18:44,540 --> 01:18:50,920
here's an example that we did together with gregory set which shows the naive way

1349
01:18:50,920 --> 01:18:56,850
of using well and they've approach of using this parameter discrete morse theory for sequential

1350
01:18:56,850 --> 01:19:02,020
image analysis analysis now for computer scientists which is the majority of the population here

1351
01:19:02,400 --> 01:19:05,830
i apologize because this is not the serious and i know that there are many

1352
01:19:05,830 --> 01:19:11,150
many many very good algorithms for tracing figures along images

1353
01:19:11,190 --> 01:19:15,650
in the field for example in six sequence of images but i believe strongly that

1354
01:19:16,120 --> 01:19:17,230
this is just the

1355
01:19:17,250 --> 01:19:21,540
an experiment and i really believe that it's a good way to try to devise

1356
01:19:21,540 --> 01:19:25,620
a good algorithm for doing this but there's a lot of work a lot of

1357
01:19:25,620 --> 01:19:29,150
programming and computer science involved in this and were just

1358
01:19:29,230 --> 01:19:31,150
simple mathematician so

1359
01:19:31,190 --> 01:19:33,210
we do what we can do

1360
01:19:33,270 --> 01:19:35,400
OK so let's see

1361
01:19:40,730 --> 01:19:45,040
the first part of his phd thesis

1362
01:19:45,060 --> 01:19:46,600
OK so let's see here is the

1363
01:19:46,990 --> 01:19:52,310
experiment which we came up with

1364
01:19:54,650 --> 01:19:55,750
so we go

1365
01:19:55,770 --> 01:19:59,850
so we have this very nice cure going down three great slope

1366
01:19:59,880 --> 01:20:04,440
and the algorithms follows the year so the idea here is that you're looking at

1367
01:20:04,440 --> 01:20:07,540
the great grayscale values

1368
01:20:07,540 --> 01:20:12,940
the pixels in the image as discrete morse function what you're extending the discrete morse

1369
01:20:12,940 --> 01:20:17,520
function of the regularisation of the domain so the domain is just the square here

1370
01:20:17,940 --> 01:20:22,400
triangulation is a simple thing going to extend this to a discrete morse function another

1371
01:20:22,480 --> 01:20:27,480
figure would be local maximum of the the discrete morse function of course there are

1372
01:20:27,480 --> 01:20:31,400
many of them and we're just following one of them so there are several maximal

1373
01:20:31,400 --> 01:20:35,980
several critical points and this is just the one that we we concentrated on and

1374
01:20:36,350 --> 01:20:40,170
as you can see the algorithm follows the skier quite twelve

1375
01:20:40,190 --> 01:20:42,460
there's another really nice

1376
01:20:42,520 --> 01:20:47,480
between the beginning see here in the beginning two critical points one of them is

1377
01:20:47,480 --> 01:20:49,790
the here the other one is the shadow

1378
01:20:49,810 --> 01:20:54,270
kyrgyz jumped off a cliff and flying and then when he when he comes down

1379
01:20:54,520 --> 01:20:59,150
he joined the shadow the critical points more the united

1380
01:20:59,270 --> 01:21:01,650
it seems to work quite well here

1381
01:21:03,120 --> 01:21:08,460
the problem of noise which is just mentioned before and i really didn't go into

1382
01:21:09,000 --> 01:21:13,880
is really relevant here because you can imagine that an image like this has many

1383
01:21:13,880 --> 01:21:18,460
many many critical points they're much too many of them there are little saddle sort

1384
01:21:18,460 --> 01:21:22,980
of pure over here which really are not interested just little the answers in the

1385
01:21:22,980 --> 01:21:26,770
image or you can have some actual laws

1386
01:21:26,790 --> 01:21:31,290
you can just have a little

1387
01:21:31,330 --> 01:21:34,940
sift in the value of the grayscale function which would like to ignore

1388
01:21:34,980 --> 01:21:39,920
the way this is done is it's to really nice mechanism which is caused cancelling

1389
01:21:39,920 --> 01:21:46,190
critical points if you have to two critical points local maxima

1390
01:21:46,290 --> 01:21:49,290
and the minimum

1391
01:21:49,440 --> 01:21:56,250
like sorry a maximum and the saddle someplace over here and if you have a

1392
01:21:56,350 --> 01:22:00,100
discrete of going from one to the other

1393
01:22:00,100 --> 01:22:06,190
such that the function values along this path so this is the maximum for this

1394
01:22:06,190 --> 01:22:12,540
in nature but the next step of what he is interested in what rather because

1395
01:22:12,630 --> 01:22:18,560
according of this information is not meaningful that's the problem of this is in our

1396
01:22:18,560 --> 01:22:23,310
log fire and the idea of this approach is that

1397
01:22:23,330 --> 01:22:30,000
he tried to encode all the information which the use of corresponding to an ontology

1398
01:22:30,000 --> 01:22:35,760
so we have a concept it's a few of our website as onto a pass

1399
01:22:35,800 --> 01:22:42,590
and every question which was asked corresponds to an concept every click corresponds to an

1400
01:22:42,590 --> 01:22:47,440
concept and now we have if we do not only capture

1401
01:22:48,670 --> 01:22:50,600
the html

1402
01:22:53,600 --> 01:22:59,980
do right so if he also capture to the concept corresponding to do well in

1403
01:22:59,980 --> 01:23:03,600
our log file then you know what the user are looking for the and you

1404
01:23:03,600 --> 01:23:10,350
know what he percent today and today we can analyse this semantic log file easier

1405
01:23:10,350 --> 01:23:17,630
easily and they need no further steps to look behind the information that's another idea

1406
01:23:18,050 --> 01:23:24,430
to enrich or to to easily analyse usage five usage log fires

1407
01:23:24,470 --> 01:23:29,250
the help of ontology that's not only to get some

1408
01:23:31,330 --> 01:23:34,210
so let's come

1409
01:23:34,290 --> 01:23:38,670
to last application where we can try to work

1410
01:23:38,720 --> 01:23:40,330
you know

1411
01:23:40,350 --> 01:23:45,970
some ontology is so i talked about this crawl document and this focused crawler and

1412
01:23:46,010 --> 01:23:51,900
you can control the crawler to find some information about a certain topic drift ontology

1413
01:23:52,470 --> 01:23:58,850
and i suppose we have to this set of documents crawled and could it describes

1414
01:23:59,290 --> 01:24:04,390
some information in the next step would be to present decision documents to use and

1415
01:24:04,560 --> 01:24:08,760
if it's a lot want them and then we have a lot of documents quality

1416
01:24:09,210 --> 01:24:14,890
and the idea the next step is to control to class do information if to

1417
01:24:14,890 --> 01:24:19,630
help of ontology we have a description what interested in and we will be interested

1418
01:24:19,630 --> 01:24:26,720
also in find some groups softies documents which relate to each other i each other

1419
01:24:26,720 --> 01:24:32,970
so we cluster this information corresponding to all ontology we get some info to get

1420
01:24:32,970 --> 01:24:38,670
some clusters and we presented this to help of the ontology to the use of

1421
01:24:38,670 --> 01:24:44,290
this information so we have now an insight into to collect some information and not

1422
01:24:44,290 --> 01:24:49,400
only then will the decisive will normally you type in some keywords and then you

1423
01:24:49,400 --> 01:24:54,440
get a list of that pages in this may you give our system and stopping

1424
01:24:54,440 --> 01:25:00,290
point somewhere and then he tries to find the corresponding that pages and group them

1425
01:25:00,290 --> 01:25:05,600
together way that you have a chance to understand them percent and it's another example

1426
01:25:05,780 --> 01:25:10,290
that we are working on quite successful i presented some thing to marko and some

1427
01:25:10,290 --> 01:25:12,860
other yesterday that

1428
01:25:12,890 --> 01:25:16,290
and we tried to use ontology is tool

1429
01:25:16,300 --> 01:25:18,390
present and analyse

1430
01:25:18,980 --> 01:25:33,670
data so i give back to your cave cubicles you're issues because you can get

1431
01:25:35,680 --> 01:25:44,710
so last but not least we are coming to an end

1432
01:25:44,720 --> 01:25:49,680
reason where we are heading for what do we see as the next steps that's

1433
01:25:49,680 --> 01:25:54,750
what is important for us and this is basically work that was performed for project

1434
01:25:54,750 --> 01:25:59,390
proposal in collaboration with marko and some other partners which might be of interest for

1435
01:25:59,390 --> 01:26:00,250
you too

1436
01:26:00,360 --> 01:26:05,510
because it also shows the directions where we are actually working on so for the

1437
01:26:05,510 --> 01:26:08,670
first eighteen months we see that we need

1438
01:26:08,670 --> 01:26:13,800
a tighter combination and integration of three core technologies which is on the one hand

1439
01:26:13,800 --> 01:26:19,830
ontology and metadata technology which we seen a little bit then also human language technology

1440
01:26:19,830 --> 01:26:22,640
and knowledge discovery so these are the three key

1441
01:26:22,810 --> 01:26:26,630
technology which we see are necessary

1442
01:26:26,640 --> 01:26:29,560
to provide the basic infrastructure

1443
01:26:29,600 --> 01:26:36,220
then for the next three years we will transfer this combined technology into other application

1444
01:26:36,220 --> 01:26:40,980
areas of not only semantic web as we presented today we have some mean machine

1445
01:26:40,980 --> 01:26:46,850
process of all information for example within websites but also

1446
01:26:47,180 --> 01:26:51,830
you have other application areas like web services peer-to-peer multimedia and you can imagine a

1447
01:26:51,830 --> 01:26:55,130
lot of other ones but these are the the key ones at the european commission

1448
01:26:55,130 --> 01:26:59,480
for currently and what we see as the far goal

1449
01:26:59,510 --> 01:27:01,560
so this is really our vision

1450
01:27:01,600 --> 01:27:02,550
is that

1451
01:27:02,560 --> 01:27:07,350
there's no boundary between the document management content management and a knowledge retrieval so if

1452
01:27:07,350 --> 01:27:11,640
you look at the current product market what can you get from companies what they

1453
01:27:11,640 --> 01:27:16,090
sell well you have a document management a content management and you have a knowledge

1454
01:27:16,090 --> 01:27:21,180
retrieval on top somehow but there's no really integration and if you are the worker

1455
01:27:21,180 --> 01:27:24,550
in front of your are for example you have to switch between different tools and

1456
01:27:24,550 --> 01:27:29,590
applications to perform your daily working task and what we see is that there will

1457
01:27:29,600 --> 01:27:34,140
be no boundaries in between so like for example microsoft is trying to combine all

1458
01:27:34,140 --> 01:27:39,140
the products into one we also see that these different application areas will grow together

1459
01:27:39,180 --> 01:27:43,130
so that you don't have to switch your tools to to achieve your goals as

1460
01:27:43,130 --> 01:27:45,980
the so-called knowledge workers

1461
01:27:46,020 --> 01:27:52,850
so in a nutshell the knowledge management tasks hopefully become an almost effortless part of

1462
01:27:52,850 --> 01:27:56,360
day-to-day activity so that's the marketing story behind that

1463
01:27:56,440 --> 01:28:03,300
and we hope to achieve that end hopefully upcoming so-called sect project where we will

1464
01:28:03,300 --> 01:28:08,510
closely collaborate with marko and his group and some other groups for example the competence

1465
01:28:08,510 --> 01:28:14,550
centre in sheffield for human language technologies but for example also major industrial players like

1466
01:28:14,550 --> 01:28:20,110
and pulleys and bertelsmann so we see that this is a quite challenging task is

1467
01:28:20,110 --> 01:28:23,830
a nice project and we're looking forward to working there

1468
01:28:23,830 --> 01:28:28,330
so last but not least i hope you enjoyed our tutorial

1469
01:28:28,350 --> 01:28:33,750
i felt quite comfortable you learnt something hopefully a lot and if you're interested you

1470
01:28:33,750 --> 01:28:36,180
can also look at our website we have

1471
01:28:36,210 --> 01:28:41,800
some publications available you can also contact us personally if you have detailed questions please

1472
01:28:41,800 --> 01:28:43,130
feel free to do so

1473
01:28:44,710 --> 01:28:46,790
thank you very much

1474
01:28:46,830 --> 01:28:54,390
you have questions

1475
01:29:02,690 --> 01:29:17,830
so our oldest languages which are if you want to say well is

1476
01:29:17,830 --> 01:29:22,210
one of the things he was saying only

1477
01:29:22,330 --> 01:29:23,510
in each

1478
01:29:23,600 --> 01:29:30,920
or you can do not mentioned two which is done and then found out that

1479
01:29:31,040 --> 01:29:35,180
and and so there's a little bit of history is the first of all we

1480
01:29:35,190 --> 01:29:40,330
have a lot of different representation formalism especially if you look into the description logics

1481
01:29:40,330 --> 01:29:48,970
community specialised dialect of description logic to reduce complexity when different as to make it

1482
01:29:49,670 --> 01:29:52,640
to make it a solvable task

1483
01:29:52,670 --> 01:29:58,140
if you look at the history the current standards then the european initiative called which

1484
01:29:58,140 --> 01:30:06,290
was part of a tremendous project then the american initiative called back and then they

1485
01:30:06,290 --> 01:30:11,150
once all N customers have come in and sat down around tables every table is

1486
01:30:11,150 --> 01:30:17,250
gonna correspond to a cluster in our partition and basically the customers correspond to

1487
01:30:17,250 --> 01:30:22,550
elements of our S of our set S and tables correspond to clusters in our partition

1488
01:30:22,550 --> 01:30:29,750
row okay one of the the main property of the Chinese restaurant process is that it

1489
01:30:29,750 --> 01:30:33,690
has this rich gets richer property right so if we have a table with lots of

1490
01:30:33,690 --> 01:30:38,610
customers then the chance of new customer the comes sitting at that table will

1491
01:30:38,610 --> 01:30:44,850
be large as well because this N S term will be large so this is

1492
01:30:44,850 --> 01:30:49,230
quite useful because if you think about clustering right so if you see that there's

1493
01:30:49,230 --> 01:30:54,210
a there's a large cluster of objects then the chance of a new object that

1494
01:30:54,210 --> 01:31:00,950
you observe next coming from that cluster will also be large because that's kind of

1495
01:31:00,950 --> 01:31:04,190
if we have a large object then you expect to see more of that type

1496
01:31:04,190 --> 01:31:10,450
of object in the future okay you can also multiply this conditional

1497
01:31:10,450 --> 01:31:15,390
probabilities together and  that gives that gives us the overall probability of the

1498
01:31:15,390 --> 01:31:22,930
partition probability of row given alpha and it has a fall given by this okay

1499
01:31:22,950 --> 01:31:28,970
and this is called the exchangeable partition probability function is basically a probability function

1500
01:31:28,980 --> 01:31:34,130
tells us tells you the probability of a partition under this process and is exchangeable I'll

1501
01:31:34,320 --> 01:31:40,710
I'll come to that a bit later okay again we can work out you know as just as

1502
01:31:40,710 --> 01:31:45,230
for the the Dirichlet process we can work out the mean and the variance

1503
01:31:45,230 --> 01:31:50,490
of this process and one  thing one  quantity which we'll be interested in will be

1504
01:31:50,490 --> 01:31:56,710
the  the number of clusters in our partition okay so the expected basically this

1505
01:31:56,710 --> 01:32:00,870
is the prior mean of the number of clusters so the total number of clusters

1506
01:32:00,870 --> 01:32:10,110
in row in a among partitions of the set one to N keep

1507
01:32:10,250 --> 01:32:14,770
parameterized by alpha you can work out that that equals to something which looks

1508
01:32:14,770 --> 01:32:22,970
like this okay is basically alpha times log of one plus N divided by alpha  similarly you

1509
01:32:22,970 --> 01:32:26,710
can also work out that terms are interestingly that the variance of the number of

1510
01:32:26,710 --> 01:32:35,750
clusters is also gonna be exactly the same form okay so approximately really okay so here's a plot

1511
01:32:35,750 --> 01:32:41,370
which shows what's happening so we have a number of clusters that a number of

1512
01:32:41,370 --> 01:32:47,470
customers that increases from zero to ten  thousand and then on the Y axis we've got the

1513
01:32:47,480 --> 01:32:51,890
table that the customer sat at so the first customer was at a table one and the

1514
01:32:51,890 --> 01:32:58,430
second table and that's gonna be the second table the third table the forth

1515
01:32:58,440 --> 01:33:03,830
table and so forth and we're gonna plot a point where on the table that

1516
01:33:03,830 --> 01:33:09,970
the cust that customers sat at okay and we see that as the number of customers grows

1517
01:33:09,990 --> 01:33:13,950
the number of tables will keep on growing as well and the growth rate of

1518
01:33:13,960 --> 01:33:20,970
the number of tables is logarithmic in N and the other thing which depend

1519
01:33:20,970 --> 01:33:26,750
which this depends on is that it also depends on alpha so that if alpha is large then

1520
01:33:26,750 --> 01:33:32,580
we expect we can expect more tables and if alpha is small we tend to expect less

1521
01:33:32,580 --> 01:33:38,930
tables and the variance in the number of tables or the number of clusters

1522
01:33:38,930 --> 01:33:42,970
is in fact quite tight so if we think about the standard deviation as the

1523
01:33:42,970 --> 01:33:47,770
square or  the variance then the standard deviation's typically smaller than the expectation and than

1524
01:33:47,780 --> 01:33:52,690
the expected number of tables okay this actually has and if I'll come back to

1525
01:33:52,690 --> 01:33:58,830
this a bit later when we talk about how we might use this for clustering

1526
01:33:58,830 --> 01:34:05,890
this histogram is just  we sample from the number of clusters under our Chinese

1527
01:34:05,890 --> 01:34:09,830
restaurant  process logs of times and we just plot the histogram of the number of

1528
01:34:09,830 --> 01:34:17,180
times that we might we observe a certain number of tables oh no we

1529
01:34:17,180 --> 01:34:22,190
fix the number of customers to be a hundred and we draw lots of samples

1530
01:34:22,190 --> 01:34:26,680
we we we run this process lots of times every time we're gonna get

1531
01:34:26,750 --> 01:34:31,090
a number of tables and that number tables is gonna be random right and we're

1532
01:34:31,090 --> 01:34:34,470
gonna discollect the number of times that we see that number of tables among

1533
01:34:34,480 --> 01:34:40,870
that among one hundred customers okay and we can now plot  the histogram and it looks basically like

1534
01:34:40,870 --> 01:34:46,310
this and the mean is gonna be that quantity and the var the variance is also

1535
01:34:46,310 --> 01:34:59,870
gonna be that quantity okay yes now there's actually not no emphasis is it's just that it's

1536
01:34:59,870 --> 01:35:07,470
a perceptual thing right because we if I just plot down a random number of points on

1537
01:35:07,470 --> 01:35:11,870
some area then you can you cannot see the boundary of that area and in

1538
01:35:11,870 --> 01:35:20,530
this case the boundary of the area is given by that that curve there okay so how

1539
01:35:20,530 --> 01:35:26,770
OK so we collected off here on the project that have actually has had a

1540
01:35:26,770 --> 01:35:33,080
very good question what's needed to celebrate what what what to lose with the project

1541
01:35:33,700 --> 01:35:37,390
you actually lose the bit i mean works well it's kind of elegant but it's

1542
01:35:37,390 --> 01:35:42,780
nowhere near as good as the simultaneous out and there's a couple reasons for that

1543
01:35:44,120 --> 01:35:47,550
for this reason so it's past and so it has a lot of good uses

1544
01:35:47,870 --> 01:35:51,870
will actually show it using a working

1545
01:35:51,890 --> 01:35:53,570
so what i want to show that

1546
01:35:53,600 --> 01:35:55,250
civil work

1547
01:36:06,150 --> 01:36:09,010
and this is just a kind of a hack together once is this is that

1548
01:36:09,010 --> 01:36:11,760
they are kind of state-of-the-art was still kind of

1549
01:36:11,780 --> 01:36:14,530
getting this but hopefully with you

1550
01:36:14,560 --> 01:36:24,070
help you with your relation

1551
01:36:26,290 --> 01:36:29,420
it's a little bit there but they can be about

1552
01:36:29,430 --> 01:36:35,320
so that using the so this is off the shelf and b and saying it's

1553
01:36:35,320 --> 01:36:36,290
just basically

1554
01:36:37,230 --> 01:36:39,210
matrix multiplications

1555
01:36:39,650 --> 01:36:45,090
referring to actually doing every frame we're doing twenty five twenty five iterations

1556
01:36:45,100 --> 01:36:49,560
and that's a pretty cool you can kind of

1557
01:36:49,560 --> 01:36:52,100
your nomination was kind of doing

1558
01:36:52,150 --> 01:36:54,590
having a problem like about another

1559
01:36:54,620 --> 01:36:56,620
this is what we did for the discovery channel

1560
01:36:56,640 --> 01:36:59,230
with a

1561
01:37:01,010 --> 01:37:07,820
a kind of greece because we can always skipping ahead but somehow to protect them

1562
01:37:12,090 --> 01:37:16,090
so is my friend ralph

1563
01:37:16,150 --> 01:37:16,790
and so

1564
01:37:16,810 --> 01:37:20,040
this is kind of showing him just in the kind of living space around

1565
01:37:21,460 --> 01:37:22,950
that's the sort of

1566
01:37:22,960 --> 01:37:24,140
there go

1567
01:37:24,150 --> 01:37:30,290
so this can maybe and smiling

1568
01:37:30,360 --> 01:37:34,290
so this is real time systems records time was in the sequence

1569
01:37:34,290 --> 01:37:40,700
things and then we incorporated the face recognition and so it's green recognising

1570
01:37:41,740 --> 01:37:44,420
and then you could walk off

1571
01:37:44,480 --> 01:37:49,990
it was built

1572
01:37:52,170 --> 01:37:54,290
the tracks me

1573
01:37:56,600 --> 01:37:59,130
and this is not real because right

1574
01:37:59,170 --> 01:38:03,230
was another thing is the the nice thing with this stuff to is that this

1575
01:38:03,260 --> 01:38:06,390
whole problem so i like expression variation

1576
01:38:06,390 --> 01:38:09,720
and all these other things that are problems with advice as soon as you start

1577
01:38:09,720 --> 01:38:14,330
doing the registration and it all falls apart because you actually can normalise first expression

1578
01:38:14,330 --> 01:38:18,700
you can normalize pose as opposed to dealing with these kind of these boxes

1579
01:38:18,720 --> 01:38:21,240
things and so

1580
01:38:21,540 --> 01:38:24,580
so it's kind of funny because it's going to it's coming away from this box

1581
01:38:24,580 --> 01:38:30,610
detectors and we're trying to do stuff now and ameicans people walking and we're also

1582
01:38:31,510 --> 01:38:37,170
doing a lot of stuff on actually structure from motion so basically what can i

1583
01:38:37,170 --> 01:38:40,410
from this viewpoint of my advisor by register properly

1584
01:38:40,420 --> 01:38:44,260
can i infer what i look like france is actually a bit of work done

1585
01:38:44,260 --> 01:38:46,450
on this famous work by

1586
01:38:46,490 --> 01:38:50,390
a guy called volker blanz and he's really good work on that but the problem

1587
01:38:50,390 --> 01:38:53,880
with his work is that he does a really good job inferring the frontal view

1588
01:38:53,880 --> 01:38:58,130
but it doesn't do a good job of actually doing tracking for the registration for

1589
01:38:58,170 --> 01:39:02,480
patients so we're trying to do both things at the same time so that we

1590
01:39:03,170 --> 01:39:06,940
one asked more about it and this was is or cut up to i see

1591
01:39:07,760 --> 01:39:10,110
magic in the school

1592
01:39:10,140 --> 01:39:12,830
in the math and we were all together

1593
01:39:12,850 --> 01:39:16,040
you too could put that together pretty simple so

1594
01:39:17,100 --> 01:39:20,660
so we can keep on going going with that so the thing that so that

1595
01:39:20,660 --> 01:39:24,570
was project this or that and so the nice thing is that because of that

1596
01:39:24,570 --> 01:39:30,890
it was projecting out the appearance variation so maybe some illumination changes in

1597
01:39:30,890 --> 01:39:34,420
prices and things but you do those things because

1598
01:39:36,910 --> 01:39:42,040
with the simultaneous of

1599
01:39:42,120 --> 01:39:44,790
i'm actually at every iteration

1600
01:39:44,850 --> 01:39:48,870
i'm getting what update in an appearance

1601
01:39:48,880 --> 01:39:52,860
and what i'm doing also is instead of with so with project at every iteration

1602
01:39:52,860 --> 01:39:53,790
i'm essentially

1603
01:39:53,840 --> 01:39:55,470
projecting out

1604
01:39:55,500 --> 01:39:59,480
this appearance of the forgetting about i'm not doing anything with it

1605
01:39:59,530 --> 01:40:03,030
so i said you kind of projected but with the simultaneous that one does is

1606
01:40:03,030 --> 01:40:07,930
it takes it encompasses well i know that this is how much the appearance changed

1607
01:40:07,930 --> 01:40:10,330
a lot that's no longer

1608
01:40:10,350 --> 01:40:11,690
i'm going to update

1609
01:40:11,710 --> 01:40:15,290
and so what actually happens is that instead of me trying to

1610
01:40:15,400 --> 01:40:19,280
slowly just rely on everything in terms of the war

1611
01:40:19,330 --> 01:40:21,960
intense my optimisation on country

1612
01:40:21,990 --> 01:40:25,730
increasing knowledge intensity appearance increasing knowledge in terms of the war

1613
01:40:25,730 --> 01:40:29,630
both at the same time and so the sum of ten is the is inherently

1614
01:40:30,000 --> 01:40:32,250
a lot nicer in a lot better so

1615
01:40:32,300 --> 01:40:37,480
there has actually been i'm not covering it here because it's still a really kind

1616
01:40:37,480 --> 01:40:39,330
of far research topic

1617
01:40:39,370 --> 01:40:45,270
but there's been some really interesting work done by a guy called adrian but only

1618
01:40:45,290 --> 01:40:50,150
a ring road in europe and he's done some stuff called is called tool

1619
01:40:50,160 --> 01:40:51,460
inverse composition

1620
01:40:51,470 --> 01:40:56,790
and what he proposed is that you can not only invest compose

1621
01:40:56,800 --> 01:40:57,860
the geometry

1622
01:40:57,880 --> 01:41:01,300
of the shape you can actually invest compose the texture

1623
01:41:01,480 --> 01:41:04,710
and you can actually think of actually

1624
01:41:04,730 --> 01:41:08,750
you can actually think of one of the following what's on the texture so you

1625
01:41:08,750 --> 01:41:10,420
can have like game by

1626
01:41:10,480 --> 01:41:12,400
so i like to into the

1627
01:41:12,410 --> 01:41:14,170
in seeking three points

1628
01:41:14,170 --> 01:41:14,900
and then

1629
01:41:14,920 --> 01:41:18,340
in one d the final is essentially just

1630
01:41:18,380 --> 01:41:21,330
where positioned in the scale

1631
01:41:21,680 --> 01:41:22,680
and so

1632
01:41:22,710 --> 01:41:24,410
because it's composable

1633
01:41:24,420 --> 01:41:29,430
you could in theory invest compose and so he sent stuff for investment posing for

1634
01:41:29,430 --> 01:41:30,840
the game boy as

1635
01:41:30,880 --> 01:41:32,380
it's a lot trickier

1636
01:41:32,410 --> 01:41:37,650
and it's interesting we we think at the moment it can be done but it's

1637
01:41:37,650 --> 01:41:40,640
right things are for so everyone so yeah

1638
01:41:41,090 --> 01:41:46,600
i've been tasked to essentially given introduction on neural nets

1639
01:41:47,500 --> 01:41:50,260
so what i'm going to talk about this fairly basic

1640
01:41:50,570 --> 01:41:54,220
concepts but those are concepts that you're building on for the rest

1641
01:41:54,230 --> 01:41:58,500
of this week and i mention this just so aware of the amount of pressure

1642
01:41:58,510 --> 01:42:04,350
areas put on me for in this talk so while essentially cover is

1643
01:42:04,360 --> 01:42:08,100
you know basic feedforward neural net multilayer perceptrons

1644
01:42:08,100 --> 01:42:10,060
is another where people describe them

1645
01:42:11,110 --> 01:42:14,310
what i'll go through our first talk about

1646
01:42:14,500 --> 01:42:18,100
essentially given some input x i'll we make predictions using neural

1647
01:42:18,100 --> 01:42:21,850
net well refer to f of x as the output that is

1648
01:42:21,850 --> 01:42:24,640
if you're already somewhat familiar neural nets we take an input

1649
01:42:24,640 --> 01:42:27,320
vector x and p per neural nets will compute

1650
01:42:27,500 --> 01:42:31,730
a series of layers of features which are hidden layers and eventually

1651
01:42:31,730 --> 01:42:35,140
ultimately compute an output layer that serves to make a prediction

1652
01:42:35,140 --> 01:42:39,980
about the label for some input x i'll focus mostly on classification

1653
01:42:39,990 --> 01:42:43,880
during my my talk and there's all talk about furs

1654
01:42:44,190 --> 01:42:49,070
how you compute all these units in the neural nets to forward propagation

1655
01:42:49,440 --> 01:42:52,670
the different types of units activation functions that people

1656
01:42:52,670 --> 01:42:56,570
tend to use have a bit of discussion as to what is the capacity

1657
01:42:56,570 --> 01:42:59,060
of neural nets and and why they are interesting

1658
01:42:59,260 --> 01:43:03,120
in terms of their ability to model various complicated functions

1659
01:43:03,710 --> 01:43:06,830
and then i'll move on to actually talk about you know given

1660
01:43:06,830 --> 01:43:10,270
that we've accepted that this is the type of model multilayer neural

1661
01:43:10,270 --> 01:43:13,830
net that we want to fit to our data how do we actually fit it on

1662
01:43:13,830 --> 01:43:17,160
they are we train neural networks i'll discuss type loss function

1663
01:43:17,160 --> 01:43:21,070
we use for classification talk about the backpropagation algorithm

1664
01:43:21,260 --> 01:43:25,410
which loss gradient to perform some gradient based learning

1665
01:43:25,650 --> 01:43:29,490
and describe some of these in descent algorithms a few other tricks

1666
01:43:29,500 --> 01:43:33,720
of the trade kind of information that you need for apply

1667
01:43:33,730 --> 01:43:37,390
neural nets in practice and ultimately i'll talk a bit smaller

1668
01:43:37,400 --> 01:43:41,570
about you model weighting the case of deep neural network and talk

1669
01:43:41,580 --> 01:43:44,600
about some of the more recent advances that

1670
01:43:45,910 --> 01:43:50,030
have surface in the literature and that were sort of inspired by

1671
01:43:50,030 --> 01:43:53,370
this you know addition of trying to learn deeper neural networks

1672
01:43:53,370 --> 01:43:56,820
things like drop batch normalization unsupervised pre-training

1673
01:43:56,890 --> 01:44:02,000
yeah this is sort of two lectures yes so i am not crazy

1674
01:44:02,850 --> 01:44:09,240
so thanks to any ject so and the first part i'll do is that i'll

1675
01:44:09,250 --> 01:44:13,850
cover forward propagation and backpropagation mainly all the computing

1676
01:44:13,860 --> 01:44:19,320
gradients then we'll talk about that we have lunch and after i talk

1677
01:44:19,320 --> 01:44:22,110
about the complete learning algorithm for fitting neural net

1678
01:44:22,110 --> 01:44:24,290
on data and talk about the learning part

1679
01:44:25,670 --> 01:44:29,640
i curiosity who has seen for propagation backpropagation before

1680
01:44:29,650 --> 01:44:36,050
raise your hand ok we super useful so yeah

1681
01:44:36,270 --> 01:44:39,280
presented layer so hopefully will be like

1682
01:44:39,460 --> 01:44:42,740
watching a movie really like for the second time more

1683
01:44:44,210 --> 01:44:47,540
and hopefully will not be like when you watch a movie you liked

1684
01:44:47,540 --> 01:44:50,450
and you realize all this special effects one that great

1685
01:44:51,500 --> 01:44:54,590
so right let's get going on for propagation

1686
01:44:55,100 --> 01:44:59,300
and i'll just start simple just start with describing what we mean

1687
01:44:59,310 --> 01:45:04,810
by an artificial neuron so and artificial neuron will be essentially

1688
01:45:04,810 --> 01:45:07,560
characterized by a few things will take some input x

1689
01:45:07,560 --> 01:45:10,450
and it would have a set of weights which connection between

1690
01:45:10,450 --> 01:45:13,750
the inputs and the activation neuron itself

1691
01:45:14,130 --> 01:45:17,550
rather a bias and i will have an activation function

1692
01:45:17,970 --> 01:45:20,940
and we will compute the activation of neuron

1693
01:45:21,390 --> 01:45:24,450
is that we will first take the bias so what i'm

1694
01:45:24,490 --> 01:45:27,770
going to first compute what i call the pre activation function

1695
01:45:27,800 --> 01:45:30,960
i'm noting a of x it's going to be a bias

1696
01:45:31,200 --> 01:45:35,630
loss a linear combination of the inputs where each input dimension

1697
01:45:35,640 --> 01:45:40,310
my vector x is weighted by some way w i the weights w guys are just

1698
01:45:40,320 --> 01:45:43,190
the elements dimensions my with vector w

1699
01:45:43,500 --> 01:45:48,310
so this some i can represented as vector operations

1700
01:45:48,480 --> 01:45:52,580
bias b plus the dot product between my weight vector w in my

1701
01:45:52,590 --> 01:45:57,000
input x that i get my pre-activation and to actually get

1702
01:45:57,010 --> 01:46:00,530
the activation in some zero we refer to as the output of a neuron

1703
01:46:01,560 --> 01:46:06,380
will just be some nonlinear function or g which is our activation

1704
01:46:06,390 --> 01:46:09,970
function applied on the pre-activation so our role everything

1705
01:46:10,240 --> 01:46:14,790
it's that equation here and so this is kind of visual illustration

1706
01:46:14,800 --> 01:46:19,100
of what neuron is and often the bias will is represented as actually

1707
01:46:19,110 --> 01:46:24,610
a the weight of on a unit that is constant and equal to one

1708
01:46:24,830 --> 01:46:27,920
that's another way of representing that full some as just

1709
01:46:27,930 --> 01:46:30,700
multiplying a extended vector with w and

1710
01:46:30,910 --> 01:46:37,850
be added attended that vector so ok what kind of functions can

1711
01:46:37,860 --> 01:46:41,360
we represent what artificial neurons we can just plug the output

1712
01:46:41,370 --> 01:46:45,180
of some neuron if i do that into dimensions

1713
01:46:45,680 --> 01:46:49,350
x that's easier to visualize so here we have one axis where have

1714
01:46:49,360 --> 01:46:52,630
the first dimension of some input and here second axes of

1715
01:46:53,470 --> 01:46:56,320
second a second axis for the second dimension my input

1716
01:46:56,540 --> 01:47:00,060
and here this is just the output of the activation of my neuron

1717
01:47:00,760 --> 01:47:04,930
and so typically what we have is that in some region of the input

1718
01:47:04,940 --> 01:47:07,280
space what i'll have is essentially flatten

1719
01:47:07,430 --> 01:47:11,790
small value and as i move along in the direction of the vector w

1720
01:47:12,010 --> 01:47:14,850
eventually the activation starts increasing

1721
01:47:15,220 --> 01:47:19,130
that's but we typically get and often we can even get a

1722
01:47:20,130 --> 01:47:23,580
kind of bridge that separates due to regions so this is the kind

1723
01:47:23,590 --> 01:47:27,050
of function you can represent the so the bias b will determine

1724
01:47:27,280 --> 01:47:31,000
where l long vector w that ridge you will be

1725
01:47:31,430 --> 01:47:36,820
and the range of the values taken by the activation of a neuron

1726
01:47:36,820 --> 01:47:40,260
is going to be determined by the choice of my activation function

1727
01:47:40,970 --> 01:47:45,450
that's a single neuron or please do interrupt me any point

1728
01:47:45,450 --> 01:47:47,460
during the presentation yeah questions

1729
01:47:48,110 --> 01:47:50,340
be happy to make this more attractive than

1730
01:47:50,980 --> 01:47:53,890
discuss some the sum of the details so

1731
01:47:55,200 --> 01:47:58,710
i talked about activation functions one of the choices one

1732
01:47:58,710 --> 01:48:00,910
a set of

1733
01:48:00,930 --> 01:48:07,330
so the most likely set of whether tags so somebody mentioned before part of speech

1734
01:48:07,330 --> 01:48:11,070
tagging has problem like part of speech tagging

1735
01:48:11,690 --> 01:48:14,660
it is part of speech tagging of a sequence of words

1736
01:48:14,670 --> 01:48:18,660
and for every word you have to assign a part of speech like now over

1737
01:48:20,870 --> 01:48:26,620
so in this case one of the words

1738
01:48:26,710 --> 01:48:30,200
so the number of ice friends right that's what we observe and the hidden things

1739
01:48:30,200 --> 01:48:34,020
we're trying to reconstruct instead of part of speech tags we're whether tags instead of

1740
01:48:34,020 --> 01:48:37,390
having forty or fifty then we'll have to called

1741
01:48:37,440 --> 01:48:40,870
because that will make it possible actually visualize it

1742
01:48:40,880 --> 01:48:44,790
if you have struck down the problem to only three words and only two part

1743
01:48:44,790 --> 01:48:50,880
of speech tags plus start end like the special beginning sentence and the sentence words

1744
01:48:50,940 --> 01:48:53,450
four million well

1745
01:48:53,460 --> 01:48:56,000
i want to say that those words and tags that i should say that there

1746
01:48:56,000 --> 01:48:57,540
are special tags

1747
01:48:57,690 --> 01:49:00,440
OK so

1748
01:49:00,460 --> 01:49:05,300
so how do we get this graph

1749
01:49:05,310 --> 01:49:07,010
we did by a

1750
01:49:07,020 --> 01:49:13,860
considering all possible tag sequences so there are thirty three days in the diary

1751
01:49:13,870 --> 01:49:17,190
so how many possible tag sequences

1752
01:49:17,210 --> 01:49:19,210
two to the thirty third place

1753
01:49:19,290 --> 01:49:23,440
each one of those of pattern this crash so this is the past hot hot

1754
01:49:25,290 --> 01:49:26,750
this is the past

1755
01:49:26,760 --> 01:49:31,570
cold cold cold so how likely is it to this well what is the probability

1756
01:49:31,570 --> 01:49:33,070
of cold

1757
01:49:35,560 --> 01:49:37,670
let me explain what this graph means

1758
01:49:37,690 --> 01:49:41,230
this state here if we come to the state this is this means that the

1759
01:49:41,230 --> 01:49:44,980
first day was higher than the state is representing midnight at the end of the

1760
01:49:44,980 --> 01:49:46,750
first day

1761
01:49:46,750 --> 01:49:49,140
OK and records whether that day

1762
01:49:49,160 --> 01:49:50,200
it was hierarchical

1763
01:49:50,210 --> 01:49:53,110
so called

1764
01:49:53,120 --> 01:49:54,710
with the first day

1765
01:49:54,730 --> 01:49:57,690
it was a hot day we end up with the hot stated in i

1766
01:49:57,690 --> 01:50:00,500
second there was a hot day thursday was a cold a

1767
01:50:00,510 --> 01:50:02,460
and this here is the intervening day

1768
01:50:02,480 --> 01:50:07,050
this part here is the intervening day between the two minutes

1769
01:50:07,060 --> 01:50:11,750
so the first day we want know there are some cost the first they can

1770
01:50:11,750 --> 01:50:15,230
be either hot or cold we decided to choose high

1771
01:50:15,270 --> 01:50:17,610
we point five chance of picking high

1772
01:50:17,620 --> 01:50:21,870
and point five chance of picking called given the the previous day restore that's different

1773
01:50:21,880 --> 01:50:27,070
here here we have a whereas hot state probability that the next day will be

1774
01:50:27,070 --> 01:50:31,450
high is point eight probability the next day will be called is only point one

1775
01:50:31,450 --> 01:50:34,760
so here we choose whether to go to hot or cold were more likely to

1776
01:50:34,760 --> 01:50:39,680
choose heart this is a higher probability patterns that involve this are or higher probability

1777
01:50:39,680 --> 01:50:43,600
perhaps there is another reason that this talk is more likely than that of

1778
01:50:43,660 --> 01:50:46,360
on the second day we have to decide only

1779
01:50:46,390 --> 01:50:50,740
we have to decide only not only whether it's going to be harder day but

1780
01:50:50,740 --> 01:50:54,520
given that choice how many ice ice-creams

1781
01:50:55,340 --> 01:50:57,760
when i say we have to decide what i mean is something like you know

1782
01:50:57,760 --> 01:51:00,840
the world decides god decides what the weather will be

1783
01:51:00,850 --> 01:51:04,450
somebody decides randomly what the weather will be

1784
01:51:04,540 --> 01:51:06,930
and how many ice-creams getting

1785
01:51:06,980 --> 01:51:12,200
many god decides the weather and i decided i screens so

1786
01:51:12,220 --> 01:51:13,550
so this is saying

1787
01:51:14,640 --> 01:51:18,940
eighty percent chance that a hot day and then a seventy percent chance that we

1788
01:51:18,940 --> 01:51:20,610
decided three ice-creams

1789
01:51:20,670 --> 01:51:24,930
this is saying what a ten percent chance that called a given that the previous

1790
01:51:24,930 --> 01:51:28,550
day is high and then once i decided to call what's the chance that i

1791
01:51:28,550 --> 01:51:31,310
would be three ice-creams only ten percent

1792
01:51:33,440 --> 01:51:37,090
there are other choices that could have happened right so i've only shown to be

1793
01:51:37,320 --> 01:51:39,770
sort of being hot they are called a

1794
01:51:39,810 --> 01:51:43,190
i've only show the cases where we even write three i three i didn't show

1795
01:51:43,190 --> 01:51:47,010
the other choice of be one or two ice-free because they didn't have we don't

1796
01:51:47,010 --> 01:51:50,600
have to put them in the graph we're only concerned about has that actually explain

1797
01:51:50,640 --> 01:51:52,690
the data that we observed

1798
01:51:52,690 --> 01:51:56,640
so this is an article that explains why i three streams it was a hot

1799
01:51:56,650 --> 01:51:59,210
day and i chose transference pretty likely

1800
01:51:59,220 --> 01:52:03,320
o point eight times point seven this alternative art is another way of explaining why

1801
01:52:03,690 --> 01:52:08,330
three history it was called a and i race and both those things are unlikely

1802
01:52:08,560 --> 01:52:12,540
there's an unlikely are point one point five six

1803
01:52:12,570 --> 01:52:15,840
OK so we prefer this are other things equal

1804
01:52:15,850 --> 01:52:19,050
but other things might not be equal because in order to take this article we

1805
01:52:19,050 --> 01:52:22,110
need to take other works that get here and other works that would get away

1806
01:52:22,110 --> 01:52:24,440
from here

1807
01:52:24,640 --> 01:52:28,570
so that's kind of the inside is we're interested in looking at all of the

1808
01:52:28,570 --> 01:52:34,220
past the probability of the path is the probability of all of the weather events

1809
01:52:34,220 --> 01:52:37,810
and all of the ice cream that's necessary to make that after

1810
01:52:37,820 --> 01:52:41,850
OK so there's two to the thirty third past so very interested in knowing whether

1811
01:52:41,850 --> 01:52:43,360
the second it was hard

1812
01:52:43,400 --> 01:52:45,950
we want to know whether we went through this state h at the end of

1813
01:52:45,950 --> 01:52:48,290
the second

1814
01:52:48,310 --> 01:52:50,290
that's what we want

1815
01:52:51,590 --> 01:52:59,600
of the two to the thirty third past how many go through here

1816
01:52:59,610 --> 01:53:01,310
i think i heard

1817
01:53:01,350 --> 01:53:03,380
more than two

1818
01:53:04,190 --> 01:53:05,490
here's one

1819
01:53:05,510 --> 01:53:06,810
here's another

1820
01:53:06,820 --> 01:53:10,130
i here so once we get the it's true that there's only two pounds that

1821
01:53:10,130 --> 01:53:13,570
come to here but there has to go away also so there's actually a lot

1822
01:53:13,570 --> 01:53:18,460
of has to go through here

1823
01:53:18,480 --> 01:53:22,490
two to the thirty two actually so there's two to thirty three has about a

1824
01:53:22,490 --> 01:53:26,810
trillion right and having to go through this and having them go through that

1825
01:53:26,810 --> 01:53:30,920
so does that mean that the probability we go through here is i half

1826
01:53:30,960 --> 01:53:34,800
no because the path that go through here maybe on average more likely than the

1827
01:53:34,800 --> 01:53:36,760
paths that go through

1828
01:53:36,780 --> 01:53:41,790
so we're interested in is the total probability balthazar go through the state and the

1829
01:53:41,790 --> 01:53:45,390
total probability vol passing through that

1830
01:53:45,580 --> 01:53:49,020
well it's not too bad so we have up four trillion pounds here and four

1831
01:53:49,020 --> 01:53:51,060
trillion pounds there were done

1832
01:53:51,110 --> 01:53:53,150
and you could actually do that

1833
01:53:53,170 --> 01:53:54,580
just be slow

1834
01:53:54,640 --> 01:53:56,730
so can we find a faster way

1835
01:53:56,910 --> 01:54:02,150
so what we're interested in knowing is you know maybe seventy percent of the

1836
01:54:02,170 --> 01:54:05,940
actually the let's just take a moment and think about what these numbers are so

1837
01:54:05,950 --> 01:54:10,130
what's the probability of the first three days being hot or cold

1838
01:54:10,140 --> 01:54:15,110
given the evidence that we're sorry what's the excuse me what is the product of

1839
01:54:15,110 --> 01:54:17,800
these three hours later let's just right there

1840
01:54:17,810 --> 01:54:21,770
so we've got point one times point five six times point

1841
01:54:21,780 --> 01:54:23,270
o one

1842
01:54:23,290 --> 01:54:25,590
no that is actually fairly likely

1843
01:54:25,600 --> 01:54:29,390
i really like the likely outcome for the first three days the most likely path

1844
01:54:29,390 --> 01:54:33,920
is hot hot hot point one times point five six times point five six still

1845
01:54:33,920 --> 01:54:36,970
seems pretty unlikely

1846
01:54:36,980 --> 01:54:38,780
so why is it so unlikely

1847
01:54:38,810 --> 01:54:41,840
hi guys sorry sorry didn't quite work for

1848
01:54:41,850 --> 01:54:45,310
so what's the

1849
01:54:45,330 --> 01:54:48,780
why why is it that these patterns are

1850
01:54:52,700 --> 01:54:56,700
the patterns are not only predicting possible whether sequences remember

1851
01:54:56,710 --> 01:55:01,270
there are predicting whether sequences and the ice cream that i so there's eight possibilities

1852
01:55:01,270 --> 01:55:04,430
for the first three days and you can see all the patterns there

1853
01:55:04,430 --> 01:55:09,240
each of those has its probability is a different way of eating two followed by

1854
01:55:09,240 --> 01:55:13,380
three followed by three ice-creams which is in fact what i a

1855
01:55:13,400 --> 01:55:17,470
on the first three days

1856
01:55:17,720 --> 01:55:19,860
over here we have

1857
01:55:19,880 --> 01:55:26,630
two three and three ice-creams so we have these a has

1858
01:55:26,650 --> 01:55:30,880
all we get is we've divided on my ways of eating two three three

1859
01:55:30,900 --> 01:55:35,280
in eight possible into a possibilities just as we divide it up horses into a

1860
01:55:35,280 --> 01:55:39,430
lot of possible ways of getting horses there are mutually exclusive we add up the

1861
01:55:39,430 --> 01:55:43,170
probabilities and that gives the probability beating two three three ice there are a lot

1862
01:55:43,170 --> 01:55:45,900
of other choice of ice creams like one one one one one two one one

1863
01:55:45,900 --> 01:55:50,150
one three and one two one one two two one two and so

1864
01:55:50,150 --> 01:55:51,590
for example

1865
01:55:51,610 --> 01:55:52,770
i don't have

1866
01:55:52,800 --> 01:55:56,570
some of them but it means that i am repeating

1867
01:55:57,720 --> 01:55:58,790
and thanks

1868
01:55:58,810 --> 01:56:03,650
so it would be like having one first treaty

1869
01:56:03,660 --> 01:56:11,300
three scenes then and time tree nodes tree nodes connected between them but it's repeated

1870
01:56:11,310 --> 01:56:13,220
several times

1871
01:56:13,230 --> 01:56:18,880
and so here you see that we have two plates one in the world space

1872
01:56:18,890 --> 01:56:22,770
i have here reputation for each of my words

1873
01:56:23,660 --> 01:56:27,000
of this this part of the of the graph

1874
01:56:27,090 --> 01:56:32,920
and here a repetition over the the number of documents in the database of also

1875
01:56:34,460 --> 01:56:37,450
and these are the d school run

1876
01:56:39,560 --> 01:56:43,150
so it looks like a fairly simple

1877
01:56:44,460 --> 01:56:46,140
was the

1878
01:56:46,180 --> 01:56:52,590
latent variable which is the topics we've seen examples of that day yesterday

1879
01:56:52,720 --> 01:56:57,770
which because so we did that will give me conditional independence between

1880
01:56:57,800 --> 01:56:59,630
my documents and my

1881
01:57:00,510 --> 01:57:01,900
and the words

1882
01:57:03,630 --> 01:57:08,880
so let's call it a little bit into into detail in this document

1883
01:57:08,900 --> 01:57:10,980
so the the PLSA model

1884
01:57:11,020 --> 01:57:13,220
and in fact it's a

1885
01:57:13,240 --> 01:57:18,310
the its aim is to have a modernization of the words document

1886
01:57:20,200 --> 01:57:23,770
color called co occurrence occurrence

1887
01:57:23,800 --> 01:57:25,660
for each document

1888
01:57:25,670 --> 01:57:27,450
in the database

1889
01:57:27,910 --> 01:57:30,680
and each word in the dictionary so

1890
01:57:30,690 --> 01:57:34,920
we have the joint probability of documents in the database

1891
01:57:34,920 --> 01:57:35,960
and the

1892
01:57:35,990 --> 01:57:38,900
words in the dictionary

1893
01:57:41,250 --> 01:57:45,210
OK so is just to put across

1894
01:57:45,950 --> 01:57:47,200
using the

1895
01:57:47,220 --> 01:57:48,960
common main assumption

1896
01:57:48,970 --> 01:57:52,220
i replaced the probability of a

1897
01:57:52,260 --> 01:57:54,720
word given document

1898
01:57:54,720 --> 01:57:57,910
as this mixture over the

1899
01:57:57,920 --> 01:58:00,410
the topics

1900
01:58:00,420 --> 01:58:06,090
so here everything is represented in this clearly similar everything is represented with

1901
01:58:06,150 --> 01:58:08,200
every conditional probability

1902
01:58:08,220 --> 01:58:11,580
is there is a multinomial probability

1903
01:58:12,260 --> 01:58:17,730
i don't know if you know what i mean so in this probability of w

1904
01:58:17,790 --> 01:58:22,550
and given he got it's it's a it's an element of language and on the

1905
01:58:23,800 --> 01:58:26,950
you have the probability of each of these one

1906
01:58:26,980 --> 01:58:31,200
the same for four days probability and this is the

1907
01:58:31,210 --> 01:58:33,090
i don't know how he could that

1908
01:58:33,110 --> 01:58:39,290
but this is fairly simple also something review committee on

1909
01:58:42,980 --> 01:58:45,680
OK let's go to the next one

1910
01:58:47,510 --> 01:58:50,770
in terms of the model and

1911
01:58:50,780 --> 01:58:52,960
we are not considering the

1912
01:58:53,330 --> 01:58:57,470
this time is not the model of the culture itself

1913
01:58:57,630 --> 01:59:02,640
document in words but it's a it's try to model the probability of document

1914
01:59:02,660 --> 01:59:04,120
and so in

1915
01:59:04,130 --> 01:59:09,050
in this model the document space is partitioned into

1916
01:59:09,110 --> 01:59:15,570
eight steam so the standard x variable of my document space is in fact

1917
01:59:15,590 --> 01:59:17,360
available over

1918
01:59:18,380 --> 01:59:22,340
random variable saying which teams with steam on

1919
01:59:22,360 --> 01:59:28,700
and so the idea is that you have the document which is generated

1920
01:59:30,670 --> 01:59:31,820
bye bye

1921
01:59:31,840 --> 01:59:36,490
by team so i was first i select team then

1922
01:59:36,500 --> 01:59:40,470
this team will give me probabilities over over topics

1923
01:59:40,510 --> 01:59:44,930
and topics we give the probability of over

1924
01:59:46,440 --> 01:59:49,720
you can see it as a mixture of documents is seen as a major

1925
01:59:49,750 --> 01:59:54,000
over over the probability of the image given

1926
01:59:54,050 --> 01:59:56,970
the team

1927
01:59:58,500 --> 02:00:00,170
this probability

1928
02:00:00,180 --> 02:00:01,390
we do

1929
02:00:02,510 --> 02:00:04,660
we did we do the assumption that

1930
02:00:04,670 --> 02:00:06,630
given the team

1931
02:00:06,650 --> 02:00:09,380
we have

1932
02:00:09,430 --> 02:00:13,180
independence between the the words in the dictionary so i can

1933
02:00:13,200 --> 02:00:16,130
that's right is as as the president

1934
02:00:16,140 --> 02:00:19,490
over of the words that are in my document

1935
02:00:19,540 --> 02:00:24,210
of the products of the probability of one word given the team

1936
02:00:25,830 --> 02:00:29,870
here is our main assumption coming into into

1937
02:00:31,130 --> 02:00:37,890
i can replace all the CC k as the as the sum

1938
02:00:38,390 --> 02:00:43,920
over over the topic so that's that's what we already the common assumption

1939
02:00:43,960 --> 02:00:46,720
of the probability of the word given the topic

1940
02:00:46,770 --> 02:00:52,160
OK so here again everything is represented as

1941
02:00:52,200 --> 02:00:57,510
as multinomial probit so the probability of a of

1942
02:00:57,560 --> 02:01:02,180
so first the probability of the team then the probability of topic given the team

1943
02:01:02,220 --> 02:01:09,160
and in the end the probability of word given the topic of that our tables

1944
02:01:09,560 --> 02:01:12,770
so this one

1945
02:01:13,870 --> 02:01:19,310
OK so that's that because you can have a word that appear more than one

1946
02:01:19,320 --> 02:01:21,580
time in the document

1947
02:01:21,610 --> 02:01:26,730
so i have to raise it

1948
02:01:26,870 --> 02:01:31,870
so is the for the ten the frequency of these words in the document

1949
02:01:32,060 --> 02:01:39,720
so one is at the level of the document and the other is it's the

1950
02:01:39,720 --> 02:01:41,740
level of of words

1951
02:01:41,770 --> 02:01:43,720
so let's give an example

1952
02:01:43,730 --> 02:01:52,190
it's a have i have no comments about support and the comments about the history

1953
02:01:52,230 --> 02:01:55,400
and so that's two different teams

1954
02:01:55,530 --> 02:01:59,370
i'm just it's it's not what he's doing but let's have

1955
02:01:59,380 --> 02:02:02,980
imagine that what that he's doing so

1956
02:02:03,010 --> 02:02:06,300
that's my two team and then i have in the

1957
02:02:06,310 --> 02:02:12,740
the talking about sport i have attacked and he and so on so really war

1958
02:02:13,580 --> 02:02:19,650
and in the in the history also i have this discovered this kind of words

1959
02:02:19,650 --> 02:02:23,680
which will be in the topic which would be the topic war for example

1960
02:02:24,860 --> 02:02:27,730
you get

1961
02:02:31,230 --> 02:02:36,290
OK let's go to the last example last model which is the last thing to

1962
02:02:36,300 --> 02:02:39,220
the euclid allocations

1963
02:02:39,390 --> 02:02:44,200
which again is as well as i say is there is a model of of

1964
02:02:44,200 --> 02:02:45,300
the document

1965
02:02:45,310 --> 02:02:50,790
and this time instead of having a partition of the

1966
02:02:50,800 --> 02:02:52,360
of the space

1967
02:02:52,360 --> 02:02:55,480
in two if you number of teams

1968
02:02:55,510 --> 02:02:59,260
we have a random mixture over

1969
02:02:59,310 --> 02:03:01,330
that's topic so it's

1970
02:03:01,490 --> 02:03:04,020
you can see it as it has

1971
02:03:04,110 --> 02:03:06,790
infinite mixture over something

1972
02:03:06,810 --> 02:03:08,990
being in close to two

1973
02:03:09,000 --> 02:03:10,110
to the things

1974
02:03:11,360 --> 02:03:17,800
so this in this in this model the x j x is replaced by a

1975
02:03:17,820 --> 02:03:18,900
tall variables

1976
02:03:18,960 --> 02:03:23,650
OK so again it's this one is really

