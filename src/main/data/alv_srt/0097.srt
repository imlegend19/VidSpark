1
00:00:00,000 --> 00:00:06,060
yeah what can can I just continue my conversational part without the formulas

2
00:00:06,170 --> 00:00:09,130
I guess it seems to me that a key problem is

3
00:00:09,330 --> 00:00:11,500
we're doing squares

4
00:00:12,280 --> 00:00:19,330
were doing estimation where is that that satellite or that missile

5
00:00:19,350 --> 00:00:22,810
and then we get another measurement

6
00:00:22,830 --> 00:00:26,070
so that's 1 more bit of information

7
00:00:27,300 --> 00:00:31,110
updates gives us a correction to the estimated position

8
00:00:33,780 --> 00:00:34,930
because that

9
00:00:34,960 --> 00:00:38,820
this is just 1 more bit of that information is selected somehow this at a

10
00:00:38,820 --> 00:00:44,070
rank 1 change in our in our in our estimation problem and then we're looking

11
00:00:44,070 --> 00:00:46,310
for the change in the answer

12
00:00:46,330 --> 00:00:48,630
and that's what

13
00:00:48,680 --> 00:00:55,520
a recursive least-squares the Kalman filter and filtering is all about

14
00:00:55,570 --> 00:00:57,040
is updated

15
00:00:57,040 --> 00:01:01,630
from new information updating the old answer with new information

16
00:01:01,740 --> 00:01:05,430
OK and that's what we're doing here we can I guess and regarding here is

17
00:01:05,500 --> 00:01:07,670
t inverse is our only answer

18
00:01:07,760 --> 00:01:08,630
that we

19
00:01:08,650 --> 00:01:11,560
that we've found because of especially easy

20
00:01:11,890 --> 00:01:14,650
and now I'm updating by that much

21
00:01:15,540 --> 00:01:20,680
and I'm looking for the new OK i'm just gonna write down the formula that

22
00:01:20,760 --> 00:01:24,560
that we will see later so that you answer that I want is the old

23
00:01:28,590 --> 00:01:33,480
incomes of Canada others k minus T

24
00:01:35,070 --> 00:01:39,960
as this is this rank one matrix yeah maybe I'll just show just keep people

25
00:01:40,070 --> 00:01:45,500
came minus I think maybe it maybe here is the magic formula

26
00:01:45,610 --> 00:01:47,280
right without

27
00:01:47,310 --> 00:01:52,500
my eyes I think it's gonna involve only 2

28
00:01:52,500 --> 00:01:56,130
and I k minus T

29
00:01:56,240 --> 00:01:59,060
and other team inverse

30
00:01:59,060 --> 00:02:00,850
that looks pretty good

31
00:02:01,260 --> 00:02:03,890
and now down below

32
00:02:06,150 --> 00:02:13,610
I think it's something like 1 + choose

33
00:02:13,630 --> 00:02:17,280
Italian number here

34
00:02:17,280 --> 00:02:19,060
that was good

35
00:02:19,300 --> 00:02:24,020
right down here 0 so I have to say with these 2 this rank one

36
00:02:24,020 --> 00:02:31,090
matrix splits and so like can what made would enjoy give that vector

37
00:02:31,490 --> 00:02:35,060
Italy's something that's

38
00:02:35,430 --> 00:02:39,300
is that column vector and then this is the

39
00:02:42,260 --> 00:02:46,040
and so what shows up down here I think it is

40
00:02:48,410 --> 00:02:49,870
in numerous

41
00:02:50,760 --> 00:02:52,890
or maybe it's

42
00:02:52,910 --> 00:02:56,780
no it's the it's the other side because I don't end up with a number

43
00:02:57,130 --> 00:03:00,700
it's easy transpose

44
00:03:02,260 --> 00:03:05,670
In 1st iii

45
00:03:05,730 --> 00:03:09,000
think that's right that might not have all the signs right

46
00:03:11,310 --> 00:03:14,890
this is the beauty of that formula

47
00:03:15,270 --> 00:03:16,810
right for

48
00:03:17,180 --> 00:03:22,520
it's almost right and and about

49
00:03:22,540 --> 00:03:24,760
about a hundred people have discovered this

50
00:03:25,420 --> 00:03:29,700
and they all thought wonderful and the names attached to it so the formula that

51
00:03:29,700 --> 00:03:31,480
so many names now

52
00:03:31,540 --> 00:03:35,090
I could mention also maintained would operate

53
00:03:37,520 --> 00:03:45,300
the old time remaining shoe over all the all out when I do this formula

54
00:03:45,300 --> 00:03:50,330
properly I'll produce more but you see that it has exactly of what we hope

55
00:03:50,980 --> 00:03:56,000
is because what we know what we know here we know T t inverse

56
00:03:56,000 --> 00:03:58,830
so there's 2 universes and we know OK

57
00:03:59,020 --> 00:04:02,460
and this is a rank one matrix and this is just a number down here

58
00:04:02,780 --> 00:04:06,570
so we know everything here and it gives us the inverse of the new and

59
00:04:06,570 --> 00:04:12,350
if I didn't write it would produce this correction this this whole stuff

60
00:04:12,370 --> 00:04:14,280
would be this correction

61
00:04:14,900 --> 00:04:18,660
and we would have already

62
00:04:18,780 --> 00:04:23,130
OK now I don't know entirely satisfied because

63
00:04:23,150 --> 00:04:30,610
the formula got produced under the name of I'm quite since it and what I

64
00:04:30,610 --> 00:04:32,020
want to say is

65
00:04:32,020 --> 00:04:40,760
this really looking ahead that that that this formula and the

66
00:04:40,890 --> 00:04:45,700
that is the change in the matrix is some rank one matrix

67
00:04:46,450 --> 00:04:50,810
the change in the universe is also rank 1 it's more complicated looking thing about

68
00:04:50,810 --> 00:04:53,020
this guy is the right 1

69
00:04:53,020 --> 00:04:55,630
so the whole thing is rank 1

70
00:04:55,650 --> 00:05:02,520
and and there it was in this particular instance OK so I was quite happy

71
00:05:02,520 --> 00:05:07,270
is the thing

72
00:05:15,340 --> 00:05:17,210
thank you

73
00:05:23,510 --> 00:05:25,950
one from

74
00:05:34,760 --> 00:05:43,320
i'm not

75
00:06:48,140 --> 00:06:54,050
and then

76
00:07:33,730 --> 00:07:40,410
the result

77
00:07:40,740 --> 00:07:45,280
i have a lot of that

78
00:07:53,120 --> 00:07:57,170
i mean

79
00:07:57,180 --> 00:08:01,970
and are people

80
00:08:37,920 --> 00:08:40,280
thank you

81
00:08:52,660 --> 00:08:58,750
around a little

82
00:08:58,770 --> 00:09:05,470
i assume that are here

83
00:09:05,490 --> 00:09:06,570
and then you

84
00:09:22,110 --> 00:09:25,390
we're not going to

85
00:09:25,590 --> 00:09:30,570
a lot of things

86
00:09:38,480 --> 00:09:51,050
well known

87
00:09:54,890 --> 00:09:57,370
o thing

88
00:11:06,240 --> 00:11:10,560
i e

89
00:11:10,560 --> 00:11:23,600
this presentation is delivered by the stanford center for professional development

90
00:11:23,620 --> 00:11:27,400
OK good morning welcome back on

91
00:11:27,420 --> 00:11:29,440
what i want to do today is

92
00:11:30,400 --> 00:11:33,950
actually wrap up our discussion on learning theory and

93
00:11:35,760 --> 00:11:39,730
inspired by talking about bayesian statistics and regularisation

94
00:11:40,800 --> 00:11:45,880
and then take very brief digression to save for online learning and almost of today's

95
00:11:45,880 --> 00:11:51,410
lecture will actually be on various pieces by supplying machine learning algorithms for problems like

96
00:11:51,530 --> 00:11:55,940
like approach to other problems go work on after you graduate from the school

97
00:11:55,960 --> 00:12:01,140
on the topic of bayesian statistics improvisation

98
00:12:01,160 --> 00:12:07,960
so remember from last week starts learning theory and we learned about bias and variance

99
00:12:08,720 --> 00:12:12,500
and they get in the previous lecture was for most of the previous lecture talking

100
00:12:12,500 --> 00:12:15,670
about more talking about algorithms for model selection

101
00:12:16,210 --> 00:12:22,230
and feature selection itself has validation so most methods he talks about in the previous

102
00:12:22,230 --> 00:12:27,540
lecture on what waits for you to try to simplify the model so for example

103
00:12:27,560 --> 00:12:31,170
the features section out we talked about gives you a way to eliminate the number

104
00:12:31,180 --> 00:12:35,910
of features of system reduce the number of parameters need to fit and thereby reduce

105
00:12:35,910 --> 00:12:39,690
overfitting that feature selection algorithms

106
00:12:39,710 --> 00:12:41,180
chooses subset

107
00:12:41,200 --> 00:12:45,490
chooses subset of the features so that you have less parameters and may be less

108
00:12:45,490 --> 00:12:51,590
likely to want to do today is talk about a different way to prevent overfitting

109
00:12:51,590 --> 00:12:56,490
on and this is the magic of regularisation and is awaiting lets you keep all

110
00:12:56,490 --> 00:12:58,060
the parameters

111
00:12:58,080 --> 00:13:05,410
so here's the idea and we illustrate this example with arms a linear regression

112
00:13:07,500 --> 00:13:09,030
so if we take

113
00:13:09,140 --> 00:13:17,220
the linear regression model to the very first small we learned about right arm we

114
00:13:17,220 --> 00:13:20,230
said that we would choose the parameters

115
00:13:21,550 --> 00:13:24,170
maximum likelihood

116
00:13:24,290 --> 00:13:33,240
and that meant that you know you choose the parameters theta

117
00:13:33,260 --> 00:13:38,470
that maximize

118
00:13:38,490 --> 00:13:44,880
on the probability of the data which is from stay that maximize the probability of

119
00:13:44,880 --> 00:13:45,860
the data we have

120
00:13:49,680 --> 00:13:50,890
and so

121
00:13:50,900 --> 00:13:56,390
to get this so the procedure and this is one example was called the frequencies

122
00:13:56,390 --> 00:14:04,470
procedure and frequencies you think of as as one school of statistics on

123
00:14:04,490 --> 00:14:09,450
and the philosophical view behind breaking this down was we envision that there was some

124
00:14:09,450 --> 00:14:15,480
true around the theta out there that generated yet exist otherwise it true data

125
00:14:15,520 --> 00:14:16,510
they govern

126
00:14:16,520 --> 00:14:19,250
housing prices y is the function of x

127
00:14:19,260 --> 00:14:22,510
and we don't know what the value of batteries

128
00:14:22,560 --> 00:14:26,420
and would like to come up with some procedure for estimating the value of data

129
00:14:26,500 --> 00:14:30,240
and so that's the likelihood is just one possible procedure

130
00:14:30,250 --> 00:14:32,280
for estimating the unknown value

131
00:14:32,300 --> 00:14:33,510
the data

132
00:14:33,520 --> 00:14:36,680
and in

133
00:14:36,690 --> 00:14:40,120
the way for later this year the data was not a random variable

134
00:14:40,130 --> 00:14:43,850
that's why it's so data is just some true value out there's is not random

135
00:14:43,850 --> 00:14:46,970
or anything we just don't know what it is we have a procedure called as

136
00:14:47,020 --> 00:14:49,170
like the first made

137
00:14:49,250 --> 00:14:53,220
value for data so there's one example with color sequences procedure

138
00:14:54,560 --> 00:14:56,300
the alternative to the

139
00:14:56,350 --> 00:15:03,480
because the frequencies school statistics is the bayesian school

140
00:15:03,500 --> 00:15:04,820
in which

141
00:15:05,860 --> 00:15:08,790
when say that we don't know what features

142
00:15:08,810 --> 00:15:12,160
and so we will put a prior

143
00:15:12,180 --> 00:15:13,740
on theta

144
00:15:13,760 --> 00:15:18,260
so so in the bayesian because is to say well i don't know what validate

145
00:15:18,260 --> 00:15:24,520
chrysalis represents uncertainty over data with the prime

146
00:15:27,310 --> 00:15:30,650
so for example

147
00:15:30,670 --> 00:15:35,650
of high on data may be gaussians distribution

148
00:15:35,700 --> 00:15:41,500
with mean zero and covariance matrix given by house where i

149
00:15:41,520 --> 00:15:43,280
and so

150
00:15:44,460 --> 00:15:46,040
two values as they in my

151
00:15:46,050 --> 00:15:48,640
training set well

152
00:15:48,650 --> 00:15:59,440
so say to represent my beliefs about what the prime dissolved in the absence of

153
00:15:59,440 --> 00:16:02,250
any date not having seen any data

154
00:16:02,250 --> 00:16:05,880
was the john nash is nobel prize

155
00:16:06,900 --> 00:16:08,790
very nearly

156
00:16:08,810 --> 00:16:11,380
not granted it was

157
00:16:11,400 --> 00:16:17,370
i almost voted in an hour before it was supposed to be enhanced

158
00:16:17,420 --> 00:16:22,880
it this is in october nineteen ninety four now i didn't learn that

159
00:16:22,940 --> 00:16:27,690
when i was working on a

160
00:16:27,850 --> 00:16:29,540
time story then

161
00:16:29,560 --> 00:16:30,920
i learned it

162
00:16:30,920 --> 00:16:37,310
not until several years later after i had in fact already delivered the first draft

163
00:16:37,330 --> 00:16:39,110
of the biography

164
00:16:39,130 --> 00:16:41,600
the whole nobel process

165
00:16:41,670 --> 00:16:48,710
is a closely guarded secret there's almost never any leaks

166
00:16:48,770 --> 00:16:50,690
and in fact

167
00:16:50,730 --> 00:16:54,380
all of the deliberations are sealed

168
00:16:55,250 --> 00:16:58,900
the nobel archives for fifty years

169
00:16:58,920 --> 00:17:01,500
after the war prize

170
00:17:01,540 --> 00:17:06,830
the final vote does not take place until the morning that the price is made

171
00:17:08,330 --> 00:17:14,400
in in fact the reporters already milling around in the interior of the the

172
00:17:14,420 --> 00:17:17,190
swedish academy of sciences

173
00:17:17,210 --> 00:17:19,150
we have the final

174
00:17:19,150 --> 00:17:22,790
you know which administers the prize and whose members

175
00:17:22,830 --> 00:17:25,230
get to cast the final

176
00:17:25,250 --> 00:17:28,040
vote making the prize official

177
00:17:28,060 --> 00:17:29,880
it turned out

178
00:17:30,630 --> 00:17:37,580
on october fourteen nineteen ninety four when ashes prize came up for what usually is

179
00:17:37,580 --> 00:17:40,100
a completely full form of old

180
00:17:40,100 --> 00:17:43,790
it turned out that many members

181
00:17:43,900 --> 00:17:49,610
the swedish academy and most especially the physicists

182
00:17:49,630 --> 00:17:51,230
i objected to

183
00:17:51,250 --> 00:17:52,750
the idea

184
00:17:52,750 --> 00:17:57,250
stowing his ultimate scientific honor

185
00:17:58,060 --> 00:17:59,920
and that

186
00:17:59,920 --> 00:18:00,600
there a

187
00:18:00,600 --> 00:18:03,540
we're worried that john nash

188
00:18:03,560 --> 00:18:06,790
would embarrass

189
00:18:06,810 --> 00:18:08,020
the prize

190
00:18:08,040 --> 00:18:11,810
at the ceremony which was to take place in december

191
00:18:11,810 --> 00:18:18,170
and they were worried that giving him nearly giving him the prize would sully the

192
00:18:19,380 --> 00:18:24,520
now fortunately for john nash and i think for the rest of us

193
00:18:24,560 --> 00:18:29,560
there were some very courageous members of the academy who

194
00:18:29,560 --> 00:18:32,080
i took the position

195
00:18:32,130 --> 00:18:39,170
nash is mental illness was no more relevant to the question of whether his contribution

196
00:18:39,170 --> 00:18:44,730
to economics married prize then if he'd suffered from some physical

197
00:18:45,920 --> 00:18:47,690
and they

198
00:18:47,690 --> 00:18:50,790
narrowly carried the vote

199
00:18:51,880 --> 00:18:52,960
as an aside

200
00:18:52,980 --> 00:18:59,040
the morning after the academy awards i call janowicz in that way

201
00:19:00,580 --> 00:19:03,250
he said well so how do you feel

202
00:19:04,600 --> 00:19:10,940
there was a silence about ten seconds john almost never answers the question directly

203
00:19:12,190 --> 00:19:17,000
when you start speaking again all of a sudden he was comparing the

204
00:19:18,150 --> 00:19:20,500
with the nobels

205
00:19:20,520 --> 00:19:21,900
well he said

206
00:19:21,960 --> 00:19:24,900
they both involve academies

207
00:19:24,920 --> 00:19:26,870
and they both involve

208
00:19:26,920 --> 00:19:29,230
a lot of politics

209
00:19:29,940 --> 00:19:33,460
back to the day that the prize

210
00:19:33,500 --> 00:19:37,830
it was announced i was sitting in the news newsroom the new york times

211
00:19:37,850 --> 00:19:40,540
scrolling through the wire stories

212
00:19:41,060 --> 00:19:42,560
had really

213
00:19:42,610 --> 00:19:48,500
you know i hadn't really thought much more about this amazing story but when i

214
00:19:48,500 --> 00:19:50,330
saw john nash his name

215
00:19:50,690 --> 00:19:52,500
the AP story

216
00:19:52,500 --> 00:19:56,960
i jumped up and ran over to my editor

217
00:19:56,980 --> 00:19:58,730
i had not

218
00:19:58,790 --> 00:20:03,190
i hadn't wanted to write the story you have earlier

219
00:20:03,250 --> 00:20:08,670
when i first heard that our and fascinating arc his life

220
00:20:10,170 --> 00:20:15,500
i felt that it was such a long shot that someone who had been out

221
00:20:15,500 --> 00:20:18,400
of accademia for so long

222
00:20:18,400 --> 00:20:21,540
that someone like me which would actually win

223
00:20:21,560 --> 00:20:24,920
i thought it would be a real invasion of privacy

224
00:20:24,940 --> 00:20:30,020
to write about his life in the new york times at that point but

225
00:20:30,940 --> 00:20:33,190
having won the prize

226
00:20:33,230 --> 00:20:37,190
having the protective mantle i was going to write

227
00:20:37,190 --> 00:20:39,610
i knew i had to write that story

228
00:20:39,630 --> 00:20:44,230
now it was a very tough story together

229
00:20:44,230 --> 00:20:46,560
i worked on for about three weeks

230
00:20:46,600 --> 00:20:53,400
none of the economists are mathematicians who i initially called we're willing to put national

231
00:20:53,400 --> 00:20:55,980
schizophrenia on the record

232
00:20:55,980 --> 00:20:59,000
you have to understand this is an illness

233
00:20:59,100 --> 00:21:02,380
even in the nineteen nineties

234
00:21:02,400 --> 00:21:05,060
it is so stigmatized

235
00:21:05,100 --> 00:21:06,670
but even if

236
00:21:06,690 --> 00:21:08,980
you have thought of as a quote

237
00:21:09,040 --> 00:21:15,230
the most remarkable mathematician of the second half the century even if you had just

238
00:21:16,080 --> 00:21:18,330
a nobel prize

239
00:21:18,380 --> 00:21:22,460
the people who care about you your friends and admirers

240
00:21:22,480 --> 00:21:26,810
i felt the need to protect you from the s word

241
00:21:26,830 --> 00:21:29,110
and it was nashi sister martha

242
00:21:29,130 --> 00:21:31,100
who finally decided

243
00:21:31,110 --> 00:21:34,580
so to speak frankly about her brother's illness

244
00:21:34,580 --> 00:21:38,080
and she had not decided to break the silence

245
00:21:38,110 --> 00:21:42,500
nash is inspiring story would have remained

246
00:21:44,400 --> 00:21:48,150
now i've got a tremendous number of letters

247
00:21:48,210 --> 00:21:52,580
when the story ran which was that three weeks after nash got prize

248
00:21:52,600 --> 00:21:54,710
and let me tell you about one

249
00:21:55,730 --> 00:21:57,630
i've never forgotten

250
00:21:57,650 --> 00:21:59,770
it arrived in this

251
00:22:01,270 --> 00:22:03,920
envelope no return address

252
00:22:03,940 --> 00:22:09,000
it was scrawled on this neon orange piece of paper

253
00:22:09,040 --> 00:22:12,580
it was signed only berklee baby

254
00:22:12,600 --> 00:22:18,380
it would never have gone past the new york times mailed after the anthrax scare

255
00:22:19,270 --> 00:22:20,750
this letter

256
00:22:20,830 --> 00:22:23,520
it turns out to have been written

257
00:22:23,520 --> 00:22:24,900
by someone

258
00:22:25,920 --> 00:22:28,580
in the mid nineteen seventies

259
00:22:29,730 --> 00:22:31,020
he two

260
00:22:31,040 --> 00:22:35,650
was diagnosed with paranoid schizophrenia

261
00:22:35,690 --> 00:22:37,440
had been

262
00:22:40,170 --> 00:22:45,060
the new york times rising editor at the new york times

263
00:22:45,110 --> 00:22:46,980
since then

264
00:22:48,040 --> 00:22:50,170
given up his given name

265
00:22:50,190 --> 00:22:54,350
had adopted the name berklee baby and had been living

266
00:22:54,400 --> 00:22:58,250
untreated on the streets of berkeley california

267
00:22:58,270 --> 00:23:00,830
near the university

268
00:23:00,870 --> 00:23:02,880
a set figure

269
00:23:04,230 --> 00:23:08,040
much like the phantom final

270
00:23:08,060 --> 00:23:12,690
and what he wrote was that he had read john nash story

271
00:23:12,710 --> 00:23:14,060
in the paper

272
00:23:14,080 --> 00:23:15,750
and what he said was

273
00:23:15,770 --> 00:23:17,940
johnny cash's story

274
00:23:18,000 --> 00:23:19,670
it gives me hope

275
00:23:19,730 --> 00:23:21,250
that one day

276
00:23:21,250 --> 00:23:25,560
the world will come back to me too

277
00:23:25,600 --> 00:23:27,290
and that letter

278
00:23:27,310 --> 00:23:29,870
and a lot of letters like it

279
00:23:31,150 --> 00:23:32,880
i never got one

280
00:23:32,900 --> 00:23:33,880
that was

281
00:23:33,880 --> 00:23:36,130
quite that eloquent

282
00:23:36,150 --> 00:23:38,710
that letter helped convince me

283
00:23:38,730 --> 00:23:43,610
to go ahead and spent two and half years on leave from the times to

284
00:23:44,520 --> 00:23:46,580
nash's biography

285
00:23:46,610 --> 00:23:48,650
you should know

286
00:23:48,670 --> 00:23:52,850
by the way the biography was not authorized

287
00:23:52,900 --> 00:23:57,770
almost on the first day of my really hard one leave

288
00:23:57,770 --> 00:24:00,350
when i just arrived in princeton

289
00:24:00,690 --> 00:24:04,080
john nash sent me an email in which you inform me

290
00:24:04,080 --> 00:24:05,330
most about two

291
00:24:05,350 --> 00:24:09,900
actually what i talked about before

292
00:24:10,080 --> 00:24:18,120
and the right to be one of the first one

293
00:24:18,140 --> 00:24:20,640
i just half

294
00:24:20,730 --> 00:24:23,370
how are

295
00:24:23,390 --> 00:24:25,460
but don't can

296
00:24:26,980 --> 00:24:29,620
so it's just measure

297
00:24:29,680 --> 00:24:34,610
the how problem is

298
00:24:34,630 --> 00:24:39,630
the first is also in the middle

299
00:24:39,640 --> 00:24:42,560
the other thing is that

300
00:24:43,820 --> 00:24:46,490
the complexity of

301
00:24:46,500 --> 00:24:53,570
and what want to the call

302
00:24:53,580 --> 00:24:57,240
half of the

303
00:24:57,250 --> 00:24:59,110
the show

304
00:24:59,130 --> 00:25:03,310
it is called

305
00:25:05,330 --> 00:25:07,830
no it's not

306
00:25:13,930 --> 00:25:15,180
so there is a

307
00:25:16,910 --> 00:25:19,190
the same

308
00:25:32,860 --> 00:25:39,990
what are the problems affecting established

309
00:25:40,080 --> 00:25:43,970
actually which is why

310
00:25:49,210 --> 00:25:50,990
woman which collection

311
00:25:54,330 --> 00:25:55,710
they have

312
00:25:55,720 --> 00:25:58,250
the usual

313
00:25:58,270 --> 00:26:00,350
about hundred images

314
00:26:00,360 --> 00:26:04,640
and they all have a right

315
00:26:04,660 --> 00:26:10,750
of course it's lots of different systems with a

316
00:26:10,760 --> 00:26:13,920
collection because it

317
00:26:13,940 --> 00:26:15,230
the band

318
00:26:15,250 --> 00:26:16,700
what the colour

319
00:26:16,720 --> 00:26:20,130
let's try to search

320
00:26:22,970 --> 00:26:24,480
the park

321
00:26:25,250 --> 00:26:26,370
the became

322
00:26:36,010 --> 00:26:37,000
they have

323
00:26:43,160 --> 00:26:44,440
well picture

324
00:26:44,600 --> 00:26:46,070
pictures first

325
00:26:46,090 --> 00:26:50,010
they don't like this

326
00:26:59,410 --> 00:27:02,100
well the that is

327
00:27:02,750 --> 00:27:04,850
but in the last

328
00:27:04,850 --> 00:27:06,570
this week

329
00:27:07,790 --> 00:27:11,160
like the rest of their lives

330
00:27:11,170 --> 00:27:16,160
my my

331
00:27:18,690 --> 00:27:20,410
it looks like

332
00:27:20,530 --> 00:27:24,570
o main just below on

333
00:27:24,570 --> 00:27:29,370
and that are social

334
00:27:29,380 --> 00:27:34,510
the reason for the

335
00:27:34,570 --> 00:27:35,670
the result

336
00:27:37,410 --> 00:27:42,970
so you can we collect

337
00:27:45,790 --> 00:27:51,620
the other

338
00:27:53,410 --> 00:27:56,380
you could

339
00:28:02,900 --> 00:28:05,630
below a real big island

340
00:28:05,750 --> 00:28:15,500
i mean it was a member one million just

341
00:28:15,530 --> 00:28:22,980
he was elected to the to the but lost the election

342
00:28:23,010 --> 00:28:28,710
from we are you and others that there

343
00:28:28,740 --> 00:28:35,030
the river it was never

344
00:28:36,240 --> 00:28:39,310
and the reason people

345
00:28:41,510 --> 00:28:43,470
not only

346
00:28:43,490 --> 00:28:47,100
you see is to participate in

347
00:28:47,150 --> 00:28:50,010
there were some

348
00:28:50,890 --> 00:28:54,180
let's make

349
00:28:54,310 --> 00:28:57,120
here i'm

350
00:28:59,030 --> 00:29:02,680
one of those people who

351
00:29:05,310 --> 00:29:07,100
and now

352
00:29:08,860 --> 00:29:11,110
the colour

353
00:29:15,770 --> 00:29:18,840
and you know

354
00:29:18,840 --> 00:29:23,030
in the commercial world you've got too many people writing new drivers

355
00:29:23,030 --> 00:29:24,900
and not enough people fixing

356
00:29:24,930 --> 00:29:29,180
if you take a team of people writing new drivers change their job description manager

357
00:29:29,180 --> 00:29:34,150
comes in one morning and said congratulations from now on your fixing cars

358
00:29:34,160 --> 00:29:38,620
maybe one of the leaves but the rest of canada

359
00:29:38,670 --> 00:29:42,610
in the free software well you can't just people say well we've got too many

360
00:29:42,610 --> 00:29:46,010
developed developers to any driver writers so

361
00:29:46,020 --> 00:29:47,240
you knew

362
00:29:47,250 --> 00:29:49,990
you've been volunteered to extend instead

363
00:29:50,740 --> 00:29:53,800
doesn't actually work in the results

364
00:29:53,800 --> 00:29:56,060
in fact it and you can use

365
00:29:56,100 --> 00:29:59,220
find the good drink beer instead

366
00:30:00,920 --> 00:30:03,000
so what

367
00:30:03,040 --> 00:30:06,500
so we have this problem and it's a lot it's an image problem is this

368
00:30:06,500 --> 00:30:09,520
idea that writing new drivers cool

369
00:30:09,530 --> 00:30:12,950
everybody wants to be right right

370
00:30:12,970 --> 00:30:15,340
we have the same problem on the biggest scale

371
00:30:15,350 --> 00:30:18,650
because everybody wants to be involved in kind

372
00:30:18,660 --> 00:30:22,520
so how can we can kinda cool project is the the cardinality depends on the

373
00:30:25,000 --> 00:30:28,660
true everything depends on things like c libraries well

374
00:30:28,690 --> 00:30:31,380
but i g live c

375
00:30:31,390 --> 00:30:36,090
simply doesn't have the ring it doesn't look so good guide your CV

376
00:30:36,320 --> 00:30:38,220
business card

377
00:30:38,220 --> 00:30:41,930
even though it's actually probably harder project

378
00:30:41,950 --> 00:30:44,540
it's a lot better documents

379
00:30:44,540 --> 00:30:47,230
and it's far more important

380
00:30:47,290 --> 00:30:50,740
you do have to do with all equities is also is actually a really nice

381
00:30:50,740 --> 00:30:56,080
guy and then the GTC meaningless for you

382
00:30:56,100 --> 00:31:01,010
the other places you people really really want to write a new drive

383
00:31:01,040 --> 00:31:04,730
the windows developers desperately need more people

384
00:31:04,740 --> 00:31:07,650
because expenses are very difficult times

385
00:31:07,670 --> 00:31:11,670
which ended up with a big splash in extrinsic or

386
00:31:11,870 --> 00:31:14,800
x became started

387
00:31:14,800 --> 00:31:18,340
and in that they lost a lot of people got upset with the always thank

388
00:31:18,340 --> 00:31:20,470
you very bad things would

389
00:31:20,480 --> 00:31:25,020
and they have got nothing being done sensibly properly very rapidly

390
00:31:25,180 --> 00:31:28,070
not picked up enough people

391
00:31:28,110 --> 00:31:32,390
and that the next people got manual

392
00:31:34,540 --> 00:31:38,620
people got manual

393
00:31:38,650 --> 00:31:41,090
the car drivers need fixing

394
00:31:41,110 --> 00:31:43,590
the got new drivers new writing

395
00:31:43,720 --> 00:31:51,080
got people like HCI finally working with trying to right heart complicated three d drive

396
00:31:51,090 --> 00:31:57,130
so if you want to write new co-driver k x people would really really love

397
00:31:59,660 --> 00:32:05,580
this is

398
00:32:05,590 --> 00:32:12,330
maintaining can seize the small detail record below

399
00:32:12,340 --> 00:32:16,580
the way to maintain his campaign is quite interesting

400
00:32:16,600 --> 00:32:21,440
nobody goes around in circles if you been elevated to the job maintain

401
00:32:23,390 --> 00:32:27,700
because somebody dominant up like patches together

402
00:32:27,750 --> 00:32:31,780
and if they are good at it and the the end of the ninety nine

403
00:32:32,050 --> 00:32:35,130
we do not maintain its level kernel

404
00:32:35,160 --> 00:32:38,910
which in theory tells you who is maintained

405
00:32:38,950 --> 00:32:42,640
maintain this fact is not itself have maintained

406
00:32:42,660 --> 00:32:45,780
this is occasionally a problem because it does get day

407
00:32:45,850 --> 00:32:50,160
people really good so i will maintain

408
00:32:50,170 --> 00:32:55,590
the really bad at remembering to remove themselves when they decided what

409
00:32:55,630 --> 00:32:57,780
and sometimes people just disappear

410
00:32:58,280 --> 00:33:00,900
as with anything else

411
00:33:00,910 --> 00:33:07,280
politics people might occasionally something that catches my eye

412
00:33:07,690 --> 00:33:11,340
we also have a good process for removing maintain just manage

413
00:33:11,810 --> 00:33:18,380
sometimes we want somebody will be maintaining disappear six

414
00:33:18,410 --> 00:33:22,300
although still maintaining my that nothing happens

415
00:33:22,340 --> 00:33:27,050
we don't have the press i find it was now maintained

416
00:33:27,100 --> 00:33:31,890
so have people disappear into google for six months and then suddenly reappear

417
00:33:31,910 --> 00:33:33,370
i like what you

418
00:33:33,420 --> 00:33:39,890
i have removed to maintain his entry in the meantime again

419
00:33:39,900 --> 00:33:44,000
we have lots and lots of all code in the car

420
00:33:44,040 --> 00:33:49,340
one of the problems with all code is there's always somebody still using it

421
00:33:49,360 --> 00:33:53,500
in the commercial world decisions get made along the lines of

422
00:33:53,510 --> 00:33:56,530
if we remove this drive we will upset

423
00:33:56,540 --> 00:33:58,880
no point three percent user base

424
00:33:58,910 --> 00:34:00,530
we don't care

425
00:34:01,160 --> 00:34:03,520
in the face of the world is much more difficult

426
00:34:04,000 --> 00:34:07,050
and the so

427
00:34:07,150 --> 00:34:10,930
we tend to do is got people like aided by systematic trying to remove every

428
00:34:10,930 --> 00:34:13,220
single piece canopy

429
00:34:13,230 --> 00:34:16,500
on the basis that you find use

430
00:34:16,520 --> 00:34:20,650
other people trying to stop it

431
00:34:20,680 --> 00:34:24,540
and we do try to do when you got some small number of users

432
00:34:24,550 --> 00:34:26,840
is persuade them to maintain

433
00:34:26,900 --> 00:34:29,060
because if you saw it using the kernel

434
00:34:29,070 --> 00:34:30,240
ten years

435
00:34:30,260 --> 00:34:32,240
as a very small number users

436
00:34:32,250 --> 00:34:34,160
chances are works

437
00:34:34,180 --> 00:34:35,850
it will continue to work

438
00:34:35,870 --> 00:34:39,280
and it probably never any match-fixing

439
00:34:39,300 --> 00:34:44,180
if nobody is interested in maintaining sometimes can fall by the wayside because nobody can

440
00:34:44,180 --> 00:34:45,720
be bothered fix

441
00:34:45,720 --> 00:34:47,200
what some change occurs

442
00:34:47,240 --> 00:34:50,410
finally break the point

443
00:34:50,430 --> 00:34:55,390
and occasionally we just give up the class some architectural

444
00:34:55,410 --> 00:34:59,350
so everywhere that we recently got rid MCA possible

445
00:34:59,600 --> 00:35:04,450
you have the PS two your attic picture elements are about

446
00:35:04,510 --> 00:35:10,740
one hard ones and if you will be when we get rid lice are possible

447
00:35:10,760 --> 00:35:13,430
when we get rid of three things

448
00:35:13,470 --> 00:35:17,850
we did try getting rid of three things present for around two point six

449
00:35:18,350 --> 00:35:20,010
people object

450
00:35:20,010 --> 00:35:25,740
because when people start running embedded system of everything she

451
00:35:25,740 --> 00:35:29,500
well this result trying images

452
00:35:29,510 --> 00:35:30,630
they've been a little bit

453
00:35:30,640 --> 00:35:32,850
process without talk about later

454
00:35:32,860 --> 00:35:35,210
so what to show you

455
00:35:35,220 --> 00:35:38,790
we've all seen patterns of c

456
00:35:38,800 --> 00:35:40,880
this is very since this

457
00:35:40,890 --> 00:35:43,300
this is this is this

458
00:35:43,310 --> 00:35:47,740
and this is constant so here we have the right

459
00:35:47,760 --> 00:35:51,200
are they oxygen columns

460
00:35:51,260 --> 00:35:53,400
this is cast at all

461
00:35:53,410 --> 00:35:57,760
so we see the intensity ratio is also on

462
00:35:57,780 --> 00:36:02,370
on this basis i would say more complex

463
00:36:02,410 --> 00:36:04,920
the best

464
00:36:04,940 --> 00:36:08,530
this is one important thing on the other hand of course

465
00:36:08,550 --> 00:36:09,860
we have listing

466
00:36:09,910 --> 00:36:13,910
the images are more noise was you

467
00:36:13,930 --> 00:36:16,320
and this is not a ban on the

468
00:36:16,550 --> 00:36:18,580
a lot of

469
00:36:18,590 --> 00:36:20,860
whereas the higher

470
00:36:21,000 --> 00:36:23,640
it is this one

471
00:36:23,800 --> 00:36:26,890
more sensitive

472
00:36:30,650 --> 00:36:31,860
so should remind

473
00:36:31,910 --> 00:36:39,250
now so that the business at causes makes the king oxygen carbon which is about

474
00:36:39,250 --> 00:36:42,480
fifteen percent one twenty one seven

475
00:36:44,120 --> 00:36:48,030
can really resolve the difference in total number

476
00:36:48,160 --> 00:36:51,590
between the two columns

477
00:36:51,610 --> 00:36:53,930
just another thing about the specimens

478
00:36:53,940 --> 00:36:58,040
well this is a situation we always

479
00:36:58,090 --> 00:37:00,070
amorphous is there risk

480
00:37:00,980 --> 00:37:04,690
i call this information there

481
00:37:04,710 --> 00:37:06,540
which is composed of some

482
00:37:06,550 --> 00:37:08,040
time trials

483
00:37:08,060 --> 00:37:10,420
which the

484
00:37:10,440 --> 00:37:14,100
we have a very fine control by is

485
00:37:14,120 --> 00:37:17,430
no matter can crawl space

486
00:37:17,440 --> 00:37:21,870
spaces and some of the problems

487
00:37:21,880 --> 00:37:28,260
and that was the best she really really rare but we don't have the

488
00:37:29,300 --> 00:37:30,800
but you can

489
00:37:30,820 --> 00:37:33,570
yes this is what was here

490
00:37:33,590 --> 00:37:37,350
here's one the i

491
00:37:37,510 --> 00:37:41,470
i mean this amorphous layer and the house

492
00:37:42,670 --> 00:37:43,400
the course

493
00:37:43,430 --> 00:37:49,110
other things that you have to be like plasma etching in order to this hydrocarbon

494
00:37:49,110 --> 00:37:50,580
thriller which

495
00:37:50,600 --> 00:37:55,140
is the one which really caused the contamination

496
00:37:56,450 --> 00:37:57,110
it was

497
00:37:57,160 --> 00:38:01,340
actually introduction and then i'll show you

498
00:38:01,390 --> 00:38:03,380
the results on

499
00:38:03,390 --> 00:38:07,180
in the is that it is a cost

500
00:38:07,240 --> 00:38:11,450
i was going to use these images

501
00:38:11,470 --> 00:38:12,490
his something

502
00:38:12,510 --> 00:38:17,270
the basic idea is a lot of systems

503
00:38:17,280 --> 00:38:22,580
obviously this is an old one which is much more difficult

504
00:38:22,590 --> 00:38:27,220
and of course all the images according to

505
00:38:27,380 --> 00:38:28,920
the microscope

506
00:38:28,930 --> 00:38:31,300
actually show

507
00:38:32,470 --> 00:38:35,670
number one is that some solutions

508
00:38:35,690 --> 00:38:38,030
so the solution in order

509
00:38:38,050 --> 00:38:39,990
but it just random

510
00:38:40,000 --> 00:38:43,740
so the solution which we use for atoms are incorporated into the

511
00:38:43,750 --> 00:38:45,080
this matrix

512
00:38:45,090 --> 00:38:48,000
of nodes on the simpsons

513
00:38:49,080 --> 00:38:51,210
and this very guys are also

514
00:38:51,220 --> 00:38:54,020
i don't know what does that

515
00:38:54,090 --> 00:38:57,510
is very material shown

516
00:38:57,580 --> 00:38:59,920
this is the solution that they

517
00:39:00,010 --> 00:39:03,010
he gets into lot

518
00:39:03,030 --> 00:39:08,180
so on a way that you can see his vision to

519
00:39:08,240 --> 00:39:10,440
and if you look

520
00:39:10,460 --> 00:39:13,910
closer inspection regions this

521
00:39:13,930 --> 00:39:17,990
course if we exclude the possible opted spaces

522
00:39:19,100 --> 00:39:21,280
so we start with the relation

523
00:39:23,340 --> 00:39:25,570
you see this

524
00:39:27,100 --> 00:39:29,850
and is a these regions

525
00:39:29,860 --> 00:39:31,840
we're a science

526
00:39:33,120 --> 00:39:35,770
well you should be true

527
00:39:36,050 --> 00:39:37,610
not all

528
00:39:37,620 --> 00:39:43,280
should use it on this is this this is not all that sort solution

529
00:39:43,300 --> 00:39:48,270
and also decides to have larger and larger numbers

530
00:39:48,290 --> 00:39:51,100
all the average

531
00:39:52,930 --> 00:39:54,410
o seven hundred

532
00:39:54,420 --> 00:39:56,420
one is sense

533
00:39:59,340 --> 00:40:05,260
this is the essence around thirty four years

534
00:40:05,270 --> 00:40:08,260
order be some work done recently

535
00:40:08,310 --> 00:40:10,090
many years

536
00:40:13,950 --> 00:40:17,950
it's about this type of

537
00:40:17,960 --> 00:40:21,520
this is actually only matters

538
00:40:21,530 --> 00:40:26,570
they can be used same individual atoms within a crystal

539
00:40:27,440 --> 00:40:30,790
research on such a scale

540
00:40:30,810 --> 00:40:32,840
another thing was faces

541
00:40:32,850 --> 00:40:36,740
this is one of the combined work

542
00:40:36,750 --> 00:40:40,690
well aware that i mentioned

543
00:40:40,740 --> 00:40:42,410
it's also

544
00:40:42,430 --> 00:40:44,410
so someone that materials

545
00:40:44,470 --> 00:40:46,290
and this is a

546
00:40:46,310 --> 00:40:49,920
and this is what i mean it's one one o o xo

547
00:40:49,970 --> 00:40:54,980
that means that the question was what is terminating plate between the palladium is going

548
00:40:54,980 --> 00:40:56,760
to try to

549
00:40:56,800 --> 00:40:59,390
not just because of all nations

550
00:40:59,400 --> 00:41:05,310
i was in were shown in playing tenuous oxygen between the palladium

551
00:41:05,330 --> 00:41:10,310
and this is something that also included just based on the intensity

552
00:41:10,330 --> 00:41:12,970
another thing that the study was

553
00:41:12,990 --> 00:41:15,110
this diodes

554
00:41:15,130 --> 00:41:16,370
which is

555
00:41:16,400 --> 00:41:18,230
very important

556
00:41:18,240 --> 00:41:19,360
replacing the

557
00:41:19,370 --> 00:41:21,940
raise additional sources

558
00:41:24,700 --> 00:41:31,160
well actually the right to use this

559
00:41:31,170 --> 00:41:32,990
one parts

560
00:41:33,000 --> 00:41:35,570
it's all the superstructure

561
00:41:35,590 --> 00:41:39,190
let's structure which is composed of gallium nitrate

562
00:41:39,310 --> 00:41:41,420
in early in the united right

563
00:41:42,220 --> 00:41:47,070
function is supposed to be on the

564
00:41:47,080 --> 00:41:48,880
and the question was

565
00:41:48,900 --> 00:41:50,310
can we

566
00:41:50,330 --> 00:41:53,660
resolved by using the inductance stand

567
00:41:53,710 --> 00:41:58,950
those different layers which are only about three to six also the question was posted

568
00:41:58,950 --> 00:42:01,230
does this work

569
00:42:01,240 --> 00:42:03,990
if here

570
00:42:04,000 --> 00:42:04,730
the first

571
00:42:06,430 --> 00:42:11,340
we might as well you

572
00:42:11,360 --> 00:42:16,170
this makes it easy to resolve these solutions

573
00:42:16,180 --> 00:42:19,350
so what you see is actually an millions of

574
00:42:19,370 --> 00:42:23,360
the magnifications since

575
00:42:23,380 --> 00:42:26,170
this first these relations

576
00:42:27,190 --> 00:42:35,680
very few people getting so she can see is that is a lot of issues

577
00:42:35,720 --> 00:42:36,950
this one

578
00:42:36,970 --> 00:42:42,420
which is the visible and said he was going to use all

579
00:42:42,430 --> 00:42:43,890
these differences

580
00:42:44,790 --> 00:42:46,910
it is see this image here

581
00:42:46,920 --> 00:42:49,790
white's dark

582
00:42:49,810 --> 00:42:51,370
so i would say

583
00:42:52,020 --> 00:42:53,960
and signs

584
00:42:53,970 --> 00:42:57,460
the started once the media

585
00:42:57,470 --> 00:42:58,620
and the guardian

586
00:42:58,640 --> 00:43:00,800
for right one

587
00:43:00,820 --> 00:43:03,550
well it's a shame it really

588
00:43:03,570 --> 00:43:05,700
quite noisy

589
00:43:05,700 --> 00:43:09,800
one of them

590
00:43:28,780 --> 00:43:32,220
and so

591
00:43:32,600 --> 00:43:36,130
it may of my

592
00:44:01,240 --> 00:44:05,660
i'm on a roll

593
00:44:32,700 --> 00:44:37,680
you may be

594
00:44:50,310 --> 00:44:52,830
i will

595
00:44:53,460 --> 00:44:57,690
there are already

596
00:45:23,210 --> 00:45:25,140
the very

597
00:45:25,150 --> 00:45:29,980
they are

598
00:45:30,400 --> 00:45:33,130
don't know

599
00:46:49,110 --> 00:46:50,290
a year

600
00:46:52,850 --> 00:46:56,200
three of the car

601
00:47:54,190 --> 00:47:58,980
of all of

602
00:49:29,450 --> 00:49:31,540
well on

603
00:49:32,500 --> 00:49:33,920
one day

604
00:49:33,940 --> 00:49:38,710
you might read the model

605
00:49:41,330 --> 00:49:44,700
he is what he

606
00:49:44,720 --> 00:49:46,430
all right

607
00:49:49,460 --> 00:49:53,910
from here

608
00:49:56,270 --> 00:49:58,500
he all

609
00:49:58,500 --> 00:50:01,850
we're going to do is to to compute the posterior distribution of the axis i'm

610
00:50:01,850 --> 00:50:04,500
going to make a very particular assumption

611
00:50:04,550 --> 00:50:06,640
which is that i want

612
00:50:06,650 --> 00:50:10,420
a posterior distribution on which is factorized

613
00:50:10,430 --> 00:50:13,590
so i want to get i don't want to have a joint distribution of the

614
00:50:13,590 --> 00:50:17,510
axis can be way too complicated to five hundred times that we have a joint

615
00:50:17,510 --> 00:50:21,680
vision hundred things that's as unwieldy so instead i'm going to try and get the

616
00:50:21,680 --> 00:50:23,610
marginal distribution of each of the axes

617
00:50:23,780 --> 00:50:26,270
so what i'm going to do is i'm going to

618
00:50:26,310 --> 00:50:27,270
look for

619
00:50:27,280 --> 00:50:31,500
a distribution qx one which is costing the next one

620
00:50:31,520 --> 00:50:33,740
xt which is constant x two and so on

621
00:50:33,760 --> 00:50:37,360
and the way i can ensure that is only to approximate my factors in a

622
00:50:37,360 --> 00:50:38,420
particular way

623
00:50:38,430 --> 00:50:42,740
only due to ensure that is a is the only factors that are really bugging

624
00:50:42,740 --> 00:50:45,020
me so the factors that are connected to y

625
00:50:45,460 --> 00:50:48,880
are really a problem because goshen and only one variable anyway

626
00:50:49,010 --> 00:50:53,720
the real problem of transition factors the transition vectors connecting two axes

627
00:50:53,740 --> 00:50:55,310
together and i don't want that

628
00:50:55,350 --> 00:50:59,240
because if i want to multiply those together get a couple distribution

629
00:50:59,270 --> 00:51:02,850
so we're going to do is i'm going to factor and approximate each of those

630
00:51:03,740 --> 00:51:05,220
transition factors

631
00:51:05,230 --> 00:51:07,150
with two separate

632
00:51:07,170 --> 00:51:11,770
singleton factors going so

633
00:51:11,820 --> 00:51:15,390
so if the transition factors this one

634
00:51:15,470 --> 00:51:19,130
so this is the p of x two given x one

635
00:51:19,140 --> 00:51:22,600
so these factors which cause coupling in my problem

636
00:51:22,640 --> 00:51:24,390
and i want that so

637
00:51:24,400 --> 00:51:27,140
what we do is going to take each one of those factors

638
00:51:27,180 --> 00:51:29,210
it is used to be

639
00:51:29,220 --> 00:51:30,960
if x two given x one

640
00:51:30,960 --> 00:51:34,320
what i'm going to do is i'm going to replace that with two gaussians and

641
00:51:34,320 --> 00:51:37,640
replace it with the gas in the next one and gas in NXT

642
00:51:37,690 --> 00:51:39,480
that would be my approximation

643
00:51:39,690 --> 00:51:42,430
i will call that

644
00:51:46,020 --> 00:51:50,170
backward distribution for x one and the other one will called

645
00:51:50,220 --> 00:51:55,590
the for distribution facts

646
00:51:56,900 --> 00:51:58,780
just the right a different way

647
00:51:58,840 --> 00:52:01,000
going to take it

648
00:52:01,020 --> 00:52:03,680
this transition factor when approximated as

649
00:52:03,710 --> 00:52:07,820
the product of two gaussians

650
00:52:07,880 --> 00:52:10,310
one x one one x two

651
00:52:10,340 --> 00:52:14,810
and i'm going to from the transition factors and it should be clear that have

652
00:52:14,810 --> 00:52:15,880
done that

653
00:52:15,890 --> 00:52:16,600
i know

654
00:52:16,610 --> 00:52:19,340
i have just a separate gaussian distribution of my

655
00:52:19,350 --> 00:52:23,420
variables right because this distribution here

656
00:52:23,460 --> 00:52:25,310
was already quite friendly

657
00:52:25,320 --> 00:52:29,720
was just p y one given x

658
00:52:29,740 --> 00:52:33,810
which was already also in this case

659
00:52:33,900 --> 00:52:38,220
and now i want you is multiply that with the background distribution of the

660
00:52:47,050 --> 00:52:51,130
could i approximated by harold by distribution depends on both x one x two

661
00:52:51,150 --> 00:52:53,670
i certainly could be the negative posterior which is

662
00:52:53,680 --> 00:52:58,060
i joint function about x one x two so i said from the outset i

663
00:52:58,060 --> 00:53:02,320
want i want just marginal distributions are not to each other variables

664
00:53:02,350 --> 00:53:10,310
absolutely yes so if you want a better approximation you can use bigger and bigger

665
00:53:10,680 --> 00:53:13,800
something so you can say that i want to use

666
00:53:13,810 --> 00:53:16,430
the first two distribution what the first two variables to be in one of my

667
00:53:16,430 --> 00:53:22,140
approximate posteriors second variables within one and so on and so do that and in

668
00:53:22,140 --> 00:53:26,420
fact what you do in that case is you just simply not break

669
00:53:26,430 --> 00:53:30,180
the transition between one and two parts they would only break the transition between two

670
00:53:30,180 --> 00:53:34,320
and three part for example that would give you what you wanted give joint and

671
00:53:34,320 --> 00:53:36,980
one to enjoy something more

672
00:53:37,010 --> 00:53:41,970
so it's really just the choice of we want decided what you want your approximation

673
00:53:41,970 --> 00:53:45,140
to look like how much structure you want to be in you then can you

674
00:53:45,140 --> 00:53:50,030
doesn't work anymore for all that your semantic web people towards

675
00:53:50,090 --> 00:53:54,550
and the last point is reason about the state of it you wrote from different

676
00:53:57,410 --> 00:54:02,080
this is a standard problem is not something that was not

677
00:54:02,090 --> 00:54:07,470
see there's a problem before the semantic web it's about integration problem OK and several

678
00:54:07,470 --> 00:54:13,010
ways to solve it comes from the database community one possible way to so this

679
00:54:13,130 --> 00:54:14,250
is a

680
00:54:14,330 --> 00:54:20,530
writing down an abstract representation of all the sources this up the representation of those

681
00:54:21,220 --> 00:54:26,140
to query from the top but we vote caring about the different of the sources

682
00:54:26,290 --> 00:54:30,290
so you have an object representation that is the coupling the queries

683
00:54:30,430 --> 00:54:34,880
from the actual representation of the different sources but connected

684
00:54:35,940 --> 00:54:39,190
you have the problem of

685
00:54:39,200 --> 00:54:41,400
merging the results extracting the

686
00:54:41,470 --> 00:54:45,850
the information from the repository and merging them and if you do that then of

687
00:54:45,850 --> 00:54:51,710
course the problem is solved because you can start making creating all over the entire

688
00:54:51,710 --> 00:54:53,980
group of information so

689
00:54:53,990 --> 00:54:56,840
drawing it here

690
00:54:56,840 --> 00:54:59,350
the solution is physically

691
00:54:59,400 --> 00:55:08,330
the lot of an abstract model

692
00:55:08,340 --> 00:55:09,710
that the alone

693
00:55:10,880 --> 00:55:12,730
the different sources down here

694
00:55:12,750 --> 00:55:16,720
this one is musicbrainz is one is music awards

695
00:55:16,760 --> 00:55:21,710
and this one is the PDB

696
00:55:21,720 --> 00:55:27,440
to be represented at these levels of it you can write it freely

697
00:55:27,490 --> 00:55:30,090
we're about to really caring about beatings

698
00:55:30,090 --> 00:55:32,200
of these different representations

699
00:55:36,750 --> 00:55:39,230
on the way evans represented this these

700
00:55:39,240 --> 00:55:39,940
so the

701
00:55:40,120 --> 00:55:45,110
you don't hear you have the different representations of things in relation of four months

702
00:55:45,110 --> 00:55:50,010
things about a document things that are XML stuff whatever the case that it genius

703
00:55:50,020 --> 00:55:50,750
in many

704
00:55:50,840 --> 00:55:56,030
a different way than you have absolute more than p of application that use query

705
00:55:56,040 --> 00:55:58,730
to manipulate it

706
00:55:59,630 --> 00:56:05,570
what the semantic web in the last seven years is providing technology to address this

707
00:56:05,570 --> 00:56:07,850
problem of data integration so

708
00:56:07,900 --> 00:56:13,350
we have a way to do the abstract model which is using these

709
00:56:13,360 --> 00:56:17,210
very simple data model which is rts

710
00:56:17,230 --> 00:56:23,650
OK so a graph based approach to represent all the different for source of information

711
00:56:23,910 --> 00:56:28,490
technology to move from the specific services

712
00:56:28,500 --> 00:56:30,460
octave abstract model

713
00:56:30,510 --> 00:56:32,030
and you see

714
00:56:32,120 --> 00:56:36,780
agreed which is a way to move from from XML to what yes

715
00:56:36,830 --> 00:56:41,380
and you have a way to expose like DFA

716
00:56:41,420 --> 00:56:42,380
and if a

717
00:56:42,390 --> 00:56:45,410
make data inside the neck

718
00:56:45,450 --> 00:56:50,750
xml pages so you physically tagging the page and the data can distraught

719
00:56:51,400 --> 00:56:53,170
so our technology to

720
00:56:53,180 --> 00:56:56,340
to move from here to here

721
00:56:56,350 --> 00:57:01,390
and then you have also technology to characterize you to more than in terms of

722
00:57:01,390 --> 00:57:06,500
all an ontological language so the relationship that kind of resources you might collating and

723
00:57:06,510 --> 00:57:10,390
so forth like rdfs how scores

724
00:57:10,400 --> 00:57:12,250
and you have your query language

725
00:57:12,270 --> 00:57:15,900
it was possibility to try to a from the top

726
00:57:15,930 --> 00:57:19,720
and clearly that's more

727
00:57:19,770 --> 00:57:22,770
so more going back to this picture from evil

728
00:57:22,820 --> 00:57:25,770
you have on this board

729
00:57:25,830 --> 00:57:29,410
they agreed that if a you have that you

730
00:57:29,460 --> 00:57:34,000
as a way to represent these and not here you have this article about inference

731
00:57:34,280 --> 00:57:36,370
as a way to access the for

732
00:57:37,080 --> 00:57:38,590
so i mean

733
00:57:38,630 --> 00:57:42,970
in these like what i try to do is really to show that the technology

734
00:57:42,970 --> 00:57:46,540
of their so it's up to us to use them in the best possible way

735
00:57:46,540 --> 00:57:51,630
to address real problems like the one that ten percent

736
00:57:51,720 --> 00:57:58,250
the ingredient is software engineering i mean we are developing a semantic web application but

737
00:57:58,370 --> 00:58:03,940
still a web application or is still an application OK so what we did in

738
00:58:03,940 --> 00:58:06,900
this seven here it also

739
00:58:06,910 --> 00:58:12,730
trying to address the problem of how we develop an application which contains semantics so

740
00:58:12,730 --> 00:58:17,240
of course we start from aspartame more the like of the buenos but in more

741
00:58:17,260 --> 00:58:21,120
which is represented here and i hope that you are familiar with these kinds of

742
00:58:21,120 --> 00:58:25,190
things because it's too hard to explain them but the basic idea is that you

743
00:58:25,190 --> 00:58:28,260
move from one prototype to another

744
00:58:31,340 --> 00:58:33,440
iteration by iteration

745
00:58:33,470 --> 00:58:38,390
to address a bigger problem so you first try to collectivity it's very easy requirement

746
00:58:38,390 --> 00:58:43,750
to develop the prototype that satisfy them in a very lightweight that you try to

747
00:58:43,750 --> 00:58:48,580
collect the more detailed requirements and then you develop a better prototype it tries to

748
00:58:48,580 --> 00:58:53,420
satisfy them in a better way and you keep doing this and can you believe

749
00:58:53,420 --> 00:58:58,600
that you got all the requirements and on in that moment develop their application

750
00:58:58,660 --> 00:59:02,540
OK so this is the standard

751
00:59:02,600 --> 00:59:08,600
four long way of developing application from design implementation but you do that on in

752
00:59:08,600 --> 00:59:14,170
the end and the rest is fast prototyping spiraling out to the core of the

753
00:59:15,630 --> 00:59:20,440
so what i will follow is our extended approach to these

754
00:59:20,500 --> 00:59:26,070
it's basically the same thing but we put the problem of inferring semantics during the

755
00:59:27,460 --> 00:59:33,270
so up here the first steps as we did that we have the user needs

756
00:59:33,440 --> 00:59:38,080
to you try to understand what the big need the need will be broken down

757
00:59:38,090 --> 00:59:42,740
into requirements but thing the first problem is very is very neat and then you

758
00:59:42,740 --> 00:59:46,830
do something which is called risk analysis you try to to

759
00:59:46,870 --> 00:59:51,640
understand if it's meaningful to satisfy these needs are if it's not possible OK so

760
00:59:51,640 --> 00:59:55,110
if you go through the steps which is something that i mean we're not addressing

761
00:59:55,110 --> 00:59:59,600
here than what you have to do is to stop the requirements we believe that

762
00:59:59,600 --> 01:00:04,040
in a semantic web application you should care about two different experts on one side

763
01:00:04,040 --> 01:00:06,590
you have a softer requirements as always

764
01:00:06,690 --> 01:00:11,370
but on the other side you have very strong requirement on the content what a

765
01:00:11,370 --> 01:00:16,210
drove there is the standard way to and data integration

766
01:00:16,270 --> 01:00:21,320
so the data is as important as the software but does integration OK so we

767
01:00:21,320 --> 01:00:26,660
believe that at this point you have to to distinguish these two parallel requirement phases

768
01:00:26,670 --> 01:00:30,990
on one side you do the standard requirement analysis for the software on the other

769
01:00:30,990 --> 01:00:36,160
side you do this same requirement analysis for the content

770
01:00:36,170 --> 01:00:42,220
then what then you started designing and redesigning as more than software engineering is more

771
01:00:42,220 --> 01:00:43,170
than trivial

772
01:00:43,220 --> 01:00:47,730
so you start developing more than the more they will be guiding fuel through the

773
01:00:47,730 --> 01:00:53,250
entire development process to implementation in our case we believe it is possible instead of

774
01:00:53,250 --> 01:00:56,890
using UML and stand modelling frameworks to use

775
01:00:57,490 --> 01:01:02,390
semantic web languages so OWL rdfs as a way to develop these models and then

776
01:01:02,390 --> 01:01:06,450
use this model for the entire applications so as you do

777
01:01:06,460 --> 01:01:10,940
well the UML diagrams and things like that when you design

778
01:01:10,950 --> 01:01:17,900
the application here you do the model of the application ontology so the application ontology

779
01:01:17,900 --> 01:01:23,260
i'm replacing working in american dealer was originally going to give this talk but he

780
01:01:23,260 --> 01:01:24,110
had to

781
01:01:24,120 --> 01:01:26,740
only had back home so

782
01:01:26,800 --> 01:01:28,680
so i'm doing it for him

783
01:01:28,730 --> 01:01:34,010
and this talk will be of quite a different nature because what be describing is

784
01:01:35,680 --> 01:01:44,050
large scale machine learning application within microsoft with a little bit of context of what

785
01:01:44,050 --> 01:01:47,240
the machine learning problem is why we need to solve it

786
01:01:47,290 --> 01:01:49,030
what the impact is and then

787
01:01:49,080 --> 01:01:53,770
drilling down a little bit to perform our bayesian methodology and if there sometimes

788
01:01:53,790 --> 01:01:55,590
i'll i'll touch a little bit of one

789
01:01:55,600 --> 01:02:01,750
i will address the the scale problems in this this is joint work with thomas

790
01:02:01,750 --> 01:02:03,830
borchert ralf herbrich and

791
01:02:03,870 --> 01:02:10,880
and keen on our online services and advertising research group in microsoft research cambridge

792
01:02:10,980 --> 01:02:15,330
so here is the outline of the talk i'll give you first an idea of

793
01:02:15,330 --> 01:02:17,370
the applications

794
01:02:17,550 --> 01:02:23,050
within which we're working which is paid search advertising within the

795
01:02:23,150 --> 01:02:25,060
the bing search engine

796
01:02:25,220 --> 01:02:28,980
then i describe our model predictor which

797
01:02:29,020 --> 01:02:34,650
over the last summer was introduced into the search engine is now driving one hundred

798
01:02:36,840 --> 01:02:40,660
so the stuff that you hear about is actually

799
01:02:40,720 --> 01:02:42,010
the stuff running

800
01:02:42,020 --> 01:02:46,930
within our search and and then i'm not quite sure have some time for these

801
01:02:47,230 --> 01:02:48,780
these are things

802
01:02:48,790 --> 01:02:55,060
so let's let's begin by describing the task the the general context

803
01:02:55,070 --> 01:03:00,200
just to give you an idea about the advertising industry business size

804
01:03:00,250 --> 01:03:07,920
this is split according to various advertising channels here and you see it's been steadily

805
01:03:07,920 --> 01:03:09,390
going out

806
01:03:09,440 --> 01:03:15,300
and this is the GDP of denmark is microsoft revenue so that's that must be

807
01:03:15,300 --> 01:03:22,690
the reason why why are management is interested in pursuing this despite the harsh competition

808
01:03:22,770 --> 01:03:24,290
that's out there

809
01:03:24,310 --> 01:03:26,710
but we keep trying

810
01:03:27,630 --> 01:03:30,630
if we look at a little bit the growth figures just to give some more

811
01:03:30,630 --> 01:03:34,800
motivation to why one would want to be in this business here the more traditional

812
01:03:34,800 --> 01:03:40,940
advertising channels that people have been using and they all hovering at around ten percent

813
01:03:40,940 --> 01:03:42,300
growth rate

814
01:03:42,310 --> 01:03:44,140
per year

815
01:03:44,150 --> 01:03:47,320
and this is online advertising

816
01:03:47,330 --> 01:03:51,900
so again it's it comes from a relatively low level but certainly there seems to

817
01:03:51,900 --> 01:03:56,650
be some great opportunities out there and it's well known that the budget is going

818
01:03:56,650 --> 01:03:59,210
from from the sky over two

819
01:03:59,250 --> 01:04:02,070
two online advertising as well

820
01:04:02,080 --> 01:04:07,410
so this is our humble attempt to search i would recommend to tried everything now

821
01:04:07,410 --> 01:04:12,780
and then because the competition certainly must be a good thing even in this market

822
01:04:12,820 --> 01:04:18,930
and we have an advertising model that works pretty much like yahoo and google model

823
01:04:18,930 --> 01:04:19,690
as well

824
01:04:19,780 --> 01:04:24,180
so this is basically an explanation of those systems as well

825
01:04:24,230 --> 01:04:30,110
someone enters the theory into the box here and there are organic search results and

826
01:04:30,110 --> 01:04:36,080
then they're paid search results on top here and on the sidebar here and advertisers

827
01:04:36,100 --> 01:04:40,360
are keen on getting those because there's a great targeting opportunity

828
01:04:40,370 --> 01:04:44,010
because the user indicates some intent

829
01:04:44,320 --> 01:04:46,780
through the theory and

830
01:04:46,800 --> 01:04:52,070
advertisers would like to capitalize on that potentially offer their services and goods to the

831
01:04:55,160 --> 01:04:59,550
do that through an option and they bid a certain amount of this time the

832
01:04:59,560 --> 01:05:02,820
might one dollar this one minus two dollars and so on

833
01:05:02,870 --> 01:05:07,100
in order to get their stuff displayed on the page because there is typically more

834
01:05:07,100 --> 01:05:08,800
slots than there are people

835
01:05:08,850 --> 01:05:10,930
so if you slots the people

836
01:05:10,980 --> 01:05:16,110
interested in this slot but more importantly the top slots are much more interesting in

837
01:05:16,110 --> 01:05:21,040
terms of click-through rate the bottom slots here which hardly ever clicked

838
01:05:21,050 --> 01:05:25,790
and that makes for for healthy competition in that the market

839
01:05:25,890 --> 01:05:30,040
so how does this work now to do we just sort these by these numbers

840
01:05:30,090 --> 01:05:34,280
and then allocate those spaces no that's not how it works

841
01:05:34,320 --> 01:05:40,870
there's an additional quantity coming in this click-through rate here the estimated probability of click

842
01:05:40,880 --> 01:05:44,760
for this particular and if it was shown in this main line

843
01:05:44,780 --> 01:05:46,520
to actually be clicked

844
01:05:46,530 --> 01:05:51,150
because as a search and we're interested in maximizing the product of the bids

845
01:05:51,240 --> 01:05:56,640
and the probability of click because expected that we

846
01:05:56,690 --> 01:06:00,890
and again this is all true for yahoo and google as well for

847
01:06:00,940 --> 01:06:06,210
and so what is actually sorted by is the expected revenue

848
01:06:06,230 --> 01:06:07,940
based on bit and

849
01:06:07,950 --> 01:06:09,810
link and now

850
01:06:10,260 --> 01:06:15,400
we don't use the first price auction as would normally be the case but generalized

851
01:06:15,400 --> 01:06:19,630
second price auction which creates a nice incentive structures for people

852
01:06:19,680 --> 01:06:23,430
to be more likely to actually bid what they think the thing is worth without

853
01:06:23,430 --> 01:06:26,310
having to fear that they are being exploited

854
01:06:26,390 --> 01:06:30,550
and that's how these people are these figures you are i've at

855
01:06:30,560 --> 01:06:33,160
and basically you can think of

856
01:06:33,210 --> 01:06:34,620
the ordering as

857
01:06:35,190 --> 01:06:38,360
we ordered by this product bid

858
01:06:38,370 --> 01:06:40,330
times probability of click

859
01:06:40,340 --> 01:06:42,950
so the one for the first guy needs to be greater than the one for

860
01:06:42,950 --> 01:06:45,000
the second guy and so on

861
01:06:45,000 --> 01:06:46,170
a which

862
01:06:46,190 --> 01:06:51,230
and that with it you can see qualitatively has to decrease with firm size but

863
01:06:51,230 --> 01:06:55,790
when one plants that we are paper it decreases

864
01:06:55,810 --> 01:06:57,000
with the power law

865
01:06:57,020 --> 01:07:03,290
the typically extends over five or six or seven orders of magnitude remarkably long range

866
01:07:03,540 --> 01:07:07,290
of this power law and the slope is always approximately point

867
01:07:07,310 --> 01:07:09,020
little as a little more

868
01:07:09,020 --> 01:07:14,830
and this behaviour described not only as we as people later found out the firm

869
01:07:14,850 --> 01:07:21,710
growth problem but a number of other growth situations even in which i will which

870
01:07:21,710 --> 01:07:24,270
we can talk about privately later

871
01:07:24,270 --> 01:07:30,040
so the quantity that we describe as the pdf of the growth rate given the

872
01:07:30,040 --> 01:07:35,620
size of the company and that's this exponential class distribution it's much more normal is

873
01:07:35,620 --> 01:07:36,650
to gibraltar

874
01:07:36,710 --> 01:07:41,290
theory would have led us to believe so this overthrows the ten

875
01:07:41,290 --> 01:07:46,810
if brought theory very dramatically because there's no way in which this tend looks like

876
01:07:46,850 --> 01:07:48,690
a logo motivation

877
01:07:48,710 --> 01:07:53,870
and the way it decays as a power law

878
01:07:54,080 --> 01:07:56,020
very recently

879
01:07:56,140 --> 01:07:59,420
when a user logs data became available

880
01:07:59,440 --> 01:08:01,560
thanks to collaboration

881
01:08:01,810 --> 01:08:03,940
with pamela lee and rick about me

882
01:08:03,960 --> 01:08:06,920
and look and see an inference

883
01:08:06,940 --> 01:08:12,940
it became clear that in in addition to this ten distribution the little deviations in

884
01:08:12,940 --> 01:08:17,520
the wings which actually you can already see you notice reporting start to do this

885
01:08:17,520 --> 01:08:21,440
block started to so about one or two

886
01:08:21,500 --> 01:08:23,440
units of growth rate

887
01:08:23,500 --> 01:08:28,060
there is a deviation and when you have and they do you can see all

888
01:08:28,620 --> 01:08:32,790
friends and you can see that this deviation is very very real in fact ten

889
01:08:32,960 --> 01:08:36,520
is the thing that doesn't look so real so what is that what is the

890
01:08:36,520 --> 01:08:42,370
full distribution or just as montana found the truncated levy distribution with the gives rise

891
01:08:42,390 --> 01:08:47,730
to power law of large right so here the ten shape look class distribution gives

892
01:08:47,730 --> 01:08:49,230
rise to power law

893
01:08:49,270 --> 01:08:53,560
at large in the tails depart exponent happens to be three

894
01:08:53,580 --> 01:08:56,810
but don't confuse it with the other one because this is the PDF not the

895
01:08:56,810 --> 01:09:01,080
cumulative so this thread is like other three but so far as we know they

896
01:09:01,080 --> 01:09:03,890
have nothing to do with each other maybe someone in this room will find the

897
01:09:05,250 --> 01:09:06,620
OK so

898
01:09:06,620 --> 01:09:10,520
this is the new law of economics that cries out for explanation and we think

899
01:09:10,520 --> 01:09:15,980
we have that explanation with the reason number of collaborators and i have pointed to

900
01:09:16,890 --> 01:09:20,600
so let's make take message for this part of the talk and then we'll wrap

901
01:09:20,600 --> 01:09:21,420
it up

902
01:09:21,420 --> 01:09:26,730
the decline message messages the histogram of growth rates for in economics which is the

903
01:09:26,730 --> 01:09:32,480
growth rates of companies impact companies is not brought all but is will plus in

904
01:09:32,480 --> 01:09:34,620
the centre and universal

905
01:09:34,640 --> 01:09:39,290
and that this histogram crosses over to a power law in the

906
01:09:40,210 --> 01:09:41,420
there's is no

907
01:09:41,460 --> 01:09:46,630
theory that for the exponent the describes the power law for the win but there

908
01:09:46,630 --> 01:09:49,580
is a theory for the growth rate power law

909
01:09:49,750 --> 01:09:54,460
so i think i exhausted my time i think it's good place to try to

910
01:09:54,460 --> 01:10:01,690
summarise i think the key summary is motivated by these discussions i've had with some

911
01:10:01,690 --> 01:10:03,310
real statisticians

912
01:10:03,420 --> 01:10:07,230
here this meeting already and that is the statistics

913
01:10:07,230 --> 01:10:09,170
if cycled over this

914
01:10:09,190 --> 01:10:13,380
in general what the stream is not linearly separable

915
01:10:13,380 --> 01:10:15,990
so now can you say anything is not going to be there is not going

916
01:10:15,990 --> 01:10:18,560
to be any convergence here

917
01:10:18,600 --> 01:10:22,560
because the stream can be arbitrary but can you say something about the behaviour of

918
01:10:22,560 --> 01:10:24,380
the ag nevertheless

919
01:10:24,400 --> 01:10:27,330
but this is trickier invisible

920
01:10:27,370 --> 01:10:30,330
can say i mean this system is arbitrary and the other is not is going

921
01:10:30,330 --> 01:10:31,290
to be

922
01:10:31,350 --> 01:10:35,080
erratic going economic out they might be up to date

923
01:10:35,770 --> 01:10:38,100
completely opposite

924
01:10:38,150 --> 01:10:43,480
was gonna walk back and forth endlessly but what the point here but still

925
01:10:43,480 --> 01:10:51,110
we can make visible analysis even in this apparently hopelessness of the situation

926
01:10:51,270 --> 01:10:54,600
so the idea

927
01:10:54,610 --> 01:10:58,380
o k

928
01:11:07,630 --> 01:11:12,310
eventually in this is what i'm going to prove

929
01:11:12,880 --> 01:11:13,870
and of course

930
01:11:13,880 --> 01:11:20,150
it's a question the sky be infinitely many you so which one is fine

931
01:11:26,440 --> 01:11:27,900
o work

932
01:11:33,500 --> 01:11:37,610
should be questions before going

933
01:11:37,850 --> 01:11:45,060
can get

934
01:11:49,980 --> 01:11:52,690
the very is slightly inference

935
01:11:52,710 --> 01:11:59,420
device different properties he was asking if you can

936
01:11:59,480 --> 01:12:04,690
if but you know by using this as an optimisation problem if you can use

937
01:12:04,690 --> 01:12:06,810
a different

938
01:12:06,850 --> 01:12:08,630
up to that

939
01:12:10,350 --> 01:12:15,290
possibly using real action may perform the update

940
01:12:15,400 --> 01:12:18,940
but perhaps taking into account for the previous example

941
01:12:18,960 --> 01:12:23,880
and the answer is yes you can and will be doing this

942
01:12:23,920 --> 01:12:26,790
as an optimisation problem more closely

943
01:12:26,810 --> 01:12:31,690
in in the in the near future and we allow us a lot about relationship

944
01:12:31,690 --> 01:12:32,920
between the

945
01:12:32,960 --> 01:12:35,290
let's say the SVM problem

946
01:12:36,110 --> 01:12:39,100
as a problem in the online problem that

947
01:12:39,100 --> 01:12:40,960
idolizing here

948
01:12:40,960 --> 01:12:44,810
so we will explore a little bit the relationship between

949
01:12:44,810 --> 01:12:48,980
optimisation of online learning and what

950
01:12:49,190 --> 01:12:54,460
to the extent that to the extent that can understand them which is not too

951
01:12:54,460 --> 01:12:55,480
much so

952
01:12:55,500 --> 01:13:02,400
you don't need a lot of a lot of background in an optimisation convex optimisation

953
01:13:02,400 --> 01:13:05,380
here so let's call it a bit the

954
01:13:05,400 --> 01:13:07,540
comparison here

955
01:13:07,560 --> 01:13:09,480
now that you know

956
01:13:10,710 --> 01:13:12,040
first of all

957
01:13:12,080 --> 01:13:17,600
one way to get to see some so let's look at more ambitious goal stream

958
01:13:17,600 --> 01:13:21,420
there is no nonseparable what can i what can i say we can prove what

959
01:13:21,420 --> 01:13:26,810
kind of book and about how in i prove political views into the picture and

960
01:13:26,810 --> 01:13:31,480
he of y about minus one is the

961
01:13:31,600 --> 01:13:35,500
the indicator function of found that

962
01:13:36,080 --> 01:13:42,020
al gore brings to the perceptron made a mistake at he

963
01:13:44,600 --> 01:13:47,420
so this is zero or one

964
01:13:47,480 --> 01:13:48,900
according to

965
01:13:49,880 --> 01:13:54,350
because the current hypothesis was correct or

966
01:13:54,400 --> 01:13:56,230
wrong the

967
01:13:56,270 --> 01:14:01,190
example example industry so one way to go about it would be OK i would

968
01:14:01,190 --> 01:14:03,480
like to prove something like this

969
01:14:06,670 --> 01:14:11,310
now now i think in general in between three not necessarily linearly separable

970
01:14:11,330 --> 01:14:16,600
so want to bound the number of states this is just the count of mistakes

971
01:14:16,750 --> 01:14:23,020
and what i found by something that could be something like this

972
01:14:33,710 --> 01:14:39,060
some other ones

973
01:14:39,080 --> 01:14:42,770
so what is this this is the number of mistakes

974
01:14:42,810 --> 01:14:45,580
made on the same stream

975
01:14:45,580 --> 01:14:46,650
made by

976
01:14:46,650 --> 01:14:48,420
some fixed

977
01:14:48,460 --> 01:14:52,790
it doesn't depend on the context the prior is expected

978
01:14:52,810 --> 01:14:53,770
it is

979
01:14:53,830 --> 01:14:58,690
runners has been affected by the difference between and counting the number of states then

980
01:14:58,690 --> 01:15:01,000
it's on x x the first

981
01:15:01,040 --> 01:15:05,830
maybe regularisation things and i think in the best of the best

982
01:15:05,850 --> 01:15:09,190
linear classifiers for that

983
01:15:09,230 --> 01:15:12,130
so this could be a patients so

984
01:15:12,290 --> 01:15:16,230
to the extent that there is a good fit for something to classifier for that

985
01:15:17,100 --> 01:15:18,020
i want to be

986
01:15:18,040 --> 01:15:21,370
i want my number mistakes to be

987
01:15:21,370 --> 01:15:25,230
close to with some small additional

988
01:15:25,290 --> 01:15:26,500
this this would be a

989
01:15:26,580 --> 01:15:32,960
now this is imprecise right but i have been impressed because not useful what's the

990
01:15:32,960 --> 01:15:34,020
problem here

991
01:15:34,060 --> 01:15:36,370
the problem is

992
01:15:36,380 --> 01:15:37,750
you know

993
01:15:37,790 --> 01:15:40,690
you might not have this problem here

994
01:15:40,690 --> 01:15:48,250
in general this optimisation problem so give me a general

995
01:15:48,270 --> 01:15:54,600
general said not necessarily separable i want to minimize want to find the apply at

996
01:15:54,690 --> 01:15:59,730
play that minimizes the number of mistakes on this this this this problem is NP

997
01:16:00,960 --> 01:16:07,960
the what is the number of papers i'm not going to get into the details

998
01:16:11,250 --> 01:16:14,860
this is a little discouraging in this is that if you want to come up

999
01:16:14,860 --> 01:16:18,230
if you want if you're interested in efficiency simplicity

1000
01:16:18,250 --> 01:16:23,500
you can possibly hope to get it said you approximate

1001
01:16:25,730 --> 01:16:28,210
so it's

1002
01:16:28,250 --> 01:16:32,330
it is not the right thing to look at should be looking at something like

1003
01:16:32,370 --> 01:16:33,520
the different

1004
01:16:33,560 --> 01:16:34,560
if you want

1005
01:16:35,210 --> 01:16:39,880
still two end up with the a vicious rules so first of all that about

1006
01:16:39,900 --> 01:16:42,210
likely to the perceptron is able to

1007
01:16:42,230 --> 01:16:48,270
is called and it's like an efficient algorithm and showing average is cool because even

1008
01:16:48,270 --> 01:16:51,850
the batch problem even if you're given history

1009
01:16:51,900 --> 01:16:53,330
it's already hard

1010
01:16:53,370 --> 01:16:55,920
in competition to so

1011
01:16:56,130 --> 01:17:02,650
this can change due this is the motivation to change this objective go

1012
01:17:02,710 --> 01:17:04,270
it something

1013
01:17:07,610 --> 01:17:10,270
if something is so you want to build something

1014
01:17:10,290 --> 01:17:14,100
that you already doing optimisation which is a relaxation of the problem is how to

1015
01:17:14,100 --> 01:17:16,750
relax the problem

1016
01:17:16,880 --> 01:17:19,830
he relaxes by

1017
01:17:21,750 --> 01:17:23,670
making that

1018
01:17:23,710 --> 01:17:27,670
making the right hand side larger so you want to compare

1019
01:17:27,730 --> 01:17:30,960
we allow a linear classifier

1020
01:17:30,980 --> 01:17:35,710
which is worse performance is measured according to different

1021
01:17:35,730 --> 01:17:37,980
a different measure than mistakes

1022
01:17:38,750 --> 01:17:42,270
so and that

1023
01:17:42,270 --> 01:17:46,690
i mean marionettes you know and cannot be talking about markov chain monte carlo said

1024
01:17:46,690 --> 01:17:50,230
just before i start could i have a show of hands who has actually use

1025
01:17:51,370 --> 01:17:52,950
in their work already

1026
01:17:52,960 --> 01:17:57,320
OK and i i know from talking to some this morning from you have stated

1027
01:17:57,320 --> 01:18:01,020
that i have i apologize that it might be a bit slow hope you'll get

1028
01:18:01,020 --> 01:18:04,610
something out of it but again to give you an overview of just why we

1029
01:18:04,610 --> 01:18:07,680
would do monte carlo of any sort at tall and

1030
01:18:07,700 --> 01:18:09,130
for those of you who already

1031
01:18:09,150 --> 01:18:14,090
no infinity might not see anything new intel maybe the second

1032
01:18:14,110 --> 01:18:20,490
so using a lot about the probability distributions graphs and in the morning and so

1033
01:18:20,490 --> 01:18:21,900
i'm going to start with something

1034
01:18:21,920 --> 01:18:24,320
very trivial it is i think probably the first

1035
01:18:24,350 --> 01:18:28,260
statistical problem given in high school which is how do you find the mean of

1036
01:18:28,260 --> 01:18:32,470
a bunch of objects a if i was interested in knowing the average height of

1037
01:18:32,470 --> 01:18:33,530
a bunch of people

1038
01:18:33,540 --> 01:18:37,220
i would go to measure the heights and i would add them up and divide

1039
01:18:37,220 --> 01:18:41,110
and are divided into the sum of the quantity is just the same algorithm the

1040
01:18:41,150 --> 01:18:46,100
IQ or for some reason john went after so what we're left or right handed

1041
01:18:46,140 --> 01:18:49,070
for the practical so there are people who actually go out into the sort of

1042
01:18:52,190 --> 01:18:57,910
the danger going to a lot of technical lectures about graphical models and probability distributions

1043
01:18:57,910 --> 01:19:02,770
is that you end up adopting the funny language so let's say instead of

1044
01:19:02,790 --> 01:19:06,490
knowing the height of lectures here i was interested in knowing the average height of

1045
01:19:06,490 --> 01:19:08,550
people in cambridge then

1046
01:19:08,600 --> 01:19:13,350
that's a some of the height of each person from the old people in cambridge

1047
01:19:13,350 --> 01:19:16,040
by by the number of people in cambridge and when you go to these lectures

1048
01:19:16,040 --> 01:19:20,290
in out of the language where you say this is an intractable inference problem because

1049
01:19:20,290 --> 01:19:22,270
it's very large computation

1050
01:19:22,270 --> 01:19:26,800
whereas we know that you know this isn't a difficult problem all you actually need

1051
01:19:26,800 --> 01:19:29,100
to do is just go and grab a bunch of people look at how high

1052
01:19:29,100 --> 01:19:32,630
they are averaged the heights and you have a pretty good idea of what the

1053
01:19:32,630 --> 01:19:36,040
average people in cambridge and

1054
01:19:36,050 --> 01:19:39,680
but a lot of the problems that we lost about how difficult they are

1055
01:19:39,690 --> 01:19:41,130
it's just as easy

1056
01:19:41,130 --> 01:19:44,320
to to solve problems we are interested and say

1057
01:19:44,330 --> 01:19:49,690
if you've got any integral that's an expectation is an integral over probability distribution of

1058
01:19:49,690 --> 01:19:54,650
some function you can play exactly the same trick instead of actually on your computer

1059
01:19:54,650 --> 01:19:59,020
visiting every little element of your space and measuring the height of your function and

1060
01:19:59,020 --> 01:20:02,800
adding them all up you can just take a bunch of samples from the distribution

1061
01:20:02,800 --> 01:20:08,490
the averaging over take the empirical average over the samples that gathered and get a

1062
01:20:08,490 --> 01:20:11,210
pretty good idea of what the end call is that the

1063
01:20:11,270 --> 01:20:15,620
ideas to implement carlo that is all montecarlo in

1064
01:20:16,330 --> 01:20:17,930
an application

1065
01:20:17,950 --> 01:20:21,330
so you you had to bayesian inference problem and you have some data and you

1066
01:20:21,330 --> 01:20:25,680
wanted to make a prediction about some new quantity then

1067
01:20:27,150 --> 01:20:32,270
predictive distribution is just an average you look at the posterior distribution over the parameters

1068
01:20:32,270 --> 01:20:35,550
in order to be a probabilistic model is and you say well if any new

1069
01:20:35,550 --> 01:20:39,420
those parameters i would know how to make predictions someone told me what the ground

1070
01:20:39,420 --> 01:20:44,050
truth current model if i make predictions so i do that for every setting the

1071
01:20:44,050 --> 01:20:47,980
parameters and weight by how probable and again you just you can do it by

1072
01:20:47,980 --> 01:20:53,390
just sampling the parameters from the distribution and take an empirical average

1073
01:20:53,420 --> 01:20:58,010
so monte carlo has an obvious application whenever you're doing great inferences tentacles like this

1074
01:20:58,010 --> 01:21:01,760
is difficult it also have applications in

1075
01:21:01,770 --> 01:21:06,020
all sorts of places where probability distributions come up so if you have to fit

1076
01:21:06,020 --> 01:21:11,460
parameters using an algorithm called and then you often need to compute statistics this can

1077
01:21:11,460 --> 01:21:15,010
be written as averages and geoff hinton next week my talk a lot about both

1078
01:21:15,060 --> 01:21:22,370
machines involved in machine learning algorithm involves the sort of something average

1079
01:21:23,300 --> 01:21:27,050
so we have to be a bit more formal and thing we just grab a

1080
01:21:27,050 --> 01:21:30,590
bunch of samples and then look at the average we can we can say properties

1081
01:21:30,590 --> 01:21:31,580
about it's a

1082
01:21:31,730 --> 01:21:36,390
the we can look at the expectation of this estimator and it

1083
01:21:36,450 --> 01:21:40,240
it is trivial to show that this is unbiased say

1084
01:21:40,260 --> 01:21:43,150
on average this estimator will get the right answer

1085
01:21:43,150 --> 01:21:45,700
and you can also say well happy

1086
01:21:45,710 --> 01:21:46,770
how much will

1087
01:21:46,800 --> 01:21:50,830
the estimator deviate from the answer so you can compute the variance of the estimator

1088
01:21:50,830 --> 01:21:55,610
and was knighted that it shrinks with the number of samples say if you want

1089
01:21:55,610 --> 01:21:58,890
to veterans who discover more samples you don't have to go and implementing new algorithm

1090
01:21:58,920 --> 01:22:01,350
is just wait longer

1091
01:22:01,400 --> 01:22:06,010
and because this is the variance in we normally interested and are about the error

1092
01:22:06,010 --> 01:22:09,810
bars shrink with the square root of the number of samples although you just have

1093
01:22:09,810 --> 01:22:13,010
to wait long you end up having to wait longer and longer if you want

1094
01:22:13,010 --> 01:22:15,330
your about to be ten times shorter you're going to have to wait a hundred

1095
01:22:15,330 --> 01:22:19,810
times fungus say is the trade-off

1096
01:22:19,820 --> 01:22:21,570
OK so

1097
01:22:21,620 --> 01:22:24,640
i said i saw this example long time ago i'm sure lots of people have

1098
01:22:24,640 --> 01:22:27,940
done that a concrete example of how something works

1099
01:22:27,950 --> 01:22:30,080
say we want to approximate pi

1100
01:22:30,090 --> 01:22:32,630
which is four times the area of

1101
01:22:32,640 --> 01:22:33,970
this red

1102
01:22:34,520 --> 01:22:39,620
area if this is the unit square so we can write this as an integral

1103
01:22:39,620 --> 01:22:44,060
which is the integral of the indicator function thing i would like this area averaged

1104
01:22:44,060 --> 01:22:46,920
over a uniform distribution within the square

1105
01:22:46,940 --> 01:22:51,660
so this is like the most stupid way of computing pi can possibly imagine a

1106
01:22:51,830 --> 01:22:56,970
vehicles four times integral can now approximate this integral by sampling from the distribution which

1107
01:22:56,970 --> 01:23:00,840
is straight points in the square and then just see whether you get one or

1108
01:23:00,840 --> 01:23:03,270
zero i red circle or not

1109
01:23:03,320 --> 01:23:08,890
and so without knowing anything really about geometry can get with twelve samples the is

1110
01:23:08,890 --> 01:23:11,780
about three and that sort of the feel of monte carlo you can just get

1111
01:23:11,780 --> 01:23:14,520
a few samples and you get the general idea of what's going on because you

1112
01:23:14,520 --> 01:23:15,820
ball-park figure

1113
01:23:15,830 --> 01:23:19,310
which can be good to sort of sanity check if you're doing something else like

1114
01:23:19,310 --> 01:23:23,960
EP or different approximate inference algorithm gives you completely different ballpark number you may have

1115
01:23:23,960 --> 01:23:27,380
done something wrong you can also be patient and wait longer so i could draw

1116
01:23:27,380 --> 01:23:29,170
ten million samples

1117
01:23:29,230 --> 01:23:32,830
and i would get the is three point one four something that is not very

1118
01:23:32,830 --> 01:23:36,080
accurate and is never going to be a way to get really precise numbers so

1119
01:23:36,080 --> 01:23:39,900
if you're interested in that we probably don't want to be doing college

1120
01:23:39,920 --> 01:23:42,210
i mean

1121
01:23:43,890 --> 01:23:48,130
if you want to compute pi by numerical integration you could just use numerical quadrature

1122
01:23:48,400 --> 01:23:49,650
and with only

1123
01:23:49,670 --> 01:23:53,990
a hundred evaluations or maybe a thousand you can get to sort of accuracy you

1124
01:23:53,990 --> 01:23:55,090
can represent

1125
01:23:56,700 --> 01:24:00,220
you really don't want to be the one carlo sort of a sort of indigo

1126
01:24:00,250 --> 01:24:04,360
and this is summarized by alan cycle is a funny guy and he this with

1127
01:24:04,360 --> 01:24:08,830
this quote with the opening to his lecture course on monte carlo methods

1128
01:24:08,850 --> 01:24:14,140
and he has actually done an awful lot of good work in monte carlo methods

1129
01:24:14,140 --> 01:24:18,160
as well and i don't completely agree with this quite well i sort of agree

1130
01:24:18,160 --> 01:24:19,230
with that but

1131
01:24:19,250 --> 01:24:24,210
i think you could replace colour with anything because if x is the method it

1132
01:24:24,210 --> 01:24:27,460
should only be used when all the other methods were so you can agree with

1133
01:24:27,460 --> 01:24:31,150
the homework that he had to do that night from eight thirty version of himself

1134
01:24:31,210 --> 01:24:33,870
bring it back to six thirty and it's done on time

1135
01:24:33,890 --> 01:24:37,650
OK so he does that he gets to eight thirty eight thirty cabins like

1136
01:24:37,700 --> 01:24:41,400
what i did do there's no time to do it it's bedtime and like women

1137
01:24:41,400 --> 01:24:44,260
who was supposed to do this the two of them the six thirty calvin eight

1138
01:24:44,280 --> 01:24:48,370
thirty calvin getting the time machine and they go to seven thirty and confront the

1139
01:24:48,370 --> 01:24:51,620
seven thirty calvin was sitting on his bed reading a comic book and so here

1140
01:24:51,630 --> 01:24:52,630
they are high

1141
01:24:52,650 --> 01:24:55,710
here we are at seven thirty six my past and future

1142
01:24:55,730 --> 01:24:59,950
the comic book and do our homework you get to work for a why should

1143
01:24:59,950 --> 01:25:02,270
i do all the work you you can do it too

1144
01:25:02,270 --> 01:25:05,580
but i didn't at six thirty in out seven thirty and thirty will be too

1145
01:25:05,580 --> 01:25:09,340
late the last chance you going start writing and we have the pound you

1146
01:25:09,350 --> 01:25:13,150
my future self will be the one that hurts

1147
01:25:13,170 --> 01:25:16,410
so of course they end not doing homework

1148
01:25:16,450 --> 01:25:18,370
all of them together

1149
01:25:18,380 --> 01:25:24,190
but really this is a story about PAC MDP near bayes optimal in the

1150
01:25:24,220 --> 01:25:26,670
please that's where i thought when i was laughing about it so

1151
01:25:26,680 --> 01:25:31,970
so PAC MDP is this idea that says the future self gets is this is

1152
01:25:31,970 --> 01:25:35,110
literally what it's like to get to the cartoon second the the first thing is

1153
01:25:35,380 --> 01:25:39,990
future self gets near optimal reward PAC MDP things as an experimental but now it

1154
01:25:39,990 --> 01:25:43,920
will be some mistakes now but my future self all future steps i need to

1155
01:25:43,920 --> 01:25:48,270
be getting near optimal reward k so you know i'm willing to do the homework

1156
01:25:49,160 --> 01:25:52,100
so my future self will have had the homework done right i'm going to do

1157
01:25:52,120 --> 01:25:55,440
learning i make mistakes now benefits will come later

1158
01:25:55,450 --> 01:25:57,140
all right so that's PAC MDP

1159
01:25:57,150 --> 01:26:02,430
which sounds kind of like the guy in the story anywhere near bayes optimal

1160
01:26:02,470 --> 01:26:06,110
says that the current self is the one that's going to get near optimal reward

1161
01:26:06,110 --> 01:26:09,620
so in particular it's not willing to make mistakes

1162
01:26:12,300 --> 01:26:14,440
unless they really are important now

1163
01:26:14,460 --> 01:26:17,980
it's not going to try something that may eventually lead to be able to get

1164
01:26:18,090 --> 01:26:19,620
high future reward

1165
01:26:19,630 --> 01:26:22,150
for future version of itself

1166
01:26:22,190 --> 01:26:26,910
it has to impact the expected value of the current page doesn't

1167
01:26:26,960 --> 01:26:30,710
don't even by just put it off because it's not me anymore somebody else

1168
01:26:31,930 --> 01:26:34,350
and we want around here this is the the

1169
01:26:34,350 --> 01:26:38,610
future selves count as part of the reward here but there are that they get

1170
01:26:38,610 --> 01:26:40,810
is kind of only attributable back to me

1171
01:26:40,820 --> 01:26:45,510
through the discount factor in for expected value so if it's even bigger if it's

1172
01:26:45,510 --> 01:26:46,680
far enough away

1173
01:26:46,740 --> 01:26:50,820
i don't care mistakes mistakes will happen later this is mistakes are allowed to happen

1174
01:26:50,820 --> 01:26:52,690
later so let's do that now

1175
01:26:52,710 --> 01:26:56,380
so with that kind of interesting and maybe feel like i always felt like i

1176
01:26:56,380 --> 01:26:59,620
was doing this PAC MDP stuff and it was the wrong problem but at least

1177
01:26:59,620 --> 01:27:02,370
the solvable problem this is unsolvable but

1178
01:27:02,370 --> 01:27:05,370
it's sort of the right problem now not so sure now kind of willing to

1179
01:27:05,370 --> 01:27:08,980
believe that this is a right problem for little bit of human literature it turns

1180
01:27:08,980 --> 01:27:12,340
out that human behavior is somewhere in between it's neither of these any kind of

1181
01:27:12,340 --> 01:27:17,080
pure sense human behaviour has been shown to be consistent with something called hyperbolic discounting

1182
01:27:17,340 --> 01:27:22,060
so this new bayesian is is using regular geometric discounting it says that the words

1183
01:27:22,070 --> 01:27:23,660
in the future words count

1184
01:27:23,680 --> 01:27:28,840
gamma two however far in the future they are hyperbolic discounting says it decays more

1185
01:27:28,840 --> 01:27:30,850
slowly than that

1186
01:27:30,860 --> 01:27:34,050
OK and the result of that is it's it's it's actually going to make some

1187
01:27:34,050 --> 01:27:36,810
mistakes now and make some mistakes later

1188
01:27:36,820 --> 01:27:41,060
and i know that some compromise between these two people just broke in or we

1189
01:27:41,060 --> 01:27:44,160
had entirely the wrong model but it's interesting to me that sort of neither of

1190
01:27:44,160 --> 01:27:45,350
these two when you watch

1191
01:27:45,450 --> 01:27:50,350
birds hiding seeds in the springtime and if it is hyperbolic discounting models

1192
01:27:50,360 --> 01:27:55,150
we have a learning algorithm that uses these bayesian priors to make decisions which i

1193
01:27:55,150 --> 01:27:56,700
don't think i have time to give in

1194
01:27:56,740 --> 01:28:00,280
any detail but let me just get out for you so

1195
01:28:00,290 --> 01:28:03,140
the basic idea is

1196
01:28:03,170 --> 01:28:05,160
we're maintain these priors

1197
01:28:05,180 --> 01:28:09,330
and to get optimism under uncertainty and to get this kind of

1198
01:28:09,370 --> 01:28:15,080
this PAC MDP kind of kind of property from it each time it takes this

1199
01:28:15,080 --> 01:28:19,260
every once in a while resamples the poster what possible models might be

1200
01:28:19,260 --> 01:28:22,160
it's teaches them together to form an MDP

1201
01:28:24,180 --> 01:28:28,230
chooses behaviours and it's the best of all the models in the samples that would

1202
01:28:28,230 --> 01:28:32,470
perform the with regard to kind of the union of all these models

1203
01:28:32,570 --> 01:28:35,810
and it's going to keep using the policy that learned until something new is learned

1204
01:28:35,810 --> 01:28:41,060
the posterior changes and they can resample from the posterior and and repeat this and

1205
01:28:41,060 --> 01:28:42,850
we were able to show is that

1206
01:28:43,530 --> 01:28:46,870
there are several different ways that you could achieve this sort of goal i think

1207
01:28:47,100 --> 01:28:50,260
but we able to show for this kind of algorithm is if the set is

1208
01:28:50,260 --> 01:28:53,600
big enough to the number of models that sampling in reasoning about is large enough

1209
01:28:53,810 --> 01:28:57,620
then we can actually get near optimality with high probability

1210
01:28:57,620 --> 01:28:59,800
with a polynomial number of samples

1211
01:29:00,870 --> 01:29:03,340
what's cool about this is it will

1212
01:29:03,340 --> 01:29:09,240
say C transpose X minus the dual objective then you can show that that's equal to

1213
01:29:09,240 --> 01:29:15,020
Z transpose S  and that's actually easy to see from this condition if you multiply

1214
01:29:15,030 --> 01:29:19,840
this vector on the left with the this vector X Z then on the left you

1215
01:29:19,840 --> 01:29:24,830
get S transpose Z on the right because this is askew symmetric matrix the product

1216
01:29:24,830 --> 01:29:28,940
for this first term will be zero if you multiply with X Z on the

1217
01:29:29,220 --> 01:29:35,780
left so on the right you get C transpose X plus B transpose Z so anyway

1218
01:29:35,780 --> 01:29:40,940
if this is satisfied by X S and Z then you always have this equality so

1219
01:29:40,940 --> 01:29:44,460
this last condition is just another way of saying that the primal  objective is equal

1220
01:29:44,460 --> 01:29:50,300
to the dual objective right so these together give you the optimality conditions feasibility and then

1221
01:29:50,310 --> 01:30:01,780
equality between primal and dual objectives is that clear and this should be a yeah so the second

1222
01:30:01,780 --> 01:30:10,660
part of this lecture will discuss barrier methods for conic optimization problems

1223
01:30:10,660 --> 01:30:15,480
so first some history so the history of barrier methods actually goes back to the sixties

1224
01:30:15,480 --> 01:30:22,440
there was a method called the sequentially unconstrained minimization technique or SUMT that was actually

1225
01:30:22,440 --> 01:30:28,160
developed for general convex optimization problems in this form so the idea is simply

1226
01:30:28,160 --> 01:30:34,020
that one solves a sequence of unconstrained optimization problems you take the objective

1227
01:30:34,020 --> 01:30:39,710
weighted with the parameter T and then you add this term it's called the

1228
01:30:39,720 --> 01:30:44,220
barrier logarithmic barrier function for the inequalities so you can show that this is a

1229
01:30:44,220 --> 01:30:50,000
convex function if the F Is are convex and the purpose is to of this term it's

1230
01:30:50,000 --> 01:30:58,320
a barrier function that traps X inside the feasible set strictly so you an you

1231
01:30:58,320 --> 01:31:04,350
move the inequalities and represent them indirectly using these barrier functions and then you solve

1232
01:31:04,350 --> 01:31:10,900
this for increasing values of T and so intuitively you can expect that if T

1233
01:31:10,900 --> 01:31:16,060
is sufficiently large by solving this unconstrained problem you get a solution that's feasible for

1234
01:31:16,060 --> 01:31:21,640
this problem because the barrier term will trap X inside the feasible set and then if

1235
01:31:21,650 --> 01:31:25,660
the weight on the objective is sufficiently large you get a very good approximation of the

1236
01:31:25,660 --> 01:31:33,200
optimum so that's the barrier methods from the sixties in the eighties there was a

1237
01:31:33,440 --> 01:31:38,940
sort of a revival of these methods following Karmarkar's paper on interior

1238
01:31:38,940 --> 01:31:45,300
point methods for linear programming because people realized that if you apply this method to

1239
01:31:45,300 --> 01:31:52,620
an LP and increase  T in a specific way you actually get a polynomial time

1240
01:31:52,620 --> 01:32:02,200
method so it's a method that has a polynomial time worst-case complexity and then so

1241
01:32:02,200 --> 01:32:05,500
those are interior point methods for linear programming that we'll discuss a little bit in

1242
01:32:05,500 --> 01:32:10,820
the next  slides and then in the nineties people extended these LP barrier methods

1243
01:32:10,820 --> 01:32:18,100
to just nonpolyhedral conic LPs second order cone problems semi-definite programming problems and so on

1244
01:32:18,420 --> 01:32:22,740
so we'll first look at the what these methods do for linear programming and then it'll

1245
01:32:22,740 --> 01:32:29,860
be easy to see how they extend to the general nonpolyhedral  problem so if

1246
01:32:29,860 --> 01:32:34,300
you actually use exactly this idea and apply to a linear programming problem then

1247
01:32:34,370 --> 01:32:40,320
you use this barrier function for the non-negative orthant so you can define phi of S for a

1248
01:32:40,320 --> 01:32:45,900
positive S as just minus the sum of the log of S I so that's a convex function

1249
01:32:45,900 --> 01:32:51,080
of S it's defined on the positive orthant and then you apply that to

1250
01:32:51,080 --> 01:32:56,700
B minus A X and the resulting function psi is a barrier function for this

1251
01:32:56,700 --> 01:33:03,620
set of linear inequalities just straight componentwise linear inequalities so it's defined on the interior of

1252
01:33:03,620 --> 01:33:09,520
this set  so if V minus A X is positive then that's useful as a barrier function

1253
01:33:09,520 --> 01:33:16,560
for this interior of a polyhedron right it's a smooth convex function  exists

1254
01:33:16,560 --> 01:33:23,100
if S is  positive it's convex that follows from the composition rules that we saw yesterday

1255
01:33:23,100 --> 01:33:30,240
and if you approach the boundary of this polyhedron then this function increases to infinity

1256
01:33:30,240 --> 01:33:33,120
because one of the S Is will be or one or more of the S Is will

1257
01:33:33,120 --> 01:33:39,000
become zero and then this phi function  blow up to infinity so that's what we normally

1258
01:33:39,000 --> 01:33:45,400
mean by a barrier function smooth convex function that blows up at the boundary of the

1259
01:33:45,400 --> 01:33:55,020
sets and then later we'll actually need the gradient and the Hessian of this function psi just simply

1260
01:33:55,020 --> 01:34:01,060
from the composition rule the gradient of psi is A transpose times the negative gradient of

1261
01:34:01,060 --> 01:34:05,920
this function phi and the gradient of this is just one  over S I with the negative

1262
01:34:05,920 --> 01:34:12,100
sign for the Hessian of this function you get something like this and all we'll actually need

1263
01:34:12,100 --> 01:34:16,340
to remember is that the Hessian of in the middle here is a diagonal matrix

1264
01:34:16,340 --> 01:34:22,440
positive diagonal matrix that's the Hessian of this function at the point S so the

1265
01:34:22,440 --> 01:34:26,390
Hessian of this barrier function is A  transpose  times oppose the diagonal matrix times A

1266
01:34:26,390 --> 01:34:34,020
so then we can so the idea of the barrier method is to do

1267
01:34:34,150 --> 01:34:40,660
unconstrained minimizations of the objective in the LP weighted with a barrier parameter T

1268
01:34:40,660 --> 01:34:47,360
plus this logarithmic barrier function and the set of minimizes of this are combination of

1269
01:34:47,360 --> 01:34:51,640
the two as a function of T is called the central path of the linear

1270
01:34:51,640 --> 01:34:58,060
inequalities so this unfortunately is not very clear but if I have a polyhedron and then

1271
01:34:58,060 --> 01:35:03,260
the dashed lines or the level curves of the barrier function so it's minimize somewhere

1272
01:35:03,260 --> 01:35:08,880
in the interior and then it increases to infinity if you approach the boundary of the domain

1273
01:35:08,880 --> 01:35:12,560
and then if I look at the path the set of minimizers of this function for

1274
01:35:12,560 --> 01:35:18,000
different values of T I get this curve through the interior of the feasible set

1275
01:35:18,060 --> 01:35:24,740
of the  polyhedron and it leads to an optimal solution X star of the LP

1276
01:35:25,040 --> 01:35:30,620
and intuitively that's clear that so you have these two terms and the objective the barrier function

1277
01:35:30,620 --> 01:35:35,800
that tries to push the points away from the boundary and then you have this

1278
01:35:35,860 --> 01:35:41,720
C transpose X function that decreases in this direction if C points this way so if you increase

1279
01:35:41,720 --> 01:35:46,420
the weight on the first term you actually will approach the optimum from the interior

1280
01:35:46,420 --> 01:35:52,600
of the feasible set you also can get a nice interpretation from the optipality conditions

1281
01:35:52,600 --> 01:35:57,900
for this so this is a unconstrained problem the barrier's differentiable so you

1282
01:35:57,900 --> 01:36:02,180
can just set the gradient of this equal to zero to get an equivalent

1283
01:36:02,180 --> 01:36:06,500
parameterization or definition of the central path so the condition you get is  T times C

1284
01:36:06,520 --> 01:36:12,400
that's the gradient of the first term minus A transpose times so this is the gradient

1285
01:36:12,400 --> 01:36:22,260
of the barrier function is A transpose times the gradient of phi which is just a

1286
01:36:22,260 --> 01:36:28,600
positive a  negative vector defined as the slacks at that point so to compute the gradient

1287
01:36:28,600 --> 01:36:33,780
of X you compute the slacks B minus A X those are positive numbers and then you have

1288
01:36:33,780 --> 01:36:40,380
this optimality condition so the C points in this direction so you see

1289
01:36:40,380 --> 01:36:46,300
at a point where on the central path the second term A transpose

1290
01:36:46,300 --> 01:36:51,760
the gradient of the ba barrier is perpendicular to the level curve through that

1291
01:36:51,760 --> 01:36:56,220
point right so if the dashed line the dashed line is the level curve of this

1292
01:36:56,220 --> 01:37:02,640
barrier function then the gradient at this point is perpendicular to the it's a normal to

1293
01:37:02,640 --> 01:37:09,050
it's an area of machine learning it is really very crucial practical importance and it's

1294
01:37:09,050 --> 01:37:16,330
something that seems to require genuinely novel algorithms and novel styles of analysis

1295
01:37:17,750 --> 01:37:19,760
just two

1296
01:37:19,780 --> 01:37:22,710
motivated to little bit and just two

1297
01:37:22,730 --> 01:37:24,960
start specifying the model

1298
01:37:24,960 --> 01:37:29,690
a lot of the a lot of the work a lot of what is known

1299
01:37:29,690 --> 01:37:36,620
about classification is in this setting where every data point has an associated label OK

1300
01:37:36,900 --> 01:37:39,050
but in reality

1301
01:37:39,080 --> 01:37:44,110
it's frequently of very very often the case that the rule form of data is

1302
01:37:45,230 --> 01:37:49,660
and the label was something that you actually have to buy or you know actually

1303
01:37:49,660 --> 01:37:56,260
obtain it's some sort of expense OK so for instance if you're building a document

1304
01:37:56,260 --> 01:38:02,370
classification system you can download millions of documents off the web while you're away for

1305
01:38:03,300 --> 01:38:04,870
but labelling

1306
01:38:04,910 --> 01:38:09,910
one of these documents takes a long time OK you have some human being has

1307
01:38:09,910 --> 01:38:14,390
to say to look at the document and decide whether it's about sports or business

1308
01:38:14,410 --> 01:38:18,580
or health or whatever and it is something that takes effort

1309
01:38:18,590 --> 01:38:23,750
if you're building a speech recognizer you can just add a microphone down someplace and

1310
01:38:23,750 --> 01:38:28,520
obtain hours and hours of speech but if you wanted labelled and some human being

1311
01:38:28,520 --> 01:38:32,700
has got to look at the speech waveform and decide with each phoneme begins in

1312
01:38:32,700 --> 01:38:35,890
the next one ends and so on which is very painful

1313
01:38:35,950 --> 01:38:41,780
OK so these are all situations where you can easily obtain masses of unlabelled data

1314
01:38:41,920 --> 01:38:43,920
but labels come cost

1315
01:38:43,940 --> 01:38:50,080
and so what motivates the model in which some distinction is made between unlabelled data

1316
01:38:50,170 --> 01:38:52,890
and labels that are subsequently obtain

1317
01:38:54,060 --> 01:38:57,390
so here the general picture then you're in a setting where

1318
01:38:57,410 --> 01:39:01,550
there's some vast number of unlabeled points

1319
01:39:01,560 --> 01:39:07,620
in regular supervised learning what effectively happens is that

1320
01:39:07,640 --> 01:39:11,830
you get if you get labels for some of them and you ignore the rest

1321
01:39:11,850 --> 01:39:16,670
as a result the size of the dataset might not be all that large

1322
01:39:16,700 --> 01:39:20,330
but you ignore the rest of the points and now you try to separate these

1323
01:39:20,330 --> 01:39:21,920
in in fact in this case the

1324
01:39:21,950 --> 01:39:24,640
many ways to do that

1325
01:39:24,650 --> 01:39:29,550
more recently it's become

1326
01:39:29,580 --> 01:39:31,050
sort of

1327
01:39:31,070 --> 01:39:34,760
you know there's a lot of people have started looking at semi supervised and active

1328
01:39:35,570 --> 01:39:39,540
we're just as in the supervised case you labels for some of the points but

1329
01:39:39,540 --> 01:39:42,150
now we don't ignore the unlabelled data

1330
01:39:42,330 --> 01:39:45,320
in a semi supervised model

1331
01:39:45,330 --> 01:39:48,920
what happens is that you just choose to some of the points at random to

1332
01:39:48,920 --> 01:39:50,430
get labels for

1333
01:39:50,450 --> 01:39:54,490
and you take account of the locations of the unlabeled points as well so for

1334
01:39:55,320 --> 01:39:59,880
if you want to build the classifier just on the basis of these labels points

1335
01:39:59,890 --> 01:40:04,350
there are many ways to many places where you could put the boundary

1336
01:40:04,360 --> 01:40:09,460
in this case if you were to also take stock of where the unlabeled points

1337
01:40:09,460 --> 01:40:14,460
it constrains the choice is somewhat because they seem to kind of p two clusters

1338
01:40:14,490 --> 01:40:18,480
and you really want to put the boundary somewhere between those two clusters

1339
01:40:18,490 --> 01:40:22,890
so this is the sense in which in a semi supervised model you get this

1340
01:40:22,900 --> 01:40:27,800
additional information about the underlying distribution from the unlabeled points

1341
01:40:27,860 --> 01:40:31,820
in active learning you go one step further

1342
01:40:31,990 --> 01:40:38,760
once you have a few labels what you do is you try and find new

1343
01:40:38,760 --> 01:40:42,930
adaptively choose which point to query next so you look at you look at it

1344
01:40:42,950 --> 01:40:45,550
and you say oh you know i think about is going to be here may

1345
01:40:45,550 --> 01:40:48,710
be very informative point to query would be this

1346
01:40:48,860 --> 01:40:51,110
instead of say this one which

1347
01:40:51,150 --> 01:40:53,610
we're pretty sure is going to be read

1348
01:40:53,630 --> 01:40:59,770
OK so the hope is that by a you know choosing these these points very

1349
01:40:59,770 --> 01:41:05,770
intelligently you quickly converge on a good classifier is that we have to set up

1350
01:41:05,990 --> 01:41:09,260
so let me just give you a couple of examples of sort of recent case

1351
01:41:09,260 --> 01:41:14,800
studies that have involved active learning the first one is one

1352
01:41:14,830 --> 01:41:17,700
which was authored by

1353
01:41:17,730 --> 01:41:20,080
the previous speaker manfred environment

1354
01:41:20,100 --> 01:41:22,390
along with gunnar raetsch and

1355
01:41:22,900 --> 01:41:28,320
and a bunch of chemists and the setting over here is some sort of computational

1356
01:41:28,320 --> 01:41:32,640
drug design because the idea is that you want to find

1357
01:41:32,660 --> 01:41:37,130
a compound that binds to a particular target

1358
01:41:37,140 --> 01:41:45,090
and you have a vast library of compounds from which you can choose OK so

1359
01:41:45,120 --> 01:41:50,210
in this case the pharmaceutical company has got a lot of compounds can choose from

1360
01:41:50,220 --> 01:41:52,150
some of these virtual

1361
01:41:52,160 --> 01:41:56,990
you know the things that have never before been synthesized but can be synthesized on-demand

1362
01:41:57,010 --> 01:41:59,770
OK you have some examples

1363
01:41:59,810 --> 01:42:04,440
of compounds that bind to the target and you want to find more now one

1364
01:42:04,440 --> 01:42:09,180
way in which this would be one way in which this what what happened

1365
01:42:09,200 --> 01:42:11,500
is that a bunch of scientists today down

1366
01:42:11,520 --> 01:42:15,200
and they look at the look at which compounds seem to bind to the

1367
01:42:15,240 --> 01:42:18,390
the target and the come up with some hypotheses about you know what are the

1368
01:42:18,390 --> 01:42:22,830
chemical properties of the compound that make blind and then OK so based on the

1369
01:42:22,830 --> 01:42:26,480
when they do and you crawl they could just compute that score

1370
01:42:26,480 --> 01:42:32,450
and and they don't have to deal with millions of queries right of computing query

1371
01:42:32,750 --> 01:42:37,650
specific score but it's of course also weakness because what we're saying here is that

1372
01:42:37,650 --> 01:42:40,770
certain pages are relevant

1373
01:42:40,800 --> 01:42:45,150
just based on the link structure irrespective of what their content actually is right and

1374
01:42:45,150 --> 01:42:47,140
what the query was

1375
01:42:48,010 --> 01:42:51,440
in that respect right you could you could be that imagine that few keywords show

1376
01:42:51,440 --> 01:42:53,880
up on a page

1377
01:42:53,890 --> 01:42:59,350
that is really not the most authoritative page on that subject but is just overall

1378
01:42:59,350 --> 01:43:02,210
an important page and a lot of people link there

1379
01:43:02,220 --> 01:43:05,030
in that case

1380
01:43:05,050 --> 01:43:08,090
you know it might just not be optimal like

1381
01:43:08,100 --> 01:43:10,740
of course nowadays that you know they doing a lot of other things that look

1382
01:43:10,740 --> 01:43:15,240
at anchor text information for instance very important and you know tons of other things

1383
01:43:15,240 --> 01:43:19,180
but i mean that's kind of you know these are the two most important ingredient

1384
01:43:19,180 --> 01:43:25,250
school in retrieval and the

1385
01:43:33,700 --> 01:43:40,250
right well in that case you basically just always random

1386
01:43:40,310 --> 01:43:42,670
just random

1387
01:43:42,670 --> 01:43:47,210
this teleportation thing if there is no i think is a unit that and

1388
01:43:47,220 --> 01:43:48,700
right you always

1389
01:43:50,200 --> 01:43:57,530
you always just use in a sample of random page point

1390
01:44:11,350 --> 01:44:15,720
four one one

1391
01:44:15,720 --> 01:44:19,070
is which is

1392
01:44:19,100 --> 01:44:21,830
what is

1393
01:44:22,440 --> 01:44:28,410
well you know one ten billion is something you should be able to handle

1394
01:44:30,160 --> 01:44:35,140
or when you need

1395
01:44:35,160 --> 01:44:39,920
you can do a little bit more efficient when you look at this transition matrix

1396
01:44:39,920 --> 01:44:45,320
y ten billion times ten billion pages and only let's say on average three or

1397
01:44:45,320 --> 01:44:48,030
four links of the same page

1398
01:44:48,050 --> 01:44:55,390
of course you wouldn't do the matrix vector multiplication what you have matrix

1399
01:44:55,430 --> 01:45:00,440
i have a lot of constant entries basically this entry here whatever you know one

1400
01:45:00,440 --> 01:45:04,900
minus of over and ends up being is just almost everywhere and then at some

1401
01:45:04,920 --> 01:45:10,150
o point you have significantly larger values in these right so when when you do

1402
01:45:10,150 --> 01:45:13,400
the computation you will take that into account right to be a little bit more

1403
01:45:14,530 --> 01:45:18,590
but on the other but but but but then yes i mean that's i think

1404
01:45:18,590 --> 01:45:19,500
what you do

1405
01:45:19,500 --> 01:45:20,690
right here

1406
01:45:20,700 --> 01:45:24,000
and you can do it i mean it's not

1407
01:45:24,140 --> 01:45:25,300
you know what i mean

1408
01:45:25,310 --> 01:45:29,060
if you google you can do that that's the least of your problems two two

1409
01:45:29,060 --> 01:45:31,530
to apply this poem

1410
01:45:37,270 --> 01:45:42,820
well it's trying kind of reverse engineering and what they really do

1411
01:45:43,100 --> 01:45:44,730
not sure about

1412
01:45:46,440 --> 01:45:50,390
but it can be done very efficient

1413
01:45:56,210 --> 01:45:57,770
railway here

1414
01:45:57,820 --> 01:46:01,530
OK so let's look at the second approach

1415
01:46:01,540 --> 01:46:04,570
which is due to fly called the

1416
01:46:04,590 --> 01:46:08,940
perhaps approach or hyperlink induced

1417
01:46:08,940 --> 01:46:18,810
topic search something like that so here's another approach if you could take two exploiting

1418
01:46:18,840 --> 01:46:21,230
hyperlink structure

1419
01:46:21,230 --> 01:46:26,640
the motivation here as as jon kleinberg sport but it is to be better

1420
01:46:26,690 --> 01:46:31,230
to be able to better deal with so called broad topic queries cases in other

1421
01:46:31,310 --> 01:46:38,540
different types of queries specific questions similarity queries and what not more women interested in

1422
01:46:38,540 --> 01:46:42,400
is broad topic queries like you know i want to find information on its own

1423
01:46:42,410 --> 01:46:47,540
each german philosopher and there's a lot of information out there on this topic and

1424
01:46:47,540 --> 01:46:49,160
i'd like to know what is really the

1425
01:46:49,790 --> 01:46:53,290
o point and authoritative work

1426
01:46:53,310 --> 01:46:59,750
and what not so with broad topic queries were facing this abundance problem as he

1427
01:46:59,750 --> 01:47:00,710
he is

1428
01:47:03,270 --> 01:47:03,730
the story

1429
01:47:30,770 --> 01:47:31,390
you know

1430
01:47:38,730 --> 01:47:39,460
you can

1431
01:47:42,080 --> 01:47:42,810
what you

1432
01:48:05,960 --> 01:48:06,480
all the

1433
01:48:21,270 --> 01:48:21,710
can you

1434
01:48:43,440 --> 01:48:46,140
i don't think you

1435
01:48:51,140 --> 01:48:53,350
three which are

1436
01:49:14,560 --> 01:49:15,040
thank you

1437
01:49:17,230 --> 01:49:17,620
you can

1438
01:49:42,810 --> 01:49:43,370
you can

1439
01:49:44,020 --> 01:49:44,560
and you

1440
01:50:15,390 --> 01:50:16,410
video machines

1441
01:50:23,730 --> 01:50:24,980
in nature you

1442
01:50:27,460 --> 01:50:31,750
and what have you are interested in the test description

1443
01:50:33,310 --> 01:50:37,100
you bring and these are the

1444
01:50:37,560 --> 01:50:38,560
what you see here

1445
01:50:39,910 --> 01:50:40,440
so what you

1446
01:50:42,730 --> 01:50:43,020
and the

1447
01:50:45,330 --> 01:50:47,170
what you think people in the world

1448
01:50:47,390 --> 01:50:49,890
they is many ways

1449
01:50:52,650 --> 01:50:53,690
two ago

1450
01:50:54,770 --> 01:50:56,060
to talk about

1451
01:51:02,980 --> 01:51:04,370
thank you very much

1452
01:51:04,600 --> 01:51:05,170
just part

1453
01:51:05,920 --> 01:51:06,500
that's not

1454
01:51:12,120 --> 01:51:13,250
brain regions

1455
01:51:13,790 --> 01:51:14,770
using the

1456
01:51:17,210 --> 01:51:19,350
and works

1457
01:51:23,060 --> 01:51:23,770
and here

1458
01:51:40,310 --> 01:51:40,580
the we

1459
01:51:42,870 --> 01:51:44,370
as you can

1460
01:51:46,890 --> 01:51:49,270
so the brain

1461
01:52:04,120 --> 01:52:04,730
thank you

1462
01:52:04,730 --> 01:52:08,770
patterns where he's creating tree structures out of these queries

1463
01:52:08,850 --> 01:52:14,770
like a followed by b followed by c combined with in nodes of these followed

1464
01:52:14,770 --> 01:52:19,560
by operators and one top node which is the tree root of whether complex event

1465
01:52:19,640 --> 01:52:24,100
created and dealing with this

1466
01:52:24,100 --> 01:52:27,450
more meaningful structure you can detect quite lot like overlap

1467
01:52:27,480 --> 01:52:28,750
and so on

1468
01:52:28,770 --> 01:52:31,810
and this is his methodology

1469
01:52:31,830 --> 01:52:34,520
there is a

1470
01:52:34,580 --> 01:52:40,020
and again cyclic workflow of creating these patterns like generating them first and then the

1471
01:52:40,020 --> 01:52:41,500
recommendation of

1472
01:52:41,930 --> 01:52:43,230
what other people

1473
01:52:43,270 --> 01:52:45,980
have been doing so finding similar event patterns

1474
01:52:46,000 --> 01:52:50,750
in the system and then execution which is the run time phase which create statistics

1475
01:52:51,080 --> 01:52:54,890
so i have written the pattern a followed by b followed by c but c

1476
01:52:54,890 --> 01:52:59,660
never happened so i can only detect this after the execution phase so we're mixing

1477
01:52:59,660 --> 01:53:05,640
design time and run time by creating this cycle of event processing evolution then is

1478
01:53:05,640 --> 01:53:07,310
the way of

1479
01:53:07,330 --> 01:53:08,620
saying yes

1480
01:53:08,620 --> 01:53:11,370
i might have modelled this in the wrong fashion let's

1481
01:53:11,410 --> 01:53:12,830
strike the sea

1482
01:53:12,890 --> 01:53:17,140
and just to take a maybe which then brings back to the start

1483
01:53:17,230 --> 01:53:19,600
of creating this new patterns

1484
01:53:19,620 --> 01:53:24,540
and there's semantic modeling i don't want to go into too much detail where we

1485
01:53:24,540 --> 01:53:30,080
model events an event types and we want to model complex event patterns and the

1486
01:53:30,120 --> 01:53:34,100
problem space so that we can get this semantic

1487
01:53:34,120 --> 01:53:42,100
vocabulary of talking about related patterns i think that's that's enough and this is about

1488
01:53:42,120 --> 01:53:45,350
the product based inference engine

1489
01:53:45,370 --> 01:53:48,100
so we're doing rule based

1490
01:53:48,100 --> 01:53:50,700
complex event processing with on the fly

1491
01:53:50,700 --> 01:53:52,040
reasoning so

1492
01:53:52,060 --> 01:53:55,230
if i get an event of a certain subtypes

1493
01:53:55,250 --> 01:53:59,230
i will for example immediately be be able to detect this event also in the

1494
01:53:59,230 --> 01:54:04,950
pattern of the super type so so we have proper inference and which is very

1495
01:54:04,950 --> 01:54:07,930
nice feature of of this problem based

1496
01:54:08,020 --> 01:54:13,270
the event processing engine which allows me to write more declarative patterns so i can

1497
01:54:13,270 --> 01:54:18,600
i can detect events which are in relationship to other events without explicitly modelling a

1498
01:54:18,620 --> 01:54:22,520
very very long pattern which lots of if then else

1499
01:54:22,540 --> 01:54:26,000
which is more in the sense of of knowledge management

1500
01:54:26,020 --> 01:54:29,850
and there's different dialect

1501
01:54:30,790 --> 01:54:33,140
event processing languages

1502
01:54:36,310 --> 01:54:38,200
most notably

1503
01:54:38,290 --> 01:54:43,390
the SQL sequel dialect was amended by many people

1504
01:54:43,410 --> 01:54:49,700
for example by the s four open source engine and which again provides you with

1505
01:54:50,040 --> 01:54:53,180
select from where syntax

1506
01:54:53,200 --> 01:54:56,750
which is a very nice starting point for creating an event processing system so if

1507
01:54:56,750 --> 01:54:58,250
you ever

1508
01:54:58,250 --> 01:55:02,710
you have to create a streaming system as a prototype for example in java

1509
01:55:02,730 --> 01:55:09,930
and this is often the way to do prototyping which is a nice java library

1510
01:55:10,270 --> 01:55:15,410
with no graphical user interface suggests import this into your program and you can

1511
01:55:15,830 --> 01:55:20,450
right new queries like to select a followed by b followed by c and you

1512
01:55:20,450 --> 01:55:23,890
will create the java classes a b and c

1513
01:55:23,910 --> 01:55:28,700
and you fill them in real time into this library and the library will call

1514
01:55:28,710 --> 01:55:29,580
the trigger

1515
01:55:29,580 --> 01:55:35,930
once the complex event is detected so this is a very simple SQL style processing

1516
01:55:35,930 --> 01:55:38,700
engine and

1517
01:55:38,750 --> 01:55:41,410
the rest of these are all commercial

1518
01:55:42,500 --> 01:55:47,950
most importantly this is an enterprise service bus company in the US

1519
01:55:47,970 --> 01:55:51,660
which is famously selling complex event processing to businesses

