1
00:00:00,000 --> 00:00:03,800
whatever it is I'm not saying you have to use last year's data by any means its an example

2
00:00:03,800 --> 00:00:06,030
of what you might use in building a prior

3
00:00:08,280 --> 00:00:11,960
but but the mathematical point is either of these choices still leads you

4
00:00:11,960 --> 00:00:15,690
to the same position but you're not combining the data

5
00:00:15,730 --> 00:00:20,070
you can either combine them completely or not at all but you can't do anything inbetween and the effects are

6
00:00:20,070 --> 00:00:24,650
simply that you slightly regularize the estimates so

7
00:00:24,690 --> 00:00:26,010
for example

8
00:00:26,030 --> 00:00:29,880
hospital A we really annoyed about this cause it had this perfect rate before

9
00:00:29,880 --> 00:00:32,480
alright it had this perfect

10
00:00:32,480 --> 00:00:37,030
didn't kill anybody before but now these bayesians are telling us

11
00:00:37,420 --> 00:00:41,360
we a new mortality rate because

12
00:00:41,440 --> 00:00:45,590
bringing in the prior information is reflecting the platform they were a bit lucky they didn't do

13
00:00:45,590 --> 00:00:52,070
very many of operations there were a bit lucky to to to to

14
00:00:52,190 --> 00:00:55,710
they were a bit lucky to get away with it

15
00:00:55,960 --> 00:00:59,570
let me get back to this point I'm not for a minute suggesting this is a serious analysis of

16
00:00:59,570 --> 00:01:05,260
this data I'm trying to show you the the the implications of different styles

17
00:01:05,260 --> 00:01:06,280
of modelling

18
00:01:06,320 --> 00:01:09,990
and of course if you were really in the position of monitoring public health you you'd

19
00:01:09,990 --> 00:01:13,820
take the process of a prior choice very seriously

20
00:01:15,780 --> 00:01:20,420
which is best well neither of those is best really they both have problems

21
00:01:20,420 --> 00:01:27,800
cause you only got the data from hospital I used in inferring theta I

22
00:01:32,980 --> 00:01:36,500
here's a here's a slightly different way to think about it

23
00:01:40,590 --> 00:01:42,110
let's imagine that

24
00:01:42,710 --> 00:01:47,360
twelve hospitals are not the only hospitals in the world these twelve hospitals are themselves

25
00:01:47,360 --> 00:01:50,420
a sample from a collection of hospitals

26
00:01:50,860 --> 00:01:52,210
and that

27
00:01:52,260 --> 00:01:56,610
across that collection of hospitals there is variation of theta I

28
00:01:56,630 --> 00:02:00,460
so that's one way one way to give a concrete

29
00:02:00,840 --> 00:02:04,390
as it were interpretation of what a prior distribution might mean it's really a

30
00:02:04,620 --> 00:02:08,860
it's not a it's not just a degree of belief now it's a model about variation

31
00:02:08,860 --> 00:02:10,050
across the different hospitals

32
00:02:11,440 --> 00:02:15,400
let's suppose we take this beta distribution we'd we'd be in the same position

33
00:02:15,400 --> 00:02:20,670
of how do we choose values for Alpha and Beta Well there's circle empirical bayes approach

34
00:02:20,670 --> 00:02:24,900
I'll say a liitle bit more about this tomorrow where you get your Alpha and Beta

35
00:02:24,940 --> 00:02:26,550
From analyzing this year's data

36
00:02:28,120 --> 00:02:32,920
so you do something like this I'm not advocating this in the details are not important but

37
00:02:32,920 --> 00:02:38,600
take something like this year's data you you summerize you'd find means and variations means and variances

38
00:02:38,600 --> 00:02:43,920
and you'd you'd you'd use that

39
00:02:43,960 --> 00:02:48,360
that observed empirical variation in the

40
00:02:48,400 --> 00:02:53,360
mortality rates as a guide to the population variation in hospitals

41
00:02:53,440 --> 00:02:56,940
sounds like a sort of sensible thing to do

42
00:02:57,050 --> 00:03:00,960
and the difficulty with it mathematically and

43
00:03:01,030 --> 00:03:05,590
in in principle is that you're using the data twice you'd be essentially using the data

44
00:03:05,590 --> 00:03:08,420
to decide you prior and you use that that

45
00:03:08,460 --> 00:03:13,400
prior to analyze the data and so each bit of data comes into the analysis twice and the effect of

46
00:03:13,500 --> 00:03:16,440
that is is is a bit like saying we got twice as much data

47
00:03:16,440 --> 00:03:17,820
as we really have

48
00:03:17,860 --> 00:03:23,230
but of course you have more data you have less variation so the effect is that you're going to estimate

49
00:03:23,320 --> 00:03:27,820
how precise you really are

50
00:03:27,860 --> 00:03:34,800
so the the the hierarchical approach avoids all these horrible pitfalls it it avoids

51
00:03:34,800 --> 00:03:40,680
you having to take the extreme view about whether hospitals are separate or all the same and it

52
00:03:40,680 --> 00:03:46,920
avoids you recognizing the differences in what without counting data twice

53
00:03:47,020 --> 00:03:51,710
and what it comes to is saying there's another level of variation here

54
00:03:51,880 --> 00:03:54,190
there are only three levels of variation

55
00:03:54,300 --> 00:03:58,460
it's not just parameters and data there are three levels

56
00:03:59,070 --> 00:04:05,230
the actual hazard of this kind of operation how how dangerous is this operation

57
00:04:05,280 --> 00:04:08,130
we don't know so that's uncertain

58
00:04:08,150 --> 00:04:09,980
OK so that's a random variable

59
00:04:10,210 --> 00:04:15,980
then this variability in the actual rate but a particular hospital

60
00:04:16,010 --> 00:04:18,000
has practised

61
00:04:18,000 --> 00:04:23,590
so some particular hospitals might be more or less risky because they have

62
00:04:23,610 --> 00:04:28,820
you know the surgeons have better practice or worse practice whichever it is

63
00:04:29,730 --> 00:04:32,730
and then this chance factors the individual patient's outcome

64
00:04:33,880 --> 00:04:35,050
you know even if you know

65
00:04:35,070 --> 00:04:41,460
that theta is exactly point zero seven that doesn't predict the actual outcome for

66
00:04:41,460 --> 00:04:43,010
any single

67
00:04:43,030 --> 00:04:48,210
single baby so we have three real levels of variation and the idea of hierarchical modeling

68
00:04:48,460 --> 00:04:53,010
is to model those three as random variables and put them all together so the approach in this case

69
00:04:53,010 --> 00:04:57,460
might be to put priors on Alpha and Beta

70
00:04:57,630 --> 00:05:03,570
so Alpha and Beta are the are the parameters that describe the population variation

71
00:05:03,610 --> 00:05:11,010
population of hospitals that that variation in in the individual

72
00:05:11,010 --> 00:05:12,190
mortality rates

73
00:05:12,500 --> 00:05:17,050
and we put priors on alpha and beta themselves there're three levels of the model

74
00:05:17,780 --> 00:05:21,180
and that the effect of that is that you get so what we call it borrowing a

75
00:05:21,180 --> 00:05:23,380
strength it provides a principled way

76
00:05:23,480 --> 00:05:29,150
of sharing information between the different hospitals because we don't know what alpha and beta is

77
00:05:29,150 --> 00:05:32,690
but the you know the first eleven hospitals

78
00:05:32,710 --> 00:05:36,940
lead to us learning something about alpha and beta and that information propagates down

79
00:05:36,940 --> 00:05:38,630
to things we say about the twelfth hospital

80
00:05:40,670 --> 00:05:45,780
so this is a common theme in physical modeling that we think about

81
00:05:45,780 --> 00:05:51,650
variation properly at different levels and certain we consider adding an extra layer and that

82
00:05:51,650 --> 00:05:55,880
the effect of that is to have a system have a method a

83
00:05:55,880 --> 00:05:58,500
mathematical method for sharing information across

84
00:06:00,940 --> 00:06:02,230
across units

85
00:06:02,540 --> 00:06:07,440
now I don't wanna say very much about graphical models cause cause Martin is going to next week but

86
00:06:07,440 --> 00:06:12,560
but these these these different choices can be symbolized

87
00:06:12,560 --> 00:06:19,220
by these pictures these are called directed acyclic graphs in these pictures the circles are

88
00:06:19,220 --> 00:06:22,100
variables that we you know we

89
00:06:22,110 --> 00:06:28,060
the random variables that we don't know and also eventually things like

90
00:06:28,060 --> 00:06:33,390
the data that we'll at some point know and the the the blocks the squares represent

91
00:06:33,390 --> 00:06:36,460
replication the plates as they're called

92
00:06:36,460 --> 00:06:41,850
and these these are the different these are the different situations and so the hierarchical model is

93
00:06:41,850 --> 00:06:45,150
numbers that are really they belong a

94
00:06:45,170 --> 00:06:48,210
don't belong to the set at all no matter fuzzy are not so the degree

95
00:06:48,210 --> 00:06:52,550
of membership is zero numbers really nicely in between a and b

96
00:06:52,590 --> 00:06:56,290
strongly belong to that set to a degree of one but somewhere here have to

97
00:06:56,580 --> 00:07:00,810
this so-called all we don't quite know a number here just tiny tiny tiny bit

98
00:07:00,810 --> 00:07:05,290
smaller than a still belongs to the set roughly in a b is would say

99
00:07:05,310 --> 00:07:10,650
this has a degree of membership of one point nine so this is what the

100
00:07:10,670 --> 00:07:15,390
this is this is it was the idea of fuzzy sets to model it using

101
00:07:15,390 --> 00:07:20,310
membership functions the membership function that assigns elements the degree of membership so now you

102
00:07:20,310 --> 00:07:24,810
can say an element belongs to the set to a certain degree not it's not

103
00:07:24,810 --> 00:07:28,030
in and out but it can also be in the world and that we could

104
00:07:28,030 --> 00:07:32,030
go back to this age example said all young people someone who is thirty years

105
00:07:32,030 --> 00:07:33,030
and one day

106
00:07:33,050 --> 00:07:37,490
OK he's underrated degree i don't know point nine five something that that chance to

107
00:07:37,490 --> 00:07:41,590
decline a little bit and push this mid-life crisis out just a tiny bit

108
00:07:41,610 --> 00:07:42,390
but but

109
00:07:42,510 --> 00:07:46,990
OK so if you remember membership function that would already be

110
00:07:47,010 --> 00:07:54,990
good interesting very very values are things when you try to model concepts that actually

111
00:07:54,990 --> 00:07:58,670
have meaning like this age example

112
00:07:58,690 --> 00:08:03,700
linguistic variable is very of labelled domain of the fuzzy sets something like age use

113
00:08:03,700 --> 00:08:08,170
hominid how many many years does somebody have

114
00:08:08,190 --> 00:08:13,130
this government is so to speak linguistic values of the labelled connection fuzzy sets understanding

115
00:08:13,140 --> 00:08:17,950
so an example would be we have age car in this environment two fuzzy sets

116
00:08:17,950 --> 00:08:20,850
and old could looks

117
00:08:21,790 --> 00:08:25,630
this is an example that always needs to be adjusted for the audience so you

118
00:08:25,630 --> 00:08:27,970
all again

119
00:08:27,990 --> 00:08:32,810
something like that if that's our linguistic variable x

120
00:08:32,850 --> 00:08:37,270
and we have are trying to figure out genius years and trying to

121
00:08:37,310 --> 00:08:41,310
determine degrees of membership for the set of young people and for the set of

122
00:08:41,310 --> 00:08:44,860
old people this you can see the they i shows the right now on the

123
00:08:45,080 --> 00:08:49,490
something we can choose ourselves i'll get back to that in a second better example

124
00:08:49,490 --> 00:08:54,490
will say everybody who's below thirty seven thirty is definitely young is not

125
00:08:54,550 --> 00:08:57,650
an inch not a little tiny bit old he's just that you belong to the

126
00:08:57,650 --> 00:09:01,170
set of young people to one when he hits thirty starts

127
00:09:01,190 --> 00:09:07,130
declining some so some forty he belongs to both sets to equal degree point five

128
00:09:07,130 --> 00:09:12,010
or something and then many hits fifty nobody nobody in his right mind so consider

129
00:09:12,100 --> 00:09:14,790
and that not that

130
00:09:14,810 --> 00:09:16,530
tells you that i'm not yet fifty

131
00:09:17,970 --> 00:09:22,230
OK because of the same size as a tiny bit more interesting have three membership

132
00:09:22,230 --> 00:09:27,650
functions and i just shows this example state of course these here the degrees of

133
00:09:27,650 --> 00:09:31,360
membership obviously add up to one that's not something that we need to require you

134
00:09:31,370 --> 00:09:36,170
can justify your membership functions any they you wish would have something size small medium

135
00:09:36,170 --> 00:09:41,560
tall OK someone who's below a hundred and fifty centimetres considered small somewhere between a

136
00:09:41,560 --> 00:09:46,470
hundred and fifty eight hundred ninety he gradually ceases to be small becomes more sort

137
00:09:46,470 --> 00:09:52,010
of medium size and then that under nineteen would say OK everybody above is definitely

138
00:09:52,010 --> 00:09:54,790
tall not medium small

139
00:09:54,970 --> 00:09:56,630
but should raise questions

140
00:09:56,650 --> 00:09:58,150
but i mean i mean it

141
00:09:58,190 --> 00:10:02,390
we are doing this would work but i mean what about a basketball player boss

142
00:10:02,500 --> 00:10:09,270
basketball player you certainly wouldn't consider one hundred ninety centimetre tall basketball player tall basketball

143
00:10:09,570 --> 00:10:13,410
and those guys hardly ever make it to the NBA so for basketball players this

144
00:10:13,410 --> 00:10:19,610
thing would completely different maybe start everybody below a hundred and eighty impossible really small

145
00:10:19,610 --> 00:10:23,870
he will not make actually there are people come forward to something there's some there's

146
00:10:23,870 --> 00:10:28,410
usually one value can be a little bit smaller impossible then you have people so

147
00:10:28,410 --> 00:10:32,570
the film and eighty to twenty twenty there's sort of medium everybody about above twenty

148
00:10:32,570 --> 00:10:39,590
twenty may be considered tall basketball players you see those guys that's probably even considers

149
00:10:40,350 --> 00:10:43,030
but the same now is to look at this time for basketball players but but

150
00:10:43,030 --> 00:10:48,330
what the jockey try to imagine someone with hundred twenty centimetres in racing horse

151
00:10:48,350 --> 00:10:53,030
so for jockeys this thing is going to be very very different probably have something

152
00:10:53,030 --> 00:10:57,790
like that medium jockey is probably somewhere in the range from fifteen sixteen and everybody

153
00:10:57,790 --> 00:11:02,470
above finances is definitely considered tall so that tells you that how we define these

154
00:11:02,480 --> 00:11:08,210
tables and chairs the those linguistic variables and is that has context dependent depends on

155
00:11:08,210 --> 00:11:12,670
the context depends on what you are trying to model i'm trying to model long

156
00:11:12,670 --> 00:11:21,430
snails fast race cars fast trucks it's a completely different scale again

157
00:11:21,430 --> 00:11:25,470
remember the same expression appeared in step one

158
00:11:25,530 --> 00:11:27,720
and is equal to

159
00:11:27,730 --> 00:11:32,180
this down here we just like that in

160
00:11:32,180 --> 00:11:37,040
and now we're just adding up over distribution so the sum over distribution is one

161
00:11:37,050 --> 00:11:39,950
we just get productivity

162
00:11:39,970 --> 00:11:47,730
OK and the third step we're trying to upper bound the training air

163
00:11:47,730 --> 00:11:50,580
we already have a bound on the training year in terms of the product of

164
00:11:50,580 --> 00:11:54,970
the normalization factors so now it turns out we can just compute

165
00:11:54,990 --> 00:11:58,710
the normalisation constants normalization factors

166
00:11:58,710 --> 00:12:01,620
directly exactly

167
00:12:01,630 --> 00:12:05,990
so to see this we just write down the definition of the normalisation constant this

168
00:12:05,990 --> 00:12:09,230
is its definition this is where you need to set the two so that

169
00:12:09,240 --> 00:12:12,650
the sum of the weights will be equal to one

170
00:12:12,650 --> 00:12:18,230
we can take the sum into two parts of the examples which are misclassified incorrectly

171
00:12:19,620 --> 00:12:23,930
and now if you just look at this

172
00:12:23,940 --> 00:12:25,960
this is just the weighted air

173
00:12:25,970 --> 00:12:28,400
of the weak classifier which is epsilon t

174
00:12:28,410 --> 00:12:30,220
and likewise the sum

175
00:12:30,240 --> 00:12:33,220
is one minus epsilon t

176
00:12:33,230 --> 00:12:38,380
and now if you plug in their choice of alpha t get exactly this expression

177
00:12:38,390 --> 00:12:41,000
so now i can answer the question where did this

178
00:12:41,030 --> 00:12:43,580
choice of alpha t come from

179
00:12:43,640 --> 00:12:48,230
alpha t was chosen so as to minimize this expression we want ziti to be

180
00:12:49,310 --> 00:12:53,900
because e t the product of ETS is an upper bound on the training air

181
00:12:53,940 --> 00:12:58,200
so we choose alpha t in this expression to make it as small as possible

182
00:12:58,200 --> 00:13:01,120
and that gives the choice about used by adaboost

183
00:13:01,130 --> 00:13:09,000
and when you plug it in you get this expression

184
00:13:10,210 --> 00:13:14,010
people are being extremely quiet so

185
00:13:14,040 --> 00:13:17,290
please to interrupt me with questions and

186
00:13:17,300 --> 00:13:21,190
kind of scary because partly it's a big group and partly and we appear on

187
00:13:21,190 --> 00:13:23,970
this high stage so you know

188
00:13:23,970 --> 00:13:27,290
it seems like he with a bolt of lightning or something if you ask a

189
00:13:27,290 --> 00:13:30,230
question that i was OK

190
00:13:30,250 --> 00:13:32,400
all right so

191
00:13:32,440 --> 00:13:35,650
here i'll ask myself the question

192
00:13:36,490 --> 00:13:39,330
so you just proved this theorem on training air

193
00:13:39,340 --> 00:13:43,240
who cares about training year that's our learning is about learning is about minimizing the

194
00:13:43,240 --> 00:13:44,340
test there

195
00:13:44,360 --> 00:13:47,630
right so what can you say about the tester

196
00:13:47,640 --> 00:13:49,320
that's a great question

197
00:13:50,800 --> 00:13:54,230
actually usually say that's a great question what's question i have no idea what the

198
00:13:54,230 --> 00:13:57,660
answer is OK

199
00:13:57,660 --> 00:14:01,420
OK so how do we expect the test error behave

200
00:14:01,500 --> 00:14:03,210
so here's a cartoon

201
00:14:03,230 --> 00:14:07,570
we're in plotting the air of the combined classifier

202
00:14:07,630 --> 00:14:12,440
the final combined classifier not the weak classifiers as a function of the number of

203
00:14:12,440 --> 00:14:15,230
rounds as a function of capital t

204
00:14:15,420 --> 00:14:20,430
so we just to prove this theorem that says that the training air goes down

205
00:14:20,430 --> 00:14:22,190
exponentially fast

206
00:14:22,240 --> 00:14:27,280
so we expect the training year to go down very very quickly exponentially fast

207
00:14:27,690 --> 00:14:30,660
and what about the test error

208
00:14:30,810 --> 00:14:32,750
well at first

209
00:14:32,840 --> 00:14:36,570
at first we're doing a better job of fitting the training data so we expect

210
00:14:36,570 --> 00:14:37,660
the test there

211
00:14:37,670 --> 00:14:39,730
to also be dropping

212
00:14:41,290 --> 00:14:42,380
every time

213
00:14:42,380 --> 00:14:45,400
we run another round of boosting

214
00:14:45,450 --> 00:14:51,010
we have one new weak classifiers to the combined classifier to the final classifier

215
00:14:51,030 --> 00:14:54,720
so that combine classifiers getting bigger and bigger

216
00:14:54,720 --> 00:14:57,050
and more and more complicated

217
00:14:57,110 --> 00:15:01,990
so at some point we expect overfitting to send we expect this test there to

218
00:15:01,990 --> 00:15:03,270
start going up

219
00:15:03,760 --> 00:15:06,650
again we expect to see overfitting because they

220
00:15:06,660 --> 00:15:08,220
combined classifier

221
00:15:08,230 --> 00:15:13,320
like i said is becoming bigger and bigger and more and more complex

222
00:15:13,400 --> 00:15:16,720
this is the entry for about this and other tutorials is

223
00:15:16,730 --> 00:15:22,640
sometimes called occam's razor occam's razor is the idea that the have two

224
00:15:24,120 --> 00:15:28,040
all else being equal you should prefer the simpler explanation

225
00:15:28,070 --> 00:15:30,520
and learning that means that

226
00:15:30,530 --> 00:15:32,520
give into me

227
00:15:33,940 --> 00:15:36,790
which perform equal equally well

228
00:15:36,810 --> 00:15:38,450
on your training data

229
00:15:38,570 --> 00:15:45,010
all else being equal you should expect the simple one to give better predictions

230
00:15:45,090 --> 00:15:47,960
now overfitting is a huge problem

231
00:15:47,960 --> 00:15:51,190
it's a big problem and it comes up all the time in machine learning so

232
00:15:51,190 --> 00:15:52,700
in a case like this

233
00:15:52,720 --> 00:15:56,810
you really want to start training right about here to get that best test error

234
00:15:57,880 --> 00:16:01,160
by your according to your training set

235
00:16:01,160 --> 00:16:05,140
performance is getting better that's why it's called overfitting because things seem to be getting

236
00:16:05,140 --> 00:16:07,870
better you seem to be getting the data better

237
00:16:07,890 --> 00:16:11,740
when in fact things are getting worse and it makes it very hard to decide

238
00:16:11,740 --> 00:16:15,400
when to stop training

239
00:16:15,420 --> 00:16:18,300
so that's what we expect

240
00:16:18,320 --> 00:16:19,810
here's an actual

241
00:16:19,820 --> 00:16:21,300
typical run

242
00:16:21,310 --> 00:16:24,950
so this is using boosting on top of c four point five

243
00:16:24,990 --> 00:16:29,320
which is the decision tree learning algorithm on

244
00:16:29,370 --> 00:16:32,500
a benchmark dataset called the letter dataset

245
00:16:32,520 --> 00:16:36,700
and again were plotting the air of the final combined classifier

246
00:16:36,700 --> 00:16:39,770
to understand the interaction of the network in order to solve the problem that's to

247
00:16:39,770 --> 00:16:41,390
better understand biology

248
00:16:41,440 --> 00:16:45,020
and one of the results that came out of we awoke he found in the

249
00:16:45,020 --> 00:16:51,440
network motifs which are basic interaction patterns if it in the case in biological networks

250
00:16:51,440 --> 00:16:53,310
this type forty but do

251
00:16:54,370 --> 00:16:56,620
in the same markets

252
00:16:56,640 --> 00:17:01,980
in the present a type of network future NATO form but at the moment so

253
00:17:01,980 --> 00:17:05,020
if you know if you know for example in one

254
00:17:05,750 --> 00:17:08,170
in fact if for example you can learn also

255
00:17:08,180 --> 00:17:13,020
about the motives in my and and learn more about the understanding that the biology

256
00:17:14,320 --> 00:17:19,770
for biological network which is also scale free was probably about she also is metabolic

257
00:17:19,770 --> 00:17:23,390
network port au port network and this is the structure of the structure is very

258
00:17:23,390 --> 00:17:29,600
complicated so we need to understand such complex networks in the to understand the biology

259
00:17:29,650 --> 00:17:31,240
another example

260
00:17:31,260 --> 00:17:35,520
it is a common design that which was also published by gordon bible-bashing and other

261
00:17:35,670 --> 00:17:37,740
in two thousand seven

262
00:17:37,930 --> 00:17:42,990
you can see what they did is it took a day in diseases

263
00:17:43,000 --> 00:17:48,720
a the supremes and the links if the gene the same gene

264
00:17:48,730 --> 00:17:54,640
a are influenced by moses and you can see by these because he for example

265
00:17:54,640 --> 00:17:59,030
that you can group the diseases and you can find it all counts are actually

266
00:17:59,030 --> 00:18:03,070
grouped in one place and that that this is another and you can learn more

267
00:18:03,070 --> 00:18:10,390
about genes which actually a cause diseases and this i think is important for many

268
00:18:10,390 --> 00:18:14,590
seem very important from this and so this is in biology and medicine

269
00:18:14,610 --> 00:18:17,930
and we go to social network i don't have to speak not because yes the

270
00:18:18,390 --> 00:18:24,070
very nice talk by rats and and he gave them together as example but for

271
00:18:24,070 --> 00:18:29,150
example what learned last years in the few years last few years by papers in

272
00:18:29,150 --> 00:18:31,230
nature and PNAS

273
00:18:31,240 --> 00:18:32,390
by these people

274
00:18:32,410 --> 00:18:36,520
is that if if you have community structure is usually goes like this that you

275
00:18:36,520 --> 00:18:40,840
have small groups subgroups of people are connected to each other

276
00:18:40,850 --> 00:18:45,630
very much and small it's only few connections to other groups and you can see

277
00:18:45,670 --> 00:18:50,980
this is the forms of same form a network you can see the groups you

278
00:18:51,040 --> 00:18:55,840
can only really coming out this according to ships and the people that i put

279
00:18:55,840 --> 00:19:00,050
together they also so this is a complex structure of network and we have to

280
00:19:00,150 --> 00:19:02,660
in order to understand better the social

281
00:19:02,670 --> 00:19:10,100
systems you have to understand the network structure and this can pass better find that's

282
00:19:10,100 --> 00:19:15,180
sort this is the figure actually that duncan watts sure yesterday which is the importance

283
00:19:15,180 --> 00:19:20,180
of the networking social you can see for example the possibility to make a friend

284
00:19:20,180 --> 00:19:24,390
if you if you have a if you have a if they are friends of

285
00:19:24,390 --> 00:19:28,130
your friend is much i know that if they are not so the father in

286
00:19:28,140 --> 00:19:33,390
the middle the closer of the ability to be a friendship and the father who

287
00:19:33,850 --> 00:19:36,010
goes down in this wood is very important for

288
00:19:36,400 --> 00:19:37,650
so shall

289
00:19:37,650 --> 00:19:40,010
understanding of such systems

290
00:19:40,020 --> 00:19:44,510
so this example is social network and show you another example

291
00:19:44,520 --> 00:19:49,290
of global climate what can be learned something from the network first of all you

292
00:19:49,290 --> 00:19:54,570
can see this is a typical network in in in USA the network of line

293
00:19:54,590 --> 00:19:58,800
and this is the the distribution which means the number of lines the number of

294
00:20:00,800 --> 00:20:03,040
lines that go form one city

295
00:20:03,050 --> 00:20:06,540
so this is the degree and p of k the number of a

296
00:20:06,560 --> 00:20:11,410
cities that have a degree of hundred for example and this is normalized and this

297
00:20:11,410 --> 00:20:15,870
is the scale free degree distribution and lambda is around one point eight

298
00:20:15,920 --> 00:20:20,270
can use this to predict something and this was shown in the paper nice paper

299
00:20:20,670 --> 00:20:24,260
like all its own vespignani PNAS two thousand six

300
00:20:24,310 --> 00:20:27,510
and they actually use the network

301
00:20:27,510 --> 00:20:28,750
is the

302
00:20:29,030 --> 00:20:32,270
and use the network an epidemic models

303
00:20:32,290 --> 00:20:35,890
and if you as an academic more than and you want to see the epidemic

304
00:20:35,900 --> 00:20:42,220
starts in the epidemic spreads and and that's what we found so they took a

305
00:20:42,480 --> 00:20:47,740
network network and they make the model of epidemics and they found that they can

306
00:20:47,740 --> 00:20:51,010
explain the can predict the principle road

307
00:20:51,260 --> 00:20:54,930
this house epidemic started in in

308
00:20:56,340 --> 00:21:02,910
eleven two thousand three will it be affected so for example the u

309
00:21:02,940 --> 00:21:08,610
is the correct prediction of outbreak the outbreak was and this is then simply using

310
00:21:09,370 --> 00:21:11,320
the airline network

311
00:21:11,330 --> 00:21:17,200
global and will now correct prediction of outbreak and also correct prediction of not like

312
00:21:17,530 --> 00:21:23,180
this other line that lies the only places where they do not predict well

313
00:21:23,200 --> 00:21:28,790
is there a green places here where they incorrect predictions predictions so you can see

314
00:21:28,790 --> 00:21:34,390
that actually one can use maybe this type of work and maybe to include also

315
00:21:34,430 --> 00:21:37,240
the network the transportation like like

316
00:21:37,370 --> 00:21:41,200
in their cars and all these things in order to better understand it

317
00:21:41,540 --> 00:21:46,900
epidemics spreading and maybe to try to prevent it if you know the network which

318
00:21:46,900 --> 00:21:52,640
causes the problem you can maybe also use some restrictions on transportation and to make

319
00:21:52,640 --> 00:21:57,670
it not that it will not be the case where the

320
00:21:59,240 --> 00:22:03,000
in use of network and application

321
00:22:03,010 --> 00:22:07,880
it is related to transport and it will be somebody that comes out is on

322
00:22:07,880 --> 00:22:12,140
the line it will be possible and the question is now

323
00:22:12,180 --> 00:22:14,710
it can be improved transport

324
00:22:14,720 --> 00:22:20,030
how can we make a simple food we need to improve transport improve all way

325
00:22:20,110 --> 00:22:25,440
on the past all the links and nodes all these a subset of nodes which

326
00:22:25,440 --> 00:22:31,890
is very important for full full a for the network which is critical to use

327
00:22:31,910 --> 00:22:36,590
tool to improve maybe i want to claim to show you that there is such

328
00:22:36,590 --> 00:22:41,740
a subset and if you can identify which is not difficult to prove this subset

329
00:22:41,740 --> 00:22:45,940
is very small and this carries most of the ground transport in the middle so

330
00:22:45,940 --> 00:22:50,950
first let me explain that the passport is very much related to the concept which

331
00:22:50,950 --> 00:22:52,870
is called the minimum spanning tree

332
00:22:52,870 --> 00:22:55,030
i mean it was running three

333
00:22:55,040 --> 00:22:59,990
this is a network let's you meaning for me and and these are the right

334
00:23:00,050 --> 00:23:04,290
the network you and the weights and then let's them now as it was for

335
00:23:04,290 --> 00:23:06,460
with increasing frequency

336
00:23:06,460 --> 00:23:09,040
if my frequency increases

337
00:23:09,100 --> 00:23:13,780
the wavelength increases which is something absurd we always think that if

338
00:23:13,830 --> 00:23:18,550
the frequency increases that the wavelength will gets shorter well i mean by that is

339
00:23:18,550 --> 00:23:19,850
not the case

340
00:23:19,870 --> 00:23:21,370
that is what dispersions

341
00:23:21,380 --> 00:23:26,030
is all about

342
00:23:26,030 --> 00:23:28,030
i can create

343
00:23:28,110 --> 00:23:32,600
a pattern like this was to wait for you which i will do

344
00:23:32,640 --> 00:23:35,660
and i do that in the graphical sense

345
00:23:35,660 --> 00:23:37,300
that i have

346
00:23:37,320 --> 00:23:40,450
two transparency is one transparency is

347
00:23:41,530 --> 00:23:45,290
black bars on its many as you will see shortly

348
00:23:45,310 --> 00:23:48,680
and they have separation the

349
00:23:48,720 --> 00:23:50,940
and i have many of them

350
00:23:50,950 --> 00:23:53,530
maybe on hundred i count

351
00:23:53,660 --> 00:23:56,100
and have another transparency

352
00:23:56,160 --> 00:23:56,990
which has

353
00:23:57,000 --> 00:23:59,610
bars which are five percent

354
00:24:00,940 --> 00:24:04,460
and also the opening between them

355
00:24:04,520 --> 00:24:06,220
five percent larger

356
00:24:06,240 --> 00:24:08,600
so here we have

357
00:24:08,620 --> 00:24:11,960
one point o five t

358
00:24:12,050 --> 00:24:15,430
and i line these up

359
00:24:15,470 --> 00:24:17,920
so this is the very first all my sheet

360
00:24:17,940 --> 00:24:20,110
this is number one

361
00:24:20,170 --> 00:24:22,920
and this is number one

362
00:24:22,930 --> 00:24:26,350
so they line up

363
00:24:26,380 --> 00:24:27,760
this one

364
00:24:27,760 --> 00:24:32,070
the opening the separation is five percent larger

365
00:24:32,130 --> 00:24:34,280
what it means when i read here

366
00:24:34,290 --> 00:24:35,630
all my sheet

367
00:24:35,640 --> 00:24:38,510
number twenty

368
00:24:38,510 --> 00:24:41,750
then i read here number nineteen

369
00:24:41,760 --> 00:24:44,810
but they are again on top of each other

370
00:24:44,830 --> 00:24:47,310
they are in phase so to to speak

371
00:24:47,320 --> 00:24:49,290
so my number twenty

372
00:24:49,340 --> 00:24:52,330
coincides with number nineteen year because

373
00:24:52,380 --> 00:24:53,930
this one

374
00:24:54,010 --> 00:25:00,570
five percent larger spacing the wavelength is five percent larger

375
00:25:00,700 --> 00:25:04,290
that means halfway in between

376
00:25:04,470 --> 00:25:06,000
the black lines rule

377
00:25:06,010 --> 00:25:08,570
from this seat well exactly

378
00:25:08,630 --> 00:25:09,860
the clips

379
00:25:09,870 --> 00:25:10,970
he opens

380
00:25:11,680 --> 00:25:13,170
of this one

381
00:25:13,180 --> 00:25:15,370
so these black ones here

382
00:25:15,490 --> 00:25:19,010
o codes to use the astronomical

383
00:25:19,040 --> 00:25:25,530
phrase all called the openings and so this whole central portion will do black

384
00:25:25,580 --> 00:25:29,210
black will be on top of light light will be impossible black and likely to

385
00:25:29,210 --> 00:25:31,180
apply here however

386
00:25:31,180 --> 00:25:34,110
it's back to what it was here

387
00:25:34,170 --> 00:25:38,570
so you're beginning to see this kind of pattern think of this for now

388
00:25:38,600 --> 00:25:40,620
as being dark

389
00:25:40,670 --> 00:25:42,100
and single this now

390
00:25:42,190 --> 00:25:44,230
has been white

391
00:25:44,270 --> 00:25:47,070
this kind of pattern is what you're going to see if i put it to

392
00:25:47,070 --> 00:25:49,310
transparency is on top of each other

393
00:25:49,330 --> 00:25:52,230
as i will do very shortly

394
00:25:52,260 --> 00:25:53,960
now comes the wonderful thing

395
00:25:53,970 --> 00:25:58,030
not only will you see such pattern

396
00:25:58,050 --> 00:25:59,850
but imagine now

397
00:25:59,860 --> 00:26:03,170
that i move one of these should eighty appreciate

398
00:26:03,170 --> 00:26:06,410
over the distance half the

399
00:26:06,510 --> 00:26:10,750
so i moved it only over this distance

400
00:26:10,770 --> 00:26:13,050
remember year

401
00:26:13,070 --> 00:26:15,620
the black ones swirl coffee

402
00:26:15,630 --> 00:26:18,520
the open once but if i shift by half the

403
00:26:18,530 --> 00:26:19,960
it will be reversed

404
00:26:19,970 --> 00:26:22,140
the black ones will no longer called

405
00:26:22,180 --> 00:26:23,310
he open ones

406
00:26:23,370 --> 00:26:29,360
and so this whole black area will instantaneously turn light again

407
00:26:29,410 --> 00:26:30,530
that means

408
00:26:30,550 --> 00:26:32,710
that is overall pattern

409
00:26:32,730 --> 00:26:35,430
moves twenty times faster

410
00:26:35,440 --> 00:26:37,140
then the motion

411
00:26:37,200 --> 00:26:39,160
of one of these sheets

412
00:26:39,260 --> 00:26:40,860
if i move this sheet

413
00:26:40,880 --> 00:26:42,870
over the full distance the

414
00:26:42,880 --> 00:26:46,340
this entire pattern that you see here which i think of as being the group

415
00:26:47,380 --> 00:26:50,690
moved twenty times faster

416
00:26:50,690 --> 00:26:53,580
and that's what i'm going to show you

417
00:26:53,720 --> 00:26:58,000
the reason why i get twenty times of course because i have chosen

418
00:26:59,350 --> 00:27:02,300
wavelengths to be five percent part

419
00:27:03,270 --> 00:27:05,390
i'm going to give them a different

420
00:27:05,450 --> 00:27:07,610
phase velocity

421
00:27:07,630 --> 00:27:11,040
the first thing that i want to do is to give them the same phase

422
00:27:13,310 --> 00:27:16,730
you see those two sheets on top of each other

423
00:27:16,730 --> 00:27:21,120
i e mark one with the red mark and one was the remark

424
00:27:21,180 --> 00:27:24,070
i think the blue one is the one that has five percent

425
00:27:24,130 --> 00:27:26,090
larger spacing

426
00:27:26,140 --> 00:27:30,320
and you see exactly what i predicted you see those dark bands

427
00:27:30,330 --> 00:27:32,490
so this idea is here

428
00:27:35,560 --> 00:27:38,180
these are the areas the dark areas where the

429
00:27:38,190 --> 00:27:43,000
black bands of one of course opening of the other

430
00:27:43,010 --> 00:27:45,910
and then in between you see

431
00:27:47,740 --> 00:27:49,170
like here

432
00:27:49,220 --> 00:27:51,640
you seems like there

433
00:27:51,760 --> 00:27:54,270
if i can manage to move them both

434
00:27:54,290 --> 00:27:56,080
with the same speed

435
00:27:56,120 --> 00:27:58,750
so i think confirmed in in my hand

436
00:27:58,750 --> 00:28:01,540
and i'm going to move them both with the same speed

437
00:28:02,630 --> 00:28:04,000
that the bars

438
00:28:04,020 --> 00:28:07,550
individual bars move exactly the same speed

439
00:28:08,530 --> 00:28:09,960
the whole para

440
00:28:10,010 --> 00:28:11,740
so that means the group velocity

441
00:28:11,750 --> 00:28:15,640
and the phase velocity at the same tried it once more there we go

442
00:28:15,700 --> 00:28:17,950
i from in my hands

443
00:28:17,960 --> 00:28:21,370
so you see the group velocity and phase velocity at the same

444
00:28:21,500 --> 00:28:26,240
now i'm going to move one relative to the other watch closely

445
00:28:26,290 --> 00:28:29,830
i'm going to move one relative to the other you can see by the

446
00:28:29,840 --> 00:28:34,260
you can tell by the by comparing the red spot was the blue spot

447
00:28:34,270 --> 00:28:37,760
after lines look compared to red blood was the blue spot you can see the

448
00:28:37,760 --> 00:28:39,480
relative motion

449
00:28:39,530 --> 00:28:41,030
and moving them now

450
00:28:41,070 --> 00:28:44,530
look how fast the group velocities compared to

451
00:28:44,600 --> 00:28:48,530
the motion between the two you can actually hardly see

452
00:28:48,630 --> 00:28:51,580
that the red one is moving relative to the blue one you can hardly see

453
00:28:52,460 --> 00:28:55,700
but look how fast the group velocity is you see that

454
00:28:55,710 --> 00:28:57,340
twenty times faster

455
00:28:57,360 --> 00:29:00,460
you see how to read one separates from the blue one of the very low

456
00:29:00,460 --> 00:29:05,730
speed that the phase velocity

457
00:29:05,780 --> 00:29:09,020
it's the difference between the phase velocity that's really what i should have said the

458
00:29:09,020 --> 00:29:11,050
difference between the face

459
00:29:11,130 --> 00:29:13,620
and so then the group velocity goes

460
00:29:13,670 --> 00:29:17,660
twenty times faster

461
00:29:17,670 --> 00:29:22,600
so now i want to return to are continuous strings

462
00:29:22,620 --> 00:29:23,670
when we

463
00:29:23,670 --> 00:29:27,680
delta was continuous strings

464
00:29:27,730 --> 00:29:31,110
we derive the wave equation

465
00:29:31,130 --> 00:29:34,950
and i still remember how we derived it

466
00:29:34,960 --> 00:29:40,070
we took into account that the tension is responsible for the restoring force

467
00:29:40,070 --> 00:29:42,260
well you started from

468
00:29:42,280 --> 00:29:46,510
again if the state space is finite that means that you should

469
00:29:46,530 --> 00:29:48,400
to get anywhere

470
00:29:48,420 --> 00:29:50,510
with probability one

471
00:29:52,650 --> 00:29:58,050
of course if you have a finite state space if the chain is irreducible

472
00:29:58,070 --> 00:30:02,580
because there is only number of finite number of

473
00:30:02,590 --> 00:30:05,560
places you can go to of states you can go to

474
00:30:05,570 --> 00:30:10,590
two of the positive pretty go somewhere is like having your

475
00:30:10,650 --> 00:30:12,930
but you want to go to this place

476
00:30:13,390 --> 00:30:17,300
the the world outside finite state space

477
00:30:17,320 --> 00:30:18,500
and so

478
00:30:18,500 --> 00:30:21,230
in the discrete state space

479
00:30:21,240 --> 00:30:23,750
rec around means that

480
00:30:23,760 --> 00:30:25,220
was number

481
00:30:25,230 --> 00:30:26,390
of the

482
00:30:26,410 --> 00:30:27,550
two estate

483
00:30:27,570 --> 00:30:30,480
is infinity and transience

484
00:30:30,490 --> 00:30:31,230
it is

485
00:30:31,240 --> 00:30:33,380
that the average number of visits

486
00:30:33,410 --> 00:30:34,560
two us they

487
00:30:34,580 --> 00:30:37,310
two state is fine

488
00:30:37,320 --> 00:30:39,050
in other words after

489
00:30:39,060 --> 00:30:40,410
seven times

490
00:30:40,430 --> 00:30:41,760
you never come back

491
00:30:41,800 --> 00:30:45,230
to this state omega

492
00:30:46,870 --> 00:30:49,800
in the going space

493
00:30:49,810 --> 00:30:55,940
you just have to replace all media bias set a which dataset a

494
00:30:56,710 --> 00:30:58,350
positive measure

495
00:30:58,360 --> 00:31:05,810
your state a is wrecker and is that if starting from any x the number

496
00:31:05,820 --> 00:31:08,710
of visits to your state is infinite

497
00:31:08,730 --> 00:31:14,530
is transient if it's finite and when the chain is irreducible

498
00:31:14,540 --> 00:31:18,500
the set of the same probably so they are all current

499
00:31:19,340 --> 00:31:21,000
all chains

500
00:31:22,210 --> 00:31:24,430
which makes him funny because

501
00:31:24,460 --> 00:31:27,550
if you think that if you have a finite number of visits to the set

502
00:31:27,680 --> 00:31:32,210
what the chain has to go somewhere else but if the space is continuous admitted

503
00:31:32,210 --> 00:31:36,790
go farther and farther away and after one doesn't come back to this entry started

504
00:31:38,360 --> 00:31:42,780
this is the one of the purposes of of infinity

505
00:31:47,550 --> 00:31:49,750
redcurrants mean that

506
00:31:49,750 --> 00:31:53,060
the the expected number of visits to the set

507
00:31:53,970 --> 00:32:00,770
infinity there's something slightly stronger is called harris recurrence and i mentioned it because this

508
00:32:00,770 --> 00:32:06,070
is the central notion for the ergodic theorem for the law of large numbers and

509
00:32:06,070 --> 00:32:09,760
harris recurrence as you know that's slightly stronger

510
00:32:09,780 --> 00:32:13,360
because not only is the expectation is infinite but

511
00:32:13,390 --> 00:32:18,440
the value of the random bible number of visits to the set a is itself

512
00:32:19,240 --> 00:32:21,020
of course it's it's strong

513
00:32:21,560 --> 00:32:26,590
the probability that the number of visitors infinity is one method you come over and

514
00:32:26,590 --> 00:32:31,930
over and over again and about all firing time you should

515
00:32:31,930 --> 00:32:35,560
to come back to our set a for every set a

516
00:32:37,880 --> 00:32:41,240
well this is course

517
00:32:44,780 --> 00:32:47,410
now we write level that

518
00:32:47,440 --> 00:32:48,790
of stability

519
00:32:48,810 --> 00:32:50,030
that there

520
00:32:50,220 --> 00:32:52,360
that if the chain is recurrent

521
00:32:52,390 --> 00:32:54,630
it was just exploring space

522
00:32:54,640 --> 00:32:59,050
in a kind of regular way that is after wall you visit set a and

523
00:32:59,050 --> 00:33:02,290
afterwards to the set a kind of

524
00:33:02,360 --> 00:33:05,640
stability that if you're on your chain a long time

525
00:33:05,650 --> 00:33:06,860
we have

526
00:33:06,870 --> 00:33:08,280
the picture

527
00:33:09,540 --> 00:33:10,850
the support

528
00:33:10,870 --> 00:33:12,710
of of your change

529
00:33:12,710 --> 00:33:15,100
OK and you want to know

530
00:33:15,470 --> 00:33:22,090
it with with regular monte carlo method this stabilisation as a kind of frequency

531
00:33:22,110 --> 00:33:26,690
that you can work with this and this is linked with the national

532
00:33:26,720 --> 00:33:28,870
of environment mission

533
00:33:29,970 --> 00:33:34,810
your change is stable if it is reckoned it would be even more stable

534
00:33:34,860 --> 00:33:40,600
hostettler are if it has an invariant measure and for measure

535
00:33:42,030 --> 00:33:44,950
which is such that if you apply the kernel

536
00:33:44,960 --> 00:33:46,210
to the measured

537
00:33:46,220 --> 00:33:48,070
you get the sense

538
00:33:48,100 --> 00:33:51,290
OK so if i should start

539
00:33:51,300 --> 00:33:56,290
measuring your sets with the major part of your reply the transition

540
00:33:56,290 --> 00:33:58,510
two your face

541
00:33:58,520 --> 00:34:00,080
only get to measure

542
00:34:00,140 --> 00:34:04,560
because it is difficult to understand if i is not the probability measure if by

543
00:34:04,560 --> 00:34:09,330
the probability measure is just tells do that if you start from

544
00:34:09,340 --> 00:34:12,290
i'm trying to work to

545
00:34:12,300 --> 00:34:16,610
no matter what and is if you if you find distribution by time and your

546
00:34:16,610 --> 00:34:18,430
destination by

547
00:34:18,440 --> 00:34:22,000
at time n plus one and of course

548
00:34:22,020 --> 00:34:23,280
over and over

549
00:34:23,290 --> 00:34:25,220
after a time and

550
00:34:26,110 --> 00:34:27,350
in the distribution by

551
00:34:27,370 --> 00:34:28,710
once you have it

552
00:34:28,720 --> 00:34:30,290
the decision

553
00:34:32,070 --> 00:34:33,500
and this

554
00:34:33,550 --> 00:34:38,490
should probably that is linked with and that if you have a

555
00:34:38,500 --> 00:34:39,770
probability measure

556
00:34:39,830 --> 00:34:44,450
that is in band for a chain of oil biochemical because kind of property it

557
00:34:44,450 --> 00:34:45,370
tells you that

558
00:34:46,190 --> 00:34:47,970
it is the fixed point four k

559
00:34:47,980 --> 00:34:50,110
mike is equal to pi

560
00:34:50,130 --> 00:34:53,860
if if i suppose measure that necessarily

561
00:34:54,370 --> 00:34:56,810
recurrent change and you cannot have

562
00:34:56,830 --> 00:35:01,830
trying to change that has the property measure as invariant distribution

563
00:35:01,850 --> 00:35:07,050
so in that case we say that the chain is positive records

564
00:35:07,080 --> 00:35:10,790
the trick is that if there exists an invariant measures

565
00:35:10,850 --> 00:35:15,440
that is not a probability measure so it is sigma finite it is an infinite

566
00:35:16,410 --> 00:35:18,810
you must have the transition

567
00:35:18,830 --> 00:35:27,710
chain markov chain may be transient and enjoy an environment measure and if it example

568
00:35:27,710 --> 00:35:30,530
is the random walk in dimension three

569
00:35:30,530 --> 00:35:32,300
if you have are working mission three

570
00:35:32,320 --> 00:35:37,570
it is transient so it would go to infinity but the measure

571
00:35:37,610 --> 00:35:39,260
is environment

572
00:35:39,270 --> 00:35:44,760
in dimension two it is recurrent and the back measure is inference

573
00:35:44,940 --> 00:35:46,610
but not for

574
00:35:46,630 --> 00:35:48,950
we just focus on this case

575
00:35:49,050 --> 00:35:55,050
she we use we use markov chain that offers achieve wrecker

576
00:35:55,080 --> 00:35:57,390
now just

577
00:36:00,240 --> 00:36:06,690
from reason why we link the existence of invariant measures and monte properties of the

578
00:36:07,990 --> 00:36:14,080
often frequencies y so important for political purposes

579
00:36:14,100 --> 00:36:16,750
do that if there exists an invariant measure

580
00:36:16,790 --> 00:36:19,810
it is not the remaining resume monk

581
00:36:19,840 --> 00:36:24,470
range properties of your change that if you run your change long enough and the

582
00:36:24,570 --> 00:36:26,500
if there is invariant measure

583
00:36:26,540 --> 00:36:29,800
you will get close to the invariant measure

584
00:36:29,830 --> 00:36:31,690
and the reason why

585
00:36:31,720 --> 00:36:34,530
is simply a fixed point property

586
00:36:35,220 --> 00:36:40,250
in the limit to the issues that from the

587
00:36:40,270 --> 00:36:41,970
arbitrary measure

588
00:36:42,040 --> 00:36:46,580
and if you run your change p is not k but it should be k

589
00:36:46,600 --> 00:36:49,190
if you're on your kernel

590
00:36:49,190 --> 00:36:50,290
ten times

591
00:36:50,360 --> 00:36:54,480
and if you take n going to infinity and and in the limit

592
00:36:54,480 --> 00:36:56,870
the two called gamma mu of a

593
00:36:56,890 --> 00:36:58,150
you start from you

594
00:36:58,150 --> 00:37:00,880
so important is actually something that is not

595
00:37:00,930 --> 00:37:02,650
often mentioned

596
00:37:02,770 --> 00:37:04,660
but is important

597
00:37:04,680 --> 00:37:05,550
is that

598
00:37:05,560 --> 00:37:06,950
even assuming

599
00:37:07,000 --> 00:37:10,160
you would be able to sample what exactly

600
00:37:10,190 --> 00:37:12,470
formed was still distribution of

601
00:37:14,780 --> 00:37:18,420
clearly because the number of viable

602
00:37:18,440 --> 00:37:22,130
your interest probably we would increase with

603
00:37:22,230 --> 00:37:27,700
then you could the computational complexity of the algorithm for sample from that would increase

604
00:37:28,540 --> 00:37:35,090
i mean sampling was always going to be inelastic but it expensive and sampling like

605
00:37:35,140 --> 00:37:37,180
then followed

606
00:37:37,190 --> 00:37:39,690
so it is important because it means that

607
00:37:39,700 --> 00:37:40,440
we learn

608
00:37:40,610 --> 00:37:42,780
if you could

609
00:37:42,820 --> 00:37:44,280
from four

610
00:37:44,390 --> 00:37:47,410
from this exactly

611
00:37:47,500 --> 00:37:50,320
then that would already solved problem

612
00:37:51,330 --> 00:37:54,460
the problem is that you want to come up with an algorithm to estimate the

613
00:37:54,460 --> 00:37:59,390
sequence of distributions which are the kind of fixed computational complexity of time

614
00:38:00,260 --> 00:38:02,820
to summarize

615
00:38:02,860 --> 00:38:04,600
we have hold

616
00:38:04,620 --> 00:38:07,430
we cannot use the full name of the cult sampling

617
00:38:07,780 --> 00:38:10,020
i don't want to sample from p

618
00:38:10,020 --> 00:38:12,140
x one n y one

619
00:38:12,200 --> 00:38:15,820
on even if i couldn't that wouldn't some

620
00:38:15,820 --> 00:38:20,610
i would also problem because i would like to come up with algorithm

621
00:38:20,630 --> 00:38:26,700
so as to approximate the with computational complexity does increase the time index

622
00:38:27,340 --> 00:38:28,750
so we need to come out

623
00:38:28,770 --> 00:38:30,440
we need to solve part

624
00:38:30,460 --> 00:38:33,450
and for going to tackle the problem

625
00:38:33,460 --> 00:38:36,700
on then we'll discuss that later on

626
00:38:38,250 --> 00:38:42,510
the first problem that is because i'm unable to form or

627
00:38:42,550 --> 00:38:45,830
from the both help x one n y one n

628
00:38:45,880 --> 00:38:49,680
i to come up with an alternative method of the element of the day

629
00:38:50,720 --> 00:38:52,400
so what do i do

630
00:38:52,420 --> 00:38:55,470
and then use a method called importance sampling

631
00:38:56,760 --> 00:38:58,850
this method

632
00:38:58,860 --> 00:39:01,450
it relies on the production

633
00:39:01,470 --> 00:39:03,140
of the looks you i

634
00:39:03,150 --> 00:39:05,350
o point distribution

635
00:39:05,360 --> 00:39:07,410
which is defined in the same space

636
00:39:07,430 --> 00:39:11,760
i the first l distribution of and so what i'm doing and how important it

637
00:39:11,760 --> 00:39:13,460
was OK

638
00:39:13,480 --> 00:39:19,340
point distribution who q which is different from the which is a function of of

639
00:39:19,340 --> 00:39:21,450
x one x two x

640
00:39:22,750 --> 00:39:27,610
the notation doesn't mean is it was still distribution you just means that is the

641
00:39:27,720 --> 00:39:34,860
probability distribution which would find it can depend on your television to keep in mind

642
00:39:34,980 --> 00:39:39,010
thing that i mean the without permission if you want to cite the fact that

643
00:39:39,010 --> 00:39:43,620
i myself web oaks distribution dependent

644
00:39:43,660 --> 00:39:50,890
on the television one for example this information would be i dimensional gaussian distribution of

645
00:39:52,390 --> 00:39:53,620
even by the the

646
00:39:53,620 --> 00:39:54,700
only the

647
00:39:55,340 --> 00:39:56,660
which is diagonal with

648
00:39:57,620 --> 00:39:59,870
that's important

649
00:39:59,880 --> 00:40:02,410
so these started i distribution

650
00:40:02,460 --> 00:40:04,720
i'm gonna ask

651
00:40:04,730 --> 00:40:06,520
i want to add properties

652
00:40:06,560 --> 00:40:11,020
i want to to be such that

653
00:40:11,030 --> 00:40:12,820
the support of q

654
00:40:12,850 --> 00:40:15,520
includes all of the target distribution

655
00:40:15,570 --> 00:40:18,070
that is wherever

656
00:40:18,210 --> 00:40:22,940
the first division i'm interested in is it i want to make sure

657
00:40:24,900 --> 00:40:28,890
also i would be is all over the way i would yes

658
00:40:28,900 --> 00:40:31,360
i want you to have some positive

659
00:40:31,640 --> 00:40:35,010
the very simple one

660
00:40:35,040 --> 00:40:37,310
but i still think the problem

661
00:40:37,360 --> 00:40:38,870
is that

662
00:40:38,890 --> 00:40:41,420
i'm going to pick two

663
00:40:41,470 --> 00:40:44,080
so is that it is very easy

664
00:40:44,120 --> 00:40:47,100
to obtain realization

665
00:40:47,150 --> 00:40:49,330
so where i might be impossible or

666
00:40:49,600 --> 00:40:52,630
in or for these guys

667
00:40:52,650 --> 00:40:57,400
because of this women is probably where i was one of the i'm going to

668
00:40:57,400 --> 00:40:58,820
select the u

669
00:40:58,860 --> 00:41:03,880
from which it very easy to obtain one demoralization

670
00:41:03,900 --> 00:41:06,220
so for example you could pick

671
00:41:07,430 --> 00:41:10,730
divide gulshan mean given by y

672
00:41:10,800 --> 00:41:12,660
on go and

673
00:41:12,710 --> 00:41:13,800
which is you know

674
00:41:14,150 --> 00:41:17,230
you can do but you pick it so that

675
00:41:17,280 --> 00:41:20,100
you know what we're all in this

676
00:41:20,220 --> 00:41:21,960
that's important

677
00:41:21,980 --> 00:41:23,350
on the this the way

678
00:41:23,360 --> 00:41:26,230
what we're have to do is is going to be very careful in the design

679
00:41:26,230 --> 00:41:27,500
of the

680
00:41:27,510 --> 00:41:30,300
OK so we've got this distribution

681
00:41:30,310 --> 00:41:32,750
i'm not going to use and i don't know

682
00:41:32,800 --> 00:41:36,060
which is based on the basis of on opening

683
00:41:36,070 --> 00:41:38,870
what i want to commit suicide at some point you

684
00:41:38,890 --> 00:41:42,460
there's already know nothing technical about it

685
00:41:47,100 --> 00:41:50,220
this because i don't think we need to introduce something which is called an problem

686
00:41:51,130 --> 00:41:52,630
on importance weight

687
00:41:52,660 --> 00:41:54,180
is a function of both

688
00:41:54,320 --> 00:41:58,340
the basic components on the validation uneasy called

689
00:41:58,350 --> 00:42:00,530
the joint distribution of x and y

690
00:42:00,560 --> 00:42:02,980
divided by the

691
00:42:03,040 --> 00:42:04,860
distribution q

692
00:42:04,870 --> 00:42:07,540
all these guys well first

693
00:42:07,800 --> 00:42:11,340
i thing these things you can compute it exactly

694
00:42:11,340 --> 00:42:13,840
we can assign truth values to

695
00:42:13,910 --> 00:42:15,650
more complex

696
00:42:15,720 --> 00:42:18,920
so the constant true and false

697
00:42:19,510 --> 00:42:21,260
standard top

698
00:42:21,280 --> 00:42:22,880
two is always true

699
00:42:22,920 --> 00:42:24,890
and so always falls

700
00:42:24,930 --> 00:42:29,310
and a and b is true is true and b

701
00:42:29,990 --> 00:42:33,310
and not just in case of

702
00:42:33,840 --> 00:42:38,050
it's a bit unusual is that the last one

703
00:42:38,240 --> 00:42:41,240
is this meaning or implication

704
00:42:41,250 --> 00:42:44,530
so a is a implies b is true

705
00:42:44,670 --> 00:42:48,990
even on a is false or a is true and b

706
00:42:49,040 --> 00:42:50,910
so if

707
00:42:50,910 --> 00:42:53,150
the left hand side of the arrow

708
00:42:53,260 --> 00:42:57,070
it was recorded and issued a statement

709
00:42:57,170 --> 00:43:02,800
is falls then the statement is true

710
00:43:02,930 --> 00:43:09,050
so this is called the material implication

711
00:43:12,040 --> 00:43:14,940
in assigning the truth if we need out

712
00:43:14,950 --> 00:43:19,810
the truth value of the property is somewhere even

713
00:43:19,870 --> 00:43:23,220
an assignment of truth values to

714
00:43:23,280 --> 00:43:25,140
properties that are

715
00:43:25,150 --> 00:43:26,810
so let's formalize this

716
00:43:27,640 --> 00:43:30,370
concept of william beveridge

717
00:43:30,650 --> 00:43:34,860
within is just mapping function like from

718
00:43:34,910 --> 00:43:38,610
propositional variables to the set of traffic

719
00:43:38,650 --> 00:43:42,670
now we just represents set the truth so year in one

720
00:43:42,670 --> 00:43:45,490
zero mean false ones

721
00:43:46,250 --> 00:43:47,910
example of wooden pallets

722
00:43:47,920 --> 00:43:50,120
this write it down

723
00:43:50,170 --> 00:43:52,170
this mapping

724
00:43:52,170 --> 00:43:56,540
for example x assigned to true white four and so on

725
00:43:56,550 --> 00:43:59,040
now we say that a model

726
00:44:00,510 --> 00:44:03,190
it's just within valleys and m

727
00:44:03,200 --> 00:44:05,550
such that and and

728
00:44:05,570 --> 00:44:07,430
so at valid true

729
00:44:07,440 --> 00:44:09,780
on the validation

730
00:44:09,820 --> 00:44:12,090
the valid and assigned

731
00:44:13,300 --> 00:44:15,520
and you have

732
00:44:15,800 --> 00:44:17,580
it about true

733
00:44:19,420 --> 00:44:22,150
this is usually

734
00:44:23,360 --> 00:44:25,490
with this notation

735
00:44:29,030 --> 00:44:32,040
we have

736
00:44:32,870 --> 00:44:35,280
because within protestantism at

737
00:44:36,760 --> 00:44:40,570
so far the truth found principally in it

738
00:44:41,510 --> 00:44:46,540
usually when we write down the value the only concerned that that appear in form

739
00:44:47,540 --> 00:44:51,140
i find

740
00:44:51,570 --> 00:44:56,250
now a formalized essentially william

741
00:44:57,030 --> 00:44:59,570
describes how you assign

742
00:44:59,640 --> 00:45:00,820
truth and

743
00:45:00,880 --> 00:45:02,540
zero one two

744
00:45:02,590 --> 00:45:04,750
form based on

745
00:45:04,790 --> 00:45:06,530
the of its power

746
00:45:09,450 --> 00:45:15,300
a very easy way to represent which is just to come

747
00:45:15,360 --> 00:45:17,030
because the truth table

748
00:45:17,170 --> 00:45:24,530
it's just this systematically all possible values that can take and calculate

749
00:45:24,540 --> 00:45:27,960
the corresponding

750
00:45:27,970 --> 00:45:32,240
the truth value of the context of these standards

751
00:45:32,240 --> 00:45:34,440
definitions of and or

752
00:45:34,460 --> 00:45:36,500
implication negation

753
00:45:37,410 --> 00:45:39,200
and suppose

754
00:45:39,240 --> 00:45:44,730
you want to know what's what's the behavior and become part the truth

755
00:45:44,780 --> 00:45:49,460
the a x and y one to come through a and y

756
00:45:49,510 --> 00:45:52,490
and it is systematically this of course classes

757
00:45:52,550 --> 00:45:54,810
sounds like a

758
00:45:54,860 --> 00:45:56,890
because these are billions is only

759
00:45:56,900 --> 00:45:59,700
two possible values each day

760
00:45:59,720 --> 00:46:02,270
so x is zero one

761
00:46:02,320 --> 00:46:03,450
so on

762
00:46:03,450 --> 00:46:05,700
is the unit ball

763
00:46:05,740 --> 00:46:08,680
in reproducing kernel expect

764
00:46:08,720 --> 00:46:10,530
so this is basically the space

765
00:46:10,550 --> 00:46:12,820
of all functions if

766
00:46:13,340 --> 00:46:25,680
and the product was between faith and for the next

767
00:46:25,870 --> 00:46:30,140
four which holds

768
00:46:30,160 --> 00:46:31,470
the norm

769
00:46:31,490 --> 00:46:33,390
of data

770
00:46:33,390 --> 00:46:37,720
it is less equal in one

771
00:46:37,800 --> 00:46:39,600
and if i have

772
00:46:39,700 --> 00:46:42,050
a so-called universal

773
00:46:42,100 --> 00:46:45,970
now consider RBF kernel is one low-cost incredible is one

774
00:46:45,990 --> 00:46:48,010
that's why the sock

775
00:46:48,050 --> 00:46:49,720
o thing

776
00:46:49,760 --> 00:46:51,870
this class of functions

777
00:46:51,950 --> 00:46:56,350
is just as powerful as the at telling me with the peak was q

778
00:46:56,350 --> 00:47:03,510
as these continuous function bounded functions are

779
00:47:04,430 --> 00:47:08,010
well this is not because we actually have hope that

780
00:47:08,050 --> 00:47:09,280
with this now

781
00:47:09,390 --> 00:47:12,990
we know a lot about how to deal with kernels we can compute this quantity

782
00:47:14,990 --> 00:47:17,510
so how we go about proving it

783
00:47:17,530 --> 00:47:19,430
proof is actually very simple

784
00:47:19,430 --> 00:47:25,030
so if people ask you well that's quite easy well and you think you zero

785
00:47:25,030 --> 00:47:26,950
from training

786
00:47:26,950 --> 00:47:29,490
if p is not equal q

787
00:47:29,620 --> 00:47:34,370
then by the previous theorem which is due essentially derived from dudley there must be

788
00:47:34,370 --> 00:47:36,160
some function for which

789
00:47:36,220 --> 00:47:39,160
the expectations different

790
00:47:40,370 --> 00:47:44,260
after all otherwise we would have a a contradiction to previous theorem

791
00:47:44,280 --> 00:47:46,850
i don't know what the function is but there must be some function for which

792
00:47:46,850 --> 00:47:47,570
this is

793
00:47:47,580 --> 00:47:49,910
it's all

794
00:47:49,950 --> 00:47:52,510
but since the hilbert space was universal

795
00:47:52,530 --> 00:47:55,410
we can find some function the hilbert space

796
00:47:55,450 --> 00:47:57,640
that is

797
00:47:57,660 --> 00:48:00,350
epsilon epsilon over two close

798
00:48:02,100 --> 00:48:04,350
this function which

799
00:48:04,390 --> 00:48:07,870
as a witness for peanut not equal to q

800
00:48:10,950 --> 00:48:13,370
and then all we have to do is we're just

801
00:48:13,370 --> 00:48:18,640
so therefore i mean that expectation for if star

802
00:48:18,660 --> 00:48:20,990
is also going to be greater than zero

803
00:48:21,050 --> 00:48:23,180
that is now from the hilbert space

804
00:48:23,200 --> 00:48:26,180
now i have to do is just shrink it back to the unit ball

805
00:48:26,200 --> 00:48:30,030
that's not gonna make a nonzero quantity zero it's just gonna make it smaller but

806
00:48:30,030 --> 00:48:32,350
not vanish

807
00:48:32,390 --> 00:48:33,510
and thereby

808
00:48:33,530 --> 00:48:37,350
it would have witness from our in polynomial space

809
00:48:37,410 --> 00:48:38,740
which also

810
00:48:39,510 --> 00:48:43,120
the difference of expectation between p and q

811
00:48:43,120 --> 00:48:45,490
it's great to this year

812
00:48:45,510 --> 00:48:49,990
that i prove it

813
00:48:49,990 --> 00:48:51,220
this is great news

814
00:48:51,260 --> 00:48:55,180
because what we've done is we've come up with a fairly abstract criteria

815
00:48:55,200 --> 00:48:57,720
but a single distribution

816
00:48:57,740 --> 00:49:01,280
we've shown that this gives us necessary and sufficient conditions

817
00:49:01,300 --> 00:49:03,100
and finally we've now seen

818
00:49:03,120 --> 00:49:04,890
it actually for

819
00:49:04,990 --> 00:49:08,550
a very nice hilbert space

820
00:49:08,550 --> 00:49:10,720
this also happens to be true

821
00:49:10,740 --> 00:49:13,260
now all need to do is

822
00:49:13,320 --> 00:49:16,890
empirical estimates for this qualities here

823
00:49:16,930 --> 00:49:19,240
based on my data set x and y

824
00:49:19,350 --> 00:49:22,680
they want to get convergence guarantees

825
00:49:28,510 --> 00:49:31,870
before we do so let's make a small

826
00:49:31,890 --> 00:49:36,260
which sort of the kolmogorov smirnov test

827
00:49:38,890 --> 00:49:40,390
sometimes you would

828
00:49:40,410 --> 00:49:44,600
musical come up smelling tasty if you have just univariate random variables you want to

829
00:49:45,370 --> 00:49:46,260
with the

830
00:49:46,280 --> 00:49:50,950
the two datasets are drawn from the same distribution

831
00:49:51,930 --> 00:49:55,780
commencement of test is quite powerful and works well and easy to use and all

832
00:49:55,780 --> 00:50:00,970
that and the most likely it's implemented in our matlab state and whatever

833
00:50:01,620 --> 00:50:05,950
well let's just see whether that doesn't actually come out as a special case of

834
00:50:05,950 --> 00:50:10,320
our definition

835
00:50:10,890 --> 00:50:14,600
so the function class

836
00:50:14,600 --> 00:50:19,180
so come made of test works in one dimension i mean the multivariate expansions but

837
00:50:20,930 --> 00:50:24,780
you'll see afterwards that with this criterion that we set up

838
00:50:24,850 --> 00:50:29,010
will get tastes which bits which we've done

839
00:50:33,070 --> 00:50:34,620
the function class

840
00:50:34,640 --> 00:50:37,320
i just real valued functions in one dimension

841
00:50:37,350 --> 00:50:42,620
and let's just take all the functions with total variation less than one

842
00:50:42,700 --> 00:50:45,600
so the total variation of a function

843
00:50:45,660 --> 00:50:47,490
this is my functional ever

844
00:50:47,490 --> 00:50:49,160
and the total variation

845
00:50:49,910 --> 00:50:51,780
of a if

846
00:50:51,800 --> 00:50:54,260
would be just integral

847
00:50:55,800 --> 00:50:57,550
the if

848
00:51:01,890 --> 00:51:03,910
absolute value of that

849
00:51:10,970 --> 00:51:14,990
and what i'm going to do is i'm going to ask the question well

850
00:51:15,010 --> 00:51:17,990
for this function class

851
00:51:18,010 --> 00:51:19,120
what is this

852
00:51:19,120 --> 00:51:22,010
distance between p and q

853
00:51:22,030 --> 00:51:26,470
so i'm going to try and solve the following optimisation problems

854
00:51:30,970 --> 00:51:34,080
overall the functions have

855
00:51:34,100 --> 00:51:35,990
with the property that the integral

856
00:51:45,680 --> 00:51:50,760
is less equal than one

857
00:51:51,910 --> 00:51:54,260
the expected value

858
00:51:54,280 --> 00:51:57,490
of f of x

859
00:51:57,530 --> 00:51:59,200
with respect to p

860
00:51:59,260 --> 00:52:03,140
minus the expected value of the q

861
00:52:03,160 --> 00:52:06,970
well if it's

862
00:52:10,720 --> 00:52:17,050
now this is a very special optimisation problem here

863
00:52:17,070 --> 00:52:21,220
because the objective function here

864
00:52:21,280 --> 00:52:23,990
is linear in n

865
00:52:24,010 --> 00:52:26,100
so this clearly convex

866
00:52:26,160 --> 00:52:28,780
the linear function is always convex

867
00:52:28,840 --> 00:52:31,370
that's the main here

868
00:52:31,390 --> 00:52:33,430
is a convex domain

869
00:52:33,430 --> 00:52:35,030
you have to believe that the

870
00:52:35,070 --> 00:52:36,780
you can easily check it

871
00:52:36,800 --> 00:52:38,390
so what we have is

872
00:52:38,410 --> 00:52:41,350
a convex maximization problem

873
00:52:42,570 --> 00:52:47,510
it doesn't sound particularly impressive that been talking about convex minimization problems

874
00:52:47,530 --> 00:52:48,740
all of yesterday

875
00:52:48,760 --> 00:52:52,390
and i tell you how wonderful convex minimization problems

876
00:52:52,720 --> 00:52:57,930
now convex maximization problems can also be very nice they can be very nice insofar

877
00:52:57,930 --> 00:53:00,800
as there a theorem due to rockefeller which states

878
00:53:02,220 --> 00:53:06,370
optimal solution of a convex maximization problem is found

879
00:53:06,410 --> 00:53:09,120
on the vertices of the state

880
00:53:09,180 --> 00:53:11,260
it's fun on the boundaries

881
00:53:11,300 --> 00:53:15,850
so what this means is that if

882
00:53:16,260 --> 00:53:18,390
my context i

883
00:53:18,430 --> 00:53:20,800
of all functions if

884
00:53:20,870 --> 00:53:25,050
rather than searching for the entire inside of that the main as well

885
00:53:25,070 --> 00:53:28,870
i can focus completely

886
00:53:28,870 --> 00:53:33,630
of c and this one again for the same reason by p

887
00:53:33,640 --> 00:53:35,770
a might one

888
00:53:37,880 --> 00:53:41,470
in the and then by my claim i know

889
00:53:41,500 --> 00:53:45,610
that this is a model

890
00:53:49,140 --> 00:53:50,880
so here

891
00:53:50,900 --> 00:53:52,180
he the

892
00:53:54,720 --> 00:53:57,960
this fact

893
00:53:59,280 --> 00:54:03,010
there is some up over here and a good number of big

894
00:54:03,020 --> 00:54:04,900
i set up here

895
00:54:04,920 --> 00:54:07,150
and again why

896
00:54:07,160 --> 00:54:11,120
the the

897
00:54:11,120 --> 00:54:12,810
svm dual

898
00:54:12,860 --> 00:54:15,630
four is an assignment of about

899
00:54:15,680 --> 00:54:21,120
which is by weak duality upper bounded by up which is the thing over there

900
00:54:26,560 --> 00:54:30,350
but that

901
00:54:31,040 --> 00:54:33,590
can i can now right

902
00:54:33,610 --> 00:54:36,310
the whole thing to more manageable way

903
00:54:36,330 --> 00:54:43,360
by including content inside the objective is to get usually for you

904
00:54:45,040 --> 00:54:46,770
all i get

905
00:54:47,560 --> 00:54:50,660
big everything together to it sound

906
00:54:50,680 --> 00:54:52,100
all the

907
00:54:52,100 --> 00:54:53,500
one of the

908
00:54:53,520 --> 00:54:56,830
that the man of one mistakes a one

909
00:54:56,880 --> 00:55:00,840
is it most now i have

910
00:55:02,000 --> 00:55:05,190
there i have one other means c one

911
00:55:05,660 --> 00:55:08,010
which is the max

912
00:55:11,100 --> 00:55:13,000
what we're seeing

913
00:55:13,020 --> 00:55:17,260
and i have e

914
00:55:17,270 --> 00:55:20,270
svm objective functions

915
00:55:20,360 --> 00:55:23,150
we can write this

916
00:55:26,970 --> 00:55:29,940
although i have

917
00:55:29,960 --> 00:55:32,390
your credit plus c

918
00:55:32,400 --> 00:55:35,040
some of the

919
00:55:39,540 --> 00:55:40,860
that's it

920
00:55:40,870 --> 00:55:42,090
but this is

921
00:55:42,100 --> 00:55:44,440
the bound

922
00:55:44,500 --> 00:55:47,910
on the number of mistakes of the one

923
00:55:47,940 --> 00:55:52,880
in terms of the number of users couldn't you and you wanted him to build

924
00:55:52,880 --> 00:55:54,770
for any value

925
00:55:54,780 --> 00:55:57,140
depends on this you know

926
00:55:57,170 --> 00:55:59,350
it is the two

927
00:55:59,350 --> 00:56:00,420
because the school

928
00:56:00,530 --> 00:56:02,720
or if the book

929
00:56:02,730 --> 00:56:04,820
i had the

930
00:56:08,850 --> 00:56:11,000
matched betting

931
00:56:11,220 --> 00:56:17,190
this is you know this is nothing better than the perception

932
00:56:26,680 --> 00:56:28,220
o to get the

933
00:56:28,230 --> 00:56:30,300
just like in the perception

934
00:56:30,320 --> 00:56:32,610
you will get the norm

935
00:56:32,620 --> 00:56:34,980
the squared norm of being

936
00:56:35,000 --> 00:56:36,290
the larger

937
00:56:36,300 --> 00:56:38,020
number of the active

938
00:56:38,040 --> 00:56:40,180
multiplying this guy

939
00:56:40,200 --> 00:56:45,360
the viking perception you can do with

940
00:56:54,910 --> 00:56:56,820
by looking at the the

941
00:56:57,160 --> 00:56:58,620
by looking at the

942
00:56:58,640 --> 00:57:01,030
the optimum of the SVM

943
00:57:01,160 --> 00:57:07,720
i can i can bound the number of states of the one

944
00:57:07,780 --> 00:57:12,320
in terms of the optimal value of the objective

945
00:57:12,340 --> 00:57:13,510
this so

946
00:57:13,560 --> 00:57:18,920
because by exploiting weak duality i know that i don't get with simon

947
00:57:21,390 --> 00:57:22,520
now this

948
00:57:22,880 --> 00:57:24,760
i have pointers

949
00:57:25,350 --> 00:57:26,660
there that is this

950
00:57:26,680 --> 00:57:28,820
this is a fake

951
00:57:28,860 --> 00:57:32,500
now you can think of any kind of up

952
00:57:32,500 --> 00:57:34,690
and that

953
00:57:34,940 --> 00:57:37,010
that the

954
00:57:37,030 --> 00:57:37,870
what is this

955
00:57:37,890 --> 00:57:40,410
do not act on the

956
00:57:40,430 --> 00:57:43,970
on a single the lagrange multiplier that he

957
00:57:44,020 --> 00:57:49,580
but the then the tried to modify all the part the lagrange multipliers from one

958
00:57:49,790 --> 00:57:51,580
up them

959
00:57:51,590 --> 00:57:52,860
but that

960
00:57:52,870 --> 00:57:55,510
is a second order kind of up

961
00:57:57,930 --> 00:57:59,390
there's that

962
00:57:59,400 --> 00:58:01,370
i mean you can look at

963
00:58:01,390 --> 00:58:03,530
telescopic sum

964
00:58:03,530 --> 00:58:05,340
and you can look

965
00:58:05,360 --> 00:58:07,320
ways of

966
00:58:07,340 --> 00:58:10,360
increase of lower bounding

967
00:58:10,370 --> 00:58:12,700
the difference in the coping

968
00:58:12,780 --> 00:58:15,640
but there are more effective than the

969
00:58:15,690 --> 00:58:19,920
if you have all these that of showing you are first order up our like

970
00:58:19,920 --> 00:58:25,950
gradient descent equivalent for classification and indeed they can be with with that

971
00:58:25,960 --> 00:58:29,450
so you have more powerful computer up that

972
00:58:29,500 --> 00:58:34,000
try to take into account the time he also

973
00:58:34,010 --> 00:58:38,090
in a sense the measurement devices were found in the past

974
00:58:38,130 --> 00:58:43,170
OK even if you have any more questions that they

975
00:58:43,190 --> 00:58:45,130
forget repeat anyway

976
00:58:45,370 --> 00:58:48,270
but you're like

977
00:58:48,290 --> 00:58:50,850
i would move to

978
00:58:51,820 --> 00:58:56,120
abroad in area organisation

979
00:58:56,150 --> 00:58:58,130
the perceptron algorithm

980
00:58:58,180 --> 00:59:00,010
which will input

981
00:59:00,010 --> 00:59:01,340
you can run them in in

982
00:59:01,900 --> 00:59:07,620
reproducing kernel hilbert space you maybe mention a little bit about it and then they

983
00:59:07,620 --> 00:59:12,590
have a strong performance guarantees as i said because they do not need

984
00:59:14,950 --> 00:59:16,550
data generation model

985
00:59:16,570 --> 00:59:24,060
to be analyzed in the on the other hand you can derive stochastic risk bounds

986
00:59:24,060 --> 00:59:29,060
using those online to batch conversions are going to be mentioned in the talk and

987
00:59:29,060 --> 00:59:34,960
the back to natural active learning variants whenever the true label comes at the cost

988
00:59:34,960 --> 00:59:38,650
you don't have it for free but you have to explicitly ask for it and

989
00:59:38,660 --> 00:59:48,230
last but not least you can actually easily term in most cases you can turn

990
00:59:48,540 --> 00:59:54,690
usually mistake bounds and so the analysis for online algorithms into an analysis that takes

991
00:59:54,690 --> 01:00:01,830
into account the fact that the data mining be well explained by a single linear

992
01:00:03,160 --> 01:00:06,610
so i i i want mentioned this but this is interesting and so this is

993
01:00:06,610 --> 01:00:12,270
the case in which free cystatin is again the stream of data is such that

994
01:00:12,270 --> 01:00:18,230
there is very very good linear feet for the for the first part then some

995
01:00:18,230 --> 01:00:20,440
other good linear feet for the

996
01:00:20,460 --> 01:00:25,560
central part and then yet another different linear feet for the part and all the

997
01:00:25,620 --> 01:00:29,080
organs can be

998
01:00:29,120 --> 01:00:33,240
modified in such a way that they are able to detect change points

999
01:00:33,260 --> 01:00:39,810
in the the best linear feet of the stream that is facing without actually knowing

1000
01:00:39,810 --> 01:00:44,670
how many change points out there and the way they are situated in the stream

1001
01:00:44,680 --> 01:00:46,500
and of course the the

1002
01:00:46,510 --> 01:00:50,240
the bound you are able to prove it in this case this will depend on

1003
01:00:50,890 --> 01:00:55,900
the degree of most narrative of the data this is an interesting and interesting but

1004
01:00:55,900 --> 01:01:00,880
but i don't want to have time to cover it

1005
01:01:00,900 --> 01:01:02,790
OK so few

1006
01:01:02,800 --> 01:01:08,320
let's start by with the perceptron the perception is the simplest online learning algorithm

1007
01:01:08,330 --> 01:01:15,580
get a comes from the fifties was introduced by rosenblatt the to study the visual

1008
01:01:16,960 --> 01:01:21,710
the visual recognition abilities of the brain but is very simple linear model so you

1009
01:01:21,710 --> 01:01:24,720
have an initial empty weight it is the zero vector

1010
01:01:24,760 --> 01:01:28,060
and then we have to get this is just like that so

1011
01:01:28,180 --> 01:01:30,060
if you remember any

1012
01:01:30,070 --> 01:01:33,820
linear online learning algorithms just characterised by the

1013
01:01:33,830 --> 01:01:37,480
but this step here everything else is fixed and this is the only place in

1014
01:01:37,480 --> 01:01:38,570
which you can

1015
01:01:38,580 --> 01:01:43,830
you can say something we can show how the add what is the action of

1016
01:01:43,840 --> 01:01:45,820
the argument on the screen

1017
01:01:45,840 --> 01:01:51,010
OK and the update rule for the perceptron is is that is a whenever no

1018
01:01:51,010 --> 01:01:53,690
mistake is made on the current instance

1019
01:01:53,790 --> 01:01:59,750
then the weights remains the weight vector remains the same otherwise we

1020
01:01:59,840 --> 01:02:02,710
had the the

1021
01:02:02,740 --> 01:02:05,060
current distance multiplied by the label

1022
01:02:05,070 --> 01:02:06,740
so that's the very simple

1023
01:02:06,750 --> 01:02:08,500
at the true

1024
01:02:08,520 --> 01:02:09,600
so now

1025
01:02:09,620 --> 01:02:12,640
a little bit of intuition because this is

1026
01:02:12,650 --> 01:02:18,310
should be beneficial for for learning and so the idea is that

1027
01:02:18,880 --> 01:02:22,140
clearly if you're going to make a mistake

1028
01:02:22,150 --> 01:02:24,100
if you use the linear classifier

1029
01:02:24,110 --> 01:02:28,100
whenever you're margin which is that quantity over there

1030
01:02:28,110 --> 01:02:29,900
is negative so

1031
01:02:29,920 --> 01:02:34,380
whenever the inner product between the current winter clintonistas doesn't have the same sign as

1032
01:02:34,380 --> 01:02:37,440
the true label of course in that case you to make a mistake

1033
01:02:37,590 --> 01:02:41,590
so if you made a mistake in the coming to example then you want to

1034
01:02:41,590 --> 01:02:45,380
link then imagine was negative you want increase it to make it positive so that

1035
01:02:45,380 --> 01:02:47,110
you want in current state

1036
01:02:47,120 --> 01:02:49,060
if the same

1037
01:02:49,080 --> 01:02:51,440
it element comes up again industry

1038
01:02:51,500 --> 01:02:54,740
so now you can look at the effect of the perceptron update on on the

1039
01:02:54,740 --> 01:02:57,640
margin and you can easily see that

1040
01:02:57,660 --> 01:03:01,990
the up if the at the margin of the updated weight

1041
01:03:02,040 --> 01:03:08,620
is measured on the same example because the mistake on the previews with vector then

1042
01:03:08,620 --> 01:03:10,850
some people ask

1043
01:03:12,940 --> 01:03:15,280
it should be discussed in greater detail

1044
01:03:15,290 --> 01:03:20,090
should be the structure of proteins all touch on very briefly this morning different kinds

1045
01:03:20,090 --> 01:03:21,930
of bonding

1046
01:03:21,990 --> 01:03:24,420
tertiary and quaternary structure

1047
01:03:24,430 --> 01:03:31,980
condensation or dehydration reactions and in fact much of those issues should be many of

1048
01:03:31,980 --> 01:03:33,490
those should be addressed

1049
01:03:33,510 --> 01:03:36,070
in the recitation sections that's the ideal

1050
01:03:36,080 --> 01:03:39,830
place to begin to clarify things which although they were mentioned here

1051
01:03:39,850 --> 01:03:43,410
may not have been mentioned in the degree of detail

1052
01:03:43,430 --> 01:03:47,360
that you really need to assimilate them properly

1053
01:03:47,370 --> 01:03:50,050
and so i urge you

1054
01:03:50,110 --> 01:03:55,810
to raise these issues with the recitation sections structures that's exactly what they for

1055
01:03:55,830 --> 01:04:00,630
having said that i just want give back briefly into protein structure even though we

1056
01:04:00,630 --> 01:04:04,420
turn our back on the end of last time just to reinforce some things that

1057
01:04:04,420 --> 01:04:06,440
i realized i might have

1058
01:04:06,450 --> 01:04:08,870
i should have mentioned perhaps in greater detail

1059
01:04:08,880 --> 01:04:12,970
here for example are different ways of depicting

1060
01:04:12,980 --> 01:04:17,530
the three-dimensional structure of the protein by the way here we see that these are

1061
01:04:17,530 --> 01:04:21,920
beta pleated sheets in the light brown and these are alpha helix is there's two

1062
01:04:21,920 --> 01:04:25,450
of them here in green one going this way the other going this way a

1063
01:04:25,450 --> 01:04:26,940
third one going this way

1064
01:04:26,980 --> 01:04:31,200
and the other blue areas are not structured i e are not structure in the

1065
01:04:31,200 --> 01:04:34,750
sense that they are in any way obviously

1066
01:04:34,750 --> 01:04:37,500
alpha helix is or beta pleated sheets

1067
01:04:37,500 --> 01:04:42,000
here's the space filling models a space filling depiction of the protein we talked about

1068
01:04:42,000 --> 01:04:46,920
that last time here is a trace of the backbone of the peptide backbone of

1069
01:04:46,920 --> 01:04:48,090
the same protein

1070
01:04:48,140 --> 01:04:52,410
where the side chains are left out and obviously we're one is only plotting the

1071
01:04:52,410 --> 01:04:58,670
three-dimensional coordinates of each of the backbone atoms CC NCC NCC and

1072
01:04:58,700 --> 01:05:04,270
here is yet another way of planting exactly the same protein in terms of indicating

1073
01:05:04,270 --> 01:05:11,820
as we just said the structure these alveolar the other regions is the secondary structure

1074
01:05:11,820 --> 01:05:18,250
of this protein and here's list of fourth way plotting of depicting the same structure

1075
01:05:18,250 --> 01:05:19,330
of the protein

1076
01:05:19,360 --> 01:05:21,590
we're roughly one is depicted in the

1077
01:05:21,600 --> 01:05:23,930
the configuration of

1078
01:05:24,520 --> 01:05:25,880
i mean i said

1079
01:05:25,930 --> 01:05:29,290
in terms of large sausage excuse me

1080
01:05:29,340 --> 01:05:32,450
if one were to use space filling models we go up here so these are

1081
01:05:32,450 --> 01:05:34,840
just four ways of looking at the same protein

1082
01:05:34,890 --> 01:05:37,820
with different degrees of simplification

1083
01:05:37,910 --> 01:05:42,370
another point that i thought i would like to reinforce and make was the following

1084
01:05:42,450 --> 01:05:46,140
we talked about transmembrane proteins in the past

1085
01:05:46,190 --> 01:05:47,800
that is proteins which

1086
01:05:47,820 --> 01:05:51,260
patrol through the membrane from one side to the other

1087
01:05:51,350 --> 01:05:54,800
and the point that i realised i'd like to make is that if we look

1088
01:05:54,800 --> 01:06:01,320
at a transmembrane proteins here's one that is starting out in the cytoplasm of the

1089
01:06:02,110 --> 01:06:07,330
and by the way the soluble part of the cytoplasm is sometimes called the cytoplasm

1090
01:06:07,350 --> 01:06:11,340
here's the little a they violated we talked about it later

1091
01:06:11,390 --> 01:06:15,670
and here is is the extracellular domain of the same protein

1092
01:06:15,760 --> 01:06:19,440
now how is always organised well the fact of the matter is

1093
01:06:19,450 --> 01:06:24,460
we discussed the fact that this hydrophobic space in the lipid bilayer is so hydrophobic

1094
01:06:24,460 --> 01:06:26,040
that really doesn't like

1095
01:06:26,090 --> 01:06:27,340
to be in the presence

1096
01:06:27,350 --> 01:06:33,230
of hydrophilic molecules including in this case amino acids and what we see here is

1097
01:06:33,230 --> 01:06:38,710
the fact that almost all of the amino acids in this region of the protein

1098
01:06:38,730 --> 01:06:42,580
which is called the transmembrane region of the protein

1099
01:06:42,600 --> 01:06:46,180
because it reaches one side to the other are all

1100
01:06:46,190 --> 01:06:52,100
hydrophobic or neutral amino acids which are reasonably comfortable in the hydrophobic space of the

1101
01:06:52,100 --> 01:06:53,600
lipid bilayer

1102
01:06:53,650 --> 01:06:58,500
there happened to be two parent violators of this glutamine histidine you see these two

1103
01:06:59,640 --> 01:07:02,310
i mean glutamic acid is today

1104
01:07:02,330 --> 01:07:04,310
OK ten i guess it his today

1105
01:07:04,320 --> 01:07:09,020
one is negatively charged and therefore is highly hydrophilic and the other is positively charged

1106
01:07:09,120 --> 01:07:13,280
and is therefore hardly have highly hydrophilic and on the surface that would seem to

1107
01:07:13,280 --> 01:07:16,820
violate the rule i just articulated

1108
01:07:16,870 --> 01:07:18,500
but the fact is

1109
01:07:18,510 --> 01:07:24,040
the as it turns out of this particular proteins these two charges are these two

1110
01:07:24,040 --> 01:07:27,900
amino acids are so closely juxtaposed with one another

1111
01:07:27,910 --> 01:07:33,450
the positive and negative charges are used to neutralize one another and as a consequence

1112
01:07:33,460 --> 01:07:37,340
in fact there is no strong charging or polarity in this area

1113
01:07:37,360 --> 01:07:38,970
or in this area

1114
01:07:38,990 --> 01:07:40,550
the take-home lesson

1115
01:07:40,560 --> 01:07:41,950
is that somehow

1116
01:07:42,000 --> 01:07:47,790
proteins managed to insert themselves into remain stable in the lipid bilayer by virtue of

1117
01:07:47,790 --> 01:07:50,650
either using only structures of hydrophobic core

1118
01:07:50,660 --> 01:07:52,750
or non polar amino acids

1119
01:07:52,760 --> 01:07:58,070
or they use tricks like this of neutralising any charges it happened to be there

1120
01:07:58,210 --> 01:07:59,670
note by the way

1121
01:07:59,680 --> 01:08:01,940
because there are hydrophilic

1122
01:08:01,960 --> 01:08:04,070
i mean OS is down here

1123
01:08:04,120 --> 01:08:09,290
and it turned out that the hydrophilic amino acid around here arginine and here there's

1124
01:08:09,290 --> 01:08:14,490
a whole bunch of basic amino acids note that this keeps the transmembrane protein from

1125
01:08:14,500 --> 01:08:19,200
getting pulled in one direction or the other because this arginine likes the likes to

1126
01:08:19,200 --> 01:08:22,410
associate with the negative phosphate on the outside

1127
01:08:22,420 --> 01:08:26,820
of the phospholipids and the same thing is here and all that means is is

1128
01:08:26,820 --> 01:08:32,080
that this transmembrane protein is firmly anchored to the lipid bilayer a point we'll talk

1129
01:08:32,080 --> 01:08:35,900
about later in greater detail we talk about membrane structure

1130
01:08:35,910 --> 01:08:39,960
one other little point oh mentioned here in passing which will also get into in

1131
01:08:39,960 --> 01:08:41,220
greater detail

1132
01:08:41,240 --> 01:08:43,180
is that once a

1133
01:08:43,210 --> 01:08:45,250
protein has been polymerize

1134
01:08:45,270 --> 01:08:49,610
that polymerization is not the last thing that happens to it once it's polymerize ten

1135
01:08:49,620 --> 01:08:51,100
fold it into place

1136
01:08:51,110 --> 01:08:57,210
because we know the proteins undergo what is called post translational modifications

1137
01:08:57,230 --> 01:08:59,230
and as we talk about

1138
01:08:59,240 --> 01:09:07,110
in the coming weeks the process of synthesizing a protein is called translation

1139
01:09:09,220 --> 01:09:13,230
when we talk about post translational modifications

1140
01:09:13,240 --> 01:09:18,170
what we're talking about is opening our eyes to the possibility even after the primary

1141
01:09:18,170 --> 01:09:24,700
amino acid sequences but polymerize their chemical alterations that can subsequently be imposed on the

1142
01:09:24,700 --> 01:09:28,360
amino acid side chains to further modify the protein

1143
01:09:28,370 --> 01:09:33,690
one such modification by example is a proteolytic degradation

1144
01:09:33,770 --> 01:09:36,370
my talk about proteolytic degradation

1145
01:09:36,390 --> 01:09:40,500
i'm talking about the fact that one can break down of proteins

1146
01:09:41,960 --> 01:09:47,180
is the breaking down of protein and when we talk about degradation

1147
01:09:47,230 --> 01:09:48,840
we're talking about

1148
01:09:48,890 --> 01:09:52,390
destroying what's been synthesized

1149
01:09:52,440 --> 01:09:56,700
in the case of many proteins once are synthesized there may be a stretch of

1150
01:09:56,700 --> 01:10:00,620
amino acids one into the other that simply clipped off

1151
01:10:00,680 --> 01:10:06,930
therefore creating the protein which is smaller than the initially synthesized product of protein synthesis

1152
01:10:06,930 --> 01:10:10,910
i e the initially synthesized product of translation

1153
01:10:10,950 --> 01:10:16,370
here we see yet another kind of post translational modification because it turns out

1154
01:10:16,420 --> 01:10:21,110
that in many proteins which protrude into the extracellular space

1155
01:10:21,110 --> 01:10:24,340
six different modes of communication face-to-face guy

1156
01:10:25,070 --> 01:10:25,450
the phone

1157
01:10:28,440 --> 01:10:29,380
instant messaging

1158
01:10:29,910 --> 01:10:33,970
text thing and then combining email with social networking sites

1159
01:10:35,260 --> 01:10:38,240
and it's very striking if you look the top left graph there

1160
01:10:39,450 --> 01:10:44,260
the difference between face-to-face and sky and these other modes of communication i was actually

1161
01:10:44,260 --> 01:10:48,130
quite surprised how well sky because i thought it is actually to clunky

1162
01:10:49,090 --> 01:10:51,580
to to match face-to-face interaction but still

1163
01:10:52,260 --> 01:10:53,730
they're clearly much much better

1164
01:10:54,560 --> 01:10:56,700
more satisfying than these other

1165
01:10:57,330 --> 01:10:58,870
a single

1166
01:11:02,330 --> 01:11:03,410
modes interaction

1167
01:11:04,140 --> 01:11:08,130
and one of the several reasons for the we think one is the fact that both sky

1168
01:11:08,900 --> 01:11:12,390
and obviously face-to-face give you a sense being in the room together

1169
01:11:12,960 --> 01:11:14,700
that's is important co-presidents

1170
01:11:15,100 --> 01:11:16,230
as it sometimes called

1171
01:11:17,110 --> 01:11:22,250
but also its immediacy what multi-channel so you are getting many different

1172
01:11:22,790 --> 01:11:29,060
messages coming through quite redundancy in information you're getting but also the responses are immediately you can see

1173
01:11:29,640 --> 01:11:30,990
the smile beginning to

1174
01:11:31,440 --> 01:11:34,220
break on somebody's face has to tell a joke

1175
01:11:35,070 --> 01:11:36,070
is that having to wait

1176
01:11:36,450 --> 01:11:39,520
until you've told they read it if it's a text-based what

1177
01:11:40,150 --> 01:11:41,690
and this may explain why

1178
01:11:43,000 --> 01:11:45,160
you know jokes seem not to work terribly well

1179
01:11:45,910 --> 01:11:46,790
on email

1180
01:11:47,820 --> 01:11:49,630
the you know things that you'd fall about

1181
01:11:50,210 --> 01:11:53,390
laughing at t in the park you can look at it as an email

1182
01:11:53,880 --> 01:11:54,460
message and go

1183
01:11:54,940 --> 01:11:56,340
what planet available today

1184
01:11:58,360 --> 01:11:59,600
the comics that okay so

1185
01:12:00,490 --> 01:12:01,670
the suggestion is that

1186
01:12:03,150 --> 01:12:05,730
so the laughed that occurs in it

1187
01:12:06,140 --> 01:12:11,990
interactions is actually very important in mediating the sensor satisfaction or happiness you

1188
01:12:13,060 --> 01:12:16,850
so the middle graph here is simply the frequency have laughter

1189
01:12:17,270 --> 01:12:19,770
recorded in interactions either as real laughter

1190
01:12:20,170 --> 01:12:22,720
in cases where you can see or hear it auras

1191
01:12:23,480 --> 01:12:26,060
virtual laughter in the digital one side

1192
01:12:27,550 --> 01:12:28,270
hello saw

1193
01:12:29,680 --> 01:12:30,520
or something like that

1194
01:12:31,410 --> 01:12:37,690
and you can see that you inevitably get much more laughter in face-to-face and scott than any of the other

1195
01:12:40,990 --> 01:12:45,640
but the important thing here on on the right hand graph is that if you then look at

1196
01:12:46,380 --> 01:12:49,040
these these different media in terms the satisfaction

1197
01:12:49,580 --> 01:12:52,230
of those interactions relative to whether or not this

1198
01:12:52,690 --> 01:12:54,930
laughter curves the black bars at the top

1199
01:12:55,520 --> 01:12:59,540
are those interactions and laughter occurred again this is this the mean happiness

1200
01:13:01,300 --> 01:13:03,450
the bottom clause open squares

1201
01:13:06,040 --> 01:13:09,670
interactions when none of because there's a massive massive difference so

1202
01:13:11,610 --> 01:13:12,630
laughter seems to

1203
01:13:15,680 --> 01:13:18,150
interactions and hence all relationships to flow

1204
01:13:19,280 --> 01:13:21,930
in a very important way come back to that later

1205
01:13:22,650 --> 01:13:23,320
so the question is

1206
01:13:23,750 --> 01:13:24,670
why can

1207
01:13:25,660 --> 01:13:26,970
isn't it the case that

1208
01:13:27,820 --> 01:13:28,960
facebook opens up

1209
01:13:29,460 --> 01:13:30,540
an entire new

1210
01:13:31,030 --> 01:13:31,790
this therefore you

1211
01:13:32,640 --> 01:13:33,900
he also goes back to

1212
01:13:35,010 --> 01:13:39,860
the so called social brain hypothesis which is an explanation for why primates have particularly

1213
01:13:39,860 --> 01:13:44,770
large brains have much bigger brains body size any other group members by very considerable

1214
01:13:44,770 --> 01:13:45,620
mark margin

1215
01:13:46,500 --> 01:13:48,140
some of the key evidence for that's his

1216
01:13:48,540 --> 01:13:49,980
data like this is putting

1217
01:13:50,510 --> 01:13:53,180
a measuring social complexity a very simple one here

1218
01:13:53,800 --> 01:13:55,070
do with many different measures

1219
01:13:55,760 --> 01:13:57,710
this is just the average group size

1220
01:13:58,380 --> 01:13:59,830
first species each these

1221
01:14:00,850 --> 01:14:02,570
dogs is one species

1222
01:14:03,130 --> 01:14:05,270
and is plotted against the measured brain size

1223
01:14:06,010 --> 01:14:08,020
the neocortex is the critical bit so

1224
01:14:08,890 --> 01:14:12,590
this is the ratio of neocortex to the rest of the brain so it's sort

1225
01:14:13,240 --> 01:14:14,050
controlling fall

1226
01:14:14,520 --> 01:14:16,320
differences in total brain size really

1227
01:14:17,110 --> 01:14:17,360
the key

1228
01:14:18,000 --> 01:14:19,250
results here are there

1229
01:14:21,670 --> 01:14:27,810
a significant relationship between group size and relative neocortex size and there are serious and

1230
01:14:27,810 --> 01:14:31,380
so what i do now

1231
01:14:32,230 --> 01:14:35,600
while these robber robbers still in place

1232
01:14:35,620 --> 01:14:37,030
i think in the part

1233
01:14:37,030 --> 01:14:39,970
so when i take them apart the negative charge is trapped

1234
01:14:40,020 --> 01:14:41,690
and this positive charge

1235
01:14:41,750 --> 01:14:43,580
is trapped and so i have thereby

1236
01:14:43,600 --> 01:14:46,050
created a negative charge on this one

1237
01:14:46,060 --> 01:14:49,820
positive on this one and is equal in magnitude so i have the dipole

1238
01:14:49,840 --> 01:14:54,830
what i want to demonstrate to you is that indeed i have

1239
01:14:54,890 --> 01:14:58,840
the positive charge here negative here that that is the difference in polarity between these

1240
01:14:58,840 --> 01:14:59,840
two and that's

1241
01:14:59,890 --> 01:15:00,640
the way

1242
01:15:01,650 --> 01:15:03,690
i'll do the experiment

1243
01:15:03,700 --> 01:15:07,680
i will not show you that the amount of charge is exactly the same

1244
01:15:07,760 --> 01:15:11,400
on each which of course it has to be

1245
01:15:11,420 --> 01:15:14,250
so let me give you some

1246
01:15:14,290 --> 01:15:16,540
but i like we have to get the

1247
01:15:16,590 --> 01:15:18,320
u graph of overhead

1248
01:15:18,320 --> 01:15:20,790
you see there for the first time an electroscope

1249
01:15:20,810 --> 01:15:24,640
we discussed it last time it is

1250
01:15:24,660 --> 01:15:27,300
feasible one and four very thin

1251
01:15:27,300 --> 01:15:29,680
with a metal rod next to it

1252
01:15:29,740 --> 01:15:32,820
and when i put charge on the road will also going to be

1253
01:15:32,870 --> 01:15:34,170
aluminum foil

1254
01:15:34,180 --> 01:15:36,790
and they repel each other and so

1255
01:15:37,470 --> 01:15:39,820
the one in ten so will go to the right

1256
01:15:39,870 --> 01:15:41,570
and the more charge there is on it

1257
01:15:41,570 --> 01:15:42,360
the father

1258
01:15:42,380 --> 01:15:44,390
it goes to the right

1259
01:15:44,410 --> 01:15:46,110
so let me first

1260
01:15:46,160 --> 01:15:47,860
what is together

1261
01:15:47,870 --> 01:15:51,480
make sure they are completely discharged

1262
01:15:51,520 --> 01:15:52,260
and now

1263
01:15:52,270 --> 01:15:53,900
i'm going to bring

1264
01:15:53,930 --> 01:15:57,920
these two into an electric field

1265
01:15:57,940 --> 01:16:01,080
which is produced by this

1266
01:16:01,080 --> 01:16:02,860
rubber rod

1267
01:16:02,940 --> 01:16:04,820
have to rob was capped for

1268
01:16:04,820 --> 01:16:07,180
and i believe it was negative

1269
01:16:07,240 --> 01:16:08,140
but if you

1270
01:16:08,160 --> 01:16:11,470
you never have to remember what is negative or positive of course that is not

1271
01:16:11,480 --> 01:16:12,640
so important

1272
01:16:12,650 --> 01:16:14,630
what's in a name after all

1273
01:16:14,640 --> 01:16:16,340
but it happens to be negative

1274
01:16:16,390 --> 01:16:19,630
OK so now we go here

1275
01:16:19,640 --> 01:16:22,990
bring it here i hope you know sparks will fly over because the ruins the

1276
01:16:25,840 --> 01:16:29,720
now notice what i do rather like the role this year

1277
01:16:29,730 --> 01:16:33,070
i separate them

1278
01:16:33,080 --> 01:16:34,980
so as i was holding it there

1279
01:16:35,030 --> 01:16:39,380
things were going on in there and you and i couldn't see the electrons

1280
01:16:39,430 --> 01:16:42,780
there are always negative electrons were shifting in this direction

1281
01:16:42,780 --> 01:16:45,270
and this is not positive and that is no negative

1282
01:16:45,320 --> 01:16:46,930
if i take this one

1283
01:16:46,950 --> 01:16:50,650
and i touch was the electroscope

1284
01:16:50,690 --> 01:16:53,990
you clearly see that there is charged on this

1285
01:16:54,020 --> 01:16:55,590
how can i show you now

1286
01:16:55,650 --> 01:16:57,130
that there is

1287
01:16:57,140 --> 01:17:00,840
charge of different polarity on the other one

1288
01:17:01,890 --> 01:17:05,530
the way i will do that is i will approach this electroscope

1289
01:17:05,530 --> 01:17:09,190
i bring this fear very close to it

1290
01:17:09,250 --> 01:17:11,680
if this charge is different

1291
01:17:11,730 --> 01:17:13,790
then the charge that is on it

1292
01:17:13,840 --> 01:17:16,570
the electroscope will

1293
01:17:16,700 --> 01:17:19,060
the reading will become smaller

1294
01:17:19,100 --> 01:17:22,330
and why is that while the reading become smaller

1295
01:17:26,190 --> 01:17:28,830
is the situation of the electroscope now

1296
01:17:28,860 --> 01:17:30,760
and here

1297
01:17:30,770 --> 01:17:32,060
is that

1298
01:17:32,070 --> 01:17:34,060
all that you see on top

1299
01:17:34,110 --> 01:17:35,370
this is

1300
01:17:35,520 --> 01:17:36,870
upside down there

1301
01:17:36,910 --> 01:17:38,270
this is all negative

1302
01:17:38,290 --> 01:17:45,350
that's why apart is now i approach this year

1303
01:17:45,370 --> 01:17:46,580
with an object

1304
01:17:46,590 --> 01:17:50,850
which is positively charged and i claim that this one now is positively charged because

1305
01:17:50,850 --> 01:17:53,040
this one was negatively charged

1306
01:17:53,920 --> 01:17:59,020
electrons are afraid of the positive charge so more will go

1307
01:17:59,320 --> 01:18:04,840
the electrons all the positive charge so electrons want to come to the positive charge

1308
01:18:04,900 --> 01:18:06,420
so d electrons

1309
01:18:06,450 --> 01:18:07,850
drift down again

1310
01:18:07,860 --> 01:18:09,530
and so if they come down

1311
01:18:09,540 --> 01:18:12,800
if you will be here and so you'll see this

1312
01:18:14,570 --> 01:18:15,840
i put here

1313
01:18:15,870 --> 01:18:17,220
negative rods

1314
01:18:17,220 --> 01:18:20,530
then the electrons which are here want to go far away

1315
01:18:20,540 --> 01:18:22,060
they will stream up

1316
01:18:22,070 --> 01:18:23,620
and therefore

1317
01:18:23,640 --> 01:18:27,040
the reading will become larger so you can only through induction

1318
01:18:28,260 --> 01:18:30,300
what the polarities of your charge

1319
01:18:30,310 --> 01:18:34,760
let's hope that this one is still holding its charge whilst talking so i claim

1320
01:18:34,760 --> 01:18:37,060
now that this polarity is different

1321
01:18:37,110 --> 01:18:41,420
and if it's still there when i approached the electroscope come very close that the

1322
01:18:41,420 --> 01:18:44,570
reading should become a little smaller without even touching it

1323
01:18:44,680 --> 01:18:46,990
see what the works

1324
01:18:47,070 --> 01:18:48,810
see goes down

1325
01:18:48,850 --> 01:18:50,600
c goes down

1326
01:18:50,670 --> 01:18:52,230
goes down

1327
01:18:52,310 --> 01:18:56,860
so for induction i have demonstrated that this has indeed the different polarity from this

1328
01:18:57,630 --> 01:18:59,500
if i proceed with this one

1329
01:18:59,540 --> 01:19:01,070
it will go further out

1330
01:19:01,080 --> 01:19:03,790
and that it already is the the maximum let's try that

1331
01:19:03,790 --> 01:19:07,610
the same goes for about

1332
01:19:07,640 --> 01:19:11,720
so not only have i demonstrated that i created the dipole

1333
01:19:11,720 --> 01:19:13,120
but you've also seen

1334
01:19:13,880 --> 01:19:15,490
by means of induction

1335
01:19:15,540 --> 01:19:17,680
that you can demonstrate

1336
01:19:18,180 --> 01:19:19,300
that the

1337
01:19:19,360 --> 01:19:21,630
there's a difference in polarity

1338
01:19:21,660 --> 01:19:26,360
between the two spheres

1339
01:19:26,530 --> 01:19:29,680
if i create a

1340
01:19:31,450 --> 01:19:33,240
and i put the dipole

1341
01:19:33,240 --> 01:19:34,530
in a

1342
01:19:34,590 --> 01:19:37,000
electric field

1343
01:19:37,050 --> 01:19:38,410
the dipole

1344
01:19:38,430 --> 01:19:42,620
i will start to rotate

1345
01:19:42,700 --> 01:19:45,550
first talk about why rotates and then

1346
01:19:45,550 --> 01:19:46,650
i will try to

1347
01:19:46,680 --> 01:19:48,340
demonstrate that

1348
01:19:48,340 --> 01:19:51,420
by making the dipole they want this big

1349
01:19:51,480 --> 01:19:55,010
right in front of you almost as big as the one there

1350
01:19:55,060 --> 01:19:57,310
so let's have

1351
01:19:57,360 --> 01:19:59,240
the electric field

1352
01:19:59,250 --> 01:20:01,470
like so

1353
01:20:01,550 --> 01:20:07,490
and i bringing this

1354
01:20:07,500 --> 01:20:08,650
electric field

1355
01:20:08,670 --> 01:20:10,360
a dipole

1356
01:20:11,600 --> 01:20:15,390
this is the one i'm going to use for this demonstration

1357
01:20:15,400 --> 01:20:18,110
ping pong balls only decide they are conducting

1358
01:20:18,120 --> 01:20:23,500
and there can connected with a road which is not conducting

1359
01:20:23,580 --> 01:20:27,240
so he was this dipole

1360
01:20:27,320 --> 01:20:29,220
this rather is not conducting

1361
01:20:29,280 --> 01:20:30,570
this is conducting

1362
01:20:30,570 --> 01:20:31,780
this is a conductor

1363
01:20:31,800 --> 01:20:33,620
let's suppose this is positive

1364
01:20:33,680 --> 01:20:35,490
this is negative for now

1365
01:20:35,530 --> 01:20:38,390
and also how we get the charge on it

1366
01:20:38,450 --> 01:20:40,410
well the positive charge

1367
01:20:40,430 --> 01:20:44,680
will experience a force in this direction always in the direction of the electric field

1368
01:20:44,720 --> 01:20:47,470
and the negative charge will experience a force

1369
01:20:47,490 --> 01:20:49,280
always upstream

1370
01:20:49,370 --> 01:20:51,450
and now to work on this

1371
01:20:51,530 --> 01:20:56,820
and the user talk on this dipole it will start to rotate clockwise

1372
01:20:56,870 --> 01:20:57,850
and of course

1373
01:20:57,890 --> 01:21:00,200
if it overshoots the field lines

1374
01:21:00,240 --> 01:21:03,280
well it is in this direction the program reverse

1375
01:21:03,300 --> 01:21:04,680
it's very easy to see

1376
01:21:04,820 --> 01:21:08,350
so what you'll see is going to oscillate and if there is enough then thing

1377
01:21:08,350 --> 01:21:13,950
it will come to hold more less in the direction of the field one

1378
01:21:14,010 --> 01:21:15,800
and this is something that i can

1379
01:21:18,320 --> 01:21:19,220
i have two

1380
01:21:20,240 --> 01:21:21,370
a dipole

1381
01:21:21,390 --> 01:21:24,800
of this kind and the way i will do that is the following

1382
01:21:24,820 --> 01:21:27,720
this is no more

1383
01:21:27,780 --> 01:21:30,450
it is into later

1384
01:21:30,510 --> 01:21:31,930
and here

1385
01:21:31,970 --> 01:21:33,010
is this

1386
01:21:33,050 --> 01:21:35,200
as these two ping pong balls

1387
01:21:35,240 --> 01:21:37,030
the one on this side

1388
01:21:37,100 --> 01:21:39,450
as yellow mark early on that site

1389
01:21:39,470 --> 01:21:41,640
as an orange marker

1390
01:21:41,660 --> 01:21:44,430
and i'm going to attach them

1391
01:21:44,450 --> 01:21:47,140
holding them up against this metal board

1392
01:21:47,140 --> 01:21:50,140
in other words

1393
01:21:50,180 --> 01:21:51,320
here is

1394
01:21:51,410 --> 01:21:55,780
this dipole it's not the dipole yet

1395
01:21:55,800 --> 01:21:57,140
mental mental

1396
01:21:57,160 --> 01:21:59,930
and here is the metal bar it is a conductor

1397
01:21:59,970 --> 01:22:01,990
which connects them

1398
01:22:01,990 --> 01:22:04,610
then you can simply search

1399
01:22:04,660 --> 01:22:07,110
and we found say thousand documents

1400
01:22:07,260 --> 01:22:08,680
and stop

1401
01:22:08,680 --> 01:22:11,240
and chances are pretty good that you'll see

1402
01:22:11,320 --> 01:22:16,930
you get the same results as the research full index is not guaranteed a heuristic

1403
01:22:17,610 --> 01:22:22,660
notice that none of the major web search engines like you see past the thousand

1404
01:22:22,700 --> 01:22:24,950
so you can never prove whether they're doing this or not

1405
01:22:25,380 --> 01:22:30,240
it's it's a clever that's one of the several reasons why

1406
01:22:30,340 --> 01:22:33,430
the only see past thousands and this is one of them is that you can

1407
01:22:33,430 --> 01:22:34,660
you can optimize it

1408
01:22:36,470 --> 01:22:40,170
for this case but if you let people see past that the keeping things consistent

1409
01:22:41,630 --> 01:22:45,400
people can find out cheating

1410
01:22:45,490 --> 01:22:47,930
and not also distributed search

1411
01:22:47,990 --> 01:22:50,950
you read that down

1412
01:22:50,950 --> 01:22:52,630
where you can

1413
01:22:52,650 --> 01:22:58,360
have a bunch of nodes on the network each which are searching different collections

1414
01:22:58,530 --> 01:23:01,280
so we partition the database by document

1415
01:23:01,380 --> 01:23:04,010
and the new broadcast query to all of these

1416
01:23:04,110 --> 01:23:06,590
and collect the top ten results

1417
01:23:06,780 --> 01:23:09,450
and then display the top scorer

1418
01:23:09,760 --> 01:23:11,300
so standard

1419
01:23:11,420 --> 01:23:13,010
distributed search

1420
01:23:19,610 --> 01:23:20,720
that has some

1421
01:23:20,760 --> 01:23:22,550
ambitious scalability goals

1422
01:23:23,030 --> 01:23:27,380
especially for an open source project where we want to scale the entire web

1423
01:23:27,450 --> 01:23:31,010
pages on millions of servers billions of pages overall

1424
01:23:31,050 --> 01:23:34,420
complete calls can take weeks to run

1425
01:23:34,430 --> 01:23:39,650
the data is very noisy there's lots of badly formed HTML out there

1426
01:23:39,740 --> 01:23:43,990
lots of servers that are going to be malicious various ways

1427
01:23:44,030 --> 01:23:48,760
it also needs on the search side support very high-traffic you'd like to be able

1428
01:23:49,280 --> 01:23:51,510
support thousands of searches per second

1429
01:23:51,530 --> 01:23:54,240
like the search quality to be

1430
01:23:54,280 --> 01:23:55,180
state of the art

1431
01:23:55,180 --> 01:23:55,970
this is

1432
01:23:55,970 --> 01:23:59,010
obviously you steep thing if you look at the

1433
01:23:59,070 --> 01:24:03,800
the number of people it takes to meet these goals at that google yahoo it's

1434
01:24:03,800 --> 01:24:06,340
hundreds if not thousands of people etc so it's it's

1435
01:24:06,340 --> 01:24:07,430
big process

1436
01:24:07,430 --> 01:24:11,200
so we will probably not going to do quite as well as they doing right

1437
01:24:11,930 --> 01:24:14,800
but it's something to aim for

1438
01:24:14,840 --> 01:24:17,820
maybe over time we'll catch up

1439
01:24:17,880 --> 01:24:24,300
so to meet these goals some some sort of simple performance numbers

1440
01:24:24,380 --> 01:24:28,240
we this this is roughly what we've we've hit

1441
01:24:28,280 --> 01:24:33,840
we can crawl around one hundred pages second on CPU which means you can get

1442
01:24:34,200 --> 01:24:35,700
about ten million pages a day

1443
01:24:35,860 --> 01:24:37,180
machine that's

1444
01:24:37,200 --> 01:24:40,180
and this by adding more machines you can you can scale

1445
01:24:40,220 --> 01:24:47,430
we can update the database of what we've seen that about a hundred pages seconds

1446
01:24:47,430 --> 01:24:48,950
well that's we can keep up

1447
01:24:49,010 --> 01:24:52,880
and so that's the rate that it takes to pass

1448
01:24:52,930 --> 01:24:56,660
the page and then after that both of those required

1449
01:24:56,680 --> 01:25:02,030
and then for searching it depends on

1450
01:25:02,090 --> 01:25:06,740
this is how much traffic you have how how you want to lay things out

1451
01:25:06,760 --> 01:25:07,900
if you

1452
01:25:07,920 --> 01:25:12,220
if you if you had two million pages per no then you might get forty

1453
01:25:12,220 --> 01:25:15,680
searches per second or better even if you're doing index sorting

1454
01:25:15,700 --> 01:25:20,200
twenty million pages per node and you might get one service per second depends on

1455
01:25:20,300 --> 01:25:21,970
how many notes you have the how much

1456
01:25:22,070 --> 01:25:24,840
traffic you want to have a collection of this

1457
01:25:24,860 --> 01:25:27,050
time space tradeoffs there

1458
01:25:27,070 --> 01:25:30,030
but the is former looks

1459
01:25:30,030 --> 01:25:32,740
or is it reasonable it's not

1460
01:25:32,780 --> 01:25:37,760
we're in the right ballpark and all these to scale to billions

1461
01:25:37,820 --> 01:25:41,700
you know we're certainly within an order of magnitude of what what i think anyone

1462
01:25:41,700 --> 01:25:42,530
out there

1463
01:25:42,530 --> 01:25:44,070
it would google in our

1464
01:25:44,070 --> 01:25:46,240
in terms of the the base performance

1465
01:25:46,240 --> 01:25:49,110
we you we may only be we maybe twice as slow or something like that

1466
01:25:49,130 --> 01:25:50,650
would not usually off

1467
01:25:50,740 --> 01:25:52,740
so let's all get

1468
01:25:53,900 --> 01:25:56,110
so that's sort of where we were initially

1469
01:25:56,510 --> 01:26:00,740
and we have a few places where we we things were

1470
01:26:00,740 --> 01:26:05,150
there are some serial bottlenecks and we get a demonstration of a hundred million web

1471
01:26:06,220 --> 01:26:08,550
but it didn't really

1472
01:26:08,590 --> 01:26:11,800
scale easily to billions of pages

1473
01:26:11,820 --> 01:26:13,280
it was

1474
01:26:13,320 --> 01:26:16,840
running more than three running four machines

1475
01:26:16,860 --> 01:26:21,360
doing crawl and then running search with a full-time job

1476
01:26:21,380 --> 01:26:24,900
and if you had you know twenty machines that we would have been very hard

1477
01:26:24,900 --> 01:26:28,970
to really take advantage of them we we didn't have the tools to manage things

1478
01:26:28,990 --> 01:26:34,240
the space allocation got to be really really tricky dicky in copying files around looking

1479
01:26:34,240 --> 01:26:36,760
we're just full

1480
01:26:37,150 --> 01:26:42,270
so we were effectively limited for single user two hundred million pages what's in didn't

1481
01:26:42,270 --> 01:26:43,200
seem very good

1482
01:26:45,720 --> 01:26:47,650
started a new project

1483
01:26:47,700 --> 01:26:49,200
to solve this problem

1484
01:26:49,240 --> 01:26:51,930
if we want to get past a hundred million pages how do how do we

1485
01:26:51,930 --> 01:26:53,930
do it

1486
01:26:53,950 --> 01:26:59,530
this project is called hadoop started from the beginning of this year it was spun

1487
01:26:59,530 --> 01:27:02,610
out of some work that have been started within within such

1488
01:27:02,650 --> 01:27:05,450
and it's a platform for distributed computing

1489
01:27:05,590 --> 01:27:09,530
and the sort of distributed computing that we need for

1490
01:27:09,550 --> 01:27:11,360
for information retrieval tasks

1491
01:27:11,430 --> 01:27:14,740
and it's become the foundation for all of the

1492
01:27:15,450 --> 01:27:16,800
core operations

1493
01:27:16,800 --> 01:27:18,570
so how did has

1494
01:27:18,650 --> 01:27:22,740
two major subsystems the distributed filesystem GFS

1495
01:27:22,740 --> 01:27:25,320
which is modelled after google's GFS

1496
01:27:25,340 --> 01:27:29,530
and the quick overview the architecture of a single name no

1497
01:27:30,130 --> 01:27:36,800
that's on the on the network and it maintains the mapping file names to the

1498
01:27:36,800 --> 01:27:39,240
list of blocks that compose file

1499
01:27:39,800 --> 01:27:44,900
and then it also keeps the mapping from the block idea

1500
01:27:44,950 --> 01:27:50,150
two hosts that that can that have shown that block

1501
01:27:50,200 --> 01:27:56,930
support pair so it's very not very simple just these two tables and then

1502
01:27:56,930 --> 01:28:00,870
is equal to one over are square are is the distance between these two charge

1503
01:28:00,900 --> 01:28:07,710
groups obviously the further apart you get the weaker the attraction with one another

1504
01:28:07,730 --> 01:28:10,910
there are also what are called van der waals interactions

1505
01:28:10,920 --> 01:28:13,650
these are largely of interest

1506
01:28:14,500 --> 01:28:16,850
a very small community

1507
01:28:16,850 --> 01:28:19,450
of biochemists

1508
01:28:19,470 --> 01:28:23,150
you will probably never you may never hear this term beginning your life and then

1509
01:28:23,150 --> 01:28:28,460
towards interactions comes from the fact that if we were to have

1510
01:28:28,510 --> 01:28:34,560
for example two molecules over here which are not normally charge anyway let's just talk

1511
01:28:34,560 --> 01:28:35,460
about two

1512
01:28:35,530 --> 01:28:39,160
aliphatic chains again

1513
01:28:39,160 --> 01:28:43,020
and i will put in all the protons and everything but just imagine a situation

1514
01:28:43,020 --> 01:28:44,790
like this

1515
01:28:44,850 --> 01:28:47,910
what will happen is that because the fluctuations

1516
01:28:47,920 --> 01:28:51,690
of electrons because the electrons are swimming around here all the time moving from one

1517
01:28:51,690 --> 01:28:53,040
area to the next

1518
01:28:53,100 --> 01:28:54,170
there never

1519
01:28:54,180 --> 01:28:59,490
equally distributed homogeneously over a long period of time there will be brief instants in

1520
01:28:59,490 --> 01:29:03,710
time micro seconds or even nano seconds when there's

1521
01:29:03,760 --> 01:29:07,300
this happens to be more electrons over here the right here

1522
01:29:07,320 --> 01:29:09,050
just by chance

1523
01:29:09,070 --> 01:29:16,960
and this area of on air unequal distribution of electrons will in turn induce

1524
01:29:16,970 --> 01:29:21,250
the opposite kind of electron shift in neighbouring molecule down here

1525
01:29:21,270 --> 01:29:26,610
obviously depending on the distance between them but it the negative here will repel electrons

1526
01:29:26,610 --> 01:29:32,040
down here the positive here will attract electrons down here so you have these two

1527
01:29:32,050 --> 01:29:34,200
clauses i polar

1528
01:29:34,210 --> 01:29:37,190
arrangements here and here

1529
01:29:37,240 --> 01:29:38,570
very ephemeral

1530
01:29:38,600 --> 01:29:43,980
that is lasting for very short transition period of time but nonetheless sifts sufficient to

1531
01:29:43,980 --> 01:29:47,390
give a very weak interaction between these two molecules

1532
01:29:47,480 --> 01:29:53,100
which may persist only for microsecond then be dissipated because the charges then redistributed once

1533
01:29:54,080 --> 01:29:59,200
and as a consequence of that one has very weak interactions which in the great

1534
01:29:59,200 --> 01:30:02,370
scheme of things play only a very minor role

1535
01:30:03,620 --> 01:30:09,940
the overall energy which holds molecules together

1536
01:30:09,960 --> 01:30:14,150
now with that background my let's begin to elaborate on on how we can make

1537
01:30:16,070 --> 01:30:22,370
i have interesting properties that enable them among other things to participate in the construction

1538
01:30:23,900 --> 01:30:25,930
of lipid bilayers

1539
01:30:26,020 --> 01:30:31,330
which will be the first object of our attention still in terms of actual

1540
01:30:31,370 --> 01:30:33,700
biochemistry so here's a

1541
01:30:33,750 --> 01:30:35,740
here's the fatty acid

1542
01:30:35,780 --> 01:30:39,330
we see that up here i don't affect through the structure of the fatty acid

1543
01:30:39,330 --> 01:30:41,960
up here already once before

1544
01:30:41,960 --> 01:30:45,080
and what we can see is through a

1545
01:30:45,140 --> 01:30:48,330
a linkage notice esterification

1546
01:30:48,350 --> 01:30:50,230
we can create this molecule

1547
01:30:50,290 --> 01:30:53,170
so what do i mean by esterification

1548
01:30:53,180 --> 01:30:55,020
well in this case

1549
01:30:55,070 --> 01:30:58,550
we're talking about a situation here

1550
01:30:58,690 --> 01:31:01,180
where we have the carbon atom over here

1551
01:31:01,200 --> 01:31:06,370
like this with a hydroxyl group you see it over here and what we're doing

1552
01:31:06,370 --> 01:31:11,650
is we're dehydrate in this were pulling out one molecule of water and each time

1553
01:31:11,650 --> 01:31:14,330
we do that on three separate occasions

1554
01:31:14,350 --> 01:31:16,740
what we end up doing is to create

1555
01:31:16,760 --> 01:31:18,790
instead of this

1556
01:31:18,860 --> 01:31:22,970
is to create a a covalent bond

1557
01:31:22,980 --> 01:31:24,850
between these two and so the

1558
01:31:24,850 --> 01:31:27,510
the product of of the hydrating this

1559
01:31:27,530 --> 01:31:30,740
pulling out one net molecule of water

1560
01:31:30,830 --> 01:31:35,070
is that we end up with a a structure that looks like this

1561
01:31:35,110 --> 01:31:42,210
and you see that happening

1562
01:31:42,210 --> 01:31:46,740
on atleast three different occasions

1563
01:31:46,760 --> 01:31:47,970
here here and here

1564
01:31:47,980 --> 01:31:50,560
well actually i put carbon over here

1565
01:31:50,600 --> 01:31:51,740
so here we have

1566
01:31:51,760 --> 01:31:54,320
three esterification

1567
01:31:54,570 --> 01:31:59,980
a hydroxyl group in each case is reacting with the carboxyl group here

1568
01:32:00,030 --> 01:32:02,710
pulling out one water

1569
01:32:02,710 --> 01:32:07,460
and in each case creating a what's called try

1570
01:32:07,470 --> 01:32:11,000
try a sort glycerol or triglyceride

1571
01:32:11,010 --> 01:32:14,820
triglyceride refers to the fact that we start with the glycerol and and we have

1572
01:32:14,820 --> 01:32:16,360
now is terrified

1573
01:32:17,200 --> 01:32:20,880
in fact there are two there are two directions here in this kind of reaction

1574
01:32:20,880 --> 01:32:26,940
esterification is the kind of linkage we just showed here

1575
01:32:26,950 --> 01:32:32,690
and the truth is that vast numbers of biochemical linkages are made by esterification reactions

1576
01:32:32,700 --> 01:32:34,740
and reversed by

1577
01:32:34,790 --> 01:32:38,150
reactions that are called simply hydrolysis

1578
01:32:38,160 --> 01:32:41,570
and in this case we are referring to was the fact that if one were

1579
01:32:41,570 --> 01:32:46,860
to reintroduce a water molecule in each of these three linkages one two and three

1580
01:32:46,890 --> 01:32:50,930
we would break the bond and caused this entire structure to revert

1581
01:32:51,010 --> 01:32:54,000
the two precursors that existed or pre-existing

1582
01:32:54,010 --> 01:32:57,460
prior to these three esterification reactions

1583
01:32:57,500 --> 01:33:02,710
and time and again you'll see over the next weeks the esterification reactions are important

1584
01:33:02,710 --> 01:33:06,430
for constructing different kinds of

1585
01:33:06,470 --> 01:33:08,760
of molecules

1586
01:33:08,790 --> 01:33:14,320
now the fact of the matter is we can do other kinds of modifications of

1587
01:33:15,470 --> 01:33:18,100
a way

1588
01:33:18,100 --> 01:33:23,960
the glycerol like this here what we've done instead of adding a third fatty acid

1589
01:33:24,000 --> 01:33:27,210
no what was done here here through an esterification

1590
01:33:27,220 --> 01:33:29,170
let's look at this one here

1591
01:33:29,210 --> 01:33:35,630
instead of adding a third fatty acid we're saying we reserve one of the three

1592
01:33:35,630 --> 01:33:37,380
groups of glycerol

1593
01:33:37,380 --> 01:33:41,410
here's what we shot just before we've saved one of the three groups the glycerol

1594
01:33:41,420 --> 01:33:43,650
and put on instead

1595
01:33:43,700 --> 01:33:46,560
this highly hydrophilic phosphate groups

1596
01:33:46,620 --> 01:33:51,060
once again through a dehydration reaction an esterification reaction

1597
01:33:51,080 --> 01:33:54,940
and now what we've done is an insult to injury because in the absence of

1598
01:33:54,940 --> 01:33:57,870
this who had a hydroxyl here which is

1599
01:33:57,910 --> 01:33:59,690
mildly hydrophilic

1600
01:33:59,710 --> 01:34:00,610
but now

1601
01:34:00,620 --> 01:34:05,500
look how strongly charge this is here's two negative charges one electronic and this is

1602
01:34:05,500 --> 01:34:11,560
already the electronegative so here we have an extremely potent hydrophilic entity

1603
01:34:11,620 --> 01:34:15,780
and here the degree of schizophrenia between one and the molecule and the other is

1604
01:34:15,780 --> 01:34:17,620
greatly exaggerated

1605
01:34:17,630 --> 01:34:21,150
here in fact this is extremely high hydrophilic

1606
01:34:21,160 --> 01:34:26,110
and as a consequence of that is really likes to stick its head inside water

1607
01:34:26,280 --> 01:34:30,970
and when we therefore talk about we draw the the images of different kinds of

1608
01:34:32,210 --> 01:34:35,230
like this i showed you before the two tails

1609
01:34:35,330 --> 01:34:39,610
here saw that the tales i grew before in the diagram

1610
01:34:39,610 --> 01:34:42,260
here so what we can imagine they actually look like

1611
01:34:42,290 --> 01:34:45,890
in more real molecular terms

1612
01:34:45,900 --> 01:34:49,450
and the hydrophilic heads sticking in the water

1613
01:34:49,460 --> 01:34:51,780
this is just repeating what we saw before

1614
01:34:51,900 --> 01:34:53,980
become even more hydrophilic

1615
01:34:54,000 --> 01:34:57,050
if we look at a molecule like this let's look at this thing here here's

1616
01:34:57,050 --> 01:34:59,670
a very long hydrophobic tails

1617
01:34:59,720 --> 01:35:03,200
here's the two glycerol once again here's the phosphate

1618
01:35:03,300 --> 01:35:07,750
and keep in mind the phosphate obviously has these extra oxygen is fast we can

1619
01:35:07,750 --> 01:35:12,780
react with more than just one partner the glycerol down here in this case we

1620
01:35:12,790 --> 01:35:15,150
had this group appear

1621
01:35:15,150 --> 01:35:22,960
so I come from the Empirical Inference Department which I think is a nice term

1622
01:35:23,120 --> 01:35:28,530
for what we are doing in machine learning it refers to the process of drawing

1623
01:35:28,530 --> 01:35:37,320
conclusions from empirical data from observational data and one example of such conclusions is something

1624
01:35:37,340 --> 01:35:42,660
we we do as scientists we we do the scientific inference so we measures some data

1625
01:35:42,660 --> 01:35:47,540
points let's say we have two observables X and Y

1626
01:35:47,850 --> 01:35:52,310
we have two observables we measure pairs of these observables

1627
01:35:52,430 --> 01:35:57,180
let's say we find this kind of relationships than we might be tempted to say

1628
01:35:57,180 --> 01:36:02,320
it looks like there's a linear law connecting these two observables

1629
01:36:02,950 --> 01:36:10,810
but lately it's already pointed out the philosopher of mathematician latence that even

1630
01:36:10,820 --> 01:36:13,890
if you

1631
01:36:13,930 --> 01:36:17,390
I mean in this case you can maybe come up with a linear law but even if

1632
01:36:17,390 --> 01:36:22,480
you randomly produce points on a sheet of paper for instance by taking an ink

1633
01:36:22,820 --> 01:36:27,980
acquittal pen and just shaking it so you get some randoms point you will also

1634
01:36:27,980 --> 01:36:32,150
find some mathematical expression that somehow

1635
01:36:32,180 --> 01:36:34,450
more or less explains the data points

1636
01:36:34,760 --> 01:36:38,620
so if you have these random points here you might come up with more

1637
01:36:38,620 --> 01:36:44,440
complicated solution it might even be a curdle expansion that we will talk about more in

1638
01:36:44,440 --> 01:36:50,060
this lecture and then the question comes up why should we trust

1639
01:36:50,060 --> 01:36:56,150
the one solution more than the the other is there anything that allows us to justify why we

1640
01:36:56,150 --> 01:36:58,400
trust something

1641
01:36:58,530 --> 01:37:04,530
and are there any infor any general principles how to arrive at such at such solutions

1642
01:37:05,210 --> 01:37:10,130
now I mentioned this is a from the scientific inference and of course scientists do this all the time

1643
01:37:10,130 --> 01:37:14,950
and they have their own ways of handling it and this is a quote from the

1644
01:37:14,950 --> 01:37:22,290
physicist Rutherford who once said if your experiment needs statistics you ought to have done a better experiment

1645
01:37:22,890 --> 01:37:26,830
so this is the the enemy of machine learning and first machine learning didn't exist at

1646
01:37:26,830 --> 01:37:27,060
that time

1647
01:37:27,950 --> 01:37:34,430
so he's basically saying that data point should be such that the inference what is

1648
01:37:34,430 --> 01:37:34,750
the law

1649
01:37:35,160 --> 01:37:39,220
from the data point should be trivial so the real problem is producing the right

1650
01:37:39,220 --> 01:37:45,250
data not producing inference from data and of course we are specializing in inference

1651
01:37:45,250 --> 01:37:52,560
from data so we want to convince people that nowadays this is different and in

1652
01:37:52,560 --> 01:37:56,470
the sence even from our point of view the interesting part is hard to do

1653
01:37:56,470 --> 01:38:03,810
the inference from data now here is a second example of empirical inference this is from the field

1654
01:38:04,000 --> 01:38:07,680
of perceptions but this is also something that we do all the time

1655
01:38:07,720 --> 01:38:10,400
as animals

1656
01:38:10,440 --> 01:38:16,340
and here are some visual patterns that we have no problem recognizing this is one

1657
01:38:16,340 --> 01:38:21,650
of the favorite data sets of machinery people the US Postal Service

1658
01:38:21,720 --> 01:38:25,650
set of hundred digits and

1659
01:38:25,780 --> 01:38:30,250
so if you see these digits you have no problem assigning them to the correct class

1660
01:38:30,340 --> 01:38:35,620
however just to show you that this is actually not such a trivial inference problem

1661
01:38:35,620 --> 01:38:38,830
if I apply a permutation to the digits

1662
01:38:38,870 --> 01:38:43,210
and this is the same fixed permutation I'm just changing the pixels

1663
01:38:43,250 --> 01:38:46,590
apply the same fixed permutation everywhere

1664
01:38:46,620 --> 01:38:50,400
I get these kind of patterns that have the same information

1665
01:38:50,440 --> 01:38:55,060
but now it's difficult to assign them to the classes maybe you can still see that the

1666
01:38:55,080 --> 01:38:58,030
zeros look a bit different from the threes

1667
01:38:58,400 --> 01:39:03,900
but it's it's no longer trivial to recognize these patterns so this means that

1668
01:39:03,910 --> 01:39:10,160
well or one of the explanations for this and maybe the main explanation is that this

1669
01:39:10,160 --> 01:39:14,220
appears trivial to us because we've been trained on this kind of patterns for a

1670
01:39:14,220 --> 01:39:15,970
long time

1671
01:39:16,070 --> 01:39:22,500
so we've been trained on the contiguous smooth patterns in general and also specifically on

1672
01:39:22,590 --> 01:39:29,050
on hand written digits we've been trained a lot so for us it's easy to recognize these but it's not so trivial to recognize

1673
01:39:29,050 --> 01:39:36,280
these even if we were given a sizable training set and for a machinery algorithm

1674
01:39:36,280 --> 01:39:41,450
unless you do something very clever and try to add prior knowledge these problems are

1675
01:39:41,450 --> 01:39:46,450
in principle the same because these images are normally represented as vectors

1676
01:39:46,630 --> 01:39:51,860
so far machinery algorithm this is as hard as this one and we can tell

1677
01:39:51,860 --> 01:39:54,390
this one isn't isn't actually that easy

1678
01:39:54,400 --> 01:39:56,150
to begin with

1679
01:39:56,270 --> 01:40:03,920
and there is famous neuroscientist actually once said that brain is nothing but a

1680
01:40:03,920 --> 01:40:06,470
statistical decision organ so

1681
01:40:06,780 --> 01:40:11,950
we have different organs the heart is a pump and the brain is our statistics engine

1682
01:40:11,950 --> 01:40:14,980
so from that point of view I think we have a neuroscientist here I think

1683
01:40:15,010 --> 01:40:18,720
very good to have neuroscientist here because if we want to understand what the

1684
01:40:18,720 --> 01:40:22,330
brain is doing and if it's true that the brain is a statistical decision organ

1685
01:40:22,330 --> 01:40:26,360
then we need to understand statistics and the inference

1686
01:40:31,360 --> 01:40:39,130
so this so this second example was an example of a relatively hard inference problem

1687
01:40:39,130 --> 01:40:45,480
and I'll say a little bit more about what I mean by this before I show you another hard inference

1688
01:40:45,480 --> 01:40:55,970
problem which is from more standard machinery problem nowadays from bioinformatics so in this case

1689
01:40:55,970 --> 01:40:57,980
the task is to

1690
01:40:58,360 --> 01:41:01,860
is a classification task on human DNA sequences

1691
01:41:01,900 --> 01:41:08,240
so you can use portions of the DNA sequence to predict or classify certain things

1692
01:41:08,240 --> 01:41:13,530
in this case it's a three-way classification problem maybe you don't have to worry about the

1693
01:41:13,530 --> 01:41:19,320
biological details but it's a classification problem from which we can get label data

1694
01:41:19,400 --> 01:41:25,860
so we can perform biological experiments to check which of the three classes our piece of

1695
01:41:25,860 --> 01:41:30,690
DNA sequence belongs to and we can produce a large

1696
01:41:30,840 --> 01:41:33,040
training set and then

1697
01:41:33,150 --> 01:41:37,220
train some machine and system on it and it doesn't matter so full of I want

1698
01:41:37,220 --> 01:41:42,650
to make what system it is and also doesn't matter exactly what error measure we use

1699
01:41:42,650 --> 01:41:47,220
or what correctness measure we use the only point I want to make here is

1700
01:41:47,390 --> 01:41:52,770
this is a program where we have up to fifteen million training points over here

1701
01:41:52,770 --> 01:41:58,600
and if we use few training points where few means let's say a thousand or less

1702
01:41:58,600 --> 01:42:05,270
then our performance is more or less zero so we are more or less at chance level that this is certain kind of error

1703
01:42:05,270 --> 01:42:10,740
measure and if we use a lot of training points and we can

1704
01:42:10,740 --> 01:42:12,100
be very accurate

1705
01:42:12,570 --> 01:42:18,040
so this is a kind of regularity out there in the world which we wouldn't see from

1706
01:42:18,040 --> 01:42:22,770
small data sets or which we wouldn't see as humans because there's no way for

1707
01:42:22,770 --> 01:42:27,040
us to look at 15 million sequences of DNA locations so if we just look

1708
01:42:27,040 --> 01:42:33,040
at the data as biologists we wouldn't see the regularity

1709
01:42:33,470 --> 01:42:38,210
so this is an example of of such a hard inference problem that we as

1710
01:42:38,210 --> 01:42:42,330
humans cannot solve because we didn't grow up with this kind of data so we grew

1711
01:42:42,330 --> 01:42:46,800
up with visual patterns what we didn't grow up with DNA sequences

1712
01:42:46,800 --> 01:42:47,320
and i'm gonna say

1713
01:42:47,850 --> 01:42:49,120
the following say it

1714
01:42:49,720 --> 01:42:51,720
the probability that this is zero half

1715
01:42:54,570 --> 01:42:56,370
obviously the probability that this is one

1716
01:42:57,140 --> 01:42:57,620
is a half

1717
01:43:00,260 --> 01:43:00,800
we can actually

1718
01:43:01,240 --> 01:43:04,050
right down the inference the probability that this is one

1719
01:43:04,740 --> 01:43:06,430
given are zero one one

1720
01:43:08,280 --> 01:43:09,600
one minus the squared

1721
01:43:11,320 --> 01:43:11,970
by half

1722
01:43:12,700 --> 01:43:13,320
divided by

1723
01:43:14,280 --> 01:43:14,990
the same term

1724
01:43:15,780 --> 01:43:18,070
down squares downstairs like this

1725
01:43:20,930 --> 01:43:21,530
you know the term

1726
01:43:24,300 --> 01:43:25,320
you know the explanation

1727
01:43:31,820 --> 01:43:33,120
and i can simplify this lot

1728
01:43:38,410 --> 01:43:39,490
when you cancel everything out

1729
01:43:40,030 --> 01:43:40,340
you get

1730
01:43:41,390 --> 01:43:41,990
one minus

1731
01:43:43,030 --> 01:43:47,010
so we've now answered in the table we just had are going to have

1732
01:43:48,260 --> 01:43:49,890
we've looked at one rather that's

1733
01:43:50,370 --> 01:43:51,720
the road a zero one

1734
01:43:52,470 --> 01:43:55,140
and now what we've worked out is the probability

1735
01:43:55,850 --> 01:43:56,530
that's is

1736
01:43:58,990 --> 01:44:00,030
are is one

1737
01:44:01,010 --> 01:44:04,200
it is in fact one minus which is ninety percent

1738
01:44:04,760 --> 01:44:07,070
so it was a sensible thing to say you should guess

1739
01:44:08,850 --> 01:44:12,410
right and i can repeat for all eight possible answers

1740
01:44:13,550 --> 01:44:16,950
almost all of them will come out just the same way this one did but

1741
01:44:17,030 --> 01:44:20,550
if you receive zero zero zero one one get a slightly different

1742
01:44:21,030 --> 01:44:22,660
answer for the probability

1743
01:44:24,180 --> 01:44:26,010
what the code says you should guess

1744
01:44:31,780 --> 01:44:32,700
just affinities of

1745
01:44:33,120 --> 01:44:35,800
labouring it's because it's good to be clear on the simple case

1746
01:44:36,430 --> 01:44:37,180
and we can go and

1747
01:44:38,760 --> 01:44:39,820
two more complicated ones

1748
01:44:42,930 --> 01:44:44,160
the probability that it was a one

1749
01:44:44,800 --> 01:44:46,010
is bigger than the probability that

1750
01:44:46,470 --> 01:44:47,030
it was a zero

1751
01:44:49,220 --> 01:44:50,180
this is one

1752
01:44:50,950 --> 01:44:51,780
is the best this

1753
01:44:55,410 --> 01:44:58,620
and it's the best guess because it involves the smallest number of flips

1754
01:45:02,990 --> 01:45:04,600
so i'll leave as an exercise

1755
01:45:04,990 --> 01:45:07,200
during the case are is one one

1756
01:45:07,620 --> 01:45:09,600
so you can work out the posterior probability

1757
01:45:10,700 --> 01:45:11,280
in the case

1758
01:45:13,970 --> 01:45:15,350
now we have a new task

1759
01:45:16,180 --> 01:45:18,950
this was using probabilities to do inference

1760
01:45:19,870 --> 01:45:23,600
now what i want to work out is how well does this decoder perform

1761
01:45:33,120 --> 01:45:35,580
so this is an example of forward probability

1762
01:45:42,390 --> 01:45:45,240
i want to ask the question if we use a repetition code

1763
01:45:49,950 --> 01:45:50,600
think codex

1764
01:45:51,350 --> 01:45:55,340
with senti over a binary symmetric channel with the flip probability at

1765
01:45:55,950 --> 01:45:58,010
and then we use the majority vote decoder

1766
01:45:58,760 --> 01:45:59,200
to give us

1767
01:46:01,780 --> 01:46:03,280
what is the probability

1768
01:46:04,300 --> 01:46:05,140
the estat

1769
01:46:06,120 --> 01:46:07,100
well not equal

1770
01:46:08,140 --> 01:46:13,510
less well i'm talking here about a single bit so we send one bit over the channel using this procedure

1771
01:46:14,010 --> 01:46:18,530
what's the probability that the asshat we end up spilling out about decoder will not

1772
01:46:18,530 --> 01:46:21,740
be the same as the bit that were in please have a chapter you neighbour

1773
01:46:38,910 --> 01:46:39,620
any answers

1774
01:46:40,700 --> 01:46:41,910
the flip probability is after

1775
01:46:43,660 --> 01:46:44,410
the probability

1776
01:46:45,470 --> 01:46:46,910
an error in the system

1777
01:46:52,760 --> 01:46:55,530
what probability distribution are you thinking about anyone

1778
01:46:58,640 --> 01:47:00,450
the binomial distribution and that's

1779
01:47:01,550 --> 01:47:03,930
so is the binomial distribution and what

1780
01:47:04,870 --> 01:47:08,910
part what outcomes would cause this thing that happened which

1781
01:47:08,910 --> 01:47:13,840
over all variables can be written in the factor form so the joint distribution over

1782
01:47:13,840 --> 01:47:18,680
the variables x is whatever some normalizing constant the product of

1783
01:47:18,910 --> 01:47:20,420
these factors

1784
01:47:22,090 --> 01:47:25,340
where the notation i'm using is that

1785
01:47:25,380 --> 01:47:26,770
x so

1786
01:47:28,670 --> 01:47:33,010
it's just cj is the subset of all the nodes in the graph so there

1787
01:47:33,010 --> 01:47:35,730
are k nodes in the graph k variables

1788
01:47:35,740 --> 01:47:41,610
cj is the j th subset of that corresponds to the j factor here except

1789
01:47:41,610 --> 01:47:48,260
cj is just take the subset of the vector indexed by siege

1790
01:47:54,450 --> 01:47:57,710
if we have a factorisation like this

1791
01:47:57,720 --> 01:48:01,710
then we can specify an undirected graph in the following way

1792
01:48:01,750 --> 01:48:04,600
create a node for each variable

1793
01:48:04,660 --> 01:48:07,540
and then connecting two nodes i and k

1794
01:48:07,560 --> 01:48:12,930
if there is some set cj such that both i is in that sense cj

1795
01:48:12,930 --> 01:48:18,150
and case that's cj in other words if there's some factor

1796
01:48:18,150 --> 01:48:21,250
we both

1797
01:48:21,300 --> 01:48:22,040
x i

1798
01:48:22,060 --> 01:48:23,280
and xk

1799
01:48:24,810 --> 01:48:28,180
both part of that fact

1800
01:48:28,190 --> 01:48:34,270
so these sets form the cliques of the graph is a fully connected subgraphs

1801
01:48:34,290 --> 01:48:36,300
so let's look at one of these

1802
01:48:37,270 --> 01:48:41,930
here we have it is an undirected graphical model the difference between this and factor

1803
01:48:42,790 --> 01:48:48,180
is that we're not explicitly representing those factors which is directly connecting the nodes to

1804
01:48:48,180 --> 01:48:50,720
each other with these undirected edges

1805
01:48:50,720 --> 01:48:52,990
OK so this

1806
01:48:53,000 --> 01:48:56,830
undirected graphical model corresponds to this

1807
01:48:56,850 --> 01:49:01,430
factorisation of the joint distribution of these five variables

1808
01:49:01,430 --> 01:49:05,960
again we have the fact that x is independent of y

1809
01:49:06,010 --> 01:49:07,340
given the

1810
01:49:07,360 --> 01:49:13,720
if every path between x and y contains some node in this set b

1811
01:49:14,790 --> 01:49:18,850
and again we have a car larry given the neighbors of x

1812
01:49:18,900 --> 01:49:22,300
the variable x is conditionally independent of all other variables

1813
01:49:22,320 --> 01:49:23,880
just like in factor graphs

1814
01:49:23,960 --> 01:49:28,280
and we can define a you know a a couple of other things the markov

1815
01:49:31,720 --> 01:49:35,840
the markov blanket is the set of variables for

1816
01:49:36,090 --> 01:49:42,000
these are these are markov blanket for x if only if x is independent of

1817
01:49:42,000 --> 01:49:44,940
y given v the for all

1818
01:49:44,940 --> 01:49:50,340
why not in the union of the sets x and envy

1819
01:49:50,400 --> 01:49:57,710
markov boundary is the minimal markov blanket that's what makes you independent of everything else

1820
01:49:57,710 --> 01:50:00,000
and that's generally does the neighbors of

1821
01:50:00,010 --> 01:50:01,190
your note

1822
01:50:01,910 --> 01:50:06,740
and you can define these more generally for a set of nodes if you want

1823
01:50:06,750 --> 01:50:07,500
all right

1824
01:50:07,520 --> 01:50:13,680
so i've introduced undirected graphs and factor graphs and they seem remarkably similar so what's

1825
01:50:13,680 --> 01:50:15,220
the point with the reason why

1826
01:50:15,870 --> 01:50:19,050
we have these two things one of them is historical as undirected graphs have been

1827
01:50:19,050 --> 01:50:24,400
around much longer than factor graphs that's why i think it's important for me to

1828
01:50:24,400 --> 01:50:26,650
introduce them here

1829
01:50:26,800 --> 01:50:31,990
but they don't represent exactly the same form

1830
01:50:32,000 --> 01:50:36,220
properties of this richer forms of distribution

1831
01:50:36,230 --> 01:50:43,430
let's consider these three graph a b and c are the nodes

1832
01:50:43,440 --> 01:50:46,710
in these graphs have exactly the same neighbors

1833
01:50:46,720 --> 01:50:52,250
right the neighbours of the ERC in the in all three of these graphs

1834
01:50:53,210 --> 01:50:59,420
therefore these three graphs represent exactly the same conditional independence relationships

1835
01:50:59,430 --> 01:51:03,990
so as far as conditional independence relationships go there not different

1836
01:51:04,000 --> 01:51:05,730
but c

1837
01:51:07,190 --> 01:51:13,780
also represents the fact that the probability factors into a product of pairwise functions

1838
01:51:13,810 --> 01:51:16,270
so consider this factor here

1839
01:51:16,290 --> 01:51:18,940
that connects e c and d

1840
01:51:18,940 --> 01:51:20,530
in general

1841
01:51:20,590 --> 01:51:23,500
that's the function of three variables

1842
01:51:23,510 --> 01:51:29,300
kate and here in the undirected graph we have explicitly represented the factor so we

1843
01:51:29,300 --> 01:51:34,180
generally need to have a function of these three variables to capture

1844
01:51:34,180 --> 01:51:36,930
the relationships between these variables

1845
01:51:36,950 --> 01:51:38,550
we're here

1846
01:51:38,570 --> 01:51:41,570
it's explicitly shown that

1847
01:51:41,580 --> 01:51:44,780
these three variables

1848
01:51:44,790 --> 01:51:49,090
interact through pairwise functions functions of two variables

1849
01:51:51,210 --> 01:51:57,530
this is significant from the point of view of representation in implementation and message passing

1850
01:51:57,530 --> 01:51:59,860
et cetera because

1851
01:51:59,880 --> 01:52:06,710
if if we just think of the variables is being discrete let's say they take

1852
01:52:06,710 --> 01:52:13,000
on k possible values then the functions in a and b are generally tables of

1853
01:52:13,000 --> 01:52:14,930
order k cute

1854
01:52:15,950 --> 01:52:20,720
whereas the functions in c are tables of order k squared

1855
01:52:20,740 --> 01:52:24,320
more generally if we have a bigger

1856
01:52:24,350 --> 01:52:29,030
network with lots of interconnections the factor graph

1857
01:52:29,060 --> 01:52:30,510
might represent

1858
01:52:30,510 --> 01:52:31,860
the forms

1859
01:52:31,880 --> 01:52:37,360
of the functions more compactly more succinctly than the undirected graph

1860
01:52:37,370 --> 01:52:40,980
because it can represent the fact that

1861
01:52:41,020 --> 01:52:44,240
these functions are not necessarily functions of

1862
01:52:44,250 --> 01:52:51,180
large sets that they can be functions small so it's very well when you right

1863
01:52:51,700 --> 01:52:57,250
the rest pressure conditions of the

1864
01:52:58,080 --> 01:53:04,710
qualitatively in terms of probability distributions in terms of conditional independence

1865
01:53:04,720 --> 01:53:08,780
they're going to be equivalent OK

1866
01:53:08,810 --> 01:53:13,940
but but i mean it's fundamentally different to say that this

1867
01:53:13,990 --> 01:53:16,520
distribution factors into

1868
01:53:16,540 --> 01:53:21,340
products of pairwise functions are three functions are some like that it's a bit more

1869
01:53:21,340 --> 01:53:26,980
fine grained representation if you write out each other factors expose

1870
01:53:32,550 --> 01:53:35,770
yes the question is in particular here

1871
01:53:35,770 --> 01:53:39,440
this is the subset of all distributions captured by the

1872
01:53:40,510 --> 01:53:43,500
that's a qualitative

1873
01:53:47,590 --> 01:53:50,970
so now why is that not y

1874
01:53:51,770 --> 01:53:55,800
can we just use undirected graphs and factor graphs well

1875
01:53:55,800 --> 01:53:57,280
in the given graph

1876
01:53:59,900 --> 01:54:05,550
there are separate not when i said yes

1877
01:54:15,520 --> 01:54:18,460
this is list

1878
01:54:19,420 --> 01:54:24,360
we can reformulate again this is

1879
01:54:24,380 --> 01:54:26,550
what was

1880
01:54:26,610 --> 01:54:29,020
each of the undirected edges

1881
01:54:29,050 --> 01:54:39,070
are you can well you can have that because in directed models directed graphical models

1882
01:54:39,130 --> 01:54:41,480
they are directed acyclic graph

1883
01:54:41,500 --> 01:54:44,130
right OK

1884
01:54:44,190 --> 01:54:48,000
you have models that have edges in both directions and you can have graphs that

1885
01:54:48,760 --> 01:54:52,070
edges pointing to the toes to go to their actions

1886
01:54:52,070 --> 01:54:55,760
but those graphs has have nothing to do with vision

1887
01:54:57,690 --> 01:55:00,760
that's important sometimes i mean of course

1888
01:55:00,880 --> 01:55:06,500
language is highly overlord the graphical models and should go the

1889
01:55:07,280 --> 01:55:09,690
the computer graphics literature they will

1890
01:55:09,690 --> 01:55:12,550
i think we are talking about something very different

1891
01:55:12,630 --> 01:55:14,340
OK so

1892
01:55:14,340 --> 01:55:15,690
let's keep going here

1893
01:55:15,710 --> 01:55:19,090
let's see what's the initial for markov random field

1894
01:55:19,150 --> 01:55:20,840
so basically

1895
01:55:20,860 --> 01:55:24,070
in the same way that vision therefore was

1896
01:55:24,110 --> 01:55:26,750
a set of probability distributions

1897
01:55:26,840 --> 01:55:28,320
it was consistent with the

1898
01:55:28,420 --> 01:55:30,190
traditional forgive

1899
01:55:30,190 --> 01:55:34,800
directed acyclic graphs so much in the field is also set of probability distributions p

1900
01:55:34,800 --> 01:55:36,730
of axioms

1901
01:55:36,750 --> 01:55:42,440
and we require that every possible probability history is for people

1902
01:55:42,460 --> 01:55:45,690
such that there exists an undirected graph

1903
01:55:45,710 --> 01:55:48,590
with disjoint subsets of nodes a b and c

1904
01:55:48,650 --> 01:55:51,480
in which whenever c d

1905
01:55:51,500 --> 01:55:54,110
whenever c separates a from b

1906
01:55:54,130 --> 01:55:55,400
in the graph

1907
01:55:56,590 --> 01:55:59,070
this conditional independence hall

1908
01:55:59,070 --> 01:56:00,750
in p

1909
01:56:00,750 --> 01:56:02,630
look it's also

1910
01:56:02,630 --> 01:56:03,670
so four

1911
01:56:03,690 --> 01:56:06,230
you need to exactly understand what's going on

1912
01:56:06,300 --> 01:56:07,230
we have to

1913
01:56:07,250 --> 01:56:12,860
parallel worlds here we have the will of probability distributions we have the all the

1914
01:56:15,030 --> 01:56:19,550
these definitions saying well it's actually relating those things excelling well

1915
01:56:19,570 --> 01:56:24,280
we're going to talk about the class of probability model

1916
01:56:24,300 --> 01:56:28,500
that can be represented by a graph

1917
01:56:28,570 --> 01:56:32,550
and the way you understand the probabilistic model is precisely

1918
01:56:32,650 --> 01:56:38,920
the both probabilistic models they are able to in cold conditional independence statements

1919
01:56:39,000 --> 01:56:43,550
if you define conditional independence in that grass is given by

1920
01:56:43,610 --> 01:56:45,780
graph separation

1921
01:56:48,590 --> 01:56:50,400
and this is critical because

1922
01:56:50,400 --> 01:56:51,280
i mean

1923
01:56:51,280 --> 01:56:53,960
their that probability models that you may not

1924
01:56:54,000 --> 01:56:55,040
be able

1925
01:56:55,130 --> 01:57:01,150
define the conditions existing conditional independence statements as graph separation

1926
01:57:01,150 --> 01:57:05,170
but we are talking about that models the model but

1927
01:57:05,190 --> 01:57:07,610
models for example

1928
01:57:07,650 --> 01:57:12,440
you have mixed type model hybrid model which parties

1929
01:57:12,590 --> 01:57:16,070
bayesian network and part of the microphone feel for example

1930
01:57:16,070 --> 01:57:17,130
i mean those

1931
01:57:17,190 --> 01:57:19,440
more complicated types of models and

1932
01:57:20,840 --> 01:57:24,500
of conditional independence statements that can represent

1933
01:57:24,550 --> 01:57:26,020
with those models

1934
01:57:26,190 --> 01:57:28,110
is large

1935
01:57:30,320 --> 01:57:34,070
but also but in practice we just say well the microphone if you use the

1936
01:57:34,070 --> 01:57:35,280
undirected graph

1937
01:57:35,300 --> 01:57:37,300
what it actually is is

1938
01:57:37,320 --> 01:57:39,610
the set of probability distributions

1939
01:57:39,710 --> 01:57:42,980
who's conditional independence statements can be

1940
01:57:42,980 --> 01:57:44,960
read by graph separation

1941
01:57:45,340 --> 01:57:48,570
in an undirected graph

1942
01:57:48,730 --> 01:57:57,380
the connection between

1943
01:57:58,710 --> 01:58:03,710
yes there are no viable sorry is implicit here

1944
01:58:03,710 --> 01:58:06,420
that there are no viable

1945
01:58:06,440 --> 01:58:09,280
yes i a b and c

1946
01:58:09,340 --> 01:58:13,050
the nodes of the graph they just represent the environment

1947
01:58:13,110 --> 01:58:14,920
in this case a

1948
01:58:14,940 --> 01:58:17,630
these are separate from the virus

1949
01:58:25,110 --> 01:58:26,800
OK so i just

1950
01:58:26,860 --> 01:58:30,190
a quick look at this so basically i mean we saw the vision that have

1951
01:58:30,190 --> 01:58:32,710
a convenient factorized form

1952
01:58:32,730 --> 01:58:37,900
we saw the factorisation of business after comes from the conditional independence statements in the

1953
01:58:40,110 --> 01:58:42,980
we saw the the converse is also true

1954
01:58:43,020 --> 01:58:46,780
so the factorisation flight condition the pen

1955
01:58:46,820 --> 01:58:50,320
in this sort these equivalence is important

1956
01:58:50,340 --> 01:58:52,440
because it connects what we know

1957
01:58:52,440 --> 01:58:53,920
which are the local

1958
01:58:53,920 --> 01:58:56,460
conditional independence statements that you can

1959
01:58:56,500 --> 01:58:58,980
get from the domain experts for example

1960
01:58:59,030 --> 01:59:01,530
with what you want which is

1961
01:59:01,650 --> 01:59:05,440
the is basically a probabilistic machine

1962
01:59:05,440 --> 01:59:06,530
that p

1963
01:59:06,550 --> 01:59:10,420
of x in the factorized form where we can actually compute things

1964
01:59:11,400 --> 01:59:15,800
the question is do we have something analogous with markov random field

1965
01:59:15,820 --> 01:59:16,570
i mean

1966
01:59:16,570 --> 01:59:19,070
in bayesian networks we had

1967
01:59:20,840 --> 01:59:23,170
i do have something from fields

1968
01:59:23,210 --> 01:59:26,380
which looks like factorisation as well

1969
01:59:26,380 --> 01:59:28,170
or some other structure

1970
01:59:28,170 --> 01:59:30,460
we need to have something

1971
01:59:30,520 --> 01:59:36,090
the answer is yes and now we're going to go into that

1972
01:59:36,300 --> 01:59:43,480
very beautiful subject

1973
01:59:45,030 --> 01:59:46,400
let's see some

1974
01:59:46,440 --> 01:59:51,340
definitions concerning undirected graph because this definition will be required

1975
01:59:51,380 --> 01:59:54,480
so that we can make progress here OK

