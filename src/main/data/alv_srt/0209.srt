1
00:00:00,000 --> 00:00:06,930
but it's uncertain examples often cases even for the human it wasn't it was very

2
00:00:06,930 --> 00:00:11,870
difficult to distinguish the right feeling of of that sentence

3
00:00:11,880 --> 00:00:15,630
and so there are few include table that

4
00:00:15,690 --> 00:00:18,590
it's about time i'll skip over it

5
00:00:18,670 --> 00:00:23,550
but this uncertainty sampling and relevance and for more negative

6
00:00:23,550 --> 00:00:24,670
they were quite

7
00:00:24,680 --> 00:00:26,220
so successful

8
00:00:26,260 --> 00:00:30,450
and then i go to the first to the last five minutes

9
00:00:30,450 --> 00:00:37,980
we have a project together with many computer vision people

10
00:00:38,000 --> 00:00:39,620
there we

11
00:00:39,630 --> 00:00:41,580
try to

12
00:00:41,620 --> 00:00:45,340
how the computer vision this is the first objective

13
00:00:45,370 --> 00:00:48,840
to help computer could recognition in computer vision

14
00:00:48,870 --> 00:00:53,600
based on text analyses but also we want to build a second goal

15
00:00:53,620 --> 00:00:56,300
is to build a joint models

16
00:00:56,310 --> 00:00:58,540
models of content recognition

17
00:00:58,540 --> 00:01:05,290
they jointly exploit text images and this is the european projects the class project you

18
00:01:05,290 --> 00:01:06,760
can put in

19
00:01:06,790 --> 00:01:10,470
some some reference to its tool in the text

20
00:01:10,480 --> 00:01:16,300
it's a joint project with the university of oxford and systems which is very good

21
00:01:16,920 --> 00:01:20,340
group with india and france where it

22
00:01:20,370 --> 00:01:23,840
the machine learning group of two billion of that not show

23
00:01:23,920 --> 00:01:25,330
schalke of

24
00:01:25,840 --> 00:01:30,170
so we and also with university of helsinki so we have

25
00:01:30,230 --> 00:01:33,010
quite interested in this topic

26
00:01:33,050 --> 00:01:37,210
now i'm going to show a very small part of this project of which one

27
00:01:37,210 --> 00:01:39,410
of the phd students work

28
00:01:41,230 --> 00:01:43,170
he is interested in

29
00:01:43,170 --> 00:01:49,880
semantic role recognition sentence so my my you shut up walks towards the field

30
00:01:49,930 --> 00:01:52,330
so we have all these terms

31
00:01:52,340 --> 00:01:56,290
we have to recognise this one to be named entity recognition

32
00:01:56,300 --> 00:01:58,600
so we have these terms which

33
00:01:58,620 --> 00:02:03,870
described by a feature vector we want to find clusters y

34
00:02:03,920 --> 00:02:09,370
maria is active the move we want to recognise the movement actions

35
00:02:09,380 --> 00:02:11,170
two locations

36
00:02:11,230 --> 00:02:15,160
also this is also common sense two

37
00:02:15,220 --> 00:02:17,770
two locations

38
00:02:17,800 --> 00:02:19,710
and i have

39
00:02:20,770 --> 00:02:22,710
we rely on

40
00:02:23,310 --> 00:02:26,290
video debates from buffy the vampire slayer

41
00:02:26,290 --> 00:02:27,640
i don't know if know

42
00:02:27,670 --> 00:02:29,210
fifty three different

43
00:02:29,220 --> 00:02:31,140
i i don't have the sound

44
00:02:31,210 --> 00:02:35,080
but i can say

45
00:02:35,090 --> 00:02:36,380
that's it

46
00:02:36,390 --> 00:02:38,750
no it doesn't

47
00:02:41,220 --> 00:02:44,290
they should be

48
00:02:44,290 --> 00:02:50,390
thank you

49
00:02:52,470 --> 00:02:57,460
i wanted to play video but some sentiment

50
00:03:06,730 --> 00:03:09,310
we have these these videos

51
00:03:09,330 --> 00:03:14,090
of buffy the vampire slayer and these soaps for teenagers and there are also lots

52
00:03:14,090 --> 00:03:15,670
of teenagers who

53
00:03:16,170 --> 00:03:17,920
all right

54
00:03:17,920 --> 00:03:23,450
this writing scripts for its videos on the web you can find so

55
00:03:23,450 --> 00:03:25,870
actually they follow the images

56
00:03:25,880 --> 00:03:28,540
and they very accurately right

57
00:03:28,550 --> 00:03:33,670
what is being done so buffy opening the refrigerator it or somewhere i wanted to

58
00:03:33,680 --> 00:03:35,000
stop the video

59
00:03:35,010 --> 00:03:39,980
so buffy opening the refrigerator and taking out a carton of milk

60
00:03:40,000 --> 00:03:49,060
you see opening the refrigerator this kind of action that is being performed by the

61
00:03:49,060 --> 00:03:50,680
actor the

62
00:03:50,690 --> 00:03:53,710
she's also taking out a carton of milk so these are

63
00:03:53,710 --> 00:03:58,190
semantic role that we want to to recognise

64
00:04:02,470 --> 00:04:07,600
there is a lot of these in this area include it also some references

65
00:04:07,620 --> 00:04:09,410
the earliest

66
00:04:09,480 --> 00:04:15,910
references from several should they are in two thousand and two in computational linguistics

67
00:04:15,970 --> 00:04:18,720
but we have currently who

68
00:04:18,730 --> 00:04:22,050
the issue of computational linguistics

69
00:04:22,050 --> 00:04:24,960
assigned to these semantic rules section

70
00:04:24,970 --> 00:04:30,970
now our task is to do weakly supervised learning here so we a limited amount

71
00:04:30,970 --> 00:04:35,970
of training examples and also to combine evidence from the images

72
00:04:36,670 --> 00:04:43,760
the first task we have now called accomplished the second one is its

73
00:04:43,770 --> 00:04:51,580
so we use hidden markov models conditional random fields

74
00:04:51,600 --> 00:04:53,620
and we have to fight

75
00:04:53,660 --> 00:04:55,480
learning of

76
00:04:55,500 --> 00:04:58,230
we will also submit to speed

77
00:04:58,290 --> 00:05:02,260
we learn mixture multiple mixture models

78
00:05:02,270 --> 00:05:07,850
we learn that interference in based on expectation maximisation but also

79
00:05:08,050 --> 00:05:13,770
approximate experiencing gibbs sampling with a markov chain monte carlo

80
00:05:13,790 --> 00:05:17,180
two too weakly supervised

81
00:05:17,230 --> 00:05:21,020
but then this is quite difficult it's quite difficult to

82
00:05:21,040 --> 00:05:23,380
improve the results based on

83
00:05:25,800 --> 00:05:28,720
so we have lots of petulance

84
00:05:28,760 --> 00:05:32,790
we have this sentence forest features which are not

85
00:05:32,810 --> 00:05:34,880
usually very accurate

86
00:05:34,890 --> 00:05:37,090
and i defer is also

87
00:05:37,090 --> 00:05:38,340
on the

88
00:05:38,340 --> 00:05:41,660
there are some work some years ago

89
00:05:41,680 --> 00:05:44,050
by like university of pittsburgh

90
00:05:44,050 --> 00:05:46,760
funny building housing kevin actually

91
00:05:46,780 --> 00:05:51,140
and the child to recognise factors and factors are

92
00:05:51,160 --> 00:05:54,220
abstract concepts in the law

93
00:05:54,240 --> 00:05:58,430
like medical malpractice might be a factor

94
00:05:58,470 --> 00:06:04,950
or maybe it's it's maybe already more an issue but factors are a certain constellation

95
00:06:04,950 --> 00:06:05,700
of facts

96
00:06:06,130 --> 00:06:11,970
and issues of certain constellation of factors so they are more abstract concepts that they

97
00:06:11,970 --> 00:06:18,340
want to recognise in these descriptions so that because they usually search

98
00:06:19,010 --> 00:06:22,930
you can have some facts pictures in long

99
00:06:22,950 --> 00:06:25,890
when you are really looking at similar case

100
00:06:25,890 --> 00:06:29,110
based on the exact facts of the case

101
00:06:29,180 --> 00:06:34,840
so like also intelligence services of police services you have some effect and you want

102
00:06:34,840 --> 00:06:37,180
to find similar cases

103
00:06:37,180 --> 00:06:41,780
but in low there is also another type of search when you're looking for

104
00:06:41,840 --> 00:06:48,110
abstract concepts and you want to find that all these cases where certain concept applied

105
00:06:48,110 --> 00:06:51,590
whether the facts the facts can play can be

106
00:06:51,590 --> 00:06:57,870
can be there from different domains to have similar concepts and then they want to

107
00:06:57,890 --> 00:07:00,660
reason with it is concept even if it's

108
00:07:00,700 --> 00:07:07,280
you know there's a lot of cases in other domains for instance of india

109
00:07:07,560 --> 00:07:08,780
this was

110
00:07:09,490 --> 00:07:12,800
they use also machine learning techniques

111
00:07:12,820 --> 00:07:17,660
main base and decision tree learners use in this case

112
00:07:17,720 --> 00:07:20,450
so induction of decision trees

113
00:07:20,450 --> 00:07:24,430
and that it was difficult days

114
00:07:24,490 --> 00:07:28,360
they did not succeed very well in finding automatically

115
00:07:28,360 --> 00:07:29,910
these factors

116
00:07:30,280 --> 00:07:35,660
this set where difficulties in the language or the language combined with the typical legal

117
00:07:36,990 --> 00:07:39,680
syntax semantics

118
00:07:39,720 --> 00:07:44,260
lou is characterized by long phrases sometimes answers

119
00:07:44,280 --> 00:07:45,970
lots of close

120
00:07:45,990 --> 00:07:49,430
so far my and difficulties

121
00:07:50,030 --> 00:07:53,050
there were some of us have this

122
00:07:55,280 --> 00:07:58,090
this is something we are doing our group

123
00:07:58,140 --> 00:08:00,760
is recognition of argumentation

124
00:08:00,760 --> 00:08:03,260
so we want to find out

125
00:08:03,260 --> 00:08:09,840
what is the conclusion of work primarily legal cases you also the european court court

126
00:08:09,840 --> 00:08:11,390
of human rights

127
00:08:11,510 --> 00:08:15,370
we want to find out what is the final conclusion of the church

128
00:08:15,430 --> 00:08:21,360
and how did he came to that conclusion what are the arguments that

129
00:08:21,370 --> 00:08:23,160
the premise somehow

130
00:08:23,180 --> 00:08:24,630
that's way

131
00:08:24,640 --> 00:08:28,340
that are the basis for that conclusion

132
00:08:28,360 --> 00:08:30,090
but it's not so easy

133
00:08:30,890 --> 00:08:36,160
you have very little discourse cues and if they are they are quite ambiguous

134
00:08:36,180 --> 00:08:40,700
and also argumentation business that so we have

135
00:08:40,720 --> 00:08:42,680
one of the conclusions

136
00:08:42,680 --> 00:08:47,740
the final verdict and then you have this is to that conclusion that each of

137
00:08:47,740 --> 00:08:53,170
these parameters can be a conclusion itself and can have supremicist there so it's a

138
00:08:53,170 --> 00:08:54,160
very nice

139
00:08:54,160 --> 00:08:56,180
it's almost tree so

140
00:08:56,180 --> 00:08:59,070
most of the cases the tree like structure

141
00:08:59,240 --> 00:09:02,090
so that makes it a little bit more easier

142
00:09:02,110 --> 00:09:04,160
but it's still

143
00:09:04,240 --> 00:09:05,760
quite difficult

144
00:09:05,800 --> 00:09:08,640
but also here we think

145
00:09:08,640 --> 00:09:11,220
there is some rules also

146
00:09:11,240 --> 00:09:17,720
when monitoring meetings or discussions political discussions speeches political speech

147
00:09:18,720 --> 00:09:20,450
also maybe

148
00:09:20,470 --> 00:09:22,030
medical decisions

149
00:09:22,050 --> 00:09:23,820
what are the arguments

150
00:09:23,820 --> 00:09:31,840
for a certain treatment for instance of patients so we could we could find these

151
00:09:33,240 --> 00:09:37,200
then we have this processing of informal texts

152
00:09:37,890 --> 00:09:40,840
well as

153
00:09:40,860 --> 00:09:42,390
you know spam

154
00:09:42,410 --> 00:09:44,700
email spam for instance

155
00:09:44,720 --> 00:09:48,740
in lots of diversity need lots of

156
00:09:48,780 --> 00:09:51,240
information that is being and

157
00:09:51,260 --> 00:09:53,160
because the u

158
00:09:53,160 --> 00:09:56,430
spammers want to mislead the filters

159
00:09:56,450 --> 00:09:58,200
so they tried to

160
00:09:58,280 --> 00:10:01,280
to use all kinds of tricks actually

161
00:10:01,300 --> 00:10:06,300
in these emails to this doesn't make the processing

162
00:10:06,320 --> 00:10:09,010
of this information easier

163
00:10:10,950 --> 00:10:12,610
it's quite a challenge

164
00:10:13,680 --> 00:10:16,720
when you try to hide compared to in the males

165
00:10:16,740 --> 00:10:19,320
we have be some publication on this

166
00:10:19,320 --> 00:10:31,510
the second yeah good as gold mining station i use relations that always explain

167
00:10:31,650 --> 00:10:35,250
now i get that sort of the only things

168
00:10:36,180 --> 00:10:38,420
you've probably seen by state machines

169
00:10:38,460 --> 00:10:41,860
it's some point in your

170
00:10:41,870 --> 00:10:46,970
courses on this is a probabilistic finite state automata OK i mean if you move

171
00:10:46,970 --> 00:10:50,340
between states with some probability not deterministically

172
00:10:50,700 --> 00:10:54,120
so here we have two states as well as to

173
00:10:54,150 --> 00:10:56,860
you have two actions change or stay

174
00:10:56,910 --> 00:10:59,530
when you next in action

175
00:10:59,570 --> 00:11:03,930
the probability p called whenever on air healthy with probability

176
00:11:03,950 --> 00:11:05,880
you take that transition

177
00:11:05,890 --> 00:11:10,280
so if i phi phi device could actually stay in stage one

178
00:11:10,320 --> 00:11:13,850
it was point nine probability of transition back state one

179
00:11:13,860 --> 00:11:15,370
with point with probability

180
00:11:15,390 --> 00:11:16,470
i transition

181
00:11:16,490 --> 00:11:19,640
stay one to the right to stage two

182
00:11:20,430 --> 00:11:24,490
this that the transition probabilities of next given the current state this on the one

183
00:11:25,490 --> 00:11:30,140
now if i change deterministically i always transition state one state two and was a

184
00:11:30,140 --> 00:11:32,050
institute and this stuff

185
00:11:32,100 --> 00:11:35,570
but actually the probability one will transition back to state two

186
00:11:35,640 --> 00:11:37,470
OK can read this

187
00:11:37,470 --> 00:11:41,450
problems the question of how many

188
00:11:47,470 --> 00:11:48,890
given given to

189
00:11:48,910 --> 00:11:53,260
take an action s

190
00:11:56,090 --> 00:12:00,530
you stated symbols

191
00:12:02,360 --> 00:12:07,240
i can find the present tense problems however one so so try to reach much

192
00:12:07,260 --> 00:12:12,410
natural language into high label these things the idea and this this is the same

193
00:12:12,410 --> 00:12:17,950
as the previous problem that i showed you were essentially you can change the state

194
00:12:17,950 --> 00:12:19,140
one state two

195
00:12:19,180 --> 00:12:22,450
but but if you try to change the state to just get stuck

196
00:12:22,510 --> 00:12:23,840
stuck in situ two

197
00:12:23,890 --> 00:12:29,510
i recorded in absorbing state in in this instance is instead of illinois

198
00:12:36,340 --> 00:12:38,510
yeah exactly

199
00:12:38,510 --> 00:12:42,780
so so stochastic you take an action you want that certain what the world will

200
00:12:42,780 --> 00:12:44,280
choose for you

201
00:12:44,320 --> 00:12:47,590
with some probability how were stacked

202
00:12:48,300 --> 00:12:50,490
the sequence of again

203
00:12:50,510 --> 00:12:54,010
fully their this notice assumption

204
00:12:54,050 --> 00:12:55,530
OK states

205
00:12:55,530 --> 00:12:59,220
so to set of one two actions stare change

206
00:13:01,140 --> 00:13:05,720
OK stay because one article stay i get to it given the red

207
00:13:08,280 --> 00:13:09,260
i get

208
00:13:11,120 --> 00:13:14,070
if the states one

209
00:13:14,090 --> 00:13:15,910
OK this is a i i think

210
00:13:16,160 --> 00:13:18,910
inconsistent here

211
00:13:18,950 --> 00:13:22,260
if they are

212
00:13:22,280 --> 00:13:25,090
technically would condition reward and next as well

213
00:13:25,140 --> 00:13:29,570
in order to get this diagram to show it to be our current state current

214
00:13:29,570 --> 00:13:31,390
action next state

215
00:13:31,410 --> 00:13:34,780
then you get rewards that shelter in the red

216
00:13:35,100 --> 00:13:40,510
so yes

217
00:13:40,590 --> 00:13:48,010
let me

218
00:13:48,100 --> 00:13:53,320
rewards can be conditioned on the current state

219
00:13:53,320 --> 00:13:56,840
the current action in the next day

220
00:13:56,840 --> 00:14:01,800
the two

221
00:14:01,910 --> 00:14:03,970
you would be doing ten

222
00:14:13,970 --> 00:14:16,220
in in this diagram

223
00:14:16,280 --> 00:14:19,590
many you know it if if you want to model them

224
00:14:19,640 --> 00:14:20,570
then know

225
00:14:20,660 --> 00:14:23,030
and that's what i want

226
00:14:23,050 --> 00:14:26,490
o point point this is a bug in my slide i'm trying to cover up

227
00:14:26,490 --> 00:14:29,410
as i'm in it this way

228
00:14:30,680 --> 00:14:35,240
but you can only think of the next state and reward function it is legal

229
00:14:35,240 --> 00:14:41,530
and finally there some complexity term penalizes for the number because otherwise you would always

230
00:14:41,530 --> 00:14:46,120
prefer to explain everything with more layers for this is what chris and john promises

231
00:14:46,120 --> 00:14:50,950
in the christmas gifts the number one christmas gift the arkansas is you need some

232
00:14:50,950 --> 00:14:53,490
kind of complexity term here as well

233
00:14:53,940 --> 00:14:56,700
these are

234
00:14:56,710 --> 00:15:00,360
after four hours of use that's important but i do want to emphasise that this

235
00:15:00,360 --> 00:15:05,820
is not a quadratic energy function so you have binary then multiplying the quadratic term

236
00:15:06,230 --> 00:15:11,090
and you have sort of parts model kind of interaction and so the minimisation is

237
00:15:11,090 --> 00:15:17,860
much more complicated you can solve this simply by inverting the linear system of equations

238
00:15:18,190 --> 00:15:22,730
but the use of sort of algorithms to minimize this we did find that these

239
00:15:22,730 --> 00:15:25,180
kinds of low energy minimisation models

240
00:15:25,250 --> 00:15:29,950
pretty much wider range of perception human vision that is not

241
00:15:30,090 --> 00:15:34,360
but the previous algorithms that but they can also explain for example here in this

242
00:15:34,360 --> 00:15:36,650
display that we've been looking at

243
00:15:36,690 --> 00:15:41,670
they take these four dots group them with the sorry this down here

244
00:15:41,690 --> 00:15:44,860
so this is the assignment variable to number two

245
00:15:44,870 --> 00:15:47,730
and this is the calculated motion field the rotation

246
00:15:47,750 --> 00:15:51,900
whereas these fifty that are perceived to be moving up and down

247
00:15:51,950 --> 00:15:57,980
and then just showing but we found that for a large range of stimuli

248
00:15:58,000 --> 00:16:01,490
this kind of low energy minimisation model correctly

249
00:16:01,520 --> 00:16:06,900
predict the motions to correctly here in quotes because again there is no such thing

250
00:16:06,940 --> 00:16:10,860
as the correct answer these are inherently opposed to

251
00:16:10,930 --> 00:16:14,420
but you can find a solution that is

252
00:16:14,850 --> 00:16:17,640
consistent with what humans see

253
00:16:17,720 --> 00:16:20,710
he served

254
00:16:20,720 --> 00:16:24,700
that was my thesis defense at least part of the natural question is what if

255
00:16:24,700 --> 00:16:30,000
you can do so these ellipses can you translate models on the real

256
00:16:30,020 --> 00:16:36,030
sequences like this one that is can we take an input image sequence of images

257
00:16:36,030 --> 00:16:37,160
that look like this

258
00:16:37,170 --> 00:16:41,900
that was photographed in the real world and somehow automatically figure out how many layers

259
00:16:41,910 --> 00:16:43,820
there are the motion of each layer

260
00:16:44,200 --> 00:16:47,440
any assignment of pixels to layers

261
00:16:47,450 --> 00:16:54,040
and one thing that became apparent that when you start thinking about this is that

262
00:16:55,030 --> 00:17:00,070
all these small constants that appear in these energy functions you might be able to

263
00:17:00,070 --> 00:17:06,070
get away and tweaking if you need to explain i don't know ten different stimuli

264
00:17:06,190 --> 00:17:10,830
between different stimuli but if you really want to know that become complex a large

265
00:17:10,830 --> 00:17:16,540
number of complex sequences and that those energy functions will tend to have many many

266
00:17:16,540 --> 00:17:21,020
parameters and this is just because of course it is hopeless to try to tweak

267
00:17:21,020 --> 00:17:26,120
these families by hand we method that would allow us to learn this energy function

268
00:17:27,950 --> 00:17:34,540
the challenges the real sequences poses the optimisation of the inference challenge

269
00:17:34,670 --> 00:17:41,500
again binary variables so this these are nonquadratic energy functions with continuous and discrete models

270
00:17:41,740 --> 00:17:47,940
so maybe you can get some heuristics for some very simple stimuli but actually optimizing

271
00:17:48,540 --> 00:17:50,660
these energy functions

272
00:17:50,960 --> 00:17:56,900
for arbitrary complex sequences of very challenging problem

273
00:17:56,910 --> 00:18:03,020
so here's the the journey and the gene has two parts large part and

274
00:18:03,030 --> 00:18:07,500
different parts of of the dream is that we should

275
00:18:07,570 --> 00:18:11,620
just lose energy functions automatically from data that is

276
00:18:11,700 --> 00:18:16,550
just showing two randomly chosen image sequences here but imagine downloading tens of thousands of

277
00:18:16,550 --> 00:18:18,500
genes of image sequences from youtube

278
00:18:18,690 --> 00:18:20,470
feeling those

279
00:18:20,490 --> 00:18:22,810
image sequences into learning algorithm

280
00:18:22,810 --> 00:18:28,210
and that will come magical energy function that would have no free parameters everything will

281
00:18:28,220 --> 00:18:31,920
be completely dictated by the learning algorithm

282
00:18:31,930 --> 00:18:34,350
so that's the first part of the dream

283
00:18:34,370 --> 00:18:38,760
and the second part is an inference stream that supposed to be figured out this

284
00:18:38,760 --> 00:18:39,830
energy function

285
00:18:39,910 --> 00:18:41,670
we were taking your image

286
00:18:41,840 --> 00:18:43,280
in sequence

287
00:18:43,290 --> 00:18:47,740
and just get into an optimisation and inference algorithm

288
00:18:47,760 --> 00:18:52,780
i mean automatically find the global optimum of the energy that would come to layers

289
00:18:52,800 --> 00:18:57,800
the number of layers and the motions of each layer

290
00:18:58,170 --> 00:19:01,670
so as you can see by the fact that colour doing you can realise that

291
00:19:02,480 --> 00:19:04,210
i'm not going to give you

292
00:19:04,230 --> 00:19:09,310
the solution to those two parts but in my defense i have tell do you

293
00:19:09,310 --> 00:19:10,450
something my brother

294
00:19:10,470 --> 00:19:13,300
who a mathematician told me which is how to to get a ph d in

295
00:19:15,050 --> 00:19:18,410
so is the algorithm advisor proposes an open problem

296
00:19:18,450 --> 00:19:23,090
the student fails to solve the problem from lemmas along the way

297
00:19:23,110 --> 00:19:29,640
in the ph d thesis describes the lemmas so the rest of this talk will

298
00:19:29,650 --> 00:19:31,200
follow similar

299
00:19:31,220 --> 00:19:34,780
so we really solve this problem

300
00:19:34,930 --> 00:19:39,780
but i tell you about two so-called like that we've managed to prove along the

301
00:19:41,390 --> 00:19:46,560
and not really was of course the more sort of results are as follows the

302
00:19:46,560 --> 00:19:47,660
first one

303
00:19:47,690 --> 00:19:53,030
is an optimisation or an inference result which is that for many real world low

304
00:19:53,030 --> 00:19:59,190
level vision problems we can efficiently find the global optimum of the energy function and

305
00:19:59,190 --> 00:20:00,240
the second one is learning

306
00:20:00,270 --> 00:20:04,640
result which says that for many real world low level vision problems we can efficiently

307
00:20:04,640 --> 00:20:06,870
learn energy functions

308
00:20:07,220 --> 00:20:09,440
and actually be discussing

309
00:20:09,450 --> 00:20:14,140
examples of these results from that i listed here just

310
00:20:14,310 --> 00:20:18,740
a small number of additional results by many other researchers and this is just the

311
00:20:18,740 --> 00:20:23,250
a sample and they think these two kinds of results

312
00:20:23,270 --> 00:20:26,940
have been demonstrated not only now but in a number of other papers over the

313
00:20:26,940 --> 00:20:32,930
last ten years until about the first one first the inspired

314
00:20:32,930 --> 00:20:37,390
this is the issue of global optimisation in stereo vision

315
00:20:37,990 --> 00:20:39,910
so stereo vision is that

316
00:20:40,870 --> 00:20:44,020
what i was doing the day-to-day basis when we look at the world through two

317
00:20:44,020 --> 00:20:46,980
of his fingers stereo

318
00:20:47,000 --> 00:20:51,990
it's actually is given a sequence of images showing here is a man meeting a

319
00:20:51,990 --> 00:20:56,560
sequence of images but the sequence of images were not actually taken by camera over

320
00:20:57,400 --> 00:20:59,640
but taken simultaneously by

321
00:20:59,650 --> 00:21:04,640
many different cameras so if this were in the case of human vision you could

322
00:21:04,640 --> 00:21:10,160
imagine and animating the two views taken by two hours at a given time and

323
00:21:10,170 --> 00:21:12,950
the point is if you do that if you have things this way

324
00:21:12,960 --> 00:21:15,610
this way the motion the relative motion

325
00:21:15,610 --> 00:21:16,970
between the two

326
00:21:17,000 --> 00:21:22,720
this corresponds to death so that's called the disparity of every pixel and basically things

327
00:21:22,720 --> 00:21:24,420
are very far from us

328
00:21:25,430 --> 00:21:28,630
you appear to be the same in the two hours was things that closer to

329
00:21:28,640 --> 00:21:33,670
us will have very different location two hours and this is a very strong view

330
00:21:33,690 --> 00:21:35,770
that we use to estimate the

331
00:21:35,790 --> 00:21:39,500
so this is an example lot of this task because our job is to go

332
00:21:39,600 --> 00:21:40,790
to put

333
00:21:40,810 --> 00:21:42,390
at every pixel

334
00:21:42,400 --> 00:21:44,120
the disparity at every pixel

335
00:21:44,130 --> 00:21:50,100
and i was surprised of population of the energy minimisation this goes back at least

336
00:21:50,100 --> 00:21:54,830
to martin junior these is by energy minimisation

337
00:21:54,850 --> 00:22:00,260
writing down an energy function has two terms a local data

338
00:22:00,280 --> 00:22:04,930
which essentially just looks at a single pixel and says given the current in the

339
00:22:04,930 --> 00:22:06,330
sign that i

340
00:22:06,330 --> 00:22:09,590
how likely is it that this pixel moved three

341
00:22:09,610 --> 00:22:13,100
up to the left of the pixels to the right so that the data terms

342
00:22:13,370 --> 00:22:15,540
and there's also the smoothness term

343
00:22:15,920 --> 00:22:21,730
which penalizes pages that penalizes discontinuities in the disparity field

344
00:22:22,730 --> 00:22:27,190
typical instead this one energy would also take into consideration the colour

345
00:22:27,220 --> 00:22:29,390
so you larger penalty

346
00:22:29,400 --> 00:22:30,840
for discontinuity

347
00:22:30,850 --> 00:22:35,160
if two neighboring pixels have the same colour and much smaller penalty

348
00:22:35,160 --> 00:22:37,500
all of them so on

349
00:22:37,510 --> 00:22:42,750
on one side we have the simplest answer this would be character level then

350
00:22:42,760 --> 00:22:48,020
from characters we compose words we can deal with words and phrases and part of

351
00:22:48,020 --> 00:22:49,850
speech tags and taxonomies like

352
00:22:50,340 --> 00:22:55,550
wordnet and so on so this sort of this would be kind of lexical

353
00:22:55,570 --> 00:23:01,620
type of representation next next to vector space model language models

354
00:23:01,660 --> 00:23:06,500
what are seeing and cross modality so i've pulled this

355
00:23:06,520 --> 00:23:11,130
representations under the so called syntactic syntactic level and at the most

356
00:23:11,160 --> 00:23:18,550
used actually in text mining so especially vector space model would be most often useless

357
00:23:18,560 --> 00:23:23,310
in information three will text learning and so on

358
00:23:23,370 --> 00:23:30,750
and the semantic web presentations would be that's a collaborative tagging social web two point

359
00:23:30,750 --> 00:23:37,590
zero would be hidden somewhere here templates frames so popular presentation and so one of

360
00:23:37,600 --> 00:23:40,510
the most complex ones ontologies and first order

361
00:23:45,030 --> 00:23:50,530
slightly roughly semantic so or collaborative tagging is hard to say whether this is really

362
00:23:50,530 --> 00:23:58,390
semantics not but somehow it's closer to semantics and it's just syntactic level

363
00:23:58,410 --> 00:24:04,040
OK now let's go first this character level

364
00:24:04,050 --> 00:24:11,130
so character level means simply just taking the documents taking the text and split on

365
00:24:11,130 --> 00:24:19,120
the characters and maybe use just sequences of two or three characters and that's it

366
00:24:19,130 --> 00:24:26,290
another this sequences of characters we constructed constructed vectors of frequencies and this then used

367
00:24:26,290 --> 00:24:29,430
by learning and because

368
00:24:29,450 --> 00:24:33,270
he said that text has this nice provided that

369
00:24:33,350 --> 00:24:38,270
uh it has a lot of redundant data it's a lot of redundancy in information

370
00:24:38,670 --> 00:24:39,640
so it's

371
00:24:39,660 --> 00:24:43,140
quite likely that we will be able to solve some of the problems

372
00:24:43,150 --> 00:24:45,800
so this representation has

373
00:24:45,810 --> 00:24:54,420
a couple of very important strengths first is very robust because it avoids language morphology

374
00:24:56,100 --> 00:25:03,640
and especially resentation is used but for language identification that a lot of applications including

375
00:25:03,640 --> 00:25:07,880
i think that's a good will and so on some are using language and ification

376
00:25:07,880 --> 00:25:11,230
and this can be done very efficiently just by

377
00:25:11,280 --> 00:25:13,490
checking the frequencies of

378
00:25:15,120 --> 00:25:17,130
the character sequences

379
00:25:19,000 --> 00:25:23,200
you can also capture the simple patterns on character level and this is then used

380
00:25:23,200 --> 00:25:29,360
for spam detection copy detection so these problems are fit very nicely to this presentation

381
00:25:33,250 --> 00:25:37,840
we can do also learning clustering search with this representation but somehow these this can

382
00:25:37,840 --> 00:25:44,580
get them quite limited is also quite popular inspection this kernel methods support vector machine

383
00:25:45,010 --> 00:25:53,030
so-called string kernels which are trying to capture this more complex sequences of characters but

384
00:25:53,040 --> 00:25:58,050
some of this can get quite complicated and a little bit less efficient but this

385
00:25:59,190 --> 00:26:01,190
experiment i guess this was

386
00:26:01,580 --> 00:26:07,820
quite well accepted from the community for some deeper semantic desk tasks like understanding the

387
00:26:07,860 --> 00:26:13,610
this text this presentation with me wouldn't be a properties simply too weak

388
00:26:13,620 --> 00:26:16,190
we don't have even worse become just a couple of

389
00:26:16,210 --> 00:26:17,610
character sequences

390
00:26:17,620 --> 00:26:25,310
so for this next level we can go to the level of words

391
00:26:27,650 --> 00:26:33,030
obviously this is very obvious representation of text documents so that we just

392
00:26:33,040 --> 00:26:40,270
do they could tokenisation and split the document into the more items you can find

393
00:26:40,310 --> 00:26:46,500
numerous of the positions of the packages on the maps it is quite easy what's

394
00:26:46,500 --> 00:26:52,620
also important to note that worked for some languages at least it's not so well

395
00:26:52,620 --> 00:26:56,500
defined as in china some important somehow worked

396
00:26:56,900 --> 00:27:04,830
words are not so nicely list split investor languages so this is just

397
00:27:07,320 --> 00:27:11,640
what are the most relevant words properties

398
00:27:13,490 --> 00:27:16,250
the main ones would with the following so

399
00:27:16,260 --> 00:27:17,840
homonomy so

400
00:27:17,850 --> 00:27:24,950
we have the same surface form but different meaning plenty river bank or financial institution

401
00:27:24,990 --> 00:27:29,080
so the the bank means what

402
00:27:29,100 --> 00:27:30,690
and we need to be able to

403
00:27:30,710 --> 00:27:32,540
decompose these two meanings

404
00:27:32,900 --> 00:27:35,630
december great this

405
00:27:35,640 --> 00:27:39,140
two meanings in the proper than polysemy

406
00:27:39,150 --> 00:27:42,460
very have the same forum and related meaning like

407
00:27:42,480 --> 00:27:46,830
the bank will be the blood bank or financial institution again two

408
00:27:46,870 --> 00:27:49,800
meanings but similar ones

409
00:27:49,820 --> 00:27:56,580
synonymy this is usually the most problematic one different performance a meaning like singer or

410
00:27:56,580 --> 00:28:00,340
locally so they mean the same but they have different

411
00:28:00,350 --> 00:28:05,590
surface form and hyponymy one word denotes a subclass of

412
00:28:05,790 --> 00:28:08,150
the other one like breakfast meal

413
00:28:08,160 --> 00:28:11,400
it would be such a relationship

414
00:28:11,420 --> 00:28:16,280
so we need to be able to deal with this property is when we are

415
00:28:16,280 --> 00:28:21,000
doing some kind of analysis of texts

416
00:28:21,430 --> 00:28:25,570
what also very important to say so word frequencies in texts have the so-called power

417
00:28:25,570 --> 00:28:31,100
distribution which we will see later on when we will discuss graphs and ling link

418
00:28:31,100 --> 00:28:34,220
structures there are basically

419
00:28:35,170 --> 00:28:38,900
it was down to the following two properties we have small number of very frequent

420
00:28:38,900 --> 00:28:44,260
words like the and the and so on and big number of very low frequency

421
00:28:46,130 --> 00:28:50,700
and this the the structure generates redundancy in

422
00:28:50,710 --> 00:28:55,930
in fact so this is very important underlying property which

423
00:28:55,980 --> 00:29:02,050
usually we are not even aware of it but some of the generates lot has

424
00:29:02,050 --> 00:29:07,940
a lot of consequences on algorithms which we use for text

425
00:29:07,950 --> 00:29:14,450
so a few more things about this word level representation so we often use so-called

426
00:29:14,450 --> 00:29:19,470
mismatch between the doctors and the people who actually looking for the information OK so

427
00:29:19,470 --> 00:29:25,380
you have the technical vocabulary on one hand and then you have the

428
00:29:25,400 --> 00:29:26,210
you know

429
00:29:26,230 --> 00:29:31,610
layman's terms of describing certain symptoms and diseases and so on and so forth and

430
00:29:31,910 --> 00:29:36,500
but very often you have documents that were both things corker

431
00:29:36,540 --> 00:29:40,400
and so you know using statistical methods you could actually figure this out

432
00:29:40,450 --> 00:29:42,150
so here is actually

433
00:29:42,460 --> 00:29:47,370
right and then you can we can also do things like

434
00:29:47,380 --> 00:29:53,660
actually disambiguating queries this just an example of which

435
00:29:53,670 --> 00:29:57,740
i used to show you that once you have learnt a richer model in the

436
00:29:58,410 --> 00:30:03,220
so you can you can you know there's certain usability features that you can derive

437
00:30:03,220 --> 00:30:07,120
from that so here is a case where we have a query like in this

438
00:30:07,120 --> 00:30:08,570
case bleeding

439
00:30:09,600 --> 00:30:13,240
which is in a sense highly ambiguous because

440
00:30:13,250 --> 00:30:13,980
you know

441
00:30:14,000 --> 00:30:17,960
there are things like concept one concept his bleeding disorders

442
00:30:18,630 --> 00:30:25,770
gastrin and intestinal bleeding administration and so on and so forth so

443
00:30:25,810 --> 00:30:28,280
if someone as the query like bleeding

444
00:30:28,290 --> 00:30:31,750
right what would be the right answer well you know we believe there is no

445
00:30:31,750 --> 00:30:37,060
right answer because the queries underspecified right someone you might think of you know one

446
00:30:37,060 --> 00:30:40,530
of these right but might not be aware of the fact that it's you know

447
00:30:40,530 --> 00:30:43,890
it's highly ambiguous so what you can do that in these cases you can you

448
00:30:43,890 --> 00:30:48,670
can make the ambiguity explicit by saying well look in know their different results if

449
00:30:48,670 --> 00:30:50,080
you mean

450
00:30:50,100 --> 00:30:54,110
you know bleeding disorders then these are good results but if you're interested in administration

451
00:30:54,110 --> 00:30:57,200
problems and so on and so forth and know this might be good for

452
00:30:57,980 --> 00:31:00,010
so anyway you know you can

453
00:31:00,110 --> 00:31:04,560
some people call you know a few things like this for result clustering writing search

454
00:31:04,560 --> 00:31:08,950
and if you remember so he really result clustering but we're using

455
00:31:09,350 --> 00:31:12,040
a concept structure that we identify

456
00:31:12,050 --> 00:31:16,330
prior to the to the query it's not that you know we first computer resulted

457
00:31:16,330 --> 00:31:21,980
in an cluster it but you know we we compute this prior to the query

458
00:31:23,350 --> 00:31:26,480
there's been a

459
00:31:26,490 --> 00:31:28,870
nice improvement on this model

460
00:31:29,560 --> 00:31:33,670
which i just want to mention briefly here which

461
00:31:33,700 --> 00:31:38,580
has been called the latent dirichlet allocation model by david blyth lyon rating and michael

462
00:31:39,630 --> 00:31:41,980
this is fairly recent work

463
00:31:41,990 --> 00:31:45,080
we're actually

464
00:31:45,110 --> 00:31:47,480
this parameter

465
00:31:47,480 --> 00:31:50,210
pi in the

466
00:31:50,220 --> 00:31:53,750
in the probabilistic latent semantic analysis model which is the probability

467
00:31:54,120 --> 00:31:58,280
of given the document of the particular concepts

468
00:31:58,280 --> 00:32:02,370
right there was true there was a parameter in this model is treated here as

469
00:32:02,380 --> 00:32:06,570
random variables in this latent dirichlet allocation model

470
00:32:06,600 --> 00:32:10,550
and they put a dirichlet density

471
00:32:10,620 --> 00:32:15,450
prior on top of this by variable so what is this good for well for

472
00:32:15,450 --> 00:32:18,300
one thing is you know you have a nice bayesian

473
00:32:18,320 --> 00:32:23,760
another layer of bayesian modeling so you can smooth things out a little bit you

474
00:32:23,770 --> 00:32:26,200
know you get someone

475
00:32:26,220 --> 00:32:27,480
a better estimate

476
00:32:27,770 --> 00:32:32,070
so you know it's kind of the bias variance trade of course but usually it

477
00:32:32,070 --> 00:32:33,220
pays off to

478
00:32:33,960 --> 00:32:38,370
introduce some bias and reduce the variance of the estimate here and the other thing

479
00:32:38,370 --> 00:32:42,330
is that if you do that way you actually get truly generative model where you

480
00:32:42,330 --> 00:32:46,550
could also generate new documents because then you have the distribution of these things

481
00:32:46,580 --> 00:32:49,140
right well before we just had those

482
00:32:49,160 --> 00:32:52,720
number for every document but for a new document we wouldn't know

483
00:32:52,720 --> 00:32:57,890
right but here we can actually sample new document by sampling the spy and then

484
00:32:57,920 --> 00:33:00,390
something worth recording

485
00:33:00,420 --> 00:33:03,700
you know the is basically the mixing proportions here

486
00:33:05,000 --> 00:33:09,510
so the way you would generate documents is you generate a mix of topics

487
00:33:09,550 --> 00:33:11,670
according to the dirichlet distribution

488
00:33:12,130 --> 00:33:16,110
and then you would generate a certain number of words you know what

489
00:33:16,120 --> 00:33:21,130
according to that the same appeal as a like a mixture model right we would

490
00:33:21,450 --> 00:33:25,240
pick a topic according to distribution and then pick award according to the

491
00:33:26,710 --> 00:33:29,370
to these probabilities here that we've seen before

492
00:33:29,720 --> 00:33:33,210
and we can we can actually integrate out these

493
00:33:33,220 --> 00:33:36,880
pi variables OK switch here and then

494
00:33:38,550 --> 00:33:39,290
you know we

495
00:33:39,310 --> 00:33:45,460
we get we can compute the marginal distribution over of particular particular

496
00:33:47,320 --> 00:33:51,930
OK so and there are some issues here too you know i well first of

497
00:33:51,930 --> 00:33:57,120
all is also geometric interpretation is also true for the kids and the way it

498
00:33:57,150 --> 00:34:01,000
is you know you can think of these topics are concepts

499
00:34:01,040 --> 00:34:03,700
it's kind of spending subspace

500
00:34:03,710 --> 00:34:06,140
you know what you're doing is every document

501
00:34:07,600 --> 00:34:13,210
is a convex combination of these topics right in this mixture and different documents are

502
00:34:13,210 --> 00:34:18,150
different convex combinations but you only have a very small number of topics are concepts

503
00:34:18,150 --> 00:34:21,280
compared to the number of dimensions in your space which is the number of terms

504
00:34:21,280 --> 00:34:25,930
a right so what you then do is you you know you're topic

505
00:34:25,950 --> 00:34:30,720
you can think of these as vectors that spanned some subspace

506
00:34:30,730 --> 00:34:35,290
o actually a set of points and then the convex hull of these points is

507
00:34:35,290 --> 00:34:40,590
everything you can represent right document will always be in the convex hull of those

508
00:34:41,560 --> 00:34:46,420
so what you're asking for the in the way is you know in this very

509
00:34:46,420 --> 00:34:49,330
high dimensional space you're looking for low dimensional

510
00:34:49,350 --> 00:34:57,950
the simplex or you or convex region of this of this type of dimension humans

511
00:34:58,700 --> 00:35:03,750
or or less so that you know you can you can find a good approximation

512
00:35:03,750 --> 00:35:08,220
for each document within that within that context switching

513
00:35:08,540 --> 00:35:13,490
so the geometry and in in terms of if you do this just allocation model

514
00:35:13,490 --> 00:35:14,820
as opposed to the

515
00:35:14,860 --> 00:35:16,410
simple kid is a model

516
00:35:17,820 --> 00:35:20,850
it get a little more sophisticated you you have to

517
00:35:20,960 --> 00:35:23,260
one way you could do it is you can

518
00:35:23,260 --> 00:35:28,280
two variation OEM and other people have suggested other methods case it gets a little

519
00:35:28,280 --> 00:35:33,450
more involved in this trade off between approximation and efficiency

520
00:35:33,500 --> 00:35:36,990
but i want to talk about this you can look this up here and

521
00:35:37,060 --> 00:35:38,450
in the paper

522
00:35:38,460 --> 00:35:39,470
OK so

523
00:35:39,480 --> 00:35:43,250
can someone tell me how much time is left

524
00:35:43,310 --> 00:35:46,070
in this

525
00:35:46,090 --> 00:35:47,240
ten minutes

526
00:35:49,530 --> 00:35:53,320
all right then i am

527
00:35:53,340 --> 00:35:57,150
just to you know the i wanted to talk about

528
00:35:57,170 --> 00:36:01,270
but i could also skip

529
00:36:01,280 --> 00:36:09,060
who's who is familiar with this hits argument by jon kleinberg

530
00:36:09,080 --> 00:36:11,130
relatively few

531
00:36:11,150 --> 00:36:12,790
OK well then i should

532
00:36:12,790 --> 00:36:15,740
to do something different

533
00:36:15,750 --> 00:36:16,950
they say you know what

534
00:36:16,990 --> 00:36:19,240
since it's a hundred times more

535
00:36:19,300 --> 00:36:21,850
you click through is a hundred times higher

536
00:36:21,880 --> 00:36:25,400
i can actually take on the rest a child you're only somebody clicks because you

537
00:36:25,400 --> 00:36:26,290
know one

538
00:36:26,300 --> 00:36:30,440
i'm pretty sure some instantly and make more money by charging on click-through purchases charges

539
00:36:30,450 --> 00:36:31,620
just show you

540
00:36:31,720 --> 00:36:34,500
which is other than this

541
00:36:35,880 --> 00:36:40,610
and being naive newcomer to the space i started asking some very simple questions

542
00:36:40,630 --> 00:36:43,740
from a strategic perspective again from from the side

543
00:36:43,740 --> 00:36:46,640
which is what's unique about search

544
00:36:46,670 --> 00:36:48,680
but search for instance

545
00:36:48,690 --> 00:36:52,300
of interactive applications

546
00:36:52,330 --> 00:36:53,840
well thank you know to point

547
00:36:53,860 --> 00:36:56,600
he words in the box you get six hundred thousand results look at the top

548
00:36:59,120 --> 00:37:04,020
that's very very simple interactive applications such you can get a box and when he

549
00:37:06,310 --> 00:37:08,320
imagine applications online

550
00:37:08,340 --> 00:37:10,250
like for example

551
00:37:10,260 --> 00:37:15,320
a travel integrators like yahoo travel expedia any of right

552
00:37:15,350 --> 00:37:22,110
you're actually configuring online right so this thing knows all of your schedule when traveling

553
00:37:22,210 --> 00:37:26,230
to fly preferences are what you tell preferences go for the cheaper to go the

554
00:37:26,230 --> 00:37:28,440
expenses business pleasure

555
00:37:29,090 --> 00:37:33,300
family in other travellers such

556
00:37:33,320 --> 00:37:35,330
so if it's all about relevance

557
00:37:36,540 --> 00:37:41,610
then think about that i just want a bunch of interactions with

558
00:37:41,630 --> 00:37:46,080
in fact i can infer more knowledge about the context and your hand

559
00:37:46,090 --> 00:37:47,500
i for more

560
00:37:47,530 --> 00:37:51,900
then he said that a hundred keywords one after the other

561
00:37:53,640 --> 00:37:57,060
that's the question of our you know this is what i said how much to

562
00:37:57,060 --> 00:38:01,730
charge for me to the right of the travel confirmation page but when i know

563
00:38:01,730 --> 00:38:03,810
everything about the

564
00:38:03,830 --> 00:38:08,460
and the answer was well that's a premium so which are something like thirty to

565
00:38:08,460 --> 00:38:09,740
forty dollars

566
00:38:09,740 --> 00:38:14,280
see here meaning thirty four dollars four thousand views

567
00:38:14,300 --> 00:38:18,740
if i did not participate that they could search just a

568
00:38:19,150 --> 00:38:23,020
semantically more powerful search because i actually no intent way more

569
00:38:23,060 --> 00:38:24,610
i should be charged

570
00:38:24,620 --> 00:38:26,110
ten thousand dollars

571
00:38:26,130 --> 00:38:27,500
four thousand years

572
00:38:28,750 --> 00:38:33,530
three orders of magnitude bigger two or three or to begin on the money side

573
00:38:33,540 --> 00:38:37,010
right the market has figured that out and part of what i want to show

574
00:38:37,010 --> 00:38:40,440
you here is that you know we're getting there slowly

575
00:38:40,460 --> 00:38:43,300
for example related to search

576
00:38:43,330 --> 00:38:47,530
somebody search and this is an example of moving objects

577
00:38:47,540 --> 00:38:52,370
what if i just did somebody search ignoring everything else politics and everything else and

578
00:38:52,370 --> 00:38:53,730
what if i

579
00:38:53,730 --> 00:38:55,250
for the next

580
00:38:55,270 --> 00:39:00,120
our actually turned out to be forty eight hours what found by that search

581
00:39:00,140 --> 00:39:01,390
by the way

582
00:39:01,400 --> 00:39:03,940
brown had this is graphical

583
00:39:03,980 --> 00:39:06,120
so there statistically more likely

584
00:39:06,170 --> 00:39:09,250
to be out of the category of financial services in this case

585
00:39:09,990 --> 00:39:12,150
what do i get

586
00:39:15,480 --> 00:39:21,520
about two hundred percent the law twenty five percent two hundred percent lived

587
00:39:21,540 --> 00:39:23,290
english words

588
00:39:23,300 --> 00:39:25,190
which is not

589
00:39:25,200 --> 00:39:30,100
surprise but remember this is not just a graphical applications one

590
00:39:30,130 --> 00:39:34,070
i can search and i said OK if i see this person again which they

591
00:39:34,070 --> 00:39:38,420
come back again and again with no within forty eight hours short

592
00:39:38,430 --> 00:39:39,860
we get that look like

593
00:39:39,880 --> 00:39:44,350
that created whole new products on the market actually direct marketers who would only spend

594
00:39:44,350 --> 00:39:47,490
money on direct marketing suddenly started by

595
00:39:48,400 --> 00:39:50,110
practical oriented

596
00:39:50,120 --> 00:39:52,600
that's a big deal today we actually has

597
00:39:52,630 --> 00:39:56,290
behavioral targeting and other things to it but so is just one of the words

598
00:39:56,550 --> 00:40:00,010
in this case i'm just showing what happens if you search so an example of

599
00:40:00,010 --> 00:40:01,690
how you can use the data

600
00:40:03,040 --> 00:40:07,050
that are much more relevant to the consumer and of course the business

601
00:40:07,570 --> 00:40:11,540
we after a lot of things have to anonymize the data after this category don't

602
00:40:11,540 --> 00:40:17,260
actually keep he doesn't exactly where private citizens and so forth

603
00:40:18,580 --> 00:40:22,430
looking very good example to convince you that it's very important

604
00:40:22,470 --> 00:40:26,270
and is actually in all the examples but it's very positive

605
00:40:26,420 --> 00:40:32,300
which is you know you know everything somebody does example of yahoo autos

606
00:40:32,320 --> 00:40:34,480
and you know

607
00:40:34,510 --> 00:40:36,510
somebody with four cars

608
00:40:36,520 --> 00:40:38,330
for the last that's and looking for

609
00:40:38,350 --> 00:40:44,300
if somebody looks at a car manufacturer website

610
00:40:44,330 --> 00:40:47,940
if somebody goes on in order to him interested in kind

611
00:40:47,950 --> 00:40:50,590
it turns out that this is together

612
00:40:50,610 --> 00:40:54,240
we can actually find something like three hundred thousand consumers

613
00:40:54,320 --> 00:40:59,090
well we know seventy percent confidence by ninety days

614
00:40:59,110 --> 00:41:03,480
so you have some of month in the US is about

615
00:41:03,480 --> 00:41:06,440
about thirty percent of the US car

616
00:41:06,490 --> 00:41:08,600
i guess what we actually have ninety days

617
00:41:08,670 --> 00:41:10,250
the chosen one

618
00:41:10,260 --> 00:41:13,060
because you see them all the ninety days before the fight

619
00:41:13,090 --> 00:41:15,030
right i

620
00:41:17,830 --> 00:41:22,610
just a quick can show you some of the other members that show with sort

621
00:41:22,610 --> 00:41:23,800
of public numbers

622
00:41:23,820 --> 00:41:26,530
we need to test campaign

623
00:41:26,570 --> 00:41:29,110
of saying that these people

624
00:41:29,130 --> 00:41:35,390
four at outside the orders which is the only place where you will automobile manufacturers

625
00:41:35,390 --> 00:41:37,110
like to spend money on you

626
00:41:37,120 --> 00:41:40,790
so if you want to email what about this

627
00:41:40,810 --> 00:41:43,490
and what we found was the target guy

628
00:41:43,540 --> 00:41:47,860
based on what you know about which is an inference that the market for cars

629
00:41:47,880 --> 00:41:50,280
that the conversion

630
00:41:50,820 --> 00:41:54,220
as measured in this case by some actions they took on the advertiser side is

631
00:41:54,220 --> 00:41:56,940
up by two three orders

632
00:41:56,950 --> 00:42:00,740
that's a list nine hundred percent that's huge

633
00:42:00,810 --> 00:42:03,300
that's not that's not trivial stuff

634
00:42:03,350 --> 00:42:07,720
that shows you how much is an example of the power of behavioral targeting

635
00:42:07,720 --> 00:42:12,940
because nothing predicts behavior like human

636
00:42:12,940 --> 00:42:17,930
actually even how many different definitions of what is see subgaussian but basically the idea

637
00:42:17,930 --> 00:42:19,980
is that in a certain way you have

638
00:42:20,240 --> 00:42:21,710
you have to

639
00:42:21,730 --> 00:42:27,760
every every probability distribution function you might see on the space of distributions into two

640
00:42:27,780 --> 00:42:31,640
and it's just enough to have the have

641
00:42:31,660 --> 00:42:37,800
PSI that belongs to the right half of the space

642
00:42:37,810 --> 00:42:42,570
so maximum likelihood estimation is more is easier than you might think you don't really

643
00:42:42,570 --> 00:42:54,910
need to have a good estimate the nation of this density

644
00:43:04,350 --> 00:43:05,060
on this

645
00:43:05,070 --> 00:43:18,910
the communist

646
00:43:23,570 --> 00:43:45,400
OK this species

647
00:43:45,410 --> 00:43:50,140
so what happened actually here because i mean the display here doesn't change at all

648
00:43:50,180 --> 00:43:52,840
whatever i do what what actually happened here

649
00:43:52,840 --> 00:43:56,100
it is shown pages

650
00:43:56,910 --> 00:44:01,610
OK because of this screen i see on the same page so i thought disposal

651
00:44:01,650 --> 00:44:17,390
OK let's just

652
00:44:17,390 --> 00:44:20,590
if you just to run actor but again it doesn't matter if this doesn't show

653
00:44:37,750 --> 00:44:44,490
OK well let's let's get it on the screen name

654
00:45:15,610 --> 00:45:18,230
the reason for

655
00:45:47,720 --> 00:45:51,400
this overview says very much but i just previously said

656
00:45:52,050 --> 00:45:56,400
this most of these approaches can be as they can be also interpreted as maximizing

657
00:45:56,400 --> 00:46:01,280
the number of independent component so well that's my viewpoint of somebody might say that

658
00:46:01,280 --> 00:46:07,400
all of these approaches can be interpreted as maximisation likely

659
00:46:07,410 --> 00:46:11,190
so what actually but there is one thing that we have to to choose which

660
00:46:11,190 --> 00:46:16,570
is this not a nonquadratic function that we had in each of these methods so

661
00:46:17,820 --> 00:46:20,290
looking kurtosis we had the fourth power

662
00:46:20,300 --> 00:46:25,070
then in differential entropy we had always the logarithm of the

663
00:46:25,090 --> 00:46:29,490
of the probability density function which we have so we were in the likelihood we

664
00:46:29,490 --> 00:46:34,940
get on with the probability density function of the independent components and likelihood we get

665
00:46:34,940 --> 00:46:36,730
a logarithmic of that

666
00:46:36,760 --> 00:46:41,420
and that is always and there is always someone with nonquadratic function because the only

667
00:46:41,460 --> 00:46:49,100
distribution that has a quadratic pdf for local pdf is the gaussian distribution

668
00:46:50,110 --> 00:46:52,050
yes so

669
00:46:52,060 --> 00:46:55,200
what we can use so we get it the fourth power of the law of

670
00:46:55,210 --> 00:47:00,420
the density of then like in these approximations all entropy what i proposed is that

671
00:47:00,430 --> 00:47:05,680
you could use something like that small approximation of the absolute value which is kind

672
00:47:05,680 --> 00:47:09,240
of a seems to be working with universal approximation

673
00:47:09,600 --> 00:47:11,980
and then another choice that we

674
00:47:12,000 --> 00:47:16,050
i have to make is whether we want to estimate these components one by one

675
00:47:16,050 --> 00:47:20,730
as i proposed in the in the first in the first place as as as

676
00:47:21,040 --> 00:47:26,090
a simple at least at least you know intuitively simple method of estimating these things

677
00:47:26,090 --> 00:47:30,980
always we estimate all the components at the same time well you actually expected to

678
00:47:30,980 --> 00:47:36,000
estimate all the components at the same time although you know it's it's it's perhaps

679
00:47:36,000 --> 00:47:39,960
easier to understand this method estimate to make them one by one and also it's

680
00:47:39,960 --> 00:47:43,780
easier to pull things like consistency in the case where you have access to them

681
00:47:43,780 --> 00:47:46,200
one by one

682
00:47:46,250 --> 00:47:49,890
and then one thing that i didn't really talk about is whether we

683
00:47:49,930 --> 00:47:53,390
do make this constraints where

684
00:47:53,390 --> 00:47:57,950
the data is first whitened and then we use or not transformation which is the

685
00:47:57,950 --> 00:48:01,480
same thing as as concerning the estimates to be white

686
00:48:01,820 --> 00:48:06,080
it is and this is quite necessary especially in when you don't like maximum likelihood

687
00:48:06,080 --> 00:48:11,760
estimation you can skip the constraints so in that case you have a slightly larger

688
00:48:11,760 --> 00:48:14,070
space of

689
00:48:14,140 --> 00:48:21,490
of all the parasite large parameter space where you search for parameters and probably in

690
00:48:21,490 --> 00:48:27,690
most cases you all the optimal parameter values will not exactly like in this constraints

691
00:48:27,690 --> 00:48:34,220
constraints because i mean only theoretically in theory for infinite sample sizes and assuming all

692
00:48:34,230 --> 00:48:38,470
the making of the assumptions of the model we we know that constraint is exactly

693
00:48:38,470 --> 00:48:43,070
true but in reality in practice it is only approximately two

694
00:48:43,110 --> 00:48:50,170
but that doesn't mean that this this and usually make a big difference the estimation

695
00:48:51,080 --> 00:48:52,230
next question is

696
00:48:52,250 --> 00:48:56,400
how we actually maximize this objective functions

697
00:48:59,240 --> 00:49:03,100
of course the first thing that you could try to do is to use some

698
00:49:03,100 --> 00:49:05,780
kind of

699
00:49:05,890 --> 00:49:10,680
some kind of a gradient methods

700
00:49:10,730 --> 00:49:14,630
and that's what people usually do it is it is not very difficult to derive

701
00:49:14,650 --> 00:49:22,900
in closed form the the the gradient of the likelihood of these measures some gaussian

702
00:49:22,960 --> 00:49:24,370
for example this is

703
00:49:24,370 --> 00:49:29,760
what if all the various reasons people often use this these adaptive or stochastic gradient

704
00:49:29,760 --> 00:49:32,400
methods which means that

705
00:49:33,110 --> 00:49:36,390
did not already talk about this

706
00:49:37,580 --> 00:49:40,920
yes i will never die

707
00:49:40,960 --> 00:49:43,400
i wanted to make sure

708
00:49:43,990 --> 00:49:45,380
OK so

709
00:49:45,400 --> 00:49:50,490
so this is is what the stochastic gradient of maximum of like the likelihood looks

710
00:49:50,490 --> 00:49:52,590
like and it's relatively simple

711
00:49:53,140 --> 00:49:55,670
but the problem is that we have here

712
00:49:55,680 --> 00:49:59,630
the universe of w

713
00:49:59,640 --> 00:50:04,080
which is basically the same thing as a but we also believe but in this

714
00:50:04,080 --> 00:50:09,030
case we have to invert the running estimates of w at every step which is

715
00:50:09,030 --> 00:50:13,210
of course competition not very nice now is a very funny tricks that you can

716
00:50:13,210 --> 00:50:17,230
actually do to avoid this thing what you do is that

717
00:50:17,260 --> 00:50:18,790
you just multiply

718
00:50:18,790 --> 00:50:22,220
the log partition function is just to the data

719
00:50:22,220 --> 00:50:26,040
and once again we don't have the natural parameters

720
00:50:26,040 --> 00:50:28,970
because the ones in the natural parameters are not the ones

721
00:50:29,220 --> 00:50:31,500
one of the ones we are familiar with

722
00:50:31,540 --> 00:50:36,750
the natural parameter space occupies the entire real line

723
00:50:36,750 --> 00:50:43,350
and the ones we are interested in i actually need to the natural parameter

724
00:50:44,430 --> 00:50:49,100
so in this case the natural parameters longer here

725
00:50:52,890 --> 00:50:54,520
let's do one more example

726
00:50:54,520 --> 00:50:56,330
this one's a little trickier

727
00:50:56,370 --> 00:51:00,200
because the we need more than one feature

728
00:51:00,370 --> 00:51:04,240
this is the number gaussians with mean o and variance sigma squares

729
00:51:04,290 --> 00:51:06,120
so this is the form of the gaussians

730
00:51:06,140 --> 00:51:09,270
and now we want to write this is an exponential family

731
00:51:09,310 --> 00:51:12,540
as an exponential family distribution

732
00:51:12,540 --> 00:51:14,540
if you expand out the square

733
00:51:14,540 --> 00:51:17,470
you end up with two both in x and x squared

734
00:51:17,480 --> 00:51:21,350
so you need to pick you need to pick two features x has to be

735
00:51:21,350 --> 00:51:24,180
a feature and x squared also has to be a feature

736
00:51:24,200 --> 00:51:27,770
and if applicable to the things that you can write the exponent is a linear

737
00:51:29,660 --> 00:51:33,220
so in this case of these set just are

738
00:51:33,270 --> 00:51:34,830
of these measures

739
00:51:34,850 --> 00:51:39,120
you know for convenience i to one of the two pi i could just make

740
00:51:39,120 --> 00:51:42,020
it uniform constant still matter

741
00:51:42,040 --> 00:51:45,520
and i need to pick two features not this is not sufficient to just one

742
00:51:45,520 --> 00:51:47,810
features i pick x and x squared

743
00:51:47,850 --> 00:51:50,830
and not just from this i can read of what the natural parameter is going

744
00:51:50,830 --> 00:51:52,470
to be

745
00:51:52,470 --> 00:51:56,060
and once again we can compute the log partition function

746
00:51:56,080 --> 00:51:58,770
and in this case the natural parameter space

747
00:51:58,770 --> 00:52:01,200
these are times

748
00:52:02,310 --> 00:52:06,350
so so fine so there are a lot of these distributions that are clearly important

749
00:52:06,350 --> 00:52:11,950
standard distributions and we found these can be coerced into the exponential family form

750
00:52:11,980 --> 00:52:16,330
right but what is it by us to actually deal with this formalism

751
00:52:16,370 --> 00:52:17,980
you know is it the case that

752
00:52:18,140 --> 00:52:21,870
when we first look to the exponential family thing it looked like it was really

753
00:52:21,870 --> 00:52:25,560
specific it's you know what kind of distributions could possibly have that form

754
00:52:25,580 --> 00:52:27,720
now it seems like it might be too general

755
00:52:27,720 --> 00:52:33,190
you know we effortlessly express the bernoulli only the warsaw the gaussians exponential family this

756
00:52:33,190 --> 00:52:34,810
models right

757
00:52:34,850 --> 00:52:39,540
it is so general that it's not useful for us in the world wide highway

758
00:52:39,560 --> 00:52:45,060
what's so good about this exponential family what's special about exponential family distributions

759
00:52:45,120 --> 00:52:49,520
and it turns out that a lot of the special properties of this distribution

760
00:52:49,520 --> 00:52:55,270
lie where one initially were not expected to live namely in the log partition function

761
00:52:55,350 --> 00:52:58,220
OK so the weight is log partition function came about is that it was just

762
00:52:58,220 --> 00:53:00,040
the normalizer

763
00:53:00,040 --> 00:53:02,750
it was just the thing you can attack on in front to make it all

764
00:53:02,750 --> 00:53:04,370
add up to one

765
00:53:04,410 --> 00:53:06,120
however it turns out

766
00:53:06,220 --> 00:53:12,060
the this normalized actually contains a wealth of information about the distribution

767
00:53:12,080 --> 00:53:15,660
OK and we're going to use its properties quite heavily

768
00:53:15,660 --> 00:53:18,890
when we when we get back to maximum entropy

769
00:53:18,970 --> 00:53:21,680
is the first in first of all it turns out that this thing is strictly

770
00:53:22,850 --> 00:53:24,080
so for instance

771
00:53:24,080 --> 00:53:25,220
the force on

772
00:53:25,240 --> 00:53:29,060
in the fossil case it was e to the data

773
00:53:29,060 --> 00:53:30,720
the exponent which is

774
00:53:30,770 --> 00:53:34,120
it was like this is a strictly convex function

775
00:53:35,540 --> 00:53:38,850
among other things it means that is derivative is one to one if you have

776
00:53:38,850 --> 00:53:43,600
a strictly convex function and its derivative with the derivative is increasing its one to

777
00:53:44,290 --> 00:53:46,080
to one to one

778
00:53:46,100 --> 00:53:48,810
the derivative actually has all the properties

779
00:53:48,830 --> 00:53:52,890
the derivative turns out to simply be the mean of the distribution

780
00:53:53,290 --> 00:53:58,330
so if you take the derivative of the log partition function

781
00:53:58,370 --> 00:54:02,560
the log partition function just compute first derivative and and that turns out to be

782
00:54:02,560 --> 00:54:04,370
the mean of the distribution

783
00:54:04,370 --> 00:54:10,060
the expected value on the random draws from the distribution of of the feature vector

784
00:54:10,060 --> 00:54:14,750
OK so so why is this the case well let's in general the feature vector

785
00:54:14,750 --> 00:54:17,700
has many components it it to one o to two and so on

786
00:54:17,750 --> 00:54:20,680
let's take the derivative with respect to the i th component

787
00:54:20,700 --> 00:54:25,200
you take the derivative of this with respect to data with respect to it i

788
00:54:25,220 --> 00:54:30,390
well the derivative log x is just one over XTX so it's so one over

789
00:54:30,390 --> 00:54:31,290
this thing

790
00:54:31,310 --> 00:54:34,240
that the derivative of the top which is this

791
00:54:35,250 --> 00:54:38,980
and i can see that this is just the probability

792
00:54:40,120 --> 00:54:41,410
so it's just me

793
00:54:44,330 --> 00:54:46,830
of that particular feature

794
00:54:46,850 --> 00:54:50,910
again this and if you do the similar things for the very all for the

795
00:54:50,910 --> 00:54:52,330
second derivative

796
00:54:52,330 --> 00:54:54,410
you get the variance of the distribution

797
00:54:54,450 --> 00:54:59,290
OK so just by taking the first derivative second derivative of the log partition function

798
00:54:59,310 --> 00:55:02,250
so we end up getting the moments of the distribution

799
00:55:02,290 --> 00:55:04,100
so there's a wealth of information

800
00:55:04,120 --> 00:55:06,600
contained in this log partition function

801
00:55:08,930 --> 00:55:11,100
so so what else wall

802
00:55:11,120 --> 00:55:14,310
in one of the ways the exponential family me

803
00:55:14,330 --> 00:55:15,270
what is

804
00:55:15,270 --> 00:55:18,720
what is special about why we singled out these distributions

805
00:55:18,750 --> 00:55:23,830
as being important once you know well what else is unique about them

806
00:55:23,830 --> 00:55:29,620
so the real the real benefit comes when you start looking at maximum likelihood estimation

807
00:55:29,640 --> 00:55:34,580
suppose dealing with some exponential family OK and as we've seen to specify an exponential

808
00:55:34,580 --> 00:55:38,370
family just specify the base measure and the features are going to look at the

809
00:55:38,370 --> 00:55:42,220
support for dealing with some exponential family and now we see a bunch of data

810
00:55:42,240 --> 00:55:45,650
and we want to find the best member of the family because of privacy budget

811
00:55:45,650 --> 00:55:48,910
points and we wanted go out into the

812
00:55:48,970 --> 00:55:53,180
so let's find the maximum likelihood choice of the parameter

813
00:55:53,200 --> 00:55:57,700
OK so you just plug into this we want to maximize the log likelihood

814
00:55:57,700 --> 00:56:03,450
so take the logo of this it's it density x i minus two events

815
00:56:03,450 --> 00:56:05,850
last longer side

816
00:56:05,870 --> 00:56:10,020
you're invited because you can pull the it out

817
00:56:10,080 --> 00:56:12,270
can get some of these two guys

818
00:56:12,270 --> 00:56:14,450
and these this is just a constant

819
00:56:14,470 --> 00:56:17,370
and they get the sum of the log book size

820
00:56:17,370 --> 00:56:20,560
and to maximize this is the derivative and you take the derivative is that it

821
00:56:20,560 --> 00:56:21,700
is zero

822
00:56:21,700 --> 00:56:23,300
and many of them

823
00:56:23,310 --> 00:56:25,320
so there a move away from things

824
00:56:25,450 --> 00:56:27,630
context today's

825
00:56:27,660 --> 00:56:31,920
this was part of literature on and structural pattern recognition probably maybe problems you have

826
00:56:31,920 --> 00:56:33,800
not thought of before

827
00:56:33,810 --> 00:56:37,820
that really needs

828
00:56:38,640 --> 00:56:40,820
addressing one cor

829
00:56:41,880 --> 00:56:45,390
on the core problem is correspondence

830
00:56:45,410 --> 00:56:47,580
correspondences above

831
00:56:47,590 --> 00:56:50,910
in in learning and inference

832
00:56:50,980 --> 00:56:56,440
and what's happened in the last decade also is that been free approaches free new

833
00:56:56,440 --> 00:57:00,840
approaches to solving the problem of correspondence that they are interesting

834
00:57:00,850 --> 00:57:03,350
and at some way optimal

835
00:57:03,360 --> 00:57:05,080
this is what is optimal

836
00:57:05,160 --> 00:57:07,380
and i'm going to talk about the three men

837
00:57:07,400 --> 00:57:09,360
the spectral methods

838
00:57:09,370 --> 00:57:14,090
the least squares methods in the bayesian network methods

839
00:57:14,120 --> 00:57:16,620
so you got the there's some

840
00:57:16,650 --> 00:57:20,070
some sense of smell hearing is in this formulation

841
00:57:20,070 --> 00:57:28,540
china japan and i've been working on a on bayesian networks methods internet

842
00:57:28,560 --> 00:57:32,770
it was kelly is a work on spectral methods so

843
00:57:32,790 --> 00:57:37,990
i'll get my aim today is to give some flavour of these of these methods

844
00:57:38,010 --> 00:57:40,550
i don't have to get to the bayesian stuff all the

845
00:57:41,300 --> 00:57:44,350
but will see how we can

846
00:57:44,360 --> 00:57:47,010
so spectral methods right

847
00:57:47,020 --> 00:57:48,860
the core problem again just now

848
00:57:48,880 --> 00:57:50,160
of all my

849
00:57:50,170 --> 00:57:57,950
this talk is how do i correspond end points two points

850
00:57:57,960 --> 00:57:59,540
well points

851
00:58:00,580 --> 00:58:03,760
our vertices of the graph

852
00:58:03,770 --> 00:58:05,130
so it's classical

853
00:58:05,160 --> 00:58:07,550
subgraph matching

854
00:58:12,240 --> 00:58:13,910
in spectral methods

855
00:58:13,930 --> 00:58:16,520
and the least squares method

856
00:58:16,520 --> 00:58:18,440
the most common

857
00:58:18,470 --> 00:58:23,210
why need to encode a relational structure

858
00:58:23,220 --> 00:58:24,820
is in terms

859
00:58:24,830 --> 00:58:27,740
although matrix

860
00:58:27,770 --> 00:58:31,870
for a graph with n vertices

861
00:58:32,520 --> 00:58:38,830
their attributes for a vertex and

862
00:58:38,880 --> 00:58:41,800
attributes for a change

863
00:58:42,620 --> 00:58:44,470
well matrix

864
00:58:44,490 --> 00:58:46,960
well this example

865
00:58:46,980 --> 00:58:48,020
well got

866
00:58:48,970 --> 00:58:52,940
all way i through

867
00:58:52,990 --> 00:58:54,850
and i was injective

868
00:58:55,110 --> 00:58:57,730
you might be covered

869
00:58:57,800 --> 00:59:02,840
in my opinion the human values i hold to the

870
00:59:02,890 --> 00:59:04,510
the next one might be the

871
00:59:04,520 --> 00:59:06,180
the next year

872
00:59:06,210 --> 00:59:09,550
because community values properties of vertex

873
00:59:09,550 --> 00:59:14,280
i might be the saturation value will not be the

874
00:59:14,290 --> 00:59:15,360
and so on

875
00:59:16,430 --> 00:59:21,810
submatrices down to build the binary values like the distances between the vertices in the

876
00:59:21,810 --> 00:59:23,850
angles of something like this

877
00:59:23,850 --> 00:59:24,900
so in thing

878
00:59:24,920 --> 00:59:27,020
this may be true

879
00:59:27,020 --> 00:59:28,110
in the

880
00:59:28,120 --> 00:59:30,510
but in

881
00:59:30,540 --> 00:59:32,630
the matrix

882
00:59:32,660 --> 00:59:33,820
and this

883
00:59:33,840 --> 00:59:36,160
while encouraging

884
00:59:36,180 --> 00:59:38,370
and attributed

885
00:59:39,640 --> 00:59:41,440
as the matrix

886
00:59:41,460 --> 00:59:49,790
with guy and one correspondence the from a submatrices submatrix corresponding to diagnose playing some

887
00:59:49,790 --> 00:59:50,970
matrix for each

888
00:59:51,120 --> 00:59:52,880
attribute of the vertex

889
00:59:52,890 --> 00:59:58,740
and they are diagonal matrices being able to the attributes for each to the edges

890
00:59:58,760 --> 01:00:00,030
now to

891
01:00:00,260 --> 01:00:02,420
what's being done in the literature

892
01:00:02,450 --> 01:00:04,220
this has been

893
01:00:04,420 --> 01:00:06,720
this really not being considered

894
01:00:06,730 --> 01:00:14,040
and i talked about today about about the degenerate case the matrix is simply

895
01:00:14,360 --> 01:00:17,770
the adjacency matrix which is a very general form

896
01:00:17,790 --> 01:00:23,210
do not on the density matrix would be this is the most simple form would

897
01:00:23,210 --> 01:00:27,720
so this is not a property of tablet PC and all of this or powerpoint

898
01:00:27,720 --> 01:00:30,020
is a function of my laptop which is

899
01:00:30,060 --> 01:00:32,550
become very unstable

900
01:00:32,560 --> 01:00:35,780
in the last twenty four hours

901
01:00:39,770 --> 01:00:42,460
i'm not saying it's not follow the software just saying that it doesn't usually do

902
01:00:43,860 --> 01:00:52,310
try one more time

903
01:01:05,320 --> 01:01:13,670
he said you get lots of practice of this

904
01:01:13,680 --> 01:01:17,530
i wasn't quite what i had in mind

905
01:01:17,640 --> 01:01:20,030
so imagine that we

906
01:01:24,690 --> 01:01:27,200
he has given x

907
01:01:27,250 --> 01:01:30,980
supposing we want distribution to have the property

908
01:01:31,000 --> 01:01:31,920
the the

909
01:01:31,930 --> 01:01:34,040
the distribution is that given x and y

910
01:01:34,070 --> 01:01:36,160
let's say doesn't depend on y

911
01:01:36,230 --> 01:01:40,360
as an example of the conditional independence property and will be looking at those in

912
01:01:40,360 --> 01:01:44,510
detail later what effectively we're going to do that is to cross at that link

913
01:01:45,880 --> 01:01:49,480
we write down the distribution of z given its parents it will only have the

914
01:01:49,480 --> 01:01:51,690
parent x and so why

915
01:01:51,690 --> 01:01:52,310
i will

916
01:01:52,340 --> 01:01:55,270
not appear in that conditioning statements

917
01:01:55,270 --> 01:01:56,880
so that's essentially the idea

918
01:01:56,910 --> 01:01:59,810
so think

919
01:01:59,840 --> 01:02:00,680
this is

920
01:02:00,710 --> 01:02:07,520
the point i think it seems to me is the pen

921
01:02:07,540 --> 01:02:12,440
well i have to resort to have project john shawe taylor probably

922
01:02:12,450 --> 01:02:15,260
happy buying something

923
01:02:18,730 --> 01:02:24,100
the joint distribution that is the product of all these conditionals

924
01:02:24,160 --> 01:02:28,250
i'm going to do this and to do this

925
01:02:28,290 --> 01:02:31,680
is john here

926
01:02:33,620 --> 01:02:36,510
can a hundred and twenty people be sworn to secrecy

927
01:02:56,530 --> 01:03:01,270
as everybody got a copy of the and that the body

928
01:03:02,880 --> 01:03:05,680
so in right and the joint distribution

929
01:03:06,640 --> 01:03:10,690
x one x seven

930
01:03:10,750 --> 01:03:18,410
so this particular examples would you go through this example sort of laboriously you get

931
01:03:18,420 --> 01:03:19,480
the point

932
01:03:19,510 --> 01:03:24,690
so this is a joint distribution for the project was a joint distribution over seven

933
01:03:24,690 --> 01:03:28,600
variables so there's one node for each of the variables and

934
01:03:28,630 --> 01:03:32,020
there is there are a bunch of links but you'll see there are lots of

935
01:03:32,020 --> 01:03:35,610
missing links in this graph not every pair of nodes has the link between them

936
01:03:36,200 --> 01:03:41,520
and those missing links in code for certain properties of the joint distribution

937
01:03:41,520 --> 01:03:44,740
so in this but for that particular graph i simply write down the i go

938
01:03:44,740 --> 01:03:47,260
through each variable in turn halifax

939
01:03:47,260 --> 01:03:49,200
it doesn't matter what i take them in

940
01:03:49,220 --> 01:03:51,470
which is the distribution of the

941
01:03:51,600 --> 01:03:57,100
variable conditional on its parents well it's one doesn't have any parents are just one

942
01:03:57,130 --> 01:03:59,970
and x two

943
01:04:00,010 --> 01:04:04,920
it doesn't have any parents x three doesn't have any parents

944
01:04:04,930 --> 01:04:07,660
x four

945
01:04:07,680 --> 01:04:13,260
as parents x one x two x three

946
01:04:13,340 --> 01:04:17,790
x five getting point by now you smart people

947
01:04:17,810 --> 01:04:22,520
x one and x three

948
01:04:22,520 --> 01:04:26,680
six is conditioned on all

949
01:04:26,690 --> 01:04:28,740
and then take seven

950
01:04:28,790 --> 01:04:37,950
so when we look into their look like thing x four x five

951
01:04:38,020 --> 01:04:41,230
OK so that's a particular example

952
01:04:46,230 --> 01:04:49,320
so i think this point then you should be clear about the definition of the

953
01:04:49,320 --> 01:04:53,970
joint distribution for for directed graphs so directed graph

954
01:04:54,780 --> 01:04:58,040
this expression

955
01:04:58,050 --> 01:05:03,820
for the factorisation of the joint probability distribution

956
01:05:03,830 --> 01:05:09,110
and so the factorisation in terms of factors which are conditional

957
01:05:09,380 --> 01:05:13,000
conditional probabilities and there are lots and lots of models that you

958
01:05:13,010 --> 01:05:15,000
maybe we should have heard of

959
01:05:15,010 --> 01:05:18,540
which can be expressed as directed graphs i just put a list up here to

960
01:05:18,540 --> 01:05:20,850
show you that there actually quite widespread

961
01:05:20,880 --> 01:05:24,590
and we'll see some examples of these in detail as we go through the through

962
01:05:24,590 --> 01:05:26,520
the tutorial

963
01:05:26,530 --> 01:05:33,580
so that's directed graphs going to show you know the other

964
01:05:33,620 --> 01:05:38,320
very popular class of graphical model the undirected graph but i should say that

965
01:05:38,330 --> 01:05:39,780
these two

966
01:05:39,800 --> 01:05:42,780
classes of graph and not the only possibility the other

967
01:05:42,790 --> 01:05:48,410
families of graph search chain graphs in factor graphs and you could invent others the

968
01:05:48,410 --> 01:05:49,610
point is that

969
01:05:51,190 --> 01:05:55,280
the family of graphs of has to represent a consistent mathematical framework and it's very

970
01:05:55,280 --> 01:05:56,540
really telling me

971
01:05:56,590 --> 01:05:58,540
was that the world is flat

972
01:05:58,550 --> 01:06:00,810
and i wrote that down in my notebook

973
01:06:00,910 --> 01:06:02,320
the world is

974
01:06:02,370 --> 01:06:05,240
i go back to my hotel i ran to my room i called my wife

975
01:06:05,240 --> 01:06:06,920
and the first i said honey

976
01:06:06,940 --> 01:06:10,740
i'm going to write a book called the world is flat

977
01:06:10,750 --> 01:06:15,920
she now says she thought that was a brilliant idea

978
01:06:15,920 --> 01:06:20,500
that's not how i remember the conversation but the men are from mars women are

979
01:06:20,500 --> 01:06:25,070
from venus but i that i don't come home and basically call my owners of

980
01:06:25,070 --> 01:06:29,140
the new york times and say please i need to go on sabbatical immediately

981
01:06:29,170 --> 01:06:33,330
i need to go on sabbatical because my software the intellectual framework to which i

982
01:06:33,330 --> 01:06:34,450
look at the world

983
01:06:34,500 --> 01:06:35,800
is out of date

984
01:06:35,810 --> 01:06:39,670
i'm i'm a basic engineer and it's the job world

985
01:06:39,670 --> 01:06:44,160
and if i don't go going leave immediately and update my software

986
01:06:44,200 --> 01:06:47,440
i'm going to write something really stupid

987
01:06:47,490 --> 01:06:50,070
in the new york times

988
01:06:50,090 --> 01:06:52,500
to great way to get leave i have to say

989
01:06:52,540 --> 01:06:57,800
so i started this book in march of two thousand four i turned in in

990
01:06:57,800 --> 01:07:03,320
december do not try this trick on kids i blew my forearms along the way

991
01:07:03,320 --> 01:07:07,290
but in the field of energy and passion produced kind of the core thesis of

992
01:07:07,290 --> 01:07:08,330
this book

993
01:07:08,340 --> 01:07:11,350
and the course is to sort of the meta argument of this book is very

994
01:07:11,350 --> 01:07:15,270
simple the meta arguments that there have been three what call three great era of

995
01:07:15,270 --> 01:07:20,780
globalisation the first cycle globalization one point o began fourteen ninety two and lasted until

996
01:07:20,780 --> 01:07:26,240
the early eighteen hundreds with the beginning of global arbitrage that globalization shrunk the world

997
01:07:26,240 --> 01:07:28,490
from size larger size medium

998
01:07:28,500 --> 01:07:33,880
and then you globalization was really spearheaded by countries globalizing there is you went global

999
01:07:33,880 --> 01:07:41,090
through your country nation-states were the dynamic agent of globalization so was spain exploring america's

1000
01:07:41,090 --> 01:07:45,190
britain colonizing india portugal east asia you went global

1001
01:07:45,210 --> 01:07:46,790
through your country

1002
01:07:46,800 --> 01:07:50,190
globalization two point o began in the early eighteen hundreds last till the year two

1003
01:07:50,190 --> 01:07:52,270
thousand three just and

1004
01:07:52,310 --> 01:07:56,300
it's around the world from size medium to size small in that year globalization was

1005
01:07:56,300 --> 01:08:01,760
really spearheaded by companies companies going global for markets and for labour

1006
01:08:01,770 --> 01:08:04,410
UN global through your company

1007
01:08:04,520 --> 01:08:07,520
while you were sleeping or this while i was sleeping

1008
01:08:07,650 --> 01:08:11,490
we actually entered globalization three point from the year two thousand to the present the

1009
01:08:11,490 --> 01:08:13,930
trick in the world from size small size

1010
01:08:13,950 --> 01:08:14,710
i mean

1011
01:08:14,720 --> 01:08:18,680
and flattening the global economic playing field at the same time on there was really

1012
01:08:18,680 --> 01:08:20,820
new really different really exciting

1013
01:08:20,840 --> 01:08:22,600
and really terrified

1014
01:08:22,620 --> 01:08:24,530
is it this year globalization

1015
01:08:24,540 --> 01:08:30,770
it's certainly not spearheaded by countries and it's not exclusively headed by companies anymore

1016
01:08:30,790 --> 01:08:33,880
it's really new really different really exciting

1017
01:08:33,890 --> 01:08:36,050
is it this area of globalization

1018
01:08:36,070 --> 01:08:37,330
is spearheaded by

1019
01:08:39,050 --> 01:08:43,210
we've gone from globalization built around countries to one built around companies

1020
01:08:43,270 --> 01:08:44,760
to one built around

1021
01:08:47,630 --> 01:08:49,410
how did we get here

1022
01:08:49,460 --> 01:08:51,160
how did we move

1023
01:08:51,170 --> 01:08:52,770
down this progression

1024
01:08:52,780 --> 01:08:54,920
two point where individuals

1025
01:08:54,990 --> 01:08:57,410
can compete connect and collaborate

1026
01:08:58,560 --> 01:09:00,790
as individuals

1027
01:09:00,790 --> 01:09:04,810
we had to create a platform really for that to happen and i talk about

1028
01:09:04,830 --> 01:09:10,030
the whole platform article the ten flatters me talk about four forty one is basically

1029
01:09:10,030 --> 01:09:15,440
the relevant both open course ware anything to where we are today

1030
01:09:15,460 --> 01:09:18,220
basically the first of these forty flatters was

1031
01:09:18,240 --> 01:09:22,640
i'm to do this really quickly the PC what the personal computer allowed in this

1032
01:09:22,640 --> 01:09:27,950
is really the beginning of the whole individual nature of globalization the PC allowed was

1033
01:09:27,950 --> 01:09:29,720
for individuals

1034
01:09:29,750 --> 01:09:34,700
individuals to offer their own content in digital form

1035
01:09:34,750 --> 01:09:37,320
it's great to be able to talk about this at MIT right not explain what

1036
01:09:37,320 --> 01:09:39,830
those things mean

1037
01:09:39,890 --> 01:09:43,920
what PC allowed was for the first time in the history of the world

1038
01:09:43,960 --> 01:09:48,710
four individuals to offer their own content in digital form now

1039
01:09:48,780 --> 01:09:53,760
individuals have offered their own content ever since k women caveman etched on cave walls

1040
01:09:53,850 --> 01:09:55,090
with the PC

1041
01:09:55,150 --> 01:10:01,250
individuals can offer their own content words photo data spreadsheets video music

1042
01:10:01,260 --> 01:10:06,120
in the form of bits and bytes in digital form and once you're content

1043
01:10:06,130 --> 01:10:11,090
it was created in digital form it could be manipulated in so many more ways

1044
01:10:11,140 --> 01:10:12,650
by individuals

1045
01:10:12,660 --> 01:10:15,120
and dispatch so many more places

1046
01:10:15,280 --> 01:10:19,410
it was the first basic basically second flatten or some of these technologies others are

1047
01:10:19,410 --> 01:10:24,400
dates second flight nurse today eight nine ninety five august nine nineteen ninety five which

1048
01:10:24,400 --> 01:10:25,930
i consider to be

1049
01:10:25,930 --> 01:10:30,150
for the whole game to work we need encoding system to add

1050
01:10:31,570 --> 01:10:34,350
this is an informal thing i'm saying here

1051
01:10:37,490 --> 01:10:38,690
to the transmission

1052
01:10:38,700 --> 01:10:43,610
that redundancy which the decoder will know about it will know the system that's being

1053
01:10:43,610 --> 01:10:50,050
used to add redundancy that redundancy has been to help the decoder to infer

1054
01:10:50,060 --> 01:10:53,210
but the source signal

1055
01:10:55,530 --> 01:10:57,380
it is critical

1056
01:10:57,400 --> 01:10:58,310
that's the plan

1057
01:10:58,320 --> 01:11:03,340
it's going to exploit the non redundancy adding system to do that inference and i

1058
01:11:03,340 --> 01:11:06,940
say inferred in the sense of well we may want to use bayes theorem when

1059
01:11:06,940 --> 01:11:10,070
the time comes to the some decoding

1060
01:11:11,590 --> 01:11:14,360
so let's have an example

1061
01:11:14,480 --> 01:11:17,260
to discuss this small

1062
01:11:17,280 --> 01:11:22,790
how encoders and decoders work give ourselves an example channel

1063
01:11:22,810 --> 01:11:26,560
you need more light on this board

1064
01:11:39,850 --> 01:11:43,020
so the models and we're going to discuss

1065
01:11:43,020 --> 01:11:44,280
for the rest of today

1066
01:11:44,310 --> 01:11:48,980
so this is just the model one information theory covers lots of lots channels this

1067
01:11:48,980 --> 01:11:50,280
is the favourite model

1068
01:11:50,350 --> 01:11:54,440
simple channel is called the binary symmetric channel

1069
01:11:54,450 --> 01:12:02,250
and it looks like this is going input and it's own output

1070
01:12:02,260 --> 01:12:06,880
and every time cycle every quadratic you get put one in

1071
01:12:06,890 --> 01:12:10,900
and immediately and output comes out of it and the input you can put in

1072
01:12:10,900 --> 01:12:13,780
is a binary variable is either zero or one

1073
01:12:13,810 --> 01:12:18,660
and the output comes out will be zero or one and you hoping to zero

1074
01:12:18,950 --> 01:12:22,770
zero one to one but sometimes you're unlucky and you get what we call the

1075
01:12:23,580 --> 01:12:25,970
the bit split

1076
01:12:26,030 --> 01:12:28,340
the input of course say

1077
01:12:29,180 --> 01:12:30,590
the output y

1078
01:12:30,600 --> 01:12:37,900
and probability one minus from these two arose and probably on this arrow property

1079
01:12:37,950 --> 01:12:40,090
on the side of this little diagram here

1080
01:12:41,090 --> 01:12:46,280
shorthand for a set of false statements of the form the probability that y equals

1081
01:12:46,280 --> 01:12:49,930
one given that it's zero is

1082
01:12:50,040 --> 01:12:54,330
that's the meaning of this arrow with anathema this isn't the graphical

1083
01:12:55,340 --> 01:13:00,050
this would be a graph model

1084
01:13:00,080 --> 01:13:02,610
this is a diagram showing you the probabilities

1085
01:13:02,630 --> 01:13:04,410
all right

1086
01:13:04,460 --> 01:13:08,240
OK so this for such statements and the one i've written down is one for

1087
01:13:08,250 --> 01:13:11,190
this error

1088
01:13:15,480 --> 01:13:19,410
you can think of this as the model for this drive let's imagine

1089
01:13:19,490 --> 01:13:23,180
that with your certificate of

1090
01:13:23,180 --> 01:13:26,770
engineering competence in your pocket you decide let's go and make a start up company

1091
01:13:27,410 --> 01:13:31,490
selling this rising got a wonderful new nano

1092
01:13:31,510 --> 01:13:37,170
super-low temperature way of making this rise and we went prototype and it is

1093
01:13:37,180 --> 01:13:40,680
and the problem is it flips ten percent of bits that you store on the

1094
01:13:43,650 --> 01:13:45,410
here's a question for you

1095
01:13:45,430 --> 01:13:49,600
imagine that this model is an accurate model

1096
01:13:52,060 --> 01:13:58,010
new describe you've you've made you've got this described unfortunately flextime to bits and imagine

1097
01:13:58,010 --> 01:14:01,840
that you sell it anyway you know yes yes snake-oil salespeople to help you with

1098
01:14:01,850 --> 01:14:05,020
the pitch to the VC's and so forth stop producing these

1099
01:14:05,420 --> 01:14:09,510
and you say i want to you first customer and the first customer tries to

1100
01:14:10,780 --> 01:14:12,960
ten thousand bit file

1101
01:14:12,970 --> 01:14:17,470
on this day strife and then read it off again how many bits will be

1102
01:14:17,470 --> 01:14:21,060
flipped is the the first question

1103
01:14:21,080 --> 01:14:22,810
so if you store

1104
01:14:22,840 --> 01:14:24,130
ten thousand bits

1105
01:14:24,140 --> 01:14:27,440
i read off again roughly how many bits are going to be for i want

1106
01:14:27,440 --> 01:14:32,160
to rob and so complete with a robust police habitat your neighbor

1107
01:15:50,280 --> 01:15:52,450
OK roughly how many bits

1108
01:15:53,680 --> 01:15:56,630
philip chan yuan

1109
01:15:58,870 --> 01:16:03,770
a thousand plus and minus

1110
01:16:03,790 --> 01:16:09,620
one thousand plus minus thirty any other uncertain apart from one thousand plus minus thirty

1111
01:16:10,870 --> 01:16:12,860
plus and minus sixty

1112
01:16:13,740 --> 01:16:15,840
why saying sixty

1113
01:16:15,880 --> 01:16:19,180
two standard deviations why saying thirty

1114
01:16:21,040 --> 01:16:25,720
but this just conventions right ground

1115
01:16:25,750 --> 01:16:29,880
excellent so you all know the binomial distribution and one of the really wonderful things

1116
01:16:29,880 --> 01:16:34,130
about information theory is you only need to know the binomial distribution to do pretty

1117
01:16:34,130 --> 01:16:36,270
much the whole topic

1118
01:16:38,460 --> 01:16:43,980
question one a with your user be happy with that number of bits being flipped

1119
01:16:43,980 --> 01:16:47,670
never i imagine not OK

1120
01:16:47,720 --> 01:16:49,230
well how

1121
01:16:49,260 --> 01:16:54,790
many bits might they be happy with in a typical file

1122
01:16:54,870 --> 01:16:57,950
zero OK so

1123
01:16:57,960 --> 01:17:04,190
now let's imagine that by making either a new physical channel all by adding encoding

1124
01:17:04,190 --> 01:17:09,550
and decoding systems we managed to create a new channel that is a binary symmetric

1125
01:17:09,550 --> 01:17:13,360
channel with a much lower for probability how small the need to be if we're

1126
01:17:13,360 --> 01:17:17,840
going to get what he wants which is zero errors so his question to to

1127
01:17:17,840 --> 01:17:22,240
make successful business selling one about describes how small

1128
01:17:22,260 --> 01:17:24,190
if you actually want to flip probability

1129
01:17:24,220 --> 01:17:30,810
to be about punishment channel habitat your neighbour

1130
01:17:56,940 --> 01:18:40,140
you're not chatting to each other you done

1131
01:18:40,150 --> 01:18:43,120
you have an answer what that

1132
01:18:43,170 --> 01:18:44,550
what's that

1133
01:18:44,660 --> 01:18:46,900
what's that

1134
01:18:49,240 --> 01:19:07,340
we like there i want to another and so not zero talk to each other

1135
01:19:16,020 --> 01:19:18,200
to each other

1136
01:19:32,450 --> 01:19:34,620
OK let's get some answers

1137
01:19:34,620 --> 01:19:39,340
if you want to go

1138
01:19:39,350 --> 01:19:42,540
not long

1139
01:20:01,710 --> 01:20:05,390
part of

1140
01:20:14,650 --> 01:20:19,210
you can

1141
01:20:29,270 --> 01:20:36,150
you know

1142
01:21:10,720 --> 01:21:17,310
the old people

1143
01:23:10,910 --> 01:23:13,220
one two three

1144
01:23:31,220 --> 01:23:34,090
during the

1145
01:23:34,090 --> 01:23:35,260
you might

1146
01:24:16,240 --> 01:24:22,850
and is well

1147
01:24:43,190 --> 01:24:51,390
and you

1148
01:25:05,070 --> 01:25:08,280
you might

1149
01:25:40,250 --> 01:25:43,390
in my

1150
01:25:56,950 --> 01:26:06,130
you know

1151
01:26:06,130 --> 01:26:07,080
OK so

1152
01:26:07,110 --> 01:26:08,750
what i want to do now

1153
01:26:08,810 --> 01:26:10,970
is show you

1154
01:26:11,020 --> 01:26:16,810
the reverse to summarise what was sounds summarises whenever you have a derivation

1155
01:26:18,100 --> 01:26:21,380
then phi gamma had i do and i said well if if you tell me

1156
01:26:21,620 --> 01:26:24,610
the derivation will look at its shape if

1157
01:26:27,220 --> 01:26:30,640
a one step derivations so its size in gamma

1158
01:26:30,650 --> 01:26:35,280
then we know its ideological and if it's an instance of an axiom one step

1159
01:26:35,290 --> 01:26:38,950
the relation with this is an instance of an axiom then again a consequence of

1160
01:26:38,950 --> 01:26:43,310
gamma because the axiom schemes are such that they all all instances of them are

1161
01:26:43,320 --> 01:26:45,970
valid points of the empty set

1162
01:26:49,710 --> 01:26:55,700
and then the only tricky bit will modus ponens necessitation induction hypothesis so

1163
01:26:56,920 --> 01:27:00,240
it is shoulder-to-shoulder derivation shown

1164
01:27:01,620 --> 01:27:04,510
change the turnstile to the other turnstile

1165
01:27:04,530 --> 01:27:09,350
and now we have to do is this is the property of logical consequence

1166
01:27:10,640 --> 01:27:13,560
but we have to prove and if you look through your notes you'll see that

1167
01:27:13,560 --> 01:27:15,130
somewhere if this

1168
01:27:16,630 --> 01:27:17,820
and if this

1169
01:27:17,830 --> 01:27:19,200
in this

1170
01:27:19,220 --> 01:27:21,480
they should be able to check this in your

1171
01:27:24,970 --> 01:27:26,730
what about consciousness

1172
01:27:26,780 --> 01:27:29,340
i want to show complete winamp

1173
01:27:30,720 --> 01:27:33,240
five is a logical consequence of gamma

1174
01:27:33,250 --> 01:27:35,740
then there's the deduction

1175
01:27:37,290 --> 01:27:42,790
OK says that calculus is complete all of the things that should be deduced

1176
01:27:44,010 --> 01:27:49,480
so we do the standard lindenbaum construction using maximal consistent sets it up that i've

1177
01:27:49,480 --> 01:27:53,550
got might differ from the one that john's lane used in these lectures but

1178
01:27:54,690 --> 01:27:56,110
the flavour is

1179
01:27:57,400 --> 01:27:58,690
what's the lemma

1180
01:27:58,700 --> 01:28:02,450
you give me enough consistent set gamma

1181
01:28:02,460 --> 01:28:07,390
the main consistent means i can't deduce phi and not five from

1182
01:28:09,460 --> 01:28:14,640
i am not five from for any five that's consistency me every consistent set gamma

1183
01:28:14,640 --> 01:28:16,190
is icts instances

1184
01:28:20,960 --> 01:28:21,850
greater than

1185
01:28:25,410 --> 01:28:26,680
joe kidd

1186
01:28:29,290 --> 01:28:30,340
and how

1187
01:28:31,840 --> 01:28:35,250
so i'm going to do it not directly

1188
01:28:35,260 --> 01:28:38,630
so the direct way would be to say if i is a logical consequence of

1189
01:28:41,140 --> 01:28:44,640
and now show that there is a reduction of five and if i tell you

1190
01:28:44,640 --> 01:28:48,030
that five a logical consequence of gamma what do i tell you

1191
01:28:49,370 --> 01:28:52,710
i give you anything to work with all i tell you is that whenever you

1192
01:28:52,710 --> 01:28:55,960
have any model like which makes demetri everywhere

1193
01:28:57,320 --> 01:29:00,080
but if i say OK choose the model model

1194
01:29:01,430 --> 01:29:05,910
where do i start a moral might have fifteen states the two states

1195
01:29:07,260 --> 01:29:11,550
OK so i have nothing concrete to work with whereas in the other direction in

1196
01:29:11,550 --> 01:29:16,620
the derivation had structure a tree structure i could do induction on the tree structure

1197
01:29:18,150 --> 01:29:22,170
it is we use classical meta reasoning and we do the contrapositive so what we

1198
01:29:22,170 --> 01:29:26,590
say is if you want to show whenever this holds in this whole

1199
01:29:29,320 --> 01:29:31,380
to improve the contrapositive

1200
01:29:31,470 --> 01:29:32,490
that with

1201
01:29:33,930 --> 01:29:37,270
this doesn't hold that's what i'm saying up on the slide

1202
01:29:37,320 --> 01:29:40,060
so if there is no did

1203
01:29:41,400 --> 01:29:45,270
then they then phi cannot be a logical consequence of gamma that's it i'm going

1204
01:29:45,270 --> 01:29:49,360
to show you how you do it and assume it's an if statement that you

1205
01:29:49,380 --> 01:29:54,600
something then something and could canonical model construction

1206
01:29:57,440 --> 01:30:00,590
OK i'm going to assume that gamma

1207
01:30:02,040 --> 01:30:04,840
and now i'm going to construct a model

1208
01:30:04,850 --> 01:30:07,070
called the canonical correlates

1209
01:30:09,080 --> 01:30:10,740
is the set of worlds

1210
01:30:10,760 --> 01:30:13,910
reachability relation are added value you

1211
01:30:17,750 --> 01:30:22,400
is not forced by that model was

1212
01:30:23,780 --> 01:30:28,390
i'm going to assume that there is no need to show that is the model

1213
01:30:28,390 --> 01:30:29,680
such that

1214
01:30:32,280 --> 01:30:33,850
it is not pause

1215
01:30:35,400 --> 01:30:40,190
which means there is some world in that model which makes five vols

1216
01:30:42,490 --> 01:30:46,580
he's going to make all of the things in gamma true it's the other condition

1217
01:30:46,590 --> 01:30:48,980
i mean that it is that

1218
01:30:50,370 --> 01:30:55,170
there is some model which makes all the things in diameter subway

1219
01:30:56,880 --> 01:30:59,150
that's what this lecture

1220
01:31:00,890 --> 01:31:06,200
eight to do is only just switch to a local consequence relation is the important

1221
01:31:06,200 --> 01:31:07,720
that define

1222
01:31:10,700 --> 01:31:13,680
the turnstile within l

1223
01:31:13,690 --> 01:31:16,050
right not distance different ones

1224
01:31:17,400 --> 01:31:21,250
which is defined in terms of the bigger one just says

1225
01:31:21,300 --> 01:31:24,830
all right let's locally deducible from x

1226
01:31:26,330 --> 01:31:29,620
if there is a finite subset size

1227
01:31:31,090 --> 01:31:32,470
such that

1228
01:31:32,470 --> 01:31:36,160
the conjunction implies phi is the

1229
01:31:37,640 --> 01:31:39,760
so this deduction

1230
01:31:40,800 --> 01:31:44,940
what we've got over

1231
01:31:46,320 --> 01:31:49,780
but i'm just going to use this is shorthand flight right

1232
01:31:51,260 --> 01:31:57,240
so everywhere you see this means is a finite subset of silence junction of the

1233
01:31:57,240 --> 01:32:00,030
size implies fast

1234
01:32:01,490 --> 01:32:05,740
now as think that if this whole in other words if their home

1235
01:32:08,350 --> 01:32:11,100
doctor using the traditional media

1236
01:32:12,500 --> 01:32:14,950
and you'll actually be able to do that by the

1237
01:32:14,960 --> 01:32:17,060
given here

1238
01:32:19,610 --> 01:32:29,050
city is maximal if what

1239
01:32:35,910 --> 01:32:39,720
try OK

1240
01:32:39,740 --> 01:32:41,380
infinite set of formulae

1241
01:32:41,390 --> 01:32:44,940
and this city is maximal if you like and either that formula will be in

1242
01:32:44,940 --> 01:32:48,920
their or or its negation or so x is already in

1243
01:32:52,680 --> 01:32:56,510
what's the idea of max melody words that you can't squeeze anything in a linear

1244
01:32:56,560 --> 01:32:58,540
formula phi

1245
01:32:59,800 --> 01:33:03,150
and identifies in there or not five

1246
01:33:04,450 --> 01:33:07,030
you can't you can't add anything more

1247
01:33:07,090 --> 01:33:09,100
without making an inconsistent

1248
01:33:09,120 --> 01:33:09,910
and what

1249
01:33:11,480 --> 01:33:16,570
consistency means that sign not so i are likely to do so

1250
01:33:16,670 --> 01:33:19,500
the system means you have both

1251
01:33:25,710 --> 01:33:29,230
in other words i can't deduce sign outside from

1252
01:33:29,310 --> 01:33:32,060
then i kx systems nick starr

1253
01:33:33,260 --> 01:33:35,900
which is a superset of gamma

1254
01:33:35,950 --> 01:33:39,290
how do i do it action of formulae phi one of the fine

1255
01:33:40,600 --> 01:33:44,080
just number them anyway you like so

1256
01:33:45,460 --> 01:33:50,990
and do that can

1257
01:33:52,390 --> 01:33:55,170
so you start off with gamma not

1258
01:33:55,220 --> 01:33:56,890
and you see whether

1259
01:33:56,890 --> 01:34:00,850
as X transpose move the parentheses

1260
01:34:00,870 --> 01:34:06,870
right it X transpose A transpose Y and move the parentheses

1261
01:34:08,610 --> 01:34:13,610
so that I'm looking at a transposed was then a transpose what what that have

1262
01:34:14,730 --> 01:34:23,470
to kind of what yesterday

1263
01:34:26,790 --> 01:34:32,890
the key steps so simple or matrices corresponds the integration by parts

1264
01:34:32,940 --> 01:34:40,440
continuum problems that we've met for a while OK and now why is this so

1265
01:34:40,440 --> 01:34:47,920
that's OK right actually was at an even equal now that was right there OK

1266
01:34:47,960 --> 01:34:50,580
0 that was even an equal sign

1267
01:34:50,610 --> 01:34:56,770
was that was even equals 0 because they X was exactly the OK now emerging

1268
01:34:56,770 --> 01:35:00,180
from the step that this is less or equal this now why is that

1269
01:35:02,110 --> 01:35:03,780
because A transpose Y

1270
01:35:04,310 --> 01:35:09,170
if you don't mind what order this inner product comes in those images to vectors

1271
01:35:09,170 --> 01:35:13,840
of I'm happy to write X transpose see if you if you would like to

1272
01:35:14,050 --> 01:35:18,530
let me right up there is x transforms which is the same as C transpose

1273
01:35:18,530 --> 01:35:19,370
X just

1274
01:35:19,870 --> 01:35:28,150
just the dot product doesn't matter OK here is the key step then obviously and

1275
01:35:28,150 --> 01:35:29,010
why is that true

1276
01:35:30,650 --> 01:35:35,110
well 1st of all a transpose why that we have to it's beautiful all the

1277
01:35:35,110 --> 01:35:40,430
requirements community A transpose Y is less or equal to see

1278
01:35:40,440 --> 01:35:42,320
so does that make this true

1279
01:35:42,910 --> 01:35:48,630
right away if A transpose Y is less Rakosi not right away I need to

1280
01:35:48,630 --> 01:35:55,230
know that I multiplying these this inequality transforms wireless frequency by positive numbers

1281
01:35:55,790 --> 01:36:00,250
but if you might take inequality multiplied by negative number it reverses

1282
01:36:00,490 --> 01:36:04,590
but that's exactly where this comes

1283
01:36:07,130 --> 01:36:08,370
and in a way

1284
01:36:08,410 --> 01:36:11,520
get more than in a way here we can see

1285
01:36:12,570 --> 01:36:14,330
that is an

1286
01:36:15,240 --> 01:36:22,650
so that so that any feasible 1 match was less frequently than any feasible while

1287
01:36:22,660 --> 01:36:28,350
I gave us the transfers why that was smaller than or equal to C transpose

1288
01:36:28,350 --> 01:36:29,850
X now a point is that

1289
01:36:30,330 --> 01:36:34,970
so that's the easy steps and the right that is at and there was a

1290
01:36:35,010 --> 01:36:40,210
corresponding to the easy step when we were doing

1291
01:36:40,260 --> 01:36:47,570
minimizing potential energy there was something called the complementary energy that involves the

1292
01:36:48,130 --> 01:36:50,070
the flows

1293
01:36:50,110 --> 01:36:52,350
the of the year

1294
01:36:52,390 --> 01:36:53,420
for the

1295
01:36:53,910 --> 01:37:02,750
stresses whatever with dual variable was and we could have found a weak duality and

1296
01:37:02,750 --> 01:37:03,650
then ultimately

1297
01:37:05,890 --> 01:37:12,390
so so I'm from like at you'll see that often tell to prove is that

1298
01:37:12,400 --> 01:37:17,710
in 1 direction it's it's easy that's the direction we talk and but now

1299
01:37:18,030 --> 01:37:20,410
together equality is not so

1300
01:37:20,930 --> 01:37:26,370
but actually when with equality holds so now this is actually not only gives us

1301
01:37:26,490 --> 01:37:35,810
of the easy argument but also gives us a conditions for optimality when

1302
01:37:38,050 --> 01:37:39,390
when the simple

1303
01:37:40,230 --> 01:37:44,250
went well ways since we have all equal sign I only have to ask right

1304
01:37:44,250 --> 01:37:49,770
there when does well it's 1 they're equal 1 very polite

1305
01:37:49,810 --> 01:37:53,570
as X transpose

1306
01:37:53,630 --> 01:37:58,020
so but when will equality holds X transpose

1307
01:38:00,240 --> 01:38:06,840
not just gonna put them on the same side of the equation times C minus

1308
01:38:06,840 --> 01:38:14,570
A transpose nu what has to be worked well right it always greater or equal

1309
01:38:14,580 --> 01:38:21,740
0 from feasibility but to get equality so this is to get to him to

1310
01:38:21,740 --> 01:38:27,740
to get equality equal sign

1311
01:38:27,780 --> 01:38:31,160
to be optimal so that was an important word in there

1312
01:38:31,700 --> 01:38:33,280
optimality that meant

1313
01:38:33,790 --> 01:38:36,050
what's the condition like

1314
01:38:36,070 --> 01:38:38,050
derivative equals 0

1315
01:38:38,160 --> 01:38:45,020
this in this world will well it's it's this it's just that this has to

1316
01:38:45,020 --> 01:38:46,890
be equal to 0 right

1317
01:38:47,980 --> 01:38:53,020
I just coming out of just looking at the only place there was any questions

1318
01:38:53,090 --> 01:38:56,240
and saying everyone equality

1319
01:38:56,430 --> 01:39:01,780
then we gotta have equality at that step so there's less or equal followed immediately

1320
01:39:01,870 --> 01:39:03,030
for any x and y

1321
01:39:03,870 --> 01:39:09,730
equality is only going to happen at the best ones so maybe I should put

1322
01:39:09,730 --> 01:39:15,670
the stars X . transpose C minus A transpose nu y star that's how we

1323
01:39:16,910 --> 01:39:17,330
the winner

1324
01:39:19,350 --> 01:39:23,030
and and you see what's going on here this is the dot product of 2

1325
01:39:24,610 --> 01:39:28,650
and what's the deal with those vectors what we know about those vectors

1326
01:39:29,310 --> 01:39:33,690
well there orthogonal but then each of those vectors

1327
01:39:33,700 --> 01:39:34,980
it is

1328
01:39:35,020 --> 01:39:42,760
non-negative the X is non-negative and C minus A transpose Y is non-negative so all

1329
01:39:42,850 --> 01:39:45,850
well and good to

1330
01:39:45,850 --> 01:39:48,180
non-negative vectors

1331
01:39:48,240 --> 01:39:50,440
the biorthogonal

1332
01:39:50,440 --> 01:39:56,380
helium too stable and looking at bonding versus antibody titre let's ask if we take

1333
01:39:56,750 --> 01:39:58,500
Avogadro's number

1334
01:39:58,520 --> 01:40:03,710
of wave functions and blend them in something like copper can we make sense of

1335
01:40:03,710 --> 01:40:09,330
what's going on so this is what their contribution was it was linear combination of

1336
01:40:09,330 --> 01:40:15,110
atomic orbitals inter molecular orbitals for for large

1337
01:40:15,870 --> 01:40:20,060
ensembles of larger and

1338
01:40:20,340 --> 01:40:23,940
and so here's what his that goes let's take a look at

1339
01:40:25,020 --> 01:40:29,780
occurred to you recall last day we looked at something like the 1 to ask

1340
01:40:29,780 --> 01:40:35,180
whether any blends with another way to make a 2 we can start at at

1341
01:40:35,180 --> 01:40:40,260
some level I just look at the valence shell OK so look at the valence

1342
01:40:41,120 --> 01:40:46,340
orbitals this is atomic orbitals and when to come close together we used to have

1343
01:40:46,350 --> 01:40:50,920
become close enough that the operating as 1 system in order not to violate the

1344
01:40:50,920 --> 01:40:55,680
Pauli exclusion principle these energies have to just a little bit otherwise we run the

1345
01:40:55,680 --> 01:41:00,760
risk of having more than 1 electron with the same set of quantum numbers so

1346
01:41:00,760 --> 01:41:08,300
these will rearrange themselves into what we call anti bonding and bonding orbitals were anti

1347
01:41:08,300 --> 01:41:12,220
bonding or slightly higher energy than the parents

1348
01:41:12,450 --> 01:41:18,990
atomic orbital and blinding are and slightly lower in energy than the parents were so

1349
01:41:18,990 --> 01:41:23,400
this is a tuna suppose to I do this 1 more time suppose I take

1350
01:41:23,400 --> 01:41:28,800
a + 82 and I wanna make a 3 what would happen if I took

1351
01:41:28,800 --> 01:41:34,920
a + 82 will again not to violate the Pauli exclusion principle this initial whatever

1352
01:41:34,920 --> 01:41:40,400
this orbital is let's call it an acid only 3 as whatever it's I have

1353
01:41:40,400 --> 01:41:42,220
to allow for

1354
01:41:42,370 --> 01:41:45,040
that number of states

1355
01:41:45,060 --> 01:41:49,160
I think you can see by analogy this is what the hyper and that they

1356
01:41:49,160 --> 01:41:51,920
kept going until they got a

1357
01:41:51,990 --> 01:41:57,940
n where N is large how large all about the size of

1358
01:41:57,950 --> 01:41:58,680
you thumb

1359
01:41:59,520 --> 01:42:04,120
on the order of the cubic centimeters in other words a block of matter

1360
01:42:04,130 --> 01:42:07,870
a block of metal so there is a large what happens this thing keeps going

1361
01:42:08,110 --> 01:42:14,160
there's a little bit of a broadening flat

1362
01:42:14,180 --> 01:42:20,330
this is what happens we have many many levels here starting at the highest anti

1363
01:42:20,330 --> 01:42:23,300
bonding down to the lowest bonding but now

1364
01:42:23,870 --> 01:42:29,970
the space so the energy difference between successive levels is getting vanishingly small

1365
01:42:30,130 --> 01:42:34,330
and in fact you can think about it would what's how many how many of

1366
01:42:35,620 --> 01:42:39,100
atoms would there be a cubic centimeter of copper

1367
01:42:39,700 --> 01:42:46,360
what's think about the forget cubic centimeters I can look on the periodic table if

1368
01:42:46,360 --> 01:42:53,080
you look the periodic table the molar volume they call atomic volume the volume molar

1369
01:42:53,080 --> 01:43:01,600
volume of copper is 7 . 1 1 cubic centimeters so there's Avogadro's number in

1370
01:43:01,600 --> 01:43:06,210
the right who's buried in grants to grant how many atoms in a molar volume

1371
01:43:06,210 --> 01:43:13,180
of 1 mole so this contains this contains and have gabbro Adams

1372
01:43:14,110 --> 01:43:20,710
so divided so that ever get around can divide through a pretty soon you figure

1373
01:43:20,710 --> 01:43:26,510
out how many states there are variety if of either 1 cubic centimetre it's roughly

1374
01:43:26,630 --> 01:43:29,130
10 to the 23

1375
01:43:29,360 --> 01:43:31,060
patterns of

1376
01:43:31,660 --> 01:43:33,660
centimeter cube

1377
01:43:34,280 --> 01:43:38,580
so that means I'm going to have 10 to the 23 states here

1378
01:43:38,590 --> 01:43:42,870
and what's this energy difference is on the order of some tens of electron volts

1379
01:43:43,090 --> 01:43:46,250
so suppose I put in here I'll even give you a thousand electron volts cause

1380
01:43:46,250 --> 01:43:49,450
I got I got a lot of protons in the nucleus suppose this is on

1381
01:43:49,450 --> 01:43:55,700
the order of 10 to the 3rd but it's divided by 10 to the 23rd

1382
01:43:55,730 --> 01:44:02,080
contributors so what's the delta in here it's about 10 to the minus 20 electron

1383
01:44:02,080 --> 01:44:06,370
volts which is about 10 to the minus 40

1384
01:44:06,420 --> 01:44:12,450
this is less energy in the universe movement of the eyeball mosquito there's nothing here

1385
01:44:12,450 --> 01:44:13,090
there's nothing

1386
01:44:14,070 --> 01:44:20,600
so from a distance you I get really close this is still out of principle

1387
01:44:21,280 --> 01:44:26,140
the electrons come to you just as Noah's are from the bottom up to the

1388
01:44:26,150 --> 01:44:31,880
top these are all distinct quantum states but from a distance there so close together

1389
01:44:32,160 --> 01:44:38,320
that this appears to be a plan continued almost continuum of

1390
01:44:41,220 --> 01:44:45,960
so that the band is a set of very closely spaced orbitals

1391
01:44:46,030 --> 01:44:50,590
now you see how to populate and for something like copper we know coppers got

1392
01:44:51,010 --> 01:45:00,900
its coffers are 3 D 10 4 1 sole coppers 2 by 2 coming in

1393
01:45:01,080 --> 01:45:03,820
and fill up to the halfway point of the bank

1394
01:45:04,140 --> 01:45:10,240
because I can put 2 per orbital but only have 1 for copper so if

1395
01:45:10,240 --> 01:45:19,440
I blow this up what do I have about this bandits half-filled and it's half

1396
01:45:19,440 --> 01:45:25,480
filled and then the next level is a vanishingly small energy difference and so forth

1397
01:45:25,530 --> 01:45:30,380
then I apply potential extending side of the crystal negative sentiment the side of the

1398
01:45:30,400 --> 01:45:35,240
crystal positive it takes a tiny bit of energy to pull in electron out of

1399
01:45:35,240 --> 01:45:39,760
the topmost orbital put up here and now it's free to move about

1400
01:45:39,780 --> 01:45:45,110
so this accounts for the high degree of metallic nature

1401
01:45:45,610 --> 01:45:50,860
of materials because the energy levels were so close together in the past we suffered

1402
01:45:51,090 --> 01:45:56,160
a classical gas-phase atoms it took a substantial amount of energy to move something out

1403
01:45:56,160 --> 01:46:01,060
of its ground state in this case the move ground state takes a trivial amount

1404
01:46:01,060 --> 01:46:06,830
of energy and the gets curtains and indicate this goes back to drew with his

1405
01:46:07,080 --> 01:46:10,330
cannot be some books will actually call the

1406
01:46:10,920 --> 01:46:15,800
atomic or scatter but there'd there'd be a sloppy with the terminology because it's the

1407
01:46:16,260 --> 01:46:19,040
it's the ionic or consisting of

1408
01:46:20,420 --> 01:46:25,990
protons in the nucleus plus the inner shell electrons but in electrical engineering to refer

1409
01:46:25,990 --> 01:46:30,970
to this as can the city of valence electrons so here's a cartoon showing what's

1410
01:46:30,970 --> 01:46:37,300
happening as you progressively and larger and larger numbers and not to violate the poly

1411
01:46:37,300 --> 01:46:38,680
now all this

1412
01:46:38,680 --> 01:46:41,840
looks like usual but actually it was known

1413
01:46:41,870 --> 01:46:43,140
in a sense

1414
01:46:45,490 --> 01:46:52,460
by people that we're just trying to write down supersymmetric gravity feeds people had realize

1415
01:46:52,460 --> 01:46:59,350
that or at about the same time in parallel that there are two speciality supersymmetric

1416
01:46:59,350 --> 01:47:04,030
gravity theories which are maximally in the sense that they we have as much approximately

1417
01:47:04,030 --> 01:47:05,550
as you can have

1418
01:47:05,570 --> 01:47:10,300
in two dimensions which are known as type two and type to be supergravity

1419
01:47:10,300 --> 01:47:16,930
and all these is simply one one-horse supersymmetric company of graviton that's also in a

1420
01:47:16,930 --> 01:47:22,490
sense it looks complicated but it simply gave it to super symmetrized in ten dimensions

1421
01:47:22,490 --> 01:47:28,300
there is no freedom here everything is very rigid because supersymmetry fixes everything and it

1422
01:47:28,300 --> 01:47:31,120
also completely fixes the effective

1423
01:47:31,300 --> 01:47:36,180
and interaction so this theories is very unique and it takes

1424
01:47:36,200 --> 01:47:38,680
a lot of work which has been found

1425
01:47:38,700 --> 01:47:43,160
to show that the notion of private string theories are consistent theory surely does give

1426
01:47:43,160 --> 01:47:49,340
exactly in the effective low energy limit this two supergravity theories called type three or

1427
01:47:49,340 --> 01:47:54,970
type to b

1428
01:47:55,030 --> 01:48:00,010
now here comes the last take control i want to discuss which are usually brains

1429
01:48:00,030 --> 01:48:05,050
and which in a sense has full revolutionized this subject in the mid nineties

1430
01:48:05,070 --> 01:48:08,490
well i talked before about

1431
01:48:08,510 --> 01:48:15,930
closed training giving super gravitons described by supergravity theories which are still far from what

1432
01:48:15,930 --> 01:48:20,340
we want in nature but of course remember the big thing about them is that

1433
01:48:20,340 --> 01:48:25,530
they are finite consistent can try to use them as quantum theories and then try

1434
01:48:25,550 --> 01:48:28,510
to see how to recover the standard model later on

1435
01:48:28,530 --> 01:48:31,120
as the tomorrow but

1436
01:48:31,140 --> 01:48:33,030
how about the open strings

1437
01:48:33,050 --> 01:48:39,510
now i remember from this far the open strings have two types of boundary conditions

1438
01:48:39,510 --> 01:48:42,390
at the end months or the slayer

1439
01:48:42,430 --> 01:48:44,970
no correspond to free

1440
01:48:45,010 --> 01:48:52,590
moving open strings deviously correspond to stock strings we can have some of the some

1441
01:48:52,590 --> 01:48:57,890
moments and initially why not after all and what do they describe what they describe

1442
01:48:57,890 --> 01:49:02,760
open strings push toward poets of stock or fixed

1443
01:49:03,990 --> 01:49:08,200
surfaces in space time so here is it

1444
01:49:09,570 --> 01:49:13,950
brain that's how they are called the dirichlet brain think of it as some surface

1445
01:49:13,970 --> 01:49:15,370
in space time

1446
01:49:15,390 --> 01:49:21,070
the trajectory may be of some extended objects and effect of time which is very

1447
01:49:21,070 --> 01:49:27,320
indirectly described not by an equations yet by simply but by simply saying that open

1448
01:49:27,320 --> 01:49:32,780
strings can have their endpoints talk of course this thing is free event to go

1449
01:49:32,780 --> 01:49:39,320
inside the bottle be theory by the end points out there are therefore open strings

1450
01:49:39,340 --> 01:49:43,860
are intimately tied with these dish usually brains now

1451
01:49:44,410 --> 01:49:49,430
is the spatial extent of this object it could be a particle in which case

1452
01:49:49,430 --> 01:49:55,450
one talks about the desire of brain particles this would then have been simply straight

1453
01:49:55,450 --> 01:49:57,340
line going up in time

1454
01:49:57,340 --> 01:49:58,870
time goes up here

1455
01:49:58,890 --> 01:50:03,720
so open strings with this fact to live in the same space boring and then

1456
01:50:03,720 --> 01:50:06,050
moving time of course

1457
01:50:06,370 --> 01:50:10,160
but then it would be one of these would be a so-called this string

1458
01:50:10,180 --> 01:50:11,430
he could be too

1459
01:50:11,430 --> 01:50:15,030
three and so on it can be anything up to nine

1460
01:50:15,050 --> 01:50:19,100
we'll just have nine space dimensions you

1461
01:50:19,120 --> 01:50:23,360
so the picture is that you have this open string stock there have closed things

1462
01:50:23,360 --> 01:50:25,620
that can move everywhere

1463
01:50:25,680 --> 01:50:30,950
and of course you cannot be copied the they can interact because one of these

1464
01:50:30,950 --> 01:50:36,430
close things can go at an open string and make it longer for instance

1465
01:50:36,450 --> 01:50:41,990
no polchinski realized in ninety five that the correct way to think about this the

1466
01:50:41,990 --> 01:50:48,030
brain so that was the big surprise is simply as new kinds of excitation surely

1467
01:50:48,030 --> 01:50:51,300
tonic like excitations of string theory

1468
01:50:51,320 --> 01:50:56,930
i would have a mass density tension of their own more than one charged a

1469
01:50:56,930 --> 01:51:02,200
couple of these bizarre moment fields as elementary charges

1470
01:51:02,260 --> 01:51:12,600
and they really are very much analogous to magnetic monopole some grand unified theories

1471
01:51:13,970 --> 01:51:20,870
now let's think a little bit about the simplest non solid this is the so-called

1472
01:51:20,870 --> 01:51:22,260
team actually

1473
01:51:22,300 --> 01:51:25,280
because then you'll see every everything goes through

1474
01:51:25,320 --> 01:51:29,910
now the kink is simply domain wall in some theory that has to degenerate vacua

1475
01:51:30,050 --> 01:51:36,280
shoots this separation between viking one and viking two that can happen in real space

1476
01:51:36,300 --> 01:51:42,200
now let's take a very simple example just to be precise here is a scalar

1477
01:51:42,200 --> 01:51:47,450
theory it has to scalar fields fire high it has the potential which is the

1478
01:51:47,450 --> 01:51:50,120
and you can see that actually the same

1479
01:51:50,130 --> 01:51:54,080
just taking the first half of information gain in the formula

1480
01:51:54,090 --> 01:51:55,490
it's just taking the

1481
01:51:55,550 --> 01:51:57,050
one occurrence

1482
01:51:57,100 --> 01:52:01,480
into account not something we then were not occurring

1483
01:52:01,530 --> 01:52:04,990
which is crucial for the performance here

1484
01:52:05,040 --> 01:52:07,920
it's also intuitive in the way fitting in

1485
01:52:07,960 --> 01:52:11,880
there are many words that are not in your testing document and if you base

1486
01:52:11,890 --> 01:52:15,430
your model on saying this word is not in the document and this is not

1487
01:52:15,430 --> 01:52:18,730
and this is not of course there are not many other or not

1488
01:52:18,780 --> 01:52:24,460
and if your mother mainly concentrates on saying which words should not be the document

1489
01:52:24,510 --> 01:52:27,170
it's not going to perform very well

1490
01:52:27,170 --> 01:52:29,860
at least not if you have a problem where

1491
01:52:29,870 --> 01:52:30,980
most of the

1492
01:52:31,040 --> 01:52:33,690
documents negative

1493
01:52:34,180 --> 01:52:37,360
the mutual information for text

1494
01:52:37,390 --> 01:52:39,330
also seen you're just

1495
01:52:39,340 --> 01:52:42,770
if you look with a bit more into what's happening in here the difference is

1496
01:52:43,840 --> 01:52:45,510
but here

1497
01:52:45,530 --> 01:52:48,670
in addition to class probability

1498
01:52:48,740 --> 01:52:53,570
which is here there is joint class probability class and words

1499
01:52:53,570 --> 01:52:57,250
which is multiplied here in front of the fact

1500
01:52:57,270 --> 01:52:58,730
and it performs

1501
01:52:58,770 --> 01:53:02,150
similar to cross entropy a little bit worse but

1502
01:53:02,210 --> 01:53:06,960
i can't really tell you that no consistent

1503
01:53:06,970 --> 01:53:12,730
alterations to measure i mentioned that makes difference between the class values

1504
01:53:12,770 --> 01:53:15,980
so if you forget about this

1505
01:53:16,030 --> 01:53:21,590
the last factor which kind of just to help the calculation

1506
01:53:21,590 --> 01:53:25,070
what it does it selects features or words

1507
01:53:25,090 --> 01:53:26,890
they according positive

1508
01:53:28,080 --> 01:53:30,930
and not in negative documents in

1509
01:53:30,980 --> 01:53:32,470
abstracting away

1510
01:53:32,480 --> 01:53:36,670
so features that are characteristic for positive documents

1511
01:53:36,670 --> 01:53:38,670
and hopefully not from negative

1512
01:53:38,720 --> 01:53:40,850
and that performs

1513
01:53:40,890 --> 01:53:43,710
very well

1514
01:53:43,940 --> 01:53:47,610
and the frequency just taking frequency of the word

1515
01:53:47,610 --> 01:53:48,770
and taking no

1516
01:53:48,800 --> 01:53:54,000
the most frequent words but don't forget what people usually do first remove stopwords

1517
01:53:54,040 --> 01:53:55,770
so you don't really take

1518
01:53:55,780 --> 01:54:00,490
you know the most really most frequent words like that at and so on you

1519
01:54:00,490 --> 01:54:03,550
remove them and then take the most frequent words

1520
01:54:03,590 --> 01:54:06,620
and there is this being normal separation

1521
01:54:06,630 --> 01:54:07,800
which also has

1522
01:54:07,820 --> 01:54:14,530
this idea of taking was characteristic for positive but and measuring how the differences between

1523
01:54:14,530 --> 01:54:17,980
characteristic for positive and four negative documents

1524
01:54:18,070 --> 01:54:19,680
this one was shown to

1525
01:54:20,600 --> 01:54:23,860
the performance of the night

1526
01:54:23,880 --> 01:54:31,350
so all these different feature scoring measures influence the performance in some way

1527
01:54:31,390 --> 01:54:35,500
but also depends on the machine learning algorithm that is used i already mentioned that

1528
01:54:35,500 --> 01:54:36,780
for SVM

1529
01:54:36,800 --> 01:54:40,350
they don't really help much most of them

1530
01:54:40,400 --> 01:54:46,150
and it's possible because SVM has already embedded this weighting of features

1531
01:54:46,190 --> 01:54:47,480
so in some way

1532
01:54:47,540 --> 01:54:50,880
has some feature selection embedded in the system so

1533
01:54:50,890 --> 01:54:55,660
it does not perfect so much of some pre processing of the data

1534
01:54:55,680 --> 01:54:58,160
while naive bayes actually

1535
01:54:58,480 --> 01:55:03,490
really can use feature subset selection to improve the performance

1536
01:55:03,500 --> 01:55:05,160
but at the end of the day

1537
01:55:05,180 --> 01:55:11,510
the overall performance of naive bayes and document categorisation at least the experiments show you

1538
01:55:11,520 --> 01:55:14,260
is towards the SVM

1539
01:55:15,040 --> 01:55:19,440
you would rather use a linear SVM and be happy

1540
01:55:19,450 --> 01:55:24,170
so i some slides to show you

1541
01:55:25,400 --> 01:55:29,120
of running different feature scoring measures on data

1542
01:55:29,130 --> 01:55:33,410
the first couple of slides naive bayes classifier

1543
01:55:33,420 --> 01:55:34,890
on yahoo data

1544
01:55:34,920 --> 01:55:38,140
hierarchy of the pages

1545
01:55:38,150 --> 01:55:39,450
and then

1546
01:55:39,500 --> 01:55:44,120
the other experiments of ideas two thousand news dataset

1547
01:55:44,430 --> 01:55:46,650
using a linear SVM

1548
01:55:46,940 --> 01:55:50,850
also in a space and perceptron

1549
01:55:50,860 --> 01:55:52,580
is there any questions so far

1550
01:55:52,600 --> 01:55:56,210
we went through all the number two is just and just going to show some

1551
01:55:56,210 --> 01:56:01,120
experiment results

1552
01:56:03,170 --> 01:56:12,890
i can't

1553
01:56:20,800 --> 01:56:23,590
this work

1554
01:56:25,650 --> 01:56:30,170
so he found that works but i can't really in this paper he doesn't compare

1555
01:56:30,260 --> 01:56:33,360
with the other one so i can't tell you if he really tried and this

1556
01:56:33,360 --> 01:56:35,930
will work better or not

1557
01:56:36,000 --> 01:56:38,270
but that's interesting

1558
01:56:44,490 --> 01:56:47,040
however during the time

1559
01:56:50,530 --> 01:56:52,520
so just to give you

1560
01:56:52,530 --> 01:56:55,890
the idea of the data not going to be the just

1561
01:56:55,930 --> 01:56:58,470
if you know what the next slide going to talk about

1562
01:56:58,530 --> 01:57:01,100
so these are five datasets

1563
01:57:01,110 --> 01:57:05,000
extracted subsets from the yahoo hierarchy tree

1564
01:57:05,770 --> 01:57:09,910
the whole problem behind this karaoke and cold but we won't talk about that here

1565
01:57:09,910 --> 01:57:14,630
so this is the number of documents in each of the datasets so you can

1566
01:57:14,630 --> 01:57:19,160
see we go from two thousand up to eighty thousand documents

1567
01:57:19,210 --> 01:57:22,340
this is the number of features going from thousand

1568
01:57:22,350 --> 01:57:24,450
two about thirty thousand

1569
01:57:24,510 --> 01:57:29,730
and the number of classes or categories going from about one hundred to eight hundred

1570
01:57:29,740 --> 01:57:31,710
so you can see we have a range

1571
01:57:31,720 --> 01:57:33,940
of difficulty of data

1572
01:57:34,510 --> 01:57:39,440
and maybe just mention as features

1573
01:57:39,480 --> 01:57:42,370
in addition to using single words

1574
01:57:42,430 --> 01:57:45,430
some pairs and three posts up to

1575
01:57:45,650 --> 01:57:52,100
of words i used sequences of words that are occurring frequently nothing to data

1576
01:57:52,610 --> 01:57:55,910
so in this way in statistical they get phrases like

1577
01:57:55,960 --> 01:57:57,960
i don't know text mining or

1578
01:57:57,980 --> 01:58:00,900
dimensionality reduction also

1579
01:58:00,910 --> 01:58:02,700
so what is the performance

1580
01:58:02,760 --> 01:58:04,940
so we go from about one hundred

1581
01:58:04,980 --> 01:58:08,150
to about eight thousand different categories

1582
01:58:08,150 --> 01:58:12,890
some decide what they want us to show is how the

1583
01:58:13,360 --> 01:58:15,650
a growing number of features

1584
01:58:15,670 --> 01:58:18,900
selected in the feature subset

1585
01:58:18,910 --> 01:58:21,570
in places the performance

1586
01:58:21,580 --> 01:58:24,680
and the idea of this light is only to give you

1587
01:58:24,740 --> 01:58:27,220
show experimentally

1588
01:58:27,230 --> 01:58:29,590
the different scoring measures

1589
01:58:29,630 --> 01:58:31,630
we so there formulas

1590
01:58:32,860 --> 01:58:35,050
really behave differently

1591
01:58:35,070 --> 01:58:38,820
depending on the number of features you use of course at the end

1592
01:58:38,840 --> 01:58:42,360
when you use all the features they all have the same performance

1593
01:58:42,370 --> 01:58:43,820
but at the start

1594
01:58:43,860 --> 01:58:44,980
and the

1595
01:58:45,040 --> 01:58:46,740
o point be interested in

1596
01:58:46,740 --> 01:58:51,360
out to map with the same and i am quality differently strength

1597
01:58:51,370 --> 01:58:52,220
it just

1598
01:58:52,290 --> 01:58:54,230
they excel why

1599
01:58:54,270 --> 01:58:55,630
and the right

1600
01:58:55,650 --> 01:59:01,430
the product of the individual probabilities by the product of the joint probability

1601
01:59:01,440 --> 01:59:03,990
if it is about one they are independent

1602
01:59:04,010 --> 01:59:05,240
if it is high

1603
01:59:05,360 --> 01:59:07,610
they are very correlated if it is

1604
01:59:07,650 --> 01:59:10,390
low they are negatively correlated

1605
01:59:10,680 --> 01:59:15,430
but during the orientation because it is the definition of this is the same is

1606
01:59:15,430 --> 01:59:17,360
usually takes with the y

1607
01:59:17,380 --> 01:59:22,070
the result is the same so we don't have the orientation

1608
01:59:22,340 --> 01:59:26,910
i am not going to enter into the many alternative so we only mention one

1609
01:59:26,910 --> 01:59:28,600
later on

1610
01:59:28,610 --> 01:59:29,780
so how is the process

1611
01:59:29,790 --> 01:59:32,750
this the decision making process as follows

1612
01:59:32,810 --> 01:59:37,450
you the set with a number of sections show each of which is a set

1613
01:59:37,450 --> 01:59:38,890
of items

1614
01:59:38,910 --> 01:59:42,880
and surprise for for support for features

1615
01:59:42,890 --> 01:59:43,940
and you know it

1616
01:59:43,950 --> 01:59:49,090
back all the rules that hold in this dataset and dust levels are high

1617
01:59:49,220 --> 01:59:51,620
and usually what you eat

1618
01:59:51,730 --> 01:59:57,860
is that the show what is your guess you're investing your i about one hundred

1619
01:59:57,990 --> 02:00:03,010
fifty thousand three hundred ninety seven to one in the image show them order to

1620
02:00:04,510 --> 02:00:07,560
you don't want the system to show you

1621
02:00:07,570 --> 02:00:11,840
one hundred fifty thousand rich

1622
02:00:12,210 --> 02:00:18,660
how do we identify the london who have although we take off and was

1623
02:00:18,710 --> 02:00:22,780
there are many not just for the next year and this is my favourite because

1624
02:00:22,810 --> 02:00:24,330
they are

1625
02:00:24,370 --> 02:00:27,810
about four or five different ways of expressing than she

1626
02:00:27,860 --> 02:00:30,240
each of them says he will but not exactly

1627
02:00:30,280 --> 02:00:33,610
like the other and they are now to be equivalent

1628
02:00:33,610 --> 02:00:35,370
which is not

1629
02:00:35,430 --> 02:00:39,590
so i am not convinced that by the end of this is that with one

1630
02:00:39,680 --> 02:00:43,980
that means that the confidence will be high in every

1631
02:00:43,980 --> 02:00:48,140
now a as i mentioned it in passing before

1632
02:00:48,200 --> 02:00:53,410
the notion of course itemsets is behind the scenes and you have to take it

1633
02:00:53,410 --> 02:00:58,280
into account but let me not get into the taste of what it means to

1634
02:00:58,280 --> 02:01:00,880
share the same single space with

1635
02:01:00,940 --> 02:01:05,040
action and this conditional selling groceries

1636
02:01:05,060 --> 02:01:08,680
and this invariant is particularly with because it

1637
02:01:09,810 --> 02:01:18,240
create separately implications with the minimum size bases they can basis and the but implications

1638
02:01:18,240 --> 02:01:20,770
with or one minimum size

1639
02:01:22,460 --> 02:01:24,720
how does it work

1640
02:01:24,860 --> 02:01:29,930
movie is not that much into the basement essentially you take place where the confidence

1641
02:01:29,930 --> 02:01:33,110
is high enough but if you are doing

1642
02:01:33,120 --> 02:01:38,560
it with the consequent or smaller to see it it doesn't work and confidence falls

1643
02:01:38,560 --> 02:01:40,870
below the a

1644
02:01:40,910 --> 02:01:46,900
and then there are choices so where he invented the social question but in other

1645
02:01:46,900 --> 02:01:52,880
cases it's not necessary because there are necessary to finance this into the they share

1646
02:01:52,880 --> 02:01:58,350
they do that they call the confidence that all the rules that hold with confidence

1647
02:01:58,350 --> 02:02:00,250
can be inferred from them

1648
02:02:00,260 --> 02:02:04,260
and that any alternative set of rules having the same two properties is at least

1649
02:02:04,610 --> 02:02:07,610
as large or or or

1650
02:02:07,660 --> 02:02:11,520
and here is that after the growing

1651
02:02:11,590 --> 02:02:13,480
the size of the basis

1652
02:02:13,490 --> 02:02:18,010
for one specific data set of all sets which really

1653
02:02:18,070 --> 02:02:19,270
this means

1654
02:02:19,320 --> 02:02:22,650
that you that lower confidence

1655
02:02:22,680 --> 02:02:25,270
this is the number of rules

1656
02:02:25,280 --> 02:02:31,190
in each of the two bases iterations the representative assembly have these

1657
02:02:31,240 --> 02:02:35,330
and now somebody may say i don't understand

1658
02:02:35,370 --> 02:02:36,020
you know that

1659
02:02:36,030 --> 02:02:38,480
at ninety five percent more

1660
02:02:38,490 --> 02:02:41,000
and that ninety nine percent of course

1661
02:02:41,020 --> 02:02:42,500
but then how come

1662
02:02:42,560 --> 02:02:44,320
due to the lower

1663
02:02:44,360 --> 02:02:46,860
in your love for me

1664
02:02:46,900 --> 02:02:51,340
and with less many

1665
02:02:51,350 --> 02:02:55,290
lower confidence means that we let pass more things

1666
02:02:55,310 --> 02:02:57,240
how can it happened

1667
02:02:57,290 --> 02:03:00,110
you get less

1668
02:03:00,120 --> 02:03:02,180
and the reason is

1669
02:03:02,180 --> 02:03:05,980
that we are taking out redundant users

1670
02:03:06,750 --> 02:03:10,010
well there wasn't that much more than that here

1671
02:03:10,870 --> 02:03:15,360
as we got official a bit lower will happen it will it pass a very

1672
02:03:15,360 --> 02:03:16,700
powerful ruler

1673
02:03:16,700 --> 02:03:19,660
that wasn't there may be have computers

1674
02:03:19,670 --> 02:03:21,800
ninety or eighty five

1675
02:03:21,950 --> 02:03:24,830
at ninety five

1676
02:03:24,880 --> 02:03:27,080
percent confidence it was not

1677
02:03:28,380 --> 02:03:30,680
eighty five it gets seen

1678
02:03:30,720 --> 02:03:35,230
and this very simple very strong misalignment many many many many others

1679
02:03:35,250 --> 02:03:36,730
he takes them off

1680
02:03:36,730 --> 02:03:37,740
there it is

1681
02:03:37,760 --> 02:03:41,740
and this is why sometimes use these two sites

1682
02:03:41,780 --> 02:03:43,270
you're confused

1683
02:03:43,370 --> 02:03:48,740
number further growth but sometimes you just computers number of course tricks

1684
02:03:48,780 --> 02:03:51,220
because one of four hours

1685
02:03:51,230 --> 02:03:52,160
have been

1686
02:03:52,650 --> 02:03:55,850
and if this has to be this notion

1687
02:03:55,930 --> 02:03:58,420
now were proposing a few years so i

1688
02:03:58,430 --> 02:04:00,200
confidence we

1689
02:04:00,250 --> 02:04:01,440
so essentially

1690
02:04:01,510 --> 02:04:11,700
the government there are many others that that individual here not here or

1691
02:04:11,970 --> 02:04:16,050
rule of confidence a ninety five percent was out of the basis and the basis

1692
02:04:16,140 --> 02:04:17,190
this point

1693
02:04:17,190 --> 02:04:21,590
and will at some point here maybe asking kind of by a stronger

1694
02:04:21,650 --> 02:04:26,160
this we think of the entire that's interesting

1695
02:04:26,170 --> 02:04:29,970
if you if have a rule that has a very sharp with

1696
02:04:30,020 --> 02:04:34,910
of confidence is where there is no basis means that well it has a high

1697
02:04:35,340 --> 02:04:37,520
confidence that some other

1698
02:04:37,530 --> 02:04:39,920
but not much

1699
02:04:39,930 --> 02:04:43,580
whereas if it stays in the race for a long time

1700
02:04:43,600 --> 02:04:47,580
it means that it has it is much much more confidence

1701
02:04:47,640 --> 02:04:51,520
then there all those who was alternative rules that would have the same effect

1702
02:04:51,570 --> 02:04:57,030
so this is a region of novelty

1703
02:04:57,050 --> 02:05:01,290
first we discover the missing that is going on

1704
02:05:01,300 --> 02:05:05,340
this covers entailed by the basis kids and they

1705
02:05:05,380 --> 02:05:09,760
the result of left not intended by others that means that it has a higher

1706
02:05:09,760 --> 02:05:12,470
confidence that what do others suggest

1707
02:05:13,140 --> 02:05:14,230
the question is

1708
02:05:14,270 --> 02:05:15,790
how much

1709
02:05:15,830 --> 02:05:18,700
higher confidence

1710
02:05:18,700 --> 02:05:23,920
so we take the a bayesian based confidence of which there and compare it with

1711
02:05:23,920 --> 02:05:25,510
some confidence

1712
02:05:25,570 --> 02:05:29,130
and this may be one or much more than one or a lot more than

1713
02:05:31,730 --> 02:05:32,740
so if

1714
02:05:32,790 --> 02:05:38,340
this in turn this is low this internally short this means that at some level

1715
02:05:38,400 --> 02:05:39,700
is very close by

1716
02:05:39,710 --> 02:05:41,670
and so on

1717
02:05:41,690 --> 02:05:43,200
whereas if this

1718
02:05:43,210 --> 02:05:46,030
watching these high missing that only flourish

1719
02:05:46,120 --> 02:05:49,010
or use that make the kind of london

1720
02:05:49,020 --> 02:05:53,950
i have confidence much much slower it's confidence much higher than would be expected from

1721
02:05:53,950 --> 02:05:55,450
the other

1722
02:05:55,500 --> 02:05:59,370
this is somehow price combined with the other

1723
02:05:59,380 --> 02:06:02,030
it's not exactly the

1724
02:06:02,060 --> 02:06:04,730
quantity that works well

1725
02:06:08,070 --> 02:06:11,530
he said about the website

1726
02:06:11,580 --> 02:06:15,710
what what

1727
02:06:15,740 --> 02:06:21,330
you've all from there and more sophisticated and more sophisticated than the bush but in

1728
02:06:21,330 --> 02:06:24,760
practice but has war's former properties

1729
02:06:24,820 --> 02:06:29,100
i'm not particularly happy with either one of them is the one i i would

