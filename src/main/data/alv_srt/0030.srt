1
00:00:00,000 --> 00:00:05,090
so the more you know

2
00:01:12,560 --> 00:01:17,000
the the seventh

3
00:01:54,540 --> 00:02:00,540
so the the

4
00:02:03,200 --> 00:02:06,640
you should be

5
00:02:08,270 --> 00:02:10,740
it's just that

6
00:02:11,600 --> 00:02:13,300
it's just because

7
00:02:15,450 --> 00:02:19,770
this one class of algorithms that we have been working on which of is many

8
00:02:19,770 --> 00:02:23,730
many whole at which is based on sparse coding which is a very old idea

9
00:02:23,730 --> 00:02:28,200
due to also has the field as an idea for learning these of i guess

10
00:02:28,210 --> 00:02:29,670
feature hierarchies

11
00:02:29,680 --> 00:02:31,360
so this

12
00:02:31,360 --> 00:02:33,580
unsupervised learning

13
00:02:33,600 --> 00:02:37,890
o which we start by taking natural images like like that you can actually check

14
00:02:37,890 --> 00:02:42,050
x shown on the left and we learn to decompose these images into a linear

15
00:02:42,050 --> 00:02:48,860
combination of a the small set the dictionary basis functions so on the right hand

16
00:02:48,860 --> 00:02:54,550
side of three examples of dictionary basis functions by sparse coding on what this is

17
00:02:54,550 --> 00:02:59,920
done has learned to decompose you know this natural image patch that is the combination

18
00:02:59,920 --> 00:03:02,700
of a small number of this dictionary races

19
00:03:02,710 --> 00:03:08,140
if you're on sparse coding on a large set of unlabeled images this is also

20
00:03:08,150 --> 00:03:12,670
basis functions you can correspond roughly to age detectors was he lost to texas but

21
00:03:12,670 --> 00:03:17,680
is not that exciting but when i think this was coding is that the hypothesized

22
00:03:17,680 --> 00:03:21,740
is that it's like patches of the called basic elements of the scene

23
00:03:21,760 --> 00:03:25,510
and those resident images in terms of the edges appear

24
00:03:25,520 --> 00:03:30,610
and therefore it gives us a higher level or most the same or abstract representation

25
00:03:30,610 --> 00:03:34,960
of an image that the raw pixel intensity values because now have a representation of

26
00:03:34,960 --> 00:03:37,850
an image in terms of the edges appear in a rather in terms of pixel

27
00:03:37,850 --> 00:03:43,980
intensity values and does unsurprisingly they can use large amounts of unlabeled data

28
00:03:43,980 --> 00:03:48,800
so more interestingly is of course if you can you also go from governor input

29
00:03:48,800 --> 00:03:55,510
image to detect edges one just things this idea is repeated in deep architecture so

30
00:03:55,610 --> 00:03:59,930
what we did was implement a sparse version of the deep belief network is given

31
00:03:59,930 --> 00:04:05,310
how at least where we go have information to detect edges it's even more higher

32
00:04:05,310 --> 00:04:10,060
commonly more high-level representations of combinations of edges

33
00:04:12,050 --> 00:04:16,460
one of the challenge is that and if you do this thing started the interesting

34
00:04:16,460 --> 00:04:17,920
feature hierarchies

35
00:04:19,370 --> 00:04:23,040
one of the challenges that we face in this work and i think community faces

36
00:04:23,040 --> 00:04:28,050
and scale these algorithms so turns out to be you know papers and this you

37
00:04:28,190 --> 00:04:32,670
a lot of people work on forty by forty images we work fourteen fourteen images

38
00:04:32,760 --> 00:04:36,180
some of us bite the bullet we really scale up to thirty two but the

39
00:04:36,190 --> 00:04:37,170
two images

40
00:04:37,190 --> 00:04:38,670
and you know

41
00:04:38,690 --> 00:04:42,130
if you learned from data about the two images there is really no there's some

42
00:04:42,130 --> 00:04:45,970
stuff but it's really not that much interesting things going on in those thirty about

43
00:04:45,970 --> 00:04:47,300
these two images

44
00:04:47,310 --> 00:04:48,590
only when you

45
00:04:48,610 --> 00:04:53,170
can scale up to large real size and more realistic sized images that is much

46
00:04:53,170 --> 00:04:55,360
more interesting structure to be learned

47
00:04:55,420 --> 00:05:00,730
so the main conference hall at present the papers of one method for scaling up

48
00:05:00,920 --> 00:05:05,050
which is using convolutional deviance and since he gave it to the main conferences someone

49
00:05:05,060 --> 00:05:11,320
to go over this in the two slides three slides so motivation is to

50
00:05:11,360 --> 00:05:15,970
use ideas from convolutional neural learning the scale up to larger

51
00:05:15,980 --> 00:05:18,600
unsurprisingly models

52
00:05:18,630 --> 00:05:20,310
measures a yorkshire

53
00:05:20,320 --> 00:05:24,400
school has to work on related conditional models as well

54
00:05:24,410 --> 00:05:25,980
o so

55
00:05:25,990 --> 00:05:31,160
the sea is is very quickly i guess given an input image we don't have

56
00:05:31,170 --> 00:05:35,420
feature detectors in the first layer of the out of the woods highways so these

57
00:05:35,420 --> 00:05:41,370
are convolved feature detector over the input image i intersect features clear and then select

58
00:05:41,420 --> 00:05:46,420
convolutional neural networks we don't have the pooling layer that compresses representation by factor two

59
00:05:48,980 --> 00:05:52,030
whereas a convolutional neural nets

60
00:05:52,050 --> 00:05:56,220
young and gives you which is very old algorithms there is this idea of max

61
00:05:56,220 --> 00:05:58,160
pooling which she would take

62
00:05:58,170 --> 00:06:01,000
four feature detectors and just compute the max

63
00:06:01,010 --> 00:06:06,900
right in a DBN we're not working these real numbers in the DBN working with

64
00:06:06,900 --> 00:06:09,060
these binary valued random variables

65
00:06:09,110 --> 00:06:14,180
and so rather than taking for real numbers we instead the max for binary valued

66
00:06:14,180 --> 00:06:15,810
random variables

67
00:06:15,860 --> 00:06:21,590
and in addition to make our more scalable where actually do we call probabilistic max

68
00:06:21,590 --> 00:06:26,690
pooling which is where we enforce constraint that the low-level features in this DBN are

69
00:06:26,690 --> 00:06:31,240
mutually exclusive so that only one of them can be turned on all the time

70
00:06:31,250 --> 00:06:36,750
and the fact that is that rather than these variables having two n possible combinations

71
00:06:36,790 --> 00:06:43,170
collapses into n possible configurations and so this makes learning inference much more efficient

72
00:06:43,180 --> 00:06:46,980
these are some of the

73
00:06:50,110 --> 00:06:59,490
do you think that'll work

74
00:06:59,550 --> 00:07:09,980
so here i think

75
00:07:09,990 --> 00:07:14,380
i have a hard right in the current implementation it doesn't overlap but that be

76
00:07:14,380 --> 00:07:16,360
a beach thing to do

77
00:07:19,660 --> 00:07:26,350
so when we created our one natural images these are the sorts representations you were

78
00:07:26,350 --> 00:07:31,040
the first there like everyone else we get edge detectors and the second there on

79
00:07:31,050 --> 00:07:35,100
the our river which now has the largest receptive fields well start to put together

80
00:07:35,110 --> 00:07:39,940
a short edges in order to form colonies in final on contours and so on

81
00:07:43,360 --> 00:07:44,480
more interestingly

82
00:07:44,480 --> 00:07:48,880
if you the sea in these experiments the first there was trained on natural images

83
00:07:48,880 --> 00:07:50,110
so the first there

84
00:07:50,120 --> 00:07:55,220
yes detectors and the second and third in areas where the and trains on objects

85
00:07:55,220 --> 00:08:00,370
of one specific object class in the leftmost column the second third is which trains

86
00:08:00,370 --> 00:08:05,940
on images of faces i see the second there so there are parts of faces

87
00:08:05,970 --> 00:08:10,360
by putting together edges and the third there then put together it is possible basis

88
00:08:10,360 --> 00:08:15,040
the more complete models of faces and if you do this with of you know

89
00:08:15,080 --> 00:08:16,380
other objects as well

90
00:08:16,400 --> 00:08:22,000
then again you see this pattern of learning object passes second air and that there

91
00:08:22,290 --> 00:08:24,380
are more complete objects of

92
00:08:24,390 --> 00:08:28,650
a more complete models of offerings is i think this is so this example of

93
00:08:28,650 --> 00:08:35,430
when these sort of object parts hierarchies emerge for completely unsupervised learning algorithm and the

94
00:08:35,430 --> 00:08:42,320
OK show will use this thing here given its early in the morning and

95
00:08:42,330 --> 00:08:47,210
voice is not quite up to speed

96
00:08:47,220 --> 00:08:52,220
OK so this is going to talk about two topics

97
00:08:52,240 --> 00:08:55,640
i've been working on

98
00:08:55,650 --> 00:08:57,750
so one is the question

99
00:08:57,770 --> 00:09:01,470
the thomas already been hinting at the question how

100
00:09:01,510 --> 00:09:08,190
we sample neural topographies using multivariate analysis using pattern analysis and the other question is

101
00:09:08,190 --> 00:09:14,820
how we can compare the information we extract using classifiers to any information we extract

102
00:09:14,820 --> 00:09:21,140
from cognitive paradigms for example social paradigms for the ideas that we directly relates the

103
00:09:21,140 --> 00:09:25,560
accuracy that we measure to the behaviour

104
00:09:28,080 --> 00:09:32,070
here a sampling of high-resolution neural maps with voxels

105
00:09:32,090 --> 00:09:34,450
it's going to be the first part of the talk

106
00:09:34,460 --> 00:09:36,680
so idea is we can actually

107
00:09:36,700 --> 00:09:41,860
virtually increase the resolution of fmri or we can actually use the right structure that

108
00:09:41,900 --> 00:09:45,200
much higher resolution than we might have thought previously

109
00:09:45,240 --> 00:09:49,860
and this is the second section comparing cognitive neural information i'll show you a number

110
00:09:49,860 --> 00:09:58,780
of paradigms just as examples where we directly compared the accuracy with cognitive accuracy

111
00:09:59,560 --> 00:10:01,810
i'll start looking in a very

112
00:10:02,160 --> 00:10:05,030
difficult case for from our research

113
00:10:05,040 --> 00:10:08,490
so this is orientation processing in the ones three fundamental

114
00:10:08,520 --> 00:10:14,180
aspect of neural processing visual processing is orientation processing one of the basic features of

115
00:10:14,180 --> 00:10:21,180
visual stimuli and so we've got orientation selectivity of single cells in the visual cortex

116
00:10:24,240 --> 00:10:27,700
and in one

117
00:10:27,790 --> 00:10:33,230
all o

118
00:10:33,250 --> 00:10:39,540
because has better so single cells have orientation selectivity which means that they respond preferentially

119
00:10:39,540 --> 00:10:44,410
to specific orientations but not to others and if we look at the orientation selectivity

120
00:10:44,420 --> 00:10:48,180
is distributed across the cortex you can see that here this is an optical imaging

121
00:10:48,180 --> 00:10:51,290
map obtained from monkey visual cortex

122
00:10:51,300 --> 00:10:56,800
you get such topography word is very fine scale topography where different colours indicate regions

123
00:10:56,800 --> 00:11:00,620
of different orientations selectivity and i'll show you problem

124
00:11:00,630 --> 00:11:05,500
if we sample the brain using these larger fmri voxels the problem we get run

125
00:11:05,500 --> 00:11:09,890
into is that this voxel is far too large to actually resolve these individual regions

126
00:11:09,890 --> 00:11:14,810
of different orientations selectivity so so for example want to zoom in on horizontal orientations

127
00:11:15,010 --> 00:11:17,820
we can't do that because if we is if you want to get the horizontal

128
00:11:17,820 --> 00:11:21,370
we also get all the other orientations same time so this is basically what people

129
00:11:21,460 --> 00:11:24,920
knew from right to study orientation processing

130
00:11:25,810 --> 00:11:32,860
and sir thomas already shown so work that originated primarily not just for us but

131
00:11:32,860 --> 00:11:37,020
mainly from you can italian in front on so if you go into visual cortex

132
00:11:37,020 --> 00:11:38,150
is just the sketch

133
00:11:38,300 --> 00:11:45,630
and you acquire pattern was a small set of voxels the responses a small set

134
00:11:45,630 --> 00:11:50,040
of voxels what you find is that they actually distinguish between different orientations although the

135
00:11:50,040 --> 00:11:52,800
spatial scale is quite different

136
00:11:52,820 --> 00:11:57,360
and the reason that happens is this is one of the possible models is that

137
00:11:57,370 --> 00:12:01,640
in fact that if the if you see the this this topographic map with regions

138
00:12:01,640 --> 00:12:07,150
of different orientations selectivity here with sampling with these voxels although the map is rather

139
00:12:07,150 --> 00:12:13,020
random we will still get slight fluctuations in the density of cells of different types

140
00:12:13,390 --> 00:12:17,100
and that will create a subtle bias in the measurement of these molecules and will

141
00:12:17,100 --> 00:12:21,470
allow us to record for each waffle has a subtle small

142
00:12:21,480 --> 00:12:26,600
information for example the top left the voxel about the orientation of the stimulus and

143
00:12:26,600 --> 00:12:30,730
the ideas of the pattern analysis allows us to accumulate this information across a large

144
00:12:30,730 --> 00:12:33,660
number of voxels that's the that's the idea

145
00:12:33,720 --> 00:12:39,740
so just show you just one representative voxel we can actually see that this orientation

146
00:12:39,740 --> 00:12:42,770
preference actually occurs at a single voxel level

147
00:12:42,780 --> 00:12:47,640
so this is basically the the angle shows the orientation of the stimulus and the

148
00:12:47,720 --> 00:12:52,400
eccentricity shows you responsible for example this box response stronger two left older than to

149
00:12:52,400 --> 00:12:54,620
write tilted similar so

150
00:12:54,640 --> 00:13:00,380
and single voxel if you do find these biases we can cross between microscopic topographic

151
00:13:00,380 --> 00:13:03,120
maps and voxels

152
00:13:03,130 --> 00:13:09,720
and these orientation biases of individual voxels their measured here using the t value for

153
00:13:09,720 --> 00:13:14,310
the difference between the response of each works between the two conditions so ultimately is

154
00:13:14,310 --> 00:13:19,380
t value captures whether the voxel responds strongly to one orientation astronomy to the other

155
00:13:19,380 --> 00:13:24,150
orientations and what you get is you get this distribution of these orientation selectivity is

156
00:13:24,310 --> 00:13:27,780
are not random and the easiest way to see that is if you split data

157
00:13:27,780 --> 00:13:31,420
set into two halves you can see the orientation preference in the first of the

158
00:13:32,480 --> 00:13:37,410
and the second half of the data highly correlated so basically these small preferences of

159
00:13:37,410 --> 00:13:41,210
individual voxels are quite reliable

160
00:13:41,260 --> 00:13:45,030
and that creates pattern so if we look at this on the cortical surface and

161
00:13:45,040 --> 00:13:48,330
the way we do that is we segment the cortical surface shown here in green

162
00:13:48,330 --> 00:13:51,730
so this lecture will be about what my plans

163
00:13:52,630 --> 00:13:54,510
this will be

164
00:13:54,550 --> 00:13:59,630
well as part of this thing will be logic much that users is not really

165
00:13:59,710 --> 00:14:04,120
what to this will be connected to some of the other three

166
00:14:07,130 --> 00:14:08,370
what planning

167
00:14:08,390 --> 00:14:11,830
most cases about is about

168
00:14:11,990 --> 00:14:14,240
deciding which is actually a

169
00:14:14,240 --> 00:14:16,120
so as to some

170
00:14:16,170 --> 00:14:18,490
even objective and

171
00:14:18,620 --> 00:14:21,990
yeah think it works in the order autonomous robots

172
00:14:22,030 --> 00:14:23,710
the same source

173
00:14:23,790 --> 00:14:26,850
thank you input from the same source

174
00:14:26,850 --> 00:14:34,040
and it was that these since the sequence of actions x

175
00:14:34,090 --> 00:14:39,630
well no normally there would also like to knowledge base is updated

176
00:14:39,640 --> 00:14:40,530
twenty states

177
00:14:40,530 --> 00:14:44,100
sensory input and then well

178
00:14:45,060 --> 00:14:48,990
is that knowledge is lost since they will fall

179
00:14:49,000 --> 00:14:54,100
the robot its knowledge base and with

180
00:14:54,120 --> 00:14:56,070
and the planning area

181
00:14:56,540 --> 00:14:58,460
the on

182
00:14:58,510 --> 00:15:01,630
general purpose methods for planning something

183
00:15:01,680 --> 00:15:09,820
input language planning system like PDDL is explicitly not for describing almost any kind of

184
00:15:09,990 --> 00:15:11,340
planning problems

185
00:15:12,260 --> 00:15:13,790
well understood

186
00:15:13,850 --> 00:15:15,120
of course

187
00:15:16,320 --> 00:15:21,130
and the idea is used to use this to a problem the

188
00:15:21,150 --> 00:15:31,150
the purpose out planning forestalling any instance in planning problem that che

189
00:15:31,170 --> 00:15:32,980
so this is

190
00:15:32,980 --> 00:15:34,280
a different school

191
00:15:34,290 --> 00:15:37,560
specialized on for solving particles

192
00:15:37,680 --> 00:15:42,070
so the idea is that of this language which problems and

193
00:15:42,070 --> 00:15:44,430
some of the are following

194
00:15:46,960 --> 00:15:48,760
examples of

195
00:15:48,760 --> 00:15:53,850
like problems people have been targeting planning research complex

196
00:15:53,870 --> 00:15:55,360
control of any kind of

197
00:15:55,390 --> 00:15:57,040
complex system

198
00:15:57,100 --> 00:16:03,750
examples would be like a NASA autonomous spacecraft they have some experiments where they have

199
00:16:03,750 --> 00:16:04,850
been using

200
00:16:04,890 --> 00:16:09,040
there are plenty for controlling the

201
00:16:09,520 --> 00:16:13,820
the complex systems currently sits on

202
00:16:13,830 --> 00:16:19,730
planning what actions to take control of the electricity distribution network when the out

203
00:16:20,690 --> 00:16:25,450
so the great makers we is have to be closed or open

204
00:16:25,600 --> 00:16:28,730
supply that as many people as possible

205
00:16:28,800 --> 00:16:31,640
they have been

206
00:16:31,670 --> 00:16:41,520
and in the manufacturing systems hull cool what actions to take manufacturer sort of

207
00:16:42,080 --> 00:16:46,670
in the little robots is all uppercase automated planning

208
00:16:46,730 --> 00:16:50,600
and all kinds of problem solving for example

209
00:16:50,690 --> 00:16:54,300
one user games like it you can be described in

210
00:16:54,320 --> 00:16:57,790
the input language planning systems

211
00:16:57,790 --> 00:16:59,160
and now lots of

212
00:16:59,170 --> 00:17:03,750
related problems like more narrowly on the planet and some respect so

213
00:17:03,820 --> 00:17:07,570
like just getting timetabling these are also

214
00:17:07,920 --> 00:17:11,100
problems about deciding

215
00:17:11,110 --> 00:17:19,750
when the they given that this all as efficiently as possible

216
00:17:19,760 --> 00:17:23,790
i'll have a couple of examples

217
00:17:23,820 --> 00:17:27,850
later multiple planning that and this is sort of simpler

218
00:17:28,070 --> 00:17:29,660
one of the most simple

219
00:17:30,170 --> 00:17:37,010
problem scenarios that can use for demonstrating planning this is a lot of

220
00:17:37,070 --> 00:17:42,100
blocks consists of this table and the number of blocks

221
00:17:42,110 --> 00:17:44,320
and we will be

222
00:17:44,380 --> 00:17:47,350
the idea is that lot can be stacked in different ways

223
00:17:48,950 --> 00:17:55,050
they don't use any geometric information about like beats according to four

224
00:17:55,070 --> 00:17:57,350
and the only thing that matters is whether

225
00:17:57,350 --> 00:17:59,380
so the block is on top of another

226
00:18:00,200 --> 00:18:01,770
of this matter

227
00:18:01,820 --> 00:18:05,180
and we also don't allow

228
00:18:07,450 --> 00:18:09,010
more than one block

229
00:18:09,010 --> 00:18:13,130
on top of of the one directly on top of another block so

230
00:18:13,140 --> 00:18:15,010
each block can be

231
00:18:15,020 --> 00:18:16,130
the only

232
00:18:17,280 --> 00:18:18,450
directly under

233
00:18:18,470 --> 00:18:19,210
i did

234
00:18:19,270 --> 00:18:22,210
below and director of all the works of the

235
00:18:22,370 --> 00:18:25,950
thirty years and is not loading blocks

236
00:18:27,270 --> 00:18:30,950
the variance of blogs have different number of blocks this

237
00:18:31,010 --> 00:18:33,330
example of what's wrong with three

238
00:18:33,390 --> 00:18:34,990
works and

239
00:18:35,000 --> 00:18:37,800
in these blocks are they are these thirteen

240
00:18:37,840 --> 00:18:39,740
it is possible states of the world

241
00:18:39,750 --> 00:18:40,830
so all the

242
00:18:40,880 --> 00:18:44,820
possible ways of backing those three blocks on the table

243
00:18:44,890 --> 00:18:48,010
one or more on on top of another block

244
00:18:48,020 --> 00:18:49,450
with the world

245
00:18:49,500 --> 00:18:53,220
considering because he had earlier so we need to look inside the book does not

246
00:18:53,240 --> 00:18:59,150
matter and only of what can be only one one another block the other way

247
00:19:02,220 --> 00:19:04,310
the planning problem in

248
00:19:04,410 --> 00:19:06,260
normally have been interested in

249
00:19:06,270 --> 00:19:08,010
is given in

250
00:19:08,720 --> 00:19:11,430
the initial state of the world

251
00:19:11,470 --> 00:19:15,010
let's say this that characterised by this

252
00:19:15,110 --> 00:19:18,310
it's like looks on the lower

253
00:19:18,370 --> 00:19:19,910
left-hand corner

254
00:19:19,930 --> 00:19:28,890
what are the actions that be taken at the some of the best also

255
00:19:28,900 --> 00:19:32,830
the other thing is that the world would be flexible the one on the on

256
00:19:33,760 --> 00:19:36,860
right most the world with with the

257
00:19:36,900 --> 00:19:41,400
block on top and then the red we

258
00:19:42,970 --> 00:19:48,320
in this book world the act different actors correspond to different woman so more so

259
00:19:48,680 --> 00:19:50,640
one action is because this

260
00:19:50,650 --> 00:19:52,760
look from one location and with

261
00:19:52,840 --> 00:19:56,570
some of of the fish and of course from this graph is c

262
00:19:57,070 --> 00:20:02,520
what is actually said was that people from the united nations so it simply follow

263
00:20:03,440 --> 00:20:08,210
arrows in the graph and these are of course one action and

264
00:20:09,090 --> 00:20:12,490
that's also planned

265
00:20:12,530 --> 00:20:14,130
this pretty simple

266
00:20:14,130 --> 00:20:18,490
why why why why is this the research problem and why why don't we just

267
00:20:19,720 --> 00:20:23,840
dard shortest allegory like doctor algorithm

268
00:20:23,840 --> 00:20:24,630
the great

269
00:20:24,640 --> 00:20:27,660
that would be the case that would be

270
00:20:27,710 --> 00:20:30,790
well as you can obviously be very happy

271
00:20:30,850 --> 00:20:33,770
they are

272
00:20:34,550 --> 00:20:37,890
they were pretty very happy about

273
00:20:37,900 --> 00:20:40,240
we're talking about the guy

274
00:20:40,680 --> 00:20:44,350
first we have see all the

275
00:20:48,950 --> 00:20:54,190
so you can use long-range in

276
00:20:54,230 --> 00:20:59,690
unfortunately what this immediately leads to a tractable comes in fact

277
00:20:59,700 --> 00:21:01,810
come the begin to make

278
00:21:01,860 --> 00:21:05,560
this is what you want to use of

279
00:21:05,580 --> 00:21:09,490
so it became very much on the specific application

280
00:21:09,560 --> 00:21:13,770
now what be very nice if you could just take some time

281
00:21:13,850 --> 00:21:17,900
tell you what don't want the right across the wall

282
00:21:17,910 --> 00:21:20,160
and to some extent people do that

283
00:21:20,210 --> 00:21:23,220
so a lot of

284
00:21:23,270 --> 00:21:24,870
bayesian networks

285
00:21:28,450 --> 00:21:32,500
well people are very very active working in this area

286
00:21:32,530 --> 00:21:35,850
the others were very well in practice

287
00:21:35,860 --> 00:21:37,230
there is

288
00:21:39,650 --> 00:21:41,920
just like very

289
00:21:41,970 --> 00:21:44,650
this it's very hard to point

290
00:21:44,660 --> 00:21:48,210
to prove optimality examples

291
00:21:48,270 --> 00:21:52,610
so you need to find good heuristics for

292
00:21:52,650 --> 00:21:56,030
and then the question is what is used to be one

293
00:21:56,090 --> 00:21:58,330
you can prove that is

294
00:21:58,350 --> 00:22:06,030
but that is something that

295
00:22:06,100 --> 00:22:18,180
i would like to mention that this is the information that was

296
00:22:18,950 --> 00:22:22,650
you don't have to

297
00:22:24,050 --> 00:22:25,650
also collection

298
00:22:30,060 --> 00:22:33,550
to model the log

299
00:22:33,600 --> 00:22:36,610
well this is what i previously

300
00:22:36,660 --> 00:22:39,450
mainly what we have is that

301
00:22:39,510 --> 00:22:44,760
what if i have some all year round one thing

302
00:22:44,980 --> 00:22:47,080
henri e

303
00:22:47,090 --> 00:22:49,110
they not all the

304
00:22:51,190 --> 00:22:54,750
one the things

305
00:22:55,880 --> 00:22:58,690
we also know know that it has to be

306
00:22:58,720 --> 00:23:01,010
decomposition property of graphical model

307
00:23:02,450 --> 00:23:06,690
can one of the

308
00:23:06,700 --> 00:23:09,210
overall they

309
00:23:09,280 --> 00:23:10,450
on the

310
00:23:13,470 --> 00:23:16,870
it follows from that

311
00:23:18,290 --> 00:23:21,800
due to both along the place that idea

312
00:23:21,810 --> 00:23:25,990
the problem of the

313
00:23:29,890 --> 00:23:31,010
it c

314
00:23:31,030 --> 00:23:35,610
what they see

315
00:23:35,650 --> 00:23:37,080
and then

316
00:23:38,270 --> 00:23:39,460
he that

317
00:23:39,480 --> 00:23:40,960
one of the

318
00:23:42,220 --> 00:23:46,090
now you might think that this completely option made by

319
00:23:48,190 --> 00:23:49,690
it's actually what

320
00:23:49,710 --> 00:23:51,280
i mean first of all you

321
00:23:51,300 --> 00:23:59,130
he had the position including one but one can always assume that some facts fig

322
00:23:59,130 --> 00:24:00,780
and we have should

323
00:24:01,020 --> 00:24:05,330
and there's a universal approximation result

324
00:24:05,390 --> 00:24:07,400
those certain sufficient statistic

325
00:24:07,410 --> 00:24:08,690
it shares

326
00:24:08,720 --> 00:24:10,890
that is

327
00:24:10,900 --> 00:24:16,670
can be well approximated with a financial and small

328
00:24:16,680 --> 00:24:20,780
at the moment for that is not

329
00:24:20,800 --> 00:24:25,310
so in the exponential family of the year fixed term in front so you can

330
00:24:25,310 --> 00:24:30,720
just look the anything that will approximate the exactly right now it's not that way

331
00:24:30,720 --> 00:24:31,960
that the

332
00:24:31,970 --> 00:24:33,360
so that

333
00:24:35,090 --> 00:24:36,860
i mean

334
00:24:36,880 --> 00:24:39,010
the bottom line is

335
00:24:39,030 --> 00:24:40,940
you have

336
00:24:40,980 --> 00:24:42,690
well just like the fact that

337
00:24:42,690 --> 00:24:47,210
contrast that to the mean producing spaces

338
00:24:47,470 --> 00:24:49,260
things in

339
00:24:49,280 --> 00:24:51,160
these easier zero cost

340
00:24:51,190 --> 00:24:53,450
a continuous functions

341
00:24:53,550 --> 00:24:54,630
on the right

342
00:24:54,640 --> 00:24:55,670
you have

343
00:24:55,690 --> 00:24:56,740
are you

344
00:24:56,760 --> 00:24:59,470
correspondingly that

345
00:25:00,380 --> 00:25:01,600
some part of

346
00:25:01,620 --> 00:25:06,460
by will be essentially the same despite all these

347
00:25:06,490 --> 00:25:09,340
what some constraints may

348
00:25:10,380 --> 00:25:12,830
so prove that in the television

349
00:25:12,840 --> 00:25:15,090
the hard for me in terms of the

350
00:25:15,110 --> 00:25:18,960
the distance is rather easy

351
00:25:19,220 --> 00:25:23,600
at the moment there are only problem or

352
00:25:23,610 --> 00:25:28,450
proves anything that's all you can really consider them constructive

353
00:25:29,270 --> 00:25:33,170
finding good right despite all this this will be wonderful

354
00:25:33,180 --> 00:25:34,890
at the moment nobody's

355
00:25:34,910 --> 00:25:38,150
so that it is not known

356
00:25:38,200 --> 00:25:39,010
the paper

357
00:25:39,050 --> 00:25:40,990
tracks song

358
00:25:40,990 --> 00:25:41,770
the back

359
00:25:41,780 --> 00:25:43,020
sort of the back

360
00:25:43,040 --> 00:25:47,700
so in one one of the things that you would be seeing here

361
00:25:47,710 --> 00:25:51,340
this is after all a summer school so you sort of interest in learning about

362
00:25:51,340 --> 00:25:54,300
all sorts of machine learning algorithms and so

363
00:25:54,430 --> 00:25:58,320
in terms of the coverage you would wind up saying that is more of knowledge

364
00:25:58,320 --> 00:26:03,530
based learning algorithms that we talked about in this picture then let's say a couple

365
00:26:03,530 --> 00:26:06,560
of hours of a you start totally backtracking

366
00:26:06,570 --> 00:26:11,090
the other thing that sort of interesting in terms of machine learning algorithms have been

367
00:26:11,090 --> 00:26:15,810
used in planning learning intersection is most of them have been relational

368
00:26:15,920 --> 00:26:21,160
it turns out that you will see as you see in planning most of the

369
00:26:21,160 --> 00:26:26,380
time the background the domain knowledge itself is given as actions can which has which

370
00:26:26,380 --> 00:26:32,150
are very easily understood in some of this relation languages and so much of the

371
00:26:32,150 --> 00:26:36,190
learning in plan it much of the learning algorithms have been relational OK so that's

372
00:26:36,190 --> 00:26:40,830
the other aspects of in terms of coverage you learn a bit more about relational

373
00:26:40,830 --> 00:26:45,630
learning algorithms and a bit more about knowledge based on meaning algorithms

374
00:26:48,500 --> 00:26:54,370
so i also i mean i guess should say a little about my background i

375
00:26:54,370 --> 00:27:00,810
already told you my large number of ICML publications i want to tell you a

376
00:27:00,810 --> 00:27:06,010
few more things basically my primary interest is in automated planning and i do a

377
00:27:06,010 --> 00:27:10,310
lot of work on plants and this is improving the speed at which you can

378
00:27:10,310 --> 00:27:15,460
not be generated plan given the problem and in the scaling of the performance of

379
00:27:15,460 --> 00:27:16,440
this planets

380
00:27:16,590 --> 00:27:21,350
i have for quite a long time in the past i have looked that actually

381
00:27:21,350 --> 00:27:26,290
speed of learning in the context of learning and i do have talk about it

382
00:27:26,290 --> 00:27:29,160
a bit but you know i have a lot of interest in

383
00:27:29,170 --> 00:27:32,010
speeding up learning performance

384
00:27:32,020 --> 00:27:37,010
and i'll show you did later that there is an interesting

385
00:27:37,060 --> 00:27:42,920
they at which this work has been done at one time planning algorithms was really

386
00:27:42,920 --> 00:27:45,190
really really really really slow

387
00:27:45,240 --> 00:27:49,930
and when you slow you want to see if there's any way can improve performance

388
00:27:49,930 --> 00:27:55,560
by learning so learning you was pretty much required crutch for planning programs at one

389
00:27:55,560 --> 00:28:01,070
point of time that would be like pre nineteen ninety four ninety five frame and

390
00:28:01,070 --> 00:28:04,340
there's a lot of work in speed up learning i'm planning that was done during

391
00:28:04,340 --> 00:28:08,920
that time when i was also very much into that over the period of time

392
00:28:08,920 --> 00:28:13,070
intervening period of time we lost interest and some of that because planners is just

393
00:28:13,070 --> 00:28:16,900
out of the box of much much faster this is much faster and i'll tell

394
00:28:16,900 --> 00:28:20,990
you exactly why they became pastor and one of the things so we sort of

395
00:28:20,990 --> 00:28:23,880
lost interest at least in the speed of part i mean it's not that you

396
00:28:23,890 --> 00:28:27,970
learning itself is going on the body it's no longer necessary crutch just to be

397
00:28:27,970 --> 00:28:31,830
able to do this of planet but then

398
00:28:31,840 --> 00:28:37,070
i am currently freshly interested like many other people in actually

399
00:28:37,090 --> 00:28:41,570
in the intersection of planning and learning for two different reasons one as i will

400
00:28:41,570 --> 00:28:46,970
tell you is that a new crop of learning algorithm that in the of his

401
00:28:47,270 --> 00:28:51,040
research efforts to basically shown that has

402
00:28:51,090 --> 00:28:55,660
impressive as the scale up in planning has been you can still improve

403
00:28:55,920 --> 00:29:00,430
you know which much more to get them to even faster speed ups and part

404
00:29:00,430 --> 00:29:04,000
of it has been as early as we can see that in the part of

405
00:29:04,000 --> 00:29:09,600
it has been because we have this competition for planning performance garnered international planning competition

406
00:29:10,060 --> 00:29:14,740
and there was there are two two tracks one track with essentially domain independent all

407
00:29:14,970 --> 00:29:16,940
tell the planet is

408
00:29:16,950 --> 00:29:18,120
your actions

409
00:29:18,130 --> 00:29:22,310
and then it's given the problem and the goal just supposed to sequence actions of

410
00:29:22,360 --> 00:29:27,980
actions to get the plan there was another track knowledge based planning which was held

411
00:29:27,990 --> 00:29:31,510
at least forty two competitions and real people

412
00:29:31,520 --> 00:29:35,150
what LOD to right on top of the

413
00:29:35,200 --> 00:29:39,610
the main action descriptions anything else they want to tell the plan such as you

414
00:29:39,610 --> 00:29:44,370
take these branches when you're doing search do the planning in the following way a

415
00:29:44,370 --> 00:29:48,500
lot of extra control knowledge could be given now the interesting thing is this was

416
00:29:48,500 --> 00:29:50,400
being done by humans

417
00:29:50,440 --> 00:29:55,240
so humans would write his knowledge and what's interesting of course is we saw that

418
00:29:55,240 --> 00:29:59,660
these sorts of knowledge based planners were significantly faster

419
00:29:59,670 --> 00:30:01,340
then the domain

420
00:30:01,350 --> 00:30:02,660
independent plants

421
00:30:02,680 --> 00:30:06,690
and so that again opened up an entire interest in

422
00:30:06,710 --> 00:30:11,760
learning that knowledge that people are writing by hand let's try to learn that automatically

423
00:30:11,770 --> 00:30:16,860
so that would be another reason why people got quite interested in

424
00:30:18,250 --> 00:30:21,840
and learning right now and tell you some of the work that's going on that

425
00:30:21,840 --> 00:30:24,530
i think is pretty neat

426
00:30:24,540 --> 00:30:30,840
and then i'm also interested like many other people of late in actually learning not

427
00:30:32,510 --> 00:30:36,020
know that much of the work in the speed of learning has happened with this

428
00:30:36,020 --> 00:30:37,000
right on

429
00:30:38,140 --> 00:30:40,790
definition here

430
00:31:00,040 --> 00:31:04,430
so we just

431
00:31:08,100 --> 00:31:17,000
this is true here

432
00:31:22,460 --> 00:31:25,640
OK so now let's try to prove these four

433
00:31:25,660 --> 00:31:29,710
different properties and i'll i'll give it some time for this but maybe i'll give

434
00:31:29,710 --> 00:31:32,000
you a couple of minutes before

435
00:31:32,000 --> 00:31:36,770
and how to proceed so there

436
00:31:36,790 --> 00:31:40,810
once you've seen the film afterwards you're probably say they trivial but if you have

437
00:31:40,810 --> 00:31:45,410
never seen this definition of a positive definite kernel before then it would be non-trivial

438
00:31:45,410 --> 00:31:48,330
to think about it and write them down even though

439
00:31:48,390 --> 00:31:52,180
each one of them is maybe just two lines so

440
00:31:52,210 --> 00:31:55,500
so the first one is we have to prove that this is a positive definite

441
00:31:56,290 --> 00:32:00,870
to prove that this is a positive definite kernel we just have to verify that

442
00:32:00,940 --> 00:32:02,730
this condition he holds true

443
00:32:02,750 --> 00:32:05,600
for a kernel which is defined like that

444
00:32:05,600 --> 00:32:09,370
so that's the direct verification

445
00:32:09,390 --> 00:32:12,850
have to use things like linearity of the dot product and so on

446
00:32:14,250 --> 00:32:15,810
the next one

447
00:32:15,850 --> 00:32:20,910
this may be slightly more tricky even though it turns out this very simple if

448
00:32:22,460 --> 00:32:26,350
so in this next one we have to prove that effectively the

449
00:32:26,370 --> 00:32:28,810
if the kernel is positive definite

450
00:32:28,830 --> 00:32:30,560
so here in

451
00:32:30,600 --> 00:32:36,770
if the kernel is positive definite then its diagonal elements are positive or negative so

452
00:32:36,790 --> 00:32:42,430
diamonds scale xmax or if you think of the kernel matrix is the diagonal elements

453
00:32:42,430 --> 00:32:45,160
of the kernel matrix

454
00:32:45,180 --> 00:32:50,290
so here in all these three cases we assume is positive definite little incomplete so

455
00:32:50,290 --> 00:32:52,830
i should have written here so here

456
00:32:52,850 --> 00:32:56,910
this is a complete statement but the statement should also a if k

457
00:32:56,960 --> 00:32:59,000
is positive definite then

458
00:32:59,020 --> 00:33:02,640
this holds true this of this so here

459
00:33:02,700 --> 00:33:03,810
there is

460
00:33:03,830 --> 00:33:08,410
a simple proof but maybe i won't tell you know because it's hard to give

461
00:33:08,410 --> 00:33:12,200
him without giving away the whole idea is very simple but you think about it

462
00:33:12,200 --> 00:33:13,830
a a little bit

463
00:33:13,850 --> 00:33:15,040
the second one

464
00:33:15,080 --> 00:33:17,540
i have given you a hint here

465
00:33:17,560 --> 00:33:24,620
it's with this hint it should reasonably simple but still requires some thinking

466
00:33:26,270 --> 00:33:30,960
and this is the general generalized cushy schwartz inequality

467
00:33:30,960 --> 00:33:32,580
and you can be seen from here

468
00:33:32,600 --> 00:33:35,210
and the last one

469
00:33:35,250 --> 00:33:41,640
is this property that if a kernel is zero on the diagonal

470
00:33:41,760 --> 00:33:46,040
for all possible many then it's actually zero everywhere

471
00:33:46,060 --> 00:33:47,480
so it's

472
00:33:47,540 --> 00:33:52,790
the last properties of leave these up on the screen and again try to remember

473
00:33:52,790 --> 00:33:56,810
whether i can separate myself and give you some time to to the process mean

474
00:33:58,410 --> 00:34:01,890
and then we'll be together

475
00:38:57,500 --> 00:39:02,390
as you seem to be you and spare this

476
00:39:04,580 --> 00:39:06,540
in that time

477
00:39:06,660 --> 00:39:07,680
could then

478
00:39:09,330 --> 00:39:12,520
i have to draw brought

479
00:39:12,540 --> 00:39:17,910
thank you

480
00:39:24,080 --> 00:39:33,270
the output the room in the knowledge sources into thinking he on wednesday

481
00:39:33,290 --> 00:39:36,180
the only one

482
00:39:36,350 --> 00:39:39,730
one the book

483
00:39:39,730 --> 00:39:41,920
PTFE teflon minus 60

484
00:39:42,710 --> 00:39:46,230
a 1st thing to notice is to look at the range and if you look

485
00:39:46,230 --> 00:39:49,910
at the range in the periodic table of the range of something like density so

486
00:39:49,910 --> 00:39:54,790
you have lithium scattered density . 5 3 and you got osmium that's around 22

487
00:39:54,830 --> 00:40:00,410
so a range of about 40 per but this 10 to the 7th 10 to

488
00:40:00,410 --> 00:40:04,210
the minus 16 a range of 10 to the 20 3rd

489
00:40:04,330 --> 00:40:06,600
10 to the 23rd

490
00:40:07,710 --> 00:40:12,770
what we know about the compounds we've seen so far ionic compounds

491
00:40:12,790 --> 00:40:18,680
conductors are insulated we know the they got tightly bound electrons in a stable octet

492
00:40:18,680 --> 00:40:22,710
configurations so that's not going to help us covalent compounds

493
00:40:22,770 --> 00:40:27,290
well that work ones like diamonds very very good insulators

494
00:40:27,380 --> 00:40:32,560
on compounds let's look at that a argon they got the answer would be a

495
00:40:32,560 --> 00:40:38,100
good conductors of electrons know why extended gas configuration those of the 3 types of

496
00:40:38,100 --> 00:40:43,770
primary bonding and I can explain how three-quarters of periodic table has this abnormally high

497
00:40:43,770 --> 00:40:48,400
value of electrical conductivity sounds to me like we need to come up with yet

498
00:40:48,440 --> 00:40:52,580
another form of bonding

499
00:40:52,730 --> 00:40:58,440
and the answer is metallic bonding wiser metallic bonding because it's practiced in metals so

500
00:40:58,440 --> 00:41:04,540
let's look at metallic bonding the need for metallic bonding was brought on by the

501
00:41:04,540 --> 00:41:09,900
observation of electrical conductivity so if you're going to be true to form a 3

502
00:41:09,900 --> 00:41:13,170
0 9 1 we have to look at the electronic structure

503
00:41:13,360 --> 00:41:17,520
and the 1st model the 1st model that

504
00:41:17,830 --> 00:41:26,910
attempted to account for the highly highly observed value of the electrical conductivity was enunciated

505
00:41:26,910 --> 00:41:32,190
in 1900 by German physicist by the name of Paul

506
00:41:32,500 --> 00:41:36,190
pultruded in 1900

507
00:41:36,430 --> 00:41:40,750
what he said was that party could model metal

508
00:41:40,810 --> 00:41:47,790
a mixture model the metal has a mixture of

509
00:41:48,020 --> 00:41:51,350
positive nuclei

510
00:41:52,440 --> 00:42:00,420
plus the inner shell electrons for the inner shell electrons which he called this whole

511
00:42:00,420 --> 00:42:05,920
mix of the positive nuclei all the inner shell electrons he called that the atomic

512
00:42:06,270 --> 00:42:12,400
core and then the valence electrons he said were separate so you get the atomic

513
00:42:12,400 --> 00:42:20,670
or plus valence electrons which he assumed were free

514
00:42:20,670 --> 00:42:29,560
were free that is to say they could move about the material number words they

515
00:42:29,560 --> 00:42:33,270
were not associated with any single

516
00:42:33,950 --> 00:42:38,530
so we call them the localiza The Glass they're used the term the localized

517
00:42:38,540 --> 00:42:45,400
we'll come back to that in a bit so this is in essence of modeling

518
00:42:45,420 --> 00:42:47,540
the metal as

519
00:42:47,560 --> 00:42:48,850
I solid

520
00:42:48,850 --> 00:42:54,540
consisting of course and the electrons freely moving much the same way as a gas

521
00:42:54,540 --> 00:43:00,580
so this was a tournament argued that turned the free electron gas model number gas

522
00:43:00,580 --> 00:43:06,360
of free electron free electron gas what and what this they had some success they

523
00:43:06,360 --> 00:43:12,980
were able to account for such things as the heat capacity of solids so he

524
00:43:13,000 --> 00:43:16,690
capacities of solids they could they could model

525
00:43:16,850 --> 00:43:21,500
that was good and I think there's something else you're able to do

526
00:43:22,170 --> 00:43:29,080
but by and large but this didn't stand up because it wasn't quantitative

527
00:43:29,150 --> 00:43:32,440
so there's not quantitative what's happening right off the bat

528
00:43:32,600 --> 00:43:37,540
Nobel Prize you get no Nobel Prize reported no quantitative Nobel Prize will just put

529
00:43:37,540 --> 00:43:38,900
this year knows

530
00:43:38,900 --> 00:43:43,810
those shown in the bottom of the maybe be no causal relationship

531
00:43:43,820 --> 00:43:46,870
you know actions the kind of leader one can cause the other

532
00:43:46,920 --> 00:43:51,830
so looks grown right and i've been bragging about learning causes announcements this example really

533
00:43:51,830 --> 00:43:56,190
can't learn any causes all

534
00:43:56,240 --> 00:44:00,140
let's look at this more complex examples what i said the beginning of the talk

535
00:44:00,140 --> 00:44:03,630
is that we can learn something about classes when we have data

536
00:44:03,680 --> 00:44:06,570
and at least four variables

537
00:44:06,670 --> 00:44:08,210
you need for

538
00:44:08,230 --> 00:44:11,130
suppose these are four variables

539
00:44:11,150 --> 00:44:14,250
and x and y are independent then w

540
00:44:14,310 --> 00:44:16,230
is independent of x and y

541
00:44:16,280 --> 00:44:17,970
given c

542
00:44:17,980 --> 00:44:22,320
this was again an example we looked at before

543
00:44:22,410 --> 00:44:26,830
p is embedded faithfully in the DAG in a and in b

544
00:44:26,840 --> 00:44:28,700
this is that we learned before

545
00:44:28,710 --> 00:44:32,050
right x because y like this is because w

546
00:44:32,070 --> 00:44:35,120
this can be hidden cost

547
00:44:35,170 --> 00:44:37,810
the same independencies

548
00:44:37,860 --> 00:44:39,730
as this one

549
00:44:41,390 --> 00:44:44,950
and why are independent given by the argument i gave before

550
00:44:45,000 --> 00:44:50,690
they still for the independent owing to the separation and when i condition and the

551
00:44:50,690 --> 00:44:55,920
that strict state of the markov condition w is independent of its non descendants which

552
00:44:55,920 --> 00:44:57,340
included x and y

553
00:44:57,350 --> 00:44:58,850
given c

554
00:44:58,930 --> 00:45:01,920
right so all these dags

555
00:45:02,870 --> 00:45:05,480
these conditional independencies

556
00:45:05,490 --> 00:45:08,020
so these two

557
00:45:08,030 --> 00:45:14,000
causal inferences are not necessarily causal influences could be hidden because just like before

558
00:45:14,010 --> 00:45:16,800
but p is not embedded faithfully

559
00:45:16,810 --> 00:45:19,850
in the i can see

560
00:45:19,870 --> 00:45:26,560
this kind of tricky that entails the w is independent of x and y

561
00:45:26,610 --> 00:45:28,660
just like i said before

562
00:45:29,530 --> 00:45:38,780
land and sea

563
00:45:38,830 --> 00:45:41,710
these two variables are independent right

564
00:45:41,750 --> 00:45:43,370
it's like the earthquake

565
00:45:43,460 --> 00:45:47,340
burglar h one and h three are independent

566
00:45:47,350 --> 00:45:51,420
and since there are independent given by d separation w

567
00:45:51,440 --> 00:45:53,540
and x are independent

568
00:45:53,550 --> 00:45:55,040
so say i can't

569
00:45:55,050 --> 00:45:58,570
take this one can replace it by hidden cause

570
00:45:59,830 --> 00:46:04,050
even if i go with this zero that i know what you too many dags

571
00:46:04,880 --> 00:46:07,220
that went directly there would still say

572
00:46:07,260 --> 00:46:10,400
the same thing because that's what what

573
00:46:10,450 --> 00:46:13,090
i would say x and h three are independent

574
00:46:13,100 --> 00:46:17,080
and then by d separation w was also independent of x

575
00:46:17,090 --> 00:46:18,640
the bottom line is

576
00:46:18,680 --> 00:46:22,260
this page has to go and here

577
00:46:22,310 --> 00:46:25,330
whether it's true causing now

578
00:46:25,350 --> 00:46:27,440
it has to be an arrow into

579
00:46:28,180 --> 00:46:30,230
from the top

580
00:46:30,280 --> 00:46:31,700
if i replace

581
00:46:31,710 --> 00:46:33,360
the COI policy

582
00:46:33,410 --> 00:46:37,050
i know into to the which is called the head to head meeting

583
00:46:37,060 --> 00:46:40,610
i'm going to make xnw independent

584
00:46:40,630 --> 00:46:42,580
so i can take this

585
00:46:42,620 --> 00:46:45,040
arrow and make it go up

586
00:46:45,110 --> 00:46:48,840
the hidden cost just like i can make it go up without having because they

587
00:46:48,850 --> 00:46:52,530
can make it go up with the hidden cost because and up with this head-to-head

588
00:46:59,830 --> 00:47:01,740
we can still conclude

589
00:47:01,760 --> 00:47:05,270
the causes w

590
00:47:05,950 --> 00:47:08,730
this is a causal relationship can be learned

591
00:47:08,790 --> 00:47:14,170
these two can that but again now if i had long linguistic variables here all

592
00:47:14,170 --> 00:47:15,780
of them would be good

593
00:47:15,790 --> 00:47:20,930
it's just these initial ones at the top that are not good

594
00:47:20,980 --> 00:47:21,980
and that's

595
00:47:22,090 --> 00:47:25,720
the argument i gave before that we can learn something if we had at least

596
00:47:25,720 --> 00:47:27,760
four variables

597
00:47:28,010 --> 00:47:30,670
here it is

598
00:47:36,160 --> 00:47:42,740
i mean if i have a hidden causes in the chain here b

599
00:47:45,240 --> 00:47:46,020
these two

600
00:47:47,270 --> 00:47:50,180
eight if thousand an error from h one h two

601
00:47:50,180 --> 00:47:51,740
you wouldn't have

602
00:47:51,760 --> 00:47:53,450
these independencies

603
00:48:00,320 --> 00:48:04,680
well if we have these independencies assuming it can be hidden causes present but

604
00:48:04,720 --> 00:48:09,510
they couldn't be ones like this because you wouldn't observe those independencies

605
00:48:09,530 --> 00:48:13,320
so we're assuming whatever they are the such a nature that this is what we

606
00:48:16,910 --> 00:48:26,020
well making no assumption about the causes of oxygen might be present but whatever they

607
00:48:26,020 --> 00:48:29,570
are they have to be present in such a way that

608
00:48:29,620 --> 00:48:39,370
these are the observed dependencies

609
00:48:39,390 --> 00:48:43,280
this example is entirely with time

610
00:48:43,310 --> 00:48:45,780
this is this is what i had before

611
00:48:45,800 --> 00:48:47,190
what i learned

612
00:48:48,520 --> 00:48:52,830
you know it wasn't there i think now we realize that means there is a

613
00:48:52,830 --> 00:48:54,470
hidden common cause

614
00:48:54,480 --> 00:48:59,510
once we make the causal embedded faithfulness assumption we i simply observe independencies means c

615
00:48:59,510 --> 00:49:03,640
and w have a hidden common causes the the only explanation

616
00:49:03,690 --> 00:49:05,660
in my example was

617
00:49:05,730 --> 00:49:10,890
as one can see

618
00:49:10,960 --> 00:49:13,260
and i'm going to get some some

619
00:49:13,300 --> 00:49:17,060
quote real life examples this is not really a real life example but the next

620
00:49:17,060 --> 00:49:18,220
two are

621
00:49:18,270 --> 00:49:21,860
this is an example may not just illustrate how you could learn that smoking causes

622
00:49:21,860 --> 00:49:24,030
lung cancer

623
00:49:24,040 --> 00:49:26,260
well it's just assume that

624
00:49:26,310 --> 00:49:32,610
but things that made this example i i saw something in particular but likely more

625
00:49:32,610 --> 00:49:38,650
there's an actual study very similar variables indicating smoking causes lung cancer but made this

626
00:49:38,650 --> 00:49:40,810
up because it easy to track

627
00:49:40,930 --> 00:49:43,210
let's say you have these for variables

628
00:49:43,220 --> 00:49:46,170
our which is the parents smoking history

629
00:49:46,220 --> 00:49:48,680
a which is alcohol consumption

630
00:49:48,740 --> 00:49:51,690
individual smoking-related lung cancer

631
00:49:51,790 --> 00:49:55,540
and suppose we learn from data that these are independent

632
00:49:55,550 --> 00:49:57,050
whether person

633
00:49:57,100 --> 00:50:00,430
carrying small is independent of whether the individual

634
00:50:00,440 --> 00:50:02,020
the child

635
00:50:02,080 --> 00:50:03,600
as opposed

636
00:50:03,640 --> 00:50:07,570
right so those are independent and we have and will end up learning that one

637
00:50:07,570 --> 00:50:10,960
cancer is independent of both of these

638
00:50:11,010 --> 00:50:12,960
conditional on smoking

639
00:50:12,970 --> 00:50:15,610
the along cancer is not independent of whether the

640
00:50:15,610 --> 00:50:19,730
current small one cancer is also not independent of whether person

641
00:50:20,850 --> 00:50:24,600
but these two are independent once we know the person smokes

642
00:50:24,660 --> 00:50:28,300
right so just like there is an actual real life example that is very similar

643
00:50:28,300 --> 00:50:30,180
to this but let's just

644
00:50:30,220 --> 00:50:33,380
and this is just say this is true

645
00:50:33,430 --> 00:50:36,350
we will learn the graph on the right

646
00:50:36,430 --> 00:50:39,540
this is very similar to what i was just like the example i had on

647
00:50:39,540 --> 00:50:43,330
the previous slide i replace x y c and w

648
00:50:43,420 --> 00:50:45,760
they are a

649
00:50:45,810 --> 00:50:48,440
why did why is this graph look different

650
00:50:48,480 --> 00:50:51,010
because now i'm using

651
00:50:51,020 --> 00:50:56,640
notation that i use when i'm actually learning causal influences

652
00:50:56,680 --> 00:50:59,010
by one hundred year-old

653
00:50:59,050 --> 00:51:00,570
what i mean is

654
00:51:01,370 --> 00:51:04,730
that the causal DAG could be that causes as

655
00:51:04,820 --> 00:51:07,810
or it could be a hidden common cause

656
00:51:07,860 --> 00:51:11,460
that's the notation are used for the for the the fact that i don't know

657
00:51:11,460 --> 00:51:14,870
what's going on right i don't know what causes us

658
00:51:14,890 --> 00:51:19,240
well there is a hidden common cause but i know one of them is true

659
00:51:19,240 --> 00:51:25,240
places that look good but also explore places that has looked at much

660
00:51:25,250 --> 00:51:29,700
what is the value of this essay is the average

661
00:51:29,730 --> 00:51:32,110
the different things

662
00:51:32,280 --> 00:51:33,400
right so

663
00:51:33,850 --> 00:51:35,690
so it works out

664
00:51:36,180 --> 00:51:39,270
theoretically believed to say for now that

665
00:51:39,320 --> 00:51:43,030
you destroy the average of what you say you can do different types of backups

666
00:51:43,030 --> 00:51:45,250
if you want to UCT does this

667
00:51:49,580 --> 00:51:54,520
basically the tree policy might say well i guess i'll go down here because it's

668
00:51:54,530 --> 00:51:58,160
looked more promising and it selects node

669
00:51:58,180 --> 00:52:02,020
and now we're the state where we have it's like that all the actions so

670
00:52:02,020 --> 00:52:04,150
it's going to just generate this

671
00:52:04,160 --> 00:52:07,070
and run the default policy right

672
00:52:07,080 --> 00:52:12,570
there are trees bigger we update our values and perhaps now at the root

673
00:52:12,580 --> 00:52:16,490
and know maybe it would select this again

674
00:52:16,540 --> 00:52:21,600
and now maybe based on the same maybe it would explore this path and so

675
00:52:21,600 --> 00:52:25,620
on so there's always two distinct phases the tree policy that will sort of find

676
00:52:25,620 --> 00:52:29,820
the first leaf and the default policy that once you find all run to the

677
00:52:30,700 --> 00:52:33,660
and the values we store the averages

678
00:52:33,750 --> 00:52:37,280
so that the critical question is what is the appropriate role of policy and what

679
00:52:37,280 --> 00:52:43,410
is an appropriate tree policy and UCT is a particular choice of those two things

680
00:52:43,440 --> 00:52:49,530
so the basic UCT algorithm is basically just going to use a random rollout policy

681
00:52:49,530 --> 00:52:57,650
that's theoretically what what what uses you you typically use something that's informed

682
00:52:57,680 --> 00:53:00,400
the tree policy is based on UCB

683
00:53:00,420 --> 00:53:05,330
so remember every state we encounter we can keep count of how many times we've

684
00:53:05,330 --> 00:53:06,940
been there

685
00:53:06,940 --> 00:53:10,770
that is what we're going to store here so state as we've been there at

686
00:53:10,770 --> 00:53:14,560
this time we can also keep to count on how many times we tried each

687
00:53:14,560 --> 00:53:19,150
action that's an essay and we can keep counts on the average reward we've seen

688
00:53:19,250 --> 00:53:22,150
we've got to that state that's qs a

689
00:53:23,660 --> 00:53:25,400
and the UCT

690
00:53:25,400 --> 00:53:27,830
o tree policy is simply the

691
00:53:29,660 --> 00:53:33,520
except theoretically you have to have a constant here

692
00:53:35,850 --> 00:53:41,570
theoretically these these these payoffs you get when you pull the arms are not independent

693
00:53:41,570 --> 00:53:48,150
from one another and they are also not stationary so so theoretically the main contribution

694
00:53:48,150 --> 00:53:51,980
is to show that the same form works as long as you have a constant

695
00:53:51,990 --> 00:53:54,830
the theoretical constant is the horizon

696
00:53:54,860 --> 00:53:57,140
typically this is something you too

697
00:53:57,150 --> 00:54:02,870
for a particular application domain just try bunch of values is this the best value

698
00:54:02,870 --> 00:54:04,700
for go for example

699
00:54:04,710 --> 00:54:06,540
so let's

700
00:54:06,560 --> 00:54:11,560
UCT and so just to show you one more view of this

701
00:54:11,570 --> 00:54:15,950
so i suppose that this is our current tree we've already generating

702
00:54:15,950 --> 00:54:18,790
the city now wants to do another policy

703
00:54:18,820 --> 00:54:21,860
it starts at the root has all the statistics

704
00:54:21,870 --> 00:54:24,040
you have the bandit problem here

705
00:54:24,090 --> 00:54:26,870
to choose the summer this are and

706
00:54:26,900 --> 00:54:29,350
basically use this rule

707
00:54:29,350 --> 00:54:32,610
it will choose one of the arms

708
00:54:32,620 --> 00:54:35,730
and similarly the bandit problem there

709
00:54:35,740 --> 00:54:41,200
and will choose one of the arms and then it will distill random rollout from

710
00:54:42,540 --> 00:54:47,690
the area where we need to

711
00:54:47,740 --> 00:54:54,710
the theoretical result is for the the constant is the horizon time so that you

712
00:54:55,320 --> 00:55:01,200
but in practice you don't use that because it ends up exploring too much basically

713
00:55:01,200 --> 00:55:04,620
so so they don't don't actually have a theory

714
00:55:05,390 --> 00:55:09,440
the city result but basically it's in terms of

715
00:55:17,410 --> 00:55:22,190
so so you might think that maybe should do sort of store

716
00:55:22,190 --> 00:55:25,940
the maximum of the q values of these

717
00:55:26,150 --> 00:55:31,790
you might be able to improve doing that people play with the stuff theoretically what

718
00:55:31,810 --> 00:55:33,060
happens is

719
00:55:33,110 --> 00:55:35,410
remember what you see be does

720
00:55:35,420 --> 00:55:37,640
if an arm is suboptimal

721
00:55:37,690 --> 00:55:40,560
it's only going to be pulled sort of log n times

722
00:55:40,570 --> 00:55:44,780
so the average sort of is dominated by the best arm that's actually the key

723
00:55:44,780 --> 00:55:49,190
insight behind these algorithms you have to do and max you can do an average

724
00:55:49,190 --> 00:55:54,190
as long as you don't pull the bad arms too often so

725
00:55:54,230 --> 00:55:59,450
so that's the that's actually the the key insight behind the two thousand five paper

726
00:55:59,450 --> 00:56:04,420
in fact and then this sort of built on that

727
00:56:04,500 --> 00:56:09,100
so as the theoretically the the results are in terms of number of trajectories

728
00:56:10,440 --> 00:56:15,830
basically as you know there a polynomial relation between the number of trajectories and

729
00:56:17,560 --> 00:56:21,330
close you're going to be to the optimal value here and is independent of the

730
00:56:21,330 --> 00:56:22,740
state space size

731
00:56:25,020 --> 00:56:30,450
the balance to sparse sampling the sort of equivalent you can also bound the probability

732
00:56:30,570 --> 00:56:33,320
of selecting the the

733
00:56:33,360 --> 00:56:34,580
optimal action

734
00:56:34,610 --> 00:56:39,770
so in terms of the number of trajectories and that's the polynomial bound in

735
00:56:39,790 --> 00:56:43,900
one over t where t is the number of trajectories so so as you pull

736
00:56:43,900 --> 00:56:45,610
the arm more and more

737
00:56:45,610 --> 00:56:48,870
the probability selects suboptimal arm goes down

738
00:56:48,900 --> 00:56:54,280
fairly quickly with with the number of trajectories you generate

739
00:56:54,290 --> 00:56:56,950
but but don't the key thing here is

740
00:56:56,990 --> 00:57:00,610
you can stop pretty much at any time it doesn't take much time to generate

741
00:57:00,640 --> 00:57:02,740
trajectory so you generate

742
00:57:02,780 --> 00:57:08,950
ten thousand one hundred thousand trajectories you can stop and you have something nontrivial to

743
00:57:08,950 --> 00:57:15,200
to work with the root and it just so happens that it's non-trivial enough so

744
00:57:15,200 --> 00:57:20,370
you get state-of-the-art performance go so this is sort of a recap not gonna go

745
00:57:20,370 --> 00:57:24,070
through here

746
00:57:24,080 --> 00:57:25,400
computer go

747
00:57:25,780 --> 00:57:29,120
the here just some quotes

748
00:57:29,190 --> 00:57:34,110
that indicate how hard people think it is a it is a hard problem we're

749
00:57:34,110 --> 00:57:39,820
not even close to master level performance yes even with UCT but this is what

750
00:57:39,820 --> 00:57:44,270
i think is chinese origins is played lot in asia

751
00:57:44,270 --> 00:57:48,920
so in nineteen by nineteen borini by nine board and you're basically trying to surround

752
00:57:48,920 --> 00:57:52,080
your opponents with these black and white stones

753
00:57:52,490 --> 00:57:58,580
minimax tree search is very bad nobody's ever got to work very well

754
00:57:58,600 --> 00:58:03,990
primarily i guess because we don't have good heuristics and also the branching factor is

755
00:58:03,990 --> 00:58:08,830
a lot larger than chess for example in the branching factor is basically all the

756
00:58:08,830 --> 00:58:14,690
places you could put a stop that's enormous and heuristics

757
00:58:14,990 --> 00:58:19,020
i mean this is very visual game we don't really know

758
00:58:19,020 --> 00:58:22,740
how to compute good heuristics for go released

759
00:58:24,570 --> 00:58:31,280
i was put this animations screwed up but two thousand five i mean basically

760
00:58:31,320 --> 00:58:36,020
goes impossible there there's ago conference but mean for years and years is just a

761
00:58:36,020 --> 00:58:40,210
which sit in my equivalence classes fourteen dimensions variable xj

762
00:58:40,230 --> 00:58:42,350
and that's summarised this thing in the thing

763
00:58:42,370 --> 00:58:43,410
capt data

764
00:58:43,430 --> 00:58:47,760
and this is identifiable objects which you can identify from p

765
00:58:47,770 --> 00:58:53,440
so from data it should be possible to get a handle on this data

766
00:58:53,450 --> 00:58:56,550
his picture is an oracle

767
00:58:56,560 --> 00:59:00,680
this is the probability of the data generating probability distribution

768
00:59:00,690 --> 00:59:03,980
there is an algorithm used the PC algorithm

769
00:59:04,030 --> 00:59:06,490
we can estimate the equivalence class

770
00:59:06,500 --> 00:59:11,320
and then in principle in numerator all that members in the class

771
00:59:11,380 --> 00:59:17,100
then you do the do intervention calculus you get all the intervention effects story

772
00:59:17,100 --> 00:59:21,180
in this cat feeder

773
00:59:21,970 --> 00:59:25,450
so let me just say this is a disaster in high dimensions

774
00:59:25,470 --> 00:59:27,280
we don't want to enumerate all

775
00:59:27,280 --> 00:59:30,970
these members is computationally intractable

776
00:59:30,990 --> 00:59:32,470
you need to do something

777
00:59:32,490 --> 00:59:36,200
but let me first say what can you do not have to model is a

778
00:59:36,260 --> 00:59:41,430
set of all potential intervention effects and i claim but we can get the bound

779
00:59:41,450 --> 00:59:43,640
on causal effect very trivial

780
00:59:43,660 --> 00:59:44,990
so what you do is

781
00:59:45,100 --> 00:59:49,450
you take just for example absolute value of your intervention effects

782
00:59:49,450 --> 00:59:50,870
and go over all

783
00:59:50,870 --> 00:59:54,100
the members an equivalence class of all the hours

784
00:59:54,120 --> 00:59:57,510
and for a certain variable very two dimension j

785
00:59:57,530 --> 00:59:58,890
so these alpha j

786
00:59:59,700 --> 01:00:04,470
three only a lower bound for the true information of the true one is one

787
01:00:04,470 --> 01:00:05,810
of the state s

788
01:00:05,810 --> 01:00:10,930
so trivially the alpha j lower bound

789
01:00:11,720 --> 01:00:15,580
the issue is here and there are many issues and i have no time to

790
01:00:15,580 --> 01:00:16,790
go into that

791
01:00:16,810 --> 01:00:20,990
you need to somehow sorted out more practically

792
01:00:21,010 --> 01:00:22,740
and the point is

793
01:00:22,760 --> 01:00:27,160
if you true underlying graph is sparse

794
01:00:27,160 --> 01:00:28,550
you can actually

795
01:00:28,560 --> 01:00:33,810
have an algorithm which is polynomial kind of the degree of sparsity index of the

796
01:00:35,100 --> 01:00:40,030
and you can go through that and get that multiset without enumerating all that members

797
01:00:40,030 --> 01:00:46,060
you can go directly from class to all potential effects and the number of effects

798
01:00:46,060 --> 01:00:51,720
q is typically much much smaller than the number of members equivalence class

799
01:00:51,740 --> 01:00:56,950
and this is particularly true if the true underlying graph bars

800
01:00:56,950 --> 01:01:02,310
OK so this is kind of maybe more some sort of

801
01:01:02,330 --> 01:01:04,220
the future

802
01:01:04,240 --> 01:01:09,950
directions that you can do something in this area we need based on this is

803
01:01:09,950 --> 01:01:11,830
what the experts

804
01:01:11,850 --> 01:01:16,680
we rely on the PC algorithm this is again fully experts i don't know about

805
01:01:16,680 --> 01:01:22,410
any convex relaxation algorithm so far too more equivalence class that

806
01:01:22,410 --> 01:01:28,140
but you can for example use PC algorithm it has been invented by peter spirtes

807
01:01:28,140 --> 01:01:29,140
and claude

808
01:01:29,160 --> 01:01:31,510
climber and that's why it's called PC

809
01:01:31,530 --> 01:01:33,990
peter and clark algorithms

810
01:01:34,080 --> 01:01:35,580
that gives you

811
01:01:35,600 --> 01:01:39,830
in a statement of the equivalence class of that's

812
01:01:39,830 --> 01:01:41,720
and then you can also

813
01:01:41,760 --> 01:01:43,870
get to the slower by

814
01:01:43,890 --> 01:01:45,930
the three key areas

815
01:01:45,950 --> 01:01:52,140
local computations only try and maybe to finish here

816
01:01:52,160 --> 01:01:56,350
mister last says that everything was convex optimization

817
01:01:56,370 --> 01:02:02,140
that was fast i didn't show you actually our interest is extremely fast extremely efficient

818
01:02:02,160 --> 01:02:06,100
now he learning that's an equivalence classes of that

819
01:02:06,120 --> 01:02:09,140
i don't know how to do it in the convex relaxation manner

820
01:02:09,160 --> 01:02:13,410
but you can work on local computations on graphs which is not as fast as

821
01:02:13,410 --> 01:02:18,930
a convex optimization but it's still feasible and reasonably efficient

822
01:02:23,060 --> 01:02:24,970
i think i should things right

823
01:02:25,030 --> 01:02:30,780
i mean i just want to i isolate it is one example on the east

824
01:02:30,830 --> 01:02:35,410
and i just want to finish another example we really

825
01:02:35,470 --> 01:02:38,310
employed this causal technique

826
01:02:38,330 --> 01:02:42,330
in arabidopsis so the response variable was

827
01:02:42,330 --> 01:02:47,510
here are plans and you want to my experiment is as follows you have plans

828
01:02:47,510 --> 01:02:52,870
and you want to create a fast-growing plants because it only fast growing rice or

829
01:02:52,870 --> 01:03:00,240
maize and so the first desirable jobs to make principle experiment and ask about the

830
01:03:00,240 --> 01:03:07,010
response of interest the phenotype is how many days does it take the the flower

831
01:03:07,030 --> 01:03:08,870
the plant is flowering

832
01:03:08,890 --> 01:03:11,640
two days to flowering of the plant

833
01:03:11,660 --> 01:03:14,550
and the aim is to have a kind of

834
01:03:14,560 --> 01:03:18,780
low values and by just a few days is school

835
01:03:18,810 --> 01:03:23,530
and you have extreme expression profile plants observational data

836
01:03:23,620 --> 01:03:24,930
forty seven

837
01:03:25,580 --> 01:03:27,330
morning twenty thousand

838
01:03:27,350 --> 01:03:32,640
if in collaboration with by going being and increase in the tree

839
01:03:32,660 --> 01:03:34,700
so what we do is the

840
01:03:34,740 --> 01:03:39,060
cannot infer the causal effects but can go through this machinery giving the bounds of

841
01:03:39,060 --> 01:03:42,580
causal effects this is my scoring scheme

842
01:03:46,760 --> 01:03:49,240
look at this here the last line

843
01:03:49,260 --> 01:03:54,080
i really encourage you to put some bootstrapping or subsampling on top of of it

844
01:03:54,080 --> 01:03:57,760
i can apply my stability selection machinery to this problem as well

845
01:03:57,780 --> 01:03:58,790
might help

846
01:03:58,810 --> 01:04:05,490
one hundred variables these discourse bounds scoring let's just apply stability selection let's use that

847
01:04:05,490 --> 01:04:10,890
formula which i showed you before i don't know whether exchangeability holes condition holds or

848
01:04:11,620 --> 01:04:13,990
but i just using a more practical way

849
01:04:13,990 --> 01:04:14,790
and then

850
01:04:14,810 --> 01:04:17,160
you can come up with this list

851
01:04:17,180 --> 01:04:19,280
here are the top twenty

852
01:04:20,470 --> 01:04:22,680
jean interventions

853
01:04:22,700 --> 01:04:25,490
experiments suggest that what you should do

854
01:04:25,530 --> 01:04:29,450
and the plant guy said that he knows the three genes

855
01:04:29,470 --> 01:04:33,490
and he was willing and that i find kind of interesting he was willing to

856
01:04:33,490 --> 01:04:37,260
go forty others exactly according to this scale

857
01:04:37,280 --> 01:04:42,550
i don't no other biological know how well this is better than that justice involve

858
01:04:42,550 --> 01:04:45,740
those three and so i don't want to do these experiments but i do the

859
01:04:45,740 --> 01:04:52,100
remaining seventeen and interstate twenty is enough if want to do more OK

860
01:04:52,120 --> 01:04:59,870
so what do seventeen gene intervention experiments while unfortunately could do to seventeen could only

861
01:04:59,870 --> 01:05:02,680
note also that every pure

862
01:05:02,700 --> 01:05:07,760
other linear decision but that doesn't have any anything linear decisions such as for example

863
01:05:07,760 --> 01:05:08,890
cutting plane

864
01:05:08,890 --> 01:05:12,910
it is equivalent to some pure in a linear decision method such as

865
01:05:13,030 --> 01:05:16,060
which of the composition

866
01:05:16,100 --> 01:05:21,980
do the equivalent in the mathematical sense

867
01:05:22,010 --> 01:05:22,840
so much

868
01:05:29,190 --> 01:05:37,090
so so let me talk about special cases

869
01:05:37,120 --> 01:05:41,260
OK because the class classical cutting plane method maps into the framework

870
01:05:41,320 --> 01:05:43,830
the equivalent

871
01:05:43,880 --> 01:05:45,790
for this problem here

872
01:05:45,800 --> 01:05:48,310
the equivalent is too

873
01:05:50,190 --> 01:05:52,880
the indicator function of the

874
01:05:52,910 --> 01:05:55,820
of the constraints that has a second function

875
01:05:55,820 --> 01:05:57,260
and to introduce

876
01:05:57,260 --> 01:06:00,610
a constraint that x one and x two must be equal so this is the

877
01:06:00,630 --> 01:06:02,590
subspace constraints k

878
01:06:02,780 --> 01:06:04,590
so just two functions

879
01:06:04,590 --> 01:06:05,820
two components

880
01:06:05,820 --> 01:06:07,700
and the substrate right

881
01:06:10,180 --> 01:06:14,610
in the classical cutting plane algorithm we are dealing arises f

882
01:06:14,650 --> 01:06:17,260
and we have this unchanged

883
01:06:18,050 --> 01:06:24,180
the approximating in p has this form where f has been replaced by its

884
01:06:26,010 --> 01:06:29,340
and capitalize a finite set of subgradients

885
01:06:29,360 --> 01:06:31,760
of f that have been obtained so far

886
01:06:31,820 --> 01:06:34,070
so it's the method that i showed earlier

887
01:06:34,090 --> 01:06:35,360
i have a

888
01:06:35,380 --> 01:06:42,360
i the linear eyes just f i keep c and changed have finite set of

889
01:06:42,360 --> 01:06:46,990
subgradients minimize get extract value of new

890
01:06:48,610 --> 01:06:52,280
subgradient and keep going like this

891
01:06:52,280 --> 01:06:54,030
so that's the special case

892
01:06:54,030 --> 01:06:58,110
that corresponds to cutting plane

893
01:06:58,110 --> 01:07:04,090
the special case correspond to simplicity compositions for the same problem is a little different

894
01:07:04,160 --> 01:07:07,260
again the equivalent is like this

895
01:07:07,280 --> 01:07:10,910
this is the indicator function of the constraints that the second component

896
01:07:10,930 --> 01:07:14,530
but here we are in the linear eyes the constraints

897
01:07:14,590 --> 01:07:16,570
in p f and change

898
01:07:17,820 --> 01:07:22,090
and then the approximating problem has this form was see bars

899
01:07:22,150 --> 01:07:27,320
is a finite point convex hull was in that approximates

900
01:07:28,970 --> 01:07:33,220
and the enlargements step is

901
01:07:33,260 --> 01:07:38,550
is is is involves using

902
01:07:38,630 --> 01:07:42,800
after news of the primal approximating problem you have to solve the dual

903
01:07:42,860 --> 01:07:44,390
well if you you're about

904
01:07:44,410 --> 01:07:47,660
the dual optimal solutions are of this form here

905
01:07:47,660 --> 01:07:49,950
four white had a subgradient

906
01:07:49,990 --> 01:07:54,030
white is that is a subgradient of f at x

907
01:07:54,050 --> 01:07:59,300
they had such that minus white heart is in the normal cone

908
01:07:59,360 --> 01:08:02,360
of this polyhedral approximation

909
01:08:02,490 --> 01:08:04,680
now in the classical case

910
01:08:04,720 --> 01:08:06,680
f is differentiable

911
01:08:06,680 --> 01:08:09,840
and why patterns very simple playing by

912
01:08:09,860 --> 01:08:10,970
this condition

913
01:08:10,990 --> 01:08:15,320
in the non differential case which is new actually

914
01:08:15,320 --> 01:08:19,700
you have to do some computation here to find not just any subgradient but also

915
01:08:19,700 --> 01:08:23,150
one that belongs to to the normal component of this

916
01:08:23,970 --> 01:08:24,760
in fact

917
01:08:24,780 --> 01:08:28,610
many high it's more complicated than just differentiated

918
01:08:28,630 --> 01:08:32,090
and now after you obtain an extreme point of the constraint set

919
01:08:32,110 --> 01:08:37,200
or after obtained after you solve this problem then you

920
01:08:37,200 --> 01:08:39,510
finding new extreme points by

921
01:08:39,530 --> 01:08:40,360
by this

922
01:08:40,360 --> 01:08:42,550
minimisation here

923
01:08:43,360 --> 01:08:47,860
which in the case was useful here is the linear programming problem

924
01:08:47,890 --> 01:08:52,070
the point i want to make here is that the classical methods are special cases

925
01:08:52,090 --> 01:08:54,990
involving this problem here

926
01:08:55,110 --> 01:08:57,410
and either in the linear

927
01:08:57,450 --> 01:09:01,550
minimisation of this or other minimisation of the

928
01:09:01,720 --> 01:09:22,950
OK so complicated thing this way took so long to learn OK shows simplicity composition

929
01:09:22,970 --> 01:09:26,090
for the case of a differentiable function i explained

930
01:09:27,030 --> 01:09:30,660
you obtain extreme points by going in the direction the negative gradient

931
01:09:30,780 --> 01:09:36,240
this is a depiction of simplicity compositions for nondifferentiable cost function that has this level

932
01:09:36,240 --> 01:09:37,320
sets here

933
01:09:38,410 --> 01:09:43,820
given this inner approximation you find a sub gradient of the cost function at this

934
01:09:43,820 --> 01:09:48,280
point that's also in the normal comes to mind is that in the normal course

935
01:09:48,280 --> 01:09:49,700
of the

936
01:09:49,760 --> 01:09:51,320
of the

937
01:09:51,380 --> 01:09:52,930
of the convex hull

938
01:09:52,930 --> 01:09:54,860
now you have this white can't

939
01:09:54,880 --> 01:09:55,930
you go

940
01:09:55,950 --> 01:09:59,530
as far as you can see in the direction of mine why had to obtain

941
01:09:59,530 --> 01:10:00,990
an extreme point

942
01:10:00,990 --> 01:10:02,410
and that's the

943
01:10:02,510 --> 01:10:06,070
x still there that large this

944
01:10:06,070 --> 01:10:08,200
this current approximation

945
01:10:08,220 --> 01:10:09,180
yet a new

946
01:10:09,220 --> 01:10:12,510
subgradient that's in the normal component of the approximation

947
01:10:12,510 --> 01:10:14,820
go in the direction of a

948
01:10:14,840 --> 01:10:17,700
mine is that you pay optimal point here

949
01:10:17,780 --> 01:10:20,800
the reason being that both f

950
01:10:20,820 --> 01:10:23,070
and c are polyhedra

951
01:10:23,090 --> 01:10:27,430
and this is a generic property if everything is fully hero there yet

952
01:10:27,450 --> 01:10:32,820
convergence in a finite number of steps

953
01:10:32,860 --> 01:10:37,590
the reason being that you have expand the polyhedral approximation eventually you're going to be

954
01:10:37,590 --> 01:10:42,660
adding more and more points and get the original

955
01:10:42,780 --> 01:10:45,990
let me skip this slide because we're running out of time

956
01:10:46,010 --> 01:10:49,610
this is the case where you have the minimum of two functions

957
01:10:49,660 --> 01:10:51,450
with a common

958
01:10:51,470 --> 01:10:52,300
from one

959
01:10:52,320 --> 01:10:54,950
dependence on on x

960
01:10:54,990 --> 01:10:55,910
and this

961
01:10:55,930 --> 01:10:56,930
the figure

962
01:10:56,930 --> 01:11:02,240
to show the duality between in the linear decision in a linear is one corresponds

963
01:11:02,240 --> 01:11:05,070
to the linear rising the other in the door

964
01:11:05,130 --> 01:11:09,780
this is an insightful figure if you have some time to study then unfortunately we

965
01:11:09,780 --> 01:11:12,530
don't have time to do that

966
01:11:12,570 --> 01:11:15,970
so let me say a few things about converges

967
01:11:16,010 --> 01:11:21,130
comment is a little tricky is not as simple as you might think in fact

968
01:11:21,130 --> 01:11:22,700
i'm still working

969
01:11:22,720 --> 01:11:25,430
let's distinguish between the polygonal case

970
01:11:25,430 --> 01:11:29,860
we're all out the linear is functions are finite he

971
01:11:30,010 --> 01:11:32,470
finite meaning real

972
01:11:32,490 --> 01:11:38,200
all in the linear eyes functions are called finite when he meaning that conjugate is

973
01:11:38,200 --> 01:11:39,680
is finite

974
01:11:39,780 --> 01:11:42,030
and let's also require that

975
01:11:42,030 --> 01:11:46,820
he knew refinement points that you have to the point he the approximations

976
01:11:46,820 --> 01:11:49,520
language but i want you to start thinking that we don't think of it as

977
01:11:49,830 --> 01:11:53,480
a specific box into which were putting things think of it as a link to

978
01:11:53,480 --> 01:11:56,500
evaluate i could have for example

979
01:11:58,530 --> 01:11:59,780
simon statement

980
01:11:59,970 --> 01:12:03,230
that creates a binding

981
01:12:03,280 --> 01:12:04,780
from y

982
01:12:04,820 --> 01:12:06,040
into that same

983
01:12:07,580 --> 01:12:10,840
one of the things the concept i can do is i could have a statement

984
01:12:12,830 --> 01:12:14,120
the z

985
01:12:14,130 --> 01:12:18,170
the bound to the value of x y said it deliberately that way that statement

986
01:12:18,170 --> 01:12:22,290
says get the value of x which is this link

987
01:12:22,330 --> 01:12:25,640
and gives ia pointer

988
01:12:25,690 --> 01:12:27,070
to the same place

989
01:12:27,130 --> 01:12:28,480
the the value

990
01:12:28,480 --> 01:12:30,570
not to x

991
01:12:30,630 --> 01:12:33,450
OK now what is plant that idea when come back to later on as we

992
01:12:33,520 --> 01:12:35,780
as we carry on

993
01:12:35,850 --> 01:12:38,360
OK so if we have variables

994
01:12:38,370 --> 01:12:42,290
the questions we can ask is what's the time

995
01:12:42,300 --> 01:12:45,570
the variable

996
01:12:45,600 --> 01:12:50,310
and the answer is it inherited from its value

997
01:12:56,470 --> 01:12:59,980
so somewhere in my code

998
01:13:00,020 --> 01:13:04,230
i have that statement that assignment statement x now is the variable whose value is

999
01:13:04,230 --> 01:13:06,550
an integer

1000
01:13:06,610 --> 01:13:11,250
unfortunately at least in my mind implies on these variable bindings

1001
01:13:11,260 --> 01:13:12,680
or dynamic

1002
01:13:12,690 --> 01:13:15,160
the type brothers dynamics

1003
01:13:18,780 --> 01:13:21,030
it changes depending on what

1004
01:13:21,050 --> 01:13:22,940
the current value is

1005
01:13:22,940 --> 01:13:27,720
four set to different way of somewhere later on in the program i do this

1006
01:13:30,720 --> 01:13:33,110
x now has changed its title

1007
01:13:33,130 --> 01:13:34,270
from and

1008
01:13:34,270 --> 01:13:36,110
to strengthen

1009
01:13:36,130 --> 01:13:38,060
now why should you care

1010
01:13:38,180 --> 01:13:41,540
my view is

1011
01:13:41,550 --> 01:13:42,990
and like

1012
01:13:42,990 --> 01:13:46,180
especially in the presence of operator overloading

1013
01:13:46,190 --> 01:13:49,960
because i might have wrote written some code in which i'm expecting a particular variable

1014
01:13:49,960 --> 01:13:53,000
to have an integer value if some more later on in the code it shifts

1015
01:13:53,000 --> 01:13:54,360
to string

1016
01:13:54,380 --> 01:13:58,660
i might not be manipulating that getting actual values out but not what i wanted

1017
01:13:58,870 --> 01:14:01,210
him to be really hard for me to chase back

1018
01:14:01,280 --> 01:14:03,930
so one of the things i would like to suggest is that you develop some

1019
01:14:03,930 --> 01:14:06,740
good style here in particular

1020
01:14:06,780 --> 01:14:09,590
don't change types

1021
01:14:12,660 --> 01:14:18,810
our central can

1022
01:14:19,000 --> 01:14:24,090
meaning sometimes you need to do this but in general this at least in my

1023
01:14:24,090 --> 01:14:26,550
view on a drama degree you just don't want to do this you don't want

1024
01:14:26,550 --> 01:14:30,770
to make those changes this leads to trouble down the road

1025
01:14:32,330 --> 01:14:35,290
now let's think about variables and i'm going to start pushing on this is work

1026
01:14:36,660 --> 01:14:38,670
the answer is

1027
01:14:38,690 --> 01:14:40,860
a user variable

1028
01:14:40,920 --> 01:14:43,610
anywhere you can use the value

1029
01:14:43,640 --> 01:14:52,490
any place it's legal to use the value

1030
01:14:58,810 --> 01:15:01,490
this is just sort of bringing us back up to speed and adding a few

1031
01:15:01,490 --> 01:15:04,720
more details in what we really want to do not do now we start using

1032
01:15:04,720 --> 01:15:05,940
this stuff

1033
01:15:08,760 --> 01:15:11,010
let's take expressions get values out

1034
01:15:11,020 --> 01:15:14,290
we can store them away in places but ultimately we want to do something with

1035
01:15:14,290 --> 01:15:16,870
them so we need to now start talking about what are the things we can

1036
01:15:16,870 --> 01:15:20,450
do inside or any programming language to manipulate them

1037
01:15:20,470 --> 01:15:23,800
and for that we're going to have statements

1038
01:15:33,990 --> 01:15:38,480
when i think about it legal possible to use the word expression except misuse that

1039
01:15:38,480 --> 01:15:40,070
elsewhere so legal

1040
01:15:42,500 --> 01:15:48,450
python can enter

1041
01:15:48,500 --> 01:15:54,000
you've already seen a couple of them

1042
01:15:59,100 --> 01:16:05,250
two obvious statements commands to do something assignment is binding and name to value printed

1043
01:16:05,250 --> 01:16:09,060
saying put it back out on the screen obviously of princes away putting things that

1044
01:16:09,080 --> 01:16:11,690
we expect to have ways of getting input and we can see an example that

1045
01:16:11,720 --> 01:16:14,910
the second and if you go through the next two lectures run adding more and

1046
01:16:14,910 --> 01:16:18,750
more these statements but let's look at what we could do with this

1047
01:16:18,780 --> 01:16:22,210
and to do this i'm going to use some code have already typed in

1048
01:16:22,260 --> 01:16:25,350
so i'm hoping you can read that is also in your hand out this is

1049
01:16:25,350 --> 01:16:27,670
a little file i created

1050
01:16:27,740 --> 01:16:30,370
right and i'm going to start with

1051
01:16:30,430 --> 01:16:33,570
the sequence of these things and walking along again i invite you to put comments

1052
01:16:33,570 --> 01:16:36,730
on that handle so that you can follow it's going to do

1053
01:16:37,320 --> 01:16:39,540
so let's look at the first part of this right now this is just the

1054
01:16:39,540 --> 01:16:41,360
text file

1055
01:16:41,420 --> 01:16:44,000
OK and highlighted in blue up there sort of the pieces are going to start

1056
01:16:44,000 --> 01:16:48,220
with what i have a sequence of commands got an assignment statement

1057
01:16:48,220 --> 01:16:52,120
another assignment statement out of print statements about an input statement which will come back

1058
01:16:52,130 --> 01:16:53,750
to the second

1059
01:16:53,760 --> 01:16:56,520
and i want to be is basically trying to use these things to do something

1060
01:16:56,520 --> 01:16:57,460
with them

1061
01:16:57,510 --> 01:17:00,840
the second thing i want to note is the little hash mark the pound sign

1062
01:17:00,840 --> 01:17:02,720
OK so

1063
01:17:02,740 --> 01:17:06,850
in the first half i talked about how you would learn multiple is the feature

1064
01:17:06,850 --> 01:17:09,230
vectors in an unsupervised way

1065
01:17:09,280 --> 01:17:11,990
once you've done that

1066
01:17:12,190 --> 01:17:15,220
you can then fine tuning for discrimination

1067
01:17:18,720 --> 01:17:21,000
there's way fine tuning them

1068
01:17:21,000 --> 01:17:26,960
that is non discriminative that i've sort of suppression and talked about i'm talking about

1069
01:17:26,960 --> 01:17:29,020
how you can fine-tune them

1070
01:17:32,270 --> 01:17:36,400
and why you think get much better models

1071
01:17:36,420 --> 01:17:42,940
that is classification regression models than if you try and use only just go into

1072
01:17:44,180 --> 01:17:48,890
so it's been a long history of people saying that you should use discriminative training

1073
01:17:48,890 --> 01:17:52,190
rather than generative training if you wanted discrimination

1074
01:17:52,250 --> 01:17:56,240
and i give you some reasons of data and the tutorial why i think that's

1075
01:17:56,240 --> 01:18:00,430
completely wrong as soon as you have interesting data data with a lot of structure

1076
01:18:00,430 --> 01:18:02,470
and it is really pays to do

1077
01:18:02,890 --> 01:18:06,570
generative training rather than just going to training to begin with

1078
01:18:09,760 --> 01:18:11,920
one reason why it works better

1079
01:18:11,950 --> 01:18:15,400
to train a multilayer network generatively

1080
01:18:15,480 --> 01:18:17,170
and then discrimination

1081
01:18:17,260 --> 01:18:20,460
is the greedy layer by layer learning scales really well

1082
01:18:20,480 --> 01:18:25,120
you can learn one layer at a time you don't have to worry about going

1083
01:18:25,120 --> 01:18:29,070
backwards through many many many layers and is getting very small which is always worrying

1084
01:18:30,450 --> 01:18:34,400
so you don't actually start doing your fine tuning until

1085
01:18:34,920 --> 01:18:40,010
you've got a deep net with sensible weights that because each layer is quite good

1086
01:18:40,060 --> 01:18:42,810
reconstructing what's going on in before

1087
01:18:42,820 --> 01:18:46,320
and that means the derivatives are going to be quite good when you back propagate

1088
01:18:46,320 --> 01:18:48,510
they're not going to be tiny not going to be huge

1089
01:18:51,120 --> 01:18:54,500
you end up getting much better local optima

1090
01:18:54,510 --> 01:18:58,030
if you think about it

1091
01:18:58,120 --> 01:19:02,110
it's silly to use the local search great being non-linear space

1092
01:19:02,120 --> 01:19:05,780
by studying the origin hoping that it's you could by going downhill

1093
01:19:05,780 --> 01:19:08,200
you can find a good region that space to be

1094
01:19:08,220 --> 01:19:12,270
well you do it does unsupervised learning is going to a region of this great

1095
01:19:12,270 --> 01:19:14,400
big space and non linear functions

1096
01:19:14,490 --> 01:19:17,780
and you're doing that without using up any of the information in the labels and

1097
01:19:17,780 --> 01:19:21,310
then in that region you can do local fine tuning where you have to improve

1098
01:19:21,310 --> 01:19:24,720
things a lot

1099
01:19:24,750 --> 01:19:26,250
this is sort of

1100
01:19:26,300 --> 01:19:28,440
generalisation view of it

1101
01:19:28,490 --> 01:19:30,370
so you get better after

1102
01:19:30,470 --> 01:19:34,500
it's also much better way to use the information in the labels

1103
01:19:35,090 --> 01:19:38,690
images contain more information than labels usually

1104
01:19:38,810 --> 01:19:41,880
and often only some of them are labeled

1105
01:19:41,900 --> 01:19:46,340
so if you can design your feature detectors without using any labels

1106
01:19:46,350 --> 01:19:51,430
even if you have some labels then you got a very good start and you

1107
01:19:51,430 --> 01:19:57,100
only have to fine-tune them using the labels that just change the discrimination boundaries slightly

1108
01:19:57,150 --> 01:20:03,120
so your much better dealing with that there is much information about two labels

1109
01:20:03,180 --> 01:20:08,280
people in you should avengers group have done a lot of empirical research showing the

1110
01:20:08,280 --> 01:20:12,240
what i just claimed is true i'm just going to show you one of our

1111
01:20:12,240 --> 01:20:13,600
nation examples

1112
01:20:13,620 --> 01:20:17,090
the kind of digit recognition network at all about the other day was trained at

1113
01:20:17,090 --> 01:20:17,900
the top

1114
01:20:17,960 --> 01:20:20,350
to to learn a joint density

1115
01:20:20,370 --> 01:20:24,810
a sort of pure cases just to train it purely unsupervised so in the joint

1116
01:20:24,810 --> 01:20:29,120
density here is pure unsupervised then we stick ten labels on the top

1117
01:20:29,120 --> 01:20:33,120
we treat we only consider the forward connections now just back propagate through to train

1118
01:20:33,120 --> 01:20:35,100
these full connections more

1119
01:20:35,120 --> 01:20:39,500
and it turns out that helps a lot

1120
01:20:39,520 --> 01:20:42,000
so if you take the best you can do with backprop if you start from

1121
01:20:42,000 --> 01:20:44,600
random weights is like that

1122
01:20:44,620 --> 01:20:45,650
if you

1123
01:20:45,650 --> 01:20:49,970
first learn this model purely unsupervised then you have the labels and then you won't

1124
01:20:49,970 --> 01:20:51,630
forget you do this

1125
01:20:52,960 --> 01:20:55,270
more recently i've tried doing things like

1126
01:20:55,280 --> 01:20:57,330
when you add the labels

1127
01:20:57,340 --> 01:21:00,710
you make a lot of them wrong

1128
01:21:00,800 --> 01:21:03,720
and you still do very well this technique

1129
01:21:03,740 --> 01:21:07,560
we can handle lots of errors in the labels

1130
01:21:09,120 --> 01:21:12,020
shows you what happens when you do the fine tuning

1131
01:21:12,090 --> 01:21:15,270
these are the feature is the first layer feature vectors

1132
01:21:15,310 --> 01:21:17,390
and before you do any fine tuning

1133
01:21:17,450 --> 01:21:20,290
so here's the feature vector like this little piece struck with

1134
01:21:20,300 --> 01:21:22,730
but and most road nearby

1135
01:21:22,760 --> 01:21:26,150
he is the same thing after fine tuning and basically nothing changed

1136
01:21:26,150 --> 01:21:29,490
there's almost no change if you look at the coast of anglesey cheaper few degrees

1137
01:21:29,490 --> 01:21:30,970
of the most

1138
01:21:30,980 --> 01:21:34,130
so the fine tuning didn't have to learn the features

1139
01:21:34,150 --> 01:21:38,020
features sort of or what kind of feature that be it just made very slight

1140
01:21:38,020 --> 01:21:40,110
difference to a large number of features

1141
01:21:40,130 --> 01:21:44,050
and caused by doing that you can move discrimination boundary quite significantly

1142
01:21:44,080 --> 01:21:48,240
so what stories just relocating the boundaries right to get the right answers

1143
01:21:48,280 --> 01:21:50,840
but not really learning features

1144
01:21:50,840 --> 01:21:53,840
and that's where the wing comes from

1145
01:21:54,600 --> 01:22:00,910
those who is that this is same it's going to be in the region of

1146
01:22:00,920 --> 01:22:04,580
the space that you find with the pre training and then it's going to dig

1147
01:22:04,580 --> 01:22:07,410
a deep hole just and it's not going to consider all the rest of the

1148
01:22:09,090 --> 01:22:14,130
and to show that's much better than starting at the origin and wandering around

1149
01:22:14,150 --> 01:22:17,530
by gradient stochastic gradient descent

1150
01:22:20,520 --> 01:22:23,460
this is the test error a whole bunch of nets

1151
01:22:23,460 --> 01:22:25,090
these are ones

1152
01:22:25,170 --> 01:22:27,460
without pre training these ones with pre training

1153
01:22:27,480 --> 01:22:28,960
this was on

1154
01:22:28,970 --> 01:22:33,940
a data set made from the mnist digits by introducing lots of translations and rotations

1155
01:22:33,940 --> 01:22:38,800
and scalings so an almost infinite amount of data

1156
01:22:40,110 --> 01:22:42,840
it does much better when you the pre training

1157
01:22:42,840 --> 01:22:43,560
so the

1158
01:23:02,050 --> 01:23:03,830
topic for today

1159
01:23:13,700 --> 01:23:18,460
today we're going to talk and post linear equations the next time

1160
01:23:18,520 --> 01:23:22,250
instead of i think it's a good idea since

1161
01:23:22,390 --> 01:23:27,530
in real life most of the differential equations are solved by numerical methods

1162
01:23:27,540 --> 01:23:30,470
to introduce you to those right away

1163
01:23:30,490 --> 01:23:32,960
even when you see the computer we use all

1164
01:23:32,970 --> 01:23:34,960
the computer screen

1165
01:23:34,970 --> 01:23:36,880
the solution is being drawn

1166
01:23:36,920 --> 01:23:38,150
of course there

1167
01:23:38,210 --> 01:23:44,000
what really was happening is that the computer was calculating the solutions numerically and plotting

1168
01:23:44,000 --> 01:23:45,240
the points

1169
01:23:47,370 --> 01:23:49,120
this is the main way

1170
01:23:49,140 --> 01:23:53,480
numerically is in a way differential equations are actually solve

1171
01:23:53,510 --> 01:23:59,930
the any complexity at all now so the problem is an initial value problem let's

1172
01:23:59,930 --> 01:24:05,620
write a first order problem the way we talked about it on wednesday

1173
01:24:05,640 --> 01:24:10,920
and now all specifically add to that the starting point that you use when you

1174
01:24:10,920 --> 01:24:14,960
did the computer experiments and i'll write the starting point this way

1175
01:24:14,980 --> 01:24:16,930
the y of x zero should be

1176
01:24:16,960 --> 01:24:18,350
y zero

1177
01:24:18,420 --> 01:24:20,420
so this is the initial condition

1178
01:24:20,460 --> 01:24:23,030
and this is the first order differential equation

1179
01:24:23,120 --> 01:24:25,670
and as you know the two of them together

1180
01:24:25,680 --> 01:24:27,180
are called

1181
01:24:27,230 --> 01:24:28,900
and i vp

1182
01:24:28,920 --> 01:24:32,930
an initial value problem which means two things

1183
01:24:33,030 --> 01:24:36,150
the differential equation and the initial value

1184
01:24:36,230 --> 01:24:37,810
that you are

1185
01:24:37,820 --> 01:24:41,260
i want to start the solution that

1186
01:24:45,650 --> 01:24:48,040
the math

1187
01:24:48,060 --> 01:24:50,000
we're going to talk about

1188
01:24:50,020 --> 01:24:51,280
the basic

1189
01:24:51,310 --> 01:24:56,340
method of which many others are merely refinements in one way or another

1190
01:24:56,430 --> 01:24:59,990
is called alleles methods

1191
01:25:08,300 --> 01:25:10,930
did of course everything analysis

1192
01:25:10,940 --> 01:25:11,960
i didn't

1193
01:25:12,000 --> 01:25:16,720
as far as i know didn't actually use it to compute solutions of differential equations

1194
01:25:16,720 --> 01:25:18,620
is addressed was theoretical

1195
01:25:18,620 --> 01:25:25,550
he used it as a method of proving the existence theorem proving that solutions exist

1196
01:25:25,560 --> 01:25:28,340
but nowadays is used

1197
01:25:28,360 --> 01:25:30,840
to calculate the solutions numerically

1198
01:25:30,880 --> 01:25:32,500
and the method

1199
01:25:32,520 --> 01:25:36,330
it is very simple to describe it so naive

1200
01:25:36,340 --> 01:25:40,360
you probably think that if you have been living three hundred years ago you would

1201
01:25:40,360 --> 01:25:45,120
have discovered it and covered yourself with glory for all eternity

1202
01:25:45,180 --> 01:25:48,750
here is so here's our starting point

1203
01:25:48,880 --> 01:25:50,490
x zero

1204
01:25:50,530 --> 01:25:52,310
y zero

1205
01:25:52,490 --> 01:25:59,490
what information do we have at that point all we have is the little line

1206
01:25:59,490 --> 01:26:03,370
elements whose slope is given by f of x y

1207
01:26:04,750 --> 01:26:06,880
if i start the solution

1208
01:26:06,930 --> 01:26:11,000
the only way this solution could possibly go would be to start off in that

1209
01:26:11,000 --> 01:26:14,490
direction since i have no other information

1210
01:26:14,970 --> 01:26:19,720
at least it's career has the correct direction eg zero y zero but of course

1211
01:26:19,720 --> 01:26:23,430
it's not likely to have the correct direction anywhere else

1212
01:26:23,470 --> 01:26:25,340
now what you do then is

1213
01:26:25,360 --> 01:26:28,530
choose the step size all draw just a few

1214
01:26:28,550 --> 01:26:30,120
two steps to method

1215
01:26:30,130 --> 01:26:31,630
that's i think good enough

1216
01:26:33,310 --> 01:26:39,830
choose the steps size uniform step size which is usually called a change ge

1217
01:26:39,870 --> 01:26:42,310
and you continue that solution

1218
01:26:42,310 --> 01:26:46,680
until you get to the next point which will be at zero plus age as

1219
01:26:46,680 --> 01:26:49,810
i draw on the picture so we get to hear

1220
01:26:49,860 --> 01:26:53,560
we stop at that point and now

1221
01:26:53,590 --> 01:26:57,310
you read calculate what the line element is here

1222
01:26:57,370 --> 01:26:59,830
suppose here the line element now

1223
01:26:59,850 --> 01:27:04,820
through this point goes like that well then that's the new direction that you should

1224
01:27:04,820 --> 01:27:09,120
start out with going from here and so the the next step of the process

1225
01:27:09,120 --> 01:27:11,330
will carry here

1226
01:27:11,350 --> 01:27:17,100
that's two steps of oil is meant that notice it produces a broken line approximation

1227
01:27:17,110 --> 01:27:18,380
to the solution

1228
01:27:18,390 --> 01:27:21,930
but in fact you only see the broken line

1229
01:27:21,940 --> 01:27:24,240
if you're a computer

1230
01:27:24,360 --> 01:27:28,890
if you're looking at the computer visual for example which is whose purpose is to

1231
01:27:28,890 --> 01:27:31,390
well it's great for you while is not there

1232
01:27:31,410 --> 01:27:33,790
in actual practice what you see is

1233
01:27:34,920 --> 01:27:40,370
the computer simply calculating the point that point that point and the succession of points

1234
01:27:40,370 --> 01:27:45,820
and many programs will just automatically connect those points by smooth looking care

1235
01:27:45,920 --> 01:27:50,010
if that's what you prefer to see

1236
01:27:51,250 --> 01:27:55,330
that's that's all there is to the method of what we have to do now

1237
01:27:56,810 --> 01:27:59,700
to derive the equations for the method

1238
01:28:00,180 --> 01:28:05,180
we can do that well the essence of it is how to get from the

1239
01:28:05,180 --> 01:28:06,520
m step

1240
01:28:06,550 --> 01:28:12,380
to the employees first so i'm going to draw the picture just to illustrate that

1241
01:28:12,440 --> 01:28:18,200
so now we're not x zero but let's say we've already got into x and

1242
01:28:18,200 --> 01:28:21,620
y and

1243
01:28:21,620 --> 01:28:25,840
how do i take the next step well if i take the line element and

1244
01:28:25,840 --> 01:28:26,930
it goes up

1245
01:28:27,670 --> 01:28:30,370
that let's say because the slope is

1246
01:28:30,380 --> 01:28:33,000
this i'm going to call that slope

1247
01:28:33,080 --> 01:28:34,550
and so

1248
01:28:35,520 --> 01:28:40,430
of course a savannah is the value of the right hand side of the point

1249
01:28:40,430 --> 01:28:42,930
x and y and it will need an equation

1250
01:28:43,040 --> 01:28:45,940
but i think it will be a little clearer by just give it a capital

1251
01:28:45,940 --> 01:28:47,670
letter at this point

1252
01:28:50,360 --> 01:28:54,290
how this is the new point and all i want to know is what are

1253
01:28:54,290 --> 01:28:57,930
its coordinates well the axon plus one is there

1254
01:28:57,980 --> 01:29:01,320
the y n plus one is here

1255
01:29:01,330 --> 01:29:05,120
clearly i should rather trying to complete the triangle

1256
01:29:05,160 --> 01:29:10,070
this side of the triangle this the of news has slope a and

1257
01:29:10,170 --> 01:29:14,890
this side of the triangle has length h h is the step size perhaps i

1258
01:29:14,890 --> 01:29:19,420
better indicate that actually put that up so that you know the words

1259
01:29:19,430 --> 01:29:20,940
step size

1260
01:29:20,950 --> 01:29:24,860
it means health it's the step size on the x axis of far you have

1261
01:29:24,860 --> 01:29:28,070
to go to get from the x to the next one

1262
01:29:28,120 --> 01:29:29,890
what this well

1263
01:29:29,910 --> 01:29:31,800
if that slope

1264
01:29:31,810 --> 01:29:36,870
as this is the slope and this is a then this must be a times

1265
01:29:38,660 --> 01:29:41,160
link to that site

1266
01:29:41,240 --> 01:29:43,880
in order that the ratio of the height

1267
01:29:43,930 --> 01:29:46,980
so this with should be a

1268
01:29:46,990 --> 01:29:49,380
and that gives us the method

1269
01:29:49,470 --> 01:29:51,920
what's the

1270
01:29:51,970 --> 01:29:56,390
how do i get from the clearly to get from x and two x n

1271
01:29:56,390 --> 01:30:00,740
plus y simply at age that's the trivial part of it the interesting thing is

1272
01:30:00,740 --> 01:30:04,310
how do i get the new y n plus one and so the best way

1273
01:30:04,310 --> 01:30:06,990
to write it as that y n plus one

1274
01:30:07,000 --> 01:30:10,160
minus y and

1275
01:30:10,250 --> 01:30:11,740
divided by

1276
01:30:12,990 --> 01:30:17,070
well why and site why n

1277
01:30:17,130 --> 01:30:19,750
plus one minus y and is

1278
01:30:19,770 --> 01:30:21,310
is this line

1279
01:30:21,380 --> 01:30:29,260
the same as the line h times and so that's the way to write

1280
01:30:30,570 --> 01:30:34,700
since the computer is interested in calculating y n plus one itself

1281
01:30:34,700 --> 01:30:36,080
so on

1282
01:30:36,100 --> 01:30:38,400
it's insulting terrible

1283
01:30:38,450 --> 01:30:41,720
let's simply assume that our choice for given y

1284
01:30:41,770 --> 01:30:46,010
what is way to long enough is to advance to the next cycle

1285
01:30:46,060 --> 01:30:50,370
so we will have to the size of the green destruction green

1286
01:30:50,390 --> 01:30:54,260
in order action OK so really have to state the light

1287
01:30:54,990 --> 01:30:58,100
let me

1288
01:30:58,110 --> 01:30:59,860
we do this

1289
01:31:04,470 --> 01:31:07,290
so with sixty four states

1290
01:31:10,260 --> 01:31:15,710
OK so the action is just do i see any current lights data i switch

1291
01:31:16,760 --> 01:31:20,330
assume that all the issues with the time delays are taking care of

1292
01:31:20,370 --> 01:31:26,210
so what's my action space

1293
01:31:26,260 --> 01:31:28,610
so how how how how many

1294
01:31:28,620 --> 01:31:31,530
actions do have

1295
01:31:31,580 --> 01:31:34,510
four right good OK because for each flight

1296
01:31:34,570 --> 01:31:35,640
i can

1297
01:31:38,710 --> 01:31:41,270
or change

1298
01:31:43,600 --> 01:31:47,900
so the action of four actions

1299
01:31:47,910 --> 01:31:51,100
OK what my observation

1300
01:31:51,310 --> 01:31:54,090
set here

1301
01:31:54,170 --> 01:31:55,880
this is the question

1302
01:32:00,970 --> 01:32:05,390
my sense really tell me everything that everything that i can know about

1303
01:32:06,100 --> 01:32:06,850
the world

1304
01:32:10,360 --> 01:32:14,440
so what i could have done i could have instead

1305
01:32:14,510 --> 01:32:16,450
put the

1306
01:32:16,570 --> 01:32:19,620
sensors the the inductive loops

1307
01:32:20,020 --> 01:32:21,870
into the

1308
01:32:21,870 --> 01:32:23,290
observations that

1309
01:32:23,330 --> 01:32:25,030
first try that

1310
01:32:25,080 --> 01:32:26,060
is really

1311
01:32:26,110 --> 01:32:31,400
i observe inductive loops but those are way the observation what's actually going on

1312
01:32:31,420 --> 01:32:38,220
OK so instead listing my observation space

1313
01:32:38,230 --> 01:32:43,150
the standard inductive loops

1314
01:32:43,200 --> 01:32:47,270
now what's higher fidelity way of modelling stage

1315
01:32:47,270 --> 01:32:51,200
i just want the course price or not but i start to use observations

1316
01:32:51,220 --> 01:32:53,870
about cars cassie reductive groups over time

1317
01:32:53,880 --> 01:32:55,550
to start estimated

1318
01:32:55,550 --> 01:32:57,110
q length maybe

1319
01:33:01,410 --> 01:33:02,910
you know if you have five

1320
01:33:02,920 --> 01:33:08,610
like screen and you have twenty cars passing over that not while green

1321
01:33:08,620 --> 01:33:13,880
OK you can start estimate whether it's ten costcutter twenty professor how long q might

1322
01:33:14,940 --> 01:33:18,870
so in the noisy observation on the stage

1323
01:33:18,950 --> 01:33:20,690
OK so i

1324
01:33:20,700 --> 01:33:24,170
what stated me let me relaxed you links to b

1325
01:33:24,190 --> 01:33:25,250
real numbers

1326
01:33:25,250 --> 01:33:27,410
so you really cannot happen car

1327
01:33:27,460 --> 01:33:29,050
but it makes things a bit easier

1328
01:33:29,090 --> 01:33:31,020
so i'm going to say

1329
01:33:31,060 --> 01:33:33,830
for every q we have a real numbers see how long that q is now

1330
01:33:33,830 --> 01:33:35,630
expanded and so on which is

1331
01:33:35,660 --> 01:33:42,000
for simplicity was a through real number and we have four so there are four

1332
01:33:42,040 --> 01:33:43,640
so now how many

1333
01:33:43,660 --> 01:33:48,410
states do we have

1334
01:33:48,420 --> 01:33:51,540
the distinct states

1335
01:33:51,610 --> 01:33:54,820
infinity right

1336
01:33:55,190 --> 01:33:56,790
so your point six

1337
01:33:56,810 --> 01:34:01,510
today after this example on the talk about discrete states OK discrete is clearly the

1338
01:34:01,510 --> 01:34:03,830
simplest case or with its finite

1339
01:34:03,870 --> 01:34:07,680
i realize the real problems might have

1340
01:34:08,610 --> 01:34:10,310
elements the of the state space

1341
01:34:10,780 --> 01:34:14,790
now there are solution methods to deal with that i can't cause days

1342
01:34:14,940 --> 01:34:18,230
big big big at the time but there are methods to deal with

1343
01:34:18,250 --> 01:34:20,760
six this

1344
01:34:22,030 --> 01:34:25,890
this is a where have any questions about what my states actions observations are here

1345
01:34:25,950 --> 01:34:27,540
OK i observed

1346
01:34:27,540 --> 01:34:29,700
cars passing relieves every time step

1347
01:34:29,720 --> 01:34:32,790
whether or not use that to determine

1348
01:34:32,830 --> 01:34:35,100
estimates of the killing everyone

1349
01:34:35,180 --> 01:34:38,130
i also know inherently what the state of each

1350
01:34:38,190 --> 01:34:39,330
light is

1351
01:34:39,400 --> 01:34:44,570
and the actions i can take reach layers stay change

1352
01:34:50,190 --> 01:34:51,470
but as a no

1353
01:34:51,480 --> 01:34:59,960
we've got a few more things to do

1354
01:34:59,970 --> 01:35:02,860
so were function as

1355
01:35:03,840 --> 01:35:06,350
the observation function

1356
01:35:06,420 --> 01:35:08,860
the maps

1357
01:35:15,960 --> 01:35:19,700
observations and states two

1358
01:35:21,580 --> 01:35:25,980
so really activation function is the probability of a state

1359
01:35:26,040 --> 01:35:27,370
give an

1360
01:35:27,390 --> 01:35:33,690
what you observed a conditional probability

1361
01:35:34,020 --> 01:35:36,260
this model the complex

1362
01:35:36,300 --> 01:35:38,680
OK my one is kalman filter

1363
01:35:38,710 --> 01:35:43,400
something to try to model gal queue length

1364
01:35:43,440 --> 01:35:45,140
the on this course

1365
01:35:46,530 --> 01:35:52,590
as there was going going going to be

1366
01:35:52,660 --> 01:36:00,250
tel aviv probability so we we needed additional constraints that summer of his career observation

1367
01:36:00,250 --> 01:36:05,110
is some sums to one

1368
01:36:07,510 --> 01:36:12,760
assume that given observations i have the weight featuring this probability

1369
01:36:13,830 --> 01:36:19,000
are there any probability course will tell you ways to model that

1370
01:36:19,110 --> 01:36:25,050
transition function maps states actions to next states

1371
01:36:25,080 --> 01:36:26,820
with some probability

1372
01:36:27,040 --> 01:36:31,770
if we make markov assumption we can model this probability of as

1373
01:36:32,880 --> 01:36:37,020
so if i take a state action

1374
01:36:37,020 --> 01:36:37,950
we just think about

1375
01:36:41,930 --> 01:36:43,310
euzenat along that

1376
01:36:43,890 --> 01:36:49,990
bigger than little element like out then the probability of making an infected wound

1377
01:36:51,160 --> 01:36:51,790
the tiny

1378
01:36:53,890 --> 01:36:56,740
in a long time and you don't need to worry about real life

1379
01:36:57,560 --> 01:36:59,520
i think that in the short time intervals

1380
01:37:00,600 --> 01:37:03,770
and the chance thing on the pancake is tiny

1381
01:37:04,370 --> 01:37:04,850
it's gonna be

1382
01:37:05,680 --> 01:37:06,830
the volume of the banque de

1383
01:37:12,220 --> 01:37:13,870
in it in subspace the

1384
01:37:14,310 --> 01:37:14,720
thank you

1385
01:37:15,220 --> 01:37:18,700
we got at length write down the number of dimensions

1386
01:37:20,350 --> 01:37:21,390
well thank you

1387
01:37:21,770 --> 01:37:24,910
and then you divide by the volume that that there is a growing problem

1388
01:37:25,500 --> 01:37:25,870
this is

1389
01:37:27,140 --> 01:37:29,020
the probability of making that

1390
01:37:33,390 --> 01:37:34,080
and is

1391
01:37:34,790 --> 01:37:35,520
ratio of

1392
01:37:36,560 --> 01:37:42,700
tell that length over here that i want to raise possibly quite a big number down the number one the

1393
01:37:44,060 --> 01:37:45,220
we could easily be about

1394
01:37:46,290 --> 01:37:47,390
this is a disaster

1395
01:37:48,990 --> 01:37:49,640
make one

1396
01:37:50,910 --> 01:37:52,930
in fact if make it bigger than a little help

1397
01:37:54,200 --> 01:37:55,540
essentially the constrained

1398
01:37:57,370 --> 01:37:58,020
you can't

1399
01:37:59,430 --> 01:37:59,770
go there

1400
01:38:02,220 --> 01:38:04,700
at one greater than health

1401
01:38:05,770 --> 01:38:06,390
the bad news

1402
01:38:08,060 --> 01:38:08,370
because of

1403
01:38:10,910 --> 01:38:11,390
even though

1404
01:38:12,020 --> 01:38:15,410
you wanted to in order reduce you random what properties

1405
01:38:16,290 --> 01:38:17,180
he wanted to go there

1406
01:38:18,040 --> 01:38:22,100
you what we end up with saying that still make local

1407
01:38:26,060 --> 01:38:26,700
except for

1408
01:38:27,560 --> 01:38:30,540
very very small chance finally making a proposal

1409
01:38:31,060 --> 01:38:32,140
makes the night before

1410
01:38:32,850 --> 01:38:35,290
thank you so much what it happens so rarely

1411
01:38:35,870 --> 01:38:37,040
you don't want to be there

1412
01:38:37,720 --> 01:38:38,520
in practice

1413
01:38:39,290 --> 01:38:40,120
be feasible

1414
01:38:43,290 --> 01:38:44,180
values that's long

1415
01:38:48,870 --> 01:38:49,450
there are less

1416
01:38:51,390 --> 01:38:53,680
obviously don't want to make small so

1417
01:38:56,020 --> 01:38:56,830
it is where you want me

1418
01:38:57,600 --> 01:39:01,160
what somehow know what the smallest like nobody's business

1419
01:39:01,620 --> 01:39:05,220
and that's roughly what you want know i want to be

1420
01:39:06,430 --> 01:39:10,740
you're using in the problem with something simple proposal distribution like

1421
01:39:11,080 --> 01:39:11,450
are there

1422
01:39:14,660 --> 01:39:17,720
so in practice people using those all

1423
01:39:18,790 --> 01:39:19,600
the problem is that that

1424
01:39:21,790 --> 01:39:22,870
will then find

1425
01:39:23,930 --> 01:39:25,310
the optimal at all

1426
01:39:25,770 --> 01:39:26,470
is roughly

1427
01:39:27,060 --> 01:39:28,220
the smallest length scale

1428
01:39:29,350 --> 01:39:34,120
then when they run them properly with with the smallest lengthscale that prize

1429
01:39:35,970 --> 01:39:36,870
they will find

1430
01:39:37,700 --> 01:39:41,080
that they're expecting roughly fifty percent of the total be

1431
01:39:43,020 --> 01:39:44,260
the resulting optimal

1432
01:39:44,830 --> 01:39:45,760
although it

1433
01:39:53,350 --> 01:39:54,620
is roughly half

1434
01:39:55,020 --> 01:39:56,160
probably a little bit bigger than

1435
01:39:56,810 --> 01:39:58,970
but not a million miles from

1436
01:40:07,740 --> 01:40:08,410
so i went

1437
01:40:09,120 --> 01:40:10,890
eight years later and what

1438
01:40:15,020 --> 01:40:19,580
and now we want to do is revealed way that getting rid these random walk on

1439
01:40:20,390 --> 01:40:25,490
was shown here is not to which one is that we have a random walk behavior like counting methods

1440
01:40:26,040 --> 01:40:31,450
don't get around by random what that has said that in going in a particular direction

1441
01:40:32,000 --> 01:40:32,490
in some way

1442
01:40:32,990 --> 01:40:33,660
that's that's

1443
01:40:34,060 --> 01:40:37,410
you still at involved in growing like going in the right direction

1444
01:40:40,720 --> 01:40:45,080
you keep going on short time scales and in incident direction

1445
01:40:46,450 --> 01:40:50,120
so that's golden ball goal number two it's a pain in the neck

1446
01:40:50,740 --> 01:40:56,220
have carefully tune the side if you don't know what we should something a little problem is

1447
01:40:56,870 --> 01:41:01,370
it will really pay all we find out that uses like big

1448
01:41:01,910 --> 01:41:04,830
will likely be running a simulation that will be nothing left behind

1449
01:41:05,810 --> 01:41:08,600
to make it small you're wasting time order badly

1450
01:41:09,810 --> 01:41:14,810
it would be really have methods that didn't require this special events that have a

1451
01:41:14,830 --> 01:41:19,970
tuning that we now have a robot that self-tuning automatically chooses

1452
01:41:20,430 --> 01:41:21,140
the right that's

1453
01:41:22,160 --> 01:41:24,640
in a way that is valid or not using any sort of attack

1454
01:41:25,830 --> 01:41:26,680
so let's now

1455
01:41:27,720 --> 01:41:29,330
address but those

1456
01:41:31,240 --> 01:41:31,790
it is

1457
01:41:33,470 --> 01:41:35,620
so we have the through all these years there

1458
01:41:38,000 --> 01:41:38,330
we are now

1459
01:41:38,330 --> 01:41:42,180
five point one eight meters

1460
01:41:43,950 --> 01:41:46,830
quite impressive

1461
01:41:46,870 --> 01:41:49,510
so we have a pendulum with l

1462
01:41:49,790 --> 01:41:54,930
five point one was minus point o five metres we cannot measure it any better

1463
01:41:55,990 --> 01:41:59,830
by sending because it has to be on the stretch when we naturally

1464
01:41:59,890 --> 01:42:02,200
and now you have to go all the way to the ceiling and all the

1465
01:42:02,200 --> 01:42:03,100
way down

1466
01:42:03,100 --> 01:42:06,430
markus does that the risking his life

1467
01:42:06,430 --> 01:42:09,240
and he claims that the best you can do is five said we had thirty

1468
01:42:09,240 --> 01:42:13,780
one pounds hanging on the

1469
01:42:13,830 --> 01:42:17,370
we try during the summer believe me

1470
01:42:17,370 --> 01:42:19,950
we tried with technicians of MIT

1471
01:42:19,990 --> 01:42:21,970
have that kind of you

1472
01:42:22,020 --> 01:42:25,240
and then one day it looks good but finally said no we can do it

1473
01:42:25,240 --> 01:42:27,910
you can install it here's the safety issue

1474
01:42:29,760 --> 01:42:31,080
we don't have the money

1475
01:42:31,120 --> 01:42:33,560
of all patterns here

1476
01:42:33,600 --> 01:42:36,390
in twenty six one hundred twenty lecture

1477
01:42:36,450 --> 01:42:38,600
newtonian mechanics

1478
01:42:38,660 --> 01:42:41,260
i demonstrated

1479
01:42:41,310 --> 01:42:44,140
that the period that this pendulum produces

1480
01:42:44,160 --> 01:42:49,220
it's really close within the error of measurement which will predict

1481
01:42:49,240 --> 01:42:53,510
in other words the mass of the stream is indeed negligible is small

1482
01:42:53,520 --> 01:42:57,700
compared to the mass of the object we even though we really

1483
01:42:57,760 --> 01:43:01,720
string ones i remember i don't remember what it was but it was such a

1484
01:43:01,720 --> 01:43:02,870
small fraction

1485
01:43:02,890 --> 01:43:05,040
of and that indeed

1486
01:43:05,100 --> 01:43:06,780
could be ignored

1487
01:43:06,810 --> 01:43:09,020
and sort of prediction then is

1488
01:43:09,040 --> 01:43:11,720
if you simply put is alan there

1489
01:43:11,720 --> 01:43:14,510
he predicted

1490
01:43:14,540 --> 01:43:17,700
purely on the basis of that simple equation

1491
01:43:17,760 --> 01:43:21,040
equals four point five seven

1492
01:43:21,140 --> 01:43:26,620
plus or minus o point o two seconds

1493
01:43:26,640 --> 01:43:30,220
and this is o point o two is the result of this point of five

1494
01:43:30,220 --> 01:43:33,180
is one percent every in

1495
01:43:33,200 --> 01:43:35,830
right five out of five hundred eighteen

1496
01:43:35,850 --> 01:43:38,830
is one percent

1497
01:43:38,910 --> 01:43:42,970
and so aren half percent because description

1498
01:43:43,020 --> 01:43:47,200
so you get a half percent and i rounded off

1499
01:43:47,240 --> 01:43:50,640
so that's the prediction

1500
01:43:50,680 --> 01:43:53,540
and then the two measurements

1501
01:43:53,620 --> 01:43:56,810
one five degree angle

1502
01:43:56,890 --> 01:44:00,680
and one ten degree angle an idea that ten times

1503
01:44:00,720 --> 01:44:02,910
so ten t

1504
01:44:02,950 --> 01:44:04,310
five degrees

1505
01:44:04,350 --> 01:44:05,870
and ten t

1506
01:44:06,010 --> 01:44:07,330
ten degrees

1507
01:44:07,910 --> 01:44:11,600
but it was in nineteen ninety nine those were my good days

1508
01:44:11,670 --> 01:44:14,470
well my good times right that is always

1509
01:44:14,510 --> 01:44:15,410
the good

1510
01:44:15,450 --> 01:44:16,760
and so on

1511
01:44:16,780 --> 01:44:17,930
i then

1512
01:44:17,950 --> 01:44:22,410
claims that i could do this to an accuracy of o point one second

1513
01:44:22,470 --> 01:44:26,180
and a lot of current in those days and i met at the five degree

1514
01:44:26,200 --> 01:44:28,140
and what did i find

1515
01:44:29,600 --> 01:44:31,450
truly unbelievable

1516
01:44:31,510 --> 01:44:33,410
if you're really lucky

1517
01:44:33,490 --> 01:44:34,560
i found

1518
01:44:34,560 --> 01:44:38,660
exactly that number which of course is an accident because my accuracy was no better

1519
01:44:38,660 --> 01:44:43,140
than o point one seconds and then i did a ten degree angle

1520
01:44:43,180 --> 01:44:46,330
and then i found this

1521
01:44:46,410 --> 01:44:48,080
so i demonstrated

1522
01:44:48,100 --> 01:44:52,680
that indeed five to ten degrees i still considered small angles

1523
01:44:52,720 --> 01:44:55,580
four that approximation

1524
01:44:55,620 --> 01:44:59,140
and it is within the uncertainty of my measurement

1525
01:44:59,180 --> 01:45:00,100
what you

1526
01:45:04,020 --> 01:45:06,510
i wanted to demonstrate

1527
01:45:06,560 --> 01:45:09,410
which is not so intuitive

1528
01:45:09,450 --> 01:45:12,720
that the period is independent of mass

1529
01:45:12,740 --> 01:45:15,760
which is not the case for spring

1530
01:45:15,810 --> 01:45:18,410
so now if you change the mass

1531
01:45:18,410 --> 01:45:20,100
you don't change l

1532
01:45:20,100 --> 01:45:21,100
you expect

1533
01:45:21,120 --> 01:45:23,280
no change in period

1534
01:45:23,290 --> 01:45:25,520
and that's what i really wanted

1535
01:45:25,580 --> 01:45:27,510
to show you here

1536
01:45:27,560 --> 01:45:28,660
but i can

1537
01:45:28,680 --> 01:45:30,680
therefore i cited

1538
01:45:30,680 --> 01:45:33,390
show you what i did in nineteen ninety nine

1539
01:45:33,410 --> 01:45:35,990
if you can show is that two minutes

1540
01:45:36,040 --> 01:45:38,910
version of my

1541
01:45:40,410 --> 01:45:43,790
you can judge for yourself to what extent

1542
01:45:43,850 --> 01:45:45,240
the mass

1543
01:45:45,290 --> 01:45:46,830
does not influence

1544
01:45:53,700 --> 01:45:56,790
one of the most remarkable things i just mentioned to you

1545
01:45:56,850 --> 01:45:58,580
is that the

1546
01:45:58,680 --> 01:46:02,640
period of the oscillations is independent of the mass

1547
01:46:02,640 --> 01:46:04,370
of the object

1548
01:46:04,450 --> 01:46:05,600
that would mean

1549
01:46:05,640 --> 01:46:08,540
if i joined the bald

1550
01:46:08,560 --> 01:46:11,180
and i think that was the ball

1551
01:46:11,240 --> 01:46:13,020
that you should get that same period

1552
01:46:13,120 --> 01:46:15,290
we should do not

1553
01:46:15,310 --> 01:46:18,350
i'm asking you a question before we do this

1554
01:46:18,370 --> 01:46:22,370
awful experiments

1555
01:46:22,390 --> 01:46:26,140
what the period come out to be the same or not

1556
01:46:26,200 --> 01:46:29,680
some of you think it's the same

1557
01:46:29,700 --> 01:46:31,290
have you thought about it

1558
01:46:31,290 --> 01:46:32,600
i'm a little bit

1559
01:46:32,600 --> 01:46:34,220
following these objects

1560
01:46:34,240 --> 01:46:35,490
and that therefore

1561
01:46:35,490 --> 01:46:37,240
maybe effectively the

1562
01:46:37,310 --> 01:46:40,810
the length of the string has become a little less

1563
01:46:40,830 --> 01:46:42,180
if i set up like this

1564
01:46:42,200 --> 01:46:44,850
and if the length of the spring is a little less

1565
01:46:44,850 --> 01:46:47,620
the period would be a little shorter

1566
01:46:48,410 --> 01:46:49,890
be prepared for that

1567
01:46:49,910 --> 01:46:54,950
on the other hand i'm also pretty well i'm not quite prepared

1568
01:46:55,040 --> 01:46:56,430
i'll try

1569
01:46:56,430 --> 01:46:59,950
to hold my body is horizontal as i possibly can

1570
01:46:59,950 --> 01:47:01,240
in order to

1571
01:47:01,260 --> 01:47:03,370
b at the same level as the ball

1572
01:47:03,370 --> 01:47:08,980
so again here is restricted isometry property and if every presentable condition is somewhere in

1573
01:47:08,980 --> 01:47:09,870
the middle

1574
01:47:09,890 --> 01:47:13,500
and the restricted one i can condition i told you this way more on the

1575
01:47:13,500 --> 01:47:15,390
right and

1576
01:47:15,400 --> 01:47:21,190
in my view i would really claim europe presentable condition for the variable selection problem

1577
01:47:21,190 --> 01:47:23,600
is way more restrictive

1578
01:47:23,650 --> 01:47:28,710
then the compatibility condition or the restricted l one i can condition so

1579
01:47:28,730 --> 01:47:31,400
if you're interested in doing practical stuff

1580
01:47:31,400 --> 01:47:35,650
you should be very about preventable condition

1581
01:47:35,670 --> 01:47:41,920
OK so i think it's in a way not very realistic and so

1582
01:47:41,920 --> 01:47:45,870
all we can still run it i did run it i select twenty six variables

1583
01:47:45,870 --> 01:47:50,000
in my example so what can i expect from this

1584
01:47:50,000 --> 01:47:53,400
and he was kind of a nice story which i think is important to bring

1585
01:47:53,420 --> 01:48:00,060
our that in my view this one restrict restrict that what i'm about to condition

1586
01:48:00,060 --> 01:48:03,940
or ortiz compatibility condition is kind of reasonable in practice

1587
01:48:03,940 --> 01:48:05,250
so i

1588
01:48:05,330 --> 01:48:11,670
i told you before under these conditions we get this bound on the estimation error

1589
01:48:11,690 --> 01:48:15,080
and i think this is kind of reasonable impact

1590
01:48:15,100 --> 01:48:16,620
OK so i had this one

1591
01:48:16,630 --> 01:48:18,250
northbound bound

1592
01:48:18,270 --> 01:48:20,020
if this kind of more technical

1593
01:48:20,020 --> 01:48:21,520
four million here

1594
01:48:21,540 --> 01:48:23,830
but now this is really a trivial thought

1595
01:48:23,850 --> 01:48:28,210
now you say OK the hope is maybe to get at least as strong variables

1596
01:48:29,960 --> 01:48:35,940
so suppose some of the variables has huge quite regression coefficients can we get them

1597
01:48:36,020 --> 01:48:40,420
so i define not this is not the issue this is not the set of

1598
01:48:40,420 --> 01:48:47,040
active variables these are the variables which is have sufficiently strong sufficiently large coefficients to

1599
01:48:47,040 --> 01:48:48,980
s relevant rd

1600
01:48:49,000 --> 01:48:53,960
one which have sufficiently large coefficients and is actually just copied from this formula just

1601
01:48:53,960 --> 01:48:56,400
the larger in this bound

1602
01:48:56,400 --> 01:49:01,620
and i just say here then clearly and i mean this is really trivial suppose

1603
01:49:01,630 --> 01:49:06,020
you have one of these variables and suppose to estimate will be zero

1604
01:49:06,810 --> 01:49:11,290
and this is an immediate contradiction that you have to say one norm bound

1605
01:49:11,290 --> 01:49:14,650
again suppose he would say

1606
01:49:14,670 --> 01:49:16,520
they had to a zero

1607
01:49:16,560 --> 01:49:22,060
but in fact is larger than this cannot happen because you have this bound here

1608
01:49:22,170 --> 01:49:26,480
that has a very interesting interpretation so clearly i know

1609
01:49:26,630 --> 01:49:32,330
with high probability from this stuff with high probability the exact contains all the relevant

1610
01:49:33,580 --> 01:49:35,520
and that's great news in way

1611
01:49:35,520 --> 01:49:38,920
i mean this is only one side of the story but in a way i

1612
01:49:38,920 --> 01:49:43,520
don't miss them i get them and a couple of other stuff

1613
01:49:44,770 --> 01:49:50,850
so what i'm saying is screening for detecting the relevant variables is quite

1614
01:49:50,850 --> 01:49:55,460
likely to be possible in practice

1615
01:49:55,480 --> 01:49:59,890
OK i selected twenty six variables in my example it kind of means that through

1616
01:49:59,890 --> 01:50:04,230
stuff is hopefully among the twenty six

1617
01:50:06,210 --> 01:50:09,630
so now you can move on if you say OK you

1618
01:50:09,650 --> 01:50:11,230
a zero

1619
01:50:11,250 --> 01:50:16,080
i mean this is the active they actually all the nonzero coefficients are sufficiently large

1620
01:50:16,170 --> 01:50:18,730
then you have screening forty active

1621
01:50:18,750 --> 01:50:23,080
but that may be questionable assumption but if you're willing to say OK either zero

1622
01:50:23,100 --> 01:50:28,250
or their large enough then you know DS had from lots of real actually contain

1623
01:50:28,420 --> 01:50:30,810
two active variables

1624
01:50:31,690 --> 01:50:37,690
so i didn't say it was last stands for so tibshirani in this great paper

1625
01:50:37,730 --> 01:50:43,730
i mean coined the term lasts only stands for least absolute shrinkage and selection operator

1626
01:50:43,750 --> 01:50:45,130
and here is my new

1627
01:50:45,150 --> 01:50:51,690
translation of this i think it should be rated at least absolute shrinkage and screening

1628
01:50:52,900 --> 01:50:55,730
it's really screens in the sense of

1629
01:50:55,750 --> 01:50:58,670
you catch it but the couple of other stuff

1630
01:50:58,730 --> 01:51:02,600
you don't select really for a field

1631
01:51:05,290 --> 01:51:08,270
from a practical perspective

1632
01:51:08,290 --> 01:51:12,290
if you want to do that is still the land around right

1633
01:51:12,290 --> 01:51:17,270
and i decided know well and come up later to this issue from another perspective

1634
01:51:17,270 --> 01:51:21,560
but it's hard to get good land for selection

1635
01:51:21,580 --> 01:51:26,060
well maybe some BIC would do the job but at least nobody has proved that

1636
01:51:26,060 --> 01:51:28,810
is what i've seen something like that

1637
01:51:28,870 --> 01:51:33,250
you just trying to cross validation but the relevant variables or for the through so

1638
01:51:33,250 --> 01:51:35,120
in practice is kind of nice

1639
01:51:35,170 --> 01:51:39,790
you have maybe too small and they just get too many variables

1640
01:51:39,810 --> 01:51:42,370
now why is this interesting

1641
01:51:42,390 --> 01:51:46,390
the other issue which i should mention is what is the size of this is

1642
01:51:46,390 --> 01:51:50,330
hacked of course if you have this just all the variables you always have the

1643
01:51:50,330 --> 01:51:57,170
screen property that had an interesting screening operations it's hacked for any kind of london

1644
01:51:57,190 --> 01:52:03,040
in absolute value cardinality is less or equal than meaning p this is

1645
01:52:03,080 --> 01:52:09,190
trivial to show easy to show so the lasso selects at most meaning the variables

1646
01:52:09,210 --> 01:52:12,790
if p is in the ten thousand during the millions and then it into dozens

1647
01:52:12,790 --> 01:52:16,500
or hundreds you reduce two thousand one hundred variables

1648
01:52:17,390 --> 01:52:18,250
that's great

1649
01:52:18,270 --> 01:52:19,750
and so

1650
01:52:19,810 --> 01:52:25,750
this is really the screening is a huge dimensionality reduction for your problem i mean

1651
01:52:25,750 --> 01:52:30,060
in original called covariance are not doing some sort of PCE

1652
01:52:30,100 --> 01:52:36,480
linear combination are so it's just a huge dimensionality reduction in the original covariates and

1653
01:52:36,480 --> 01:52:40,960
it is a huge gain very useful machine

1654
01:52:40,960 --> 01:52:44,940
kind of in my experience

1655
01:52:44,980 --> 01:52:49,000
OK so once you have it once we have done this reduction

1656
01:52:49,020 --> 01:52:52,580
you're down to a couple variables so to speak

1657
01:52:52,600 --> 01:52:56,310
i mean if ten is not too large and then i just say no you

1658
01:52:56,310 --> 01:52:59,620
just apply your favorite method of choice it's not

1659
01:52:59,630 --> 01:53:01,690
very high dimensional anymore

1660
01:53:01,730 --> 01:53:07,310
OK people have done some things but typically what do some sort of reestimation

1661
01:53:07,390 --> 01:53:13,400
for example you choose water really squares reestimation you're smaller set from a zero

1662
01:53:13,460 --> 01:53:16,900
and you apply some sort of BIC penalty there

1663
01:53:17,000 --> 01:53:21,600
or you thresholding of the coefficients and all reestimation

1664
01:53:21,600 --> 01:53:24,650
or if you apply adaptive lasso

1665
01:53:24,670 --> 01:53:28,830
and so on all fours so the thing is really you cannot

1666
01:53:28,900 --> 01:53:31,060
do it in one stage

1667
01:53:31,060 --> 01:53:35,870
you do the first stage for screening and in the second or even third stage

1668
01:53:35,870 --> 01:53:39,040
you need to do something else to get rid of the false positive

1669
01:53:39,230 --> 01:53:44,390
OK summary two for the last

1670
01:53:44,400 --> 01:53:45,980
variable selection

1671
01:53:46,000 --> 01:53:50,750
so if you really want to infer is zero the two activities are very ambitious

1672
01:53:50,750 --> 01:53:54,540
goal and it requires necessarily

1673
01:53:54,560 --> 01:54:00,920
it is rather restrictive assumptions theory presentable condition and that the non-zero regression coefficients are

1674
01:54:00,920 --> 01:54:02,350
sufficiently large

1675
01:54:02,370 --> 01:54:08,000
both of them are restrictive but as a kind of indicated variable screening

1676
01:54:08,060 --> 01:54:12,690
is much more realistic it works with the much weaker restricted their one i can

1677
01:54:12,960 --> 01:54:15,890
condition and and then you get the results like this

1678
01:54:16,960 --> 01:54:17,920
if you

1679
01:54:17,940 --> 01:54:19,600
how many small

1680
01:54:19,620 --> 01:54:23,830
non-zero regression coefficient in the truth it would still get i mean the ones which

1681
01:54:23,830 --> 01:54:27,310
are real the large coefficients you get

1682
01:54:27,330 --> 01:54:32,460
and if you assume the truth is such that it's either zero or the coefficients

1683
01:54:32,460 --> 01:54:38,080
are sufficiently large actually screen for the true underlying it

1684
01:54:38,100 --> 01:54:44,960
again mainly focused on the lasso in linear models many extensions have been worked out

1685
01:54:45,420 --> 01:54:50,670
group lasso fused lasso so and so on and so forth some people but is

1686
01:54:50,670 --> 01:54:57,960
concave penalties they argue to have better selection properties scavenge seed class and so on

1687
01:54:58,020 --> 01:55:00,980
matching pursuit boosting

1688
01:55:01,000 --> 01:55:03,060
again on the rough scale

1689
01:55:03,060 --> 01:55:08,190
i would say it's kind of a similar story i'm of i mean the details

1690
01:55:08,190 --> 01:55:12,440
one of these

1691
01:55:19,660 --> 01:55:20,180
given the

1692
01:55:20,210 --> 01:55:23,660
so we can all

1693
01:55:28,250 --> 01:55:32,390
you will find a way to make them

1694
01:55:32,440 --> 01:55:34,040
so what

1695
01:55:34,060 --> 01:55:38,290
what about

1696
01:55:43,810 --> 01:55:47,080
call four

1697
01:55:47,100 --> 01:55:49,770
i wouldn't say what

1698
01:55:49,890 --> 01:55:51,440
we are one

1699
01:55:51,460 --> 01:55:54,560
what have

1700
01:55:54,580 --> 01:55:59,180
you have

1701
01:55:59,310 --> 01:56:01,730
the car is

1702
01:56:09,230 --> 01:56:11,850
what i'm going to

1703
01:56:15,180 --> 01:56:19,080
suffering from

1704
01:56:25,270 --> 01:56:36,580
sure there

1705
01:56:47,830 --> 01:56:50,160
so the

1706
01:56:52,270 --> 01:56:56,580
we need to focus on

1707
01:57:16,730 --> 01:57:18,270
you might

1708
01:57:24,770 --> 01:57:26,480
to that

1709
01:57:26,500 --> 01:57:27,440
march most

1710
01:58:22,540 --> 01:58:25,270
would be great

1711
01:58:25,440 --> 01:58:34,270
it's not like the

1712
01:58:35,620 --> 01:58:38,270
you know what

1713
01:58:44,640 --> 01:58:48,080
as a lot more

1714
01:58:49,810 --> 01:58:51,750
the boolean model

1715
01:58:52,580 --> 01:58:54,410
no they

1716
01:59:04,830 --> 01:59:06,040
the from

1717
01:59:07,190 --> 01:59:08,600
i'm sure

1718
01:59:08,620 --> 01:59:12,790
i have one of

1719
01:59:15,620 --> 01:59:19,100
so so

1720
01:59:27,000 --> 01:59:29,500
so o

1721
01:59:29,520 --> 01:59:32,520
so this seems to

1722
01:59:39,060 --> 01:59:45,870
you know you can which we can or

1723
01:59:48,290 --> 01:59:53,480
one class of people

1724
01:59:53,500 --> 02:00:00,640
now the one we are going to post

1725
02:00:00,750 --> 02:00:03,620
one of them

1726
02:00:07,140 --> 02:00:11,910
hi drawn on the left

1727
02:00:11,940 --> 02:00:15,790
but we're not

1728
02:00:15,790 --> 02:00:18,560
when i say

1729
02:00:18,580 --> 02:00:20,180
well we don't have

1730
02:00:21,660 --> 02:00:25,710
one with a lot of things that

1731
02:00:25,730 --> 02:00:27,850
you know

1732
02:00:27,870 --> 02:00:29,580
i do

1733
02:00:29,620 --> 02:00:31,750
i have had

1734
02:00:31,960 --> 02:00:34,180
what they did

1735
02:00:35,730 --> 02:00:39,020
there is a primary goal

1736
02:00:56,100 --> 02:00:57,870
what mean

1737
02:00:57,890 --> 02:01:00,190
or no

1738
02:01:02,100 --> 02:01:04,770
well what good thing

1739
02:01:06,270 --> 02:01:11,830
well what about these questions

1740
02:01:11,850 --> 02:01:14,980
so we have to

1741
02:01:17,730 --> 02:01:18,920
because you want

1742
02:01:19,060 --> 02:01:22,080
you know

1743
02:01:26,890 --> 02:01:31,600
we are one

1744
02:01:31,620 --> 02:01:36,770
are they a each

1745
02:01:39,750 --> 02:01:41,310
what what

1746
02:01:42,350 --> 02:01:45,690
well what is a

1747
02:01:45,710 --> 02:01:52,330
you can

1748
02:01:52,390 --> 02:01:57,520
we try to go

1749
02:02:01,020 --> 02:02:04,440
i e

1750
02:02:05,500 --> 02:02:08,160
can do

1751
02:02:18,120 --> 02:02:22,830
or that

1752
02:02:22,850 --> 02:02:23,980
with your

1753
02:02:24,100 --> 02:02:28,710
it is

1754
02:02:28,770 --> 02:02:31,690
we want

1755
02:02:44,460 --> 02:02:46,980
really hard

1756
02:02:47,060 --> 02:02:49,020
o thing

1757
02:02:51,500 --> 02:02:59,540
what you

1758
02:03:00,270 --> 02:03:01,750
there are no

1759
02:03:06,060 --> 02:03:10,370
and what

1760
02:03:10,440 --> 02:03:12,560
i mean

1761
02:03:12,790 --> 02:03:19,270
you have

1762
02:03:19,870 --> 02:03:21,750
she she

1763
02:03:21,750 --> 02:03:23,160
estimating it on

1764
02:03:23,180 --> 02:03:25,190
trick something

1765
02:03:28,360 --> 02:03:33,000
the next thing is that we can

1766
02:03:33,080 --> 02:03:35,940
remove that

1767
02:03:35,970 --> 02:03:38,360
expectation here

1768
02:03:38,380 --> 02:03:41,080
through the same

1769
02:03:41,130 --> 02:03:44,190
and we will only actually increase

1770
02:03:44,200 --> 02:03:46,080
the value

1771
02:03:46,100 --> 02:03:48,080
the reason for that is

1772
02:03:48,100 --> 02:03:53,940
OK we're still going to randomly trying to these expectations together on the outside it

1773
02:03:53,950 --> 02:03:56,330
so the reason we can move through the city

1774
02:03:56,340 --> 02:03:57,420
is there

1775
02:03:57,440 --> 02:03:59,120
if we have the

1776
02:03:59,130 --> 02:04:01,780
expectation inside the

1777
02:04:01,800 --> 02:04:07,370
we don't have the option of optimizing h with respect to a particular

1778
02:04:07,390 --> 02:04:08,740
and still

1779
02:04:08,760 --> 02:04:12,630
it's sort of h has to be optimized to the average of the still

1780
02:04:12,650 --> 02:04:16,910
but if the answer to the outside h has the option of not only optimizing

1781
02:04:16,910 --> 02:04:19,800
with respect to s but also with respect to us till

1782
02:04:19,820 --> 02:04:22,720
so we can do better you can actually get bigger

1783
02:04:22,740 --> 02:04:26,290
and therefore the actual expectation

1784
02:04:26,330 --> 02:04:28,030
will increase

1785
02:04:28,050 --> 02:04:29,500
can increase

1786
02:04:29,520 --> 02:04:31,500
so we actually get the

1787
02:04:31,520 --> 02:04:32,690
this is now

1788
02:04:32,700 --> 02:04:35,530
that was what we started with is less than or equal to

1789
02:04:35,550 --> 02:04:37,710
the expectation of the

1790
02:04:37,720 --> 02:04:41,790
with respect to the sample and the expectation with respect to the

1791
02:04:41,800 --> 02:04:44,180
go sample all of this

1792
02:04:44,200 --> 02:04:46,720
which was what was left inside

1793
02:04:46,770 --> 02:04:48,160
which is this summer

1794
02:04:48,210 --> 02:04:51,230
where we had the average of the

1795
02:04:51,250 --> 02:04:55,190
ghost sample and the average of the true sample and i just put the two

1796
02:04:55,190 --> 02:04:58,120
sons together make into single some

1797
02:04:59,150 --> 02:05:00,300
so now

1798
02:05:01,020 --> 02:05:04,810
we're at the point where we can do symmetrisation so things sort of go very

1799
02:05:04,810 --> 02:05:09,790
smoothly in this in this view of the world you know sort of

1800
02:05:09,800 --> 02:05:11,880
so what these things in as you go

1801
02:05:11,890 --> 02:05:15,580
symmetrisation will now correspond

1802
02:05:15,590 --> 02:05:16,680
to just

1803
02:05:16,690 --> 02:05:23,600
again with the same thing exchanging corresponding elements remember we took the first element of

1804
02:05:23,600 --> 02:05:28,360
the sample and the first element to the ghost sample and swap or didn't and

1805
02:05:28,360 --> 02:05:31,180
the first element second the second self

1806
02:05:31,200 --> 02:05:35,100
now if we do that that corresponds to just swapping these two

1807
02:05:35,110 --> 02:05:39,540
guys write z one until the and z one here

1808
02:05:39,550 --> 02:05:41,970
and said to till the and z two

1809
02:05:42,050 --> 02:05:44,630
what happens if we did what those two

1810
02:05:44,650 --> 02:05:49,180
all that would happen would be that the sign of this

1811
02:05:49,200 --> 02:05:53,310
some of this entry in this some somewhat change

1812
02:05:53,330 --> 02:05:55,880
OK instead of being a hz z i

1813
02:05:55,900 --> 02:06:01,360
so one till the minus eighty percent one it will be hz one minus hz

1814
02:06:01,360 --> 02:06:02,400
one till

1815
02:06:02,410 --> 02:06:03,800
if we made this one

1816
02:06:03,820 --> 02:06:07,320
so we can make this wall by just multiplying by

1817
02:06:07,340 --> 02:06:08,900
a minus one

1818
02:06:08,910 --> 02:06:12,840
so that's the next

1819
02:06:14,460 --> 02:06:18,040
sorry control night

1820
02:06:18,210 --> 02:06:20,960
you know that's right so that's the trick

1821
02:06:20,980 --> 02:06:25,090
it is to just put in a sigma i here

1822
02:06:25,610 --> 02:06:29,450
which is going to be variable which can be plus or minus one with equal

1823
02:06:32,570 --> 02:06:36,830
and that sigma into the expectation here

1824
02:06:36,850 --> 02:06:39,800
so that now is an expectation over sigma

1825
02:06:39,810 --> 02:06:41,590
and because of the fact that

1826
02:06:41,610 --> 02:06:45,200
the two samples are generated according to the same distribution

1827
02:06:45,250 --> 02:06:48,040
there is nothing special about

1828
02:06:48,050 --> 02:06:48,800
you know

1829
02:06:48,810 --> 02:06:50,480
as an as still there

1830
02:06:50,490 --> 02:06:54,010
the equivalent swapping will not change the value

1831
02:06:54,020 --> 02:06:55,570
of the

1832
02:06:55,580 --> 02:06:58,380
of the probability of the expectation

1833
02:06:58,390 --> 02:07:00,940
so exactly the same argument we had before

1834
02:07:00,960 --> 02:07:05,510
he was saying OK if you choose the sample and then apply a random permutation

1835
02:07:05,510 --> 02:07:10,080
it won't change the probability of that particular set of examples being generated

1836
02:07:10,110 --> 02:07:11,720
if we swap those two

1837
02:07:11,740 --> 02:07:13,920
it won't change the probability

1838
02:07:14,140 --> 02:07:16,700
it's just as likely under that

1839
02:07:16,760 --> 02:07:19,540
choice of sigma s under any other choice of sigma

1840
02:07:19,550 --> 02:07:22,550
so adding in those are not change the probability

1841
02:07:23,560 --> 02:07:25,430
so you know

1842
02:07:25,450 --> 02:07:27,080
things go very smoothly

1843
02:07:27,090 --> 02:07:29,050
and now on this point

1844
02:07:29,060 --> 02:07:32,290
what we're going to do is to

1845
02:07:32,300 --> 02:07:34,660
split apart this sum now

1846
02:07:35,610 --> 02:07:39,770
the two some something my age that i tell them

1847
02:07:39,780 --> 02:07:43,870
and something that minus some sigma z i

1848
02:07:44,130 --> 02:07:47,390
sorry seem i hz i

1849
02:07:47,410 --> 02:07:49,550
and then that the difference of two

1850
02:07:49,570 --> 02:07:50,910
things are going

1851
02:07:51,150 --> 02:07:55,940
band that by the sum of the absolute values of the two

1852
02:07:55,970 --> 02:07:59,990
individual sums but they're both the the same some effectively

1853
02:08:00,930 --> 02:08:06,080
that's something my agent said i expectation with respect to some

1854
02:08:06,080 --> 02:08:09,240
sample and we're going to also

1855
02:08:09,650 --> 02:08:13,900
take the suit into both of which again is just going to increase the value

1856
02:08:13,900 --> 02:08:16,190
because we can take this on either

1857
02:08:16,210 --> 02:08:18,340
so we just get twice

1858
02:08:18,350 --> 02:08:19,670
this value here

1859
02:08:19,690 --> 02:08:23,940
so it's quite a few steps in that stage from here to here

1860
02:08:23,970 --> 02:08:26,190
maybe it's a little

1861
02:08:26,210 --> 02:08:28,410
condensed but hopefully

1862
02:08:28,410 --> 02:08:30,840
you can see the basically

1863
02:08:31,620 --> 02:08:33,690
some here

1864
02:08:33,700 --> 02:08:37,570
and will be if you separate that will have exactly the same sum is the

1865
02:08:37,570 --> 02:08:43,010
sum will occur twice once was until the once was that i and effectively

1866
02:08:43,030 --> 02:08:46,340
we can take the issue to be super bowl

1867
02:08:46,340 --> 02:08:48,290
and separate them into two

1868
02:08:48,320 --> 02:08:51,940
and will only increase the resulting values

1869
02:08:51,960 --> 02:08:58,120
and so that just then to just comes outside and get this presentation is that

1870
02:08:58,120 --> 02:09:01,840
it's not okay i can write up some steps on the slide if you prefer

1871
02:09:03,750 --> 02:09:06,940
these people walking the plank

