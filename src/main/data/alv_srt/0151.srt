1
00:00:00,000 --> 00:00:01,230
so that you have

2
00:00:01,290 --> 00:00:05,540
a total forest of something like two million decision tree nodes so we've got this

3
00:00:05,540 --> 00:00:09,160
extremely complicated classifier

4
00:00:09,180 --> 00:00:12,620
and yet it's performing really well on this dataset

5
00:00:12,680 --> 00:00:17,200
which is really kind of surprising was also surprising even more surprising is that the

6
00:00:17,200 --> 00:00:18,180
test here

7
00:00:18,230 --> 00:00:22,310
continues to drop even after the training year has reached zero

8
00:00:22,370 --> 00:00:24,600
so if you look at these numbers down here

9
00:00:24,600 --> 00:00:26,830
so after five rounds the training here

10
00:00:27,770 --> 00:00:31,080
after a thousand around the training year course is still zero

11
00:00:31,140 --> 00:00:33,540
what is occam's razor say

12
00:00:33,540 --> 00:00:35,330
here you've got

13
00:00:35,350 --> 00:00:39,560
an explanation of the data which is two hundred times more complicated

14
00:00:39,580 --> 00:00:41,310
then you have over here

15
00:00:41,310 --> 00:00:43,390
so occam's razor says

16
00:00:43,410 --> 00:00:46,960
this once it should certainly be people performing worse

17
00:00:46,980 --> 00:00:48,460
and this one

18
00:00:48,480 --> 00:00:51,620
and that's exactly the opposite of what we actually observe

19
00:00:51,640 --> 00:00:55,350
instead what we observe is that the test there is a point four percent after

20
00:00:55,350 --> 00:00:56,500
five rounds

21
00:00:56,500 --> 00:00:59,810
after thousand rounds down to three point one percent

22
00:00:59,960 --> 00:01:02,500
question about

23
00:01:02,520 --> 00:01:08,370
so that's the one thing that is only going up for one round and round

24
00:01:10,290 --> 00:01:12,020
so some of this

25
00:01:12,160 --> 00:01:16,680
it but it's very exaggerated on this graph because it's a logarithmic scale

26
00:01:16,680 --> 00:01:19,430
so it's kind of a

27
00:01:19,430 --> 00:01:23,890
kind of analogous to what happened on that toy problem that toy example

28
00:01:23,890 --> 00:01:28,450
where we started out with an of thirty percent

29
00:01:28,460 --> 00:01:31,100
it was

30
00:01:31,100 --> 00:01:33,390
let's say

31
00:01:33,410 --> 00:01:36,830
right so in that example you started out with an area of thirty percent on

32
00:01:36,830 --> 00:01:38,040
the next round

33
00:01:38,060 --> 00:01:42,850
the weak classifier had an area of twenty one percent so would have higher

34
00:01:42,890 --> 00:01:46,200
weight and on the first round so it tends to happen is

35
00:01:46,350 --> 00:01:48,060
in the second round

36
00:01:48,080 --> 00:01:50,700
the second we classify tends to have higher weight

37
00:01:50,750 --> 00:01:54,750
in the first week classifier to be taken into what weighted majority vote of just

38
00:01:54,750 --> 00:01:55,730
those two

39
00:01:55,930 --> 00:01:58,770
the one with larger weight will always dominate

40
00:01:58,790 --> 00:02:02,460
the other one so effectively you're just looking at that second we classifier which tends

41
00:02:02,480 --> 00:02:03,830
to be worse

42
00:02:05,560 --> 00:02:10,700
training in

43
00:02:10,730 --> 00:02:13,660
the training here in principle can go up

44
00:02:15,770 --> 00:02:19,060
i don't see that very often i'm not sure i've ever seen

45
00:02:19,080 --> 00:02:25,890
other than that second round of boosting the bound on the training are always decreases

46
00:02:37,080 --> 00:02:39,230
i'm sorry

47
00:02:46,120 --> 00:02:50,350
question is do we need to prevent overfitting the weak classifiers

48
00:03:00,060 --> 00:03:05,040
so if the weak classifiers are really overfitting the data then there's nothing boosting can

49
00:03:05,810 --> 00:03:07,020
to help

50
00:03:07,040 --> 00:03:07,870
right so

51
00:03:07,870 --> 00:03:10,660
they do have to do some kind of prevention of over fitting of the weak

52
00:03:11,810 --> 00:03:15,060
there have been some experiments where people are

53
00:03:15,120 --> 00:03:17,200
term pruning on or off

54
00:03:17,220 --> 00:03:20,910
for decision tree algorithms pruning is supposed to prevent overfitting

55
00:03:20,950 --> 00:03:25,250
and that turns out not to make much of a difference for boosting but certainly

56
00:03:25,250 --> 00:03:28,350
need to do some prevention of overfitting

57
00:03:28,700 --> 00:03:35,330
i'm sorry

58
00:03:37,370 --> 00:03:42,230
if think is coming one

59
00:03:42,330 --> 00:03:49,930
i still quite a really loud

60
00:03:51,480 --> 00:03:54,930
it was producing a combination of four by

61
00:03:58,930 --> 00:04:00,600
o was that what we're doing here

62
00:04:02,520 --> 00:04:05,120
that's what i would like to think

63
00:04:05,500 --> 00:04:07,980
that that's

64
00:04:08,000 --> 00:04:11,000
that's exactly what we're doing

65
00:04:13,080 --> 00:04:15,580
there goes OK

66
00:04:15,800 --> 00:04:19,370
that's what we're doing we're using c four point five is the weak learning algorithm

67
00:04:19,370 --> 00:04:21,350
in this particular experiment

68
00:04:21,350 --> 00:04:24,020
we're using boosting on top of that

69
00:04:28,910 --> 00:04:34,500
he is so i

70
00:04:42,750 --> 00:04:47,890
so if use

71
00:04:49,450 --> 00:04:53,620
OK so so again it's a question about overfitting

72
00:04:53,620 --> 00:04:58,790
so that says i'm holding the flag which represents the likelihood that has the right

73
00:04:58,790 --> 00:05:02,130
here at this time and i'm going to go for it and make sure they

74
00:05:02,200 --> 00:05:03,560
are properly represent

75
00:05:03,570 --> 00:05:05,720
right so

76
00:05:05,750 --> 00:05:10,660
in which each node you replace the birds arriving at that node with a single

77
00:05:10,680 --> 00:05:15,490
about that carries the sum of their values and now you don't need an exponential

78
00:05:15,490 --> 00:05:16,790
number of bugs

79
00:05:16,800 --> 00:05:19,410
you just need a quadratic number of bugs

80
00:05:19,430 --> 00:05:23,470
you end up here you have to take them in square and then it is

81
00:05:23,480 --> 00:05:28,230
representative democracy thing you get back down and then you replicated in square back down

82
00:05:28,240 --> 00:05:31,560
and and so on so you never need more than and squared where n is

83
00:05:31,560 --> 00:05:33,540
the number of states

84
00:05:33,540 --> 00:05:37,080
so the trail is called dynamic programming

85
00:05:37,140 --> 00:05:43,990
extremely powerful algorithm comes up and all kinds of fields from statistics to engineering to

86
00:05:43,990 --> 00:05:46,070
computer science

87
00:05:46,090 --> 00:05:50,790
and it can be used whenever you have any kind of summation or search for

88
00:05:50,790 --> 00:05:55,470
maximisation problem that can be set up on the grid in in this way so

89
00:05:55,470 --> 00:05:57,140
that the values

90
00:05:57,190 --> 00:06:01,310
something as it arrives at a particular state

91
00:06:01,320 --> 00:06:04,430
depends you only care about one

92
00:06:04,450 --> 00:06:05,370
o point in the greatest

93
00:06:05,680 --> 00:06:09,120
that you don't care about its history once once again

94
00:06:09,140 --> 00:06:10,480
so ago

95
00:06:10,580 --> 00:06:15,070
don't have to be time state they can be anything else that that you want

96
00:06:15,090 --> 00:06:22,080
they can be you know position and and high or anything else to compute various

97
00:06:22,110 --> 00:06:23,570
difficult some

98
00:06:23,580 --> 00:06:28,110
so this is exactly what population

99
00:06:28,130 --> 00:06:31,720
it's exactly dynamic programming on the screen

100
00:06:31,720 --> 00:06:37,280
so that was the trick for computing the likelihood and the last thing here is

101
00:06:37,280 --> 00:06:42,410
just to show that you can do the same thing for computing in particular marginal

102
00:06:42,410 --> 00:06:47,060
this is now posterior marginal probability of xt given y

103
00:06:47,080 --> 00:06:50,170
at some time so there's a simple

104
00:06:50,240 --> 00:06:59,520
equation here for computing this which involves another recursion variable data energies run recursion backwards

105
00:06:59,560 --> 00:07:01,490
and i can show

106
00:07:01,960 --> 00:07:06,540
the corresponding trick is here on the graph

107
00:07:07,560 --> 00:07:12,820
we need to compute the marginal probability on this HMM graph in egypt

108
00:07:13,020 --> 00:07:19,450
lot particular note or some combination of state in time and ask what is this

109
00:07:19,650 --> 00:07:24,100
the termites

110
00:07:25,350 --> 00:07:31,280
what is the probability that in generating the observation sequence

111
00:07:31,460 --> 00:07:36,270
i going go through a particular state at time xt

112
00:07:36,290 --> 00:07:41,040
if you do that then you could just divide that by the total probability that

113
00:07:41,040 --> 00:07:45,010
would be the first year of being in that state at that time

114
00:07:45,030 --> 00:07:47,470
as i see that

115
00:07:47,480 --> 00:07:52,830
if you ask the sum of the paths that happened to go through this node

116
00:07:52,870 --> 00:07:56,240
then just by dividing that some by the total sum which we already saw the

117
00:07:56,240 --> 00:07:58,320
computer you can get the posterior

118
00:08:02,680 --> 00:08:08,880
this is to do this we just basically equivalent trick from

119
00:08:08,900 --> 00:08:10,970
both sides so we

120
00:08:10,970 --> 00:08:13,410
we run this recursion for

121
00:08:13,480 --> 00:08:16,500
from time zero up to this time t here

122
00:08:16,520 --> 00:08:20,800
and we compute the sum of all the things that land upon this node using

123
00:08:20,800 --> 00:08:22,540
the same trick i showed you before

124
00:08:22,550 --> 00:08:27,130
and then we started the end and one of the bugs this way and we

125
00:08:27,130 --> 00:08:32,480
compute many of them end up here and that we add those quantities together and

126
00:08:32,490 --> 00:08:33,680
that gives us

127
00:08:33,740 --> 00:08:35,830
the sum one

128
00:08:37,330 --> 00:08:39,630
so actually

129
00:08:39,650 --> 00:08:44,650
and this is the the same triggers in belief propagation we can actually just run

130
00:08:44,660 --> 00:08:49,510
one forward pass to compute all the alphas and another backward pass to compute all

131
00:08:49,520 --> 00:08:51,660
debaters and any

132
00:08:51,740 --> 00:08:55,780
what we want is the product of alpha and beta that node

133
00:08:55,990 --> 00:09:00,880
so it looks here like you to do special recursion to get the posterior at

134
00:09:00,880 --> 00:09:05,120
it's the solution if you like to the differential equation that's a fancy way of

135
00:09:05,120 --> 00:09:06,010
saying it

136
00:09:06,030 --> 00:09:07,710
the y dt

137
00:09:07,740 --> 00:09:11,000
equals a times y

138
00:09:11,090 --> 00:09:15,710
now course test is not unique we can make it unique by putting an initial

139
00:09:15,710 --> 00:09:20,280
value so if i want to get this function and not constant times the i

140
00:09:20,940 --> 00:09:25,400
make this an initial value problem and say that y is zero should be one

141
00:09:25,400 --> 00:09:27,720
and now i will get only the function

142
00:09:27,790 --> 00:09:28,990
in the eighty

143
00:09:29,000 --> 00:09:33,330
so in other words that characterizes this function

144
00:09:33,340 --> 00:09:37,950
it's the only function of the whole world which has that property

145
00:09:38,000 --> 00:09:41,990
now if you could call something into the i data

146
00:09:42,030 --> 00:09:43,620
it should have

147
00:09:43,700 --> 00:09:45,670
we want that to be true

148
00:09:47,390 --> 00:09:52,490
so here my question is it true that e to the i data one

149
00:09:52,490 --> 00:09:57,570
let's use that time the i th data two

150
00:09:57,580 --> 00:10:02,490
i see i'm on a collision course here but that's easily fixed

151
00:10:02,500 --> 00:10:11,500
does that equal to e to the i data one state two

152
00:10:11,510 --> 00:10:14,640
if that turns out to be so

153
00:10:14,690 --> 00:10:17,400
that's all big step

154
00:10:17,460 --> 00:10:21,580
what would we like to be true here well will be true that the derivative

155
00:10:21,580 --> 00:10:23,130
with respect to t

156
00:10:23,170 --> 00:10:28,620
of the i th data i would like that to come be equal to or

157
00:10:28,620 --> 00:10:31,920
a times

158
00:10:31,980 --> 00:10:33,690
e to the data

159
00:10:33,770 --> 00:10:36,650
so the question

160
00:10:41,660 --> 00:10:45,500
i think those are the two most significant things now the nodes do a third

161
00:10:45,500 --> 00:10:48,590
thing talk about infinite series

162
00:10:48,600 --> 00:10:50,230
i since

163
00:10:50,270 --> 00:10:55,220
we haven't done infinite series was anyway is not officially part of the syllabus

164
00:10:55,260 --> 00:11:00,350
the kind of power series that are required but i will put it down for

165
00:11:00,350 --> 00:11:03,600
the sake of completeness as people like to say so

166
00:11:04,060 --> 00:11:05,370
it should

167
00:11:05,630 --> 00:11:09,340
hey right with there is the infinite series should be nice

168
00:11:09,380 --> 00:11:11,210
infinite series should

169
00:11:11,350 --> 00:11:13,740
work out should

170
00:11:13,820 --> 00:11:16,770
work should be

171
00:11:16,820 --> 00:11:23,270
there's no way for this should work out let's say

172
00:11:23,320 --> 00:11:25,670
it was a little music

173
00:11:27,020 --> 00:11:29,660
we are music i is it only me that here

174
00:11:29,690 --> 00:11:38,770
you have

175
00:11:38,850 --> 00:11:45,040
somebody up there i feel i'm being watched up there was a terrible

176
00:11:48,220 --> 00:11:49,340
so there's one guy

177
00:11:49,350 --> 00:11:51,370
here's another guy

178
00:11:51,380 --> 00:11:57,530
and i will put a box around the infinite series is i'm not going to

179
00:11:57,530 --> 00:11:59,290
say anything about it

180
00:12:01,280 --> 00:12:06,640
these things in fact are both true otherwise why would i be saying and why

181
00:12:06,640 --> 00:12:12,410
would oiler made the formula but what's interesting to see is what's behind them

182
00:12:12,440 --> 00:12:17,240
let's select and that gives you a little practice also in calculating with the complex

183
00:12:18,120 --> 00:12:22,960
so let's look at the first one what it say what it is asking the

184
00:12:22,960 --> 00:12:24,520
question it says please

185
00:12:24,570 --> 00:12:28,990
calculate the product of these two things OK i do what i'm told i will

186
00:12:28,990 --> 00:12:31,390
calculate the product of cosine

187
00:12:31,400 --> 00:12:32,540
they don't want

188
00:12:32,560 --> 00:12:35,990
plus i cosine created two

189
00:12:42,390 --> 00:12:46,830
so i paid one

190
00:12:46,870 --> 00:12:51,360
that's EDI the i data one right that corresponds to

191
00:12:52,110 --> 00:12:54,840
the other factor times the other factor

192
00:12:57,610 --> 00:13:00,160
data to class i sin theta

193
00:13:00,640 --> 00:13:02,870
OK what is that come out to be

194
00:13:03,000 --> 00:13:06,000
well again will use the method of grouping

195
00:13:06,010 --> 00:13:10,280
what's the real part of the the real part of it is co-signed they a

196
00:13:11,020 --> 00:13:12,880
cosine data two

197
00:13:14,750 --> 00:13:17,960
and then there's the real part which comes from these two factors

198
00:13:17,990 --> 00:13:21,480
it's going to occur with a minus sign because of the i squared and what's

199
00:13:21,480 --> 00:13:23,360
left is signed data one

200
00:13:23,370 --> 00:13:26,380
signed data two

201
00:13:26,440 --> 00:13:32,110
and the imaginary part of factor out the all i

202
00:13:32,120 --> 00:13:35,840
and then what's left we'll but i want to keep repeating the eye so it

203
00:13:35,840 --> 00:13:36,570
will be

204
00:13:37,250 --> 00:13:39,660
data one cosine data two

205
00:13:39,740 --> 00:13:46,210
and the other factor will be

206
00:13:47,530 --> 00:13:50,120
cosine data one signed data two

207
00:13:52,130 --> 00:13:53,160
they two

208
00:13:53,170 --> 00:13:56,340
cosine data one

209
00:13:57,120 --> 00:13:59,540
well looks like a mess but

210
00:13:59,560 --> 00:14:01,660
again high school to the rescue

211
00:14:01,670 --> 00:14:05,560
what is this

212
00:14:05,610 --> 00:14:08,980
the top thing is nothing in the sky

213
00:14:09,860 --> 00:14:12,620
a disguised form

214
00:14:12,670 --> 00:14:16,230
cosine if theta one plus they two

215
00:14:16,240 --> 00:14:19,730
and the bottom line is

216
00:14:19,740 --> 00:14:23,690
sign theta one of the two

217
00:14:27,540 --> 00:14:31,090
so the product of these two things is this

218
00:14:31,100 --> 00:14:33,580
and that's exactly the formula

219
00:14:33,590 --> 00:14:39,350
in other words this formula is the way of writing those two trigonometric identities for

220
00:14:39,350 --> 00:14:43,910
the cosine of the sum and the sign of the sum

221
00:14:43,970 --> 00:14:48,740
instead of the two identities taking up that much space written one after the other

222
00:14:48,740 --> 00:14:53,730
they take up this much space and they say exactly the same thing

223
00:14:53,760 --> 00:14:58,940
those two trigonometric identities are exactly the same as saying that you the i th

224
00:14:58,940 --> 00:15:01,580
data satisfies the exponential law

225
00:15:01,600 --> 00:15:03,370
now you know

226
00:15:03,380 --> 00:15:05,830
people ask you know what

227
00:15:05,860 --> 00:15:10,120
was beautiful in mathematics to me that's beautiful but i think that's great

228
00:15:10,160 --> 00:15:15,210
something along turns into something short and it's just as good and moreover connects with

229
00:15:15,210 --> 00:15:19,670
all these other things in the world differential equations infant survival

230
00:15:20,730 --> 00:15:23,330
i had to sell oil results themselves

231
00:15:25,190 --> 00:15:28,480
how about the other one

232
00:15:28,600 --> 00:15:30,310
how about the other one

233
00:15:34,110 --> 00:15:36,830
that's obviously

234
00:15:38,710 --> 00:15:45,990
has been well i haven't said something because i for one thing a differentiated this

235
00:15:45,990 --> 00:15:47,280
eighty here

236
00:15:47,360 --> 00:15:59,640
t down there are clouds OK that's easily fixed

237
00:15:59,700 --> 00:16:03,490
but at what says how differentiate this

238
00:16:03,530 --> 00:16:06,990
what kind of the guy is in the i data well

239
00:16:07,410 --> 00:16:09,380
if i right now

240
00:16:09,440 --> 00:16:12,410
take a look at what it is it's cosine data

241
00:16:12,470 --> 00:16:14,820
plus i signed data

242
00:16:14,860 --> 00:16:19,740
i stated very is the function

243
00:16:19,740 --> 00:16:22,620
the variable is real

244
00:16:22,630 --> 00:16:24,860
fate is the real variable

245
00:16:24,860 --> 00:16:30,330
you know it's the angle in radians better runs from negative infinity to infinity

246
00:16:30,360 --> 00:16:35,010
so what if you think functions as a black box what's going on is a

247
00:16:35,010 --> 00:16:39,470
real number but what's coming out is a complex number

248
00:16:39,530 --> 00:16:41,690
so schematically

249
00:16:41,720 --> 00:16:42,940
if p is the

250
00:16:42,950 --> 00:16:45,350
i data box and you like to think that way

251
00:16:45,350 --> 00:16:46,700
data goes in

252
00:16:46,700 --> 00:16:50,660
i'm very quickly going to recap the first half i talked about people in the

253
00:16:50,660 --> 00:16:52,520
very can think that

254
00:16:52,530 --> 00:16:55,360
that will be useful idea and then

255
00:16:55,380 --> 00:16:58,550
as i promised gonna move on to the topic of the

256
00:17:00,740 --> 00:17:03,950
first off i started by saying that we need to get numbers into a computer

257
00:17:03,950 --> 00:17:06,790
somehow so we have a data which is

258
00:17:06,880 --> 00:17:10,900
in matrix of implications things like

259
00:17:10,900 --> 00:17:15,400
amount of aggregate my concrete amount of sand in my concrete say

260
00:17:15,400 --> 00:17:18,610
each of the columns will be a feature there

261
00:17:18,620 --> 00:17:21,110
the amount of standing

262
00:17:21,430 --> 00:17:27,250
concrete type one man standing concrete type two and so on

263
00:17:27,260 --> 00:17:29,470
and then i have a second attribute

264
00:17:29,470 --> 00:17:31,280
and i'll continue so i have

265
00:17:32,100 --> 00:17:33,860
matrix the inputs

266
00:17:33,870 --> 00:17:36,000
and then i have the vector of

267
00:17:37,040 --> 00:17:38,080
which could be

268
00:17:38,110 --> 00:17:43,370
real numbers for concrete strength or plus one to minus one for classification problems

269
00:17:46,680 --> 00:17:49,530
and then we need to build some sort of

270
00:17:49,570 --> 00:17:53,620
description of this data set which i can use in order to

271
00:17:53,640 --> 00:17:55,590
do useful things in the future

272
00:17:55,650 --> 00:17:58,090
so one of the

273
00:17:58,110 --> 00:18:04,230
ways of describing this dataset was using the parametric model and one of the parametric

274
00:18:04,230 --> 00:18:08,040
models with logistic regression which was the model for when we have plus one to

275
00:18:08,040 --> 00:18:09,120
minus one

276
00:18:09,170 --> 00:18:12,970
so the logistic regression model said i think that

277
00:18:12,980 --> 00:18:14,730
these labels

278
00:18:14,730 --> 00:18:20,110
are noisy draws from a distribution that depends on the matrix

279
00:18:22,230 --> 00:18:26,340
one of the rows of this matrix which i'm going to call x so this

280
00:18:26,340 --> 00:18:27,590
road here

281
00:18:27,590 --> 00:18:29,760
it is located vectors being

282
00:18:29,810 --> 00:18:34,090
i'm going to assume that the label y

283
00:18:35,040 --> 00:18:36,370
y one

284
00:18:36,390 --> 00:18:37,790
came from

285
00:18:39,220 --> 00:18:41,480
which was equal to

286
00:18:41,510 --> 00:18:43,040
something i wrote down

287
00:18:43,060 --> 00:18:45,010
and it depended on

288
00:18:45,080 --> 00:18:47,370
the weight vector interacting

289
00:18:48,060 --> 00:18:51,480
this in fact right

290
00:18:51,500 --> 00:18:53,390
such that

291
00:18:53,430 --> 00:18:56,640
if the weight is strongly aligned with the input direction

292
00:18:56,660 --> 00:19:00,680
so that really likes it says very high probability that you're going to be a

293
00:19:00,680 --> 00:19:01,890
plus one

294
00:19:02,760 --> 00:19:06,630
w and x are orthogonal at right angles to each other and this is zero

295
00:19:06,630 --> 00:19:10,240
and it's going to be half probability this it's zero or

296
00:19:10,420 --> 00:19:12,360
but it's the one or minus one

297
00:19:12,410 --> 00:19:16,420
and similarly if x and up to point in opposite directions and there's a high

298
00:19:16,420 --> 00:19:18,490
probability will get minus one

299
00:19:19,140 --> 00:19:20,550
so then what we do is

300
00:19:20,570 --> 00:19:23,360
we come up with an objective for

301
00:19:23,390 --> 00:19:28,790
fitting w such that the the probability of seeing this particular data is high

302
00:19:28,850 --> 00:19:32,460
so that was the basic setup logistic regression

303
00:19:32,510 --> 00:19:36,740
and it turned out that we could get this simple description of the data and

304
00:19:36,740 --> 00:19:39,290
make it into a much more powerful method

305
00:19:39,320 --> 00:19:43,540
with this trick of adding extra columns to this matrix

306
00:19:45,240 --> 00:19:51,460
only five you miss as i spoke quickly exactly where this extra columns came from

307
00:19:51,510 --> 00:19:56,360
and maybe the reason you missed it is it's kind of arbitrary so

308
00:19:56,360 --> 00:20:00,190
this is a column of numbers that i actually am i was actually given

309
00:20:00,200 --> 00:20:04,200
from my problem you know this is the amount of sand or something like that

310
00:20:04,210 --> 00:20:07,820
and at some point the real columns well and and it turned out to be

311
00:20:07,820 --> 00:20:12,820
useful to add a column of ones because then it meant that the decision plane

312
00:20:12,820 --> 00:20:16,820
could not go through the origin w transpose x will always be zero if x

313
00:20:16,820 --> 00:20:19,450
is also sorry if w is all

314
00:20:19,540 --> 00:20:21,050
there is a

315
00:20:21,480 --> 00:20:23,790
the effect all there then

316
00:20:23,790 --> 00:20:25,460
this will always be

317
00:20:25,800 --> 00:20:27,820
there are no matter what WS

318
00:20:27,830 --> 00:20:31,760
so adding this column of one stocks

319
00:20:31,800 --> 00:20:34,980
then we add extra columns

320
00:20:35,040 --> 00:20:38,320
using any rule we like so an example is we could get

321
00:20:38,380 --> 00:20:39,790
these numbers in square

322
00:20:39,790 --> 00:20:42,260
so this will be twenty five point something

323
00:20:42,700 --> 00:20:46,540
and this will be twenty five points more and

324
00:20:46,600 --> 00:20:50,770
we just take this column we square every element and create another whole column we

325
00:20:50,770 --> 00:20:51,700
could also

326
00:20:51,710 --> 00:20:55,040
take the first two columns and multiply them together and that could be another column

327
00:20:55,040 --> 00:20:58,360
we could take it to the calls of this column make

328
00:20:58,390 --> 00:21:02,710
make that's another column any non function you want to cook up in theory you

329
00:21:02,710 --> 00:21:04,650
could do that and add an extra column

330
00:21:04,660 --> 00:21:09,390
then you just the the same thing pretending that

331
00:21:09,500 --> 00:21:10,630
the right

332
00:21:10,640 --> 00:21:16,810
x was actually longer than what the application person gave has all these extra things

333
00:21:16,850 --> 00:21:18,960
why do we do that well

334
00:21:18,990 --> 00:21:19,760
in the

335
00:21:19,760 --> 00:21:23,370
if i only had two features to begin with we could plot some problem in

336
00:21:23,380 --> 00:21:27,030
two d so maybe all of the two classes

337
00:21:27,060 --> 00:21:31,460
and the negative class is

338
00:21:31,560 --> 00:21:34,760
location since we've got the two d input space

339
00:21:34,770 --> 00:21:36,950
of x one

340
00:21:38,060 --> 00:21:39,680
it might be impossible

341
00:21:40,460 --> 00:21:45,820
find a plane that separates two classes describe the data center

342
00:21:48,440 --> 00:21:52,600
so this logistic regression model is limited to fitting planes because this w vectors just

343
00:21:52,600 --> 00:21:54,220
described plane

344
00:21:54,300 --> 00:21:56,840
so one thing we can do is go away

345
00:21:56,890 --> 00:22:01,450
and build a more complicated model that describes more than planes and that's what neural

346
00:22:01,450 --> 00:22:05,240
networks and a lot of models that are well worth knowing about do

347
00:22:05,280 --> 00:22:06,950
a simple thing

348
00:22:06,960 --> 00:22:10,690
which isn't quite as powerful that can get you along way

349
00:22:12,550 --> 00:22:14,880
do this trick by adding extra column

350
00:22:14,930 --> 00:22:16,470
and now

351
00:22:16,490 --> 00:22:17,580
this algorithm

352
00:22:17,640 --> 00:22:21,820
still can just be supplying but if it's the plane this very high dimensional space

353
00:22:21,820 --> 00:22:26,710
rather than two dimensions it's in many dimensions depending on how many columns with added

354
00:22:27,250 --> 00:22:29,000
and if we look at

355
00:22:29,010 --> 00:22:32,740
what that plane is doing in the original three dimensional space

356
00:22:32,760 --> 00:22:35,320
it can be a curve so we can end up with the decision boundary that

357
00:22:35,320 --> 00:22:39,890
actually separates classes so we take a linear methods to make it nonlinear by adding

358
00:22:39,890 --> 00:22:41,780
these extra column

359
00:22:41,810 --> 00:22:43,890
OK this is a simple version

360
00:22:45,310 --> 00:22:47,180
the kernel trick which

361
00:22:48,200 --> 00:22:52,430
something you can hear a lot more about in future lectures so those who heard

362
00:22:52,430 --> 00:22:57,120
about the kernel trick or support vector machines which often use the kernel trick that

363
00:22:57,120 --> 00:23:00,080
something slightly fancier than i'm describing here but it

364
00:23:00,130 --> 00:23:03,350
based on this idea

365
00:23:06,500 --> 00:23:08,030
that's sort

366
00:23:08,060 --> 00:23:12,350
overview of all types of models i described and then we had a question

367
00:23:12,400 --> 00:23:16,680
what we probably have many questions that one of them is well how many extra

368
00:23:16,680 --> 00:23:19,760
column so i had when i add extra columns i can get

369
00:23:19,820 --> 00:23:24,450
funky really weekly decision boundaries which is more powerful but maybe

370
00:23:24,460 --> 00:23:29,090
it's not justified by my data maybe i don't have enough data to fit arbitrarily

371
00:23:29,090 --> 00:23:30,580
complicated models

372
00:23:30,680 --> 00:23:35,000
so maybe i want to limit the number of columns

373
00:23:35,010 --> 00:23:37,220
it turns out that

374
00:23:37,300 --> 00:23:43,500
big flexible models with many parameters often work well in applications fact reflecting the fact

375
00:23:43,500 --> 00:23:45,710
that the real world is complicated

376
00:23:45,720 --> 00:23:49,520
you just have to be careful using them so you have to be careful to

377
00:23:49,530 --> 00:23:53,400
stop your fitting process from fitting

378
00:23:53,410 --> 00:23:55,390
massively overconfident

379
00:23:55,400 --> 00:24:00,370
weekly decision boundaries that when do very well when you deploy your system

380
00:24:00,450 --> 00:24:03,370
so that was when i talked about regularisation

381
00:24:03,410 --> 00:24:07,530
the idea of regularisation is two at the beginning before you look at the data

382
00:24:08,220 --> 00:24:09,280
you know

383
00:24:09,300 --> 00:24:14,060
large weight vectors have lots of very big values probably correspond to with the solutions

384
00:24:14,060 --> 00:24:20,530
essentially commoditized a lot of the stack for operating large scale distributed systems

385
00:24:20,540 --> 00:24:27,040
things for managing clusters the kind of techniques that google has pioneered published like mapreduce

386
00:24:27,040 --> 00:24:31,460
and bigtable are now being heavily developed in the open source community

387
00:24:31,540 --> 00:24:34,480
a kind of small

388
00:24:34,490 --> 00:24:37,510
interesting anecdote for me was that i was we

389
00:24:37,530 --> 00:24:42,680
we were approached by people have any chance of making a very large korean search

390
00:24:42,680 --> 00:24:45,770
engine and it turned out that we were already

391
00:24:45,810 --> 00:24:48,160
we approach because we were already collaborating

392
00:24:48,210 --> 00:24:51,430
powerset has made a version of

393
00:24:52,410 --> 00:24:56,040
our own open-source version that we're given out and are shown the open source community

394
00:24:56,410 --> 00:25:03,480
for enabling very large scale distributed storage and manipulation tables and those folks at naver

395
00:25:03,490 --> 00:25:09,070
nhn we're already working with us so without even anything at the business level are

396
00:25:09,090 --> 00:25:14,000
engineers and developers are working together and by the way there are also applying this

397
00:25:14,260 --> 00:25:17,030
in conjunction with university two

398
00:25:17,070 --> 00:25:20,490
deal with RDF storage on top of the table

399
00:25:20,580 --> 00:25:23,260
there are a couple of our age basically large scales

400
00:25:23,280 --> 00:25:28,290
another major trend is a commodity computing you no longer have to have all your

401
00:25:28,330 --> 00:25:33,920
own computers they're now increasingly sets of players providing services where you can essentially pay

402
00:25:33,940 --> 00:25:36,630
for the computing resources as you go

403
00:25:36,640 --> 00:25:44,090
amazon has been a pioneer here with their elastic computing cloud and their simple storage

404
00:25:44,090 --> 00:25:46,370
system and tuple stores

405
00:25:46,570 --> 00:25:53,320
so you can basically get large-scale access to computing resources without having yourself

406
00:25:53,330 --> 00:25:57,940
OK now known into some examples of power how it's actually working

407
00:25:58,000 --> 00:26:02,210
so i powerset reads each sentence one of the time

408
00:26:02,230 --> 00:26:05,900
so for example sir edward heath died from pneumonia

409
00:26:05,910 --> 00:26:09,780
powerset passes sentence on the page so in this case

410
00:26:09,950 --> 00:26:11,860
there's the verb to die

411
00:26:11,870 --> 00:26:15,160
so edward heath is pneumonia is now

412
00:26:15,200 --> 00:26:20,650
extract entities and semantic relationships sir edward heath is not just about the name

413
00:26:20,660 --> 00:26:23,390
and pneumonia

414
00:26:23,460 --> 00:26:27,910
OK the relationships here are sir edward heath is the subject of the dying

415
00:26:27,920 --> 00:26:34,070
and pneumonia is the the from or the cause of the time

416
00:26:34,130 --> 00:26:39,110
and then using some level of inference powers identifies expands these things to similar entities

417
00:26:39,110 --> 00:26:42,280
relationships and abstractions

418
00:26:42,290 --> 00:26:46,250
so far he does but he is also UK prime minister

419
00:26:46,280 --> 00:26:48,300
and as a politician

420
00:26:48,320 --> 00:26:50,780
pneumonia is also a disease

421
00:26:50,880 --> 00:26:57,410
and to derive from something is also to be killed by it

422
00:26:57,440 --> 00:27:02,710
so for each sentence this kind of information extracted and then a lot facts

423
00:27:02,780 --> 00:27:08,280
are indexed so that our are index actually already then derives benefits from the results

424
00:27:08,280 --> 00:27:11,070
of these kinds of inferences

425
00:27:11,130 --> 00:27:14,250
thereafter multiple queries can retrieve the same facts

426
00:27:15,190 --> 00:27:21,330
what killed sir edward heath matches the sentence about or he died from pneumonia

427
00:27:21,340 --> 00:27:26,510
and so does which prime minister died from pneumonia

428
00:27:26,520 --> 00:27:29,320
and from what it said ahead were he to die and

429
00:27:29,450 --> 00:27:33,950
so on

430
00:27:34,010 --> 00:27:37,780
ultimately something like which politician died from pneumonia

431
00:27:37,880 --> 00:27:42,400
or politicians who died from disease these are all capable of matching the same the

432
00:27:42,400 --> 00:27:46,230
sentences and at that point there need be no connection between the actual words in

433
00:27:46,230 --> 00:27:51,160
the sentence the words in the query but at a conceptual level the relationships are

434
00:27:58,780 --> 00:28:03,910
OK i'm go show humor like really light screenshots moment but

435
00:28:03,960 --> 00:28:05,410
in addition to

436
00:28:05,420 --> 00:28:11,160
extracting information from the tax and doing a text search powers that also integrates diverse

437
00:28:11,160 --> 00:28:13,140
resources so

438
00:28:13,260 --> 00:28:16,860
you can see that here

439
00:28:16,870 --> 00:28:19,820
in the middle of this page the query is what is steve jobs say about

440
00:28:19,820 --> 00:28:20,780
the ipod

441
00:28:20,870 --> 00:28:25,130
and mill this page a bunch of search results highlighting things like steve jobs states

442
00:28:25,130 --> 00:28:28,200
that everyone listen to music and the ipods for everybody

443
00:28:28,210 --> 00:28:32,680
and steve jobs argued that the ipod nano was necessary risk and so on

444
00:28:32,780 --> 00:28:39,620
so those are web search results clubs

445
00:28:39,640 --> 00:28:42,280
as web search results but in addition to

446
00:28:42,840 --> 00:28:47,710
powerset can go out and access the resources and pulled together to decompose view

447
00:28:47,730 --> 00:28:52,040
so from resource like freebase and i'll talk about freebase shortly

448
00:28:52,190 --> 00:28:56,540
powerset gets bioinformation photos steve jobs

449
00:28:56,620 --> 00:29:00,860
and then from a video search engine labelings powerset is able to formulate queries out

450
00:29:00,860 --> 00:29:03,860
and then pull back the information from the video search

451
00:29:03,980 --> 00:29:09,370
and this is unlike normal keyword search which we which only pulls back information for

452
00:29:09,370 --> 00:29:13,070
the keywords you entered when you actually have a natural language system you can pull

453
00:29:13,070 --> 00:29:18,180
back information for the answers or other significant concepts that are derived from the query

454
00:29:18,180 --> 00:29:23,700
the ventured as related to from the text and other sources

455
00:29:23,760 --> 00:29:29,790
OK and in addition you can do real-time queries to databases so in this example

456
00:29:29,810 --> 00:29:33,630
who won an academy award in two thousand one you know one of the answers

457
00:29:33,640 --> 00:29:34,700
halle berry

458
00:29:34,710 --> 00:29:39,290
and this is this is converted into a database query thrown at database in this

459
00:29:39,290 --> 00:29:43,740
case that freebase and then because the system comes back with this kind of result

460
00:29:43,740 --> 00:29:51,180
and also creates a presentation page strong other information structured database

461
00:29:53,500 --> 00:29:57,150
i think i'll actually jump into jump into some of the demos

462
00:29:57,200 --> 00:29:59,730
right now

463
00:30:00,450 --> 00:30:05,380
can you i hope you can all see that this that this resolution

464
00:30:05,430 --> 00:30:07,650
OK in the back there

465
00:30:07,670 --> 00:30:09,060
so so

466
00:30:09,080 --> 00:30:14,080
i will also try and talk to it so here's the query politicians were killed

467
00:30:14,080 --> 00:30:16,820
by disease

468
00:30:18,970 --> 00:30:22,190
you know one of the results that comes up here is dengel paying died on

469
00:30:22,190 --> 00:30:24,380
february nineteen nineteen ninety seven

470
00:30:24,450 --> 00:30:25,950
at age ninety two

471
00:30:25,960 --> 00:30:31,130
from lung infection and parkinson's disease and disease but his influence continued

472
00:30:31,150 --> 00:30:37,390
another result lower down

473
00:30:37,520 --> 00:30:38,770
it is and what he

474
00:30:38,780 --> 00:30:43,370
sir edward heath died from pneumonia at nineteen thirty on the evening of seventeen july

475
00:30:43,370 --> 00:30:44,540
two thousand five

476
00:30:44,570 --> 00:30:46,100
at the age of eighty nine

477
00:30:46,110 --> 00:30:50,530
so those are two results and out this edward heath won this is one examples

478
00:30:50,530 --> 00:30:51,700
i was using before

479
00:30:51,780 --> 00:30:54,780
there's no word in common with the query

480
00:30:54,790 --> 00:30:58,340
and the and the texas returned

481
00:30:58,350 --> 00:31:06,020
so now let's go look a little deeper at how this is actually happening

482
00:31:06,050 --> 00:31:08,770
OK here's a view the parsing process

483
00:31:08,790 --> 00:31:13,210
for that sentence then jumping died on february nineteen nineteen ninety seven aged ninety two

484
00:31:13,210 --> 00:31:18,860
from lung infections and parkinson's disease but his influence continued

485
00:31:18,880 --> 00:31:20,260
so here we see

486
00:31:20,260 --> 00:31:25,340
and these this particular terms class subclass and so on have some extra meaning additional

487
00:31:25,340 --> 00:31:30,390
semantics which ensures that they behave in the way that we expect them to behave

488
00:31:32,260 --> 00:31:35,370
so for example RDF schema we have things like

489
00:31:35,600 --> 00:31:40,260
class property types of cells and so on and so i can then begin to

490
00:31:40,260 --> 00:31:46,000
use some of this vocabulary to represent the triples that were examples but had in

491
00:31:46,020 --> 00:31:51,700
this graph earlier on so i can talk about persons being kind of class i

492
00:31:51,730 --> 00:31:56,420
talk about hascolleague being a property professor being a subclass of person and so on

493
00:31:56,470 --> 00:32:00,220
so i can begin to build up some terms that describe my domain and the

494
00:32:00,220 --> 00:32:05,220
relationship between those terms

495
00:32:05,220 --> 00:32:07,330
just skip over this

496
00:32:07,520 --> 00:32:10,910
OK so so what is this what is this then begin to give us and

497
00:32:10,910 --> 00:32:13,960
this is at this point we can begin to get into this notion of inference

498
00:32:13,970 --> 00:32:15,110
and this is where the

499
00:32:15,120 --> 00:32:17,480
the power this begins to detect

500
00:32:17,480 --> 00:32:21,350
OK so here i have an RDF graph and i know that i have this

501
00:32:21,840 --> 00:32:26,020
class of lecturers have a class of academics and i have a class of persons

502
00:32:26,100 --> 00:32:28,200
and i know that all these things are classes

503
00:32:28,210 --> 00:32:32,250
and know there some subclass relationship between lecture academic person

504
00:32:32,260 --> 00:32:35,760
OK so that's the thing i can now do because of the fact that i

505
00:32:35,760 --> 00:32:41,470
know that the because of the underlying semantics interpretation i'm giving the subclass property

506
00:32:41,870 --> 00:32:43,540
it's like you know for

507
00:32:43,560 --> 00:32:46,210
the lecturer must be a subclass of person

508
00:32:46,250 --> 00:32:47,610
OK now this is

509
00:32:47,630 --> 00:32:50,980
on the face of it this is a rather simple inference to draw

510
00:32:50,980 --> 00:32:54,220
OK this is a one level one if it one could say on this is

511
00:32:54,220 --> 00:32:59,230
this is obvious OK but the reason is obvious because we have some

512
00:32:59,250 --> 00:33:03,500
interpretation of these terms and this is the way that we know that subclass behaves

513
00:33:03,530 --> 00:33:07,180
what i have to ensure that have to ensure that within the the sort the

514
00:33:07,180 --> 00:33:12,660
semantics my language this is how subclasses interpreted so i do actually have this transitivity

515
00:33:12,850 --> 00:33:18,480
transitive relationship and i know that if lecture subclass of academic and academic suppressive person

516
00:33:18,850 --> 00:33:21,010
that i have this inference being drawn

517
00:33:21,020 --> 00:33:25,740
OK so it's important i have this enshrined in the in the semantic my language

518
00:33:25,750 --> 00:33:29,120
so the language behave in a particular way

519
00:33:29,140 --> 00:33:34,630
similarly i know that there some interaction between the type relationship the subclass relationship so

520
00:33:34,630 --> 00:33:39,750
sean lecturer and lecturer is a subclass of academics i can make an inference that

521
00:33:40,540 --> 00:33:41,340
must be

522
00:33:41,360 --> 00:33:45,590
an instance of academic and again this is something that the but we can can

523
00:33:45,590 --> 00:33:49,430
clearly see if we look at this it's obvious to ask OK but we have

524
00:33:49,430 --> 00:33:51,920
to make sure that the the the

525
00:33:51,970 --> 00:33:56,820
semantics of the language semantics of the representation of such that these kind of side

526
00:33:56,820 --> 00:34:01,660
effects happen we can draw inferences

527
00:34:01,680 --> 00:34:06,070
so what RDF schema than this is

528
00:34:06,080 --> 00:34:09,590
pretty much what we have solve the case we have this kind of

529
00:34:09,610 --> 00:34:13,300
to model we have the ability to use your eyes can share we can integrate

530
00:34:14,050 --> 00:34:16,130
but we know that we get

531
00:34:16,160 --> 00:34:20,590
some kind of consistent vocabulary used because i know the game being able to define

532
00:34:20,600 --> 00:34:24,780
a kind of simple terminology some simple vocabulary is described by domain

533
00:34:24,780 --> 00:34:27,970
i consistently uses market share

534
00:34:27,980 --> 00:34:31,470
and i began to get some simple inference which allows me to then cannot draw

535
00:34:31,470 --> 00:34:36,140
some inferences about the integrated data

536
00:34:36,220 --> 00:34:41,550
however the schema is still still suffers from some problems

537
00:34:41,560 --> 00:34:42,530
OK so

538
00:34:42,620 --> 00:34:45,520
the language that we have an idea scheme is very weak

539
00:34:45,520 --> 00:34:49,500
OK it doesn't allow me to describe resources in sufficient detail i really wanted to

540
00:34:49,500 --> 00:34:51,770
characterize my domain in some way

541
00:34:51,790 --> 00:34:55,860
OK RDF schema doesn't allow me to do this so there are no what we

542
00:34:55,860 --> 00:35:00,920
might call localised range and domain constraints case if i want to say that

543
00:35:00,950 --> 00:35:03,410
i have published by relationship

544
00:35:03,510 --> 00:35:06,750
OK so i talk about papers being published by a publisher

545
00:35:06,850 --> 00:35:08,040
i can't say

546
00:35:09,460 --> 00:35:14,280
if it a journal that that OK when when i

547
00:35:14,290 --> 00:35:16,780
the journal is published by a publisher

548
00:35:16,860 --> 00:35:20,320
OK and the technical report was published by institution

549
00:35:20,380 --> 00:35:24,040
these are facts these are kind of relationship i would perhaps want to represent my

550
00:35:24,040 --> 00:35:28,350
domain constraints on the way in which published by can can be used i can

551
00:35:28,370 --> 00:35:32,910
represent this in RDF schema there's no language that there's nothing in the representation of

552
00:35:32,910 --> 00:35:34,980
allow me to express

553
00:35:34,980 --> 00:35:36,870
there's a relationship

554
00:35:36,940 --> 00:35:41,150
so in this this talk about discrete random variables just for simplicity is alright some

555
00:35:41,310 --> 00:35:45,310
the integrals but you can do this with was well see find the f divergence

556
00:35:45,310 --> 00:35:47,690
between two measures mu and pi

557
00:35:47,750 --> 00:35:51,370
as f of the likelihood ratio

558
00:35:51,370 --> 00:35:55,020
and the integrate or you average with respect to the pie

559
00:35:55,060 --> 00:35:58,810
OK so that's how i killed divergence in fact if f is chosen to be

560
00:35:59,230 --> 00:36:00,500
u log u

561
00:36:00,500 --> 00:36:02,350
you get a little versions

562
00:36:02,370 --> 00:36:05,980
right but if f is chosen to be absolute value minus one for example

563
00:36:05,980 --> 00:36:11,640
then you get out the variational distance which is here just the l one

564
00:36:11,640 --> 00:36:13,850
distance on measures

565
00:36:13,870 --> 00:36:19,560
after the any continuous convex function so these are particular examples here's another one if

566
00:36:19,560 --> 00:36:22,830
you like that one and you get out the hellinger distance and this goes on

567
00:36:22,830 --> 00:36:26,100
for several pages so you plug in a continuous convex

568
00:36:26,150 --> 00:36:30,920
when you look at you now define a new ali silly or after versions

569
00:36:30,940 --> 00:36:36,170
right so why these guys use the f divergences well were somehow intuitively appealing but

570
00:36:36,170 --> 00:36:39,830
there is a little bit of kind of underlying theory behind that choice

571
00:36:40,020 --> 00:36:45,500
it's not entirely satisfactory that's it's not necessary at all but it is a good

572
00:36:45,500 --> 00:36:51,810
starting place and the theorem was due to david blackwell nineteen fifty one a classical

573
00:36:51,810 --> 00:36:57,040
paper well well worth reading had a big impact impact on economics

574
00:36:57,100 --> 00:37:02,480
so if there is to the following procedure a some kind of an estimator has

575
00:37:02,480 --> 00:37:05,020
a smaller f divergence the procedure b

576
00:37:05,040 --> 00:37:08,810
for some particular choice of f versions

577
00:37:08,830 --> 00:37:13,730
OK then there exist some set of prior probabilities the class probabilities here in one

578
00:37:13,730 --> 00:37:18,830
class the other so some set of prior problem classes such that procedure a has

579
00:37:18,830 --> 00:37:21,920
a smaller probability air the procedure being well that's what you care about that's the

580
00:37:21,920 --> 00:37:26,460
risk so we now this procedure is a small risk the procedure be that's good

581
00:37:26,500 --> 00:37:29,210
we should not use proper procedure a

582
00:37:29,270 --> 00:37:33,270
and we were told that by looking after versions is after this gives an average

583
00:37:34,210 --> 00:37:38,670
i would just an existence statement that says there exist some sort of properties we

584
00:37:38,670 --> 00:37:43,370
don't know what the problems are always that after versions give this ranking

585
00:37:43,520 --> 00:37:47,120
we don't know in our particular problem which after three years

586
00:37:47,120 --> 00:37:51,540
OK so it is not that helpful in several helpful practice but it does at

587
00:37:51,540 --> 00:37:55,330
least suggest that after are not unreasonable objects to be looking at if you're trying

588
00:37:55,330 --> 00:37:57,100
to minimize the probability there

589
00:37:57,140 --> 00:38:01,620
and that's a good thing because when i probably areas course nonconvex problem the risk

590
00:38:01,620 --> 00:38:04,140
of zero one loss is not convex

591
00:38:04,350 --> 00:38:07,540
so you try to find some other kind of function you can optimize and these

592
00:38:07,540 --> 00:38:09,270
things are convex

593
00:38:09,290 --> 00:38:12,480
and therefore you might try them by just there

594
00:38:12,480 --> 00:38:17,480
so that's what people did as famous paper skylark nineteen sixty seven and so on

595
00:38:17,520 --> 00:38:22,230
jews in particular divergences and just kind of in some sense hoping that the priors

596
00:38:22,250 --> 00:38:25,650
our right for that after the particular problem

597
00:38:25,670 --> 00:38:31,140
another some supporting arguments for asymptotic show this divergence also arise in other ways back

598
00:38:31,150 --> 00:38:36,920
the original kullback leibler divergence arose by analysis of hypothesis testing it still carries the

599
00:38:36,920 --> 00:38:42,370
power function in hypothesis testing we're two class sustain a fixed distance apart as the

600
00:38:42,370 --> 00:38:46,870
number of data points gets large similar chernoff distance arise same analysis in a bayesian

601
00:38:46,870 --> 00:38:51,120
setting we have prior classes these divergence were you know kind of in some ways

602
00:38:51,120 --> 00:38:56,940
talking about probability air directly in this case but it's an asymptotic arguments hypothesis testing

603
00:38:57,020 --> 00:39:00,000
so anyway it's still heuristic literature

604
00:39:00,140 --> 00:39:03,100
announced on the the other side of the coin had to the discriminant function

605
00:39:03,120 --> 00:39:06,620
and so you can now know this stuff this is the machine learning can one

606
00:39:06,670 --> 00:39:11,440
one you choose a loss function that measures the distance between your class label here

607
00:39:12,500 --> 00:39:16,080
you you are we could start with zero one loss is the loss we're trying

608
00:39:16,080 --> 00:39:17,330
to optimize

609
00:39:17,380 --> 00:39:21,750
and in the binary case you can write that is the indicator function of when

610
00:39:21,750 --> 00:39:23,560
the labels disagree

611
00:39:23,560 --> 00:39:29,430
OK the labels disagree using why is is one minus one the discriminant function outputs

612
00:39:29,430 --> 00:39:33,380
also one minus one are real or real number general so they disagree in their

613
00:39:33,380 --> 00:39:38,420
sign that's bad and that's you pay loss of one in that case the logic

614
00:39:40,080 --> 00:39:46,120
OK so the main focuses on the discriminant function and now we know also from

615
00:39:46,120 --> 00:39:49,960
this point of view it is intractable unless you one last this argument as well

616
00:39:49,980 --> 00:39:53,440
so instead what people have done if it picked the surrogate loss function which are

617
00:39:53,440 --> 00:39:57,150
convex upper bounds and zero one loss so we've all seen this picture

618
00:39:57,150 --> 00:39:58,370
here's the

619
00:39:58,380 --> 00:40:02,000
zero one loss expressed in terms of this margin value i the product of the

620
00:40:02,000 --> 00:40:04,790
y and gamma c

621
00:40:04,790 --> 00:40:07,580
and if you disagree on this side you pay loss of one of the ways

622
00:40:07,580 --> 00:40:09,000
you pay loss is zero

623
00:40:09,020 --> 00:40:13,880
and so is intractable optimise this instead people look at these are upper bounds and

624
00:40:13,880 --> 00:40:18,960
the blue line is the more it is the support vector machine the hinge loss

625
00:40:18,960 --> 00:40:23,540
the the red one here's logistic the green one i think is the boosting of

626
00:40:23,580 --> 00:40:28,540
the exponential loss which is what are what you should boosting

627
00:40:28,720 --> 00:40:33,640
and there's a whole bunch of others there that this page is littered with examples

628
00:40:34,060 --> 00:40:38,600
OK so all the references are different from you know optimisation point of view this

629
00:40:38,620 --> 00:40:42,250
write down these most of these particular choice of curves

630
00:40:42,250 --> 00:40:43,820
for each of the features

631
00:40:43,840 --> 00:40:45,650
and on the other extreme got the

632
00:40:45,690 --> 00:40:47,470
fifteen with minutes

633
00:40:47,490 --> 00:40:49,120
which converges faster

634
00:40:49,130 --> 00:40:50,550
simply because it's got

635
00:40:50,590 --> 00:40:51,870
the fewer number of features

636
00:40:52,500 --> 00:40:57,650
filter log histories

637
00:40:57,670 --> 00:40:59,340
OK so what we do here

638
00:40:59,400 --> 00:41:03,350
we use nine the data training held back some set testing in each time we

639
00:41:05,010 --> 00:41:06,430
one hundred trees

640
00:41:06,450 --> 00:41:09,710
the forests averaged over one hundred trials these are the

641
00:41:09,760 --> 00:41:12,830
test error rates using standard random forest

642
00:41:13,920 --> 00:41:18,060
confidence interval parallel technique also using a two stage

643
00:41:18,120 --> 00:41:22,090
cart rebuild a single country to begin with

644
00:41:22,110 --> 00:41:27,280
we don't currently billed as large as possible and because each stage is optimizing over

645
00:41:27,360 --> 00:41:32,460
features you get an optimal information gain value for each feature and explanatory so build

646
00:41:32,460 --> 00:41:33,420
a single tree

647
00:41:33,440 --> 00:41:35,460
to the average information gain for that

648
00:41:35,520 --> 00:41:37,170
and then use that to fix

649
00:41:37,210 --> 00:41:41,510
the feature set happiest nation and then build forced from their

650
00:41:41,900 --> 00:41:45,400
it is because it does form

651
00:41:45,450 --> 00:41:48,470
actually very well in this case there is no guarantee

652
00:41:48,520 --> 00:41:51,210
on the reliability of this method

653
00:41:51,250 --> 00:41:55,420
and a composite math because is actually

654
00:41:55,430 --> 00:41:59,280
particularly for the two artificial data

655
00:41:59,890 --> 00:42:01,800
significantly improving performance

656
00:42:01,850 --> 00:42:07,780
and quite importantly never significantly worse so this problem of initially overweighting features

657
00:42:07,830 --> 00:42:10,640
has been avoided in this case

658
00:42:10,650 --> 00:42:16,140
OK so there irrelevant features

659
00:42:16,160 --> 00:42:19,810
we said that the average information gain is the name of the non negative samples

660
00:42:19,860 --> 00:42:20,990
so we get

661
00:42:21,130 --> 00:42:25,670
expected information gain irrelevant feature is going to have some non-zero value

662
00:42:25,680 --> 00:42:28,520
now if you apply this to the sampling distribution

663
00:42:28,550 --> 00:42:31,170
you have a high proportion of irrelevant features

664
00:42:31,180 --> 00:42:33,360
although you may be giving them small

665
00:42:33,370 --> 00:42:36,350
the probability of being sampled when you actually

666
00:42:36,400 --> 00:42:37,790
it combines

667
00:42:39,210 --> 00:42:41,750
the total probability of choosing an irrelevant feature

668
00:42:41,790 --> 00:42:43,330
is actually

669
00:42:43,380 --> 00:42:49,140
still significant and will still degrade performance forms

670
00:42:49,140 --> 00:42:53,650
so what want to try and do is calculated this expected information gain values

671
00:42:53,660 --> 00:42:55,340
these irrelevant features

672
00:42:55,430 --> 00:43:00,400
so we got a parent node to the parent node and

673
00:43:00,450 --> 00:43:03,810
when you put this we could lefts and rights and

674
00:43:04,200 --> 00:43:05,690
so there are no

675
00:43:05,730 --> 00:43:09,900
examples in left descendant iol positive examples and then we know that there are n

676
00:43:09,930 --> 00:43:11,880
examples and the parent node i

677
00:43:11,900 --> 00:43:13,620
positive examples in the pattern

678
00:43:13,630 --> 00:43:15,570
so from that we can work out

679
00:43:15,620 --> 00:43:19,690
the corresponding numbers right center

680
00:43:19,850 --> 00:43:23,400
what we do is for each possible arrangements

681
00:43:23,460 --> 00:43:26,700
when you project the data down to the one dimensional space for each possible arrangement

682
00:43:26,710 --> 00:43:28,880
you can try these difference the values

683
00:43:28,900 --> 00:43:32,110
and so give values for an hour and i l and you want to choose

684
00:43:32,250 --> 00:43:35,360
split which basically minimizes expression

685
00:43:35,450 --> 00:43:40,850
it gives you the values of an outlier which minimizes this expression is the parent

686
00:43:43,140 --> 00:43:47,440
we tried this before

687
00:43:47,440 --> 00:43:49,320
some values

688
00:43:49,400 --> 00:43:54,870
but note composition so here we have three to know because was the number of

689
00:43:54,870 --> 00:43:57,540
negative examples in the number of positive examples

690
00:43:57,550 --> 00:44:00,760
and so if you take diagonal segment

691
00:44:00,800 --> 00:44:02,810
along here we've got to fix size

692
00:44:03,710 --> 00:44:07,790
basically just changing the class proportions with the money

693
00:44:10,210 --> 00:44:13,390
we found the of something in this matters

694
00:44:13,410 --> 00:44:18,530
hugely expensive because you have to build each of the narrator arrangements optimise information gain

695
00:44:18,530 --> 00:44:20,010
the average

696
00:44:20,010 --> 00:44:21,540
overall the

697
00:44:21,620 --> 00:44:24,150
we did find it for fixed value and

698
00:44:24,160 --> 00:44:25,080
the the

699
00:44:25,110 --> 00:44:29,830
highest value expected information gain actually occurs when the class proportions are equal

700
00:44:29,840 --> 00:44:31,090
so down

701
00:44:31,110 --> 00:44:33,020
center line here

702
00:44:33,050 --> 00:44:37,380
actually increasing the size of the node is actually half of each class

703
00:44:37,390 --> 00:44:39,810
and the minimum is occurring

704
00:44:39,820 --> 00:44:41,220
when we have

705
00:44:41,230 --> 00:44:43,840
one example of one class

706
00:44:43,890 --> 00:44:49,960
and all the other examples from the past

707
00:44:49,970 --> 00:44:53,430
OK so if we to just take the value and

708
00:44:53,480 --> 00:44:56,080
so we the notion of size n

709
00:44:56,100 --> 00:44:59,430
because there is a lower bound is given when we have

710
00:44:59,430 --> 00:45:00,260
only one

711
00:45:00,270 --> 00:45:02,000
example of one class

712
00:45:02,050 --> 00:45:03,800
and we can actually

713
00:45:03,810 --> 00:45:08,220
calculate this lower bound on expected information gain irrelevant feature

714
00:45:08,260 --> 00:45:10,280
and we found that we could approximate

715
00:45:10,380 --> 00:45:13,610
the upper bound of the past portion recall

716
00:45:14,160 --> 00:45:16,780
basically by plotting the

717
00:45:16,790 --> 00:45:22,300
the function of a logarithmic scale family approximately straight line

718
00:45:22,320 --> 00:45:25,150
and so these then gives the bounds

719
00:45:25,160 --> 00:45:31,390
on the expected information about for developing features

720
00:45:31,410 --> 00:45:33,130
so it is we just below

721
00:45:33,150 --> 00:45:35,530
one hundred reasoning artificial datasets

722
00:45:36,100 --> 00:45:38,760
the two relevant features features one and two

723
00:45:38,810 --> 00:45:42,380
and the remaining seven are irrelevant

724
00:45:42,430 --> 00:45:44,780
these stars represent the

725
00:45:44,790 --> 00:45:48,030
average information gain is achieved by to the features

726
00:45:48,040 --> 00:45:54,000
and then these bounds are bound on the expected performance of relevant feature

727
00:45:54,050 --> 00:45:58,510
as you can see that there features are within the bounds are expected to be

728
00:46:00,010 --> 00:46:02,310
two relevant pages actually

729
00:46:02,390 --> 00:46:04,270
above these ground

730
00:46:04,270 --> 00:46:11,010
OK so what we did then is just to see

731
00:46:11,010 --> 00:46:14,220
how are these were b shows feature selection threshold

732
00:46:14,270 --> 00:46:15,500
which is basically

733
00:46:15,510 --> 00:46:18,340
the halfway point between these two bounds

734
00:46:18,400 --> 00:46:24,290
we try the three mandates that which is normally regression we threshold at two

735
00:46:25,530 --> 00:46:27,320
a binary classification problem

736
00:46:27,340 --> 00:46:29,150
which was

737
00:46:29,210 --> 00:46:32,530
pretty much equal balance

738
00:46:33,380 --> 00:46:38,860
you start feature selection scheme using our feature selection threshold build one hundred trees

739
00:46:39,790 --> 00:46:42,270
i got the bounds expected

740
00:46:42,290 --> 00:46:43,620
information gain

741
00:46:43,710 --> 00:46:46,720
is the threshold is the halfway point between towns

742
00:46:46,770 --> 00:46:49,070
and then selected features which had

743
00:46:49,090 --> 00:46:52,250
on average information gain about this

744
00:46:52,320 --> 00:46:53,580
and we found that

745
00:46:53,630 --> 00:46:56,330
over one hundred trials

746
00:46:56,340 --> 00:47:00,550
argmaxy puts out the five relevant features all the time

747
00:47:00,590 --> 00:47:03,450
and some of the irrelevant features

748
00:47:03,450 --> 00:47:06,880
sometime now CFS

749
00:47:06,900 --> 00:47:10,390
is an organ competitors correlation based feature selection

750
00:47:10,440 --> 00:47:15,680
which is symmetric uncertainty to examine the correlation between

751
00:47:17,510 --> 00:47:22,000
feature and the class as well as the feature feature correlations actually analyse redundancy in

752
00:47:22,000 --> 00:47:23,590
the data as well

753
00:47:23,640 --> 00:47:26,100
and this our room

754
00:47:26,150 --> 00:47:27,830
doesn't use any of the the

755
00:47:27,910 --> 00:47:29,150
there is

756
00:47:29,200 --> 00:47:31,850
but actually drops

757
00:47:31,870 --> 00:47:35,550
some of the relevant features as well

758
00:47:35,550 --> 00:47:40,150
and it turns out you need about a hundred galaxies to even get its texans

759
00:47:40,190 --> 00:47:42,470
and one thing this

760
00:47:42,570 --> 00:47:45,170
so then yes

761
00:47:45,210 --> 00:47:47,180
the great

762
00:47:47,870 --> 00:47:56,270
yes really really yes OK good question

763
00:47:56,370 --> 00:48:01,490
OK so i showed you very very beautiful picture which was very extreme cases where

764
00:48:01,490 --> 00:48:03,110
you can see that there was

765
00:48:03,130 --> 00:48:07,070
the actual distortion was different different parts of the galaxy

766
00:48:07,330 --> 00:48:11,430
that's a very extreme case i said because i just love that picture and i

767
00:48:11,430 --> 00:48:16,150
think it's very beautiful but it's actually very atypical so only in you know point

768
00:48:16,150 --> 00:48:22,290
o one percent of the sky will you see anything like that not less

769
00:48:22,490 --> 00:48:26,590
so it's not

770
00:48:26,870 --> 00:48:30,350
that's right with you

771
00:48:32,810 --> 00:48:37,610
yes yes that's exactly right so we that's right so we can't help or anything

772
00:48:37,620 --> 00:48:39,770
galaxy whether it's been sheared

773
00:48:39,790 --> 00:48:44,350
what we we we had we the signal that we have measured is where you

774
00:48:45,730 --> 00:48:47,630
more than one galaxy at the time

775
00:48:47,670 --> 00:48:50,130
and for example you're looking for

776
00:48:50,170 --> 00:48:54,870
pairs of galaxies which appear to be pointing on average in the same direction because

777
00:48:54,870 --> 00:49:00,150
galaxies which are near to each other will pass through similar dark matter distribution to

778
00:49:00,160 --> 00:49:04,880
the destruction of similar amount and if we take pairs of galaxies and cross correlate

779
00:49:05,050 --> 00:49:10,770
is the the measured shapes then we find galaxies tend on average to be aligned

780
00:49:10,770 --> 00:49:11,770
with each other

781
00:49:11,790 --> 00:49:15,410
so that's exactly the signal that we're looking for me and so in fact to

782
00:49:15,430 --> 00:49:19,810
to do this you need hundreds of thousands of galaxies to even detect signal at

783
00:49:20,790 --> 00:49:24,490
and we need typically a billion galaxies to do the dark energy measurements we want

784
00:49:24,490 --> 00:49:25,930
to do in the future

785
00:49:26,030 --> 00:49:32,930
good question the questions stage

786
00:49:32,950 --> 00:49:39,130
great so the rest of it is is i'm sure you'll find really trivial so

787
00:49:39,370 --> 00:49:42,270
so we have to you we we have an atmosphere

788
00:49:42,290 --> 00:49:48,030
we often observing through and have the telescope and both of these induce the competition

789
00:49:48,030 --> 00:49:50,150
with with the kernel

790
00:49:50,210 --> 00:49:55,330
so beautiful galaxy with spiral arms becomes this blurry blob

791
00:49:55,410 --> 00:50:00,550
and typically in fact the size of the galaxy is typically about the same size

792
00:50:00,550 --> 00:50:02,450
has become the completion kernel

793
00:50:02,770 --> 00:50:06,670
so it makes it quite difficult because the kernel tends to be elliptical

794
00:50:06,970 --> 00:50:11,930
slightly and that doesn't actually much bigger effect than she were trying to measure we

795
00:50:12,020 --> 00:50:15,340
to remove this kind of very very accurately indeed

796
00:50:15,690 --> 00:50:20,910
pick asian we have detectors which pick up the light and these detectors roughly have

797
00:50:20,910 --> 00:50:27,890
square not overlapping elements so we end up binning the lights into the square pixels

798
00:50:27,930 --> 00:50:33,630
and typically the size of the pixels depends on your definition of size but typically

799
00:50:33,630 --> 00:50:37,170
the pixels are the bigger than shown in this picture here

800
00:50:37,210 --> 00:50:38,950
about twice the size

801
00:50:38,990 --> 00:50:42,130
and then the sleeve got noise

802
00:50:42,230 --> 00:50:47,110
and we've got what's on noise because we're detecting a certain number of photons and

803
00:50:47,110 --> 00:50:54,110
you can also have some noise and bad pixels induce the detectors

804
00:50:54,170 --> 00:50:59,070
and typically we're looking at galaxies which are the knowledge this we've typically going around

805
00:50:59,070 --> 00:51:02,670
the total amount of light about five percent

806
00:51:02,710 --> 00:51:06,490
and we tried to measure the intensity of these galaxies

807
00:51:06,530 --> 00:51:10,430
so this is just sums up the whole that picture

808
00:51:10,470 --> 00:51:15,790
so the key the top line here we've got a galaxy being sheared

809
00:51:15,830 --> 00:51:17,890
that's what you want to measure

810
00:51:17,990 --> 00:51:20,490
we've got this combination with the kernel

811
00:51:20,710 --> 00:51:26,490
we've got the pics isolation noise and typically with real images what we do is

812
00:51:26,490 --> 00:51:32,130
we got stars which are effectively points and so we can use those stars to

813
00:51:32,130 --> 00:51:33,810
measure the

814
00:51:34,130 --> 00:51:38,470
and that's how we managed to do you can vote kernel

815
00:51:38,510 --> 00:51:46,130
OK so the typical image customer the clinic and here's a typical star

816
00:51:46,150 --> 00:51:50,210
that's used to decode evolved to find accommodation kernel and here's a typical galaxy which

817
00:51:50,210 --> 00:51:51,510
you can't really

818
00:51:51,590 --> 00:51:54,570
which is used to measure the shears

819
00:51:54,770 --> 00:51:58,870
and in all this mess up here for example this is very very bright star

820
00:51:58,890 --> 00:52:03,030
these that it's saturated it's it's no longer linear we we haven't measured it properly

821
00:52:03,030 --> 00:52:05,030
it's lots of mess as well

822
00:52:06,770 --> 00:52:11,450
so the great oh eight challenge we have identified a small part of this whole

823
00:52:11,450 --> 00:52:15,430
problem now we are stuck on and which we need your help with

824
00:52:15,450 --> 00:52:18,830
and so we're just looking at

825
00:52:18,850 --> 00:52:22,690
measuring we we're going to give you the kernel accommodation kernel

826
00:52:22,870 --> 00:52:30,170
and we want you to take a few million galaxy images and extract the shear

827
00:52:30,290 --> 00:52:35,910
and typically in cosmology we would then apply some statistic for example correlations of shapes

828
00:52:35,950 --> 00:52:42,250
and compare those with what you get from simulations and measure the doctrine dark energy

829
00:52:42,270 --> 00:52:44,670
but we just going to focus on this box here

830
00:52:45,050 --> 00:52:48,450
within great oh eight

831
00:52:48,470 --> 00:52:56,190
so the data that we we will provide you with is this three different datasets

832
00:52:56,270 --> 00:52:57,530
i certainly

833
00:52:57,650 --> 00:53:02,290
we've got some warm-up data some fairly high signal to noise

834
00:53:02,340 --> 00:53:07,110
no noise data and which one is that we give you the shears

835
00:53:07,130 --> 00:53:09,710
and is that is blind

836
00:53:09,850 --> 00:53:15,310
there's about ten thousand images that and then there's about a hundred thousand images in

837
00:53:15,310 --> 00:53:16,990
the main challenge

838
00:53:17,560 --> 00:53:21,590
scared you don't like large data sets that was a bit frightening in the previous

839
00:53:21,590 --> 00:53:28,670
talk brain that team this to the high enough precision it's necessary to have this

840
00:53:28,670 --> 00:53:30,010
many galaxies

841
00:53:30,090 --> 00:53:34,730
they're all the same format that there's just one galaxy in each in each image

842
00:53:35,670 --> 00:53:39,950
but unfortunately it's necessary to have a huge amount of data they would like to

843
00:53:39,950 --> 00:53:45,570
hear your thoughts on that there's a number of simplifications we've made each image contains

844
00:53:45,570 --> 00:53:46,930
just one galaxy

845
00:53:47,010 --> 00:53:48,590
and the kernel is given

846
00:53:48,610 --> 00:53:52,710
this one shear per typically

847
00:53:52,750 --> 00:53:55,070
and huge background

848
00:53:55,150 --> 00:53:59,710
i did so it's almost guaranteed in fact

849
00:53:59,950 --> 00:54:03,890
so what we would like from

850
00:54:03,910 --> 00:54:07,130
this is what at

851
00:54:07,410 --> 00:54:11,090
and we will then compare that with the truth

852
00:54:11,130 --> 00:54:14,140
we do want a that make a large number of good

853
00:54:14,570 --> 00:54:20,630
and so then we propose to have an active leaderboard showing the results ordered by

854
00:54:20,630 --> 00:54:23,250
this quality factor q

855
00:54:23,290 --> 00:54:26,590
and where we are at the moment in our community is a q of about

856
00:54:27,150 --> 00:54:31,870
about a hundred and this this obviously is not done this exact challenge yet so

857
00:54:31,870 --> 00:54:35,950
we'll see on this exact challenge and then this is what we're aiming for the

858
00:54:35,950 --> 00:54:38,910
future surveys is the a q of about a thousand

859
00:54:39,530 --> 00:54:43,670
so what we were going to do actually is over the next few months we're

860
00:54:44,110 --> 00:54:48,790
running our own methods on the exact simulations which you're we're going to to make

861
00:54:48,790 --> 00:54:53,850
public this challenge and we'll put our own methods on this leaderboard so you can

862
00:54:53,850 --> 00:54:55,070
see where we are

863
00:54:55,370 --> 00:54:59,210
and we also make our code public so if you want to just tweak our

864
00:54:59,210 --> 00:55:03,510
code a little bit then that's that's far but what we actually really hoping you'll

865
00:55:03,510 --> 00:55:07,310
just have a fresh look at this whole thing and come up with some brilliant

866
00:55:07,310 --> 00:55:09,470
new method

867
00:55:09,490 --> 00:55:14,190
and after the discussion earlier i think it would be great to have some kind

868
00:55:14,190 --> 00:55:16,220
and you might expect

869
00:55:16,270 --> 00:55:19,090
it that they would come to speak a pidgin

870
00:55:19,110 --> 00:55:24,560
but they don't what happens is in the course of a single generation they developed

871
00:55:24,560 --> 00:55:26,160
their own language

872
00:55:26,230 --> 00:55:31,580
they create a language with rich syntax and morphology more in phenomenology

873
00:55:31,600 --> 00:55:34,060
terms that will understand a few minutes

874
00:55:34,070 --> 00:55:38,670
and this language they create is called the trail

875
00:55:38,690 --> 00:55:44,170
and languages that we know now as creole the word refers back to the history

876
00:55:44,210 --> 00:55:47,640
that means that they were developed from pigeons

877
00:55:47,650 --> 00:55:53,380
and this is interesting because it suggested to some extent the ability to use and

878
00:55:53,380 --> 00:55:57,840
understand and learn language is part of human nature it doesn't require an extensive cultural

879
00:55:57,840 --> 00:56:01,260
history rather just about any normal child

880
00:56:01,280 --> 00:56:05,650
even when not exposed to full-fledged language can create language

881
00:56:05,690 --> 00:56:12,150
and more recently has been case studies of children who acquire sign language

882
00:56:12,200 --> 00:56:19,240
there's a wonderful case nicaraguan sign language where they acquire sign language from adults who

883
00:56:19,240 --> 00:56:25,440
themselves are not reversed in sign language disorders second language learners struggling along

884
00:56:25,470 --> 00:56:29,950
what you might have expected to be the children with use whatever system adults use

885
00:56:29,980 --> 00:56:32,360
but they don't take realise

886
00:56:32,460 --> 00:56:37,430
they take this makeshift communication system developed by health and again they turn it into

887
00:56:37,430 --> 00:56:39,480
a full-blown language

888
00:56:39,530 --> 00:56:46,150
suggesting that to some extent is part of our human nature to create languages

889
00:56:46,200 --> 00:56:47,630
i also

890
00:56:47,680 --> 00:56:50,180
every normal human

891
00:56:50,200 --> 00:56:54,550
has language not everybody in this room can ride a bicycle

892
00:56:54,600 --> 00:56:57,540
not everybody in this room can play chess

893
00:56:57,560 --> 00:56:59,470
but everybody

894
00:56:59,520 --> 00:57:05,290
possesses at least one language and everybody started to possess at least one language when

895
00:57:05,290 --> 00:57:07,070
they were child

896
00:57:07,080 --> 00:57:08,720
there are exceptions

897
00:57:08,810 --> 00:57:12,810
but exceptions come about due to some sort of brain damage

898
00:57:12,830 --> 00:57:19,200
any neurologically normal human will come to possess a language

899
00:57:19,240 --> 00:57:21,570
what else do we know well

900
00:57:21,590 --> 00:57:24,860
the claim that language is part of human nature

901
00:57:25,170 --> 00:57:29,880
supported by neurological studies some which were referred to in chapter in the brain you

902
00:57:29,880 --> 00:57:34,380
read earlier that talk about dedicated parts of the brain

903
00:57:34,820 --> 00:57:36,770
that work for language

904
00:57:36,780 --> 00:57:41,670
and parts of these brains parts of these parts of the brain damage you get

905
00:57:41,670 --> 00:57:44,780
language deficits or or features

906
00:57:44,800 --> 00:57:48,970
where you might lose the ability to understand or create language

907
00:57:52,810 --> 00:57:54,870
there's been some fairly recent works

908
00:57:54,890 --> 00:58:01,830
studying the genetic basis of language looking at the genes that are directly responsible

909
00:58:02,750 --> 00:58:05,960
the capacity to learn a new language

910
00:58:05,970 --> 00:58:11,110
and one bit of evidence that these genes are implicated is that some unfortunate people

911
00:58:11,150 --> 00:58:13,660
have point mutations in these genes

912
00:58:13,700 --> 00:58:17,790
and such people are unable to learn a new language

913
00:58:18,610 --> 00:58:23,230
in general there some support at least a very broad level for the claim that

914
00:58:23,230 --> 00:58:26,580
language is in some sense part of human nature

915
00:58:26,660 --> 00:58:30,080
what do we mean by language what are we talking about when we talk about

916
00:58:30,080 --> 00:58:34,930
language we don't want to restrict ourselves for instance to english or french

917
00:58:34,980 --> 00:58:37,920
what what all languages share

918
00:58:39,500 --> 00:58:43,840
all languages are creative

919
00:58:43,890 --> 00:58:46,700
and this means a couple of things

920
00:58:46,710 --> 00:58:51,140
one meaning is the meaning emphasise by renee descartes

921
00:58:51,190 --> 00:58:55,440
when renee descartes argued that we are more than merely machines

922
00:58:55,490 --> 00:59:00,430
his best piece of evidence for him was human capacity for language no machine could

923
00:59:00,430 --> 00:59:01,320
do this

924
00:59:01,370 --> 00:59:05,240
because our capacity for language is unbounded and free

925
00:59:05,250 --> 00:59:07,920
we could say anything we choose to say

926
00:59:08,020 --> 00:59:09,840
we have free will

927
00:59:09,860 --> 00:59:12,430
and in fact language allows us

928
00:59:12,440 --> 00:59:15,790
to produce a virtually infinity of sentences

929
00:59:15,840 --> 00:59:19,750
so we could create understand things is we never heard before

930
00:59:19,800 --> 00:59:22,600
and there are a lot of sentences

931
00:59:23,450 --> 00:59:28,190
if you want to estimate how many grammatical sentences under twenty words in english the

932
00:59:29,530 --> 00:59:30,870
o lord

933
00:59:30,930 --> 00:59:40,530
and what this means is that any theory of language use and language comprehension can

934
00:59:40,530 --> 00:59:44,490
not simply appeal to all list

935
00:59:44,510 --> 00:59:47,160
when you understand the sentence i said

936
00:59:47,170 --> 00:59:50,720
you have to have the capacity to understand the sentence even if you've never heard

937
00:59:50,720 --> 00:59:51,760
it before

938
00:59:51,820 --> 00:59:56,970
and this is because we can effortlessly produced and understand sentences but no human

939
00:59:56,980 --> 00:59:58,420
has ever said

940
00:59:58,460 --> 01:00:00,630
before on earth

941
01:00:00,680 --> 01:00:02,990
would anybody volunteered as a sentence

942
01:00:03,040 --> 01:00:07,610
none of seems non derogatory has never been spoken before

943
01:00:13,770 --> 01:00:16,290
there are all star

944
01:00:20,060 --> 01:00:23,730
it's surprisingly easy to get a purple tie on ebay if you don't care much

945
01:00:23,730 --> 01:00:26,280
about quality

946
01:00:26,290 --> 01:00:28,830
i can imagine no one else in the world as it was before

947
01:00:28,980 --> 01:00:37,410
i am upset that one not easily download buffy the vampire slayer through items

948
01:00:38,920 --> 01:00:41,910
it's possible somebody said both those sentences before

949
01:00:41,920 --> 01:00:47,290
but you probably have not heard of but you understand the medium

950
01:00:47,300 --> 01:00:50,770
so how do you do it well

951
01:00:50,790 --> 01:00:51,940
you have rules

952
01:00:51,960 --> 01:00:54,260
you have rules in your head you've learned

953
01:00:54,310 --> 01:00:56,470
what the words mean

954
01:00:56,490 --> 01:01:01,130
but you have abstract unconscious rules that take these words

955
01:01:01,140 --> 01:01:04,480
figure out the order and in a fraction of a second

956
01:01:04,500 --> 01:01:06,390
give rise to understanding

957
01:01:06,400 --> 01:01:08,710
that's the sort of thing linguist

958
01:01:10,100 --> 01:01:13,310
take some standard examples from the linguistic study

959
01:01:15,460 --> 01:01:19,500
and bear in mind the rules we're talking about here are not rules you explicitly

960
01:01:20,340 --> 01:01:24,160
there are automatic rules of the same sort of talk about in the context of

961
01:01:24,160 --> 01:01:28,120
visual perception in that the implicit and unconscious

962
01:01:28,140 --> 01:01:29,840
and not accessible

963
01:01:29,850 --> 01:01:30,830
two two

964
01:01:30,840 --> 01:01:37,690
explicit understanding so for instance i immediately read the pig is eager to eat

965
01:01:37,790 --> 01:01:42,170
versus the pig is easy to see

966
01:01:42,220 --> 01:01:44,760
and in a fraction of a second you know

967
01:01:44,770 --> 01:01:49,990
there's an important difference the pig is really means the state of affairs that we're

968
01:01:49,990 --> 01:01:52,540
talking about is one to pick those thing

969
01:01:52,560 --> 01:01:53,770
the ECB

970
01:01:53,800 --> 01:01:55,990
is when the pig is being eaten

971
01:01:56,000 --> 01:01:57,520
you see a sentence like

972
01:01:57,530 --> 01:02:00,390
bill knew that john like ten

973
01:02:00,440 --> 01:02:03,730
and you know without even knowing how you know

974
01:02:03,740 --> 01:02:07,130
but this can mean that bill knew that john white

975
01:02:08,630 --> 01:02:11,340
or it could mean that belong to the john light

976
01:02:12,830 --> 01:02:16,350
but it can mean that bill knew that john light john

977
01:02:16,360 --> 01:02:20,460
natural interpretation factors that belong to the john like built

978
01:02:20,480 --> 01:02:23,090
the two words corefer

979
01:02:23,110 --> 01:02:26,370
contrast that will build new that john like himself

980
01:02:26,420 --> 01:02:30,250
which only has the meaning belonging the john light john

981
01:02:30,300 --> 01:02:32,530
and this is what linguists do for a living

982
01:02:32,540 --> 01:02:35,750
so if you hear me talking with is that i want to spend the next

983
01:02:35,750 --> 01:02:37,810
four years of my life stating that

984
01:02:37,820 --> 01:02:39,160
you should become like this

985
01:02:39,180 --> 01:02:44,230
but that this sort of those are sort of phenomenon that we're interested in

986
01:02:44,860 --> 01:02:48,190
it gets more complicated those are examples from syntax

987
01:02:48,190 --> 01:02:54,010
we're going to focus a lot today on the whole issue of biochemical reactions and

988
01:02:54,010 --> 01:02:55,480
the issue of energy

989
01:02:55,530 --> 01:02:59,040
and this gets us into the realization

990
01:02:59,090 --> 01:03:03,210
that there really are two kinds of biochemical reactions some of you may have a

991
01:03:03,220 --> 01:03:06,260
lot to learn this along time ago either

992
01:03:06,270 --> 01:03:08,240
x organic reactions

993
01:03:08,260 --> 01:03:11,710
that release energy to produce energies are produced

994
01:03:11,720 --> 01:03:13,140
as they proceed

995
01:03:13,230 --> 01:03:17,720
or conversely and organic reactions which

996
01:03:17,730 --> 01:03:21,230
require an investment of energy in order to move forward

997
01:03:21,240 --> 01:03:24,960
so here obviously if this is this is a high energy state

998
01:03:25,010 --> 01:03:26,400
and we're talking about

999
01:03:26,420 --> 01:03:32,630
the free energy of the system which is one way to depict in thermodynamic language

1000
01:03:32,740 --> 01:03:35,040
how much energy is in the molecule

1001
01:03:35,050 --> 01:03:39,030
if we go from a higher energy state to a lower energy state

1002
01:03:39,080 --> 01:03:42,620
then what we can draw this like this and we can realize

1003
01:03:42,660 --> 01:03:46,770
then in order to conserve energy the energy that was inherited this molecule the high

1004
01:03:46,770 --> 01:03:51,670
potential energy is released as this ball of this molecule rolls down the hill

1005
01:03:51,720 --> 01:03:57,060
and therefore the reaction yields energy it's actually gonna can conversely if we want this

1006
01:03:57,060 --> 01:03:58,500
energy to proceed

1007
01:03:58,510 --> 01:04:01,990
we need to invest excuse me want this reaction to proceed

1008
01:04:02,040 --> 01:04:06,030
we need to invest free energy in order to make it happen the free energy

1009
01:04:06,030 --> 01:04:10,820
happens to be more often than not in the form of chemical bonds i e

1010
01:04:10,830 --> 01:04:16,210
energy that can be invested for example by taking advantage of the potential energy stored

1011
01:04:16,210 --> 01:04:22,790
in these phospho diaster in this phosphate phosphate linkages indicated right here

1012
01:04:22,810 --> 01:04:27,230
here by the way the space filling model of ATP just for

1013
01:04:27,240 --> 01:04:30,210
for your information that's the way it would actually look in life and this is

1014
01:04:30,210 --> 01:04:32,680
the way we we actually brought

1015
01:04:32,700 --> 01:04:35,540
now having said that

1016
01:04:35,550 --> 01:04:39,270
if we look at the the free energy profile of

1017
01:04:40,370 --> 01:04:48,100
the biochemical changes then we can depict them once again this very schematic way here

1018
01:04:48,150 --> 01:04:49,500
and by the way

1019
01:04:49,630 --> 01:04:52,970
free energy is called

1020
01:04:53,360 --> 01:05:00,420
this is called g the gibbs free energy after joe site gibbs who was thermodynamic

1021
01:05:00,420 --> 01:05:03,470
which is in the nineteenth century at yale in new haven

1022
01:05:03,510 --> 01:05:06,730
and here what we see is that the change in free energy

1023
01:05:06,860 --> 01:05:10,990
between the reactants and the products is given by delta g

1024
01:05:11,000 --> 01:05:15,570
so by definition we start out the reaction with reactance and we end up at

1025
01:05:15,570 --> 01:05:16,990
the end of the reaction

1026
01:05:17,000 --> 01:05:24,060
with products and overall there if the reaction is actually gonna get will proceed forward

1027
01:05:24,100 --> 01:05:27,990
it releases energy in the release the net release of energy is indicated here by

1028
01:05:27,990 --> 01:05:29,610
delta g

1029
01:05:29,620 --> 01:05:31,840
but more often than not

1030
01:05:31,860 --> 01:05:35,790
biochemical reactions that are energetically favored

1031
01:05:35,800 --> 01:05:37,320
there ex are gonna make

1032
01:05:37,340 --> 01:05:44,090
actually can happen spontaneously they don't happen spontaneously because for various reasons they have to

1033
01:05:44,090 --> 01:05:46,370
pass through an intermediate state

1034
01:05:46,420 --> 01:05:49,740
which actually represents a much higher

1035
01:05:49,920 --> 01:05:53,380
free energy than the initial reactance process

1036
01:05:53,400 --> 01:05:58,230
and this higher free energy that they need to acquire in order to move over

1037
01:05:58,230 --> 01:06:03,290
the hill and down into the valley is called the energy of activation the activation

1038
01:06:04,330 --> 01:06:07,930
and therefore if i were to supply these reactance

1039
01:06:08,990 --> 01:06:13,460
the energy principle it's whatsoever to heat up these reactants and therefore give them high

1040
01:06:13,460 --> 01:06:16,800
degree of thermal energy which they might be able to use

1041
01:06:16,810 --> 01:06:19,290
to move up to this high energy state

1042
01:06:19,300 --> 01:06:23,400
i gave i supplied them with free energy by giving them heat

1043
01:06:23,450 --> 01:06:27,460
then they might be able to move up here and then rolled down the hill

1044
01:06:27,480 --> 01:06:32,630
but in the absence of actually actively intervening in supplying with them and then they

1045
01:06:32,640 --> 01:06:37,260
remain right here and they may remain like right there for a million years

1046
01:06:37,270 --> 01:06:38,950
even though in principle

1047
01:06:38,970 --> 01:06:42,450
if they were to reach down here there would be much happier in terms of

1048
01:06:42,660 --> 01:06:44,920
reaching a much lower energy state

1049
01:06:44,940 --> 01:06:49,760
to state the obvious all these kinds of reactions which to state which to reached

1050
01:06:49,760 --> 01:06:51,970
the lowest energy state possible

1051
01:06:51,980 --> 01:06:58,730
but in real time it can't happen if there is a high energy of activation

1052
01:06:58,750 --> 01:07:01,100
now what do enzymes do

1053
01:07:01,150 --> 01:07:05,630
as always i'm glad i asked the question what they do is they lower the

1054
01:07:05,630 --> 01:07:07,880
energy of activation

1055
01:07:07,970 --> 01:07:08,890
and this is

1056
01:07:08,930 --> 01:07:12,110
in one sense obvious and in one sense it subtle

1057
01:07:12,130 --> 01:07:16,830
because enzymes have no effect on the free energy state of the reactants they have

1058
01:07:16,830 --> 01:07:19,490
no effect on the free energy of the products

1059
01:07:19,570 --> 01:07:22,500
all they do is to lower the hump

1060
01:07:22,510 --> 01:07:25,080
and they may lower very substantially

1061
01:07:25,090 --> 01:07:27,560
and because the lower it substantially

1062
01:07:27,610 --> 01:07:32,020
it might be that these some of the reactance here made just through chance

1063
01:07:32,400 --> 01:07:34,680
acquisition of thermal energy

1064
01:07:34,690 --> 01:07:35,680
be able to

1065
01:07:35,690 --> 01:07:40,680
move over the much lower home and moved down into the state right here

1066
01:07:40,690 --> 01:07:43,070
that the actual

1067
01:07:43,350 --> 01:07:47,690
difference in in the gibbs free energy is totally unaffected

1068
01:07:47,770 --> 01:07:50,360
all that happens is that the enzymes

1069
01:07:50,370 --> 01:07:56,090
by lowering the energy of activation make this possible in real time

1070
01:07:56,110 --> 01:08:00,900
the fact is that ultimately if we were to

1071
01:08:00,920 --> 01:08:06,650
two plot many kinds of reactions many reactions as is indicated here

1072
01:08:06,660 --> 01:08:09,450
i have a very high activation energy

1073
01:08:09,470 --> 01:08:12,560
and therefore we look like this

1074
01:08:12,580 --> 01:08:18,510
but there could be other reactions which might have an activation energy looks like this

1075
01:08:19,980 --> 01:08:21,910
almost nothing at all

1076
01:08:21,920 --> 01:08:27,860
and these reactions could happen spontaneously in the room temperature in the absence of any

1077
01:08:27,860 --> 01:08:29,570
intervention by

1078
01:08:30,670 --> 01:08:35,220
an enzyme for example let's say we're talking about a carboxyl group which discharges the

1079
01:08:36,320 --> 01:08:39,790
we've talked about that already

1080
01:08:39,880 --> 01:08:45,810
well that reaction happens spontaneously at room temperature doesn't need an enzyme to make it

1081
01:08:45,810 --> 01:08:50,440
happen it can happen because there is essentially no energy of activation

1082
01:08:50,480 --> 01:08:56,420
but the great majority of biochemical reactions to have such a an activation energy and

1083
01:08:56,420 --> 01:08:59,480
therefore do require a lowering

1084
01:08:59,540 --> 01:09:02,420
like this in order to take place

1085
01:09:02,460 --> 01:09:04,600
now let's imagine other

1086
01:09:04,620 --> 01:09:08,200
other versions of the

1087
01:09:08,230 --> 01:09:11,160
of the energy profile of reaction

1088
01:09:11,200 --> 01:09:14,600
and keep in mind what i'm showing here on the abscissa is just the course

1089
01:09:14,610 --> 01:09:15,930
of the reaction

1090
01:09:15,980 --> 01:09:19,950
you can imagine that i'm not really plotting time i'm just talking about the situation

1091
01:09:20,040 --> 01:09:24,740
where to the left the reaction hasn't happened and to the right it has happened

1092
01:09:24,740 --> 01:09:25,460
but but but

1093
01:09:26,530 --> 01:09:29,180
and proceeded cells are

1094
01:09:30,230 --> 01:09:32,550
it's really hard to say

1095
01:09:32,760 --> 01:09:37,760
so is all

1096
01:09:38,610 --> 01:09:40,760
who were team room

1097
01:09:44,050 --> 01:09:45,450
all of

1098
01:09:45,460 --> 01:09:47,440
now let us hope

1099
01:09:47,510 --> 01:09:57,730
it's not homework one embarrassing to come to

1100
01:09:57,750 --> 01:10:03,530
people who do devote move back for a lot of time like say to you

1101
01:10:05,360 --> 01:10:07,850
the whole world which is the

1102
01:10:07,920 --> 01:10:09,190
we're not going

1103
01:10:09,210 --> 01:10:15,130
and that's why stuff like that we have in the nineteenth century when farmers shopkeepers

1104
01:10:19,390 --> 01:10:21,510
that is that is

1105
01:10:21,720 --> 01:10:25,980
house councils so we're going to work

1106
01:10:25,990 --> 01:10:33,340
one of the specialist players have and i think it is it's

1107
01:10:36,190 --> 01:10:39,570
so we know how to use this

1108
01:10:39,930 --> 01:10:41,460
this the last slide

1109
01:10:41,470 --> 01:10:45,770
in person together with spend more time with a

1110
01:10:45,800 --> 01:10:49,060
one with

1111
01:10:49,110 --> 01:10:52,270
now that the technology is in the

1112
01:10:52,280 --> 01:10:55,370
all the these changes in the middle of

1113
01:10:55,390 --> 01:11:03,330
for different media all but this is medium this is going to have to get

1114
01:11:03,430 --> 01:11:04,830
around reading

1115
01:11:04,850 --> 01:11:09,800
stuff like that that's all the more or

1116
01:11:09,870 --> 01:11:12,340
come back to the magazine

1117
01:11:13,710 --> 01:11:14,720
i work

1118
01:11:14,740 --> 01:11:20,750
for services process and we found

1119
01:11:20,770 --> 01:11:25,910
on the first numbers are interested in those of analysis

1120
01:11:26,990 --> 01:11:33,830
all but one ratio of field work in biotechnology this is what this

1121
01:11:37,560 --> 01:11:39,770
this five four

1122
01:11:39,780 --> 01:11:46,490
one percent seven percent of all he was saying you don't think about

1123
01:11:46,500 --> 01:11:51,310
technology which basically similar to that of course is

1124
01:11:51,340 --> 01:11:52,790
part of their lives

1125
01:11:53,530 --> 01:11:57,250
we treat web science is big

1126
01:11:57,270 --> 01:12:01,840
the way something like twenty to so that there

1127
01:12:08,340 --> 01:12:10,580
speak for yourself

1128
01:12:10,660 --> 01:12:13,680
i think

1129
01:12:13,850 --> 01:12:20,040
so what we're going to

1130
01:12:20,560 --> 01:12:26,520
once again this is just an assertion of american data once it

1131
01:12:26,530 --> 01:12:29,370
you can see that were part

1132
01:12:29,480 --> 01:12:32,310
of the every household has

1133
01:12:32,330 --> 01:12:33,780
fair amount

1134
01:12:33,800 --> 01:12:40,010
can will see that was going to say but i don't you one of our

1135
01:12:40,020 --> 01:12:41,610
so is this

1136
01:12:41,670 --> 01:12:44,730
more individual mobility parker

1137
01:12:48,240 --> 01:12:50,980
many interesting here

1138
01:12:51,030 --> 01:12:53,370
this was agreed with the way

1139
01:12:56,500 --> 01:13:01,520
well i think find injects get out research

1140
01:13:01,530 --> 01:13:05,750
dave in nineteen sixty five to my first

1141
01:13:06,500 --> 01:13:08,520
i'm interested trying to do

1142
01:13:08,530 --> 01:13:14,830
he speeches switch switch and stuff like that place like the bus

1143
01:13:19,760 --> 01:13:23,810
and the slide what may show

1144
01:13:23,830 --> 01:13:28,460
this page is more pieces in the developed world there has been a major

1145
01:13:28,680 --> 01:13:31,270
know that similar

1146
01:13:31,280 --> 01:13:38,970
many of words but this is cheating we know are three trials

1147
01:13:38,990 --> 01:13:41,660
three right track

1148
01:13:41,670 --> 01:13:44,910
between north america and europe is

1149
01:13:44,930 --> 01:13:49,430
so willing to push these restrictions on the sovereignty

1150
01:13:49,450 --> 01:13:50,250
but still

1151
01:13:50,260 --> 01:13:52,840
it's pretty easy for all of us to

1152
01:13:52,870 --> 01:13:56,270
this is not

1153
01:13:56,850 --> 01:14:00,810
performances and my grandmother was the challenge is to show that the

1154
01:14:01,270 --> 01:14:03,220
just make sure

1155
01:14:03,230 --> 01:14:09,440
it was on her now we can get on sky to easily of talk to

1156
01:14:09,440 --> 01:14:10,330
each other

1157
01:14:10,640 --> 01:14:13,270
how many people all the

1158
01:14:13,290 --> 01:14:17,260
in the past few days

1159
01:14:17,260 --> 01:14:18,880
and then i will leave it alone

1160
01:14:18,900 --> 01:14:21,110
i'll get into the state

1161
01:14:21,170 --> 01:14:22,490
you will see

1162
01:14:23,380 --> 01:14:25,030
is really normal modes

1163
01:14:25,050 --> 01:14:27,720
they have is one and only one frequency

1164
01:14:27,720 --> 01:14:31,240
the country will hold at the same moment in time there in phase

1165
01:14:31,240 --> 01:14:32,670
you will clearly see

1166
01:14:32,840 --> 01:14:34,440
this distance

1167
01:14:34,490 --> 01:14:38,900
more than doubled this one then have go this one

1168
01:14:38,940 --> 01:14:40,590
i will try that

1169
01:14:45,220 --> 01:14:50,720
double pendulum oh my goodness

1170
01:14:50,760 --> 01:14:54,240
this is the double pendulum

1171
01:15:02,510 --> 01:15:04,900
so i have to

1172
01:15:04,900 --> 01:15:08,220
private little but it really only very little

1173
01:15:10,900 --> 01:15:13,470
and then i will stop driving it

1174
01:15:13,530 --> 01:15:15,740
this is the model

1175
01:15:15,780 --> 01:15:18,280
is at the bottom one is further away than

1176
01:15:19,010 --> 01:15:20,990
but what

1177
01:15:20,990 --> 01:15:22,990
this is the normal mode

1178
01:15:23,030 --> 01:15:25,970
you see that is not a straight line

1179
01:15:26,090 --> 01:15:28,530
it's hard to say that this one was the square root of two of course

1180
01:15:28,530 --> 01:15:29,720
from seeds

1181
01:15:29,740 --> 01:15:31,280
but this

1182
01:15:31,280 --> 01:15:33,170
that's what it is

1183
01:15:33,170 --> 01:15:35,170
no excite

1184
01:15:35,190 --> 01:15:41,650
second normal mode which is the residence

1185
01:15:41,740 --> 01:15:43,400
that's it

1186
01:15:43,510 --> 01:15:46,150
i can almost stopped swinging it

1187
01:15:46,190 --> 01:15:51,030
notice that the other one is much larger and due to the lower

1188
01:15:51,070 --> 01:15:52,030
the ratio

1189
01:15:52,050 --> 01:15:55,470
one with the square to you know to the out of phase

1190
01:15:55,510 --> 01:15:57,610
it's the minus sign

1191
01:15:57,670 --> 01:15:59,420
out of phase

1192
01:15:59,470 --> 01:16:02,280
and the animal they were in fact

1193
01:16:02,380 --> 01:16:04,840
so all of that will follow

1194
01:16:05,650 --> 01:16:08,170
the recipe

1195
01:16:08,170 --> 01:16:11,190
if i give this problem on an exam you would have

1196
01:16:12,630 --> 01:16:15,760
OK let me because the pages long

1197
01:16:15,780 --> 01:16:18,110
i would certainly where border professor

1198
01:16:18,130 --> 01:16:19,800
on campus

1199
01:16:19,800 --> 01:16:21,990
that these because you have symmetry

1200
01:16:22,030 --> 01:16:24,300
this doesn't have symmetry

1201
01:16:25,670 --> 01:16:32,510
c o c one c two two

1202
01:16:32,530 --> 01:16:37,050
i haven't justified what i call woman two problems i call this one

1203
01:16:37,070 --> 01:16:38,740
i call this time

1204
01:16:38,760 --> 01:16:42,450
so c two oversee one plus two point four

1205
01:16:42,670 --> 01:16:47,130
it's good that you asked that and here i change the sequence because i noted

1206
01:16:47,130 --> 01:16:50,800
this one was larger than this one so you seen one obviously two

1207
01:16:50,870 --> 01:16:54,840
it's minus two points for the minus sign means out of phase

1208
01:16:55,220 --> 01:16:57,570
good that you asked very good

1209
01:17:04,550 --> 01:17:08,610
and slowly slowly going to turn you into exports

1210
01:17:08,650 --> 01:17:11,940
and now we're going to try something else to see

1211
01:17:11,990 --> 01:17:14,510
good you're

1212
01:17:14,530 --> 01:17:15,820
intuition is

1213
01:17:15,820 --> 01:17:17,530
i want you to understand

1214
01:17:17,590 --> 01:17:22,510
you intuition is no better no worse than my own

1215
01:17:22,570 --> 01:17:24,610
in other words there is no way

1216
01:17:24,630 --> 01:17:28,210
i could have done any better than you did when i was

1217
01:17:28,320 --> 01:17:32,300
in high school or whatever it was maybe college when i first saw this i

1218
01:17:32,300 --> 01:17:34,280
also my first impression was

1219
01:17:34,320 --> 01:17:35,940
what the hell is straight line

1220
01:17:36,150 --> 01:17:38,150
give a little thought fuel

1221
01:17:38,170 --> 01:17:39,490
consider conclusion

1222
01:17:39,610 --> 01:17:41,320
cannot be

1223
01:17:41,340 --> 01:17:45,490
then ultimately comes out of this so don't feel bad if you intuition that you

1224
01:17:46,220 --> 01:17:49,360
i'm as that you are with

1225
01:17:49,400 --> 01:17:50,920
so now we're going to

1226
01:17:50,940 --> 01:17:53,320
evaluate the system

1227
01:17:53,340 --> 01:17:54,670
which will take you

1228
01:17:54,720 --> 01:17:57,820
not going have to work out the general recipe

1229
01:17:57,860 --> 01:18:00,490
and that is for strings and three

1230
01:18:04,440 --> 01:18:07,470
one car

1231
01:18:10,070 --> 01:18:12,400
three cars

1232
01:18:12,440 --> 01:18:15,240
and also things that spring constant k

1233
01:18:15,280 --> 01:18:18,280
nicely symmetric you think

1234
01:18:18,320 --> 01:18:26,150
and all have all objects of mass and

1235
01:18:26,210 --> 01:18:28,220
i want to know

1236
01:18:28,280 --> 01:18:33,090
i want to see whether we have any intuition

1237
01:18:33,110 --> 01:18:35,380
without being too complicated

1238
01:18:35,420 --> 01:18:37,260
forty three

1239
01:18:37,320 --> 01:18:41,780
normal mode solutions there must be an omega minus

1240
01:18:41,840 --> 01:18:45,550
all three are in phase

1241
01:18:45,630 --> 01:18:47,710
then there must be an omega plus

1242
01:18:47,710 --> 01:18:49,340
which is more complicated

1243
01:18:49,360 --> 01:18:51,420
and then there must be an omega

1244
01:18:51,470 --> 01:18:56,420
plus plus which is the highest of all three

1245
01:18:56,470 --> 01:18:59,970
in the highest possible frequency

1246
01:19:01,440 --> 01:19:06,990
object two adjacent objects always out of phase so if you have twenty objects in

1247
01:19:06,990 --> 01:19:08,550
the highest mold

1248
01:19:08,550 --> 01:19:11,550
number one is out of phase was number two and number two is out of

1249
01:19:11,550 --> 01:19:17,550
phase with number three number three is out of phase with four and so on

1250
01:19:17,570 --> 01:19:21,090
let's first look at omega minus

1251
01:19:21,110 --> 01:19:25,170
i won't even asking you whether you have any idea what omega minus is you

1252
01:19:25,170 --> 01:19:27,940
don't have that i don't have

1253
01:19:27,940 --> 01:19:31,010
well object to be

1254
01:19:31,010 --> 01:19:34,380
so for instance i displays than the first one

1255
01:19:34,420 --> 01:19:40,630
over the distance which ideas normalized to plus what i call that plus one

1256
01:19:40,670 --> 01:19:48,050
and the idea here where this one will be any idea where that one

1257
01:19:48,090 --> 01:19:50,840
what you think the second one will be

1258
01:19:50,880 --> 01:19:51,740
plus one

1259
01:19:51,760 --> 01:19:54,240
what will be the third one b

1260
01:19:54,260 --> 01:19:55,380
plus one

1261
01:19:57,360 --> 01:19:59,860
why does it have to be wrong

1262
01:20:00,440 --> 01:20:02,090
suppose this one

1263
01:20:02,110 --> 01:20:04,130
is a plus one

1264
01:20:04,170 --> 01:20:08,010
and suppose this one is also a plus one

1265
01:20:08,040 --> 01:20:10,050
this approach

1266
01:20:10,050 --> 01:20:13,360
it is no longer than it wants to be and this is no longer than

1267
01:20:13,360 --> 01:20:16,130
it wants to be so there's no force on this object

1268
01:20:16,150 --> 01:20:17,490
so that's not possible

1269
01:20:17,690 --> 01:20:20,530
object cannot go anywhere

1270
01:20:20,530 --> 01:20:22,050
any suggestions

1271
01:20:22,050 --> 01:20:23,400
which of those

1272
01:20:23,420 --> 01:20:24,320
is wrong

1273
01:20:24,340 --> 01:20:25,800
is this correct

1274
01:20:25,840 --> 01:20:30,240
it's plus one

1275
01:20:30,260 --> 01:20:32,820
about the middle one

1276
01:20:32,860 --> 01:20:36,510
he wanted to for a little less

1277
01:20:36,610 --> 01:20:42,070
all have to be in phase don't use one minus signs

1278
01:20:42,070 --> 01:20:44,820
if you use the word minus sign then how can you do that you have

1279
01:20:44,820 --> 01:20:46,050
to be in face

1280
01:20:46,070 --> 01:20:48,240
must be on this site

1281
01:20:48,260 --> 01:20:50,720
we make the distance larger or smaller

1282
01:20:53,050 --> 01:20:59,840
and i tell you what it is

1283
01:20:59,970 --> 01:21:01,420
tell you what it is

1284
01:21:01,470 --> 01:21:02,990
you want to know

1285
01:21:03,030 --> 01:21:06,300
how you really want to know

1286
01:21:06,320 --> 01:21:10,030
i do

1287
01:21:10,030 --> 01:21:12,400
that obvious

1288
01:21:14,690 --> 01:21:19,050
now the next one

1289
01:21:19,070 --> 01:21:21,780
the next one is interesting

1290
01:21:21,800 --> 01:21:23,340
i will again

1291
01:21:23,370 --> 01:21:25,650
put this one plus one

1292
01:21:25,690 --> 01:21:30,950
because that is my

1293
01:21:30,970 --> 01:21:35,400
my starting point i always put plus one

1294
01:21:35,440 --> 01:21:37,130
now what

1295
01:21:37,130 --> 01:21:39,010
this one you can guess

1296
01:21:39,050 --> 01:21:41,610
really can

1297
01:21:41,630 --> 01:21:45,360
i think now about the symmetry of the system

1298
01:21:50,070 --> 01:21:53,220
very middle one they still

1299
01:21:53,260 --> 01:21:55,780
and this one comes to mind one

1300
01:21:59,720 --> 01:22:04,710
now to figure out what

1301
01:22:04,720 --> 01:22:09,380
remember the highest mode

1302
01:22:09,440 --> 01:22:13,130
they were always out of phase with the enormity home many you have

1303
01:22:13,130 --> 01:22:16,670
so it's already certain that this one must be on this site

1304
01:22:16,670 --> 01:22:21,760
and there's already certain that this will mostly on that site

1305
01:22:21,760 --> 01:22:27,590
the following content is provided under creative commons license your support will help MIT opencourseware

1306
01:22:27,590 --> 01:22:31,700
continue to offer high quality educational resources for free

1307
01:22:31,720 --> 01:22:36,530
to make a donation or to view additional materials from hundreds of MIT courses

1308
01:22:36,550 --> 01:22:43,600
his MIT opencourseware OCW MIT that EDU

1309
01:22:43,600 --> 01:22:45,650
so let's start right away with

1310
01:22:45,650 --> 01:22:49,670
step we will need to to see before we can go into more advanced things

1311
01:22:49,670 --> 01:22:53,740
so hopefully yesterday in recitation you had a bit about vectors

1312
01:22:53,790 --> 01:22:57,680
how many of you actually knew about that before that

1313
01:22:57,740 --> 01:22:59,020
OK that's

1314
01:22:59,030 --> 01:23:00,430
the vast majority

1315
01:23:00,460 --> 01:23:05,670
if you're not one of those people well hopefully you will

1316
01:23:05,780 --> 01:23:09,990
none about the task right now i'm sorry but the learning company it's cheaper for

1317
01:23:10,020 --> 01:23:12,310
the first week but hopefully

1318
01:23:12,530 --> 01:23:16,160
you will be just fine if you have you know

1319
01:23:16,170 --> 01:23:17,410
probable factors

1320
01:23:17,460 --> 01:23:21,930
do go to your instructor position is because of his cells for

1321
01:23:21,950 --> 01:23:24,560
next practice if you feel the need to

1322
01:23:24,600 --> 01:23:26,030
you see it

1323
01:23:26,070 --> 01:23:27,950
pretty easy

1324
01:23:28,920 --> 01:23:32,290
just to remind you

1325
01:23:35,010 --> 01:23:37,540
is the quantity that has both

1326
01:23:37,570 --> 01:23:38,480
the direction

1327
01:23:38,530 --> 01:23:40,380
and the magnitude

1328
01:23:51,100 --> 01:23:57,530
so completely the way you talk

1329
01:23:57,540 --> 01:23:58,370
it is

1330
01:23:58,380 --> 01:24:01,260
by some wholly that's OK

1331
01:24:01,320 --> 01:24:05,790
and so it has a length and it's pointing in some direction

1332
01:24:07,570 --> 01:24:12,840
so now the way that we compute things with is typically is we introduce the

1333
01:24:12,850 --> 01:24:14,350
coordinate system

1334
01:24:14,400 --> 01:24:18,010
so when the plane x y axis if we're in space

1335
01:24:18,010 --> 01:24:19,760
x y z axes

1336
01:24:19,780 --> 01:24:21,680
usually i will try to follow

1337
01:24:21,730 --> 01:24:25,670
my x y x is consistently to look like this

1338
01:24:25,680 --> 01:24:30,070
and then i can represented by a vector in terms of its components

1339
01:24:30,070 --> 01:24:33,100
along the coordinate axes so that means

1340
01:24:33,110 --> 01:24:36,600
when i have this whole i can ask how much does it go in the

1341
01:24:36,600 --> 01:24:39,700
x direction how much is it going to y direction how much does it go

1342
01:24:39,700 --> 01:24:41,950
in the z direction

1343
01:24:44,800 --> 01:24:48,670
well it's called is that a so small convention

1344
01:24:48,720 --> 01:24:53,350
when we have a vector quantity we put on our own remind us effect

1345
01:24:53,410 --> 01:24:58,360
if it's in the textbook sometimes it's in bold because it's easier to typeset

1346
01:24:58,730 --> 01:25:03,770
if you've tied into your favourite word processor does bold is easy and the cells

1347
01:25:03,790 --> 01:25:05,300
on the easy

1348
01:25:07,890 --> 01:25:09,480
the vector

1349
01:25:09,640 --> 01:25:12,510
you can try to decompose in terms

1350
01:25:14,320 --> 01:25:16,100
unit vectors

1351
01:25:16,110 --> 01:25:21,290
directed along the coordinate axes so the convention is there's a vector that we call

1352
01:25:21,300 --> 01:25:23,920
i had

1353
01:25:25,630 --> 01:25:26,790
that points

1354
01:25:26,860 --> 01:25:30,110
along the x axis and has length one

1355
01:25:30,170 --> 01:25:34,950
this effect called change that does the same the y axis and k had

1356
01:25:34,950 --> 01:25:38,200
but does the same along the z axis

1357
01:25:40,850 --> 01:25:45,230
we can express any vector in terms of its components soviet annotations

1358
01:25:46,380 --> 01:25:48,850
a one a two a three

1359
01:25:50,170 --> 01:25:53,520
these square brackets

1360
01:25:55,390 --> 01:25:58,320
it's angular like

1361
01:25:59,910 --> 01:26:01,440
the length

1362
01:26:01,450 --> 01:26:03,420
the vector

1363
01:26:03,440 --> 01:26:05,750
we denote by

1364
01:26:05,760 --> 01:26:08,970
if you want to the same notation as the absolute value so that's going to

1365
01:26:08,970 --> 01:26:11,100
be an number

1366
01:26:11,110 --> 01:26:15,520
as we say now a scalar quantity is scalar quantities are useful

1367
01:26:15,540 --> 01:26:19,380
numerical quantity as opposed to effect

1368
01:26:19,390 --> 01:26:22,320
and its direction

1369
01:26:22,450 --> 01:26:27,880
sometimes called

1370
01:26:27,940 --> 01:26:33,640
the of a and that can be obtained just by

1371
01:26:33,660 --> 01:26:40,280
scaling the vector down to unit length for example by dividing it by its length

1372
01:26:44,260 --> 01:26:52,650
well there's a lot of notation to be around

1373
01:26:52,660 --> 01:26:55,520
so for example if i have two points

1374
01:26:57,710 --> 01:27:00,260
then i can talk about from p to q

1375
01:27:00,310 --> 01:27:03,090
that is called the coffee

1376
01:27:04,700 --> 01:27:07,140
so maybe would a

1377
01:27:07,160 --> 01:27:11,960
but that doesn't really have necessarily starting point and the ending point

1378
01:27:11,970 --> 01:27:13,260
this we find

1379
01:27:13,280 --> 01:27:14,900
decide to stop here

1380
01:27:14,910 --> 01:27:18,450
and i go by the same distance in the same direction this is also the

1381
01:27:18,450 --> 01:27:22,570
topic it's the same that

1382
01:27:22,570 --> 01:27:25,870
so a lot of those will host anything at the origin but we don't have

1383
01:27:27,910 --> 01:27:32,140
so let's just cheque and c

1384
01:27:32,810 --> 01:27:35,650
how things went in recitation

1385
01:27:40,640 --> 01:27:43,760
so let's say that i give you the vector

1386
01:27:43,760 --> 01:27:45,760
three two one

1387
01:27:45,770 --> 01:27:50,370
and so

1388
01:27:50,450 --> 01:28:02,060
what do you think about the length of this vector

1389
01:28:03,320 --> 01:28:05,280
i see

1390
01:28:05,300 --> 01:28:07,210
and so forming

1391
01:28:07,220 --> 01:28:09,900
so a lot of you are

1392
01:28:09,930 --> 01:28:12,830
and in the same thing evidence for photos

1393
01:28:15,700 --> 01:28:21,320
OK i think the overwhelming vote is in favor of and so on the two

1394
01:28:21,880 --> 01:28:24,830
i see some six is i don't know if it's a perfectly good and so

1395
01:28:24,830 --> 01:28:26,460
too but hopefully

1396
01:28:26,510 --> 01:28:28,570
in a few minutes if you want to be

1397
01:28:28,580 --> 01:28:30,940
i don't know any more

1398
01:28:31,010 --> 01:28:33,910
so let's see how do we find

1399
01:28:33,960 --> 01:28:41,720
the line

1400
01:28:41,740 --> 01:28:45,360
of the vector three two one

1401
01:28:47,120 --> 01:28:49,750
so this that a

1402
01:28:49,810 --> 01:28:50,690
it comes

1403
01:28:50,700 --> 01:28:51,700
towards us

1404
01:28:51,720 --> 01:28:53,300
along the x axis

1405
01:28:53,310 --> 01:28:55,380
by the units

1406
01:28:55,430 --> 01:28:56,280
it goes

1407
01:28:56,310 --> 01:28:57,310
to the right

1408
01:28:57,320 --> 01:28:58,770
and on the y axis

1409
01:28:58,820 --> 01:29:00,660
by two units

1410
01:29:00,680 --> 01:29:02,320
and then it goes

1411
01:29:02,330 --> 01:29:05,950
by one unit along the z axis

1412
01:29:05,960 --> 01:29:08,660
so it's pointing towards here

1413
01:29:10,070 --> 01:29:13,140
he help paul

1414
01:29:13,870 --> 01:29:17,640
how do we get life well maybe we can start with something easier the length

1415
01:29:17,640 --> 01:29:19,270
of the vector in the plane

1416
01:29:19,280 --> 01:29:22,180
so i self

1417
01:29:22,190 --> 01:29:25,090
that you know a is obtained from the to be

1418
01:29:25,120 --> 01:29:26,880
the plane x b

1419
01:29:29,120 --> 01:29:31,340
i had to change that

1420
01:29:31,360 --> 01:29:35,700
and then we just have to still go up by one unit

1421
01:29:35,760 --> 01:29:38,660
OK so let me try to take picture

1422
01:29:38,680 --> 01:29:41,510
in the vertical plane that contains a and b

1423
01:29:41,520 --> 01:29:42,750
if i do it

1424
01:29:42,760 --> 01:29:44,380
in the vertical plane

1425
01:29:44,400 --> 01:29:45,900
so that's the xerxes

1426
01:29:45,930 --> 01:29:48,560
that's not any particular boxes

1427
01:29:48,560 --> 01:29:51,550
for this task is the localisation conference

1428
01:29:51,570 --> 01:29:54,670
and you for instance we are looking for car

1429
01:29:54,700 --> 01:29:57,700
organizing where they are

1430
01:29:57,720 --> 01:30:00,990
and task will be identification

1431
01:30:00,990 --> 01:30:04,590
and differently from verification and detection

1432
01:30:04,610 --> 01:30:05,700
what task is

1433
01:30:05,720 --> 01:30:08,170
discriminate between different classes

1434
01:30:08,180 --> 01:30:10,600
of objects for instance

1435
01:30:10,620 --> 01:30:13,500
identification will be especially related

1436
01:30:14,130 --> 01:30:17,160
to identify

1437
01:30:17,190 --> 01:30:20,430
an object within a specific class

1438
01:30:20,440 --> 01:30:21,920
for instance here

1439
01:30:22,230 --> 01:30:26,050
we recognise this is a human face

1440
01:30:26,110 --> 01:30:27,950
we want to identify

1441
01:30:28,250 --> 01:30:31,310
this special this box within

1442
01:30:31,320 --> 01:30:36,120
the cost of faces

1443
01:30:36,130 --> 01:30:42,980
b identification with the task of categorisation we want to distinguish between different objects in

1444
01:30:42,980 --> 01:30:44,550
the scene scene

1445
01:30:45,620 --> 01:30:46,840
well this guy

1446
01:30:46,850 --> 01:30:49,800
well being one phase and so on

1447
01:30:49,810 --> 01:30:52,670
and how are we we want

1448
01:30:52,890 --> 01:30:56,620
i understand which kind of scene is

1449
01:30:56,630 --> 01:30:58,310
like endorsing

1450
01:30:58,340 --> 01:31:00,560
other seen all which kind of

1451
01:31:02,470 --> 01:31:08,550
showing pictures like traffic

1452
01:31:08,600 --> 01:31:09,800
of course

1453
01:31:09,860 --> 01:31:12,030
you system

1454
01:31:12,070 --> 01:31:13,470
is one aspiration

1455
01:31:13,480 --> 01:31:14,720
so built

1456
01:31:15,470 --> 01:31:17,230
artificial vision systems

1457
01:31:17,240 --> 01:31:19,600
although the computer vision

1458
01:31:19,610 --> 01:31:21,240
the recession do not

1459
01:31:21,300 --> 01:31:22,620
and later

1460
01:31:23,390 --> 01:31:24,480
in approach

1461
01:31:24,490 --> 01:31:25,780
the outcome

1462
01:31:25,800 --> 01:31:28,510
and what is the motivation behind

1463
01:31:28,570 --> 01:31:30,940
but the per cent if it were your city

1464
01:31:30,950 --> 01:31:33,950
are provided by many many many applications

1465
01:31:33,970 --> 01:31:35,450
of computer vision

1466
01:31:35,480 --> 01:31:36,760
in industry

1467
01:31:38,550 --> 01:31:39,620
this link

1468
01:31:39,630 --> 01:31:42,290
you can find a list of combining

1469
01:31:42,550 --> 01:31:44,230
collected by david lowe

1470
01:31:45,190 --> 01:31:48,750
which sells computer vision problem

1471
01:31:48,780 --> 01:31:52,610
and research efforts are going to develop

1472
01:31:52,620 --> 01:31:55,550
solutions to improve forest and

1473
01:31:55,560 --> 01:31:57,300
image search

1474
01:31:59,300 --> 01:32:01,050
or there are

1475
01:32:01,070 --> 01:32:03,100
very challenging projects like

1476
01:32:03,110 --> 01:32:05,370
like that to build

1477
01:32:05,380 --> 01:32:07,560
portable visual system

1478
01:32:07,850 --> 01:32:11,720
so people like people in find rather in the supermarket

1479
01:32:12,570 --> 01:32:14,260
assistive technology

1480
01:32:17,240 --> 01:32:17,990
to give

1481
01:32:19,660 --> 01:32:20,660
the ability

1482
01:32:20,670 --> 01:32:27,990
the dark forest in the eastern part of the process of

1483
01:32:28,000 --> 01:32:33,440
learning is that key component for sending intelligence system and because

1484
01:32:33,470 --> 01:32:36,010
c is intelligence

1485
01:32:36,060 --> 01:32:38,370
learning is an essential component

1486
01:32:38,920 --> 01:32:41,170
just to the study of

1487
01:32:41,180 --> 01:32:43,190
we just visual system

1488
01:32:43,200 --> 01:32:45,340
far from the point of view

1489
01:32:45,350 --> 01:32:47,930
neuroscience and computer vision

1490
01:32:47,940 --> 01:32:49,820
what is on their science

1491
01:32:50,300 --> 01:32:54,950
concentrate on mechanism that load the cortex

1492
01:32:54,970 --> 01:32:58,740
two task and that it's a great

1493
01:32:58,790 --> 01:33:02,000
computer vision times advising

1494
01:33:03,420 --> 01:33:09,140
fifty retrieval system or artificialintelligence pakistan vision vision

1495
01:33:09,160 --> 01:33:10,980
and vision systems

1496
01:33:10,990 --> 01:33:13,130
the that

1497
01:33:13,140 --> 01:33:17,110
represent one of the most trained in computer vision so

1498
01:33:17,160 --> 01:33:18,300
but the recognition

1499
01:33:18,970 --> 01:33:23,470
a key component for these and russia learning to become one of these so

1500
01:33:23,550 --> 01:33:25,560
is a key component in the study

1501
01:33:25,570 --> 01:33:29,100
of computer vision

1502
01:33:29,130 --> 01:33:32,950
and the mother computer vision the categorisation task

1503
01:33:33,000 --> 01:33:36,480
it is seen as

1504
01:33:36,490 --> 01:33:39,420
i supervised learning task

1505
01:33:39,430 --> 01:33:41,380
so given an emergency input

1506
01:33:41,430 --> 01:33:43,070
the desired output

1507
01:33:43,120 --> 01:33:45,990
is a need is labeled

1508
01:33:46,070 --> 01:33:49,520
they indicate the class of the image

1509
01:33:49,540 --> 01:33:53,600
and the machine vision system after training with a set of sample

1510
01:33:53,620 --> 01:33:54,910
very slightly

1511
01:33:56,310 --> 01:33:57,500
to these

1512
01:33:57,510 --> 01:34:00,640
we represent images as vectors

1513
01:34:00,700 --> 01:34:03,360
and you're presenting images as vectors

1514
01:34:03,470 --> 01:34:05,660
we should take into consideration

1515
01:34:06,010 --> 01:34:09,870
invariance specific informational emotional elimination

1516
01:34:10,820 --> 01:34:12,630
other changes that

1517
01:34:12,650 --> 01:34:15,350
are within the scene

1518
01:34:15,360 --> 01:34:16,730
and typically

1519
01:34:16,740 --> 01:34:20,120
dimension reduction are applied to reduce

1520
01:34:20,130 --> 01:34:23,490
the space the dimension of the space

1521
01:34:23,490 --> 01:34:27,340
and or to provide a powerful

1522
01:34:27,350 --> 01:34:32,860
the more powerful feature space like in the case of negative i think fertilization

1523
01:34:35,220 --> 01:34:38,110
after representing images

1524
01:34:38,120 --> 01:34:39,420
that i use it

1525
01:34:39,990 --> 01:34:47,010
together with statistical learning making mechanism to infer the costs of the scene or possibly

1526
01:34:48,490 --> 01:34:50,890
and these are very keen

1527
01:34:50,900 --> 01:34:53,700
ingredients in pattern recognition

1528
01:34:53,720 --> 01:34:56,500
but also used in computer vision

1529
01:34:56,520 --> 01:35:00,360
to solve some kind of problem within the of

1530
01:35:00,650 --> 01:35:06,330
pacification queries

1531
01:35:06,360 --> 01:35:11,140
but why computer vision is scene is so difficult because we have a similar problem

1532
01:35:11,140 --> 01:35:12,360
in classification

1533
01:35:13,630 --> 01:35:15,770
in many domains

1534
01:35:15,860 --> 01:35:18,450
first of all

1535
01:35:18,640 --> 01:35:20,860
a lot of data are collected

1536
01:35:20,890 --> 01:35:22,440
each second

1537
01:35:22,450 --> 01:35:24,120
we we need to

1538
01:35:24,370 --> 01:35:28,880
be very fast in understanding images because we want to

1539
01:35:28,890 --> 01:35:33,220
recognise to understand the images real time

1540
01:35:33,320 --> 01:35:35,450
this is the first challenge

1541
01:35:35,470 --> 01:35:37,580
and also

1542
01:35:39,120 --> 01:35:40,650
we were we to be

1543
01:35:40,660 --> 01:35:43,700
representation after the war and projection

1544
01:35:44,610 --> 01:35:47,490
the generation process

1545
01:35:47,500 --> 01:35:50,720
so it's theoretically possible to invent

1546
01:35:50,730 --> 01:35:53,030
and we need to find assumptions

1547
01:35:53,040 --> 01:35:55,420
to recover three d structure from two d

1548
01:35:59,190 --> 01:36:06,060
knowledge and context are key components for human visual system to understand images

1549
01:36:06,080 --> 01:36:08,860
so we we need to

1550
01:36:08,860 --> 01:36:11,620
try to embed in our system

1551
01:36:12,860 --> 01:36:14,490
and giants two

1552
01:36:14,570 --> 01:36:16,600
understand the context so

1553
01:36:16,610 --> 01:36:21,940
we should put priors on knowledge in in our system

1554
01:36:21,950 --> 01:36:27,630
to better understand the scene for instance if i ask you what is this

1555
01:36:27,650 --> 01:36:29,560
probably you

1556
01:36:29,610 --> 01:36:31,370
say this is the car

1557
01:36:31,380 --> 01:36:33,150
but why you

1558
01:36:33,740 --> 01:36:38,460
recognise this part of is transported

1559
01:36:38,470 --> 01:36:40,510
because you are looking probably

1560
01:36:40,520 --> 01:36:42,350
this part in the context

1561
01:36:43,120 --> 01:36:44,860
call this kind of you

1562
01:36:44,920 --> 01:36:46,410
to understand

1563
01:36:46,660 --> 01:36:48,640
what i'm about

1564
01:36:48,660 --> 01:36:52,030
is it

1565
01:36:52,110 --> 01:36:53,750
and another

1566
01:36:53,760 --> 01:36:57,960
aspect this must be taken into account

1567
01:36:57,970 --> 01:37:01,380
and the sending images is that a pixel value

1568
01:37:01,390 --> 01:37:02,920
is a result

1569
01:37:02,940 --> 01:37:04,370
from many combining

1570
01:37:04,380 --> 01:37:06,950
but this

1571
01:37:08,090 --> 01:37:12,630
an image that main child measurement of the real world

1572
01:37:16,820 --> 01:37:18,010
there are many

1573
01:37:18,030 --> 01:37:20,090
so it's ability

1574
01:37:20,100 --> 01:37:23,360
that lead

1575
01:37:24,810 --> 01:37:28,750
of the peaks you take from your fish that camera

1576
01:37:28,760 --> 01:37:31,140
and i'm going to tabulate

1577
01:37:31,200 --> 01:37:35,470
some of the major part of the human visual system can do with very well

1578
01:37:35,480 --> 01:37:38,140
and that should be taken into account

1579
01:37:39,610 --> 01:37:40,870
vision system

1580
01:37:40,880 --> 01:37:44,940
in which factor going shopping involved

1581
01:37:45,010 --> 01:37:46,620
the first thing

1582
01:37:46,620 --> 01:37:50,070
o thing that influence measurement of pixels

1583
01:37:50,070 --> 01:37:52,140
the routine way that we analyse

1584
01:37:52,610 --> 01:37:58,220
i think the potential functions so the costs of MTF

1585
01:37:58,260 --> 01:37:59,390
of s

1586
01:37:59,450 --> 01:38:01,140
it's just the summation

1587
01:38:01,180 --> 01:38:13,070
of the individual costs

1588
01:38:13,090 --> 01:38:15,320
OK by definition

1589
01:38:15,350 --> 01:38:18,070
and that is just the sum

1590
01:38:18,090 --> 01:38:19,820
michael's one two

1591
01:38:22,280 --> 01:38:24,700
the amortized cost

1592
01:38:24,700 --> 01:38:28,820
plus the

1593
01:38:28,840 --> 01:38:31,050
minus the change in potential

1594
01:38:32,640 --> 01:38:46,140
do this right now the princes in the wrong place

1595
01:38:46,160 --> 01:38:52,700
i right to

1596
01:38:52,840 --> 01:38:59,780
so this is so in the past what i did was i expressed

1597
01:38:59,800 --> 01:39:01,890
the amortized cost

1598
01:39:01,930 --> 01:39:06,110
as being equal to psi i plus the change that are destroying these two terms

1599
01:39:06,110 --> 01:39:08,620
over the other side saying what's the true cost

1600
01:39:08,660 --> 01:39:11,140
in terms of the amortized cost

1601
01:39:12,260 --> 01:39:13,070
i get

1602
01:39:13,110 --> 01:39:19,740
the i minus one last piece of ally lines one minus p of l i

1603
01:39:19,760 --> 01:39:22,070
by making the substitution

1604
01:39:23,470 --> 01:39:27,680
that's less thirty four two surfaces is a linear by now with some of the

1605
01:39:28,180 --> 01:39:31,160
ties crosses it's at most four c i

1606
01:39:32,430 --> 01:39:34,720
so the some of them is at most

1607
01:39:34,740 --> 01:39:36,470
that some

1608
01:39:36,530 --> 01:39:39,220
i equals one s

1609
01:39:39,470 --> 01:39:44,220
four c i star

1610
01:39:44,240 --> 01:39:48,510
and then this happens in all these things you get telescope

1611
01:39:48,530 --> 01:39:54,090
these terms every term is added in one subtracted out one except for

1612
01:39:54,620 --> 01:39:56,660
once to limit so i get

1613
01:39:56,680 --> 01:39:57,820
plus b

1614
01:39:57,840 --> 01:40:02,200
l zero minus p

1615
01:40:02,200 --> 01:40:05,260
of health

1616
01:40:05,280 --> 01:40:09,840
so the cardinality of s

1617
01:40:09,840 --> 01:40:14,050
and now this term is

1618
01:40:16,180 --> 01:40:19,780
and this term is

1619
01:40:19,800 --> 01:40:22,370
great inequalities zero

1620
01:40:23,390 --> 01:40:30,180
OK so therefore this whole thing is less than or equal to one what's that

1621
01:40:30,200 --> 01:40:32,620
that's just four times

1622
01:40:32,620 --> 01:40:34,180
at cost

1623
01:40:34,200 --> 01:40:47,890
so our for competitive

1624
01:40:48,260 --> 01:40:51,620
this is

1625
01:40:51,640 --> 01:40:55,180
this is amazing i

1626
01:40:55,200 --> 01:40:57,370
it's not that hard

1627
01:40:58,350 --> 01:41:00,220
but it's quite amazing

1628
01:41:00,220 --> 01:41:05,740
that just by doing this simple heuristic here nearly as good as any

1629
01:41:05,740 --> 01:41:07,220
i'm nations

1630
01:41:07,720 --> 01:41:12,030
algorithm could possibly be

1631
01:41:12,050 --> 01:41:13,890
they are nearly as good

1632
01:41:13,910 --> 01:41:18,570
in fact in practice this is great theorist server you have things like a hash

1633
01:41:18,570 --> 01:41:21,340
table the axon by changing

1634
01:41:21,390 --> 01:41:24,300
OK what often it's the case that when you access

1635
01:41:24,300 --> 01:41:28,910
the elements bring them up to the front of the last anon sorted this that

1636
01:41:28,950 --> 01:41:31,590
put an into just bring them up to the front

1637
01:41:31,590 --> 01:41:36,050
you can easily save thirty forty percent in in

1638
01:41:37,320 --> 01:41:39,140
the axis in the hash table

1639
01:41:39,180 --> 01:41:42,280
because it be much more likely to find the elements inside of course it depends

1640
01:41:42,280 --> 01:41:43,620
on the distribution

1641
01:41:43,700 --> 01:41:47,140
and so forth and you know for empirical matters but the point is that you're

1642
01:41:47,140 --> 01:41:49,160
not going to be too far off from

1643
01:41:49,220 --> 01:41:51,680
from the ordering that an optimal

1644
01:41:57,660 --> 01:41:59,570
algorithm would do

1645
01:41:59,590 --> 01:42:03,510
optimal offline algorithm really amazing

1646
01:42:03,530 --> 01:42:06,090
OK for offline

1647
01:42:06,140 --> 01:42:08,930
now it turns out that day in the reading that we

1648
01:42:08,990 --> 01:42:11,410
science we signed you

1649
01:42:11,430 --> 01:42:14,450
later in the margins of original paper

1650
01:42:14,470 --> 01:42:19,220
in that reading

1651
01:42:19,240 --> 01:42:33,240
they actually have a slightly different model where they count

1652
01:42:36,550 --> 01:42:39,890
the move

1653
01:42:39,910 --> 01:42:42,010
accessed element x

1654
01:42:42,220 --> 01:42:48,760
the front of the last

1655
01:42:48,780 --> 01:42:55,320
as for free

1656
01:43:00,950 --> 01:43:03,200
and this basically models

1657
01:43:03,220 --> 01:43:08,200
so basically is the ideas by actually have a linked list

1658
01:43:08,200 --> 01:43:12,320
then when i chased down once i find x i can actually move x up

1659
01:43:12,340 --> 01:43:14,550
to the front

1660
01:43:14,550 --> 01:43:15,370
we just

1661
01:43:15,370 --> 01:43:18,010
a constant number of pointers

1662
01:43:19,660 --> 01:43:22,470
this place it output up to the front

1663
01:43:22,490 --> 01:43:25,930
i don't actually have to transpose all the way back there

1664
01:43:25,930 --> 01:43:30,110
OK so that's kind of the model that they use more realistic models

1665
01:43:30,120 --> 01:43:33,140
OK presented this argument because of its simplicity

1666
01:43:33,160 --> 01:43:37,030
OK in the models little bit support but in their model they have accused it

1667
01:43:37,030 --> 01:43:40,550
when you access something you want to bring it up to the front or anything

1668
01:43:40,550 --> 01:43:43,240
if you happen to go across during that time you can bring up to the

1669
01:43:43,240 --> 01:43:44,280
is this a

1670
01:43:47,150 --> 01:43:50,700
does the data point i belongs to and here when you when you hear class

1671
01:43:50,700 --> 01:43:53,180
so you have to thank god OK

1672
01:43:53,190 --> 01:43:54,660
so basically

1673
01:43:54,740 --> 01:44:00,830
is basically a random variable that can take capital k values OK

1674
01:44:00,860 --> 01:44:05,180
and now we give these random variable we give it we give it a prior

1675
01:44:07,540 --> 01:44:10,110
and we call it by k

1676
01:44:10,120 --> 01:44:11,420
so pi k

1677
01:44:12,740 --> 01:44:14,990
the probability

1678
01:44:15,000 --> 01:44:19,290
so if the prior distribution over the alphabet of values that this random variable can

1679
01:44:19,290 --> 01:44:21,630
take OK so what he says is

1680
01:44:21,650 --> 01:44:25,960
a priori if i'm gonna if i'm going to generate the data points

1681
01:44:26,010 --> 01:44:28,130
which got i'm going to pick

1682
01:44:28,190 --> 01:44:29,540
and this one

1683
01:44:29,540 --> 01:44:33,470
you can see it's the distribution over over capital k values

1684
01:44:33,500 --> 01:44:34,850
it's sort of says

1685
01:44:34,870 --> 01:44:40,600
what the probability of picking the ghost in small k is OK

1686
01:44:40,640 --> 01:44:43,540
all right so once once we have that

1687
01:44:43,570 --> 01:44:45,120
if we if we write

1688
01:44:46,150 --> 01:44:48,190
the likelihood of the data

1689
01:44:48,470 --> 01:44:50,130
going to be

1690
01:44:50,140 --> 01:44:52,000
for each data point

1691
01:44:52,010 --> 01:44:57,030
i sort of first of all multiplied by the probability of choosing

1692
01:44:58,170 --> 01:45:02,530
girls in component OK there will be this i take the case one

1693
01:45:02,620 --> 01:45:06,680
and then what i do is i evaluate the probability of the data points under

1694
01:45:06,680 --> 01:45:08,690
that specific goals and OK

1695
01:45:08,790 --> 01:45:10,740
then i do it with the next one with the next one with the next

1696
01:45:10,740 --> 01:45:13,170
one and it just average overall

1697
01:45:13,190 --> 01:45:15,700
the other girls

1698
01:45:17,270 --> 01:45:23,800
so to simplify a little bit the notation although that might confuse you some more

1699
01:45:23,820 --> 01:45:25,260
capital p

1700
01:45:25,280 --> 01:45:26,610
i k

1701
01:45:26,630 --> 01:45:28,520
yet another symbol

1702
01:45:28,540 --> 01:45:30,110
is simply

1703
01:45:30,130 --> 01:45:35,010
the probability of the data points are evaluated under discussion OK

1704
01:45:35,080 --> 01:45:37,930
and each of the gaussians is going to have its own mean and its own

1705
01:45:37,930 --> 01:45:39,710
covariance matrix

1706
01:45:39,740 --> 01:45:44,750
all right

1707
01:45:44,760 --> 01:45:48,450
and now the learning problem here is we want to learn for each galaxy we

1708
01:45:48,450 --> 01:45:51,320
want to learn the mean and the covariance matrix

1709
01:45:51,330 --> 01:45:56,840
and we also want to learn the mixing proportions

1710
01:45:56,850 --> 01:46:05,030
so far if i manage going to try

1711
01:46:05,100 --> 01:46:08,400
try to show you

1712
01:46:08,460 --> 01:46:17,210
are there more reflect

1713
01:46:57,680 --> 01:47:05,920
cynicism go since moving around

1714
01:47:05,940 --> 01:47:07,490
the gaussians

1715
01:47:07,500 --> 01:47:11,070
and actually those are very very poor run i must say

1716
01:47:11,090 --> 01:47:16,010
but that's OK because we can always resort

1717
01:47:28,780 --> 01:47:33,290
exactly that's what you want to use k means

1718
01:48:09,040 --> 01:48:11,280
OK so this is just you know

1719
01:48:11,300 --> 01:48:13,460
the charm of life the most

1720
01:48:13,530 --> 01:48:21,580
anyway that's the idea you can see the initialisation is definitely

1721
01:48:21,590 --> 01:48:24,700
the problem

1722
01:48:24,720 --> 01:48:42,770
i'm going to give you one more chance

1723
01:48:46,380 --> 01:48:49,860
could have been better

1724
01:48:49,870 --> 01:48:51,950
so the reason why in here

1725
01:48:51,960 --> 01:48:57,330
there's this few yellow points and and it's mostly dominated by the green is because

1726
01:48:57,370 --> 01:48:58,550
the pike k

1727
01:48:58,550 --> 01:49:03,020
associated to the green guide is much much bigger than the pike k associated to

1728
01:49:03,020 --> 01:49:08,490
to the yellow one so although you only have a yellow component here it's it's

1729
01:49:08,490 --> 01:49:12,960
prior probability of picking it is very very small so effectively the model is using

1730
01:49:12,960 --> 01:49:16,440
three gaussians rather than for OK

1731
01:49:16,460 --> 01:49:20,880
so the points are colored according to the to the product of that k

1732
01:49:20,890 --> 01:49:24,820
and the their probability under the ghost OK OK

1733
01:49:24,830 --> 01:49:33,490
so that's sort of the the role of the pie case if you will

1734
01:49:39,490 --> 01:49:40,840
OK so how

1735
01:49:40,880 --> 01:49:43,260
can we actually

1736
01:49:43,300 --> 01:49:44,930
how can we actually learn

1737
01:49:44,950 --> 01:49:48,640
the parameters

1738
01:49:48,690 --> 01:49:52,310
any any questions

1739
01:49:52,380 --> 01:49:59,110
so remember the means we the covariance matrices and the mixing proportions all right

1740
01:50:01,610 --> 01:50:06,520
and we remember that by k is the prior probability of picking one given

1741
01:50:06,520 --> 01:50:07,970
and PIK

1742
01:50:08,030 --> 01:50:14,360
is the probability of the ice data point why i evaluated under that goes OK

1743
01:50:14,420 --> 01:50:21,160
so as usual we assume that the data are some sample independently which which doesn't

1744
01:50:21,160 --> 01:50:26,850
mean that that the components of the data points are are independent right

1745
01:50:26,900 --> 01:50:30,440
so what i what i get is a get a fairly

1746
01:50:30,450 --> 01:50:33,540
uncomfortable product of sums

1747
01:50:33,580 --> 01:50:37,270
uncomfortable because when i take the logarithm now i cannot push the algorithm all the

1748
01:50:37,270 --> 01:50:40,830
way through into the gauss and as a as i would like right

1749
01:50:40,840 --> 01:50:42,100
i hate to some

1750
01:50:42,100 --> 01:50:43,630
semidefinite cone it

1751
01:50:43,690 --> 01:50:45,560
i find set

1752
01:50:48,710 --> 01:50:54,720
so let's look at some examples in each case will for many problems like this

1753
01:50:54,780 --> 01:51:01,630
these are somewhat similar to the examples of linear programming that we saw yesterday so

1754
01:51:01,650 --> 01:51:03,720
we have problems not immediately

1755
01:51:03,740 --> 01:51:08,220
in this form but by introducing some new variables and extra constraints you can put

1756
01:51:08,220 --> 01:51:10,340
them in the standard form

1757
01:51:10,350 --> 01:51:15,450
so suppose a of x is a symmetric matrix and it's fine in some variables

1758
01:51:16,180 --> 01:51:19,260
constant matrix palestinians transonic

1759
01:51:19,290 --> 01:51:23,080
and the first problem we are interested in minimizing the maximum i value of the

1760
01:51:23,080 --> 01:51:25,990
symmetric matrix as a function of x

1761
01:51:26,040 --> 01:51:29,590
so yesterday seen as a convex function of x

1762
01:51:29,630 --> 01:51:32,910
but it wasn't clear how you actually minimize it

1763
01:51:32,950 --> 01:51:39,710
semidefinite program gives us and practically of actually solving this problem minimizing this nondifferentiable functions

1764
01:51:39,730 --> 01:51:41,260
so and this is the current is the

1765
01:51:41,870 --> 01:51:44,940
minimize an auxiliary variable t scale t

1766
01:51:44,950 --> 01:51:48,220
subject to this matrix inequality constraint

1767
01:51:48,230 --> 01:51:52,560
so this is at x the symmetric matrix is this article t times an identity

1768
01:51:53,740 --> 01:51:59,340
again this is matrix inequality means that the i minus e x is positive semidefinite

1769
01:51:59,400 --> 01:52:03,700
and these two are equivalent because if you fix x and this constraint

1770
01:52:03,710 --> 01:52:04,720
and he just

1771
01:52:04,830 --> 01:52:10,100
try to find the optimal t that minimizes the mean multi subject to these constraints

1772
01:52:10,110 --> 01:52:14,820
then this constraint really says that he is created all the ideas values of x

1773
01:52:14,830 --> 01:52:18,480
so the optimality is the maximum like value of a pixel

1774
01:52:18,490 --> 01:52:23,210
and so for fixed axis just to maximize value if you optimize of ex ante

1775
01:52:23,210 --> 01:52:25,820
jointly you're solving this problem

1776
01:52:26,050 --> 01:52:32,400
so this allows us to write a complicated nondifferentiable constraints as linear problem but we

1777
01:52:32,400 --> 01:52:36,430
have these matrix inequalities

1778
01:52:38,930 --> 01:52:41,870
based on

1779
01:52:43,130 --> 01:52:50,190
and we all know no experts have to be diagonal the optimum

1780
01:52:50,210 --> 01:52:51,790
so the

1781
01:52:51,810 --> 01:52:55,420
i values of this matrix t i minus e x

1782
01:52:55,460 --> 01:52:58,800
will be equal to t minus the values of x

1783
01:52:58,930 --> 01:53:03,580
so this means that this page is positive semidefinite if all the icons on the

1784
01:53:03,580 --> 01:53:08,420
negative so t minus the values of x are nonnegative

1785
01:53:09,850 --> 01:53:11,830
but it have to be done

1786
01:53:11,840 --> 01:53:16,170
if x is diagonal then this will be just linear inequality

1787
01:53:16,190 --> 01:53:17,840
and there will be a linear programme

1788
01:53:17,990 --> 01:53:18,700
so it's

1789
01:53:18,740 --> 01:53:21,980
quite marginal

1790
01:53:22,000 --> 01:53:24,490
so this is a similar problem we minimize

1791
01:53:24,510 --> 01:53:29,320
and on the function of the same variable x so is be transposed times inverse

1792
01:53:29,320 --> 01:53:31,240
of a times b

1793
01:53:31,300 --> 01:53:34,880
so that's a differentiable function it's nonlinear

1794
01:53:34,930 --> 01:53:37,570
and you can actually show it's convex

1795
01:53:37,580 --> 01:53:39,710
and district x to the set of

1796
01:53:39,760 --> 01:53:43,300
tables for this matrix is positive semidefinite

1797
01:53:43,320 --> 01:53:48,130
in fact it's positive semidefinite singular then actually have to explain what i mean by

1798
01:53:48,130 --> 01:53:49,460
the universe

1799
01:53:49,480 --> 01:53:54,110
but if we just ignore that and just assume that h is positive definite

1800
01:53:57,430 --> 01:53:58,700
you can write this

1801
01:54:00,490 --> 01:54:03,590
the optimisation problem again this is the p

1802
01:54:03,650 --> 01:54:07,780
by introducing new variables t and the matrix inequality constraint of this form

1803
01:54:07,800 --> 01:54:12,400
very ornament expert column the robie transpose and t in the

1804
01:54:12,440 --> 01:54:13,950
the lower right corner

1805
01:54:13,970 --> 01:54:16,930
and the reason is that if you fix a x

1806
01:54:16,940 --> 01:54:20,570
so first of all x must be positive semidefinite because it's a block of this

1807
01:54:20,570 --> 01:54:23,240
matrix so that's gives the constraint

1808
01:54:23,290 --> 01:54:26,160
fix x and you optimize over t

1809
01:54:26,210 --> 01:54:30,040
then the minimum value of t is

1810
01:54:30,060 --> 01:54:32,430
b transpose inverse to be

1811
01:54:32,450 --> 01:54:33,760
using should complement

1812
01:54:33,850 --> 01:54:39,310
and again you see these are equivalent this is nonlinear

1813
01:54:39,400 --> 01:54:44,290
defensible but quite complicated this is a linear constraints in x and t

1814
01:54:44,310 --> 01:54:46,510
because it's symmetric matrix on the left

1815
01:54:46,530 --> 01:54:49,130
and it depends linearly on x and t

1816
01:54:49,130 --> 01:54:50,980
right so it sounds quite convoluted

1817
01:54:51,440 --> 01:54:53,290
but it follows four are

1818
01:54:55,260 --> 01:54:55,910
process saying

1819
01:54:56,400 --> 01:54:57,380
situations where

1820
01:54:58,080 --> 01:54:58,960
the true model

1821
01:54:59,670 --> 01:55:00,610
is truly

1822
01:55:01,300 --> 01:55:02,980
a complex saying where

1823
01:55:03,900 --> 01:55:07,480
all you can do about it knowledge you can use from it is

1824
01:55:08,150 --> 01:55:09,370
only simulating

1825
01:55:09,970 --> 01:55:10,580
to the data

1826
01:55:12,950 --> 01:55:13,260
there are

1827
01:55:13,810 --> 01:55:15,510
again many ways of implementing

1828
01:55:15,970 --> 01:55:17,180
this principle

1829
01:55:18,460 --> 01:55:20,850
there are many choices to to calibrate the methods

1830
01:55:24,550 --> 01:55:25,120
for instance

1831
01:55:26,190 --> 01:55:31,640
once you pick up pseudo-model model you can use directly these are maximum likelihood estimator

1832
01:55:31,640 --> 01:55:35,750
r four r subsidy model that so much more likely is to mirror and from

1833
01:55:35,750 --> 01:55:36,440
there compare

1834
01:55:37,130 --> 01:55:39,330
you are bit ahead do observations we

1835
01:55:39,970 --> 01:55:40,900
beta had at

1836
01:55:41,380 --> 01:55:41,900
the sample

1837
01:55:42,380 --> 01:55:43,810
of pseudo-observations

1838
01:55:44,570 --> 01:55:45,300
or else

1839
01:55:46,270 --> 01:55:47,920
in the study way you can use

1840
01:55:48,410 --> 01:55:49,300
the score vector

1841
01:55:50,670 --> 01:55:51,460
in the same

1842
01:55:53,660 --> 01:55:54,940
now doesn't work well

1843
01:55:57,180 --> 01:55:57,880
if you look at

1844
01:55:58,790 --> 01:56:00,820
the creators of the mess grew

1845
01:56:01,460 --> 01:56:02,160
and of war

1846
01:56:03,790 --> 01:56:04,230
the of

1847
01:56:04,870 --> 01:56:05,800
rather vague about

1848
01:56:06,460 --> 01:56:10,490
that the cases wait wait words but there this interesting sentence that would

1849
01:56:11,130 --> 01:56:11,970
find again

1850
01:56:12,570 --> 01:56:13,350
in maybe see

1851
01:56:13,910 --> 01:56:15,390
namely that's of course be

1852
01:56:15,850 --> 01:56:17,760
must contain enough information

1853
01:56:18,250 --> 01:56:22,070
two recall ver the parameters theta and so in a sense within

1854
01:56:26,360 --> 01:56:26,910
must be

1855
01:56:27,530 --> 01:56:28,510
the equivalent of

1856
01:56:30,670 --> 01:56:30,990
and then

1857
01:56:32,330 --> 01:56:32,750
the get

1858
01:56:33,220 --> 01:56:37,120
to this interesting sentence at uh recovery later

1859
01:56:37,550 --> 01:56:38,160
maybe that's

1860
01:56:38,780 --> 01:56:39,700
the best case

1861
01:56:40,590 --> 01:56:42,640
if i interpret the words the best case

1862
01:56:43,090 --> 01:56:45,570
is when the parameter r

1863
01:56:46,790 --> 01:56:49,970
see is just identifying namely when

1864
01:56:50,390 --> 01:56:51,650
the model to the model

1865
01:56:52,300 --> 01:56:53,330
is parametrized

1866
01:56:54,060 --> 01:56:56,280
in the sparsest possible way

1867
01:56:56,890 --> 01:56:57,760
not to induce

1868
01:56:58,410 --> 01:56:59,920
extra dimensions that

1869
01:57:00,190 --> 01:57:00,980
would slow down

1870
01:57:01,660 --> 01:57:02,890
convergence and

1871
01:57:03,540 --> 01:57:04,940
leads to more inefficiency

1872
01:57:05,840 --> 01:57:08,680
so that's missing feature of indirect inference

1873
01:57:10,000 --> 01:57:14,790
of course there is a lot of arbitrary its justification is you can write literatur

1874
01:57:15,360 --> 01:57:17,700
is a justification of consistency

1875
01:57:18,530 --> 01:57:20,920
and the convergence of those estimators at

1876
01:57:21,470 --> 01:57:22,310
a given speed so

1877
01:57:22,830 --> 01:57:23,560
we cannot recover

1878
01:57:25,100 --> 01:57:26,980
four hour problems with a given

1879
01:57:27,560 --> 01:57:28,400
sample size

1880
01:57:29,350 --> 01:57:32,660
that there is an interesting follow-up to be indirect inference paper

1881
01:57:34,020 --> 01:57:34,860
and offered you see

1882
01:57:36,090 --> 01:57:37,670
england injuries be

1883
01:57:38,510 --> 01:57:39,730
and fore where

1884
01:57:40,280 --> 01:57:40,870
they tried to

1885
01:57:41,490 --> 01:57:51,090
have more practical evaluation and the methods by comparing choices of the models and of estimators in

1886
01:57:52,010 --> 01:57:54,240
vidio situations in the game is

1887
01:57:55,070 --> 01:57:56,010
the conclusions at

1888
01:57:56,620 --> 01:57:57,190
bit ahead

1889
01:57:58,130 --> 01:57:59,680
had to be dependency that's so

1890
01:58:00,290 --> 01:58:01,970
in statistical terms if you pick

1891
01:58:02,930 --> 01:58:05,520
and ancillary statistic is not going to help very much

1892
01:58:06,570 --> 01:58:10,630
which makes sense but the same time once you get information about sea ice that

1893
01:58:10,630 --> 01:58:13,460
is when you had varies when senior

1894
01:58:13,920 --> 01:58:14,550
there is

1895
01:58:14,990 --> 01:58:16,140
you must have

1896
01:58:17,360 --> 01:58:18,630
concentration as possible

1897
01:58:20,060 --> 01:58:23,400
the distribution of bit ahead of cedar which again makes sense because

1898
01:58:24,050 --> 01:58:25,310
the more concentrated it is

1899
01:58:26,320 --> 01:58:29,040
the easier it is to get info together

1900
01:58:29,430 --> 01:58:31,800
the relevant information about parameter it

1901
01:58:34,530 --> 01:58:35,470
there will be a talk

1902
01:58:36,670 --> 01:58:40,870
in one of maybe stations about the use of indirect inference in connection

1903
01:58:41,690 --> 01:58:44,060
these maybe see by their crystallinity

1904
01:58:46,200 --> 01:58:51,820
another project i put under the head of econometrics although it's growing debt

1905
01:58:52,630 --> 01:58:53,780
is empirical likelihood

1906
01:58:54,970 --> 01:58:59,010
and the original it's not a matter that is directly related to stimulation

1907
01:58:59,420 --> 01:59:00,410
nor is maybe see

1908
01:59:01,790 --> 01:59:03,800
but i will make the link in minutes

1909
01:59:05,210 --> 01:59:07,230
likelihood is another way to process

1910
01:59:07,860 --> 01:59:08,780
complex models

1911
01:59:10,470 --> 01:59:11,270
you don't want to make

1912
01:59:13,240 --> 01:59:14,670
the assumptions you need to

1913
01:59:16,240 --> 01:59:17,660
create a complete model

1914
01:59:19,420 --> 01:59:20,450
you introduce a parameter

1915
01:59:20,840 --> 01:59:22,590
that your primitive interest and

1916
01:59:23,220 --> 01:59:26,440
link with this parameter you define expectations

1917
01:59:27,270 --> 01:59:29,670
define the parameters so essentially

1918
01:59:30,540 --> 01:59:32,170
you introduce a parameter that's

1919
01:59:32,780 --> 01:59:37,870
you defined moments or behaviors of the data that are of interest to u and

1920
01:59:37,870 --> 01:59:39,300
you create a function of the data

1921
01:59:39,730 --> 01:59:43,980
and this parameter so that you get a five little quantity age of wine seat up

1922
01:59:44,970 --> 01:59:45,770
with expectation

1923
01:59:46,610 --> 01:59:47,910
at cedar is

1924
01:59:48,400 --> 01:59:48,730
as you

1925
01:59:50,250 --> 01:59:54,460
the french and if he is the mean otherwise it you just get a

1926
01:59:54,770 --> 01:59:57,370
you flyboys to tell you quartier here

1927
01:59:58,770 --> 02:00:02,290
and based on this definition the model which is not a model just

1928
02:00:02,820 --> 02:00:04,120
a collection of features

1929
02:00:04,760 --> 02:00:06,390
about you are your problem

1930
02:00:07,260 --> 02:00:08,830
you define the empirical likelihood

1931
02:00:11,750 --> 02:00:17,810
when you start from general that puts weights one hour end to each point in your sample and you start

1932
02:00:18,360 --> 02:00:20,460
twisting the weight so that's

1933
02:00:21,050 --> 02:00:21,830
the constraint

1934
02:00:22,710 --> 02:00:23,670
it is fulfilled

1935
02:00:24,550 --> 02:00:25,970
is satisfied and so

1936
02:00:26,550 --> 02:00:30,180
you must get some other people are age of why i see talk to be

1937
02:00:30,180 --> 02:00:33,040
equal zero which is a translation of this

1938
02:00:34,930 --> 02:00:36,020
of these constraints

1939
02:00:37,230 --> 02:00:42,930
and this is not a new this is due to a heart or in in in the late eighties

1940
02:00:45,470 --> 02:00:46,790
has not been very much

1941
02:00:49,820 --> 02:00:50,750
the bayesian approach

1942
02:00:51,580 --> 02:00:53,930
although this is coming in particular in this

1943
02:00:54,550 --> 02:00:56,490
meetings there will be an entire session

1944
02:00:57,760 --> 02:00:58,530
dedicated to

1945
02:00:59,730 --> 02:01:01,910
empirical likelihood and division approach

1946
02:01:03,560 --> 02:01:05,960
so i mean this is illustration in the case of the mean

1947
02:01:07,120 --> 02:01:10,430
you start picking the weights so that's easy

1948
02:01:10,840 --> 02:01:13,530
a weighted average of the wise you see

1949
02:01:15,710 --> 02:01:18,960
optimize the product of the ways to get the largest possible

1950
02:01:20,120 --> 02:01:21,050
empirical likelihood

1951
02:01:22,160 --> 02:01:25,760
and the more senior the father are is from

1952
02:01:26,340 --> 02:01:30,390
the true seat the one that corresponds today of the harder it is to fit

1953
02:01:30,770 --> 02:01:32,570
this constraint and therefore the smaller

1954
02:01:32,570 --> 02:01:34,950
they will talk about linear programming hand

1955
02:01:35,690 --> 02:01:36,890
according in the course on

1956
02:01:37,590 --> 02:01:38,410
monday will

1957
02:01:38,940 --> 02:01:39,600
move on to

1958
02:01:40,730 --> 02:01:41,600
convex and

1959
02:01:42,300 --> 02:01:43,810
more general nonlinear optimization

1960
02:01:49,230 --> 02:01:51,590
linear programming i guess we should start with what is it

1961
02:01:55,750 --> 02:01:58,080
so we want to maximize or minimize

1962
02:01:58,650 --> 02:02:00,230
eh linear function

1963
02:02:01,590 --> 02:02:02,670
so constraints

1964
02:02:03,110 --> 02:02:06,490
chris burgess maximizing a linear function with no constraints probably rather

1965
02:02:08,060 --> 02:02:09,940
infinity would generally be the answer and

1966
02:02:10,520 --> 02:02:11,240
order constant

1967
02:02:13,260 --> 02:02:15,850
so we have constraints and there's different ways one can

1968
02:02:16,540 --> 02:02:17,690
oppose the constraints

1969
02:02:18,690 --> 02:02:20,140
i called standard form

1970
02:02:20,610 --> 02:02:23,430
if the constraints imposed exactly like this that u

1971
02:02:23,870 --> 02:02:24,950
has some inequalities

1972
02:02:25,510 --> 02:02:28,220
linear inequalities as with this matrix eight

1973
02:02:29,530 --> 02:02:35,130
i'll try to be consistent and my notation later and it will have an rose and columns

1974
02:02:37,280 --> 02:02:39,160
so if x is less than or equal to be

1975
02:02:39,770 --> 02:02:40,930
and by less than or equal to

1976
02:02:41,440 --> 02:02:45,520
be as an analytic vector and by lesser equal to i mean that each component

1977
02:02:45,520 --> 02:02:48,330
of the to a vector on the left is smaller than or equal to

1978
02:02:48,750 --> 02:02:49,860
the component on the right

1979
02:02:50,550 --> 02:02:56,000
and also the vector x take to be nonnegative now you've may have encountered

1980
02:02:56,580 --> 02:02:59,940
linear optimization problems already in here work

1981
02:03:00,420 --> 02:03:04,380
hand they may not have been in this form but it's very easy to convert

1982
02:03:04,380 --> 02:03:09,380
from one form to another form and and i like this format this i call

1983
02:03:09,380 --> 02:03:10,160
it standard form

1984
02:03:10,680 --> 02:03:13,220
for reasons that i hope will become very clear

1985
02:03:14,280 --> 02:03:15,550
has these lectures proceed

1986
02:03:17,250 --> 02:03:20,150
so that's the problem many many applications

1987
02:03:21,010 --> 02:03:22,000
it's important course

1988
02:03:22,660 --> 02:03:25,810
uh broadly and machine learning applications

1989
02:03:28,900 --> 02:03:29,800
the first thing we gonna do

1990
02:03:30,540 --> 02:03:33,610
is have algorithm end the first step to

1991
02:03:34,260 --> 02:03:38,000
the simplex method is to introduce something which are called dictionary

1992
02:03:38,860 --> 02:03:39,570
and so

1993
02:03:40,090 --> 02:03:44,420
all i'm doing here is i'm using notation for the objective function

1994
02:03:45,530 --> 02:03:46,100
and i'm also

1995
02:03:46,670 --> 02:03:49,220
introducing slack variables that represent the

1996
02:03:50,670 --> 02:03:53,700
much this inequality is deviates from being equality

1997
02:03:54,140 --> 02:03:58,630
so w is just be minus x it's the difference between the right hand side

1998
02:03:58,780 --> 02:04:02,890
and the left hand side and so because this inequality w should be greater article

1999
02:04:04,140 --> 02:04:05,650
and i call this a dictionary

2000
02:04:06,640 --> 02:04:08,860
because the variables on the right x

2001
02:04:09,320 --> 02:04:11,590
define the variables on the left

2002
02:04:12,230 --> 02:04:15,140
and this is of course completely trivial right here but

2003
02:04:15,820 --> 02:04:20,670
we're going to modify this dictionary and the next slide and and the whole simplex

2004
02:04:20,670 --> 02:04:24,930
methods based on iteratively modifying how we represent the dictionary

2005
02:04:26,480 --> 02:04:29,880
associated with this dictionary and the subsequent ones that we're going to see

2006
02:04:30,420 --> 02:04:32,680
it is what we call a dictionary solution

2007
02:04:33,810 --> 02:04:35,870
and that's a particular r

2008
02:04:36,690 --> 02:04:38,370
potential solution to the problem

2009
02:04:39,540 --> 02:04:40,450
sort of itself

2010
02:04:41,900 --> 02:04:44,230
candidate for solution but what is called a solution

2011
02:04:44,750 --> 02:04:49,110
and with this that distinguish it from the final solution which we'll call the optimal solution

2012
02:04:49,730 --> 02:04:50,180
so that

2013
02:04:50,520 --> 02:04:51,710
dictionary solution

2014
02:04:52,710 --> 02:04:53,510
you simply get by

2015
02:04:53,990 --> 02:04:55,390
you get by simply setting the

2016
02:04:56,280 --> 02:04:57,530
x variables equal to zero

2017
02:04:58,120 --> 02:05:00,470
and rating of the w is completely trivial in this case

2018
02:05:01,010 --> 02:05:04,330
but it will become more interesting as as we iterate things

2019
02:05:06,070 --> 02:05:10,550
now when you so that a particular solution associated with this way of writing the equations

2020
02:05:11,330 --> 02:05:12,170
end if

2021
02:05:14,020 --> 02:05:16,690
o and as we do the course then w is just equal to be

2022
02:05:17,340 --> 02:05:22,660
end if these critical the zero then w critical zero and and this dictionary would

2023
02:05:22,660 --> 02:05:28,970
be feasible it satisfies the constraints of the problem might not be optimal that it

2024
02:05:29,030 --> 02:05:30,990
be is critical zero it's feasible

2025
02:05:32,630 --> 02:05:36,950
if these particular zero which makes it feasible fantasy is less than or equal to

2026
02:05:36,950 --> 02:05:41,100
zero then there's no further improvement that you can make and this dictionary solution would

2027
02:05:41,100 --> 02:05:42,360
be optimal for the problem

2028
02:05:43,340 --> 02:05:46,320
but not very many problems have the property that zero zero

2029
02:05:46,790 --> 02:05:47,990
is the optimal solution

2030
02:05:49,980 --> 02:05:50,710
and so we need to

2031
02:05:51,100 --> 02:05:52,080
iterate generally

2032
02:06:00,440 --> 02:06:02,860
see here okay so i

2033
02:06:03,600 --> 02:06:05,480
here's how the simplex methods goes

2034
02:06:06,100 --> 02:06:09,490
there's two things there's feasibility enough melody and first discuss

2035
02:06:10,290 --> 02:06:11,720
getting a feasible solution

2036
02:06:12,600 --> 02:06:13,460
sorry stuff

2037
02:06:13,960 --> 02:06:15,960
going from a feasible solution to an optimal solution

2038
02:06:16,470 --> 02:06:18,820
so the primal simplex methods

2039
02:06:19,490 --> 02:06:21,890
it refers to exactly the word feasible

2040
02:06:22,420 --> 02:06:24,930
and we are working our way toward optimally

2041
02:06:25,650 --> 02:06:29,570
so the bee in this picture would be great article zero but the sea is

2042
02:06:29,570 --> 02:06:33,770
not or less than or equal to zero and so i did that to address

2043
02:06:33,770 --> 02:06:36,230
the fact that the objective function can be improved

2044
02:06:37,850 --> 02:06:40,800
so i look at the objective function so well there's the index

2045
02:06:41,320 --> 02:06:45,530
jay for which the coefficient on the variable x j is positive

2046
02:06:45,960 --> 02:06:48,920
that i could improve things by looking variable increases

2047
02:06:50,550 --> 02:06:52,490
from the from zero which is worth

2048
02:06:52,980 --> 02:06:54,970
it is in my dictionary solution

2049
02:06:55,570 --> 02:06:57,740
so at the very will increase from zero

2050
02:06:58,410 --> 02:07:01,910
end and and will call that the entering variable

2051
02:07:03,860 --> 02:07:05,020
maybe i should say right now

2052
02:07:05,650 --> 02:07:07,850
be at the word entry refers to

2053
02:07:08,560 --> 02:07:12,340
these the variables are over here are called basic variables

2054
02:07:13,190 --> 02:07:13,790
end here

2055
02:07:14,280 --> 02:07:15,400
entering the basis

2056
02:07:16,060 --> 02:07:20,730
and these are called nonbasic variables because right now these are just the original variables and as slacks

2057
02:07:22,440 --> 02:07:24,290
but eventually the more

2058
02:07:26,550 --> 02:07:27,390
so we

2059
02:07:28,720 --> 02:07:32,220
so x enter it will become one of the variables is not zero

2060
02:07:34,260 --> 02:07:38,770
we want to preserve this property having a dictionary so if i take one other variables on the right

2061
02:07:39,230 --> 02:07:42,500
and put a number on the left and you take the labels on the left

2062
02:07:42,670 --> 02:07:43,880
and put a number on the right

2063
02:07:44,330 --> 02:07:50,930
my system of equations here so i rewrite my system of equations with one exchange swapped with one other w's

2064
02:07:51,980 --> 02:07:53,310
so how do we figure out which

2065
02:07:54,650 --> 02:07:55,660
it has to go

2066
02:07:57,010 --> 02:08:00,200
so we take this index jailer increase in a particular order

2067
02:08:01,010 --> 02:08:03,300
variable x j all the other ones are equal to zero

2068
02:08:03,990 --> 02:08:05,160
and as we increase it

2069
02:08:05,710 --> 02:08:08,380
the column of this matrix associated with

2070
02:08:09,060 --> 02:08:11,040
index we were very lax jay

2071
02:08:12,400 --> 02:08:13,980
starts to get subtracted of

2072
02:08:14,400 --> 02:08:15,520
from the vector be

2073
02:08:17,250 --> 02:08:19,930
and that be was critical zero cause are feasible

2074
02:08:20,450 --> 02:08:21,600
and eventually slow

2075
02:08:22,300 --> 02:08:25,290
one is done when the components and w will hit zero

2076
02:08:26,340 --> 02:08:27,820
he might happen right away but

2077
02:08:29,600 --> 02:08:32,390
eventually it certainly should happen end

