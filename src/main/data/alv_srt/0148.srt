1
00:00:00,000 --> 00:00:06,920
for for many problems it's hard to think of sensible real valued feature functions but

2
00:00:06,920 --> 00:00:08,250
the formalism

3
00:00:08,290 --> 00:00:09,650
allows for the

4
00:00:14,840 --> 00:00:19,360
here's my

5
00:00:19,370 --> 00:00:22,040
OK f sixteen of x and y

6
00:00:23,700 --> 00:00:25,970
the length of y

7
00:00:27,780 --> 00:00:29,120
length of x

8
00:00:29,150 --> 00:00:31,690
so that's the example of a real valued feature function

9
00:00:31,710 --> 00:00:34,890
the length of y minus the length of x

10
00:00:34,960 --> 00:00:37,690
and supposing that

11
00:00:37,700 --> 00:00:43,320
x was an english sentence and y was a french sentence

12
00:00:44,350 --> 00:00:45,930
a lot

13
00:00:45,950 --> 00:00:50,810
french sentence when you translate from english into french translation tends to

14
00:00:50,810 --> 00:00:53,200
be longer than the original

15
00:00:55,400 --> 00:00:58,580
the length of one minus length of x

16
00:00:58,620 --> 00:00:59,640
it is

17
00:00:59,650 --> 00:01:05,540
it it tends to be positive for correct translations

18
00:01:05,580 --> 00:01:09,530
and if length of y is less than ninety five x and y is probably

19
00:01:11,670 --> 00:01:15,130
if we were trying to learn to log linear model the translation

20
00:01:15,220 --> 00:01:19,550
then this is the feature function that would probably end up with a positive weight

21
00:01:19,590 --> 00:01:23,720
and this is the this is a real valued feature function

22
00:01:25,590 --> 00:01:29,370
for any application of log linear models

23
00:01:29,780 --> 00:01:32,200
including conditional random fields

24
00:01:32,220 --> 00:01:34,100
the first thing you have to do

25
00:01:35,530 --> 00:01:38,190
define all your feature functions

26
00:01:38,210 --> 00:01:39,720
and the

27
00:01:42,780 --> 00:01:45,620
then the learning process

28
00:01:45,630 --> 00:01:49,250
is to find weights for the feature functions

29
00:01:49,280 --> 00:01:50,410
so that

30
00:01:50,430 --> 00:01:54,190
when the feature function is positive and the weight is positive

31
00:01:55,280 --> 00:01:56,210
the y

32
00:01:56,240 --> 00:01:57,690
more likely

33
00:01:57,700 --> 00:02:01,940
and when when the product wj fj

34
00:02:04,700 --> 00:02:06,790
the why should be

35
00:02:06,790 --> 00:02:08,400
less likely

36
00:02:12,250 --> 00:02:13,280
let me

37
00:02:15,900 --> 00:02:18,860
try to

38
00:02:20,150 --> 00:02:21,310
a little bit

39
00:02:41,340 --> 00:02:44,410
yes yes

40
00:03:11,150 --> 00:03:18,370
i don't think there's any particular

41
00:03:20,840 --> 00:03:28,190
of having a real valued and binary feature functions versus having just binary feature functions

42
00:03:28,250 --> 00:03:34,290
more ink think it's but some problems naturally lend themselves to binary feature functions you

43
00:03:34,290 --> 00:03:37,110
may not have any real valued functions at all

44
00:03:37,130 --> 00:03:42,650
and the problems may lend themselves to both or lend themselves only two of them

45
00:03:42,650 --> 00:03:43,820
will valued

46
00:03:43,840 --> 00:03:45,650
feature functions

47
00:03:45,660 --> 00:03:48,120
my it's and

48
00:03:49,010 --> 00:03:54,290
the it just depends on the problem and i certainly wouldn't summer try to create

49
00:03:54,290 --> 00:04:05,310
feature functions of one type or the other type if they were natural

50
00:04:11,330 --> 00:04:15,210
yes so it could still

51
00:04:15,230 --> 00:04:20,220
right right

52
00:04:20,280 --> 00:04:25,640
and even the the normalisation of the model

53
00:04:25,690 --> 00:04:28,880
the normalisation of the range of the feature functions

54
00:04:28,880 --> 00:04:33,380
that's important if you're going to be doing stochastic gradient with single learning rate lambda

55
00:04:33,500 --> 00:04:35,090
single stepsize lambda

56
00:04:35,100 --> 00:04:39,170
it may be less important any user different training algorithm

57
00:04:42,890 --> 00:04:46,320
let me

58
00:04:46,320 --> 00:04:51,680
and by short i don't mean to pages but but something like seven pages eight

59
00:04:51,680 --> 00:04:54,630
pages on something that you want to write about now let me give you some

60
00:04:54,630 --> 00:04:59,880
examples just off the top of my head if you have any interest in painting

61
00:04:59,880 --> 00:05:03,720
for example would be more interesting to take to take

62
00:05:04,200 --> 00:05:05,940
look by

63
00:05:05,950 --> 00:05:11,880
impression to impressionist painters like bizarro and and and run while in the sea how

64
00:05:11,880 --> 00:05:17,330
they viewed the transformation of nineteenth-century paris the big boulevards and all of them

65
00:05:17,470 --> 00:05:20,900
or you could take another novel i mean you me now one of the interesting

66
00:05:20,900 --> 00:05:24,840
things about it is that it is the document of history you the novel

67
00:05:24,890 --> 00:05:26,720
so these are invented people

68
00:05:26,760 --> 00:05:31,660
but if the document history in some ways as is lost the great literature on

69
00:05:31,660 --> 00:05:34,160
world war one and there isn't any period in

70
00:05:34,210 --> 00:05:37,640
modern history that has so much much griping literature

71
00:05:37,650 --> 00:05:39,680
about it as great war

72
00:05:39,690 --> 00:05:44,430
the british war poets like siegfried sassoon and lies people you know we're dead after

73
00:05:44,430 --> 00:05:51,390
they wrote system wasn't immediately camera days nineteen eighteen or not but i do take

74
00:05:51,400 --> 00:05:54,150
to take some of the poetry or the writing of the war

75
00:05:54,200 --> 00:05:55,740
and writer paper by

76
00:05:55,760 --> 00:06:01,690
or three in the diplomatic history something like me consul paper reevaluating order the crimean

77
00:06:02,490 --> 00:06:06,310
and then you put you to sleep before the put your change but we don't

78
00:06:06,310 --> 00:06:09,290
you can imagine a good a good paper and that you can do whatever you

79
00:06:09,920 --> 00:06:13,920
one of the lectures and when i do the enlightenment brought from my good friend

80
00:06:13,920 --> 00:06:18,590
bob dart you know the thing at the beginning about why the line was important

81
00:06:18,590 --> 00:06:20,110
and what it is

82
00:06:20,120 --> 00:06:25,300
this is the secularisation rational inquiry and all that stuff that you may already know

83
00:06:25,300 --> 00:06:29,290
maybe not but it's in the book but then what i do is look at

84
00:06:29,290 --> 00:06:33,340
some of the the third string kind or the third division in the european football

85
00:06:33,340 --> 00:06:40,730
sense of enlightenment hacks and what they wrote about royalty about aristocrats and the way

86
00:06:40,730 --> 00:06:45,490
they can undermine the traditional hierarchies that would be swept away

87
00:06:45,540 --> 00:06:48,570
to a large extent by the french revolution

88
00:06:48,630 --> 00:06:52,330
or you could take somebody out of the french revolution such as the steely size

89
00:06:52,330 --> 00:06:57,550
used in raphael's mother silver age sixteen or something and one on the grand tour

90
00:06:57,550 --> 00:07:01,710
of france and talk about him and the committee of public safety signed away the

91
00:07:01,710 --> 00:07:06,080
lives of what people may have also saved the revolution you can do whatever you

92
00:07:06,880 --> 00:07:10,140
well you know like it should have something to do with the course in the

93
00:07:10,140 --> 00:07:14,930
time period where we're talking about i mean nothing at all on the red sox

94
00:07:14,930 --> 00:07:19,500
or something but but you have you know work with their teaching participate and i'm

95
00:07:19,500 --> 00:07:23,870
an email animal i'm always available and email and office hours well the people come

96
00:07:23,970 --> 00:07:25,170
much anymore setting

97
00:07:25,370 --> 00:07:32,480
NBA dotcom because email made office hours are oblivious i mean irrelevant not oblivious but

98
00:07:32,670 --> 00:07:36,780
people are oblivious to my office hours but anyway there

99
00:07:36,820 --> 00:07:39,370
where are they are monday's one two

100
00:07:39,380 --> 00:07:40,360
two thirty

101
00:07:40,370 --> 00:07:44,740
used to be free but i said to myself one to two thirty i in

102
00:07:44,740 --> 00:07:45,970
branford college

103
00:07:46,260 --> 00:07:48,990
k k thirteen

104
00:07:49,000 --> 00:07:53,520
there are also two other movies when we get to fascism

105
00:07:53,540 --> 00:07:55,260
we get out of it

106
00:07:55,310 --> 00:08:00,260
we and and he was only one of a whole bunch of dictators there's hardly

107
00:08:00,880 --> 00:08:07,390
parliamentary regimes left and in continental europe nineteen thirty three nine comes a woman many

108
00:08:07,390 --> 00:08:11,330
reference style who just died in two thousand two with eight hundred two

109
00:08:11,380 --> 00:08:16,130
she in when she was young woman did propaganda film for hitler and hitler like

110
00:08:16,130 --> 00:08:20,740
mussolini believed in tech is one of the first people to use the radio franklin

111
00:08:20,740 --> 00:08:26,940
roosevelt used the fireside chat the radio mussolini was already there is no piling falsehood

112
00:08:26,940 --> 00:08:31,700
upon falsehood and an italian to can barely forty e all had the radios in

113
00:08:31,700 --> 00:08:35,300
the same thing happened in germany as well and she did have a book which

114
00:08:35,300 --> 00:08:38,800
is the movie called a documentary called triumph of the will

115
00:08:38,810 --> 00:08:43,700
about nuremberg and is truly chilling is an amazing

116
00:08:43,710 --> 00:08:48,990
looks like a political convention or something in some ways i and it's all these

117
00:08:48,990 --> 00:08:53,450
movies you can see in the privacy of your your luxury suites and brentford appears

118
00:08:53,470 --> 00:08:58,660
in college or wherever because they available now in ways i don't even understand what

119
00:08:58,660 --> 00:09:01,560
i'm here under the internet we used actually show them here

120
00:09:02,280 --> 00:09:07,250
used to use movie great movie called the serendipity the ground

121
00:09:07,260 --> 00:09:12,420
and it was four hours long and and people described the two six-pack movie

122
00:09:12,440 --> 00:09:16,690
i the the generous complained get somebody robot over rattling around the course before the

123
00:09:16,690 --> 00:09:22,260
drinking water core of our was raise of course i don't show that will be

124
00:09:22,320 --> 00:09:26,420
i think that kind of thing that would help but anyway i don't i don't

125
00:09:26,420 --> 00:09:29,320
i don't show that movie anymore but but but i you should triumph of the

126
00:09:29,330 --> 00:09:34,510
will and you can watch at home and the other one is a is

127
00:09:34,530 --> 00:09:37,810
because one of the last lectures i talk about resistance

128
00:09:37,890 --> 00:09:39,140
and collaboration

129
00:09:39,150 --> 00:09:40,820
in europe

130
00:09:40,830 --> 00:09:44,080
because i live in france much of the time i i talk about france

131
00:09:44,090 --> 00:09:49,970
and what is a form some good by children some of have probably seen was

132
00:09:49,970 --> 00:09:52,950
made by the man who just died a couple years ago

133
00:09:52,990 --> 00:09:55,120
and was about when he was

134
00:09:55,140 --> 00:09:58,570
call as he was so he was equivalent to seventeen a great

135
00:09:58,580 --> 00:10:00,670
and there's a new boy shows up at school

136
00:10:00,680 --> 00:10:04,620
during world war two in fontainebleau which is just south east of paris

137
00:10:04,630 --> 00:10:05,700
and he

138
00:10:05,710 --> 00:10:07,600
a boy invented before

139
00:10:07,610 --> 00:10:09,320
and he's jewish boy

140
00:10:09,360 --> 00:10:11,730
and it's about his friendship with this boy

141
00:10:11,740 --> 00:10:12,990
and what happened

142
00:10:13,000 --> 00:10:18,590
you at the end it's not happy film this is the great great films on

143
00:10:18,640 --> 00:10:19,610
what else

144
00:10:19,630 --> 00:10:23,670
but to say there's a midterm i don't like to waste the lecture giving imagery

145
00:10:23,670 --> 00:10:26,320
rather give lecture but we have to have something

146
00:10:26,830 --> 00:10:29,090
to report to you

147
00:10:29,110 --> 00:10:32,740
if you if you to but if you don't do very well at all

148
00:10:32,750 --> 00:10:34,890
we don't count as much as do well

149
00:10:34,910 --> 00:10:39,140
i people ask these questions i know how much is it worth while images of

150
00:10:39,140 --> 00:10:43,320
is more like the great but it's it's something like twenty five percent of papers

151
00:10:43,330 --> 00:10:49,170
twenty five percent section participation is ten percent whatever we work out and then and

152
00:10:49,170 --> 00:10:50,110
then the final

153
00:10:50,190 --> 00:10:54,590
so it's just it's an exercise in seeing how you're

154
00:10:54,610 --> 00:10:59,510
how you doing it know really is is is no big deal

155
00:10:59,520 --> 00:11:01,490
it will help you pull the themes

156
00:11:02,310 --> 00:11:08,010
of course together and it's no carry situation and we all live in this sort

157
00:11:08,010 --> 00:11:10,080
of a minus b plus

158
00:11:10,130 --> 00:11:14,600
range rdf a couple years ago i ran the student

159
00:11:14,600 --> 00:11:18,510
this means make them so they don't change it since that they don't change over

160
00:11:18,510 --> 00:11:19,650
time really

161
00:11:19,820 --> 00:11:24,640
zion them the way that that once you have assigned to you i change

162
00:11:24,860 --> 00:11:29,510
a second use different UIC different things this is especially in the context

163
00:11:29,640 --> 00:11:32,370
when we talk about the page about something

164
00:11:32,380 --> 00:11:33,780
and the thing itself

165
00:11:33,810 --> 00:11:35,450
document about

166
00:11:35,500 --> 00:11:36,740
the distillery

167
00:11:36,750 --> 00:11:40,350
it's not the same as the mystery itself so two different your eyes

168
00:11:40,450 --> 00:11:41,420
so i think

169
00:11:41,430 --> 00:11:46,760
but still the URI for this solution should be possible to look it up

170
00:11:46,910 --> 00:11:50,090
still if you put it into web browser if you do an http request

171
00:11:51,300 --> 00:11:52,230
you want to

172
00:11:52,250 --> 00:11:55,000
at the end of the day get get the document

173
00:11:55,080 --> 00:11:58,670
and we see how to hold true

174
00:11:58,760 --> 00:11:59,980
how to

175
00:12:00,010 --> 00:12:03,450
combined these two words they seem to contradict each

176
00:12:04,510 --> 00:12:06,160
we don't want to we want you

177
00:12:06,170 --> 00:12:09,980
we use different URI for for the agent for the thing but still we kind

178
00:12:09,980 --> 00:12:11,070
of want to get

179
00:12:11,130 --> 00:12:12,610
two three if the page

180
00:12:12,640 --> 00:12:14,110
when we look up to

181
00:12:14,160 --> 00:12:16,140
so what we were seeing how to

182
00:12:16,150 --> 00:12:17,300
how to do this

183
00:12:17,350 --> 00:12:21,400
and last important thing URI ownership so there is the principal

184
00:12:21,640 --> 00:12:25,890
if you have registered the domain then you open despite few i space

185
00:12:25,990 --> 00:12:28,190
so don't

186
00:12:28,240 --> 00:12:34,950
don't squat URI space by kind of saying that this URI in someone else's URI

187
00:12:34,950 --> 00:12:39,340
space means the certain thing it's just something kind of that violates the social conflict

188
00:12:39,340 --> 00:12:41,950
of the web so something that you should have

189
00:12:42,270 --> 00:12:43,650
you should avoid doing

190
00:12:43,720 --> 00:12:45,650
so you have some

191
00:12:45,770 --> 00:12:46,760
URI is

192
00:12:46,770 --> 00:12:50,680
for some some some potentially guys for this

193
00:12:50,790 --> 00:12:53,140
is going to be don't want to use

194
00:12:53,300 --> 00:12:56,840
for example wikipedia dot org slash wiki first telescope

195
00:12:57,280 --> 00:13:00,410
no this is a URI of a wikipedia article is not that but i offer

196
00:13:00,420 --> 00:13:03,810
brand of whisky so it's two different things don't do this

197
00:13:03,950 --> 00:13:07,190
and at the same time it's in someone else's space so we need to be

198
00:13:08,250 --> 00:13:13,430
if they didn't if wikipedia didn't plan to make this an RDF compatible URI that

199
00:13:13,440 --> 00:13:17,380
really identifies the particular and then don't go there

200
00:13:17,510 --> 00:13:19,100
the next

201
00:13:19,120 --> 00:13:21,260
you this it's whiskey com

202
00:13:21,290 --> 00:13:23,300
brian talisker about RDF

203
00:13:23,320 --> 00:13:27,550
no this is the document the the RDF variant of discontinuity the document

204
00:13:27,650 --> 00:13:30,500
so this is the document is also not a brand of whisky

205
00:13:30,510 --> 00:13:32,100
same with the next one

206
00:13:32,170 --> 00:13:33,710
brand telescope about

207
00:13:33,760 --> 00:13:36,130
this is the generic document

208
00:13:36,190 --> 00:13:38,890
so it's not the whiskey that of different things

209
00:13:39,050 --> 00:13:40,250
and the final

210
00:13:40,510 --> 00:13:42,160
we use the i and here

211
00:13:42,170 --> 00:13:45,720
the thing is this you and it's not you can't look it up so you

212
00:13:45,720 --> 00:13:48,520
can put this into prose or if you just

213
00:13:48,670 --> 00:13:51,160
make an edge you can make a request

214
00:13:51,210 --> 00:13:53,450
to look up the definition of the i

215
00:13:53,480 --> 00:13:56,830
so it's also not a good choice

216
00:13:57,470 --> 00:14:03,230
remember generic document is this URI brand talisker about

217
00:14:03,330 --> 00:14:06,050
and now we have two approaches that

218
00:14:06,550 --> 00:14:07,960
how we can

219
00:14:08,070 --> 00:14:11,480
like two approaches that we i would say this is how we should do it

220
00:14:11,560 --> 00:14:12,710
first this

221
00:14:13,510 --> 00:14:18,030
use something called actually p three o three redirect

222
00:14:18,040 --> 00:14:19,190
here we have

223
00:14:19,290 --> 00:14:21,930
so we end up with the URI for the brand

224
00:14:21,940 --> 00:14:25,540
called wiskii com brands slash talisker

225
00:14:25,590 --> 00:14:28,030
so without the slash about india

226
00:14:28,170 --> 00:14:30,860
it is different from the URI for the page

227
00:14:31,440 --> 00:14:34,340
it fulfills this requirement is different

228
00:14:34,410 --> 00:14:37,790
and we can set up a redirect using the special

229
00:14:38,110 --> 00:14:41,230
three we HTTP status code

230
00:14:41,260 --> 00:14:42,760
this means that if

231
00:14:42,810 --> 00:14:45,320
client makes best to this visualise

232
00:14:45,380 --> 00:14:47,830
it will be redirected to another URI

233
00:14:47,920 --> 00:14:52,230
and in this case we just redirect it to the slash about the generic URI

234
00:14:52,370 --> 00:14:56,360
and this means so if put those u i if we make make you request

235
00:14:56,380 --> 00:14:58,300
to this

236
00:14:58,420 --> 00:14:59,970
to the URI of this brand

237
00:14:59,980 --> 00:15:01,190
we will end up

238
00:15:01,200 --> 00:15:05,670
at the document so we will at the end the HTML document or web browser

239
00:15:05,940 --> 00:15:09,780
or our data browser will load the RDF description

240
00:15:09,940 --> 00:15:12,600
so we have different UI so there's is no ambiguity

241
00:15:12,760 --> 00:15:17,690
between like which which thing is which things meant here by this year

242
00:15:18,480 --> 00:15:22,090
it's actually appears several state status courts

243
00:15:22,160 --> 00:15:25,750
this field he one is especially

244
00:15:26,050 --> 00:15:28,490
it's the one that should be used in this case the if you

245
00:15:28,730 --> 00:15:30,500
a redirect from

246
00:15:30,500 --> 00:15:31,700
the square

247
00:15:31,710 --> 00:15:33,610
you can write it like this

248
00:15:33,620 --> 00:15:40,020
you get traces suppose a times the easy transport vector matrix c

249
00:15:41,040 --> 00:15:45,000
this that we get a linear this is now a linear function and the variables

250
00:15:45,020 --> 00:15:48,140
policy and

251
00:15:48,150 --> 00:15:53,030
so this problem that had this hard combinatorial problems according to this

252
00:15:53,080 --> 00:15:56,060
the problem

253
00:15:56,070 --> 00:15:57,410
there have linear

254
00:15:57,430 --> 00:16:01,080
the objective function in the table constraints here

255
00:16:01,090 --> 00:16:04,990
and a very common and now all the complex is in this second

256
00:16:06,160 --> 00:16:10,300
basically this is saying that c is a one matrix but this

257
00:16:10,410 --> 00:16:13,860
small so that's a very difficult

258
00:16:13,910 --> 00:16:17,620
it has to be difficult because it equivalent to the original all the complexity now

259
00:16:17,620 --> 00:16:19,910
is in this

260
00:16:22,490 --> 00:16:25,880
using exactly equivalent so how do you obtain relaxation vol

261
00:16:26,510 --> 00:16:33,020
if i replace this equality is equal to the inequality the rhetoric you before

262
00:16:33,030 --> 00:16:35,800
and the second problem becomes very easy

263
00:16:35,810 --> 00:16:37,800
so i just place this

264
00:16:37,850 --> 00:16:39,770
constrained the inequality

265
00:16:39,960 --> 00:16:44,340
and that can also be written like this

266
00:16:44,390 --> 00:16:46,280
that's the that

267
00:16:46,360 --> 00:16:52,510
that should comply couldn't this is greater than the fifty and suppose that semantics linear

268
00:16:52,510 --> 00:16:57,070
matrix inequality everything inside here is linear in the variables in problem

269
00:16:57,200 --> 00:17:00,510
because the matrix is the variable that is able

270
00:17:00,520 --> 00:17:03,400
so this is an SDP linear objective

271
00:17:03,450 --> 00:17:07,590
having equality constraints and mechanical

272
00:17:07,730 --> 00:17:12,150
so that's the problem can so easily

273
00:17:12,230 --> 00:17:16,770
it's not equivalent to the original problem because i replaced the quality of inequality is

274
00:17:16,800 --> 00:17:19,470
to maximize over a larger set

275
00:17:19,490 --> 00:17:23,040
this problem solving

276
00:17:23,130 --> 00:17:27,020
minimizing the same cost function but over a large set so this will always give

277
00:17:27,040 --> 00:17:30,070
a lower bound on the actual problem

278
00:17:30,190 --> 00:17:34,150
that's why it's called the axis is lower bound on the action of the original

279
00:17:35,170 --> 00:17:39,680
lower bound can computed efficiently efficiently

280
00:17:39,690 --> 00:17:43,530
and sometimes just the fact of having a lower bound if it's another lower bound

281
00:17:43,530 --> 00:17:47,320
is a very hard problem can be used

282
00:17:47,330 --> 00:17:52,300
and then after solving this relaxing the excitation can also think of good ways of

283
00:17:52,300 --> 00:17:57,480
actual guessing good suboptimal solution for the original problem

284
00:17:57,490 --> 00:18:01,780
and he could do different things right for example of this is the p

285
00:18:01,830 --> 00:18:05,220
and just throw away the matrix c and keep the vector

286
00:18:05,230 --> 00:18:09,860
all the other as your this for the suboptimal solutions for problem

287
00:18:09,870 --> 00:18:11,180
you take this policy

288
00:18:11,280 --> 00:18:13,980
around the components to plus or minus one

289
00:18:14,030 --> 00:18:17,130
and i think that there are several

290
00:18:17,180 --> 00:18:22,320
another more sophisticated method that's often uses randomized rounding

291
00:18:22,370 --> 00:18:28,310
so in this method you interpret you also try to use the matrix enable computers

292
00:18:28,360 --> 00:18:31,780
i interpret this constraint as

293
00:18:31,830 --> 00:18:35,570
saying that the see effectively is the mean of the distribution

294
00:18:35,640 --> 00:18:38,120
this matrix is the is actually

295
00:18:38,250 --> 00:18:39,950
the second moments

296
00:18:39,970 --> 00:18:42,230
of the distribution

297
00:18:42,240 --> 00:18:47,150
and this is a condition that says that this this this this matrix and vector

298
00:18:47,160 --> 00:18:49,780
can be interpreted as mean and

299
00:18:49,810 --> 00:18:53,110
covariance of the second moment of the

300
00:18:53,160 --> 00:18:54,780
that's how you interpret that

301
00:18:55,000 --> 00:18:57,240
thank see in the matrix

302
00:18:57,290 --> 00:19:03,000
and i suppose we assume it's because distribution every generates random vectors from this distribution

303
00:19:03,010 --> 00:19:06,010
and then random two person months

304
00:19:06,020 --> 00:19:10,990
and then you can do that multiple times and just keep the best solution x

305
00:19:10,990 --> 00:19:13,490
is called randomized rounding

306
00:19:13,690 --> 00:19:19,980
so this is an example that occurred and the fifty five the matrix a

307
00:19:19,990 --> 00:19:24,390
in this case it would be possible to compute all possible choices

308
00:19:24,440 --> 00:19:30,590
then we solve the SDP relaxation and so we normalize

309
00:19:30,600 --> 00:19:34,980
for example axis here so that one is the lower bound on the

310
00:19:35,090 --> 00:19:38,080
that's a lower bound on the podium

311
00:19:38,180 --> 00:19:39,670
the problem

312
00:19:39,680 --> 00:19:43,750
so i'm not sure that the optimum is because it's difficult to compute the it's

313
00:19:43,750 --> 00:19:46,470
of less than one

314
00:19:46,520 --> 00:19:53,050
then another heuristic would be to just solve the least squares problem without the peak

315
00:19:53,100 --> 00:19:58,860
border constraints solve the discrete problem and around solution might give this

316
00:20:00,130 --> 00:20:05,900
and this histogram is the result of the randomized rounding applied to the solution of

317
00:20:05,910 --> 00:20:08,250
the SDP relaxation

318
00:20:08,300 --> 00:20:10,830
we generate random vectors x

319
00:20:10,870 --> 00:20:14,730
some all of them are much better than to find the best solution

320
00:20:14,810 --> 00:20:17,750
so you can tell about this but then is that the global optimum of the

321
00:20:17,750 --> 00:20:19,980
border problem by some

322
00:20:28,230 --> 00:20:34,000
back then

323
00:20:34,340 --> 00:20:39,470
then have another example of an SDP approach probably skip this

324
00:20:39,480 --> 00:20:42,230
it's about expanding

325
00:20:42,280 --> 00:20:45,610
the standard chebyshev inequalities probability

326
00:20:45,660 --> 00:20:51,130
well known classical inequality would be you have greater than the table x

327
00:20:51,150 --> 00:20:53,550
with zero mean given variance

328
00:20:55,020 --> 00:20:59,110
the classical chebyshev inequality gives a lower bound on the probability of x having that

329
00:21:01,870 --> 00:21:03,600
one minus the of

330
00:21:03,610 --> 00:21:10,350
and it's tight bound exist distributions that have achieved or

331
00:21:10,390 --> 00:21:14,800
so tried to explain possible extension of this would be followed first u

332
00:21:14,860 --> 00:21:19,230
extended from the scalar variable x to factor variable x

333
00:21:19,280 --> 00:21:21,050
with unknown mean

334
00:21:23,100 --> 00:21:24,900
the second moment

335
00:21:24,950 --> 00:21:30,110
suppose that so you know about the distribution mean the first and second moments

336
00:21:30,150 --> 00:21:34,710
and then instead of just an upper bound on the scalar x here we look

337
00:21:34,710 --> 00:21:39,490
at a set defined by quadratic inequality

338
00:21:39,500 --> 00:21:45,040
not that could be indefinite i to be convex set of quadratic inequality

339
00:21:45,090 --> 00:21:48,280
and i want to lower bound for probability

340
00:21:48,290 --> 00:21:52,080
that x lies in this set

341
00:21:52,090 --> 00:21:56,870
and that sort of generalized chebyshev inequality you're bounding the probability that it has a

342
00:21:56,870 --> 00:21:58,400
palace one

343
00:21:58,410 --> 00:22:03,510
here in the probability that the fact that x lies in this set described by

344
00:22:03,520 --> 00:22:04,940
thank you

345
00:22:07,580 --> 00:22:09,170
credit card

346
00:22:09,190 --> 00:22:13,480
well that's probably can actually can find the bound by solving is the

347
00:22:13,490 --> 00:22:20,410
and again it's five and are distributions that achieve that

348
00:22:20,710 --> 00:22:24,870
the example of this

349
00:22:24,880 --> 00:22:28,550
to just to finish this part of con programming we talked about the

350
00:22:28,560 --> 00:22:32,260
second order cone semidefinite programming

351
00:22:32,270 --> 00:22:39,470
so actually reflects the change in how people view convex optimisation since the nineteen nineties

352
00:22:39,480 --> 00:22:44,180
so the more traditional view of convex optimization is that it's a special case of

353
00:22:44,180 --> 00:22:46,370
linear programming

354
00:22:46,420 --> 00:22:50,350
and united in the general form minimizing fcx the picture and i think this is

355
00:22:51,680 --> 00:22:56,050
a special case of linear programming that con functions happen to be convex so you

356
00:22:56,050 --> 00:22:58,160
have some additional interesting theory

357
00:22:58,770 --> 00:23:02,700
so in this core programming notation you actually and as i said that the

358
00:23:04,230 --> 00:23:05,960
articles related to LP

359
00:23:05,970 --> 00:23:10,380
right it almost like an LP except that using non non-political inequality

360
00:23:10,430 --> 00:23:12,640
two tropical rain

361
00:23:12,640 --> 00:23:18,860
lp norms and one and so on the popular in russia

362
00:23:18,980 --> 00:23:21,120
a different set of axioms

363
00:23:21,160 --> 00:23:23,600
surprise different set of axioms

364
00:23:23,700 --> 00:23:26,800
no cases you need seven

365
00:23:26,820 --> 00:23:32,770
i will mention the culprit of this

366
00:23:32,780 --> 00:23:37,610
the modified distances what i mean by this is an example of where things start

367
00:23:37,620 --> 00:23:38,910
to get out

368
00:23:38,960 --> 00:23:44,720
there is an example of a fortified that was going to fly command number four

369
00:23:44,730 --> 00:23:46,270
which is embedded in the

370
00:23:46,280 --> 00:23:47,500
this inspired

371
00:23:47,550 --> 00:23:54,790
because of course there a lot of color transfer is often involve

372
00:23:54,800 --> 00:23:59,240
it was an evaluation

373
00:23:59,280 --> 00:24:04,550
if you look at this particular

374
00:24:04,570 --> 00:24:06,320
colouring of the scheme

375
00:24:06,370 --> 00:24:10,960
the shortest distance between two points

376
00:24:11,020 --> 00:24:16,460
is all the decision to appoint is in two cases in the in the same

377
00:24:17,620 --> 00:24:18,870
or in two

378
00:24:18,880 --> 00:24:21,600
that's going to far more

379
00:24:21,620 --> 00:24:26,520
so this is the green line is the shortest distance between two content of our

380
00:24:30,170 --> 00:24:34,950
so if s is the length of the edge of the

381
00:24:34,990 --> 00:24:37,650
hexagonal tiling

382
00:24:40,550 --> 00:24:46,200
x minus one of the most tool as if x y are in the same

383
00:24:46,250 --> 00:24:47,120
this is

384
00:24:47,150 --> 00:24:48,910
five was

385
00:24:48,960 --> 00:24:51,780
angle of one hundred twenty degrees

386
00:24:51,790 --> 00:24:54,340
and it would be greatly full then

387
00:24:54,380 --> 00:24:59,610
seven as if x y have same color with the financial

388
00:24:59,620 --> 00:25:02,440
that means we have a language with s

389
00:25:02,450 --> 00:25:04,100
they would still give us

390
00:25:04,120 --> 00:25:06,900
a because of the plane with seven followers

391
00:25:07,040 --> 00:25:09,660
so now we can take our graphs and

392
00:25:09,730 --> 00:25:13,640
inflate them

393
00:25:13,660 --> 00:25:18,110
and that is as long as as between these two numbers get caught the this

394
00:25:18,110 --> 00:25:21,620
is seven cars

395
00:25:21,630 --> 00:25:24,110
and this is the study that was

396
00:25:24,120 --> 00:25:27,730
we generated by jeff x two

397
00:25:27,770 --> 00:25:33,420
and yes what that just relaxing that have more edges what we

398
00:25:34,120 --> 00:25:39,610
an edge between two vertices if the distance between two points is between one one

399
00:25:39,610 --> 00:25:40,790
y that

400
00:25:40,850 --> 00:25:43,500
in one class

401
00:25:43,590 --> 00:25:47,250
and this is where it gets really they managed to produce a graph with at

402
00:25:47,250 --> 00:25:49,700
least two hundred seven votes is

403
00:25:49,710 --> 00:25:52,160
and with exhaustive computer searches

404
00:25:52,220 --> 00:25:54,620
claim that is seven com

405
00:25:59,420 --> 00:26:03,870
we started

406
00:26:03,870 --> 00:26:08,120
a different variation and this is what i call the distance one

407
00:26:08,140 --> 00:26:14,540
this was done by an international school

408
00:26:22,250 --> 00:26:24,140
i'm not good at

409
00:26:24,160 --> 00:26:28,950
highway authorities are quite stunning to appear me as if you

410
00:26:28,960 --> 00:26:31,670
the other one being

411
00:26:31,700 --> 00:26:32,990
this that's whole

412
00:26:33,000 --> 00:26:36,300
two something i do not know how to

413
00:26:36,330 --> 00:26:38,210
despite either

414
00:26:38,260 --> 00:26:41,130
slovak and they all canadian smallpox

415
00:26:41,140 --> 00:26:44,870
i don't know if these two

416
00:26:47,540 --> 00:26:50,540
again american-israeli always the american

417
00:26:50,590 --> 00:26:58,010
and simoncelli whose israel israeli if we we're seeing jerusalem

418
00:26:58,020 --> 00:27:02,330
OK so it's the quick description of the this is what

419
00:27:02,880 --> 00:27:04,610
the six distance is

420
00:27:04,620 --> 00:27:10,250
by four points in the plane cannot be altered

421
00:27:10,290 --> 00:27:14,510
this is the problem the putnam exam in nineteen ninety two

422
00:27:15,590 --> 00:27:16,720
there was more

423
00:27:16,730 --> 00:27:22,070
general observation that in the paper by one what was what the as follows

424
00:27:22,340 --> 00:27:26,840
the maximum number of points of these such that all distances among them is an

425
00:27:26,840 --> 00:27:28,490
odd integer

426
00:27:28,570 --> 00:27:30,200
is the last slide

427
00:27:30,240 --> 00:27:31,700
unless there is

428
00:27:31,710 --> 00:27:37,860
fourteen months sixty o in dimension fourteen you can actually have two simplices

429
00:27:38,010 --> 00:27:40,510
actually each other will the distances

430
00:27:40,550 --> 00:27:44,120
the two surfaces are each have edges point nine

431
00:27:44,130 --> 00:27:46,430
and then these

432
00:27:46,430 --> 00:27:48,940
connecting the two in

433
00:27:48,960 --> 00:27:52,040
eight says if distance five

434
00:27:54,040 --> 00:27:59,210
using an elementary that's for this once short very nicely only four points

435
00:27:59,250 --> 00:28:01,550
so one of them is the origin

436
00:28:01,740 --> 00:28:05,050
this is the old cosine inequality

437
00:28:05,070 --> 00:28:09,310
and you let me stop for a second and i'm going to use it

438
00:28:09,510 --> 00:28:12,710
assume that all the instances of

439
00:28:12,740 --> 00:28:13,650
that means

440
00:28:13,660 --> 00:28:18,880
you might be square this for all the integers therefore this square is one with

441
00:28:20,410 --> 00:28:21,490
and therefore

442
00:28:21,510 --> 00:28:28,440
minus two times you've is also one of the one with a probability model

443
00:28:28,480 --> 00:28:31,020
and then you form my i

444
00:28:31,040 --> 00:28:33,820
say we actually make links of these

445
00:28:33,830 --> 00:28:34,990
three that forest

446
00:28:35,000 --> 00:28:41,000
so this is clearly a flag most to the the

447
00:28:41,060 --> 00:28:43,580
and use this one eight and you get

448
00:28:43,630 --> 00:28:48,330
the matrix and the determinant of this matrix is for an clearly deal with a

449
00:28:48,390 --> 00:28:50,010
and this is the country

450
00:28:50,060 --> 00:28:51,180
so not all

451
00:28:52,480 --> 00:28:54,220
can be an odd integer

452
00:28:54,230 --> 00:28:58,590
actually this is the same thing as in the unity since the unit distance graph

453
00:28:58,600 --> 00:28:59,940
can the full

454
00:28:59,950 --> 00:29:00,990
but this is

455
00:29:01,010 --> 00:29:04,490
the form of a four because north pole

456
00:29:04,510 --> 00:29:09,850
six this is equal to one

457
00:29:09,900 --> 00:29:16,050
so in nineteen ninety four of i ask for the query was whether he can

458
00:29:16,050 --> 00:29:18,590
be called in a finite number of features

459
00:29:18,610 --> 00:29:23,750
so the two points of in the this instead this thing colours

460
00:29:23,860 --> 00:29:29,510
obviously the all these four because already for this one needs

461
00:29:29,560 --> 00:29:34,400
she made the input is only

462
00:29:35,080 --> 00:29:40,730
classical ask what is the maximum number of the distances my

463
00:29:40,810 --> 00:29:42,130
and points

464
00:29:42,130 --> 00:29:45,170
in the plane

465
00:29:49,810 --> 00:29:53,900
we can easily have an upper bound for the maximum number of distances both for

466
00:29:53,900 --> 00:29:57,010
the unit distance and four

467
00:29:57,030 --> 00:29:58,570
the or distance

468
00:29:58,580 --> 00:29:59,780
because we know

469
00:29:59,790 --> 00:30:03,000
you have an influence

470
00:30:03,070 --> 00:30:07,310
this is a graph on n vertices that does not contain the k four

471
00:30:07,360 --> 00:30:09,380
so we can use to still

472
00:30:09,420 --> 00:30:13,320
and we have an upper bound immediately on the number of distances before and by

473
00:30:13,320 --> 00:30:15,840
the way is to numbers

474
00:30:15,840 --> 00:30:20,780
so i'm going to the local economy i don't live is that

475
00:30:22,010 --> 00:30:23,010
yeah good

476
00:30:23,020 --> 00:30:28,760
i want the locals simon godsill and signal processing here in the engineering department the

477
00:30:28,760 --> 00:30:32,180
same department as zoubin

478
00:30:32,200 --> 00:30:37,370
well i don't mind that but it's not

479
00:30:37,390 --> 00:30:43,870
OK i going to be talking about sequential monte carlo methods all particle filtering methods

480
00:30:43,870 --> 00:30:47,380
it's in two sessions the first session today

481
00:30:47,390 --> 00:30:52,490
it's pretty much a basic session so people that already know about particle filters will

482
00:30:52,490 --> 00:30:58,000
probably not very much about this will have maybe some insights perhaps they can point

483
00:30:58,010 --> 00:30:58,940
out their own

484
00:30:58,960 --> 00:31:02,640
views on particular things i say feel free to

485
00:31:02,660 --> 00:31:06,450
ask me questions on point things out along the way within reason

486
00:31:09,430 --> 00:31:16,590
the application area domain within is where we're looking at large sequentially evolving datasets we

487
00:31:16,590 --> 00:31:17,570
don't necessarily want to do

488
00:31:18,010 --> 00:31:22,360
an inference in batch learning a lot of time shining through all of the data

489
00:31:22,760 --> 00:31:23,980
in one go

490
00:31:24,000 --> 00:31:31,250
and clearly there are vast numbers of applications that fall into this category sequential data

491
00:31:31,260 --> 00:31:32,720
evolving data

492
00:31:32,730 --> 00:31:40,410
tracking is where this methodology probably first started out with the is organised in earnest

493
00:31:40,870 --> 00:31:44,980
in the nineteen nineties with the seminal paper by new gordon

494
00:31:45,040 --> 00:31:53,920
and his collaborators simon smith that says adrian smith great bayesian nearest

495
00:31:53,970 --> 00:31:59,210
but also in computer vision i start up around about the same time with andrew

496
00:31:59,210 --> 00:32:08,160
blake work and various other applications where these things are used routinely nowadays

497
00:32:08,200 --> 00:32:11,540
so basically

498
00:32:11,560 --> 00:32:14,990
as you expect in machine learning

499
00:32:15,010 --> 00:32:22,210
workshop summer school remaining going to be distributing with dealing with probability distributions and uncertainty

500
00:32:22,440 --> 00:32:28,530
so we'll be looking for ways to take a bayesian approach to sequential updating of

501
00:32:28,530 --> 00:32:34,250
probability distributions as the data evolve and in particular the probability distributions are some latent

502
00:32:34,970 --> 00:32:39,210
state variables that we want to learn as they evolve

503
00:32:41,010 --> 00:32:47,190
obviously for this talk focusing on the sequential may make money monte carlo methodology

504
00:32:47,230 --> 00:32:52,370
i think that's synonymous with particle filtering i don't think there are any differences between

505
00:32:52,370 --> 00:32:58,770
those although there may be some subtle interpretations there are numerous papers around about this

506
00:32:58,770 --> 00:33:03,020
i'm just pointing to a handful of them here

507
00:33:03,030 --> 00:33:09,460
that's gordon salmond and smith was the first in recent times although there are papers

508
00:33:09,460 --> 00:33:14,310
going back to the nineteen sixties and and and before where people were using sequential

509
00:33:14,390 --> 00:33:20,080
importance sampling of various types and

510
00:33:20,090 --> 00:33:25,690
and then these other papers are sort of review papers that that involved myself but

511
00:33:25,690 --> 00:33:29,990
there are numerous other papers in this area you took out

512
00:33:30,010 --> 00:33:38,540
so we're looking for estimating or finding the posterior probability distributions for some hidden process

513
00:33:38,540 --> 00:33:42,290
which we call the state of the system and they results have be obtained from

514
00:33:42,540 --> 00:33:50,000
noisy convolved or non linearly distorted observations particularly interested in this methodology with things that

515
00:33:50,000 --> 00:33:57,120
don't limit the class of functions and nonlinear processes nongaussian processes that we can deal

516
00:33:57,120 --> 00:33:59,920
with so that they in their final form

517
00:33:59,930 --> 00:34:06,390
a very generic they can work with pretty well any nonlinear state space model stochastic

518
00:34:06,390 --> 00:34:10,520
state space model if you throw at it at least in principle it may take

519
00:34:10,930 --> 00:34:14,930
a lot of monte carlo samples to do so in principle they can do that

520
00:34:14,940 --> 00:34:19,260
you'll find that some of the more tailored algorithms the things that have come about

521
00:34:19,260 --> 00:34:21,050
over the last ten years or so

522
00:34:21,070 --> 00:34:27,100
research may then be directed more towards specific classes of model for example

523
00:34:27,110 --> 00:34:31,410
a major classes model that's been a a big success with these methods is that

524
00:34:31,410 --> 00:34:33,070
those which have some partial

525
00:34:33,170 --> 00:34:38,130
the gaussians structure so the bit about model because the model which you can handle

526
00:34:38,140 --> 00:34:44,220
using a standard kalman filtering methodology and you should do that if you're effectively marginalizing

527
00:34:44,220 --> 00:34:45,850
that the posterior

528
00:34:45,870 --> 00:34:50,910
probability sense from the equations and then you just do the hard particle filtering part

529
00:34:50,940 --> 00:34:55,290
on the non linear nongaussian that's left behind so that's an example of a case

530
00:34:55,810 --> 00:35:00,080
they are very tailored methods for a particular class of models the most of what

531
00:35:00,080 --> 00:35:05,600
the understanding was that atoms were the most basic constituent of matter meaning you could

532
00:35:05,620 --> 00:35:09,410
break adams up into anything smaller that was that you're done

533
00:35:09,460 --> 00:35:15,540
and with using newtonian mechanics it was assumed sense this this type of mechanics worked

534
00:35:15,540 --> 00:35:20,090
so well to describe everything we could see it even describe the universe planet that

535
00:35:20,090 --> 00:35:25,180
of course we can use newtonian mechanics to describe how an electron

536
00:35:25,240 --> 00:35:29,400
actually we didn't even know what electron here but how atoms behaved and it turns

537
00:35:29,400 --> 00:35:31,730
out this is not the case

538
00:35:31,740 --> 00:35:34,980
and the first step in and discovering this is not the case

539
00:35:35,000 --> 00:35:42,390
was accomplished by JJ thomson and JJ thomson is credited for discovering the electron

540
00:35:43,110 --> 00:35:48,280
it was a physicist in england and what his laboratory was studying is something called

541
00:35:48,280 --> 00:35:49,910
cathode rays

542
00:35:49,970 --> 00:35:55,480
in cathode rays are simply rays that are emitted when you have a higher electron

543
00:35:55,520 --> 00:35:57,220
a high voltage difference

544
00:35:57,350 --> 00:36:00,220
between two electrodes so

545
00:36:00,230 --> 00:36:02,030
if you look at this set up

546
00:36:02,040 --> 00:36:06,050
what he did when he was studying is raised as he had been evacuated to

547
00:36:06,100 --> 00:36:07,930
which is schematically shown here

548
00:36:08,340 --> 00:36:13,910
were evacuated of all air field instead just with hydrogen gas and he had this

549
00:36:13,920 --> 00:36:18,340
high voltage difference with between anode and the cathode and he actually put a little

550
00:36:18,340 --> 00:36:23,190
hole in and out here so these cathode rays that were produced could shoot out

551
00:36:23,190 --> 00:36:29,030
of the cathode and actually could be detected the luminescence by spot on the detector

552
00:36:30,120 --> 00:36:34,970
so what people were studying at the right time one reason they actually gave us

553
00:36:35,000 --> 00:36:39,280
the bright glow if you put them in an evacuated glass tube about these crazy

554
00:36:39,280 --> 00:36:41,500
patterns and glowing colours though

555
00:36:41,530 --> 00:36:45,930
after that even with very high issue in terms of research

556
00:36:45,980 --> 00:36:50,670
but also no one really knew what these were and thompson was seeking to figure

557
00:36:50,670 --> 00:36:55,500
out some more properties of them and he had the theory that maybe they were

558
00:36:55,500 --> 00:37:00,670
actually charged particles of some sort and others have proposed this in the past but

559
00:37:00,670 --> 00:37:05,670
they didn't really have an experimental setup to test and that's what thompson dead

560
00:37:05,670 --> 00:37:08,290
and what he did was he put two detection played

561
00:37:08,290 --> 00:37:11,080
on either side of the cathode rays

562
00:37:11,100 --> 00:37:12,640
and when he

563
00:37:12,650 --> 00:37:16,170
put a voltage difference between the two plates he wanted to see if we could

564
00:37:16,170 --> 00:37:19,860
actually bend the range and tested are actually charged and i

565
00:37:19,870 --> 00:37:23,290
so when the voltage difference between the plates is zero

566
00:37:23,340 --> 00:37:27,350
when we just don't have the place they're all those that cathode rays are not

567
00:37:27,350 --> 00:37:30,780
bent they just go right in a straight line and they can be detected on

568
00:37:30,780 --> 00:37:33,190
the screen

569
00:37:33,250 --> 00:37:38,340
when he actually cranked up the voltage between these two players what he saw was

570
00:37:38,340 --> 00:37:42,620
really moving to him which is that he actually was able to bend these races

571
00:37:42,620 --> 00:37:48,480
had never been observed before in any capacity and able to detect on screen that

572
00:37:48,480 --> 00:37:53,030
there was this deflection and you could even measure of the degree of deflection that

573
00:37:53,030 --> 00:37:53,990
he had

574
00:37:54,070 --> 00:37:59,530
so we know now that we have charged particles are these negatively or positively charged

575
00:37:59,530 --> 00:38:01,880
based on the evidence

576
00:38:01,890 --> 00:38:03,220
yeah that's right so

577
00:38:03,270 --> 00:38:06,270
so what we have here cathode rays we now know

578
00:38:06,280 --> 00:38:09,340
are negatively charged particles

579
00:38:09,400 --> 00:38:11,510
and in fact

580
00:38:11,580 --> 00:38:17,140
he had been this negatively charged particles if anyone know what he named them

581
00:38:17,180 --> 00:38:24,010
no not electrons very good captain corpus of has only one part of corpus so

582
00:38:24,050 --> 00:38:26,160
little then yes o

583
00:38:26,170 --> 00:38:30,050
it was later named that these particles were in fact electrons and that's what they

584
00:38:30,750 --> 00:38:35,790
i did something continued to call the proposals for many many many years after everyone

585
00:38:35,790 --> 00:38:39,550
else called the electrons but i'm sure no one minded because he did in fact

586
00:38:39,550 --> 00:38:40,590
discover them

587
00:38:40,650 --> 00:38:42,160
and was actually

588
00:38:42,180 --> 00:38:45,950
able to find out more than just that these were charged

589
00:38:45,960 --> 00:38:51,540
in classical electromagnetism you could actually relate the degree of deflection that he saw to

590
00:38:51,540 --> 00:38:55,200
the charge and the mass of the particles

591
00:38:55,220 --> 00:38:57,560
so using that you could say

592
00:38:57,570 --> 00:39:01,870
that act and we'll put some negative because we know now that these are negative

593
00:39:04,470 --> 00:39:05,940
is proportional

594
00:39:07,910 --> 00:39:09,380
the charge

595
00:39:09,410 --> 00:39:11,540
on particle

596
00:39:11,580 --> 00:39:15,210
over which is the mass so we have

597
00:39:15,220 --> 00:39:20,050
e being equal to the charge of the negative particles

598
00:39:20,140 --> 00:39:22,360
and of course

599
00:39:22,410 --> 00:39:24,240
it equal to the mass

600
00:39:24,290 --> 00:39:27,160
of those particles

601
00:39:27,870 --> 00:39:30,050
thompson didn't stop here

602
00:39:30,060 --> 00:39:35,830
he actually continued experimenting with different voltages

603
00:39:35,840 --> 00:39:41,400
and what we found was if he really really ran the voltage up between those

604
00:39:41,400 --> 00:39:42,370
two play

605
00:39:42,390 --> 00:39:47,490
you can actually detect something else and what he could detect here is that there's

606
00:39:47,490 --> 00:39:48,380
this little

607
00:39:48,790 --> 00:39:52,700
one of them and after that you can see on the screen that was barely

608
00:39:52,710 --> 00:39:55,510
deflected at all certainly comparison

609
00:39:55,540 --> 00:40:01,580
how strongly this first particle is deflected the second particle is deflected almost not at

610
00:40:01,580 --> 00:40:04,110
all but what can tell

611
00:40:04,140 --> 00:40:07,210
from the fact that there was a second part going on the fact that it

612
00:40:07,210 --> 00:40:11,770
was in this direction is that in addition to his negative particle he also of

613
00:40:11,770 --> 00:40:16,650
course had a positive particle that was with the stream of rays that are coming

614
00:40:18,460 --> 00:40:22,510
so course he can use the same relationship for the positive particle

615
00:40:22,520 --> 00:40:24,300
so delta act

616
00:40:24,320 --> 00:40:26,520
now of the positive

617
00:40:26,560 --> 00:40:28,120
is proportional

618
00:40:28,720 --> 00:40:31,410
the charge on the positive particle

619
00:40:31,420 --> 00:40:33,200
all over

620
00:40:33,220 --> 00:40:36,080
the mass of the positive particle

621
00:40:36,080 --> 00:40:38,780
derivative of the mean squared error loss

622
00:40:38,840 --> 00:40:41,840
is just the inner product of

623
00:40:42,850 --> 00:40:44,670
and the

624
00:40:44,730 --> 00:40:48,370
the area between t and put the model predicts

625
00:40:48,400 --> 00:40:49,560
and then we have

626
00:40:49,560 --> 00:40:51,710
some ten here

627
00:40:51,940 --> 00:40:56,320
so we want to say this to zero and hopefully solve for w

628
00:40:56,360 --> 00:40:57,370
so again

629
00:40:57,390 --> 00:41:00,460
for those of you who are not familiar with this sort of notation and the

630
00:41:00,590 --> 00:41:03,550
manipulation of

631
00:41:03,590 --> 00:41:09,010
derivatives out of calculus vector calculus then it's it's a good idea to become familiar

632
00:41:09,010 --> 00:41:12,980
with that is going to be doing a serious machine learning

633
00:41:13,210 --> 00:41:17,870
but before we go ahead we just need to satisfy ourselves that this is

634
00:41:17,870 --> 00:41:21,390
the stationary point is the minimum

635
00:41:21,400 --> 00:41:22,960
and again

636
00:41:22,980 --> 00:41:25,400
if you remember

637
00:41:25,410 --> 00:41:26,970
you're still

638
00:41:27,020 --> 00:41:31,260
calculus then for single variable

639
00:41:31,280 --> 00:41:33,810
the second derivatives at stationary points

640
00:41:33,860 --> 00:41:35,500
i have to be strictly positive

641
00:41:35,510 --> 00:41:40,260
four that stationary point to the minimum of the function

642
00:41:40,450 --> 00:41:44,980
and the multiparameter generalisation is that

643
00:41:45,110 --> 00:41:50,360
the matrix of partial derivatives partial second derivatives are called h

644
00:41:50,530 --> 00:41:52,870
requires to be positive definite

645
00:41:52,870 --> 00:41:54,610
so in other words if we take any

646
00:41:55,520 --> 00:42:02,820
vector eight then the the inner product does this quadratic term here always has to

647
00:42:07,900 --> 00:42:09,870
we typically this after this

648
00:42:09,870 --> 00:42:15,320
matrix is the haitian matrix so we need the expression for that so again we

649
00:42:16,300 --> 00:42:20,150
just do some more schoolboy calculus

650
00:42:20,170 --> 00:42:24,200
and you can stack all of these second derivatives

651
00:42:24,250 --> 00:42:26,150
in two are

652
00:42:26,230 --> 00:42:30,210
a two by two matrix in this case a more general case of the parameters

653
00:42:30,230 --> 00:42:32,270
b d by d

654
00:42:32,270 --> 00:42:37,980
and i'll leave it as an exercise for you you can see that each of

655
00:42:37,980 --> 00:42:39,090
these elements

656
00:42:39,110 --> 00:42:47,330
basically corresponds to a constant some of the actual values of attributes in india in

657
00:42:47,540 --> 00:42:49,820
in these

658
00:42:49,840 --> 00:42:50,780
elements here

659
00:42:50,790 --> 00:42:56,950
and then this failed diagonal term is the sum of the squared attribute values

660
00:42:56,970 --> 00:43:02,280
again we can write this matrix we can write this

661
00:43:02,280 --> 00:43:05,200
matrix of second derivatives in

662
00:43:05,220 --> 00:43:07,960
matrix format and it turns out to be

663
00:43:07,980 --> 00:43:10,220
transpose x

664
00:43:11,000 --> 00:43:12,090
this is actually

665
00:43:12,130 --> 00:43:14,940
a very important

666
00:43:14,950 --> 00:43:18,460
matrix and i'll talk a little bit more about it later

667
00:43:18,470 --> 00:43:22,580
and the interesting thing is that

668
00:43:23,190 --> 00:43:27,150
the target values don't appear in here at all the only thing that appears of

669
00:43:27,170 --> 00:43:32,590
attribute values so we have this and a product of the the attribute values will

670
00:43:32,590 --> 00:43:33,680
return to this

671
00:43:33,690 --> 00:43:35,290
later on

672
00:43:36,170 --> 00:43:40,850
x transpose it can be inverted

673
00:43:40,890 --> 00:43:41,520
can be

674
00:43:42,670 --> 00:43:45,090
it will be positive definite

675
00:43:46,480 --> 00:43:53,150
providing that in where n is the number of observations is greater than d

676
00:43:53,160 --> 00:43:54,110
where do you

677
00:43:54,110 --> 00:43:55,870
is the number of parameters

678
00:43:56,760 --> 00:43:59,140
then the his

679
00:43:59,180 --> 00:44:03,540
will be positive definite and can be inverted in this case

680
00:44:05,520 --> 00:44:09,580
and as n is for of twenty five and is o the two then

681
00:44:10,810 --> 00:44:14,540
this is positive definite so stationary point

682
00:44:14,580 --> 00:44:17,000
of the mean squared error as the minimum

683
00:44:17,010 --> 00:44:19,370
and everything's good

684
00:44:20,770 --> 00:44:24,900
because this can be inverted then we can obtain

685
00:44:24,950 --> 00:44:26,930
the estimates of our

686
00:44:27,050 --> 00:44:28,590
parameter vector

687
00:44:28,600 --> 00:44:31,160
w hat where hi corresponds to

688
00:44:31,180 --> 00:44:35,610
and estimate empirical estimate

689
00:44:35,620 --> 00:44:37,630
we obtain

690
00:44:37,650 --> 00:44:38,830
the estimate

691
00:44:38,830 --> 00:44:40,290
which minimizes

692
00:44:40,300 --> 00:44:45,800
minimize squared error so it yields the least square there

693
00:44:45,880 --> 00:44:47,960
and if you remember

694
00:44:48,040 --> 00:44:50,600
let me just

695
00:44:50,610 --> 00:44:53,690
quickly dived by to this here

696
00:44:53,710 --> 00:44:55,930
the expression for the

697
00:44:58,000 --> 00:44:58,880
was this

698
00:44:58,890 --> 00:45:01,610
so we said this to date to zero

699
00:45:01,610 --> 00:45:02,620
this something

700
00:45:02,640 --> 00:45:06,280
again schoolboy algebra and we can

701
00:45:06,320 --> 00:45:09,090
immediately solve for w

702
00:45:09,100 --> 00:45:12,870
and it turns out to be

703
00:45:15,290 --> 00:45:20,210
the well-known least squares estimator which we all know and love

704
00:45:20,230 --> 00:45:21,810
so w heart

705
00:45:21,810 --> 00:45:23,680
here is that expression

706
00:45:23,680 --> 00:45:27,560
at the heart again this reduces to

707
00:45:27,620 --> 00:45:29,220
this expression

708
00:45:29,270 --> 00:45:32,270
and if we take the logo of the score

709
00:45:32,330 --> 00:45:36,490
the amazing thing that comes out is that the log of the score for any

710
00:45:36,490 --> 00:45:38,350
item we're trying to score

711
00:45:38,370 --> 00:45:41,620
it's simply going to be a linear function of

712
00:45:41,680 --> 00:45:44,790
the features and that idea

713
00:45:44,830 --> 00:45:47,000
this is about as simple

714
00:45:47,060 --> 00:45:49,680
a scoring method that you get

715
00:45:49,680 --> 00:45:52,790
coming out of any possible model

716
00:45:52,850 --> 00:45:54,290
so essentially

717
00:45:55,470 --> 00:45:57,930
this is the linear function of the

718
00:45:57,930 --> 00:46:00,930
features of that item x don j

719
00:46:00,930 --> 00:46:02,020
is just

720
00:46:03,220 --> 00:46:05,810
the j feature of item axis

721
00:46:05,830 --> 00:46:08,390
and that can be either zero or one

722
00:46:08,580 --> 00:46:13,160
the business park binary data and now hughes of j

723
00:46:13,220 --> 00:46:14,970
is a vector

724
00:46:14,990 --> 00:46:16,810
that represent

725
00:46:17,810 --> 00:46:20,260
query set

726
00:46:20,270 --> 00:46:23,080
as some direction

727
00:46:23,990 --> 00:46:27,890
and the base of the features of item

728
00:46:28,850 --> 00:46:32,310
so we had a set of items in our query that's going to be represented

729
00:46:32,310 --> 00:46:33,180
by some

730
00:46:33,200 --> 00:46:35,430
vector in x space

731
00:46:35,450 --> 00:46:37,160
and then we score

732
00:46:37,430 --> 00:46:41,260
items by the dot product of

733
00:46:41,310 --> 00:46:45,640
their features with that particular factor

734
00:46:45,680 --> 00:46:50,180
and it all came out that we didn't make any approximation this actually came out

735
00:46:50,180 --> 00:46:56,700
from starting from that probabilistic model and doing the integral and doing some simplifications

736
00:46:56,720 --> 00:47:01,160
so if we take all the items in our universe and we back them into

737
00:47:01,950 --> 00:47:03,640
giant matrix

738
00:47:03,640 --> 00:47:07,330
of the number of items in our universe by the number of features then we

739
00:47:07,330 --> 00:47:09,410
can people log score

740
00:47:09,470 --> 00:47:11,580
by just a single

741
00:47:13,040 --> 00:47:15,700
matrix vector multiplication

742
00:47:15,720 --> 00:47:18,850
which is what i promise before we

743
00:47:20,910 --> 00:47:28,290
i tell you in a minute

744
00:47:28,370 --> 00:47:33,430
the question was a repeat the question and then i'll get your question is how

745
00:47:33,430 --> 00:47:36,160
the the bias that and i'm about that

746
00:47:47,660 --> 00:47:48,790
until this

747
00:47:48,790 --> 00:47:50,490
depends on

748
00:47:50,500 --> 00:47:52,640
the query that

749
00:47:53,410 --> 00:47:56,770
so we have two kinds of data we have in the query set which we

750
00:47:57,560 --> 00:48:00,020
the system the eight elders in the

751
00:48:00,290 --> 00:48:04,970
the output filters in the battle that depend on the query set

752
00:48:05,020 --> 00:48:05,830
and then

753
00:48:05,850 --> 00:48:09,540
we have x which is the item that we're trying to score

754
00:48:09,560 --> 00:48:12,100
to come up with the rank with the by them

755
00:48:13,740 --> 00:48:16,870
the album better the don't depend on x

756
00:48:23,260 --> 00:48:25,060
that was the question of priors

757
00:48:26,200 --> 00:48:30,120
what we're gonna do for priors is the following

758
00:48:30,120 --> 00:48:33,330
we have a whole universe of items

759
00:48:33,370 --> 00:48:37,180
and in our universe of items we we can represent the matrix that might look

760
00:48:37,180 --> 00:48:40,450
like this we have items on this axis and the

761
00:48:40,580 --> 00:48:43,990
so far binary features on the fact that

762
00:48:44,040 --> 00:48:44,790
and now

763
00:48:44,810 --> 00:48:47,740
we can compute the mean

764
00:48:50,970 --> 00:48:53,770
the mean over the rows of this matrix

765
00:48:53,770 --> 00:48:56,260
and that gives us

766
00:48:56,260 --> 00:48:58,200
the mean frequency

767
00:48:58,220 --> 00:48:59,370
with which we

768
00:48:59,370 --> 00:49:03,080
observed each of the items in our universe of

769
00:49:04,350 --> 00:49:08,890
and we're going to come up with a broad empirical prior

770
00:49:11,000 --> 00:49:12,740
is proportional

771
00:49:12,770 --> 00:49:17,180
two the main feature frequency so we're that out to be

772
00:49:17,220 --> 00:49:22,830
kappa and the mean frequency so alpha is the vector of the ranges over the

773
00:49:22,830 --> 00:49:24,620
features one two j

774
00:49:24,660 --> 00:49:28,060
and and is also a factor and so we just set out

775
00:49:29,040 --> 00:49:34,890
some proportionality constant times and and we they'd have to be the same proportionality constant

776
00:49:34,890 --> 00:49:37,410
times one mind

777
00:49:37,410 --> 00:49:40,600
and for all the examples we use

778
00:49:40,620 --> 00:49:42,060
cap-i-tals two

779
00:49:42,080 --> 00:49:44,990
and we never came to work OK

780
00:49:44,990 --> 00:49:49,520
so those fairly robust to changes in half and now

781
00:49:49,520 --> 00:49:50,540
people right

782
00:49:50,720 --> 00:49:54,270
well a little bit and they might say oh but the prior here seems to

783
00:49:54,270 --> 00:49:55,990
depend on the data

784
00:49:56,000 --> 00:49:58,140
which is a little weird but

785
00:49:58,180 --> 00:50:01,370
in fact the prior here

786
00:50:01,390 --> 00:50:06,100
is remember it's the prior on the unknown concept that we're trying to query which

787
00:50:06,100 --> 00:50:08,310
is a subset of the data

788
00:50:08,350 --> 00:50:10,790
so what this prior does is simply

789
00:50:10,830 --> 00:50:16,990
depends on the entire universe of data it simply depends on the average frequency of

790
00:50:17,930 --> 00:50:20,390
in the universe of item OK

791
00:50:20,580 --> 00:50:22,100
so it's not really

792
00:50:23,560 --> 00:50:27,930
the thing that would be really problematic is the prior dependent on the query that

793
00:50:27,930 --> 00:50:30,700
would if we're not doing that we're just say a

794
00:50:30,720 --> 00:50:34,330
we can use some knowledge about how frequent features are

795
00:50:35,220 --> 00:50:36,830
this site

796
00:50:36,850 --> 00:50:39,100
what we think we are a are

797
00:50:39,580 --> 00:50:43,180
are query concept might

798
00:50:43,200 --> 00:50:47,740
involved in fact the prior is simply saying that are query concepts are going to

799
00:50:47,740 --> 00:50:50,850
have on average the

800
00:50:50,870 --> 00:50:57,500
the same each frequencies as far as our universe items

801
00:50:57,520 --> 00:50:58,830
which are frequently

802
00:51:10,020 --> 00:51:15,290
it is very similar to the basic binary model is similar to naive bayes

803
00:51:15,290 --> 00:51:18,760
where you some of the features are independent

804
00:51:20,700 --> 00:51:22,740
naive bayes

805
00:51:23,870 --> 00:51:27,600
usually thought of as the model for classification where

806
00:51:27,620 --> 00:51:31,830
what you have is given each class of the features are independent

807
00:51:33,560 --> 00:51:35,390
we have also

808
00:51:35,490 --> 00:51:38,330
given the concept of the features are independent

809
00:51:38,390 --> 00:51:42,870
but we are not getting the parameters of the model

810
00:51:43,600 --> 00:51:45,310
maximum likelihood

811
00:51:45,370 --> 00:51:48,260
or map or anything like that we're saying

812
00:51:48,270 --> 00:51:51,740
our features are independent we don't know the parameters are so we integrate over the

813
00:51:51,740 --> 00:51:56,200
parameters so yes it is related to the naive bayes in the sense that naive

814
00:51:56,200 --> 00:51:58,160
bayes assumes independent features

815
00:51:58,700 --> 00:52:00,330
let me just be honest

816
00:52:00,330 --> 00:52:05,410
you know in real data i don't believe that the features are going to be

817
00:52:09,060 --> 00:52:13,060
we have the trade-off here between having more and more complicated models that we actually

818
00:52:13,060 --> 00:52:14,270
believe in

819
00:52:14,310 --> 00:52:19,180
but for which these integrals will be very difficult to compute and having a simple

820
00:52:20,040 --> 00:52:22,970
which we think might give us a good approximation

821
00:52:23,020 --> 00:52:29,100
two what we believe it and be able to compute these things extremely efficient so

822
00:52:29,100 --> 00:52:33,160
what we've shown is that for this simple model we can compute all these integrals

823
00:52:33,200 --> 00:52:35,270
extremely efficiently

824
00:52:35,270 --> 00:52:41,130
in california loma linda you see this is the same content which was fermilab way

825
00:52:41,210 --> 00:52:46,590
this injector linac synchrotron this to other people to go there

826
00:52:46,600 --> 00:52:50,550
in a very short time and then that for doing this and almost the entire

827
00:52:50,660 --> 00:52:52,180
which are structures

828
00:52:52,210 --> 00:52:56,430
we support magnets which bend the the up

829
00:52:56,450 --> 00:52:59,270
and down so that you can turn it around

830
00:52:59,310 --> 00:53:04,470
the body of the page to you this is the first hospital-based center which is

831
00:53:04,470 --> 00:53:11,900
treating now fifteen hundred patients a year each patient scams about twenty times so fifteen

832
00:53:11,960 --> 00:53:16,450
one hundred missions twenty times to make a statement

833
00:53:16,470 --> 00:53:17,890
this is the first

834
00:53:17,890 --> 00:53:19,360
the the

835
00:53:19,370 --> 00:53:24,210
singletons i being bedroom in japan

836
00:53:24,240 --> 00:53:32,350
mitsubishi in japan and axel in germany and by i in the states c so

837
00:53:32,630 --> 00:53:35,150
this at two cyclotrons normal cycle

838
00:53:35,170 --> 00:53:40,040
superconducting cyclotron into singletons you recognise this shape

839
00:53:40,050 --> 00:53:47,050
so in short i think this is now commercial producing centers for portal therapies are

840
00:53:47,050 --> 00:53:49,880
commercial and you see here the IBA

841
00:53:50,980 --> 00:53:53,490
this is the same which this cell

842
00:53:53,500 --> 00:53:56,330
a hundred and fifty to thirty million euros

843
00:53:56,340 --> 00:54:00,700
there is a reception

844
00:54:00,750 --> 00:54:04,780
for the visiting visits of the

845
00:54:04,790 --> 00:54:07,240
patients seem

846
00:54:07,270 --> 00:54:12,940
and then the beam is reduced in energy by these energy selection system and then

847
00:54:12,940 --> 00:54:16,540
it goes to the gantries and this is the patient

848
00:54:16,560 --> 00:54:20,930
this is laying here and this is the end of the being which can be

849
00:54:20,930 --> 00:54:25,450
located according to the action chosen by the oncologist

850
00:54:25,470 --> 00:54:30,340
now that is being are more than sixty thousand patients along the eighteen ninety two

851
00:54:30,340 --> 00:54:31,680
thousand you see

852
00:54:31,700 --> 00:54:37,430
now we have and we have about the same as most of them now motion

853
00:54:37,440 --> 00:54:39,170
which are either of those

854
00:54:39,180 --> 00:54:41,390
working of stuff too

855
00:54:41,960 --> 00:54:46,540
treating patients so potent that it is moving because effect

856
00:54:46,550 --> 00:54:53,590
it's certainly better as tissues and it is also i really really

857
00:54:53,600 --> 00:55:00,520
the moment it is certain qualitative quantitative qualitative better than thought

858
00:55:00,540 --> 00:55:04,540
the next is it is not the case for carbon ions because the carbon has

859
00:55:04,660 --> 00:55:05,880
different types

860
00:55:06,000 --> 00:55:13,670
o of therapy it is qualitatively different you make it different to the cell so

861
00:55:13,700 --> 00:55:18,340
that's something which is more recent and that's what i want to discuss

862
00:55:18,400 --> 00:55:22,160
in china back to built by professor lower

863
00:55:23,300 --> 00:55:28,520
twenty years ago and directed since fifteen years first associate medical doctor who was if

864
00:55:28,520 --> 00:55:30,120
physics bees is

865
00:55:30,140 --> 00:55:36,670
this started working ninety two is a genuine carbon therapy it is made of single

866
00:55:36,920 --> 00:55:40,330
you see them in the next to inject in fact there are two singletons very

867
00:55:40,330 --> 00:55:46,250
big ones and there are one two and three to three patients treated five thousand

868
00:55:47,330 --> 00:55:49,540
in this year five thousand patients

869
00:55:49,580 --> 00:55:50,330
it is the

870
00:55:50,340 --> 00:55:53,400
just the center in the world

871
00:55:53,430 --> 00:55:56,970
in germany in the years last ten years

872
00:55:56,990 --> 00:56:02,270
what about the first craft damaged his side and under the leadership of professor the

873
00:56:02,270 --> 00:56:07,800
use of heidelberg clinic is a medical doctor has been in this laboratory to my

874
00:56:07,800 --> 00:56:09,280
lab at GSI

875
00:56:09,320 --> 00:56:14,940
damage that it being carbon ions only horizontal stock going three to treat patients mainly

876
00:56:14,940 --> 00:56:20,490
brain patients in about five hundred being treated that's all what we know so sixty

877
00:56:20,490 --> 00:56:23,350
thousand patients for both only fifty five

878
00:56:25,900 --> 00:56:28,400
for carbon ions

879
00:56:28,420 --> 00:56:33,180
so what have we learned this is all the patients have been treated see this

880
00:56:33,180 --> 00:56:41,460
is a typical body and woman and you can do it but for instance this

881
00:56:41,470 --> 00:56:45,300
could also call must encode almost so we can do something

882
00:56:45,310 --> 00:56:46,420
in the brain

883
00:56:46,450 --> 00:56:54,110
this is soft tissue sarcoma very dangerous tool and is locally advanced prostate carcinoma

884
00:56:54,120 --> 00:56:57,410
a in particular cases for prostate

885
00:56:57,470 --> 00:57:03,900
it's been shown that carbon ions but this is still not a very large number

886
00:57:03,920 --> 00:57:08,790
in spite of the sixty five thousand patients so there's lot of clinical studies to

887
00:57:08,790 --> 00:57:14,040
be done but one can only say something about this for instance the first important

888
00:57:14,040 --> 00:57:20,680
results which has not in boston harbor side then confirmed many other places and has

889
00:57:20,680 --> 00:57:24,820
a long long story because people have been followed for fifteen years

890
00:57:25,840 --> 00:57:27,520
he was in the

891
00:57:27,530 --> 00:57:33,580
i had called control sarcoma single domus which after five years can be controlled ninety

892
00:57:33,620 --> 00:57:38,790
percent to about seventy percent where all conventional radiotherapy

893
00:57:38,800 --> 00:57:44,450
to control them at thirty five percent more recently served as that the jump this

894
00:57:45,190 --> 00:57:50,280
may be due to people with here but still there is a clear advantage was

895
00:57:50,280 --> 00:57:52,670
support of for these students and many others

896
00:57:52,700 --> 00:57:53,910
and for that

897
00:57:53,920 --> 00:57:57,300
this is a complicated table put in so that you do it but only want

898
00:57:57,320 --> 00:58:01,680
to tell you this story numbers for two most of the

899
00:58:01,690 --> 00:58:05,110
palestine as soon as is here you see

900
00:58:05,110 --> 00:58:06,620
in high park

901
00:58:06,630 --> 00:58:13,570
there will be in control only to six percent and the standards twenty distribution for

902
00:58:13,990 --> 00:58:19,110
two most which have to do is very typical to of this kind of therapy

903
00:58:19,140 --> 00:58:25,530
they about eight hundred percent a five-year survival

904
00:58:25,540 --> 00:58:30,060
instead of twenty five percent and for salivary gland tumours was

905
00:58:30,100 --> 00:58:37,110
boasting high sitting at about seventy percent with respect also came to twenty five percent

906
00:58:37,110 --> 00:58:43,570
this at all three and you must you must which is known particularly resistance to

907
00:58:43,570 --> 00:58:46,820
these two match

908
00:58:46,870 --> 00:58:52,060
so the sequence and before

909
00:58:57,660 --> 00:59:05,100
just text

910
00:59:06,530 --> 00:59:07,650
x y

911
00:59:07,680 --> 00:59:19,100
fairly simple just tack on x i

912
00:59:21,360 --> 00:59:24,570
that's the case

913
00:59:25,500 --> 00:59:31,850
is it going on up to

914
00:59:31,900 --> 00:59:35,120
minus one

915
00:59:35,270 --> 00:59:38,620
that's certainly common sequence

916
00:59:38,620 --> 00:59:40,850
x y

917
00:59:43,780 --> 00:59:46,550
i minus one

918
00:59:52,460 --> 00:59:55,620
one to

919
00:59:58,300 --> 01:00:02,630
this is the longest common sequences along its sequence is

920
01:00:02,690 --> 01:00:06,800
from x one i y one j

921
01:00:06,830 --> 01:00:10,080
and we know what the last character is just

922
01:00:10,790 --> 01:00:12,570
equivalently y j

923
01:00:12,610 --> 01:00:14,690
so therefore everything

924
01:00:14,780 --> 01:00:17,070
except the last character

925
01:00:17,110 --> 01:00:20,290
most of these common sequence

926
01:00:21,890 --> 01:00:27,620
i'm much what i might one y one j one

927
01:00:27,670 --> 01:00:29,340
everybody with me

928
01:00:29,440 --> 01:00:33,080
there must be common sequence now

929
01:00:33,120 --> 01:00:40,980
what he also suspect

930
01:00:41,000 --> 01:00:44,590
we also suspect

931
01:00:45,770 --> 01:00:50,190
he won to common sequence of these two

932
01:00:50,240 --> 01:00:54,420
so it's longest common sequence

933
01:00:54,770 --> 01:00:57,870
so that claim

934
01:01:02,950 --> 01:01:06,390
one particular one isn't it

935
01:01:06,440 --> 01:01:10,480
longest common subsequence of x of

936
01:01:11,270 --> 01:01:13,890
i like this one

937
01:01:14,960 --> 01:01:22,450
why j much

938
01:01:22,890 --> 01:01:28,090
so it proved that

939
01:01:28,170 --> 01:01:32,360
claims of several version approved claim

940
01:01:33,920 --> 01:01:36,190
so so us

941
01:01:38,350 --> 01:01:40,660
it is

942
01:01:47,850 --> 01:01:52,070
is there a link to w

943
01:01:52,120 --> 01:01:53,100
the figure

944
01:01:53,270 --> 01:01:59,450
and came minus one

945
01:02:02,330 --> 01:02:04,340
common sequence

946
01:02:05,450 --> 01:02:08,840
the you want to became minus one so it's going to be handling

947
01:02:08,840 --> 01:02:11,810
minus one longer

948
01:02:12,750 --> 01:02:17,320
now what we do is we use the classic argument c

949
01:02:17,350 --> 01:02:22,070
multiple times not just this week which will be important for this week but

950
01:02:22,080 --> 01:02:27,680
through several lectures it's called cut and paste or

951
01:02:27,700 --> 01:02:33,170
the idea is

952
01:02:33,200 --> 01:02:36,600
let's take a look at w

953
01:02:37,060 --> 01:02:39,040
made with the

954
01:02:39,630 --> 01:02:43,950
that last characters UK

955
01:02:44,070 --> 01:02:45,630
this is

956
01:02:56,570 --> 01:02:57,770
it's not just mine

957
01:02:57,780 --> 01:03:00,850
reality for string concatenation

958
01:03:00,870 --> 01:03:01,940
so i take

959
01:03:01,960 --> 01:03:04,880
well acquainted with the longer common

960
01:03:06,610 --> 01:03:09,660
and i

961
01:03:09,850 --> 01:03:13,690
in i nasir pages

962
01:03:14,130 --> 01:03:17,160
that is certainly

963
01:03:17,180 --> 01:03:19,680
common sequence

964
01:03:27,490 --> 01:03:32,620
i one

965
01:03:40,430 --> 01:03:42,150
he has

966
01:03:44,830 --> 01:03:54,840
because basically what is it's like

967
01:03:55,790 --> 01:04:00,380
like the w three c minus one i one character

968
01:04:00,430 --> 01:04:01,390
so this

969
01:04:01,410 --> 01:04:06,150
combination here now is playing bigger than k

970
01:04:06,440 --> 01:04:13,360
and that's a contradiction

971
01:04:15,480 --> 01:04:19,780
thereby proving the point

972
01:04:19,890 --> 01:04:21,660
simply saying

973
01:04:21,740 --> 01:04:23,200
queen this

974
01:04:23,240 --> 01:04:25,320
suppose longer one

975
01:04:25,360 --> 01:04:29,720
let me show you my longer comment sequence the first

976
01:04:31,940 --> 01:04:35,060
prefixes were dropped the character

977
01:04:35,100 --> 01:04:37,910
both strange with long hair

978
01:04:38,270 --> 01:04:41,590
made the whole thing wrong

979
01:04:41,590 --> 01:04:43,350
that be

980
01:04:43,390 --> 01:04:45,730
so therefore this must be a

981
01:04:45,750 --> 01:04:50,520
longest common subsequence

982
01:04:50,520 --> 01:04:56,120
so good afternoon everyone my name's additional i'm going to talk about the UN recognition

983
01:04:56,120 --> 01:04:58,570
in videos by learning from web data

984
01:04:58,580 --> 01:05:03,250
this is joint work with my professor professor to she professor i've gotten from nanyang

985
01:05:03,250 --> 01:05:09,860
technological university in singapore and that the capable of from kodak research labs rochester us

986
01:05:09,860 --> 01:05:12,180
so this is the outline of my presentation

987
01:05:12,200 --> 01:05:16,110
first give you an overview of the event recognition system

988
01:05:16,130 --> 01:05:17,610
and in the system

989
01:05:17,630 --> 01:05:18,930
there two issues

990
01:05:18,950 --> 01:05:22,320
the first issue is how to measure the similarity between videos

991
01:05:22,340 --> 01:05:26,000
to address this issue proposed aligned space time pyramid matching

992
01:05:26,000 --> 01:05:27,360
and the second issue

993
01:05:27,370 --> 01:05:32,780
it is crossed them in his quest to problem and we propose adaptive multiple kernel

994
01:05:32,780 --> 01:05:37,360
learning to solve this problem and give you more details on these two issues later

995
01:05:37,640 --> 01:05:41,570
after that the experimental results followed by conclusions

996
01:05:41,580 --> 01:05:45,350
so first that's looking to the overview of the system

997
01:05:45,360 --> 01:05:50,570
the goal of our system is to recognise consumer videos the consumer videos ideally taken

998
01:05:50,570 --> 01:05:54,910
by consumers during some events such as weddings sports

999
01:05:54,940 --> 01:05:57,210
technique and so on

1000
01:05:57,240 --> 01:06:01,080
so customer videos have large intra class variability is

1001
01:06:01,080 --> 01:06:02,000
and also

1002
01:06:02,050 --> 01:06:04,660
there already i lived in a limited number of

1003
01:06:04,940 --> 01:06:09,830
videos because the customers may not be waiting to label lots of videos

1004
01:06:09,880 --> 01:06:13,080
so as as using a limited number of

1005
01:06:13,100 --> 01:06:17,860
the training samples can america so as to make use of

1006
01:06:17,960 --> 01:06:23,160
the with auxiliary data so thanks to the internet they can download videos from the

1007
01:06:24,000 --> 01:06:28,490
for example the can of the web videos from youtube so they can leverage large

1008
01:06:28,490 --> 01:06:34,520
number of loosely labeled web videos to learn a good classifier for the consumer videos

1009
01:06:34,580 --> 01:06:37,330
this is the flow chart of our system

1010
01:06:37,380 --> 01:06:42,100
the system is designed in the traditional way that given the video database we first

1011
01:06:42,100 --> 01:06:43,410
training class for

1012
01:06:43,430 --> 01:06:47,710
and for adding test video will use the cash for two to predict its label

1013
01:06:47,710 --> 01:06:49,270
as output

1014
01:06:49,330 --> 01:06:52,410
so just as i said there are two issues in the system

1015
01:06:52,440 --> 01:06:56,330
the first one is how to measure the similarity between videos

1016
01:06:56,350 --> 01:07:00,670
several predetermined matching methods have been proposed to address this issue

1017
01:07:00,680 --> 01:07:06,390
i this method each video is divided into multiple volumes on the ten axis space

1018
01:07:06,390 --> 01:07:10,230
axes both space and time axes

1019
01:07:10,240 --> 01:07:13,980
and this method you have two steps in the first step the

1020
01:07:14,030 --> 01:07:19,450
volume two volume distance is computed and then the second step the distance between two

1021
01:07:20,240 --> 01:07:23,860
is computed based on the bottom two volume distances

1022
01:07:23,870 --> 01:07:24,980
so in

1023
01:07:24,990 --> 01:07:29,140
the space pure matching method proposed by doctoral active

1024
01:07:29,150 --> 01:07:31,060
the volume of radio can

1025
01:07:31,080 --> 01:07:35,730
only in be matched to the video in the same position of not the radio

1026
01:07:35,800 --> 01:07:40,890
so following the existing work in are aligned space time pyramid matching we divide each

1027
01:07:40,900 --> 01:07:41,930
video into

1028
01:07:41,930 --> 01:07:44,370
my people now overlapped space time volumes

1029
01:07:44,400 --> 01:07:49,280
here it always the liver number and the experiment was the equal to zero and

1030
01:07:50,780 --> 01:07:55,330
and our method one video you might want you the video can be managed to

1031
01:07:55,330 --> 01:07:59,060
avoid doing it in the four different positions of not radio

1032
01:07:59,110 --> 01:08:04,460
so our method has a greater variability than the online service pyramid matching

1033
01:08:04,560 --> 01:08:09,960
for example while object may move around into videos so may appear in different positions

1034
01:08:09,960 --> 01:08:12,390
of of the two videos

1035
01:08:12,400 --> 01:08:18,050
so in this case our method can match the two volumes both containing the object

1036
01:08:18,140 --> 01:08:21,400
but the this template matching cannot

1037
01:08:21,400 --> 01:08:24,480
and our method is also a two-step approach

1038
01:08:24,490 --> 01:08:27,050
in the first step we directly followed

1039
01:08:27,060 --> 01:08:30,740
adaptive work to compute the volume to volume distances

1040
01:08:30,740 --> 01:08:34,740
so in that sense that to compute the distance between two videos

1041
01:08:34,860 --> 01:08:36,140
there was zero

1042
01:08:36,270 --> 01:08:42,120
each radio station directly consider as space time volumes so the distance between two videos

1043
01:08:42,120 --> 01:08:44,120
can be directly attend

1044
01:08:44,170 --> 01:08:50,110
at one which divide each video into eight that overlapped space time volumes with the

1045
01:08:50,110 --> 01:08:51,270
same size

1046
01:08:51,300 --> 01:08:53,110
and the way i don't

1047
01:08:53,120 --> 01:08:55,270
integer flow EMD

1048
01:08:55,300 --> 01:09:01,180
the worst distance to compute distance between the two videos so after performing major flow

1049
01:09:01,270 --> 01:09:06,360
earthmover distance we can find the best matching between the volumes so for example in

1050
01:09:06,360 --> 01:09:08,010
this figure the

1051
01:09:08,030 --> 01:09:10,340
two what is the right colour match

1052
01:09:10,350 --> 01:09:15,410
and two volumes in green colour amassed and so on the rest rest photos

1053
01:09:15,460 --> 01:09:17,090
so with the

1054
01:09:17,140 --> 01:09:20,590
after finding the most voters we can compute the

1055
01:09:20,600 --> 01:09:22,960
the distance between two videos with

1056
01:09:23,000 --> 01:09:26,150
and the distance between the master volume

1057
01:09:26,230 --> 01:09:32,620
so the here comes the second you show that there is a cross the problem

1058
01:09:32,630 --> 01:09:33,760
so as we know

1059
01:09:33,770 --> 01:09:38,500
the customer videos and naturally captured by consumers and the web videos may

1060
01:09:38,520 --> 01:09:43,940
maybe edited and they are usually selected before uploading onto the internet so that the

1061
01:09:44,120 --> 01:09:45,580
distribution of this

1062
01:09:45,590 --> 01:09:49,370
of the consumer videos and that we use may be different

1063
01:09:49,410 --> 01:09:52,400
so let us consider the customer video

1064
01:09:52,410 --> 01:09:57,460
consumer videos as the target domain and videos as a source domain

1065
01:09:57,470 --> 01:09:58,600
so too many the

1066
01:09:58,620 --> 01:10:04,210
the distribution mismatch between the two as we are not a criteria accord that's meets

1067
01:10:04,390 --> 01:10:06,130
discrepancy MMD

1068
01:10:06,170 --> 01:10:08,620
in this criteria

1069
01:10:08,660 --> 01:10:14,510
the each sample x is mapped into a higher dimensional space phi x

1070
01:10:15,200 --> 01:10:19,760
as the MMD computes the distance between the means of the two distributions

1071
01:10:19,900 --> 01:10:24,020
and also in the preserve somehow this the statistics

1072
01:10:24,030 --> 01:10:29,580
and there was some simple mathematical operations we can simply by the square of the

1073
01:10:29,960 --> 01:10:31,270
MMD distance

1074
01:10:31,270 --> 01:10:32,660
and the trace of

1075
01:10:33,090 --> 01:10:38,380
that is k times metrics this here the escapees could come and fix it is

1076
01:10:38,380 --> 01:10:42,780
i and we we so this already that the expectation of a nature the conditional

1077
01:10:42,780 --> 01:10:49,360
probability is the optimal fire risk right we can't do any better than minimize pointwise

1078
01:10:49,380 --> 01:10:51,820
OK so you know that's it the the the

1079
01:10:51,840 --> 01:10:54,620
the heart of it is just yet and inequality once we have the

1080
01:10:55,010 --> 01:11:00,080
the right definition for this function psi

1081
01:11:03,140 --> 01:11:08,700
that's all i wanted to say about this relationship between excess risk and excess phi

1082
01:11:08,700 --> 01:11:11,180
risk you know we have

1083
01:11:12,690 --> 01:11:14,640
simple conditions

1084
01:11:14,700 --> 01:11:19,760
we get a picture of we have the simple condition that but for convex functions

1085
01:11:19,760 --> 01:11:23,400
phi the only thing that we need to cheque is the behavior

1086
01:11:23,420 --> 01:11:27,820
at zero needs to be differentiable zero and have the negative gradient and obviously this

1087
01:11:27,820 --> 01:11:31,840
is the case for for all of these functions

1088
01:11:31,930 --> 01:11:36,340
right and any other cost function that we dream up and there have been

1089
01:11:36,340 --> 01:11:41,400
many of these in in the literature you can immediately check if that

1090
01:11:41,410 --> 01:11:45,510
does indeed make sense from the point of view of that classification

1091
01:11:46,130 --> 01:11:50,740
what i would look at next is exploiting this relationship between excess risk and excess

1092
01:11:50,740 --> 01:11:53,890
by risk to come up with some idea of of

1093
01:11:53,900 --> 01:11:55,570
how rapidly we are

1094
01:11:56,910 --> 01:11:59,080
minimising the risk

1095
01:11:59,100 --> 01:12:01,140
right by looking at how

1096
01:12:01,160 --> 01:12:03,060
we minimize the fire risk

1097
01:12:03,840 --> 01:12:05,640
let's get back to that

1098
01:12:05,700 --> 01:12:13,100
OK so i will look now at the natural approximation estimation decomposition that the PC

1099
01:12:15,940 --> 01:12:19,920
in the first talk we saw an approximation estimation decomposition for

1100
01:12:19,950 --> 01:12:23,470
for risk where we said well we've got some class of functions were concerned with

1101
01:12:23,890 --> 01:12:25,710
if we're choosing from that class

1102
01:12:25,720 --> 01:12:29,750
you know there is that the natural action of approximation error there was the infimum

1103
01:12:29,750 --> 01:12:30,940
over the class

1104
01:12:30,980 --> 01:12:32,440
of the risk

1105
01:12:32,450 --> 01:12:35,030
right of some function

1106
01:12:35,220 --> 01:12:42,910
here where we're not we're not working with the minimal risk of threshold functions right

1107
01:12:42,910 --> 01:12:46,740
we're not we're not really shooting for for for such a thing all we're trying

1108
01:12:46,740 --> 01:12:50,290
to do is minimized the fire risk and so the natural way to define

1109
01:12:50,300 --> 01:12:53,110
the approximation errors in terms of this virus

1110
01:12:53,130 --> 01:12:58,680
OK so for thinking of st ann's right it's the minimum over over here

1111
01:12:58,800 --> 01:13:02,050
ball in an RKHS for instance of the

1112
01:13:02,070 --> 01:13:04,670
one of the expectation of this hinge

1113
01:13:04,700 --> 01:13:07,390
a of this hinge loss

1114
01:13:07,410 --> 01:13:10,350
OK this is the

1115
01:13:10,360 --> 01:13:13,930
in the natural way to define think i guess i guess it might be worthwhile

1116
01:13:14,400 --> 01:13:18,830
actually that's one another example that's with with thinking about what is this function phi

1117
01:13:18,840 --> 01:13:24,400
look like for the hinge loss i guess i need to connect the picture there

1118
01:13:25,400 --> 01:13:27,060
is piecewise linear

1119
01:13:28,390 --> 01:13:29,390
and so

1120
01:13:29,400 --> 01:13:32,190
psi is just a linear function

1121
01:13:32,210 --> 01:13:36,070
right so the relationship between between the excess risk and excess by risk in that

1122
01:13:36,070 --> 01:13:37,810
case is just linear

1123
01:13:37,850 --> 01:13:40,970
right so we have we have the best possible convex

1124
01:13:43,850 --> 01:13:45,660
in that case

1125
01:13:45,820 --> 01:13:47,820
OK so let's

1126
01:13:47,820 --> 01:13:49,310
let's look at this

1127
01:13:49,330 --> 01:13:52,620
the approximation estimation decomposition

1128
01:13:52,640 --> 01:13:56,320
so step back now and think about a general

1129
01:13:56,780 --> 01:14:02,120
model selection kind of approach and will view

1130
01:14:02,560 --> 01:14:07,010
various of these large margin classification algorithms in this kind of way

1131
01:14:07,030 --> 01:14:09,170
right so

1132
01:14:09,210 --> 01:14:11,790
let's think of

1133
01:14:11,850 --> 01:14:14,870
two approaches to mention this

1134
01:14:14,890 --> 01:14:17,680
earlier i guess two approaches to

1135
01:14:19,400 --> 01:14:20,930
minimizing the

1136
01:14:20,940 --> 01:14:24,800
the fire escape are very rich class i suppose we have a very rich class

1137
01:14:24,810 --> 01:14:27,290
like the span of

1138
01:14:27,350 --> 01:14:34,900
decision trees in sense of spanish function disinterested inside or

1139
01:14:35,140 --> 01:14:39,320
a reproducing kernel hilbert space which means that we think of it as being a

1140
01:14:40,400 --> 01:14:41,530
of some

1141
01:14:41,800 --> 01:14:45,330
the complexity classes writing as k increases with think even more

1142
01:14:45,340 --> 01:14:51,040
complex classes may be bigger balls in reproducing kernel hilbert space or maybe

1143
01:14:51,060 --> 01:14:56,670
we split up the standard according to the number of terms in a linear combination

1144
01:14:56,670 --> 01:14:59,600
for instance right so so

1145
01:14:59,600 --> 01:15:02,760
you know this is a method of sieves kind of approach was splitting splitting a

1146
01:15:02,760 --> 01:15:04,390
average class up into

1147
01:15:04,470 --> 01:15:09,580
into this this hierarchy which using a function as

1148
01:15:12,760 --> 01:15:15,340
the minimizer over some

1149
01:15:15,350 --> 01:15:20,640
if k in in the hierarchy year of the empirical forest and the k that

1150
01:15:20,640 --> 01:15:25,180
we choose will eat some increasing function of a right so as sample size increases

1151
01:15:25,450 --> 01:15:27,720
we might choose more complex

1152
01:15:27,750 --> 01:15:30,490
more complex class perhaps bigger

1153
01:15:30,500 --> 01:15:34,280
bowling RKHS we minimize the empirical forest cover

1154
01:15:34,320 --> 01:15:35,830
over the bigger ball

1155
01:15:35,850 --> 01:15:39,570
minimizex the sample average of the hinge loss for instance

1156
01:15:40,800 --> 01:15:45,150
an alternative and this is where you should have used the adaboost example appear in

1157
01:15:45,150 --> 01:15:50,450
alternative and this is more like what's done in the SVM case we might choose

1158
01:15:50,450 --> 01:15:52,580
the minimum over

1159
01:15:52,660 --> 01:15:57,760
the the full class of you know some regularized version of the empirical fires

1160
01:15:57,790 --> 01:16:01,130
right so we add a regularisation functional right this

1161
01:16:01,610 --> 01:16:03,030
the negative f is

1162
01:16:03,040 --> 01:16:04,040
it's like the

1163
01:16:04,050 --> 01:16:08,650
squared RKHS norm in this case

1164
01:16:08,700 --> 01:16:11,430
we have some regularisation coefficient

1165
01:16:11,440 --> 01:16:16,180
written as lambda then so is the sample size n increases we would expect to

1166
01:16:16,180 --> 01:16:21,160
which we should be somehow decreasing the regularisation right

1167
01:16:21,160 --> 01:16:22,850
is there

1168
01:16:22,870 --> 01:16:24,500
third lecture

1169
01:16:24,660 --> 01:16:28,000
devoted to nuclear physics

1170
01:16:28,680 --> 01:16:34,630
i'm going to give you you some examples of recent studies that are made so

1171
01:16:34,630 --> 01:16:38,820
nowadays so these are both experimental studies

1172
01:16:38,820 --> 01:16:42,410
theoretical studies that i made all over the world

1173
01:16:42,420 --> 01:16:47,780
so i will first about the figure of merit of the present approaches so

1174
01:16:47,820 --> 01:16:51,210
very good results compared to experimental data

1175
01:16:51,280 --> 01:16:53,450
what about exotic nuclei

1176
01:16:53,460 --> 01:16:56,020
isomers shape coexistence super behavior

1177
01:16:56,030 --> 01:17:00,350
and i want to talk just a little bit concerning these weak interactions i have

1178
01:17:00,390 --> 01:17:04,990
question two days ago and what how can we test the fundamental symmetries and this

1179
01:17:04,990 --> 01:17:07,320
weak interactions

1180
01:17:08,950 --> 01:17:10,210
i want to

1181
01:17:10,240 --> 01:17:12,420
to begin with this

1182
01:17:12,460 --> 01:17:16,960
extensive calculations that are from now because because of this

1183
01:17:17,000 --> 01:17:18,920
huge computer where

1184
01:17:18,930 --> 01:17:25,150
we can now have two flow computer so we can really former systematic calculations so

1185
01:17:26,010 --> 01:17:29,750
as many nuclei as we want so we can really

1186
01:17:29,790 --> 01:17:31,450
check how

1187
01:17:31,480 --> 01:17:35,680
valid in the theory that we use so that's why a lot of different groups

1188
01:17:35,680 --> 01:17:38,930
have from this kind of systematic calculations

1189
01:17:38,950 --> 01:17:41,450
to compare with all the experimental data

1190
01:17:41,530 --> 01:17:46,150
so this work as big as began in two thousand seven and still continuing now

1191
01:17:46,150 --> 01:17:48,290
for different labels

1192
01:17:48,320 --> 01:17:53,280
so this is the main thing base calculations for the first two plastic citation

1193
01:17:53,290 --> 01:17:55,400
in even even nuclei

1194
01:17:55,420 --> 01:17:59,650
what is plotted here in the exhibition in energy for theory

1195
01:17:59,650 --> 01:18:01,510
and here experiment

1196
01:18:01,530 --> 01:18:06,310
so the straight line means that you have perfect calculations

1197
01:18:06,340 --> 01:18:10,060
and all the points out the calculated points

1198
01:18:10,090 --> 01:18:16,120
so two plus states have been observed and studied in five hundred fifty seven

1199
01:18:16,210 --> 01:18:17,400
nicky i

1200
01:18:17,450 --> 01:18:22,060
so calculation having banned from almost all these nuclei i except sixteen

1201
01:18:22,070 --> 01:18:24,210
the lightest one

1202
01:18:24,260 --> 01:18:25,350
so why

1203
01:18:25,370 --> 01:18:30,020
so the reason that i told you this yesterday that when we are doing kind

1204
01:18:30,020 --> 01:18:34,330
of mean field calculation we are averaging over the number of nucleons

1205
01:18:34,340 --> 01:18:38,030
so when this number is too small the average is not good at all and

1206
01:18:38,030 --> 01:18:40,280
we can not make this approximation

1207
01:18:40,340 --> 01:18:42,490
so that's why these nuclei

1208
01:18:42,490 --> 01:18:48,590
are always eliminated when you are looking at a mean field based calculations

1209
01:18:48,620 --> 01:18:53,370
so what are the main feature so the simplex theory experiments here

1210
01:18:53,380 --> 01:18:57,800
so you see first that it goes from a few kb two if you amy

1211
01:18:57,800 --> 01:19:00,050
so three orders of magnitude

1212
01:19:00,060 --> 01:19:04,110
between means that these two players depending on the nucleus you're looking at

1213
01:19:04,120 --> 01:19:08,530
the structure of the nature is completely different you can be

1214
01:19:08,560 --> 01:19:10,280
vibrational states

1215
01:19:10,300 --> 01:19:15,240
none collective states are rotational states are mixing all of that

1216
01:19:15,250 --> 01:19:19,030
and depending on the nucleus and the method that to use you can have

1217
01:19:19,050 --> 01:19:22,130
better results for a given class of nuclei

1218
01:19:22,150 --> 01:19:25,870
and here you see that here you have a very good agreement

1219
01:19:25,880 --> 01:19:27,530
and these correspond to

1220
01:19:27,580 --> 01:19:30,400
strongly deformed actinides so

1221
01:19:30,410 --> 01:19:32,300
let's figure shows you

1222
01:19:32,310 --> 01:19:36,840
that this approach based on the mean field on collective motion is good for that

1223
01:19:36,860 --> 01:19:38,560
kind of nuclei

1224
01:19:38,560 --> 01:19:40,620
when you have here

1225
01:19:40,660 --> 01:19:45,180
very high energy for the two plus you see that the scattering is bigger

1226
01:19:45,190 --> 01:19:47,440
which means that the method is not valid

1227
01:19:47,460 --> 01:19:50,190
this as valid as for here

1228
01:19:50,210 --> 01:19:53,500
it's because you neglected kind of correlations

1229
01:19:53,530 --> 01:20:00,250
and what is neglected here this coupling between intrinsic excitations collective motion

1230
01:20:00,250 --> 01:20:04,210
what is important also these points so you see that you have some but the

1231
01:20:04,210 --> 01:20:07,460
logical point so this is very important to know

1232
01:20:07,470 --> 01:20:11,310
what all these points on why you are so bad here

1233
01:20:11,330 --> 01:20:12,960
these correspond to

1234
01:20:12,970 --> 01:20:16,530
neutron for mandatory and later isotopes

1235
01:20:16,550 --> 01:20:21,160
and we try to know why it's not so good for these points on your

1236
01:20:21,160 --> 01:20:23,340
viewpoint and it seems to be

1237
01:20:23,360 --> 01:20:28,310
that is because the single particle levels for that region is not reproduced so well

1238
01:20:28,310 --> 01:20:29,500
by the theory

1239
01:20:29,530 --> 01:20:33,060
it's because of the shape coexistence that seems that we have

1240
01:20:33,080 --> 01:20:34,620
a discrepancy

1241
01:20:34,620 --> 01:20:37,510
so we try to improve that point and we

1242
01:20:37,520 --> 01:20:39,170
i think you can help

1243
01:20:39,180 --> 01:20:41,580
for the other so

1244
01:20:41,600 --> 01:20:45,230
in fact even if it's not good to have such point we like it because

1245
01:20:45,230 --> 01:20:48,330
we can improve the theory

1246
01:20:50,450 --> 01:20:56,180
if we look now for the same calculation the quadrupole deformation so quickly moment theory

1247
01:20:56,180 --> 01:20:57,700
and experiment

1248
01:20:57,730 --> 01:21:00,870
so we have these different points here

1249
01:21:00,890 --> 01:21:03,310
there were a few nuclei where

1250
01:21:03,320 --> 01:21:06,870
we didn't know the sign at the moment either politics or overlaid so we do

1251
01:21:06,880 --> 01:21:08,800
not know the sign experimentally

1252
01:21:08,810 --> 01:21:12,190
so we have to do that twice so positive

1253
01:21:12,200 --> 01:21:13,420
and negative

1254
01:21:13,430 --> 01:21:16,680
and we see that in order to to have an agreement with experiment

1255
01:21:16,680 --> 01:21:19,240
we predict that these nicolai should be

1256
01:21:19,250 --> 01:21:23,550
all eight with a negative quadrupole moment

1257
01:21:24,180 --> 01:21:30,180
what this calculation showing that we can make some prediction where we have call it

1258
01:21:31,470 --> 01:21:35,720
and it seems to be reliable to be able then to make some predictions

1259
01:21:35,730 --> 01:21:40,740
in other new nuclear and so what we call the exotic nuclei

1260
01:21:40,790 --> 01:21:45,160
so now i'm going to go to these exotic nuclei what are the

1261
01:21:45,200 --> 01:21:47,630
durante shows now it is

1262
01:21:47,660 --> 01:21:50,040
so this is the nuclear charge

1263
01:21:50,050 --> 01:21:54,880
so what are the subject of interest are also neutron and proton drip lines

1264
01:21:54,890 --> 01:21:57,810
what we call drip line i will explain to you

1265
01:21:57,890 --> 01:21:59,990
this limit of existence

1266
01:22:00,700 --> 01:22:03,770
what can often of nuclei do exist

1267
01:22:03,790 --> 01:22:06,860
how those genes and molecules structures are

1268
01:22:08,370 --> 01:22:10,300
new shared structure

1269
01:22:10,310 --> 01:22:11,890
nuclear shapes

1270
01:22:11,920 --> 01:22:19,120
also concerning astrophysics for the different processes are process no ve extreme burst happy process

1271
01:22:19,120 --> 01:22:21,100
and supernovae

1272
01:22:21,110 --> 01:22:23,010
super heavy elements

1273
01:22:23,040 --> 01:22:26,250
many things concerning applications

1274
01:22:26,260 --> 01:22:27,610
also these

1275
01:22:27,620 --> 01:22:32,520
new decay modes nuria radioactivity you you can a need for proton rich nuclear you

1276
01:22:32,520 --> 01:22:34,570
can emit two proton and neutron

1277
01:22:34,580 --> 01:22:37,120
in coincidence

1278
01:22:37,130 --> 01:22:40,060
and also concerning neutron production pairing

1279
01:22:40,120 --> 01:22:47,420
i have the question yesterday concerning after lecture concerning BCS state so these superb wiki

1280
01:22:47,430 --> 01:22:49,110
that we have in the

1281
01:22:49,120 --> 01:22:50,120
in the nuclei

1282
01:22:50,140 --> 01:22:54,810
so we know that in the nucleus i we do have these superior trinity so

1283
01:22:54,810 --> 01:22:59,490
that the nucleons do form some peer support and put onto neutral photon pairs we

1284
01:22:59,490 --> 01:23:02,720
spin that's spin down so they gain energy like that

1285
01:23:02,740 --> 01:23:08,020
and what is what we think now is that for a number four nuclei having

1286
01:23:08,020 --> 01:23:10,130
the same number of proton and neutron

1287
01:23:10,140 --> 01:23:15,050
you can have and you should have these kind of neutron important feeling so the

1288
01:23:15,050 --> 01:23:19,060
pair formed was put onto neutral and they should appear for

1289
01:23:19,080 --> 01:23:21,820
neutron and proton with this with an

1290
01:23:21,850 --> 01:23:26,370
when you have the same number of neutrons for so you have many many

1291
01:23:26,420 --> 01:23:29,350
theoretical work devoted to that subject

1292
01:23:29,360 --> 01:23:34,990
it is very difficult to to find an experimental observable for that

1293
01:23:35,120 --> 01:23:41,060
because it's not and direct proof so we have different ideas how to change that

1294
01:23:41,060 --> 01:23:46,730
but it's not so easy it's not when they went directly to check

1295
01:23:46,740 --> 01:23:50,500
so i'm going to talk about all these red points and a little bit of

1296
01:23:50,500 --> 01:23:53,440
love this one

1297
01:23:53,490 --> 01:23:59,360
so the main questions are what are the limits to nuclear existence what new forms

1298
01:23:59,360 --> 01:24:00,990
will be found in nuclei

1299
01:24:00,990 --> 01:24:04,550
thanks so much to the

1300
01:24:04,610 --> 01:24:11,530
a participants and organizers been so possible to me

1301
01:24:11,680 --> 01:24:14,870
especially specially jeff was then

1302
01:24:14,880 --> 01:24:16,850
watching over me from

1303
01:24:16,930 --> 01:24:21,900
for many weeks now step in and thank you

1304
01:24:21,980 --> 01:24:24,510
i am

1305
01:24:24,560 --> 01:24:27,110
i've never been treated so really before

1306
01:24:31,240 --> 01:24:37,860
and i'm very excited to be learned for the first time a begin to to

1307
01:24:37,860 --> 01:24:40,260
learn about learning about

1308
01:24:40,270 --> 01:24:43,610
the machine learning this is

1309
01:24:53,660 --> 01:24:56,720
it shows you the first pages

1310
01:24:56,800 --> 01:24:58,860
are some years

1311
01:24:59,570 --> 01:25:05,960
for this is that i didn't then winkle in the tower of ivory tower of

1312
01:25:05,960 --> 01:25:09,520
mathematical philosophy and poetry

1313
01:25:10,660 --> 01:25:14,910
most of my mathematical life though when i was closer to your age

1314
01:25:14,930 --> 01:25:20,380
i did do some useful things but i am

1315
01:25:20,400 --> 01:25:24,540
the most probable became obsessed with the

1316
01:25:24,550 --> 01:25:27,330
philosophy and poetry

1317
01:25:27,350 --> 01:25:30,290
this is going to work the flickr

1318
01:25:30,330 --> 01:25:33,580
they found

1319
01:25:35,550 --> 01:25:39,820
this is an outline of the talk first world world

1320
01:25:40,820 --> 01:25:43,100
linear programming philosophy and then well

1321
01:25:43,110 --> 01:25:45,410
i will present for

1322
01:25:45,410 --> 01:25:47,130
there was another

1323
01:25:49,710 --> 01:25:51,100
details of the

1324
01:25:51,110 --> 01:25:57,910
papers the ancient papers that asian artifacts just point two

1325
01:25:58,180 --> 01:26:00,650
look at some of the details of those

1326
01:26:00,680 --> 01:26:06,790
and then we show you one

1327
01:26:06,800 --> 01:26:14,520
application but i can promise you that is actually useful application is the beautiful application

1328
01:26:14,800 --> 01:26:19,030
but number one it depends on you

1329
01:26:20,050 --> 01:26:23,110
find something practical to do with it

1330
01:26:23,110 --> 01:26:25,300
a member two

1331
01:26:25,310 --> 01:26:28,990
it depends on you to implement

1332
01:26:29,050 --> 01:26:32,650
there these are happened

1333
01:26:32,670 --> 01:26:35,310
optimum branching systems

1334
01:26:35,330 --> 01:26:40,270
it is a very sophisticated applications

1335
01:26:40,270 --> 01:26:44,610
ah parliamentary theory

1336
01:26:44,640 --> 01:26:47,080
and it's c

1337
01:26:47,080 --> 01:26:51,020
the only reasonable way i discovered

1338
01:26:52,810 --> 01:26:54,240
the the

1339
01:26:54,240 --> 01:26:55,180
however the

1340
01:26:55,180 --> 01:26:57,430
algorithms for

1341
01:26:57,450 --> 01:27:01,520
for solving a as as

1342
01:27:01,550 --> 01:27:06,050
versions of

1343
01:27:06,110 --> 01:27:08,900
well submodular minimisation and more

1344
01:27:09,580 --> 01:27:10,960
as you will see

1345
01:27:12,950 --> 01:27:14,950
what is the branching

1346
01:27:14,960 --> 01:27:18,020
does anybody have a marker

1347
01:27:18,140 --> 01:27:22,330
it suspended in a directed graph

1348
01:27:22,400 --> 01:27:27,580
you have a directed graph with weights on the edges

1349
01:27:27,670 --> 01:27:33,980
a vaccine is a spanning tree

1350
01:27:34,050 --> 01:27:39,250
branching rooted at branching rooted at node v sub-zero

1351
01:27:39,300 --> 01:27:42,990
is this spanning tree such that every pattern the tree

1352
01:27:43,000 --> 01:27:46,770
from this route said there is a directed back

1353
01:27:48,420 --> 01:27:50,840
uses all the time

1354
01:27:50,900 --> 01:27:52,870
everybody uses is well

1355
01:27:52,870 --> 01:27:55,370
i call branches spanning tree

1356
01:27:55,390 --> 01:27:59,170
so is every path from the root is directed

1357
01:28:05,140 --> 01:28:11,270
efficient algorithms for finding a cheapest or most valuable if you prefer

1358
01:28:11,280 --> 01:28:15,180
branching rooted at a given node

1359
01:28:15,180 --> 01:28:18,110
right so this the trade off between the two

1360
01:28:18,110 --> 01:28:20,680
another example

1361
01:28:20,770 --> 01:28:22,490
it would be

1362
01:28:22,540 --> 01:28:28,060
one of those p norm algorithms the divergence is now

1363
01:28:28,090 --> 01:28:34,620
this is very weird thing it's not nice circles but it's somewhat complicated divergence

1364
01:28:34,670 --> 01:28:38,960
right something like this

1365
01:28:41,830 --> 01:28:43,710
i have the trade-off between the two

1366
01:28:43,730 --> 01:28:48,180
and now in this case i would go to this point

1367
01:28:48,230 --> 01:28:54,480
OK it's more complicated

1368
01:28:56,660 --> 01:29:01,760
so i want to get a flavor

1369
01:29:03,160 --> 01:29:07,600
to prove relative loss bounds

1370
01:29:07,620 --> 01:29:09,920
so now i go on

1371
01:29:09,940 --> 01:29:12,290
i have one of those updates

1372
01:29:12,350 --> 01:29:16,890
one of those updates that look like this

1373
01:29:16,940 --> 01:29:19,540
this kind of update

1374
01:29:19,540 --> 01:29:24,460
and i do it on the tribe by trial basis

1375
01:29:24,500 --> 01:29:29,120
and now i want to found somehow the total loss of the algorithm

1376
01:29:33,750 --> 01:29:37,040
the loss looks something like this

1377
01:29:37,060 --> 01:29:42,100
it's the in it it depends on the instance and the weight vector

1378
01:29:42,120 --> 01:29:45,290
the divergence looks like this

1379
01:29:45,310 --> 01:29:47,330
they update looks like this

1380
01:29:47,480 --> 01:29:50,850
you can if you apply if you move it to the other side you apply

1381
01:29:50,850 --> 01:29:53,290
f minus one you get the normal kind of

1382
01:29:54,960 --> 01:30:00,330
OK so how do i get about in the online algorithms there is always some

1383
01:30:00,330 --> 01:30:04,500
kind of telescoping so how do you set up this telescoping

1384
01:30:05,120 --> 01:30:06,680
so you take

1385
01:30:06,690 --> 01:30:07,580
you're loss

1386
01:30:07,580 --> 01:30:08,480
of any

1387
01:30:08,520 --> 01:30:11,710
parameter vector

1388
01:30:11,710 --> 01:30:13,460
you expand that loss

1389
01:30:14,290 --> 01:30:16,180
so the taylor expansion

1390
01:30:18,790 --> 01:30:21,420
the current weight vector

1391
01:30:21,440 --> 01:30:23,310
rights issue of convexity

1392
01:30:23,330 --> 01:30:25,310
of the last which you need here

1393
01:30:25,330 --> 01:30:27,230
this is bigger than equal to

1394
01:30:27,230 --> 01:30:30,250
you get this kind of thing

1395
01:30:31,480 --> 01:30:35,690
here you have the gradient

1396
01:30:36,270 --> 01:30:40,000
this gradient here

1397
01:30:40,040 --> 01:30:44,080
you can replace by this formula by the you can use the update now

1398
01:30:44,140 --> 01:30:46,810
and get end up at this

1399
01:30:46,980 --> 01:30:48,850
but this is the box

1400
01:30:48,900 --> 01:30:53,210
remember that was one of those properties so i can get these three

1401
01:30:53,210 --> 01:30:55,580
divergences in here

1402
01:30:55,620 --> 01:30:57,390
so i took

1403
01:30:57,460 --> 01:31:03,180
convexity had a gradient of the loss from the gradient i plug in the update

1404
01:31:03,230 --> 01:31:06,890
now use the box formalized and up at this

1405
01:31:06,890 --> 01:31:11,730
and now you can see already that these two terms telescope and this is an

1406
01:31:11,730 --> 01:31:18,580
additional term so these two terms telescope

1407
01:31:18,600 --> 01:31:21,810
so some over trials

1408
01:31:21,870 --> 01:31:26,370
some of the trials here the tree divergences

1409
01:31:26,390 --> 01:31:28,390
and then what happens is

1410
01:31:28,410 --> 01:31:31,180
these two the first two terms telescope

1411
01:31:31,230 --> 01:31:33,190
this one is positive you throw it away

1412
01:31:33,210 --> 01:31:35,500
and then you have these terms that accumulate

1413
01:31:35,520 --> 01:31:38,020
so you end up with this very concise formula

1414
01:31:38,060 --> 01:31:41,790
loss of the algorithm

1415
01:31:41,850 --> 01:31:45,940
this is the last of the i'm try one two three up until trial t

1416
01:31:45,940 --> 01:31:48,790
loss of the compared

1417
01:31:48,890 --> 01:31:53,730
the distance of the compared to the initial weight vector last these

1418
01:31:53,790 --> 01:31:58,940
costs for changing your weight vector that accumulate which also involved

1419
01:31:59,000 --> 01:32:07,460
the bregman divergence

1420
01:32:13,060 --> 01:32:16,440
OK so i still have this final term

1421
01:32:17,370 --> 01:32:19,580
this one is still amassed

1422
01:32:20,330 --> 01:32:22,980
what i need to do is i need to relate this to the loss of

1423
01:32:22,980 --> 01:32:25,940
the algorithm which are not showing in this

1424
01:32:25,980 --> 01:32:28,290
case gets too complicated

1425
01:32:28,310 --> 01:32:32,350
and then you solve and you get the bound of this type were the loss

1426
01:32:32,350 --> 01:32:34,350
of the total loss of the algorithm

1427
01:32:34,370 --> 01:32:36,600
is always as good as the total loss of the

1428
01:32:36,640 --> 01:32:37,730
compared to

1429
01:32:37,730 --> 01:32:42,310
in the original piece of the editorial was to compare was always an expert now

1430
01:32:42,310 --> 01:32:45,040
the compared it could be a linear combination

1431
01:32:45,060 --> 01:32:47,140
plus the distance

1432
01:32:47,180 --> 01:32:48,790
of the initial weight vector

1433
01:32:49,690 --> 01:32:53,180
the start with that the of the compared to the start with vector and these

1434
01:32:53,180 --> 01:32:59,960
are constants that depend on the learning rate

1435
01:33:01,980 --> 01:33:04,160
very often you get bounds that

1436
01:33:04,180 --> 01:33:06,180
can be one so that

1437
01:33:06,190 --> 01:33:09,420
the total loss of the algorithm is at most the speed the total loss of

1438
01:33:09,420 --> 01:33:10,920
the compared

1439
01:33:12,000 --> 01:33:17,890
some scale terms

1440
01:33:17,910 --> 01:33:21,540
he is the kind of balance that you might get

1441
01:33:21,560 --> 01:33:26,690
yes you may incorporate again gradient descent we linear regression square loss

1442
01:33:26,710 --> 01:33:30,080
gradient descent motivated by euclidean distance squared

1443
01:33:30,080 --> 01:33:32,210
exponentiated gradient algorithm

1444
01:33:32,230 --> 01:33:34,710
offensive norm my

1445
01:33:34,730 --> 01:33:37,040
the kind of bound you get

1446
01:33:37,060 --> 01:33:40,480
are total loss of the algorithm

1447
01:33:40,500 --> 01:33:42,520
so almost as big as

1448
01:33:42,580 --> 01:33:45,850
total loss of the compared to times one plus e

1449
01:33:47,350 --> 01:33:51,140
sum norm term this at the two numbers of the instances times the two norm

1450
01:33:51,140 --> 01:33:54,060
of the weight vector

1451
01:33:54,080 --> 01:33:59,270
where the constants that depend the c depends on the learning rate

1452
01:33:59,290 --> 01:34:04,330
OK exponentiated gradient algorithm very similar form

1453
01:34:04,350 --> 01:34:08,870
except to pay different tool norms here

1454
01:34:08,910 --> 01:34:12,080
p norm algorithm which interpolates between two

1455
01:34:12,120 --> 01:34:16,830
that your corresponding peano menu norms so this would be the in the maximum infinity

1456
01:34:16,830 --> 01:34:20,460
norm of the instances one norm of the comparator

1457
01:34:24,940 --> 01:34:26,600
it turns out that

1458
01:34:26,660 --> 01:34:31,370
the only norm that is rotation invariant the two norms

1459
01:34:31,440 --> 01:34:39,000
members that the multiplicative problems were better when the target was sparse

1460
01:34:39,040 --> 01:34:41,060
and actually

1461
01:34:41,120 --> 01:34:43,080
and the instances were

1462
01:34:43,100 --> 01:34:49,020
bounded nicely between plus and minus one that is hard money examples the classical example

1463
01:34:49,020 --> 01:34:51,270
it turns out in this case

1464
01:34:51,310 --> 01:34:55,080
this product of two unknowns is very small was this product of two unknowns happens

1465
01:34:55,080 --> 01:34:57,210
to be very big

1466
01:34:58,480 --> 01:35:01,160
these two are norms actually show you

1467
01:35:01,160 --> 01:35:05,040
which of the two families are good in particular problem

1468
01:35:05,080 --> 01:35:07,020
if you have a practical problem

1469
01:35:07,060 --> 01:35:09,830
i always try both families

1470
01:35:09,890 --> 01:35:14,100
each bregman divergence is going to be given or family of all updates

1471
01:35:14,120 --> 01:35:16,520
but the main ones are always

1472
01:35:16,540 --> 01:35:26,040
gradient descent and exponentially gradient

1473
01:35:28,100 --> 01:35:34,480
questions at this point so what did i did what did i do

1474
01:35:34,480 --> 01:35:37,440
i introduced

1475
01:35:37,500 --> 01:35:41,710
a notion of divergence

1476
01:35:41,710 --> 01:35:42,890
of functions

1477
01:35:42,910 --> 01:35:44,180
we simply

1478
01:35:45,520 --> 01:35:47,220
we simply say that

1479
01:35:47,260 --> 01:35:51,280
since we don't know what our distribution is simply

1480
01:35:51,330 --> 01:35:53,730
place prior directly

1481
01:35:53,810 --> 01:35:55,630
on the distribution

1482
01:35:55,870 --> 01:35:59,770
so this prior distribution on distribution

1483
01:35:59,920 --> 01:36:02,000
and of course

1484
01:36:02,110 --> 01:36:03,720
this prior

1485
01:36:03,730 --> 01:36:06,770
we will be using a dirichlet process for this

1486
01:36:08,220 --> 01:36:11,050
the bayesian non parametric approaches

1487
01:36:11,070 --> 01:36:12,990
we say that our axis

1488
01:36:13,000 --> 01:36:16,010
is distributed according to some distribution at

1489
01:36:16,060 --> 01:36:17,310
which we don't know

1490
01:36:17,320 --> 01:36:20,990
so being bayesian we place a prior on

1491
01:36:21,040 --> 01:36:23,460
this being the dirichlet process prior

1492
01:36:24,240 --> 01:36:27,570
and then again we do exactly the same thing in which we

1493
01:36:27,780 --> 01:36:31,600
we can then compute the posterior distribution

1494
01:36:31,610 --> 01:36:33,690
over distribution at

1495
01:36:33,780 --> 01:36:35,620
the discount we

1496
01:36:36,900 --> 01:36:42,740
and the posterior is again given by the prior multiplied by the likelihood

1497
01:36:42,790 --> 01:36:47,510
normalized by the marginal likelihood of the data

1498
01:36:47,560 --> 01:36:52,010
and again we can do prediction in this case we simply want to play

1499
01:36:52,060 --> 01:36:55,300
the probability of x our even training

1500
01:36:55,310 --> 01:36:56,500
beta x

1501
01:36:56,520 --> 01:36:59,980
and again we integrate here

1502
01:37:00,260 --> 01:37:02,800
of course

1503
01:37:02,820 --> 01:37:04,650
after that

1504
01:37:04,680 --> 01:37:07,440
is the distribution we interested in those

1505
01:37:07,450 --> 01:37:11,270
the probability of x given as is simply

1506
01:37:11,280 --> 01:37:15,280
well actually that that you get is simply that you have

1507
01:37:15,380 --> 01:37:16,720
x y

1508
01:37:19,230 --> 01:37:21,630
actually this is not quite correct

1509
01:37:22,900 --> 01:37:26,220
i will come back to this point but

1510
01:37:26,240 --> 01:37:27,950
but the general idea so

1511
01:37:27,970 --> 01:37:32,340
the bayesian approach but the density estimation is

1512
01:37:33,430 --> 01:37:34,530
one two

1513
01:37:34,540 --> 01:37:38,560
we don't know that they are we don't know the underlying distribution from which the

1514
01:37:38,560 --> 01:37:39,870
they data come from

1515
01:37:39,890 --> 01:37:42,680
we simply place prior directly on

1516
01:37:42,690 --> 01:37:47,750
class of the distribution and then again can we apply to be a missionary

1517
01:37:47,770 --> 01:37:53,140
but so that's estimation

1518
01:37:53,230 --> 01:38:00,240
could actually is that it yes

1519
01:38:00,290 --> 01:38:04,550
OK so that's kind of the PCB which is actually

1520
01:38:04,600 --> 01:38:09,920
really that's what i've been talking about actually the question forgot to repeat what is

1521
01:38:09,920 --> 01:38:12,240
this at the distribution of them

1522
01:38:18,240 --> 01:38:21,960
in this case the way of writing this it is a distribution

1523
01:38:25,990 --> 01:38:29,720
if the distribution is more than half the that the and help

1524
01:38:29,740 --> 01:38:31,320
equivalent basically

1525
01:38:48,660 --> 01:38:51,860
i'm not quite sure what the question and the question was that you were saying

1526
01:38:51,870 --> 01:38:56,350
that they usually the distribution of the easy to estimate but not the density

1527
01:38:56,370 --> 01:39:02,250
but presumably believe that is true only in that one dimension although dimensions

1528
01:39:02,330 --> 01:39:05,500
it's probably not true for high dimensional

1529
01:39:06,660 --> 01:39:09,700
in which you have been in machine learning that

1530
01:39:09,710 --> 01:39:12,920
what well into our

1531
01:39:13,250 --> 01:39:15,860
come to that is that

1532
01:39:15,900 --> 01:39:20,130
the difference between the distribution later

1533
01:39:20,180 --> 01:39:24,120
right so that that information so here's a little

1534
01:39:24,540 --> 01:39:26,520
not sure what what

1535
01:39:26,530 --> 01:39:28,050
happens OK

1536
01:39:28,100 --> 01:39:30,660
so in our prior we may say

1537
01:39:33,310 --> 01:39:38,190
the red line so we we can draw samples from our distribution from the prior

1538
01:39:38,240 --> 01:39:39,240
and this being

1539
01:39:39,250 --> 01:39:42,510
the little each of the line here lot

1540
01:39:42,550 --> 01:39:44,320
actually the density of the

1541
01:39:44,330 --> 01:39:47,070
of the distribution to to be drawn from the prior

1542
01:39:47,280 --> 01:39:50,420
and you know they all now look

1543
01:39:50,480 --> 01:39:52,090
so this might be one

1544
01:39:52,140 --> 01:39:54,580
sample from the prior to look like that

1545
01:39:54,590 --> 01:39:59,320
and another example of this is mentioned line out of town

1546
01:39:59,370 --> 01:40:00,980
is it around the area

1547
01:40:01,780 --> 01:40:03,220
another one is that this

1548
01:40:03,230 --> 01:40:06,300
this blue line which has little who you can see

1549
01:40:09,670 --> 01:40:12,720
and the grey areas again held like the

1550
01:40:14,850 --> 01:40:18,290
fifty two ninety eight percent one of

1551
01:40:18,420 --> 01:40:20,520
and all of the

1552
01:40:20,540 --> 01:40:24,820
the prior over the over the density

1553
01:40:24,830 --> 01:40:27,640
the red line is the mean that he

1554
01:40:27,690 --> 01:40:29,520
and the blue is the medium

1555
01:40:29,520 --> 01:40:32,880
the new data point will tell us anything about what they really other data points are

1556
01:40:33,400 --> 01:40:35,200
if we don't know the location of the red line

1557
01:40:35,980 --> 01:40:37,490
and we know part of these data points

1558
01:40:37,940 --> 01:40:40,260
now whereas the next data point going to be we can tell

1559
01:40:40,820 --> 01:40:43,600
it's much more likely to be in this region then somewhere over here

1560
01:40:44,280 --> 01:40:46,490
a conditional unconditional they are not independent

1561
01:40:47,760 --> 01:40:52,470
okay so now the definition that we need all that the characterization that we need in order to

1562
01:40:54,160 --> 01:40:58,130
at a given criterion four fore this conditional independence something called exchangeability

1563
01:40:58,950 --> 01:41:00,420
and exchangeability is following

1564
01:41:01,570 --> 01:41:02,050
we see in

1565
01:41:02,730 --> 01:41:04,670
we assume that we see an infinite sequence of

1566
01:41:05,240 --> 01:41:06,000
random variables

1567
01:41:06,790 --> 01:41:09,540
you know that they there is an infinite sequence of such

1568
01:41:09,920 --> 01:41:11,050
blue dots here for example

1569
01:41:12,010 --> 01:41:12,480
and now we

1570
01:41:13,590 --> 01:41:15,950
they they have we look at the joint distribution of these

1571
01:41:16,780 --> 01:41:17,080
of these

1572
01:41:18,000 --> 01:41:18,510
of the stadium

1573
01:41:19,770 --> 01:41:20,440
and now we ask

1574
01:41:21,740 --> 01:41:24,630
what happens if we permute the order

1575
01:41:25,790 --> 01:41:27,540
in which the state of these data points coming

1576
01:41:29,690 --> 01:41:33,630
that the distribution is called exchangeable are also sequences called exchangeable

1577
01:41:35,000 --> 01:41:39,410
so the probability of seeing a particular sequence of data points doesn't depend

1578
01:41:40,000 --> 01:41:41,700
on this order so we can permute them

1579
01:41:42,840 --> 01:41:45,260
and it will not change the probability of occurrence and

1580
01:41:46,090 --> 01:41:46,880
so very simply

1581
01:41:47,310 --> 01:41:49,040
the order of the observations doesn't matter

1582
01:41:50,480 --> 01:41:51,300
this is a very nice

1583
01:41:53,880 --> 01:41:56,670
it's a very nice assumption for several reasons one is it's

1584
01:41:57,500 --> 01:41:59,350
it's certainly true that data is i i d

1585
01:42:00,770 --> 01:42:02,720
see that they decided we can shuffle it around

1586
01:42:03,820 --> 01:42:06,470
and so it's but it doesn't have to be i i

1587
01:42:07,240 --> 01:42:09,690
and so it's more in in a sense it's a more

1588
01:42:11,770 --> 01:42:14,760
it contains the assumption that we usually making parametric origin

1589
01:42:15,310 --> 01:42:18,050
nonparametric statistics the idea assumption a special case

1590
01:42:18,520 --> 01:42:19,710
okay so that's a good start

1591
01:42:23,790 --> 01:42:27,270
in in a way it's actually the same thing as the idea assumption but

1592
01:42:27,680 --> 01:42:29,170
let let's not get into that discussion

1593
01:42:30,420 --> 01:42:31,950
you know the thing is that it's

1594
01:42:33,700 --> 01:42:36,600
one thing is it's often true yeah you thing is it's also

1595
01:42:37,130 --> 01:42:38,420
relatively easy to argue

1596
01:42:39,250 --> 01:42:40,250
whether this is true or not

1597
01:42:40,690 --> 01:42:43,240
now if you know something about how your data is being generated

1598
01:42:43,870 --> 01:42:46,900
then it might be very hard to argue that the data is i i d e

1599
01:42:47,730 --> 01:42:51,970
but it's it's often it's reasonably easy to say okay the order in which the data come in

1600
01:42:52,910 --> 01:42:54,550
contains information are doesn't

1601
01:42:54,980 --> 01:42:58,120
you just you know if you know that the data generating process

1602
01:42:59,200 --> 01:43:00,520
okay so this is exchangeability

1603
01:43:02,720 --> 01:43:03,520
and now there's this

1604
01:43:04,160 --> 01:43:05,980
this theorem called the finetti theorem

1605
01:43:06,660 --> 01:43:07,100
which is

1606
01:43:09,280 --> 01:43:12,140
really want the the very basic theorems underlying

1607
01:43:13,750 --> 01:43:14,480
bayesian inference

1608
01:43:15,330 --> 01:43:16,750
and this this tells us

1609
01:43:17,610 --> 01:43:21,270
in short tells us the data is conditionally idea that exchangeable

1610
01:43:22,140 --> 01:43:22,730
if and only if

1611
01:43:23,350 --> 01:43:27,650
and so we can check whether this this conditional idea assumption whether that makes sense

1612
01:43:28,130 --> 01:43:31,330
by checking whether the order in which the data points coming matters not

1613
01:43:35,060 --> 01:43:37,320
and that's let's look at this in in a bit more detail

1614
01:43:37,820 --> 01:43:38,840
so what we see over here

1615
01:43:39,410 --> 01:43:41,600
it is again the joint distribution of the data points

1616
01:43:44,810 --> 01:43:46,410
what we have here on the other side is

1617
01:43:49,320 --> 01:43:50,980
integral over r

1618
01:43:51,120 --> 01:43:54,840
the parameter and i've now written the parameters he has a probability distribution

1619
01:43:55,660 --> 01:43:58,010
in the parameters not probability distribution

1620
01:43:59,620 --> 01:44:01,390
this year is again the prior distribution

1621
01:44:02,750 --> 01:44:06,250
and now the prior distribution is a distribution over probability distributions

1622
01:44:06,900 --> 01:44:09,780
okay so we sample a random probability distribution from this q

1623
01:44:10,970 --> 01:44:12,700
then we fix it and we sample

1624
01:44:14,580 --> 01:44:17,690
these points from that distribution that we've just generated around

1625
01:44:18,700 --> 01:44:19,660
and how do we sample them

1626
01:44:20,360 --> 01:44:22,450
i i d e because we have this product

1627
01:44:23,330 --> 01:44:24,310
the product society

1628
01:44:25,700 --> 01:44:27,430
okay so what this here says is

1629
01:44:28,480 --> 01:44:30,030
the distribution is exchangeable

1630
01:44:31,410 --> 01:44:32,430
if and only if

1631
01:44:33,580 --> 01:44:38,580
we can generated in this way and we put it is some distribution on probability distributions

1632
01:44:39,200 --> 01:44:42,790
we draw from that we draw probability distribution grand and fix it

1633
01:44:43,810 --> 01:44:44,900
and then sample data from

1634
01:44:44,900 --> 01:44:50,500
if you have to make it

1635
01:44:59,400 --> 01:45:02,660
quite which

1636
01:45:03,860 --> 01:45:08,460
so there like

1637
01:45:08,490 --> 01:45:09,470
when you

1638
01:45:16,200 --> 01:45:17,820
so what

1639
01:45:17,820 --> 01:45:20,420
so the back end

1640
01:45:20,940 --> 01:45:26,200
what makes a lot of

1641
01:45:26,430 --> 01:45:29,220
by first because it can help

1642
01:45:29,240 --> 01:45:30,190
in which

1643
01:45:32,010 --> 01:45:34,790
usually but always

1644
01:45:34,800 --> 01:45:38,240
great going

1645
01:45:39,670 --> 01:45:46,890
one of the great

1646
01:45:49,110 --> 01:45:51,040
o she had five

1647
01:45:55,430 --> 01:46:00,020
approximation calculate the canadian

1648
01:46:00,140 --> 01:46:02,230
completely have one

1649
01:46:06,700 --> 01:46:10,480
so the brain and

1650
01:46:15,200 --> 01:46:19,010
just based on the web

1651
01:46:22,930 --> 01:46:26,070
complex ran

1652
01:46:28,070 --> 01:46:31,280
some of the little bit better

1653
01:46:32,900 --> 01:46:36,200
it is one of the both of

1654
01:46:36,220 --> 01:46:37,950
pixels which

1655
01:46:43,220 --> 01:46:45,620
two weeks

1656
01:46:49,780 --> 01:46:52,890
so you know one

1657
01:46:55,150 --> 01:46:57,180
and these

1658
01:46:58,690 --> 01:47:01,890
one our over three

1659
01:47:03,880 --> 01:47:07,190
the british

1660
01:47:07,190 --> 01:47:08,740
x y

1661
01:47:08,750 --> 01:47:10,090
and it is

1662
01:47:11,670 --> 01:47:14,030
coli like this

1663
01:47:14,050 --> 01:47:15,530
x y

1664
01:47:15,800 --> 01:47:19,200
the former

1665
01:47:20,980 --> 01:47:26,380
it will used to just quickly

1666
01:47:30,370 --> 01:47:31,910
they can do it because

1667
01:47:35,320 --> 01:47:40,120
one the

1668
01:47:40,130 --> 01:47:43,400
and the great thing about

1669
01:47:43,420 --> 01:47:50,660
in addition to having

1670
01:47:53,910 --> 01:47:57,830
we're sorry

1671
01:47:58,650 --> 01:48:02,170
you can also look like right

1672
01:48:02,220 --> 01:48:04,680
and if you like

1673
01:48:09,010 --> 01:48:10,840
with me

1674
01:48:14,360 --> 01:48:16,170
but second

1675
01:48:16,190 --> 01:48:19,730
let let alone take a

1676
01:48:19,770 --> 01:48:22,840
because there

1677
01:48:23,040 --> 01:48:27,320
so with that

1678
01:48:27,660 --> 01:48:31,640
the the

1679
01:48:31,660 --> 01:48:35,240
because it

1680
01:48:39,490 --> 01:48:46,140
and how we can move on the fact that the

1681
01:48:46,140 --> 01:48:49,280
i can look like

1682
01:48:49,340 --> 01:48:52,440
we're like

1683
01:48:52,450 --> 01:48:53,920
mexican hat

1684
01:48:53,930 --> 01:48:58,080
we will explain why

1685
01:48:59,730 --> 01:49:02,040
you know what was

1686
01:49:02,090 --> 01:49:04,420
actually i was

1687
01:49:04,440 --> 01:49:05,290
so the biggest

1688
01:49:06,220 --> 01:49:09,170
two things the moment

1689
01:49:14,640 --> 01:49:18,460
and we have this

1690
01:49:18,490 --> 01:49:20,700
we have

1691
01:49:20,710 --> 01:49:24,450
which really and the biggest one

1692
01:49:24,470 --> 01:49:26,130
the bigger

1693
01:49:26,240 --> 01:49:29,700
and all the during

1694
01:49:29,850 --> 01:49:31,880
but the main

1695
01:49:31,900 --> 01:49:33,280
the reason why why

1696
01:49:33,350 --> 01:49:37,760
it's not a it's actually

1697
01:49:37,780 --> 01:49:40,950
ten years later

1698
01:49:41,040 --> 01:49:42,980
you're think

1699
01:49:43,000 --> 01:49:44,190
this is

1700
01:49:44,220 --> 01:49:47,970
what you to be each

1701
01:49:47,980 --> 01:49:50,000
but you have change

1702
01:49:51,880 --> 01:49:52,930
with small

1703
01:49:52,950 --> 01:49:56,880
changes then it will be totally removed by

1704
01:50:00,650 --> 01:50:02,200
look like

1705
01:50:03,710 --> 01:50:08,600
we can apply to what was

1706
01:50:14,520 --> 01:50:18,840
apply the floodlights equivalent to applying first

1707
01:50:19,480 --> 01:50:24,090
the population was

1708
01:50:24,100 --> 01:50:25,890
so you have

1709
01:50:25,910 --> 01:50:27,460
picture picturehouse

1710
01:50:27,490 --> 01:50:30,680
let's go look like this

1711
01:50:31,760 --> 01:50:34,050
all like nick

1712
01:50:35,730 --> 01:50:37,670
we try to be

1713
01:50:37,670 --> 01:50:41,840
it will be like so you can imagine is a whole

1714
01:50:41,860 --> 01:50:42,620
did you

1715
01:50:42,640 --> 01:50:43,740
because the image

1716
01:50:46,290 --> 01:50:48,550
we were we

1717
01:50:48,550 --> 01:50:53,840
in this class in this lecture we are going to discuss things that a little

1718
01:50:53,840 --> 01:50:58,260
bit more qualitative rather than quantitative

1719
01:50:58,380 --> 01:51:03,820
but it's the kind of concept that you would not have been able to understand

1720
01:51:03,820 --> 01:51:07,930
unless you went to the pain of the previous three lectures

1721
01:51:09,730 --> 01:51:16,470
what we've learned so far is that we need and the entire chemistry

1722
01:51:16,540 --> 01:51:19,770
are a very small minority of the universe

1723
01:51:19,790 --> 01:51:22,510
a lot of the universe is made by

1724
01:51:22,530 --> 01:51:28,530
some type of matter that we haven't seen directly yet and some type of energy

1725
01:51:28,530 --> 01:51:30,890
that is even weirder than that

1726
01:51:30,940 --> 01:51:36,150
and this is even made it to the the first page of the new zealand

1727
01:51:36,170 --> 01:51:40,550
say that ninety six percent of the universe is missing

1728
01:51:40,610 --> 01:51:42,790
so basically the

1729
01:51:42,800 --> 01:51:49,530
cosmology has what is called the standard cosmological model which sees the model described by

1730
01:51:49,530 --> 01:51:53,500
for example the cosmic pi that i'm showing you before

1731
01:51:53,550 --> 01:51:59,020
and do you know we know that each of these parameters to determine we percent

1732
01:52:01,130 --> 01:52:03,880
but there are some

1733
01:52:03,900 --> 01:52:09,930
things down there despite this impressive agreement of you ever more than a few parameters

1734
01:52:09,930 --> 01:52:16,720
you can describe the evolution of the universe seems to be a hundred and eighty

1735
01:52:16,720 --> 01:52:20,300
thousand years after the big bang and basically today

1736
01:52:20,330 --> 01:52:22,690
there's a lot we don't understand

1737
01:52:22,710 --> 01:52:28,520
and so all or embody if you can the impact these a little bit like

1738
01:52:29,160 --> 01:52:36,100
painting by designing the funniest woman up here and there is the concordance

1739
01:52:36,130 --> 01:52:41,880
OK everybody is happy that rangers but when you go and look this below the

1740
01:52:41,880 --> 01:52:49,770
surface case there's a lot of you know less angelic stuff going on

1741
01:52:49,770 --> 01:52:56,000
so let's go back to the twentieth century remember we discussed this before i einstein

1742
01:52:56,270 --> 01:53:02,400
that the friedmann equation to make the universe started because that's was on five to

1743
01:53:02,400 --> 01:53:03,330
from the

1744
01:53:03,390 --> 01:53:07,860
and interpretation of this stand was that the matter energy density is equal minus the

1745
01:53:07,860 --> 01:53:10,830
pressure and if you do that you get

1746
01:53:11,300 --> 01:53:15,610
and acceleration equation that you can make it equal to zero and the freeman equation

1747
01:53:15,610 --> 01:53:20,960
you can make the size to zero so you can make a static universe and

1748
01:53:20,960 --> 01:53:25,010
this is static universe will be positive because you

1749
01:53:25,020 --> 01:53:29,510
it's like a fine balance between all the terms and then you get

1750
01:53:29,550 --> 01:53:33,680
a static universe but you have to introduce this for

1751
01:53:36,460 --> 01:53:41,870
this was what einstein called it the greatest blunders of his life so in the

1752
01:53:41,870 --> 01:53:47,020
nineteen ninety i stand in the theory of relativity actually predict this function of the

1753
01:53:47,020 --> 01:53:50,680
universe right you have an unstable

1754
01:53:52,480 --> 01:53:57,680
mark will make the universe the expansion of the universe in oslo down and then

1755
01:53:57,700 --> 01:54:03,610
the collapse but seems the universe has collapsed yet and we have been around for

1756
01:54:03,610 --> 01:54:04,960
a pretty long time

1757
01:54:05,010 --> 01:54:12,210
then it's then it's more likely to say that this actually responding rather than you

1758
01:54:12,210 --> 01:54:18,830
know to collapse in on itself but again an expanding universe is not aesthetically pleasing

1759
01:54:19,890 --> 01:54:23,990
this was equation it like that so he had the time to make the universe

1760
01:54:23,990 --> 01:54:29,830
is static cosmological constant in the interpretation of this term in this equation is that

1761
01:54:29,830 --> 01:54:34,490
this land is the energy of empty space and this was discussed before

1762
01:54:34,520 --> 01:54:38,580
it's empathy and it's got an energy associated with a dozen lose because how can

1763
01:54:38,580 --> 01:54:40,430
you do something that is empty

1764
01:54:40,460 --> 01:54:42,780
it's empty already

1765
01:54:43,270 --> 01:54:48,890
and so this is the story of the vacuum energy over also called cosmological constant

1766
01:54:49,050 --> 01:54:54,960
that can edit and this is a picture from nineteen seventeen of einstein and matter

1767
01:54:54,960 --> 01:54:59,490
because the universities actually dominated by something like a vacuum energy goes under the name

1768
01:54:59,490 --> 01:55:03,800
of an american university because it was this guy that actually work it out and

1769
01:55:03,870 --> 01:55:07,100
in this guy was was the first priest

1770
01:55:07,130 --> 01:55:09,900
but it was really interested in science

1771
01:55:09,900 --> 01:55:14,460
so let's do some

1772
01:55:14,470 --> 01:55:17,730
thermodynamic arguments of

1773
01:55:17,770 --> 01:55:22,260
what wide islam this so we so imagine you have a piece them with some

1774
01:55:22,260 --> 01:55:28,880
vacuuming and this vacuum and some cosmological constant so the whoever volume v one in

1775
01:55:29,520 --> 01:55:35,540
and if there is some energy associated with it the energy corresponding to volume one

