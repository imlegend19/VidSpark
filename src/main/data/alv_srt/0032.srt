1
00:00:00,000 --> 00:00:01,350
to do

2
00:00:02,670 --> 00:00:07,400
his so this objective reasoning so we know the conclusion we know some premises to

3
00:00:07,400 --> 00:00:10,850
this conclusion i would like to add this is in the premises

4
00:00:11,230 --> 00:00:13,810
which would lead from

5
00:00:13,850 --> 00:00:17,130
known facts to the known conclusion

6
00:00:17,150 --> 00:00:18,480
and this is

7
00:00:18,480 --> 00:00:20,400
who has a motive so this would be

8
00:00:20,480 --> 00:00:23,850
for the translation of this kind of query

9
00:00:23,920 --> 00:00:25,500
so you see that

10
00:00:25,540 --> 00:00:27,480
answers are supported by

11
00:00:27,480 --> 00:00:30,770
effects from some of the public sources like CNN

12
00:00:31,710 --> 00:00:32,940
and so on

13
00:00:34,360 --> 00:00:38,980
i is you would be this hierarchy

14
00:00:41,770 --> 00:00:46,440
now we can also ask for justification while a way of fado

15
00:00:48,400 --> 00:00:51,270
had market for

16
00:00:51,330 --> 00:00:54,920
four such

17
00:00:54,920 --> 00:00:56,060
the killing

18
00:00:56,110 --> 00:00:59,400
and then we get

19
00:00:59,440 --> 00:01:00,400
sort of

20
00:01:03,250 --> 00:01:08,920
pretty much says so that he could is an advocate of living on lebanese economic

21
00:01:10,170 --> 00:01:14,040
OK that opposes lebanon lebanese economic reform

22
00:01:14,040 --> 00:01:17,330
and if you ask for

23
00:01:17,330 --> 00:01:22,460
so therefore OK that has a motive for the assassination of now if you ask

24
00:01:22,460 --> 00:01:25,750
for a slightly more detailed justification we would get

25
00:01:25,790 --> 00:01:27,230
down to very

26
00:01:27,420 --> 00:01:32,080
common sense type of rules so which we can reach

27
00:01:32,130 --> 00:01:38,730
so if some intelligent is really generic i mean would hold even for us so

28
00:01:38,810 --> 00:01:42,520
if some intelligent agent opposes some policy

29
00:01:42,630 --> 00:01:45,880
the reason is that

30
00:01:47,880 --> 00:01:51,040
for instance a good example

31
00:01:51,560 --> 00:01:52,750
and some other

32
00:01:52,790 --> 00:01:56,190
intelligent agents which by some

33
00:01:56,210 --> 00:01:57,420
kind of

34
00:01:57,480 --> 00:02:03,690
tim berners-lee victor skulking victim is an advocate of that policy

35
00:02:03,710 --> 00:02:05,230
monthly OK

36
00:02:05,250 --> 00:02:09,480
and some other intelligence agent about their

37
00:02:10,880 --> 00:02:14,830
this is responsible for

38
00:02:14,850 --> 00:02:19,920
four according to the policy OK

39
00:02:19,940 --> 00:02:23,330
and it is adopted by doctor in some of

40
00:02:24,480 --> 00:02:26,500
any some about five

41
00:02:29,580 --> 00:02:37,090
and some act prevents victim from playing the role key participant in any about part

42
00:02:42,990 --> 00:02:46,410
that's intelligent agent had the motive for act

43
00:02:46,570 --> 00:02:48,570
is the generic tool

44
00:02:48,610 --> 00:02:53,400
and pretty much what cyc found to get some so

45
00:02:53,450 --> 00:02:56,420
there are some more reasoning going behind the this is the key role which really

46
00:02:56,900 --> 00:02:58,660
find the answer

47
00:02:58,660 --> 00:03:04,710
and since this is about the reasoning far cyc

48
00:03:04,730 --> 00:03:09,530
i found that some of these facts i think the one right

49
00:03:09,530 --> 00:03:10,850
in the blue

50
00:03:10,850 --> 00:03:14,660
the blue of facts are true then

51
00:03:14,670 --> 00:03:17,740
this rule can be applied and therefore

52
00:03:17,750 --> 00:03:22,320
well we can connect some premises which we know from public sources

53
00:03:22,360 --> 00:03:25,110
and we know the conclusion that

54
00:03:25,120 --> 00:03:26,710
the not

55
00:03:29,200 --> 00:03:32,790
exactly and so therefore are

56
00:03:34,770 --> 00:03:42,700
so why are we so then pretty much we can connect all the bits and

57
00:03:42,700 --> 00:03:47,120
pieces and then cyc as i can provide some kind of well i would say

58
00:03:47,160 --> 00:03:50,450
really interesting and relevant and so this is one

59
00:03:50,530 --> 00:03:53,540
one such example of

60
00:03:53,590 --> 00:03:55,480
a relatively nontrivial

61
00:03:55,490 --> 00:04:00,060
three two real reasoning which as i can perform

62
00:04:00,070 --> 00:04:03,700
i think is the second example which i have

63
00:04:03,700 --> 00:04:05,520
so this is a summary no

64
00:04:06,670 --> 00:04:11,690
so what

65
00:04:11,950 --> 00:04:21,310
so pretty much the this is going to for family of the topics we have

66
00:04:21,310 --> 00:04:24,980
so i could also lecture with an equal infinity

67
00:04:25,000 --> 00:04:30,690
so we want they i one call circulants anymore other column templates because the diagonal

68
00:04:30,710 --> 00:04:32,290
go forever

69
00:04:32,350 --> 00:04:34,830
but if we have

70
00:04:36,440 --> 00:04:39,020
image which caused we do

71
00:04:39,100 --> 00:04:42,410
then there has to be some closure

72
00:04:42,690 --> 00:04:46,230
one way to close it one way to close it is

73
00:04:47,710 --> 00:04:49,390
make it periodic

74
00:04:49,390 --> 00:04:50,670
so this is the

75
00:04:50,690 --> 00:04:53,370
period periodic version of

76
00:04:53,390 --> 00:04:55,890
of a the filter

77
00:04:55,890 --> 00:04:58,770
this is the a filter with wraparound

78
00:04:59,870 --> 00:05:03,710
points coming to mind about that matrix

79
00:05:03,730 --> 00:05:08,980
i guess i do want to ask about one point it starts as usual my

80
00:05:09,020 --> 00:05:12,230
lecture is running a little over

81
00:05:12,330 --> 00:05:19,390
that's a matrix whose i can vectors i am interested in

82
00:05:19,430 --> 00:05:22,940
that's the matrix whose i can vectors are very important

83
00:05:25,170 --> 00:05:29,810
beauty is the eigenvectors of the same for every circle

84
00:05:29,830 --> 00:05:34,910
so there's a little little world of circulant matrices

85
00:05:35,430 --> 00:05:37,230
of a given size

86
00:05:38,960 --> 00:05:42,310
and its eigen vectors

87
00:05:44,080 --> 00:05:49,270
of everybody in the world this is that the world has the same eigen vectors

88
00:05:49,290 --> 00:05:50,910
so that's what i want

89
00:05:50,960 --> 00:05:57,410
so so so what i want to say i can vectors

90
00:05:57,430 --> 00:05:58,810
of of c

91
00:05:58,980 --> 00:06:00,640
of my search

92
00:06:01,810 --> 00:06:05,890
we haven't really understood the the

93
00:06:05,940 --> 00:06:06,890
right now

94
00:06:06,910 --> 00:06:11,690
i introduced circumstances the matrix vector shorthand

95
00:06:11,710 --> 00:06:13,750
to to produce

96
00:06:18,750 --> 00:06:22,640
we see might have been the course actually

97
00:06:22,710 --> 00:06:27,100
whatsoever wasting these matrices are really

98
00:06:29,710 --> 00:06:31,210
almost right

99
00:06:31,230 --> 00:06:34,390
in our in those first lectures

100
00:06:34,440 --> 00:06:36,330
the minus one is

101
00:06:36,980 --> 00:06:38,870
was minus one

102
00:06:38,870 --> 00:06:41,600
well but now that we see it

103
00:06:41,600 --> 00:06:45,910
with minus ones in the corners

104
00:06:45,930 --> 00:06:49,560
maybe in the notes with

105
00:06:49,580 --> 00:06:51,690
the periodic version

106
00:06:54,370 --> 00:06:58,370
one of the eigenvectors

107
00:06:58,390 --> 00:07:01,660
one one one is an i can make good good good

108
00:07:01,710 --> 00:07:02,540
i mean

109
00:07:02,560 --> 00:07:05,560
could be all right

110
00:07:05,640 --> 00:07:07,100
so the idea back to mean

111
00:07:07,140 --> 00:07:08,520
make a list of them

112
00:07:08,620 --> 00:07:10,980
is an organ of one one one

113
00:07:11,000 --> 00:07:15,960
how does everybody every was the i value what if i multiply this matrix by

114
00:07:15,960 --> 00:07:18,910
one one one

115
00:07:18,930 --> 00:07:24,040
let's just say yes sure enough is an organ vector multiply by one one one

116
00:07:24,230 --> 00:07:27,250
and what do i get

117
00:07:27,250 --> 00:07:32,000
so i'm really involving you could say with the one one one vector and what

118
00:07:35,060 --> 00:07:39,580
the eigen value is c zero plus c two c one

119
00:07:39,600 --> 00:07:43,980
times one one one

120
00:07:50,370 --> 00:07:53,370
every row it's the same number

121
00:07:54,250 --> 00:07:57,210
and what is the number well over the so

122
00:07:57,440 --> 00:08:02,870
i just want to ask you to identify what we thought we just discovered

123
00:08:02,890 --> 00:08:09,230
one of the vectors and its eigen value or one

124
00:08:09,250 --> 00:08:12,710
this i can value in this

125
00:08:12,730 --> 00:08:15,000
a column of my free matrix

126
00:08:15,020 --> 00:08:17,000
it's the d term

127
00:08:17,020 --> 00:08:19,120
it's the one one one column

128
00:08:19,120 --> 00:08:22,330
of my f three fourier matrix

129
00:08:22,350 --> 00:08:24,620
and so this is going to be the point

130
00:08:24,660 --> 00:08:29,170
the eigen vectors of c are the columns

131
00:08:29,230 --> 00:08:30,850
are are the

132
00:08:32,850 --> 00:08:34,250
the columns

133
00:08:37,430 --> 00:08:40,910
so as the matrix

134
00:08:40,910 --> 00:08:43,850
contains the eigen vectors

135
00:08:43,870 --> 00:08:48,640
of every circulated it'll diagonalizes circulant matrix

136
00:08:48,670 --> 00:08:50,430
in other words OK

137
00:08:50,440 --> 00:08:57,620
so what's happening now the this is so wonderful taken circa matrix

138
00:08:57,620 --> 00:09:02,000
OK so when using amplifiers you don't really have to worry about the details of

139
00:09:02,000 --> 00:09:03,520
maxwell's equations

140
00:09:03,640 --> 00:09:08,310
OK i'll give you some very simple abstract rules of behavior for an amplifier can

141
00:09:08,310 --> 00:09:10,240
go very interesting systems

142
00:09:11,200 --> 00:09:15,170
really really knowing how maxwell's equations applies applies to that

143
00:09:15,200 --> 00:09:18,150
because you will be working on this abstractly

144
00:09:18,160 --> 00:09:19,660
however since the engineers

145
00:09:19,670 --> 00:09:23,720
and you will buildings system is very important for you to understand how we make

146
00:09:23,720 --> 00:09:30,040
this leap from the laws of physics into some of the very primitive engineering abstractions

147
00:09:30,050 --> 00:09:34,730
so once you try and six double two

148
00:09:34,790 --> 00:09:38,720
but the basic to starts here

149
00:09:38,850 --> 00:09:42,740
you start from the laws of physics and then proceeds all the way out

150
00:09:42,850 --> 00:09:44,500
so once we

151
00:09:44,520 --> 00:09:46,720
talk about amplifiers

152
00:09:46,720 --> 00:09:49,620
with a two pass

153
00:09:51,660 --> 00:09:53,080
on the amplifier

154
00:09:53,150 --> 00:09:58,810
will build the next abstraction called the digital abstraction

155
00:09:58,860 --> 00:10:07,400
OK and the digital abstraction to build a new elements such as inverse and communist

156
00:10:08,660 --> 00:10:12,300
OK so small as to be of the building bigger and bigger things which have

157
00:10:12,300 --> 00:10:16,600
more and more complicated behavior inside of but which is very simple to describe

158
00:10:16,660 --> 00:10:20,790
right so following the digital abstraction

159
00:10:25,150 --> 00:10:30,270
so i propose the combinational logic abstract on top of that and define functional blocks

160
00:10:30,290 --> 00:10:33,810
that look like this some inputs some function

161
00:10:33,850 --> 00:10:36,120
some outputs

162
00:10:36,140 --> 00:10:39,930
the next abstract top of that will be

163
00:10:40,040 --> 00:10:42,200
the clocked digital abstraction

164
00:10:43,800 --> 00:10:48,120
we will have some notion of time introduced into the system

165
00:10:49,430 --> 00:10:50,980
and this will be some

166
00:10:52,100 --> 00:10:56,160
and it would be a clock that introduces time into

167
00:10:56,160 --> 00:10:59,750
so the logic values that functions operate upon

168
00:10:59,870 --> 00:11:01,800
following that

169
00:11:01,830 --> 00:11:03,890
the next level of abstraction that go

170
00:11:03,920 --> 00:11:06,710
it's called instruction set abstraction

171
00:11:06,720 --> 00:11:12,080
OK now you begin to see things that consumers get to look at

172
00:11:12,220 --> 00:11:17,750
can someone give me an example of a instruct or name of the instruction set

173
00:11:17,800 --> 00:11:20,090
so the fraction

174
00:11:20,100 --> 00:11:25,860
x being OK so x eighty six

175
00:11:25,870 --> 00:11:27,720
it is one to abstractions

176
00:11:27,830 --> 00:11:30,180
and in fact in many universities

177
00:11:30,220 --> 00:11:32,210
in many universities

178
00:11:32,250 --> 00:11:37,850
education could start just by saying OK here's an abstraction the x eighty six instructions

179
00:11:38,700 --> 00:11:43,040
some MIT gurus that design is often the microprocessor

180
00:11:43,060 --> 00:11:47,750
OK so you just worry about you take this abstraction the european assembly instructions and

181
00:11:47,750 --> 00:11:49,840
you're going both systems on top of that

182
00:11:49,850 --> 00:11:52,460
because it is an abstraction of the x eighty six layers

183
00:11:52,500 --> 00:11:57,050
the abstraction layers in six double four

184
00:11:57,100 --> 00:11:59,040
you will learn about the ability of

185
00:11:59,520 --> 00:12:03,310
the beta

186
00:12:03,410 --> 00:12:06,530
again is other attractions at this point

187
00:12:06,540 --> 00:12:08,700
so six w six table two

188
00:12:08,720 --> 00:12:11,090
kind of goes on here

189
00:12:11,100 --> 00:12:16,510
six the little dixie from the world of physics all the way the world of

190
00:12:16,510 --> 00:12:18,840
interesting analog and digital systems

191
00:12:18,850 --> 00:12:20,180
o double four

192
00:12:20,270 --> 00:12:26,630
course and computation structures which are notable computers all the way from simple objects all

193
00:12:26,640 --> 00:12:28,450
the way to a big systems

194
00:12:28,460 --> 00:12:32,010
following that you learn about

195
00:12:32,060 --> 00:12:34,550
language abstractions

196
00:12:34,560 --> 00:12:36,620
jealousy in other languages

197
00:12:36,680 --> 00:12:40,570
that is external to

198
00:12:40,620 --> 00:12:43,580
and there set of other courses that cover that

199
00:12:43,590 --> 00:12:44,610
following this

200
00:12:44,620 --> 00:12:46,850
little about surface system abstraction

201
00:12:46,920 --> 00:12:51,880
and so systems to learn about operating system

202
00:12:51,890 --> 00:12:57,720
an example of an operating system abstraction that people are there

203
00:12:57,730 --> 00:12:59,580
what's that

204
00:12:59,630 --> 00:13:01,180
the next

205
00:13:01,200 --> 00:13:04,530
what else

206
00:13:04,620 --> 00:13:06,670
i just wonder how long will left to go to the you know what i

207
00:13:06,670 --> 00:13:07,470
want to hear

208
00:13:15,830 --> 00:13:17,880
so if you want to suffer system

209
00:13:17,990 --> 00:13:23,760
these are nothing but abstractions simply implies a set of system calls

210
00:13:23,810 --> 00:13:25,400
that the programs listed here

211
00:13:25,420 --> 00:13:30,180
windows another set of system calls that's it and so much money they made

212
00:13:30,200 --> 00:13:35,120
OK it's all about abstraction layers all start from nature right but that

213
00:13:35,130 --> 00:13:38,000
fraction of an abstraction upon abstract one abstraction

214
00:13:38,010 --> 00:13:39,480
and not here

215
00:13:39,480 --> 00:13:41,310
o dollars

216
00:13:42,600 --> 00:13:47,880
so so probability that fractions we can then be useful things for human beings we

217
00:13:47,880 --> 00:13:48,990
can build up

218
00:13:49,020 --> 00:13:49,980
you know

219
00:13:50,030 --> 00:13:51,780
useful thing

220
00:13:51,800 --> 00:13:55,440
the game

221
00:13:55,620 --> 00:13:57,070
OK so

222
00:13:57,080 --> 00:14:01,220
OK we can send

223
00:14:01,230 --> 00:14:05,630
the space shuttle up and a whole bunch of other systems but it's based on

224
00:14:05,630 --> 00:14:07,390
these that's traction layers

225
00:14:07,450 --> 00:14:09,730
what's unique about education at MIT

226
00:14:09,750 --> 00:14:12,730
what's unique about six table two and ECS

227
00:14:12,780 --> 00:14:14,480
it is to my knowledge

228
00:14:14,530 --> 00:14:18,690
there are not many other places in the world where you will be educated you

229
00:14:18,690 --> 00:14:23,250
get an education in everything going all the way from nature two hard about very

230
00:14:23,250 --> 00:14:27,810
complicated analog and digital systems can be show you layer upon layer upon layer upon

231
00:14:28,530 --> 00:14:31,730
you know where the onion under you're down the raw nature

232
00:14:31,780 --> 00:14:33,490
OK so maxwell's equations

233
00:14:33,500 --> 00:14:37,020
six o two double for the three three

234
00:14:37,110 --> 00:14:42,600
OK six one seventy and and so on

235
00:14:42,610 --> 00:14:48,250
the whole ECS is about building an abstraction layers one atop the other

236
00:14:48,260 --> 00:14:49,810
so that's one pattern

237
00:14:49,830 --> 00:14:55,440
the and look back at all but to take an amplifier abstraction layer called the

238
00:14:56,960 --> 00:15:04,530
see also the look

239
00:15:04,710 --> 00:15:06,460
the amplifier

240
00:15:07,240 --> 00:15:08,350
the world

241
00:15:08,400 --> 00:15:12,950
and the operational amplifier in the animal or just slightly different ways of looking at

242
00:15:12,950 --> 00:15:14,580
the same same devices

243
00:15:14,620 --> 00:15:18,500
so the bill and lots accessible the operational amplifier

244
00:15:18,560 --> 00:15:20,920
and then you we go and up building

245
00:15:20,940 --> 00:15:24,460
a whole bunch of interesting

246
00:15:25,760 --> 00:15:27,830
system components

247
00:15:27,840 --> 00:15:31,370
OK these components might look like

248
00:15:33,380 --> 00:15:36,080
but look like filters

249
00:15:36,120 --> 00:15:38,400
OK to look like

250
00:15:38,430 --> 00:15:40,100
o supplies

251
00:15:40,120 --> 00:15:45,300
a whole bunch of very interesting abstract component which put together can then give you

252
00:15:45,310 --> 00:15:48,860
the next set of systems and these systems might be

253
00:15:48,870 --> 00:15:52,710
the posters

254
00:15:52,800 --> 00:16:01,340
all right often so for example other analog like you know by various control

255
00:16:01,380 --> 00:16:06,950
there is control systems for power plants and so on and so forth and ultimately

256
00:16:06,960 --> 00:16:11,260
in a fun and dollars

257
00:16:11,280 --> 00:16:12,440
OK so

258
00:16:13,240 --> 00:16:14,580
so sixty two

259
00:16:14,590 --> 00:16:17,950
it's about going from physics

260
00:16:18,020 --> 00:16:19,190
all the way

261
00:16:19,960 --> 00:16:24,770
this point will be build interesting and analog systems and you all the way up

262
00:16:25,350 --> 00:16:30,930
a interesting system components from which double forward take you all the way to building

263
00:16:30,930 --> 00:16:32,060
yeah that's true so

264
00:16:33,050 --> 00:16:35,330
i think in in the psychology literature

265
00:16:35,890 --> 00:16:38,780
it's certainly the case that once you've made the bet

266
00:16:39,530 --> 00:16:42,850
you believe anything much much more than before you made about

267
00:16:44,200 --> 00:16:45,030
is not rational

268
00:16:45,490 --> 00:16:47,870
because the bad actually give you no information

269
00:16:52,160 --> 00:16:53,660
so this is the dutch book theorem

270
00:16:55,600 --> 00:17:00,200
an economic argument fore we use the probability theory through the beliefs

271
00:17:01,080 --> 00:17:03,850
so let's talk about some of the theoretical properties over

272
00:17:05,300 --> 00:17:06,200
bayesian methods

273
00:17:07,260 --> 00:17:08,720
if you are worried about

274
00:17:09,200 --> 00:17:12,490
let's say well let's come back to the royal statistics a little bit

275
00:17:13,050 --> 00:17:15,140
on imagine the following scenario

276
00:17:17,370 --> 00:17:19,510
you have a data set dc and

277
00:17:20,370 --> 00:17:21,410
then data points

278
00:17:21,930 --> 00:17:23,600
and he was generated by some

279
00:17:25,450 --> 00:17:27,560
with some true parameter theta star

280
00:17:30,930 --> 00:17:33,320
now of course from a bayesian point of view

281
00:17:33,780 --> 00:17:37,140
i'm perfectly happy to accept that there is a true parameter out there

282
00:17:37,950 --> 00:17:45,700
bayesianism doesn't mean i believe the parameters random okay that's a really bad way of very incorrect way of describing

283
00:17:47,510 --> 00:17:49,620
i can believe there is a true parameter out there

284
00:17:50,910 --> 00:17:52,410
for example the parameter could be

285
00:17:53,050 --> 00:17:56,370
the number of people in this room that i'm trying to infer there's some true

286
00:17:56,390 --> 00:17:57,560
number of people in this room

287
00:17:58,830 --> 00:18:00,640
but i'm uncertain about their

288
00:18:02,700 --> 00:18:04,180
i'm going to represent

289
00:18:04,620 --> 00:18:11,680
my uncertainty by probability distribution so consider the following statistics kind of scenario where i

290
00:18:11,740 --> 00:18:16,450
a data set dc and consisting of n data points is generated from some true

291
00:18:16,450 --> 00:18:17,800
parameter theta star

292
00:18:21,510 --> 00:18:27,830
under some regularity conditions and that's careful the footnotes as careful with the regularity conditions

293
00:18:27,830 --> 00:18:29,990
these are just sketches of the theoretical results

294
00:18:30,990 --> 00:18:32,430
as long as

295
00:18:33,100 --> 00:18:34,550
i could

296
00:18:34,950 --> 00:18:37,160
to be more formal here i put some

297
00:18:37,760 --> 00:18:39,870
positive probability mass

298
00:18:40,350 --> 00:18:41,560
around theta star

299
00:18:46,300 --> 00:18:50,660
in this i've written it has be density and theta star is greater than zero

300
00:18:50,660 --> 00:18:54,180
but what i really mean is not just the density that stars good here but

301
00:18:54,180 --> 00:18:55,510
there is some probability mass

302
00:18:56,030 --> 00:18:57,430
around theta star

303
00:18:58,330 --> 00:18:59,050
a priority

304
00:19:01,580 --> 00:19:02,260
the limit

305
00:19:03,180 --> 00:19:07,600
has these amount of data goes to infinity over the posterior here

306
00:19:09,390 --> 00:19:12,050
delta function centred around the theta star

307
00:19:12,700 --> 00:19:13,760
so what does this mean

308
00:19:14,200 --> 00:19:15,010
this means there

309
00:19:17,240 --> 00:19:20,600
as long as i put some probability mass around

310
00:19:21,080 --> 00:19:23,200
all reasonable theta stars

311
00:19:23,830 --> 00:19:24,600
in the limit

312
00:19:25,160 --> 00:19:27,640
my posterior will converge to delta function

313
00:19:28,780 --> 00:19:32,220
so what are the young regularity what are the conditions that we need to worry

314
00:19:32,220 --> 00:19:35,180
about i won't go into details but what are the conditions is

315
00:19:36,050 --> 00:19:39,180
for example identifiability of my model i need to be able to

316
00:19:39,620 --> 00:19:44,640
make sure there are multiple different fade that are all equivalent to each other for example

317
00:19:46,550 --> 00:19:48,470
all the status can be uniquely identified

318
00:19:49,060 --> 00:19:53,160
so these are similar to the regularity conditions that you need four r

319
00:19:53,370 --> 00:19:56,850
on consistency of classical estimators as well

320
00:19:57,300 --> 00:19:58,450
so any questions about the

321
00:19:59,720 --> 00:20:03,600
you can read about it i e

322
00:20:09,260 --> 00:20:11,620
i think you can generalize this to

323
00:20:15,140 --> 00:20:17,580
the more general case and non ninety data

324
00:20:18,120 --> 00:20:18,800
but then the

325
00:20:19,450 --> 00:20:20,930
conditions become more complicated

326
00:20:24,580 --> 00:20:25,030
so now

327
00:20:25,600 --> 00:20:30,050
this is a nice this is essentially a case a lot of people worry about

328
00:20:30,050 --> 00:20:31,800
how to choose the prior this says

329
00:20:32,300 --> 00:20:38,430
if you choose your prior so that you put probability mass near all reasonable status that many such priors right

330
00:20:39,010 --> 00:20:40,390
there isn't one right pryr

331
00:20:41,370 --> 00:20:47,200
there's no such thing but if you choose a prior that puts some probability mass near most reasonable fade us

332
00:20:48,010 --> 00:20:48,780
then don't worry

333
00:20:49,870 --> 00:20:50,080
you know

334
00:20:51,430 --> 00:20:52,930
in the limit you'll just

335
00:20:53,680 --> 00:20:55,890
find find out what the right feeders yeah

336
00:20:56,640 --> 00:20:57,410
the rest of the

337
00:21:01,740 --> 00:21:05,410
well if you have too simple a model and let me give you a concrete example

338
00:21:05,930 --> 00:21:07,140
let's take polynomials

339
00:21:07,680 --> 00:21:08,990
if i put my whole

340
00:21:11,120 --> 00:21:12,160
prior mass

341
00:21:12,760 --> 00:21:14,220
on linear polynomials

342
00:21:14,700 --> 00:21:16,850
my data actually came from a quadratic

343
00:21:17,830 --> 00:21:20,100
then it doesn't matter how much data they observe

344
00:21:20,490 --> 00:21:25,430
i put all my mass on linear polynomials our learned the best possible linear polynomial there is

345
00:21:26,580 --> 00:21:28,350
that's exactly what the second point says

346
00:21:28,830 --> 00:21:31,010
so any unrealizable case

347
00:21:31,550 --> 00:21:33,990
where the data was generated by some key

348
00:21:34,410 --> 00:21:38,080
so the next which cannot be modelled by any theta for example

349
00:21:38,510 --> 00:21:39,620
eight quadratic

350
00:21:41,010 --> 00:21:42,700
that can be modeled by any linear here

351
00:21:44,280 --> 00:21:48,010
then the posterior will converge to some theta hat

352
00:21:48,010 --> 00:21:49,530
just two

353
00:21:52,860 --> 00:21:54,230
we like

354
00:22:01,880 --> 00:22:06,920
the problem is that

355
00:22:26,130 --> 00:22:27,590
when new

356
00:22:33,550 --> 00:22:35,280
this change

357
00:22:35,320 --> 00:22:38,440
and you i

358
00:22:39,170 --> 00:22:42,710
change the world

359
00:22:42,730 --> 00:22:44,780
can we

360
00:22:44,800 --> 00:22:45,920
and i

361
00:22:45,940 --> 00:22:51,050
what about what it does

362
00:22:51,070 --> 00:22:57,360
what i probably get lot

363
00:23:05,250 --> 00:23:13,340
but so that was the not

364
00:23:13,360 --> 00:23:19,300
one of the

365
00:23:19,300 --> 00:23:25,480
what should

366
00:23:25,480 --> 00:23:31,860
right so we believe that run by the same role

367
00:23:35,900 --> 00:23:36,650
what about

368
00:23:37,840 --> 00:23:41,840
this is correct

369
00:23:41,860 --> 00:23:47,750
i have no problem with that so he said

370
00:24:06,860 --> 00:24:12,440
the question is if you look

371
00:24:12,460 --> 00:24:14,610
like know not just

372
00:24:16,300 --> 00:24:20,130
all of the you

373
00:24:21,250 --> 00:24:22,550
what we like

374
00:24:22,570 --> 00:24:33,300
that it's very unlikely to be so the are

375
00:24:33,320 --> 00:24:35,210
it's kind of

376
00:24:35,230 --> 00:24:38,130
but not this is

377
00:24:40,960 --> 00:24:46,270
is an issue

378
00:24:49,730 --> 00:24:52,280
he was

379
00:24:53,730 --> 00:25:08,860
say what you may be assuming in the same

380
00:25:12,010 --> 00:25:17,900
but actually

381
00:25:20,190 --> 00:25:21,650
prior to

382
00:25:21,670 --> 00:25:24,900
it's not

383
00:25:25,730 --> 00:25:28,340
that's right in

384
00:25:33,900 --> 00:25:42,030
well i think about it you feel that the number of points on the main

385
00:25:45,500 --> 00:25:49,500
the problem of seeing

386
00:25:49,670 --> 00:25:53,360
he has assuming that you need

387
00:25:53,380 --> 00:25:56,960
you know what i need to make predictions about

388
00:25:56,980 --> 00:25:59,360
i don't know what they are

389
00:25:59,840 --> 00:26:06,630
so that we can only see three

390
00:26:06,630 --> 00:26:07,780
o point

391
00:26:07,800 --> 00:26:10,480
what's happening is that

392
00:26:13,320 --> 00:26:17,840
they can be made into a hundred point

393
00:26:22,110 --> 00:26:28,840
that's why you know that when you can find

394
00:26:28,900 --> 00:26:30,050
but what

395
00:26:30,070 --> 00:26:31,910
we but want to predict

396
00:26:33,880 --> 00:26:40,000
i'd like to do all we know is one of the gas density

397
00:26:40,050 --> 00:26:46,090
the three-dimensional space of the observation that may

398
00:26:50,150 --> 00:26:53,840
so next let's show you some pictures

399
00:26:53,860 --> 00:26:55,000
you should

400
00:26:55,000 --> 00:26:57,500
do not just one

401
00:26:57,500 --> 00:27:01,820
being one in the state of

402
00:27:03,750 --> 00:27:07,380
one two

403
00:27:08,770 --> 00:27:12,800
so there are these

404
00:27:12,820 --> 00:27:15,820
process is the

405
00:27:16,480 --> 00:27:21,880
they a rule by

406
00:27:23,010 --> 00:27:24,730
seven three

407
00:27:24,730 --> 00:27:27,960
thank you very much for the invitation to speak here

408
00:27:28,000 --> 00:27:33,530
it's not my usual activity to talk to machine learning people but with a bit

409
00:27:33,530 --> 00:27:36,500
of luck i learned something about the interface

410
00:27:36,590 --> 00:27:42,620
between probability and statistics and machine learning during the my time here

411
00:27:43,870 --> 00:27:49,800
levy processes well that that's where my research is and the application that i have

412
00:27:49,800 --> 00:27:54,390
looked into a little bit of this in mathematical finance so i'll give this a

413
00:27:54,390 --> 00:27:59,320
little bit of finance flavour when it comes to to the modelling part of the

414
00:28:00,530 --> 00:28:02,380
but i'll first

415
00:28:02,390 --> 00:28:04,140
spend some time

416
00:28:05,390 --> 00:28:09,750
and looking at the structure of the processes so that you get the feeling for

417
00:28:10,180 --> 00:28:16,450
so what the modelling freedom really is and how we are modelling using live processes

418
00:28:16,460 --> 00:28:21,250
so this is going to have to involve some form of the technical

419
00:28:21,250 --> 00:28:26,060
material related to infinite divisibility and the le vy khintchine formula

420
00:28:26,100 --> 00:28:30,480
but then really with a view towards constructing

421
00:28:30,870 --> 00:28:34,850
and assimilating these these levy processes

422
00:28:34,870 --> 00:28:39,150
when doing inference later it it is going to be very useful to have some

423
00:28:39,150 --> 00:28:43,590
parametric families at our disposal so i will review some

424
00:28:43,620 --> 00:28:48,510
some of these parametric families that have been in studied and used for for applications

425
00:28:48,510 --> 00:28:53,820
particularly in finance but there is no no reason why that would be the

426
00:28:53,840 --> 00:28:56,590
the application that that you have in mind

427
00:28:56,610 --> 00:29:01,560
so in one this is this is the main part of the talk

428
00:29:01,560 --> 00:29:06,920
and when i then turned to two modelling and inference out just look

429
00:29:06,960 --> 00:29:07,900
in in

430
00:29:07,950 --> 00:29:14,210
very very briefly about model is more general than the processes where the processes play

431
00:29:14,230 --> 00:29:20,070
a key role as as driving process of of some some other stochastic processes that

432
00:29:21,250 --> 00:29:25,420
can model financial price processes or other quantities

433
00:29:26,170 --> 00:29:30,070
and if this time then i'll talk a little bit about quadratic variation

434
00:29:30,090 --> 00:29:34,230
and the the problem of of estimating

435
00:29:34,260 --> 00:29:37,260
on making inferences on on volatility

436
00:29:37,290 --> 00:29:40,130
and financial

437
00:29:40,860 --> 00:29:42,450
and then

438
00:29:42,520 --> 00:29:46,470
well this this last bit would be some some of the work that i've been

439
00:29:46,470 --> 00:29:47,630
involved in

440
00:29:49,070 --> 00:29:52,570
volatility inference in particular in the in the presence

441
00:29:52,580 --> 00:29:55,020
of jobs

442
00:29:55,040 --> 00:29:57,940
OK so this is true for some

443
00:29:57,950 --> 00:29:59,630
some motivation

444
00:30:02,260 --> 00:30:08,990
our lady processes being studied while there's bottom-up reason that you may want to have

445
00:30:08,990 --> 00:30:10,390
continuous time

446
00:30:10,390 --> 00:30:13,890
analogues are limits of random walks

447
00:30:13,910 --> 00:30:19,730
or indeed generalisations of brownian motion realizing that brownian motion is maybe not the very

448
00:30:19,730 --> 00:30:24,630
best model for the kind of data that you want to fit has also top-down

449
00:30:24,630 --> 00:30:30,800
reason for studying the processes if you're looking at larger classes of stochastic processes is

450
00:30:30,800 --> 00:30:34,290
might markov processes are semimartingales

451
00:30:34,300 --> 00:30:39,420
that that you look at for various reasons such as simple dependence structure or that

452
00:30:39,420 --> 00:30:43,240
you want to do some stochastic calculus with them

453
00:30:43,260 --> 00:30:48,480
then the process of prototypes of these and actually a very good idea

454
00:30:48,520 --> 00:30:49,640
to study them

455
00:30:49,860 --> 00:30:54,760
first and then moving to this more general classes of processes if needed

456
00:30:55,770 --> 00:31:00,980
other than this historically the main applications have been insurance ruin

457
00:31:00,990 --> 00:31:02,790
storage problems

458
00:31:02,800 --> 00:31:04,730
dams in particular

459
00:31:04,730 --> 00:31:10,290
that's been driving the very early research in direction of the processes

460
00:31:11,020 --> 00:31:16,490
the process of being more general context of markov processes as a means of turning

461
00:31:16,490 --> 00:31:23,130
one markov process into another markov process by time change called subordination

462
00:31:23,140 --> 00:31:28,450
and we can study the process is also because they're related to other markov processes

463
00:31:28,450 --> 00:31:34,920
that you may be interested in the test branching processes or self similar markov processes

464
00:31:34,950 --> 00:31:36,270
more recently

465
00:31:36,290 --> 00:31:38,570
the applications

466
00:31:38,580 --> 00:31:41,080
that have driven recent research

467
00:31:41,090 --> 00:31:45,930
and put the process is very much at the forefront of

468
00:31:46,710 --> 00:31:53,660
of visibility and essentially is they come from financial modeling

469
00:31:53,670 --> 00:31:58,380
it's got no normal returns and the black scholes model says that normal

470
00:31:58,400 --> 00:32:04,250
so this leads to to all sorts of undesirable effects such as volatility smiles

471
00:32:04,270 --> 00:32:09,190
and which are in contradiction with the with the model that that's being used solely

472
00:32:09,190 --> 00:32:14,660
the processes are a means of addressing such such problems and you can go further

473
00:32:14,730 --> 00:32:22,220
stochastic volatility is another such keyword from financial modeling and leverage effects so there are

474
00:32:22,220 --> 00:32:27,020
models around based on the processes that can address these

475
00:32:27,040 --> 00:32:32,970
these features that that one would like to model in in financial applications

476
00:32:33,040 --> 00:32:37,850
so i put at the very bottom something that's that's also close to my research

477
00:32:37,850 --> 00:32:41,930
but i'm not going to touch upon this which is that the importance of living

478
00:32:41,940 --> 00:32:49,520
processes in population and phylogenetic models where particularly for for large trees that one one

479
00:32:49,520 --> 00:32:50,620
looks at their

480
00:32:50,670 --> 00:32:52,620
one can find

481
00:32:52,650 --> 00:33:00,250
levy process descriptions or interpretations of the of the results in in the area

482
00:33:00,270 --> 00:33:06,440
so the structure of the processes we should define the processes

483
00:33:06,500 --> 00:33:08,750
as follows

484
00:33:08,770 --> 00:33:15,870
we are looking at a continuous time stochastic processes with three properties stationarity of increments

485
00:33:15,870 --> 00:33:20,000
independence of increments and catalan happens

486
00:33:20,030 --> 00:33:24,110
and what this means is that if we look at an increment

487
00:33:24,170 --> 00:33:31,730
of lag is overly presses then its distribution will only depend on s

488
00:33:31,780 --> 00:33:33,870
and that being true for all t

489
00:33:33,880 --> 00:33:35,000
and this

490
00:33:35,020 --> 00:33:40,330
is what we call stationarity of increments independence of increments is looking at

491
00:33:40,350 --> 00:33:46,920
increments over disjoint time intervals i've chosen to do look at consecutive

492
00:33:48,600 --> 00:33:51,660
and we require these to be independent

493
00:33:51,670 --> 00:33:53,150
random variables

494
00:33:53,160 --> 00:33:55,180
so we really building model

495
00:33:55,280 --> 00:34:00,000
from from rather basic independent random variables

496
00:34:00,030 --> 00:34:00,830
and we

497
00:34:00,840 --> 00:34:06,370
we can then specify a joint distributions based on these two

498
00:34:06,370 --> 00:34:10,630
so thank you for the invitation had to be here and must that well i

499
00:34:10,630 --> 00:34:13,490
didn't do anything really for the summer school this was all

500
00:34:13,510 --> 00:34:14,460
a lot of

501
00:34:14,500 --> 00:34:15,960
organizing committee

502
00:34:16,620 --> 00:34:21,950
you would see that large values of theta are necessary to drive distribution close to

503
00:34:22,790 --> 00:34:25,110
a close to zero

504
00:34:25,270 --> 00:34:28,250
so the probability close to one of first here

505
00:34:28,340 --> 00:34:32,810
so i'm not saying that this is necessary in a very good prior

506
00:34:32,880 --> 00:34:37,420
but if you have no idea is not actually not to be a bad thing

507
00:34:37,430 --> 00:34:40,960
i'll be discussing conjugate priors in the moment

508
00:34:42,180 --> 00:34:45,740
will get back to what this is actually not such a stupid idea

509
00:34:45,760 --> 00:34:48,690
and the research talk

510
00:34:49,970 --> 00:34:52,320
OK so let's see how it works

511
00:34:52,340 --> 00:34:53,930
the blue things

512
00:34:53,970 --> 00:34:57,010
otherwise you get through that selective restoration

513
00:34:57,020 --> 00:34:58,090
so it

514
00:34:58,100 --> 00:35:00,670
it's exactly the same but what is so before

515
00:35:00,710 --> 00:35:05,700
the bias of what get by using maximum posterior estimation

516
00:35:07,340 --> 00:35:09,840
now what you see is well

517
00:35:09,930 --> 00:35:12,140
i did not serve any force you

518
00:35:12,190 --> 00:35:14,440
nonetheless the bayesian procedure

519
00:35:15,320 --> 00:35:17,750
penalize look like procedure

520
00:35:17,850 --> 00:35:20,550
if you want if you want to be pedantic

521
00:35:20,560 --> 00:35:24,840
does something quite useful here so it still gives it the benefit of doubt that

522
00:35:24,840 --> 00:35:29,840
says well maybe it is enough to say that actually applied to this

523
00:35:30,070 --> 00:35:34,490
down these extreme probabilities as well

524
00:35:34,510 --> 00:35:38,550
the think that you should see is as we get more and more data

525
00:35:38,810 --> 00:35:42,300
the the blue and the red curve get very close to each other

526
00:35:42,950 --> 00:35:47,330
actually what you see from this is i probably didn't use the strong enough prior

527
00:35:47,570 --> 00:35:50,310
actually now that this was

528
00:35:50,360 --> 00:35:51,520
fair dice

529
00:35:51,530 --> 00:35:53,580
these are constructed this way

530
00:35:53,600 --> 00:36:01,010
and i would want that this actually ways things even will considerably evenly here

531
00:36:02,040 --> 00:36:03,620
OK but

532
00:36:03,670 --> 00:36:06,640
so this tells you that the bayesian procedure

533
00:36:06,650 --> 00:36:07,710
in this case

534
00:36:07,720 --> 00:36:10,930
is not a magic bullet but it can improve things quite a bit

535
00:36:17,530 --> 00:36:18,940
well there another prime

536
00:36:18,950 --> 00:36:20,920
i'm just going to mention briefly on

537
00:36:21,010 --> 00:36:22,320
two slides

538
00:36:22,330 --> 00:36:24,890
this this thing called conjugate priors

539
00:36:24,990 --> 00:36:29,100
and mentioning it because quite often you would treat some statistics paper and then we'll

540
00:36:29,100 --> 00:36:34,270
talk about say OK here's the binomial distribution and then the use of conjugate distribution

541
00:36:34,270 --> 00:36:38,630
like to do is actually level of lower we estimate the normal distribution and then

542
00:36:38,650 --> 00:36:42,040
use that contradict is charge and so on and so on and so on

543
00:36:42,050 --> 00:36:45,480
it's quite intimidating initially when you read this

544
00:36:45,570 --> 00:36:49,050
single how did they pull this out of the hat

545
00:36:49,120 --> 00:36:54,330
it's actually very very easy procedure hiking at the conjugate distribution if you know its

546
00:36:54,330 --> 00:36:58,580
initial families sufficient to dealing with

547
00:36:58,590 --> 00:37:03,730
that's all have fun funny names and they all come with funny parameterizations

548
00:37:03,780 --> 00:37:05,890
probably the first thing you might want to do it

549
00:37:05,940 --> 00:37:10,920
some paper talks about this just strike back in the standard as exponential families for

550
00:37:10,920 --> 00:37:13,840
minimizing is actually not very scary

551
00:37:13,970 --> 00:37:18,540
so for instance like read of the chinese restaurant process

552
00:37:18,660 --> 00:37:22,740
in this case you have a certain self-reinforcing properties now

553
00:37:22,750 --> 00:37:26,700
to say these et al explain this equation afterwards but basically this is

554
00:37:27,180 --> 00:37:31,920
three close what the chinese restaurant process will do as it sees new observations

555
00:37:34,080 --> 00:37:35,930
and the idea is very simple

556
00:37:35,940 --> 00:37:39,400
so what we actually have michael serious

557
00:37:39,440 --> 00:37:43,270
will look different from the light

558
00:37:43,320 --> 00:37:46,090
this is maybe not such a good idea

559
00:37:46,110 --> 00:37:51,370
because consequences if you go out with your friends and play a lot of

560
00:37:51,450 --> 00:37:56,820
but again with sources then you will just have seen a lot of thought

561
00:37:57,610 --> 00:38:00,820
you might think that it's a good way of

562
00:38:00,860 --> 00:38:03,310
representing your prior knowledge

563
00:38:03,560 --> 00:38:07,190
in the form of a lot of things after that you've seen

564
00:38:07,240 --> 00:38:11,700
like you've played with them before maybe ten times so maybe overall you toss the

565
00:38:11,700 --> 00:38:15,470
dice a hundred times before i play with them again in it also ten times

566
00:38:15,730 --> 00:38:20,080
but actually it's as if he's still remembered those one hundred times you toss the

567
00:38:20,080 --> 00:38:22,090
dice before

568
00:38:22,140 --> 00:38:25,450
that's a convenient way of representing prior knowledge

569
00:38:25,460 --> 00:38:28,260
just an extra

570
00:38:29,970 --> 00:38:31,800
what i want is

571
00:38:31,850 --> 00:38:39,540
that p franchise for x has the similar functional form sp affixed patches by theta

572
00:38:43,160 --> 00:38:46,490
PFC parameters by some coefficients a so

573
00:38:46,500 --> 00:38:50,510
i'm just guessing the solution and i'll show you that this does the trick

574
00:38:50,520 --> 00:38:53,720
just prior to this into the approach between states

575
00:38:53,900 --> 00:38:55,770
and sometimes a

576
00:38:55,780 --> 00:38:56,850
my sincere

577
00:38:56,870 --> 00:38:58,780
i'm still state

578
00:38:58,950 --> 00:39:02,990
which explained the whiteboard what's going on

579
00:39:03,000 --> 00:39:09,660
so really exponential families tradition

580
00:39:09,750 --> 00:39:12,930
here x parameterized by theta

581
00:39:15,130 --> 00:39:17,030
into the inner product between

582
00:39:17,110 --> 00:39:18,950
forty six

583
00:39:18,960 --> 00:39:21,380
and theta

584
00:39:23,380 --> 00:39:26,380
g off

585
00:39:27,780 --> 00:39:32,300
in particular if we had a lot of start

586
00:39:32,340 --> 00:39:33,680
this expression here

587
00:39:34,160 --> 00:39:37,520
will be replaced by

588
00:39:41,380 --> 00:39:44,580
and times new

589
00:39:44,590 --> 00:39:47,110
this will be replaced by

590
00:39:52,130 --> 00:39:54,610
so now i can write this

591
00:39:54,660 --> 00:39:56,770
also as

592
00:39:56,810 --> 00:39:59,080
the difference between the picture

593
00:40:01,650 --> 00:40:04,540
g of theta

594
00:40:12,560 --> 00:40:16,350
m times you had to

595
00:40:29,340 --> 00:40:30,920
now what is this good for

596
00:40:30,930 --> 00:40:34,640
well this looks like the product

597
00:40:34,650 --> 00:40:36,880
well this is not my feature much

598
00:40:36,940 --> 00:40:38,620
so this

599
00:40:38,700 --> 00:40:40,750
looks like

600
00:40:42,010 --> 00:40:44,910
five of satan now

601
00:40:44,930 --> 00:40:47,080
this looks like some other parameter

602
00:40:52,550 --> 00:40:54,320
so what if

603
00:40:54,340 --> 00:40:56,350
i think some distribution

604
00:40:56,370 --> 00:40:58,360
on the scissors

605
00:40:58,450 --> 00:41:00,870
when my five seater looks like this

606
00:41:01,070 --> 00:41:04,780
and have some number here

607
00:41:04,860 --> 00:41:08,460
this is precisely what's happening over there

608
00:41:08,460 --> 00:41:13,560
the following content is provided under a Creative Commons license your support will help MIT

609
00:41:13,560 --> 00:41:17,860
OpenCourseWare continue to offer high-quality educational resources for free

610
00:41:18,350 --> 00:41:22,720
to make a donation or to view additional materials from hundreds of MIT courses

611
00:41:23,190 --> 00:41:27,840
visit MIT OpenCourseWare at ocw . MIT . EDU

612
00:41:31,670 --> 00:41:36,120
1 draw your attention to the fact that the final exam schedule is out for

613
00:41:36,120 --> 00:41:41,550
about a week and a three-line 1 final will be on Wednesday finals week which

614
00:41:41,550 --> 00:41:46,130
puts that on the morning of the 15th of December of the 3 hours

615
00:41:46,480 --> 00:41:51,760
extravaganza celebration of learning where you all come to show me what you've mastered

616
00:41:52,090 --> 00:41:56,190
and it's going to be a just a grand grand time

617
00:41:57,970 --> 00:42:02,110
and it'll be over the Johnson athletics and 5 so I urge you to take

618
00:42:02,110 --> 00:42:06,050
a look at the final exam schedule and make your travel plans as soon as

619
00:42:06,050 --> 00:42:08,930
you know what you're last obligation because

620
00:42:09,190 --> 00:42:14,040
things are get books and it's going to be harder and harder to get out

621
00:42:14,040 --> 00:42:19,960
of here and certainly get out of a decent price of last day we looked

622
00:42:19,960 --> 00:42:26,460
at Electron Domain theory which allowed us to predict the shapes of molecules and this

623
00:42:26,460 --> 00:42:32,040
is taken from the book The sustainable for 3 highlighted in orange we work through

624
00:42:32,040 --> 00:42:38,650
some of these molecules last day the SF six BR 5 I O 4 minus

625
00:42:38,650 --> 00:42:42,210
and we looked at the series CH 4 s f 4

626
00:42:42,900 --> 00:42:49,710
from ICL 4 minus and we saw how our by counting the number of electrons

627
00:42:49,710 --> 00:42:55,800
domains and then partitioning them into blinding verses nonbinding domains we could proceed in a

628
00:42:55,800 --> 00:43:02,430
systematic manner towards the end of the deduction of what the molecular shape should be

629
00:43:02,690 --> 00:43:08,630
so that was that was a productive day and I urge you to understand what's

630
00:43:08,630 --> 00:43:12,520
going on in Table 4 3 and you can you can expect that to the

631
00:43:12,540 --> 00:43:17,860
testimony at the end of October and you'll have energy so you from probably wanna

632
00:43:17,890 --> 00:43:19,000
know how work through this

633
00:43:19,930 --> 00:43:21,310
this document here

634
00:43:21,370 --> 00:43:29,080
so today I want to continue this this treatment of ionic actually of a covalent

635
00:43:29,810 --> 00:43:37,220
compounds and specifically I wanted to talk broadly about how we determine state of aggregation

636
00:43:37,460 --> 00:43:41,550
number words how do we determine whether something is going to be a solid or

637
00:43:41,550 --> 00:43:47,610
liquid or a gas we've already seen that ionic compounds by their nature because they

638
00:43:47,610 --> 00:43:54,520
have unsaturated bonds they continue to allow more to belong to the initial starting point

639
00:43:54,900 --> 00:44:01,550
results in a larger aggregate larger aggregate tightly bound leads to crystal formation the crystal

640
00:44:01,550 --> 00:44:06,560
is the regularity but definitely solid formation and when when you're trying to answer the

641
00:44:06,560 --> 00:44:11,490
question is something of solid or liquid or gas under certain conditions this is the

642
00:44:15,210 --> 00:44:21,620
you have to compare the cohesive forces versus the disruptive forces so the cohesive forces

643
00:44:21,620 --> 00:44:28,270
are the binding force and the disruptive forces typically are viral in origin so we

644
00:44:28,270 --> 00:44:33,460
have to compare the binding energy of the substance to the general thermal energy

645
00:44:35,840 --> 00:44:40,710
that's shown here so binding energy that gets back to the nature itself it's an

646
00:44:40,710 --> 00:44:48,150
ionic compound this binding energies that Madelon's bearing crystal lattice energy is very negative for

647
00:44:48,150 --> 00:44:53,800
sodium chloride it's about 787 kilojoules per mole where's the thermal energy in this room

648
00:44:53,800 --> 00:44:58,210
is about 40 of an electron volt so that's that's small sodium chloride is a

649
00:44:58,370 --> 00:45:00,610
crystal at that

650
00:45:00,650 --> 00:45:04,460
room temperature so now I wanted a covalent compounds

651
00:45:04,610 --> 00:45:08,780
and ask the same question what's the state of aggregation but there's a fork in

652
00:45:08,780 --> 00:45:14,170
the road here there's 2 types of covalent compounds we saw that as shown appear

653
00:45:14,170 --> 00:45:20,280
on the slide that the results when we have just a single molecules but we

654
00:45:20,280 --> 00:45:24,600
also saw at the end of last lecture it's also possible to talk about covalent

655
00:45:24,600 --> 00:45:29,400
networks we looked at Diamond and we looked at graphite now the case of diamond

656
00:45:29,400 --> 00:45:32,690
and graphite were in a situation that's similar to

657
00:45:33,220 --> 00:45:38,720
called excuse me ionic solids in other words we have large aggregates because the SP

658
00:45:38,720 --> 00:45:42,900
three hybridization allows for carbon a bond a carbon a bonded carbon a bonded carbon

659
00:45:43,070 --> 00:45:46,960
as you end up with a large aggregate in fact it's an it's an ordered

660
00:45:47,000 --> 00:45:51,770
aggregate it's a crystal Diamond is a crystal just sodium chloride is a crystal ball

661
00:45:52,060 --> 00:45:56,840
were larger aggregates and we saw also that the binding energy and diamond is very

662
00:45:56,840 --> 00:46:02,810
very strong very strongly negative and so therefore it trumps the

663
00:46:02,840 --> 00:46:07,810
thermal energy of the system so these are solids at room temperature horrible molecules that's

664
00:46:08,150 --> 00:46:16,360
focus on today what is the nature of the blinding in molecular systems and specifically

665
00:46:16,720 --> 00:46:20,040
let's just take 1 of the molecules we looked at last day we looked at

666
00:46:20,400 --> 00:46:24,980
sulfur hexafluoride and I told you sulfur hexafluoride is a gas at room temperature you

667
00:46:24,980 --> 00:46:29,630
know this thing has a very very of high molecular weight so the question really

668
00:46:29,630 --> 00:46:35,090
we measuring interestingness by the size of of the that saying this as low variance

669
00:46:35,230 --> 00:46:41,880
the average variance so it's not very interesting is less interesting than average that's the

670
00:46:41,890 --> 00:46:48,980
russian so there say a right well people the land is bigger than the average

671
00:46:48,990 --> 00:46:50,680
and discover

672
00:46:50,690 --> 00:46:55,950
and the final method people use this one scree diagram

673
00:46:58,840 --> 00:47:02,060
what's the diagram is building that apply

674
00:47:04,190 --> 00:47:09,820
the eyes a plot of lambda i see against so this is

675
00:47:09,880 --> 00:47:11,710
plot for the

676
00:47:11,740 --> 00:47:13,190
i was data

677
00:47:13,190 --> 00:47:14,780
so the first ones

678
00:47:15,810 --> 00:47:20,860
and that's one less there was point five is so

679
00:47:20,880 --> 00:47:27,840
so the dominant variances in these sort of if you think it is just

680
00:47:29,200 --> 00:47:30,810
so you want to be

681
00:47:30,820 --> 00:47:35,080
yeah well in this diagram so

682
00:47:35,130 --> 00:47:40,120
the the album choose the ones left to available to the components to be

683
00:47:40,190 --> 00:47:43,130
all this three diagrams what this because

684
00:47:43,140 --> 00:47:46,320
the land is are in order that one has to be bigger than the one

685
00:47:46,320 --> 00:47:49,080
that was begin on august one

686
00:47:49,090 --> 00:47:54,760
so in this case i would say well in the first component because it was

687
00:47:54,760 --> 00:47:56,310
such a big difference

688
00:47:56,320 --> 00:47:59,570
the problem of

689
00:47:59,670 --> 00:48:09,320
through this right was just one final thing that people do with present problems on

690
00:48:09,320 --> 00:48:12,780
the always do this if you

691
00:48:12,820 --> 00:48:21,560
the problem with for because a few changes the measurement so if you went from

692
00:48:21,570 --> 00:48:25,950
measuring one thing is that there is another meters it will also the results

693
00:48:25,960 --> 00:48:30,800
so no one was in his ways to fix this so what do is you

694
00:48:30,800 --> 00:48:34,080
take off the average size of the object

695
00:48:34,110 --> 00:48:38,950
then divided by the standard deviation which is just the square of the variance for

696
00:48:38,950 --> 00:48:43,930
each and that it is said is that will be the normalized data and they

697
00:48:43,930 --> 00:48:47,200
put so its analysis on the normalized

698
00:48:47,210 --> 00:48:50,980
what happens if you wanted to people do this this is the sort of starts

699
00:48:50,990 --> 00:48:52,010
to happen if you

700
00:48:52,070 --> 00:48:54,620
mess about with the scale so

701
00:48:54,860 --> 00:48:56,070
in this one

702
00:48:56,130 --> 00:48:58,530
was just

703
00:48:58,580 --> 00:49:02,630
standard is an instance of the data as well as in the middle

704
00:49:02,640 --> 00:49:05,640
drawing on the principal axes

705
00:49:05,670 --> 00:49:08,320
principal component axes that correspond to

706
00:49:08,340 --> 00:49:09,870
this data

707
00:49:09,880 --> 00:49:14,510
and on the if i keep the circle with the same

708
00:49:14,530 --> 00:49:18,940
five athletes to stop early by a factor of five

709
00:49:18,940 --> 00:49:21,260
see the angle

710
00:49:21,270 --> 00:49:24,530
so the direction of these complexes start moving

711
00:49:25,780 --> 00:49:27,070
so the first

712
00:49:29,380 --> 00:49:30,940
flat space

713
00:49:31,820 --> 00:49:38,630
doing it because when you multiply by o point five increasing the variance in this

714
00:49:38,630 --> 00:49:44,650
direction so it starts being pulled into the direction and that's the same effects well

715
00:49:44,670 --> 00:49:48,460
it's not just that is less than the effective going from centimeters to meters on

716
00:49:48,460 --> 00:49:55,570
its enemies and also is quite dramatic if you swat from imperial units against

717
00:49:55,830 --> 00:50:00,590
three pounds to kilograms meters

718
00:50:00,620 --> 00:50:06,150
then you get to some differences but basically just problem was if you do the

719
00:50:06,150 --> 00:50:08,860
normalisation first it doesn't matter

720
00:50:08,920 --> 00:50:13,210
you get the same answer so there is situation where you want to not do

721
00:50:13,230 --> 00:50:20,320
this might be first-innings some analysis that whether it was sort spatial coordinates and matter

722
00:50:20,320 --> 00:50:23,740
that they were in the same scale and had to keep the scale but that's

723
00:50:23,740 --> 00:50:27,920
the same situation i think of what i would do trick

724
00:50:29,890 --> 00:50:35,030
which i want going to get into because of the downturn was when the book

725
00:50:35,360 --> 00:50:36,630
was an explanation

726
00:50:39,270 --> 00:50:43,620
why one is measuring you can look at the ideas values and try and give

727
00:50:43,620 --> 00:50:45,030
an interpretation

728
00:50:45,040 --> 00:50:49,380
all what the principal component directions is measuring

729
00:50:53,090 --> 00:50:57,980
so this is i think is an interpretation is to do with the show you

730
00:50:57,980 --> 00:50:59,510
can work out to

731
00:50:59,530 --> 00:51:01,250
the shape of the petals

732
00:51:01,630 --> 00:51:05,860
this was done on this particular set on

733
00:51:05,880 --> 00:51:09,660
how to actually come to to that conclusion is detailed in the book but that's

734
00:51:09,660 --> 00:51:11,400
the idea is that you can

735
00:51:11,410 --> 00:51:17,110
if you can when gets the and you try to give a real world definition

736
00:51:17,110 --> 00:51:23,780
of what the principal components of measuring of the things where you want to serve

737
00:51:23,780 --> 00:51:30,700
as well organised sample way home is where it survey of the employees about and

738
00:51:30,700 --> 00:51:34,630
that's the different aspects of the job and

739
00:51:34,640 --> 00:51:38,520
what you find some what we find an example of got hold of is managed

740
00:51:38,520 --> 00:51:40,120
to simplify

741
00:51:40,280 --> 00:51:45,750
it from down from about twenty different statements that about three different principal components three

742
00:51:45,750 --> 00:51:51,580
principal components that is being paid job satisfaction ambition if you

743
00:51:51,600 --> 00:51:53,350
try to interpret

744
00:51:53,360 --> 00:51:58,800
what they can have how made up of the original variables so that's the idea

745
00:51:58,830 --> 00:52:04,130
is that we try to work out how which is why one relates to lines

746
00:52:04,130 --> 00:52:09,870
that would settle intersect with other try to say so ones measuring

747
00:52:09,880 --> 00:52:12,740
it is big for long thin ones all

748
00:52:12,750 --> 00:52:16,630
short vowels also love was also looking for

749
00:52:16,760 --> 00:52:22,290
OK so so what real interpretation if we can but we can always get

750
00:52:24,750 --> 00:52:27,000
well was

751
00:52:27,010 --> 00:52:30,930
in this

752
00:52:30,950 --> 00:52:33,770
one of the things that you find is that

753
00:52:34,050 --> 00:52:38,840
later in the book and chapter i was the link between certain types of learning

754
00:52:39,250 --> 00:52:41,860
and principal components it stated the is

755
00:52:41,880 --> 00:52:44,120
well here we go

756
00:52:44,130 --> 00:52:48,620
if you have a neural network

757
00:52:48,630 --> 00:52:55,330
of of the whole by the perceptron is this

758
00:52:55,330 --> 00:53:00,390
sort of model that was developed by paul at it mighty these all feed forward

759
00:53:00,390 --> 00:53:01,950
models there is no feedback connection

760
00:53:03,030 --> 00:53:09,060
and it turns out that having these feedback connections are very useful in particular for disambiguating the input

761
00:53:10,920 --> 00:53:13,470
you can define conditional probabilities in this model

762
00:53:13,960 --> 00:53:15,660
the very straightforward to define

763
00:53:18,480 --> 00:53:22,020
but the thing is you this model just like in deep belief networks computing the

764
00:53:22,020 --> 00:53:24,320
distribution over hidden variables becomes intractable

765
00:53:24,980 --> 00:53:28,100
so that's one of the challenges and for those of you who are not familiar

766
00:53:28,100 --> 00:53:29,510
with a little bit with machine learning you know

767
00:53:29,960 --> 00:53:33,270
people who are working with markov random fields conditional random fields

768
00:53:34,000 --> 00:53:39,660
in particular sometimes people with my markov random fields with hidden variables so those are the classes of models that

769
00:53:40,210 --> 00:53:44,940
that basically similar to the markov random fields with hidden variables latent variables

770
00:53:46,710 --> 00:53:50,990
now how does it compare to are to other kinds of models if you look

771
00:53:50,990 --> 00:53:53,810
at neural nets models neural networks style models

772
00:53:54,440 --> 00:53:58,780
the difference is that neural networks style models inference is just feedforward

773
00:53:59,520 --> 00:54:00,180
given the data

774
00:54:00,750 --> 00:54:04,150
you've feed forward through the neural network and then you have output

775
00:54:04,740 --> 00:54:09,900
now typically neural networks i discriminative models right because they're trying to basically map input

776
00:54:09,920 --> 00:54:12,590
and the output in trying to classify something

777
00:54:13,290 --> 00:54:14,120
forty neural net

778
00:54:15,960 --> 00:54:21,350
models like diebold's machines which generative basically trying to explain the data distribution data as well

779
00:54:22,210 --> 00:54:24,980
compared to model like deep belief networks you know

780
00:54:25,460 --> 00:54:26,550
you can sort of see that

781
00:54:27,050 --> 00:54:27,430
you know they

782
00:54:27,820 --> 00:54:32,840
they look somewhat similar however when you're doing inference in a deep belief network as

783
00:54:32,840 --> 00:54:35,000
i mentioned before inference is just feedforward

784
00:54:37,620 --> 00:54:41,040
so you basically the way the way the entries in a deep belief network was

785
00:54:41,050 --> 00:54:43,750
to train you basically converting it into a neural network

786
00:54:44,210 --> 00:54:49,130
and then you proceed with an inference step is being basically a neural network

787
00:54:50,010 --> 00:54:52,340
so let's see what's happening with deep boltzmann machines

788
00:54:53,280 --> 00:54:56,570
if i wanted to maximum likelihood and these learning in this model

789
00:54:57,080 --> 00:54:58,780
then i basically get the same rule

790
00:54:59,940 --> 00:55:05,790
the rule just matching expected sufficient statistics driven by the data with the expected sufficient statistics driven by the model

791
00:55:06,350 --> 00:55:09,140
right so you have these expectations and these expectations

792
00:55:09,550 --> 00:55:10,500
and remarkably

793
00:55:10,950 --> 00:55:16,220
if you ever deal with undirected graphical models like markov random fields conditional random fields

794
00:55:16,230 --> 00:55:18,850
factor graphs you know they have a lot of different names

795
00:55:19,610 --> 00:55:24,950
the learning algorithm maximum likelihood learning algorithm basically comes down to define these expectations

796
00:55:27,030 --> 00:55:30,190
now the problem is that now both expectations i intractable

797
00:55:31,100 --> 00:55:34,550
and the problem is the reason why the intractable is that given the data

798
00:55:35,180 --> 00:55:40,780
because we introduce dependencies between hidden variables computing the distribution over hidden variables becomes intractable

799
00:55:41,400 --> 00:55:45,370
so it's kind of like one of those things that we made our model much more flexible

800
00:55:45,990 --> 00:55:47,050
but we suffering

801
00:55:48,190 --> 00:55:49,850
not being able to do inference

802
00:55:51,170 --> 00:55:55,160
in the membrane like restricted both machines you have one layer of hidden variables and

803
00:55:55,160 --> 00:55:58,410
there's no dependencies between hidden variables no connections between hidden variables

804
00:55:58,810 --> 00:56:00,010
so conditional on the data

805
00:56:00,700 --> 00:56:07,470
the distribution which variables becomes independently contributed exactly here introducing more flexibility so these models

806
00:56:07,470 --> 00:56:10,240
are much more flexible but inference becomes problematic

807
00:56:10,880 --> 00:56:12,500
so so what can we do

808
00:56:13,790 --> 00:56:18,850
so both express stations are intractable in particular this conditional probability

809
00:56:19,480 --> 00:56:22,780
it is no longer factorial it's no longer easy to compute so we have to

810
00:56:22,780 --> 00:56:26,490
do something else let me give you an intuition behind the warning out what is

811
00:56:26,490 --> 00:56:27,780
really this model trying to do

812
00:56:28,580 --> 00:56:29,890
imagine you have some data

813
00:56:30,430 --> 00:56:33,850
and imagine you get observe these characters these handwritten characters

814
00:56:35,010 --> 00:56:36,190
so what you like to do

815
00:56:36,800 --> 00:56:39,850
is you'd like to put some probability mass on those characters

816
00:56:40,510 --> 00:56:44,250
obviously you want say this is probable this is what i observe this is what the world looks like

817
00:56:45,620 --> 00:56:49,610
and that's what this term the first term is essentially trying to do

818
00:56:50,530 --> 00:56:54,170
this is the term that tries to put little bumps the true data point

819
00:56:55,570 --> 00:56:57,690
or the data points that you observe

820
00:56:59,180 --> 00:56:59,590
but now

821
00:57:00,970 --> 00:57:04,860
imagine that i get and i go and i just finished you know i see

822
00:57:04,860 --> 00:57:06,020
an image like this one here

823
00:57:06,420 --> 00:57:07,640
which is completely you know

824
00:57:08,040 --> 00:57:09,100
it's still a valid image

825
00:57:10,480 --> 00:57:13,860
but i'd like to say well this image should actually have small probability

826
00:57:15,310 --> 00:57:16,280
and this is the rule

827
00:57:16,730 --> 00:57:18,110
of this expectation here

828
00:57:18,570 --> 00:57:20,270
this is the role of the partition function

829
00:57:20,900 --> 00:57:23,850
which essentially says look at these exponential space

830
00:57:24,470 --> 00:57:26,420
and try to sign small probability

831
00:57:26,920 --> 00:57:29,030
to invalid data points or to to to

832
00:57:30,190 --> 00:57:31,800
very improbable data points

833
00:57:33,730 --> 00:57:38,110
right so that's that's roughly the idea behind these running and this is what they're trying to do

834
00:57:39,690 --> 00:57:43,500
now what we're gonna do here's what used different techniques and i'm going to explain

835
00:57:43,500 --> 00:57:45,970
to you a very briefly what they do

836
00:57:46,490 --> 00:57:52,020
we're gonna estimated data-dependent sufficient statistics using variational inference are and we're gonna estimates

837
00:57:52,600 --> 00:57:57,850
the expected sufficient statistics by the model here markov chain monte carlo based approximation

838
00:58:00,380 --> 00:58:02,300
so if you look at the previous work

839
00:58:03,060 --> 00:58:06,040
right there's been a lot of what people trying to figure out how we can do

840
00:58:06,440 --> 00:58:11,250
inference in these models in particular was missions and i'm not doing justice listing all the people

841
00:58:13,740 --> 00:58:16,880
but the problem with a lot of the previous models is that you know if

842
00:58:16,880 --> 00:58:21,390
you look at the real world applications we typically have thousands of latent variables seemed

843
00:58:21,420 --> 00:58:23,380
to design something that works very quickly

844
00:58:24,160 --> 00:58:28,430
and many of you know the previous algorithms when not quite successful

845
00:58:28,950 --> 00:58:32,840
when you're doing learning in these models with latent variables with hidden variables

846
00:58:33,650 --> 00:58:36,850
right so if you look at contrastive divergence if you look at you know a

847
00:58:36,850 --> 00:58:40,910
lot of different methods like score matching composite likelihood piecewise learning

848
00:58:41,590 --> 00:58:45,170
a lot of these models cannot handle latent variables hidden variables

849
00:58:46,550 --> 00:58:47,780
so what what can we do

850
00:58:49,170 --> 00:58:52,150
so let me give you a brief intuition what the algorithm is gonna trying

851
00:58:52,810 --> 00:58:53,750
it's going to

852
00:58:54,230 --> 00:58:54,770
be trying to do

853
00:58:55,720 --> 00:58:58,100
the very first thing we need to do is we need to do inference

854
00:58:58,770 --> 00:59:00,620
right i give you this data point

855
00:59:01,420 --> 00:59:03,120
and i need to figure out what features

856
00:59:03,580 --> 00:59:05,620
high-level features i seen this data

857
00:59:05,620 --> 00:59:07,260
of a certain size

858
00:59:09,870 --> 00:59:10,720
but this is

859
00:59:10,720 --> 00:59:15,140
speedy close as in terms of idea of basic ideas what is that we with

860
00:59:15,450 --> 00:59:20,930
support vector machines you know how to overcome his work you know that the decision

861
00:59:20,930 --> 00:59:26,300
problem solvers support vector machines can be cast the problem of finding the shortest u

862
00:59:26,320 --> 00:59:27,830
the cheese margin

863
00:59:27,850 --> 00:59:29,700
at least one image example

864
00:59:29,720 --> 00:59:33,490
if it takes set of examples in the parable

865
00:59:33,550 --> 00:59:39,180
good OK so we know now that the link to view matters

866
00:59:39,180 --> 00:59:43,180
if it's if we call it with the entrance

867
00:59:43,220 --> 00:59:48,100
and we are now ready to analyse the performance of perceptron this was because

868
00:59:48,160 --> 00:59:51,370
not the kind of a lot of time

869
00:59:51,370 --> 00:59:52,970
thank you

870
00:59:57,890 --> 01:00:03,200
we're going to give a general bound for perception on on january general

871
01:00:03,240 --> 01:00:08,720
on january streams nonseparable streams and then we can because they specialize for separable streams

872
01:00:08,780 --> 01:00:13,450
looking for analysis of perception

873
01:00:13,470 --> 01:00:21,100
so that

874
01:00:21,160 --> 01:00:23,950
basically two ways to analyse this

875
01:00:23,970 --> 01:00:25,580
so we have an extreme

876
01:00:26,820 --> 01:00:35,260
and there be we fix and you

877
01:00:35,370 --> 01:00:40,720
so we're gonna compare the number of mistakes made by

878
01:00:40,740 --> 01:00:42,050
the perceptron

879
01:00:42,050 --> 01:00:44,640
only with the hinge loss

880
01:00:44,680 --> 01:00:46,220
in a lot of you

881
01:00:46,260 --> 01:00:49,240
which is an upper bound on the number of mistakes made by

882
01:00:49,240 --> 01:00:51,760
but you is any guide

883
01:00:51,800 --> 01:00:53,240
any any here

884
01:00:55,930 --> 01:01:00,820
there are two basic ways to to make in this analysis one is looking at

885
01:01:00,820 --> 01:01:02,550
the evolution of this

886
01:01:03,580 --> 01:01:09,910
you over time so looking at the evolution of the squared euclidean

887
01:01:09,930 --> 01:01:12,410
distance between wt

888
01:01:12,430 --> 01:01:13,580
with you

889
01:01:13,600 --> 01:01:15,870
as time goes by

890
01:01:15,890 --> 01:01:20,890
the first one second one ongoing and all i'm going to look at the evolution

891
01:01:20,890 --> 01:01:24,410
of the inner product between u

892
01:01:24,430 --> 01:01:27,830
now i'm dropping dropping the bars because they

893
01:01:27,890 --> 01:01:32,010
the more i look into the evolution of this quantity

894
01:01:32,060 --> 01:01:33,080
for the

895
01:01:34,350 --> 01:01:37,320
the killing by the norm

896
01:01:37,370 --> 01:01:40,320
multiply by the norm of the

897
01:01:40,330 --> 01:01:42,970
the cosine of the angle of about but not

898
01:01:42,970 --> 01:01:49,390
so i'm now showing the pro based this measure here

899
01:01:49,410 --> 01:01:50,890
because it

900
01:01:50,950 --> 01:01:52,240
may be simpler

901
01:01:52,280 --> 01:01:55,910
and simpler so

902
01:01:56,910 --> 01:01:59,740
focus on

903
01:01:59,760 --> 01:02:03,680
the easiest way here is to look at the sequence so

904
01:02:03,700 --> 01:02:05,410
when no mistakes

905
01:02:05,490 --> 01:02:06,740
twenty one

906
01:02:06,890 --> 01:02:10,330
on both that's where there's no mistake nothing happens

907
01:02:10,330 --> 01:02:12,330
but that doesn't change

908
01:02:12,890 --> 01:02:18,350
the state of the art reminds remains the same on both mistakes when we're not

909
01:02:18,350 --> 01:02:20,740
on both that's when no mistake

910
01:02:20,760 --> 01:02:23,370
so we can just focus

911
01:02:23,390 --> 01:02:24,870
on both the

912
01:02:24,890 --> 01:02:27,370
well mistake look like

913
01:02:27,370 --> 01:02:29,100
just because of this

914
01:02:29,970 --> 01:02:34,450
let's take one of those steps and let's look at the people of what how

915
01:02:34,450 --> 01:02:36,700
this is going to change

916
01:02:36,820 --> 01:02:38,260
i'm looking

917
01:02:38,300 --> 01:02:41,850
now in the process of

918
01:02:44,950 --> 01:02:49,160
this is a quote

919
01:02:49,180 --> 01:02:54,550
i'm just using the update rule because they're not a mistake at the steps

920
01:02:54,550 --> 01:02:55,850
so i know

921
01:02:55,870 --> 01:02:59,330
the weight was up things way

922
01:03:03,330 --> 01:03:05,140
now i can

923
01:03:07,300 --> 01:03:12,850
history but the inner product of the summer

924
01:03:12,890 --> 01:03:25,060
so you see i'm i'm looking at aiming at getting kernel

925
01:03:25,080 --> 01:03:27,180
now i can

926
01:03:27,240 --> 01:03:33,510
two very different think again and the and track one

927
01:03:35,300 --> 01:03:39,320
now what i have is what

928
01:03:40,430 --> 01:03:47,220
plus one and then i minus one minus i write this way because they like

929
01:03:48,080 --> 01:03:49,430
like the

930
01:03:49,470 --> 01:03:51,580
you know i don't about

931
01:03:57,580 --> 01:04:01,180
cool i didn't do anything wrong again

932
01:04:01,180 --> 01:04:04,370
and then i can make this

933
01:04:07,760 --> 01:04:09,740
i can make this

934
01:04:09,740 --> 01:04:14,180
i think the bigger by sort of the whole thing is smaller because his mind

935
01:04:14,300 --> 01:04:15,930
by thinking

936
01:04:15,970 --> 01:04:17,910
the class

937
01:04:17,930 --> 01:04:21,050
so the maximum between this is zero

938
01:04:21,150 --> 01:04:34,820
four you remember this provisions the max became this zero so made this thing is

939
01:04:35,740 --> 01:04:37,160
which means that they

940
01:04:37,240 --> 01:04:39,470
so the minus sign i went down

941
01:04:39,490 --> 01:04:40,640
OK so now

942
01:04:41,030 --> 01:04:44,510
this is the hinge loss of u on the example

943
01:04:44,510 --> 01:04:49,140
and this is the indicator function that the mistake happen

944
01:04:49,180 --> 01:04:51,930
and this is the term that is that the

945
01:04:51,950 --> 01:04:54,510
so i keep on going like this

946
01:04:54,530 --> 01:05:03,740
i keep on going like this

947
01:05:03,760 --> 01:05:04,890
and the

948
01:05:04,950 --> 01:05:14,240
we end up with is the following

949
01:05:14,240 --> 01:05:16,990
OK if i

950
01:05:16,990 --> 01:05:19,680
just represents one electron

951
01:05:19,700 --> 01:05:24,230
there are going to be able to available for bonding

952
01:05:25,040 --> 01:05:30,360
so what we going do here bring in some hydrogen here's original

953
01:05:30,360 --> 01:05:33,840
here's ideas hydrogen common and here's the hydrogen

954
01:05:33,860 --> 01:05:35,800
we've got ammonia

955
01:05:35,820 --> 01:05:40,610
what did we do to form these nitrogen hydrogen bonds where we took that as

956
01:05:40,610 --> 01:05:43,650
the three function on nitrogen

957
01:05:43,670 --> 01:05:46,190
and brought in the hydrogen where

958
01:05:46,460 --> 01:05:52,980
one as wavefunctions constructed we destructively interfere with SP three wave function

959
01:05:53,030 --> 01:05:56,150
the result is a sigma bond

960
01:05:56,170 --> 01:05:59,750
it sigma it's composed of the nitrogen

961
01:05:59,750 --> 01:06:02,150
two sp three

962
01:06:02,170 --> 01:06:07,960
and the hydrogen one as we function

963
01:06:08,800 --> 01:06:11,360
the main goal right in years

964
01:06:11,400 --> 01:06:12,550
being all

965
01:06:12,650 --> 01:06:16,340
hydrogen nitrogen hydrogen angle here

966
01:06:16,380 --> 01:06:19,690
what you see there is actually a little smaller

967
01:06:19,730 --> 01:06:22,750
then the ideal tetrahedral angle

968
01:06:22,770 --> 01:06:29,230
no longer a hundred and nine point five to three hundred seventy three

969
01:06:29,250 --> 01:06:34,480
and the reason for that is because of this lone pair out here

970
01:06:34,500 --> 01:06:36,940
this long hair

971
01:06:37,010 --> 01:06:44,840
has very repulsive interaction with the electrons in these nitrogen hydrogen bonds here

972
01:06:44,860 --> 01:06:49,420
bringing an in a little bit more courses

973
01:06:49,440 --> 01:06:51,190
they're are being repelled

974
01:06:52,460 --> 01:06:59,340
so there will generally be the case if you've got a long period electron somewhere

975
01:06:59,360 --> 01:07:03,570
that will tend to repel everything will be repelled from it

976
01:07:03,610 --> 01:07:07,750
the consequence is that the single was binding you're is going to be a little

977
01:07:08,920 --> 01:07:13,630
tetrahedral angle of one hundred nine point five degrees

978
01:07:16,530 --> 01:07:21,610
OK but now here's a really important point

979
01:07:21,630 --> 01:07:24,500
you're on and for what is

980
01:07:24,550 --> 01:07:27,880
the geometry around some and

981
01:07:27,920 --> 01:07:29,440
in this case

982
01:07:29,460 --> 01:07:33,360
you ask for you might be what is the geometry

983
01:07:33,380 --> 01:07:35,690
around nitrogen

984
01:07:35,710 --> 01:07:39,030
when you ask such a question what that

985
01:07:39,030 --> 01:07:44,250
means is to you what is the geometry of the atoms

986
01:07:44,280 --> 01:07:46,320
around the nitrogen

987
01:07:46,320 --> 01:07:48,730
that's the electrons

988
01:07:48,750 --> 01:07:52,030
the distinction is

989
01:07:52,050 --> 01:07:58,170
the geometry of the electrons around the atom is about three draw

990
01:07:58,170 --> 01:07:59,460
but usually

991
01:07:59,480 --> 01:08:01,980
one is interested in there

992
01:08:01,980 --> 01:08:04,630
usually one is interested in

993
01:08:04,630 --> 01:08:09,300
what is the geometry of the other animals around the nitrogen

994
01:08:09,360 --> 01:08:13,800
so in this case here the geometry of the other and this is the trick

995
01:08:13,800 --> 01:08:16,280
and all parameter geometry

996
01:08:16,300 --> 01:08:18,840
right it's not that

997
01:08:18,860 --> 01:08:23,380
so if you ask for the shape of the nitrogen molecule the answer is

998
01:08:23,420 --> 01:08:25,170
a trigonal pyramid

999
01:08:25,190 --> 01:08:27,820
now i had redraw

1000
01:08:28,360 --> 01:08:30,190
really important

1001
01:08:30,550 --> 01:08:33,670
trips up a lot of people

1002
01:08:35,320 --> 01:08:41,270
the lone pairs pointing out here the electron geometry is approximately tetrahedral

1003
01:08:41,360 --> 01:08:46,590
that's not what we are usually interested in we're interested in where the atoms are

1004
01:08:46,670 --> 01:08:48,670
the geometry is the trick all

1005
01:08:48,690 --> 01:08:52,030
here amid

1006
01:08:52,860 --> 01:08:59,480
so that's a sp three hybridisation and and ammonia or nitrogen

1007
01:08:59,590 --> 01:09:03,480
oxigen k oxygen can also undergo

1008
01:09:03,500 --> 01:09:06,690
this as three hybridisation

1009
01:09:06,750 --> 01:09:07,690
here's the

1010
01:09:07,710 --> 01:09:10,980
the electron configuration for oxygen and

1011
01:09:11,000 --> 01:09:14,380
two electrons in the two s two

1012
01:09:14,400 --> 01:09:16,460
and four in the two p

1013
01:09:18,420 --> 01:09:20,750
what we're going to do here again

1014
01:09:20,800 --> 01:09:23,360
is we're going to let he's

1015
01:09:23,360 --> 01:09:29,000
i wavefunctions constructively and destructively interfere when we do that again

1016
01:09:29,030 --> 01:09:30,630
we're going to get

1017
01:09:30,650 --> 01:09:33,980
four new wave functions

1018
01:09:34,060 --> 01:09:37,210
sp three hybrid wavefunctions

1019
01:09:37,210 --> 01:09:41,230
but in the case of oxygen because we have one more electrons

1020
01:09:41,280 --> 01:09:46,110
than that of nitrogen we're going to have two electrons that as the free state

1021
01:09:46,230 --> 01:09:49,000
two electrons in the other SP three state

1022
01:09:49,010 --> 01:09:52,920
and then one of electronics in the free state

1023
01:09:54,800 --> 01:10:01,800
so what is the electron arrangement around oxygen going to look like

1024
01:10:01,840 --> 01:10:03,980
well it's going to look like this

1025
01:10:05,300 --> 01:10:09,210
we're again each one of these sp three

1026
01:10:09,230 --> 01:10:10,530
wave functions

1027
01:10:10,590 --> 01:10:13,170
is pointed to

1028
01:10:13,190 --> 01:10:18,110
is there a question over here that i can help somebody with

1029
01:10:18,110 --> 01:10:21,920
each one of these wavefunctions is pointed

1030
01:10:21,920 --> 01:10:23,360
two core

1031
01:10:23,380 --> 01:10:25,650
in this chapter he drowned

1032
01:10:25,650 --> 01:10:27,150
but again

1033
01:10:27,170 --> 01:10:33,900
one of these wavefunctions represents two electrons as long here another one represents

1034
01:10:33,900 --> 01:10:36,570
two electrons that long period

1035
01:10:36,590 --> 01:10:42,340
and then only these two sp three wavefunctions are going to be available for overlap

1036
01:10:42,340 --> 01:10:48,230
are the one that is most similar to this one is regarded as

1037
01:10:48,340 --> 01:10:53,780
the important centers and get a higher score than there is query based method

1038
01:10:53,820 --> 01:11:00,450
given a query this method can take similarity between the query and each sentence in

1039
01:11:00,460 --> 01:11:03,760
the document and that the one with the

1040
01:11:03,820 --> 01:11:04,760
it's the summer

1041
01:11:04,770 --> 01:11:08,880
two other will be selected to compose a summary

1042
01:11:08,960 --> 01:11:12,960
and then you can also add some position feature

1043
01:11:12,980 --> 01:11:17,920
like it gave extra weight to sentences that at at the beginning of the document

1044
01:11:17,950 --> 01:11:22,120
and this is especially helpful with the news texts

1045
01:11:22,130 --> 01:11:25,840
and then there is a new member in

1046
01:11:25,980 --> 01:11:32,550
and these this corpus or but this lexrank method is not currently in the middle

1047
01:11:32,650 --> 01:11:36,070
package so we didn't try to get

1048
01:11:36,140 --> 01:11:41,560
but it's not that difficult to implement the idea is not to compare each sentence

1049
01:11:41,560 --> 01:11:47,140
with the pseudo sentence but compared to do pairwise comparison between all the sentences

1050
01:11:48,470 --> 01:11:55,220
this sentence pairs with similarity value or certain threshold are regarded as similar to each

1051
01:11:55,220 --> 01:11:56,870
other and those

1052
01:11:56,920 --> 01:12:00,820
sentences is considered more important if it is

1053
01:12:00,830 --> 01:12:02,860
similar to many sentences

1054
01:12:02,940 --> 01:12:06,520
and those sentences are also important themselves

1055
01:12:08,550 --> 01:12:12,750
this is the idea behind this method

1056
01:12:12,770 --> 01:12:16,190
just to give you a quick idea about the package

1057
01:12:16,260 --> 01:12:21,900
and then there are also two baselines around the method and the method

1058
01:12:21,920 --> 01:12:23,900
included in the system

1059
01:12:24,870 --> 01:12:28,310
the random method basically assigned

1060
01:12:28,360 --> 01:12:33,930
were high well into two sentences happened at the beginning of the document

1061
01:12:33,950 --> 01:12:39,700
and then run the method simply pickups and that is selected randomly

1062
01:12:43,650 --> 01:12:48,770
if you have speculated these significant scores for

1063
01:12:48,800 --> 01:12:50,210
different sentences

1064
01:12:50,230 --> 01:12:52,940
and make can use

1065
01:12:52,980 --> 01:12:55,990
you can use reranking facilities

1066
01:12:56,130 --> 01:12:57,380
to adjust

1067
01:12:57,520 --> 01:12:59,740
the scores

1068
01:13:00,190 --> 01:13:04,280
one is the mean cosine reranking it's simply

1069
01:13:04,290 --> 01:13:08,730
there are always sentences about certain similarity

1070
01:13:08,810 --> 01:13:14,740
threshold with those sentences that are already included in in the summary so that not

1071
01:13:14,740 --> 01:13:15,650
all the same

1072
01:13:16,710 --> 01:13:21,460
the summary doesn't only come contains sentences that are similar to to each other and

1073
01:13:22,170 --> 01:13:30,050
and are here and here is based on CMU so i'm my are of course

1074
01:13:30,130 --> 01:13:37,630
and it actually just called the scores so that similar sentences end up receiving a

1075
01:13:37,630 --> 01:13:40,430
lower score

1076
01:13:40,480 --> 01:13:43,290
and it is actually

1077
01:13:43,360 --> 01:13:48,460
four an user it is not actually not very straightforward to use this system may

1078
01:13:48,460 --> 01:13:49,960
be for you guys

1079
01:13:50,010 --> 01:13:56,740
for more technical that there are many techniques behind this kind of systems

1080
01:13:57,380 --> 01:14:00,540
it's not so this still a long way

1081
01:14:00,550 --> 01:14:04,820
before this kind of system can be given to an end user to use it

1082
01:14:04,820 --> 01:14:06,830
in the real world setting

1083
01:14:07,230 --> 01:14:10,370
and even for us to carry out

1084
01:14:10,660 --> 01:14:14,900
experiments for research proposal

1085
01:14:17,830 --> 01:14:22,910
it takes a lot of time and effort to do like preprocessing work and things

1086
01:14:22,910 --> 01:14:24,040
like that

1087
01:14:24,120 --> 01:14:28,410
and so we have created

1088
01:14:28,510 --> 01:14:32,050
user interface elements and so

1089
01:14:32,110 --> 01:14:33,530
we use these

1090
01:14:33,570 --> 01:14:35,930
we use this tool to

1091
01:14:35,940 --> 01:14:42,280
to to do all our experiments

1092
01:14:42,520 --> 01:14:50,830
concerning to be

1093
01:14:50,910 --> 01:14:53,020
here here we can

1094
01:14:53,150 --> 01:14:56,260
just a load documents

1095
01:14:56,350 --> 01:15:03,740
donald documents directly from from the web into our document repository

1096
01:15:03,810 --> 01:15:06,640
and then we can

1097
01:15:06,660 --> 01:15:08,650
choose different types of play

1098
01:15:08,660 --> 01:15:11,730
summarisation methods

1099
01:15:11,900 --> 01:15:19,310
here you can find the features to be used and then you can define in

1100
01:15:19,310 --> 01:15:23,290
in the classifier and the weight of the features

1101
01:15:23,370 --> 01:15:28,470
and then you can also choose what kind of ranking facility would like to use

1102
01:15:28,510 --> 01:15:31,470
and you can control the compression rate

1103
01:15:31,550 --> 01:15:33,630
the length of the output

1104
01:15:33,670 --> 01:15:39,620
you can choose the compression rate based on words or sentences and you can also

1105
01:15:39,620 --> 01:15:40,910
choose to

1106
01:15:40,910 --> 01:15:44,100
the volvo labelled so

1107
01:15:44,460 --> 01:15:49,810
the scale degree of the measurement is

1108
01:15:51,450 --> 01:15:53,220
all over the world

1109
01:15:53,240 --> 01:15:58,290
so the question is could use all of the thirteen balls since twice thirteen is

1110
01:15:58,330 --> 01:16:01,740
twenty six and that's less than twenty seven

1111
01:16:01,790 --> 01:16:04,970
i'll leave that as an exercise to think about

1112
01:16:05,540 --> 01:16:10,120
another thing to think about is if i give UW weighings how many balls can

1113
01:16:10,120 --> 01:16:12,260
you do and how does it scale

1114
01:16:12,290 --> 01:16:15,200
that's the fun from and what is the general solution

1115
01:16:16,030 --> 01:16:20,140
w weighings if you're given n balls where n is the maximum you can possibly

1116
01:16:20,140 --> 01:16:23,640
do w what is the solution and actually super elegant way

1117
01:16:23,660 --> 01:16:27,370
to describe the completely full way you can say in advance what when going to

1118
01:16:28,830 --> 01:16:32,450
it doesn't actually depend on the outcome of explaining what you do that you can

1119
01:16:32,450 --> 01:16:34,700
plan the whole thing out that

1120
01:16:34,720 --> 01:16:37,350
the full solution by conway

1121
01:16:37,410 --> 01:16:39,720
so in

1122
01:16:39,930 --> 01:16:46,270
surprise surprise it has lots of problems

1123
01:16:47,450 --> 01:16:50,010
and the solution by john conway

1124
01:16:50,010 --> 01:16:55,220
the mathematician which enumerates in advance exactly always you're going to do for any number

1125
01:16:55,220 --> 01:16:57,540
of balls is in here

1126
01:16:57,540 --> 01:16:58,530
very pretty

1127
01:16:58,540 --> 01:17:00,620
good OK so

1128
01:17:00,640 --> 01:17:03,990
this is just a piece of evidence is definitely not approved but it's the piece

1129
01:17:03,990 --> 01:17:05,660
of evidence for

1130
01:17:06,950 --> 01:17:11,140
claim here the shannon information is a good way to measure information and should be

1131
01:17:11,140 --> 01:17:16,660
is a sensible thing to think about is not just an arbitrary invention of physicists

1132
01:17:16,680 --> 01:17:20,160
so let me now

1133
01:17:20,240 --> 01:17:22,450
tell you a little bit more about source coding

1134
01:17:24,620 --> 01:17:28,620
so we just tested the shannon information content idea by i

1135
01:17:28,640 --> 01:17:31,200
trying out on one of these puzzle i can come up with other puzzle of

1136
01:17:31,200 --> 01:17:34,330
the form how do you find the something or other and quite often if you

1137
01:17:34,330 --> 01:17:38,790
say well let's just go for maximum entropy strategy you may well find the thing

1138
01:17:39,060 --> 01:17:40,990
in the most efficient way possible

1139
01:17:41,010 --> 01:17:44,540
and we can come up with other ways of testing this but let's move on

1140
01:17:50,560 --> 01:17:52,950
part of source code in some

1141
01:17:52,970 --> 01:17:56,680
his fear the proof of which is in the book

1142
01:17:56,770 --> 01:18:02,790
it is the case that there exist compression algorithms which if you give them and

1143
01:18:02,810 --> 01:18:06,040
i i d samples from the source text you can compress

1144
01:18:06,060 --> 01:18:10,490
using that compression algorithm into roughly n times in tribute and p bit so that's

1145
01:18:10,490 --> 01:18:13,180
the source coding theorem i'm not going to improve it for you it's in the

1146
01:18:13,180 --> 01:18:16,760
book but i'm going to show you some algorithms that do get you in the

1147
01:18:16,760 --> 01:18:20,060
first case very close to this will sometimes close to us and in the second

1148
01:18:21,350 --> 01:18:28,260
arbitrarily close to to this limit

1149
01:18:29,120 --> 01:18:33,310
it is if we were

1150
01:18:33,430 --> 01:18:40,290
is our connection of this entropy to the physics entropy yes and there isn't time

1151
01:18:40,290 --> 01:18:42,760
for me to talk about it this morning because there's lots of other fun stuff

1152
01:18:42,760 --> 01:18:43,390
to do

1153
01:18:43,390 --> 01:18:46,160
so let's talk about symbol codes

1154
01:18:46,180 --> 01:18:48,200
in the book

1155
01:18:50,530 --> 01:18:55,850
simple codes are a particular way of trying to do compression they don't solve the

1156
01:18:55,850 --> 01:18:59,430
source coding theorem but sometimes they get you close that of symbol code is we

1157
01:18:59,430 --> 01:19:04,300
assume we are compressing something like english maybe the alphabet is a to z and

1158
01:19:04,300 --> 01:19:09,350
space and we assume that we've got a probability distribution on those characters that we

1159
01:19:09,350 --> 01:19:14,430
know what we believe is the correct probability distribution we're assuming that every character in

1160
01:19:14,430 --> 01:19:20,640
the file comes IID from this distribution so here's is the distribution of english using

1161
01:19:20,640 --> 01:19:25,030
some limits fact as the source of english

1162
01:19:25,040 --> 01:19:27,560
space is very probable so i and t

1163
01:19:27,580 --> 01:19:31,890
and the idea of a simple code is given these probabilities we then say i'm

1164
01:19:31,890 --> 01:19:34,970
going to compress this into binary

1165
01:19:34,990 --> 01:19:38,010
by giving every element in the alphabet

1166
01:19:38,060 --> 01:19:40,620
a binary name

1167
01:19:40,640 --> 01:19:43,100
the binary name is going to have some link

1168
01:19:43,100 --> 01:19:46,910
and this can be bought at the symbol code supermarket which looks like this each

1169
01:19:46,910 --> 01:19:48,180
of them could

1170
01:19:48,240 --> 01:19:50,870
every one of them can have different lengths so you say i'm going to encode

1171
01:19:51,090 --> 01:19:55,810
a with one zero be with one zero one zero i could say with zero

1172
01:19:55,810 --> 01:19:57,220
one and so forth

1173
01:19:57,260 --> 01:20:00,410
and then the rule is to compress

1174
01:20:00,430 --> 01:20:02,180
emma woodhouse bowled one

1175
01:20:03,370 --> 01:20:05,540
look what's the code for e

1176
01:20:05,540 --> 01:20:07,950
certain configuration

1177
01:20:07,970 --> 01:20:10,190
in the case of the magnetic field

1178
01:20:10,240 --> 01:20:12,890
it represents the work that i have to do

1179
01:20:12,920 --> 01:20:14,780
to get current going

1180
01:20:14,780 --> 01:20:17,160
inside pure self-interest

1181
01:20:17,220 --> 01:20:21,450
that means the resistance of the cell conductor is zero

1182
01:20:21,500 --> 01:20:23,070
and it takes work

1183
01:20:23,070 --> 01:20:26,660
because the solenoid will oppose the building up of the current

1184
01:20:26,710 --> 01:20:30,400
so i have to do work

1185
01:20:30,450 --> 01:20:35,960
so is a parallel between the two

1186
01:20:36,030 --> 01:20:38,360
i can make you see

1187
01:20:38,400 --> 01:20:40,740
in a quite dramatic way

1188
01:20:40,800 --> 01:20:44,130
how strong self inductances can fight

1189
01:20:44,170 --> 01:20:46,090
one current

1190
01:20:46,090 --> 01:20:50,290
anyway i'm going to do that this was an set up there

1191
01:20:50,340 --> 01:20:52,000
by have travelled

1192
01:20:52,070 --> 01:20:55,300
car battery

1193
01:20:55,320 --> 01:20:56,460
and i have

1194
01:20:56,560 --> 01:21:01,080
two labels

1195
01:21:01,100 --> 01:21:04,210
i have here an enormous self inductance l

1196
01:21:04,230 --> 01:21:08,660
thirty henry we will later in the course how you make such a high

1197
01:21:08,710 --> 01:21:10,870
self inductance

1198
01:21:10,910 --> 01:21:13,530
and here was the libel

1199
01:21:13,570 --> 01:21:18,670
the light as the resistance of six all

1200
01:21:18,720 --> 01:21:22,220
this self inductance there's nothing we can do about it happens to have four all

1201
01:21:23,900 --> 01:21:25,750
we don't have itself we don't have

1202
01:21:25,800 --> 01:21:27,670
superconducting wires

1203
01:21:27,690 --> 01:21:30,640
so it also has the resistance of four

1204
01:21:30,680 --> 01:21:36,370
forgive me for that but there's nothing i can do about it

1205
01:21:36,410 --> 01:21:38,640
i have here

1206
01:21:38,640 --> 01:21:41,720
and that the resistance of four

1207
01:21:43,390 --> 01:21:45,960
the light bulb which is the same ones that one

1208
01:21:46,040 --> 01:21:47,470
also six

1209
01:21:47,490 --> 01:21:50,360
and here is my car battery

1210
01:21:50,400 --> 01:21:51,540
a switch

1211
01:21:51,690 --> 01:21:55,050
and the car batteries twelve

1212
01:21:55,150 --> 01:21:58,180
and i'm going to

1213
01:21:58,250 --> 01:21:59,240
so to speak

1214
01:21:59,250 --> 01:22:00,980
i don't know

1215
01:22:00,990 --> 01:22:03,390
you'll see that this libel

1216
01:22:03,430 --> 01:22:09,160
comes almost instantaneously there's no self inductance in this little well maybe

1217
01:22:09,210 --> 01:22:12,960
few michael henry or even less

1218
01:22:12,970 --> 01:22:19,010
but in this little new self-interest and so the self inductances take it easy to

1219
01:22:19,010 --> 01:22:20,690
current thinking easy

1220
01:22:20,740 --> 01:22:24,240
just wait you will very slowly see that light bulb

1221
01:22:24,290 --> 01:22:28,300
come along and we can calculate how long it takes

1222
01:22:28,380 --> 01:22:30,220
because we have here

1223
01:22:30,280 --> 01:22:32,990
ten all six songs and for all

1224
01:22:33,010 --> 01:22:35,040
so l divided by or

1225
01:22:35,080 --> 01:22:38,200
thirty divided by ten

1226
01:22:38,250 --> 01:22:41,640
so what three seconds

1227
01:22:41,640 --> 01:22:43,210
so what that means is

1228
01:22:43,220 --> 01:22:47,880
even after six seconds which is twice this time

1229
01:22:47,930 --> 01:22:49,110
even then

1230
01:22:49,160 --> 01:22:51,530
the current through the light bulb is only

1231
01:22:51,530 --> 01:22:54,290
eighty six percent of its maximum

1232
01:22:54,370 --> 01:22:58,700
but that means since the light of course is proportional to ice cream or

1233
01:22:58,720 --> 01:22:59,430
in the

1234
01:22:59,440 --> 01:23:04,130
light bulb that the light is only seventy five percent of maximum and even if

1235
01:23:04,130 --> 01:23:06,080
you wait nine seconds

1236
01:23:06,080 --> 01:23:10,100
and the light that comes out he was still only ninety percent of its maximum

1237
01:23:10,150 --> 01:23:12,530
this one comes on immediately

1238
01:23:12,590 --> 01:23:15,130
the reason why we put the for all year

1239
01:23:15,140 --> 01:23:16,980
we want this part

1240
01:23:17,000 --> 01:23:21,470
to be from an ohmic resistance point of view to be identical is this part

1241
01:23:21,490 --> 01:23:24,000
that's why artificially added before

1242
01:23:24,050 --> 01:23:25,620
before is always

1243
01:23:25,630 --> 01:23:28,840
there the self inductances nothing we can do about it

1244
01:23:28,850 --> 01:23:31,980
so you're going to see a remarkable example

1245
01:23:32,010 --> 01:23:33,690
this is one light

1246
01:23:33,700 --> 01:23:35,160
that's the one

1247
01:23:35,200 --> 01:23:37,610
that is here

1248
01:23:37,650 --> 01:23:40,290
and this is the light bulb

1249
01:23:40,320 --> 01:23:42,430
is there

1250
01:23:42,460 --> 01:23:47,700
and the self inductances in this incredible monster here

1251
01:23:47,750 --> 01:23:51,210
make the lights change the light setting a little bit so that we can see

1252
01:23:51,210 --> 01:23:55,400
the light of the book but only eight were built not very

1253
01:23:55,400 --> 01:23:56,510
very strong

1254
01:23:56,710 --> 01:23:58,510
very bright bulbs

1255
01:23:58,570 --> 01:24:01,950
but the effect will be

1256
01:24:01,980 --> 01:24:04,900
very clear so if you're ready for this this is the one that i think

1257
01:24:04,900 --> 01:24:06,200
comes on immediately

1258
01:24:06,210 --> 01:24:07,310
this is the one

1259
01:24:07,360 --> 01:24:10,150
it takes its time

1260
01:24:10,180 --> 01:24:13,290
three two one zero

1261
01:24:16,840 --> 01:24:20,840
six you see how slowly it still not very bright

1262
01:24:20,860 --> 01:24:23,670
still getting brighter

1263
01:24:23,750 --> 01:24:25,620
still belongs right is this one

1264
01:24:25,650 --> 01:24:27,200
getting there

1265
01:24:27,210 --> 01:24:33,060
you can actually do some finding accounting

1266
01:24:33,100 --> 01:24:36,100
by the way the values of the resistance and again you are when the light

1267
01:24:36,100 --> 01:24:37,630
bulb so hot three

1268
01:24:37,680 --> 01:24:39,910
two one zero one

1269
01:24:41,870 --> 01:24:44,340
five six seven eight

1270
01:24:45,640 --> 01:24:50,720
eleven twelve i'm still seeing getting brighter

1271
01:24:50,810 --> 01:24:53,990
what you're seeing here is a remarkable example

1272
01:24:54,070 --> 01:24:55,840
that the self inductance

1273
01:24:55,890 --> 01:24:57,530
is fighting

1274
01:24:59,280 --> 01:25:04,440
that's why the name self inductances so nice

1275
01:25:06,320 --> 01:25:07,340
i want to go

1276
01:25:07,360 --> 01:25:09,460
one step further

1277
01:25:10,790 --> 01:25:13,690
i want to

1278
01:25:13,740 --> 01:25:16,250
o in the l our circuit

1279
01:25:16,300 --> 01:25:18,360
with a

1280
01:25:18,410 --> 01:25:19,740
a c

1281
01:25:19,740 --> 01:25:21,010
learning paradigm now

1282
01:25:21,800 --> 01:25:25,180
you know you have a bunch of images is a great by two million motorcycles and say well

1283
01:25:25,620 --> 01:25:26,870
can you ask me what that's

1284
01:25:27,780 --> 01:25:29,120
which is still a challenging problem

1285
01:25:31,430 --> 01:25:31,930
but let me

1286
01:25:32,660 --> 01:25:37,220
switch gears a little bit and see what we faced with the following problem

1287
01:25:37,570 --> 01:25:41,530
suppose i give you millions of unlabeled images so you get to learn something about

1288
01:25:41,530 --> 01:25:44,100
the world you get to learn something about how images should look like

1289
01:25:44,660 --> 01:25:47,070
know those could be text could be speech signal

1290
01:25:48,260 --> 01:25:51,870
different domains and then i give you partially labeled data show you a few

1291
01:25:52,350 --> 01:25:54,870
you know dolphins assure you tractors few elephants

1292
01:25:55,430 --> 01:25:56,200
now the question is

1293
01:25:56,700 --> 01:25:58,760
can you take all of his background knowledge

1294
01:25:59,140 --> 01:26:02,350
and then learn new concept from just one example a few examples

1295
01:26:04,100 --> 01:26:08,260
and obviously you know it's it's a very interesting question that people in the deep

1296
01:26:08,260 --> 01:26:12,660
learning community asking is that whenever you learn these features with high-level features

1297
01:26:13,240 --> 01:26:16,530
obviously if you can learn meaningful and useful high-level features

1298
01:26:17,050 --> 01:26:19,780
you should be able to start learning new things very quickly

1299
01:26:21,530 --> 01:26:26,660
obviously if you can recognize and pass images and intercourse here and semantic meaning

1300
01:26:27,140 --> 01:26:29,090
and if i show you a new object you've seen

1301
01:26:29,800 --> 01:26:32,450
you should be able to recognise and be able to learn very quickly

1302
01:26:32,950 --> 01:26:33,930
right if you can't

1303
01:26:34,370 --> 01:26:36,950
then probably the features that you're learning and not very meaningful

1304
01:26:38,760 --> 01:26:41,260
right and then start testing what that's so that's

1305
01:26:41,700 --> 01:26:44,850
that's the key problem in the visions speech perception in

1306
01:26:45,530 --> 01:26:48,590
natural language processing different a little different domains

1307
01:26:49,100 --> 01:26:51,600
so here's one particular classes of models that

1308
01:26:53,160 --> 01:26:53,890
people have looked at

1309
01:26:54,510 --> 01:26:57,910
so what you do is you contributing to kinds of different models

1310
01:26:58,620 --> 01:27:00,320
integrating hierarchical bayesian models

1311
01:27:00,780 --> 01:27:01,910
which are useful for

1312
01:27:02,820 --> 01:27:04,820
for constructing category structure

1313
01:27:05,390 --> 01:27:08,240
right so the way you might go around this problem can say well see kind

1314
01:27:08,280 --> 01:27:09,760
looks like a funny kind over

1315
01:27:10,470 --> 01:27:11,140
he correct

1316
01:27:11,590 --> 01:27:12,430
what kind of looks like

1317
01:27:12,910 --> 01:27:16,450
maybe some of these things obviously some relationship with integrating these classes

1318
01:27:17,320 --> 01:27:19,680
is shared high-level features right like

1319
01:27:20,300 --> 01:27:21,970
like wills in end

1320
01:27:24,010 --> 01:27:28,200
handles and such and they should low-level features like edges a combination of

1321
01:27:29,550 --> 01:27:34,830
right so deep models are very good in terms of unsupervised feature learning and learning features a hierarchy

1322
01:27:35,320 --> 01:27:38,370
and the hierarchical bayesian models are very good in terms of constructing the heart is

1323
01:27:38,370 --> 01:27:40,180
how different optics are related to each other

1324
01:27:41,950 --> 01:27:43,660
and you can basically construct

1325
01:27:44,450 --> 01:27:47,530
a hierarchical model where you have dependencies between

1326
01:27:48,030 --> 01:27:50,140
classes so you might say well horses and cows

1327
01:27:50,620 --> 01:27:51,470
should be grouped together

1328
01:27:51,910 --> 01:27:53,600
they carry a lot of high-level parts

1329
01:27:54,070 --> 01:27:57,870
cars vans and trucks should be grouped together material a high-level parts

1330
01:27:58,450 --> 01:28:01,870
i at the lower level you might have a

1331
01:28:03,430 --> 01:28:07,970
distinctive perceptual structure of a specific concept in my head edges and combination edges that get

1332
01:28:08,850 --> 01:28:09,720
they get put together

1333
01:28:10,450 --> 01:28:14,590
so there is one particular model you you can construct and you can also learn

1334
01:28:14,720 --> 01:28:16,530
what that hierarchical organization should be

1335
01:28:17,120 --> 01:28:20,300
so the tree hierarchy of relationship between objects is learned

1336
01:28:21,220 --> 01:28:24,760
that can be used something that's called nested chinese restaurant process prior

1337
01:28:25,350 --> 01:28:29,620
it is a very simple private effectively assigns probabilities for different trees

1338
01:28:31,050 --> 01:28:31,990
different tree structures

1339
01:28:32,410 --> 01:28:36,140
and you can sample the states of the latent variables from something scale hierarchical dirichlet

1340
01:28:36,140 --> 01:28:39,430
process prior again this a simple prior that allows you to share parts

1341
01:28:40,220 --> 01:28:41,550
across different objects

1342
01:28:42,220 --> 01:28:44,910
and then the distribution of the observed data

1343
01:28:45,390 --> 01:28:48,910
is gonna be given by boltzmann machine which effectively will enforce

1344
01:28:49,780 --> 01:28:53,200
approximately global consistency by looking at many many local constraints

1345
01:28:53,950 --> 01:28:57,990
so both machines are very good in terms of generating data if i tell doubles

1346
01:28:57,990 --> 01:29:01,470
machines these features e i'm seeing generate me something coherent

1347
01:29:01,870 --> 01:29:02,930
they're very good at doing that

1348
01:29:03,430 --> 01:29:08,990
hierarchical bayes models are very good explicitly defining higher it is and how objects relate

1349
01:29:08,990 --> 01:29:11,240
to each other how they can be put into groups together

1350
01:29:12,620 --> 01:29:16,600
so unlike many standard statistical models here you're trying to infer problems but you're also

1351
01:29:16,600 --> 01:29:21,070
trying to infer the category hierarchy of how different objects are related to each other

1352
01:29:21,680 --> 01:29:27,370
so for example for these models where low-level features are built in unsupervised fashion high-level

1353
01:29:27,370 --> 01:29:31,220
features are built by looking at the labelled examples are fifty thousand images have different

1354
01:29:32,700 --> 01:29:35,160
and this is the kind hearted that the model is learning right

1355
01:29:36,120 --> 01:29:39,800
so you know it puts fruits together humans animals and then there is

1356
01:29:40,760 --> 01:29:43,200
the learned high-level features learned low-level features

1357
01:29:43,870 --> 01:29:48,160
right so trying to learn the hard your politics related together as well as low

1358
01:29:48,470 --> 01:29:53,870
end high-level features and it terms of the hierarchy itself right the mopix up things

1359
01:29:53,870 --> 01:29:55,050
like you know different

1360
01:29:55,470 --> 01:29:58,490
the crocodile lizards spiders snakes get grouped together

1361
01:29:58,990 --> 01:30:00,260
you know there's different man-boy

1362
01:30:00,850 --> 01:30:06,430
girl get put together different trees that get put together right so a of that structure basically involves

1363
01:30:07,070 --> 01:30:07,740
by looking

1364
01:30:08,490 --> 01:30:09,320
good at the data

1365
01:30:11,030 --> 01:30:11,820
and how

1366
01:30:12,780 --> 01:30:15,410
transfer learning about one one is not mentioned

1367
01:30:15,590 --> 01:30:16,490
and i show you an apple

1368
01:30:17,300 --> 01:30:19,200
and these are high-level features that you get

1369
01:30:19,680 --> 01:30:20,600
i show your red apple

1370
01:30:21,100 --> 01:30:22,220
now is that the shape

1371
01:30:22,740 --> 01:30:25,010
get shared together because they have the same shape

1372
01:30:25,660 --> 01:30:27,140
and the color features are different

1373
01:30:27,820 --> 01:30:31,620
the model basically figures out that the best way to code the data and that's

1374
01:30:31,620 --> 01:30:32,680
the best way to go to right

1375
01:30:33,140 --> 01:30:34,700
you want get the shape and color

1376
01:30:35,430 --> 01:30:39,070
out but let's say i get an image over orange

1377
01:30:39,550 --> 01:30:44,530
and again orange sits on the same class and again it sort of captures these shared high-level features

1378
01:30:45,240 --> 01:30:47,160
but let's say i wanna classify sunflower

1379
01:30:48,490 --> 01:30:51,990
so what is sunflower well south was basically an orange with gotten it right

1380
01:30:53,760 --> 01:30:59,740
basically have the same structure and that's what the mos discovering right semantically sunflowers not really related to orange is

1381
01:31:00,220 --> 01:31:05,070
the visually always discovering that is basically relates to apples and oranges just

1382
01:31:05,550 --> 01:31:08,280
except with these features these couple of features that you see

1383
01:31:09,470 --> 01:31:14,260
now all these high-level features are being learned on the background set if i'm looking

1384
01:31:14,260 --> 01:31:16,470
at dolphins dolphins will have very different features

1385
01:31:17,320 --> 01:31:21,930
so by learning the hierarchy for sharing parameters as well as you know learning these

1386
01:31:21,930 --> 01:31:25,350
high-level features and how they relate to each other you can basically learn

1387
01:31:26,070 --> 01:31:31,990
a new concept very quickly by sharing high-level features as well as figuring out relationship between objects

1388
01:31:32,660 --> 01:31:37,090
right and in terms of doing sort of recognition from one two three ten fifty

1389
01:31:37,100 --> 01:31:41,680
examples these kinds of models they do much much better compared to know traditional both

1390
01:31:41,680 --> 01:31:42,470
machines only you

1391
01:31:43,430 --> 01:31:45,700
and other models you can also do fun things like

1392
01:31:46,430 --> 01:31:49,850
generating data from these examples especially three apples

1393
01:31:50,300 --> 01:31:51,180
is what it generates

1394
01:31:53,320 --> 01:31:56,680
if i show you will treat is what willow trees look like

1395
01:31:57,510 --> 01:32:00,890
it looks like because painting but this just based on an example them always seems

1396
01:32:01,990 --> 01:32:02,600
will increase

1397
01:32:03,600 --> 01:32:08,300
if i show you right it is what it generates figures out there should be something blowing up right

1398
01:32:12,010 --> 01:32:15,430
again the beauty of this moses there's nothing specific in this model

1399
01:32:15,430 --> 01:32:18,030
with the gauss and charge

1400
01:32:18,130 --> 01:32:20,950
how do you understand this

1401
01:32:20,990 --> 01:32:25,460
well the third of may be the easiest way to understand the whole

1402
01:32:25,470 --> 01:32:29,260
if you have

1403
01:32:29,380 --> 01:32:34,080
as you say you have hit distribution this amount of heat

1404
01:32:34,120 --> 01:32:39,580
what happens to be quite why do i have this form well tool

1405
01:32:39,610 --> 01:32:43,000
think about it let's just think about it at one point so if i have

1406
01:32:43,420 --> 01:32:45,680
exactly what point

1407
01:32:45,700 --> 01:32:48,770
so i have some fixed amount of heat at one point

1408
01:32:48,780 --> 01:32:50,370
but it has to be actually

1409
01:32:50,380 --> 01:32:55,550
delta function let's think about a unit amount of heat that the point what happens

1410
01:32:55,550 --> 01:33:00,680
to the well it spreads around how does it spread around it spread around according

1411
01:33:00,680 --> 01:33:03,380
to the girls and distribution so after time t

1412
01:33:03,490 --> 01:33:05,800
become foegaussian like this

1413
01:33:05,850 --> 01:33:07,460
OK so

1414
01:33:07,630 --> 01:33:09,780
the in the beginning i have

1415
01:33:09,820 --> 01:33:14,090
kind of this infinite amount of heat at one point then it becomes very little

1416
01:33:14,140 --> 01:33:17,460
goes and the fact that gauss and so on

1417
01:33:17,480 --> 01:33:19,730
and it's a sequence of

1418
01:33:19,740 --> 01:33:23,330
you know gaussians which grow progressively flat and

1419
01:33:23,350 --> 01:33:25,120
fat and flour

1420
01:33:33,650 --> 01:33:36,030
that formula follows from this

1421
01:33:36,050 --> 01:33:39,780
so if you believe that this is true that the individual amount of key it

1422
01:33:39,850 --> 01:33:43,490
becomes gauss then this formula should be

1423
01:33:43,500 --> 01:33:47,830
almost immediately actually made it because if what is integral in general is just to

1424
01:33:47,830 --> 01:33:52,260
stop so you can think about your initial q distribution having some fixed amount of

1425
01:33:52,260 --> 01:33:53,900
heat and every point

1426
01:33:53,970 --> 01:33:58,250
what happens to this heat well it's about if you becomes gauss

1427
01:33:58,260 --> 01:34:01,830
so now you have a sum of an infinite amount of gaussians coming from which

1428
01:34:01,830 --> 01:34:04,740
point well that's exactly what is far below

1429
01:34:04,930 --> 01:34:07,170
so that this kind of intuitive

1430
01:34:07,180 --> 01:34:09,210
interpretation of

1431
01:34:11,150 --> 01:34:13,590
solution to the heat equation

1432
01:34:15,180 --> 01:34:17,370
so if you believe that

1433
01:34:27,250 --> 01:34:30,180
you can

1434
01:34:32,270 --> 01:34:33,670
what can you do

1435
01:34:33,730 --> 01:34:35,430
you can say

1436
01:34:35,450 --> 01:34:41,840
if you believe that this is the correct heat equation that the plus some of

1437
01:34:41,840 --> 01:34:47,130
our enough you fixed is equal to do you over dt

1438
01:34:47,150 --> 01:34:51,160
then how would you compute the law applies manually to compute the uprising you can

1439
01:34:51,160 --> 01:34:54,240
if you know the function and its in our and you just take the derivative

1440
01:34:54,360 --> 01:34:55,640
you some them up

1441
01:34:55,660 --> 01:35:00,330
the usual thing but what if i just want to use this heat equation

1442
01:35:00,350 --> 01:35:04,340
well i think about this

1443
01:35:04,350 --> 01:35:08,020
i want to start this some f of x

1444
01:35:08,100 --> 01:35:11,300
i seek propagate at a little bit

1445
01:35:12,580 --> 01:35:15,540
and then i take the time derivative

1446
01:35:15,570 --> 01:35:16,840
all of

1447
01:35:16,860 --> 01:35:18,020
that function

1448
01:35:18,180 --> 01:35:22,250
well according to the heat equation

1449
01:35:22,260 --> 01:35:24,130
the time derivative

1450
01:35:24,180 --> 01:35:30,200
if my efforts the initial heat distribution and i propagated just a little bit the

1451
01:35:30,200 --> 01:35:33,800
time derivative of this is nothing else but a lot less

1452
01:35:33,830 --> 01:35:40,800
so this suggests a way of approximating the laplace operator well how the approximated i

1453
01:35:40,800 --> 01:35:42,470
take my function

1454
01:35:42,490 --> 01:35:45,740
i smooth it out using the heat equation

1455
01:35:45,830 --> 01:35:49,470
and i take the derivative of that

1456
01:35:49,480 --> 01:35:50,710
make sense

1457
01:35:53,720 --> 01:35:57,790
if this makes sense then well

1458
01:35:57,830 --> 01:36:02,790
what is your phi xt ut of xt is nothing else but this form of

1459
01:36:02,800 --> 01:36:07,840
this convolution so i've basically what do i have to do i just have to

1460
01:36:07,860 --> 01:36:16,090
differentiate this integral with respect to time and that is the laplace operator

1461
01:36:21,540 --> 01:36:26,760
this is it's freedom here so the laplace operator if f is the time derivative

1462
01:36:26,760 --> 01:36:30,330
of this convolution

1463
01:36:30,380 --> 01:36:33,970
well now we all know what the derivative look like

1464
01:36:34,020 --> 01:36:36,820
the derivative looks like

1465
01:36:36,930 --> 01:36:40,740
well if you want to differentiator function at the point you takes while of the

1466
01:36:40,740 --> 01:36:45,080
point you take a little bit of an incremental you take the difference in new

1467
01:36:45,080 --> 01:36:48,150
divided by the size of things right

1468
01:36:48,200 --> 01:36:49,870
what does it mean in this case

1469
01:36:54,540 --> 01:36:57,560
so what is that function at time zero

1470
01:36:57,560 --> 01:37:00,240
at time zero

1471
01:37:00,270 --> 01:37:04,860
you can see that actually well at time zero the heat propagation of course is

1472
01:37:04,860 --> 01:37:10,300
just the initial state distribution so it's just the function itself

1473
01:37:10,310 --> 01:37:13,090
so at time zero it's f of x and i put the mindset change the

1474
01:37:13,090 --> 01:37:14,340
order of

1475
01:37:14,360 --> 01:37:20,210
it should be integral minus f of x i put my here

1476
01:37:20,240 --> 01:37:23,490
so what do i have i have f of x

1477
01:37:23,510 --> 01:37:25,030
minor so this is

1478
01:37:25,040 --> 01:37:28,060
the at time zero minus at time t

1479
01:37:28,070 --> 01:37:31,920
divided by one of the t

1480
01:37:32,030 --> 01:37:34,200
so now i have an approximation

1481
01:37:35,150 --> 01:37:37,550
the laplace operator OK

1482
01:37:37,550 --> 01:37:41,920
so we now know how to compute the laplace operator from

1483
01:37:41,950 --> 01:37:43,910
the heat equation

1484
01:37:43,920 --> 01:37:48,430
i mean this is very in some sense for elementary

1485
01:37:48,580 --> 01:37:52,070
once you should have tried to think down

1486
01:37:52,400 --> 01:38:00,380
OK but this actually has implications

1487
01:38:01,120 --> 01:38:05,310
well what happens when we have data when the data and when the data is

1488
01:38:05,310 --> 01:38:08,050
sampled from

1489
01:38:08,070 --> 01:38:13,450
OK i want to say a uniform distribution on around but unfortunately there is no

1490
01:38:13,450 --> 01:38:17,870
such thing as a uniform distribution around so i'm just going to pretend that it

1491
01:38:19,310 --> 01:38:20,820
right if you

1492
01:38:20,830 --> 01:38:25,880
if there is if there is some problem then the best way is just ignore

1493
01:38:26,800 --> 01:38:31,420
so i'm just going to ignore it and say well

1494
01:38:31,440 --> 01:38:34,460
let's just say this is a uniform distribution

1495
01:38:34,540 --> 01:38:40,160
in this is it is a uniform distribution on then this farm home while there

1496
01:38:40,170 --> 01:38:43,510
should be actually one the and

1497
01:38:43,530 --> 01:38:45,260
the average of this

1498
01:38:45,260 --> 01:38:49,320
i guess i'd like to start off by since this is the last eight think

1499
01:38:49,320 --> 01:38:50,530
our hosts

1500
01:38:50,550 --> 01:38:55,350
at the summer school for those of you who are not organised such events you

1501
01:38:55,350 --> 01:39:01,410
find its millions of little details and and i think it's an excellent time for

1502
01:39:01,410 --> 01:39:03,150
me and i hope you all

1503
01:39:03,410 --> 01:39:06,290
twenty one so thank you very much for discrete

1504
01:39:12,230 --> 01:39:16,760
but all a i'm going to talk about the general

1505
01:39:16,780 --> 01:39:22,280
topic of speech production models in are automatic speech recognition i guess you could find

1506
01:39:22,280 --> 01:39:24,580
a lot of researchers who

1507
01:39:24,590 --> 01:39:31,140
have over the last twenty years twenty years investigated how we can build automatic speech

1508
01:39:31,140 --> 01:39:34,490
recognition systems that are based on

1509
01:39:34,520 --> 01:39:38,790
a models of speech production and you can find

1510
01:39:38,840 --> 01:39:45,620
a reasonable pretty good set of researchers will say that we're never going to solve

1511
01:39:45,620 --> 01:39:50,210
any of the the basic problems that we have both robustness

1512
01:39:50,220 --> 01:39:55,500
and portability to new domains and issues like that unless we

1513
01:39:55,770 --> 01:40:03,900
have our systems the more you will be more closely aligned with

1514
01:40:04,170 --> 01:40:08,950
four were built on principles that are more closely aligned

1515
01:40:08,970 --> 01:40:14,480
with speech production but on the other hand over the last twenty years most of

1516
01:40:15,110 --> 01:40:21,650
major developments of the important advances in speech recognition have been in the areas of

1517
01:40:21,680 --> 01:40:24,440
statistical formalisms for

1518
01:40:24,960 --> 01:40:32,700
we get for learning and modeling that have had some but little to do with

1519
01:40:32,700 --> 01:40:34,520
models of speech production

1520
01:40:34,530 --> 01:40:35,440
so well

1521
01:40:35,450 --> 01:40:36,770
what i'd like to do

1522
01:40:36,790 --> 01:40:40,770
is this talk a little bit about why this is a hard problem to begin

1523
01:40:40,770 --> 01:40:50,140
with the words what what what are the difficult issues associated with building systems that

1524
01:40:50,140 --> 01:40:56,530
are based on speech production models but also what of the potential payoffs what advantages

1525
01:40:56,530 --> 01:41:04,130
might we have in building systems that incorporate more of more notions of of the

1526
01:41:04,130 --> 01:41:06,360
are human articulatory system

1527
01:41:06,480 --> 01:41:09,190
in in making them more

1528
01:41:09,200 --> 01:41:13,880
but that spend a big portion of the time talking about how

1529
01:41:14,980 --> 01:41:22,230
how we might in hell and existing efforts for exploiting a speech production models in

1530
01:41:23,060 --> 01:41:26,180
we'll talk about statistical models for

1531
01:41:26,810 --> 01:41:29,760
four logical distinctive feature detection

1532
01:41:29,840 --> 01:41:35,930
how we can incorporate this distinctive feature knowledge in ASR model structure

1533
01:41:35,940 --> 01:41:38,700
building models articulatory dynamics

1534
01:41:38,720 --> 01:41:45,250
and how we can perhaps an easier problem is how we might in integrate these

1535
01:41:45,250 --> 01:41:51,720
distinctive features in existing traditional ASR systems and finally to finish up a little bit

1536
01:41:51,720 --> 01:41:59,190
of a lighter discussion will talk about some resources that exist some tools some devices

1537
01:41:59,190 --> 01:42:03,820
and tools that are used to study some of these issues and

1538
01:42:03,860 --> 01:42:07,220
maybe outline some of the projects that have been dedicated to

1539
01:42:07,230 --> 01:42:09,000
to this problem

1540
01:42:10,390 --> 01:42:11,720
the first part

1541
01:42:11,740 --> 01:42:16,230
i'll start out by trying to motivate why this is something interesting

1542
01:42:16,690 --> 01:42:21,380
then out just so we have the same terminology i understand that a lot of

1543
01:42:21,380 --> 01:42:26,440
you are not necessarily people who you got we've got some

1544
01:42:26,480 --> 01:42:31,980
computer scientists and those whose background is in areas of speech translation and natural language

1545
01:42:31,980 --> 01:42:36,390
processing so some of the terminology like that i might use might not be too

1546
01:42:36,390 --> 01:42:39,990
too familiar and so i'll spend a little bit of time reviewing some of those

1547
01:42:39,990 --> 01:42:42,780
fundamentals and i'll talk about

1548
01:42:43,210 --> 01:42:51,820
talk about some issues relating to using canonical baseform dictionaries and that's problems with full

1549
01:42:51,830 --> 01:43:00,230
nedic phonemic dictionaries not ponemic dictionaries sorry about the the type of their

1550
01:43:00,250 --> 01:43:03,080
and then i'll talk about

1551
01:43:03,120 --> 01:43:05,380
the some models of

1552
01:43:05,700 --> 01:43:09,470
how some models of how speech production

1553
01:43:09,830 --> 01:43:14,490
fits into the two models of speech perception so

1554
01:43:14,510 --> 01:43:21,640
why would we want to use articulatory representations for speech recognition well i guess our

1555
01:43:22,460 --> 01:43:26,870
some of the basic some of the basic data

1556
01:43:26,910 --> 01:43:31,870
that has been collected by people of mainly at haskins labs in the united states

1557
01:43:33,960 --> 01:43:40,520
how all the articulatory representations are put together we'll talk about speech is the organisation

1558
01:43:40,520 --> 01:43:42,410
of articulatory gestures

1559
01:43:42,420 --> 01:43:51,960
the notion of critical articulators an acoustic invariant articulatory invariance and evidence for the usefulness

1560
01:43:52,280 --> 01:43:56,820
of of articulatory knowledge in existing systems

1561
01:43:56,870 --> 01:43:59,370
so this is kind of a very interesting

1562
01:43:59,380 --> 01:44:09,980
set up plots here that was obtained that was obtained from parallel acoustic articulatory measurements

1563
01:44:09,980 --> 01:44:14,610
and there's some theoretical performance guarantees

1564
01:44:14,730 --> 01:44:18,360
on the other hand there are some difficulties it's not really clear how can we

1565
01:44:18,360 --> 01:44:21,590
incorporate prior knowledge into into the

1566
01:44:21,610 --> 01:44:27,090
posting on how this can be improved maybe the linear programming algorithm as well

1567
01:44:27,360 --> 01:44:33,880
and often realizations of essential essential so the best strategy for player but so i've

1568
01:44:34,000 --> 01:44:37,250
shown you some some strategies

1569
01:44:37,610 --> 01:44:41,060
and of course the best choice of the vehicle is not up to snuff this

1570
01:44:41,060 --> 01:44:45,460
is essentially the same problem as in support vector machines where you have to choose

1571
01:44:45,460 --> 01:44:50,230
a kernel the same problem you here you have to choose an appropriate set based

1572
01:44:50,230 --> 01:44:58,480
on appropriate courses both both ways to find some the the feature space for and

1573
01:44:58,480 --> 01:45:02,590
if just feature space is always what matters

1574
01:45:08,020 --> 01:45:14,900
people have used boosting with systems and c four point five and they often found

1575
01:45:15,210 --> 01:45:17,650
like when they compared

1576
01:45:20,060 --> 01:45:26,860
UCI benchmarks they find that points boosting stumps or compared to c four point five

1577
01:45:27,420 --> 01:45:29,420
is is better

1578
01:45:29,440 --> 01:45:33,520
i mean they one c four point five on the dataset one one point is

1579
01:45:33,520 --> 01:45:35,060
the data set k

1580
01:45:35,060 --> 01:45:40,500
then this is a transition our and they find that posting stumps they get smaller

1581
01:45:41,840 --> 01:45:43,690
the most cases but not all

1582
01:45:45,480 --> 01:45:50,520
and if you wish to c four point five that usually leads to an improvement

1583
01:45:50,520 --> 01:45:54,690
so there are four four c four point five are usually large

1584
01:45:54,690 --> 01:46:02,540
but this only shows essentially the boosting improves i mean many have one base learners

1585
01:46:02,560 --> 01:46:06,290
then boosting improves many people i mean

1586
01:46:07,170 --> 01:46:14,560
based on boosting then the generalisation becomes but often it's not compared against other based

1587
01:46:14,900 --> 01:46:20,360
or against the support vector machines and this engine then you when you do this

1588
01:46:20,360 --> 01:46:26,500
comparison the oftentimes that boosted stumps of city four point five well performed with us

1589
01:46:26,500 --> 01:46:29,920
this because of the based on c four point five is not really the best

1590
01:46:30,750 --> 01:46:33,040
which i found my own experience

1591
01:46:33,040 --> 01:46:42,460
so for instance i compared boosting their RBF networks with boosting c four point five

1592
01:46:42,460 --> 01:46:47,890
lb posting of it networks all the very often performs much better than c four

1593
01:46:47,890 --> 01:46:54,480
point five postings four five and boosting RBF fall off very close to support vector

1594
01:46:54,480 --> 01:46:58,610
machines performance with RBF kernels

1595
01:46:58,630 --> 01:47:01,480
OK so you some results

1596
01:47:01,500 --> 01:47:07,250
there's also a dataset collection which you can download from home page so this is

1597
01:47:07,250 --> 01:47:13,520
thirteen datasets thinking and used by a bayes function networks

1598
01:47:13,590 --> 01:47:21,630
than adaboost using these radial basis function networks adaboost break this regularized adaboost than linear

1599
01:47:21,630 --> 01:47:28,830
programming adaboost maybe just run adaboost twenty iterations and then after afterwards we just optimise

1600
01:47:28,830 --> 01:47:33,810
the the weights using soft margin and his QP version and you support vector machines

1601
01:47:34,090 --> 01:47:40,610
OK and you find that adaboost drake is very all the best and it's very

1602
01:47:40,630 --> 01:47:44,630
competitive with the support machines

1603
01:47:44,650 --> 01:47:48,000
so there's some odd results which i quite quite awhile

1604
01:47:48,130 --> 01:47:50,790
and you find them together with the

1605
01:47:50,920 --> 01:47:59,650
benchmark results so there are some key to results on the side

1606
01:47:59,920 --> 01:48:05,340
OK like to mention a few other applications but just very briefly so

1607
01:48:06,840 --> 01:48:11,880
boosting has been used for text classification so they forced by ship you anything they

1608
01:48:13,230 --> 01:48:18,650
stands with the normalized term frequency and multi class encoding

1609
01:48:18,690 --> 01:48:24,540
so there was some application on optical character recognition that the use of multi perceptrons

1610
01:48:24,540 --> 01:48:32,190
space and the control the improvement the the performance was quite a bit improved

1611
01:48:32,210 --> 01:48:38,790
then also natural language processing image retrieval just to mention a few i've used that

1612
01:48:39,480 --> 01:48:43,840
are OK medical diagnosis i've used it also brought detection

1613
01:48:43,900 --> 01:48:54,290
in drug discovery some applications and the quantitative structure activity relationship modelling

1614
01:48:54,330 --> 01:49:01,770
and also some application electric power monitoring so be applied to posting to RBF networks

1615
01:49:01,770 --> 01:49:05,480
and to some this was trying to show

1616
01:49:05,630 --> 01:49:10,290
so that's a much fuller list of applications in trapeze review paper and also in

1617
01:49:10,290 --> 01:49:17,340
the review paper of and myself say they find someone altercations

1618
01:49:17,360 --> 01:49:18,750
OK i'm

1619
01:49:18,810 --> 01:49:21,840
essentially at end the early sorry

1620
01:49:22,060 --> 01:49:26,420
maybe there are some more questions which can fill the last ten minutes something

1621
01:49:26,690 --> 01:49:27,980
OK so

1622
01:49:28,060 --> 01:49:29,860
thank you

1623
01:49:37,290 --> 01:49:38,630
so the more questions

1624
01:49:38,650 --> 01:49:39,940
so maybe

1625
01:49:40,110 --> 01:49:43,630
let me give you an overview slide so you remember what to what i talked

1626
01:49:45,570 --> 01:49:55,340
i mean i guess the basic issues for really clear always talk about the so

1627
01:49:55,340 --> 01:49:56,690
the idea so

1628
01:49:56,770 --> 01:50:03,960
are there any questions additional for that matter

1629
01:50:04,000 --> 01:50:05,710
OK so then this

1630
01:50:05,940 --> 01:50:10,050
this OK then maybe yes

1631
01:50:15,070 --> 01:50:17,550
sure this

1632
01:50:26,390 --> 01:50:31,260
it is

1633
01:50:32,060 --> 01:50:34,770
as many

1634
01:50:38,390 --> 01:50:43,730
so the question is about the complexity of the base is right so how how

1635
01:50:43,750 --> 01:50:48,470
complex or what the right policies right so i mean OK so there are many

1636
01:50:48,470 --> 01:50:53,050
different types of policy so there might be decision trees such completely different class from

1637
01:50:53,250 --> 01:50:58,980
matrix or kernel functions like this OK so the time you have to do designed

1638
01:50:58,980 --> 01:51:03,980
for your particular problem OK so maybe if you very smooth functions which you would

1639
01:51:03,980 --> 01:51:09,800
like to approximate maybe RBF networks are better than the decision trees it's really like

1640
01:51:09,800 --> 01:51:14,790
using very few features that may be and it's more tree like the decision then

1641
01:51:14,930 --> 01:51:17,260
maybe use decision trees

1642
01:51:17,310 --> 01:51:21,670
but then the second question is how complex you should you choose your hypothesis class

1643
01:51:21,670 --> 01:51:24,130
thinking about the properties of the network

1644
01:51:25,130 --> 01:51:28,860
so there are some technological issues

1645
01:51:30,180 --> 01:51:31,530
which have to do with

1646
01:51:31,570 --> 01:51:35,870
things like the physical medium over which are transmitting messages

1647
01:51:36,200 --> 01:51:39,850
or the properties of the links are which are transmitting the rate at which you

1648
01:51:39,850 --> 01:51:45,380
can send data the loss rate the number of percentage of messages that lost by

1649
01:51:45,380 --> 01:51:48,170
the link is transmitted so those are the kinds of things we need mean by

1650
01:51:48,170 --> 01:51:49,870
technological concerns

1651
01:51:51,930 --> 01:51:55,880
there are so one kind of technological concern is this

1652
01:51:55,890 --> 01:52:00,020
sort of the different kinds of technology that we might use for transmission and we

1653
01:52:00,040 --> 01:52:04,210
we saw this notion of the technology over the technology over dt

1654
01:52:04,330 --> 01:52:08,970
before i was just this idea that the technology just like the technology in computer

1655
01:52:08,970 --> 01:52:13,700
system has been draw evolving dramatically over time the technology of networks has been evolving

1656
01:52:13,700 --> 01:52:16,920
dramatically and we're going to start to study the race some of the range of

1657
01:52:16,920 --> 01:52:19,760
different technologies that we have to deal with

1658
01:52:19,780 --> 01:52:21,540
so our set of limits

1659
01:52:22,010 --> 01:52:23,900
any time technological

1660
01:52:23,930 --> 01:52:30,050
physical fundamental limits their are associated with networks of these are things like

1661
01:52:30,050 --> 01:52:31,290
the speed of light

1662
01:52:32,110 --> 01:52:37,790
we simply cannot over we can't send messages faster than the speed of light grey

1663
01:52:37,810 --> 01:52:41,310
with these are these are messages that being transmitted down the wire and propagated some

1664
01:52:41,310 --> 01:52:46,590
speed and that's going to be fundamentally bottleneck in in the design of computer systems

1665
01:52:46,650 --> 01:52:49,810
i have to send a message from here to california that message isn't going to

1666
01:52:49,810 --> 01:52:53,680
get any is there any sooner than the amount of time it takes for light

1667
01:52:53,680 --> 01:52:57,920
to travel travel across the country and those numbers sort of the amount of time

1668
01:52:57,920 --> 01:53:01,270
it takes to transmit a message of the country is nontrivial in computer time it

1669
01:53:01,270 --> 01:53:05,000
might be a couple hundred milliseconds and that's a long time as we saw before

1670
01:53:05,000 --> 01:53:09,720
we have a process that can execute billion instructions per second

1671
01:53:09,760 --> 01:53:15,200
OK so the other sort of interesting issues it's going to come out about networks

1672
01:53:15,480 --> 01:53:18,360
is that networks are shared infrastructure

1673
01:53:19,060 --> 01:53:28,310
what that means is that networks there are multiple users who are simultaneously using a

1674
01:53:28,310 --> 01:53:32,320
network because of course although this from using the internet and all the networks that

1675
01:53:32,340 --> 01:53:36,520
going to study in this class are basically a shared infrastructure we'll talk in a

1676
01:53:36,520 --> 01:53:42,310
minute about why sort of the networks are are fundamentally almost always going to be

1677
01:53:42,310 --> 01:53:46,070
sort of shared going to have to transmit data from multiple different users at the

1678
01:53:46,070 --> 01:53:46,950
same time

1679
01:53:46,990 --> 01:53:49,590
and that's going to present a number of challenges about how we ensure that the

1680
01:53:49,590 --> 01:53:50,520
network is

1681
01:53:50,530 --> 01:53:54,460
fair that everybody is that everybody gets the standard a when they want to send

1682
01:53:54,950 --> 01:53:55,800
how we

1683
01:53:55,810 --> 01:53:59,720
sort of allow multiple people to access the same physical wire at the same point

1684
01:53:59,720 --> 01:54:01,860
in time

1685
01:54:01,880 --> 01:54:03,720
OK so

1686
01:54:03,750 --> 01:54:06,400
i want to do is sort of talk about the talk about these two things

1687
01:54:06,400 --> 01:54:10,280
now in order to start off by talking about technology so

1688
01:54:10,290 --> 01:54:36,630
so the first interesting technological problem which i've already mentioned is that these networks are

1689
01:54:36,630 --> 01:54:40,550
extremely diverse there are heterogeneous

1690
01:54:42,360 --> 01:54:46,630
technology networking technology has evolved a great deal in the past twenty years

1691
01:54:46,730 --> 01:54:51,290
for thirty years since networks were first first started being designed and that means that

1692
01:54:51,290 --> 01:54:54,500
there is a huge sort of range of devices that we might have to transmit

1693
01:54:54,500 --> 01:54:55,570
data over

1694
01:54:56,000 --> 01:55:00,790
so let me show you what i mean this is just a graph showing the

1695
01:55:00,790 --> 01:55:04,950
rate of transmission these are the number of bits per second the concerned over different

1696
01:55:04,950 --> 01:55:06,430
networking protocols

1697
01:55:06,650 --> 01:55:10,030
and the thing to take away from this first is to know that on the

1698
01:55:10,040 --> 01:55:11,570
y axis here

1699
01:55:11,580 --> 01:55:14,770
and this is an exponential scale so

1700
01:55:14,770 --> 01:55:18,980
the technology at the very bottom things like that early telephone modems

1701
01:55:19,290 --> 01:55:22,160
maybe send data ten ten kilometres sec

1702
01:55:22,220 --> 01:55:27,050
whereas the sort of high and router class devices that are

1703
01:55:27,070 --> 01:55:28,910
running the running mate

1704
01:55:28,930 --> 01:55:30,330
internet or

1705
01:55:30,350 --> 01:55:35,280
it's very fast desktop ethernet kinds of connections are coming out today

1706
01:55:35,290 --> 01:55:40,290
are concerned more like say in this case we have we have a hundred

1707
01:55:40,300 --> 01:55:41,430
we have

1708
01:55:42,770 --> 01:55:47,510
terrible sack right so one one ten ten ten for talking about a factor of

1709
01:55:47,560 --> 01:55:49,680
ten to the seventh or ten eight

1710
01:55:49,710 --> 01:55:53,630
the difference in performance in terms of the number of bits that these different kinds

1711
01:55:53,630 --> 01:55:55,410
of devices can actually

1712
01:55:55,470 --> 01:56:06,670
that's pretty dramatic and that's gonna make it difficult to design computer systems

1713
01:56:08,230 --> 01:56:11,760
and these kinds of variations occur not only

1714
01:56:11,770 --> 01:56:14,260
at the sort of thing

1715
01:56:14,350 --> 01:56:20,840
number of bits that we can send per second

1716
01:56:22,620 --> 01:56:23,550
but also

1717
01:56:23,560 --> 01:56:26,800
in terms of the propagation delay

1718
01:56:26,840 --> 01:56:27,910
of the links

1719
01:56:29,830 --> 01:56:34,420
if i have a local area network connecting two computers to in my office together

1720
01:56:34,430 --> 01:56:37,020
i may be able to transmit the message from one computer to the other in

1721
01:56:37,200 --> 01:56:40,800
order of microseconds k but as we said before it is sent a message of

1722
01:56:40,810 --> 01:56:45,720
across the country by take two hundred milliseconds to send a message across the

1723
01:56:45,830 --> 01:56:49,710
across the pacific ocean by way of a satellite in orbit around the united states

1724
01:56:49,710 --> 01:56:53,850
might take a second to send a message to mars might take you know

1725
01:56:53,870 --> 01:56:57,330
tens of seconds for a minute so there is this huge range in terms of

1726
01:56:57,330 --> 01:57:00,570
transmission times that some of these different links have

1727
01:57:03,900 --> 01:57:08,920
and fundamentally basically the these sort of two things bit rate and propagation delay

1728
01:57:08,930 --> 01:57:12,510
are going to tell us how long it takes it we would expect to see

1729
01:57:12,540 --> 01:57:15,120
it to take for message to travel across

1730
01:57:15,220 --> 01:57:17,730
the message to travel from one place to the other

1731
01:57:19,380 --> 01:57:23,820
for example we can say bit rate the amount of time so we can

1732
01:57:23,880 --> 01:57:26,520
think of a communication link if you like

1733
01:57:26,610 --> 01:57:28,220
as being a piece of pipe

1734
01:57:28,870 --> 01:57:31,270
which has some

1735
01:57:32,490 --> 01:57:36,400
that is the bit rate

