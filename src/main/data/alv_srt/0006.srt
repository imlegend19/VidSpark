1
00:00:00,000 --> 00:00:05,380
it's it's it's the nice property that has some nice function which which almost looks

2
00:00:05,380 --> 00:00:09,600
like a rectangular function but it just trails off to zero

3
00:00:09,640 --> 00:00:11,010
and i say OK

4
00:00:11,070 --> 00:00:12,910
if i want to construct

5
00:00:12,960 --> 00:00:16,540
an orthonormal expansion

6
00:00:16,570 --> 00:00:18,730
and i want to find another function

7
00:00:18,740 --> 00:00:21,840
which is orthogonal to that function

8
00:00:21,850 --> 00:00:26,410
it has the next biggest amount fraction of its energy inside of this

9
00:00:26,470 --> 00:00:29,980
time time strictly inside this time limit

10
00:00:30,030 --> 00:00:35,810
and as much of the energy within the frequency limit as possible what's next function

11
00:00:35,830 --> 00:00:40,990
OK we solve that problem if you're very good integral equations and i certainly going

12
00:00:41,060 --> 00:00:46,740
suffer but anyway it has been solved i mean physicist for a long time have

13
00:00:46,740 --> 00:00:51,260
worried about that particular kind of question because it comes up in physics all the

14
00:00:51,260 --> 00:00:55,860
time if you time limit something and you take the fourier integral

15
00:00:56,030 --> 00:01:00,460
happy new also comes close to frequency limiting as possible

16
00:01:00,480 --> 00:01:07,940
so this particular prolates spheroidin set of orthogonal functions in fact exactly solves the problem

17
00:01:08,250 --> 00:01:12,680
of how do you how do you generate a set of orthonormal functions which in

18
00:01:12,680 --> 00:01:19,190
fact have as much energy as possible both frequency limited and time time-limited

19
00:01:19,240 --> 00:01:21,870
and what you find when you do that

20
00:01:22,110 --> 00:01:26,390
is when you take two w two you have some

21
00:01:26,410 --> 00:01:28,060
at that point

22
00:01:28,080 --> 00:01:34,390
the energy in these two orthonormal function support of it inside the band really starts

23
00:01:34,390 --> 00:01:38,860
to cut off very very shortly and t are large

24
00:01:38,860 --> 00:01:43,820
so that in fact you get two wt these functions which have almost all of

25
00:01:43,820 --> 00:01:46,240
their energy in this band

26
00:01:46,250 --> 00:01:50,000
and everything else has almost no energy in the band

27
00:01:50,010 --> 00:01:52,760
so if you ever get interested in the question

28
00:01:52,820 --> 00:01:59,690
how many waveforms really are there which are concentrated in time and frequency

29
00:01:59,700 --> 00:02:02,010
that's the answer to the problem

30
00:02:02,020 --> 00:02:06,230
and no bother to read it now but just remember that if you ever get

31
00:02:06,230 --> 00:02:10,610
interested in that problem which i'm sure you will at some point or other that's

32
00:02:10,610 --> 00:02:12,340
where the solution lies

33
00:02:12,350 --> 00:02:15,870
i hope i gave a reference to what there i think i did

34
00:02:19,600 --> 00:02:22,870
anyway so we have these functions which span of two

35
00:02:22,940 --> 00:02:27,940
this least one other orthonormal expansion that we have

36
00:02:28,680 --> 00:02:32,040
which is both time and frequency limiting

37
00:02:33,380 --> 00:02:36,860
either at the end of the day at the beginning of the month the

38
00:02:36,970 --> 00:02:40,250
we're going to find another particularly important

39
00:02:40,260 --> 00:02:44,040
a sequence of orthonormal functions

40
00:02:44,060 --> 00:02:48,980
which we actually use we're transmitting data

41
00:02:49,040 --> 00:02:50,370
OK so

42
00:02:50,380 --> 00:02:55,200
so to give an example of what we're just talking about fourier series functions span

43
00:02:55,200 --> 00:02:59,590
the space of functions over minus over two to three over two

44
00:02:59,600 --> 00:03:02,690
and normalize dysfunctions become

45
00:03:02,700 --> 00:03:05,440
one over the square root of two a

46
00:03:05,450 --> 00:03:10,450
times what we started with before namely a sinusoidal truncated

47
00:03:10,470 --> 00:03:15,340
OK before we were dealing with the orthogonal functions without the one over square root

48
00:03:16,540 --> 00:03:18,530
if you want to make them more orthonormal

49
00:03:18,540 --> 00:03:22,700
you get this square root of one over to because when you take the the

50
00:03:22,700 --> 00:03:24,640
energy in this function

51
00:03:24,660 --> 00:03:26,450
you get

52
00:03:26,480 --> 00:03:30,380
if you don't believe me set set k equal to zero and look at that

53
00:03:30,570 --> 00:03:35,710
and even i can integrate that when you integrate one from minus the over two

54
00:03:35,740 --> 00:03:37,990
t over two you get one

55
00:03:39,310 --> 00:03:40,360
then you have to

56
00:03:40,380 --> 00:03:45,380
multiplied by the square root of one over time when you look view the fourier

57
00:03:46,730 --> 00:03:49,240
functions in this why

58
00:03:49,260 --> 00:03:51,630
it's nice because they are orthonormal

59
00:03:51,640 --> 00:03:56,910
it's not nice because the square root of one over t periods everyplace but the

60
00:03:56,910 --> 00:04:02,100
nice thing about it is that then when you expand in the fourier series you

61
00:04:02,100 --> 00:04:07,110
don't find this t anywhere namely v is equal to the

62
00:04:07,110 --> 00:04:08,320
to the limit

63
00:04:08,330 --> 00:04:15,180
of these approximations were the approximation is just the sum from minus and and

64
00:04:15,190 --> 00:04:18,160
about this OK k these these k

65
00:04:18,220 --> 00:04:21,500
in office of k is just this enterprise

66
00:04:21,510 --> 00:04:26,070
OK so again you get something nicer by looking at these things in terms of

67
00:04:26,070 --> 00:04:27,360
factors you get

68
00:04:27,410 --> 00:04:32,370
you get my statements about how these things converge and you also get a very

69
00:04:32,370 --> 00:04:33,850
compact way

70
00:04:33,980 --> 00:04:36,780
writing out what expressions are

71
00:04:37,190 --> 00:04:40,360
you also get something a little bit fishy

72
00:04:40,770 --> 00:04:42,500
which a lot of people

73
00:04:42,510 --> 00:04:46,780
in the communication field especially ones who do theoretical work

74
00:04:46,900 --> 00:04:49,490
you run into problems with

75
00:04:49,540 --> 00:04:50,380
and the

76
00:04:50,400 --> 00:04:54,950
and the problem is the following when you when you start feeling all the time

77
00:04:54,950 --> 00:04:56,200
with vectors

78
00:04:56,240 --> 00:05:00,770
and you forget about the underlying functions in the underlying integrals

79
00:05:01,190 --> 00:05:06,600
you start to think that the subject is simpler than it really is

80
00:05:06,640 --> 00:05:11,120
because you forget about all the remaining issues and when you forget about all limiting

81
00:05:11,120 --> 00:05:14,620
issues its final almost all the time

82
00:05:14,630 --> 00:05:17,700
but every once in a while you get trapped

83
00:05:17,770 --> 00:05:23,320
when you get trapped you then have to go back behind all of the vector

84
00:05:23,320 --> 00:05:27,280
stuff and you have to start looking at these general again and it gets rather

85
00:05:27,280 --> 00:05:30,660
frustrated she you want to keep both of these things in mind

86
00:05:34,180 --> 00:05:37,610
let's go on let's get back

87
00:05:37,620 --> 00:05:40,360
from mathematics into

88
00:05:40,530 --> 00:05:44,580
and the worrying about the question of how do you send data

89
00:05:44,590 --> 00:05:48,430
over over communication channels

90
00:05:48,450 --> 00:05:52,510
and this is just the picture that we saw

91
00:05:52,520 --> 00:05:55,410
starting on day one of the scores

92
00:05:55,440 --> 00:05:57,730
which says the usual y

93
00:05:57,740 --> 00:05:59,470
of doing this

94
00:05:59,480 --> 00:06:02,490
is you start out

95
00:06:02,510 --> 00:06:07,360
with this

96
00:06:07,360 --> 00:06:12,620
you start out with the source you break the source they in binary data

97
00:06:12,680 --> 00:06:16,960
and then you take the binary data and you transmitted over a channel and this

98
00:06:16,960 --> 00:06:21,560
is the picture of what you get when you try to transmit binary data

99
00:06:21,600 --> 00:06:25,190
over the channel and here we broken down

100
00:06:25,200 --> 00:06:30,190
the encoder is we call the into two pieces one of which we call the

101
00:06:30,250 --> 00:06:31,940
discrete encoder

102
00:06:31,980 --> 00:06:35,160
in one of which we we call modulation

103
00:06:35,210 --> 00:06:39,010
now this is a little bit fishy also because

104
00:06:39,080 --> 00:06:43,980
there are a lot of people who now talk about coded modulation

105
00:06:44,020 --> 00:06:49,490
we're where it turns out to be nice to combine this discrete encoder with the

106
00:06:49,490 --> 00:06:51,780
modulation function

107
00:06:51,800 --> 00:06:54,700
and when you actually build modern day

108
00:06:56,550 --> 00:07:01,160
full encoders namely the whole thing from binary digits to what goes out on the

109
00:07:02,350 --> 00:07:06,480
you very often combine these two functions together

110
00:07:06,530 --> 00:07:09,630
what is it allows people to do that

111
00:07:09,630 --> 00:07:14,170
the fact that the first study how to do it when they separate the problems

112
00:07:14,170 --> 00:07:16,330
into two separate problems

113
00:07:16,350 --> 00:07:21,600
and when you separate the two problems what we wind up with is binary digits

114
00:07:21,600 --> 00:07:25,340
coming in human societies binary digits

115
00:07:25,350 --> 00:07:28,120
strictly strictly digitally

116
00:07:28,210 --> 00:07:32,270
and what comes out is the sequence of symbols

117
00:07:32,330 --> 00:07:37,050
and usually the sequence of symbols comes out at a slower rate than the binary

118
00:07:37,050 --> 00:07:40,810
digits come in for example if you take two binary digits

119
00:07:40,850 --> 00:07:43,920
and you revert into one symbol

120
00:07:43,920 --> 00:07:51,880
a lot of applications to image analysis which i solely around one

121
00:07:52,420 --> 00:07:58,770
the complexity of the reconstruction if you're talking about simplicial complexes growth terribly with dimension

122
00:07:58,770 --> 00:08:03,750
because of that this idea of using this for data of high dimension is not

123
00:08:04,520 --> 00:08:09,130
interesting if you stick to simplicial complexes but if you start working with more general

124
00:08:09,130 --> 00:08:15,750
objects x CW complexes then this becomes more possible sort of something that it's something

125
00:08:15,750 --> 00:08:22,710
that really would be worth looking into because it would enable going up now this

126
00:08:22,710 --> 00:08:24,690
is what the complexity sort of

127
00:08:25,040 --> 00:08:30,860
the complexity of the domain itself simplicial complexes big but the algorithm itself is not

128
00:08:30,860 --> 00:08:37,170
because it's the recursive algorithm which works like quite well actually it's a good from

129
00:08:37,190 --> 00:08:39,020
the point of view

130
00:08:39,080 --> 00:08:40,480
OK so the

131
00:08:40,480 --> 00:08:43,690
the result of this algorithm is not really a discrete morse function is just the

132
00:08:43,690 --> 00:08:45,940
discrete vector field but we know

133
00:08:45,940 --> 00:08:50,840
well here's the next result that would like to mention

134
00:08:50,900 --> 00:08:57,420
here's how frustrating this transparency can see the next result this is the reason this

135
00:08:57,420 --> 00:09:04,250
is from this year and it's done in collaboration with the dissident gregory set in

136
00:09:04,250 --> 00:09:10,150
some other people and this is the crux torisation of the discrete vector fields on

137
00:09:10,150 --> 00:09:12,810
noncompact domains in this case which come from

138
00:09:12,920 --> 00:09:16,810
the discrete morse functions with a certain property which is called proper i don't want

139
00:09:16,810 --> 00:09:20,400
to go into this but the question now is if we have the discrete vector

140
00:09:20,400 --> 00:09:24,170
field doesn't belong to the discrete morse function and can we come up with the

141
00:09:24,170 --> 00:09:29,650
discrete morse function which belongs to the discrete vector field well we can under certain

142
00:09:29,650 --> 00:09:34,540
conditions and here we have a characterisation was which tells us when on the simpler

143
00:09:34,560 --> 00:09:39,170
situation of compact this was already solved by forming in the very beginning when he

144
00:09:39,170 --> 00:09:42,710
initiated the discrete morse theory so

145
00:09:42,790 --> 00:09:46,110
the next thing that we're working with is a

146
00:09:46,130 --> 00:09:51,840
this is something that i did together with my graduate student gregory said this is

147
00:09:51,860 --> 00:09:55,960
the formalisation of the morse smale complex in this context OK so how do we

148
00:09:55,960 --> 00:10:00,900
describe and what actually would be the morse smale complex of discrete morse function and

149
00:10:00,900 --> 00:10:06,490
also an algorithm for the reconstruction of the discrete this more smale complex this is

150
00:10:06,790 --> 00:10:11,400
a little remind you the decomposition into little patches for the behavior of the function

151
00:10:11,400 --> 00:10:15,880
is the same practically on each patch we have the same behavior of the function

152
00:10:15,900 --> 00:10:20,770
and then discuss a little bit back but this may be the most applied part

153
00:10:20,770 --> 00:10:25,210
of all this this is the extension of others that i was talking about to

154
00:10:25,500 --> 00:10:28,020
the parametric case because so

155
00:10:35,920 --> 00:10:38,310
good move

156
00:10:43,000 --> 00:10:45,130
what do i mean by

157
00:10:45,130 --> 00:10:47,630
the parametric case what

158
00:10:47,630 --> 00:10:49,730
are we actually doing this here

159
00:10:49,750 --> 00:10:54,190
imagine instead of having one

160
00:10:54,230 --> 00:10:59,690
single function defined on the manifold or the simplicial complex

161
00:11:01,380 --> 00:11:03,340
a serious so we have this

162
00:11:03,360 --> 00:11:08,940
simple complex which i'll just try like this just systematically so we have a function

163
00:11:08,940 --> 00:11:12,330
given on this for example at time t is equal to zero and then we

164
00:11:12,330 --> 00:11:16,940
have another function at time t is equal to one in the third function at

165
00:11:16,940 --> 00:11:21,770
time t equal and so on so we have a whole series of functions given

166
00:11:21,770 --> 00:11:24,270
on this domain

167
00:11:24,330 --> 00:11:28,170
and now we're looking at the critical point of view this function and this function

168
00:11:28,210 --> 00:11:32,830
this function with trying to figure out is how these critical points evolved from this

169
00:11:32,830 --> 00:11:38,330
situation this one this one and so on try to follow the critical points along

170
00:11:38,340 --> 00:11:39,940
the time scale

171
00:11:43,710 --> 00:11:52,840
and the result is called something which is called the before case india diagram

172
00:11:52,860 --> 00:11:54,130
for the

173
00:11:54,150 --> 00:12:01,060
before question diagram actually tells us how these critical cells are connected a long time

174
00:12:01,080 --> 00:12:03,270
OK so let me show you

175
00:12:03,290 --> 00:12:09,060
a simple example on a one-dimensional manifold

176
00:12:09,560 --> 00:12:14,610
so here we have

177
00:12:14,610 --> 00:12:20,600
here we have a function of one variable x which depends on one additional parameter

178
00:12:21,630 --> 00:12:26,940
OK so this is just a simple quadratic polynomials so in the beginning when the

179
00:12:26,940 --> 00:12:30,580
beginning in our our cases t is equal to two

180
00:12:30,630 --> 00:12:36,480
we have one critical point of the functions down here but now is the this

181
00:12:42,000 --> 00:12:48,360
simple the function changes and also the number of the critical points changes

182
00:12:48,420 --> 00:12:51,230
for example if you look at the situation

183
00:12:51,250 --> 00:12:56,830
he is equal to two two well of critical point appeared over here

184
00:12:56,840 --> 00:12:57,770
so far

185
00:12:57,770 --> 00:13:03,020
at the TV quick before a critical point appeared over here it two and then

186
00:13:03,020 --> 00:13:07,460
if we go on this one critical point splits into two different critical points so

187
00:13:07,480 --> 00:13:11,770
this one critical point here at time for

188
00:13:11,770 --> 00:13:18,950
evolved into the maximum this minimum style four point five and we go on and

189
00:13:18,950 --> 00:13:23,040
at the time five well the maximum which was over here

190
00:13:23,060 --> 00:13:27,230
more just with the minimum over here so from here on again we just have

191
00:13:27,230 --> 00:13:32,560
one critical points in the the saint in between so here is the frication diagram

192
00:13:32,580 --> 00:13:36,830
which belongs to the situation four times which are smaller than four we just have

193
00:13:36,830 --> 00:13:42,340
one critical point and then share it is equal to four another one appears critical

194
00:13:42,340 --> 00:13:44,650
because you have the data

195
00:13:44,670 --> 00:13:50,130
and the putting on this can be computed because it's just the polynomial something and

196
00:13:50,420 --> 00:13:52,510
so you can

197
00:13:52,530 --> 00:13:55,050
just compute this approximation

198
00:13:55,050 --> 00:13:57,820
and then you can start it back into

199
00:13:57,840 --> 00:14:02,490
the law equation the kullback leibler divergence

200
00:14:02,550 --> 00:14:03,880
and then you do

201
00:14:03,900 --> 00:14:07,590
some calculations and then you get something like that

202
00:14:07,650 --> 00:14:09,440
above the

203
00:14:09,440 --> 00:14:10,440
so it's

204
00:14:10,450 --> 00:14:14,970
no big deal so you

205
00:14:15,610 --> 00:14:17,650
just as a reminder

206
00:14:17,670 --> 00:14:21,840
we're trying to optimize this function with respect to w

207
00:14:24,610 --> 00:14:27,070
in other words we can have some

208
00:14:27,090 --> 00:14:33,920
we would like to slide down the gradient here

209
00:14:33,940 --> 00:14:36,050
so in order to

210
00:14:40,590 --> 00:14:44,610
we can write down the differential equation

211
00:14:44,740 --> 00:14:47,780
this learning so this is like a learning rule

212
00:14:48,610 --> 00:14:50,570
continuous time

213
00:14:51,110 --> 00:14:54,360
you know for the moment it's not important

214
00:14:54,380 --> 00:14:58,150
so this is the estimate of all matrix w

215
00:14:58,150 --> 00:15:01,340
as a function of time

216
00:15:01,360 --> 00:15:04,280
so maybe you just slowly go

217
00:15:04,300 --> 00:15:07,150
so this is is the step size

218
00:15:07,170 --> 00:15:10,740
of this differential equation

219
00:15:10,760 --> 00:15:15,740
this unit matrix so if you should ignore for the moment

220
00:15:15,840 --> 00:15:18,440
w also known for the moment

221
00:15:18,440 --> 00:15:22,510
so then you have one minus u transposed

222
00:15:23,420 --> 00:15:28,340
so this equation stops has zero gradient

223
00:15:28,440 --> 00:15:31,970
so DTW to he zero

224
00:15:31,990 --> 00:15:34,400
so you have the solution when

225
00:15:34,420 --> 00:15:35,710
you have

226
00:15:35,720 --> 00:15:40,840
diagonalizing this covariance matrix you transpose

227
00:15:40,860 --> 00:15:43,360
OK this would be something like PCA

228
00:15:43,380 --> 00:15:45,990
an online version PC

229
00:15:47,820 --> 00:15:53,630
we are trying to enforce statistically independent means that we're not happy with correlation along

230
00:15:54,670 --> 00:15:56,940
PCA would give us

231
00:15:57,740 --> 00:16:00,740
we need some high order moments as well

232
00:16:00,760 --> 00:16:03,610
this is where the company

233
00:16:03,630 --> 00:16:05,470
if it does

234
00:16:07,470 --> 00:16:09,030
say are polynomials

235
00:16:09,050 --> 00:16:10,670
of course you

236
00:16:10,690 --> 00:16:18,900
which means that the actually diagonalizing some so to say generalized covariance matrix here

237
00:16:23,440 --> 00:16:24,420
the function

238
00:16:26,550 --> 00:16:30,330
so to say which expansion i've taken when i have done my cup of and

239
00:16:30,330 --> 00:16:31,970
things like that

240
00:16:32,590 --> 00:16:33,860
this is for example

241
00:16:33,900 --> 00:16:39,050
the polynomial that summary gets in his his learning rule and the w just makes

242
00:16:39,050 --> 00:16:42,380
your life easier is called natural gradient

243
00:16:42,400 --> 00:16:45,070
but just a technicality

244
00:16:48,220 --> 00:16:50,880
as the wrap-up

245
00:16:50,900 --> 00:16:57,710
to minimize this kullback leibler divergence so in other words to to be able to

246
00:16:57,720 --> 00:17:02,240
in for statistical independence the only thing you have to do

247
00:17:02,240 --> 00:17:07,210
is it is so to say diagonalized generalize the covariance matrix

248
00:17:07,220 --> 00:17:08,760
and they also

249
00:17:08,780 --> 00:17:12,280
you know there's a whole zoo of algorithms around

250
00:17:12,300 --> 00:17:16,360
and of course

251
00:17:16,380 --> 00:17:20,220
this is somewhat essential

252
00:17:23,280 --> 00:17:27,280
there there are of course some as that

253
00:17:27,300 --> 00:17:31,050
more appropriate for distributions that you after

254
00:17:32,840 --> 00:17:36,030
and some of the algorithms that are around

255
00:17:36,030 --> 00:17:41,010
you different as that reflect certain assumptions of what we are actually looking for whether

256
00:17:41,010 --> 00:17:49,260
we're looking for something super colson as subgoals in order

257
00:17:49,360 --> 00:17:55,400
so there's a different way of thinking

258
00:17:55,440 --> 00:18:02,130
and of doing blind source separation the other all the other direction is

259
00:18:02,150 --> 00:18:05,630
to take time into account

260
00:18:06,780 --> 00:18:08,610
until this point

261
00:18:08,690 --> 00:18:14,920
in all these equations are shown i didn't i just used

262
00:18:15,780 --> 00:18:20,710
ten some i didn't use any temporal structure

263
00:18:20,720 --> 00:18:22,030
of the signals

264
00:18:22,050 --> 00:18:26,260
just some statistical correlations

265
00:18:26,840 --> 00:18:32,110
most signals that we are like speech signals of brain signals they have the temporal

266
00:18:32,990 --> 00:18:36,510
so we might as well use that we might as well use the higher order

267
00:18:36,510 --> 00:18:40,720
structure of the signals and the temporal structure but let's go first to the temporary

268
00:18:42,840 --> 00:18:45,090
OK so

269
00:18:45,110 --> 00:18:49,090
the idea the assumption essentially is that

270
00:18:49,110 --> 00:18:55,090
we want to have the correlation between say the street noise in the speaker to

271
00:18:55,090 --> 00:18:59,810
minimize the length of the weight vector subject to these constraints and that's one constraint

272
00:18:59,810 --> 00:19:02,660
for each data point of the constraint

273
00:19:02,700 --> 00:19:07,420
a data point is on the right side of the separation

274
00:19:07,510 --> 00:19:12,500
what do we do next is introduced like waterfront

275
00:19:13,090 --> 00:19:15,100
maybe some of you are familiar with this

276
00:19:15,210 --> 00:19:19,940
so this thing here is called the constrained optimisation problem

277
00:19:20,050 --> 00:19:24,360
and it's an optimisation problem with inequality constraints

278
00:19:24,390 --> 00:19:28,500
maybe some of you have seen optimisation with equality constraints

279
00:19:28,580 --> 00:19:34,340
and this is a classic formalism development and launch how to deal with this by

280
00:19:34,340 --> 00:19:36,400
introducing multipliers

281
00:19:36,410 --> 00:19:41,410
here we have inequality constraints which is a little bit more difficult but it's very

282
00:19:41,410 --> 00:19:48,430
similar formalism is basically it was extended by college school talk and others and what's

283
00:19:48,430 --> 00:19:51,290
important for us that this is a nice

284
00:19:51,590 --> 00:19:56,210
objective function so this is just a quadratic function in particular convex

285
00:19:56,220 --> 00:19:59,790
and these are just linear constraints

286
00:20:00,150 --> 00:20:06,790
diseasex i constant so the there that we optimize over w and b

287
00:20:06,800 --> 00:20:07,560
so that's

288
00:20:07,570 --> 00:20:08,690
that's nice

289
00:20:08,870 --> 00:20:13,220
but we have to go through a little bit of formalism

290
00:20:13,230 --> 00:20:15,110
and and the way it works

291
00:20:15,140 --> 00:20:20,540
is we introduce these locals multipliers one multiplier for each constraint

292
00:20:20,560 --> 00:20:26,060
and then we write down what's called language in like watching has as the first

293
00:20:26,060 --> 00:20:29,140
just the original objective function this thing here

294
00:20:29,850 --> 00:20:31,390
and then it has

295
00:20:31,430 --> 00:20:35,800
the sum over all the constraints with the constraints

296
00:20:35,810 --> 00:20:37,650
it's likely reformulated

297
00:20:37,660 --> 00:20:42,250
i'm going to move this one over the other side of right this constraint is

298
00:20:43,410 --> 00:20:45,410
larger or equal to zero

299
00:20:45,510 --> 00:20:51,000
OK so maybe try to put this into your iconic memory for a second

300
00:20:51,010 --> 00:20:52,240
and then i'll

301
00:20:52,410 --> 00:20:54,840
put this could so this is not the constraint

302
00:20:54,850 --> 00:21:00,500
this is greater than zero and multiply it with the corresponding levels must find some

303
00:21:00,610 --> 00:21:01,720
all of them

304
00:21:01,760 --> 00:21:04,540
and the results are subtracted from the

305
00:21:04,850 --> 00:21:07,090
original objective function

306
00:21:07,130 --> 00:21:11,040
so this is the definition of the audience

307
00:21:11,050 --> 00:21:14,290
so you might be puzzled as to why

308
00:21:14,330 --> 00:21:18,070
this thing here helps us dealing with our problems

309
00:21:18,240 --> 00:21:23,700
so what i'm going to claim is that this logo as has to be minimized

310
00:21:23,700 --> 00:21:27,220
with respect to the so called primal variables w and b

311
00:21:27,620 --> 00:21:31,410
remember here we want to minimize this thing

312
00:21:31,440 --> 00:21:37,390
and all variables of the variables in determining the position of the hyperplane with WB

313
00:21:37,680 --> 00:21:42,010
and we get this is a hyperplane is determined by w and b

314
00:21:42,080 --> 00:21:47,160
and we want to some hard WP such that all the positive points

315
00:21:47,170 --> 00:21:50,620
and the value of this function at least plus one of the negative ones

316
00:21:50,630 --> 00:21:53,060
the most minus one

317
00:21:53,570 --> 00:21:55,430
so these are all variables

318
00:21:56,350 --> 00:21:58,390
we went to

319
00:21:59,470 --> 00:22:03,140
with respect to this primal variables that's not so surprising is the same as the

320
00:22:03,140 --> 00:22:08,140
last slide but in addition i'm saying we have to maximize with respect to the

321
00:22:08,140 --> 00:22:09,630
dual variables

322
00:22:09,630 --> 00:22:12,610
so in other words we have to find what's called the saddle point of this

323
00:22:13,710 --> 00:22:16,930
and it turns out if we do both of these things together

324
00:22:16,960 --> 00:22:19,920
if the problem is solvable we find a solution which

325
00:22:19,930 --> 00:22:25,360
minimizes with respect to the primal variables subject to the constraints that we had subject

326
00:22:25,950 --> 00:22:27,910
to these constraints

327
00:22:27,960 --> 00:22:29,410
so if you know it

328
00:22:29,620 --> 00:22:32,950
it may be boring for you isn't it sounds a little bit like magic

329
00:22:34,550 --> 00:22:37,360
i can't go through all the details that i just want to give you a

330
00:22:37,360 --> 00:22:39,680
a little bit of intrusion why this thing

331
00:22:39,690 --> 00:22:41,000
that's the right thing

332
00:22:41,030 --> 00:22:45,360
so just so that you believe me reasonable thing if you are interested in you

333
00:22:46,110 --> 00:22:49,940
look it up in the book on linear optimization

334
00:22:49,960 --> 00:22:53,040
these are very very useful tools and if you

335
00:22:53,130 --> 00:22:58,690
decide to stay in machine learning i'm actually you will you will use the

336
00:22:58,880 --> 00:23:05,470
OK so let's assume we have a constraint which is violated so what i mean

337
00:23:05,470 --> 00:23:07,390
by this

338
00:23:07,720 --> 00:23:15,620
so constraints these where the constraints of this thing here minus one is nonnegative

339
00:23:15,620 --> 00:23:19,750
now let's assume it's negative so this thing here minus one is negative so that

340
00:23:20,550 --> 00:23:26,240
there's the point that doesn't lie in the area where it's supposed to lie

341
00:23:26,250 --> 00:23:29,100
so this means this quantity here's negative

342
00:23:29,100 --> 00:23:35,500
now remember the function is maximized with respect to the dual variables

343
00:23:35,600 --> 00:23:41,180
OK so this is negative we have another minus over is positive this means the

344
00:23:41,180 --> 00:23:44,720
corresponding phi can grow can start growing

345
00:23:44,990 --> 00:23:48,990
in order to increase the value of the loss function

346
00:23:49,050 --> 00:23:53,360
so i think i have inserted these language multipliers have to be

347
00:23:55,100 --> 00:24:00,610
so this this like multiply allowed to change and it's actually trying to change such

348
00:24:00,610 --> 00:24:03,380
that the value of l increases

349
00:24:03,390 --> 00:24:08,380
if this thing here is negative then the value can be increased by the corresponding

350
00:24:08,480 --> 00:24:10,870
goes my growing

351
00:24:10,880 --> 00:24:14,110
so will grow start growing but at the same time

352
00:24:15,240 --> 00:24:20,120
the primal variables one to minimize the language functions so they should start working against

353
00:24:20,120 --> 00:24:21,140
the phi

354
00:24:21,180 --> 00:24:26,130
and the only way of working against this five that w and they have to

355
00:24:26,130 --> 00:24:27,730
change such that this

356
00:24:27,750 --> 00:24:29,240
this is no longer true

357
00:24:29,260 --> 00:24:30,220
they have two

358
00:24:30,230 --> 00:24:33,280
prevent this constrained from being

359
00:24:33,420 --> 00:24:41,630
not satisfied because otherwise the corresponding alpha will grow beyond bounds and held which increases

360
00:24:41,850 --> 00:24:43,210
beyond bonds

361
00:24:43,510 --> 00:24:45,700
so w and b have two

362
00:24:45,710 --> 00:24:50,450
decrease they have to change such that the constraint get satisfied

363
00:24:50,460 --> 00:24:54,920
which means that the position of the happening to change such that the point

364
00:24:55,070 --> 00:25:01,030
o point x i is on the right side of the hyperplane

365
00:25:01,060 --> 00:25:07,490
so this is going to happen and overall if the problem is separable so if

366
00:25:07,510 --> 00:25:09,170
there a geometric solution w b

367
00:25:09,860 --> 00:25:15,460
and this will ensure that this corresponding language multiple i i will not go to

368
00:25:15,460 --> 00:25:18,610
infinity so there will be some finite value where it stops

369
00:25:18,610 --> 00:25:20,310
then we can have a stable

370
00:25:20,360 --> 00:25:23,880
situation we can have that equilibrium

371
00:25:23,890 --> 00:25:26,800
so this one is not moving and this one is not moving

372
00:25:26,820 --> 00:25:29,020
and then we can have t two

373
00:25:29,880 --> 00:25:31,210
so a larger

374
00:25:31,210 --> 00:25:32,120
twenty one

375
00:25:32,140 --> 00:25:35,290
by using friction to our advantage

376
00:25:35,290 --> 00:25:40,230
that's blow up that center portion

377
00:25:40,280 --> 00:25:42,530
radius is

378
00:25:43,640 --> 00:25:45,760
let's have this

379
00:25:45,760 --> 00:25:47,350
wrote here

380
00:25:47,380 --> 00:25:50,260
i'll give the role color

381
00:25:50,260 --> 00:25:53,040
doesn't have to come up vertically of course

382
00:25:53,100 --> 00:25:56,700
if you have any angle

383
00:25:56,700 --> 00:25:59,010
there is so here is my t one

384
00:25:59,100 --> 00:26:02,260
and here is my t two

385
00:26:02,270 --> 00:26:04,880
and we take the situation that

386
00:26:04,900 --> 00:26:08,020
the born this site is way larger than on that site

387
00:26:08,020 --> 00:26:11,250
so that israel wants to in this direction

388
00:26:11,300 --> 00:26:18,720
that's what i would like to do

389
00:26:18,730 --> 00:26:20,580
if you look

390
00:26:21,490 --> 00:26:22,950
small little

391
00:26:22,960 --> 00:26:25,130
sections of the road

392
00:26:25,250 --> 00:26:29,220
it is immediately obvious if the world wants to slide in this direction wants to

393
00:26:29,220 --> 00:26:30,680
start stripping

394
00:26:30,730 --> 00:26:32,690
that the frictional forces

395
00:26:32,740 --> 00:26:39,390
in these little pieces here all in this direction

396
00:26:39,450 --> 00:26:41,560
all the way around

397
00:26:41,680 --> 00:26:43,870
and therefore

398
00:26:43,930 --> 00:26:46,330
it helps to one so to speak

399
00:26:46,480 --> 00:26:49,020
so because of the fraction

400
00:26:49,060 --> 00:26:51,360
which is opposing due to

401
00:26:51,370 --> 00:26:53,260
he two can now become much larger

402
00:26:55,930 --> 00:26:58,640
what you have to do to calculated analytically

403
00:26:58,720 --> 00:27:00,150
we have to evaluate

404
00:27:02,610 --> 00:27:06,990
frictional forces for these very small slices of becomes an integral

405
00:27:07,030 --> 00:27:08,350
and then you have to

406
00:27:09,400 --> 00:27:11,270
over an angle which i will call

407
00:27:11,280 --> 00:27:13,430
OK zero

408
00:27:13,470 --> 00:27:15,980
and i remember when i last lecture

409
00:27:16,030 --> 00:27:18,560
in one that was in nineteen ninety three

410
00:27:18,650 --> 00:27:21,170
i the right that in class

411
00:27:21,230 --> 00:27:25,940
the relation between t two and t one has functional this angle theta

412
00:27:25,990 --> 00:27:29,650
and it took me about five minutes and after five minutes half the students

413
00:27:29,670 --> 00:27:31,330
we're asleep

414
00:27:31,350 --> 00:27:34,290
now i'm not sure whether you want to sleep five minutes but i don't think

415
00:27:34,290 --> 00:27:36,080
frankly you deserve it so

416
00:27:36,090 --> 00:27:38,730
i decided to not do the derivation

417
00:27:38,730 --> 00:27:40,920
but to refer you to the book

418
00:27:40,970 --> 00:27:43,750
which is page three hundred sixty one

419
00:27:43,800 --> 00:27:45,200
and you will find

420
00:27:45,350 --> 00:27:47,290
in this situation that the rope

421
00:27:47,340 --> 00:27:49,800
once to start slipping in this election

422
00:27:49,810 --> 00:27:51,800
that t two

423
00:27:51,850 --> 00:27:53,530
divided by t one

424
00:27:54,240 --> 00:27:55,540
is that our

425
00:27:56,370 --> 00:27:59,580
i'm data zero if the friction coefficient

426
00:27:59,590 --> 00:28:01,470
here is new be the static

427
00:28:01,470 --> 00:28:03,220
friction coefficient

428
00:28:03,230 --> 00:28:06,920
so that's the results and noted that it is independent of

429
00:28:07,020 --> 00:28:11,720
the radius of this which is not so obvious funny things

430
00:28:11,740 --> 00:28:12,880
but the bar is

431
00:28:13,480 --> 00:28:14,370
this small

432
00:28:14,390 --> 00:28:18,290
with this small makes no difference if only depends on this angle this angle could

433
00:28:20,050 --> 00:28:23,450
very large you could wrap it around ten times

434
00:28:23,450 --> 00:28:25,000
as we will do very shortly

435
00:28:25,010 --> 00:28:27,470
so it is no restriction on theta

436
00:28:27,490 --> 00:28:31,090
if there were no friction at all nodes then

437
00:28:31,150 --> 00:28:33,290
at the moment that it starts to say

438
00:28:33,290 --> 00:28:35,260
e to the power zero is one

439
00:28:35,290 --> 00:28:39,540
that's twenty two equals the one that's obvious so you can play this game

440
00:28:39,720 --> 00:28:40,890
new is zero

441
00:28:40,930 --> 00:28:43,240
you need friction that is at the heart

442
00:28:43,290 --> 00:28:44,390
of this whole

443
00:28:45,990 --> 00:28:48,260
so now let's put in some numbers

444
00:28:48,310 --> 00:28:52,950
so we get idear of what we did

445
00:28:52,960 --> 00:28:57,310
so let's take a situation that we

446
00:28:57,990 --> 00:28:59,870
that's a three terms

447
00:28:59,890 --> 00:29:02,970
we write wrap the rope around three times

448
00:29:02,970 --> 00:29:05,600
so three terms

449
00:29:05,690 --> 00:29:08,030
that means that zero

450
00:29:08,070 --> 00:29:10,260
we call six y

451
00:29:10,300 --> 00:29:12,820
and that the friction coefficient u

452
00:29:12,830 --> 00:29:14,370
the one thing

453
00:29:14,420 --> 00:29:16,720
o point two

454
00:29:16,740 --> 00:29:18,360
e two to power

455
00:29:19,390 --> 00:29:21,380
times data zero

456
00:29:21,420 --> 00:29:22,540
is now

457
00:29:22,550 --> 00:29:24,600
about forty

458
00:29:24,610 --> 00:29:26,420
so what that means is

459
00:29:27,250 --> 00:29:29,180
with the force on this site

460
00:29:29,230 --> 00:29:31,500
which is forty times smaller

461
00:29:31,510 --> 00:29:33,400
then the force on that site

462
00:29:33,420 --> 00:29:35,650
i have a balance situation

463
00:29:35,710 --> 00:29:38,090
i can hold it in my hand

464
00:29:38,150 --> 00:29:40,720
and contrary if you want to think of it that way

465
00:29:40,720 --> 00:29:42,200
the force on this site

466
00:29:42,210 --> 00:29:43,200
which is forty

467
00:29:43,210 --> 00:29:44,890
times larger

468
00:29:44,920 --> 00:29:48,070
but if i take six terence

469
00:29:48,130 --> 00:29:50,580
then he to all you

470
00:29:50,650 --> 00:29:51,980
data zero

471
00:29:52,020 --> 00:29:53,960
will be about

472
00:29:54,040 --> 00:29:56,600
two thousand

473
00:29:56,610 --> 00:29:57,960
two thousand

474
00:29:57,970 --> 00:30:00,320
so now i can really control

475
00:30:00,420 --> 00:30:02,080
and elephant

476
00:30:02,130 --> 00:30:03,770
imagine now that

477
00:30:03,810 --> 00:30:04,820
i have here

478
00:30:04,840 --> 00:30:05,890
an object

479
00:30:07,790 --> 00:30:09,470
which would be

480
00:30:09,560 --> 00:30:11,970
five thousand kilograms

481
00:30:12,200 --> 00:30:16,090
ten thousand kilograms

482
00:30:16,110 --> 00:30:18,450
that's why i'm hanging here

483
00:30:18,450 --> 00:30:19,810
i can hang you know

484
00:30:19,870 --> 00:30:21,310
mass which is

485
00:30:21,360 --> 00:30:24,310
two thousand times less massive

486
00:30:24,320 --> 00:30:27,000
that means i could hang there five

487
00:30:31,380 --> 00:30:32,780
attention here

488
00:30:32,880 --> 00:30:36,730
the two thousand times less than the tension there the system will be about to

489
00:30:36,730 --> 00:30:38,610
start slipping

490
00:30:39,470 --> 00:30:45,330
it's not for being there is complete balance

491
00:30:45,390 --> 00:30:50,030
so imagine i hold this point in my hand here is ten thousand kilogram weight

492
00:30:50,040 --> 00:30:52,970
and all this in my hand all i need is the force of about

493
00:30:52,980 --> 00:30:57,070
fifty note and i'm standing there and on the other side is ten thousand kilogram

494
00:30:58,460 --> 00:30:59,650
and now

495
00:30:59,710 --> 00:31:00,780
i just

496
00:31:00,790 --> 00:31:04,000
make my for a little less than fifty students

497
00:31:04,030 --> 00:31:06,830
and what will happen now it will start to see

498
00:31:06,830 --> 00:31:09,900
how do you want to make that they can do lot

499
00:31:10,350 --> 00:31:12,630
how do you get end users doing

500
00:31:12,660 --> 00:31:17,850
so we get told us and so called for

501
00:31:17,870 --> 00:31:22,920
so if you know if in your country you know which one word which one

502
00:31:23,170 --> 00:31:28,330
you start with you get by six starts manage to set things

503
00:31:28,360 --> 00:31:31,850
then you talk about the universe which is broken

504
00:31:31,880 --> 00:31:34,590
version you know what

505
00:31:34,610 --> 00:31:38,440
and it will generate uery tree you build history

506
00:31:38,450 --> 00:31:40,020
you do deleted

507
00:31:40,060 --> 00:31:42,590
you know what does

508
00:31:42,620 --> 00:31:47,260
you then go back into the directory possibly having repeated again in tree doesn't

509
00:31:47,300 --> 00:31:49,330
i need to get by saying that

510
00:31:49,350 --> 00:31:50,760
give us a good

511
00:31:50,770 --> 00:31:52,580
and it produces another tree

512
00:31:52,600 --> 00:31:56,130
and you illustrate we need all the CPU

513
00:31:56,140 --> 00:31:58,070
and you history

514
00:31:58,080 --> 00:32:00,620
and you bad good

515
00:32:00,860 --> 00:32:03,250
fighting you've got something which is

516
00:32:03,260 --> 00:32:07,650
reliably reproducible which is not always the case

517
00:32:07,680 --> 00:32:12,230
you will eventually end up where we have to figure out exactly which had begun

518
00:32:12,250 --> 00:32:14,880
which changes within the game tree

519
00:32:15,040 --> 00:32:17,680
the one which broke your system

520
00:32:17,740 --> 00:32:19,680
and even if you're not to develop

521
00:32:19,730 --> 00:32:22,010
being able to cite to develop a

522
00:32:22,030 --> 00:32:23,910
this is what broke

523
00:32:23,970 --> 00:32:26,790
it's really really useful

524
00:32:26,810 --> 00:32:30,850
it doesn't rely on things being reproduced that problem

525
00:32:30,880 --> 00:32:34,380
if you've got which happens one time in one hundred

526
00:32:34,390 --> 00:32:38,540
doing a binary search doesn't work is you really you need to run each one

527
00:32:38,540 --> 00:32:41,260
several hundred times show

528
00:32:41,270 --> 00:32:44,040
well if only shows the about we

529
00:32:44,050 --> 00:32:46,540
then it it by six to eight weeks

530
00:32:46,550 --> 00:32:48,000
so thank you

531
00:32:48,010 --> 00:32:49,430
probably three

532
00:32:49,450 --> 00:32:51,280
three and a half months

533
00:32:52,000 --> 00:32:53,850
you will find it

534
00:32:56,120 --> 00:32:58,230
so we tried to use tools to

535
00:32:58,230 --> 00:33:03,150
i mean this this sort of press easier

536
00:33:03,450 --> 00:33:05,110
we have lot developers

537
00:33:05,170 --> 00:33:06,580
what is right

538
00:33:08,980 --> 00:33:12,040
they all want to write new drivers

539
00:33:12,050 --> 00:33:14,470
this is not so good

540
00:33:14,480 --> 00:33:17,920
fixing other drivers

541
00:33:17,940 --> 00:33:19,630
less interesting apparently

542
00:33:19,680 --> 00:33:22,840
it's cool to write your own right

543
00:33:22,840 --> 00:33:25,890
i think maybe reading too much random

544
00:33:26,090 --> 00:33:29,470
so we have

545
00:33:29,520 --> 00:33:31,370
problems with

546
00:33:31,720 --> 00:33:34,370
it's a big of the project

547
00:33:35,100 --> 00:33:38,310
even the council science council because i

548
00:33:38,310 --> 00:33:41,000
i want to what's level

549
00:33:41,020 --> 00:33:46,200
it's a very very big challenge where do you start several millions of lines code

550
00:33:46,280 --> 00:33:52,340
which has been documented by programmers in the custom manner documents programming

551
00:33:52,350 --> 00:33:53,080
it is

552
00:33:53,080 --> 00:33:54,290
not at all

553
00:33:54,300 --> 00:33:56,990
the comments can

554
00:33:57,090 --> 00:34:01,690
that's why i don't think there are books

555
00:34:01,810 --> 00:34:05,290
i think that it is but learning device drivers is now one

556
00:34:07,070 --> 00:34:08,490
which is a very good

557
00:34:08,560 --> 00:34:13,100
there are some others you want to get involved

558
00:34:13,130 --> 00:34:18,090
there's also a kernel newbies mailing list which is a mix of mostly people who

559
00:34:18,090 --> 00:34:23,650
are writing their first drive because they decided they want to write driver

560
00:34:23,660 --> 00:34:25,160
and a small number of

561
00:34:25,300 --> 00:34:29,330
very scenic kernel developers try help

562
00:34:29,340 --> 00:34:32,840
so gives you an environment that is fusion environment

563
00:34:33,390 --> 00:34:35,480
if you ask questions like

564
00:34:35,480 --> 00:34:37,000
i hello my module

565
00:34:37,110 --> 00:34:40,780
i can't seem to print anything

566
00:34:40,790 --> 00:34:44,990
i love and we open the file in module you you don't get met by

567
00:34:45,000 --> 00:34:46,310
the ricci

568
00:34:46,330 --> 00:34:48,350
one he tried to do

569
00:34:48,370 --> 00:34:52,420
that you're dealing with people who are trying to solve some problems on the same

570
00:34:54,120 --> 00:34:55,080
and so

571
00:34:55,080 --> 00:34:59,420
the question is again as from the new pillars point why people are neutral

572
00:34:59,430 --> 00:35:02,410
our environment we would expect so

573
00:35:02,470 --> 00:35:05,990
where a lot of the people on the list of similar experience

574
00:35:06,020 --> 00:35:10,450
so the fact better able to to explain things to each other

575
00:35:10,470 --> 00:35:15,150
and there is an expert on when people ask hard questions

576
00:35:15,180 --> 00:35:18,830
which put experts mysteriously the way for each

577
00:35:18,830 --> 00:35:20,670
but there

578
00:35:20,700 --> 00:35:23,810
the second thing is this

579
00:35:23,830 --> 00:35:28,550
think of the kernel janitors and now the mallows started this

580
00:35:28,560 --> 00:35:30,520
when he was trying to be

581
00:35:30,640 --> 00:35:34,700
a serious learning development and chief technical officer

582
00:35:34,760 --> 00:35:37,830
of a brazilian

583
00:35:37,870 --> 00:35:42,510
many fans this tend to be difficult combination to manage

584
00:35:42,520 --> 00:35:43,850
because you want

585
00:35:43,870 --> 00:35:47,920
when is the right kind of variance of the right k

586
00:35:47,940 --> 00:35:52,370
sometimes he was lost in management long period

587
00:35:52,390 --> 00:35:55,320
so here had is idea should be in this where

588
00:35:55,360 --> 00:35:57,850
thing post things in ontologies

589
00:35:57,860 --> 00:36:02,620
so some good player it would be really good if we cleaned up something or

590
00:36:02,630 --> 00:36:06,000
there are still like drivers using this old and lives

591
00:36:06,010 --> 00:36:09,740
and that list there are people on it just pick up these jobs

592
00:36:09,760 --> 00:36:11,120
you may be

593
00:36:11,150 --> 00:36:17,020
don't get much kind time of people just new to it once stop

594
00:36:17,060 --> 00:36:19,660
i think it's going to

595
00:36:19,690 --> 00:36:21,580
it needs doing eventually

596
00:36:21,590 --> 00:36:24,860
if i don't get it right it doesn't matter

597
00:36:25,500 --> 00:36:30,840
if i get it wrong and have to go around by three months doesn't matter

598
00:36:30,840 --> 00:36:34,150
it's not urgent not something critical

599
00:36:34,240 --> 00:36:38,220
that's another good were getting involved and it also help get people involved in actual

600
00:36:38,240 --> 00:36:40,830
use things like fixing the existing

601
00:36:40,910 --> 00:36:44,780
rather than trying to become a writer new drivers

602
00:36:45,200 --> 00:36:46,550
the final thing

603
00:36:46,550 --> 00:36:53,530
prec from one of had the idea that is always drunk drivers and almost

604
00:36:53,540 --> 00:37:00,600
and wouldn't it be good if we actually help vendors drives really drives into the

605
00:37:00,630 --> 00:37:06,130
greg very rapidly got more developers to drivers

606
00:37:06,160 --> 00:37:12,090
so large team people looking specifications so they can be cool and try to drive

607
00:37:12,110 --> 00:37:15,230
it would be nice if they so i decided to it would be cool makes

608
00:37:15,230 --> 00:37:16,860
it right

609
00:37:17,180 --> 00:37:19,340
in this work

610
00:37:19,340 --> 00:37:20,320
the other thing

611
00:37:20,340 --> 00:37:21,680
we see here as well

612
00:37:21,690 --> 00:37:26,830
is a sort of resource misallocation it's very hard to fix

613
00:37:26,830 --> 00:37:30,270
and it had in the preceding decade

614
00:37:30,290 --> 00:37:36,970
how did it spread it spread among individuals who had exempted themselves from

615
00:37:37,380 --> 00:37:39,750
the immunization or its

616
00:37:39,760 --> 00:37:42,100
so we have the first harbinger

617
00:37:42,190 --> 00:37:45,150
of a threat to the public health

618
00:37:45,220 --> 00:37:50,590
from exemptors

619
00:37:51,310 --> 00:37:55,850
when you look at school immunisation laws and you ask

620
00:37:55,900 --> 00:37:59,210
folks do they agree with me or not

621
00:37:59,220 --> 00:38:02,300
what you will find is in fact

622
00:38:03,750 --> 00:38:05,060
most people

623
00:38:05,070 --> 00:38:06,060
i agree

624
00:38:06,090 --> 00:38:07,750
with school

625
00:38:07,760 --> 00:38:10,310
entry innovation loss

626
00:38:10,320 --> 00:38:15,360
and in server we did it stated as the others so disagree

627
00:38:15,370 --> 00:38:18,790
here means you support school laws

628
00:38:19,870 --> 00:38:24,370
back then in two thousand eight percent essentially parents

629
00:38:24,390 --> 00:38:26,250
children should be immunized

630
00:38:26,330 --> 00:38:27,760
to attend school

631
00:38:27,770 --> 00:38:33,810
and today one of those numbers may have changed someone there's still is general support

632
00:38:33,810 --> 00:38:37,840
for school entry immunization

633
00:38:38,020 --> 00:38:46,100
schools have work because parents rely on physician recommendations about whether or not to immunize

634
00:38:46,100 --> 00:38:48,420
children and by and large

635
00:38:48,440 --> 00:38:51,930
physicians have been supportive of

636
00:38:51,940 --> 00:38:54,950
immunisation and school entry loss

637
00:38:54,970 --> 00:38:56,600
what's changing

638
00:38:56,610 --> 00:38:58,850
is that not all parents

639
00:39:01,750 --> 00:39:03,370
advice today

640
00:39:03,390 --> 00:39:06,360
and there's more heterogeneous

641
00:39:06,550 --> 00:39:10,160
particularly in some areas

642
00:39:10,290 --> 00:39:13,530
on physician's advice

643
00:39:13,910 --> 00:39:18,460
so now i would like to now go through what i see as our immediate

644
00:39:20,140 --> 00:39:22,120
and give you my

645
00:39:24,280 --> 00:39:30,440
of what we need to do as a nation to restore the societal consensus

646
00:39:30,460 --> 00:39:33,890
that we need to support immunization

647
00:39:33,940 --> 00:39:37,870
the first is that we need to be prudent about mandates

648
00:39:37,920 --> 00:39:44,650
that we need to have an exemption process that sure is informed decision making

649
00:39:44,790 --> 00:39:48,630
we need to be much more effective about communication

650
00:39:48,690 --> 00:39:51,790
about immunizations

651
00:39:51,820 --> 00:39:56,830
that we have to greatly expand their investment in vaccine safety science

652
00:39:56,930 --> 00:40:02,470
and then we have to engage the public in developing policies and the models for

653
00:40:02,830 --> 00:40:06,070
each of these have been

654
00:40:06,100 --> 00:40:10,990
australia has a rather different approach to ensuring high immunization

655
00:40:11,010 --> 00:40:15,630
they do not mandate immunization in australia

656
00:40:15,680 --> 00:40:20,770
and you can see that they have immunization rates that are roughly comparable

657
00:40:20,770 --> 00:40:22,320
two are

658
00:40:22,380 --> 00:40:26,450
so let's look at children under the age of two

659
00:40:26,500 --> 00:40:30,250
we have mandates about entry into child care

660
00:40:30,260 --> 00:40:31,050
that is

661
00:40:31,060 --> 00:40:33,450
licensed right here

662
00:40:35,580 --> 00:40:38,800
they have a financial rewards

663
00:40:38,820 --> 00:40:41,630
four individuals were immunized

664
00:40:41,660 --> 00:40:46,580
there are two ways to do this one is to a child care subsidy

665
00:40:46,580 --> 00:40:51,950
and if you're entitled to child care subsidy everybody gets some child care subsidies and

666
00:40:51,960 --> 00:40:55,960
you get a greater pay payment if your kids are fully

667
00:40:56,520 --> 00:41:02,180
there's a maternal and infant health allowance of some sort and you get

668
00:41:02,240 --> 00:41:03,850
a bigger check

669
00:41:03,860 --> 00:41:06,860
if your child is fully immunized

670
00:41:06,890 --> 00:41:09,590
at school entry they have no mandate

671
00:41:09,600 --> 00:41:11,220
and by and large those

672
00:41:11,240 --> 00:41:15,810
financial rewards apply only to the first couple of years

673
00:41:15,830 --> 00:41:17,310
of life

674
00:41:17,330 --> 00:41:19,580
interestingly in this country

675
00:41:19,600 --> 00:41:27,930
we've created have used finances to achieve station but we do through penalties

676
00:41:27,950 --> 00:41:32,840
a lower rate of medicaid supplements or

677
00:41:33,430 --> 00:41:34,490
not through

678
00:41:34,570 --> 00:41:36,820
it's so you are

679
00:41:36,890 --> 00:41:38,120
other ways

680
00:41:39,070 --> 00:41:43,280
achieve public health objectives and mandates

681
00:41:44,840 --> 00:41:48,990
my belief and now this has been manifested in our state

682
00:41:50,100 --> 00:41:53,820
you need to

683
00:41:53,870 --> 00:41:56,950
develop a transparent process

684
00:41:56,970 --> 00:42:00,010
four mandating immunization

685
00:42:00,030 --> 00:42:03,970
i'm sorry i don't know if california has dealt with that

686
00:42:03,970 --> 00:42:10,180
but i think mandates have to be applied only to diseases that are indisputable public

687
00:42:10,180 --> 00:42:12,370
health importance

688
00:42:12,370 --> 00:42:13,480
out of data

689
00:42:14,540 --> 00:42:15,140
x one

690
00:42:18,180 --> 00:42:19,000
it can

691
00:42:19,880 --> 00:42:22,460
and notation and i'll try and use consistently

692
00:42:22,970 --> 00:42:26,100
it is in general and can be mounted data we get

693
00:42:26,690 --> 00:42:30,890
little land is always going around in one-tenth whenever you see a suffix you can

694
00:42:30,890 --> 00:42:33,520
usually infer what goes up to without me saying

695
00:42:34,180 --> 00:42:36,460
so any more and i try to use the same

696
00:42:37,860 --> 00:42:38,740
fairly consistently

697
00:42:40,890 --> 00:42:41,100
all right

698
00:42:41,580 --> 00:42:47,320
and this is the gas in distribution so as to be completely explicit plea of x and given mu sigma

699
00:42:48,250 --> 00:42:48,610
it is

700
00:42:49,430 --> 00:42:50,070
on o

701
00:42:50,260 --> 00:42:52,280
where have to figures where

702
00:42:53,550 --> 00:42:55,600
he minus thank you

703
00:42:56,990 --> 00:42:57,830
squared over

704
00:42:59,270 --> 00:43:01,020
sigma squared and this has little things

705
00:43:03,020 --> 00:43:03,270
on the

706
00:43:03,790 --> 00:43:06,900
okay and the task is really want to infer

707
00:43:08,510 --> 00:43:08,950
and sigma

708
00:43:12,270 --> 00:43:12,670
the data

709
00:43:16,690 --> 00:43:17,790
let's do that's

710
00:43:28,970 --> 00:43:30,500
so let's walk through

711
00:43:31,410 --> 00:43:33,440
the world that we're working in

712
00:43:34,160 --> 00:43:39,400
and that it so that always distinguish two worlds one is hypothesis space where there parameters lived

713
00:43:40,240 --> 00:43:44,330
um and another is the world in which data layers and in this case confusingly

714
00:43:44,580 --> 00:43:48,430
they both look kind of similar to each other because of the unknown mean could

715
00:43:48,430 --> 00:43:49,150
be anywhere on

716
00:43:49,540 --> 00:43:50,130
the real line

717
00:43:50,550 --> 00:43:52,000
and the data come on the real line

718
00:43:53,320 --> 00:43:57,140
i'll try to keep track of when we're talking about data access label

719
00:43:58,100 --> 00:43:59,140
and when we talking about

720
00:44:00,100 --> 00:44:02,480
hypothesis axis that might be labelled new

721
00:44:03,770 --> 00:44:08,760
all the data points come along the state register thee the hypothesis space actually has

722
00:44:08,780 --> 00:44:10,360
two dimensions to it's got sigma

723
00:44:11,100 --> 00:44:15,290
dimension to it as well so one way of telling the difference is sometimes will

724
00:44:15,290 --> 00:44:18,930
show a new axis and the sigma factor he two dimensions we must be in

725
00:44:19,370 --> 00:44:20,920
the hypothesis space the parameter space

726
00:44:21,540 --> 00:44:23,280
so is a one-dimensional picture

727
00:44:23,960 --> 00:44:24,700
and it shows

728
00:44:26,780 --> 00:44:30,190
so it's possible different settings of mu and sigma

729
00:44:30,600 --> 00:44:35,110
all which happened have the same value sigma what the prediction about the data would

730
00:44:35,110 --> 00:44:40,890
be so these are standard guassian densities for different values have mu and sigma

731
00:44:41,630 --> 00:44:42,690
these all happened have

732
00:44:43,510 --> 00:44:44,110
different values and

733
00:44:44,890 --> 00:44:47,070
and the same value sigma namely sigma is what

734
00:44:48,080 --> 00:44:49,550
thank next

735
00:44:51,690 --> 00:44:55,520
there are another six possible hypotheses about mu and sigma

736
00:44:57,500 --> 00:44:58,830
the muse all have the same

737
00:44:59,360 --> 00:45:01,720
and the standard deviation very from point five

738
00:45:02,210 --> 00:45:02,520
up to

739
00:45:03,190 --> 00:45:04,450
two so what i do there

740
00:45:04,960 --> 00:45:07,480
i just bubble around in the hypothesis space

741
00:45:08,430 --> 00:45:10,190
hand showed you want three four

742
00:45:10,930 --> 00:45:14,170
five six point in this two-dimensional space here

743
00:45:14,720 --> 00:45:17,010
maybe i'll quickly get rid space moment

744
00:45:18,510 --> 00:45:20,420
so i j six hypotheses like

745
00:45:21,390 --> 00:45:21,700
and then

746
00:45:25,820 --> 00:45:30,050
why this is so a problem along the axis along a single axis showing you

747
00:45:30,190 --> 00:45:34,940
in data space what each of those projects so any particular one of these makes

748
00:45:34,940 --> 00:45:36,670
its prediction in the data space

749
00:45:37,300 --> 00:45:37,840
which looks

750
00:45:40,120 --> 00:45:42,980
says that's what i the data to be like if i am true

751
00:45:45,170 --> 00:45:47,190
okay anne hear are

752
00:45:47,760 --> 00:45:52,790
and other six hypotheses which have different views on different segments from each other so that's a scattering of points

753
00:45:53,540 --> 00:45:57,430
in new and sigma space and for each one i've shown it predictions

754
00:45:58,980 --> 00:45:59,570
data space

755
00:46:03,970 --> 00:46:05,100
how to do inference

756
00:46:05,910 --> 00:46:07,430
two hundred for mu and sigma

757
00:46:07,910 --> 00:46:11,040
this is the correct way to do inference you get your data

758
00:46:11,500 --> 00:46:13,180
here i'm showing what five

759
00:46:13,620 --> 00:46:14,340
data points

760
00:46:14,820 --> 00:46:16,480
which arrived these horizontal

761
00:46:16,880 --> 00:46:19,260
coordinate in other flatten up above

762
00:46:20,750 --> 00:46:21,190
the line

763
00:46:22,710 --> 00:46:24,050
at an arbitrary like coordinate

764
00:46:25,050 --> 00:46:26,630
right so got five data points

765
00:46:27,040 --> 00:46:28,820
what is based here and say you should do

766
00:46:29,660 --> 00:46:31,490
well you should update

767
00:46:32,370 --> 00:46:33,130
your beliefs

768
00:46:34,850 --> 00:46:38,690
about mu and sigma in proportion to how well each of those bodies new and sigma

769
00:46:39,240 --> 00:46:40,670
predicted what actually happened

770
00:46:49,680 --> 00:46:50,640
basically says

771
00:46:50,990 --> 00:46:53,210
household mu and sigma are given the data

772
00:46:57,290 --> 00:46:58,140
how well they predict

773
00:47:01,790 --> 00:47:03,380
multiplied by how well they were

774
00:47:04,480 --> 00:47:05,330
before you later

775
00:47:11,630 --> 00:47:12,770
the normalizing constant

776
00:47:22,230 --> 00:47:22,650
this sigma

777
00:47:23,680 --> 00:47:24,430
all this stuff

778
00:47:38,250 --> 00:47:38,820
i'm taking

779
00:47:39,640 --> 00:47:42,790
in the next few lectures quite a hardline position that i think when there's a

780
00:47:42,790 --> 00:47:45,010
well posed questions is the correct answer to it

781
00:47:45,590 --> 00:47:49,480
and this is in contrast to the way a lot of people think about statistical inference

782
00:47:50,080 --> 00:47:54,110
there's another way of doing inference where there's lots of ways inventing well posed questions

783
00:47:54,140 --> 00:47:56,050
and i think that's an extraordinary

784
00:47:56,630 --> 00:47:57,900
o thing to believe i think there is

785
00:47:58,630 --> 00:48:03,430
a correct way events from problems and i don't think inference should be an exception from this

786
00:48:04,670 --> 00:48:06,930
something make really explicit is that

787
00:48:09,320 --> 00:48:11,250
all these inferences depend on assumptions

788
00:48:12,300 --> 00:48:16,730
so people sometimes bayesian methods so subjective that kind of assumptions this you don't have

789
00:48:16,730 --> 00:48:18,850
to make assumptions in order to do inference

790
00:48:20,620 --> 00:48:23,490
i've got the right yes these all depend on assumptions

791
00:48:24,330 --> 00:48:25,500
but how could it be otherwise

792
00:48:26,050 --> 00:48:28,520
so everyone these things is conditional on age

793
00:48:29,710 --> 00:48:30,530
and the yes

794
00:48:32,000 --> 00:48:32,290
is this

795
00:48:33,740 --> 00:48:37,870
so this is an age is assumption about the data come from a gas in

796
00:48:37,870 --> 00:48:42,600
distribution andits whatever assumptions we want to make about what we believe music might be

797
00:48:44,250 --> 00:48:46,970
and i think it's on an escape all that you do have to make

798
00:48:47,580 --> 00:48:48,180
the assumptions

799
00:48:49,480 --> 00:48:49,990
all right

800
00:48:51,700 --> 00:48:53,040
what is this actually look like

801
00:48:53,460 --> 00:48:57,570
well this is saying how well they do predict the data that actually happened and this is very

802
00:48:58,220 --> 00:48:58,860
the native it

803
00:48:59,660 --> 00:49:01,530
you know if we if we haven't got any particular

804
00:49:01,930 --> 00:49:06,830
strong interesting prior knowledge about using we let it go and focus on the data dependent term

805
00:49:08,320 --> 00:49:09,330
and basically says

806
00:49:09,790 --> 00:49:13,830
you don't need to know how well they were creating things that didn't happen

807
00:49:14,280 --> 00:49:19,440
tail probabilities are not relevant there's no such thing as tails in the correct way of doing inference

808
00:49:19,440 --> 00:49:20,930
and that that

809
00:49:20,980 --> 00:49:24,790
they are the following so one is i think goes back

810
00:49:24,850 --> 00:49:29,280
five more of these i know it going back to all square number

811
00:49:29,320 --> 00:49:32,480
that you basically think of a sieve

812
00:49:32,490 --> 00:49:34,510
of models who have

813
00:49:34,530 --> 00:49:37,990
the nested parametric models for instance think of

814
00:49:38,000 --> 00:49:39,710
you know mixtures of

815
00:49:39,720 --> 00:49:43,930
one galaxy into gaussians three gaussians et cetera etcetera index them by

816
00:49:45,680 --> 00:49:47,620
and then

817
00:49:47,670 --> 00:49:50,970
for each of these you act as if the model were true

818
00:49:51,020 --> 00:49:53,620
prior to plug in the maximum likelihood estimate

819
00:49:53,630 --> 00:49:55,540
and now what's left is

820
00:49:55,550 --> 00:49:56,810
choose the m

821
00:49:56,820 --> 00:49:58,210
that's not the trivial part

822
00:49:58,220 --> 00:50:00,670
that's the nontrivial part actions

823
00:50:00,680 --> 00:50:05,540
the parallel prescription due to volcanic is you basically think of the class

824
00:50:05,580 --> 00:50:08,250
as the class of all decision procedures of all

825
00:50:08,300 --> 00:50:10,820
classifiers are predictors

826
00:50:10,830 --> 00:50:12,480
as the union of

827
00:50:12,490 --> 00:50:17,190
finite or parametric the closure of a finite parametric classes right so

828
00:50:17,250 --> 00:50:19,640
linear quadratic

829
00:50:19,690 --> 00:50:22,010
court less if you think a regression

830
00:50:22,020 --> 00:50:26,540
a regression based on the variable itself linear regression based on the variable itself

831
00:50:26,580 --> 00:50:31,470
based on the variables square q the variables were q and so on

832
00:50:31,560 --> 00:50:33,850
and that we can approximate in principle

833
00:50:33,860 --> 00:50:36,570
any function of one variable

834
00:50:36,620 --> 00:50:39,070
that's the idea if you want also behind

835
00:50:39,080 --> 00:50:42,790
the support vector story writing that that's why you can always separate

836
00:50:42,800 --> 00:50:45,980
the samples go high and up and you can approximate

837
00:50:47,210 --> 00:50:49,560
one-dimensional down

838
00:50:49,580 --> 00:50:53,530
but the public always insists that in this case you should always do is use

839
00:50:53,540 --> 00:50:56,790
for the empirical risk minimization which is

840
00:50:56,850 --> 00:50:59,010
if you take an empirical estimate

841
00:51:00,400 --> 00:51:02,790
the expected loss and minimize

842
00:51:02,840 --> 00:51:04,070
and then of course

843
00:51:04,110 --> 00:51:06,660
from that point on one side to ask about

844
00:51:06,710 --> 00:51:08,780
how well does actually do

845
00:51:08,830 --> 00:51:11,400
in in comparison to what

846
00:51:11,410 --> 00:51:13,260
the minimum value of the risk is

847
00:51:13,280 --> 00:51:19,420
but again you're left with the issue of how do you pick them

848
00:51:20,180 --> 00:51:24,060
from this point of view there's sort of two new criteria which again

849
00:51:24,100 --> 00:51:27,930
run parallel and statistics and computer science

850
00:51:27,940 --> 00:51:31,830
so you look at the minimum regret

851
00:51:31,890 --> 00:51:33,850
that you can get

852
00:51:33,900 --> 00:51:37,160
over all these possible rules

853
00:51:37,990 --> 00:51:43,640
this is of course again quantity only god can get but god gets it's got

854
00:51:43,640 --> 00:51:45,260
being restricted

855
00:51:45,310 --> 00:51:46,610
right got can

856
00:51:46,640 --> 00:51:51,080
knows what appears but has to use one of the delta hands

857
00:51:51,090 --> 00:51:53,680
one of them has

858
00:51:57,960 --> 00:52:02,540
and then you have to say something is an oracle rule if in fact the

859
00:52:02,540 --> 00:52:05,060
rate the ratio of what you can do

860
00:52:06,680 --> 00:52:10,140
by estimating and some data dependent way to work

861
00:52:10,150 --> 00:52:11,430
god can do

862
00:52:12,260 --> 00:52:16,610
ten is one

863
00:52:16,660 --> 00:52:18,670
and then there's is an analogous thing

864
00:52:18,680 --> 00:52:24,970
in statistics which is the so-called adaptive minimax us get that but really the the

865
00:52:24,970 --> 00:52:28,500
main point that i'm trying to get to which and finally and is

866
00:52:28,510 --> 00:52:30,540
that it is possible

867
00:52:30,590 --> 00:52:34,140
four very wide classes of spaces right

868
00:52:34,190 --> 00:52:35,930
spaces of functions

869
00:52:35,980 --> 00:52:37,010
which is smooth

870
00:52:37,050 --> 00:52:38,180
you look at this

871
00:52:38,190 --> 00:52:39,710
and at at

872
00:52:39,720 --> 00:52:41,700
the sphere in that class

873
00:52:41,750 --> 00:52:44,230
it is possible to choose

874
00:52:44,280 --> 00:52:47,550
in the data dependent way of course

875
00:52:47,600 --> 00:52:52,090
so that both the oracle criterion holds

876
00:52:52,100 --> 00:52:55,590
and the minimum that the minimax criterion holds

877
00:52:57,160 --> 00:53:00,940
the thing that i want to point is that what you conclude from that is

878
00:53:00,990 --> 00:53:03,660
but the best thing that in our

879
00:53:03,680 --> 00:53:05,730
going in that direction

880
00:53:05,760 --> 00:53:09,120
what you find is that the best thing that

881
00:53:09,250 --> 00:53:11,050
even god can do given

882
00:53:17,120 --> 00:53:22,610
behaves like an to the minus two k or two k plus the

883
00:53:22,660 --> 00:53:28,310
where k is some measure the smoothness of p

884
00:53:28,310 --> 00:53:34,570
and this information interpretation isn't that although some of the original interpretation was relevant logicians

885
00:53:34,570 --> 00:53:38,320
wanted to do and i know most of nursing relevant logic for the new central

886
00:53:38,320 --> 00:53:39,740
much for

887
00:53:39,880 --> 00:53:41,860
if you look at

888
00:53:41,880 --> 00:53:46,610
well relevant logicians wanted to do was avoid

889
00:53:46,650 --> 00:53:51,980
what are called the paradoxes of material and strict implication

890
00:54:06,690 --> 00:54:10,490
it's natural write a plus sign down for and but you know in in logic

891
00:54:10,490 --> 00:54:12,310
classes you should never do that

892
00:54:13,520 --> 00:54:16,530
it means or write them in addition is or

893
00:54:16,550 --> 00:54:18,110
and think back to bowl

894
00:54:18,490 --> 00:54:23,610
his his his edition his or was addition

895
00:54:24,750 --> 00:54:28,180
materials to and strict implication

896
00:54:28,280 --> 00:54:29,940
OK so

897
00:54:29,950 --> 00:54:39,610
if we read the arrow

898
00:54:45,460 --> 00:54:47,980
in inference

899
00:54:48,060 --> 00:54:50,280
let's start with this one

900
00:54:50,300 --> 00:54:52,700
this is my favorite

901
00:54:53,990 --> 00:54:57,060
from any proposition we can get any

902
00:54:57,110 --> 00:55:00,190
the topology in classical logic

903
00:55:01,130 --> 00:55:03,110
we can infer that

904
00:55:04,130 --> 00:55:09,730
this seems to be something wrong about that

905
00:55:09,760 --> 00:55:12,480
the moon is made of green cheese therefore

906
00:55:12,500 --> 00:55:14,180
you training or small

907
00:55:14,410 --> 00:55:17,880
that's not a sound argument because the moon is made of green cheese

908
00:55:17,930 --> 00:55:20,700
but according to classical logic is valid

909
00:55:20,750 --> 00:55:26,080
classical logic is a very blunt instrument it's a wonderful instrument i don't like dislike

910
00:55:26,080 --> 00:55:27,540
classical logic

911
00:55:27,590 --> 00:55:32,490
i'm always accused of this but i don't actually dislike it but is a very

912
00:55:32,490 --> 00:55:35,860
blunt instrument once the nice instrument because

913
00:55:35,900 --> 00:55:39,380
using very little prior propositional

914
00:55:39,390 --> 00:55:42,670
classical logic propositional calculus is you know

915
00:55:44,520 --> 00:55:51,410
this fantastic themes uses truth tables true and false and functions extension functions on the

916
00:55:51,570 --> 00:55:52,730
to represent

917
00:55:52,740 --> 00:55:54,680
always connected

918
00:55:54,710 --> 00:55:59,750
it is a very good job at that it just doesn't do a perfect job

919
00:55:59,800 --> 00:56:05,580
and validity is done on in terms of those as a very blunt instrument it

920
00:56:05,630 --> 00:56:09,950
it approximates and it gets a lot of things right it tells us modus ponens

921
00:56:09,950 --> 00:56:14,970
valid modus tollens is valid affirming the consequent is not valid and so on and

922
00:56:14,970 --> 00:56:16,730
you get a lot of things right

923
00:56:16,740 --> 00:56:19,230
but doesn't get everything right

924
00:56:19,470 --> 00:56:20,880
this seems on

925
00:56:22,680 --> 00:56:25,650
relevant logicians say what's on but it is this

926
00:56:25,700 --> 00:56:27,920
has nothing to do with this

927
00:56:29,350 --> 00:56:33,270
the moon is made of green cheese therefore training or it's not the move could

928
00:56:33,270 --> 00:56:37,720
be made of anything doesn't really affect the weather

929
00:56:37,760 --> 00:56:41,640
last time i gave a talk on this i i i talked about this is

930
00:56:41,640 --> 00:56:43,310
that in my

931
00:56:43,320 --> 00:56:47,810
we need professor they making give a big public address thank you that the public

932
00:56:47,810 --> 00:56:49,930
address last year they are

933
00:56:49,940 --> 00:56:55,950
you likely the audience is not allowed to ask questions but i do a lot

934
00:56:55,950 --> 00:57:00,150
of people receiving there because i talked about a beautiful beach in new zealand very

935
00:57:00,170 --> 00:57:03,460
key beach i it's being beautiful has nothing to do with the money being made

936
00:57:03,460 --> 00:57:07,370
of green cheese that somebody that was was as one one of my colleagues there

937
00:57:07,470 --> 00:57:11,050
someone whispering yes it does it's got to do with the effects of the tides

938
00:57:11,050 --> 00:57:13,720
and all the rest of yeah maybe

939
00:57:14,810 --> 00:57:19,840
that's larger that it's still logically valid inference becomes

940
00:57:19,890 --> 00:57:23,470
right you need to you if you put a lot of connecting principles physics maybe

941
00:57:23,470 --> 00:57:27,970
you could get a logically valid inference you can always get logically valid inference that

942
00:57:27,970 --> 00:57:31,630
of an invalid inference right if you have

943
00:57:31,820 --> 00:57:35,620
that's not logically valid inference right

944
00:57:35,670 --> 00:57:37,270
i hope not

945
00:57:38,930 --> 00:57:42,520
you can and

946
00:57:42,610 --> 00:57:45,360
you can add a premise

947
00:57:45,370 --> 00:57:49,160
try to get a lot you know is that premises

948
00:57:49,180 --> 00:57:55,050
to get a logically valid inference

949
00:57:55,080 --> 00:57:57,120
that's so that's not the point

950
00:57:57,160 --> 00:58:01,370
about whether it's train it's the fact is that logically they have nothing to do

951
00:58:01,370 --> 00:58:07,000
with one another you need more premises to get that it shouldn't be just becomes

952
00:58:07,000 --> 00:58:11,130
this is it at all it's always true

953
00:58:11,150 --> 00:58:12,780
doesn't really

954
00:58:12,810 --> 00:58:17,160
i mean then it follows from any and that's the problem

955
00:58:17,880 --> 00:58:19,870
a lot of

956
00:58:19,920 --> 00:58:21,710
relevant logicians

957
00:58:22,530 --> 00:58:25,870
i think in a way groping towards what's what's what's really right

958
00:58:25,870 --> 00:58:30,150
i talked about certain situations are structures in which

959
00:58:30,450 --> 00:58:34,650
world in which this wasn't true what are the relevant logicians should be doing that

960
00:58:34,650 --> 00:58:38,420
and i'll get back to that as we go along

961
00:58:38,730 --> 00:58:42,770
because the problem isn't about the truth of the conclusion the problem is about whether

962
00:58:42,770 --> 00:58:47,770
or not the fact that something is always true means it follows from everything

963
00:58:47,810 --> 00:58:52,010
OK so we needed different notion of validity if we're going to capture more fine

964
00:58:54,050 --> 00:58:58,970
so notion of good edits of argument than classical logic gives us

965
00:58:58,970 --> 00:59:02,280
OK and there are various and connected with this

966
00:59:02,290 --> 00:59:05,010
is it is the treatment of the implication

967
00:59:05,030 --> 00:59:11,190
and that it's the paradoxes of strict material conditions

968
00:59:11,220 --> 00:59:15,950
OK i was using is these before knowledge system was spears here's a better

969
00:59:16,160 --> 00:59:23,520
that makes it clear that there is no common content

970
00:59:23,540 --> 00:59:27,470
and things like that now

971
00:59:33,570 --> 00:59:35,660
OK that's enough to go on with

972
00:59:35,660 --> 00:59:40,170
will deal with these two first forget about the first

973
00:59:40,180 --> 00:59:44,310
i those have a problem in one of the immediate problem you see there

974
00:59:44,360 --> 00:59:49,420
is that there's no common content between antecedent and consequent

975
00:59:49,420 --> 00:59:51,400
right of the main connective

976
00:59:51,420 --> 00:59:54,220
now why is that the problem well

977
00:59:54,270 --> 00:59:58,760
this has nothing to do with that that's clear

978
00:59:58,770 --> 01:00:01,310
and this has nothing to do with that

979
01:00:01,330 --> 01:00:04,900
these are all if we interpret the implication is either

980
01:00:04,940 --> 01:00:09,050
material implication or or or necessary implication sense of

981
01:00:09,140 --> 01:00:10,880
eight more logic who

982
01:00:10,930 --> 01:00:16,960
you're going to get those is there

983
01:00:16,960 --> 01:00:20,040
OK so what you are trying to do

984
01:00:21,390 --> 01:00:24,100
for the next two-and-a-half to three hours or so

985
01:00:24,170 --> 01:00:27,610
is given you

986
01:00:27,880 --> 01:00:33,230
basic introduction to a class of machine learning methods that have become widely popular over

987
01:00:33,230 --> 01:00:35,460
the last ten years

988
01:00:35,480 --> 01:00:39,000
and this class of approaches to machine learning

989
01:00:39,010 --> 01:00:44,590
can more or less meat as a kernel based approaches to pattern recognition machine learning

990
01:00:44,590 --> 01:00:48,310
and so on although the most famous of these kind of a algorithms the support

991
01:00:48,310 --> 01:00:49,860
vector machine

992
01:00:49,870 --> 01:00:55,890
now in some sense kernel based approaches have very classical roots although there are many

993
01:00:55,890 --> 01:00:57,630
modern twists to it

994
01:00:57,640 --> 01:01:00,860
so there are many ways in which one can tell the story of the development

995
01:01:00,860 --> 01:01:03,150
of kernel based approaches and

996
01:01:03,160 --> 01:01:06,780
i want to emphasise that this is our own perspective on these matters

997
01:01:06,860 --> 01:01:12,970
and so we're going to corresponding correspondingly emphasise certain aspects and the de-emphasize than others

998
01:01:13,020 --> 01:01:16,870
and there are many other introductions to kernel based methods that you

999
01:01:16,920 --> 01:01:20,110
i have access to especially on the web these days

1000
01:01:20,130 --> 01:01:22,990
there's a website called kernel machines that or

1001
01:01:23,030 --> 01:01:26,500
where you can get a lot of tutorials

1002
01:01:29,730 --> 01:01:34,580
the way we are going to structure it is that i'm going to talk about

1003
01:01:34,580 --> 01:01:36,200
an hour and a half or so

1004
01:01:36,250 --> 01:01:43,750
giving you are sort of setting the stage and introducing the context of pattern recognition

1005
01:01:43,760 --> 01:01:45,130
more generally

1006
01:01:45,140 --> 01:01:51,190
and motivating and developing the basic machinery for reproducing kernel hilbert space plays a central

1007
01:01:51,190 --> 01:01:56,270
role in approaching learning problem from economics perspective

1008
01:01:56,290 --> 01:02:00,490
and then we shall talk for the next one and a half hours applying the

1009
01:02:00,490 --> 01:02:04,890
machinery of reproducing kernel hilbert spaces to a variety of problems that you see what

1010
01:02:04,910 --> 01:02:06,910
the algorithmic consequences are

1011
01:02:07,030 --> 01:02:10,150
of these of these objects

1012
01:02:15,250 --> 01:02:22,620
subject is that the border of many different disciplines it's variously called pattern recognition and

1013
01:02:22,620 --> 01:02:26,020
machine learning classification regression

1014
01:02:26,040 --> 01:02:30,410
and of course you know and i'm sure you're aware that people in engineering and

1015
01:02:30,410 --> 01:02:34,120
computer science and statistics and mathematics and these are not all the disciplines

1016
01:02:34,490 --> 01:02:40,020
i have something to say which is often mathematical computational nature on these questions so

1017
01:02:40,020 --> 01:02:41,870
it's important to emphasise that

1018
01:02:45,600 --> 01:02:50,630
the basic problem of learning from examples more or less goes like this

1019
01:02:50,680 --> 01:02:55,430
there are these these basic objects there is some space of patterns which we call

1020
01:02:55,430 --> 01:02:57,070
capital x OK

1021
01:02:57,110 --> 01:03:02,240
this is somehow the pattern space the instance space in the example space where all

1022
01:03:02,240 --> 01:03:04,330
the patterns led

1023
01:03:04,410 --> 01:03:06,760
examples that people look at

1024
01:03:06,770 --> 01:03:07,740
i would be

1025
01:03:07,750 --> 01:03:10,100
are in the standard vector space

1026
01:03:10,110 --> 01:03:12,010
a manifold m

1027
01:03:12,020 --> 01:03:14,320
are finite set x

1028
01:03:14,360 --> 01:03:17,680
of which are interesting cases the boolean hypercube

1029
01:03:17,690 --> 01:03:19,780
and sequences

1030
01:03:19,790 --> 01:03:22,290
sigma star

1031
01:03:22,300 --> 01:03:25,620
OK so this is the set where all the patterns live

1032
01:03:25,640 --> 01:03:30,340
and then there is another set which we should familiarize themselves with that's why

1033
01:03:30,390 --> 01:03:34,360
which is the label space of the predictions based on the response this

1034
01:03:34,410 --> 01:03:39,460
called by different names in different communities and these are usually for our purposes at

1035
01:03:39,460 --> 01:03:40,390
least today

1036
01:03:40,550 --> 01:03:42,330
will usually be rn

1037
01:03:42,350 --> 01:03:44,630
and usually will take into the one

1038
01:03:44,640 --> 01:03:47,420
which would be the standard regression

1039
01:03:48,830 --> 01:03:51,410
classification which is usually

1040
01:03:51,430 --> 01:03:53,920
one of a finite number of labels

1041
01:03:53,930 --> 01:03:57,200
and most of the time will actually discuss the case where n is equal to

1042
01:03:57,800 --> 01:04:01,730
so it's the two classes binary classification problem minus one one

1043
01:04:02,310 --> 01:04:04,100
so you have then

1044
01:04:04,120 --> 01:04:05,610
given x and y

1045
01:04:05,620 --> 01:04:09,160
labelled examples that is x y pairs

1046
01:04:09,170 --> 01:04:14,420
OK if somebody preventing through about and giving a label associated with that that and

1047
01:04:14,420 --> 01:04:16,640
from these labels you don't want to learn

1048
01:04:16,650 --> 01:04:19,020
you the regressor or classifier

1049
01:04:19,030 --> 01:04:22,550
depending on the context

1050
01:04:22,600 --> 01:04:23,400
so i

1051
01:04:23,410 --> 01:04:27,380
david already talked about some examples here are some more so here the digit recognition

1052
01:04:28,510 --> 01:04:32,970
so in the learning from examples settings so these would be example somebody actually gives

1053
01:04:32,970 --> 01:04:35,550
you images of handwritten digits

1054
01:04:35,570 --> 01:04:37,590
two three five seven

1055
01:04:37,640 --> 01:04:40,370
and so on and so these are the examples you want to predict the class

1056
01:04:40,370 --> 01:04:41,680
of new data point

1057
01:04:41,690 --> 01:04:45,140
so from these examples you hopefully want to learn a classifier

1058
01:04:45,150 --> 01:04:48,440
which is a map from the set x of all possible images

1059
01:04:48,490 --> 01:04:51,510
to the set y which is basically in this case

1060
01:04:51,540 --> 01:04:56,610
has been members

1061
01:04:56,770 --> 01:05:00,110
a face recognition examples of these which

1062
01:05:00,120 --> 01:05:02,120
i don't know if you're able to see you actually

1063
01:05:02,130 --> 01:05:04,080
lots and lots of faces

1064
01:05:04,130 --> 01:05:07,110
so you get lots of examples of faces

1065
01:05:07,160 --> 01:05:09,780
and then somebody present steel picture

1066
01:05:09,800 --> 01:05:13,640
and ask sue is this picture of a grandmother

1067
01:05:13,650 --> 01:05:15,620
and the answer yes or no

1068
01:05:15,700 --> 01:05:16,920
and that

1069
01:05:17,020 --> 01:05:19,560
a face recognition example problem

1070
01:05:19,600 --> 01:05:23,340
same setting so the set x is the set of all images

1071
01:05:25,260 --> 01:05:28,650
and you want to predict the class of new data point

1072
01:05:28,660 --> 01:05:31,650
here is an example from financed

1073
01:05:31,700 --> 01:05:32,790
a lot of people

1074
01:05:32,810 --> 01:05:34,520
interested in this

1075
01:05:35,080 --> 01:05:39,540
so you want to predict now a real possibility of real valued

1076
01:05:39,860 --> 01:05:42,390
function the values variables

1077
01:05:42,400 --> 01:05:46,360
which is the stock price of stock the next day

1078
01:05:46,410 --> 01:05:48,910
and you have at your disposal

1079
01:05:48,930 --> 01:05:53,830
all the variables that you might want to make sure that you think might help

1080
01:05:53,830 --> 01:05:56,840
predict stock prices in the future

1081
01:05:56,910 --> 01:06:02,250
think these could be passed historical time series of this or related stocks other economic

1082
01:06:02,250 --> 01:06:08,340
indicators and so on

1083
01:06:08,400 --> 01:06:15,080
there are more examples here's an example from grammatical inference z axis sigma star

1084
01:06:15,120 --> 01:06:18,330
OK the set of all sequences take sigma to be the set of all the

1085
01:06:18,330 --> 01:06:19,680
words in english

1086
01:06:19,740 --> 01:06:23,640
here a positive example on the top here and from there with his money

1087
01:06:23,660 --> 01:06:25,350
let's label one

1088
01:06:25,360 --> 01:06:28,830
and here at the bottom is his money with their from ronny

1089
01:06:29,150 --> 01:06:31,060
minus one

1090
01:06:31,860 --> 01:06:36,360
and so you want to learn the imagine from sigma started minus one one

1091
01:06:36,360 --> 01:06:39,030
this is of course just the language in the sense

1092
01:06:39,070 --> 01:06:40,550
and the grammar would be

1093
01:06:40,550 --> 01:06:44,480
then a generator of the of the acceptable strings

1094
01:06:44,490 --> 01:06:49,800
this is a list of this actually because in grammatical inference usually almost always you

1095
01:06:49,800 --> 01:06:52,050
learn only with positive examples

1096
01:06:52,110 --> 01:06:57,870
so when you actually infer the grammar of a language no one actually samples for

1097
01:06:57,870 --> 01:07:02,250
you densely the universe of possible negative examples

1098
01:07:02,250 --> 01:07:03,550
sigma alpha mu

1099
01:07:03,570 --> 01:07:07,720
is the mean and sigma is the variance of covariance in the multivariate case

1100
01:07:07,740 --> 01:07:13,180
here have two thousand distributions with with different means of the mean is the for

1101
01:07:13,180 --> 01:07:15,840
the black distribution here the mean for the distribution here

1102
01:07:15,950 --> 01:07:20,070
and i have a marked also the standard deviation so this a bit wider distribution

1103
01:07:20,400 --> 01:07:26,490
because it distribution to one it's also going to be have a smaller magnitude

1104
01:07:26,530 --> 01:07:29,360
the one-dimensional gas distribution

1105
01:07:30,210 --> 01:07:32,460
which is easy to draw so i can also

1106
01:07:32,470 --> 01:07:36,810
more draw two-dimensional distribution but it's very hard to draw

1107
01:07:36,820 --> 01:07:40,080
high dimensional distributions and it turns out

1108
01:07:40,090 --> 01:07:43,080
that gaston processes are just very high dimensional

1109
01:07:43,130 --> 01:07:46,140
girls in distribution like so we have to imagine

1110
01:07:46,300 --> 01:07:50,390
high dimensional gaussians here's the two dimensional gaussians so instead of

1111
01:07:50,450 --> 01:07:52,320
instead of so showing you

1112
01:07:52,970 --> 01:07:54,770
b is

1113
01:07:56,060 --> 01:07:59,060
the value of the probability density i've just shown you here

1114
01:07:59,070 --> 01:08:02,620
the mean to showing you two ellipses here that

1115
01:08:02,760 --> 01:08:07,700
correspond to the one standard deviation in the two thousand

1116
01:08:07,710 --> 01:08:08,450
and the

1117
01:08:08,710 --> 01:08:13,040
and the the variance here is encoded in the in the covariance matrix of the

1118
01:08:13,040 --> 01:08:15,640
commercial campgrounds matrix in this case has

1119
01:08:15,810 --> 01:08:19,810
two i can make i can that to this one goes in this direction won

1120
01:08:19,810 --> 01:08:22,330
the gold in this direction and the i can value

1121
01:08:22,340 --> 01:08:28,080
going in this direction so the variance in that direction corresponds to the i value

1122
01:08:28,130 --> 01:08:30,570
of the covariance

1123
01:08:30,580 --> 01:08:32,010
here are written out there

1124
01:08:32,020 --> 01:08:38,110
the actual distribution is gory detail

1125
01:08:38,190 --> 01:08:39,320
OK so the true

1126
01:08:39,390 --> 01:08:42,440
properties of the gaussians that are going to be of central importance

1127
01:08:42,450 --> 01:08:47,420
one is called the the conditional distribution in one is called the marginal distribution conditional

1128
01:08:47,420 --> 01:08:52,130
distribution so here i've got my my two-dimensional gas from before

1129
01:08:52,210 --> 01:08:56,500
and the conditional distribution of one variable given the other variables

1130
01:08:56,630 --> 01:09:00,550
try to illustrate that here let's say that this is a joint distribution over the

1131
01:09:00,560 --> 01:09:02,260
two dimensional space here

1132
01:09:02,310 --> 01:09:04,010
now let's say that i serve

1133
01:09:04,020 --> 01:09:06,150
that the value of this variable

1134
01:09:07,580 --> 01:09:10,760
this variable has this

1135
01:09:10,820 --> 01:09:14,220
this variable sorry had this particular value

1136
01:09:15,130 --> 01:09:18,300
and i say OK this parable as this particular value

1137
01:09:18,320 --> 01:09:21,140
then what does that imply about the other variable

1138
01:09:21,280 --> 01:09:24,940
so what i'm doing here is to remind slicing through the distribution

1139
01:09:25,030 --> 01:09:27,940
and seeing what's left over the function of the other variables

1140
01:09:28,430 --> 01:09:32,100
and you can see or hear if i know that this is the value of

1141
01:09:32,100 --> 01:09:36,780
this variable then the probability that the this variable takes on some

1142
01:09:36,820 --> 01:09:41,540
some negative value of value here is very small because there isn't any mass down

1143
01:09:42,220 --> 01:09:45,770
the masses is over here the conditional distribution here

1144
01:09:45,890 --> 01:09:47,320
look something like this

1145
01:09:48,800 --> 01:09:52,920
and if you think back to the gaussians is that if you condition gauss and

1146
01:09:52,920 --> 01:09:55,920
then you get another calcium the conditional distribution here

1147
01:09:55,930 --> 01:09:57,360
it's also got some

1148
01:09:57,420 --> 01:10:01,050
and the last thing we need to know about the marginal distribution so marginal distributions

1149
01:10:01,050 --> 01:10:04,820
is just that you do some out or integrate out

1150
01:10:04,820 --> 01:10:06,270
one of the variables

1151
01:10:06,740 --> 01:10:11,240
and look at what is now the marginal distribution for the other variable just collect

1152
01:10:11,240 --> 01:10:13,840
collapsing projecting down this

1153
01:10:13,900 --> 01:10:16,290
joint distribution on to one particular

1154
01:10:18,040 --> 01:10:18,890
and again

1155
01:10:18,900 --> 01:10:20,360
the marginal distribution

1156
01:10:20,400 --> 01:10:26,870
of when you marginalise the gauss and you get an account

1157
01:10:26,940 --> 01:10:29,370
OK so now what is the accounting process

1158
01:10:30,190 --> 01:10:37,290
because the process is just a generalisation of a multivariate gaussians to infinity minerals

1159
01:10:37,300 --> 01:10:40,770
so just like so it's sort of a natural extension when you go from there

1160
01:10:41,230 --> 01:10:45,420
for univariate out into multivariate gaussians and i just go to

1161
01:10:46,240 --> 01:10:51,450
a larger model that calculates the gaussians that contains infinitely many

1162
01:10:51,460 --> 01:10:53,480
many variables

1163
01:10:54,740 --> 01:10:59,320
one can ask why are we interested in infinitely many variables that sound sounds to

1164
01:10:59,320 --> 01:11:01,590
be the sort of thing to be interested in

1165
01:11:01,650 --> 01:11:05,910
but the reason for this is that we can think of functions and function source

1166
01:11:05,930 --> 01:11:11,320
of natural things to try find in so infer properties of you can think of

1167
01:11:11,320 --> 01:11:15,160
the function is just being an infinitely long vector

1168
01:11:16,550 --> 01:11:21,840
because you can simply specify what is the value of the function for any

1169
01:11:22,060 --> 01:11:24,550
value of the argument x k

1170
01:11:24,560 --> 01:11:26,530
this would be an infinitely long vector

1171
01:11:26,600 --> 01:11:29,360
this is the fifteenth like very primitive notion

1172
01:11:29,360 --> 01:11:33,190
some parameter which is more natural in adapts the data

1173
01:11:33,200 --> 01:11:37,810
so like a new parameter which automatically controls how many points should lie outside the

1174
01:11:37,810 --> 01:11:41,070
tube so in a way you would like to compute the tube such that most

1175
01:11:41,070 --> 01:11:44,630
of the points are inside some of them outside you don't know what epsilon will

1176
01:11:44,630 --> 01:11:47,040
achieve this property so you would

1177
01:11:47,060 --> 01:11:49,350
back to compute automatically

1178
01:11:49,360 --> 01:11:52,910
so if we do this we need to

1179
01:11:52,960 --> 01:11:55,410
use this optimisation problem

1180
01:11:55,430 --> 01:12:00,370
so again like in the case of classification we have here the empirical error term

1181
01:12:00,370 --> 01:12:03,230
here we have this extra term involving new

1182
01:12:03,250 --> 01:12:06,390
and epsilon so now newest chosen

1183
01:12:06,400 --> 01:12:11,060
clearly an epsilon is the variable of the optimisation problem

1184
01:12:11,070 --> 01:12:14,680
and now may give you a little intuition for

1185
01:12:14,740 --> 01:12:17,460
how this works and it's actually similar in

1186
01:12:17,480 --> 01:12:21,350
pattern recognition in order to let me try to give you a graphical

1187
01:12:23,170 --> 01:12:26,270
OK so let's

1188
01:12:26,290 --> 01:12:28,260
let's assume

1189
01:12:28,270 --> 01:12:33,800
so it's the whether this will work with and be able to follow this explanation

1190
01:12:33,800 --> 01:12:36,160
of words so our cost function

1191
01:12:37,100 --> 01:12:39,210
this one here

1192
01:12:39,230 --> 01:12:44,840
our objective function of slightly write it by dividing by c

1193
01:12:44,860 --> 01:12:48,120
now if i have found a solution of this problem to remember the variables but

1194
01:12:48,130 --> 01:12:51,480
w and epsilon

1195
01:12:51,490 --> 01:12:53,010
so far solution

1196
01:12:53,030 --> 01:12:58,730
to my optimisation problem then the solution is optimal with respect to w epsilon in

1197
01:12:58,730 --> 01:13:01,990
particular is optimal with respect to epsilon

1198
01:13:02,000 --> 01:13:04,390
so let me forget about w from now

1199
01:13:04,440 --> 01:13:07,580
and just look at the epsilon

1200
01:13:07,590 --> 01:13:10,690
so optimal with respect to epsilon which means

1201
01:13:10,700 --> 01:13:15,570
if i wiggle around upset signed a little bit then

1202
01:13:16,410 --> 01:13:18,130
solution to it

1203
01:13:18,210 --> 01:13:20,700
you get large

1204
01:13:20,720 --> 01:13:24,070
so now let's assume that new

1205
01:13:26,690 --> 01:13:31,300
well it's not let's first try to understand these two influences here so here we

1206
01:13:31,300 --> 01:13:35,300
have to do so we want to minimize this function here we have which is

1207
01:13:35,300 --> 01:13:40,310
proportional to epsilon nu is the product chosen by the user this is proportional to

1208
01:13:40,460 --> 01:13:44,600
sine and therefore if i increase epsilon

1209
01:13:44,610 --> 01:13:47,030
this term here will increase

1210
01:13:47,050 --> 01:13:51,750
at the same time if for increase epsilon two overall after

1211
01:13:51,830 --> 01:13:55,360
i win a little bit because if i have some points that lie lying outside

1212
01:13:55,360 --> 01:13:56,940
that these two guys here

1213
01:13:56,960 --> 01:14:00,370
the corresponding excise will get smaller

1214
01:14:00,470 --> 01:14:02,230
so i increase epsilon

1215
01:14:02,240 --> 01:14:06,370
i was a little bit but i win literally a little bit here

1216
01:14:06,560 --> 01:14:09,590
since these ties will get smaller

1217
01:14:09,610 --> 01:14:14,220
similarly if i can decrease epsilon

1218
01:14:14,240 --> 01:14:19,150
then i went a little bit here is to get smaller

1219
01:14:19,580 --> 01:14:22,630
but i was a little bit here because of the decrease in epsilon so maybe

1220
01:14:22,630 --> 01:14:23,580
this line

1221
01:14:23,620 --> 01:14:27,790
we lie here and for this point i have to pay a large size

1222
01:14:27,800 --> 01:14:32,990
it's not for ask how much to you in how much do you lose

1223
01:14:33,000 --> 01:14:35,630
so if i change epsilon

1224
01:14:35,640 --> 01:14:42,160
in this term i will always lose win proportionally to new

1225
01:14:42,170 --> 01:14:43,970
on the other hand if i

1226
01:14:43,990 --> 01:14:49,580
decrease or increase epsilon will win or lose here

1227
01:14:49,760 --> 01:14:53,440
if i can decrease epsilon

1228
01:14:53,460 --> 01:14:58,730
well that's the increase in salary for increase of silent will when something here that

1229
01:14:58,730 --> 01:15:04,470
will win something precisely for those point that having non-zero i

1230
01:15:04,490 --> 01:15:10,250
the positive because i support points that are lying outside if i increase epsilon

1231
01:15:10,270 --> 01:15:14,330
i win something that will mean something proportional to the number of such points

1232
01:15:14,380 --> 01:15:18,610
and what precisely is instead divided by the number of points i would proportionally to

1233
01:15:18,610 --> 01:15:23,550
the fraction of such points by increase epsilon

1234
01:15:23,560 --> 01:15:28,070
i will win proportionally to the fraction of points that have a non zero positive

1235
01:15:29,060 --> 01:15:32,500
on the other hand if i decrease epsilon

1236
01:15:32,520 --> 01:15:35,350
what do i do i will

1237
01:15:35,370 --> 01:15:36,500
there was here

1238
01:15:36,520 --> 01:15:42,100
number news proportionally to the number of points that either outside or exactly on the

1239
01:15:42,910 --> 01:15:47,750
because if that decrease epsilon that's the point which was sitting on the edge before

1240
01:15:47,760 --> 01:15:51,380
it was exactly on the edge i really we already have to pay for that

1241
01:15:51,380 --> 01:15:54,650
point when i decrease epsilon

1242
01:15:54,670 --> 01:15:58,630
OK so that's the asymmetry here when i increase epsilon up pay proportional to the

1243
01:15:58,630 --> 01:16:04,680
fraction of points that have a positive side to the call point out outliers before

1244
01:16:04,690 --> 01:16:09,940
whereas if i decrease epsilon i pay proportionally to the point i have

1245
01:16:09,950 --> 01:16:11,940
outliers on the edge

1246
01:16:11,960 --> 01:16:17,040
in other words the fraction of points that are support vectors

1247
01:16:17,050 --> 01:16:21,750
so if you put these two things together you get exactly this problem well you

1248
01:16:21,750 --> 01:16:24,280
get the first two parts of this property

1249
01:16:24,300 --> 01:16:29,470
so the first part of this new is an upper bound on the fraction of

1250
01:16:30,470 --> 01:16:35,470
OK let me call errors here was copied for the question so points that lie

1251
01:16:35,470 --> 01:16:37,100
outside of the tube

1252
01:16:37,120 --> 01:16:42,700
new upper bounds this fraction of points same time new lower bonds the fraction of

1253
01:16:42,700 --> 01:16:44,570
support vectors

1254
01:16:44,590 --> 01:16:48,630
OK and remember the difference between these two sets where exactly the points that lie

1255
01:16:48,630 --> 01:16:50,780
on the edge of the two

1256
01:16:50,790 --> 01:16:56,400
in the classification case where the points that lie exactly on the margin plus minus

1257
01:16:56,400 --> 01:16:57,350
one lines

1258
01:16:57,360 --> 01:17:03,940
so we have sandwiched new in between the fraction of errors and the fraction of

1259
01:17:03,940 --> 01:17:07,420
ESPN's and we also know the fraction of its millions and the fraction of errors

1260
01:17:07,420 --> 01:17:10,330
are not two different they only differ by the points that lie exactly on the

1261
01:17:10,330 --> 01:17:16,800
edge of the tube and it turns out some fairly mild assumptions about the problem

1262
01:17:18,370 --> 01:17:23,180
he said what's the probability that the point lies exactly on the edge of the

1263
01:17:23,180 --> 01:17:25,530
two is zero

1264
01:17:25,550 --> 01:17:27,590
for asymptotically

1265
01:17:27,600 --> 01:17:29,670
these two fractions

1266
01:17:29,720 --> 01:17:33,460
the fraction of errors fraction of support vectors will be the same

1267
01:17:33,470 --> 01:17:38,950
and also they send which new in between each other for new will equal

1268
01:17:38,970 --> 01:17:42,950
the fraction of errors it will equal the fraction of support vectors asymptotically

1269
01:17:42,970 --> 01:17:47,710
so if you have a large training set you would expect that by selecting new

1270
01:17:47,710 --> 01:17:51,930
dr. Umberto Eco the floor is yours

1271
01:17:51,950 --> 01:18:07,800
good evening

1272
01:18:07,890 --> 01:18:09,620
as probably

1273
01:18:10,100 --> 01:18:15,620
you know thank you madam director

1274
01:18:15,750 --> 01:18:22,260
as probably you know I have selected these topic for my lecture because I have

1275
01:18:22,260 --> 01:18:29,740
just edit the book on history of ugliness after three or four years that we made

1276
01:18:29,740 --> 01:18:36,370
a history of of beauty so I since get some material visual material I thought that

1277
01:18:36,370 --> 01:18:37,300
it was

1278
01:18:37,370 --> 01:18:47,870
nice to entertain the audience with some funny examples in every century philosophers and

1279
01:18:47,870 --> 01:18:54,160
artists have supplied definitions of beauty and thank to their works it's possible to reconstruct the

1280
01:18:54,180 --> 01:19:01,350
history of aesthetic ideas over time but this didn't happen with ugliness

1281
01:19:01,710 --> 01:19:06,800
most of the time ugliness was defined as the opposite of beauty

1282
01:19:06,820 --> 01:19:13,870
but almost no one ever devoted the treaties of any length to ugliness

1283
01:19:13,900 --> 01:19:20,800
the first serious study on that subject was in eighteen fifthy three the aesthetics of ugliness

1284
01:19:20,800 --> 01:19:23,900
by Karl Rosenkrantz

1285
01:19:23,940 --> 01:19:25,980
but it was the only

1286
01:19:26,460 --> 01:19:32,040
consistant example hence why the history on beauty can draw in a wide range of theoretical

1287
01:19:32,040 --> 01:19:37,490
sources philosophers and so from which we can deduce that tastes of a given

1288
01:19:37,680 --> 01:19:44,400
epoche for the most part the history of ugliness must seek out its own documents individual or

1289
01:19:44,410 --> 01:19:51,160
verbal portrayals of things so people that were in some way seen as ugly

1290
01:19:51,190 --> 01:19:56,330
I must say that to assemble material for a history of uglines was more

1291
01:19:56,330 --> 01:20:03,240
funny than to make a history of beauty beauty is in some way boring

1292
01:20:03,240 --> 01:20:10,000
even if its concept changes to the ages nevertheless a beautiful object must always follow certain

1293
01:20:10,000 --> 01:20:14,890
rules so to speak beautiful nose shouldn't be

1294
01:20:14,930 --> 01:20:18,900
longer than that or shorter than that

1295
01:20:19,000 --> 01:20:23,550
on the contrary ugly nose can be as long as the one of pinnochio

1296
01:20:23,550 --> 01:20:28,330
as big as the trunk of an elephant or like the big of an eagle and so

1297
01:20:28,330 --> 01:20:35,370
uglinessas is unpredictable and offers an infinite range of possibility let's say

1298
01:20:35,370 --> 01:20:42,740
beauty's finite ugliness is infinite like god nonetheless a

1299
01:20:42,740 --> 01:20:49,150
history of ugliness shares some common characteristics with the history of beauty first

1300
01:20:49,150 --> 01:20:54,740
for the past centuries the only documents we have are artworks

1301
01:20:54,780 --> 01:21:00,650
we can only assume that the tastes of ordinary people corresponded in some way

1302
01:21:00,650 --> 01:21:04,780
with the tastes of the artists of their day but we are

1303
01:21:04,780 --> 01:21:05,770
not sure

1304
01:21:06,150 --> 01:21:11,300
visitor from an alien from the space outer space

1305
01:21:11,360 --> 01:21:18,020
went to a gallery of contemporary art and so that woman and

1306
01:21:18,020 --> 01:21:24,170
he are describing this painting as beautiful he or she or it might

1307
01:21:24,170 --> 01:21:28,960
guess the mistaken idea that in everyday life the man of our time find a female

1308
01:21:28,970 --> 01:21:36,390
creatures with faces like that beautiful and desirable but our visitor from space might

1309
01:21:36,390 --> 01:21:43,490
modify his opinion her opinion its opinion on watching facial in which you would witness

1310
01:21:44,170 --> 01:21:50,300
the celebration of other models of beauty second both for beauty

1311
01:21:50,300 --> 01:21:56,000
and ugliness we can only speak of the Western culture for an exotic culture

1312
01:21:56,000 --> 01:22:02,420
we usually don't have the theoretical texts to tell us if that

1313
01:22:02,420 --> 01:22:08,900
if that Africa mask was intended to cause aesthetic delight holy fear hilarity

1314
01:22:08,910 --> 01:22:15,190
laugh we don't know conversely believers believers in

1315
01:22:15,190 --> 01:22:22,540
some non-European religion might be disgusted by an image of Christ scarred bleeding

1316
01:22:22,560 --> 01:22:31,780
and humiliated why this operated corporal ugliness arouses feeling of symphaty and tenderness in a Christian

1317
01:22:31,780 --> 01:22:38,480
in our civilization we don't find this ugly so concepts of beauty and ugliness are relative

1318
01:22:38,480 --> 01:22:45,250
to the various historical periods of various cultures and to quote Voltaire ask a toad

1319
01:22:45,250 --> 01:22:46,780
what beauty is

1320
01:22:46,830 --> 01:22:52,090
true beauty the tocalom and we tell you that it consists of his

1321
01:22:52,090 --> 01:22:59,890
mate with her two fine round eyes protruding from her small head her broad

1322
01:22:59,890 --> 01:23:06,840
flat throat her yellow belly and brown back and ask a devil he will tell you

1323
01:23:06,840 --> 01:23:14,960
that beauty is a pair of horns four claws and a tail attributions of beauty or ugliness

1324
01:23:15,290 --> 01:23:16,830
are often viewed

1325
01:23:16,860 --> 01:23:25,640
not to aesthetic but to social political criteria marx pointed out how the possession of

1326
01:23:25,640 --> 01:23:29,450
money may compensate for ugliness

1327
01:23:29,480 --> 01:23:35,570
he said I am ugly but I can buy myself the most beautiful women hence

1328
01:23:35,730 --> 01:23:36,710
I am not ugly

1329
01:23:36,750 --> 01:23:42,280
I am lame but money gives me twentyfour legs

1330
01:23:42,310 --> 01:23:46,250
now if we extend this observation on money to power in

1331
01:23:46,250 --> 01:23:52,780
general we can understand why many portrait of the

1332
01:23:52,960 --> 01:24:00,960
monarchs kings of centuries past were no doubt very very very ugly

1333
01:24:00,960 --> 01:24:06,020
but their own impotence lend them such a charisma and glamour that they

1334
01:24:06,070 --> 01:24:17,100
subjects saw them through adoring eyes let's red let's read

1335
01:24:17,220 --> 01:24:24,640
fredric brown sentinel fredric brown's sentinel written in the fifties one of the finest short stories produced

1336
01:24:24,640 --> 01:24:27,390
by contemporary science fiction

1337
01:24:27,420 --> 01:24:29,650
I cannot but to say

1338
01:24:29,690 --> 01:24:35,710
he was soaked to the skin and up to the eyes in mud

1339
01:24:35,710 --> 01:24:38,070
and he was hungry and cold

1340
01:24:38,100 --> 01:24:42,570
and was fifty thousand light-years far from home

1341
01:24:42,600 --> 01:24:49,460
a foreign sun emitted an icy bluish light and the gravity double

1342
01:24:49,510 --> 01:24:51,270
what he was used to

1343
01:24:51,310 --> 01:24:55,920
made the slightest movement weary and painful

1344
01:24:55,960 --> 01:24:56,720
the enemy

1345
01:24:57,400 --> 01:25:01,150
the only other intelligent race in the galaxy

1346
01:25:01,150 --> 01:25:03,540
thought carefully about how to do it at the top

1347
01:25:03,670 --> 01:25:07,670
the concrete is less dense and bottom and has a different combination of aggregate so

1348
01:25:07,670 --> 01:25:08,460
that it

1349
01:25:08,570 --> 01:25:11,270
will stay up for thousands of years

1350
01:25:11,320 --> 01:25:15,550
so if you're going to build if you're going to try and make materials like

1351
01:25:15,550 --> 01:25:21,360
concrete or like a metal well if you welding something together that contains many different

1352
01:25:22,460 --> 01:25:27,360
for particular application you need to know how to combine these things

1353
01:25:28,320 --> 01:25:32,970
both of these applications have been places where people in machine learning have actually

1354
01:25:33,000 --> 01:25:38,360
made useful progress despite the thousands of years of experience in materials these materials are

1355
01:25:38,360 --> 01:25:41,360
very well understood if you want to predict

1356
01:25:41,970 --> 01:25:46,100
how well how strong the concrete will be how flexible will be

1357
01:25:46,150 --> 01:25:48,350
how much it will slump over time

1358
01:25:48,360 --> 01:25:51,680
that's actually very hard to do from just looking at the list of components is

1359
01:25:51,680 --> 01:25:55,860
very difficult to say i understand how concrete works i can build the model of

1360
01:25:55,860 --> 01:25:58,160
exactly how these materials or combined

1361
01:25:58,210 --> 01:25:59,870
instead what you want to do

1362
01:25:59,880 --> 01:26:04,230
is learned from experience you try out tons of different concrete and then you want

1363
01:26:04,230 --> 01:26:05,420
to be able to

1364
01:26:05,470 --> 01:26:09,270
generalize from that data how concrete is going to work in the future i'm sure

1365
01:26:09,270 --> 01:26:12,940
they don't just go out and make the pantheon i'm sure they actually sort of

1366
01:26:12,960 --> 01:26:17,880
i had made a bunch of small things country probably fell down first before they

1367
01:26:17,880 --> 01:26:21,160
went and so what we want to do is how can we use computers to

1368
01:26:21,160 --> 01:26:25,790
sort of speed up the learning process and take a load off of humans

1369
01:26:26,680 --> 01:26:35,310
another application which you can get on your PC these days is speech recognition software

1370
01:26:35,320 --> 01:26:41,540
services spectrogram representation of a waveform and this is an image of the word nineteenth

1371
01:26:44,060 --> 01:26:47,620
i'm told this if you work in speech recognition for long enough

1372
01:26:47,630 --> 01:26:50,980
there are people that can look at this image and for real tell you that

1373
01:26:50,980 --> 01:26:52,850
it's the nineteenth century

1374
01:26:52,860 --> 01:26:58,310
so in some ways this is sort of being reduced to an image recognition problem

1375
01:26:58,340 --> 01:27:01,110
there's a lot more going on you have to build models of language as well

1376
01:27:01,110 --> 01:27:02,690
if you want this thing to work well

1377
01:27:02,720 --> 01:27:06,040
but and this is a common story in machine learning that you want to look

1378
01:27:06,040 --> 01:27:11,050
at problems in different ways maybe reduce problems the things you've seen before

1379
01:27:13,570 --> 01:27:18,050
finally this is something i don't think works very well yet but

1380
01:27:18,060 --> 01:27:20,670
it's something that is being actively worked on machine learning

1381
01:27:20,680 --> 01:27:26,130
if i go to amazon dot com amazon kqk it says welcome

1382
01:27:26,150 --> 01:27:31,730
in memory i have recommendations for you and i look at the end

1383
01:27:31,730 --> 01:27:35,730
i don't like very much actually like i did this this morning and

1384
01:27:35,790 --> 01:27:39,350
this is a dieting book and

1385
01:27:39,400 --> 01:27:40,360
i mean

1386
01:27:40,370 --> 01:27:42,820
i might be posting but i don't think i need to go on a diet

1387
01:27:42,820 --> 01:27:47,090
and often when i look through the whole series of recommendations one is going on

1388
01:27:47,090 --> 01:27:53,270
here but there is actually pretty sophisticated recommendation system going on in the background here

1389
01:27:53,290 --> 01:27:57,110
and the question we could ask is how can we make these things better and

1390
01:27:57,110 --> 01:28:02,410
it's a question that netflix which is an american movie rental company asked and cared

1391
01:28:02,410 --> 01:28:05,900
enough about the answer that they put a million dollar prize on people who could

1392
01:28:05,900 --> 01:28:12,620
improve recommendation systems that is an important problem and again you don't want to be

1393
01:28:12,620 --> 01:28:14,530
building rules by hand so

1394
01:28:14,540 --> 01:28:18,570
i'm very weird maybe and it's hard for me and to understand what i want

1395
01:28:19,500 --> 01:28:23,850
amazon has tens of millions of customers' money and many of them are going to

1396
01:28:23,850 --> 01:28:27,790
be pretty weird and you can't employ enough stuff to go through and fix up

1397
01:28:27,790 --> 01:28:31,970
the their recommendations by hand you need to look at this vast messy complicated data

1398
01:28:31,970 --> 01:28:36,050
set and somehow pull out what you need to know using relatively simple systems but

1399
01:28:36,050 --> 01:28:41,210
the web server can run in real time

1400
01:28:42,060 --> 01:28:46,370
so where are we going to go this afternoon and i'm going to start out

1401
01:28:46,370 --> 01:28:51,010
by talking about something which is much less sort of sexy and exciting and the

1402
01:28:51,010 --> 01:28:52,790
applications are put up because

1403
01:28:52,800 --> 01:28:54,570
the complicated say

1404
01:28:54,590 --> 01:28:58,180
i'm going to start talking about binary classification so

1405
01:28:58,260 --> 01:29:02,560
by an example of the binary classification problem is his an image does it contain

1406
01:29:02,560 --> 01:29:05,900
the face or not and you have to a given an input you have to

1407
01:29:05,900 --> 01:29:12,070
specify minus one or one no image no face of face

1408
01:29:14,220 --> 01:29:18,290
the idea of binary classifiers work also machine learning is used all over the place

1409
01:29:18,290 --> 01:29:22,260
and is part of more complicated systems so i'm going to focus on that for

1410
01:29:22,260 --> 01:29:24,620
an embarrassingly long time so we

1411
01:29:24,630 --> 01:29:27,460
you can sort of see what a lot of the issues of machine learning all

1412
01:29:27,470 --> 01:29:29,220
with this one simple example

1413
01:29:29,270 --> 01:29:32,950
so i've tried on that island move on to other stuff

1414
01:29:34,240 --> 01:29:36,250
backing off further

1415
01:29:36,290 --> 01:29:39,440
an image of the face of the pretty complicated thing as well so the is

1416
01:29:39,440 --> 01:29:43,900
a binary classification problem i'm going to talk about is distinguishing oranges from london so

1417
01:29:43,900 --> 01:29:48,090
this is going to be very toy little example and you know i'm going to

1418
01:29:48,090 --> 01:29:51,440
try and build up the more complicated settings later

1419
01:29:51,450 --> 01:29:52,880
OK so i need to

1420
01:29:52,930 --> 01:29:57,950
i one for some reason to build a computer system that given in u

1421
01:29:57,960 --> 01:30:01,350
item three will be able to say this is an hour endurance element

1422
01:30:01,370 --> 01:30:04,920
and in order to do that i'm going to have to somehow put the oranges

1423
01:30:04,920 --> 01:30:09,600
and lemons inside my computer it's going to have some kind of what they are

1424
01:30:09,620 --> 01:30:13,900
maybe i could take a photo of them but i want this to be a

1425
01:30:13,900 --> 01:30:18,690
simple example and dealing with color is actually a complete pain so this is something

1426
01:30:18,690 --> 01:30:21,890
we might note is just with this project is green with the colours washed out

1427
01:30:21,890 --> 01:30:23,140
issue of

1428
01:30:23,140 --> 01:30:24,490
the canonical

1429
01:30:24,520 --> 01:30:28,200
lexicon is one is still one of the major problems

1430
01:30:28,220 --> 01:30:33,950
OK and just as just to point out we can really think of this process

1431
01:30:33,950 --> 01:30:36,330
of lattice rescoring as

1432
01:30:36,580 --> 01:30:46,200
something similar to this analysis by synthesis model that stevens posed as part of the

1433
01:30:46,240 --> 01:30:51,350
human speech decoding problem that when you get when the knee when you

1434
01:30:51,390 --> 01:30:54,810
get a hypothesized word sequences you sort of

1435
01:30:54,830 --> 01:31:01,480
risk or that using additional knowledge you might have against the actual measurements that you've

1436
01:31:01,480 --> 01:31:02,390
sort of

1437
01:31:02,390 --> 01:31:07,180
that that you've derived and and so there is that it's not that unreasonable to

1438
01:31:07,180 --> 01:31:10,990
think of this of the problem of a sort of generating

1439
01:31:12,350 --> 01:31:16,740
of generating hypotheses and using to verify

1440
01:31:17,330 --> 01:31:22,990
something that you have you know the hypothesis that you obtain from a basic baseline

1441
01:31:22,990 --> 01:31:25,470
ASR system that's not that far

1442
01:31:25,560 --> 01:31:30,290
from how we might think of humans humans working

1443
01:31:31,040 --> 01:31:33,310
any questions on this implementation

1444
01:31:35,600 --> 01:31:50,370
right is if you're doing perhaps words body without

1445
01:32:06,540 --> 01:32:08,490
well we with the

1446
01:32:10,160 --> 01:32:17,560
you're right though this actually to do this the switchboard and and

1447
01:32:17,600 --> 01:32:21,010
i suppose the biggest issue was which sport i quoted

1448
01:32:21,040 --> 01:32:25,950
that results towards the beginning where it turns out that only about two-thirds

1449
01:32:26,010 --> 01:32:28,010
of the of the

1450
01:32:28,010 --> 01:32:33,510
phones that occur in switchboard utterances agree with what's in the canonical dictionary

1451
01:32:33,520 --> 01:32:34,540
and so

1452
01:32:35,580 --> 01:32:37,870
and so i think this

1453
01:32:41,040 --> 01:32:47,120
their canonical dictionary is still the big problem here is is probably that's probably reasonable

1454
01:32:47,120 --> 01:32:51,280
no matter what if it is but but i think your point which is a

1455
01:32:51,280 --> 01:32:53,160
good one is

1456
01:32:53,850 --> 01:32:56,640
i guess that

1457
01:32:56,660 --> 01:33:02,810
we as humans are bringing so much context to bear in conversation decoding conversational speech

1458
01:33:02,830 --> 01:33:07,790
utterance that's the acoustics are necessarily going to be weak

1459
01:33:07,810 --> 01:33:11,100
i think in and

1460
01:33:11,140 --> 01:33:12,140
i guess

1461
01:33:12,200 --> 01:33:17,200
that is well quoting that result with the the lexicon being such a bad match

1462
01:33:17,200 --> 01:33:19,330
i mean that certain that

1463
01:33:19,390 --> 01:33:21,950
verifies your

1464
01:33:22,010 --> 01:33:23,910
your claim that there

1465
01:33:25,450 --> 01:33:28,160
i'm trying to think of what experiments people have done

1466
01:33:29,140 --> 01:33:31,930
switchboard in particular

1467
01:33:31,950 --> 01:33:37,330
i mean there's there's some good examples where they've asked people

1468
01:33:37,330 --> 01:33:42,100
to take switchboard utterances i think they did this at know maybe the successor i'm

1469
01:33:42,220 --> 01:33:43,370
trying to do this

1470
01:33:45,080 --> 01:33:47,990
when we speak we're not exactly careful

1471
01:33:48,010 --> 01:33:50,700
articulators to reach their targets and we

1472
01:33:50,760 --> 01:33:52,260
speed up and we

1473
01:33:52,290 --> 01:33:57,810
hesitations and filled pauses and all that and what they ask people to do was

1474
01:33:57,810 --> 01:34:02,540
take these utterances and sit sit down and read them carefully

1475
01:34:02,540 --> 01:34:03,330
and then

1476
01:34:03,350 --> 01:34:07,640
o the rerun recognition and there was a dramatic

1477
01:34:07,660 --> 01:34:09,290
increase in performance so

1478
01:34:09,470 --> 01:34:15,120
just just dealing with the issues of spontaneous removing the issues spontaneous speech

1479
01:34:15,120 --> 01:34:17,080
it was huge

1480
01:34:17,100 --> 01:34:22,410
and it when switch for first created it was started its telephone thing but that

1481
01:34:22,410 --> 01:34:26,290
actually is not not the problem at all

1482
01:34:26,290 --> 01:34:28,640
OK so good point thank you for that

1483
01:34:31,850 --> 01:34:32,640
all right

1484
01:34:32,660 --> 01:34:37,200
the holy grail so i'm a

1485
01:34:37,260 --> 01:34:41,220
this is some of the most interesting stuff that just doesn't seem to quite work

1486
01:34:41,240 --> 01:34:46,390
if i had a lot of interesting stuff that doesn't quite work way back twenty

1487
01:34:46,390 --> 01:34:48,140
years ago

1488
01:34:48,160 --> 01:34:50,040
the robotics

1489
01:34:50,330 --> 01:34:55,830
what became sort of folk hero he worked at IBM and IBM was always

1490
01:34:57,040 --> 01:35:03,410
build systems that will big systems in and he was sort of a an outlier

1491
01:35:03,410 --> 01:35:05,740
has since retired but

1492
01:35:05,790 --> 01:35:10,410
he he was working on this model we're basically it's not really a model for

1493
01:35:10,410 --> 01:35:15,680
speech recognition is is someone can we can see that it can be incorporated model

1494
01:35:15,680 --> 01:35:16,830
for recognition

1495
01:35:16,850 --> 01:35:19,580
but basically it's it's

1496
01:35:19,580 --> 01:35:21,560
how can we model

1497
01:35:22,520 --> 01:35:24,520
speech articulation here

1498
01:35:24,540 --> 01:35:30,220
basically by starting with the notion of there being sort of a set of targets

1499
01:35:30,220 --> 01:35:34,410
articulatory positions so we'll start will assume that we start with

1500
01:35:34,450 --> 01:35:36,890
a message which is

1501
01:35:37,700 --> 01:35:42,060
transcribed according to a sequence of phonemes

1502
01:35:42,080 --> 01:35:45,100
we can take for each one of those phonemes

1503
01:35:45,140 --> 01:35:46,120
we can

1504
01:35:46,140 --> 01:35:49,510
by one mechanism or another

1505
01:35:49,520 --> 01:35:53,120
to find the target articulatory position

1506
01:35:53,160 --> 01:35:55,540
for each articulator

1507
01:35:57,470 --> 01:35:58,680
i and

1508
01:35:58,790 --> 01:36:01,490
in this case strictly through some mapping

1509
01:36:02,290 --> 01:36:03,370
we can

1510
01:36:04,180 --> 01:36:05,660
through some subtle

1511
01:36:06,560 --> 01:36:08,430
articulatory dynamics

1512
01:36:08,450 --> 01:36:15,240
and and we can call these articulate coarticulation filters if you want to describe

1513
01:36:15,290 --> 01:36:19,700
the dynamics of the of the movement of each articulator

1514
01:36:20,350 --> 01:36:23,240
and presumably this model will

1515
01:36:23,290 --> 01:36:29,950
describe the situation where the articulated doesn't quite reaches trajectory if i'm speaking if if

1516
01:36:29,950 --> 01:36:32,580
the rate is too fast and so on

1517
01:36:33,490 --> 01:36:36,060
the last component of the model would be

1518
01:36:36,080 --> 01:36:40,870
mapping from these actual articulatory

1519
01:36:42,620 --> 01:36:44,100
to the actual

1520
01:36:44,240 --> 01:36:46,260
the actual acoustics

1521
01:36:46,290 --> 01:36:47,220
and so

1522
01:36:47,220 --> 01:36:54,140
and i've already said him to the number of people boxes worked on this and

1523
01:36:54,430 --> 01:36:57,990
a number of other people have tried to build models to sort of implement this

1524
01:36:57,990 --> 01:36:59,260
kind of thing

1525
01:37:02,310 --> 01:37:07,580
we can think of generally the attempts have been to think of this mapping is

1526
01:37:07,580 --> 01:37:10,280
just being some sort of

1527
01:37:10,290 --> 01:37:14,490
you know i could design a think of this is actually being

1528
01:37:14,560 --> 01:37:20,580
short-time stationary over periods of you know the usual for ten twenty millisecond that we

1529
01:37:20,580 --> 01:37:23,780
assume in speech and that's

1530
01:37:23,790 --> 01:37:26,700
we can sort of

1531
01:37:26,740 --> 01:37:28,370
i have some sort of static

1532
01:37:28,390 --> 01:37:34,370
mapping from defined by a set of these articulatory positions

1533
01:37:35,100 --> 01:37:38,600
acoustics perhaps formants perhaps so

1534
01:37:38,660 --> 01:37:45,640
perhaps some sort of spectral energy spectral envelope representation that we generally use

1535
01:37:45,680 --> 01:37:49,370
in which traditional ASR

1536
01:37:49,370 --> 01:37:51,240
more than

1537
01:37:51,340 --> 01:37:52,730
would so love

1538
01:37:53,270 --> 01:37:57,410
it's going

1539
01:38:00,170 --> 01:38:01,800
so this will be

1540
01:38:01,940 --> 01:38:04,140
a little more

1541
01:38:04,570 --> 01:38:06,570
hands on

1542
01:38:06,600 --> 01:38:08,420
technical about how

1543
01:38:08,460 --> 01:38:12,170
these actually

1544
01:38:12,370 --> 01:38:15,700
it will be a little bit

1545
01:38:20,290 --> 01:38:23,770
idealisation of more nitty-gritty about how how

1546
01:38:23,830 --> 01:38:27,650
sec actually works and

1547
01:38:27,730 --> 01:38:31,330
and it's just sort of a hodgepodge of

1548
01:38:31,340 --> 01:38:33,050
topics i find

1549
01:38:33,080 --> 01:38:37,250
if something related to the cyclic compounds it

1550
01:38:37,410 --> 01:38:38,250
there's no

1551
01:38:38,300 --> 01:38:42,520
the main point that i'm trying to drive out here and you're free to ask

1552
01:38:42,520 --> 01:38:45,390
questions about all alternative

1553
01:38:46,220 --> 01:38:48,890
connect to the

1554
01:38:48,940 --> 01:38:53,410
KB and browse around a little bit so you can get a feeling for

1555
01:38:56,060 --> 01:38:59,670
what's actually in there and what it's doing OK

1556
01:39:01,090 --> 01:39:08,700
the overview i'm going to start and talk about natural language microtheories very briefly that's

1557
01:39:10,140 --> 01:39:11,950
that just gives consider this

1558
01:39:11,950 --> 01:39:14,440
the very very broadest structure for

1559
01:39:16,780 --> 01:39:21,090
natural language stuff is encoded in the KB and then i'm going to talk about

1560
01:39:21,110 --> 01:39:22,410
kinds of

1561
01:39:22,440 --> 01:39:27,060
semantic predicates by that i mean ways of relating say concepts two

1562
01:39:28,450 --> 01:39:34,220
and then i'm going to talk about inflectional and derivational morphology in cyc and the

1563
01:39:34,300 --> 01:39:36,500
the teams that will

1564
01:39:36,560 --> 01:39:38,780
curve eight

1565
01:39:38,810 --> 01:39:41,090
all three of these topics

1566
01:39:42,450 --> 01:39:46,190
a practical theme how to modify and add to the cyc lexicon

1567
01:39:48,220 --> 01:39:54,320
it's say want to build a slovene microsecond

1568
01:39:55,940 --> 01:40:01,940
i'm going to emphasise the generative nature of the lexicon and by that i mean

1569
01:40:01,990 --> 01:40:04,600
the possibility for deriving new

1570
01:40:04,620 --> 01:40:07,270
lexical entries from

1571
01:40:07,310 --> 01:40:08,590
i started once

1572
01:40:09,810 --> 01:40:11,910
inference with sec

1573
01:40:14,970 --> 01:40:18,630
one of the things they are the first thing that you have to know in

1574
01:40:18,630 --> 01:40:24,850
order to make a lexical insertion in cyc is which microtheory to put it in

1575
01:40:24,870 --> 01:40:28,530
this does anybody know what makes

1576
01:40:28,690 --> 01:40:33,620
OK i want to briefly explain so microtheory

1577
01:40:34,560 --> 01:40:43,540
sort of it's a way of limiting the context in which an assertion holds so

1578
01:40:44,000 --> 01:40:47,500
they can be tapped temporal microtheories so today

1579
01:40:47,600 --> 01:40:51,310
barack obama is the president of the united states

1580
01:40:53,560 --> 01:40:56,020
four years ago that was not the case

1581
01:40:56,030 --> 01:40:57,880
in the micro theory

1582
01:40:58,840 --> 01:41:01,840
including the present time

1583
01:41:01,870 --> 01:41:04,000
that assertion holds but

1584
01:41:04,000 --> 01:41:08,970
that doesn't assert assertion doesn't hold in previous metric is in america

1585
01:41:09,120 --> 01:41:11,570
you drive on the right side of the road

1586
01:41:11,590 --> 01:41:15,910
in new zealand you drive on the left side of the road

1587
01:41:15,930 --> 01:41:18,400
assertions can also be relative eyes

1588
01:41:19,990 --> 01:41:21,590
spatial things

1589
01:41:21,620 --> 01:41:26,320
so we have some microtheories give you

1590
01:41:26,710 --> 01:41:31,220
so the limit the contexts in which an assertion holds

1591
01:41:33,150 --> 01:41:37,160
we use microtheories to organize the lexical knowledge and cyc so

1592
01:41:37,360 --> 01:41:44,410
things that are true of english are asserted in english related microtheories and things that

1593
01:41:44,410 --> 01:41:46,090
are true

1594
01:41:46,090 --> 01:41:50,030
polish are sorted in polish really make is in principle

1595
01:41:50,280 --> 01:41:52,020
and then

1596
01:41:52,030 --> 01:41:55,030
within english there are different dialects others

1597
01:41:55,030 --> 01:41:58,110
commonwealth english which includes british new zealand

1598
01:41:58,200 --> 01:41:59,880
australian english

1599
01:41:59,890 --> 01:42:02,650
and the american english

1600
01:42:02,700 --> 01:42:06,380
and so if you have something specific to american english then you want to say

1601
01:42:06,470 --> 01:42:08,380
that in the american english

1602
01:42:09,720 --> 01:42:14,530
so right favor

1603
01:42:14,540 --> 01:42:16,840
it's spelled in british english with the

1604
01:42:17,630 --> 01:42:20,450
not american english so

1605
01:42:20,460 --> 01:42:22,780
in british english MT

1606
01:42:22,830 --> 01:42:28,300
the infinitive form of favour theword favour theword isa constant

1607
01:42:29,520 --> 01:42:30,550
in the KB

1608
01:42:37,200 --> 01:42:43,180
all of these entities but it's infinitive form is listed as

1609
01:42:43,220 --> 01:42:48,780
if a b o you are in the british and he and fabio are indeed

1610
01:42:48,890 --> 01:42:54,220
well it's generally two that's probably because someone was lazy normally we just assert everything

1611
01:42:54,220 --> 01:42:55,470
in the

1612
01:42:55,470 --> 01:42:59,440
that is quite distinct from the background nevertheless

1613
01:42:59,500 --> 01:43:01,990
to do it in time using

1614
01:43:02,280 --> 01:43:04,910
that this is pretty

1615
01:43:09,300 --> 01:43:12,180
see in the slide i'm going to just

1616
01:43:13,850 --> 01:43:15,390
the mathematics law

1617
01:43:19,650 --> 01:43:21,560
what the condition is

1618
01:43:21,610 --> 01:43:23,220
it is required for

1619
01:43:23,290 --> 01:43:26,070
simulated annealing to work

1620
01:43:26,150 --> 01:43:32,920
it's very simple really

1621
01:43:38,480 --> 01:43:45,650
goddess lies somewhere in the right place wrong find just before so the idea is

1622
01:43:45,690 --> 01:43:49,330
we've got a cost function

1623
01:43:49,410 --> 01:43:51,650
i jv two nodes

1624
01:43:53,120 --> 01:43:54,110
x i

1625
01:43:54,190 --> 01:43:56,240
xj that's my

1626
01:43:57,650 --> 01:44:01,820
my two to cost function corresponding cliques of size two

1627
01:44:02,650 --> 01:44:04,000
i'm going to do

1628
01:44:04,010 --> 01:44:08,320
so these ex i j

1629
01:44:09,220 --> 01:44:13,160
my label set

1630
01:44:13,890 --> 01:44:17,220
i'm reduces to a binary problem

1631
01:44:18,740 --> 01:44:20,500
i do alpha expansion

1632
01:44:20,510 --> 01:44:21,360
and so

1633
01:44:21,360 --> 01:44:22,930
there will be

1634
01:44:24,050 --> 01:44:27,910
associated with the move which are called the alpha j

1635
01:44:27,920 --> 01:44:29,400
and there

1636
01:44:29,440 --> 01:44:30,740
i have

1637
01:44:30,790 --> 01:44:33,230
two variables

1638
01:44:34,290 --> 01:44:35,810
u i

1639
01:44:35,820 --> 01:44:37,020
u j

1640
01:44:37,050 --> 01:44:39,410
are in

1641
01:44:39,450 --> 01:44:40,600
zero one

1642
01:44:40,600 --> 01:44:42,830
so i'm going to do

1643
01:44:42,880 --> 01:44:48,250
a binary problem where is that each node has the options switching or not

1644
01:44:48,310 --> 01:44:49,690
and the question is

1645
01:44:49,710 --> 01:44:51,570
what is the cost

1646
01:44:51,600 --> 01:44:53,990
right so just look at this look at

1647
01:44:54,090 --> 01:44:56,400
now my rule is

1648
01:44:58,000 --> 01:45:01,650
URI equals zero

1649
01:45:01,730 --> 01:45:05,640
pixel remains

1650
01:45:05,680 --> 01:45:08,130
the same

1651
01:45:08,170 --> 01:45:10,150
if you i

1652
01:45:10,160 --> 01:45:12,600
because what

1653
01:45:13,940 --> 01:45:16,210
goes to o

1654
01:45:18,560 --> 01:45:22,910
that's that's the basis whatsoever nearly the same thing sorry

1655
01:45:22,920 --> 01:45:26,340
after expressions about knows look at

1656
01:45:26,340 --> 01:45:28,400
let's look at two notes here

1657
01:45:30,520 --> 01:45:32,360
and j

1658
01:45:32,460 --> 01:45:36,070
and supposing they have values x i

1659
01:45:36,070 --> 01:45:37,650
and xj

1660
01:45:37,660 --> 01:45:40,670
at present

1661
01:45:40,680 --> 01:45:43,410
and they're going to switch to the new value x prob

1662
01:45:43,410 --> 01:45:44,670
which new label

1663
01:45:44,680 --> 01:45:48,320
the label may be the same or maybe alps so what happens in the four

1664
01:45:50,060 --> 01:45:51,650
if you want to the cost

1665
01:45:53,080 --> 01:45:56,640
zero zero in other words you URI being zero and u j

1666
01:45:56,650 --> 01:45:59,630
being zero well that means

1667
01:46:01,140 --> 01:46:02,860
they remain what they were

1668
01:46:04,840 --> 01:46:06,290
that will be

1669
01:46:08,250 --> 01:46:10,310
the cost of these being zero

1670
01:46:10,320 --> 01:46:17,550
will be equal to my original cost of x i x j

1671
01:46:17,560 --> 01:46:19,400
if if my

1672
01:46:19,410 --> 01:46:20,860
w zero

1673
01:46:20,860 --> 01:46:23,830
there remains same so that cost

1674
01:46:27,180 --> 01:46:28,900
one zero then

1675
01:46:28,910 --> 01:46:30,580
one becomes alpha

1676
01:46:30,590 --> 01:46:34,400
the other remains xj so this is a

1677
01:46:38,090 --> 01:46:39,870
and the

1678
01:46:39,880 --> 01:46:41,910
of zero one

1679
01:46:41,960 --> 01:46:43,580
well that will be

1680
01:46:47,290 --> 01:46:49,020
x y

1681
01:46:49,080 --> 01:46:54,910
alpha because zero mean remains and finally the

1682
01:46:54,920 --> 01:46:56,440
one one

1683
01:46:56,540 --> 01:46:59,140
it means both pixels switch to have

1684
01:46:59,190 --> 01:47:03,210
so the cost of that is called the

1685
01:47:03,250 --> 01:47:05,870
alpha alpha

1686
01:47:05,940 --> 01:47:08,190
from these

1687
01:47:08,810 --> 01:47:10,790
what is my condition for

1688
01:47:10,890 --> 01:47:13,890
the thing to be solved my submodularity condition

1689
01:47:15,920 --> 01:47:17,710
zero zero loss

1690
01:47:18,620 --> 01:47:22,230
one one necessary equals zero

1691
01:47:23,100 --> 01:47:25,390
one right

1692
01:47:25,440 --> 01:47:27,230
substituting in

1693
01:47:27,270 --> 01:47:29,390
you get this means

1694
01:47:31,060 --> 01:47:33,560
this means that e

1695
01:47:33,620 --> 01:47:35,150
x i

1696
01:47:36,390 --> 01:47:37,940
must be

1697
01:47:38,270 --> 01:47:40,890
alpha has been this article to

1698
01:47:43,060 --> 01:47:45,440
x y

1699
01:47:46,620 --> 01:47:48,190
class b

1700
01:47:51,210 --> 01:47:52,920
which is exactly the condition

1701
01:47:53,000 --> 01:47:55,210
i stated on the slide

1702
01:47:55,210 --> 01:47:59,520
if you make that one e alpha alpha zero cost of having the same label

1703
01:47:59,540 --> 01:48:01,600
zero which is always

1704
01:48:01,640 --> 01:48:03,560
arranged to have

1705
01:48:03,560 --> 01:48:06,850
then this just becomes the same triangle inequality

1706
01:48:10,040 --> 01:48:12,540
one of the things about the triangle inequality

1707
01:48:13,370 --> 01:48:18,120
if you make the same sort of assumption as before

1708
01:48:18,170 --> 01:48:20,910
that he

1709
01:48:20,910 --> 01:48:22,810
x i x j

1710
01:48:22,830 --> 01:48:24,920
some function g

1711
01:48:24,960 --> 01:48:27,170
x i

1712
01:48:27,190 --> 01:48:28,830
minus xj

1713
01:48:28,830 --> 01:48:32,640
one of the rules called

1714
01:48:32,670 --> 01:48:33,740
you probably

1715
01:48:33,780 --> 01:48:39,960
distribution of and distributed usually

1716
01:48:40,010 --> 01:48:41,960
which rules

1717
01:48:42,090 --> 01:48:46,660
in you can drive distribution

1718
01:48:48,890 --> 01:48:50,810
the distribution y

1719
01:48:52,750 --> 01:48:54,640
because it's

1720
01:48:54,730 --> 01:48:56,920
so kind of into it

1721
01:48:59,690 --> 01:49:05,310
we have this in a situation where you have that

1722
01:49:05,320 --> 01:49:07,520
we have a and

1723
01:49:07,540 --> 01:49:13,030
least b or c thank you have either a and b or in c

1724
01:49:13,110 --> 01:49:15,890
make sense

1725
01:49:15,910 --> 01:49:19,300
so what we normally do is just add this in is another rule

1726
01:49:19,340 --> 01:49:21,480
that's what interests and l

1727
01:49:21,520 --> 01:49:25,120
that's an ugly thing to do but they just added to the rule

1728
01:49:25,170 --> 01:49:30,040
with the subscript is means to say

1729
01:49:30,120 --> 01:49:31,380
it's very

1730
01:49:31,390 --> 01:49:34,900
it block something called the normalized it in the UK is going to be cut

1731
01:49:34,900 --> 01:49:37,460
elimination from john slaney at least

1732
01:49:37,500 --> 01:49:41,960
wade NZ right he told me that i mentioned talk about for about ten minutes

1733
01:49:42,350 --> 01:49:49,370
OK the normalisation pretty much like cut elimination it's one of these

1734
01:49:49,400 --> 01:49:52,170
what it tells you

1735
01:49:52,240 --> 01:49:56,820
i normalisation is that you never really you never have to derive

1736
01:49:59,000 --> 01:50:00,740
in fact the same formula

1737
01:50:00,800 --> 01:50:06,260
twice over again in this kind of gives you a feel for it makes a

1738
01:50:06,260 --> 01:50:08,480
proof of decidability

1739
01:50:08,500 --> 01:50:13,800
full relevant logic with negation and everything else is not decidable

1740
01:50:13,810 --> 01:50:19,120
with distribution is not decidable either so makes the proof is possible

1741
01:50:20,830 --> 01:50:26,320
weaker systems of relevant to our decidable so they

1742
01:50:26,970 --> 01:50:30,830
it's nice to have to have proof that

1743
01:50:30,860 --> 01:50:36,090
this proof theoretic techniques as well as other sorts approves to to do that and

1744
01:50:36,110 --> 01:50:42,320
want to improve the the decidability results also came up with a away way

1745
01:50:42,320 --> 01:50:45,500
of treating disjunction that's a bit different than

1746
01:50:45,530 --> 01:50:48,980
and it's very intuitive

1747
01:50:51,570 --> 01:50:53,370
and it's

1748
01:50:54,080 --> 01:50:57,500
i think very elegant

1749
01:50:57,540 --> 01:51:02,570
show that he was not you know it's unfortunate the put as writing about writing

1750
01:51:02,570 --> 01:51:07,330
has also told that forty pages was about limit of thirty eight total

1751
01:51:07,390 --> 01:51:11,820
so the the thirty seven so i e there was a whole bunch of stuff

1752
01:51:11,890 --> 01:51:14,530
and also i don't think i'd be able to have time to include it with

1753
01:51:14,680 --> 01:51:15,770
the clear that idea

1754
01:51:16,560 --> 01:51:21,220
OK now

1755
01:51:21,230 --> 01:51:22,730
when we do this

1756
01:51:22,780 --> 01:51:23,820
what sort of

1757
01:51:23,830 --> 01:51:24,670
just to

1758
01:51:24,740 --> 01:51:30,260
forget about the set from its impact on what i mentioned this already

1759
01:51:31,740 --> 01:51:32,930
sort of

1760
01:51:32,960 --> 01:51:36,260
natural mathematical inference procedure you

1761
01:51:36,270 --> 01:51:37,960
representing here

1762
01:51:39,380 --> 01:51:43,890
destruction and anyone pull apart what's it called

1763
01:51:43,900 --> 01:51:47,290
and doing mathematics

1764
01:51:47,330 --> 01:51:49,320
the that through proof by

1765
01:51:55,480 --> 01:52:00,320
that's effectively what we're doing but we're doing something that in the relevant case that

1766
01:52:00,320 --> 01:52:03,210
seems a bit like overkill

1767
01:52:04,750 --> 01:52:08,580
we're proving that

1768
01:52:08,580 --> 01:52:12,850
if you have a or b and you have a implies c and b implies

1769
01:52:12,850 --> 01:52:14,590
c that there's these

1770
01:52:14,670 --> 01:52:16,160
errors there

1771
01:52:18,330 --> 01:52:21,880
even get c

1772
01:52:22,000 --> 01:52:28,010
and implications i said in relevant logic is quite strong

1773
01:52:28,060 --> 01:52:30,870
and that's it

1774
01:52:30,890 --> 01:52:34,580
it requires a fair bit as i told you that

1775
01:52:34,580 --> 01:52:38,080
so what i want to talk about first is how to fine-tune these nets to

1776
01:52:38,080 --> 01:52:40,420
be better at discrimination

1777
01:52:40,450 --> 01:52:42,850
so what you can do is you can learn

1778
01:52:42,880 --> 01:52:46,610
without knowing the answers, just unsupervised one layer at a time

1779
01:52:46,650 --> 01:52:51,510
and then treat that as fine--as pre-training this layer of features and then you

1780
01:52:51,510 --> 01:52:55,060
can fine-tune it and you can find tune it to be better generating the data

1781
01:52:55,060 --> 01:52:56,240
the images

1782
01:52:56,280 --> 01:52:59,260
that's the contrastive wake sleep algorithm which i'm not going to talk about

1783
01:52:59,280 --> 01:53:03,150
or you can fine-tune it to be better at doing discrimination

1784
01:53:03,190 --> 01:53:07,390
and that works much better than standard backpropagation

1785
01:53:07,410 --> 01:53:11,480
so there's two sort of reasons why it works better

1786
01:53:11,490 --> 01:53:13,460
the optimisation view

1787
01:53:13,490 --> 01:53:17,740
which says that we can optimize these feedforward neural networks better

1788
01:53:17,810 --> 01:53:21,840
by doing pre training and then back propagating a gradient than we can by just back propagating a

1789
01:53:22,880 --> 01:53:27,560
one reason is we can learn layers one at a time this way

1790
01:53:27,610 --> 01:53:30,090
so we can learn the layers separately

1791
01:53:30,130 --> 01:53:34,780
the other reason is that when we back propagate, we're starting from a sensible place

1792
01:53:34,780 --> 01:53:37,340
that is we're starting from features in each layer

1793
01:53:37,350 --> 01:53:40,990
that are good at capturing the correlations in the level below

1794
01:53:41,000 --> 01:53:45,290
as opposed to starting near the middle of this big non-linear space and hoping that

1795
01:53:45,290 --> 01:53:47,720
going downhill is going to get you to the right place

1796
01:53:48,850 --> 01:53:52,750
it was always a sort of miracle that it worked at all

1797
01:53:52,770 --> 01:53:54,670
and then there's the overfitting view

1798
01:53:54,720 --> 01:53:56,270
which is that

1799
01:53:56,300 --> 01:53:58,640
if i've got labels, like i've got say

1800
01:53:58,690 --> 01:54:00,580
suppose i had eight classes

1801
01:54:00,630 --> 01:54:04,200
there are only three bits of information in each label

1802
01:54:04,250 --> 01:54:09,100
and so each training case only put three bits of constraint on the mapping from

1803
01:54:09,100 --> 01:54:11,200
input to output

1804
01:54:11,250 --> 01:54:13,190
so if i've got, say,

1805
01:54:13,200 --> 01:54:16,810
a million training cases, i've still only got three million bits of constraint

1806
01:54:16,860 --> 01:54:19,300
which isn't going to be enough to learn millions of weights

1807
01:54:19,850 --> 01:54:24,640
whereas if i'm trying to model the data itself, the images, they're typically much bigger

1808
01:54:24,640 --> 01:54:27,610
or more complicated, and i've got hundreds of bits of constraint per image

1809
01:54:27,690 --> 01:54:31,220
so i can fit much bigger models if i fit them generatively,

1810
01:54:31,300 --> 01:54:33,410
than if i fit them discriminatively

1811
01:54:33,470 --> 01:54:35,410
that wouldn't be true of course if the

1812
01:54:35,470 --> 01:54:39,690
answers, if the output, the labels, there were sort of gazillions of them

1813
01:54:39,700 --> 01:54:41,750
but it's not normally like that

1814
01:54:42,990 --> 01:54:44,520
then the

1815
01:54:44,580 --> 01:54:47,860
local optimisation that you do with backprop

1816
01:54:47,920 --> 01:54:51,580
is not really going to be designing the features. it's just can be making use

1817
01:54:51,580 --> 01:54:54,800
of features designed by somebody else, the pre training,

1818
01:54:54,860 --> 01:54:56,870
using all the information in the

1819
01:54:57,900 --> 01:55:01,330
and just changing slightly, just slightly changing the discrimination values

1820
01:55:01,390 --> 01:55:05,890
and in fact when you back prop in these nets, things hardly change at all

1821
01:55:06,020 --> 01:55:10,050
i'll show you evidence of that later

1822
01:55:10,090 --> 01:55:12,840
so if you take the same model as before

1823
01:55:12,860 --> 01:55:16,170
but we just pre train it with no labels

1824
01:55:16,230 --> 01:55:20,240
and then we stick ten labels on top and do backpropagation

1825
01:55:21,360 --> 01:55:23,540
it'll work better

1826
01:55:24,300 --> 01:55:27,300
when we did the joint density model, we got 1.25%

1827
01:55:27,340 --> 01:55:30,240
if you do backpropagation now, you get 1.15%

1828
01:55:30,290 --> 01:55:31,640
and actually

1829
01:55:31,680 --> 01:55:35,800
i can now get that down close to 1% by doing the discriminative training

1830
01:55:35,800 --> 01:55:36,960
slightly better. but

1831
01:55:37,010 --> 01:55:39,890
basically it improves things, and this difference is getting bigger

1832
01:55:43,200 --> 01:55:46,270
this is a picture from yoshua bengio's group, the last--the next four slides i

1833
01:55:46,360 --> 01:55:47,760
took from yoshua bengio.

1834
01:55:47,770 --> 01:55:52,450
these are the weights before fine tuning

1835
01:55:52,490 --> 01:55:54,270
this is on mnist digits,

1836
01:55:54,270 --> 01:55:56,240
but with distortions in them

1837
01:55:56,300 --> 01:55:59,930
these are the weights after fine tuning and you can't see any difference

1838
01:55:59,990 --> 01:56:03,920
righ? it's just that these will be getting like 3% error and these will be

1839
01:56:06,340 --> 01:56:10,150
and what's happened is you've--you've got this feature vectors and you take the feature vectors and you

1840
01:56:10,150 --> 01:56:12,170
just change it ever so slightly

1841
01:56:12,230 --> 01:56:15,050
in order to manipulate discrimination boundaries

1842
01:56:15,110 --> 01:56:17,330
and of course if i can change discrimination boundaries,

1843
01:56:17,360 --> 01:56:17,990
i can

1844
01:56:18,020 --> 01:56:20,580
just move it slightly, i can improve the error a whole lot.

1845
01:56:20,620 --> 01:56:23,590
if i got the boundary just slightly wrong, between fours and nines

1846
01:56:23,610 --> 01:56:25,520
tiny changes to the weights will

1847
01:56:25,580 --> 01:56:29,540
make big differences

1848
01:56:32,420 --> 01:56:37,490
this is a demonstration that pre training does something

1849
01:56:39,300 --> 01:56:42,090
this is this was done on a big cluster, and you train lots and lots of

1850
01:56:44,290 --> 01:56:45,420
and look at

1851
01:56:45,430 --> 01:56:47,450
what error they get

1852
01:56:47,460 --> 01:56:50,730
on a version of mnist. there's lots of extra data.

1853
01:56:50,760 --> 01:56:53,010
and then you pre train them

1854
01:56:54,080 --> 01:56:59,430
train them again. so this is backprop, this is pre-training followed by backprop. and the distributions hardly overlap

1855
01:57:00,470 --> 01:57:03,270
that's with one hidden layer i think

1856
01:57:04,510 --> 01:57:07,580
this is as you add more hidden layers

1857
01:57:07,580 --> 01:57:10,270
something's banging. do you know what's banging?

1858
01:57:11,640 --> 01:57:15,410
as you

1859
01:57:15,460 --> 01:57:17,480
as you add more layers

1860
01:57:17,530 --> 01:57:19,150
this effect gets much bigger

1861
01:57:19,160 --> 01:57:20,770
and now there's really

1862
01:57:20,810 --> 01:57:23,020
there's a couple of these guys didn't work very well

1863
01:57:23,080 --> 01:57:26,290
basically there's almost no overlap. these are getting better

1864
01:57:26,350 --> 01:57:29,190
and these getting worse

1865
01:57:30,870 --> 01:57:31,930
the old

1866
01:57:31,950 --> 01:57:36,120
history of backpropagation which was putting lots of hidden layers didn't seem to help

1867
01:57:36,160 --> 01:57:39,870
it was to do with optimisation, right? these are the--these are the same networks

1868
01:57:39,880 --> 01:57:44,090
they got all the extra hidden layers in, and they doing better than these networks

1869
01:57:44,140 --> 01:57:49,570
yes just like

1870
01:57:52,100 --> 01:57:54,330
the problem was optimization

1871
01:57:54,330 --> 01:58:00,770
and if you do this pre training, you're optimizing much better.

1872
01:58:00,830 --> 01:58:04,270
OK so here is another picture, basically the same thing. if you don't do tree--

1873
01:58:04,270 --> 01:58:06,400
pre training and you add layers,

1874
01:58:06,460 --> 01:58:09,530
after a couple of layers, things get worse

1875
01:58:10,940 --> 01:58:13,350
things get better up to about four layers

1876
01:58:13,370 --> 01:58:18,070
and they're all better these anyway.

1877
01:58:18,180 --> 01:58:20,900
this is a nice picture that yoshua's group made

