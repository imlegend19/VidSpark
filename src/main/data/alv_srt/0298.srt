1
00:00:00,000 --> 00:00:02,110
you can see that by

2
00:00:02,160 --> 00:00:04,320
writing down the formula for each

3
00:00:04,330 --> 00:00:05,180
of the

4
00:00:05,200 --> 00:00:06,800
of the functions

5
00:00:08,230 --> 00:00:11,830
what we're interested in most particularly is quadratic functions

6
00:00:11,890 --> 00:00:13,550
i'm interested said

7
00:00:13,590 --> 00:00:16,130
in cliques of size two

8
00:00:16,140 --> 00:00:18,160
cocliques size only two

9
00:00:18,170 --> 00:00:19,900
then you would be able to have

10
00:00:23,410 --> 00:00:27,390
because every variable intakes every function takes two variables

11
00:00:28,110 --> 00:00:33,300
good recall of course look closely at x one square the same thing as x

12
00:00:33,300 --> 00:00:34,950
one because

13
00:00:35,010 --> 00:00:37,360
x one zero one

14
00:00:37,410 --> 00:00:41,740
and so you never have squares as if you only got two variables it just

15
00:00:41,740 --> 00:00:43,360
has to be

16
00:00:43,370 --> 00:00:47,180
what makes one x you can't you can't have x one x two square

17
00:00:47,190 --> 00:00:50,170
so it's always of the form

18
00:00:50,180 --> 00:00:53,190
and i prefer to write it in the form where i

19
00:00:53,200 --> 00:00:56,440
on the right a i j x i x j

20
00:00:57,110 --> 00:00:58,580
the first one is bad

21
00:00:58,670 --> 00:01:01,390
the second one is not

22
00:01:02,400 --> 00:01:04,330
also unique representation

23
00:01:04,340 --> 00:01:08,360
given that i've said i've listened to order

24
00:01:11,380 --> 00:01:12,780
so basically

25
00:01:14,360 --> 00:01:18,040
energy function that you're trying to minimize

26
00:01:18,040 --> 00:01:23,150
which only has likely size two can be written

27
00:01:23,190 --> 00:01:24,440
in that form

28
00:01:26,160 --> 00:01:27,150
look at the two

29
00:01:27,160 --> 00:01:28,810
displayed formula

30
00:01:29,780 --> 00:01:32,620
this is instance of the way that you

31
00:01:32,630 --> 00:01:34,120
i think these

32
00:01:34,170 --> 00:01:36,810
if you take at node i

33
00:01:36,890 --> 00:01:39,640
the its corresponding

34
00:01:39,650 --> 00:01:41,620
its corresponding

35
00:01:41,630 --> 00:01:44,370
very exciting

36
00:01:44,410 --> 00:01:48,660
then you can write the linear

37
00:01:48,740 --> 00:01:50,760
linear term is

38
00:01:50,800 --> 00:01:53,340
just the cost

39
00:01:53,360 --> 00:01:54,290
he i

40
00:01:55,670 --> 00:01:57,780
so excited because he

41
00:01:58,020 --> 00:02:00,690
i zero

42
00:02:00,870 --> 00:02:05,310
x i ivana was that this means the effect size one

43
00:02:05,330 --> 00:02:07,430
as in this one

44
00:02:07,520 --> 00:02:09,830
that's zero so the cost

45
00:02:09,890 --> 00:02:12,590
it's just a i one which is the cost

46
00:02:12,670 --> 00:02:15,110
of the i being one

47
00:02:16,430 --> 00:02:18,010
x i

48
00:02:18,020 --> 00:02:19,280
well zero

49
00:02:19,320 --> 00:02:21,050
fixi like zero

50
00:02:21,060 --> 00:02:22,560
and that is one

51
00:02:22,570 --> 00:02:26,060
that gives you the cost of being zero so

52
00:02:26,150 --> 00:02:27,090
if you just

53
00:02:27,140 --> 00:02:30,910
in tabular form so what's the cost of being zero was cost being one

54
00:02:30,980 --> 00:02:31,700
that gives you

55
00:02:32,960 --> 00:02:35,140
similarly the quadratic terms

56
00:02:35,250 --> 00:02:39,290
can be expressed in terms of what's the cost of find zero zero

57
00:02:39,340 --> 00:02:44,530
what's the cost signing one one cosine zero one one zero

58
00:02:44,570 --> 00:02:47,620
and so on with high degree to you're interested

59
00:02:50,870 --> 00:02:52,020
i described

60
00:02:52,020 --> 00:02:53,340
what the general

61
00:02:54,370 --> 00:02:56,130
markov random fields

62
00:02:56,140 --> 00:02:58,970
processes leave in the two variables

63
00:03:00,590 --> 00:03:03,520
this is an NP hard problem to find

64
00:03:05,720 --> 00:03:08,180
the maximum probability of cost

65
00:03:08,180 --> 00:03:08,760
now with

66
00:03:08,780 --> 00:03:11,040
all intents and purposes you can do it

67
00:03:11,090 --> 00:03:14,930
particularly if you're dealing with images which may have a million pixels and have tens

68
00:03:14,930 --> 00:03:15,810
of millions

69
00:03:15,850 --> 00:03:17,280
random variables

70
00:03:17,280 --> 00:03:21,500
this is not a polynomial you've got no

71
00:03:23,900 --> 00:03:25,920
and it's easy to

72
00:03:25,960 --> 00:03:29,800
see that is the case because particular example of this

73
00:03:32,480 --> 00:03:34,370
we just had a few terms

74
00:03:34,390 --> 00:03:36,420
like this cubic literals

75
00:03:36,460 --> 00:03:38,810
i think it comes down to

76
00:03:38,840 --> 00:03:40,550
the so called

77
00:03:40,600 --> 00:03:42,280
three SAT problem

78
00:03:42,450 --> 00:03:43,750
is there a way

79
00:03:45,670 --> 00:03:49,770
probabilities to each of those terms each year

80
00:03:49,780 --> 00:03:55,700
the the ninth reset satisfiability boolean expressions with three times

81
00:03:55,750 --> 00:03:59,530
so well known standard NP hard problem

82
00:04:00,870 --> 00:04:04,520
get out how this is how we can be represented by graphs

83
00:04:04,520 --> 00:04:05,710
as shown you

84
00:04:05,720 --> 00:04:06,800
how to help

85
00:04:06,860 --> 00:04:09,770
previously basically

86
00:04:09,770 --> 00:04:11,730
if you think a weighted graphs

87
00:04:11,770 --> 00:04:12,420
the the

88
00:04:12,430 --> 00:04:16,880
the vertices on the edges have weights to give weights

89
00:04:16,900 --> 00:04:20,430
and think of pseudo boolean function

90
00:04:22,080 --> 00:04:25,510
you can see the graphs with n plus two vertices in the interest to this

91
00:04:25,510 --> 00:04:31,360
is one of the six regional variables because you have this is zero one

92
00:04:32,630 --> 00:04:36,040
what we're going to do

93
00:04:36,060 --> 00:04:43,240
is up

94
00:04:43,240 --> 00:04:46,940
i think in situation where we have some function f

95
00:04:46,940 --> 00:04:48,860
longer names

96
00:04:48,900 --> 00:04:53,450
and we already know the actions are through motor cortex perception is through all these

97
00:04:53,450 --> 00:04:54,820
areas in the back

98
00:04:54,840 --> 00:04:58,200
and then through frontal cortex so we can map the this whole thing onto to

99
00:04:58,200 --> 00:05:03,280
the brain and people are very excited about this especially because there are two nuclei

100
00:05:03,290 --> 00:05:06,190
two dopamine ergic nuclei which seemed to

101
00:05:06,190 --> 00:05:10,650
project the same kind of signal but two different areas so that many people immediately

102
00:05:10,650 --> 00:05:14,960
think hot actor critic right this one single goes to the credit in one single

103
00:05:14,980 --> 00:05:20,590
goes to that it's the same signal because two different places

104
00:05:20,610 --> 00:05:22,360
so is there any evidence

105
00:05:22,370 --> 00:05:23,480
for this

106
00:05:23,530 --> 00:05:27,460
in the brain i should say this is a very very prominent theory of learning

107
00:05:27,460 --> 00:05:29,480
in the brain and i think

108
00:05:29,490 --> 00:05:33,160
my my opinion that this might be wrong but there are not many such a

109
00:05:33,160 --> 00:05:36,910
neuroscientist here to tell me that i'm wrong but my opinion is really it was

110
00:05:36,910 --> 00:05:39,020
because of this excitement of two

111
00:05:39,040 --> 00:05:40,960
nuclei of dopamine

112
00:05:40,980 --> 00:05:45,030
the signal the same thing to two different areas not not based on

113
00:05:45,050 --> 00:05:46,540
more stringent

114
00:05:48,670 --> 00:05:51,070
it just looks really good

115
00:05:51,120 --> 00:05:52,860
but there is some evidence

116
00:05:52,870 --> 00:05:56,280
and if i show some evidence of small side

117
00:05:56,340 --> 00:05:58,330
the functional imaging

118
00:05:58,370 --> 00:06:02,080
and how that helps us look for this kind of evidence and i'm doing this

119
00:06:02,460 --> 00:06:06,410
where i didn't do it for other methods in neuroscience because this is kind of

120
00:06:06,480 --> 00:06:10,240
the new wave of research into reinforcement learning and the brain so it's good for

121
00:06:10,240 --> 00:06:13,190
you to know how it's done

122
00:06:14,070 --> 00:06:18,660
fmri functional magnetic resonance imaging and all start with the big man it looks like

123
00:06:18,660 --> 00:06:23,710
this and the subjects lies on the bed and is inserted so that their heads

124
00:06:23,710 --> 00:06:28,240
is in the magnet and they see stimuli that they have like married basically the

125
00:06:28,240 --> 00:06:32,580
stimuli from the screen to have about box and they make their choices are do

126
00:06:32,580 --> 00:06:36,440
whatever the experiment tells them to do or even get you into their mouth

127
00:06:36,660 --> 00:06:39,480
it was a little too they just do the experiment lying down and looking at

128
00:06:39,480 --> 00:06:42,800
the camera and the former

129
00:06:42,830 --> 00:06:45,670
and trying to not move their head at all

130
00:06:45,780 --> 00:06:49,120
the police we hope they don't move head because the idea of what we what

131
00:06:49,120 --> 00:06:51,400
we're measuring these experiments

132
00:06:52,160 --> 00:06:54,400
what's called a bold signal are

133
00:06:54,410 --> 00:06:59,400
that's right for blood oxygenation level dependent signal

134
00:07:00,150 --> 00:07:01,520
what happens is

135
00:07:01,530 --> 00:07:05,000
in the brain there is there's a lot of blood there are lots of blood

136
00:07:05,000 --> 00:07:07,440
vessels every neuron its blood

137
00:07:07,450 --> 00:07:09,300
because it needs oxygen

138
00:07:09,320 --> 00:07:15,170
and there's the difference in the magnetic properties of oxygenated and de oxygenated

139
00:07:15,200 --> 00:07:17,580
chemical hemoglobin in the blood

140
00:07:17,620 --> 00:07:22,950
so after the accident has been taken was by the neurons the blood has a

141
00:07:22,950 --> 00:07:25,620
different magnetic properties than it did before

142
00:07:26,000 --> 00:07:30,490
we can detect this these little magnetic fluctuations with this

143
00:07:30,500 --> 00:07:36,760
huge super conducting magnets that makes a lot of noise and cost lots of money

144
00:07:36,800 --> 00:07:41,540
and so what we're relying here is is on the fact that the brain is

145
00:07:41,540 --> 00:07:46,980
functionally modular so different areas of the brain get blood by different vessels and if

146
00:07:46,980 --> 00:07:52,160
one area is active it will use up more oxygen will actually asking the body

147
00:07:52,160 --> 00:07:57,240
for more oxygen than it needs and we will see more oxygenation in that area

148
00:07:57,240 --> 00:08:01,490
because it was active

149
00:08:01,500 --> 00:08:06,260
so what is does reason it's called functional MRI is it shows this function shows

150
00:08:06,260 --> 00:08:09,650
us what brain area was used at what time

151
00:08:09,820 --> 00:08:12,740
to some very very coarse

152
00:08:12,750 --> 00:08:14,490
resolution so

153
00:08:14,500 --> 00:08:18,900
the spatial resolution is about three millimeters

154
00:08:18,900 --> 00:08:21,160
cupid so we can kind of tell

155
00:08:21,170 --> 00:08:26,660
these kind of these small voxels of three by three by three which is certainly

156
00:08:26,660 --> 00:08:31,050
not at the level of one neuron to the level of thousands of neurons but

157
00:08:31,070 --> 00:08:36,090
the motor cortex sensory motor areas that they are differentiated enough that we can tell

158
00:08:36,090 --> 00:08:37,660
them apart

159
00:08:37,670 --> 00:08:40,660
the temporal resolution is also quite shabby

160
00:08:40,660 --> 00:08:44,120
it's five to ten seconds because it takes time for the blood to get there

161
00:08:44,470 --> 00:08:49,910
and to await like for every spike of the neuron we see some blood supply

162
00:08:49,950 --> 00:08:52,660
difference but if the whole area is active for a while

163
00:08:52,700 --> 00:08:55,910
five seconds later we'll see this extra blood that

164
00:08:57,210 --> 00:09:00,950
what happens is that it asks for more blood than it needs so we see

165
00:09:00,980 --> 00:09:04,660
signal five seconds later

166
00:09:04,750 --> 00:09:06,780
so what can we do with this

167
00:09:06,830 --> 00:09:10,460
we can study reinforcement learning and the brain in humans

168
00:09:10,500 --> 00:09:14,910
we can take this hidden variables the model prediction errors inference and say OK during

169
00:09:14,910 --> 00:09:21,370
my experiment where my subject was playing some games choosing stimuli getting rewards doing gambling

170
00:09:21,370 --> 00:09:23,070
or whatever in my

171
00:09:23,080 --> 00:09:28,620
little experiment video game the model predicts that there would have been prediction errors over

172
00:09:28,620 --> 00:09:34,110
time the second one would have been positive prediction the second for negative one and

173
00:09:34,110 --> 00:09:35,200
so forth

174
00:09:35,200 --> 00:09:38,230
so i predict these prediction errors

175
00:09:38,240 --> 00:09:39,400
and i know

176
00:09:40,690 --> 00:09:45,880
the BOLD signal usually has the typical time constant or response it looks like this

177
00:09:45,880 --> 00:09:48,410
so if there was an event here

178
00:09:48,420 --> 00:09:51,540
five seconds later we'll see a peak in bold

179
00:09:51,590 --> 00:09:55,700
fifteen seconds later will feed it any you the the whole thing levels out after

180
00:09:55,700 --> 00:09:59,400
about thirty two seconds basically five seconds later we'll see

181
00:09:59,410 --> 00:10:00,870
a peak so we can

182
00:10:00,880 --> 00:10:03,740
convolve our prediction error signals

183
00:10:03,750 --> 00:10:07,950
delta functions as we call them although they're not really delta function to have different

184
00:10:09,130 --> 00:10:10,160
with this

185
00:10:11,340 --> 00:10:12,410
and yet

186
00:10:12,420 --> 00:10:16,710
the signal that we're expecting in the brain were expecting a p caused by this

187
00:10:16,710 --> 00:10:21,280
letter trough caused by this p caused by this etcetera

188
00:10:21,320 --> 00:10:26,000
and take this in use linear regression to regress this against the activity of the

189
00:10:26,000 --> 00:10:29,980
brain and ask is there an area in the brain that looks like this

190
00:10:29,990 --> 00:10:34,880
like this green line

191
00:10:34,900 --> 00:10:36,130
and so that's what

192
00:10:36,160 --> 00:10:37,570
o'doherty et al

193
00:10:37,580 --> 00:10:42,580
did in two thousand four to look for actor critic mechanisms in the brain

194
00:10:42,580 --> 00:10:45,640
so this is the same as here so we can send which in between so

195
00:10:45,640 --> 00:10:46,920
this is lower bounded by this

196
00:10:47,680 --> 00:10:48,660
that is what is written here

197
00:10:49,840 --> 00:10:52,430
now we take the lambda square to the right hand side

198
00:10:53,510 --> 00:10:55,370
and divided by lower case are

199
00:10:56,160 --> 00:10:58,200
and we left this thing was that this thing here

200
00:10:59,030 --> 00:11:00,660
so what have we shown we shown that

201
00:11:01,850 --> 00:11:02,470
if we assume

202
00:11:04,220 --> 00:11:05,520
w is upper bounded by lambda

203
00:11:07,470 --> 00:11:11,840
the dataset is shutouts dataset of are points is shattered by canonic hyperplanes

204
00:11:12,650 --> 00:11:15,140
the data set is upper bounded length they are

205
00:11:16,630 --> 00:11:16,890
and then

206
00:11:17,410 --> 00:11:19,140
as a consequence of these assumptions

207
00:11:19,620 --> 00:11:22,660
the numbers are must be upper bounded by this thing here

208
00:11:23,260 --> 00:11:23,780
so if

209
00:11:24,270 --> 00:11:25,230
dataset shadows

210
00:11:25,720 --> 00:11:26,780
it cannot be larger than this

211
00:11:27,740 --> 00:11:31,020
therefore this is an upper bound on the see dimension because we see dimension is

212
00:11:31,690 --> 00:11:35,110
and the largest number of points that can be chapters

213
00:11:41,190 --> 00:11:42,330
big big are is

214
00:11:49,890 --> 00:11:54,030
where is it written it's the length and the radius of the smallest sphere around

215
00:11:54,030 --> 00:11:56,050
the origin that contains the data set

216
00:11:57,060 --> 00:11:58,180
and we use it in the proof

217
00:12:02,140 --> 00:12:03,180
i think in the second part

218
00:12:03,740 --> 00:12:07,170
so in the second part we derive this upper bound on this quantity and now

219
00:12:07,170 --> 00:12:08,490
we are saying each one of those

220
00:12:09,210 --> 00:12:10,730
is upper bounded by our square dance

221
00:12:11,360 --> 00:12:12,640
so then we got this thing here

222
00:12:16,860 --> 00:12:19,680
because they're so busy we have two things one is the radius of the

223
00:12:20,300 --> 00:12:21,950
the smallest sphere containing the data points

224
00:12:22,900 --> 00:12:23,620
and one is

225
00:12:24,980 --> 00:12:28,240
in a sense the radius of the sphere containing the weight vectors that are large

226
00:12:28,270 --> 00:12:30,380
so it's an upper bound on the length of the weight vectors

227
00:12:36,050 --> 00:12:36,830
and one can also

228
00:12:37,240 --> 00:12:42,980
intuitively understand why it's reasonable to have this kind of a bound on the dimension

229
00:12:43,840 --> 00:12:45,640
that is should play an important role

230
00:12:46,360 --> 00:12:47,740
one way to think about is

231
00:12:49,590 --> 00:12:52,960
would be just say well and this is just a purely intuitive argument

232
00:12:55,130 --> 00:12:58,660
what if we have some kind of noise in the data points so this fringe

233
00:12:58,660 --> 00:13:00,660
there's an old paper about neural network

234
00:13:01,830 --> 00:13:02,690
chris bishop that

235
00:13:03,500 --> 00:13:04,170
argues that

236
00:13:05,230 --> 00:13:06,390
putting noise in the data

237
00:13:06,980 --> 00:13:10,310
it a monster taken off regularization on network

238
00:13:10,770 --> 00:13:15,630
so putting on noise and data is some form of regularization of a function class also

239
00:13:16,020 --> 00:13:17,410
in implicitly in here

240
00:13:18,120 --> 00:13:21,180
because if we were to put bounded noise each data point

241
00:13:22,050 --> 00:13:25,480
maybe we sample from each data point many many points

242
00:13:27,590 --> 00:13:29,390
with the distribution that has this bounded noise

243
00:13:30,020 --> 00:13:32,610
and then we train a normal hyperplane without

244
00:13:33,020 --> 00:13:34,400
looking for optimal margin

245
00:13:34,870 --> 00:13:36,950
we might get something which is similar to this

246
00:13:37,370 --> 00:13:39,120
the maximum margin kind of hyperplane if

247
00:13:39,920 --> 00:13:43,350
radius the noise here is as large as it can be

248
00:13:44,230 --> 00:13:44,950
or another

249
00:13:45,700 --> 00:13:49,770
way to interpret maximum margin would be minimal in terms minimum description description length

250
00:13:51,170 --> 00:13:53,350
so minimum description length is an approach to

251
00:13:55,160 --> 00:13:56,880
inference on generalization

252
00:13:57,890 --> 00:13:58,770
and we trying to say

253
00:14:00,530 --> 00:14:01,460
i uh

254
00:14:02,420 --> 00:14:06,420
finding a function that generalizes amongst generalizes amounts to finding

255
00:14:07,370 --> 00:14:10,770
a short description of a data set amounts a compression of the data set and

256
00:14:10,770 --> 00:14:12,830
the smaller the compression the better we can generalize

257
00:14:13,560 --> 00:14:17,610
and here you can also find a large margin hyperplane is a compression of the

258
00:14:17,610 --> 00:14:19,170
labels of the dataset so we

259
00:14:19,810 --> 00:14:20,940
we are given the data points

260
00:14:22,760 --> 00:14:26,370
what we can do is we throw away the labels and instead store the hyperplane

261
00:14:26,370 --> 00:14:29,340
because we know we can regenerate the labels using the hyperplane

262
00:14:29,900 --> 00:14:34,010
and it turns out that if your lies in a ball of bonded radius

263
00:14:34,990 --> 00:14:38,440
end if the margin of separation is large is rho

264
00:14:39,590 --> 00:14:41,780
and then you can work out how accurately

265
00:14:42,370 --> 00:14:45,670
u u have to store these ants in this simple picture here

266
00:14:46,120 --> 00:14:49,000
storing the hyperplane monster starring this angle gamma

267
00:14:52,170 --> 00:14:53,440
and the larger

268
00:14:54,690 --> 00:14:55,880
ratio of rho

269
00:14:56,680 --> 00:14:57,170
two are

270
00:14:58,030 --> 00:14:59,420
the small of all the

271
00:15:00,530 --> 00:15:05,410
new so you can store come up the less accuracy you need for storing them and therefore less bits

272
00:15:06,400 --> 00:15:10,020
so in a few if you work this out in this simple picture again you

273
00:15:10,020 --> 00:15:12,380
end up with this ratio of of robot are

274
00:15:16,760 --> 00:15:19,950
anyway so now it's created about the theory

275
00:15:20,980 --> 00:15:23,980
just this just accept it's good to classify

276
00:15:24,760 --> 00:15:28,710
uh uh data using large margin hyperplanes

277
00:15:28,710 --> 00:15:33,180
you're going to have a certain probability going left certain probability of going right in

278
00:15:33,180 --> 00:15:36,600
your opponent knows this probability is the a now

279
00:15:36,620 --> 00:15:39,150
in new way of thinking is just think of kind of you know you can

280
00:15:39,320 --> 00:15:40,470
see thing now

281
00:15:40,490 --> 00:15:41,860
you can go left

282
00:15:41,870 --> 00:15:45,070
and they can stop you more often than not you know you can go right

283
00:15:45,070 --> 00:15:48,850
they can study more often than not the best strategy in this game

284
00:15:49,860 --> 00:15:52,140
tissue fifty b then

285
00:15:52,190 --> 00:15:55,010
the goal you only have a fifty percent chance stop

286
00:15:55,160 --> 00:16:01,330
that's the minimax optimal strategy this to the minimax optimal strategy is randomized strategy that

287
00:16:01,340 --> 00:16:05,480
the best thing to play if your opponent knows the strategy

288
00:16:05,860 --> 00:16:10,090
to make sense to make

289
00:16:10,130 --> 00:16:11,830
will do some examples

290
00:16:11,910 --> 00:16:17,370
you can solve for these and my own game using linear programming or not

291
00:16:17,430 --> 00:16:22,150
because this can we just said that the minimax optimal strategies for both of the

292
00:16:22,150 --> 00:16:26,630
shooter fifty fifty and for the goal is also fifty

293
00:16:26,720 --> 00:16:32,490
if you shoot fifty fifty is guaranteed whatever you're part the goal goes left to

294
00:16:32,490 --> 00:16:36,710
right here in the fifty percent chance of getting the goal here is that is

295
00:16:38,380 --> 00:16:40,180
any other strategy

296
00:16:40,180 --> 00:16:41,380
we do worse

297
00:16:42,180 --> 00:16:45,520
the opponent knows your problem

298
00:16:46,210 --> 00:16:48,600
fifty fifty

299
00:16:48,790 --> 00:16:51,650
all right so i think of a will be more interesting game with the goal

300
00:16:51,650 --> 00:16:57,830
is weaker on the left so i have done is change this entry here

301
00:17:01,150 --> 00:17:03,980
so goal is weaker on the left so if you shoot left even if the

302
00:17:03,990 --> 00:17:09,300
goalie dies left is still a fifty percent chance the ball

303
00:17:09,380 --> 00:17:13,990
now what's the minimax awful thing

304
00:17:14,100 --> 00:17:16,850
i mean since the goal is we can't allow the user forgot the right this

305
00:17:16,850 --> 00:17:18,320
is always the overlap

306
00:17:18,380 --> 00:17:21,620
because then i a fifty percent chance of online

307
00:17:21,630 --> 00:17:25,630
but not really taking advantage of that we going to even before a fifty percent

308
00:17:25,630 --> 00:17:32,740
chance of one in now really should left fifty percent building better

309
00:17:32,850 --> 00:17:40,890
one think with minimax optimal strategies here so the goal is to something for the

310
00:17:40,890 --> 00:17:43,550
shoes let's say you're the shooter in you know

311
00:17:43,550 --> 00:17:47,530
the goal is weaker the last issue activity potentially it's going to go and even

312
00:17:47,530 --> 00:17:49,220
if the goalie dive left

313
00:17:49,240 --> 00:17:52,650
course going wrong way in percent

314
00:17:52,650 --> 00:17:57,990
if you should decide it go going only if the goalie dived by

315
00:18:02,530 --> 00:18:05,770
the most of the time left but occasionally to the right

316
00:18:05,830 --> 00:18:09,080
i think that is stretching OK

317
00:18:09,090 --> 00:18:10,960
how occasionally

318
00:18:10,970 --> 00:18:19,240
that's right so you want you can't mean fifty fifty

319
00:18:19,330 --> 00:18:22,930
see if you always go after should always goal after fifty percent

320
00:18:23,090 --> 00:18:26,590
what with the shooter do for the going to if you're fifty

321
00:18:26,590 --> 00:18:31,560
if you're fifty fifty what should the goal

322
00:18:31,620 --> 00:18:33,520
and the goal is to go right

323
00:18:33,580 --> 00:18:37,110
and you also a fifty percent chance came to use two different strategies about fifty

324
00:18:38,060 --> 00:18:42,150
the somewhere in between there's gotta be a better shot right

325
00:18:42,200 --> 00:18:45,370
so we could solve

326
00:18:45,420 --> 00:18:50,960
some probability on the left and some probability going right

327
00:18:51,000 --> 00:18:55,750
so if probability p going left and one minus going right

328
00:18:55,770 --> 00:18:59,210
and if the goalie dive left are part of the our total thing is p

329
00:18:59,210 --> 00:19:00,470
times the have

330
00:19:00,500 --> 00:19:02,810
plus one minus p one

331
00:19:02,890 --> 00:19:07,720
going the other way we get p one plus one minus zero we want to

332
00:19:07,720 --> 00:19:09,090
make the

333
00:19:09,140 --> 00:19:12,830
maximize the minimum which you can do by setting make

334
00:19:12,870 --> 00:19:15,710
if we do that we get

335
00:19:15,750 --> 00:19:17,930
exactly what what

336
00:19:17,960 --> 00:19:20,870
he said about two thirds probability here

337
00:19:20,870 --> 00:19:23,830
one third probability

338
00:19:23,840 --> 00:19:27,780
if the goalie that's left over two-thirds times one-half to one-third plus

339
00:19:27,800 --> 00:19:32,210
one for terms one two thirds chance of getting in the way that left

340
00:19:32,270 --> 00:19:35,900
and we also have two thirds chance of getting in the way that

341
00:19:35,930 --> 00:19:41,800
he causes the case

342
00:19:41,830 --> 00:19:45,250
building from the goal is perspective what should the goal

343
00:19:45,260 --> 00:19:48,480
minimax optimal strategy for the goal

344
00:19:48,500 --> 00:19:53,360
the as this this guarantees expected gain at least two thousand so this strategy for

345
00:19:53,590 --> 00:19:58,850
that guarantees the shooter at least two-thirds transfer exactly third chance

346
00:19:58,880 --> 00:20:01,280
getting going

347
00:20:01,340 --> 00:20:03,970
and we call six always perspective

348
00:20:04,020 --> 00:20:07,060
so the only the side

349
00:20:07,100 --> 00:20:12,030
the i know the shooter and knows well what's what's meant by minimax optimal strategies

350
00:20:12,100 --> 00:20:15,030
what the deletion i go left i go right

351
00:20:15,130 --> 00:20:18,140
you can sort of the same thing

352
00:20:18,150 --> 00:20:22,220
this game symmetric so it's actually the same answer

353
00:20:22,310 --> 00:20:23,750
the goalie

354
00:20:23,760 --> 00:20:29,100
should have two thirds chance diving like one third chance five

355
00:20:29,150 --> 00:20:30,850
that guarantees

356
00:20:30,910 --> 00:20:33,320
whatever the peter decides to do it

357
00:20:33,360 --> 00:20:35,650
you know one

358
00:20:37,950 --> 00:20:39,350
well good luck to that

359
00:20:39,380 --> 00:20:42,640
two thirds meinshausen

360
00:20:43,630 --> 00:20:46,530
one third in one

361
00:20:46,630 --> 00:20:47,830
they thirds

362
00:20:47,890 --> 00:20:49,140
this way

363
00:20:50,880 --> 00:20:53,970
two hundred twenty one people

364
00:20:55,170 --> 00:21:01,450
this strategy for the shooter and the goalie guarantees whatever she decides to do

365
00:21:01,460 --> 00:21:08,540
the goalie has almost two-thirds of the only the expected loss legalism of

366
00:21:08,590 --> 00:21:12,340
is the kind of interesting things that the shooter can

367
00:21:12,400 --> 00:21:17,280
take a strategy that guarantees whatever the goalie does is at least two-thirds chance in

368
00:21:17,310 --> 00:21:21,480
the goal only can pick something so the matter the shooter does he is most

369
00:21:21,480 --> 00:21:23,380
two-thirds chancellors

370
00:21:23,470 --> 00:21:26,720
so clearly the only the only help to guarantee that the US

371
00:21:26,730 --> 00:21:28,460
since that would contradict that

372
00:21:28,470 --> 00:21:34,280
this shooter could guarantee by some countries for us so that the shows you of

373
00:21:34,310 --> 00:21:42,000
i mean that's theorem tells you at meeting always happens

374
00:21:42,040 --> 00:21:44,580
good time for game

375
00:21:44,650 --> 00:21:46,950
so here's the game now i really like

376
00:21:46,960 --> 00:21:50,570
this game is american centric

377
00:21:50,660 --> 00:21:57,280
the talks about a quarter of the so quarter is twenty five cents and nickels

378
00:22:00,450 --> 00:22:05,730
so here's the game so i i i what i have

379
00:22:05,750 --> 00:22:10,530
either twenty five cents in nickel five cents the conical because they were made of

380
00:22:10,530 --> 00:22:12,780
nickel were five cents

381
00:22:12,830 --> 00:22:17,230
and i have one of those banned you get to guess which i have

382
00:22:17,270 --> 00:22:18,780
if you get it right

383
00:22:18,830 --> 00:22:21,340
you get the coin

384
00:22:21,360 --> 00:22:23,010
if get wrong

385
00:22:23,060 --> 00:22:23,710
you know

386
00:22:23,720 --> 00:22:28,650
you get

387
00:22:28,660 --> 00:22:30,580
the second n

388
00:22:32,690 --> 00:22:36,760
so cell

389
00:22:36,770 --> 00:22:38,530
this again makes sense

390
00:22:38,640 --> 00:22:41,830
and when the whole OK now now peak in australia

391
00:22:41,830 --> 00:22:43,490
blah blah blah blah

392
00:22:45,470 --> 00:22:46,810
which are all strings

393
00:22:46,830 --> 00:22:50,020
integer but most of them are strings

394
00:22:50,030 --> 00:22:51,900
and and this is called

395
00:22:51,980 --> 00:22:55,020
the earliest documentation

396
00:22:55,020 --> 00:23:02,200
which means instead of writing machine readable five it should the low to to plug

397
00:23:02,220 --> 00:23:04,700
reason and that and so far they say

398
00:23:05,400 --> 00:23:06,900
the web is full of people

399
00:23:06,930 --> 00:23:09,060
and people in need documentation

400
00:23:09,070 --> 00:23:10,790
and needs to use what's

401
00:23:10,830 --> 00:23:15,060
OK so there is a XML and very it be

402
00:23:15,080 --> 00:23:16,190
OK so

403
00:23:16,230 --> 00:23:17,550
amount plus interest

404
00:23:17,570 --> 00:23:21,810
is what we want to use and if you go down

405
00:23:21,820 --> 00:23:25,550
here you would see

406
00:23:25,550 --> 00:23:29,230
these exampl

407
00:23:29,240 --> 00:23:30,900
which is the one that i run

408
00:23:33,270 --> 00:23:35,680
you can see it just plain your

409
00:23:35,810 --> 00:23:38,580
OK says go to these URL

410
00:23:38,640 --> 00:23:39,640
and we get

411
00:23:39,650 --> 00:23:44,690
and they get should contain some stuff then the list of keywords and then if

412
00:23:44,690 --> 00:23:46,020
you like the location

413
00:23:46,150 --> 00:23:49,690
and then if you like the thought that is something that happens in the future

414
00:23:49,980 --> 00:23:52,350
and this is the rest service

415
00:23:52,390 --> 00:23:56,990
i mean it goes to o two to is also which is there are here

416
00:23:59,490 --> 00:24:04,650
and although these researchers performs things that you normally performed on the web

417
00:24:04,650 --> 00:24:09,160
so what you we perform is it gets posted things like that so if you

418
00:24:09,160 --> 00:24:13,070
have to get information to get if you have to update the post

419
00:24:13,080 --> 00:24:15,980
more or less this is point

420
00:24:16,020 --> 00:24:20,370
and so you have these different way of thinking which is

421
00:24:20,430 --> 00:24:23,690
now nowadays because the rest of this

422
00:24:23,740 --> 00:24:27,740
but i'm not sure this is a good way to do that and that people

423
00:24:27,740 --> 00:24:29,360
that try to bridge the two

424
00:24:29,370 --> 00:24:33,430
for instance the the reason i stuff which is quite interesting

425
00:24:33,470 --> 00:24:40,860
where it is in the

426
00:24:41,010 --> 00:24:46,860
like the one that i was talking about then when i was talking about the

427
00:24:47,860 --> 00:24:54,270
OK this microformats should be in lightweight machine processable

428
00:24:54,270 --> 00:24:55,720
approach two

429
00:24:55,760 --> 00:24:57,890
describe service

430
00:24:58,020 --> 00:25:00,890
it's like the we the so i

431
00:25:00,940 --> 00:25:06,190
the same things of wisdom but it does it in a much lighter way approach

432
00:25:06,470 --> 00:25:08,240
to rest for all of you

433
00:25:08,320 --> 00:25:12,070
it's just technology so what i'm doing here

434
00:25:12,070 --> 00:25:16,270
so if i didn't explain it but i thought it was not the pacific

435
00:25:16,450 --> 00:25:18,360
i be too

436
00:25:18,400 --> 00:25:22,650
did i mean that i will but he also enjoyed that did is not is

437
00:25:22,650 --> 00:25:27,060
almost caught by using his arrest idea

438
00:25:27,070 --> 00:25:29,730
and the rest idea is right to you

439
00:25:29,780 --> 00:25:31,550
and get results

440
00:25:31,600 --> 00:25:35,990
nothing more than that

441
00:25:36,810 --> 00:25:39,320
OK so we are here talking about linking

442
00:25:41,400 --> 00:25:42,890
got the idea of linking

443
00:25:46,100 --> 00:25:47,830
so we are close to the end

444
00:25:48,110 --> 00:26:00,720
now we have all these data we have all of them represented

445
00:26:00,730 --> 00:26:05,230
in their own content sources we just have to write these big query

446
00:26:05,270 --> 00:26:10,260
but gets the results out and then to represent them in a way that

447
00:26:10,270 --> 00:26:15,760
can be understood by the ajax framework that we choose

448
00:26:16,350 --> 00:26:20,430
let's talk about his ajax framework is name exhibit as i said

449
00:26:20,450 --> 00:26:23,650
it's a fast sector browsing tool

450
00:26:23,790 --> 00:26:27,820
we're far sec means that given result set you can navigate

451
00:26:27,860 --> 00:26:35,150
you can select a subset of the of the results so you can or you're

452
00:26:35,160 --> 00:26:41,900
the resulting you can show the result set in different representation least a timeline map

453
00:26:41,900 --> 00:26:44,350
and things like that so

454
00:26:44,370 --> 00:26:49,480
what we want to do is to allow grouping and filtering by name nationality side

455
00:26:49,480 --> 00:26:52,650
time better to have the right answer calibration

456
00:26:52,650 --> 00:26:55,860
OK so bayesian work is focused on coherence

457
00:26:55,890 --> 00:27:00,510
well frequencies work hasn't been too worried about causing a brief affair statement so bayesians

458
00:27:01,050 --> 00:27:04,720
get kind of coherence for free because they joint problems which undermine everything and that's

459
00:27:04,720 --> 00:27:08,750
the source of the coherence and they love to bash frequencies because they find places

460
00:27:08,750 --> 00:27:13,570
where frequencies work not coherent and lots and of other papers written about that the

461
00:27:13,570 --> 00:27:17,350
frequency and not so worried about that they are interested in a particular inference problem

462
00:27:17,350 --> 00:27:20,540
at one particular time and do the best you know finding lost function the targets

463
00:27:20,540 --> 00:27:24,440
that law that problem and you know you can go here all the time you

464
00:27:24,440 --> 00:27:29,770
know that's life has kind of you one lecture one so you know i'm not

465
00:27:30,020 --> 00:27:36,650
that cohen while cohen is a hundred and cohen mornings

466
00:27:36,670 --> 00:27:42,330
but consistent is perhaps there were that's another technical meaning so you might consider i'll

467
00:27:42,340 --> 00:27:45,450
tell you one thing one day in the northeast something else six months later and

468
00:27:45,730 --> 00:27:47,910
that's life

469
00:27:48,780 --> 00:27:54,630
now had frequencies workers focus on calibration calibration again kind of like the sort of

470
00:27:54,630 --> 00:28:00,930
coverage that you the kind of numerical values is so severe procedure really come out

471
00:28:00,930 --> 00:28:01,940
in practice

472
00:28:01,980 --> 00:28:04,890
can have been taught about calibration

473
00:28:04,900 --> 00:28:07,410
OK now that's kind of a bit of a problem with the basic perspective you

474
00:28:07,410 --> 00:28:10,730
know you are writing much of priority right procedure when your data and that's it

475
00:28:10,730 --> 00:28:12,010
you're done

476
00:28:12,040 --> 00:28:14,670
i will guarantee that you give me and can you tell me you know that

477
00:28:14,670 --> 00:28:18,020
you do the procedure multiple times it would come out have error you're going to

478
00:28:18,020 --> 00:28:19,360
turn out to be true

479
00:28:19,380 --> 00:28:24,050
i will be starting to worry about that you know that much time enough now

480
00:28:24,090 --> 00:28:28,480
good bayesians almost bayesians and statistics are actually a little bit frequencies two and so

481
00:28:28,480 --> 00:28:30,990
they will often look at a little bit of a frequency analysis of what they're

482
00:28:30,990 --> 00:28:34,880
doing to compare the coverage for example of their bayesian procedure

483
00:28:34,890 --> 00:28:39,270
anyway if you don't do that if you just to appear bayesian then you certainly

484
00:28:39,270 --> 00:28:45,860
get caught coherence but you know you go don't street calibration on the other hand

485
00:28:45,860 --> 00:28:47,580
if you're poor frequentist so you just

486
00:28:47,600 --> 00:28:54,490
they're worried about pure calibration you can be calibrated completely useless so know ninety five

487
00:28:54,490 --> 00:28:58,930
percent of the time about error bars that are appointed the hunt and in europe

488
00:28:58,980 --> 00:29:01,350
by the time you get reversed to cover everything

489
00:29:01,360 --> 00:29:04,770
so the average works out to be some kind of confidence interval but on any

490
00:29:04,770 --> 00:29:08,720
given saturday to give out a useless answer

491
00:29:08,720 --> 00:29:13,640
so you can make use of your account history completely incoherent you can be completely

492
00:29:13,640 --> 00:29:18,090
incoherent and completely wrong it is clinical here to give out the answer wonder whatever

493
00:29:18,090 --> 00:29:19,780
request problem you ask me

494
00:29:19,840 --> 00:29:23,590
going about

495
00:29:23,610 --> 00:29:27,770
OK and so most statisticians find some kind of bland a natural way to proceed

496
00:29:27,770 --> 00:29:31,050
because they tend to achieve both terms calibration but in some sense given you the

497
00:29:31,050 --> 00:29:32,890
answer my question my title

498
00:29:32,900 --> 00:29:37,460
i think many statisticians not every single one but many of them find that they

499
00:29:37,460 --> 00:29:41,430
are both a little bit a little bit frequentist and these things can be made

500
00:29:41,430 --> 00:29:46,650
into conflict are ways of focusing on calibration coherence and showing one perspective doesn't achieve

501
00:29:46,650 --> 00:29:52,110
it but they really do act complementary and each other

502
00:29:52,140 --> 00:29:57,760
and it's a little bit like wave particle duality one we think about it sort

503
00:29:57,760 --> 00:30:01,730
of waves and particles are both there they're both can always be true there's something

504
00:30:01,750 --> 00:30:06,410
write about both of them but they don't quite really work you know together as

505
00:30:06,410 --> 00:30:09,270
well as they should i think it's true about bayesian frequentist two

506
00:30:09,290 --> 00:30:14,070
they're both right in some way they go around forever ones i could vanish

507
00:30:14,090 --> 00:30:19,510
but they don't quite know merge entirely they do fight each other in various ways

508
00:30:19,510 --> 00:30:23,950
in particular in intestine model selection problems and i think they'll probably be eventually be

509
00:30:23,960 --> 00:30:28,170
some more resolution there even but it's going to take

510
00:30:28,230 --> 00:30:32,720
all right so your comments about the sociology really so

511
00:30:33,150 --> 00:30:37,490
the the frequencies world is this hodgepodge of people you to do any kind of

512
00:30:37,490 --> 00:30:42,160
technique is largely given analysis that's frequencies and so it just a big big field

513
00:30:42,240 --> 00:30:46,610
bayesian it'll smaller and it's really gets to student main subdivisions so you have subjective

514
00:30:46,610 --> 00:30:50,990
bayesian objective basis simple but these are the kind of two main schools

515
00:30:53,000 --> 00:30:58,470
and the subjective bayesian is believed that the prior comes from some from a person

516
00:30:58,900 --> 00:31:00,730
or maybe a small group of people

517
00:31:00,740 --> 00:31:05,020
and so the goal is to work with that person domain experts and you want

518
00:31:05,020 --> 00:31:08,520
to figure out what's the prior that person has in their head and what loss

519
00:31:08,520 --> 00:31:12,190
function they have in their had also the model to somehow come from the domain

520
00:31:12,190 --> 00:31:16,720
experts also figure out what the model is list the model OK

521
00:31:16,740 --> 00:31:20,350
and the the subjective bayesian argument is that if you got a bad answers from

522
00:31:20,350 --> 00:31:23,770
your your bayesian procedure didn't work hard enough to get the prior and the loss

523
00:31:23,770 --> 00:31:24,770
function model

524
00:31:24,830 --> 00:31:26,140
she worked harder

525
00:31:26,150 --> 00:31:32,190
right and put that weights are hard to argue with phi space a million years

526
00:31:32,190 --> 00:31:33,720
ago the right prior you know

527
00:31:34,210 --> 00:31:38,440
something you would get out the right inference

528
00:31:38,520 --> 00:31:41,740
so what kind of if there is you know just what you have the prior

529
00:31:41,810 --> 00:31:45,990
loss and the model you're done you know they use bayes rule and so on

530
00:31:45,990 --> 00:31:49,220
there's not much else to talk about the what kind of research he is about

531
00:31:49,230 --> 00:31:52,960
to be quite a lot of things you start here last two weeks i think

532
00:31:52,960 --> 00:31:57,110
a lot of the work you price i was in fact only subjective bayesian natural

533
00:31:57,110 --> 00:32:01,180
words are not used what it was mean what you about the new kinds of

534
00:32:02,250 --> 00:32:07,770
why well because i'm subjective bayesian go on face some new problem i like library

535
00:32:07,770 --> 00:32:09,980
of models i might bring bear new problem

536
00:32:10,100 --> 00:32:13,840
so some some of what people worked on models of the big library

537
00:32:13,860 --> 00:32:16,270
so that's what you could do the other thing you can do is the bayesian

538
00:32:16,440 --> 00:32:21,750
know bayes rule integrate to get the denominator so better developed lots of procedures for

539
00:32:21,750 --> 00:32:25,750
integration that can be hard to do a lot of work goes into subjective bayesian

540
00:32:25,750 --> 00:32:29,460
research on immigration and anything you print talk as much about but if you have

541
00:32:29,600 --> 00:32:33,140
real subjective bayesian you really have to worry about how to get the priors it's

542
00:32:33,140 --> 00:32:37,280
not that easy and you better work at techniques for eliciting in assessing person individuals

543
00:32:37,400 --> 00:32:41,070
is a whole literature on that and a lot of bayesian neural networks and machine

544
00:32:41,070 --> 00:32:44,440
learning you don't focus on that nearly enough really are going to be these new

545
00:32:44,440 --> 00:32:46,540
better worry about how to do that

546
00:32:46,560 --> 00:32:49,800
anyway those kind of some of the main areas of research there are others with

547
00:32:49,810 --> 00:32:54,030
the main so there's a lot of focus on analysis of did my procedure work

548
00:32:54,030 --> 00:32:56,730
and so on and that's really what frequencies do because you know if you have

549
00:32:56,730 --> 00:32:57,480
the right

550
00:32:57,490 --> 00:33:00,990
input bayesian outputs could be a good one

551
00:33:01,010 --> 00:33:01,770
right so

552
00:33:01,990 --> 00:33:05,250
again you can really argue about that from a philosophical point of view it's coherent

553
00:33:05,250 --> 00:33:08,590
it's pretty it's it's nice but in practice there are really lots and lots of

554
00:33:09,500 --> 00:33:14,490
and the main one is that you know what with really complicated models are hierarchy

555
00:33:14,500 --> 00:33:19,360
is there's more than very quantities there's no matrices

556
00:33:19,380 --> 00:33:24,020
so on and so forth and all those in brain to new newport new parameters

557
00:33:24,020 --> 00:33:27,540
in the problem you know whatever a wishart distribution that whole matrix the parameters set

558
00:33:27,540 --> 00:33:28,490
in there

559
00:33:28,510 --> 00:33:30,910
you got put distribution on that

560
00:33:31,040 --> 00:33:33,230
right well that's hard

561
00:33:33,320 --> 00:33:37,280
and so the more common here model gets the more primaries and it's not to

562
00:33:37,280 --> 00:33:40,000
take you know a long time to get the domain expert to kind of say

563
00:33:40,000 --> 00:33:42,800
well i prior on that wishart thing this is this

564
00:33:42,800 --> 00:33:48,220
i know laptops in your hard drive your cellphones all of museo control codes even

565
00:33:48,220 --> 00:33:49,450
a grocery store

566
00:33:49,490 --> 00:33:52,170
so whenever you scan something like a barcode

567
00:33:52,220 --> 00:33:57,070
right if use the scanner scanner is misaligned you might misread the digits of the

568
00:33:58,280 --> 00:34:02,650
so what barcodes actually having them they have some extra parity checks

569
00:34:02,650 --> 00:34:07,130
they have some extra numbers it's say this is the first second third number should

570
00:34:07,130 --> 00:34:10,550
sum up to something maybe that's zero in march two that would be a simple

571
00:34:10,550 --> 00:34:11,800
example of

572
00:34:11,950 --> 00:34:13,800
the parity check code

573
00:34:13,820 --> 00:34:17,720
it means that when the scanners misaligned even if it gets a few digits wrong

574
00:34:17,720 --> 00:34:20,420
those parity checks allowed to correct or

575
00:34:20,420 --> 00:34:23,360
fix those errors

576
00:34:23,380 --> 00:34:26,920
now it turns out that the lord of the best codes the things that are

577
00:34:26,920 --> 00:34:28,950
actually used in practice are

578
00:34:28,970 --> 00:34:31,920
based on graphs

579
00:34:31,930 --> 00:34:37,420
here i'm showing you a a slightly different kind of graphical model that something that

580
00:34:37,420 --> 00:34:40,490
you'll also see in the literature something called the factor graph

581
00:34:40,720 --> 00:34:45,130
right so in a factor graph you have notes these are what we've seen before

582
00:34:45,130 --> 00:34:48,110
these represent variables

583
00:34:48,170 --> 00:34:52,780
in the case we're talking about these would represent things that are being transmitted for

584
00:34:53,700 --> 00:34:56,670
when your cell phone works

585
00:34:56,690 --> 00:35:01,740
it uses radio waves but to transmits essentially sequence encoded form sequence of zeros and

586
00:35:03,130 --> 00:35:07,280
so this is what's being transmitted or noisy version of this is being transmitted

587
00:35:07,300 --> 00:35:13,470
and what these boxes represent these boxes represent parity check so this box saying on

588
00:35:13,570 --> 00:35:19,400
look at one five three five and seven and whatever you send i wanted to

589
00:35:19,400 --> 00:35:23,130
be the case that that adds up to zero in march two there should be

590
00:35:23,130 --> 00:35:26,280
an even number of ones

591
00:35:27,240 --> 00:35:28,360
right so

592
00:35:28,380 --> 00:35:29,650
how many

593
00:35:29,700 --> 00:35:34,610
so it's codeword is something that satisfies all of these parity checks to to sequence

594
00:35:34,610 --> 00:35:36,190
of zeros and ones

595
00:35:36,340 --> 00:35:41,240
of length seven in this case such that the first third fifth and seventh you

596
00:35:41,240 --> 00:35:42,880
add them up is

597
00:35:42,900 --> 00:35:47,190
zero in march two and so on for the other three once

598
00:35:47,200 --> 00:35:51,470
so how many codewords how many sort of valid zero one sequences can you send

599
00:35:51,470 --> 00:35:55,530
in this case

600
00:35:55,550 --> 00:36:05,820
right there's two to the seventh possible sequences but not all of them are valid

601
00:36:05,820 --> 00:36:11,340
right this guy is not valid are one one is not valid why is that

602
00:36:11,360 --> 00:36:13,900
he must violate something

603
00:36:13,920 --> 00:36:19,130
yes he would violate this parity check because only seven is one

604
00:36:19,190 --> 00:36:23,050
and five three and one or zero so he has an even number of one

605
00:36:23,070 --> 00:36:27,200
so he would not be valid

606
00:36:27,220 --> 00:36:30,670
right so the total number of codewords to to the four

607
00:36:30,690 --> 00:36:34,860
he started with two to this seven sort of had seven degrees of freedom and

608
00:36:34,860 --> 00:36:38,360
you lost one for every parity checks seven minus three

609
00:36:38,430 --> 00:36:41,200
three checks you end up with four

610
00:36:41,240 --> 00:36:44,050
so in this case there are sixteen code words total

611
00:36:44,130 --> 00:36:49,300
that could be sent in a much larger space of two the seven

612
00:36:49,470 --> 00:36:54,030
now in practice people don't do this this was seven variables they do it with

613
00:36:54,030 --> 00:36:59,150
something like ten thousand one hundred thousand variables and

614
00:36:59,200 --> 00:37:03,740
what really has revolutionized and what's implemented and in many other devices is the sum

615
00:37:03,740 --> 00:37:05,130
product algorithm

616
00:37:05,150 --> 00:37:08,150
so many people the sum product algorithm

617
00:37:10,170 --> 00:37:15,420
you're OK understand so when to talk about it tomorrow it's a message passing algorithm

618
00:37:15,470 --> 00:37:20,420
what's important about the message passing it means that it does local operations at the

619
00:37:20,420 --> 00:37:21,900
nodes of the graph

620
00:37:21,930 --> 00:37:27,740
it sends things along the edges it's an extremely efficient algorithm and it's very parallelizable

621
00:37:27,800 --> 00:37:31,630
section many of your cellphones are running a version of sum product right now

622
00:37:31,650 --> 00:37:36,260
and it's very very fast because obviously if you're speaking you want to decode the

623
00:37:36,260 --> 00:37:37,930
class a will be

624
00:37:37,930 --> 00:37:40,200
at the three seven and twenty

625
00:37:40,350 --> 00:37:42,880
class b will pick up tons five

626
00:37:42,900 --> 00:37:47,350
twenty five thirteen and therefore i'm going to be able to differentiate between them but

627
00:37:47,350 --> 00:37:51,210
let's hope there's no indication that will happen

628
00:37:51,220 --> 00:37:54,000
so we actually in right

629
00:37:54,720 --> 00:37:58,540
that we're not going to hope for these women forced

630
00:37:58,560 --> 00:38:02,040
well can enforce these two cases women enforce

631
00:38:02,050 --> 00:38:05,830
that if you learning to addictions for two classes they are good for one about

632
00:38:05,830 --> 00:38:07,230
forty hours

633
00:38:07,240 --> 00:38:11,210
OK because it might happen that for bikes is also good for non box you

634
00:38:11,220 --> 00:38:12,240
know that

635
00:38:12,950 --> 00:38:14,440
and we are going to impose

636
00:38:14,450 --> 00:38:20,380
the different classes use different coefficients different that when imposed that into the variational formulation

637
00:38:20,640 --> 00:38:23,010
and this is related to some works in the

638
00:38:23,060 --> 00:38:28,850
in their probabilistic approaches of generative and discriminative so this is a big

639
00:38:28,970 --> 00:38:33,210
controversy in the learning community between generative and discriminative don't want to go into the

640
00:38:33,380 --> 00:38:35,770
so let me just give you one example

641
00:38:35,800 --> 00:38:38,960
OK with multiple dictionaries

642
00:38:38,960 --> 00:38:43,200
dictionary one is because for class one but for class b

643
00:38:43,220 --> 00:38:44,850
four class two

644
00:38:44,860 --> 00:38:47,600
and they had two is going to be bad for class one and good for

645
00:38:48,610 --> 00:38:50,930
so this is what we have before

646
00:38:50,950 --> 00:38:52,390
we're gonna

647
00:38:52,420 --> 00:38:53,260
try to

648
00:38:53,290 --> 00:38:58,290
fixed the image ix approximate as good as we can with the dictionary so this

649
00:38:58,290 --> 00:38:59,750
is why we have before

650
00:38:59,770 --> 00:39:01,700
nothing is and what we had before

651
00:39:01,700 --> 00:39:05,830
but we going to be more sophisticated this is what we had before

652
00:39:06,160 --> 00:39:08,130
and we're gonna put a term

653
00:39:08,140 --> 00:39:09,610
that comes from

654
00:39:09,660 --> 00:39:14,080
regression from SVM from whatever classification you like

655
00:39:14,140 --> 00:39:17,650
and the basic idea and look at these mine was

656
00:39:17,660 --> 00:39:20,560
and this comes with supplies and this comes with a minus

657
00:39:20,650 --> 00:39:23,170
so this is encouraging

658
00:39:24,490 --> 00:39:26,190
two have smaller or

659
00:39:26,190 --> 00:39:27,480
for the right class

660
00:39:27,520 --> 00:39:30,600
and large error or for the other class

661
00:39:30,600 --> 00:39:33,370
because one comes with a plus that was the mind

662
00:39:33,420 --> 00:39:38,740
so just classical logistic regression or techniques like that so the same time i wanted

663
00:39:38,740 --> 00:39:43,460
to make sure good to be good for bikes but for everybody else

664
00:39:43,520 --> 00:39:47,350
OK so i'm not leaving this detracts

665
00:39:48,060 --> 00:39:50,470
so just some examples

666
00:39:50,480 --> 00:39:53,710
this is the standard brought texture database

667
00:39:53,730 --> 00:39:55,410
this is what you get out

668
00:39:55,460 --> 00:40:00,040
this is there are about ten percent what you get if you learn one dictionary

669
00:40:00,040 --> 00:40:01,430
per class

670
00:40:01,430 --> 00:40:02,700
but they don't know

671
00:40:02,710 --> 00:40:05,940
that they're going to be competing with the other direction

672
00:40:05,970 --> 00:40:08,680
if you tell them that they going to be competing

673
00:40:08,720 --> 00:40:11,630
you have there

674
00:40:12,330 --> 00:40:14,210
so you're telling the dictionary

675
00:40:14,220 --> 00:40:17,630
you be good for your class but the but for the rest of the same

676
00:40:18,460 --> 00:40:24,560
OK and these are basically is competing with eleven eleventh hour classes

677
00:40:26,040 --> 00:40:32,180
OK this is our right it's alright this image that actually comes better here this

678
00:40:32,180 --> 00:40:36,500
is this is with the grass database that we actually trying to learn to detect

679
00:40:36,500 --> 00:40:38,640
bikes against non bikes

680
00:40:38,650 --> 00:40:40,160
and these are the results

681
00:40:40,200 --> 00:40:42,960
basically every patch will tell me

682
00:40:42,980 --> 00:40:48,160
if it survives or not by going through that part and we basically draw this

683
00:40:48,160 --> 00:40:52,520
that's hot pixels so here is to identify regions that have a very high chance

684
00:40:52,520 --> 00:40:53,700
of being

685
00:40:53,750 --> 00:40:55,290
what do i mean by that

686
00:40:55,300 --> 00:40:58,910
these parties are very well represented with the bikes

687
00:40:58,950 --> 00:41:01,410
dictionary very badly represented

688
00:41:01,430 --> 00:41:03,760
within on bike bikes dictionary

689
00:41:03,850 --> 00:41:06,730
and similar for this other images

690
00:41:07,740 --> 00:41:10,530
and once again i can show you we show in the paper that if we

691
00:41:10,530 --> 00:41:14,460
don't impose that the results are not as good

692
00:41:14,520 --> 00:41:17,220
let me give you a different example

693
00:41:17,220 --> 00:41:22,150
in the previous case i say let's learn dictionaries that compete with each other

694
00:41:22,150 --> 00:41:24,860
you can do is to of learning the dictionary

695
00:41:24,900 --> 00:41:28,950
you can actually impose that it with efficiency use for class one

696
00:41:28,990 --> 00:41:32,620
are different i they could that the coefficients used by class two

697
00:41:33,470 --> 00:41:35,240
so basically

698
00:41:35,250 --> 00:41:39,720
this is this is just one metric that we use but the basic idea is

699
00:41:39,720 --> 00:41:43,720
i one day vector for class one to be orthogonal to the alpha victor for

700
00:41:45,010 --> 00:41:47,570
OK just as one example you can put

701
00:41:47,610 --> 00:41:51,360
other things here everything makes the there

702
00:41:51,410 --> 00:41:55,720
optimisation of this even more complicated and i am skipping that part

703
00:41:55,800 --> 00:41:59,940
but the basic idea is that you can put the task

704
00:41:59,940 --> 00:42:01,620
in your optimisation

705
00:42:01,730 --> 00:42:05,900
the reconstruction one b is good because the picture has to sacrifice

706
00:42:06,700 --> 00:42:09,610
but the classification would be much better

707
00:42:09,660 --> 00:42:13,150
and if you want to flag and show you reconstructions with their

708
00:42:13,160 --> 00:42:18,420
pure reconstructive dictionary so reconstructed plus gasification to another school

709
00:42:19,970 --> 00:42:23,100
let me just give the results it works OK

710
00:42:23,130 --> 00:42:27,150
but the but i want to just give you the concept and you can combine

711
00:42:27,150 --> 00:42:30,750
both concepts of learning dictionaries that are

712
00:42:30,780 --> 00:42:35,670
separating and learning outputs that are separating and this is related to a paper and

713
00:42:35,670 --> 00:42:42,080
we had last year NIPS with chilean and and our colleagues and once again i

714
00:42:42,080 --> 00:42:44,150
think the important part is to see

715
00:42:44,150 --> 00:42:46,520
if you put the discrimination

716
00:42:46,540 --> 00:42:50,830
so this is for the for this dataset for the digits dataset if you don't

717
00:42:50,960 --> 00:42:54,800
the discrimination you get the four percent error the moment you step

718
00:42:54,820 --> 00:42:59,570
and then we learned dictionaries for discrimination you go down to one percent

719
00:43:00,220 --> 00:43:02,520
and it's more fair because i'm telling

720
00:43:02,530 --> 00:43:09,080
it is unfair to use these because my optimisation they might task now my optimisations

721
00:43:09,080 --> 00:43:10,860
including my task

722
00:43:10,870 --> 00:43:12,480
so some more fair

723
00:43:17,460 --> 00:43:19,060
that was the second part

724
00:43:19,070 --> 00:43:21,240
we're going learn

725
00:43:21,320 --> 00:43:26,550
to classify or to other task so the last part of the dogs

726
00:43:26,600 --> 00:43:28,730
is to learn to sense

727
00:43:28,740 --> 00:43:31,360
and that's related to compressed sensing

728
00:43:32,390 --> 00:43:35,050
compressed sensing say the following

729
00:43:35,110 --> 00:43:36,500
just very

730
00:43:36,540 --> 00:43:41,830
rough waters saying that amount of sensing them out of samples need from the signal

731
00:43:41,880 --> 00:43:43,690
is proportional

732
00:43:43,690 --> 00:43:46,610
two this sparsity level of that scene

733
00:43:46,650 --> 00:43:48,470
OK not tonight nyquist

734
00:43:48,530 --> 00:43:51,640
but is proportional to the sparsity level of that season

735
00:43:53,000 --> 00:43:56,300
that means that in the more sparse signal is

736
00:43:56,370 --> 00:44:01,990
there's some unit so that we have been addressing already because i've been creating dictionaries

737
00:44:01,990 --> 00:44:02,810
to get

738
00:44:02,830 --> 00:44:04,780
sigma is very sparse

739
00:44:05,500 --> 00:44:07,040
so we have been doing

740
00:44:07,050 --> 00:44:08,380
a service

741
00:44:08,410 --> 00:44:10,650
to this already without

742
00:44:10,700 --> 00:44:13,510
doing it on purpose but it came out

743
00:44:14,300 --> 00:44:16,800
that's a request from the dictionary

744
00:44:16,810 --> 00:44:19,010
now how are we going to sense

745
00:44:19,060 --> 00:44:22,810
part of the beauty of the theory is that most of the compressed sensing literature

746
00:44:22,810 --> 00:44:28,100
is about sensing with random actresses you get your image you multiply by random image

747
00:44:28,110 --> 00:44:31,460
by random matrix and that's the way to sense

748
00:44:31,550 --> 00:44:35,690
there are a couple of reasons for random sampling one universality

749
00:44:35,700 --> 00:44:39,440
it was for the for every signal and the other is because the inverse

750
00:44:39,460 --> 00:44:41,780
problem is very stable

751
00:44:41,800 --> 00:44:47,010
but we like to learn things we don't like to take off the shelf dictionaries

752
00:44:47,010 --> 00:44:50,260
or off-the-shelf sensing mattresses

753
00:44:50,280 --> 00:44:55,080
so the first question is show we other the senses to the data type and

754
00:44:55,080 --> 00:44:59,240
OK so we can start by the first thing i would like to thank the

755
00:44:59,240 --> 00:45:02,050
european community for supporting

756
00:45:02,060 --> 00:45:04,560
me through my body

757
00:45:04,570 --> 00:45:10,510
and so european fellowship so in this presentation i will give you an introduction to

758
00:45:10,510 --> 00:45:13,280
graphical models so let's start by

759
00:45:13,370 --> 00:45:19,390
defining graphical models and give a little bit of motivation why we want to

760
00:45:19,410 --> 00:45:23,140
to to know about them so graphical monocyte graphs

761
00:45:23,160 --> 00:45:30,660
o which contain links and nodes in order represent random variables

762
00:45:30,660 --> 00:45:32,810
y means represents

763
00:45:32,940 --> 00:45:42,730
statistical dependencies between the variables so graphical model give us provide us with these what

764
00:45:43,440 --> 00:45:47,080
for reasoning under uncertainty

765
00:45:47,080 --> 00:45:48,880
and as we know

766
00:45:48,900 --> 00:45:56,840
machine learning and related disciplines which are concerned modelling of the water is that information

767
00:45:56,840 --> 00:46:00,050
about the time

768
00:46:00,100 --> 00:46:07,680
uncertainty arises mainly from two sources and our limited understanding of the four and the

769
00:46:07,680 --> 00:46:14,380
limited amount of data that is available to us therefore having methods which account for

770
00:46:14,430 --> 00:46:19,040
uncertainty is very important

771
00:46:19,040 --> 00:46:23,490
but why is it that these are the representation important

772
00:46:23,580 --> 00:46:28,820
what about when point can we do with that graphical models allow us to

773
00:46:30,290 --> 00:46:36,960
answer to question about independent random variables by just looking at the structure of the

774
00:46:36,960 --> 00:46:39,790
graph and therefore

775
00:46:39,800 --> 00:46:47,850
without requiring complex algebraic manipulation and in machine learning and related disciplines

776
00:46:47,850 --> 00:46:52,710
when we have a probabilistic model of something we normally

777
00:46:52,710 --> 00:46:57,730
i have to deal with a huge number of articles so

778
00:46:57,740 --> 00:47:02,580
i did break my feliciano despite what is often very very complicated

779
00:47:02,650 --> 00:47:05,830
graphical models also fine

780
00:47:06,250 --> 00:47:11,070
i use the full because they enable us to define general i agree that form

781
00:47:11,430 --> 00:47:14,920
fee efficiently inference problems

782
00:47:14,930 --> 00:47:20,300
as a consequence of that we can say that the graphical model provided us with

783
00:47:20,580 --> 00:47:29,210
a framework of common framework for representing and understanding the properties of probabilistic models and

784
00:47:29,420 --> 00:47:31,780
enable us to

785
00:47:31,800 --> 00:47:34,830
late modernist which were developed

786
00:47:34,840 --> 00:47:45,150
independently in different communities such as for example statistics physics mission that control theory and

787
00:47:45,150 --> 00:47:51,170
engineering and other many other communities and they have also isolated in my opinion

788
00:47:51,180 --> 00:47:56,900
progress in modelling because they have enabled us to have overcome

789
00:47:57,030 --> 00:47:59,110
some difficulties

790
00:47:59,120 --> 00:48:04,210
there are many types of graphical models in the literature and today will focus on

791
00:48:04,210 --> 00:48:09,680
mainly on belief networks markov networks and factor graphs

792
00:48:09,830 --> 00:48:16,100
so the yesterday they gave a very nice introduction about probability the passion so i

793
00:48:16,100 --> 00:48:22,240
will go very quickly through detroit we've seen these already so i just

794
00:48:22,240 --> 00:48:25,800
i remind you that if we are interested in

795
00:48:25,860 --> 00:48:27,990
a and the

796
00:48:28,090 --> 00:48:32,120
we have a kind of prior belief about the

797
00:48:32,150 --> 00:48:34,960
and then we we observe b

798
00:48:34,990 --> 00:48:39,270
we can update our belief about failing to compute the

799
00:48:39,370 --> 00:48:44,800
conditional distribution of a given b by simply multiply our prior distribution

800
00:48:45,920 --> 00:48:49,740
you will be given a divided by p of p

801
00:48:50,800 --> 00:48:55,860
also give you a little bit of refreshing about what we mean by condition

802
00:48:55,870 --> 00:49:01,400
imagine the dependence between random variables intuitively to the bible

803
00:49:01,490 --> 00:49:06,200
a and b are independent if the state law in the state of the

804
00:49:06,210 --> 00:49:12,100
one of the bible doesn't tell us anything about the state of the art body

805
00:49:12,340 --> 00:49:14,340
formally we can write that by

806
00:49:14,350 --> 00:49:19,020
think that the probability of a given b is equal to the probability of a

807
00:49:19,020 --> 00:49:22,090
or equivalently

808
00:49:22,100 --> 00:49:26,530
we can write down the joint distribution of a and b as in the factorized

809
00:49:26,530 --> 00:49:31,250
form was the distribution of times the distribution of b and similarly for

810
00:49:31,270 --> 00:49:37,420
conditional independence between a and b

811
00:49:37,460 --> 00:49:43,670
now before starting the description about graphical model we need first introduce you to the

812
00:49:43,770 --> 00:49:45,570
to the notation of graphs

813
00:49:45,650 --> 00:49:49,900
so first of all graphs consist of nodes

814
00:49:49,910 --> 00:49:56,650
they can be undirected or directed so here for example i show an undirected graph

815
00:49:56,650 --> 00:49:59,590
while here i show a directed graph

816
00:49:59,600 --> 00:50:07,250
apart from node to another node is the sequence of connected nodes

817
00:50:07,330 --> 00:50:11,360
for example

818
00:50:11,410 --> 00:50:12,480
x one

819
00:50:12,500 --> 00:50:13,320
x two

820
00:50:13,330 --> 00:50:14,150
x three

821
00:50:14,160 --> 00:50:15,640
former box

822
00:50:15,690 --> 00:50:17,270
of nodes

823
00:50:17,270 --> 00:50:19,780
which is in this case undirected

824
00:50:19,790 --> 00:50:22,770
and x one x five

825
00:50:22,780 --> 00:50:27,750
eight form apart from x one to x eight this part is that you can

826
00:50:27,750 --> 00:50:30,670
notice that in the in the second is also

827
00:50:30,690 --> 00:50:34,260
undirected part within directed graphs

828
00:50:34,270 --> 00:50:40,690
in order to here x two is said to be

829
00:50:40,700 --> 00:50:45,580
department of and all other nodes x five because there is direct thing from x

830
00:50:45,580 --> 00:50:49,610
two to x five is similarly x five is said to be

831
00:50:49,630 --> 00:50:52,110
child or of

832
00:50:52,130 --> 00:50:55,830
x one is an ancestor of x a

833
00:50:55,840 --> 00:50:56,830
eight states

834
00:50:56,840 --> 00:50:59,720
it seems that is that part

835
00:50:59,750 --> 00:51:01,770
starting from x one

836
00:51:02,160 --> 00:51:06,460
and nineteen at sixty eight not x one on the other hand is not an

837
00:51:06,460 --> 00:51:08,510
ancestor of axes

838
00:51:08,520 --> 00:51:11,010
because the line here is

839
00:51:11,080 --> 00:51:12,080
is in the wrong

840
00:51:13,730 --> 00:51:19,580
and similarly exceed is going to be a descendant of x five or x one

841
00:51:19,590 --> 00:51:26,210
because there is a path which and the dixie and start x have five or

842
00:51:27,400 --> 00:51:35,400
the last finisher we need is definition of a directed acyclic graph

843
00:51:35,410 --> 00:51:43,360
a graph is said i cyclic even by following the direction of the arrows in

844
00:51:43,360 --> 00:51:47,480
with like the model outright there's just that's the rationale there's no moral scruple they

845
00:51:47,480 --> 00:51:52,330
destroy these helpless but still humans were chafing under the tyrannical and unjust and uncaring

846
00:51:53,150 --> 00:51:56,940
in the biblical story when the when the israelites told the story they modified it

847
00:51:57,430 --> 00:52:02,020
it's god's uncompromising ethical standards that lead him to bring the flood in an act

848
00:52:02,020 --> 00:52:06,450
of divine justice he's punishing the evil corruption of human beings that he so lovingly

849
00:52:06,450 --> 00:52:11,040
created and whose degradation he can't bear to witness so it's it's saying something different

850
00:52:11,040 --> 00:52:13,340
it's providing a very different message

851
00:52:13,420 --> 00:52:17,080
so when we compare the bible with the literature of the ancient near east

852
00:52:17,090 --> 00:52:18,980
we'll see not only

853
00:52:19,030 --> 00:52:21,820
the incredible cultural and literary heritage

854
00:52:21,860 --> 00:52:26,640
it was obviously common to them but we'll see the ideological gulf that separated them

855
00:52:26,640 --> 00:52:32,240
and we'll see how biblical writers so beautifully and cleverly manipulated and use these stories

856
00:52:32,240 --> 00:52:36,040
as a set is the vehicle for the expression of a radically new idea

857
00:52:36,050 --> 00:52:39,460
they drew upon these sources but they blended and shape them

858
00:52:39,470 --> 00:52:41,090
in a particular way

859
00:52:41,100 --> 00:52:45,450
and that brings us to a critical problem facing anyone who

860
00:52:45,460 --> 00:52:48,330
six to reconstruct ancient israelite

861
00:52:48,340 --> 00:52:52,010
religion or culture on the basis of the biblical materials

862
00:52:52,030 --> 00:52:57,210
the that problem is the conflict in perspective

863
00:52:57,230 --> 00:53:00,850
between the final editors of the text

864
00:53:00,870 --> 00:53:04,920
and some of the older sources that are incorporated into the bible some of the

865
00:53:04,920 --> 00:53:08,070
older sources that they were obviously drawing on

866
00:53:08,150 --> 00:53:12,700
those who were responsible for the final editing the final forms of the tax

867
00:53:12,720 --> 00:53:19,520
had a decidedly monotheistic perspective at the gaumont theistic perspective and they attempted to impose

868
00:53:19,520 --> 00:53:22,160
that perspective on their older source materials

869
00:53:22,210 --> 00:53:24,700
and for the most part they were successful

870
00:53:24,710 --> 00:53:30,300
but at times the result of their efforts are deeply conflicted deeply ambiguous text and

871
00:53:30,300 --> 00:53:33,790
again that's going to be one of the most fun things for you is readers

872
00:53:33,790 --> 00:53:37,260
of this text if you're alert to it if you're ready to listen to the

873
00:53:37,260 --> 00:53:40,790
cacophony of voices that are within the text

874
00:53:40,840 --> 00:53:43,610
in many respects the bible represents

875
00:53:43,670 --> 00:53:46,370
or expresses a basic discontent

876
00:53:46,380 --> 00:53:51,700
with the larger cultural milieu in which it was produced and that's interesting for us

877
00:53:51,700 --> 00:53:55,020
because a lot of modern people have a tendency to think of the bible

878
00:53:55,110 --> 00:53:58,140
as an emblem of conservatism right we tend to think of this as an old

879
00:53:58,140 --> 00:54:01,710
fuddy-duddy document it's outdated has outdated ideas

880
00:54:01,720 --> 00:54:04,070
and i think the challenge of this course

881
00:54:04,080 --> 00:54:05,970
if you read the bible

882
00:54:05,980 --> 00:54:09,300
with fresh eyes so that you can appreciate it for

883
00:54:09,310 --> 00:54:14,310
what it was in many ways what it continues to be a revolutionary

884
00:54:14,320 --> 00:54:15,930
cultural critique

885
00:54:16,700 --> 00:54:22,160
we can read the bible with fresh and appreciative eyes only if we first technologies

886
00:54:22,160 --> 00:54:25,120
set aside some of our presuppositions about the bible

887
00:54:25,170 --> 00:54:29,430
it's really impossible in fact they do not have some opinions about this work because

888
00:54:29,430 --> 00:54:33,520
it's an intimate part of our culture so even if you've never opened it

889
00:54:33,530 --> 00:54:35,620
already yourself i bet you can

890
00:54:35,630 --> 00:54:38,410
satan line two and i for an eye tooth for tooth

891
00:54:38,430 --> 00:54:40,120
but you don't really know what it means

892
00:54:40,130 --> 00:54:42,560
the poor will always be with you i'm sure you don't really know what that

893
00:54:42,560 --> 00:54:46,510
means these are things increases that we hear and they create with and as a

894
00:54:46,510 --> 00:54:49,900
certain impression of the biblical text and how it functions

895
00:54:49,910 --> 00:54:52,360
verses are quoted alluded to

896
00:54:52,370 --> 00:54:57,660
whether to be championed and valorise or whether to be lampooned and pillory but we

897
00:54:57,660 --> 00:54:59,750
feel that we have a rough idea

898
00:54:59,800 --> 00:55:02,300
of the bible into a rough idea of its outlook

899
00:55:02,320 --> 00:55:05,490
one in fact we really have a popular misconceptions

900
00:55:05,500 --> 00:55:08,750
that come from the way in which the bible been used or misused

901
00:55:08,800 --> 00:55:11,940
most of our cherished presuppositions about

902
00:55:11,960 --> 00:55:14,580
the bible are based on astonishing claims

903
00:55:14,590 --> 00:55:17,920
that others have made on behalf of the bible claims that the bible has not

904
00:55:17,920 --> 00:55:20,120
made on behalf of itself

905
00:55:20,140 --> 00:55:24,740
so before we proceed i need to ask you to set aside for the purposes

906
00:55:24,740 --> 00:55:26,000
of this course

907
00:55:26,010 --> 00:55:29,280
some of the more common myths about the bible have

908
00:55:29,330 --> 00:55:31,590
list here for you

909
00:55:31,610 --> 00:55:33,090
the first is

910
00:55:33,100 --> 00:55:34,960
the idea that the bible the book

911
00:55:37,410 --> 00:55:40,980
the bible is not a book

912
00:55:40,990 --> 00:55:44,730
with all that that implies that it has a uniform style and message in a

913
00:55:44,730 --> 00:55:47,610
single author the sorts of things we think of when we think in the conventional

914
00:55:47,610 --> 00:55:49,210
sense of the word book

915
00:55:49,230 --> 00:55:50,830
it's a library

916
00:55:50,940 --> 00:55:52,650
it's an anthology

917
00:55:52,660 --> 00:55:57,230
of writings are books written and edited over an extensive period of time by people

918
00:55:57,230 --> 00:55:59,480
the coupling constant of the

919
00:56:00,750 --> 00:56:02,170
so you see that

920
00:56:02,190 --> 00:56:08,630
by comparing this to you get a universality lation heterotic string theory which relates

921
00:56:08,630 --> 00:56:13,980
the young males grappling in our case the grand unified coupling constant

922
00:56:14,000 --> 00:56:18,000
this scale the string scale the state should be

923
00:56:18,020 --> 00:56:20,500
the thing scale

924
00:56:20,540 --> 00:56:22,790
and thanks constant

925
00:56:24,540 --> 00:56:30,480
are known to measure the fewest assuming unification therefore there is a very precise

926
00:56:30,480 --> 00:56:34,380
prediction for what this thing scale is if you believe the meaning

927
00:56:34,590 --> 00:56:36,610
heterotic unification

928
00:56:36,670 --> 00:56:41,840
in particular note is that this volume has dropped out in their ratio that's crucial

929
00:56:41,900 --> 00:56:48,310
so it doesn't matter what this volume was this relation is universal

930
00:56:48,310 --> 00:56:53,000
now i have you done in a assumption that i just mentioned earlier views but

931
00:56:53,020 --> 00:56:57,230
there is a completely general and honest argument which

932
00:56:57,270 --> 00:57:02,690
they do that independently of how you do this compactification this universe isolationist correct in

933
00:57:02,690 --> 00:57:05,310
the heterotic string

934
00:57:05,340 --> 00:57:10,440
so the obvious meaning hypothesis is that the the string scale is the unification scale

935
00:57:10,460 --> 00:57:16,190
that's where things should meet because that's the fundamental scale string theory

936
00:57:16,210 --> 00:57:21,000
and if you buy these you see that now we have two parameters for for

937
00:57:21,000 --> 00:57:24,090
predictions as opposed to two four three eight

938
00:57:24,110 --> 00:57:29,920
because you can also predict new tons captain constant or the gravitational capture

939
00:57:29,960 --> 00:57:34,170
so in a sense you may system theory or at least this particular class of

940
00:57:34,170 --> 00:57:37,020
theories predict one more thing for you

941
00:57:37,040 --> 00:57:41,230
and this could have fallen anywhere and actually if you calculated

942
00:57:41,250 --> 00:57:44,040
it follows that into the seventeen gv

943
00:57:44,060 --> 00:57:48,130
whereas in reality it should have been two point four times ten to the eighteen

944
00:57:49,980 --> 00:57:52,590
now here you may a one of platitudes

945
00:57:52,610 --> 00:57:54,400
so well doesn't work

946
00:57:54,400 --> 00:57:58,360
but of course that's a bit too quick we don't want to throw out the

947
00:57:58,360 --> 00:58:02,340
baby with the bath as they say way because

948
00:58:02,340 --> 00:58:06,770
when you think about this prediction on a logarithmic scale it's only off by a

949
00:58:06,770 --> 00:58:08,670
few percent

950
00:58:08,750 --> 00:58:12,710
there is if you think about it there is absolutely no reason why the standard

951
00:58:14,630 --> 00:58:19,980
gauge coupling should know anything about newton's constant and here we are discovering that we

952
00:58:19,980 --> 00:58:22,230
do they need

953
00:58:22,250 --> 00:58:26,460
in such a way that they predict the gravitational coupling very close to its real

954
00:58:27,420 --> 00:58:32,690
they could have meant when we extrapolate this carries anyone else including into the forty

955
00:58:32,690 --> 00:58:35,480
TGV or someone

956
00:58:35,500 --> 00:58:40,400
so i think this is a very successful prediction of is minimal heterotic unification and

957
00:58:40,400 --> 00:58:43,590
in this sense it's the one hundred billion

958
00:58:43,610 --> 00:58:48,690
extra prediction which fits nicely in place

959
00:58:49,960 --> 00:58:51,290
i have here

960
00:58:51,310 --> 00:58:53,880
this debate is namely this

961
00:58:53,920 --> 00:59:00,210
calculation was classical you have to correct for this vessel corrections this depends on what

962
00:59:00,210 --> 00:59:03,650
there's a lot of work has gone into trying to

963
00:59:03,670 --> 00:59:08,150
improve this calculation get rid of this small discrepancy

964
00:59:08,170 --> 00:59:11,380
there's no compelling model organism two

965
00:59:11,400 --> 00:59:15,400
i believe that this discrepancy goes away in some nice

966
00:59:15,400 --> 00:59:21,060
way although there are nice ideas about how this may work but here is then

967
00:59:21,060 --> 00:59:23,670
again the this picture which i think

968
00:59:24,380 --> 00:59:30,000
they suggested regain that meaningless string unification seems to be on the right track

969
00:59:30,020 --> 00:59:36,210
here are the three weak strong and electromagnetic gauge couplings and said they meet

970
00:59:36,210 --> 00:59:41,170
and actually if you also try to promote the gravitational force on the same plot

971
00:59:41,210 --> 00:59:47,560
this has very different behavior because instead of evolving logarithmically with energy

972
00:59:47,590 --> 00:59:51,900
it involves has the power to know gravity couples to energy shows you raise the

973
00:59:51,900 --> 00:59:57,500
energy comes from different stronger on the log plot this is an exponential rise

974
00:59:57,500 --> 01:00:02,290
like any dynamical system that satisfy the constraints i talked about finite action far observation

975
01:00:02,420 --> 01:00:04,540
this week time

976
01:00:04,680 --> 01:00:13,640
mean that one

977
01:00:16,320 --> 01:00:20,970
this is not control by this is describing the system so

978
01:00:21,030 --> 01:00:24,950
every deterministic system then after one thousand years

979
01:00:25,990 --> 01:00:29,940
the deterministic system then prediction to be one of them

980
01:00:30,010 --> 01:00:35,680
but in general to stochastic systems these and all the data

981
01:00:38,450 --> 01:00:39,370
all right

982
01:00:39,370 --> 01:00:40,390
his car

983
01:00:40,400 --> 01:00:42,180
of course somebody said

984
01:00:42,230 --> 01:00:45,490
his first here is the reason why all this

985
01:00:45,500 --> 01:00:48,160
one of the coordinate system this way

986
01:00:48,300 --> 01:00:49,980
the system has the right

987
01:00:50,060 --> 01:00:52,720
which is the rank of this matrix

988
01:00:52,730 --> 01:00:53,840
we call that

989
01:00:53,850 --> 01:00:55,450
let the rank pn

990
01:00:55,560 --> 01:00:58,900
by whatever the and is capital and

991
01:00:58,910 --> 01:01:02,830
what i'm going to show you that this is a good measure of the linear

992
01:01:02,830 --> 01:01:06,130
complexity of this of the system so let's go one more step

993
01:01:07,840 --> 01:01:09,140
it is

994
01:01:09,190 --> 01:01:11,120
this one i think yes

995
01:01:11,130 --> 01:01:15,310
so what does it mean for the system to have rank and it means that

996
01:01:15,310 --> 01:01:20,850
there are at most n linearly independent columns and rows in this matrix

997
01:01:22,560 --> 01:01:27,290
let me for pictorial is imagine that the n rows the caught between these dashed

998
01:01:27,290 --> 01:01:28,730
lines are

999
01:01:28,760 --> 01:01:30,340
linearly independent

1000
01:01:30,380 --> 01:01:36,200
let me give special name to those future q one qn called what are called

1001
01:01:36,200 --> 01:01:39,830
in the core tests the core features

1002
01:01:39,850 --> 01:01:42,720
here's the point here is the key point of this

1003
01:01:42,740 --> 01:01:46,180
this this this this material

1004
01:01:46,480 --> 01:01:49,720
the fact that this is some nice matrix has rank n means the following has

1005
01:01:49,720 --> 01:01:51,060
the following implication

1006
01:01:51,070 --> 01:01:54,610
that for any history h any past

1007
01:01:54,640 --> 01:01:57,620
if i give you these and numbers

1008
01:01:57,670 --> 01:02:00,140
then i can make any predictions

1009
01:02:00,290 --> 01:02:02,740
what any future from the same history

1010
01:02:02,880 --> 01:02:07,760
by linearly combining these are numbers by that's what right means

1011
01:02:07,780 --> 01:02:12,940
that is any column is linearly can put a linearly is a linear function of

1012
01:02:12,940 --> 01:02:14,350
these and columns

1013
01:02:14,360 --> 01:02:19,260
therefore if i want to make a prediction for any particular view possible future

1014
01:02:19,320 --> 01:02:21,910
it's just some linear function of these are numbers

1015
01:02:25,140 --> 01:02:29,310
so let me first that right to the protection of any caste t any arbitrator

1016
01:02:30,570 --> 01:02:35,810
any arbitrary future all in this history h is a linear function of the the

1017
01:02:35,830 --> 01:02:39,950
numbers let me give this special name that's p of q given HEQ is that

1018
01:02:39,980 --> 01:02:44,700
set of special test score test is the vector numbers

1019
01:02:44,720 --> 01:02:49,900
brigadier you know in april and and seventy is the set up is the parameters

1020
01:02:49,940 --> 01:02:51,600
that compute

1021
01:02:51,640 --> 01:02:53,970
this answer for this test e

1022
01:02:54,240 --> 01:02:55,910
the critical thing is

1023
01:02:55,950 --> 01:02:59,100
that ends up t is completely independent of history

1024
01:02:59,140 --> 01:03:03,130
that is no matter what their what history it was

1025
01:03:03,400 --> 01:03:05,670
to get the answer for this test

1026
01:03:05,810 --> 01:03:08,830
requires the same linear combination

1027
01:03:08,860 --> 01:03:11,620
by this definition right

1028
01:03:11,660 --> 01:03:15,680
OK so what i what i just showed you what i've just shown you is

1029
01:03:15,680 --> 01:03:17,940
that these and numbers

1030
01:03:18,990 --> 01:03:21,200
all the information there is

1031
01:03:21,870 --> 01:03:25,490
arbitrary histories

1032
01:03:25,550 --> 01:03:31,260
because i can predict anything about the future i want from just these are numbers

1033
01:03:35,510 --> 01:03:37,570
it is their relationships

1034
01:03:37,570 --> 01:03:40,550
two this has that flavor yes

1035
01:03:40,560 --> 01:03:48,410
i mean i mean no i mean i made the

1036
01:03:48,430 --> 01:03:49,680
and could be infinite

1037
01:03:49,800 --> 01:03:53,450
that's exactly what i didn't tell you what it is and could potentially be infinite

1038
01:03:53,460 --> 01:03:57,140
OK let me anticipate the result of that work my way do it

1039
01:03:57,260 --> 01:04:00,650
the result of the the main result that we have on the discrete sites is

1040
01:04:00,650 --> 01:04:04,920
the following that if you have any system that can be modeled by a partially

1041
01:04:04,920 --> 01:04:09,480
observable markov decision process think humans i think many of you have more knowledge about

1042
01:04:09,890 --> 01:04:14,450
and ponte piece of engagements any HMM any system that can be modeled by an

1043
01:04:14,450 --> 01:04:17,910
HMM with underlying and states

1044
01:04:17,970 --> 01:04:20,390
well i rank no more than

1045
01:04:20,460 --> 01:04:26,650
so i'm going to show you

1046
01:04:26,680 --> 01:04:30,810
is that we have an alternative representation to HMM upon the piece

1047
01:04:30,820 --> 01:04:33,640
it doesn't use any hidden variables

1048
01:04:33,760 --> 01:04:36,720
the same dimensionality as hmm

1049
01:04:36,740 --> 01:04:39,530
after the same cluster systems

1050
01:04:39,530 --> 01:04:42,660
but without using any hidden variables at all

1051
01:04:42,710 --> 01:04:47,140
because that's where i'm going

1052
01:04:47,140 --> 01:04:52,700
right well i hope how many of you know what a tremendous

1053
01:04:52,720 --> 01:04:53,690
not all of you

1054
01:04:53,740 --> 01:04:56,440
OK some of you

1055
01:04:56,450 --> 01:05:01,370
OK i'll i'll talk about politics led mdps you have a few minutes and that's

1056
01:05:01,370 --> 01:05:05,480
when going i'm trying to build a model of dynamical systems that would otherwise the

1057
01:05:05,480 --> 01:05:11,360
model is a tremendous upon peace but without using any any hidden variables at all

1058
01:05:11,400 --> 01:05:15,050
so i have yet to find the model completely but

1059
01:05:15,090 --> 01:05:18,490
i'm going to keep the key concept for this concept of the rank of this

1060
01:05:22,730 --> 01:05:29,170
so so but i can be something which is that my representation of state

1061
01:05:29,180 --> 01:05:32,540
it is going to be these are numbers

1062
01:05:32,650 --> 01:05:34,580
in order to nothing in this

1063
01:05:34,600 --> 01:05:38,330
there's no reference to any hidden variables all in this matrix

1064
01:05:38,430 --> 01:05:41,710
but there's no notion of underlying unobserved states

1065
01:05:41,740 --> 01:05:48,740
everything is expressed in terms of statistics of observable quantities

1066
01:05:49,360 --> 01:05:53,550
so they show you some some results like the restructuring of the power of this

1067
01:05:53,550 --> 01:05:59,050
idea which is some results suppose you have a markov decision process

1068
01:05:59,070 --> 01:06:00,450
with n states

1069
01:06:00,640 --> 01:06:02,920
i'm going to show you the rank of the

1070
01:06:02,930 --> 01:06:07,250
system dynamics matrix corresponding to the MDP copy more than

1071
01:06:07,320 --> 01:06:08,360
not the case

1072
01:06:08,370 --> 01:06:12,360
pretty straightforward right so they give you a markov decision processes and states what that

1073
01:06:12,360 --> 01:06:17,300
mean that means that the future from any history that and in the same state

1074
01:06:17,370 --> 01:06:19,220
is going to be identical

1075
01:06:19,240 --> 01:06:24,280
but that would mean for the probably markov that the last observation determines the entire

1076
01:06:24,280 --> 01:06:28,870
if these approach affixed to exceed sixty things and this is the big table that

1077
01:06:28,950 --> 01:06:31,990
was showing you what you know this is simply the probability of a given x

1078
01:06:31,990 --> 01:06:32,510
one x two

1079
01:06:32,990 --> 01:06:36,740
you're in all these factors as factors tables have been given to u and i'm

1080
01:06:36,740 --> 01:06:38,430
just going in fact is explicitly

1081
01:06:39,100 --> 01:06:42,390
so so then what would do is that i'm going to write the joint distribution

1082
01:06:43,410 --> 01:06:47,640
as a product of local functions i'm doing an obstruction right and the reason i'm

1083
01:06:47,640 --> 01:06:49,740
doing an abstraction is that if you don't understand

1084
01:06:50,320 --> 01:06:52,300
why low-density parity-check codes

1085
01:06:52,680 --> 01:06:54,990
and viterbi decoding on the same algorithm

1086
01:06:55,450 --> 01:07:00,450
units abstraction because through this fractional understand that this thing has been rediscovered so many times

1087
01:07:01,050 --> 01:07:03,870
so this is a joint distribution px y of x one x two x three

1088
01:07:03,870 --> 01:07:07,260
and this is the exact same formula just plug these back and you will get

1089
01:07:07,260 --> 01:07:08,050
the joint distribution

1090
01:07:08,890 --> 01:07:09,930
what i managed to do

1091
01:07:10,430 --> 01:07:14,510
is it by introducing these factors and i have introduced all the right places you

1092
01:07:14,510 --> 01:07:18,280
know this is the vector which includes x three and exports about this vector so

1093
01:07:18,280 --> 01:07:20,780
this fact nor will connect those to be able

1094
01:07:21,490 --> 01:07:23,590
i can actually write it as a bipartite graph

1095
01:07:24,820 --> 01:07:28,550
right with factors on one side and variables on other side

1096
01:07:29,070 --> 01:07:34,240
and this back the the simply show which variables participate in which factors

1097
01:07:35,160 --> 01:07:35,850
the hope is

1098
01:07:36,570 --> 01:07:39,200
the factors involved a small number of variables

1099
01:07:39,570 --> 01:07:41,740
each factory was a small number of variables

1100
01:07:42,450 --> 01:07:46,910
there can be many factors many variables that's okay what i'm asking for each factor

1101
01:07:46,910 --> 01:07:48,720
only involves a small number of we ts

1102
01:07:49,970 --> 01:07:52,390
it's a product of local functions that's what i'm calling it

1103
01:07:53,930 --> 01:07:56,350
if that's indeed true there a very efficient way

1104
01:07:56,930 --> 01:07:59,320
to some this kind of thing over here

1105
01:07:59,950 --> 01:08:01,780
let's say everything other than x one

1106
01:08:02,300 --> 01:08:03,320
or something on that's are

1107
01:08:04,160 --> 01:08:07,330
so the problem is we mostly interested in and this is the inference problem

1108
01:08:07,910 --> 01:08:09,030
is that's given

1109
01:08:09,870 --> 01:08:14,280
a product of functions product of local functions they inference problem is simply

1110
01:08:14,910 --> 01:08:15,970
to marginalize

1111
01:08:17,370 --> 01:08:18,890
i want to some all

1112
01:08:19,390 --> 01:08:20,370
x two x three

1113
01:08:20,760 --> 01:08:21,830
x fourteen x five

1114
01:08:23,970 --> 01:08:29,100
so so i want to get your fix one what what remains on x one

1115
01:08:29,910 --> 01:08:33,720
i want to marginalize out all the other guys by taking into account the dependencies

1116
01:08:34,450 --> 01:08:38,160
now if you'd just write some missions outside the force missions outside

1117
01:08:38,550 --> 01:08:42,200
if there are any variables and each each variable x two values

1118
01:08:42,820 --> 01:08:44,410
then we talked about in some missions

1119
01:08:45,010 --> 01:08:49,070
right to do then comes in the summation and that's an exponential time algorithms if

1120
01:08:49,070 --> 01:08:50,760
you're trying to do is to do this in my day

1121
01:08:51,510 --> 01:08:55,620
it's an exponential time algorithm however you soon realize that you can push

1122
01:08:56,100 --> 01:08:57,970
some of the missions inside

1123
01:08:58,550 --> 01:08:59,600
it has further to go

1124
01:09:00,010 --> 01:09:01,410
right so so export

1125
01:09:01,470 --> 01:09:05,070
can be summed up by purely summing this combat everything other than next three

1126
01:09:05,780 --> 01:09:09,640
x five can some out what that supporting the summation inside

1127
01:09:10,970 --> 01:09:11,620
have a name

1128
01:09:11,870 --> 01:09:13,740
in mathematics called the distributive law

1129
01:09:15,240 --> 01:09:16,010
with simplices

1130
01:09:16,570 --> 01:09:17,300
that people are

1131
01:09:17,370 --> 01:09:21,490
please see is eight times people see why am i going for kids and that's

1132
01:09:21,740 --> 01:09:23,050
the reason i'm going for this

1133
01:09:23,800 --> 01:09:26,950
is that what we are trying to come up with efficient algorithms

1134
01:09:27,530 --> 01:09:29,660
let's come the computations on the left side

1135
01:09:30,660 --> 01:09:33,450
i have modifications and one addition

1136
01:09:34,430 --> 01:09:35,220
on the left side

1137
01:09:35,990 --> 01:09:42,100
i have one multiplication and one addition on the right-hand side distributive law is a brilliant competition

1138
01:09:43,800 --> 01:09:44,140
right so

1139
01:09:44,640 --> 01:09:49,990
so this whole idea stands at how use distributed line a clever way and this

1140
01:09:49,990 --> 01:09:52,870
is about pushing sums has the real goal

1141
01:09:53,600 --> 01:09:58,510
you know not repeat computations dynamic programming one one right so so this is this

1142
01:09:58,510 --> 01:09:59,430
is what we're trying to do

1143
01:09:59,990 --> 01:10:04,370
but we need how do it far like the examples we can of course do

1144
01:10:04,370 --> 01:10:08,820
it i'm gonna show you examples with millions of variables in a in a few

1145
01:10:09,820 --> 01:10:11,010
but we have to pull this off

1146
01:10:11,490 --> 01:10:13,850
you can't write it out and try to push the sum so we need an

1147
01:10:13,850 --> 01:10:17,530
algorithm i'm not going to go into the details of the algorithm

1148
01:10:18,350 --> 01:10:21,320
but but it's in the best sense of the algorithm is bad

1149
01:10:22,890 --> 01:10:24,830
if you're think the bipartite graph

1150
01:10:24,830 --> 01:10:26,280
all that stuff

1151
01:10:26,410 --> 01:10:33,660
over q one

1152
01:10:36,820 --> 01:10:38,220
now the first law

1153
01:10:38,270 --> 01:10:41,220
is going to hold in all of these steps and we're going around in a

1154
01:10:42,320 --> 01:10:46,600
right so what does that tell us about of the state function like you what

1155
01:10:46,600 --> 01:10:50,920
delta u going around the whole thing

1156
01:10:50,950 --> 01:10:54,680
of course in the answer what's delta u

1157
01:10:56,350 --> 01:10:59,050
MIT students yes

1158
01:10:59,080 --> 01:11:01,880
OK so the first

1159
01:11:02,120 --> 01:11:05,450
we know that

1160
01:11:05,500 --> 01:11:07,730
going around the cycle

1161
01:11:08,720 --> 01:11:09,990
delta u

1162
01:11:17,460 --> 01:11:19,580
and that has to equal

1163
01:11:19,630 --> 01:11:23,500
q plus w summed up for all the steps

1164
01:11:25,270 --> 01:11:26,860
which is to say

1165
01:11:26,870 --> 01:11:28,550
q one

1166
01:11:28,590 --> 01:11:31,100
classic youtube

1167
01:11:31,110 --> 01:11:35,880
the album minus w one plus w one last year two

1168
01:11:35,900 --> 01:11:37,550
was w two

1169
01:11:45,830 --> 01:11:50,270
so that means we can re write this efficiency we can replace this sum

1170
01:11:50,280 --> 01:11:53,170
by just the sum of q one plus q two

1171
01:11:53,210 --> 01:11:54,300
right so

1172
01:11:56,950 --> 01:11:57,990
q one

1173
01:11:58,820 --> 01:11:59,950
q two

1174
01:12:00,070 --> 01:12:03,320
over q one

1175
01:12:03,370 --> 01:12:06,450
or you can write it as one

1176
01:12:06,490 --> 01:12:08,360
plus you too

1177
01:12:08,530 --> 01:12:15,050
or q one

1178
01:12:18,670 --> 01:12:20,770
that already tells us

1179
01:12:20,820 --> 01:12:24,920
what we know which is the efficiency is going to be something less than zero

1180
01:12:25,680 --> 01:12:27,080
because we've got one

1181
01:12:27,080 --> 01:12:29,680
and then read in q two and q one

1182
01:12:29,770 --> 01:12:33,860
two it but we got negative q two is positive

1183
01:12:33,920 --> 01:12:36,750
number right he was flown in this way

1184
01:12:36,770 --> 01:12:40,070
so q the heat in this way is negative

1185
01:12:40,110 --> 01:12:44,750
i q two i q one is negative q two course positivity flowing from the

1186
01:12:44,800 --> 01:12:46,720
reservoir two engine

1187
01:12:46,880 --> 01:12:52,270
so that efficiency is something less than one would like to figure out what that

1188
01:12:57,150 --> 01:12:59,530
let's just

1189
01:12:59,590 --> 01:13:00,930
state that

1190
01:13:03,130 --> 01:13:04,250
well now

1191
01:13:04,320 --> 01:13:09,520
let's go back to each of individual steps somewhat based on what we know about

1192
01:13:10,080 --> 01:13:15,360
the how about the weight of the dynamic changes that take place here let's look

1193
01:13:15,360 --> 01:13:18,070
at each one of the steps and see what happens

1194
01:13:23,050 --> 01:13:32,780
from one to two

1195
01:13:32,820 --> 01:13:34,410
it's isothermal

1196
01:13:34,420 --> 01:13:38,320
and now we're going to specify what to do a coral cycle for an ideal

1197
01:13:39,970 --> 01:13:43,370
it's an ideal gas is an isothermal change

1198
01:13:43,430 --> 01:13:47,600
what's delta u

1199
01:13:47,670 --> 01:13:51,310
you know it's like like lots of course is in all sorts of things that

1200
01:13:51,310 --> 01:13:52,400
you sing

1201
01:13:52,440 --> 01:13:54,900
there are always the same

1202
01:13:54,910 --> 01:13:57,060
what delta u

1203
01:13:59,470 --> 01:14:03,670
delta u is zero

1204
01:14:03,780 --> 01:14:04,980
and it's also

1205
01:14:05,000 --> 01:14:09,040
equal to this so that says you want is

1206
01:14:09,050 --> 01:14:11,910
the opposite of w one

1207
01:14:12,910 --> 01:14:14,400
it's an isothermal

1208
01:14:19,600 --> 01:14:21,080
w is just

1209
01:14:21,090 --> 01:14:25,090
native pdvt sort this is just the integral from

1210
01:14:25,100 --> 01:14:26,810
one two p

1211
01:14:30,030 --> 01:14:31,910
and it's an ideal gas

1212
01:14:31,920 --> 01:14:34,580
isothermal right

1213
01:14:34,620 --> 01:14:37,730
rt over they were

1214
01:14:37,820 --> 01:14:42,050
make it from all gas are times t one

1215
01:14:42,450 --> 01:14:45,820
and then we'll have dv over v

1216
01:14:45,830 --> 01:14:47,970
that's just the logo of the two

1217
01:14:47,980 --> 01:14:50,730
v one

1218
01:14:52,840 --> 01:14:56,530
one more

1219
01:14:56,590 --> 01:14:59,430
n equals one

1220
01:15:00,410 --> 01:15:03,910
two going to three

1221
01:15:03,950 --> 01:15:05,680
that's this is a back

1222
01:15:06,430 --> 01:15:09,780
so delta use just equal

1223
01:15:09,810 --> 01:15:11,310
so the work

1224
01:15:12,630 --> 01:15:16,150
we also know what happens because the temperature is changing from t one to t

1225
01:15:21,690 --> 01:15:24,950
it's just a CV time t two

1226
01:15:24,960 --> 01:15:26,970
minus he one

1227
01:15:27,030 --> 01:15:31,330
he received EDT and ideal gas

1228
01:15:31,700 --> 01:15:38,750
and that's equal to w one prime

1229
01:15:44,430 --> 01:15:47,570
this is the reversible adiabatic passed

1230
01:15:47,610 --> 01:15:49,910
so there's is a relationship that i'm

1231
01:15:49,950 --> 01:15:51,670
sure you remember

1232
01:15:57,400 --> 01:15:58,300
he two

1233
01:15:58,310 --> 01:16:00,070
over c one

1234
01:16:00,150 --> 01:16:01,310
is equal to

1235
01:16:01,350 --> 01:16:03,180
the two

1236
01:16:03,230 --> 01:16:08,330
over three three don't be confused by the subscripts that we're talking about a quarter

1237
01:16:08,330 --> 01:16:12,510
is both starting at two we're going to three

1238
01:16:12,560 --> 01:16:16,720
so v two and v three the different times the temperature of the water is

1239
01:16:16,720 --> 01:16:19,810
the same as it was once restored ten t one

1240
01:16:19,810 --> 01:16:21,500
and you

1241
01:16:21,520 --> 01:16:23,830
exponentiated both sides of the

1242
01:16:23,830 --> 01:16:25,310
the inequality

1243
01:16:25,310 --> 01:16:29,230
and because the exponential is a bijective function

1244
01:16:29,250 --> 01:16:32,560
this is true for any variable x

1245
01:16:35,270 --> 01:16:38,840
the probability that x larger than is is the same as the probability that the

1246
01:16:40,180 --> 01:16:43,520
lambda x is larger than exponential and that he

1247
01:16:43,520 --> 01:16:46,250
you apply chernoff again and that's what you get

1248
01:16:46,270 --> 01:16:49,410
and since this is true for any lambda

1249
01:16:49,410 --> 01:16:53,480
it's also true for the infimum over land

1250
01:17:01,430 --> 01:17:04,180
so this is

1251
01:17:04,270 --> 01:17:08,770
i'm not saying that we will use all of this but just these are inequality

1252
01:17:08,810 --> 01:17:13,600
you might encounter everywhere if you play around with their probabilities

1253
01:17:13,620 --> 01:17:20,190
so it's good to know them

1254
01:17:20,250 --> 01:17:21,120
OK so

1255
01:17:21,140 --> 01:17:22,830
now let's come back to

1256
01:17:22,870 --> 01:17:24,350
our topic

1257
01:17:24,350 --> 01:17:27,710
we wanted to see if i come back a few slides

1258
01:17:27,730 --> 01:17:28,980
we wanted to

1259
01:17:31,520 --> 01:17:36,410
this quantity the expected loss to the empirical loss

1260
01:17:36,460 --> 01:17:38,810
so one thing that we will do now

1261
01:17:38,830 --> 01:17:45,140
is to rewrite this into form which is slightly more convenient i mean it's there's

1262
01:17:45,140 --> 01:17:47,730
no magic here but it's just that is

1263
01:17:47,750 --> 01:17:51,180
comm more convenient to see what it means

1264
01:17:51,180 --> 01:17:53,190
so we introduce

1265
01:17:53,210 --> 01:17:55,450
some kind of meat are random variables

1266
01:17:55,450 --> 01:18:00,390
which is the pair x of x i y i write so this

1267
01:18:00,460 --> 01:18:02,960
pair here we call it the i

1268
01:18:03,000 --> 01:18:06,180
and same x y with quality

1269
01:18:06,230 --> 01:18:09,250
and in addition to that

1270
01:18:09,270 --> 01:18:10,330
we define

1271
01:18:10,350 --> 01:18:15,480
another set of functions so remember we had this set of classifiers which were functions

1272
01:18:15,480 --> 01:18:16,410
from x

1273
01:18:16,450 --> 01:18:18,350
to y

1274
01:18:18,370 --> 01:18:20,430
now we define functions from

1275
01:18:20,480 --> 01:18:21,980
x times y

1276
01:18:23,000 --> 01:18:25,370
the real numbers

1277
01:18:25,370 --> 01:18:27,270
here it would be zero one

1278
01:18:27,290 --> 01:18:31,060
that actually correspond to taking the loss

1279
01:18:31,080 --> 01:18:32,770
so i should have written

1280
01:18:32,790 --> 01:18:34,520
the small and here but

1281
01:18:34,520 --> 01:18:40,730
now we're kind of restrict ourselves to the the binary classification setting so it's the

1282
01:18:40,730 --> 01:18:43,390
indicator that your text is not equal to y

1283
01:18:45,210 --> 01:18:46,520
OK so this is

1284
01:18:46,540 --> 01:18:49,230
the loss of pain by my classifier

1285
01:18:50,980 --> 01:18:53,160
and for each classifier in

1286
01:18:53,370 --> 01:18:57,890
capital city i define such an and i take the set of all such as

1287
01:18:57,890 --> 01:19:00,770
and i couldn't capital

1288
01:19:02,620 --> 01:19:06,810
now once i have done this

1289
01:19:06,830 --> 01:19:08,500
of course i can

1290
01:19:08,540 --> 01:19:09,750
we're right

1291
01:19:09,770 --> 01:19:14,500
the loss as the expected value of f

1292
01:19:14,520 --> 01:19:17,350
computed at x y or z

1293
01:19:17,460 --> 01:19:19,660
and the empirical loss

1294
01:19:19,710 --> 01:19:23,450
as the average value of f computed at

1295
01:19:23,460 --> 01:19:25,250
the data

1296
01:19:26,520 --> 01:19:29,660
OK it's just a simplification of notation

1297
01:19:30,950 --> 01:19:34,060
expectation of functions sometimes denoted by

1298
01:19:34,080 --> 01:19:37,250
p where p is the distribution of this function

1299
01:19:37,310 --> 01:19:40,020
and the empirical expectation

1300
01:19:41,190 --> 01:19:47,140
five years after that as pn f and n is usually called the empirical distribution

1301
01:19:47,140 --> 01:19:50,310
so it's kind of your some of diracs

1302
01:19:50,350 --> 01:19:53,250
centre that the data

1303
01:19:56,600 --> 01:19:57,890
so is just

1304
01:19:57,890 --> 01:19:59,540
to avoid having to

1305
01:19:59,540 --> 01:20:05,060
fridays and x and y now we just have some of f of z

1306
01:20:06,660 --> 01:20:10,410
some of the above the i here an expectation of f of z

1307
01:20:11,640 --> 01:20:15,480
and of course we can go back and forth because like for each f in

1308
01:20:15,480 --> 01:20:21,830
f there exists g corresponding to it then for each t there is a christmas

1309
01:20:23,330 --> 01:20:24,730
once we have done that

1310
01:20:24,730 --> 01:20:32,210
if we look at these quantities so this is nothing but lost minus empirical loss

1311
01:20:32,270 --> 01:20:35,290
OK so it's this it's around and viable

1312
01:20:35,310 --> 01:20:39,460
and if we look at the collection of all random variables that we obtain when

1313
01:20:39,460 --> 01:20:40,770
we vary

1314
01:20:40,790 --> 01:20:41,770
the function

1315
01:20:41,770 --> 01:20:46,120
g in in capital g or f in capital and equivalently

1316
01:20:46,390 --> 01:20:51,480
then we obtain what is called an empirical process what is a process is a

1317
01:20:51,480 --> 01:20:55,560
collection of random variables and an empirical process

1318
01:20:55,620 --> 01:20:59,310
is a collection of such random variables for which each random bible

1319
01:20:59,330 --> 01:21:04,710
as the so-called empirical distribution which means it's the difference between

1320
01:21:04,730 --> 01:21:09,000
and expected value and an empirical average

1321
01:21:09,160 --> 01:21:10,870
so that's what we have here

1322
01:21:12,020 --> 01:21:12,830
i mean

1323
01:21:12,850 --> 01:21:14,890
again this does not bring any

1324
01:21:15,290 --> 01:21:20,620
significant information but just for you to hear this name empirical prob and that's really

1325
01:21:20,620 --> 01:21:22,640
what we are so

1326
01:21:22,850 --> 01:21:29,160
there is a whole series of empirical processes where lots of tools are developed

1327
01:21:29,890 --> 01:21:34,870
o thing upper bounds on quantities like this

1328
01:21:35,810 --> 01:21:38,930
so why is this quantity interesting so now

1329
01:21:39,160 --> 01:21:41,770
is where this

1330
01:21:41,960 --> 01:21:44,870
fundamental inequalities come in

1331
01:21:44,890 --> 01:21:46,810
but before that

1332
01:21:46,850 --> 01:21:52,430
i give you some properties of the empirical distribution

1333
01:21:54,540 --> 01:21:58,330
o thing lost minus empirical loss is the difference between

1334
01:21:58,410 --> 01:22:00,430
the empirical average

1335
01:22:00,450 --> 01:22:02,950
and then expectation

1336
01:22:03,000 --> 01:22:05,540
as we say

1337
01:22:05,560 --> 01:22:06,830
or as

1338
01:22:06,850 --> 01:22:09,540
you probably know

1339
01:22:09,560 --> 01:22:10,660
there is this

1340
01:22:10,660 --> 01:22:14,890
property called the law of large numbers that ensures that if you have

1341
01:22:14,930 --> 01:22:18,750
a random variable that you and you take the value of this from viable several

1342
01:22:18,750 --> 01:22:20,750
times you take the average value

1343
01:22:21,730 --> 01:22:22,710
in the limits

1344
01:22:22,730 --> 01:22:27,180
if you do that enough it will converge to the expected value of the random

1345
01:22:27,180 --> 01:22:31,210
variables so this is what is written here

1346
01:22:31,250 --> 01:22:33,600
when n goes to infinity

1347
01:22:33,620 --> 01:22:37,620
this quantity goes to zero with probability one

1348
01:22:39,520 --> 01:22:40,540
not only

1349
01:22:40,560 --> 01:22:42,500
is the expected value

1350
01:22:42,540 --> 01:22:45,000
of the empirical loss equal

1351
01:22:45,000 --> 01:22:49,180
to the true loss but also when n goes to infinity they expect the empirical

1352
01:22:49,180 --> 01:22:51,770
loss converges to the true

1353
01:22:51,950 --> 01:22:54,270
so now the question is how to quantify

1354
01:22:54,330 --> 01:22:57,310
how's fast this convergence occurs

1355
01:22:58,850 --> 01:22:59,660
there is

1356
01:22:59,710 --> 01:23:05,410
what is called of things inequality which does exactly that

1357
01:23:05,410 --> 01:23:07,610
can do without you being able to get

1358
01:23:07,620 --> 01:23:09,200
possibly all the details

1359
01:23:09,200 --> 01:23:10,580
but this is you know

1360
01:23:10,600 --> 01:23:14,210
where the questions are and so on

1361
01:23:14,340 --> 01:23:17,050
so i think it's more useful for research

1362
01:23:17,090 --> 01:23:20,350
in general this is how practical methods

1363
01:23:20,400 --> 01:23:22,990
you have the target distribution

1364
01:23:23,030 --> 01:23:25,890
over space that increasing

1365
01:23:25,910 --> 01:23:30,090
every time i add of blue note that graph

1366
01:23:30,200 --> 01:23:34,880
what's really happening is that the distribution is increasing over time one interpretation is that

1367
01:23:34,880 --> 01:23:36,740
the dynamical system is

1368
01:23:36,750 --> 01:23:39,050
you're only interested in the current time slice

1369
01:23:39,060 --> 01:23:42,050
another interpretation is that really what's happening

1370
01:23:42,060 --> 01:23:43,950
if you look at the graphical model

1371
01:23:43,960 --> 01:23:47,530
is that you're learning a distribution that keeps growing

1372
01:23:47,570 --> 01:23:52,560
you keep having to learn more and more and more and more

1373
01:23:52,570 --> 01:23:55,470
so you target distribution time n

1374
01:23:55,480 --> 01:23:58,680
is the distribution of an unknown

1375
01:23:58,710 --> 01:24:02,550
the the next time step the distribution of and plus one nodes and so on

1376
01:24:02,610 --> 01:24:06,540
serial our learning over this of the joint

1377
01:24:07,830 --> 01:24:12,000
the problem is we never know the normalisation constant z

1378
01:24:12,010 --> 01:24:16,500
we don't know the normalisation constant we do have an expression which is usually

1379
01:24:16,550 --> 01:24:19,080
in the basic a surprise comes alive

1380
01:24:19,130 --> 01:24:21,170
now particle filters are not

1381
01:24:21,220 --> 01:24:25,080
something that she is on probation but you can use them from many other things

1382
01:24:26,310 --> 01:24:29,750
it's good to look at it from the more general perspective

1383
01:24:29,750 --> 01:24:34,560
again said and this what makes this thing some one would think

1384
01:24:34,570 --> 01:24:36,550
we define the weights

1385
01:24:36,590 --> 01:24:40,700
as the ratio of the quantity of f over q

1386
01:24:40,740 --> 01:24:44,480
just like we did yesterday and importance sampling

1387
01:24:44,530 --> 01:24:48,900
and what we can do that is we can take the weights

1388
01:24:49,010 --> 01:24:51,810
we multiply by the weights and the bias by the weights

1389
01:24:51,860 --> 01:24:55,260
because if n minus one divided by q and minus one is just w n

1390
01:24:55,270 --> 01:24:56,650
minus one

1391
01:24:56,660 --> 01:25:00,780
so this step all i did was i multiplied and divided by the weights

1392
01:25:00,830 --> 01:25:03,220
this is the way to minus one

1393
01:25:03,220 --> 01:25:07,290
my goal is to obtain the recursive estimator for the weight

1394
01:25:07,410 --> 01:25:11,640
that's what i wanna do i wanted the recursion

1395
01:25:14,750 --> 01:25:17,570
and q n minus one distribution

1396
01:25:17,590 --> 01:25:19,770
so this is p of a and b

1397
01:25:19,810 --> 01:25:21,290
this is p of p

1398
01:25:21,300 --> 01:25:25,810
so here what i get is p of a given b

1399
01:25:26,080 --> 01:25:31,010
and that's the more general expression for the importance

1400
01:25:31,090 --> 01:25:33,070
for the importance weights

1401
01:25:33,070 --> 01:25:35,160
that's how you would arrive at

1402
01:25:35,170 --> 01:25:39,260
the important thing is you writing this importance weights on the past and the whole

1403
01:25:39,260 --> 01:25:43,570
part of the problem not just at a particular step

1404
01:25:43,580 --> 01:25:46,960
and that's the only way to the right there is the way to the right

1405
01:25:46,960 --> 01:25:48,410
the marginal space

1406
01:25:48,470 --> 01:25:55,310
but it turns out the competition successes and squared

1407
01:25:55,350 --> 01:26:03,630
this is the same thing

1408
01:26:03,730 --> 01:26:05,140
so example

1409
01:26:05,200 --> 01:26:08,220
bayesian filtering in bayesian filtering

1410
01:26:08,240 --> 01:26:11,370
the joint this at is really the product

1411
01:26:11,430 --> 01:26:12,910
if all these

1412
01:26:13,070 --> 01:26:18,140
increasing the graph rewriting all these stages you know having the horizontal

1413
01:26:18,160 --> 01:26:20,340
you know you you you have to say

1414
01:26:20,360 --> 01:26:23,500
is at is that if any of these then you have these then you have

1415
01:26:24,660 --> 01:26:28,760
and the vertical lines of the pure white is given xt is the horizontal ones

1416
01:26:28,990 --> 01:26:35,380
effects is given xt minus one in this distribution can grow over time is important

1417
01:26:35,400 --> 01:26:38,870
and the normalisation constant is the integral of the sky

1418
01:26:38,880 --> 01:26:41,980
which in this case is just be i want to add which is again the

1419
01:26:43,680 --> 01:26:47,110
which is the thing that you would use like if you had two models

1420
01:26:47,150 --> 01:26:50,580
you would compute this quantity for each of the models and that's what you would

1421
01:26:50,760 --> 01:26:54,240
to choose between the models

1422
01:26:54,270 --> 01:26:56,390
the weights in this case

1423
01:26:56,410 --> 01:26:59,570
following the expression with arrived in the previously

1424
01:26:59,580 --> 01:27:02,550
and you just replace your fnx one two n

1425
01:27:02,600 --> 01:27:04,020
the guy

1426
01:27:04,070 --> 01:27:08,310
and by the same guy within the only goes and minus one

1427
01:27:08,310 --> 01:27:13,810
everything councils except the last guy so you get this expression

1428
01:27:24,290 --> 01:27:28,550
in fact the checked

1429
01:27:28,550 --> 01:27:32,150
so this is just a combinatorial question

1430
01:27:32,170 --> 01:27:34,930
so let me just

1431
01:27:36,660 --> 01:27:39,780
a very simple way of viewing a solution to that

1432
01:27:39,800 --> 01:27:43,780
so essentially if we view those

1433
01:27:43,800 --> 01:27:45,130
m slots here

1434
01:27:46,500 --> 01:27:47,620
pigeon holes

1435
01:27:47,640 --> 01:27:48,920
and we're gonna put it

1436
01:27:48,930 --> 01:27:51,130
symbols in

1437
01:27:51,260 --> 01:27:55,100
some of them might have any balls and some will have more than one and

1438
01:27:55,100 --> 01:27:57,460
so on

1439
01:27:58,150 --> 01:27:59,370
what i'm going to do

1440
01:27:59,380 --> 01:28:02,780
so these this is if you like corresponds to alpha one

1441
01:28:02,830 --> 01:28:05,810
and this would indicate that alpha one was equal to two because there are two

1442
01:28:05,810 --> 01:28:07,060
balls there

1443
01:28:07,070 --> 01:28:12,580
and so on alpha two beta two alpha three will be one and alpha forward

1444
01:28:12,610 --> 01:28:14,340
is zero and so on

1445
01:28:14,360 --> 01:28:16,300
and this will be

1446
01:28:16,350 --> 01:28:18,070
alpha m here

1447
01:28:18,090 --> 01:28:20,200
which would be equal to two in that case

1448
01:28:20,400 --> 01:28:23,230
now what i'm going to do is put a sort of a blank line on

1449
01:28:23,230 --> 01:28:25,710
the bottom here

1450
01:28:28,030 --> 01:28:31,230
with entities in

1451
01:28:31,250 --> 01:28:32,830
sort of

1452
01:28:32,840 --> 01:28:34,990
space at the bottom of the

1453
01:28:35,000 --> 01:28:36,110
the slots

1454
01:28:36,130 --> 01:28:40,640
and now i'm going to do is read off this

1455
01:28:40,650 --> 01:28:42,570
sequence from the left

1456
01:28:42,610 --> 01:28:45,740
into a single line

1457
01:28:45,750 --> 01:28:46,610
it one

1458
01:28:47,880 --> 01:28:49,690
start at the first ball

1459
01:28:49,700 --> 01:28:54,060
and read to the bottom so in this case i get talked of blank

1460
01:28:54,350 --> 01:28:57,060
dot dot blank

1461
01:29:01,750 --> 01:29:03,310
dot thank

1462
01:29:03,330 --> 01:29:07,190
and so on and i finish with actually dot dot because i'm going to actually

1463
01:29:07,190 --> 01:29:08,830
ignore them

1464
01:29:08,850 --> 01:29:11,580
that one that the final one will be ignored

1465
01:29:12,740 --> 01:29:18,940
what i maintain is now this of course is is now of length

1466
01:29:23,090 --> 01:29:24,980
minus one

1467
01:29:24,990 --> 01:29:26,500
because remember there were

1468
01:29:26,510 --> 01:29:31,930
and banksia and there will be balls and i actually dropped off one so it's

1469
01:29:31,930 --> 01:29:34,550
plus be minus one

1470
01:29:34,560 --> 01:29:36,490
and now

1471
01:29:36,580 --> 01:29:39,730
clearly here we have

1472
01:29:39,740 --> 01:29:42,830
in these plus b minus one slots now

1473
01:29:42,840 --> 01:29:47,070
we have the balls placed book people's about placed OK

1474
01:29:47,150 --> 01:29:51,170
now what i maintain is the one to one card there is a one-to-one correspondence

1475
01:29:51,170 --> 01:29:52,760
between putting

1476
01:29:52,770 --> 01:29:58,490
b balls into this sequence of n plus b minus one slots and putting the

1477
01:29:59,680 --> 01:30:02,230
into these pigeonholes

1478
01:30:02,280 --> 01:30:07,590
and i've shown you how you convert pigeonhole allocation into

1479
01:30:09,430 --> 01:30:11,060
an application into this

1480
01:30:11,130 --> 01:30:13,440
length m plus b minus one

1481
01:30:13,450 --> 01:30:18,730
sequence to get the other way round given any sequence in here

1482
01:30:18,740 --> 01:30:21,420
you simply start at the left

1483
01:30:23,140 --> 01:30:25,990
whenever you start filling this

1484
01:30:26,000 --> 01:30:27,370
hole here

1485
01:30:27,390 --> 01:30:29,780
and you feel it until you reach a blank

1486
01:30:29,800 --> 01:30:32,210
and then you move to the next pigeonholes

1487
01:30:32,940 --> 01:30:34,760
so in this case the two

1488
01:30:34,770 --> 01:30:37,970
dots indicate that those going to the first if i was doing it this way

1489
01:30:38,780 --> 01:30:42,920
those two but thanks those two balls going into the first pigeonhole

1490
01:30:42,930 --> 01:30:46,120
then there's a blank so i moved to the second pigeon hole

1491
01:30:46,140 --> 01:30:48,880
there are two there so i put them into that

1492
01:30:49,170 --> 01:30:53,890
there's a blank side finish that one then this one ball in the next pigeonhole

1493
01:30:53,890 --> 01:30:57,330
and then there's blank so i moved to the next one is an immediately another

1494
01:30:57,330 --> 01:31:00,960
blank so i don't put anything in this pigeonholing go immediately to the next one

1495
01:31:00,980 --> 01:31:03,710
another blank so that gain is is empty

1496
01:31:03,750 --> 01:31:05,640
and then this one here and so on

1497
01:31:05,710 --> 01:31:07,650
and however you put the balls in

1498
01:31:07,660 --> 01:31:11,130
you can perform that operation and you will exactly fill up

1499
01:31:11,150 --> 01:31:15,130
a certain peoples into the pigeon holes and there is a one-to-one correspondence you can

1500
01:31:15,130 --> 01:31:16,690
see i think quite badly

1501
01:31:16,990 --> 01:31:20,990
another reason for doing that is it's not very easy to i mean the

1502
01:31:21,010 --> 01:31:25,220
the actual way of counting the number of ways of putting the balls into n

1503
01:31:25,220 --> 01:31:29,490
plus b minus one source is just the standard combinatorial

1504
01:31:29,540 --> 01:31:32,500
a plus b

1505
01:31:32,530 --> 01:31:33,670
minus one

1506
01:31:33,690 --> 01:31:36,530
choose be

1507
01:31:36,550 --> 01:31:38,750
so that's the that's the result

1508
01:31:38,760 --> 01:31:44,210
it i gave it in a slightly different form because actually isis

1509
01:31:44,210 --> 01:31:45,860
this is the definition we're done

1510
01:31:45,910 --> 01:31:48,370
step three is probably the hardest part

1511
01:31:48,420 --> 01:31:50,720
step four we've already done

1512
01:31:53,630 --> 01:31:57,840
let's start with step one

1513
01:31:57,960 --> 01:32:16,560
so the first thing i need to do is to find a convex function because

1514
01:32:16,560 --> 01:32:17,440
we're gonna

1515
01:32:17,500 --> 01:32:19,450
manipulate that definition

1516
01:32:23,090 --> 01:32:24,890
so this is notion from

1517
01:32:25,020 --> 01:32:28,140
real analysis

1518
01:32:30,140 --> 01:32:34,490
analysis of fancy work calculus taken

1519
01:32:34,540 --> 01:32:36,400
proper analysis class

1520
01:32:36,440 --> 01:32:39,580
should have seen convexity in any calculus class

1521
01:32:39,590 --> 01:32:41,880
convex functions one looks like this

1522
01:32:43,980 --> 01:32:47,900
one way to formalize that notion

1523
01:32:47,910 --> 01:32:50,730
is to consider any two points on this curve

1524
01:32:50,750 --> 01:32:54,450
so i'm i'm only interested in functions for real to real so

1525
01:32:54,470 --> 01:32:57,200
looks like this this is for something

1526
01:32:57,250 --> 01:33:00,060
and this is the something

1527
01:33:00,100 --> 01:33:02,290
if i take two points on this curve

1528
01:33:02,330 --> 01:33:03,700
and i draw

1529
01:33:03,720 --> 01:33:08,290
the line segment connecting them that line segment is always above the curve

1530
01:33:08,290 --> 01:33:10,400
that's the meaning of convexity

1531
01:33:10,410 --> 01:33:14,600
it has the geometric notion which is basically the same for functions

1532
01:33:14,600 --> 01:33:18,230
this line segment should stay above the curve the line does not state above the

1533
01:33:18,230 --> 01:33:22,820
curve extended it farther it goes beneath the curve of course

1534
01:33:22,870 --> 01:33:24,960
but that segment should

1535
01:33:25,010 --> 01:33:28,600
so i'm going to formalize a little bit of call this axis

1536
01:33:28,630 --> 01:33:31,170
and then this is f of x

1537
01:33:31,190 --> 01:33:32,910
and i call this

1538
01:33:34,130 --> 01:33:35,650
and this is after y

1539
01:33:35,680 --> 01:33:38,350
so the claim is if i take any number

1540
01:33:38,450 --> 01:33:40,660
between x and y

1541
01:33:40,720 --> 01:33:42,790
and i look up

1542
01:33:42,800 --> 01:33:45,170
i said OK here's the point on the curve

1543
01:33:45,230 --> 01:33:49,850
here's the point on the line segment the value of that point the y value

1544
01:33:49,910 --> 01:33:54,520
here should be greater than or equal to the y value here

1545
01:33:55,150 --> 01:33:57,440
to figure out what this point is

1546
01:33:57,460 --> 01:33:59,830
we need some

1547
01:33:59,900 --> 01:34:02,310
and i would call it geometry features

1548
01:34:02,330 --> 01:34:03,860
analysis concept two

1549
01:34:03,870 --> 01:34:05,740
but i mean geometry

1550
01:34:05,770 --> 01:34:07,540
i get to college geometry

1551
01:34:07,540 --> 01:34:09,960
if you have two points

1552
01:34:10,040 --> 01:34:12,230
p and q

1553
01:34:12,250 --> 01:34:16,100
and you want to parameterize this line segment between

1554
01:34:16,120 --> 01:34:18,650
so one approach for some point here

1555
01:34:18,690 --> 01:34:20,580
the way to do it

1556
01:34:20,580 --> 01:34:22,790
is to take a linear combination

1557
01:34:22,790 --> 01:34:24,080
and if you

1558
01:34:24,120 --> 01:34:26,120
should have taken some linear algebra

1559
01:34:26,230 --> 01:34:29,440
linear combination looks something like this

1560
01:34:29,460 --> 01:34:33,270
and in fact we're going to take something called an affine combination

1561
01:34:33,270 --> 01:34:36,810
where alpha postpaid equals one

1562
01:34:36,810 --> 01:34:38,120
it turns out

1563
01:34:38,140 --> 01:34:39,420
if you take

1564
01:34:39,420 --> 01:34:41,850
if you take all such points

1565
01:34:41,910 --> 01:34:46,290
some number alpha times the point p for some number of times the point you're

1566
01:34:46,290 --> 01:34:48,140
output has been equals one

1567
01:34:48,150 --> 01:34:51,870
take all the points you get the entire line here

1568
01:34:51,890 --> 01:34:56,020
which is nifty but we don't want the entire line if you also constraints

1569
01:34:56,040 --> 01:34:57,980
alpha data to be nonnegative

1570
01:34:58,000 --> 01:34:59,710
just get this line segment

1571
01:34:59,730 --> 01:35:04,060
so this forces have invaded between zero and one

1572
01:35:04,080 --> 01:35:06,060
is that the sum to one another

1573
01:35:06,060 --> 01:35:10,310
so what we're going to do here is take alpha time axis was beta times

1574
01:35:11,620 --> 01:35:12,710
that's going to be

1575
01:35:12,730 --> 01:35:16,690
our point in between with these constraints about the possibility because one alpha and beta

1576
01:35:16,690 --> 01:35:18,690
particles zero

1577
01:35:18,710 --> 01:35:21,620
then in this point

1578
01:35:21,620 --> 01:35:23,140
is f of that

1579
01:35:23,190 --> 01:35:25,620
for alpha x

1580
01:35:25,640 --> 01:35:28,350
plus ba y

1581
01:35:28,370 --> 01:35:30,920
and this point

1582
01:35:30,980 --> 01:35:35,830
is the linear interpolation between f of x in f of why the same one

1583
01:35:35,850 --> 01:35:38,540
alpha times f of x

1584
01:35:38,540 --> 01:35:41,310
because data times from one

1585
01:35:42,330 --> 01:35:43,730
that's the intuition

1586
01:35:43,730 --> 01:35:46,060
he didn't follow is not too

1587
01:35:46,100 --> 01:35:48,770
big deal because all we care about are these

1588
01:35:48,810 --> 01:35:52,520
the symbolic cancer proving things but that's where this comes from

1589
01:35:52,560 --> 01:35:54,960
so here's the definition

1590
01:35:56,540 --> 01:35:58,060
function is convex

1591
01:35:59,060 --> 01:36:01,690
for all x and y

1592
01:36:04,770 --> 01:36:06,020
and all

1593
01:36:06,040 --> 01:36:08,810
alpha and beta

1594
01:36:08,870 --> 01:36:12,560
great american zero sum

1595
01:36:12,580 --> 01:36:13,750
this one

1596
01:36:14,100 --> 01:36:18,060
we have

1597
01:36:20,750 --> 01:36:24,270
f alpha x beta y

1598
01:36:24,310 --> 01:36:27,120
is less than or equal to alpha f of x

1599
01:36:27,170 --> 01:36:30,270
because they from more

1600
01:36:30,270 --> 01:36:33,290
that's just saying this why corner here

1601
01:36:33,310 --> 01:36:37,770
it is greater than this what are classical of this one caught

1602
01:36:37,870 --> 01:36:41,230
but that's the symbolism behind that

1603
01:36:44,060 --> 01:36:48,710
OK so now we want to prove jensen's inequality OK we're not quite there yet

1604
01:36:48,730 --> 01:36:50,390
we're going to prove

1605
01:36:50,420 --> 01:36:52,620
a simple and

1606
01:36:52,670 --> 01:36:55,500
from which it will be easy to derive

1607
01:36:55,540 --> 01:36:58,120
jensen's inequality

1608
01:36:58,170 --> 01:36:59,830
so this is the

1609
01:36:59,830 --> 01:37:02,500
there more proving

1610
01:37:09,770 --> 01:37:15,100
there's a lot more about convex functions you may have seen before

1611
01:37:15,100 --> 01:37:20,540
it will be crucial jensen's inequality

1612
01:37:20,670 --> 01:37:27,690
we have

1613
01:37:27,750 --> 01:37:35,120
suppose this is a statement about half combinations of and things instead of two things

1614
01:37:35,140 --> 01:37:38,330
so this will the convexity can be generalized to

1615
01:37:38,370 --> 01:37:40,370
taking and things

1616
01:37:41,390 --> 01:37:44,250
so suppose we have n real numbers

1617
01:37:44,350 --> 01:37:46,420
and we have and

1618
01:37:46,480 --> 01:37:48,040
values alpha

1619
01:37:48,940 --> 01:37:50,330
one of the alpha and

1620
01:37:50,390 --> 01:37:52,730
they're all nonnegative

1621
01:37:52,730 --> 01:37:54,940
and there are some is one

1622
01:37:54,960 --> 01:37:57,020
the sum of alpha

1623
01:37:57,080 --> 01:37:58,870
OK i guess

1624
01:37:58,980 --> 01:38:02,140
OK because want and

1625
01:38:02,190 --> 01:38:05,080
it is one

1626
01:38:05,080 --> 01:38:06,830
because of the assumptions

1627
01:38:08,120 --> 01:38:12,810
is the same thing but summing over all k

1628
01:38:12,830 --> 01:38:15,710
OK i want and

1629
01:38:15,810 --> 01:38:17,670
o k x k

1630
01:38:17,730 --> 01:38:23,620
take f of that versus taking the sum of the alphas times the earth's

1631
01:38:23,710 --> 01:38:29,020
because what

1632
01:38:29,020 --> 01:38:35,760
less than 100 million accuracy and in those year dollars but it was an excellent

1633
01:38:35,760 --> 01:38:43,920
program every tests that was conducted was documented analyzed and and we can make that

1634
01:38:45,060 --> 01:38:52,680
so that concludes the things that I had to take a year and we get

1635
01:38:52,680 --> 01:38:57,580
there are just about right and do you have any questions it should like that

1636
01:39:00,580 --> 01:39:12,800
if not I'll give you test yes all of the right of the highway effect

1637
01:39:12,870 --> 01:39:17,860
the train on all of you

1638
01:39:17,920 --> 01:39:21,040
1 of the most

1639
01:39:21,100 --> 01:39:29,350
this is going to be give you the rest of the building of all that

1640
01:39:29,370 --> 01:39:35,040
kind of stuff like sites

1641
01:39:35,940 --> 01:39:39,840
and when I say hypersonic let me take it down to modify

1642
01:39:40,470 --> 01:39:46,220
was probably the best its predictions that we had

1643
01:39:47,020 --> 01:39:53,430
a flat plate which the belly of the order has a little bit of surface

1644
01:39:53,560 --> 01:39:58,940
depending both in the page playing and in the all playing for

1645
01:39:59,020 --> 01:40:05,250
for stability if you put a big sphere segment of sphere you could place the

1646
01:40:05,670 --> 01:40:14,040
on that in UCI so I know us fear sadness fear stability now at the

1647
01:40:16,020 --> 01:40:23,240
turned out not to be a normal forces or even sad forces pitching moment is

1648
01:40:23,240 --> 01:40:28,080
the 1 that you tend to to miss the most and sometimes you don't know

1649
01:40:28,080 --> 01:40:32,180
if you have a wrong pitching moment are wrong center of gravity they

1650
01:40:32,230 --> 01:40:40,290
most precipitated delta pitching moment coefficient changed and the big born doors we have that

1651
01:40:40,290 --> 01:40:45,730
in the system we control not only a rate control system but an attitude control

1652
01:40:45,730 --> 01:40:53,480
systems and so it takes with the 70 square foot Volodymyr just takes a small

1653
01:40:53,480 --> 01:40:58,750
amount of change to correct any that you might have in CG pitching moment so

1654
01:40:58,750 --> 01:41:04,400
you control mechanism is with the big elevators and also the body flap a little

1655
01:41:04,400 --> 01:41:09,790
bit if you would use that as the reaction control system is right system and

1656
01:41:09,790 --> 01:41:14,230
I didn't mention that in that we intentionally put the act of

1657
01:41:14,600 --> 01:41:20,320
reaction control system in the wake of the vehicle even though for orbital manoeuvring we

1658
01:41:20,320 --> 01:41:26,260
have an jet-setter the forward firing about so much uncertainty and how that changes the

1659
01:41:26,450 --> 01:41:33,780
dynamics and surface not a hard surface and that interaction is you can change things

1660
01:41:34,200 --> 01:41:38,520
in such a way that you amplifier reduce it and so we elected not to

1661
01:41:38,880 --> 01:41:42,680
gotta grapple with that problem and you just cfgs but it's just for a control

1662
01:41:42,680 --> 01:41:44,620
system and pH

1663
01:41:44,760 --> 01:41:51,100
at distance that separated that any all both on both sides and enrolled

1664
01:41:51,280 --> 01:42:01,000
so those that that's that's where the theory of information and

1665
01:42:01,040 --> 01:42:09,290
x 15 was a lol angle of attack airplane type configurations from March 25 to

1666
01:42:09,290 --> 01:42:16,210
modify add new flying a spacecraft can configuration where you basically role about the velocity

1667
01:42:16,210 --> 01:42:20,980
vector OK you wanna maintain that pitch trim

1668
01:42:21,000 --> 01:42:26,980
and wrote about the velocity vector for crossrange in either direction now when you begin

1669
01:42:26,980 --> 01:42:31,420
transition because you gotta be down here when you landed and you pick the best

1670
01:42:31,420 --> 01:42:34,830
place to do that and the best place to do that is back in the

1671
01:42:34,830 --> 01:42:43,140
market below 5 courses and above the sub sonic regime and we try not to

1672
01:42:43,140 --> 01:42:48,230
have any maneuvers Rosetta required during that time period anytime you going from a totally

1673
01:42:48,230 --> 01:42:55,290
separated flow in the back of the vehicle to 1 that is now attached separated

1674
01:42:55,290 --> 01:43:01,230
flow can make up its mind sometimes it will be separated as you be attached

1675
01:43:01,640 --> 01:43:05,820
not the whole vehicle but even on small pieces of the vehicle and so there's

1676
01:43:05,820 --> 01:43:12,080
uncertainties that come in there but but that's that's the area uh that that that

1677
01:43:12,080 --> 01:43:15,360
that today but you can be sure enough for you or OPT as far away

1678
01:43:15,370 --> 01:43:19,790
as we and became more and more brave and to get those coefficients and in

1679
01:43:19,790 --> 01:43:31,680
another 100 per good job of it you have the thats right here thats it

1680
01:43:31,770 --> 01:43:38,750
instead of the things like that of the area

1681
01:43:38,830 --> 01:43:43,860
OK what you do doing you're change in the Newtonian flow now my flat plates

1682
01:43:43,860 --> 01:43:49,690
70 square foot per serving here at very end the India

1683
01:43:50,230 --> 01:43:55,920
so now I've got a normal force coefficients assigned squared of the angle OK and

1684
01:43:55,920 --> 01:44:00,270
I just change the angle up and reduce the forces on that and that gives

1685
01:44:00,270 --> 01:44:05,480
me a knows about bring it down that gives me a nose-down works beautifully which

1686
01:44:05,480 --> 01:44:08,870
means body flap elevators both

1687
01:44:10,720 --> 01:44:15,810
about 5 and a half control services based on the white hats off the best

1688
01:44:15,820 --> 01:44:23,280
of their land and what sort of reasoning that other factors transitional itself and so

1689
01:44:23,280 --> 01:44:28,620
actually that we safely energy know everything and then we have designed and these palate

1690
01:44:28,620 --> 01:44:36,260
test inputs to extract aerodynamic characteristics same technique that the Edwards developed and they did

1691
01:44:36,260 --> 01:44:40,800
develop that by the way on the x 15 and on other vehicles to and

1692
01:44:40,800 --> 01:44:46,250
we use the same technique which sent to 1 or Manhattan he spent about almost

1693
01:44:46,260 --> 01:44:51,220
a half a year learning all the techniques Robert that bag and developed that the

1694
01:44:51,740 --> 01:44:57,560
and of work reaction at very very closely training for those manoeuvres we extracted data

1695
01:44:57,560 --> 01:45:02,440
for those manoeuvres we would change the aerodynamic characteristics and not tell what they are

1696
01:45:02,520 --> 01:45:06,710
and see you know and in in the simulators and make sure that we didn't

1697
01:45:06,710 --> 01:45:10,660
get me difficult and making sure there'll people could extract the data right and tell

1698
01:45:10,660 --> 01:45:18,000
us how we change it they all got ladies and were very very well so

1699
01:45:18,000 --> 01:45:24,300
far from the notices that did that when that was for the approach and landing

1700
01:45:24,300 --> 01:45:29,360
test but for the entry those were supersonic any of you here and you have

1701
01:45:29,360 --> 01:45:37,890
a sense of what was the 1st full of what was the 1st phase of

1702
01:45:37,950 --> 01:45:43,540
the of the of the of the of the only

1703
01:45:43,650 --> 01:45:48,460
the technologies that people

1704
01:45:48,600 --> 01:45:56,040
the wording of the question of what you

1705
01:45:56,400 --> 01:46:05,370
with the local of because I know all about it the and removed from

1706
01:46:05,650 --> 01:46:11,100
so we would like to thank you for your help

1707
01:46:11,760 --> 01:46:14,670
and all the other kinds of

1708
01:46:15,720 --> 01:46:21,920
because the computerized so that we would like to do

1709
01:46:21,920 --> 01:46:31,720
she on of this side

1710
01:47:04,420 --> 01:47:10,420
you can see

1711
01:47:23,060 --> 01:47:33,910
thank you so he

1712
01:47:45,250 --> 01:47:47,720
so you

1713
01:48:10,450 --> 01:48:17,370
my advisor

1714
01:48:20,180 --> 01:48:27,360
he did not know

1715
01:48:27,480 --> 01:48:38,880
he was

1716
01:48:45,090 --> 01:48:48,870
so it

1717
01:48:53,230 --> 01:49:00,570
he's is of course not

1718
01:49:14,310 --> 01:49:17,560
it is

1719
01:49:22,680 --> 01:49:27,960
and this

1720
01:49:28,950 --> 01:49:36,680
she was the

1721
01:49:38,580 --> 01:49:55,600
that's not

1722
01:50:19,440 --> 01:50:22,610
this is

1723
01:50:26,680 --> 01:50:28,100
he was

