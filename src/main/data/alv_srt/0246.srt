1
00:00:00,000 --> 00:00:03,250
being able on the table or one ball against a wall

2
00:00:04,120 --> 00:00:05,750
these ten balls on the table

3
00:00:06,260 --> 00:00:07,660
those options were popular

4
00:00:08,750 --> 00:00:12,460
having identified those possible options as as the first plane

5
00:00:13,040 --> 00:00:15,590
i've been asked the question what shall advise you to do

6
00:00:16,050 --> 00:00:16,960
well shall we say

7
00:00:17,370 --> 00:00:19,260
if you want to get the most information

8
00:00:19,790 --> 00:00:24,840
on average out if your first weighing maybe should put it the first weighing

9
00:00:25,470 --> 00:00:28,830
so should be outcome the way has the biggest possible entropy

10
00:00:29,560 --> 00:00:32,820
and what we are about to do now is go through each of those six

11
00:00:33,130 --> 00:00:35,460
possible cases and work out what the entropy

12
00:00:36,920 --> 00:00:37,690
the outcome

13
00:00:38,230 --> 00:00:39,330
if you choose

14
00:00:40,330 --> 00:00:41,030
for example way

15
00:00:41,720 --> 00:00:44,690
six again six that's the first case we now discuss

16
00:00:49,270 --> 00:00:51,700
let's backtrack and let's imagine

17
00:00:52,210 --> 00:00:54,190
thatwe way sixty six

18
00:00:54,910 --> 00:00:56,260
then the possible outcomes

19
00:00:56,770 --> 00:00:58,590
when weighing six against sex

20
00:01:04,170 --> 00:01:04,520
and this

21
00:01:07,540 --> 00:01:12,180
so that's out and the probabilities these three outcomes anyone

22
00:01:13,900 --> 00:01:14,340
half the

23
00:01:18,310 --> 00:01:21,420
and that's the entropy about probability distribution

24
00:01:22,980 --> 00:01:25,680
and we then we all half of talk about

25
00:01:26,220 --> 00:01:26,960
end you get

26
00:01:32,030 --> 00:01:34,530
let's make a little note that's somewhere if we do

27
00:01:35,160 --> 00:01:37,970
six against six then the first outcome

28
00:01:40,870 --> 00:01:41,450
one bit

29
00:01:44,170 --> 00:01:46,040
by this five and very popular

30
00:01:47,460 --> 00:01:50,260
but if we had gone with five against five

31
00:01:53,700 --> 00:01:56,080
and what's the probability that it would

32
00:01:59,440 --> 00:02:01,690
five and fought against five to

33
00:02:02,120 --> 00:02:02,730
on the table

34
00:02:06,570 --> 00:02:07,920
was probably a bad outcome

35
00:02:10,960 --> 00:02:11,550
one six

36
00:02:12,970 --> 00:02:14,370
one six why was six

37
00:02:23,090 --> 00:02:23,620
okay so

38
00:02:24,440 --> 00:02:27,320
it will bounce one of his book on the table is available

39
00:02:28,140 --> 00:02:30,820
and that's to balls out of a total of twelve

40
00:02:31,260 --> 00:02:33,710
so it's a two twelfth temperatures once

41
00:02:36,400 --> 00:02:40,930
by symmetry remaining probability shared reading ten twelve s

42
00:02:41,790 --> 00:02:42,710
equally between these

43
00:02:43,460 --> 00:02:44,030
i itself

44
00:02:45,390 --> 00:02:46,280
five fold that's

45
00:02:47,670 --> 00:02:48,480
andy and

46
00:02:49,410 --> 00:02:50,840
the outcome in this case

47
00:02:51,850 --> 00:02:53,120
if we wait five five

48
00:02:53,970 --> 00:02:54,890
i have worked out

49
00:02:55,110 --> 00:02:56,960
you one point four eight

50
00:02:58,300 --> 00:03:00,110
so sentences that gives you more information

51
00:03:00,810 --> 00:03:02,050
curious that no one went

52
00:03:05,520 --> 00:03:08,790
well that's get it well we could wait for or against four

53
00:03:09,880 --> 00:03:10,500
if we way

54
00:03:11,720 --> 00:03:12,830
all against four

55
00:03:15,570 --> 00:03:16,570
what's the probability

56
00:03:17,120 --> 00:03:18,120
that will balance

57
00:03:19,620 --> 00:03:20,300
someone else

58
00:03:27,120 --> 00:03:28,020
can someone

59
00:03:28,250 --> 00:03:29,420
can just four twelve

60
00:03:29,830 --> 00:03:32,610
it doesn't balance if wonderful book on the table

61
00:03:33,190 --> 00:03:34,100
is they are all

62
00:03:34,940 --> 00:03:36,650
and the remaining twelve split

63
00:03:41,470 --> 00:03:42,550
so far this is

64
00:03:44,200 --> 00:03:46,120
gives you the entropy all

65
00:03:47,890 --> 00:03:48,850
the third

66
00:03:49,390 --> 00:03:49,870
which is

67
00:03:50,270 --> 00:03:51,540
no place to three

68
00:03:52,400 --> 00:03:54,270
which is one point five eight

69
00:03:57,540 --> 00:03:58,180
if we wait

70
00:03:58,450 --> 00:03:59,230
against three

71
00:04:03,040 --> 00:04:07,130
then what's the probability that balances we've left six on the table

72
00:04:08,250 --> 00:04:10,410
what's the probability that at one of

73
00:04:12,800 --> 00:04:15,460
okay so there must be a equal entity to these

74
00:04:16,700 --> 00:04:17,820
when you look at the entropy

75
00:04:19,150 --> 00:04:19,580
you get

76
00:04:20,510 --> 00:04:20,910
times to

77
00:04:21,950 --> 00:04:23,100
that's half times one

78
00:04:24,130 --> 00:04:26,010
it is one point five

79
00:04:30,660 --> 00:04:32,510
anselme fill in the remaining

80
00:04:33,140 --> 00:04:34,630
that's the easy way to get to

81
00:04:35,040 --> 00:04:37,540
then there is one point two five bits

82
00:04:38,100 --> 00:04:39,230
if you're one against one

83
00:04:39,720 --> 00:04:40,880
and is the point

84
00:04:45,170 --> 00:04:46,400
something that could be done

85
00:04:48,190 --> 00:04:52,290
if we believe in shannon just say well the entropy for these different possible

86
00:04:52,730 --> 00:04:53,660
actions we could take

87
00:04:54,540 --> 00:04:58,880
let's just go with the one that maximizes the information content and not what a sentence

88
00:04:59,900 --> 00:05:00,860
this choice which

89
00:05:01,500 --> 00:05:02,620
maybe after some fall

90
00:05:03,890 --> 00:05:04,540
all you have

91
00:05:05,080 --> 00:05:05,680
arrived at

92
00:05:08,960 --> 00:05:11,360
right let's go ahead and go without choice

93
00:05:13,090 --> 00:05:18,030
so we've got some balls now which we could say or possibly like possibly like right

94
00:05:19,870 --> 00:05:21,540
but possibly having if this outcome

95
00:05:22,000 --> 00:05:22,880
here occurs

96
00:05:25,840 --> 00:05:27,660
what should we call these um

97
00:05:28,480 --> 00:05:30,310
these are good models gene

98
00:05:34,330 --> 00:05:38,750
that's when possible thing that could happen or possible outcome assuming way forward in four

99
00:05:40,550 --> 00:05:41,520
the left side goes up

100
00:05:42,310 --> 00:05:43,540
the right side goes down

101
00:05:44,280 --> 00:05:46,910
we've got falls on the table which we now know good balls

102
00:05:47,730 --> 00:05:53,160
so you'd like to make a suggestion it's got a bunch of suggestions and what we do next is this

103
00:05:54,620 --> 00:05:57,020
would be outcome have a chance to enable

104
00:06:02,300 --> 00:06:04,420
let's get some suggestions what we do next

105
00:06:41,780 --> 00:06:43,170
we would like to suggest

106
00:06:53,390 --> 00:06:55,260
we want against each other

107
00:07:01,390 --> 00:07:02,300
okay so that way

108
00:07:03,150 --> 00:07:04,280
goods against

109
00:07:06,850 --> 00:07:07,780
possibly lights

110
00:07:15,400 --> 00:07:16,590
okay suggestion and the two

111
00:07:18,600 --> 00:07:20,180
the median of your suggestion

112
00:07:20,180 --> 00:07:24,750
the prior probability of the missing attribute values that's one possible approach

113
00:07:27,010 --> 00:07:28,360
so if

114
00:07:29,990 --> 00:07:33,100
value if attributes are not binary

115
00:07:33,120 --> 00:07:34,740
the question is

116
00:07:34,750 --> 00:07:36,460
whether we should split

117
00:07:36,500 --> 00:07:40,380
the training set into just two binary

118
00:07:40,400 --> 00:07:46,870
subsets or into as many subsets as the advantages of the attribute it turns out

119
00:07:46,870 --> 00:07:51,590
in the development of the system learning algorithm it was shown that it is better

120
00:07:51,600 --> 00:07:54,280
to always split the node into

121
00:07:54,330 --> 00:07:55,680
two some nodes

122
00:07:55,690 --> 00:08:02,330
because the classification accuracy in that case will be higher but then the question is

123
00:08:02,330 --> 00:08:06,550
of course which led us to put here in which there is a subset of

124
00:08:06,550 --> 00:08:09,740
values here so these are all open questions which are

125
00:08:10,220 --> 00:08:15,060
implementation details four different variants of decision tree learning algorithms

126
00:08:15,070 --> 00:08:23,670
then some of these some training examples can be can not be classified they can

127
00:08:23,670 --> 00:08:28,870
also be the situation that once you have split three

128
00:08:28,920 --> 00:08:31,730
in two sub

129
00:08:31,780 --> 00:08:37,540
sup notes that some ground can lead

130
00:08:37,550 --> 00:08:42,610
to the so called null leaf with no corresponding training

131
00:08:43,640 --> 00:08:48,030
in that case you again assign majority class

132
00:08:48,080 --> 00:08:56,220
of examples into that not so these are all the different implementation details which have

133
00:08:56,220 --> 00:08:59,650
to be taken care of in the particular implementation of

134
00:09:01,080 --> 00:09:04,660
for decision trees that can conclude

135
00:09:04,670 --> 00:09:09,000
so this would stop the stop the decision tree induction part

136
00:09:09,050 --> 00:09:16,160
i wouldn't start to lead the rule learning part now but to would make a

137
00:09:16,160 --> 00:09:18,750
small break

138
00:09:18,760 --> 00:09:20,270
so it's

139
00:09:20,280 --> 00:09:24,370
six o'clock we have one more hour to go if you survive

140
00:09:27,910 --> 00:09:29,380
predictive data mining

141
00:09:29,390 --> 00:09:33,650
so we said the predictive data mining is characterized by

142
00:09:33,660 --> 00:09:36,460
the fact that we would like to build a model

143
00:09:36,480 --> 00:09:40,340
which will enable prediction and classification

144
00:09:40,470 --> 00:09:46,590
it assumes that the training data we have available is labelled by class labels

145
00:09:46,610 --> 00:09:50,450
and we've seen the decision tree

146
00:09:50,460 --> 00:09:51,900
approach to building

147
00:09:51,920 --> 00:09:56,350
a classification model now we will see

148
00:09:56,370 --> 00:09:59,720
the approach of how to learn a set of

149
00:10:01,470 --> 00:10:05,680
we will

150
00:10:05,690 --> 00:10:09,650
describe the rule sets representation and

151
00:10:09,810 --> 00:10:13,360
one approach we have already seen

152
00:10:13,370 --> 00:10:17,940
but in the decision tree and then converting it into a set of rules

153
00:10:20,390 --> 00:10:21,350
other is

154
00:10:21,360 --> 00:10:23,250
building rules direct

155
00:10:23,300 --> 00:10:26,860
either building a set of rules or this the rules

156
00:10:26,870 --> 00:10:32,610
and we have already met in the rules can be either

157
00:10:33,910 --> 00:10:37,370
that would be a set of rules or that can be a list of rules

158
00:10:37,370 --> 00:10:41,480
or ordered list of rules which is in the form of if then s

159
00:10:41,490 --> 00:10:43,980
so one rule is built

160
00:10:44,000 --> 00:10:48,270
after the other and there are also interpreted into if then else

161
00:10:50,540 --> 00:10:52,800
you will also see some heuristics

162
00:10:52,820 --> 00:10:56,470
we will also deal with overfitting the data and we will

163
00:10:56,520 --> 00:10:59,920
i mentioned some pruning of the set of rules

164
00:10:59,970 --> 00:11:04,780
so heuristics will be slightly different from the heuristics so we have encountered in decision

165
00:11:04,780 --> 00:11:10,670
tree learning a decision tree this heuristic which we have encountered was take the most

166
00:11:10,670 --> 00:11:17,680
informative attributes such that the entropy of the set will decrease as much as possible

167
00:11:17,700 --> 00:11:25,920
OK now again we have the same representation of data which are objects characterised with

168
00:11:25,920 --> 00:11:32,940
attributes and they are are labelled with this are different class labels

169
00:11:32,950 --> 00:11:38,780
if we look at the decision tree learning versus learning of a set of rules

170
00:11:39,010 --> 00:11:41,400
decision tree learning

171
00:11:41,640 --> 00:11:46,080
performances follows suppose we have a binary classification problem with some

172
00:11:46,270 --> 00:11:52,290
instances of labelled with plus the other labelled with class label minus

173
00:11:53,090 --> 00:11:55,930
the decision tree learning approaches as follows

174
00:11:55,940 --> 00:11:58,710
suppose we have just two attributes one

175
00:11:59,420 --> 00:12:03,330
on x x is the other is on the y axis and then

176
00:12:03,350 --> 00:12:08,160
based on supposed the most informative activities the one on the x axis

177
00:12:09,160 --> 00:12:13,970
find a way of splitting the set into a sort of subsets

178
00:12:14,010 --> 00:12:17,350
this subset and this subset

179
00:12:17,360 --> 00:12:19,210
this is the clean subset

180
00:12:19,230 --> 00:12:21,360
with entropy zero

181
00:12:21,370 --> 00:12:23,220
so this would be one live

182
00:12:23,240 --> 00:12:27,390
and then in the other subset there are still examples of both classes

183
00:12:28,510 --> 00:12:33,380
suppose we take another attribute split based on the values of these other attributes in

184
00:12:33,380 --> 00:12:38,090
the way that we get clean subsets just minus

185
00:12:38,140 --> 00:12:41,610
examples on the one hand side and what the others

186
00:12:41,650 --> 00:12:47,410
with the class plus the other side so this is also quite called the splitting

187
00:12:48,430 --> 00:12:51,170
because we always

188
00:12:51,180 --> 00:12:54,500
split the set of all instances into subsets

189
00:12:55,030 --> 00:12:56,050
it is also

190
00:12:58,450 --> 00:13:06,020
top down induction of decision trees and the representative of the algorithms is i'd three

191
00:13:06,030 --> 00:13:08,330
in the rule learning

192
00:13:08,350 --> 00:13:12,320
we are taking a different approach for every class

193
00:13:12,380 --> 00:13:16,300
we are building a set of rules so suppose we first building a set of

194
00:13:16,300 --> 00:13:18,790
rules for class plus

195
00:13:18,840 --> 00:13:20,360
with the first rule

196
00:13:20,370 --> 00:13:24,060
we will cover some positive examples

197
00:13:24,100 --> 00:13:28,710
which means that with the rule the rule will be such that a summary

198
00:13:28,750 --> 00:13:30,760
descriptions of the examples

199
00:13:30,770 --> 00:13:36,460
are satisfied with the satisfy the conditions of the rules and then

200
00:13:36,540 --> 00:13:43,650
those examples covered by the first rule will be eliminated from set the remaining

201
00:13:43,710 --> 00:13:47,630
a set of examples remain and we will build another rule

202
00:13:47,650 --> 00:13:50,880
for the class plus then the two classes will

203
00:13:50,930 --> 00:13:56,340
change then we will start building rules for the other class and in this case

204
00:13:56,500 --> 00:13:58,540
as is shown in this

205
00:13:58,590 --> 00:14:02,940
simplified slide that will be just one draw four plus minds

206
00:14:03,100 --> 00:14:08,560
this is called the so-called called covering approach because with a certain rule we want

207
00:14:08,560 --> 00:14:12,250
to cover as many examples of the class for which we are building the rule

208
00:14:13,090 --> 00:14:19,380
so it's the covering approach with and we are building the so-called characteristic descriptions of

209
00:14:20,490 --> 00:14:21,440
in this way

210
00:14:21,440 --> 00:14:26,760
we go into the user requirements and then we go into the development cycle to

211
00:14:26,760 --> 00:14:32,550
what must be developing software has always because we are talking about application but the

212
00:14:32,550 --> 00:14:37,260
fork is of course on semantics over ontologies that we have to develop

213
00:14:38,750 --> 00:14:43,260
there will be green background lights and like that others the blue one about the

214
00:14:43,260 --> 00:14:48,080
tutorial the green one idea about background knowledge so about introduction to the semantic web

215
00:14:48,080 --> 00:14:50,690
RDF technology and so far so those of you

216
00:14:51,120 --> 00:14:52,740
that's why i'm not too

217
00:14:52,780 --> 00:14:56,930
really interesting to you again and again the same basic topics can keep all the

218
00:14:56,930 --> 00:15:02,210
green one so you can write emails and things like that during the green light

219
00:15:02,210 --> 00:15:07,240
fixtures code is available you can download it and play with beautiful like it's for

220
00:15:07,240 --> 00:15:11,120
free and you can just do whatever you like with it i'm not sure if

221
00:15:11,120 --> 00:15:14,130
this is the last version here but that we make sure that the last version

222
00:15:14,130 --> 00:15:20,530
is published by many cases there will be something here which is running

223
00:15:21,700 --> 00:15:26,250
what's the semantic web so the greens lights according to the needham and i took

224
00:15:26,250 --> 00:15:31,560
this life from him the semantic web is a way to address the very interesting

225
00:15:31,560 --> 00:15:33,890
topics of the web is just a

226
00:15:33,900 --> 00:15:36,510
it's time for humans so basically you have

227
00:15:36,560 --> 00:15:42,010
this natural language pages plus many more media content and what you do

228
00:15:42,130 --> 00:15:48,620
is tried to get an answer from the web by deducing faster creating mental situation

229
00:15:49,480 --> 00:15:55,270
in using your experience and understanding of the pages to derive the information which is

230
00:15:55,270 --> 00:15:57,320
needed to solve your problem

231
00:15:57,320 --> 00:16:00,930
and in the end this is quite easy for humans even if you're a person

232
00:16:00,930 --> 00:16:04,900
with disabilities but my not being that it's so easy

233
00:16:06,640 --> 00:16:08,860
the problem is that

234
00:16:08,890 --> 00:16:10,400
we always

235
00:16:10,420 --> 00:16:13,550
merge information that comes from this finding pages

236
00:16:13,560 --> 00:16:16,610
so we're not going to have to find the page and read it through and

237
00:16:16,610 --> 00:16:20,570
that the answer no money the point is we go on one website we collect

238
00:16:20,570 --> 00:16:24,430
some data using the knowledge that we call we go to another website with people

239
00:16:24,740 --> 00:16:30,280
searching and bring in knowledge together in order to solve the problem we do that

240
00:16:30,280 --> 00:16:34,130
when we organize it rather using the beautiful travel agency

241
00:16:34,180 --> 00:16:39,950
then i will tell bookings solution and then train solution forth full of course is

242
00:16:39,950 --> 00:16:43,050
done in many other cases like digital libraries for instance

243
00:16:43,230 --> 00:16:44,760
and i mean this is

244
00:16:44,810 --> 00:16:49,530
the easy because we are humans and we know how to do it and

245
00:16:49,540 --> 00:16:53,400
the point is always is that we cannot expect the machine to do the same

246
00:16:53,490 --> 00:16:56,090
because basically it's

247
00:16:56,110 --> 00:17:00,460
so so difficult for a machine to work with partial information so you get a

248
00:17:00,460 --> 00:17:03,700
couple of four from there are couple of from that you know a lot

249
00:17:03,750 --> 00:17:10,600
are you able to derive the results but machine can not work like that

250
00:17:10,640 --> 00:17:14,500
it is very difficult to make sense of multimedia for instance in image

251
00:17:14,510 --> 00:17:19,220
if you have a very fancy will maps images useful for us but almost useless

252
00:17:19,220 --> 00:17:20,420
for life

253
00:17:20,430 --> 00:17:26,880
and then there is the program of drawing analogies this problem of linking information from

254
00:17:26,880 --> 00:17:30,730
comes from two different sites if this is very very difficult it's difficult for us

255
00:17:31,130 --> 00:17:32,960
to exactly know what we are

256
00:17:33,000 --> 00:17:38,810
understanding and if it's really a link it's very difficult for much just to make

257
00:17:39,330 --> 00:17:45,360
concrete is these stuck here so creative is same as another cut down the

258
00:17:45,760 --> 00:17:50,020
how to yes of course and then they are the same word but values into

259
00:17:50,020 --> 00:17:54,720
different XML schema maybe they mean the same maybe not what does it happen if

260
00:17:54,720 --> 00:17:56,400
we believe that means the same

261
00:17:56,480 --> 00:18:02,750
people are good at this machine so

262
00:18:02,760 --> 00:18:07,430
going back to to our music event explorer what's the point

263
00:18:07,480 --> 00:18:12,040
as i said the application is designed to sort of put to rest the user

264
00:18:12,050 --> 00:18:13,970
need and the need is basically

265
00:18:14,340 --> 00:18:19,630
to know which events are around the world about the given music style so i

266
00:18:19,630 --> 00:18:20,660
don't want to

267
00:18:20,720 --> 00:18:25,210
really to go to an event about one very famous artists

268
00:18:25,250 --> 00:18:29,210
it's more and more in the mood to to know about events close to me

269
00:18:29,250 --> 00:18:33,590
but i'm about to a given music style and i don't really care about the

270
00:18:33,590 --> 00:18:37,650
performer i mean i do care but the point is given that i don't know

271
00:18:37,650 --> 00:18:42,620
all the performance of the given music style especially for very unique music studies but

272
00:18:42,620 --> 00:18:46,010
i like what kind of music i would like to know which events are out

273
00:18:46,010 --> 00:18:48,720
there but are about this music style

274
00:18:48,760 --> 00:18:53,440
and to do that what you do is really to go on the web and

275
00:18:53,440 --> 00:18:57,640
then perform several tasks that we want to be is application next to do for

276
00:18:57,640 --> 00:18:59,210
you so we want to the

277
00:18:59,640 --> 00:19:04,420
application to find all the artists that play forty stands for music

278
00:19:04,480 --> 00:19:09,670
so you go on database where you have the information about the artist and size

279
00:19:09,680 --> 00:19:11,740
and you ask your creating

280
00:19:11,780 --> 00:19:16,280
place for music and you got back a bunch of bands and artists that play

281
00:19:16,280 --> 00:19:20,640
is this this time when you have is a bunch of artists so what you

282
00:19:20,640 --> 00:19:24,430
want to do normally is to understand what they are so you go to a

283
00:19:24,430 --> 00:19:26,460
database music archive

284
00:19:26,470 --> 00:19:28,460
you could ask for each artist

285
00:19:28,510 --> 00:19:35,270
what we did with the they for you have what rocks all their info about

286
00:19:35,270 --> 00:19:38,880
them and when they are born with without from things like that

287
00:19:38,880 --> 00:19:41,380
and then we all these very

288
00:19:41,430 --> 00:19:43,200
good bunch of information

289
00:19:43,200 --> 00:19:48,530
this is not just the static prediction problems but in fact this this predict there

290
00:19:48,530 --> 00:19:51,650
will be in the big blue and it'll help decide

291
00:19:51,810 --> 00:19:56,590
its own training data but because it will decide which ads actually get displayed and

292
00:19:56,590 --> 00:20:01,370
then those will get click that will be fed back into the predictor so

293
00:20:01,410 --> 00:20:04,150
we need some means of exploration

294
00:20:04,160 --> 00:20:08,030
because of if we only ever show those ads that we already know are good

295
00:20:08,070 --> 00:20:12,440
we were not ever find the opportunities for the for the interesting ones you can

296
00:20:12,440 --> 00:20:18,410
see here how the how our probability over the probability of click beetle like probability

297
00:20:18,410 --> 00:20:23,210
distributions can help us do that because we can for example instead of just taking

298
00:20:23,210 --> 00:20:29,080
the expected peak click we can sample p clicks from these distributions so that in

299
00:20:29,080 --> 00:20:30,740
this particular case

300
00:20:31,750 --> 00:20:32,960
and here

301
00:20:32,970 --> 00:20:38,590
is already a fairly established low variance probability of click here

302
00:20:38,600 --> 00:20:42,390
which is greater than the one for this thing but this and will have the

303
00:20:42,410 --> 00:20:44,180
chance of sam pling

304
00:20:44,190 --> 00:20:48,800
p clicks that will be bigger than for this one so in fact we will

305
00:20:48,800 --> 00:20:53,790
be able to collect information about it which in turn will reduce our belief variance

306
00:20:53,810 --> 00:20:58,090
for it and so we can explore the entire pool of ads that we have

307
00:21:00,300 --> 00:21:03,900
is the training work we know how this forward model works now

308
00:21:03,910 --> 00:21:07,130
but we need to understand how we actually update the those weights

309
00:21:08,600 --> 00:21:14,030
here's his the factor graph the basically does it we have belief distributions here and

310
00:21:14,890 --> 00:21:15,630
and then

311
00:21:15,650 --> 00:21:19,910
we view this as a message passing algorithm here so the messages come from the

312
00:21:19,910 --> 00:21:22,860
prior go down are combined into the sum

313
00:21:22,880 --> 00:21:27,510
a score through the nonlinearity and give us some kind of prediction

314
00:21:27,560 --> 00:21:28,360
and i

315
00:21:28,370 --> 00:21:32,850
of course when we observe either no click all the click event

316
00:21:32,900 --> 00:21:37,690
that message flowing the other way around we get an updated belief distribution for this

317
00:21:37,690 --> 00:21:42,340
quantity here and that is then distributed to these weight here

318
00:21:42,350 --> 00:21:44,670
and the important effect here is

319
00:21:44,680 --> 00:21:49,840
that a weight which already had a fairly small variance in its prior will only

320
00:21:49,840 --> 00:21:51,920
receive a small update

321
00:21:51,970 --> 00:21:56,530
and the way that will that has brought the variance in the prior will receive

322
00:21:56,530 --> 00:22:03,260
the bigger so we'll have that assignment of responsibility so to speak to those ads

323
00:22:03,260 --> 00:22:06,730
where we're all those features were really certain

324
00:22:06,750 --> 00:22:10,440
here the update rules in detail

325
00:22:10,490 --> 00:22:16,460
and if we quickly go through them because i think there are actually quite quite

326
00:22:16,460 --> 00:22:20,360
elegant because they're are closed for you don't have to view it as as the

327
00:22:20,360 --> 00:22:25,560
message passing so we basically collapsed the messages i was showing before and now this

328
00:22:25,560 --> 00:22:27,370
is basically the closed form

329
00:22:28,450 --> 00:22:35,130
and so this is the update equations for the mean of a feature weight i

330
00:22:35,130 --> 00:22:39,850
and this is the update equation for the variance of weight i

331
00:22:39,870 --> 00:22:43,740
and so the first thing is we see that this is an additive update here

332
00:22:43,740 --> 00:22:44,850
for the mu

333
00:22:44,890 --> 00:22:48,960
and the update is the scaled by the variance of the weights as you would

334
00:22:48,960 --> 00:22:55,120
expect a very big you take big update step small small update step

335
00:22:55,240 --> 00:23:01,180
the similar construction here this is a multiplicative update for the sigma

336
00:23:01,240 --> 00:23:02,910
for the sigma square

337
00:23:02,960 --> 00:23:08,150
and again the sigma i scored the old one act as scaling parameter is a

338
00:23:08,150 --> 00:23:10,330
step size parameters you like

339
00:23:10,340 --> 00:23:13,280
so that accounts for the fact that

340
00:23:13,340 --> 00:23:17,960
weights with larger variance are updated more than weight smaller variance

341
00:23:18,070 --> 00:23:20,230
and now we have these terms here

342
00:23:20,330 --> 00:23:22,150
these functions here and they

343
00:23:22,170 --> 00:23:27,210
basically encode the fact that if this some of the weights of the use of

344
00:23:27,210 --> 00:23:29,300
the weights was negative

345
00:23:29,370 --> 00:23:33,750
we're on this side here then there would be some kind of surprise we observed

346
00:23:33,830 --> 00:23:37,400
click this is the these are the updates for the click event but the system

347
00:23:37,400 --> 00:23:42,140
predicted no click so then we'll have a big update here

348
00:23:42,180 --> 00:23:48,640
whereas if we if this one is already positive then we only have a small

349
00:23:48,650 --> 00:23:54,600
so they all this comes out of an analysis of approximate bayesian inference using expectation

350
00:23:59,290 --> 00:24:04,930
these are basically the the updates rules that we're using and now we can look

351
00:24:04,930 --> 00:24:08,390
a little bit of what the what type of parameters come out of this

352
00:24:08,450 --> 00:24:13,700
for example here we're looking at all the weights of the client IP client p

353
00:24:13,700 --> 00:24:19,660
can be interesting the client APP can be interesting because it tells you about particular

354
00:24:19,660 --> 00:24:23,570
users for example above maybe using a particular client IP so you want to know

355
00:24:23,570 --> 00:24:28,270
that would be very good in explaining why the is always clicking or never clicking

356
00:24:30,080 --> 00:24:38,560
you can see here that in fact this feature alone allows us to find local

357
00:24:38,610 --> 00:24:41,330
these are probably bots that never click

358
00:24:41,760 --> 00:24:46,260
and these are high clickers that will click with very high probability

359
00:24:46,260 --> 00:24:53,400
and we also relayed this information to of fraud department because click clicks by clearly

360
00:24:53,400 --> 00:24:56,360
on anything you want to charge customers for

361
00:24:56,420 --> 00:25:00,110
and something that was very into important for us

362
00:25:00,120 --> 00:25:02,140
it was to have a good calibration

363
00:25:02,160 --> 00:25:04,280
and so

364
00:25:04,280 --> 00:25:08,530
we were looking a lot of these calibration plot so what this basically is the

365
00:25:08,530 --> 00:25:13,540
plot of the predicted probability of the first the empirical probability of click where of

366
00:25:13,540 --> 00:25:16,690
course the empirical probability of think we can only get if we have

367
00:25:16,780 --> 00:25:21,660
if we have some binning going on here so this is basically just pinning this

368
00:25:21,660 --> 00:25:26,090
line here into one hundred or in this case maybe a thousand bins

369
00:25:26,150 --> 00:25:30,040
and looking at all the impression seeing what prediction we make for them to decide

370
00:25:30,180 --> 00:25:34,960
which been they end up here and then averaging for those bins what the empirical

371
00:25:35,630 --> 00:25:39,220
probability of click was and you see here that this is a very nice to

372
00:25:39,220 --> 00:25:40,740
calibrate system

373
00:25:40,740 --> 00:25:45,260
which is not always something that you would automatically expect for example if you're using

374
00:25:45,260 --> 00:25:49,900
a naive bayes classifier instead you will see alot of over prediction

375
00:25:49,960 --> 00:25:56,270
going on here because those features will be highly correlated sometimes that you have inputs

376
00:25:56,280 --> 00:26:01,380
and then you will have double counting in the in the naive bayes

377
00:26:01,400 --> 00:26:02,860
so just to wrap up

378
00:26:02,910 --> 00:26:09,510
one of the system gives as in production is an automatic learning rate per feature

379
00:26:10,340 --> 00:26:16,410
it is highly calibrated which is very desirable q one charge your customers based on

380
00:26:16,410 --> 00:26:17,930
on this estimate

381
00:26:18,010 --> 00:26:23,120
we can use a lot of features even if they are correlated because we have

382
00:26:23,130 --> 00:26:27,940
the the fact that in an update the presence of features are taking into account

383
00:26:27,940 --> 00:26:33,230
by the updates are not assume independence and conditional independence in base

384
00:26:33,250 --> 00:26:37,560
we model the uncertainty explicitly and the cost for that is basically since we assume

385
00:26:37,570 --> 00:26:43,280
diagonal prior just twice as many parameters because instead of just waits we have the

386
00:26:43,290 --> 00:26:47,090
means and variances for the weights and it turned out that that was what the

387
00:26:47,090 --> 00:26:50,000
really good decision to enable these other things

388
00:26:50,040 --> 00:26:54,810
it also gives us this natural exploration mode where we can really explore the pool

389
00:26:54,820 --> 00:27:00,300
of ads in this dynamic system where the predictor is in the loop and decides

390
00:27:01,070 --> 00:27:03,890
its own input data

391
00:27:03,890 --> 00:27:06,050
aggression males

392
00:27:06,060 --> 00:27:11,420
i mean i'm summarizing means not a technical term as females and in males are

393
00:27:11,680 --> 00:27:13,920
more physically violent

394
00:27:14,000 --> 00:27:17,820
there are more violence in the room in utero

395
00:27:17,830 --> 00:27:20,100
there are more violent as children

396
00:27:20,110 --> 00:27:22,980
and the more violent as adults

397
00:27:24,670 --> 00:27:25,980
this is not to say

398
00:27:26,000 --> 00:27:28,140
that you can find violent women

399
00:27:28,140 --> 00:27:34,630
or nonviolent issues on average there is this difference became more

400
00:27:34,650 --> 00:27:36,100
males kick more

401
00:27:36,110 --> 00:27:37,230
in the uterus

402
00:27:37,260 --> 00:27:40,960
i'm as children are more involved in play fighting and violence

403
00:27:40,970 --> 00:27:44,980
combat like sports and as adults wherever you go

404
00:27:45,000 --> 00:27:47,200
you will find prison

405
00:27:47,210 --> 00:27:48,460
and wherever you go

406
00:27:48,470 --> 00:27:52,010
you will find that prisoners mostly philip full of men

407
00:27:52,440 --> 00:27:57,480
they are far more likely to kill one another and to harm one another male

408
00:27:57,480 --> 00:28:00,430
sex hormones test by testosterone

409
00:28:00,490 --> 00:28:03,860
artists or not the sort of thing one would want to inject and somebody

410
00:28:03,860 --> 00:28:06,300
unless you want them to turn kind i mean

411
00:28:06,350 --> 00:28:12,370
if they increased aggressiveness both in humans and other primates

412
00:28:12,380 --> 00:28:15,160
what about sexual choosing this

413
00:28:15,170 --> 00:28:18,950
two male humans and female humans differ

414
00:28:19,600 --> 00:28:21,390
the extent to which

415
00:28:21,400 --> 00:28:23,480
they will

416
00:28:23,500 --> 00:28:26,050
favor anonymous sex

417
00:28:27,460 --> 00:28:33,010
this is relevant from an evolutionary perspective because the parental investment theory predicts males should

418
00:28:33,010 --> 00:28:36,760
be more receptive to anonymous sex because

419
00:28:36,770 --> 00:28:38,100
four males

420
00:28:38,100 --> 00:28:40,810
to impregnate somebody else

421
00:28:41,800 --> 00:28:44,340
fortuitously lead to another offspring

422
00:28:44,360 --> 00:28:45,970
it may be good for you

423
00:28:46,030 --> 00:28:48,460
and doesn't carry the sort of

424
00:28:48,510 --> 00:28:51,210
the females and then have very picky

425
00:28:51,230 --> 00:28:53,850
because they have to choose carefully

426
00:28:53,870 --> 00:28:57,250
remember these systems evolved before both birth control

427
00:28:57,270 --> 00:28:59,310
and vasectomies and so on

428
00:28:59,310 --> 00:29:03,860
so what do we know cross culturally and psychologically well

429
00:29:05,470 --> 00:29:07,480
is a universally

430
00:29:07,490 --> 00:29:09,980
near are near universally male

431
00:29:11,700 --> 00:29:14,030
there are male prostitutes of course

432
00:29:14,050 --> 00:29:15,720
but contrary to some

433
00:29:15,740 --> 00:29:20,020
various fantasies and sitcoms they cater to male customers

434
00:29:20,060 --> 00:29:22,960
pornography is a human universal

435
00:29:22,990 --> 00:29:25,320
everything in every society

436
00:29:25,510 --> 00:29:28,640
males have done some sort of depictions

437
00:29:28,650 --> 00:29:33,740
the naked females for the purpose of arousal often a card them into trees

438
00:29:33,760 --> 00:29:36,690
or to sort of sculptures

439
00:29:36,810 --> 00:29:40,610
one of the weirdest findings in the last decade or so

440
00:29:40,620 --> 00:29:44,000
is that extends as well the monkey points

441
00:29:44,030 --> 00:29:44,730
and so

442
00:29:44,750 --> 00:29:49,680
some scientists to set up a situation where monkeys could

443
00:29:49,860 --> 00:29:55,390
fruit is by giving up through to look at the picture either the females hindquarters

444
00:29:55,400 --> 00:29:57,520
or of a celebrity monkey

445
00:29:57,570 --> 00:29:59,230
the socially dominant market

446
00:29:59,260 --> 00:30:04,220
some sort of a combination of people magazine penthouse

447
00:30:04,270 --> 00:30:12,410
and so there's some interest in this even by non-human primates

448
00:30:12,410 --> 00:30:16,160
what about actual preference for sexual right

449
00:30:16,870 --> 00:30:18,920
you can get this in different ways

450
00:30:18,940 --> 00:30:23,560
there is what biologists describe

451
00:30:23,570 --> 00:30:27,060
as and the coolidge effect of this here

452
00:30:27,070 --> 00:30:30,520
and the coolidge effect is based on president calvin coolidge

453
00:30:30,550 --> 00:30:34,240
and it's a story about calvin coolidge and his wife

454
00:30:34,330 --> 00:30:37,360
who being shown around the separately

455
00:30:37,400 --> 00:30:40,420
and the person showing around his wife pointed out

456
00:30:40,440 --> 00:30:43,920
that there was a lot of hands she she noticed that there's a lot of

457
00:30:43,920 --> 00:30:46,120
hands but only one was

458
00:30:46,130 --> 00:30:49,470
and she yes the guy showing around is one renowned

459
00:30:49,490 --> 00:30:50,740
i said well you know

460
00:30:50,770 --> 00:30:54,920
the rooster to work very hard to resist sex dozens of times a day

461
00:30:55,000 --> 00:30:58,260
and and she said well we should tell elected president

462
00:30:58,280 --> 00:30:59,550
the story goes

463
00:30:59,550 --> 00:31:03,500
president went around that the guy tells the story to the president

464
00:31:03,530 --> 00:31:07,200
the present master has sex dozens of times each day

465
00:31:07,210 --> 00:31:09,600
same hand every time

466
00:31:09,610 --> 00:31:15,360
this is no different and every time he says tell it to missus coolidge

467
00:31:16,390 --> 00:31:20,450
now there's two responses to this sort of story

468
00:31:20,480 --> 00:31:23,810
and what kind of negative with it will everybody knows

469
00:31:23,850 --> 00:31:27,660
males prefer anonymous sex with strange when the

470
00:31:28,440 --> 00:31:29,880
other responses

471
00:31:29,890 --> 00:31:32,650
that's sexist claptrap

472
00:31:32,670 --> 00:31:36,420
you might think you might be email and say that's not me

473
00:31:36,420 --> 00:31:39,930
you might know males and say the males i know i like that

474
00:31:39,950 --> 00:31:43,310
so how do you find out what is indirect measures such as who go to

475
00:31:44,730 --> 00:31:50,920
but is also fairly direct measures one fairly direct merger is used as people

476
00:31:50,970 --> 00:31:52,780
an anonymous surveys

477
00:31:52,800 --> 00:31:57,380
so in fact give you some anonymous surveys are not as people use disaster

478
00:31:57,430 --> 00:31:58,490
so so

479
00:31:58,510 --> 00:32:02,510
so for instance

480
00:32:02,530 --> 00:32:05,380
i want everybody to consider this question

481
00:32:05,380 --> 00:32:08,930
how many sexual partners you want to have in the next months

482
00:32:08,960 --> 00:32:11,130
what is coming up to april

483
00:32:11,130 --> 00:32:13,180
how many sexual partners you want to

484
00:32:13,200 --> 00:32:15,850
to see

485
00:32:15,880 --> 00:32:17,660
the next two years

486
00:32:17,710 --> 00:32:19,770
think many through graduation

487
00:32:19,780 --> 00:32:23,380
when you leave you only one i had x sexual partners and that's what i

488
00:32:25,210 --> 00:32:26,400
your lifetime

489
00:32:26,400 --> 00:32:30,630
we get people to answer these questions professor china

490
00:32:30,680 --> 00:32:34,980
last year in this course had clickers got people than now

491
00:32:35,080 --> 00:32:38,690
do it we are not so high tech soldiers do it in our heads

492
00:32:38,810 --> 00:32:41,990
but here's the way the answers come out

493
00:32:42,000 --> 00:32:44,950
women say less than one next month that doesn't mean they want

494
00:32:44,970 --> 00:32:49,240
less than one it means many of them is that many of them say zero

495
00:32:49,260 --> 00:32:51,620
some say one and so on

496
00:32:53,360 --> 00:32:55,180
forty five

497
00:33:02,850 --> 00:33:05,120
you can ask other questions

498
00:33:05,140 --> 00:33:07,480
from this population to seagrass

499
00:33:07,510 --> 00:33:12,110
when you have sex with the desirable partner you have now so really desirable

500
00:33:12,120 --> 00:33:13,660
for a year

501
00:33:13,670 --> 00:33:15,160
women say i

502
00:33:15,210 --> 00:33:18,700
yes six months and sure we less now

503
00:33:22,370 --> 00:33:24,530
well many we get a majority

504
00:33:24,550 --> 00:33:26,370
going down

505
00:33:26,390 --> 00:33:28,600
two to five minutes

506
00:33:29,060 --> 00:33:34,930
this all q and a pen and pencil sort of things

507
00:33:34,960 --> 00:33:38,360
some grave scientists have actually done experiments

508
00:33:38,360 --> 00:33:41,170
and in one experiment

509
00:33:42,250 --> 00:33:45,200
i don't you know this is sort thing which you probably wouldn't do nowadays is

510
00:33:45,200 --> 00:33:50,500
more than ten years ago we have an incredibly attractive man incredibly attractive woman

511
00:33:50,520 --> 00:33:53,180
and they approached people on campus

512
00:33:53,190 --> 00:33:56,090
not from campus to actors brought

513
00:33:56,100 --> 00:33:58,670
and they go to people to strangers and they say

514
00:33:58,670 --> 00:34:02,610
what's the easiest way of actually finding a maximum

515
00:34:02,620 --> 00:34:05,600
easier than doing hill climbing

516
00:34:12,770 --> 00:34:15,050
that's that's what we were that

517
00:34:15,090 --> 00:34:19,480
that's we worked out but sort of the the principle we used was said to

518
00:34:19,680 --> 00:34:22,130
equal to zero

519
00:34:23,090 --> 00:34:24,550
let's say OK

520
00:34:27,170 --> 00:34:28,160
d by

521
00:34:28,170 --> 00:34:33,150
debated j of log conditional likelihood equals zero

522
00:34:33,210 --> 00:34:39,220
you know maybe will be able to solve this equals zero may we won't

523
00:34:39,230 --> 00:34:41,430
what do we get

524
00:34:41,470 --> 00:34:44,910
we get

525
00:34:44,960 --> 00:34:48,640
so this is some of the i

526
00:34:48,650 --> 00:34:53,050
of why i minus p i

527
00:34:53,080 --> 00:34:54,150
x i j

528
00:34:54,180 --> 00:34:55,300
so we get

529
00:34:55,340 --> 00:34:57,860
some of the i

530
00:34:57,870 --> 00:35:00,530
y i x i j

531
00:35:00,580 --> 00:35:03,560
because some of the i

532
00:35:03,570 --> 00:35:06,300
here i x i j

533
00:35:08,060 --> 00:35:12,630
remember this is over the training data

534
00:35:12,650 --> 00:35:15,380
and why is the training labels

535
00:35:15,420 --> 00:35:17,220
and then the PRI's

536
00:35:17,230 --> 00:35:21,020
the predictive probabilities for the

537
00:35:21,060 --> 00:35:22,830
training examples

538
00:35:23,680 --> 00:35:26,200
this equality is going to be true

539
00:35:26,210 --> 00:35:28,340
for the trained model

540
00:35:29,350 --> 00:35:31,740
y equals

541
00:35:32,660 --> 00:35:37,130
training labels

542
00:35:37,130 --> 00:35:38,630
p i

543
00:35:40,000 --> 00:35:43,030
predicted probabilities

544
00:35:43,040 --> 00:35:44,340
by the

545
00:35:44,350 --> 00:35:50,970
fully trained model

546
00:35:54,620 --> 00:35:58,120
somebody explain so in intuitively what it's saying

547
00:35:58,130 --> 00:36:06,300
that the that this equals the sum

548
00:36:16,540 --> 00:36:19,380
this is

549
00:36:19,480 --> 00:36:22,870
this is the sum of all training examples

550
00:36:22,890 --> 00:36:25,880
this is the sum of all the training examples so

551
00:36:25,900 --> 00:36:30,570
this is not really saying anything about any individual training examples

552
00:36:30,570 --> 00:36:34,740
it is only saying something about the training set

553
00:36:34,790 --> 00:36:45,020
so what is it saying about the training set

554
00:36:46,670 --> 00:36:48,530
here's my

555
00:36:48,530 --> 00:36:50,800
interpretation it was a so

556
00:36:52,480 --> 00:36:55,860
take any feature xj

557
00:36:59,810 --> 00:37:03,100
total value in the training set

558
00:37:03,140 --> 00:37:05,780
for the positive examples

559
00:37:05,780 --> 00:37:06,950
so here

560
00:37:06,970 --> 00:37:09,330
we're taking any feature xj

561
00:37:09,340 --> 00:37:13,020
and then was summing over the training set would not have nine y y i

562
00:37:13,060 --> 00:37:15,640
so we really only summing

563
00:37:16,530 --> 00:37:21,560
some of the ice such that y equals one

564
00:37:25,480 --> 00:37:27,930
x idea maybe

565
00:37:28,020 --> 00:37:29,090
this is

566
00:37:30,070 --> 00:37:35,230
most informative way of writing so the sum over the whole training set the sum

567
00:37:35,230 --> 00:37:39,270
of all the positive training examples x i j

568
00:37:39,280 --> 00:37:42,020
is the sum of all the training examples

569
00:37:42,090 --> 00:37:45,050
of x i j times p i

570
00:37:45,060 --> 00:37:48,810
yes so this is the

571
00:37:48,870 --> 00:37:51,140
expected value

572
00:37:56,890 --> 00:38:00,370
where the expectation is according to number

573
00:38:01,560 --> 00:38:03,720
and this

574
00:38:03,750 --> 00:38:05,040
it's a

575
00:38:05,040 --> 00:38:07,430
x this is a

576
00:38:12,800 --> 00:38:19,770
so the

577
00:38:20,780 --> 00:38:23,280
the peas

578
00:38:26,340 --> 00:38:27,360
the the

579
00:38:27,380 --> 00:38:30,600
predicted probability to get the training set

580
00:38:34,740 --> 00:38:40,050
that that typically they're non-zero every training example

581
00:38:40,800 --> 00:38:44,630
they have to spread themselves out so that each feature

582
00:38:44,680 --> 00:38:50,040
the the average value of that feature for the positive examples is the same as

583
00:38:50,050 --> 00:38:51,920
the weighted average value

584
00:38:51,940 --> 00:38:54,550
that feature for all the examples

585
00:38:54,560 --> 00:38:58,380
using the predictive politics as the weights

586
00:39:01,980 --> 00:39:08,480
his special case

587
00:39:09,100 --> 00:39:12,880
some of

588
00:39:12,890 --> 00:39:16,130
i such that y equals one

589
00:39:17,340 --> 00:39:20,130
x y zero

590
00:39:21,250 --> 00:39:23,440
some of the i

591
00:39:23,450 --> 00:39:26,130
PRI times x i

592
00:39:27,350 --> 00:39:34,390
and what we say x i zero equals

593
00:39:34,390 --> 00:39:37,400
yes this ties in a little bit with what you're talking about

594
00:39:37,410 --> 00:39:41,270
getting more users getting more people involved

595
00:39:41,930 --> 00:39:46,550
i will talk so much about the details of what octave can do

596
00:39:46,560 --> 00:39:51,350
you know features that has because you can go and downloaded around it and find

597
00:39:51,350 --> 00:39:53,330
out for yourself

598
00:39:54,420 --> 00:39:57,670
as before start though

599
00:39:57,690 --> 00:40:03,710
how many people here use matlab how many people are more familiar with mental abnormalities

600
00:40:03,740 --> 00:40:06,980
and how many people have you

601
00:40:07,010 --> 00:40:09,900
so almost as many or more

602
00:40:10,790 --> 00:40:13,610
OK well that's that's good enough

603
00:40:13,620 --> 00:40:19,340
so a brief discussion of the history of octave

604
00:40:20,230 --> 00:40:25,550
discussion of ways we might expand the community of developers and then discuss some some

605
00:40:25,550 --> 00:40:27,790
of the challenges for the future of the project

606
00:40:28,200 --> 00:40:30,520
but if i also

607
00:40:30,580 --> 00:40:33,440
think some people who have made octave possible

608
00:40:33,480 --> 00:40:38,510
the first is jim rawlings use my thesis adviser texas and and my is also

609
00:40:38,510 --> 00:40:40,230
been my supervisor for

610
00:40:40,240 --> 00:40:45,880
the last sixteen years and he he secured nearly all the funding to

611
00:40:45,880 --> 00:40:49,380
employ me to work on active from that time so i'm really grateful

612
00:40:49,970 --> 00:40:54,410
after had him as an adviser and someone who is a champion of active all

613
00:40:54,410 --> 00:40:55,600
this time

614
00:40:56,120 --> 00:41:00,080
all the volunteers who have worked on octave over the years there's no way it

615
00:41:00,080 --> 00:41:03,070
would have been as successful as it is without them

616
00:41:03,080 --> 00:41:05,320
and then i also have acknowledge

617
00:41:05,320 --> 00:41:09,660
the canoe project richard stallman because that was my introduction to free software

618
00:41:09,680 --> 00:41:12,320
and without that i don't think i would have become interested in

619
00:41:12,320 --> 00:41:14,430
producing free software on my own

620
00:41:14,470 --> 00:41:18,220
so what this is that

621
00:41:18,230 --> 00:41:23,570
three system for numerical computations mostly compatible with matlab at the syntax level doesn't have

622
00:41:23,570 --> 00:41:25,620
a fancy IDE

623
00:41:25,670 --> 00:41:28,150
although some people don't like that very much anyway so i don't know if it's

624
00:41:28,150 --> 00:41:31,540
a big loss

625
00:41:31,590 --> 00:41:33,870
it does most of these things and more

626
00:41:37,520 --> 00:41:42,320
in the last few years it's it's really changed quite a bit is does the

627
00:41:42,320 --> 00:41:45,700
the number of people working on has really

628
00:41:45,720 --> 00:41:49,680
can increased and i've seen a lot more activity and

629
00:41:51,190 --> 00:41:52,380
for example

630
00:41:52,440 --> 00:41:54,890
on this list of all these things

631
00:41:55,410 --> 00:41:59,640
an emotional raise sparse matrices

632
00:41:59,650 --> 00:42:05,450
mister the control theory functions on the improvements in graphics signal processing image almost all

633
00:42:05,470 --> 00:42:07,450
these things are things that other people have done

634
00:42:07,530 --> 00:42:08,950
and i things that i wrote

635
00:42:10,500 --> 00:42:12,210
and that was true

636
00:42:12,220 --> 00:42:16,130
i guess the first four five years working in october i was

637
00:42:16,180 --> 00:42:17,940
if you look back in the change logs

638
00:42:17,990 --> 00:42:18,860
my name

639
00:42:18,880 --> 00:42:19,940
all the time

640
00:42:20,460 --> 00:42:25,020
and that was not a very good model for developing a project like this

641
00:42:25,410 --> 00:42:30,270
there are some reasons why that happened on going into a little bit

642
00:42:30,290 --> 00:42:32,930
so why would you want to use active well there's no license manager is free

643
00:42:32,930 --> 00:42:35,540
software so you can put it anywhere

644
00:42:37,320 --> 00:42:41,400
black boxes so you can look at everything not just the

645
00:42:41,410 --> 00:42:42,350
m files

646
00:42:42,370 --> 00:42:43,320
you can look at

647
00:42:43,330 --> 00:42:44,490
how do we do

648
00:42:44,510 --> 00:42:46,080
you know

649
00:42:46,090 --> 00:42:48,170
how do we solve linear equations

650
00:42:48,180 --> 00:42:49,430
down to the level of

651
00:42:49,470 --> 00:42:53,420
you know you know they say in in malabar uses was but you don't know

652
00:42:53,420 --> 00:42:54,960
exactly what they do

653
00:42:54,970 --> 00:42:59,850
before they call the hours routines are how they call an output sequence happens so

654
00:43:00,300 --> 00:43:05,860
that portable opposing systems with standard c plus plus compiler so you can put it

655
00:43:06,750 --> 00:43:10,070
any kind of system reported to mobile phones

656
00:43:12,010 --> 00:43:17,440
there is a large support community and

657
00:43:17,460 --> 00:43:21,060
it's it's growing more or less although

658
00:43:21,080 --> 00:43:22,800
australia applied in one of the

659
00:43:23,760 --> 00:43:25,890
we'll see if that trend will continue

660
00:43:29,720 --> 00:43:30,690
i not

661
00:43:30,720 --> 00:43:32,230
set out to write

662
00:43:32,320 --> 00:43:33,990
alan clone

663
00:43:34,010 --> 00:43:36,720
that's that's how it happened but

664
00:43:36,770 --> 00:43:39,350
i started to write some software

665
00:43:40,440 --> 00:43:43,410
my supervisor jim rawlings wanted to use as

666
00:43:43,420 --> 00:43:46,360
software to go along with the textbook was writing

667
00:43:47,750 --> 00:43:50,620
we this is about nineteen eighty nine

668
00:43:50,640 --> 00:43:52,420
we started talking about this

669
00:43:53,420 --> 00:43:55,890
at that point we were four can users

670
00:43:55,940 --> 00:43:59,210
so we thought well we could write some fortran routines

671
00:43:59,220 --> 00:44:03,950
target mapping shipment this get stuck in the back of the book

672
00:44:05,310 --> 00:44:07,870
after thinking about that for a while we decided that that was not going to

673
00:44:07,870 --> 00:44:10,460
be successful we saw other people try that

674
00:44:10,480 --> 00:44:14,320
and the students can use the software it was worthless to them

675
00:44:14,330 --> 00:44:17,830
and and we knew people who had started use mallory time and it seemed like

676
00:44:17,830 --> 00:44:21,440
an easy system to use so we thought well maybe we can do something like

677
00:44:21,440 --> 00:44:23,610
that it wouldn't be

678
00:44:23,660 --> 00:44:26,570
now i have it would be sort of like it we we could distributed to

679
00:44:26,570 --> 00:44:34,850
OK so this is the tutorial

680
00:44:34,910 --> 00:44:40,570
so it's not just my material on presenting and it's on looking at a class

681
00:44:40,570 --> 00:44:45,770
of models which can deal with recognition of object classes

682
00:44:45,790 --> 00:44:48,490
and i'm going to cover

683
00:44:48,490 --> 00:44:49,990
material from

684
00:44:50,000 --> 00:44:51,270
myself but also

685
00:44:51,290 --> 00:44:56,020
from other groups that focus group she

686
00:44:56,040 --> 00:44:58,110
shimon ullman OK

687
00:44:58,130 --> 00:45:04,270
in terms of objectives the get this work is to be able to recognise visual

688
00:45:05,590 --> 00:45:10,740
like for example this motorbike which is the visual class

689
00:45:10,810 --> 00:45:13,010
and in particular

690
00:45:13,980 --> 00:45:18,490
ten is designed to be able to work from what's called semi unsupervised learning or

691
00:45:18,490 --> 00:45:20,820
define what these two things are so

692
00:45:22,140 --> 00:45:23,200
it means here

693
00:45:23,210 --> 00:45:27,340
being able to identify the object class for example that it's a motorbike or a

694
00:45:27,340 --> 00:45:30,130
plane or a person

695
00:45:30,270 --> 00:45:34,340
and also as well as identifying what classes there

696
00:45:34,810 --> 00:45:37,730
specifying the location coarsely

697
00:45:37,730 --> 00:45:39,730
OK so here you see

698
00:45:39,870 --> 00:45:41,480
the cars

699
00:45:41,930 --> 00:45:44,960
the region of interest which is the car inside

700
00:45:45,010 --> 00:45:46,960
in this talk i'm not going to go this far

701
00:45:46,960 --> 00:45:49,700
and segmentation so this part of the bottom here

702
00:45:49,710 --> 00:45:55,270
we're not demanding that we're demanding that the recognition method exactly gives the alpha matter

703
00:45:55,310 --> 00:45:57,040
the object that's not necessary here

704
00:45:57,060 --> 00:46:00,880
what we want is a rough localisation of the objects

705
00:46:02,900 --> 00:46:07,150
semi unsupervised learning what that means is that the island and learned from a set

706
00:46:07,800 --> 00:46:09,290
images which have been

707
00:46:09,300 --> 00:46:12,850
objects in them in this case faces

708
00:46:14,190 --> 00:46:17,890
that's all the information is provided a set of images which has the object in

709
00:46:17,890 --> 00:46:21,550
them and then a set of what's called background images where the object is not

710
00:46:23,090 --> 00:46:27,150
and in the training images here the

711
00:46:27,190 --> 00:46:31,050
the position of the object is specified nor the features OK so the idea is

712
00:46:31,050 --> 00:46:32,850
to learn from this type of data

713
00:46:32,860 --> 00:46:36,070
that's what we by semi unsupervised

714
00:46:36,090 --> 00:46:40,310
so here some of classes will be talking about

715
00:46:40,320 --> 00:46:43,250
you look at the columns here

716
00:46:43,270 --> 00:46:46,710
you see motorbikes airplanes faces

717
00:46:46,720 --> 00:46:51,440
cars side cars rear spotted cats and some background images

718
00:46:51,460 --> 00:46:56,140
now one thing to notice is that the cars went about cars objects which we've

719
00:46:56,140 --> 00:46:58,630
got cars here twice because

720
00:46:58,670 --> 00:47:00,430
the album so far

721
00:47:00,450 --> 00:47:01,570
we have to have

722
00:47:01,600 --> 00:47:06,530
a different recognition systems for the various visual aspects of the side of the car

723
00:47:06,600 --> 00:47:07,840
the rear of the car

724
00:47:07,860 --> 00:47:11,430
they can be recognised separately for the moment

725
00:47:11,450 --> 00:47:13,270
because this is significantly different

726
00:47:14,740 --> 00:47:18,690
OK now why is so difficult problem people aren't really computer vision people here these

727
00:47:18,690 --> 00:47:22,400
are the typical vision things one has to do with have size variation you see

728
00:47:22,400 --> 00:47:24,680
the cars here here very small or very large

729
00:47:24,690 --> 00:47:27,080
so due perspective size variation

730
00:47:27,090 --> 00:47:30,950
background clutter that's all this stuff which is the

731
00:47:32,500 --> 00:47:36,300
OK that has to be that we don't know what's the foreground object which is

732
00:47:36,300 --> 00:47:37,980
the background objects

733
00:47:38,020 --> 00:47:40,780
also this partial occlusion here for example

734
00:47:40,790 --> 00:47:45,580
so the object class were interested in might not be completely visible

735
00:47:45,600 --> 00:47:49,370
and the key thing we have to deal with this is this intraclass variation

736
00:47:49,380 --> 00:47:52,690
we're not looking at the same object were looking at a class of objects going

737
00:47:52,690 --> 00:47:54,210
down the columns

738
00:47:57,160 --> 00:48:01,580
now the class of model i'm going to talk about this troilus is what's called

739
00:48:01,580 --> 00:48:03,290
a pictorial structure

740
00:48:03,340 --> 00:48:06,600
this originated back in the early seventies

741
00:48:06,610 --> 00:48:11,540
and is the schematic of the idea you have parts

742
00:48:11,900 --> 00:48:14,070
the object surface this was the face should have hair

743
00:48:14,100 --> 00:48:15,880
mouth eyes nose

744
00:48:15,900 --> 00:48:18,650
that's one aspect of these parts which are going to turn out to be two

745
00:48:18,650 --> 00:48:20,540
d image fragments

746
00:48:20,650 --> 00:48:25,420
and then you have spatial relationships between these parts of the two component parts and

747
00:48:25,420 --> 00:48:28,220
their spatial organisation configuration of these parts

748
00:48:28,280 --> 00:48:30,160
that's what has to be learned

749
00:48:30,180 --> 00:48:36,510
this class model very successful in the past five years also because i just recently

750
00:48:36,510 --> 00:48:40,820
this class models being combined with learning techniques that's made it sufficiently powerful to recognise

751
00:48:40,820 --> 00:48:42,550
all these different visual classes

752
00:48:42,560 --> 00:48:44,600
and as you'll see

753
00:48:44,610 --> 00:48:49,310
the reason it's powerful is it captures what's meant by visual class his

754
00:48:49,330 --> 00:48:50,810
a few examples of the car

755
00:48:50,820 --> 00:48:53,490
and we can identify things which in common this is

756
00:48:53,520 --> 00:48:55,030
front wheel

757
00:48:55,410 --> 00:48:57,840
the top of the windscreen

758
00:48:59,080 --> 00:49:03,240
this is what this is defining what's in common between these instances of the car

759
00:49:03,250 --> 00:49:05,610
and there are two things i had to see

760
00:49:05,630 --> 00:49:08,940
if you if you try to put these in correspondence you've got the part like

761
00:49:08,940 --> 00:49:10,810
this like mapping to this light

762
00:49:10,820 --> 00:49:14,890
and you got the spatial configuration of these parts of the these three parts occur

763
00:49:14,890 --> 00:49:18,950
in the same spatial configuration because that's what's capturing the visual class

764
00:49:18,960 --> 00:49:24,850
and what you see is that you can build some generalisation into the definition of

765
00:49:24,850 --> 00:49:28,850
parts and into the definition of this of the structure

766
00:49:30,250 --> 00:49:33,850
one of the advantages of these parts methods is that

767
00:49:34,460 --> 00:49:35,690
it allows

768
00:49:35,700 --> 00:49:38,600
as you can see certain you can

769
00:49:38,740 --> 00:49:42,440
controls defamation OK so a and b here

770
00:49:42,450 --> 00:49:44,400
we would say faces

771
00:49:44,420 --> 00:49:47,690
c and d are not and maybe we can learn this information what constitutes a

772
00:49:50,770 --> 00:49:53,500
by having these parts rather than the global model

773
00:49:53,500 --> 00:49:57,600
and what noise was and say to yourself philosophically what is it is it something

774
00:49:57,600 --> 00:49:58,450
that is independent

775
00:49:59,850 --> 00:50:02,300
non this definition this could be dependent areas

776
00:50:03,980 --> 00:50:04,800
it's actually i think

777
00:50:05,470 --> 00:50:07,920
for me the best definition noise is it

778
00:50:08,510 --> 00:50:09,680
the piece if your system

779
00:50:10,340 --> 00:50:11,700
that you're not counting poriginal

780
00:50:13,680 --> 00:50:17,320
now very often we then assume it's independent because we can't do much if we

781
00:50:17,320 --> 00:50:21,370
don't assume is independent we still have to do some more modelling the noise but

782
00:50:21,370 --> 00:50:24,570
i think it's a nice when you think about the physics side of things

783
00:50:25,320 --> 00:50:27,950
these are the bits in the system you are interested in and these are the

784
00:50:27,950 --> 00:50:29,980
bits in the system you're not interested in yeah

785
00:50:31,040 --> 00:50:34,590
that's one way of thinking about noise not saying is the only way it's probably

786
00:50:34,590 --> 00:50:37,510
not the original definition noise because the way i like to think about noise

787
00:50:38,620 --> 00:50:40,640
so the problem solved by sampling

788
00:50:41,810 --> 00:50:46,590
these these values from some distribution and the person thought about this so much with le plus

789
00:50:48,420 --> 00:50:50,100
but class and the class

790
00:50:50,560 --> 00:50:51,220
i believed

791
00:50:53,060 --> 00:50:54,780
so the maximum entropy like idea

792
00:50:55,720 --> 00:50:56,460
solar plants

793
00:50:57,600 --> 00:50:59,500
so instead what if i don't know something

794
00:51:00,110 --> 00:51:02,060
i should assume is uniformly distributed

795
00:51:03,320 --> 00:51:07,680
and that's kinda reasonable assumption that's actually this maximum entropy textile assumption

796
00:51:09,680 --> 00:51:13,100
and it's great but it doesn't work very well on continuous systems so that apply

797
00:51:13,130 --> 00:51:16,980
this discrete systems like coin tosses it applied to

798
00:51:17,600 --> 00:51:19,160
justify his choice prior

799
00:51:19,850 --> 00:51:23,130
and we feel justified prior distributions in his in bayesian inference

800
00:51:24,210 --> 00:51:28,950
uh ceases about and i'm going assume uniform now people can argue against this assumption

801
00:51:30,270 --> 00:51:32,490
there are many arguments against it but it some

802
00:51:33,020 --> 00:51:34,900
however it is this principle of least

803
00:51:35,370 --> 00:51:35,880
something rather

804
00:51:36,640 --> 00:51:38,300
can't remember it's he

805
00:51:39,040 --> 00:51:44,090
it's a reasonable thing to do and it's effectively what maximum is doing it problematically

806
00:51:44,090 --> 00:51:47,260
though you can't assume uniform distributions across

807
00:51:48,120 --> 00:51:49,490
uh infinite spaces

808
00:51:50,430 --> 00:51:53,880
so he spent an enormous amount of effort trying to build

809
00:51:55,020 --> 00:51:56,310
what they call error functions

810
00:51:56,800 --> 00:52:02,620
which reflected these types of ideas that they will heavy tailed are functions they had quite complex forms

811
00:52:03,650 --> 00:52:06,750
he said he said well it should be uniform across this interval and uniform across

812
00:52:06,750 --> 00:52:10,940
this interval and spend a lot time constructing these things very difficult and when he

813
00:52:10,940 --> 00:52:11,420
got them

814
00:52:11,830 --> 00:52:15,850
beating them to data with virtually impossible because he just had a pen and paper

815
00:52:18,190 --> 00:52:21,450
i guess is if it whoever it is whichever lower frenchman

816
00:52:23,260 --> 00:52:27,800
he invented squares he comes along and says that minimize the squared error

817
00:52:29,060 --> 00:52:31,860
and actually doesn't motivated from the error function point-of-view

818
00:52:33,490 --> 00:52:37,330
why gauss is known for the calcium is all he does is plug these two

819
00:52:37,330 --> 00:52:41,690
ideas together class if the error function man that says this is the principle what's

820
00:52:41,690 --> 00:52:42,120
going on

821
00:52:43,210 --> 00:52:44,690
the other frenchmen if they

822
00:52:45,360 --> 00:52:50,500
person algorithmically is a reasonable way of doing it and gal says okay if you

823
00:52:50,500 --> 00:52:53,050
make the error function this form the calcium

824
00:52:54,080 --> 00:52:59,190
then you get least squares now ironically in gasses paper he does nothing to justify

825
00:52:59,190 --> 00:53:01,730
the choice over a function other than saying

826
00:53:02,720 --> 00:53:06,440
if you to error function you get least squares and we know least squares works

827
00:53:06,440 --> 00:53:09,790
well so this must be good error function so it's still a circular

828
00:53:10,280 --> 00:53:14,740
argument and of course i think it's plastic produces one of the earliest versions of

829
00:53:14,740 --> 00:53:16,830
the central limit theorem which is the real

830
00:53:17,230 --> 00:53:19,900
so the physical justification as to why this might be true

831
00:53:20,400 --> 00:53:25,350
if you have a bunch of errors as long as there very limited and additive

832
00:53:25,810 --> 00:53:29,900
you will eventually have a guassian air so that's a good at a reasonable justification

833
00:53:29,900 --> 00:53:32,250
and gas in noise model i

834
00:53:32,300 --> 00:53:32,530
and it

835
00:53:33,050 --> 00:53:34,830
it's the plants that first arrives the

836
00:53:35,440 --> 00:53:38,000
in distribution in the context bayesian inference

837
00:53:38,780 --> 00:53:44,240
it's placid that doesn't notice that it's a sensible distribution forearm we are finally comes

838
00:53:44,240 --> 00:53:45,700
up with the concept that the error function

839
00:53:46,830 --> 00:53:47,670
and it's class

840
00:53:48,840 --> 00:53:51,790
demonstrates in effect why the gas might be an error function

841
00:53:52,240 --> 00:53:54,050
but we still call the distribution gaussians

842
00:53:55,020 --> 00:53:59,350
and the class gets back one which is just the absolute value two things which

843
00:53:59,600 --> 00:54:01,470
no one even knows called class in the

844
00:54:01,980 --> 00:54:02,780
this shows you

845
00:54:03,950 --> 00:54:04,660
you can't win them all

846
00:54:04,660 --> 00:54:09,330
the importance distribution similarly using exactly the same reasoning and what i've done at the

847
00:54:09,330 --> 00:54:14,800
time to you come up very close your question for the importance which is important

848
00:54:14,800 --> 00:54:18,710
for the time and its importance weight at time n minus one time what i

849
00:54:18,710 --> 00:54:22,750
call these increment or importance weights

850
00:54:22,780 --> 00:54:23,880
that's it

851
00:54:23,910 --> 00:54:30,090
so to summarize essentially the i is the kind of an important sampling type techniques

852
00:54:30,250 --> 00:54:32,490
so as to approximate

853
00:54:32,510 --> 00:54:37,540
the sequence of target distribution pi and on the associated to a normalizing constant palsied

854
00:54:37,540 --> 00:54:38,470
as follows

855
00:54:38,480 --> 00:54:41,650
five someone is so important capital

856
00:54:41,670 --> 00:54:47,280
particles called a random sample so i here goes from one to capital and capitolhillmom

857
00:54:47,280 --> 00:54:52,660
sample according to involve the distribution q on all the computer associated importance weight okay

858
00:54:53,040 --> 00:54:55,620
for this expression

859
00:54:55,630 --> 00:54:58,290
the following time index OK

860
00:54:58,300 --> 00:55:00,820
when you have time and the only thing you need to do

861
00:55:00,830 --> 00:55:05,450
is you need to solve for the component xn according to conditional distribution two and

862
00:55:05,450 --> 00:55:09,410
which can be dependent on the past values of simulated before x one x two

863
00:55:09,410 --> 00:55:12,590
x minus one on this thing something you pick up this is the degree of

864
00:55:12,590 --> 00:55:18,100
freedom in your arteries of what you do want to solve all xn you have

865
00:55:18,160 --> 00:55:23,070
data associated importance weight which is emerging which basically correct for the discrepancy between the

866
00:55:23,070 --> 00:55:27,370
target on the the importance distribution

867
00:55:28,530 --> 00:55:31,080
so essentially any time

868
00:55:31,090 --> 00:55:37,850
you have essentially a collection of capital and one them so important distributed according to

869
00:55:37,860 --> 00:55:42,720
the part of the solution to an on you have also the associated importance weight

870
00:55:42,760 --> 00:55:47,120
so when you are once you've got basically those guys OK

871
00:55:47,200 --> 00:55:49,330
then you can obviously

872
00:55:49,350 --> 00:55:52,550
approximates the target distribution

873
00:55:52,560 --> 00:55:58,460
by using one of the average of the unnormalized importance weight

874
00:56:00,100 --> 00:56:05,010
o and you can approximate basically the target distribution of interest

875
00:56:05,030 --> 00:56:10,380
but basically the weighted

876
00:56:10,410 --> 00:56:12,030
convex combination

877
00:56:12,050 --> 00:56:15,400
of the

878
00:56:15,420 --> 00:56:19,080
that i don't like mass

879
00:56:19,090 --> 00:56:21,630
which have been sampled according

880
00:56:21,650 --> 00:56:23,510
two two in case

881
00:56:23,530 --> 00:56:27,570
that's it this is what we've been doing so we have no boundaries

882
00:56:27,580 --> 00:56:32,850
basically it was an important sampling is which is very glad to be obviously power

883
00:56:32,850 --> 00:56:34,290
lies very easily

884
00:56:34,310 --> 00:56:39,810
all provide us we've essentially multicolour estimate of the target distribution of interest on z

885
00:56:40,100 --> 00:56:44,490
of the knowledge society normalizing constant so not obviously if you were to use that

886
00:56:44,500 --> 00:56:46,810
in the real world applications

887
00:56:46,820 --> 00:56:48,550
OK well

888
00:56:48,570 --> 00:56:54,070
since she's going away because what i've been doing is that just basically

889
00:56:54,090 --> 00:56:56,950
implemented importance sampling

890
00:56:56,960 --> 00:57:02,660
in potentially high dimensional space because as an increasing basically

891
00:57:02,670 --> 00:57:06,720
the dimension of the target distribution increases OK

892
00:57:06,730 --> 00:57:11,660
well i told you about something i dimensional space it typically violence of the estimate

893
00:57:11,660 --> 00:57:15,440
increases exponentially fast with the time in the show

894
00:57:15,460 --> 00:57:22,220
because the algorithm implementing is just a very specific chase of importance sampling where i

895
00:57:22,220 --> 00:57:29,130
use destructive structure sorry importance distribution i can expect to bypass this problem so that's

896
00:57:29,130 --> 00:57:35,980
basically what i've been doing here i've been basically using the sequential importance sampling ategories

897
00:57:36,030 --> 00:57:39,050
so as to approximate

898
00:57:42,120 --> 00:57:46,400
target distribution the posterior distribution

899
00:57:46,410 --> 00:57:51,240
in the state space model is a stochastic volatility model which

900
00:57:51,260 --> 00:57:53,970
so this is an example that describe

901
00:57:53,980 --> 00:57:59,210
yesterday it's summer of it relevant to our problem but

902
00:57:59,220 --> 00:58:03,460
assume you're interested basically an approximating

903
00:58:03,500 --> 00:58:05,470
the posterior distribution

904
00:58:05,490 --> 00:58:07,710
of the campaign for all states

905
00:58:07,730 --> 00:58:13,780
of the state space model given the first observation on assume use essentially the sequential

906
00:58:13,780 --> 00:58:19,460
importance sampling ategories then what you will be observing when you're on this side is

907
00:58:19,950 --> 00:58:25,670
is that after a very few times step then you're gonna have

908
00:58:25,700 --> 00:58:31,880
essentially the normalized which was results such that essentially one of them full set index

909
00:58:31,880 --> 00:58:36,470
i zero is very close to one or the other one

910
00:58:37,450 --> 00:58:43,230
four g from fuzzy very close to zero on that's coming from the fact this

911
00:58:43,230 --> 00:58:47,970
is the normalizing parliament that's coming from the fact that the violence of the unnormalized

912
00:58:47,970 --> 00:58:52,930
importance weights is increasing over time so if you look at what i've displayed is

913
00:58:52,930 --> 00:58:55,840
the nice to one essentially

914
00:58:55,850 --> 00:59:01,530
the log of the normalized importance weight at the very first time index it's fine

915
00:59:01,540 --> 00:59:04,980
importance something is no i mentioned it works kind of OK

916
00:59:06,370 --> 00:59:10,960
the time index dimensional point increase so if i try to approximate the posterior distribution

917
00:59:11,280 --> 00:59:15,690
of the fifty first state given the fifty first observation i see that essentially the

918
00:59:15,780 --> 00:59:21,680
the histogram of the much more to disperse on basically if you basically any or

919
00:59:21,680 --> 00:59:26,590
all of this is what you would upset say for a lot of the normalized

920
00:59:26,590 --> 00:59:30,600
importance weights it means that if you go back to detect the exponential of these

921
00:59:30,600 --> 00:59:33,350
changes but it doesn't have to be fixed

922
00:59:33,420 --> 00:59:35,850
i could change my external pressure

923
00:59:35,850 --> 00:59:40,890
the whole process and that's the we talked about the last being very important defining

924
00:59:40,890 --> 00:59:41,900
the path

925
00:59:41,920 --> 00:59:43,630
right so

926
00:59:43,640 --> 00:59:45,280
we're my pressure

927
00:59:45,280 --> 00:59:46,860
is changing

928
00:59:46,900 --> 00:59:49,370
and i can go directly from

929
00:59:49,420 --> 00:59:54,030
this large volume to the small i have to go in little steps

930
00:59:54,040 --> 00:59:55,900
infinitely small steps

931
00:59:55,920 --> 00:59:57,710
so instead of writing

932
00:59:58,810 --> 01:00:02,030
the the negative of external turned out to be

933
01:00:02,070 --> 01:00:04,540
i'm going to write

934
01:00:07,860 --> 01:00:09,730
why this extra

935
01:00:11,670 --> 01:00:14,860
this depends on the path and

936
01:00:14,880 --> 01:00:19,080
on pattern is change as the change

937
01:00:21,730 --> 01:00:24,270
i'm going to add a little thing here i'm going to put it will

938
01:00:24,310 --> 01:00:26,750
all right here

939
01:00:28,100 --> 01:00:29,770
o bar here means that this

940
01:00:29,790 --> 01:00:32,020
dw that i'm putting it here

941
01:00:32,120 --> 01:00:34,790
is not

942
01:00:35,850 --> 01:00:39,080
the french what i mean

943
01:00:42,350 --> 01:00:44,020
i mean that by taking into rule

944
01:00:44,040 --> 01:00:46,060
of this to find out

945
01:00:46,150 --> 01:00:48,000
how much work

946
01:00:48,020 --> 01:00:50,210
i've done the system

947
01:00:50,210 --> 01:00:52,330
i need to know the

948
01:00:52,350 --> 01:00:53,690
that's what this means here

949
01:00:53,810 --> 01:00:55,860
it's not enough to know

950
01:00:55,920 --> 01:00:58,040
the initial state

951
01:00:58,080 --> 01:01:00,830
in the final state to find w is

952
01:01:00,880 --> 01:01:02,330
you also need to know

953
01:01:02,350 --> 01:01:05,130
how you got it

954
01:01:05,150 --> 01:01:10,620
this is very different from the from the functions of state like pressure and temperature

955
01:01:10,710 --> 01:01:13,440
is the volume the temperature the pressure here

956
01:01:13,460 --> 01:01:17,900
this the volume temperature and pressure here corresponding to the system here

957
01:01:17,960 --> 01:01:20,580
and the bottom temperature and pressure doesn't care

958
01:01:20,580 --> 01:01:22,080
how you got there

959
01:01:22,130 --> 01:01:23,690
it is what it is

960
01:01:23,750 --> 01:01:26,980
it defines the state of the system

961
01:01:27,000 --> 01:01:29,360
the amount of work you put in to get here

962
01:01:29,380 --> 01:01:33,170
depends on the it's not a function of the

963
01:01:33,190 --> 01:01:35,690
it's not an exact differential so that

964
01:01:35,730 --> 01:01:40,580
delta v here's an exact differential but this w is now

965
01:01:40,630 --> 01:01:42,920
that's going to be really important

966
01:01:42,920 --> 01:01:47,130
right so if you want to find out how much work done the integral

967
01:01:47,150 --> 01:01:51,020
from the initial state to final state

968
01:01:55,770 --> 01:01:57,210
twenty two

969
01:02:01,400 --> 01:02:05,420
and you got to know what the pattern

970
01:02:05,440 --> 01:02:08,000
so let's look at this path dependence

971
01:02:15,420 --> 01:02:16,460
going to do

972
01:02:16,480 --> 01:02:19,500
two different

973
01:02:19,520 --> 01:02:21,980
see how the different

974
01:02:22,020 --> 01:02:23,980
in terms of the work

975
01:02:26,210 --> 01:02:31,100
ideal gas we can assume that it's idealistic are going forward

976
01:02:32,270 --> 01:02:34,770
noninteracting gas

977
01:02:34,790 --> 01:02:37,900
you do put compression

978
01:02:37,900 --> 01:02:39,170
they are gone

979
01:02:39,270 --> 01:02:41,000
with a certain

980
01:02:41,020 --> 01:02:44,650
gas pressure one by one

981
01:02:44,850 --> 01:02:47,310
in the final state oregon

982
01:02:50,350 --> 01:02:51,770
he two

983
01:02:51,790 --> 01:02:53,290
where v one

984
01:02:53,750 --> 01:02:56,060
is greater than two

985
01:02:58,690 --> 01:03:00,310
so if i draw this

986
01:03:00,330 --> 01:03:03,460
on the PV diagram

987
01:03:03,460 --> 01:03:07,520
it's this certain degree ofconditional independence here so you can do some multivariategaussian

988
01:03:07,520 --> 01:03:14,020
updatesor in A and lambdathe dimension not more than a number of factors

989
01:03:14,020 --> 01:03:19,120
so this this thisgoesquite well this thissix minuteson a laptop

990
01:03:19,370 --> 01:03:25,260
about thisterrible oldlaptopI've got here andyou knowit was a few years ago

991
01:03:25,260 --> 01:03:28,800
in fact when you when youconditionall sorts of thingsthat are fixedthe data

992
01:03:28,800 --> 01:03:33,760
and somefixed hyperparameters these are the variableswe're having to update and it'sfairly

993
01:03:33,760 --> 01:03:41,680
easy to organize theorganize thecomputationI'll just talk about a few issues

994
01:03:41,680 --> 01:03:48,600
one oneproblem you always have with thefactor analysis is the

995
01:03:48,600 --> 01:03:57,560
consequences of this lack ofidentifiability you know cause your're basically modelling X

996
01:03:57,560 --> 01:04:02,000
as a product oftwo things that you don't know so youhave this you have some trading off

997
01:04:02,000 --> 01:04:07,860
I've already talked about the variances and otherssort ofgeometrical degree of indeterminancy is

998
01:04:07,860 --> 01:04:12,480
that if you have a rotation matrix in there than the model doesn't change if

999
01:04:12,480 --> 01:04:19,500
you post multiplyA by R and pre multiply lambda by our inverse ofcancels out

1000
01:04:19,600 --> 01:04:25,560
actually that's true for any nonsingular matrixA R but in particular if you use a

1001
01:04:25,560 --> 01:04:32,740
rotation matrix it doesn't eventhe prior structure you know the the rotation ofmultivariate gaussian

1002
01:04:32,740 --> 01:04:41,020
with sphericalit's still asphericalgaussian so the rotation isn't evenidentified inthe Bayesian

1003
01:04:41,170 --> 01:04:47,470
analysis where we have we have priorssothere's some care needed and

1004
01:04:47,480 --> 01:04:52,620
we we've adopted various strategies essentially do withrotating aback to a standard

1005
01:04:52,620 --> 01:05:02,210
orientation to to to assist thingshere asmallerdimensional class oftransformations are leaving

1006
01:05:02,210 --> 01:05:07,260
under variantparticular case of rotationiswhatI callwe calla sign permutation

1007
01:05:07,260 --> 01:05:15,340
matrix so simply swapping the labelsalso the factorsand possibly reversingtheir sign that

1008
01:05:15,340 --> 01:05:23,390
those were examples ofof anonsingularR reflectionsand rotationsthat you need to look at so

1009
01:05:23,410 --> 01:05:27,780
it havebeen various devices used to to combat thatand what we do is to monitor rotation

1010
01:05:27,940 --> 01:05:33,450
and essentiallywe we use linear algebratototo sort ofstandardize the rotation

1011
01:05:33,450 --> 01:05:42,140
to to a standardviewsensitivity analysiswhat wedid we did

1012
01:05:42,140 --> 01:05:47,320
a big sensitivity analysisand I'm now gonna showa little bit of a herein particular

1013
01:05:47,350 --> 01:05:52,200
we cared about various aspects of this model those here I'm gonnatalk about the

1014
01:05:52,200 --> 01:05:59,260
variancesbut we also looked at sensitivity to the mixture parameters sowe've got thatmixture

1015
01:05:59,260 --> 01:06:06,660
model for thefactor loadingso that was something we do hereso the

1016
01:06:06,660 --> 01:06:14,940
the nonhierarchical version of this would put the inverse variances for the factor loading whenit's not

1017
01:06:14,940 --> 01:06:20,950
zero the variable J and and thethatL have thissort of

1018
01:06:20,950 --> 01:06:26,920
structurenotice index only by the factors soagain we're borrowing strength across the

1019
01:06:26,920 --> 01:06:30,760
variables within factors

1020
01:06:30,780 --> 01:06:35,880
and you can takefixed values of alpha and betathere and you you you

1021
01:06:35,880 --> 01:06:42,700
you you get something but then we we found variousboth numerical

1022
01:06:42,700 --> 01:06:49,210
but more importantly inferentialproblems have beeninterpreting inferentia inferencethat led us to

1023
01:06:49,210 --> 01:06:54,500
to question thatand so we thought welooked ata we look at the

1024
01:06:54,500 --> 01:07:00,280
hierarchical prior where thebeta turned the scalepart of that gamma has a has agamma

1025
01:07:00,280 --> 01:07:05,720
distributionand thisjustgive you some idea of how things change so this is

1026
01:07:05,720 --> 01:07:16,620
this visualizesthe factor loadingsundertwo different prior settings in the

1027
01:07:16,620 --> 01:07:25,220
nonhierarchical priorthese these are the values offactor loadingand how it shows

1028
01:07:25,220 --> 01:07:29,060
how they changewhen yougo fromone priorspecification to the other and theseare

1029
01:07:29,060 --> 01:07:34,060
freedomof factorsandso really quite big changesI meand asa as agraphical

1030
01:07:34,060 --> 01:07:39,400
wayof demonstrating the the knock-on effecttwo levelsfor thedown inthe hierarchy of this

1031
01:07:39,400 --> 01:07:45,540
change in the hyper hyperparameterswhereasin thehierarchicalsetting is much lessmuch less

1032
01:07:45,540 --> 01:07:51,160
sensitive so this is commonly found if you if you put you know if you putthe

1033
01:07:51,160 --> 01:07:57,340
the certainity on further one levelfurther away from the datathen you get lesssensitivity

1034
01:07:57,340 --> 01:08:06,220
that's just a small anecdote if you like aboutsensitivity and answers thet the the now the big

1035
01:08:06,220 --> 01:08:11,620
issue andone that we haven't solved is the whole question of model choice

1036
01:08:11,620 --> 01:08:17,820
so if Iwas listeningto myselfanhour ago andIwould now put aprior on

1037
01:08:17,820 --> 01:08:24,660
thebasically on the sparsitya structural prior of sparsity which allowed me to choose between

1038
01:08:24,660 --> 01:08:28,800
all right so now you have some understanding of the kinds of problems are interested

1039
01:08:28,800 --> 01:08:32,870
in solving and you have some understanding of the algorithms that we might use if

1040
01:08:32,870 --> 01:08:36,490
we had to actually make the prediction once the model but now let's spend the

1041
01:08:36,490 --> 01:08:38,450
rest of the time talking about learning

1042
01:08:38,500 --> 01:08:42,290
and when these models are gonna come from

1043
01:08:42,310 --> 01:08:47,050
OK so we've seen a bunch of the

1044
01:08:47,060 --> 01:08:48,210
OK so

1045
01:08:48,220 --> 01:08:52,820
a little more notation i'm going to refer to use told us to remind you

1046
01:08:53,040 --> 01:08:58,260
things are training data and that's when the test and although i i realize what

1047
01:08:58,260 --> 01:09:01,470
i was going over my slides almost never talk about the testing given here just

1048
01:09:01,470 --> 01:09:06,800
assume you have some and remember that are white so the

1049
01:09:06,810 --> 01:09:08,950
they didn't come from god

1050
01:09:08,960 --> 01:09:11,270
they came from they came from some people

1051
01:09:11,280 --> 01:09:16,500
who shows conventions that are up for debate and then they they are they pay

1052
01:09:16,500 --> 01:09:20,840
people to to mark down those conventions on real data and those people who wrote

1053
01:09:20,840 --> 01:09:24,090
down the conventions may have been lazy or may not agree on may have many

1054
01:09:24,140 --> 01:09:28,780
valid disagreements and so on so none of our data are perfect never ever assume

1055
01:09:28,780 --> 01:09:31,270
that are perfect

1056
01:09:31,290 --> 01:09:35,970
OK i'm going to bring back the loss function is kind of important for learning

1057
01:09:36,910 --> 01:09:41,930
l is going to be a a way of measuring the cost if i make

1058
01:09:41,930 --> 01:09:46,090
a prediction y but the correct answer is why starting give three arguments just in

1059
01:09:46,090 --> 01:09:48,820
case it's important to look at what the input was so why

1060
01:09:48,890 --> 01:09:52,700
the first why there is that is what i guess the white stars the correct

1061
01:09:54,110 --> 01:09:58,630
you know most machine learning algorithms that go going trying trying to model for us

1062
01:09:58,630 --> 01:10:01,390
we're only talking about linear models today but when we want to go to train

1063
01:10:01,390 --> 01:10:04,270
some kind of model we want to minimize

1064
01:10:04,290 --> 01:10:06,130
the loss of the training sample

1065
01:10:06,140 --> 01:10:12,250
and maybe add some term penalizes highly complex models and so this is this is

1066
01:10:12,250 --> 01:10:15,630
sort of maybe this doesn't cover everything but it covers a lot of things

1067
01:10:15,670 --> 01:10:20,090
there's been an awful lot of effort in machine learning and trying to make a

1068
01:10:20,090 --> 01:10:24,090
loss function using training look like the evaluation function that we care about on the

1069
01:10:24,090 --> 01:10:28,390
testing that we expect to do best if we can actually optimize loss

1070
01:10:28,420 --> 01:10:31,550
in the way that we're really going to measure it

1071
01:10:31,590 --> 01:10:33,890
nlp makes this really hard

1072
01:10:33,900 --> 01:10:37,960
so the the first thing is that the loss functions are usually very vague in

1073
01:10:37,990 --> 01:10:41,540
ill-defined i want to translate better i want to write better summaries i want to

1074
01:10:41,540 --> 01:10:44,800
generate better text or i want to pass better about passing maybe is a little

1075
01:10:44,800 --> 01:10:48,540
better but i want i want to correctly represent the meaning of the sentence

1076
01:10:48,580 --> 01:10:53,170
people don't agree about loss functions for a long time for a period of about

1077
01:10:53,170 --> 01:10:57,580
five years there were several papers year just on evaluating machine translation systems it was

1078
01:10:57,580 --> 01:11:02,270
an active area of research often we care about more than one thing classic case

1079
01:11:02,270 --> 01:11:05,700
of this is precision and recall we want both the things we recovered to be

1080
01:11:05,700 --> 01:11:09,050
correct but also we want to recover but we want to recover all the correct

1081
01:11:09,050 --> 01:11:11,760
things and sometimes you can have one without the other

1082
01:11:13,520 --> 01:11:18,800
often measuring measuring the loss function we really care about is is expensive because it

1083
01:11:18,800 --> 01:11:23,330
requires us to embed our models in real system and go you know through the

1084
01:11:23,330 --> 01:11:26,860
system and in the world or pay people to pretend to use it to use

1085
01:11:26,860 --> 01:11:31,790
it and see how well you know how well things work and you know the

1086
01:11:31,790 --> 01:11:34,730
differences you get when you change your part of speech tagger in some kind of

1087
01:11:34,730 --> 01:11:38,020
dialogue system may be very very very small hard to measure

1088
01:11:38,100 --> 01:11:42,960
we're always fighting over these things they change frequently and often they require people as

1089
01:11:42,960 --> 01:11:48,080
i mentioned before so if your evaluation requires you to higher thirty undergraduates to come

1090
01:11:48,080 --> 01:11:50,050
in and use it a piece of software

1091
01:11:50,070 --> 01:11:54,430
you're going to think very carefully about when you do that evaluation

1092
01:11:54,450 --> 01:11:57,110
OK more often though

1093
01:11:57,320 --> 01:12:03,020
for these reasons people tend to focus on intrinsic intrinsic measures of the quality of

1094
01:12:03,020 --> 01:12:06,920
a particular NLP model so we're tagging you can count the number of words wrongly

1095
01:12:06,920 --> 01:12:12,710
tagged chunking you can use maybe one minus one four different run types are passing

1096
01:12:12,710 --> 01:12:17,020
we often use sort of precision recall measures we want to minimize the number of

1097
01:12:17,020 --> 01:12:21,500
incorrect phrases in a parse tree or incorrect attachments of words there's an ocean of

1098
01:12:21,500 --> 01:12:23,710
precision recall for reference as well

1099
01:12:24,020 --> 01:12:27,480
we really rather see how these affect things in real systems

1100
01:12:27,520 --> 01:12:32,420
but for the purposes of making progress on modules in natural language processing and because

1101
01:12:32,420 --> 01:12:37,300
that's expensive we often settle for these kinds of loss functions

1102
01:12:37,320 --> 01:12:39,990
OK so the first thing to talk about

1103
01:12:40,050 --> 01:12:44,320
for structured prediction is the use of generative models

1104
01:12:44,350 --> 01:12:45,740
this was sort of

1105
01:12:45,750 --> 01:12:49,590
until maybe two thousand one or so this is pretty much what most people were

1106
01:12:50,350 --> 01:12:53,860
so the idea is that you are going to define some kind of stochastic process

1107
01:12:54,280 --> 01:12:55,540
that produces

1108
01:12:55,550 --> 01:13:00,480
the the cross product is texas script with different probabilities and we're going to use

1109
01:13:00,480 --> 01:13:05,360
maximum likelihood or maximum a posteriori estimation to choose the parameters of that of that

1110
01:13:05,360 --> 01:13:11,380
model and usually in NLP this means a hidden markov model or probabilistic context free

1111
01:13:11,380 --> 01:13:14,920
grammar or something very much like that

1112
01:13:14,980 --> 01:13:16,910
so the challenge of course

1113
01:13:16,920 --> 01:13:21,000
is that if you have some ideas if you if you want to start with

1114
01:13:21,030 --> 01:13:25,350
feature based you that i for when i talked about decoding and go to design

1115
01:13:25,350 --> 01:13:26,730
a generative model

1116
01:13:26,810 --> 01:13:28,290
the producers the data

1117
01:13:28,300 --> 01:13:32,350
like this you would have a really hard time the rules if you want if

1118
01:13:32,350 --> 01:13:36,170
you want to well well-formed generative model for which maximum likelihood estimation is going to

1119
01:13:36,170 --> 01:13:39,970
be straightforward you really want to make sure that you predict each piece of structure

1120
01:13:40,180 --> 01:13:42,930
in your output y exactly once and no more

1121
01:13:43,170 --> 01:13:47,270
and you really want to make sure that you know you you i guess that's

1122
01:13:47,270 --> 01:13:49,990
sort of the key rules so i'll give an example of this in the second

1123
01:13:50,170 --> 01:13:53,370
if you do it that way then then you can use very simple estimators that

1124
01:13:53,370 --> 01:13:58,050
just involve counting and normalizing although there are extensions that allow you to deal with

1125
01:13:58,780 --> 01:14:03,130
general features most predominantly this is a generative models work so so here i'm going

1126
01:14:03,130 --> 01:14:06,710
to go through what happens when an HMM generates a sequence and show how each

1127
01:14:06,710 --> 01:14:10,410
piece of structure history once we start at the start state and then in the

1128
01:14:10,410 --> 01:14:15,150
next round we we've randomly chosen the first the first eight y one and then

1129
01:14:15,150 --> 01:14:18,620
that model of course it's easy to proves that the age of 3 days of

1130
01:14:18,620 --> 01:14:27,550
motor function and if which which of the nice the but of course again at

1131
01:14:27,550 --> 01:14:33,750
the computer educated explicitly because if you want to conclude that you to compute injured

1132
01:14:33,750 --> 01:14:37,920
on done well to integrate with respect 55 but what you do is that you

1133
01:14:37,920 --> 01:14:44,880
are you you will lose Mr. Cosic approximation type of Corazon which means that you

1134
01:14:44,880 --> 01:14:50,830
can look at exists in Centennial system it as a noisy observation of age

1135
01:14:50,840 --> 01:14:58,160
of the decade in which you suggest suggested that laws so that that's too good

1136
01:14:58,470 --> 01:15:03,570
them to justify promises about is we still what you do is is that you

1137
01:15:03,600 --> 01:15:08,070
that digitization I wouldn't you in the year

1138
01:15:08,070 --> 01:15:10,180
my metropolis testing and poisoned

1139
01:15:10,430 --> 01:15:18,360
it's also just starting from game I do proposal according to do

1140
01:15:18,400 --> 01:15:24,570
identical once Patrick's budget by Beijing decade next thereby will exit the reject according to

1141
01:15:24,570 --> 01:15:31,270
exist probability and then because I want to implement my us couple commission posted here

1142
01:15:31,270 --> 01:15:37,440
I will choose now the skating of my egoism into June discarding of categories and

1143
01:15:37,440 --> 01:15:41,470
I we're computer decade exists so

1144
01:15:41,470 --> 01:15:47,380
and which is compatible with the weather and so before so is positively converged on

1145
01:15:47,390 --> 01:15:50,440
the average to Zoo routes

1146
01:15:50,640 --> 01:15:56,620
because all the other in the limits if every single word loses quantity will converge

1147
01:15:56,620 --> 01:16:02,660
XKA would be decision or a gym a Hideki we also converge sources was quantity

1148
01:16:02,810 --> 01:16:10,250
things and it would be a kind of doses of this

1149
01:16:10,250 --> 01:16:15,770
average over the decision we expected except in the case of that's what they called

1150
01:16:16,330 --> 01:16:21,230
a control top without poisoning which included the type of idea what to do with

1151
01:16:21,230 --> 01:16:25,120
it which is says the idea of trying to use the past simulation in order

1152
01:16:25,120 --> 01:16:29,830
to be able to adopt your glories as it is of course the simplest you

1153
01:16:29,830 --> 01:16:36,970
can imagine I would try to convince you that is of argument can be extended

1154
01:16:36,980 --> 01:16:40,750
to more general fatigue to just 2

1155
01:16:40,910 --> 01:16:48,030
To CLT it works so here he said it's so that's a metropolis of Gorazde

1156
01:16:48,050 --> 01:16:55,230
use about him scaling solos a skating which has been computed from optimizing the few

1157
01:16:55,230 --> 01:17:00,360
times the previous models and for that mobile and here's this citizens in the talk

1158
01:17:00,360 --> 01:17:07,510
was of course somewhere you learn Aziz skating during his arrest and considers it so

1159
01:17:07,680 --> 01:17:13,840
it's it's a pretty it's a pretty it works pretty nicely given it dimensions of

1160
01:17:13,840 --> 01:17:20,380
isn't it when we image large will stay in a reasonable cost

1161
01:17:21,010 --> 01:17:29,860
looking time flights like arrow ask so now all you can extend this uh soars

1162
01:17:29,860 --> 01:17:31,590
as the

1163
01:17:31,590 --> 01:17:37,250
you have said that the is very this extremely important is not general all because

1164
01:17:37,250 --> 01:17:42,270
you would pretend that you want to see it for distribution is the product of

1165
01:17:42,270 --> 01:17:47,120
ethics Eisler affairs in a recession

1166
01:17:47,400 --> 01:17:52,160
it is the less you that you want to target distribution is is not something

1167
01:17:52,160 --> 01:17:58,230
that exists that something which can be read and exist it's just transforming reality X

1168
01:17:58,230 --> 01:18:04,010
according to the inverse of this magic signaled the end biology is is a product

1169
01:18:04,020 --> 01:18:11,100
of the bonds which appear to have that kind of Candida symmetry here is it's

1170
01:18:11,110 --> 01:18:13,970
not generals crucified him most of them

1171
01:18:15,050 --> 01:18:22,290
is more general than the previous models later when you can analyze this is realized

1172
01:18:22,290 --> 01:18:28,530
by the Republican are very recently by a student of business shouldn't of Jeff frozen

1173
01:18:28,530 --> 01:18:34,730
pork homeland Utah which may be reduced to this topic and very very nice dozens

1174
01:18:35,070 --> 01:18:40,490
and she proved that the acceptance rate is still viewed 0 adults to free for

1175
01:18:40,490 --> 01:18:48,700
which seems to be robust number but a share

1176
01:18:49,140 --> 01:18:55,040
the shares that the the achievable marginal speedy strongly affected by a signal so and

1177
01:18:55,080 --> 01:18:59,750
sure assumes that you propose according to Swedish you do distribution is is the target

1178
01:18:59,750 --> 01:19:07,070
distribution business and is now exists and cause according to move which are still according

1179
01:19:07,070 --> 01:19:14,100
to their identity Maddox than the westernmost loss improve loss which is linked with the

1180
01:19:14,100 --> 01:19:21,600
Commission on the world's this covert tricks but cancels its use rather obvious that opposes

1181
01:19:21,600 --> 01:19:26,920
calls for what I call in which dimensional scaling which was the cause of which

1182
01:19:26,920 --> 01:19:32,380
were originally proposed by Ottawa and tax man

1183
01:19:32,730 --> 01:19:34,550
and legal exists

1184
01:19:34,830 --> 01:19:39,050
but they analyzes of poisoned with totally different in a way which is the very

1185
01:19:39,050 --> 01:19:43,990
different which is whether they did not obtain the type of results

1186
01:19:44,090 --> 01:19:51,100
it and will exist as AD when you have them and insincere egoism with

1187
01:19:51,120 --> 01:19:57,100
coolers that again became the target be as this type of behavior are what you

1188
01:19:57,100 --> 01:20:05,120
do you should propose you should put according to an introvert distribution which is a

1189
01:20:05,120 --> 01:20:10,990
Z Z is a distributed demarcate with a simple violence and the tiger distribution admitted

1190
01:20:11,060 --> 01:20:12,900
like getting guns

1191
01:20:12,920 --> 01:20:16,620
and the skinny goes that should be so what you wear of called director-general is

1192
01:20:16,620 --> 01:20:18,540
not known so what we read

1193
01:20:18,540 --> 01:20:24,660
split off certain common endings of words so for example important might not to import

1194
01:20:24,670 --> 01:20:28,780
which actually seems wrong if we're trying to understand the sentence is important and important

1195
01:20:28,880 --> 01:20:34,720
two very different meanings even if england angle without e seem like the that might

1196
01:20:34,720 --> 01:20:35,740
be plausible

1197
01:20:35,760 --> 01:20:41,940
another thing people do is they lemmatize introduce words to their simplest forms

1198
01:20:41,950 --> 01:20:44,470
we had an example cats becomes cat

1199
01:20:44,490 --> 01:20:49,640
this is mostly solved in short perl scripts this isn't really a difficult problem people

1200
01:20:49,640 --> 01:20:53,960
don't write papers about these these things anymore but they are instances of structured prediction

1201
01:20:54,080 --> 01:20:58,150
it's just that we make the prediction using rules using very simple methods

1202
01:21:00,760 --> 01:21:06,640
but there are other ways of reducing the vocabulary and some of them are

1203
01:21:06,680 --> 01:21:08,930
are much more challenging so

1204
01:21:08,960 --> 01:21:12,640
the next thing to talk about this is the sort of extreme former tokenisation or

1205
01:21:12,710 --> 01:21:16,720
map we map words into word classes are part of speech you've probably seen this

1206
01:21:16,720 --> 01:21:21,140
problem before it's kind of a canonical natural language processing problem so here i've taken

1207
01:21:21,140 --> 01:21:26,260
each word in the sentence and assigned it blew part of speech determiners nouns

1208
01:21:27,890 --> 01:21:32,280
these are these are the parts of speech as as decided by you know somebody

1209
01:21:32,480 --> 01:21:36,530
i think is these go back you know two terrorists toddler something in the beginning

1210
01:21:36,810 --> 01:21:39,300
but you know you can come if you can kind of agree on the convention

1211
01:21:39,300 --> 01:21:42,710
and you may be able to decide on a way to map parts of speech

1212
01:21:42,710 --> 01:21:47,060
and this is this is done almost universally known NLP and you might think you

1213
01:21:48,300 --> 01:21:52,590
why is this what's challenging here the the problem of course is that many words

1214
01:21:52,590 --> 01:21:53,650
are ambiguous

1215
01:21:53,680 --> 01:21:55,330
and so

1216
01:21:55,350 --> 01:21:59,470
you could independently classify each word on its own based on

1217
01:21:59,480 --> 01:22:03,170
based on what you've seen before and you can get close to ninety percent accuracy

1218
01:22:03,170 --> 01:22:07,180
on this problem if you've got enough data in terms of the fraction of words

1219
01:22:07,180 --> 01:22:09,320
you correctly tagged

1220
01:22:09,560 --> 01:22:12,560
but context actually is really important so for example the word leaves can be a

1221
01:22:12,560 --> 01:22:17,020
plural noun or singular that is probably a verb is followed by a noun

1222
01:22:17,060 --> 01:22:19,410
similarly the word varies

1223
01:22:19,420 --> 01:22:22,980
communicating in honor but if it has a determiner in front of it you can

1224
01:22:22,980 --> 01:22:24,860
but it's probably now so

1225
01:22:24,880 --> 01:22:30,030
these the sort of local interactions among the words tend to allow us to do

1226
01:22:30,030 --> 01:22:33,010
a lot better if we can model and so the best performance on part of

1227
01:22:33,010 --> 01:22:36,720
speech tagging i think these days is around ninety seven percent and these are based

1228
01:22:36,720 --> 01:22:40,310
on structured models where we make joint decisions about all the words at the same

1229
01:22:43,090 --> 01:22:45,940
OK so this is this is sort of the simplest example

1230
01:22:45,940 --> 01:22:50,980
where the current NLP paradigm took off and continues to be the dominant way of

1231
01:22:50,980 --> 01:22:53,560
doing things so the general

1232
01:22:53,570 --> 01:22:57,240
framework here is that you get some people who understand part of speech tagging or

1233
01:22:57,240 --> 01:23:01,020
whatever you linguistic structure is you get them to label some text

1234
01:23:01,060 --> 01:23:03,560
you probably pay them a little bit of money to do that

1235
01:23:03,660 --> 01:23:07,340
and then you go perform supervised learning

1236
01:23:07,440 --> 01:23:11,700
and that could be some kind of structured prediction model i would advocate here but

1237
01:23:11,700 --> 01:23:14,840
you should you should be a little bit wary this these part of speech

1238
01:23:14,850 --> 01:23:18,070
you know if you find the dataset gives you parts of speech you should remember

1239
01:23:18,070 --> 01:23:19,630
that this didn't come from god

1240
01:23:19,650 --> 01:23:23,860
this came from somebody who had some insight about what parts of speech are useful

1241
01:23:23,860 --> 01:23:27,220
for that language and then he paid adviser to sit down and write down what

1242
01:23:27,220 --> 01:23:29,440
the part speech words in text

1243
01:23:29,450 --> 01:23:31,060
this is this is a choice

1244
01:23:31,060 --> 01:23:34,190
somebody somebody had some thoughts and you might you know if you do the language

1245
01:23:34,190 --> 01:23:37,380
you might disagree with them personally i disagree with some of the conventions that were

1246
01:23:37,380 --> 01:23:43,440
chosen in the current standard datasets for part of speech further even once you've laid

1247
01:23:43,440 --> 01:23:47,060
down what the conventions are people who are annotating don't always agree with each other

1248
01:23:47,070 --> 01:23:49,680
so annotation is actually a major

1249
01:23:49,720 --> 01:23:54,690
efforts that usually requires you know bring in lots of different people making sure they

1250
01:23:54,690 --> 01:23:59,530
agree going through various phases two to measure inter annotator agreement building up the case

1251
01:23:59,530 --> 01:24:01,300
that we've got is actually real

1252
01:24:01,900 --> 01:24:05,520
let's go back to work again

1253
01:24:06,130 --> 01:24:10,560
for a long time and focused mainly mainly on english but of course not everyone

1254
01:24:10,560 --> 01:24:15,040
speaks english and we'd like to do natural language processing for other languages so here's

1255
01:24:15,040 --> 01:24:16,410
a sentence in korean

1256
01:24:16,420 --> 01:24:17,920
and the

1257
01:24:17,940 --> 01:24:22,460
the graph at the bottom is is a little light far as network that shows

1258
01:24:22,460 --> 01:24:27,630
you the different possible analysis of the screen speak korean

1259
01:24:27,640 --> 01:24:30,680
no OK i'm safe

1260
01:24:30,690 --> 01:24:32,460
each of these is this

1261
01:24:32,470 --> 01:24:35,570
each of these words many of the words in the in that sequence can be

1262
01:24:35,570 --> 01:24:39,400
analyzed more than one way so i think if you multiply this out there something

1263
01:24:39,400 --> 01:24:43,810
like forty different analyses of the sentence depending on how you break the words of

1264
01:24:43,930 --> 01:24:45,140
how you tagged

1265
01:24:45,180 --> 01:24:48,040
so this is sort of showing you different ways of

1266
01:24:48,210 --> 01:24:52,600
segmenting in tagging the morphemes in this sense

1267
01:24:52,720 --> 01:24:58,760
OK so this may be a surprise if you only speak english but morphological disambiguation

1268
01:24:58,760 --> 01:25:04,180
is really kind of crucial for NLP in other languages so again this is just

1269
01:25:04,180 --> 01:25:08,860
the sequence the sequence mapping problem you get a sequence of surface words or characters

1270
01:25:08,860 --> 01:25:12,970
maybe and you want to you want to break them into morphemes and possibly tag

1271
01:25:12,970 --> 01:25:15,620
as well

1272
01:25:15,660 --> 01:25:19,730
this is one of my favourite examples

1273
01:25:19,740 --> 01:25:25,740
does anyone know what language this is

1274
01:25:25,790 --> 01:25:28,100
this is the word in turkish

1275
01:25:28,110 --> 01:25:31,010
it's very long and it means something like behaving as if you are among those

1276
01:25:31,010 --> 01:25:33,180
whom we could not civilise

1277
01:25:33,200 --> 01:25:34,950
it's a single word

1278
01:25:34,970 --> 01:25:38,750
and if you wanted to you know if you could say this is kind of

1279
01:25:38,750 --> 01:25:42,100
weird this is like some weird fringe effect but there are more than sixty million

1280
01:25:42,100 --> 01:25:46,230
people in the world to speak this language and we wanted to provide intelligent software

1281
01:25:46,230 --> 01:25:49,450
they could manage their language for them we would have to break that word into

1282
01:25:49,450 --> 01:25:52,780
its parts and figure out what the sum of the parts mean

1283
01:25:52,830 --> 01:25:56,960
what makes things even more nasty with turkish is often the valves change throughout the

1284
01:25:56,960 --> 01:26:00,100
sequence to match each other there's this thing called the harmony

1285
01:26:00,140 --> 01:26:04,520
so the problem gets even heard it's almost as if they've conspired to make their

1286
01:26:04,520 --> 01:26:06,640
language hard

1287
01:26:08,830 --> 01:26:12,840
so this going to move up a little bit from words

1288
01:26:12,850 --> 01:26:15,960
a lot of people have built their careers on unjust handling the words in different

1289
01:26:15,960 --> 01:26:19,110
languages but we're gonna we're going to keep going so

1290
01:26:19,120 --> 01:26:23,450
one thing that often useful is defined specific kinds of substrings syntax from the call

1291
01:26:23,450 --> 01:26:27,920
these chucks so for example if we go back to our cat cat sentence this

1292
01:26:27,920 --> 01:26:32,640
sentence has two noun phrases into prepositional phrases which are marked two based on the

1293
01:26:32,640 --> 01:26:37,160
positions and so we can we can use structured prediction two

1294
01:26:37,170 --> 01:26:44,110
to find these important segments are interesting segments chunks using what well

1295
01:26:44,160 --> 01:26:48,330
the representation is usually used to turn this into a sequence labeling problem we label

1296
01:26:48,330 --> 01:26:53,980
the beginning in interior and exterior points for each chong so this is just another

1297
01:26:53,980 --> 01:26:57,880
string transduction problem you have sense coming in and what you're going to send out

1298
01:26:57,880 --> 01:27:00,360
of these i be labels possibly

1299
01:27:00,570 --> 01:27:04,230
unlimited with chunks so this means that this is the beginning of the noun phrase

1300
01:27:04,260 --> 01:27:09,050
the interior and now we're starting a prepositional phrase the interior the interior and our

1301
01:27:09,050 --> 01:27:13,070
outside and so the the encoding is really not that important it's very widely used

1302
01:27:13,070 --> 01:27:14,240
it's worth seeing

1303
01:27:14,300 --> 01:27:18,380
so this is this kind of this technique is used for base noun phrase chunking

1304
01:27:18,380 --> 01:27:21,050
also for other kinds of shallow parsing for

1305
01:27:21,120 --> 01:27:25,020
other types of phrases and most commonly named entity recognition so i think that's what

1306
01:27:25,020 --> 01:27:28,300
i want to show to here's the sentence

1307
01:27:28,320 --> 01:27:30,710
from the news not too long ago

1308
01:27:30,730 --> 01:27:34,910
and there are the named entities

1309
01:27:34,920 --> 01:27:38,850
and we might label this entities the first one is the place the second one

1310
01:27:38,850 --> 01:27:41,930
is the person and the third one

1311
01:27:41,940 --> 01:27:46,890
what is robocop i guess robocop is a fantastical person i would call that miscellaneous

1312
01:27:46,890 --> 01:27:49,190
depends on your convention

1313
01:27:49,250 --> 01:27:52,180
maybe robocop is a person

1314
01:27:52,200 --> 01:27:56,490
OK so if you take this idea finding interesting substrings two

1315
01:27:56,500 --> 01:27:57,790
two it's extreme

1316
01:27:57,840 --> 01:28:01,370
and imagine that we want to find all the interesting substrings and we believe that

1317
01:28:01,370 --> 01:28:05,380
they have a hierarchical structure then you get the passing problem

1318
01:28:05,400 --> 01:28:09,700
and this is this is a problem that has dominated and continues to dominate LP

1319
01:28:09,700 --> 01:28:13,450
it's one of the main areas if you go to computational linguistics conference there will

1320
01:28:13,450 --> 01:28:17,720
be tons of sessions on passing and this is this is sort of passing in

1321
01:28:17,720 --> 01:28:22,420
what what what

1322
01:28:29,980 --> 01:28:32,700
thank you

1323
01:28:32,710 --> 01:28:34,890
i don't know

1324
01:28:51,510 --> 01:28:54,060
the hi

1325
01:28:59,270 --> 01:29:08,580
this is

1326
01:29:22,740 --> 01:29:27,850
and for the time being

1327
01:29:46,260 --> 01:29:50,270
or you know what

1328
01:29:54,400 --> 01:30:10,130
so four very well

1329
01:30:26,570 --> 01:30:32,750
people like know

1330
01:30:41,040 --> 01:30:42,820
well was

1331
01:30:53,940 --> 01:31:04,070
we also know that one

1332
01:31:28,320 --> 01:31:42,700
i mean

1333
01:32:09,980 --> 01:32:11,200
one five

1334
01:32:13,910 --> 01:32:16,700
we have to use

1335
01:32:24,260 --> 01:32:30,390
so i i

1336
01:32:30,390 --> 01:32:32,040
as for minus

1337
01:32:32,050 --> 01:32:33,710
ions which i noted

1338
01:32:33,760 --> 01:32:35,800
current carrier inside this

1339
01:32:36,750 --> 01:32:38,210
it's going from the right

1340
01:32:38,230 --> 01:32:40,500
going from the right to the left

1341
01:32:40,510 --> 01:32:43,450
so why would as o four miners ions

1342
01:32:43,480 --> 01:32:46,130
travel through an electric field

1343
01:32:46,150 --> 01:32:47,540
that opposes them

1344
01:32:47,550 --> 01:32:50,000
that opposes them motion

1345
01:32:50,010 --> 01:32:52,400
and they do that because in doing so

1346
01:32:52,420 --> 01:32:53,750
they engage

1347
01:32:53,760 --> 01:32:55,480
in a chemical reaction

1348
01:32:55,490 --> 01:32:58,020
which yields more energy than it cost

1349
01:32:58,040 --> 01:32:59,120
to climb

1350
01:32:59,140 --> 01:33:01,580
the electric hill

1351
01:33:01,680 --> 01:33:05,880
and while a current is flowing

1352
01:33:05,950 --> 01:33:09,930
while the s four minus is going from the right to left

1353
01:33:09,980 --> 01:33:14,180
you get fewer as o four minus ions here

1354
01:33:14,230 --> 01:33:16,540
this liquid he remains neutral

1355
01:33:16,550 --> 01:33:19,050
so complex must disappear

1356
01:33:19,100 --> 01:33:22,710
and it precipitates onto this copper bar so like

1357
01:33:22,790 --> 01:33:24,850
copper plating

1358
01:33:24,860 --> 01:33:29,180
on this side you get an increase of as so for minors

1359
01:33:29,190 --> 01:33:32,490
therefore we also must get an increase of zinc plus

1360
01:33:32,540 --> 01:33:34,500
because again this

1361
01:33:34,500 --> 01:33:36,400
link with there remains neutral

1362
01:33:36,420 --> 01:33:37,510
and that means

1363
01:33:37,520 --> 01:33:39,690
that some of the same

1364
01:33:39,760 --> 01:33:41,150
is being dissolved

1365
01:33:41,170 --> 01:33:42,790
so you get increase

1366
01:33:42,800 --> 01:33:44,170
in the concentration

1367
01:33:44,180 --> 01:33:45,310
of the

1368
01:33:46,880 --> 01:33:51,320
so the charge carriers inside these better DSO for miners ions

1369
01:33:51,360 --> 01:33:55,290
travel through very and they go from here to here

1370
01:33:55,310 --> 01:33:57,680
so to travel through the electric field

1371
01:33:57,750 --> 01:33:59,510
that opposes the motion

1372
01:33:59,520 --> 01:34:03,360
and this happens at the expense of chemical energy

1373
01:34:03,450 --> 01:34:06,760
now in the copper solution

1374
01:34:06,820 --> 01:34:10,190
becomes very dilute because all the copper has

1375
01:34:10,200 --> 01:34:12,110
been plated onto copper

1376
01:34:12,120 --> 01:34:17,210
and when this becomes concentrated this implies the the better we stop

1377
01:34:17,230 --> 01:34:20,300
and now what you can do you can run the current in the opposite direction

1378
01:34:20,310 --> 01:34:22,890
so you can run the current now in this direction

1379
01:34:22,920 --> 01:34:26,650
you can force the current to run without external power supply

1380
01:34:26,680 --> 01:34:30,050
and now chemical reactions will reverse

1381
01:34:30,100 --> 01:34:31,680
so now

1382
01:34:31,750 --> 01:34:33,880
comparable go back into the solution

1383
01:34:33,890 --> 01:34:35,050
so it will

1384
01:34:36,130 --> 01:34:37,740
and now the zinc

1385
01:34:37,750 --> 01:34:38,880
will be

1386
01:34:38,940 --> 01:34:40,950
precipitated onto using

1387
01:34:40,980 --> 01:34:43,900
so now if you do this long enough you can run the better you get

1388
01:34:43,900 --> 01:34:47,310
in the way this here

1389
01:34:47,360 --> 01:34:49,040
a car battery

1390
01:34:49,120 --> 01:34:52,080
is exactly this kind of better except that you have led

1391
01:34:52,130 --> 01:34:54,730
and that oxide instead of zinc and copper

1392
01:34:54,770 --> 01:34:56,500
but you also have sulfuric acid

1393
01:34:56,510 --> 01:34:58,480
like you have here

1394
01:34:59,610 --> 01:35:01,860
nickel cadmium batteries well known

1395
01:35:01,880 --> 01:35:03,670
you can charge that too

1396
01:35:03,700 --> 01:35:06,190
the ones that are readily available

1397
01:35:06,250 --> 01:35:08,900
in the stories you can run your flashlights

1398
01:35:08,920 --> 01:35:13,580
with these nickel cadmium batteries

1399
01:35:13,650 --> 01:35:14,690
this symbol

1400
01:35:14,700 --> 01:35:18,540
for better way that we will be using in our circuits

1401
01:35:18,550 --> 01:35:20,440
is this

1402
01:35:20,450 --> 01:35:22,200
this is the positive side

1403
01:35:22,250 --> 01:35:25,290
and this is the negative side

1404
01:35:25,330 --> 01:35:28,460
this is the symbol the symbolized that we're dealing with a

1405
01:35:28,480 --> 01:35:30,630
with the battery

1406
01:35:30,650 --> 01:35:33,520
but at this point b

1407
01:35:33,540 --> 01:35:36,400
and is going to be a

1408
01:35:36,420 --> 01:35:39,130
and here

1409
01:35:39,150 --> 01:35:43,810
we have a resistor r

1410
01:35:43,880 --> 01:35:46,620
so we have the current going

1411
01:35:46,670 --> 01:35:48,100
the current is going

1412
01:35:48,180 --> 01:35:50,380
in this direction

1413
01:35:50,560 --> 01:35:57,120
the current i

1414
01:35:57,140 --> 01:35:59,190
this could be libeled

1415
01:35:59,250 --> 01:36:00,440
could be laptop

1416
01:36:00,450 --> 01:36:01,750
the hairdryer

1417
01:36:03,140 --> 01:36:07,750
that you supply

1418
01:36:07,810 --> 01:36:09,610
if these are isn't there

1419
01:36:09,620 --> 01:36:14,230
that means that the resistance is infinitely large

1420
01:36:14,240 --> 01:36:19,940
that means that the current it is running is zero

1421
01:36:19,950 --> 01:36:23,140
then the voltage that we would measure over this

1422
01:36:23,190 --> 01:36:24,480
battery which is

1423
01:36:24,490 --> 01:36:27,270
minus eight

1424
01:36:28,130 --> 01:36:30,140
for which i will simply write down

1425
01:36:30,220 --> 01:36:32,350
view of the battery

1426
01:36:33,620 --> 01:36:34,800
we call

1427
01:36:34,830 --> 01:36:41,270
curled e which stands for EMF which is electromotive force

1428
01:36:41,290 --> 01:36:44,680
i will show you that later

1429
01:36:44,750 --> 01:36:47,180
if i put a resistance are in here

1430
01:36:47,210 --> 01:36:49,340
which is not infinitely large

1431
01:36:49,410 --> 01:36:52,120
then the current will start to flow

1432
01:36:53,410 --> 01:36:55,510
we should never forget

1433
01:36:55,570 --> 01:36:58,860
between the points a and b invisible to the

1434
01:36:58,900 --> 01:37:00,260
human i

1435
01:37:00,270 --> 01:37:06,120
is always an internal resistance which i call little are of high

1436
01:37:06,120 --> 01:37:08,290
and so if the current starts to flow

1437
01:37:08,300 --> 01:37:12,040
it goes not only through capitol but it also goes through these little are

1438
01:37:12,070 --> 01:37:14,520
so according to ohms law

1439
01:37:14,540 --> 01:37:15,790
the EMF

1440
01:37:15,970 --> 01:37:17,630
is now i

1441
01:37:18,650 --> 01:37:25,650
extra resistance plus the internal one

1442
01:37:25,660 --> 01:37:31,910
before the age that you would measure between point b and a

1443
01:37:31,970 --> 01:37:33,630
it's not going to change

1444
01:37:33,680 --> 01:37:34,970
that voltage

1445
01:37:34,990 --> 01:37:37,880
according to ohms law is i r

1446
01:37:37,880 --> 01:37:43,620
to b and then you would have caused

1447
01:37:43,630 --> 01:37:44,760
OK let's

1448
01:37:45,420 --> 01:37:48,950
keep going

1449
01:37:55,640 --> 01:37:59,230
OK so

1450
01:37:59,920 --> 01:38:02,910
is this good for another we have the definition we could figure out what we

1451
01:38:02,910 --> 01:38:04,750
can do with it

1452
01:38:07,240 --> 01:38:10,180
what are the implications of that

1453
01:38:10,210 --> 01:38:15,020
well will discover new applications of the product throughout the entire semester but let me

1454
01:38:15,020 --> 01:38:16,850
tell you at least about rules that

1455
01:38:16,870 --> 01:38:19,160
are readily visible

1456
01:38:19,180 --> 01:38:21,520
so one is

1457
01:38:21,530 --> 01:38:22,500
to compute

1458
01:38:25,580 --> 01:38:27,840
and angles

1459
01:38:33,050 --> 01:38:34,870
let's do an example

1460
01:38:34,970 --> 01:38:37,900
let's say that

1461
01:38:39,100 --> 01:38:41,510
i have in space

1462
01:38:41,700 --> 01:38:44,530
you have a point

1463
01:38:46,520 --> 01:38:50,200
which is that one is the all whole

1464
01:38:50,250 --> 01:38:51,700
i have a point

1465
01:38:52,600 --> 01:38:56,560
that is the all ones are also it's distance one here

1466
01:38:56,580 --> 01:38:57,580
one here

1467
01:38:57,590 --> 01:39:00,140
they have point

1468
01:39:00,160 --> 01:39:04,720
see also tools so its height two

1469
01:39:04,810 --> 01:39:08,450
and let's say that i'm curious and i'm wondering

1470
01:39:08,500 --> 01:39:10,130
what is the angle

1471
01:39:10,150 --> 01:39:13,100
so if you have a triangle in space

1472
01:39:13,150 --> 01:39:14,850
i thing pq one

1473
01:39:15,210 --> 01:39:17,860
i'm wondering what is this angle

1474
01:39:18,040 --> 01:39:23,380
so of course one solution is to build a model and then you go and

1475
01:39:23,380 --> 01:39:24,460
measure the angle

1476
01:39:24,580 --> 01:39:26,290
but we can do better than that

1477
01:39:26,290 --> 01:39:28,970
we can just find the angle using the product

1478
01:39:28,980 --> 01:39:31,310
so how would we do that

1479
01:39:35,480 --> 01:39:38,550
if we look at this formula

1480
01:39:38,560 --> 01:39:40,030
we see

1481
01:39:40,080 --> 01:39:41,580
so let's say

1482
01:39:41,600 --> 01:39:43,620
but we want to find here

1483
01:39:43,670 --> 01:39:45,980
well let's look at the formula for

1484
01:39:45,980 --> 01:39:47,320
pq that

1485
01:39:47,350 --> 01:39:49,720
p r

1486
01:39:49,820 --> 01:39:51,670
when we say it should be

1487
01:39:51,700 --> 01:39:53,050
length q

1488
01:39:53,060 --> 01:39:54,930
times length p r

1489
01:39:54,960 --> 01:39:57,540
and the cosine of the angle

1490
01:39:58,170 --> 01:40:01,830
now what do we know what do we not know well something at this point

1491
01:40:01,840 --> 01:40:02,840
we don't know

1492
01:40:02,850 --> 01:40:06,240
the cosine of the angle that's what we would like to find

1493
01:40:06,270 --> 01:40:07,560
but the length

1494
01:40:07,580 --> 01:40:11,080
so we can compute we know how to find these legs

1495
01:40:11,330 --> 01:40:16,380
it's not for that we know how to compute because we have an easy formula

1496
01:40:16,430 --> 01:40:21,420
OK so we can compute everything else and then find the so what we do

1497
01:40:21,550 --> 01:40:23,330
as we will find theta

1498
01:40:26,890 --> 01:40:30,350
in this way

1499
01:40:30,390 --> 01:40:34,290
we take the dot product of the q with p r and then we divide

1500
01:40:34,430 --> 01:40:38,150
by four lengths

1501
01:41:13,660 --> 01:41:17,460
OK so

1502
01:41:19,120 --> 01:41:25,820
so we say cosine theta is a q

1503
01:41:27,390 --> 01:41:30,230
of length p q

1504
01:41:30,240 --> 01:41:31,890
next we we are

1505
01:41:32,040 --> 01:41:37,450
so let's try to figure out what is the topic you want to go from

1506
01:41:37,480 --> 01:41:39,290
he two q i should go

1507
01:41:39,310 --> 01:41:41,730
minus one unit along the x direction

1508
01:41:41,750 --> 01:41:43,980
plus one unit and on the way the actions

1509
01:41:43,990 --> 01:41:46,560
and i'm not moving visitor action

1510
01:41:46,580 --> 01:41:49,360
so to go from p to q i have to move

1511
01:41:49,380 --> 01:41:50,910
by minus one

1512
01:41:53,590 --> 01:41:55,550
to go from p two are

1513
01:41:55,560 --> 01:41:59,820
i call minus one on the x axis and press two along the axis

1514
01:42:00,760 --> 01:42:03,490
p are claimed is this

1515
01:42:05,580 --> 01:42:08,080
then the length of these vectors

1516
01:42:10,770 --> 01:42:11,640
we take

1517
01:42:11,670 --> 01:42:17,610
minus one squared plus one square sail square square and the same thing with the

1518
01:42:21,090 --> 01:42:27,910
so the denominator will become one as well true and this is well defined

1519
01:42:29,260 --> 01:42:32,100
what about the new night

1520
01:42:33,110 --> 01:42:37,260
so remember to do the product we multiply this by this

1521
01:42:37,260 --> 01:42:40,580
that that that by that and we have

1522
01:42:40,600 --> 01:42:43,510
minus one minus one makes

1523
01:42:44,880 --> 01:42:48,100
plus one and say that's still

1524
01:42:48,110 --> 01:42:50,750
so all things to is zero again

1525
01:42:50,750 --> 01:42:52,170
so we will get

1526
01:42:52,250 --> 01:42:54,210
one of those quotes

1527
01:42:56,560 --> 01:42:58,360
that's the cosine of the angle

1528
01:42:58,390 --> 01:43:02,020
and of course if we want the actual angle

1529
01:43:02,020 --> 01:43:04,220
what we have to take calculator

1530
01:43:04,350 --> 01:43:06,110
find the inverse cosine

1531
01:43:06,120 --> 01:43:07,810
and you'll find that it's about

1532
01:43:07,850 --> 01:43:11,560
seventy one point five

1533
01:43:11,610 --> 01:43:14,570
actually will be using mostly gradients that for today

1534
01:43:14,620 --> 01:43:17,150
i but certainly are not speaking

1535
01:43:17,840 --> 01:43:23,300
OK any questions about that

1536
01:43:27,060 --> 01:43:30,480
OK so in particular i should point out one thing that really need to about

1537
01:43:30,480 --> 01:43:31,710
the answer

1538
01:43:31,800 --> 01:43:35,480
c and we got this number we don't really know what it means exactly because

1539
01:43:35,480 --> 01:43:38,490
it makes to give over lands and the angle

1540
01:43:38,510 --> 01:43:42,830
but one thing interesting here it's the sign of the answer

1541
01:43:42,850 --> 01:43:45,490
the fact that we got positive number

1542
01:43:45,500 --> 01:43:47,280
so if you think about it

1543
01:43:47,300 --> 01:43:49,460
the land always positive

1544
01:43:49,470 --> 01:43:53,810
so the sign of the dot product is the same as the sign of

1545
01:43:53,930 --> 01:43:56,410
because i

1546
01:43:57,590 --> 01:43:58,950
in fact

1547
01:43:59,030 --> 01:44:08,900
the sign of the

1548
01:44:08,920 --> 01:44:12,330
is going to be positive

1549
01:44:12,350 --> 01:44:16,630
the angle is less than ninety degrees so that means dramatically

1550
01:44:16,640 --> 01:44:21,120
two vectors are growing more or less the same direction to to make an acute

1551
01:44:22,560 --> 01:44:24,410
it's going to be zero

1552
01:44:26,050 --> 01:44:28,830
the angle is exactly ninety degrees

1553
01:44:31,790 --> 01:44:35,490
OK because that's when the cosine will is

1554
01:44:35,620 --> 01:44:40,310
be negative if the angle is more than ninety

1555
01:44:40,360 --> 01:44:41,840
so that means they go

1556
01:44:41,900 --> 01:44:44,220
in opposite directions

1557
01:44:44,230 --> 01:44:46,290
OK so

1558
01:44:48,530 --> 01:44:52,590
one way to think about what the product measure measures how much the two vectors

1559
01:44:52,620 --> 01:44:57,520
going along each other

1560
01:44:57,580 --> 01:45:00,150
and that actually leads us to the next application

1561
01:45:00,160 --> 01:45:04,780
so we have another one

1562
01:45:04,820 --> 01:45:07,880
yes so if they had no when they must have number two

1563
01:45:07,880 --> 01:45:11,150
because a lot of the minimum wage

1564
01:45:11,940 --> 01:45:15,960
secondly it is not so far

1565
01:45:15,980 --> 01:45:17,740
so you can as well

1566
01:45:17,760 --> 01:45:21,230
what does the gas was that this is

1567
01:45:21,290 --> 01:45:24,430
compute the value of more likely

1568
01:45:24,440 --> 01:45:25,800
it provides

1569
01:45:25,830 --> 01:45:33,230
about two-and-a-half operator which means so this is how this

1570
01:45:33,240 --> 01:45:36,480
to the right is

1571
01:45:37,320 --> 01:45:40,370
then continue on two

1572
01:45:40,380 --> 01:45:42,580
two applications to that

1573
01:45:43,660 --> 01:45:44,570
show you

1574
01:45:46,450 --> 01:45:49,190
example of rank two

1575
01:45:49,280 --> 01:45:59,440
the fresh we think that

1576
01:46:04,590 --> 01:46:09,760
but this is exactly what i like to watch what it depends on the properties

1577
01:46:09,760 --> 01:46:14,530
of this in this case actually generated in this small

1578
01:46:15,340 --> 01:46:19,940
o thing until is to do real data

1579
01:46:20,650 --> 01:46:22,490
it's really question i think

1580
01:46:22,520 --> 01:46:24,070
the second

1581
01:46:24,090 --> 01:46:28,890
the function look like a lot more

1582
01:46:28,890 --> 01:46:30,180
right now

1583
01:46:31,090 --> 01:46:34,970
this really look stationary monitor talk station

1584
01:46:34,980 --> 01:46:38,050
but you and i regret not

1585
01:46:38,070 --> 01:46:41,130
you can ask questions like that

1586
01:46:41,160 --> 01:46:42,670
you use machinery

1587
01:47:11,760 --> 01:47:19,040
and the trail to you actually have

1588
01:47:19,050 --> 01:47:22,280
make some observations observations we

1589
01:47:23,030 --> 01:47:24,240
this is one

1590
01:47:24,260 --> 01:47:25,710
in one

1591
01:47:25,740 --> 01:47:27,720
with the same thing

1592
01:47:27,740 --> 01:47:32,130
you under and you get something data which not so well explained by this

1593
01:47:32,130 --> 01:47:35,140
is very smooth

1594
01:47:36,090 --> 01:47:38,540
first lesson

1595
01:47:38,560 --> 01:47:40,000
then the case

1596
01:47:40,070 --> 01:47:42,790
more like the trade for the same

1597
01:47:42,810 --> 01:47:43,780
how much

1598
01:47:43,830 --> 01:47:50,770
OK to come back to this

1599
01:47:50,780 --> 01:47:54,490
come back

1600
01:47:55,180 --> 01:48:02,570
it's not just the two sides we have of how to use the process direction

1601
01:48:03,090 --> 01:48:04,970
because of this

1602
01:48:04,980 --> 01:48:10,320
so first of all we have that

1603
01:48:10,330 --> 01:48:12,260
one of the boldest

1604
01:48:16,220 --> 01:48:18,340
after the training points these are

1605
01:48:18,370 --> 01:48:19,690
on unknown

1606
01:48:19,710 --> 01:48:21,120
my prior

1607
01:48:21,140 --> 01:48:23,570
non-zero values for

1608
01:48:23,740 --> 01:48:27,250
all the

1609
01:48:28,060 --> 01:48:31,140
then was

1610
01:48:31,240 --> 01:48:35,730
so as soon as

1611
01:48:39,090 --> 01:48:42,320
by wise function

1612
01:48:47,470 --> 01:48:51,200
just like the fact that

1613
01:48:51,840 --> 01:48:54,180
probably any of these y

1614
01:48:54,190 --> 01:48:57,040
only depends on the car

1615
01:48:57,060 --> 01:48:59,270
what you want to

1616
01:49:00,400 --> 01:49:01,940
so particular input

1617
01:49:02,180 --> 01:49:04,730
unknown latent value once get

1618
01:49:04,740 --> 01:49:06,240
that tells you all you could possible

1619
01:49:07,830 --> 01:49:11,030
what happens on the basis that

1620
01:49:11,930 --> 01:49:13,450
so the

1621
01:49:13,460 --> 01:49:15,780
like that

1622
01:49:15,810 --> 01:49:18,710
and also along

1623
01:49:19,350 --> 01:49:23,650
i need to to turn the reason that want to make inferences

1624
01:49:26,620 --> 01:49:28,220
he also

1625
01:49:28,240 --> 01:49:30,480
the like

1626
01:49:32,620 --> 01:49:34,450
and how the situation is

1627
01:49:34,470 --> 01:49:37,770
this is house and this is

1628
01:49:37,800 --> 01:49:39,210
so this

1629
01:49:39,310 --> 01:49:43,430
is the fact that the world is here

1630
01:49:43,440 --> 01:49:47,510
this is the as you why

1631
01:49:47,540 --> 01:49:51,150
that's what the gas in form

1632
01:49:52,600 --> 01:49:55,140
the expression to utilize this

1633
01:49:55,150 --> 01:49:56,650
when distance between

1634
01:49:58,200 --> 01:50:00,620
we use the fact that like

1635
01:50:00,660 --> 01:50:02,680
also function

1636
01:50:05,590 --> 01:50:08,700
this becomes way for me

1637
01:50:08,710 --> 01:50:13,440
OK so what do we do

1638
01:50:13,450 --> 01:50:14,920
well we

1639
01:50:15,120 --> 01:50:16,620
make predictions

1640
01:50:19,520 --> 01:50:22,160
to make predictions about now say something about

1641
01:50:23,340 --> 01:50:26,930
the location called start

1642
01:50:26,940 --> 01:50:31,300
also for example x dy

1643
01:50:31,330 --> 01:50:32,230
and so

1644
01:50:32,240 --> 01:50:34,700
what i need to do that is to say well

1645
01:50:35,590 --> 01:50:36,470
one is the

1646
01:50:36,480 --> 01:50:40,240
is this is the sort of the identification

1647
01:50:40,250 --> 01:50:41,730
so the joint

1648
01:50:41,730 --> 01:50:44,850
star cascade

1649
01:50:44,880 --> 01:50:46,230
the same

1650
01:50:47,470 --> 01:50:50,600
just that for the model that

1651
01:50:50,600 --> 01:50:55,350
these are goals which are linked by some control data flow

1652
01:50:55,370 --> 01:51:00,450
so into to booker trip and in this example there is a request for flight

1653
01:51:00,540 --> 01:51:04,080
request for hotel and then you put the two

1654
01:51:04,880 --> 01:51:06,390
i request comes in

1655
01:51:06,400 --> 01:51:12,400
why are some goal based invocation and then either at design time or runtime these

1656
01:51:12,400 --> 01:51:14,240
go request matched

1657
01:51:14,260 --> 01:51:16,420
two semantic descriptions of web services

1658
01:51:16,440 --> 01:51:21,750
so the flight request is matched to a flight web service and hotel request and

1659
01:51:21,750 --> 01:51:28,640
hotel bookings at to hotel web service descriptions and these descriptions have choreography

1660
01:51:28,650 --> 01:51:33,510
and maybe grounded i don't have time to talk about grounding to some deployed web

1661
01:51:33,510 --> 01:51:35,160
services out on the

1662
01:51:35,180 --> 01:51:37,090
on the on the web

1663
01:51:37,100 --> 01:51:42,670
OK so that's three of the components of the last one is the

1664
01:51:42,860 --> 01:51:49,080
mediators so this is the top-level element in with no which doesn't have a corresponding

1665
01:51:49,480 --> 01:51:51,140
component analysis

1666
01:51:51,160 --> 01:51:53,870
but there is an interesting paper from

1667
01:51:53,870 --> 01:51:55,360
two thousand four

1668
01:51:55,380 --> 01:51:58,850
i think by paolo

1669
01:51:59,430 --> 01:52:03,830
comparing OWL s and the way was my hands handles mediation

1670
01:52:03,890 --> 01:52:08,460
so here are some of the motivation for why we have native

1671
01:52:08,480 --> 01:52:10,300
mediation is the top level

1672
01:52:10,330 --> 01:52:14,860
so this is statistics that for every dollar spent on programming five to nine dollars

1673
01:52:14,870 --> 01:52:16,530
spent on integration

1674
01:52:16,580 --> 01:52:19,170
so integrating components of big

1675
01:52:19,190 --> 01:52:22,800
a big slice of any application

1676
01:52:22,860 --> 01:52:28,880
creating tasks so we assume that there always necessary and we call this our safe

1677
01:52:28,880 --> 01:52:33,340
sex principle whenever you stick one component to any other component with may you always

1678
01:52:33,340 --> 01:52:37,490
put a mediator in the middle and the type of mediated depends on the type

1679
01:52:37,490 --> 01:52:40,410
of components that you're trying to put together

1680
01:52:40,610 --> 01:52:44,530
the other confusing aspect of the mediation in with me is that this is a

1681
01:52:44,530 --> 01:52:46,140
description of the role

1682
01:52:46,160 --> 01:52:51,260
so you can have the same deployed web service and in some applications it plays

1683
01:52:51,300 --> 01:52:54,820
the role of the standard web service you invoke in you get a result

1684
01:52:54,820 --> 01:52:58,450
in other applications it may play in mediating function

1685
01:52:58,450 --> 01:53:02,880
so it's not it's the role it plays in a particular context

1686
01:53:02,950 --> 01:53:07,950
so here's a pictorial representation to types of mediators in which in was not so

1687
01:53:07,970 --> 01:53:11,340
in with no we have ontologies web services and goals that we want to put

1688
01:53:11,340 --> 01:53:15,660
together putting two ontologies together and some sort of mismatch

1689
01:53:15,680 --> 01:53:17,720
then we have another mediator

1690
01:53:17,740 --> 01:53:23,880
which may point to some component designed by natasha

1691
01:53:23,910 --> 01:53:29,280
or she right if we put web services together and there is a mismatch between

1692
01:53:29,280 --> 01:53:32,760
say the inputs of one and the in the outputs of another then we have

1693
01:53:32,760 --> 01:53:36,720
ww mediator some and mismatch between the goal

1694
01:53:36,720 --> 01:53:43,260
of some client and what functionality web service we may have WG mediator in the

1695
01:53:44,030 --> 01:53:45,610
so this allows

1696
01:53:45,760 --> 01:53:47,010
and more

1697
01:53:47,050 --> 01:53:52,610
web services to be used against specific functionality

1698
01:53:52,630 --> 01:53:58,300
here's what the mediator looks like underneath we have a source component one or more

1699
01:53:58,300 --> 01:54:02,380
of these which want to be which need to be linked to the target component

1700
01:54:02,380 --> 01:54:04,950
and then the mediator points to some description

1701
01:54:05,010 --> 01:54:07,860
for mediators there may be a set of rules

1702
01:54:07,880 --> 01:54:11,880
for if this the functionality we may put up a web service in here web

1703
01:54:11,880 --> 01:54:14,570
service description or goals

1704
01:54:14,630 --> 01:54:17,990
and then run time will designed time the goal of the match to an appropriate

1705
01:54:17,990 --> 01:54:24,640
web services can actually carry out the mediation that requires

1706
01:54:24,720 --> 01:54:30,400
OK so let me quickly go through

1707
01:54:30,410 --> 01:54:31,800
an example

1708
01:54:31,820 --> 01:54:34,010
one application to be built in the

1709
01:54:34,130 --> 01:54:39,760
did a larger european project on semantic web services using it although we have at

1710
01:54:39,760 --> 01:54:43,010
the open university called the IRS three

1711
01:54:43,010 --> 01:54:46,490
so i had this is the real story

1712
01:54:46,510 --> 01:54:51,360
the BBC websites people don't believe me if i tell the story so in the

1713
01:54:51,360 --> 01:54:57,860
end of january two thousand three one centimetre snow fell in southern england and brought

1714
01:54:57,860 --> 01:54:59,680
all the traffic and trains

1715
01:54:59,680 --> 01:55:05,220
and airports to stand still complete chaos so here's the story of the twenty hour

1716
01:55:05,280 --> 01:55:11,970
battle my allow the driver to the motorways literally became a lot of people spend

1717
01:55:12,490 --> 01:55:14,200
at least one night

1718
01:55:15,360 --> 01:55:19,780
the application we created in conjunction with essex county council

1719
01:55:19,800 --> 01:55:26,910
it was to use semantic web services to support emergency situations related to drastic weather

1720
01:55:28,900 --> 01:55:34,880
so in terms of the scenario this is part of the essex county county which

1721
01:55:35,180 --> 01:55:39,240
we have on the left there is a major motorway

1722
01:55:39,260 --> 01:55:42,320
the m eleven which runs near stansted airport

1723
01:55:42,360 --> 01:55:44,030
and there was

1724
01:55:44,140 --> 01:55:45,910
this is where the heavy snow

1725
01:55:45,910 --> 01:55:46,800
i felt

1726
01:55:46,820 --> 01:55:52,590
and then there is the requirement that we want to link to meteorological web services

1727
01:55:52,610 --> 01:55:56,760
which will tell us about the forthcoming

1728
01:55:56,760 --> 01:56:00,490
weatherevent and i'm planning emergency response

1729
01:56:00,490 --> 01:56:07,950
we also want to get various details of the type of event coming

1730
01:56:08,160 --> 01:56:10,530
and identify rest centres

1731
01:56:10,530 --> 01:56:16,720
so if we have people that maybe you may have a coach of children he

1732
01:56:16,720 --> 01:56:18,660
may have elderly people

1733
01:56:19,200 --> 01:56:22,360
they told the story that they once had fifty dogs

1734
01:56:22,380 --> 01:56:25,780
that needed to rest centres of the type of dress entity choose depends on the

1735
01:56:25,780 --> 01:56:33,010
type of people that stuck anything different types of of people citizens will require different

1736
01:56:33,010 --> 01:56:35,510
types of rest centres

1737
01:56:35,530 --> 01:56:41,530
they also have something called the viewessex spatial data to this is a large databases

1738
01:56:41,530 --> 01:56:45,200
can containing all the elements such as hotels

1739
01:56:45,240 --> 01:56:50,720
shops schools that can be required

1740
01:56:50,720 --> 01:56:55,930
so here with the ontologies to be created the application was called emerges we had

1741
01:56:56,070 --> 01:56:57,820
was no ontology top

1742
01:56:57,840 --> 01:57:05,180
we had a archetypes ontology so lay persons view of geographical data

1743
01:57:05,780 --> 01:57:07,720
we also had

1744
01:57:07,910 --> 01:57:15,550
ontologies for meteorological data ontologies for the also covered another application to do with

1745
01:57:15,700 --> 01:57:20,090
what happens if a lorry with toxic gases

1746
01:57:20,140 --> 01:57:25,860
four i show you in in a small video in the moment ontologies for emergency

1747
01:57:25,860 --> 01:57:32,430
GIS and then one for instant messaging system called body space

1748
01:57:33,360 --> 01:57:39,070
just before i i show you a quick video of the screencast of application to

1749
01:57:39,070 --> 01:57:44,070
this is the generic structure applications that we've been creating over number years and it's

1750
01:57:44,070 --> 01:57:49,180
similar to the diagram that david showed earlier so that viewers when you build applications

1751
01:57:49,180 --> 01:57:53,300
using semantic web services you have these legacy systems

1752
01:57:53,320 --> 01:57:58,340
and we found in in the e government context which you're working in any government

1753
01:57:58,340 --> 01:57:59,760
at least in the UK

1754
01:57:59,780 --> 01:58:04,740
the problem is that you have many many different parties involved so you have these

1755
01:58:04,740 --> 01:58:07,380
four tiers of government some

1756
01:58:07,470 --> 01:58:12,930
agencies like the police force controlled by the national government some controlled locally you have

