1
00:00:00,000 --> 00:00:04,290
in the process of

2
00:00:06,520 --> 00:00:09,970
but not all

3
00:00:10,090 --> 00:00:14,720
this is a a

4
00:00:15,530 --> 00:00:20,730
they are problems on

5
00:00:20,750 --> 00:00:23,050
i don't know

6
00:00:23,190 --> 00:00:26,280
very good work on

7
00:00:26,290 --> 00:00:28,850
that's what

8
00:00:30,590 --> 00:00:34,610
one of

9
00:01:15,840 --> 00:01:17,000
it's not true

10
00:01:21,210 --> 00:01:24,670
one way

11
00:01:26,560 --> 00:01:30,640
right so

12
00:01:30,660 --> 00:01:33,090
do you want

13
00:01:34,720 --> 00:01:36,920
o five

14
00:02:07,120 --> 00:02:10,810
very small malls

15
00:02:16,170 --> 00:02:20,080
well for all right

16
00:02:20,180 --> 00:02:22,500
o or one

17
00:02:28,930 --> 00:02:33,620
world war two

18
00:02:37,070 --> 00:02:45,320
for example for example at small

19
00:02:46,890 --> 00:02:49,170
what is

20
00:02:49,180 --> 00:02:51,910
these are

21
00:03:07,350 --> 00:03:10,890
i know

22
00:03:21,420 --> 00:03:23,180
on the

23
00:03:25,450 --> 00:03:33,580
now the role

24
00:03:33,590 --> 00:03:37,440
all right

25
00:03:37,460 --> 00:03:40,260
all of the cell

26
00:03:40,260 --> 00:03:43,570
now we have

27
00:03:50,710 --> 00:03:54,850
the fact that a lot of work on

28
00:04:00,010 --> 00:04:05,050
one of

29
00:04:23,020 --> 00:04:24,320
i will

30
00:04:25,770 --> 00:04:27,720
my i

31
00:04:29,370 --> 00:04:31,450
here we go

32
00:05:02,940 --> 00:05:05,620
you know

33
00:05:13,820 --> 00:05:17,620
if you

34
00:05:17,920 --> 00:05:22,060
well written

35
00:05:25,310 --> 00:05:28,810
you he said his goal

36
00:05:28,820 --> 00:05:32,150
she is

37
00:05:37,570 --> 00:05:40,870
the problem

38
00:05:40,870 --> 00:05:42,650
by the

39
00:05:49,210 --> 00:05:53,060
the above

40
00:05:53,080 --> 00:05:54,910
right small

41
00:05:57,320 --> 00:06:05,230
you get what to you mean the mean of all right

42
00:06:10,220 --> 00:06:11,680
you have

43
00:06:13,720 --> 00:06:17,550
your your family

44
00:06:24,100 --> 00:06:25,960
right now

45
00:06:26,140 --> 00:06:30,340
let's call our

46
00:06:34,810 --> 00:06:39,450
well we have that we able get a one

47
00:06:43,270 --> 00:06:46,200
you've got a

48
00:06:46,200 --> 00:06:48,750
much much larger h models

49
00:06:48,760 --> 00:06:56,240
so that they can actually generate output words that are not in your face

50
00:06:56,290 --> 00:06:58,990
OK decoding

51
00:06:59,050 --> 00:07:03,900
well you know this is the same thing in germany and how they will do

52
00:07:03,900 --> 00:07:07,250
we translate you know each for time

53
00:07:07,310 --> 00:07:11,040
but the problem is we have all these different choices and there actually no real

54
00:07:11,040 --> 00:07:14,330
extract only the top four phrases four kinds of input phrase and you can see

55
00:07:14,340 --> 00:07:17,470
the stuff in much of history

56
00:07:17,680 --> 00:07:21,590
and the task of decoding as you figure out which are the phrase i'm going

57
00:07:21,590 --> 00:07:24,240
to use the image automatically using it

58
00:07:24,290 --> 00:07:30,850
and that's this search probably started beginning with the policies all these fine extensions and

59
00:07:30,910 --> 00:07:35,820
themes explored pretty badly but somewhere there is the best path due to pruning

60
00:07:37,950 --> 00:07:42,010
so what changes now when we do affect more according

61
00:07:42,250 --> 00:07:46,850
so we do additional complexity but also previously we had a nice phrase mapping simple

62
00:07:46,850 --> 00:07:48,790
phrase self-professed now we have

63
00:07:48,800 --> 00:07:50,160
you know all these

64
00:07:50,180 --> 00:07:52,140
mapping steps in there

65
00:07:52,140 --> 00:07:57,540
so here in this case the like three that's where the map limit on the

66
00:07:57,540 --> 00:08:04,090
amount of money that morphology morphology and generations that chris surface form

67
00:08:04,990 --> 00:08:08,590
the interesting thing is that can do

68
00:08:08,590 --> 00:08:12,440
all this beforehand before you do they actually coding you can

69
00:08:12,520 --> 00:08:14,580
figure out for each input phrase

70
00:08:14,590 --> 00:08:15,960
one of the possible

71
00:08:15,980 --> 00:08:18,290
rendering so

72
00:08:18,380 --> 00:08:21,950
the big mapping of input phrase supper phrase you don't have to do that during

73
00:08:21,950 --> 00:08:25,860
that according to do that before he started according to you know of the source

74
00:08:26,890 --> 00:08:28,420
and then you can figure out

75
00:08:28,450 --> 00:08:30,350
one of the all the possible

76
00:08:35,400 --> 00:08:38,470
so all these things can be precomputed

77
00:08:38,510 --> 00:08:40,520
that means

78
00:08:40,590 --> 00:08:45,040
when you actually go around the decoding we only have same decoding algorithm

79
00:08:45,050 --> 00:08:46,510
but for each

80
00:08:46,570 --> 00:08:47,840
of the

81
00:08:48,210 --> 00:08:51,720
input words you know how much richer

82
00:08:51,750 --> 00:08:55,050
annotation feature the output see also much

83
00:08:55,140 --> 00:08:59,570
attention otherwise the whole thing is exactly the same

84
00:08:59,910 --> 00:09:04,960
of course there are

85
00:09:04,980 --> 00:09:11,540
many more translation options this way because you all these different steps in the combine

86
00:09:11,540 --> 00:09:15,040
are much much more ways and you have seen the training data so you have

87
00:09:15,040 --> 00:09:16,860
to cut down at some point

88
00:09:18,040 --> 00:09:20,330
there's also an issue with

89
00:09:20,360 --> 00:09:22,800
especially of current implementation

90
00:09:22,810 --> 00:09:25,250
when you go through these mapping steps so

91
00:09:25,270 --> 00:09:28,870
consider first generated word

92
00:09:29,260 --> 00:09:31,800
the lemma we have made twenty choices that

93
00:09:31,840 --> 00:09:34,120
then j then you

94
00:09:35,510 --> 00:09:36,410
in two

95
00:09:36,420 --> 00:09:40,280
part of speech tagging if i try to say something year five part of different

96
00:09:40,280 --> 00:09:42,980
part of speech tags in twenty different

97
00:09:43,000 --> 00:09:49,680
phrases twenty times five hundred different possibilities of this can actually look badly

98
00:09:49,730 --> 00:09:53,440
we can have a solution that works

99
00:09:54,150 --> 00:09:59,160
pretty rough if you don't do too many mapping steps the generation model

100
00:09:59,370 --> 00:10:04,100
but is actually something to pull off but if you make it more complicated

101
00:10:04,100 --> 00:10:07,060
need some work

102
00:10:07,290 --> 00:10:12,580
but according to otherwise so that phrase expansion of this

103
00:10:13,100 --> 00:10:15,480
the execution of all these mappings that doesn't get

104
00:10:15,500 --> 00:10:21,400
too expensive the recording as the same as the first the second so much

105
00:10:21,400 --> 00:10:23,000
OK so what can we do with

106
00:10:23,010 --> 00:10:26,900
OK this was the first thing you always work

107
00:10:26,980 --> 00:10:30,570
the motivation is if you have only produced the output

108
00:10:30,570 --> 00:10:34,080
but also part of speech tags maybe you can have some kind of model the

109
00:10:34,090 --> 00:10:39,600
checksum grammaticality output so you can build seven gram model nine model as any arch

110
00:10:39,610 --> 00:10:42,470
large and model of part of speech tags

111
00:10:42,620 --> 00:10:46,420
so the same as the tree structure but it does give you some kind of

112
00:10:46,420 --> 00:10:49,860
you know grammatical here

113
00:10:50,130 --> 00:10:55,030
so how does it work well you want to stations that maps input ports of

114
00:10:56,180 --> 00:10:58,790
and then you generation step

115
00:10:58,790 --> 00:11:02,880
using the graph structure now right i'm using the graph breaks the objective into nodes

116
00:11:02,880 --> 00:11:06,730
and edges

117
00:11:06,880 --> 00:11:10,810
OK so one last step now now we can think about this expectation

118
00:11:10,820 --> 00:11:13,690
if i write this out long longhand what you could think is this is sort

119
00:11:13,690 --> 00:11:19,730
of like taking an expectation over this is the marginal distribution of q

120
00:11:19,750 --> 00:11:23,540
this induces a marginal

121
00:11:25,150 --> 00:11:26,190
x of s

122
00:11:27,610 --> 00:11:31,310
csco up a little bit

123
00:11:31,420 --> 00:11:33,980
so this is like

124
00:11:34,000 --> 00:11:36,590
a sum over that marginal

125
00:11:36,610 --> 00:11:42,940
and this is like a sum over the joint marginal

126
00:11:43,040 --> 00:11:51,150
it the joint mu st is the joint marginal over the pair of variables

127
00:11:56,590 --> 00:12:00,840
i'm very dirty and what's happening

128
00:12:00,840 --> 00:12:07,930
sorry about that affect legibility OK so now this is an interesting form because you

129
00:12:07,930 --> 00:12:11,320
know in general this is an exponentially large vector this q

130
00:12:11,380 --> 00:12:13,820
but what i've shown you now is it

131
00:12:13,840 --> 00:12:17,860
in terms of how it interacts with the cost function because the cost function splits

132
00:12:17,860 --> 00:12:20,020
it's graph structure cost function

133
00:12:20,070 --> 00:12:22,980
it only depends on a vector

134
00:12:23,210 --> 00:12:27,820
very low dimensional right that's a vector of length two in my binary case and

135
00:12:27,820 --> 00:12:31,360
this thing here is the matrix a two by two matrix

136
00:12:31,400 --> 00:12:36,110
right so somehow now all of a sudden have gone from exponentially large two

137
00:12:36,130 --> 00:12:37,440
very small

138
00:12:37,440 --> 00:12:41,880
right this thing is the worst if you had not binary valued and states you

139
00:12:41,880 --> 00:12:45,110
now have at most m squared numbers per edge

140
00:12:45,170 --> 00:12:49,690
and yet and roughly edges c of m squared times an edges if you're on

141
00:12:49,690 --> 00:12:50,440
the tree

142
00:12:50,460 --> 00:12:53,090
and here you have numbers per node

143
00:12:53,190 --> 00:12:58,960
so this is very low complexity and this is the representation that belief propagation max

144
00:12:58,960 --> 00:13:03,020
product is exploiting it six it's working in the space of these museums

145
00:13:09,040 --> 00:13:10,110
OK so

146
00:13:10,180 --> 00:13:13,670
i'm telling you works in the space of muse and i told you that the

147
00:13:13,670 --> 00:13:18,110
messages and belief propagation or enforcing constraints

148
00:13:18,130 --> 00:13:21,840
so we have to figure out what are the constraints that the messages are enforcing

149
00:13:21,840 --> 00:13:28,270
right so what are the conditions that these muse have to satisfy

150
00:13:28,270 --> 00:13:30,690
right so this is a vector

151
00:13:30,730 --> 00:13:32,250
and this is the matrix

152
00:13:32,420 --> 00:13:39,960
what are some conditions that those guys have to satisfy

153
00:13:40,040 --> 00:13:46,920
right so he said the sum of all entries in this vector this marginal though

154
00:13:46,920 --> 00:13:48,480
should be one

155
00:13:48,540 --> 00:13:53,050
and of course it should have non negative entries probabilities are non negative between zero

156
00:13:53,050 --> 00:13:56,460
and one so non negative plus sums to one that will do the trick

157
00:13:56,460 --> 00:14:01,040
the same thing for this guy right this guy if double some him over everything

158
00:14:01,040 --> 00:14:02,290
he should be one

159
00:14:02,340 --> 00:14:08,440
right somebody said column sums in row sums

160
00:14:08,480 --> 00:14:12,920
that's the interesting constraint if we collapse the sky over x of t

161
00:14:13,090 --> 00:14:16,670
what should i get if i some over x fifty this guy

162
00:14:19,020 --> 00:14:22,750
if i collapse the joint marginal then i should get back this marginal

163
00:14:22,790 --> 00:14:26,150
and those should be the same thing because i told you that they came from

164
00:14:26,150 --> 00:14:27,320
the same q

165
00:14:27,340 --> 00:14:31,190
right so if we took fixed q and i computed mu s computed new st

166
00:14:31,190 --> 00:14:35,210
they must be consistent they have to be locally consistent in that way

167
00:14:35,210 --> 00:14:40,170
right so that that local consistency constraints that's exactly the constraint that the message is

168
00:14:40,170 --> 00:14:41,820
trying to enforce

169
00:14:41,860 --> 00:14:45,710
the message that's passed along edge s t there's two of them

170
00:14:45,710 --> 00:14:49,990
and there's two constraints one depending whether you some this out one depending whether you

171
00:14:49,990 --> 00:14:54,540
some that out those messages are exactly for those constraints

172
00:14:54,540 --> 00:14:59,820
OK so now i can show you the the slide

173
00:15:03,670 --> 00:15:05,070
OK so

174
00:15:05,070 --> 00:15:08,920
what i've done years i've walked you through this derivation

175
00:15:08,980 --> 00:15:11,440
i said that

176
00:15:11,530 --> 00:15:15,270
instead of thinking about max product is in

177
00:15:16,400 --> 00:15:20,130
you can think about is a different kind of optimisation problem actually what's called a

178
00:15:20,130 --> 00:15:21,610
linear programme

179
00:15:21,630 --> 00:15:26,480
linear program this means all the constraints are linear in the objective function is linear

180
00:15:26,550 --> 00:15:31,190
linear programs are nice there's sort of the easiest class of convex programs probably even

181
00:15:31,270 --> 00:15:34,520
with mentioned them i think maybe spoke about conic programs

182
00:15:34,540 --> 00:15:37,290
more general but anyway linear programs are

183
00:15:37,320 --> 00:15:39,190
generally very easy

184
00:15:39,190 --> 00:15:45,570
and what we've seen a very good at this place things

185
00:15:45,630 --> 00:15:48,480
where do you go the humans

186
00:15:48,500 --> 00:15:55,520
what we see is that some of these marginal distributions should be relevant here

187
00:15:55,570 --> 00:15:59,040
showing you how they look if you had three states not just two

188
00:16:00,270 --> 00:16:05,540
we've just answer this question what are the constraints that the marginals have to satisfy

189
00:16:05,590 --> 00:16:08,320
they have to satisfy the if you collapse the

190
00:16:08,340 --> 00:16:11,420
across the columns you should get back this factor

191
00:16:11,440 --> 00:16:14,440
and if you collapse the rows you should get back the other vector the mu

192
00:16:14,440 --> 00:16:18,020
t factor

193
00:16:20,920 --> 00:16:26,570
OK so if we think about what we're doing

194
00:16:26,610 --> 00:16:30,480
a bit more generally so what's going on as we went to the space of

195
00:16:30,480 --> 00:16:32,210
probability distributions

196
00:16:32,270 --> 00:16:35,790
and then we said it's enough to sort of thing set of probability distributions were

197
00:16:35,790 --> 00:16:41,750
what will work over marginal distributions over node marginals and over joint marginals are matrices

198
00:16:41,750 --> 00:16:43,340
on edges

199
00:16:43,500 --> 00:16:49,190
and so what we're actually optimizing is over the set is the set of all

200
00:16:49,190 --> 00:16:53,230
marginals that you could obtain if if you let the choice of here i'm going

201
00:16:53,230 --> 00:16:57,040
to p of mu here i was calling it q if you let that choice

202
00:16:57,040 --> 00:17:01,860
of the global distribution very you're gonna get different choices of local marginals

203
00:17:01,860 --> 00:17:03,750
and what you'd like to understand this

204
00:17:03,750 --> 00:17:05,080
e eight is circular

205
00:17:05,670 --> 00:17:07,420
what happens if you rotate a circle

206
00:17:09,900 --> 00:17:11,020
yeah nice isn't it

207
00:17:11,540 --> 00:17:12,960
what happens if you rotate

208
00:17:13,730 --> 00:17:14,270
and overall

209
00:17:16,480 --> 00:17:19,120
it develops into a full covariance calcium yeah

210
00:17:20,920 --> 00:17:22,750
the fact that the problem exists

211
00:17:23,690 --> 00:17:26,750
this trick being able to rotate the circle and it's still you come

212
00:17:27,480 --> 00:17:31,210
tell you don't change the nature of the spherical noise if you rotate it that's

213
00:17:31,230 --> 00:17:35,670
why you can do this nice simple i can value decomposition

214
00:17:36,190 --> 00:17:37,420
on the uh

215
00:17:38,150 --> 00:17:40,500
principal policies you can't do it for factor analysis

216
00:17:42,350 --> 00:17:42,790
okay good

217
00:17:48,600 --> 00:17:53,190
so this leads to this i can value decomposition actually while we're at it since i've got me

218
00:17:54,870 --> 00:17:56,580
want to interpret this that's

219
00:17:57,330 --> 00:17:59,310
well what's going on is you've got

220
00:18:01,140 --> 00:18:02,690
you're covariance if you data

221
00:18:06,060 --> 00:18:07,440
what you're trying to find is that

222
00:18:07,850 --> 00:18:09,960
principal directions you're trying to find a china

223
00:18:11,640 --> 00:18:14,830
is the direction that will allow you to axisaligned this dataset

224
00:18:15,620 --> 00:18:16,900
so this is like you one

225
00:18:17,790 --> 00:18:18,790
and this is likely due to

226
00:18:20,480 --> 00:18:22,000
so that's what you're looking forward said

227
00:18:22,420 --> 00:18:26,690
the covariance the sample covariance for your data is given by this circle here

228
00:18:28,400 --> 00:18:32,150
and then you do and i can decomposition allow which gives you the principal axes

229
00:18:33,670 --> 00:18:39,600
practice comes from principal component analysis of the covariance now the rotational invariance is very

230
00:18:39,600 --> 00:18:42,960
interesting because the rotational invariance we had before remember was u

231
00:18:43,370 --> 00:18:44,190
al are

232
00:18:44,480 --> 00:18:45,980
transpose are el

233
00:18:46,560 --> 00:18:48,460
u transpose regan

234
00:18:50,650 --> 00:18:54,840
these things converter w actions so that's u because it's actually w one because it's

235
00:18:54,840 --> 00:18:58,810
multiplied by igon value to give its land use the direction like that's

236
00:18:58,980 --> 00:19:00,620
and the length is given by the eigen value

237
00:19:01,870 --> 00:19:02,750
this rotation guy

238
00:19:03,170 --> 00:19:04,480
what he does is things like this

239
00:19:06,270 --> 00:19:08,150
all these are valid solutions for w

240
00:19:09,350 --> 00:19:14,620
yeah so these two russian those different colours action so uh which either uh these two

241
00:19:15,100 --> 00:19:16,810
would be one says solutions

242
00:19:17,210 --> 00:19:17,830
and these two

243
00:19:18,420 --> 00:19:19,810
would be another set solutions

244
00:19:20,520 --> 00:19:21,770
the rotated versions

245
00:19:22,330 --> 00:19:26,290
although orthogonal set so we tend to use your orthogonal also known to be orthogonal

246
00:19:26,290 --> 00:19:29,210
because in that i can value decomposition that's incentive

247
00:19:29,900 --> 00:19:30,460
what i think

248
00:19:31,150 --> 00:19:32,250
as orthonormal vectors

249
00:19:33,270 --> 00:19:37,920
and that's a set of scales yeah so these these guys must be right angles to each other

250
00:19:42,640 --> 00:19:47,460
so that's giving somebody intuitions i have all i think of myself about

251
00:19:48,170 --> 00:19:51,580
principal component analysis but just a finnish what's going on in the solution

252
00:19:54,370 --> 00:19:56,420
u i can vectors of

253
00:19:57,000 --> 00:19:57,690
the covariance

254
00:19:59,000 --> 00:20:02,690
sigma squared plus elsewhere the igon values now in traditional

255
00:20:03,100 --> 00:20:04,520
principal component analysis

256
00:20:05,750 --> 00:20:09,730
you just beat el squared sigma squared is taken zero that's the noise

257
00:20:10,810 --> 00:20:13,900
that's in hotelling's work the noise was considered a zero

258
00:20:15,920 --> 00:20:20,730
but inference probabilistic principal component analysis you had this sigma squared onto the i can

259
00:20:20,730 --> 00:20:24,080
values now what's that's saying is i means that is

260
00:20:24,770 --> 00:20:27,310
you're constantly adding to your forearm

261
00:20:27,770 --> 00:20:28,370
your data

262
00:20:30,440 --> 00:20:32,080
so yes merging it so

263
00:20:34,120 --> 00:20:37,750
if we were doing one dimensional principal component analysis like this

264
00:20:39,100 --> 00:20:41,290
okay so we're looking for the first principal direction

265
00:20:42,210 --> 00:20:46,460
and then when you add noise to it what you actually do is you at the second component to it

266
00:20:47,330 --> 00:20:50,640
a little bit of noise in the direction and you add a little bit on the end as well

267
00:20:51,710 --> 00:20:57,190
so the i can values you get principal component analysis probabilistic principal component analysis are

268
00:20:57,210 --> 00:21:00,710
slightly different from the ones you're getting if you get peace you because you had

269
00:21:00,710 --> 00:21:04,120
this little bit of noise in the end but you will so it means is

270
00:21:04,140 --> 00:21:08,250
a proper probabilistic set because if you just take the vector

271
00:21:08,690 --> 00:21:13,900
w w transpose the one-dimensional vector that's not a valid covariance it just defines

272
00:21:14,460 --> 00:21:14,900
what is this

273
00:21:15,290 --> 00:21:17,520
semi definite covariance it defines a line

274
00:21:18,270 --> 00:21:19,750
so adding a little bit of noise

275
00:21:20,440 --> 00:21:21,250
across the diagonal

276
00:21:22,230 --> 00:21:23,900
that's a little bit a little bit there

277
00:21:24,670 --> 00:21:26,830
so you end up having subtract out of

278
00:21:28,190 --> 00:21:33,100
in your forearm solution for peace that comes out of the max but that's my

279
00:21:33,100 --> 00:21:36,190
intuition the mass i think you can see in the papers i try to give

280
00:21:36,190 --> 00:21:39,310
you information because that's what it took me longer to work out

281
00:21:40,080 --> 00:21:44,190
so far the manipulation shows that we can constrain this then the solution is given

282
00:21:44,190 --> 00:21:47,310
by the largest eigen values what you see here is that we

283
00:21:47,920 --> 00:21:49,620
i can values you need to retain

284
00:21:50,650 --> 00:21:53,580
if you're reducing the dimension a view of the largest ones but that's

285
00:21:54,190 --> 00:21:55,170
you can show them as well

286
00:21:55,560 --> 00:21:57,190
so the letter q largest eigen values

287
00:22:01,000 --> 00:22:04,440
so some further work shows the principal eigen vectors and then it turns out you

288
00:22:04,440 --> 00:22:07,620
can also work out this this requires quite a lot of manipulation

289
00:22:08,060 --> 00:22:13,170
the maximum likelihood value for sigma squared is given by the average of the discarded igon values

290
00:22:14,060 --> 00:22:17,020
okay so if you want to know what sigma squared is you have to take

291
00:22:17,020 --> 00:22:21,370
the average of the discarded i can values and that's sigma squared so this is

292
00:22:21,370 --> 00:22:25,770
some intuition people have always had about peace yea that somehow they look for these

293
00:22:26,980 --> 00:22:29,100
people talk about all those finding dimensionality

294
00:22:29,750 --> 00:22:32,850
these are amazing thing that don't actually exist in real data

295
00:22:33,310 --> 00:22:37,960
the idea is if you do piece e eight you look at the igon values along this axis here

296
00:22:40,170 --> 00:22:40,920
and you see some so

297
00:22:41,710 --> 00:22:46,870
elbow like that's the i values go down and this is the album the mythical elbow

298
00:22:48,790 --> 00:22:52,500
and the idea is that this is the noise level and then you retain only those components

299
00:22:53,020 --> 00:22:56,640
case appreciate making this more explicit it's if few really took

300
00:22:59,000 --> 00:23:02,620
i can decomposition about matrix form w w transpose

301
00:23:03,330 --> 00:23:07,170
plus sigma squared i u really would see this this will be sigma squared

302
00:23:08,670 --> 00:23:12,350
yeah there would be a constant igon value and this will be the dimensionality q

303
00:23:12,880 --> 00:23:13,980
all these matrices here

304
00:23:14,560 --> 00:23:15,000
yeah so

305
00:23:15,520 --> 00:23:18,150
is encoded in this model is being fitted to the data

306
00:23:19,120 --> 00:23:22,500
but in fact if youtube if you try and fit the number components it will

307
00:23:22,560 --> 00:23:26,600
always say that everything is a component of the maximum likelihood is found by including

308
00:23:26,600 --> 00:23:31,080
all components there's no magical thing where find this noise before you see we have

309
00:23:31,080 --> 00:23:34,370
to one or two things one is the classic thing we all know and love

310
00:23:34,370 --> 00:23:36,690
which is disliked the latent dimensionality q

311
00:23:38,150 --> 00:23:43,140
and then fit sigma squared is the average of the discarded igon values but other cool thing you can do

312
00:23:43,650 --> 00:23:47,870
which people often don't understand appreciate because they may not understand published piece e eight

313
00:23:48,310 --> 00:23:51,620
is you can set sigma squared some known value

314
00:23:52,420 --> 00:23:52,850
let's call it

315
00:23:54,060 --> 00:23:55,330
and if you do not

316
00:23:56,210 --> 00:24:02,230
then i will determine the dimensionality of the latent space four-year automatically but what it turns out is

317
00:24:06,790 --> 00:24:10,850
but in mind this so this is you can the first q i can vectors

318
00:24:11,940 --> 00:24:13,790
although the consump covariance

319
00:24:14,440 --> 00:24:16,020
there is a q by q matrix

320
00:24:16,600 --> 00:24:17,060
is given by

321
00:24:17,060 --> 00:24:21,560
most of what we do these days are methods are theories or ideas are shaped

322
00:24:21,560 --> 00:24:24,930
to some extent by p edges influence and so what i want to do is

323
00:24:24,930 --> 00:24:28,910
big in this class is going to talk about content development by talking about his

324
00:24:31,360 --> 00:24:35,790
his idea was the children are active thinkers

325
00:24:35,810 --> 00:24:40,900
they're trying to figure out the world he often described him as little scientists

326
00:24:40,910 --> 00:24:45,720
and incidentally to know nowhere is coming from on this

327
00:24:45,780 --> 00:24:48,140
he had a very

328
00:24:49,710 --> 00:24:51,440
an ambitious goal

329
00:24:51,450 --> 00:24:55,220
he didn't start up because he was interested in children he started off because he

330
00:24:55,220 --> 00:24:58,890
is interested in emergence of knowledge in general

331
00:24:58,900 --> 00:25:03,660
it was the discipline he described as genetic epistemology

332
00:25:03,710 --> 00:25:05,700
the origins of knowledge

333
00:25:05,830 --> 00:25:11,780
but he study development of the individual child because he was convinced that this development

334
00:25:11,780 --> 00:25:16,740
will tell about the development of knowledge more generally is very snooty phrase

335
00:25:16,750 --> 00:25:19,750
that i don't have you heard before the great phrase

336
00:25:19,760 --> 00:25:23,860
it's ontology ni ri capitulates for large

337
00:25:23,900 --> 00:25:29,200
and the idea of this what that means is that the development of an individual

338
00:25:29,290 --> 00:25:32,810
mimics are repeats development of the species

339
00:25:33,690 --> 00:25:35,380
it's entirely not true

340
00:25:35,390 --> 00:25:37,680
but it's a beautiful phrase and

341
00:25:37,730 --> 00:25:41,050
he actually was committed to this he was very interesting look will figure out how

342
00:25:41,050 --> 00:25:46,970
to keep develops and that will tell us about the development of knowledge more generally

343
00:25:47,640 --> 00:25:54,200
view the child as signed as a scientist who developed his understanding these schemas these

344
00:25:54,200 --> 00:25:56,930
little miniature theories of the world

345
00:25:56,950 --> 00:26:00,630
and they did this through two sorts mechanisms

346
00:26:00,650 --> 00:26:03,460
assimilation and accommodation

347
00:26:03,470 --> 00:26:05,090
so assimilation

348
00:26:05,100 --> 00:26:10,080
would be the act of expanding the range of things that you respond

349
00:26:10,100 --> 00:26:14,880
p edges example would be a baby baby's used to sucking on her breasts

350
00:26:14,890 --> 00:26:16,800
might come to suck on on

351
00:26:16,810 --> 00:26:19,190
a bottle or on around

352
00:26:19,210 --> 00:26:22,200
that's changing the scope of things

353
00:26:22,240 --> 00:26:23,840
you respond to

354
00:26:25,640 --> 00:26:31,090
it is changing how you do it baby will form the smell differently depending on

355
00:26:31,090 --> 00:26:32,780
what is sucking

356
00:26:32,800 --> 00:26:38,610
so these processes where you take in should have given us and very physical way

357
00:26:38,660 --> 00:26:42,660
but in a more psychological set way of looking at the world you would expanded

358
00:26:42,690 --> 00:26:46,130
to encompass new things assimilation

359
00:26:46,140 --> 00:26:47,880
but you could also change

360
00:26:47,900 --> 00:26:49,820
your system of knowledge itself

361
00:26:51,980 --> 00:26:58,180
and phd argue that these two mechanisms of learning drove the child from different stages

362
00:26:58,480 --> 00:27:03,440
and yet stage theory which was quite different from the freudian stage theory we've been

363
00:27:03,440 --> 00:27:05,240
introduced to

364
00:27:05,250 --> 00:27:09,920
so was methods were asked questions children to solve problems

365
00:27:09,930 --> 00:27:12,210
and ask questions

366
00:27:12,230 --> 00:27:16,870
and his discoveries that they did them in different ways at different ages lead to

367
00:27:16,870 --> 00:27:19,600
the emergence of the stage theory

368
00:27:19,650 --> 00:27:21,560
so for prj

369
00:27:21,570 --> 00:27:24,390
the first stage

370
00:27:24,420 --> 00:27:25,920
is the sensory motor

371
00:27:25,960 --> 00:27:28,850
stage or sensorimotor period

372
00:27:28,870 --> 00:27:33,330
for here the child

373
00:27:33,400 --> 00:27:36,140
is purely physical creature

374
00:27:36,150 --> 00:27:40,420
the child has no understanding in any real way of the external world

375
00:27:40,430 --> 00:27:45,620
there's no understanding of the past no understanding of the future no stability

376
00:27:45,630 --> 00:27:47,420
no differentiation

377
00:27:49,130 --> 00:27:50,970
the child just touches

378
00:27:50,990 --> 00:27:52,180
and sees

379
00:27:52,190 --> 00:27:57,110
but doesn't get reason

380
00:27:57,160 --> 00:28:05,880
and is through this stage the child gradually comes to acquire object permanence

381
00:28:05,900 --> 00:28:11,920
object permanence is the understanding that things exist when you no longer see them

382
00:28:11,970 --> 00:28:15,270
so those of you in front you're looking at me and i go

383
00:28:15,290 --> 00:28:20,770
it could be a great magic trick by then appeared back

384
00:28:21,640 --> 00:28:24,500
now on the that object permanence

385
00:28:25,600 --> 00:28:29,800
if i win under here and then people said well hell i go

386
00:28:29,860 --> 00:28:33,610
classes over that would show a lack of object permanence

387
00:28:33,620 --> 00:28:39,700
so i don't have object permanence png is very interesting claim is

388
00:28:39,710 --> 00:28:41,390
kids don't

389
00:28:41,830 --> 00:28:44,250
before six-month-old

390
00:28:44,270 --> 00:28:48,750
piaggio observed you take an object the kid likes the chorale

391
00:28:48,760 --> 00:28:53,340
you hide it you put it behind something is like it's gone

392
00:28:53,350 --> 00:28:55,490
and the claim which i really think

393
00:28:55,510 --> 00:28:56,630
is gone

394
00:28:56,650 --> 00:29:01,290
things don't continue to exist when i'm not looking at them anymore

395
00:29:01,770 --> 00:29:05,230
and so he noticed they are surprised by peek-a-boo

396
00:29:05,280 --> 00:29:08,970
and he and his claim was one reason why the surprise peekaboo if you go

397
00:29:08,970 --> 00:29:13,450
you know you look at the kid is my god o thing because recovery face

398
00:29:13,450 --> 00:29:15,660
because he's gone

399
00:29:16,100 --> 00:29:17,390
people call

400
00:29:18,970 --> 00:29:20,600
these gone

401
00:29:20,660 --> 00:29:23,030
and you really you know that's the class

402
00:29:23,080 --> 00:29:26,050
PNG also discovered older children

403
00:29:27,060 --> 00:29:30,710
and it has this is known as the eighty not the task

404
00:29:30,720 --> 00:29:36,480
and peter gray in a psychology textbook refers to it as the changing hiding places

405
00:29:37,490 --> 00:29:39,800
which is probably a better name for

406
00:29:39,850 --> 00:29:41,010
and idea

407
00:29:41,020 --> 00:29:45,400
you take a nine-month-old entropy energy and i'm opposed just starting to make sense

408
00:29:45,420 --> 00:29:46,880
of objects

409
00:29:46,900 --> 00:29:50,980
and and or permanent you take an object and you put it

410
00:29:52,050 --> 00:29:53,900
in in a couple

411
00:29:54,000 --> 00:29:55,730
or you can see it was in the car

412
00:29:55,770 --> 00:29:58,090
so to keep your ticket will reach for it

413
00:29:58,110 --> 00:29:59,330
you do it again

414
00:29:59,350 --> 00:30:00,370
three four

415
00:30:00,420 --> 00:30:04,500
you do it again reached for that point then you take you move it over

416
00:30:05,680 --> 00:30:08,740
we actually observe kids would still reach for this

417
00:30:08,750 --> 00:30:12,240
it's like you're not smart enough to figure out is not there anymore even if

418
00:30:12,240 --> 00:30:13,660
they see it will

419
00:30:13,710 --> 00:30:19,300
and this was more evidence that they just don't understand objects in the this the

420
00:30:19,360 --> 00:30:23,630
the thing takes a lot of time and learning to develop

421
00:30:23,700 --> 00:30:26,860
the next stage the pre operational stage

422
00:30:27,350 --> 00:30:32,200
the pre operational status shall starts stop grasping the world only in the physical way

423
00:30:32,200 --> 00:30:33,670
the sensorimotor way

424
00:30:33,710 --> 00:30:38,620
but when he gets to pre operational period the capacity to represent the world

425
00:30:38,680 --> 00:30:40,870
that the world inside your head

426
00:30:40,880 --> 00:30:43,150
comes into being

427
00:30:43,160 --> 00:30:45,350
but it's limited

428
00:30:45,360 --> 00:30:48,800
and this limited in the couple of striking ways

429
00:30:48,820 --> 00:30:53,170
one way in which is limited is that children are egocentric

430
00:30:53,180 --> 00:30:58,900
he goes centrism has a meaning in common english which means to be selfish he

431
00:30:58,900 --> 00:31:01,180
actually meant more technical way

432
00:31:01,200 --> 00:31:04,520
he claimed the trunk lid at this age literally

433
00:31:04,540 --> 00:31:08,990
ten to understand others can see the world differently from them

434
00:31:09,010 --> 00:31:13,040
so one of these demonstrations was the three mountains task

435
00:31:13,060 --> 00:31:17,020
three mountains over there you put a child on one side of the mountains and

436
00:31:17,020 --> 00:31:18,370
you ask him to draw

437
00:31:18,610 --> 00:31:22,040
four five year-old can do it easily

438
00:31:22,060 --> 00:31:26,900
but then you ask him to draw as it would appear from the other side

439
00:31:26,940 --> 00:31:29,940
and sure find it's extraordinarily difficult

440
00:31:29,990 --> 00:31:34,180
they find it very difficult to grasp the world as another person

441
00:31:34,190 --> 00:31:36,100
you might see it

442
00:31:36,120 --> 00:31:41,330
another significant findings actually had about

443
00:31:41,340 --> 00:31:43,340
this phase of development

444
00:31:43,350 --> 00:31:46,080
concerns was called conservation

445
00:31:46,090 --> 00:31:48,350
and the notion of conservation is

446
00:31:48,360 --> 00:31:51,340
that there's ways to transform things

447
00:31:51,360 --> 00:31:53,260
such that

448
00:31:53,270 --> 00:31:58,110
some aspects of the change but others remain the same

449
00:31:58,160 --> 00:32:00,310
so for instance

450
00:32:01,790 --> 00:32:03,850
if you take a glass of water

451
00:32:03,860 --> 00:32:06,510
newport into another class this shallow

452
00:32:06,510 --> 00:32:15,210
it would be

453
00:32:15,270 --> 00:32:21,310
by the

454
00:32:24,230 --> 00:32:32,530
this is from move from to

455
00:32:39,000 --> 00:32:42,140
the set of all

456
00:32:42,230 --> 00:32:50,680
those who is also because of the

457
00:32:50,790 --> 00:32:53,070
you want to

458
00:32:58,170 --> 00:33:03,320
o thing is

459
00:33:03,540 --> 00:33:15,850
you have to do with this

460
00:33:17,650 --> 00:33:32,620
the last of the interaction we is to the number of homes also

461
00:33:33,950 --> 00:33:37,340
let's sure

462
00:33:37,370 --> 00:33:44,900
has exactly the same as one of those

463
00:33:44,930 --> 00:33:47,570
thank you for have know

464
00:33:47,580 --> 00:33:50,130
it's a pleasure to be here is university

465
00:33:50,140 --> 00:33:53,010
today i'm going to

466
00:33:53,020 --> 00:33:56,760
discuss financial crisis and systemic risks

467
00:33:57,410 --> 00:33:59,690
the motivation of

468
00:33:59,700 --> 00:34:02,010
OK many but let start with two

469
00:34:02,020 --> 00:34:04,350
one is

470
00:34:04,350 --> 00:34:09,470
and has been discussion among the community around this risk initiative

471
00:34:09,550 --> 00:34:14,450
is the emphasis on extremes that punctuate complex system in the

472
00:34:14,470 --> 00:34:21,610
interesting system in natural sciences or social sciences any of this system are punctuated by

473
00:34:21,610 --> 00:34:23,250
extreme events

474
00:34:23,260 --> 00:34:29,020
and this extreme events are not as much appreciated in there

475
00:34:29,040 --> 00:34:32,070
in fact in the consequences and so on

476
00:34:32,100 --> 00:34:36,380
here the emphasis on the study of fashion crashes offers

477
00:34:36,390 --> 00:34:39,540
first possibility to quantitatively

478
00:34:39,600 --> 00:34:42,480
access one of these questions of

479
00:34:42,500 --> 00:34:47,670
what are these extreme events what what what how are they characterize can we detect

480
00:34:47,690 --> 00:34:50,290
them can we say something quantitative

481
00:34:50,300 --> 00:34:55,480
another motivation of course is that we want to put this research agenda

482
00:34:55,500 --> 00:35:00,760
on the quantitative level in the social sciences and of course financial economics offers a

483
00:35:00,760 --> 00:35:02,260
remarkable example

484
00:35:02,270 --> 00:35:05,510
of access to quality data so i'm using these

485
00:35:06,540 --> 00:35:08,720
tion hopes to start

486
00:35:09,700 --> 00:35:11,630
address this question

487
00:35:11,690 --> 00:35:15,350
of the importance of extreme events in social sciences

488
00:35:16,320 --> 00:35:18,600
the question i'm going to address one by one

489
00:35:18,610 --> 00:35:21,190
in this discussion of the following

490
00:35:21,200 --> 00:35:23,820
i will put the perspective broadly

491
00:35:26,200 --> 00:35:31,510
from the perspective of my financially colleagues in academia

492
00:35:32,610 --> 00:35:37,440
but also in crash real things that are worthwhile to study or even do they

493
00:35:38,730 --> 00:35:43,690
you see that even this question which seems to stop stupid to start with actually

494
00:35:43,700 --> 00:35:48,320
is not still the forefront of the research in academia

495
00:35:48,330 --> 00:35:52,860
so the question i'm going to discuss is why many existing we pass over the

496
00:35:52,860 --> 00:35:55,250
first hurdle life except exist

497
00:35:55,290 --> 00:35:59,200
why are they so difficult to diagnose

498
00:36:00,050 --> 00:36:03,500
can we even though know them first after that first

499
00:36:03,580 --> 00:36:04,910
is about

500
00:36:04,960 --> 00:36:08,720
could it be possible to actually that no them before they were so that we

501
00:36:08,720 --> 00:36:10,140
can have some body

502
00:36:10,200 --> 00:36:12,270
interesting policy making

503
00:36:12,280 --> 00:36:15,010
and that will then pass on to

504
00:36:15,410 --> 00:36:20,130
recent events with the real estate bubble on sediments worldwide

505
00:36:20,140 --> 00:36:24,620
examining examining a few instances in the UK and the US especially

506
00:36:24,630 --> 00:36:28,260
the subsequent about about but which is the mortgage

507
00:36:28,270 --> 00:36:30,690
mortgage-backed security

508
00:36:30,700 --> 00:36:34,830
above all can be considered as a specific level depending on

509
00:36:34,910 --> 00:36:37,470
the financial but the real estate bubble

510
00:36:37,510 --> 00:36:42,490
and the consequent dangers and possible systemic risks

511
00:36:42,550 --> 00:36:48,880
that derived from these two interleague levels and then i try to show you that

512
00:36:48,890 --> 00:36:50,310
there is some hope

513
00:36:50,320 --> 00:36:53,820
by developing metrics ought to be quantitative

514
00:36:53,870 --> 00:36:55,510
and i think that all the debate

515
00:36:56,270 --> 00:36:59,930
would be raised in the first part of the talk

516
00:37:00,570 --> 00:37:04,120
especially in all of that but i do think this can we can we say

517
00:37:04,120 --> 00:37:06,320
something about them

518
00:37:06,320 --> 00:37:08,200
can we do policy-making

519
00:37:09,390 --> 00:37:14,070
should be articulated around developing new and better metrics

520
00:37:14,080 --> 00:37:19,180
to inform us about this and show some progress the output of the blood

521
00:37:19,450 --> 00:37:21,520
over the last decade or so

522
00:37:22,200 --> 00:37:24,680
and some they would finish with some

523
00:37:24,690 --> 00:37:26,930
prediction we

524
00:37:27,320 --> 00:37:30,410
in which we stick all make out and

525
00:37:30,460 --> 00:37:33,680
published in advance and what we the outcome

526
00:37:33,720 --> 00:37:37,710
so let's start menu in for surfaces along the new

527
00:37:37,950 --> 00:37:41,180
so what is how do we detect them how

528
00:37:41,270 --> 00:37:45,330
can we predict i summarise you single slide

529
00:37:45,360 --> 00:37:48,240
i would say the smallest the

530
00:37:48,250 --> 00:37:49,620
o point of view

531
00:37:49,630 --> 00:37:56,310
common wisdom in financial economics which is summarized by this review paper quite recent

532
00:37:56,360 --> 00:38:02,080
and you can read that basically says what you can really developed

533
00:38:03,570 --> 00:38:07,630
as that's when you look at the literature that for every paper that finds evidence

534
00:38:07,630 --> 00:38:09,760
of financial bubble there is another one

535
00:38:09,810 --> 00:38:15,310
that fits the data equally well without allowing for about

536
00:38:15,570 --> 00:38:20,070
and we are still unable to the extra requested and able to distinguish bubbles from

537
00:38:20,070 --> 00:38:21,820
time varying or regime

538
00:38:21,820 --> 00:38:25,160
switching fundamentals mean that indeed there

539
00:38:25,390 --> 00:38:28,580
in the kind of study that found can observe

540
00:38:29,740 --> 00:38:34,610
these always from the framework way to justify these

541
00:38:34,650 --> 00:38:36,200
supposedly numbers

542
00:38:36,210 --> 00:38:38,710
price behavior pricing

543
00:38:38,720 --> 00:38:44,320
in terms of different for series of the financial value

544
00:38:44,330 --> 00:38:45,990
other firms

545
00:38:46,000 --> 00:38:48,520
an interesting these but because they have

546
00:38:48,530 --> 00:38:51,200
i should have said in the beginning there are

547
00:38:53,110 --> 00:38:57,550
if they do exist if we can identify them and i think that in any

548
00:38:57,570 --> 00:38:59,570
this debate scientific if we make

549
00:38:59,580 --> 00:39:03,990
progress by looking at the moment is of course the monsters and the height

550
00:39:04,010 --> 00:39:10,330
actually they don't have to reveal a lot of information between competing forces that are

551
00:39:12,260 --> 00:39:17,240
seven interceptor way in normal times when these the extreme

552
00:39:17,250 --> 00:39:19,260
we hope to see to learn more

553
00:39:19,260 --> 00:39:22,880
so you see the understanding from the

554
00:39:22,890 --> 00:39:26,890
economics fear in fact economics is pretty dismal now

555
00:39:26,900 --> 00:39:28,570
let's shift two

556
00:39:28,620 --> 00:39:31,450
professional a certain kind of professional

557
00:39:31,500 --> 00:39:34,370
the federal reserve in the voice of

558
00:39:35,320 --> 00:39:39,110
the previous federal reserve chairman greenspan

559
00:39:39,120 --> 00:39:43,760
supposedly at the time the most powerful man in the world group

560
00:39:43,770 --> 00:39:45,570
i give believe according to several

561
00:39:48,060 --> 00:39:52,860
this is the statements in one of the papers he wrote

562
00:39:53,130 --> 00:39:55,130
in the conference in

563
00:39:55,180 --> 00:39:57,380
december two thousand two so

564
00:39:57,390 --> 00:40:03,050
basically two years after the ICT internet communication technology bubble burst

565
00:40:03,090 --> 00:40:07,700
led to approximately forty percent drop on the nasdaq and some of the major indices

566
00:40:07,700 --> 00:40:08,930
in the US

567
00:40:08,940 --> 00:40:10,860
also in major places

568
00:40:10,880 --> 00:40:13,430
in europe and japan

569
00:40:13,440 --> 00:40:20,130
and where was subsequently followed by continuous set of small shops and big towns

570
00:40:20,140 --> 00:40:23,490
commutativity losing sixty seventy percent depending on the

571
00:40:23,500 --> 00:40:26,000
financial centers

572
00:40:26,030 --> 00:40:29,020
and you can see that what he said is pretty interesting we at the federal

573
00:40:29,020 --> 00:40:31,130
reserve recognised that despite

574
00:40:31,140 --> 00:40:33,710
our suspicions it was very difficult

575
00:40:33,760 --> 00:40:40,440
definitely identifiable bubble until after the fact that is when its bursting confirmed its existence

576
00:40:40,500 --> 00:40:42,780
and in addition even

577
00:40:42,800 --> 00:40:46,770
so continued even if we had detectors such about

578
00:40:46,790 --> 00:40:49,680
we wouldn't have done anything to prevent it

579
00:40:49,710 --> 00:40:50,720
to prevent

580
00:40:50,730 --> 00:40:53,460
what they were outcome could

581
00:40:53,480 --> 00:40:54,690
from it

582
00:40:55,690 --> 00:40:58,230
the risk of triggering

583
00:40:58,630 --> 00:41:03,290
aftershocks of consequences that would have been content

584
00:41:03,340 --> 00:41:07,350
that's very interesting from they said this was the most powerful man in in the

585
00:41:10,010 --> 00:41:12,750
revealing complete lack of a

586
00:41:12,810 --> 00:41:16,480
now lets me from this point of view now

587
00:41:16,490 --> 00:41:17,760
try to build

588
00:41:17,770 --> 00:41:18,940
the case

589
00:41:18,970 --> 00:41:20,290
four indeed

590
00:41:20,290 --> 00:41:23,810
pick a different randomisation unscramble the column

591
00:41:23,820 --> 00:41:28,310
and hopefully by this destroy the complete structure which has been in your data

592
00:41:28,350 --> 00:41:32,570
and then use this new grammar data to generate you reference

593
00:41:35,290 --> 00:41:36,850
two to compute this

594
00:41:36,860 --> 00:41:41,640
this uniform consisted taken scrambled data and apply to

595
00:41:41,810 --> 00:41:44,740
i have some examples of this looks like

596
00:41:45,310 --> 00:41:51,970
it's actually quite interesting to see what happens so here we have an original dataset

597
00:41:52,120 --> 00:41:53,610
it's one we start off with

598
00:41:53,670 --> 00:41:58,010
now the first they what i said you just generate a uniform distribution on the

599
00:41:58,010 --> 00:42:01,420
same domain of this dataset so just check what is the smallest value and the

600
00:42:01,420 --> 00:42:04,910
largest value it in this domain so that's the box here and you do the

601
00:42:05,730 --> 00:42:11,310
for y axis you generate every uniform distribution in this box

602
00:42:12,080 --> 00:42:16,280
the other thing the scrambling of the data

603
00:42:16,430 --> 00:42:19,360
you see it on this figure already does something which

604
00:42:19,420 --> 00:42:26,410
does not look like a uniform distribution so what happened here is

605
00:42:26,450 --> 00:42:29,150
so you see that something which is sort of symmetric

606
00:42:30,010 --> 00:42:30,760
we have

607
00:42:30,770 --> 00:42:32,290
lots of so for example

608
00:42:32,300 --> 00:42:34,760
none of the data points features in this

609
00:42:34,780 --> 00:42:36,220
in this area here

610
00:42:36,230 --> 00:42:39,680
and after spending we also have the features in this area here

611
00:42:39,690 --> 00:42:42,070
so some of the scrambling

612
00:42:43,740 --> 00:42:45,350
at least a bit

613
00:42:45,360 --> 00:42:46,570
and the there

614
00:42:46,570 --> 00:42:48,350
the area of the features

615
00:42:48,410 --> 00:42:51,180
but it's not clear whether this is something good or bad

616
00:42:51,360 --> 00:42:53,690
i think it can have both

617
00:42:55,660 --> 00:42:59,840
and so what i have here is now plotted all those statistics

618
00:42:59,900 --> 00:43:06,960
so no need to look

619
00:43:06,980 --> 00:43:10,560
OK so the black line which is actually covered by the box here is the

620
00:43:10,560 --> 00:43:16,280
original so we always with so for this dataset always concluded with since cluster similarity

621
00:43:16,430 --> 00:43:19,700
that should be the measure which i want to evaluate my data

622
00:43:19,790 --> 00:43:22,370
so if i look at the black curve it goes down like this so that

623
00:43:22,390 --> 00:43:25,650
the curve and it's not very helpful

624
00:43:25,690 --> 00:43:31,820
now if you look at the same curve for the uniform data

625
00:43:31,840 --> 00:43:32,930
like this

626
00:43:32,940 --> 00:43:37,830
it that's the other countries have which is like it's going down like this

627
00:43:37,840 --> 00:43:40,590
but now if you look at the ratio between those

628
00:43:40,650 --> 00:43:44,600
it's the started line here

629
00:43:44,620 --> 00:43:46,950
it actually

630
00:43:46,960 --> 00:43:48,420
that's this line here

631
00:43:48,480 --> 00:43:54,170
in this case it's not really helpful so what it says something like it is

632
00:43:54,170 --> 00:43:57,930
so i can say i don't know whether you can see it goes like this

633
00:43:57,940 --> 00:44:01,010
so it has at its lowest point two

634
00:44:01,030 --> 00:44:06,120
and then it goes up and small constant somehow and now take the minimum of

635
00:44:06,120 --> 00:44:10,490
this curve you would say i take two clusters

636
00:44:10,560 --> 00:44:14,030
and you can do it you can do is now for others like the dashed

637
00:44:14,030 --> 00:44:14,870
line is

638
00:44:14,890 --> 00:44:20,120
the line we used to describe the data here is the reference statistics it's more

639
00:44:20,120 --> 00:44:24,570
or less constant so in this case it didn't really work very well actually

640
00:44:24,580 --> 00:44:27,480
so if you take the minimum wage would end up with six h

641
00:44:27,510 --> 00:44:31,460
but i mean that's more or less the way

642
00:44:31,470 --> 00:44:33,570
it is assumed to

643
00:44:41,870 --> 00:44:46,360
so in this example i only run once but of course it's better to run

644
00:44:46,360 --> 00:44:50,850
it several times so what usually people do is you run you run your group

645
00:44:50,860 --> 00:44:54,910
several times slightly different in sensational on the on your original data and you do

646
00:44:54,920 --> 00:44:59,090
the same on the and in particular if you if is it to this randomisation

647
00:44:59,090 --> 00:45:01,780
here i mean this is just a random instance so you should

648
00:45:01,870 --> 00:45:08,740
draw maybe ten instances here and then average over it

649
00:45:08,760 --> 00:45:12,960
actually i i can remember probably this is only for one example but i can't

650
00:45:12,960 --> 00:45:16,280
remember how i

651
00:45:18,970 --> 00:45:24,040
here we have the second example

652
00:45:24,070 --> 00:45:27,770
well actually it doesn't show anything and so is what we see again is that

653
00:45:27,770 --> 00:45:31,110
this kind data somehow

654
00:45:31,140 --> 00:45:36,610
doesn't look the same as uniform data so someone just

655
00:45:36,620 --> 00:45:40,710
it's also

656
00:45:40,840 --> 00:45:45,920
right so

657
00:45:45,930 --> 00:45:49,390
right so i think the effect in principle the effect is

658
00:45:49,440 --> 00:45:51,840
it's got that

659
00:45:51,900 --> 00:45:53,920
exactly that's what i want to the

660
00:45:53,940 --> 00:45:58,070
sample from many parts of the space where i don't have data that could be

661
00:45:58,070 --> 00:46:01,350
that it's a bit too restrictive so that means

662
00:46:02,520 --> 00:46:06,680
it could be that i just by by chance only observed few features but in

663
00:46:06,680 --> 00:46:10,820
fact there are some more features which are actually probably would observe more data but

664
00:46:10,820 --> 00:46:15,260
i haven't so this so i think it's it was both the two extremes in

665
00:46:15,260 --> 00:46:18,690
the in the the uniform case you just say i don't care at all is

666
00:46:18,690 --> 00:46:20,840
simply random uniformly from the square

667
00:46:20,900 --> 00:46:22,690
and the other cases say

668
00:46:22,700 --> 00:46:26,870
i really only want to get results is or features it was exactly the same

669
00:46:26,870 --> 00:46:30,050
values of as the ones i have my data so

670
00:46:30,060 --> 00:46:32,030
something inbetween might be

671
00:46:33,200 --> 00:46:36,540
so i mean of course but you could also try i mean by that i

672
00:46:36,540 --> 00:46:38,360
mean again heuristics that you could try

673
00:46:38,440 --> 00:46:41,370
simple sometimes for uniform sometimes from

674
00:46:41,390 --> 00:46:44,940
from this grammar data and then take the average indian

675
00:46:44,950 --> 00:46:50,240
i don't know whether that would be helpful

676
00:46:50,270 --> 00:46:54,680
OK so that was about the gap statistic there's another way of

677
00:46:54,810 --> 00:46:59,710
selecting the number of clusters which is the stability approach

678
00:46:59,730 --> 00:47:00,800
and it's based

679
00:47:01,990 --> 00:47:06,460
on a very high level some based on the fact that so we're doing science

680
00:47:06,530 --> 00:47:13,740
working datasets and somehow one the results you obtained certain dataset is reproducible so if

681
00:47:14,220 --> 00:47:20,560
i'm i don't have bioinformatics lab i do a certain gene microarray data experiment in

682
00:47:20,560 --> 00:47:24,120
the cluster later on in all somebody else comes in a different levels the same

683
00:47:24,120 --> 00:47:29,700
experiment clusters it with the same argument people would like that hopefully the results are

684
00:47:29,750 --> 00:47:31,850
more less stable

685
00:47:31,870 --> 00:47:36,220
OK on this high level i think most people would agree that we want the

686
00:47:36,240 --> 00:47:39,450
results table of course something which is nice

687
00:47:39,470 --> 00:47:44,510
now the idea for selecting the number of clusters is

688
00:47:44,520 --> 00:47:47,450
we're just run over many times

689
00:47:47,500 --> 00:47:49,530
four different numbers of clusters

690
00:47:49,540 --> 00:47:52,470
we measure how stable the results in a certain way

691
00:47:52,530 --> 00:47:57,810
and then we select the number of clusters the result is most stable

692
00:47:58,480 --> 00:48:01,110
and i will show some figures how this works

693
00:48:01,130 --> 00:48:02,030
in practice

694
00:48:02,030 --> 00:48:04,840
two class case fifty fifth

695
00:48:04,860 --> 00:48:06,450
it's a reasonable thing to do

696
00:48:06,540 --> 00:48:09,510
and if the class is finite and small

697
00:48:09,550 --> 00:48:13,990
it's often a good way to proceed to five small ten is more

698
00:48:15,060 --> 00:48:16,580
in the case of

699
00:48:16,590 --> 00:48:21,600
the continuous case like the ability to the class we had ten thousand interval zero

700
00:48:22,930 --> 00:48:28,540
i mean this class is infinite but at least it's mathematically compact and you can

701
00:48:28,580 --> 00:48:30,850
have a higher density

702
00:48:30,860 --> 00:48:33,670
and especially uniform density here

703
00:48:33,790 --> 00:48:37,600
the problem with the uniform density or

704
00:48:37,620 --> 00:48:42,610
they generalizing the symmetry principle to the continuous class in this naive way is the

705
00:48:47,430 --> 00:48:51,430
i assume that parameter in zero one

706
00:48:51,530 --> 00:48:54,870
images p of ten to equal to

707
00:48:56,860 --> 00:48:58,360
but then you know

708
00:48:58,370 --> 00:49:03,550
but now somebody comes along and say OK let's think of different parameterizations

709
00:49:03,870 --> 00:49:05,330
these square

710
00:49:05,340 --> 00:49:10,040
which is also in the interval zero one

711
00:49:10,080 --> 00:49:12,280
so what we then have we have

712
00:49:15,780 --> 00:49:18,180
the detector

713
00:49:18,240 --> 00:49:22,810
and you see the distribution size no longer uniform

714
00:49:23,090 --> 00:49:26,840
so and i mean why is i mean this parameterisation

715
00:49:27,070 --> 00:49:29,870
ten being by coin rather than taking

716
00:49:29,880 --> 00:49:31,380
the square root of this

717
00:49:34,140 --> 00:49:35,490
so that's the problem

718
00:49:35,530 --> 00:49:40,710
which jeffreys and ran out of coal and so there is an analogue in

719
00:49:42,780 --> 00:49:47,690
that is don't take it mathematically to literally sort of compact parametric

720
00:49:48,720 --> 00:49:53,230
and you can employ jeffreys prior and if you would do that in the case

721
00:49:53,230 --> 00:49:54,530
like glass case

722
00:49:54,850 --> 00:49:59,520
it would be for instance ten times one minus that

723
00:50:00,680 --> 00:50:02,960
two the power minus one

724
00:50:04,430 --> 00:50:09,090
and what you would get is instead of la plata because k plus one divided

725
00:50:09,090 --> 00:50:10,670
by n plus two

726
00:50:10,710 --> 00:50:14,520
you will get half one

727
00:50:14,530 --> 00:50:16,760
so that's sort of the default

728
00:50:18,190 --> 00:50:22,530
prior in base so if you have a compact metric class

729
00:50:22,580 --> 00:50:25,990
we have no prior background knowledge two labourers two

730
00:50:25,990 --> 00:50:30,130
to get it then use jeffreys prior before fortress

731
00:50:36,780 --> 00:50:48,480
no they have actually already

732
00:50:48,570 --> 00:50:52,070
fifty fifty belief seems to have policies but then i see my data in the

733
00:50:52,070 --> 00:50:55,580
past year then concentrates you know on the policies which is in this case is

734
00:50:55,580 --> 00:50:56,830
more likely

735
00:50:56,890 --> 00:51:00,590
i mean this case for instance that the MAP estimator then reduces to the maximum

736
00:51:04,830 --> 00:51:07,720
i mean this is curves here so

737
00:51:07,720 --> 00:51:10,010
i mean we saw in the continuous case

738
00:51:14,710 --> 00:51:17,060
i mean you start with a uniform prior

739
00:51:17,080 --> 00:51:18,340
tend to

740
00:51:18,390 --> 00:51:20,930
and then you see some data concentrates

741
00:51:21,070 --> 00:51:23,900
around the

742
00:51:23,950 --> 00:51:25,890
true parameter

743
00:51:25,950 --> 00:51:27,960
and the same is true in the discrete case

744
00:51:32,260 --> 00:51:49,080
it so you get the maximum likelihood for you do but

745
00:51:49,520 --> 00:51:53,880
if you see i mean if my parameter tend to live in

746
00:51:53,940 --> 00:51:56,530
in zero infinity

747
00:51:56,580 --> 00:52:01,590
i could apply maximum likelihood what i cannot use a uniform prior anymore

748
00:52:01,600 --> 00:52:05,030
so this principle breaks down to do something else or if my class is the

749
00:52:05,030 --> 00:52:08,100
class of polynomials of degree one two three four five

750
00:52:08,110 --> 00:52:11,960
they assume even you compactify the range parameters

751
00:52:12,000 --> 00:52:17,060
i mean what is a uniform parameter distribution over different dimensions

752
00:52:18,750 --> 00:52:22,570
so that's sort of you you can argue maximum likelihood is good

753
00:52:22,670 --> 00:52:25,840
if there exists a uniform bayesian prior

754
00:52:25,860 --> 00:52:26,700
which is

755
00:52:26,930 --> 00:52:30,070
often true this argument

756
00:52:30,080 --> 00:52:33,530
the answer the question

757
00:52:39,540 --> 00:52:52,120
OK so

758
00:52:52,170 --> 00:52:54,560
but OK all the problem

759
00:52:54,590 --> 00:52:58,350
this is the symmetry or principle jeffreys prior

760
00:52:58,400 --> 00:53:01,420
is it doesn't work for large model classes

761
00:53:01,430 --> 00:53:04,200
so even for countably infinite number what this

762
00:53:04,210 --> 00:53:07,840
i mean there's no uniform prior over the natural numbers

763
00:53:07,900 --> 00:53:11,680
you have to do something and the same there is no uniform prior over the

764
00:53:11,690 --> 00:53:15,020
real or not impressed by all the real

765
00:53:15,020 --> 00:53:16,720
so the solution is

766
00:53:16,730 --> 00:53:20,190
the book come phrases which

767
00:53:20,240 --> 00:53:21,900
favors simplicity

768
00:53:21,920 --> 00:53:26,510
so what we need is and this is the next slide

769
00:53:26,640 --> 00:53:32,290
quantitative and universal measure of complexity

770
00:53:34,620 --> 00:53:38,100
let's assume that start simple let's assume you are binary

771
00:53:38,110 --> 00:53:39,260
binary data

772
00:53:39,260 --> 00:53:40,260
by spring

773
00:53:40,270 --> 00:53:41,790
five or something

774
00:53:41,850 --> 00:53:45,000
and i want to measure some of the complexity of the time

775
00:53:45,050 --> 00:53:50,020
the file contains only zeros one million zero would say OK it's pretty simple

776
00:53:51,760 --> 00:53:53,650
he recruited for the edges of pi

777
00:53:54,020 --> 00:53:57,150
i mean there's more information and more complex

778
00:53:57,180 --> 00:54:00,740
if there is no if you can't even figure out the structure so i mean

779
00:54:00,740 --> 00:54:03,510
there's probably a lot of information in there

780
00:54:03,540 --> 00:54:08,780
so the measure should i mean respect of course our intuitive notion of complexity

781
00:54:08,920 --> 00:54:11,850
so what have you done for me you see the million one

782
00:54:11,880 --> 00:54:13,830
i mean zero

783
00:54:13,880 --> 00:54:17,260
the reason why you think this is simple because you have a simple description of

784
00:54:17,260 --> 00:54:19,850
these are just one million zero

785
00:54:23,940 --> 00:54:25,850
this description in english

786
00:54:25,870 --> 00:54:30,490
so we need to formalize that so let's take a description of formal description

787
00:54:30,500 --> 00:54:32,900
and the former language we can take

788
00:54:32,920 --> 00:54:34,870
the programming language c

789
00:54:34,930 --> 00:54:36,630
by small c program

790
00:54:36,680 --> 00:54:41,070
four look for i one two one million brain zero

791
00:54:41,100 --> 00:54:43,050
so you is

792
00:54:43,090 --> 00:54:46,990
a universal turing machine or your favourite programming language

793
00:54:47,030 --> 00:54:49,780
and x you binary string

794
00:54:49,790 --> 00:54:51,850
and you look for programs which

795
00:54:51,970 --> 00:54:54,030
plug into

796
00:54:54,050 --> 00:54:56,280
universal turing machine the screen

797
00:54:56,290 --> 00:54:59,180
your data x and there are many such programs

798
00:54:59,180 --> 00:55:02,360
we look forward to take the length of the program and look for the shortest

799
00:55:03,780 --> 00:55:08,730
and the length of this problem is called the kolmogorov complexity

800
00:55:08,780 --> 00:55:09,580
and you see

801
00:55:09,610 --> 00:55:11,340
that i mean

802
00:55:11,350 --> 00:55:14,710
o zeros have short program

803
00:55:14,790 --> 00:55:16,580
digits of pi

804
00:55:16,600 --> 00:55:20,010
also quite a short programs of computer that high

805
00:55:20,050 --> 00:55:22,260
but some random

806
00:55:24,230 --> 00:55:27,180
the only way to store that

807
00:55:27,180 --> 00:55:32,010
o to compute that some just print quotation mark in the data

808
00:55:33,280 --> 00:55:37,810
if you have nonsense objects like a number of functions or whatever

809
00:55:38,030 --> 00:55:40,960
you just call them in a binary way in

810
00:55:41,030 --> 00:55:42,520
one way doesn't really matter

811
00:55:42,550 --> 00:55:47,270
and then you look for the short program and computing by j

812
00:55:47,290 --> 00:55:49,520
so in natural numbers

813
00:55:49,600 --> 00:55:52,230
i mean there is a bijection between

814
00:55:52,230 --> 00:55:55,830
and the to to the finer things we do that then you look for short

815
00:55:55,830 --> 00:55:58,610
programs in the finance

816
00:55:58,610 --> 00:56:00,450
and the icing

817
00:56:01,390 --> 00:56:03,120
let's sing council

818
00:56:03,140 --> 00:56:04,670
if we have this symmetry

819
00:56:04,690 --> 00:56:06,580
in the probability distribution

820
00:56:06,580 --> 00:56:10,030
of of r epsilon

821
00:56:11,440 --> 00:56:13,870
you generate YT centered

822
00:56:13,890 --> 00:56:15,330
that x t

823
00:56:15,340 --> 00:56:17,600
and when you come to the ratio

824
00:56:18,730 --> 00:56:22,360
of y or of x then you would have

825
00:56:22,390 --> 00:56:27,430
q of x given y over q of y given x but you don't even

826
00:56:27,450 --> 00:56:28,680
y is equal

827
00:56:28,690 --> 00:56:30,430
q of ref

828
00:56:30,610 --> 00:56:33,280
o y given x and therefore

829
00:56:33,310 --> 00:56:34,400
the council

830
00:56:34,400 --> 00:56:37,180
and all that directs

831
00:56:37,190 --> 00:56:38,850
the use of

832
00:56:38,850 --> 00:56:41,640
this random work in the ratio

833
00:56:41,690 --> 00:56:43,940
of of the house

834
00:56:43,950 --> 00:56:45,960
because just the speed

835
00:56:45,980 --> 00:56:52,440
and the duration of stays at specific values are directed by f on

836
00:56:52,450 --> 00:56:58,530
which is very nice thing because it makes are and if your interpretation of the

837
00:56:59,710 --> 00:57:02,000
so for instance if you're rich

838
00:57:02,020 --> 00:57:03,720
and next to

839
00:57:03,770 --> 00:57:09,920
that has a very large compared with all the other if you close the mood

840
00:57:09,930 --> 00:57:11,440
of f

841
00:57:11,450 --> 00:57:12,760
it will take

842
00:57:12,790 --> 00:57:16,290
for time to get out of the point

843
00:57:16,300 --> 00:57:20,000
maybe if you propose values that are far away from that node

844
00:57:20,340 --> 00:57:23,020
there will always fall in love regions

845
00:57:23,080 --> 00:57:24,450
and you will stay

846
00:57:24,480 --> 00:57:26,000
a long time

847
00:57:26,050 --> 00:57:27,180
in that

848
00:57:27,190 --> 00:57:30,380
xt on the other hand if c

849
00:57:30,430 --> 00:57:32,920
is on the origin of

850
00:57:32,980 --> 00:57:35,500
most of the value we propose

851
00:57:35,720 --> 00:57:37,220
higher at

852
00:57:37,230 --> 00:57:38,710
and therefore you removed

853
00:57:39,780 --> 00:57:42,520
to this new that so on average

854
00:57:42,530 --> 00:57:43,770
you will stay

855
00:57:44,590 --> 00:57:46,900
to point

856
00:57:46,950 --> 00:57:49,110
proportionally to at

857
00:57:49,130 --> 00:57:52,440
therefore the frequency of visits of x

858
00:57:53,170 --> 00:57:56,210
up to constant f of

859
00:57:56,230 --> 00:58:01,100
never it makes sense that f is the stationary distribution of that she could f

860
00:58:01,100 --> 00:58:02,900
of x

861
00:58:02,900 --> 00:58:06,430
direct the duration of stay at

862
00:58:07,230 --> 00:58:13,150
for the

863
00:58:21,040 --> 00:58:23,390
so the disability

864
00:58:23,680 --> 00:58:28,020
could could hold in in

865
00:58:28,030 --> 00:58:30,210
theory and not in practice

866
00:58:30,220 --> 00:58:37,330
well would say

867
00:58:37,480 --> 00:58:39,620
example once more

868
00:58:41,050 --> 00:58:43,120
OK i will processes that

869
00:58:43,130 --> 00:58:44,750
we wrong proposal

870
00:58:44,780 --> 00:58:46,020
and you will see that

871
00:58:46,030 --> 00:58:49,560
there are two mode if you keep jumping

872
00:58:51,100 --> 00:58:52,690
small scale

873
00:58:55,470 --> 00:58:57,360
you can reach this but

874
00:58:57,360 --> 00:59:01,020
the scale is too small to get to

875
00:59:15,090 --> 00:59:19,530
to give an historical example here is the example of how things

876
00:59:19,560 --> 00:59:23,010
in his original paper

877
00:59:23,030 --> 00:59:26,420
it's the example of course you want to generate and normal

878
00:59:26,420 --> 00:59:28,240
and instead of using the normal

879
00:59:28,300 --> 00:59:29,940
use uniform

880
00:59:29,950 --> 00:59:31,330
so the random walk

881
00:59:31,350 --> 00:59:32,460
is just

882
00:59:32,590 --> 00:59:36,910
xt and you move xt within the range of miners downtown

883
00:59:36,910 --> 00:59:37,930
plus sixty

884
00:59:38,240 --> 00:59:40,220
sixty plus delta

885
00:59:40,600 --> 00:59:44,930
and of course the a property effective because around the world

886
00:59:44,970 --> 00:59:46,970
is uniform

887
00:59:47,020 --> 00:59:49,760
does anybody uniform is only the ratio

888
00:59:49,770 --> 00:59:51,720
of the normals

889
00:59:54,940 --> 00:59:58,730
you can get sample statistics that are not

890
00:59:58,740 --> 01:00:01,230
of importance but the output

891
01:00:02,160 --> 01:00:07,440
all of this algorithm depending on down here i began to take with you

892
01:00:07,450 --> 01:00:09,680
one delta because you

893
01:00:09,690 --> 01:00:13,110
o point five and that i call one

894
01:00:13,960 --> 01:00:16,390
depending on the choice of tar

895
01:00:16,390 --> 01:00:19,680
the move will be

896
01:00:19,690 --> 01:00:21,870
on different scales of course so

897
01:00:23,090 --> 01:00:24,930
down a small

898
01:00:24,940 --> 01:00:28,920
i'll take millwall to go around and so this is the empirical mean you can

899
01:00:28,920 --> 01:00:30,450
see that the empirical mean

900
01:00:30,470 --> 01:00:35,950
it is very very correlated the integration of the sequence is very high

901
01:00:35,950 --> 01:00:41,970
the phone content is provided by MIT opencourseware under a creative commons license

902
01:00:42,050 --> 01:00:51,570
additional information about relations in MIT opencourseware in general is available OCW MIT the EU

903
01:00:52,610 --> 01:00:59,200
so in this course because i am biological give you some examples between chemistry and

904
01:01:00,270 --> 01:01:03,550
teaching the principles of chemistry and explained you

905
01:01:03,550 --> 01:01:09,110
why that's relevant relevant how it applies to more life sciences biological areas is very

906
01:01:09,110 --> 01:01:11,490
good sense of how i transition

907
01:01:11,530 --> 01:01:16,180
being someone did not like chemistry i to someone who has now been a chemist

908
01:01:16,180 --> 01:01:17,820
for the rest of my life

909
01:01:17,880 --> 01:01:20,960
so this is another important point i like to make

910
01:01:20,960 --> 01:01:22,250
which is never say

911
01:01:22,260 --> 01:01:24,350
we need a subject

912
01:01:24,360 --> 01:01:27,570
because as i said i really not like chemistry

913
01:01:27,610 --> 01:01:32,400
and i became the doing chemistry for the rest of my life that happened to

914
01:01:32,420 --> 01:01:34,610
very careful what you say

915
01:01:34,610 --> 01:01:40,680
you may discover later that that's really what you want to be do it

916
01:01:40,720 --> 01:01:41,500
all right

917
01:01:42,880 --> 01:01:46,490
what are we going to do in the second half of the course

918
01:01:46,500 --> 01:01:52,720
so we're going to start out by finishing chemical equilibrium professor there started this selection

919
01:01:52,720 --> 01:01:54,460
from dynamic

920
01:01:54,500 --> 01:01:57,210
the chemical equilibrium introduced you

921
01:01:57,220 --> 01:02:00,030
the chemical equilibrium constant k

922
01:02:00,100 --> 01:02:03,080
so we're going to take it from there review

923
01:02:03,130 --> 01:02:05,990
what you cover on monday as well

924
01:02:06,030 --> 01:02:10,110
then we move into acid base equilibrium this is four

925
01:02:11,990 --> 01:02:14,930
and from there to oxidation reduction

926
01:02:14,970 --> 01:02:18,970
reactions and just from this connection between chemistry

927
01:02:18,990 --> 01:02:20,380
in biology

928
01:02:20,430 --> 01:02:25,570
enzymes are proteins in your body that catalyze reactions which allows to

929
01:02:25,610 --> 01:02:28,720
walk around breathe all those kind of things

930
01:02:28,730 --> 01:02:33,430
most of these enzymatic reactions involve either

931
01:02:33,440 --> 01:02:36,510
as these are catalyzed reaction

932
01:02:36,530 --> 01:02:39,110
four oxidation reaction

933
01:02:39,120 --> 01:02:43,620
so these are fundamental parts of living systems work

934
01:02:43,670 --> 01:02:46,130
then we cover transition metals

935
01:02:46,140 --> 01:02:51,190
and i'm going to explain during that part about medals in biology a lot of

936
01:02:51,190 --> 01:02:54,750
enzymes required transition metals for their activity

937
01:02:54,770 --> 01:02:57,100
so we'll talk a little bit about that

938
01:02:57,180 --> 01:03:01,600
and then we end with that's in terms of the connection with biology

939
01:03:01,640 --> 01:03:03,850
genetics rates of reaction

940
01:03:03,870 --> 01:03:08,540
what enzymes do is they work it's how help you know those reaction

941
01:03:08,550 --> 01:03:16,990
so all all these topics are fundamental chemistry topics they also applied to biology

942
01:03:17,070 --> 01:03:21,270
so it's quite the second course is quite differently

943
01:03:22,730 --> 01:03:29,490
in the first half of course go on the basic principles atomic theory orbitals bonding

944
01:03:29,650 --> 01:03:35,030
lewis structures one really fundamental things which thermodynamics which you need to know to be

945
01:03:35,030 --> 01:03:38,590
able to do the second half second happens is really more

946
01:03:40,020 --> 01:03:42,350
these basic principles

947
01:03:42,400 --> 01:03:44,900
and when you put this together

948
01:03:44,910 --> 01:03:48,870
get the fundamentals of chemistry that you need to go on and study

949
01:03:48,880 --> 01:03:56,470
chemistry organic chemistry inorganic biology life sciences the fundamental principles of chemistry which you get

950
01:03:56,470 --> 01:03:59,090
through this has list of course

951
01:03:59,100 --> 01:04:02,330
so it's one of the different transition i just like to

952
01:04:02,350 --> 01:04:03,910
what you know

953
01:04:04,120 --> 01:04:08,820
time it's going to be like the different cause you have a different structure

954
01:04:08,930 --> 01:04:14,460
and really different material will be what you were before is really more applications so

955
01:04:14,460 --> 01:04:18,470
it's to have a little bit different feel to it so don't be surprised by

956
01:04:20,650 --> 01:04:25,360
are there any questions also the technical aspect of course is the same

957
01:04:25,430 --> 01:04:29,930
lecture notes the with structure and everything

958
01:04:29,980 --> 01:04:32,370
like that that's going to be the same person

959
01:04:32,420 --> 01:04:34,140
calculators all

960
01:04:34,160 --> 01:04:36,250
the rules of course do not change

961
01:04:36,270 --> 01:04:38,850
but it's a little bit different materials

962
01:04:38,900 --> 01:04:40,000
are there

963
01:04:40,080 --> 01:04:41,780
any specific questions

964
01:04:41,800 --> 01:04:44,770
feel free to raise your hand try

965
01:04:45,470 --> 01:04:47,260
it is can help me i don't

966
01:04:47,270 --> 01:04:51,420
so we see that someone has a question

967
01:04:51,440 --> 01:04:54,050
in one extension usually know

968
01:04:54,100 --> 01:04:58,420
what's fast

969
01:04:58,440 --> 01:04:59,890
right so

970
01:04:59,900 --> 01:05:03,720
we're we're doing a fairly smooth transition from material

971
01:05:03,740 --> 01:05:05,650
on monday tuesday

972
01:05:05,670 --> 01:05:11,050
we're still in the chapter nine o'clock on chemical equilibrium

973
01:05:11,060 --> 01:05:15,240
and i just want to point out some things about the handouts received at the

974
01:05:15,240 --> 01:05:19,330
time and i have a chapter and page numbers available

975
01:05:19,350 --> 01:05:21,450
so if you want to

976
01:05:21,460 --> 01:05:25,670
where this material is in your book so always going to be listed at the

977
01:05:25,670 --> 01:05:27,300
top of the hand

978
01:05:27,350 --> 01:05:32,860
i also on each hand out of the top topics that are covered in canada

979
01:05:32,860 --> 01:05:38,510
or in lecture is just useful when you're going back and reviewing for the final

980
01:05:38,560 --> 01:05:42,920
the yes on this day we cover these topics so that's going to be listed

981
01:05:42,930 --> 01:05:45,180
at the top of each

982
01:05:45,190 --> 01:05:47,930
and as as professor sara told you in a

983
01:05:48,970 --> 01:05:50,560
is this lecture

984
01:05:51,590 --> 01:05:52,830
help you

985
01:05:54,070 --> 01:05:58,710
you don't have to scroll right all the time conference back in with more and

986
01:05:58,710 --> 01:06:00,050
write things

987
01:06:00,130 --> 01:06:04,820
like professor sara or something like sometimes when if you want to ask questions

988
01:06:04,900 --> 01:06:08,050
so there are something like that you know and so

989
01:06:09,510 --> 01:06:14,570
are available because there are many places are going to be wanting to write

990
01:06:14,680 --> 01:06:17,360
we provide the summaries to make it easier

991
01:06:17,410 --> 01:06:22,760
you don't have to scribble constantly is actually going on so that you can listen

992
01:06:22,860 --> 01:06:25,070
what's going on

993
01:06:25,120 --> 01:06:30,970
so that's pretty much the same what you experience course don't like lecture series and

994
01:06:31,010 --> 01:06:32,030
no put way

995
01:06:32,090 --> 01:06:34,570
he notes that you

996
01:06:34,570 --> 01:06:38,580
even if it is simple size very close to and there's no point introducing this

997
01:06:38,580 --> 01:06:44,200
was something operation because just introduce local violence OK and additional limestone so you don't

998
01:06:44,200 --> 01:06:49,310
want to do that so whenever it is sample size that you can monitor is

999
01:06:49,310 --> 01:06:54,450
essentially quite i just respond out of date and parties on your own you resample

1000
01:06:54,720 --> 01:07:00,770
when essentially you find the effective sample size is can become quite small below and

1001
01:07:00,770 --> 01:07:01,810
of the two

1002
01:07:01,830 --> 01:07:08,880
OK i'm not gonna i'm going to speak to you that but important thing is

1003
01:07:09,540 --> 01:07:15,700
despite the fact that you're using this with sampling the approximation of the state the

1004
01:07:16,260 --> 01:07:21,520
importance of the target distribution of interest but you also have an approximation of the

1005
01:07:21,520 --> 01:07:23,950
marginal likelihood of in terest OK

1006
01:07:24,150 --> 01:07:30,310
the only anything with this approximation is that despite these very complex was sampling operation

1007
01:07:30,310 --> 01:07:37,340
you're doing this estimate is unbiased so you can still do this week from c

1008
01:07:37,340 --> 01:07:44,190
or something or is it doesn't matter to estimate of the marginal likelihood is unbiased

1009
01:07:44,220 --> 01:07:49,520
so it may be seems to be like a complete detailed plans of that you

1010
01:07:49,530 --> 01:07:55,180
try to develop sophisticated algorithm later on the fact that this guy is is is

1011
01:07:55,220 --> 01:07:58,140
actually a very important part so

1012
01:07:58,150 --> 01:08:02,780
because my voice is weighted by

1013
01:08:02,800 --> 01:08:05,640
i'm just going to go through an example nothing else to

1014
01:08:05,810 --> 01:08:10,520
so let's go back to the you know remember the example discussed before two despite

1015
01:08:10,530 --> 01:08:15,160
the fact that sequential importance sampling doesn't work so

1016
01:08:15,190 --> 01:08:20,460
remember the example we had we had an email to aggressive policies for xk this

1017
01:08:20,460 --> 01:08:22,350
acceleration equation

1018
01:08:22,380 --> 01:08:27,740
and what i should in this case is that was are you will using the

1019
01:08:27,740 --> 01:08:29,310
prior distribution

1020
01:08:29,780 --> 01:08:35,930
essentially a clever approximation of the optimal distribution in both cases categories and was collapsing

1021
01:08:36,230 --> 01:08:41,530
in the sense that as n was increasing the effective sample size was going from

1022
01:08:41,770 --> 01:08:43,760
capital and to one

1023
01:08:43,780 --> 01:08:45,740
so now what we're going to do

1024
01:08:45,740 --> 01:08:47,200
which is going to do

1025
01:08:47,210 --> 01:08:50,900
this one fails on this one of the first

1026
01:08:53,520 --> 01:08:57,760
so what we're going to do

1027
01:08:57,800 --> 01:09:01,680
which is going to first

1028
01:09:01,750 --> 01:09:03,970
check the case

1029
01:09:04,000 --> 01:09:05,520
so this is the

1030
01:09:05,540 --> 01:09:08,930
the same algorithm as before except that now

1031
01:09:08,930 --> 01:09:14,630
whenever this is the is using is an important distribution something quite them because it

1032
01:09:14,640 --> 01:09:18,340
is the point of what is going to do now is that each time we

1033
01:09:18,340 --> 01:09:22,680
monitor effects on both sides on each time becoming bad

1034
01:09:22,730 --> 01:09:28,790
i'm using resampling step that is i'm getting rid of particles basically we've low weight

1035
01:09:28,790 --> 01:09:33,100
on them into playing particles with high weights so if you check know what's going

1036
01:09:35,180 --> 01:09:40,950
the algorithm to this is the empirical measure of the sample particles we see that

1037
01:09:42,490 --> 01:09:44,830
these songs to is very

1038
01:09:44,900 --> 01:09:51,920
simple resampling i'd like to reason is still a bold to keep track basically of

1039
01:09:51,930 --> 01:09:59,960
the true sequence of mister distribution that is actually already is just straightforward modification still

1040
01:09:59,990 --> 01:10:05,480
so we can obstruct basically of the sequence of the distribution but if you look

1041
01:10:05,990 --> 01:10:11,220
a bit what's happening to the effective sample size you can see that basically

1042
01:10:11,240 --> 01:10:17,020
sometimes egoism actually some sunk kind of collapse because you see for example at stage

1043
01:10:17,020 --> 01:10:19,330
eight all

1044
01:10:19,340 --> 01:10:25,130
in part of the solution was extremely bad and effective sample size at this stage

1045
01:10:25,170 --> 01:10:31,580
basically collapsed one which means that essentially guide or is completely cosh actually but never

1046
01:10:31,580 --> 01:10:37,260
mind found the resampling stage we were able to kind of course on we managed

1047
01:10:37,260 --> 01:10:43,000
to basically come back to keep track of the tall postal distribution

1048
01:10:48,860 --> 01:10:53,330
it may be delayed by one or something like that because it's a bit suspicious

1049
01:10:54,570 --> 01:11:01,240
it's true actually the remark

1050
01:11:01,250 --> 01:11:05,240
that must be delayed by one degree

1051
01:11:05,320 --> 01:11:08,740
so now let's have a look at

1052
01:11:10,110 --> 01:11:16,740
sequential importance sampling resampling algorithm where in this case instead of trying to using the

1053
01:11:17,130 --> 01:11:22,970
sampling distribution instead of using just prior distribution is involved in something distributions i'm using

1054
01:11:22,970 --> 01:11:24,500
these approximations

1055
01:11:24,550 --> 01:11:28,160
of the optimal importance distribution

1056
01:11:28,170 --> 01:11:34,190
we see that the algorithm still managed to keep track also of the second episode

1057
01:11:35,500 --> 01:11:39,800
and in particular if you look at the effective sample size which is the really

1058
01:11:39,800 --> 01:11:43,010
much better is that we see the effective sample size doesn't go

1059
01:11:43,520 --> 01:11:45,740
really really low

1060
01:11:45,780 --> 01:11:50,650
basically so it seems that it means in the sense that in agrees and they

1061
01:11:50,650 --> 01:11:57,150
leave a better approximation of the posterior distribution for given number of samples on that

1062
01:11:57,150 --> 01:11:59,390
is that you have to be careful because

1063
01:11:59,400 --> 01:12:01,280
it could be that actually

1064
01:12:01,300 --> 01:12:04,040
sampling from the basically

1065
01:12:04,050 --> 01:12:10,460
approximation of the human body distribution is very time consuming so maybe for and computational

1066
01:12:10,460 --> 01:12:15,890
complexity is better to increase the number of particles onto sampling from the point but

1067
01:12:15,890 --> 01:12:20,850
these things completely time this problem dependent so i don't have forty any i cannot

1068
01:12:20,850 --> 01:12:28,410
give you any general recommendations thing is like

1069
01:12:32,670 --> 01:12:39,630
so now i just wanted you to finish today by an important remark actually

1070
01:12:41,450 --> 01:12:42,860
from the very beginning

1071
01:12:43,000 --> 01:12:45,960
this morning we discuss

1072
01:12:45,980 --> 01:12:50,980
SMC methods in the context of in the framework of hmm OK

1073
01:12:50,980 --> 01:12:52,850
basically remember that we

1074
01:12:52,850 --> 01:12:54,120
so far

1075
01:12:54,310 --> 01:12:58,250
all the bayesian networks and markov random fields we end up with

1076
01:13:00,380 --> 01:13:02,380
we end up with the factorisation

1077
01:13:02,510 --> 01:13:05,490
and now the question is

1078
01:13:07,570 --> 01:13:11,180
should probably wait a bit until everyone is

1079
01:13:11,230 --> 01:13:12,460
it's here

1080
01:13:12,570 --> 01:13:13,250
i mean

1081
01:13:13,270 --> 01:13:17,450
there are some parrots outside people are excited about those things

1082
01:13:17,480 --> 01:13:20,180
i told them to bring inside

1083
01:13:20,200 --> 01:13:24,480
i think they have been bassett to bring the parents inside

1084
01:13:24,490 --> 01:13:27,730
whatever you call those animals

1085
01:13:27,740 --> 01:13:33,770
OK so certainly municipal graphical models are not as interesting as a

1086
01:13:33,770 --> 01:13:36,770
we can do much about

1087
01:13:37,730 --> 01:13:42,880
so well at least they have some green and red star

1088
01:13:50,160 --> 01:13:56,380
basically what we really saw so far in terms of practical terms was followed

1089
01:13:56,390 --> 01:14:00,920
a lot of all theory and laudable definitions and concepts

1090
01:14:01,190 --> 01:14:03,030
about graphical models

1091
01:14:03,910 --> 01:14:07,440
from the point of view of practice

1092
01:14:07,480 --> 01:14:08,980
what we saw

1093
01:14:09,000 --> 01:14:10,810
it was basically

1094
01:14:10,810 --> 01:14:16,330
of the fact that we have a factorisation of the joint probabilities for the question

1095
01:14:16,330 --> 01:14:20,160
is how do we actually compute things with this

1096
01:14:21,560 --> 01:14:26,230
representations of the joint probability distribution that's the question

1097
01:14:26,250 --> 01:14:28,330
if you want approach

1098
01:14:29,220 --> 01:14:34,950
so again i talking about distributive law fields and we think about it once again

1099
01:14:35,000 --> 01:14:36,340
i mean

1100
01:14:36,360 --> 01:14:37,500
what is being

1101
01:14:37,530 --> 01:14:41,280
what we are going to do here in in order to exploit

1102
01:14:41,300 --> 01:14:43,810
the structure

1103
01:14:43,810 --> 01:14:46,890
of the probability space that arises

1104
01:14:48,220 --> 01:14:51,200
the conditional independence statements

1105
01:14:51,220 --> 01:14:53,140
it will be the distributive law

1106
01:14:53,200 --> 01:14:55,880
and this is basically

1107
01:14:55,890 --> 01:14:58,390
the tool for performing

1108
01:14:58,440 --> 01:15:00,210
exact probabilistic

1109
01:15:01,280 --> 01:15:03,060
in a given probabilistic model

1110
01:15:05,240 --> 01:15:10,940
so in their essences here i mentioned already basically if you

1111
01:15:10,970 --> 01:15:13,130
use a common factor

1112
01:15:13,160 --> 01:15:15,410
in both terms of some

1113
01:15:15,430 --> 01:15:17,030
you're busy losing

1114
01:15:23,820 --> 01:15:28,060
basically you have three operations here the left hand side into operation the right hand

1115
01:15:28,060 --> 01:15:29,910
side actually

1116
01:15:29,910 --> 01:15:33,310
give you the same result

1117
01:15:33,820 --> 01:15:37,280
so you don't want to do the left hand side with the right hand side

1118
01:15:37,310 --> 01:15:42,820
so you want to put all the common factors in all the firms outside

1119
01:15:43,530 --> 01:15:44,880
performed the song

1120
01:15:46,250 --> 01:15:50,440
and this is actually the basic idea of exact inference in graphical models will just

1121
01:15:50,440 --> 01:15:52,840
instantiate general principle

1122
01:15:52,850 --> 01:15:56,060
in particular type article models and see how it works

1123
01:15:57,750 --> 01:15:58,910
so basically

1124
01:15:58,930 --> 01:16:03,690
for example consider and compute the marginal probability

1125
01:16:03,740 --> 01:16:05,090
p of x one

1126
01:16:05,100 --> 01:16:07,970
for a given microphone feel

1127
01:16:08,090 --> 01:16:12,660
remember that in the market field we have a normalisation constant

1128
01:16:12,780 --> 01:16:14,260
xe here

1129
01:16:14,310 --> 01:16:15,840
and have a product

1130
01:16:15,870 --> 01:16:20,210
over arbitrary functions of the maximal clique

1131
01:16:20,250 --> 01:16:25,090
in this particular case is probably takes this four i'm assuming given

1132
01:16:25,150 --> 01:16:26,720
graph in my mind

1133
01:16:29,120 --> 01:16:30,530
psi x

1134
01:16:30,530 --> 01:16:32,620
i an exit one

1135
01:16:32,680 --> 01:16:35,120
from i corps one two n minus one

1136
01:16:35,120 --> 01:16:37,530
so who would be able to tell me

1137
01:16:37,570 --> 01:16:40,810
which graph is this

1138
01:16:40,820 --> 01:16:42,620
so again

1139
01:16:42,660 --> 01:16:49,590
now each such suchanek yes such a so basically you have changed here OK

1140
01:16:49,590 --> 01:16:51,540
what you're utility from having the

1141
01:16:51,570 --> 01:16:55,900
if you have ten links may be getting one more than seventy five hundred forty

1142
01:16:55,900 --> 01:16:58,250
five two hundred and get them more

1143
01:16:58,250 --> 01:17:00,900
that we like overflow even what as your

1144
01:17:00,920 --> 01:17:04,890
so the fuel offload now right so the question is how much utility

1145
01:17:05,110 --> 01:17:08,980
get from from links and i think if we could get some handle on that

1146
01:17:08,980 --> 01:17:10,110
then it would be

1147
01:17:10,170 --> 01:17:13,340
they couldn't actually answer questions you asking missing

1148
01:17:13,380 --> 01:17:17,630
then you could define see what is the utility people are getting and then that

1149
01:17:17,630 --> 01:17:20,050
would be very nice to connect to

1150
01:17:20,070 --> 01:17:21,250
two adoption

1151
01:17:21,270 --> 01:17:24,130
so the way people creating links and so on

1152
01:17:24,670 --> 01:17:27,710
i think i think it's it's so it's a good point and i think the

1153
01:17:27,710 --> 01:17:29,320
only sort of

1154
01:17:29,380 --> 01:17:30,290
the the

1155
01:17:30,310 --> 01:17:32,840
the basically the work of

1156
01:17:32,900 --> 01:17:37,060
fabrikant papadimitriou is i think that the only one with the some kind of optimisation

1157
01:17:37,060 --> 01:17:40,150
going right when they say you know you want to create things but you don't

1158
01:17:40,150 --> 01:17:41,630
want them to be too long

1159
01:17:41,630 --> 01:17:45,690
and then power was much much

1160
01:17:45,710 --> 01:17:48,730
and i think there is a lot of connections to like

1161
01:17:48,790 --> 01:17:50,860
did here and so on

1162
01:17:50,880 --> 01:17:52,820
how to design

1163
01:17:52,840 --> 01:17:55,020
but i think it's a good point

1164
01:18:03,040 --> 01:18:05,070
the reason of

1165
01:18:05,090 --> 01:18:10,360
i think that's

1166
01:18:43,110 --> 01:18:47,820
it's it's called

1167
01:18:52,770 --> 01:18:57,040
actually we reject both so it's true what you're saying that if you start collecting

1168
01:18:57,040 --> 01:19:00,340
the citation network you will have this problem of you know taking up paper not

1169
01:19:00,340 --> 01:19:05,040
having sort of the he's right so have to start collecting one then actually checking

1170
01:19:05,040 --> 01:19:05,980
for that

1171
01:19:06,040 --> 01:19:10,920
and our conclusions they don't change and we also have networks for example

1172
01:19:10,940 --> 01:19:15,590
the networks social dislike linking and so on there computer networks that we know all

1173
01:19:15,590 --> 01:19:18,050
the edges that happened next densify

1174
01:19:18,980 --> 01:19:24,540
it seems densification is that for two uncertain about paper specifically we also just measure

1175
01:19:24,650 --> 01:19:29,650
was the average number of citations of paper has that also increases

1176
01:19:29,670 --> 01:19:34,320
so yeah i would reference lists getting

1177
01:19:45,300 --> 01:19:50,730
sure sure

1178
01:20:09,670 --> 01:20:12,320
on her

1179
01:20:12,340 --> 01:20:17,230
are like so many papers

1180
01:20:40,880 --> 01:21:01,230
it's been three years so i don't have a good for medium

1181
01:21:01,250 --> 01:21:05,730
we definitely checked like the everything that was going on and you should check medium

1182
01:21:05,730 --> 01:21:08,570
so this ability tell us is it like

1183
01:21:08,590 --> 01:21:11,520
you know is it on the average in some sense or is it like you

1184
01:21:12,150 --> 01:21:15,400
the data i steinwart papers

1185
01:21:15,550 --> 01:21:17,900
and then

1186
01:21:19,650 --> 01:21:21,320
four robustness

1187
01:21:21,340 --> 01:21:24,360
so what you know about the is that you know if you're going to remove

1188
01:21:24,380 --> 01:21:28,400
animals from the network then you really need to remove a lot to change things

1189
01:21:28,440 --> 01:21:30,730
but if you're going to remove high degree ones

1190
01:21:30,770 --> 01:21:32,690
you really change things alone

1191
01:21:33,590 --> 01:21:37,360
it seems to be like a universal property of networks and

1192
01:21:37,380 --> 01:21:41,190
now of course the question is how many emotions people like then that would be

1193
01:21:41,190 --> 01:21:43,000
doing that to destroy

1194
01:21:44,210 --> 01:21:45,650
i don't know if i have a

1195
01:21:45,670 --> 01:21:46,810
the answer

1196
01:21:46,820 --> 01:21:50,130
i mean in general we you know that going mess up with high degree people

1197
01:21:50,130 --> 01:21:52,520
that that for whatever reason there

1198
01:21:52,590 --> 01:21:57,070
that would mess of the network along

1199
01:21:57,070 --> 01:22:00,590
and that's probably the reason because we have a skewed distributions and then this and

1200
01:22:00,590 --> 01:22:04,000
this sort of extreme people are much more extreme than they've ever been

1201
01:22:04,000 --> 01:22:05,020
if we look can

1202
01:22:05,040 --> 01:22:12,650
if we can find gulshan distribution

1203
01:22:31,770 --> 01:22:34,250
here a

1204
01:23:25,530 --> 01:23:43,310
i mean this as i'm already excited now because sort of we are able to

1205
01:23:43,690 --> 01:23:47,780
two sort of modern humans as galaxies right or we see that even humans are

1206
01:23:47,790 --> 01:23:51,020
no different from galaxies in some sense which is sort of fun right

1207
01:23:51,410 --> 01:23:53,140
on the other hand

1208
01:23:53,530 --> 01:23:59,570
i don't think you that but hopefully he will be some questions also what statistics

1209
01:23:59,620 --> 01:24:03,630
does one they can use sort of because also

1210
01:24:03,690 --> 01:24:05,980
for example one problem today is like i showed you

1211
01:24:05,990 --> 01:24:06,780
i showed you case

1212
01:24:06,790 --> 01:24:10,650
two months my talk of the same thing so depending on what

1213
01:24:10,650 --> 01:24:14,870
but what granularity you want to model which is a different model so

1214
01:24:14,880 --> 01:24:17,440
we still don't have you no one working with one would

1215
01:24:17,450 --> 01:24:22,340
do everything correctly i sort of individual edge level and as fight on one long

1216
01:24:22,340 --> 01:24:26,850
enough and i think this global statistic everything with way so one thing that we

1217
01:24:26,850 --> 01:24:28,150
are doing this

1218
01:24:28,330 --> 01:24:32,360
getting this to sort of views together and trying to do everything by saying one

1219
01:24:33,700 --> 01:24:35,710
i don't know

1220
01:24:35,730 --> 01:24:41,890
i don't know if i can say more

1221
01:24:41,910 --> 01:24:46,310
so one

1222
01:24:48,210 --> 01:24:59,700
this this

1223
01:25:13,380 --> 01:25:19,650
harmon when i compare statistics

1224
01:25:19,660 --> 01:25:23,660
no no i mean i haven't done because i'm not true

1225
01:25:23,660 --> 01:25:28,440
so so what i know like i can prove that distributions we have this particular

1226
01:25:28,440 --> 01:25:33,880
parametric forms right what what this proves don't value is how these distributions are correlated

1227
01:25:33,880 --> 01:25:37,460
in some sense i can say for some value of parameters i can get

1228
01:25:37,460 --> 01:25:39,840
at the end of a distinguished career

1229
01:25:39,850 --> 01:25:40,980
in russia

1230
01:25:41,040 --> 01:25:45,190
whereas i was a teenager i was in high school when i had the idea

1231
01:25:45,200 --> 01:25:46,520
and i wrote my

1232
01:25:46,540 --> 01:25:50,030
first paper on this one i fifteen

1233
01:25:50,040 --> 01:25:52,710
when i was in high school and i wrote my first one paper on this

1234
01:25:52,710 --> 01:25:54,570
subject when i was eighteen

1235
01:25:54,580 --> 01:25:56,960
and half of it was published when i was nineteen in the journal of the

1236
01:25:56,960 --> 01:26:00,710
ACM and the other half was published a few years later in the gymnasium the

1237
01:26:00,710 --> 01:26:03,590
paper was only split into two halves

1238
01:26:05,400 --> 01:26:09,410
OK so the question the kolmogorov and i were most interested in was how to

1239
01:26:09,410 --> 01:26:14,180
define lack of structure or random you know some kind of local latin absolute lack

1240
01:26:14,180 --> 01:26:16,980
of any pattern structure in a string of bits

1241
01:26:17,000 --> 01:26:18,540
in fact in both cases

1242
01:26:18,960 --> 01:26:22,520
solomonoff was interested in induction in predicting the future

1243
01:26:23,750 --> 01:26:28,790
and that was not the main concentration the comoros and i had

1244
01:26:32,730 --> 01:26:34,570
how what i compare

1245
01:26:34,650 --> 01:26:40,250
this work will come over the maybe one or two small papers proposing this definition

1246
01:26:40,870 --> 01:26:46,270
and basically he was the end of his career and i was just beginning

1247
01:26:46,320 --> 01:26:49,560
so how would i compare

1248
01:26:50,460 --> 01:26:52,230
our work in this area

1249
01:26:53,330 --> 01:26:58,780
kolmogorov can be you know considering that the eighty had i would say it's remarkable

1250
01:26:58,780 --> 01:27:00,780
that he came up with the idea that all

1251
01:27:02,930 --> 01:27:06,070
you know as you get older you tend to get more conservative

1252
01:27:06,080 --> 01:27:07,830
so so

1253
01:27:07,850 --> 01:27:14,280
but the original version of program size complexity that he and i proposed and also

1254
01:27:14,280 --> 01:27:15,650
so of

1255
01:27:15,700 --> 01:27:19,930
proposed to send the same idea the size of the smallest program

1256
01:27:19,940 --> 01:27:21,630
i was wrong

1257
01:27:21,640 --> 01:27:26,230
it was actually wrong and i'm going to discuss that in a and part b

1258
01:27:26,250 --> 01:27:28,410
the question is self-limiting programs

1259
01:27:28,430 --> 01:27:31,460
so so on the one hand the original definition is wrong

1260
01:27:31,470 --> 01:27:35,720
and ten years later i changed the definitions the whole theory the theory was good

1261
01:27:35,720 --> 01:27:37,200
first ten

1262
01:27:37,250 --> 01:27:41,640
this is common and i came up with essentially the same definition

1263
01:27:41,650 --> 01:27:45,810
about that but we were both wrong so i'll tell you what the mistake was

1264
01:27:45,810 --> 01:27:47,570
and it only took ten years

1265
01:27:47,590 --> 01:27:52,040
in my case to get it right and come off i think was the

1266
01:27:52,060 --> 01:27:56,020
almost at that point so he did not didn't realize the definition was the wrong

1267
01:28:00,210 --> 01:28:04,450
second of all there is the question of what to apply this to

1268
01:28:04,460 --> 01:28:06,960
and as you know

1269
01:28:06,970 --> 01:28:09,320
i'm not interested in practical applications

1270
01:28:09,370 --> 01:28:11,390
but i'm interested in

1271
01:28:11,410 --> 01:28:13,330
in theoretical applications

1272
01:28:13,330 --> 01:28:14,980
and the application

1273
01:28:14,990 --> 01:28:17,630
the reason i came up with this idea

1274
01:28:17,690 --> 01:28:22,800
it is because my basic interest was incompleteness the basic question that fascinated me

1275
01:28:22,820 --> 01:28:26,210
i was getting competence

1276
01:28:26,230 --> 01:28:29,620
and what interested me was the question of

1277
01:28:29,630 --> 01:28:33,350
you know when you can see any structure in the world of pure math is

1278
01:28:33,350 --> 01:28:36,980
it's your fault or there is no structure

1279
01:28:36,990 --> 01:28:37,750
you see

1280
01:28:37,760 --> 01:28:41,990
because the good old talks about cases where there is no way to prove something

1281
01:28:42,000 --> 01:28:43,090
is true

1282
01:28:43,100 --> 01:28:46,700
so the question is if we can prove something

1283
01:28:46,700 --> 01:28:51,530
the way i do it i i i'm not a physicist but i studied a

1284
01:28:51,530 --> 01:28:53,340
lot of physics i love physics

1285
01:28:53,350 --> 01:28:56,070
i'm a mathematician mathematician but

1286
01:28:56,080 --> 01:28:58,060
but i read a lot of physics

1287
01:28:58,070 --> 01:29:02,090
and you know in physics you talk about the notion of randomness was discussed a

1288
01:29:02,090 --> 01:29:06,550
great deal in the nineteen twenties when quantum mechanics was developed

1289
01:29:06,590 --> 01:29:10,810
and when i was a schoolboy in this with the nineteen fifties

1290
01:29:10,830 --> 01:29:12,690
of the nineteen sixties

1291
01:29:12,700 --> 01:29:13,930
you know people

1292
01:29:14,810 --> 01:29:20,880
creators of quantum mechanics people like max born and schrodinger and einstein and bore was

1293
01:29:20,880 --> 01:29:21,970
still alive

1294
01:29:21,990 --> 01:29:24,620
and they were still discussing

1295
01:29:24,630 --> 01:29:28,820
they were still arguing over whether god plays dice in the physical universe you see

1296
01:29:28,820 --> 01:29:31,740
because the schrodinger equation is a probabilistic equation

1297
01:29:31,810 --> 01:29:38,820
quantum mechanics statistical physics you see in this other einstein for example is not deterministic

1298
01:29:38,820 --> 01:29:40,000
view of the world

1299
01:29:40,190 --> 01:29:45,720
so there now we've forgotten maybe about all of this controversy but but when i

1300
01:29:45,720 --> 01:29:46,990
when i was

1301
01:29:47,000 --> 01:29:48,990
as a student

1302
01:29:49,000 --> 01:29:52,210
the the people who worried about this was still alive

1303
01:29:52,220 --> 01:29:56,010
and also the echoes of that controversy which basically was from the nineteen twenties and

1304
01:29:56,870 --> 01:30:02,580
you you you know this this was still in people's minds

1305
01:30:04,660 --> 01:30:08,620
my idea you see the question we get all the fundamental problem

1306
01:30:08,630 --> 01:30:13,700
with kernel get show that there are mathematical assertions in arithmetic having to do with

1307
01:30:13,700 --> 01:30:18,450
zero one two three four five addition and multiplication he showed that their mathematical assertions

1308
01:30:18,450 --> 01:30:19,870
which are unprovable

1309
01:30:20,870 --> 01:30:22,470
that are true but unprovable

1310
01:30:22,480 --> 01:30:26,220
now unprovable depends on the formal axiomatic theory you're using in in fact he was

1311
01:30:26,220 --> 01:30:28,670
have theories have different composition of red and white

1312
01:30:29,490 --> 01:30:33,280
ships and then proportions which one is more likely what's the relative probability that the

1313
01:30:33,290 --> 01:30:35,740
standard data and it turns out people are the kinds of things

1314
01:30:36,890 --> 01:30:40,990
you give them just a few statistics that they could plug in today's rule in a very elementary way

1315
01:30:41,410 --> 01:30:43,670
and the average person that often doesn't know what to do with it

1316
01:30:44,120 --> 01:30:44,690
and this

1317
01:30:45,200 --> 01:30:46,630
crept into the popular r

1318
01:30:46,890 --> 01:30:49,010
press and all sorts of ways and you know maybe the young

1319
01:30:49,490 --> 01:30:53,550
the most famous was one stephen jay gould's most famous essays again this is what

1320
01:30:53,550 --> 01:30:57,590
people are reading when i feel saying our minds are not built for whatever reason

1321
01:30:57,590 --> 01:31:01,280
to work by the rules of probability but people who who came with age when

1322
01:31:01,280 --> 01:31:04,880
i did you know they saw that not just the workaday prose doing all the

1323
01:31:05,350 --> 01:31:10,590
probabilistic revolution in machine learning and i i just thought you know that's that's crazy clearly this

1324
01:31:11,020 --> 01:31:16,080
this this is the strong candidates for understanding something about intelligence so that the beginning

1325
01:31:16,080 --> 01:31:19,910
of this research programme was for list was work that i did together with tom

1326
01:31:19,910 --> 01:31:24,310
griffiths where we took a number of basic cognitive capacities and just try to see

1327
01:31:24,310 --> 01:31:29,320
how could we understand these as basic forms into the statistics typically bayesian statistics where

1328
01:31:29,320 --> 01:31:30,040
there's some kind of

1329
01:31:30,700 --> 01:31:34,410
key role being played by some prior knowledge that we can formalize and simple kind

1330
01:31:34,420 --> 01:31:37,750
of textbook-style statistical distributions of show you just one example of this

1331
01:31:38,220 --> 01:31:41,270
because it's it's one that anybody who study bayesian statistics

1332
01:31:41,960 --> 01:31:45,550
you might recognise and it sort of sets up the logic you might want to

1333
01:31:45,850 --> 01:31:50,010
obtain foreign and raises the challenge for how do we get more structured kinds abstract

1334
01:31:50,870 --> 01:31:53,470
so here a bunch of problems that all have a similar form

1335
01:31:54,240 --> 01:31:56,960
you read about a movie that has made sixty million dollars to date how much

1336
01:31:56,960 --> 01:31:57,960
money will it make in total

1337
01:31:58,380 --> 01:32:01,060
you see that something has been baking in the oven for thirty four minutes how

1338
01:32:01,060 --> 01:32:04,050
long until it's ready you meet someone who is seventy eight years old how long

1339
01:32:04,050 --> 01:32:04,560
where they lived

1340
01:32:05,110 --> 01:32:08,410
your friend quotes to you from line seventeen of his favourite poem how long is the poem

1341
01:32:08,950 --> 01:32:12,220
are you need the ust congressman who served for eleven years how long will he serve in total

1342
01:32:12,640 --> 01:32:14,900
so each of these is what we call an everyday prediction

1343
01:32:16,070 --> 01:32:16,790
four there's some

1344
01:32:17,170 --> 01:32:18,460
quantity event duration

1345
01:32:19,450 --> 01:32:20,430
will represent their with tea

1346
01:32:20,850 --> 01:32:24,270
that's u i would like to make inference about but you don't deserve

1347
01:32:24,760 --> 01:32:29,380
this you just observe some sample the state it was sorry and here's my notation

1348
01:32:29,450 --> 01:32:34,560
teetotal is represents the unknown total extent and duration and one of these phenomena and

1349
01:32:34,560 --> 01:32:36,330
you observe one data point tea

1350
01:32:36,830 --> 01:32:39,710
all you know is that it's a random sample between zero and teetotal and you

1351
01:32:39,710 --> 01:32:43,800
have to make a guess teetotal now what's nice about these problems is that these

1352
01:32:43,930 --> 01:32:47,940
different classes everyday events we can go out with publicly available data come up with

1353
01:32:48,140 --> 01:32:48,990
the right prior

1354
01:32:49,530 --> 01:32:52,410
the those are shown along the top you know what really goes through the specifics

1355
01:32:52,610 --> 01:32:57,220
but free to these different domains movie grosses poems and so on lifespans you can

1356
01:32:57,220 --> 01:32:58,200
compute the empirical

1357
01:32:58,630 --> 01:33:00,910
distribution on on how long these things last

1358
01:33:01,400 --> 01:33:05,090
and then you can now do bayesian inference you can condition this prior on one observation

1359
01:33:05,470 --> 01:33:09,340
and then make a guess the particular estimated that reusing is what's called the posterior

1360
01:33:09,340 --> 01:33:14,120
median so updating this this prior with one sample that we know is all we

1361
01:33:14,120 --> 01:33:16,020
know is that between zero end

1362
01:33:16,470 --> 01:33:19,150
and the team that's the prior on two to total

1363
01:33:20,110 --> 01:33:24,000
and then we take our posterior distribution we look at the median so that's our

1364
01:33:24,000 --> 01:33:26,170
best guess how long this thing is going to last

1365
01:33:26,860 --> 01:33:30,820
and what you can see on the bottom are plots of the posterior median predictor

1366
01:33:31,930 --> 01:33:34,360
four fourteen for the empirical priors there

1367
01:33:35,280 --> 01:33:39,770
and they plotted against people's judgments four five five different data points that be

1368
01:33:40,200 --> 01:33:46,380
the x axis is represents the value that people were given the why axis is the median people's guesses on

1369
01:33:46,840 --> 01:33:49,910
you know i need to these problems given one of those inputs so

1370
01:33:50,350 --> 01:33:54,070
for just to be very concrete that's means like some subjects were told about the

1371
01:33:54,080 --> 01:33:57,420
movie that made sixty million dollars but others were told about when amazing thirty million

1372
01:33:57,420 --> 01:34:01,010
dollars or a hundred billion dollars and there are five different values across subjects for

1373
01:34:01,010 --> 01:34:02,520
each of these different questions

1374
01:34:03,180 --> 01:34:07,060
and all i want you to take out from this is that's people are extremely

1375
01:34:07,060 --> 01:34:10,990
close to the bayesian predictions there's a sense here which people are sort of w

1376
01:34:10,990 --> 01:34:15,800
optimal the optimal in the sense that what's the mapping from the top to the

1377
01:34:15,800 --> 01:34:19,820
bottom of the prior to posterior predictive is the optimal bayesian prediction

1378
01:34:20,520 --> 01:34:24,760
and people seem to match that's but notice how these the qualitative form of the

1379
01:34:24,760 --> 01:34:28,580
optimal bayesian prediction varies across each of these domains because the priors have different forms

1380
01:34:28,580 --> 01:34:31,210
that shows up in the past year prediction people seem to get it's as if

1381
01:34:31,450 --> 01:34:35,660
they're doing the right mapping from prior to posterior with the right prior to enter

1382
01:34:35,660 --> 01:34:36,630
the structure of the domain

1383
01:34:37,310 --> 01:34:42,080
so this another kind very basic quantitative successes convinced us and others take the idea

1384
01:34:42,290 --> 01:34:45,110
of observed the probabilistic inference framework for

1385
01:34:45,570 --> 01:34:46,930
cognition seriously

1386
01:34:47,610 --> 01:34:50,580
but how do you get this disk these hard problems i'm saying we really want

1387
01:34:50,580 --> 01:34:52,140
to solve like the problem of learning

1388
01:34:52,820 --> 01:34:57,640
a label for objects from a few examples are intuitive reasoning about physical scenes are

1389
01:34:57,890 --> 01:35:02,490
intentional agents interactions you it's not even clear what is the right prior hypothesis space

1390
01:35:02,580 --> 01:35:06,530
how you can measure it process scientist let alone for human learners who are trying

1391
01:35:06,530 --> 01:35:10,760
to build up this knowledge so this leads us to to turn toward some kind

1392
01:35:10,760 --> 01:35:14,690
of more expressive knowledge representation language something that isn't just you know

1393
01:35:15,570 --> 01:35:18,080
exponential family distribution you can get for bayesian stats

1394
01:35:18,790 --> 01:35:20,470
and you know like many people in

1395
01:35:20,970 --> 01:35:25,610
any i was it heavily influenced by the work of a not just probe and

1396
01:35:25,610 --> 01:35:28,050
many others in the traditional policy graphical models

1397
01:35:28,590 --> 01:35:33,170
bayes nets directed graphical models seem particularly appealing because so much of human cognition so

1398
01:35:33,170 --> 01:35:34,540
much of what i'm talking about here

1399
01:35:34,990 --> 01:35:36,190
is essentially causal

1400
01:35:37,030 --> 01:35:39,740
but it more generally the toolkit graphical models

1401
01:35:40,140 --> 01:35:43,680
has been very influential not only in any i but according to model as for

1402
01:35:43,680 --> 01:35:46,670
similar reasons it gives you two main things that gives you eight

1403
01:35:47,120 --> 01:35:52,490
eight general-purpose knowledge representation language right away to represent the cut this the structure of some complex domain

1404
01:35:53,070 --> 01:35:56,090
and it gives a general tools for doing inference saying you know all you have

1405
01:35:56,090 --> 01:35:59,820
to do with the model as described the causal structure here using bayes net

1406
01:36:00,420 --> 01:36:04,840
and just these sort of direct causal dependencies that's the errors in the graph and

1407
01:36:04,840 --> 01:36:08,840
then the the tools and bayes nets give you in principle and working tools that

1408
01:36:08,840 --> 01:36:09,440
people are built

1409
01:36:10,010 --> 01:36:14,800
and ability to do arbitrary conditional inferences to observe some variables and make inferences about

1410
01:36:14,800 --> 01:36:18,860
others right so in the famous q mark network air you might observe symptoms and

1411
01:36:18,860 --> 01:36:23,090
make inferences about diseases you might then make predictions about other symptoms the results medical

1412
01:36:23,090 --> 01:36:23,810
tests you might see

1413
01:36:24,350 --> 01:36:26,290
and so on this is again very familiar

1414
01:36:27,660 --> 01:36:32,220
and something basic you i think some there's something basically right about some kind of

1415
01:36:32,420 --> 01:36:37,050
probabilistic also representation language is the way to think about common sense but what's missing

1416
01:36:37,050 --> 01:36:37,440
from this

1417
01:36:39,290 --> 01:36:43,320
and this is what leads to these these kinds of representations we call probabilistic programs

1418
01:36:43,350 --> 01:36:45,200
basically it's the it's the idea that a graph

1419
01:36:45,770 --> 01:36:48,560
is too impoverished representation too simple

1420
01:36:50,760 --> 01:36:54,660
describe because the kind of rich knowledge that humans have about the causal structure of

1421
01:36:54,670 --> 01:36:59,110
the world we need some kind of more powerful probabilistic language some more prop powerful

1422
01:36:59,110 --> 01:37:03,450
way of describing the probabilistic causal structure in the world so think about this kind

1423
01:37:03,460 --> 01:37:06,970
this workshop seen that i've shown a few times right how can you take an

1424
01:37:06,970 --> 01:37:10,110
image and make an inference inference about not just what's wear

1425
01:37:11,260 --> 01:37:14,560
what what supporting what would happen you know if you if you move the table

1426
01:37:14,560 --> 01:37:15,800
you know that those things would fall

1427
01:37:16,740 --> 01:37:19,650
you can see there's a maybe you can see over there on the lower right

1428
01:37:19,650 --> 01:37:23,880
there's a tyre leaning up against the table you can probably make the inference that

1429
01:37:23,880 --> 01:37:26,770
tyres a little more precarious than say one of those

1430
01:37:27,770 --> 01:37:30,930
cups right there very solidly on the top of the table so a slight bump

1431
01:37:30,930 --> 01:37:33,690
might make table might make the tyre fall that won't affect the cups on

1432
01:37:34,460 --> 01:37:35,340
how can you do this

1433
01:37:36,080 --> 01:37:38,690
well here's a way of thinking about it which in some sense looks like a

1434
01:37:38,690 --> 01:37:39,930
bayes net a way to capture this

1435
01:37:40,350 --> 01:37:44,620
intuitive knowledge here but but also i think draws attention to what's missing so we

1436
01:37:44,620 --> 01:37:48,540
have this have this picture here which describes the well i think it is roughly

1437
01:37:48,540 --> 01:37:51,160
the causal processes that are that are giving rise this data

1438
01:37:51,610 --> 01:37:54,820
there some kind of image that you observe and here i'm showing just a static

1439
01:37:54,820 --> 01:37:58,040
image but in general we could be looking at dynamic scenes like those movies showed

1440
01:37:58,720 --> 01:37:59,960
and that's image is

1441
01:38:00,360 --> 01:38:05,370
in some kind of output of graphics processor right graphics is that it let's say

1442
01:38:05,370 --> 01:38:12,060
dealing with this process of taking disease that story potentially vast array

1443
01:38:12,930 --> 01:38:19,020
acoustic parameters and using those as input to our feature detector

1444
01:38:19,070 --> 01:38:22,660
extracting a set of features for example these might be

1445
01:38:22,680 --> 01:38:32,020
for the multivalued set that attributed to king here we might have i guess eight

1446
01:38:32,020 --> 01:38:36,540
different features and each of these features can take on

1447
01:38:36,960 --> 01:38:40,650
the values that are listed here in the table

1448
01:38:40,670 --> 01:38:41,580
and so

1449
01:38:41,600 --> 01:38:46,800
i in that case i would have one any cause eight features and these could

1450
01:38:46,800 --> 01:38:52,990
take on these distinct values i may then perhaps as output at this stage i

1451
01:38:52,990 --> 01:38:58,820
would i might have perhaps the posterior probabilities associated with these features that would get

1452
01:38:58,820 --> 01:39:00,810
passed along two

1453
01:39:01,050 --> 01:39:06,610
this search phase where i might have a lexicon that is defined in terms of

1454
01:39:06,610 --> 01:39:14,180
feature distinctive feature bundles as we talked about earlier in the sort landmark based model

1455
01:39:14,200 --> 01:39:19,370
and we might have various other knowledge sources that we can represent in terms of

1456
01:39:19,370 --> 01:39:20,940
those qualities as well

1457
01:39:22,170 --> 01:39:25,560
something is one way of doing so

1458
01:39:27,580 --> 01:39:29,950
there again there's problems

1459
01:39:29,960 --> 01:39:32,320
and the biggest one here is

1460
01:39:32,330 --> 01:39:35,370
how do we see you know we're doing supervised training if we're going to train

1461
01:39:35,370 --> 01:39:40,470
the detector will we have to give examples that have labels if i'm going to

1462
01:39:40,470 --> 01:39:42,110
train detector for say

1463
01:39:43,450 --> 01:39:44,490
let's say

1464
01:39:44,510 --> 01:39:46,980
for the manner of articulation

1465
01:39:46,990 --> 01:39:51,710
i'm going to have to label my speech frames

1466
01:39:51,720 --> 01:39:55,710
with the proper

1467
01:39:55,770 --> 01:40:03,470
many manner for that that segment and assume that will serve as the truth

1468
01:40:03,490 --> 01:40:07,770
as i as i as a trainee systems

1469
01:40:08,630 --> 01:40:12,620
the fact that we really don't know these quantities and that in fact there are

1470
01:40:12,620 --> 01:40:16,370
hidden in all we can see is the acoustics is a big problem so

1471
01:40:16,390 --> 01:40:20,510
one thing we can do is we could we could say that we have the

1472
01:40:20,510 --> 01:40:26,060
feature transcriptions right we can say that we've got phonemes right we have let's say

1473
01:40:26,060 --> 01:40:30,330
we have had labelled phonemes will assume that we can expand each phonemes in each

1474
01:40:30,330 --> 01:40:36,140
phoneme into a list of features editor of all it's a it's you know like

1475
01:40:36,140 --> 01:40:39,930
i could go back to each of these this of all i i could tell

1476
01:40:40,580 --> 01:40:46,040
the what the manner of articulation is i could tell you whether it's low mid

1477
01:40:46,040 --> 01:40:47,370
or high vowel

1478
01:40:47,390 --> 01:40:52,080
we could say is voice phonation so basically i can look for each of my

1479
01:40:52,130 --> 01:40:54,910
phonemes i can look up in this table

1480
01:40:54,950 --> 01:41:00,530
and say OK here the the distinctive features is the correct values for this distinctive

1481
01:41:00,530 --> 01:41:04,500
features and i can then use those supervision and training

1482
01:41:04,510 --> 01:41:05,820
problem with that

1483
01:41:05,840 --> 01:41:10,580
it is the actual feature values are going to change in these canonical values and

1484
01:41:10,780 --> 01:41:15,420
really what we acquired if all we're going to do is mapped from the phonemes

1485
01:41:15,420 --> 01:41:21,720
to the features and and train these these detectors that we just in that case

1486
01:41:21,730 --> 01:41:26,160
coming up with just a different representation phonemes in fact that's not true but but

1487
01:41:26,160 --> 01:41:29,120
that's something to worry about the

1488
01:41:29,130 --> 01:41:35,640
the other possibility is let's uses direct physical measurements let's go to the the databases

1489
01:41:35,640 --> 01:41:44,600
where we've collected aligned acoustics and and and articulatory measurements let's just take those measurements

1490
01:41:44,600 --> 01:41:46,340
and use those as the truth

1491
01:41:46,360 --> 01:41:51,260
well first that it's a bit hard to come by will have people spitting out

1492
01:41:51,760 --> 01:41:56,840
you know the the coils in their their articulagraph

1493
01:41:56,860 --> 01:42:01,740
choking and before we get enough data and the other thing is that it can

1494
01:42:01,740 --> 01:42:09,500
be difficult to actually convert these sort of geometric physical measurements to to feature values

1495
01:42:10,400 --> 01:42:15,550
well i'm new writing the possibilities here and they all got their downsides the we

1496
01:42:15,620 --> 01:42:20,440
can actually go through and manually labeled distinctive features take an expert in go

1497
01:42:20,490 --> 01:42:27,100
decide whether or not there's voicing at this point separate from the question of whether

1498
01:42:27,100 --> 01:42:31,100
or not the place of articulation is where it's supposed to be

1499
01:42:31,120 --> 01:42:37,520
that's that's fine too but there's the issue of defining labeling scheme this repeatable that's

1500
01:42:37,520 --> 01:42:44,620
and and where we get agreement among labelers and that takes time actually carried of

1501
01:42:45,460 --> 01:42:51,490
i looked into that i guess with simon king as well at the works the

1502
01:42:51,490 --> 01:42:54,700
art workshop in two thousand six

1503
01:42:54,740 --> 01:42:58,580
and i think the labelled the whopping like eighty utterances

1504
01:42:58,590 --> 01:43:03,040
and they figured it to become a thousand times real time

1505
01:43:03,100 --> 01:43:07,750
and again i guess was practice you get back but it it's a thousand yeah

1506
01:43:08,000 --> 01:43:12,410
OK so there's again the issue of

1507
01:43:12,430 --> 01:43:18,770
of the that being difficult to come by one interesting solution that people have been

1508
01:43:18,770 --> 01:43:24,060
looking at recently is is doing embedded training we start off by using these kinds

1509
01:43:24,060 --> 01:43:30,290
of features is the basically start with the phonetic phonemic transcription map those to the

1510
01:43:30,290 --> 01:43:35,820
distinctive features and then we do it training procedure that allows these things the boundaries

1511
01:43:35,840 --> 01:43:38,710
between the individual features slide around

1512
01:43:38,720 --> 01:43:40,510
OK and i will describe

1513
01:43:40,510 --> 01:43:42,020
essentially the

1514
01:43:43,140 --> 01:43:45,730
this is to be quite active in a lot

1515
01:43:45,770 --> 01:43:49,540
in dealing with places so not just face recognition but

1516
01:43:49,550 --> 01:43:51,480
do expression recognition

1517
01:43:52,540 --> 01:43:58,350
trying to actually this track faces synthesizing new faces and things like that and

1518
01:43:58,400 --> 01:44:04,100
if you have a really good registration like this registration you can do very very

1519
01:44:04,100 --> 01:44:06,850
good job all these other things become a lot easier

1520
01:44:06,850 --> 01:44:11,130
my trying to recognise the face from recognise expression because you actually getting the proper

1521
01:44:11,130 --> 01:44:12,670
measurements from five

1522
01:44:12,680 --> 01:44:14,740
whereas up until kind of

1523
01:44:14,780 --> 01:44:19,170
quite recently opened the last five years or so people tend to do this stuff

1524
01:44:19,170 --> 01:44:24,060
does using a box kernel is putting to big-box in into supplying extract machine learning

1525
01:44:24,130 --> 01:44:26,990
but what i'm going to try and argue to you guys today

1526
01:44:26,990 --> 01:44:28,240
it is actually

1527
01:44:28,240 --> 01:44:31,390
we should be relying on computer vision for a lot more than just providing box

1528
01:44:31,390 --> 01:44:35,560
you should be trying to get very very dense registration to better the registration that

1529
01:44:35,560 --> 01:44:40,060
you have the body going into in the machine learning task trying to do

1530
01:44:40,070 --> 01:44:41,950
at least in the vision role

1531
01:44:41,960 --> 01:44:45,060
also i think registration is sorely seeping into

1532
01:44:45,710 --> 01:44:50,240
well the techniques from vision that we're developing seeping into other domains as well but

1533
01:44:50,260 --> 01:44:52,090
particularly talking about vision

1534
01:44:52,130 --> 01:44:55,460
some other kind of cool things process my

1535
01:44:55,480 --> 01:45:00,260
it's kind of cool this is something that we're doing for car company essentially and

1536
01:45:00,260 --> 01:45:03,070
you have the light detector frontal face

1537
01:45:03,090 --> 01:45:07,540
and we use something called interest point detectors and you don't have to have a

1538
01:45:07,540 --> 01:45:11,390
model of what the person's face looks like from this angle of this single what

1539
01:45:11,390 --> 01:45:14,870
it does is simply track individual interest points over time

1540
01:45:14,920 --> 01:45:19,180
and you have the interplay that until some so obviously a human space doesn't look

1541
01:45:19,180 --> 01:45:22,400
like it so that they can actually do a pretty good job of tracking the

1542
01:45:22,400 --> 01:45:26,540
face and actually getting ideas of the pose and things like that

1543
01:45:26,540 --> 01:45:30,450
some of the things we had to do with prices we deal with body tracking

1544
01:45:30,490 --> 01:45:35,350
and again it's sort of like the non rigid movement and there are things like

1545
01:45:35,740 --> 01:45:36,100
you can do

1546
01:45:36,570 --> 01:45:42,430
background subtraction and the types of things that actually get meaningful features where the body

1547
01:45:42,430 --> 01:45:48,040
bodies but there is the additional problem of everything and doing that the registration and

1548
01:45:48,040 --> 01:45:50,910
really what i'm going to try to do this kind of going to

1549
01:45:50,920 --> 01:45:51,850
there are the

1550
01:45:51,860 --> 01:45:53,660
the nuts-and-bolts what's going on

1551
01:45:53,670 --> 01:45:55,850
this is something that comes

1552
01:45:55,880 --> 01:45:59,890
close to my heart this is what i actually did my phd thesis in visual

1553
01:45:59,890 --> 01:46:03,070
speech recognition couple of a number of years ago now

1554
01:46:03,110 --> 01:46:04,600
and so

1555
01:46:04,600 --> 01:46:12,100
and again it's another visual task that essentially is very very important for

1556
01:46:12,610 --> 01:46:14,190
doing the computer vision

1557
01:46:14,230 --> 01:46:16,820
now if you were to ask this

1558
01:46:16,850 --> 01:46:20,830
kennedy had is now viewed as a my

1559
01:46:20,850 --> 01:46:23,820
head boss CMU to canada

1560
01:46:23,830 --> 01:46:25,010
what the three

1561
01:46:25,020 --> 01:46:30,100
the biggest problems in computer vision that he basically just rattles off registration

1562
01:46:30,100 --> 01:46:36,480
registration and registration that's and basically we're at CMU kind and not about registration that's

1563
01:46:36,480 --> 01:46:39,860
what we can do a lot of work on been doing work over that for

1564
01:46:39,860 --> 01:46:43,350
a long time and i think we're going to continue working on it because there's

1565
01:46:43,350 --> 01:46:44,980
a lot of

1566
01:46:45,020 --> 01:46:48,520
problems that still open in the slot and new avenues where we believe we can

1567
01:46:48,520 --> 01:46:49,950
apply a lot of this stuff

1568
01:46:49,980 --> 01:46:55,360
another application is in harm's palm recognition so we can come to see that this

1569
01:46:55,360 --> 01:47:00,020
measure is very small this machine more exact set using the exact same technique but

1570
01:47:00,020 --> 01:47:00,640
what is

1571
01:47:00,660 --> 01:47:01,630
this is kind of odd

1572
01:47:01,670 --> 01:47:03,850
visual example showing that

1573
01:47:03,850 --> 01:47:08,380
we can apply the technique to both things and also in contract

1574
01:47:08,390 --> 01:47:11,520
so this is something that's interesting here two and again this is what kind of

1575
01:47:11,520 --> 01:47:13,200
back to the box depicted here

1576
01:47:13,250 --> 01:47:17,920
even with the box effect these sort of things are difficult because when you tracking

1577
01:47:18,170 --> 01:47:22,330
an object over time especially all of the time and in traditional tracking what these

1578
01:47:22,330 --> 01:47:28,450
do is essentially use the last box was tracked incoming use that as the model

1579
01:47:28,450 --> 01:47:29,500
for the next frame

1580
01:47:29,540 --> 01:47:32,910
but what tends to happen at the time to get this thing on the drift

1581
01:47:32,910 --> 01:47:35,850
so if you look at this track are long enough time

1582
01:47:35,860 --> 01:47:39,630
this box will come to get caught noise solid drift dot

1583
01:47:39,640 --> 01:47:44,570
so a lot of computer vision researchers also that can recognise problems with drift so

1584
01:47:44,570 --> 01:47:46,850
that you can

1585
01:47:46,890 --> 01:47:50,570
adaptor things at the time but you don't lose the object itself

1586
01:47:50,580 --> 01:47:53,580
this is some motivations obviously there's a whole

1587
01:47:53,600 --> 01:47:54,510
what more

1588
01:47:54,540 --> 01:47:59,550
applications of things is sort of things that i worked on various government projects and

1589
01:47:59,590 --> 01:48:06,440
papers and other things sort of and so forth for your visual pleasure so

1590
01:48:06,710 --> 01:48:11,440
so that's the kind of motivation for why that we're looking looking at this problem

1591
01:48:11,470 --> 01:48:14,340
now if you guys who now

1592
01:48:14,370 --> 01:48:16,260
can i get a kind of a

1593
01:48:16,300 --> 01:48:21,660
sample from class who's done anything in computer vision you all kind of like that

1594
01:48:21,680 --> 01:48:24,880
that is not really does it so

1595
01:48:24,890 --> 01:48:29,140
hopefully some of this you also get something out of this for a larger audience

1596
01:48:29,140 --> 01:48:33,220
out there i just want to collect make a kind of

1597
01:48:33,270 --> 01:48:35,090
these are my thoughts

1598
01:48:36,110 --> 01:48:40,160
as a machine learning person or someone who is applying machine learning division what should

1599
01:48:40,160 --> 01:48:42,560
actually be thinking about and essentially

1600
01:48:42,610 --> 01:48:48,760
machine learning has to revolutionize a lot of money i disciplines say computer vision to

1601
01:48:48,860 --> 01:48:52,090
example speech

1602
01:48:52,120 --> 01:48:55,560
some of the text mining and so on and so forth i mean and machine

1603
01:48:55,560 --> 01:49:01,640
learning providers with you a set of ubiquitous tools that can kind of plug-and-play into

1604
01:49:01,640 --> 01:49:06,840
these different domains but i think hopefully what you guys together into this last

1605
01:49:06,860 --> 01:49:09,050
two weeks with two weeks are wrapping up

1606
01:49:09,060 --> 01:49:13,090
is that all of time with this machine learning tools there's a little bit of

1607
01:49:13,090 --> 01:49:17,770
carol you really have to think about the domain specific information

1608
01:49:18,300 --> 01:49:22,880
that goes into applying the tools so something that my work extremely well

1609
01:49:24,760 --> 01:49:29,380
speech may not work well at all in vision and actually that's that's very true

1610
01:49:29,690 --> 01:49:31,210
looking speech literature

1611
01:49:31,260 --> 01:49:36,320
things known as generative models like hidden markov models gas mixture models will be sort

1612
01:49:36,320 --> 01:49:41,450
of these these things that are very very prolific in speech gives much mention in

1613
01:49:41,450 --> 01:49:46,570
vision spew we over time out of this but the main assumptions and domain specifics

1614
01:49:46,570 --> 01:49:51,810
about the problem and so what translators if you can think of vision problems is

1615
01:49:51,810 --> 01:49:52,930
being cast

1616
01:49:52,940 --> 01:49:57,150
and you're gonna make you kind of got the the standard that screwdrivers got the

1617
01:49:57,150 --> 01:50:01,440
ball in you get everything else

1618
01:50:01,490 --> 01:50:04,080
if you're not

1619
01:50:04,130 --> 01:50:05,830
take into account

1620
01:50:07,240 --> 01:50:11,830
the domain specific information redundancy for the distribution so if you try and kind of

1621
01:50:11,830 --> 01:50:15,200
you to take advantage of that we don't to have knowledge about

1622
01:50:15,250 --> 01:50:17,710
this is the sort of thing that goes there goes on

1623
01:50:18,700 --> 01:50:20,940
it's also what kind of goes on when i

1624
01:50:20,950 --> 01:50:23,570
that's my concern markup but

1625
01:50:26,630 --> 01:50:29,110
so what kind of one

1626
01:50:29,820 --> 01:50:31,930
essentially was you guys is that

1627
01:50:31,960 --> 01:50:35,780
really and this is not just the vision this for anything where you are applying

1628
01:50:35,780 --> 01:50:38,080
machine learning and that's what

1629
01:50:38,090 --> 01:50:43,160
in my research so far that's really what i've done been really inspired a lot

1630
01:50:43,210 --> 01:50:46,990
machine learning but i'm really motivated by finding applications

1631
01:50:47,050 --> 01:50:51,760
and a big thing that i kind of motivates me is going to disappoint all

1632
01:50:51,760 --> 01:50:54,820
the people just avoided for active

1633
01:50:54,840 --> 01:50:59,130
but there's nothing to prevent you from having to additional develop

1634
01:50:59,140 --> 01:51:03,530
so what you then would have basically

1635
01:51:03,660 --> 01:51:06,690
a decision function

1636
01:51:06,710 --> 01:51:13,180
well you wouldn't

1637
01:51:14,150 --> 01:51:15,280
white star

1638
01:51:15,310 --> 01:51:16,340
of x

1639
01:51:16,350 --> 01:51:21,350
being the argmax

1640
01:51:21,400 --> 01:51:23,520
overall y fronts

1641
01:51:25,580 --> 01:51:30,120
part of x and y fronts of it

1642
01:51:30,180 --> 01:51:32,170
but it will just use that

1643
01:51:37,530 --> 01:51:39,890
the trouble with this is that as you get

1644
01:51:39,930 --> 01:51:43,830
lots of y for instance if looking at every station

1645
01:51:43,830 --> 01:51:49,580
you get in factory many why you would actually need to for the efficient way

1646
01:51:49,630 --> 01:51:54,040
it's not really so much trouble finding good representation for it

1647
01:51:54,180 --> 01:51:58,020
if you're talking about the five hundred plus from the recovery

1648
01:51:58,030 --> 01:52:00,380
you're talking about graph matching

1649
01:52:00,390 --> 01:52:02,160
sequence that i

1650
01:52:02,170 --> 01:52:04,490
web page ranking

1651
01:52:04,500 --> 01:52:05,640
don't bother

1652
01:52:05,640 --> 01:52:07,920
just to complicate the story

1653
01:52:07,970 --> 01:52:11,980
it will just really seriously messed up the optimisation of

1654
01:52:12,030 --> 01:52:14,880
i mean as in the implementation of make it so much harder to do but

1655
01:52:14,930 --> 01:52:17,490
for the

1656
01:52:17,510 --> 01:52:19,570
any other questions

1657
01:52:19,580 --> 01:52:24,420
i have also seen this

1658
01:52:24,470 --> 01:52:31,210
approach to multiclass problems where people have four in classes they they have in separate

1659
01:52:31,230 --> 01:52:35,790
is millions and then they somehow combine they are going to think about this system

1660
01:52:35,790 --> 01:52:38,700
obsolete now OK so

1661
01:52:38,710 --> 01:52:41,200
there are various ways on

1662
01:52:41,220 --> 01:52:44,290
solving multiclass SVM optimisation problem

1663
01:52:44,340 --> 01:52:47,380
so one of the ways of solving them is by

1664
01:52:47,430 --> 01:52:53,750
well i mean yes you could do exactly what you suggest namely have in one

1665
01:52:53,750 --> 01:52:55,660
was the risk classifiers

1666
01:52:57,000 --> 01:53:01,260
and then you just fold in one of the rest of the misation problem

1667
01:53:01,310 --> 01:53:02,670
and the idea would be

1668
01:53:02,690 --> 01:53:05,510
at the correct class will always win

1669
01:53:05,560 --> 01:53:09,430
but you can actually get lots of ambiguous solutions out of it

1670
01:53:09,460 --> 01:53:13,470
this is one of the reasons why am i might not want to choose

1671
01:53:13,850 --> 01:53:19,180
there's another one way you could simply have like the tournament

1672
01:53:19,440 --> 01:53:22,430
for let's say like like this or

1673
01:53:23,240 --> 01:53:27,590
in the final round around the world cup it just how well you character classes

1674
01:53:27,590 --> 01:53:30,570
as the colony with further down and down

1675
01:53:30,580 --> 01:53:31,600
you can do this

1676
01:53:31,610 --> 01:53:33,560
you will also get some people

1677
01:53:33,570 --> 01:53:36,580
not happy the uniqueness problem anymore

1678
01:53:37,270 --> 01:53:40,900
i mean how do you know which way you should combine the class

1679
01:53:40,930 --> 01:53:44,180
he might actually be making a problem much harder

1680
01:53:44,240 --> 01:53:45,140
there are

1681
01:53:45,140 --> 01:53:55,380
very nice productions like all by pope john langford

1682
01:53:56,190 --> 01:54:00,980
but but but really what is the point at which

1683
01:54:00,990 --> 01:54:02,850
the net

1684
01:54:02,850 --> 01:54:05,350
a lot of other stuff companies like

1685
01:54:06,460 --> 01:54:08,660
he has come up with a

1686
01:54:08,700 --> 01:54:10,720
nontrivial version of this

1687
01:54:10,760 --> 01:54:13,060
tournament scheme

1688
01:54:13,100 --> 01:54:18,180
and he works local reduction so what is massive is married to find some

1689
01:54:18,240 --> 01:54:21,230
essentially combination of finding that problem

1690
01:54:21,280 --> 01:54:25,660
which also performed fairly well multiple

1691
01:54:25,660 --> 01:54:31,130
it doesn't work so well for structured estimation but multiclass has also been

1692
01:54:31,140 --> 01:54:32,720
i'm not saying that

1693
01:54:32,740 --> 01:54:36,100
this is why i think and why tried mapping is the only way you the

1694
01:54:36,100 --> 01:54:39,780
thing is also well is not only one simple way of mapping x and y

1695
01:54:39,780 --> 01:54:42,280
to be just right

1696
01:54:43,460 --> 01:54:49,020
you can probably cover some of the other things but just in the right

1697
01:54:49,090 --> 01:54:54,150
it's by the politics and what i

1698
01:54:55,650 --> 01:54:57,380
and in days

1699
01:54:57,430 --> 01:54:58,260
a set of

1700
01:54:59,380 --> 01:55:00,750
this information

1701
01:55:00,810 --> 01:55:06,170
written and experimented and corroborated by people from the might

1702
01:55:06,180 --> 01:55:10,580
which argues that actually those one versus the rest

1703
01:55:10,590 --> 01:55:14,660
but it was incorrect somewhat better

1704
01:55:14,660 --> 01:55:16,370
OK beta distribution

1705
01:55:16,380 --> 01:55:21,160
so now we are going to into more and more exotic exotic territory of distributions

1706
01:55:21,160 --> 01:55:23,920
that you might encounter

1707
01:55:24,040 --> 01:55:26,560
well actually not quite

1708
01:55:26,590 --> 01:55:28,540
you and before

1709
01:55:28,550 --> 01:55:29,540
we had logo

1710
01:55:29,550 --> 01:55:33,540
where x in one minus six is our sufficient statistics

1711
01:55:33,590 --> 01:55:34,700
for the

1712
01:55:34,700 --> 01:55:36,500
binomial distribution

1713
01:55:36,550 --> 01:55:40,370
here we have the loss of those

1714
01:55:40,420 --> 01:55:45,030
then i just taking product with some parameters theta one and theta two and i

1715
01:55:45,080 --> 01:55:47,340
again restricted domain of the axis

1716
01:55:47,390 --> 01:55:49,730
now to be the interval between zero and one

1717
01:55:49,740 --> 01:55:53,840
before that we had just events zero in one

1718
01:55:53,860 --> 01:55:56,390
well that's a beta functions

1719
01:55:56,410 --> 01:55:58,750
that's why it's called the beta distribution

1720
01:56:01,740 --> 01:56:05,350
well you can essentially model a lot of

1721
01:56:06,650 --> 01:56:10,270
and peaked distribution on the unit interval with this

1722
01:56:10,270 --> 01:56:14,870
they don't have to be symmetric but i've just drawn three symmetric ones

1723
01:56:14,890 --> 01:56:19,790
for instance the uniform distribution over the unit interval is member of it one that's

1724
01:56:19,790 --> 01:56:21,130
peaked at the tail

1725
01:56:21,140 --> 01:56:27,070
one this peaked in the middle or you could also have some which are skewed

1726
01:56:28,580 --> 01:56:32,320
you can actually model a lot of distributions over the unit interval

1727
01:56:32,360 --> 01:56:35,660
by the distribution

1728
01:56:35,710 --> 01:56:38,060
likewise you could take a gamma distribution

1729
01:56:39,210 --> 01:56:42,580
well now rather than taking the unit interval

1730
01:56:42,620 --> 01:56:45,310
takes into all between zero and infinity

1731
01:56:45,340 --> 01:56:50,880
and we get slightly different sufficient statistics you get the slightly different integrals here

1732
01:56:50,930 --> 01:56:54,810
enough you go gain

1733
01:56:54,810 --> 01:56:58,880
the one thing to notice is that he had this to say to

1734
01:56:58,890 --> 01:56:59,910
it's actually

1735
01:56:59,920 --> 01:57:02,250
in a rather funny space

1736
01:57:03,230 --> 01:57:07,530
theta one so the first quite efficient here can take any value between zero and

1737
01:57:08,240 --> 01:57:12,030
let's equal infinity whereas the second parameter

1738
01:57:12,080 --> 01:57:16,920
it is within this opening to all of them both scenes

1739
01:57:16,930 --> 01:57:20,740
so the overall domain is convex but there is no guarantee

1740
01:57:21,430 --> 01:57:26,110
the interval that the domains are actually close

1741
01:57:26,150 --> 01:57:28,990
make sense so for instance when you

1742
01:57:29,060 --> 01:57:31,270
i have a normal distribution

1743
01:57:31,280 --> 01:57:32,950
well you can have

1744
01:57:32,960 --> 01:57:35,260
i mean there in an inverse variance

1745
01:57:35,310 --> 01:57:38,290
which goes as close to zero as you want to know what the variance can

1746
01:57:38,290 --> 01:57:41,970
be as large as you want but it never will be infinity

1747
01:57:42,010 --> 01:57:45,020
so this means that the inverse variance will never be zero

1748
01:57:45,070 --> 01:57:50,220
and so therefore we always have an open interval in the parameters

1749
01:57:50,300 --> 01:57:51,810
there is no guarantee

1750
01:57:51,820 --> 01:57:56,670
that the intervals on which are operating on closed

1751
01:57:56,680 --> 01:58:02,010
and actually have sometimes get really funny effect happening at the boundaries and if you

1752
01:58:02,010 --> 01:58:04,680
want to know more about that you should talk to have issues with an entire

1753
01:58:04,680 --> 01:58:05,810
papers on that

1754
01:58:05,820 --> 01:58:11,620
some galleys submissions

1755
01:58:11,730 --> 01:58:17,060
OK and basically you can get things which are really peak that's zero

1756
01:58:17,070 --> 01:58:19,310
or something which is

1757
01:58:19,520 --> 01:58:24,870
little bit what else but essentially reasonably heavy tailed

1758
01:58:24,890 --> 01:58:27,450
mister can make them this way

1759
01:58:27,450 --> 01:58:30,310
is an entire solar g some in eq

1760
01:58:30,340 --> 01:58:35,390
go on and talk about more so you can have this latest revision of heart

1761
01:58:35,390 --> 01:58:37,350
and what have covered all those

1762
01:58:37,390 --> 01:58:38,850
and the way they are all

1763
01:58:38,890 --> 01:58:42,300
just different in terms of their choice to fall picks

1764
01:58:42,350 --> 01:58:43,640
and the domain

1765
01:58:43,640 --> 01:58:47,030
in the measure

1766
01:58:48,200 --> 01:58:52,080
as soon as you pick any combination of some topics

1767
01:58:52,330 --> 01:58:54,490
the line which you define it

1768
01:58:54,490 --> 01:58:59,010
and the measure with respect to which you want to measure this domain

1769
01:58:59,080 --> 01:59:04,080
you have everything that you need for exponential families distribution

1770
01:59:04,100 --> 01:59:07,910
in quite a few cases you can be solved in the integral analytically

1771
01:59:07,950 --> 01:59:09,530
and it's great

1772
01:59:09,570 --> 01:59:13,350
and those are the cases that most likely would have already been studied before in

1773
01:59:13,350 --> 01:59:15,100
some stats

1774
01:59:17,740 --> 01:59:22,970
well then you just put things together and you can build larger inference model so

1775
01:59:24,970 --> 01:59:30,910
for instance if you use this as a conjugate prior in certain cases you can

1776
01:59:30,910 --> 01:59:32,530
produce an entire

1777
01:59:32,580 --> 01:59:37,530
stream of papers based on exchangeability and so on so make sure been

1778
01:59:37,550 --> 01:59:39,950
using this

1779
01:59:40,070 --> 01:59:43,740
distribution here quite extensively

1780
01:59:43,760 --> 01:59:45,570
and for instance if you have

1781
01:59:45,570 --> 01:59:54,350
what happened on the a

1782
01:59:57,740 --> 02:00:06,430
so you see that you should be done in time for the next two seasons

1783
02:00:06,430 --> 02:00:08,120
times k

1784
02:00:08,410 --> 02:00:10,370
this just

1785
02:00:13,760 --> 02:00:15,680
he was

1786
02:00:18,280 --> 02:00:22,490
it's not that the way

1787
02:00:26,930 --> 02:00:30,910
this is kind of

1788
02:00:30,930 --> 02:00:33,870
common in real

1789
02:00:34,030 --> 02:00:36,370
the structure

1790
02:00:38,820 --> 02:00:43,780
and i think i was attacked by three wrong

1791
02:00:44,660 --> 02:00:49,720
compute trees have or common

1792
02:00:49,740 --> 02:00:53,240
so what happens between two

1793
02:00:53,280 --> 02:00:54,280
three years

1794
02:01:00,070 --> 02:01:03,430
this is a very

1795
02:01:03,490 --> 02:01:06,220
the concentration of

1796
02:01:06,260 --> 02:01:08,120
these are usually

1797
02:01:10,090 --> 02:01:12,410
so three

1798
02:01:16,550 --> 02:01:20,570
you should be extension

1799
02:01:33,620 --> 02:01:38,180
we we can actually do the right thing to do

1800
02:01:58,490 --> 02:02:02,780
i mean use

1801
02:02:09,620 --> 02:02:14,510
it's not

1802
02:02:20,140 --> 02:02:23,300
what we can it

1803
02:02:24,950 --> 02:02:27,700
well actually

1804
02:02:27,760 --> 02:02:30,430
they really

1805
02:02:31,490 --> 02:02:36,200
but i have to be

1806
02:02:36,220 --> 02:02:40,030
well don't know

1807
02:02:47,870 --> 02:02:53,010
OK so the

1808
02:02:53,030 --> 02:02:54,550
probabilistic models

1809
02:03:01,160 --> 02:03:03,660
approaches one

1810
02:03:05,070 --> 02:03:09,550
so we also have the right all of

1811
02:03:09,550 --> 02:03:13,430
we're trying to use

1812
02:03:13,530 --> 02:03:17,050
like it

1813
02:03:24,700 --> 02:03:27,780
reading of the world

1814
02:03:34,910 --> 02:03:36,740
the model

1815
02:03:36,760 --> 02:03:37,800
to the

1816
02:03:44,370 --> 02:03:45,570
he well

1817
02:03:45,590 --> 02:03:47,820
she was

1818
02:03:50,430 --> 02:03:52,720
and you can i

1819
02:03:52,760 --> 02:03:53,590
don't know

1820
02:03:53,810 --> 02:03:56,120
we can put a

1821
02:04:01,280 --> 02:04:02,720
we can actually

1822
02:04:05,140 --> 02:04:06,490
and the

1823
02:04:06,740 --> 02:04:09,490
a ten

1824
02:04:24,260 --> 02:04:28,140
OK case

1825
02:04:30,950 --> 02:04:35,260
so this is quite interesting

1826
02:04:35,300 --> 02:04:38,180
this model is

1827
02:04:38,510 --> 02:04:42,510
the probability of generating

1828
02:04:42,720 --> 02:04:46,030
three of the to be

1829
02:04:46,070 --> 02:04:47,280
we found

1830
02:04:49,280 --> 02:04:50,930
with a lot

1831
02:04:51,250 --> 02:04:54,720
in some way

1832
02:04:54,740 --> 02:04:55,820
and this is a

1833
02:04:55,870 --> 02:04:59,620
small providing one feature which is the probability of the

1834
02:05:00,660 --> 02:05:03,120
how about of those two

1835
02:05:03,120 --> 02:05:05,740
examples in all

1836
02:05:08,470 --> 02:05:13,850
as i said from the rest of the pie

1837
02:05:16,100 --> 02:05:18,200
the set of all time

1838
02:05:18,300 --> 02:05:20,760
thank you the for

1839
02:05:21,780 --> 02:05:23,700
to some

1840
02:05:24,550 --> 02:05:26,180
an efficient way to do this

1841
02:05:30,760 --> 02:05:34,780
what i do

1842
02:05:34,890 --> 02:05:40,240
you need to know

1843
02:05:41,620 --> 02:05:44,050
i think it would be down

1844
02:05:45,470 --> 02:05:46,600
we need to work

1845
02:05:46,700 --> 02:05:50,550
i mean this is to do so

1846
02:05:50,570 --> 02:05:53,390
design the

1847
02:05:57,410 --> 02:05:58,490
so i

1848
02:05:58,510 --> 02:06:02,970
not sure

1849
02:06:02,990 --> 02:06:05,760
OK so

1850
02:06:05,800 --> 02:06:09,300
i think it's

1851
02:06:15,300 --> 02:06:19,050
this ridge regression is called

1852
02:06:27,760 --> 02:06:32,800
that's because you find additional probabilistic information

1853
02:06:32,870 --> 02:06:34,490
it's fun

1854
02:06:37,220 --> 02:06:40,280
we've seen

1855
02:06:40,280 --> 02:06:41,390
what you need

1856
02:06:50,640 --> 02:06:55,030
some really

1857
02:06:55,050 --> 02:06:58,490
so this whole

1858
02:06:58,760 --> 02:07:00,550
the problem

1859
02:07:07,160 --> 02:07:13,330
and there's also something called a query for which she

1860
02:07:13,370 --> 02:07:16,930
and just

1861
02:07:16,970 --> 02:07:19,100
i two

1862
02:07:19,220 --> 02:07:21,590
this is that it's important just

1863
02:07:28,180 --> 02:07:32,890
new solution regression just

1864
02:07:34,240 --> 02:07:37,760
which is much more actually

1865
02:07:40,030 --> 02:07:42,800
this is really

1866
02:07:47,050 --> 02:07:55,760
and that is the reason why we're trying to

1867
02:07:55,970 --> 02:08:00,820
for example

1868
02:08:00,820 --> 02:08:04,240
so i mean people that to

1869
02:08:04,260 --> 02:08:06,320
this mission

1870
02:08:06,330 --> 02:08:07,990
one that this is

1871
02:08:07,990 --> 02:08:08,970
coming off

1872
02:08:10,470 --> 02:08:12,220
why is this idea

1873
02:08:13,260 --> 02:08:14,490
so far

1874
02:08:16,450 --> 02:08:18,780
rather than the

1875
02:08:19,970 --> 02:08:21,370
the question which is what

1876
02:08:22,760 --> 02:08:23,490
he saw

1877
02:08:24,570 --> 02:08:29,740
i'm long on my top talk i have zero

1878
02:08:29,760 --> 02:08:34,760
can lost art i know what

