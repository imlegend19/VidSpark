1
00:00:00,000 --> 00:00:03,470
two maybe there's something in one can you know go to the next one you

2
00:00:03,470 --> 00:00:05,430
know do something then come back

3
00:00:05,480 --> 00:00:06,840
and also of course

4
00:00:06,860 --> 00:00:12,490
we need machine learning algorithms that are efficient because you know it's pretty clear this

5
00:00:12,540 --> 00:00:16,110
is that the way you can actually of the state and be able to handle

6
00:00:16,110 --> 00:00:17,730
it has to go through

7
00:00:17,740 --> 00:00:19,220
machine learning approach

8
00:00:19,950 --> 00:00:23,540
in essence what i'm going to do here is to propose an example of this

9
00:00:23,560 --> 00:00:27,520
and the example is this language called markov logic

10
00:00:27,540 --> 00:00:31,530
here's the one slide summary of what markov logic is right so if you fall

11
00:00:31,530 --> 00:00:36,370
asleep after this you still haven't you noticed that the main idea of the talk

12
00:00:36,370 --> 00:00:40,390
markov logic is a language that unifies first order logic

13
00:00:40,400 --> 00:00:42,640
and probabilistic graphical models

14
00:00:42,660 --> 00:00:46,340
which i believe is is a good way to try to achieve these goals because

15
00:00:46,340 --> 00:00:51,820
first of the logic handles structured information right knowledge bases that first order logic and

16
00:00:51,870 --> 00:00:53,150
things like that log

17
00:00:53,210 --> 00:00:58,150
really special cases of first order logic probability handles unstructured information

18
00:00:58,160 --> 00:01:03,730
the noise the uncertainty inherent in text and you know semi structured sources and it

19
00:01:03,730 --> 00:01:08,700
doesn't just them together it really truly i hope i will say that markov logic

20
00:01:08,710 --> 00:01:10,620
really does unified the two

21
00:01:10,640 --> 00:01:15,110
there is no separation between the logic and the probability and the structure unstructured and

22
00:01:15,120 --> 00:01:15,470
in my

23
00:01:15,480 --> 00:01:17,550
logic anymore because so

24
00:01:17,570 --> 00:01:20,730
if you get to this from of mine that you don't have to worry about

25
00:01:20,740 --> 00:01:23,950
you know well the structured here and there is an structure over there any more

26
00:01:24,080 --> 00:01:27,070
than just think think of it all in a in a uniform way

27
00:01:27,190 --> 00:01:31,730
it does of course a lot on previous work ongoing and not really going to

28
00:01:31,730 --> 00:01:34,940
lock in the interests of time but i would just like to mention that here

29
00:01:35,130 --> 00:01:38,360
it builds on ideas that go as far back as the early nineties and something

30
00:01:38,360 --> 00:01:42,950
called knowledge based model construction where he was taken knowledge base and it's you know

31
00:01:42,950 --> 00:01:49,040
extract vision was from things like probabilistic relational models and so forth but compared to

32
00:01:49,040 --> 00:01:53,840
the previous work i think there's an important difference which is markov logic really is

33
00:01:53,840 --> 00:01:57,500
the first practical language as opposed to just a theoretical proposal

34
00:01:57,560 --> 00:02:02,840
that actually comes with real efficient learning and inference algorithms and the complete open source

35
00:02:02,840 --> 00:02:05,600
implementation of them they can actually use today

36
00:02:05,650 --> 00:02:07,860
so as far as i know

37
00:02:07,910 --> 00:02:11,550
at this point marco was is really the only language that does this but you

38
00:02:11,550 --> 00:02:15,190
know i expect that the more will appear in the future

39
00:02:15,210 --> 00:02:17,010
OK so

40
00:02:17,050 --> 00:02:21,290
he is the one slide summary of markov logic and what i'm going to talk

41
00:02:21,290 --> 00:02:22,360
about today

42
00:02:22,380 --> 00:02:24,500
the syntax of markov logic

43
00:02:24,510 --> 00:02:29,400
right so what is this language markov logic syntax is very very simple syntax is

44
00:02:30,140 --> 00:02:35,340
formulas in first order logic no difference from before one addition we going have to

45
00:02:35,340 --> 00:02:37,190
wait for the formula

46
00:02:38,390 --> 00:02:43,760
the semantics as all see is that we're going to interpret these formulas as templates

47
00:02:43,790 --> 00:02:45,800
for constructing graphical models

48
00:02:45,820 --> 00:02:49,200
in particular for the features of my networks

49
00:02:49,250 --> 00:02:54,520
so markov logic sets up to be probabilistic graphical model markov you know knowledge base

50
00:02:54,600 --> 00:02:59,350
markov logic and then we're gonna system inference algorithms which not surprisingly combine ideas from

51
00:02:59,350 --> 00:03:00,740
logic and probability

52
00:03:00,750 --> 00:03:07,020
including things like satisfiability testing markov chain monte carlo knowledge base model construction and so

53
00:03:07,780 --> 00:03:12,220
and similarly learning is going to combine ideas from the logic and from the statistical

54
00:03:13,310 --> 00:03:20,180
things like the perceptron pseudo likelihood inductive logic programming programming and so on

55
00:03:20,290 --> 00:03:25,290
and as i said this is all available in the alchemy software package that's what

56
00:03:25,290 --> 00:03:27,570
it's called an output up your own opinion

57
00:03:27,650 --> 00:03:34,240
and it's been used already for a wide array of information and knowledge management applications

58
00:03:34,240 --> 00:03:40,780
like information extraction web mining social networks ontology refinement personal systems et cetera the best

59
00:03:40,780 --> 00:03:45,440
paper award that's you can last year was for that apply markov logic to do

60
00:03:45,600 --> 00:03:47,040
ontology learning

61
00:03:47,050 --> 00:03:52,110
so you know we will see more interesting examples of successes of markov logic

62
00:03:52,130 --> 00:03:55,770
so let me in the interests of fairness now walk over to the side of

63
00:03:59,320 --> 00:04:03,140
let's start with a little bit of background and you know most of you are

64
00:04:03,140 --> 00:04:06,350
probably familiar with at least some of this background but just to get us on

65
00:04:06,350 --> 00:04:09,910
the same page and it's going to be very briefly covers two things

66
00:04:09,920 --> 00:04:12,920
markov networks and first order logic

67
00:04:13,710 --> 00:04:17,570
many of you are probably at least passing familiar with bayesian networks markov networks might

68
00:04:17,570 --> 00:04:23,180
be slightly less ability still images briefly introduced and here a markov network is an

69
00:04:23,180 --> 00:04:25,060
undirected graphical models

70
00:04:25,080 --> 00:04:28,350
so it models the joint distribution of the set of variables

71
00:04:28,370 --> 00:04:32,620
so for example here's four billion barrels smoking cancer asthma and cough

72
00:04:32,640 --> 00:04:36,420
and the markov network has two parts one is the graph and the other one

73
00:04:36,420 --> 00:04:42,200
is the parameters again like vision network except that its undirected and not between two

74
00:04:42,200 --> 00:04:45,930
variables means that they are directly dependent on each other

75
00:04:45,940 --> 00:04:50,190
and conversely if i take out the arcs that are adjacent to an art

76
00:04:50,200 --> 00:04:53,610
that makes it independent of the rest of the network can

77
00:04:53,620 --> 00:04:58,450
so the structure of the network gives you the conditional independencies between variables

78
00:04:58,470 --> 00:05:02,970
now the parameters are defined as follows for every clique in the graph and here

79
00:05:02,970 --> 00:05:06,820
there's cliques smoking cancer and cancer asthma cough

80
00:05:06,980 --> 00:05:09,330
we're going to define the potential function

81
00:05:09,340 --> 00:05:12,610
and the potential function is just a real valued function

82
00:05:12,620 --> 00:05:13,740
of the state

83
00:05:13,760 --> 00:05:15,030
so for example

84
00:05:15,140 --> 00:05:19,600
real valued nonnegative function of the state so for example for smoking cancer click there's

85
00:05:19,600 --> 00:05:24,060
obviously four states in his about the potential function for each one of them

86
00:05:24,180 --> 00:05:27,490
and the way compute the probability of the state is just going to each clique

87
00:05:27,490 --> 00:05:31,370
you see what's the same you get the corresponding value of the potential function and

88
00:05:31,370 --> 00:05:32,720
you multiply them all

89
00:05:33,320 --> 00:05:36,510
and then of course you have to normalized by the sum of the several states

90
00:05:36,510 --> 00:05:38,550
to make sure that it's illegal probability

91
00:05:39,340 --> 00:05:43,250
now this is all very nice but there's actually a problem here which is this

92
00:05:43,250 --> 00:05:44,610
only works

93
00:05:44,620 --> 00:05:47,970
in terms of scalability if cliques are small

94
00:05:47,990 --> 00:05:51,000
but if you have a large clique i have to do a large number states

95
00:05:51,000 --> 00:05:54,850
and things fall apart fortunately there something else that we can do

96
00:05:54,860 --> 00:05:59,300
which is something that is very popular with statisticians which is to represent the markov

97
00:05:59,300 --> 00:06:02,340
network in the form of what's called the log linear models

98
00:06:02,350 --> 00:06:03,880
a linear model

99
00:06:03,910 --> 00:06:08,940
is just the normalized exponentiated is over teachers

100
00:06:08,960 --> 00:06:12,730
and you can convert from one to the other because the product of potential if

101
00:06:12,730 --> 00:06:15,060
you take lot becomes the sum of things

102
00:06:15,790 --> 00:06:20,660
so in the simplest case i can just have one feature for every possible state

103
00:06:20,670 --> 00:06:21,810
of every click

104
00:06:21,820 --> 00:06:25,190
and the way it is the logo of the corresponding potential right so i can

105
00:06:25,190 --> 00:06:26,970
convert from potential function

106
00:06:27,020 --> 00:06:29,200
four to log linear form very easily

107
00:06:29,210 --> 00:06:33,110
but of course the the the the thing that we gain here is that

108
00:06:33,160 --> 00:06:35,970
i don't have to have one feature for every possible state

109
00:06:35,980 --> 00:06:39,580
i could have a very very large clique relating many things

110
00:06:39,580 --> 00:06:46,370
so what we can say is that if c is a maximal clique

111
00:06:46,420 --> 00:06:47,610
then for all

112
00:06:48,870 --> 00:06:51,400
outside scene

113
00:06:51,410 --> 00:06:53,860
the the number of edges

114
00:06:54,020 --> 00:06:56,220
scholars this with the number of edges

115
00:06:56,260 --> 00:06:57,700
between shame

116
00:06:57,700 --> 00:06:59,730
and an arbitrary line

117
00:06:59,760 --> 00:07:02,150
where i is a member of c

118
00:07:02,160 --> 00:07:07,890
must be less than three months less than or equal can start less

119
00:07:10,340 --> 00:07:12,520
otherwise cannot be max

120
00:07:13,750 --> 00:07:15,750
it strictly maximal clique

121
00:07:16,940 --> 00:07:18,210
this i mean

122
00:07:18,230 --> 00:07:24,610
the number of edges start from any outside external graphics incident two

123
00:07:24,610 --> 00:07:27,090
the the vertices inside

124
00:07:27,140 --> 00:07:28,100
must be

125
00:07:28,120 --> 00:07:31,300
strictly less than the cardinality of c minor

126
00:07:31,360 --> 00:07:35,700
the reason for having strictly maximal cliques is that

127
00:07:35,750 --> 00:07:39,730
maximal cliques in one sense are unstable

128
00:07:39,740 --> 00:07:41,440
in the sense that

129
00:07:41,490 --> 00:07:43,630
if i i for example

130
00:07:43,660 --> 00:07:46,700
take this click here

131
00:07:46,740 --> 00:07:51,410
and i dropped the vertex and they had this verdict than this

132
00:07:51,410 --> 00:07:53,150
it's not the maximum degree

133
00:07:53,200 --> 00:07:57,660
it's unstable or the reported in the sense that they can just take one vertex

134
00:07:57,670 --> 00:07:59,850
in the maximum clique throw it away

135
00:07:59,850 --> 00:08:03,080
at the external and then they get the new mexico

136
00:08:03,130 --> 00:08:09,420
while strictly maximal clique more stable since no external vendors

137
00:08:09,430 --> 00:08:10,930
so external that

138
00:08:10,950 --> 00:08:14,670
is a sufficient number of edges incident to the

139
00:08:14,710 --> 00:08:16,040
internal vertices

140
00:08:16,050 --> 00:08:18,530
so can just take one of these

141
00:08:18,540 --> 00:08:20,200
so we would have one

142
00:08:20,250 --> 00:08:24,930
so it's in one sense and more stable this is a distinction which reflected

143
00:08:25,790 --> 00:08:27,170
the structure of

144
00:08:27,180 --> 00:08:33,700
local optima we have we have an optimisation problem continuous optimisation problems for example

145
00:08:33,810 --> 00:08:34,960
you might have

146
00:08:34,970 --> 00:08:38,790
strict local optima and local optima which are not strict

147
00:08:38,800 --> 00:08:42,850
and we shall see later on the actually there is a one-to-one correspondence between

148
00:08:42,850 --> 00:08:47,620
the notion of a strictly maximum weight and the notion of the max

149
00:08:49,260 --> 00:08:52,440
now it's clear that when

150
00:08:52,440 --> 00:08:56,080
we are just have binary similarities

151
00:08:56,120 --> 00:09:00,210
the notion of the maximum clique is exactly the notion we're looking for

152
00:09:00,670 --> 00:09:05,660
the notion of a cluster coincide with no inside the notion of the maximum clique

153
00:09:05,660 --> 00:09:06,750
is satisfied

154
00:09:06,760 --> 00:09:09,510
both internal and external

155
00:09:09,530 --> 00:09:14,080
so now the question is how can we generalize the notion of the maximal clique

156
00:09:14,080 --> 00:09:18,780
two edge weighted graphs and this is where the concept count

157
00:09:18,800 --> 00:09:22,300
it each

158
00:09:22,310 --> 00:09:27,660
yeah so you might might have several so the maximum

159
00:09:27,700 --> 00:09:28,710
OK so

160
00:09:29,930 --> 00:09:31,460
now our objective is

161
00:09:31,480 --> 00:09:34,740
to generalize the notion of a maximal clique which

162
00:09:34,810 --> 00:09:37,170
OK let's wait let let's take

163
00:09:37,170 --> 00:09:42,510
an edge weighted graphs let's take a set of what is this as

164
00:09:42,560 --> 00:09:46,610
let's take a vertex that belongs to us all

165
00:09:46,620 --> 00:09:49,440
this situation

166
00:09:49,510 --> 00:09:51,110
set s

167
00:09:51,150 --> 00:09:53,890
of is vertex i

168
00:09:55,640 --> 00:09:57,400
we we picture

169
00:10:00,880 --> 00:10:04,970
OK so in this set there are other viruses probably

170
00:10:05,710 --> 00:10:11,480
just some of the similarities between i and all the bodies inside

171
00:10:11,550 --> 00:10:14,240
and get what we call the average weighted degree

172
00:10:14,250 --> 00:10:17,290
all i with respect to s

173
00:10:17,330 --> 00:10:18,770
so that's something

174
00:10:18,790 --> 00:10:21,520
which is the sum over all age a

175
00:10:21,530 --> 00:10:25,530
which it belongs to us is just sort of average similarity

176
00:10:25,700 --> 00:10:30,460
average wages similarity between i and the other objects

177
00:10:30,480 --> 00:10:33,880
in c in in s

178
00:10:33,920 --> 00:10:39,040
now we derive a notion of relative similarity between two objects i and j

179
00:10:39,060 --> 00:10:40,180
i remember that

180
00:10:40,190 --> 00:10:44,000
AIJ elements of the metrics similarity metrics

181
00:10:44,030 --> 00:10:48,900
AJ represents an absolute similarity between two

182
00:10:48,940 --> 00:10:50,490
now we

183
00:10:50,500 --> 00:10:53,220
we introduce the notion of the relative similarity

184
00:10:53,270 --> 00:10:56,700
we are in this situation is here to

185
00:10:56,700 --> 00:11:02,470
OK let's in this way

186
00:11:02,480 --> 00:11:03,940
we have

187
00:11:03,940 --> 00:11:05,420
set s

188
00:11:05,470 --> 00:11:08,310
we have a vertex change

189
00:11:08,470 --> 00:11:13,000
and the vertex inside as she outside US

190
00:11:13,270 --> 00:11:19,050
so the number to derive here is the the absolute similarity between i and j

191
00:11:20,560 --> 00:11:22,660
o introduced by as i j

192
00:11:22,720 --> 00:11:24,460
is just the difference between

193
00:11:24,480 --> 00:11:27,050
the actual similarity between i and j

194
00:11:27,060 --> 00:11:32,160
and the average similarity between i and all the objects in

195
00:11:32,200 --> 00:11:37,190
so this is very simple notion i just comparing the similarity

196
00:11:37,200 --> 00:11:39,170
by and j with respect to

197
00:11:39,960 --> 00:11:43,420
i similar to the other objects and j of course

198
00:11:43,460 --> 00:11:48,940
this phi can be negative or positive is positive when the absolute similarity between i

199
00:11:48,940 --> 00:11:49,590
and j

200
00:11:49,600 --> 00:11:54,980
is greater than the relative itself and this negative

201
00:11:55,040 --> 00:11:58,200
now comes the main definition which allows us to to

202
00:11:58,210 --> 00:12:01,740
define the notion of common sense

203
00:12:02,220 --> 00:12:06,090
it's actually looks quite complicated but it's quite simple

204
00:12:06,140 --> 00:12:08,800
it's a recursive definition

205
00:12:08,970 --> 00:12:12,290
let's consider a subset of this as

206
00:12:12,310 --> 00:12:15,990
let's take verdicts i belong to us

207
00:12:17,930 --> 00:12:24,850
OK so the recursive definition is actually allows us to assign to each vertex

208
00:12:31,520 --> 00:12:38,440
this allows us to assign to each vertex in s a weight remember that we

209
00:12:38,440 --> 00:12:41,010
have an edge weighted graphs in this way we are

210
00:12:41,020 --> 00:12:42,340
we're signing

211
00:12:42,340 --> 00:12:46,150
weight the vertices but with respect to s

212
00:12:46,200 --> 00:12:50,150
so we have a set as we have vertex i and then if the set

213
00:12:50,150 --> 00:12:53,780
s is a singleton then my definition WSI i

214
00:12:53,810 --> 00:12:55,480
it's one might think

215
00:12:55,520 --> 00:12:59,810
otherwise if i have more than one object in the in the set

216
00:12:59,860 --> 00:13:05,050
what i do was just the following and just summing all the relative similarities these

217
00:13:06,310 --> 00:13:11,300
between i and all the remaining in s

218
00:13:11,350 --> 00:13:15,730
but this is not just plain so i'm just waiting this is the weighted average

219
00:13:15,730 --> 00:13:17,890
action i'm waiting it's

220
00:13:17,940 --> 00:13:23,620
these are relative similarities with the weight assigned to j

221
00:13:23,620 --> 00:13:28,700
training data putting in to your solution vectors some parts

222
00:13:28,700 --> 00:13:33,060
of the some component which is orthogonal to the space spend

223
00:13:33,310 --> 00:13:42,840
by the training data would appear a very you know it would play no role because it would have a zero in a product

224
00:13:42,840 --> 00:13:45,300
with all of the training data so this extra component

225
00:13:45,300 --> 00:13:48,680
that will be orthogonal to the span of the training data

226
00:13:48,820 --> 00:13:52,000
would have no effect and since we have that regularization

227
00:13:52,000 --> 00:13:54,840
term which is trying to minimize the norm you would

228
00:13:54,840 --> 00:13:58,460
immediately throw that component away and get a lower

229
00:13:58,590 --> 00:14:07,300
optimisation of your your objective here because throwing that component away would have no effect on

230
00:14:07,300 --> 00:14:09,630
this because it would have no effect on these inner

231
00:14:09,620 --> 00:14:13,220
products the component is orthogonal to the span of the training data

232
00:14:13,240 --> 00:14:16,280
so would have no effect on this but it would reduce this

233
00:14:16,280 --> 00:14:22,200
because you've actually projected the data the the weight vector down into a into a

234
00:14:22,200 --> 00:14:35,860
smaller vector in this space spend by the training data so it's clear you know if you think about it that way that the the weight vector will be in the span of the

235
00:14:35,860 --> 00:14:39,360
of the training data therefore it will be expressible in this form that

236
00:14:39,360 --> 00:14:43,640
I've put here and indeed this just verifies that fact but also

237
00:14:43,640 --> 00:14:46,260
explicitly computes the form of that alpha would

238
00:14:46,260 --> 00:14:51,380
actually be of this form all okay so this is the so-called dual

239
00:14:51,380 --> 00:14:55,830
representation now it's a slightly at odds with the

240
00:14:56,440 --> 00:15:03,400
optimisation theory where you have primal and dual sometimes they coincide sometimes they don't exactly in

241
00:15:03,420 --> 00:15:06,800
support vector machines there's a very nice coincidence between

242
00:15:06,800 --> 00:15:12,760
the primal and dual of representation that I've referred to here and the primal and dual in

243
00:15:12,760 --> 00:15:15,800
optimisation theory but I think if we are being honest we need

244
00:15:15,800 --> 00:15:19,440
to just say this is our terminology that we use in

245
00:15:19,440 --> 00:15:23,660
kernel methods and it's loosely borrowed from optimisation

246
00:15:23,660 --> 00:15:36,070
theory but doesn't always exactly accord with optimisation theory so think of it as just a way of representing your your solution vector in terms of the training data and

247
00:15:36,080 --> 00:15:40,200
now what we want to do is not try and learn the W but try and

248
00:15:40,210 --> 00:15:43,480
learn alpha so we've got a re sort of jigging at the

249
00:15:43,480 --> 00:15:47,840
problem we now rather than try and learn W we try and learn alpha so

250
00:15:47,840 --> 00:15:55,480
sort of implicitly learning W through learning its dual representation okay well let's try and do that in this case here's

251
00:15:55,510 --> 00:16:00,220
our alpha here is an equation for it but unfortunately it involves

252
00:16:00,220 --> 00:16:03,440
W so we're not gonna get alpha out of this equation

253
00:16:03,440 --> 00:16:13,860
because it's sort of a recursive but what we can do is just substitute for this W what we know it can be expressed as in terms of

254
00:16:13,860 --> 00:16:16,480
alpha so we're just gonna substitute X primed alpha in

255
00:16:16,480 --> 00:16:20,680
here and we'll now have an equation involving only alpha so

256
00:16:20,680 --> 00:16:26,040
that's what we do here and this is the equation that you get lambda alpha

257
00:16:26,050 --> 00:16:32,400
equals Y minus X primed X X primed alfa okay so that's just multiplied by this lambda

258
00:16:32,640 --> 00:16:38,420
Y minus X X primed alpha so it just subsitute that in there okay so now

259
00:16:38,570 --> 00:16:41,940
rearranging terms we've now got the following equation

260
00:16:41,940 --> 00:16:44,160
it looks very similar to what we had for the primal

261
00:16:44,190 --> 00:16:47,700
solution but actually in some respects it's even simpler because now

262
00:16:47,700 --> 00:16:51,320
on the right hand side we just Y whereas before we had X

263
00:16:51,320 --> 00:16:56,680
prime Y on the right hand side here and this term

264
00:16:56,680 --> 00:17:04,500
here was X primed X plus lambda rhi in the equation for alpha is X X prime so the order of

265
00:17:04,500 --> 00:17:08,180
these two has swapped and we've got rid of this X primed on the right

266
00:17:08,180 --> 00:17:13,640
hand side in in this equation here right so it's X X

267
00:17:13,640 --> 00:17:16,420
primed plus lambda rhi of course the dimension now is

268
00:17:16,420 --> 00:17:21,360
different the X primed X had dimension equal to the

269
00:17:21,360 --> 00:17:25,920
feature space dimension because of course the

270
00:17:25,920 --> 00:17:30,650
weight vector had has dimension equal to the feature space dimension

271
00:17:30,720 --> 00:17:33,240
and so the identity matrix also have the same dimension

272
00:17:33,240 --> 00:17:37,000
that we had it but in this case alpha has dimension equal

273
00:17:37,000 --> 00:17:39,500
to the number of training points remember there was one alpha

274
00:17:39,500 --> 00:17:42,380
i have a huge investment into any offspring

275
00:17:42,390 --> 00:17:45,890
i'm not investment in the sense of hard work and effort but there is that

276
00:17:46,770 --> 00:17:49,510
investment in the sense that when you're when your

277
00:17:49,530 --> 00:17:51,210
and just when you have e

278
00:17:51,220 --> 00:17:55,750
when you're pregnant with one offspring you can have another

279
00:17:55,760 --> 00:18:00,190
what this does is it has ramifications percolate upwards

280
00:18:03,960 --> 00:18:06,000
in the to different psychologies

281
00:18:06,020 --> 00:18:11,730
males as single market for several females forcing some males to the main list and

282
00:18:11,730 --> 00:18:12,780
giving rise

283
00:18:12,860 --> 00:18:15,320
the competition

284
00:18:15,330 --> 00:18:18,410
to see who can the most females

285
00:18:18,460 --> 00:18:19,700
for females

286
00:18:19,710 --> 00:18:22,800
however females can always find mates

287
00:18:22,850 --> 00:18:25,290
so sheer numbers don't count

288
00:18:26,270 --> 00:18:29,380
this competition to may with the right males

289
00:18:29,390 --> 00:18:31,590
those is offering has the best chance

290
00:18:31,600 --> 00:18:33,190
of surviving

291
00:18:34,330 --> 00:18:36,050
the competition

292
00:18:36,060 --> 00:18:40,720
now explain suppose we started with it explains why males are typically larger

293
00:18:40,730 --> 00:18:43,580
and often why males have evolved special weapons

294
00:18:43,590 --> 00:18:48,870
the special weapons of all fighting other males for reproductive access

295
00:18:48,920 --> 00:18:50,950
it also explain something else

296
00:18:50,960 --> 00:18:54,020
females biologically are choosy

297
00:18:54,120 --> 00:18:59,820
so males have to compete not merely of other males two

298
00:18:59,840 --> 00:19:04,910
two two to get reproductive access but also to woo females

299
00:19:04,930 --> 00:19:08,420
so often males have all special displays

300
00:19:08,480 --> 00:19:10,430
like this

301
00:19:10,440 --> 00:19:12,380
which existed only

302
00:19:12,390 --> 00:19:13,440
to be beautiful

303
00:19:13,450 --> 00:19:19,610
only to be attractive and to attract mates this called evolutionary logic was captured in

304
00:19:19,610 --> 00:19:22,530
this cartoon

305
00:19:26,570 --> 00:19:31,600
really does sum up a hundred years of mate selection research

306
00:19:33,140 --> 00:19:34,320
the logic

307
00:19:34,330 --> 00:19:35,730
it goes like this then

308
00:19:35,740 --> 00:19:40,510
the difference in the size of sex cells leads to differences in typical parental investment

309
00:19:40,510 --> 00:19:45,510
leading the differences in the sort of psychological and physiological mechanisms that of all OK

310
00:19:45,510 --> 00:19:47,560
that's a good story

311
00:19:47,570 --> 00:19:49,480
what sort of evidence is there for it

312
00:19:49,490 --> 00:19:53,490
well it turns out this could explain some otherwise surprising things

313
00:19:53,540 --> 00:19:54,920
for instance

314
00:19:54,940 --> 00:19:59,220
there should be there are some cases where the parental investment to switch

315
00:19:59,230 --> 00:20:04,920
in some cases where it turns out when males and with more investment than the

316
00:20:06,390 --> 00:20:09,820
and the and the theory predicts

317
00:20:09,840 --> 00:20:13,490
that in these cases you should get an asymmetry

318
00:20:14,210 --> 00:20:18,510
in cases like fish for instance the male takes the eggs

319
00:20:18,570 --> 00:20:19,870
in two appel

320
00:20:19,890 --> 00:20:22,420
and plug them into his bloodstream

321
00:20:22,430 --> 00:20:26,420
the females should they have less investment in the males

322
00:20:26,440 --> 00:20:27,610
in this case

323
00:20:27,640 --> 00:20:29,610
you would predict as is true

324
00:20:29,620 --> 00:20:31,960
the females should be larger

325
00:20:31,970 --> 00:20:33,370
the females by

326
00:20:33,380 --> 00:20:38,060
other females more than males but males and the females try to compete for the

327
00:20:38,060 --> 00:20:40,540
attention of the males

328
00:20:41,500 --> 00:20:43,950
the movie march of the penguins

329
00:20:43,980 --> 00:20:45,810
we saw a clip from it

330
00:20:45,820 --> 00:20:50,150
and this was in the context of discussing the emotions that have evolved towards their

331
00:20:50,150 --> 00:20:54,650
offspring but remember the story and how both the male and female have to go

332
00:20:54,650 --> 00:20:57,060
to tremendous lengths

333
00:20:57,070 --> 00:20:59,070
to protect the eggs

334
00:20:59,080 --> 00:21:03,380
and if one of them fails they dies and neither one has

335
00:21:03,410 --> 00:21:07,120
you should then not even have to remember

336
00:21:07,130 --> 00:21:09,800
whether male females are much bigger

337
00:21:09,800 --> 00:21:11,230
and female penguins

338
00:21:11,240 --> 00:21:14,170
you should realize they should not be in fact they are

339
00:21:14,240 --> 00:21:15,800
they're about the same size

340
00:21:15,800 --> 00:21:18,880
because the parental investment is equal

341
00:21:18,900 --> 00:21:24,150
you should be able to predict the size differences in aggression differences based on differing

342
00:21:24,150 --> 00:21:25,600
parental investment

343
00:21:25,610 --> 00:21:27,070
so for instance

344
00:21:27,960 --> 00:21:29,770
elephant seals

345
00:21:29,770 --> 00:21:35,100
are four times the males are enormous there four times bigger than the females

346
00:21:35,120 --> 00:21:40,230
and this is in large part because elephant seals compete for herons the females

347
00:21:40,250 --> 00:21:42,590
is the winner-take-all

348
00:21:42,620 --> 00:21:44,960
given names are about the same size

349
00:21:45,010 --> 00:21:50,540
this is because gibbons are pretty monogamous to raise children together

350
00:21:50,820 --> 00:21:56,830
this illustrates something which is is not always the case

351
00:21:57,310 --> 00:22:00,000
that male parental investment

352
00:22:01,240 --> 00:22:03,790
there are some species including gibbons

353
00:22:03,800 --> 00:22:09,240
where is in the male's reproductive advantage to care for offspring imagine a situation for

354
00:22:09,240 --> 00:22:12,160
instance where an offspring would die

355
00:22:12,210 --> 00:22:16,840
if both parents didn't want to for many years

356
00:22:16,850 --> 00:22:18,350
and where

357
00:22:19,550 --> 00:22:24,090
devoted offspring had to be exclusive if you focus on another family or went away

358
00:22:24,480 --> 00:22:28,350
offering with died in that case you have equal investment

359
00:22:28,430 --> 00:22:33,050
it wouldn't matter equally to male and female to invest in offspring and the cost

360
00:22:33,060 --> 00:22:35,880
would be the same

361
00:22:35,890 --> 00:22:39,590
there's no species is hard to the species that had that

362
00:22:39,640 --> 00:22:43,270
much of an ecosystem but some primates are close to it

363
00:22:43,280 --> 00:22:45,090
and this raises the question then

364
00:22:45,110 --> 00:22:47,540
what about humans what about us

365
00:22:47,540 --> 00:22:48,530
what do we know

366
00:22:48,540 --> 00:22:51,590
about the differences between males and females

367
00:22:56,430 --> 00:22:59,130
are relatively polygamous species

368
00:22:59,180 --> 00:23:04,900
most cultures most human cultures are are polygamous

369
00:23:04,950 --> 00:23:08,840
american culture is what they call serial monogamy

370
00:23:08,880 --> 00:23:12,320
so we're not like some species of birds we domain for life we do a

371
00:23:12,320 --> 00:23:14,050
series of pair bonding

372
00:23:14,060 --> 00:23:17,080
for some period of time it could be for life but it need not be

373
00:23:17,080 --> 00:23:20,790
in usually isn't

374
00:23:20,830 --> 00:23:23,100
males are bigger in females

375
00:23:23,160 --> 00:23:27,740
human males the size estimates vary so much but the average human males fifteen percent

376
00:23:27,780 --> 00:23:32,560
larger on average human female this suggests that there are some may has been in

377
00:23:32,560 --> 00:23:36,400
our evolutionary history some male male competition

378
00:23:37,510 --> 00:23:39,660
for access to females

379
00:23:39,680 --> 00:23:43,960
which suggests in turn that parental investment is not quite equal

380
00:23:44,340 --> 00:23:48,060
males have smaller testicles for their body size

381
00:23:48,070 --> 00:23:51,030
then chimpanzees but larger testicles

382
00:23:51,210 --> 00:23:52,800
gorillas and gibbons

383
00:23:52,810 --> 00:23:57,250
and this suggested there was some intermediate amount of competition

384
00:23:57,260 --> 00:24:00,120
the capacity to create space

385
00:24:00,230 --> 00:24:05,960
and this is relevant for different sort of competition with regards impregnation of females multiple

386
00:24:07,440 --> 00:24:12,560
and this suggests that over evolutionary history women were not wantonly promiscuous but we're not

387
00:24:12,560 --> 00:24:17,350
entirely monogamous either so much so that the pavement evolutionary point of view

388
00:24:17,400 --> 00:24:23,810
you do all males throughout the capacity to to produce more sperm than other males

389
00:24:23,810 --> 00:24:26,720
as i said the idea is to work with something

390
00:24:26,730 --> 00:24:31,310
much more rich than the dictionary what you would like a is to put in

391
00:24:31,310 --> 00:24:36,990
your dictionary any features that may be useful to do your classification and potentially the

392
00:24:36,990 --> 00:24:43,330
dictionary they become you wish so as a result you get very large and redundant

393
00:24:43,350 --> 00:24:48,530
dictionary of of and then the question you mask is of course the same

394
00:24:48,530 --> 00:24:54,950
how can i approximates f by picking the best elements from detection

395
00:24:54,960 --> 00:24:59,390
and very quickly of course you realize that this is an NP complete problem it's

396
00:24:59,400 --> 00:25:05,220
very closely related for example by the three set covering problem so you need to

397
00:25:05,220 --> 00:25:11,840
approximate is NP complete problem with some computational leave feasible solution

398
00:25:11,920 --> 00:25:16,730
one approach which is the simplest which is classical when you have an NP complete

399
00:25:16,730 --> 00:25:17,910
problem is to

400
00:25:17,930 --> 00:25:23,510
use greedy approach so that's the idea of the matching pursuit algorithms the basic idea

401
00:25:23,510 --> 00:25:29,160
is instead of picking globally the vectors you pick them one by one and which

402
00:25:29,160 --> 00:25:33,700
will be the best vector you pick the one that absorbs the maximum amount of

403
00:25:33,700 --> 00:25:38,140
energy from the signal you take it out to get the residue what's the next

404
00:25:38,140 --> 00:25:42,450
second vector the one that will absorb the maximum energy from the residue you pick

405
00:25:42,450 --> 00:25:46,610
it out and so on and you iterate and it does indeed converge but potentially

406
00:25:46,610 --> 00:25:48,130
extremely slow

407
00:25:48,150 --> 00:25:50,840
now there is a second approach

408
00:25:50,960 --> 00:25:56,290
that was introduced by and chan is this idea to say well what you are

409
00:25:56,290 --> 00:26:01,690
optimizing in some sense the number of elements the number of elements of decomposition vector

410
00:26:01,910 --> 00:26:04,870
this is the else zero norm let's replies

411
00:26:04,880 --> 00:26:09,250
this l zero norm which is non convex by the closest lp norm which is

412
00:26:09,250 --> 00:26:11,490
convex and you get the l one norm

413
00:26:11,500 --> 00:26:13,320
and you end up with this

414
00:26:13,430 --> 00:26:18,650
minimisation which is weighted by the l one norm and you can indeed out of

415
00:26:18,650 --> 00:26:25,430
that extracts a sparse representation if such a representation does exist

416
00:26:25,450 --> 00:26:28,010
OK so let me now

417
00:26:28,030 --> 00:26:29,040
go to

418
00:26:29,050 --> 00:26:31,200
inverse problems

419
00:26:31,230 --> 00:26:34,700
so in an inverse problem what you have is your data u

420
00:26:34,710 --> 00:26:39,270
and what you'd like to recover is the signal that has been degraded by an

421
00:26:39,270 --> 00:26:40,430
operator you

422
00:26:40,440 --> 00:26:43,590
which is typically not invertible

423
00:26:43,610 --> 00:26:47,770
plus some noise and your data d let's say belongs to

424
00:26:47,780 --> 00:26:50,880
a space of dimension q so you have to data

425
00:26:50,890 --> 00:26:55,790
and if you're doing resolution what you'd like is not only to estimate if but

426
00:26:55,790 --> 00:27:00,500
you'd like to estimate f at a higher resolution than the input resolution in other

427
00:27:00,500 --> 00:27:04,330
words would like to estimate f in a space of dimension n

428
00:27:04,340 --> 00:27:09,230
which is the larger of dimension mentioned in the space of the which is larger

429
00:27:09,260 --> 00:27:13,870
has the dimension sorry larger than the dimension of the input data so basically you'd

430
00:27:13,870 --> 00:27:16,000
like to increase the resolution

431
00:27:16,020 --> 00:27:23,530
this is a very fundamental problem in many application domains so for example satellite satellite

432
00:27:23,530 --> 00:27:29,420
data are blurred often because the satellites moves because also the imperfection of the optics

433
00:27:29,710 --> 00:27:35,270
so you'd like of course is the convolution and potentially increase the resolution this are

434
00:27:35,270 --> 00:27:41,000
major industrial issues the second example i'm going on the right are cystic sparse spike

435
00:27:41,000 --> 00:27:47,570
deconvolution so insisting data you have to inverts the wave equation and you'd like to

436
00:27:47,570 --> 00:27:54,750
get very sparse representation providing you the geophysical layers that are the most meaningful elements

437
00:27:54,750 --> 00:28:00,300
for the analysis by the geophysicist of the data

438
00:28:00,320 --> 00:28:03,900
OK now i would like also to speak so these two problems we worked on

439
00:28:03,900 --> 00:28:09,760
them as i said initially when we began to do start up and then we

440
00:28:09,760 --> 00:28:14,320
realized that we were not going to survive by working on this kind of professional

441
00:28:14,320 --> 00:28:18,060
data so we decided to go for

442
00:28:18,070 --> 00:28:22,830
consumer product much larger market and the idea was sent only to realize while the

443
00:28:23,700 --> 00:28:29,280
high-definition television around i don't know if you've ever boat high-definition television

444
00:28:29,310 --> 00:28:32,780
but when you buy in the store you have

445
00:28:32,800 --> 00:28:38,290
amazing beautiful images you take the television you go back home

446
00:28:38,330 --> 00:28:40,450
and images completely crap

447
00:28:40,880 --> 00:28:41,950
now why is that

448
00:28:41,980 --> 00:28:45,200
well the reason is that when you buy it in the store the guy

449
00:28:45,240 --> 00:28:50,340
puts a high-definition television DVD so the image is beautiful you come back home and

450
00:28:50,340 --> 00:28:56,140
what you get into your TV standard definition images and UTV nowhere another has to

451
00:28:56,140 --> 00:29:01,010
take small images in larger to fit your screen OK so that he has to

452
00:29:01,010 --> 00:29:03,500
interpolate and the result is pretty bad

453
00:29:03,520 --> 00:29:07,760
so what is the TV doing first of all it has to be interlaced image

454
00:29:08,390 --> 00:29:12,200
one nine out of two has disappeared it has to be recreated

455
00:29:12,230 --> 00:29:16,910
it has also to increase the size of this is typically superresolution and now issue

456
00:29:16,910 --> 00:29:22,570
by new screen you've sixty image per second input and in fact it's displaying one

457
00:29:22,570 --> 00:29:27,360
hundred twenty image per second so it has to recreate an image and if you

458
00:29:27,360 --> 00:29:31,620
have for example a tennis ball well if you just do an interpolation you'll get

459
00:29:31,650 --> 00:29:36,240
two tennis ball in the middle image like in this example we have a moving

460
00:29:36,240 --> 00:29:42,290
words so that doesn't work so you need to indeed interpolates but followed the geometrical

461
00:29:42,290 --> 00:29:44,190
properties in space time

462
00:29:44,210 --> 00:29:46,210
OK so

463
00:29:46,250 --> 00:29:50,370
that's the type of problem and just to give you an idea so you have

464
00:29:50,370 --> 00:29:56,310
your small size image basically have to multiply by twenty the amount of data

465
00:29:56,320 --> 00:29:58,860
to get to fits the high-definition screen

466
00:29:58,870 --> 00:30:02,580
and if you count the amount of data you have per second it is about

467
00:30:02,580 --> 00:30:04,680
seventy get by per second

468
00:30:04,690 --> 00:30:09,310
and you have to do super resolution out of seventy get by second now the

469
00:30:09,310 --> 00:30:11,660
reason why mentioning that is that

470
00:30:11,670 --> 00:30:17,110
suddenly you have an issue of scalability on your can you have an algorithm which

471
00:30:17,110 --> 00:30:21,400
treats seventy get by per second on the chip which is supposed to be less

472
00:30:21,400 --> 00:30:24,690
in on training samples and some corrupt the training samples can be mapped back to

473
00:30:24,690 --> 00:30:28,100
a training sample because that's exactly what we want so that will show the energy

474
00:30:28,100 --> 00:30:31,960
surface in the right way but just limiting the information content in the code and

475
00:30:31,960 --> 00:30:33,990
we have to push up on the energy of anything

476
00:30:34,050 --> 00:30:37,960
OK so that's kind of indirect way solving the partition function problem by not having

477
00:30:37,960 --> 00:30:42,140
to push push did you know of anything but just restrict information content of the

478
00:30:43,320 --> 00:30:45,520
you can show this on sort of

479
00:30:45,540 --> 00:30:49,700
so you can actually interpret pretty much any unsupervised learning algorithm in those terms in

480
00:30:49,700 --> 00:30:53,580
terms of how the shape the energy surface PCA is an encoder decoder models both

481
00:30:53,580 --> 00:30:54,660
are linear

482
00:30:54,730 --> 00:30:59,200
you could near the corners near the transpose of each other and the way PC

483
00:30:59,200 --> 00:31:02,530
and limit the information content of the code is that it just makes the dimension

484
00:31:02,530 --> 00:31:03,530
of the good small

485
00:31:04,070 --> 00:31:04,990
so does

486
00:31:11,480 --> 00:31:15,320
the sense of sparse coding it's what i just described i can means is an

487
00:31:15,320 --> 00:31:19,100
interesting thing could kick k means you can also as a special case of

488
00:31:19,180 --> 00:31:21,500
the encoder decoder architecture

489
00:31:21,520 --> 00:31:26,830
it except doesn't have an article really has decoder so k means doesn't have this

490
00:31:26,840 --> 00:31:30,320
it doesn't have that it imposes user one of n code

491
00:31:30,340 --> 00:31:32,070
and the reconstruction is linear

492
00:31:32,090 --> 00:31:36,700
and the error function is quadratic OK so basically replaced so you find the values

493
00:31:36,700 --> 00:31:41,450
e which is which is closest which is going to minimize reconstruction error OK

494
00:31:41,460 --> 00:31:45,140
inference process which means what it means is sort of a simplified version of this

495
00:31:45,140 --> 00:31:48,600
if you want to get sparse coding deal size and scale models sparse coding is

496
00:31:48,600 --> 00:31:53,740
similar you also don't have an encoder and you have to do for the

497
00:31:53,770 --> 00:31:54,970
he could near

498
00:31:54,980 --> 00:31:58,290
this is quadratic and there was requisitioned term here which is kind of an l

499
00:31:58,290 --> 00:32:02,390
one penalty or something that i know in color so i think about having an

500
00:32:03,290 --> 00:32:05,710
is that inferences is quick

501
00:32:05,820 --> 00:32:07,180
should don't have an encoder

502
00:32:07,200 --> 00:32:09,740
and you have one of those terms here if i give you an x you

503
00:32:09,740 --> 00:32:13,320
have to run an optimisation algorithm to find the of wolsey that minimizes the reconstruction

504
00:32:13,320 --> 00:32:18,080
right so inference is complicated because you have to do this sort of minimisation process

505
00:32:18,360 --> 00:32:21,370
whereas if you have an encoder encoder is going to improve the prediction with the

506
00:32:21,370 --> 00:32:25,970
solution the minimizes the energy that you have to get rid of the decoder altogether

507
00:32:25,990 --> 00:32:28,680
OK that's why this why i was not

508
00:32:34,940 --> 00:32:40,000
so this particular implementation of this called predictive sparse decomposition that we've been pretty fond

509
00:32:40,000 --> 00:32:43,170
of the last year or two

510
00:32:43,190 --> 00:32:46,060
and the energy function that is

511
00:32:46,070 --> 00:32:47,910
it's very simple so it's the

512
00:32:47,930 --> 00:32:50,770
also since model with an encoder

513
00:32:50,790 --> 00:32:52,940
and so if the encoder decoder here

514
00:32:52,950 --> 00:32:58,940
so the decoder is a linear decoder where the columns of the matrix are normalised

515
00:32:59,040 --> 00:33:00,600
to the one

516
00:33:02,200 --> 00:33:08,220
and the the distance function is just a quadratic distance function there is a specific

517
00:33:08,220 --> 00:33:10,770
constraints here which could be an l one penalty on the on the could that

518
00:33:10,770 --> 00:33:12,400
something that will

519
00:33:12,420 --> 00:33:13,570
OK and the

520
00:33:13,670 --> 00:33:17,340
the anchor the is the sort of non-linear regressor or if you want OK so

521
00:33:17,780 --> 00:33:19,440
in know case is going to be very simple

522
00:33:19,450 --> 00:33:20,200
sort of

523
00:33:20,250 --> 00:33:23,930
you know one or two layer neural net some kind could be anything you want

524
00:33:23,930 --> 00:33:28,900
to come machine whatever any parameterized function they can differentiate respect the parameters it's fine

525
00:33:28,900 --> 00:33:31,500
for any function you can train to regression

526
00:33:31,520 --> 00:33:35,820
and so the way you trying to thing is you play you plug your plugin

527
00:33:35,820 --> 00:33:37,280
observation input

528
00:33:37,330 --> 00:33:40,450
you find the value of the code that minimizes the sum of the three are

529
00:33:41,190 --> 00:33:43,570
the three energy terms

530
00:33:43,580 --> 00:33:45,740
OK so something that's going to

531
00:33:45,770 --> 00:33:49,650
reconstruct the input properly there is not too far from where this guy predicted and

532
00:33:49,650 --> 00:33:51,950
this is at the same time is sparse

533
00:33:51,970 --> 00:33:57,320
and once you have use this as desired output for this regressors transistor russir two

534
00:33:57,320 --> 00:34:02,820
cannot predict that value and you train this guy to predict that value by me

535
00:34:02,820 --> 00:34:04,640
to minimize OK

536
00:34:04,650 --> 00:34:06,100
very simple

537
00:34:06,120 --> 00:34:09,820
just a very simple you can do everything by gradient descent or smart method if

538
00:34:09,820 --> 00:34:10,500
you want

539
00:34:10,950 --> 00:34:13,420
and the and once we have trained the model

540
00:34:13,420 --> 00:34:16,580
if you want to use it as feature extractor

541
00:34:16,590 --> 00:34:18,890
basically get rid of this part

542
00:34:18,910 --> 00:34:21,360
you just next run the encoder

543
00:34:21,380 --> 00:34:23,400
forget about this just use that value

544
00:34:23,410 --> 00:34:27,520
and actually running through the first past fire and you can use this as features

545
00:34:27,610 --> 00:34:29,420
can use it features

546
00:34:29,430 --> 00:34:31,750
as will see little bit later

547
00:34:31,780 --> 00:34:34,230
which the

548
00:34:34,260 --> 00:34:36,690
where the weights

549
00:34:36,860 --> 00:34:37,920
in which

550
00:34:38,300 --> 00:34:43,030
that's right so like recognition with the reconstruction weight

551
00:34:43,050 --> 00:34:47,290
and i don't know if it's kind of like weeks limit

552
00:34:47,360 --> 00:34:50,120
is an inference process and then you can train the both

553
00:34:50,120 --> 00:34:54,390
with the same time which is

554
00:35:00,060 --> 00:35:01,740
you can't

555
00:35:06,250 --> 00:35:09,440
what was the

556
00:35:12,360 --> 00:35:15,140
OK so the the way you use right is that you go

557
00:35:15,160 --> 00:35:17,460
you know trained as you go from one stage of this

558
00:35:17,510 --> 00:35:19,870
in the second using the output of

559
00:35:19,880 --> 00:35:21,390
of those

560
00:35:21,410 --> 00:35:25,250
use as input to another stage trend that stage three multiple stage like this and

561
00:35:25,250 --> 00:35:28,180
what it gives you a good starting point for supervised learning

562
00:35:29,260 --> 00:35:30,240
we should

563
00:35:30,260 --> 00:35:32,350
this is a

564
00:35:34,270 --> 00:35:38,190
and sparsity OK so that's really important so sparse it is good not just because

565
00:35:38,190 --> 00:35:41,940
it solves the partition function problem but also because it produces features that are high

566
00:35:41,940 --> 00:35:44,210
dimensional and sparse and those are good for

567
00:35:44,220 --> 00:35:50,260
recognition right these revisions were

568
00:35:50,430 --> 00:35:52,670
this is what

569
00:35:52,680 --> 00:35:56,440
OK this is not sparse PCA is very compact so think of this like an

570
00:35:56,440 --> 00:35:58,730
expert on the kernel right so if you have a kernel

571
00:35:58,760 --> 00:36:02,350
first you do that with the kernel layer does that expands the input on to

572
00:36:02,380 --> 00:36:04,980
sort of high dimensional space is very sparse

573
00:36:05,030 --> 00:36:07,930
that's why it works because now you have one independent component

574
00:36:07,930 --> 00:36:11,340
the parameter for every training sample particularly OK so this is kind of same thing

575
00:36:12,450 --> 00:36:16,330
because of the sparse the sparsity here you can create a high dimensional feature vector

576
00:36:16,330 --> 00:36:21,350
is very sparse and a high dimensional feature a feature space things are more easily

577
00:36:21,520 --> 00:36:24,330
this criminal case

578
00:36:24,380 --> 00:36:29,190
and he is the fact that you get the high dimensional representation to now there

579
00:36:29,200 --> 00:36:31,230
are several criteria you can use to make it

580
00:36:31,410 --> 00:36:35,760
efficient one of them is sparsity other people use you know

581
00:36:35,780 --> 00:36:40,280
other the criteria but sparsity is it is an easy one two three four five

582
00:36:42,130 --> 00:36:44,050
where is

583
00:36:44,080 --> 00:36:46,920
six that

584
00:36:52,220 --> 00:36:56,940
all problems

585
00:36:58,930 --> 00:37:03,030
on one hand and all

586
00:37:03,430 --> 00:37:09,540
one that in it is our universe

587
00:37:09,540 --> 00:37:12,970
actually what's the formal way of solving i mean

588
00:37:12,990 --> 00:37:16,060
you can think of k two events are independent i want to think about the

589
00:37:16,060 --> 00:37:19,510
second just compute the probability of the first

590
00:37:20,660 --> 00:37:24,930
in detail the way you should do it is to compute the probability of every

591
00:37:24,930 --> 00:37:27,410
one of the four possible events

592
00:37:27,420 --> 00:37:31,490
hands and heads tails and tails heads and tails tails heads

593
00:37:32,590 --> 00:37:35,930
you know that there is a uniform distribution on

594
00:37:37,680 --> 00:37:41,050
so what you should do is to some

595
00:37:42,620 --> 00:37:44,420
it holding instances where the

596
00:37:44,430 --> 00:37:46,040
first thought

597
00:37:47,130 --> 00:37:49,250
they one of us

598
00:37:50,230 --> 00:37:55,790
and that will be point twenty five was point two five two point five

599
00:37:55,800 --> 00:37:57,130
so i mean

600
00:37:57,180 --> 00:37:59,290
this is marginalization

601
00:37:59,340 --> 00:38:04,120
but in many cases the virus are not independent like she is example just came

602
00:38:05,570 --> 00:38:07,290
in some cases

603
00:38:07,300 --> 00:38:09,680
the marginalisation

604
00:38:09,730 --> 00:38:13,980
everything almost everything that you

605
00:38:14,020 --> 00:38:18,090
do in terms of what we would call inference in graphical models

606
00:38:18,150 --> 00:38:22,520
basically computing is marginalizations OK

607
00:38:22,570 --> 00:38:27,700
computing this marginalization is just there are efficient algorithms for cases where

608
00:38:27,750 --> 00:38:31,650
you have a probabilistic model which is structured according to

609
00:38:31,720 --> 00:38:34,380
conditional independence assumption

610
00:38:37,750 --> 00:38:42,380
now what i mean this is just the beginning of the the

611
00:38:42,440 --> 00:38:43,460
of the r

612
00:38:45,320 --> 00:38:46,000
i mean

613
00:38:46,010 --> 00:38:47,310
it's probably a good

614
00:38:47,320 --> 00:38:50,500
please stop anyway because

615
00:38:50,560 --> 00:38:52,210
will start getting into

616
00:38:52,220 --> 00:38:54,290
more technical detail

617
00:38:55,010 --> 00:38:56,570
why don't stop now

618
00:38:56,590 --> 00:38:59,840
and we come back in ten minutes then

619
00:39:04,450 --> 00:39:07,690
i propose that we restart now

620
00:39:07,700 --> 00:39:11,100
so let's

621
00:39:11,250 --> 00:39:13,100
let's try to

622
00:39:13,110 --> 00:39:15,950
to restart from where we were so basically

623
00:39:15,990 --> 00:39:18,990
i want to tell you that we have

624
00:39:19,010 --> 00:39:20,640
these two basic rule

625
00:39:20,700 --> 00:39:21,740
which will

626
00:39:22,460 --> 00:39:24,670
navigating with you wherever you go

627
00:39:24,730 --> 00:39:29,360
in the autumn of the model basically you have conditionally

628
00:39:29,420 --> 00:39:30,530
which is

629
00:39:32,590 --> 00:39:34,500
which is failure the

630
00:39:34,600 --> 00:39:39,100
probability of x a given x you will be given by these

631
00:39:39,120 --> 00:39:40,630
and modernization

632
00:39:40,640 --> 00:39:45,850
so now

633
00:39:45,860 --> 00:39:51,970
let's start to goal means that the central concept that we have already

634
00:39:55,530 --> 00:39:59,010
conditionally independent k

635
00:39:59,070 --> 00:40:04,390
first let's start with the independent i'm sure most of you know

636
00:40:04,400 --> 00:40:07,850
how we define independence between

637
00:40:07,890 --> 00:40:09,860
a pair of friendlies

638
00:40:10,990 --> 00:40:13,990
if you have a pair of water

639
00:40:14,020 --> 00:40:17,460
for example xmxb

640
00:40:17,470 --> 00:40:20,850
we say that these random variables are independent

641
00:40:20,860 --> 00:40:28,080
if this equation holds for every instantiated from of x a and x b

642
00:40:30,240 --> 00:40:31,450
on no

643
00:40:32,670 --> 00:40:37,030
novelty here so let's look at conditional independence no

644
00:40:39,790 --> 00:40:42,620
essentially the same thing

645
00:40:42,660 --> 00:40:43,530
but with that

646
00:40:43,540 --> 00:40:47,440
a slight modification

647
00:40:47,510 --> 00:40:52,720
the difference between independence and conditional independence is that the

648
00:40:52,730 --> 00:40:57,380
atomic concept of independence requires to run via

649
00:40:57,440 --> 00:41:02,150
the atomic concept of conditional independence requires three written by

650
00:41:03,200 --> 00:41:05,320
and conditional independence

651
00:41:05,350 --> 00:41:07,850
makes a statement about

652
00:41:08,820 --> 00:41:11,120
two of the three

653
00:41:11,240 --> 00:41:13,230
random violence

654
00:41:14,090 --> 00:41:18,880
how how two of the trio one viable depend on each other

655
00:41:19,920 --> 00:41:23,620
think state of the third round

656
00:41:27,040 --> 00:41:30,990
that's what we're are going to see so assume instead of having random variables x

657
00:41:30,990 --> 00:41:34,620
a and x being you have an additional random variable x

658
00:41:34,630 --> 00:41:36,910
in the game OK

659
00:41:36,970 --> 00:41:39,420
and now we are going to assume

660
00:41:39,450 --> 00:41:40,650
that we

661
00:41:40,710 --> 00:41:45,580
observed value or the realities asian of the inviolable x

662
00:41:45,590 --> 00:41:46,710
OK we know

663
00:41:46,720 --> 00:41:50,920
it's fixed

664
00:41:51,900 --> 00:41:52,960
when the

665
00:41:52,970 --> 00:41:57,120
the value of x e six

666
00:41:57,120 --> 00:42:01,250
times only got this grid

667
00:42:01,250 --> 00:42:03,930
the one that follows

668
00:42:03,960 --> 00:42:05,230
this too

669
00:42:05,290 --> 00:42:07,680
omega zero square

670
00:42:07,730 --> 00:42:11,850
and the one that follows which i will call omega plus plus

671
00:42:11,980 --> 00:42:13,830
going to be true

672
00:42:15,180 --> 00:42:17,960
this claimed to have two

673
00:42:17,980 --> 00:42:20,770
times only guys script

674
00:42:20,810 --> 00:42:22,060
none of these

675
00:42:22,100 --> 00:42:23,080
of course

676
00:42:24,350 --> 00:42:30,330
but none of the resonance frequencies are coupled double pendulum were

677
00:42:30,330 --> 00:42:31,660
there's no way

678
00:42:31,660 --> 00:42:32,850
but you can even

679
00:42:32,870 --> 00:42:38,580
look at this and say oh yeah of course

680
00:42:38,620 --> 00:42:40,980
actually what did i do wrong

681
00:42:40,980 --> 00:42:42,640
yes thank you very much

682
00:42:42,640 --> 00:42:44,410
i have an omega zero there

683
00:42:44,410 --> 00:42:46,120
extra credits

684
00:42:46,120 --> 00:42:49,870
i've called omega s

685
00:42:49,940 --> 00:42:53,890
if you want to change all the omega omega zero that's fine

686
00:42:53,890 --> 00:42:57,310
but you cannot have one omega because you on omega

687
00:42:57,440 --> 00:42:59,790
so this is the

688
00:42:59,810 --> 00:43:02,390
square of the frequency of the single

689
00:43:03,620 --> 00:43:08,910
oscillating mass and thank you very much for pointing that out

690
00:43:08,930 --> 00:43:16,600
i want you all sorts of extra credit the centre of the night

691
00:43:16,620 --> 00:43:19,730
i want to be able to get some extra credit

692
00:43:19,770 --> 00:43:24,310
due to one of simulator and was the

693
00:43:24,440 --> 00:43:28,330
the thank you letters square here

694
00:43:28,390 --> 00:43:31,000
more people than one claim this one

695
00:43:31,020 --> 00:43:33,390
all right thank you very much

696
00:43:34,960 --> 00:43:36,620
so now

697
00:43:36,640 --> 00:43:40,080
for any given value of omega

698
00:43:40,120 --> 00:43:43,430
for any given value of you'll find three

699
00:43:43,480 --> 00:43:45,040
and as you can see one

700
00:43:45,080 --> 00:43:46,120
get two

701
00:43:46,120 --> 00:43:49,710
you get c three in the steady state solution

702
00:43:49,790 --> 00:43:50,620
of course

703
00:43:50,620 --> 00:43:56,040
you get infinite amplitudes which physically meaningless if you to stay away new solutions from

704
00:43:56,680 --> 00:43:57,910
in fact it is

705
00:43:57,930 --> 00:44:00,830
but you see if you don't come too close to infinity

706
00:44:00,830 --> 00:44:02,930
that the results that you get

707
00:44:02,940 --> 00:44:05,620
are quite accurate for high

708
00:44:05,730 --> 00:44:08,710
q systems

709
00:44:08,710 --> 00:44:09,850
and so now

710
00:44:09,850 --> 00:44:13,370
i'll show you

711
00:44:13,410 --> 00:44:15,910
the three amplitudes

712
00:44:15,960 --> 00:44:17,700
for this system

713
00:44:17,750 --> 00:44:20,620
which will also be put up this afternoon

714
00:44:20,640 --> 00:44:22,200
and so that the

715
00:44:22,230 --> 00:44:25,020
three core systems

716
00:44:25,040 --> 00:44:28,500
which we have set up

717
00:44:28,520 --> 00:44:31,560
the only difference is the previous one is that

718
00:44:31,600 --> 00:44:34,730
we don't know he only got divided by omega zero

719
00:44:35,390 --> 00:44:36,730
the square

720
00:44:36,770 --> 00:44:38,620
this book was provided

721
00:44:38,680 --> 00:44:40,560
to me by professor which was

722
00:44:41,520 --> 00:44:42,540
o eight o three

723
00:44:42,540 --> 00:44:43,870
you will go

724
00:44:43,890 --> 00:44:47,270
i will take for a time in a very kind of me

725
00:44:48,140 --> 00:44:52,330
it's plot even and it is my request is very nice

726
00:44:52,350 --> 00:44:54,390
car one is the first car

727
00:44:55,370 --> 00:44:58,160
then the car two is the red one second car

728
00:44:58,160 --> 00:45:01,350
this is supposed to be you don't see it but in any case if you

729
00:45:01,350 --> 00:45:02,870
think that that's fine

730
00:45:02,930 --> 00:45:06,040
then the the black line

731
00:45:06,060 --> 00:45:07,810
so horizontally

732
00:45:07,810 --> 00:45:09,460
is the ratio

733
00:45:09,520 --> 00:45:11,770
of the frequency squared

734
00:45:11,770 --> 00:45:14,200
so you see that the second resonance

735
00:45:14,250 --> 00:45:16,270
it is indeed year

736
00:45:16,350 --> 00:45:22,440
that what i have there on the

737
00:45:22,480 --> 00:45:27,230
if we plotted above zero it means that it is in phase with the driver

738
00:45:27,310 --> 00:45:29,100
if we plotted below

739
00:45:29,120 --> 00:45:30,500
zero it means

740
00:45:30,520 --> 00:45:34,790
it's out of phase with the drive

741
00:45:34,850 --> 00:45:36,390
if no

742
00:45:36,430 --> 00:45:37,810
look at this

743
00:45:37,830 --> 00:45:44,250
that already at all because you see something that by no means intuitive

744
00:45:44,310 --> 00:45:46,330
notice that c one

745
00:45:46,350 --> 00:45:47,330
c two

746
00:45:47,370 --> 00:45:48,790
nc three

747
00:45:48,890 --> 00:45:52,160
all substantially lower than at the zero

748
00:45:52,180 --> 00:45:56,770
this is in units of zero and not even the same to all three different

749
00:45:56,810 --> 00:45:58,910
what i have participated that

750
00:46:00,310 --> 00:46:01,620
i wouldn't

751
00:46:01,980 --> 00:46:04,370
maybe to be the same for me not

752
00:46:04,390 --> 00:46:05,810
all three different

753
00:46:05,870 --> 00:46:09,330
and that's the case the all three different

754
00:46:09,370 --> 00:46:12,660
when we approached residents things got out of hand

755
00:46:12,660 --> 00:46:15,680
all three are in phase

756
00:46:15,730 --> 00:46:20,410
and then you just crossed over the first residence residence they all three again in

757
00:46:20,410 --> 00:46:22,180
phase but out of phase

758
00:46:22,230 --> 00:46:24,850
with driver that's not so

759
00:46:24,890 --> 00:46:27,790
surprising all by itself

760
00:46:27,810 --> 00:46:32,560
now look at this ridiculous point

761
00:46:32,580 --> 00:46:34,000
if i

762
00:46:34,020 --> 00:46:36,430
right that system

763
00:46:36,480 --> 00:46:41,290
raise the resonance frequency of individual spring with one last on

764
00:46:41,290 --> 00:46:46,020
because remember this square is one and only god divided by because it also one

765
00:46:46,060 --> 00:46:49,850
but that's exactly the square root of k over

766
00:46:51,790 --> 00:46:54,910
this one will stance two

767
00:46:54,960 --> 00:46:58,180
and these two

768
00:46:58,210 --> 00:47:04,160
at roughly the same amplitude you could eyeballing looks like almost crosses over there

769
00:47:04,160 --> 00:47:05,440
and it's about

770
00:47:05,460 --> 00:47:07,140
is at zero

771
00:47:07,160 --> 00:47:10,250
because it's about minus one

772
00:47:10,310 --> 00:47:13,540
but i mean write it on the blackboard

773
00:47:13,580 --> 00:47:16,460
that's quite bizarre situations

774
00:47:16,480 --> 00:47:19,430
so that one frequency

775
00:47:19,430 --> 00:47:20,660
so omega

776
00:47:20,660 --> 00:47:24,750
because the square root of k over m

777
00:47:24,770 --> 00:47:27,460
i get see one zero

778
00:47:27,460 --> 00:47:33,390
NYC through about c three maybe even exactly c three i never checked that

779
00:47:33,390 --> 00:47:35,560
and that is roughly minus

780
00:47:35,560 --> 00:47:37,810
at the one at that

781
00:47:37,870 --> 00:47:40,250
and to see that

782
00:47:40,310 --> 00:47:41,830
it remains

783
00:47:41,830 --> 00:47:42,750
so here

784
00:47:42,770 --> 00:47:46,500
by take multiplying the one that you could take the role sounds and he already

785
00:47:46,500 --> 00:47:49,380
has the same so if you take the difference is zero

786
00:47:50,280 --> 00:47:52,910
think about it for a few minutes but

787
00:47:52,910 --> 00:47:54,970
it's pretty obvious that this thing has to be zero

788
00:47:55,110 --> 00:47:57,460
so what we know is

789
00:47:57,470 --> 00:47:59,920
you know all i are nonnegative

790
00:48:00,580 --> 00:48:02,100
the if if

791
00:48:02,120 --> 00:48:05,210
we have the smallest so the smallest angle we can have a zero and actually

792
00:48:05,210 --> 00:48:11,030
we have we have this angle zero and the corresponding eigenvectors are constant one victory

793
00:48:13,890 --> 00:48:16,450
now there is actually very nice relations

794
00:48:16,480 --> 00:48:19,170
already between the i vectors

795
00:48:19,210 --> 00:48:23,150
of the graph which i finally zero and the number of clusters

796
00:48:23,150 --> 00:48:25,390
and the relation is as follows

797
00:48:26,130 --> 00:48:30,110
of course the i'm the matrix could have the iron value zero several times so

798
00:48:30,110 --> 00:48:33,340
it could be that the in space is just does not only contain one that

799
00:48:33,420 --> 00:48:36,020
but contains several vectors

800
00:48:36,050 --> 00:48:40,260
and what turns out now is that there is actually only happens

801
00:48:40,270 --> 00:48:43,900
in the situation where graph contains of different disconnected parts

802
00:48:43,930 --> 00:48:46,050
so if f looks like this

803
00:48:46,110 --> 00:48:49,410
so we have two parts and there are no connections between each other

804
00:48:49,460 --> 00:48:51,740
then one can prove that

805
00:48:51,750 --> 00:48:55,840
the multiplicity of the zero will be exactly two

806
00:48:55,840 --> 00:49:00,700
because we have to connect the components and the iron vectors which correspond to this

807
00:49:00,760 --> 00:49:02,230
to those two parts

808
00:49:02,240 --> 00:49:03,750
are essentially

809
00:49:03,800 --> 00:49:08,670
like the indicator vectors of the parts so if one i so if the vertices

810
00:49:08,700 --> 00:49:12,910
or that this is the first group in the second before nine vector which looks

811
00:49:12,910 --> 00:49:14,100
like this

812
00:49:14,110 --> 00:49:15,610
and the second one

813
00:49:15,620 --> 00:49:17,650
looks like this

814
00:49:17,660 --> 00:49:21,110
and those trying to spend the space of five minutes

815
00:49:21,120 --> 00:49:22,770
and actually

816
00:49:22,790 --> 00:49:26,280
that already shows that the small eyeing vectors of

817
00:49:26,330 --> 00:49:29,670
from eight of the class matrix might have something to do with the

818
00:49:29,690 --> 00:49:33,670
number with with the clustering because we see that if if you would look at

819
00:49:33,670 --> 00:49:36,290
the first two i vectors of this matrix

820
00:49:36,360 --> 00:49:40,900
it is that they really contain the cluster indicator vectors of the two tools in

821
00:49:40,900 --> 00:49:42,920
our data

822
00:49:42,940 --> 00:49:46,760
and of course is only holds if there's not nice thing hear it gets more

823
00:49:46,760 --> 00:49:51,940
complicated than that what spectral clustering about but that's really the intuition or that's that's

824
00:49:51,940 --> 00:49:54,250
a very important observation

825
00:49:54,260 --> 00:49:56,890
the nice thing about this

826
00:49:56,890 --> 00:50:00,770
statement here is that the proof is actually very simple and very elegant and i

827
00:50:00,770 --> 00:50:01,860
like it or not

828
00:50:01,880 --> 00:50:04,460
and we would do it exercise

829
00:50:09,840 --> 00:50:10,860
OK so

830
00:50:12,600 --> 00:50:14,870
the matrix and we've looked at so far is

831
00:50:14,880 --> 00:50:19,010
what is called the unnormalized laplacian so is one of the world normalised so far

832
00:50:19,010 --> 00:50:22,600
because we didn't have any anomalous once but now we introduce two other versions of

833
00:50:22,600 --> 00:50:25,110
love thousands which are normalized graph laplacians

834
00:50:25,120 --> 00:50:27,670
so this would be the unnormalized version

835
00:50:27,680 --> 00:50:30,120
and essentially we will see that later

836
00:50:30,140 --> 00:50:32,240
we need many this one

837
00:50:32,380 --> 00:50:37,820
so the normalisation what it essentially does is it normalizes matrix such that the iron

838
00:50:37,840 --> 00:50:41,460
vectors are the angle is always in the same range

839
00:50:41,500 --> 00:50:42,670
which is not

840
00:50:42,680 --> 00:50:44,620
the case for the

841
00:50:44,660 --> 00:50:46,540
the unnormalized laplacian

842
00:50:46,720 --> 00:50:49,710
and many workers this matrix

843
00:50:49,870 --> 00:50:54,460
where we normalized by multiplying the numbers of people to the front of

844
00:50:54,590 --> 00:50:58,450
and if you multiply also out so you have l and i put e to

845
00:50:58,450 --> 00:50:59,990
the minus one front

846
00:51:00,070 --> 00:51:04,030
and here it can so that gets the identity matrix here

847
00:51:04,080 --> 00:51:05,580
if qf t

848
00:51:05,590 --> 00:51:09,850
e to the minus one times as so that part here

849
00:51:11,370 --> 00:51:14,770
and the reason why i put this index are our reitz

850
00:51:14,790 --> 00:51:18,820
because it's so it stands for a random walk so

851
00:51:18,860 --> 00:51:21,980
but for those of you know what the random open graph is this is the

852
00:51:21,980 --> 00:51:23,820
transition matrix of

853
00:51:23,960 --> 00:51:29,170
so who actually knows whether random walkers

854
00:51:29,910 --> 00:51:34,650
and going to the later so for those who know that's the relation with the

855
00:51:36,430 --> 00:51:40,820
OK and there's also another normalisation but i don't want to talk about this actually

856
00:51:40,820 --> 00:51:41,970
the lecture

857
00:51:41,990 --> 00:51:46,230
just take it there's another normalisation don't

858
00:51:46,240 --> 00:51:49,620
but the demons won in front but we multiply from both sets we e to

859
00:51:49,620 --> 00:51:51,090
the minus one half

860
00:51:51,270 --> 00:51:55,890
the advantages of this matrix is still symmetric the matrix here is not symmetric

861
00:51:55,920 --> 00:51:56,820
but it

862
00:51:56,830 --> 00:52:02,510
forget sort of so what is important is that the properties of the random of

863
00:52:02,510 --> 00:52:04,390
both matrices are actually very

864
00:52:04,440 --> 00:52:09,440
close to the other ones also both positive semidefinite and both have the properties that

865
00:52:11,120 --> 00:52:14,890
this property about the number of clusters corresponds to the first to the

866
00:52:14,900 --> 00:52:23,960
class to the multiplicity of the first i mean that

867
00:52:23,980 --> 00:52:26,210
OK now i want to introduce

868
00:52:26,230 --> 00:52:27,570
i want to show

869
00:52:28,730 --> 00:52:35,870
the rationality OK so i think there

870
00:52:35,870 --> 00:52:39,600
that's a very good question so

871
00:52:39,610 --> 00:52:42,590
maybe we see that later i mean i will show later on by this i

872
00:52:42,590 --> 00:52:45,660
mean right now it's a mystery i know so i will try to solve the

873
00:52:45,660 --> 00:52:53,200
mystery later i think the most important point is really this this property we had

874
00:52:55,830 --> 00:53:00,440
this one you have transpose frequent some as j

875
00:53:00,450 --> 00:53:05,290
i'm minus j because somehow it is

876
00:53:05,300 --> 00:53:10,510
it is so machine learning it is often seen as a regularizer because punishers function

877
00:53:10,520 --> 00:53:13,830
on the graph there is a lot on what parts of the graph is closely

878
00:53:15,310 --> 00:53:19,490
i mean there are very many really i mean there's a few call spectral graph

879
00:53:19,490 --> 00:53:24,440
theory which does nothing but look at different matrices and how those properties of matrices

880
00:53:24,440 --> 00:53:26,150
are related to the graph

881
00:53:26,200 --> 00:53:30,630
and often this have because then you can use linear algebra things tool for example

882
00:53:30,640 --> 00:53:34,000
compute how many comic connected component the graph has or

883
00:53:34,010 --> 00:53:37,650
it's just a different view of looking graphs i think

884
00:53:37,690 --> 00:53:42,940
and this one is just the matrix which turns out somehow to be very convenient

885
00:53:42,940 --> 00:53:45,900
but it's nothing you can see on it's not obvious in the first glance it's

886
00:53:45,900 --> 00:53:46,840
ready to go

887
00:53:46,850 --> 00:53:50,270
the printer

888
00:53:50,270 --> 00:53:51,220
determine how

889
00:53:51,240 --> 00:53:54,620
how is explicit for link is contributing to the

890
00:53:54,650 --> 00:53:57,670
in training squad

891
00:53:57,680 --> 00:54:00,750
so this it

892
00:54:02,300 --> 00:54:09,550
well you need to have a training data very know course

893
00:54:09,570 --> 00:54:14,070
i don't need to know where

894
00:54:14,080 --> 00:54:23,110
and it and it so i feel all i mean for every position there

895
00:54:23,140 --> 00:54:27,240
we are we have to search the whole way back essentially this this could be

896
00:54:27,240 --> 00:54:31,060
as long as the DNA maybe there a limit on the length but still you

897
00:54:31,060 --> 00:54:35,130
can do you think i mean you can take this is not going to be

898
00:54:38,880 --> 00:54:42,670
it was down to if you have a

899
00:54:42,680 --> 00:54:44,240
in model

900
00:54:44,680 --> 00:54:50,850
from this point on this quality constant and you only pay only penalized by the

901
00:54:50,880 --> 00:54:55,630
by the point i mean by a factor of many look at how wait so

902
00:54:55,630 --> 00:55:00,060
it's not like can allow maybe hundred k entrance here

903
00:55:00,080 --> 00:55:05,950
maybe from that point on on your intron link model is constant and maybe

904
00:55:06,220 --> 00:55:10,010
you pay only

905
00:55:10,270 --> 00:55:15,030
so it's a bit but if you don't have infinite model then you don't pay

906
00:55:15,050 --> 00:55:19,880
and all for the link

907
00:55:19,950 --> 00:55:25,630
OK so how can we parametrize and so here we have a substitution matrix this

908
00:55:25,630 --> 00:55:33,300
is simply bipartite metrics that four letters and the deletion like so we have a

909
00:55:33,380 --> 00:55:37,930
duty of the mind so very like conditions deletion

910
00:55:38,070 --> 00:55:42,040
so this is essentially twenty five primary

911
00:55:42,060 --> 00:55:47,510
we also have these people function here of the length of the building and we

912
00:55:48,920 --> 00:55:50,090
i mean it

913
00:55:50,100 --> 00:55:55,050
i had a simple choice you be model that as a piecewise linear function

914
00:55:55,090 --> 00:55:59,940
so it's quite simple to think about this choice is that all the time is

915
00:56:01,310 --> 00:56:07,080
so we don't need cuts support doing this and then estimate one-dimensional functions piecewise linear

916
00:56:07,080 --> 00:56:08,550
functions on

917
00:56:08,630 --> 00:56:10,840
but then it is

918
00:56:10,860 --> 00:56:17,330
so if you think twenty or thirty support points but these things

919
00:56:17,370 --> 00:56:23,530
this just formula with the end we have prime directive which consists of these four

920
00:56:23,530 --> 00:56:26,060
point is working on

921
00:56:26,070 --> 00:56:28,940
and this substitution

922
00:56:28,970 --> 00:56:30,760
so i for given beta

923
00:56:30,770 --> 00:56:33,720
you can now compute the alignment score

924
00:56:34,090 --> 00:56:35,910
four certain lines

925
00:56:39,170 --> 00:56:43,650
so how can we optimize and how can you find these parameters

926
00:56:43,670 --> 00:56:50,190
the idea again i would like to find parameters such that for non alignment good

927
00:56:50,190 --> 00:56:53,140
alignment is much larger than what we need

928
00:56:53,150 --> 00:56:54,340
from like

929
00:56:55,310 --> 00:56:57,790
so and given in training examples

930
00:56:57,800 --> 00:57:01,810
we have to solve this but i think the vision problem is that we would

931
00:57:01,810 --> 00:57:07,410
like to find the parameters theta such that will be of good alignment is greater

932
00:57:07,410 --> 00:57:08,990
than any problem

933
00:57:09,070 --> 00:57:15,010
and he just have the same problem exponentially many constraints and we have this iterative

934
00:57:15,010 --> 00:57:17,950
algorithm which can solve the optimisation

935
00:57:17,980 --> 00:57:22,390
objectives might not

936
00:57:22,410 --> 00:57:23,980
this is clear

937
00:57:23,990 --> 00:57:26,050
so it's

938
00:57:26,100 --> 00:57:30,890
so we we have a function which is computing the

939
00:57:30,940 --> 00:57:33,330
alignment score for a particular line

940
00:57:33,340 --> 00:57:36,370
OK so this function is linear in the

941
00:57:36,450 --> 00:57:39,050
therefore these constraints are going to be linear

942
00:57:39,090 --> 00:57:41,140
therefore this is just

943
00:57:42,090 --> 00:57:46,840
but i think this problem we have maybe somebody comes billion years

944
00:57:46,880 --> 00:57:53,550
and just how many constrained by iterating into it i can solve the

945
00:57:53,590 --> 00:57:56,990
what we get in the end get parameter settings

946
00:57:57,000 --> 00:58:00,770
which are good for predicting

947
00:58:01,240 --> 00:58:04,080
the correct line

948
00:58:05,770 --> 00:58:09,630
make sure you simulation example

949
00:58:09,640 --> 00:58:14,900
so we need to be considered excellent results so these are real sequences which we

950
00:58:14,900 --> 00:58:22,990
took from some database for four and we consider like sequence of px

951
00:58:23,040 --> 00:58:30,220
and this what they told by blood and some quite strict alignment filtering

952
00:58:30,450 --> 00:58:33,320
so then we cut some sequences

953
00:58:33,330 --> 00:58:37,000
to make the problem in a more typical case we had

954
00:58:37,010 --> 00:58:41,670
out the x cut some sequences to make the next one really short to be

955
00:58:42,520 --> 00:58:46,860
between two with fifty nucleotides long uniformly distributed

956
00:58:47,120 --> 00:58:50,840
the good thing about this kind of thing is that this place not really affected

957
00:58:51,060 --> 00:58:56,320
so to ensure that the fact that most of the information about the license

958
00:58:56,330 --> 00:58:59,830
OK so now we have artificially generated a

959
00:58:59,830 --> 00:59:03,890
so to do this we now publisher assumptions on the risk on the covariance which

960
00:59:03,890 --> 00:59:08,020
is what we're all always trying to avoid regression to this never felt right to

961
00:59:08,020 --> 00:59:13,300
me has kind of a methodology right but there is a big literature on it

962
00:59:13,400 --> 00:59:18,140
his user you know its practical applications of this definitely want to kind of

963
00:59:18,180 --> 00:59:22,460
graphs graphs graphical approaches to regression is this

964
00:59:22,550 --> 00:59:25,580
so a little bit more formally here's our regression

965
00:59:25,600 --> 00:59:29,220
now otherwise is the response we have an initial covariance the goals define what's called

966
00:59:29,220 --> 00:59:33,760
the central subspace and it just defined by saying well i suppose i could write

967
00:59:33,760 --> 00:59:35,470
the regression

968
00:59:35,690 --> 00:59:41,140
as just a problem distribution of y given the projection of x onto some matrix

969
00:59:42,650 --> 00:59:46,390
so although i'm going to project on to be was straw that here

970
00:59:46,390 --> 00:59:50,090
the one dimensional so here's my accent that can be

971
00:59:50,100 --> 00:59:51,620
i get to be

972
00:59:52,030 --> 00:59:57,670
transfer here and maybe once i know that i can predict y just as well

973
00:59:57,670 --> 01:00:00,830
as five use text itself by the conditional independence

974
01:00:00,840 --> 01:00:06,590
of x and y conditioning on the transpose x

975
01:00:06,600 --> 01:00:09,290
so far to find such a b

976
01:00:09,290 --> 01:00:13,580
i've kind of managed to project and not lose any information

977
01:00:13,640 --> 01:00:17,340
OK so that's what defined to be the central subspace is this matrix b

978
01:00:17,390 --> 01:00:23,560
the subspace defined by projection based gerbil example of that if you have logistic regression

979
01:00:23,600 --> 01:00:27,510
in x one x according to look like that i then if you were to

980
01:00:27,510 --> 01:00:30,890
just look at the x one corner project and the next one chord that you

981
01:00:30,890 --> 01:00:35,020
get perfectly good regression and you would have lost in information if you're projecting next

982
01:00:35,090 --> 01:00:40,390
quarter you've lost information so thirty one issues the central subspace bx one axis

983
01:00:40,400 --> 01:00:44,750
right PCA of course kids project with state in the x one x two space

984
01:00:44,940 --> 01:00:48,480
the PCA principal axes may have nothing to do with x one just depends on

985
01:00:48,480 --> 01:00:52,180
the variance scale is which is related to x one

986
01:00:52,220 --> 01:00:57,330
OK so there's tons of methods they're all interesting icon to regression particular is a

987
01:00:57,330 --> 01:00:59,690
whole bunch of very clever ideas going on here

988
01:00:59,730 --> 01:01:02,330
and i want to talk i would get into these little bit just tell you

989
01:01:02,330 --> 01:01:06,390
that there are all interesting clever and there's this all publicity analytical theory for all

990
01:01:06,400 --> 01:01:09,060
these under certain assumptions these all work

991
01:01:09,100 --> 01:01:12,520
OK this assumptions are too strong however is get into a major is a linear

992
01:01:12,520 --> 01:01:17,460
sum shorthand and even assumption or most painfully elliptic assumption on the distribution of x

993
01:01:17,460 --> 01:01:21,830
is a very common assumption is imposed to just doesn't feel right to me i

994
01:01:21,830 --> 01:01:26,680
don't imagine political they arise all that often in the x space not for regression

995
01:01:26,680 --> 01:01:29,800
OK so we'd like to get out of that kind of a kind of web

996
01:01:29,800 --> 01:01:31,090
of assumptions

997
01:01:31,100 --> 01:01:35,960
right so now be really how this relates to conditional independence member b is our

998
01:01:35,960 --> 01:01:40,550
projection matrix so we take axe let's fill it out of they the orthogonal complement

999
01:01:40,550 --> 01:01:45,120
of that c has or called to be and then define you to be the

1000
01:01:45,120 --> 01:01:50,000
good stuff the projection and what we want to retain the VDB everything else

1001
01:01:50,000 --> 01:01:55,300
OK so the statement that begins the projectron the central subspace is equivalent to saying

1002
01:01:55,300 --> 01:01:58,330
that the conditional distribution of y given x

1003
01:01:58,350 --> 01:02:03,180
it's the same probably why given the transpose access the definition

1004
01:02:03,180 --> 01:02:07,840
i and that's the same as saying that this conditional distribution of y given

1005
01:02:07,900 --> 01:02:12,630
both components you would be light to reconstruct axis is the same as saying this

1006
01:02:12,640 --> 01:02:16,900
because you is equal to be transpose x for all y u and v

1007
01:02:16,940 --> 01:02:20,680
and that's just a way to express conditional independence of y and v given the

1008
01:02:20,680 --> 01:02:22,390
projection onto u

1009
01:02:23,270 --> 01:02:26,810
right so that's if you want to define the matrix b does this is equivalent

1010
01:02:26,810 --> 01:02:30,680
to find the matrix b that achieves conditional independence

1011
01:02:30,850 --> 01:02:33,840
OK so now you've got to do is find a way to express conditional independence

1012
01:02:33,840 --> 01:02:37,290
an objective function that you can go downhill in terms of b

1013
01:02:37,300 --> 01:02:41,090
so how do you express conditional independence is objective function that was our task that

1014
01:02:41,090 --> 01:02:45,050
we set out to try to solve in this line of work

1015
01:02:47,430 --> 01:02:54,020
right so we we did this using a nonparametric tools so let's come back to

1016
01:02:54,020 --> 01:02:58,020
you know we learned about represent progress faces by the work of many other people

1017
01:02:58,020 --> 01:02:59,380
to give a talk on this

1018
01:02:59,390 --> 01:03:03,470
and we noticed this is basically just a way to get the basis functions and

1019
01:03:03,470 --> 01:03:08,680
then competition efficient space without well the main some richer users of these spaces we

1020
01:03:08,680 --> 01:03:12,630
can maybe express operators on these spaces and look at those classic operators and see

1021
01:03:12,640 --> 01:03:16,680
those are interesting not just the functions in the space itself with the operators

1022
01:03:16,720 --> 01:03:22,040
and maybe they can use to go to lead us towards non supervised methods so

1023
01:03:22,040 --> 01:03:25,010
that's kind of what this talk really ended up with this line of research in

1024
01:03:25,010 --> 01:03:30,590
that have been the started with something called kernel ICA

1025
01:03:30,600 --> 01:03:32,840
and then when in this in this direction

1026
01:03:32,890 --> 01:03:38,340
OK so we could have and we're going to talk about conditional independence we use

1027
01:03:38,340 --> 01:03:43,930
represent progress was to characterize both independence and conditional independence and so we actually end

1028
01:03:43,930 --> 01:03:46,550
up to talk to talk about these kind of things we need to talk about

1029
01:03:46,550 --> 01:03:50,800
independence we need to speak to represent problems spaces one here one here two the

1030
01:03:50,800 --> 01:03:54,390
original data space acts another data space y

1031
01:03:54,430 --> 01:03:58,890
and they're going to map into the corresponding representing the kernel hilbert spaces and then

1032
01:03:58,890 --> 01:04:02,600
we'll talk about the relation among data points pairs of data in these two spaces

1033
01:04:05,810 --> 01:04:09,470
OK so you already had talk and rk stages i want to belabor this

1034
01:04:09,520 --> 01:04:12,430
the main thing about our gives you this kernel function

1035
01:04:12,430 --> 01:04:17,510
and the kernel has two arguments and if you fix the second arguement and this

1036
01:04:17,510 --> 01:04:20,680
is now a function of the first arguement that is an object that lies in

1037
01:04:20,680 --> 01:04:24,840
the rip curl curl her space and the inner product of another function in space

1038
01:04:24,840 --> 01:04:28,510
you just get evaluation is called the reproducing property i hope you've seen that somewhere

1039
01:04:32,230 --> 01:04:38,100
OK so now if you take into products of sophie is this mapping of for

1040
01:04:38,100 --> 01:04:40,390
you take a point xt mapping the function

1041
01:04:40,440 --> 01:04:44,880
that were the second arguement the kernels pin down so that's that's called give access

1042
01:04:44,900 --> 01:04:49,290
a feature map if you will and he now take into products of fever one

1043
01:04:49,290 --> 01:04:54,350
data point the for another data point that just amounts to kernel evaluation and you

1044
01:04:54,350 --> 01:05:00,960
can actually expand the basis functions are given by the kernel evaluated at particular points

1045
01:05:01,010 --> 01:05:03,430
i just kind of basic kernel methods

1046
01:05:03,440 --> 01:05:06,510
now we're not not talk about the kind of the points in the space but

1047
01:05:06,510 --> 01:05:09,540
the actual operators on the reproducing kernel hilbert space

1048
01:05:09,580 --> 01:05:11,170
OK the operators

1049
01:05:11,180 --> 01:05:16,590
that's the way you do this is order to define covariance operators and so you

1050
01:05:16,590 --> 01:05:21,120
look like at random variables on on the august eight you take that kernel function

1051
01:05:21,230 --> 01:05:24,300
and you can have the second arguement actually that is random and so we have

1052
01:05:24,300 --> 01:05:26,220
a random function now for every

1053
01:05:26,270 --> 01:05:31,300
for every choice of x we get different functions is random function

1054
01:05:32,190 --> 01:05:36,150
so that we can talk about covariance operators which are to give you guys i

1055
01:05:36,150 --> 01:05:39,800
covariances on on functions on function space

1056
01:05:39,800 --> 01:05:43,450
want to compute two types of things you want to compute predictions if you want

1057
01:05:43,450 --> 01:05:48,010
to know what this function do in other places in places where we didn't measure

1058
01:05:49,400 --> 01:05:53,170
so so to do prediction so again we use the likelihood function which was the

1059
01:05:53,170 --> 01:05:58,020
likelihood function say about those new predictions given particular weights and then we average over

1060
01:05:58,020 --> 01:06:02,910
the posterior distribution always try to have a high posterior somewhere then you get a

1061
01:06:02,910 --> 01:06:06,500
large the you get lucky the large weight to those predictions and if you have

1062
01:06:06,500 --> 01:06:10,950
a low posterior than those predict the prediction coming out of that particular setting of

1063
01:06:10,950 --> 01:06:16,480
the model would influence the actual predictions so we've seen this before

1064
01:06:16,500 --> 01:06:19,420
and another thing you might be able to might be interested in computing is the

1065
01:06:19,430 --> 01:06:25,180
is the marginal likelihood much like it was important for model selection talk much more

1066
01:06:25,180 --> 01:06:30,430
about that later on and again the marginal likelihood is is is

1067
01:06:30,430 --> 01:06:35,720
is that you get that by marginalizing over the distribution of the weight so you

1068
01:06:35,720 --> 01:06:39,410
can see the interesting things that you want to compute actually computed by doing integrals

1069
01:06:39,410 --> 01:06:40,910
over the over the weights

1070
01:06:42,050 --> 01:06:46,400
so this is where this is very always happens in

1071
01:06:46,420 --> 01:06:48,050
in parametric models

1072
01:06:48,060 --> 01:06:53,220
you can sort of see that there's something a little bit funny about the right

1073
01:06:53,220 --> 01:06:55,280
we're not really interested in the weights

1074
01:06:55,300 --> 01:07:00,930
the weights are sometimes called nuisance parameters right in these kinds of models usually uninterested

1075
01:07:00,930 --> 01:07:03,280
in the way you just want to know what other the predictions

1076
01:07:04,210 --> 01:07:10,040
so sometimes if you have a parametric model you might actually be interested in what

1077
01:07:10,040 --> 01:07:13,080
is the value of one of the parameters when we talk about the case when

1078
01:07:13,330 --> 01:07:17,180
i'm talking about you know complex nonlinear relationships and you want to know you know

1079
01:07:17,180 --> 01:07:22,390
what is the prediction not interested in the weight so the way gaussianprocess process models

1080
01:07:22,390 --> 01:07:26,220
works is it explicitly says well the prior here

1081
01:07:26,710 --> 01:07:30,820
notice that the posterior has nothing but the likelihood times the prior is proportional to

1082
01:07:30,820 --> 01:07:36,290
the likelihood and the prior right so somehow the prior is here prior on weights

1083
01:07:36,290 --> 01:07:41,430
right but the way it works is it induces a prior on the functions that

1084
01:07:41,780 --> 01:07:45,790
if you set the weights to something particular then you get a particular function right

1085
01:07:45,790 --> 01:07:49,380
so if you have a distribution over those weights then you get a distribution over

1086
01:07:50,280 --> 01:07:54,760
so what we're going to do differently here is we're gonna it instead of going

1087
01:07:54,760 --> 01:07:56,290
wide by the weights

1088
01:07:56,340 --> 01:07:57,900
which were not interested in

1089
01:07:57,910 --> 01:08:03,160
we're going to go directly for distribution over the things that were interested in over

1090
01:08:03,160 --> 01:08:04,360
the predictions

1091
01:08:04,420 --> 01:08:08,150
OK and in fact this is going to turn out turn out to be much

1092
01:08:08,150 --> 01:08:13,790
simpler than going by this two-step procedure introducing model weights and then getting rid of

1093
01:08:13,790 --> 01:08:14,510
them again

1094
01:08:15,810 --> 01:08:21,520
so you might think that that the way to to actually talk to get good

1095
01:08:21,520 --> 01:08:26,890
results here is to look at complicated functions to dream up you know complicated nonlinear

1096
01:08:26,890 --> 01:08:30,660
functions about of ways here to be able to do

1097
01:08:30,680 --> 01:08:36,300
you know very bad things and dream complicated priors on the weights and try to

1098
01:08:36,300 --> 01:08:37,280
somehow do

1099
01:08:37,290 --> 01:08:42,140
these integrals and what i'm going to try and persuade you today is that that's

1100
01:08:42,140 --> 01:08:46,160
not the way to do it but the way to do it is different and

1101
01:08:46,420 --> 01:08:50,690
it turns out that just involves gauss in distribution

1102
01:08:51,290 --> 01:08:54,030
before i can tell you about that let's make sure

1103
01:08:54,230 --> 01:08:56,090
we're talking about the same gaussians

1104
01:08:56,160 --> 01:09:02,040
so the gas distribution has location parameter means and has

1105
01:09:02,080 --> 01:09:04,320
a standard standardisation or variance

1106
01:09:04,330 --> 01:09:10,460
two examples here the two gas distributions in one dimension here

1107
01:09:10,650 --> 01:09:15,020
and over here i've i've drawn a picture of the gas industry mission two dimensions

1108
01:09:15,070 --> 01:09:20,080
in particular is the correlated strongly anti correlated to what i drawn here by the

1109
01:09:20,080 --> 01:09:25,050
two variables on this axis here and this is a good probability contours so there's

1110
01:09:25,050 --> 01:09:29,550
a high probability inside this region and lower probability as you move out the probability

1111
01:09:29,550 --> 01:09:33,710
goes down very rapidly in this direction but got goes down much more slowly in

1112
01:09:33,710 --> 01:09:34,820
this direction

1113
01:09:34,830 --> 01:09:38,190
so what i'm going to use on the talk here i'm going to use gas

1114
01:09:38,330 --> 01:09:41,900
distributions that live in high dimensional spaces

1115
01:09:41,910 --> 01:09:46,470
OK unfortunately i can only draw a two-dimensional gas and we have to try and

1116
01:09:46,470 --> 01:09:51,180
think of what do these distributions look like in much much higher dimensions right and

1117
01:09:51,840 --> 01:09:53,880
going to be part of the challenge

1118
01:09:54,570 --> 01:09:57,190
so i so i can draw those things

1119
01:09:59,110 --> 01:10:03,330
what can you do with gaston in distributions well you can you can condition gauss

1120
01:10:03,340 --> 01:10:10,090
in distribution so conditional distribution means that if i now know somehow that the value

1121
01:10:10,090 --> 01:10:15,840
of this variable has this particular the value of this variable sorry has this particular

1122
01:10:15,840 --> 01:10:20,200
value then what does that tell me about the all the variables right and of

1123
01:10:20,200 --> 01:10:23,610
course if the gas in correlated like this that tells me quite a lot in

1124
01:10:23,610 --> 01:10:27,470
particular tells me if i had this value value here is very unlikely that my

1125
01:10:27,480 --> 01:10:32,930
all variable would have would have values over here in fact the conditional distribution of

1126
01:10:32,930 --> 01:10:37,310
the other variables given that this variable is set to this value is again calsyntenin

1127
01:10:37,320 --> 01:10:39,590
centered over here

1128
01:10:40,520 --> 01:10:46,270
so that's conditioning and nothing we can do is marginalizations of marginalization just means something

1129
01:10:46,270 --> 01:10:51,250
out or integrating out some of the of the variables and looking at just the

1130
01:10:51,250 --> 01:10:55,590
distance the marginal distribution of the other variables right and here again in this two-dimensional

1131
01:10:55,910 --> 01:11:01,090
example here i just marginalize this joint galson by summing in this direction and when

1132
01:11:01,090 --> 01:11:06,260
i get back is again gas distribution turns out to be crucial that we can

1133
01:11:06,260 --> 01:11:12,100
do both of these conditioning and calcium in modernization and the resources of of those

1134
01:11:12,100 --> 01:11:14,310
operations are again gaussians

1135
01:11:14,320 --> 01:11:20,360
i've also given you the equations here so if i have this if i have

1136
01:11:20,360 --> 01:11:26,020
to just joined as distribution over x and y here with mean mean vector which

1137
01:11:26,020 --> 01:11:32,430
contains a and b and the covariance which can be schematically written like this then

1138
01:11:32,430 --> 01:11:34,500
i've given you hear equations for

1139
01:11:35,420 --> 01:11:39,610
the marginal distribution of one of the variables of the conditional distribution of one of

1140
01:11:39,610 --> 01:11:45,170
the variables given the other variables but let's not look too much of algebra right

1141
01:11:45,170 --> 01:11:51,060
but the slide is in there if you want to look at it later on

1142
01:11:51,090 --> 01:11:53,730
OK so now

1143
01:11:53,770 --> 01:11:57,800
what about what about the casting process is one of the gaussianprocess i'm going be

1144
01:11:57,800 --> 01:11:59,810
talk about distribution so far

1145
01:12:00,080 --> 01:12:06,590
so that's the process is just a generalization of gas distribution to infinitely many many

1146
01:12:07,550 --> 01:12:10,910
and the reason why we want to do that is we want to do inference

1147
01:12:10,910 --> 01:12:12,140
about functions

1148
01:12:12,300 --> 01:12:16,250
so we have to be able to put probability distributions over functions

1149
01:12:17,240 --> 01:12:20,580
this is sort of an informal analogy here we can think of an infinitely long

1150
01:12:20,580 --> 01:12:24,310
vector we can think about is being function right if i let's think about a

1151
01:12:24,310 --> 01:12:29,140
one-dimensional function if i want to specify what the one-dimensional function is i just specify

1152
01:12:29,240 --> 01:12:31,550
what is the function value for each input

1153
01:12:31,570 --> 01:12:34,480
OK then i specified what the function

1154
01:12:34,520 --> 01:12:38,720
the problem might be that there are infinitely many possible inputs to the function right

1155
01:12:38,720 --> 01:12:40,110
so the vector

1156
01:12:40,130 --> 01:12:42,050
will become very very long

1157
01:12:42,050 --> 01:12:45,380
OK but let's not worry about that for the time being

1158
01:12:45,970 --> 01:12:47,970
mathematically this the

1159
01:12:48,490 --> 01:12:53,350
function is sort of like anything long vector is not mathematically very precise turns out

1160
01:12:53,350 --> 01:12:56,760
to be precise enough for what i'm going to the way i'm going to use

1161
01:12:57,800 --> 01:13:03,270
so we simply think of functions as being infinitely long vectors OK so of course

1162
01:13:03,270 --> 01:13:07,960
you might start getting an uncomfortable feeling now because well i can write down these

1163
01:13:07,960 --> 01:13:11,100
things in my computer of course why because only has finite memory so i better

1164
01:13:11,100 --> 01:13:13,800
not try to do the right and i will try to do that

1165
01:13:13,820 --> 01:13:17,640
OK so somehow will be able to do what we need to do without writing

1166
01:13:17,640 --> 01:13:19,810
down those vectors

1167
01:13:19,810 --> 01:13:23,690
for the root mean square error or in this case even simpler you don't have

1168
01:13:23,690 --> 01:13:27,290
any intention section you have the integral of

1169
01:13:27,300 --> 01:13:30,790
of this area

1170
01:13:30,810 --> 01:13:35,070
so now for the next step our supervised that is

1171
01:13:36,730 --> 01:13:43,310
matching procedure where we want to map the detected lines to the mode number small

1172
01:13:43,310 --> 01:13:47,620
numbers starting by one two in our case twenty five

1173
01:13:47,630 --> 01:13:49,870
some might not be there

1174
01:13:50,960 --> 01:13:56,600
it might be the only ten modes have been identified by the engineer and annotated

1175
01:13:56,640 --> 01:14:00,480
is possible and so we have three strategies for this

1176
01:14:00,570 --> 01:14:03,230
the priority matching their you match

1177
01:14:03,350 --> 01:14:05,090
you're detected line

1178
01:14:05,110 --> 01:14:09,480
but in the order of the detection so you you first attacked line with the

1179
01:14:09,480 --> 01:14:12,880
highest values so the strongest line in the picture

1180
01:14:12,980 --> 01:14:16,730
so we could do this or you do agree the best fit

1181
01:14:16,740 --> 01:14:18,620
so you the line

1182
01:14:18,630 --> 01:14:21,820
we had between line area is minimized

1183
01:14:21,840 --> 01:14:24,640
and that's right to use the look research

1184
01:14:24,700 --> 01:14:27,390
which you could do on top of one of them

1185
01:14:28,380 --> 01:14:30,840
the first approaches

1186
01:14:30,850 --> 01:14:33,330
you could swap the assignment

1187
01:14:33,740 --> 01:14:36,350
in order to minimize you're our most

1188
01:14:36,370 --> 01:14:38,450
in this respect

1189
01:14:38,490 --> 01:14:41,970
and we have a data set a real life datasets

1190
01:14:41,980 --> 01:14:43,670
from right to voice

1191
01:14:43,680 --> 01:14:48,410
and we have two hundred and one campbell diagrams from

1192
01:14:48,450 --> 01:14:54,440
fourteen vibration test coming from one engine and we have some e if e pretty

1193
01:14:55,050 --> 01:15:00,370
iffy predictions and some level i which we used in our libraries

1194
01:15:00,430 --> 01:15:01,930
and we

1195
01:15:01,970 --> 01:15:05,520
performed three experiments one only four

1196
01:15:05,540 --> 01:15:10,520
assessment of the quality of the line detection so nothing to do with the assignment

1197
01:15:10,520 --> 01:15:12,000
of mode number

1198
01:15:12,010 --> 01:15:18,670
and then two experiments to assess the quality of our matching

1199
01:15:20,170 --> 01:15:25,340
so and dimension we of course detect light new lines which has had are not

1200
01:15:25,340 --> 01:15:29,890
in the background model not in the SP model model and not in the lab

1201
01:15:29,890 --> 01:15:33,970
model and they are not annotated and so we cannot ever evaluate them at the

1202
01:15:33,970 --> 01:15:38,240
moment we can because we don't have any ground truth for them so it just

1203
01:15:41,070 --> 01:15:46,970
so our first experiment to assess the quality of our line detection approach

1204
01:15:47,020 --> 01:15:49,810
so what we did is we

1205
01:15:50,110 --> 01:15:53,830
met our detected lives against annotation

1206
01:15:53,840 --> 01:16:00,350
because you want to calculate the integral of the and annotated line and the fact

1207
01:16:00,350 --> 01:16:01,390
that line

1208
01:16:01,470 --> 01:16:03,700
and and what you see here is

1209
01:16:05,460 --> 01:16:10,020
approach so our have transform with background knowledge using

1210
01:16:10,430 --> 01:16:13,350
the last they that background knowledge

1211
01:16:14,410 --> 01:16:20,120
what you see here is now the are you compare the annotated lines

1212
01:16:20,250 --> 01:16:22,250
and you could compare

1213
01:16:22,260 --> 01:16:27,540
this to the last model the last model is only predicting exactly what is in

1214
01:16:27,540 --> 01:16:32,190
the left data so if the left in the last few measure frequency of two

1215
01:16:32,190 --> 01:16:37,510
thousand just reading this without any machine learning nothing the problem here in general is

1216
01:16:37,510 --> 01:16:40,600
that you have only a few

1217
01:16:40,640 --> 01:16:42,010
let measurements

1218
01:16:42,750 --> 01:16:44,180
components and

1219
01:16:44,750 --> 01:16:49,440
so you could have per capita diagram only two months

1220
01:16:49,450 --> 01:16:51,890
of out of twenty five

1221
01:16:51,950 --> 01:16:55,770
so that means here we see that our

1222
01:16:56,320 --> 01:16:58,320
approach is quite good

1223
01:16:59,090 --> 01:17:00,890
we have only

1224
01:17:01,660 --> 01:17:05,220
comm is in pixel would be on the y axis

1225
01:17:05,240 --> 01:17:09,740
about two point five pixel difference in

1226
01:17:09,750 --> 01:17:11,840
on average on all images

1227
01:17:11,850 --> 01:17:16,910
from the two annotation is quite small it means our

1228
01:17:16,930 --> 01:17:21,640
i remember quite accurate in finding the right lines

1229
01:17:21,660 --> 01:17:26,010
we are seeing here is where we look at the st

1230
01:17:26,040 --> 01:17:27,860
model as the background

1231
01:17:27,870 --> 01:17:34,050
information we could compare that to the SVM model here and model has more prediction

1232
01:17:34,050 --> 01:17:35,600
so more lines

1233
01:17:35,610 --> 01:17:39,100
but it's much more inaccurate myself

1234
01:17:39,110 --> 01:17:44,110
so but still you see there is a different height difference between the two

1235
01:17:44,130 --> 01:17:45,680
so it has

1236
01:17:45,720 --> 01:17:47,470
two use our

1237
01:17:47,510 --> 01:17:49,520
model instead of only predict

1238
01:17:49,550 --> 01:17:54,930
predicting the if if the lines

1239
01:17:55,000 --> 01:17:57,800
the second experiment is

1240
01:17:57,840 --> 01:18:03,590
where we want to evaluate our metric

1241
01:18:03,680 --> 01:18:07,380
procedure so we match against SVM model

1242
01:18:07,440 --> 01:18:12,100
we detect aligned with our approach and then we will make it to the if

1243
01:18:12,110 --> 01:18:16,170
the model not to the annotation model

1244
01:18:16,210 --> 01:18:21,000
and this year we have only SVM data so we assume that there was no

1245
01:18:21,810 --> 01:18:28,090
measurements and we can predict four hundred seventeen lines so the recall you will see

1246
01:18:28,090 --> 01:18:31,710
is higher because if the model contains more

1247
01:18:32,870 --> 01:18:38,120
to achieve this performance by using the priority match which was our first

1248
01:18:39,710 --> 01:18:44,160
and we see here that our method has lower

1249
01:18:44,180 --> 01:18:45,800
our embassy which is good

1250
01:18:45,810 --> 01:18:48,930
that means however is smaller

1251
01:18:49,670 --> 01:18:53,340
and what i need to mention here if what we do

1252
01:18:53,360 --> 01:18:57,470
in this case in the matching case is that if you don't find the line

1253
01:18:57,470 --> 01:18:59,980
close to line

1254
01:19:00,000 --> 01:19:03,200
but we assume that the if the prediction is correct

1255
01:19:04,630 --> 01:19:07,270
take this line into our results

1256
01:19:07,290 --> 01:19:10,210
so some of the

1257
01:19:10,440 --> 01:19:11,770
after performance

1258
01:19:11,790 --> 01:19:15,930
can come from this this might be a problem

1259
01:19:15,950 --> 01:19:19,790
so we see it even though it had here to use

1260
01:19:19,810 --> 01:19:22,040
this makes strategy

1261
01:19:22,060 --> 01:19:25,030
in order to assign the mode numbers

1262
01:19:25,040 --> 01:19:27,300
last experiment is on

1263
01:19:27,350 --> 01:19:28,470
the matching

1264
01:19:28,480 --> 01:19:32,020
again here on the left it i would much much better

1265
01:19:32,560 --> 01:19:34,310
so the let let

1266
01:19:34,320 --> 01:19:37,930
one is much better you see here a our your

1267
01:19:37,930 --> 01:19:39,730
six thousand compared to

1268
01:19:39,800 --> 01:19:42,610
more than twenty thousand for the st model

1269
01:19:42,690 --> 01:19:46,610
and here we see we are comparable to the

1270
01:19:46,650 --> 01:19:49,190
if you wanted to the left model

1271
01:19:49,210 --> 01:19:50,870
but we can not

1272
01:19:50,870 --> 01:19:52,580
achieve better results

1273
01:19:52,610 --> 01:19:53,920
in this case

1274
01:19:56,110 --> 01:19:57,830
we cannot improve

1275
01:19:57,890 --> 01:20:01,320
the model but but the problem here is that we can predict only two hundred

1276
01:20:01,320 --> 01:20:03,180
forty seven lines it's much

1277
01:20:03,730 --> 01:20:07,780
the recall is much smaller because the if you dialect model is much

1278
01:20:07,810 --> 01:20:09,210
small so only

1279
01:20:09,210 --> 01:20:13,700
my number of lines contain there

1280
01:20:13,710 --> 01:20:17,350
to conclude we have introduced iterative

1281
01:20:17,360 --> 01:20:20,060
have transform using background knowledge

1282
01:20:20,080 --> 01:20:21,900
which allows to detect

1283
01:20:22,030 --> 01:20:23,670
blurred lines

1284
01:20:23,680 --> 01:20:25,540
and allows sources to

1285
01:20:25,570 --> 01:20:31,570
distinguish between i can with an expectation or as we saw that it has an

1286
01:20:31,570 --> 01:20:36,170
and you move from one to the other by gradually adding objects so that is

1287
01:20:36,170 --> 01:20:39,810
not as a categorical boundary here

1288
01:20:39,860 --> 01:20:42,570
so here for instance england to show you how to go

1289
01:20:42,590 --> 01:20:46,290
from the city to the mountains consist than step so instead of the city

1290
01:20:46,380 --> 01:20:50,040
and then we know little bit on the surface of this kind of stuff appearing

1291
01:20:50,090 --> 01:20:53,270
the sky because larger buildings become smaller

1292
01:20:53,290 --> 01:20:57,880
i gain no there been this because even smaller buildings disappear from one side of

1293
01:20:57,880 --> 01:21:00,750
the main on the other side the buildings go away from both sides but there

1294
01:21:00,750 --> 01:21:02,480
are still here are

1295
01:21:02,570 --> 01:21:06,750
now there are the remains but the buildings have been replaced by montanes

1296
01:21:07,420 --> 01:21:09,900
we have more maintains that because the smaller

1297
01:21:09,920 --> 01:21:13,270
the world is almost gone in the realm only maintain some of and half on

1298
01:21:13,270 --> 01:21:18,400
the mountains so there's a very gradual progression from one scene to another generally not

1299
01:21:18,400 --> 01:21:23,060
just by a little little by little suggesting that really had any categorical boundary and

1300
01:21:23,130 --> 01:21:26,270
the and you end up in a very different place

1301
01:21:27,360 --> 01:21:30,570
and of course the fact that there are no categorical boundaries doesn't mean that you

1302
01:21:30,570 --> 01:21:34,170
can not just categories is just that there are some transitions in which you may

1303
01:21:34,880 --> 01:21:37,420
it may be depending on what you want to do with the image due may

1304
01:21:37,440 --> 01:21:41,130
focus more on one particular aspect of the category of the scene now and then

1305
01:21:41,130 --> 01:21:47,310
another aspect so here is an example of images organised along a continuous space by

1306
01:21:47,310 --> 01:21:50,670
measuring things for instance the degree of expansion

1307
01:21:50,690 --> 01:21:54,790
what is the degree of perspective on the space or the degree of openness how

1308
01:21:54,790 --> 01:21:59,400
many buildings are blocking your view how open the space and you have a continuous

1309
01:21:59,400 --> 01:22:05,630
organisation that goes from very open spaces to close spaces or from very expanded spaces

1310
01:22:05,630 --> 01:22:08,900
with strong perspective to flat videos

1311
01:22:08,920 --> 01:22:11,090
in which you can move much

1312
01:22:12,090 --> 01:22:14,750
and categories will live in this space

1313
01:22:14,750 --> 01:22:18,500
with the soft transitions between them but there are some portions of this space in

1314
01:22:18,500 --> 01:22:21,940
which to can actually be quite confident about the labelled one i just described those

1315
01:22:22,210 --> 01:22:23,610
those images

1316
01:22:25,480 --> 01:22:29,020
this is the same thing for nature again you can measure in what i said

1317
01:22:29,020 --> 01:22:31,500
you can put the degree of elevation of the terrain

1318
01:22:31,500 --> 01:22:35,710
in the ordered axes commission again how open is the space and gained have a

1319
01:22:35,710 --> 01:22:40,420
continuous space that goes from one kind of image to another kind of image go

1320
01:22:40,420 --> 01:22:45,610
from coastlines and you can go to the forest by going through the countryside and

1321
01:22:45,610 --> 01:22:47,540
you get very different kinds of scenes

1322
01:22:47,540 --> 01:22:52,500
all of them by a small changes in their opinions and still you decision different

1323
01:22:54,460 --> 01:22:58,650
OK so now that we have seen how you can go directly to the scene

1324
01:22:58,670 --> 01:23:00,310
let's go back to objects

1325
01:23:00,360 --> 01:23:04,810
let's look at the more those of a context for object recognition

1326
01:23:04,810 --> 01:23:08,440
well the first thing is of course you then do they really then

1327
01:23:08,500 --> 01:23:12,230
you don't need to just context you can look at this image in everything is

1328
01:23:12,230 --> 01:23:16,710
going on here despite this subject is out of context it's clear that you can

1329
01:23:17,520 --> 01:23:21,490
the canadian jobs locally i you don't need to reason about the scene in fact

1330
01:23:21,490 --> 01:23:25,460
this object he has everything wrong even actually is wrong so they can even neglect

1331
01:23:25,480 --> 01:23:28,440
some of the local aspects of the object does not a problem

1332
01:23:29,460 --> 01:23:32,400
it is true that you can do this in many images but there are also

1333
01:23:32,400 --> 01:23:35,360
many cases for which is actually quite hard to do

1334
01:23:35,380 --> 01:23:39,380
so this is a picture of a dome by joan steiner

1335
01:23:39,400 --> 01:23:43,480
which is an that is that builds this small walls with objects that are not

1336
01:23:43,480 --> 01:23:46,840
what they expect to be and she composes new scenes

1337
01:23:46,840 --> 01:23:50,190
so in this case this looks like a kitchen but all the different pieces that

1338
01:23:50,190 --> 01:23:53,770
compose this kitchen are just with a different meaning

1339
01:23:53,790 --> 01:23:55,710
for instance this one

1340
01:23:55,770 --> 01:23:58,730
and this is what plug

1341
01:23:58,750 --> 01:23:59,440
this is

1342
01:23:59,460 --> 01:24:00,630
i can hear

1343
01:24:00,650 --> 01:24:02,880
this is nailed

1344
01:24:02,940 --> 01:24:07,360
and in many cases you need to make an conscious f four in order to

1345
01:24:07,360 --> 01:24:11,130
recognise what these objects are you have to look attentively to them and you are

1346
01:24:11,130 --> 01:24:15,150
always discovering new objects the more time to spend looking at the image the more

1347
01:24:15,150 --> 01:24:19,880
obvious will emerge and the reason is because juggling this global reasoning

1348
01:24:19,980 --> 01:24:23,960
that is really difficult to shut down but you can not that it takes you

1349
01:24:23,960 --> 01:24:27,790
a lot of effort to start realizing what the local instances are

1350
01:24:27,860 --> 01:24:32,940
she has many many different examples really really hard to see what the local properties

1351
01:24:33,000 --> 01:24:36,790
and in many cases also context is going to be very important because it is

1352
01:24:36,790 --> 01:24:40,810
going to define what the object actually is so here the have three pictures that

1353
01:24:40,810 --> 01:24:45,090
contain cars but not all of them are cast the first one is not really

1354
01:24:45,090 --> 01:24:48,020
a car is the picture on the cover of the magazine to canon drive that

1355
01:24:48,020 --> 01:24:50,380
one you can go there and opened the door

1356
01:24:50,380 --> 01:24:51,250
it's a picture

1357
01:24:51,270 --> 01:24:52,610
this one

1358
01:24:52,650 --> 01:24:54,770
is not the kind of game is just a toy

1359
01:24:54,810 --> 01:24:58,960
you can only see that you put that the poem relationship with all the objects

1360
01:24:59,090 --> 01:25:01,500
and this is real kind of this is that one you can drive

1361
01:25:01,540 --> 01:25:06,920
but this is only defined by context also contest going to what unexpected event is

1362
01:25:06,940 --> 01:25:10,980
so here am now in this case you know there's something strange but this is

1363
01:25:10,980 --> 01:25:14,310
not just going to be just the local information lead you do need to act

1364
01:25:14,460 --> 01:25:17,960
reason about the scene and know what is expected and what is not

1365
01:25:17,980 --> 01:25:21,570
or this seen here

1366
01:25:22,860 --> 01:25:26,330
the use of context has been studied in human vision for a a long time

1367
01:25:26,880 --> 01:25:27,960
so one of the

1368
01:25:27,960 --> 01:25:30,960
influential works is also by the man

1369
01:25:31,590 --> 01:25:35,340
so be the man to find that there are five fundamental kinds of context of

1370
01:25:35,340 --> 01:25:40,810
your violations that just may have scenes one is the support of disrespect to be

1371
01:25:40,810 --> 01:25:44,420
supported by some other objects like supported by the grounds on the set of rules

1372
01:25:44,730 --> 01:25:46,310
that they have to wait over a

1373
01:25:46,330 --> 01:25:47,630
in that position

1374
01:25:47,630 --> 01:25:49,190
crystalline phases

1375
01:25:49,210 --> 01:25:52,840
it secondly order what like my says of

1376
01:25:52,860 --> 01:25:54,400
the lumley phase

1377
01:25:54,630 --> 01:26:00,130
of liquid crystals which is very similar to that of natural they are organisation of

1378
01:26:00,130 --> 01:26:01,880
natural systems

1379
01:26:01,960 --> 01:26:04,710
the question is can we make

1380
01:26:04,770 --> 01:26:09,170
parliament networks from these systems more complicated systems

1381
01:26:09,230 --> 01:26:14,000
OK it's straightforward when we have such an effect and to make polymaths effect in

1382
01:26:14,000 --> 01:26:18,630
such a way of polymers surfactants such away

1383
01:26:18,650 --> 01:26:24,130
there are many different different possibilities we investigate or the system during the last twenty

1384
01:26:24,130 --> 01:26:27,670
years and with the systematic variation u

1385
01:26:27,670 --> 01:26:33,310
actually end up with all these structures that are known for low molar mass liquid

1386
01:26:34,310 --> 01:26:36,500
i will give you some

1387
01:26:36,500 --> 01:26:39,040
just some very few

1388
01:26:39,090 --> 01:26:43,750
experiments on some recent networks that has been

1389
01:26:43,750 --> 01:26:47,190
i have been synthesized by hand by some i hope

1390
01:26:47,230 --> 01:26:50,980
because i think it's highly interesting chemistry

1391
01:26:51,040 --> 01:26:53,110
here we have the construction

1392
01:26:53,110 --> 01:26:58,190
that's the problem big backbone is water soluble so hydrophilic

1393
01:26:58,210 --> 01:27:00,480
the polymer backbone

1394
01:27:00,500 --> 01:27:03,070
and then the cycle groups hydrophobic

1395
01:27:03,090 --> 01:27:10,090
in other words you see the typical hydrophilic and hydrophobic parts of specific molecules

1396
01:27:10,130 --> 01:27:11,690
and additionally

1397
01:27:11,690 --> 01:27:13,500
we have here one

1398
01:27:15,170 --> 01:27:21,630
and the ability of the nitrogen is that you can form sort

1399
01:27:21,710 --> 01:27:25,830
make it quite canonization and then you get the sort in other words simply by

1400
01:27:25,830 --> 01:27:28,830
changing the ph value of q

1401
01:27:28,840 --> 01:27:34,750
so solution of polymer soap solution you can strongly influence

1402
01:27:34,790 --> 01:27:37,270
the hydrophilic hydrophobic balance

1403
01:27:37,310 --> 01:27:43,210
all the water solubility of the system and additionally from a non ionic

1404
01:27:43,270 --> 01:27:46,500
surfactant or police effect on you

1405
01:27:46,500 --> 01:27:49,460
change into an ionic

1406
01:27:49,920 --> 01:27:51,810
so effect and of

1407
01:27:51,860 --> 01:27:54,880
polyelectrolyte and the polyelectrolyte

1408
01:27:54,900 --> 01:28:00,540
it has a very exceptional properties were not going to the question is whether this

1409
01:28:00,560 --> 01:28:02,840
can be done or not

1410
01:28:02,840 --> 01:28:08,380
it can be done here you see such a polymer in water

1411
01:28:08,400 --> 01:28:13,690
a typical phase diagram temperature concentration of the polymer here we have a pure polymer

1412
01:28:13,840 --> 01:28:20,190
and we see a nice exec interface in this temperature region brought miss ability get

1413
01:28:20,230 --> 01:28:26,250
where the system exists in two coexisting phases

1414
01:28:28,150 --> 01:28:34,210
to make it i should show you this result because this is from a highly

1415
01:28:35,500 --> 01:28:38,520
following one of mass effect and it's well known that

1416
01:28:38,560 --> 01:28:44,000
if you change the water-soluble of solubility of the hydrophilic part of the height of

1417
01:28:44,000 --> 01:28:48,440
folly public part you end up with all these funny structures with

1418
01:28:48,540 --> 01:28:52,150
right likes effect and spherical surfactants and so on

1419
01:28:52,150 --> 01:28:57,830
here we have strongly modified how to felicity and hydrophobicity of the system but we

1420
01:28:57,830 --> 01:29:00,690
only observe exec interface

1421
01:29:00,730 --> 01:29:03,650
always the heat signature space

1422
01:29:03,670 --> 01:29:10,360
independent of the so-called hydrophilic hydrophobic balance this nice indication

1423
01:29:10,380 --> 01:29:12,480
but not only

1424
01:29:12,500 --> 01:29:15,790
i don't feel kind of public part of surfactant

1425
01:29:15,810 --> 01:29:18,210
three times the mass aggregation

1426
01:29:18,210 --> 01:29:22,360
but also the geometry of the system or in other words

1427
01:29:22,400 --> 01:29:27,460
with the polymer chemistry we have the chance to tailor make the size and the

1428
01:29:27,460 --> 01:29:29,690
dimension of our

1429
01:29:29,860 --> 01:29:36,040
surfactants in so ution of the aggregates in solution

1430
01:29:36,060 --> 01:29:40,290
we have examined her face and when we start with the non ionic

1431
01:29:40,400 --> 01:29:42,130
of my at

1432
01:29:42,190 --> 01:29:47,560
for example hydrochloric acid or another is to make such as soil formation

1433
01:29:47,560 --> 01:29:50,770
as mentioned before we change the solubility

1434
01:29:50,830 --> 01:29:55,540
this has huge consequence of the appearance of the liquid crystalline phase

1435
01:29:55,590 --> 01:29:57,880
if we look at at the temperature

1436
01:29:57,940 --> 01:30:02,710
up to which the exact place exists we see that

1437
01:30:02,710 --> 01:30:05,530
would be or cosine data plus all i time or

1438
01:30:05,920 --> 01:30:07,530
sign data

1439
01:30:07,590 --> 01:30:08,680
which of course

1440
01:30:08,740 --> 01:30:13,530
will factor out and make it cosine data plus y sin data

1441
01:30:13,580 --> 01:30:20,100
now it was oiler took the decisive step

1442
01:30:20,100 --> 01:30:22,770
and said hey look

1443
01:30:22,780 --> 01:30:27,030
i'm going to call that e the i th data

1444
01:30:30,600 --> 01:30:33,040
he do that well because

1445
01:30:33,260 --> 01:30:37,950
everything seems to indicate that it should but that's certainly worth for

1446
01:30:38,030 --> 01:30:41,720
the best color we have which is what

1447
01:30:41,720 --> 01:30:43,470
get low here OK

1448
01:30:43,480 --> 01:30:45,160
nonetheless it's worth think

1449
01:30:45,160 --> 01:30:53,600
i'll even give him is do toilet

1450
01:30:53,620 --> 01:31:01,790
sometimes is called oilers formula but i really shouldn't be it's not a formula definition

1451
01:31:01,840 --> 01:31:05,520
so in some sense you can argue it is a well if you want to

1452
01:31:06,490 --> 01:31:11,270
putting the complex number in power and calling that you know you can but

1453
01:31:11,280 --> 01:31:16,510
one can certainly as why he did it and the answer i guess is

1454
01:31:16,530 --> 01:31:19,600
that all the evidence seems to point to the fact that it was the thing

1455
01:31:19,600 --> 01:31:21,220
to do

1456
01:31:22,360 --> 01:31:26,320
i think it's important to talk about that a little bit because it's

1457
01:31:26,330 --> 01:31:27,910
in my opinion

1458
01:31:27,910 --> 01:31:30,070
using this for the first time

1459
01:31:30,080 --> 01:31:32,420
even if you read about it last night

1460
01:31:33,260 --> 01:31:36,780
it's the most it's the mysterious thing

1461
01:31:36,830 --> 01:31:40,410
and the one needs to see it from every possible point of view

1462
01:31:40,460 --> 01:31:42,270
it's something you get used to

1463
01:31:42,280 --> 01:31:45,870
you will never see it on the fly sudden flash of insight

1464
01:31:45,910 --> 01:31:47,200
it'll just get

1465
01:31:47,210 --> 01:31:49,340
as familiar to you is more

1466
01:31:49,340 --> 01:31:55,340
common arithmetic and algebraic calculus processes are

1467
01:31:55,390 --> 01:32:00,960
but look what is that we demand if you're gonna call something an exponential

1468
01:32:00,990 --> 01:32:06,160
one is that we want an exponential to do what something the right

1469
01:32:06,220 --> 01:32:11,820
at expression like this the right to be called the to the i th data

1470
01:32:11,830 --> 01:32:16,780
the answer is x i know i can creep inside oilers mind

1471
01:32:16,820 --> 01:32:19,900
it must have been a very big day in his life

1472
01:32:20,070 --> 01:32:22,490
a lot of big days but when he

1473
01:32:22,510 --> 01:32:24,320
i realized that

1474
01:32:24,330 --> 01:32:29,100
it was the thing to write down as the definition of EDI data but what

1475
01:32:29,100 --> 01:32:32,170
what what is one wants seven exponential

1476
01:32:35,420 --> 01:32:39,030
the high school and so surely is

1477
01:32:39,030 --> 01:32:41,410
you want to satisfy the exponential law

1478
01:32:41,830 --> 01:32:45,640
now to my shock i realize a lot of people don't know in my analysis

1479
01:32:45,640 --> 01:32:47,400
class you know these are

1480
01:32:47,410 --> 01:32:52,460
some math majors graduate engineers in various subjects and if i say

1481
01:32:52,510 --> 01:32:56,590
processes that using the exponential law i'm sure to get at least half a dozen

1482
01:32:56,590 --> 01:33:01,230
emails asking me what the exponential or OK

1483
01:33:01,320 --> 01:33:03,570
the exponential law is

1484
01:33:03,600 --> 01:33:04,910
eight to the

1485
01:33:06,100 --> 01:33:09,890
times a the y equals a BX course one

1486
01:33:12,910 --> 01:33:15,070
the laws of exponents

1487
01:33:15,120 --> 01:33:18,840
that's the most important reason why one

1488
01:33:18,850 --> 01:33:23,510
that's the most the single most important thing about x about exponent of the way

1489
01:33:23,510 --> 01:33:24,870
what them

1490
01:33:24,890 --> 01:33:30,150
and this is the exponential function called the exponential function because it

1491
01:33:30,160 --> 01:33:34,340
it r

1492
01:33:34,350 --> 01:33:40,600
because all the significant stop is in the exponent

1493
01:33:40,650 --> 01:33:49,260
so it should satisfy we want first of all the exponential law to be true

1494
01:33:49,330 --> 01:33:53,150
but that's not all that's high heist high school answer

1495
01:33:53,210 --> 01:33:56,350
a an MIT answer

1496
01:33:56,360 --> 01:33:59,300
it would be what what is the role

1497
01:33:59,650 --> 01:34:03,980
i mean why is e to the exocet to popular function

1498
01:34:04,080 --> 01:34:09,180
well of course it does satisfy the exponential about pride and ferocity even more reasonable

1499
01:34:09,180 --> 01:34:13,380
thing it's the function which when you differentiate you get the same thing you started

1500
01:34:13,380 --> 01:34:14,870
with and it

1501
01:34:14,930 --> 01:34:18,490
apart from a constant factor the only such functions

1502
01:34:18,510 --> 01:34:21,560
now in terms of differential equations

1503
01:34:21,600 --> 01:34:25,350
it means that it's the solution

1504
01:34:25,450 --> 01:34:30,190
e to the let's build generous make it into the axe

1505
01:34:30,270 --> 01:34:32,820
the better not to use x because x

1506
01:34:32,830 --> 01:34:37,700
complex numbers tend to be called experts i let's use t

1507
01:34:37,710 --> 01:34:42,050
as more neutral variable with standing outside the fray as it were

1508
01:34:43,500 --> 01:34:46,830
it satisfies the relationship that

1509
01:34:46,830 --> 01:34:49,510
and related methodologies

1510
01:34:49,610 --> 01:34:53,800
and the connection with case analysis and i think this is a very interesting book

1511
01:34:53,820 --> 01:34:56,100
for you to to go and read consult

1512
01:34:56,110 --> 01:35:00,290
well at a few reading on the web or perhaps some paper copies for the

1513
01:35:00,290 --> 01:35:01,800
course participants

1514
01:35:01,830 --> 01:35:07,510
we didn't have time with these on the web before the course

1515
01:35:07,590 --> 01:35:12,720
so these kind of background reading is even beyond beyond the reading that participants have

1516
01:35:13,540 --> 01:35:15,820
o through

1517
01:35:16,150 --> 01:35:18,630
some textbooks

1518
01:35:19,780 --> 01:35:21,020
the best textbook

1519
01:35:25,160 --> 01:35:26,790
actually the only one in english

1520
01:35:27,330 --> 01:35:31,900
it's pretty much up to date by myself and charles writing as editors but with

1521
01:35:32,140 --> 01:35:34,070
five or six other colleagues

1522
01:35:34,170 --> 01:35:37,690
so called confusion compared to methods

1523
01:35:37,700 --> 01:35:40,290
you're assuming techniques

1524
01:35:40,390 --> 01:35:43,360
it is an obligatory reading for this course we're going to use it from a

1525
01:35:43,360 --> 01:35:44,270
to z

1526
01:35:44,270 --> 01:35:48,750
also going to circulated from for the non participants

1527
01:35:48,860 --> 01:35:51,570
just came out last year last autumn

1528
01:35:51,580 --> 01:35:56,470
actually we used are experience as in teaching these things

1529
01:35:56,520 --> 01:35:58,780
in order to elaborate the course i mean the

1530
01:35:58,790 --> 01:36:00,610
book so it's really

1531
01:36:00,730 --> 01:36:06,930
conceived to be used as a textbook for such a course

1532
01:36:07,000 --> 01:36:11,030
there is also for those of you read german for german

1533
01:36:11,080 --> 01:36:13,160
OK i quite number

1534
01:36:13,220 --> 01:36:15,880
there's an excellent textbook in

1535
01:36:17,120 --> 01:36:18,870
which is called

1536
01:36:18,890 --> 01:36:21,280
qualitative comparative analysis went fuzzy sets

1537
01:36:21,320 --> 01:36:25,800
i never fulfilled and then the and you know it's not involved

1538
01:36:25,930 --> 01:36:30,260
mit einem for fun chancery

1539
01:36:31,360 --> 01:36:33,140
you could also small

1540
01:36:33,240 --> 01:36:39,190
it's pretty complimentary to the english language textbook

1541
01:36:39,280 --> 01:36:43,120
it's quite more advanced in many ways so it's like a complement to the english

1542
01:36:43,120 --> 01:36:44,140
language textbook

1543
01:36:44,250 --> 01:36:46,880
it has some some additional

1544
01:36:46,980 --> 01:36:49,440
let's it bonds being developed

1545
01:36:50,260 --> 01:36:55,390
by constant that ontologies document or to german colleagues

1546
01:36:55,480 --> 01:36:59,200
so you might want to read both if you want to elaborate more some technical

1547
01:37:00,300 --> 01:37:02,160
hopefully this german

1548
01:37:02,170 --> 01:37:04,160
book on the basis of this book

1549
01:37:04,260 --> 01:37:09,040
i will be translated into some time in english some chapters and so on and

1550
01:37:09,080 --> 01:37:12,530
maybe available from some of you in english

1551
01:37:25,020 --> 01:37:27,330
good all of you please

1552
01:37:27,350 --> 01:37:29,550
right names

1553
01:37:29,610 --> 01:37:31,480
i think get that's why

1554
01:37:31,490 --> 01:37:38,980
is also a kind of all the textbook in french

1555
01:37:39,530 --> 01:37:42,290
by myself and the them i didn't bring it here

1556
01:37:42,640 --> 01:37:46,620
it's really not on the technical side up to date but if you're interested in

1557
01:37:46,620 --> 01:37:48,260
the foundations of course you

1558
01:37:48,310 --> 01:37:53,760
especially bowling brian logics there's some stuff in the in two

1559
01:37:53,770 --> 01:37:58,450
i want to use powerpoint slides throughout the course

1560
01:37:58,770 --> 01:38:01,860
and it be posted by the teaching assistants

1561
01:38:02,140 --> 01:38:07,390
as well as complementary material on the databases

1562
01:38:07,890 --> 01:38:09,450
a lot of data files

1563
01:38:09,510 --> 01:38:13,630
because accuracy is i'm show you pretty soon is

1564
01:38:13,660 --> 01:38:16,040
said also of data analysis techniques

1565
01:38:16,080 --> 01:38:17,450
for tabular data

1566
01:38:17,500 --> 01:38:19,570
related to cases

1567
01:38:19,630 --> 01:38:23,650
and so we can use a lot of data files

1568
01:38:23,700 --> 01:38:29,230
made of data or we live data from published articles and books

1569
01:38:29,240 --> 01:38:31,680
so you ask for those who want exposed to bring

1570
01:38:31,800 --> 01:38:34,620
we use key because we're going to give you some files also

1571
01:38:34,630 --> 01:38:39,020
we should be using the lab and so on and so forth

1572
01:38:39,100 --> 01:38:44,310
they will be exercises assignments also part of the course resources

1573
01:38:44,360 --> 01:38:46,720
and we're gonna use software

1574
01:38:46,820 --> 01:38:50,870
basically two software packages around they're both freeware

1575
01:38:50,880 --> 01:38:53,230
freely downloadable on the web

1576
01:38:53,240 --> 01:38:54,930
one is called the mind

1577
01:38:55,010 --> 01:38:56,050
it stands for

1578
01:38:56,060 --> 01:38:58,970
tools for small analysis

1579
01:38:59,030 --> 01:39:03,750
developed by lester confused and depressed russir two german colleagues

1580
01:39:03,790 --> 01:39:08,000
last is actually political scientist but also the problem

1581
01:39:08,040 --> 01:39:10,490
so he has developed very nice software

1582
01:39:10,510 --> 01:39:14,200
which is very much user centred quite user-friendly

1583
01:39:14,270 --> 01:39:18,770
it is the main one of the use during the first week and the second

1584
01:39:18,800 --> 01:39:22,670
software is called fist uses developed by charles region in USA

1585
01:39:22,680 --> 01:39:24,540
and st

1586
01:39:24,610 --> 01:39:29,700
and it's quite complimentary to to my

1587
01:39:29,720 --> 01:39:32,880
lastly we use a lot

1588
01:39:32,920 --> 01:39:35,120
a resource called combats

1589
01:39:35,160 --> 01:39:39,500
i have the time to go through this today

1590
01:39:39,510 --> 01:39:43,460
i to go to the core of the matter that compounds is a

1591
01:39:43,460 --> 01:39:49,210
derivations which is you have to enforce the constraints on the parameters properly

1592
01:39:49,840 --> 01:39:51,890
in the multinomial model

1593
01:39:51,910 --> 01:39:53,440
the probability of

1594
01:39:53,460 --> 01:39:57,510
the di coming up one two three up to k those numbers between zero and

1595
01:39:57,510 --> 01:40:00,670
one but they have to to add up to one

1596
01:40:00,680 --> 01:40:04,740
and when you take the derivative you have to enforce factor three years

1597
01:40:04,830 --> 01:40:10,070
and that constraint has been forced usually with the electron multiplier leads to this extra

1598
01:40:10,070 --> 01:40:13,750
term here the total number of coin flips if you don't do that you get

1599
01:40:13,750 --> 01:40:14,480
your ass

1600
01:40:15,140 --> 01:40:21,180
again not very deep but just to let you know that's important for us

1601
01:40:21,240 --> 01:40:24,290
this is not very common example

1602
01:40:24,300 --> 01:40:28,040
multivariate gaussians so the galaxy and is

1603
01:40:28,050 --> 01:40:32,540
it is very common distribution for continuous

1604
01:40:33,850 --> 01:40:37,160
because and has the property of being the maximum

1605
01:40:37,180 --> 01:40:42,270
entropy estimator consistent with first and second moments

1606
01:40:42,300 --> 01:40:47,620
so i tell you have some data has a mean mu and variance sigma square

1607
01:40:47,630 --> 01:40:50,710
please find me any distribution in the world

1608
01:40:50,740 --> 01:40:52,430
the tasmanian

1609
01:40:52,430 --> 01:40:58,590
the variance sigma square and trying to maximize the entropy the answer is yes

1610
01:41:01,600 --> 01:41:05,500
so the origin of the gaussians it also comes up

1611
01:41:05,530 --> 01:41:06,860
as the limits

1612
01:41:07,120 --> 01:41:10,980
the addition of independent random variables you have a lot of random variables together

1613
01:41:12,640 --> 01:41:15,220
their distributions tend to go the

1614
01:41:16,760 --> 01:41:20,740
i like to think in more engineering mindset if you add two random variables together

1615
01:41:20,740 --> 01:41:24,120
with you do distributions you could involve them

1616
01:41:24,120 --> 01:41:28,640
and if you take any sort of function like this and you can work with

1617
01:41:28,640 --> 01:41:32,430
itself over and over again what happens you get the gas

1618
01:41:32,450 --> 01:41:35,370
right so you can just try this you can just take a square wave and

1619
01:41:35,390 --> 01:41:39,990
convolved with itself or five times and you get to the

1620
01:41:41,130 --> 01:41:46,570
my systems and signals professor undergrad said that the only two approximations you need to

1621
01:41:47,250 --> 01:41:51,440
was what is lot of one class x one x is small

1622
01:41:51,450 --> 01:41:54,940
and how many times you need to all the square wave with itself before it

1623
01:41:54,940 --> 01:41:57,070
looks basically get

1624
01:41:57,110 --> 01:41:59,410
so maybe

1625
01:41:59,420 --> 01:42:02,950
maybe there really only two approximations you need to know about this you should know

1626
01:42:02,950 --> 01:42:04,260
that no one

1627
01:42:04,280 --> 01:42:05,680
you oh by the way

1628
01:42:05,700 --> 01:42:07,610
what is left

1629
01:42:07,620 --> 01:42:10,500
in fact the

1630
01:42:10,550 --> 01:42:15,860
among many help

1631
01:42:15,870 --> 01:42:18,970
back in one

1632
01:42:20,570 --> 01:42:23,140
namely we associate with the

1633
01:42:28,460 --> 01:42:31,750
but we digress i

1634
01:42:32,370 --> 01:42:36,090
maximum likelihood estimation of the galaxy and it's the same thing but now quantities are

1635
01:42:36,090 --> 01:42:38,130
continuous and categorical

1636
01:42:38,140 --> 01:42:43,600
and then you can go through the same kind of derivation you find a seemingly

1637
01:42:43,600 --> 01:42:46,110
trivial but very useful facts

1638
01:42:46,130 --> 01:42:51,100
the maximum likelihood galaxy in the two months of data is the gaussians whose mean

1639
01:42:51,100 --> 01:42:52,960
is the mean of your data

1640
01:42:53,020 --> 01:42:57,460
and use covariance covariance is the covariance

1641
01:42:58,240 --> 01:43:01,490
that sounds pretty good trying to ride

1642
01:43:01,510 --> 01:43:05,350
right for the multidimensional that's actually pretty good exercise

1643
01:43:05,360 --> 01:43:07,740
but that fact will serve you well

1644
01:43:09,470 --> 01:43:13,150
all this to a couple more examples here and then we can move on two

1645
01:43:14,040 --> 01:43:17,980
important things so the example now i'm going to do is linear regression

1646
01:43:18,000 --> 01:43:22,680
and this example is distinguished by having two nodes in the graphical model right so

1647
01:43:22,680 --> 01:43:27,290
for example they did which is one of graphical model so two kind of silly

1648
01:43:28,680 --> 01:43:30,970
see our first real graph model

1649
01:43:30,980 --> 01:43:32,670
and this is the third of june

1650
01:43:32,680 --> 01:43:35,160
graphical models so this is linear regression

1651
01:43:35,180 --> 01:43:37,750
and the idea is that you have some input x

1652
01:43:37,770 --> 01:43:39,920
and so output y

1653
01:43:39,920 --> 01:43:45,550
and because y axis parents we said that we need to specify the joint distribution

1654
01:43:45,550 --> 01:43:47,970
of x and y in this way

1655
01:43:49,340 --> 01:43:51,990
i'm here what arguments

1656
01:43:52,990 --> 01:43:54,030
and now

1657
01:43:54,130 --> 01:43:56,170
some pairs of x y

1658
01:43:56,170 --> 01:44:00,670
and i'd like to learn both the distribution in the distribution of

1659
01:44:00,670 --> 01:44:02,470
the joint distribution

1660
01:44:02,490 --> 01:44:06,380
in linear regression we assume that this distribution

1661
01:44:06,400 --> 01:44:09,450
he of y given x

1662
01:44:09,470 --> 01:44:11,900
is a gaussian

1663
01:44:11,920 --> 01:44:15,300
with the media which is a linear function of x

1664
01:44:15,320 --> 01:44:18,360
so here are just call this the transpose

1665
01:44:19,860 --> 01:44:24,840
covariance some very difficult to describe so for now assume that y is scale

1666
01:44:24,860 --> 01:44:31,130
so this is really the geometric problem of fitting a straight line to some data

1667
01:44:33,820 --> 01:44:36,920
when you start to to think about the problem you realize that you have to

1668
01:44:36,920 --> 01:44:39,030
be careful how you define

1669
01:44:39,050 --> 01:44:42,320
so how do you define the problem of finding some straight lines in the way

1670
01:44:42,320 --> 01:44:47,570
we define it here is to assume that we make the horizontal coordinates of every

1671
01:44:47,570 --> 01:44:50,240
data point perfectly

1672
01:44:50,260 --> 01:44:53,610
and we measure the vertical coordinate the output with some

1673
01:44:54,610 --> 01:44:56,510
which is gas industry

1674
01:44:56,970 --> 01:45:00,090
and now we want to maximize the likelihood going to see this

1675
01:45:00,110 --> 01:45:07,200
maximum likelihood in this galaxy an error model reduces to minimizing the square difference vertically

1676
01:45:07,200 --> 01:45:09,380
between each data point on the line

1677
01:45:09,400 --> 01:45:11,320
and that's the least squares

1678
01:45:11,340 --> 01:45:15,900
so when least squares is nothing more than the maximum likelihood of gaussian model

1679
01:45:15,900 --> 01:45:18,450
but when you do that you

1680
01:45:18,470 --> 01:45:23,300
realize that there is a big something you have no noise in your measurement that

1681
01:45:23,320 --> 01:45:24,110
and that

1682
01:45:24,130 --> 01:45:29,380
almost certainly untrue in many real world situations to make one revert something more complicated

1683
01:45:29,380 --> 01:45:35,510
like total square which tries to also incorporate access for now

1684
01:45:35,530 --> 01:45:39,590
in the simple case here

1685
01:45:39,650 --> 01:45:42,320
so each linear regression

1686
01:45:42,320 --> 01:45:44,550
so parents

1687
01:45:44,570 --> 01:45:48,490
and all the children are continuous value

1688
01:45:50,530 --> 01:45:54,990
the way we set up is exactly like i said it's against the distribution

1689
01:45:55,010 --> 01:45:59,280
where the mean is linear function of the continuous here

1690
01:45:59,300 --> 01:46:03,320
now if you had some discrete parents you have different

1691
01:46:03,340 --> 01:46:10,070
linear model for every setting the discrete that's relating axes vector which is married to

1692
01:46:10,760 --> 01:46:16,280
you might have z which is a binary variable which is also married to y

1693
01:46:16,300 --> 01:46:21,380
so we have two models here we have one linear regression model which is why

1694
01:46:21,380 --> 01:46:25,420
is a function of x y and z is zero and different linear regression model

1695
01:46:25,420 --> 01:46:28,860
with for y is a function of x y z is one

1696
01:46:28,880 --> 01:46:33,570
so you can make a continuous and discrete parents in your graphical model

1697
01:46:33,590 --> 01:46:39,320
as long as each setting of the discrete parents you have some of the continuum

1698
01:46:40,450 --> 01:46:42,550
the other way around is very complicated

1699
01:46:42,570 --> 01:46:46,860
having discrete children have continuous parents is a

1700
01:46:47,110 --> 01:46:48,360
the top row

1701
01:46:48,380 --> 01:46:49,740
to model that

1702
01:46:49,780 --> 01:46:50,510
so what

1703
01:46:52,050 --> 01:46:53,650
OK so

1704
01:46:53,670 --> 01:46:57,680
here we can see that the likelihood is that the squared error cost

1705
01:46:57,700 --> 01:47:00,860
you just get this by taking the log of the gaussians and

1706
01:47:01,650 --> 01:47:07,630
and the maximum likelihood parameters can be solved using these services

1707
01:47:07,630 --> 01:47:11,570
one of the most famous formulas in all of data analysis

1708
01:47:11,630 --> 01:47:13,550
called the discrete mean filter

1709
01:47:13,570 --> 01:47:18,490
and it gives you the solution to these linear least squares problem and that is

1710
01:47:18,740 --> 01:47:23,380
what is being computed in a numerically stable and efficient way when you do that

1711
01:47:23,380 --> 01:47:31,270
only for maybe I use yellow chalk so girl to only boy for

1712
01:47:32,440 --> 01:47:35,220
girls 3

1713
01:47:35,240 --> 01:47:36,070
2 months

1714
01:47:38,100 --> 01:47:42,300
and girl for

1715
01:47:45,480 --> 01:47:46,210
number 2

1716
01:47:47,000 --> 01:47:53,260
all makes that dashed lines not because it's different but just just to

1717
01:47:54,130 --> 01:47:55,600
so I act

1718
01:47:55,620 --> 01:47:58,500
Of course I'd like to create a matrix

1719
01:47:58,760 --> 01:48:06,080
that describes this this up

1720
01:48:06,270 --> 01:48:08,590
it's sort of a graph right

1721
01:48:08,760 --> 01:48:16,630
this is called actually the official name for this would be a bipartite graph so

1722
01:48:16,650 --> 01:48:25,690
these are nodes in the graph and the word bipartite means that's got 2 parts

1723
01:48:25,700 --> 01:48:29,380
the girls and the boys and all edges go between the two part

1724
01:48:30,060 --> 01:48:36,340
never anyway that bipartite graph this is the reason everything I say today there is

1725
01:48:36,340 --> 01:48:37,240
a host of

1726
01:48:39,560 --> 01:48:44,500
there just have the knack of these combinatorial questions that today

1727
01:48:45,620 --> 01:48:46,800
that that's

1728
01:48:46,810 --> 01:48:53,330
this is becoming a very significant part of modern modern mathematics pure and applied

1729
01:48:54,100 --> 01:48:58,380
OK so let me express the graph this way the 1st row will tell

1730
01:49:00,900 --> 01:49:07,030
that default for boys are allowed the 2nd row will tell us that only way

1731
01:49:07,080 --> 01:49:12,600
1 allowed boy for the 2nd will tell us that 2 informal out and the

1732
01:49:12,600 --> 01:49:17,860
3rd that only 2 ways that would be the matrix that so

1733
01:49:17,950 --> 01:49:26,570
so a i j equals 1 a i j equals one half girl ally and

1734
01:49:27,130 --> 01:49:32,570
here OK and otherwise 0

1735
01:49:33,770 --> 01:49:36,450
right here

1736
01:49:38,860 --> 01:49:43,080
it's not OK I know which

1737
01:49:45,060 --> 01:49:50,830
what's our jobs well a complete matching so let me introduce you these words a

1738
01:49:50,830 --> 01:49:52,830
complete matching would be

1739
01:49:53,360 --> 01:49:56,620
get everybody will get everybody married

1740
01:49:56,800 --> 01:50:01,460
thank you thank you thank you thank you

1741
01:50:02,460 --> 01:50:04,460
and their marriage problem is

1742
01:50:07,150 --> 01:50:12,060
when cannot be done cannot be done in this example and what's the necessary and

1743
01:50:12,060 --> 01:50:14,940
sufficient conditions for doing

1744
01:50:15,700 --> 01:50:17,500
OK so that

1745
01:50:18,230 --> 01:50:24,960
the Mac and of course 1 of after the last lecture you want a maximization

1746
01:50:24,960 --> 01:50:31,380
problem really it here so the maximization problem would be how many marriages can we

1747
01:50:31,380 --> 01:50:37,520
get maximize the number of marriages a complete matching means all married our problem would

1748
01:50:37,520 --> 01:50:43,140
be to our top if we can't our problem would be to maximize the number

1749
01:50:43,200 --> 01:50:53,140
of matches so this is like a specific question can everybody gets matched

1750
01:50:54,310 --> 01:51:00,520
if not the optimization problem the maximum problem would be to maximize the number of

1751
01:51:01,740 --> 01:51:04,340
and there's gonna be some minimum problems

1752
01:51:04,640 --> 01:51:05,740
and the max

1753
01:51:06,540 --> 01:51:09,880
will match the man that

1754
01:51:09,900 --> 01:51:16,680
duality at that the winner OK it now for for this specific that which I

1755
01:51:16,680 --> 01:51:22,450
would like 1st asked this question is a complete against the complete or when there's

1756
01:51:22,450 --> 01:51:24,300
a complete matching possible

1757
01:51:25,260 --> 01:51:27,740
up public access

1758
01:51:28,880 --> 01:51:34,580
and let's see that's not good question

1759
01:51:34,590 --> 01:51:38,850
visited we'll have a matrix and

1760
01:51:38,900 --> 01:51:42,870
well of course the matrix of all ones

1761
01:51:44,610 --> 01:51:47,280
so that would be a case where

1762
01:51:47,300 --> 01:51:52,680
everybody's will you marry everybody and the ranges 1

1763
01:51:52,730 --> 01:51:57,090
out my 1st thought was well what about the identity matrix well the identity matrix

1764
01:51:57,090 --> 01:52:01,370
a course that's a perfect we have a perfect matching girl 1 on

1765
01:52:01,400 --> 01:52:04,640
but they're matched their match their match the match great

1766
01:52:07,140 --> 01:52:11,060
yeah so somehow rank is somewhere hiding here

1767
01:52:11,160 --> 01:52:15,710
what about this specific examples

1768
01:52:15,730 --> 01:52:20,780
could I do a complete matching and if you tell me why not you probably

1769
01:52:21,560 --> 01:52:23,780
I guess that the

1770
01:52:24,440 --> 01:52:26,940
general theorem why not

1771
01:52:26,950 --> 01:52:32,040
the answer is no and what's from domain the reasons

1772
01:52:33,160 --> 01:52:39,160
well let's dance this question rank

1773
01:52:39,900 --> 01:52:42,460
and at all

1774
01:52:42,720 --> 01:52:49,060
the in in this language here of of edges

1775
01:52:49,160 --> 01:52:54,570
would the media that is a way of looking at the end of the words

1776
01:52:54,570 --> 01:53:07,950
I got all know the way to the end of the world that right OK

1777
01:53:07,950 --> 01:53:14,690
so now can we express that put that into our way that doesn't take out

1778
01:53:14,700 --> 01:53:24,400
girl to as like guilty for voice our whatever

1779
01:53:25,160 --> 01:53:29,350
the had and so on

1780
01:53:29,610 --> 01:53:34,470
OK but some of it

1781
01:53:35,380 --> 01:53:42,500
let start this sentence the problem is that girls 2 3 and 4

1782
01:53:47,990 --> 01:53:53,260
so again and allowed only like this don't like to avoid

1783
01:53:53,280 --> 01:53:58,450
we had 3 girls there that only can match

1784
01:53:58,470 --> 01:54:03,970
the only acceptable only only so matches over with 2 boys so you can't do

1785
01:54:04,920 --> 01:54:06,540
I would like

1786
01:54:06,560 --> 01:54:12,060
there's something but but I'll I'll stay with that language so my my my my

1787
01:54:12,060 --> 01:54:15,710
theorems will be complete matching of possible

1788
01:54:15,900 --> 01:54:20,940
if and only if the express at every

1789
01:54:22,880 --> 01:54:31,360
said that was the right right in here of of the girls saying say K

1790
01:54:33,020 --> 01:54:35,040
are girls

1791
01:54:35,970 --> 01:54:41,330
every set of girls every subset of girls whatever something you take a take any

1792
01:54:41,330 --> 01:54:47,020
group of girls say K of K

1793
01:54:49,720 --> 01:54:51,420
must work

1794
01:54:54,640 --> 01:54:58,660
much match and there must be at least k voice

1795
01:54:58,730 --> 01:55:01,060
that's clearly

1796
01:55:03,800 --> 01:55:08,590
right if I have a set of playgirls adopt among them like these 3 that

1797
01:55:08,590 --> 01:55:10,920
among them only connect

1798
01:55:10,940 --> 01:55:13,900
except 2 boys and I can't do it

1799
01:55:14,020 --> 01:55:14,970
so I got

1800
01:55:15,460 --> 01:55:21,080
it's like that duality weak duality that had before in some way half of the

1801
01:55:21,090 --> 01:55:24,580
theorem is going to be easy on me finish the state every set of girls

1802
01:55:24,580 --> 01:55:26,520
k of them like

1803
01:55:29,380 --> 01:55:36,540
collectively collectively

1804
01:55:37,160 --> 01:55:42,060
at least k

1805
01:55:46,540 --> 01:55:49,040
certainly necessary

1806
01:55:50,160 --> 01:55:54,140
not clear at all

1807
01:55:54,140 --> 01:55:58,170
OK and the final example for text i want to talk about is this statistical

1808
01:55:58,170 --> 01:55:59,920
machine translation

1809
01:55:59,940 --> 01:56:03,090
and this is a little bit harder not only do i need a large corpus

1810
01:56:03,090 --> 01:56:06,410
of english text upon and try to to translate into english but i'm trying to

1811
01:56:06,410 --> 01:56:11,740
translate from german also need a corpus of parallel text and when i was in

1812
01:56:11,740 --> 01:56:14,850
my hotel in berlin i was able to do that

1813
01:56:14,860 --> 01:56:18,050
but they had a nice collection of parallel text they get little brochure on one

1814
01:56:18,050 --> 01:56:21,900
side was englishness over german and now i can say that these are aligned with

1815
01:56:21,900 --> 01:56:24,030
each other and i've ever given

1816
01:56:27,410 --> 01:56:29,890
phrase in germany that i know

1817
01:56:29,930 --> 01:56:31,880
but this is a translation in english

1818
01:56:32,040 --> 01:56:37,070
but what i'm not given exactly that how can i use this information anyway

1819
01:56:37,110 --> 01:56:38,740
well i guess i've

1820
01:56:38,810 --> 01:56:41,950
i start off knowing that this is the alignment and now i want to try

1821
01:56:41,950 --> 01:56:47,310
to align word by word and one possibility is that these two words are lined

1822
01:56:47,310 --> 01:56:52,060
up according to each other and so assign probabilities to that but also might be

1823
01:56:52,060 --> 01:56:53,080
that there are lined up

1824
01:56:53,580 --> 01:56:57,070
those two words are are lined up inside probability there

1825
01:56:57,960 --> 01:57:00,930
could be all the way to the and or it could be one word

1826
01:57:01,270 --> 01:57:05,960
lives with two and vice versa so from one sentence i get a very broad

1827
01:57:05,960 --> 01:57:08,820
probability distribution all these things are possible

1828
01:57:08,980 --> 01:57:12,660
and then i just keep on adding them up over and over and over

1829
01:57:12,720 --> 01:57:18,270
and notice that she you know lots of times constant art occur

1830
01:57:18,270 --> 01:57:23,630
in sentences that are aligned and commenced in luxury did not occur often in the

1831
01:57:23,630 --> 01:57:26,430
probability going to converge

1832
01:57:26,660 --> 01:57:30,900
and eventually you say well that's going to be the right one

1833
01:57:30,950 --> 01:57:32,970
how well to hear some airbags

1834
01:57:32,990 --> 01:57:39,880
translate english comes out like this i've underlined the places where there's disfluencies so you

1835
01:57:39,880 --> 01:57:43,440
can go more than a sentence or two without saying here's an example of something

1836
01:57:43,440 --> 01:57:48,130
that's not quite fluent english text but is good enough that you get the idea

1837
01:57:48,130 --> 01:57:52,600
of what's going on in all the right who's doing what to whom is represented

1838
01:57:55,430 --> 01:57:57,170
similar kind of thing

1839
01:57:57,190 --> 01:58:04,630
there it's more like two or so disfluencies percent so rather than one probably reflecting

1840
01:58:04,630 --> 01:58:06,820
the fact that chinese is more

1841
01:58:07,020 --> 01:58:10,340
different from english there because

1842
01:58:10,380 --> 01:58:13,070
i had to go through the process

1843
01:58:13,120 --> 01:58:17,540
well we start off we go one character at a time first character

1844
01:58:17,560 --> 01:58:21,920
look up the probabilities for the alignments and he is the most likely know and

1845
01:58:21,920 --> 01:58:23,160
in this case i happen to

1846
01:58:23,390 --> 01:58:29,240
folded case together so he in the capital here consider different tokens

1847
01:58:29,270 --> 01:58:31,690
go to the next one

1848
01:58:31,710 --> 01:58:34,530
and we have set here

1849
01:58:34,570 --> 01:58:39,360
and now i notice when we want to the probability the stupa pillars we have

1850
01:58:39,360 --> 01:58:42,380
to deal with one is probability of the translation

1851
01:58:42,410 --> 01:58:45,530
from chinese into english and that represented by

1852
01:58:45,540 --> 01:58:47,070
these lists here

1853
01:58:47,080 --> 01:58:50,620
and the other is the probability that what we generate a sense of english and

1854
01:58:50,620 --> 01:58:56,160
that we test against corpus of english and in that case we we probably recognise

1855
01:58:56,160 --> 01:59:01,780
that the top probability here the latter is not very probable in english

1856
01:59:01,790 --> 01:59:03,790
but if you go down the list a little bit

1857
01:59:03,800 --> 01:59:06,380
his letter would be more probable

1858
01:59:06,390 --> 01:59:10,180
and so we be able to make that kind of correction there of course we

1859
01:59:10,180 --> 01:59:13,630
don't have to look just one character at a time can also combine them in

1860
01:59:13,630 --> 01:59:14,870
fact we've seen

1861
01:59:14,900 --> 01:59:17,290
this sequence before

1862
01:59:17,290 --> 01:59:23,640
and we have recognised that his letter at his right alignment and in many cases

1863
01:59:23,640 --> 01:59:27,970
for this more probable than the latter but we also notice that it's this also

1864
01:59:27,970 --> 01:59:31,030
is a proper name and that's more probable

1865
01:59:33,000 --> 01:59:38,730
and in fact in this case we've actually seen three character sequence corresponding to this

1866
01:59:38,730 --> 01:59:44,200
already so it is more cases of look then pasting things together

1867
01:59:44,460 --> 01:59:47,660
but for the rest of it now we're going to start pasting together with what

1868
01:59:47,660 --> 01:59:54,590
were essentially doing is trying to find a path through the history probabilities i problem

1869
01:59:54,600 --> 02:00:00,350
and this is just a chart here just echoes the first chart we had saying

1870
02:00:00,350 --> 02:00:04,430
that as we get more data this course keep going up and we have reached

1871
02:00:04,430 --> 02:00:05,460
the point yet

1872
02:00:05,680 --> 02:00:11,290
where the scores as and so there's still more to gain from the data

1873
02:00:12,060 --> 02:00:17,420
and i want to switch little and talk about the image data

1874
02:00:17,440 --> 02:00:24,090
and here's an example from google image search where you type in the name area

1875
02:00:24,090 --> 02:00:30,090
type to model is years results you get back pretty good but there's a variety

1876
02:00:30,090 --> 02:00:35,610
of ones there and who knows really what the best one

1877
02:00:37,730 --> 02:00:43,820
you know it's it's no secret that google like most of the other providers of

1878
02:00:43,820 --> 02:00:47,790
image search for the most part until very recently

1879
02:00:47,860 --> 02:00:51,860
had been throwing away the images and just looking at the surrounding words in names

1880
02:00:51,860 --> 02:00:55,850
and things like that and and for the most part that works pretty well

1881
02:00:56,040 --> 02:01:00,780
but we can prove that by looking at the data in the images themselves

1882
02:01:01,030 --> 02:01:04,930
and in this case what we're really looking for saying well there is a canonical

1883
02:01:06,100 --> 02:01:09,210
two this query and we want to be able to find that another on the

1884
02:01:09,210 --> 02:01:11,520
web very good at canonical

1885
02:01:11,530 --> 02:01:14,600
answers you type IBM

1886
02:01:14,610 --> 02:01:19,580
all search engines will give you idea dotcom that's an easy one because

1887
02:01:19,630 --> 02:01:23,750
not because of the contents of what's on that page but because of all the

1888
02:01:23,750 --> 02:01:27,550
links that link to it and so there good agreement that that's the right answer

1889
02:01:27,550 --> 02:01:32,630
to the query on the web itself with pictures that's not the case there is

1890
02:01:32,630 --> 02:01:39,170
one canonical model is image or or artifact in the real world but that's not

1891
02:01:39,170 --> 02:01:44,140
represented by one picture on the web is represented by lots of different pictures

1892
02:01:44,170 --> 02:01:49,030
so we have to be able to figure out which ones of those represent the

1893
02:01:49,030 --> 02:01:51,600
same object in the real world

1894
02:01:53,230 --> 02:02:00,140
and we do that by first looking at the images running sort of standard feature

1895
02:02:00,140 --> 02:02:06,030
extractor and then conceptually looking at all pairs of images and lining up which features

1896
02:02:06,030 --> 02:02:10,620
line up with each other and then we get to measure similarity between each pair

1897
02:02:10,620 --> 02:02:13,530
of images and of course you take shortcuts so you don't have to look at

1898
02:02:13,810 --> 02:02:19,350
all possible and square pairs but just the ones that makes sense to look

1899
02:02:19,350 --> 02:02:24,920
and now since we draw graphs of the connectedness between those how close each images

1900
02:02:24,930 --> 02:02:26,220
are to each other

