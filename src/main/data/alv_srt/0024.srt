1
00:00:00,000 --> 00:00:02,200
not optimal solution to a problem

2
00:00:02,210 --> 00:00:04,890
contains optimal solutions

3
00:00:04,930 --> 00:00:12,950
two some

4
00:00:18,000 --> 00:00:20,650
we're trying to

5
00:00:23,540 --> 00:00:27,100
so here for example

6
00:00:27,180 --> 00:00:28,550
OK if

7
00:00:31,460 --> 00:00:34,630
this is the longest common subsequence of x and y

8
00:00:48,270 --> 00:00:51,420
is a

9
00:00:51,460 --> 00:00:54,180
one is common subsequence

10
00:00:54,200 --> 00:00:56,280
the prefix

11
00:01:13,000 --> 00:01:14,680
so this is basically

12
00:01:14,680 --> 00:01:23,440
the problem i can see that there is optimal substructure going on

13
00:01:23,510 --> 00:01:25,320
in this case

14
00:01:25,350 --> 00:01:30,270
and the idea is that almost always this means that there a cut-and-paste argument you

15
00:01:30,270 --> 00:01:33,360
can do to demonstrate

16
00:01:33,410 --> 00:01:36,080
it is the

17
00:01:36,190 --> 00:01:38,950
some structure were not optimal

18
00:01:39,000 --> 00:01:41,770
and you be able to find

19
00:01:41,850 --> 00:01:46,590
eight their solutions to the overall

20
00:01:46,640 --> 00:01:49,880
using cut-and-paste

21
00:02:00,340 --> 00:02:21,780
his as a strategy for being able to compute the longest common subsequence

22
00:02:21,970 --> 00:02:45,260
so ignore these cases

23
00:02:56,220 --> 00:04:00,940
basically just implementing

24
00:04:00,950 --> 00:04:04,510
is there

25
00:04:06,570 --> 00:04:08,090
so it's either

26
00:04:08,130 --> 00:04:11,070
the longest common subsequence if

27
00:04:11,090 --> 00:04:12,450
if the match

28
00:04:12,450 --> 00:04:14,860
one common subsequence of one

29
00:04:14,900 --> 00:04:18,380
the prefix that we drop that character

30
00:04:18,400 --> 00:04:21,010
both strings one

31
00:04:21,070 --> 00:04:22,720
that's the matching one

32
00:04:23,920 --> 00:04:25,880
drop a character from

33
00:04:25,920 --> 00:04:29,570
in fact since longest common subsequence of

34
00:04:29,630 --> 00:04:31,670
you draw a character from y

35
00:04:31,720 --> 00:04:34,650
which everyone knows is one

36
00:04:34,650 --> 00:04:36,530
it ends up being

37
00:04:36,590 --> 00:04:42,010
longest common subsequence

38
00:04:47,940 --> 00:04:52,110
which is what's the worst case

39
00:04:55,670 --> 00:04:58,530
this program was going to happen in the worst case

40
00:04:58,570 --> 00:05:03,030
which of these two clauses

41
00:05:03,220 --> 00:05:08,990
causes more havoc

42
00:05:09,050 --> 00:05:13,950
the second one second

43
00:05:14,070 --> 00:05:24,170
during two LCS calculus up calculations here only do when i to decrement

44
00:05:24,200 --> 00:05:26,320
this is here

45
00:05:27,840 --> 00:05:28,990
basically go

46
00:05:29,010 --> 00:05:34,010
you only get decrement one indexing i calculate two of the that's can generate

47
00:05:34,070 --> 00:05:37,570
the tree on the worst case

48
00:05:37,570 --> 00:05:38,760
x y

49
00:05:38,800 --> 00:05:41,170
it is not equal to j e

50
00:05:43,180 --> 00:05:47,740
and j

51
00:05:47,760 --> 00:05:52,090
let's draw a recursion tree

52
00:05:52,170 --> 00:05:53,610
this program

53
00:05:53,670 --> 00:05:57,700
circuit understanding as to what's going on

54
00:06:06,150 --> 00:06:09,720
i mean do we have an equal say and

55
00:06:09,780 --> 00:06:13,300
and it also six

56
00:06:14,260 --> 00:06:19,760
most of the time with my two indices being seven six

57
00:06:19,800 --> 00:06:23,720
and then the worst case we have to execute these

58
00:06:23,720 --> 00:06:25,860
so this is going to end up being

59
00:06:25,880 --> 00:06:27,610
six six

60
00:06:27,670 --> 00:06:30,650
and seven five

61
00:06:30,700 --> 00:06:32,670
during this is after

62
00:06:32,740 --> 00:06:34,300
first call

63
00:06:34,320 --> 00:06:40,470
in this guise because we're is going to be

64
00:06:40,550 --> 00:06:42,940
produced by six here

65
00:06:43,900 --> 00:06:46,320
first index i

66
00:06:46,320 --> 00:06:49,010
i keep going down here in

67
00:06:49,070 --> 00:06:51,320
four six

68
00:06:53,380 --> 00:06:55,220
you guys

69
00:06:55,240 --> 00:06:57,630
standing here i get

70
00:06:57,650 --> 00:06:59,170
six five

71
00:07:02,610 --> 00:07:03,920
six four

72
00:07:03,960 --> 00:07:08,490
OK over here

73
00:07:12,050 --> 00:07:15,590
the first index six five

74
00:07:19,920 --> 00:07:22,030
these guys

75
00:07:22,050 --> 00:07:25,720
here i get more

76
00:07:25,720 --> 00:07:29,880
they get sixty four

77
00:07:29,880 --> 00:07:33,520
just getting what costa photocopying by thing in and eric and i are just you

78
00:07:33,520 --> 00:07:39,150
know roman and so so it's probably cheap enough right

79
00:07:39,180 --> 00:07:43,690
you can you can finally get one more general one without being too much obsolete

80
00:07:44,740 --> 00:07:49,710
that said people i think in general find course reader to be an asset we

81
00:07:49,710 --> 00:07:53,860
do not require reading it does have a lot of material with very

82
00:07:53,870 --> 00:07:57,010
useful and can understand the cause there are people who don't make is as big

83
00:07:57,010 --> 00:08:01,320
a use of that but there are some sections i think there is really very

84
00:08:01,320 --> 00:08:03,720
very valuable and the ones that may be more or less depending on the kind

85
00:08:03,720 --> 00:08:07,460
of your learning style whether you you appreciate more chance to be background material more

86
00:08:07,460 --> 00:08:12,970
examples and there's also a lot of good sample problems in an interview questions that

87
00:08:12,970 --> 00:08:18,200
helped texture understanding

88
00:08:18,210 --> 00:08:19,810
you know it's tricky

89
00:08:19,870 --> 00:08:24,450
right because the the universe in general discouraged from giving alternate exams

90
00:08:24,460 --> 00:08:29,720
because of of you can imagine the issues of having a example actually has been

91
00:08:29,720 --> 00:08:32,740
seen by some members to before some numbers since take it even though we're all

92
00:08:32,740 --> 00:08:36,240
bound by the honor code it does can create situation where there is some temptation

93
00:08:36,260 --> 00:08:40,220
it so it's possible to to be a little bit earlier without logging that but

94
00:08:40,220 --> 00:08:43,530
i think we have to make a lot of help is the truth rate we

95
00:08:43,530 --> 00:08:46,880
can talk about it that but the point is is is not i would say

96
00:08:46,880 --> 00:08:49,830
so if if there's something really important about that you know that you definitely know

97
00:08:49,830 --> 00:08:53,440
your enemy around the exam week that's going to be a problem you need to

98
00:08:53,440 --> 00:08:56,230
talk to me

99
00:08:58,990 --> 00:09:02,010
and then what about compilers that we

100
00:09:02,100 --> 00:09:06,910
you see both pos and we also use some custom libraries which limits us to

101
00:09:06,910 --> 00:09:10,260
us to determine a certain number of platforms we've had a good chance to test

102
00:09:10,260 --> 00:09:15,460
on and work with so the compilers that we have support for a is x

103
00:09:15,460 --> 00:09:19,530
connect ajax code is apple's free developer tools so anybody with mac OS ten can

104
00:09:19,530 --> 00:09:25,270
freely download that source libraries were using microsoft visual studio version two thousand five on

105
00:09:25,280 --> 00:09:29,010
windows and we have arranged with microsoft where they have distributed the software free of

106
00:09:29,010 --> 00:09:30,430
charge to students so on

107
00:09:30,490 --> 00:09:34,160
if you would like to install and run windows computer which gives some handouts on

108
00:09:34,160 --> 00:09:35,460
friday that tell you

109
00:09:35,460 --> 00:09:38,350
what you need to do to kind of go get the comparison get installed on

110
00:09:38,350 --> 00:09:42,620
two system so thank you to make sure that and are cluster computers in the

111
00:09:42,620 --> 00:09:44,300
dorms in libraries and

112
00:09:44,720 --> 00:09:45,720
the layer

113
00:09:45,770 --> 00:09:51,470
are you have both of our the maximum pcs have compilers and libraries installed issues

114
00:09:51,470 --> 00:09:55,290
working clustering do anything special it just walk up and it's got the stuff ready

115
00:09:55,290 --> 00:09:55,880
to go

116
00:10:01,140 --> 00:10:06,490
well i can ask me that is televised high and that person and that person

117
00:10:06,490 --> 00:10:10,140
forever so i can get over the fact that you go to the start menu

118
00:10:10,140 --> 00:10:16,900
to shut the machine down that makes when is impossible for radius so that i

119
00:10:16,900 --> 00:10:20,140
was in the campus wide you know this big people probably there is far more

120
00:10:20,140 --> 00:10:23,080
windows machines and max on campus so if you

121
00:10:23,470 --> 00:10:27,060
i want to take a popular vote you do that but if you want to

122
00:10:27,410 --> 00:10:29,910
be on the side of the mac big you can can be with me that's

123
00:10:29,940 --> 00:10:35,030
good i get to the rest of the visual studio also seems perfectly serviceable

124
00:10:38,430 --> 00:10:43,470
well look this i got ten minutes more

125
00:10:43,520 --> 00:10:45,950
to the people

126
00:10:45,950 --> 00:10:50,420
because that is the next journey right we're going to go on together

127
00:10:50,440 --> 00:10:54,510
but also like the which i think is why are we doing this do you

128
00:10:54,660 --> 00:10:58,570
like i i just java and now you tell me to throw in my job

129
00:10:58,570 --> 00:11:01,780
and start over OK so let's let's look

130
00:11:01,800 --> 00:11:04,270
in general below for the c plus plus OK

131
00:11:04,280 --> 00:11:09,650
but the advantages of early multilingualism so i have two small children

132
00:11:09,650 --> 00:11:14,770
two and four and the lot about bilingualism and stuff to raise your children chemical

133
00:11:14,810 --> 00:11:18,770
world and there's not very clear evidence for natural language is right that when you

134
00:11:18,800 --> 00:11:22,410
acquiring language you know at the young age is right that that the best time

135
00:11:22,410 --> 00:11:26,100
to introduce the second or third language we're kind of building all the pathways in

136
00:11:26,100 --> 00:11:30,900
honor of this really training research there and so the same thing that in terms

137
00:11:30,900 --> 00:11:33,910
of programming languages as well that when you're learning a programming language right there are

138
00:11:33,910 --> 00:11:38,760
certain kind of of rocks your mind gets into about the way language is that

139
00:11:38,770 --> 00:11:42,250
it is based on your experiences early on and if you spend a very long

140
00:11:42,250 --> 00:11:46,460
time working only in one language the wreckage deeper and you have a certain way

141
00:11:46,460 --> 00:11:50,500
of thinking where you are stuck in that paradigm and its approach what's easy to

142
00:11:50,500 --> 00:11:53,650
do in that language was hard to do in that language right now tends to

143
00:11:53,690 --> 00:11:57,350
make a stronger impression on you know it makes it harder as you as you

144
00:11:57,360 --> 00:12:00,550
grow and explore the languages to get out right to kind of shake itself that

145
00:12:00,550 --> 00:12:05,520
it so there's been some pretty good evidence that actually that about one-quarter between one

146
00:12:05,520 --> 00:12:08,740
and two is a really good time actually think about branch and a little bit

147
00:12:08,920 --> 00:12:12,200
and starting to think about different ways of doing that and seeing some different interactions

148
00:12:12,200 --> 00:12:18,600
some different ideas to help cannot just create building the flexibility from an early age

149
00:12:19,490 --> 00:12:22,090
in your career to kind of by u

150
00:12:22,170 --> 00:12:26,600
some strength later so that's what we're doing some of part it is actually a

151
00:12:26,600 --> 00:12:30,310
lot of the upper division courses right rely on knowledge of c and c plus

152
00:12:30,310 --> 00:12:35,290
plus the family of languages and the longer we postpone the kind of more painful

153
00:12:35,290 --> 00:12:37,050
it becomes because the

154
00:12:37,180 --> 00:12:41,770
in the later courses right we learn about compilers are graphics or networking they don't

155
00:12:41,790 --> 00:12:42,730
have the

156
00:12:42,790 --> 00:12:46,330
time of process stop and teachers seriously plus possible they need no simple fast to

157
00:12:46,330 --> 00:12:49,140
get the work done in sort of where we going make sure you get the

158
00:12:49,140 --> 00:12:52,230
foundation and to remove you get an improvement classes to make the most sense in

159
00:12:52,230 --> 00:12:57,220
the context of our curriculum so we we do switch over here the good news

160
00:12:57,220 --> 00:13:01,580
is right is that it's not is big changes it might sound first one seven

161
00:13:01,580 --> 00:13:05,800
going from a totally different language which job actually highly derivative c possible superstar writer

162
00:13:05,800 --> 00:13:07,700
for a fixed now

163
00:13:07,710 --> 00:13:08,870
we consider

164
00:13:08,880 --> 00:13:09,680
this is

165
00:13:09,700 --> 00:13:12,630
essentially conditioned on a fixed

166
00:13:14,840 --> 00:13:16,090
double sample

167
00:13:16,110 --> 00:13:19,740
and then we take expectation that over the choice of the double samples

168
00:13:19,760 --> 00:13:21,560
so we fix the double sample

169
00:13:21,570 --> 00:13:26,980
we consider permuting the elements what is the probability that when we permit

170
00:13:27,000 --> 00:13:31,730
by random choice from the sense that we have zero error on the right on

171
00:13:31,730 --> 00:13:37,410
the left-hand side only on the training sample and epsilon o two error

172
00:13:37,430 --> 00:13:39,270
on the ghost sample

173
00:13:44,220 --> 00:13:49,210
essentially know independently we can think of independently swapping these guys

174
00:13:49,220 --> 00:13:52,000
and how many of those what can we do

175
00:13:52,010 --> 00:13:53,180
in such away

176
00:13:53,190 --> 00:13:57,770
but we end up with all of the errors on the right-hand side OK so

177
00:13:57,770 --> 00:14:01,540
the overall in the whole double sample their exon

178
00:14:01,560 --> 00:14:07,140
and over two this is an error rates because they're in the second sample the

179
00:14:07,140 --> 00:14:10,940
actual number of barrels is epsilon over two

180
00:14:10,960 --> 00:14:12,420
or at least that many

181
00:14:12,430 --> 00:14:14,290
so epsilon m over two

182
00:14:14,310 --> 00:14:16,350
errors in this double sample

183
00:14:17,800 --> 00:14:20,340
in order for this property to be true

184
00:14:20,390 --> 00:14:22,120
in other words for this

185
00:14:22,130 --> 00:14:23,790
this quantity this

186
00:14:23,830 --> 00:14:27,260
being here to hold we need all of those areas to be on the right-hand

187
00:14:29,610 --> 00:14:31,690
i can say OK

188
00:14:31,700 --> 00:14:36,570
and because of the way we've chosen to problem the the set of sigma we

189
00:14:36,570 --> 00:14:37,870
have essentially

190
00:14:38,090 --> 00:14:43,410
independently the probability is one half of swapping those

191
00:14:43,430 --> 00:14:48,090
OK so that particular error on the site and its corresponding point on the other

192
00:14:48,090 --> 00:14:49,860
side could be swapped

193
00:14:49,880 --> 00:14:53,000
but we're not allowed to do that's what if we're going to keep this property

194
00:14:53,000 --> 00:14:55,760
that all of the errors on the right-hand side

195
00:14:55,770 --> 00:15:01,740
so very effectively that reduces the number of permutations that we can apply by half

196
00:15:01,740 --> 00:15:04,070
each time for each error

197
00:15:06,420 --> 00:15:07,760
the probability

198
00:15:07,770 --> 00:15:11,940
over this random choices to to the minus epsilon over two

199
00:15:11,950 --> 00:15:13,760
essentially half

200
00:15:13,780 --> 00:15:17,620
times to the power of the number of errors

201
00:15:17,700 --> 00:15:20,750
but we have on the right-hand side

202
00:15:20,760 --> 00:15:24,060
so for each of those i mean you can do just accounting argument is another

203
00:15:24,060 --> 00:15:27,350
way of thinking of it counts the permutations

204
00:15:27,390 --> 00:15:32,140
but actually leave those guys on the right-hand side but it's all permutations that do

205
00:15:32,140 --> 00:15:35,010
anything to the other pairs but don't change

206
00:15:35,650 --> 00:15:38,320
the set of points so as to to the

207
00:15:38,520 --> 00:15:43,330
you know and minus on and two and then divided by two to the end

208
00:15:43,330 --> 00:15:47,190
because that's the total number of permutations of that is that the ratio of the

209
00:15:47,620 --> 00:15:50,910
number that keep it happy with those that

210
00:15:51,600 --> 00:15:56,060
and that these two the minus epsilon two and the expectation that this is not

211
00:15:56,060 --> 00:15:57,980
independent of the

212
00:15:58,000 --> 00:16:02,340
of the actual set of sample points so

213
00:16:02,360 --> 00:16:06,270
the expectation is bounded the for every set of samples

214
00:16:06,290 --> 00:16:07,890
double samples

215
00:16:07,910 --> 00:16:12,330
it's bounded by two the mindset from end two then certain expectations bounded by two

216
00:16:12,350 --> 00:16:15,660
concepts on two

217
00:16:18,450 --> 00:16:21,870
any questions

218
00:16:30,440 --> 00:16:42,900
OK so the question was

219
00:16:43,250 --> 00:16:45,690
thank you right was that

220
00:16:45,710 --> 00:16:52,150
how do limitations affect the ghost sample of choice because some effectively

221
00:16:52,200 --> 00:16:55,810
OK so i think

222
00:16:55,820 --> 00:16:58,090
at the point where we start applying these

223
00:16:58,110 --> 00:16:59,550
limitations here

224
00:16:59,570 --> 00:17:01,020
at this point here

225
00:17:01,030 --> 00:17:06,250
we've effectively forgotten in a way that there was a sample of ghost sample

226
00:17:06,490 --> 00:17:11,870
and what we're thinking of doing is randomly mixing points with the ghost and between

227
00:17:11,870 --> 00:17:14,180
the ghost and the true sample

228
00:17:14,200 --> 00:17:15,840
so sort interchanging

229
00:17:16,030 --> 00:17:18,570
and was saying OK

230
00:17:18,590 --> 00:17:22,610
because of the way the probability stack up it shouldn't make any difference if we

231
00:17:23,490 --> 00:17:24,670
those two

232
00:17:24,760 --> 00:17:29,870
and but we find out that this is significant number we aren't allowed to swap

233
00:17:29,890 --> 00:17:34,130
because if we did it would actually make invalidate conclusions that we would join

234
00:17:34,280 --> 00:17:35,400
and so

235
00:17:35,410 --> 00:17:40,070
we're working out the fraction of those which invalidates and saying OK

236
00:17:40,130 --> 00:17:44,060
the probability then is bounded by that fraction

237
00:17:44,070 --> 00:17:48,140
OK effectively what we're saying is that the the errors if there are a lot

238
00:17:49,100 --> 00:17:51,940
it's weird that they're on the right-hand side

239
00:17:51,960 --> 00:17:56,840
that's weird it's unlikely you know it because you know this is an even training

240
00:17:56,840 --> 00:18:01,170
set this it shouldn't be there although there is on the right hand side

241
00:18:01,180 --> 00:18:03,150
and that we're bounding the unlike unlikely

242
00:18:03,160 --> 00:18:05,680
likelihood of that event

243
00:18:05,760 --> 00:18:10,640
this is just a technical way symmetrisation technical way of sort of

244
00:18:10,650 --> 00:18:12,770
making that intuition sort of

245
00:18:18,000 --> 00:18:19,740
the other

246
00:18:19,840 --> 00:18:22,820
questions is OK

247
00:18:25,380 --> 00:18:27,600
so the trick again is just too

248
00:18:27,610 --> 00:18:33,320
to consider this probability that the event holds under these random permutations where we swap

249
00:18:33,390 --> 00:18:34,790
corresponding elements

250
00:18:34,810 --> 00:18:36,300
and we know that

251
00:18:36,320 --> 00:18:39,820
in order to have this whole all of the error elements have to stay in

252
00:18:39,820 --> 00:18:44,550
and that's actually quite useful because many of the best algorithms things like plan shows

253
00:18:44,550 --> 00:18:49,250
methods or conjugate gradient for computing eigen values are actually what they're actually doing is

254
00:18:49,250 --> 00:18:53,430
trying to solve this problem there are actually optimisation algorithms

255
00:18:53,430 --> 00:18:57,210
so we're going to see is the max product and the sum product algorithms are

256
00:18:57,210 --> 00:18:59,590
also optimisation algorithms

257
00:18:59,640 --> 00:19:03,650
and didn't even talk about lagrange multipliers in

258
00:19:03,660 --> 00:19:06,940
OK so what we're going to see is that the messages

259
00:19:06,970 --> 00:19:10,700
right these kind of mysterious things here that are being updated that i just sort

260
00:19:10,700 --> 00:19:13,650
of pulled out of the hat we're going to see is if you take the

261
00:19:13,650 --> 00:19:19,650
logarithm of these messages those are going to be the lagrange multipliers for certain constraints

262
00:19:19,660 --> 00:19:24,720
right so whenever constrained optimisation problem and the messages are going to be away via

263
00:19:24,720 --> 00:19:37,220
lagrange multipliers are trying to enforce the constraints that's that's the variational interpretation

264
00:19:37,240 --> 00:19:39,270
OK so let me

265
00:19:39,340 --> 00:19:42,650
that was my review so if you're fast you so i was going to say

266
00:19:42,650 --> 00:19:45,920
let's let me work through it on this like so i think it's better for

267
00:19:45,920 --> 00:19:47,320
me and for you

268
00:19:47,320 --> 00:20:01,170
OK so let's

269
00:20:01,170 --> 00:20:06,420
let me do a simple reformulation let's again go back to a maximisation problem

270
00:20:06,440 --> 00:20:08,340
and let's

271
00:20:08,630 --> 00:20:10,690
the sort of think about a different way

272
00:20:10,710 --> 00:20:14,710
a different perspective we could take on that problem

273
00:20:14,710 --> 00:20:18,380
right so we do have a simple example will do just three notes that that

274
00:20:18,380 --> 00:20:21,440
sort of toy example but the ideas are general

275
00:20:21,460 --> 00:20:23,820
so it's me who's making the paper

276
00:20:25,590 --> 00:20:28,560
it doesn't take a rocket scientist to figure out why is that why am i

277
00:20:28,560 --> 00:20:29,760
so dirty

278
00:20:36,240 --> 00:20:41,070
OK so let's do px one x two x three

279
00:20:41,070 --> 00:20:44,940
that's what we're doing is we're maximizing

280
00:20:44,990 --> 00:20:49,090
this is the concrete let's do an example where everything is binary it is to

281
00:20:49,090 --> 00:20:51,050
get the intuition

282
00:20:51,050 --> 00:20:55,860
so what you're doing is you're trying to maximize this over

283
00:20:55,940 --> 00:21:00,190
overall binary configurations

284
00:21:00,570 --> 00:21:07,280
let me put the logarithm here too

285
00:21:07,340 --> 00:21:12,710
it doesn't matter if i maximize p or i maximise log of p

286
00:21:14,280 --> 00:21:17,400
let's think about a distribution factorizes let me do it

287
00:21:17,540 --> 00:21:21,570
up there on the slide i have overwritten the compatibility is in the form e

288
00:21:21,570 --> 00:21:23,360
to the data three

289
00:21:23,650 --> 00:21:28,440
exponential theta two three let me use those just because when i take logarithms and

290
00:21:28,440 --> 00:21:30,740
i'll get the fate is that will come down

291
00:21:30,760 --> 00:21:33,780
so if i use that form that i have up there this would be something

292
00:21:33,780 --> 00:21:34,860
like max

293
00:21:34,900 --> 00:21:37,780
of x one x two x three

294
00:21:37,820 --> 00:21:40,340
again binary and then i would get some

295
00:21:40,380 --> 00:21:43,710
i get some over as equals one to three

296
00:21:43,710 --> 00:21:46,090
they state as x of s

297
00:21:46,110 --> 00:21:50,650
and then i will get some over the edges in my graph whatever they are

298
00:21:50,670 --> 00:21:53,300
of the pairwise functions

299
00:21:55,420 --> 00:21:58,610
there might be a constant here too but we don't care about that someone trying

300
00:21:58,650 --> 00:22:02,990
to be constant from the normalisation

301
00:22:03,010 --> 00:22:12,010
OK so everything geometrically this is why i like this small example this is not

302
00:22:12,050 --> 00:22:15,900
a graphical model this this is that the space over which were searching

303
00:22:15,960 --> 00:22:19,690
the space over which searching is just binary hypercube

304
00:22:19,740 --> 00:22:24,380
it was searching over all configurations like zero zero zero here in one one one

305
00:22:24,480 --> 00:22:29,150
and zero one zero and so on

306
00:22:29,240 --> 00:22:32,630
right what makes the problem hard of course is that in high dimensions of a

307
00:22:32,630 --> 00:22:37,940
hypercube has many different vertices has exponentially many

308
00:22:37,960 --> 00:22:41,010
so what we're going to do is instead of searching over that space

309
00:22:41,170 --> 00:22:44,150
what i do this and introduce

310
00:22:44,150 --> 00:22:46,280
some other distribution

311
00:22:46,340 --> 00:22:53,440
it's called the distribution q

312
00:22:53,440 --> 00:22:55,650
so it's going to be

313
00:22:55,710 --> 00:23:00,380
a probability distribution over all binary vectors

314
00:23:00,400 --> 00:23:02,670
and i'm going to maximize

315
00:23:02,710 --> 00:23:05,380
the expectation

316
00:23:05,380 --> 00:23:07,740
so let me call this whole cost function

317
00:23:07,740 --> 00:23:10,610
it's called c

318
00:23:10,610 --> 00:23:12,460
x of data it's

319
00:23:12,480 --> 00:23:17,570
the cost function that depends on experts parameterized by data i could instead let's call

320
00:23:17,570 --> 00:23:23,380
it el maybe because you see the article l x of theta

321
00:23:24,460 --> 00:23:26,300
the claim is that

322
00:23:26,360 --> 00:23:29,110
i could instead solve this problem

323
00:23:29,130 --> 00:23:35,170
so we need to think about why this is the same thing to do

324
00:23:35,320 --> 00:23:41,900
so by the probability simplex in this case i just mean eight dimensional vectors

325
00:23:41,900 --> 00:23:47,110
nonnegative entries because that the probabilities and the sum to one

326
00:23:47,130 --> 00:23:50,690
right and what i'm doing is i maximizing over those vectors in any one of

327
00:23:50,690 --> 00:23:54,940
those vectors i can use it to take an expectation of my cost function

328
00:23:54,940 --> 00:24:00,460
so why are these two problems the same that's the question

329
00:24:05,460 --> 00:24:11,440
right so he said that your optimal solution what you would do is say one

330
00:24:11,440 --> 00:24:15,820
one one was optimal configuration you can always recover that by making q put mass

331
00:24:15,820 --> 00:24:17,130
one on

332
00:24:17,150 --> 00:24:21,300
that configuration and zeros everywhere else

333
00:24:22,510 --> 00:24:23,400
so that

334
00:24:23,420 --> 00:24:26,900
it's not clear to just sort of think about a little bit in one direction

335
00:24:26,900 --> 00:24:29,110
this direction is certainly clear

336
00:24:29,130 --> 00:24:32,530
because by doing this unjust just enlarging the space

337
00:24:32,530 --> 00:24:36,990
and i'm taking the convex hull somehow instead of just the vertices of to bigger

338
00:24:38,030 --> 00:24:42,690
because of the special structure of the objective function because the objective function is linear

339
00:24:42,690 --> 00:24:45,880
then in fact equality does hold here

340
00:24:46,010 --> 00:24:51,210
OK so it seems like OK fine but that doesn't really help us yet because

341
00:24:51,210 --> 00:24:56,630
i started with an exponentially large discrete space and now i have an exponentially large

342
00:24:56,630 --> 00:24:59,900
continuous space the set of all probability distributions

343
00:24:59,920 --> 00:25:03,340
right so it sort of seems like OK playing games but you haven't made the

344
00:25:03,340 --> 00:25:05,460
problem any easier

345
00:25:05,480 --> 00:25:08,740
but now we can do is expectations are linear

346
00:25:08,880 --> 00:25:12,570
and your cost function is linear and has terms that break up into turns on

347
00:25:12,570 --> 00:25:14,510
nodes in terms of edges

348
00:25:14,510 --> 00:25:16,940
so what i can do is i can move

349
00:25:16,940 --> 00:25:20,220
this is an expectation all use the shorthand

350
00:25:20,280 --> 00:25:22,900
e for expectation under q

351
00:25:22,920 --> 00:25:24,280
of l

352
00:25:26,170 --> 00:25:28,530
right that's the shorthand i can

353
00:25:28,570 --> 00:25:32,380
i can use that push the expectation inside the subterms

354
00:25:32,400 --> 00:25:36,980
so what will happen is that this will be equal to the maximum

355
00:25:37,900 --> 00:25:41,460
what i'm going to get here is a sum

356
00:25:41,510 --> 00:25:43,010
over my notes

357
00:25:43,010 --> 00:25:45,840
of expectation under q of

358
00:25:45,880 --> 00:25:49,190
the node terms

359
00:25:50,940 --> 00:25:55,610
expectation under q of an edge terms

360
00:25:55,650 --> 00:26:02,170
that's using linearity of expectation

361
00:26:02,190 --> 00:26:06,460
OK so that looks a bit more hopeful because somehow there's some amusing that i'm

362
00:26:06,460 --> 00:26:11,390
two true known motif in this example so it has biological meaning

363
00:26:11,420 --> 00:26:15,110
and the red guy which is actually slightly more stable

364
00:26:15,970 --> 00:26:21,050
biologists a reasonable one but i don't want to make more claims to others good

365
00:26:21,050 --> 00:26:23,160
additional supportive evidence

366
00:26:23,210 --> 00:26:24,230
but is

367
00:26:24,250 --> 00:26:29,890
stop there but nevertheless by one of the true ones and an interesting one which

368
00:26:30,210 --> 00:26:32,670
is stable

369
00:26:32,980 --> 00:26:35,840
so what can we learn from these examples

370
00:26:35,850 --> 00:26:39,430
using the is haslam that just from las

371
00:26:39,440 --> 00:26:43,990
four is serialised said before this is very questionable it

372
00:26:44,080 --> 00:26:48,730
from a theoretical point of view many things can go wrong as i explained before

373
00:26:49,810 --> 00:26:52,520
if you even would get something out here

374
00:26:52,590 --> 00:26:57,510
the issues you have not sign any measure of somehow uncertainty

375
00:26:57,510 --> 00:26:59,550
or stability or something like that

376
00:26:59,560 --> 00:27:02,760
and this is no way what i would like to

377
00:27:02,770 --> 00:27:06,730
so this is what we call stability selection which is exactly this idea

378
00:27:06,770 --> 00:27:11,770
it's based on subsampling or bootstrapping is not the big deal here

379
00:27:11,790 --> 00:27:16,270
and so let's begin worked first for the linear model international you also things for

380
00:27:16,350 --> 00:27:19,540
africa so here's my linear model

381
00:27:19,550 --> 00:27:22,230
i want to have the set of active variables

382
00:27:22,270 --> 00:27:23,800
now i have a

383
00:27:23,820 --> 00:27:28,430
variable selection procedure your favourite one the last a prime example just you know it

384
00:27:28,430 --> 00:27:33,550
but it's hard land land some sort of regularisation parameter but you can use boosting

385
00:27:33,620 --> 00:27:39,070
or you can use whatever your favourite feature selection algorithm

386
00:27:40,070 --> 00:27:41,460
this is the setting

387
00:27:41,960 --> 00:27:44,270
now what you do is what i just explained

388
00:27:44,320 --> 00:27:47,410
you draw a subsample of sydenham

389
00:27:47,480 --> 00:27:54,100
without replacement denoted by i started subsample run the selection algorithm on this i start

390
00:27:54,100 --> 00:27:58,100
on these random subsample i do it many times and at the end of the

391
00:27:58,680 --> 00:28:02,900
i just count the relative selection frequencies which you can write you down with the

392
00:28:02,900 --> 00:28:07,260
peace dollar i have the probability that the j th covariate

393
00:28:07,270 --> 00:28:09,680
it's actually selected

394
00:28:11,410 --> 00:28:14,260
among these subsampling runs OK

395
00:28:14,300 --> 00:28:17,890
this is nothing else in the relative selection frequencies if you do it on the

396
00:28:19,770 --> 00:28:28,060
OK you could use bootstrap sampling gives you empirically and theoretically almost the same result

397
00:28:28,080 --> 00:28:31,810
OK then what are you doing this this now you just have

398
00:28:31,870 --> 00:28:33,360
this thing here

399
00:28:35,970 --> 00:28:38,250
and then you want to say OK now

400
00:28:38,270 --> 00:28:41,620
what is it good variable while a hundred percent

401
00:28:41,640 --> 00:28:45,580
stability great ninety six percent is pretty good

402
00:28:45,590 --> 00:28:49,420
but what about eighty percent is stable or not should we have this variable or

403
00:28:50,890 --> 00:28:53,450
and i will come to this point

404
00:28:53,460 --> 00:28:55,280
but just

405
00:28:55,360 --> 00:29:01,280
define now the set of stably selected variables these are just the covariance between relatives

406
00:29:01,280 --> 00:29:07,650
selection frequency is bigger than a certain threshold because the percent between nineteen percent

407
00:29:07,650 --> 00:29:13,430
don't pick your ninety five percent from confidence interval this is quite a different story

408
00:29:13,530 --> 00:29:17,490
OK now the thing will be the choice of this threshold and i will come

409
00:29:17,490 --> 00:29:19,700
to that in a so

410
00:29:19,710 --> 00:29:23,840
so there's also the issue

411
00:29:23,860 --> 00:29:27,740
people look at the regularisation path and you can do that here as well

412
00:29:27,890 --> 00:29:30,870
is modified finish never stable variable

413
00:29:30,900 --> 00:29:35,080
you say i'm mean here i have relative selection frequencies

414
00:29:35,220 --> 00:29:39,390
as a function of my regularisation parameter here was for the land and then i

415
00:29:39,390 --> 00:29:41,520
just see which of the variable

416
00:29:41,530 --> 00:29:43,740
pops up a lot which not

417
00:29:43,750 --> 00:29:46,430
and that just brought racial line defined

418
00:29:46,450 --> 00:29:52,550
to the set of state variables as the one where the relative selection frequencies just

419
00:29:52,590 --> 00:29:58,830
show this line that some of these land k so whenever a line goes above

420
00:29:58,990 --> 00:30:05,620
whenever curve goes above a fixed threshold variable is in my statement

421
00:30:05,640 --> 00:30:11,370
OK so this is a bit exploratory but it's quite interesting to see what's happening

422
00:30:12,020 --> 00:30:16,210
so this is again my riboflavin my by the production example i've shown to the

423
00:30:16,210 --> 00:30:17,400
very beginning

424
00:30:17,420 --> 00:30:22,220
so why is the riboflavin production rate exceeds high dimensional covariance

425
00:30:22,240 --> 00:30:25,900
and now i do this but i just briefly mention to break into the senior

426
00:30:25,920 --> 00:30:28,360
artificial semen synthetic data

427
00:30:28,390 --> 00:30:30,510
what i do is i have six

428
00:30:30,520 --> 00:30:32,560
variables six covariates

429
00:30:32,580 --> 00:30:37,030
which are important also from the biological point of view i leave them

430
00:30:37,040 --> 00:30:39,340
that permute all other variables

431
00:30:39,350 --> 00:30:43,220
so by doing that i know all these permuted variables they have to be noise

432
00:30:43,230 --> 00:30:44,730
variables if it

433
00:30:44,750 --> 00:30:48,230
this is noise i know about the noise and not a hundred percent sure about

434
00:30:48,230 --> 00:30:52,890
through but i think the six variables or through real variables

435
00:30:52,900 --> 00:30:58,140
now you have here the last so regularisation of solution path as a function of

436
00:30:59,400 --> 00:31:05,150
this is quite common that you look at the regularisation path and the path

437
00:31:05,160 --> 00:31:09,980
the lines the red line to correspond to the six non permuted variables supposedly the

438
00:31:09,980 --> 00:31:11,270
true variables

439
00:31:11,270 --> 00:31:16,560
and the black lines correspond to attributed the noise variables

440
00:31:16,580 --> 00:31:22,000
and here is the stability selection scale here is the relative selection frequencies again the

441
00:31:22,000 --> 00:31:27,690
red one the non permuted black then always able bodied to see exploratory from these

442
00:31:30,040 --> 00:31:34,710
fifty four or up to six through variables

443
00:31:34,720 --> 00:31:39,040
the red line here they stick out much more clearly from the noise covariance

444
00:31:39,060 --> 00:31:43,920
and here in your original as to just by looking at this picture i hope

445
00:31:43,920 --> 00:31:49,450
i can convince you that the stability selection scale so to speak have a much

446
00:31:49,450 --> 00:31:55,360
better chance such that the true signal can be distinguished from noise

447
00:31:55,380 --> 00:32:00,480
OK clearly i mean the stability selection is not just the issue using the land

448
00:32:00,480 --> 00:32:03,580
the right it just provides the fundamentally

449
00:32:03,580 --> 00:32:10,410
different solution path and this one is from the last of these outer subset exercise

450
00:32:10,420 --> 00:32:15,860
OK an interesting question is really there should be put the threshold right how should

451
00:32:15,860 --> 00:32:18,850
you choose to traditional

452
00:32:18,860 --> 00:32:22,090
and he was an answer which is kind of based on some

453
00:32:22,110 --> 00:32:24,850
theoretical or guiding principle

454
00:32:26,760 --> 00:32:29,780
it's a very general theorem which i tried to

455
00:32:31,230 --> 00:32:34,460
and what we have here is just consider any

456
00:32:34,510 --> 00:32:39,200
selection procedure and make things a bit simpler to explain the theorems just take a

457
00:32:39,200 --> 00:32:45,090
selection procedure b which selects q variables to topic you variables for example if the

458
00:32:45,090 --> 00:32:46,130
loss of

459
00:32:46,140 --> 00:32:51,700
the variables which have larger estimated coefficients are the ones which come first in the

460
00:32:51,700 --> 00:32:57,960
regularisation or forward selection just the first fifty variables

461
00:32:57,960 --> 00:33:01,900
your favourite procedure takes the qprop variables

462
00:33:02,830 --> 00:33:04,460
and then you you know by the

463
00:33:04,470 --> 00:33:08,760
the number of false positives so this is kind of the non active sedentary is

464
00:33:08,760 --> 00:33:13,390
the stably selected said that have been said before

465
00:33:13,400 --> 00:33:14,980
and here is the theory

466
00:33:15,010 --> 00:33:18,950
it has the main assumption which i will explain on the next slide

467
00:33:18,950 --> 00:33:23,600
the mean squared error that is proportional to the information content of the object i

468
00:33:23,600 --> 00:33:28,690
wish we cover so for example if xs s nonzero components is it possible to

469
00:33:29,520 --> 00:33:31,640
i mean squared error that will scale

470
00:33:31,670 --> 00:33:35,430
was the complexity of the output of the object that is is the object gets

471
00:33:35,430 --> 00:33:42,040
sparser and sparser the mean squared error comes down and down and down

472
00:33:43,770 --> 00:33:44,530
and so

473
00:33:44,540 --> 00:33:48,460
to think about these problems i always find it useful to think about it

474
00:33:48,460 --> 00:33:50,600
as having the help of an oracle

475
00:33:50,620 --> 00:33:51,730
and i recall

476
00:33:51,760 --> 00:33:55,720
we do the following things suppose that our calls

477
00:33:56,690 --> 00:33:57,940
is going to

478
00:33:57,960 --> 00:34:03,450
big essentially giveaways the problem is going to say many i've seen your genes

479
00:34:03,480 --> 00:34:07,430
i know which genes are responsible for you disease i've seen them

480
00:34:07,440 --> 00:34:09,430
i i'm going to give them to you

481
00:34:09,450 --> 00:34:13,940
and you know know you want to make estimate x but the article tells you

482
00:34:13,960 --> 00:34:18,760
well i see x and i can tell you that x seventeen is not zero

483
00:34:18,770 --> 00:34:22,540
i can tell you that a hundred and forty three is nonzero and i can

484
00:34:22,540 --> 00:34:24,940
tell you that the two seventeenth

485
00:34:24,940 --> 00:34:30,460
two seven gene is now use this region out everybody else's

486
00:34:30,480 --> 00:34:33,230
and of course that would be giving out the problem in let's say we have

487
00:34:33,230 --> 00:34:38,450
such a genie that comes around and says i have seen many tell you which

488
00:34:38,450 --> 00:34:41,280
ones are zero and which were not

489
00:34:41,300 --> 00:34:47,270
now the problem is underdetermined anymore because it's another gene is more than the number

490
00:34:47,520 --> 00:34:50,760
of observations is not underdetermined so i would say

491
00:34:50,770 --> 00:34:55,540
but look at all these genes don't contribute so i'm going to remove them

492
00:34:55,560 --> 00:34:58,410
and i'm going to create the submatrix

493
00:35:01,470 --> 00:35:06,150
that essentially only takes the columns corresponding to the locations of the genes

494
00:35:06,150 --> 00:35:11,700
so column seventeen columns one hundred forty three and columns to seven

495
00:35:11,720 --> 00:35:14,050
so everybody else is irrelevant

496
00:35:14,070 --> 00:35:18,170
thank you very much you give me a very precise information now let me delete

497
00:35:18,170 --> 00:35:23,510
everything that is relevant me just fit the data to what i know is relevant

498
00:35:23,530 --> 00:35:26,500
so how do i do this well by least square what i would say that

499
00:35:27,330 --> 00:35:30,910
i'm going to remove the relevant variables learning for

500
00:35:30,910 --> 00:35:33,890
on the complement of TT beings

501
00:35:33,940 --> 00:35:36,150
index set corresponding to the

502
00:35:36,170 --> 00:35:38,510
columns where x is non-zero

503
00:35:38,540 --> 00:35:43,210
one issue impose that xt complement zero and otherwise i'm just going to do squares

504
00:35:43,220 --> 00:35:47,000
now it's not underdetermined fact after many observations and all of us

505
00:35:47,050 --> 00:35:54,130
and i call this an idealist squares estimate because of course the g comes in

506
00:35:54,130 --> 00:35:58,360
and gives away the problem telling me exactly ahead of time which genes

507
00:35:58,380 --> 00:36:03,930
telling me exactly which is a significant and which ones are not

508
00:36:03,950 --> 00:36:09,240
OK and then i go back now it's becoming very classical ideally square and so

509
00:36:09,240 --> 00:36:12,420
i would just regress to use statistical

510
00:36:12,430 --> 00:36:18,210
language we regressive observed response y onto the variables that i know are in my

511
00:36:19,730 --> 00:36:21,850
that would be my

512
00:36:22,950 --> 00:36:26,180
and outside i would foresee estimator be

513
00:36:26,190 --> 00:36:27,690
and when i do this

514
00:36:27,700 --> 00:36:31,350
it's a very simple calculation i have squares estimator

515
00:36:31,370 --> 00:36:33,440
it's bias is zero here

516
00:36:33,460 --> 00:36:35,050
and it's variance

517
00:36:35,060 --> 00:36:37,030
it's essentially the trace

518
00:36:37,150 --> 00:36:41,210
one of the hat matrix will be

519
00:36:41,220 --> 00:36:42,580
so the trace of

520
00:36:42,610 --> 00:36:45,740
they start t eighty

521
00:36:47,150 --> 00:36:51,250
very simple things can also calculate the mean squared error

522
00:36:51,280 --> 00:36:54,400
and then you have the trees and now if you have this kind of restricted

523
00:36:54,400 --> 00:37:00,610
isometry property this submatrices of a well they have can values between point five and

524
00:37:00,610 --> 00:37:01,910
one point five

525
00:37:01,910 --> 00:37:04,720
that's all the igon values of

526
00:37:04,730 --> 00:37:08,210
they start t eighty are between about one

527
00:37:08,220 --> 00:37:13,260
roughly between point five and two let's say is it all about one

528
00:37:13,280 --> 00:37:15,110
and so when you calculate this

529
00:37:15,120 --> 00:37:16,250
you say well

530
00:37:16,280 --> 00:37:20,580
if all the eigen values are between about one i mean do i

531
00:37:20,600 --> 00:37:24,300
how many do i have well the size of t that's as sparsity

532
00:37:24,320 --> 00:37:28,650
so the mean squared error is about this time sigma square and what we discover

533
00:37:28,650 --> 00:37:30,380
is with the help of an are

534
00:37:30,460 --> 00:37:32,980
i can get

535
00:37:33,020 --> 00:37:37,720
and which is much smaller than enhancing must for example there are three genes

536
00:37:37,770 --> 00:37:41,160
you know the previous results so the the air is about the number of patients

537
00:37:41,160 --> 00:37:42,960
time sigma square this would say

538
00:37:42,970 --> 00:37:44,580
o point three six

539
00:37:44,600 --> 00:37:49,530
it's just proportional to the true number of components as the true number of things

540
00:37:49,530 --> 00:37:53,170
that need to estimate and not anything else

541
00:37:54,420 --> 00:37:57,630
and of course i would never be able to get such rules mean squared error

542
00:37:57,630 --> 00:38:00,810
because i get to se to get there

543
00:38:01,330 --> 00:38:02,950
i need to have access to

544
00:38:02,960 --> 00:38:06,180
information that of course i don't

545
00:38:06,190 --> 00:38:09,370
and so the question is can we just do as well as our bodies can

546
00:38:09,370 --> 00:38:13,980
we have an mean squared that is adaptive in the sense that it really depends

547
00:38:13,980 --> 00:38:15,290
on the complexity

548
00:38:15,570 --> 00:38:18,450
of what is it that we're trying to estimate

549
00:38:19,370 --> 00:38:20,970
OK so to do this

550
00:38:22,050 --> 00:38:23,390
one way to do this

551
00:38:23,900 --> 00:38:26,580
is to kind of by semantic selection

552
00:38:26,600 --> 00:38:28,230
which is

553
00:38:28,660 --> 00:38:32,250
and the estimators that i propose with terence tao

554
00:38:32,280 --> 00:38:34,770
and so we have model

555
00:38:34,790 --> 00:38:38,620
now we can again use and l one criterion except that now serves as a

556
00:38:38,620 --> 00:38:41,020
data fitting term is slightly different

557
00:38:41,030 --> 00:38:44,160
i'm going to say is well i'm going to it data by getting minimizing the

558
00:38:44,160 --> 00:38:45,770
l one norm

559
00:38:45,790 --> 00:38:50,420
and imposing their will look at the residual vector there is a mismatch between the

560
00:38:50,420 --> 00:38:53,110
data and my eight times my guess

561
00:38:53,150 --> 00:38:56,460
my predictions if you will the residuals lectures

562
00:38:56,510 --> 00:39:00,030
then when i correlated this residual ways

563
00:39:00,050 --> 00:39:04,140
the columns of a that is when i look at the vector is star times

564
00:39:04,140 --> 00:39:05,370
the residual

565
00:39:05,380 --> 00:39:09,220
and all the components of these vectors are small

566
00:39:09,250 --> 00:39:12,700
so what it says it's minimized in one norm

567
00:39:12,750 --> 00:39:18,600
as usual announcer data fitting term as the the residual not be not correlated

568
00:39:18,620 --> 00:39:21,140
where is the columns of

569
00:39:21,190 --> 00:39:22,980
so it says

570
00:39:23,010 --> 00:39:26,480
now when i look at a star are

571
00:39:26,500 --> 00:39:28,470
it's as if

572
00:39:28,540 --> 00:39:31,020
if they are use the ice

573
00:39:31,040 --> 00:39:34,300
variable or the ice column of the matrix

574
00:39:34,320 --> 00:39:39,350
it's a is the vector AI times are

575
00:39:39,370 --> 00:39:42,720
is small

576
00:39:42,800 --> 00:39:47,710
it must be within the noise level sigma the standard deviation

577
00:39:47,710 --> 00:39:53,970
OK so i'm asking that the signal the residuals cannot be well aligned with the

578
00:39:53,970 --> 00:39:59,730
predictor variables because if they were actually probably as include them in my mind

579
00:39:59,760 --> 00:40:04,430
so it's convex optimisation programme it's actually a linear programming disguise that's why called the

580
00:40:04,430 --> 00:40:06,120
dantzig selector

581
00:40:06,120 --> 00:40:08,610
in honor of

582
00:40:08,640 --> 00:40:13,060
of george then six father of linear programming and so it's a something that is

583
00:40:13,070 --> 00:40:15,200
easy to solve

584
00:40:17,400 --> 00:40:19,240
all right so

585
00:40:19,270 --> 00:40:22,010
the result that we have when we try to get to see is more subtle

586
00:40:22,010 --> 00:40:24,330
i configurator quote unquote

587
00:40:24,380 --> 00:40:26,500
by such

588
00:40:28,370 --> 00:40:31,940
we got all we have to do we have to tell for instance for this

589
00:40:33,100 --> 00:40:34,820
this sort of here

590
00:40:34,820 --> 00:40:36,210
test points

591
00:40:37,240 --> 00:40:38,850
three and four

592
00:40:38,870 --> 00:40:42,140
so that would be circled the

593
00:40:45,810 --> 00:40:47,900
but if you look

594
00:40:47,960 --> 00:40:50,980
more carefully you see that this is not

595
00:40:50,990 --> 00:40:54,580
the configuration that i was talking about because now we have two

596
00:40:54,600 --> 00:40:56,300
it lies

597
00:40:57,680 --> 00:40:58,640
for instance

598
00:40:59,590 --> 00:41:01,990
this article that meets

599
00:41:02,720 --> 00:41:04,130
different point

600
00:41:04,760 --> 00:41:08,190
one axiom of configurations of points and lines

601
00:41:08,320 --> 00:41:10,240
it's not satisfied

602
00:41:10,250 --> 00:41:15,150
but these structures can be studied in the past and there are also interesting

603
00:41:18,740 --> 00:41:20,390
let's go back

604
00:41:20,410 --> 00:41:23,460
let's take this configuration table

605
00:41:30,420 --> 00:41:33,090
that living graph to this configuration

606
00:41:33,110 --> 00:41:38,160
so now i have green and yellow points not black and white

607
00:41:38,220 --> 00:41:40,880
and four points correspond

608
00:41:40,910 --> 00:41:47,130
the four vertices correspond to points one to up to four and four let's say

609
00:41:48,180 --> 00:41:51,030
of vertices corresponds to a b c

610
00:41:51,080 --> 00:41:53,090
so let's look what we have here

611
00:41:53,120 --> 00:41:56,360
for instance

612
00:41:56,400 --> 00:41:57,940
if we

613
00:41:58,660 --> 00:42:01,830
this is a

614
00:42:01,850 --> 00:42:04,730
then a

615
00:42:04,790 --> 00:42:05,790
consists of

616
00:42:05,800 --> 00:42:07,860
o point one

617
00:42:07,890 --> 00:42:09,660
o point two

618
00:42:10,680 --> 00:42:13,920
o point four

619
00:42:13,970 --> 00:42:15,320
so what now

620
00:42:15,340 --> 00:42:17,530
one could go and try to

621
00:42:17,540 --> 00:42:18,910
write down everything

622
00:42:18,930 --> 00:42:21,210
but if you read this

623
00:42:22,390 --> 00:42:23,890
then you get this

624
00:42:23,940 --> 00:42:26,730
nice graph which looks like it you

625
00:42:29,220 --> 00:42:41,070
of this is just this graph drawing the different way

626
00:42:42,100 --> 00:42:46,120
in this configuration here or in this this

627
00:42:47,320 --> 00:42:49,000
you have in fact

628
00:42:49,010 --> 00:42:52,730
all the information needed

629
00:42:52,750 --> 00:42:53,530
to get

630
00:42:53,610 --> 00:42:57,370
the graph of a few

631
00:42:57,470 --> 00:43:02,520
let's keep this because we have too much

632
00:43:02,560 --> 00:43:03,970
motivation so

633
00:43:04,010 --> 00:43:08,030
let me give you another example

634
00:43:08,040 --> 00:43:13,520
this comes also from geometry is called mechanical calculation

635
00:43:15,840 --> 00:43:17,150
you have

636
00:43:19,800 --> 00:43:22,040
again points in circles

637
00:43:22,340 --> 00:43:35,170
these two are considered as degenerate circles

638
00:43:38,370 --> 00:43:42,560
let me give you an example

639
00:43:42,580 --> 00:43:43,940
this one here

640
00:43:43,990 --> 00:43:48,100
if you take an arbitrary graph

641
00:43:48,150 --> 00:43:52,570
it may be considered as an incidence structure

642
00:43:52,580 --> 00:43:56,530
so we need to the types of objects

643
00:43:56,730 --> 00:44:15,310
we need the types of objects

644
00:44:15,360 --> 00:44:16,300
but this is

645
00:44:16,300 --> 00:44:17,820
one two three four

646
00:44:17,870 --> 00:44:21,210
and edges a b c d e

647
00:44:21,220 --> 00:44:24,480
and what is the as well as the if

648
00:44:24,870 --> 00:44:26,790
vertex one

649
00:44:26,810 --> 00:44:31,640
it is it this with the edge a then that would these

650
00:44:31,660 --> 00:44:33,170
relations so

651
00:44:33,480 --> 00:44:35,550
if i say p

652
00:44:35,550 --> 00:44:37,510
equals one two

653
00:44:37,520 --> 00:44:38,770
three four

654
00:44:40,710 --> 00:44:42,550
equals a b

655
00:44:43,220 --> 00:44:46,390
the the then i

656
00:44:46,410 --> 00:44:48,070
it will be the set of

657
00:44:48,070 --> 00:44:50,370
let's say

658
00:44:54,240 --> 00:44:59,920
a and so on all flanks so flags here in this case in this case

659
00:45:00,400 --> 00:45:02,090
just hours

660
00:45:02,120 --> 00:45:05,520
but we talk about that later when you get to graphs which we're not

661
00:45:05,530 --> 00:45:06,500
about it

662
00:45:06,520 --> 00:45:11,050
and there are other examples but this is

663
00:45:11,050 --> 00:45:13,660
and you can see why this is

664
00:45:13,740 --> 00:45:16,070
imagine i took rotation matrix

665
00:45:16,090 --> 00:45:19,990
are OK so it's going to have the property that are transpose is is the

666
00:45:21,880 --> 00:45:25,640
i know i have multiplied by lambda as by are OK i so lambda is

667
00:45:25,640 --> 00:45:27,590
not equal to lambda times are

668
00:45:27,700 --> 00:45:31,030
you can see that lambda times than the transpose doesn't change

669
00:45:31,090 --> 00:45:31,950
all right

670
00:45:31,950 --> 00:45:34,360
so what this is saying is that

671
00:45:34,370 --> 00:45:35,870
you know

672
00:45:35,870 --> 00:45:36,910
if i

673
00:45:36,910 --> 00:45:40,370
if i if i do PCA and i sort of have you know strong idea

674
00:45:40,370 --> 00:45:45,550
of how the dimensions are like we're directions of maximum variance are then i'm always

675
00:45:45,550 --> 00:45:50,600
going to get the same representation in in my in my two d space right

676
00:45:50,600 --> 00:45:54,570
if i were to run factor analysis many times it's possible that i'm going to

677
00:45:54,570 --> 00:45:59,510
get this representation but you know rotated so the relation between

678
00:46:00,120 --> 00:46:03,200
the the means is going to be always saying is just i think my sheet

679
00:46:03,200 --> 00:46:04,950
of paper sort of

680
00:46:04,990 --> 00:46:08,550
can rotate around OK

681
00:46:08,590 --> 00:46:13,370
the other thing just to keep the just to keep the bayes in hide alive

682
00:46:13,430 --> 00:46:16,950
the proper thing to do would be to

683
00:46:16,990 --> 00:46:22,300
to actually specify priors on on land and on

684
00:46:22,340 --> 00:46:24,800
on the noise right and just

685
00:46:24,800 --> 00:46:26,950
compute posteriors and turned

686
00:46:26,990 --> 00:46:29,140
marginalisation sort of go crazy

687
00:46:29,160 --> 00:46:35,510
but i'm not going to talk about

688
00:46:35,510 --> 00:46:42,180
that's correct yes so the way the way you treat it is used up here

689
00:46:42,180 --> 00:46:43,740
you take you take the

690
00:46:43,760 --> 00:46:46,300
likelihood so just as

691
00:46:46,360 --> 00:46:48,840
as a reminder

692
00:46:48,990 --> 00:46:53,360
this is a gaussian distribution rights of these actually that all the

693
00:46:53,450 --> 00:46:55,470
is actually the matrix of all the

694
00:46:55,490 --> 00:46:57,240
otherwise would be

695
00:46:57,240 --> 00:47:01,390
and times the matrix of otherwise it's interesting because you use and for the number

696
00:47:01,390 --> 00:47:05,340
of data points and attend and and for the that mentions and i tend to

697
00:47:05,340 --> 00:47:09,640
use and for the number of data points and the for the dimensions but i

698
00:47:09,640 --> 00:47:11,640
guess one has to get used

699
00:47:11,680 --> 00:47:13,700
the fact that this

700
00:47:13,720 --> 00:47:15,590
many many ways of

701
00:47:15,640 --> 00:47:19,220
using this notation so this is the gas and has zero mean and he has

702
00:47:19,240 --> 00:47:22,340
covariance matrix

703
00:47:22,390 --> 00:47:26,820
lambda lambda transpose plus the noise diagonal OK and then i could do exactly the

704
00:47:26,820 --> 00:47:30,200
same thing as we did when we maximised

705
00:47:30,240 --> 00:47:33,860
the likelihood of the normal girls of the double the standard multivariate gaussians take the

706
00:47:33,860 --> 00:47:36,200
log on i think derivatives

707
00:47:36,260 --> 00:47:38,260
and maybe you should just right

708
00:47:38,300 --> 00:47:41,990
it's it's nice derivatives come out it's a lot of fun

709
00:47:41,990 --> 00:47:46,160
OK so

710
00:47:46,160 --> 00:47:51,450
what we've seen is a specific case of of the so called latent variable model

711
00:47:52,160 --> 00:47:54,410
because we had this

712
00:47:54,410 --> 00:48:00,030
we invented some variables that they weren't related to anything we could see OK

713
00:48:00,090 --> 00:48:04,120
and the important thing is we didn't try to learn their value either

714
00:48:04,140 --> 00:48:05,620
we just use them

715
00:48:06,260 --> 00:48:09,700
to generate our model to generate all distribution OK

716
00:48:09,760 --> 00:48:11,160
we don't

717
00:48:11,200 --> 00:48:16,760
bear in mind we do not maximize anything with respect to x right

718
00:48:16,800 --> 00:48:20,780
we take x the sources

719
00:48:20,870 --> 00:48:23,090
we invent some prior on them

720
00:48:23,120 --> 00:48:24,660
and then we sort of say

721
00:48:24,700 --> 00:48:28,180
they're gonna generate the data using the land matrices

722
00:48:28,870 --> 00:48:32,660
but i'm not interested in the axis so let's just integrate them out right so

723
00:48:32,660 --> 00:48:40,340
that's how we obtained actually all prior distribution otherwise we did by saying we have

724
00:48:40,340 --> 00:48:43,530
y is equal to lambda explores noise

725
00:48:45,910 --> 00:48:49,990
and now i been myself p of y given x

726
00:48:50,070 --> 00:48:54,120
and now what i do is i integrate this out using the prior over x

727
00:48:55,800 --> 00:48:57,720
i'm not interested in x

728
00:48:57,740 --> 00:49:03,340
i just using it conceptually to build model OK so that's the kind of

729
00:49:03,390 --> 00:49:06,820
that's the kind of stuff we've done right so we had the

730
00:49:06,820 --> 00:49:09,180
a a prior over the hidden sources

731
00:49:09,180 --> 00:49:12,700
we knew how to relate that to to whatever we were gonna

732
00:49:13,890 --> 00:49:16,320
we could be the joint distribution by

733
00:49:16,340 --> 00:49:18,360
multiplying two and then

734
00:49:18,370 --> 00:49:21,740
integrate out marginalise of the axes

735
00:49:21,760 --> 00:49:24,930
and the typical example for you is

736
00:49:24,990 --> 00:49:28,740
you know if you're if you worked with the dimensionality reduction

737
00:49:28,780 --> 00:49:33,090
who's who's on any dimensionality reduction

738
00:49:33,760 --> 00:49:35,620
what what what have you used

739
00:49:35,660 --> 00:49:39,360
besides PCA

740
00:49:40,090 --> 00:49:44,390
so SVD is is very similar to PCA right

741
00:49:44,430 --> 00:49:49,530
it is the same spirit really so what gets used

742
00:49:49,550 --> 00:49:52,530
OK that's interesting

743
00:49:52,590 --> 00:49:54,370
so that was four

744
00:49:54,390 --> 00:49:57,510
you also had targets he was supervised learning problem

745
00:49:58,410 --> 00:50:01,160
what is

746
00:50:02,390 --> 00:50:04,450
feature selection absolutely

747
00:50:04,850 --> 00:50:10,640
has anyone use the elderly

748
00:50:15,090 --> 00:50:17,910
OK those are nonlinear techniques

749
00:50:17,950 --> 00:50:20,200
has anyone use the

750
00:50:20,200 --> 00:50:21,720
back constrained

751
00:50:21,720 --> 00:50:24,050
latent variable

752
00:50:24,160 --> 00:50:27,360
get back constraints i don't even remember the name

753
00:50:27,470 --> 00:50:30,870
got process latent variable models

754
00:50:30,930 --> 00:50:35,720
as the band OK anyway

755
00:50:36,720 --> 00:50:39,860
so if we were to learn if we want to know to learn

756
00:50:39,870 --> 00:50:44,370
the parameters of this model that lambda and the matrix of noise we would take

757
00:50:45,370 --> 00:50:47,620
the logarithm of the gaussians

758
00:50:47,640 --> 00:50:52,150
and if we want to minimize because we just have coded does minimisation is maximisation

759
00:50:52,150 --> 00:50:56,660
we just change the sign and then we get this objective function and

760
00:50:56,680 --> 00:51:00,070
if you want to have some fun as i say you just take gradients of

761
00:51:00,070 --> 00:51:03,470
this thing with respect to london with respect to the to the noise

762
00:51:03,510 --> 00:51:07,970
it's actually not that hard once once you've taken that this kind of gradients in

763
00:51:08,010 --> 00:51:09,390
you know forever

764
00:51:13,340 --> 00:51:14,390
so tomorrow

765
00:51:14,430 --> 00:51:20,470
actually will not be here so we will not be studying the EMI grid tomorrow

766
00:51:20,510 --> 00:51:28,570
but we will be starting today

767
00:51:28,590 --> 00:51:34,430
so before we take a break

768
00:51:34,450 --> 00:51:36,240
i'm going to try and

769
00:51:36,260 --> 00:51:39,780
relate factor analysis two

770
00:51:43,340 --> 00:51:46,410
so this one now

771
00:51:46,410 --> 00:51:50,800
one model that is extremely similar to

772
00:51:50,840 --> 00:51:52,240
factor analysis

773
00:51:52,280 --> 00:51:55,370
is probabilistic PCA

774
00:51:56,990 --> 00:52:01,180
and he was

775
00:52:01,320 --> 00:52:03,470
proposed by

776
00:52:03,490 --> 00:52:05,700
mike tipping who used to work at

777
00:52:05,720 --> 00:52:07,450
microsoft research in cambridge

778
00:52:07,590 --> 00:52:10,970
and by my boss chris bishop

779
00:52:14,180 --> 00:52:16,280
the idea that the difference

780
00:52:19,320 --> 00:52:22,510
the difference here too

781
00:52:22,510 --> 00:52:24,990
factor analysis is that

782
00:52:25,010 --> 00:52:26,470
instead of having

783
00:52:26,470 --> 00:52:31,180
one noise term for each of the variables in in data space

784
00:52:31,180 --> 00:52:35,490
we now say that the noise has the same variance for all of them

785
00:52:35,590 --> 00:52:39,030
it's not a very big differences

786
00:52:39,050 --> 00:52:40,890
so that's essentially

787
00:52:40,910 --> 00:52:42,870
that's essentially the only variation

788
00:52:42,910 --> 00:52:47,950
and now the interesting thing is that if you do that little change

789
00:52:47,970 --> 00:52:50,240
now you can prove

790
00:52:50,240 --> 00:52:52,880
OK so let's get back to the the

791
00:52:52,890 --> 00:52:55,120
a little-known field of financial astrology

792
00:52:55,130 --> 00:52:59,370
and if you come common that you can find some software which

793
00:52:59,390 --> 00:53:01,330
provides tool

794
00:53:02,330 --> 00:53:07,120
predict things like share prices based on the positions of the planets that kind of

795
00:53:07,120 --> 00:53:09,990
stuff and if you look on the web page not going to say

796
00:53:10,010 --> 00:53:16,360
which packets is scared of libel laws and what not but google the powerful tool

797
00:53:16,400 --> 00:53:20,150
anyway if you look at the screenshot you find you know there's all this stuff

798
00:53:20,150 --> 00:53:23,510
to do with math models you can kind of like different planets the thing might

799
00:53:23,510 --> 00:53:28,800
have a bearing on europe which on this is ue looking at and there's various

800
00:53:28,800 --> 00:53:35,710
claims about the performance of this other software on historical data so looking at they

801
00:53:35,710 --> 00:53:36,760
claim we

802
00:53:36,760 --> 00:53:40,500
we looked at the last couple of years of some particular

803
00:53:40,510 --> 00:53:46,150
indices and we found we could predict them with an extremely high rates of accuracy

804
00:53:46,150 --> 00:53:51,080
the implication being by this model is going to continue predicting with the same accuracy

805
00:53:52,510 --> 00:53:56,020
you're investments can repair itself many times over

806
00:53:56,490 --> 00:53:58,100
OK so maybe you could

807
00:53:58,240 --> 00:54:02,550
so i started to feel the something not quite not quite right about this

808
00:54:02,770 --> 00:54:05,120
clearly put finger like what the

809
00:54:05,130 --> 00:54:08,380
what potential

810
00:54:08,410 --> 00:54:13,690
issues might here the before you break show your cash on this

811
00:54:13,710 --> 00:54:14,880
on the software

812
00:54:14,930 --> 00:54:17,490
any niggling doubt in your mind

813
00:54:26,740 --> 00:54:34,880
what does that mean that does not mention at all in the middle of automated

814
00:54:34,880 --> 00:54:39,270
stock trading and like it's a lot of financial trading companies to use automated trading

815
00:54:39,870 --> 00:54:47,050
of some solid so the answer is not knowing this

816
00:54:51,850 --> 00:54:56,630
sure sure might be the minimum monkey whatever you exactly right

817
00:54:58,530 --> 00:55:02,400
OK so that might be the objection that perhaps there no structure in the space

818
00:55:02,430 --> 00:55:05,890
whatsoever patches like the lottery and it's completely random

819
00:55:05,930 --> 00:55:10,670
you get the same thing for lottery scheme predict next week lottery numbers and there

820
00:55:10,680 --> 00:55:15,120
you know there's clearly an objection that this thing should surely that's independent you know

821
00:55:15,150 --> 00:55:19,680
like it doesn't involve the history or any other environmental factors

822
00:55:19,700 --> 00:55:21,780
so even if there was structure

823
00:55:21,920 --> 00:55:25,560
you know they tested this on previous data like three years data and it's got

824
00:55:25,560 --> 00:55:27,280
the right thing almost all the time

825
00:55:27,330 --> 00:55:32,050
how could i could be wrong that sounds ten great

826
00:55:32,080 --> 00:55:36,620
it's all fancy math of planets their brilliant in our time

827
00:55:36,620 --> 00:55:40,150
i would like to my credit card details

828
00:55:40,170 --> 00:55:43,580
all right well there's a couple problems here one is

829
00:55:44,640 --> 00:55:46,170
this data has all

830
00:55:46,180 --> 00:55:50,180
past so you know one thing we can do is memorize the price on each

831
00:55:52,860 --> 00:55:56,780
when someone asked to predict predict surprise

832
00:55:56,800 --> 00:55:57,650
we just say

833
00:55:57,680 --> 00:56:00,110
OK what they is that i look at my database

834
00:56:00,120 --> 00:56:03,740
i give you the price and that predicts a hundred percent

835
00:56:03,740 --> 00:56:06,090
and all attorneys memorize each day

836
00:56:06,110 --> 00:56:08,650
and it's not going to tell me anything about the future

837
00:56:08,710 --> 00:56:13,140
right that's kind of an extreme case where

838
00:56:13,150 --> 00:56:14,980
in extreme case where we get

839
00:56:15,080 --> 00:56:22,800
zero learning and a hundred percent prediction on past data

840
00:56:22,810 --> 00:56:27,250
because that's kind of one warning about testing on your training data

841
00:56:27,270 --> 00:56:31,990
let's just think about a few reasons to believe something is true here's here's some

842
00:56:31,990 --> 00:56:36,740
things i propose are reasons that should make you believe that the system some kind

843
00:56:36,740 --> 00:56:39,240
of learning machines doing the right thing

844
00:56:39,300 --> 00:56:42,560
OK one thing is successful predictions of the future

845
00:56:42,580 --> 00:56:46,080
predictions on unseen data if

846
00:56:46,120 --> 00:56:50,420
if this software which will remain nameless if we tried it out and we

847
00:56:50,430 --> 00:56:53,300
we gave it a go for the next six months

848
00:56:53,330 --> 00:56:55,250
and i got the right data

849
00:56:55,270 --> 00:56:56,400
all the time

850
00:56:56,400 --> 00:56:59,770
then i would start to believe that something was was going on i would be

851
00:56:59,770 --> 00:57:03,300
a hundred percent convinced it may just be that does

852
00:57:03,310 --> 00:57:08,290
those random noise and i just got lucky every time the some non-zero probability that

853
00:57:08,290 --> 00:57:11,870
it just it just kind of got lucky but the more i see

854
00:57:11,880 --> 00:57:13,280
the doing the right thing

855
00:57:13,300 --> 00:57:16,280
on data which could possibly have been available

856
00:57:16,290 --> 00:57:17,930
when the thing was being developed

857
00:57:17,950 --> 00:57:22,100
then i started to think or maybe this may be something

858
00:57:22,120 --> 00:57:28,680
OK another reason remember we talked about inductive bias right at the start and

859
00:57:28,690 --> 00:57:32,050
there were particular kinds of things that we were

860
00:57:33,380 --> 00:57:34,930
we felt were

861
00:57:34,940 --> 00:57:37,740
what properties of

862
00:57:37,750 --> 00:57:39,130
believable models

863
00:57:39,140 --> 00:57:41,820
and one of them was simplicity

864
00:57:41,840 --> 00:57:43,800
and if we can find

865
00:57:43,810 --> 00:57:48,850
some model or some learning system which takes data in the past and give a

866
00:57:48,850 --> 00:57:50,910
simple explanation of the data

867
00:57:50,920 --> 00:57:55,450
maybe there's very few parameters which accurately describe large quantity of data

868
00:57:55,470 --> 00:58:00,620
then that's another reason i might have to to believe to believe that model

869
00:58:00,630 --> 00:58:02,880
so if we had something like

870
00:58:02,890 --> 00:58:03,930
you know some

871
00:58:05,780 --> 00:58:10,260
particle physics experiments in some simple property like you called t squared

872
00:58:10,290 --> 00:58:13,550
to simple thing it fits the data very well

873
00:58:13,560 --> 00:58:16,220
and that's one point one which i might

874
00:58:16,670 --> 00:58:22,190
i might choose to find out more credible

875
00:58:22,200 --> 00:58:24,170
OK so whether you think simple models

876
00:58:24,180 --> 00:58:26,680
there are more

877
00:58:26,690 --> 00:58:29,560
there are more reliable than complicated models

878
00:58:30,300 --> 00:58:33,570
at some stage that's up for debate but we cannot handle the start going to

879
00:58:33,570 --> 00:58:37,550
make some assumptions which enable us to move forward that was one of them that

880
00:58:37,550 --> 00:58:43,490
we prefer simplicity over complexity given that and here's the consequence of that assumption

881
00:58:43,660 --> 00:58:47,340
OK there may be things specific to

882
00:58:47,360 --> 00:58:49,500
applications which

883
00:58:49,530 --> 00:58:52,990
which make it plausible if you've got some

884
00:58:53,680 --> 00:58:57,590
maybe a bit difficult to

885
00:58:57,610 --> 00:59:00,380
explain this with our example but

886
00:59:00,410 --> 00:59:04,070
if you have some model then you may be able to find some consequence of

887
00:59:04,120 --> 00:59:06,870
that model which you can which you can text in the data

888
00:59:06,890 --> 00:59:10,450
i'm kind of going and wait from the river i can give you

889
00:59:10,490 --> 00:59:12,720
examples if you like

890
00:59:12,760 --> 00:59:14,500
three and you should

891
00:59:14,530 --> 00:59:18,420
not believe the model is because it has high likelihood of the data you've already

892
00:59:18,420 --> 00:59:23,920
tested so when you're building model usually trying to optimize something

893
00:59:23,930 --> 00:59:26,230
and that thing you've optimized

894
00:59:26,240 --> 00:59:30,090
after you've carried out process you already know it's as good as its

895
00:59:30,110 --> 00:59:33,800
as it can be it's like you use that statistic and then you have to

896
00:59:33,800 --> 00:59:37,040
find something else to test it on to the need new data

897
00:59:37,120 --> 00:59:38,140
or you need

898
00:59:38,170 --> 00:59:40,220
some other way of evaluating

899
00:59:40,230 --> 00:59:41,430
the model

900
00:59:41,440 --> 00:59:44,850
other than what you've already used to train the model

901
00:59:44,890 --> 00:59:46,370
so the problem here is

902
00:59:46,380 --> 00:59:47,730
the models being

903
00:59:48,800 --> 00:59:50,940
with respect to accuracy on

904
00:59:50,940 --> 00:59:52,030
so i

905
00:59:52,080 --> 00:59:55,590
excellent scientific data side effects if you don't have

906
00:59:55,640 --> 00:59:58,510
you know anyone who takes medication in this day and age it doesn't look as

907
00:59:58,510 --> 00:59:59,620
side effects

908
00:59:59,670 --> 01:00:01,750
what taken is taking the chance

909
01:00:01,760 --> 01:00:04,510
so that's a really important scientific data

910
01:00:04,550 --> 01:00:08,800
that you know in the the most experimental fields as pharmaceutical companies

911
01:00:08,810 --> 01:00:10,120
operate under

912
01:00:10,160 --> 01:00:15,160
so similar to compute doesn't get it either quotation by mars humans to p one

913
01:00:15,160 --> 01:00:16,290
side to the other

914
01:00:16,310 --> 01:00:20,000
you end up debate all the time and not doing the research

915
01:00:20,010 --> 01:00:22,320
some new standard here

916
01:00:22,340 --> 01:00:27,020
except external reality but detected objective viewpoints we talked about talk about

917
01:00:27,110 --> 01:00:30,680
being abductive

918
01:00:30,690 --> 01:00:35,520
and values play an important role just like it does in qualitative research

919
01:00:35,540 --> 01:00:38,790
we try to combine form informal techniques

920
01:00:38,810 --> 01:00:40,820
and started writing

921
01:00:40,840 --> 01:00:42,990
we think that you could look for causation

922
01:00:43,000 --> 01:00:45,400
you can pin down exactly

923
01:00:45,410 --> 01:00:49,620
and we research is influenced by theory hypotheses and so forth but also by of

924
01:00:49,660 --> 01:00:52,620
observations facts evidence

925
01:00:52,640 --> 01:00:54,510
so how is supposed to get there

926
01:00:58,170 --> 01:01:01,710
how however some slides i'm this gonna get going hit the high points and tries

927
01:01:01,710 --> 01:01:03,460
which have underlined boat

928
01:01:03,490 --> 01:01:07,170
how is it you know very big name in the US

929
01:01:07,170 --> 01:01:08,630
published large

930
01:01:08,640 --> 01:01:12,800
he is on record as saying that mixed methods on this development which really concern

931
01:01:12,810 --> 01:01:14,040
me when said that

932
01:01:14,560 --> 01:01:17,490
so let's look at some of the names of a lot of these are from

933
01:01:17,490 --> 01:01:18,540
the US but

934
01:01:18,580 --> 01:01:22,260
you may have heard of them before like giving lincoln who played a big role

935
01:01:22,280 --> 01:01:27,160
in the qualitative movement issued landmark book in nineteen eighty five

936
01:01:27,190 --> 01:01:30,580
and mean just see over the years what some of the people

937
01:01:30,610 --> 01:01:33,060
some critics of mixed methods have said

938
01:01:33,160 --> 01:01:37,680
for example both quantum quarter-on-quarter methods may be used to populate a new research paradigm

939
01:01:37,700 --> 01:01:39,410
ninety four

940
01:01:39,500 --> 01:01:46,130
nineteen eighty nine they stated information may be quantitative or qualitative

941
01:01:46,150 --> 01:01:47,990
eighty five and in my book

942
01:01:48,020 --> 01:01:49,810
they said that

943
01:01:49,820 --> 01:01:54,540
there are many opportunities for naturalistic investigator utilize qualitative data

944
01:01:54,540 --> 01:01:56,910
probably more than appreciated

945
01:01:56,920 --> 01:02:00,030
and then

946
01:02:00,070 --> 01:02:01,520
they also

947
01:02:01,750 --> 01:02:03,720
paradise commercial

948
01:02:03,780 --> 01:02:07,990
and they said it was cautious yes that's in two thousand five

949
01:02:08,010 --> 01:02:09,840
that's the whole reason

950
01:02:09,900 --> 01:02:15,290
and this one should like questions such a distinction between quantitative and qualitative

951
01:02:18,120 --> 01:02:21,790
so she really get rid of that those terms and i argue that point later

952
01:02:22,890 --> 01:02:25,050
the slides

953
01:02:25,170 --> 01:02:29,270
there are many definitions that is are the can

954
01:02:30,290 --> 01:02:32,180
myself and lisa

955
01:02:32,190 --> 01:02:36,090
when we see it so it's effect is a powerful third paradigm

956
01:02:36,110 --> 01:02:38,790
the company quantitative and qualitative again

957
01:02:38,810 --> 01:02:43,020
we really respect quantitative research you know is that while we expect quite a research

958
01:02:43,020 --> 01:02:43,900
is that well

959
01:02:43,940 --> 01:02:49,170
so i'm not saying it replaces we say this complements this at supplements

960
01:02:50,430 --> 01:02:52,630
the philosophy pragmatism

961
01:02:52,640 --> 01:02:57,280
civil for the languages are less likely to win more medals

962
01:02:57,290 --> 01:02:59,580
it's appreciative increase local and

963
01:03:02,040 --> 01:03:06,770
again you have access to messages access to articles has this definition taking the continuous

964
01:03:06,770 --> 01:03:07,960
time afterwards

965
01:03:07,990 --> 01:03:13,030
office an important approach to generate an important question that's really important thing that we

966
01:03:13,030 --> 01:03:14,330
think mixed methods

967
01:03:14,340 --> 01:03:16,820
allows you to generate the questions

968
01:03:16,990 --> 01:03:19,920
the same is a better than b the that we could we could do better

969
01:03:19,920 --> 01:03:21,320
than that

970
01:03:21,380 --> 01:03:24,460
what's exciting forces at the loss of jobs when i first started off

971
01:03:24,510 --> 01:03:29,040
like that mixed methods about fifteen years ago it was so difficult to get anything

972
01:03:30,090 --> 01:03:32,140
i was known in the field

973
01:03:32,400 --> 01:03:36,420
as a about that's why you see some slides of their

974
01:03:36,440 --> 01:03:39,310
controversial people come up and say are you so controversial

975
01:03:39,320 --> 01:03:41,910
o station makes quantitative qualitative

976
01:03:41,950 --> 01:03:45,380
i would like to tell the story of when i was doing a presentation about

977
01:03:45,380 --> 01:03:47,290
nineteen years ago

978
01:03:47,300 --> 01:03:51,470
there is no impact on either this side and the woman sitting because the fact

979
01:03:51,550 --> 01:03:54,920
the person and i was talking just gets angry with me

980
01:03:54,930 --> 01:03:59,400
you can kind of stoking the pencil and next thing that is not enough

981
01:03:59,400 --> 01:04:02,660
and i started i thought that was my neck you know

982
01:04:02,730 --> 01:04:04,490
i will be here today itself

983
01:04:04,550 --> 01:04:06,960
it's really amazing and maybe

984
01:04:07,020 --> 01:04:09,590
it has been such debates have been so

985
01:04:11,130 --> 01:04:14,500
in europe in straight in other places africa about

986
01:04:14,680 --> 01:04:18,760
can certainly say the US has been really is that is that it's not as

987
01:04:18,760 --> 01:04:19,930
bad now

988
01:04:20,140 --> 01:04:24,940
so people get really angry when you mention one of the major mixed methods

989
01:04:24,960 --> 01:04:28,070
these are several journals that so you may have heard of

990
01:04:28,110 --> 01:04:32,330
and i'm sure there's some political science not familiar with political science journals so i'm

991
01:04:32,340 --> 01:04:33,210
sure there

992
01:04:33,270 --> 01:04:35,100
if few the public scrutiny

993
01:04:35,140 --> 01:04:39,320
mixed methods according to

994
01:04:39,330 --> 01:04:44,120
for those looking for a job with that publishes mixed methods research

995
01:04:44,130 --> 01:04:46,100
john mixed methods research

996
01:04:46,110 --> 01:04:50,280
came out in two thousand seven was tajik calling creswell with editors

997
01:04:50,390 --> 01:04:53,380
outgoing now the decision up a few manuscripts

998
01:04:53,430 --> 01:04:55,630
and you have to sit mountains

999
01:04:55,630 --> 01:04:57,520
and men's max bergmann

1000
01:05:00,420 --> 01:05:03,060
so she is on their

1001
01:05:03,070 --> 01:05:04,900
so as an outlet you have there

1002
01:05:04,950 --> 01:05:09,680
this action video that's come called multiple research approaches and that's

1003
01:05:10,100 --> 01:05:11,750
that's also very new

1004
01:05:11,800 --> 01:05:13,660
so that's not second

1005
01:05:13,710 --> 01:05:15,550
jill that routinely publishes

1006
01:05:15,550 --> 01:05:17,420
expected research

1007
01:05:17,450 --> 01:05:21,820
a lot of special issues have been published or planned publication like the ones that

1008
01:05:22,780 --> 01:05:26,920
and this is one that's come in that you study about international journal of

1009
01:05:26,960 --> 01:05:28,610
what research approaches

1010
01:05:28,640 --> 01:05:31,670
and nancy my colleagues and myself

1011
01:05:31,710 --> 01:05:34,250
yes it is thomas in percent

1012
01:05:35,410 --> 01:05:38,950
these are some of the more recent books and mixed method still not that many

1013
01:05:38,950 --> 01:05:41,320
books compared to quantitative qualitative

1014
01:05:41,370 --> 01:05:43,720
slowly growing and

1015
01:05:48,510 --> 01:05:51,070
chaco intently

1016
01:05:51,080 --> 01:05:53,910
and now working on the second-hand book you'll see some

1017
01:05:53,960 --> 01:05:56,460
so as we speak was to work in other chapters are you going to get

1018
01:05:56,460 --> 01:05:59,950
a preview of some of some of those thing or two from that chapter that

1019
01:05:59,960 --> 01:06:02,360
still ongoing that's such kind of this is that you

1020
01:06:02,420 --> 01:06:03,750
things are still

1021
01:06:04,860 --> 01:06:07,550
this presentation

1022
01:06:07,860 --> 01:06:11,630
this is the same that we use

1023
01:06:11,680 --> 01:06:16,030
two when i teach i i want to do research this for step process for

1024
01:06:16,030 --> 01:06:18,380
mixed methods research

1025
01:06:18,420 --> 01:06:23,960
i what looks like a legal over baby featured those stats

1026
01:06:23,970 --> 01:06:26,820
i would i will guarantee you follow

1027
01:06:26,820 --> 01:06:28,430
the steps

1028
01:06:28,490 --> 01:06:32,230
that there is a very good chance you're going to have a good study good

1029
01:06:32,230 --> 01:06:33,620
rigorous study

1030
01:06:33,640 --> 01:06:37,000
it's actually an article which go for those who take the class

1031
01:06:37,550 --> 01:06:40,600
that we follow the steps to the termites

1032
01:06:40,630 --> 01:06:43,910
you know its publisher american educational research journal and

1033
01:06:43,910 --> 01:06:48,810
the goal is to make sure you're know it's going to go in this direction

1034
01:06:48,810 --> 01:06:50,520
and intensity

1035
01:06:50,540 --> 01:06:53,230
should the magnitude of the control

1036
01:06:53,250 --> 01:06:59,100
so do the integration get particle trajectories

1037
01:06:59,140 --> 01:07:03,000
and that is either flow maps your

1038
01:07:03,000 --> 01:07:04,830
you've seen several times

1039
01:07:04,850 --> 01:07:08,850
and so the

1040
01:07:08,870 --> 01:07:15,330
our main duty behind this is pretty simple and this relates to the minimisation of

1041
01:07:15,330 --> 01:07:16,770
dynamical system

1042
01:07:17,460 --> 01:07:22,640
so we have the location of the particle x y t x y and then

1043
01:07:22,660 --> 01:07:26,980
the optical flow is you which sort of change the particles and we have to

1044
01:07:26,980 --> 01:07:29,270
find this a

1045
01:07:29,310 --> 01:07:32,040
fixed point of this system can

1046
01:07:34,350 --> 01:07:35,980
we know that is

1047
01:07:35,980 --> 01:07:41,560
this fixed points which is accessed online store and then

1048
01:07:41,560 --> 01:07:42,870
we can

1049
01:07:42,890 --> 01:07:48,690
do this you is given by this and these going about that then we do

1050
01:07:48,690 --> 01:07:53,660
that kind of substitution of new coordinates with x minus sixty because even

1051
01:07:54,520 --> 01:07:59,420
last week of the two and we do some manipulation and at the end of

1052
01:07:59,420 --> 01:08:01,660
the show that he

1053
01:08:01,660 --> 01:08:05,480
the optical flow fields even easy to

1054
01:08:05,500 --> 01:08:07,350
is expressed basically

1055
01:08:07,370 --> 01:08:08,980
the the

1056
01:08:09,000 --> 01:08:13,580
david you test x and i don't you suspect why

1057
01:08:13,600 --> 01:08:15,580
and similarly these experts but data

1058
01:08:15,580 --> 01:08:21,660
the petersburg x y which is essentially the jacobian matrix so this is the system

1059
01:08:21,670 --> 01:08:25,850
that is you really want to that the jacobian gives you the rate of change

1060
01:08:25,850 --> 01:08:28,330
if you really want to see a doctor

1061
01:08:29,120 --> 01:08:34,980
jacobian is going to play a part in analyzing his behavior which is you nice

1062
01:08:35,560 --> 01:08:39,930
so so what we can do is look at the eigen values of the jacobian

1063
01:08:39,930 --> 01:08:46,160
matrix this two by two matrix so has two i can use and then the

1064
01:08:46,160 --> 01:08:50,520
interesting thing is that if you look at the things

1065
01:08:50,540 --> 01:08:53,540
the case where the highway continues

1066
01:08:53,600 --> 01:08:58,290
both of them are negative and then that essentially to bottleneck

1067
01:08:59,000 --> 01:09:01,080
and then the

1068
01:09:03,660 --> 01:09:05,270
both positive and

1069
01:09:05,290 --> 01:09:08,750
then that gives you the departure from band

1070
01:09:11,410 --> 01:09:17,290
when have one of the eigen values you then that as it essentially the same

1071
01:09:17,500 --> 01:09:22,910
which is very nice elegant way to find behavior that set point

1072
01:09:23,000 --> 01:09:29,210
is one of the two and one is positive and it becomes blocking OK

1073
01:09:29,230 --> 01:09:32,960
and finally the centre

1074
01:09:32,980 --> 01:09:36,930
if these ideas are kind of complex then that gives you

1075
01:09:36,940 --> 01:09:38,390
or just

1076
01:09:38,410 --> 01:09:44,710
so it's pretty interesting very simple idea but you know it's very powerful and it's

1077
01:09:44,710 --> 01:09:45,520
going to work

1078
01:09:46,460 --> 01:09:52,000
our idea approach is that they can provide you compute optical flow

1079
01:09:52,040 --> 01:09:57,790
and they first thing is want to find out what the regions of interest

1080
01:09:58,540 --> 01:10:04,850
and for that reason that should get flow maps and you look at the accumulation

1081
01:10:04,850 --> 01:10:10,640
of these particles we can do something called denstream et cetera look can you know

1082
01:10:10,710 --> 01:10:17,330
that how many particles each of pixels are going through a new cluster those two

1083
01:10:17,350 --> 01:10:21,670
identify these regions of interest

1084
01:10:21,730 --> 01:10:27,560
and those players we want to look at these behaviors of present OK so these

1085
01:10:27,980 --> 01:10:32,870
regions of interest gives us the can points and then you to take go the

1086
01:10:32,870 --> 01:10:37,040
optical flow games and computers i can really met OK

1087
01:10:37,040 --> 01:10:40,330
and the jacobian lighting and so on

1088
01:10:40,350 --> 01:10:43,270
then using those we have to

1089
01:10:43,310 --> 01:10:46,910
decide which behavior is

1090
01:10:46,960 --> 01:10:52,910
so so given the sequence like this which is as you see it debauchery

1091
01:10:52,930 --> 01:10:58,850
optical flow again column order the direction of some kind of

1092
01:10:58,890 --> 01:11:01,310
intensity as the magnitude

1093
01:11:01,310 --> 01:11:05,480
and these are the particle trajectories and this is the density schema

1094
01:11:05,500 --> 01:11:12,370
so then semantics is that how many particles are going to excel

1095
01:11:12,390 --> 01:11:14,290
and sequence

1096
01:11:14,290 --> 01:11:15,690
optical flow

1097
01:11:15,690 --> 01:11:17,270
and decrease

1098
01:11:17,290 --> 01:11:19,460
and they don't seem ok

1099
01:11:19,480 --> 01:11:22,210
so we have to

1100
01:11:23,270 --> 01:11:31,390
look at this image new find regions so this is potentially to and for that

1101
01:11:31,870 --> 01:11:33,020
our article does it

1102
01:11:33,060 --> 01:11:34,330
the train through that

1103
01:11:34,440 --> 01:11:41,290
pixel and they look at the history of itself these trajectories

1104
01:11:41,330 --> 01:11:46,870
and that to provide this information dense in the second example here

1105
01:11:46,940 --> 01:11:49,310
and these are the the particle trajectories

1106
01:11:49,330 --> 01:11:52,080
and this is the story of these languages

1107
01:11:52,100 --> 01:11:55,770
and this is the second these systems histogram so

1108
01:11:55,790 --> 01:12:00,620
simple criterion is that is a high variance

1109
01:12:00,640 --> 01:12:03,330
these are the attendance for the

1110
01:12:03,350 --> 01:12:07,140
what if there is no evidence that i cannot spot

1111
01:12:07,160 --> 01:12:10,020
lindsay arches of things

1112
01:12:10,020 --> 01:12:17,580
so once we get these candidates then look at optical flow again and compute the

1113
01:12:17,850 --> 01:12:18,980
the jacobian

1114
01:12:19,140 --> 01:12:23,460
and look at the science of these ideas values and then you have for fatherly

1115
01:12:23,460 --> 01:12:27,230
the these four cases as i've shown you before

1116
01:12:27,270 --> 01:12:33,830
on the the naked to israel positive zero and one z

1117
01:12:33,850 --> 01:12:35,890
in one of them is use blue

1118
01:12:35,910 --> 01:12:41,140
and so on green is you know one positive and negative and then

1119
01:12:41,160 --> 01:12:45,580
for the complex conjugate can use cases

1120
01:12:45,580 --> 01:12:51,620
so far in this example we get the idea make like this

1121
01:12:51,640 --> 01:12:54,500
and these three ones are the

1122
01:12:54,520 --> 01:12:56,690
when both of them and get

1123
01:12:56,690 --> 01:13:02,990
quickly walked right into whether energy by just try to say what type thing what

1124
01:13:02,990 --> 01:13:08,010
about Bellizzi change joined I mean can you say I don't mean any norm was

1125
01:13:08,010 --> 01:13:15,090
that publicist exchange that at how would you actually say by the way Thomas Taggart

1126
01:13:15,090 --> 01:13:18,990
that was beach I don't agree with that I think it should it's not about

1127
01:13:19,000 --> 01:13:25,010
the each obviously not tag of X . arm and then then that biggest thing

1128
01:13:25,010 --> 01:13:28,350
at the ultimately because product was that is equivalent to what we do in history

1129
01:13:28,350 --> 01:13:33,030
of Europe in ontology matching her son till years when they stop the goes and

1130
01:13:33,030 --> 01:13:39,410
the other converts them that same problem syntactical tight spaces so still community flicker has

1131
01:13:39,410 --> 01:13:46,890
as text-based marauding delicious travel and they actually legitimately have slipped you know their folk

1132
01:13:46,890 --> 01:13:51,330
songs so the sets of words wrote to the community Club with but how do

1133
01:13:51,330 --> 01:13:55,690
you intact talk about the text-based themselves as entities that you can reason about them

1134
01:13:55,870 --> 01:14:01,370
otherwise you couldn't do these processes that translates so these are these questions are real

1135
01:14:01,370 --> 01:14:04,530
and their posed to you can actually say if the answer goes this way we

1136
01:14:04,690 --> 01:14:08,780
seen so the other was the so that's and is in that the mathematics of

1137
01:14:08,780 --> 01:14:12,150
the formalism did any work it was the humans were forced to be clear about

1138
01:14:12,190 --> 01:14:15,970
I think that's a real contribution at the semantic Web we forget that we bring

1139
01:14:15,970 --> 01:14:17,650
its methodology thing

1140
01:14:18,230 --> 01:14:22,450
and now the status of this thing I'm afraid to say I let the ball

1141
01:14:22,450 --> 01:14:27,710
drop on this is busier and anything armor and those those issues came out the

1142
01:14:27,710 --> 01:14:32,050
1st week it wasn't hard cover the issues on the re-signed and that if is

1143
01:14:32,050 --> 01:14:36,590
because it's real work to solve the issues is not reward technically headed done by

1144
01:14:36,600 --> 01:14:42,090
now is work socially politically so we we need to come up with the idea

1145
01:14:42,090 --> 01:14:45,590
now I have a thing contact comments as a clever word that that the domain

1146
01:14:45,600 --> 01:14:49,780
registered etc. were all ready to go on the political process with that because of

1147
01:14:49,780 --> 01:14:53,370
posting contributed I guess you don't have to be boring working on it but if

1148
01:14:53,370 --> 01:14:59,410
you guys wonna participate what I need 1st and foremost is folks to volunteer reference

1149
01:14:59,410 --> 01:15:04,670
applications that would do something with this because of the something worse than a budget

1150
01:15:05,120 --> 01:15:12,110
like myself run around same like says the go really why searching the diamond line

1151
01:15:12,110 --> 01:15:14,850
but you say Oh by the way if this happens then this is called which

1152
01:15:14,880 --> 01:15:20,310
machinery for translating we can come into play was machinery for information extraction or whatever

1153
01:15:20,310 --> 01:15:25,570
and not in a surgical that occasions you could look up to the stuff fantastic

1154
01:15:25,670 --> 01:15:30,490
and then of course if you actually have machines that create Tiger applications respect and

1155
01:15:30,490 --> 01:15:35,420
finally isn't enough knowledge engineers now but they have to learn how to write I

1156
01:15:35,420 --> 01:15:39,930
mean I know this is hard skills in running open-source project they were that's that

1157
01:15:39,940 --> 01:15:48,550
neural tube playoff Hasek example is about that pretty negative structure

1158
01:15:49,030 --> 01:15:53,760
now the thing is interesting here this is about my musical travels my example but

1159
01:15:53,920 --> 01:15:59,470
apply any place and any knowledge of collective knowledge system to have this property that

1160
01:15:59,470 --> 01:16:04,750
it gives better as more people use it tested do a couple of things 1st

1161
01:16:04,750 --> 01:16:09,750
of has to motivate people to come out and do something with it contributed that's

1162
01:16:09,750 --> 01:16:14,910
really the harder part Bessler learned from enterprise suffer work interest that we had a

1163
01:16:14,910 --> 01:16:19,590
collective we added to the collective mind for enterprises but almost 4 focus in the

1164
01:16:19,590 --> 01:16:23,410
source of the problem which is missing the point that the real problem was collecting

1165
01:16:23,410 --> 01:16:28,610
people's actual work products online so could searched and that's the problem solving and from

1166
01:16:28,610 --> 01:16:34,510
that knowledge and collective memory emerged the same thing is here that the technology can

1167
01:16:34,510 --> 01:16:38,830
actually helped motivate people to come will 1 of the waste and the 2nd here

1168
01:16:38,840 --> 01:16:42,510
to the cost to make it so that the more people coming contributed the more

1169
01:16:42,510 --> 01:16:48,350
value to everyone who's using it into the network that these things growing stable it

1170
01:16:48,350 --> 01:16:53,530
turns out that we have some tools are toolbox for using the structure part of

1171
01:16:53,530 --> 01:16:58,750
this mix of structure and function that secondhand have those properties they can solve problems

1172
01:16:59,030 --> 01:17:05,330
so I think it's as with blue words set so the the problem is this

1173
01:17:05,480 --> 01:17:11,810
value traded values grow sort shot and solution or the approach of solution really is

1174
01:17:11,810 --> 01:17:12,800
welcome to lecture five

1175
01:17:13,530 --> 01:17:17,940
in the last lecture we defined symbol codes we ask theoretical

1176
01:17:18,470 --> 01:17:20,550
and practical questions about symbol codes

1177
01:17:23,200 --> 01:17:28,200
that's gave us even more evidence for these claims which we've now proved sufficiently i'm

1178
01:17:28,200 --> 01:17:32,260
going take them as true so the claim was the right way to measure information

1179
01:17:32,260 --> 01:17:34,210
content is with long

1180
01:17:34,680 --> 01:17:37,110
base to of one over pe of an outcome

1181
01:17:38,360 --> 01:17:42,800
that's not only sensible measure of information content it is how many bits you should

1182
01:17:42,800 --> 01:17:46,690
expect to be able to encode the outcome into if you're using a good compression

1183
01:17:47,750 --> 01:17:50,050
the entropy is the average information content

1184
01:17:51,000 --> 01:17:52,960
hand the source coding theorem says

1185
01:17:53,530 --> 01:17:56,800
if you've got a source with entropy age you can compress

1186
01:17:57,560 --> 01:17:59,420
and comes in an age bits

1187
01:18:00,670 --> 01:18:01,970
we looked a symbol codes where

1188
01:18:02,780 --> 01:18:08,710
without punctuation you give binary strings strings of zeros and ones to each letter in your alphabet

1189
01:18:09,410 --> 01:18:11,110
hand we established

1190
01:18:11,610 --> 01:18:14,880
some theoretical results about the some of them without proof

1191
01:18:15,690 --> 01:18:16,070
so we

1192
01:18:16,450 --> 01:18:17,680
came to an understanding of how

1193
01:18:18,170 --> 01:18:24,030
unique decodability constrains the length you comical length really short you have to make some

1194
01:18:24,030 --> 01:18:26,830
length shorter you have to make up the length of your code words

1195
01:18:30,540 --> 01:18:36,610
introduce notion a prefix code which are easy to decode codes that are associated with binary trees

1196
01:18:37,780 --> 01:18:39,080
and we noticed that's

1197
01:18:39,850 --> 01:18:43,320
if you've had code length equal to the information content then you got the best

1198
01:18:43,320 --> 01:18:46,750
possible compression and the average length per symbol was the entropy

1199
01:18:47,830 --> 01:18:55,000
hand we stated without proof that would be optimal symbol code you can get within one bit of the entropy

1200
01:18:55,980 --> 01:18:56,900
and finally

1201
01:18:58,400 --> 01:19:03,810
so the huffman algorithm which is an algorithm is extremely simple for making optimal

1202
01:19:04,250 --> 01:19:05,120
symbol codes

1203
01:19:05,620 --> 01:19:08,860
so let's just recap at with these examples and

1204
01:19:10,050 --> 01:19:11,510
so probabilities are

1205
01:19:13,020 --> 01:19:13,430
if if they

1206
01:19:18,230 --> 01:19:23,130
huffman algorithm says combine the two smaller with smallest probability you can take your pick

1207
01:19:23,970 --> 01:19:24,830
give to fifty

1208
01:19:25,810 --> 01:19:27,640
combine that with small probability

1209
01:19:29,380 --> 01:19:30,620
and then combine the two is

1210
01:19:31,280 --> 01:19:32,940
probability again take your pick

1211
01:19:36,280 --> 01:19:38,010
and then combine these got one

1212
01:19:38,500 --> 01:19:41,310
and then by putting it zeros and ones on edges

1213
01:19:43,290 --> 01:19:46,970
we can we can define codes this one is zero zero this

1214
01:19:47,730 --> 01:19:49,030
one coming down the tree

1215
01:19:49,670 --> 01:19:51,170
this is one there's error

1216
01:19:52,150 --> 01:19:53,140
this one zero one

1217
01:19:54,200 --> 01:19:54,840
and this one is

1218
01:19:55,610 --> 01:19:56,050
one one

1219
01:19:56,680 --> 01:19:59,130
one what i've done wrong such as they one

1220
01:20:04,810 --> 01:20:05,400
any questions

1221
01:20:07,260 --> 01:20:11,950
so let's do another example the probability is proportional to one two three four

1222
01:20:19,100 --> 01:20:19,710
and ten

1223
01:20:22,500 --> 01:20:23,520
half one says

1224
01:20:24,450 --> 01:20:25,180
combine these

1225
01:20:26,990 --> 01:20:30,130
these statistics and combine these nine

1226
01:20:30,700 --> 01:20:31,480
and take to

1227
01:20:34,160 --> 01:20:35,030
get twelve

1228
01:20:36,290 --> 01:20:37,850
then seventy eight

1229
01:20:41,170 --> 01:20:41,810
and then what

1230
01:20:42,360 --> 01:20:44,190
carefully we take two nines

1231
01:20:47,830 --> 01:20:48,500
get eighteen

1232
01:20:50,710 --> 01:20:53,370
then who does ten go with ten goals with twelve

1233
01:20:57,910 --> 01:20:58,220
twenty two

1234
01:21:00,450 --> 01:21:02,510
and then the two smallest articulating

1235
01:21:04,250 --> 01:21:04,980
looking at the history

1236
01:21:05,860 --> 01:21:06,200
and the

1237
01:21:07,600 --> 01:21:08,820
his this fifty five

1238
01:21:11,090 --> 01:21:15,570
now we slap on zeros and ones anyway you want and we've got ourselves the optimal symbol code

1239
01:21:16,390 --> 01:21:17,110
the ensemble why

1240
01:21:18,290 --> 01:21:18,950
any questions

1241
01:21:22,460 --> 01:21:26,910
yes the question was does the huffman algorithm ensures that we've got a prefix code

1242
01:21:27,160 --> 01:21:31,040
and the answer is yes because when we put the labels on when making a

1243
01:21:31,040 --> 01:21:31,680
binary tree

1244
01:21:34,040 --> 01:21:37,570
well defined in the code words from the root of the tree at words what

1245
01:21:37,570 --> 01:21:41,790
we'll get is a prefix code there is no way that one codeword can be

1246
01:21:41,790 --> 01:21:45,070
a prefix or another if defined in terms of a binary tree

1247
01:21:45,540 --> 01:21:48,660
so the have algorithm always gives you a prefix code

1248
01:21:49,840 --> 01:21:50,880
hand it's awful

1249
01:21:51,750 --> 01:21:53,720
you can't make a better symbol code

1250
01:21:54,360 --> 01:21:57,020
and you might think that that's then wraps up

1251
01:22:00,230 --> 01:22:00,750
one thing

1252
01:22:01,250 --> 01:22:03,870
you might ask in addition about prefix codes is

1253
01:22:04,860 --> 01:22:09,340
it is it sufficient to make a prefix code if someone said i've got a

1254
01:22:09,340 --> 01:22:13,090
really coming idea i'm gonna make a code that is not a prefix code and

1255
01:22:13,090 --> 01:22:15,120
it's going to be better can we prove

1256
01:22:15,590 --> 01:22:17,380
that that's is untrue

1257
01:22:18,770 --> 01:22:19,840
and the answer is

1258
01:22:20,360 --> 01:22:21,590
yes we can prove it's untrue

1259
01:22:22,090 --> 01:22:26,730
by the following fairly simple argument imagine that someone has made a symbol code

1260
01:22:27,290 --> 01:22:31,070
which they claim is wonderful and it's not a prefix code you say okay please

1261
01:22:31,070 --> 01:22:33,600
tell me the lengths of all of your symbol codes

1262
01:22:34,230 --> 01:22:35,750
starting with the shortest lengths

1263
01:22:36,460 --> 01:22:39,200
up to the longest length give me them sorted by length

1264
01:22:40,260 --> 01:22:44,090
then i'm going to go to the symbol code supermarket which we introduced last time

1265
01:22:44,280 --> 01:22:47,620
and i will walk across it from one side to the other and i will

1266
01:22:47,620 --> 01:22:52,390
buy the shortest code words first and then having bought some short ones move across

1267
01:22:52,390 --> 01:22:53,260
to the next while

1268
01:22:53,670 --> 01:22:57,710
by some the next length and so forth in accordance with the shopping list that you've given me

1269
01:22:58,420 --> 01:23:02,570
happened as long as you have given me a shopping list that doesn't exceed the budget

1270
01:23:03,310 --> 01:23:04,900
the crack budget and one

1271
01:23:05,480 --> 01:23:07,830
then i will be able to buy your lengths of

1272
01:23:11,070 --> 01:23:12,900
and because i stepped across

1273
01:23:13,310 --> 01:23:15,750
from one mile to the next assignment across should

1274
01:23:16,140 --> 01:23:17,860
make clear what i mean so let's say i had to buy

1275
01:23:19,170 --> 01:23:19,650
link to

1276
01:23:20,180 --> 01:23:22,030
and then you wanted one of length three

1277
01:23:22,550 --> 01:23:23,200
like this one

1278
01:23:23,600 --> 01:23:26,770
and then sum the rest length four by all these

1279
01:23:28,700 --> 01:23:29,300
as an example

1280
01:23:29,890 --> 01:23:35,070
and because i'm moving across and never overlapping with myself i'll end up with a prefix code

1281
01:23:35,830 --> 01:23:39,630
the only way you would violate the prefix property if you but to the from the same

1282
01:23:40,160 --> 01:23:41,100
the vertical coordinate

1283
01:23:41,560 --> 01:23:42,260
in the supermarket

1284
01:23:43,350 --> 01:23:43,640
all right

1285
01:23:46,060 --> 01:23:50,710
they huffman algorithm is already optimal makes prefix codes so we didn't need to worry

1286
01:23:50,710 --> 01:23:54,280
about this but it just in case someone is claiming i want to do something

1287
01:23:54,280 --> 01:24:00,370
clever not using prefix codes well whatever expected length they could've got with non prefix

1288
01:24:00,370 --> 01:24:03,850
code we can match it we can just replace the code by a prefix code

1289
01:24:04,700 --> 01:24:06,310
and the code i showed you before

1290
01:24:08,360 --> 01:24:09,240
was indeed

1291
01:24:09,790 --> 01:24:13,480
they could maybe the huffman algorithm and did say prefix code

1292
01:24:17,280 --> 01:24:18,520
so how we wrapped up

1293
01:24:19,490 --> 01:24:24,620
compression by saying his had made simple codes and here's how to make optimal symbol codes

1294
01:24:25,090 --> 01:24:29,180
well for the rest of today i want to argue that we are not done with compression

1295
01:24:29,670 --> 01:24:33,490
and the reason we're not done is because this lurking plus one

1296
01:24:34,320 --> 01:24:35,470
in these inequality

1297
01:24:39,200 --> 01:24:39,830
are we done

1298
01:24:47,160 --> 01:24:49,170
let me give you one really simple example

1299
01:24:50,590 --> 01:24:51,590
justify this

1300
01:24:53,230 --> 01:24:55,660
and it's example we started with the bent coin

1301
01:24:57,310 --> 01:25:03,110
let's imagine we have been calling and two faces are called an be let's say have probabilities point nine nine

1302
01:25:03,530 --> 01:25:04,430
o point zero one

1303
01:25:05,390 --> 01:25:07,300
and now they i will compress it using

1304
01:25:07,760 --> 01:25:11,500
i have an algorithm because they make optimal symbol codes and you carefully find the

1305
01:25:11,500 --> 01:25:14,850
two symbols the smallest probability you combine them into one

1306
01:25:14,850 --> 01:25:21,470
stop words the force would be the birds which have from this non-existing you don't

1307
01:25:21,470 --> 01:25:27,010
carry information and we usually just remove them that they have just dysfunctional also listed

1308
01:25:27,010 --> 01:25:32,280
for english this would be a about above across and so on and usually we

1309
01:25:32,280 --> 01:25:35,630
just remove these words and algorithms

1310
01:25:35,650 --> 01:25:36,810
work better

1311
01:25:36,820 --> 01:25:43,510
after that this and similar lists and if similar but different lists we have for

1312
01:25:43,520 --> 01:25:46,630
other languages

1313
01:25:46,660 --> 01:25:50,020
what's also interesting to mention which

1314
01:25:50,020 --> 01:25:55,430
usually people don't talk about this but this can be quite

1315
01:25:55,460 --> 01:25:57,610
annoying so

1316
01:25:57,620 --> 01:25:58,900
the characters

1317
01:25:58,910 --> 01:26:03,250
i'm not necessarily even the characters are not necessarily written in the same way that

1318
01:26:03,270 --> 01:26:07,840
especially if we use unicode let's this

1319
01:26:07,850 --> 01:26:13,840
a circle above it so it has two representations in unicode and if we want

1320
01:26:15,490 --> 01:26:19,380
the normalisation of words we need to be aware of this and so this means

1321
01:26:19,380 --> 01:26:23,790
that usually be we need to use some some

1322
01:26:23,830 --> 01:26:29,570
proper effects of the packages for this kind of normalisation

1323
01:26:29,580 --> 01:26:33,100
i think which we need to be aware of

1324
01:26:33,110 --> 01:26:38,430
when dealing with words is stemming so

1325
01:26:38,450 --> 01:26:40,920
stemming is

1326
01:26:40,930 --> 01:26:43,610
is a procedure which

1327
01:26:44,040 --> 01:26:50,550
four different forms of the same words into its smallest normally

1328
01:26:50,610 --> 01:26:55,950
normalized form so we can have stemming which usually just takes the root of the

1329
01:26:55,950 --> 01:26:59,880
word or lemmatisation which really tags that generates the

1330
01:26:59,940 --> 01:27:04,400
normalized proper normalized word

1331
01:27:04,420 --> 01:27:07,650
just store scheme is it as i can just

1332
01:27:07,660 --> 01:27:15,860
fashion show you the most often used stammer which is also public domain from this

1333
01:27:15,860 --> 01:27:18,970
website so this is the set of rules for english

1334
01:27:20,870 --> 01:27:25,850
these are the rules which we apply one after another

1335
01:27:25,850 --> 01:27:30,590
we tried to transformation rules on the suffix of the word this rule would apply

1336
01:27:30,590 --> 01:27:33,400
in such cases of relational would be

1337
01:27:33,450 --> 01:27:35,950
would get transformed into relates

1338
01:27:35,960 --> 01:27:42,490
could the second one conditional condition and so on and by applying this rule then

1339
01:27:42,500 --> 01:27:48,120
most likely not often but not always but most often some of the get normalized

1340
01:27:48,120 --> 01:27:57,930
but but that's occasionally get also areas like universe university i think they would get

1341
01:27:57,940 --> 01:28:03,600
normalized into the same into the same which is not correct

1342
01:28:03,920 --> 01:28:09,180
but this porterstemmer some others as well as the fact standard for english-language other other

1343
01:28:09,180 --> 01:28:10,360
languages have

1344
01:28:10,380 --> 01:28:15,320
there are sets of rules occasionally you'll have even the rules which are learned by

1345
01:28:15,320 --> 01:28:19,840
machine learning techniques especially for smaller languages and this

1346
01:28:19,850 --> 01:28:23,280
this can be quite quite efficient action

1347
01:28:23,300 --> 01:28:26,820
OK the next level

1348
01:28:26,840 --> 01:28:31,500
which is sort of similar to words but still different are phrases

1349
01:28:36,320 --> 01:28:41,970
so instead of just having single words we can have sequences of words we have

1350
01:28:41,970 --> 01:28:49,630
two types of phrases so one would be this frequent contiguous word sequences are

1351
01:28:49,650 --> 01:28:54,330
possibilities also to have frequent non contiguous perfect for sequences which

1352
01:28:54,330 --> 01:28:58,300
the location because of proximity

1353
01:28:58,310 --> 01:29:05,560
features both type of features can be identified with a relatively simple dynamic programming algorithm

1354
01:29:05,570 --> 01:29:08,910
so this is not a big science and

1355
01:29:08,930 --> 01:29:15,160
but it so that often this is not being done on this preprocessing clever when

1356
01:29:15,160 --> 01:29:16,400
dealing with text

1357
01:29:19,110 --> 01:29:20,810
do we need this just by

1358
01:29:20,820 --> 01:29:26,000
having dealing with phrases means that we can more precise precisely identify sense and you

1359
01:29:26,000 --> 01:29:30,930
can get rid of we can get rid of

1360
01:29:30,960 --> 01:29:34,520
some of these annoying properties which i mentioned before

1361
01:29:34,540 --> 01:29:38,370
like synonymy and so on

1362
01:29:38,380 --> 01:29:43,210
what we're saying here is that google released last year

1363
01:29:43,700 --> 01:29:49,070
the biggest ingram corpus so we can get it on this

1364
01:29:49,280 --> 01:29:54,250
link i think which

1365
01:29:55,150 --> 01:30:03,110
if you order this get twenty four gigawatts of compressed text files basically

1366
01:30:03,120 --> 01:30:09,530
in the dataset because thirteen million unigram so single words

1367
01:30:09,570 --> 01:30:16,210
thirty three hundred fourteen millions of bi grams of double double words it goes up

1368
01:30:16,210 --> 01:30:22,110
to five grams which is over one billion so if you have an effect

1369
01:30:22,390 --> 01:30:26,630
DVD then you would get such

1370
01:30:27,380 --> 01:30:35,640
ingrams and so as the frequency of ingram so i guess on will corpus

1371
01:30:36,800 --> 01:30:42,020
so this is i think quite quite available corpus place

1372
01:30:42,040 --> 01:30:47,180
the next level of textual representation would be part of speech

1373
01:30:50,200 --> 01:30:51,270
this is already

1374
01:30:51,270 --> 01:30:52,890
step away from

1375
01:30:53,510 --> 01:31:01,690
two words are small category of words are now

1376
01:31:02,180 --> 01:31:08,850
by introducing this part of speech tags somehow we

1377
01:31:08,870 --> 01:31:09,800
try to

1378
01:31:09,800 --> 01:31:13,700
and they have different work functions why do do we need this so

1379
01:31:14,120 --> 01:31:19,280
most often in text mining can be would use these for information extraction techniques which

1380
01:31:19,280 --> 01:31:25,670
we will see a little bit afterwards in one other possible uses just to perform

1381
01:31:25,690 --> 01:31:30,010
sort feature selection so that we reduce the vocabulary of all words which we do

1382
01:31:30,010 --> 01:31:32,000
it so

1383
01:31:32,020 --> 01:31:32,950
that's a

1384
01:31:32,970 --> 01:31:37,990
it's quite well known that the most information is carried by nouns and so we

1385
01:31:37,990 --> 01:31:40,950
can easily extract just is by

1386
01:31:40,960 --> 01:31:47,120
preprocessing of the text with its part of speech tagger part of speech tagging he's

1387
01:31:47,120 --> 01:31:52,610
usually are taggers are learned by hidden markov model algorithms

1388
01:31:52,610 --> 01:31:56,940
one of the first cover corpus which is manually annotated and then if it's hidden

1389
01:31:56,940 --> 01:32:00,500
markov model we learned model and then we apply this

1390
01:32:00,850 --> 01:32:02,440
also to unseen

1391
01:32:02,450 --> 01:32:04,120
previously unseen texts

1392
01:32:05,880 --> 01:32:12,440
these are the most typical categories when we try to annotate documents all of

1393
01:32:13,440 --> 01:32:17,310
these are categories which we used to annotate words

1394
01:32:17,310 --> 01:32:26,350
works words of verbs nouns adjectives adverbs pronouns prepositions conjunctions and interjections and when you

1395
01:32:26,350 --> 01:32:31,960
probably know what was the function of each of the word so works would be

1396
01:32:32,010 --> 01:32:33,360
like action or state

1397
01:32:33,770 --> 01:32:38,610
now this would be things and so on

1398
01:32:38,620 --> 01:32:42,760
so these are categories and now if you look at one example

1399
01:32:42,760 --> 01:32:48,300
the feature elements would be identifiability within spilling out features will actually reveal the relevance

1400
01:32:48,300 --> 01:32:50,260
of the future

1401
01:32:50,410 --> 01:32:52,380
there the problem with this method

1402
01:32:53,020 --> 01:32:57,760
the reliability to samples information gain is dependent upon the node composition

1403
01:32:57,810 --> 01:33:01,740
obviously got smaller nodes and unless stated splitting then

1404
01:33:01,780 --> 01:33:03,980
you can actually split it purely by chance

1405
01:33:04,120 --> 01:33:09,310
then larger and more data actually the more reliable measures information gain

1406
01:33:09,320 --> 01:33:13,180
and also irrelevant features will still non-zero measure relevance

1407
01:33:13,190 --> 01:33:16,300
because in the worst case for example the information gain

1408
01:33:16,370 --> 01:33:19,710
museo so the average information gain is always going to be the mean

1409
01:33:19,720 --> 01:33:21,450
the non negative sample

1410
01:33:23,040 --> 01:33:25,400
we still don't have some some non-zero

1411
01:33:25,490 --> 01:33:30,470
many of these relevance for relevant

1412
01:33:30,480 --> 01:33:33,490
OK now complexity compensation

1413
01:33:33,500 --> 01:33:35,460
chris anderson nodes reason to split

1414
01:33:35,470 --> 01:33:39,980
we're taking average information in situ by feature what we want to do is is

1415
01:33:40,970 --> 01:33:43,600
where each sample so we have a weighted average

1416
01:33:43,650 --> 01:33:45,870
and weighted by some

1417
01:33:45,910 --> 01:33:47,930
measure of the reliability of these nodes

1418
01:33:47,940 --> 01:33:49,700
all the complexity

1419
01:33:49,750 --> 01:33:51,340
so what we do is

1420
01:33:51,350 --> 01:33:54,220
we project the states down to the one dimensional space

1421
01:33:54,280 --> 01:33:55,880
is given by the feature

1422
01:33:56,740 --> 01:33:59,430
we're using a binary classification problem here with

1423
01:33:59,440 --> 01:34:02,540
and examples and i positive examples

1424
01:34:02,790 --> 01:34:05,430
we have in mind side negative examples

1425
01:34:05,450 --> 01:34:09,110
and the number of possible arrangements when you project mistakes down to the one dimensional

1426
01:34:09,110 --> 01:34:13,990
space is given by the combinatorial functions

1427
01:34:14,080 --> 01:34:16,990
the next thing to consider is that some of these arrangements when projected onto the

1428
01:34:16,990 --> 01:34:19,670
space is actually can be non unique

1429
01:34:19,720 --> 01:34:22,320
so such is here where we have

1430
01:34:22,400 --> 01:34:25,390
the notice two examples one each class

1431
01:34:25,440 --> 01:34:27,130
weak acid is reflected

1432
01:34:27,180 --> 01:34:30,310
we have two possible arrangements but basically the same

1433
01:34:30,320 --> 01:34:34,630
and if you to optimize information to optimize that maximize information gain

1434
01:34:34,680 --> 01:34:37,620
you actually achieve the same value for both

1435
01:34:37,630 --> 01:34:43,080
but also those of other arrangements is symmetric about the center returns unique

1436
01:34:43,100 --> 01:34:44,780
and these were given

1437
01:34:44,790 --> 01:34:47,690
during unique value the optimized information gain

1438
01:34:49,050 --> 01:34:51,190
they yourself consider

1439
01:34:51,240 --> 01:34:54,870
although these are the same arrangement the probability of the current is doubled because there

1440
01:34:54,870 --> 01:34:56,340
are two ways

1441
01:34:56,350 --> 01:35:00,860
this arrangement aka

1442
01:35:01,200 --> 01:35:04,390
the way we work out

1443
01:35:04,400 --> 01:35:07,250
the number of unique arrangements are given by these functions here

1444
01:35:07,260 --> 01:35:11,640
which is dependent upon whether and there i odd or even

1445
01:35:11,690 --> 01:35:15,920
you have an even number examples of number of positive examples of conformal

1446
01:35:15,970 --> 01:35:18,550
an arrangement which is symmetric about its center

1447
01:35:18,570 --> 01:35:21,910
so therefore there are no unique arrangements in that case

1448
01:35:21,960 --> 01:35:25,710
and then using information theory what we can do is say the probability

1449
01:35:25,730 --> 01:35:31,080
of a unique arrangement is given by one over the total number of possible arrangements

1450
01:35:31,090 --> 01:35:35,080
and probability the non unique arrangements is two over the total number of

1451
01:35:35,090 --> 01:35:36,170
possible arrangements

1452
01:35:36,180 --> 01:35:37,250
and i would say the

1453
01:35:37,260 --> 01:35:42,100
negative base two logarithm that give us information then we wait them by the proportions

1454
01:35:42,150 --> 01:35:44,190
you need in nineteen eight

1455
01:35:46,250 --> 01:35:49,790
and then because supplies down the kernel function here which is a measure of of

1456
01:35:54,640 --> 01:35:56,610
OK i want to

1457
01:35:56,660 --> 01:36:00,360
test to see how well this measure actually works

1458
01:36:00,410 --> 01:36:04,700
and what we want to do is look at the information gain density functions for

1459
01:36:04,700 --> 01:36:05,950
each feature

1460
01:36:06,000 --> 01:36:07,690
so if you imagine you

1461
01:36:07,730 --> 01:36:12,180
sampling information for each feature in different areas and different nodes

1462
01:36:12,200 --> 01:36:15,700
and if should give us some sort of distribution

1463
01:36:15,750 --> 01:36:18,140
these information games for each of these features

1464
01:36:18,150 --> 01:36:19,690
so we did built

1465
01:36:19,740 --> 01:36:22,110
the random forest using five trees

1466
01:36:22,120 --> 01:36:24,110
on an artificial dataset

1467
01:36:24,160 --> 01:36:25,670
and we recorded

1468
01:36:25,680 --> 01:36:28,750
e to the information given by features

1469
01:36:28,800 --> 01:36:30,040
and then

1470
01:36:30,070 --> 01:36:31,580
use that to construct

1471
01:36:31,590 --> 01:36:34,750
a density functions

1472
01:36:34,790 --> 01:36:38,990
we use the one way using unit weights for example

1473
01:36:39,010 --> 01:36:44,470
and another way we actually weighted by some measure of complexity

1474
01:36:44,470 --> 01:36:47,500
there is a pairwise comparison and then you choose the one which you think is

1475
01:36:49,110 --> 01:36:51,570
and then there was

1476
01:36:51,580 --> 01:36:54,430
recently before propose modifications

1477
01:36:54,430 --> 01:36:58,490
or combination of those this approach with cross validation

1478
01:36:59,740 --> 01:37:04,260
which is based on this agreement on test examples

1479
01:37:04,270 --> 01:37:07,260
of the model of the two approaches and their

1480
01:37:07,290 --> 01:37:11,770
the the the the higher disagreement means lower trust to cross validation

1481
01:37:11,820 --> 01:37:14,000
and the combination is the way in the way

1482
01:37:14,010 --> 01:37:15,550
of waiting weighting the

1483
01:37:15,560 --> 01:37:22,850
waiting in some way combining in some way one or choosing one or the other

1484
01:37:22,930 --> 01:37:26,370
OK so that was about represent similar

1485
01:37:26,460 --> 01:37:28,750
the third group embedded

1486
01:37:28,930 --> 01:37:34,430
it's very simple there is no preprocessing the whole feature selection happens in the loop

1487
01:37:34,480 --> 01:37:37,020
in the machine learning algorithm

1488
01:37:37,040 --> 01:37:38,220
and then

1489
01:37:38,220 --> 01:37:40,690
you use the algorithm is it is so there is

1490
01:37:40,700 --> 01:37:44,030
no additional work to select the features

1491
01:37:45,320 --> 01:37:48,550
and one simple example of that is all decision tree

1492
01:37:48,560 --> 01:37:54,110
there is some inherited feature selection that happens in decision tree induction

1493
01:37:54,160 --> 01:37:56,780
and it's a so-called embedded approach

1494
01:37:57,640 --> 01:37:59,100
and of course some other

1495
01:37:59,160 --> 01:38:03,060
approaches to feature selection these which is this in that idea

1496
01:38:03,110 --> 01:38:05,250
one is

1497
01:38:05,250 --> 01:38:08,830
at each iteration of the incremental optimisation

1498
01:38:08,850 --> 01:38:11,670
of the more the use of fast gradient

1499
01:38:11,670 --> 01:38:14,500
basically stick to find the most promising future

1500
01:38:14,520 --> 01:38:18,780
so while we induce the model we already do feature selection

1501
01:38:18,790 --> 01:38:21,810
and the other one the idea that

1502
01:38:21,860 --> 01:38:23,940
features that are relevant

1503
01:38:24,150 --> 01:38:25,770
four ford

1504
01:38:25,820 --> 01:38:32,080
concept we are learning should affect the generalisation error bound of the nonlinear SVM more

1505
01:38:32,080 --> 01:38:33,720
than irrelevant features

1506
01:38:33,820 --> 01:38:35,760
and the idea is in use

1507
01:38:35,840 --> 01:38:39,720
when evaluating the feature subset and

1508
01:38:39,750 --> 01:38:42,700
here they use backward elimination

1509
01:38:47,540 --> 01:38:49,410
there was also this

1510
01:38:49,440 --> 01:38:51,920
the idea that i already mentioned using

1511
01:38:51,990 --> 01:38:54,780
features that occur in decision trees

1512
01:38:54,800 --> 01:38:57,260
as the selected feature subset

1513
01:38:57,300 --> 01:39:01,760
is it is a kind of using embedded in approach

1514
01:39:01,760 --> 01:39:05,990
four instead of optimisation in the filtering approach

1515
01:39:07,480 --> 01:39:09,690
this proposal was

1516
01:39:09,710 --> 01:39:13,320
around decision tree induction on the data

1517
01:39:13,350 --> 01:39:16,090
take a look in the induced decision tree

1518
01:39:16,110 --> 01:39:20,820
take the features that occur in the decision tree and that is your feature subset

1519
01:39:20,830 --> 01:39:26,000
and and then run whatever algorithm you want on the feature subset

1520
01:39:26,020 --> 01:39:31,580
and in this case they run the nearest neighbour great

1521
01:39:33,290 --> 01:39:35,200
and simple filters

1522
01:39:35,200 --> 01:39:36,000
there that

1523
01:39:36,080 --> 01:39:38,430
the first group of the approaches

1524
01:39:38,500 --> 01:39:41,720
it's basically the same as filters

1525
01:39:41,720 --> 01:39:44,000
but assuming future independence

1526
01:39:44,900 --> 01:39:49,210
in this that instead of evaluating the the whole feature subset

1527
01:39:49,250 --> 01:39:52,510
we evaluate just individual feature

1528
01:39:52,570 --> 01:39:54,060
it's about

1529
01:39:54,070 --> 01:39:57,800
usually evaluated all the features one by one

1530
01:39:57,850 --> 01:39:59,770
you collect the features score

1531
01:39:59,780 --> 01:40:04,340
or they're all the features by the score and then select the top

1532
01:40:04,390 --> 01:40:05,580
some features

1533
01:40:05,600 --> 01:40:07,560
so how many of the features is

1534
01:40:09,450 --> 01:40:14,390
people decide for different numbers or just you know in the research papers observe how

1535
01:40:14,390 --> 01:40:19,990
the number of selected features influences the performance of the system

1536
01:40:23,290 --> 01:40:25,450
so what is happening in

1537
01:40:25,460 --> 01:40:27,710
domains with many features is

1538
01:40:28,390 --> 01:40:31,190
usually this involves entering approach

1539
01:40:31,240 --> 01:40:34,670
and that's also what use on text data

1540
01:40:35,230 --> 01:40:38,850
and then the methods differ in

1541
01:40:38,900 --> 01:40:43,330
the way how to evaluate individual feature now so we have different feature scoring measures

1542
01:40:43,330 --> 01:40:44,590
that they use

1543
01:40:44,610 --> 01:40:47,490
so here i organise them in

1544
01:40:47,500 --> 01:40:48,890
kind of three groups

1545
01:40:48,900 --> 01:40:50,060
one group is

1546
01:40:50,080 --> 01:40:52,530
supervised feature matches that

1547
01:40:52,540 --> 01:40:54,870
take into account the class value

1548
01:40:54,910 --> 01:40:56,940
like information gain

1549
01:40:56,990 --> 01:41:00,070
a variant of information cross entropy for text

1550
01:41:00,180 --> 01:41:02,360
hotel information for text

1551
01:41:02,440 --> 01:41:04,680
then there is a group of

1552
01:41:05,110 --> 01:41:08,640
feature of feature scoring measures

1553
01:41:08,670 --> 01:41:13,720
but actually they class into account and moreover they work for binary class

1554
01:41:13,760 --> 01:41:17,070
which is not a problem but what's ratio also

1555
01:41:17,160 --> 01:41:21,510
takes into account that one of the class values is the target class value

1556
01:41:21,560 --> 01:41:25,000
which is not uncommon for text data

1557
01:41:25,000 --> 01:41:26,580
for instance if you

1558
01:41:26,600 --> 01:41:30,640
if you do user profiling and you want to find all the documents that interesting

1559
01:41:30,640 --> 01:41:35,220
for your user you know that this is the target class value and the whole

1560
01:41:35,220 --> 01:41:37,000
universe is negative

1561
01:41:37,030 --> 01:41:39,640
so it's good that you're classifier

1562
01:41:39,770 --> 01:41:44,300
that year feature scoring measure knows that it should be kind of concentrate on positive

1563
01:41:44,300 --> 01:41:47,520
examples finding them because it's like

1564
01:41:47,570 --> 01:41:51,610
zero point one percent of positive and everything else is negative

1565
01:41:51,670 --> 01:41:54,910
and the normal separation that actually

1566
01:41:54,910 --> 01:41:57,120
so far

1567
01:41:57,170 --> 01:42:01,250
is the only measure i have seen in combination with it

1568
01:42:01,300 --> 01:42:05,840
the inner SVM on document categorisation that is improving

1569
01:42:05,890 --> 01:42:07,160
the performance of

1570
01:42:08,240 --> 01:42:11,240
because there is a feature subsets selected

1571
01:42:11,290 --> 01:42:15,760
and then there are surprised measures reaching grow or to the class value like

1572
01:42:15,770 --> 01:42:19,200
just taking frequency of the word in the document and

1573
01:42:19,230 --> 01:42:21,220
no selecting the most frequent

1574
01:42:22,460 --> 01:42:25,230
which also surprisingly

1575
01:42:25,310 --> 01:42:30,170
performed surprisingly well i'm not better than the other but surprisingly well

1576
01:42:30,180 --> 01:42:35,230
and i'll show you some results of comparison on the next slides

1577
01:42:35,250 --> 01:42:37,240
and there's is another approach

1578
01:42:37,250 --> 01:42:40,760
actually also based on the simple filtering approach

1579
01:42:40,780 --> 01:42:42,760
but using this idea of

1580
01:42:42,810 --> 01:42:46,540
using embedded approach in order to select the feature subset

1581
01:42:46,580 --> 01:42:51,750
and here it's not embedded in approach to select the whole feature subset that assigns

1582
01:42:51,760 --> 01:42:54,700
score scores to the features

1583
01:42:54,740 --> 01:42:59,100
so instead of calculating information gain or cross entropy or something like that

1584
01:42:59,180 --> 01:43:00,370
actually the

1585
01:43:00,390 --> 01:43:04,800
i mean i SVM is trained on the whole data on all the features

1586
01:43:04,820 --> 01:43:05,990
and then

1587
01:43:06,470 --> 01:43:10,460
there are weights in the normal to the hyperplane SVM model

1588
01:43:10,510 --> 01:43:13,080
and these weights are used as scores

1589
01:43:13,090 --> 01:43:14,640
four individual features

1590
01:43:14,640 --> 01:43:18,450
so the higher the weight you know the feature is more important for the models

1591
01:43:19,150 --> 01:43:20,730
just take that value

1592
01:43:20,740 --> 01:43:25,310
and this feature selection they also show some experimental comparison

1593
01:43:25,370 --> 01:43:27,430
how this performance compared to

1594
01:43:27,430 --> 01:43:30,000
the other feature selection measures and also

1595
01:43:30,010 --> 01:43:31,940
in combination with different

1596
01:43:32,060 --> 01:43:36,370
learning algorithms

1597
01:43:36,420 --> 01:43:44,180
so just a little bit of insights what is happening in this scoring measures

1598
01:43:44,230 --> 01:43:46,990
which actually can help us to understand

1599
01:43:47,000 --> 01:43:50,730
what we see in the next slides how they a

1600
01:43:50,730 --> 01:43:52,490
perform differently

1601
01:43:52,510 --> 01:43:55,970
so for information gain

1602
01:43:56,140 --> 01:44:00,440
it goes over class values

1603
01:44:00,440 --> 01:44:02,920
but also the feature values

1604
01:44:02,970 --> 01:44:09,370
so it ignores this difference between word occurring or not occurring in the document just

1605
01:44:10,370 --> 01:44:11,760
it's zero one

1606
01:44:11,770 --> 01:44:14,460
these are two values of my feature so

1607
01:44:14,460 --> 01:44:19,940
it's a real and symmetric and i just can't click information going over that

1608
01:44:21,000 --> 01:44:23,170
in the in the next slide two will see

1609
01:44:23,180 --> 01:44:27,500
at least in the on the data on the yahoo data that was tried it

1610
01:44:27,500 --> 01:44:30,220
didn't perform well at all

1611
01:44:30,270 --> 01:44:32,840
so there is cross entropy for text

1612
01:44:32,890 --> 01:44:34,480
which performs

1613
01:44:34,530 --> 01:44:37,760
significantly better than information gain

1614
01:44:37,760 --> 01:44:41,490
so each of those things can be a little computer the sensor network

1615
01:44:43,690 --> 01:44:44,400
and then

1616
01:44:44,420 --> 01:44:48,210
well you send messages and when they want on their you do that and all

1617
01:44:48,210 --> 01:44:49,590
the nodes

1618
01:44:49,610 --> 01:44:55,800
and in here all the message that you need to get probabilities

1619
01:44:57,090 --> 01:45:01,820
really nice simple idea

1620
01:45:01,840 --> 01:45:06,230
the questions here

1621
01:45:06,260 --> 01:45:09,440
who can tell me if i run this imperiled

1622
01:45:09,460 --> 01:45:12,130
there is a node does this at the same time

1623
01:45:12,210 --> 01:45:14,130
how long it will take

1624
01:45:14,150 --> 01:45:17,820
until i get the correct message here

1625
01:45:17,840 --> 01:45:22,570
and i'll tell you what requires initialisation if you've got the right message from the

1626
01:45:22,570 --> 01:45:23,630
other banality it

1627
01:45:23,630 --> 01:45:27,190
you just invent something

1628
01:45:28,610 --> 01:45:30,110
and assume that the system

1629
01:45:30,130 --> 01:45:31,710
this will be the right message

1630
01:45:33,360 --> 01:45:35,280
now how long would it take

1631
01:45:35,340 --> 01:45:42,170
to get the correct probabilities here

1632
01:45:42,190 --> 01:45:47,380
you want to use

1633
01:45:47,440 --> 01:45:49,690
i don't think about it for a moment

1634
01:45:50,530 --> 01:45:53,670
in order for this message here to be correct

1635
01:45:53,690 --> 01:45:56,610
this is no need to send the message here

1636
01:45:56,630 --> 01:45:59,230
that you need to do something in in the message on

1637
01:45:59,280 --> 01:46:02,650
so in order to get that message create any two states

1638
01:46:04,860 --> 01:46:05,900
with this one

1639
01:46:05,970 --> 01:46:08,760
also need to six one two

1640
01:46:08,970 --> 01:46:10,380
in this case you

1641
01:46:11,610 --> 01:46:12,740
one two

1642
01:46:15,050 --> 01:46:20,130
so after three iterations of the spiral now

1643
01:46:20,280 --> 01:46:21,970
this message clear

1644
01:46:21,990 --> 01:46:25,550
i can compute this probability exactly

1645
01:46:25,610 --> 01:46:27,380
how long

1646
01:46:27,420 --> 01:46:33,610
would it take for this note

1647
01:46:33,630 --> 01:46:50,280
es ist

1648
01:46:50,340 --> 01:46:53,280
its heart

1649
01:47:01,990 --> 01:47:04,230
i said

1650
01:47:04,240 --> 01:47:07,610
so it's really easy to you look at the graph just count with along this

1651
01:47:07,610 --> 01:47:09,050
path is

1652
01:47:09,070 --> 01:47:14,070
and that's what it takes until everything is converged

1653
01:47:16,550 --> 01:47:19,360
so these out

1654
01:47:27,420 --> 01:47:31,190
well because basically

1655
01:47:31,210 --> 01:47:34,340
all the messages until you get the right one

1656
01:47:34,340 --> 01:47:35,230
i mean

1657
01:47:35,240 --> 01:47:39,740
so don't really matter i mean you might as well not sending great message in

1658
01:47:39,740 --> 01:47:42,400
that case so if you were to implement that

1659
01:47:42,420 --> 01:47:43,880
on a single node

1660
01:47:43,940 --> 01:47:46,760
you would not implemented in parallel

1661
01:47:46,780 --> 01:47:48,380
what you would do doing

1662
01:47:48,400 --> 01:47:50,170
this the following

1663
01:47:50,170 --> 01:47:52,740
i will first so in that case they would just

1664
01:47:53,150 --> 01:47:56,960
first look at well we can actually or is in the message to you would

1665
01:47:56,960 --> 01:47:59,570
start at the leaves of this tree

1666
01:48:00,150 --> 01:48:03,420
so i can see all the messages from the leaves can do this one that

1667
01:48:03,420 --> 01:48:04,590
one that one

1668
01:48:04,630 --> 01:48:05,820
one that one

1669
01:48:05,860 --> 01:48:09,150
that one that one i

1670
01:48:09,170 --> 01:48:12,920
well whatever nobody other two during that time doesn't really matter

1671
01:48:12,960 --> 01:48:16,050
now all the seventies this now realise

1672
01:48:16,070 --> 01:48:18,400
i got a message from here message from there

1673
01:48:18,420 --> 01:48:21,010
so i can at least in the message on here

1674
01:48:21,110 --> 01:48:25,530
OK so therefore this

1675
01:48:28,150 --> 01:48:29,240
this note here

1676
01:48:29,340 --> 01:48:34,030
let's i received an incoming messages from here and i can forward their

1677
01:48:34,030 --> 01:48:36,990
like the right now

1678
01:48:37,010 --> 01:48:39,440
the next step fifty will be doing

1679
01:48:39,490 --> 01:48:44,240
all others can actually do anything with haven't received any other message

1680
01:48:44,240 --> 01:48:46,840
now things start getting interesting

1681
01:48:46,840 --> 01:48:48,780
this known here

1682
01:48:48,820 --> 01:48:52,460
the case received the message from here to seek permission from there

1683
01:48:52,550 --> 01:48:54,440
the second single message

1684
01:48:55,860 --> 01:48:59,880
you can also send the message down here

1685
01:49:02,320 --> 01:49:06,070
so by the time already this right now then this clean

1686
01:49:06,130 --> 01:49:09,570
i will or will know exactly what the probabilities are

1687
01:49:09,840 --> 01:49:13,380
now what's the queen of has received incoming message

1688
01:49:13,440 --> 01:49:17,900
dresses up a message from here from their second centered on

1689
01:49:17,900 --> 01:49:23,490
likewise the message from here and from here i consider up there

1690
01:49:23,550 --> 01:49:26,510
so this is how you would implement it sequential

1691
01:49:26,610 --> 01:49:28,670
this considerably less wasteful

1692
01:49:28,670 --> 01:49:29,690
but if

1693
01:49:30,110 --> 01:49:32,800
the other nodes to something

1694
01:49:32,800 --> 01:49:34,760
stupid while

1695
01:49:35,030 --> 01:49:38,570
in the meantime well OK these are

1696
01:49:38,630 --> 01:49:42,670
we're we're

1697
01:49:44,190 --> 01:49:48,650
if you have a single computer on which implement all this you're absolutely right

1698
01:49:48,670 --> 01:49:50,320
what i suggested is stupid

1699
01:49:51,570 --> 01:49:52,940
if you have say

1700
01:49:52,960 --> 01:49:54,710
the really big graphical models

1701
01:49:54,730 --> 01:49:57,110
in each of those nodes the computer

1702
01:49:57,130 --> 01:50:01,090
he might not even know how big the network is

1703
01:50:01,320 --> 01:50:04,960
and so it would be a good idea just ignoring it started running

1704
01:50:05,010 --> 01:50:08,380
the other thing is and this is quite curious phenomenon

1705
01:50:08,420 --> 01:50:09,860
sometimes really

1706
01:50:09,860 --> 01:50:11,440
so far away

1707
01:50:11,610 --> 01:50:13,670
now may not have

1708
01:50:13,690 --> 01:50:16,840
much of an effect on what you're going to do local

1709
01:50:16,860 --> 01:50:20,190
so even though you started with the wrong message

1710
01:50:20,210 --> 01:50:23,780
eventually as he passes through lots and lots and lots and lots of nodes which

1711
01:50:23,780 --> 01:50:29,210
are reduced to do the right thing to make a wrong thing right

1712
01:50:29,210 --> 01:50:34,360
what's the point if you have a network of computers something might go down something

1713
01:50:35,340 --> 01:50:36,440
you might be running

1714
01:50:36,490 --> 01:50:38,630
and you might not really no

1715
01:50:38,670 --> 01:50:40,030
before and which ones

1716
01:50:40,050 --> 01:50:42,360
are up and which ones are that

1717
01:50:43,670 --> 01:50:47,190
you wouldn't want to just send around the message along if you

1718
01:50:47,210 --> 01:50:48,470
i don't have any other

1719
01:50:48,490 --> 01:50:50,710
i don't have an incoming message

1720
01:50:50,730 --> 01:50:52,840
so a really good idea is

1721
01:50:52,860 --> 01:50:54,820
to take some pride

1722
01:50:56,030 --> 01:51:00,260
seeing the right message that looks most similar to what you what you would have

1723
01:51:01,690 --> 01:51:03,820
two neighbours

1724
01:51:03,820 --> 01:51:08,240
a very nice paper by colours christian and mark asking

1725
01:51:08,260 --> 01:51:09,340
from you i

1726
01:51:09,340 --> 01:51:11,400
will square these to compensate

1727
01:51:11,420 --> 01:51:12,700
before it

1728
01:51:12,720 --> 01:51:14,440
and then

1729
01:51:14,490 --> 01:51:15,320
so we have

1730
01:51:15,330 --> 01:51:16,870
the map

1731
01:51:17,080 --> 01:51:18,920
so this is

1732
01:51:18,930 --> 01:51:20,950
the answer and

1733
01:51:20,970 --> 01:51:22,800
and this is

1734
01:51:22,850 --> 01:51:24,090
by s

1735
01:51:25,910 --> 01:51:28,270
and we are very very

1736
01:51:28,330 --> 01:51:32,670
so we have this square of the difference becomes it's one of the first describe

1737
01:51:32,690 --> 01:51:37,970
the second life first place and second here is the same as one the first

1738
01:51:37,970 --> 01:51:41,540
described the second place the product

1739
01:51:41,620 --> 01:51:46,830
and and here i have my linearity of expectation with expectation of this minus these

1740
01:51:46,830 --> 01:51:54,760
processes is expectation of these mines this practitioner face but this is a constant

1741
01:51:54,800 --> 01:51:56,950
so we had system this

1742
01:51:57,020 --> 01:52:00,650
the speculation of the expectation expectation is a constant

1743
01:52:00,680 --> 01:52:02,380
i mean

1744
01:52:02,390 --> 01:52:08,860
here i am looking at that it was because then here i have one expectations

1745
01:52:08,860 --> 01:52:11,600
squared i have another once you have to

1746
01:52:11,650 --> 01:52:15,690
but i have minus two here of the same thing expectations square

1747
01:52:15,700 --> 01:52:18,230
so all these

1748
01:52:18,270 --> 01:52:22,110
concepts of

1749
01:52:22,130 --> 01:52:24,250
so what i mean

1750
01:52:24,250 --> 01:52:26,220
what's left

1751
01:52:26,270 --> 01:52:27,770
the square

1752
01:52:27,820 --> 01:52:30,250
this presentation of the square of the estimator

1753
01:52:30,300 --> 01:52:35,140
minus prices product plus the square of the value to be predicted

1754
01:52:35,170 --> 01:52:36,890
and here

1755
01:52:36,930 --> 01:52:39,080
i am

1756
01:52:39,120 --> 01:52:43,060
taking expectations again

1757
01:52:43,780 --> 01:52:48,740
is a constant tensions partition i come with the corresponding and this is a constant

1758
01:52:48,760 --> 01:52:51,230
is expectation is the same cost

1759
01:52:51,330 --> 01:52:57,100
here i have now three is uncomfortable them out by linearity and i get expectations

1760
01:52:57,150 --> 01:53:04,930
of this quantity and this quantity sounds like the square of the difference not

1761
01:53:08,610 --> 01:53:11,470
the first square

1762
01:53:11,510 --> 01:53:13,230
energy committee

1763
01:53:13,290 --> 01:53:15,790
is one of symmetry office

1764
01:53:16,810 --> 01:53:19,590
the real value you want to predict y

1765
01:53:19,600 --> 01:53:24,650
squared error is that the square the others part of the research partition of one

1766
01:53:24,660 --> 01:53:26,620
and what you saying here

1767
01:53:26,690 --> 01:53:29,250
is that you can move

1768
01:53:29,300 --> 01:53:30,440
that they

1769
01:53:30,860 --> 01:53:32,580
square error

1770
01:53:32,610 --> 01:53:34,350
equals the sum

1771
01:53:34,370 --> 01:53:47,040
of the ninety s plus the square of the bias

1772
01:53:47,090 --> 01:53:51,600
this is a list of what is what happens now

1773
01:53:51,660 --> 01:53:53,590
do you o four

1774
01:53:53,640 --> 01:53:55,230
the a b

1775
01:53:55,250 --> 01:53:56,340
at each

1776
01:53:56,370 --> 01:53:58,770
hypothesis class of which

1777
01:54:00,450 --> 01:54:03,220
you can is very much

1778
01:54:03,230 --> 01:54:05,640
you're biased

1779
01:54:05,660 --> 01:54:09,330
but what's going to happen to the variance if you're sure which do have so

1780
01:54:09,330 --> 01:54:12,730
many possibilities you can approximate very well anything

1781
01:54:12,730 --> 01:54:16,050
you know attacking the knowledge that is in your sample

1782
01:54:16,090 --> 01:54:20,220
only on their feet adjusting so what do you have some

1783
01:54:20,270 --> 01:54:21,430
during car

1784
01:54:21,450 --> 01:54:24,900
latin dictionary because of the high

1785
01:54:24,910 --> 01:54:26,260
on the other hand

1786
01:54:26,270 --> 01:54:35,400
if you have a very robust they very boring passes class either this or this

1787
01:54:35,420 --> 01:54:39,060
well maybe you commit to matter because of the bias but you will be

1788
01:54:39,090 --> 01:54:44,720
must i mean if i need the height in metres

1789
01:54:44,730 --> 01:54:45,610
they take

1790
01:54:45,630 --> 01:54:47,600
the is of different centers

1791
01:54:47,610 --> 01:54:50,800
and it has to be or one that and something

1792
01:54:50,800 --> 01:54:53,930
my answer is going to be one that i will be pretty independent of the

1793
01:54:57,300 --> 01:55:00,270
if i miss to the mean

1794
01:55:00,320 --> 01:55:06,010
well i just change one based in the sample and the whole thing i

1795
01:55:06,020 --> 01:55:09,890
what is the best there is no reason no best

1796
01:55:09,910 --> 01:55:15,590
you have to be careful if both sources of

1797
01:55:15,590 --> 01:55:21,930
and this will show up very briefly in the next few slides

1798
01:55:23,030 --> 01:55:25,990
a bit faster now

1799
01:55:26,010 --> 01:55:31,130
so you're making a prediction on the basis of the given data

1800
01:55:31,150 --> 01:55:35,270
is it true that the data determine your religion only that you don't know which

1801
01:55:38,220 --> 01:55:39,970
this is actually the

1802
01:55:40,020 --> 01:55:42,140
you have all the data you would like to

1803
01:55:42,170 --> 01:55:44,310
and the extra factor there

1804
01:55:44,310 --> 01:55:49,240
that you don't have a way of accounting for

1805
01:55:49,290 --> 01:55:54,050
many people call this a lot and i was some there are variables that influence

1806
01:55:54,060 --> 01:55:57,470
your outcome but do have access to to the this problem

1807
01:55:57,480 --> 01:56:00,230
and the HMM switch up this

1808
01:56:01,070 --> 01:56:02,970
later on

1809
01:56:03,180 --> 01:56:11,470
or they may not repeat exactly the and you have a small amusement centre

1810
01:56:12,580 --> 01:56:14,640
for instance

1811
01:56:14,750 --> 01:56:18,380
aviation frogs

1812
01:56:18,390 --> 01:56:20,870
but how how do do you how do you

1813
01:56:21,130 --> 01:56:26,640
how do you manage do know the conditional probability of class is given the values

1814
01:56:26,640 --> 01:56:27,370
of the

1815
01:56:27,380 --> 01:56:32,370
parameters but are not from not to flowers that the exactly the same number of

1816
01:56:32,370 --> 01:56:36,630
many of weight of with of of the

1817
01:56:36,640 --> 01:56:38,820
o well

1818
01:56:38,820 --> 01:56:40,070
as soon as you are

1819
01:56:40,140 --> 01:56:42,300
using some sort of

1820
01:56:45,080 --> 01:56:50,260
this is what in problem but be aware you have one x bias or you

1821
01:56:50,260 --> 01:56:52,020
have one more

1822
01:56:52,780 --> 01:56:56,040
externally this is you to

1823
01:56:56,240 --> 01:56:59,020
prediction process

1824
01:56:59,030 --> 01:57:02,170
as an aside

1825
01:57:02,230 --> 01:57:05,810
if you write this

1826
01:57:05,860 --> 01:57:07,120
if you look at this

1827
01:57:07,130 --> 01:57:09,330
you recognise this exactly

1828
01:57:09,330 --> 01:57:10,610
the probability

1829
01:57:10,640 --> 01:57:12,800
that you that this vector

1830
01:57:12,800 --> 01:57:17,560
in gaussian with mean and variance

1831
01:57:17,600 --> 01:57:22,670
but just as a function of n and and observation come back to the other

1832
01:57:23,650 --> 01:57:28,800
you can assume that m and the feature or you can assume that they

1833
01:57:28,810 --> 01:57:31,810
feature is the same

1834
01:57:31,860 --> 01:57:33,970
in one case they call it

1835
01:57:34,500 --> 01:57:37,820
reliability and in other cases they call likelihood

1836
01:57:38,570 --> 01:57:44,500
if you're some people here is a computer scientist this is

1837
01:57:44,890 --> 01:57:46,590
we're doing this sort of

1838
01:57:46,590 --> 01:57:47,290
o thing

1839
01:57:47,350 --> 01:57:48,530
all of the

1840
01:57:48,540 --> 01:57:53,360
ten two are parameters into data they to parameters and

1841
01:57:53,380 --> 01:57:59,510
so what is behind these as assumption that similar observations lead to similar to what

1842
01:57:59,510 --> 01:58:02,950
you said and it was so much about the extreme case

1843
01:58:02,970 --> 01:58:08,470
of this assumption you work it out is is nearest neighbors is where you

1844
01:58:08,490 --> 01:58:12,700
eight of your prediction by looking at the cases that are most similar

1845
01:58:12,740 --> 01:58:17,800
the ones that you have around a probably so yesterday that it depends very much

1846
01:58:17,800 --> 01:58:19,780
for how to choose the distance

1847
01:58:19,890 --> 01:58:23,370
and there are a number of issues

1848
01:58:23,370 --> 01:58:28,570
me and computationally an issue because in high dimensions finding out the k nearest neighbors

1849
01:58:28,570 --> 01:58:30,350
through topic e

1850
01:58:30,350 --> 01:58:32,260
the reason

1851
01:58:32,300 --> 01:58:34,990
but now imagine you're working for

1852
01:58:35,160 --> 01:58:39,200
bank and was just asking you to some data-mining

1853
01:58:39,240 --> 01:58:41,700
and you come up with the naive bayes predictors

1854
01:58:41,720 --> 01:58:45,550
you showed us tables full of real numbers to your boss

1855
01:58:45,600 --> 01:58:49,120
it's going to like it

1856
01:58:52,010 --> 01:58:53,350
you managed to to

1857
01:58:53,350 --> 01:58:56,220
okay so that's all nice and there's also a lot of hype

1858
01:58:58,840 --> 01:59:01,770
there are some real research there remains to be done and that

1859
01:59:04,250 --> 01:59:05,550
some interesting challenges

1860
01:59:06,790 --> 01:59:07,130
so this

1861
01:59:07,700 --> 01:59:08,030
the trail

1862
01:59:08,470 --> 01:59:12,230
it's about these challenges also introducing you to some of the concepts

1863
01:59:12,730 --> 01:59:13,600
behind deep learning

1864
01:59:15,340 --> 01:59:17,960
end i'll try to tell u about

1865
01:59:18,440 --> 01:59:21,010
what i consider the major challenge which is

1866
01:59:21,740 --> 01:59:27,570
how to move the successes we've had with supervised learning most of these applications are being using supervised learning

1867
01:59:28,930 --> 01:59:33,420
unsupervised or semi supervised learning we have tons of unlabeled or weakly labeled data

1868
01:59:34,260 --> 01:59:35,830
and structured outputs which is

1869
01:59:36,300 --> 01:59:38,460
tightly connected to the unsupervised learning question

1870
01:59:40,160 --> 01:59:42,610
be off-putting the system is not just a category

1871
01:59:43,390 --> 01:59:44,080
a few numbers

1872
01:59:44,530 --> 01:59:50,390
but it's something that's very high dimensional like a sentence in image war some complicated data structure

1873
01:59:51,450 --> 01:59:52,330
the thing with these

1874
01:59:53,110 --> 01:59:54,970
both supervised both of these

1875
01:59:55,460 --> 01:59:58,020
structured outputs and unsupervised learning scenarios is

1876
01:59:59,850 --> 02:00:02,200
probabilistic models quickly become intractable

1877
02:00:05,860 --> 02:00:08,540
both from a computational point of view and from a statistical point of view

1878
02:00:10,860 --> 02:00:14,500
we use deepening in these areas could have a fairly important impact

1879
02:00:15,390 --> 02:00:16,190
all tell you about

1880
02:00:16,990 --> 02:00:21,110
the challenge of scaling up from a computational point of view because what we've seen

1881
02:00:21,110 --> 02:00:24,790
in recent years is the better models are the bigger ones

1882
02:00:25,390 --> 02:00:26,170
and we limited

1883
02:00:26,690 --> 02:00:29,160
just by our ability to try data models

1884
02:00:30,310 --> 02:00:33,780
and related to this will tell you about some of the challenges arising in

1885
02:00:36,100 --> 02:00:36,700
deep nets

1886
02:00:37,420 --> 02:00:39,580
so there's a numerical optimization issue

1887
02:00:40,460 --> 02:00:42,730
even the techniques we are using are

1888
02:00:43,650 --> 02:00:44,580
doing amazingly well

1889
02:00:45,620 --> 02:00:47,580
we have some indication that

1890
02:00:48,620 --> 02:00:53,940
we're not doing as well as we could end all tell you about and some ideas to get around these

1891
02:00:55,980 --> 02:00:56,490
and if we can

1892
02:00:56,940 --> 02:00:59,130
if we can face these challenges than our

1893
02:00:59,860 --> 02:01:00,460
it can be a much

1894
02:01:02,180 --> 02:01:04,970
more impressive impact on only i think

1895
02:01:05,860 --> 02:01:10,490
with computers i can see here understand natural language understanding humans

1896
02:01:11,620 --> 02:01:14,210
and provide better service to to humans

1897
02:01:14,820 --> 02:01:18,480
i've course maybe even understand help us understand how humans and animals

1898
02:01:19,940 --> 02:01:22,470
managed to solve difficult learning problems

1899
02:01:23,900 --> 02:01:24,260
all right

1900
02:01:24,980 --> 02:01:25,430
now let's

1901
02:01:25,930 --> 02:01:27,770
go a bit more technical slowly

1902
02:01:30,490 --> 02:01:30,770
how do we

1903
02:01:31,170 --> 02:01:34,300
build intelligent machines well on

1904
02:01:35,180 --> 02:01:36,930
you need knowledge in order to

1905
02:01:37,650 --> 02:01:39,060
take intelligent decisions

1906
02:01:39,740 --> 02:01:43,370
and you need doesn't always to come from somewhere in part of it could be that we

1907
02:01:44,380 --> 02:01:46,720
feedback knowledge directly machines but

1908
02:01:47,350 --> 02:01:51,420
what we learned in the last few decades is not a large part of it has to be

1909
02:01:52,120 --> 02:01:55,320
directly acquired by the machine and that's what machine learning is about

1910
02:01:56,750 --> 02:01:57,550
and if u

1911
02:01:59,840 --> 02:02:04,140
learn about machine learning and try to figure out what corps elements

1912
02:02:04,570 --> 02:02:06,810
that you find in machine learning well

1913
02:02:07,480 --> 02:02:10,070
one core element is priors that there no

1914
02:02:10,520 --> 02:02:12,430
there is no general purpose machine learning algorithm

1915
02:02:13,490 --> 02:02:18,330
you need some priors and price can be very specialized to your application or they can be very broad

1916
02:02:18,870 --> 02:02:21,510
and in the i was trying to get very broad priors

1917
02:02:24,200 --> 02:02:25,670
both both could be interesting

1918
02:02:26,330 --> 02:02:31,810
then there is always an element of optimization or search were searching forward a good

1919
02:02:31,810 --> 02:02:36,430
solution to problem in a large family of solution or in the case of inference

1920
02:02:36,430 --> 02:02:40,750
we're looking forward searching for a good explanation to particular input

1921
02:02:41,280 --> 02:02:42,160
like when you are doing

1922
02:02:42,900 --> 02:02:45,370
machine translation or speech recognition

1923
02:02:45,810 --> 02:02:48,870
there are many explanations for things we see and we're looking for the best one

1924
02:02:48,890 --> 02:02:51,180
so there's search query optimization comes up

1925
02:02:52,160 --> 02:02:53,170
and these are the times

1926
02:02:55,330 --> 02:02:59,920
practically from a computer science perspective there is also an issue efficiency of computation

1927
02:03:01,060 --> 02:03:01,670
we've seen

1928
02:03:02,180 --> 02:03:04,050
what programs in recent years due to

1929
02:03:04,860 --> 02:03:09,640
do you use that can paralyze a lot of the computations so thinking about that aspect is also important

1930
02:03:14,150 --> 02:03:15,450
the more fundamental

1931
02:03:16,340 --> 02:03:19,390
question behind machine learning is generalization how do we get

1932
02:03:20,880 --> 02:03:22,050
are learned here to

1933
02:03:22,790 --> 02:03:25,100
provide the right answers for new cases

1934
02:03:27,030 --> 02:03:27,790
the take that i'm

1935
02:03:28,960 --> 02:03:33,510
trying to put forward on this generalization question is eight geometrical one so

1936
02:03:34,470 --> 02:03:36,270
during the presentation you'll see this come back

1937
02:03:36,760 --> 02:03:39,160
well i'm thinking about generalization is

1938
02:03:39,640 --> 02:03:41,840
how we take the probability mass

1939
02:03:42,850 --> 02:03:44,160
that's we know should be

1940
02:03:45,360 --> 02:03:48,820
on the training examples in and spreading it in the right way

1941
02:03:50,250 --> 02:03:52,770
i'm guessing where it should concentrate

1942
02:03:55,260 --> 02:03:56,820
you'll see this notion come back

1943
02:03:59,390 --> 02:04:01,260
unfortunately in high dimensional spaces

1944
02:04:02,210 --> 02:04:04,970
guessing where to put a probability mass is

1945
02:04:05,610 --> 02:04:07,750
seems intractable because there's eight

1946
02:04:08,760 --> 02:04:11,250
exponentially large large number of configurations

1947
02:04:12,410 --> 02:04:14,490
of places where you could put protein mass

1948
02:04:15,340 --> 02:04:16,950
so that's the curse dimensionality

1949
02:04:18,630 --> 02:04:19,890
and how do we get around at

1950
02:04:23,130 --> 02:04:25,140
something i've been pushing the last few years

1951
02:04:26,100 --> 02:04:27,150
this idea in mind

1952
02:04:28,190 --> 02:04:30,070
we can get around the curse of dimensionality

1953
02:04:31,180 --> 02:04:33,440
if we are learning systems

1954
02:04:36,420 --> 02:04:38,690
and disentangle separate out

1955
02:04:39,890 --> 02:04:40,820
these underlying

1956
02:04:41,430 --> 02:04:44,130
factors that explain the data underlying causes

1957
02:04:44,930 --> 02:04:46,420
basically making sense of the data

1958
02:04:47,100 --> 02:04:50,740
so it may seem like a hard thing to do and of course it is

1959
02:04:52,590 --> 02:04:53,700
i i've been trying to

1960
02:04:54,790 --> 02:04:56,550
pushed as the agenda four

1961
02:04:57,260 --> 02:04:58,980
making really serious progress towards there

1962
02:05:02,070 --> 02:05:03,460
all right so now let's go back to

1963
02:05:03,460 --> 02:05:07,170
but i'm also proud of my alumni in this course who

1964
02:05:07,180 --> 02:05:09,670
are not in the world of finance

1965
02:05:09,720 --> 02:05:12,770
i think this course is go beyond

1966
02:05:12,820 --> 02:05:17,170
it's not just for people who are planning careers in finance

1967
02:05:20,900 --> 02:05:24,610
finance is a very important technology

1968
02:05:24,630 --> 02:05:29,840
and it's very important to know finance to understand what happens

1969
02:05:29,850 --> 02:05:31,230
in the real world

1970
02:05:31,250 --> 02:05:34,110
just about any human endeavor

1971
02:05:34,190 --> 02:05:35,950
it involves finance

1972
02:05:35,970 --> 02:05:36,960
you might say

1973
02:05:36,970 --> 02:05:38,430
i could be a poet

1974
02:05:38,440 --> 02:05:40,840
and what does that have to do with finances

1975
02:05:40,850 --> 02:05:45,680
well it probably ends up having something to do with finance because as a poet

1976
02:05:45,720 --> 02:05:48,760
you probably want to publish your poetry writing

1977
02:05:48,780 --> 02:05:52,490
and you're going to be talking to publishers and before you know it they're going

1978
02:05:52,490 --> 02:05:57,440
to be talking about their financial situation and how you fit into it so i

1979
02:05:57,440 --> 02:06:00,730
believe its fundamental and important and so

1980
02:06:00,740 --> 02:06:02,560
i think you

1981
02:06:02,610 --> 02:06:05,610
will find this course has not

1982
02:06:05,630 --> 02:06:07,810
a vocational course not

1983
02:06:07,850 --> 02:06:12,060
primarily of occasional vocational course but the intellectual course

1984
02:06:12,080 --> 02:06:14,610
about how things work really work

1985
02:06:14,630 --> 02:06:19,890
i see finances the underpinning of so much that happens it's a powerful force that

1986
02:06:19,890 --> 02:06:25,510
goes behind the scenes and i hope we can draw that in this course

1987
02:06:25,520 --> 02:06:30,860
there is another course in that we have two basic courses in finance for undergraduates

1988
02:06:31,550 --> 02:06:36,010
the other one is economics two fifty one

1989
02:06:36,060 --> 02:06:41,080
financial theory this financial markets that one is financial theory

1990
02:06:41,100 --> 02:06:44,480
last year i was taught by rafael romo

1991
02:06:44,700 --> 02:06:49,510
that was because john john accomplice who usually teaches the course

1992
02:06:49,530 --> 02:06:51,150
i was on the

1993
02:06:51,170 --> 02:06:53,720
and so

1994
02:06:53,760 --> 02:06:57,880
so we had to find someone else but i assume that next fall

1995
02:06:57,920 --> 02:07:02,540
john john accomplished will be again teaching two fifty one

1996
02:07:02,550 --> 02:07:06,890
so what happened why do we have these two courses well it was something like

1997
02:07:07,190 --> 02:07:08,450
eight years ago

1998
02:07:08,470 --> 02:07:12,710
that we really reach the present

1999
02:07:12,950 --> 02:07:17,530
the situation with to finance courses and john an accomplice and i

2000
02:07:17,580 --> 02:07:22,400
had a meeting and we tried to divide up the subject matter of finance in

2001
02:07:22,410 --> 02:07:26,500
the two courses and so we thought of financial theory in financial markets would be

2002
02:07:26,500 --> 02:07:27,340
the two

2003
02:07:27,360 --> 02:07:33,800
but the the problem was that both john and i are interested both in theory

2004
02:07:33,930 --> 02:07:36,580
and other applications

2005
02:07:36,920 --> 02:07:38,560
john john accomplice

2006
02:07:38,580 --> 02:07:41,740
is actually chief economist for large

2007
02:07:41,750 --> 02:07:46,900
investment company called ellington capital in greenwich connecticut which you see a lot of the

2008
02:07:46,900 --> 02:07:49,060
news has been very successful

2009
02:07:50,060 --> 02:07:53,770
and so he is very much interested in the real world

2010
02:07:53,820 --> 02:07:59,590
and so my interested in financial theory and so we find it

2011
02:07:59,600 --> 02:08:03,850
we decided after talking about that we really can't

2012
02:08:03,870 --> 02:08:07,190
divide up the subject matter of finance

2013
02:08:07,230 --> 02:08:09,640
into separate courses on theory

2014
02:08:09,720 --> 02:08:10,950
in practice

2015
02:08:11,070 --> 02:08:15,740
the either one if you try to do one alone it would not work

2016
02:08:15,760 --> 02:08:19,340
so what we decided to do was to divide it up

2017
02:08:19,380 --> 02:08:24,040
imperfectly and there may be some repetition between our two courses

2018
02:08:24,090 --> 02:08:25,340
both of them

2019
02:08:26,840 --> 02:08:33,370
self-contained courses so you could take either two fifty one or two fifty two or

2020
02:08:33,370 --> 02:08:35,030
you could take both

2021
02:08:35,040 --> 02:08:40,090
and i think maybe the best option is to take both if you're really interested

2022
02:08:40,090 --> 02:08:41,420
in the subject matter

2023
02:08:45,120 --> 02:08:49,590
it is true though that his cause is more

2024
02:08:49,640 --> 02:08:51,350
turned into

2025
02:08:51,400 --> 02:08:53,500
theoretical detail than mine

2026
02:08:53,630 --> 02:08:58,070
and john is a mathematical economist

2027
02:08:58,090 --> 02:09:02,500
and and we both love mathematics but may be done

2028
02:09:02,510 --> 02:09:06,370
is going to do more of it than than i am this course

2029
02:09:06,420 --> 02:09:10,300
actually we will not use a heavy amount of

2030
02:09:14,160 --> 02:09:15,890
try to keep it so that

2031
02:09:15,900 --> 02:09:20,350
people who are not comfortable with a lot of math would take this course

2032
02:09:20,420 --> 02:09:26,430
and so

2033
02:09:26,470 --> 02:09:28,680
but i

2034
02:09:28,690 --> 02:09:31,050
so i want to emphasise that this is

2035
02:09:31,060 --> 02:09:36,620
i i've said that i think this course is

2036
02:09:36,670 --> 02:09:39,580
vocational preparation in a sense

2037
02:09:39,630 --> 02:09:44,080
i pride myself on the fact that people who have taken this course find it

2038
02:09:45,100 --> 02:09:47,110
in a subsequent lives

2039
02:09:47,130 --> 02:09:50,310
but on the other hand i think that it's

2040
02:09:50,440 --> 02:09:56,620
really interested in how this i find it really interesting and so i hope that

2041
02:09:56,620 --> 02:09:57,710
you will too

2042
02:09:57,730 --> 02:10:00,230
now i don't know i may be different than other people

2043
02:10:00,250 --> 02:10:05,760
i think organic chemistry is really interesting how many of you have that feeling

2044
02:10:05,780 --> 02:10:11,050
show of hands who is interested in organic chemistry

2045
02:10:11,070 --> 02:10:17,550
are not getting along with cancer hayes unfortunately i've never taken a course in it

2046
02:10:17,550 --> 02:10:23,450
but accelerating lately out of just like a broad intellectual interests

2047
02:10:24,450 --> 02:10:28,570
that is of course that has a bad reputation doesn't because people say i've got

2048
02:10:28,630 --> 02:10:31,940
take that if i want to be in medical pre-med

2049
02:10:34,200 --> 02:10:35,640
you know to me

2050
02:10:35,680 --> 02:10:39,590
there's a lot of detail in organic chemistry to me

2051
02:10:39,640 --> 02:10:42,190
when you read the details you're getting into

2052
02:10:42,430 --> 02:10:45,050
something deep and important

2053
02:10:45,070 --> 02:10:46,860
about the way everything works

2054
02:10:46,900 --> 02:10:49,260
and so i start to find it interesting

2055
02:10:49,310 --> 02:10:54,150
so i don't know how people feel about taking turning off by saying that there's

2056
02:10:54,150 --> 02:10:56,510
going to be a lot of detail in this course

2057
02:10:56,560 --> 02:11:00,280
but i may be made a big mistake by likening it to an organic chemistry

2058
02:11:00,280 --> 02:11:02,790
course so i don't mean to turn you on

2059
02:11:02,810 --> 02:11:05,600
the idea in this course

2060
02:11:05,630 --> 02:11:07,050
is that

2061
02:11:07,060 --> 02:11:11,800
it being in financial markets course is you have to know how the world works

2062
02:11:11,850 --> 02:11:16,310
we're going to be thinking about that in connection with financial theory

2063
02:11:16,320 --> 02:11:19,130
but we we have to get into the details

2064
02:11:19,140 --> 02:11:20,150
and so

2065
02:11:20,160 --> 02:11:21,790
we are going to be

2066
02:11:21,850 --> 02:11:25,290
we are going to be learning about

2067
02:11:25,300 --> 02:11:27,010
in fact

2068
02:11:29,180 --> 02:11:31,980
so let me

2069
02:11:32,060 --> 02:11:35,100
let me start by talking about the the textbooks

2070
02:11:37,700 --> 02:11:39,720
principal textbook

2071
02:11:43,750 --> 02:11:45,730
rec posi

2072
02:11:45,750 --> 02:11:51,050
the other authors are modigliani jones and very

2073
02:11:54,700 --> 02:11:57,820
foundations of financial markets and institutions

2074
02:11:57,830 --> 02:11:59,610
this textbook

2075
02:12:01,540 --> 02:12:03,740
very detailed

2076
02:12:05,790 --> 02:12:10,020
it may be some p i had some reaction to students is more than i

2077
02:12:10,020 --> 02:12:11,720
wanted to know

