1
00:00:00,000 --> 00:00:02,730
nine light situation

2
00:00:02,770 --> 00:00:07,380
so is not completely dark but

3
00:00:07,400 --> 00:00:10,550
so now i'm going to move the light across the two wires

4
00:00:10,710 --> 00:00:15,570
you're going to see that there are locations where the potential difference is very large

5
00:00:15,630 --> 00:00:17,840
because we had resonance

6
00:00:17,880 --> 00:00:21,570
and therefore you'll see a lot of light

7
00:00:21,630 --> 00:00:24,520
and there are locations where there are no deadlines

8
00:00:24,570 --> 00:00:26,800
and you will see no light

9
00:00:26,900 --> 00:00:30,090
let's first go here that's the open end

10
00:00:30,130 --> 00:00:33,980
the open and is clearly where the electric field is fairly high it's like

11
00:00:34,000 --> 00:00:37,250
the situation of the string which an open and

12
00:00:37,250 --> 00:00:39,380
you an and i know there

13
00:00:39,440 --> 00:00:41,780
we have an entire node right here

14
00:00:41,890 --> 00:00:45,780
but look when i move the light a quarter wavelength goes out

15
00:00:45,800 --> 00:00:50,170
i must be known new to why maybe not exactly all the node line

16
00:00:50,230 --> 00:00:51,230
and now

17
00:00:51,230 --> 00:00:55,320
i move another quarter wavelength and the light goes on again

18
00:00:55,320 --> 00:00:58,150
now i'm going to move it back to the open end

19
00:00:58,190 --> 00:00:59,820
you see the light is on

20
00:00:59,820 --> 00:01:02,710
i move a quarter wavelength light goes off

21
00:01:02,750 --> 00:01:06,550
i more than a quarter wavelength light goes on

22
00:01:06,550 --> 00:01:10,320
i more than a a quarter wavelength light goes off

23
00:01:10,360 --> 00:01:12,400
there is amazing

24
00:01:12,480 --> 00:01:16,610
now i think i'm going to move within a quarter wavelength

25
00:01:16,630 --> 00:01:18,630
and the light goes on again

26
00:01:18,670 --> 00:01:21,900
so the amazing thing that you see in front of your eyes

27
00:01:21,940 --> 00:01:23,940
spending voltage

28
00:01:23,940 --> 00:01:26,710
wave between these two i

29
00:01:26,750 --> 00:01:28,340
it is across the wires

30
00:01:29,130 --> 00:01:31,650
along the wires

31
00:01:31,710 --> 00:01:33,250
now what i'm going to do

32
00:01:33,300 --> 00:01:35,550
i'm going to short is out here

33
00:01:35,550 --> 00:01:38,940
when i put the light bulb right in the middle here

34
00:01:39,040 --> 00:01:42,840
you could put the light bulb a quarter wavelength

35
00:01:42,880 --> 00:01:45,300
now when i showed up at the end

36
00:01:45,340 --> 00:01:47,480
i'm no longer residence

37
00:01:47,550 --> 00:01:51,800
no longer meets the resonance condition which i mean now an open

38
00:01:51,840 --> 00:01:54,550
something that you can very easily

39
00:01:54,670 --> 00:01:57,500
now the moment that i no longer had resonance

40
00:01:57,610 --> 00:02:00,840
i no longer have very high potential difference

41
00:02:00,860 --> 00:02:02,610
at the end i nodes

42
00:02:02,710 --> 00:02:05,520
which i had before when i was resonance

43
00:02:05,590 --> 00:02:07,020
but for sure

44
00:02:07,130 --> 00:02:11,690
the moment that i sure about these lines at the end

45
00:02:11,710 --> 00:02:13,190
the locations

46
00:02:13,210 --> 00:02:16,770
which i now know lines must become entire nodes

47
00:02:16,770 --> 00:02:21,570
now maybe not super duper and i nodes with huge potential difference

48
00:02:21,630 --> 00:02:25,190
but probably high enough for that like to go on

49
00:02:26,090 --> 00:02:27,250
ready for this

50
00:02:27,250 --> 00:02:31,690
now i'm going to show it is out

51
00:02:31,710 --> 00:02:33,190
and as you can see

52
00:02:33,210 --> 00:02:35,130
the light goes on

53
00:02:35,190 --> 00:02:37,070
now it may not be as bright

54
00:02:37,090 --> 00:02:38,980
as you've seen it before

55
00:02:39,630 --> 00:02:41,610
this is again and i note

56
00:02:41,690 --> 00:02:43,920
and the light is on

57
00:02:43,940 --> 00:02:46,610
so using a remarkable

58
00:02:46,670 --> 00:02:49,840
demonstration here you see the standing waves

59
00:02:49,860 --> 00:02:53,570
you see the note aligned you see and the lines you also see

60
00:02:53,590 --> 00:02:56,270
that depends on the boundary condition

61
00:02:56,300 --> 00:02:57,170
but if you

62
00:02:57,190 --> 00:03:00,940
make these open one-quarter wavelength from the open end

63
00:03:00,960 --> 00:03:05,130
there's an ant i know if you have it open one-quarter wavelength from open and

64
00:03:05,130 --> 00:03:09,440
is notable line and if you should short could then it becomes and n type

65
00:03:14,090 --> 00:03:15,380
i have a second

66
00:03:17,090 --> 00:03:19,780
which is also very interesting let me turn this off

67
00:03:19,800 --> 00:03:24,590
for some of you touch those get some problems

68
00:03:24,840 --> 00:03:28,860
i have another very nice demonstration which is a cable which is a hundred and

69
00:03:28,860 --> 00:03:31,130
twenty seven meters long

70
00:03:31,150 --> 00:03:34,150
see here

71
00:03:34,250 --> 00:03:35,840
starts there

72
00:03:35,920 --> 00:03:39,040
goes all the way to make you see that is really big

73
00:03:39,050 --> 00:03:41,800
and then it goes back there

74
00:03:41,880 --> 00:03:44,040
and in this cable

75
00:03:44,040 --> 00:03:49,880
i'm going to send in a voltage pulse physical actual cable

76
00:03:49,940 --> 00:03:53,280
and correctional cable

77
00:03:53,300 --> 00:03:55,610
has the wire which is at the core

78
00:03:56,980 --> 00:03:59,540
and then there's this in and around it

79
00:03:59,590 --> 00:04:04,770
that's why it's called correctional this is conducted this conductor and in this field here

80
00:04:04,780 --> 00:04:07,500
with dielectric material to insulate

81
00:04:07,590 --> 00:04:09,270
the two conducting

82
00:04:11,980 --> 00:04:15,360
and what we're going to do we're going to send in your

83
00:04:15,440 --> 00:04:17,650
voltage pulses

84
00:04:17,710 --> 00:04:22,090
which have length of one hundred nanometers

85
00:04:22,110 --> 00:04:25,150
so in nano seconds

86
00:04:25,190 --> 00:04:30,500
one hundred times ten to the minus nine seconds hundred nano-second length of the

87
00:04:32,090 --> 00:04:35,300
in other words if you look at the wire from the side here is the

88
00:04:36,800 --> 00:04:39,460
and he is the shell

89
00:04:39,460 --> 00:04:42,650
just a cross-section is so you see only that show

90
00:04:43,650 --> 00:04:45,360
as the voltage falls

91
00:04:45,380 --> 00:04:47,380
moved in this direction

92
00:04:47,440 --> 00:04:49,650
you will see locally between

93
00:04:49,690 --> 00:04:54,710
the outer conductor and in conductor locally you'll see that if you

94
00:04:54,750 --> 00:04:57,780
goes to zero here and that marches then

95
00:04:57,800 --> 00:05:00,730
with a certain speed of light

96
00:05:00,780 --> 00:05:03,020
so as the whole passes

97
00:05:03,020 --> 00:05:05,650
there will be in the field like this

98
00:05:05,780 --> 00:05:08,860
then there is no poles there is no field between this and so in that

99
00:05:08,860 --> 00:05:13,090
sense it's sort of similar to this transmission line except that the geometry is different

100
00:05:13,090 --> 00:05:17,090
he is the collection cable

101
00:05:17,110 --> 00:05:18,840
now comes the

102
00:05:20,250 --> 00:05:23,960
i have an option to keep this and open

103
00:05:24,000 --> 00:05:25,150
so sort the line

104
00:05:25,150 --> 00:05:28,170
the very of the city and the idea is completely over

105
00:05:28,210 --> 00:05:29,570
and i have the option

106
00:05:29,590 --> 00:05:33,480
two shortcut

107
00:05:33,540 --> 00:05:36,920
and when i shorted out

108
00:05:38,150 --> 00:05:39,280
comes back

109
00:05:41,280 --> 00:05:45,320
now shorted out mountain comes back

110
00:05:45,380 --> 00:05:46,420
as the value

111
00:05:46,460 --> 00:05:50,460
when i leave it open month and comes back

112
00:05:50,480 --> 00:05:52,550
two months

113
00:05:52,570 --> 00:05:57,360
we're going to show you this poles at three locations

114
00:05:57,360 --> 00:05:58,360
today i will be

115
00:05:59,320 --> 00:06:00,400
talking about systems

116
00:06:01,010 --> 00:06:01,820
but before that's

117
00:06:02,390 --> 00:06:03,920
i'll try and tell you why you should care

118
00:06:07,450 --> 00:06:10,810
i call the scale out beyond mapreduce because big data

119
00:06:12,240 --> 00:06:14,590
it's going to be somewhat a buzzword i'm a little embarrassed

120
00:06:15,110 --> 00:06:16,350
specific data but

121
00:06:18,560 --> 00:06:19,770
this is work in

122
00:06:20,200 --> 00:06:22,360
that has been carried out with people on my

123
00:06:23,570 --> 00:06:24,510
team at microsoft

124
00:06:25,320 --> 00:06:28,100
the cloud and information services lab system

125
00:06:30,420 --> 00:06:31,710
the outline of this talk

126
00:06:32,150 --> 00:06:33,710
it's really up to par stock

127
00:06:35,790 --> 00:06:36,530
the first part

128
00:06:37,240 --> 00:06:39,670
i'll tell you about the kind of applications

129
00:06:40,410 --> 00:06:43,150
they gave rise to the technology be talking about

130
00:06:43,680 --> 00:06:46,880
at how this is now crossing the chasm from

131
00:06:48,040 --> 00:06:48,950
the web companies

132
00:06:50,050 --> 00:06:52,070
these kinds of scenarios first played out

133
00:06:53,010 --> 00:06:57,700
to enterprise companies ant scenarios that affect pretty much every individual

134
00:07:00,920 --> 00:07:03,310
i want to give you some broad architectural

135
00:07:06,300 --> 00:07:09,840
at without context and get the second part of my talk

136
00:07:10,450 --> 00:07:10,990
which is

137
00:07:11,530 --> 00:07:13,810
about a system called reef which we have

138
00:07:13,880 --> 00:07:17,080
working on land about to open source in the next month or so

139
00:07:17,920 --> 00:07:19,990
if you are interested in building the next mahood

140
00:07:21,180 --> 00:07:23,980
i strongly encourage you to download from consider

141
00:07:24,480 --> 00:07:25,300
building on top of it

142
00:07:27,010 --> 00:07:31,080
if you want to work with what many many petabytes you may actually find this useful

143
00:07:34,020 --> 00:07:40,740
of processes and i keep getting asked what as oh well applied research lab that i started at

144
00:07:41,240 --> 00:07:43,520
microsoft i got there about a year and a half ago

145
00:07:44,680 --> 00:07:45,060
it is

146
00:07:46,620 --> 00:07:50,470
an attempt to clone the yahoo applied research model which allowed

147
00:07:52,320 --> 00:07:56,160
the focus for the group this global practice platforms machine learning

148
00:07:56,750 --> 00:08:02,630
i both from a practical perspective andfrom data-driven enterprise applications

149
00:08:03,620 --> 00:08:05,110
we work closely with the product team

150
00:08:05,810 --> 00:08:08,140
in fact we are in the product side the house

151
00:08:10,390 --> 00:08:16,090
the group contains basically scientists and research software development engineers andy experiment in a balance between

152
00:08:16,530 --> 00:08:20,880
publications external research at internationally-backed it in

153
00:08:23,500 --> 00:08:24,460
let's go to the main topic

154
00:08:24,950 --> 00:08:26,320
why should you care about big data

155
00:08:28,580 --> 00:08:30,180
i think the technology secondary

156
00:08:30,590 --> 00:08:32,990
you should care about it because what we do with

157
00:08:34,410 --> 00:08:37,530
i give you a sense of this going forward

158
00:08:39,450 --> 00:08:40,930
what we are doing with it today

159
00:08:41,760 --> 00:08:45,230
is something that simply would not have been cost-effective or even feasible

160
00:08:46,470 --> 00:08:49,960
five ten years ago technology trends are playing a big role in making all this

161
00:08:50,400 --> 00:08:51,320
come together now

162
00:08:56,090 --> 00:08:57,400
other technical challenges

163
00:08:59,810 --> 00:09:02,540
and in fact i think there are significant technical advances

164
00:09:03,190 --> 00:09:05,050
if you will see the coming five ten years

165
00:09:05,750 --> 00:09:06,520
on the system size

166
00:09:08,560 --> 00:09:10,680
but frankly i don't think there are deal breakers you

167
00:09:11,710 --> 00:09:15,670
well i think there will be many difficult challenges and some really nice work

168
00:09:16,190 --> 00:09:17,910
they will have good the solutions

169
00:09:18,940 --> 00:09:22,540
it's also gonna need to address the social and legal issues

170
00:09:23,140 --> 00:09:26,180
these frankly gonna take much longer to really understand

171
00:09:27,510 --> 00:09:28,600
but i will talk about them to be

172
00:09:29,660 --> 00:09:30,550
the biggest gap before

173
00:09:31,110 --> 00:09:31,710
might be people

174
00:09:32,330 --> 00:09:32,830
people like you

175
00:09:34,790 --> 00:09:36,390
who understand how to

176
00:09:37,070 --> 00:09:37,950
uses these

177
00:09:38,770 --> 00:09:40,270
data analytics platforms

178
00:09:41,630 --> 00:09:43,610
i numerous groups have

179
00:09:44,180 --> 00:09:45,310
identified a shortage

180
00:09:45,780 --> 00:09:48,320
and the national academy of engineering cats committee

181
00:09:49,290 --> 00:09:53,820
of a channel part is planning the workshop full curriculum training

182
00:09:54,450 --> 00:09:55,550
talking about these issues

183
00:09:56,460 --> 00:09:57,040
in early

184
00:10:00,900 --> 00:10:02,260
so what's the big to do about it

185
00:10:02,680 --> 00:10:05,690
all start with one example drawn from a young days

186
00:10:08,240 --> 00:10:09,690
the idea of the web of

187
00:10:10,700 --> 00:10:12,560
concepts all the web of things

188
00:10:13,350 --> 00:10:13,980
is simply this

189
00:10:15,120 --> 00:10:16,590
people don't want to get back

190
00:10:17,870 --> 00:10:19,400
do you want elsewhere in this section

191
00:10:20,150 --> 00:10:22,920
right they think in terms of some real world concepts

192
00:10:23,540 --> 00:10:25,980
and those are the terms in which they want to see the results

193
00:10:26,520 --> 00:10:27,540
and you can do this

194
00:10:28,060 --> 00:10:31,400
if you crawl the web do information extraction at scale

195
00:10:31,960 --> 00:10:33,590
and organize the resulting data

196
00:10:35,320 --> 00:10:37,360
in an entry in an entity centric manner

197
00:10:37,860 --> 00:10:39,470
so when i search for say mumbai

198
00:10:40,470 --> 00:10:42,460
i don't want all pages that mention by

199
00:10:43,040 --> 00:10:44,280
i want everything

200
00:10:44,820 --> 00:10:48,660
i want to know about the city mumbai and they expect to to recognize

201
00:10:49,320 --> 00:10:50,630
but there is an entity called by

202
00:10:51,030 --> 00:10:52,880
and that information is find out

203
00:10:57,160 --> 00:10:57,830
here's an example

204
00:10:59,040 --> 00:10:59,970
if you search

205
00:11:03,100 --> 00:11:03,730
ann arbor

206
00:11:04,560 --> 00:11:07,030
you'll find a map and most restaurants

207
00:11:08,790 --> 00:11:09,550
quite commentary

208
00:11:10,430 --> 00:11:11,810
the sort of thing here do not

209
00:11:13,280 --> 00:11:15,700
this was done by classifying web pages

210
00:11:16,230 --> 00:11:17,790
at restaurant web pages

211
00:11:18,370 --> 00:11:21,710
inside those domains menu pages as many pages

212
00:11:22,320 --> 00:11:27,550
then extracting menu items from those menu pages and associating them the entity

213
00:11:28,300 --> 00:11:28,610
the rest

214
00:11:29,360 --> 00:11:30,640
right that particular restaurant

215
00:11:31,170 --> 00:11:33,790
and now when someone searches for all really an hour

216
00:11:34,960 --> 00:11:38,400
you'll only see those restaurants that actually serve daily

217
00:11:40,490 --> 00:11:41,360
a south indian dish

218
00:11:43,090 --> 00:11:43,700
in the restaurant

219
00:11:44,490 --> 00:11:46,570
a funny story once driving through chicago

220
00:11:47,240 --> 00:11:50,320
i promised my kids those which is a south indian classical

221
00:11:51,190 --> 00:11:52,820
i put out my trusty

222
00:11:53,220 --> 00:11:53,860
yahoo search

223
00:11:55,890 --> 00:11:57,970
and it came up with a restaurant called taj mahal

224
00:11:58,890 --> 00:12:01,590
my being smart so that's not rest

225
00:12:02,050 --> 00:12:02,760
you get those

226
00:12:03,730 --> 00:12:04,690
as an are

227
00:12:05,300 --> 00:12:06,780
and i extraction works

228
00:12:07,170 --> 00:12:08,510
so went and ordered

229
00:12:10,590 --> 00:12:11,210
or try to

230
00:12:12,370 --> 00:12:13,210
it wasn't on the menu

231
00:12:14,450 --> 00:12:17,150
so this is really embarrassing all the way out said

232
00:12:17,150 --> 00:12:22,170
described systems that do that are coming up here

233
00:12:22,220 --> 00:12:27,030
OK and but course as we know it we can allow features the slide all

234
00:12:27,320 --> 00:12:28,630
place but we don't know

235
00:12:28,680 --> 00:12:31,400
how accurate those are when done

236
00:12:34,100 --> 00:12:39,250
training is this right from the very beginning this is a rather ill-defined problem

237
00:12:40,670 --> 00:12:46,830
we recollect that from our discussion of quantal theory this notion that there's all these

238
00:12:46,830 --> 00:12:54,360
sort of nonlinear discontinuity isn't going from production articulation to the acoustics and acoustic perception

239
00:12:54,370 --> 00:12:56,160
so we know that

240
00:12:56,170 --> 00:13:00,450
these articulatory distances and acoustic distances can be really nonlinear

241
00:13:00,470 --> 00:13:05,670
and really there's only small regions of the acoustic space correspond to regions of high

242
00:13:05,690 --> 00:13:11,350
articulatory discriminability that is that there very sort of that we can think of there

243
00:13:11,350 --> 00:13:18,420
being these decision boundaries that are fairly sort of narrow things within within our space

244
00:13:18,430 --> 00:13:19,500
that actually

245
00:13:19,510 --> 00:13:21,320
is it is an nice

246
00:13:21,330 --> 00:13:26,630
a problem for support vector machines and so we see all support vector machines being

247
00:13:26,630 --> 00:13:28,500
used a lot for this problem

248
00:13:28,520 --> 00:13:30,390
but more generally

249
00:13:30,400 --> 00:13:33,310
i think we see a lot of examples where

250
00:13:33,320 --> 00:13:35,350
people have applied

251
00:13:36,960 --> 00:13:43,560
the learning algorithms support vector machines time delay neural networks multi layer perceptrons

252
00:13:45,490 --> 00:13:47,950
it's really difficult to say

253
00:13:47,960 --> 00:13:54,140
from from there exist from one of the problems is is all these people are

254
00:13:54,140 --> 00:13:56,210
not working on a common task

255
00:13:56,220 --> 00:14:03,230
and i think you could also say that it's very so it's very difficult to

256
00:14:03,230 --> 00:14:08,070
compare their results but in general you find that you can

257
00:14:08,250 --> 00:14:13,800
four four depending on the feature system that you can do a reasonable job get

258
00:14:14,160 --> 00:14:17,680
anywhere depending for particular for this

259
00:14:17,690 --> 00:14:23,010
feature system here depending on the feature you can get anywhere from say seventy five

260
00:14:23,630 --> 00:14:26,660
ninety five percent accuracy in feature detection

261
00:14:27,300 --> 00:14:29,020
and is that good enough

262
00:14:29,780 --> 00:14:33,340
i think you certainly in experiments that we don't we see that as we do

263
00:14:33,340 --> 00:14:37,630
better and better in designing the feature detectors are overall system gets better and better

264
00:14:37,640 --> 00:14:41,280
in other words the system that might actually do word recognition

265
00:14:43,020 --> 00:14:46,060
in a way that's driven by these features

266
00:14:47,340 --> 00:14:50,440
it's not clear what is what is good enough

267
00:14:51,660 --> 00:14:58,650
i think i mean it's also not clear exactly exactly how wonderful would properly define

268
00:14:58,650 --> 00:15:01,340
the task to see how well you doing but

269
00:15:01,380 --> 00:15:10,540
you get the given the scenarios that i described basic neural network detectors can do

270
00:15:10,540 --> 00:15:11,870
reasonably well

271
00:15:13,530 --> 00:15:15,400
i mentioned this issue

272
00:15:16,170 --> 00:15:20,410
embedded training and allowing for asynchrony

273
00:15:20,460 --> 00:15:25,730
among distinct distinctive features of was talking to minutes

274
00:15:25,770 --> 00:15:32,160
about how we can use models of vocal tract dynamics to allow for describing sort

275
00:15:32,160 --> 00:15:44,000
of these loosely synchronized trajectories of the articulators in that's again something as difficult to

276
00:15:44,000 --> 00:15:47,950
use the internet ASR engine but we can build these kinds of miles

277
00:15:47,960 --> 00:15:49,440
but for now

278
00:15:49,450 --> 00:15:56,190
i think the most interesting work that's been done is in a really compelling formalism

279
00:15:56,410 --> 00:16:01,870
to use here is is is bayesian networks

280
00:16:02,420 --> 00:16:06,380
and we see this has been done investigated by number of people

281
00:16:06,840 --> 00:16:15,390
but in general a bayesian network is a special kind of directed case asynchronous graph

282
00:16:15,510 --> 00:16:20,830
that allows us to describe dependencies in in a very general way

283
00:16:21,080 --> 00:16:27,150
in particular what i've taken a figure here from a paper by jill franklyn

284
00:16:27,200 --> 00:16:28,980
he has

285
00:16:31,310 --> 00:16:37,810
a set of dependencies in this in this graph where for example

286
00:16:37,860 --> 00:16:41,540
acoustic variables which we assume are these continuous

287
00:16:43,520 --> 00:16:48,450
things where they might be of feature vectors of any kind

288
00:16:49,700 --> 00:16:51,180
depend on

289
00:16:51,190 --> 00:16:53,060
these discrete

290
00:16:53,070 --> 00:16:54,500
he hidden

291
00:16:54,550 --> 00:17:00,130
distinctive feature variables which are represented here these sort of open nodes

292
00:17:00,140 --> 00:17:05,480
and each of these discrete hidden variables will basically correspond to

293
00:17:05,980 --> 00:17:08,310
the distinctive features

294
00:17:10,570 --> 00:17:13,310
and so we see that

295
00:17:13,360 --> 00:17:21,800
we can describe the dependencies amongst our observable variables and are hidden feature variables in

296
00:17:21,800 --> 00:17:23,660
terms of the edges

297
00:17:23,670 --> 00:17:25,580
of this graph

298
00:17:26,590 --> 00:17:31,330
they basically along each edge we have the conditional probability

299
00:17:31,500 --> 00:17:34,000
and so we we have away then

300
00:17:34,010 --> 00:17:40,390
apriori describing the relationships amongst are distinctive features

301
00:17:40,410 --> 00:17:43,210
and basically we can do that by

302
00:17:43,210 --> 00:17:46,630
by establishing

303
00:17:48,500 --> 00:17:55,220
the the the fourth of this graph in terms of the allowable edges here that

304
00:17:55,220 --> 00:18:00,250
that are considered significant and you can see for example here or the

305
00:18:00,740 --> 00:18:05,550
all the distinctive features have the dependency on the manor class

306
00:18:06,830 --> 00:18:10,780
you know if if you're if you're an HMM person we could think of these

307
00:18:10,800 --> 00:18:13,400
these variables is basically hidden states

308
00:18:14,190 --> 00:18:15,150
these are the

309
00:18:15,420 --> 00:18:21,050
observations that we have whatever they might be there might be are they might be

310
00:18:23,100 --> 00:18:26,310
we can think of the dependencies between

311
00:18:26,390 --> 00:18:33,390
these observable acoustic variables and these hidden states as being described by

312
00:18:33,410 --> 00:18:34,940
garcia mixtures

313
00:18:34,950 --> 00:18:36,070
if you like those

314
00:18:36,080 --> 00:18:37,900
if you're HMM person

315
00:18:37,920 --> 00:18:40,870
and we can

316
00:18:40,900 --> 00:18:46,920
a bit the dependencies between the states as being very much markovian right this is

317
00:18:46,960 --> 00:18:52,220
the notion of a dynamic bayesian network is one that has these time dependencies and

318
00:18:52,270 --> 00:18:56,150
that's what distinguishes the bayes network from dynamic bayes network

319
00:18:56,170 --> 00:19:01,410
and you're getting a hideous introduction to bayes networks is that's what you if if

320
00:19:01,630 --> 00:19:08,280
if you need that night there's a lot of excellent tutorial references for bayes nets

321
00:19:08,710 --> 00:19:11,090
that you could be referred to

322
00:19:11,140 --> 00:19:12,680
but anyway

323
00:19:12,700 --> 00:19:16,640
so we have this markovian dependency and the previous time

324
00:19:16,690 --> 00:19:24,160
and we are the system designer is basically has the freedom to establish what dependencies

325
00:19:24,410 --> 00:19:28,820
they think are significant between the the state variables

326
00:19:28,830 --> 00:19:30,360
OK so

327
00:19:33,680 --> 00:19:35,140
questions about our

328
00:19:35,140 --> 00:19:38,570
for one thing i

329
00:19:38,790 --> 00:19:41,560
so i good even if you don't understand efficient

330
00:19:41,700 --> 00:19:43,710
quezon ship should tell you

331
00:19:45,870 --> 00:19:47,370
the classi i

332
00:19:47,420 --> 00:19:50,440
so the point is when i hash forty nine

333
00:19:50,490 --> 00:19:54,800
should forty nine produces me some index on the table say i

334
00:19:54,810 --> 00:19:56,190
OK and everything

335
00:19:56,200 --> 00:19:58,920
hashes to that same location

336
00:19:59,030 --> 00:20:02,440
is linked together into west

337
00:20:02,450 --> 00:20:05,290
every record

338
00:20:05,300 --> 00:20:08,850
any questions about what the mechanics of this i hope that most of you have

339
00:20:08,850 --> 00:20:09,800
seen this

340
00:20:09,890 --> 00:20:12,070
scene hashing basic action

341
00:20:12,080 --> 00:20:17,880
in six one right teachers sixty institutions external one

342
00:20:17,920 --> 00:20:19,430
some piercings

343
00:20:21,810 --> 00:20:23,200
is to teach

344
00:20:23,460 --> 00:20:26,130
so what

345
00:20:26,170 --> 00:20:29,670
so i realized this

346
00:20:30,670 --> 00:20:32,360
this strategy

347
00:20:35,370 --> 00:20:39,800
the first two worst case

348
00:20:39,980 --> 00:20:47,070
so what happens in the worst case

349
00:20:47,090 --> 00:20:50,920
with passion

350
00:20:51,030 --> 00:20:54,900
narrator and so i could call

351
00:20:55,060 --> 00:20:57,640
all as he's

352
00:21:01,780 --> 00:21:04,290
all the keys in s

353
00:21:04,300 --> 00:21:06,500
i happened pick because as

354
00:21:06,570 --> 00:21:09,340
where my hash function happens to match

355
00:21:09,360 --> 00:21:11,570
mapped them all the same value

356
00:21:11,610 --> 00:21:13,400
that would be that OK

357
00:21:13,420 --> 00:21:17,360
every key

358
00:21:20,550 --> 00:21:26,080
two the the same slot

359
00:21:26,260 --> 00:21:30,300
so therefore if that happens

360
00:21:30,340 --> 00:21:35,650
then what is actually built is a fancy linked list

361
00:21:35,770 --> 00:21:37,530
keeping this structure

362
00:21:37,580 --> 00:21:40,610
OK all this stuff to the table the hashing et cetera

363
00:21:41,580 --> 00:21:43,220
all that matters is

364
00:21:43,250 --> 00:21:45,250
i have a long link lists

365
00:21:46,170 --> 00:21:48,580
then how long is an access take

366
00:21:48,640 --> 00:21:51,090
i want to take me to insert something

367
00:21:51,100 --> 00:21:56,300
or more importantly that for example search for something find out whether something

368
00:21:56,320 --> 00:21:58,850
in the worst case

369
00:21:58,920 --> 00:22:00,900
it takes or in time

370
00:22:01,010 --> 00:22:05,450
because they're all just to link to someone less access take

371
00:22:12,510 --> 00:22:18,830
we assume the size of this is called

372
00:22:18,850 --> 00:22:24,060
the worst case twenty this doesn't work so

373
00:22:24,110 --> 00:22:25,640
so attractor

374
00:22:26,730 --> 00:22:27,840
will see

375
00:22:27,870 --> 00:22:32,140
data structures in worst-case do very well for this problem

376
00:22:33,970 --> 00:22:37,030
they don't do as good as the average case the passion

377
00:22:37,070 --> 00:22:44,480
so let's analyse the average case

378
00:22:44,520 --> 00:22:47,880
or in the average case

379
00:22:47,970 --> 00:22:53,380
after whatever you averages one probability have to state your assumptions

380
00:22:54,140 --> 00:22:55,130
have to say

381
00:22:55,130 --> 00:22:57,110
selection bias doesn't hurt me

382
00:22:57,150 --> 00:23:01,800
the causes i'm able to learn i can store

383
00:23:01,840 --> 00:23:06,520
it was it's not like i don't make that assumption you know like something almost

384
00:23:06,520 --> 00:23:10,530
to determine out doesn't hurt me

385
00:23:10,570 --> 00:23:13,680
so what can we conclude here that with

386
00:23:13,700 --> 00:23:17,220
if we only make embedded faithfulness assumption

387
00:23:17,240 --> 00:23:17,950
and you

388
00:23:17,970 --> 00:23:22,840
crosses we wanna find even if there's hidden common causes even if this selection bias

389
00:23:22,970 --> 00:23:27,570
the only thing that can really confounders is

390
00:23:27,570 --> 00:23:30,950
the walls at things like the finasteride DHT

391
00:23:30,950 --> 00:23:34,240
so there's not too much that can confound you but there is one thing in

392
00:23:34,240 --> 00:23:36,130
my case i think that's what happened

393
00:23:37,450 --> 00:23:41,030
morrison myself in that study

394
00:23:41,130 --> 00:23:44,820
here's causal learning with temporal information

395
00:23:44,840 --> 00:23:49,220
four we probably have time for this like i something at the end of causal

396
00:23:49,220 --> 00:23:51,400
learning based on two variables

397
00:23:51,450 --> 00:23:54,130
but i don't think it's a good idea to ever talk to do which is

398
00:23:54,130 --> 00:23:55,820
getting fatigued and

399
00:23:57,200 --> 00:24:03,050
several talk a for more than an hour because they get the team

400
00:24:04,380 --> 00:24:07,860
this gives us all the more time so i'm going to cover this

401
00:24:07,880 --> 00:24:10,090
number a unique for variables

402
00:24:10,110 --> 00:24:13,360
but if you have some temporal information

403
00:24:13,360 --> 00:24:15,860
suppose i have three variables

404
00:24:15,860 --> 00:24:17,340
x y and c

405
00:24:17,360 --> 00:24:21,300
and that's why are independent conditional on c

406
00:24:21,320 --> 00:24:22,840
number before

407
00:24:22,840 --> 00:24:30,780
with embedded faithfulness assumption we can not learn any causal influences

408
00:24:30,880 --> 00:24:34,510
assuming embedded faithfulness we have to have these lines

409
00:24:34,530 --> 00:24:39,530
now suppose there can be no causal path from x to the

410
00:24:39,590 --> 00:24:43,010
because we know that precedes the time

411
00:24:43,050 --> 00:24:49,700
it is only by temporal information they you know the temporal ordering of some variables

412
00:24:49,760 --> 00:24:53,320
i think that's always came before in

413
00:24:53,320 --> 00:24:54,300
in any

414
00:24:57,420 --> 00:25:01,990
the link between text and c must be either that in b or c effects

415
00:25:01,990 --> 00:25:03,820
precedes the

416
00:25:03,900 --> 00:25:06,220
every element of a property

417
00:25:06,630 --> 00:25:12,170
population either they have a hidden common cause that causes the we assume

418
00:25:12,240 --> 00:25:18,220
again this this body here i'm not a philosopher they've been a philosopher

419
00:25:18,300 --> 00:25:18,880
o for

420
00:25:18,880 --> 00:25:19,950
no philosophers

421
00:25:19,950 --> 00:25:21,510
i because

422
00:25:21,530 --> 00:25:23,130
five back in possibilities

423
00:25:23,170 --> 00:25:27,780
many on list because such that it's only reasonable to assume that something precedes something

424
00:25:27,820 --> 00:25:28,800
some time

425
00:25:28,800 --> 00:25:31,360
then in the can cause acts

426
00:25:32,570 --> 00:25:38,840
the causes precede the effects of these are the only possibilities

427
00:25:38,880 --> 00:25:47,050
so the causal DAG must be either that in d or e

428
00:25:48,180 --> 00:25:50,220
of the arguments i gave before

429
00:25:50,220 --> 00:25:51,470
is an edge

430
00:25:51,630 --> 00:25:54,900
this was the situation if this and went this way

431
00:25:54,950 --> 00:25:57,320
with x and y are independent

432
00:25:57,450 --> 00:26:00,170
that's not what we observe

433
00:26:00,180 --> 00:26:01,380
if this was

434
00:26:01,380 --> 00:26:06,440
there again this one this way with h and y are independent which due to

435
00:26:06,470 --> 00:26:10,740
the separation these acts my independent and it's now we observed

436
00:26:10,840 --> 00:26:15,090
so see if you have travel information then you can learn

437
00:26:15,150 --> 00:26:17,360
data from very

438
00:26:17,510 --> 00:26:19,880
three variables

439
00:26:19,880 --> 00:26:24,700
actually it depends on have my two thousand four book

440
00:26:24,720 --> 00:26:29,570
all the cartoon i haven't beginning but that's as an example of how this can

441
00:26:29,570 --> 00:26:34,090
be and i want to get that picture but i can't find it anymore

442
00:26:34,150 --> 00:26:38,070
this is the stuff of learning causes from data on two variables

443
00:26:38,090 --> 00:26:39,840
so it

444
00:26:39,900 --> 00:26:41,860
pressure cover the

445
00:26:41,920 --> 00:26:46,340
because of the fact that people are going to see the whole concept it turns

446
00:26:46,340 --> 00:26:49,650
out we can learn a little bit about causes from data on two variables almost

447
00:26:49,650 --> 00:26:53,570
like a light weight and you can you can learn much but i take you

448
00:26:53,570 --> 00:26:54,740
through this stuff

449
00:26:54,860 --> 00:26:57,880
of taking through something and then at the end up going to let you know

450
00:26:57,880 --> 00:27:00,860
that it's not of much value

451
00:27:00,950 --> 00:27:02,880
do you want it

452
00:27:02,900 --> 00:27:05,490
if you want to see it

453
00:27:05,510 --> 00:27:08,880
so do until too

454
00:27:14,820 --> 00:27:18,810
one way to learn dead structure from data i mentioned this earlier i i talked

455
00:27:18,810 --> 00:27:21,550
about the constraint based method but another way

456
00:27:21,610 --> 00:27:24,650
which in some sense is powerful scoring

457
00:27:24,670 --> 00:27:26,010
the DAG

458
00:27:26,010 --> 00:27:28,490
now the woods before i i took the data

459
00:27:28,550 --> 00:27:32,010
i want conditional independencies in front of my learning

460
00:27:32,010 --> 00:27:36,590
the structure here i take the data and i scored again i'm not going to

461
00:27:36,590 --> 00:27:39,780
go into the ways of scoring at a simple

462
00:27:39,800 --> 00:27:44,590
simple score compute the probability of the data conditional on the DAG

463
00:27:44,630 --> 00:27:48,670
given this tag what's the probability of this data we sum over all values of

464
00:27:48,680 --> 00:27:51,180
the parameters of the DAG

465
00:27:51,200 --> 00:27:53,610
and then compute the probability

466
00:27:53,630 --> 00:27:55,380
the more likely

467
00:27:55,400 --> 00:27:59,630
the data given that the higher the score today arguments then you choose

468
00:27:59,740 --> 00:28:04,240
right that that's the alternative way of learning structure i say it's not intuitive like

469
00:28:04,240 --> 00:28:07,150
the other way but it is another way

470
00:28:07,170 --> 00:28:13,050
for certain classes of models a smallest that includes the probability distribution will be chosen

471
00:28:13,050 --> 00:28:16,070
when the data set is large

472
00:28:16,110 --> 00:28:20,030
now what do i mean by the smallest DAG that includes the probability distribution these

473
00:28:20,030 --> 00:28:21,680
two concepts here

474
00:28:21,700 --> 00:28:24,880
but the concept of a small band

475
00:28:24,920 --> 00:28:30,070
product concept of including the probability distribution

476
00:28:30,130 --> 00:28:33,380
and i'll show you what i mean by this with

477
00:28:33,400 --> 00:28:35,820
a couple of examples

478
00:28:37,360 --> 00:28:39,030
we have two variables

479
00:28:39,050 --> 00:28:43,220
both binary and we find that they are independent

480
00:28:43,280 --> 00:28:46,240
these are two models we could have

481
00:28:51,070 --> 00:28:54,090
kind on reverse here i

482
00:28:54,200 --> 00:28:58,550
this is the conditional independence the but it's not something we learned from data and

483
00:28:58,570 --> 00:29:00,920
i'm trying to tell you that this is what's true

484
00:29:00,990 --> 00:29:01,970
all right

485
00:29:01,990 --> 00:29:03,840
if you have two variables

486
00:29:03,840 --> 00:29:05,150
what are

487
00:29:05,200 --> 00:29:09,530
the possible the first of all binary variables you they could be

488
00:29:10,760 --> 00:29:13,130
and and what parameters you need

489
00:29:13,180 --> 00:29:17,900
you need the probability of axis forces value probably wise first value

490
00:29:17,900 --> 00:29:19,970
so in binary

491
00:29:20,010 --> 00:29:23,450
you don't need any other probabilities could probably of x two will be one minus

492
00:29:23,610 --> 00:29:24,950
the next one

493
00:29:24,970 --> 00:29:28,470
so those are the only two parameters you need

494
00:29:28,510 --> 00:29:30,950
if you've got this DAG

495
00:29:30,970 --> 00:29:33,260
you need the probability that x one

496
00:29:33,280 --> 00:29:36,260
y one given x one y one given x two

497
00:29:36,340 --> 00:29:38,280
need three parameters

498
00:29:38,340 --> 00:29:41,070
right so these are the two possible

499
00:29:41,130 --> 00:29:45,300
models there's another y going that's but that's markov equivalence

500
00:29:45,320 --> 00:29:47,260
two possible their models

501
00:29:47,880 --> 00:29:50,990
two binary variables

502
00:29:50,990 --> 00:29:52,150
now suppose

503
00:29:52,170 --> 00:29:53,740
in our data

504
00:29:53,740 --> 00:29:56,280
because of the dependency

505
00:29:56,320 --> 00:30:00,460
on the past values of y t

506
00:30:00,470 --> 00:30:04,880
but this is not relevant because we only use

507
00:30:04,890 --> 00:30:07,400
the markov chain xt

508
00:30:08,500 --> 00:30:10,690
the reason why it works is is

509
00:30:10,730 --> 00:30:11,850
is really

510
00:30:13,970 --> 00:30:17,440
is this equation

511
00:30:17,460 --> 00:30:19,930
if you write down the transition kernel

512
00:30:20,610 --> 00:30:22,270
this occurs

513
00:30:22,290 --> 00:30:25,320
so either you move used to you stay where you are

514
00:30:25,330 --> 00:30:28,450
well this perfect symmetry

515
00:30:28,470 --> 00:30:31,990
identity that is if you take full football

516
00:30:32,010 --> 00:30:34,480
and you multiply the come from y

517
00:30:35,660 --> 00:30:38,700
and actually on the by my support

518
00:30:38,710 --> 00:30:41,880
that was because the the state i

519
00:30:41,900 --> 00:30:46,010
everything is x x x x x if you move

520
00:30:46,260 --> 00:30:50,560
you have y the probability of moving which is rho

521
00:30:51,760 --> 00:30:56,280
the distribution that you chose to remove q of y given x one can see

522
00:30:56,280 --> 00:30:59,710
that f of one row of y axis

523
00:30:59,760 --> 00:31:04,260
i q i given y is exactly equal to f of x

524
00:31:04,330 --> 00:31:07,400
the role of x y q y given x

525
00:31:07,410 --> 00:31:08,150
so you

526
00:31:09,270 --> 00:31:10,600
in x and y

527
00:31:10,620 --> 00:31:13,710
in the joint distribution of the current value

528
00:31:13,730 --> 00:31:15,840
and the next value

529
00:31:16,150 --> 00:31:21,300
and because of this perfect symmetry which is called detailed balance your chain is reversible

530
00:31:21,320 --> 00:31:24,730
that is if you look at it one way or another

531
00:31:24,750 --> 00:31:26,500
it is the same

532
00:31:26,510 --> 00:31:29,730
time has no direction in that in that case

533
00:31:29,740 --> 00:31:31,290
and more importantly

534
00:31:31,310 --> 00:31:34,710
you just have to integrate both sides of this equation

535
00:31:34,720 --> 00:31:38,730
to realize that if you start from f of why you do one step at

536
00:31:38,730 --> 00:31:41,710
x is again distributed as an

537
00:31:41,790 --> 00:31:43,790
therefore f is really

538
00:31:43,810 --> 00:31:47,180
xe stationary distribution of that ship

539
00:31:47,200 --> 00:31:48,690
thank OK

540
00:31:48,860 --> 00:31:54,760
now of course you have not going to change you just need to have one

541
00:31:54,760 --> 00:31:58,290
more condition which is that the james be irreducible

542
00:31:58,300 --> 00:31:59,620
and if you do that

543
00:31:59,630 --> 00:32:01,510
and the detailed balance condition

544
00:32:01,520 --> 00:32:05,650
that if you can move anywhere in a finite number of steps

545
00:32:05,670 --> 00:32:06,690
that's enough

546
00:32:06,710 --> 00:32:09,240
to induce positive recurrent

547
00:32:09,250 --> 00:32:10,670
well your change

548
00:32:10,680 --> 00:32:12,130
because they don't need much

549
00:32:12,150 --> 00:32:14,690
in terms of the camels the kernel

550
00:32:15,490 --> 00:32:17,940
q three q proposal

551
00:32:19,550 --> 00:32:22,740
spectrum of spread that is large enough

552
00:32:22,790 --> 00:32:23,590
two goals

553
00:32:24,990 --> 00:32:29,710
you know have the number of steps that besides condition thank if you choose q

554
00:32:29,710 --> 00:32:30,030
to be

555
00:32:30,410 --> 00:32:34,240
a positive density in particular were

556
00:32:35,180 --> 00:32:41,410
you don't your change is good and therefore it's always converging to a stationary distribution

557
00:32:41,410 --> 00:32:46,980
so there's really universal algorithm because you just no condition

558
00:32:47,000 --> 00:32:48,590
two impose on

559
00:32:48,610 --> 00:32:51,930
on your parameters you going to

560
00:32:53,400 --> 00:32:57,180
the point is not really important periodicity city

561
00:32:57,240 --> 00:33:00,130
is just a nuisance but it's not

562
00:33:00,150 --> 00:33:03,320
it's a minor nuisance

563
00:33:07,350 --> 00:33:12,540
so there is a big collision friends and this say for instance to impose this

564
00:33:12,540 --> 00:33:15,080
is much too strong but you can impose

565
00:33:15,110 --> 00:33:16,290
that the camel

566
00:33:16,640 --> 00:33:21,570
is positive everyone but if it even if it is positive in the neighborhood

567
00:33:21,580 --> 00:33:24,580
which large enough diameter

568
00:33:24,620 --> 00:33:25,810
that's not

569
00:33:26,710 --> 00:33:28,340
and once you have that

570
00:33:28,360 --> 00:33:36,180
harris recurrence and therefore you have the two theorems already yesterday basic convergence properties that

571
00:33:36,180 --> 00:33:38,080
are empirical mean

572
00:33:39,160 --> 00:33:41,750
two is the integral of interest or

573
00:33:41,760 --> 00:33:44,610
the distribution of the change time

574
00:33:45,210 --> 00:33:49,330
converges to f which is the distribution of interest

575
00:33:49,350 --> 00:33:55,700
because of the whole theory of metropolis hastings algorithm in this line that's that's all

576
00:33:55,700 --> 00:33:56,480
you need

577
00:33:56,530 --> 00:33:59,590
two know to be convinced that it works

578
00:34:01,370 --> 00:34:04,680
so that for theoretical convergence of course in practice

579
00:34:04,690 --> 00:34:05,790
we get

580
00:34:05,810 --> 00:34:07,210
different behaviours

581
00:34:07,230 --> 00:34:08,680
depending on the choice

582
00:34:08,700 --> 00:34:09,980
of q

583
00:34:12,640 --> 00:34:14,390
i just go through

584
00:34:14,470 --> 00:34:15,990
a few examples

585
00:34:16,010 --> 00:34:17,080
to show you

586
00:34:17,100 --> 00:34:18,360
how it works

587
00:34:19,470 --> 00:34:20,770
or less

588
00:34:22,450 --> 00:34:26,930
and i start with the first choice of q

589
00:34:26,940 --> 00:34:29,150
by choosing q which is not

590
00:34:29,200 --> 00:34:30,850
the conditional distribution

591
00:34:31,070 --> 00:34:34,300
may well choose a decision to

592
00:34:34,320 --> 00:34:39,100
that only depend on the current value to which killed y given x is q

593
00:34:39,100 --> 00:34:40,020
of one

594
00:34:40,710 --> 00:34:43,810
therefore i write g

595
00:34:45,680 --> 00:34:51,380
in the standard simulation algorithm where i picked the wrong distribution they're correct to get

596
00:34:51,390 --> 00:34:52,730
to the right distribution

597
00:34:53,670 --> 00:34:55,840
so the idea that the algorithm

598
00:34:55,850 --> 00:35:00,380
you just chances are good that is at any point you generate value

599
00:35:00,400 --> 00:35:03,200
always from the same distribution g

600
00:35:03,260 --> 00:35:06,870
and you can get from using the wrong distribution

601
00:35:06,890 --> 00:35:09,040
by using the ratio

602
00:35:09,050 --> 00:35:10,320
of the gene

603
00:35:10,380 --> 00:35:16,070
you see that they acceptance probability is fg off the new value or

604
00:35:16,120 --> 00:35:17,050
f of g

605
00:35:17,750 --> 00:35:20,300
the previous value and so if

606
00:35:20,320 --> 00:35:21,350
four g

607
00:35:22,500 --> 00:35:24,650
you accept the new

608
00:35:24,670 --> 00:35:26,600
and you stay there

609
00:35:26,620 --> 00:35:27,940
it is the time that is

610
00:35:27,960 --> 00:35:29,710
propositional to f

611
00:35:29,720 --> 00:35:30,700
of the g

612
00:35:30,720 --> 00:35:32,700
of the current value

613
00:35:35,770 --> 00:35:40,110
but of course we are using a GP that produces a sequence of i eighty

614
00:35:40,110 --> 00:35:41,950
that is but because we pick

615
00:35:42,780 --> 00:35:44,290
and reject all others

616
00:35:44,970 --> 00:35:48,180
why we reject some values we stay

617
00:35:48,320 --> 00:35:53,850
the same values again sample we produce is a markov chain is not i i

618
00:35:56,180 --> 00:36:03,420
strong connection with actor project as i hinted at yesterday which is that if

619
00:36:03,430 --> 00:36:05,350
user gene that is

620
00:36:05,360 --> 00:36:06,620
fat tails

621
00:36:06,630 --> 00:36:08,150
so that ever g

622
00:36:08,180 --> 00:36:09,560
is bounded

623
00:36:09,580 --> 00:36:12,250
if at knowledge is bounded you get

624
00:36:13,810 --> 00:36:15,920
good city

625
00:36:15,940 --> 00:36:18,920
in the convergence that is your kernel

626
00:36:18,930 --> 00:36:20,350
at the end

627
00:36:20,370 --> 00:36:22,480
is close to

628
00:36:23,240 --> 00:36:27,900
by a factor of one minus one over and to the poor and you have

629
00:36:27,910 --> 00:36:30,470
geometric degrees two

630
00:36:31,750 --> 00:36:35,840
that is not all depending on the starting value

631
00:36:36,930 --> 00:36:39,080
OK so that's not system

632
00:36:39,090 --> 00:36:45,790
of because it's related to be the same properties of facts project work in that

633
00:36:45,790 --> 00:36:47,960
case but again

634
00:36:47,970 --> 00:36:50,560
it we that project

635
00:36:50,580 --> 00:36:54,830
the priority of acceptance in one step is one of our hands

636
00:36:54,850 --> 00:36:56,600
and the first thing here is that

637
00:36:56,620 --> 00:36:58,950
the problem of acceptance is larger

638
00:36:59,080 --> 00:37:03,010
and one of them and you can see very easily

639
00:37:03,020 --> 00:37:04,840
because when we

640
00:37:05,870 --> 00:37:10,170
this primitive acceptance it's a YT he would you y

641
00:37:10,210 --> 00:37:11,450
and you divide

642
00:37:11,470 --> 00:37:17,240
by f of xt overdue of so f of XTO energy efficiency is less

643
00:37:17,250 --> 00:37:18,360
then and

644
00:37:18,370 --> 00:37:22,900
and therefore you divide by something that is smaller than n the probability to accept

645
00:37:22,900 --> 00:37:24,640
is large

646
00:37:24,650 --> 00:37:26,520
OK so you move faster

647
00:37:26,530 --> 00:37:29,410
with this algorithm then was accepted

648
00:37:29,430 --> 00:37:31,630
then we accept reject

649
00:37:34,370 --> 00:37:39,920
just remark which is that

650
00:37:39,920 --> 00:37:43,040
martin going to talk about

651
00:37:43,130 --> 00:37:48,070
semantic web services which is a little different to the other areas

652
00:37:48,130 --> 00:37:50,140
there is an acknowledgement

653
00:37:50,500 --> 00:37:52,830
i won't go through them all

654
00:37:52,860 --> 00:37:55,200
so what's the web service

655
00:37:55,210 --> 00:38:00,540
here we're basically talking about programming so whereas

656
00:38:00,560 --> 00:38:03,540
several the other presentations focused on data

657
00:38:03,560 --> 00:38:05,740
here we focus on function

658
00:38:05,780 --> 00:38:09,650
so it's the program programmatically accessible over the web

659
00:38:09,670 --> 00:38:13,290
and we're talking about functionalities which are provided

660
00:38:13,410 --> 00:38:16,390
in a distributed fashion

661
00:38:16,440 --> 00:38:21,950
if you ever go to the web services presentation the first slide show is this

662
00:38:22,570 --> 00:38:28,160
so we have two roles you have service consumer so some client which may be

663
00:38:28,160 --> 00:38:31,540
a programmed wants to consumer service and then

664
00:38:31,550 --> 00:38:35,760
some organisation that provides functionality on the web

665
00:38:36,580 --> 00:38:41,610
can seem to find the appropriate service in something called the UDDI registry

666
00:38:41,620 --> 00:38:47,950
which is the syntactic registry of services they have a textual description and some XML

667
00:38:49,140 --> 00:38:50,840
two i was still file

668
00:38:50,890 --> 00:38:56,280
but still is the syntactic description of the components of the service

669
00:38:56,280 --> 00:38:57,920
basically the

670
00:38:57,940 --> 00:39:03,730
types in the service and the data schema

671
00:39:03,750 --> 00:39:05,360
once the

672
00:39:05,580 --> 00:39:12,060
consumer has found this information they invoke service by sending an XML based message

673
00:39:12,090 --> 00:39:13,150
called so

674
00:39:13,170 --> 00:39:18,060
to the end point of the web service and the result comes back

675
00:39:18,090 --> 00:39:24,140
the interesting thing about web services from the web services community is that they can

676
00:39:24,140 --> 00:39:27,140
be a proxy for business services

677
00:39:27,150 --> 00:39:34,170
which means that any organisation which sells value to customers can be transformed from a

678
00:39:34,170 --> 00:39:38,390
single black box single black box monolithic entity

679
00:39:38,420 --> 00:39:41,190
into a set of micro functionalities

680
00:39:41,200 --> 00:39:44,920
and these marker functionalities provide some micro value

681
00:39:44,940 --> 00:39:49,510
when invoked and available on the web and so you can transform the nature of

682
00:39:51,420 --> 00:39:56,320
interesting things are happening in the web services world probably everyone knows about this so

683
00:39:56,330 --> 00:39:57,700
through amazon

684
00:39:57,730 --> 00:39:59,640
you can

685
00:40:00,640 --> 00:40:02,950
potentially all seemingly

686
00:40:02,960 --> 00:40:05,480
infinite computing resources

687
00:40:05,510 --> 00:40:07,730
through the clouds

688
00:40:07,730 --> 00:40:13,080
they have a simple storage service where you can store huge amounts of data for

689
00:40:13,260 --> 00:40:16,490
only a fraction of the few cents per month

690
00:40:16,510 --> 00:40:22,920
they have databases that you can invoke so through these services the argument is the

691
00:40:22,930 --> 00:40:28,760
one doesn't need ninety department anymore you can outsource all the RIT infrastructure

692
00:40:28,790 --> 00:40:32,380
they also have the amazon mechanical turk

693
00:40:32,410 --> 00:40:38,700
service which is also called artificial artificialintelligence where they provide a framework where people can

694
00:40:38,700 --> 00:40:43,580
sign up and offer tasks that only people come off and then you can invoke

695
00:40:43,580 --> 00:40:45,490
these as a web service

696
00:40:45,510 --> 00:40:50,210
so there's an interesting statistic in two thousand three nine billion hours

697
00:40:50,230 --> 00:40:53,040
of online so was played

698
00:40:53,050 --> 00:40:54,140
so now you can

699
00:40:54,140 --> 00:40:59,140
view the web is a huge computing resource where the computers and people

700
00:40:59,160 --> 00:41:02,610
and you can inductive the services

701
00:41:03,700 --> 00:41:08,140
so this is the web services in a few slides so the problems

702
00:41:08,140 --> 00:41:11,110
from web services to not really scalable

703
00:41:11,140 --> 00:41:12,850
because of the task

704
00:41:12,950 --> 00:41:15,540
associated with creating an application

705
00:41:15,550 --> 00:41:19,710
out of web services can components have to be carried out by people

706
00:41:19,730 --> 00:41:24,730
specifically by ITV personnel so if you want to find an appropriate service if you

707
00:41:24,730 --> 00:41:28,700
want to put them together if they mismatches between the components if you want to

708
00:41:28,700 --> 00:41:34,080
invoke service you need to phone up your department and ask a program to do

709
00:41:34,080 --> 00:41:35,360
some work for you

710
00:41:36,260 --> 00:41:41,160
if you imagine where at the moment there are thirty billion web resources

711
00:41:41,300 --> 00:41:42,860
are out there

712
00:41:42,890 --> 00:41:46,020
if you want to have services out on the web in some sort of scale

713
00:41:46,290 --> 00:41:49,580
you need to have some sort of automation which will require

714
00:41:49,630 --> 00:41:52,830
on semantic markup

715
00:41:54,450 --> 00:41:59,980
this slide we used to present to give an overview of the vision of the

716
00:42:00,230 --> 00:42:03,780
research is that you can say that is

717
00:42:04,660 --> 00:42:05,740
first when

718
00:42:05,750 --> 00:42:09,370
the standard web which is based in your eyes HTML and HTTP

719
00:42:09,380 --> 00:42:12,720
but services bring function

720
00:42:12,730 --> 00:42:17,490
so as opposed to data component then we have the semantic web where the focus

721
00:42:17,490 --> 00:42:22,670
is on machine readability so you have the web for machines and semantic web services

722
00:42:22,670 --> 00:42:24,240
you put the two together

723
00:42:24,240 --> 00:42:26,000
canada is a very large country

724
00:42:26,060 --> 00:42:28,000
was very empty

725
00:42:32,570 --> 00:42:38,280
like in alberta if i'm not your percent of canada's this man

726
00:42:38,390 --> 00:42:43,060
very small percentage of canada's and so most number companies

727
00:42:43,080 --> 00:42:44,940
number on ground

728
00:42:44,990 --> 00:42:46,950
to do that you have to have very

729
00:42:46,960 --> 00:42:50,240
one more is very very clear about what percentage

730
00:42:50,990 --> 00:42:52,610
forest you can create

731
00:42:52,630 --> 00:42:57,160
this page seven percent if you doing clear cutting this process still seven percent

732
00:42:57,180 --> 00:43:00,180
well he missus seven percent if you're talking about

733
00:43:00,200 --> 00:43:03,940
sixteen thousand square kilometres

734
00:43:03,950 --> 00:43:07,920
this is this is already covered this this boreal forest cover four million square kilometres

735
00:43:09,420 --> 00:43:13,240
and you can't go and count between

736
00:43:13,270 --> 00:43:18,800
the trouble is that the remote sensing community and the computer vision community has made

737
00:43:18,800 --> 00:43:21,530
promises never really lived up to

738
00:43:22,240 --> 00:43:24,490
the systems are not reliable

739
00:43:24,510 --> 00:43:27,760
and not only that but not true that we reliable

740
00:43:27,790 --> 00:43:32,010
and the question is how do you prove that something is working well when you

741
00:43:32,010 --> 00:43:35,450
can simply sample a thousand trees

742
00:43:35,460 --> 00:43:38,560
as adequate sample when you really have to count

743
00:43:41,880 --> 00:43:45,990
there is a need to have some sort of technology that in some sense have

744
00:43:46,000 --> 00:43:48,440
its own internal

745
00:43:50,110 --> 00:43:52,480
credibility criteria

746
00:43:52,500 --> 00:43:54,900
previous methods for this

747
00:43:54,940 --> 00:44:01,640
one more or less OK i e in in in in plantation forests for single

748
00:44:01,640 --> 00:44:04,140
space is the safest spruce

749
00:44:04,150 --> 00:44:10,050
where typical models included matching building up

750
00:44:10,060 --> 00:44:13,710
you can five isio would you reference cameras

751
00:44:13,710 --> 00:44:18,510
OK so you know the orientation and position the cameras six parameters in over time

752
00:44:18,510 --> 00:44:20,750
of day so you know the position of the sun

753
00:44:20,780 --> 00:44:21,950
so you know

754
00:44:21,960 --> 00:44:22,730
you can then

755
00:44:22,760 --> 00:44:25,510
right what the template of

756
00:44:25,530 --> 00:44:27,230
since true should be

757
00:44:27,260 --> 00:44:29,840
flying the single this time of day

758
00:44:30,810 --> 00:44:33,860
and many of the techniques and to cross correlation

759
00:44:33,880 --> 00:44:38,960
normalized cross correlation three one cross correlation all the variations on cross correlation

760
00:44:38,980 --> 00:44:42,770
and find pics can be picks but of course they they

761
00:44:42,820 --> 00:44:47,090
they fall down others do that the following methods

762
00:44:47,200 --> 00:44:52,670
all have not been used by industry for many reasons including that how do i

763
00:44:52,670 --> 00:44:54,640
know this is correct and secondly

764
00:44:55,020 --> 00:45:00,770
you change the lighting overcast you go from plantations in mixed forests and the terrible

765
00:45:00,790 --> 00:45:02,130
because like this

766
00:45:03,920 --> 00:45:09,270
so the ball of fire actually a mixed mixture of deciduous trees like aspen and

767
00:45:09,920 --> 00:45:12,210
and conifers like spruce

768
00:45:13,680 --> 00:45:15,170
these things

769
00:45:15,190 --> 00:45:20,020
so you really can't have and you certainly can't assume that your from

770
00:45:20,050 --> 00:45:23,030
like this so

771
00:45:23,060 --> 00:45:26,040
most of the forestry done in canada

772
00:45:26,060 --> 00:45:31,630
we can not use presentation for this has been very successful in scandinavian countries

773
00:45:31,640 --> 00:45:36,340
where most of the first of presentation because many people argue we in australia as

774
00:45:36,340 --> 00:45:38,880
well as candidates should be going to plantation not

775
00:45:38,900 --> 00:45:40,990
you not using all graph

776
00:45:41,000 --> 00:45:42,700
the natural forests and

777
00:45:42,710 --> 00:45:45,500
the support is

778
00:45:45,550 --> 00:45:50,850
so the current systems really have been very much open open loop

779
00:45:52,150 --> 00:45:53,390
no learning

780
00:45:53,460 --> 00:45:57,880
um no handling of public spaces

781
00:45:57,920 --> 00:46:01,430
and in particular no internal mechanisms were saying

782
00:46:01,450 --> 00:46:05,740
how do i know this is a list circles the evidence of the interpretation is

783
00:46:08,740 --> 00:46:14,290
our approach was in this context what scepticism what can we do

784
00:46:14,350 --> 00:46:19,660
so in the bayesian sense already in philosophical placing since almost we said well what's

785
00:46:19,660 --> 00:46:22,780
the best thing will be good evidence for us

786
00:46:22,880 --> 00:46:24,840
interpreting the same

787
00:46:25,000 --> 00:46:28,320
one good evidence could be if i take pictures of scenes

788
00:46:28,340 --> 00:46:30,490
and i reconstruct the forest

789
00:46:30,490 --> 00:46:34,210
and then take pictures of the reconstructed it should be the same feature to the

790
00:46:35,640 --> 00:46:41,360
it could be circumstantial evidence that the on on the right path is not true

791
00:46:41,390 --> 00:46:45,170
the circumstantial evidence of i can reconstruct the forest

792
00:46:45,190 --> 00:46:47,230
and take a picture from the pictures were taken

793
00:46:47,250 --> 00:46:48,850
then they should be somehow match

794
00:46:49,720 --> 00:46:53,190
the pitch is i'm going to generate a not necessarily exactly the same features that

795
00:46:53,190 --> 00:46:56,880
have the same sort of statistics on the like for example top trees in these

796
00:46:56,880 --> 00:46:58,860
sorts of things

797
00:46:58,880 --> 00:47:02,490
so the complex system that we opened we would think we

798
00:47:03,970 --> 00:47:07,480
from from a bayesian perspective is this type of system

799
00:47:07,500 --> 00:47:09,660
what we do is we take images

800
00:47:10,290 --> 00:47:12,420
i have a system that does annotations

801
00:47:14,310 --> 00:47:16,300
region labeling

802
00:47:16,320 --> 00:47:18,840
we have three d system stereo

803
00:47:18,850 --> 00:47:21,940
and that nothing mysterious part is using stereo

804
00:47:21,940 --> 00:47:25,860
but actually you could use light are all the top selected senses these days which

805
00:47:25,860 --> 00:47:27,680
will fall by the way

806
00:47:27,700 --> 00:47:31,000
these these active sources are not

807
00:47:31,030 --> 00:47:32,330
and not

808
00:47:32,350 --> 00:47:36,030
and then you have to combine these cells to get

809
00:47:36,050 --> 00:47:37,990
to get volumes

810
00:47:37,990 --> 00:47:41,110
if we treat the volumes and then you can predict images

811
00:47:41,160 --> 00:47:46,150
so this talk a system we impose but this really is a hybrid system is

812
00:47:46,190 --> 00:47:54,150
a complex system and you know it's very hard to get it i practical

813
00:47:54,380 --> 00:48:00,530
and not just abstract formulation of how how optimize complex systems

814
00:48:00,560 --> 00:48:06,410
so our approach was really a modular approach say how can we formulate it to

815
00:48:06,410 --> 00:48:09,450
and we average over the training set

816
00:48:09,540 --> 00:48:13,100
so this is

817
00:48:13,120 --> 00:48:15,740
probably but not his covered this pretty well

818
00:48:15,760 --> 00:48:19,100
so this is sort of the classical set up in machine learning

819
00:48:19,120 --> 00:48:22,910
but it contains this loop over the training set

820
00:48:22,970 --> 00:48:26,760
and for large sets of data that loop is inefficient

821
00:48:26,780 --> 00:48:29,470
and if you believe that never ends

822
00:48:29,490 --> 00:48:33,510
or it's nonstationary so as you go through the data the

823
00:48:33,530 --> 00:48:37,330
the underlying statistics of the dataset change on you

824
00:48:37,330 --> 00:48:42,120
right and this becomes an inappropriate formalism

825
00:48:42,140 --> 00:48:46,490
so what we really need to do is stochastic approximation whereby

826
00:48:46,510 --> 00:48:48,160
we just

827
00:48:48,160 --> 00:48:49,410
the loss

828
00:48:49,470 --> 00:48:51,310
on the current data

829
00:48:51,330 --> 00:48:54,930
and the current value of these parameters

830
00:48:54,950 --> 00:49:00,740
and we do some quick and dirty optimisation over that to get the next set

831
00:49:00,740 --> 00:49:02,870
of parameter values

832
00:49:02,890 --> 00:49:04,910
and then we iterate that

833
00:49:05,540 --> 00:49:12,060
the next batch of data is going to be evaluated on the fresh parameters

834
00:49:14,240 --> 00:49:17,890
so these are i gather some of these equations are missing

835
00:49:17,910 --> 00:49:21,160
on slides of apologize that's the font problem

836
00:49:21,180 --> 00:49:24,660
so if you want to leave those lights on a little bit longer if you

837
00:49:24,660 --> 00:49:25,370
want to

838
00:49:25,370 --> 00:49:27,030
to enter them

839
00:49:29,760 --> 00:49:34,330
this one is fine now on the other ones don't started writing too early because

840
00:49:34,330 --> 00:49:38,660
i have animated equation so they're going to change on you so so i'll tell

841
00:49:38,660 --> 00:49:42,700
you when the last version is up and you can start writing a book

842
00:49:42,720 --> 00:49:48,490
so now

843
00:49:48,530 --> 00:49:53,140
we come to the key problem

844
00:49:53,160 --> 00:49:56,530
OK sorry you were not the case

845
00:49:56,580 --> 00:49:58,060
well what's

846
00:49:58,080 --> 00:50:01,910
go on

847
00:50:07,530 --> 00:50:08,700
may two

848
00:50:17,430 --> 00:50:20,760
the key problem is that people for

849
00:50:20,810 --> 00:50:22,560
centre is really

850
00:50:22,600 --> 00:50:24,100
have developed these

851
00:50:24,120 --> 00:50:29,590
fancy optimisation methods so this first order gradient descent i should say that i explained

852
00:50:29,590 --> 00:50:32,280
to you right this is sort of

853
00:50:32,330 --> 00:50:34,280
very very low level

854
00:50:34,280 --> 00:50:38,580
it's almost as bad as evolutionary algorithms if you talk to anybody in the math

855
00:50:38,580 --> 00:50:42,180
department does optimisation going to laugh at you

856
00:50:42,220 --> 00:50:46,620
if you if you come with what everybody does is you know

857
00:50:46,640 --> 00:50:51,060
second order with lots of bells and whistles and this that and the other machinery

858
00:50:51,060 --> 00:50:52,490
on top right

859
00:50:52,510 --> 00:50:55,640
really heavy heavy duty hardcore stuff

860
00:50:55,660 --> 00:51:01,510
so actually hundreds of years of mathematical expertise have have gone into pushing things in

861
00:51:01,510 --> 00:51:03,160
this direction

862
00:51:03,180 --> 00:51:05,620
more and more complex machinery

863
00:51:05,640 --> 00:51:09,930
to be more and more efficient at solving these problems

864
00:51:09,950 --> 00:51:15,280
now the key problem that we're getting here now so everybody down

865
00:51:19,510 --> 00:51:25,410
the last two years

866
00:51:25,410 --> 00:51:29,970
yeah so the idea is that

867
00:51:29,990 --> 00:51:31,220
at time t

868
00:51:31,220 --> 00:51:32,620
it will take

869
00:51:32,640 --> 00:51:35,030
the parameter values at that time

870
00:51:35,040 --> 00:51:35,850
and the there

871
00:51:35,850 --> 00:51:38,120
and the current data

872
00:51:38,140 --> 00:51:42,760
and we're going to to change the parameter values based on that

873
00:51:42,780 --> 00:51:45,760
we're going to do a little optimisation step

874
00:51:45,760 --> 00:51:48,410
and then we take those new values

875
00:51:48,470 --> 00:51:53,810
and that the next time step we going to measure the gradient for instance on

876
00:51:53,810 --> 00:51:58,620
the new data using those new values that the new point already

877
00:51:58,640 --> 00:52:04,010
so we take measurements always at the current point so far optimise the hops around

878
00:52:04,030 --> 00:52:06,580
those measurements will also help around

879
00:52:06,600 --> 00:52:12,760
that's that's what that's why it's sort of an approximation that's why not known exactly

880
00:52:12,810 --> 00:52:15,100
to do this

881
00:52:21,060 --> 00:52:26,650
so here's one here's little picture of the hierarchy of optimisation algorithms try to at

882
00:52:26,650 --> 00:52:30,720
the bottom you have evolutionary algorithms which is really one of the

883
00:52:30,740 --> 00:52:36,450
amateurs do that don't know anything about optimizing one level is what i'm doing

884
00:52:36,560 --> 00:52:40,890
and then the real algorithm so here

885
00:52:40,970 --> 00:52:44,510
and the two axis you plotted

886
00:52:44,600 --> 00:52:49,140
the order of the algorithm in terms of the dimensionality of the subspace of this

887
00:52:49,160 --> 00:52:52,530
sort of one or the audience with of q

888
00:52:53,200 --> 00:52:57,330
it gets more and more expensive to go to the right

889
00:52:57,350 --> 00:53:00,600
and on the other axis is the convergence speed

890
00:53:00,640 --> 00:53:04,660
so it's kind of the inverse of how many iterations you have to run you

891
00:53:04,660 --> 00:53:06,430
optimize for

892
00:53:06,450 --> 00:53:07,830
until you get

893
00:53:07,870 --> 00:53:14,080
good solution evolution is you have to run forever gradient descent like this forever and

894
00:53:14,080 --> 00:53:18,740
here they start getting really fast

895
00:53:18,760 --> 00:53:23,240
now the problem is when you do stochastic approximation which

896
00:53:23,260 --> 00:53:27,200
i said you need when you deal with the large data set because otherwise the

897
00:53:27,200 --> 00:53:31,450
first iteration takes too long you can wait for you just one pass through the

898
00:53:33,300 --> 00:53:36,760
if you use a stochastic approximation

899
00:53:36,780 --> 00:53:38,540
that approximation

900
00:53:40,240 --> 00:53:44,140
it's true that all this advanced machinery here

901
00:53:44,140 --> 00:53:47,830
so actually what happens is

902
00:53:47,850 --> 00:53:49,430
when you go online users

903
00:53:49,450 --> 00:53:50,850
with that

904
00:53:50,850 --> 00:53:53,090
if you like it makes it look a little bit more like the problem is

905
00:53:53,090 --> 00:53:56,950
that most people in machine learning and working on so here the the ways in

906
00:53:56,950 --> 00:54:01,370
which making more complex instead of just assuming we have a single dimension of shape

907
00:54:01,620 --> 00:54:05,930
we're going to allow multiple dimensions of shape and colour and texture if you like

908
00:54:05,930 --> 00:54:09,240
and then what we're learning is in this sense which out of a large number

909
00:54:09,240 --> 00:54:12,870
of dimensions are the right ones to pay attention to we're also going to instead

910
00:54:12,870 --> 00:54:17,310
of assuming here that our training data are fully labeled

911
00:54:17,680 --> 00:54:21,870
in sorted according to counter we might say well suppose the training data are more

912
00:54:21,870 --> 00:54:26,300
in a semi supervised setting either on or or unsupervised they might be completely unlabeled

913
00:54:26,300 --> 00:54:29,800
or we may only observe labels for a small number of examples this is the

914
00:54:29,800 --> 00:54:33,310
situation where the child is in the real world where they see lots of chairs

915
00:54:33,310 --> 00:54:37,130
you lots of tables lots of dogs but only a small number of them are

916
00:54:37,130 --> 00:54:40,730
ever actually label

917
00:54:40,740 --> 00:54:42,860
there's there's a few other

918
00:54:42,910 --> 00:54:47,600
interesting ways of this model is complex but i was really problem here so this

919
00:54:47,600 --> 00:54:52,240
is the the data look more like this there instead of

920
00:54:52,250 --> 00:54:55,140
well the in the simulation we had

921
00:54:55,160 --> 00:54:56,770
eight dimensions of which

922
00:54:56,770 --> 00:54:58,710
though you may not see it here

923
00:54:58,730 --> 00:55:03,870
four of them are kind of like shape so there's four dimensions along which

924
00:55:03,930 --> 00:55:10,570
the categories are are coherently organised only other interesting complexity is that unlike here here

925
00:55:10,570 --> 00:55:14,480
there's a perfect correlation between this one shape dimension and the category variable but here

926
00:55:14,750 --> 00:55:18,950
the correlation might be imperfect so you can see what's going on more clearly if

927
00:55:20,120 --> 00:55:22,720
sort these

928
00:55:22,720 --> 00:55:27,000
these vectors as follows so these are just exactly the same

929
00:55:27,050 --> 00:55:28,080
strings of

930
00:55:28,110 --> 00:55:30,150
of digits

931
00:55:30,200 --> 00:55:34,120
o and this this this this one and that one two four and so on

932
00:55:34,120 --> 00:55:37,240
those are category labels so most of them are unlabelled

933
00:55:37,250 --> 00:55:40,520
so now what i've done here is that sort of them and i have also

934
00:55:40,520 --> 00:55:45,250
highlighted in gray the the four dimensions which happened to be the first four bits

935
00:55:45,610 --> 00:55:51,600
four dimensions which are coherently co very with with category identity

936
00:55:51,630 --> 00:55:55,020
and the the learning problem essentially is to go from this to this and we're

937
00:55:55,020 --> 00:55:59,230
also and this is not going to tell our system learner how many categories there

938
00:55:59,230 --> 00:56:05,000
are well you know the the few labels we get are somewhat informative but notice

939
00:56:05,000 --> 00:56:07,420
here we didn't actually get an example of category three

940
00:56:07,460 --> 00:56:12,160
labelled examples and we're going to do this by taking exactly the same dirichlet multinomial

941
00:56:12,160 --> 00:56:16,430
model but now adding this extra lane variable z which

942
00:56:16,460 --> 00:56:22,140
is indicator for the category and it's follows the chinese restaurant process which i guess

943
00:56:22,140 --> 00:56:25,310
people have used that word but i think it has been formally introduced to write

944
00:56:25,930 --> 00:56:28,200
but it's it's it's sort of the

945
00:56:28,200 --> 00:56:32,880
combinatorics behind the dirichlet process and you i will tell you about that in the

946
00:56:32,880 --> 00:56:37,340
second lecture but basically disease just think of it as a likely indicator variable for

947
00:56:37,340 --> 00:56:42,640
a mixture of which can which mixture component and object comes from but unlike regular

948
00:56:42,640 --> 00:56:45,820
finite mixture xe actually can take on any

949
00:56:45,830 --> 00:56:49,910
value integers there is an infinite number of components here and the chinese restaurant process

950
00:56:49,910 --> 00:56:55,080
is the distribution on on the partition of the objects or assigning integers to the

951
00:56:55,080 --> 00:56:59,060
objects that tends to cluster them into a small number of groups to just like

952
00:56:59,060 --> 00:57:01,550
in the dirichlet process mixture

953
00:57:01,560 --> 00:57:04,210
so i mean this basically is the dirichlet process

954
00:57:04,250 --> 00:57:06,700
mixture with a multinomial

955
00:57:06,730 --> 00:57:08,820
model and dirichlet conjugate

956
00:57:09,060 --> 00:57:11,200
prior to that

957
00:57:11,200 --> 00:57:16,340
the the the twist on that is that which again is is more less people

958
00:57:16,340 --> 00:57:19,980
have been doing this in machine learning and bayesian statistics for a while but what's

959
00:57:19,980 --> 00:57:23,800
relatively new for cognitive science here is we're doing this we're learning in this in

960
00:57:23,800 --> 00:57:27,820
this infinite mixture model but we're also going to be learning parameters of the base

961
00:57:27,820 --> 00:57:30,800
distribution of of the DP if you like we're going to be learning is alpha

962
00:57:30,810 --> 00:57:32,890
invaders and so that

963
00:57:32,950 --> 00:57:36,560
that's going to then give us some knowledge which will transfer to new cases and

964
00:57:36,560 --> 00:57:40,370
give us the second order generalizations so the second marginalisation here is to see an

965
00:57:40,370 --> 00:57:44,800
object which has in this case completely novel values on all dimensions

966
00:57:44,820 --> 00:57:49,780
so just it's a simple any kind of simple notion of similarity is really going

967
00:57:49,780 --> 00:57:53,280
to help us here but we want to be able to learn that the first

968
00:57:53,280 --> 00:57:57,320
four dimensions here are tend to be homogeneous within categories so for example this one

969
00:57:57,320 --> 00:58:00,550
is more likely to be five than this one which happens to match on the

970
00:58:00,550 --> 00:58:01,820
last four

971
00:58:01,840 --> 00:58:09,020
and without going into too many details we we run a number of experiments with

972
00:58:09,020 --> 00:58:12,120
people i think people are over here in the model is over here and we

973
00:58:12,120 --> 00:58:17,080
do things like very for example whether category labels are present or absent during training

974
00:58:17,080 --> 00:58:22,120
so this this was the case where a category labels are mostly absent we actually

975
00:58:22,120 --> 00:58:24,110
we did one for to completely absent

976
00:58:24,160 --> 00:58:27,390
he said the experiment their solutions have to sort them you know we just a

977
00:58:27,390 --> 00:58:32,310
sort is in the some categories and then we show them anything any say for

978
00:58:32,310 --> 00:58:37,100
this new thing it's in some category whichever one it is i mean we without

979
00:58:37,110 --> 00:58:39,370
telling you but we just want to know which of these is in the same

980
00:58:39,370 --> 00:58:40,650
category is that one

981
00:58:40,670 --> 00:58:44,970
and both the model and the people can do almost as well when category labels

982
00:58:44,970 --> 00:58:49,120
were absent during training unsupervised training as it as when they present and also both

983
00:58:49,120 --> 00:58:52,810
of them to the second generalisation as well as the first authorization there are also

984
00:58:52,810 --> 00:58:56,520
both affected by coherence which is just how how

985
00:58:56,530 --> 00:58:59,110
how much for anyone to mention how

986
00:58:59,130 --> 00:59:01,820
homogeneous is it within the category

987
00:59:01,820 --> 00:59:06,010
the main interesting empirical results other than that the model and and people are the

988
00:59:06,010 --> 00:59:09,420
people can do basic with the model is that people are always a bit worse

989
00:59:09,420 --> 00:59:12,260
so all of these that are a little bit lower than all of these bars

990
00:59:12,590 --> 00:59:17,930
and we're into we're currently exploring what might be one interesting suggestion comes from looking

991
00:59:18,590 --> 00:59:21,320
when people let's say in the unsupervised case

992
00:59:22,320 --> 00:59:26,090
people just sort these objects and sometimes they kind of get the right sort sometimes

993
00:59:26,090 --> 00:59:29,240
they they cluster them in the same way that the model but other times they

994
00:59:29,240 --> 00:59:32,920
don't and if you just look at the people who cluster them in the right

995
00:59:32,920 --> 00:59:36,820
way during training and then they look almost perfect just like the model so it

996
00:59:36,820 --> 00:59:39,020
seems like people might be doing some kind of

997
00:59:39,060 --> 00:59:39,600
you know

998
00:59:39,620 --> 00:59:43,740
it less good approximate inference which we can try to model for example by saying

999
00:59:43,740 --> 00:59:48,600
well maybe they're doing gibbs sampling but with very few samples of fewer than the

1000
00:59:48,600 --> 00:59:52,840
model would or maybe they're doing a particle filter but only a small number of

1001
00:59:52,840 --> 00:59:56,990
particles in this is something which i referred to last time is kind of interesting

1002
00:59:57,360 --> 01:00:03,010
very interesting state-of-the-art place in computational cognitive science where people are trying to model with

1003
01:00:03,010 --> 01:00:04,890
ways in which the brain performs

1004
01:00:04,900 --> 01:00:11,380
online processing approximate inference as somehow approximate forms of approximate inference that were familiar with

1005
01:00:11,380 --> 01:00:15,320
the machine learning like particle filters with very few particles or gibbs sampling with very

1006
01:00:15,320 --> 01:00:16,660
few samples

1007
01:00:16,670 --> 01:00:20,550
and that we think that will produce these sort of

1008
01:00:20,550 --> 01:00:23,850
the difference between what we see in the people in the model here

1009
01:00:24,140 --> 01:00:27,180
any questions about that

1010
01:00:27,200 --> 01:00:30,820
so so what i what i want to do for the rest of the time

1011
01:00:31,240 --> 01:00:35,010
is tell you about some things which are not just

1012
01:00:35,030 --> 01:00:38,580
textbook models or standard things are used to seeing machine learning but most of the

1013
01:00:38,580 --> 01:00:43,490
things i'm going to tell you about here are models that we came up with

1014
01:00:44,700 --> 01:00:47,830
to try to solve some aspects of trying to make sense of some at some

1015
01:00:47,830 --> 01:00:51,800
kinds of learning from people solve which seem to go beyond what the problem is

1016
01:00:51,800 --> 01:00:54,490
that people are usually working on in machine learning and i hope some of these

1017
01:00:54,490 --> 01:00:58,030
ideas will also be useful to machine learning people in some cases a lot of

1018
01:00:58,030 --> 01:01:02,490
little do is just which motivate the problem from cognitive because basis and then show

1019
01:01:02,490 --> 01:01:07,240
you applications to machine learning problems try to illustrate

1020
01:01:07,240 --> 01:01:14,130
OK so what i decided to tell you about is about some work the talk

1021
01:01:14,130 --> 01:01:18,190
is going to be a bit conceptual not very technical but a bit conceptual about

1022
01:01:18,760 --> 01:01:23,940
some recent work about the theory of low rank modeling and so there in the

1023
01:01:23,940 --> 01:01:27,100
first part of the talk i'm going to talk a lot about what we have

1024
01:01:27,100 --> 01:01:29,220
learned through the modelling

1025
01:01:29,240 --> 01:01:35,010
with low rank matrices and then in the second part i will show you what

1026
01:01:35,010 --> 01:01:38,540
i believe are kind of exciting applications in computer vision

1027
01:01:39,340 --> 01:01:40,790
OK so

1028
01:01:40,830 --> 01:01:45,360
i would say that in my field of research is an explosion of research on

1029
01:01:45,360 --> 01:01:50,090
the theory of low rank modeling when i grew up to tell you the story

1030
01:01:50,090 --> 01:01:54,330
to start with when i grew up i worked on sparsity it was we're because

1031
01:01:54,330 --> 01:01:56,190
nobody was working on sparsity

1032
01:01:56,430 --> 01:01:59,520
now it seems that if you try to publish a paper at least in my

1033
01:01:59,520 --> 01:02:02,520
field was that they were sparsity in the abstract good luck

1034
01:02:02,580 --> 01:02:07,970
but i can see the same stuff happening now not use sparse models but using

1035
01:02:07,970 --> 01:02:09,220
low rank models

1036
01:02:09,250 --> 01:02:12,180
and that's what i would like to discuss some of the works i'm going to

1037
01:02:12,180 --> 01:02:15,580
present his hours and some of it is not

1038
01:02:15,600 --> 01:02:19,520
OK so i'm just going to talk about two very simple things one is matrix

1039
01:02:20,460 --> 01:02:21,800
and the other one is

1040
01:02:21,800 --> 01:02:25,430
ways of doing dimensionality reduction in very robust way

1041
01:02:25,440 --> 01:02:29,300
which i think important for people working in machine learning

1042
01:02:29,350 --> 01:02:34,020
OK so let's start with the first topic matrix completion if you do not know

1043
01:02:34,020 --> 01:02:39,290
the netflix prize check the name of this summer school again so i assume that

1044
01:02:39,290 --> 01:02:43,320
everybody is familiar with the netflix prize everybody knows what he's doing half people don't

1045
01:02:43,320 --> 01:02:46,540
know what is the netflix prize

1046
01:02:46,550 --> 01:02:48,490
no kidding

1047
01:02:50,130 --> 01:02:52,960
you're not serious are you

1048
01:02:52,970 --> 01:02:55,290
you're serious

1049
01:02:55,300 --> 01:03:07,090
so all the OK OK price is a major initiative that was launched by a

1050
01:03:07,090 --> 01:03:12,320
company name netflix netflix once movies out to users they have about half a million

1051
01:03:12,320 --> 01:03:17,590
users at the time the large surprise and they were anything about eighteen thousand titles

1052
01:03:18,770 --> 01:03:23,090
after you UV was mail service so what you do is you go on the

1053
01:03:23,090 --> 01:03:26,420
internet and you say you want to run the movie and then

1054
01:03:26,430 --> 01:03:31,020
netflix with a male user DVD would watch movie return to DVD would make the

1055
01:03:31,020 --> 01:03:32,740
next one and so on and so forth

1056
01:03:32,930 --> 01:03:36,610
but very quickly what they would ask users via email is what it was all

1057
01:03:36,610 --> 01:03:40,700
about you like the movies actually as iterate the movie you had just seen and

1058
01:03:40,700 --> 01:03:46,530
so users would go out and sparsely enter some entries in a huge database that

1059
01:03:46,530 --> 01:03:51,930
netflix was assembling database in which we have columns which are movies and about their

1060
01:03:51,950 --> 01:03:56,050
twenty thousand of them at the time there were about five hundred thousand users say

1061
01:03:56,050 --> 01:04:01,050
to you today matrix and when you see this process in this data matrix it's

1062
01:04:01,050 --> 01:04:06,780
a movie that this user has right of course more made data matrix is extraordinarily

1063
01:04:06,780 --> 01:04:13,550
is sparsely sampled because users on average round about thirty to fifty movies so instead

1064
01:04:13,550 --> 01:04:17,650
of having eighteen thousand ratings we have about thirty to fifty

1065
01:04:17,650 --> 01:04:21,270
and what netflix wants if they want to complete the netflix matrix what does this

1066
01:04:21,270 --> 01:04:25,520
mean that there are many entries that are not seen that have many movies that

1067
01:04:25,520 --> 01:04:32,460
have not been rated by users on netflix lounge to price the netflix prize where

1068
01:04:32,460 --> 01:04:39,200
people were to do this challenge to make predictions about the the items or entries

1069
01:04:39,200 --> 01:04:41,170
of the matrix you had not seen

1070
01:04:41,180 --> 01:04:44,620
and why wouldn't netflix wants to do such a thing i think the reason is

1071
01:04:44,620 --> 01:04:50,490
pretty obvious which is of course if you can predict accurately movies that people have

1072
01:04:50,490 --> 01:04:55,250
not yet seen and of course you can build an efficient recommender system and so

1073
01:04:55,250 --> 01:04:58,950
of course you can generate a lot of cost

1074
01:04:58,990 --> 01:05:02,250
you have lots of customers lots of happy customers and so you generate a lot

1075
01:05:02,250 --> 01:05:05,960
of income now of course as you can imagine there's not only netflix interested in

1076
01:05:05,960 --> 01:05:10,770
such things but you know facebook has the same problems apple sending music has the

1077
01:05:10,770 --> 01:05:15,490
same problems you everywhere you go in the online commerce people would like to change

1078
01:05:15,520 --> 01:05:18,250
their preferences for items based on few

1079
01:05:20,120 --> 01:05:26,620
so there are many such problems this is an area called collaborative filtering in statistics

1080
01:05:26,620 --> 01:05:31,300
we have similar problems in which we have questionnaires for example this could be questions

1081
01:05:31,300 --> 01:05:35,180
could be this could be questions this could be people being surveyed and some people

1082
01:05:35,180 --> 01:05:38,440
may not want to answer questions and you would like to know what if they

1083
01:05:38,440 --> 01:05:40,100
had answered the question

1084
01:05:40,120 --> 01:05:46,360
OK so for those of you who don't know about the netflix prize essentially netflix

1085
01:05:46,360 --> 01:05:51,380
offered a million dollars to whomever would be able to come up with the prediction

1086
01:05:51,380 --> 01:05:57,680
algorithm that was besting their own in-house of by ten percent

1087
01:05:57,700 --> 01:06:01,890
another problem which is not the netflix prize

1088
01:06:01,910 --> 01:06:06,360
what i'm going to try to show you that matrix completion occurs in many different

1089
01:06:06,360 --> 01:06:12,780
areas of science and engineering another thing that happens in in electrical engineering this time

1090
01:06:12,780 --> 01:06:18,030
is that what you've got is you've got a point in space with as xj

1091
01:06:18,170 --> 01:06:23,170
and what you know if you have partial information about those pairwise distances so you

1092
01:06:23,170 --> 01:06:23,970
have a matrix

1093
01:06:24,620 --> 01:06:30,060
l which records pairwise distances between pairs of points are point j

1094
01:06:30,120 --> 01:06:33,790
but the problem is you can only see if you have this pairwise distances perhaps

1095
01:06:33,790 --> 01:06:38,490
because you have a sensor nets so you have a wireless

1096
01:06:38,510 --> 01:06:40,180
network where

1097
01:06:40,190 --> 01:06:45,040
people or sensors can only construct a distance estimate is the nearest neighbors but not

1098
01:06:45,040 --> 01:06:49,930
too two sensors that are far away because of power constraints of you

1099
01:06:50,030 --> 01:06:53,610
OK so now you've got this data matrix which is very large

1100
01:06:53,700 --> 01:06:58,620
and what you would like to know if you'd like given pairwise local distances can

1101
01:06:58,620 --> 01:07:03,450
actually recoverable distances and why is it important important important because

1102
01:07:03,470 --> 01:07:08,980
in sensor we always want to know where stuff is so low localization of sensors

1103
01:07:08,980 --> 01:07:13,060
is the same as was if i can localize sensors i can compute all the

1104
01:07:13,090 --> 01:07:17,700
pairwise distances but it goes it's a two-way street in the sense that if you

1105
01:07:17,700 --> 01:07:21,680
can compute all pairwise distances and of course you know the location of the senses

1106
01:07:21,680 --> 01:07:23,430
up to rigid transformation

1107
01:07:23,450 --> 01:07:24,990
and so

1108
01:07:24,990 --> 01:07:31,430
the problem is from a few entries of this matrix can actually recover or global

1109
01:07:31,430 --> 01:07:35,560
distances so that i can locate the sensors so another problem in which we have

1110
01:07:35,560 --> 01:07:40,140
entries of the matrix of a very different kind obviously on the netflix data matrix

1111
01:07:40,190 --> 01:07:44,120
where there lots of missing entries and we'd like to interfere the value of those

1112
01:07:44,120 --> 01:07:46,240
missing entries

1113
01:07:49,180 --> 01:07:53,050
there are many other problems are a great friend of mine even is gonna talk

1114
01:07:53,050 --> 01:07:58,470
next week about convex optimisation perhaps issue we will show you applications of matrix completion

1115
01:07:58,470 --> 01:08:05,430
in control inside system identification and i have exciting applications in quantum state tomography quantum

1116
01:08:05,430 --> 01:08:09,510
mechanics which are fantastic due to the growth

1117
01:08:09,680 --> 01:08:13,170
you have problems of this kind in

1118
01:08:13,790 --> 01:08:19,380
signal processing of course it's a big problem in machine learning and computer vision solve

1119
01:08:19,380 --> 01:08:22,930
this problem of trying to be able to have partial information about the large data

1120
01:08:22,930 --> 01:08:27,940
matrix and try to recover in here's entire matrix is the problem that comes up

1121
01:08:27,940 --> 01:08:29,740
all the time

1122
01:08:29,750 --> 01:08:34,720
so what is the matrix completion problem is the matrix completion problem is

1123
01:08:34,750 --> 01:08:36,450
something like this

1124
01:08:36,930 --> 01:08:40,820
we've got a data matrix l which i'm going to call out we observe a

1125
01:08:40,820 --> 01:08:47,180
subset of its entries very few perhaps in the netflix case it's below percent and

1126
01:08:47,180 --> 01:08:49,130
a bound on the

1127
01:08:49,140 --> 01:08:51,520
performance of the support vector machines

1128
01:08:51,520 --> 01:08:55,480
and the important point here of course is that now

1129
01:08:56,520 --> 01:08:59,590
complexity depends on the margin

1130
01:08:59,640 --> 01:09:03,340
so we're actually able to take into account the fact that the

1131
01:09:05,370 --> 01:09:07,860
although the VC dimension is infinite

1132
01:09:07,870 --> 01:09:11,110
of the whole space the fact that we've got a particular

1133
01:09:11,230 --> 01:09:15,510
classifier has a margin of some value on the training set

1134
01:09:15,550 --> 01:09:17,890
it gives us the bound is nontrivial

1135
01:09:17,920 --> 01:09:21,070
so this ban will be non-trivial potentially for

1136
01:09:22,770 --> 01:09:24,190
four functions that

1137
01:09:24,220 --> 01:09:26,840
you know have a reasonable margin

1138
01:09:26,880 --> 01:09:29,220
this is the marginratio to the

1139
01:09:29,270 --> 01:09:31,980
the radius of the ball containing the data

1140
01:09:34,370 --> 01:09:37,710
obviously affected by the size of the training set here

1141
01:09:37,730 --> 01:09:42,140
and this is relatively benign log factor here

1142
01:09:42,160 --> 01:09:46,780
so this is sort of the bound with if you like justify

1143
01:09:46,790 --> 01:09:48,640
the claim that the

1144
01:09:48,710 --> 01:09:51,980
gamma is a measure of the the ninth of the distribution

1145
01:09:52,060 --> 01:10:03,550
so the

1146
01:10:09,010 --> 01:10:11,370
this band will not work

1147
01:10:11,400 --> 01:10:13,100
OK the rademacher stuff

1148
01:10:13,100 --> 01:10:15,430
i will work actually in that case

1149
01:10:17,920 --> 01:10:22,070
yes the rademacher will work i should get to that later on but this will

1150
01:10:22,070 --> 01:10:24,090
not work

1151
01:10:24,090 --> 01:10:30,800
the point that

1152
01:10:30,820 --> 01:10:34,200
no no you can observe it i mean that was the point i mentioned about

1153
01:10:34,200 --> 01:10:38,370
being structural risk minimisation over the choice of gamma you essentially have to prove the

1154
01:10:38,370 --> 01:10:41,130
theorem first range of values of gamma

1155
01:10:41,210 --> 01:10:47,580
and then use it for the value that you observe on your data

1156
01:10:47,580 --> 01:10:53,420
the true distribution in some sense

1157
01:10:53,450 --> 01:10:57,500
you know you have no idea what is gamma is your right

1158
01:10:57,940 --> 01:11:02,400
that said with high confidence they will be close to the gamma you observe because

1159
01:11:02,450 --> 01:11:06,640
the chances that you observe such a margin if there isn't one

1160
01:11:06,670 --> 01:11:09,800
is going to be pretty small but you don't have to worry about that exact

1161
01:11:09,800 --> 01:11:14,140
exactly mean that's the beauty of this that you actually measure on the training data

1162
01:11:14,140 --> 01:11:16,650
and infer something about the distribution

1163
01:11:16,700 --> 01:11:20,280
at the same time as inferring something about the function you learn so you really

1164
01:11:20,280 --> 01:11:22,860
doing inference on two things at once

1165
01:11:27,340 --> 01:11:32,380
it's it's valid

1166
01:11:32,400 --> 01:11:36,260
i mean it's valid OK said to make it strictly have to apply this for

1167
01:11:36,260 --> 01:11:37,840
a sequence of gammas

1168
01:11:37,840 --> 01:11:40,390
and you say you apply for

1169
01:11:40,450 --> 01:11:44,590
ten gammas you will have the ten factor in here or you know some sequence

1170
01:11:44,590 --> 01:11:45,410
some number

1171
01:11:45,460 --> 01:11:47,380
coming in as the delta here

1172
01:11:47,450 --> 01:11:50,610
and then you just use the one closest to the

1173
01:11:50,630 --> 01:11:53,780
gamma observed the next size down to one

1174
01:11:53,780 --> 01:11:58,120
from the camera so you can take the second sort of doubling sequence then you

1175
01:11:58,120 --> 01:12:01,100
just half the gammy observed

1176
01:12:01,130 --> 01:12:03,870
use that

1177
01:12:04,120 --> 01:12:06,700
they can

1178
01:12:06,890 --> 01:12:10,500
any other questions

1179
01:12:12,590 --> 01:12:15,700
so i think you know it's kind of a very nice to be able to

1180
01:12:15,700 --> 01:12:17,320
see that you're able to

1181
01:12:17,330 --> 01:12:21,230
combine the two inferences and and just work with empirical quantities

1182
01:12:21,300 --> 01:12:22,560
in in in

1183
01:12:22,580 --> 01:12:23,650
estimating this

1184
01:12:24,620 --> 01:12:27,800
so this is just to show you if you remember when we started i was

1185
01:12:27,800 --> 01:12:30,480
talking about you know the danger of this

1186
01:12:30,510 --> 01:12:32,640
tail of the distribution

1187
01:12:32,710 --> 01:12:35,750
being you know what you don't want this is the

1188
01:12:35,750 --> 01:12:39,360
the example the samples that cause you grief

1189
01:12:39,370 --> 01:12:42,310
and you'd like to avoid those samples

1190
01:12:42,320 --> 01:12:46,140
because you have no choice your from the sample you don't have a choice of

1191
01:12:46,140 --> 01:12:47,460
the sample you get

1192
01:12:47,520 --> 01:12:50,150
and you'd like to guard as best you can

1193
01:12:50,170 --> 01:12:52,160
against the case where

1194
01:12:52,180 --> 01:12:55,730
you actually have the sample is misleading you quite badly

1195
01:12:55,750 --> 01:12:59,100
so this is now applying the SVM

1196
01:12:59,110 --> 01:13:03,320
in place of what i had before the blue is the parzen window i had

1197
01:13:04,160 --> 01:13:05,590
for certain

1198
01:13:05,600 --> 01:13:10,750
set sizes this is doing exactly the same sampling of trying distribution of sorry of

1199
01:13:10,750 --> 01:13:12,030
training samples

1200
01:13:12,080 --> 01:13:15,030
repeatedly and creating a histogram

1201
01:13:15,040 --> 01:13:18,110
and seeing what the histogram looks like

1202
01:13:18,160 --> 01:13:23,130
i'm using exactly the same function space so it's it's still linear functions i'm not

1203
01:13:23,130 --> 01:13:24,710
using kernel here

1204
01:13:24,880 --> 01:13:29,290
so you know actually learning in the same feature space exactly

1205
01:13:29,300 --> 01:13:30,810
and the question is

1206
01:13:30,820 --> 01:13:31,630
you know

1207
01:13:31,630 --> 01:13:32,360
times x

1208
01:13:35,220 --> 01:13:36,740
you can think of this as the error

1209
01:13:37,150 --> 01:13:42,240
the neuron is currently making on an example multiplied by the input so to work

1210
01:13:42,240 --> 01:13:44,470
out the gradient you put each input in

1211
01:13:44,940 --> 01:13:48,720
if you look at the error between the output and the target then you multiply

1212
01:13:48,720 --> 01:13:52,220
the error by the input to get the gradient and it offered up and d

1213
01:13:52,220 --> 01:13:53,860
that's simple operation

1214
01:13:55,590 --> 01:14:02,690
which we got by differentiation is sometimes called backpropagation backpropagation is the neural networks community

1215
01:14:03,470 --> 01:14:05,050
the neural network communities name four

1216
01:14:13,340 --> 01:14:17,630
backpropagation is a particular algorithm for doing differentiation in feedforward

1217
01:14:18,420 --> 01:14:20,760
networks and doing it efficiently

1218
01:14:22,550 --> 01:14:25,690
okay so what happens if we do that we start of the neuron

1219
01:14:26,700 --> 01:14:28,260
with some randomly chosen weights

1220
01:14:28,740 --> 01:14:32,690
and the weights change so his gradient descent doing it's thing and i'm showing you

1221
01:14:32,690 --> 01:14:34,630
the three weight values and they

1222
01:14:35,190 --> 01:14:35,970
will blow up

1223
01:14:36,610 --> 01:14:41,280
what was happening in w one w two space looks like this the weights one

1224
01:14:41,300 --> 01:14:45,130
of in one direction and then one of another direction so as they went downhill

1225
01:14:45,130 --> 01:14:46,240
the direction downhill

1226
01:14:48,030 --> 01:14:53,630
what was going on well here's the data is initial weight vector that i i picked random

1227
01:14:54,190 --> 01:14:56,900
so it's not doing a very good job of separating

1228
01:14:57,340 --> 01:14:58,150
the blue from yellow

1229
01:14:58,570 --> 01:14:59,720
and you do gradient descent

1230
01:15:01,470 --> 01:15:06,130
the weights change and where the red line is changes rotates around

1231
01:15:06,760 --> 01:15:08,760
and then it changes rotates

1232
01:15:09,650 --> 01:15:10,400
around small

1233
01:15:11,090 --> 01:15:13,070
hand the weights are getting bigger and bigger

1234
01:15:15,240 --> 01:15:18,420
and after forty thousand iterations that's where the weights have got to

1235
01:15:19,590 --> 01:15:19,860
all right

1236
01:15:20,690 --> 01:15:21,420
so that's learning

1237
01:15:22,170 --> 01:15:23,090
and you might say well

1238
01:15:24,090 --> 01:15:24,940
i'm not happy

1239
01:15:26,320 --> 01:15:28,470
maybe i could do better and that's because

1240
01:15:29,550 --> 01:15:34,070
o moments ago before we got to the final optimized setting the weights with

1241
01:15:34,860 --> 01:15:39,800
we've managed to find a place where computer cliff that completely separates yellow points from the blue points

1242
01:15:40,510 --> 01:15:41,200
a moment before

1243
01:15:41,630 --> 01:15:42,530
when we reason halfway there

1244
01:15:42,970 --> 01:15:46,490
the answer looked a bit more reasonable this this this is now say okay you

1245
01:15:46,490 --> 01:15:50,300
win another example are classified four you and wherever arrived you say

1246
01:15:51,190 --> 01:15:52,110
ninety nine point nine

1247
01:15:52,490 --> 01:15:57,470
nine percent sure it's a blue or that is the yellow because we've got this incredibly steep cliff

1248
01:15:58,150 --> 01:15:59,510
and might say that's stupid

1249
01:15:59,970 --> 01:16:01,860
we don't like this outcome

1250
01:16:02,740 --> 01:16:03,590
has managed to

1251
01:16:04,260 --> 01:16:07,220
minimize the objective function but maybe we picked the wrong objective function

1252
01:16:09,800 --> 01:16:12,440
if we don't like this what we do well sensible people

1253
01:16:12,960 --> 01:16:14,030
change object a function

1254
01:16:17,130 --> 01:16:19,050
so now what we're gonna do is learning

1255
01:16:34,820 --> 01:16:36,050
i don't like these enormous

1256
01:16:36,440 --> 01:16:37,760
state sharp

1257
01:16:38,200 --> 01:16:42,380
changes changes in function or a neural network language we call weight decay

1258
01:16:43,030 --> 01:16:44,490
because you get state functions

1259
01:16:46,690 --> 01:16:47,610
when the weights are big

1260
01:16:49,990 --> 01:16:51,240
we added extra term

1261
01:16:51,740 --> 01:16:54,820
and these object function gets changed so it's not just gee

1262
01:16:55,970 --> 01:16:57,530
the new objective function of l and

1263
01:16:58,630 --> 01:16:59,090
about energy

1264
01:16:59,610 --> 01:17:00,490
plus an extra term

1265
01:17:02,260 --> 01:17:02,940
which says

1266
01:17:05,190 --> 01:17:06,420
i don't like it breaks

1267
01:17:07,760 --> 01:17:10,840
so the object has changed from please for the data as well as possible to

1268
01:17:11,360 --> 01:17:15,030
please for the data but add on a penalty for having the weights

1269
01:17:15,700 --> 01:17:16,530
in w

1270
01:17:19,240 --> 01:17:20,380
it can be defined to be

1271
01:17:29,630 --> 01:17:31,260
right so we do that's

1272
01:17:34,940 --> 01:17:35,940
now here's what happens

1273
01:17:36,340 --> 01:17:37,860
i've set this parameter alpha

1274
01:17:39,840 --> 01:17:40,860
which is the call

1275
01:17:41,760 --> 01:17:44,470
the regularizer if you from the statistics well

1276
01:17:45,900 --> 01:17:47,670
well sorry the regularisation constant

1277
01:17:52,050 --> 01:17:54,280
he w is called the regularizer

1278
01:17:56,860 --> 01:18:00,130
or if you are in neural networks alpha is called the weight decay rate

1279
01:18:06,820 --> 01:18:10,940
when you add on what happens to a gradient well it's just an extra turn

1280
01:18:10,940 --> 01:18:15,220
out and the gradient of he plus alpha y w is this lot plus

1281
01:18:15,690 --> 01:18:16,860
alpha times weights

1282
01:18:17,700 --> 01:18:20,800
so going downhill mat means you've got a minus alpha

1283
01:18:22,610 --> 01:18:23,220
term and

1284
01:18:23,880 --> 01:18:27,470
what happens when you do the minimization is shown here in terms of what the

1285
01:18:27,470 --> 01:18:30,090
weights do they used a blow up but now there's a whole down

1286
01:18:30,630 --> 01:18:31,170
and they don't

1287
01:18:32,090 --> 01:18:34,360
we don't have an ever steepening function

1288
01:18:34,900 --> 01:18:36,880
so all the way to settle down and

1289
01:18:37,400 --> 01:18:37,920
here's what

1290
01:18:38,670 --> 01:18:42,800
use the happen and we didn't have the regularizer the orange curves show everything blowing up

1291
01:18:44,070 --> 01:18:46,570
here's what happens in w one w two space

1292
01:18:46,970 --> 01:18:50,380
we go downhill downhill and sort a stop along the trajectory

1293
01:18:50,780 --> 01:18:52,070
effectively is what happens

1294
01:18:52,800 --> 01:18:56,260
so we follow almost the same trajectory in weight space but we stop

1295
01:18:56,840 --> 01:18:57,840
at an optimum

1296
01:18:58,260 --> 01:19:03,780
this new objective function this is where we used to be going after forty thousand iterations of steepest descent

1297
01:19:04,220 --> 01:19:10,070
now when we switch on the weight decay we go downhill and we end up in this place here

1298
01:19:11,490 --> 01:19:13,130
resize the looks

1299
01:19:16,700 --> 01:19:19,650
alright and you might look and say good that's better

1300
01:19:22,380 --> 01:19:26,860
because now it's not making completely unreasonable overconfident answers for example if you say

1301
01:19:27,380 --> 01:19:28,740
for another point arriving in this

1302
01:19:29,260 --> 01:19:30,110
location here

1303
01:19:31,490 --> 01:19:33,740
what's the chance that this in the blue grass it'll say

1304
01:19:34,190 --> 01:19:37,240
and about ninety percent is say i'm hundred potential

1305
01:19:38,900 --> 01:19:40,150
hand if you get

1306
01:19:40,800 --> 01:19:43,570
and no one is yellow and white it'll say fifty fifty

1307
01:19:44,130 --> 01:19:47,280
and i think yes that's not about answer based on

1308
01:19:47,920 --> 01:19:50,420
the data we've got here on apples and raisins

1309
01:19:51,360 --> 01:19:55,490
you might still be dissatisfied and if you are hold that thought we'll will come

1310
01:19:55,490 --> 01:19:56,380
back to this in a moment

1311
01:19:57,780 --> 01:19:59,720
now we're going to switch to

1312
01:20:01,130 --> 01:20:02,240
larger networks

1313
01:20:02,820 --> 01:20:06,400
and then we'll come back to the we the single neuron when you've got some more ideas

1314
01:20:08,030 --> 01:20:08,590
let me just

1315
01:20:09,820 --> 01:20:14,880
tell you a couple more things about the single neuron and then we'll come back to this learning

1316
01:20:18,470 --> 01:20:22,190
first i can give you an example of using a single neuron actually do

1317
01:20:22,630 --> 01:20:26,390
a real problems so i talked about pebbles and raisins or here instead is a

1318
01:20:26,390 --> 01:20:29,470
handwriting problem you can get yourself a data set of

1319
01:20:30,090 --> 01:20:33,590
handwritten digits that are being digitized in black and white dots

1320
01:20:35,050 --> 01:20:38,820
in two hundred fifty six dimensions so you can get yourself a few thousand

1321
01:20:39,800 --> 01:20:41,340
examples are twos and threes

1322
01:20:43,300 --> 01:20:45,650
are used the algorithm i just described

1323
01:20:46,220 --> 01:20:47,840
to come up with a single neuron

1324
01:20:48,280 --> 01:20:52,670
which has two hundred fifty six inputs has two hundred fifty seven weights

1325
01:20:53,090 --> 01:20:57,070
and its output can be viewed as the probability that this thing i'm looking at is a two or three

1326
01:20:57,490 --> 01:20:59,010
and when you do that this is what the weights

1327
01:21:00,610 --> 01:21:01,030
look like

1328
01:21:01,610 --> 01:21:03,860
andy error rate the classifier that u

1329
01:21:04,470 --> 01:21:09,280
i have made is about ten percent so on this database handwritten digits it gets

1330
01:21:09,490 --> 01:21:13,670
ninety percent of the right so it's not completely useless so even a single neuron

1331
01:21:13,970 --> 01:21:16,880
can be an interesting quick and dirty way to solve

1332
01:21:17,300 --> 01:21:17,900
some simple

1333
01:21:18,670 --> 01:21:19,530
inference problems

1334
01:21:22,200 --> 01:21:24,570
a question you could ask about a single neuron

1335
01:21:26,650 --> 01:21:28,630
you can view it as a communication channel

1336
01:21:32,010 --> 01:21:35,920
this is just a little aside and there's a whole chapter on this in the book if you interested

1337
01:21:37,220 --> 01:21:39,440
one way of thinking about what's going on here is

1338
01:21:39,860 --> 01:21:45,900
o i don't really believe that the output of the neuron is the correct probability distribution so forget the idea

1339
01:21:46,300 --> 01:21:49,470
but i do believe that the neuron is a helpful way to package up

1340
01:21:50,280 --> 01:21:51,570
the contents of the dataset

1341
01:21:53,400 --> 01:21:54,590
if you give me a data set

1342
01:21:55,550 --> 01:21:57,670
with lots and lots of examples of handwritten digits

1343
01:21:57,670 --> 01:22:01,540
she was

1344
01:22:03,060 --> 01:22:16,440
we see that

1345
01:22:42,050 --> 01:22:45,050
i know her

1346
01:22:45,070 --> 01:22:48,420
in our

1347
01:24:56,860 --> 01:25:00,340
and we see

1348
01:25:40,200 --> 01:25:45,440
the government

1349
01:25:48,820 --> 01:25:54,690
you have a

1350
01:26:19,830 --> 01:26:23,150
because property

1351
01:26:25,330 --> 01:26:26,920
a one

1352
01:26:40,270 --> 01:26:49,380
see now

1353
01:26:49,400 --> 01:26:58,950
it's the

1354
01:27:46,110 --> 01:27:50,440
the solution is

1355
01:27:52,460 --> 01:27:54,650
this is one

1356
01:27:54,670 --> 01:27:59,950
we see is

1357
01:27:59,950 --> 01:28:00,780
and two

1358
01:28:01,160 --> 01:28:04,560
this is not one one possibility would be to search

1359
01:28:04,590 --> 01:28:10,110
all possible entrance stop so this is this is going to be the intron and

1360
01:28:10,110 --> 01:28:14,710
and have to thank the interest on the to maximize of all positions

1361
01:28:14,730 --> 01:28:18,400
this makes more expensive

1362
01:28:18,440 --> 01:28:22,640
so to fill this metrics now you have these three choices here and you have

1363
01:28:22,810 --> 01:28:25,170
this maximisation in

1364
01:28:25,180 --> 01:28:31,980
they maximize into the past you try to find the start of

1365
01:28:32,000 --> 01:28:36,630
so and with this condition we we consider the last score and we have some

1366
01:28:38,140 --> 01:28:41,420
entrance score in this entrance school might depend on the link

1367
01:28:41,430 --> 01:28:46,960
might depend on the start of this let's spice that's what the start and spies

1368
01:28:46,970 --> 01:28:49,000
that's what interface

1369
01:28:49,020 --> 01:28:51,690
they can be kind of like this

1370
01:28:51,700 --> 01:28:57,370
so we have the contributions one is willing to use the lights at school at

1371
01:28:57,370 --> 01:29:02,300
the start of the twentieth of the end of it to be simply precompute this

1372
01:29:02,300 --> 01:29:08,300
by sets course for each position and then we somehow take these scores into account

1373
01:29:08,310 --> 01:29:12,600
and how we take this into account this is learned by the structure of

1374
01:29:12,640 --> 01:29:16,110
and so in the end we have

1375
01:29:16,130 --> 01:29:20,340
parameters which is the substitution matrix and then the function

1376
01:29:22,650 --> 01:29:26,990
determine how how explicit for link is contributing to the

1377
01:29:27,020 --> 01:29:30,040
in short squat

1378
01:29:30,050 --> 01:29:33,120
so this it

1379
01:29:37,840 --> 01:29:41,920
well you need to have the training data very you know of course

1380
01:29:42,620 --> 01:29:46,440
i don't need to know where the

1381
01:29:49,540 --> 01:29:55,190
and it it so i feel all i mean for every position there

1382
01:29:55,250 --> 01:29:59,980
we obvious to search the whole way back essentially this this could be as long

1383
01:29:59,980 --> 01:30:03,580
as the DNA maybe there is a limit on the length but still you can

1384
01:30:03,580 --> 01:30:09,630
do think we can take this is not going to be

1385
01:30:11,260 --> 01:30:15,080
it was down to if you have a

1386
01:30:15,080 --> 01:30:16,600
in trundling model

1387
01:30:17,120 --> 01:30:21,910
from this point on this quality constant and you only pay you only penalized by

1388
01:30:21,920 --> 01:30:27,990
the by the the point i mean by factor of how many nokia right so

1389
01:30:27,990 --> 01:30:32,420
it's not like can allow maybe hundred k entrance here

1390
01:30:32,440 --> 01:30:39,120
maybe from that point on on your in trundling model is constant and maybe

1391
01:30:39,250 --> 01:30:42,410
housing you pay only

1392
01:30:42,430 --> 01:30:47,380
so it's bit better if you don't have interlink model then you don't pay

1393
01:30:47,420 --> 01:30:52,240
and all for the link

1394
01:30:53,140 --> 01:30:57,870
OK so how can we parametrize these things so here we have a substitution matrix

1395
01:30:57,870 --> 01:31:05,500
this simply bipartite metrics that four letters and the deletion later so we have a

1396
01:31:05,500 --> 01:31:10,380
a c g t in the miners so they we have like insertion deletion

1397
01:31:10,390 --> 01:31:14,390
so this is essentially twenty five primary

1398
01:31:14,410 --> 01:31:20,030
we also have these three functions here of the length of the building and we

1399
01:31:21,280 --> 01:31:26,740
i mean if had a simple choice you the model them has a piecewise linear

1400
01:31:27,460 --> 01:31:32,300
so it's quite simple the thing about this choice is that all the time is

1401
01:31:33,260 --> 01:31:39,440
so we don't need cuts but we're doing this and estimate one-dimensional functions piecewise linear

1402
01:31:39,440 --> 01:31:40,910
functions are

1403
01:31:40,950 --> 01:31:43,190
but then it is

1404
01:31:43,220 --> 01:31:47,000
so if use think twenty or thirty

1405
01:31:47,020 --> 01:31:49,690
support points but piecewise linear

1406
01:31:49,730 --> 01:31:51,970
this just formula with

1407
01:31:51,980 --> 01:31:56,270
in the end we have a parameter vector which consists of these are the point

1408
01:31:57,000 --> 01:31:58,400
is within

1409
01:31:58,440 --> 01:32:01,330
and this substitution

1410
01:32:01,340 --> 01:32:06,080
so i for given beta can now compute an alignment score

1411
01:32:06,100 --> 01:32:08,350
four certain lines

1412
01:32:11,540 --> 01:32:16,110
so how can we optimize and how can you find these parameters

1413
01:32:16,290 --> 01:32:22,550
the idea again i would like to find parameters such that for non alignment good

1414
01:32:22,550 --> 01:32:25,520
alignment score is much larger than what we need

1415
01:32:25,530 --> 01:32:26,710
rather like

1416
01:32:27,670 --> 01:32:30,220
so and given n training examples

1417
01:32:30,250 --> 01:32:34,730
we have to solve this particular vision problem is that we would like to find

1418
01:32:34,730 --> 01:32:39,940
these parameters theta such that this will be positive of good alignment is greater than

1419
01:32:39,940 --> 01:32:41,400
any from alignment

1420
01:32:41,400 --> 01:32:44,760
first access thing out the catch even if that first access thing is

1421
01:32:44,780 --> 01:32:46,180
i read all the time

1422
01:32:46,190 --> 01:32:49,590
l are you have this sort of captures the intuition which we want which is

1423
01:32:49,590 --> 01:32:54,040
that something that's been accessed recently is likely to be accessed again and something that

1424
01:32:54,040 --> 01:32:58,930
hasn't been accessed very recently probably is less likely to be accessed again so it's

1425
01:32:58,930 --> 01:33:04,110
sort of captures are in our intuitive idea that we want locality of reference and

1426
01:33:04,110 --> 01:33:08,000
so the text talks much more carefully about these different page removal policy but any

1427
01:33:08,000 --> 01:33:12,370
time you see anybody talk about cache you should serve immediately as thing to ask

1428
01:33:12,370 --> 01:33:17,540
the question what's the page removal policy being used

1429
01:33:18,200 --> 01:33:20,670
so that basically does it for

1430
01:33:20,760 --> 01:33:25,430
our discussion of caching and performance that we started last time

1431
01:33:25,460 --> 01:33:29,350
what we do now is move on to this topic of of networking

1432
01:33:29,370 --> 01:33:30,550
thank you

1433
01:33:32,430 --> 01:33:52,850
so a computer network and you should all be familiar with the idea of a

1434
01:33:52,850 --> 01:33:54,360
computer network is its

1435
01:33:54,380 --> 01:34:01,190
some sort of a connection that connects multiple computers together so

1436
01:34:01,200 --> 01:34:05,750
why why we interested in studying computer network what is it relevant about computer network

1437
01:34:05,750 --> 01:34:07,540
for the purposes of this class

1438
01:34:07,550 --> 01:34:09,100
and there's really sort of two

1439
01:34:09,110 --> 01:34:11,870
the primary reasons the first one is that

1440
01:34:11,930 --> 01:34:14,600
computer networks are commonly

1441
01:34:14,670 --> 01:34:16,710
used as components of

1442
01:34:20,190 --> 01:34:24,540
OK so anytime we're building a big computer system is very likely that it's can

1443
01:34:24,570 --> 01:34:26,090
involve a computer network

1444
01:34:26,540 --> 01:34:29,830
so it's going to be important for us to understand what the properties of computer

1445
01:34:29,830 --> 01:34:31,720
networks are and how

1446
01:34:31,830 --> 01:34:35,100
sort of computer networks affect the design of our system if we want to use

1447
01:34:35,100 --> 01:34:37,240
them effectively in our systems

1448
01:34:37,300 --> 01:34:41,540
and fear networks have a number of uses in computer systems to do things like

1449
01:34:41,860 --> 01:34:42,990
the last

1450
01:34:44,300 --> 01:34:47,650
geographical limits

1451
01:34:48,330 --> 01:34:50,210
you guys are all familiar with

1452
01:34:50,220 --> 01:34:54,300
now the ability of the internet to allow you to check the score some sporting

1453
01:34:54,300 --> 01:34:56,540
game that's happening on the other side of the country

1454
01:34:56,550 --> 01:34:59,240
or to be able to send an email to your parents back home

1455
01:34:59,300 --> 01:35:00,460
so obviously

1456
01:35:00,470 --> 01:35:05,470
those are things that networks so so being able to sort of transmit data over

1457
01:35:05,470 --> 01:35:09,660
long distances is clearly a good thing for many kinds of systems

1458
01:35:09,680 --> 01:35:14,780
they also allow us to access remote data

1459
01:35:14,830 --> 01:35:18,230
this is related to the other thing to the other one but you know if

1460
01:35:18,230 --> 01:35:20,830
you have data that's not on your computer that you want to get hold of

1461
01:35:20,830 --> 01:35:24,310
you may want to contact your bank and ask them for your bank balance

1462
01:35:24,480 --> 01:35:26,750
and then finally

1463
01:35:27,970 --> 01:35:35,550
it can be used to physically separate client server

1464
01:35:35,620 --> 01:35:40,180
so we talked about that the last few lectures it's been awhile talking about why

1465
01:35:40,210 --> 01:35:45,010
the ability the fact that we can use things like threads and address spaces in

1466
01:35:45,010 --> 01:35:50,120
order to provide a sort of enforced modularity between models running on the same computer

1467
01:35:50,130 --> 01:35:54,300
but there are lots of situations in which we want to actually physically separate client

1468
01:35:54,330 --> 01:35:59,240
server your bank doesn't really want you to be running you know you're copy of

1469
01:35:59,240 --> 01:36:04,650
quicken on their server right because this notion enforced modularity we talked about isn't a

1470
01:36:04,650 --> 01:36:09,550
perfect separation between the client and the server whereas putting these things on really separate

1471
01:36:09,550 --> 01:36:13,570
machines that are only connected by this network that is controlled by this piece of

1472
01:36:13,570 --> 01:36:18,990
hardware that this network are sort of talks the network that's that's sort of that's

1473
01:36:18,990 --> 01:36:22,730
a good reason may be to separate these things from each other

1474
01:36:22,780 --> 01:36:26,190
and then finally the second major reason why we want to study networks

1475
01:36:26,420 --> 01:36:33,080
is simply that they themselves are an interesting computer system

1476
01:36:33,120 --> 01:36:36,800
so we talked in the first lecture about some of the interesting so the properties

1477
01:36:36,800 --> 01:36:40,530
that the computer systems have well you know the image is not many computer systems

1478
01:36:40,530 --> 01:36:44,320
are bigger than the internet right so perfect example of the giant

1479
01:36:44,330 --> 01:36:47,980
complex interesting system with all sorts of complicated behaviour

1480
01:36:48,030 --> 01:36:50,960
i will talk about some of those complicated behaviors today

1481
01:36:50,980 --> 01:36:52,080
as we

1482
01:36:52,080 --> 01:36:55,660
overview networks

1483
01:36:55,700 --> 01:36:56,840
OK so

1484
01:36:56,850 --> 01:36:58,360
the goal

1485
01:36:59,070 --> 01:37:03,710
the our discussion of networking in some sense is going to be to develop a

1486
01:37:03,710 --> 01:37:08,590
set of tools that allow us to have a universal communication

1487
01:37:08,660 --> 01:37:12,370
between the number of different clients

1488
01:37:20,440 --> 01:37:24,380
so what do i mean by that suppose i have some collection

1489
01:37:24,480 --> 01:37:29,630
the machines which are described little boxes

1490
01:37:29,650 --> 01:37:31,340
there are scattered around

1491
01:37:31,400 --> 01:37:34,970
and they are connected together what we want to do is to

1492
01:37:35,080 --> 01:37:37,010
connect these guys together

1493
01:37:37,040 --> 01:37:38,090
thank you

1494
01:37:38,190 --> 01:37:42,090
and the network system that we're going to design is going to be the interconnected

1495
01:37:42,090 --> 01:37:44,320
wells these guys to talk to each other

1496
01:37:44,350 --> 01:37:47,470
and the abstraction that what we want this network to be able to provide a

1497
01:37:47,470 --> 01:37:48,460
very high level

1498
01:37:48,550 --> 01:37:53,590
is the ability to do anything any communication

1499
01:37:53,670 --> 01:37:54,630
i think so

1500
01:37:54,650 --> 01:37:57,450
we want you know this is a and this is the

1501
01:37:57,460 --> 01:37:59,860
we want a to be able to send a message to be

1502
01:37:59,900 --> 01:38:03,090
or anybody else is in the network and that should be true for any for

1503
01:38:03,090 --> 01:38:05,250
any of the pairs that we can find

1504
01:38:05,370 --> 01:38:07,750
and so in some sense what we're going to do

1505
01:38:07,960 --> 01:38:11,190
over the next could cause the next couple weeks sort of see how we design

1506
01:38:11,230 --> 01:38:14,670
this cloud sits in between all these different computers

1507
01:38:14,720 --> 01:38:18,600
if you like you can think of this cloud often times the internet is represented

1508
01:38:18,600 --> 01:38:22,710
as the cloud is just some black box that you sort of sent messages into

1509
01:38:22,710 --> 01:38:26,710
with an address and on the other side the sort of message that pops out

1510
01:38:26,830 --> 01:38:29,220
at the other end so we're going to dive into the cloud and see what

1511
01:38:29,220 --> 01:38:31,420
happens inside

1512
01:38:31,520 --> 01:38:37,910
OK so

1513
01:38:37,990 --> 01:38:39,380
in order to

1514
01:38:39,430 --> 01:38:42,670
start to understand

1515
01:38:42,680 --> 01:38:45,570
some of the challenges of building this cloud

1516
01:38:45,680 --> 01:38:49,710
it's sort of we we should start we're going to start looking at the top

1517
01:38:49,710 --> 01:38:52,540
down to look at the biggest issues that we have to deal with then we're

1518
01:38:52,540 --> 01:38:55,240
going to break up those issues into small pieces

1519
01:39:02,640 --> 01:39:06,060
so what are the issues that sort of immediately pop out when you first start

1520
01:39:06,060 --> 01:39:12,080
for instance by computing first this vector w connecting the two class means and then

1521
01:39:12,600 --> 01:39:15,010
see the midpoint numbers

1522
01:39:15,060 --> 01:39:22,290
connection and we can compute this vector x minus c exists test point represent

1523
01:39:22,310 --> 01:39:26,450
in the feature space and then we simply have to check whether this vector in

1524
01:39:26,450 --> 01:39:30,800
this vector enclose an angle smaller than ninety degrees of larger than ninety degrees

1525
01:39:30,810 --> 01:39:32,380
so i to do that

1526
01:39:32,390 --> 01:39:33,750
we just have to

1527
01:39:34,710 --> 01:39:38,590
sheikh we just have to compute the dot product between these two vectors is the

1528
01:39:38,590 --> 01:39:41,870
dot product between the two vectors is positive

1529
01:39:41,880 --> 01:39:47,010
the angle is more than ninety degrees the product is negative the angle is larger

1530
01:39:47,010 --> 01:39:50,790
than ninety degrees so that's one simple way of doing things

1531
01:39:50,800 --> 01:39:52,530
and if we

1532
01:39:52,550 --> 01:39:54,760
go through the algebra

1533
01:39:54,780 --> 01:39:58,610
we just substitute everything we get

1534
01:39:58,650 --> 01:40:01,120
a simple solution

1535
01:40:01,140 --> 01:40:03,810
so we substitute everything into here

1536
01:40:03,860 --> 01:40:10,900
we then replace the dot products between points mapped into the feature space by kernel

1537
01:40:14,740 --> 01:40:20,780
so back here i have a constant which just all the time don't depend on

1538
01:40:22,360 --> 01:40:25,370
and then we we get this decision rules

1539
01:40:25,400 --> 01:40:26,210
which is now

1540
01:40:26,840 --> 01:40:32,760
a kernel based decision rules so it's an expansion in terms of kernels sentence here

1541
01:40:32,760 --> 01:40:38,440
or positive points and he is centered on the good point is some constant so

1542
01:40:38,440 --> 01:40:42,940
you can think of this may for the ones that didn't as before there

1543
01:40:42,960 --> 01:40:45,810
people who are statisticians here

1544
01:40:45,830 --> 01:40:47,460
that's what i forgot

1545
01:40:47,470 --> 01:40:48,680
OK we have

1546
01:40:48,740 --> 01:40:53,510
three statisticians also so for the statisticians

1547
01:40:54,470 --> 01:40:57,780
in some cases depending on what kind of kernel induces but for instance if we

1548
01:40:57,780 --> 01:41:01,140
choose a gaussian function is a kernel we normalize it

1549
01:41:01,160 --> 01:41:04,990
such that it has integral one so this is some kind of a given the

1550
01:41:05,010 --> 01:41:09,010
density model then

1551
01:41:09,020 --> 01:41:11,800
this is also density model we normalize by

1552
01:41:11,830 --> 01:41:16,920
the number of such kernels this is again it's like ipods and windows density estimate

1553
01:41:16,920 --> 01:41:20,510
of the positive class and this is the parzen window density is the density estimate

1554
01:41:20,510 --> 01:41:22,140
of the negative class

1555
01:41:22,150 --> 01:41:28,710
so in this case in this simple geometric classification in the feature space and so

1556
01:41:28,730 --> 01:41:30,860
giving us something like

1557
01:41:30,880 --> 01:41:33,560
a something like classifier based on

1558
01:41:33,580 --> 01:41:38,750
like a plug classification rule based on past windows density estimates

1559
01:41:38,790 --> 01:41:43,870
so it's a very simple algorithm and

1560
01:41:45,440 --> 01:41:47,550
this would be a good time to

1561
01:41:47,560 --> 01:41:52,750
take a few minutes try to everybody to derive this classifier in in a slightly

1562
01:41:52,750 --> 01:41:54,010
different way

1563
01:41:54,020 --> 01:41:56,000
so what i'd like you to try

1564
01:41:56,340 --> 01:42:00,640
my experience to learn a lot more if you try to make yourself also so

1565
01:42:00,640 --> 01:42:02,550
i'd like you to try

1566
01:42:02,560 --> 01:42:05,980
to derive this classification rule here

1567
01:42:06,010 --> 01:42:12,760
this one or something like this one maybe you have a slightly different signs somewhere

1568
01:42:13,340 --> 01:42:18,680
but like you to write this classification rule

1569
01:42:18,710 --> 01:42:25,440
simply by directing computing the distance so you compute distance between this point and this

1570
01:42:25,440 --> 01:42:27,790
point in the feature space

1571
01:42:27,810 --> 01:42:32,660
and you subtract the distance between these two points

1572
01:42:32,720 --> 01:42:36,970
now this is something one can compute using the products you can use the squared

1573
01:42:36,970 --> 01:42:43,680
distance between these two points you subtract the squared distance between these two points

1574
01:42:43,690 --> 01:42:45,180
and the undertaker

1575
01:42:45,190 --> 01:42:46,360
the the sign

1576
01:42:46,370 --> 01:42:51,050
of this difference to see whether you get something that's in this this one here

1577
01:42:51,060 --> 01:42:54,550
so let's take a few minutes and try this

1578
01:42:54,580 --> 01:42:58,310
everybody on his own or or means whatever

1579
01:42:58,330 --> 01:43:06,440
and i will try to try it myself when i say do it also

1580
01:43:06,490 --> 01:43:19,380
so if i don't know which slide i should be

1581
01:43:19,390 --> 01:43:21,810
leaving government valley this one

1582
01:43:21,830 --> 01:43:25,410
so how many people have

1583
01:43:25,430 --> 01:43:27,210
on the solution yet

1584
01:43:27,220 --> 01:43:33,330
to ask this not because we are in this phase transition where the last sound

1585
01:43:33,330 --> 01:43:37,150
level is increasing i remember i was once the talk not far from here by

1586
01:43:37,150 --> 01:43:43,190
stephen hawking and someone someone asked the question of the talk and then he started

1587
01:43:43,240 --> 01:43:46,880
preparing the answer which is difficult for him with some kind of machine that is

1588
01:43:46,880 --> 01:43:52,420
using so at the beginning people waiting very quietly for the answer but that it

1589
01:43:52,420 --> 01:43:57,880
took about ten to fifteen minutes to prepare the answer so after ten minutes gradually

1590
01:43:57,880 --> 01:43:59,100
there was this

1591
01:43:59,120 --> 01:44:03,890
increasing noise levels and after when he really answered your water it was as large

1592
01:44:03,890 --> 01:44:09,020
as in the train station we start answering suddenly everything was required again the answer

1593
01:44:09,020 --> 01:44:11,010
was just one sentence

1594
01:44:11,030 --> 01:44:15,130
so but how much more time to people need to know how many people have

1595
01:44:15,130 --> 01:44:17,670
solved it by now

1596
01:44:17,680 --> 01:44:22,460
i would say that's more than half business someone who would like to have more

1597
01:44:24,330 --> 01:44:25,310
OK so then

1598
01:44:25,320 --> 01:44:28,690
we can get to the solution and we can either

1599
01:44:28,710 --> 01:44:33,290
the research that i do it you or someone else does it and i have

1600
01:44:33,290 --> 01:44:38,310
to think about some kind of reward maybe i have three or four such problems

1601
01:44:38,310 --> 01:44:43,700
during my lectures will solve the largest number we get some kind of reward so

1602
01:44:43,700 --> 01:44:46,100
if you solve this one

1603
01:44:46,100 --> 01:44:48,230
you could you could do it here

1604
01:44:48,350 --> 01:44:49,350
you should

1605
01:44:49,350 --> 01:44:53,140
and somehow we would like to bound the loss of the imperial q of the

1606
01:44:53,600 --> 01:44:56,180
of the to classifier in terms

1607
01:44:56,190 --> 01:44:58,980
off the empirical loss

1608
01:44:59,000 --> 01:45:05,420
so i understand the VC bounds usually use the classification error of that period classifier

1609
01:45:05,580 --> 01:45:08,990
and the margin bounds use the

1610
01:45:09,840 --> 01:45:14,900
can margin of the current classifier OK so that is classifier that is the classification

1611
01:45:15,160 --> 01:45:17,630
the much

1612
01:45:19,940 --> 01:45:24,090
OK so i guess peter department to find i mean many others to find very

1613
01:45:24,090 --> 01:45:29,560
the VC dimension before you've seen the bones i just want very briefly define again

1614
01:45:29,600 --> 01:45:32,980
it's because i'm going to need in the next slide

1615
01:45:32,990 --> 01:45:37,920
OK so we need someone to define the complexity of a function class in order

1616
01:45:37,930 --> 01:45:42,710
to say something about this part of solar TV

1617
01:45:43,100 --> 01:45:48,460
a hypothesis class and functions which map from our

1618
01:45:48,530 --> 01:45:52,890
d two plus or minus one and the question is how complex is class

1619
01:45:52,900 --> 01:45:55,390
and you want to find the concept of shattering

1620
01:45:57,160 --> 01:46:07,050
and one says that f shatters set subset of rd if it essentially all that

1621
01:46:07,050 --> 01:46:13,140
cartoon is an excellent means let's say you three points and the function classes all

1622
01:46:13,140 --> 01:46:21,080
hyperplanes then the function class can actually achieve all different classifications of these three data

1623
01:46:21,080 --> 01:46:24,430
points OK so you can put you can put it here

1624
01:46:24,810 --> 01:46:27,570
he here and so on which all different

1625
01:46:27,580 --> 01:46:30,800
all eight different classification

1626
01:46:31,560 --> 01:46:32,550
so this is

1627
01:46:32,560 --> 01:46:38,020
so three points so the question is does chapter of the four point seven are

1628
01:46:38,020 --> 01:46:40,270
in two dimensions it is not

1629
01:46:41,100 --> 01:46:46,820
so the VC dimension is the size of the largest church set of x

1630
01:46:47,470 --> 01:46:52,020
so in two dimensions the VC dimension is three because you can check point

1631
01:46:52,070 --> 01:47:02,810
OK so when we have the VC dimension then we can derive such VC bounds

1632
01:47:02,890 --> 01:47:08,230
so the idea is that one balance the difference of the empirical error from the

1633
01:47:08,230 --> 01:47:09,260
expected error

1634
01:47:09,390 --> 01:47:13,540
that depends on the VC dimension of the function class so the larger the function

1635
01:47:13,540 --> 01:47:17,710
class the more the tool estimates might indicate

1636
01:47:17,720 --> 01:47:20,260
OK and the bound look like looks like this

1637
01:47:20,290 --> 01:47:27,290
so i'll have that the loss of the expected loss of n the miners help

1638
01:47:28,100 --> 01:47:36,070
of the the empirical loss with an example that difference is bounded by something which

1639
01:47:36,070 --> 01:47:37,240
depends on the VC dimension

1640
01:47:39,170 --> 01:47:41,120
divided by the number of examples

1641
01:47:41,130 --> 01:47:47,990
and something which depends on the time and the bound holds with probability at least

1642
01:47:47,990 --> 01:47:49,530
one minus

1643
01:47:49,540 --> 01:47:51,250
so if you would like to have a very

1644
01:47:51,650 --> 01:47:55,940
i mean you would like to have a pretty high that is known to hold

1645
01:47:56,120 --> 01:48:00,990
this term contributes quite a bit of the VC dimension is very high then

1646
01:48:01,090 --> 01:48:04,090
this bonus was

1647
01:48:04,100 --> 01:48:07,930
so we could interpret this as a kind of into confidence intervals

1648
01:48:07,940 --> 01:48:12,580
so for instance we can i mean this this holds for any function f so

1649
01:48:12,850 --> 01:48:15,230
for any function in the function class

1650
01:48:15,290 --> 01:48:19,770
so in particular for the current minimizer so that's one which is minimizing

1651
01:48:19,790 --> 01:48:21,860
our empirical loss

1652
01:48:21,870 --> 01:48:27,180
OK so we can not just plugging this and make it little simplified so then

1653
01:48:27,190 --> 01:48:35,640
the expected loss of incorrectly classified correctly classify is bounded by the

1654
01:48:35,690 --> 01:48:41,200
the best classifier plus something which depends on the VC dimension and that that comes

1655
01:48:43,290 --> 01:48:49,110
during the from vapnik chervonenkis and nineteen nineteen seventy one

1656
01:48:53,960 --> 01:48:59,080
so this is this is about so this bound becomes tight when the number of

1657
01:48:59,080 --> 01:49:07,350
examples increases so we have essentially this dimension is fixed then when n increases this

1658
01:49:07,350 --> 01:49:08,510
the terminals this term

1659
01:49:08,840 --> 01:49:15,380
go to infinity let's say what increase in these terms the was zero

1660
01:49:15,430 --> 01:49:22,290
OK and asymptotically these two terms essentially identical

1661
01:49:22,340 --> 01:49:28,300
OK so this is bound becomes tight with the increasing number of examples so on

1662
01:49:28,300 --> 01:49:29,740
the other hand if the

1663
01:49:31,820 --> 01:49:40,120
the complexity of increasing the VC dimension df then the empirical error becomes smaller because

1664
01:49:40,120 --> 01:49:43,770
we have a larger set of functions where we can choose a function from

1665
01:49:43,820 --> 01:49:46,530
so we have strong this year

1666
01:49:46,540 --> 01:49:50,400
so this is the the total this is expected error

1667
01:49:50,410 --> 01:49:54,200
OK on this side the complexity of the function class

1668
01:49:54,710 --> 01:49:57,230
OK and if you increase the function class

1669
01:49:57,310 --> 01:50:00,020
then the training error of course course down

1670
01:50:00,040 --> 01:50:01,750
because we can choose more

1671
01:50:01,760 --> 01:50:03,790
from the larger set of functions

1672
01:50:04,230 --> 01:50:09,140
on the other hand the VC dimension of the function class increases so this penalty

1673
01:50:09,140 --> 01:50:13,020
for the CBC dimension that's this to over here

1674
01:50:13,590 --> 01:50:16,420
and the bound says essentially

1675
01:50:16,440 --> 01:50:20,710
some of these two things up so the training error class this penalty and then

1676
01:50:20,710 --> 01:50:27,140
you get this bound some minimum at some point OK you can arbitrarily this country

1677
01:50:27,160 --> 01:50:35,420
so essentially overfitting is this this regime years training are very small and found an

1678
01:50:35,440 --> 01:50:39,550
independent is large

1679
01:50:39,590 --> 01:50:43,330
other questions for that

1680
01:50:45,370 --> 01:50:49,170
so how how does this apply to boost so

1681
01:50:49,270 --> 01:50:51,320
remember that the

1682
01:50:52,880 --> 01:50:58,140
function which is generated by adaboost is a linear combination of of t

1683
01:50:58,450 --> 01:51:00,510
t different functions

1684
01:51:00,630 --> 01:51:06,900
so essentially the function class of adaboost t iterations is a convex combination of functions

1685
01:51:06,900 --> 01:51:09,820
on this basic process at h

1686
01:51:09,840 --> 01:51:14,600
OK so this this this set here so that all the other ends the great

1687
01:51:14,610 --> 01:51:18,700
equals zero and sum to one and then we just choose t functions of the

1688
01:51:18,700 --> 01:51:20,730
function class

1689
01:51:21,460 --> 01:51:26,290
so i don't know this set by of eight so this is the convex hull

1690
01:51:26,300 --> 01:51:30,890
of the key elements of a

1691
01:51:30,970 --> 01:51:35,560
and also for this for this of course can just apply this part so

1692
01:51:35,740 --> 01:51:38,050
the the

1693
01:51:38,260 --> 01:51:43,690
expected error smaller than the empirical error plus some complexity term which depends on the

1694
01:51:43,990 --> 01:51:46,170
VC dimension of that

1695
01:51:47,020 --> 01:51:48,130
convex hull

1696
01:51:48,140 --> 01:51:53,330
OK now the question is how how large is the VC dimension that connects hart

1697
01:51:53,350 --> 01:51:59,100
so in the simplest in the simplest case compound that by simply

1698
01:51:59,150 --> 01:52:00,850
multiplying v

1699
01:52:00,860 --> 01:52:07,630
BC dimension of the basic policies class that's the VC dimension eight times t

1700
01:52:07,670 --> 01:52:12,360
that's the number of combined time slot t because you have some community coefficients so

1701
01:52:12,360 --> 01:52:17,130
that's that's a a very rough estimate so that it will be is sufficient to

1702
01:52:17,130 --> 01:52:19,860
show the posting results came so

1703
01:52:19,870 --> 01:52:24,040
we can bound the VC dimension of i have a

1704
01:52:24,040 --> 01:52:28,430
done in chicago a few years ago people were asked to religion

1705
01:52:28,480 --> 01:52:31,750
and then when asked what would happen to them when they died

1706
01:52:31,800 --> 01:52:34,290
most people in the sample were christian

1707
01:52:34,340 --> 01:52:37,930
and about ninety six percent of christians that when i die i going to go

1708
01:52:37,930 --> 01:52:39,500
to heaven

1709
01:52:39,550 --> 01:52:44,370
some of the sample was jewish that judaism is actually relation with the less than

1710
01:52:44,370 --> 01:52:47,070
clear story about the afterlife

1711
01:52:48,040 --> 01:52:51,960
most of the subjects who identify themselves as jewish said when they die they will

1712
01:52:51,960 --> 01:52:53,410
go to heaven

1713
01:52:53,630 --> 01:52:58,050
some of the sample denied having any religion at all said they have no religion

1714
01:52:58,070 --> 01:52:59,470
at all still

1715
01:52:59,480 --> 01:53:02,400
when these people were asked what would happen when they die

1716
01:53:02,410 --> 01:53:06,930
most of them and said i'm going to go to heaven

1717
01:53:07,810 --> 01:53:11,510
dualism is a it a lot rests on

1718
01:53:11,550 --> 01:53:14,330
but as chris points out that

1719
01:53:14,350 --> 01:53:18,630
the scientific consensus now is the dualism is wrong

1720
01:53:18,640 --> 01:53:20,230
there's no new

1721
01:53:20,240 --> 01:53:22,720
separable or separate

1722
01:53:22,730 --> 01:53:24,640
from your body

1723
01:53:24,650 --> 01:53:29,340
in particular there's no use separable from your brain

1724
01:53:29,360 --> 01:53:33,890
to put it in the way cognitive scientists and psychologists and neuroscientists like to put

1725
01:53:35,690 --> 01:53:39,010
the mind is what the brain does

1726
01:53:39,020 --> 01:53:44,170
the brain the mind reflects the workings of the brain just like computation reflects the

1727
01:53:44,170 --> 01:53:46,680
working of a computer

1728
01:53:46,730 --> 01:53:51,410
now why would you hold such an outrageous view why would you reject dualism in

1729
01:53:51,410 --> 01:53:52,980
favour this alternative

1730
01:53:53,000 --> 01:53:54,870
well a few reasons

1731
01:53:54,880 --> 01:54:00,490
one reason is dualism has always had problems for one thing it's a profoundly unscientific

1732
01:54:00,490 --> 01:54:04,910
doctrine we want to know is curious people

1733
01:54:04,950 --> 01:54:07,760
how children learn language

1734
01:54:07,770 --> 01:54:13,530
what we find attractive or unattractive what's the basis for mental illness and dualism simply

1735
01:54:14,490 --> 01:54:17,380
it's all non physical was part of the east

1736
01:54:17,390 --> 01:54:19,690
and hence fails to explain

1737
01:54:19,740 --> 01:54:23,100
more specifically dual is like a car

1738
01:54:23,150 --> 01:54:25,040
struggle to explain

1739
01:54:25,050 --> 01:54:27,140
how physical body

1740
01:54:27,150 --> 01:54:29,700
connects to an immaterial soul

1741
01:54:29,710 --> 01:54:31,270
what's the conduit

1742
01:54:31,290 --> 01:54:34,870
how this connection be made after all they knew full well

1743
01:54:34,880 --> 01:54:37,290
that you there is such a connection

1744
01:54:37,340 --> 01:54:40,330
in your body will sure commands

1745
01:54:40,720 --> 01:54:43,880
if you bang your toaster due feel pain

1746
01:54:43,900 --> 01:54:47,470
if you drink alcohol ineffectual reasoning

1747
01:54:47,490 --> 01:54:50,870
but he can only wave his hands as to how the physical

1748
01:54:50,880 --> 01:54:57,810
o thing in the world could connect to an immaterial mind

1749
01:54:57,850 --> 01:54:59,400
they car

1750
01:54:59,440 --> 01:55:01,370
when he was alive

1751
01:55:01,390 --> 01:55:05,900
i was reasonable enough concluding that physical objects

1752
01:55:05,920 --> 01:55:08,040
can not do certain things

1753
01:55:08,090 --> 01:55:12,550
he was reasonable concluding for instance as did that there's no way in the early

1754
01:55:12,550 --> 01:55:15,800
physical object could ever play a game at chess

1755
01:55:15,850 --> 01:55:20,530
because in that such a capacity is beyond the capacity of the physical world and

1756
01:55:20,530 --> 01:55:22,320
hence you have to apply

1757
01:55:22,340 --> 01:55:26,240
you have to extend explanation to an immaterial soul

1758
01:55:26,260 --> 01:55:27,100
but now

1759
01:55:27,110 --> 01:55:28,170
we know

1760
01:55:28,180 --> 01:55:31,220
we have what scientists call in existence proofs

1761
01:55:31,230 --> 01:55:32,340
we know

1762
01:55:32,400 --> 01:55:37,280
physical objects can do complicated and interesting things we know for instance

1763
01:55:37,290 --> 01:55:39,040
machines can play chess

1764
01:55:39,080 --> 01:55:45,220
we know machines can manipulate symbols we know machines have limited capacities two

1765
01:55:45,240 --> 01:55:48,730
engage in mathematical and logical reasoning

1766
01:55:48,750 --> 01:55:51,010
to recognise things

1767
01:55:51,020 --> 01:55:53,520
to do various forms of computations

1768
01:55:53,530 --> 01:55:56,500
and this makes it at least possible

1769
01:55:56,510 --> 01:55:57,310
that we

1770
01:55:57,330 --> 01:55:59,190
are such machines

1771
01:55:59,200 --> 01:56:03,720
so when you can no longer say look physical things just can't do that

1772
01:56:03,770 --> 01:56:06,420
because we know physical thinking to align

1773
01:56:06,430 --> 01:56:08,510
and this opens up the possibility

1774
01:56:08,520 --> 01:56:13,520
that that that humans are physical things in particular that humans are brains

1775
01:56:16,370 --> 01:56:18,370
there is strong evidence

1776
01:56:18,390 --> 01:56:21,550
that the brain is involved in the light

1777
01:56:21,560 --> 01:56:24,200
somebody will the heller do all this view

1778
01:56:24,220 --> 01:56:27,970
that said that what we do and what we decide what we think and what

1779
01:56:27,970 --> 01:56:32,060
we want are all had nothing to do with the physical world

1780
01:56:32,070 --> 01:56:37,540
i would be embarrassed by the fact that the brain seems to correspond intricate and

1781
01:56:37,540 --> 01:56:40,370
elaborate ways to one mental y

1782
01:56:40,710 --> 01:56:44,430
this has been known for a long time

1783
01:56:44,440 --> 01:56:47,640
philosophers and psychologists new for long time getting

1784
01:56:47,660 --> 01:56:49,670
smacked in the head

1785
01:56:49,710 --> 01:56:51,850
could change the mental faculties

1786
01:56:52,020 --> 01:56:56,030
diseases like syphilis can make you range that

1787
01:56:56,080 --> 01:56:59,110
chemicals like caffeine and alcohol can affect

1788
01:56:59,210 --> 01:57:00,670
how things

1789
01:57:00,690 --> 01:57:04,660
but what's new is we can now in different ways see the direct effects of

1790
01:57:04,660 --> 01:57:05,680
mental life

1791
01:57:05,700 --> 01:57:07,970
somebody with

1792
01:57:07,980 --> 01:57:10,020
a severe and profound

1793
01:57:10,040 --> 01:57:12,080
loss of mental faculties

1794
01:57:12,090 --> 01:57:13,750
the deficit will be shown here

1795
01:57:13,750 --> 01:57:14,960
OK so

1796
01:57:14,970 --> 01:57:19,150
so i'm going to give you couple of examples of how to mine these patterns

1797
01:57:19,160 --> 01:57:24,790
and if they if it looks a bit difficult to follow don't worry because the

1798
01:57:25,610 --> 01:57:27,570
interest fees

1799
01:57:28,410 --> 01:57:31,930
basically what we're after is to find these patterns

1800
01:57:31,980 --> 01:57:34,510
president in one one class in the present

1801
01:57:34,530 --> 01:57:38,620
but want to really interested in these methods then you can go and study from

1802
01:57:38,620 --> 01:57:40,130
the references

1803
01:57:40,540 --> 01:57:44,900
so in order to start with what i'm trying to do is a concise way

1804
01:57:44,900 --> 01:57:46,940
of representing these patterns

1805
01:57:46,950 --> 01:57:50,450
is this is some kind of a convex set

1806
01:57:50,460 --> 01:57:54,730
the way you're going to define a convex set these by means of two

1807
01:57:55,970 --> 01:58:01,600
so there is a minimum in bonding said and max wants so mean balance that

1808
01:58:01,800 --> 01:58:07,570
is as an example here it says the mean starts with items one two that's

1809
01:58:07,570 --> 01:58:08,580
one set

1810
01:58:08,630 --> 01:58:10,180
another one starts with

1811
01:58:10,190 --> 01:58:12,730
items one and three

1812
01:58:12,740 --> 01:58:14,920
and the max one says you can go

1813
01:58:14,930 --> 01:58:19,530
one two three four five these are each one means a particular item just we

1814
01:58:19,570 --> 01:58:23,630
mapped them into one you know the moment you see one which means is a

1815
01:58:23,630 --> 01:58:27,670
particular item you're talking about but i to run

1816
01:58:27,680 --> 01:58:32,010
so so what it means is that if you're given objects of these elements

1817
01:58:32,190 --> 01:58:34,600
are we trying to say is everything that

1818
01:58:34,650 --> 01:58:36,280
so set

1819
01:58:36,290 --> 01:58:37,960
of these elements

1820
01:58:38,000 --> 01:58:42,030
and including those elements but they have to be subset of one of these things

1821
01:58:42,030 --> 01:58:43,280
so for example

1822
01:58:43,290 --> 01:58:44,960
you start with one two

1823
01:58:44,970 --> 01:58:48,890
the inner one two e part of that set

1824
01:58:48,940 --> 01:58:50,370
and anything

1825
01:58:50,410 --> 01:58:52,880
added to that so for example if i had

1826
01:58:52,890 --> 01:58:55,640
later extended this one too

1827
01:58:55,650 --> 01:58:56,800
one two three

1828
01:58:56,810 --> 01:58:59,930
this has to be subset of one of these two so in this case one

1829
01:58:59,930 --> 01:59:01,780
two three is a subset of that

1830
01:59:01,800 --> 01:59:03,630
so then i can grow

1831
01:59:04,480 --> 01:59:08,570
one two three four and so forth so so basically what you can see that

1832
01:59:08,620 --> 01:59:10,200
is that you start with one two

1833
01:59:10,580 --> 01:59:14,760
and you expand all these terms and you end up with with

1834
01:59:14,770 --> 01:59:17,140
and and similarly you can start with one three

1835
01:59:17,150 --> 01:59:19,930
this minimum one second in one

1836
01:59:19,940 --> 01:59:23,930
keep growing but whenever you grow this exercise it has to be subset of one

1837
01:59:23,950 --> 01:59:24,960
of those things

1838
01:59:24,970 --> 01:59:29,650
and what you can show is that all these elements have nice convex property

1839
01:59:30,310 --> 01:59:34,010
so the convex properties defined as that

1840
01:59:36,510 --> 01:59:39,260
it will contain all these elements

1841
01:59:39,710 --> 01:59:41,910
that an element y

1842
01:59:42,040 --> 01:59:46,310
is a superset of x but body subset of z

1843
01:59:46,320 --> 01:59:47,510
which means

1844
01:59:47,550 --> 01:59:52,490
that any element that is present in the set would would would have that property

1845
01:59:52,600 --> 01:59:55,950
so this is a concise way of representing all these

1846
01:59:56,000 --> 01:59:57,350
possible patterns

1847
01:59:57,360 --> 02:00:00,350
by simply using these for that

1848
02:00:00,400 --> 02:00:01,380
so you can do

1849
02:00:01,390 --> 02:00:03,580
you know quite a strong

1850
02:00:03,590 --> 02:00:05,130
compression you can do

1851
02:00:05,140 --> 02:00:07,250
by applying this kind of techniques

1852
02:00:07,270 --> 02:00:10,460
but of course what you can show using the worst case

1853
02:00:10,480 --> 02:00:12,960
it is going to be exponential

1854
02:00:12,980 --> 02:00:16,460
so in practice it's a very concise representation

1855
02:00:16,470 --> 02:00:19,990
but from a theoretical point of view still you can end up you know if

1856
02:00:19,990 --> 02:00:21,840
you are given a bunch of points

1857
02:00:21,860 --> 02:00:26,000
in to me point is one of these elements like you know one two one

1858
02:00:26,010 --> 02:00:26,830
two three

1859
02:00:26,870 --> 02:00:30,950
is a concise way can presented for given all these sets i can say i

1860
02:00:30,950 --> 02:00:33,950
can represent this by using these four

1861
02:00:36,000 --> 02:00:40,010
but of course you can always begin a set which makes this that's really large

1862
02:00:40,010 --> 02:00:41,380
that means you end up

1863
02:00:41,470 --> 02:00:46,760
a huge number here a huge number

1864
02:00:47,450 --> 02:00:50,520
so what we're trying to do is that if you are given two

1865
02:00:52,830 --> 02:00:56,640
so what you can do is you can say well my positive data sets can

1866
02:00:56,640 --> 02:00:58,140
be represented by

1867
02:00:58,430 --> 02:01:01,470
elements of this this this kind of one by

1868
02:01:01,550 --> 02:01:02,700
and then

1869
02:01:03,120 --> 02:01:07,700
the negative set is like this contrast patterns are nothing but set of all patterns

1870
02:01:07,700 --> 02:01:12,000
in this which is quite easy you can generate all possible patterns from from this

1871
02:01:12,000 --> 02:01:16,410
does it from that you remove everything that present

1872
02:01:16,460 --> 02:01:21,000
is the same in some sense it's a very simple definition of contrast patterns

1873
02:01:21,040 --> 02:01:22,250
for imaging

1874
02:01:22,300 --> 02:01:24,110
all it telling you is that

1875
02:01:24,330 --> 02:01:28,040
you know the dataset you know we just not sure there is only one data

1876
02:01:28,040 --> 02:01:29,900
which is feature one

1877
02:01:29,950 --> 02:01:34,050
taking value one let's if FT two two and so forth and what we try

1878
02:01:34,090 --> 02:01:37,330
to do is that there is the need set has these ones

1879
02:01:37,370 --> 02:01:39,450
i'm interested in all the patterns

1880
02:01:39,470 --> 02:01:40,300
that are

1881
02:01:40,310 --> 02:01:41,480
i present here

1882
02:01:41,490 --> 02:01:44,280
but not so for example you could say

1883
02:01:44,290 --> 02:01:45,510
one is

1884
02:01:45,520 --> 02:01:48,100
contrast pattern because one is present here

1885
02:01:48,110 --> 02:01:49,290
but not in this

1886
02:01:49,340 --> 02:01:50,680
and you can also say

1887
02:01:50,720 --> 02:01:54,880
i want to use present one is not and so forth that is

1888
02:01:54,890 --> 02:01:56,710
two three is present here

1889
02:01:56,730 --> 02:01:59,560
two three also present day doesn't come

1890
02:02:00,280 --> 02:02:02,820
so you can very concisely define

1891
02:02:02,830 --> 02:02:05,000
contrast patterns like this

1892
02:02:05,060 --> 02:02:06,290
but it could be

1893
02:02:06,300 --> 02:02:08,880
an exponential feminist computing

1894
02:02:08,940 --> 02:02:15,770
so this border differential algorithm is basically an iterative method it's minimally expands these patterns

1895
02:02:15,770 --> 02:02:18,170
to make sure that the patterns of

1896
02:02:19,390 --> 02:02:20,570
in fact

1897
02:02:21,870 --> 02:02:25,180
in contrast patterns so for example you start with one

1898
02:02:25,190 --> 02:02:27,010
and you know that immediately

1899
02:02:27,030 --> 02:02:32,140
if you take letter one is the taken first and immediately we know that

1900
02:02:32,160 --> 02:02:35,120
in one or any extended terms of one

1901
02:02:35,140 --> 02:02:38,390
that is a subset of this is going to be a contrast pattern because we

1902
02:02:38,390 --> 02:02:39,600
don't have

1903
02:02:39,660 --> 02:02:43,610
and then if i start with that bit too

1904
02:02:43,620 --> 02:02:45,820
of course two is not contrast patterns

1905
02:02:45,870 --> 02:02:51,030
even if i extended to three contest at the moment i find two three four

1906
02:02:51,040 --> 02:02:53,850
that i know that it is the contrast pattern because there is no

1907
02:02:54,310 --> 02:02:55,720
two three four here

1908
02:02:55,730 --> 02:02:56,930
OK so therefore

1909
02:02:56,940 --> 02:03:00,880
two three four is a subset of the set the four two three four is

1910
02:03:00,940 --> 02:03:03,900
context border if is basically

1911
02:03:03,910 --> 02:03:07,770
very cleverly expand this this sets

1912
02:03:07,820 --> 02:03:09,430
and the looks into the

1913
02:03:09,460 --> 02:03:10,860
into the negative set

1914
02:03:10,870 --> 02:03:12,150
if it is there

1915
02:03:12,160 --> 02:03:16,850
then it proves that structure and then and then go so the moment i find

1916
02:03:16,850 --> 02:03:20,900
it then there's no point in expanding because two three four

1917
02:03:20,920 --> 02:03:23,390
any other pattern will be by definition

1918
02:03:23,400 --> 02:03:26,660
would be a congress

1919
02:03:29,560 --> 02:03:32,090
the border diff can handle up to

1920
02:03:32,230 --> 02:03:33,130
you know

1921
02:03:33,220 --> 02:03:37,680
at that time and we looked at the end of PC

1922
02:03:37,740 --> 02:03:41,800
again the question is it you know is really interesting that

1923
02:03:41,810 --> 02:03:43,070
people say

1924
02:03:43,090 --> 02:03:45,160
we have infinite computing power

1925
02:03:45,210 --> 02:03:48,380
but not in infinite memory

1926
02:03:48,390 --> 02:03:50,380
because you know you can take any PC

1927
02:03:50,390 --> 02:03:53,640
run for right you've got infinite computing

1928
02:03:53,720 --> 02:03:56,900
so computing is not a problem memories the problem

1929
02:03:56,910 --> 02:04:00,700
so it's the kind of these interesting concept but of course we don't live that

1930
02:04:00,700 --> 02:04:02,080
long and therefore

1931
02:04:02,160 --> 02:04:06,290
you know we want the results to be seen so when you see these statements

1932
02:04:06,290 --> 02:04:08,320
like that there is an implicit notion

1933
02:04:08,450 --> 02:04:11,450
that i can't wait for more than an hour or a day or maybe three

1934
02:04:11,450 --> 02:04:13,570
months you know depending on what you do

1935
02:04:14,640 --> 02:04:18,680
so when you see these things let's assume there are some constraints defined so with

1936
02:04:18,680 --> 02:04:21,330
those constraints what we found is that

1937
02:04:21,380 --> 02:04:25,780
about seventy five attributes you have patients anything higher

