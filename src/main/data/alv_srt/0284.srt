1
00:00:00,000 --> 00:00:03,450
one thing i should point out before you forget is that if you're interested in

2
00:00:03,580 --> 00:00:08,800
using this software looking at restricted boltzmann machines deep belief nets convolutional nets and such

3
00:00:09,440 --> 00:00:13,430
if you go deep learning don't see is that all the data u on the source code

4
00:00:14,680 --> 00:00:18,770
you know you'll see a lot of different energy be implementation of convolutional nets

5
00:00:19,370 --> 00:00:20,590
there is deep learning models

6
00:00:21,180 --> 00:00:24,740
there's something that i'm gonna talk about the multimodal neural language models you know if

7
00:00:24,750 --> 00:00:27,920
you're interested in doing face recognition facial expression recognition

8
00:00:28,450 --> 00:00:31,020
with deep belief networks you'll find a lot of different

9
00:00:32,420 --> 00:00:33,360
tools and codes

10
00:00:33,880 --> 00:00:36,290
coding over there if if if you're interested

11
00:00:41,310 --> 00:00:44,840
so now we're gonna we're gonna talk a little bit more about more advanced models

12
00:00:45,580 --> 00:00:48,340
just give you a flavor so this is more about what's been happening in the

13
00:00:48,340 --> 00:00:50,910
deep learning community in the last you know a few years

14
00:00:53,280 --> 00:00:54,100
what i'm gonna do is

15
00:00:54,510 --> 00:00:55,360
i'm gonna give you a little

16
00:00:55,850 --> 00:00:57,100
a brief introduction into

17
00:00:57,690 --> 00:00:59,330
something is called deep boltzmann machines

18
00:01:00,350 --> 00:01:04,560
these are very similar models the deep belief networks but they are have

19
00:01:05,130 --> 00:01:06,090
so differences

20
00:01:06,800 --> 00:01:11,960
and then got show you how we can start looking at learning more structured models

21
00:01:12,510 --> 00:01:13,790
or more robust models

22
00:01:15,570 --> 00:01:21,000
as well as getting in the problem and the interesting problem of transfer learning one shot learning

23
00:01:22,180 --> 00:01:23,130
and we'll see some of

24
00:01:23,950 --> 00:01:28,160
and now then i'm going conclude by looking at multimodal learning so this is where

25
00:01:28,160 --> 00:01:31,570
we're gonna look at language images and trying to combine different modalities

26
00:01:32,520 --> 00:01:36,680
together it's been very exciting topic in the deep learning community in particular looking at

27
00:01:37,080 --> 00:01:40,410
you know images speech text or a or a

28
00:01:40,840 --> 00:01:41,700
jointly together

29
00:01:43,860 --> 00:01:44,410
which is what we

30
00:01:46,420 --> 00:01:48,490
talk a little bit about people's machines is sort of

31
00:01:48,970 --> 00:01:50,670
i different classes of models

32
00:01:51,140 --> 00:01:52,480
if you look at the two models

33
00:01:53,250 --> 00:01:56,970
you know structurally they almost look the same but there are some subtle differences

34
00:01:57,570 --> 00:02:02,940
in particular deep boltzmann machines are undirected graphical models so you see there no causal links there

35
00:02:04,840 --> 00:02:10,430
in terms of the deep belief networks inference probably inference in these deep belief networks

36
00:02:10,600 --> 00:02:12,380
is problematic due to explaining away

37
00:02:13,390 --> 00:02:17,210
by the way before i forget i should also point out that i'm gonna put my slides on the web

38
00:02:17,900 --> 00:02:20,100
this evening so you can basically have access

39
00:02:20,550 --> 00:02:21,450
these slides as well

40
00:02:22,920 --> 00:02:28,260
um so deep belief networks inference deep belief networks the proper inference is quite problematic

41
00:02:28,260 --> 00:02:32,680
due to something called explaining away so people basically do some approximation to it

42
00:02:33,570 --> 00:02:37,880
there is also greedy pre-training so if you want training deep belief networks typically you

43
00:02:37,880 --> 00:02:40,010
just go to the trail is bilayers

44
00:02:40,470 --> 00:02:44,780
there is no joint optimization of all the layers right that's that's that's a drawback

45
00:02:44,780 --> 00:02:48,350
of these models and then if you look at the approximate inference algorithm for deep

46
00:02:48,350 --> 00:02:50,350
belief networks there's no bottom-up top-down

47
00:02:50,950 --> 00:02:51,750
so these are sort of

48
00:02:52,270 --> 00:02:56,650
kind of differences that both missions were designed to to address

49
00:02:57,380 --> 00:02:59,420
and i'm gonna show you some of them

50
00:03:01,270 --> 00:03:05,800
so what is it both machine it's basically markov random fields um

51
00:03:06,520 --> 00:03:09,280
u half dependencies between hidden variables

52
00:03:09,860 --> 00:03:11,850
right but all connections undirected

53
00:03:12,610 --> 00:03:13,710
so what's the difference between

54
00:03:14,390 --> 00:03:16,410
restricted both machines and enables machines well

55
00:03:17,140 --> 00:03:22,200
if you look at deep boltzmann machines you basically have these two additional terms in the definition of the model

56
00:03:22,730 --> 00:03:28,210
and these traditional terms these actually modeling correlations between each one managed to manage to energy

57
00:03:31,620 --> 00:03:33,970
very much similar to what deep belief nets are trying to do

58
00:03:36,150 --> 00:03:37,240
there's one particular

59
00:03:39,380 --> 00:03:44,040
structure in these models is that's you can combine bottom-up and top-down so if you

60
00:03:44,040 --> 00:03:48,770
look at the probability of a particular variable he knew being wrong it depends on

61
00:03:48,770 --> 00:03:49,920
what's coming from above

62
00:03:50,390 --> 00:03:51,430
and what's coming from below

63
00:03:52,610 --> 00:03:56,910
and this is unlike many other existing models if you look convolutional neural nets models

64
00:03:57,070 --> 00:04:00,040
or if you look deep belief networks so if you look at age max

65
00:04:00,040 --> 00:04:00,810
the nodes

66
00:04:00,830 --> 00:04:03,310
the next lecture hall at all

67
00:04:03,330 --> 00:04:05,520
by that come

68
00:04:05,540 --> 00:04:08,040
when they are not

69
00:04:08,100 --> 00:04:14,810
any of the you compute dissimilarity function which is based on the feature vector

70
00:04:19,620 --> 00:04:23,830
at their value school could

71
00:04:23,850 --> 00:04:28,060
which is operated on children that

72
00:04:28,190 --> 00:04:31,690
so this request functional is a lie

73
00:04:34,810 --> 00:04:37,020
until the complete

74
00:04:37,080 --> 00:04:38,750
if you could

75
00:04:38,770 --> 00:04:39,500
can you

76
00:04:45,410 --> 00:04:46,690
so far

77
00:04:47,390 --> 00:04:49,460
support vector

78
00:04:53,750 --> 00:04:57,460
the next step is nice

79
00:04:57,480 --> 00:05:00,040
this is a popular model

80
00:05:00,140 --> 00:05:01,830
my mother

81
00:05:01,850 --> 00:05:05,890
it's simple to implement and it is also

82
00:05:05,890 --> 00:05:06,810
there is

83
00:05:06,810 --> 00:05:11,290
well one of the reasons quite popular in that

84
00:05:13,730 --> 00:05:15,500
simple computations

85
00:05:17,060 --> 00:05:17,940
but here

86
00:05:17,940 --> 00:05:22,290
full text classification and i will give examples

87
00:05:25,690 --> 00:05:27,160
i think it

88
00:05:27,170 --> 00:05:30,210
i would give an example would used

89
00:05:30,230 --> 00:05:31,040
such an

90
00:05:31,310 --> 00:05:36,230
the classifier with music for motion detection sex

91
00:05:38,890 --> 00:05:39,930
it can

92
00:05:39,940 --> 00:05:41,000
not that

93
00:05:41,790 --> 00:05:43,120
in if the at

94
00:05:43,140 --> 00:05:46,000
it's quite useful

95
00:05:50,040 --> 00:05:56,850
the posterior probability that a new previously unseen objects belong to a certain class

96
00:05:56,910 --> 00:05:58,410
given the features

97
00:05:58,810 --> 00:06:00,460
of the object

98
00:06:00,460 --> 00:06:07,620
is computed the the problem is that individual features are related to the class

99
00:06:08,500 --> 00:06:13,660
it could be clear you need more people so compute x

100
00:06:13,690 --> 00:06:15,250
the probability

101
00:06:15,270 --> 00:06:17,430
so given

102
00:06:17,520 --> 00:06:20,250
object representation

103
00:06:20,270 --> 00:06:24,290
according to the feature vector or individual words

104
00:06:24,310 --> 00:06:26,190
we compute the probability

105
00:06:27,600 --> 00:06:32,390
class of a given configuration of

106
00:06:32,440 --> 00:06:37,230
this a generative model we use these rules

107
00:06:39,350 --> 00:06:44,830
translate into the probability that would give us

108
00:06:46,310 --> 00:06:47,910
which we can easily

109
00:06:49,670 --> 00:06:56,370
so it's broken at some prior probabilities

110
00:06:56,370 --> 00:06:58,540
and by the rules these

111
00:06:58,540 --> 00:07:06,330
by the probability that this configuration certain of course

112
00:07:06,350 --> 00:07:09,430
it is if we do ranking

113
00:07:09,440 --> 00:07:10,890
well actually

114
00:07:10,940 --> 00:07:13,710
we compute the most likely

115
00:07:15,410 --> 00:07:17,940
it is defined to be that of course

116
00:07:17,960 --> 00:07:19,250
we can

117
00:07:19,290 --> 00:07:23,140
it's all in the way

118
00:07:25,410 --> 00:07:28,060
two weeks this equation

119
00:07:28,100 --> 00:07:31,390
but still the probability that does that

120
00:07:31,410 --> 00:07:32,790
of word

121
00:07:32,960 --> 00:07:36,190
of these words p

122
00:07:36,370 --> 00:07:39,120
let's see people

123
00:07:39,140 --> 00:07:41,640
given a certain cluster

124
00:07:41,640 --> 00:07:45,440
there is very difficult to compute it

125
00:07:46,660 --> 00:07:48,210
well we assume

126
00:07:48,230 --> 00:07:51,370
independent of the features

127
00:07:51,390 --> 00:07:54,140
which language is not

128
00:07:54,290 --> 00:07:56,960
always valid

129
00:07:56,960 --> 00:08:00,480
so we can add to more

130
00:08:00,500 --> 00:08:02,440
the probability of the certain

131
00:08:04,120 --> 00:08:05,330
class g

132
00:08:05,430 --> 00:08:09,750
what w i put in the sky classification

133
00:08:09,750 --> 00:08:18,990
author of the

134
00:08:19,010 --> 00:08:22,790
today is going to be 1 of the more difficult lectures so

135
00:08:22,870 --> 00:08:28,050
but on your thinking caps as they say elementary school the topic is going be

136
00:08:28,050 --> 00:08:35,960
what's called a convolution

137
00:08:36,390 --> 00:08:43,950
the convolution something very peculiar that you functions together 3rd function but has its own

138
00:08:43,950 --> 00:08:45,300
special symbol

139
00:08:45,330 --> 00:08:51,660
effort to asterisk is the universal symbols used for that

140
00:08:51,700 --> 00:08:58,340
so that's the dual function of t which bears very little resemblance to the ones

141
00:08:58,340 --> 00:09:02,020
effort distorted where are

142
00:09:02,400 --> 00:09:07,140
I'm going to give you the formula for it right 1st that there are 2

143
00:09:07,140 --> 00:09:11,440
ways of motivating and both the important

144
00:09:11,450 --> 00:09:15,670
there is a formal motivation which is why it's talked into this section on plus

145
00:09:15,690 --> 00:09:22,590
transform and the formal motivation is the following suppose we start with all plus transform

146
00:09:22,610 --> 00:09:26,390
2 different from those 2 functions

147
00:09:26,470 --> 00:09:33,470
now the most natural question to ask is plus transform a really a pain to

148
00:09:33,470 --> 00:09:36,190
calculate is from

149
00:09:36,410 --> 00:09:41,990
old will plus transforms is it easy to get new ones and the 1st thing

150
00:09:41,990 --> 00:09:47,340
1 would cost something functions as easy I give you some transforms but a more

151
00:09:47,340 --> 00:09:54,940
natural question would be suppose I wanna multiply effort ingenuity is this sum hopefully some

152
00:09:54,940 --> 00:10:00,180
neat formula by multiply the productivity take a product of these 2 is that some

153
00:10:00,180 --> 00:10:06,860
neat formula for all plus transform of that product that would simplify life greater

154
00:10:07,000 --> 00:10:11,470
and the answer is there is no such formula and then it will be

155
00:10:13,120 --> 00:10:17,940
well we will not give up entirely suppose we ask the other questions

156
00:10:19,180 --> 00:10:25,410
I instead I multiply that will plus transform could that be related to something I

157
00:10:25,410 --> 00:10:33,410
cook up out of Africa ingenuity could it be the something about fugitive and that's

158
00:10:33,410 --> 00:10:39,660
what the convolution is for the answer is that this time this G of X

159
00:10:39,690 --> 00:10:48,000
turns out to be the water transform of convolution the convolutions and that's 1 way

160
00:10:48,000 --> 00:10:52,160
of defining it is the thing that you should functionality but you should put in

161
00:10:52,160 --> 00:10:57,120
there in order that it's will plus transform turned out to be the product of

162
00:10:57,120 --> 00:10:58,790
that event types you

163
00:11:02,470 --> 00:11:07,340
I'll give you the moment the formula for it but I think I will

164
00:11:07,370 --> 00:11:15,800
1 of the quarter minutes of wealth through the of motivation as to why this

165
00:11:15,800 --> 00:11:18,640
should be such a formula

166
00:11:18,750 --> 00:11:22,620
now I will calculate this out to the end because I don't have time but

167
00:11:22,620 --> 00:11:26,500
here's the reason why they should be such a formula and you might suspect and

168
00:11:26,500 --> 00:11:28,410
therefore will be worth looking for

169
00:11:29,060 --> 00:11:35,060
it's because remember I told you with all plus transform came from the plus transform

170
00:11:35,060 --> 00:11:40,900
was a continuous analog overpowers areas so that when you ask a general question like

171
00:11:40,900 --> 00:11:45,650
that place to look for is if you know what analogous ideas state doesn't work

172
00:11:45,760 --> 00:11:52,180
something like that were there so here I have a power series summation A x

173
00:11:52,180 --> 00:11:53,420
to the end

174
00:11:53,440 --> 00:11:57,220
remember you can write this in computer notation is a then to make it look

175
00:11:57,220 --> 00:11:59,850
like effort FFT

176
00:12:00,030 --> 00:12:06,090
and the analog is between and is turned into T when you turn a power

177
00:12:06,090 --> 00:12:13,030
series into plus transform and X gets turned into the negative that's 1 formula distance

178
00:12:13,190 --> 00:12:13,560
the other

179
00:12:14,160 --> 00:12:18,780
OK so there's a formula for f of x this is the analogue of plus

180
00:12:18,790 --> 00:12:26,660
transport and similarly G of X here is summation b and x to the end

181
00:12:26,680 --> 00:12:33,570
now again the naive question would be well suppose I multiply the 2 coefficient corresponding

182
00:12:33,570 --> 00:12:34,690
coefficients together

183
00:12:35,420 --> 00:12:41,310
and add up that power series summation a and B n times x to be

184
00:12:44,100 --> 00:12:48,320
is that somehow at some related to f and g and of course everybody knows

185
00:12:48,320 --> 00:12:50,700
the answer to that is the the relation with

186
00:12:54,600 --> 00:13:00,240
but suppose instead I multiply these 2 guys well in that case I'll get a

187
00:13:00,500 --> 00:13:07,940
new power series I don't know what it's coefficients let's write them down was just

188
00:13:07,940 --> 00:13:10,060
call them and

189
00:13:10,190 --> 00:13:14,710
so what I'm asking is that this corresponds to the product of the 2 will

190
00:13:14,750 --> 00:13:23,210
plus transform and what I want to know is is there a formula

191
00:13:23,270 --> 00:13:29,270
which says that CNN is equal to something that can be calculated calculated out of

192
00:13:29,270 --> 00:13:31,000
the AI and clear

193
00:13:32,640 --> 00:13:35,480
know the answer to that is yes there is

194
00:13:35,560 --> 00:13:41,620
and the formulas is called the convolution but now

195
00:13:41,650 --> 00:13:45,480
you could figure out this formula yourself you figure

196
00:13:45,840 --> 00:13:53,960
anyone smart enough to be interested in the question in the 1st place is smart

197
00:13:53,960 --> 00:13:57,760
enough to figure out what the formula is and will give you great pleasure to

198
00:13:57,760 --> 00:14:00,980
see that it's just like the formula for the convolution that I'm going to give

199
00:14:00,980 --> 00:14:02,940
you that

200
00:14:03,310 --> 00:14:08,860
so what is the formula for the convolutions OK hang on right there you not

201
00:14:08,860 --> 00:14:09,620
going to like it

202
00:14:10,700 --> 00:14:16,720
but you didn't like the formula plus transforming the are usual wiser grown up getting

203
00:14:16,720 --> 00:14:21,100
it but it's a mouthful to swallow is something you get used to slowly and

204
00:14:21,100 --> 00:14:25,240
you'll get used to the convolution equally

205
00:14:25,250 --> 00:14:35,330
so what is the convolution of and it's a function calculated according to the corresponding

206
00:14:35,370 --> 00:14:38,420
it's a function of T

207
00:14:38,450 --> 00:14:42,510
it's the integral from 0 to 3 all

208
00:14:42,570 --> 00:14:46,620
half of you you is a dummy variable because I know it's going to be

209
00:14:46,620 --> 00:14:53,680
integrated out when I do the great G of the might the think

210
00:14:55,370 --> 00:15:00,360
that's it I didn't make it up and just very bad news

211
00:15:09,240 --> 00:15:14,480
40 do when you see a formula that seems to you

212
00:15:15,050 --> 00:15:20,520
well courses try calculating just just to get some feeling for what kind of guy

213
00:15:20,530 --> 00:15:20,830
you know

214
00:15:21,520 --> 00:15:26,460
what was that was let's try some examples of what it was calculated

215
00:15:26,770 --> 00:15:29,150
what would be a modest beginning

216
00:15:29,400 --> 00:15:34,220
was calculated convolution of RTD with itself

217
00:15:34,240 --> 00:15:38,560
0 barrier let's calculate the convolution just so that you can tell the difference of

218
00:15:38,560 --> 00:15:40,560
the with the square

219
00:15:42,010 --> 00:15:43,940
a t squared retrieved articles

220
00:15:43,980 --> 00:15:50,330
but the convolution is symmetric F star G is the same thing as she start

221
00:15:50,370 --> 00:15:54,800
with but that that explicitly I forgot to last period so tell the guys who

222
00:15:54,800 --> 00:15:57,390
came to 1 o'clock lecture you know something that they

223
00:15:59,540 --> 00:16:08,330
well that's a theory it's community this operation is computed other words now

224
00:16:08,330 --> 00:16:15,080
with latent semantic analysis this mmr reranker since performed

225
00:16:15,090 --> 00:16:18,560
consistently better than the other two

226
00:16:18,580 --> 00:16:21,270
and then things another

227
00:16:21,280 --> 00:16:22,460
feature way

228
00:16:22,480 --> 00:16:27,570
explore a little bit is that because IMF staff reports was written in such a

229
00:16:27,570 --> 00:16:28,670
way that

230
00:16:28,690 --> 00:16:33,690
each section contains different paragraphs and in

231
00:16:33,700 --> 00:16:35,580
each paragraph since

232
00:16:35,580 --> 00:16:41,990
relatively independent and the first sentence of each paragraph is kind of seems to be

233
00:16:41,990 --> 00:16:44,210
kind of

234
00:16:44,650 --> 00:16:51,420
condensed content in in that sentence to represent the whole paragraph so we explore a

235
00:16:51,420 --> 00:16:56,170
little bit this paragraph lead based method and the result is

236
00:16:57,250 --> 00:16:58,640
better than

237
00:16:58,950 --> 00:17:01,960
previous methods

238
00:17:02,430 --> 00:17:05,430
but this this is

239
00:17:05,440 --> 00:17:10,100
this is even if we sum up with some will try to summarize the report

240
00:17:10,100 --> 00:17:16,790
paragraph by paragraph if the position feature is only giving a we to one where

241
00:17:16,790 --> 00:17:22,380
that which means the centroid method plays a more significant role in that case

242
00:17:22,390 --> 00:17:26,950
the result is not so much better but only one u

243
00:17:26,990 --> 00:17:28,340
he is

244
00:17:28,340 --> 00:17:36,600
he much higher rates to the position feature the results even improves much better

245
00:17:37,960 --> 00:17:39,480
and so

246
00:17:40,060 --> 00:17:46,350
i don't have some much time but i think we can now report

247
00:17:47,650 --> 00:17:50,420
sure some of the insights

248
00:17:51,820 --> 00:17:55,590
we can see through these experiments

249
00:17:56,160 --> 00:18:02,120
so all the change of the parameters of the summarisation methods in the ranking

250
00:18:02,410 --> 00:18:07,500
the reranker has an evident effect on summary of those

251
00:18:07,630 --> 00:18:14,740
and compare rate has much more significant effort on the summary content sometimes it is

252
00:18:14,750 --> 00:18:16,150
not so obvious

253
00:18:16,170 --> 00:18:21,880
well just from this to this kind of evaluation is this kind of quantitative evaluations

254
00:18:21,880 --> 00:18:23,550
but when you look at

255
00:18:23,600 --> 00:18:26,360
when you really look at the out food

256
00:18:26,370 --> 00:18:34,410
o yourself you can see that the ten percent summary much much more better than

257
00:18:34,420 --> 00:18:39,410
the five percent summary but these these these you can you cannot show is it's

258
00:18:39,410 --> 00:18:43,670
not showing this kind of quantitative evaluation methods

259
00:18:44,320 --> 00:18:51,530
and then different IDF database seems that's not have a significant effect

260
00:18:51,640 --> 00:18:53,820
on the evaluation results

261
00:18:53,990 --> 00:18:59,660
however this is also related to what kind of compression rate you you take

262
00:18:59,670 --> 00:19:05,440
the problem is that in the end comparing with the baseline methods the MEAD methods

263
00:19:05,450 --> 00:19:08,590
is not evidently better

264
00:19:10,070 --> 00:19:14,340
so there might be a problem with the summarisation methods are there might be a

265
00:19:14,340 --> 00:19:19,290
problem with the evaluation methods and i don't know what is exactly the problem and

266
00:19:19,480 --> 00:19:20,830
and here

267
00:19:21,290 --> 00:19:24,840
to hear about your opinions about it

268
00:19:24,860 --> 00:19:31,780
and and the with the semantic analysis evaluation we know there is there is a

269
00:19:31,930 --> 00:19:38,140
limitation it has to do with what kind of semantic spaces available for to use

270
00:19:38,140 --> 00:19:39,960
because the results can change

271
00:19:39,980 --> 00:19:46,720
can change dramatically depends on the sixth semantics space you use

272
00:19:46,730 --> 00:19:52,850
and then these lexical overlapping methods they are designed only for comparing

273
00:19:52,930 --> 00:19:57,160
system extracts to human extract means

274
00:19:57,350 --> 00:19:59,170
the the one that the

275
00:19:59,190 --> 00:20:03,970
the model summary hard appears summary compared to

276
00:20:03,980 --> 00:20:10,890
our extracts themselves they take origin of sentences from the document they are not like

277
00:20:10,890 --> 00:20:12,450
written by stuff

278
00:20:12,570 --> 00:20:15,550
and stuff not that kind of summary

279
00:20:15,580 --> 00:20:18,300
so so it is not

280
00:20:18,360 --> 00:20:20,080
well it tries to

281
00:20:20,080 --> 00:20:22,550
you know actually that is quite remarkable in the sense that

282
00:20:22,950 --> 00:20:26,360
just because some of the people who are involved in that really famous it's making

283
00:20:26,360 --> 00:20:30,870
bayesian nonparametrics very fashionable suddenly with people who were not interested in what

284
00:20:31,500 --> 00:20:33,360
but there are some really neat results here

285
00:20:33,950 --> 00:20:34,830
i just would like to

286
00:20:37,360 --> 00:20:38,090
to know roughly

287
00:20:38,700 --> 00:20:41,140
what what questions are being asked in this area

288
00:20:41,720 --> 00:20:43,940
and what kind of results you know what

289
00:20:44,510 --> 00:20:47,140
it's kind of involved in formulating these results

290
00:20:50,250 --> 00:20:50,810
okay so

291
00:20:52,690 --> 00:20:54,350
well if drawn here is a cartoon

292
00:20:56,980 --> 00:20:58,640
of the set of probability measures

293
00:21:00,930 --> 00:21:06,400
in this case this would be actually is a set of probability measures over three possible events and so on

294
00:21:06,900 --> 00:21:07,500
like a

295
00:21:08,340 --> 00:21:09,650
say a coin was

296
00:21:10,410 --> 00:21:14,960
three sites are like say coin with two sides and it can land on the edge right this

297
00:21:15,740 --> 00:21:18,100
this is heads tails this is no

298
00:21:21,450 --> 00:21:24,000
now you can imagine that if you have like if you have a forty went

299
00:21:24,030 --> 00:21:25,730
then you would get something like guitar trader

300
00:21:26,130 --> 00:21:27,120
right this is the simplex

301
00:21:27,840 --> 00:21:29,870
this is the domain of cherokee distribution if you will

302
00:21:30,550 --> 00:21:31,880
and if you may if you view

303
00:21:32,590 --> 00:21:34,510
at more and more events in the general case

304
00:21:34,990 --> 00:21:37,570
of of probability distributions is usually infinite

305
00:21:38,010 --> 00:21:39,370
then you really get something that's

306
00:21:39,950 --> 00:21:40,540
i mean that the

307
00:21:42,160 --> 00:21:43,840
description is really something like that

308
00:21:44,090 --> 00:21:46,410
trying only in in a very high dimensional space

309
00:21:48,370 --> 00:21:48,990
okay so this

310
00:21:49,500 --> 00:21:52,590
these these every single point in the set here is

311
00:21:53,100 --> 00:21:54,370
the probability distribution

312
00:21:55,190 --> 00:21:56,680
and the corner points

313
00:21:57,120 --> 00:22:02,110
the extreme points of probabilities on a single event which place all the mass on a single event can

314
00:22:04,440 --> 00:22:04,900
now what is

315
00:22:05,320 --> 00:22:06,140
what is the model

316
00:22:06,630 --> 00:22:10,900
statistical model statistical model is a family of probability distributions

317
00:22:12,790 --> 00:22:14,160
like the family of all gasses

318
00:22:16,050 --> 00:22:17,540
so you can imagine this you to be

319
00:22:17,980 --> 00:22:18,980
the set of probability

320
00:22:19,740 --> 00:22:22,380
represent a set of probability distributions on the real line

321
00:22:22,970 --> 00:22:27,710
and then the set of all guassian distributions would be some subset of the and i've i've

322
00:22:28,750 --> 00:22:30,750
i've i've drawn that here and include

323
00:22:31,850 --> 00:22:34,220
so there is some subset of the set of probability distributions

324
00:22:34,700 --> 00:22:36,130
and then if we have a particular

325
00:22:37,590 --> 00:22:39,780
some particular model is a single point here

326
00:22:42,680 --> 00:22:43,310
all right now

327
00:22:45,150 --> 00:22:46,320
let's say not

328
00:22:48,820 --> 00:22:51,490
the distribution that actually has generated our data

329
00:22:52,200 --> 00:22:56,160
we don't know which was which distribution that is but there are some true distribution

330
00:22:56,160 --> 00:22:57,730
which is actually generated our data

331
00:22:58,280 --> 00:23:00,120
and now we try to find this distribution

332
00:23:01,250 --> 00:23:02,170
positing a model

333
00:23:03,670 --> 00:23:06,170
and then estimates the model parameter from the data

334
00:23:07,070 --> 00:23:11,790
and if if we have chosen the right model and are estimators good then as

335
00:23:11,790 --> 00:23:14,580
we see more and more data we would get close to the estimate that we

336
00:23:14,580 --> 00:23:17,130
get we get closer and closer to the true distribution

337
00:23:18,570 --> 00:23:21,210
very basic idea of of statistical inference

338
00:23:22,470 --> 00:23:23,800
all right now so what can happen

339
00:23:24,310 --> 00:23:24,620
if we

340
00:23:25,420 --> 00:23:30,030
if we choose a model then that's an assumption right it's a modeling assumption

341
00:23:32,330 --> 00:23:37,940
one thing that could happen is that we get lucky an hour the true distribution is actually inside the model

342
00:23:39,650 --> 00:23:40,010
that's good

343
00:23:40,690 --> 00:23:42,400
and then there's another case which is

344
00:23:42,850 --> 00:23:43,890
the true distribution is

345
00:23:44,330 --> 00:23:49,300
outside the model and in statistics is this case is called misspecification now so we

346
00:23:49,300 --> 00:23:50,910
say that the model is misspecified

347
00:23:51,450 --> 00:23:54,110
if the true distribution of the data is not inside the model

348
00:23:55,100 --> 00:23:55,930
you know you can ask

349
00:23:57,970 --> 00:24:00,970
in that case we can ask if we if we see more and more data

350
00:24:01,580 --> 00:24:05,710
how quickly do that's our estimate converge to the true model how much data do

351
00:24:05,710 --> 00:24:07,690
we need to get a good estimate of what the

352
00:24:08,160 --> 00:24:09,040
the real distribution is

353
00:24:09,640 --> 00:24:11,930
and in this case we can additionally ask what

354
00:24:13,660 --> 00:24:15,290
since the true distribution is not in there

355
00:24:15,790 --> 00:24:20,300
policy estimator going to be and so i would certainly like to see in this case is

356
00:24:21,910 --> 00:24:26,480
our estimate the estimate you get something that's over here that's as close as possible

357
00:24:26,480 --> 00:24:29,750
to this rather than something over here because we have to define what close means

358
00:24:29,750 --> 00:24:30,530
in this space but

359
00:24:35,730 --> 00:24:36,660
and this is in

360
00:24:38,120 --> 00:24:41,340
one of the year all the more theoreti from a theoretical point you one of

361
00:24:41,340 --> 00:24:43,350
the motivations for using nonparametric models

362
00:24:43,870 --> 00:24:45,050
because essentially if you use

363
00:24:45,050 --> 00:24:46,260
and humans

364
00:24:46,270 --> 00:24:47,960
are able to understand

365
00:24:51,670 --> 00:24:52,890
single plants

366
00:24:52,940 --> 00:25:01,090
hundred milliseconds this by the number of objects with different pose scale ends up instead

367
00:25:01,150 --> 00:25:03,380
are within the scene

368
00:25:04,460 --> 00:25:09,050
we distinguish in computer vision three main tasks for recognition

369
00:25:09,080 --> 00:25:11,670
we call categorisation within cluster

370
00:25:11,680 --> 00:25:15,200
classification for reasons why we want to distinguish between

371
00:25:15,290 --> 00:25:18,440
faces versus faces

372
00:25:19,180 --> 00:25:21,130
or identification

373
00:25:21,170 --> 00:25:24,710
two five within class classification

374
00:25:24,750 --> 00:25:29,310
and parameter estimation we have some although and one of the parameters

375
00:25:30,060 --> 00:25:33,240
first all is for instance of

376
00:25:33,240 --> 00:25:37,070
or when one is banned the degree of

377
00:25:37,090 --> 00:25:38,530
at this

378
00:25:38,530 --> 00:25:40,460
fashion inspiration and so on

379
00:25:40,480 --> 00:25:46,270
and here here we are going to look for categorisation task

380
00:25:47,440 --> 00:25:49,750
we are going to discuss

381
00:25:49,760 --> 00:25:52,790
about categorization specifically

382
00:25:52,820 --> 00:25:55,770
turning an image in two thousand visual words

383
00:25:56,820 --> 00:26:00,280
categorisation possible

384
00:26:00,280 --> 00:26:03,040
and focusing on categorisation

385
00:26:03,120 --> 00:26:06,760
image representation although i'm going to discuss

386
00:26:06,780 --> 00:26:09,510
originating from this community

387
00:26:09,520 --> 00:26:11,130
where the back of

388
00:26:11,150 --> 00:26:12,800
one more than

389
00:26:12,850 --> 00:26:16,860
i've been used as a tool to describe the commons

390
00:26:18,360 --> 00:26:20,890
histogram of the frequency

391
00:26:20,910 --> 00:26:23,470
of some words within

392
00:26:23,790 --> 00:26:25,310
the documents

393
00:26:26,070 --> 00:26:27,630
every word

394
00:26:28,890 --> 00:26:32,000
wasn't in it and set of topics

395
00:26:35,750 --> 00:26:38,090
is use it more often than others

396
00:26:38,110 --> 00:26:39,350
and for instance

397
00:26:39,360 --> 00:26:40,970
the were brain

398
00:26:42,930 --> 00:26:45,210
optikom are more users in

399
00:26:45,980 --> 00:26:47,520
it is in government

400
00:26:48,640 --> 00:26:50,190
business commenced

401
00:26:50,190 --> 00:26:53,390
and the point is that that if i give you

402
00:26:53,450 --> 00:26:57,330
people like these

403
00:26:57,450 --> 00:26:59,150
that's like these

404
00:26:59,160 --> 00:27:02,710
there are many things to read if you want understand

405
00:27:03,070 --> 00:27:06,380
what is the topic of the document

406
00:27:06,450 --> 00:27:09,020
but if i give you some

407
00:27:09,060 --> 00:27:10,330
he works

408
00:27:10,350 --> 00:27:11,780
light rain

409
00:27:13,120 --> 00:27:15,240
the brain and so on

410
00:27:15,240 --> 00:27:16,980
probably you can guess

411
00:27:16,990 --> 00:27:19,470
which is that category

412
00:27:19,520 --> 00:27:22,260
of the documents by looking just

413
00:27:25,270 --> 00:27:26,540
thank you

414
00:27:28,360 --> 00:27:30,500
this means that

415
00:27:30,510 --> 00:27:33,820
even without syntactic

416
00:27:33,840 --> 00:27:35,030
we can guess

417
00:27:35,030 --> 00:27:36,030
by looking

418
00:27:36,050 --> 00:27:38,850
so more within the government

419
00:27:40,100 --> 00:27:42,760
the idea is that you and i said to works

420
00:27:44,270 --> 00:27:46,500
by looking at which words

421
00:27:46,520 --> 00:27:49,880
the vocabulary correspond to which topics

422
00:27:49,900 --> 00:27:53,040
a system is trained to look at the histogram

423
00:27:53,080 --> 00:27:55,330
of the frequency of the words

424
00:27:55,350 --> 00:27:56,790
to understand

425
00:27:56,840 --> 00:28:00,070
which is the cost of the document

426
00:28:00,140 --> 00:28:01,640
and in representing

427
00:28:03,660 --> 00:28:05,270
we can two problems

428
00:28:05,270 --> 00:28:07,040
the first one is that

429
00:28:07,060 --> 00:28:09,810
comments have different lengths

430
00:28:09,830 --> 00:28:11,330
and the second one

431
00:28:11,380 --> 00:28:12,480
is that

432
00:28:12,490 --> 00:28:13,580
there are some

433
00:28:13,610 --> 00:28:15,760
works that are not

434
00:28:15,770 --> 00:28:18,850
realising you've got to discriminate between documents

435
00:28:18,880 --> 00:28:23,040
first and or themes and so on

436
00:28:25,190 --> 00:28:26,730
text mining community

437
00:28:27,960 --> 00:28:29,070
they introduced

438
00:28:29,100 --> 00:28:31,690
they TF IDF

439
00:28:32,810 --> 00:28:33,580
two normally

440
00:28:35,500 --> 00:28:39,070
histogram with respect to the number of the words

441
00:28:39,090 --> 00:28:40,510
within a document

442
00:28:41,240 --> 00:28:42,450
the idea

443
00:28:42,500 --> 00:28:44,910
is use it too

444
00:28:44,930 --> 00:28:48,130
caps two and going out

445
00:28:49,270 --> 00:28:50,860
not significant words

446
00:28:50,890 --> 00:28:53,250
for discrimination

447
00:28:53,260 --> 00:28:56,840
and in computer vision

448
00:28:56,860 --> 00:29:00,680
recently we have using this similarity

449
00:29:00,680 --> 00:29:03,790
to build representation for images

450
00:29:03,820 --> 00:29:05,270
we want to look for

451
00:29:07,360 --> 00:29:09,060
so we we should

452
00:29:09,110 --> 00:29:12,040
look for some markets in the image

453
00:29:12,050 --> 00:29:15,280
and we should describe this marking the image

454
00:29:15,290 --> 00:29:17,080
and and then we should be able to

455
00:29:17,110 --> 00:29:20,110
regional vocabulary

456
00:29:20,130 --> 00:29:28,130
and then we can build an histogram over the visual vocabulary to represent the images

457
00:29:28,140 --> 00:29:32,040
so you know that the bag words model in a very intuitive way

458
00:29:32,050 --> 00:29:33,810
what we're doing is

459
00:29:33,830 --> 00:29:35,560
we have an image

460
00:29:35,580 --> 00:29:37,180
and we're using

461
00:29:37,220 --> 00:29:39,770
a collection of building blocks

462
00:29:39,790 --> 00:29:42,610
and here the building blocks are simple

463
00:29:42,660 --> 00:29:43,620
just to

464
00:29:43,630 --> 00:29:45,300
o point out that

465
00:29:45,310 --> 00:29:47,520
most of the bag of visual words model

466
00:29:47,520 --> 00:29:50,410
you know take into account the geometric information

467
00:29:50,430 --> 00:29:52,820
of the image

468
00:29:52,840 --> 00:29:55,370
and here are the basic representations

469
00:29:55,380 --> 00:29:58,210
you have a different class of of the image

470
00:29:58,210 --> 00:30:02,030
you somehow find a dictionary of the building blocks

471
00:30:02,080 --> 00:30:04,380
and you are presented your class

472
00:30:04,390 --> 00:30:10,560
as an histogram over the visual vocabulary so the problem is in finding a way

473
00:30:11,850 --> 00:30:13,460
these are works

474
00:30:14,440 --> 00:30:19,550
defending the and when we have a vocabulary which should be in the vocabulary

475
00:30:19,580 --> 00:30:25,300
four images

476
00:30:25,320 --> 00:30:29,750
you could say this slide summarise the framework

477
00:30:30,010 --> 00:30:33,030
one of the framework is used in computer vision for

478
00:30:33,050 --> 00:30:34,750
categorisation task

479
00:30:36,050 --> 00:30:39,110
this is composed by two park the landing party

480
00:30:39,120 --> 00:30:41,130
and the initial part

481
00:30:41,140 --> 00:30:42,960
and deleting parts

482
00:30:42,970 --> 00:30:44,450
what he wanted to

483
00:30:45,100 --> 00:30:47,480
to learn visual vocabulary

484
00:30:47,510 --> 00:30:49,020
that often

485
00:30:49,070 --> 00:30:51,480
useful to describe the image

486
00:30:52,430 --> 00:30:54,600
and that the visual vocabulary

487
00:30:54,630 --> 00:30:59,030
we chose struck some features from the image

488
00:30:59,040 --> 00:31:00,360
we should represent

489
00:31:00,380 --> 00:31:03,140
this feature

490
00:31:03,150 --> 00:31:04,320
after that

491
00:31:04,340 --> 00:31:05,860
we can

492
00:31:09,100 --> 00:31:10,030
by using

493
00:31:10,030 --> 00:31:12,710
the previously learned representation

494
00:31:12,720 --> 00:31:15,030
and when a new part

495
00:31:15,810 --> 00:31:17,490
we can represent

496
00:31:17,490 --> 00:31:21,760
by the previous learning vocabulary and then we can use

497
00:31:22,980 --> 00:31:24,440
the models

498
00:31:24,450 --> 00:31:28,260
for categorisation

499
00:31:28,310 --> 00:31:34,550
we look at the representation issue first and they are forced steps in representing images

500
00:31:34,550 --> 00:31:42,140
and it's also clear that heartbeat signal something very complicated so it's some some three-dimensional

501
00:31:42,150 --> 00:31:44,250
preceding vector

502
00:31:44,270 --> 00:31:48,350
so it should be a subspace

503
00:31:48,810 --> 00:31:50,700
and there some

504
00:31:52,090 --> 00:31:54,210
mother components here

505
00:31:54,710 --> 00:32:00,370
and some some for things so with this you can see that that these would

506
00:32:01,290 --> 00:32:03,590
willing to trust

507
00:32:03,630 --> 00:32:08,530
the other is not

508
00:32:08,550 --> 00:32:13,160
and if you actually compared to these up in this case then the matrix is

509
00:32:13,160 --> 00:32:18,650
rather grey and black which means that there's no real stable projection that can be

510
00:32:21,040 --> 00:32:25,260
you can use any other algorithm says well OK so

511
00:32:25,270 --> 00:32:28,600
so we have used this as well with any g

512
00:32:28,610 --> 00:32:31,930
and in energy often you have

513
00:32:31,940 --> 00:32:38,200
the modern images have three hundred channels so you do an ICA

514
00:32:42,100 --> 00:32:45,450
it's very hard to actually show three hundred channels tour

515
00:32:45,600 --> 00:32:47,640
two and d

516
00:32:47,640 --> 00:32:50,990
because it doesn't have the time to look at all these things and after looking

517
00:32:50,990 --> 00:32:54,910
at twenty channels anyway it's

518
00:32:54,940 --> 00:32:58,000
you see always the same thing

519
00:33:00,250 --> 00:33:06,570
the question is can we reduce the number of interesting channels such that

520
00:33:06,620 --> 00:33:08,660
then these have to look at less

521
00:33:08,770 --> 00:33:12,260
and this is a particular experiment that i don't want to

522
00:33:12,320 --> 00:33:19,180
go into very much but it's measures somebody in the electric field magnetic fields in

523
00:33:21,040 --> 00:33:23,940
OK so

524
00:33:25,270 --> 00:33:27,000
this is the

525
00:33:27,010 --> 00:33:31,650
and MEGA it so she you don't have the slide in the new

526
00:33:31,680 --> 00:33:32,540
the note

527
00:33:32,570 --> 00:33:33,950
so it's

528
00:33:33,990 --> 00:33:35,360
shielded room

529
00:33:35,370 --> 00:33:39,900
and this is the shielded room PTB in berlin

530
00:33:40,300 --> 00:33:42,210
and you see that plane are

531
00:33:42,240 --> 00:33:43,210
m g

532
00:33:43,210 --> 00:33:45,710
that so this is the door

533
00:33:45,710 --> 00:33:55,100
which contains all these squid electrodes and it's about forty something i think forty nine

534
00:33:55,150 --> 00:33:57,760
and the d c

535
00:33:57,790 --> 00:33:59,440
imaging measures

536
00:33:59,450 --> 00:34:08,230
other than the image e four d actually but seen here tries to find the

537
00:34:08,230 --> 00:34:14,010
senior currents in your brain somewhere here in the auditory cortex

538
00:34:14,050 --> 00:34:15,760
OK so

539
00:34:15,800 --> 00:34:22,110
an image is very expensive because about two or three million depends on the type

540
00:34:22,150 --> 00:34:25,540
in the room like that also another half million

541
00:34:25,550 --> 00:34:30,600
so so so this is the reason why we wouldn't be using this for BCI

542
00:34:30,600 --> 00:34:36,210
experiments it's something that you cannot put into the home somebody

543
00:34:37,450 --> 00:34:40,070
so here's the separability matrices

544
00:34:40,120 --> 00:34:42,660
and if you look at them

545
00:34:44,000 --> 00:34:44,720
i mean

546
00:34:44,730 --> 00:34:49,940
in this case for example jade you see you see this there's no structure and

547
00:34:49,950 --> 00:34:53,700
it all looks very bad

548
00:34:53,710 --> 00:34:57,230
in this case you see that there some structure

549
00:34:57,260 --> 00:34:58,650
and there's a big

550
00:34:58,690 --> 00:35:02,100
the noise subspace

551
00:35:02,150 --> 00:35:06,340
and it's now interesting to i mean you will remove this one

552
00:35:06,360 --> 00:35:12,170
and you try to get these one-dimensional subspaces or towards a low dimensional subspaces and

553
00:35:12,170 --> 00:35:14,760
look at them

554
00:35:14,790 --> 00:35:21,200
OK so

555
00:35:21,210 --> 00:35:23,470
in fact i showed show here too

556
00:35:23,490 --> 00:35:26,120
three one-dimensional

557
00:35:32,180 --> 00:35:36,240
so this is the drift

558
00:35:36,260 --> 00:35:39,460
of the energy which is normal

559
00:35:39,510 --> 00:35:42,170
it's very strong signals to some

560
00:35:42,170 --> 00:35:43,910
particularly noise

561
00:35:43,920 --> 00:35:45,690
fifty hertz

562
00:35:45,710 --> 00:35:50,450
because it's measured in germany and here is the

563
00:35:50,470 --> 00:35:52,100
the third component

564
00:35:52,100 --> 00:35:55,380
which is the signal of interest

565
00:35:55,400 --> 00:35:56,490
if you look at it

566
00:35:57,850 --> 00:36:00,390
so so what did we do

567
00:36:00,410 --> 00:36:06,450
and actually i will not discuss very much about the the medical content but what

568
00:36:07,190 --> 00:36:12,130
what did we do is we did and i see a decomposition of these

569
00:36:12,550 --> 00:36:15,640
forty of old channels

570
00:36:15,660 --> 00:36:22,250
we found the separability matrices there were grey and for some algorithms

571
00:36:22,270 --> 00:36:24,140
which means unreliable

572
00:36:24,160 --> 00:36:27,450
and structured father algorithms to means

573
00:36:27,480 --> 00:36:29,280
quite reliable

574
00:36:29,340 --> 00:36:36,090
and then we looked at the most reliable components and the most reliable components turn

575
00:36:36,100 --> 00:36:39,570
out to be addressed artefact

576
00:36:39,590 --> 00:36:41,660
fifty hertz artefact

577
00:36:41,680 --> 00:36:46,060
and the signal of interest and it makes perfect sense that the artifact is something

578
00:36:46,060 --> 00:36:51,710
very stable the plugs always on the road in the same place

579
00:36:52,380 --> 00:36:58,900
and just to get you an idea of this this is a dipole

580
00:36:59,560 --> 00:37:01,970
in the auditory cortex

581
00:37:03,950 --> 00:37:07,210
the paradigm was that they were

582
00:37:07,230 --> 00:37:15,720
thirty seconds thirty seconds of music but in fact it was but not

583
00:37:15,730 --> 00:37:18,470
not to the

584
00:37:18,470 --> 00:37:19,560
for example

585
00:37:19,580 --> 00:37:21,200
we use

586
00:37:21,220 --> 00:37:23,230
beta one four dead

587
00:37:23,290 --> 00:37:28,910
bid for home my beats eight four TV

588
00:37:28,910 --> 00:37:35,500
and for watching and affordable all-cash function i don't know what function was used by

589
00:37:35,500 --> 00:37:39,430
by the smart guy maybe you simply

590
00:37:39,470 --> 00:37:42,060
court of all characters in this war

591
00:37:42,100 --> 00:37:43,160
and then

592
00:37:43,450 --> 00:37:49,060
divided by module a all this all the time

593
00:37:49,080 --> 00:37:50,850
and for this

594
00:37:50,870 --> 00:37:52,770
beta in place

595
00:37:52,790 --> 00:37:55,230
you saying OK we're using

596
00:37:55,250 --> 00:37:59,950
fair enough long binary strings and this is that all these

597
00:38:00,000 --> 00:38:01,750
and what we can do

598
00:38:01,770 --> 00:38:07,790
we can incorporate all documents simply stating b because the took using back towards the

599
00:38:07,790 --> 00:38:11,500
interesting thing about words in the document

600
00:38:11,520 --> 00:38:13,160
so what you are doing

601
00:38:14,520 --> 00:38:18,000
again this example

602
00:38:18,020 --> 00:38:18,560
that is

603
00:38:18,640 --> 00:38:21,230
reading a book so we

604
00:38:21,310 --> 00:38:23,640
thirteen two one

605
00:38:24,520 --> 00:38:26,600
that belongs to daddy

606
00:38:26,620 --> 00:38:28,470
said setting to one

607
00:38:28,470 --> 00:38:30,660
the to do that belongs to

608
00:38:30,680 --> 00:38:33,870
he is if not skipping stop words

609
00:38:35,140 --> 00:38:39,580
retaining one what is going to ring et cetera et cetera so

610
00:38:39,620 --> 00:38:43,100
if all matrix cells flowers

611
00:38:43,120 --> 00:38:44,720
if selected

612
00:38:44,770 --> 00:38:48,160
pretty long binary stream

613
00:38:48,160 --> 00:38:50,390
and if you

614
00:38:50,830 --> 00:38:54,350
selected good test functions with is not that

615
00:38:54,370 --> 00:38:57,200
uniformly distributed this one

616
00:38:57,250 --> 00:38:59,230
what we do

617
00:38:59,290 --> 00:39:00,990
it will compress

618
00:39:02,700 --> 00:39:07,040
are also you know matrix so and all columns semantic

619
00:39:07,060 --> 00:39:09,140
became much smaller

620
00:39:09,140 --> 00:39:13,600
number of call a number of roles in this matter be simply a number of

621
00:39:13,600 --> 00:39:15,580
these here

622
00:39:15,580 --> 00:39:18,080
OK so it seems like

623
00:39:18,990 --> 00:39:23,600
she a lot the information we work with this

624
00:39:24,470 --> 00:39:28,640
we put them all together in one of them is that some words in our

625
00:39:28,660 --> 00:39:33,470
the same the same crash called how we can search and the structure and doesn't

626
00:39:33,470 --> 00:39:34,970
have any sense

627
00:39:35,140 --> 00:39:37,140
but actually can

628
00:39:37,200 --> 00:39:39,790
so if you have query monday

629
00:39:39,830 --> 00:39:42,640
which again from this example

630
00:39:42,680 --> 00:39:46,810
so what we can do we can things that this is a document

631
00:39:46,830 --> 00:39:50,500
and create a binary string for this

632
00:39:50,580 --> 00:39:55,160
then you can our compressed metrics

633
00:39:55,200 --> 00:39:57,700
and if you look for

634
00:39:57,750 --> 00:39:59,290
we're all in this

635
00:39:59,310 --> 00:40:01,850
in this new compress matrix

636
00:40:01,890 --> 00:40:04,770
well we have to be set

637
00:40:04,770 --> 00:40:11,700
you know that in the target documents this these should be why because the documents

638
00:40:11,700 --> 00:40:13,810
that contain this work

639
00:40:13,850 --> 00:40:19,310
of course of course you have some number of documents because we have this correlation

640
00:40:19,500 --> 00:40:21,410
because different kinds

641
00:40:21,430 --> 00:40:23,700
i have this one b

642
00:40:23,770 --> 00:40:28,120
that's not really in this world but what we can do that

643
00:40:28,140 --> 00:40:29,720
what we can do that

644
00:40:29,770 --> 00:40:30,870
we can look

645
00:40:30,870 --> 00:40:33,100
through this metric

646
00:40:33,180 --> 00:40:34,410
we can

647
00:40:35,680 --> 00:40:40,500
we can select only documents that

648
00:40:40,540 --> 00:40:46,060
are supposed to have a lot of work that we're looking for

649
00:40:46,160 --> 00:40:51,910
then we can a lot of these documents and naive scan over the documents and

650
00:40:51,910 --> 00:40:53,080
that its

651
00:40:53,140 --> 00:40:55,330
again our search engine

652
00:40:55,330 --> 00:41:02,640
now can judge and in war and peace and all russian literature on these cheap

653
00:41:04,080 --> 00:41:06,450
and this can be done in

654
00:41:06,470 --> 00:41:08,080
ten seconds

655
00:41:08,100 --> 00:41:13,000
you have a small structure if you have if you like this all this has

656
00:41:13,000 --> 00:41:16,560
functions and is selected for long

657
00:41:16,640 --> 00:41:19,270
binary stream

658
00:41:19,330 --> 00:41:20,410
and that's it

659
00:41:20,450 --> 00:41:23,390
we created the structure compress and extraction

660
00:41:23,450 --> 00:41:24,620
the fun

661
00:41:24,680 --> 00:41:27,220
this signature file

662
00:41:27,220 --> 00:41:28,750
i saw

663
00:41:28,770 --> 00:41:32,720
articles about signature files maybe

664
00:41:32,770 --> 00:41:37,470
and we are talking about signature in this application because there are other signature file

665
00:41:37,470 --> 00:41:38,830
hosting service

666
00:41:38,850 --> 00:41:42,000
but in this application last time i saw

667
00:41:42,020 --> 00:41:46,330
six articles about signature file maybe ten years ago

668
00:41:47,500 --> 00:41:51,520
i tried to implement signature first member thirty years ago

669
00:41:51,560 --> 00:41:54,720
and they held

670
00:41:54,770 --> 00:41:57,560
a lot of helpful during this period of time

671
00:41:58,870 --> 00:42:03,160
right now it's not so important because you will see that

672
00:42:04,720 --> 00:42:09,810
this is a good combination by this my this money guy

673
00:42:09,830 --> 00:42:11,750
actually combine

674
00:42:11,750 --> 00:42:13,660
three technologies

675
00:42:13,660 --> 00:42:15,600
that we do

676
00:42:15,640 --> 00:42:18,160
learned later except scan

677
00:42:18,850 --> 00:42:20,500
user has

678
00:42:20,520 --> 00:42:22,620
will filter and can

679
00:42:22,640 --> 00:42:27,910
we already talked about can and will talk about passion littlefield on the next lecture

680
00:42:27,910 --> 00:42:30,430
signature file like

681
00:42:30,470 --> 00:42:31,970
jumping ahead

682
00:42:31,990 --> 00:42:35,500
you know that you can compress and you don't know how to do this well

683
00:42:35,500 --> 00:42:37,060
but we press

684
00:42:37,080 --> 00:42:40,390
more than they did not seem to compress into the

685
00:42:40,410 --> 00:42:44,450
a lot of the compression because he lost some information about you managed to fix

686
00:42:44,450 --> 00:42:46,830
this you know that even if you

687
00:42:46,870 --> 00:42:50,540
losing some information you can you can manage to a

688
00:42:50,560 --> 00:42:52,250
search for

689
00:42:52,660 --> 00:42:57,770
signature file is very good structure from this point of view but this description that

690
00:42:57,790 --> 00:43:02,370
there's a naive implementation of all this stuff it's a bit outdated and now people

691
00:43:02,390 --> 00:43:03,410
are thinking

692
00:43:03,430 --> 00:43:05,750
from this point of view

693
00:43:05,770 --> 00:43:07,180
o thing about

694
00:43:07,220 --> 00:43:08,560
in the

695
00:43:09,520 --> 00:43:12,450
if your extremely smart guy

696
00:43:12,470 --> 00:43:15,500
i think that at this moment say OK

697
00:43:15,540 --> 00:43:16,620
it's enough

698
00:43:16,640 --> 00:43:19,180
i tried this stuff i tried this

699
00:43:19,200 --> 00:43:24,730
you've brought me many new collection that is going to be indexed by

700
00:43:24,790 --> 00:43:28,160
by my signature flower because

701
00:43:28,160 --> 00:43:29,320
we have seen

702
00:43:29,370 --> 00:43:33,400
our characterization in terms of continuous optimisation

703
00:43:33,450 --> 00:43:37,600
and we have seen how great going hold to find dominance

704
00:43:37,610 --> 00:43:43,440
now we are shown application of dominant set which is not our first application whether

705
00:43:43,450 --> 00:43:45,250
many different ones actually

706
00:43:45,340 --> 00:43:46,910
we applied

707
00:43:46,930 --> 00:43:50,050
the dominant set framework to image segmentation

708
00:43:50,070 --> 00:43:51,680
the problem is very

709
00:43:51,700 --> 00:43:54,480
easy to state image segmentation is the classical

710
00:43:54,500 --> 00:43:57,210
clustering problem actually are given an image

711
00:43:57,300 --> 00:44:00,890
the objects to be clustered pixels

712
00:44:00,920 --> 00:44:03,500
and then you want to segment

713
00:44:03,560 --> 00:44:04,690
image into

714
00:44:04,710 --> 00:44:06,920
maximally coherent

715
00:44:07,040 --> 00:44:08,390
this is one of the

716
00:44:08,460 --> 00:44:11,030
basic problems in computer vision

717
00:44:11,230 --> 00:44:14,930
this is the first step for several other albums like

718
00:44:14,930 --> 00:44:21,680
object recognition and many other more sophisticated so it's very important basic problem

719
00:44:22,930 --> 00:44:28,060
this in this application the objects to be clustered are just because

720
00:44:28,970 --> 00:44:32,110
the graph we create is just a graph where

721
00:44:32,130 --> 00:44:34,160
it's always about it

722
00:44:34,190 --> 00:44:35,100
this graph

723
00:44:35,220 --> 00:44:40,580
and we have an edge weights which reflect the similarity between the

724
00:44:42,250 --> 00:44:45,300
we have defined the notion of

725
00:44:45,330 --> 00:44:48,520
the dominant set but we don't talk about how

726
00:44:48,570 --> 00:44:49,830
i can partition

727
00:44:49,830 --> 00:44:52,800
set of objects into the

728
00:44:52,820 --> 00:44:56,140
so there are several ways to do that but probably

729
00:44:56,190 --> 00:44:57,800
the simplest one

730
00:44:57,910 --> 00:45:02,220
what we call which was called the of strategy

731
00:45:02,270 --> 00:45:05,240
it's very simple actually i just fine

732
00:45:05,300 --> 00:45:11,130
the first dominant set which is usually quite large in terms of numbers of objects

733
00:45:11,270 --> 00:45:12,360
i just wrote

734
00:45:12,380 --> 00:45:17,130
the dominant set the way we apply the argument over the remaining part of the

735
00:45:17,130 --> 00:45:19,440
graph and so on and so forth

736
00:45:19,490 --> 00:45:24,000
until a cluster of objects made there's almost

737
00:45:24,180 --> 00:45:25,540
this is actually

738
00:45:25,630 --> 00:45:29,490
very in a simple strategy that we actually used in this

739
00:45:29,540 --> 00:45:31,890
image segmentation experiments

740
00:45:31,940 --> 00:45:34,780
we found segment for the weights so

741
00:45:34,800 --> 00:45:36,020
there are several

742
00:45:36,110 --> 00:45:39,730
alternatives to this actually the end of the talk will also describe

743
00:45:39,750 --> 00:45:41,290
the more sophisticated way

744
00:45:41,310 --> 00:45:42,450
to find

745
00:45:42,470 --> 00:45:44,440
maybe overlapping

746
00:45:45,420 --> 00:45:49,220
some applications it's important to have overlapping class

747
00:45:49,320 --> 00:45:51,780
OK so

748
00:45:51,840 --> 00:45:56,190
in order to apply this framework we have to define a notion of similarity of

749
00:45:56,190 --> 00:45:59,350
course you are just in the last year of pixels

750
00:45:59,620 --> 00:46:01,820
so what we did actually was to

751
00:46:01,880 --> 00:46:05,570
to do exactly the very same experiments that

752
00:46:05,570 --> 00:46:08,660
i a group from berkeley did a few years ago

753
00:46:08,690 --> 00:46:16,130
you can dramatic example she was overtly they introduced what they call the normalized cut

754
00:46:17,800 --> 00:46:19,530
four aways classes

755
00:46:19,540 --> 00:46:21,500
which is based on the idea of

756
00:46:21,560 --> 00:46:26,500
trying to find the best possible way of captain graph into pieces

757
00:46:26,500 --> 00:46:29,000
you know two or more pieces in order to

758
00:46:29,000 --> 00:46:33,700
i have a coherent regions and this leads to a spectral class

759
00:46:33,800 --> 00:46:38,650
the nice thing about this approach is that it starts from a graph theoretic problems

760
00:46:38,650 --> 00:46:39,920
how can i

761
00:46:39,970 --> 00:46:46,480
divide the graph into two max career regions and it ends up with the characterisation

762
00:46:46,500 --> 00:46:49,350
in terms of i eigenspectrum

763
00:46:49,480 --> 00:46:53,450
OK so in that paper was apparently people i'm is the leading journal in computer

764
00:46:53,450 --> 00:47:00,100
vision and pattern recognition is that police transactions machine in that paper which actually was

765
00:47:00,100 --> 00:47:02,450
published in the conference years before

766
00:47:02,470 --> 00:47:05,150
they did this some experiments

767
00:47:05,190 --> 00:47:11,000
to see whether they normalized gap approach were not only missing station

768
00:47:11,010 --> 00:47:12,450
so the proposal

769
00:47:12,560 --> 00:47:16,230
is similarity measure which is quite common actually

770
00:47:16,250 --> 00:47:19,450
are we distinguish they distinguish actually we

771
00:47:19,470 --> 00:47:24,410
we we put ourselves in exactly the same setting and there's just

772
00:47:24,660 --> 00:47:27,850
let us compare with the results

773
00:47:27,910 --> 00:47:29,600
so for example they

774
00:47:29,820 --> 00:47:34,340
use of graylevel images so this means that each pixel

775
00:47:34,380 --> 00:47:37,220
it is characterized by a single number

776
00:47:37,260 --> 00:47:42,590
between say zero and one he says we're actually sees the degree level of that

777
00:47:43,660 --> 00:47:46,090
or we might have

778
00:47:46,140 --> 00:47:53,160
color images in this case each pixel is characterized by three numbers and depending on

779
00:47:53,160 --> 00:47:57,900
the space color on the color is that you're using a different tribal

780
00:47:57,910 --> 00:48:02,100
they use what is called the age as we tried

781
00:48:02,100 --> 00:48:02,970
and so

782
00:48:02,990 --> 00:48:07,060
associated which big so there is a fact three numbers

783
00:48:07,080 --> 00:48:10,520
this two similarity measures work really well

784
00:48:11,410 --> 00:48:13,600
in cases where you have

785
00:48:13,620 --> 00:48:15,700
a texture image

786
00:48:15,710 --> 00:48:21,240
c is the role whatever they don't work because if you apply either of these

787
00:48:21,430 --> 00:48:27,010
similarity measures the zebra will be segmented into black and white striped

788
00:48:28,040 --> 00:48:33,360
the similarity between pixels in this case we need something more sophisticated instead of missions

789
00:48:33,410 --> 00:48:38,100
similar direct similarity between two weeks we try to measure the similarity

790
00:48:38,290 --> 00:48:42,350
all the windows of excel centre on that it's

791
00:48:42,400 --> 00:48:43,980
so what they did was to use

792
00:48:43,990 --> 00:48:46,280
a bank of filters

793
00:48:47,990 --> 00:48:52,340
filters and that scales and this ends up with the vector

794
00:48:52,350 --> 00:48:54,180
associated to each

795
00:48:54,200 --> 00:48:57,970
the article itself which gives information about

796
00:48:57,990 --> 00:49:02,420
OK so text information around the particular pixel

797
00:49:02,450 --> 00:49:07,390
case we have two vectors coming the back of filters and just measure the similarity

798
00:49:07,390 --> 00:49:09,470
between the field the

799
00:49:09,520 --> 00:49:12,410
the vectors using the very same for actually

800
00:49:12,470 --> 00:49:14,970
we take the exponential minds

801
00:49:15,020 --> 00:49:18,220
the quadratic distance euclidean distance between

802
00:49:18,240 --> 00:49:23,030
the the feature vector and then we divide this parameter which is given

803
00:49:23,080 --> 00:49:24,090
this is the way

804
00:49:24,100 --> 00:49:30,540
white standard way to transform it distance interesting

805
00:49:32,040 --> 00:49:36,290
these are some results this is typically used by

806
00:49:36,350 --> 00:49:38,160
berkeley grew

807
00:49:38,210 --> 00:49:40,140
this is the whole image

808
00:49:40,150 --> 00:49:42,720
this is what we get is in dominant sets

809
00:49:42,720 --> 00:49:45,830
this is what they got using normalized cut

810
00:49:46,720 --> 00:49:48,340
one you can see is

811
00:49:48,450 --> 00:49:51,660
the qualitative results results much better

812
00:49:51,660 --> 00:49:53,640
all the types of

813
00:49:53,640 --> 00:50:00,860
of measurements error rate is false negative plus false positive over total number of examples

814
00:50:01,440 --> 00:50:04,540
instead of using here you can use the couple

815
00:50:04,550 --> 00:50:09,160
so rate false alarm rate or hit rate precision or hit rate fraction of selected

816
00:50:09,160 --> 00:50:17,640
to people depending on the domain people use different different things you can use balanced

817
00:50:17,640 --> 00:50:20,910
error rate which is the average of the errors of the positive class and other

818
00:50:20,910 --> 00:50:26,640
than ever the negative class and in you know text processing people use the f

819
00:50:26,640 --> 00:50:32,240
measure which i'm writing you hear from memory and you can then also vary the

820
00:50:32,240 --> 00:50:36,200
threshold as i said before and if you the special you can plot the ROC

821
00:50:36,200 --> 00:50:39,620
curve you can plot the rate lift curve or you can plot the precision recall

822
00:50:39,620 --> 00:50:42,910
curves this gives you know

823
00:50:42,920 --> 00:50:44,760
an idea

824
00:50:44,780 --> 00:50:47,320
what is a risk functional

825
00:50:47,340 --> 00:50:51,840
so risk functional is is the function of the parameters of the learning machine assessing

826
00:50:51,840 --> 00:50:55,040
how much it is expected to fail

827
00:50:55,240 --> 00:50:57,450
on the given task

828
00:50:57,470 --> 00:51:03,220
and giving examples already right focused mutation problem and the error rate is going to

829
00:51:03,220 --> 00:51:08,390
be a functional one minus the area under the rocker so whatever

830
00:51:08,400 --> 00:51:15,820
measurement or objective you set yourself about the performance of your learning machine that is

831
00:51:15,820 --> 00:51:22,740
going to define for you a risk functional for regression people use most of you

832
00:51:22,760 --> 00:51:24,020
mean square

833
00:51:24,030 --> 00:51:28,320
so in that case you taking the output of your learning machine and the desired

834
00:51:28,320 --> 00:51:34,370
output making the different squaring and averaging so it's the average discrepancy

835
00:51:34,380 --> 00:51:42,660
between the target we want to achieve and what you obtain from your learning machine

836
00:51:42,680 --> 00:51:45,700
what do you irresponsible well that's

837
00:51:45,740 --> 00:51:51,370
that's used for training writer once you've defined how you're going to be measuring how

838
00:51:51,370 --> 00:51:53,370
are you going to be performing

839
00:51:53,400 --> 00:51:57,220
then you want to optimise its and so you're going to

840
00:51:57,260 --> 00:52:00,740
very the weights of your learning machine

841
00:52:00,780 --> 00:52:06,740
such that you're going to be optimizing the risk functional you want to minimise the

842
00:52:06,740 --> 00:52:11,800
risk and that can be done with a variety of methods including gradient descent mathematical

843
00:52:11,800 --> 00:52:18,510
programming assimilated milling genetic algorithms all means of going down

844
00:52:18,530 --> 00:52:21,010
in the beaks you know

845
00:52:21,890 --> 00:52:27,470
surface of the that the risk functional defines in parameter space so this is a

846
00:52:27,620 --> 00:52:32,160
very simple drawing in which you know considering the there is only one parameter in

847
00:52:32,160 --> 00:52:33,830
your learning machine

848
00:52:33,880 --> 00:52:38,960
your risk functional defines know peaks and valleys and what you want is to find

849
00:52:39,020 --> 00:52:45,160
the minimum which is the w star your weight your optimal weight in reality you

850
00:52:45,160 --> 00:52:52,260
have occasional spanners space we have very complex high dimensional surface which trying to the

851
00:52:53,740 --> 00:52:58,320
and so we're going to be continuing that in the next lectures so will be

852
00:52:58,320 --> 00:53:04,550
defining risk functional and defining ways of optimizing it typically gradient descent

853
00:53:04,720 --> 00:53:12,600
in summary for you know this introduction lecture with linear threshold units that are analogous

854
00:53:12,600 --> 00:53:20,880
to neurons we can build many different kinds of learning machines including linear discrimination it's

855
00:53:21,080 --> 00:53:29,120
functions which are you know this which is special case in the naive bayes algorithm

856
00:53:29,130 --> 00:53:33,980
the kernel methods and that they're all methods that are linear in the parameters not

857
00:53:33,980 --> 00:53:40,960
necessarily in the input components and neural networks which are non-linear both in the parameters

858
00:53:40,960 --> 00:53:50,600
and the not in the input components decision trees that have also elementary nodes that

859
00:53:50,610 --> 00:53:59,240
to make a simple linear decisions don't just thresholds in that case and the architecture

860
00:53:59,240 --> 00:54:04,980
of the learning machines have hypercritical hyper parameters of which we are going to be

861
00:54:04,980 --> 00:54:10,640
talking in the next lectures and those may include the choice of the basis functions

862
00:54:10,640 --> 00:54:16,760
so which phi functions you're using in the perception which kernel using in the kernel

863
00:54:16,760 --> 00:54:24,280
machine the number of units and how arranged in the case of the neural network

864
00:54:24,560 --> 00:54:30,360
and learning meets it means fitting the parameters of the weights and also the hyper

865
00:54:30,360 --> 00:54:37,500
parameters and one has to be aware of the fit versus robustness trade-off from so

866
00:54:37,500 --> 00:54:44,400
it's not necessarily best to obtain a decision boundary that learns exactly well the training

867
00:54:44,400 --> 00:54:49,160
examples what needs to care which we need to care about is how well we're

868
00:54:49,160 --> 00:54:54,120
going to be doing on our future examples that we haven't seen training data and

869
00:54:54,130 --> 00:54:59,940
in that respect using a linear decision boundary sometimes better than using a more complex

870
00:54:59,940 --> 00:55:06,760
non-linear decision boundary and if you want you know to learn more about machine learning

871
00:55:06,760 --> 00:55:12,740
there are a lot of good sources which include the well-known textbook of duda hart

872
00:55:12,740 --> 00:55:20,960
installed one pattern classification the book of to show any interest freedman the elements of

873
00:55:21,090 --> 00:55:23,040
statistical learning

874
00:55:25,050 --> 00:55:32,870
days chapter that covers some of the material i've presented to you here in the

875
00:55:32,930 --> 00:55:38,740
the perspective of you know going from percent crimes two kernel methods and vice versa

876
00:55:39,470 --> 00:55:46,200
presenting some simple algorithm could have algorithm that presented today and some other ones and

877
00:55:46,220 --> 00:55:52,850
then there is the this book i mentioned before that using four and for the

878
00:55:52,850 --> 00:55:58,430
lectures that i'll be talking about this week and that includes also the city was

879
00:55:58,450 --> 00:56:01,350
the data of the of the challenge on

880
00:56:01,900 --> 00:56:03,820
the features section

881
00:56:03,840 --> 00:56:12,920
and so thanks very much for your attention and you will see

882
00:56:14,990 --> 00:56:23,430
what are you should know was talking too fast know looking at the clock in

883
00:56:23,450 --> 00:56:27,260
the morning

884
00:56:27,260 --> 00:56:30,550
anybody wants hold

885
00:56:31,680 --> 00:56:32,700
this problem

886
00:56:32,780 --> 00:56:35,490
that's question of

887
00:56:38,490 --> 00:56:40,890
you want that

888
00:56:40,900 --> 00:56:42,980
what about you know about

889
00:56:43,120 --> 00:56:48,240
that's actually that's good question first that because you don't know very much in the

890
00:56:48,240 --> 00:56:54,510
audience yet but so should have started is that we were thought that you have

891
00:56:54,830 --> 00:57:01,140
it so the whole asking you know we were euphoric which so who who in

892
00:57:01,140 --> 00:57:09,330
this class is still it's preparing a phd degree

893
00:57:09,330 --> 00:57:10,220
for each

894
00:57:10,240 --> 00:57:13,520
data point which tells us which clusters belong to

895
00:57:15,160 --> 00:57:19,860
now consists of setting the latent variables of these data cases to be equal to

896
00:57:19,860 --> 00:57:23,490
each other so when we think grouping we mean all of these might have the

897
00:57:23,490 --> 00:57:27,720
latent variables z equal to one of these might have the latent variables z equal

898
00:57:27,720 --> 00:57:31,510
to two and all these might have latent variables and equal to three that's what

899
00:57:31,520 --> 00:57:35,100
clustering really means in the latent variable settings and you can think of it just

900
00:57:35,100 --> 00:57:40,430
like classification with missing targets k and the same problem here

901
00:57:40,440 --> 00:57:46,480
in terms of dimensionality reduction factor model so another very common unsupervised learning problem is

902
00:57:46,480 --> 00:57:48,050
i show you some data

903
00:57:48,060 --> 00:57:53,150
the data normally lives in the two dimensional space here in this picture but you

904
00:57:53,150 --> 00:57:57,590
believe that the data has fewer degrees of freedom in two here we think it

905
00:57:57,590 --> 00:58:02,010
only has one degree of freedom plus noise and you'd like to find a low

906
00:58:02,050 --> 00:58:03,860
degree of freedom model

907
00:58:03,870 --> 00:58:06,540
it captures the data you actually saw

908
00:58:06,560 --> 00:58:09,480
so that's the kind of dimensionality reduction or

909
00:58:09,490 --> 00:58:15,770
factor model kind of problem and that's exactly like regression with missing targets

910
00:58:15,910 --> 00:58:20,390
so you can think of it as if i really knew this continuous

911
00:58:20,420 --> 00:58:24,630
underlying cause as the generated each data point in this would just be a regression

912
00:58:24,630 --> 00:58:30,250
problem right would just be trying to fit a straight line here but because i

913
00:58:30,250 --> 00:58:34,400
don't know that i have to do something a little bit more clever and n

914
00:58:34,400 --> 00:58:39,870
that involves the kinds of things that apple mentioned principal component analysis and factor analysis

915
00:58:39,870 --> 00:58:44,670
which we'll talk about the intellectual so both of these problems have this graphical model

916
00:58:44,670 --> 00:58:51,360
and unobserved latent variable and the observed data variable if this variable is discrete multinomial

917
00:58:51,360 --> 00:58:55,220
that it looks like clustering and this variable is continuous and it looks like dimensionality

918
00:58:58,730 --> 00:59:03,160
let's now see a little bit more about why

919
00:59:03,170 --> 00:59:05,330
the the learning problem is hard

920
00:59:05,340 --> 00:59:09,170
so as i said before

921
00:59:09,190 --> 00:59:13,650
we're going to focus on the case where some variables are always unobserved is going

922
00:59:13,660 --> 00:59:18,040
to be these edges so we have some joint distribution over x and z this

923
00:59:18,040 --> 00:59:21,590
is the data in the latent variable which we don't observe

924
00:59:21,610 --> 00:59:26,830
and we can rewrite it of course as a prior distribution on z

925
00:59:26,850 --> 00:59:29,540
so this might be your prior on you know

926
00:59:29,550 --> 00:59:34,080
each cluster how often you think it's used to populate it and then this is

927
00:59:34,080 --> 00:59:38,280
a conditional distribution what does the data look like if it's from cluster one if

928
00:59:38,280 --> 00:59:40,270
its own cluster two and so on

929
00:59:40,290 --> 00:59:44,090
OK now what we want to maximize is the law

930
00:59:45,270 --> 00:59:50,330
probability p of x right and to get that we have to sum over z

931
00:59:50,340 --> 00:59:54,240
and then when we take the we get this log of the sun OK and

932
00:59:54,240 --> 00:59:55,810
so when i try

933
00:59:55,920 --> 01:00:02,180
estimate these variables x invaders and there are no longer independent estimation problems

934
01:00:02,200 --> 01:00:08,180
in the first case i could just estimate pz this parameter theta z by just

935
01:00:08,180 --> 01:00:14,170
observing the Z's and fitting the distribution and could estimate all these parameters theta x

936
01:00:14,560 --> 01:00:18,710
by conditioning on z taking all the data cases where z equals one and fitting

937
01:00:18,750 --> 01:00:22,500
the data for those taking technology the data cases where z equals two and fitting

938
01:00:22,500 --> 01:00:23,810
the data for those and so on

939
01:00:23,860 --> 01:00:26,570
i can do that here these variables are now coupled

940
01:00:29,460 --> 01:00:33,600
what are we going to do well there's two

941
01:00:33,610 --> 01:00:35,920
approaches and actually

942
01:00:35,940 --> 01:00:39,070
although this slide is sort of making fun of

943
01:00:39,750 --> 01:00:45,010
naive approach the naive approach is actually sometimes just as good as the fancy algorithm

944
01:00:45,010 --> 01:00:49,720
approach to talk but when me pretend that we don't know that and let's let's

945
01:00:50,280 --> 01:00:52,570
make fun of the naive approach so

946
01:00:52,600 --> 01:00:55,570
we wanted do is we could say

947
01:00:55,580 --> 01:00:57,390
oh well

948
01:00:57,410 --> 01:00:58,920
like you know

949
01:00:58,940 --> 01:01:00,140
who cares

950
01:01:00,150 --> 01:01:03,300
so we have this gives some but you know

951
01:01:03,310 --> 01:01:07,560
we have seen complicated likelihood functions in the past it doesn't really bother us let's

952
01:01:07,560 --> 01:01:11,800
just take the gradient of the likelihood function and whatever the gradient is that's what

953
01:01:11,800 --> 01:01:16,530
the gradient is the eldest followed in maximum likelihood learning using gradient optimizers just like

954
01:01:16,530 --> 01:01:17,390
we always

955
01:01:17,410 --> 01:01:22,350
and that's fine you can actually compute the gradient of these models by taking the

956
01:01:22,350 --> 01:01:26,080
gradient of the log and you get a term which is one over the marginal

957
01:01:26,080 --> 01:01:30,700
probability and you can compute the gradient and attack it directly with the gradient optimizer

958
01:01:30,700 --> 01:01:31,890
of two things

959
01:01:31,940 --> 01:01:34,160
and the what starts boiling and is so

960
01:01:34,170 --> 01:01:36,150
that's when water and steam coexist

961
01:01:36,170 --> 01:01:37,450
what i'm saying

962
01:01:37,460 --> 01:01:40,700
certain conditions of pressure and temperature and volume

963
01:01:40,710 --> 01:01:41,740
so that

964
01:01:41,760 --> 01:01:45,260
water ice and steam will coexist

965
01:01:45,410 --> 01:01:50,660
that is a unique situation you cannot get to that for any other mean

966
01:01:50,690 --> 01:01:52,490
and that temperature we will call

967
01:01:52,510 --> 01:01:55,400
plus two seven three point one six

968
01:01:55,430 --> 01:01:58,430
in this absolute units

969
01:01:58,440 --> 01:02:01,890
so basically what you have done by going to absolute units is you shifted the

970
01:02:01,890 --> 01:02:05,720
zero to more natural point that all graphs meet

971
01:02:05,770 --> 01:02:07,740
then you define one degree

972
01:02:08,770 --> 01:02:12,390
to be so that two seven point one six of calvin brings you to the

973
01:02:12,390 --> 01:02:15,300
triple point of water

974
01:02:15,340 --> 01:02:18,300
if you find that confusing i'm just saying the boiling point of water is not

975
01:02:18,300 --> 01:02:21,730
a fixed number you go to the mountains changes

976
01:02:21,750 --> 01:02:25,070
but only under one condition can water and ice

977
01:02:25,110 --> 01:02:28,010
and steam coexist

978
01:02:28,070 --> 01:02:31,910
that cannot be you cannot get that any other way to everybody will agree on

979
01:02:31,910 --> 01:02:38,060
that particular situation that will be called to seven point one six killed

980
01:02:38,070 --> 01:02:39,290
now that's the rule

981
01:02:40,260 --> 01:02:42,650
you can see degrees centigrade

982
01:02:42,660 --> 01:02:46,150
not supposed to degree kelvin

983
01:02:46,150 --> 01:02:48,710
that's a big deal made in a lot of books

984
01:02:48,800 --> 01:02:52,410
i keep forgetting so in fact i forgot again

985
01:02:52,410 --> 01:02:54,910
and nothing terrible has happened to me so i don't think

986
01:02:54,930 --> 01:02:58,880
you should pay too much attention to whether you can call something degree kelvin or

987
01:02:58,880 --> 01:03:00,630
simply count

988
01:03:00,630 --> 01:03:04,990
i think the purpose of languages to have no ambiguities when is degree kelvin and

989
01:03:04,990 --> 01:03:09,610
i find that you guys don't get confused i don't think that's the big find

990
01:03:09,750 --> 01:03:12,430
video i person will never

991
01:03:12,440 --> 01:03:14,880
great degree help

992
01:03:14,930 --> 01:03:18,410
but having said that i don't hold me to the standards i just don't feel

993
01:03:18,410 --> 01:03:20,750
any affiliation to this particular

994
01:03:20,770 --> 01:03:24,630
completely are artificial and in the convention

995
01:03:24,670 --> 01:03:28,440
but you have supposedly member if you take the GRE or something is not called

996
01:03:28,440 --> 01:03:30,600
the become

997
01:03:30,660 --> 01:03:32,870
OK so as far as we are concerned

998
01:03:32,870 --> 01:03:37,160
the kelvin scale is like the centigrade scale except zero is shifted

999
01:03:37,180 --> 01:03:38,630
two here

1000
01:03:38,640 --> 01:03:40,320
that's it

1001
01:03:40,480 --> 01:03:42,010
the temperature scale

1002
01:03:42,040 --> 01:03:43,430
we will use

1003
01:03:43,450 --> 01:03:46,550
the absolute temperature men i t from now on

1004
01:03:46,590 --> 01:03:48,640
i'm talking about health

1005
01:03:48,650 --> 01:03:51,640
sending a

1006
01:03:51,650 --> 01:03:54,370
now that's all about to eat

1007
01:03:54,410 --> 01:03:57,830
i mean all about temperature and i'm going to talk about he

1008
01:03:57,910 --> 01:04:00,590
he is denoted by the symbol q

1009
01:04:00,610 --> 01:04:02,290
you don't ask yourself

1010
01:04:02,310 --> 01:04:04,840
what are we talking about when we talk about heat

1011
01:04:04,860 --> 01:04:09,050
again let's use your intuitive sense what of what he is

1012
01:04:09,100 --> 01:04:12,150
you say a bucket of water i want heated up

1013
01:04:12,240 --> 01:04:14,800
and how do you do that you put the bucket on top of something else

1014
01:04:14,800 --> 01:04:16,460
which i think is hard

1015
01:04:16,480 --> 01:04:19,180
and when the two are brought together somehow

1016
01:04:19,220 --> 01:04:21,460
the water begins to feel hard to

1017
01:04:21,470 --> 01:04:23,410
so we say we the water

1018
01:04:23,420 --> 01:04:26,960
and we say we are transferred heat

1019
01:04:27,080 --> 01:04:30,320
people were not sure what really was being transferred

1020
01:04:30,360 --> 01:04:32,470
what is the best going from the store

1021
01:04:32,490 --> 01:04:34,260
the water

1022
01:04:35,100 --> 01:04:36,690
why is it that the store

1023
01:04:36,770 --> 01:04:40,580
it was not plugged in is getting cooler than the what is getting hot

1024
01:04:40,620 --> 01:04:47,490
there is this edit to call it the calorific fluid

1025
01:04:47,540 --> 01:04:49,020
they imagined

1026
01:04:49,040 --> 01:04:51,040
this is fluid

1027
01:04:51,080 --> 01:04:53,070
which is abundant in hot things

1028
01:04:53,080 --> 01:04:57,240
and not stubborn called things when you put hard called together this magical fluid

1029
01:04:57,290 --> 01:04:59,330
flows from optical

1030
01:04:59,350 --> 01:05:02,790
and in the process heaps of the call

1031
01:05:02,810 --> 01:05:06,270
and they decided to measured in calories

1032
01:05:06,290 --> 01:05:09,650
so you have to define what the calendar years the what you want to ask

1033
01:05:09,700 --> 01:05:12,950
how much he does it take to heat this back

1034
01:05:12,960 --> 01:05:15,510
so what

1035
01:05:15,520 --> 01:05:19,900
and the rule they made up was to find something called calorie

1036
01:05:19,910 --> 01:05:22,650
where the number of calories

1037
01:05:22,670 --> 01:05:24,810
you need

1038
01:05:24,810 --> 01:05:28,140
it's equal mass of water

1039
01:05:28,150 --> 01:05:31,800
and the change in temperature

1040
01:05:31,850 --> 01:05:35,420
that's going to be calories

1041
01:05:35,450 --> 01:05:40,410
the words if i had a container with ten grams of water

1042
01:05:40,410 --> 01:05:48,640
and the temperature went up i'm sorry this same as the water ingress

1043
01:05:48,650 --> 01:05:50,900
one gram of water

1044
01:05:50,940 --> 01:05:54,510
and you did something to within the temperature went the by seven degrees will have

1045
01:05:54,510 --> 01:06:01,450
by definition pumped in seven calories

1046
01:06:01,470 --> 01:06:03,730
if this was a kilogram of water

1047
01:06:03,750 --> 01:06:05,380
this would be called a

1048
01:06:05,390 --> 01:06:08,000
q locality

1049
01:06:08,100 --> 01:06:13,290
sometimes they use grams and calories sometimes use kilograms calories but

1050
01:06:13,330 --> 01:06:19,340
the definitions are consistently for particular on the ground particularly in the county

1051
01:06:20,820 --> 01:06:22,910
now suppose you say

1052
01:06:22,940 --> 01:06:24,840
i don't want to just talk about water

1053
01:06:24,850 --> 01:06:28,950
i want to talk what he thinks something else

1054
01:06:29,000 --> 01:06:30,210
maybe i one

1055
01:06:30,220 --> 01:06:33,180
he diagram of copper

1056
01:06:33,190 --> 01:06:34,960
so then you write down

1057
01:06:35,000 --> 01:06:37,320
the following rule

1058
01:06:37,330 --> 01:06:39,600
and one that he takes

1059
01:06:39,620 --> 01:06:41,750
he the anything

1060
01:06:41,790 --> 01:06:44,790
i think your own favourite material go

1061
01:06:44,830 --> 01:06:47,810
and now what he i think we can all appreciate

1062
01:06:47,820 --> 01:06:51,320
must be proportional the amount of stuff you're trying to heat up

1063
01:06:51,340 --> 01:06:54,150
that's our intuitive notion because i want to go gold

1064
01:06:54,170 --> 01:06:58,040
that takes some number calendars of second second identical chunk

1065
01:06:58,080 --> 01:07:00,620
by definition that predict the same number of calories

1066
01:07:00,660 --> 01:07:01,980
put them together

1067
01:07:01,990 --> 01:07:06,460
it is clear that whatever this gallery fluid is need double that

1068
01:07:06,480 --> 01:07:08,570
so it's got to be proportional

1069
01:07:08,590 --> 01:07:11,100
the mass of the substance

1070
01:07:11,110 --> 01:07:17,020
and it's got proportional to what you're aiming for namely an increase in temperature

1071
01:07:17,080 --> 01:07:18,920
but this is true for any substance

1072
01:07:18,950 --> 01:07:20,870
but it copper wood

1073
01:07:21,850 --> 01:07:24,980
i wonder what your heating it's true the heat needed is proportional to mass and

1074
01:07:24,980 --> 01:07:26,410
the temperature

1075
01:07:26,470 --> 01:07:29,870
so what is it that distinguishes one material from another

1076
01:07:29,910 --> 01:07:31,540
you put a number here

1077
01:07:31,560 --> 01:07:34,730
and that number is called the specific heat

1078
01:07:34,790 --> 01:07:36,980
specific is the property

1079
01:07:37,050 --> 01:07:44,840
of anything

1080
01:07:44,890 --> 01:07:48,260
we don't understand certain formulas will depend on

1081
01:07:48,290 --> 01:07:55,360
certain parameters in the genetic way and some things that depend on the activity

1082
01:07:55,370 --> 01:07:59,250
in fact this a similar quantity i mean maybe i'll take a second

1083
01:07:59,310 --> 01:08:02,270
if you go to liquid that i said with expanding you can do the same

1084
01:08:03,310 --> 01:08:04,630
take a ride

1085
01:08:04,630 --> 01:08:05,940
and start eating it

1086
01:08:05,960 --> 01:08:10,320
and ask which will expect

1087
01:08:10,330 --> 01:08:15,200
if i heated by someone built fifty

1088
01:08:15,260 --> 01:08:19,620
what will be proportional to anybody think of what it may be proportional to

1089
01:08:20,430 --> 01:08:24,640
depends on the original length of the right the ways that

1090
01:08:24,690 --> 01:08:33,350
when we think it's going to be proportional to the length of the right

1091
01:08:33,350 --> 01:08:40,310
yeah based on what it had before yes

1092
01:08:40,460 --> 01:08:43,460
it's kind i think the way you one way to say that it's taken me

1093
01:08:43,460 --> 01:08:44,620
to stay

1094
01:08:44,640 --> 01:08:46,050
expand some amount

1095
01:08:46,060 --> 01:08:50,180
put another major stick next to it that expand the same amount by definition of

1096
01:08:50,180 --> 01:08:51,520
identical things

1097
01:08:51,550 --> 01:08:56,210
for the two meters thick will expand by places much

1098
01:08:56,270 --> 01:08:58,880
so we put the length of that

1099
01:08:58,930 --> 01:09:02,610
so no matter what your heating block of wood block of steve this is true

1100
01:09:02,620 --> 01:09:04,700
but then the fact that he

1101
01:09:04,720 --> 01:09:07,920
has different effects on copper versus would

1102
01:09:07,940 --> 01:09:12,480
is so pain is indicated by putting in number here

1103
01:09:12,480 --> 01:09:15,560
alpha is called the coefficient of

1104
01:09:15,580 --> 01:09:18,540
linear expansion

1105
01:09:18,650 --> 01:09:21,040
and that depends on the material

1106
01:09:21,060 --> 01:09:26,290
these are true no matter what your heating

1107
01:09:26,330 --> 01:09:30,230
so the specific numbers this corporations this out is the coming going to come in

1108
01:09:30,230 --> 01:09:34,480
that those days when you're just making use of the

1109
01:09:34,490 --> 01:09:38,510
relations between the different genes you can improve by

1110
01:09:38,520 --> 01:09:44,010
what is currently going on in robotics is this collective inference if you want to

1111
01:09:44,020 --> 01:09:45,760
do semantic labeling

1112
01:09:45,810 --> 01:09:49,680
so you get a point plots and you would like to know if the tree

1113
01:09:49,680 --> 01:09:54,820
is the wall in the ground floor can i rather than there are

1114
01:09:54,830 --> 01:09:58,840
and then definitely you know that at least locally you should have something like the

1115
01:09:58,840 --> 01:10:03,800
smoothness assumption that one pixel voxel in terms of the class label is not so

1116
01:10:03,800 --> 01:10:06,780
much different from the neighbouring one right you

1117
01:10:06,790 --> 01:10:09,620
walk out of the building you're not expecting that there

1118
01:10:09,620 --> 01:10:11,230
road is all of a sudden

1119
01:10:11,250 --> 01:10:16,540
stopping and then for the next millimetres the tree and this again i wrote on

1120
01:10:16,540 --> 01:10:21,540
the wall in this tree and that we expect somehow because there is a kind

1121
01:10:21,540 --> 01:10:26,630
of manifold that locally everything rather than within are changing radical

1122
01:10:26,630 --> 01:10:31,010
so again to make use saying my neighboring pixels road i'm more likely to be

1123
01:10:31,010 --> 01:10:33,650
a road segment

1124
01:10:33,700 --> 01:10:34,810
right so

1125
01:10:34,890 --> 01:10:39,570
they also interesting because we may view here relations as constraints so it's not like

1126
01:10:39,570 --> 01:10:43,760
you just use the relation to propagate the emails theater

1127
01:10:44,700 --> 01:10:46,410
or another one where we

1128
01:10:46,420 --> 01:10:51,110
it can work with a whole new options for example web search so definitely if

1129
01:10:51,110 --> 01:10:53,020
i do a very good certain

1130
01:10:53,050 --> 01:10:54,420
documents that

1131
01:10:54,430 --> 01:10:59,060
and you can think about preference relations but definitely you also have to

1132
01:10:59,360 --> 01:11:05,140
reference relation between documents here and you know that one is relevant than similar ones

1133
01:11:05,140 --> 01:11:09,540
should be relevant well you may also use the similarity relations two

1134
01:11:09,550 --> 01:11:15,220
are completely different interesting questions like if you want to have a diversification

1135
01:11:15,220 --> 01:11:19,140
you know if i pretend that document should not present that one

1136
01:11:19,210 --> 01:11:23,570
so people are using relations all the time or even though they don't give it

1137
01:11:23,570 --> 01:11:28,070
in a statistical relational learning but somehow they do it already and we would just

1138
01:11:28,070 --> 01:11:30,290
want to go one step had make it more

1139
01:11:32,070 --> 01:11:37,020
and of course you can show that relational approach was really outperform standard approach of

1140
01:11:37,020 --> 01:11:40,650
course you have to pay the price that may be learning and outer inference is

1141
01:11:40,650 --> 01:11:45,690
taking a little bit more complex sometimes but it really pains doing

1142
01:11:45,730 --> 01:11:47,920
we skip that one

1143
01:11:47,920 --> 01:11:51,430
i think we can also skip that once topic model there

1144
01:11:51,450 --> 01:11:56,400
a lot of relational extension for that but you may also use these kinds of

1145
01:11:56,410 --> 01:12:00,640
the techniques developed within statistical relational learning with another

1146
01:12:00,650 --> 01:12:05,220
so for example if you look at how least my students spend their spare time

1147
01:12:05,890 --> 01:12:10,430
they really what youtube what i said well actually not only this but also while

1148
01:12:12,290 --> 01:12:18,750
they but also youtube is facing the problem of information broadcasts right so you want

1149
01:12:18,750 --> 01:12:20,270
to have the latest movie

1150
01:12:20,290 --> 01:12:23,790
the question is where can i don't know what is the best policy to download

1151
01:12:23,940 --> 01:12:27,950
if one of my friends already having some parts of the longer movie may be

1152
01:12:27,950 --> 01:12:31,790
downloaded there and download the other part from somebody else

1153
01:12:33,870 --> 01:12:39,520
many of these big companies nowadays really facing the problem of having complex networks trying

1154
01:12:39,520 --> 01:12:43,700
to broadcast information there or i think in the last tory we solve this one

1155
01:12:43,700 --> 01:12:49,040
problem as well with the the microsoft the called service update

1156
01:12:49,170 --> 01:12:54,400
system the same problem right from microsoft doesn't want to send everybody now because this

1157
01:12:54,400 --> 01:12:58,160
is really putting a lot of traffic

1158
01:12:58,200 --> 01:13:02,030
since then he would like to have more in a peer-to-peer network like to use

1159
01:13:02,030 --> 01:13:05,060
username to one computer they tend to the next one

1160
01:13:05,090 --> 01:13:06,560
so to keep the

1161
01:13:06,570 --> 01:13:09,930
for the traffic quite low

1162
01:13:09,930 --> 01:13:11,090
so here

1163
01:13:11,400 --> 01:13:15,680
petrol talk a little bit more about lifted inference to say that we were able

1164
01:13:15,690 --> 01:13:20,380
to show that lifted inference techniques so far the belief propagation c right so you

1165
01:13:20,380 --> 01:13:26,410
can really show that even in task where belief propagation algorithm already quite fast and

1166
01:13:26,410 --> 01:13:31,770
really use you can make use of redundancies and their to speed up computation

1167
01:13:31,890 --> 01:13:34,900
yet another one recent applications

1168
01:13:34,950 --> 01:13:39,760
we wish we were able to show with relational techniques you can better predict heart

1169
01:13:40,620 --> 01:13:45,370
diseases than any of the propositional systems and here we are not talking really about

1170
01:13:45,370 --> 01:13:46,450
money because

1171
01:13:46,510 --> 01:13:50,490
cardiovascular disease is really cost already the EU

1172
01:13:50,500 --> 01:13:55,620
several millions in the year so if we can somehow get that down that's good

1173
01:13:55,620 --> 01:14:00,260
but you see something else which is important for relational systems if they are sometimes

1174
01:14:00,260 --> 01:14:04,240
more declarative better to understand so what you see here is the

1175
01:14:04,290 --> 01:14:05,590
and of local

1176
01:14:05,880 --> 01:14:10,010
conditional probability table which is now making use of abstractions

1177
01:14:10,020 --> 01:14:16,030
the variables the lower part variables so you test whether a person is female or

1178
01:14:16,760 --> 01:14:19,290
and what you see is what you can't see here but you have to trust

1179
01:14:19,940 --> 01:14:22,190
may the female subtree

1180
01:14:22,200 --> 01:14:23,750
it's much smaller

1181
01:14:23,860 --> 01:14:25,410
then the male tree

1182
01:14:25,420 --> 01:14:29,190
and that's actually what many medical doctors no but they never had the chance to

1183
01:14:29,630 --> 01:14:34,630
validate that using the propositional case so it's known that females

1184
01:14:34,640 --> 01:14:39,480
females are women have a much lower risk for heart attacks and may bite and

1185
01:14:39,480 --> 01:14:42,170
so on and so on and so it's very easy to sit down with the

1186
01:14:42,170 --> 01:14:47,630
expert medical expert that's what we did here and here and the shear

1187
01:14:47,650 --> 01:14:48,400
right so

1188
01:14:48,420 --> 01:14:49,930
we were even able

1189
01:14:49,970 --> 01:14:54,160
the mission to to get new insights or

1190
01:14:55,030 --> 01:14:57,480
well trying to illustrate

1191
01:14:57,500 --> 01:14:59,210
interesting applications

1192
01:14:59,210 --> 01:15:03,130
i think it already popped up then we should ask the question what our relation

1193
01:15:03,130 --> 01:15:04,360
i will start

1194
01:15:04,380 --> 01:15:06,300
when i come to a halt here

1195
01:15:06,320 --> 01:15:07,920
there we go

1196
01:15:14,960 --> 01:15:17,250
you can

1197
01:15:17,250 --> 01:15:19,280
this but

1198
01:15:19,280 --> 01:15:29,000
i don't

1199
01:15:29,020 --> 01:15:38,570
thank you

1200
01:15:57,070 --> 01:16:12,920
ten two was one of the lower

1201
01:16:12,940 --> 01:16:14,340
forty five

1202
01:16:14,360 --> 01:16:21,090
o o point six one minus o point one seconds physics works i'm telling you

1203
01:16:21,920 --> 01:16:23,860
he was the

1204
01:16:26,400 --> 01:16:29,960
period of a pendulum

1205
01:16:30,000 --> 01:16:34,110
this is the mass of provided that you can ignore

1206
01:16:34,110 --> 01:16:37,090
the mass of the tree itself which is the case

1207
01:16:37,150 --> 01:16:40,590
with the band

1208
01:16:40,650 --> 01:16:44,110
many pendulums and some we'll see in a three

1209
01:16:45,210 --> 01:16:47,170
complex more complicated

1210
01:16:47,190 --> 01:16:49,030
simply a mass history

1211
01:16:49,050 --> 01:16:51,690
was an object

1212
01:16:51,710 --> 01:16:53,020
those pendulums

1213
01:16:54,360 --> 01:16:55,800
the physical plant

1214
01:16:55,860 --> 01:16:58,860
for instance i could have this compasses

1215
01:16:58,880 --> 01:17:01,820
just let it also like this

1216
01:17:01,840 --> 01:17:03,550
it's not just simple

1217
01:17:03,630 --> 01:17:08,820
or i could have a rule like this would hold true here have been

1218
01:17:08,840 --> 01:17:10,360
have but also like

1219
01:17:10,360 --> 01:17:12,610
but i can also have also played here

1220
01:17:12,610 --> 01:17:14,030
different here

1221
01:17:14,110 --> 01:17:16,020
i also like right middle

1222
01:17:16,020 --> 01:17:18,590
and does oscillate

1223
01:17:18,610 --> 01:17:22,400
now comes the question is how do we deal with that most of you

1224
01:17:22,460 --> 01:17:23,800
must have seen that

1225
01:17:23,800 --> 01:17:28,210
in the one that i want to address that quite some detail

1226
01:17:28,260 --> 01:17:30,960
so physical pendulum and

1227
01:17:31,030 --> 01:17:33,550
looks like this

1228
01:17:33,570 --> 01:17:35,570
this is an object

1229
01:17:35,590 --> 01:17:37,570
and i really a whole year

1230
01:17:39,530 --> 01:17:41,420
that will depend on the wall

1231
01:17:41,440 --> 01:17:42,300
and it can

1232
01:17:42,320 --> 01:17:46,020
without friction can oscillate back and forth

1233
01:17:46,030 --> 01:17:48,730
and that the centre of mass this year

1234
01:17:48,750 --> 01:17:51,630
precision o

1235
01:17:51,670 --> 01:17:53,980
this separation between the

1236
01:17:54,000 --> 01:17:55,480
p and all

1237
01:17:55,520 --> 01:17:58,400
is the always the centre of mass

1238
01:17:58,440 --> 01:18:03,280
you can choose to be any way you want it is no restriction

1239
01:18:03,360 --> 01:18:05,110
so you see that this

1240
01:18:05,130 --> 01:18:07,320
pendulum is offset

1241
01:18:07,360 --> 01:18:11,320
over angle theta and it will start to oscillate back and forth

1242
01:18:11,400 --> 01:18:14,230
and the question is what is the

1243
01:18:16,030 --> 01:18:17,250
so clearly

1244
01:18:17,260 --> 01:18:19,780
we make

1245
01:18:19,820 --> 01:18:24,610
but you find gravitational force at point always the centre of mass this is

1246
01:18:24,670 --> 01:18:26,860
the force acting

1247
01:18:26,940 --> 01:18:29,650
that points

1248
01:18:29,690 --> 01:18:33,690
now comes the question and any other forces acting on this

1249
01:18:33,750 --> 01:18:36,380
four is is the only

1250
01:18:36,400 --> 01:18:37,590
when study

1251
01:18:37,590 --> 01:18:40,530
all forces

1252
01:18:40,530 --> 01:18:44,260
always happy that we have all forces into account

1253
01:18:44,280 --> 01:18:46,050
raise your hand

1254
01:18:46,070 --> 01:18:51,400
also known as to be at least one of four

1255
01:18:51,420 --> 01:18:54,380
which forces that

1256
01:18:54,400 --> 01:19:00,420
english queries that where does it act

1257
01:19:00,440 --> 01:19:04,090
so there must be somehow force at p

1258
01:19:04,150 --> 01:19:08,050
to hold otherwise it would just start to accelerate down and i'm not even sure

1259
01:19:08,050 --> 01:19:10,530
that straight up i doubt that

1260
01:19:10,530 --> 01:19:14,070
it may simply be an direction i don't want to think about that

1261
01:19:14,090 --> 01:19:18,650
but surely there has to be forced out

1262
01:19:21,880 --> 01:19:23,920
every because i mean

1263
01:19:26,750 --> 01:19:29,320
when you deal with rotation of objects

1264
01:19:29,360 --> 01:19:31,520
this is going to be irrotational

1265
01:19:31,530 --> 01:19:34,130
then this equation changes into

1266
01:19:34,170 --> 01:19:35,710
paul talk

1267
01:19:35,760 --> 01:19:38,020
its moment of inertia

1268
01:19:38,020 --> 01:19:39,920
times of far

1269
01:19:40,030 --> 01:19:41,520
by all five

1270
01:19:41,570 --> 01:19:45,530
is thing that double dot

1271
01:19:45,570 --> 01:19:53,710
the angular acceleration

1272
01:19:53,760 --> 01:19:55,250
so if i pick

1273
01:19:55,250 --> 01:19:59,030
the as my point of origin

1274
01:19:59,050 --> 01:20:00,630
then the talk

1275
01:20:00,690 --> 01:20:04,130
due to this force does not contribute

1276
01:20:04,190 --> 01:20:06,530
my talk equation because

1277
01:20:06,570 --> 01:20:07,820
the talk is

1278
01:20:07,840 --> 01:20:12,030
i cross as the cross product between the position vector

1279
01:20:12,090 --> 01:20:13,530
and the fourth

1280
01:20:13,550 --> 01:20:15,610
and this is the action vector two

1281
01:20:15,670 --> 01:20:19,750
the centre of

1282
01:20:20,590 --> 01:20:23,590
and the position vector from p two p zero

1283
01:20:23,610 --> 01:20:28,630
so if we deal with the poor relative to point b

1284
01:20:28,630 --> 01:20:30,090
that force

1285
01:20:30,150 --> 01:20:32,960
is of no consequence

1286
01:20:33,030 --> 01:20:36,030
so i'm going to take is my origin

1287
01:20:36,090 --> 01:20:38,900
so now is the question what is the talk relative to

1288
01:20:38,940 --> 01:20:40,900
o point p

1289
01:20:42,210 --> 01:20:44,000
it's ok across f

1290
01:20:44,050 --> 01:20:48,030
or is this this and which is b

1291
01:20:48,090 --> 01:20:50,670
f is

1292
01:20:50,710 --> 01:20:53,730
but i have a cross product so i have to take the sign of this

1293
01:20:53,730 --> 01:20:57,800
angle into account

1294
01:20:57,820 --> 01:21:01,750
that is the magnitude of the talk

1295
01:21:01,820 --> 01:21:07,750
and the magnitude of that talk them according to my

1296
01:21:07,940 --> 01:21:10,670
visual equivalent of vehicles may

1297
01:21:10,670 --> 01:21:12,960
he calls the moment of inertia

1298
01:21:12,980 --> 01:21:15,630
rotation about that point p

1299
01:21:15,630 --> 01:21:19,710
times that double dot

1300
01:21:24,150 --> 01:21:26,920
it a restoring torque

1301
01:21:26,920 --> 01:21:28,380
the talk

1302
01:21:28,400 --> 01:21:30,250
you can do that with your

1303
01:21:30,300 --> 01:21:34,090
right hand where you have to learn how to do that the talk is in

1304
01:21:34,090 --> 01:21:41,090
the blackboard perpendicular to the blackboard in the blackboard are cross efforts in the back

1305
01:21:41,150 --> 01:21:43,820
i have rotated counterclockwise

1306
01:21:43,840 --> 01:21:44,690
which is

1307
01:21:44,780 --> 01:21:47,480
vector out of the blackboard

1308
01:21:47,480 --> 01:21:51,200
OK so i'll talk about algorithms for

1309
01:21:51,220 --> 01:22:00,300
solving these problems in different settings for still talk about the stochastic setting where

1310
01:22:00,320 --> 01:22:07,120
accessing the words are generated from some distribution and we talk about an adversarial setting

1311
01:22:07,120 --> 01:22:13,630
where no assumptions are made about how access and they were to generate

1312
01:22:13,640 --> 01:22:16,320
and then i'll talk about solutions for

1313
01:22:16,340 --> 01:22:19,420
we use unsupervised learning algorithms to

1314
01:22:19,490 --> 01:22:26,240
to solve these problems in both online and batch settings where you have

1315
01:22:26,280 --> 01:22:30,160
exploration the line around that you want to use

1316
01:22:30,220 --> 01:22:33,560
but you don't have control over which actions

1317
01:22:33,630 --> 01:22:36,100
were chosen to

1318
01:22:36,120 --> 01:22:38,630
and john we'll talk about

1319
01:22:39,670 --> 01:22:43,030
and a number of practical extensions

1320
01:22:43,100 --> 01:22:45,240
on the setting

1321
01:22:48,740 --> 01:22:54,370
it was a bunch of interesting observations and descriptions

1322
01:22:55,270 --> 01:23:00,220
so here's the simplest way

1323
01:23:00,240 --> 01:23:04,410
the simplest algorithm that you can think about

1324
01:23:05,870 --> 01:23:10,890
and this abstract how any supervised learning algorithm

1325
01:23:10,930 --> 01:23:13,360
can be used to

1326
01:23:13,400 --> 01:23:20,310
approach these problems but will see that it's not a great solution OK so

1327
01:23:20,340 --> 01:23:28,110
let's say that you are in some time step t and you have accumulated

1328
01:23:28,120 --> 01:23:31,880
the don't this form so you have an axe

1329
01:23:31,890 --> 01:23:35,730
an action taken in the reward for that action and now you want to know

1330
01:23:35,730 --> 01:23:37,050
what to do

1331
01:23:37,070 --> 01:23:39,930
in the next time step

1332
01:23:40,020 --> 01:23:43,440
so you can for each policy pi

1333
01:23:43,470 --> 01:23:48,190
you can look at the examples where the policy agrees

1334
01:23:48,200 --> 01:23:50,320
was the action taken

1335
01:23:50,360 --> 01:23:52,520
and you can somebody words

1336
01:23:52,530 --> 01:23:55,800
over those rounds and use it as

1337
01:23:55,810 --> 01:23:58,020
some estimate of the value

1338
01:23:58,090 --> 01:24:02,450
of the policy and then go the best policy

1339
01:24:02,470 --> 01:24:03,870
in the set

1340
01:24:03,910 --> 01:24:05,430
so far

1341
01:24:05,490 --> 01:24:08,630
right so you choose to policy

1342
01:24:08,650 --> 01:24:12,150
maximize the sum of rewards

1343
01:24:13,150 --> 01:24:15,020
the previous rounds

1344
01:24:15,020 --> 01:24:20,590
where the policy agreed with the action taken because that's the data you can use

1345
01:24:20,610 --> 01:24:23,200
for everybody then that policy

1346
01:24:23,220 --> 01:24:27,900
it protects the same action

1347
01:24:27,990 --> 01:24:31,860
if the policy protects the same action then you can use the word

1348
01:24:31,870 --> 01:24:36,650
some indicator of how good that policy is because if it doesn't agree with the

1349
01:24:36,650 --> 01:24:40,470
action you don't know what to them is that example

1350
01:24:40,480 --> 01:24:43,180
you don't know how to use that example

1351
01:24:43,230 --> 01:24:50,300
two very few policy

1352
01:24:50,320 --> 01:24:53,170
right if your policy protects take action

1353
01:24:54,600 --> 01:24:56,630
and in that example

1354
01:24:56,640 --> 01:24:59,790
action to mistake you have no idea of how to use

1355
01:24:59,800 --> 01:25:02,050
that example

1356
01:25:02,070 --> 01:25:04,730
to tell how good your policy is

1357
01:25:04,840 --> 01:25:16,160
because you know your feedback is only partially only know the reward of action to

1358
01:25:16,200 --> 01:25:18,520
any policy predict section one so

1359
01:25:18,610 --> 01:25:23,040
how the use that

1360
01:25:26,490 --> 01:25:28,230
so that's

1361
01:25:28,270 --> 01:25:31,480
and the flow of

1362
01:25:31,500 --> 01:25:34,560
solving this problem

1363
01:25:34,570 --> 01:25:37,050
in india your so you choose

1364
01:25:37,060 --> 01:25:40,130
the best that the their so far

1365
01:25:40,140 --> 01:25:44,560
based on the examples you have accumulated

1366
01:25:45,640 --> 01:25:46,940
you observe

1367
01:25:48,360 --> 01:25:52,740
and you predict the same way the leader predicts on that acts

1368
01:25:52,750 --> 01:25:54,700
so you choose the action

1369
01:25:54,760 --> 01:25:59,350
predicted by the lead and the observe the would you have that example

1370
01:25:59,370 --> 01:26:03,020
the you set so that you can

1371
01:26:03,030 --> 01:26:05,340
do the same

1372
01:26:05,390 --> 01:26:10,000
on next rounds

1373
01:26:10,020 --> 01:26:12,020
and this is

1374
01:26:12,070 --> 01:26:18,450
basically worked any supervised learning algorithm tries to do is to find an empirical

1375
01:26:20,370 --> 01:26:23,310
on the day that you have so far

1376
01:26:25,310 --> 01:26:30,270
the problem is that even in the stochastic setting even have access accessing the words

1377
01:26:30,740 --> 01:26:33,230
are drawn from the distribution

1378
01:26:33,250 --> 01:26:39,480
you can do very well where you started you can be very badly

1379
01:26:39,480 --> 01:26:43,620
and substitute kernel for these products and thereby

1380
01:26:45,410 --> 01:26:50,000
carry out these algorithms in the representation induced by five

1381
01:26:50,010 --> 01:26:54,630
so for instance in the space spanned by all products of all the way could

1382
01:26:54,630 --> 01:26:58,180
the could be any natural number

1383
01:26:58,310 --> 01:27:01,980
so if you want you can

1384
01:27:02,010 --> 01:27:09,370
draw this final decision function as some kind of neural networks this is the

1385
01:27:09,400 --> 01:27:14,160
the decision function what it doesn't it compares a test point x

1386
01:27:14,170 --> 01:27:15,160
two days

1387
01:27:15,210 --> 01:27:18,980
twenty point x i which are the support vectors

1388
01:27:18,990 --> 01:27:22,770
so for instance if you did something like digit recognition the support vectors would be

1389
01:27:22,770 --> 01:27:24,100
simply did it

1390
01:27:24,500 --> 01:27:26,120
so they can comp

1391
01:27:26,140 --> 01:27:32,350
input from test point of these digits gives you some values which we set up

1392
01:27:32,360 --> 01:27:36,280
characterizes all the notion of similarity

1393
01:27:36,800 --> 01:27:40,500
and then these values are linearly combined by some weights

1394
01:27:40,520 --> 01:27:43,340
which we follow these with the product of the levels

1395
01:27:43,360 --> 01:27:48,180
multiplying the class labels and then there are fed into this

1396
01:27:48,190 --> 01:27:52,270
decision function so that's one the architecture

1397
01:27:52,290 --> 01:27:54,640
maybe i'll show you a a little

1398
01:27:55,050 --> 01:27:58,710
example of little demo for support vector machines

1399
01:27:58,730 --> 01:28:01,990
with matlab

1400
01:28:02,070 --> 01:28:10,640
so again i will be using a gaussian kernel that you already saw yesterday

1401
01:28:10,820 --> 01:28:15,460
only yesterday when i showed you the table you didn't know yet what the kernel

1402
01:28:15,480 --> 01:28:17,590
is now u

1403
01:28:17,600 --> 01:28:19,290
you know all the details

1404
01:28:19,650 --> 01:28:24,070
you even know how to construct the feature space associated with the kernel i think

1405
01:28:24,070 --> 01:28:25,990
you know more about kernels that

1406
01:28:25,990 --> 01:28:31,570
maybe the majority of people applying it seems nowadays

1407
01:28:31,640 --> 01:28:36,580
which is actually this is true

1408
01:28:36,600 --> 01:28:42,740
maximizing people applying ESPN's didn't even know that occur in an SVM has to satisfy

1409
01:28:42,740 --> 01:28:45,510
some conditions for the whole thing to make sense

1410
01:28:47,560 --> 01:28:53,510
but again start with a simple problem

1411
01:28:54,970 --> 01:29:01,090
separate this problem so what we see here is the separating hyperplane

1412
01:29:01,180 --> 01:29:05,400
actually that is not even simple so here i have parameter sigma

1413
01:29:05,410 --> 01:29:08,370
which is the width of the gaussian kernel

1414
01:29:09,230 --> 01:29:10,870
you might remember

1415
01:29:10,900 --> 01:29:13,200
mister the costs incurred

1416
01:29:28,160 --> 01:29:31,140
looks like this so we have the parameter

1417
01:29:31,370 --> 01:29:35,760
it's a completely opposite from sigma here

1418
01:29:35,780 --> 01:29:40,600
and the are lots of this proud sigma the less non-linear the the kernel you

1419
01:29:40,600 --> 01:29:41,760
can actually

1420
01:29:41,760 --> 01:29:48,320
in some sense argue that sigma goes to infinity this will

1421
01:29:48,350 --> 01:29:49,930
in some sense converge

1422
01:29:49,930 --> 01:29:51,750
two was a linear kernel

1423
01:29:51,990 --> 01:29:55,490
this kernel here

1424
01:29:55,500 --> 01:29:59,000
which is the product so in this case we will be

1425
01:29:59,100 --> 01:30:04,220
in this case we would just be computing the hyperplane directly in the input space

1426
01:30:04,220 --> 01:30:07,950
to make the signal very large i would expect to find a separation which is

1427
01:30:07,950 --> 01:30:12,270
almost a straight line here and you can see that it is almost

1428
01:30:12,330 --> 01:30:17,450
so this is the separation is dotted lines are the lines on which they

1429
01:30:17,450 --> 01:30:19,040
are there

1430
01:30:19,070 --> 01:30:22,640
argument the decision function this function w dot x

1431
01:30:22,680 --> 01:30:24,290
plus b

1432
01:30:24,350 --> 01:30:28,660
this takes the value plus one minus one

1433
01:30:28,670 --> 01:30:32,650
this line and this line and on these lines you can see the support vectors

1434
01:30:32,700 --> 01:30:36,890
so now we have three support vectors in this case and all these points back

1435
01:30:36,890 --> 01:30:40,810
here i could delete and that would get the same solution so

1436
01:30:40,830 --> 01:30:46,740
the case it's pretty obvious for me to delete some of them

1437
01:30:46,810 --> 01:30:49,620
trying again and now

1438
01:30:50,970 --> 01:30:52,580
not coming

1439
01:30:52,600 --> 01:30:54,350
problem more difficult

1440
01:30:54,490 --> 01:30:59,040
and then you might wonder what's going to happen because so far the only told

1441
01:30:59,060 --> 01:31:02,350
you how to deal with problems that are linearly separable

1442
01:31:02,700 --> 01:31:06,270
but actually here i have an implementation that also

1443
01:31:06,290 --> 01:31:11,290
can deal with non separable problems although it turns out this probably still separable but

1444
01:31:11,290 --> 01:31:15,600
let me make it a bit worse

1445
01:31:15,620 --> 01:31:18,700
i don't think this would be linear separation

1446
01:31:18,970 --> 01:31:21,580
OK so here we have some points which are on the wrong side of the

1447
01:31:22,770 --> 01:31:27,330
so far you don't know how to deal with them but i'll tell you the

1448
01:31:27,350 --> 01:31:30,830
but what also you know is that i can still

1449
01:31:30,830 --> 01:31:34,930
this is that this is there and the second lecture

1450
01:31:34,980 --> 01:31:37,460
in linear algebra

1451
01:31:37,480 --> 01:31:39,750
and the i've

1452
01:31:39,750 --> 01:31:41,030
but below

1453
01:31:41,210 --> 01:31:45,230
my main topics for today

1454
01:31:45,250 --> 01:31:49,000
that i put right there a system of equations it's going to be

1455
01:31:49,000 --> 01:31:51,000
our example to work with

1456
01:31:51,090 --> 01:31:55,130
but let me what are we going to do with

1457
01:31:55,150 --> 01:31:57,310
we're going to solve it

1458
01:31:57,340 --> 01:32:01,650
and the method of solution will not be determinant

1459
01:32:01,690 --> 01:32:07,690
determinants are something that will come later the method will use is called elimination

1460
01:32:09,080 --> 01:32:11,510
it's the way every

1461
01:32:11,550 --> 01:32:14,730
software package solve the equation

1462
01:32:17,340 --> 01:32:19,150
and an elimination

1463
01:32:19,840 --> 01:32:23,120
well if it succeeds it gets the answer

1464
01:32:23,120 --> 01:32:29,780
and normally it does succeed if the matrix a that's coming into that system is

1465
01:32:29,780 --> 01:32:32,470
good matrix and i think this one is

1466
01:32:33,340 --> 01:32:35,810
elimination will work will get the answer

1467
01:32:35,810 --> 01:32:38,050
in an efficient way

1468
01:32:38,090 --> 01:32:42,580
but why don't we as long as we're seeing how elimination work

1469
01:32:42,620 --> 01:32:46,000
it's always good to ask how could it fail

1470
01:32:46,010 --> 01:32:48,340
so at the same time we'll see

1471
01:32:48,360 --> 01:32:53,310
we'll see how elimination decides whether the matrix is a good one or has

1472
01:32:53,370 --> 01:32:54,780
has problems

1473
01:32:54,830 --> 01:32:57,970
then to complete the answer there is

1474
01:32:58,050 --> 01:33:00,530
obvious that back substitution

1475
01:33:00,550 --> 01:33:01,950
in fact

1476
01:33:02,000 --> 01:33:04,220
the idea of elimination is

1477
01:33:04,250 --> 01:33:06,190
you would have thought of it

1478
01:33:07,560 --> 01:33:08,940
i mean gaussians

1479
01:33:08,950 --> 01:33:10,340
i thought of it before

1480
01:33:10,360 --> 01:33:13,000
we did but only because he was

1481
01:33:13,050 --> 01:33:15,890
born earlier it's is natural idea

1482
01:33:15,910 --> 01:33:19,690
and died earlier two

1483
01:33:19,780 --> 01:33:20,830
OK so

1484
01:33:22,060 --> 01:33:23,640
but then

1485
01:33:25,080 --> 01:33:26,780
and you've seen the idea

1486
01:33:26,890 --> 01:33:28,680
but now

1487
01:33:28,700 --> 01:33:33,520
the part that i want to show you is elimination expressed in matrix language because

1488
01:33:33,540 --> 01:33:35,210
the whole course

1489
01:33:35,700 --> 01:33:39,980
all the key ideas get expressed as matrix operations

1490
01:33:40,040 --> 01:33:43,140
not as a word

1491
01:33:43,170 --> 01:33:48,050
and one of the operations of course that will meet is how do we multiply

1492
01:33:48,050 --> 01:33:50,180
matrices a and y

1493
01:33:51,420 --> 01:33:53,920
there is a system of equations

1494
01:33:53,930 --> 01:33:58,040
three equations in three unknowns

1495
01:33:58,050 --> 01:34:03,140
and there's the matrix the three by three matrix that that so this is the

1496
01:34:05,050 --> 01:34:11,010
x equal be this is this is our system to solve a x equal

1497
01:34:11,040 --> 01:34:13,610
on the right hand side is the vector

1498
01:34:13,630 --> 01:34:15,480
two twelve two

1499
01:34:18,240 --> 01:34:23,450
well i describe elimination it gets to be a pain to keep writing the equal

1500
01:34:23,450 --> 01:34:29,790
signs and the pluses and so on it's it's matrix that the totally matters

1501
01:34:29,830 --> 01:34:32,110
everything is in the matrix

1502
01:34:32,130 --> 01:34:34,640
but behind is those equations

1503
01:34:34,650 --> 01:34:37,170
so what is elimination do

1504
01:34:37,200 --> 01:34:40,510
what's the first step of elimination

1505
01:34:42,140 --> 01:34:45,800
we we accept the first equation it's OK

1506
01:34:45,820 --> 01:34:50,670
i'm going to multiply that equation by the right number the right multiplayer

1507
01:34:50,710 --> 01:34:52,960
and i'm going to subtracted

1508
01:34:52,980 --> 01:34:55,700
from the second equation

1509
01:34:55,730 --> 01:35:01,230
with what purpose what what what that will decide what the multiplier should be

1510
01:35:01,270 --> 01:35:04,630
our purpose is to knock out

1511
01:35:04,670 --> 01:35:07,010
the x

1512
01:35:07,040 --> 01:35:09,960
part of the equation two

1513
01:35:09,980 --> 01:35:12,550
so our purpose is to reduce

1514
01:35:12,600 --> 01:35:14,730
to eliminate act

1515
01:35:14,770 --> 01:35:17,170
so what do i multiply

1516
01:35:17,350 --> 01:35:21,320
and again i would do it with this matrix because i can do it short

1517
01:35:21,330 --> 01:35:24,790
what what's the multiplayer here what do i multiply

1518
01:35:24,800 --> 01:35:29,800
equation one and subtract notice i'm saying that words subtract i like to stick to

1519
01:35:29,800 --> 01:35:32,260
that conventional do subtraction

1520
01:35:32,270 --> 01:35:33,140
so what

1521
01:35:33,230 --> 01:35:35,010
first of all

1522
01:35:35,040 --> 01:35:35,830
this is

1523
01:35:35,850 --> 01:35:37,220
the key number right

1524
01:35:37,270 --> 01:35:39,530
that i'm starting with

1525
01:35:39,580 --> 01:35:41,910
and that's called the pivot

1526
01:35:42,010 --> 01:35:44,970
put a box around it and write named

1527
01:35:45,060 --> 01:35:47,600
that's the first ever

1528
01:35:47,620 --> 01:35:49,440
the first ever

1529
01:35:49,450 --> 01:35:52,880
OK so i'm going to use

1530
01:35:52,930 --> 01:35:57,150
that's sort like the key number in that equation and now what's the multiplier so

1531
01:35:57,150 --> 01:35:58,070
i'm going to

1532
01:35:58,100 --> 01:35:59,880
i'm going to

1533
01:35:59,950 --> 01:36:03,240
my first row won't change

1534
01:36:03,250 --> 01:36:05,750
but i mean that the pivot role

1535
01:36:05,770 --> 01:36:08,470
but i'm going to use it

1536
01:36:08,520 --> 01:36:11,440
now finally let me ask you what the multiplayer

1537
01:36:14,200 --> 01:36:18,220
three times in the first equation will not count that three

1538
01:36:18,220 --> 01:36:22,350
and because the probability is zero point five we know that t one that does

1539
01:36:22,350 --> 01:36:25,460
not satisfy the requirements

1540
01:36:25,520 --> 01:36:30,330
so this is the problem is to search for the carry on uncertain categorical data

1541
01:36:30,360 --> 01:36:36,340
and how to build a factory next to help in such kind of qualities

1542
01:36:36,360 --> 01:36:41,260
we use a probabilistic in british inverted index

1543
01:36:41,280 --> 01:36:46,650
so for a list of all the elements in the answer to attributes we have

1544
01:36:46,690 --> 01:36:53,650
a list of them and the associated with each list of couples having their possible

1545
01:36:53,650 --> 01:36:56,090
value of of the article

1546
01:36:56,150 --> 01:37:04,340
and in the show we are those couples in the probability decreasing order

1547
01:37:04,440 --> 01:37:08,190
we have acquiring they are here this is

1548
01:37:08,290 --> 01:37:10,090
example we have

1549
01:37:11,610 --> 01:37:13,780
possible values in the query q

1550
01:37:13,790 --> 01:37:16,980
and with their associated the probability for each

1551
01:37:18,620 --> 01:37:21,050
and we want to find is

1552
01:37:21,090 --> 01:37:23,520
temple t will come at the query q

1553
01:37:23,610 --> 01:37:26,590
with probability had and there were twenty three

1554
01:37:26,600 --> 01:37:29,950
so in order to do the inquiry we don't

1555
01:37:30,450 --> 01:37:35,350
so because we don't want to exact each sample of one want to avoid something

1556
01:37:36,090 --> 01:37:39,910
we can do two things the first is called the column pruning

1557
01:37:39,960 --> 01:37:42,640
that means if we have a couple of t

1558
01:37:42,660 --> 01:37:45,730
and for each element in t

1559
01:37:45,790 --> 01:37:50,340
the probability of this element is already smaller than a threshold

1560
01:37:50,380 --> 01:37:55,950
that means that each component of this quarrying fallen it's more

1561
01:37:57,410 --> 01:37:59,450
of course this you cannot be

1562
01:37:59,630 --> 01:38:04,850
match the to q with high probability that the we can prove this temple

1563
01:38:04,870 --> 01:38:09,510
the second column pruning means we call it vertically

1564
01:38:09,520 --> 01:38:13,910
and the the second of putting technology is called technique is called a rule pruning

1565
01:38:13,960 --> 01:38:17,580
so if t only contains two element

1566
01:38:17,590 --> 01:38:20,840
and the probability of this element in the query

1567
01:38:21,250 --> 01:38:25,740
in the query q is smaller than top say here there are one other point

1568
01:38:25,740 --> 01:38:26,720
one to point to the

1569
01:38:27,170 --> 01:38:28,430
it's more than ten

1570
01:38:29,070 --> 01:38:33,610
because they cannot be matched to two do so you can only matter to this

1571
01:38:34,170 --> 01:38:37,330
elements that means

1572
01:38:37,340 --> 01:38:43,000
the here this i this component is zero and for the last two components it

1573
01:38:43,000 --> 01:38:45,980
can never be higher than the point three

1574
01:38:45,990 --> 01:38:50,350
so what the topology only contains p six and p a d six and d

1575
01:38:50,350 --> 01:38:54,810
eight we from the property is that all the row pruning we can prove that

1576
01:38:54,820 --> 01:38:56,840
the way i element

1577
01:38:56,860 --> 01:39:00,930
so this is the idea of how to have probabilistic says well carry on certain

1578
01:39:00,930 --> 01:39:04,520
categorical data

1579
01:39:04,540 --> 01:39:10,570
next i will introduce a few ranking queries answered data because the data is uncertain

1580
01:39:10,570 --> 01:39:14,840
so the semantics of running because of uncertainty we have a few different proposals of

1581
01:39:14,870 --> 01:39:16,520
the iraq inquiries

1582
01:39:17,450 --> 01:39:21,170
first of all if we can look at this problem is to table and the

1583
01:39:21,170 --> 01:39:26,100
first of all we we ignore the confidence column so we have temperature and we

1584
01:39:26,100 --> 01:39:30,670
want to rank of data according to the temperature and select the couples with the

1585
01:39:30,670 --> 01:39:32,920
highest top too high temperature

1586
01:39:32,960 --> 01:39:34,220
and the answer is

1587
01:39:34,220 --> 01:39:36,330
r one and r two

1588
01:39:36,820 --> 01:39:38,850
but if the data is uncertain

1589
01:39:38,870 --> 01:39:42,050
there are a few things we need to pay attention to first of all i

1590
01:39:42,050 --> 01:39:42,840
one or two

1591
01:39:43,240 --> 01:39:46,200
there may not exist together in one possible world

1592
01:39:46,310 --> 01:39:49,680
the returning these two temples may not make sense

1593
01:39:49,690 --> 01:39:55,310
in the second missing is in different possible worlds the answer to this query different

1594
01:39:55,350 --> 01:39:59,390
so we don't know which one to return

1595
01:39:59,450 --> 01:40:04,960
so this is industry the challenges of ranking carries the data we want to first

1596
01:40:04,960 --> 01:40:11,820
of all figure out to the semantic of currently rocking curve mean unpublished data

1597
01:40:11,860 --> 01:40:16,060
and the second thing is would in order to acquire we know that the semantic

1598
01:40:16,230 --> 01:40:20,120
concept surely we need to forward all the possible world by too costly went on

1599
01:40:20,120 --> 01:40:23,630
to do that we want to find more efficient algorithms

1600
01:40:23,640 --> 01:40:25,990
that's the second challenge

1601
01:40:27,220 --> 01:40:32,520
in literature there are different proposals concurrently and they can be categorized into

1602
01:40:32,800 --> 01:40:34,070
two classes

1603
01:40:34,110 --> 01:40:39,460
the first class is the extension of the traditional ranking is you have it it's

1604
01:40:39,460 --> 01:40:42,350
ranking based on optimal objective functions

1605
01:40:42,420 --> 01:40:46,340
so we have three different curries

1606
01:40:46,340 --> 01:40:50,810
and the the second car ranking based on out the probabilities

1607
01:40:50,820 --> 01:40:54,800
the output probability means the probability that

1608
01:40:54,810 --> 01:40:57,130
the temple satisfy the query

1609
01:40:58,010 --> 01:41:02,970
of course the the two are not disjoint we can do ranking but both based

1610
01:41:02,980 --> 01:41:07,980
on objective functions and then i'll put probability the global top cookery is in this

1611
01:41:09,120 --> 01:41:13,000
introduce this carries one by one

1612
01:41:13,050 --> 01:41:15,830
first of all ranking based on objective functions

1613
01:41:15,870 --> 01:41:20,850
so we have a scoring function here for example we want to rank all the

1614
01:41:20,850 --> 01:41:23,870
objects according to their temperature

1615
01:41:23,880 --> 01:41:27,720
now we have a total order over all the titles here

1616
01:41:27,770 --> 01:41:29,520
if we want to find the top two

1617
01:41:29,540 --> 01:41:36,760
then the different inquiry only difference in how to select the top two answers

1618
01:41:36,770 --> 01:41:40,500
first of all it is called that you topic inquiry

1619
01:41:40,510 --> 01:41:46,090
if we unfold all possible worlds we can find top to list of each possible

1620
01:41:46,090 --> 01:41:48,600
world there might be different

1621
01:41:48,640 --> 01:41:53,090
and they can here we consider each one as that to the mean the order

1622
01:41:53,090 --> 01:41:55,800
between the two answers really matters

1623
01:41:55,840 --> 01:41:59,380
so we compute the probability of those answers each

1624
01:41:59,450 --> 01:42:03,250
in all possible worlds we list them here

1625
01:42:03,910 --> 01:42:06,590
and you talk inquiry tried to return

1626
01:42:06,610 --> 01:42:07,920
most probable

1627
01:42:08,090 --> 01:42:10,570
answer possible world

1628
01:42:10,580 --> 01:42:15,380
here are the list are five and three has the highest probability so we were

1629
01:42:15,380 --> 01:42:17,790
to five and also as a

1630
01:42:17,800 --> 01:42:19,140
this is one proposal

1631
01:42:19,160 --> 01:42:24,690
but in some applications we'd we really want to look at those positions pop up

1632
01:42:24,690 --> 01:42:30,220
one and the second position separately so here we consider the others separately in all

1633
01:42:30,230 --> 01:42:31,560
possible worlds

1634
01:42:31,570 --> 01:42:33,600
and computers that probability

1635
01:42:33,610 --> 01:42:39,550
we want to return the highest that we want to return the most probable answer

1636
01:42:39,610 --> 01:42:41,290
each position

1637
01:42:41,300 --> 01:42:45,950
so in position while we find that are five has has the highest probability for

1638
01:42:45,960 --> 01:42:47,140
it return

1639
01:42:47,160 --> 01:42:52,010
and also in a position to five also has the highest probability so it's also

1640
01:42:52,010 --> 01:42:53,970
returned either one

1641
01:42:54,020 --> 01:42:58,440
so this is different from the first query because depending on the application we want

1642
01:42:58,440 --> 01:43:00,340
to use

1643
01:43:00,350 --> 01:43:06,610
and proposal is that in some situations we do not really need to distinguish the

1644
01:43:06,670 --> 01:43:10,690
order or the opposition we only care who the top two

1645
01:43:10,710 --> 01:43:15,420
one of the best right so we did we do not distinguish of them and

1646
01:43:15,420 --> 01:43:19,760
then we compute the probability that one temple lies in the top two all possible

1647
01:43:20,890 --> 01:43:26,460
it's a combination of the probability and pollution one and the probability in position to

1648
01:43:26,480 --> 01:43:31,850
so we compute them and because in some situations although it's the most probable one

1649
01:43:31,850 --> 01:43:35,990
the probability might be very small say zero point zero point five

1650
01:43:35,990 --> 01:43:38,570
beyond the rules so that this particular statement you

1651
01:43:39,330 --> 01:43:46,320
cannot be proven using rules the way in which is inscribed constructed tells you that nevertheless believe it's true

1652
01:43:46,920 --> 01:43:49,880
so people usually state there and around the negative way

1653
01:43:50,310 --> 01:43:51,950
so there are certain things you can't prove

1654
01:43:52,500 --> 01:43:56,350
but what that means you can't prove using a very specific set of rules

1655
01:43:57,060 --> 01:44:00,880
but if you trust those rules and then you can see the statement is true

1656
01:44:02,100 --> 01:44:02,550
and this is

1657
01:44:03,000 --> 01:44:04,910
very striking thing about the goal state

1658
01:44:06,210 --> 01:44:08,760
clearly during was very much aware of this

1659
01:44:09,290 --> 01:44:12,230
and that is when i talked about the ordinals ordinal

1660
01:44:12,680 --> 01:44:15,880
logic systems that he was trying all the ordinal extension

1661
01:44:16,440 --> 01:44:17,160
of these

1662
01:44:18,060 --> 01:44:18,740
let's was called

1663
01:44:21,270 --> 01:44:24,650
there are logical systems which depend on these things called almost out of a little

1664
01:44:24,650 --> 01:44:26,490
bit about what was later but not very much

1665
01:44:27,140 --> 01:44:28,600
these things and they wanted to

1666
01:44:29,060 --> 01:44:31,990
i have a sort of hierarchy statements used to have your rules are

1667
01:44:32,690 --> 01:44:36,880
and then you construct stuever and make you add that's the thing had before and

1668
01:44:36,880 --> 01:44:38,610
then you do that again and again and again and again

1669
01:44:39,120 --> 01:44:40,850
and you can get more and more and more powerful

1670
01:44:41,300 --> 01:44:43,250
methods proof that's the idea so he was

1671
01:44:43,860 --> 01:44:46,860
during the studying how far you can go with that kind of thing

1672
01:44:48,020 --> 01:44:48,560
i should say

1673
01:44:49,090 --> 01:44:51,790
the phrase this is more or less during the

1674
01:44:52,540 --> 01:44:54,690
goes which in many respects

1675
01:44:55,510 --> 01:44:56,720
is was more general

1676
01:44:57,340 --> 01:44:58,860
and it's easy to prove to

1677
01:44:59,390 --> 01:45:00,410
describe interesting

1678
01:45:00,880 --> 01:45:03,090
this is stating intensive these

1679
01:45:05,390 --> 01:45:06,530
aspect of it is not

1680
01:45:07,220 --> 01:45:11,170
terms phrased in terms of logical systems and formal systems and so on

1681
01:45:11,640 --> 01:45:13,900
which i have to spend the rest of the lecture explaining them

1682
01:45:14,120 --> 01:45:16,470
but if you know what a mechanical bull

1683
01:45:16,650 --> 01:45:20,860
the system is know a computer is if you know what a general-purpose computers that's

1684
01:45:20,860 --> 01:45:21,860
all we need for the state

1685
01:45:23,220 --> 01:45:24,640
okay i'll give you an example

1686
01:45:25,740 --> 01:45:30,640
here's an example of the sort rules you might be talking about well i

1687
01:45:30,690 --> 01:45:34,970
there's a logical statements might be involved to but the basic piece mathematics

1688
01:45:35,490 --> 01:45:39,570
is what's called mathematical induction again i'm just talking about the natural numbers

1689
01:45:40,560 --> 01:45:41,850
suppose you have some

1690
01:45:42,860 --> 01:45:46,040
statement he then which holds for all

1691
01:45:47,710 --> 01:45:51,010
the question is doesn't hold for all these natural numbers one two three four and

1692
01:45:51,010 --> 01:45:52,320
you're trying to prove that us

1693
01:45:54,080 --> 01:45:57,200
that's an infinite number things you have to prove you prove that for one then

1694
01:45:57,210 --> 01:45:58,590
for two and three and four

1695
01:45:59,390 --> 01:46:02,650
and then i would take an infinite length time well of course that's no good

1696
01:46:03,320 --> 01:46:06,490
so use this very well established in very familiar

1697
01:46:09,030 --> 01:46:10,500
established appeared zero

1698
01:46:11,040 --> 01:46:14,330
if you think that's just one thing to establish so know it's okay for zero

1699
01:46:14,840 --> 01:46:16,920
and then you show that if it's true for ten

1700
01:46:17,380 --> 01:46:18,510
and it's true friend plus one

1701
01:46:19,370 --> 01:46:20,950
and these two things to establish

1702
01:46:21,580 --> 01:46:24,310
enable to say is true for all numbers

1703
01:46:24,760 --> 01:46:26,370
and that's a very unusual

1704
01:46:27,000 --> 01:46:28,080
general procedure

1705
01:46:28,580 --> 01:46:32,350
for proving things in mathematics is things we use in schools approval sorts of things

1706
01:46:33,250 --> 01:46:36,990
it is well known as technical terms as first-order peano arithmetic

1707
01:46:37,850 --> 01:46:44,980
questions of computational rules such as those provided by ordinary mathematical induction induction sufficient

1708
01:46:45,440 --> 01:46:46,260
for establishing

1709
01:46:46,720 --> 01:46:50,020
he perceivable truths arithmetic those

1710
01:46:50,480 --> 01:46:51,740
that we can actually see true

1711
01:46:52,310 --> 01:46:53,500
and goes theorem says no

1712
01:46:55,200 --> 01:46:56,740
i'm gonna give you an example

1713
01:46:57,560 --> 01:47:01,250
all something which cannot be proved using

1714
01:47:01,880 --> 01:47:03,040
mathematical induction

1715
01:47:03,600 --> 01:47:05,080
there are lots of such examples

1716
01:47:05,640 --> 01:47:12,110
people used to say well okay maybe there are things like that's also incredibly complicated and uninteresting that who cares

1717
01:47:13,140 --> 01:47:14,160
well give you one is

1718
01:47:15,440 --> 01:47:15,850
this is the

1719
01:47:16,420 --> 01:47:17,220
a nice example

1720
01:47:17,840 --> 01:47:18,480
is due to

1721
01:47:19,270 --> 01:47:20,190
kirby in paris

1722
01:47:20,680 --> 01:47:21,460
nineteen eighty two

1723
01:47:22,340 --> 01:47:23,410
achilles and the hydra

1724
01:47:24,310 --> 01:47:25,090
so here we have

1725
01:47:25,550 --> 01:47:26,140
particularly is

1726
01:47:27,190 --> 01:47:31,790
yesterday and i hope is is incredibly strong he can keep on going

1727
01:47:32,240 --> 01:47:33,250
for as long as you like

1728
01:47:33,830 --> 01:47:35,340
he just never gives up okay

1729
01:47:36,340 --> 01:47:37,460
there is also a bit stupid

1730
01:47:38,970 --> 01:47:41,700
okay well the stupidity doesn't matter actions using them in

1731
01:47:42,490 --> 01:47:43,580
when he doesn't see

1732
01:47:44,350 --> 01:47:46,710
cuts of one of the heads of the hydra

1733
01:47:47,300 --> 01:47:49,380
each time he swings his sword cuts ahead of

1734
01:47:50,210 --> 01:47:51,130
this is

1735
01:47:51,680 --> 01:47:53,760
but each time because the head of l

1736
01:47:53,950 --> 01:47:55,180
it grows more heads

1737
01:47:56,360 --> 01:48:00,130
and the procedure is like this i this is i don't scan schematically here

1738
01:48:01,160 --> 01:48:04,070
if he cuts the head of which is the one is just about job of

1739
01:48:04,970 --> 01:48:07,630
what happens when you go back to work throughout

1740
01:48:08,130 --> 01:48:09,020
go down one more

1741
01:48:09,800 --> 01:48:14,010
you see what else lies above just here and then you repeat what's underneath

1742
01:48:16,320 --> 01:48:20,230
so he's going to my head he's got one of it's got to more not much good is it

1743
01:48:20,710 --> 01:48:21,300
next time

1744
01:48:22,870 --> 01:48:26,490
this is so stalkers you go back to this is the origin of it

1745
01:48:27,490 --> 01:48:29,420
okay this time it size to cut one of

1746
01:48:29,850 --> 01:48:33,480
so you go back to the beginning down here and it goes to more next time

1747
01:48:33,890 --> 01:48:37,710
next time you cut these cuts this over the goes three more formal five more

1748
01:48:38,160 --> 01:48:41,190
and you can see the number heads is growing and growing and growing and growing

1749
01:48:41,190 --> 01:48:43,130
and growing we're gonna win or not

1750
01:48:43,830 --> 01:48:46,650
the head going to keep on going and you just be chopping forever

1751
01:48:47,130 --> 01:48:52,490
whether theorem hockey and hydra theorem says that no matter how stupid he is

1752
01:48:53,110 --> 01:48:54,320
he always wins

1753
01:48:55,840 --> 01:48:58,130
it doesn't matter whether he's clever some

1754
01:48:59,080 --> 01:49:01,760
chopping some heads of will win faster than others

1755
01:49:02,640 --> 01:49:06,710
but probably it will take much much longer than the age of the universe

1756
01:49:06,710 --> 01:49:08,500
in the second term you have

1757
01:49:08,500 --> 01:49:12,290
the second hypothesis namely that you don't have the disease

1758
01:49:12,380 --> 01:49:15,640
OK so from the previous overhead you can plug in all the numbers

1759
01:49:15,670 --> 01:49:18,800
before you look at the results probably already have to think about it it's a

1760
01:49:18,800 --> 01:49:23,600
pretty reliable test right ninety seven percent reliable are ninety percent reliable however you want

1761
01:49:23,600 --> 01:49:27,050
to say it and you took the thing and you got a positive outcome should

1762
01:49:27,060 --> 01:49:29,050
you be really worried

1763
01:49:29,050 --> 01:49:31,230
right but if you put in the numbers

1764
01:49:31,280 --> 01:49:33,570
you're probability that you have the disease

1765
01:49:33,590 --> 01:49:37,340
it's only three point two percent c probably OK

1766
01:49:37,360 --> 01:49:39,920
that's kind of surprising results

1767
01:49:39,980 --> 01:49:42,450
think about why intuitively

1768
01:49:42,460 --> 01:49:44,640
is that right

1769
01:49:44,660 --> 01:49:46,510
intuitively it makes sense

1770
01:49:46,510 --> 01:49:48,450
because the prior probabilities

1771
01:49:48,510 --> 01:49:52,810
to have the disease the probability that anybody in the population has the disease is

1772
01:49:52,810 --> 01:49:53,960
so small

1773
01:49:53,980 --> 01:49:57,370
and notice that this probability is to have the disease given that you've got a

1774
01:49:57,370 --> 01:50:03,230
positive outcome is proportional not only to this probability which is ninety percent but also

1775
01:50:03,230 --> 01:50:04,670
proportional to the

1776
01:50:04,690 --> 01:50:08,390
what's called the prior probability to have the disease which in this case is very

1777
01:50:09,430 --> 01:50:15,340
so that's what the results than in this relatively small numbers surprisingly small number that

1778
01:50:15,340 --> 01:50:16,080
you have

1779
01:50:16,730 --> 01:50:18,830
the disease

1780
01:50:18,840 --> 01:50:22,470
so on the other hand think about it this way before you took the test

1781
01:50:22,470 --> 01:50:25,040
your probability was was ten to the minus three

1782
01:50:25,090 --> 01:50:29,090
after having taken the test and getting positive outcome it's jumped from ten to the

1783
01:50:29,090 --> 01:50:32,670
minus three of the three point two percent so maybe you want to take another

1784
01:50:32,670 --> 01:50:34,640
test or something

1785
01:50:37,960 --> 01:50:41,750
now one of the interesting things about this this particular example

1786
01:50:41,760 --> 01:50:43,130
is that

1787
01:50:43,160 --> 01:50:48,160
it can actually make sense regardless of whether you interpret the probability as the degree

1788
01:50:48,160 --> 01:50:51,180
of belief or some sort of limiting frequency

1789
01:50:51,200 --> 01:50:55,530
and so for example if you're the person taking the test you might want to

1790
01:50:55,530 --> 01:50:56,180
know what is my

1791
01:50:56,570 --> 01:50:58,960
the degree of belief that i have the disease

1792
01:50:58,980 --> 01:51:02,880
so that's subjective degree of belief can now be three point two percent

1793
01:51:02,980 --> 01:51:06,830
on the other hand your doctor use user statistics of lots and lots of people

1794
01:51:07,100 --> 01:51:10,770
and he would say in the limit that i see a large number of patients

1795
01:51:10,960 --> 01:51:16,340
three point two percent of them we have that result will have the disease

1796
01:51:16,350 --> 01:51:20,590
so bayes theorem just because you're using bayes theorem doesn't mean you're you're doing bayesian

1797
01:51:20,590 --> 01:51:26,350
statistics right you can you can use bayes theorem in problems with both frequentist probability

1798
01:51:26,350 --> 01:51:28,590
and with subjective probability

1799
01:51:28,730 --> 01:51:31,880
all right so if you go to the web site of the university of london

1800
01:51:31,880 --> 01:51:36,600
course i have some some homework problems actually applying this in particle physics standpoint i

1801
01:51:36,600 --> 01:51:41,590
think will will even come back to one of these examples later in the week

1802
01:51:41,610 --> 01:51:42,920
right so now

1803
01:51:43,100 --> 01:51:47,300
i don't want you gone too long about sort of foundational issues in philosophy but

1804
01:51:47,300 --> 01:51:51,860
i do want to just give to overheads on the basic philosophy people behind frequentist

1805
01:51:51,860 --> 01:51:55,690
statistics and bayesian statistics and that'll be it then for the rest of the week

1806
01:51:55,690 --> 01:51:58,490
i think mainly talk about frequentist methods

1807
01:51:58,560 --> 01:52:02,950
but here in then is the the general philosophy of frequentist statistics

1808
01:52:03,000 --> 01:52:07,500
so the thing to keep in mind is that in frequentist probability

1809
01:52:07,510 --> 01:52:12,080
in frequentist statistics probabilities are associated only with the data

1810
01:52:12,090 --> 01:52:14,710
only with the outcomes of these

1811
01:52:14,850 --> 01:52:20,060
observations that are in principle repeatable and very often i use some sort of shorthand

1812
01:52:20,600 --> 01:52:25,250
that like x or x may be the vector symbol over it to indicate the

1813
01:52:27,780 --> 01:52:29,590
the probability is the

1814
01:52:29,630 --> 01:52:33,440
the fraction of times that you would see such an outcome in the limit you

1815
01:52:33,440 --> 01:52:37,340
repeat the observation that many a large number of times and so therefore things like

1816
01:52:37,590 --> 01:52:43,350
the probability that some constant of nature like alpha s lies between two fixed values

1817
01:52:43,350 --> 01:52:46,800
like point one seven point one c one

1818
01:52:46,800 --> 01:52:51,340
well is is either in general or not no repetition of any experiment is going

1819
01:52:51,340 --> 01:52:55,110
to change that so that the probabilities either zero or one

1820
01:52:55,140 --> 01:52:57,980
but i don't know which necessarily

1821
01:52:58,110 --> 01:53:02,480
so that seems to say the tools of frequentist statistics are not allowing you to

1822
01:53:02,480 --> 01:53:05,520
answer the questions that you really like to know i really want to know what's

1823
01:53:05,520 --> 01:53:06,750
the what's my

1824
01:53:06,830 --> 01:53:11,000
is office in that interval or not and i would like to quantify how confident

1825
01:53:11,000 --> 01:53:13,560
i am of such a statement

1826
01:53:13,560 --> 01:53:17,480
so what the tools of frequentist statistics tell you is i tell you what to

1827
01:53:17,480 --> 01:53:22,460
expect expect about these these hypothetical repeated observations and so

1828
01:53:22,480 --> 01:53:23,940
the the

1829
01:53:24,270 --> 01:53:30,900
the use of such a statistical framework is it says that the preferred theories

1830
01:53:30,920 --> 01:53:35,290
are those for which our observations would be regarded as usual

1831
01:53:35,380 --> 01:53:37,060
so in other words if if i

1832
01:53:37,150 --> 01:53:43,170
if the model or theory is correct then it probably predicts a very high probability

1833
01:53:43,170 --> 01:53:45,090
for data that you've got

1834
01:53:45,110 --> 01:53:50,020
and low probability for data that's very different from what you've got

1835
01:53:50,150 --> 01:53:53,960
there seems to be sort of a convoluted way of talking about it and then

1836
01:53:53,960 --> 01:53:57,270
it does actually get a little bit complicated times

1837
01:53:57,290 --> 01:53:59,520
that's one of the reasons why people prefer

1838
01:53:59,520 --> 01:54:01,710
sometimes bayesian statistics

1839
01:54:01,750 --> 01:54:03,270
general philosophy

1840
01:54:03,270 --> 01:54:09,270
bayesian statistics is that you extend the interpretation of probability to include subjective probability for

1841
01:54:10,440 --> 01:54:12,980
when a hypothesis can mean

1842
01:54:13,040 --> 01:54:17,860
that is the nature is governed by the standard model only or

1843
01:54:17,920 --> 01:54:22,580
supersymmetry or the the higgs boson exists or does not exist

1844
01:54:22,610 --> 01:54:27,690
and what we would like to work out what is the probability of those hypotheses

1845
01:54:27,730 --> 01:54:31,850
given that we got a certain set of outcomes in our measurement so that's what

1846
01:54:31,850 --> 01:54:36,420
we want the probability of the hypothesis given the data

1847
01:54:36,540 --> 01:54:40,900
that's what is called the posterior probability because that's what comes after having seen the

1848
01:54:42,080 --> 01:54:45,650
so how do you get that you get that using bayes theorem which is why

1849
01:54:45,650 --> 01:54:47,810
this is then called bayesian statistics

1850
01:54:47,810 --> 01:54:52,420
and so what bayes theorem says is that the probability of the hypothesis given the

1851
01:54:53,400 --> 01:54:57,540
it is equal to the probability of the data given the hypotheses

1852
01:54:57,560 --> 01:55:00,500
times the probability of the hypothesis

1853
01:55:00,520 --> 01:55:04,460
and then in the numerator here what we have is it's what corresponded to p

1854
01:55:04,460 --> 01:55:09,770
of be a couple of transparency to go and what you have here is that

1855
01:55:09,830 --> 01:55:14,270
what is in the in the denominator here service serves to simply normalize

1856
01:55:14,290 --> 01:55:17,460
this probability to unity if you're too

