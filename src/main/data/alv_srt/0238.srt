1
00:00:00,000 --> 00:00:05,040
well connected through these lines that brings tweedy

2
00:00:05,070 --> 00:00:06,170
OK so

3
00:00:06,180 --> 00:00:08,790
line was already the degree one

4
00:00:10,760 --> 00:00:14,490
we were looking out into

5
00:00:14,510 --> 00:00:21,160
the events and we discovered that there were also using metro line which was these

6
00:00:21,160 --> 00:00:22,160
metro lines

7
00:00:22,230 --> 00:00:27,700
that represents the events so they the red line was also existing out anyone can

8
00:00:27,700 --> 00:00:28,550
be this station

9
00:00:28,950 --> 00:00:31,430
across the red and the green line

10
00:00:31,450 --> 00:00:33,370
what we did

11
00:00:35,300 --> 00:00:41,410
however enough of tutorial was actually understanding that we have to be the blue line

12
00:00:41,420 --> 00:00:43,380
four music style

13
00:00:43,380 --> 00:00:46,620
and we also have to develop

14
00:00:46,670 --> 00:00:48,060
a yellow line

15
00:00:48,080 --> 00:00:50,660
no sorry the yellow and i was there

16
00:00:50,720 --> 00:00:55,200
but they were scheme is about time and place and we have develop the red

17
00:00:56,200 --> 00:00:58,090
which is about events

18
00:00:58,350 --> 00:01:05,280
so mexes here is a is a metro station that use something which was and

19
00:01:05,330 --> 00:01:06,610
is something new

20
00:01:07,440 --> 00:01:10,270
gives a new ontology is

21
00:01:10,310 --> 00:01:12,970
black line which is this one

22
00:01:12,980 --> 00:01:15,030
but i'm loath to query

23
00:01:15,050 --> 00:01:17,050
all the other lines

24
00:01:17,140 --> 00:01:22,830
so it's quite true now i understand also share this with me but this representation

25
00:01:22,830 --> 00:01:28,730
is quite meaningful because shows how you develop an application while you develop exactly like

26
00:01:28,730 --> 00:01:33,830
that of the these that line here use things and what you enable

27
00:01:34,920 --> 00:01:36,330
these black line

28
00:01:36,420 --> 00:01:39,080
and others can put in new

29
00:01:39,110 --> 00:01:46,640
metro station will very to maybe you're using other ontology so that we use or

30
00:01:48,200 --> 00:01:52,330
so thank you for listening so far

31
00:01:52,420 --> 00:01:59,000
next is a sparql and then down into implementation

32
00:01:59,020 --> 00:02:03,030
right let's do that means that you might like to do is

33
00:02:03,080 --> 00:02:05,330
is there anybody out there

34
00:02:05,340 --> 00:02:07,830
that already did this work

35
00:02:07,860 --> 00:02:10,750
so you've write you be overseen chair

36
00:02:10,780 --> 00:02:13,150
you might find our ontology

37
00:02:13,190 --> 00:02:16,990
actually you don't OK but you might find our ontology

38
00:02:17,000 --> 00:02:19,530
but we did for this tutorial

39
00:02:19,990 --> 00:02:22,910
so you can reuse it

40
00:02:22,930 --> 00:02:27,970
so reason why you should use the semantic web search engine

41
00:02:28,000 --> 00:02:30,530
is to look

42
00:02:30,600 --> 00:02:35,690
around and see if actually very easy anybody data the were before you

43
00:02:35,690 --> 00:02:39,120
in a way that you like so that you can reuse

44
00:02:40,240 --> 00:02:45,660
this is a dax

45
00:02:45,680 --> 00:02:52,750
the index of the RDF ice and seeing the also index microformats

46
00:02:52,750 --> 00:02:57,660
so if you have things like the idea of a

47
00:02:57,660 --> 00:02:58,940
or are

48
00:02:58,960 --> 00:03:01,630
other microformats is also

49
00:03:01,650 --> 00:03:06,540
that was the index finger because the world

50
00:03:12,320 --> 00:03:17,970
i mean that i'm not an expert overseeing this you might look around for people

51
00:03:17,970 --> 00:03:20,090
from the galway know

52
00:03:20,190 --> 00:03:27,840
how they did it what i know is that the the index everything using their

53
00:03:27,850 --> 00:03:29,990
the names of the tribes was

54
00:03:30,030 --> 00:03:31,490
these what i know

55
00:03:33,310 --> 00:03:34,710
review of

56
00:03:35,930 --> 00:03:36,970
the new

57
00:03:37,470 --> 00:03:39,000
you was

58
00:03:39,380 --> 00:03:40,600
you been

59
00:03:42,260 --> 00:03:44,160
replace it with me

60
00:03:46,500 --> 00:03:50,940
five of discovery of the

61
00:03:53,570 --> 00:03:55,530
when you do

62
00:03:55,690 --> 00:03:59,040
you could be very few of them yes

63
00:03:59,090 --> 00:04:05,660
i mean for instance if you go and search for music you will surely find

64
00:04:05,690 --> 00:04:07,430
music ontology

65
00:04:07,500 --> 00:04:13,380
which was developed after we did is this tutorial in this it was really one

66
00:04:13,380 --> 00:04:16,750
year ago more or less so his music and he was not really

67
00:04:16,750 --> 00:04:21,560
and yes later on we discovered that they did it probably

68
00:04:21,610 --> 00:04:26,190
if i have have to do it again i will use the entire music ontology

69
00:04:27,190 --> 00:04:31,190
would be much nicer to say look i goes on seeing this i found the

70
00:04:31,190 --> 00:04:37,140
music ontology if you use it unfortunately is not that easy i mean

71
00:04:37,170 --> 00:04:41,760
i think it as a piece of code can not just on blog piece of

72
00:04:41,760 --> 00:04:48,260
code from your blog and others you have to try to doctors in many cases

73
00:04:48,270 --> 00:04:50,310
right OK

74
00:04:50,360 --> 00:04:54,480
and you have a question from the previous part

75
00:04:54,520 --> 00:04:56,510
OK so

76
00:04:56,560 --> 00:04:59,230
i was

77
00:04:59,250 --> 00:05:01,140
call for sparql

78
00:05:01,190 --> 00:05:04,980
because i believe it is useful for what i'm going to

79
00:05:04,990 --> 00:05:11,750
to explain later on i don't remember how many of you don't know about sparql

80
00:05:11,750 --> 00:05:15,710
i really don't know about spike

81
00:05:17,240 --> 00:05:20,060
so it's worth fifty to say something about

82
00:05:23,000 --> 00:05:26,790
sparql is the query language of the semantic web

83
00:05:26,810 --> 00:05:33,460
he twice careers that back in two thousand three RDF was there i was almost

84
00:05:34,060 --> 00:05:36,920
and many of the coolest things with there

85
00:05:36,940 --> 00:05:43,130
but no query language was sparql was

86
00:05:43,150 --> 00:05:45,010
basically put together

87
00:05:45,020 --> 00:05:49,270
in the last year and becomes a recommendation last year

88
00:05:49,320 --> 00:05:53,440
and and this is queries because we want to query language you cannot really the

89
00:05:53,440 --> 00:05:55,260
morning about a

90
00:05:55,270 --> 00:05:57,610
my name is to run the mile

91
00:05:57,620 --> 00:05:59,230
the idea

92
00:05:59,270 --> 00:06:02,490
and thank you very much in the pre nineteen year

93
00:06:02,500 --> 00:06:05,330
falling we're standing

94
00:06:05,340 --> 00:06:06,900
after four days

95
00:06:08,910 --> 00:06:11,740
i was trying to do this as painless as possible

96
00:06:11,740 --> 00:06:14,240
and stable either

97
00:06:14,350 --> 00:06:16,350
very general to join

98
00:06:16,350 --> 00:06:19,150
just for applications

99
00:06:19,460 --> 00:06:22,080
i want to focus on a particular model

100
00:06:22,080 --> 00:06:24,170
using computer vision

101
00:06:24,190 --> 00:06:27,390
which are not about the recognition already been

102
00:06:28,890 --> 00:06:29,540
to build

103
00:06:29,560 --> 00:06:33,810
it's with the full feature space through the representation of the image

104
00:06:35,180 --> 00:06:40,110
also have been used for image classification

105
00:06:41,810 --> 00:06:45,680
outline of this seminar is very simple

106
00:06:45,700 --> 00:06:49,090
i was sent by introducing the field of computer vision

107
00:06:49,100 --> 00:06:50,560
and the challenges

108
00:06:50,570 --> 00:06:51,990
in this field

109
00:06:52,000 --> 00:06:52,990
at least four

110
00:06:53,000 --> 00:06:55,840
the task of categorisation

111
00:06:55,850 --> 00:06:57,530
and then i will focus on

112
00:06:57,550 --> 00:07:00,300
recognition task and most specifically

113
00:07:00,320 --> 00:07:02,190
on categorisation

114
00:07:04,140 --> 00:07:05,600
we about of visual

115
00:07:05,770 --> 00:07:07,330
world model

116
00:07:07,350 --> 00:07:09,350
and then we show so

117
00:07:09,360 --> 00:07:14,670
examples of application domain where the bag of visual words model being

118
00:07:14,860 --> 00:07:16,720
has been used

119
00:07:16,720 --> 00:07:18,130
the main goal of

120
00:07:18,140 --> 00:07:19,190
the same thing

121
00:07:19,240 --> 00:07:21,110
is giving a brain

122
00:07:21,130 --> 00:07:23,380
a very brief introduction of the field

123
00:07:23,420 --> 00:07:29,080
and show how to recognition algorithm is used in computer vision

124
00:07:29,100 --> 00:07:32,720
and to provide some references and pointers

125
00:07:32,800 --> 00:07:34,830
four people are interested in

126
00:07:34,940 --> 00:07:38,350
best thing they like the recognition algorithms in

127
00:07:38,360 --> 00:07:40,320
challenging problems

128
00:07:40,350 --> 00:07:44,110
offered a vision domain

129
00:07:44,130 --> 00:07:47,500
what species

130
00:07:47,550 --> 00:07:51,750
in nineteen ninety two in it as a joke book

131
00:07:53,160 --> 00:07:56,880
the lower friend shark intuitive answer

132
00:07:56,890 --> 00:08:00,600
to the question of what it means to deceive

133
00:08:00,610 --> 00:08:02,430
and according to my

134
00:08:02,440 --> 00:08:03,730
to see means

135
00:08:03,750 --> 00:08:04,970
to know

136
00:08:04,980 --> 00:08:07,060
what is where by looking

137
00:08:09,060 --> 00:08:13,690
vision is the process of discovering from images

138
00:08:14,900 --> 00:08:16,470
is in the real world

139
00:08:16,480 --> 00:08:18,760
and where it is

140
00:08:18,780 --> 00:08:22,690
and this the formulation that express the competition of the

141
00:08:22,690 --> 00:08:26,690
the human vision consists of processing images

142
00:08:26,690 --> 00:08:28,260
i've seen

143
00:08:28,880 --> 00:08:30,180
as soon

144
00:08:30,190 --> 00:08:32,160
make explicit

145
00:08:32,220 --> 00:08:35,350
what needs to be no idea about the same

146
00:08:35,370 --> 00:08:37,010
and on this account

147
00:08:37,030 --> 00:08:38,910
low level vision

148
00:08:38,950 --> 00:08:41,220
recall some information

149
00:08:41,220 --> 00:08:45,700
the basic information of the image and high level vision

150
00:08:45,720 --> 00:08:48,260
try to understand the meaning

151
00:08:48,320 --> 00:08:49,450
of the sea

152
00:08:49,470 --> 00:08:51,970
for instance which object inside

153
00:08:52,090 --> 00:08:55,060
which kind of the scenes

154
00:08:55,100 --> 00:08:59,000
but most pretty well specifically

155
00:08:59,870 --> 00:09:01,850
do we want

156
00:09:01,930 --> 00:09:03,880
by looking only match

157
00:09:03,900 --> 00:09:07,370
we can ask questions like what is in the sequel goal and

158
00:09:07,380 --> 00:09:09,340
well is

159
00:09:09,350 --> 00:09:12,130
and this kind of questions multiple answer

160
00:09:12,160 --> 00:09:13,220
for instance

161
00:09:13,230 --> 00:09:15,970
we can answer that there is a big so

162
00:09:15,970 --> 00:09:17,560
in the december of the cocoa

163
00:09:17,560 --> 00:09:20,260
with brain as one hundred forty three

164
00:09:20,280 --> 00:09:22,000
other the position x y

165
00:09:22,000 --> 00:09:23,590
twenty four

166
00:09:23,600 --> 00:09:25,730
fifty four

167
00:09:25,780 --> 00:09:27,630
or we can answer that

168
00:09:27,630 --> 00:09:29,340
that's part of the match

169
00:09:29,350 --> 00:09:33,900
look like bottom edge of whiteboard showing at the top of the image

170
00:09:33,940 --> 00:09:39,680
the difference in this slide is not just a matter of scale

171
00:09:39,690 --> 00:09:41,070
in the first answer

172
00:09:41,090 --> 00:09:42,320
we're trying to

173
00:09:42,810 --> 00:09:44,930
they measurements remains

174
00:09:44,970 --> 00:09:46,850
the second one

175
00:09:48,130 --> 00:09:49,910
what we are perceiving

176
00:09:49,930 --> 00:09:54,430
in terms of what humans care about so the goal of the computer vision

177
00:09:54,440 --> 00:09:56,280
what where

178
00:09:56,320 --> 00:09:59,970
i'm tens of what humans care about

179
00:09:59,980 --> 00:10:01,930
and what you might care about

180
00:10:02,540 --> 00:10:04,220
one thing we can do

181
00:10:04,280 --> 00:10:06,130
is finding something

182
00:10:06,220 --> 00:10:07,720
that we have in mind

183
00:10:07,750 --> 00:10:12,110
or verify whether this thing is there not

184
00:10:12,130 --> 00:10:15,520
and we call this stuff verification task

185
00:10:15,630 --> 00:10:20,590
well we have an initial questions like this that class

186
00:10:20,620 --> 00:10:22,120
or sometimes we

187
00:10:23,550 --> 00:10:27,200
and we don't know if such object these in the image

188
00:10:27,210 --> 00:10:29,000
by way i look for it

189
00:10:29,050 --> 00:10:30,680
and the key components

190
00:10:30,680 --> 00:10:33,990
the project going to tell you about as

191
00:10:34,080 --> 00:10:39,550
based on what is called by spectral by spectrum is not the venue think people

192
00:10:39,550 --> 00:10:43,780
have been looking for a long time even in the classical case of representation of

193
00:10:43,780 --> 00:10:48,160
signals just on the real line but people not in signal processing may not be

194
00:10:48,160 --> 00:10:52,100
familiar with it so i'm going to gradually build things up from the classical case

195
00:10:52,100 --> 00:10:55,620
there's no group here in the first part and then generalize it to the case

196
00:10:55,620 --> 00:10:59,990
of noncommutative groups in particular the rigid body motion groups and see what we get

197
00:10:59,990 --> 00:11:01,910
that way OK

198
00:11:03,510 --> 00:11:08,100
in the simplest thing is to just look at functions on

199
00:11:08,530 --> 00:11:14,030
the cyclic group cases like group and numbers zero to and again you can imagine

200
00:11:14,030 --> 00:11:19,970
as the numbers sitting around the circle neatly like this function on this group and

201
00:11:19,970 --> 00:11:27,200
you're looking at the end invariance with respect to the natural group action translation from

202
00:11:27,200 --> 00:11:32,050
the respective transition of functions with respect to groups with sorry with respect to this

203
00:11:32,080 --> 00:11:36,630
group with is just like rotating the so-called OK so this is this is just

204
00:11:36,630 --> 00:11:41,330
the classical thing is given a signal eg periodic signal and you want to shift

205
00:11:41,330 --> 00:11:47,780
invariance worshipped is interpreted in the cyclically symmetric way so what goes that one side

206
00:11:47,780 --> 00:11:50,640
comes back on the other side and your domain

207
00:11:53,700 --> 00:11:55,870
let's start with the fourier transform

208
00:11:55,910 --> 00:11:57,530
because we know

209
00:11:57,550 --> 00:12:01,850
this is just the fourier transform and like the discrete fourier transform it appropriate to

210
00:12:01,870 --> 00:12:02,890
this group

211
00:12:02,970 --> 00:12:04,100
we knew

212
00:12:04,120 --> 00:12:07,120
that the fourier transform

213
00:12:07,140 --> 00:12:16,030
or base this translation property rights of the individual fourier components behave in simple

214
00:12:16,050 --> 00:12:21,740
three prescribed manner on the translation in particular what happens here

215
00:12:21,760 --> 00:12:24,950
i think then you have a raised

216
00:12:24,970 --> 00:12:28,010
is that if you have a function f

217
00:12:28,030 --> 00:12:30,370
you have its fourier transform

218
00:12:33,170 --> 00:12:38,470
but you transform then translate the functions so looking

219
00:12:38,700 --> 00:12:45,660
and the fourier transform of the translated function at some frequency can it's just going

220
00:12:45,660 --> 00:12:46,260
to be

221
00:12:46,280 --> 00:12:49,870
two to five times k

222
00:12:51,220 --> 00:12:55,660
of the transfer of the original functions

223
00:12:58,120 --> 00:12:59,510
so this is nice

224
00:12:59,530 --> 00:13:06,660
because it immediately tells you that if you look at the power spectral

225
00:13:06,700 --> 00:13:09,390
and that's going to be invariant

226
00:13:09,410 --> 00:13:12,680
the power spectrum is just

227
00:13:13,280 --> 00:13:16,530
the modulus of the

228
00:13:16,550 --> 00:13:21,780
what like the amount of power in each fourier component which you get by multiplying

229
00:13:21,780 --> 00:13:26,930
the fourier components with its complex conjugate and then these factors that you get get

230
00:13:26,930 --> 00:13:29,890
from the translation immediately cancelled

231
00:13:30,450 --> 00:13:34,030
this is not surprising that if you have a signal and you're not looking at

232
00:13:34,030 --> 00:13:36,180
the signal it's just looking at how much

233
00:13:36,220 --> 00:13:40,950
energy has in each band then that quantity is going to be invariant with respect

234
00:13:40,950 --> 00:13:45,780
to translation so this is a classical thing that people in single processing use to

235
00:13:45,780 --> 00:13:49,350
capture signals in translation invariant way

236
00:13:49,350 --> 00:13:52,320
of course the problem with that is that knowing the

237
00:13:52,330 --> 00:13:58,720
power distribution of the signal tells you very little about the underlying signal so a

238
00:13:58,720 --> 00:14:03,410
lot of information is lost all the phase information is lost right so this is

239
00:14:03,410 --> 00:14:09,640
a very lossy type of representation of signals and and essentially the problem is just

240
00:14:09,640 --> 00:14:13,560
what i said before the the the the phase information is lost you want some

241
00:14:13,560 --> 00:14:16,260
phase information to two

242
00:14:16,280 --> 00:14:22,300
to go away because you want a single translation to not to affect your representation

243
00:14:22,490 --> 00:14:26,720
but you want all the relative phases to be there right so somehow you want

244
00:14:26,720 --> 00:14:31,970
to get rid of one global phase but you want to preserve the relative phases

245
00:14:31,970 --> 00:14:34,050
of the different free components

246
00:14:34,060 --> 00:14:36,060
the natural way to do that

247
00:14:36,120 --> 00:14:38,780
is to start

248
00:14:40,450 --> 00:14:45,010
OK is to start combining

249
00:14:45,030 --> 00:14:46,660
how for

250
00:14:46,830 --> 00:14:51,620
different free components of free components corresponding to different frequencies

251
00:14:51,620 --> 00:14:53,970
and this is what gives rise to the

252
00:14:53,990 --> 00:14:55,240
by spectrum

253
00:14:55,260 --> 00:14:58,080
so the by spectrum is similar to the power spectrum

254
00:14:58,100 --> 00:15:03,760
except that this is a cubic things in the original signals explicitly couples the different

255
00:15:04,990 --> 00:15:06,780
OK so

256
00:15:06,800 --> 00:15:12,330
this will stop the relative phase information from going away because you are still

257
00:15:12,370 --> 00:15:15,640
doing this complex conjugation things and

258
00:15:15,680 --> 00:15:21,620
the exponential factors coming from translation going to can solve for that reason but because

259
00:15:21,620 --> 00:15:23,990
you explicitly coupling different

260
00:15:24,010 --> 00:15:27,120
forty frequencies going to be

261
00:15:27,280 --> 00:15:30,680
retaining at least some of the relative phases

262
00:15:30,890 --> 00:15:35,740
the interesting thing is that it turns out that you retain all the information so

263
00:15:35,740 --> 00:15:36,600
if you

264
00:15:36,600 --> 00:15:42,200
compute all possible by special components let k one and k two independently range over

265
00:15:42,220 --> 00:15:44,350
there are possible values

266
00:15:44,350 --> 00:15:47,080
then this thing is going to be

267
00:15:47,100 --> 00:15:52,120
a complete translation invariant representation of the original signal

268
00:15:52,260 --> 00:15:53,660
so this has been known in

269
00:15:53,680 --> 00:16:02,640
and single processing for wind and so the number of applications in particular people have

270
00:16:02,640 --> 00:16:05,910
looked at using it to

271
00:16:05,930 --> 00:16:08,030
representing variance in vision

272
00:16:08,050 --> 00:16:15,660
with some modifications it can capture scaling and rotation people have used it for rotation

273
00:16:15,660 --> 00:16:22,160
and for translation but people haven't really put these different types of transformations together because

274
00:16:22,180 --> 00:16:24,330
whenever you try to use the classical

275
00:16:24,390 --> 00:16:27,200
by spectrum are always limited to

276
00:16:27,240 --> 00:16:29,370
the translation group

277
00:16:30,390 --> 00:16:34,970
in general abelian groups right so as long as you have just one of these

278
00:16:34,990 --> 00:16:42,530
constituent transformations such as to rotation or translation or scaling then all this classical theory

279
00:16:42,530 --> 00:16:44,160
from signal processing

280
00:16:44,160 --> 00:16:46,820
that's right i wanted to use the next one and a half hours to prepare

281
00:16:46,840 --> 00:16:48,000
my talk

282
00:16:48,380 --> 00:16:53,280
and then i have to remember whatever i want to talk about so

283
00:16:53,780 --> 00:16:57,730
you're ready for quite a lot about nonparametric bayesian methods

284
00:16:58,550 --> 00:17:01,520
you've heard two lectures by karl august in processes

285
00:17:01,540 --> 00:17:05,800
and you've heard a lecture by the way which was mostly about the the dirichlet

286
00:17:08,770 --> 00:17:10,050
what are

287
00:17:10,070 --> 00:17:14,360
my talk on my two talks here are mostly about is first of all they

288
00:17:14,360 --> 00:17:20,430
are basically a better title would perhaps be theoretical foundations of nonparametric bayesian models but

289
00:17:20,430 --> 00:17:24,710
that would be way too long it's just foundations and

290
00:17:26,570 --> 00:17:28,780
what what i would

291
00:17:28,810 --> 00:17:30,040
like two

292
00:17:30,070 --> 00:17:31,710
conveyed here is

293
00:17:31,740 --> 00:17:37,390
the intuition that that ghosting processes inherently processes and other non parametric bayesian called stochastic

294
00:17:37,390 --> 00:17:39,040
process based models

295
00:17:39,140 --> 00:17:40,920
are basically

296
00:17:41,880 --> 00:17:43,880
instances of one and the same

297
00:17:43,890 --> 00:17:46,030
approach to constructing model

298
00:17:46,040 --> 00:17:47,650
and so

299
00:17:48,240 --> 00:17:53,720
i would like to convince you that you can think of nonparametric bayesian models or

300
00:17:53,740 --> 00:17:56,640
that good way to think of nonparametric bayesian models is

301
00:17:56,660 --> 00:17:58,180
think of them as

302
00:17:58,190 --> 00:18:02,850
bayesian models on infinite dimensional spaces

303
00:18:03,710 --> 00:18:08,720
we have to talk a little bit about how you can construct useful probability distributions

304
00:18:08,720 --> 00:18:13,460
on infinite dimensional spaces because usually when we construct the probability distribution just write down

305
00:18:13,460 --> 00:18:14,440
the density

306
00:18:14,470 --> 00:18:17,520
and that doesn't really work when we go to infinite dimension

307
00:18:21,350 --> 00:18:25,940
what i would do today what i hope to do today in the remaining time

308
00:18:28,080 --> 00:18:30,380
first of all i will have to talk for

309
00:18:31,880 --> 00:18:34,720
o forty minutes or something about measure theory

310
00:18:35,750 --> 00:18:38,140
the theory is what we substitute

311
00:18:38,160 --> 00:18:40,830
four densities if we can't work with density

312
00:18:42,830 --> 00:18:44,680
then i will try to

313
00:18:44,720 --> 00:18:49,300
tell you a little bit about bayesian estimation in that context of measure theory of

314
00:18:49,300 --> 00:18:53,840
course levels in bayesian estimation before but in the context of measure theory looks at

315
00:18:57,070 --> 00:19:01,600
then i will tell you how we can construct stochastic processes and how we can

316
00:19:01,620 --> 00:19:06,440
use these measure theoretic concepts to construct the gaussian process the dirichlet process

317
00:19:06,450 --> 00:19:07,600
and then

318
00:19:07,660 --> 00:19:09,440
from there and i hope you

319
00:19:09,450 --> 00:19:13,130
you will start you will be encouraged to start thinking about how you can what

320
00:19:13,130 --> 00:19:18,040
other nonparametric bayesian models you can possibly construct right the girls process gives you distribution

321
00:19:18,040 --> 00:19:23,000
over functions during the process of probability measures but maybe you want to construct

322
00:19:23,010 --> 00:19:28,480
nonparametric bayesian models over the infinite dimensional objects over kernels

323
00:19:28,500 --> 00:19:32,190
infinite graphs infinite permutations of one

324
00:19:36,380 --> 00:19:38,410
i'm not

325
00:19:38,440 --> 00:19:39,320
going to do

326
00:19:39,340 --> 00:19:43,150
very much in in terms of introduction because you've already heard

327
00:19:43,160 --> 00:19:47,060
by the dirichlet process and the girls process in quite some detail

328
00:19:47,100 --> 00:19:51,130
but i think it's it's about time that we write down the definition for what

329
00:19:51,130 --> 00:19:53,150
a nonparametric model is

330
00:19:53,720 --> 00:19:58,010
and you has already mentioned the nonparametric model is not a model which doesn't have

331
00:19:58,010 --> 00:20:01,750
parameters actually it's model which was which has lots of parameters

332
00:20:06,100 --> 00:20:09,880
first let's look at the different of definition of a parametric model

333
00:20:11,280 --> 00:20:12,750
this is the pointer

334
00:20:12,760 --> 00:20:13,780
it is

335
00:20:15,650 --> 00:20:19,290
nonparametric model is the model which is not a parametric models

336
00:20:19,320 --> 00:20:22,500
but what is the parametric model is the parametric model is

337
00:20:22,560 --> 00:20:27,820
a parameterized family of distributions to model is always the family of probability distributions right

338
00:20:27,820 --> 00:20:33,150
in their indexed by some values some number of some parameter

339
00:20:34,570 --> 00:20:39,380
so this is this is what essentially what the name says but then some fine

340
00:20:39,380 --> 00:20:41,130
print and the find princess

341
00:20:41,150 --> 00:20:44,720
the number of parameters does not depend on the sample size

342
00:20:44,720 --> 00:20:46,400
so think of the gaussians

343
00:20:46,410 --> 00:20:50,450
distribution that you fit to data you dataset and you fit in the gaussians with

344
00:20:50,720 --> 00:20:54,320
maximum likelihood saying one dimensions we have two parameters

345
00:20:54,340 --> 00:20:56,910
and that number two is constant

346
00:20:56,950 --> 00:20:59,720
with respect to the sample size it doesn't matter whether you have a hundred points

347
00:20:59,720 --> 00:21:01,900
or thousand points you always have two parameters

348
00:21:06,410 --> 00:21:10,060
nonparametric model is the model which is also

349
00:21:10,150 --> 00:21:12,280
is also a parameterized model

350
00:21:12,340 --> 00:21:13,400
just like this

351
00:21:13,410 --> 00:21:15,800
but one that doesn't match the fine print so

352
00:21:15,830 --> 00:21:18,540
the number of parameters can grow with sample size

353
00:21:18,610 --> 00:21:21,850
and since we already when we are really talking about gaussians

354
00:21:21,850 --> 00:21:23,840
in a way that you can do

355
00:21:23,850 --> 00:21:27,220
nonparametric estimation of the gaussians or

356
00:21:27,280 --> 00:21:30,210
say if if we fit model to data

357
00:21:30,260 --> 00:21:34,850
just to girls parameterize model parametric model what kind of estimation is that

358
00:21:34,890 --> 00:21:38,850
i mean is that regression classification

359
00:21:38,890 --> 00:21:41,660
but what are we doing

360
00:21:41,810 --> 00:21:43,850
density estimation

361
00:21:43,920 --> 00:21:46,290
fitting riveting in density into

362
00:21:46,300 --> 00:21:50,590
so if we wanted to density estimation and nonparametric way

363
00:21:50,600 --> 00:21:55,310
and still use gaussian bell curve function what we can do is part the estimator

364
00:21:55,340 --> 00:21:59,400
so what we have basically do what we basically do is we at each data

365
00:21:59,400 --> 00:22:02,350
point that we observe we fix

366
00:22:02,400 --> 00:22:03,730
the gaussians

367
00:22:03,760 --> 00:22:07,160
one small garcia and then we add them all up and normalized be smoothing over

368
00:22:07,160 --> 00:22:08,550
the data with the gaussians

369
00:22:08,920 --> 00:22:11,010
like a low pass filter

370
00:22:11,020 --> 00:22:13,280
and that means that we get

371
00:22:13,450 --> 00:22:16,780
three that the number of parameters to grow with sample size because we get one

372
00:22:16,780 --> 00:22:19,650
goes location parameter for each data

373
00:22:19,850 --> 00:22:24,100
so that's a nonparametric estimator

374
00:22:25,770 --> 00:22:28,150
in general a good intuition

375
00:22:28,160 --> 00:22:31,660
four for parametric models is that the number of parameters

376
00:22:31,850 --> 00:22:35,430
it gives us a measure of model complexity

377
00:22:35,450 --> 00:22:38,870
and the more parameters we have to estimate the more complex model is the more

378
00:22:38,870 --> 00:22:42,270
data we need to get a reliable estimate

379
00:22:42,300 --> 00:22:47,060
and if there is this complexity is constant with respect to sample size

380
00:22:47,100 --> 00:22:50,910
then it's it's easy to imagine that we get nice convergence properties when we see

381
00:22:50,910 --> 00:22:53,980
more and more data then asymptotically

382
00:22:53,990 --> 00:22:55,980
we get a very good estimate

383
00:22:58,270 --> 00:23:03,660
for nonparametric model that's that's not the case anymore so this the situation doesn't hold

384
00:23:03,660 --> 00:23:06,950
any more but it turns out over the last maybe

385
00:23:07,160 --> 00:23:11,180
thirty forty years and statistics

386
00:23:11,210 --> 00:23:15,100
they have been quite quite a few results which basically say that the convergence properties

387
00:23:15,100 --> 00:23:17,020
for nonparametric estimators are

388
00:23:17,030 --> 00:23:21,490
almost as good as the parametric one usually it's it's not that that the convergence

389
00:23:21,490 --> 00:23:25,150
properties of that much burst is just that they harder to prove

390
00:23:30,040 --> 00:23:31,680
in bayesian estimation

391
00:23:31,740 --> 00:23:37,510
in bayesian estimation our basic approaches that we set up the parameter space and then

392
00:23:37,510 --> 00:23:41,510
we put a probability distribution on the parameter space and call it prior

393
00:23:42,290 --> 00:23:43,890
if you want to do that

394
00:23:43,910 --> 00:23:47,460
where is was a nonparametric estimator like this

395
00:23:47,530 --> 00:23:51,400
and that's a problem because the number of parameters can change the dimension of the

396
00:23:51,400 --> 00:23:53,140
parameter space will change

397
00:23:53,170 --> 00:23:56,410
so can we put a probability distribution on the space

398
00:23:56,430 --> 00:23:57,990
was changing dimensions

399
00:23:58,610 --> 00:24:00,680
and the answer to that is that you

400
00:24:00,730 --> 00:24:02,020
that you use

401
00:24:02,030 --> 00:24:05,060
a space which has enough dimensions

402
00:24:05,090 --> 00:24:08,340
which has more dimensions than you really need it has enough dimensions

403
00:24:08,350 --> 00:24:11,680
for you to explain all possible samples that you might look at

404
00:24:11,780 --> 00:24:15,400
and typically if we don't want to bound the number of samples beforehand

405
00:24:15,450 --> 00:24:18,430
that means that we need an infinite dimensional space

406
00:24:19,100 --> 00:24:22,150
so infinite dimensional space you means

407
00:24:22,200 --> 00:24:23,720
we want to accommodate

408
00:24:23,760 --> 00:24:27,150
sample sizes which are finite but arbitrarily large

409
00:24:27,400 --> 00:24:32,600
we want to be able to invest parameters to everything

410
00:24:33,210 --> 00:24:39,040
so typically typically in nonparametric model is model was an infinite dimensional parameters

411
00:24:39,160 --> 00:24:43,020
actually in some textbooks for example larry wasserman in nonparametric

412
00:24:43,080 --> 00:24:44,800
textbook right said

413
00:24:44,850 --> 00:24:48,810
another name for nonparametric estimation would be infinite dimension estimation

414
00:24:48,810 --> 00:24:51,070
the lebanon war

415
00:24:51,080 --> 00:24:55,950
so you think that news you take your the social and network analysis and you

416
00:24:55,950 --> 00:24:58,350
can immediately find the key people are

417
00:24:58,440 --> 00:25:00,190
i contacted

418
00:25:00,210 --> 00:25:04,970
so you you take the political situation in europe the deriving

419
00:25:05,180 --> 00:25:08,130
the the network involved in the political situation

420
00:25:08,460 --> 00:25:12,940
you could do the same thing for individual person

421
00:25:13,570 --> 00:25:18,130
once you've done this analysis you can then choose somebody let's say david cameron

422
00:25:19,340 --> 00:25:21,940
you can show is network

423
00:25:21,970 --> 00:25:26,450
and in this case we got like the family network likely that is why

424
00:25:26,970 --> 00:25:30,090
then we got the red these are people attacking in

425
00:25:30,130 --> 00:25:34,610
or people who are criticizing him and he's got support network and tony blair courses

426
00:25:37,250 --> 00:25:39,900
and you can see

427
00:25:39,950 --> 00:25:43,100
now you can do this also for microsoft outlook and google

428
00:25:43,110 --> 00:25:45,250
and you can find out who the

429
00:25:45,260 --> 00:25:48,010
who's attacking them who they supporting

430
00:25:48,030 --> 00:25:52,580
and in fact we've got the rest recesses of these companies so this is a

431
00:25:52,580 --> 00:25:53,590
nice way

432
00:25:53,600 --> 00:25:55,930
finding your enemies and friends

433
00:25:58,630 --> 00:26:01,050
the last topic i can go

434
00:26:01,340 --> 00:26:03,670
i'm i'm at the time but i'm going

435
00:26:03,680 --> 00:26:07,270
just quickly go through this

436
00:26:07,300 --> 00:26:10,740
we have been working on a tool for

437
00:26:10,770 --> 00:26:13,780
web mining which is using the same

438
00:26:13,840 --> 00:26:18,030
the same techniques but instead of mourning

439
00:26:18,050 --> 00:26:20,600
this set of mining

440
00:26:20,640 --> 00:26:23,740
the these sites will manage the web in general

441
00:26:23,780 --> 00:26:27,840
so what do we do we decided to use google cause because google was the

442
00:26:27,840 --> 00:26:29,130
best search engine

443
00:26:29,190 --> 00:26:31,720
so we will have tool that sits on top of google

444
00:26:31,780 --> 00:26:33,370
you do a search

445
00:26:33,480 --> 00:26:37,980
you get some results from google

446
00:26:38,080 --> 00:26:40,080
but what's different now is

447
00:26:40,120 --> 00:26:44,910
once you've got the search results which can we take the first thousand an advanced

448
00:26:44,910 --> 00:26:47,690
searching google we take a thousand

449
00:26:47,700 --> 00:26:51,280
URL's and we send out

450
00:26:51,320 --> 00:26:54,480
multi threaded spiders to everyone

451
00:26:55,980 --> 00:26:59,500
we extract the text using the same technique is for the news and we bring

452
00:26:59,500 --> 00:27:01,670
them back to the guys computer

453
00:27:01,690 --> 00:27:05,740
this is the this is the tallest being asked for violence security people and the

454
00:27:05,770 --> 00:27:15,180
law enforcement so it's information extracting information retrieval use google finds sites extract information

455
00:27:15,200 --> 00:27:16,920
eliminate duplicates

456
00:27:17,350 --> 00:27:21,400
don't follow dead links et cetera

457
00:27:21,530 --> 00:27:24,730
OK so you get the stuff back the coarse who hire you can then highlight

458
00:27:26,180 --> 00:27:28,770
the people's names in the looking for

459
00:27:28,780 --> 00:27:30,320
and you can highlight the

460
00:27:30,340 --> 00:27:32,610
the the search terms obviously

461
00:27:32,620 --> 00:27:33,960
once you've got the data

462
00:27:33,970 --> 00:27:36,100
so you've got all this stuff sitting on computer

463
00:27:36,100 --> 00:27:38,430
and you want to do something with it

464
00:27:38,490 --> 00:27:43,720
and the analysis so we have the same software has been deployed in newsexplorer now

465
00:27:43,720 --> 00:27:44,620
sits in this talk

466
00:27:45,010 --> 00:27:46,740
and i will go and

467
00:27:46,760 --> 00:27:49,180
extract all people's names dates

468
00:27:49,200 --> 00:27:52,600
geographic locations et cetera and extracted from the

469
00:27:52,620 --> 00:27:56,860
the text you just go back to the web

470
00:27:56,870 --> 00:27:59,760
what's expected data

471
00:27:59,770 --> 00:28:02,100
it puts it in database

472
00:28:02,100 --> 00:28:03,130
and you can then

473
00:28:03,150 --> 00:28:04,600
browse around

474
00:28:04,600 --> 00:28:06,230
and c

475
00:28:06,250 --> 00:28:08,310
who's related to who now

476
00:28:08,350 --> 00:28:12,180
what we don't have working here is the relation extraction that i was just mentioned

477
00:28:12,230 --> 00:28:14,770
this is based on two people

478
00:28:14,780 --> 00:28:17,600
being mentioned in a document within a

479
00:28:17,610 --> 00:28:19,510
path lengths

480
00:28:19,530 --> 00:28:25,390
there's no there's no pattern recognition on on the type of relationship which we should

481
00:28:25,390 --> 00:28:28,850
add to open the soft eventually

482
00:28:28,850 --> 00:28:32,070
so what you got that you can then create you can but you can read

483
00:28:32,070 --> 00:28:34,330
the documents and you can

484
00:28:34,340 --> 00:28:36,150
you can see the documents et cetera

485
00:28:36,240 --> 00:28:39,800
and visualize

486
00:28:39,820 --> 00:28:46,230
the relationships again this is now using a plugin visualization tools sitting on top of

487
00:28:47,490 --> 00:28:49,630
OK now

488
00:28:49,640 --> 00:28:53,860
that is orchestrated conclusions

489
00:28:53,950 --> 00:29:00,690
and then we'll

490
00:29:00,700 --> 00:29:03,080
OK so

491
00:29:03,120 --> 00:29:05,770
i think i have tried to

492
00:29:05,800 --> 00:29:09,120
emphasise that this security application areas the big

493
00:29:09,190 --> 00:29:11,490
is a big thing is a big growth area

494
00:29:11,550 --> 00:29:14,700
and your data mining tools applied

495
00:29:14,720 --> 00:29:16,210
these are

496
00:29:16,220 --> 00:29:18,670
are in strong demand

497
00:29:19,650 --> 00:29:25,090
five by business and by the sea by national law enforcement agencies are

498
00:29:27,460 --> 00:29:31,120
there have been recently many new entrants into this new material which i think is

499
00:29:31,500 --> 00:29:32,530
that we saw

500
00:29:32,550 --> 00:29:34,220
traditional well

501
00:29:34,220 --> 00:29:39,880
it's our bread and butter is what we do in artemio basically run media monitoring

502
00:29:39,880 --> 00:29:41,660
for the commission

503
00:29:41,840 --> 00:29:49,540
the mobile web is an interesting one because syndication can become more and more important

504
00:29:50,340 --> 00:29:56,220
mobile devices are assessed the perfect for for

505
00:29:56,220 --> 00:30:01,500
so this is this is the second lecture on machine learning and cognitive science

506
00:30:01,510 --> 00:30:04,090
and i have probably

507
00:30:04,110 --> 00:30:09,780
by some counts two thirds of material still cover we're gonna we're going to pick

508
00:30:09,780 --> 00:30:13,730
up where we left off talking about learning concepts from examples going beyond some some

509
00:30:13,730 --> 00:30:19,050
what are basically textbook kinds of models they were doing things like simple kinds of

510
00:30:19,050 --> 00:30:21,770
mixture models last time

511
00:30:21,790 --> 00:30:25,730
two more interesting structure knowledge and just kind of scalar hopefully get into something that

512
00:30:25,730 --> 00:30:28,470
looks like an intuitive theory at the very end

513
00:30:28,480 --> 00:30:29,550
that's mostly

514
00:30:29,550 --> 00:30:33,880
but the the into intuitive theory land is actually mostly open research frontier

515
00:30:33,940 --> 00:30:37,880
i'll say you know one of the reasons why one a little bit more slowly

516
00:30:37,880 --> 00:30:41,450
than i thought i might last time was when i started to explain his experiments

517
00:30:41,450 --> 00:30:46,410
to people who don't have much of cognitive psychology experimental human subject experimental background realise

518
00:30:46,420 --> 00:30:50,130
there a lot of things that i you know there wasn't explaining that you need

519
00:30:50,190 --> 00:30:53,340
to do in order to interpret the grass so i was

520
00:30:53,350 --> 00:30:57,990
i was trying to decide how to adjust for that going forward i thought about

521
00:30:57,990 --> 00:31:01,360
just cutting out all of the human experiments but that wouldn't be any fun so

522
00:31:01,780 --> 00:31:05,800
i'm going to still try to talk about some of them but because i want

523
00:31:05,800 --> 00:31:09,140
to make sure have time to talk about some of the interesting structure models that

524
00:31:09,140 --> 00:31:13,750
were doing i may not like me and i put up the graph which you

525
00:31:13,750 --> 00:31:17,140
which i may not expect actually understand in detail it's more of a queue for

526
00:31:17,140 --> 00:31:21,090
me just to summarise what the point of the experiment is but then reference you

527
00:31:21,090 --> 00:31:23,550
can go followed up and i'm happy to point you to any papers you want

528
00:31:23,550 --> 00:31:26,670
to learn about the details of the experiment okay hope that will that will be

529
00:31:26,670 --> 00:31:31,160
a good compromise feel free to ask any questions about the experiments with the models

530
00:31:31,160 --> 00:31:33,970
whatever you like but i may just say i don't want to go into detail

531
00:31:33,970 --> 00:31:36,300
of experiment because otherwise

532
00:31:36,310 --> 00:31:37,530
we take too long

533
00:31:39,480 --> 00:31:44,280
the motivating example we have for talking about learning concepts from examples was this example

534
00:31:44,280 --> 00:31:46,750
here you have a few examples of

535
00:31:46,780 --> 00:31:50,630
one of the new wording too far and from that you are able to generalize

536
00:31:50,630 --> 00:31:54,480
and we said well if you think this is just the problem of learning from

537
00:31:54,500 --> 00:31:59,250
from three positive examples it seems very hard impossible it seems a little bit more

538
00:31:59,250 --> 00:32:02,360
approachable if you think of it as a semi supervised learning problem with these all

539
00:32:02,360 --> 00:32:06,750
these unlabelled examples and from that you might be able to extract something about the

540
00:32:06,870 --> 00:32:10,720
the general structure of how categories organizing this domain which this

541
00:32:10,740 --> 00:32:14,600
very simple picture captured if you're if you look at the state and say others

542
00:32:14,600 --> 00:32:19,310
three clusters there then all have to do is label or and you assume that

543
00:32:19,310 --> 00:32:24,410
the concepts are are mapping one-to-one onto these clusters that you can pick out then

544
00:32:24,410 --> 00:32:27,280
one positive example should be enough

545
00:32:27,280 --> 00:32:28,940
but what this

546
00:32:28,960 --> 00:32:32,350
picture obscure will it offline and essentially what i'm going to be talking about today

547
00:32:32,350 --> 00:32:35,860
is all the things that obscures and how to learn how to solve those harder

548
00:32:35,860 --> 00:32:39,130
problems so one of the most basic problems that come up in any machine learning

549
00:32:39,130 --> 00:32:44,550
and statistics in particular when you're dealing with things like images of natural objects that

550
00:32:44,550 --> 00:32:48,620
are intrinsically high dimensional or maybe even infinite dimension i mean like about the image

551
00:32:48,620 --> 00:32:52,060
is a high dimensional vector you could say well there's really object in the world

552
00:32:52,280 --> 00:32:55,160
and there's all sorts of things you could measure about it

553
00:32:55,190 --> 00:32:58,070
only if you come up with the right features are you going to see the

554
00:32:58,070 --> 00:33:01,750
right clustering so the problem of feature selection if you like is is the familiar

555
00:33:02,940 --> 00:33:06,310
and as many different ways to approach but one way we can get some traction

556
00:33:06,310 --> 00:33:10,470
on this is to say well imagine that we have lots of useful features we

557
00:33:10,470 --> 00:33:12,720
compute in the brain is is

558
00:33:12,780 --> 00:33:17,540
highly optimized revolution with the senses another processing to pick two to compute lots of

559
00:33:17,540 --> 00:33:23,030
useful features then the problem might come down to one of sort sorting through different

560
00:33:23,030 --> 00:33:27,190
features and deciding which ones are relevant for particular problem that we're trying to solve

561
00:33:27,530 --> 00:33:32,690
and here is where the idea of semi supervised learning meets up with another popular

562
00:33:32,690 --> 00:33:36,260
theme in machine learning these days also sometimes called transfer learning or learning to learn

563
00:33:36,510 --> 00:33:40,870
we want to think about how by learning some concepts maybe from a few labeled

564
00:33:40,870 --> 00:33:45,040
examples you might not only learn how to generalize those concepts but also something more

565
00:33:45,540 --> 00:33:49,530
abstract about what kinds of features matter for other concepts like those in the same

566
00:33:49,530 --> 00:33:54,600
domain does get some kind of transfer essentially learning inductive bias that you need to

567
00:33:54,600 --> 00:33:59,430
solve these kinds of problems from learning from just a few examples based on other

568
00:33:59,440 --> 00:34:04,650
so higher level learning of other concepts for from a few examples

569
00:34:04,660 --> 00:34:10,250
so let me get concrete by giving a phenomenon from developmental psychology which motivated some

570
00:34:10,250 --> 00:34:12,740
of the work on the talk about here and just because there is one of

571
00:34:12,740 --> 00:34:17,840
the most interesting human learning phenomena that i know that it's interesting because it's not

572
00:34:17,840 --> 00:34:21,870
all that complicated shouldn't be all that counterintuitive but it has a couple of really

573
00:34:21,870 --> 00:34:27,400
surprising features and it's it's really nicely into a hierarchical bayes sort of framework which

574
00:34:27,400 --> 00:34:29,710
are which will be using to treat

575
00:34:29,730 --> 00:34:34,810
so here the this is it the phenomenon about how children learn the meanings of

576
00:34:34,810 --> 00:34:39,560
words like we're talking about and it's it's called the shape bias

577
00:34:39,620 --> 00:34:42,860
so here's the demonstration of the shape bias you can do this with any two-year-old

578
00:34:42,860 --> 00:34:48,600
these any two-year-old has grown up speaking english this verizon an interesting ways across languages

579
00:34:48,870 --> 00:34:53,300
and that's actually related to to the evidence that this might be something that's that's

580
00:34:53,330 --> 00:34:57,230
a learned a constraint on inductive inference

581
00:34:57,250 --> 00:34:59,820
so here's something you can do with the two children speak english you can show

582
00:34:59,820 --> 00:35:04,620
them these funny objects again they've never seen before you can say this this one

583
00:35:04,620 --> 00:35:07,140
is the tax and ask which of these

584
00:35:07,170 --> 00:35:09,510
is it that show me the next

585
00:35:09,540 --> 00:35:13,170
and what one of the objects in this one matches the

586
00:35:13,180 --> 00:35:16,200
the dax in shape another one

587
00:35:16,210 --> 00:35:21,400
this one here matches in material is something this one doesn't pointers little regular this

588
00:35:22,100 --> 00:35:24,060
this one here doesn't match any

589
00:35:24,060 --> 00:35:26,980
future but the key challenge is to say

590
00:35:26,990 --> 00:35:30,940
well should generalize this new work based on shape or based on material

591
00:35:30,940 --> 00:35:36,850
and two-year-olds speaking english habit have preference to generalize based on shape so i mean

592
00:35:36,850 --> 00:35:40,110
this is just a simple kind of feature selection problem here is really you can

593
00:35:40,110 --> 00:35:43,960
think of it as two features shape material and maybe even both cheaper material are

594
00:35:43,960 --> 00:35:48,800
really sets of a lot of features and this is that you thinking it is

595
00:35:48,800 --> 00:35:52,760
this feature selection that's kind of a more abstract level but somehow children come in

596
00:35:52,760 --> 00:35:54,230
by this page two

597
00:35:54,240 --> 00:35:58,170
knowing that a certain set of features those about shape are are the ones that

598
00:35:58,170 --> 00:36:01,940
are preferentially going to guide the the learning of new words

599
00:36:01,950 --> 00:36:06,690
and this is the powerful green one thank you

600
00:36:06,690 --> 00:36:12,230
so that enables the one-shot generalization in this very simple task now an interesting thing

601
00:36:12,230 --> 00:36:15,440
about the shape bias is that it's some

602
00:36:15,460 --> 00:36:16,500
it's not

603
00:36:16,500 --> 00:36:19,250
president if in slightly younger kids so

604
00:36:19,260 --> 00:36:20,870
twenty month olds

605
00:36:20,880 --> 00:36:26,350
it's not that they are insensitive to object shape and it's indeed if you look

606
00:36:26,350 --> 00:36:29,130
at younger children even infants say free

607
00:36:29,150 --> 00:36:31,270
twelve month old infants they

608
00:36:31,290 --> 00:36:37,800
in some sense expect categories to be organised more about shape and they preferentially attend

609
00:36:37,890 --> 00:36:40,170
to the shape of an object in

610
00:36:40,210 --> 00:36:44,210
nonlinguistic task these are kids who who haven't even learned any words yet but they're

611
00:36:44,210 --> 00:36:45,560
just sort of

612
00:36:45,740 --> 00:36:49,370
and watching objects move around and expect they think of shape as as the natural

613
00:36:49,370 --> 00:36:52,200
way projects to group but

614
00:36:52,210 --> 00:36:57,020
when it comes actually learning this particular kind of categories words project

615
00:36:57,060 --> 00:37:01,630
objects here they don't have a preference to just completely chance here going with say

616
00:37:01,680 --> 00:37:03,560
she matcher material match

617
00:37:03,570 --> 00:37:07,000
now it's possible all sorts of could be all sorts of explanations for that maybe

618
00:37:07,270 --> 00:37:10,450
you know your brains just have to make sure there's a lot of aspects of

619
00:37:10,500 --> 00:37:14,550
of the brain that are not necessarily something we think of as learning just what

620
00:37:14,550 --> 00:37:16,800
what kind of scientists will call maturation

621
00:37:16,830 --> 00:37:21,120
certain parts of the brain just get more neurons and maybe as a result you

622
00:37:22,440 --> 00:37:27,040
you can remember more for example or process more complex sentences or something as you

623
00:37:27,040 --> 00:37:28,050
get older

624
00:37:28,060 --> 00:37:33,500
but there's a really nice evidence this is the kind of learning that was obtained

625
00:37:33,500 --> 00:37:37,710
by linda smith and colleagues linda smith along with marble and others for studied this

626
00:37:37,710 --> 00:37:40,130
predictions conditioned on the data

627
00:37:40,150 --> 00:37:45,000
so we know how many ice-creams were reconstructing the weather from that the original model

628
00:37:45,000 --> 00:37:47,190
was inconsistent with the data

629
00:37:47,210 --> 00:37:51,320
by feeding the results of the model back into get new model were getting something

630
00:37:51,320 --> 00:37:57,440
which is more consistent still not perfectly consistent and you can imagine

631
00:37:57,500 --> 00:38:00,530
you can sell thinking about what we're going to do about that

632
00:38:00,570 --> 00:38:04,550
but first i want to show you how we got these how we got these

633
00:38:08,000 --> 00:38:11,510
actually before we do that let's just look at the differences between those who are

634
00:38:13,320 --> 00:38:15,420
no we probably don't enter to do

635
00:38:15,440 --> 00:38:18,500
OK so

636
00:38:18,550 --> 00:38:22,650
let's look at the computation here of how we figured out

637
00:38:22,770 --> 00:38:25,570
how we got those new numbers so he said

638
00:38:25,590 --> 00:38:30,400
OK on day three

639
00:38:30,440 --> 00:38:34,250
we were we had one percent chance of it being called a ninety nine percent

640
00:38:34,250 --> 00:38:38,440
chance of being hot now we know that the three i screamed

641
00:38:38,480 --> 00:38:41,610
we know that

642
00:38:42,770 --> 00:38:44,800
we believe

643
00:38:44,820 --> 00:38:47,840
that we have

644
00:38:49,650 --> 00:38:52,230
before i do this and give intuitions

645
00:38:52,250 --> 00:38:59,420
so over when we did this re estimation

646
00:38:59,480 --> 00:39:00,690
over here

647
00:39:00,710 --> 00:39:05,440
day twenty seven is being carried is like fifty percent likely to be hot fifty

648
00:39:05,440 --> 00:39:07,960
percent likely to be called

649
00:39:08,000 --> 00:39:09,820
so i said before

650
00:39:09,820 --> 00:39:14,690
we were counting you were interested in the fraction of two ice cream days of

651
00:39:14,690 --> 00:39:18,430
hot days and cold is according to this graph that were to icelanders do you

652
00:39:18,430 --> 00:39:21,320
think we can twenty seven as a

653
00:39:21,630 --> 00:39:23,750
as a hot or cold

654
00:39:23,780 --> 00:39:26,980
when we're computing

655
00:39:27,010 --> 00:39:29,710
these numbers over here

656
00:39:29,710 --> 00:39:37,230
it was added to ice cream hot there are two extreme called a

657
00:39:37,230 --> 00:39:41,940
what would be the right thing to do

658
00:39:41,940 --> 00:39:45,150
you throw it out the be throwing everything out because none of these are like

659
00:39:45,150 --> 00:39:48,380
really hot or cold

660
00:39:48,730 --> 00:39:51,150
right so we say that cannot happen beach

661
00:39:51,460 --> 00:39:54,360
so if you're trying to do this like figuring out how many pizzas you're going

662
00:39:54,360 --> 00:39:59,110
to get party so if you are i invite ten people to party

663
00:39:59,610 --> 00:40:03,500
if some of them are spp rsvps in three hours you no

664
00:40:03,510 --> 00:40:05,860
the new order pizzas for seven people

665
00:40:05,920 --> 00:40:09,500
but if all the forest i have a seventy percent chance of coming or something

666
00:40:09,500 --> 00:40:12,170
like that then you still are seven pieces

667
00:40:12,190 --> 00:40:15,840
right so we're just guessing you know whether is likely to be hard likely to

668
00:40:15,840 --> 00:40:18,480
be called so let's this is

669
00:40:18,570 --> 00:40:22,670
well i started showing you the spreadsheet over let's go today twenty seven which were

670
00:40:22,670 --> 00:40:26,710
just talking about here we have fifty one percent chance of it being called the

671
00:40:26,710 --> 00:40:28,860
few one percent chance of being high

672
00:40:28,880 --> 00:40:30,800
now we know

673
00:40:30,820 --> 00:40:32,960
that's the two i day

674
00:40:34,710 --> 00:40:38,130
look at the headers up here this is the chance of a cold one ice

675
00:40:38,130 --> 00:40:42,830
cream called two ice recalled three ice-cream day it's definitely two ice cream day has

676
00:40:42,830 --> 00:40:46,530
a fifty one percent chance of being cold to iceland

677
00:40:46,590 --> 00:40:48,840
and looking at these hot columns here

678
00:40:48,840 --> 00:40:53,010
it has a forty nine percent chance of being a heart two ice day and

679
00:40:53,010 --> 00:40:55,840
we know it's definitely not a one or three hours from day because we saw

680
00:40:55,840 --> 00:40:57,250
the diary

681
00:40:57,250 --> 00:41:01,500
OK so now we go down to the bottom i total these up

682
00:41:01,550 --> 00:41:06,460
and you can see that we had a total of about ten called one ice

683
00:41:06,460 --> 00:41:11,530
cream days three called to ice cream days and one half called three i screamed

684
00:41:11,550 --> 00:41:15,250
and we just so that allows us to figure out

685
00:41:15,270 --> 00:41:20,710
right and so we have a total of about fourteen point seven called this period

686
00:41:20,770 --> 00:41:21,570
OK so

687
00:41:21,590 --> 00:41:24,420
this point nine six one happen to go into the

688
00:41:24,460 --> 00:41:29,940
called point nine six one went into the two ice cream color so in general

689
00:41:29,940 --> 00:41:34,690
all of this point all this fourteen point seven cold days they all got divided

690
00:41:34,690 --> 00:41:38,440
up between the one two and three ice cream called

691
00:41:38,550 --> 00:41:40,770
so now we know what fraction

692
00:41:40,770 --> 00:41:42,860
of one two and three ice cream

693
00:41:42,880 --> 00:41:45,960
days are called

694
00:41:46,030 --> 00:41:48,030
just by doing that computation

695
00:41:48,030 --> 00:41:52,150
this is easy to calculate we see about sixty percent of the

696
00:41:52,300 --> 00:41:56,920
fourteen point seven called days or one i screamed is about twenty one percent or

697
00:41:56,920 --> 00:41:59,820
two ice cream and ten percent three

698
00:41:59,920 --> 00:42:02,780
so that's how we got these numbers

699
00:42:04,980 --> 00:42:14,380
notice that we also re estimated these probabilities

700
00:42:15,730 --> 00:42:17,500
let's go back to this graph

701
00:42:17,500 --> 00:42:20,780
so what do you think from this graph

702
00:42:21,190 --> 00:42:25,340
do you think that

703
00:42:25,720 --> 00:42:31,230
i don't know what you get by by looking at this reconstruction graph do you

704
00:42:31,230 --> 00:42:37,210
think the holiday is likely to follow another heart

705
00:42:37,230 --> 00:42:39,070
looks like it probably is

706
00:42:39,090 --> 00:42:41,770
because we have a lot of things which seem almost sort of the heart disease

707
00:42:41,770 --> 00:42:43,090
which are in sequence

708
00:42:43,090 --> 00:42:46,170
and they seem to be less likely to follow called

709
00:42:46,210 --> 00:42:49,820
nevertheless are going to maintain that this graph is not enough

710
00:42:49,880 --> 00:42:51,480
to re estimate is

711
00:42:51,630 --> 00:42:55,050
transition probabilities like by graham probability

712
00:42:55,160 --> 00:42:59,630
are asking whether two parts of speech two whether days are likely to succeed each

713
00:42:59,630 --> 00:43:03,090
other how cold cold hot and cold cold

714
00:43:04,270 --> 00:43:05,500
so the claim

715
00:43:05,510 --> 00:43:08,710
that we can we can not from this picture

716
00:43:08,820 --> 00:43:10,360
figure out

717
00:43:10,980 --> 00:43:15,860
whether hot days were likely to be followed by heart is let me prove it

718
00:43:15,860 --> 00:43:17,480
to you

719
00:43:17,500 --> 00:43:20,960
so suppose we make the distribution of ice cream the same and hot and cold

720
00:43:23,050 --> 00:43:25,780
so at this point what we have

721
00:43:25,840 --> 00:43:29,590
it's kind of an interesting experiment

722
00:43:29,630 --> 00:43:32,170
what's going on here

723
00:43:32,170 --> 00:43:34,340
so this is saying called a

724
00:43:34,360 --> 00:43:38,590
i usually three i strings hot day also usually three ice-creams

725
00:43:38,610 --> 00:43:42,460
what happened to our weather predictions

726
00:43:42,480 --> 00:43:45,690
what's the diary telling us

727
00:43:45,690 --> 00:43:47,170
anything about the weather

728
00:43:47,190 --> 00:43:51,530
nothing about whether the weather is totally uncorrelated with the number of creams according to

729
00:43:51,530 --> 00:43:53,210
these assumptions

730
00:43:55,510 --> 00:43:57,380
according to this model

731
00:43:57,400 --> 00:44:01,800
when we if we reconstructed these days if we look at all the patterns

732
00:44:01,820 --> 00:44:04,940
the high probability pads under this model

733
00:44:04,960 --> 00:44:08,590
the the high probability has tend to have hot days followed by hot days are

734
00:44:08,590 --> 00:44:15,710
hot days followed by cold

735
00:44:15,770 --> 00:44:17,400
what do you think

736
00:44:17,420 --> 00:44:21,270
so this is you know with those changes to the ice cream probabilities

737
00:44:21,270 --> 00:44:25,260
what we come up with the a curve that is somewhat similar to that

738
00:44:25,300 --> 00:44:27,460
so wrong if you put on the first

739
00:44:27,590 --> 00:44:32,200
half kilogram

740
00:44:32,240 --> 00:44:35,340
the always start to oscillate a little bit

741
00:44:35,470 --> 00:44:38,670
and so we have to be a little patient

742
00:44:38,730 --> 00:44:40,030
and in the beginning

743
00:44:40,130 --> 00:44:41,910
you may be bored because

744
00:44:42,810 --> 00:44:43,570
we're going to do

745
00:44:43,770 --> 00:44:45,410
part of the curve

746
00:44:45,460 --> 00:44:48,600
so it goes very slowly very gradually

747
00:44:48,620 --> 00:44:50,150
we have

748
00:44:50,160 --> 00:44:51,670
five centimetres

749
00:44:51,680 --> 00:45:04,860
for the first half kilogram could you remove the half kilogram

750
00:45:04,950 --> 00:45:07,820
it returns practically two zero

751
00:45:07,840 --> 00:45:10,330
maybe it's a little higher but that

752
00:45:10,340 --> 00:45:19,200
it's not very significant which make it one kilogram

753
00:45:20,880 --> 00:45:23,670
the high

754
00:45:23,730 --> 00:45:25,740
i are

755
00:45:25,790 --> 00:45:28,010
o it's about nine centimetres

756
00:45:28,030 --> 00:45:31,890
nine ten three using it in the linear part

757
00:45:31,970 --> 00:45:34,620
nine to ten centimetres

758
00:45:34,680 --> 00:45:35,810
you removed the

759
00:45:35,830 --> 00:45:38,780
of kilogram wrong

760
00:45:38,910 --> 00:45:42,710
one kilogram was one kilogram of

761
00:45:42,760 --> 00:45:51,310
you have to just wait let them powder little oscillating

762
00:45:51,350 --> 00:45:54,490
it's possible that you're ready to see

763
00:45:54,560 --> 00:45:56,740
a small deformations

764
00:45:56,750 --> 00:46:05,300
maybe one one and a half centimetres would question the question mark are possible

765
00:46:05,350 --> 00:46:07,920
OK but what i have kilograms of

766
00:46:07,930 --> 00:46:11,250
yeah i think it is

767
00:46:11,330 --> 00:46:17,120
what we will question why

768
00:46:17,170 --> 00:46:18,360
so now we are at

769
00:46:18,410 --> 00:46:22,930
one and a half

770
00:46:22,950 --> 00:46:27,780
so it is strictly linear you would expect something like fifteen

771
00:46:28,010 --> 00:46:30,990
that's what of this fifty so it's still doing quite well can you take them

772
00:46:33,880 --> 00:46:42,300
one of the half

773
00:46:42,340 --> 00:46:44,990
but you see no longer

774
00:46:45,030 --> 00:46:46,700
want to return

775
00:46:47,570 --> 00:46:52,900
its original length is clearly long enough information has already occurred

776
00:46:53,010 --> 00:46:56,120
so we now something like six centimeters

777
00:46:56,180 --> 00:47:12,540
you make it two kilograms

778
00:47:12,620 --> 00:47:14,750
two kilogrammes

779
00:47:14,790 --> 00:47:15,790
it is linear

780
00:47:15,800 --> 00:47:18,120
you would expect twenty

781
00:47:18,180 --> 00:47:21,380
it's still amazing linear

782
00:47:21,460 --> 00:47:24,680
it's as close as i can see it twenty but all these readings and lectures

783
00:47:24,680 --> 00:47:26,990
no more accurate than half a centimeter or so

784
00:47:27,040 --> 00:47:33,560
OK you removed two kilograms

785
00:47:33,590 --> 00:47:39,100
all or look at that this community formation

786
00:47:39,150 --> 00:47:41,150
no longer returns

787
00:47:41,160 --> 00:47:43,040
zero and it is

788
00:47:43,080 --> 00:47:46,730
o confidently ten centimetres long

789
00:47:46,760 --> 00:47:54,660
OK you make it two and a half

790
00:47:54,680 --> 00:47:58,820
now slowly approaching the part i hope you're going to see

791
00:47:58,830 --> 00:48:01,740
that it is going to take off like a rocket

792
00:48:01,790 --> 00:48:04,280
it was a little bit of extra weight

793
00:48:04,320 --> 00:48:05,530
it will start

794
00:48:05,530 --> 00:48:10,240
remove substantially we haven't reached that point

795
00:48:10,310 --> 00:48:12,300
we're close to it

796
00:48:12,310 --> 00:48:13,580
well now

797
00:48:13,660 --> 00:48:16,320
twenty six twenty five twenty six

798
00:48:16,320 --> 00:48:17,760
it looks quite linear

799
00:48:17,760 --> 00:48:19,320
you take it off wrong

800
00:48:19,330 --> 00:48:21,950
actually no need to take it off any more

801
00:48:22,050 --> 00:48:25,570
because it's clear that we

802
00:48:25,590 --> 00:48:29,660
that we have prominent information no sense in following that so what don't make three

803
00:48:30,980 --> 00:48:33,280
so what was what i said it was

804
00:48:33,360 --> 00:48:36,090
was the numbers

805
00:48:36,100 --> 00:48:38,330
twenty one

806
00:48:38,340 --> 00:48:42,010
twenty five or so

807
00:48:42,100 --> 00:48:46,140
so we have three now

808
00:48:46,220 --> 00:48:49,750
is why it was hanging in there i must tell you

809
00:48:49,770 --> 00:48:50,770
thirty two

810
00:48:50,900 --> 00:48:53,980
you make it four

811
00:48:54,030 --> 00:48:56,450
watch very closely and on the wall

812
00:48:56,500 --> 00:48:59,810
because the drama is about to start now

813
00:48:59,820 --> 00:49:04,050
what did i say thirty

814
00:49:04,080 --> 00:49:06,160
so thirty two

815
00:49:12,840 --> 00:49:14,280
still moving

816
00:49:14,320 --> 00:49:17,400
still moving

817
00:49:17,490 --> 00:49:19,700
fifty two

818
00:49:19,750 --> 00:49:21,910
that at fifty three

819
00:49:21,970 --> 00:49:23,160
now don't get

820
00:49:23,180 --> 00:49:27,890
the what now look at this spot now you have remember number right

821
00:49:27,900 --> 00:49:29,240
fifty three

822
00:49:29,240 --> 00:49:36,270
you eat one kilogram not

823
00:49:36,350 --> 00:49:38,480
look at that

824
00:49:38,660 --> 00:49:42,060
became almost twice as long it is still moving

825
00:49:42,080 --> 00:49:45,820
still going

826
00:49:45,870 --> 00:49:48,290
still going

827
00:49:48,330 --> 00:49:53,320
o people will settle right on my fifty three

828
00:49:53,400 --> 00:49:59,480
five kilograms

829
00:49:59,520 --> 00:50:06,490
with five right

830
00:50:06,540 --> 00:50:10,520
ninety seven output on six ninety seven ninety nine

831
00:50:10,530 --> 00:50:13,000
i know what's the point

832
00:50:17,480 --> 00:50:20,890
now you clearly the plastic flow of course

833
00:50:20,910 --> 00:50:26,900
and in one kilogram of what the point is still moving was five ninety seven

834
00:50:27,050 --> 00:50:32,750
so we're not six kilograms

835
00:50:32,780 --> 00:50:36,610
o actually that is still easy for me to estimate i would say

836
00:50:36,640 --> 00:50:37,840
it's about

837
00:50:37,890 --> 00:50:40,130
double the length of that

838
00:50:40,180 --> 00:50:44,560
i think that we have on the wall the stick is meters long

839
00:50:44,600 --> 00:50:49,260
moving at more than a little bit more than four metres

840
00:50:49,270 --> 00:50:52,540
close enough for me to support the idea

841
00:50:52,550 --> 00:50:57,180
four minnesota four hundred

842
00:50:57,240 --> 00:51:00,110
we don't have

843
00:51:00,130 --> 00:51:02,630
go through the ceiling now

844
00:51:02,640 --> 00:51:04,970
we lose it

845
00:51:05,060 --> 00:51:06,550
what i want to do now

846
00:51:06,560 --> 00:51:08,850
i want to get to the breaking point

847
00:51:08,900 --> 00:51:12,800
we can no longer measure place

848
00:51:12,890 --> 00:51:15,240
but we are very close to the breaking point

849
00:51:15,280 --> 00:51:18,290
so we're going to load up to the point that will break

850
00:51:18,330 --> 00:51:20,360
that allows us to measure the

851
00:51:20,450 --> 00:51:22,180
ultimate tensile strength

852
00:51:22,230 --> 00:51:23,840
we at seven hours

853
00:51:23,860 --> 00:51:26,920
you put it on

854
00:51:26,990 --> 00:51:28,320
we're running out of

855
00:51:33,660 --> 00:51:37,080
you could see it said it was put on that you actually look at the

856
00:51:38,290 --> 00:51:40,670
OK so it eight kilogrammes

857
00:51:40,730 --> 00:51:45,730
the rate

858
00:51:45,830 --> 00:51:48,640
OK let's of forty seven

859
00:51:48,650 --> 00:51:50,640
numbers in here

860
00:51:50,650 --> 00:51:54,140
so we have a one and a half half a kilogram

861
00:51:54,140 --> 00:51:56,750
distinct three vectors in that set

862
00:51:56,770 --> 00:51:59,050
of restrictions of functions in f two

863
00:52:00,560 --> 00:52:03,660
OK we can get your minus

864
00:52:03,710 --> 00:52:07,670
we can get the first point plus and the next to the other two minus

865
00:52:07,670 --> 00:52:08,950
by function that

866
00:52:08,970 --> 00:52:09,900
does this

867
00:52:09,910 --> 00:52:11,930
missing picture that does this

868
00:52:11,940 --> 00:52:16,850
it just goes down from plus one to minus one just between x one and

869
00:52:16,850 --> 00:52:20,500
x two is a function that goes down between x two and x three and

870
00:52:20,500 --> 00:52:22,270
we get all ones we can do

871
00:52:22,280 --> 00:52:24,930
you know and so on and minuses

872
00:52:24,950 --> 00:52:29,580
minus is to the left of x three in the next resupply so there one

873
00:52:29,580 --> 00:52:31,950
two three four five six of these

874
00:52:32,350 --> 00:52:34,140
three vectors

875
00:52:34,160 --> 00:52:37,890
because the full the full set of all possible labellings we could get is of

876
00:52:37,890 --> 00:52:40,640
size two to three is a

877
00:52:40,650 --> 00:52:43,190
so if the cardinality of the

878
00:52:43,210 --> 00:52:47,320
the cardinality of restrictions of functions in class x

879
00:52:47,330 --> 00:52:50,970
two two sequence x of length n is equal to the m

880
00:52:51,020 --> 00:52:56,430
then f is as powerful as it can be for that sequence of elements of

881
00:52:56,430 --> 00:53:01,350
x it computes all possible dichotomies of x one through x

882
00:53:01,400 --> 00:53:05,930
OK so

883
00:53:05,950 --> 00:53:08,300
when we're thinking about the growth function

884
00:53:08,320 --> 00:53:11,020
you know in fact in this case this is the

885
00:53:11,030 --> 00:53:14,520
you we you can come up with another sequence x one

886
00:53:14,570 --> 00:53:18,630
x two x three that gives a better a bigger table and this the gives

887
00:53:18,630 --> 00:53:19,950
more rows than this

888
00:53:20,160 --> 00:53:26,240
right a there are either distinctly not interesting to get more taxonomies so the the

889
00:53:26,240 --> 00:53:27,530
growth function

890
00:53:27,540 --> 00:53:33,510
is exhibited by any three distinct points and it's it's equal to six at omega

891
00:53:33,550 --> 00:53:36,370
three in this case

892
00:53:36,380 --> 00:53:41,970
OK so just to remind you the growth function for a class f

893
00:53:42,020 --> 00:53:48,960
for a particular value of m is the maximum cardinality restrictions of after that x

894
00:53:48,960 --> 00:53:53,230
is the number of rows in the table in the previous slide when we when

895
00:53:53,230 --> 00:53:57,250
we choose the sequence x that maximizes the number of rows in a table

896
00:53:57,840 --> 00:53:58,950
OK so

897
00:53:58,960 --> 00:54:02,070
a couple of obvious things

898
00:54:02,110 --> 00:54:04,650
that if we're going to find a class

899
00:54:05,610 --> 00:54:09,420
the functions f then the growth function is no bigger than the cardinality of the

900
00:54:10,700 --> 00:54:12,090
right we can have

901
00:54:12,130 --> 00:54:13,980
restrictions on the

902
00:54:13,990 --> 00:54:19,230
a number of restrictions exceeding the number of functions and clearly as n goes to

903
00:54:20,560 --> 00:54:22,190
right the growth function

904
00:54:22,280 --> 00:54:26,240
must approach the cardinality of the class

905
00:54:26,260 --> 00:54:29,010
OK this is really a distinct functions then we have to be able to see

906
00:54:29,010 --> 00:54:31,110
that on on these

907
00:54:31,160 --> 00:54:32,520
these sequences

908
00:54:32,540 --> 00:54:35,390
if f is finite

909
00:54:35,400 --> 00:54:40,630
and it's obviously always smaller into the in the disordered earlier

910
00:54:40,650 --> 00:54:43,030
right to the the is the

911
00:54:43,050 --> 00:54:46,010
total number of dichotomies of n points that we can achieve

912
00:54:46,030 --> 00:54:52,890
so linear threshold functions on the growth function

913
00:54:52,900 --> 00:54:55,590
n is equal to two n

914
00:54:55,610 --> 00:54:59,610
OK why is that we arrange our suppose we have

915
00:54:59,740 --> 00:55:03,230
in points if they're not distinct and then we're not

916
00:55:03,240 --> 00:55:09,580
getting as many dichotomies because in addition to the distinct that gives the maximum

917
00:55:09,620 --> 00:55:11,030
number of dichotomies

918
00:55:12,370 --> 00:55:16,010
we can place after we can get the all class one c or minus ones

919
00:55:16,010 --> 00:55:19,150
and then we can place the threshold points between all of them

920
00:55:19,170 --> 00:55:23,570
all of these points so that's two plus and minus one plus and minus one

921
00:55:23,570 --> 00:55:26,180
is two n

922
00:55:27,000 --> 00:55:31,620
more generally for linear threshold functions on rd

923
00:55:31,630 --> 00:55:34,160
OK which is it doesn't have this nice or property

924
00:55:34,270 --> 00:55:38,610
so he was thinking about functions that map from x two

925
00:55:38,990 --> 00:55:43,090
i guess from threshold here

926
00:55:43,280 --> 00:55:47,220
so it's a little different from this case maps from x to the sign of

927
00:55:47,240 --> 00:55:48,870
the inner product between x and

928
00:55:48,890 --> 00:55:51,850
and the parameter vector w and d

929
00:55:51,870 --> 00:55:54,680
the number of farms

930
00:55:54,690 --> 00:56:00,720
these dichotomies looks like well you know it's a it's a binomial some here so

931
00:56:00,720 --> 00:56:06,030
something like that and the tendency of the the biggest order term is like into

932
00:56:06,030 --> 00:56:08,020
the power d right it's n choose k

933
00:56:08,610 --> 00:56:13,450
OK range in my twenties KEK ranges from zero to d is something that grows

934
00:56:13,450 --> 00:56:15,180
polynomially with m

935
00:56:15,200 --> 00:56:17,930
in the exponent that polynomial polynomials d

936
00:56:30,190 --> 00:56:34,730
the VC dimension of that nature of interest dimension

937
00:56:34,800 --> 00:56:36,440
it is defined as follows

938
00:56:36,460 --> 00:56:38,650
so it

939
00:56:38,670 --> 00:56:43,680
if the cardinality of the restrictions of f two some sequence

940
00:56:43,690 --> 00:56:46,560
of length m is equal to two the m

941
00:56:46,610 --> 00:56:50,680
then we say that f shatters that that sequence of that set

942
00:56:50,690 --> 00:56:52,890
as i've written here

943
00:56:52,940 --> 00:56:55,340
OK so shatters means we get all

944
00:56:55,360 --> 00:57:00,700
all possible dichotomies of the of the set the VC dimension is the size

945
00:57:00,710 --> 00:57:04,240
one of the largest set that shattered by

946
00:57:04,240 --> 00:57:08,930
kalman filtering and going to begin although i been made much of saying we're observing

947
00:57:08,930 --> 00:57:12,510
curves and when you look at an observation post because it should be thought of

948
00:57:12,800 --> 00:57:17,240
as the point it's just an observation of the tangent of the curve nonetheless the

949
00:57:17,260 --> 00:57:22,910
discussion i'm going to revert for a little bit to tracking with

950
00:57:22,950 --> 00:57:25,710
o point observations

951
00:57:25,730 --> 00:57:31,800
and i mean this is not without its its applications so for example the

952
00:57:31,830 --> 00:57:39,080
california department of transportation sponsors research into automatic detection of traffic density on highways and

953
00:57:39,080 --> 00:57:42,700
so on and cars are happily

954
00:57:42,740 --> 00:57:43,780
quite close to

955
00:57:43,790 --> 00:57:46,090
polyhedral especially in the elderly

956
00:57:46,110 --> 00:57:50,490
this beautiful ones and so we might well be able to pick out enough points

957
00:57:50,490 --> 00:57:55,150
on them actually tracked points a point-to-point tracking is useful

958
00:57:55,160 --> 00:58:01,010
it even more artificial situations people want in the computer graphics industry who want to

959
00:58:01,010 --> 00:58:04,940
track facial movements in love to be able to use computer vision so you could

960
00:58:04,940 --> 00:58:06,080
just walk in there

961
00:58:06,100 --> 00:58:10,430
and make faces and and have all this picked up for posterity

962
00:58:10,440 --> 00:58:13,160
but just to be sure the very conservative guys

963
00:58:13,230 --> 00:58:16,630
they take the time to take the director and put a little

964
00:58:16,650 --> 00:58:17,480
sort of

965
00:58:17,500 --> 00:58:23,590
starts all over their faces make look like punks and then track the movie

966
00:58:25,970 --> 00:58:32,480
and here's the department of california tracking the density of cars going down the highway

967
00:58:32,500 --> 00:58:36,630
over the whole day and use this is an actually the speed of so the

968
00:58:36,630 --> 00:58:42,400
average speed is up here at sixty miles an hour and then drops to catastrophic

969
00:58:42,610 --> 00:58:44,160
five miles an hour

970
00:58:44,160 --> 00:58:47,650
one would suspect these must be the morning and evening rush hours and some some

971
00:58:47,670 --> 00:58:49,730
other things you can do with this

972
00:58:49,760 --> 00:58:52,500
of course such systems got work by day and by night which is a bit

973
00:58:52,500 --> 00:58:55,290
of a challenge and ask so you can have

974
00:58:55,350 --> 00:59:02,010
switch to flick i suppose and r by day the corners on the vehicles are

975
00:59:02,240 --> 00:59:03,990
quite visible by night

976
00:59:05,000 --> 00:59:10,050
you know the headlamps the most visible things also the cars not prepared especially in

977
00:59:10,050 --> 00:59:14,480
traffic jams are most interested to separate themselves from one another just for the convenience

978
00:59:14,480 --> 00:59:16,110
of your metrology

979
00:59:16,140 --> 00:59:20,910
system so you know you can't deal with overlapping features in computer vision sorry they're

980
00:59:20,910 --> 00:59:25,800
going to drop tubes in the rotor magnetic strips or something and computer vision will

981
00:59:25,800 --> 00:59:29,570
be trash but you know be great to have a computer vision system solution because

982
00:59:29,570 --> 00:59:32,560
then you could put the camera on the pole you would have to dig up

983
00:59:32,560 --> 00:59:35,690
the road and i mean you saw the density of traffic even getting on the

984
00:59:35,690 --> 00:59:36,780
road to dig it up

985
00:59:36,800 --> 00:59:38,330
is going to be seriously difficult

986
00:59:38,340 --> 00:59:41,330
so be nice to be able to put your solution on a on a traffic

987
00:59:46,440 --> 00:59:47,540
enough of that

988
00:59:47,560 --> 00:59:53,760
here here is that this is a nice plot of the distance moved by

989
00:59:53,770 --> 00:59:56,690
cars in traffic jam monitored by one of these

990
00:59:56,690 --> 00:59:58,440
systems and you see

991
00:59:58,450 --> 01:00:02,610
at some point this shock wave travelling through the system where you know jam built

992
01:00:02,610 --> 01:00:06,070
up and propagate backwards down this line of cars

993
01:00:06,130 --> 01:00:08,060
it's quite pretty picture

994
01:00:08,090 --> 01:00:09,390
OK so now

995
01:00:09,400 --> 01:00:15,080
what about the back to the business of filtering of fusing information

996
01:00:15,110 --> 01:00:16,500
so actually

997
01:00:16,520 --> 01:00:22,660
well i can do is considered

998
01:00:22,700 --> 01:00:27,050
rather as i did with the curve fitting problem i can consider the business of

999
01:00:27,100 --> 01:00:33,640
dealing with successive point observations sequentially so i can imagine some process that sort of

1000
01:00:33,640 --> 01:00:37,890
runs around the car picking up all of the corners of the visible their positions

1001
01:00:38,230 --> 01:00:39,390
together with some

1002
01:00:39,400 --> 01:00:45,590
variability in position and so and that it adds the new piece of information into

1003
01:00:45,590 --> 01:00:47,730
the current opinion of where

1004
01:00:47,800 --> 01:00:50,320
the car is and in fact

1005
01:00:51,040 --> 01:00:56,960
will be that will generalize nicely across time because although happens

1006
01:00:56,960 --> 01:01:00,470
when you have at time step is that you now start with a new piece

1007
01:01:00,470 --> 01:01:05,050
of data and you just traversed the curve the curve just reverse the outline of

1008
01:01:05,050 --> 01:01:07,470
the car again the same car and

1009
01:01:07,490 --> 01:01:14,010
again taking the positions of these points so serially into your fusion machine and update

1010
01:01:14,010 --> 01:01:17,350
your opinion on whether car is so

1011
01:01:17,410 --> 01:01:18,910
in order to make this work

1012
01:01:19,730 --> 01:01:25,790
consider a simple state space where the car is simply translating so in place of

1013
01:01:25,790 --> 01:01:29,470
capital will have little x which is just going to be translation in the plane

1014
01:01:29,560 --> 01:01:32,950
but we will now have a statistical distribution

1015
01:01:32,950 --> 01:01:39,180
gas distribution representing your state of knowledge about where the car is a particular instance

1016
01:01:39,190 --> 01:01:43,380
so let's say we believe that its position x bar is the mean of the

1017
01:01:44,350 --> 01:01:48,860
and the covariance matrix for this galaxy distribution is p not

1018
01:01:48,910 --> 01:01:53,540
and let's say that you just let's take one measurement you taking one measurement of

1019
01:01:54,040 --> 01:01:58,700
the corner which is also in two dimensions

1020
01:01:58,810 --> 01:02:01,250
let's say that the center that made this measurement

1021
01:02:01,310 --> 01:02:06,650
does it's best not to live an unbiased sense so its mean position is the

1022
01:02:06,650 --> 01:02:10,720
true position of the car but also there's covariance

1023
01:02:10,760 --> 01:02:14,080
associated with uncertainty in that

1024
01:02:14,140 --> 01:02:15,970
so in that sense

1025
01:02:16,040 --> 01:02:18,210
in this case the feature detector in the camera

1026
01:02:18,290 --> 01:02:21,530
and so it turns out that the rule for

1027
01:02:21,550 --> 01:02:25,460
absorbing this information assimilating it into your current state of knowledge about where the car

1028
01:02:25,460 --> 01:02:29,450
is is quite straightforward and intuitive we already had this idea

1029
01:02:29,520 --> 01:02:30,650
from the

1030
01:02:30,660 --> 01:02:33,990
recursive least squares fit for curves of

1031
01:02:34,000 --> 01:02:38,630
the inverse variance being a measure of information

1032
01:02:38,640 --> 01:02:41,230
and so what happens is that the

1033
01:02:43,980 --> 01:02:45,650
information for the

1034
01:02:45,710 --> 01:02:48,460
new distribution that were about to construct

1035
01:02:48,510 --> 01:02:53,240
which is the normal distribution with mean x invariant covariance p

1036
01:02:53,290 --> 01:02:58,460
it is constructed by adding information so we take the the inverse of this variance

1037
01:02:58,460 --> 01:03:01,080
and that's going to be the new information and the new information is the sum

1038
01:03:01,480 --> 01:03:07,910
of the information that we had originally that's the inverse appear nought plus the information

1039
01:03:07,940 --> 01:03:09,670
in the new

1040
01:03:11,880 --> 01:03:13,210
you know

1041
01:03:13,220 --> 01:03:15,750
this is sort of a rule of thumb you might ask

1042
01:03:15,800 --> 01:03:19,220
you know i began by saying we have distributions in what we're doing is fusing

1043
01:03:19,220 --> 01:03:23,240
information using gas distributions so it gives me the right to some

1044
01:03:23,240 --> 01:03:27,560
information like this and this apparently rule of thumb manner but it turns out that

1045
01:03:27,560 --> 01:03:32,070
OK so

1046
01:03:32,100 --> 01:03:34,650
i'm going to talk about bayesian nonparametrics obviously

1047
01:03:34,670 --> 01:03:40,530
and the title of my talk is why bayesian nonparametrics

1048
01:03:40,540 --> 01:03:42,680
which i think

1049
01:03:44,670 --> 01:03:49,520
so of closely related to the the vision of this workshop

1050
01:03:49,540 --> 01:03:50,770
just to figure out

1051
01:03:50,790 --> 01:03:55,260
you know why people were doing it are doing it and it doesn't really make

1052
01:03:55,260 --> 01:03:57,520
sense what a

1053
01:03:57,700 --> 01:04:00,940
advantages and shortcomings

1054
01:04:00,970 --> 01:04:07,010
so this is obviously a personal view in machine learning researchers you know i'm becoming

1055
01:04:07,010 --> 01:04:12,610
more and more statistician but i'm not card-carrying statistician and different people have different motivations

1056
01:04:12,610 --> 01:04:14,750
for doing this nonparametric

1057
01:04:16,750 --> 01:04:19,180
in preparing the slides i was born

1058
01:04:19,190 --> 01:04:23,180
try to think at some point i was very excited about doing business on her

1059
01:04:24,290 --> 01:04:29,290
and so there are some very positive feelings in here about it and then the

1060
01:04:29,290 --> 01:04:30,160
other points so was

1061
01:04:30,530 --> 01:04:35,090
was sort of trying to figure out all the downsides and so there's ups and

1062
01:04:35,090 --> 01:04:37,340
downs including this talk is you

1063
01:04:41,330 --> 01:04:45,500
so i bring the question up into two questions

1064
01:04:45,580 --> 01:04:47,760
why be bayesian

1065
01:04:47,810 --> 01:04:51,040
and why do nonparametrics

1066
01:04:52,480 --> 01:04:55,060
i think the answer is sort of family

1067
01:04:56,950 --> 01:05:02,220
we like to be bayesian because it's a very simple framework so simplicity goes

1068
01:05:02,280 --> 01:05:05,830
in our favor as compared to other frameworks for doing

1069
01:05:05,870 --> 01:05:07,780
learning and inference

1070
01:05:07,790 --> 01:05:10,920
on the other hand we want to nonparametrics

1071
01:05:11,330 --> 01:05:13,730
because of complexity because

1072
01:05:13,750 --> 01:05:17,090
the real world is actually complicated in the data

1073
01:05:17,140 --> 01:05:19,450
that we're trying to model

1074
01:05:19,470 --> 01:05:22,890
come from complicated phenomenon

1075
01:05:22,900 --> 01:05:26,970
so let me first focus on the first part

1076
01:05:26,980 --> 01:05:30,370
why bayesian and i apologize to people who

1077
01:05:30,390 --> 01:05:31,480
you know

1078
01:05:32,280 --> 01:05:33,680
he didn't sleep

1079
01:05:33,700 --> 01:05:35,370
this stuff

1080
01:05:37,870 --> 01:05:42,940
well it's so damn simple and the framework in machine learning can you write down

1081
01:05:42,940 --> 01:05:46,410
with two rules OK even i can remember these rules

1082
01:05:47,530 --> 01:05:48,930
you know everything falls through

1083
01:05:48,940 --> 01:05:53,530
from two simple rules of probability the sum rule in the product rule

1084
01:05:53,620 --> 01:05:55,750
and you know

1085
01:05:55,780 --> 01:05:56,620
if you

1086
01:05:56,630 --> 01:06:00,250
forget everything else you could try to derive what you're doing from the sum rule

1087
01:06:00,250 --> 01:06:01,940
in the product rule

1088
01:06:01,960 --> 01:06:06,880
you know there's no rule that says

1089
01:06:06,900 --> 01:06:10,370
you have to come back to buy something and optimise it and then you know

1090
01:06:10,560 --> 01:06:16,570
the regularizer and dance around the table three times and then try again with the

1091
01:06:16,590 --> 01:06:19,720
cross validation you just write down to model

1092
01:06:19,740 --> 01:06:22,880
you apply some rule in the product rule is best you can do this the

1093
01:06:22,880 --> 01:06:25,250
footnote there

1094
01:06:25,410 --> 01:06:29,410
talk about the fact that we are part of the talk

1095
01:06:29,470 --> 01:06:30,810
what i'm really

1096
01:06:30,810 --> 01:06:38,250
you know fundamentalist bayesian nonparametric chen and you know

1097
01:06:38,280 --> 01:06:40,250
this is a beautiful

1098
01:06:40,270 --> 01:06:44,280
all right so we can use some of product

1099
01:06:44,300 --> 01:06:51,090
i wrote the sum in concerning computer scientists like myself former computer scientist of

1100
01:06:51,100 --> 01:06:56,000
in the audience because decided to get integrals but just look at intervals and turn

1101
01:06:56,000 --> 01:06:57,810
them into songs

1102
01:06:57,840 --> 01:06:58,650
there some

1103
01:06:58,660 --> 01:07:01,150
the math behind that you know all that

1104
01:07:03,440 --> 01:07:07,220
so we can apply to reliable sorts of interesting things

1105
01:07:07,220 --> 01:07:08,910
for example

1106
01:07:08,940 --> 01:07:10,180
two doing

1107
01:07:10,190 --> 01:07:13,380
inference about the parameters theta

1108
01:07:13,400 --> 01:07:15,150
given some data the

1109
01:07:15,210 --> 01:07:19,470
there are some model and some rule product rule we get bayes rule coming out

1110
01:07:19,520 --> 01:07:25,500
we can give these things names like likelihood prior and posterior

1111
01:07:25,530 --> 01:07:30,530
and then we know somebody says always the user that posted our predictions

1112
01:07:30,530 --> 01:07:32,650
i mean engineer there were predictions

1113
01:07:32,690 --> 01:07:34,060
OK well

1114
01:07:34,270 --> 01:07:37,680
predictions come from some rule

1115
01:07:37,720 --> 01:07:39,240
prior product rule

1116
01:07:39,250 --> 01:07:41,530
there's no other way of doing it

1117
01:07:41,570 --> 01:07:43,620
OK no arbitrariness

1118
01:07:43,660 --> 01:07:48,050
so it's very sensible you will be queen you say yes

1119
01:07:48,060 --> 01:07:49,620
makes sense

1120
01:07:50,390 --> 01:07:55,460
each of parameters make some predictions predictions of probability distribution you don't know which is

1121
01:07:55,460 --> 01:07:59,370
the right parameter you can do anything crazy like pick one of them

1122
01:08:01,370 --> 01:08:04,440
you know average

1123
01:08:04,450 --> 01:08:05,750
because you don't know

1124
01:08:05,750 --> 01:08:10,350
you know that another fundamental thing it's not just two equations in mathematics

1125
01:08:10,360 --> 01:08:13,990
there's something fundamental about bayesians is

1126
01:08:14,040 --> 01:08:16,930
we don't know where uncertain all the time

1127
01:08:16,940 --> 01:08:20,500
the that's why a few of us were wandering around granada

1128
01:08:20,510 --> 01:08:24,800
for half an hour trying to figure restaurant because

1129
01:08:24,920 --> 01:08:29,120
we just didn't know you know what was the best thing to do

1130
01:08:29,130 --> 01:08:35,120
we don't have sampling from them but you know i don't do with this equation

1131
01:08:35,120 --> 01:08:40,800
tells you is you take your predictions you have with respect your parameters and you

1132
01:08:40,800 --> 01:08:43,310
have written them with the weights given by

1133
01:08:43,310 --> 01:08:46,210
you know you are familiar some product rule a from

1134
01:08:46,370 --> 01:08:49,310
the data we observed in the model and the assumptions you put in

1135
01:08:49,810 --> 01:08:52,310
OK so that's one fundamental thing

1136
01:08:52,310 --> 01:08:54,210
you always uncertain

1137
01:08:54,240 --> 01:08:58,740
the only language for dealing with uncertainty is probability theory as far as i'm concerned

1138
01:08:58,830 --> 01:09:00,520
it's a bit like you know

1139
01:09:00,570 --> 01:09:02,580
the language for

1140
01:09:02,630 --> 01:09:05,120
dealing with the rate of change is calculus

1141
01:09:05,160 --> 01:09:09,040
i don't think there are people now arguing i got a better framework and calculus

1142
01:09:09,040 --> 01:09:12,000
for rate of change is something like that you know that that issue has been

1143
01:09:12,000 --> 01:09:12,990
settled for

1144
01:09:13,070 --> 01:09:15,870
a couple hundred years

1145
01:09:15,870 --> 01:09:19,930
the probability is language for dealing with uncertainty

1146
01:09:19,940 --> 01:09:23,300
so we express all forms of uncertainty

1147
01:09:23,350 --> 01:09:27,790
in terms of probabilities and models are all about uncertainty

1148
01:09:28,180 --> 01:09:31,550
the only thing was certain values data

1149
01:09:31,980 --> 01:09:37,670
so everything else we have to treat is uncertain quantities of probability distributions over the

1150
01:09:37,680 --> 01:09:39,700
which represent our

1151
01:09:39,710 --> 01:09:43,080
state of uncertainty before

1152
01:09:43,100 --> 01:09:45,690
and after observing the data

1153
01:09:45,710 --> 01:09:50,000
OK and then we can do other the nice things like model comparisons you

1154
01:09:50,000 --> 01:09:51,440
and a is the set

1155
01:09:51,450 --> 01:09:53,110
and we just

1156
01:09:53,120 --> 01:09:56,530
we just integrate over the set a so here we are integrating the constant one

1157
01:09:58,780 --> 01:10:00,420
that gives us the number

1158
01:10:00,450 --> 01:10:05,110
and we simply regard this as a function that maps set to number

1159
01:10:05,120 --> 01:10:06,230
and that's it

1160
01:10:06,250 --> 01:10:10,050
and we can we can do that we can be a bit more sophisticated

1161
01:10:10,110 --> 01:10:11,200
and put in

1162
01:10:11,210 --> 01:10:13,510
something here some function here

1163
01:10:13,870 --> 01:10:18,220
and the only requirement that we make is that function never becomes negative

1164
01:10:18,540 --> 01:10:22,870
so we're not interested in cases where we can we can integrate over region and

1165
01:10:22,870 --> 01:10:27,510
get number and then we integrate over a larger region and the number decreases

1166
01:10:27,870 --> 01:10:31,030
not looking in that case

1167
01:10:31,050 --> 01:10:34,990
in table and if we make this region larger the integral always increases and on

1168
01:10:34,990 --> 01:10:36,220
strict sense

1169
01:10:38,480 --> 01:10:40,910
and the interpretation here is that

1170
01:10:40,920 --> 01:10:43,870
that year of a is is

1171
01:10:43,880 --> 01:10:48,300
kind of mass of a so in the geometric cases could be volume

1172
01:10:48,350 --> 01:10:51,250
or it could be the physical mass of the body and that is actually this

1173
01:10:51,250 --> 01:10:55,080
is actually the context where the word density comes from density functions if you have

1174
01:10:55,370 --> 01:10:57,390
if you have a physical object

1175
01:10:57,420 --> 01:11:01,140
and you know it's you know it's density is mass density

1176
01:11:01,160 --> 01:11:03,650
so massive volumes if you want to get

1177
01:11:03,670 --> 01:11:08,610
the mass you know the volume you multiply density but if the if the density

1178
01:11:08,610 --> 01:11:13,260
is evenly distributed throughout the object then you have to integrate the density over the

1179
01:11:13,260 --> 01:11:14,990
volume of the object to get the

1180
01:11:17,190 --> 01:11:19,550
and in the case of probabilities

1181
01:11:20,200 --> 01:11:24,910
the idea is that this is the probability mass of of a random event what

1182
01:11:24,910 --> 01:11:28,830
we call random event and this event is some random variable takes its value in

1183
01:11:39,960 --> 01:11:40,890
it's very

1184
01:11:40,910 --> 01:11:42,300
primitive picture

1185
01:11:42,320 --> 01:11:44,500
but it's still have to keep in mind

1186
01:11:44,510 --> 01:11:46,930
she so we have some

1187
01:11:46,930 --> 01:11:48,540
some overall set

1188
01:11:48,600 --> 01:11:50,340
of values that we

1189
01:11:50,360 --> 01:11:53,160
some overall space that i will always call omega

1190
01:11:54,350 --> 01:11:58,630
this is our sample space of random variable taking values in the sample space

1191
01:11:58,640 --> 01:12:02,990
and then we have some thirty years called a

1192
01:12:03,000 --> 01:12:07,260
and the question is how how large is the probability that if we make a

1193
01:12:07,260 --> 01:12:12,040
random graph on a random variable and in this in this sense

1194
01:12:26,310 --> 01:12:31,730
now if we if we think of this function here as in this way so

1195
01:12:31,730 --> 01:12:35,150
far it's only an arbitrary function that maps et cetera number

1196
01:12:35,510 --> 01:12:38,490
that's that's that's what we call a set function is a function that maps sets

1197
01:12:38,490 --> 01:12:39,790
the number

1198
01:12:39,820 --> 01:12:45,760
and but but if we know that if we want to to stick to this

1199
01:12:45,760 --> 01:12:47,750
intuition of this

1200
01:12:47,760 --> 01:12:49,960
this function representing an integral

1201
01:12:49,960 --> 01:12:53,090
then it must have some additional properties so we can look at the properties of

1202
01:12:54,320 --> 01:12:58,100
and the rise properties of that function from

1203
01:12:58,150 --> 01:13:01,070
and integral has certain decomposition properties so

1204
01:13:01,130 --> 01:13:05,470
first of all this one is not actually decomposition property but we always know

1205
01:13:05,490 --> 01:13:09,750
no matter what function we put underneath integral here if we integrate with the empty

1206
01:13:09,750 --> 01:13:11,790
set and we always get zero

1207
01:13:12,510 --> 01:13:16,910
so a measure of the empty set no matter how we what else we do

1208
01:13:16,910 --> 01:13:20,510
to define this measure the measure the entities that must always busy

1209
01:13:21,820 --> 01:13:28,400
the second thing is that if we if we integrate over disjoint sets so i

1210
01:13:28,400 --> 01:13:31,990
think we integrate over one interval and or another into

1211
01:13:32,040 --> 01:13:36,480
then the the integral over the union is just the sum of the two integrals

1212
01:13:36,970 --> 01:13:39,840
the integrals are additive or disjoint sets

1213
01:13:41,020 --> 01:13:44,010
then we can demand that our or we have to be mounted on men should

1214
01:13:44,010 --> 01:13:46,830
also have this property so if we have two disjoint sets and we take the

1215
01:13:46,830 --> 01:13:48,930
union then

1216
01:13:48,930 --> 01:13:53,440
measure of union is some of measures that only works if these are two are

1217
01:13:53,440 --> 01:13:55,340
disjoint because if

1218
01:13:55,400 --> 01:13:57,670
if they overlap recounting things double

1219
01:13:59,460 --> 01:14:02,170
OK but we can do this not only with two sets we can do that

1220
01:14:02,170 --> 01:14:04,260
with three sets forces and so on

1221
01:14:04,290 --> 01:14:06,450
and because in

1222
01:14:06,460 --> 01:14:09,230
in calculus we are able to compute

1223
01:14:09,250 --> 01:14:10,690
infinite sums

1224
01:14:10,710 --> 01:14:15,340
countably infinite sums this actually works was a countably infinite union

1225
01:14:17,040 --> 01:14:21,230
countably infinite sum is something that can be well well-defined converge to converge to a

1226
01:14:23,490 --> 01:14:28,780
OK and finally if we if we have a subset

1227
01:14:28,830 --> 01:14:30,390
set in the subset

1228
01:14:30,430 --> 01:14:31,280
and then

1229
01:14:31,310 --> 01:14:36,080
because we say that that we don't put in negative functions here we know that

1230
01:14:36,080 --> 01:14:39,360
the integral always increases when we when we go from the set to a larger

1231
01:14:40,100 --> 01:14:44,550
so if b is a subset of a then the integral of of the or

1232
01:14:44,550 --> 01:14:48,550
the the measure of b must be smaller than the measure of a

1233
01:14:48,600 --> 01:14:51,880
and the second thing about subsets is

1234
01:14:51,930 --> 01:14:53,250
if we

1235
01:14:53,490 --> 01:14:57,530
if we take the set minus yes so if we if we cut out to

1236
01:14:57,530 --> 01:15:01,890
be set b from set a then the integral will be will be the difference

1237
01:15:01,890 --> 01:15:02,990
of the two

1238
01:15:03,930 --> 01:15:06,740
it's all things that we know about integrals without

1239
01:15:07,720 --> 01:15:11,440
any further making any further assumptions

1240
01:15:14,800 --> 01:15:18,060
and the the historic approach of back

1241
01:15:18,480 --> 01:15:23,030
who originally introduced the introduced measures in integration theory

1242
01:15:23,040 --> 01:15:27,890
was that he that he try to extract from this way that the integral used

1243
01:15:27,890 --> 01:15:29,340
to be five defined

1244
01:15:29,440 --> 01:15:33,450
until then in the way that the integral used to be defined the nineteenth century

1245
01:15:33,460 --> 01:15:37,820
the riemann integral was that you that you have the technical definition of an algorithmic

1246
01:15:37,820 --> 01:15:41,150
definition that tells you how to compute the integral of given function

1247
01:15:42,260 --> 01:15:43,860
he completely

1248
01:15:43,910 --> 01:15:47,130
completely changed the idea in the way that he said OK i'm going to start

1249
01:15:47,130 --> 01:15:49,980
from those integrals that we have defined already

1250
01:15:49,980 --> 01:15:56,860
i know my name is and they and the phd students today i will present

1251
01:15:56,860 --> 01:16:01,590
my work that is about moment approach for structured output prediction

1252
01:16:01,610 --> 01:16:05,880
and i would like to thank the person that i've been involved with the project

1253
01:16:06,060 --> 01:16:09,750
they and we still have to be an edit distance

1254
01:16:09,760 --> 01:16:15,980
from university of bristol and we developed this project while i was at university of

1255
01:16:15,980 --> 01:16:19,340
bristol as visiting student

1256
01:16:19,360 --> 01:16:21,900
and so i will briefly

1257
01:16:21,910 --> 01:16:24,310
the following outline of this talk

1258
01:16:24,330 --> 01:16:29,230
i will briefly introduce what is learning in structured output spaces how we do it

1259
01:16:29,240 --> 01:16:36,230
quite quickly because of the car did whole this morning and i will introduce our

1260
01:16:36,230 --> 01:16:42,420
approach for learning inspectorate output space that are based on the new objective function the

1261
01:16:44,300 --> 01:16:50,310
how we present some experimental results and discuss some computational issues related to our approach

1262
01:16:50,700 --> 01:16:55,150
and i will conclude with some ideas for four works

1263
01:16:55,160 --> 01:16:56,800
OK so

1264
01:16:56,820 --> 01:17:03,180
is a lot of problems that involve structured data which can be represented by sequences

1265
01:17:03,180 --> 01:17:05,240
trees or in general by graphs

1266
01:17:06,070 --> 01:17:14,910
sequences trees and graphs substantially more than temporarily specialist structural dependencies between objects in the

1267
01:17:15,740 --> 01:17:22,270
and this phenomenon writing in several fields such as computer vision computational biology and natural

1268
01:17:22,270 --> 01:17:25,920
language processing or web data analysis

1269
01:17:26,530 --> 01:17:31,900
machine learning and data mining algorithms must be able to treat and to cope with

1270
01:17:31,900 --> 01:17:32,900
this problem

1271
01:17:32,940 --> 01:17:39,120
and to analyse efficiently and automatically disallowed amount of complex and structured data

1272
01:17:39,130 --> 01:17:44,730
and that's why i recently a lot of effort has been poured so developed structure

1273
01:17:44,740 --> 01:17:46,290
learning algorithm

1274
01:17:46,360 --> 01:17:51,740
that can predict complex structures such as sequences trees or graphs

1275
01:17:51,750 --> 01:18:00,590
and using traditional algorithms to cope with problems involving structured that is suboptimal choice because

1276
01:18:00,650 --> 01:18:03,990
sometimes we know often we lost the

1277
01:18:04,010 --> 01:18:10,760
information about the structure of the problem and this concept was explained by task this

1278
01:18:10,760 --> 01:18:16,870
morning i will explain again quickly but with the other case studies

1279
01:18:16,890 --> 01:18:23,820
first of all i will briefly review what is the the traditional framework initial framework

1280
01:18:23,850 --> 01:18:28,770
for supervised learning is that we have that and that are available in form of

1281
01:18:28,770 --> 01:18:34,560
examples and their associated correct concept so we have a simple talk page where the

1282
01:18:34,560 --> 01:18:37,160
input is about y

1283
01:18:37,170 --> 01:18:38,880
he is usually is

1284
01:18:38,910 --> 01:18:45,980
colour and we want to find similar ipods and sage taken from inoperative ipod in

1285
01:18:50,990 --> 01:18:56,510
we wanted to do if the hypothesis performs well on a new test sample

1286
01:18:56,530 --> 01:19:00,850
drawn from the same distribution of the training set

1287
01:19:00,860 --> 01:19:07,030
and to do that usually we propose that the hypothesis performs well on the training

1288
01:19:08,680 --> 01:19:12,400
i think about supervised learning task is classification

1289
01:19:12,440 --> 01:19:19,220
and in classification one should have assigned to object to one of the number of

1290
01:19:21,310 --> 01:19:24,510
oh sorry

1291
01:19:24,580 --> 01:19:28,910
it's a bit

1292
01:19:30,200 --> 01:19:36,490
i mean i will show you an example of classification in the case of named

1293
01:19:36,490 --> 01:19:37,760
entity recognition

1294
01:19:37,870 --> 01:19:43,150
named entity recognition is the task of looking at the name entity the text and

1295
01:19:43,420 --> 01:19:49,800
entity of interest can be personal names organisation names location names or indication of times

1296
01:19:49,800 --> 01:19:52,120
or dates with them so

1297
01:19:52,170 --> 01:19:57,890
this problem can be more than in the traditional framework by a multiclass classification problem

1298
01:19:57,990 --> 01:20:01,750
so all four of for example yet we have the spanish war

1299
01:20:01,770 --> 01:20:07,170
and for each one of the in the sentence this story is spanish sentence for

1300
01:20:07,170 --> 01:20:11,930
each word in the sentence we should assign a label

1301
01:20:11,940 --> 01:20:19,440
and we know how to do that we usually consider so we got the information

1302
01:20:19,440 --> 01:20:25,610
about the world the in the feature vector a x that represent the input we

1303
01:20:25,610 --> 01:20:27,800
want to assign level two

1304
01:20:27,820 --> 01:20:30,390
each one of them so

1305
01:20:30,410 --> 01:20:34,140
we do it apparently so for the first order we should assign

1306
01:20:34,160 --> 01:20:41,110
the class their label all that indicates that p is an organisation

1307
01:20:41,130 --> 01:20:45,100
the same for the second one the same for the third and as well for

1308
01:20:45,100 --> 01:20:50,690
all the words in the sentence but one can easily argue that this is suboptimal

1309
01:20:50,690 --> 01:20:56,210
task because we should be able to consider the correlation between adjacent words

1310
01:20:56,230 --> 01:21:02,880
after that we should do we go we realize a joint labeling for all the

1311
01:21:02,880 --> 01:21:04,490
words in the center

1312
01:21:04,490 --> 01:21:06,740
but we

1313
01:21:10,610 --> 01:21:13,010
exactly what

1314
01:21:17,540 --> 01:21:22,040
eighteen months

1315
01:21:23,750 --> 01:21:26,170
what you see here

1316
01:21:28,100 --> 01:21:34,500
we were all

1317
01:21:36,320 --> 01:21:41,350
we are working on this from

1318
01:21:42,470 --> 01:21:46,320
i assume that work

1319
01:22:05,710 --> 01:22:07,640
now here we are

1320
01:22:07,830 --> 01:22:11,720
one of the kind of

1321
01:22:11,750 --> 01:22:13,730
all right

1322
01:22:27,090 --> 01:22:33,060
are or

1323
01:22:34,150 --> 01:22:39,030
if you are going

1324
01:22:43,330 --> 01:22:48,160
the second or you are in

1325
01:22:57,220 --> 01:23:04,300
all right

1326
01:23:04,460 --> 01:23:08,550
many of

1327
01:23:08,640 --> 01:23:22,160
or if they it would be

1328
01:23:23,260 --> 01:23:25,890
one of the

1329
01:24:08,510 --> 01:24:17,550
the problem

1330
01:24:45,240 --> 01:24:47,870
on the whole

1331
01:25:02,180 --> 01:25:07,530
all of

1332
01:25:39,560 --> 01:25:42,910
i b

1333
01:25:42,910 --> 01:25:44,750
now we need to decide what i measure replacements

1334
01:25:45,490 --> 01:25:48,380
and when we measure it so we can measure it here and here and here and

1335
01:25:50,430 --> 01:25:53,030
and add up all these distances between

1336
01:25:53,460 --> 01:25:53,920
these two

1337
01:25:56,890 --> 01:25:57,420
so yes

1338
01:25:58,720 --> 01:26:02,840
and when it have way of measuring distances between the two differences some sort

1339
01:26:03,340 --> 01:26:05,870
and we need have a decision about what's the density

1340
01:26:06,520 --> 01:26:08,230
points that which we measure the difference

1341
01:26:09,140 --> 01:26:09,440
we could

1342
01:26:09,870 --> 01:26:12,170
so i only care about the city in this region

1343
01:26:12,740 --> 01:26:13,600
so might say what

1344
01:26:14,420 --> 01:26:15,490
efficient idea

1345
01:26:16,010 --> 01:26:16,720
what do you hear

1346
01:26:17,970 --> 01:26:18,270
so this

1347
01:26:20,090 --> 01:26:20,910
there's an issue of

1348
01:26:21,320 --> 01:26:22,130
how do we measure

1349
01:26:22,890 --> 01:26:25,880
and you come up with a whole advances are not saying it is impossible there

1350
01:26:25,880 --> 01:26:29,340
is the the number of ways of measuring how close to see

1351
01:26:30,000 --> 01:26:30,310
each other

1352
01:26:31,050 --> 01:26:33,970
an idea that has a problem because it now means there an infinite number of ways

1353
01:26:34,750 --> 01:26:37,630
that's the question is how do we fit lambda

1354
01:26:38,160 --> 01:26:39,590
the state what we think lenders

1355
01:26:41,140 --> 01:26:42,760
okay so you do things that yes

1356
01:26:43,420 --> 01:26:47,640
but we get rid a bit weird but it still leaves some interesting questions left over yes

1357
01:26:50,810 --> 01:26:53,660
really aren't okay so idea number six

1358
01:26:54,170 --> 01:26:57,670
how do we find the mean of the gas in distribution well we just take

1359
01:26:57,930 --> 01:27:01,550
all the data points then we summarize the right plan

1360
01:27:03,350 --> 01:27:05,270
and then gives us a number we call the data

1361
01:27:05,690 --> 01:27:09,490
and then we can use them to construct an estimator and for galcians

1362
01:27:10,190 --> 01:27:11,080
new hat

1363
01:27:11,630 --> 01:27:14,580
equals x by actually turned out to be a really sensible idea

1364
01:27:15,230 --> 01:27:18,710
so that's something we can do with no bins that all we can take all these data points

1365
01:27:19,170 --> 01:27:19,910
we can take some

1366
01:27:22,540 --> 01:27:26,180
because the windows got the left-hand side remember the left hand side window

1367
01:27:26,910 --> 01:27:27,980
is at eight

1368
01:27:28,400 --> 01:27:33,070
so you can't have any points left and i made it be sensible to subtract of eh

1369
01:27:37,200 --> 01:27:37,470
so q

1370
01:27:39,020 --> 01:27:39,490
not essential

1371
01:27:41,420 --> 01:27:43,270
that's interesting thing that's the mean

1372
01:27:44,190 --> 01:27:46,150
all the distances from the left hand side window

1373
01:27:47,300 --> 01:27:48,520
and that's clearly has

1374
01:27:49,310 --> 01:27:54,130
and expected dependence on land big about value lambda has more points all

1375
01:27:54,590 --> 01:27:58,870
search to the right hand side and eventually they don't have the right to go to the middle average

1376
01:27:59,510 --> 01:28:01,500
this window if you have a really large value lambda

1377
01:28:02,320 --> 01:28:04,050
then you get a uniform distribution

1378
01:28:05,200 --> 01:28:07,680
this is precisely related to lambda

1379
01:28:08,850 --> 01:28:10,770
but in the case get goes to infinity

1380
01:28:11,540 --> 01:28:14,320
if the window is infinitely wide on the right hand side

1381
01:28:16,120 --> 01:28:18,510
something you might remember about exponential distributions

1382
01:28:19,250 --> 01:28:20,170
i mean this guy

1383
01:28:20,650 --> 01:28:21,850
he effects given lambda

1384
01:28:22,530 --> 01:28:24,330
this is the minus on lambda now

1385
01:28:25,050 --> 01:28:25,540
we know in

1386
01:28:27,120 --> 01:28:27,670
my by z

1387
01:28:28,650 --> 01:28:29,650
i mean this distribution

1388
01:28:30,080 --> 01:28:30,580
is lambda

1389
01:28:32,400 --> 01:28:32,990
expected value

1390
01:28:34,340 --> 01:28:34,770
is lambda

1391
01:28:35,260 --> 01:28:36,810
so i think that's a really good idea

1392
01:28:37,990 --> 01:28:40,410
maybe there's something in the idea of computing the mean

1393
01:28:46,200 --> 01:28:47,310
absolutely we can

1394
01:28:47,760 --> 01:28:51,890
so you're now saying let's have a measure of discrepancy between the data mining

1395
01:28:53,870 --> 01:28:54,510
the predicted me

1396
01:28:56,210 --> 01:28:59,620
let's imagine doing at it is not a very difficult calculation to do

1397
01:29:01,160 --> 01:29:02,730
so i will just sketch in which

1398
01:29:04,490 --> 01:29:06,030
and we keep asking questions as well

1399
01:29:12,540 --> 01:29:13,900
the expected value x

1400
01:29:14,550 --> 01:29:17,160
for the real distribution is integral from there to be

1401
01:29:18,130 --> 01:29:19,460
it might affect lambda

1402
01:29:21,870 --> 01:29:22,620
divided by z

1403
01:29:26,180 --> 01:29:30,420
and you can solve at and it's they function of avian so this is some sort of

1404
01:29:31,280 --> 01:29:32,330
basically new all

1405
01:29:39,900 --> 01:29:40,180
this is

1406
01:29:41,160 --> 01:29:41,620
this is be

1407
01:29:42,840 --> 01:29:46,550
this is lambda what new actually looks like this is roughly linear

1408
01:29:46,550 --> 01:29:54,790
if you want to look start

1409
01:29:54,810 --> 01:29:57,560
and then click for the latest nightly built

1410
01:29:57,580 --> 01:29:59,310
and that should watch

1411
01:30:00,950 --> 01:30:04,680
the other thing is that i want to do this

1412
01:30:04,740 --> 01:30:11,720
so the people who are starting to keep on going people who want to download

1413
01:30:11,720 --> 01:30:14,970
it go to swim downwards

1414
01:30:14,990 --> 01:30:18,390
like their systems and in this case

1415
01:30:18,410 --> 01:30:21,490
you probably want one of the nightly builds

1416
01:30:22,890 --> 01:30:26,140
this will be already mapped maccabi but

1417
01:30:30,990 --> 01:30:33,580
so go into the folder that's that's

1418
01:30:34,580 --> 01:30:36,240
when you when you've unzipped up to

1419
01:30:36,260 --> 01:30:38,490
and there's a couple of

1420
01:30:38,520 --> 01:30:39,870
brown's first

1421
01:30:39,890 --> 01:30:44,290
one of them is what it's like seems

1422
01:30:50,100 --> 01:30:53,060
if you're going to be going along the

1423
01:30:53,080 --> 01:30:55,600
very much all of them

1424
01:31:03,020 --> 01:31:06,020
so these on some capital

1425
01:31:06,020 --> 01:31:08,640
but it did not have to work

1426
01:33:38,600 --> 01:33:39,490
the model

1427
01:33:39,550 --> 01:33:59,540
so it so it's very convenient something

1428
01:34:04,220 --> 01:34:05,770
this is just a memory

1429
01:34:28,770 --> 01:34:31,720
and o

1430
01:35:55,790 --> 01:35:59,740
there's a lot of

1431
01:35:59,740 --> 01:36:04,700
most people mostly

1432
01:36:04,720 --> 01:36:06,850
very slow

1433
01:36:06,870 --> 01:36:10,580
so also

1434
01:36:21,660 --> 01:36:25,600
so far there

1435
01:36:25,620 --> 01:36:27,200
you can just

1436
01:36:27,200 --> 01:36:29,430
the are fine

1437
01:36:29,450 --> 01:36:30,310
on the web

1438
01:36:30,330 --> 01:36:31,740
and then to load

1439
01:36:31,760 --> 01:36:33,740
an ontology learning

1440
01:36:37,890 --> 01:36:42,080
all of this is

1441
01:36:42,200 --> 01:36:50,430
one of the

1442
01:36:54,970 --> 01:36:58,680
these these things so i have this too

1443
01:36:59,640 --> 01:37:03,020
an ontology and the web you can do to my these slides are public so

1444
01:37:03,020 --> 01:37:05,390
it might be that if you if you went there

1445
01:37:05,390 --> 01:37:07,220
to copy and paste your rules

1446
01:37:07,220 --> 01:37:12,330
we're going to be

1447
01:37:20,020 --> 01:37:23,370
one of the

1448
01:37:23,540 --> 01:37:25,930
and before

1449
01:37:25,930 --> 01:37:31,540
now that we are going to change your redirect or something useful

1450
01:37:31,790 --> 01:37:39,660
so i'm going to think what need to use color other ontologies

1451
01:37:39,680 --> 01:37:40,640
it's a

1452
01:37:40,660 --> 01:37:44,040
don't want to switch to everybody so diligently typing

1453
01:37:44,060 --> 01:38:10,040
you can also just google for claude that should come up

1454
01:38:10,100 --> 01:38:13,810
is god

1455
01:38:13,810 --> 01:38:14,760
living be

1456
01:38:46,120 --> 01:38:47,810
to replace the OK

1457
01:38:47,830 --> 01:38:49,040
and let me

1458
01:38:49,060 --> 01:38:53,890
so we can have the ability to

1459
01:38:53,910 --> 01:38:55,260
since all of the word

1460
01:38:59,600 --> 01:39:00,700
so you can

1461
01:39:01,430 --> 01:39:03,740
you can probably find an ontology and the work

1462
01:39:03,760 --> 01:39:07,320
you're all just pop it and you can look at the bookmarks my bookmarks would

1463
01:39:07,320 --> 01:39:10,490
be a little bit different than yours but it's easy enough to

1464
01:39:10,500 --> 01:39:14,560
click on move on

1465
01:39:15,410 --> 01:39:17,160
they also get a humiliated

1466
01:39:21,790 --> 01:39:24,310
you know

1467
01:39:25,580 --> 01:39:26,770
so we did

1468
01:39:26,790 --> 01:39:29,870
the system and see the UK

1469
01:39:31,870 --> 01:39:32,700
of the

1470
01:39:38,370 --> 01:39:41,660
so far

1471
01:39:58,560 --> 01:40:01,830
running the

1472
01:40:02,950 --> 01:40:06,560
nine of the one so you have to look at it to make it work

1473
01:40:08,490 --> 01:40:10,680
will be looking for

1474
01:40:10,700 --> 01:40:12,450
and what is not discrimination

1475
01:40:12,640 --> 01:40:14,160
it is

1476
01:40:16,140 --> 01:40:20,450
if you look at the patterns given the parameters you need template them

1477
01:40:33,000 --> 01:40:36,180
i do i

1478
01:40:47,200 --> 01:40:49,000
told the passes

1479
01:40:49,060 --> 01:40:52,990
in this

1480
01:41:18,040 --> 01:41:19,410
one to two million

1481
01:41:19,430 --> 01:41:22,290
what are only the issues feel free

1482
01:41:22,310 --> 01:41:24,290
and this is something that's a good thing to do

1483
01:41:24,350 --> 01:41:27,760
you should spend some time doing it just loading arbitrary

1484
01:41:27,760 --> 01:41:29,240
we are getting is the

1485
01:41:29,280 --> 01:41:32,930
comparison between two values in association

1486
01:41:33,010 --> 01:41:34,510
of one value

1487
01:41:34,510 --> 01:41:36,450
with the object so

1488
01:41:36,470 --> 01:41:38,970
trying to get through this quickly and

1489
01:41:39,030 --> 01:41:40,780
the sometimes it's nine OK

1490
01:41:41,200 --> 01:41:43,090
let's let's move on

1491
01:41:43,110 --> 01:41:46,320
because i want to get to one other thing

1492
01:41:46,910 --> 01:41:52,590
and yet this is in progress and we want to do more with

1493
01:41:53,930 --> 01:41:58,610
interpretations for comparisons and

1494
01:41:58,750 --> 01:42:02,740
OK so another example

1495
01:42:02,780 --> 01:42:05,070
it was a derivational morphology

1496
01:42:05,090 --> 01:42:05,840
it is

1497
01:42:05,840 --> 01:42:08,740
these nouns like assassination which are

1498
01:42:08,740 --> 01:42:10,180
which pretty

1499
01:42:11,300 --> 01:42:13,090
are the verbal so

1500
01:42:13,180 --> 01:42:16,590
suffixes produced nouns from verbs

1501
01:42:16,610 --> 01:42:18,340
they appear

1502
01:42:18,570 --> 01:42:20,860
is like fascination of senior

1503
01:42:21,900 --> 01:42:23,820
there are times

1504
01:42:28,530 --> 01:42:30,590
the idea is

1505
01:42:31,640 --> 01:42:34,630
like verbs they have associated roles

1506
01:42:34,680 --> 01:42:36,800
the victim and perpetrator and so the

1507
01:42:36,910 --> 01:42:38,470
question is how do you

1508
01:42:38,490 --> 01:42:40,300
link the noun phrase

1509
01:42:40,320 --> 01:42:42,430
to the appropriate role

1510
01:42:42,530 --> 01:42:49,130
and this is actually a big topic in linguistics ever since chance wrote remarks

1511
01:42:49,180 --> 01:42:50,720
on nominalisation

1512
01:42:54,740 --> 01:42:57,760
the state of the art in terms of

1513
01:42:57,780 --> 01:43:00,610
the description of the phenomenon as far as i know

1514
01:43:01,680 --> 01:43:08,530
given in this book by jane grimshaw who argued that there are some

1515
01:43:08,570 --> 01:43:16,050
some nouns that have argument structure like verbs and nouns that don't so in general

1516
01:43:16,090 --> 01:43:19,930
nouns do appear to be somewhat like verbs that they can take the same

1517
01:43:22,050 --> 01:43:26,530
the enemy destroyed the city and that corresponds to the enemy's destruction of the city

1518
01:43:26,970 --> 01:43:30,660
or the destruction of the city by the enemy

1519
01:43:30,760 --> 01:43:31,930
so the

1520
01:43:34,840 --> 01:43:37,180
the enemy is the agent just to

1521
01:43:37,200 --> 01:43:38,880
the agent and the

1522
01:43:39,740 --> 01:43:41,030
transitive sentence

1523
01:43:41,030 --> 01:43:46,240
and you can have CP complements for both nouns and verbs infinitival complements locative PP

1524
01:43:46,240 --> 01:43:50,720
complement but the difference between verbs and nouns but

1525
01:43:50,720 --> 01:43:52,260
whereas for us

1526
01:43:53,220 --> 01:43:57,010
obligatory arguments seems like

1527
01:43:57,010 --> 01:43:59,530
the of phrase is optional pronounce

1528
01:43:59,550 --> 01:44:04,280
so you can see the doctor's examination was successful but or

1529
01:44:04,320 --> 01:44:07,950
the doctor's examination of the patients was successful but

1530
01:44:07,970 --> 01:44:12,260
you can see the doctor examined you have to have the object

1531
01:44:13,910 --> 01:44:19,630
but argues that if you disambiguate between process readings

1532
01:44:19,990 --> 01:44:22,590
it turns out that some nouns really do have

1533
01:44:22,590 --> 01:44:25,220
are structure like verbs and

1534
01:44:25,240 --> 01:44:27,550
have obligatory arguments

1535
01:44:34,990 --> 01:44:36,370
on the result

1536
01:44:36,410 --> 01:44:41,040
the name means like the exam that was given to the students john's examination was

1537
01:44:42,750 --> 01:44:46,620
there is an asian person john

1538
01:44:46,640 --> 01:44:48,960
is the author of the examination

1539
01:44:50,750 --> 01:44:53,830
process really

1540
01:44:53,870 --> 01:44:58,400
john's examination of the patients took a long time john is the one

1541
01:44:58,400 --> 01:45:01,110
during the examination to

1542
01:45:01,120 --> 01:45:03,180
but the difference

1543
01:45:03,210 --> 01:45:07,580
is the agent and examining that

1544
01:45:11,420 --> 01:45:16,370
has the phrase disambiguates between result in the process

1545
01:45:17,970 --> 01:45:21,990
and maybe i shouldn't talk about this for too long but

1546
01:45:30,580 --> 01:45:34,780
yes of always good to push so

1547
01:45:34,790 --> 01:45:37,110
comm shows

1548
01:45:37,130 --> 01:45:42,290
conclusion is that like verbs certain nouns do have obligatory arguments that there's

1549
01:45:43,040 --> 01:45:44,670
structure in there

1550
01:45:48,550 --> 01:45:51,080
his her turn from process nominals

1551
01:45:51,170 --> 01:45:53,460
are the ones that have obligatory arguments

1552
01:45:55,040 --> 01:45:59,000
four process nominals the of phrase is obligatory

1553
01:45:59,090 --> 01:46:03,320
linked to this to whatever the object would be of the verb

1554
01:46:03,760 --> 01:46:12,510
and then the question becomes which nouns have process readings and therefore had this argument

1555
01:46:12,510 --> 01:46:14,450
structure associated with them

1556
01:46:14,960 --> 01:46:19,530
it's not the case that any noun that has a corresponding verb

1557
01:46:19,580 --> 01:46:22,420
has process reading so

1558
01:46:22,420 --> 01:46:24,500
you can say to another child

1559
01:46:25,370 --> 01:46:27,420
mother child doesn't mean

1560
01:46:27,470 --> 01:46:29,790
the mothering of a child an event

1561
01:46:29,880 --> 01:46:31,040
mother and child

1562
01:46:31,040 --> 01:46:33,120
so zero right now

1563
01:46:34,250 --> 01:46:35,460
process readings

1564
01:46:35,600 --> 01:46:43,900
and it seems to be pretty much the only morphologically complex nouns like examination assignment

1565
01:46:43,960 --> 01:46:46,080
have this process readings

1566
01:46:46,130 --> 01:46:47,580
although i was

1567
01:46:49,250 --> 01:46:50,110
the we

1568
01:46:50,120 --> 01:46:52,490
the plane here

1569
01:46:53,620 --> 01:47:02,160
elders in marjah the former taliban stronghold seized this month in a NATO offensive

1570
01:47:02,160 --> 01:47:06,500
he president he was a piece of their minds this week during his first visit

1571
01:47:06,500 --> 01:47:07,500
to the town

1572
01:47:07,540 --> 01:47:08,900
and sharing

1573
01:47:08,960 --> 01:47:13,870
the leaders complained of excessive use of force by u s troops here

1574
01:47:13,960 --> 01:47:14,910
we have

1575
01:47:16,090 --> 01:47:19,280
which is not what it lacks

1576
01:47:19,320 --> 01:47:22,220
along the of phrase and the by phrase

1577
01:47:22,250 --> 01:47:26,930
and according to show that should be used is the process normal so i get

1578
01:47:26,930 --> 01:47:28,580
maybe users

1579
01:47:28,620 --> 01:47:30,670
isolated examples of

1580
01:47:30,700 --> 01:47:33,180
morphologically simple process normal but

1581
01:47:33,200 --> 01:47:35,620
i think that by looking

1582
01:47:37,550 --> 01:47:39,530
such as the new york times we would

1583
01:47:39,610 --> 01:47:43,370
come to a better understanding of what's actually going

1584
01:47:43,450 --> 01:47:45,030
process nominals

1585
01:47:47,570 --> 01:47:51,360
but anyway that exceptional site

1586
01:47:51,420 --> 01:47:52,910
have this rule here

1587
01:47:55,110 --> 01:47:57,870
the semantic interpretation for

1588
01:47:57,870 --> 01:48:02,200
so here

1589
01:48:11,930 --> 01:48:18,820
so called because most of them also show

1590
01:48:24,140 --> 01:48:27,530
right thanks colin for the clutch great to be here by the way

1591
01:48:27,530 --> 01:48:32,210
the first team to economy b six years ago my first trip to sub-saharan africa

1592
01:48:32,220 --> 01:48:35,640
and i liked it so much i end up coming back to live in UK

1593
01:48:35,640 --> 01:48:39,840
and on the other side so i've been mcgarry for about three years now

1594
01:48:39,850 --> 01:48:43,900
and one thing i found is there are abundant opportunities for machine learning

1595
01:48:43,920 --> 01:48:47,970
and that's something i have to tell you a bit about

1596
01:48:47,990 --> 01:48:50,340
current standards

1597
01:48:50,390 --> 01:48:52,570
so by the way if you're

1598
01:48:52,600 --> 01:48:56,420
puzzled curious scared stick your hand up and

1599
01:48:56,880 --> 01:48:59,020
sort and

1600
01:48:59,030 --> 01:49:00,210
OK so the

1601
01:49:00,990 --> 01:49:03,040
the thing we want to deal with here is is

1602
01:49:03,060 --> 01:49:06,760
machines being able to deal with very high level task

1603
01:49:07,820 --> 01:49:09,120
where you

1604
01:49:09,140 --> 01:49:13,450
have something specified in very abstract terms or something very high level then we need

1605
01:49:13,450 --> 01:49:16,460
to be able to deal with uncertainty

1606
01:49:16,590 --> 01:49:20,870
so this uncertainty might come from different slightly different types of uncertainty

1607
01:49:21,980 --> 01:49:25,450
you might have a problem which is defined incompletely

1608
01:49:25,450 --> 01:49:29,980
so we have examples of some of the types of input output we might want

1609
01:49:30,430 --> 01:49:33,400
so this is this is the face of

1610
01:49:33,450 --> 01:49:34,100
was a

1611
01:49:34,120 --> 01:49:39,730
find me another collection from the set of photographs which have face and but we

1612
01:49:39,730 --> 01:49:44,010
only have a few examples the problem is not completely specified the input space is

1613
01:49:44,010 --> 01:49:49,840
very large and we've got to try to work out what to do for unseen

1614
01:49:50,040 --> 01:49:54,340
OK we may also have uncertainty when the problem is just too big so think

1615
01:49:54,340 --> 01:49:57,650
of the traveling salesman person if we have a hundred cities that we need to

1616
01:49:57,650 --> 01:50:02,850
run between and find the shortest the shortest tour covering all cities the number of

1617
01:50:04,200 --> 01:50:08,120
is it's not only larger than the number of atoms in the universe of just

1618
01:50:08,120 --> 01:50:12,430
notices the squared to the number of atoms in the universe square that number and

1619
01:50:12,430 --> 01:50:15,530
you get the a number of ways ordering hundred city so this

1620
01:50:15,580 --> 01:50:19,090
obviously no possible way that any machine can work that out

1621
01:50:19,110 --> 01:50:20,460
exactly so we

1622
01:50:20,610 --> 01:50:24,900
to employ some approximation uncertainty in in the situation

1623
01:50:24,910 --> 01:50:30,680
we may also be trying to tackle the problem which is unheard inherently uncertain so

1624
01:50:30,680 --> 01:50:33,270
if we look at whether it's a chaotic system

1625
01:50:34,900 --> 01:50:40,610
never going to have enough information to be able to find an exact predictions

1626
01:50:40,620 --> 01:50:44,530
so i guess the

1627
01:50:44,550 --> 01:50:48,550
the angle what if you're coming to this from is being used to writing software

1628
01:50:48,560 --> 01:50:52,440
where the some inputs some outputs it's pretty well specified

1629
01:50:52,490 --> 01:50:57,330
what do we get this inverts we follow some logic and there's is that there

1630
01:50:57,340 --> 01:50:58,020
is the

1631
01:50:58,030 --> 01:50:59,400
action we take

1632
01:51:00,000 --> 01:51:01,810
but here that's

1633
01:51:01,810 --> 01:51:05,530
not really going to be enough we have this uncertainty to deal with and the

1634
01:51:05,530 --> 01:51:08,560
solution is we have various techniques to show

1635
01:51:08,620 --> 01:51:12,310
our machine examples of what we want to achieve and have it learn what to

1636
01:51:13,210 --> 01:51:14,490
so we look at how to

1637
01:51:14,520 --> 01:51:18,960
how to get that working for a few a few problems

1638
01:51:18,960 --> 01:51:22,270
it's going to be a fairly practical session

1639
01:51:22,400 --> 01:51:27,610
i think lines for how to are kind of cookbook various recipes to to get

1640
01:51:27,610 --> 01:51:29,210
things working

1641
01:51:29,330 --> 01:51:30,830
it's going to be

1642
01:51:32,370 --> 01:51:34,960
broad rather than deep we

1643
01:51:34,960 --> 01:51:36,310
we're going to want to cover

1644
01:51:36,310 --> 01:51:39,710
you know if few concepts and in order to do that within the time we

1645
01:51:39,710 --> 01:51:44,490
have will not be able to go very far into the into each particular technique

1646
01:51:44,490 --> 01:51:51,090
but at least hopefully you will get a flavor of how they had they work

1647
01:51:51,750 --> 01:51:53,270
but let me show you some

1648
01:51:53,280 --> 01:51:57,090
examples of uncertainty in practical applications particularly ones which

1649
01:51:57,090 --> 01:51:59,330
you may be interested in tackling here

1650
01:51:59,340 --> 01:52:04,090
will think about how to deal with uncertainty in general terms

1651
01:52:04,110 --> 01:52:07,490
look at a few more examples of what what kind of machine learning applications we

1652
01:52:07,490 --> 01:52:11,800
might be interested in and then after we've seen some applications we've got an idea

1653
01:52:11,800 --> 01:52:12,500
of the

1654
01:52:12,520 --> 01:52:15,490
problems we might want to solve then we'll

1655
01:52:15,490 --> 01:52:18,990
fire into how to go about solving them

1656
01:52:19,070 --> 01:52:23,300
all right here's a a blood smear

1657
01:52:23,310 --> 01:52:27,110
and i'm sure all of you have had the experience where you start to feel

1658
01:52:27,110 --> 01:52:32,430
feverish you have to go down to get get malaria test guy pricks her finger

1659
01:52:32,440 --> 01:52:34,010
some on the glass

1660
01:52:34,040 --> 01:52:37,180
slide they stain put under a microscope

1661
01:52:37,180 --> 01:52:39,350
now when the lab technician is

1662
01:52:39,360 --> 01:52:42,810
peering through the microscope this is what he is looking at

1663
01:52:42,820 --> 01:52:44,800
and we have

1664
01:52:44,810 --> 01:52:48,080
very is broken down red blood cell material

1665
01:52:48,100 --> 01:52:51,570
and the dark stains spots are DNA

1666
01:52:51,600 --> 01:52:56,570
matters so we have nucleotides

1667
01:52:56,750 --> 01:53:00,990
we have this guy here which is present in so this is the this is

1668
01:53:00,990 --> 01:53:05,170
the parasite and he was caught in its engagement rings stage was the kind of

1669
01:53:05,180 --> 01:53:06,390
ring with full

1670
01:53:06,440 --> 01:53:07,810
you click on that

1671
01:53:08,080 --> 01:53:11,660
and when you speak to lab technicians about what they are looking for

1672
01:53:12,770 --> 01:53:15,880
explain the pattern in those kind of terms i'm looking for

1673
01:53:15,890 --> 01:53:18,100
engagement rings are looking for a common

1674
01:53:18,110 --> 01:53:23,020
or pair of headphones with these two nuclei in some connecting matter between them

1675
01:53:23,070 --> 01:53:26,700
now if you could take these images and

1676
01:53:26,740 --> 01:53:29,120
automatically from this image

1677
01:53:29,130 --> 01:53:33,870
be able to identify here's where the parasite is that would be an application of

1678
01:53:33,870 --> 01:53:36,310
enormous value we can start automate the

1679
01:53:36,370 --> 01:53:40,480
the testing process so even more experts are not available to

1680
01:53:40,520 --> 01:53:43,750
two look through the microscope we might be able to

1681
01:53:43,880 --> 01:53:48,040
make some headway on being able to do that automatically

1682
01:53:48,050 --> 01:53:51,570
so let's think about the uncertainty in here

1683
01:53:51,580 --> 01:53:55,500
when we have an image that i think about the space the input space of

1684
01:53:55,520 --> 01:53:57,140
possible images

1685
01:53:57,140 --> 01:53:59,950
high order cliques if if i get around

1686
01:54:06,280 --> 01:54:09,220
you call this pcx energy function

1687
01:54:09,530 --> 01:54:14,050
now the point about this which is the one of these pcx this is the

1688
01:54:14,050 --> 01:54:17,090
semantic function associated with the clique c

1689
01:54:17,210 --> 01:54:18,850
it takes as

1690
01:54:18,920 --> 01:54:23,160
input the random victim but the important thing is only depends

1691
01:54:23,200 --> 01:54:26,900
on the values of those XI which automatically

1692
01:54:28,850 --> 01:54:32,980
now in this particular case means your energy functions are the energy functions of two

1693
01:54:34,150 --> 01:54:36,720
or energy functions of one variable

1694
01:54:38,290 --> 01:54:41,110
because i have clique size two

1695
01:54:54,320 --> 01:54:56,400
and all that satisfy

1696
01:54:56,410 --> 01:55:00,610
it's fine expect right you have an energy function for those two

1697
01:55:00,620 --> 01:55:02,850
and g function for those

1698
01:55:04,900 --> 01:55:09,490
so in fact that's going to happen it's weight cop comes down to age weight

1699
01:55:09,740 --> 01:55:12,100
since for each other

1700
01:55:12,110 --> 01:55:16,680
now slides and not new as i have a slightly later on which says this

1701
01:55:16,850 --> 01:55:19,160
just say now

1702
01:55:19,720 --> 01:55:20,600
i find it

1703
01:55:24,070 --> 01:55:30,410
yes maximizing typically what you want to is maximize the probability of finding most probable

1704
01:55:30,460 --> 01:55:32,110
o thing so

1705
01:55:32,120 --> 01:55:33,290
that means

1706
01:55:33,300 --> 01:55:34,290
since the

1707
01:55:34,300 --> 01:55:36,970
if you take the logarithm of the formula

1708
01:55:38,120 --> 01:55:42,210
the logarithm of problem xie xie was constant which is log k

1709
01:55:42,260 --> 01:55:43,960
log one z rather

1710
01:55:44,010 --> 01:55:46,790
and the product becomes some

1711
01:55:46,850 --> 01:55:51,260
and the exponential go away because taking the logarithm of so you get the log

1712
01:55:51,260 --> 01:55:53,760
the probability that for

1713
01:55:53,770 --> 01:55:56,210
log px constant minus

1714
01:55:56,270 --> 01:55:59,010
some of these energy functions

1715
01:55:59,020 --> 01:56:03,220
and so if you take the maximum probability which is what we want to maximize

1716
01:56:03,220 --> 01:56:05,450
the probability of the minimum

1717
01:56:05,460 --> 01:56:09,160
of this energy function right so in other words

1718
01:56:09,200 --> 01:56:11,410
that's important result here is

1719
01:56:11,420 --> 01:56:13,590
any markov random field

1720
01:56:13,650 --> 01:56:17,920
you find the maximum probability essentially just an energy minimisation problem

1721
01:56:17,930 --> 01:56:19,280
having said that

1722
01:56:19,280 --> 01:56:21,070
we really just

1723
01:56:22,020 --> 01:56:26,020
for all intents and purposes last is the way we think about markov random fields

1724
01:56:26,020 --> 01:56:27,270
energy function

1725
01:56:27,290 --> 01:56:31,390
where the sum of energy functions which just one two

1726
01:56:31,400 --> 01:56:34,450
very depending on the size of the clique

1727
01:56:40,870 --> 01:56:42,340
of y energy

1728
01:56:42,350 --> 01:56:47,470
well you could cost like

1729
01:56:47,510 --> 01:56:50,610
i mean it's so by analogy with them

1730
01:56:50,620 --> 01:56:55,190
you think of things having springs between ages and you try to find out

1731
01:56:55,940 --> 01:57:01,300
you're trying to get down to the maximum state it's also it is energy when

1732
01:57:01,300 --> 01:57:02,680
you do these things with

1733
01:57:03,180 --> 01:57:07,640
in sort thermodynamic situations and things like that which also

1734
01:57:07,650 --> 01:57:09,920
represent by markov random field

1735
01:57:09,930 --> 01:57:15,240
basically thermodynamic systems will tend to go for negative energy states in fact these costs

1736
01:57:15,240 --> 01:57:17,010
represent energy

1737
01:57:17,020 --> 01:57:20,310
in that sense

1738
01:57:24,460 --> 01:57:26,880
potential certainly not connecticut moving

1739
01:57:30,020 --> 01:57:35,150
let's let's look now the case we just got two labels so each each random

1740
01:57:35,150 --> 01:57:37,250
variable has two labels

1741
01:57:37,260 --> 01:57:38,550
going to find

1742
01:57:39,370 --> 01:57:40,940
energy function

1743
01:57:40,950 --> 01:57:42,050
i will be

1744
01:57:42,200 --> 01:57:45,400
function from b and b is zero and one

1745
01:57:45,470 --> 01:57:49,080
the real right generally that for

1746
01:57:49,090 --> 01:57:52,460
the exiles the variables

1747
01:57:52,500 --> 01:57:59,090
literals these exile the exile barron all right so excited by our

1748
01:57:59,100 --> 01:58:00,040
excited by it

1749
01:58:00,060 --> 01:58:02,400
one minus six five point being

1750
01:58:02,430 --> 01:58:03,930
effect size one

1751
01:58:03,940 --> 01:58:08,040
zero zero this is why i

1752
01:58:08,090 --> 01:58:11,950
x one minus six i

1753
01:58:15,230 --> 01:58:19,940
generally were looking at we're looking at any energy function which is just where we've

1754
01:58:19,940 --> 01:58:21,700
only got two labels

1755
01:58:21,740 --> 01:58:24,180
is of this form so let's look at

1756
01:58:24,220 --> 01:58:26,840
there are various ways to represent things you

1757
01:58:26,850 --> 01:58:31,370
functions one is to list all the values for all possibilities all

1758
01:58:31,390 --> 01:58:32,630
two to the end

1759
01:58:34,350 --> 01:58:36,340
where n is the number of variables

1760
01:58:36,380 --> 01:58:38,060
think positive form

1761
01:58:38,170 --> 01:58:41,640
positive form which would probably need to do much

1762
01:58:41,650 --> 01:58:45,770
is like this constant posteriors linear terms

1763
01:58:45,800 --> 01:58:47,440
single very last

1764
01:58:47,510 --> 01:58:53,270
double ones plus so on where you i are either

1765
01:58:53,320 --> 01:58:54,400
x i

1766
01:58:54,460 --> 01:58:55,960
there are other nexi

1767
01:58:56,060 --> 01:58:57,680
born excited by

1768
01:58:59,600 --> 01:59:03,590
all your constant asia theropod

1769
01:59:03,680 --> 01:59:08,200
we'll see that or finally you can always put polynomial for

1770
01:59:08,210 --> 01:59:09,550
in in

1771
01:59:09,590 --> 01:59:14,900
in the variables so ultimately women i polynomial function

1772
01:59:15,340 --> 01:59:19,880
values of x i which is zero one

1773
01:59:20,870 --> 01:59:22,480
let's have a look

1774
01:59:22,540 --> 01:59:25,570
at that is the example tabla

1775
01:59:28,020 --> 01:59:31,920
so i listed x just got three variables here for instance of is the x

1776
01:59:31,930 --> 01:59:35,730
one x two three all eight possible values

1777
01:59:35,770 --> 01:59:39,340
and then i suppose in distress some value could three

1778
01:59:39,380 --> 01:59:42,180
two minus five these values function

1779
01:59:42,190 --> 01:59:46,890
here so f zero zero zero equals three zero zero one loss two

1780
01:59:46,900 --> 01:59:49,790
now the term that corresponds to

1781
01:59:49,840 --> 01:59:51,900
rather right

1782
01:59:51,960 --> 01:59:56,710
if x i x one zero x two zero next three is zero

1783
01:59:56,720 --> 02:00:01,850
corresponds to the termites one by x by extreme meaning

1784
02:00:03,070 --> 02:00:09,330
your variables x one x two and x three to take the values if and

1785
02:00:09,330 --> 02:00:10,330
only if

1786
02:00:10,370 --> 02:00:11,960
that term

1787
02:00:11,990 --> 02:00:14,060
thanks value one

1788
02:00:14,070 --> 02:00:16,130
right so

1789
02:00:16,180 --> 02:00:17,240
x one

1790
02:00:17,250 --> 02:00:21,090
let's take this to the second one x one by x two x three

1791
02:00:21,090 --> 02:00:24,290
when can that be one

1792
02:00:24,380 --> 02:00:28,790
when have value once either zero or one right because each one

1793
02:00:28,840 --> 02:00:30,350
well this is going to be

1794
02:00:31,220 --> 02:00:32,760
then x one

1795
02:00:32,850 --> 02:00:35,740
must equal zero x one by one

1796
02:00:35,750 --> 02:00:39,110
x two nasty one

1797
02:00:39,230 --> 02:00:42,450
extreme st one right

1798
02:00:42,460 --> 02:00:44,240
it's the

1799
02:00:44,470 --> 02:00:47,970
that section four

1800
02:00:48,710 --> 02:00:52,750
the point is one of only one of those terms can be one

1801
02:00:52,810 --> 02:00:55,880
in a given time for any assignment of values

1802
02:00:55,930 --> 02:00:58,440
so once you've got that right now

1803
02:00:58,460 --> 02:00:59,560
policy form

1804
02:00:59,570 --> 02:01:00,900
for the function

1805
02:01:01,940 --> 02:01:06,590
three times once each three times the first time and for any value function

1806
02:01:06,630 --> 02:01:10,070
any value of x one x two x three that represents

1807
02:01:10,080 --> 02:01:11,720
the fourth column

1808
02:01:11,770 --> 02:01:14,590
so that's the tableau and also the

1809
02:01:14,820 --> 02:01:19,620
this is not neither positive for more polynomial how to get this policy form

1810
02:01:19,630 --> 02:01:24,090
well get was transferred to polynomial

1811
02:01:24,250 --> 02:01:28,590
so first order polynomial we just replace each x i

1812
02:01:28,630 --> 02:01:29,190
by bar

1813
02:01:29,200 --> 02:01:31,190
by one minus six i

1814
02:01:32,590 --> 02:01:38,550
multiplied out function like that think it just replacing x one by one minus x

1815
02:01:38,550 --> 02:01:42,670
one and you get a point which represents the function

1816
02:01:45,170 --> 02:01:47,550
closed form representations are ambiguous

1817
02:01:47,560 --> 02:01:51,600
the x one x two but can also be seen to be

1818
02:01:53,810 --> 02:01:56,290
this policy for

1819
02:01:56,300 --> 02:02:00,170
whereas polynomial representation have been

1820
02:02:00,170 --> 02:02:00,710
going forth

1821
02:02:06,270 --> 02:02:11,310
okay binary symmetric channel with inputs zero and one and it flips the fraction effort

1822
02:02:11,310 --> 02:02:14,850
the bits so we'll talk a lot about the challenges course but we want to

1823
02:02:14,850 --> 02:02:16,960
understand coding theory for any noisy channel

1824
02:02:18,940 --> 02:02:22,080
andy whenever dealing with noisy channels we need to know how to do inference and

1825
02:02:22,080 --> 02:02:25,830
i used to introduce people to inference for the different puzzle instead of three cards

1826
02:02:26,040 --> 02:02:27,520
i would show people the three doors

1827
02:02:29,170 --> 02:02:30,690
where the game show host says

1828
02:02:31,080 --> 02:02:32,480
here's the rules of the game

1829
02:02:32,690 --> 02:02:36,330
i'm gonna hide a very desirable prize here eight national rail pass

1830
02:02:37,630 --> 02:02:39,860
behind one of these three doors

1831
02:02:41,040 --> 02:02:44,450
the game is always explains the rules first before the game happens in the rule

1832
02:02:44,450 --> 02:02:46,900
the i will hide the prize behind these doors

1833
02:02:47,540 --> 02:02:50,770
then i will ask u the player to choose the door you have a free

1834
02:02:50,770 --> 02:02:53,830
choice and new choose it by naming it and we don't open it

1835
02:02:54,860 --> 02:03:00,420
then i became co-host guarantee that i will open another the doors not one shows

1836
02:03:00,670 --> 02:03:04,480
and i guarantee when i do that's promise me i promise you trust me

1837
02:03:05,040 --> 02:03:07,590
that the prize will not be revealed at stage

1838
02:03:08,880 --> 02:03:11,020
so then the prize is clearly the behind

1839
02:03:11,440 --> 02:03:12,900
i don't want to model to in this

1840
02:03:13,400 --> 02:03:18,400
the example shown here the one being when you chosen door to be other door that he didn't open

1841
02:03:18,860 --> 02:03:19,710
and then be

1842
02:03:20,500 --> 02:03:22,310
player gets a chance to stick or switch

1843
02:03:22,730 --> 02:03:23,290
if you want

1844
02:03:24,940 --> 02:03:30,210
and then you'll get what's behind the final or you end up at after eva sticking or switching

1845
02:03:31,150 --> 02:03:32,310
andy options

1846
02:03:32,920 --> 02:03:33,900
for this puzzle then

1847
02:03:34,940 --> 02:03:38,810
like this there was the rules being explained now know you go ahead and play the game

1848
02:03:39,790 --> 02:03:40,980
the player chooses the one

1849
02:03:41,400 --> 02:03:45,090
the host said in accordance with the rules are now open in the door to

1850
02:03:45,110 --> 02:03:47,630
model three and will not reveal price and i promise you

1851
02:03:48,590 --> 02:03:49,690
opens the door is still three

1852
02:03:50,540 --> 02:03:51,500
it doesn't reveal price

1853
02:03:52,850 --> 02:03:53,770
and the question is

1854
02:03:55,190 --> 02:03:55,830
should you state

1855
02:03:58,500 --> 02:03:59,210
city districts

1856
02:04:02,860 --> 02:04:04,040
others it made no difference

1857
02:04:11,060 --> 02:04:12,460
and i used to use this as my

1858
02:04:13,270 --> 02:04:17,480
introductory example in probability theory because people would argue very hotly about and say well

1859
02:04:17,830 --> 02:04:20,270
it's even behind aaron don't wanna do

1860
02:04:20,710 --> 02:04:21,630
it's fifty fifty

1861
02:04:23,420 --> 02:04:25,900
is indeed automated differences fifty fifty

1862
02:04:26,880 --> 02:04:30,210
and other people would say oh no you should switch and some people would say should stick

1863
02:04:32,670 --> 02:04:36,210
and it used to be contentious but unfortunately most people have heard it is possible

1864
02:04:36,920 --> 02:04:40,500
and so it doesn't really work anymore with let's have a quick vote votes for eight

1865
02:04:42,000 --> 02:04:43,170
beaches raised

1866
02:04:43,770 --> 02:04:47,860
let's say it makes no difference so you see is a useless educational device

1867
02:04:49,860 --> 02:04:52,690
it's been ruined because everyone's gonna talk about it

1868
02:04:53,190 --> 02:04:56,520
and the really annoying thing is they talked about it in a way such that

1869
02:04:56,520 --> 02:04:58,520
no one has actually learnt anything

1870
02:04:59,590 --> 02:05:01,920
and due my proof about because a moment ago

1871
02:05:02,480 --> 02:05:03,330
you've voted four

1872
02:05:04,110 --> 02:05:04,560
a lot of you

1873
02:05:05,000 --> 02:05:06,790
not all the proof that this lot

1874
02:05:07,520 --> 02:05:08,020
you this

1875
02:05:08,610 --> 02:05:10,210
controversy had between me and see

1876
02:05:13,960 --> 02:05:19,110
education disaster has happened the three doors puzzle which is a fantastic puzzle is really

1877
02:05:19,110 --> 02:05:23,400
educational has been ruined because everyone knows the answer but they don't understand it

1878
02:05:24,900 --> 02:05:30,290
maybe see here these are exactly equivalent to each other three cards and three doors

1879
02:05:30,290 --> 02:05:32,020
they are the same problem as each other

1880
02:05:33,670 --> 02:05:37,500
yet i showed you the same problem and you didn't get a right even though you allegedly

1881
02:05:37,940 --> 02:05:39,460
merchant the answer so

1882
02:05:40,250 --> 02:05:42,710
learning is about memorizing things it's about understanding

1883
02:05:43,250 --> 02:05:45,040
so that's my little rant on the three doors

1884
02:05:48,650 --> 02:05:51,920
the message people should be getting from the three does problem is don't memorize the

1885
02:05:51,920 --> 02:05:56,480
answer to a stupid puzzle because then you be useless so in future inference problems

1886
02:05:56,730 --> 02:05:59,730
instead learn the message that you should use probability theory

1887
02:06:00,130 --> 02:06:04,000
and then you will be equipped solve any puzzle of this type in the future

1888
02:06:05,790 --> 02:06:06,330
rant over

1889
02:06:07,130 --> 02:06:08,130
so what we do now

1890
02:06:08,560 --> 02:06:09,670
let's talk about

1891
02:06:10,270 --> 02:06:11,000
noisy channels

1892
02:06:13,310 --> 02:06:17,500
what we're going to do with noisy channels is there are always can have an input and output

1893
02:06:18,670 --> 02:06:19,710
andy channel

1894
02:06:20,730 --> 02:06:25,920
defines a set of conditional distributions if you condition on input the channel defines what

1895
02:06:25,920 --> 02:06:27,880
the probability of the output is gonna be

1896
02:06:28,400 --> 02:06:30,540
the channel doesn't define a joint distribution

1897
02:06:31,060 --> 02:06:33,360
it just defines a set of conditional

1898
02:06:35,790 --> 02:06:38,230
and we can only ask do inference if you've got a

