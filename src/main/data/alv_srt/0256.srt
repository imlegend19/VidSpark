1
00:00:00,000 --> 00:00:02,680
to allow users to find them but also

2
00:00:02,690 --> 00:00:07,330
you come up with alternate techniques to going to figure out that all these things

3
00:00:07,330 --> 00:00:09,700
refer to the same entity

4
00:00:09,720 --> 00:00:16,030
and then finally we have multiple meanings thank you so if you

5
00:00:16,040 --> 00:00:19,970
go and search for swimming and facebook and you say that you want to like

6
00:00:19,980 --> 00:00:21,550
swimming for instance

7
00:00:21,610 --> 00:00:23,770
we probably don't want

8
00:00:24,800 --> 00:00:26,760
link you to this

9
00:00:26,770 --> 00:00:28,000
the other guys

10
00:00:28,040 --> 00:00:30,750
film from the sun that film festival

11
00:00:30,770 --> 00:00:33,890
probably when you said the lights make you remember you

12
00:00:33,900 --> 00:00:38,990
so if we can to do all these things well with the bias

13
00:00:39,000 --> 00:00:40,610
and that is the

14
00:00:40,630 --> 00:00:43,980
you know by having this nice

15
00:00:44,240 --> 00:00:49,080
picture you know how all these things are related how people related to them

16
00:00:49,140 --> 00:00:54,420
it just makes everything sort of more complete it means that

17
00:00:54,430 --> 00:01:00,410
not only are you link to the right thing when you go and find activities

18
00:01:01,560 --> 00:01:06,420
now it is the quality of experience side because

19
00:01:06,430 --> 00:01:09,680
and also on the other side because

20
00:01:10,170 --> 00:01:11,570
you know

21
00:01:11,580 --> 00:01:21,030
so what's going on

22
00:01:29,240 --> 00:01:32,330
one of the things that you find all these things it is the

23
00:01:32,610 --> 00:01:36,910
if you have the that from different sites right so in the database for instance

24
00:01:36,910 --> 00:01:41,910
in the information is going to give you a somewhat different plants the same thing

25
00:01:41,910 --> 00:01:46,270
and now by together you end up with the union of all the information from

26
00:01:46,270 --> 00:01:47,790
all the different

27
00:01:47,850 --> 00:01:53,610
but it's important to maintain high precision that over merge and

28
00:01:53,620 --> 00:01:57,250
i mean the same thing mean

29
00:02:00,430 --> 00:02:04,620
so i but what we want to do this was

30
00:02:04,680 --> 00:02:06,120
based on one

31
00:02:06,160 --> 00:02:13,460
we don't number studies where we look at the importance of social context and you

32
00:02:13,460 --> 00:02:20,730
know what happens when you put these spaces next things for instance and we found

33
00:02:20,730 --> 00:02:25,240
is pretty significant increases in terms of these are the overall performance of all these

34
00:02:25,240 --> 00:02:26,760
things that are

35
00:02:26,790 --> 00:02:28,770
so if you can

36
00:02:28,790 --> 00:02:31,770
when people to these things and then you're going to play with

37
00:02:31,770 --> 00:02:33,400
it's appropriate pages

38
00:02:33,400 --> 00:02:38,220
and you get this two-and-a-half times increase in social activities so people are much more

39
00:02:38,650 --> 00:02:42,980
likely to look like but if the faces of their friends and st

40
00:02:44,520 --> 00:02:52,640
and then also the more is because the business so if you can put faces

41
00:02:52,640 --> 00:02:57,900
necks things like and provide some context for advertisment i actually make people much more

42
00:02:57,900 --> 00:03:01,280
likely to but interact the ad and also the

43
00:03:01,530 --> 00:03:06,600
brand recognition remember what they seen because it's associated with this and there is no

44
00:03:06,600 --> 00:03:09,170
one like

45
00:03:14,420 --> 00:03:19,420
so if you take away from the first there and so there a huge amount

46
00:03:19,430 --> 00:03:25,290
of video it's it's not just offensive but this huge that lots of different types

47
00:03:25,300 --> 00:03:32,440
of things have pages in places and people so and so forth and and we'd

48
00:03:32,600 --> 00:03:36,620
like to have more and more structured data just because it's easier to work with

49
00:03:36,620 --> 00:03:38,990
in the context and things like that

50
00:03:39,000 --> 00:03:43,950
but according the very high quality really want to make sure that there are lots

51
00:03:43,950 --> 00:03:46,980
and lots of different entities the same thing

52
00:03:47,030 --> 00:03:52,480
and we want to build the tools that enable users to merge things together in

53
00:03:52,480 --> 00:03:55,170
the same thing

54
00:03:55,180 --> 00:03:58,770
it so that from the fact that

55
00:03:58,790 --> 00:04:03,240
so something we can do that was once they accomplished the first part and gather

56
00:04:03,240 --> 00:04:07,520
all the structured data in sort of a high precision and high quality

57
00:04:13,020 --> 00:04:15,230
so the first thing is actually

58
00:04:15,250 --> 00:04:18,450
and unstructured view of test

59
00:04:18,500 --> 00:04:23,800
and to this what we did was this is

60
00:04:23,860 --> 00:04:29,000
relatively simple automated techniques looking at updates from all of the hundreds of millions of

61
00:04:29,000 --> 00:04:29,970
of users

62
00:04:29,970 --> 00:04:34,390
the findings it is interesting to note that there as an interesting

63
00:04:34,410 --> 00:04:38,860
and we want to be extracted was

64
00:04:38,920 --> 00:04:41,740
and if you go back to two thousand eight two thousand nine

65
00:04:41,750 --> 00:04:45,430
there this big for swine flu scare right

66
00:04:45,440 --> 00:04:47,330
so you can see

67
00:04:47,350 --> 00:04:52,440
the the red line here is just occurrences of the word well the living lab

68
00:04:52,440 --> 00:04:55,670
and h one n one swine flu

69
00:04:56,700 --> 00:05:03,580
going back to NACD sort of follows the seasonal trend that state and you get

70
00:05:03,580 --> 00:05:07,770
this huge spike which is one the swine flu scare off

71
00:05:07,790 --> 00:05:13,130
and then to die down some people so i forgot about until the fall and

72
00:05:13,180 --> 00:05:16,470
it is actually an old people started to get this one

73
00:05:17,140 --> 00:05:18,530
so this

74
00:05:19,030 --> 00:05:25,410
we know it's all the same attractor here but this is very rapid oscillations

75
00:05:25,410 --> 00:05:31,260
first all the people of swine flu and so we will not interesting

76
00:05:31,270 --> 00:05:32,860
let's zoom in on the

77
00:05:32,880 --> 00:05:35,050
so averaged over many weeks

78
00:05:35,090 --> 00:05:39,920
and we don't want to release of the the y axis here is the fraction

79
00:05:40,920 --> 00:05:43,300
think that contained the word balloons

80
00:05:45,740 --> 00:05:47,310
so in the same way

81
00:05:47,340 --> 00:05:49,740
people tend to get the on wednesdays

82
00:05:50,920 --> 00:05:54,070
they tend to think they have answer is

83
00:05:54,090 --> 00:05:56,560
in that last weekend

84
00:06:02,090 --> 00:06:03,390
another big thing

85
00:06:03,480 --> 00:06:06,810
came out of this text analysis

86
00:06:06,830 --> 00:06:09,570
and was this mean for now

87
00:06:09,590 --> 00:06:11,520
and the families

88
00:06:11,690 --> 00:06:16,870
let's say people will go and post about the bad things happen to them

89
00:06:16,890 --> 00:06:20,490
and you can see where this website that was

90
00:06:20,490 --> 00:06:22,530
around ten thousand i

91
00:06:22,670 --> 00:06:24,690
but first no became

92
00:06:24,730 --> 00:06:28,980
it wasn't just find website really was what

93
00:06:29,000 --> 00:06:32,860
and again you see these solutions

94
00:06:32,930 --> 00:06:39,840
when you go into everything that kind of money that and then the other you

95
00:06:39,910 --> 00:06:44,290
know it it's about a third from the celebrity

96
00:06:45,000 --> 00:06:51,780
there's a few things that know how structure this is all just like me unigrams

97
00:06:52,980 --> 00:06:58,000
and now if you think about it was somebody started

98
00:06:59,560 --> 00:07:01,030
most users

99
00:07:01,030 --> 00:07:03,950
are in some way associated with some countries

100
00:07:03,950 --> 00:07:13,180
so it's not just the faceted of

101
00:07:13,370 --> 00:07:15,720
we know approximately

102
00:07:15,810 --> 00:07:20,530
because of course they have to be an expert in a we know where they

103
00:07:20,530 --> 00:07:21,420
come from

104
00:07:21,460 --> 00:07:23,770
so we only get in his way

105
00:07:23,890 --> 00:07:27,230
so how are people talking about something in different countries

106
00:07:27,270 --> 00:07:32,940
and this is an example from the recent japanese earthquake where you can see you

107
00:07:32,940 --> 00:07:37,460
can see what happens is this big spike mentions of the earthquake and the green

108
00:07:37,460 --> 00:07:43,440
outputs you might imagine that at test time i might just accidentally by sampling sample

109
00:07:43,440 --> 00:07:46,100
the same case as the training data

110
00:07:46,100 --> 00:07:50,020
so am i allowed to include that in the test set or not when i

111
00:07:50,020 --> 00:07:52,100
evaluate generalization

112
00:07:52,120 --> 00:07:58,500
that's the subtle distinction is generalisation strictly training data or is it just from future

113
00:07:58,500 --> 00:08:01,620
data drawn i i d from the same distribution

114
00:08:01,640 --> 00:08:06,190
OK i really think the right way to think about generalisation is data that is

115
00:08:06,190 --> 00:08:11,620
not your training data explicitly not your training data because then anything you can do

116
00:08:11,620 --> 00:08:17,270
better than chance on generalisation really means that you've learned something otherwise you might just

117
00:08:17,270 --> 00:08:19,210
if you see for example

118
00:08:19,290 --> 00:08:23,520
one of the possible instances in your training data then you could get good tested

119
00:08:23,520 --> 00:08:25,600
even without knowing anything

120
00:08:28,980 --> 00:08:36,710
these are just some some last comments here to formalise these these ideas so capacity

121
00:08:36,710 --> 00:08:43,940
the the term capacity refers to the complexity of the hypothesis space OK so

122
00:08:43,960 --> 00:08:47,440
if you're hypothesis space contains many many functions

123
00:08:47,460 --> 00:08:52,460
then in your machine or the hypothesis space is said to have a large capacity

124
00:08:52,640 --> 00:08:57,690
and learning is really going to be nothing more than a search in hypothesis space

125
00:08:57,690 --> 00:09:00,830
within the second hypothesis space for

126
00:09:00,850 --> 00:09:04,900
members of the function classes that do well on the training data

127
00:09:04,920 --> 00:09:08,830
and this will will see the second have some other properties like they look not

128
00:09:08,850 --> 00:09:10,120
too complicated

129
00:09:10,170 --> 00:09:13,980
any inductive learning hypothesis here

130
00:09:14,080 --> 00:09:18,060
is a generalisation is possible

131
00:09:19,100 --> 00:09:23,040
that sounds like a pithy statement but it's a very deep statement

132
00:09:23,040 --> 00:09:30,250
the inductive learning hypothesis says it is actually possible to learn something about future data

133
00:09:30,250 --> 00:09:32,560
that you haven't seen before

134
00:09:32,580 --> 00:09:36,980
this is the solution to this paradox and the solution is pick very restricted function

135
00:09:36,980 --> 00:09:42,170
class that is to say a hypothesis class with low capacity and apply it to

136
00:09:42,170 --> 00:09:43,350
the training data

137
00:09:43,370 --> 00:09:46,690
notice that you do well on the training data and then you you have some

138
00:09:46,690 --> 00:09:49,830
reason to believe you're going to do well on the test

139
00:09:50,580 --> 00:09:54,160
you can actually prove in very strong

140
00:09:54,160 --> 00:09:55,640
mathematical sense

141
00:09:55,960 --> 00:10:01,020
that there are limits on how badly you can do on the test error if

142
00:10:01,770 --> 00:10:07,250
the capacity of the hypothesis space is small and you have low training so

143
00:10:07,250 --> 00:10:11,940
again this is the sort of the whole field of learning theory and you're very

144
00:10:11,940 --> 00:10:13,440
fortunate to be

145
00:10:13,640 --> 00:10:17,790
one of the hotbeds of learning theory in the world so i won't even pretend

146
00:10:17,790 --> 00:10:21,790
that i know anything about this area except to say that it's always been a

147
00:10:21,790 --> 00:10:27,870
little bit beyond me and kind of mysterious but it has these very powerful results

148
00:10:27,870 --> 00:10:29,080
which are the

149
00:10:29,080 --> 00:10:34,140
there's a sort of backdrop of the probabilistic approach machine learning and one

150
00:10:34,170 --> 00:10:37,620
i want to say i want to defend learning theory a little bit here people

151
00:10:37,620 --> 00:10:43,290
often disingenuously criticized learning theory by saying well but the band that people come up

152
00:10:43,290 --> 00:10:47,580
with in learning theory on the test error those bands is so loose that kind

153
00:10:47,580 --> 00:10:53,080
of useless so what contribution has learning theory really really made

154
00:10:53,100 --> 00:10:59,770
OK to criticisms people make of learning and the answer is that learning theory showed

155
00:11:00,690 --> 00:11:04,520
one particular properties of the data

156
00:11:04,620 --> 00:11:10,040
of the function class and of the search algorithm that you use on all three

157
00:11:10,040 --> 00:11:14,040
of those learning theory has something to say about how those affect the future test

158
00:11:14,670 --> 00:11:18,960
so even if numerically the bounds that you get out of learning theory might not

159
00:11:18,960 --> 00:11:24,660
be very powerful to the general properties that it elucidated about the structure of the

160
00:11:24,660 --> 00:11:28,370
learning problem have been invaluable and it's really made a big difference in

161
00:11:28,770 --> 00:11:34,270
in the practice of machine learning so even though you might make a strong argument

162
00:11:34,270 --> 00:11:37,850
that the bound themselves in a very tight i think that the contribution of learning

163
00:11:37,850 --> 00:11:42,540
theory has been very very important in particular answers this question how could we ever

164
00:11:42,540 --> 00:11:45,770
do machine learning it s is that in very formal

165
00:11:47,730 --> 00:11:49,230
just a small

166
00:11:49,250 --> 00:11:52,330
defensive learning theory OK

167
00:11:52,330 --> 00:11:57,640
so the converse of the inductive learning hypothesis is that if you don't make assumptions

168
00:11:57,920 --> 00:12:00,330
you can never learn anything

169
00:12:00,350 --> 00:12:03,210
OK so remember this example

170
00:12:03,230 --> 00:12:07,670
of course you guys are all assuming because you have a million visual cortex that's

171
00:12:07,980 --> 00:12:11,960
very good at keeping you alive that these functions are smooth because you like to

172
00:12:11,960 --> 00:12:15,690
connect things with smooth surfaces but

173
00:12:16,850 --> 00:12:20,870
if you make any assumptions there's no one could predict this function

174
00:12:22,020 --> 00:12:26,390
there's no way you could predict the value of this point in particular what if

175
00:12:26,390 --> 00:12:27,520
you just

176
00:12:27,540 --> 00:12:30,250
so given in training cases

177
00:12:30,310 --> 00:12:32,750
and you for a polynomial

178
00:12:32,750 --> 00:12:35,100
of order n minus one

179
00:12:35,120 --> 00:12:38,250
to the training data

180
00:12:38,270 --> 00:12:42,100
what would your training error be if we measure air by how far your prediction

181
00:12:42,100 --> 00:12:46,460
is from the correct the training would be zero because the polynomial and degree and

182
00:12:46,460 --> 00:12:52,000
minus one can always go exactly through any endpoints by your polynomials gonna look crazy

183
00:12:52,000 --> 00:12:56,180
OK but i didn't do they have the same time as a

184
00:12:56,230 --> 00:12:59,970
black communities of doing this is my questions back

185
00:13:00,290 --> 00:13:04,140
good morning

186
00:13:06,050 --> 00:13:08,690
they all try to describe

187
00:13:08,740 --> 00:13:10,620
two u er

188
00:13:10,620 --> 00:13:11,960
some of the

189
00:13:11,960 --> 00:13:17,570
the main advances in this subject over the last ten years which have a lot

190
00:13:17,570 --> 00:13:21,690
to do with black holes show this will take us first a little bit

191
00:13:21,690 --> 00:13:24,820
a far away from particle physics

192
00:13:24,840 --> 00:13:29,960
but then it will come back in a very interesting way to QCD and particle

193
00:13:29,960 --> 00:13:33,030
physics as using end

194
00:13:34,970 --> 00:13:43,090
let's start slowly with black holes black holes are fascinating astrophysical objects there is for

195
00:13:43,090 --> 00:13:48,310
instance most likely one in the centre of our own galaxy that people are actively

196
00:13:49,060 --> 00:13:51,070
trying to look at

197
00:13:51,130 --> 00:13:55,270
a it has a mass which is equal to a few million times

198
00:13:55,330 --> 00:13:56,880
the short mass

199
00:13:56,940 --> 00:13:58,890
if you convert these two

200
00:13:58,910 --> 00:14:04,910
frank units like mass units usually it's a huge number ten to the forty four

201
00:14:04,960 --> 00:14:08,440
implant which is the scale of quantum gravity

202
00:14:09,440 --> 00:14:15,890
and they have also ABI which are correspondingly large ten billion kilometres roughly for this

203
00:14:17,280 --> 00:14:23,890
these are called supermassive black holes they are the largest such objects in the sky

204
00:14:23,890 --> 00:14:24,750
and there

205
00:14:24,800 --> 00:14:29,500
essentially as classical an object as it can get actually

206
00:14:29,550 --> 00:14:34,720
it's hard to get something that's more classical such black holes

207
00:14:35,630 --> 00:14:41,020
when one thinks about such black holes there is one shot of defining feature in

208
00:14:41,020 --> 00:14:45,440
classical general relativity for what the black hole is

209
00:14:45,470 --> 00:14:49,160
this is the fact that it has an event horizon

210
00:14:49,190 --> 00:14:51,270
an event horizon is

211
00:14:51,270 --> 00:14:52,910
a very simple

212
00:14:54,320 --> 00:14:56,050
it is simply

213
00:14:56,100 --> 00:14:59,720
the horizon so surface surrounding this

214
00:14:59,730 --> 00:15:05,470
very massive black hole that resulted from some gravitational collapse of some

215
00:15:05,510 --> 00:15:07,160
super heavy body

216
00:15:07,160 --> 00:15:12,820
and this surface first because the property that even light cannot escape show the gravitational

217
00:15:12,820 --> 00:15:14,940
attraction is so huge

218
00:15:14,950 --> 00:15:16,380
that even lie

219
00:15:16,380 --> 00:15:19,410
turns around as it tries to escape the black hole

220
00:15:19,530 --> 00:15:21,060
falls back in

221
00:15:21,130 --> 00:15:26,280
therefore behind this horizon you just don't see anything there is no signal that can

222
00:15:26,280 --> 00:15:27,500
come out out

223
00:15:27,570 --> 00:15:29,870
we from behind the horizon

224
00:15:29,910 --> 00:15:35,600
the region behind the horizon is see them to the outside observers

225
00:15:35,660 --> 00:15:37,130
this is a very nice

226
00:15:37,190 --> 00:15:40,900
a literary analogy which has been

227
00:15:40,940 --> 00:15:47,060
most distressed by lenny susskind that find it intuitively very appealing because it really

228
00:15:47,100 --> 00:15:50,940
it gives you the correct mindset when you think about the horrors of the black

229
00:15:52,130 --> 00:15:58,380
and this is basically the analogy with very very actually such things that even constructed

230
00:15:58,380 --> 00:16:01,410
in the lab they are called acoustic black holes

231
00:16:01,430 --> 00:16:06,940
basically the analogies the following think of somebody here it's there are in except they

232
00:16:06,970 --> 00:16:12,290
have taken in time reversed order because i'm going from our

233
00:16:12,310 --> 00:16:16,970
chef houses here before usually are

234
00:16:17,370 --> 00:16:22,940
it is out of the false really going downstream another upstream but suppose for a

235
00:16:22,940 --> 00:16:27,600
moment that it was the other way around you are moving in from some asymptotic

236
00:16:27,600 --> 00:16:30,040
region where the river doesn't flow much

237
00:16:30,530 --> 00:16:35,950
so the velocity field uses very very slow and you can narrow upstream or downstream

238
00:16:35,950 --> 00:16:38,070
without any difficulty

239
00:16:38,120 --> 00:16:43,440
and then as you approach the singularity which is waterfall

240
00:16:43,480 --> 00:16:47,720
the river gets faster and faster and at some point it so fast

241
00:16:47,760 --> 00:16:50,590
that's actually the velocity field the flow

242
00:16:50,620 --> 00:16:53,730
of very very is fast

243
00:16:53,780 --> 00:16:58,600
then the olympic champion of borrowing say what's in our case of black horses of

244
00:16:59,500 --> 00:17:00,820
the speed of light

245
00:17:00,840 --> 00:17:05,570
now in this case there is absolutely no way you can upstream

246
00:17:05,840 --> 00:17:11,840
necessarily going downstream you're therefore don't download you can get off the shore here but

247
00:17:11,840 --> 00:17:16,190
in the black hole there is no shortage so you definitely don't and you willfully

248
00:17:16,190 --> 00:17:21,830
and yet you don't really feel very much while you are approaching this wisely because

249
00:17:21,830 --> 00:17:27,570
after all you are nicely along and who who knows what will happen next and

250
00:17:27,570 --> 00:17:33,480
as a lens suskind was staying in the california league a new passing before i

251
00:17:33,650 --> 00:17:36,250
seems very ancient while it is happening

252
00:17:36,270 --> 00:17:41,800
it's like being in a rowboat above the niagara falls if you accidentally pass the

253
00:17:41,810 --> 00:17:45,790
point where the current moves faster than you can row row you don't

254
00:17:45,800 --> 00:17:47,370
but there is no sign

255
00:17:47,380 --> 00:17:50,000
danger point of knowledge

256
00:17:51,190 --> 00:17:55,930
maybe on very very rare signs but not on the horizon of a black hole

257
00:17:55,940 --> 00:17:58,320
so this those you know things

258
00:17:58,330 --> 00:18:00,810
this horrifies rose are in a sense

259
00:18:00,860 --> 00:18:05,980
a place where nothing very dramatic happens as long as you are going along and

260
00:18:05,980 --> 00:18:11,090
falling into the black hole you don't feel much on the other hand from far

261
00:18:11,090 --> 00:18:17,990
seen from the asymptotic regions there are definitely very dramatic surfaces because beyond the horizon

262
00:18:18,510 --> 00:18:22,380
there is not a problem that will ever come back and tell you what happens

263
00:18:22,380 --> 00:18:25,270
which cannot be modelled by any theta

264
00:18:25,310 --> 00:18:27,100
then the posterior

265
00:18:28,630 --> 00:18:31,760
given the data will converge to a delta function

266
00:18:31,810 --> 00:18:34,800
around some value data had

267
00:18:34,810 --> 00:18:38,940
where data is the value of the data that minimizes the

268
00:18:38,950 --> 00:18:43,350
kl divergence to the true probability distribution p

269
00:18:43,490 --> 00:18:48,580
so you converge to the closest possible points in data space

270
00:18:49,450 --> 00:18:52,160
two that he stuck

271
00:18:53,270 --> 00:18:57,980
and the data had that minimizes

272
00:18:57,990 --> 00:19:01,720
the KL divergence the star is also equivalently

273
00:19:01,770 --> 00:19:06,500
the data had that has maximum likelihood

274
00:19:06,510 --> 00:19:08,920
under the generative model

275
00:19:09,040 --> 00:19:12,510
you're basically cannot converge to the maximum likelihood estimate

276
00:19:12,630 --> 00:19:16,550
data that this is the log likelihood averages over the

277
00:19:16,600 --> 00:19:21,810
the district the true distribution p of x

278
00:19:25,080 --> 00:19:32,350
how different can beliefs be between different bases so one thing that seems a little

279
00:19:32,610 --> 00:19:37,220
worrying about the basic framework is that subjective you have different bayesians coming with different

280
00:19:40,170 --> 00:19:43,600
what effect does that have well there's another result

281
00:19:44,050 --> 00:19:49,300
called asymptotic and then the results of everything i mean states for finite dimensional data

282
00:19:51,410 --> 00:19:58,530
so consider two two bayesians with different priors p one theta and p two theta

283
00:19:58,540 --> 00:20:01,050
who observe the same data d

284
00:20:01,100 --> 00:20:02,490
so for example

285
00:20:03,560 --> 00:20:08,740
data could be a number between zero and one representing the probability of a coin

286
00:20:09,660 --> 00:20:12,220
coming out heads or tails

287
00:20:12,230 --> 00:20:16,310
and imagine you have two very different based in one of them

288
00:20:17,450 --> 00:20:19,470
his math on

289
00:20:19,520 --> 00:20:23,510
the extremes near zero and one and the other one

290
00:20:24,590 --> 00:20:27,640
almost all of that on

291
00:20:27,690 --> 00:20:29,230
values around point four

292
00:20:32,090 --> 00:20:37,600
here's the key assumption assume both bayesians agree on the set of possible and impossible

293
00:20:37,600 --> 00:20:39,300
values of data

294
00:20:39,350 --> 00:20:44,200
which means that even though this one put are almost all that in this one

295
00:20:44,200 --> 00:20:46,630
but almost all the math here and here

296
00:20:46,680 --> 00:20:50,290
the value for which

297
00:20:50,720 --> 00:20:55,080
he wanted data is strictly greater than zero and p two of data is strictly

298
00:20:55,080 --> 00:20:58,590
greater than zero are equal to each other for both of the

299
00:21:01,660 --> 00:21:05,300
the domain is the interval between zero and one

300
00:21:05,470 --> 00:21:08,180
then in the limit

301
00:21:08,870 --> 00:21:11,360
the number of points going to infinity

302
00:21:11,380 --> 00:21:18,810
the posterior distribution for the first made the and the second basically will converge

303
00:21:18,870 --> 00:21:20,820
in this matter of

304
00:21:21,050 --> 00:21:25,640
uniform distance between distibutions thank you start out with

305
00:21:25,700 --> 00:21:31,230
different beliefs as long as you think the set of possibilities

306
00:21:31,270 --> 00:21:32,580
is the same

307
00:21:32,590 --> 00:21:36,120
you will eventually converge to the same beliefs

308
00:21:36,130 --> 00:21:37,640
having observed there

309
00:21:43,000 --> 00:21:45,460
so another thing that

310
00:21:45,470 --> 00:21:49,850
people worry about a lot of where you choose your prior prob

311
00:21:49,900 --> 00:21:53,680
and this is something that there are lots of different schools of thought

312
00:21:54,270 --> 00:22:00,350
all kind of paraphrase different principles our ideas you can use which is your right

313
00:22:01,770 --> 00:22:05,600
objective bayesians try to come up with objective priors

314
00:22:05,610 --> 00:22:09,230
so in general that corresponds to non informative priors

315
00:22:09,280 --> 00:22:12,900
that attempt to capture ignorance of some kind

316
00:22:12,910 --> 00:22:16,340
and then they try to have good frequentist properties

317
00:22:16,390 --> 00:22:21,380
and there's a lot of work on trying to develop objective priors in the very

318
00:22:22,710 --> 00:22:23,840
o thing to do

319
00:22:24,120 --> 00:22:26,550
some people might argue is impossible to do

320
00:22:26,930 --> 00:22:30,890
so maybe if you can afford

321
00:22:30,940 --> 00:22:31,890
you know

322
00:22:31,930 --> 00:22:35,230
some problems that objective higher so

323
00:22:35,240 --> 00:22:39,380
one of the issues is that you try to come up with this not informative

324
00:22:40,460 --> 00:22:41,870
but sometimes

325
00:22:41,880 --> 00:22:47,920
that leads to improper priors so for example non informative prior on

326
00:22:48,400 --> 00:22:49,600
the mean

327
00:22:49,610 --> 00:22:56,110
the gaussians we might end up being a uniform distribution from minus infinity to infinity

328
00:22:56,160 --> 00:22:59,900
which is not a proper distribution it is not something you can

329
00:22:59,910 --> 00:23:01,490
sample from or

330
00:23:01,850 --> 00:23:06,510
you know normalized or anything like that that's called an improper prior to its use

331
00:23:06,510 --> 00:23:11,220
improper priors then you get inconsistencies you get incoherent

332
00:23:11,260 --> 00:23:14,260
and incoherencies with these to be exposed to

333
00:23:14,280 --> 00:23:18,650
the dutch book and stuff like that so it's dangerous business

334
00:23:18,700 --> 00:23:21,010
then there is

335
00:23:21,060 --> 00:23:26,040
priors of convenience school they've seen me do this a bit in the last electrode

336
00:23:26,150 --> 00:23:27,500
the some priors

337
00:23:27,550 --> 00:23:29,650
like for example conjugate priors

338
00:23:29,660 --> 00:23:35,220
lend themselves to analytical solutions and computationally efficient inference

339
00:23:36,050 --> 00:23:43,230
you know we're all practical people will will use these practical considerations often choose prior

340
00:23:43,270 --> 00:23:48,390
with the hope that we've managed to capture the range of possible parameter values that

341
00:23:48,400 --> 00:23:54,390
we believe the actual parameters belong to an remember by the previous arguments of asymptotic

342
00:23:55,400 --> 00:23:58,160
and consensus we don't have to worry

343
00:23:58,170 --> 00:24:03,520
about having exactly the right prior to come to that in a minute there is

344
00:24:03,550 --> 00:24:07,670
sort of exactly the right prior we just want to try capture our beliefs and

345
00:24:07,670 --> 00:24:11,500
then be somewhat practical and how we deal with things as well

346
00:24:11,520 --> 00:24:14,240
so that's the friday prior to to being in

347
00:24:14,250 --> 00:24:15,950
hierarchical priors

348
00:24:15,960 --> 00:24:20,780
are the concept of well i have the prior on this parameter data let me

349
00:24:20,790 --> 00:24:23,960
defined that by having hyperparameters

350
00:24:23,970 --> 00:24:29,570
and OK those defined the distribution over data but then what distribution which i put

351
00:24:29,570 --> 00:24:33,670
on the hyperparameters and you go one layer of the user can be defined some

352
00:24:33,670 --> 00:24:37,900
hyper hyperparameters and so on and so forth and so

353
00:24:37,960 --> 00:24:42,120
hierarchical way of defining models actually often quite sensible

354
00:24:42,550 --> 00:24:46,110
at some point you want to stop the hierarchy because it doesn't make any more

355
00:24:46,110 --> 00:24:49,410
and they also don't get very good local optima and had deep that's

356
00:24:49,420 --> 00:24:53,510
this will see later you can get much better optima

357
00:24:53,590 --> 00:24:57,010
so the idea is we want to keep what's good about back propagation which is

358
00:24:57,010 --> 00:25:00,140
stochastic gradient descent

359
00:25:01,420 --> 00:25:05,270
we want to get rid of what's bad which is everything depending on the labels

360
00:25:05,350 --> 00:25:08,950
so instead of learning the probability of a label given image

361
00:25:09,000 --> 00:25:12,830
we just can the probability of an image we can learn density model images

362
00:25:12,950 --> 00:25:17,730
and now we don't need labels and what's more each image has much more information

363
00:25:17,730 --> 00:25:18,750
in it

364
00:25:18,770 --> 00:25:20,700
the typical label

365
00:25:21,540 --> 00:25:22,570
can be big

366
00:25:22,590 --> 00:25:26,000
and lots of pixels and so each image was a lot of constraint on a

367
00:25:26,000 --> 00:25:27,160
density function

368
00:25:27,260 --> 00:25:29,700
was if i give you an image in the label

369
00:25:29,710 --> 00:25:32,940
i might try and get the right answer i don't get much constraint on the

370
00:25:32,940 --> 00:25:34,750
mapping from images to labels

371
00:25:34,800 --> 00:25:38,190
the bits constraint on the mapping imposed by training example just number bits it takes

372
00:25:38,190 --> 00:25:41,270
to say what the answer is which is very many

373
00:25:41,280 --> 00:25:46,410
so now the question is what kind of generative model should we learn

374
00:25:48,050 --> 00:25:51,840
as you all know i think has been a big revolution in statistics and a

375
00:25:53,850 --> 00:25:57,590
and there's a kind of model which is directed basically graph

376
00:25:57,640 --> 00:25:59,760
and there's been lots of work on how you

377
00:25:59,770 --> 00:26:01,890
learn these

378
00:26:01,940 --> 00:26:03,900
and how to do inference in these

379
00:26:03,910 --> 00:26:04,700
is there

380
00:26:04,710 --> 00:26:08,190
sparsely connected each node only has a few parents

381
00:26:08,200 --> 00:26:13,670
this clever exact inference algorithms division has many parents imprint gets much trickier

382
00:26:13,680 --> 00:26:14,670
so in these

383
00:26:14,690 --> 00:26:16,320
i'll be using

384
00:26:16,330 --> 00:26:18,160
directed nets in which

385
00:26:18,170 --> 00:26:20,020
the observations of the leaves

386
00:26:20,030 --> 00:26:21,270
one of the

387
00:26:22,090 --> 00:26:24,260
and in that kind of net

388
00:26:24,310 --> 00:26:27,710
then the inference problem is if i you know the the parameters that you know

389
00:26:28,440 --> 00:26:32,370
the states of these nodes determine the distribution for this note

390
00:26:32,380 --> 00:26:36,520
the inference problem is figuring out what states of the there's probably were when you

391
00:26:36,520 --> 00:26:37,560
see the data

392
00:26:37,610 --> 00:26:39,460
the main problem is figuring out

393
00:26:39,480 --> 00:26:44,250
weights on these connections to give you some ground right function for

394
00:26:44,270 --> 00:26:48,800
the probability distribution here given states of these guys

395
00:26:48,810 --> 00:26:52,530
typical values binary units we can generalize and later

396
00:26:52,540 --> 00:26:55,380
so the initial user just stochastic binary units where

397
00:26:55,390 --> 00:26:57,470
you get some total input

398
00:26:57,980 --> 00:26:59,700
from other units

399
00:26:59,710 --> 00:27:01,100
so when you're doing

400
00:27:01,120 --> 00:27:04,170
generation this will be top down input

401
00:27:05,310 --> 00:27:13,650
the probability of turning into unit on is just the logistic function this template

402
00:27:13,660 --> 00:27:15,960
so now if all only units like that

403
00:27:15,980 --> 00:27:19,680
so rapid neil introduced nets like this in about nineteen ninety two

404
00:27:19,700 --> 00:27:22,000
william is like that

405
00:27:22,050 --> 00:27:23,910
it turns out is easy

406
00:27:23,920 --> 00:27:29,110
to learn the parameters if you can compute the posterior distribution over the hidden causes

407
00:27:29,120 --> 00:27:32,300
a data vector notation

408
00:27:32,350 --> 00:27:35,860
if you can compute the full posterior here or even if you can just get

409
00:27:35,860 --> 00:27:39,640
an unbiased sample from the posterior learning is easy

410
00:27:39,650 --> 00:27:41,110
so i show you data vector

411
00:27:41,130 --> 00:27:42,770
if you can give me back

412
00:27:42,780 --> 00:27:45,590
an unbiased sample some binary vector

413
00:27:45,610 --> 00:27:49,960
this is sort of plausible way in which this stuff might have cause that stuff

414
00:27:49,970 --> 00:27:52,400
then learning is very easy

415
00:27:52,450 --> 00:27:54,050
the difficult thing is getting this

416
00:27:55,480 --> 00:27:59,790
sample from the posterior

417
00:27:59,840 --> 00:28:03,750
so here's the learning rule suppose we have an unbiased sample from the posterior

418
00:28:03,800 --> 00:28:05,900
so i gave me some data down here

419
00:28:05,980 --> 00:28:10,130
my unbiased sample from seriously binary states for these units

420
00:28:10,180 --> 00:28:12,350
and the learning rule is just

421
00:28:12,410 --> 00:28:18,270
for the weight on this connection what i need to do is compute the difference

422
00:28:19,360 --> 00:28:23,130
the actual state of this guy kind by sampling from posterior

423
00:28:23,140 --> 00:28:25,690
this is just like this one or zero

424
00:28:25,700 --> 00:28:30,930
and the probability with which this guy will be turned on by the generative model

425
00:28:30,950 --> 00:28:33,750
given the actual states of these guys in the posterior

426
00:28:33,800 --> 00:28:35,210
that's all these

427
00:28:35,830 --> 00:28:37,880
s j is here

428
00:28:37,890 --> 00:28:40,270
so the probability to on i

429
00:28:40,280 --> 00:28:43,850
it's just the logistics of the input it gets from all these actual structure

430
00:28:43,860 --> 00:28:47,330
and it turns out the maximum likelihood learning will is two

431
00:28:47,350 --> 00:28:48,760
to follow the

432
00:28:48,770 --> 00:28:51,530
direction in which shows increasing the problem the

433
00:28:51,550 --> 00:28:53,010
observed data thank you

434
00:28:53,030 --> 00:28:56,160
is to change the weight in proportion to the learning rate times

435
00:28:56,200 --> 00:29:00,170
the state of this guy times the difference between the stated that guy what you

436
00:29:00,170 --> 00:29:02,050
and who would like to have some more time

437
00:29:05,210 --> 00:29:05,710
let's get on

438
00:29:06,390 --> 00:29:07,170
okay so is there

439
00:29:08,560 --> 00:29:09,340
is anyone who

440
00:29:10,300 --> 00:29:11,770
it's relatively confidence

441
00:29:12,200 --> 00:29:13,590
that he or she has found a solution

442
00:29:15,010 --> 00:29:15,660
the other

443
00:29:21,920 --> 00:29:22,400
right right

444
00:29:23,190 --> 00:29:23,660
so u

445
00:29:24,350 --> 00:29:25,780
these are you apply the definition

446
00:29:28,930 --> 00:29:30,870
you substitute this from the kernel

447
00:29:44,530 --> 00:29:44,930
and then u

448
00:29:45,390 --> 00:29:46,140
his linearity

449
00:29:47,040 --> 00:29:48,940
so linearity means you can take these

450
00:29:49,890 --> 00:29:51,070
coefficients inside the sum

451
00:29:51,910 --> 00:29:55,200
it also means you take and you can take the sums necessary inside the dot

452
00:29:55,200 --> 00:29:58,460
product it also means you can take the sums into the dot product

453
00:30:01,570 --> 00:30:02,940
this to both at the same time

454
00:30:16,760 --> 00:30:17,430
okay and then

455
00:30:20,510 --> 00:30:21,120
and that's because

456
00:30:21,650 --> 00:30:22,660
so this vector here

457
00:30:24,550 --> 00:30:25,280
it's called this me

458
00:30:25,920 --> 00:30:29,410
this is the same that are doesn't matter that we use different index so this

459
00:30:29,410 --> 00:30:31,280
is the product of some vector with itself

460
00:30:32,040 --> 00:30:33,250
and that's not negative

461
00:30:36,960 --> 00:30:38,270
so is that clear to everybody

462
00:30:51,710 --> 00:30:54,370
so you're saying that we only put this

463
00:30:55,210 --> 00:30:57,730
and this inside but we did the sums outside it

464
00:30:58,600 --> 00:30:59,370
something like this

465
00:31:08,060 --> 00:31:09,390
well it turns out that

466
00:31:10,110 --> 00:31:11,630
not all these terms will be positive

467
00:31:12,540 --> 00:31:15,360
because this is not the same as this not necessarily right that

468
00:31:16,000 --> 00:31:18,880
terms if i equals say they were positively other ones not

469
00:31:19,930 --> 00:31:20,490
and so

470
00:31:22,020 --> 00:31:25,680
when they when you sum them all up there will be positive but some of them might well be negative

471
00:31:26,980 --> 00:31:28,610
okay thank you very much so let's

472
00:31:29,230 --> 00:31:32,540
then we skip the second one because it would be too complicated now but later

473
00:31:32,540 --> 00:31:33,580
on it would be very easy

474
00:31:37,160 --> 00:31:39,420
yes so the question is he says

475
00:31:39,870 --> 00:31:42,820
phi is a function of x and d

476
00:31:42,910 --> 00:31:45,250
of five expires function of ex-prime so that both

477
00:31:45,770 --> 00:31:48,500
functions well all the elements of human space right

478
00:31:49,190 --> 00:31:51,550
question how to define a dot product in the hilbert space

479
00:31:52,970 --> 00:31:56,750
it depends what kind of objects they also have a space by definition is a

480
00:31:56,750 --> 00:31:58,170
vector space with the dot product

481
00:31:58,910 --> 00:32:00,770
which is complete in a certain sense

482
00:32:01,420 --> 00:32:02,460
but uh

483
00:32:03,430 --> 00:32:07,060
so you can you can define dot products for functions have been defined dot product for vectors

484
00:32:07,730 --> 00:32:10,600
four functions a typical product for instance is

485
00:32:11,650 --> 00:32:13,580
the integral over the product of two functions

486
00:32:14,430 --> 00:32:17,780
but in the reproducing kernel hilbert space which is what we're dealing with today it

487
00:32:17,790 --> 00:32:18,830
will be a little bit different

488
00:32:19,370 --> 00:32:25,880
so maybe just for now just think about it abstractly dot product is just this is symmetric by linear form

489
00:32:26,610 --> 00:32:28,710
with certain properties and so

490
00:32:30,050 --> 00:32:30,600
yeah i don't know

491
00:32:31,030 --> 00:32:31,930
i times better

492
00:32:32,530 --> 00:32:36,270
so it's just the dot product is just a certain mathematical object in

493
00:32:37,310 --> 00:32:42,360
certain type of vector space actually in between him a space of three hundred space is not complete

494
00:32:43,610 --> 00:32:48,070
and there is a mapping phi i could go into a space of functions but

495
00:32:48,130 --> 00:32:51,450
it could also be a mapping into a vector space if you prefer to think

496
00:32:51,450 --> 00:32:54,070
of it as a vector space and then use it on on the product

497
00:32:56,790 --> 00:32:57,980
any more questions about this

498
00:33:00,980 --> 00:33:03,580
so yes so i assume the spaces are

499
00:33:04,620 --> 00:33:08,300
this is something that maybe the physicists among you will be familiar with and for

500
00:33:08,300 --> 00:33:12,090
the computer scientists maybe will initially be be a bit scary to have this

501
00:33:12,890 --> 00:33:15,670
the spaces that could be potentially infinite dimensional

502
00:33:16,270 --> 00:33:19,570
but the hyperspace are mathematically very nice objects very

503
00:33:19,970 --> 00:33:21,090
well behaved and usually

504
00:33:21,520 --> 00:33:23,060
most the things that work in normal

505
00:33:24,300 --> 00:33:27,310
the euclidean vector space is also working hilbert spaces

506
00:33:29,020 --> 00:33:29,850
okay so let's

507
00:33:30,690 --> 00:33:32,600
well to a few more properties so basically the

508
00:33:33,360 --> 00:33:34,310
my goal now is

509
00:33:35,970 --> 00:33:36,500
we've seen

510
00:33:37,150 --> 00:33:38,470
this is true in the first

511
00:33:39,440 --> 00:33:40,480
of these two statements

512
00:33:41,210 --> 00:33:43,260
and basically want to prove the converse

513
00:33:43,700 --> 00:33:45,260
which which i mean the following

514
00:33:45,920 --> 00:33:48,920
if we have some positive definite kernel so kernel

515
00:33:49,470 --> 00:33:50,530
that satisfies

516
00:33:51,080 --> 00:33:52,740
this property here down here

517
00:33:53,740 --> 00:33:54,480
my laser pointer

518
00:33:55,980 --> 00:33:57,680
the that satisfies this thing

519
00:33:59,740 --> 00:34:01,760
there exists some mapping phi

520
00:34:02,600 --> 00:34:03,020
and some

521
00:34:03,460 --> 00:34:05,450
space age with the dot product

522
00:34:06,100 --> 00:34:07,140
that phi maps into

523
00:34:07,890 --> 00:34:10,780
such that the kernel can be written in this form

524
00:34:11,660 --> 00:34:12,230
so this means

525
00:34:13,550 --> 00:34:16,230
everything that has this form is a positive definite kernel

526
00:34:16,910 --> 00:34:20,930
and every positive definite kernel actually has the form of can be given that fall

527
00:34:21,860 --> 00:34:25,620
not necessarily with a unique five but there is some five such that the kernel

528
00:34:25,620 --> 00:34:27,150
can be written this form and it's very

529
00:34:27,830 --> 00:34:31,580
nice because then we can basically think of the kernel as the dot product

530
00:34:32,000 --> 00:34:35,530
just somewhere else and a lot of things become very clean and nice and simple

531
00:34:36,180 --> 00:34:38,150
point this thing becomes very simple proof

532
00:34:39,230 --> 00:34:40,900
and since we're not going to use this one

533
00:34:41,340 --> 00:34:44,000
in constructing the converse of this

534
00:34:44,800 --> 00:34:47,080
maybe we just skip it for now so that was

535
00:34:47,820 --> 00:34:48,950
get the converse and then

536
00:34:49,510 --> 00:34:50,550
let's see how it goes

537
00:34:52,090 --> 00:34:54,800
okay so let's think of a few more properties

538
00:34:55,380 --> 00:34:59,160
maybe we don't have to prove all of them but again i would like to play around a bit

539
00:34:59,950 --> 00:35:00,330
and the

540
00:35:01,570 --> 00:35:02,220
it's a

541
00:35:02,650 --> 00:35:03,610
so this one may be worse

542
00:35:05,010 --> 00:35:06,170
the first one and d

543
00:35:06,610 --> 00:35:06,930
let's see

544
00:35:06,930 --> 00:35:09,590
describing this important experiment

545
00:35:09,600 --> 00:35:13,680
don't worry about it not just run a intuitively

546
00:35:13,730 --> 00:35:14,640
this thing

547
00:35:14,710 --> 00:35:16,770
talked about the probability

548
00:35:16,780 --> 00:35:17,790
if you

549
00:35:17,800 --> 00:35:24,070
the toss coin n times this is the probability of getting a sequence of results

550
00:35:24,110 --> 00:35:25,690
with the property

551
00:35:25,830 --> 00:35:27,420
for this sequence

552
00:35:27,430 --> 00:35:28,720
the empirical mean

553
00:35:28,770 --> 00:35:33,200
and the expectation differ by these epsilon

554
00:35:33,280 --> 00:35:37,590
and this little variant of telephone which we will use later on

555
00:35:37,860 --> 00:35:39,930
which is a very

556
00:35:39,940 --> 00:35:45,000
the situation where rather than comparing here mean to expectation

557
00:35:45,010 --> 00:35:47,580
comparing two different empirical means

558
00:35:47,590 --> 00:35:48,690
so we

559
00:35:48,700 --> 00:35:51,180
we do the experiment two in times

560
00:35:51,190 --> 00:35:52,220
we compare

561
00:35:52,230 --> 00:35:54,430
the average over the first n

562
00:35:54,480 --> 00:35:56,080
coin classes

563
00:35:56,100 --> 00:35:59,600
with the average over the second and the independent

564
00:35:59,680 --> 00:36:01,140
you can imagine that if

565
00:36:01,160 --> 00:36:03,760
we know that this thing here is true

566
00:36:03,770 --> 00:36:07,960
whenever we toss in times is expectation

567
00:36:08,010 --> 00:36:11,470
and of course because again and again it's also close to the station so that

568
00:36:11,470 --> 00:36:14,660
both within epsilon with expectations with

569
00:36:14,670 --> 00:36:16,780
put things together

570
00:36:16,800 --> 00:36:18,740
five inequality

571
00:36:19,070 --> 00:36:22,220
we get a bound on half of these two things can be from each other

572
00:36:22,230 --> 00:36:24,980
and it's a similar bond

573
00:36:25,030 --> 00:36:26,180
the that now

574
00:36:26,190 --> 00:36:29,810
instead of epsilon the right hand side one side two

575
00:36:29,920 --> 00:36:32,180
we have seen some of two here

576
00:36:32,260 --> 00:36:36,270
was where it gets at point here

577
00:36:36,280 --> 00:36:43,700
OK so this is bound tells us how far empirical means be from each other

578
00:36:43,720 --> 00:36:46,490
and this is something that goes into this part is that

579
00:36:46,540 --> 00:36:48,170
these are

580
00:36:48,220 --> 00:36:52,750
you have two n independent trials

581
00:36:52,830 --> 00:37:08,440
can use the product

582
00:37:08,450 --> 00:37:13,920
oh you mean can you have a different probability measures here and here

583
00:37:14,000 --> 00:37:16,460
but with the same expectations

584
00:37:19,140 --> 00:37:21,870
i don't see why it should work

585
00:37:22,060 --> 00:37:24,410
OK but the last eight years

586
00:37:24,430 --> 00:37:26,870
humans that the same but

587
00:37:26,880 --> 00:37:29,780
i think it also work

588
00:37:29,790 --> 00:37:32,350
from each of the same

589
00:37:38,220 --> 00:37:41,230
and this will be quite useful this thing here down here

590
00:37:42,670 --> 00:37:45,080
remember only before the test

591
00:37:45,220 --> 00:37:48,300
we cannot compute expectation of risk

592
00:37:48,320 --> 00:37:51,370
we can compute the expectation of test error

593
00:37:51,390 --> 00:37:53,450
in other words can compute the rest

594
00:37:53,470 --> 00:37:56,150
this thing here not the talk about

595
00:37:56,200 --> 00:38:01,480
something and uncomputable more one less just talks about to run ferns

596
00:38:01,500 --> 00:38:04,470
later we will try to reduce our

597
00:38:04,630 --> 00:38:08,020
problem something like this and they we want we want to use this bound

598
00:38:08,970 --> 00:38:11,270
also also

599
00:38:12,950 --> 00:38:16,800
very slowly with respect to machine and the terminology

600
00:38:16,850 --> 00:38:18,610
and this last

601
00:38:18,630 --> 00:38:20,390
so the the first part here

602
00:38:20,480 --> 00:38:23,860
here means that the probability of obtaining

603
00:38:23,910 --> 00:38:26,640
m sample for example i mean

604
00:38:26,690 --> 00:38:29,960
so pairs x y

605
00:38:30,230 --> 00:38:34,610
where the training error and test differ by more than epsilon

606
00:38:34,630 --> 00:38:38,760
it founded by this quantity here the probability that there

607
00:38:38,850 --> 00:38:43,400
different file then founded on

608
00:38:46,550 --> 00:38:52,130
so far i talked about one fixed function f defined these losses

609
00:38:52,980 --> 00:38:55,510
police said that

610
00:38:55,560 --> 00:39:00,650
these are independent bernoulli trials values one fixed function f x y i

611
00:39:00,660 --> 00:39:06,250
now the question is what happens if we have

612
00:39:06,290 --> 00:39:08,470
we have several functions

613
00:39:09,810 --> 00:39:11,490
maybe even

614
00:39:11,730 --> 00:39:13,490
this is the

615
00:39:13,540 --> 00:39:16,640
maybe these functions

616
00:39:16,660 --> 00:39:17,950
you can chose

617
00:39:17,970 --> 00:39:19,590
from the training points

618
00:39:19,600 --> 00:39:20,740
by this i mean

619
00:39:20,900 --> 00:39:24,830
so this function could be the function minimizing the training error

620
00:39:24,900 --> 00:39:26,920
this is a function that has seen

621
00:39:26,920 --> 00:39:31,790
as much hydrogen ions at the mountain after that you put in water so it's

622
00:39:31,820 --> 00:39:38,360
ionizing completely and weak acid means that you're not ionizing that much and not forming

623
00:39:38,360 --> 00:39:46,830
that much i truly am i and when you put the acid in water

624
00:39:46,880 --> 00:39:51,110
and you can also consider whether some things are strong acid

625
00:39:51,150 --> 00:39:53,520
in terms of its pk

626
00:39:53,540 --> 00:39:55,930
as well not just k

627
00:39:55,940 --> 00:39:59,900
and remember the relationship here so pk

628
00:39:59,900 --> 00:40:02,720
equals minus log of k

629
00:40:02,730 --> 00:40:06,610
so we saw you know with the PKW before

630
00:40:06,690 --> 00:40:09,300
so these are the the same expressions

631
00:40:09,340 --> 00:40:13,070
so if you have a small value less than one

632
00:40:13,110 --> 00:40:14,570
here this is small

633
00:40:14,590 --> 00:40:17,550
going to have a higher value for p

634
00:40:17,570 --> 00:40:18,380
and so

635
00:40:18,410 --> 00:40:19,770
the higher the value

636
00:40:19,780 --> 00:40:23,880
the weaker the acid so you can think about whether some things we gathered in

637
00:40:23,880 --> 00:40:25,220
terms of its k

638
00:40:25,230 --> 00:40:27,630
or in terms of its pk

639
00:40:27,660 --> 00:40:30,520
depending on what information is given to you

640
00:40:30,530 --> 00:40:36,720
and that's the relationship between those terms

641
00:40:36,770 --> 00:40:38,770
so here are some examples

642
00:40:38,820 --> 00:40:42,580
of acids and there's a bunch of these that'll be in your book

643
00:40:44,480 --> 00:40:47,150
here we have the strong acid

644
00:40:47,170 --> 00:40:50,650
and you can see these k numbers are enormous

645
00:40:50,690 --> 00:40:55,520
there are very big so you go pretty much all to product got all the

646
00:40:55,520 --> 00:40:58,050
hydro any amine concentration

647
00:40:58,110 --> 00:41:00,920
so they ionize completely up here

648
00:41:00,920 --> 00:41:04,800
and once you feel most probably hydrochloric acid

649
00:41:04,820 --> 00:41:08,420
hydro makassar those would be the one thing you probably c

650
00:41:08,980 --> 00:41:11,360
in problems that in the book

651
00:41:11,380 --> 00:41:12,320
and then

652
00:41:12,320 --> 00:41:16,460
we go from the strong acid down here week these are cases of less than

653
00:41:17,420 --> 00:41:22,370
so here you don't have nearly as much ionisation and of course when these numbers

654
00:41:22,370 --> 00:41:26,780
get small pk also get bigger

655
00:41:26,790 --> 00:41:28,480
and we can keep going

656
00:41:28,500 --> 00:41:30,130
there are a lot of methods

657
00:41:30,150 --> 00:41:32,980
keep going down here to where you have really

658
00:41:33,000 --> 00:41:39,210
truly small k numbers and quite large PK's these would be very weak

659
00:41:39,230 --> 00:41:40,630
very little

660
00:41:40,630 --> 00:41:47,210
ionisation when you put that on that as it into water

661
00:41:48,070 --> 00:41:59,840
let's do the same thing for bases

662
00:41:59,860 --> 00:42:09,590
so let's look at the base in water

663
00:42:09,590 --> 00:42:11,190
so let's take

664
00:42:11,280 --> 00:42:12,550
NH three

665
00:42:12,570 --> 00:42:18,210
in water

666
00:42:18,270 --> 00:42:23,340
the base will accept the high iron forming NH four plus

667
00:42:29,320 --> 00:42:32,360
so when you have a base in water

668
00:42:32,420 --> 00:42:34,780
you're going to form hydroxide

669
00:42:34,790 --> 00:42:40,660
when you have an acid in water are going to be performing hydro neon ions

670
00:42:40,670 --> 00:42:42,850
so in this base in water

671
00:42:42,860 --> 00:42:45,750
you're going to be talking about the these

672
00:42:45,750 --> 00:42:48,820
ionisation constant

673
00:42:48,840 --> 00:42:53,190
which is you talking about

674
00:42:56,150 --> 00:42:58,130
and so KB

675
00:42:58,160 --> 00:43:00,450
is going to be equal to

676
00:43:00,590 --> 00:43:03,820
concentration of products

677
00:43:03,880 --> 00:43:06,900
and equilibria

678
00:43:06,910 --> 00:43:11,080
over the concentration of a reactant

679
00:43:11,090 --> 00:43:16,000
again water is not included solvent in this case

680
00:43:16,090 --> 00:43:18,710
in the KB here

681
00:43:18,750 --> 00:43:21,360
is equal to one point eight

682
00:43:21,570 --> 00:43:23,740
ten to the minus five

683
00:43:23,790 --> 00:43:26,690
so it's a fairly small number again

684
00:43:26,690 --> 00:43:28,850
thank you for coming to my talk

685
00:43:28,950 --> 00:43:33,790
in many respects because i'm not doing post google art

686
00:43:34,700 --> 00:43:36,220
before that just

687
00:43:36,240 --> 00:43:38,580
very short biography i

688
00:43:39,520 --> 00:43:46,420
linguistic of the university and it is and then i went to south germany got

689
00:43:46,420 --> 00:43:51,810
a message to the station was extended phd also germany the

690
00:43:52,100 --> 00:43:57,390
research institute which is called the media and after that i

691
00:43:57,420 --> 00:43:59,420
that is cost that goal

692
00:43:59,420 --> 00:44:05,060
and today i'm going to talk about and the measurement process and

693
00:44:06,200 --> 00:44:10,850
some topics which i know more about his age

694
00:44:10,870 --> 00:44:14,270
so if i'm talking to quickly or if you have questions feel free to drop

695
00:44:14,270 --> 00:44:15,490
me a note

696
00:44:20,070 --> 00:44:24,070
well i'm going to introduce new people or to my friends and they asked me

697
00:44:24,070 --> 00:44:31,290
what i do have access to computational linguistics and natural language processing go they me

698
00:44:31,290 --> 00:44:36,280
what is that to with little world war natural image processing what race relations the

699
00:44:36,540 --> 00:44:39,900
so the first two bytes of just explaining

700
00:44:39,960 --> 00:44:41,560
where the information comes

701
00:44:41,570 --> 00:44:47,880
so natural language processing is the processing of major languages and the images are

702
00:44:47,900 --> 00:44:51,730
languages like russian english german

703
00:44:51,740 --> 00:44:57,650
all of the approximately five thousand languages exist in the world and

704
00:44:57,820 --> 00:45:01,120
now part of google's mission is to

705
00:45:01,130 --> 00:45:04,060
organize the world's information and make it universally

706
00:45:04,070 --> 00:45:06,840
universally accessible so

707
00:45:06,850 --> 00:45:11,480
it means in the first place to organize the information available on the web that

708
00:45:12,310 --> 00:45:18,670
the information available on the sites and most of the information there is

709
00:45:18,680 --> 00:45:23,240
concord nature which text serious people talk to each other

710
00:45:23,240 --> 00:45:25,470
so it's all about language

711
00:45:25,490 --> 00:45:27,020
and it means two

712
00:45:27,150 --> 00:45:32,460
organized was information is to understand those documents to understand the text to understand natural

713
00:45:32,460 --> 00:45:36,610
language to present to the users

714
00:45:37,240 --> 00:45:38,840
the sample is

715
00:45:38,860 --> 00:45:42,000
the use and the information retrieval

716
00:45:42,020 --> 00:45:43,410
so we have users

717
00:45:43,810 --> 00:45:49,710
this was search for things but it also gives rise to different topics machine translation

718
00:45:49,710 --> 00:45:55,020
speech recognition of civilization and

719
00:45:55,020 --> 00:45:59,860
there are two sides the image processing so that people also

720
00:45:59,900 --> 00:46:04,150
why do we need linguistic you can build statistical models two

721
00:46:04,160 --> 00:46:06,660
so many things without knowing

722
00:46:06,680 --> 00:46:10,150
much of the language and one that's true so right

723
00:46:10,160 --> 00:46:15,120
now might techniques need for scalable ontology language processing and those

724
00:46:15,580 --> 00:46:18,370
bag of words model or

725
00:46:18,530 --> 00:46:20,720
which are used for example

726
00:46:20,750 --> 00:46:22,520
vector space models

727
00:46:22,520 --> 00:46:28,300
things like TFIDF which are not that much is to try to find get some

728
00:46:28,310 --> 00:46:31,590
comments so there's not much is going on there

729
00:46:31,650 --> 00:46:33,840
but deep understanding

730
00:46:33,860 --> 00:46:35,650
implies analysis

731
00:46:35,690 --> 00:46:44,120
linguistic analysis of on different levels of participation information syntactic information semantic analysis discourse analysis

732
00:46:44,140 --> 00:46:46,240
so to say

733
00:46:46,270 --> 00:46:49,220
and today

734
00:46:49,280 --> 00:46:54,030
i'm going to talk about the following things such as this one and see more

735
00:46:54,030 --> 00:46:58,970
about just a brief introduction what kind of in the tests

736
00:46:58,990 --> 00:47:04,780
dean so that will spend about twenty minutes talking about things like machine translation speech

737
00:47:05,870 --> 00:47:09,640
information extraction i won't go into much detail here

738
00:47:09,680 --> 00:47:11,400
because the

739
00:47:11,410 --> 00:47:18,870
several reasons that complex topics that one second it's not really what i'm working more

740
00:47:18,890 --> 00:47:22,810
competent to talk about things like civilization and this is what the second part of

741
00:47:22,810 --> 00:47:24,370
my talk is going to be about

742
00:47:24,370 --> 00:47:28,710
so the second part of my talk i will present

743
00:47:28,720 --> 00:47:36,290
and that two summaries using different languages and i will present experiments that will use

744
00:47:36,320 --> 00:47:37,780
if you know the site

745
00:47:38,050 --> 00:47:41,710
and then we also showed

746
00:47:42,710 --> 00:47:45,400
youtube and how users comments to be used

747
00:47:45,400 --> 00:47:48,440
also to get an idea what the media is that

748
00:47:48,460 --> 00:47:50,300
so that's one

749
00:47:51,660 --> 00:47:54,400
i guess most of you know that google

750
00:47:54,420 --> 00:47:56,960
his interest in machine translation so

751
00:47:56,970 --> 00:47:59,000
there's one which translates

752
00:47:59,010 --> 00:48:04,300
i thought the dotcom they come and any of

753
00:48:04,330 --> 00:48:10,000
one sentence in language you don't understand or something that you understand want to translate

754
00:48:10,010 --> 00:48:12,580
and this translation works

755
00:48:12,590 --> 00:48:16,460
four about what it the moment the quality difference

756
00:48:16,500 --> 00:48:20,630
quality of german english or english german translations definitely

757
00:48:20,630 --> 00:48:26,690
so this presentation is about to turn nature i usually like to help people in

758
00:48:26,690 --> 00:48:32,250
trouble with questions so please don't hesitate if there is something you would like to

759
00:48:38,800 --> 00:48:47,280
so this is the way i defined feature selection

760
00:48:47,300 --> 00:48:50,420
imagine that you have the data table

761
00:48:50,440 --> 00:48:51,830
the lines are

762
00:48:51,850 --> 00:48:56,260
patterns and the columns are features

763
00:48:56,260 --> 00:49:01,440
and you may have thousands to millions of low level features in a lot of

764
00:49:01,440 --> 00:49:03,330
recent applications

765
00:49:03,340 --> 00:49:06,480
they represent themselves in this way

766
00:49:06,500 --> 00:49:09,330
and you would like to select the most relevant features

767
00:49:09,340 --> 00:49:15,000
in order to build better faster and easier to understand learning machines which in essence

768
00:49:15,010 --> 00:49:15,910
was down to

769
00:49:16,360 --> 00:49:21,260
selecting the number of columns of your data matrix

770
00:49:21,290 --> 00:49:26,150
so in the first couple of examples of applications of this problem

771
00:49:26,160 --> 00:49:29,020
i've been working a lot in the medical

772
00:49:29,190 --> 00:49:35,550
data analysis and the first example is taken from the kenya diagnosis problem

773
00:49:35,570 --> 00:49:43,210
in which looking at patients have been had had their blood drawn and

774
00:49:43,210 --> 00:49:45,190
from there sarah

775
00:49:45,260 --> 00:49:47,260
what has assessed the

776
00:49:49,960 --> 00:49:56,520
all of their genes and this is done with the so-called DNA microarrays

777
00:49:56,540 --> 00:50:01,240
so in in this slide each mailbox here represents

778
00:50:02,040 --> 00:50:07,070
the level of expression of a given gene if the gene is underexpressed compared to

779
00:50:07,070 --> 00:50:12,290
normal it's blue and otherwise it's right it's overexpressed

780
00:50:12,350 --> 00:50:18,900
and as i said before each line represents the patient which con represents a future

781
00:50:18,900 --> 00:50:20,460
in that case the gene

782
00:50:20,490 --> 00:50:23,930
there are some special causes the tower to the right and the left and those

783
00:50:23,930 --> 00:50:25,480
were present target

784
00:50:25,680 --> 00:50:32,290
there are two subpopulations in the data the patients and DAML patients are two different

785
00:50:32,290 --> 00:50:33,820
kinds of leukemia

786
00:50:33,860 --> 00:50:37,820
and those are represented by here you're ready

787
00:50:37,860 --> 00:50:40,630
bar and here the blue bar

788
00:50:40,660 --> 00:50:46,360
so one very simple way of selecting future is to looking for those columns in

789
00:50:46,360 --> 00:50:50,850
the data set which are most correlated with the target or which are most often

790
00:50:50,920 --> 00:50:55,280
correlated with the target so here we have the opposite of the target

791
00:50:55,290 --> 00:51:00,980
we usually use target values plus or minus one so would have all plus ones

792
00:51:01,010 --> 00:51:05,100
for the ll and or minus ones for the AML so we call in that

793
00:51:05,100 --> 00:51:08,670
case the positive class class

794
00:51:08,720 --> 00:51:12,720
so this is what people in this early paper in nineteen ninety nine had done

795
00:51:12,760 --> 00:51:14,980
they look for those

796
00:51:15,410 --> 00:51:23,510
genes that were either correlated are correlated with all correlated with the target

797
00:51:23,630 --> 00:51:28,600
you don't need to limit yourself to two classification problems you can do multiclass you

798
00:51:28,600 --> 00:51:33,860
can do regression in the a variety of other cases but the problem always boils

799
00:51:33,860 --> 00:51:40,070
down to finding a subset of the original variables the most predictive

800
00:51:40,070 --> 00:51:44,610
in this example you see another representation of the data instead of showing the data

801
00:51:44,610 --> 00:51:49,220
matrix like in the previous slide we show three features and we have a scatter

802
00:51:49,220 --> 00:51:53,290
plots of the examples in this three dimensional space

803
00:51:53,320 --> 00:51:57,450
so imagine that you run a feature selection algorithm and you find those three

804
00:51:57,470 --> 00:51:59,340
informative features

805
00:51:59,350 --> 00:52:04,540
and imagine that you had three classes in that case it's prostate cancer problem and

806
00:52:04,540 --> 00:52:09,380
there are three populations tissues

807
00:52:09,420 --> 00:52:15,170
tissues that correspond to benign prostate hyperplasia BPH and two different ways of cancer grade

808
00:52:15,170 --> 00:52:17,410
three and grade four

809
00:52:19,100 --> 00:52:26,310
here are represented by these edits as we show the three clusters forming by the

810
00:52:26,310 --> 00:52:29,310
three populations in this three-dimensional space

811
00:52:29,320 --> 00:52:35,660
so feature selection can be both used for improving predictions

812
00:52:35,670 --> 00:52:42,950
or for facilitating like here visualization of data

813
00:52:42,970 --> 00:52:48,910
number to show you a couple of examples of feature selection to illustrate that are

814
00:52:48,980 --> 00:52:54,090
feature selection does not always yield performance improvement

815
00:52:54,100 --> 00:52:55,470
in this paper

816
00:52:55,480 --> 00:52:56,710
the oscars

817
00:52:56,720 --> 00:52:59,290
try to perform cancer diagnosis

818
00:52:59,300 --> 00:53:03,840
also from DNA microarray data and they vary the number of genes in here they

819
00:53:03,850 --> 00:53:06,350
show accuracy of classification

820
00:53:06,360 --> 00:53:12,160
for the problem of classifying fourteen tumours and then they tried a number of different

821
00:53:12,160 --> 00:53:17,160
classifiers without entering into details in these new have different curves here corresponding to different

822
00:53:17,160 --> 00:53:21,370
strategies to be adopted and you see that in some cases

823
00:53:21,390 --> 00:53:23,160
there is an optimum

824
00:53:23,980 --> 00:53:26,450
accuracy increases as ring

825
00:53:26,460 --> 00:53:32,770
increase the number of features and then decreases again in other cases even monotonically basically

826
00:53:32,770 --> 00:53:38,280
monotonically increases so the more features the better

827
00:53:38,290 --> 00:53:44,160
so as it turns out that nowadays there are some very of some parameters that

828
00:53:44,160 --> 00:53:46,740
are very robust against overfitting

829
00:53:48,090 --> 00:53:52,850
in the old days people used to say that there is this curse of dimensionality

830
00:53:52,850 --> 00:53:57,830
and that it is extremely important to do space dimensionality reduction in order to get

831
00:53:57,830 --> 00:54:01,280
good performance when you have very few training examples

832
00:54:01,330 --> 00:54:08,040
nowadays this is no longer true you can work in very very high dimensional spaces

833
00:54:08,040 --> 00:54:15,400
and if you have a highly regularized classifiers you can get a very good performance

834
00:54:15,950 --> 00:54:21,970
without performing first space dimensionality reduction in this is illustrated by his here the more

835
00:54:21,980 --> 00:54:25,920
features the better performance gets in the way a few

836
00:54:25,930 --> 00:54:30,610
throw away some features you throw away some information because you're not sure that you're

837
00:54:30,610 --> 00:54:34,710
going to be eliminating the bad ones

838
00:54:34,720 --> 00:54:40,240
however for some problems you do see a very clear optimal

839
00:54:40,350 --> 00:54:43,700
so for some problems in which you have a large number of features that are

840
00:54:45,270 --> 00:54:47,700
then filtering them out

841
00:54:47,720 --> 00:54:52,980
does improve performance and as you can see here in this problem of drug screening

842
00:54:53,060 --> 00:54:59,860
and in which there were around two thousand five hundred compounds that were tested for

843
00:54:59,870 --> 00:55:04,000
their ability to bind to a target sites

844
00:55:04,020 --> 00:55:10,160
the the purpose was to determine whether a molecule was going to be active or

845
00:55:10,160 --> 00:55:12,620
inactive and there were

846
00:55:12,640 --> 00:55:17,920
over a hundred thousand binary features describing the properties of the

847
00:55:17,930 --> 00:55:20,370
three-dimensional molecule

848
00:55:20,390 --> 00:55:25,600
most of these features being completely useless and in fact as you can see as

849
00:55:25,600 --> 00:55:30,090
a function of success rate the optimal number feature is between

850
00:55:30,170 --> 00:55:32,300
at twenty and forty

851
00:55:32,300 --> 00:55:35,060
here's how to solve the optimisation problem

852
00:55:35,070 --> 00:55:41,330
and i have shown you want application i would now like to

853
00:55:41,350 --> 00:55:45,070
i would like to retain is but i'm not i don't have the flow

854
00:55:47,460 --> 00:55:48,480
thank you

855
00:55:53,420 --> 00:55:56,680
OK thank you

856
00:55:56,850 --> 00:56:01,580
so i would like to show you a period which is

857
00:56:01,630 --> 00:56:03,640
very important for kernel methods

858
00:56:03,680 --> 00:56:07,410
it's important not just for support vector machines but also for other types of kernel

859
00:56:08,620 --> 00:56:10,550
and it doesn't

860
00:56:10,690 --> 00:56:13,600
it's not complicated to prove so i would like to to show it

861
00:56:13,750 --> 00:56:17,440
so think about ten minutes and then i will

862
00:56:17,450 --> 00:56:22,510
i have time to tell you a little bit about about logarithms of and support

863
00:56:22,510 --> 00:56:24,070
vector machines

864
00:56:24,120 --> 00:56:28,640
so this is called the representer theorem and in the original the first person which

865
00:56:28,640 --> 00:56:35,760
is less general than this one it was proved by bobbi bind kimeldorf

866
00:56:35,770 --> 00:56:39,690
in the theorem goes as follows so suppose we have a positive definite kernel

867
00:56:39,700 --> 00:56:43,240
we have a training set

868
00:56:43,480 --> 00:56:49,180
training set this is for the case of regression estimation but

869
00:56:49,220 --> 00:56:51,920
you will see later also applies to classification

870
00:56:51,970 --> 00:56:56,060
so we have here inputs and we've output values y

871
00:56:56,900 --> 00:57:02,090
moreover we have a strictly monotonic increasing function

872
00:57:02,310 --> 00:57:04,810
on the positive half axis

873
00:57:04,830 --> 00:57:06,550
so this function is

874
00:57:06,920 --> 00:57:08,700
on the other

875
00:57:08,710 --> 00:57:12,110
and plays the role of rescaling are

876
00:57:12,120 --> 00:57:14,350
martin to or regularisation term

877
00:57:14,740 --> 00:57:18,490
and in addition we have a cost function

878
00:57:18,490 --> 00:57:23,820
it's a bit complicated by the cost function that measures

879
00:57:23,940 --> 00:57:29,770
the the empirical error also you want is to compare the empirical error could be

880
00:57:29,770 --> 00:57:35,130
for instance squared error that we use so we we measure the square deviation between

881
00:57:35,210 --> 00:57:40,210
the target values why i end of prediction f of x i

882
00:57:40,350 --> 00:57:45,360
but then some all these i that would be one possible such cost function but

883
00:57:45,550 --> 00:57:49,920
this one is more general in that it allows dependence

884
00:57:49,930 --> 00:57:51,460
jointly on all

885
00:57:51,510 --> 00:57:53,400
points at the same time

886
00:57:53,420 --> 00:57:55,590
so it doesn't have to decompose into some

887
00:57:55,830 --> 00:57:59,390
in the statement of the representer theorem is that

888
00:57:59,420 --> 00:58:00,770
any solution

889
00:58:00,800 --> 00:58:04,760
of this problem solution of minimizing this problem

890
00:58:04,770 --> 00:58:09,460
so the problem is to minimize the empirical out plus

891
00:58:09,490 --> 00:58:13,780
this regularisation term to think of this as the margin to before we had a

892
00:58:13,780 --> 00:58:17,270
norm of w squares to think of this is the norm of w square by

893
00:58:17,300 --> 00:58:18,330
the modern world

894
00:58:18,590 --> 00:58:19,430
so any

895
00:58:19,450 --> 00:58:22,510
solution to this problem can be written

896
00:58:22,520 --> 00:58:27,050
as an expansion in terms of kernels now the crucial thing is kernels centered on

897
00:58:27,050 --> 00:58:32,010
the training points so these exciting of the training points

898
00:58:32,020 --> 00:58:33,150
OK so

899
00:58:35,590 --> 00:58:41,230
so surprising maybe because we are solving the problem in the in this reproducing kernel

900
00:58:41,230 --> 00:58:42,430
hilbert space

901
00:58:42,430 --> 00:58:46,620
which can be very high dimensional hilbert space in the case of the gaussian kernel

902
00:58:46,620 --> 00:58:49,210
it's an infinite dimensional hilbert space

903
00:58:49,280 --> 00:58:52,030
which contains all possible functions

904
00:58:53,390 --> 00:58:59,180
sitting on different points all linear combinations of all these functions and even the limits

905
00:58:59,270 --> 00:59:03,580
and i'm saying that the solution to this problem always has an expansion

906
00:59:03,590 --> 00:59:06,270
o is the linear combination of kernels

907
00:59:06,280 --> 00:59:10,830
sitting on the training points in the training set could be just a tiny subset

908
00:59:10,840 --> 00:59:12,270
of the whole

909
00:59:12,300 --> 00:59:16,860
set of possible points on which you could have come city

910
00:59:16,870 --> 00:59:22,680
so that's nice because somehow it reduces the problem of finding

911
00:59:22,700 --> 00:59:30,460
the function minimizing this risk functional minimizing this objective function it reduces the problem of

912
00:59:30,460 --> 00:59:35,180
finding a function in high dimensions are in infinite dimensional hilbert spaces to the problem

913
00:59:35,200 --> 00:59:36,210
of finding

914
00:59:36,260 --> 00:59:40,050
a finite number of coefficients of five

915
00:59:40,120 --> 00:59:42,090
and this is actually

916
00:59:42,110 --> 00:59:46,710
you've already seen an example of this which was the support vector machine

917
00:59:47,780 --> 00:59:50,390
and see where it is

918
00:59:54,210 --> 00:59:58,060
the support vector machine i was telling you that

919
00:59:58,080 --> 01:00:01,690
there's an expansion of the solution w

920
01:00:01,730 --> 01:00:04,890
in terms of the training points

921
01:00:04,900 --> 01:00:07,880
so that was nice because that it that meant we only have to find the

922
01:00:07,880 --> 01:00:09,660
coefficients alpha i

923
01:00:09,710 --> 01:00:11,640
so the same thing is true in the more

924
01:00:11,650 --> 01:00:17,510
general setting

925
01:00:17,520 --> 01:00:19,790
and i'll show you why

926
01:00:19,820 --> 01:00:23,850
OK so this is the year so it means that

927
01:00:23,880 --> 01:00:28,470
many learning algorithms all the ones that satisfy the conditions of this theorem have solutions

928
01:00:28,470 --> 01:00:32,170
that can be expressed as expansions in terms of the training examples

929
01:00:32,210 --> 01:00:35,310
the or in their original form

930
01:00:35,910 --> 01:00:37,170
cost function

931
01:00:37,210 --> 01:00:42,830
took the form of a sum over quadratic errors

932
01:00:42,850 --> 01:00:47,660
but it works for arbitrary cost functions even depending jointly and all the points and

933
01:00:47,660 --> 01:00:52,580
it also works for more general regularizers but that's not important for us

934
01:00:52,660 --> 01:00:56,520
it's a to see why this theorem is true

935
01:00:56,540 --> 01:01:00,870
so the way we're going to do is we decompose

936
01:01:01,460 --> 01:01:06,980
that's the solution is assume f is the solution of all problems this optimisation problem

937
01:01:06,980 --> 01:01:09,070
and will decompose into apart

938
01:01:09,460 --> 01:01:13,920
that is a linear combination of the training examples of the kernel sitting in training

939
01:01:14,910 --> 01:01:18,190
plus some power that's orthogonal to it

940
01:01:18,200 --> 01:01:20,650
we can do this because we're working in hilbert space

941
01:01:20,830 --> 01:01:25,830
so it's this notion of the orthogonality we have the product

942
01:01:25,950 --> 01:01:30,580
and so we can we can decompose this function in this way and it probably

943
01:01:30,580 --> 01:01:34,070
would think that this is a very low dimensional subspace here

944
01:01:34,170 --> 01:01:38,480
subspace where they spent by the kernel sitting on the training points and this could

945
01:01:38,480 --> 01:01:42,710
be something in a huge space and what we're going to show is that this

946
01:01:42,710 --> 01:01:45,260
actually begins at this part zero the

947
01:01:45,270 --> 01:01:48,920
without making the solution works

948
01:01:48,950 --> 01:01:52,290
remember in the formulation from before

949
01:01:52,320 --> 01:01:53,870
i was saying

950
01:01:55,810 --> 01:01:58,020
you seem to prove so

951
01:01:58,020 --> 01:02:05,270
so we have this decomposition is a linear combination in terms of the training points

952
01:02:05,270 --> 01:02:10,500
this is still from the part of from the path is orthogonal to all these

953
01:02:10,520 --> 01:02:11,630
basis vectors

954
01:02:11,650 --> 01:02:16,020
spanning this linear subspace to all the girls single training points

955
01:02:16,090 --> 01:02:17,330
so now we

956
01:02:18,450 --> 01:02:19,660
all functions

957
01:02:19,670 --> 01:02:22,710
to some training point xj

958
01:02:22,780 --> 01:02:27,700
if we do this we remember by the reproducing kernel property from yesterday

959
01:02:27,980 --> 01:02:32,580
application of function to a training point is the same as taking the dot product

960
01:02:32,580 --> 01:02:35,130
with the kernel sitting on that point

961
01:02:35,150 --> 01:02:37,650
so the kernel is

962
01:02:37,730 --> 01:02:42,080
he's actually this point evaluation functional that if told you about

963
01:02:42,330 --> 01:02:47,460
yesterday the kernel is the represent of point evaluation

964
01:02:48,250 --> 01:02:50,480
we can write this as product

965
01:02:50,500 --> 01:02:54,060
in this function space between us and the concert in the next day

966
01:02:54,520 --> 01:02:59,150
and not just substitute my expression for f in here and here i case the

967
01:02:59,150 --> 01:03:04,400
next day and this is some so i can take the two dot product separately

968
01:03:04,650 --> 01:03:09,210
now i know that the product between f perpendicular and k zero

969
01:03:09,220 --> 01:03:12,940
so this thing here banishes so what we're left with is the dot product between

970
01:03:13,830 --> 01:03:15,190
and this

971
01:03:15,330 --> 01:03:20,220
and now we can take some of what we want but the crucial point is

972
01:03:20,220 --> 01:03:25,160
that the results of application of f to a training point is independent of this

973
01:03:25,170 --> 01:03:31,420
point of this part here so if we only worried about flying elephant training points

974
01:03:31,420 --> 01:03:34,120
this thing isn't going to make a difference

975
01:03:34,120 --> 01:03:34,910
so in this

976
01:03:34,920 --> 01:03:37,680
the first part of the tutorial i talked about

977
01:03:37,700 --> 01:03:41,640
adaboost and a couple of ways of understanding the algorithm

978
01:03:41,960 --> 01:03:46,730
one way is to think is in terms of the original technical meaning of what

979
01:03:46,730 --> 01:03:48,170
boosting means

980
01:03:48,190 --> 01:03:52,170
which is to take a weak learning algorithm converted into a strong learning algorithm

981
01:03:52,200 --> 01:03:55,050
is this microphone OK by the way

982
01:03:56,360 --> 01:04:09,220
so better

983
01:04:09,240 --> 01:04:12,350
the good

984
01:04:12,360 --> 01:04:13,330
all right

985
01:04:13,340 --> 01:04:16,010
raise your hand or something if you can hear me OK

986
01:04:16,030 --> 01:04:21,460
so we looked at boosting in the technical sense very briefly and then we talked

987
01:04:21,470 --> 01:04:26,520
a lot about understanding boosting as an algorithm for maximizing the margins as the margin

988
01:04:26,520 --> 01:04:31,050
maximisation algorithm similar but different to support vector machines

989
01:04:31,060 --> 01:04:33,660
so what's kind of interesting about adaboost

990
01:04:33,670 --> 01:04:38,000
is that there are many many different ways of understanding this one

991
01:04:39,500 --> 01:04:42,380
and so what i wanted to do

992
01:04:42,430 --> 01:04:49,660
is tried it describes some of those other ways of understanding adaboost which are really

993
01:04:49,660 --> 01:04:51,990
very different ways of understanding

994
01:04:52,750 --> 01:04:56,450
i plan to talk about the game theoretic view

995
01:04:56,500 --> 01:05:01,250
of adaboost and then go on and talk about boosting as a method for minimizing

996
01:05:01,270 --> 01:05:03,420
the loss minimisation methods

997
01:05:03,440 --> 01:05:07,850
but running a little bit short of time so i'm going to have to

998
01:05:08,150 --> 01:05:12,660
kind of skip over the game theoretic you which is that a high level

999
01:05:12,670 --> 01:05:17,970
a game you know we're talking about very simple matrix games like this one for

1000
01:05:17,970 --> 01:05:20,360
rock paper scissors

1001
01:05:20,380 --> 01:05:21,620
and one of the

1002
01:05:21,630 --> 01:05:25,320
i'm just going to zip right through this so one of the core

1003
01:05:25,970 --> 01:05:32,190
theorems in game theory is what's called the minmax theorem which says that basically if

1004
01:05:32,290 --> 01:05:36,460
one player goes before the other player you get the same result as that the

1005
01:05:36,460 --> 01:05:38,010
other player goes before

1006
01:05:38,030 --> 01:05:41,460
the first player this is kind of the fundamental theorem of

1007
01:05:41,540 --> 01:05:44,120
zero sum games

1008
01:05:44,130 --> 01:05:50,320
and it turns out that you can associate a game very natural game with adaboost

1009
01:05:50,990 --> 01:05:55,160
adaboost is kind of an interaction or boosting in general is an interaction is an

1010
01:05:55,160 --> 01:06:01,080
interaction between the boosting algorithm on the one hand and the weak learning algorithm on

1011
01:06:01,080 --> 01:06:02,360
the other hand

1012
01:06:02,420 --> 01:06:05,350
so we can associate a very natural

1013
01:06:05,390 --> 01:06:08,540
game matrix game between those two players

1014
01:06:08,560 --> 01:06:09,430
in which

1015
01:06:09,470 --> 01:06:13,080
the boosting algorithm has as its choices of play

1016
01:06:13,100 --> 01:06:18,320
the training examples and the weak learning algorithm has as its choices of play the

1017
01:06:18,320 --> 01:06:21,690
weak classifiers so on each round of boosting

1018
01:06:22,460 --> 01:06:27,920
boosting algorithm is choosing a distribution over the training examples that kind of its play

1019
01:06:27,930 --> 01:06:33,430
and the weak learning algorithm is responding by choosing one of these weak classifiers

1020
01:06:34,230 --> 01:06:37,890
the point is that obviously going to pass to get any of the details but

1021
01:06:37,890 --> 01:06:39,860
the point is that you can associate

1022
01:06:39,870 --> 01:06:42,420
this game with boosting

1023
01:06:42,440 --> 01:06:46,380
so what they feel can build the matrix what's the big deal

1024
01:06:46,430 --> 01:06:52,160
well it turns out that the minimax theorem the fundamental theorem of game theory

1025
01:06:52,210 --> 01:06:53,370
it turns out

1026
01:06:53,390 --> 01:06:56,310
for this game to say something really interesting

1027
01:06:56,320 --> 01:07:00,180
it says that if we start with the weak learning assumption which says that for

1028
01:07:00,180 --> 01:07:02,400
any distribution over examples

1029
01:07:02,410 --> 01:07:05,700
you can find the weak classifiers accuracy

1030
01:07:05,710 --> 01:07:09,470
is a little bit more than random guessing or hoosier is little bit less than

1031
01:07:09,470 --> 01:07:10,850
random guessing

1032
01:07:10,860 --> 01:07:14,000
then you plug that into the minimax theorem

1033
01:07:14,030 --> 01:07:15,300
what it tells you

1034
01:07:15,310 --> 01:07:20,920
is that there has to exist a weighted majority vote of the classifiers such as

1035
01:07:20,930 --> 01:07:23,690
the weighted majority vote which is found by adaboost

1036
01:07:23,740 --> 01:07:28,600
which correctly classifies all of the training examples and in fact it does it

1037
01:07:28,610 --> 01:07:30,190
with positive margin

1038
01:07:30,200 --> 01:07:34,700
so the minimax theorem tells us that there exists a weighted majority vote

1039
01:07:35,930 --> 01:07:39,200
which classifies all the training examples correctly

1040
01:07:40,420 --> 01:07:43,250
in fact adaboost turns out to be

1041
01:07:43,470 --> 01:07:45,660
algorithm for playing this game

1042
01:07:48,020 --> 01:07:49,760
so it turns out that adaboost

1043
01:07:49,780 --> 01:07:54,490
it's actually just a special case of a much more general algorithm for playing games

1044
01:07:54,490 --> 01:07:58,850
repeatedly in such a way that eventually you solve the games meaning that you find

1045
01:08:00,480 --> 01:08:03,460
strategies is optimal plays

1046
01:08:05,780 --> 01:08:11,560
so for instance we can show that the distribution of examples that computed by adaboost

1047
01:08:11,560 --> 01:08:12,550
is actually

1048
01:08:12,600 --> 01:08:17,010
in approximate minmax strategy for the boosting game so this kind of tells us

1049
01:08:17,030 --> 01:08:18,960
something very specific about

1050
01:08:18,970 --> 01:08:23,780
what those distributions are converting to for adaboost

1051
01:08:23,790 --> 01:08:25,680
and it also turns out that this

1052
01:08:25,720 --> 01:08:27,370
game playing algorithm

1053
01:08:27,390 --> 01:08:30,520
if you set things up a little bit differently then you end up with an

1054
01:08:30,520 --> 01:08:35,140
online learning algorithm similar to the ones that you're on talked about yesterday

1055
01:08:35,150 --> 01:08:39,210
it gives us a different algorithm called the weighted majority algorithm which is also an

1056
01:08:39,210 --> 01:08:41,090
online learning algorithm

1057
01:08:41,100 --> 01:08:45,250
in fact we that adaboost actually came about was we started with the weighted majority

1058
01:08:46,390 --> 01:08:47,530
and we kind of

1059
01:08:47,540 --> 01:08:49,710
i ended up applying to

1060
01:08:49,760 --> 01:08:53,090
the game that i was talking about that here

1061
01:08:53,100 --> 01:08:55,360
OK so that's a very quick

1062
01:08:55,370 --> 01:09:00,140
look at the connection between an interesting game theory

1063
01:09:01,090 --> 01:09:05,080
there's a more popular view of adaboost which

1064
01:09:05,100 --> 01:09:07,230
grew largely out of

1065
01:09:07,280 --> 01:09:09,340
the statistical view of boosting

1066
01:09:09,360 --> 01:09:14,250
which is to view boosting as an algorithm for minimizing a loss function

1067
01:09:14,290 --> 01:09:18,430
so we've heard a lot about loss functions over the last several days and probably

1068
01:09:18,430 --> 01:09:19,890
last week two

1069
01:09:20,880 --> 01:09:26,940
there are many or most learning algorithms minimize some kind of loss function

1070
01:09:26,970 --> 01:09:29,550
minimize some kind of loss functions

1071
01:09:29,600 --> 01:09:35,310
so it is natural to ask is there a loss function that associated with adaboost

1072
01:09:35,330 --> 01:09:39,270
and it turns out that there's turns out that you can view adaboost as an

1073
01:09:40,290 --> 01:09:42,930
for minimizing loss function

1074
01:09:42,940 --> 01:09:47,100
so if you go back to the training year proved that we went through before

1075
01:09:47,150 --> 01:09:51,090
you can see that what adaboost is actually doing this it's not minimizing the number

1076
01:09:51,090 --> 01:09:53,130
of mistakes on the training set

1077
01:09:53,150 --> 01:09:58,790
but it's really minimizing is the product of these normalisation constant right we had step

1078
01:09:58,800 --> 01:10:00,730
two of that group way back when

1079
01:10:00,770 --> 01:10:03,230
was that the training year is upper bounded

1080
01:10:03,960 --> 01:10:07,200
this product of the normalization factors

1081
01:10:07,220 --> 01:10:11,170
and if you work out what that product of normalization factors as

1082
01:10:11,180 --> 01:10:13,590
well it works out to be

1083
01:10:13,640 --> 01:10:17,940
this expression here which is just one over the number of examples

1084
01:10:17,950 --> 01:10:19,530
of this funny

1085
01:10:19,540 --> 01:10:23,640
exponential loss function which is e to the minus y times at the back side

1086
01:10:23,640 --> 01:10:28,170
where have backs again is this weighted sum of the weak classifiers

1087
01:10:28,190 --> 01:10:30,630
OK so the claim is that this

1088
01:10:30,660 --> 01:10:32,170
exponential loss

1089
01:10:32,320 --> 01:10:36,300
the natural loss function associated with adaboost

1090
01:10:36,320 --> 01:10:38,080
so what is that really mean well

1091
01:10:38,100 --> 01:10:41,860
it turns out that on every round of boosting with adaboost is doing is it's

1092
01:10:41,860 --> 01:10:43,840
greedily choosing

1093
01:10:43,900 --> 01:10:45,200
alpha t

1094
01:10:45,220 --> 01:10:47,880
and the weak classifier ht

1095
01:10:47,890 --> 01:10:50,810
so as to minimize the loss function

1096
01:10:50,820 --> 01:10:54,220
it turns out that adaboost is really kind of the greedy procedure

1097
01:10:54,230 --> 01:10:58,110
for minimizing this loss function

1098
01:10:58,120 --> 01:10:59,720
so then you might ask

1099
01:10:59,720 --> 01:11:03,050
i will not go through your body x-rays may go through your body gonna rays

1100
01:11:03,190 --> 01:11:08,010
funny but infrared radiation and visible light gets absorbed by your body and so the

1101
01:11:08,010 --> 01:11:10,660
question now is real that time you

1102
01:11:10,700 --> 01:11:14,880
and then the answer is no you would hardly notice fifteen watts per square meter

1103
01:11:14,910 --> 01:11:16,600
your body itself

1104
01:11:16,610 --> 01:11:22,210
radiates about hundred joules per second because of the body heats that you have

1105
01:11:22,270 --> 01:11:24,280
and so the thirteen what's

1106
01:11:25,460 --> 01:11:27,140
the square meter that

1107
01:11:27,150 --> 01:11:31,590
you have your that's a uris cross sectional area is about one square meter just

1108
01:11:31,590 --> 01:11:32,680
to round it off

1109
01:11:32,720 --> 01:11:36,500
so that means that in joules per second would be observed by your body

1110
01:11:36,550 --> 01:11:37,800
and that will not

1111
01:11:37,810 --> 01:11:40,450
factor you in any way

1112
01:11:40,560 --> 01:11:42,730
but let's now take a situation

1113
01:11:43,570 --> 01:11:44,560
we have

1114
01:11:44,570 --> 01:11:46,300
he is zero

1115
01:11:46,330 --> 01:11:48,730
equal ten times higher

1116
01:11:48,750 --> 01:11:51,670
which is the thousand volts per meter

1117
01:11:51,680 --> 01:11:53,830
now the poynting vector

1118
01:11:53,890 --> 01:11:56,130
goes up by the square of the

1119
01:11:56,200 --> 01:11:57,730
so now you're going to get

1120
01:11:57,760 --> 01:11:59,230
that the poynting

1121
01:11:59,290 --> 01:12:02,590
i mean we factor is one point three

1122
01:12:02,660 --> 01:12:04,680
kilowatts per square metre

1123
01:12:04,720 --> 01:12:07,230
and that will fry

1124
01:12:07,310 --> 01:12:08,550
if you walk

1125
01:12:08,560 --> 01:12:09,560
make it

1126
01:12:09,570 --> 01:12:12,270
in the field infrared optical

1127
01:12:12,310 --> 01:12:13,550
well by the

1128
01:12:14,700 --> 01:12:17,410
one point three kilowatts per square metre

1129
01:12:17,420 --> 01:12:19,470
that is very dangerous

1130
01:12:19,530 --> 01:12:20,480
it gets

1131
01:12:20,640 --> 01:12:23,520
skin cancer and worse

1132
01:12:23,680 --> 01:12:27,330
now why do i mention that and why do i focus on that one point

1133
01:12:28,810 --> 01:12:30,810
kilowatts per square metre

1134
01:12:30,820 --> 01:12:35,470
because that's exactly what the sun does

1135
01:12:35,530 --> 01:12:36,650
there's the song

1136
01:12:36,660 --> 01:12:40,650
anderson is a rather powerful light bulb

1137
01:12:40,680 --> 01:12:42,230
about three point nine

1138
01:12:42,250 --> 01:12:45,200
times ten to the twenty six watts

1139
01:12:45,210 --> 01:12:48,060
and this is where you are on earth

1140
01:12:48,070 --> 01:12:50,020
and the distance to earth

1141
01:12:50,070 --> 01:12:51,650
is hundred fifty

1142
01:12:51,670 --> 01:12:55,440
million kilometres

1143
01:12:55,500 --> 01:12:58,310
so i can calculate now

1144
01:12:58,420 --> 01:13:00,480
how many joules per second

1145
01:13:00,540 --> 01:13:05,440
go through one square meter here and this one square metres held perpendicular

1146
01:13:05,450 --> 01:13:08,800
two the direction to the sun

1147
01:13:08,810 --> 01:13:10,480
so that

1148
01:13:10,550 --> 01:13:12,680
value for us

1149
01:13:12,680 --> 01:13:13,670
it is of course

1150
01:13:13,680 --> 01:13:17,170
the radiation that leaves attended my three point nine

1151
01:13:17,210 --> 01:13:20,440
times ten to the twenty six

1152
01:13:20,450 --> 01:13:23,930
and now i have to divided by the entire surface area

1153
01:13:23,940 --> 01:13:25,530
of the sphere

1154
01:13:25,550 --> 01:13:27,550
it goes in all directions

1155
01:13:27,550 --> 01:13:29,230
that has this rate is

1156
01:13:29,270 --> 01:13:30,600
and that surface area

1157
01:13:30,670 --> 01:13:32,050
is four pi

1158
01:13:32,060 --> 01:13:33,860
times the radius square

1159
01:13:33,880 --> 01:13:35,530
so is hundred fifty

1160
01:13:35,540 --> 01:13:39,170
times ten to the ninth because i have to go to MKS units

1161
01:13:39,180 --> 01:13:41,700
it's great

1162
01:13:41,760 --> 01:13:44,790
and that is now the number of watts per square meter

1163
01:13:45,720 --> 01:13:48,380
we receive at earth from the sun

1164
01:13:48,390 --> 01:13:49,640
and that number

1165
01:13:49,660 --> 01:13:52,940
it's a very famous number that is one point four

1166
01:13:54,050 --> 01:13:55,650
per square metre

1167
01:13:55,660 --> 01:13:58,610
and and that's why i picked the one point three show you

1168
01:13:58,670 --> 01:14:00,960
that if you walk around on the beach

1169
01:14:01,020 --> 01:14:02,970
you don't take care of yourself

1170
01:14:02,980 --> 01:14:04,610
if you do that too long

1171
01:14:04,670 --> 01:14:09,600
that is very dangerous because your body absorbs that

1172
01:14:09,610 --> 01:14:12,710
this number is called the solar

1173
01:14:13,940 --> 01:14:19,930
it has major implications of course for people who want to harvest solar energy

1174
01:14:19,980 --> 01:14:22,550
the maximum that you can ever harvest

1175
01:14:23,470 --> 01:14:26,920
for every square meter to dedicate to solar energy

1176
01:14:26,930 --> 01:14:28,290
you only get

1177
01:14:28,300 --> 01:14:32,600
fourteen hundred joules per second you can ever get more

1178
01:14:32,660 --> 01:14:37,560
the electric power capacity of the united states is seven

1179
01:14:37,570 --> 01:14:40,720
a hundred thousand megawatts

1180
01:14:40,730 --> 01:14:44,130
for which you need seven hundred powerplants

1181
01:14:44,170 --> 01:14:45,980
a full-size power plant

1182
01:14:46,010 --> 01:14:48,850
it's about a thousand megawatt power plant

1183
01:14:48,930 --> 01:14:51,960
we need seven hundred of these power plants

1184
01:14:51,970 --> 01:14:53,750
to have the capacity

1185
01:14:53,800 --> 01:14:55,300
forty united states

1186
01:14:55,310 --> 01:14:56,730
we are energy

1187
01:14:58,310 --> 01:15:02,790
we consume more than one quarter of all the energy in the world

1188
01:15:02,830 --> 01:15:05,700
so if you want to replace by solar energy

1189
01:15:05,710 --> 01:15:07,670
that's a major problem

1190
01:15:07,680 --> 01:15:09,600
because you can only get

1191
01:15:09,610 --> 01:15:14,390
one point four kilometres per second for every square meter you can calculate

1192
01:15:14,470 --> 01:15:18,890
how many hundreds of square miles in the desert you would have to

1193
01:15:19,840 --> 01:15:22,650
with solar cells which are extremely expensive

1194
01:15:22,660 --> 01:15:25,040
in order to get electricity

1195
01:15:25,070 --> 01:15:27,600
and the efficiency of course is never

1196
01:15:27,650 --> 01:15:31,100
one hundred percent and also when the sun is low on the horizon then you

1197
01:15:31,100 --> 01:15:33,460
don't have this one square metre

1198
01:15:33,480 --> 01:15:36,800
perpendicular to the direction of the sun so all of that has to be taken

1199
01:15:36,800 --> 01:15:38,160
into account

1200
01:15:38,170 --> 01:15:39,780
so energy

1201
01:15:39,790 --> 01:15:44,500
it is not very important in our lives unfortunately

1202
01:15:44,550 --> 01:15:45,930
and this is the number

1203
01:15:45,940 --> 01:15:49,360
that is the ultimate limit

1204
01:15:49,500 --> 01:15:51,400
now comes the question

1205
01:15:51,480 --> 01:15:53,680
is there such a thing

1206
01:15:53,690 --> 01:15:57,530
as a electric field of a thousand volts per meter

1207
01:15:57,540 --> 01:15:59,330
in the solar radiation

1208
01:15:59,340 --> 01:16:00,950
could we actually measure

1209
01:16:00,960 --> 01:16:02,310
the electric field

1210
01:16:02,320 --> 01:16:06,380
just from the radiation of the sun and the answer is no

1211
01:16:06,430 --> 01:16:10,080
and the reason is that the radiation is not really in the four

1212
01:16:10,090 --> 01:16:11,950
of our idealised

1213
01:16:11,960 --> 01:16:16,180
plane wave but more important than anything else is that there is no such thing

1214
01:16:16,470 --> 01:16:19,510
as as just one away from the sun

1215
01:16:22,170 --> 01:16:27,380
has the amplitude of thousand volts per meter in fact the radiation reaches us in

1216
01:16:27,380 --> 01:16:29,280
small packages

1217
01:16:29,320 --> 01:16:32,930
broken up into pieces so to speak

1218
01:16:32,980 --> 01:16:36,060
however since the energy flow

1219
01:16:36,080 --> 01:16:37,020
it is

1220
01:16:37,030 --> 01:16:39,520
one point four

1221
01:16:39,550 --> 01:16:41,970
kilowatts per square metre

1222
01:16:42,000 --> 01:16:46,630
it is perfectly OK with me that you refer to this number as the poynting

1223
01:16:46,630 --> 01:16:48,830
vector i have no problem with that

1224
01:16:48,920 --> 01:16:50,850
but it is a little bit naive

1225
01:16:50,900 --> 01:16:52,930
to associate with that

1226
01:16:52,930 --> 01:16:55,590
at different intensities

1227
01:16:55,600 --> 01:17:00,750
that would be interesting to which can you see why the band gap in 1

1228
01:17:00,750 --> 01:17:04,120
. 1 EV versus visible light is really important

1229
01:17:04,710 --> 01:17:08,710
how well this 1

1230
01:17:08,730 --> 01:17:13,470
what if instead of having light coming in 1 I connected this to a power

1231
01:17:14,640 --> 01:17:16,790
and I

1232
01:17:16,840 --> 01:17:22,660
part the electrons out of the valence band in the conduction band from which they

1233
01:17:22,660 --> 01:17:24,570
would fall

1234
01:17:24,570 --> 01:17:27,210
and then emit photons

1235
01:17:27,230 --> 01:17:33,620
so I could make a light-emitting device by pumping things up and having fall this

1236
01:17:33,620 --> 01:17:39,660
is why this class of materials is dominating the modern world materials with the band

1237
01:17:40,810 --> 01:17:43,340
around the value of the

1238
01:17:43,360 --> 01:17:51,470
energy in visible light are called semiconductor per cent for a reason you'll see shortly

1239
01:17:51,470 --> 01:17:57,790
in terms of their connectivity their electrical conductivity is the least appealing of the properties

1240
01:17:57,810 --> 01:18:01,320
it's their ability to interact with

1241
01:18:01,530 --> 01:18:03,460
radiation makes them so

1242
01:18:03,660 --> 01:18:06,420
so exciting

1243
01:18:06,670 --> 01:18:11,840
national we could actually do a task we could we have actually measure measure with

1244
01:18:11,840 --> 01:18:19,510
these band gaps are by tuning the value of the incident radiation so for example

1245
01:18:19,580 --> 01:18:21,070
if we did this with

1246
01:18:21,330 --> 01:18:22,990
long-wave radiation

1247
01:18:23,570 --> 01:18:28,770
keep moving moving moving until we get to a value of photon energy that causes

1248
01:18:28,770 --> 01:18:35,430
expectations we could have a sensor that detects when the incident radiation is being absorbed

1249
01:18:35,670 --> 01:18:41,950
and we can plot that energy supply so we can have sustainable of the incident

1250
01:18:41,950 --> 01:18:53,450
energy when the incident energy and this is the 1st absorption the percent absorption

1251
01:18:54,640 --> 01:19:03,320
by the crystal and this would from 0 to 100 so energy moves from right

1252
01:19:03,320 --> 01:19:08,580
to left it goes inversely with Llandysul so high wavelength is low energy so at

1253
01:19:08,580 --> 01:19:17,250
low energy we expect materials to be transparent at high energy label at some critical

1254
01:19:18,250 --> 01:19:28,490
of energy absorption begins and this value here is called the absorption and the absorption

1255
01:19:28,530 --> 01:19:30,840
so I'm here very very high

1256
01:19:30,860 --> 01:19:40,640
wavelengths are low energies the material that appears transparent transparent because the incident is less

1257
01:19:40,640 --> 01:19:51,290
than the band gap and a high energies the material appears OK it's absorbing

1258
01:19:51,290 --> 01:19:57,510
because here the incident is greater than the sum of the

1259
01:20:00,320 --> 01:20:08,710
and surrounded series due to defects that it's not perfect shop so perfectly shopping search

1260
01:20:09,270 --> 01:20:14,300
and so what I did OK let's do the calculation Cycle and Carriage silicon and

1261
01:20:14,300 --> 01:20:22,510
diamond silicon diamond so the band band gaps and electron volts for silicon at 1

1262
01:20:22,510 --> 01:20:28,990
. 1 4 . 5 . 4 and ordered is just said well band gap

1263
01:20:28,990 --> 01:20:34,350
energy is now converted to photon energy so just do HC overland so of the

1264
01:20:34,350 --> 01:20:40,450
absorption edge land of the absorption edge this is equal to the

1265
01:20:41,680 --> 01:20:47,680
band gap is equal to h c over lambda of the absorption edge for silicon

1266
01:20:47,680 --> 01:20:55,360
it's way infrared it's 1125 use these ugly units mm and diamond is down at

1267
01:20:57,930 --> 01:21:00,210
and words visible light

1268
01:21:00,250 --> 01:21:07,750
visible light is around numbers 400 to 700 manages to you can see that this

1269
01:21:07,750 --> 01:21:12,990
is out I and this is down in the UV

1270
01:21:13,050 --> 01:21:18,470
so we could make a plot here that this is 400 nanometers this is 700

1271
01:21:18,470 --> 01:21:21,380
nanometers so this is visible light

1272
01:21:21,480 --> 01:21:26,940
then this would be silicon out here at 1125 mm

1273
01:21:27,290 --> 01:21:39,170
and the diamond diamond over here that 229 manometers invisible library between

1274
01:21:39,310 --> 01:21:44,680
and I thought well there's just probably another chemistry lesson here self

1275
01:21:44,810 --> 01:21:49,160
1st what to summarize here what we see we've seen here that the metal has

1276
01:21:49,160 --> 01:21:53,790
no band gap so there's there's no gap between the top of the bonding in

1277
01:21:53,790 --> 01:21:58,070
the bottom of the anti bonding orbitals in an insulated there's a huge band gap

1278
01:21:58,490 --> 01:22:03,880
in the semiconductor there is a moderate band gap were moderate is defined literally in

1279
01:22:03,880 --> 01:22:06,600
reference to visible light

1280
01:22:06,620 --> 01:22:12,530
now this was an interesting thing I've here's here's your group 42 us the periodic

1281
01:22:12,530 --> 01:22:17,310
table and these are the values of band gap energy of Amari showed you that

1282
01:22:17,310 --> 01:22:19,290
Diamond is 5 . 4

1283
01:22:19,330 --> 01:22:23,770
and silicon is 1 . 1 and again would now germanium is going out to

1284
01:22:24,290 --> 01:22:28,070
n equals 4 so so that it's still S P 3 hybridized you this is

1285
01:22:28,070 --> 01:22:31,030
forced to 4 p 2 forms for

1286
01:22:31,620 --> 01:22:34,280
SP three hybrid orbitals

1287
01:22:34,820 --> 01:22:41,310
0 . 7 to 10 it may surprise you to has 2 different crystallographic forms

1288
01:22:41,330 --> 01:22:46,110
called allah tropes 1 of them was metallic and the other 1 is s p

1289
01:22:46,110 --> 01:22:52,750
3 hybridised so to actually has a semiconducting form it's a tiny tiny band gap

1290
01:22:52,750 --> 01:22:56,400
the stochastic process namely the random variable t

1291
01:22:56,410 --> 01:22:58,280
as a function of x and y

1292
01:22:58,340 --> 01:23:01,950
it is not a random variable of data

1293
01:23:01,960 --> 01:23:08,220
and i can ask well what's the distribution of the stochastic process

1294
01:23:08,240 --> 01:23:11,090
two fixed indexed by x and y

1295
01:23:11,150 --> 01:23:16,160
well obviously since this is calcium that also has to be garcia

1296
01:23:16,210 --> 01:23:18,230
furthermore the mean vanishes

1297
01:23:18,250 --> 01:23:20,820
can just pull the expectation in

1298
01:23:22,050 --> 01:23:23,320
the covariance

1299
01:23:23,330 --> 01:23:27,220
well the current duration y you explain what prime

1300
01:23:27,220 --> 01:23:30,120
well this is the expectation of o theta

1301
01:23:30,260 --> 01:23:33,170
o for x and y with theta

1302
01:23:33,180 --> 01:23:36,790
theta was far explaining why prime

1303
01:23:39,700 --> 01:23:42,040
this debate here doesn't depend on theta

1304
01:23:42,140 --> 01:23:44,180
notice that it

1305
01:23:44,270 --> 01:23:47,760
so i can pull the expectations on

1306
01:23:47,780 --> 01:23:51,970
so the expectation of state is the transpose

1307
01:23:52,010 --> 01:23:53,590
just to correct

1308
01:23:53,640 --> 01:23:58,120
now this is just six months with setting

1309
01:23:58,150 --> 01:24:02,330
so this all collapse and just getting up from twenty five x y

1310
01:24:02,420 --> 01:24:05,330
for fixed right

1311
01:24:05,680 --> 01:24:09,500
so what i've done is i've rewrite yet another way

1312
01:24:09,520 --> 01:24:12,190
of how to get

1313
01:24:12,190 --> 01:24:14,320
the second

1314
01:24:14,410 --> 01:24:23,180
just go back

1315
01:24:25,830 --> 01:24:29,860
this was the kind

1316
01:24:31,560 --> 01:24:34,350
well now sits well we could just as well

1317
01:24:36,840 --> 01:24:40,220
with those terms

1318
01:24:40,230 --> 01:24:41,800
not trying to normal

1319
01:24:41,830 --> 01:24:44,410
with this as the covariance matrix

1320
01:24:44,460 --> 01:24:47,700
and all the estimation straight in that

1321
01:24:47,820 --> 01:24:51,740
in some cases this is going to be computationally more efficient

1322
01:24:51,750 --> 01:24:54,660
for instance whenever the inverse covariance matrix

1323
01:24:54,670 --> 01:24:59,080
it's something that you have easy access to the information matrix itself isn't

1324
01:24:59,100 --> 01:25:00,720
and that's the smart thing to do

1325
01:25:00,730 --> 01:25:06,540
can be cases where for instance you take as a graph the graph passing as

1326
01:25:06,590 --> 01:25:10,010
inverse covariance matrix or some modification of that

1327
01:25:10,860 --> 01:25:12,920
operating dusty parameters

1328
01:25:12,920 --> 01:25:16,790
can speed up things by orders of magnitude

1329
01:25:16,850 --> 01:25:19,220
but that's a numerical thing

1330
01:25:19,230 --> 01:25:23,010
in terms of statistics is that it's exactly the same thing happening so that in

1331
01:25:23,010 --> 01:25:23,740
one case

1332
01:25:23,920 --> 01:25:26,160
you might the office

1333
01:25:26,410 --> 01:25:33,920
in that case you optimize over t it was k times of

1334
01:25:33,940 --> 01:25:36,490
so if you're casting process paper

1335
01:25:36,540 --> 01:25:38,540
we talk about this

1336
01:25:38,590 --> 01:25:41,490
you know what it means

1337
01:25:41,530 --> 01:25:44,440
it is all confused you now you can just forget about it because i'm not

1338
01:25:44,440 --> 01:25:46,760
going to use it afterwards

1339
01:25:46,870 --> 01:25:50,710
but next time you read about process paper in this country you pull out that

1340
01:25:50,710 --> 01:25:52,850
slide number nine

1341
01:25:52,890 --> 01:25:58,420
and just use it to translate things

1342
01:25:58,450 --> 01:26:00,150
right now let's look at a couple of

1343
01:26:00,190 --> 01:26:01,900
simple cases

1344
01:26:01,950 --> 01:26:05,530
so don't be scared fifty five slides on that many with lots of pictures in

1345
01:26:07,770 --> 01:26:09,350
and here

1346
01:26:10,520 --> 01:26:13,320
well if i have a linear also kx

1347
01:26:13,340 --> 01:26:15,550
the next prime is just

1348
01:26:15,600 --> 01:26:17,800
it's time that's fine

1349
01:26:17,980 --> 01:26:21,600
so this what has nothing to do with the previous slide in world is just

1350
01:26:21,600 --> 01:26:23,440
the linear covariance function

1351
01:26:23,450 --> 01:26:28,070
pretty boring but this will give me a linear models

1352
01:26:28,100 --> 01:26:31,540
but the people in economics use that

1353
01:26:31,560 --> 01:26:39,180
the classes so that would be just e to the minus explore exploit

1354
01:26:39,180 --> 01:26:42,590
the jean-philippe show you that last week

1355
01:26:45,340 --> 01:26:47,250
everybody investing before

1356
01:26:49,010 --> 01:26:51,290
the kernel of order three

1357
01:26:51,310 --> 01:26:54,730
looks a bit weird but it's also kind of

1358
01:26:54,740 --> 01:26:58,390
so done is space fixed one argument and but that's the value of the other

1359
01:26:58,390 --> 01:27:00,510
ones throughout the entire range

1360
01:27:00,680 --> 01:27:05,060
what you would probably notice that these polynomial kernels can be hard to control at

1361
01:27:06,010 --> 01:27:09,670
it's the values can become very large

1362
01:27:09,740 --> 01:27:12,250
if you don't risk scared about in the right way

1363
01:27:12,300 --> 01:27:14,730
this is a risky thing to do

1364
01:27:14,800 --> 01:27:16,210
to you get new

1365
01:27:17,710 --> 01:27:22,250
it's probably a good idea not to use the polynomial kernel in the first instance

1366
01:27:22,270 --> 01:27:26,360
just use a simple RBF kernel trying to see how well how are you doing

1367
01:27:26,420 --> 01:27:29,540
and then maybe look at polynomial kernels

1368
01:27:29,560 --> 01:27:32,190
i'm not saying that they might not really work well

1369
01:27:32,230 --> 01:27:35,750
so for instance for optical character recognition that great

1370
01:27:35,770 --> 01:27:38,970
four strings they can be really great

1371
01:27:39,010 --> 01:27:40,190
in comparison

1372
01:27:40,200 --> 01:27:41,080
so could not

1373
01:27:41,190 --> 01:27:43,800
might be able to tell you a lot more about that

1374
01:27:46,220 --> 01:27:48,540
plates a first use an RBF kernel

1375
01:27:48,560 --> 01:27:49,760
you will be

1376
01:27:49,770 --> 01:27:51,290
it say

1377
01:27:52,610 --> 01:27:56,670
if you want to get support you can use something like lines

1378
01:27:56,710 --> 01:27:58,260
not just here

1379
01:27:58,280 --> 01:28:01,920
and that's just multiple convolutions of the unit interval

1380
01:28:02,210 --> 01:28:05,890
if somebody goes to you tells you well my ten kernel is really what you

1381
01:28:05,890 --> 01:28:07,720
an engineer

1382
01:28:09,930 --> 01:28:15,110
they can be seen as a sort mathematical formalisation very simple evolving systems and we

1383
01:28:15,110 --> 01:28:17,870
talk a little bit more about that tomorrow

1384
01:28:18,180 --> 01:28:24,190
i'm going to present is very highly prestigious you of what's important to make sure

1385
01:28:24,310 --> 01:28:27,320
work OK so this is what i believe

1386
01:28:27,330 --> 01:28:32,920
are important to making hinges were other people would have would tell you very different

1387
01:28:33,720 --> 01:28:40,440
but it's my lectures i can tell you what i want because here is a

1388
01:28:40,490 --> 01:28:42,540
can only called g a

1389
01:28:42,630 --> 01:28:45,310
initialize population

1390
01:28:45,320 --> 01:28:46,740
the sun

1391
01:28:46,780 --> 01:28:50,760
period of time you evaluate the fitness you selected

1392
01:28:51,170 --> 01:28:57,880
a new population based on the thickness you need to members of population you to

1393
01:28:57,890 --> 01:29:03,530
cross over and he returned the best member of the population of the sometimes there's

1394
01:29:03,530 --> 01:29:08,490
lots of variants on this ritual talk about some of those later

1395
01:29:08,500 --> 01:29:11,260
so let me give you a very quick example of this is this is the

1396
01:29:11,290 --> 01:29:14,500
graph coloring problem to a given a graph

1397
01:29:14,620 --> 01:29:20,120
chris evert citizens images what you want to do is you want to call it

1398
01:29:20,130 --> 01:29:24,550
the graph you want to sign a colour to each of the vertices

1399
01:29:24,710 --> 01:29:30,120
to minimize the number of edges with the same colour differences

1400
01:29:30,130 --> 01:29:34,480
so these are called colour conflicts this is a perfectly coloured graph because on the

1401
01:29:35,500 --> 01:29:43,890
complex and how did that produces produces by first occurrence the

1402
01:29:43,940 --> 01:29:47,700
i do culling that season and putting in the edges of i didn't get any

1403
01:29:47,700 --> 01:29:48,660
colour conflicts so

1404
01:29:49,050 --> 01:29:55,660
i cheated but wish to see algorithms to to do this but then g

1405
01:30:00,300 --> 01:30:07,560
let's take the g g eight in slow motion so you firstly just generate a

1406
01:30:07,560 --> 01:30:11,330
big population and and often people use randomized

1407
01:30:11,350 --> 01:30:16,740
colourings you could think of having a more intelligent way of

1408
01:30:16,810 --> 01:30:24,230
forming a new population there's some advantages of just using around population is it gives

1409
01:30:24,230 --> 01:30:31,530
you a very diverse population to start off with so these are just randomly coloured

1410
01:30:34,380 --> 01:30:37,140
and then you have to

1411
01:30:37,860 --> 01:30:39,380
evaluate the fitness

1412
01:30:39,430 --> 01:30:43,360
see you take a graph and you can

1413
01:30:43,560 --> 01:30:45,490
the number of

1414
01:30:46,280 --> 01:30:47,930
edges where

1415
01:30:47,940 --> 01:30:50,930
you have third

1416
01:30:50,940 --> 01:30:54,930
for all the same colour in this case the cost is a

1417
01:30:55,040 --> 01:30:57,630
when you select a

1418
01:30:57,660 --> 01:31:01,010
new population

1419
01:31:01,130 --> 01:31:08,560
but using preferentially fit members one way of doing that is you signed some kind

1420
01:31:08,560 --> 01:31:10,130
of weight

1421
01:31:13,610 --> 01:31:20,040
so far this is actually to do a technically in biological speak fitness is a

1422
01:31:20,040 --> 01:31:25,580
measure of the number of offspring you compute potentially

1423
01:31:25,590 --> 01:31:27,230
potentially produce

1424
01:31:27,290 --> 01:31:33,560
and this is kind of use slightly ambiguously in an evolution in GA's because people

1425
01:31:33,560 --> 01:31:35,000
actually just talk about

1426
01:31:35,170 --> 01:31:42,410
fitness is can any measure of how fit you are of one to be strictly

1427
01:31:42,410 --> 01:31:45,220
proportional to the number of children you produced

1428
01:31:47,700 --> 01:31:50,810
so when you have some probability to do with

1429
01:31:50,980 --> 01:31:53,230
your fitness

1430
01:31:53,430 --> 01:31:56,060
and then you select

1431
01:31:56,100 --> 01:31:58,800
from this probability

1432
01:31:59,060 --> 01:32:05,330
distribution so there's many ways of doing this election there's many ways of going from

1433
01:32:07,330 --> 01:32:09,560
belief in how

1434
01:32:10,660 --> 01:32:14,180
your members of the population are

1435
01:32:14,520 --> 01:32:18,860
to producing this mission measure in some of these are better than others and the

1436
01:32:18,860 --> 01:32:20,150
talk very

1437
01:32:20,160 --> 01:32:23,310
briefly about how they define what they are

1438
01:32:25,010 --> 01:32:29,560
mutation you basically change account of one or more of the vertices of we have

1439
01:32:29,720 --> 01:32:33,470
a problem like this we choose

1440
01:32:33,500 --> 01:32:35,550
the third season random

1441
01:32:35,560 --> 01:32:38,270
so which is this one and we just change

1442
01:32:38,680 --> 01:32:39,850
its colour

1443
01:32:39,880 --> 01:32:43,060
OK that may or may not increase

1444
01:32:43,080 --> 01:32:48,420
the cost in this case it increases the cost of decreased the cost were trying

1445
01:32:48,420 --> 01:32:51,670
to reduce the cost

1446
01:32:51,680 --> 01:32:52,630
i could

1447
01:32:52,650 --> 01:32:55,750
two mutations involve more than one

1448
01:32:57,510 --> 01:32:59,280
mentioned more than once

1449
01:33:00,430 --> 01:33:02,350
so it changes colors

1450
01:33:04,210 --> 01:33:06,610
one of them was one of those

1451
01:33:08,730 --> 01:33:10,580
what happens

1452
01:33:10,590 --> 01:33:18,660
one the increases it increases a sort of like you can present wrong but

1453
01:33:18,680 --> 01:33:20,890
that's so in that case

1454
01:33:21,110 --> 01:33:25,560
i just actually end up swapping

1455
01:33:25,690 --> 01:33:30,930
when my colour complexes so as to solve blind imitation

1456
01:33:30,930 --> 01:33:39,180
and in crossover ideas i take two members of my population ages like those random

1457
01:33:39,180 --> 01:33:42,380
are pair of members of the population random

1458
01:33:42,390 --> 01:33:48,310
and then i take one half of one and the other half of the

1459
01:33:51,300 --> 01:33:53,700
in this case

1460
01:33:53,760 --> 01:33:58,180
i've taken this half of this one and this half that one third of like

1461
01:34:01,420 --> 01:34:05,900
i can also produced to complement of that we take this half of this one

1462
01:34:05,900 --> 01:34:10,880
and this offer of one so often you can do the complement and

1463
01:34:10,890 --> 01:34:14,920
to some advantages to doing both at the same time so you don't lose any

1464
01:34:14,920 --> 01:34:17,800
genetic material

1465
01:34:18,210 --> 01:34:23,280
so is it usually costs associated with crossover

1466
01:34:23,300 --> 01:34:29,270
so here are two graphs are quite low cost graphs

1467
01:34:29,650 --> 01:34:31,840
this one actually

1468
01:34:31,900 --> 01:34:35,800
evolved to find these these are quite low cost graphs

1469
01:34:35,960 --> 01:34:38,930
if i came across some over

1470
01:34:38,930 --> 01:34:45,050
so before crossing a sort of dividing them into how does not really any particular

1471
01:34:45,050 --> 01:34:51,650
instead you could say OK i'm going to divide basically my my my initial

1472
01:34:51,690 --> 01:34:58,070
state vector into some vectors of lower dimensionality on that they these guys want to

1473
01:34:58,140 --> 01:34:59,510
time using say

1474
01:35:00,470 --> 01:35:06,380
initial processing so the same idea this selected jury protocol MCMC idea you bright your

1475
01:35:06,610 --> 01:35:13,010
initial something problem in the sequence of support so for example i could consider exactly

1476
01:35:13,010 --> 01:35:18,670
the same thing OK well basically what i'm doing is that now instead of basically

1477
01:35:18,670 --> 01:35:23,920
proposing sampling adding appalled distribution applied to they all the company

1478
01:35:23,940 --> 01:35:29,170
at the same time at each iteration of my reason i picked world only one

1479
01:35:29,170 --> 01:35:33,880
of the coordinates of the state vector OK so uniformly between and

1480
01:35:33,900 --> 01:35:38,860
in the set one two two nx number the dimension of the problem then i

1481
01:35:38,860 --> 01:35:40,460
propose only

1482
01:35:41,520 --> 01:35:46,640
candidate new value for all of the state components white k on they use the

1483
01:35:46,650 --> 01:35:48,510
same the marginal

1484
01:35:48,520 --> 01:35:54,140
as marginally independent pop although the distribution and the marginal probability distribution on the joint

1485
01:35:54,140 --> 01:35:57,820
choice so this is simply an malden shifted

1486
01:35:58,800 --> 01:36:01,260
on the i accept

1487
01:36:01,280 --> 01:36:03,170
these basically

1488
01:36:03,280 --> 01:36:07,930
local modification of the state vector because i've only modified xk

1489
01:36:07,940 --> 01:36:13,420
using the metropolis hastings acceptance rates OK so the thing i suppose which in this

1490
01:36:13,420 --> 01:36:16,080
case is simply basically

1491
01:36:16,190 --> 01:36:20,650
what i have written before instead of having to raise your proposal all over the

1492
01:36:21,530 --> 01:36:27,770
essentially state space i just put basically the dissolution of the world component of modified

1493
01:36:41,800 --> 01:36:44,800
i'm going to discuss that next like so that's where i grew

1494
01:36:47,030 --> 01:36:51,020
so let's consider this time the case where work on that we discover it doesn't

1495
01:36:51,250 --> 01:36:52,420
agree with so

1496
01:36:52,430 --> 01:36:53,340
you do that

1497
01:36:54,590 --> 01:36:57,330
so this is basically you can compute easily

1498
01:36:58,920 --> 01:37:00,540
acceptance probability

1499
01:37:00,540 --> 01:37:05,920
of your metropolis hastings sampler OK all what you see appearing is that you see

1500
01:37:05,920 --> 01:37:11,400
appearing essentially the conditional distribution of the company in xk given the remaining value the

1501
01:37:11,400 --> 01:37:12,880
components OK

1502
01:37:12,890 --> 01:37:16,560
but i do know x minus k OK this is all the company but the

1503
01:37:16,570 --> 01:37:22,450
company xk time basically the marginal distribution of these get consoles OK

1504
01:37:22,470 --> 01:37:27,890
time to public so what you see is that these things this algorithm is nothing

1505
01:37:27,920 --> 01:37:31,150
but essentially kind of generalisation

1506
01:37:31,170 --> 01:37:32,860
of the gibbs sampler

1507
01:37:32,870 --> 01:37:36,890
because if i were to use the proposal distribution

1508
01:37:36,910 --> 01:37:41,040
for all y k the the conditional distribution company and j

1509
01:37:41,080 --> 01:37:42,790
given the remaining

1510
01:37:42,810 --> 01:37:46,830
the other component i would have an exit polls probability of one

1511
01:37:46,860 --> 01:37:50,190
so these things actually when you why did you think of what you've been doing

1512
01:37:50,190 --> 01:37:56,740
is nothing but actually a generalisation of what upsampling does except that instead of dating

1513
01:37:56,740 --> 01:38:02,310
component xk k according to the full conditional distribution you have more freedom and can

1514
01:38:02,310 --> 01:38:04,290
use any proposal distribution

1515
01:38:04,310 --> 01:38:08,260
OK so if you do it it seems that we don't actually quite well because

1516
01:38:08,260 --> 01:38:10,720
essentially using the same proposal

1517
01:38:10,740 --> 01:38:15,110
the distribution of the the case when i was trying to update essentially everybody at

1518
01:38:15,120 --> 01:38:18,800
the same time so i got the same structure full proposals

1519
01:38:18,810 --> 01:38:22,830
but i have data i do the update one at the time the acceptance right

1520
01:38:22,840 --> 01:38:26,960
now is becoming obvious fifty percent on in the sense that basically you can check

1521
01:38:26,960 --> 01:38:32,860
some are approximated europe you you actually good approximation of the target distributions like earth

1522
01:38:32,960 --> 01:38:36,330
could tell you if you had sample exactly from the y target you would have

1523
01:38:36,330 --> 01:38:37,690
equal to zero

1524
01:38:37,740 --> 01:38:42,790
someone please quite close to zero when the two bodies look so that seems that

1525
01:38:43,950 --> 01:38:48,580
in a sense we have one small decided if you the initial state vector intercept

1526
01:38:48,580 --> 01:38:52,530
component update them one at a time you might well OK because you actually have

1527
01:38:52,540 --> 01:38:54,570
much you kind of

1528
01:38:55,710 --> 01:39:00,700
acceptance rates so doesn't suffer from the curse of dimensionality and they don't infinity sorry

1529
01:39:01,420 --> 01:39:06,860
well well well that's not quite true as pointed out by one of your colleagues

1530
01:39:07,240 --> 01:39:11,080
because while he was working so well what i've been doing before

1531
01:39:11,160 --> 01:39:15,160
is because under the target distribution i was considering before

1532
01:39:15,170 --> 01:39:20,700
basically the components where independent from each other the target was factorizing so that's social

1533
01:39:20,710 --> 01:39:24,900
really simple problem actually not considered that basically

1534
01:39:24,910 --> 01:39:27,040
you have basically that

1535
01:39:27,060 --> 01:39:30,070
and the target distribution you have correlation

1536
01:39:30,740 --> 01:39:35,250
the company correlated clients basically which is sigma

1537
01:39:36,030 --> 01:39:38,050
then if you tried to

1538
01:39:38,120 --> 01:39:39,870
use basically

1539
01:39:39,890 --> 01:39:44,690
the same type of strategy so use the try to they all the guys are

1540
01:39:46,400 --> 01:39:48,500
well you find all that

1541
01:39:48,530 --> 01:39:52,600
essentially you can have an exit polls which is as bad as initial case not

1542
01:39:52,600 --> 01:39:58,440
surprising OK on this quantity which should be call approximated to the all well sampling

1543
01:39:58,830 --> 01:40:03,840
quality from this community via target distribution with zero mean he's like twenty eight so

1544
01:40:03,850 --> 01:40:06,690
you're really really OK

1545
01:40:06,710 --> 01:40:10,640
now if you try basically to the one at the time and date

1546
01:40:10,640 --> 01:40:13,470
you've acceptance rate which indeed

1547
01:40:13,490 --> 01:40:19,080
marginally you seem that basically algorithm is quite well because the acceptance rate of forty

1548
01:40:19,080 --> 01:40:23,790
percent because you when your data components one at the time but it's actually a

1549
01:40:23,790 --> 01:40:28,090
really bad on one one small why is it so bad it so bad because

1550
01:40:28,100 --> 01:40:33,070
actually what you're trying to do is that you have essentially i'm unsure vector which

1551
01:40:33,070 --> 01:40:38,470
can i where you can have extremely i'd like a large correlation between companies which

1552
01:40:38,730 --> 01:40:40,490
dated individually

1553
01:40:40,510 --> 01:40:47,490
so basically the algorithm gets stuck in local maxima even in this case word community

1554
01:40:47,490 --> 01:40:51,900
wide ocean distribution itself that that's actually

1555
01:40:51,910 --> 01:40:54,810
so more not quite good OK so

1556
01:40:54,830 --> 01:40:56,070
some all

1557
01:40:56,090 --> 01:40:59,060
you know once more just to point out that

1558
01:40:59,070 --> 01:41:04,460
you have to be very careful when using this type of MCMC tiebreaker is OK

1559
01:41:04,460 --> 01:41:08,420
is very flexible you can do this idea of like breaking up the initial vector

1560
01:41:08,430 --> 01:41:16,870
and inflexible some components of they then likes indeed early on i acceptance rate still

1561
01:41:16,870 --> 01:41:22,870
doesn't mean that the algorithm generates generated by mixing markov chain algorithm that makes very

1562
01:41:22,870 --> 01:41:28,090
quickly towards the target distribution because they are potentially really i don't want to be

1563
01:41:28,090 --> 01:41:34,710
a very strong dependency between components which are updated independently from each other termites

1564
01:41:34,730 --> 01:41:39,320
just one small warning about the use of this idea is OK

1565
01:41:41,830 --> 01:41:48,310
that's say a this kind of rejection like independent sampling egories

1566
01:41:48,320 --> 01:41:54,070
so for also form essentially the multicolored valediction says

1567
01:41:54,090 --> 01:41:55,650
you would like to dates

1568
01:41:55,670 --> 01:42:02,220
a lot of simultaneously a lot of value which are highly correlated together simultaneously

1569
01:42:03,190 --> 01:42:07,060
forty very difficult because you cannot exit polls which was very small

1570
01:42:07,880 --> 01:42:11,030
are you can try to obey them independently

1571
01:42:11,050 --> 01:42:14,800
you might have radii exit polls right doesn't mean that is a mix with k

1572
01:42:14,820 --> 01:42:20,270
so that's a bit something about the world and application and basically special case of

1573
01:42:20,270 --> 01:42:24,240
the metropolis algorithm which is actually quite useful particularly

1574
01:42:24,270 --> 01:42:25,820
it's to say well OK

1575
01:42:25,830 --> 01:42:27,750
we talk try to use

1576
01:42:27,820 --> 01:42:33,770
actually the fact that metropolis hastings gave us a bit more flexibility than an independent

1577
01:42:33,770 --> 01:42:37,900
samples so for the time being we considered the case where once you add to

1578
01:42:37,900 --> 01:42:41,940
the current state of the markov chain is x you move accordingly we're just proposing

1579
01:42:41,940 --> 01:42:45,740
and here in the denominator for the probability of the data

1580
01:42:45,800 --> 01:42:51,160
i've i've symbolically written that now is as an integral rather than something that simply

1581
01:42:51,560 --> 01:42:58,160
the thing the normalizes this posterior probability if you integrate over all possible

1582
01:42:58,180 --> 01:43:01,770
sum over all possible hypotheses

1583
01:43:04,540 --> 01:43:06,520
one of the things that you

1584
01:43:06,610 --> 01:43:11,740
will see in this formula that may appear bit odd our

1585
01:43:11,760 --> 01:43:13,000
not entirely

1586
01:43:13,020 --> 01:43:18,230
expected is that we are required to write down what our prior probability of the

1587
01:43:18,230 --> 01:43:20,630
hypothesis is before

1588
01:43:20,640 --> 01:43:22,220
you see the data

1589
01:43:22,240 --> 01:43:25,490
that seems a little strange before you've done the experiment how do you know whether

1590
01:43:25,490 --> 01:43:28,260
or not to higgs boson exists

1591
01:43:28,310 --> 01:43:35,360
well there's a bayesian statistics provides no unique recipe for writing down those prior probabilities

1592
01:43:35,390 --> 01:43:38,700
but there's no escaping them you can not somehow try to sweep them under the

1593
01:43:38,700 --> 01:43:45,040
rubble there are many attempts to do so so these these priors always exists

1594
01:43:45,040 --> 01:43:49,040
and bayes theorem really has a sort of if then character what it says is

1595
01:43:49,040 --> 01:43:54,290
that if your prior probabilities were specified by some pile of age

1596
01:43:54,310 --> 01:43:58,730
then bayes theorem indicates how those probabilities should change

1597
01:43:58,750 --> 01:44:00,560
in the light of the data

1598
01:44:00,580 --> 01:44:03,710
right it says how you're probabilities should be updated

1599
01:44:03,710 --> 01:44:06,040
when you see the outcome of your

1600
01:44:06,060 --> 01:44:07,770
of your measurement

1601
01:44:07,790 --> 01:44:10,060
the bayesian statistics itself gives no

1602
01:44:10,080 --> 01:44:11,310
unique prescription

1603
01:44:11,430 --> 01:44:13,330
for these priors

1604
01:44:13,350 --> 01:44:14,480
there are there are

1605
01:44:14,480 --> 01:44:18,410
that's a long story and i suppose i know nothing about the hypothesis to begin

1606
01:44:18,410 --> 01:44:21,700
with how can i write down priors and there are certain guidelines you can try

1607
01:44:21,700 --> 01:44:28,460
to follow but it's also very difficult aspect of bayesian analysis

1608
01:44:29,330 --> 01:44:31,850
you have ten minutes is

1609
01:44:35,680 --> 01:44:40,390
by that was that was my general comments on those two competing schools of statistics

1610
01:44:40,770 --> 01:44:46,060
now i would like to go on introduce a little more formalism about random variables

1611
01:44:46,080 --> 01:44:48,520
and i think that will bring us up to the top of the hour

1612
01:44:48,540 --> 01:44:56,390
a random variable is simply a numerical characteristic assigned to each element of the sample

1613
01:44:57,350 --> 01:45:02,460
and we can talk about discrete random variables such as the number of events or

1614
01:45:02,460 --> 01:45:07,040
or continuous random variables such as in an energy or momentum

1615
01:45:07,890 --> 01:45:13,500
let's suppose that the outcome of the experiment is some continuous quantity some variable x

1616
01:45:13,520 --> 01:45:19,350
i can ask then what is the probability to find x in some infinitesimal interval

1617
01:45:20,640 --> 01:45:25,230
near the value x o between x and x pos dx

1618
01:45:26,660 --> 01:45:33,060
you can you can argue i think very generally that probability should be proportional

1619
01:45:33,080 --> 01:45:36,790
to the size of the dx so you can imagine that probability has to be

1620
01:45:36,790 --> 01:45:38,830
proportional to dx and whatever

1621
01:45:38,870 --> 01:45:41,980
other function of x i need to multiply

1622
01:45:41,980 --> 01:45:43,890
to make that any quality

1623
01:45:43,910 --> 01:45:48,680
that is defined as the probability density function or PDF

1624
01:45:49,810 --> 01:45:52,100
the PDF f x basically

1625
01:45:52,160 --> 01:45:57,310
telling you what is the probability to find x not exactly

1626
01:45:57,370 --> 01:46:01,790
as specified value but within an infinitesimal intervals

1627
01:46:02,890 --> 01:46:06,930
if x represents the outcome of my observation and i make that observation it has

1628
01:46:06,930 --> 01:46:08,180
to be somewhere

1629
01:46:08,210 --> 01:46:13,750
so if i integrate this probability over all possible values of the variables

1630
01:46:13,750 --> 01:46:15,660
that integral has to be one

1631
01:46:15,680 --> 01:46:20,500
because the probability of the sample space is equal to one

1632
01:46:20,560 --> 01:46:26,460
in other cases if x is discrete and discrete OK suppose that is not continuous

1633
01:46:26,460 --> 01:46:29,770
but it simply has a certain fixed values x of i

1634
01:46:29,770 --> 01:46:33,480
then the corresponding statement would be that the probability of x by

1635
01:46:33,500 --> 01:46:37,580
it's just some numbers and we would call that is not a pdf simply a

1636
01:46:37,580 --> 01:46:43,770
probability sometimes the probability mass function in that case the sum of the probabilities has

1637
01:46:43,770 --> 01:46:45,810
to be equal to one that is to say

1638
01:46:45,850 --> 01:46:52,140
this variable has to take on one of its possible values make the observation

1639
01:46:52,160 --> 01:46:55,200
OK so here is another important related

1640
01:46:56,430 --> 01:47:01,520
the cumulative distribution function very often we want to ask for the probability that the

1641
01:47:01,520 --> 01:47:06,080
observed value will be less than or equal to some value x

1642
01:47:06,100 --> 01:47:11,160
so i would find that simply by integrating the pdf from negative infinity up to

1643
01:47:12,600 --> 01:47:16,730
and if i'm going to use a letter use different letters for the pdf maybe

1644
01:47:16,770 --> 01:47:18,390
f whatever

1645
01:47:18,440 --> 01:47:23,180
if i use a small f for the probability density function then i would typically

1646
01:47:23,180 --> 01:47:24,750
use a large as

1647
01:47:24,750 --> 01:47:27,980
for the cumulative distribution function

1648
01:47:28,020 --> 01:47:30,040
o space which

1649
01:47:34,310 --> 01:47:35,270
OK so

1650
01:47:35,290 --> 01:47:36,790
the cumulative distribution

1651
01:47:36,810 --> 01:47:39,350
so here just illustrate that if that was my

1652
01:47:41,810 --> 01:47:44,520
the interval between any two fixed limits

1653
01:47:44,540 --> 01:47:48,770
of the pdf that gives me the probability to make the observation between those limits

1654
01:47:48,850 --> 01:47:52,960
and if i do that from negative infinity up to some value of x and

1655
01:47:52,960 --> 01:47:56,310
plot the result as a function of x that i get a curve that always

1656
01:47:56,310 --> 01:47:57,770
starts out zero

1657
01:47:57,790 --> 01:48:01,810
and eventually comes up to one that's the cumulative distribution

1658
01:48:01,830 --> 01:48:06,270
in fact mathematicians would typically start by defining the cumulative distribution

1659
01:48:06,290 --> 01:48:10,480
and then saying the pdf is simply its derivative

1660
01:48:10,500 --> 01:48:13,430
that's a little more general in fact as you can imagine there could be cases

1661
01:48:13,430 --> 01:48:15,470
ex-prime equals y

1662
01:48:15,480 --> 01:48:17,660
why primary eagles that

1663
01:48:17,770 --> 01:48:19,800
negative x

1664
01:48:19,870 --> 01:48:26,160
we can write down the solution that directly but if you want to do i

1665
01:48:26,160 --> 01:48:29,360
give values and i vectors the matrix will look like this

1666
01:48:30,200 --> 01:48:32,910
equation will be lambda square

1667
01:48:32,960 --> 01:48:35,200
one zero lambda

1668
01:48:35,240 --> 01:48:40,990
plus one equals zero so the eigen vectors will be plus or minus argue values

1669
01:48:40,990 --> 01:48:42,850
usually plus and minus i

1670
01:48:42,870 --> 01:48:45,910
but in fact and then from then on you can work out of the usual

1671
01:48:45,910 --> 01:48:46,800
way the

1672
01:48:46,830 --> 01:48:49,060
i can vectors but

1673
01:48:49,110 --> 01:48:53,090
complex i vectors separate with the rear look you can avoid all that is by

1674
01:48:53,090 --> 01:48:54,970
writing down the solution

1675
01:48:56,150 --> 01:49:01,980
the solutions are sines and cosines one basic solution will be x equals

1676
01:49:02,010 --> 01:49:07,180
cosine t in which case what is why well why is the derivative that so

1677
01:49:07,180 --> 01:49:09,550
that will be a minus sign

1678
01:49:09,580 --> 01:49:16,360
another basic solution will start with x equals sign e in which case why will

1679
01:49:16,360 --> 01:49:17,960
the cosine t

1680
01:49:17,970 --> 01:49:19,280
it's trivial

1681
01:49:19,410 --> 01:49:25,470
if you do that

1682
01:49:25,560 --> 01:49:28,420
one of these things look like well

1683
01:49:28,470 --> 01:49:32,060
either these two basic solutions looks like a circle

1684
01:49:32,080 --> 01:49:36,870
not traced in the usual way but in the opposite way for example this starts

1685
01:49:36,870 --> 01:49:38,910
when t is equal to zero

1686
01:49:39,420 --> 01:49:43,350
it starts at the point one zero

1687
01:49:43,400 --> 01:49:48,420
now if this the minus sign were there this would be x equals cosine t

1688
01:49:48,420 --> 01:49:53,090
y equals signs which is the usual counter-clockwise circle

1689
01:49:53,100 --> 01:49:57,710
but if i change y from sin t two negative sign t it's going around

1690
01:49:57,710 --> 01:50:00,840
the other way so the circle is traced that way

1691
01:50:00,920 --> 01:50:06,020
and all the other circles this is a family of circles all of which according

1692
01:50:06,020 --> 01:50:08,700
to the values of c one c two concentric

1693
01:50:08,740 --> 01:50:11,480
all of which go around

1694
01:50:14,210 --> 01:50:20,340
so those are close trajectories those solutions the trajectory of the vector field closed

1695
01:50:20,420 --> 01:50:23,710
the come camera about right and they repeat in finite time

1696
01:50:25,120 --> 01:50:26,210
OK now

1697
01:50:26,220 --> 01:50:27,330
these are no good

1698
01:50:27,350 --> 01:50:30,700
is the kind of not interested

1699
01:50:30,710 --> 01:50:34,910
these are commonplace where we're interested in the good stuff today

1700
01:50:34,970 --> 01:50:42,790
and the good stuff we're interested in is limit cycle

1701
01:50:43,050 --> 01:50:46,300
a limit cycle

1702
01:50:46,310 --> 01:50:50,010
it is a closed trajectory with a couple of extra hypothesis

1703
01:50:50,840 --> 01:50:57,810
the closed trajectory just like those guys but has something they don't have mainly it's

1704
01:50:57,810 --> 01:50:59,220
king of the rules

1705
01:50:59,240 --> 01:51:01,590
they have to be isolated

1706
01:51:01,600 --> 01:51:04,450
no other guys nearby

1707
01:51:04,470 --> 01:51:07,680
and they also have to be stable

1708
01:51:10,060 --> 01:51:11,890
so the problem is here

1709
01:51:12,790 --> 01:51:18,150
none of these stands out that these stands out from any of the others

1710
01:51:18,160 --> 01:51:27,930
in other words there must be isolated means no others nearby

1711
01:51:28,120 --> 01:51:35,010
that's just what goes wrong here

1712
01:51:35,030 --> 01:51:39,840
arbitrarily close to each of these circles is yet another circle doing exactly the same

1713
01:51:41,030 --> 01:51:45,700
that means that it did this some there only a mild interest

1714
01:51:45,800 --> 01:51:49,580
what is much more interesting is to find out cycle

1715
01:51:49,590 --> 01:51:54,040
where there is nothing your body

1716
01:51:54,080 --> 01:52:04,980
something therefore the looks like this

1717
01:52:05,770 --> 01:52:08,740
here's our pink i let's make this window

1718
01:52:10,400 --> 01:52:13,360
there's a limit cycle seems to be

1719
01:52:15,680 --> 01:52:18,020
one of the year by guys do

1720
01:52:18,050 --> 01:52:22,380
well they should approach

1721
01:52:22,420 --> 01:52:24,390
so somebody here

1722
01:52:24,450 --> 01:52:25,920
like that

1723
01:52:25,930 --> 01:52:27,040
does this

1724
01:52:27,080 --> 01:52:30,820
spirals it gets ever ever closer to that thing

1725
01:52:30,830 --> 01:52:33,050
now it can never join

1726
01:52:33,060 --> 01:52:34,440
it can never joined it

1727
01:52:34,500 --> 01:52:38,530
because if you joined at the joining point

1728
01:52:38,580 --> 01:52:39,930
i would have to

1729
01:52:39,950 --> 01:52:46,940
solutions going through this point that's illegal

1730
01:52:47,040 --> 01:52:50,240
so all i can do is get arbitrarily close

1731
01:52:50,280 --> 01:52:55,200
other computer screen it will look as if joint it but of course you can

1732
01:52:55,210 --> 01:52:56,290
it can

1733
01:52:56,340 --> 01:52:59,440
it's just the resolution pixels not

1734
01:52:59,450 --> 01:53:00,800
the resolution is good

1735
01:53:00,810 --> 01:53:02,910
and the ones that start further away

1736
01:53:02,920 --> 01:53:04,660
it will take longer to

1737
01:53:04,680 --> 01:53:08,930
a find their way to limit cycle and i'll always stay outside the earlier guys

1738
01:53:08,930 --> 01:53:11,610
but they will get arbitrarily close to

1739
01:53:11,620 --> 01:53:13,180
how about inside

1740
01:53:13,190 --> 01:53:16,130
inside well start somewhere

1741
01:53:16,140 --> 01:53:18,190
and it does the same thing

1742
01:53:18,190 --> 01:53:21,440
and in and the first half of the talk here but i want to leave

1743
01:53:21,450 --> 01:53:24,410
you with the idea that this is just statistics

1744
01:53:24,430 --> 01:53:27,320
and by the way we come back to the example in the second half and

1745
01:53:27,320 --> 01:53:30,470
explain why i think this has not happened

1746
01:53:30,510 --> 01:53:35,790
i for example in detail this is just statistics and if i can help guide

1747
01:53:35,790 --> 01:53:39,550
our beliefs but can quite magical p value added

1748
01:53:39,600 --> 01:53:43,250
the p value greater than point zero five some people believe that for sure but

1749
01:53:43,250 --> 01:53:45,330
that's just the

1750
01:53:45,580 --> 01:53:49,500
an arbitrary complex is very similar to that there's nothing

1751
01:53:49,580 --> 01:53:51,230
stuff in it

1752
01:53:51,940 --> 01:53:58,570
after the break relax this assumption of no hidden common causes like this

1753
01:53:58,580 --> 01:54:00,010
so i guess is over

1754
01:54:00,030 --> 01:54:01,450
three thirty five thousand

1755
01:54:01,470 --> 01:54:05,170
they five minutes later

1756
01:54:06,000 --> 01:54:09,750
because this is kind of

1757
01:54:09,890 --> 01:54:15,400
called so but that's it doesn't seem like it it's often one

1758
01:54:15,820 --> 01:54:21,890
can we do anything if we relax the this assumption that assumption we can relax

1759
01:54:21,940 --> 01:54:27,360
and that's the learning quite a bit and that's what i'm going to show now

1760
01:54:27,780 --> 01:54:32,190
again i'm just saying what i've already said the main exceptions presence of hidden common

1761
01:54:33,940 --> 01:54:37,620
i guess it doesn't really seem reasonable

1762
01:54:37,640 --> 01:54:45,770
and we can still learn something if we relax the assumption

1763
01:54:46,490 --> 01:54:51,470
the causal embedded faithfulness assumption allows for hidden common causes

1764
01:54:51,500 --> 01:54:54,020
i almost hate to put the next

1765
01:54:54,080 --> 01:54:56,870
screen up here

1766
01:54:56,930 --> 01:55:00,190
all this is the one that i had to put up this one is fine

1767
01:55:00,340 --> 01:55:02,580
sometimes the data

1768
01:55:02,600 --> 01:55:08,250
tells us must be heading kamikaze missions this is this is interesting

1769
01:55:08,270 --> 01:55:12,530
in such cases we know that the faithfulness assumption is not warranted and let's look

1770
01:55:12,530 --> 01:55:14,390
at an example

1771
01:55:15,460 --> 01:55:17,720
we have four variables

1772
01:55:17,740 --> 01:55:21,010
and we have to observe these independencies

1773
01:55:21,060 --> 01:55:25,260
is independent of y axis independent of w

1774
01:55:25,270 --> 01:55:28,470
y of x and y is independent c

1775
01:55:28,480 --> 01:55:32,700
we observe those and only those independencies

1776
01:55:32,710 --> 01:55:36,210
i'm going to show that we make the faithfulness assumption

1777
01:55:36,260 --> 01:55:38,410
will obtain a contradiction

1778
01:55:38,460 --> 01:55:43,280
because because the problem causal faithfulness assumption results in a contradiction

1779
01:55:43,330 --> 01:55:47,140
that means that the assumption is not warranted for this particular

1780
01:55:47,160 --> 01:55:51,800
variables and probability distribution

1781
01:55:51,850 --> 01:55:56,240
i first of all we have to have the links in a

1782
01:55:56,380 --> 01:56:00,040
it was obvious that the white was an undirected graph well i started out with

1783
01:56:00,040 --> 01:56:03,060
an undirected graph in my learning procedure

1784
01:56:03,180 --> 01:56:06,200
i learned that these have to be

1785
01:56:06,250 --> 01:56:10,090
one thing that i started learning what direction the links at head

1786
01:56:10,150 --> 01:56:14,180
now why do these have to be willing to i mentioned that there is an

1787
01:56:14,180 --> 01:56:16,190
edge between two variables

1788
01:56:16,310 --> 01:56:18,050
if and only if

1789
01:56:18,920 --> 01:56:22,120
other variables rendered independent

1790
01:56:22,170 --> 01:56:25,670
ax is independent of y so there's no edge between x and y

1791
01:56:25,680 --> 01:56:29,570
x independent of WC there's no edge between x and w

1792
01:56:29,580 --> 01:56:32,810
why is independent of that same independency actually

1793
01:56:32,860 --> 01:56:36,700
so there's no edge wise dependency so there's no way out

1794
01:56:37,790 --> 01:56:39,450
so these are the edges

1795
01:56:39,500 --> 01:56:45,130
actually not independence these is not independent of w and y z and

1796
01:56:45,280 --> 01:56:49,590
it's just the the first step my learning process for those links

1797
01:56:49,600 --> 01:56:54,620
we have to the directions be because x is independent of y and w

1798
01:56:54,630 --> 01:56:59,170
this is some of the examples i've shown you before that since axis is obtained

1799
01:56:59,170 --> 01:57:01,320
is independent of w

1800
01:57:01,330 --> 01:57:04,690
if these links do not go this way

1801
01:57:05,080 --> 01:57:07,910
one of these things are going this way this won't whole

1802
01:57:07,960 --> 01:57:11,170
the link like this and i might as

1803
01:57:11,220 --> 01:57:16,060
i would say that x is in independent of w conditional on c

1804
01:57:16,070 --> 01:57:19,990
you know that the causal chain went that way or one this way

1805
01:57:20,000 --> 01:57:21,680
what the cause

1806
01:57:21,730 --> 01:57:23,220
and the because w

1807
01:57:23,310 --> 01:57:27,880
those are all say that x and w are independent conditional on the

1808
01:57:28,410 --> 01:57:32,430
only this direction says they are independent

1809
01:57:32,560 --> 01:57:33,850
so i know the

1810
01:57:33,900 --> 01:57:36,180
we also have to that way

1811
01:57:36,200 --> 01:57:42,450
we must have the arrow directions MCA because y is independent of and a

1812
01:57:42,500 --> 01:57:44,650
these areas have been this way

1813
01:57:44,700 --> 01:57:50,450
because it's the only direction says y and z are independent

1814
01:57:50,530 --> 01:57:53,120
so i have to be zero interactions

1815
01:57:53,170 --> 01:57:56,180
because it's the only one that's a w independent

1816
01:57:56,190 --> 01:58:01,770
after these direction because it's the only direction this is the and why are independent

1817
01:58:03,030 --> 01:58:05,600
we end up with the graph today

1818
01:58:05,610 --> 01:58:10,780
which is not a deck set directed acyclic graph contradiction and others made the causal

1819
01:58:10,780 --> 01:58:12,310
faithfulness assumption

1820
01:58:13,650 --> 01:58:18,800
an algorithm which adheres to an end up with something that

1821
01:58:18,810 --> 01:58:23,080
going be flattered that

1822
01:58:23,120 --> 01:58:27,210
there's no doubt faithful to the observed distribution that's the problem

1823
01:58:27,270 --> 01:58:32,970
this is this is certainly valid probability of you know the probability distributions which clearly

1824
01:58:32,970 --> 01:58:38,100
have these independencies and only these but there's no doubt which is which is

1825
01:58:38,150 --> 01:58:39,730
a perfect map

1826
01:58:39,740 --> 01:58:41,980
of that distribution

1827
01:58:42,030 --> 01:58:43,490
now how could that happen

1828
01:58:43,500 --> 01:58:48,260
how can that happen in the causal sense what what causal relationships could be going

1829
01:58:48,260 --> 01:58:51,650
on there would end up making a sincere

1830
01:58:51,670 --> 01:58:54,210
independence like these

1831
01:58:54,220 --> 01:58:56,350
so let's look at this supposed

1832
01:58:56,360 --> 01:59:01,340
the causal DAG in a satisfies the causal faithfulness assumption

1833
01:59:01,380 --> 01:59:05,980
lung cancer tuberculosis both because the positive chest x-ray which

1834
01:59:05,990 --> 01:59:07,330
it's true

1835
01:59:07,350 --> 01:59:11,460
my lectures and lung cancer both cause the peak which is also true say lost

1836
01:59:11,460 --> 01:59:15,810
quite a few people are probably sleeping in the rooms are i think

1837
01:59:16,120 --> 01:59:18,240
it's so

1838
01:59:18,260 --> 01:59:23,640
i suppose that is true that is that those are the causal relationships

1839
01:59:23,700 --> 01:59:28,230
then among x y and c and w we have the independencies i just showed

1840
01:59:28,360 --> 01:59:29,880
the previous slide

1841
01:59:29,960 --> 01:59:32,770
if this is the true causal graph

1842
01:59:33,630 --> 01:59:37,500
faithfulness assumption holds for this causal graph

1843
01:59:37,520 --> 01:59:40,620
this is what i want to search among these variables

1844
01:59:41,750 --> 01:59:42,920
is independent

1845
01:59:42,960 --> 01:59:45,180
of y and w

1846
01:59:45,200 --> 01:59:49,440
access to be independent of its non descendants given its parents and w and y

1847
01:59:49,440 --> 01:59:52,660
are clearly it's not descendants

1848
01:59:52,710 --> 01:59:54,730
and its parents is the empty set

1849
01:59:54,740 --> 01:59:59,020
if anything y y is independent of x a

1850
01:59:59,110 --> 02:00:02,810
right so i would have observed these independencies

1851
02:00:02,860 --> 02:00:07,440
among acts by we looked at those four variables this is what i would the

1852
02:00:07,440 --> 02:00:09,750
ones i've just shown you

1853
02:00:09,800 --> 02:00:12,480
right so that shows how

1854
02:00:12,530 --> 02:00:14,490
you can get those independencies

1855
02:00:14,680 --> 02:00:19,890
because this is hidden cause which are not observing

1856
02:00:19,910 --> 02:00:22,990
we do not have dependency on w

1857
02:00:23,040 --> 02:00:26,670
i i don't observe that

1858
02:00:26,720 --> 02:00:30,680
i'm just showing some very sad actually

1859
02:00:30,760 --> 02:00:35,980
the causal DAG containing the observed variables is the one in b

