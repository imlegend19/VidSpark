1
00:00:00,000 --> 00:00:02,700
including the start state and next state

2
00:00:02,740 --> 00:00:06,280
but my only what is at the end

3
00:00:06,320 --> 00:00:08,550
so if i start initiative is

4
00:00:08,570 --> 00:00:10,450
zeros everywhere

5
00:00:10,480 --> 00:00:14,610
then in all iteration if you think about it if the most recent if it

6
00:00:14,610 --> 00:00:16,920
is zero all this reward is zero

7
00:00:16,930 --> 00:00:21,200
i just added museo i do all of of competition for so i should not

8
00:00:21,200 --> 00:00:22,800
do that

9
00:00:23,660 --> 00:00:28,100
so the idea is that if you look at that have a self organizing your

10
00:00:28,100 --> 00:00:32,920
computation and so these are given by TSR in my opinion this is the best

11
00:00:33,060 --> 00:00:35,170
good for doing that

12
00:00:35,180 --> 00:00:37,600
so this introduces a priority queue

13
00:00:37,610 --> 00:00:41,810
and put states on the priority queue and science some prior to to the states

14
00:00:41,810 --> 00:00:45,310
and then it also states from the prior to queue then

15
00:00:45,320 --> 00:00:49,010
then it updates the state and it puts the predecessors of the state on the

16
00:00:49,010 --> 00:00:52,550
priority queue and i don't think lf two

17
00:00:53,010 --> 00:00:56,550
two to be specified in talking about these are given

18
00:00:56,600 --> 00:00:59,720
is what's the prior to value

19
00:00:59,740 --> 00:01:04,300
half of the state on the priority queue and this is nothing but

20
00:01:04,350 --> 00:01:08,380
the difference between the all the value of the state and the new way of

21
00:01:08,410 --> 00:01:10,090
the state

22
00:01:10,130 --> 00:01:14,950
so that's that's the value and as target this is really efficient

23
00:01:14,970 --> 00:01:18,500
so the good news is that these are these are given and so

24
00:01:18,510 --> 00:01:22,160
i go into combat and prediction conditions

25
00:01:22,240 --> 00:01:26,140
but still the i mean this very finite world and that could be added the

26
00:01:26,170 --> 00:01:27,860
disturbing to use so

27
00:01:27,870 --> 00:01:32,620
these are given some work if you can imagine the state space and then next

28
00:01:33,800 --> 00:01:35,490
i going to be

29
00:01:35,490 --> 00:01:38,320
how to

30
00:01:38,340 --> 00:01:41,510
how to extend this arguments to the case when

31
00:01:41,570 --> 00:01:44,120
you cannot only the states

32
00:01:44,180 --> 00:01:45,880
so that's

33
00:01:45,930 --> 00:01:54,660
the next four

34
00:01:56,490 --> 00:02:01,330
when talking about the planning problem we have to make a distinction between

35
00:02:01,370 --> 00:02:03,500
a few can so one cases

36
00:02:03,600 --> 00:02:05,110
when you have more than

37
00:02:05,120 --> 00:02:11,110
and you can avoid the transition probabilities and you can create the reward function

38
00:02:11,130 --> 00:02:15,580
so in other words you know everything and

39
00:02:15,600 --> 00:02:20,370
you can pose any questions to to the the mother is black box and you

40
00:02:20,370 --> 00:02:26,670
ask was the transition probability from transitioning from state x to state y and his

41
00:02:28,130 --> 00:02:30,670
the the other cases

42
00:02:31,700 --> 00:02:33,490
you can always sample

43
00:02:33,500 --> 00:02:38,600
and we're going to constables cases

44
00:02:38,640 --> 00:02:45,320
so in the planning problem you're given either p our or you can sample from

45
00:02:46,110 --> 00:02:49,480
and your goal is to find an optimal policy

46
00:02:49,510 --> 00:02:53,900
and you care about computational efficiency that's why what you care

47
00:02:53,960 --> 00:02:57,250
and it turns out that computational efficiency

48
00:02:57,700 --> 00:02:59,130
if you

49
00:02:59,230 --> 00:03:01,870
is the sample of these approaches so

50
00:03:05,130 --> 00:03:06,110
so you

51
00:03:06,120 --> 00:03:08,190
the computational efficiency

52
00:03:08,200 --> 00:03:11,900
will transform into sample efficiency

53
00:03:11,920 --> 00:03:13,600
for obvious reasons so

54
00:03:13,610 --> 00:03:18,230
the number of samples that you have is at least the lower bond and the

55
00:03:18,230 --> 00:03:21,580
number of computations that you want to do so

56
00:03:21,590 --> 00:03:25,290
if it turns out that you need to carry the model

57
00:03:25,920 --> 00:03:28,020
many times

58
00:03:28,040 --> 00:03:30,990
then too many times we too many times

59
00:03:31,000 --> 00:03:31,830
then you are

60
00:03:32,020 --> 00:03:36,260
must be slower i

61
00:03:37,250 --> 00:03:44,040
there exists quite a few methods for planning that are the exact solutions

62
00:03:44,110 --> 00:03:48,070
these on dynamic programming and this is what we have covered so far

63
00:03:48,080 --> 00:03:51,750
and those are only useful for pretty small problems

64
00:03:51,910 --> 00:03:55,490
and already approximate solutions and

65
00:03:55,500 --> 00:03:59,150
there are quite a few approaches that so you could do roll outs and this

66
00:03:59,150 --> 00:04:03,110
is what we are going to consider first we are actually going to

67
00:04:03,330 --> 00:04:08,670
relax the planning problem so we are going to change the problem there

68
00:04:08,700 --> 00:04:12,240
and then there are the is america was that try to be an approximate value

69
00:04:12,240 --> 00:04:18,790
functions and then there are policy search my thoughts and various hybrid models

70
00:04:18,850 --> 00:04:20,150
OK so

71
00:04:20,160 --> 00:04:25,700
i already mentioned first talk about balance curse of dimensionality so was that so

72
00:04:25,960 --> 00:04:31,200
this is the final that the running time of most of the RDF media has

73
00:04:31,210 --> 00:04:34,110
the tendency to scale exponentially

74
00:04:34,120 --> 00:04:38,830
with the dimensionality of the state space around all imagine that the state space is

75
00:04:38,830 --> 00:04:43,580
just the unit cube for euclidean space so to the dimensionality

76
00:04:43,600 --> 00:04:48,600
so the problem is that if you want to that's the first thing would be

77
00:04:48,600 --> 00:04:50,490
to discretize that right

78
00:04:50,530 --> 00:04:53,970
but if you want to discretize at q

79
00:04:53,980 --> 00:04:55,210
that has

80
00:04:55,220 --> 00:04:57,830
you know the dimensions these sites

81
00:04:57,850 --> 00:05:00,500
then how many points they need so

82
00:05:00,500 --> 00:05:04,460
the the

83
00:11:19,980 --> 00:11:29,020
you know

84
00:11:36,310 --> 00:11:37,900
in proc

85
00:15:34,210 --> 00:15:37,880
two of

86
00:15:39,400 --> 00:15:40,920
you know what

87
00:17:03,020 --> 00:17:13,750
point you want

88
00:17:13,750 --> 00:17:18,320
plus lambda times l one norm of the coefficient vector so baby is the vector

89
00:17:18,320 --> 00:17:19,690
of coefficients

90
00:17:19,730 --> 00:17:21,690
and we put in l one norm and

91
00:17:21,750 --> 00:17:24,870
on those and we optimize

92
00:17:25,720 --> 00:17:29,310
combination of residual sum of squares plus lambda times about

93
00:17:29,380 --> 00:17:33,180
as you increase lambda you're forced to go the solution of the coefficients force towards

94
00:17:34,570 --> 00:17:36,520
so what happens

95
00:17:36,520 --> 00:17:39,530
so this is a small example is a predictors

96
00:17:39,580 --> 00:17:43,820
and we start off with lambda very big so all the coefficients are zero

97
00:17:43,880 --> 00:17:47,530
and that's the left part of the plaque here this is the zero line

98
00:17:47,630 --> 00:17:51,810
and each of these curves is the coefficient as we relax lambda

99
00:17:51,870 --> 00:17:55,610
so as we relax lambda the coefficients grow away from zero

100
00:17:55,620 --> 00:17:56,830
and in this case

101
00:17:56,850 --> 00:17:57,690
there's more

102
00:17:57,700 --> 00:17:59,040
n is bigger than p

103
00:17:59,910 --> 00:18:03,050
unrestricted least squares fit is given on the right and we see the path of

104
00:18:03,060 --> 00:18:04,250
the coefficients

105
00:18:04,260 --> 00:18:07,470
so we call that the regularisation path

106
00:18:07,490 --> 00:18:10,150
and it's very useful to have such a path because

107
00:18:10,160 --> 00:18:11,410
at the end of the day

108
00:18:11,420 --> 00:18:14,870
you need to pick the parameter lambda or the bounty

109
00:18:14,880 --> 00:18:16,500
and for the coefficients

110
00:18:16,550 --> 00:18:18,390
so having the whole party

111
00:18:19,130 --> 00:18:21,660
did you mean for picking a sensible values

112
00:18:21,670 --> 00:18:23,520
an awful lot of milieux

113
00:18:23,530 --> 00:18:26,530
cross validation or some measure of prediction errors

114
00:18:26,550 --> 00:18:28,150
to pick the value

115
00:18:28,200 --> 00:18:32,130
but it's useful to have a hope of

116
00:18:32,170 --> 00:18:34,990
so in the case of the lasso

117
00:18:35,010 --> 00:18:42,980
solving this problem just naively to convex optimisation problem requires quadratic programming

118
00:18:42,990 --> 00:18:44,150
and you can

119
00:18:44,160 --> 00:18:48,280
you know that it's it's a fairly easy quadratic programme you can solve the problem

120
00:18:48,810 --> 00:18:51,740
that way and that's originally to solve

121
00:18:52,040 --> 00:18:53,780
but in

122
00:18:53,830 --> 00:18:55,580
in two thousand one

123
00:18:55,600 --> 00:18:58,770
efron and co-authors i was one of the

124
00:18:58,820 --> 00:19:01,020
but if from really discovered

125
00:19:02,160 --> 00:19:03,870
you could solve this problem

126
00:19:03,880 --> 00:19:06,020
a much more efficiently

127
00:19:06,060 --> 00:19:11,600
and the reason was that the coefficient parse profiles or piecewise linear

128
00:19:11,610 --> 00:19:15,720
so if you look at this picture you'll see you can see the piecewise linear

129
00:19:15,720 --> 00:19:21,280
parts and each of the vertical lines indicate the breakpoints as it changes

130
00:19:21,280 --> 00:19:25,140
and that led to the what is known as the lars algorithm for solving this

131
00:19:25,410 --> 00:19:28,070
this was convicted this whole part of

132
00:19:30,290 --> 00:19:31,490
to summarize

133
00:19:31,490 --> 00:19:33,800
you can compute the entire party

134
00:19:33,820 --> 00:19:37,240
in the same computations as the single least queries for

135
00:19:37,240 --> 00:19:44,310
so that's very efficient essentially unique there's the computation needed for each of the steps

136
00:19:44,370 --> 00:19:46,280
so that later

137
00:19:46,280 --> 00:19:47,950
you'd speedups in

138
00:19:47,970 --> 00:19:49,070
computing the

139
00:19:51,690 --> 00:19:56,020
so that is called the laws algorithm the names rather obscure sense for least angle

140
00:19:57,850 --> 00:20:02,300
and as i said UTF

141
00:20:02,720 --> 00:20:07,030
and that's what happened and so that was in two thousand one

142
00:20:07,080 --> 00:20:09,580
and that in turn led to

143
00:20:09,590 --> 00:20:16,070
to a you'd flood of path algorithms for all kinds of different regularisation problems

144
00:20:16,080 --> 00:20:18,990
the idea that you can get the entire solution path

145
00:20:20,230 --> 00:20:21,670
was very attractive

146
00:20:21,680 --> 00:20:24,830
and because in any problem we regularisation

147
00:20:24,860 --> 00:20:28,050
you need to compute the solution many points and see if you can compute the

148
00:20:28,050 --> 00:20:31,600
whole path efficiently that is a very attractive thing

149
00:20:32,590 --> 00:20:35,670
this is a list of some of the examples there's lots more

150
00:20:35,670 --> 00:20:38,940
this is the biased because you see my name is is the the names of

151
00:20:38,940 --> 00:20:41,290
my students in a lot of these

152
00:20:41,390 --> 00:20:45,680
but the first one the group lasso this is for the case where a

153
00:20:45,880 --> 00:20:50,230
we variables and actually come in groups we'll talk about that little bit later

154
00:20:50,660 --> 00:20:54,460
one london two thousand six they develop the possible

155
00:20:54,470 --> 00:20:56,090
four support vector machine

156
00:20:56,740 --> 00:21:01,540
the cost parameter in the sort support vector machine is the a tuning parameter regularisation

157
00:21:02,520 --> 00:21:05,160
and it turns out there is a piecewise linear path

158
00:21:05,190 --> 00:21:10,160
for the solutions of the support vector machine as you vary the cost parameter

159
00:21:10,210 --> 00:21:11,830
and that was very with with

160
00:21:11,850 --> 00:21:14,130
some of my students and tibshirani

161
00:21:14,220 --> 00:21:15,700
two thousand four

162
00:21:15,710 --> 00:21:17,810
we'll talk about the elastic net

163
00:21:17,860 --> 00:21:21,910
later on in the talk quantile regression leong yew

164
00:21:22,340 --> 00:21:25,270
logistic regression and generalized linear models

165
00:21:25,600 --> 00:21:27,490
my student union park

166
00:21:27,540 --> 00:21:33,110
and more recently the dantzig selector that's the work of kansas and tell

167
00:21:33,120 --> 00:21:34,270
so says

168
00:21:34,280 --> 00:21:35,580
a different approach

169
00:21:35,590 --> 00:21:40,080
to the let's use the criterion slightly different but the solution is very similar

170
00:21:40,100 --> 00:21:42,660
this is the context of

171
00:21:42,700 --> 00:21:46,250
signal processing and sparse representations

172
00:21:47,270 --> 00:21:50,440
other extreme the mind

173
00:21:50,460 --> 00:21:53,620
gareth james and and this doc

174
00:21:55,340 --> 00:21:58,510
the village possibilities for that

175
00:21:58,510 --> 00:22:01,250
so these all pass algorithms and this has become a little

176
00:22:01,290 --> 00:22:06,370
many craze in statistics to develop of for four for the next problem you can

177
00:22:06,370 --> 00:22:08,890
come you know that that you can

178
00:22:08,910 --> 00:22:11,010
the problem is not

179
00:22:11,020 --> 00:22:15,000
many of them don't enjoy the piecewise linearity of lars of

180
00:22:15,010 --> 00:22:19,060
that made the laws are especially to compute the path very efficiently

181
00:22:19,080 --> 00:22:20,860
four lot of these that some of them

182
00:22:20,870 --> 00:22:23,910
two piecewise linearity like the support vector machines

183
00:22:23,960 --> 00:22:25,750
and the group lasso

184
00:22:25,750 --> 00:22:29,120
solutions but this is perhaps most famous these two tables

185
00:22:29,130 --> 00:22:32,900
so what's what's the illusion here well

186
00:22:32,960 --> 00:22:36,230
let's ask you to make a judgement similar to the kind of judgement next which

187
00:22:36,230 --> 00:22:39,070
is a judgement just about what you see here on the plane

188
00:22:39,220 --> 00:22:42,990
so let's take these two lines meticulously this the this some

189
00:22:42,990 --> 00:22:44,370
the edge of the stable

190
00:22:44,380 --> 00:22:48,250
and they said that table and the question is which one is longer just here

191
00:22:48,250 --> 00:22:51,870
in the plane this one or that one

192
00:22:51,880 --> 00:22:53,610
the right one

193
00:22:53,620 --> 00:22:55,590
anyone want to the left one

194
00:22:55,600 --> 00:22:57,830
is there another answer

195
00:22:57,840 --> 00:23:02,460
the same so some people have already generalize from the one example that when people

196
00:23:02,460 --> 00:23:06,230
show you visual illusion the correct answer is always the same

197
00:23:06,250 --> 00:23:07,110
that's right

198
00:23:07,170 --> 00:23:08,900
they are the same here

199
00:23:08,920 --> 00:23:12,950
just to convince you that i will do my version of like what what nick

200
00:23:12,950 --> 00:23:18,490
did which is show you a powerpoint build up here there's a

201
00:23:18,530 --> 00:23:20,540
you know an outline i drew

202
00:23:20,550 --> 00:23:21,890
on that guy

203
00:23:21,900 --> 00:23:25,270
i haven't done this and so i switched to the new version of powerpoint so

204
00:23:25,270 --> 00:23:26,630
anyone how to rotate

205
00:23:26,820 --> 00:23:33,190
little green saying OK

206
00:23:33,200 --> 00:23:43,920
so you can see no tricks you must have actually literally had powerpoint

207
00:23:43,940 --> 00:23:45,580
but really is

208
00:23:45,620 --> 00:23:50,150
the very same shape the whole table is the same shape in the plane yet

209
00:23:50,150 --> 00:23:54,510
in the context of the scene

210
00:23:54,550 --> 00:24:00,230
makes makes them look very different as two-dimensional shapes so what's going on here well

211
00:24:00,250 --> 00:24:02,690
you know it's all the stuff that nick was talking about

212
00:24:03,490 --> 00:24:05,700
in this

213
00:24:05,710 --> 00:24:08,020
the problem

214
00:24:08,030 --> 00:24:11,830
the problem is ambiguous

215
00:24:11,840 --> 00:24:16,600
the interpretation of the image because there are multiple ways the three d world could

216
00:24:16,600 --> 00:24:20,540
be that would give rise to the same two-dimensional image falling onto retina onto this

217
00:24:20,570 --> 00:24:21,090
plane here

218
00:24:21,490 --> 00:24:24,130
so that's what the that's the problem that's the sense which we're trying to solve

219
00:24:24,130 --> 00:24:29,010
in inherently ill posed under constrained problem right i mean just mathematically the world is

220
00:24:29,010 --> 00:24:33,670
three-dimensional the retinal images two-dimensional so of course you losing information and in principle that

221
00:24:33,690 --> 00:24:38,670
an infinite number of three-dimensional geometry that could be compatible with anyone two-dimensional projection not

222
00:24:38,670 --> 00:24:41,860
to mention all the other complexities that come from the clues in in the way

223
00:24:41,890 --> 00:24:44,220
light interacts surface properties and so on

224
00:24:44,230 --> 00:24:46,900
so how do you how do you solve an impossible problem how do you work

225
00:24:46,900 --> 00:24:49,010
backwards from the two d image to the three d

226
00:24:49,020 --> 00:24:51,020
structure out there well

227
00:24:51,040 --> 00:24:54,370
our best guess is that the brain makes a good guess it's also kind of

228
00:24:54,370 --> 00:24:58,620
statistical inference problem from a bayesian point of view there's some hypothesis space world structures

229
00:24:58,620 --> 00:25:03,520
and some prior and by putting probabilities in the right way that corresponds to in

230
00:25:04,880 --> 00:25:08,480
now in full detail the correspond just enough to the way the world is it

231
00:25:08,490 --> 00:25:13,110
while still allowing tractable inference then that's that's going to be the form of the

232
00:25:14,320 --> 00:25:17,900
by studying what the mind is here right it's not really getting it wrong it's

233
00:25:17,900 --> 00:25:21,750
getting it right in the sense of the using the kind of logic needed to

234
00:25:21,750 --> 00:25:26,730
solve the real-world problems that doesn't involve looking at this image is displayed on a

235
00:25:26,730 --> 00:25:31,390
flat computer screens but involves trying to solve the analogous problem of discovering the three

236
00:25:31,390 --> 00:25:35,190
d structure of the world from the image comic coming on here right now

237
00:25:35,200 --> 00:25:38,490
now what is concurrent computer vision which

238
00:25:38,520 --> 00:25:42,380
which does rely very heavily on the kind of learning techniques that have long been

239
00:25:42,380 --> 00:25:49,010
developed in statistical machine learning things like for example perceptron support vector machines PCA dimensionality

240
00:25:49,900 --> 00:25:54,750
various boosting these kinds of methods which you've probably heard about to hear more about

241
00:25:54,750 --> 00:25:59,700
in this summer school cannot solve these kinds of scene analysis problems well here's an

242
00:25:59,700 --> 00:26:02,920
example from great paper by some of my MIT colleagues

243
00:26:02,930 --> 00:26:03,810
choi lim

244
00:26:03,830 --> 00:26:05,250
torralba and willsky

245
00:26:05,290 --> 00:26:10,820
i think it was just accepted to CVPR two thousand ten so it's state-of-the-art it's

246
00:26:11,000 --> 00:26:16,270
uses a very large number of different learning based approaches it learns detectors for many

247
00:26:16,270 --> 00:26:20,710
classes of real world objects like for example people roads so first

248
00:26:20,770 --> 00:26:24,990
and not only that it also has a notion of context this particular paper is

249
00:26:24,990 --> 00:26:28,680
trying to show how you can put together the result of bottom-up detectors which are

250
00:26:28,680 --> 00:26:34,830
trained using boosting very simple kind of the very powerful supervised learning methods but also

251
00:26:34,830 --> 00:26:38,750
put together some kind of top-down scene context and here the point of this figure

252
00:26:38,750 --> 00:26:40,880
of course as you see this so far in the middle of the road and

253
00:26:40,880 --> 00:26:43,830
its anomalous and their their models able to say

254
00:26:43,860 --> 00:26:47,230
to that anomalies still recognise it is so for the but say that's kind of

255
00:26:47,250 --> 00:26:51,390
violation of context so so here's the input and here's the output

256
00:26:51,390 --> 00:26:52,590
with this very nice

257
00:26:52,610 --> 00:26:53,960
method well

258
00:26:53,960 --> 00:26:57,640
to what extent so what the method does it draws few boxes around some regions

259
00:26:57,640 --> 00:27:01,090
in the image and puts words on the road so for person person person with

260
00:27:01,090 --> 00:27:05,360
confidence now that to some extent approximates what you see when you actually look at

261
00:27:05,360 --> 00:27:09,880
this input but how closely does approximate how how close to the richness of what

262
00:27:09,880 --> 00:27:11,660
you see here to discuss

263
00:27:11,680 --> 00:27:14,330
well on a scale from sort of here to here

264
00:27:14,350 --> 00:27:19,970
i would say it's about human rights definitely a step in the right direction but

265
00:27:20,000 --> 00:27:22,720
just think about all the things you could say here all the things you could

266
00:27:22,720 --> 00:27:26,670
describe all the questions you can answer all the richness of surfaces in the end

267
00:27:26,670 --> 00:27:30,950
and different layers of depth that you see you see these windows here this is

268
00:27:30,950 --> 00:27:32,500
sort of a glass window you see

269
00:27:32,510 --> 00:27:36,600
the car slightly bent and distorted there you don't think there's weird band car inside

270
00:27:36,600 --> 00:27:41,690
that building but rather reflecting something across history all these inferences going unconsciously you're just

271
00:27:41,690 --> 00:27:45,490
as nick was saying we knew consciously introspect what you get is the output of

272
00:27:45,490 --> 00:27:46,760
the computation

273
00:27:46,780 --> 00:27:50,970
but the but the richness of that output should tell us a lot about what

274
00:27:51,180 --> 00:27:54,580
might and maybe even what has to go on a long way from here to

275
00:27:56,020 --> 00:27:58,820
now let's turn to an example which is close to some of the work i

276
00:27:58,820 --> 00:28:03,280
do this is this is from an experiment in our lab where we were inspired

277
00:28:03,280 --> 00:28:04,660
by another kind of problem of

278
00:28:04,860 --> 00:28:10,500
inference and generalization children learning concepts learning the meanings of words anybody who's been a

279
00:28:10,500 --> 00:28:13,480
child or had a child it's easier to see this if you

280
00:28:13,490 --> 00:28:17,070
are looking at somebody else doing this rather than being in the moment yourself

281
00:28:18,490 --> 00:28:21,950
it should be familiar with the experience of word learning and the remarkable thing there

282
00:28:22,030 --> 00:28:25,710
how few examples it takes to learn the meaning of the word so a child

283
00:28:25,710 --> 00:28:29,420
might learn what which areas is or what forces producing one or a couple of

284
00:28:29,420 --> 00:28:32,830
examples labeled in the right contexts and then they can pretty much pick out all

285
00:28:32,830 --> 00:28:35,830
all the other forces they might make an occasional confusion seeing how

286
00:28:36,420 --> 00:28:39,600
you know down the field in the same farming including that of course and the

287
00:28:39,600 --> 00:28:41,660
parents say no no that's account

288
00:28:41,670 --> 00:28:46,470
right but they're are not going to be calling the fence post horse or mum

289
00:28:46,470 --> 00:28:47,570
three years

290
00:29:11,480 --> 00:29:15,780
some of

291
00:29:15,790 --> 00:29:19,470
two of my

292
00:30:01,480 --> 00:30:03,730
have you been

293
00:30:15,650 --> 00:30:18,390
early on

294
00:30:21,020 --> 00:30:26,450
there are many are

295
00:30:32,420 --> 00:30:35,580
because these

296
00:31:01,180 --> 00:31:04,170
this page

297
00:32:01,350 --> 00:32:09,720
i mean er

298
00:32:09,780 --> 00:32:13,150
or they may

299
00:32:24,430 --> 00:32:28,730
in the paper

300
00:32:37,030 --> 00:32:39,250
you know

301
00:33:24,670 --> 00:33:26,360
all right well

302
00:33:31,620 --> 00:33:35,900
we all

303
00:33:35,900 --> 00:33:40,170
i am with the department of computer science and department of statistics at university of toronto so

304
00:33:40,610 --> 00:33:42,700
and by the way as i go through the tutorial

305
00:33:43,120 --> 00:33:47,280
please feel free to ask questions you know this is gonna be

306
00:33:47,690 --> 00:33:48,830
you know partly informal

307
00:33:49,940 --> 00:33:54,560
what i'm hoping to cover his that's i'm hoping to cover a lot of different applications

308
00:33:55,200 --> 00:33:56,030
that's we've seen

309
00:33:56,470 --> 00:33:58,880
some of the successful applications and in deep learning

310
00:33:59,300 --> 00:33:59,820
but also

311
00:34:00,390 --> 00:34:06,210
i give you a little bit of theoretical foundations behind these algorithms right in particular we focus on

312
00:34:07,350 --> 00:34:08,520
graphical models and such

313
00:34:09,300 --> 00:34:12,420
so hopefully what you'll get out of this tutorial is just a little bit of

314
00:34:12,780 --> 00:34:15,770
math behind these algorithms as well as a

315
00:34:16,510 --> 00:34:17,880
different applications that that

316
00:34:19,140 --> 00:34:22,750
that people are using wikipedia just start by saying that you know my area

317
00:34:23,720 --> 00:34:27,980
of research machine learning and it's a very exciting area and i know a lot of people here traded

318
00:34:29,320 --> 00:34:33,320
also experts in in machine learning but it's kind of a very

319
00:34:33,930 --> 00:34:37,960
dynamic field right if you look at different areas like computer vision

320
00:34:38,550 --> 00:34:41,930
natural language processing information retrieval speech recognition

321
00:34:42,440 --> 00:34:45,880
robotics computational biology coming sciences right a lot of these

322
00:34:46,370 --> 00:34:48,070
fields are using

323
00:34:48,510 --> 00:34:49,180
machine learning

324
00:34:49,700 --> 00:34:51,160
what's exciting is that's

325
00:34:51,720 --> 00:34:56,100
you know in the last few years if you look at the space of data that we have

326
00:34:56,690 --> 00:35:01,780
right you can easily get access to images and videos text-to-speech

327
00:35:02,820 --> 00:35:03,430
if you look at

328
00:35:04,770 --> 00:35:09,630
i'm product recommendation out companies like netflix and d and was on

329
00:35:10,040 --> 00:35:14,490
well if you look at social network data or even if you look at scientific data

330
00:35:15,060 --> 00:35:16,800
right it's it's a lot of data

331
00:35:17,890 --> 00:35:21,580
but i would say that most of the data that we see today is unlabelled

332
00:35:22,730 --> 00:35:26,620
it's basically that means that's you know i can get millions of images of the web

333
00:35:27,110 --> 00:35:30,270
but i might not have labels for those images as

334
00:35:30,740 --> 00:35:33,160
precise description as the what's going on in those images

335
00:35:35,020 --> 00:35:38,290
so one of the things that we try to do it in particular in deep

336
00:35:38,330 --> 00:35:43,220
learning community is is trying to develop models that can deal with unlabeled data or

337
00:35:43,220 --> 00:35:48,990
models that can discover structural discovered some statistical dependencies from from the data and how

338
00:35:49,450 --> 00:35:51,080
that can be done in unsupervised

339
00:35:51,990 --> 00:35:55,180
without labels or semi supervised we have partially labeled data

340
00:35:55,860 --> 00:35:59,510
and also it's interesting you know in machine learning with sort of obsessed with developing

341
00:35:59,510 --> 00:36:02,430
algorithms that can work across multiple application domains

342
00:36:03,460 --> 00:36:04,650
and i'll show you some of

343
00:36:05,110 --> 00:36:06,390
some of the examples of debt

344
00:36:07,680 --> 00:36:09,670
what i would argue is that's you know

345
00:36:11,240 --> 00:36:12,100
models that can

346
00:36:12,600 --> 00:36:17,440
do these things as one particular framework of doing this is building these deep hierarchical models

347
00:36:18,230 --> 00:36:22,790
and deep learning models models that can support inferences was discovered structure multiple levels of

348
00:36:22,790 --> 00:36:26,770
representation right now make it more precise as i go up through the tutorial of

349
00:36:26,940 --> 00:36:28,060
of what i mean by that

350
00:36:30,050 --> 00:36:33,310
but let me show you could be before i start going into you know the

351
00:36:33,310 --> 00:36:36,880
definition of the models and not go through the tutorial but show you some of

352
00:36:36,880 --> 00:36:38,130
these examples right

353
00:36:38,610 --> 00:36:40,770
this is one particular model m

354
00:36:41,000 --> 00:36:42,450
it's called the boltzmann machine model

355
00:36:42,850 --> 00:36:46,540
and what you're trying to do he's basically trying to model images of

356
00:36:50,580 --> 00:36:56,940
trying to model distribution over those images right side of the model can recognise airplanes cars trucks and such

357
00:36:57,930 --> 00:37:01,370
and because this is a generative model what you can do is you can simulate

358
00:37:01,370 --> 00:37:05,720
from the model or generate data from the model so if i'm asking the malta

359
00:37:05,730 --> 00:37:06,030
sort of

360
00:37:06,970 --> 00:37:09,560
tell me what it believes images of airplanes should look like

361
00:37:10,270 --> 00:37:13,830
this is the model generating images of airplanes and the flickering that you see at

362
00:37:13,830 --> 00:37:17,230
the top these are the states of the latent variables are hidden variables

363
00:37:18,000 --> 00:37:22,280
right so it's kind of like discovering what airplane should look like so you can

364
00:37:22,280 --> 00:37:24,920
see that goes from one plane to another plane

365
00:37:26,500 --> 00:37:27,400
you know i argued at

366
00:37:28,300 --> 00:37:30,800
sort of has some understanding of far

367
00:37:31,280 --> 00:37:33,280
you know what what we should look like

368
00:37:33,730 --> 00:37:37,280
now the interesting thing about this model is that there is no image-specific priors sitting

369
00:37:37,280 --> 00:37:38,900
in these models the motives looks that

370
00:37:39,930 --> 00:37:41,680
you know ninety six by ninety six images

371
00:37:42,150 --> 00:37:44,780
in figures out what the point should look like

372
00:37:46,010 --> 00:37:46,630
if you look at

373
00:37:47,170 --> 00:37:50,720
no other kinds of data sets this an interesting dataset this data set was collected

374
00:37:50,890 --> 00:37:55,610
at the mitzi u have about twenty five thousand characters from fifty different alphabets and

375
00:37:55,740 --> 00:37:59,120
and if you look at these characters you know you have arabic have serial you

376
00:37:59,120 --> 00:37:59,870
have sanskrit

377
00:38:00,660 --> 00:38:05,260
right and the model has about three thousand latent or hidden variables over two million

378
00:38:05,260 --> 00:38:07,450
parameters and then you can ask the model to

379
00:38:08,120 --> 00:38:10,890
let's see what the image is of sanskrit should look like right

380
00:38:11,300 --> 00:38:15,210
and this is the model simulating what it believes images of sanskrit should look like

381
00:38:15,810 --> 00:38:18,810
they typically ask your new seceded do these look like sanskrit

382
00:38:26,640 --> 00:38:27,390
i see right

383
00:38:28,850 --> 00:38:31,570
yeah i have no idea how supposed to look like a but

384
00:38:32,430 --> 00:38:33,710
this is what the model believes

385
00:38:34,190 --> 00:38:36,320
a satellite images should look like

386
00:38:36,730 --> 00:38:41,920
and this is useful for doing things like recognizing characters right now doing character recognition

387
00:38:42,580 --> 00:38:46,860
you can also do interesting things like batting completion right so for example if i

388
00:38:46,860 --> 00:38:48,420
give you half of the input

389
00:38:48,930 --> 00:38:51,470
and i ask the model to simulate the remaining half

390
00:38:51,890 --> 00:38:53,720
right this is what the model that's

391
00:38:54,260 --> 00:38:57,870
again there's no image specific prior not telling them all at you know they should

392
00:38:57,870 --> 00:38:59,570
be strokes they should be continues

393
00:39:02,730 --> 00:39:04,020
right the model just looks at

394
00:39:05,050 --> 00:39:08,500
thirty thousand different characters and just figures out what character should look like

395
00:39:10,310 --> 00:39:12,450
so why why building these models is

396
00:39:12,950 --> 00:39:14,350
is is challenging well

397
00:39:15,090 --> 00:39:20,930
if you look at the simplest possible model right this is just a twenty by twenty eight image

398
00:39:21,410 --> 00:39:22,980
it's binary image right

399
00:39:24,090 --> 00:39:27,610
but if you look at the space of all possible characters right the space of

400
00:39:27,700 --> 00:39:32,560
all possible images you could generate well it's due to be um twenty by twenty

401
00:39:32,560 --> 00:39:34,900
eight a to to the seven eighty four possible configurations

402
00:39:35,780 --> 00:39:37,630
right so obviously doing it

403
00:39:38,140 --> 00:39:42,050
idea the brute force methods is is gonna be challenging so we have to rely

404
00:39:42,050 --> 00:39:44,970
on some approximate algorithms and we'll see some of those algorithms

405
00:39:45,930 --> 00:39:46,700
in this tutorial

406
00:39:47,300 --> 00:39:51,550
you can also the interesting thing about this was that you can also apply to other kinds of data

407
00:39:51,970 --> 00:39:56,690
so if you're looking at bag-of-words representation and this is just taking him

408
00:39:58,710 --> 00:40:01,310
documents and representing them as word counts

409
00:40:02,340 --> 00:40:05,600
if you take reuters dataset has about eight hundred thousands

410
00:40:06,700 --> 00:40:10,470
web pages and you say well how can expect structure from the data

411
00:40:10,920 --> 00:40:12,310
if i look at the high level

412
00:40:12,730 --> 00:40:15,490
representations this is what the model is discovering

413
00:40:15,920 --> 00:40:17,340
right so i'm just basically

414
00:40:18,010 --> 00:40:22,150
there are no labels i'm just fitting the model a bunch of different web pages and asking it to

415
00:40:24,340 --> 00:40:26,640
the discovered latent structure and this is what is discovery

416
00:40:27,370 --> 00:40:28,390
it's interesting to see that

417
00:40:28,390 --> 00:40:31,710
and i don't see how you teach them

418
00:40:31,760 --> 00:40:35,120
you can only learn actually by experience

419
00:40:37,930 --> 00:40:41,740
having said all that

420
00:40:41,750 --> 00:40:42,780
i still think

421
00:40:42,970 --> 00:40:44,950
that actually

422
00:40:44,960 --> 00:40:46,860
the real that's the matter

423
00:40:49,120 --> 00:40:51,300
is all the things that i've seen is teach

424
00:40:51,320 --> 00:40:55,590
even the best business schools in the world i have been around to many

425
00:40:55,630 --> 00:40:59,180
are useful i don't think they're useless

426
00:40:59,190 --> 00:41:00,620
i think they're useful

427
00:41:00,640 --> 00:41:03,240
that are not the most important things

428
00:41:03,300 --> 00:41:08,050
i to be honest some of the graduates of our business schools in britain and

429
00:41:08,050 --> 00:41:10,250
in america and in europe

430
00:41:10,370 --> 00:41:13,860
i made a pretty ugly mess of a lot of capitalism

431
00:41:13,900 --> 00:41:15,540
in recent years

432
00:41:15,560 --> 00:41:18,200
they both capitalism to distribute

433
00:41:18,470 --> 00:41:20,860
they've ripped us off

434
00:41:20,900 --> 00:41:24,640
look at the banks have been doing this for some of the terrorist people in

435
00:41:24,640 --> 00:41:26,190
our society

436
00:41:26,310 --> 00:41:29,900
and they just lost their way

437
00:41:29,900 --> 00:41:32,600
because i have been very concerned seems to me

438
00:41:32,620 --> 00:41:36,540
and what they have learned about the how of things

439
00:41:36,560 --> 00:41:39,390
sometimes about what all things you know

440
00:41:39,430 --> 00:41:44,060
how does the business work we teach what sometimes we teach what does the business

441
00:41:47,430 --> 00:41:51,420
we very seldom asked why

442
00:41:51,430 --> 00:41:53,680
and i think that's a great shame

443
00:41:53,690 --> 00:41:57,950
business leaders talk about what they do they make money

444
00:41:57,970 --> 00:42:00,740
but they don't answer

445
00:42:00,750 --> 00:42:02,200
in public why

446
00:42:02,200 --> 00:42:05,100
they make money we basically

447
00:42:05,170 --> 00:42:08,500
we make money with that governments and businesses

448
00:42:08,520 --> 00:42:13,160
and indeed as individuals in order to we may do more

449
00:42:13,230 --> 00:42:15,340
with that money to do more things

450
00:42:15,370 --> 00:42:18,150
produce more things

451
00:42:18,190 --> 00:42:19,230
different things

452
00:42:19,250 --> 00:42:20,840
the government

453
00:42:20,840 --> 00:42:25,910
we want the economy to grow so that we can actually build more hospitals built

454
00:42:25,910 --> 00:42:27,400
more schools

455
00:42:27,450 --> 00:42:31,460
we raise more taxes so that we can do more things with them

456
00:42:31,470 --> 00:42:35,240
alas we do not talk about those things enough

457
00:42:35,300 --> 00:42:39,060
we do not talk about the whole idea of business leaders get up and they

458
00:42:39,060 --> 00:42:44,320
boast about having made so much money for that company onolysis industry say look at

459
00:42:44,320 --> 00:42:46,520
these greedy people

460
00:42:46,570 --> 00:42:51,430
they're making money for themselves and not making money for themselves but they do not

461
00:42:52,810 --> 00:42:58,530
so to us it was very exciting recently in our last

462
00:42:58,540 --> 00:43:00,450
two actually interview

463
00:43:00,500 --> 00:43:02,110
and photographs

464
00:43:02,120 --> 00:43:03,960
the people that we call

465
00:43:04,020 --> 00:43:07,470
the new philanthropists

466
00:43:07,480 --> 00:43:09,040
these are people who had

467
00:43:09,110 --> 00:43:10,770
run their own businesses

468
00:43:10,790 --> 00:43:15,170
and how in the end and so the businesses or has got very rich

469
00:43:15,200 --> 00:43:20,280
and had decided to do something useful with the money

470
00:43:21,380 --> 00:43:24,710
as one of them said to me he said charles

471
00:43:24,710 --> 00:43:27,940
now i know why i make money

472
00:43:27,960 --> 00:43:30,250
so that i can use it

473
00:43:30,260 --> 00:43:32,750
on something useful for others

474
00:43:32,760 --> 00:43:35,650
now spends use it

475
00:43:35,930 --> 00:43:40,610
in other words he had suddenly found the y

476
00:43:40,700 --> 00:43:43,320
although he did with his work

477
00:43:43,430 --> 00:43:45,960
not enough people get

478
00:43:46,140 --> 00:43:47,670
but to help her

479
00:43:47,680 --> 00:43:48,950
find y

480
00:43:48,960 --> 00:43:50,470
we decided

481
00:43:50,650 --> 00:43:55,340
two just east to compile a still life

482
00:43:55,340 --> 00:43:57,190
but still life

483
00:43:57,220 --> 00:44:03,910
basically as designed and it's and developed by elizabeth my photographer partner

484
00:44:03,990 --> 00:44:09,440
it is basically a collection of five objects and the piece of nature that resemble

485
00:44:09,470 --> 00:44:12,380
represent what is terribly important to you

486
00:44:12,450 --> 00:44:14,620
in life

487
00:44:14,820 --> 00:44:16,450
this is my still life

488
00:44:16,560 --> 00:44:20,960
by the time it and i'll tell you what it means what i want to

489
00:44:20,960 --> 00:44:22,200
tell you about

490
00:44:22,200 --> 00:44:24,700
is an experiment

491
00:44:24,720 --> 00:44:30,950
that we did recently in claremont university peter drucker is business school

492
00:44:31,630 --> 00:44:34,890
i was asked to be the first

493
00:44:34,900 --> 00:44:41,040
distinguished drucker scholar trying to keep track tradition going over internment and this with an

494
00:44:41,040 --> 00:44:42,940
i toward the course

495
00:44:42,950 --> 00:44:45,710
an elective courses

496
00:44:45,720 --> 00:44:48,740
we limited to ten people

497
00:44:48,760 --> 00:44:52,710
are we had in the end for

498
00:44:52,720 --> 00:44:54,650
executives d one

499
00:44:54,710 --> 00:45:02,090
serial entrepreneur aged about sixty we had to MBA students two h two two phd

500
00:45:02,090 --> 00:45:07,110
students and two members of faculty which is quite encouraging and what we are still

501
00:45:07,200 --> 00:45:08,340
to do

502
00:45:08,350 --> 00:45:12,040
the period this force which lasted five weeks

503
00:45:12,070 --> 00:45:16,170
but the compiler personal still lives

504
00:45:16,210 --> 00:45:19,260
to represent what was important to them in life

505
00:45:19,270 --> 00:45:24,190
and then to do one for their organisation

506
00:45:24,200 --> 00:45:27,760
to see how they compare

507
00:45:27,790 --> 00:45:30,370
and and then to do what a third one

508
00:45:30,450 --> 00:45:34,750
for the organisation as they would like it to be

509
00:45:34,790 --> 00:45:39,710
and i have to tell you it was fascinating

510
00:45:39,720 --> 00:45:44,290
because we're talking here about what's important about values

511
00:45:44,380 --> 00:45:46,720
and actually

512
00:45:49,630 --> 00:45:51,110
much more revealing

513
00:45:51,170 --> 00:45:53,710
the words

514
00:45:53,770 --> 00:45:56,970
because images contain a story behind

515
00:45:56,980 --> 00:46:01,690
and the stories tend to have different levels of meaning

516
00:46:05,800 --> 00:46:06,950
this images

517
00:46:06,960 --> 00:46:10,260
you can use your imagination

518
00:46:10,310 --> 00:46:11,600
which was

519
00:46:11,600 --> 00:46:13,270
richard doll things really

520
00:46:13,370 --> 00:46:16,440
unless they conjure up pictures

521
00:46:16,450 --> 00:46:21,680
i would tell you just about one of these guys

522
00:46:21,690 --> 00:46:23,460
he just bought

523
00:46:23,480 --> 00:46:25,400
a business

524
00:46:25,410 --> 00:46:27,920
this was the travel business it was

525
00:46:27,940 --> 00:46:32,060
what he called a spiritual travel business in other words it took people around the

526
00:46:32,060 --> 00:46:34,480
world to places

527
00:46:34,500 --> 00:46:38,000
with spiritual connotation you cathedral monastery

528
00:46:38,070 --> 00:46:42,230
a special holy place and a lot of americans

529
00:46:42,280 --> 00:46:45,130
thomas with other people to like to do this sort of thing

530
00:46:47,550 --> 00:46:49,860
he produced is still life

531
00:46:49,900 --> 00:46:53,400
of the business which was dominated

532
00:46:53,420 --> 00:46:55,630
by big model boats

533
00:46:55,640 --> 00:46:57,800
because the founder of the business

534
00:46:57,820 --> 00:47:01,190
and originally taken people around the globe

535
00:47:01,190 --> 00:47:07,130
government officials right how can i sort of solicit citizen feedback about certain let's political

536
00:47:07,130 --> 00:47:11,330
issues right so what are the issues that are being raised what what aspects let's

537
00:47:11,330 --> 00:47:16,060
say of building congress is popular what is unpopular and so right so so one

538
00:47:16,060 --> 00:47:19,750
one notion here is to understand what is what is the population thinking and we

539
00:47:19,750 --> 00:47:26,300
will see some of these examples later later in the story right again that would

540
00:47:26,300 --> 00:47:31,740
be very nice industrial park about how well how can you data mining for political

541
00:47:31,740 --> 00:47:35,910
campaigns how do you how how do you figure out why in which people support

542
00:47:35,920 --> 00:47:40,360
the candidate that was a nice article in new york times about law enforcement sort

543
00:47:40,370 --> 00:47:45,410
of mining social media data sending police to the crime scene before the crime happens

544
00:47:45,410 --> 00:47:49,990
and so on so what imagination here can get one but the idea is right

545
00:47:49,990 --> 00:47:53,890
that sort of social media in some sense is the sense human lives we want

546
00:47:53,890 --> 00:47:59,010
to mind that to get to get some useful data and other applications that that

547
00:47:59,010 --> 00:48:03,700
is also interesting is what is called a real-time citizen journalism right the idea is

548
00:48:03,740 --> 00:48:08,750
that people that use it and so on it provides real time and very valuable

549
00:48:08,750 --> 00:48:15,160
information about particular events right and the of course the challenge is how to aggregate

550
00:48:15,160 --> 00:48:22,120
this little little sets of this many redundant posts users who have made to locate

551
00:48:22,120 --> 00:48:26,610
useful information right so the goal here and i want to mine this real-time data

552
00:48:26,920 --> 00:48:30,830
to produce well organised summaries of events or i want to detect events and so

553
00:48:32,140 --> 00:48:34,880
and of course another

554
00:48:34,930 --> 00:48:41,610
the relatively straightforward sort of straightforward area of application is viral marketing right understanding how

555
00:48:41,620 --> 00:48:47,960
influence spread recommendations how they spread through social networks i can start making personalized recommendations

556
00:48:48,190 --> 00:48:49,330
i can all start

557
00:48:49,380 --> 00:48:54,690
my in online forums and there have been studies that sort of show the people

558
00:48:54,690 --> 00:49:00,310
that the users of online forums and sort of advocating brands online also advocated the

559
00:49:00,420 --> 00:49:05,880
offline source of eighty percent of forum contributors friend that if had their friends and

560
00:49:06,380 --> 00:49:10,120
people online make decisions about product purchases and so on so there's a lot of

561
00:49:10,120 --> 00:49:14,210
sort of this enthusiasm about brands and so on the people share so by by

562
00:49:14,210 --> 00:49:17,800
mining this online data we may get some insights about these

563
00:49:17,810 --> 00:49:21,630
and the last the last bit of thing is what it means sort focus in

564
00:49:21,630 --> 00:49:26,230
the second part of the the story is how to process led social media content

565
00:49:26,390 --> 00:49:33,250
to provide tools to identify social networks identify groups often of course of

566
00:49:33,290 --> 00:49:38,000
in the network or identify tightly knit communities how to connect that it let's say

567
00:49:38,000 --> 00:49:44,020
evaluations topic sentiment analysis and so on so sort of this second application this this

568
00:49:44,020 --> 00:49:47,830
this current slide this is what we will talk after the coffee break what i

569
00:49:47,830 --> 00:49:52,810
mentioned the applications i mentioned before we resort to be more related to the to

570
00:49:52,810 --> 00:49:56,860
the first part of the tutorial so here is here is as i said that

571
00:49:56,880 --> 00:50:02,900
the goal for the borealis introduced methods for social media analytics i structured fully deployed

572
00:50:02,910 --> 00:50:07,230
in two parts the first part of the how do we reason and the race

573
00:50:07,230 --> 00:50:09,990
and the model the flow of information networks

574
00:50:10,020 --> 00:50:15,980
the idea will be here in particular to lead to predict how popular how much

575
00:50:15,980 --> 00:50:21,210
attention to a particular piece of information get over time we will do and then

576
00:50:21,210 --> 00:50:24,770
maybe even like when should i place pieces of information so that they they become

577
00:50:24,770 --> 00:50:30,030
popular and then the second part of the story will focus on interactions and social

578
00:50:30,030 --> 00:50:33,820
network structure so the idea here it will be that you want to go beyond

579
00:50:33,830 --> 00:50:37,330
beyond the reasoning about there is a link between a pair of nodes people there

580
00:50:37,330 --> 00:50:42,720
is no link to maybe start thinking what people think about one another trying to

581
00:50:42,720 --> 00:50:47,860
predict outcomes without seeing what people decided to do in particular cases and so on

582
00:50:47,880 --> 00:50:52,110
right so the idea here would be to go beyond just people are connected by

583
00:50:52,110 --> 00:50:58,330
poverty is OK so as i said the first part three parts with the first

584
00:50:58,330 --> 00:51:01,440
one to be how to collect how do we collect data and how can be

585
00:51:01,440 --> 00:51:04,020
sort of like the flow of information online

586
00:51:04,030 --> 00:51:07,560
the second part will then be about modelling and predicting the flow of of let's

587
00:51:07,560 --> 00:51:08,610
say items

588
00:51:08,860 --> 00:51:14,420
information in the last part will be about how do we infer networks of information

589
00:51:14,420 --> 00:51:17,260
flows because these networks and many times he the music

590
00:51:17,280 --> 00:51:19,100
OK so that's basically the idea

591
00:51:19,130 --> 00:51:21,640
and i will keep going right so

592
00:51:21,670 --> 00:51:24,790
the idea is the following right so if i have a little a social media

593
00:51:24,790 --> 00:51:29,550
platform site or just in general right information flows through its right so what i

594
00:51:29,550 --> 00:51:31,430
want to do is i want to analyse

595
00:51:31,470 --> 00:51:36,560
underlying mechanisms for the real time spread of information in this online networks and the

596
00:51:36,560 --> 00:51:42,950
questions that i like to address in the next hour and fifteen minutes is how

597
00:51:42,950 --> 00:51:47,340
does how the messages spread through social networks how to predict the spread of these

598
00:51:47,340 --> 00:51:53,080
messages or information in these networks and how to identify how to sometimes even identify

599
00:51:53,090 --> 00:51:55,270
networks or something worse

600
00:51:55,290 --> 00:52:00,010
so most of the data that i will be talking about is coming from

601
00:52:00,060 --> 00:52:04,510
a company called called spin it's small startup in san francisco and the basically collecting

602
00:52:04,520 --> 00:52:08,490
online social media data so today there are getting around

603
00:52:08,510 --> 00:52:10,890
forty five million articles per day

604
00:52:10,910 --> 00:52:14,740
and they been collecting these data since august two thousand eight and basically what you

605
00:52:14,740 --> 00:52:18,350
get is pretty much everything that google news has that is about twenty thousand news

606
00:52:18,350 --> 00:52:19,740
sources plus

607
00:52:20,150 --> 00:52:25,800
i think four million blogs and online forums and lots of twitter and facebook posts

608
00:52:25,800 --> 00:52:30,280
why mentioning this is because you can go to this URL and actually been is

609
00:52:30,280 --> 00:52:34,700
making some data available for free to academics you you can download it another good

610
00:52:34,700 --> 00:52:41,380
reference for social network data is it real website for our website that is down

611
00:52:41,380 --> 00:52:47,550
there so snap stanford edu we are making around sixty large social and information networks

612
00:52:47,550 --> 00:52:50,590
available online for people to use OK so

613
00:52:50,600 --> 00:52:54,780
we have some data that will be working with here the data is then sold

614
00:52:54,780 --> 00:52:56,360
millions of articles per day

615
00:52:56,390 --> 00:53:01,060
and the first question for us to understand what sort of the basic units of

616
00:53:01,060 --> 00:53:05,710
information so what are the pieces of information that propagate through this let's call them

617
00:53:05,710 --> 00:53:06,790
equal structure

618
00:53:07,220 --> 00:53:11,070
if you look at whether or you look at most phenomenon

619
00:53:11,130 --> 00:53:15,270
the power law is not what you're looking at structure is this is the structure

620
00:53:15,270 --> 00:53:21,220
of an isolated thing comes some of them have powerful but it's

621
00:53:21,320 --> 00:53:25,810
it's a it's not so much

622
00:53:25,890 --> 00:53:33,200
now the problem not always mean structure in fact in many

623
00:53:33,260 --> 00:53:40,950
in the particular situation of economics many of the problems that of of fluctuations in

624
00:53:40,950 --> 00:53:47,010
the market and in fact is the analogue of the boltzmann law in physics in

625
00:53:47,010 --> 00:53:53,430
fact there is an expression of maximum around on what exactly is this one and

626
00:53:53,520 --> 00:53:57,780
in fact they can be put in court in in common in

627
00:53:57,890 --> 00:54:02,930
relation with the

628
00:54:02,930 --> 00:54:10,620
efficient market hypothesis which obviously is safe that the OK so in in effect here

629
00:54:10,620 --> 00:54:14,110
yes in this respect there is agreement

630
00:54:14,130 --> 00:54:21,800
so what really

631
00:54:22,010 --> 00:54:24,020
is assumed

632
00:54:24,170 --> 00:54:31,390
being made in general with one

633
00:54:31,410 --> 00:54:33,370
in a

634
00:54:33,510 --> 00:54:37,270
and there are really only the about five

635
00:54:38,270 --> 00:54:40,630
and on the

636
00:54:40,640 --> 00:54:42,970
a weighted average age

637
00:54:43,010 --> 00:54:45,780
so that's not far

638
00:54:47,560 --> 00:54:51,220
according to one million

639
00:54:54,260 --> 00:54:58,220
can be that we need to be

640
00:55:05,060 --> 00:55:09,110
so although this is far

641
00:55:10,810 --> 00:55:15,100
so in a way a market economy

642
00:55:17,190 --> 00:55:20,190
in the same

643
00:55:20,210 --> 00:55:22,520
one we do

644
00:55:26,090 --> 00:55:28,680
a lot of the problem is

645
00:55:29,210 --> 00:55:31,490
the economy fell

646
00:55:38,700 --> 00:55:45,000
he won the day of relational data

647
00:55:45,010 --> 00:55:46,710
you can do

648
00:55:48,800 --> 00:55:52,360
i mean it a lot more

649
00:55:52,530 --> 00:55:56,350
it is before the

650
00:55:57,460 --> 00:55:59,450
is that

651
00:55:59,660 --> 00:56:03,420
two great

652
00:56:03,710 --> 00:56:07,110
before in many more

653
00:56:07,170 --> 00:56:09,920
the above the

654
00:56:10,010 --> 00:56:13,550
man is there are more

655
00:56:13,640 --> 00:56:16,050
that means that

656
00:56:16,070 --> 00:56:20,050
you can have money for any of the

657
00:56:20,060 --> 00:56:22,510
is shown to i one

658
00:56:22,550 --> 00:56:24,840
you might

659
00:56:24,870 --> 00:56:27,740
from the

660
00:56:29,010 --> 00:56:32,280
my question is

661
00:56:32,410 --> 00:56:34,960
that you

662
00:56:36,520 --> 00:56:42,880
the problem is that the in really good

663
00:56:43,030 --> 00:56:45,050
you don't need

664
00:56:46,430 --> 00:56:49,880
any kind of

665
00:56:49,910 --> 00:56:52,820
my you can use

666
00:56:52,840 --> 00:56:54,580
i four

667
00:56:54,600 --> 00:56:56,760
you can buy

668
00:56:59,310 --> 00:57:01,760
changes course

669
00:57:01,880 --> 00:57:04,080
why it goes

670
00:57:12,390 --> 00:57:18,760
one thing to be the patient through well

671
00:57:18,770 --> 00:57:21,420
it can

672
00:57:24,370 --> 00:57:26,300
is important

673
00:57:26,320 --> 00:57:31,170
it's important that only

674
00:57:31,170 --> 00:57:34,070
only a few months

675
00:57:34,090 --> 00:57:34,790
but the

676
00:57:34,800 --> 00:57:35,720
you should

677
00:57:35,720 --> 00:57:37,800
using the money

678
00:57:37,860 --> 00:57:38,940
are there

679
00:57:43,980 --> 00:57:47,980
so we see that there is what

680
00:57:48,030 --> 00:57:49,880
and at the in

681
00:57:50,140 --> 00:57:53,020
to go out of

682
00:57:53,040 --> 00:57:54,620
you only

683
00:57:54,760 --> 00:57:56,020
you know

684
00:57:58,850 --> 00:58:01,820
there are two

685
00:58:01,840 --> 00:58:05,120
to a great

686
00:58:06,330 --> 00:58:09,640
they represent the images

687
00:58:09,660 --> 00:58:10,960
well that

688
00:58:10,970 --> 00:58:13,350
i a

689
00:58:13,390 --> 00:58:15,020
is any

690
00:58:15,040 --> 00:58:17,110
are there and

691
00:58:19,700 --> 00:58:23,170
there you will by using

692
00:58:26,010 --> 00:58:27,950
it also possible

693
00:58:30,050 --> 00:58:34,700
to use my drawing only

694
00:58:37,500 --> 00:58:42,230
that very day if you run

695
00:58:43,600 --> 00:58:47,030
there was a or i

696
00:58:47,030 --> 00:58:49,910
we've done in

697
00:58:49,920 --> 00:58:52,250
is that

698
00:58:52,270 --> 00:58:54,010
was it

699
00:58:57,660 --> 00:59:05,640
after that you have to go for a moment has over five in the morning

700
00:59:05,670 --> 00:59:08,660
or they have to go on

701
00:59:08,670 --> 00:59:10,410
the same

702
00:59:10,430 --> 00:59:14,170
that is that if really

703
00:59:14,240 --> 00:59:18,610
there is no one so you really

704
00:59:19,340 --> 00:59:23,750
if you use

705
00:59:23,750 --> 00:59:25,330
my rule do

706
00:59:26,270 --> 00:59:28,240
the first of may

707
00:59:28,270 --> 00:59:31,730
excuse me for instance so

708
00:59:31,730 --> 00:59:39,210
it's to represent the data thank

709
00:59:39,210 --> 00:59:42,500
so a year they use

710
00:59:42,510 --> 00:59:46,980
on that is the agent they

711
00:59:47,270 --> 00:59:50,060
because of this

712
00:59:50,090 --> 00:59:51,280
this is

713
00:59:51,410 --> 00:59:54,270
later that day

714
00:59:54,290 --> 00:59:56,520
the very

715
00:59:56,580 --> 00:59:59,810
two through data feature

716
00:59:59,840 --> 01:00:02,480
so if use c

717
01:00:02,490 --> 01:00:04,430
in this play

718
01:00:04,450 --> 01:00:06,960
the other issue

719
01:00:06,980 --> 01:00:10,260
after a

720
01:00:10,300 --> 01:00:12,950
you can our love

721
01:00:12,960 --> 01:00:16,450
that is that

722
01:00:16,450 --> 01:00:18,570
the string comes

723
01:00:22,140 --> 01:00:25,310
this is what i just said

724
01:00:25,330 --> 01:00:27,150
this is a typical query

725
01:00:27,160 --> 01:00:34,390
finally in casualties corporation missus and jet engine discoloration and is located on component that

726
01:00:34,390 --> 01:00:35,310
i give you

727
01:00:35,350 --> 01:00:38,500
this is the level of semantics but also

728
01:00:38,530 --> 01:00:40,920
i wanted to pronounce

729
01:00:40,940 --> 01:00:46,450
the information comes from is the text contains somewhere

730
01:00:46,470 --> 01:00:49,200
so we have the triples that have distracted

731
01:00:49,320 --> 01:00:53,680
i got to the next day they derive from and so what the whole documents

732
01:00:53,680 --> 01:00:57,390
must contain blatant and also

733
01:00:57,800 --> 01:01:00,410
whatever is annotated

734
01:01:00,430 --> 01:01:05,050
as something as component contains trailing edge

735
01:01:05,050 --> 01:01:06,420
so the this is the

736
01:01:06,430 --> 01:01:10,730
you see there are different levels which you can come by and it is possible

737
01:01:10,800 --> 01:01:11,910
to show

738
01:01:11,910 --> 01:01:13,570
that any

739
01:01:13,610 --> 01:01:14,390
of the

740
01:01:14,410 --> 01:01:21,120
different paradigms from providing can be adapted to do this extended to do this

741
01:01:22,410 --> 01:01:27,110
let's try to put everything together and then i think you go to the conclusions

742
01:01:28,410 --> 01:01:30,050
we had to pass through the

743
01:01:30,060 --> 01:01:35,560
one of the many tasks we've done for also in the producer's chair manufacturers such

744
01:01:35,710 --> 01:01:40,810
jet engines was to extract information from eighteen thousand documents

745
01:01:40,830 --> 01:01:42,780
there are many forms

746
01:01:43,350 --> 01:01:48,310
they were all different shapes but continued more or less the same information with thousands

747
01:01:48,310 --> 01:01:51,250
of forms different forms

748
01:01:51,370 --> 01:01:55,550
we had an ontology developed by the university of aberdeen

749
01:01:56,660 --> 01:02:01,640
we try to train the system to do information extraction from the tables

750
01:02:02,250 --> 01:02:03,510
we selected the

751
01:02:03,550 --> 01:02:06,830
we use active media to train and two two

752
01:02:06,840 --> 01:02:11,460
annotate hundreds of documents and then use machine learning a form of direct it is

753
01:02:11,470 --> 01:02:16,330
to be developed and then we picked up all the pieces of information we could

754
01:02:16,720 --> 01:02:20,600
identify with an accuracy was more than ninety five percent

755
01:02:20,620 --> 01:02:23,270
so all these go to ninety percent

756
01:02:25,190 --> 01:02:29,380
then we have developed tool for searching for a search

757
01:02:29,560 --> 01:02:35,980
it enables both document creating and knowledge creating including graphics of knowledge so the quantification

758
01:02:36,990 --> 01:02:40,360
and we have

759
01:02:40,360 --> 01:02:44,940
testing the through the means and on the document itself using queries to the users

760
01:02:44,940 --> 01:02:47,960
have centers and also with real users

761
01:02:47,990 --> 01:02:50,570
now if you look at the accuracy

762
01:02:50,580 --> 01:02:53,940
for every query in the first twenty hits

763
01:02:53,970 --> 01:02:59,150
so we look at the twenty first of the first twenty documents returned to see

764
01:02:59,150 --> 01:03:03,220
that for keywords in terms of precision recall if you know what it means is

765
01:03:03,220 --> 01:03:07,740
basically fifty percent in this case so fifty percent accuracy

766
01:03:08,070 --> 01:03:12,550
if you talk about the ontology based pure ontology base of high precision

767
01:03:12,570 --> 01:03:14,860
because consisting of data properly

768
01:03:14,880 --> 01:03:19,590
a lot of queries are not covering so you can't query so have low recall

769
01:03:19,700 --> 01:03:22,990
but if enabled people to use hybrid search

770
01:03:23,320 --> 01:03:29,930
you actually go something like eight percent eighty four percent precision eighty four percent percent

771
01:03:29,930 --> 01:03:34,130
accuracy so you have the best two words

772
01:03:34,150 --> 01:03:36,880
so then we gave it to the thirty two

773
01:03:38,490 --> 01:03:41,380
you're looking at me two minutes

774
01:03:41,380 --> 01:03:43,180
twenty four minutes started

775
01:03:43,200 --> 01:03:45,990
but always g

776
01:03:47,920 --> 01:03:48,860
so well

777
01:03:48,880 --> 01:03:51,450
he gave it to thirty two years as they

778
01:03:51,470 --> 01:03:56,240
test for quite a while and then we'll easy to the internet on the internet

779
01:03:56,240 --> 01:03:59,880
and we wanted to understand if people could use semantic search for people to use

780
01:03:59,880 --> 01:04:01,650
the hybrid paradigm

781
01:04:01,650 --> 01:04:03,200
well you have to search

782
01:04:03,220 --> 01:04:07,650
in what turned out to these days every night data to the really what the

783
01:04:07,650 --> 01:04:10,160
fact that the system was faster

784
01:04:11,900 --> 01:04:16,050
it was actually easy to use so they understood that to the concept of semantic

785
01:04:16,050 --> 01:04:17,510
search engines

786
01:04:17,510 --> 01:04:19,380
the fact that you can

787
01:04:21,340 --> 01:04:23,470
integrated with keyword somehow

788
01:04:24,950 --> 01:04:28,760
i felt that year the system was curator

789
01:04:28,860 --> 01:04:32,380
the very acurate and it was

790
01:04:32,400 --> 01:04:33,970
more life easy o

791
01:04:33,970 --> 01:04:35,160
pretty easy to

792
01:04:35,180 --> 01:04:37,320
then two years

793
01:04:38,200 --> 01:04:41,590
but in the end it doesn't tell us if they will use it for that

794
01:04:41,610 --> 01:04:45,320
system for their everyday work actually

795
01:04:45,340 --> 01:04:50,400
what happened is that this system was voted for the two thousand seven rolls royce

796
01:04:50,400 --> 01:04:53,380
director's creativity award it is an award

797
01:04:53,400 --> 01:04:54,660
voted by

798
01:04:54,820 --> 01:04:55,820
c or

799
01:04:57,090 --> 01:05:00,990
proposed to be to by senior employees to the directors

800
01:05:02,300 --> 01:05:03,360
it is

801
01:05:03,380 --> 01:05:06,530
given to technologies that have the potentiality of

802
01:05:06,530 --> 01:05:12,570
a disruptive potentiality within the company is a very large company and we were finally

803
01:05:12,590 --> 01:05:14,450
out of three we didn't win

804
01:05:14,490 --> 01:05:19,070
but also this was the knowledge management tool the other way about the core technology

805
01:05:19,070 --> 01:05:25,240
of building jet engines so we think it's very good results and they also have

806
01:05:25,240 --> 01:05:31,430
both sides of what both sides of this equation written in standard form and both

807
01:05:31,430 --> 01:05:38,150
sides so it's going to be 1 over X and Y prime minus 1 over

808
01:05:38,150 --> 01:05:44,080
X squared and Y is equal to x squared times 1 over X which is

809
01:05:44,080 --> 01:05:45,850
simply xxx

810
01:05:47,000 --> 01:05:52,450
now if you have done the work correctly you should be able now to integrate

811
01:05:52,450 --> 01:05:57,960
the left hand side directly so I'm gonna write it this way I always recommend

812
01:05:57,960 --> 01:06:04,310
that you put in as an extra steps of well unable to put it put

813
01:06:04,310 --> 01:06:09,830
in as an extra step the reason for using the integrating factor in other words

814
01:06:09,830 --> 01:06:12,790
the left hand side is supposed to be now

815
01:06:13,270 --> 01:06:16,080
1 over X times Y

816
01:06:16,330 --> 01:06:17,850
but but

817
01:06:18,950 --> 01:06:22,850
I always put in that because there's always a chance you made a mistake or

818
01:06:22,850 --> 01:06:28,990
forgot something to look at it and mentally differentiated using the product rule just check

819
01:06:28,990 --> 01:06:32,470
that in fact it is turns out to be the same as the left hand

820
01:06:32,470 --> 01:06:38,230
side so we get 1 over X times Y prime plus y times the derivative

821
01:06:38,230 --> 01:06:42,410
of 1 over X which indeed is negative 1 over X squared

822
01:06:42,460 --> 01:06:45,040
and now finally

823
01:06:45,060 --> 01:06:50,600
that's a continue to do the integration so you going to get this if do

824
01:06:50,720 --> 01:06:56,200
on 1 board 1 over X times Y is equal to M X plus a

825
01:06:56,200 --> 01:06:59,950
constant exercise x squared over 2

826
01:06:59,950 --> 01:07:06,160
plus a constant and the final step will be there for right now what I

827
01:07:06,180 --> 01:07:08,870
want to isolate why by itself so why

828
01:07:09,410 --> 01:07:14,180
will be equal to multiply through by x x cubed over to

829
01:07:14,290 --> 01:07:24,020
policy context of but of solution

830
01:07:25,350 --> 01:07:29,600
OK let's do 1 a little slightly more complicated

831
01:07:32,970 --> 01:07:43,350
you this try this 1 now my equation is going to be 1 also keep

832
01:07:43,350 --> 01:07:48,970
to y and x is the variables well values to in a minute 1 was

833
01:07:49,180 --> 01:07:54,950
cosine acts so I'm not going to give you this 1 in standard form you

834
01:07:54,980 --> 01:08:07,410
the the trick question of why prime minus sign X times Y is equal to

835
01:08:07,410 --> 01:08:14,190
anything reasonable I guess that I think that was exciting to x

836
01:08:14,890 --> 01:08:21,910
of many more exciting OK now

837
01:08:22,120 --> 01:08:27,140
I think I should warn you what where the mistake for are just so that

838
01:08:27,140 --> 01:08:33,480
you could make all of them so this the state number 1 you don't put

839
01:08:33,480 --> 01:08:38,810
it in standard form mistake number 2

840
01:08:41,330 --> 01:08:46,720
but that generally people can do step 1 fine but mistake number 2 is you

841
01:08:46,720 --> 01:08:52,580
don't know This is my most common mistakes on very sensitive to it but that

842
01:08:52,580 --> 01:08:56,200
doesn't mean if you make it you'll get sympathy for me I don't get separately

843
01:08:56,200 --> 01:08:56,730
to myself

844
01:08:58,670 --> 01:09:04,150
years so intent and so happy having found the integrating factor you forget to multiply

845
01:09:04,240 --> 01:09:09,830
fueled by the integrating factor also you just handle the 2nd in the left hand

846
01:09:09,830 --> 01:09:13,950
side of the equation and you forget about the right hand side of the both

847
01:09:13,950 --> 01:09:20,070
the emphasis on the both theory is the right hand please include the Q please

848
01:09:20,070 --> 01:09:27,030
include the right hand side of the mistakes are well nothing that I can think

849
01:09:27,030 --> 01:09:32,490
of well maybe only when I can make any mistakes the rest of this lecture

850
01:09:32,570 --> 01:09:36,530
so what we do we write this in standard form so it's going to look

851
01:09:36,530 --> 01:09:45,570
like white prime minus sign x sign x divided by 1 . cosine x times

852
01:09:45,570 --> 01:09:56,550
y equals my heart sinks because I'm I know so molten integrate something like this

853
01:09:56,550 --> 01:10:00,790
and boy that that that's going to give me problems OK well

854
01:10:00,810 --> 01:10:06,980
not yet so what's the integrating factor integrating factor is what we want to calculate

855
01:10:06,980 --> 01:10:09,520
the integral of negative

856
01:10:09,540 --> 01:10:17,060
sign x only 1 + comes that's the integral of PBX and after that we

857
01:10:17,060 --> 01:10:21,500
have to exponentiated well can and you do that if you have if you stare

858
01:10:21,500 --> 01:10:27,230
at a little while you can see that the top is provided is the derivative

859
01:10:27,250 --> 01:10:32,040
of the body that is great than integrates to be the log of 1 was

860
01:10:32,040 --> 01:10:33,950
cosine x is the right

861
01:10:34,100 --> 01:10:38,450
1 over 1 plus cosine x times the derivative of this which is negative cosine

862
01:10:39,330 --> 01:10:43,250
therefore the integrating factor is

863
01:10:43,450 --> 01:10:56,210
lead to that in other words it is one plus cosine Iike therefore so this

864
01:10:56,210 --> 01:10:57,660
was Step 0

865
01:10:57,830 --> 01:11:02,750
the step 1 we found the integrating factor analysis step 2

866
01:11:03,330 --> 01:11:08,040
we multiply through by the integrating factor and what do we get we multiply through

867
01:11:08,040 --> 01:11:13,350
the standard form equation but the integrating factor and if you do that which again

868
01:11:14,250 --> 01:11:20,950
well why prime gets the coefficient 1 . cosine x and y prime minus sign

869
01:11:20,950 --> 01:11:28,160
x equals to x

870
01:11:33,010 --> 01:11:41,660
well I hope somebody would giggle at this point was giggle about it well that

871
01:11:41,660 --> 01:11:49,350
all this was totally wasted work called spinning your wheels now it's not spinning your

872
01:11:49,350 --> 01:11:54,610
wheels it's doing which is supposed to do on and finding out that you wasted

873
01:11:54,610 --> 01:11:57,370
the entire time doing what you were supposed to do

874
01:12:00,910 --> 01:12:06,510
well we get in other words the net effect of this is to end up

875
01:12:06,510 --> 01:12:08,910
let's go with a crazy we have

876
01:12:08,930 --> 01:12:10,240
you see we have

877
01:12:11,700 --> 01:12:14,990
he calls and g

878
01:12:15,030 --> 01:12:17,340
and then we get minus

879
01:12:17,410 --> 01:12:20,510
the this the force

880
01:12:20,550 --> 01:12:23,070
and the resistive force as term the

881
01:12:23,090 --> 01:12:25,700
and as the term in his square and

882
01:12:25,760 --> 01:12:28,090
you see the MV squared

883
01:12:28,140 --> 01:12:30,320
so this cannot be solved analytically

884
01:12:30,340 --> 01:12:33,550
but i asked my graduate students they fully

885
01:12:33,550 --> 01:12:37,410
who is one of your instructors to solve this for me

886
01:12:39,110 --> 01:12:41,930
and i'm still going to show you the results

887
01:12:41,970 --> 01:12:46,490
that he prepared is a nice view graph you can see the effect

888
01:12:46,510 --> 01:12:47,490
of time

889
01:12:49,680 --> 01:12:53,390
ball if you drop it from

890
01:12:53,410 --> 01:12:56,410
three metres

891
01:12:58,430 --> 01:13:02,390
all the numbers are that is on the web don't copy anything

892
01:13:02,410 --> 01:13:05,510
you have the values for c one and c two are given at the top

893
01:13:05,510 --> 01:13:08,610
you may not be able to see them from you see that they are there

894
01:13:08,660 --> 01:13:10,740
and what you see here

895
01:13:14,240 --> 01:13:17,760
height above the ground as a function of time this is one second is one

896
01:13:17,760 --> 01:13:20,260
and a half seconds which is the three million mark

897
01:13:20,320 --> 01:13:24,240
if there were no and drag remember we drop an apple early on

898
01:13:24,260 --> 01:13:26,700
from three meters it will hit the floor

899
01:13:27,510 --> 01:13:30,070
about seven hundred eighty millisecond

900
01:13:30,070 --> 01:13:31,910
however with the and red

901
01:13:31,990 --> 01:13:35,800
it will be about one second later more like one point eight

902
01:13:35,800 --> 01:13:40,180
seconds so point seven wasn't bad as you see but if you look here

903
01:13:40,220 --> 01:13:41,530
that's how the

904
01:13:43,780 --> 01:13:45,760
build up as a function of time

905
01:13:45,780 --> 01:13:48,530
you see it takes about three four tenths of a second

906
01:13:48,550 --> 01:13:49,610
to build up

907
01:13:50,490 --> 01:13:54,160
terminal speed which is the one point eight meters per second that you have there

908
01:13:54,240 --> 01:13:56,860
needless to say of course that the acceleration

909
01:13:57,030 --> 01:14:00,970
gravity does not remain constant but goes down very quickly

910
01:14:00,970 --> 01:14:02,530
and when the acceleration

911
01:14:02,530 --> 01:14:04,140
reaches approaches zero

912
01:14:04,160 --> 01:14:06,630
then you have terminal speed

913
01:14:06,660 --> 01:14:08,860
and then there's no longer any change

914
01:14:08,870 --> 01:14:09,840
in the

915
01:14:10,840 --> 01:14:13,820
so let's try this

916
01:14:13,840 --> 01:14:16,990
give more light

917
01:14:17,800 --> 01:14:21,160
we are going to throw subject and i don't think you're going to get one

918
01:14:21,160 --> 01:14:22,800
point eight seconds you may

919
01:14:22,820 --> 01:14:25,630
get something that is larger than one point eight seconds

920
01:14:25,630 --> 01:14:28,490
and the reasons i the following number one

921
01:14:28,550 --> 01:14:31,070
this is not a perfect sphere

922
01:14:31,090 --> 01:14:32,660
and only for years

923
01:14:32,660 --> 01:14:37,660
these calculations hold that's number one number two this thing is very springy

924
01:14:37,720 --> 01:14:41,160
so the moment and let it go it probably goes in some kind of oscillation

925
01:14:41,160 --> 01:14:44,550
that doesn't help either that will probably also slowed down

926
01:14:44,590 --> 01:14:46,990
because what causes of course this slow down

927
01:14:47,030 --> 01:14:52,590
in regime two is really turbulence turbulence is extremely hard to understand and predict

928
01:14:52,640 --> 01:14:54,720
and so almost anything i do

929
01:14:54,740 --> 01:14:56,890
i will only had turbulence

930
01:14:56,910 --> 01:15:00,450
and therefore i predict that the time that it will take from three meters will

931
01:15:00,450 --> 01:15:03,590
be probably larger than one point eight seconds

932
01:15:03,660 --> 01:15:04,390
but it

933
01:15:04,430 --> 01:15:07,990
will be substantially larger than the seven hundred eighty million seconds

934
01:15:08,010 --> 01:15:11,570
which is what you would have seen if you drop an apple

935
01:15:11,590 --> 01:15:13,220
so let's see how close

936
01:15:13,240 --> 01:15:19,360
we are

937
01:15:19,360 --> 01:15:22,300
two all this time

938
01:15:22,300 --> 01:15:23,840
thank you i zero

939
01:15:23,890 --> 01:15:25,410
i did

940
01:15:32,200 --> 01:15:39,340
not too easy to releases by the way and start time at the same moment

941
01:15:39,360 --> 01:15:42,700
it's not even so easy for me to see when it hits the ground

942
01:15:42,760 --> 01:15:45,140
but as a huge uncertainty in this

943
01:15:51,260 --> 01:15:56,430
three two one zero

944
01:15:56,450 --> 01:16:01,130
what do we see

945
01:16:01,260 --> 01:16:03,760
i see something to point zero not bad

946
01:16:04,590 --> 01:16:06,570
the prediction was one point eight

947
01:16:06,570 --> 01:16:07,910
you get two point zero

948
01:16:07,930 --> 01:16:12,930
there's no better than sixty and wrecked into account and it's not even an approximation

949
01:16:12,950 --> 01:16:15,860
it uses the entire

950
01:16:16,720 --> 01:16:18,490
linear in v as well as

951
01:16:18,550 --> 01:16:21,950
the square but it's almost exclusively

952
01:16:21,970 --> 01:16:25,950
dominated by the descriptor

953
01:16:27,070 --> 01:16:28,470
i also as they

954
01:16:28,490 --> 01:16:31,910
to show me what happens when i throw a pebble

955
01:16:31,930 --> 01:16:34,070
of the empire state building

956
01:16:34,110 --> 01:16:35,010
and the petal

957
01:16:35,010 --> 01:16:38,860
that we chose was a radius of one centimeter kind of

958
01:16:38,890 --> 01:16:42,180
all of us could find no reference to density of pebbles

959
01:16:42,280 --> 01:16:46,840
and with the throne of the empire state building we reach a terminal speed

960
01:16:46,860 --> 01:16:48,680
of about seventy five

961
01:16:48,740 --> 01:16:49,930
whilst body

962
01:16:49,970 --> 01:16:53,720
without the correct we would have reached two hundred and twenty five miles

963
01:16:53,740 --> 01:16:55,070
but our

964
01:16:55,110 --> 01:17:02,510
i want to show you that two

965
01:17:02,510 --> 01:17:06,030
now you see the empire state building which has a height of four hundred seventy

966
01:17:06,030 --> 01:17:08,220
five metres

967
01:17:08,220 --> 01:17:12,610
that's where you start t zero this one second five seconds ten seconds five seconds

968
01:17:12,610 --> 01:17:14,010
fifteen seconds

969
01:17:14,030 --> 01:17:17,470
and if there had been no and we're hit the ground

970
01:17:17,490 --> 01:17:19,360
a little less than ten seconds

971
01:17:20,110 --> 01:17:21,720
it will hit the ground more like

972
01:17:21,720 --> 01:17:23,680
sixteen seventeen seconds

973
01:17:23,740 --> 01:17:25,240
and you see that the

974
01:17:25,240 --> 01:17:27,260
terminal speed built up

975
01:17:27,280 --> 01:17:29,140
in about five six seconds

976
01:17:29,140 --> 01:17:31,260
very close to the final value

977
01:17:31,320 --> 01:17:33,240
and if there had been no and

978
01:17:33,950 --> 01:17:36,470
the speed at which it will hit the ground

979
01:17:36,470 --> 01:17:37,990
course grow linearly

980
01:17:38,050 --> 01:17:40,920
when it hits the ground it would be somewhere here which is two hundred and

981
01:17:40,920 --> 01:17:42,300
twenty five miles

982
01:17:42,320 --> 01:17:45,470
our so you see that even the pebble

983
01:17:45,490 --> 01:17:46,840
you wouldn't expect to be

984
01:17:46,860 --> 01:17:50,910
they have a large effect on and make it huge provided that you throw it

985
01:17:50,910 --> 01:17:51,910
from a

986
01:17:51,930 --> 01:17:53,340
high building

987
01:17:53,360 --> 01:17:55,970
now you may remember

988
01:17:55,990 --> 01:17:58,640
that we dropped an apple

989
01:17:58,660 --> 01:18:00,740
from three metres

990
01:18:00,760 --> 01:18:02,700
and that we calculated

991
01:18:03,410 --> 01:18:05,300
gravitational acceleration

992
01:18:05,320 --> 01:18:08,320
given the time it takes to fall was one of your

993
01:18:08,340 --> 01:18:10,550
one of the things you did in your assignment

994
01:18:10,590 --> 01:18:13,410
we had seven hundred eighty one millisecond scifi

995
01:18:13,430 --> 01:18:16,140
and i know that you can calculate genes

996
01:18:16,160 --> 01:18:19,870
right because you know to three meters is one have gt squared

997
01:18:19,930 --> 01:18:23,140
so i give you the three which uncertainty i give you the time seven hundred

998
01:18:23,140 --> 01:18:27,280
eighty one millisecond was an uncertainty of too many seconds which we allowed

999
01:18:27,370 --> 01:18:28,760
our property

1000
01:18:28,760 --> 01:18:32,530
so i believe what is the effect of and drag

1001
01:18:32,630 --> 01:18:33,890
on this apple

1002
01:18:33,930 --> 01:18:36,530
is it was the responsible thing for us

1003
01:18:37,280 --> 01:18:39,320
ignore that

1004
01:18:39,360 --> 01:18:41,640
the apple

1005
01:18:41,700 --> 01:18:43,030
has a

1006
01:18:44,140 --> 01:18:46,130
of hundred thirty four grams

1007
01:18:46,200 --> 01:18:47,820
easy to way of course

1008
01:18:47,840 --> 01:18:49,360
this was our apple

1009
01:18:49,430 --> 01:18:51,510
during our first lecture

1010
01:18:51,530 --> 01:18:54,140
and is hundred and thirty four grams

1011
01:18:54,280 --> 01:18:59,070
is almost the sphere really not quite but almost the sphere and the rate is

1012
01:18:59,090 --> 01:19:02,410
about three centimeters

1013
01:19:02,410 --> 01:19:05,280
and that leads to a terminal

1014
01:19:05,300 --> 01:19:08,090
velocity which you can calculate if you want to

1015
01:19:08,140 --> 01:19:09,430
using the

1016
01:19:09,470 --> 01:19:12,180
this criterion but i was not interested in that

1017
01:19:12,200 --> 01:19:14,240
i wanted to know

1018
01:19:14,240 --> 01:19:16,740
harmony millisecond

1019
01:19:16,820 --> 01:19:20,550
is the touchdown delayed because of the brain

1020
01:19:20,570 --> 01:19:23,470
and they've made the calculations and he found

1021
01:19:23,530 --> 01:19:28,910
that's too millisecond form three meters from one-and-a-half metres it's almost nothing

1022
01:19:29,010 --> 01:19:32,860
and the reason why is almost nothing from one-and-a-half metres you see

1023
01:19:32,860 --> 01:19:34,030
when you throw

1024
01:19:34,050 --> 01:19:35,050
and appl

1025
01:19:36,070 --> 01:19:38,110
is really in regime two

1026
01:19:38,160 --> 01:19:39,930
so you really dominated by

1027
01:19:39,950 --> 01:19:41,860
the speed square

1028
01:19:41,930 --> 01:19:46,180
and the first one and a half million doesn't get a very high speed speed

1029
01:19:46,180 --> 01:19:49,140
grows linearly and so does the last portion

1030
01:19:49,160 --> 01:19:53,220
where you really get hit by the a by the script

1031
01:19:53,240 --> 01:19:54,660
too many seconds

1032
01:19:54,700 --> 01:19:56,390
from three metres

1033
01:19:56,410 --> 01:19:57,430
three eight

1034
01:19:57,590 --> 01:19:59,510
three metres

1035
01:19:59,530 --> 01:20:01,180
there's too many second

1036
01:20:01,200 --> 01:20:03,050
let's call it delay

1037
01:20:03,280 --> 01:20:08,570
so we have on the area of being lucky unlucky

1038
01:20:08,610 --> 01:20:14,450
if you really want to calculate the gravitational acceleration using our data you should really

1039
01:20:14,450 --> 01:20:16,360
subtracted too many seconds

1040
01:20:16,410 --> 01:20:18,160
from the time on the other hand

1041
01:20:18,200 --> 01:20:22,950
since we are allowed to say to millisecond uncertainty we really weren't

1042
01:20:22,950 --> 01:20:25,670
very well in the first example

1043
01:20:25,750 --> 01:20:29,560
i was hoping for an average of about seventy five

1044
01:20:29,580 --> 01:20:32,420
class average was eighty nine

1045
01:20:32,430 --> 01:20:34,510
so that leaves us with two possibilities

1046
01:20:35,420 --> 01:20:37,280
you are very smart

1047
01:20:37,300 --> 01:20:39,480
this is an exceptional class

1048
01:20:39,530 --> 01:20:41,350
or the exams

1049
01:20:41,350 --> 01:20:42,850
it was too easy

1050
01:20:42,910 --> 01:20:44,530
now these examples taken

1051
01:20:45,250 --> 01:20:47,030
three instructors

1052
01:20:47,050 --> 01:20:49,340
way before you took

1053
01:20:49,350 --> 01:20:52,250
none of them thought it was too easy so i'd like to think that you

1054
01:20:52,250 --> 01:20:55,400
are really an exceptional class and like to congratulate you

1055
01:20:55,490 --> 01:20:57,820
that you did so well

1056
01:20:57,820 --> 01:21:01,200
here is a histogram

1057
01:21:01,210 --> 01:21:03,330
of course

1058
01:21:03,420 --> 01:21:07,030
if we had to decide on this test alone

1059
01:21:07,030 --> 01:21:10,820
getting you cruises forgetting your homework on this test along

1060
01:21:11,460 --> 01:21:15,390
the dividing line between pass and fail to sixty five

1061
01:21:15,430 --> 01:21:17,210
that means that

1062
01:21:17,250 --> 01:21:19,280
five percent of the class

1063
01:21:19,320 --> 01:21:21,020
would fail which is

1064
01:21:21,030 --> 01:21:23,690
unusually low we normally that is around

1065
01:21:23,710 --> 01:21:25,440
fifteen percent

1066
01:21:25,440 --> 01:21:29,210
time will tell whether you are indeed exceptionally smart

1067
01:21:29,250 --> 01:21:33,130
or whether the exam was too easy the good news also is

1068
01:21:33,170 --> 01:21:34,570
two pieces of good news

1069
01:21:34,610 --> 01:21:36,910
that we promise that the books will arrive

1070
01:21:36,960 --> 01:21:38,020
at the coop

1071
01:21:40,740 --> 01:21:44,170
today we're going to talk about

1072
01:21:44,190 --> 01:21:46,800
this brings about pendulums and about

1073
01:21:46,860 --> 01:21:49,670
simple harmonic oscillators one of the key

1074
01:21:49,720 --> 01:21:51,130
topics in

1075
01:21:51,130 --> 01:21:52,990
o eight o one

1076
01:21:53,020 --> 01:21:55,080
if i have a spring

1077
01:21:55,160 --> 01:22:00,430
this is the relaxed length of the string

1078
01:22:01,830 --> 01:22:04,020
i call that actually zero

1079
01:22:04,020 --> 01:22:08,430
and i extend the string which point was he

1080
01:22:08,470 --> 01:22:10,940
then there is the force

1081
01:22:11,800 --> 01:22:13,030
i want to drive

1082
01:22:13,050 --> 01:22:14,820
this spring back

1083
01:22:14,860 --> 01:22:16,550
the equilibrium

1084
01:22:16,610 --> 01:22:19,690
and it is an experimental facts

1085
01:22:19,690 --> 01:22:23,380
that many springs we call the ideal springs

1086
01:22:23,380 --> 01:22:25,410
for many springs this force

1087
01:22:27,440 --> 01:22:29,300
two the displacement

1088
01:22:31,080 --> 01:22:33,700
this is x

1089
01:22:33,710 --> 01:22:34,930
if you make x

1090
01:22:34,950 --> 01:22:37,800
three times larger than restoring force

1091
01:22:37,830 --> 01:22:40,650
is three times larger

1092
01:22:40,700 --> 01:22:44,870
this is the one dimensional problem so to avoid the vector notation we can simply

1093
01:22:44,870 --> 01:22:46,540
say that the force

1094
01:22:47,430 --> 01:22:48,460
is minus

1095
01:22:48,460 --> 01:22:51,730
a certain constant which we call the spring constant

1096
01:22:51,740 --> 01:22:53,890
this is called the spring constant

1097
01:22:53,890 --> 01:22:58,510
and the spring constant has units newtons per meter

1098
01:22:58,520 --> 01:23:03,140
the minus sign takes care of the direction

1099
01:23:03,150 --> 01:23:04,700
when x is positive

1100
01:23:04,770 --> 01:23:09,370
then the forces in the negative direction when f is negative the forces in the

1101
01:23:09,370 --> 01:23:10,790
positive direction

1102
01:23:10,830 --> 01:23:13,840
it is a restoring force

1103
01:23:13,860 --> 01:23:18,110
whenever this linear relation between f and x holds

1104
01:23:18,120 --> 01:23:19,830
that is referred to

1105
01:23:19,830 --> 01:23:22,550
as we explore

1106
01:23:22,760 --> 01:23:26,060
how can we

1107
01:23:26,080 --> 01:23:27,670
measure the

1108
01:23:27,730 --> 01:23:31,550
spring constant that's actually not too difficult

1109
01:23:31,550 --> 01:23:34,890
i can use gravity

1110
01:23:34,950 --> 01:23:36,520
here is the spring

1111
01:23:36,580 --> 01:23:38,840
it's relaxed situation

1112
01:23:38,890 --> 01:23:40,610
i hang on the spring

1113
01:23:40,650 --> 01:23:42,050
mass and

1114
01:23:42,050 --> 01:23:43,810
and i make use of the fact

1115
01:23:43,830 --> 01:23:45,390
the gravitino

1116
01:23:45,460 --> 01:23:47,840
exerts a force on the spring

1117
01:23:49,170 --> 01:23:52,510
when you factor new equilibrium this is the new equilibrium position

1118
01:23:52,620 --> 01:23:54,080
then the spring force

1119
01:23:54,120 --> 01:23:57,240
of course must be exactly the same as and

1120
01:23:57,260 --> 01:24:00,360
no acceleration when the thing is addressed

1121
01:24:00,390 --> 01:24:03,180
and so i could now make a lot

1122
01:24:04,370 --> 01:24:05,980
i could have px

1123
01:24:05,990 --> 01:24:07,770
and i could have here

1124
01:24:07,830 --> 01:24:10,900
this course f which i know because i know the masses i can change the

1125
01:24:11,770 --> 01:24:13,610
i can go through a whole lot of them

1126
01:24:13,610 --> 01:24:20,710
and you will see data points which scatter around a straight line

1127
01:24:20,770 --> 01:24:21,680
and the

1128
01:24:21,700 --> 01:24:23,730
spring constant follows then

1129
01:24:23,790 --> 01:24:25,040
if you take

1130
01:24:25,080 --> 01:24:27,230
call this delta f

1131
01:24:27,240 --> 01:24:28,640
you call this

1132
01:24:28,640 --> 01:24:30,960
delta x

1133
01:24:30,960 --> 01:24:32,460
in the spring constant k

1134
01:24:32,590 --> 01:24:35,630
is delta have divided by

1135
01:24:35,720 --> 01:24:40,440
in fact so you can even measure it you don't have to start necessarily

1136
01:24:40,470 --> 01:24:43,660
at this point where the spring is relaxed you could already start

1137
01:24:43,700 --> 01:24:45,530
when the spring is already

1138
01:24:45,600 --> 01:24:47,400
under tension that is

1139
01:24:48,740 --> 01:24:51,170
the problem

1140
01:24:51,250 --> 01:24:55,780
you'd be surprised how many springs really behave very nicely

1141
01:24:55,790 --> 01:24:58,070
according to lord

1142
01:24:58,370 --> 01:25:01,570
i have one year it's not very expensive

1143
01:25:01,630 --> 01:25:03,690
spring is here here

1144
01:25:03,780 --> 01:25:05,570
and it is here hold

1145
01:25:05,590 --> 01:25:08,260
on it so it's already a little bit under stress

1146
01:25:08,360 --> 01:25:10,700
it doesn't make any difference

1147
01:25:10,790 --> 01:25:15,860
this marks here i thirteen centimetres apart and every time i one kilogram of

1148
01:25:15,870 --> 01:25:16,750
you will see

1149
01:25:16,800 --> 01:25:20,950
that it goes down by roughly fifteen centimetres

1150
01:25:20,970 --> 01:25:23,160
goes down to this mark

1151
01:25:23,200 --> 01:25:25,170
i put another kilogram of

1152
01:25:25,200 --> 01:25:28,990
goes down to this mark

1153
01:25:29,040 --> 01:25:30,820
put another kilogram on

1154
01:25:30,870 --> 01:25:33,490
and it goes back to this mark

1155
01:25:33,500 --> 01:25:34,620
all the way down

1156
01:25:34,630 --> 01:25:37,740
and if i take them all also so what i've done is i've effectively went

1157
01:25:37,740 --> 01:25:38,940
along this curve

1158
01:25:38,970 --> 01:25:40,610
and if i take them off

1159
01:25:40,650 --> 01:25:42,300
if it is an ideal spring

1160
01:25:43,180 --> 01:25:44,250
it goes back

1161
01:25:44,300 --> 01:25:47,130
to its original length

1162
01:25:47,260 --> 01:25:48,280
which it does

1163
01:25:48,320 --> 01:25:50,930
that's a requirement of course for an ideal spring

1164
01:25:50,990 --> 01:25:53,740
it behaves according to law

1165
01:25:54,650 --> 01:25:56,870
i can of course over two things

1166
01:25:56,880 --> 01:25:59,110
i can take a spring like this one

1167
01:25:59,150 --> 01:26:03,500
and structure to the point that it no longer behaves like hooks law

1168
01:26:03,550 --> 01:26:05,360
i can damage it

1169
01:26:05,380 --> 01:26:08,510
i can do a prominent

1170
01:26:10,610 --> 01:26:13,170
look that's easy

1171
01:26:13,200 --> 01:26:15,470
we're sure workflow is no longer

1172
01:26:15,510 --> 01:26:18,990
look how much longer this spring is than it was before

1173
01:26:19,030 --> 01:26:22,430
so the comes of course limit how far you can go

1174
01:26:23,300 --> 01:26:26,010
you permanently before the spring

1175
01:26:26,040 --> 01:26:28,620
what i have done now is that spring

1176
01:26:28,670 --> 01:26:32,260
probably in the beginning i went up along the straight line

1177
01:26:32,300 --> 01:26:33,090
and then

1178
01:26:33,100 --> 01:26:35,170
something like this must have happened

1179
01:26:35,180 --> 01:26:37,170
i got a huge extensions

1180
01:26:37,170 --> 01:26:39,310
my fourth did not increase very much

1181
01:26:39,320 --> 01:26:42,490
and then when i relaxed when i took my force off

1182
01:26:42,490 --> 01:26:44,310
the spring was longer at the end

1183
01:26:44,320 --> 01:26:47,110
it it was at the beginning so i happen next

1184
01:26:47,120 --> 01:26:49,350
extension which will always be there

1185
01:26:49,400 --> 01:26:51,690
and that's not very nice of course to do that

1186
01:26:51,700 --> 01:26:56,420
two spring so law holds only within certain limitations you have to

1187
01:26:56,430 --> 01:26:58,540
o a certain amount of

1188
01:27:00,490 --> 01:27:03,180
there are ways that you can also

1189
01:27:03,240 --> 01:27:05,000
measure the spring constant

1190
01:27:05,010 --> 01:27:06,850
in a dynamic way

1191
01:27:06,860 --> 01:27:09,800
which is actually very interesting

1192
01:27:10,240 --> 01:27:12,240
i have here

1193
01:27:15,990 --> 01:27:17,010
and this spring

1194
01:27:17,030 --> 01:27:19,440
this is actually equals zero

1195
01:27:19,440 --> 01:27:21,680
is that is very small

1196
01:27:21,710 --> 01:27:25,080
and we got the idea that

1197
01:27:26,810 --> 01:27:30,460
so what we found was that the chinese

1198
01:27:30,510 --> 01:27:32,770
you see

1199
01:27:32,810 --> 01:27:36,060
these row

1200
01:27:36,100 --> 01:27:38,290
the people as

1201
01:27:38,290 --> 01:27:41,960
so basically it this is hundred problem it

1202
01:27:42,000 --> 01:27:44,080
mentions concept

1203
01:27:44,100 --> 01:27:47,140
this is one of several villages it presents

1204
01:27:47,200 --> 01:27:50,410
fifty percent this owned

1205
01:27:50,410 --> 01:27:52,710
willie and sentences

1206
01:27:52,790 --> 01:27:53,890
you don't

1207
01:27:53,910 --> 01:27:58,430
different cases all all the data points

1208
01:27:58,440 --> 01:28:03,250
the same instances

1209
01:28:10,080 --> 01:28:11,870
one k

1210
01:28:11,870 --> 01:28:15,210
the whole

1211
01:28:15,390 --> 01:28:21,890
is also used and is caused by this is the

1212
01:28:21,910 --> 01:28:27,640
so this and so the idea is that since so you know this is a

1213
01:28:27,640 --> 01:28:30,020
very small

1214
01:28:30,040 --> 01:28:34,120
he is large she will be reduced

1215
01:28:34,140 --> 01:28:39,410
changes for these things we want to along

1216
01:28:39,980 --> 01:28:41,160
who's who

1217
01:28:42,460 --> 01:28:44,290
this no

1218
01:28:44,410 --> 01:28:46,930
you to what is

1219
01:28:46,960 --> 01:28:50,040
so for example

1220
01:28:50,180 --> 01:28:56,100
so that for example you see very different from what it is

1221
01:28:58,160 --> 01:29:02,540
on this

1222
01:29:02,680 --> 01:29:05,600
so you don't

1223
01:29:09,790 --> 01:29:13,330
so he was sent

1224
01:29:19,180 --> 01:29:20,480
we did

1225
01:29:20,580 --> 01:29:22,850
that is

1226
01:29:25,740 --> 01:29:32,520
it tends to do best and it

1227
01:29:32,710 --> 01:29:37,180
so that was very

1228
01:29:37,250 --> 01:29:38,980
what was one of

1229
01:29:40,270 --> 01:29:42,410
it was a lot smaller

1230
01:29:42,480 --> 01:29:45,870
so what find

1231
01:29:45,870 --> 01:29:47,270
what's more

1232
01:29:52,460 --> 01:29:53,770
predicting this

1233
01:29:54,980 --> 01:29:58,270
this is the

1234
01:30:00,960 --> 01:30:03,870
there actually that of course is an image

1235
01:30:04,140 --> 01:30:09,330
the international space that's

1236
01:30:09,350 --> 01:30:13,040
you have one

1237
01:30:13,080 --> 01:30:17,390
one of the biggest one is

1238
01:30:17,580 --> 01:30:20,200
but not much more

1239
01:30:23,100 --> 01:30:26,210
his unit was you know

1240
01:30:26,230 --> 01:30:28,080
there are five

1241
01:30:28,080 --> 01:30:32,520
so we see to be to measure distance to as well

1242
01:30:32,580 --> 01:30:34,440
you all

1243
01:30:34,460 --> 01:30:37,710
the status quo

1244
01:30:37,750 --> 01:30:40,730
the best one

1245
01:30:40,750 --> 01:30:47,040
so that was of this all this one is

1246
01:30:47,040 --> 01:30:50,620
he calls on this

1247
01:31:00,350 --> 01:31:02,100
one is

1248
01:31:02,140 --> 01:31:03,770
this is months

1249
01:31:08,060 --> 01:31:12,600
i was in the international space station wonderful

1250
01:31:12,620 --> 01:31:16,210
it is here

1251
01:31:16,230 --> 01:31:16,960
the actually

1252
01:31:16,960 --> 01:31:19,160
across the

1253
01:31:19,210 --> 01:31:21,440
that is

1254
01:31:21,680 --> 01:31:24,580
so you actually is

1255
01:31:30,640 --> 01:31:33,180
is is that it is

1256
01:31:33,180 --> 01:31:34,370
it's fun

1257
01:31:34,390 --> 01:31:37,310
what about is

1258
01:31:37,460 --> 01:31:42,370
so this is constructed outside city hall

1259
01:31:42,390 --> 01:31:43,830
as are

1260
01:31:44,540 --> 01:31:48,660
so that was one of the ten rays

1261
01:31:54,930 --> 01:31:56,580
exactly does

1262
01:31:56,600 --> 01:31:59,560
you know who

1263
01:31:59,560 --> 01:32:01,740
put a bit more mass around

1264
01:32:01,750 --> 01:32:04,330
a third third third

1265
01:32:05,520 --> 01:32:09,370
that's setting up data there's slate ten ten ten distribution

1266
01:32:09,390 --> 01:32:12,540
put a lot of mass around the thirty third third

1267
01:32:13,480 --> 01:32:19,250
and by varying the different alpha as we can get it to put mass in

1268
01:32:19,250 --> 01:32:24,180
different parts of the space so you is that there are actually two ten two

1269
01:32:24,220 --> 01:32:26,200
which has a mean

1270
01:32:28,080 --> 01:32:29,770
value for data to

1271
01:32:29,850 --> 01:32:32,540
of ten over fourteen

1272
01:32:32,810 --> 01:32:35,120
which is somewhere

1273
01:32:35,140 --> 01:32:37,060
you know quite high

1274
01:32:37,120 --> 01:32:38,160
around here

1275
01:32:38,180 --> 01:32:39,750
a lot of mass here

1276
01:32:39,750 --> 01:32:43,480
this one was a lot of mess here and if you choose your parameters of

1277
01:32:43,480 --> 01:32:47,200
the dirichlet distribution to be less than one

1278
01:32:47,220 --> 01:32:49,100
alpha is less than one

1279
01:32:49,100 --> 01:32:53,350
put mass at the corners of this distribution so they take away message from the

1280
01:32:54,250 --> 01:32:56,120
this distribution

1281
01:32:57,600 --> 01:33:01,240
so hopefully you now have a bit of a sense of what the dirichlet distribution

1282
01:33:02,370 --> 01:33:04,370
and obviously

1283
01:33:04,390 --> 01:33:10,310
when you add up the actual healthy ends to your original out first what you

1284
01:33:10,310 --> 01:33:16,350
get is the posterior over the parameters that becomes a dirichlet distributions that the sharper

1285
01:33:16,350 --> 01:33:17,580
and sharper

1286
01:33:17,600 --> 01:33:20,850
centered around the frequency

1287
01:33:20,890 --> 01:33:28,600
the average frequency with which you you observe any particular settings of the variables

1288
01:33:29,770 --> 01:33:34,620
so here is just an example let's imagine that we started out with the do

1289
01:33:34,620 --> 01:33:36,870
actually one one one distribution

1290
01:33:36,910 --> 01:33:41,520
this corresponds to a uniform prior distribution over the parameters data

1291
01:33:41,560 --> 01:33:45,890
and how to say this isn't a very strong dogmatic prior

1292
01:33:45,890 --> 01:33:50,220
since any parameter setting is assumed a priori possible

1293
01:33:50,220 --> 01:33:55,140
remember it's important to realize when you do bayesian inference there isn't any right prior

1294
01:33:55,140 --> 01:33:58,700
you don't have to worry about getting the right prior all you have to worry

1295
01:33:58,700 --> 01:34:05,700
about is getting a reasonable prior a prior that isn't too dogmatic that doesn't

1296
01:34:07,020 --> 01:34:08,270
it doesn't say

1297
01:34:08,330 --> 01:34:13,180
that certain values of the parameters are impossible for example

1298
01:34:15,600 --> 01:34:19,140
once you have a reasonable prior like that then as you observed more and more

1299
01:34:19,140 --> 01:34:23,950
data your posterior will converge to close to

1300
01:34:23,970 --> 01:34:26,410
maximum likelihood parameters

1301
01:34:26,430 --> 01:34:27,450
in the limit

1302
01:34:27,860 --> 01:34:32,700
i'll talk about those sorts of issues may be in the last lecture we can

1303
01:34:32,700 --> 01:34:33,720
have a bit of

1304
01:34:33,770 --> 01:34:36,470
discussion about bayesian methods in general

1305
01:34:37,790 --> 01:34:41,100
after observing a data set d where the

1306
01:34:41,120 --> 01:34:44,310
what are the parameter posterior distributions well

1307
01:34:44,330 --> 01:34:46,040
it's very simple

1308
01:34:46,040 --> 01:34:48,000
the posterior distribution over

1309
01:34:48,180 --> 01:34:52,370
the table entries theta i j a dog

1310
01:34:52,430 --> 01:34:58,200
are just dirichlet a with the number of counts in

1311
01:34:58,250 --> 01:35:04,720
in that role of that particular table plus one

1312
01:35:04,770 --> 01:35:08,450
OK since one was the original office that we start out with

1313
01:35:08,500 --> 01:35:13,270
and and so what is this predict this predicts for future data

1314
01:35:13,350 --> 01:35:21,970
the probability of observing valuable alliance state k given that his parents are state j

1315
01:35:22,050 --> 01:35:28,560
and having observed the dataset the those probabilities are simply going to be an i

1316
01:35:28,580 --> 01:35:30,270
j k plus one

1317
01:35:30,270 --> 01:35:31,520
divided by

1318
01:35:31,520 --> 01:35:35,560
some overcame primary and i j k prime plus one

1319
01:35:36,850 --> 01:35:38,080
all this did

1320
01:35:38,950 --> 01:35:42,640
add one two of the accounts in the

1321
01:35:42,660 --> 01:35:43,620
in the

1322
01:35:43,640 --> 01:35:45,700
conditional probability tables

1323
01:35:49,000 --> 01:35:53,350
you know trying to make a new prediction so this is certainly much more reasonable

1324
01:35:53,810 --> 01:36:00,290
than what we had in the maximum likelihood setting because even if our observed counts

1325
01:36:00,290 --> 01:36:01,850
for something are zeros

1326
01:36:02,270 --> 01:36:05,540
this will not think that that setting is impossible

1327
01:36:07,000 --> 01:36:10,830
and this is a form of smoothing where you add one to each of the

1328
01:36:10,830 --> 01:36:15,290
counts is a very old idea which is called the policies rule

1329
01:36:15,330 --> 01:36:20,680
but the general idea here will work for any garishly prior you choose

1330
01:36:20,790 --> 01:36:25,120
and you can use that to encode any prior knowledge you have about what table

1331
01:36:25,120 --> 01:36:26,930
entries should look like

1332
01:36:26,950 --> 01:36:30,970
so questions about that

1333
01:36:34,890 --> 01:36:36,540
try to move on the it

1334
01:36:36,560 --> 01:36:40,200
let's talk about bayesian parameter learning with hidden variables

1335
01:36:40,200 --> 01:36:42,850
so now my life is a little more tricky

1336
01:36:42,870 --> 01:36:47,220
we have some observed data d we have some hidden variables x and the model

1337
01:36:47,220 --> 01:36:49,020
parameters theta

1338
01:36:49,040 --> 01:36:54,770
and let's assume that we have a discrete variables and dirichlet priors on theta

1339
01:36:54,870 --> 01:37:00,450
our goal is to infer the posterior distribution over parameters given the data

1340
01:37:00,450 --> 01:37:03,560
but unfortunately this involves summing

1341
01:37:03,580 --> 01:37:08,890
overall settings of a hidden variables the distribution of the hidden variables and parameters given

1342
01:37:10,040 --> 01:37:12,220
that's a problem because

1343
01:37:12,250 --> 01:37:14,100
the posterior distribution

1344
01:37:14,120 --> 01:37:20,370
over the parameters given d is the sum over x this distribution

1345
01:37:20,470 --> 01:37:23,700
times this distribution

1346
01:37:24,580 --> 01:37:30,680
and we know that this distribution here is a dirichlet distribution so for every way

1347
01:37:30,680 --> 01:37:33,660
of filling in the missing variables x

1348
01:37:33,680 --> 01:37:37,870
we have a dirichlet distribution over parameters here

1349
01:37:37,890 --> 01:37:43,910
but unfortunately there are exponentially many ways of filling in x

1350
01:37:43,930 --> 01:37:50,870
are hidden variables so it follows that the exact posterior distribution over are parameters

1351
01:37:50,870 --> 01:37:54,660
given the observed data is going to be a mixture

1352
01:37:54,660 --> 01:37:58,290
dirichlet distributions with exponentially many

1353
01:37:58,310 --> 01:38:00,950
terms in that mixture model

1354
01:38:02,560 --> 01:38:06,240
so this is a classic case of a computationally intractable

1355
01:38:06,340 --> 01:38:10,040
distribution over the parameters

1356
01:38:10,060 --> 01:38:15,430
and the solutions we can have to this are there are several ways of trying

1357
01:38:15,430 --> 01:38:16,930
to solve this problem

1358
01:38:16,930 --> 01:38:19,770
we could try to find a single best

1359
01:38:19,790 --> 01:38:23,000
completion of the hidden variables x

1360
01:38:23,020 --> 01:38:27,430
and kind of in analogy to what's done in hidden markov models we call this

1361
01:38:27,470 --> 01:38:29,370
the sort of turbie

1362
01:38:29,390 --> 01:38:33,180
the viterbi algorithm some way of finding the single best

1363
01:38:33,200 --> 01:38:36,310
setting of the hidden states and hidden markov model so that's why i use the

1364
01:38:36,310 --> 01:38:37,720
term to be here

1365
01:38:37,740 --> 01:38:43,100
that's an idea that silky and omohundro apply to hidden markov models but can be

1366
01:38:43,100 --> 01:38:45,580
applied more generally to any bayesian

1367
01:38:47,810 --> 01:38:49,270
graphical model

1368
01:38:49,270 --> 01:38:55,690
here's a slightly more interesting example grayscale images of faces that were taken just role

1369
01:38:55,690 --> 01:38:57,190
frames of

1370
01:38:57,200 --> 01:38:58,410
we can

1371
01:38:58,420 --> 01:39:03,770
so we got what can we ran it's you know low resolution modes give these

1372
01:39:03,810 --> 01:39:09,090
twenty by twenty eight images and and then different people sitting in in front of

1373
01:39:09,100 --> 01:39:14,800
the video camera and the consecutive frames per cent are taken as a data point

1374
01:39:14,860 --> 01:39:20,220
to eighteen people use their identities class labels the original frames

1375
01:39:20,280 --> 01:39:23,960
dimension five hundred sixty which you know hundreds

1376
01:39:23,970 --> 01:39:26,270
each person is free for

1377
01:39:26,280 --> 01:39:29,750
training nine hundred test so here the projection

1378
01:39:29,760 --> 01:39:32,630
so this is just eventually i two by

1379
01:39:32,630 --> 01:39:34,630
five hundred sixty matrix

1380
01:39:34,650 --> 01:39:37,670
but i have multiplied the raw data mining what

1381
01:39:37,730 --> 01:39:42,060
it's very hard to understand i mean it's big right so i think this is

1382
01:39:42,060 --> 01:39:47,710
actually pretty amazing that there is a linear projection because for raw pixels space to

1383
01:39:47,710 --> 01:39:53,970
this space in which the identity of people is fairly well separated out

1384
01:39:54,020 --> 01:39:59,320
so is asking what do the rows or columns

1385
01:39:59,560 --> 01:40:04,360
the image matrix so i should have displayed they look sort of like

1386
01:40:04,420 --> 01:40:09,260
the kind of thing you expect if you train the discriminative system so one them

1387
01:40:09,280 --> 01:40:14,090
has a lot of mass around four regions in another one has a lot of

1388
01:40:15,190 --> 01:40:19,940
over here i think in this dataset distinguishes

1389
01:40:19,950 --> 01:40:23,990
there some people who facial hair also women i think in our faces and then

1390
01:40:24,000 --> 01:40:28,310
you know shows background it's hard to interpret

1391
01:40:28,320 --> 01:40:33,860
they look so similar to what you get when you go train like a softmax

1392
01:40:33,860 --> 01:40:36,050
regression was arrested

1393
01:40:36,060 --> 01:40:40,910
remember this is all happening in a two-dimensional three-dimensional feature space

1394
01:40:40,980 --> 01:40:48,070
and again you can see that the errors are are substantially so that there is

1395
01:40:48,730 --> 01:40:50,380
inevitable experiments on

1396
01:40:50,390 --> 01:40:52,370
and it's good to for

1397
01:40:52,380 --> 01:40:59,960
this being traumas of experiments on the digits anyway so something in the absence of

1398
01:40:59,960 --> 01:41:04,060
strong prior knowledge about how to pick your king nearest neighbour distance

1399
01:41:04,070 --> 01:41:07,640
i think learning the metric from data seems like a good idea

1400
01:41:07,690 --> 01:41:11,730
we make this big idea we shouldn't just trying to inject knowledge by hand you

1401
01:41:11,730 --> 01:41:15,610
should learn everything from data that's what you should know that when the going to

1402
01:41:15,610 --> 01:41:17,350
k nearest neighbour which is

1403
01:41:17,360 --> 01:41:22,190
try and have the method by hand so we have all these labelled data we

1404
01:41:22,190 --> 01:41:23,210
should use that data

1405
01:41:23,420 --> 01:41:24,330
learn that

1406
01:41:24,340 --> 01:41:29,280
for k nearest neighbour and this seems like a reasonable way to learn

1407
01:41:29,310 --> 01:41:33,250
and then if you really need to go back to this is an easy way

1408
01:41:33,250 --> 01:41:38,590
to substantially reduce the dimensionality of feature space again not arbitrary

1409
01:41:38,600 --> 01:41:42,520
but by taking the principal components and that's kind of where i would think that

1410
01:41:42,770 --> 01:41:47,770
the dimensions with most variance would be the most informative class labels that no we

1411
01:41:47,780 --> 01:41:49,830
prioritise think that

1412
01:41:49,850 --> 01:41:54,210
and here is nice because it seems nothing about the form of the class distributions

1413
01:41:54,230 --> 01:41:59,360
doesn't assume that palestinian order to convex or even connected it doesn't seem anything about

1414
01:41:59,380 --> 01:42:07,110
separating surface linear linear in some feature space completely nonparametric memory based method which is

1415
01:42:07,110 --> 01:42:10,190
just k nearest neighbors taking serious

1416
01:42:10,210 --> 01:42:12,550
so i think that take

1417
01:42:12,560 --> 01:42:16,860
message to me was very surprising how far you can go with just linear mapping

1418
01:42:16,870 --> 01:42:19,680
there's a really good linear transformation you

1419
01:42:19,710 --> 01:42:24,400
based on the existence of energy trying to rob

1420
01:42:24,400 --> 01:42:25,650
OK so

1421
01:42:25,660 --> 01:42:28,380
that's all

1422
01:42:40,260 --> 01:42:45,870
they are shown here

1423
01:42:45,880 --> 01:42:49,520
there is more

1424
01:42:52,950 --> 01:42:57,800
so just you you know how to perform in terms of the spectrum of classifiers

1425
01:42:57,800 --> 01:43:02,920
like decision trees and support vector machines are used to and we didn't compare with

1426
01:43:02,960 --> 01:43:06,080
that the idea was not to try and

1427
01:43:06,990 --> 01:43:10,820
this the state-of-the-art classification here was trying to say

1428
01:43:10,830 --> 01:43:12,930
if you're going to change nearest neighbour

1429
01:43:12,950 --> 01:43:17,980
which is why people are already especially in the computer vision community to the usually

1430
01:43:17,980 --> 01:43:21,910
should try and do it in a reasonable way that leverages the data you have

1431
01:43:21,910 --> 01:43:22,640
and it doesn't

1432
01:43:22,650 --> 01:43:27,250
so we should think this is a conditional statement this statement is if you're going

1433
01:43:27,250 --> 01:43:29,240
to do the nearest neighbour

1434
01:43:29,260 --> 01:43:31,610
i have some suggestions

1435
01:43:31,740 --> 01:43:34,010
i'm not trying to make a claim

1436
01:43:34,030 --> 01:43:37,800
one way or the other should or shouldn't use k nearest neighbour he's not

1437
01:43:43,650 --> 01:43:54,420
that's exactly what we initialize the matrix a to remember this is non convex objective

1438
01:43:54,420 --> 01:43:56,170
function and the doing locals are so

1439
01:43:56,580 --> 01:43:58,150
that always brings up the question

1440
01:43:58,640 --> 01:44:03,430
it lies in the answer is we just initialized with all of the obvious things

1441
01:44:04,140 --> 01:44:07,730
a lot of random starting point we just take the one that gives us the

1442
01:44:07,730 --> 01:44:12,030
best function so all these things are the identity matrix

1443
01:44:12,050 --> 01:44:20,330
the the whitening matrix just this color is similar than the data fisher discriminant projection

1444
01:44:22,450 --> 01:44:25,740
you know essentially whenever you can think of right

1445
01:44:25,760 --> 01:44:30,130
because it doesn't matter what you do is throw that the initial condition optimise

1446
01:44:30,140 --> 01:44:36,610
but it's a good question and also i should be very clear that how initialize

1447
01:44:36,640 --> 01:44:41,570
important always start any matrix you might be missing out on some good local optimum

1448
01:44:41,570 --> 01:44:43,390
would never find myself

1449
01:44:43,410 --> 01:44:44,640
have to try

1450
01:44:44,650 --> 01:44:54,830
these things like PCA and LDA and random things to make sure to explore space

1451
01:45:02,550 --> 01:45:05,980
the performance in the case where

1452
01:45:06,610 --> 01:45:11,640
and that's what you mean winding feature space

1453
01:45:11,660 --> 01:45:22,140
well i mean i don't know what it means feature space basically non-linear decision surfaces

1454
01:45:22,140 --> 01:45:26,400
here certainly on the right

1455
01:45:26,400 --> 01:45:30,360
i mean if you draw the boundaries between classes

1456
01:45:30,380 --> 01:45:31,400
in this

1457
01:45:31,400 --> 01:45:35,380
you use some of these methods to interpret your data

1458
01:45:35,380 --> 01:45:41,730
i least here you you you can you can relax from from the bullion assignment

1459
01:45:41,730 --> 01:45:48,500
see you can relax everything and find here you have these these axes which are

1460
01:45:48,500 --> 01:45:52,980
defined on the interval between minus one and one and he oversees have to have

1461
01:45:53,020 --> 01:45:54,650
the the

1462
01:45:54,710 --> 01:45:59,570
values minus one and one so the relaxation to and cut can be solved efficiently

1463
01:45:59,570 --> 01:46:04,630
encoded by itself is is NP hard it was proven by

1464
01:46:04,630 --> 01:46:11,730
my colleague of of jitendra malik

1465
01:46:11,750 --> 01:46:13,460
and you are probably the

1466
01:46:13,480 --> 01:46:17,210
the type of of images you have seen before

1467
01:46:17,210 --> 01:46:22,650
from malik shi and malik staples

1468
01:46:26,460 --> 01:46:31,920
so that concludes basically the modelling part in the trial i still had some of

1469
01:46:31,940 --> 01:46:34,570
the methods but i think from a learning point of view

1470
01:46:35,040 --> 01:46:41,270
this this sort of couples the full spectrum of what you what you would like

1471
01:46:41,270 --> 01:46:46,880
to discuss on one hand conceptually methods which are fairly well

1472
01:46:46,900 --> 01:46:55,280
located in in the in the in the ballpark of of learning problems like a

1473
01:46:55,280 --> 01:47:01,820
means of this expressing parametric distributional clustering some of methods which are fairly popular which

1474
01:47:02,280 --> 01:47:11,320
achieves the same think partitioning local image characteristics feature values in in in in k

1475
01:47:11,320 --> 01:47:14,840
different groups but which

1476
01:47:14,840 --> 01:47:23,160
which look not be addressable analyzable in the usual way when it comes to generalisation

1477
01:47:23,160 --> 01:47:28,710
that's and cut and one relation is the one between pairwise data clustering and k

1478
01:47:28,710 --> 01:47:34,420
means there you see through this mathematical equivalence how you can go from from what

1479
01:47:34,420 --> 01:47:38,760
i would call a generative model k means in the embedding space two model where

1480
01:47:38,770 --> 01:47:42,140
it's not so clear how you how you actually

1481
01:47:42,150 --> 01:47:44,710
would generate new data

1482
01:47:44,850 --> 01:47:48,970
and that's something we have to live in the future

1483
01:47:49,150 --> 01:47:51,860
in this section too

1484
01:47:52,300 --> 01:47:54,360
i was focusing on on

1485
01:47:54,380 --> 01:47:57,380
on some noise

1486
01:47:57,390 --> 01:48:04,500
insensitive ways of robust ways and i think the notion of robustness in in application

1487
01:48:04,500 --> 01:48:07,760
domains is mostly related to be

1488
01:48:07,760 --> 01:48:15,220
so of invariant to slide show of very very little variance too slight changes in

1489
01:48:15,220 --> 01:48:22,310
your data and he basically is is what kind of a kind of

1490
01:48:22,320 --> 01:48:28,420
right you are a generalization ability of the methods from one instance in a slightly

1491
01:48:28,420 --> 01:48:33,270
different instance so maybe an instance where the signal quote unquote is the same but

1492
01:48:33,270 --> 01:48:37,850
the noise is different and sometimes it's not so clear what is actually the signal

1493
01:48:37,850 --> 01:48:41,940
if i give you to tag images in one the tigers jumping in the lower

1494
01:48:41,940 --> 01:48:46,190
left corner and in the other it sleeping in the upper right corner of the

1495
01:48:46,190 --> 01:48:50,640
signal i would say is being a tiger being present in these two images but

1496
01:48:50,640 --> 01:48:55,890
it's not clear how you distinguish on on on on the level of individual image

1497
01:48:55,890 --> 01:48:59,380
blocks i you would distinguish signal from noise

1498
01:48:59,390 --> 01:49:04,640
and the challenge for in particular the past project i think it is

1499
01:49:04,770 --> 01:49:07,640
clarifying these issues because they

1500
01:49:07,640 --> 01:49:12,230
they are very deeply related to what we mean by learning

1501
01:49:12,260 --> 01:49:15,800
so candidate solutions should be typical

1502
01:49:15,820 --> 01:49:21,070
and maybe they have to be averages of partitioning and the method which i was

1503
01:49:21,070 --> 01:49:23,600
advocating for for four

1504
01:49:23,600 --> 01:49:29,960
for a number of years nice maximum entropy principle and you have a strategy to

1505
01:49:29,960 --> 01:49:36,390
search through the the solution space and that is the space of possible partitions and

1506
01:49:36,390 --> 01:49:43,440
you you accept partitions which with costs expected costs here of

1507
01:49:43,460 --> 01:49:50,100
h small of being smaller than the constant in this constant is parametrized by the

1508
01:49:50,100 --> 01:49:55,050
temperature t so for those of you coming from statistical physics they know

1509
01:49:56,020 --> 01:49:58,100
these relations in the others

1510
01:49:58,240 --> 01:50:06,260
just should imagine the teaser parameter which allows you to tune the quality of your

1511
01:50:06,260 --> 01:50:12,220
the expected quality of fuel fuel costs you you can do that by markov chain

1512
01:50:12,220 --> 01:50:14,500
monte carlo meaningful approximation two

1513
01:50:14,920 --> 01:50:22,730
methods in that context and here you have one of these algorithms and MCMC algorithm

1514
01:50:22,900 --> 01:50:26,970
and i think the essential essentially what you should basically

1515
01:50:26,980 --> 01:50:34,510
what you should basically take from this transparency is that the algorithm explores the universe

1516
01:50:34,510 --> 01:50:39,930
of politicians which have a certain characteristic in the characteristic here is that in terms

1517
01:50:39,930 --> 01:50:44,600
of these empirical because they are below a threshold

1518
01:50:44,640 --> 01:50:48,300
and if you if you believe that noise sort of

1519
01:50:48,310 --> 01:50:53,050
as to the vari ability of your politicians and the signal was always for the

1520
01:50:53,050 --> 01:50:57,930
same type of politician an averaging over different partitions might actually give you

1521
01:50:57,960 --> 01:51:01,380
something like a prototypical politicians

1522
01:51:01,390 --> 01:51:06,310
and that's exactly what happened in these pairwise clustering because in pairwise clustering also wanted

1523
01:51:06,310 --> 01:51:11,020
to find the partition and going in the embedding space where you can do k

1524
01:51:11,020 --> 01:51:19,110
means by mathematical equivalence you find these generative partition by the centroid in that space

1525
01:51:19,130 --> 01:51:24,980
that is basically the model which which which was underlying these equivalence between k means

1526
01:51:24,980 --> 01:51:26,780
and pairwise clustering

1527
01:51:26,810 --> 01:51:32,440
here you can do that just algorithmically by searching through many of these politicians and

1528
01:51:32,440 --> 01:51:37,010
whenever they fulfil the criteria and you take them into you extract the statistics of

1529
01:51:38,520 --> 01:51:50,190
so usually what to do is to maximize the entropy with additional free parameters that

1530
01:51:50,190 --> 01:51:54,340
sort of one of the approximation schemes and then you have an OEM like iteration

1531
01:51:54,500 --> 01:52:03,270
where you you iterate between where u u maximizes the entropy and basically what you

1532
01:52:03,270 --> 01:52:08,550
do is you you you you you fix some some statistics which you have

1533
01:52:08,770 --> 01:52:14,760
four measuring how how costly it is for one of your objects being assigned to

1534
01:52:14,800 --> 01:52:20,940
particular class and the the the assignment function and that is very similar to what

1535
01:52:20,940 --> 01:52:24,930
we have discussed before in terms of k means

1536
01:52:24,970 --> 01:52:32,350
what you see is sometimes surprising you get this quote unquote phase transitions here you

1537
01:52:32,350 --> 01:52:34,090
have the data density

1538
01:52:34,110 --> 01:52:39,860
and when you have very crude approximations down here this is the the inverse temperature

1539
01:52:39,860 --> 01:52:45,250
better which is also often used then you will estimate six different classes and they

1540
01:52:45,250 --> 01:52:51,060
are all located in the same position and that's here around o point probably o

1541
01:52:51,060 --> 01:52:53,130
point three o point four

1542
01:52:53,220 --> 01:52:58,560
and then all of a sudden when you could use your approximation quality you you

1543
01:52:58,560 --> 01:53:05,900
lower the temperature and dew constraint on the acceptable quality of the solution is is

1544
01:53:05,900 --> 01:53:10,930
is is is is low and then you you you force the

1545
01:53:10,930 --> 01:53:17,130
the sort the the statistics to split up in form of the centroids are on

1546
01:53:17,130 --> 01:53:22,160
this branch in one centroid is located in here and a few lower again this

1547
01:53:22,160 --> 01:53:29,010
threshold in these approximation scheme you get further the break-ups until you have six different

1548
01:53:29,040 --> 01:53:32,150
six different clusters forty six different modes

1549
01:53:32,150 --> 01:53:33,780
log linear models

1550
01:53:35,870 --> 01:53:41,940
my point of view on this is that it's logistic regression

1551
01:53:51,660 --> 01:53:57,540
and structured output

1552
01:53:58,360 --> 01:54:02,630
in logistic regression

1553
01:54:02,690 --> 01:54:05,910
the input is an expert

1554
01:54:07,340 --> 01:54:09,010
a real valued vector

1555
01:54:09,050 --> 01:54:11,240
and the output y

1556
01:54:12,400 --> 01:54:14,080
is binary

1557
01:54:17,780 --> 01:54:21,670
we want to we want to solve a much more general supervised learning problems

1558
01:54:21,690 --> 01:54:23,390
we want to

1559
01:54:24,510 --> 01:54:26,870
x to belong to

1560
01:54:26,890 --> 01:54:28,920
any is any

1561
01:54:28,970 --> 01:54:31,470
space acts

1562
01:54:31,470 --> 01:54:35,200
and also in fact why to belong to

1563
01:54:35,250 --> 01:54:38,280
any space y

1564
01:54:38,300 --> 01:54:41,980
and so example

1565
01:54:43,390 --> 01:54:46,200
could be a sentence

1566
01:54:49,750 --> 01:54:56,000
he sat on the mat

1567
01:54:56,000 --> 01:54:57,870
and why

1568
01:54:57,890 --> 01:54:58,730
it would be

1569
01:54:58,750 --> 01:55:01,970
maybe a list of parts of speech

1570
01:55:05,620 --> 01:55:08,890
the proposition

1571
01:55:10,770 --> 01:55:13,530
and now

1572
01:55:14,660 --> 01:55:16,280
and so

1573
01:55:16,610 --> 01:55:20,640
this is a supervised learning problem is the classifier learning problem

1574
01:55:20,650 --> 01:55:25,180
we have training data which is consists of pairs and the tax is an english

1575
01:55:26,140 --> 01:55:27,800
and the y

1576
01:55:28,880 --> 01:55:31,640
a sequence of

1577
01:55:31,650 --> 01:55:37,110
part of speech tags and part of speech tagger the label for the category of

1578
01:55:37,170 --> 01:55:43,430
word and so this is a natural language processing problems that is easier than say

1579
01:55:44,480 --> 01:55:50,650
figuring out the complete grammatical structure of the sentence is really figuring out the grammatical

1580
01:55:50,650 --> 01:55:52,230
role of each word

1581
01:55:52,290 --> 01:55:59,330
without figuring out how the grammatical rules interplay with each other and

1582
01:56:02,480 --> 01:56:04,820
you know in

1583
01:56:05,860 --> 01:56:07,770
machine learning so that

1584
01:56:07,790 --> 01:56:10,990
search on generalizing on

1585
01:56:11,080 --> 01:56:18,770
two class output to multiclass output and there's also regression methods where you can have

1586
01:56:18,770 --> 01:56:22,860
a real valued output but we want to go far beyond this we want to

1587
01:56:23,820 --> 01:56:26,140
inputs and outputs to be

1588
01:56:26,140 --> 01:56:30,360
no real valued this input is not even in fact

1589
01:56:30,390 --> 01:56:33,230
because different inputs can have both

1590
01:56:33,240 --> 01:56:34,450
different lengths

1591
01:56:34,540 --> 01:56:37,730
and then this number output

1592
01:56:37,740 --> 01:56:39,980
it is more

1593
01:56:40,680 --> 01:56:42,830
not more

1594
01:56:42,830 --> 01:56:47,610
this is not a multiclass there is in some fixed number of labels and you're

1595
01:56:47,610 --> 01:56:53,770
choosing one labelled in that fixed number there's does much

1596
01:56:53,770 --> 01:56:55,320
the so

1597
01:56:55,330 --> 01:56:56,390
o here

1598
01:56:56,400 --> 01:56:58,520
the set x

1599
01:56:58,570 --> 01:56:59,730
might be

1600
01:56:59,740 --> 01:57:03,230
the set of words

1601
01:57:03,240 --> 01:57:11,630
and then here's the start the the set of all sequences of arbitrary length and

1602
01:57:11,630 --> 01:57:14,150
among the same thing for y

1603
01:57:14,170 --> 01:57:17,010
it's a set of tags

1604
01:57:17,050 --> 01:57:20,130
star so this is language notation

1605
01:57:22,110 --> 01:57:25,200
and the the only

1606
01:57:25,230 --> 01:57:27,890
assumption really i'm going to make about

1607
01:57:29,410 --> 01:57:30,940
is this why

1608
01:57:30,950 --> 01:57:33,040
it is much

1609
01:57:34,870 --> 01:57:37,770
he is an assumed finite number

1610
01:57:37,770 --> 01:57:42,770
so technically this is not even finite countable because of my

1611
01:57:42,780 --> 01:57:47,510
the tax can be of arbitrary length but supposing that i limit my sentences say

1612
01:57:47,510 --> 01:57:52,560
to be twenty words long then the y set becomes finite and that's the only

1613
01:57:52,560 --> 01:57:57,240
thing that i i need to assume much

1614
01:57:58,180 --> 01:57:59,530
more so

1615
01:57:59,540 --> 01:58:10,750
actually taxes the sentence

1616
01:58:10,780 --> 01:58:15,500
that it's rather pretty important to be able to do that for me to use

1617
01:58:15,690 --> 01:58:19,120
must imprecise terminology here so in this case so

1618
01:58:19,130 --> 01:58:20,900
one input

1619
01:58:20,910 --> 01:58:24,140
is one sentence and one output

1620
01:58:24,150 --> 01:58:26,100
is one

1621
01:58:27,130 --> 01:58:33,640
and the then that label is a string of sequence tags

1622
01:58:49,340 --> 01:58:53,040
so i was the question so

1623
01:58:53,080 --> 01:58:55,770
what i'm going to what i'm going to explain

1624
01:58:55,820 --> 01:59:00,970
in just one minute is how to represent x and y

1625
01:59:02,080 --> 01:59:08,270
as as part of everything that's going to be able to represent things like how

1626
01:59:08,270 --> 01:59:12,470
many times is of particular in fact is that

1627
01:59:12,490 --> 01:59:14,390
the question you asked

1628
01:59:15,580 --> 01:59:19,350
OK so i think i'll be able to answer that in a minute

1629
01:59:19,380 --> 01:59:25,270
the cell

1630
01:59:31,380 --> 01:59:35,900
his the the models

1631
01:59:35,950 --> 01:59:38,380
so probability of y

1632
01:59:38,390 --> 01:59:40,330
given x

1633
01:59:42,900 --> 01:59:47,890
for some reason i'm changing my notation instead of using data for the parameters i'm

1634
01:59:47,890 --> 01:59:52,030
using w for the parameters w stands for weights

1635
01:59:52,040 --> 01:59:55,030
so probably y given x and w

1636
01:59:56,700 --> 02:00:03,530
there's a there's the normalizing which underscores the axon w

1637
02:00:03,540 --> 02:00:06,280
i'll talk about that in a minute and then

1638
02:00:07,500 --> 02:00:09,490
i have an exponential

1639
02:00:10,870 --> 02:00:13,810
some of the j

1640
02:00:18,870 --> 02:00:20,430
so this is

1641
02:00:20,440 --> 02:00:23,010
fj capital fj

1642
02:00:24,770 --> 02:00:27,410
x and y

1643
02:00:29,080 --> 02:00:34,640
what i'm putting up this expression now so as to show you immediately

1644
02:00:34,690 --> 02:00:39,840
how this is this looks a lot like logistic regression so in the this is

1645
02:00:39,840 --> 02:00:41,450
an exponential

1646
02:00:41,470 --> 02:00:43,890
of a linear function

1647
02:00:44,830 --> 02:00:47,570
the probability is

1648
02:00:47,630 --> 02:00:52,500
essentially the exponential of linear function so it looks a lot like the just regression

1649
02:00:52,770 --> 02:00:57,620
and the just regression is going to turn out to be a special case of

1650
02:00:57,620 --> 02:01:02,970
the liquid of one composition and a solid the other composition what how i start

1651
02:01:02,970 --> 02:01:04,860
with two different

1652
02:01:04,880 --> 02:01:08,950
this is forty percent and this is thirty five percent

1653
02:01:09,790 --> 02:01:12,930
forty percent nickel thirty five percent nickel

1654
02:01:12,930 --> 02:01:17,370
these end members of the same there's only one variable left it's the relative amounts

1655
02:01:17,370 --> 02:01:20,360
of these because i need conservation of mass because at the end of the day

1656
02:01:20,360 --> 02:01:23,110
if i add up all the nickel in the liquid and all the nickel in

1657
02:01:23,110 --> 02:01:25,800
the solid it still has to be

1658
02:01:25,860 --> 02:01:28,770
forty percent nickel

1659
02:01:28,790 --> 02:01:32,150
only in here it's forty five percent nickel

1660
02:01:32,190 --> 02:01:33,680
up here

1661
02:01:33,720 --> 02:01:35,620
thirty two percent nickel

1662
02:01:35,700 --> 02:01:39,800
so i've got a vernier scale i got a sliding scale that i can move

1663
02:01:39,820 --> 02:01:43,250
so that if i have something that's thirty five percent of the words god

1664
02:01:43,270 --> 02:01:47,760
it needs less of this phase so the thirty five percent is going to end

1665
02:01:47,760 --> 02:01:49,220
up doing this

1666
02:01:49,270 --> 02:01:52,860
whereas the forty percent is going to end up doing this

1667
02:01:52,870 --> 02:01:58,330
the relative amounts this this line that tells me what the concentration of the solid

1668
02:01:58,340 --> 02:02:01,640
and liquid at a temperature is called the timeline

1669
02:02:01,690 --> 02:02:07,440
so the timeline defines the composition and members the liquid and solid in that two

1670
02:02:07,440 --> 02:02:08,790
phase regime

1671
02:02:08,840 --> 02:02:12,360
and we can take all of this that have been talking about and we can

1672
02:02:12,370 --> 02:02:14,210
we can clarify it

1673
02:02:14,270 --> 02:02:19,720
we can clarify it in terms of a simple rule that captures the notion of

1674
02:02:19,720 --> 02:02:25,680
phase separation and conservation of mass and that's called the level of the world

1675
02:02:25,690 --> 02:02:27,440
the level

1676
02:02:27,490 --> 02:02:29,910
the liberal answers the question

1677
02:02:29,910 --> 02:02:32,480
how much of each phrase is present

1678
02:02:32,500 --> 02:02:34,770
the relative amount

1679
02:02:34,790 --> 02:02:37,030
the relative amount

1680
02:02:39,590 --> 02:02:43,530
of each phase

1681
02:02:43,580 --> 02:02:45,240
in the two phase regime

1682
02:02:45,270 --> 02:02:50,020
in the two phase region

1683
02:02:50,040 --> 02:02:55,180
two phase region so for example i think and that's when i got this set

1684
02:02:55,180 --> 02:02:59,330
up here it is so here we are with the forty percent nickel is x

1685
02:02:59,500 --> 02:03:04,290
forty percent nickel and copper we've drop down into the two phase regime draw the

1686
02:03:04,290 --> 02:03:09,700
timeline and the thailand tells us that the liquid the liquid will put the proportion

1687
02:03:10,020 --> 02:03:14,110
follows so this is forty percent nickel

1688
02:03:14,140 --> 02:03:16,470
at all liquid

1689
02:03:16,490 --> 02:03:21,350
right now we drop the temperature this is thirteen hundred now we drop down to

1690
02:03:21,350 --> 02:03:25,510
twelve fifty and we know according to the phase diagram we're going to end up

1691
02:03:25,510 --> 02:03:30,710
with the liquid and solid and furthermore from the timeline the timeline tells us that

1692
02:03:30,710 --> 02:03:34,380
the solid is going to be forty five percent nickel

1693
02:03:34,490 --> 02:03:38,040
and the liquid is going to be thirty two percent because the question is what

1694
02:03:38,040 --> 02:03:43,870
are the relative values and interests but shown right up there on the on the

1695
02:03:43,880 --> 02:03:46,750
transparency that percent

1696
02:03:46,830 --> 02:03:49,950
of the liquid is equal to the

1697
02:03:50,010 --> 02:03:56,840
in this case the number forty five minus the individual amount at the

1698
02:03:56,850 --> 02:04:01,520
at the concentration of the bulk minus the divided by the end members of the

1699
02:04:01,520 --> 02:04:07,800
timeline will multiply that by one hundred percent and that gives us the value of

1700
02:04:07,800 --> 02:04:09,780
thirty eight percent

1701
02:04:09,840 --> 02:04:11,690
present as liquid

1702
02:04:12,280 --> 02:04:16,570
and why is it called the ever rule because if you look at the timeline

1703
02:04:16,640 --> 02:04:19,220
here's where we are we're at this value

1704
02:04:19,240 --> 02:04:21,680
which is shown in the phase diagram

1705
02:04:22,580 --> 02:04:25,130
xe this is our forty percent

1706
02:04:25,170 --> 02:04:30,150
and the amount of the liquid is x and the amount of the solid is

1707
02:04:31,050 --> 02:04:34,610
so what i'm asking how much liquid is present

1708
02:04:34,650 --> 02:04:39,460
i choose the amount opposite is as you do on on ceasar teeter totter in

1709
02:04:39,460 --> 02:04:45,920
other words to calculate what the forces here you take the amount between the fulcrum

1710
02:04:45,920 --> 02:04:50,840
and the opposite side divided by the total length so this is really this is

1711
02:04:52,070 --> 02:04:57,990
why xe over x y gives us liquid

1712
02:04:58,040 --> 02:05:02,210
and then the complement gives the amount of solid so you see that worked out

1713
02:05:02,210 --> 02:05:04,520
for you at

1714
02:05:04,710 --> 02:05:08,240
the transparency in this works for any two faces regime

1715
02:05:08,290 --> 02:05:09,930
so whenever something

1716
02:05:09,970 --> 02:05:15,630
drops in with two phase regime disproportion nation has to occur and the level rule

1717
02:05:15,630 --> 02:05:20,090
will give you the relative amounts the timeline tells you the composition

1718
02:05:20,130 --> 02:05:23,140
and the liberal tells you the relative amount

1719
02:05:23,190 --> 02:05:26,700
two things i want this to be like you're you know the and the manchurian

1720
02:05:26,700 --> 02:05:29,520
candidate you know this is your q

1721
02:05:29,570 --> 02:05:33,470
yes if you from now on if you here two phase you just go

1722
02:05:35,510 --> 02:05:37,140
the liberal liberal

1723
02:05:37,150 --> 02:05:41,530
one of the members taiwan thailand liberal thailand liberal

1724
02:05:41,550 --> 02:05:44,550
what every c two phase ricci

1725
02:05:44,600 --> 02:05:46,720
and else keep you out of trouble

1726
02:05:46,750 --> 02:05:48,410
he very troubling me

1727
02:05:48,420 --> 02:05:51,260
many times i get into trouble i just think of the living world

1728
02:05:51,320 --> 02:05:54,020
i'm going to try to so

1729
02:05:54,130 --> 02:05:58,210
now let's look at the second case second cases type two

1730
02:05:58,870 --> 02:06:00,640
type two phase diagram

1731
02:06:00,650 --> 02:06:07,090
type two phase diagram in contrast to complete solubility type two phase diagrams can is

1732
02:06:07,090 --> 02:06:08,780
characterized by

1733
02:06:08,840 --> 02:06:11,460
system that has partial

1734
02:06:11,480 --> 02:06:14,730
partial or limited

1735
02:06:14,780 --> 02:06:18,170
four limited solubility

1736
02:06:18,270 --> 02:06:22,350
so when you try to make a and b they don't mix in all proportions

1737
02:06:22,390 --> 02:06:24,680
the only makes up to a certain point

1738
02:06:24,790 --> 02:06:27,710
so that's like a solubility limit or

1739
02:06:27,760 --> 02:06:35,690
emissivity admissibility or solubility synonymous this leads to admissibility gap a range of composition over

1740
02:06:35,690 --> 02:06:40,670
which you can mix the two them form a homogeneous solution and in this case

1741
02:06:40,670 --> 02:06:42,900
there's no change of state

1742
02:06:42,930 --> 02:06:45,190
no change of state

1743
02:06:45,210 --> 02:06:49,750
so that's you always liquid

1744
02:06:49,890 --> 02:06:53,500
i always liquid or always solid

1745
02:06:53,520 --> 02:06:55,800
always saw

1746
02:06:55,820 --> 02:07:00,910
so let's look at the phase diagram for

1747
02:07:00,910 --> 02:07:03,190
one of these types of systems

1748
02:07:05,550 --> 02:07:09,930
planning composition across the abscissa

1749
02:07:09,930 --> 02:07:12,180
the ordnance temperature

1750
02:07:12,200 --> 02:07:14,530
and this gives us a

1751
02:07:14,540 --> 02:07:17,110
so climate phase diagram

1752
02:07:17,140 --> 02:07:19,240
this inclined phase diagram

1753
02:07:19,270 --> 02:07:23,110
so above we have single phase so this is either

1754
02:07:23,120 --> 02:07:26,480
just a single phase

1755
02:07:26,540 --> 02:07:28,550
and this is

1756
02:07:28,620 --> 02:07:29,920
dual phase

1757
02:07:32,900 --> 02:07:36,470
you'll find a single phase dual face so for example this could be

1758
02:07:36,490 --> 02:07:39,610
as solids this could be all liquid

1759
02:07:39,620 --> 02:07:41,940
and then in here we have two liquids

1760
02:07:41,960 --> 02:07:46,660
or it could be all solid

1761
02:07:46,690 --> 02:07:48,480
and inside here we have

1762
02:07:48,540 --> 02:07:53,760
two solids when it comes to solid solutions people use greek letters and called itself

1763
02:07:53,770 --> 02:07:55,580
and beta

1764
02:07:55,620 --> 02:07:57,920
so this is the coexistence curve

1765
02:07:57,930 --> 02:08:07,090
for the solubility limit what's the coexistence curve the coexistence curve is liquid is in

1766
02:08:07,090 --> 02:08:08,910
equilibrium with liquid one

1767
02:08:08,930 --> 02:08:10,170
and liquid two

1768
02:08:10,180 --> 02:08:13,020
in other words if you drop into this two phase regime

1769
02:08:13,040 --> 02:08:17,910
what happens as soon as you hear two phase

1770
02:08:21,000 --> 02:08:25,410
the tile i will give you the concentrations at the end or solid goes to

1771
02:08:25,410 --> 02:08:29,770
alpha plus b to both of these are solid solutions

1772
02:08:29,830 --> 02:08:31,270
let's take a look at

1773
02:08:31,290 --> 02:08:35,240
that some of these

1774
02:08:35,250 --> 02:08:40,080
this is gold nickel at low temperatures look what happens

1775
02:08:40,080 --> 02:08:43,790
classification that told it's going to be very easy after we've done that the initial

1776
02:08:43,790 --> 02:08:47,180
investment in exponential family it's all going to be

1777
02:08:47,200 --> 02:08:49,470
fairly straightforward

1778
02:08:49,470 --> 02:08:53,450
so we get conditionally multinomial models like given x

1779
02:08:53,490 --> 02:08:57,020
thing that the feature map if you think another feature map will ever get the

1780
02:08:57,020 --> 02:09:01,580
slightly different looking casting process multiclass more

1781
02:09:01,600 --> 02:09:05,450
people have actually written papers about that with a simple let's assume now that somehow

1782
02:09:05,450 --> 02:09:10,040
my labels are related to each other and maybe even want to learn is

1783
02:09:10,100 --> 02:09:15,620
and then when you would just be learning some kernel on the wise

1784
02:09:15,640 --> 02:09:20,180
so for instance you remember the error correcting output codes well the people use some

1785
02:09:20,180 --> 02:09:25,100
feature map of the wires into some aircraft now code you take product that we

1786
02:09:25,140 --> 02:09:26,750
what to get

1787
02:09:26,790 --> 02:09:28,720
the key difference is that here

1788
02:09:28,730 --> 02:09:31,790
you don't need to learn the code you know exactly what you want to get

1789
02:09:31,830 --> 02:09:34,520
as an answer for the inner products

1790
02:09:34,540 --> 02:09:39,620
sometimes they are appropriate output code may be more efficient to implement

1791
02:09:39,660 --> 02:09:45,330
but there is no there's reason why that shouldn't just be fine

1792
02:09:45,350 --> 02:09:47,640
OK so the optimisation problem is actually

1793
02:09:47,660 --> 02:09:48,580
very nice

1794
02:09:48,580 --> 02:09:50,450
the convex problem

1795
02:09:50,470 --> 02:09:52,120
they just looking

1796
02:09:52,140 --> 02:09:56,080
two the expansion coefficients these apples you optimize all

1797
02:09:56,120 --> 02:10:01,140
well that's wrong it's not the dual space is actually also called in primal space

1798
02:10:02,310 --> 02:10:03,990
removed that that's embarrassing

1799
02:10:07,370 --> 02:10:10,750
what i meant was problem space in this case it in some cases

1800
02:10:10,770 --> 02:10:13,160
actually this function for x and y

1801
02:10:13,200 --> 02:10:16,180
it's easy to compute

1802
02:10:16,230 --> 02:10:18,330
human but on from mossley

1803
02:10:18,350 --> 02:10:21,600
he was talking about bag of words models

1804
02:10:21,620 --> 02:10:25,200
in this case he has a very sparse profits and why

1805
02:10:25,250 --> 02:10:27,270
so it's a really good idea too

1806
02:10:27,310 --> 02:10:29,520
optimized was seated directly

1807
02:10:29,520 --> 02:10:34,250
or sometimes the things like string kernels to philippe was talking about that there are

1808
02:10:34,250 --> 02:10:36,450
also other string kernels

1809
02:10:36,490 --> 02:10:41,270
and sometimes these inner products can be computed very efficiently correctly

1810
02:10:41,370 --> 02:10:42,680
if you have a a very

1811
02:10:43,310 --> 02:10:47,540
compact storage of the data

1812
02:10:47,680 --> 02:10:49,790
so there's is nothing inherently

1813
02:10:49,810 --> 02:10:52,890
the same user offers rather than using the

1814
02:10:52,930 --> 02:10:54,700
it just really depends on

1815
02:10:54,870 --> 02:10:58,540
a specific problem

1816
02:11:00,250 --> 02:11:03,180
now is a couple of pitfalls

1817
02:11:04,000 --> 02:11:11,140
trooper series only approximated by the mode of the posterior

1818
02:11:11,950 --> 02:11:15,540
remember i told you will be doing and that's the story rather than the true

1819
02:11:15,540 --> 02:11:17,560
bayesian estimate

1820
02:11:17,640 --> 02:11:19,810
and sometimes that can be good

1821
02:11:19,830 --> 02:11:22,350
in some cases it's not such a bright idea

1822
02:11:22,390 --> 02:11:28,290
and that case you would want to use something like expectation propagation by thomas minka

1823
02:11:28,350 --> 02:11:31,830
but has written very nice paper on recently

1824
02:11:31,850 --> 02:11:36,680
so hard to read but it's a nice paper

1825
02:11:36,850 --> 02:11:38,390
you could also use some

1826
02:11:38,390 --> 02:11:40,810
so for instance if you read write for newspapers

1827
02:11:40,830 --> 02:11:42,730
he always use sampling

1828
02:11:42,850 --> 02:11:44,730
using a very similar

1829
02:11:46,950 --> 02:11:51,310
and the variables might look different but essentially the same

1830
02:11:51,330 --> 02:11:56,970
i mean he actually invented gaussianprocess classification

1831
02:11:58,220 --> 02:12:04,410
now the regression and will save the conditional random fields for tomorrow

1832
02:12:04,430 --> 02:12:07,680
so we might even finished five minutes early

1833
02:12:07,680 --> 02:12:12,160
now talk about the gets can have to just that

1834
02:12:12,180 --> 02:12:17,410
so if you haven't heard those converts that just means well i'm assuming that my

1835
02:12:17,410 --> 02:12:19,810
additive noise is the same everywhere

1836
02:12:19,830 --> 02:12:26,040
and heteroskedastic means well and assuming that my noise might change with the location

1837
02:12:26,080 --> 02:12:27,410
this is different from

1838
02:12:27,430 --> 02:12:29,250
i'm not really

1839
02:12:29,490 --> 02:12:32,850
sure at one location is another what my that it might be

1840
02:12:32,850 --> 02:12:38,370
this that might depend on what i've seen and often four

1841
02:12:42,660 --> 02:12:46,350
let's just call optimisation problem

1842
02:12:46,370 --> 02:12:49,270
must be you can kind of getting sick of the problem by another section it

1843
02:12:49,270 --> 02:12:50,560
so many times

1844
02:12:50,680 --> 02:12:52,390
so this is

1845
02:12:52,450 --> 02:12:54,350
i mean

1846
02:12:54,390 --> 02:12:55,870
now x and y

1847
02:12:55,890 --> 02:12:58,020
this is the normalisation term

1848
02:12:58,020 --> 02:12:59,370
and here's my

1849
02:12:59,370 --> 02:13:02,410
normal prior

1850
02:13:02,430 --> 02:13:06,700
now the key differences that before we had a discrete domain renault

1851
02:13:06,730 --> 02:13:07,850
so nations

1852
02:13:08,430 --> 02:13:10,390
we've got a continuous domain

1853
02:13:10,430 --> 02:13:11,540
it's our

1854
02:13:11,560 --> 02:13:16,250
written by BR to the a five vector valued regression

1855
02:13:16,430 --> 02:13:20,560
whatever conditionally normal distribution of y given x

1856
02:13:20,600 --> 02:13:24,750
well nothing easier than that the smell of the exponential family

1857
02:13:24,810 --> 02:13:27,140
we take the log partition function

1858
02:13:27,160 --> 02:13:29,000
do you think next

1859
02:13:29,160 --> 02:13:32,120
and this one is equal to compute in closed form

1860
02:13:32,160 --> 02:13:33,500
this also vision

1861
02:13:33,520 --> 02:13:35,810
remember that

1862
02:13:35,850 --> 02:13:37,680
the equation that had

1863
02:13:38,960 --> 02:13:40,160
the lecture

1864
02:13:40,180 --> 02:13:45,620
involved normal distribution you get your theta one and theta two is this big me

1865
02:13:46,450 --> 02:13:47,910
you want to compute what's

1866
02:13:47,970 --> 02:13:50,750
they just plug it in here

1867
02:13:53,410 --> 02:13:55,200
what we do is we assume that

1868
02:13:55,390 --> 02:13:58,870
if x and y is just wrong from august process you can go with the

1869
02:14:02,930 --> 02:14:05,830
OK now let's look at the the standard model for we we assume that the

1870
02:14:05,830 --> 02:14:08,290
variance is fixed

1871
02:14:08,290 --> 02:14:12,160
this is possibly also member of the exponential family are just fixing one parameter it's

1872
02:14:12,160 --> 02:14:16,200
still a member of the exponential family for one

1873
02:14:18,350 --> 02:14:22,700
so it's just one where we have some coefficients here times y and this quadratic

1874
02:14:23,620 --> 02:14:25,890
as you can see what i said was UBX

1875
02:14:25,990 --> 02:14:28,850
they get different means

1876
02:14:30,020 --> 02:14:36,390
quadratic function has different maximum depending on what your fixes

1877
02:14:36,410 --> 02:14:38,450
so thanks for fixing y

1878
02:14:38,470 --> 02:14:45,200
the y times for fixed and y squared nature three statistical parameter

1879
02:14:45,290 --> 02:14:48,580
so that you can do

1880
02:14:48,580 --> 02:14:50,390
but the way that some papers were

1881
02:14:51,220 --> 02:14:53,250
use this experience model

1882
02:14:53,250 --> 02:14:56,060
and then try estimating the variance

1883
02:14:56,120 --> 02:15:00,470
and i sometimes get the parameterisation wrong and get rather of

1884
02:15:00,540 --> 02:15:01,720
procedures which

1885
02:15:01,730 --> 02:15:04,720
they have no guarantees for convergence of the variance system

1886
02:15:04,770 --> 02:15:08,660
if you set up this way and then optimize was two

1887
02:15:08,720 --> 02:15:13,040
you will actually get the nice convex problem

1888
02:15:14,370 --> 02:15:15,160
and if you

1889
02:15:15,180 --> 02:15:18,720
to some very boring and straightforward algebra

1890
02:15:18,810 --> 02:15:24,160
my take page my take page but it's not terribly enlightening just sit down to

1891
02:15:24,160 --> 02:15:25,100
the masses

1892
02:15:25,160 --> 02:15:28,770
will get exactly the standard process regression model

1893
02:15:28,770 --> 02:15:30,560
now see the answer

1894
02:15:30,580 --> 02:15:34,930
this is really not much we can learn from doing the algebra me again you

1895
02:15:34,930 --> 02:15:39,430
might want to do it once in your life but never again

1896
02:15:39,500 --> 02:15:40,660
so what you do this

1897
02:15:40,720 --> 02:15:43,930
let's just assume we have some training data

1898
02:15:43,970 --> 02:15:46,790
i want to secretion that

1899
02:15:46,850 --> 02:15:48,160
so you can check

1900
02:15:48,220 --> 02:15:50,060
the mean parameters

