1
00:00:00,000 --> 00:00:02,820
and then you have the charge density

2
00:00:02,850 --> 00:00:04,250
here so

3
00:00:04,260 --> 00:00:05,970
how the particles

4
00:00:05,990 --> 00:00:09,660
in the in the nucleus and you see that it's not the shop to face

5
00:00:09,720 --> 00:00:12,940
but it's something she was

6
00:00:12,960 --> 00:00:17,520
you can so this is one way you can transfer particles so for example if

7
00:00:17,560 --> 00:00:22,260
you have project and which is the product the interaction then it's a neutron in

8
00:00:22,260 --> 00:00:23,540
the output

9
00:00:23,560 --> 00:00:26,250
due to and then pretend

10
00:00:26,260 --> 00:00:28,170
and the data can be

11
00:00:28,550 --> 00:00:33,390
this is a scatter plot

12
00:00:33,410 --> 00:00:35,810
and what we do also

13
00:00:35,810 --> 00:00:40,580
is gamma spectroscopy so then to excite the nucleus

14
00:00:40,610 --> 00:00:42,390
the way you want either by

15
00:00:42,400 --> 00:00:43,810
by projectile

16
00:00:43,810 --> 00:00:47,610
and or you can do fission

17
00:00:47,630 --> 00:00:50,320
and then you look at its decay

18
00:00:50,330 --> 00:00:53,630
so if we need photons and they

19
00:00:54,240 --> 00:00:58,490
for example in germany in particular

20
00:00:58,500 --> 00:01:03,190
this is a typical spectral so you have here is a member of typically four

21
00:01:03,190 --> 00:01:05,490
tons depending on the energy

22
00:01:05,510 --> 00:01:07,440
in may gary caldwell

23
00:01:07,460 --> 00:01:09,700
and you see some things

24
00:01:10,100 --> 00:01:15,860
you see that it's very spatial spectrum this one correspond to states

25
00:01:15,880 --> 00:01:21,980
that are that correspond to rotation about very simple very different traditional band

26
00:01:21,990 --> 00:01:25,230
that's why seems to be equally spaced here

27
00:01:25,250 --> 00:01:27,910
so you have a very deformed nuclei

28
00:01:27,940 --> 00:01:30,290
one is the ratio of the two

29
00:01:30,310 --> 00:01:33,830
other ones which emit photons

30
00:01:33,850 --> 00:01:36,050
this is the interpretation

31
00:01:36,050 --> 00:01:39,650
so then when you have these these peaks

32
00:01:39,670 --> 00:01:42,690
you have the energy and intensity so

33
00:01:42,700 --> 00:01:44,160
you can

34
00:01:44,190 --> 00:01:45,370
i mean it

35
00:01:45,470 --> 00:01:49,990
the labels can so for its nucleus we have these little kids so this is

36
00:01:49,990 --> 00:01:52,720
an example of late two

37
00:01:52,730 --> 00:01:54,810
so it's just an example don't

38
00:01:54,830 --> 00:01:57,370
don't look at the numbers you have the

39
00:01:57,390 --> 00:02:00,530
here the energy and here you have

40
00:02:00,540 --> 00:02:02,340
the the transition

41
00:02:02,370 --> 00:02:05,880
so it's really look like their code in the street

42
00:02:05,890 --> 00:02:06,880
it's true

43
00:02:06,890 --> 00:02:10,170
it's a good picture it's very good affinities

44
00:02:10,190 --> 00:02:14,370
all the nuclei do have different levels schemes

45
00:02:17,030 --> 00:02:22,230
these are some typical nuclei just to see that there are different these limits himself

46
00:02:22,230 --> 00:02:23,160
to be able better

47
00:02:23,270 --> 00:02:25,320
i'm not completely different

48
00:02:25,340 --> 00:02:28,470
from somalia and one hundred forty eight

49
00:02:28,480 --> 00:02:32,060
and getting the one hundred sixty

50
00:02:32,870 --> 00:02:34,620
you see that here

51
00:02:35,310 --> 00:02:37,820
you can see two things first

52
00:02:37,830 --> 00:02:42,550
they are different because they are equally spaced at the beginning and here

53
00:02:42,640 --> 00:02:47,060
it will see that correspond to different shapes of the nuclear

54
00:02:47,730 --> 00:02:51,660
you still feel that they are organizing bands

55
00:02:51,690 --> 00:02:55,410
it's what we do with experimentally so theoreticians do

56
00:02:55,440 --> 00:02:59,610
when have some very big transitions between states

57
00:02:59,920 --> 00:03:04,890
we say that they do do correspond to the same structure so that we associated

58
00:03:06,060 --> 00:03:07,250
so that's why

59
00:03:07,260 --> 00:03:11,280
you can see in different nuclei different

60
00:03:11,300 --> 00:03:15,520
sometimes it many transition on unleashing because the experiment

61
00:03:15,540 --> 00:03:17,980
you don't see all the transition so

62
00:03:17,990 --> 00:03:20,510
even in the well known nuclear

63
00:03:20,530 --> 00:03:24,840
a lot a lot of new transition can be discovered

64
00:03:24,870 --> 00:03:28,920
especially is a difficult one

65
00:03:31,950 --> 00:03:34,750
i will

66
00:03:34,770 --> 00:03:39,800
again you some applications of these nuclear science

67
00:03:39,810 --> 00:03:46,090
because it's i think it's something very important in all the cells after we keep

68
00:03:46,090 --> 00:03:50,010
you i'm going to talk about

69
00:03:50,870 --> 00:03:55,880
fundamental physics what we look for fundamental physics

70
00:03:55,900 --> 00:04:00,540
but nuclear physics has made a social contributions to different

71
00:04:00,540 --> 00:04:03,240
part of the society for example

72
00:04:03,250 --> 00:04:05,400
in energy production

73
00:04:05,420 --> 00:04:09,910
so for example the electricity generation

74
00:04:10,100 --> 00:04:15,040
many research are made either fission or fusion

75
00:04:15,050 --> 00:04:19,900
on fish and the new research concerned new generations of

76
00:04:20,940 --> 00:04:24,050
new fuel cycles for example the

77
00:04:24,560 --> 00:04:27,540
to look at the story embedded cycles

78
00:04:27,550 --> 00:04:28,700
it's a

79
00:04:29,520 --> 00:04:31,390
after the research

80
00:04:31,420 --> 00:04:32,730
you have

81
00:04:32,740 --> 00:04:38,610
this reduction by transmutation of the long term impact of the nuclear waste to produce

82
00:04:39,390 --> 00:04:43,950
we don't know if it will be eighty years old gen four reactors

83
00:04:43,960 --> 00:04:48,240
but this will be decided in the future so you know that eighty is an

84
00:04:48,250 --> 00:04:50,530
accelerator driven system

85
00:04:50,550 --> 00:04:52,380
and jennifer four

86
00:04:52,410 --> 00:04:57,250
reactors based on fast neutrons that are planned for

87
00:04:58,240 --> 00:05:02,360
to the two thousand fourteen

88
00:05:02,420 --> 00:05:06,520
so what we are looking for also for the far future

89
00:05:06,530 --> 00:05:09,990
so this is solution so then you have a vision of the time for example

90
00:05:09,990 --> 00:05:11,100
the time for

91
00:05:11,120 --> 00:05:13,950
i presume it project

92
00:05:13,960 --> 00:05:17,480
that is under control construction get irish

93
00:05:17,500 --> 00:05:19,500
in france so

94
00:05:19,510 --> 00:05:23,030
this is only a project of physics

95
00:05:23,050 --> 00:05:24,070
then for

96
00:05:24,090 --> 00:05:26,580
if you want to connect that to

97
00:05:26,590 --> 00:05:30,450
two four electricity for for everyone but it would be

98
00:05:30,460 --> 00:05:35,610
i don't know two thousand eighty so it on the prototype for to study physics

99
00:05:35,640 --> 00:05:39,240
then they planned to protect them more to study

100
00:05:39,240 --> 00:05:40,950
for me far

101
00:05:40,970 --> 00:05:44,200
industry but is far far future that

102
00:05:44,860 --> 00:05:48,670
plan to have electricity by solution

103
00:05:48,690 --> 00:05:51,730
it is also used for me

104
00:05:51,750 --> 00:05:54,310
izer diagnostics

105
00:05:54,340 --> 00:05:59,590
so you know that a lot of diagnostic medicine

106
00:05:59,610 --> 00:06:05,520
thanks to nuclear science for example that you can use the detection of the decay

107
00:06:05,520 --> 00:06:08,900
of radioactive isotopes so i

108
00:06:09,020 --> 00:06:13,260
spect single photon emission computed tomography

109
00:06:13,270 --> 00:06:16,910
of the expected positron emission tomography

110
00:06:16,940 --> 00:06:18,950
these are some examples

111
00:06:18,970 --> 00:06:27,070
you have also we do not want to read the irony imaging by magnetic resonance

112
00:06:27,070 --> 00:06:30,500
one thing is that the power in noise

113
00:06:30,500 --> 00:06:34,000
it's going up with the bandwidth that you're looking at because i said the band

114
00:06:34,170 --> 00:06:35,400
is everywhere

115
00:06:35,420 --> 00:06:41,460
you can avoid it and therefore if you use of wider bandwidth system

116
00:06:41,520 --> 00:06:44,000
you get more noise coming into it

117
00:06:44,080 --> 00:06:47,400
we have a certain amount of power that you're willing to transmit a certain amount

118
00:06:47,420 --> 00:06:49,260
of light and the

119
00:06:49,280 --> 00:06:50,800
and the

120
00:06:50,820 --> 00:06:55,690
the number of bits per second you can transmit is going up with w

121
00:06:55,880 --> 00:07:00,020
but look at this effect with p here the effect would pay

122
00:07:00,020 --> 00:07:05,380
his is logarithmic which says as i increases parameter and i put more and more

123
00:07:05,380 --> 00:07:08,420
bits into one symbol

124
00:07:08,440 --> 00:07:09,690
this quantity

125
00:07:09,710 --> 00:07:16,570
is going up exponentially which means the logarithm of this is going up

126
00:07:17,460 --> 00:07:18,820
just linearly

127
00:07:18,880 --> 00:07:19,980
with them

128
00:07:20,000 --> 00:07:21,630
and this

129
00:07:21,670 --> 00:07:22,590
gives you

130
00:07:22,590 --> 00:07:27,420
if you look at it carefully the same trade-off is the simple-minded example i was

131
00:07:27,420 --> 00:07:28,730
talking about

132
00:07:28,760 --> 00:07:34,940
OK but simple-minded example does not explain this formula are all this formulas rather the

133
00:07:34,960 --> 00:07:38,630
we probably won't even completely proven this term

134
00:07:38,630 --> 00:07:43,920
what what channels result says is that no matter what you do no matter how

135
00:07:43,920 --> 00:07:48,710
clever you are with this noise that exists everywhere

136
00:07:48,760 --> 00:07:51,300
the fastest you can transmit

137
00:07:51,300 --> 00:07:56,280
with the most sophisticated coding things schemes you can think of

138
00:07:56,340 --> 00:07:58,940
is this rate right here

139
00:07:58,960 --> 00:08:01,210
which is what you would think it might be

140
00:08:01,210 --> 00:08:04,520
just from the scaling in terms of w

141
00:08:04,530 --> 00:08:08,210
and and were when you increase in power goes up

142
00:08:08,230 --> 00:08:13,940
exponentially it's the same same kind of answer you get with a simple thought experiment

143
00:08:14,030 --> 00:08:17,000
the fact that you have a one and here is a little bit strange

144
00:08:17,020 --> 00:08:19,320
we'll find out where that comes from

145
00:08:19,360 --> 00:08:25,630
but the idea here is that this noise is something you can't be noise is

146
00:08:25,630 --> 00:08:32,190
fundamental as fundamental part of every communication system as i always like to say noises

147
00:08:32,880 --> 00:08:34,800
death and and taxes

148
00:08:34,940 --> 00:08:37,800
you can avoid them there

149
00:08:37,800 --> 00:08:42,150
and there's nothing you can do about them like death and taxes you can take

150
00:08:42,150 --> 00:08:44,500
care of yourself and live longer

151
00:08:44,610 --> 00:08:46,230
and with taxes

152
00:08:46,250 --> 00:08:48,650
well you can become wealthy

153
00:08:48,690 --> 00:08:50,650
and support the government

154
00:08:50,670 --> 00:08:55,360
and make all sorts of payments and reduce your taxes that why or you can

155
00:08:55,840 --> 00:08:59,460
or even higher could account stating in the things they are also

156
00:08:59,480 --> 00:09:04,730
but those things are still always there some say you're always stuck with this

157
00:09:04,750 --> 00:09:10,130
and one of the major parts of the course will be to understand what this

158
00:09:11,150 --> 00:09:15,630
is a much deeper context we would otherwise

159
00:09:15,650 --> 00:09:16,980
OK some

160
00:09:16,980 --> 00:09:20,860
save that come back to later

161
00:09:20,880 --> 00:09:25,840
we have the same kind

162
00:09:25,860 --> 00:09:28,820
of layering

163
00:09:28,820 --> 00:09:33,920
in the channel encoding my talk about channel encoding i'm just talking about not talking

164
00:09:33,920 --> 00:09:37,480
about any kind of complicated coding technique

165
00:09:37,480 --> 00:09:43,030
six four fifty one talks about all of those what i'm talking about is simply

166
00:09:43,050 --> 00:09:46,570
the questions how do you take a sequence of bits

167
00:09:46,630 --> 00:09:50,420
turn it into a sequence of waveforms in such a way

168
00:09:51,090 --> 00:09:52,590
the receiver

169
00:09:52,590 --> 00:09:58,190
we can take that sequence of waveforms and map them reliably back into the original

170
00:09:59,250 --> 00:10:02,030
there's a standard way of doing this

171
00:10:02,050 --> 00:10:04,630
which is to take the sequence of bits

172
00:10:04,650 --> 00:10:07,650
first send them through a discrete encoder

173
00:10:07,670 --> 00:10:10,670
what the discrete encoder will do

174
00:10:11,030 --> 00:10:14,880
something like this

175
00:10:14,900 --> 00:10:19,170
and maps bits one right into bits of the higher rate

176
00:10:19,230 --> 00:10:23,500
and this lets you correct channel airs a simple example of this

177
00:10:23,550 --> 00:10:24,940
as you may have zero

178
00:10:24,960 --> 00:10:26,860
and to zero zero zero

179
00:10:26,880 --> 00:10:29,190
one into one one one

180
00:10:29,210 --> 00:10:32,500
and if the rest of your system

181
00:10:33,730 --> 00:10:35,820
this part of the system

182
00:10:35,880 --> 00:10:39,530
it makes a single layer at some point

183
00:10:39,550 --> 00:10:43,150
this part of the system escape from

184
00:10:43,190 --> 00:10:44,860
OK let me

185
00:10:44,880 --> 00:10:47,730
so put these on one slide but i can

186
00:10:47,780 --> 00:10:50,860
OK this takes a single zero

187
00:10:50,880 --> 00:10:53,130
turns into three zeros

188
00:10:53,150 --> 00:10:54,630
these three zeros

189
00:10:54,630 --> 00:11:00,690
go into this much later which maps this into some way form responsive to these

190
00:11:00,690 --> 00:11:02,070
three zeros

191
00:11:02,070 --> 00:11:08,940
waveform goes through here is gets added the modulator the tax those binary digits comes

192
00:11:08,940 --> 00:11:14,550
out with some approximation to these three zero every once in awhile because the the

193
00:11:14,550 --> 00:11:19,230
noise i one of these bits comes out wrong

194
00:11:19,300 --> 00:11:23,380
again because they come out wrong this discrete decoder

195
00:11:25,550 --> 00:11:26,530
i know that

196
00:11:26,590 --> 00:11:29,960
what was sent putin was either zero zero zero

197
00:11:29,980 --> 00:11:31,550
o one one one

198
00:11:31,550 --> 00:11:36,260
it's more likely to have one area they have two areas and therefore any time

199
00:11:36,260 --> 00:11:37,690
one error occurs

200
00:11:37,710 --> 00:11:40,280
i can the code it correctly

201
00:11:41,630 --> 00:11:45,000
now that is is a miserable code

202
00:11:46,110 --> 00:11:47,800
the guy by the name of

203
00:11:47,800 --> 00:11:50,230
it became very famous

204
00:11:51,050 --> 00:11:52,900
four generalizing this

205
00:11:52,900 --> 00:11:58,690
just a little bit into another single error correcting code which came through

206
00:11:58,690 --> 00:12:02,500
a a slightly higher rate still corrected single errors

207
00:12:02,550 --> 00:12:07,710
he became an only famous because he was tireless self-promoter

208
00:12:07,920 --> 00:12:14,070
and people think that he was the person who invented error correction coding

209
00:12:14,090 --> 00:12:15,780
he really wasn't

210
00:12:15,840 --> 00:12:19,730
kind areas will probably here

211
00:12:24,020 --> 00:12:25,210
anyway coding

212
00:12:25,260 --> 00:12:32,710
of this time namely mapping binary digits into longer strings of binary digits in such

213
00:12:32,710 --> 00:12:35,550
a way that you can correct binary errors

214
00:12:35,900 --> 00:12:38,780
has always been a major part

215
00:12:38,800 --> 00:12:42,420
of of communication system research

216
00:12:42,590 --> 00:12:46,090
a large number of communication systems

217
00:12:46,150 --> 00:12:48,530
i work in exactly this way

218
00:12:48,570 --> 00:12:54,320
in fact the older systems which use coatings tend to work this way

219
00:12:54,360 --> 00:12:58,630
this is a very popular way of trying to trying to design system you would

220
00:12:58,650 --> 00:12:59,860
put it in total

221
00:12:59,880 --> 00:13:02,000
total layering across here

222
00:13:02,050 --> 00:13:07,710
namely this part of the system denote the coding going on here

223
00:13:07,780 --> 00:13:09,780
at this

224
00:13:09,800 --> 00:13:14,210
o point where you're going discrete the coding you don't know anything about what the

225
00:13:14,210 --> 00:13:16,190
detector is doing here

226
00:13:16,230 --> 00:13:21,420
doesn't take a lot of imagination to save you have the detector here

227
00:13:21,440 --> 00:13:22,320
and there's this

228
00:13:22,340 --> 00:13:26,590
symbol comes through here noise gets added to it

229
00:13:26,610 --> 00:13:28,960
we're trying to decode the symbol

230
00:13:29,020 --> 00:13:33,190
and this is calcium noise which is just sort of spread around you know it

231
00:13:34,590 --> 00:13:38,340
usually when errors occur errors occur

232
00:13:38,340 --> 00:13:40,250
just barely

233
00:13:40,460 --> 00:13:45,050
i mean what you see at this point is you almost flip the coin to

234
00:13:45,050 --> 00:13:47,230
this is these non around

235
00:13:48,510 --> 00:13:52,560
the hardware landscape is changing rapidly ant the

236
00:13:54,300 --> 00:13:56,980
from the closest data to the father's data

237
00:13:57,590 --> 00:13:58,260
is expanding

238
00:13:58,910 --> 00:14:02,870
right and i haven't even talked about systems like tape drives right

239
00:14:03,320 --> 00:14:03,820
which are there

240
00:14:04,280 --> 00:14:05,230
glacier for example

241
00:14:07,160 --> 00:14:09,980
when say shoebox i'm taking it as a given

242
00:14:10,430 --> 00:14:11,980
that you're gonna be thinking about

243
00:14:12,510 --> 00:14:15,970
but for management on steroids if you will in this large-scale work

244
00:14:18,980 --> 00:14:22,200
i would say that do more about storage today i'm afraid them

245
00:14:23,380 --> 00:14:24,050
what analysts

246
00:14:24,720 --> 00:14:29,540
also don't make the assumption that everything is going to be in your nice little shoebox

247
00:14:30,260 --> 00:14:33,910
people will keep it and other places maybe because they want have private data

248
00:14:35,310 --> 00:14:36,480
and a bus stop the club

249
00:14:38,430 --> 00:14:40,420
maybe because for various reasons

250
00:14:40,930 --> 00:14:42,710
they can't bring all the data

251
00:14:43,500 --> 00:14:46,410
from it's ring-fenced auditing compliance

252
00:14:46,830 --> 00:14:50,870
in two way you want them to put so you need to support federated access

253
00:14:52,700 --> 00:14:53,580
most importantly

254
00:14:55,080 --> 00:14:59,010
the ecosystem of analytics is exploring the number of types of tools

255
00:14:59,720 --> 00:15:01,010
this large and growing larger

256
00:15:02,220 --> 00:15:05,070
at people wanna be able to use all these tools on

257
00:15:05,730 --> 00:15:06,030
the data

258
00:15:07,000 --> 00:15:07,670
how do you this

259
00:15:08,450 --> 00:15:10,210
right the spectrum goes from signal

260
00:15:11,490 --> 00:15:16,190
just last week there was a panel on sequel on how to handle a half

261
00:15:16,190 --> 00:15:19,590
a dozen vendors we're talking about the products that bring to market in the space

262
00:15:20,910 --> 00:15:21,900
stream processing

263
00:15:22,980 --> 00:15:25,810
business intelligence machine learning graph analytics

264
00:15:26,420 --> 00:15:26,630
the main

265
00:15:28,750 --> 00:15:29,270
many more

266
00:15:29,990 --> 00:15:30,870
at the top of the slide

267
00:15:33,190 --> 00:15:33,780
how do you know this

268
00:15:35,530 --> 00:15:40,600
the bottom line is yukon each of these has we do them pretty assignments

269
00:15:41,210 --> 00:15:42,530
that's my fundamental business

270
00:15:43,670 --> 00:15:44,480
you're gonna have to

271
00:15:45,310 --> 00:15:47,090
do what we did when we built

272
00:15:47,950 --> 00:15:50,740
various data analytics in the first place or file systems

273
00:15:51,340 --> 00:15:53,790
right we abstracted some commonly is

274
00:15:55,880 --> 00:15:57,620
and reuse them again and again

275
00:15:58,260 --> 00:15:59,970
in building different analytic tools

276
00:16:00,560 --> 00:16:02,520
we need to do the same kinds both

277
00:16:07,050 --> 00:16:08,730
for this new generation of systems

278
00:16:09,510 --> 00:16:13,530
add today i will spend some time talking about the computer fabric here

279
00:16:14,070 --> 00:16:19,500
the ubiquitous commonly on top of that you can build sequel machine learning and so on and so forth

280
00:16:22,170 --> 00:16:22,530
all right

281
00:16:23,620 --> 00:16:27,890
you also want to be able to collaborate share selectively all the things people already do

282
00:16:28,670 --> 00:16:30,290
but again i won't say too much about the

283
00:16:32,320 --> 00:16:33,700
one slide on my employer

284
00:16:34,530 --> 00:16:35,850
if you look at microsoft

285
00:16:36,310 --> 00:16:37,200
the current state

286
00:16:38,210 --> 00:16:40,650
already reflects many other points and make

287
00:16:41,730 --> 00:16:46,450
right there's a product called polly base that allows you to issue sequel data

288
00:16:47,120 --> 00:16:48,300
in a relational system

289
00:16:48,740 --> 00:16:49,740
and baby sister

290
00:16:51,440 --> 00:16:52,460
and simultaneously

291
00:16:53,240 --> 00:16:56,040
axes data that's in exterior first system that small

292
00:16:56,720 --> 00:17:00,920
but you don't have to know the data from the it's the first slide into the relational site

293
00:17:01,940 --> 00:17:04,190
i integrated by tools

294
00:17:04,680 --> 00:17:07,430
to excel at the end point there are plugins

295
00:17:07,890 --> 00:17:10,920
they allow you to roll up and draw down or literally

296
00:17:11,800 --> 00:17:12,600
one hundred million rules

297
00:17:16,230 --> 00:17:18,790
policy was interactive visualization

298
00:17:19,540 --> 00:17:21,560
so if you put all these pieces together

299
00:17:22,270 --> 00:17:26,050
you could do interactive visualization land rover down

300
00:17:26,620 --> 00:17:27,650
all the data and how do

301
00:17:28,560 --> 00:17:29,280
using the stack

302
00:17:31,450 --> 00:17:32,590
it's not just microsoft

303
00:17:33,220 --> 00:17:33,710
as i said

304
00:17:36,030 --> 00:17:39,640
they hosted this sequel and battle last week

305
00:17:42,510 --> 00:17:43,570
number when this

306
00:17:44,240 --> 00:17:45,410
it is striking to see

307
00:17:45,930 --> 00:17:47,570
the architectural pictures

308
00:17:48,020 --> 00:17:49,960
look very much like the layer cake

309
00:17:50,970 --> 00:17:51,940
i talked about just

310
00:17:53,540 --> 00:17:54,830
asterdata data

311
00:17:55,640 --> 00:17:58,380
basically if you look at the back

312
00:18:00,690 --> 00:18:02,600
that's what they called by the way that's tell you pronounce it

313
00:18:03,760 --> 00:18:04,570
if you look at

314
00:18:05,600 --> 00:18:09,690
any of these companies schlumberger google hot and what's microsoft see

315
00:18:10,530 --> 00:18:11,290
the whole system

316
00:18:11,730 --> 00:18:13,130
they all share this mission

317
00:18:13,130 --> 00:18:19,720
and what this means in terms of marginal distributions is that all the pairwise marginals

318
00:18:19,720 --> 00:18:23,610
had to break into products of the singleton marginals

319
00:18:23,650 --> 00:18:30,110
that's because everything here is independent there's no dependency anymore so everything breaks into products

320
00:18:30,530 --> 00:18:33,760
that's what your approximation is tell you to do it might not be the right

321
00:18:33,760 --> 00:18:36,800
thing but that's the approximation says

322
00:18:36,900 --> 00:18:42,400
and if you specialize the variational principle that we had this is the mean field

323
00:18:43,360 --> 00:18:45,380
for this particular case

324
00:18:47,070 --> 00:18:50,510
what you see here is you see is some over nodes

325
00:18:50,550 --> 00:18:54,170
and these usually recognizes newly entropy is

326
00:18:54,220 --> 00:18:56,510
minus bernoulli entropy is

327
00:18:56,530 --> 00:19:00,630
and that's coming because you have completely factorized distribution

328
00:19:00,630 --> 00:19:05,820
so you have one entropy for each node and entropy of the product here entropy

329
00:19:05,820 --> 00:19:09,070
takes log see some terms here

330
00:19:09,320 --> 00:19:11,260
so that's where that that entropy term

331
00:19:11,280 --> 00:19:12,570
so that

332
00:19:12,590 --> 00:19:16,880
that's what you get because you said i restrict myself to special distributions like this

333
00:19:16,880 --> 00:19:21,440
you get a very simple approximate entropy it's not exact anymore

334
00:19:21,490 --> 00:19:23,340
these constraints

335
00:19:23,340 --> 00:19:24,470
coming here

336
00:19:24,470 --> 00:19:27,970
you should have these joint terms in this linear form

337
00:19:28,030 --> 00:19:32,860
but because of these constraints you can plug in products these terms here

338
00:19:32,860 --> 00:19:37,050
and that's actually where the non convexity comes in when you have constraints that look

339
00:19:37,050 --> 00:19:38,050
like this

340
00:19:38,090 --> 00:19:41,470
if you sort of think about them they look kind of like hyperbolas or something

341
00:19:41,470 --> 00:19:46,450
it's you can you can see geometrically won't be a convex problem

342
00:19:47,360 --> 00:19:50,050
so the nice thing here is the mean field algorithm all it is a very

343
00:19:50,050 --> 00:19:52,420
simple way of optimizing the function

344
00:19:52,470 --> 00:19:58,220
so the simplest way possible as coordinate ascent you fix all variables except one

345
00:19:58,240 --> 00:20:01,880
and then you optimize over one variable at a time

346
00:20:01,900 --> 00:20:05,970
so here the variables are nodes so it's just like that picture we had before

347
00:20:05,970 --> 00:20:07,470
you fix one note

348
00:20:07,470 --> 00:20:08,800
we pick one node

349
00:20:08,820 --> 00:20:10,760
if you fix all the other nodes

350
00:20:10,780 --> 00:20:13,610
and you do the best possible over that node

351
00:20:13,630 --> 00:20:16,990
and if if you can just go through a couple lines of algebra

352
00:20:17,030 --> 00:20:21,570
you'll see the coordinate ascent is exactly this algorithm here and that's the naive mean

353
00:20:21,570 --> 00:20:23,240
field algorithm

354
00:20:24,340 --> 00:20:26,490
the upshot is that these updates

355
00:20:26,490 --> 00:20:29,650
what this means is those updates will always converge

356
00:20:29,700 --> 00:20:32,050
because the problem is not convex

357
00:20:32,260 --> 00:20:36,110
they might not go to the global optimum but they will converge because you're always

358
00:20:36,110 --> 00:20:37,150
moving up

359
00:20:37,170 --> 00:20:42,170
and what it's trying to do is trying to find the best approximation to your

360
00:20:42,170 --> 00:20:48,070
model that's of this form that's that's the form using this naive case

361
00:20:48,090 --> 00:20:53,860
so any question about about mean field algorithm

362
00:20:53,880 --> 00:21:06,440
these your are

363
00:21:10,700 --> 00:21:17,880
so sort of wondering about this picture while i drawn this inside the original and

364
00:21:21,010 --> 00:21:24,670
right so remember the original and is the set of music that i could get

365
00:21:24,670 --> 00:21:27,220
from any distribution i want

366
00:21:27,380 --> 00:21:29,570
you can do whatever you want you can have

367
00:21:29,590 --> 00:21:33,780
any markov random field to never fully connected graph even if you wanted

368
00:21:33,800 --> 00:21:37,470
but what i'm saying here's your i'm restricting you lot i'm saying in this case

369
00:21:37,470 --> 00:21:40,470
you can only look at product distributions

370
00:21:40,510 --> 00:21:44,470
so you're looking at a very small set of distributions and i'm asking you what

371
00:21:44,470 --> 00:21:45,880
you generate

372
00:21:45,900 --> 00:21:48,170
using that restricted subset

373
00:21:48,260 --> 00:21:55,260
so that's why you're always looking at a smaller set of possible news

374
00:21:55,280 --> 00:22:08,880
this is sort of a high level this is the picture that you want to

375
00:22:08,880 --> 00:22:12,470
take away from mean field i mean sort of analytical details aside you want to

376
00:22:12,470 --> 00:22:17,170
take away that it's restricting its use in an inner approximation

377
00:22:17,510 --> 00:22:19,280
that gives you a lower bound

378
00:22:19,550 --> 00:22:23,530
and it's using the non convex inner approximation those sort of the three key things

379
00:22:23,530 --> 00:22:24,550
to take away

380
00:22:24,590 --> 00:22:29,010
and all contrast that in a second with what belief propagation is doing is doing

381
00:22:29,010 --> 00:22:30,880
something quite different

382
00:22:30,900 --> 00:22:37,970
but something can also be interested in the symmetry picture

383
00:22:37,990 --> 00:22:42,340
i just one last thing about mean field i won't go into details

384
00:22:42,900 --> 00:22:47,780
the whole hierarchy of mean field algorithms depending on how complex the subset of distributions

385
00:22:48,860 --> 00:22:54,950
the naive versions are based on sort using fully factor product distributions

386
00:22:54,950 --> 00:22:58,920
but of course you can use other ones for instance if you remember i talked

387
00:22:58,920 --> 00:23:05,320
about coupled hmms last wednesday these you might use for audio video fusion

388
00:23:05,340 --> 00:23:08,400
and this is a hard model to do inference on

389
00:23:08,400 --> 00:23:12,260
but what people have done mean field on it for instance

390
00:23:12,280 --> 00:23:13,300
and naturals

391
00:23:13,320 --> 00:23:18,610
substructures this to decouple the chains like this and you mean field on this structure

392
00:23:18,650 --> 00:23:23,380
and this is again easy because each of these is the change in their decoupled

393
00:23:23,420 --> 00:23:28,610
and so you can again derive updates for one on the structured mean field

394
00:23:28,630 --> 00:23:32,320
it's again trying to find the best approximation of the form

395
00:23:32,530 --> 00:23:35,150
for model that looks like that

396
00:23:35,200 --> 00:23:40,900
so the whole hierarchy here but of course the trade-off you pay is the doing

397
00:23:40,900 --> 00:23:46,650
the updates gets more complex as you start using more complex structures so this is

398
00:23:46,650 --> 00:23:49,880
in some sense is an application dependent issue how much you want to pay to

399
00:23:49,880 --> 00:23:55,090
do updates how much accuracy do need some cases naive mean field is good enough

400
00:23:55,090 --> 00:23:56,680
other cases you probably have to

401
00:23:56,720 --> 00:24:00,240
the structured cluster field algorithms

402
00:24:03,720 --> 00:24:08,200
OK so let's let's do belief propagation and is actually not much work to do

403
00:24:08,200 --> 00:24:10,180
because we already done some work on the tree

404
00:24:10,200 --> 00:24:14,780
what belief propagation is doing from the variational point of view on arbitrary graphs pretty

405
00:24:15,530 --> 00:24:17,820
what it's saying is

406
00:24:17,820 --> 00:24:20,290
that makes bounds are of this form

407
00:24:20,340 --> 00:24:22,890
you have an extravagant factor here

408
00:24:22,900 --> 00:24:25,840
and the chaining approach

409
00:24:25,860 --> 00:24:27,140
which also works

410
00:24:27,150 --> 00:24:28,790
obviously classes

411
00:24:28,830 --> 00:24:32,600
i would get because of the logan

412
00:24:32,650 --> 00:24:36,330
and the other from that you may want to recall is

413
00:24:36,390 --> 00:24:40,390
when you have you know no noise in the case where your target function belong

414
00:24:40,390 --> 00:24:44,270
to the class and you have zero noise in the kind of ideal case

415
00:24:44,290 --> 00:24:48,290
then all these bounds i mean you can get rid of the

416
00:24:48,350 --> 00:24:51,330
query and you can get something like

417
00:24:51,340 --> 00:24:52,740
the over and

418
00:24:52,750 --> 00:24:55,060
and here you can't get rid of

419
00:24:55,080 --> 00:24:57,000
so that's the

420
00:24:57,030 --> 00:25:00,090
optimal k so ideally

421
00:25:00,150 --> 00:25:02,020
the ideal case

422
00:25:02,060 --> 00:25:06,720
that's the best you can do in dimension d obviously them into the and that

423
00:25:06,790 --> 00:25:09,900
in january

424
00:25:09,960 --> 00:25:16,830
these are things that it is good to have this kind of remarks line

425
00:25:16,850 --> 00:25:21,690
and using bound

426
00:25:21,730 --> 00:25:23,650
but the last comment

427
00:25:23,670 --> 00:25:27,520
it's important to understand that

428
00:25:27,630 --> 00:25:31,270
they are saying that we can and i mean that we might have with intentionally

429
00:25:31,270 --> 00:25:35,270
in the bow and arrow things that come out from really the learning phenomenon and

430
00:25:35,840 --> 00:25:37,250
those things are

431
00:25:37,290 --> 00:25:38,530
you i mean you should be

432
00:25:38,550 --> 00:25:40,710
training or you should train yourself to

433
00:25:40,750 --> 00:25:46,210
really detect which is which when you look at about

434
00:25:48,080 --> 00:25:52,350
finally the most important messages

435
00:25:52,360 --> 00:25:57,320
the goal i mean if you want to justify something to avoid assumptions because as

436
00:25:57,320 --> 00:26:00,420
well as i mean it's fine to introduce assumption when you want to understand things

437
00:26:00,420 --> 00:26:02,950
but you don't provide justification anymore

438
00:26:04,090 --> 00:26:05,710
what assumptions

439
00:26:05,720 --> 00:26:07,340
you can't ever

440
00:26:07,360 --> 00:26:14,050
very these assumptions like even the fact that somebody like something to something that thesis

441
00:26:14,050 --> 00:26:18,790
is independent this you can't even verify so anything beyond that is

442
00:26:18,790 --> 00:26:20,570
even less verifiable anyway

443
00:26:20,600 --> 00:26:24,760
if you have an assumption and then you get a justification if you use assumption

444
00:26:24,790 --> 00:26:30,280
you get understanding can still what happens if let's assume it might target function where

445
00:26:30,610 --> 00:26:32,460
a linear classifier

446
00:26:32,500 --> 00:26:34,150
what would i do that

447
00:26:34,180 --> 00:26:37,420
doesn't mean that i mean maybe you have an algorithm that for which we can

448
00:26:37,420 --> 00:26:41,740
prove that if the target function is that the classifier is the perfect

449
00:26:42,290 --> 00:26:46,530
but then what if it not the case

450
00:26:47,860 --> 00:26:50,000
the important thing the geometry

451
00:26:50,040 --> 00:26:55,030
and again repeating the same things again and again hoping that

452
00:26:55,180 --> 00:26:59,650
will enter your brain and get out of

453
00:27:00,290 --> 00:27:02,370
the last thing

454
00:27:04,120 --> 00:27:06,550
i'm sorry for this advertisment but i have to two

455
00:27:06,640 --> 00:27:11,190
do it but maybe we can start with questions and then i will come back

456
00:27:11,190 --> 00:27:12,080
to this

457
00:27:12,130 --> 00:27:13,050
after one

458
00:27:45,460 --> 00:27:50,010
OK so the question is i'm saying that the geometry of the most important thing

459
00:27:50,010 --> 00:27:51,040
in the bound but

460
00:27:51,130 --> 00:27:52,330
the geometry

461
00:27:52,340 --> 00:27:58,130
of the function that depends on what you know about on your prior OK so

462
00:27:58,130 --> 00:28:00,390
what i'm saying here is

463
00:28:00,450 --> 00:28:03,770
if you give me a class of course you have presented so if you tell

464
00:28:03,770 --> 00:28:05,170
me i'm using

465
00:28:05,190 --> 00:28:05,750
you know

466
00:28:05,760 --> 00:28:10,790
the RKHS corresponding to the adoption kernel i'm using SVM with gaussian kernel and that's

467
00:28:10,790 --> 00:28:11,890
my classification

468
00:28:11,920 --> 00:28:16,220
that's perfectly i mean that's your choice but now what i'm saying is that

469
00:28:16,240 --> 00:28:19,560
even this function class

470
00:28:19,580 --> 00:28:24,120
OK i these this function classified the RKHS

471
00:28:24,130 --> 00:28:27,040
the geometry that time

472
00:28:27,100 --> 00:28:30,380
referring to the geometry that is induced by

473
00:28:30,460 --> 00:28:31,940
the distribution

474
00:28:31,950 --> 00:28:33,800
when you look at the plant

475
00:28:34,350 --> 00:28:37,360
two functions in this class

476
00:28:37,390 --> 00:28:38,670
would have from

477
00:28:38,670 --> 00:28:40,600
intrinsic metric

478
00:28:40,620 --> 00:28:42,250
distance between them

479
00:28:42,260 --> 00:28:45,030
which depends on the probability distribution which

480
00:28:45,090 --> 00:28:47,530
i don't know for sure but i can

481
00:28:48,570 --> 00:28:49,930
you know

482
00:28:49,930 --> 00:28:51,750
knowing these and maybe

483
00:28:51,790 --> 00:28:55,980
looking at you can even look at an empirical sample

484
00:28:56,000 --> 00:29:00,020
and you can see how the solutions you and then from that you can measure

485
00:29:01,350 --> 00:29:05,970
how big is the class and how the function in the past in a this

486
00:29:05,970 --> 00:29:07,720
is no longer i mean this is

487
00:29:07,720 --> 00:29:11,950
ok so this is a i have a webcam here and square the images and i

488
00:29:11,950 --> 00:29:14,910
can point this camera very things so the point it that's

489
00:29:16,020 --> 00:29:18,670
this computer and this is not book ok it's right

490
00:29:26,150 --> 00:29:30,870
as a right to have this monitor yeah this monitor

491
00:29:32,060 --> 00:29:32,950
complete keyboard

492
00:29:36,100 --> 00:29:40,330
it's mouse yeah about this remote control

493
00:29:40,770 --> 00:29:45,850
chose to yeah one of those how about this

494
00:29:48,070 --> 00:29:51,180
ipod ok that's actually a some some galaxy as well

495
00:29:55,470 --> 00:29:58,910
it does not have to phones and both something x this for this one

496
00:29:58,910 --> 00:30:00,920
is a set of four which is more correct

497
00:30:01,420 --> 00:30:04,730
ok but is black so it's a set of farmers it's y is and i

498
00:30:08,960 --> 00:30:13,440
so at i store little piece of history here pretzel current a ok

499
00:30:13,450 --> 00:30:16,070
so i know it's know it knows it's kind of

500
00:30:16,430 --> 00:30:18,350
history some kind know can eat it

501
00:30:22,480 --> 00:30:26,060
so may ok because you know a non plate know is to

502
00:30:26,860 --> 00:30:31,300
with this what about all ok as one of our this question kohima yeah

503
00:30:31,310 --> 00:30:36,770
as our right have this matrix which none really really doesn't

504
00:30:36,770 --> 00:30:38,990
know that actually on to those that phones

505
00:30:40,960 --> 00:30:43,220
remote-control always got he's on it might

506
00:30:43,990 --> 00:30:51,310
this to loop so that s yeah this is an class know ok that's

507
00:30:51,410 --> 00:30:53,520
and help computer of course because it's

508
00:30:53,540 --> 00:30:56,440
you know others and some widget and actually have

509
00:30:58,590 --> 00:31:01,910
a ok this this

510
00:31:10,410 --> 00:31:14,790
learning to as their yeah into for me properly

511
00:31:15,440 --> 00:31:19,070
loafer as a enough you know this is to so the idea

512
00:31:19,530 --> 00:31:22,610
what's ok and where we say this actually

513
00:31:24,040 --> 00:31:26,110
library a ok why he's ok

514
00:31:33,910 --> 00:31:36,030
monitor as close enough cash to know

515
00:31:41,190 --> 00:31:46,480
right as is so right you get the idea let me show you another

516
00:31:46,490 --> 00:31:47,350
similar demo

517
00:31:49,510 --> 00:31:50,630
but this one i on

518
00:31:54,310 --> 00:31:58,210
the probabilities well so that so this is trained with

519
00:31:58,370 --> 00:32:01,860
cross-entropy loss on it's like not the last you could think of it

520
00:32:01,860 --> 00:32:04,500
as much to learn to question so you get you get

521
00:32:04,730 --> 00:32:05,990
you probably to estimate

522
00:32:08,850 --> 00:32:13,830
you think of them scores really so calibrated scores really that's

523
00:32:13,830 --> 00:32:14,520
what they are

524
00:32:16,380 --> 00:32:17,190
ok so this

525
00:32:20,830 --> 00:32:25,050
so i can train so i can point camera something or nothing

526
00:32:25,430 --> 00:32:30,170
like this and making quick learn loops and to get to focus

527
00:32:33,920 --> 00:32:38,110
and it's kind of learn this so if i put a simple cells it's doesn't

528
00:32:38,120 --> 00:32:42,130
know but if i can back to this it focus here we go

529
00:32:43,260 --> 00:32:47,420
right now i can put that are less right side of the room so the

530
00:32:47,430 --> 00:32:51,750
room depending on the point of view so you are category b

531
00:32:52,680 --> 00:32:54,470
and you guys the other side

532
00:32:59,340 --> 00:33:02,830
and number you see here on the right side is the number of samples

533
00:33:02,830 --> 00:33:06,040
that are used to to train so excited c

534
00:33:11,510 --> 00:33:15,040
a is low light so that this was how time

535
00:33:16,910 --> 00:33:19,230
i myself have a very different from the rest

536
00:33:19,980 --> 00:33:24,180
yeah of those pictures and you know i could use whatever

537
00:33:29,700 --> 00:33:32,870
ok so we have this project whatever it is

538
00:33:34,050 --> 00:33:40,290
yeah yeah and we have this little screen down there we have

539
00:33:40,300 --> 00:33:45,720
the right so the room we have this and

540
00:33:46,340 --> 00:33:49,730
and you know i train this with a number of categories just a few

541
00:33:49,730 --> 00:33:53,740
examples what this shows is that so first to tell you how this

542
00:33:53,740 --> 00:33:56,470
works right so you take the core of the commercial net chop off

543
00:33:56,470 --> 00:33:58,500
the last layer on retrained last layer

544
00:33:58,790 --> 00:34:03,670
are using essentially what else nearest neighbour pasan window classifier

545
00:34:03,680 --> 00:34:07,890
essentially by just storing templates so we on the bottom stores

546
00:34:07,900 --> 00:34:08,840
template compares

547
00:34:12,420 --> 00:34:13,790
right so back to

548
00:34:16,460 --> 00:34:20,960
the talk in and and hoops of the direction and

549
00:34:22,290 --> 00:34:25,730
so you have humor things there is a really impressive application

550
00:34:25,730 --> 00:34:29,590
of this to image imitation to show because on tell you about unsupervised

551
00:34:31,870 --> 00:34:36,320
and since this is a three hour talk have to our left so so

552
00:34:43,530 --> 00:34:47,170
ok so as learning ok so what is unsupervised learning that's kind

553
00:34:47,180 --> 00:34:49,580
of the question we might ask and

554
00:34:51,040 --> 00:34:54,580
to me the way i like to see unsupervised learning is that capturing

555
00:34:54,580 --> 00:34:58,320
dependencies between variables and essentially one possible way

556
00:34:58,320 --> 00:35:01,020
of capturing dependencies between between variables

557
00:35:01,320 --> 00:35:05,650
are observed variables is to find so it's very likely that the

558
00:35:05,660 --> 00:35:08,460
data comes to you is going to the living in

559
00:35:09,230 --> 00:35:12,590
manifold or some surface eventually surface in the space

560
00:35:12,770 --> 00:35:16,400
and if you find if you if you can train a function that tells you

561
00:35:16,720 --> 00:35:19,790
when you show it a vector whether it's on may fall off the

562
00:35:19,790 --> 00:35:23,120
manifold or even better which tells you which direction to move

563
00:35:23,130 --> 00:35:26,480
to words to get closer to the manifold then you basically sold

564
00:35:26,480 --> 00:35:29,370
unsupervised learning because of capture the dependencies between

565
00:35:29,370 --> 00:35:35,080
variables ok and the bet one best first to do this is to an energy

566
00:35:35,090 --> 00:35:37,500
function to tell you offer you often manifold

567
00:35:37,570 --> 00:35:41,170
ok contrast function some kind so imagine your observations of those

568
00:35:41,170 --> 00:35:45,930
blue deals here here this ought to know this is a kind of

569
00:35:45,930 --> 00:35:48,430
two different learning algorithms to different parameterization

570
00:35:48,430 --> 00:35:51,780
learning algorithm learning that sort of changes an energy surface

571
00:35:51,780 --> 00:35:54,950
so that the blue beads have lower energy and everything else

572
00:35:55,010 --> 00:35:58,660
so this guy here stops flattened and the sum of those areas

573
00:35:58,670 --> 00:36:02,690
push up so that to get you know have are with roof

574
00:36:02,780 --> 00:36:05,800
this guy a the energy is designed such a way that already

575
00:36:05,800 --> 00:36:11,180
in a filling in a drug down differently this but they have and i can

576
00:36:11,180 --> 00:36:15,780
only guess so an object to be going belong to the ural river and alter

577
00:36:15,780 --> 00:36:20,780
the layer of from automatically belong to the layer of a drug it is sort

578
00:36:20,780 --> 00:36:27,150
of a superclass of these differently this particularly when a transform that in the business

579
00:36:27,150 --> 00:36:31,750
scheme i have a table for columns of the a table for physical objects and

580
00:36:31,770 --> 00:36:37,670
there is a relation between logical and physical object so the part of all the

581
00:36:37,670 --> 00:36:44,530
efficiency of its physical object is part of these logical object when describing the context

582
00:36:44,600 --> 00:36:49,990
is specified object type so if the object type is that i automatically know that

583
00:36:50,000 --> 00:36:55,430
the reader is defined as a type of i draw graffiti that type of logic

584
00:36:55,430 --> 00:37:02,970
i don't have to solve automatically i can obtain the a hierarchical information of these

585
00:37:03,170 --> 00:37:04,290
all these easily

586
00:37:04,310 --> 00:37:08,280
so it's quite different from the typical expression of bayes when we have a table

587
00:37:08,320 --> 00:37:14,510
forty should differently this is true because a lot more than that we are considering

588
00:37:14,510 --> 00:37:19,910
but in this way is quite difficult to represent i information different object in this

589
00:37:19,910 --> 00:37:24,820
case we want to represent information because for the first version of discovery task we

590
00:37:24,820 --> 00:37:30,540
are able to deal with these and so we thought we have the design these

591
00:37:30,720 --> 00:37:38,980
schema because we suppose it is a adopted find the four percent think these problems

592
00:37:41,230 --> 00:37:49,810
yes but is not by in the sign of the babies with the definition of

593
00:37:49,810 --> 00:37:56,230
product time if OK one time i decide okay this is not to pass he's

594
00:37:56,240 --> 00:38:01,700
not a musician with a drug but it is in is a relation with orthography

595
00:38:01,720 --> 00:38:07,970
it's not impossible but looking for documents i have to change in the design of

596
00:38:07,970 --> 00:38:13,650
that obeys the definition of c that is not the type that is that i'd

597
00:38:13,650 --> 00:38:17,860
rather but it is the type that is or graffiti

598
00:38:20,760 --> 00:38:24,810
these are the objects

599
00:38:25,020 --> 00:38:26,920
much structure

600
00:38:26,930 --> 00:38:33,860
the distribution on

601
00:38:33,960 --> 00:38:36,710
yes you can

602
00:38:36,730 --> 00:38:39,950
we all this

603
00:38:40,010 --> 00:38:44,160
over here

604
00:38:44,180 --> 00:38:48,950
for you

605
00:38:50,480 --> 00:38:57,710
OK this is exactly the i ching OK do

606
00:38:57,710 --> 00:39:04,630
system started the project with these people of by mistake department and they give the

607
00:39:04,630 --> 00:39:11,700
specific problem in this a OK i'm interacting modelling this type of data and exactly

608
00:39:11,700 --> 00:39:15,610
this type of data so we have designed the these ideas he i don't go

609
00:39:15,630 --> 00:39:18,440
on the specific the

610
00:39:18,450 --> 00:39:24,630
requirements begin answer anyway it's the the sound and the sound is very simple to

611
00:39:24,630 --> 00:39:30,630
be extended if i decided to introduce new avalon i have two of the allies

612
00:39:30,650 --> 00:39:37,210
or in order a academic into by following stuck same procedure is only the example

613
00:39:37,210 --> 00:39:42,950
that we have defined these specific project but really know what is important is the

614
00:39:42,950 --> 00:39:50,180
idea on the same is it we can define a very general i his son

615
00:39:50,420 --> 00:39:57,170
for the next nine it's

616
00:39:58,790 --> 00:40:05,810
yeah an example of descriptive in our several study of the different descriptors and the

617
00:40:05,810 --> 00:40:11,680
unknown example from a practical point of view it is implemented as it appears well

618
00:40:11,710 --> 00:40:18,730
proceeded to dismiss that descriptor is like a function that can be caught in this

619
00:40:18,730 --> 00:40:19,720
world clearly

620
00:40:19,740 --> 00:40:21,680
so this is the

621
00:40:21,850 --> 00:40:26,730
we we in the dust from a practical point of view and practically an example

622
00:40:26,730 --> 00:40:33,040
of competition we wanted to deter mine the because fidelity of line and the descriptor

623
00:40:33,040 --> 00:40:40,120
is typically computed on four object belonging to developing countries will these years because the

624
00:40:40,120 --> 00:40:44,410
experts said OK in this case it's interesting for us to all

625
00:40:44,430 --> 00:40:47,870
no to characterize these particles shape window

626
00:40:47,910 --> 00:40:54,120
the line and this is the idea that very simple and public to business line

627
00:40:54,120 --> 00:41:01,080
is the line we computed in goal of each segment forming this slight difference between

628
00:41:01,870 --> 00:41:06,410
and they for this difference is greater than a threshold we defined it is like

629
00:41:06,450 --> 00:41:11,370
musical style or not is it because they tend to be confined to an entity

630
00:41:11,370 --> 00:41:14,180
called topological

631
00:41:14,210 --> 00:41:17,750
and computation computational topological properties and relations

632
00:41:18,770 --> 00:41:23,890
OK the different types of queries we can compute is a salary query because the

633
00:41:23,890 --> 00:41:28,200
case was sent back to all the objects belonging to a specific assembly as in

634
00:41:28,200 --> 00:41:34,430
this case are in this case we selected exactly all the objects that belong to

635
00:41:34,430 --> 00:41:41,770
these are at eleven million eleven query that these means OK for salem not interested

636
00:41:41,770 --> 00:41:46,100
interesting in all the possible object about an interesting on only to the object belongs

637
00:41:46,100 --> 00:41:51,250
to the drug traffic and the construction only because i don't really interesting for a

638
00:41:51,250 --> 00:41:59,880
specific task so i that although objects belong to the user specifically and

639
00:41:59,930 --> 00:42:04,390
to the typical form query that we can define object level query can we can

640
00:42:04,390 --> 00:42:05,450
say that

641
00:42:05,560 --> 00:42:08,210
we interesting to the

642
00:42:09,910 --> 00:42:15,080
two the object belonging to the about of the layer and in particular for the

643
00:42:15,080 --> 00:42:17,070
road we interested to the world

644
00:42:17,310 --> 00:42:25,270
that induces sparsity and as specific properties that is not only

645
00:42:31,200 --> 00:42:35,620
set only about and the set of old and the for the road only the

646
00:42:35,620 --> 00:42:41,710
four for which of these properties is satisfied and the therefore each site uses

647
00:42:41,870 --> 00:42:45,730
selected for each cell of the model

648
00:42:45,750 --> 00:42:49,200
why did the risk is we have all the objects belonging to the geography and

649
00:42:49,200 --> 00:42:52,660
construction only the to

650
00:42:52,680 --> 00:42:58,290
OK the second parameters is that we define is that the kind of knowledge to

651
00:42:58,290 --> 00:43:02,700
be minor so the key words in this case is mine the kind of part

652
00:43:02,930 --> 00:43:07,700
to be discovered that can be as additional rules and classification seems more at this

653
00:43:07,700 --> 00:43:14,250
moment throughout we look and example for the classification rule about it's quite similar for

654
00:43:14,390 --> 00:43:16,500
official discovery

655
00:43:16,520 --> 00:43:21,200
in the classification rule and so the classification as part of a is the name

656
00:43:21,200 --> 00:43:26,000
of the city to the club to the part to the set of pattern to

657
00:43:26,000 --> 00:43:32,000
be discovered to be discovered for the concept to be learned for example OK i

658
00:43:32,000 --> 00:43:39,040
wanted to know classification rule for the class concept to fluvial landscape so to characterize

659
00:43:39,040 --> 00:43:42,560
all the objects belong to the class four landscape

660
00:43:42,630 --> 00:43:48,290
all right if i wanted to classification rule for more than one concept i can

661
00:43:48,290 --> 00:43:55,100
specify the different concepts alliances select set of descriptor to be used in generating the

662
00:43:55,100 --> 00:44:01,390
logical description of the object selected and with this group doesn't specify exactly this discrete

663
00:44:01,390 --> 00:44:04,340
can actually prove the generalisation is possible

664
00:44:04,960 --> 00:44:10,300
beautiful branch of mathematics which in my opinion to represent the best possible intersection computer

665
00:44:10,300 --> 00:44:12,310
science and mathematics

666
00:44:12,330 --> 00:44:13,500
learning experience

667
00:44:13,520 --> 00:44:16,660
and then there's you know

668
00:44:16,720 --> 00:44:24,770
many many years the VC dimension and active learning activities in some roughly speaking about

669
00:44:24,830 --> 00:44:28,790
this space is not too complicated it has a low capacity in some formal sense

670
00:44:29,100 --> 00:44:32,270
and for training set is large enough and we do well on and then you

671
00:44:32,270 --> 00:44:33,930
can bound the probability

672
00:44:33,940 --> 00:44:37,340
during significantly worse on test data

673
00:44:38,660 --> 00:44:39,710
having said

674
00:44:39,720 --> 00:44:41,410
i told you about all this you know beautiful

675
00:44:41,420 --> 00:44:43,970
the formalism we're now going to do something very simple

676
00:44:43,990 --> 00:44:48,870
you have to crawl before walk right so we're just controlled by adding a a

677
00:44:48,870 --> 00:44:50,510
penalty to lost

678
00:44:50,530 --> 00:44:55,760
we're going to say a certain parameters which i think a lot more complex those

679
00:44:56,680 --> 00:45:00,530
back in the other parameters which i model to be simple and those of an

680
00:45:00,530 --> 00:45:04,950
all-out because and that's the job of the function p so he looks after him

681
00:45:04,950 --> 00:45:05,930
and not

682
00:45:05,950 --> 00:45:10,660
i don't want to make public for the purpose of finding simple

683
00:45:10,700 --> 00:45:13,990
and then we just take the sum of a loss function which is the training

684
00:45:13,990 --> 00:45:15,820
error and the kernel

685
00:45:15,870 --> 00:45:22,660
parameters and this is called penalised regularized learning and that's how we control so good

686
00:45:22,660 --> 00:45:24,010
to the data

687
00:45:24,010 --> 00:45:24,530
to get

688
00:45:24,550 --> 00:45:27,680
training error but also going to by itself

689
00:45:27,700 --> 00:45:31,410
towards the end of

690
00:45:31,490 --> 00:45:35,030
so this is just a picture to drive this

691
00:45:35,260 --> 00:45:38,910
you focus on the right hand side here is

692
00:45:38,910 --> 00:45:40,760
it's true tables

693
00:45:40,780 --> 00:45:43,070
so let's say i gave you

694
00:45:43,120 --> 00:45:47,620
this training data it's a function which has three binary inputs you are

695
00:45:47,620 --> 00:45:53,140
x one x two x three and a single line of the white line

696
00:45:53,160 --> 00:45:57,340
and the training data i tell you the zero zero zero there is zero

697
00:45:57,360 --> 00:46:01,930
if the input is zero one zero one sort of lines field

698
00:46:01,950 --> 00:46:04,370
for an overnight stay

699
00:46:04,390 --> 00:46:09,030
please use your fancy machine learning techniques to predict for what you was going be

700
00:46:09,030 --> 00:46:10,200
when the input is zero

701
00:46:10,200 --> 00:46:11,820
are one

702
00:46:11,840 --> 00:46:16,780
so when decision starting to get really scared

703
00:46:16,800 --> 00:46:19,070
you should be thinking to himself

704
00:46:19,090 --> 00:46:23,950
this is ridiculous it's good so arbitrary truth table writing this value could be either

705
00:46:25,620 --> 00:46:28,760
i mean how many are you not what this value is right the these values

706
00:46:28,760 --> 00:46:31,280
you have nothing to do with the prince

707
00:46:31,300 --> 00:46:36,370
that you don't understand if you don't make assumptions about the world you can never

708
00:46:36,370 --> 00:46:37,450
learn anything

709
00:46:38,120 --> 00:46:42,180
example where the fact that you have to make an assumption really are really comes

710
00:46:42,180 --> 00:46:45,680
and slaps in the face of course you have to make some assumptions like maybe

711
00:46:45,680 --> 00:46:49,430
this function is conjunctive normal form

712
00:46:49,490 --> 00:46:54,010
or maybe use some of the two cause of the rules

713
00:46:54,010 --> 00:46:57,160
make some assumptions about the form of this and then you can start to fill

714
00:46:57,160 --> 00:47:01,430
in these values because there's only one function which is consistent with the training

715
00:47:02,820 --> 00:47:04,360
this is a picture where

716
00:47:04,370 --> 00:47:07,930
the fact that you have to make assumptions doesn't seem so obvious imagine given the

717
00:47:07,930 --> 00:47:11,200
training data which were these circles here

718
00:47:11,260 --> 00:47:15,530
and i told you we predict the value function somewhere between almost would have no

719
00:47:15,530 --> 00:47:20,260
problem doing the right thing to do to draw the line like this through the

720
00:47:21,140 --> 00:47:24,410
and tripoli on this line

721
00:47:24,470 --> 00:47:26,510
drawing the line through the data

722
00:47:26,530 --> 00:47:29,720
is exactly the same as making

723
00:47:29,740 --> 00:47:32,220
some assumption to fill in the truth table

724
00:47:32,240 --> 00:47:36,970
don't fool yourself because you have big visual cortex like smooth things this is you

725
00:47:36,970 --> 00:47:41,910
know more natural assumption assumption make you get when you draw a line to interpolate

726
00:47:41,910 --> 00:47:43,550
you are making assumptions

727
00:47:43,570 --> 00:47:48,100
you're making an assumption that this function is smooth in some very simple

728
00:47:48,120 --> 00:47:52,570
you know l two sets of size k and that's fine that's actually really believe

729
00:47:52,570 --> 00:47:52,930
they are

730
00:47:52,950 --> 00:47:54,840
no you're making some

731
00:47:55,700 --> 00:48:00,030
so an unbiased learner can never generalise if you don't make assumptions about the world

732
00:48:02,430 --> 00:48:03,450
OK so

733
00:48:03,550 --> 00:48:04,930
enough for me to do

734
00:48:04,990 --> 00:48:07,840
pontificating i want to get moving probabilistic

735
00:48:07,860 --> 00:48:10,220
first here so given by

736
00:48:10,240 --> 00:48:16,530
what we can think of learning and doing is estimating the joint probability function samples

737
00:48:16,530 --> 00:48:18,200
from those

738
00:48:18,320 --> 00:48:25,260
classification and regression supervised learning are estimating the conditional density of the output y given

739
00:48:25,260 --> 00:48:26,950
the inputs

740
00:48:27,000 --> 00:48:32,810
from should representation of the image and you tell me whether they should be word

741
00:48:32,810 --> 00:48:36,740
counts in the standings in the email

742
00:48:38,120 --> 00:48:40,470
you can think of just density estimation

743
00:48:41,320 --> 00:48:42,820
you want to estimate

744
00:48:43,590 --> 00:48:47,870
but you know in the central object of interest is the joint distribution between the

745
00:48:47,870 --> 00:48:50,340
input and the output between inputs

746
00:48:50,700 --> 00:48:57,140
and the maintenance of the is compactly representing the joint distribution and robustly learning exchange

747
00:48:57,140 --> 00:48:58,890
from small number

748
00:48:58,910 --> 00:49:05,050
and our inductive bias is expressed as prior assumptions about the distribution

749
00:49:05,220 --> 00:49:10,740
so in a probabilistic framework everything is about estimating large joint distributions between inputs and

750
00:49:10,740 --> 00:49:16,220
outputs as the whole game and all of your assumptions are prior assumptions on what

751
00:49:16,220 --> 00:49:19,030
these distributions can can be represented prior

752
00:49:19,590 --> 00:49:26,050
i prior distribution for the parameter the distributions of class distribution

753
00:49:26,070 --> 00:49:30,220
so the main computations we're going to need to do in the probabilistic approach

754
00:49:30,260 --> 00:49:37,640
are efficiently calculate marginal and conditional distributions from whatever compact representation of the joint distribution

755
00:49:41,640 --> 00:49:42,260
the o

756
00:49:42,260 --> 00:49:43,840
here focusing on

757
00:49:43,840 --> 00:49:49,160
the first was to represent the a joint probability distribution PX compactly even when there

758
00:49:49,160 --> 00:49:50,620
are many variables

759
00:49:50,620 --> 00:49:56,050
so just to drive this home imagine you have a one megapixel images from the

760
00:49:57,840 --> 00:50:03,100
this species are recorded sixteen bits of information each color channel

761
00:50:03,120 --> 00:50:09,840
and now you want to ask how many possible images my camera shake

762
00:50:10,910 --> 00:50:13,070
well that's one megapixel

763
00:50:13,090 --> 00:50:14,740
raise to the

764
00:50:14,740 --> 00:50:16,700
three hundred sixty

765
00:50:16,720 --> 00:50:23,160
that's going on the mind-blowingly large numbers so i just wanted to represent the distribution

766
00:50:23,160 --> 00:50:24,840
of images by saying well

767
00:50:24,890 --> 00:50:29,740
for every possible image is going to have a probability and probabilities are just numbers

768
00:50:29,760 --> 00:50:31,160
and summer have two

769
00:50:31,220 --> 00:50:35,370
OK this is going to be in deep trouble because the probability will be thinking

770
00:50:35,680 --> 00:50:39,430
as so many entries that you could never write down

771
00:50:39,450 --> 00:50:44,620
not only because computer every and even if you could write down there would be

772
00:50:44,620 --> 00:50:47,570
no way for you to estimate the distribution because you

773
00:50:47,570 --> 00:50:52,240
it's all like you know all space that only trained on the two thousand images

774
00:50:52,300 --> 00:50:55,180
to have a few thousand these observations in the

775
00:50:55,450 --> 00:51:00,070
billion billion dimensional space how you're going to do that so

776
00:51:00,090 --> 00:51:04,600
just wrong representation of joint probabilities is never going to work

777
00:51:04,620 --> 00:51:08,680
OK so you need to make some assumptions about the distribution so here's the simplest

778
00:51:08,680 --> 00:51:13,140
assumption you could make the distribution is completely factorized

779
00:51:13,200 --> 00:51:15,820
so the probability of all the

780
00:51:15,840 --> 00:51:21,550
joint probability of all the observations x is just a factorized into the product of

781
00:51:21,550 --> 00:51:23,640
the marginal distribution the

782
00:51:23,740 --> 00:51:25,260
now this is completely

783
00:51:25,510 --> 00:51:26,640
this it says

784
00:51:26,640 --> 00:51:31,370
for the data as the conjugate the multivariate bernoulli distribution

785
00:51:31,430 --> 00:51:36,550
is this beta distribution this is exactly analogous to what i showed you

786
00:51:37,680 --> 00:51:42,940
the discrete multinomial distribution and the dirichlet distribution the

787
00:51:43,420 --> 00:51:49,550
bernoulli distribution is just a discrete distribution we only have two choices

788
00:51:49,550 --> 00:51:55,910
and the debated distribution is that it exactly a dirichlet distribution when you only have

789
00:51:58,860 --> 00:52:01,360
two possible outcomes

790
00:52:01,430 --> 00:52:05,140
so this is the sort of a special case of the dirichlet that we had

791
00:52:06,080 --> 00:52:11,200
except that we just have a parameter beta j and on the other outcome is

792
00:52:11,200 --> 00:52:13,260
one minus

793
00:52:14,980 --> 00:52:21,320
i remember this term here is the normalisation constant for that

794
00:52:21,360 --> 00:52:29,220
beta distribution or dirichlet distribution and we have a hyperparameter alpha j beta j

795
00:52:29,250 --> 00:52:31,730
which i have to tell you how to do that

796
00:52:31,790 --> 00:52:33,500
before you observe the

797
00:52:33,600 --> 00:52:36,250
the query you have to be able to that the

798
00:52:36,360 --> 00:52:38,450
but these represent our sort of

799
00:52:38,510 --> 00:52:40,940
prior on where we think are

800
00:52:40,970 --> 00:52:42,690
concept might end up

801
00:52:42,700 --> 00:52:45,650
in the space of possible data

802
00:52:47,720 --> 00:52:49,760
now the nice thing about

803
00:52:49,810 --> 00:52:54,140
conjugate priors that exponential family distributions

804
00:52:54,200 --> 00:52:55,310
is that

805
00:52:55,310 --> 00:52:57,120
we can multiply

806
00:52:57,140 --> 00:53:00,670
this prior by the likelihood

807
00:53:00,680 --> 00:53:06,220
and integrate out the data and we get an analytic expression for that in the

808
00:53:07,630 --> 00:53:09,820
and in fact

809
00:53:09,850 --> 00:53:11,130
that's exactly

810
00:53:12,220 --> 00:53:14,330
an expression of the form

811
00:53:14,340 --> 00:53:17,540
the normalizing constant here

812
00:53:17,550 --> 00:53:21,280
OK because there is no one because way we got that was by

813
00:53:21,290 --> 00:53:23,230
integrating this stuff

814
00:53:23,700 --> 00:53:25,610
from zero to one

815
00:53:25,660 --> 00:53:28,040
from data going from zero to one

816
00:53:28,040 --> 00:53:31,690
that the normal i think that we need to get the whole thing to integrate

817
00:53:31,690 --> 00:53:33,040
to one

818
00:53:33,050 --> 00:53:34,170
and so

819
00:53:34,440 --> 00:53:38,360
it's exactly that kind of indigo that we need to be able to compute any

820
00:53:38,360 --> 00:53:41,190
of the marginal likelihood that i mentioned in

821
00:53:41,810 --> 00:53:43,420
but before

822
00:53:43,480 --> 00:53:46,780
so i can go through that in detail but eventually

823
00:53:46,780 --> 00:53:49,630
with just a little bit of algebra

824
00:53:49,700 --> 00:53:53,970
you get an analytic expression for the score of an idea

825
00:53:54,020 --> 00:53:58,140
which is a ratio involving all of these gamma function

826
00:53:58,160 --> 00:54:00,420
which would then be

827
00:54:00,470 --> 00:54:02,910
and what is the gamma function

828
00:54:02,960 --> 00:54:05,310
corresponding to

829
00:54:06,510 --> 00:54:11,350
the parameters of the prior these alpha j beta j

830
00:54:11,390 --> 00:54:13,030
those parameters

831
00:54:13,970 --> 00:54:16,980
the range of data before we observe the

832
00:54:17,010 --> 00:54:19,160
query that

833
00:54:19,210 --> 00:54:21,790
and then we also have parameters

834
00:54:24,380 --> 00:54:26,530
still the ambady g tilde

835
00:54:26,530 --> 00:54:28,610
which represent the

836
00:54:28,640 --> 00:54:32,680
range of the parameters after observing the query that

837
00:54:32,690 --> 00:54:38,150
distribution posterior distribution of the parameters after observing the query which is also part of

838
00:54:38,800 --> 00:54:40,080
asian so

839
00:54:40,080 --> 00:54:42,530
albertville the j

840
00:54:42,540 --> 00:54:45,380
it's just alpha j one

841
00:54:45,440 --> 00:54:48,750
the sum over the items in the query that

842
00:54:50,120 --> 00:54:55,830
the features of the g featured in the item and they don't feel the j

843
00:54:55,860 --> 00:54:58,980
is the data is the same thing

844
00:54:59,020 --> 00:55:04,630
the sum over how many times that feature did not appear in the group

845
00:55:04,650 --> 00:55:05,650
so all we do

846
00:55:05,650 --> 00:55:08,100
it is essentially for each of the features

847
00:55:08,120 --> 00:55:11,480
under the model we would have to count how many times it appears in the

848
00:55:12,600 --> 00:55:15,880
i we find it doesn't appear in the query that n is the number of

849
00:55:15,880 --> 00:55:20,980
items in the query that we can compute the output over the data the very

850
00:55:22,100 --> 00:55:24,900
and then we compute the score

851
00:55:27,630 --> 00:55:30,360
now this still looks very very nasty

852
00:55:30,380 --> 00:55:33,190
but they can be dramatically simplified because

853
00:55:33,250 --> 00:55:35,120
the gamma function is the

854
00:55:35,120 --> 00:55:37,980
it is a generalization of the factorial

855
00:55:38,020 --> 00:55:41,290
and so a lot of things that have allowed

856
00:55:41,310 --> 00:55:44,040
so if we do that

857
00:55:44,040 --> 00:55:47,320
many different ways different realms of thought

858
00:55:48,330 --> 00:55:51,390
domains of expertise or whatever

859
00:55:52,770 --> 00:55:55,260
here's a real life example

860
00:55:59,740 --> 00:56:03,390
this child child is building something in

861
00:56:03,720 --> 00:56:08,090
here are seven or eight things that this

862
00:56:08,160 --> 00:56:10,790
the girl may be thinking

863
00:56:10,850 --> 00:56:13,840
what if i pulled out the bottom block

864
00:56:14,620 --> 00:56:18,420
should i help him with the tower knocked down

865
00:56:18,440 --> 00:56:23,010
how would react to that

866
00:56:23,050 --> 00:56:29,080
can i reach the arc shape block that's here

867
00:56:29,130 --> 00:56:34,830
i forget where i left the arc shaped block maybe she can't quite see it

868
00:56:34,880 --> 00:56:41,680
and so forth how it feels to hold three these three blocks at once and

869
00:56:41,680 --> 00:56:42,670
so forth

870
00:56:42,720 --> 00:56:45,540
every one thinks all those things

871
00:56:46,430 --> 00:56:50,570
we don't know if they think them in parallel rapidly oriented

872
00:56:50,600 --> 00:56:53,610
you switch between them or whatever

873
00:56:53,620 --> 00:56:57,210
but to make a machine that has common sense

874
00:56:57,270 --> 00:57:00,880
it's going to have to be able to interpret each event

875
00:57:00,910 --> 00:57:03,620
in many different ways

876
00:57:10,070 --> 00:57:13,910
late last year i finished the book

877
00:57:13,960 --> 00:57:17,330
about this sort of thing and i recommend

878
00:57:17,370 --> 00:57:22,810
did you read it and email me if you have any complaints

879
00:57:22,860 --> 00:57:25,310
and you can either buy the book or

880
00:57:25,320 --> 00:57:29,230
almost all of its on my home page

881
00:57:32,100 --> 00:57:34,930
a slightly older than the published draft

882
00:57:34,990 --> 00:57:36,410
and probably

883
00:57:36,780 --> 00:57:42,410
better in various ways because in the last stages i smooth things out and try

884
00:57:42,430 --> 00:57:44,410
to make the language better

885
00:57:44,420 --> 00:57:50,610
and probably made it more obscure

886
00:57:50,800 --> 00:57:59,810
i'm always trying to organize what i'm going to say and

887
00:57:59,830 --> 00:58:03,940
changing my mind at the last minute

888
00:58:03,960 --> 00:58:10,550
and gerry sussman just scolded me for that

889
00:58:10,590 --> 00:58:14,660
so i'm going to say a little bit about

890
00:58:14,720 --> 00:58:18,580
what are the popular things in artificial intelligence today

891
00:58:18,610 --> 00:58:20,960
i call them fads because

892
00:58:20,980 --> 00:58:25,760
i think most of them will go away but that could be wrong

893
00:58:28,630 --> 00:58:30,960
here are eight of them

894
00:58:31,010 --> 00:58:36,020
very few people today work on the kind of symbolic reasoning

895
00:58:36,030 --> 00:58:39,380
that was needed first legal and bob rose and

896
00:58:39,440 --> 00:58:43,180
those other old program when the grids

897
00:58:43,260 --> 00:58:45,940
programs that i mentioned before

898
00:58:47,120 --> 00:58:48,900
most efforts are

899
00:58:48,910 --> 00:58:53,080
in the direction of saying that i don't want to have to program everything the

900
00:58:53,080 --> 00:58:54,450
machine does

901
00:58:54,470 --> 00:58:58,040
that would be dead and maybe would take millions of years

902
00:58:58,040 --> 00:58:59,780
there must be a better way

903
00:58:59,790 --> 00:59:04,850
so let's find some way for the machine to develop itself by

904
00:59:04,860 --> 00:59:07,820
perhaps by experience with the outer world

905
00:59:09,840 --> 00:59:14,090
being immersed in in a real body in the real

906
00:59:15,730 --> 00:59:19,530
so let's see if we can invent away for it to learn everything

907
00:59:19,540 --> 00:59:23,890
well in fact humans don't learn very much from experience

908
00:59:23,920 --> 00:59:26,550
all that we know about children

909
00:59:26,620 --> 00:59:28,900
is that if you have a child who is

910
00:59:28,930 --> 00:59:31,800
disconnected from other people

911
00:59:31,840 --> 00:59:34,920
left in its own physical world

912
00:59:34,980 --> 00:59:36,550
it will grow up

913
00:59:36,550 --> 00:59:38,460
as an idiot

914
00:59:38,520 --> 00:59:42,360
now there is no evidence for this to speak of because it's illegal to do

915
00:59:42,360 --> 00:59:44,430
that experiment

916
00:59:44,490 --> 00:59:49,320
the amusing thing is that if you want to find out about feral children

917
00:59:49,340 --> 00:59:51,510
children who grew up

918
00:59:51,700 --> 00:59:54,060
being raised by wolves or

919
00:59:54,070 --> 00:59:55,920
in nature

920
00:59:55,990 --> 00:59:59,070
or with no human contact

921
00:59:59,120 --> 01:00:02,200
the most detailed story and the most cited one

922
01:00:02,220 --> 01:00:04,580
comes from seventeen fifty

923
01:00:04,620 --> 01:00:09,930
about the wild boy of have their own who was rescued from a forest and

924
01:00:09,930 --> 01:00:11,570
apparently had been

925
01:00:11,590 --> 01:00:13,410
raised by wolves

926
01:00:14,830 --> 01:00:18,790
but and was very abnormal and never learned to speak

927
01:00:18,800 --> 01:00:20,200
and so forth

928
01:00:20,270 --> 01:00:23,490
but we don't know anything about such children because

929
01:00:23,540 --> 01:00:30,330
the most likely case seems to be that the child looks so retarded that it's

930
01:00:30,340 --> 01:00:33,730
parents just left out to die and didn't

931
01:00:33,790 --> 01:00:38,540
so no one knows but i suspect that children do not learn much from experience

932
01:00:38,550 --> 01:00:41,900
and they learned most of what we call common sense

933
01:00:41,920 --> 01:00:44,190
by being told or shown or

934
01:00:44,200 --> 01:00:45,910
four taught and they don't

935
01:00:46,790 --> 01:00:50,970
very much by themselves anyway nobody knows much about

936
01:00:50,990 --> 01:00:55,440
the most popular kind of learning theory is to have the machine do things at

937
01:00:56,450 --> 01:01:00,380
and you press a reward but when it does what you want

938
01:01:00,380 --> 01:01:06,470
you directly control how many support vectors you get home any points outside the tube

939
01:01:06,490 --> 01:01:09,970
and just a few pictures about this

940
01:01:09,980 --> 01:01:12,960
so here for instance that's

941
01:01:12,980 --> 01:01:15,660
the dataset generated from the sinc function

942
01:01:16,400 --> 01:01:21,930
x values are think uniform in this interval in the y values are sink of

943
01:01:22,760 --> 01:01:25,300
sign side of x over x

944
01:01:25,310 --> 01:01:27,500
and here we

945
01:01:27,510 --> 01:01:31,400
and it's zero noise and here we have quite a bit of noise

946
01:01:31,420 --> 01:01:33,850
the underlying function the green one

947
01:01:33,860 --> 01:01:36,020
and in both cases

948
01:01:36,040 --> 01:01:40,900
trained and new support vector regression where set new

949
01:01:41,020 --> 01:01:44,490
twenty percent to twenty percent of the points are allowed to lie outside and you

950
01:01:44,490 --> 01:01:50,040
can see that in this case the epsilon ends up being zero

951
01:01:50,120 --> 01:01:54,110
the SVM notices there is no noise in the data in this case the epsilon

952
01:01:54,150 --> 01:01:57,110
inside the rather large

953
01:01:58,260 --> 01:02:00,070
maybe i'll skip

954
01:02:00,090 --> 01:02:02,050
if you have the next slide

955
01:02:04,100 --> 01:02:09,040
rather than going into much more details on support vector regression i want to tell

956
01:02:09,040 --> 01:02:13,870
you a bit about some other algorithms and also i want to is mention the

957
01:02:13,870 --> 01:02:16,830
representer theorem yes

958
01:02:29,070 --> 01:02:33,200
so the a three hard the training procedure

959
01:02:33,440 --> 01:02:38,010
training procedure in the way i showing here is is that strange procedures

960
01:02:38,410 --> 01:02:44,720
training set that's w

961
01:02:44,840 --> 01:02:48,110
you could say that they are

962
01:02:49,190 --> 01:02:53,970
so this is unlikely that all of these properties online

963
01:02:54,020 --> 01:02:56,350
we are more

964
01:03:04,080 --> 01:03:12,300
parts of the input space of what you do not need to know

965
01:03:14,240 --> 01:03:16,950
OK so that's a slightly different question

966
01:03:16,960 --> 01:03:20,240
if you were standing on scenario would be

967
01:03:20,310 --> 01:03:25,960
you just get one point out i don't know the on different parts of the

968
01:03:25,960 --> 01:03:27,990
space point that this

969
01:03:28,790 --> 01:03:34,130
now the situation where to model noise in different parts of the space

970
01:03:34,150 --> 01:03:38,870
which is say roughly speaking with a lot of lies somewhere you probably one lot

971
01:03:39,010 --> 01:03:41,140
size if we only

972
01:03:41,150 --> 01:03:45,550
this is in case you need a small size

973
01:03:45,560 --> 01:03:47,950
that would be handled automatically

974
01:03:48,580 --> 01:03:51,790
we have some methods that it over

975
01:03:51,830 --> 01:03:57,910
which you can use if you have a problem about which parts of the space

976
01:03:58,180 --> 01:04:01,700
which part of speech you would expect a lot

977
01:04:01,720 --> 01:04:07,480
but this in this case you put in some prior knowledge could think about how

978
01:04:07,480 --> 01:04:09,270
to generalize the two

979
01:04:09,320 --> 01:04:14,590
the that also in the sense that size is automatically computed as a function

980
01:04:14,770 --> 01:04:21,670
o point space i think we need lot more data that is more suite of

981
01:04:22,650 --> 01:04:28,700
what i need to be

982
01:04:29,890 --> 01:04:30,720
OK so

983
01:04:30,750 --> 01:04:38,050
just briefly tell you about represented here because there is also a bridge between the

984
01:04:38,320 --> 01:04:42,540
price of a lot of room

985
01:04:42,610 --> 01:04:50,700
so this is the original home classic here by chris wahba in the seventies i

986
01:04:50,700 --> 01:04:56,830
believe is a classic people can and often buy it

987
01:04:56,840 --> 01:05:01,230
and i'm showing a slight generalization of this theorem

988
01:05:01,250 --> 01:05:06,700
so in theorem were given a positive definite kernel

989
01:05:06,720 --> 01:05:07,840
training sets

990
01:05:07,840 --> 01:05:13,340
a strictly monotonic increasing real valued function omega

991
01:05:13,340 --> 01:05:15,640
we will use this function omega

992
01:05:15,680 --> 01:05:18,420
to determine how or regularizer

993
01:05:18,430 --> 01:05:21,460
goes into the objective function

994
01:05:21,590 --> 01:05:24,830
and an arbitrary cost function c

995
01:05:24,840 --> 01:05:28,350
which takes as input the whole training set and

996
01:05:28,370 --> 01:05:32,940
the outputs of f on the whole training set

997
01:05:32,960 --> 01:05:39,410
the the statement is any solution to this optimisation problems of the optimisation problem of

998
01:05:39,410 --> 01:05:45,130
minimizing this thing here so this thing has this original cost function

999
01:05:45,140 --> 01:05:51,370
anyway the most general you can think of and this regularizer which is

1000
01:05:52,520 --> 01:05:57,170
objective functions for instance quadratically but it could also be different

1001
01:05:57,180 --> 01:06:01,470
any solution to this problem admits a representation in this form

1002
01:06:01,470 --> 01:06:03,760
he would say that we have

1003
01:06:03,880 --> 01:06:07,640
sorry even hundreds of thousands of features of variables

1004
01:06:09,710 --> 01:06:11,760
the whole thing might be easy

1005
01:06:11,910 --> 01:06:15,010
on the other hand so

1006
01:06:15,020 --> 01:06:18,670
text is highly redundant so if you remove half of the data

1007
01:06:19,030 --> 01:06:24,330
from the document would still be able to understand or tool to make something out

1008
01:06:24,330 --> 01:06:24,940
of the

1009
01:06:24,950 --> 01:06:27,660
this text analysis tasks

1010
01:06:28,050 --> 01:06:32,000
we know this from everyday practice let's see if e

1011
01:06:33,030 --> 01:06:37,900
newspaper articles and if somebody would remove either half of the words

1012
01:06:37,920 --> 01:06:39,580
and even for the

1013
01:06:40,060 --> 01:06:46,650
according to alphabetical order would still roughly know what's what's the text about

1014
01:06:46,670 --> 01:06:50,810
so this is this redundancy of which text has inside

1015
01:06:51,470 --> 01:06:56,140
and what's interesting that's a lot of very simple approach is actually a

1016
01:06:56,190 --> 01:06:57,080
can get a

1017
01:06:57,090 --> 01:06:59,330
really interesting results

1018
01:06:59,340 --> 01:07:03,000
and so basically a recipe for

1019
01:07:03,050 --> 01:07:07,370
so the text mining can we deal with the first

1020
01:07:07,380 --> 01:07:09,150
extract some

1021
01:07:09,170 --> 01:07:11,610
meaningful words

1022
01:07:11,680 --> 01:07:15,530
units of text phrases words or something like this

1023
01:07:18,120 --> 01:07:25,990
find meaningfully related words so this is something which connects different pieces among other between

1024
01:07:25,990 --> 01:07:30,940
documents and kind create some sort of summary so this is the recipe basically four

1025
01:07:30,990 --> 01:07:35,250
almost every text mining category is just that's what are

1026
01:07:35,650 --> 01:07:39,720
this phrase is what relationships and what what is the our so this is the

1027
01:07:39,720 --> 01:07:41,690
question we can

1028
01:07:41,700 --> 01:07:47,470
of course like in very simple ideas already works or something

1029
01:07:47,530 --> 01:07:52,160
but with more sophisticated algorithms and you can get

1030
01:07:52,290 --> 01:07:59,450
or more complex summaries more complex results so we'll see see some of the best

1031
01:08:00,810 --> 01:08:07,780
so which areas scientific areas of engineering careers are in this text analytics areas so

1032
01:08:07,810 --> 01:08:09,540
one sites

1033
01:08:09,590 --> 01:08:11,700
we have

1034
01:08:11,720 --> 01:08:14,440
machine learning text mining so this would be

1035
01:08:14,590 --> 01:08:16,270
the area of are coming from

1036
01:08:16,290 --> 01:08:22,730
in particular these areas the roots and analysis of the textual data not not so

1037
01:08:22,730 --> 01:08:26,420
much in this area is color other

1038
01:08:26,440 --> 01:08:29,620
other aspects of x then

1039
01:08:30,910 --> 01:08:32,970
even older area

1040
01:08:33,550 --> 01:08:38,940
the text natural language processing which reduces the linguistic aspects of text usually

1041
01:08:38,960 --> 01:08:40,130
natural language

1042
01:08:40,150 --> 01:08:45,830
processing unit is one sentence and then they try to pass it tends to find

1043
01:08:46,200 --> 01:08:49,020
really deep relationships between species of

1044
01:08:50,800 --> 01:08:55,390
of words phrases and so on one machine learning takes a whole bunch of the

1045
01:08:55,390 --> 01:09:00,410
whole corpus of documents and then does some analysis on the

1046
01:09:00,460 --> 01:09:05,940
then next very important area is information through information there is basically about the search

1047
01:09:05,940 --> 01:09:06,920
engine so

1048
01:09:06,970 --> 01:09:12,050
c will all kinds of search which you can find knowledge based on

1049
01:09:12,120 --> 01:09:12,900
one of them

1050
01:09:13,360 --> 01:09:15,590
usually so people

1051
01:09:16,160 --> 01:09:20,460
working in this area which would belong to this information three with

1052
01:09:20,520 --> 01:09:27,480
and the last one kind of the newest one with be semantic web which appeared

1053
01:09:28,170 --> 01:09:33,500
five years ago and the more recent brother of semantic web this web two point

1054
01:09:33,500 --> 01:09:40,700
zero which briefly touch afterwards they do it's smaller semantics of

1055
01:09:40,710 --> 01:09:42,730
text and connecting

1056
01:09:42,740 --> 01:09:46,970
text and maybe some other tool

1057
01:09:47,190 --> 01:09:50,040
vocabularies ontologies to

1058
01:09:50,060 --> 01:09:54,460
two to the more abstract libraries of more abstract

1059
01:09:54,470 --> 01:09:55,620
because concepts

1060
01:09:55,770 --> 01:09:58,080
so altogether let's say

1061
01:09:58,170 --> 01:10:01,410
we can say that this is a text text analytics

1062
01:10:01,810 --> 01:10:04,460
the area which

1063
01:10:04,910 --> 01:10:08,210
at the end tries to

1064
01:10:08,260 --> 01:10:13,480
and bring someone standing in the text in various ways

1065
01:10:13,500 --> 01:10:16,900
OK as i said so machine learning is about data analysis

1066
01:10:18,010 --> 01:10:21,410
natural language processing about computational linguistics

1067
01:10:21,420 --> 01:10:27,210
information theory is about search in databases how to store text and search

1068
01:10:27,220 --> 01:10:28,310
the top of this

1069
01:10:28,320 --> 01:10:30,190
storage and

1070
01:10:30,200 --> 01:10:36,200
semantic web is more like about the knowledge representation and reasoning tagging concerns

1071
01:10:38,370 --> 01:10:46,270
there are three major dimensions of text analytics so three major aspects which we need

1072
01:10:47,280 --> 01:10:51,410
you know if the let's say if we try to do this

1073
01:10:51,420 --> 01:10:54,230
so one is representation so how

1074
01:10:54,250 --> 01:10:56,180
how to represent text

1075
01:10:56,200 --> 01:11:00,160
usually on the input we have text in asking for more

1076
01:11:00,170 --> 01:11:06,970
bdx or in some kind of semi structure are completely constructed before and so on

1077
01:11:06,980 --> 01:11:09,210
this document needs to be represented in terms

1078
01:11:09,510 --> 01:11:15,170
and there's some and so we have on a very simple side

1079
01:11:15,190 --> 01:11:19,090
character level presentations are we presents

1080
01:11:19,140 --> 01:11:21,930
document is a sequence of characters

1081
01:11:21,940 --> 01:11:27,780
and sequence of words and someone who very complex ones which we see later on

1082
01:11:27,780 --> 01:11:33,000
which would be a first order theories which again is just another representation of text

1083
01:11:33,000 --> 01:11:38,000
but much more complex and for some tasks

1084
01:11:38,070 --> 01:11:41,970
it's enough to represent text just as a sequence of characters

1085
01:11:41,980 --> 01:11:43,850
while for some other

1086
01:11:43,990 --> 01:11:49,260
task may be first order theories wouldn't be in techniques

1087
01:11:49,820 --> 01:11:53,330
the station is only one very important aspect techniques

1088
01:11:53,330 --> 01:12:01,010
then we expect positive distance when the class is is

1089
01:12:03,230 --> 01:12:04,610
in the US

1090
01:12:04,620 --> 01:12:08,930
class and we expect the negative distance and no class ocean multiplied by water negative

1091
01:12:09,980 --> 01:12:11,870
this narrative

1092
01:12:11,880 --> 01:12:13,740
this is the vacancy

1093
01:12:13,870 --> 01:12:20,100
OK so now

1094
01:12:20,890 --> 01:12:22,730
we have this likelihood here

1095
01:12:22,750 --> 01:12:24,240
given some distance

1096
01:12:24,860 --> 01:12:28,820
which is a step and if that distances at all

1097
01:12:28,870 --> 01:12:32,640
the the the wrong side then we say zero

1098
01:12:32,720 --> 01:12:35,860
and if is on the right side we say it's one

1099
01:12:35,870 --> 01:12:38,810
and that causes problems when the data is not linearly separable

1100
01:12:38,820 --> 01:12:40,500
like in this case

1101
01:12:44,600 --> 01:12:46,370
we've got this

1102
01:12:46,980 --> 01:12:50,170
step function like which i had there while ago

1103
01:12:50,430 --> 01:12:54,060
and the solution to this is something which

1104
01:12:54,080 --> 01:12:55,370
which is mentioned here

1105
01:12:55,380 --> 01:12:57,250
with that we can use

1106
01:12:57,260 --> 01:13:00,990
a regression technique

1107
01:13:01,010 --> 01:13:04,360
minor look like what you're used to but

1108
01:13:04,410 --> 01:13:07,380
we can use a softer version of this step function

1109
01:13:07,390 --> 01:13:10,050
so it's not quite so harsh on

1110
01:13:10,060 --> 01:13:11,630
points which are

1111
01:13:11,640 --> 01:13:13,440
just one side or the other of the

1112
01:13:13,450 --> 01:13:14,790
of the boundary

1113
01:13:14,800 --> 01:13:18,250
and if we choose this function in the right way

1114
01:13:18,250 --> 01:13:20,070
then we can have

1115
01:13:24,850 --> 01:13:26,260
something which

1116
01:13:26,270 --> 01:13:29,560
when it's right on the boundary

1117
01:13:29,560 --> 01:13:33,140
the zero distance from the decision boundary that i likelihood is

1118
01:13:33,170 --> 01:13:35,060
is point five

1119
01:13:36,370 --> 01:13:40,290
as it moves in one direction or the other

1120
01:13:40,310 --> 01:13:43,010
it becomes close to close to one zero

1121
01:13:43,750 --> 01:13:49,050
so the class of functions we use here are called sigmoids and you know a

1122
01:13:49,050 --> 01:13:50,970
common choice is the

1123
01:13:50,990 --> 01:13:54,250
logistic sigmoid

1124
01:13:54,300 --> 01:13:58,500
which gives us this mapping from

1125
01:14:01,230 --> 01:14:04,940
positiveinfinity to negative infinity in a kind of squeezes down to the

1126
01:14:04,950 --> 01:14:07,440
zero to one range

1127
01:14:07,450 --> 01:14:11,090
all right so you could use that form of likelihood

1128
01:14:11,100 --> 01:14:14,170
it gives you slightly more robust

1129
01:14:14,950 --> 01:14:19,200
results when you have data which is not linearly separable if there's no single line

1130
01:14:19,200 --> 01:14:21,860
you control than it does the right thing

1131
01:14:21,870 --> 01:14:26,000
that's something called logistic regression if you're just trying to

1132
01:14:26,010 --> 01:14:31,690
maximize the likelihood of parameters given all the data points and that's something called logistic

1133
01:14:31,690 --> 01:14:34,900
regression you can also do bayesian logistic regression

1134
01:14:34,920 --> 01:14:39,140
we do the process that we've just

1135
01:14:39,150 --> 01:14:46,940
but with this likelihood spot instead of the step function

1136
01:14:46,950 --> 01:14:51,540
OK let's think about another few ways that we can extend the scheme

1137
01:14:51,550 --> 01:14:53,410
and one thing you may be thinking

1138
01:14:53,440 --> 01:14:58,530
OK this looks like we've got some simple speech recognition that can

1139
01:14:58,550 --> 01:15:00,440
recognise two different words

1140
01:15:00,450 --> 01:15:04,340
now what i thought it was three words what if you want maybe

1141
01:15:04,350 --> 01:15:08,390
or or from the entire the entire language

1142
01:15:09,170 --> 01:15:12,100
you know what we can do with the line in that case

1143
01:15:12,120 --> 01:15:13,500
and one thing you might

1144
01:15:16,700 --> 01:15:19,040
OK if you've got

1145
01:15:19,050 --> 01:15:24,660
if you have like a third class here

1146
01:15:24,690 --> 01:15:26,840
you know one kind of obvious thing is

1147
01:15:27,950 --> 01:15:28,950
you could do

1148
01:15:30,890 --> 01:15:36,190
one split here to divided up into two classes so you don't have the circles

1149
01:15:36,190 --> 01:15:40,870
on the side the pluses and triangles on the side benefits on this side you

1150
01:15:40,870 --> 01:15:42,050
would learn

1151
01:15:42,070 --> 01:15:45,020
another decision boundary dispute of those two classes

1152
01:15:45,080 --> 01:15:46,690
again something you could do

1153
01:15:46,700 --> 01:15:48,870
and in that case you would have

1154
01:15:49,620 --> 01:15:52,660
you know if you decided that if the classifier give you

1155
01:15:52,670 --> 01:15:54,220
this side of the line here

1156
01:15:54,240 --> 01:15:56,300
then you can go ahead and learn

1157
01:15:56,320 --> 01:15:59,080
like a second decision boundary which

1158
01:15:59,100 --> 01:16:00,480
what's all those up

1159
01:16:00,490 --> 01:16:03,910
so you could keep kind of adding lines to give you multi class

1160
01:16:05,170 --> 01:16:07,260
might become a bit cumbersome if you've got many

1161
01:16:07,320 --> 01:16:11,220
many classes but it's still something you could do

1162
01:16:11,230 --> 01:16:18,650
and one way with this is actually particularly effective

1163
01:16:18,660 --> 01:16:20,730
is constrained case

1164
01:16:20,750 --> 01:16:24,990
where you make all the w point masses so let's say

1165
01:16:25,080 --> 01:16:27,470
i'm going to constrained

1166
01:16:27,480 --> 01:16:28,950
all my

1167
01:16:28,990 --> 01:16:34,650
all my life and i've only got two cases and that is

1168
01:16:35,790 --> 01:16:36,630
and it's say a

1169
01:16:36,650 --> 01:16:41,500
it's either that

1170
01:16:41,600 --> 01:16:45,320
or is that

1171
01:16:45,490 --> 01:16:52,610
and if i take instead of lines being able to go at any angle anywhere

1172
01:16:52,620 --> 01:16:58,020
i instead restricted to just these two possible combinations and b can be anything

1173
01:16:58,080 --> 01:17:06,920
then what we get is decision boundaries which are always aligned to the axes

1174
01:17:07,500 --> 01:17:15,350
so in that case we get

1175
01:17:15,390 --> 01:17:16,880
our hypothesis space

1176
01:17:16,890 --> 01:17:20,880
now it looks like a bunch of lines which are all

1177
01:17:20,900 --> 01:17:22,990
which are all axis aligned

1178
01:17:23,000 --> 01:17:28,510
so the only thing we can choose its which axis we want

1179
01:17:28,520 --> 01:17:30,550
and how far along each

1180
01:17:30,570 --> 01:17:31,990
access the

1181
01:17:32,000 --> 01:17:34,820
the the the boundary is

1182
01:17:34,840 --> 01:17:38,520
and if we have

1183
01:17:38,540 --> 01:17:40,460
he plunged into darkness

1184
01:17:40,540 --> 01:17:44,970
alright thanks

1185
01:17:45,000 --> 01:17:46,620
if we

1186
01:17:46,630 --> 01:17:51,630
make this kind of constraint then it becomes possible to evaluate different

1187
01:17:51,720 --> 01:17:57,780
some different likely it seems to be some discussion here lighting conditions are

1188
01:17:57,920 --> 01:18:03,110
now one thing we might want to do is

1189
01:18:03,300 --> 01:18:05,410
now intuitively when we're

1190
01:18:10,700 --> 01:18:14,700
and sure about the decision boundary that

1191
01:18:14,720 --> 01:18:16,610
if we have a

1192
01:18:16,620 --> 01:18:19,460
some data which

1193
01:18:24,140 --> 01:18:35,550
some data which like this and are possible space of decision boundaries is all the

1194
01:18:35,550 --> 01:18:39,530
lines anywhere along here and all the lines anyone where long here going to be

1195
01:18:39,530 --> 01:18:42,990
interested in lines which somehow split up these

1196
01:18:43,000 --> 01:18:45,480
these groups are going to be interested in line

1197
01:18:45,480 --> 01:18:51,350
so so

1198
01:19:17,290 --> 01:19:28,680
it so

1199
01:20:03,080 --> 01:20:10,310
OK can

1200
01:21:23,530 --> 01:21:30,610
that's why i'm not

1201
01:22:40,280 --> 01:23:01,860
so i have a

1202
01:24:28,120 --> 01:24:32,390
you can kind of tell us

1203
01:24:44,880 --> 01:24:48,410
so can

1204
01:24:48,410 --> 01:24:53,390
have a quantum gravity theory was fundamental scale cannot be below it TV

1205
01:24:53,390 --> 01:24:59,190
this gives you very quickly hawking temperature which is in the TV show

1206
01:25:00,210 --> 01:25:02,860
and of course an objective temperature

1207
01:25:02,890 --> 01:25:08,520
what about if TV normally will disappear as instantly as it was used

1208
01:25:08,540 --> 01:25:11,780
so unless you believe that hawking radiation

1209
01:25:11,800 --> 01:25:14,500
it doesn't happen which would

1210
01:25:14,540 --> 01:25:20,690
b against all our field theory calculations now you shouldn't worry too much

1211
01:25:20,800 --> 01:25:28,280
about these things sleeping lake lemon forty he founded in geneva

1212
01:25:28,330 --> 01:25:30,330
OK now

1213
01:25:30,350 --> 01:25:34,960
so black holes have a temperature and then the rest of the argument is very

1214
01:25:34,960 --> 01:25:40,080
straightforward well if you believe in thermodynamics there is the first floor which tells you

1215
01:25:40,080 --> 01:25:42,160
that the change in energy

1216
01:25:42,180 --> 01:25:45,470
is the temperature times the change in entropy

1217
01:25:45,500 --> 01:25:50,880
here energy is the mass of the black holes so BM is the age times

1218
01:25:52,460 --> 01:25:55,020
now remember ph was

1219
01:25:55,080 --> 01:26:01,820
expression involving minors show we can integrate this formula to to define an entropy

1220
01:26:01,870 --> 01:26:03,980
as of the black hole

1221
01:26:04,100 --> 01:26:09,840
the entropy of the black hole has a very universe and more useful rewriting you

1222
01:26:09,840 --> 01:26:13,890
you can write it in terms of mass and charge of course but they prefer

1223
01:26:13,900 --> 01:26:19,080
to write it in this form because this seems to hold always in all dimensions

1224
01:26:19,120 --> 01:26:21,340
four theories

1225
01:26:21,440 --> 01:26:25,890
these are i think that the entropy of the black hole is the area of

1226
01:26:25,890 --> 01:26:27,340
the whole arise on

1227
01:26:27,350 --> 01:26:28,360
four by

1228
01:26:28,370 --> 01:26:33,770
our last square so there is in this sense the canonical entropy per unit corsi's

1229
01:26:33,780 --> 01:26:40,080
an area divided always but by four eight bar times g your

1230
01:26:40,130 --> 01:26:46,440
so one over four h but is the unit of entropy horizon area in all

1231
01:26:46,440 --> 01:26:49,420
quantum gravity theories brains

1232
01:26:49,480 --> 01:26:54,330
and again notice that the entropy divergence of course in the classical limit where h

1233
01:26:54,700 --> 01:27:02,830
goes to zero and it is a huge entropy because is very weak

1234
01:27:02,860 --> 01:27:07,140
OK now this is called the bacon's time hawking entropy and

1235
01:27:07,190 --> 01:27:14,050
now let's go back to some very elementary statistical mechanics entropy in usual thermodynamics and

1236
01:27:14,050 --> 01:27:19,970
statistical mechanics is the derived quantity if we have microscopic description of the system

1237
01:27:20,020 --> 01:27:24,700
we can actually compute entropy and here is a very simple example of how you

1238
01:27:24,710 --> 01:27:25,980
do these

1239
01:27:26,000 --> 01:27:29,570
and one which will point to a couple of points

1240
01:27:29,610 --> 01:27:33,860
many later so let's think of one of the simplest

1241
01:27:33,930 --> 01:27:40,180
systems we have in thermodynamics that's the paramagnetic some material which properties to the magnetic

1242
01:27:40,180 --> 01:27:42,010
field b

1243
01:27:42,070 --> 01:27:47,150
and now let us assume that we have some microscopic description of this potomac for

1244
01:27:47,150 --> 01:27:54,170
instance let's assume that it is described by n impurities magnetic impurities which correspond to

1245
01:27:54,180 --> 01:27:58,970
non interacting spins seeking mike that he has been seeking

1246
01:27:58,990 --> 01:28:00,890
in this

1247
01:28:00,910 --> 01:28:01,800
piece of

1248
01:28:03,180 --> 01:28:09,230
and interacting with the magnetic field in the usual way namely magnetic moment dot

1249
01:28:09,360 --> 01:28:13,990
the magnetic fields not just to get rid of factors i have here put the

1250
01:28:14,120 --> 01:28:18,930
elementary magnetic moments to be equal to twice the four-minute

1251
01:28:18,940 --> 01:28:24,230
so i have written down this people that energy functional energy function which is the

1252
01:28:24,230 --> 01:28:29,130
sum of all spins magnetic moment times magnetic field

1253
01:28:29,180 --> 01:28:33,720
how do we compute the energy well that's very simple i just have to count

1254
01:28:33,720 --> 01:28:39,480
how many spins are aligned along the magnetic field and how many in the other

1255
01:28:40,510 --> 01:28:47,930
so the total energy is the magnetic field times and plus minus and mines

1256
01:28:48,000 --> 01:28:54,890
now how do we calculate the entropy the entropy in quantum mechanics and quantum statistical

1257
01:28:54,890 --> 01:29:01,030
mechanics is simply counting problem just count how many states you have with the given

1258
01:29:01,030 --> 01:29:03,740
energy that's what the answer is

1259
01:29:03,750 --> 01:29:09,040
the more the number of states you have given energy the higher the entropy

1260
01:29:09,050 --> 01:29:12,830
actually you have to compute the logarithmic this

1261
01:29:14,300 --> 01:29:19,730
in this particular problem it's trivial counting just you can't the number of ways of

1262
01:29:19,730 --> 01:29:24,570
choosing plus out of n the total number of spins you take the log out

1263
01:29:24,570 --> 01:29:27,820
if you find this very simple expression

1264
01:29:27,910 --> 01:29:29,430
and it's

1265
01:29:29,440 --> 01:29:33,680
extensive it grows of course with the volume of the number of bins the system

1266
01:29:33,680 --> 01:29:38,590
linearly and then there is some expression involves the energy density

1267
01:29:38,590 --> 01:29:42,550
divided by the magnetic field as everything here

1268
01:29:42,580 --> 01:29:47,940
now this was a very simple calculation because they shown spins that don't interact if

1269
01:29:47,940 --> 01:29:54,240
you try to do the calculations taking into account interactions of spins it starts becoming

1270
01:29:54,250 --> 01:29:56,410
harder of course because now

1271
01:29:56,430 --> 01:29:58,720
i cannot choose independently

1272
01:29:58,750 --> 01:30:03,650
this all over the place you know nearby spins with line and so on and

1273
01:30:03,650 --> 01:30:04,590
so forth

1274
01:30:04,620 --> 01:30:08,010
so the problem is harder but the basic

1275
01:30:08,030 --> 01:30:09,800
the a technique

1276
01:30:09,820 --> 01:30:14,680
simply what can we just have to do it if more technical work and the

1277
01:30:14,680 --> 01:30:19,520
perturbation theory and you will find some expression that starts out to this one then

1278
01:30:19,530 --> 01:30:21,890
has corrections

1279
01:30:21,900 --> 01:30:25,780
if the coupling of the spin spin interactions is very strong on the the other

1280
01:30:25,780 --> 01:30:27,090
hand then

1281
01:30:27,110 --> 01:30:31,430
thinking even of spain's as the microscopic degrees of freedom

1282
01:30:31,440 --> 01:30:34,050
maybe altogether misleading and then

1283
01:30:34,070 --> 01:30:37,530
you know the score calculation may be irrelevant

1284
01:30:37,880 --> 01:30:43,200
for the problem however this is simply because we we don't have

1285
01:30:43,250 --> 01:30:48,490
in nice microscopic description that can handle if you had some other way of doing

1286
01:30:48,490 --> 01:30:53,620
it like lattice calculations it would still be OK

1287
01:30:53,650 --> 01:30:58,450
OK so this is how we do entropy calculations to statistical mechanics now one do

1288
01:30:58,450 --> 01:31:03,690
a similar calculation for the black hole actually what we have an entropy

1289
01:31:03,710 --> 01:31:09,370
the bacon's steinhardt entropy derived in a very indirect way as crucial for this

1290
01:31:09,380 --> 01:31:11,100
until in calculations

1291
01:31:11,110 --> 01:31:15,510
but to have a very simple expression can we understand it through some

1292
01:31:15,530 --> 01:31:19,870
counting problem like in the case of the potomac

1293
01:31:19,870 --> 01:31:22,840
now let's make a comment here that

1294
01:31:22,890 --> 01:31:25,370
in the case of the paramagnetic sort of

1295
01:31:25,370 --> 01:31:27,290
so this the preimage

1296
01:31:27,320 --> 01:31:29,120
and this the post image

1297
01:31:31,010 --> 01:31:32,370
post image

1298
01:31:32,380 --> 01:31:36,330
if you look about halfway down the left hand side is learned about yin-yang yang

1299
01:31:39,940 --> 01:31:41,700
first post image

1300
01:31:41,720 --> 01:31:44,330
three which is to preach

1301
01:31:44,350 --> 01:31:47,240
you might call the seeing a machine that will be confusing

1302
01:31:51,740 --> 01:31:55,760
OK so now what we did allan jepson and suggested he said well you turn

1303
01:31:55,850 --> 01:31:56,670
it on

1304
01:31:56,680 --> 01:31:58,480
sparse translations

1305
01:31:58,500 --> 01:32:03,880
why don't you give it transparent images so give it sparse stop words but some

1306
01:32:03,880 --> 01:32:06,660
don't translate one way some documents translate another

1307
01:32:06,700 --> 01:32:08,730
and we know what happens when chose to people

1308
01:32:08,740 --> 01:32:13,240
if the two directions the translation quite similar within about thirty degrees this in one

1309
01:32:14,510 --> 01:32:17,740
if there are more than about thirty degrees c two motions and emotions repelled from

1310
01:32:17,740 --> 01:32:19,960
each other just like

1311
01:32:20,730 --> 01:32:21,920
luckily the

1312
01:32:21,930 --> 01:32:25,240
roman didn't know that did this work and he was slightly irritated that they were

1313
01:32:25,550 --> 01:32:28,740
repelled in his data and i was good news radio

1314
01:32:29,210 --> 01:32:33,980
it's good news having model that doesn't work if a psychologist because maybe people don't

1315
01:32:34,090 --> 01:32:35,280
the same way

1316
01:32:38,170 --> 01:32:39,760
so we tried it on

1317
01:32:41,070 --> 01:32:43,330
but just one translation china

1318
01:32:43,340 --> 01:32:45,170
then we have the second rule

1319
01:32:45,240 --> 01:32:47,220
which we encourage to be sparse

1320
01:32:47,220 --> 01:32:52,380
all unsupervised the second thing that i learned units that are tuned to

1321
01:32:52,400 --> 01:32:56,760
the direction of motion so each unit has a preferred direction in job-loss kind of

1322
01:32:56,760 --> 01:32:57,450
the way

1323
01:32:58,520 --> 01:32:59,630
you comments

1324
01:32:59,630 --> 01:33:02,090
decide what is network perceiving

1325
01:33:02,120 --> 01:33:06,160
by taking all those units and the second in two directions

1326
01:33:06,170 --> 01:33:09,810
and adding the beliefs of all ones the all

1327
01:33:09,830 --> 01:33:13,720
actually wait how how much they want to be on what they believe and then

1328
01:33:13,720 --> 01:33:16,140
you get a distribution over directions i believe

1329
01:33:16,140 --> 01:33:18,020
but everything certain unsupervised

1330
01:33:18,090 --> 01:33:21,480
and the answer is when you presented to dot patterns if within about thirty degrees

1331
01:33:21,480 --> 01:33:22,640
it sees one motion

1332
01:33:22,650 --> 01:33:26,690
and it's more about thirty degrees apart is these two motions against two peaks and

1333
01:33:26,690 --> 01:33:29,410
what it believes and the slightly repelled

1334
01:33:29,430 --> 01:33:31,750
so that proves that this is how the brain works

1335
01:33:31,780 --> 01:33:37,660
OK i'm not going to go to time series models that's very that's one kind

1336
01:33:37,660 --> 01:33:39,800
of enters model

1337
01:33:39,820 --> 01:33:43,580
most people in our community when they want to model time series when actually like

1338
01:33:43,580 --> 01:33:48,270
to use non linear distributed representations because a much more powerful and interesting

1339
01:33:48,280 --> 01:33:52,030
but they kind of hard to learn especially if use directed models

1340
01:33:52,070 --> 01:33:55,540
so what they do is they fall back on the old faithfuls which are give

1341
01:33:55,540 --> 01:34:00,330
up on distributed and markov model will give up on non linear and nonlinear dynamical

1342
01:34:01,420 --> 01:34:06,280
now if you could go a little bit further non image should call systems

1343
01:34:06,310 --> 01:34:08,550
that you live

1344
01:34:08,570 --> 01:34:11,390
but what we're going to do is learn a far we cannot give up on

1345
01:34:11,390 --> 01:34:17,700
mathematics and learn a thoroughly non-linear distributed representations

1346
01:34:17,710 --> 01:34:19,810
so it's kind of like this

1347
01:34:19,840 --> 01:34:24,460
on the right-hand side you see the standard restricted boltzmann machine it has visible units

1348
01:34:24,460 --> 01:34:27,740
the some hidden units

1349
01:34:27,830 --> 01:34:33,760
and the states of those units are conditioned on previous time slices

1350
01:34:34,850 --> 01:34:37,540
so we we regard the history as

1351
01:34:37,580 --> 01:34:42,900
fixed and not changeable and what the previous time slices have conditioning connections which in

1352
01:34:42,900 --> 01:34:45,680
effect change the biases of the visible units

1353
01:34:45,680 --> 01:34:49,220
that's called water aggressive model use linear visible units

1354
01:34:49,230 --> 01:34:52,980
and they also have condition connections which change the biases of the hidden units

1355
01:34:53,030 --> 01:34:58,070
so this i dynamic biases that depend on the previous state

1356
01:34:58,790 --> 01:35:00,240
you can learn this

1357
01:35:01,470 --> 01:35:05,140
you put data in a given the data the previous station the current of the

1358
01:35:05,140 --> 01:35:08,140
hidden units are independent so you can sample estates

1359
01:35:08,190 --> 01:35:12,460
he then reconstruct just the current visible data

1360
01:35:12,480 --> 01:35:15,000
and then you activate the hidden units again

1361
01:35:15,010 --> 01:35:17,130
and use the difference differences statistics

1362
01:35:17,220 --> 01:35:20,480
and you take the you would have used to change the biases of the hidden

1363
01:35:20,480 --> 01:35:21,940
units visible units

1364
01:35:21,990 --> 01:35:26,420
and you back propagate that derivative to change the weights coming from previous

1365
01:35:26,430 --> 01:35:28,370
states the visible units

1366
01:35:28,380 --> 01:35:33,980
so you can all of these connections quite simply using contrastive divergence

1367
01:35:34,010 --> 01:35:36,980
once you've learned you can generate from the model

1368
01:35:37,000 --> 01:35:40,430
and the way generate from the model is you have to initialize the few sensible

1369
01:35:41,510 --> 01:35:43,500
and then given the previous frames

1370
01:35:43,530 --> 01:35:47,340
what they do is simply determine the biases of the visible and hidden units and

1371
01:35:47,340 --> 01:35:49,490
then you do alternating gibbs sampling

1372
01:35:49,510 --> 01:35:53,300
but you don't have to give a very long because we strongly determined biases there

1373
01:35:53,300 --> 01:35:55,850
are many places and really go to it has to fit in with what just

1374
01:35:56,740 --> 01:35:59,520
so you don't have to go for huge length of time

1375
01:35:59,540 --> 01:36:03,330
and then you just samples state the visible units in action is visible frame and

1376
01:36:03,330 --> 01:36:06,210
so you can keep some

1377
01:36:06,250 --> 01:36:10,640
you can actually is that these models are so you can learn one model

1378
01:36:10,680 --> 01:36:13,980
united states for the hidden units you treat those data

1379
01:36:14,000 --> 01:36:18,120
once the data you can i put in autoregressive connections between them

1380
01:36:18,120 --> 01:36:21,440
and at another hidden in the the next leg

1381
01:36:21,450 --> 01:36:23,590
and if you do that you'll get the best

1382
01:36:23,620 --> 01:36:27,820
and graham taylor shown that if you're doing this for modeling metadata adding second to

1383
01:36:27,820 --> 01:36:29,700
work much better

1384
01:36:29,740 --> 01:36:33,440
that's not what we're gonna do

1385
01:36:33,450 --> 01:36:36,450
we can apply this kind of model to motion capture data but we're going to

1386
01:36:36,460 --> 01:36:41,690
do with these three way interactions which like multiplicative things

1387
01:36:41,700 --> 01:36:45,720
so you can capture human motion by putting markers on the joints and obviously cameras

1388
01:36:45,720 --> 01:36:48,940
and figure out where they were in space the joint angles

1389
01:36:48,960 --> 01:36:52,270
so then frame is going to be a bunch of joint angles which are real

1390
01:36:53,200 --> 01:36:56,630
so can use against in binary RBM to begin with but this program is to

1391
01:36:56,630 --> 01:37:01,740
begin with but we got something better

1392
01:37:03,200 --> 01:37:10,630
i retract my work we're basically using kerosene binary RBM the visible units

1393
01:37:10,640 --> 01:37:14,070
we also had in the translation of the base of the spine

1394
01:37:14,090 --> 01:37:16,750
and the rotation of the base of the spine

1395
01:37:16,800 --> 01:37:19,460
for rotation we do the actual angles

1396
01:37:19,470 --> 01:37:21,040
for the

1397
01:37:21,700 --> 01:37:23,760
page and the world

1398
01:37:23,840 --> 01:37:25,670
but the change in angle for the

1399
01:37:25,740 --> 01:37:27,440
because gravity doesn't care about

1400
01:37:27,450 --> 01:37:31,010
the absolute value of your

1401
01:37:31,070 --> 01:37:35,920
so now we can do a three-way version of the model i showed you before

1402
01:37:35,980 --> 01:37:38,000
what we can do is take the basic model

1403
01:37:38,030 --> 01:37:41,230
a modulate all the interaction matrices

1404
01:37:41,240 --> 01:37:42,970
using a style variable

1405
01:37:42,980 --> 01:37:46,670
so the style variable is the one of and started

1406
01:37:46,720 --> 01:37:51,410
that gets expanded into a real valued vector of one hundred style features

1407
01:37:51,430 --> 01:37:56,000
those style features are used as one of the inputs to factor so each factor

1408
01:37:56,150 --> 01:38:01,740
is looking at some linear combination of style features some weighted linear combination of those

1409
01:38:01,760 --> 01:38:03,540
and then the

1410
01:38:03,540 --> 01:38:07,320
this mapping so little bit of area was there a little bit of because you

1411
01:38:07,320 --> 01:38:09,470
not perfect separation

1412
01:38:09,520 --> 01:38:13,350
so it's not intrinsically any questions

1413
01:38:15,490 --> 01:38:17,770
and actually there have been some a number of

1414
01:38:17,780 --> 01:38:23,890
application would have used methods like this so one can think of this as well

1415
01:38:23,890 --> 01:38:27,350
as kind of getting theoretical motivation for

1416
01:38:27,410 --> 01:38:29,990
a kind of approaches used in practice so

1417
01:38:33,110 --> 01:38:34,020
you take

1418
01:38:38,660 --> 01:38:40,060
basically trying to

1419
01:38:41,210 --> 01:38:42,830
should remember

1420
01:38:42,850 --> 01:38:44,850
the paper by

1421
01:38:46,830 --> 01:38:48,540
where we try

1422
01:38:48,560 --> 01:38:53,040
two your we might try to find things out there on the web for instance

1423
01:38:53,040 --> 01:38:57,610
that measuring similarity to those might give good feature information basically trying to invent new

1424
01:38:59,330 --> 01:39:04,370
finding objects to being i might be useful features

1425
01:39:04,390 --> 01:39:10,560
so so now if this is going to be an even

1426
01:39:16,120 --> 01:39:19,540
OK so so now the question OK so

1427
01:39:19,540 --> 01:39:24,050
given we're kind of thinking of these similarity functions

1428
01:39:24,100 --> 01:39:27,850
and they give us a different angle thinking about clustering so from a theoretical perspective

1429
01:39:27,850 --> 01:39:31,350
clusters are kind of funny problem because at least learning you can tell how are

1430
01:39:31,350 --> 01:39:36,480
you doing i mean you a cumbersome algorithm and

1431
01:39:36,480 --> 01:39:41,080
whatever you did something and then you can test your error on new data structure

1432
01:39:41,090 --> 01:39:42,740
dating scene

1433
01:39:42,770 --> 01:39:46,370
january ten percent doing pretty well for me to to be doing well for clustering

1434
01:39:47,830 --> 01:39:52,790
is now usually for clustering there isn't an easy objective measures so

1435
01:39:52,850 --> 01:39:56,960
so here's something to think about problem and in the dataset and not

1436
01:39:57,030 --> 01:39:58,020
let's say there

1437
01:39:58,040 --> 01:40:01,710
documents or web pages so here's my documents

1438
01:40:01,740 --> 01:40:05,700
and there is some unknown ground truth clustering is

1439
01:40:06,030 --> 01:40:09,560
imagine every object has some true label its top

1440
01:40:09,610 --> 01:40:13,270
OK so these these documents or web pages they it's about some part of that

1441
01:40:13,270 --> 01:40:14,890
everything is one time

1442
01:40:15,040 --> 01:40:19,290
these are about politics need about sports about economics everybody's got talk

1443
01:40:19,300 --> 01:40:23,910
so these guys from topic the some truly

1444
01:40:23,920 --> 01:40:26,330
and so there's there's there's

1445
01:40:26,340 --> 01:40:27,400
true labels though

1446
01:40:28,090 --> 01:40:29,580
purple one person the

1447
01:40:29,590 --> 01:40:31,300
barely visible light blue

1448
01:40:31,360 --> 01:40:33,390
and our goal

1449
01:40:33,400 --> 01:40:37,320
is to produce the hypothesis of low air so we want our algorithm to get

1450
01:40:38,170 --> 01:40:39,440
you know

1451
01:40:39,700 --> 01:40:44,710
good splitting of the data by topic of the correct answers to these guys topic

1452
01:40:44,710 --> 01:40:49,540
one topic two we were going to produce the clustering in which pretty much these

1453
01:40:49,540 --> 01:40:51,680
guys are in one topic and those guys running out

1454
01:40:51,700 --> 01:40:54,740
now we don't really care about the names the right answer is this topic one

1455
01:40:54,880 --> 01:40:58,900
topic two arguments as well as the one of the with the this clusters right

1456
01:40:58,900 --> 01:41:04,240
so up to isomorphism only one is we want to get to the ground truth

1457
01:41:04,260 --> 01:41:06,940
the problem is that we only have unlabeled data

1458
01:41:07,040 --> 01:41:10,370
right so i mean i got some labelled data the labels of the

1459
01:41:10,420 --> 01:41:14,210
so we can find separator something but what if we only have unlabeled

1460
01:41:14,370 --> 01:41:17,710
so i've got a bunch of documents i tell you each one about some topic

1461
01:41:17,710 --> 01:41:20,090
go ahead and one even for you

1462
01:41:20,120 --> 01:41:20,990
you do it

1463
01:41:21,010 --> 01:41:22,580
that's going to be a hard problem

1464
01:41:22,590 --> 01:41:25,500
but that's kind of what we want doing clustering released the way i want think

1465
01:41:25,500 --> 01:41:30,220
about how to think about clustering as a multi class learning problem we only have

1466
01:41:30,220 --> 01:41:34,260
unlabeled and zero job given these documents is the correctly

1467
01:41:34,270 --> 01:41:34,920
you know

1468
01:41:34,920 --> 01:41:36,790
partition the by topic even though

1469
01:41:36,790 --> 01:41:40,250
you haven't been given any labelled data and it seems like a pretty tough thing

1470
01:41:40,250 --> 01:41:42,440
to try to do

1471
01:41:42,450 --> 01:41:45,970
however according to imagine that we are given something and we're going to be given

1472
01:41:45,970 --> 01:41:51,890
to measure similarity three of pairwise similarity function between objects and so the question is

1473
01:41:51,890 --> 01:41:54,770
how related does that have to be the topic maps

1474
01:41:54,810 --> 01:41:56,620
in order for the help

1475
01:41:56,630 --> 01:42:00,200
that's the kind of question why i want to try to develop a theoretical way

1476
01:42:00,200 --> 01:42:02,800
of looking at this problem

1477
01:42:02,860 --> 01:42:07,960
the case of from hand you measure of similarity between objects how related doesn't have

1478
01:42:07,960 --> 01:42:11,410
to be to the notion of what objects are what topic in order for this

1479
01:42:11,410 --> 01:42:14,650
to be helpful for now to be able to use this to actually get an

1480
01:42:18,160 --> 01:42:22,950
what conditions on a similarity function be enough to allow one to cluster

1481
01:42:23,030 --> 01:42:26,680
this will lead to something like a PAC model

1482
01:42:27,840 --> 01:42:29,270
when i give this too

1483
01:42:29,280 --> 01:42:33,240
this this talk to people coming from algorithms community

1484
01:42:33,290 --> 01:42:36,310
i have to point out that so usually from say

1485
01:42:36,360 --> 01:42:41,350
approximation algorithms perspective the ways that the problem is a bit different there when you

1486
01:42:41,940 --> 01:42:42,840
is you

1487
01:42:42,860 --> 01:42:46,840
start with some weighted graph is some embedding of points in space you pick some

1488
01:42:46,840 --> 01:42:48,080
objective function

1489
01:42:48,140 --> 01:42:51,630
like k median and k means or something and you come up with a couple

1490
01:42:51,720 --> 01:42:57,510
algorithm that optimizes are approximately optimize that object for example the k median objective function

1491
01:42:57,810 --> 01:43:03,560
is find k cluster centers that minimizes the average distance between any point in it

1492
01:43:03,580 --> 01:43:05,140
cluster centre

1493
01:43:05,150 --> 01:43:09,920
the k means objective function is you find k cluster centers and minimize the square

1494
01:43:09,920 --> 01:43:11,080
of the average this

1495
01:43:11,110 --> 01:43:13,860
the average squared distance between points in cluster

1496
01:43:13,870 --> 01:43:19,190
so it's an objective function defined the distance based on the similarity function so given

1497
01:43:19,220 --> 01:43:25,450
OK you might find the cluster centers that maximize the average similarity of

1498
01:43:25,500 --> 01:43:26,650
object two

1499
01:43:26,660 --> 01:43:29,690
there are clusters at

1500
01:43:30,510 --> 01:43:34,160
that's so we really want we want to do is get the topics

1501
01:43:34,250 --> 01:43:37,490
so it's a little bit orthogonal but we want to do is to ask

1502
01:43:37,540 --> 01:43:43,340
how should the similarity function be related topic maps doing something with a similarity function

1503
01:43:43,380 --> 01:43:46,700
allows us to actually get the

1504
01:43:46,710 --> 01:43:49,140
to get the answer looking for

1505
01:43:49,160 --> 01:43:56,110
yes i'm going estimate there are some and think of the number of topics is

1506
01:43:57,530 --> 01:44:02,520
compared to this many clustering problems actually we have lots of little clusters like

1507
01:44:02,570 --> 01:44:06,700
comes up when you're trying to do disambiguation you know these three

1508
01:44:06,720 --> 01:44:09,770
things are all really the same thing in three and they think in a relatively

1509
01:44:09,770 --> 01:44:14,330
small number of topics fixed in advance and then a lot of objects so it's

1510
01:44:14,330 --> 01:44:19,060
like to be three labels for labels

1511
01:44:19,080 --> 01:44:23,410
and the question target how should the similarity function be related to the ground truth

1512
01:44:23,430 --> 01:44:24,720
to alaska

1513
01:44:26,540 --> 01:44:27,660
so i think in nice

1514
01:44:27,670 --> 01:44:30,870
the way to think about this is

1515
01:44:30,920 --> 01:44:34,880
imagine you want the algorithm the clusters of documents the way you would cost

1516
01:44:34,930 --> 01:44:38,690
but documents and you want them kind of clusters in some way

1517
01:44:38,780 --> 01:44:41,540
you know i do it yourself because too painful causes a million of them see

1518
01:44:41,540 --> 01:44:43,800
what i have now we do it

1519
01:44:43,860 --> 01:44:48,120
and you're going to hear the similarity function so how closely related that similarity function

1520
01:44:48,130 --> 01:44:50,680
have to be to what's in your hand about what you're thinking of was the

1521
01:44:50,690 --> 01:44:55,080
topic in order to have to allow the algorithm to do

1522
01:44:55,110 --> 01:44:56,510
you know the right thing

1523
01:44:56,510 --> 01:44:59,320
the resistance of the light bulb

1524
01:45:01,270 --> 01:45:03,680
i believe is twenty five

1525
01:45:03,700 --> 01:45:06,680
and half

1526
01:45:06,710 --> 01:45:10,860
is about two hundred and fifty ohms

1527
01:45:10,880 --> 01:45:13,840
you difference

1528
01:45:13,850 --> 01:45:18,570
so if the

1529
01:45:18,650 --> 01:45:21,690
this is right thing called resistance i would get

1530
01:45:21,730 --> 01:45:23,290
five and is

1531
01:45:23,310 --> 01:45:26,560
by the time the book hot would only get an and

1532
01:45:26,610 --> 01:45:28,290
so huge difference

1533
01:45:28,300 --> 01:45:31,220
what i want to show you

1534
01:45:31,220 --> 01:45:33,450
again with your service called

1535
01:45:33,510 --> 01:45:35,320
is the current

1536
01:45:35,340 --> 01:45:37,020
as a function of time

1537
01:45:37,060 --> 01:45:39,010
you switch on the lights

1538
01:45:40,690 --> 01:45:43,610
you would expect if law holds

1539
01:45:43,870 --> 01:45:45,800
you stress on the current

1540
01:45:45,860 --> 01:45:48,130
switched on the vault existence a

1541
01:45:48,140 --> 01:45:50,340
you see this

1542
01:45:50,350 --> 01:45:53,280
this is the new five mps

1543
01:45:53,280 --> 01:45:55,140
and then i will explain

1544
01:45:55,190 --> 01:45:56,640
first of all i here

1545
01:45:57,610 --> 01:46:02,020
that the voltage divided by the current remains constant

1546
01:46:02,030 --> 01:46:04,600
however what you're going to see is like this

1547
01:46:04,670 --> 01:46:05,740
current goes out

1548
01:46:05,780 --> 01:46:07,750
then the resistance goes down

1549
01:46:07,770 --> 01:46:11,290
then the resistance goes up and the current goes up the resistance goes up

1550
01:46:11,310 --> 01:46:13,590
and therefore the current will go down

1551
01:46:13,640 --> 01:46:17,960
and the level of an level which is substantially below this so you're looking very

1552
01:46:19,170 --> 01:46:20,710
at the breakdown

1553
01:46:20,750 --> 01:46:23,200
of the floor

1554
01:46:23,250 --> 01:46:29,200
so that's what i want to show you now

1555
01:46:29,250 --> 01:46:38,440
so we need to twenty five fold

1556
01:46:38,480 --> 01:46:41,210
and there's the light bulb

1557
01:46:41,230 --> 01:46:43,780
when i throw this switch

1558
01:46:43,840 --> 01:46:45,240
you will see

1559
01:46:45,330 --> 01:46:48,840
the pattern of the current versus time you won't see it once and then recreated

1560
01:46:48,840 --> 01:46:50,670
with your service

1561
01:46:50,920 --> 01:46:53,280
is often

1562
01:46:53,290 --> 01:46:55,600
so look closely now

1563
01:46:57,060 --> 01:46:59,540
get these little ripple that you see on it has to do with the way

1564
01:46:59,540 --> 01:47:00,250
that we

1565
01:47:00,320 --> 01:47:03,110
there was a hundred and twenty five vols

1566
01:47:03,170 --> 01:47:08,090
so you see horizontal time the time between two adjacent vertical lines

1567
01:47:08,140 --> 01:47:09,540
is twenty minutes seconds

1568
01:47:09,580 --> 01:47:14,210
so indeed very early on the current search story to very high value

1569
01:47:14,210 --> 01:47:16,100
and then the filament heats up

1570
01:47:16,190 --> 01:47:18,370
and sort of resistance goes up

1571
01:47:18,390 --> 01:47:19,240
the light bulb

1572
01:47:19,240 --> 01:47:21,570
the current just goes back again

1573
01:47:21,590 --> 01:47:23,040
from the far left

1574
01:47:23,050 --> 01:47:24,540
to the far right

1575
01:47:24,560 --> 01:47:27,500
on the screen is about two hundred millisecond

1576
01:47:27,510 --> 01:47:29,920
that's about two-tenths of a second

1577
01:47:29,930 --> 01:47:31,620
and you get the current levels

1578
01:47:31,640 --> 01:47:33,010
this is way lower

1579
01:47:33,060 --> 01:47:34,230
what you get there

1580
01:47:34,280 --> 01:47:35,460
that's breakdown

1581
01:47:36,800 --> 01:47:42,980
all of

1582
01:47:42,980 --> 01:47:45,460
it's actually very nice

1583
01:47:45,520 --> 01:47:48,550
that resistance is go up

1584
01:47:48,680 --> 01:47:51,150
libel when the temperature

1585
01:47:51,200 --> 01:47:53,990
goes up because supposedly the other way around

1586
01:47:54,040 --> 01:47:56,430
suppose you turn on the light bulb

1587
01:47:56,480 --> 01:47:59,480
and the resistance would go down libel got hot

1588
01:47:59,530 --> 01:48:03,340
resistance goes down that means the current goes up instead of down the current goes

1589
01:48:04,060 --> 01:48:05,540
that means it gets smaller

1590
01:48:05,570 --> 01:48:10,100
that means the resistance goes even further than that means the current goes even further

1591
01:48:10,200 --> 01:48:13,300
so what mean is that every time you turn on the light bulb it was

1592
01:48:13,310 --> 01:48:15,970
right in front of your eyes district itself

1593
01:48:15,970 --> 01:48:17,260
that's not happening

1594
01:48:17,290 --> 01:48:18,740
the other way around

1595
01:48:18,750 --> 01:48:20,560
so in a way it's fortunate

1596
01:48:20,590 --> 01:48:23,020
that the resistance goes up and the light bulb

1597
01:48:27,500 --> 01:48:28,580
all right

1598
01:48:28,600 --> 01:48:30,320
that's now

1599
01:48:30,340 --> 01:48:32,710
be a little bit more complicated

1600
01:48:32,720 --> 01:48:36,010
on some

1601
01:48:36,030 --> 01:48:40,360
networks of resistors

1602
01:48:40,420 --> 01:48:42,330
and we'll have

1603
01:48:42,390 --> 01:48:45,090
do were few problems like that

1604
01:48:45,140 --> 01:48:47,980
whereby we just assume naively

1605
01:48:47,990 --> 01:48:52,420
that small holes in other words we always assume that the values for the resistance

1606
01:48:52,420 --> 01:48:53,930
is that we you

1607
01:48:54,000 --> 01:48:56,690
will not change

1608
01:48:56,750 --> 01:49:00,220
so we will assume that the that is produced

1609
01:49:00,230 --> 01:49:02,560
i will not

1610
01:49:02,580 --> 01:49:05,760
play any important role

1611
01:49:05,760 --> 01:49:07,280
so we just use

1612
01:49:07,330 --> 01:49:08,630
on now

1613
01:49:08,650 --> 01:49:10,980
and if you can use it will be very

1614
01:49:10,990 --> 01:49:12,780
pacific about

1615
01:49:14,400 --> 01:49:16,240
i have here

1616
01:49:16,370 --> 01:49:18,880
three point eight

1617
01:49:18,900 --> 01:49:21,780
and point b

1618
01:49:21,830 --> 01:49:25,240
i have to resist this one

1619
01:49:25,350 --> 01:49:28,440
and r two

1620
01:49:28,450 --> 01:49:31,280
and suppose i apply a potential difference

1621
01:49:31,300 --> 01:49:33,690
between a and b

1622
01:49:33,690 --> 01:49:35,050
this class

1623
01:49:35,080 --> 01:49:38,970
this been minus the potential difference v

1624
01:49:38,980 --> 01:49:41,290
and you know v

1625
01:49:41,310 --> 01:49:42,500
it is known

1626
01:49:42,550 --> 01:49:43,740
will be

1627
01:49:44,010 --> 01:49:47,630
this resistance and i give you that one

1628
01:49:47,650 --> 01:49:49,980
i could ask you now

1629
01:49:50,010 --> 01:49:53,500
what is the current that is going to flow

1630
01:49:53,650 --> 01:49:59,470
i could also ask you then what is the potential difference over this resistance along

1631
01:49:59,480 --> 01:50:01,260
with call the one

1632
01:50:01,310 --> 01:50:04,340
what is the potential difference over the

1633
01:50:04,350 --> 01:50:06,740
second process was called to

1634
01:50:06,780 --> 01:50:09,990
very straightforward question

1635
01:50:10,000 --> 01:50:13,130
well you apply now owns law

1636
01:50:13,190 --> 01:50:18,170
and so between a and b there are two resistors in series

1637
01:50:18,210 --> 01:50:19,740
so the

1638
01:50:19,830 --> 01:50:23,040
because it has to go through both

1639
01:50:23,090 --> 01:50:25,550
and so the potential difference the

1640
01:50:25,600 --> 01:50:28,800
know is not the total current

1641
01:50:28,900 --> 01:50:30,890
times are positive

1642
01:50:34,520 --> 01:50:38,140
both these two resistors would say that the same length

1643
01:50:38,200 --> 01:50:40,110
same cross sectional area

1644
01:50:40,130 --> 01:50:43,080
people doing series you have twice the length

1645
01:50:43,090 --> 01:50:44,940
well so twice the length

1646
01:50:44,970 --> 01:50:49,490
remember resistance is linearly proportional with the length of the line

1647
01:50:49,540 --> 01:50:52,090
and so you add them up

1648
01:50:52,140 --> 01:50:54,370
so now you know i want you know are two

1649
01:50:54,380 --> 01:50:55,310
you know the

1650
01:50:55,400 --> 01:50:57,240
we already know the current

1651
01:50:57,280 --> 01:50:59,190
very simple

1652
01:50:59,190 --> 01:51:01,720
you can also apply only law

1653
01:51:01,740 --> 01:51:03,010
and was it holds

1654
01:51:03,020 --> 01:51:05,560
for this along

1655
01:51:05,580 --> 01:51:08,380
then you get he one

1656
01:51:08,390 --> 01:51:10,360
because i times

1657
01:51:10,360 --> 01:51:11,780
fifteen PSI

1658
01:51:11,800 --> 01:51:13,490
which is one atmosphere

1659
01:51:13,540 --> 01:51:18,740
so i've got one atmosphere here plus an extra atmospheres running at two atmospheres pressure

1660
01:51:18,750 --> 01:51:22,480
so that will raise the boiling point and stave off

1661
01:51:22,530 --> 01:51:25,780
the dangerous gas evolution

1662
01:51:25,820 --> 01:51:30,280
and i can do one more thing instead of running pure water i will add

1663
01:51:30,280 --> 01:51:32,670
ethylene glycol

1664
01:51:32,690 --> 01:51:36,490
o and ethylene glycol about fifty fifty

1665
01:51:36,510 --> 01:51:39,180
per volume you know this is anti-freeze

1666
01:51:39,190 --> 01:51:42,850
it gives the freezing point depression we'll talk about the next day

1667
01:51:42,920 --> 01:51:47,490
but also gives boiling point elevation so in the summertime you should always run with

1668
01:51:47,490 --> 01:51:52,370
anti-freeze because the combination of the pressure cap plus

1669
01:51:52,420 --> 01:51:57,560
the addition of the glycol raises the boiling point from two hundred twelve and talking

1670
01:51:57,560 --> 01:52:02,880
about cars somebody fahrenheit here two hundred twelve degrees fahrenheit to two hundred and sixty

1671
01:52:02,880 --> 01:52:04,870
five degrees fahrenheit

1672
01:52:04,900 --> 01:52:07,470
this buys you much higher question

1673
01:52:07,480 --> 01:52:10,650
so what are we doing were tailoring this by

1674
01:52:10,680 --> 01:52:13,090
making the boiling point of function of

1675
01:52:13,100 --> 01:52:19,010
pressure and now i'm showing you but it's also a function of composition

1676
01:52:20,650 --> 01:52:21,960
this whole business of

1677
01:52:21,970 --> 01:52:26,570
solid state stability i hope i'm showing you has a little bit more

1678
01:52:26,660 --> 01:52:28,790
two it then just looking up the

1679
01:52:28,850 --> 01:52:34,510
transformation temperature on the periodic table of the handbook so i want to talk to

1680
01:52:35,070 --> 01:52:42,010
the next three lectures about phase diagrams phase diagrams are atlases their maps their maps

1681
01:52:42,010 --> 01:52:45,540
of stability that answer the question if you specify

1682
01:52:47,990 --> 01:52:50,600
temperature is one of the stable phase

1683
01:52:50,610 --> 01:52:55,050
that's what you need to know so let's talk about phase diagrams

1684
01:52:55,060 --> 01:52:56,920
phase diagrams

1685
01:52:56,920 --> 01:52:59,360
as stability maps

1686
01:52:59,370 --> 01:53:06,220
stability maps stability of what the stability of the state of aggregation is going to

1687
01:53:06,220 --> 01:53:09,400
tell us if it's a solid liquid

1688
01:53:10,460 --> 01:53:13,170
and under what

1689
01:53:13,220 --> 01:53:15,910
circumstances so

1690
01:53:15,920 --> 01:53:17,110
let's look at some

1691
01:53:17,990 --> 01:53:22,740
simple phase diagrams forming do it i need to define some terms for you to

1692
01:53:22,740 --> 01:53:26,800
first of all it's defined the term phase the phase is a region of the

1693
01:53:26,800 --> 01:53:32,050
substance a region of a substance that has the following characteristics

1694
01:53:34,560 --> 01:53:37,310
and chemical composition

1695
01:53:37,430 --> 01:53:39,960
uniform and chemical composition

1696
01:53:39,990 --> 01:53:44,360
the second thing about it is that it's physically distinct

1697
01:53:44,380 --> 01:53:50,040
physically distinct now give you some rich examples i want to put this down just

1698
01:53:50,040 --> 01:53:53,750
to document and then after you see a few examples these words will mean some

1699
01:53:53,750 --> 01:54:00,440
pictures physically distinct and are in the extreme it's mechanically separable

1700
01:54:00,480 --> 01:54:02,570
this mechanically separable

1701
01:54:02,650 --> 01:54:07,770
physically distinct and mechanically set

1702
01:54:07,860 --> 01:54:13,640
so let's look at some examples of one face and two phase systems and you

1703
01:54:13,650 --> 01:54:18,110
this one of these days where we were using symbols to mean multiple

1704
01:54:18,130 --> 01:54:24,070
meanings of already used p to represent pressure if use p now to represent

1705
01:54:24,080 --> 01:54:26,760
number of phases you won't be able to tell one from the other side i

1706
01:54:26,760 --> 01:54:30,640
feel little bit western today so make it circle piece of circle p

1707
01:54:30,660 --> 01:54:32,610
equals the number

1708
01:54:32,610 --> 01:54:35,380
o phase is not to be confused with p

1709
01:54:35,390 --> 01:54:41,310
which is the pressure so number of phases i designated by circle p so let's

1710
01:54:41,310 --> 01:54:42,240
look at some

1711
01:54:42,410 --> 01:54:44,240
simple single

1712
01:54:44,240 --> 01:54:51,870
systems so let's look at people's one p equals one so how about pure water

1713
01:54:52,890 --> 01:54:56,470
pure water liquid

1714
01:54:56,480 --> 01:54:57,860
i have a beaker of water

1715
01:54:57,870 --> 01:55:05,340
it's a uniform chemical composition physically distinct mechanically separable about about like gold

1716
01:55:05,410 --> 01:55:07,230
white gold

1717
01:55:07,240 --> 01:55:10,210
why gold consists of an alloy of

1718
01:55:10,230 --> 01:55:14,370
certainly gold which we had silver we had no article

1719
01:55:14,610 --> 01:55:19,200
and these are all fcc metals and at the animalistic level you have them substituting

1720
01:55:19,200 --> 01:55:24,770
for one another on the gold lattice and so i have uniform chemical composition i

1721
01:55:24,770 --> 01:55:28,650
have no boundaries that could be grain boundaries but that's different but at the grain

1722
01:55:28,650 --> 01:55:34,840
boundaries do not represent something that is a difference in chemical composition so this is

1723
01:55:34,850 --> 01:55:36,000
this is pure

1724
01:55:36,020 --> 01:55:42,180
and this is a solution itself it's pure material or a solution

1725
01:55:42,190 --> 01:55:44,990
it will qualify as single phase

1726
01:55:45,040 --> 01:55:49,320
OK let's look so here's the liquid here's the solid let's look at a gas

1727
01:55:49,330 --> 01:55:50,450
about air

1728
01:55:50,460 --> 01:55:52,730
air is a solution of

1729
01:55:55,180 --> 01:55:56,270
our goal

1730
01:55:56,330 --> 01:56:00,130
increasingly sio two in the city when you get

1731
01:56:00,190 --> 01:56:03,380
coal-burning for electric power there's s o two

1732
01:56:03,390 --> 01:56:08,620
let's not forget our power nitrous oxide if you live near now call smelter you

1733
01:56:08,640 --> 01:56:14,070
have CF four if you live near magnesium smelter no chlorine but in cambridge mercifully

1734
01:56:14,530 --> 01:56:16,070
we just have to enjoy

1735
01:56:16,130 --> 01:56:20,720
these cells and this is all single phase you don't see

1736
01:56:20,730 --> 01:56:24,960
the gas settling according to density they mix intimately

1737
01:56:25,040 --> 01:56:27,610
in all proportions and one more solid

1738
01:56:27,650 --> 01:56:29,380
calcium zirconium

1739
01:56:29,400 --> 01:56:35,370
we talked about this before this is the line stabilized zirconia for oxygen sensors and

1740
01:56:35,370 --> 01:56:39,160
i'll show you later that this is the material that used for

1741
01:56:39,220 --> 01:56:43,780
the full diamond this is a solid solution calcium

1742
01:56:43,830 --> 01:56:48,720
i'm sitting on zirconium sites this single phase there's no calcium islands

1743
01:56:48,750 --> 01:56:53,590
calcium oxide ions are commonly marks site islands this is a solid

1744
01:56:53,590 --> 01:56:55,530
less than linear

1745
01:56:55,540 --> 01:57:00,890
OK the curvature flatten out OK what that means is that pointing to something that's

1746
01:57:00,900 --> 01:57:06,110
far away disproportionately faster than point to something that's nearby

1747
01:57:06,130 --> 01:57:10,140
because the the finger travels your hand travels

1748
01:57:10,150 --> 01:57:13,200
at a higher average speed the greater the distance

1749
01:57:16,270 --> 01:57:18,280
nothing happens if c

1750
01:57:18,770 --> 01:57:21,000
target is small

1751
01:57:21,080 --> 01:57:24,750
small terriers are disproportionately hard to point to

1752
01:57:24,780 --> 01:57:29,630
this see how the curve rise up as target size of what that means is

1753
01:57:29,630 --> 01:57:34,390
that if it's if the target is tiny it takes disproportionately long to point two

1754
01:57:34,450 --> 01:57:40,890
OK well profits worked this out as verified by a lot of experimental studies and

1755
01:57:41,100 --> 01:57:46,310
it turns out that there's this interesting quantitative formulas

1756
01:57:46,320 --> 01:57:48,270
he but

1757
01:57:48,290 --> 01:57:52,130
just had a battery failure

1758
01:57:52,270 --> 01:57:56,560
interesting quantitative relationship

1759
01:57:56,570 --> 01:58:01,100
people have made very heavy use of a very good chance that you hear mention

1760
01:58:01,100 --> 01:58:04,600
that this conference has

1761
01:58:04,650 --> 01:58:09,890
it's something very fundamental about when you give people

1762
01:58:09,890 --> 01:58:13,190
device to interact with the computer

1763
01:58:13,200 --> 01:58:14,890
especially when do we

1764
01:58:14,900 --> 01:58:19,910
he was the first thing that happened with graphical interface needed graphical pointing device turns

1765
01:58:19,910 --> 01:58:22,880
out the mouse is a good choice and

1766
01:58:22,890 --> 01:58:23,860
this is the

1767
01:58:23,870 --> 01:58:30,130
basic empirical fact that explains why the mouse is a good choice in many circumstances

1768
01:58:30,130 --> 01:58:35,870
but can be a terrible choice in other circumstances OK when the very first macintosh

1769
01:58:35,870 --> 01:58:39,340
appeared it did not have the cursor keys

1770
01:58:39,470 --> 01:58:42,090
OK only the mouse

1771
01:58:42,110 --> 01:58:44,050
right away about

1772
01:58:44,070 --> 01:58:49,380
i was amazed how painful was to get the cursor position between two periods

1773
01:58:49,620 --> 01:58:54,830
well first refusal do that very well because that's where the mouse does not work

1774
01:58:57,960 --> 01:59:02,380
if you want to zip around the screen point something here point something years and

1775
01:59:02,380 --> 01:59:03,770
here's something here

1776
01:59:03,780 --> 01:59:06,540
we're in this regime over here

1777
01:59:06,570 --> 01:59:09,390
i can be very fast and very efficient

1778
01:59:11,170 --> 01:59:12,980
so the mouse follows

1779
01:59:13,030 --> 01:59:17,170
properly designed mouse follows fitts law and that's why it works so well

1780
01:59:17,390 --> 01:59:22,690
very well for long used small targets can be a problem other devices and this

1781
01:59:22,690 --> 01:59:28,050
has been measured the classic card moran and newell book included some of this

1782
01:59:28,070 --> 01:59:32,240
work christakis are linear with the x and y distance so if you have to

1783
01:59:32,240 --> 01:59:36,530
go a long way diagonally across the screen they don't work well at all compared

1784
01:59:36,530 --> 01:59:43,220
to mouse trackball can be good but not better than agood mouse if you have

1785
01:59:43,220 --> 01:59:47,030
to stroke the trackball in different control regime

1786
01:59:47,210 --> 01:59:51,560
potentially much slower than the mouse if you don't have to struggle to be the

1787
01:59:51,560 --> 01:59:57,080
same thing joysticks can be very different because there's a lot of different ways of

1788
01:59:57,080 --> 02:00:02,590
controlling the position and velocity of the mouse depending on what you do the joystick

1789
02:00:03,020 --> 02:00:06,210
there's some control regimes that are very difficult to make

1790
02:00:06,240 --> 02:00:12,330
to use you often see those in computer games OK so

1791
02:00:12,340 --> 02:00:15,530
but is this point here is that he can be faster if you only need

1792
02:00:15,530 --> 02:00:21,530
a few keystrokes an important principle here mention these numbers more later is that this

1793
02:00:21,530 --> 02:00:25,390
is why well designed keyboard interface can still be very good

1794
02:00:25,610 --> 02:00:29,350
you even know people rarely bother with them these days there are

1795
02:00:29,390 --> 02:00:37,400
a certain user domains speed is important for symmetry once told me really like having

1796
02:00:37,400 --> 02:00:39,850
all keyboard interface for that reason

1797
02:00:42,930 --> 02:00:50,020
good idea because pretty by hand is actually relatively slow compared to the typical user

1798
02:00:50,020 --> 02:00:54,870
of the keyboard these days in the and about a quarter of the second right

1799
02:00:54,870 --> 02:01:02,390
hand writing a character can take credit along with an extremely good typists are remarkably

1800
02:01:02,390 --> 02:01:07,040
fast so the lesson to be learned from this immediately is that depend based interface

1801
02:01:07,040 --> 02:01:11,680
can have a lot of advantages but speed is not one of them compared to

1802
02:01:12,560 --> 02:01:21,150
americans i guess english people from england are used to the query keyboard

1803
02:01:21,190 --> 02:01:27,590
there's a lot of legends about the critic keyboard and there's a lot of basic

1804
02:01:27,600 --> 02:01:33,350
it lies and slanders and then made about shows the guy who invented the first

1805
02:01:33,350 --> 02:01:36,840
practical typewriter and accordingly

1806
02:01:36,950 --> 02:01:41,470
if you ask me i will be delighted to tell you about what i'll try

1807
02:01:41,480 --> 02:01:45,230
to write a paper about one of these days but what he was really trying

1808
02:01:45,230 --> 02:01:46,120
to do

1809
02:01:46,240 --> 02:01:50,400
and how that turned out to be the right answer is first typing speed even

1810
02:01:50,400 --> 02:01:54,560
though the time he did it nobody was touch typist

1811
02:01:54,570 --> 02:01:58,770
after i can see more also something that talking the whole resttime about the story

1812
02:01:58,770 --> 02:02:00,320
of you

1813
02:02:00,530 --> 02:02:04,600
what it shows is keyword turned up doing is taking advantage of the fact that

1814
02:02:04,600 --> 02:02:08,790
you can keys on alternating hands faster

1815
02:02:08,800 --> 02:02:12,850
then you can make use of the same name which is how your hands work

