1
00:00:00,000 --> 00:00:01,740
so i'm very low and

2
00:00:01,750 --> 00:00:07,010
from carnegie mellon and this is chris christus was unable to make it and they

3
00:00:07,010 --> 00:00:11,380
will make it today he sent his sincere apologies and i think the slides are

4
00:00:11,380 --> 00:00:13,230
online the

5
00:00:13,240 --> 00:00:19,460
what if you go to the conference website website and click the tutorial tab should

6
00:00:19,480 --> 00:00:22,450
the PBS show should be at the bottom

7
00:00:22,450 --> 00:00:26,540
and they interact with the clarification questions

8
00:00:26,550 --> 00:00:30,250
since we're in tutorial i want to make sure that i don't lose anybody

9
00:00:31,230 --> 00:00:36,120
the tutorials on graph mining techniques for social media analysis and so before actually getting

10
00:00:36,120 --> 00:00:39,370
the real meat of this tutorial i want to talk about what graph mining is

11
00:00:39,380 --> 00:00:42,850
then why we would want to do such a thing

12
00:00:45,730 --> 00:00:50,340
so graph mining is essentially extracting useful useful knowledge for

13
00:00:50,370 --> 00:00:55,770
that is patterns outliers centre from structured data can be represented as a graph meant

14
00:00:55,770 --> 00:01:00,840
for our purposes this is usually associated social network however you can

15
00:01:00,850 --> 00:01:05,830
use different kinds of network besides just keep a social network of people for instance

16
00:01:06,250 --> 00:01:07,360
so here's a

17
00:01:07,420 --> 00:01:09,860
here two examples one to facebook graph which

18
00:01:10,140 --> 00:01:15,470
the government touchgraph website that were no nodes are networks in

19
00:01:16,160 --> 00:01:21,970
and then from for the graph from livejournal violent from lehman in color

20
00:01:21,980 --> 00:01:26,270
where each each node is one then the links are

21
00:01:26,280 --> 00:01:28,690
extracted from all the

22
00:01:28,690 --> 00:01:31,250
the friends list

23
00:01:31,270 --> 00:01:37,390
so a couple examples of graph mining is some unnamed social media host just to

24
00:01:37,390 --> 00:01:42,120
be unbiased tries to look at certain online groups and predict whether some group all

25
00:01:43,140 --> 00:01:47,000
continue or disband within the next few months based on the interaction that they would

26
00:01:47,000 --> 00:01:51,310
get from that group another example is that some phone provider might look at cell

27
00:01:51,310 --> 00:01:55,860
phone call call graphs to try to figure out if there are certain users are

28
00:01:55,860 --> 00:01:58,620
users in in the network that are

29
00:01:58,640 --> 00:02:01,660
that are result of identity theft

30
00:02:01,670 --> 00:02:06,530
just by based on who are calling and when they're calling

31
00:02:06,530 --> 00:02:10,790
so why why would we want to use graph mining

32
00:02:12,560 --> 00:02:17,970
well thanks to the web and social media it's only now that were even able

33
00:02:17,970 --> 00:02:20,470
to get this sort of data on a very large scale

34
00:02:20,470 --> 00:02:24,650
it used to be the case that sociologists would have to go into high schools

35
00:02:24,650 --> 00:02:29,820
and monasteries and survey each person in their to and ask where their friends are

36
00:02:29,940 --> 00:02:32,690
this is a very practical to do and it's

37
00:02:32,720 --> 00:02:36,440
time consuming you have to get IRB approval but now thanks to net thanks to

38
00:02:36,440 --> 00:02:39,940
social media we can just crawl the web get the stuff for free because high-schoolers

39
00:02:39,940 --> 00:02:44,030
just like to tell us who friends are on myspace

40
00:02:44,030 --> 00:02:47,680
so it's also on a much larger scale because we have millions of users not

41
00:02:47,680 --> 00:02:49,940
just hundreds or thousands even

42
00:02:52,010 --> 00:02:59,370
and that we also want to be able to understand relationships as well as content

43
00:02:59,370 --> 00:03:01,030
like we may be able to

44
00:03:01,030 --> 00:03:04,210
graph content such as text and images from the web but we also want to

45
00:03:04,210 --> 00:03:11,530
understand the link structure and what these relationships mean

46
00:03:11,570 --> 00:03:14,810
and large amounts of data will always raise new questions

47
00:03:14,820 --> 00:03:18,340
if you have a big monsters amount of data you need a bit more need

48
00:03:18,370 --> 00:03:21,690
for need for organisation of this data

49
00:03:21,780 --> 00:03:25,440
so some motivating questions and graph mining are

50
00:03:25,480 --> 00:03:30,540
but we want to answer is how how do networks form evolve evolving collapse

51
00:03:30,560 --> 00:03:34,400
what tools can we use to study these networks

52
00:03:34,410 --> 00:03:39,410
who are the most influential central members of the network

53
00:03:39,440 --> 00:03:41,840
how do ideas diffuse

54
00:03:41,850 --> 00:03:46,490
can we extract communities

55
00:03:46,500 --> 00:03:49,630
and what sort of anomaly detection can we perform

56
00:03:49,690 --> 00:03:53,510
and these are questions i will answer in this tutorial and here's how we're going

57
00:03:53,510 --> 00:03:56,990
to fit those and in part one

58
00:03:57,000 --> 00:04:01,250
i'll go over here is the question of how do networks form evolve and collapse

59
00:04:01,470 --> 00:04:06,560
first few for some introductory material and definitions and then some patterns and laws that

60
00:04:06,560 --> 00:04:11,030
have been have been discovered on in real real graphs

61
00:04:11,040 --> 00:04:12,880
part two

62
00:04:12,880 --> 00:04:16,600
are going to go into some detail on some tools that are commonly used to

63
00:04:16,600 --> 00:04:20,500
study these networks and part which which also includes

64
00:04:20,990 --> 00:04:24,450
it's the questioner ranking in how

65
00:04:24,480 --> 00:04:28,670
how we find the most important members of the network

66
00:04:28,690 --> 00:04:29,880
and in part three

67
00:04:30,440 --> 00:04:34,000
after a quick break that's what your five-minute break will be right after part two

68
00:04:34,850 --> 00:04:36,830
we can look forward to that

69
00:04:37,070 --> 00:04:39,300
so far part three will

70
00:04:39,450 --> 00:04:43,000
go into some case studies which we answer some of the some other questions that

71
00:04:43,960 --> 00:04:51,820
that are active research areas such as diffusion a community extraction anomaly detection

72
00:04:52,740 --> 00:04:56,250
part one of first go over six definitions and then some patterns so i'm just

73
00:04:56,250 --> 00:04:59,140
starting from the very very bottom up so it

74
00:04:59,170 --> 00:05:02,900
there be six very basic definitions to look over and then explain some of the

75
00:05:03,990 --> 00:05:08,700
and enrique in real graphs

76
00:05:08,730 --> 00:05:12,620
so in the first definition the network is we can define as the graph a

77
00:05:12,620 --> 00:05:15,250
set of vertices or nodes and the set of edges

78
00:05:15,290 --> 00:05:18,480
and these edges may have numerical weights between them for instance this is a little

79
00:05:18,480 --> 00:05:19,930
toy blog graph

80
00:05:19,940 --> 00:05:23,560
where we have a lot one block two block three block four we have directed

81
00:05:23,560 --> 00:05:28,600
edges going going between the blogs and sometimes the blog belong mailing to itself

82
00:05:28,620 --> 00:05:30,000
it's in case

83
00:05:30,020 --> 00:05:34,550
before right here at least two itself three times and it links to blog two

84
00:05:34,550 --> 00:05:44,460
twice and log one blogs you need link to each other and so on

85
00:05:44,500 --> 00:05:50,170
so we represent represent the graph the graph is using adjacency matrix

86
00:05:50,770 --> 00:05:58,250
for instance if you if one has one link to be two then we would

87
00:05:58,450 --> 00:06:01,670
i find the find the i j

88
00:06:01,690 --> 00:06:08,570
entry in the adjacency matrix from from from the one to be two

89
00:06:08,600 --> 00:06:11,790
and before link to itself three times you would have

90
00:06:11,810 --> 00:06:14,680
have into right here

91
00:06:14,950 --> 00:06:20,680
so on unweighted graphs all the entries are going to be easier one because we

92
00:06:20,680 --> 00:06:24,050
will be using these numerical weight here

93
00:06:24,750 --> 00:06:26,630
in an undirected graph

94
00:06:26,640 --> 00:06:30,880
the matrix to be symmetric because they're being mutual links going either way between all

95
00:06:30,880 --> 00:06:32,120
the graphs

96
00:06:32,130 --> 00:06:33,760
between all the nodes

97
00:06:33,790 --> 00:06:39,500
so next bipartite graph has to two distinct sets of vertices and edges will only

98
00:06:39,500 --> 00:06:40,870
occur between

99
00:06:40,880 --> 00:06:43,200
nodes of the different set

100
00:06:43,200 --> 00:06:54,520
this presentation is delivered by the stanford center for professional development

101
00:06:59,740 --> 00:07:01,260
so welcome back

102
00:07:01,280 --> 00:07:06,070
what i want to do today is that have our discussion on support vector machines

103
00:07:06,130 --> 00:07:10,050
and in particular was the idea of kernels

104
00:07:10,060 --> 00:07:14,860
and then talk about the one norm soft margin SVM is well others

105
00:07:15,000 --> 00:07:18,280
apply it seems even today that is not linearly separable

106
00:07:18,600 --> 00:07:23,770
all about the SMO algorithm on which is an algorithm for solving the optimisation problem

107
00:07:23,770 --> 00:07:26,310
that that that we post last time

108
00:07:28,170 --> 00:07:29,780
so to recap

109
00:07:29,800 --> 00:07:37,660
right we wrote down to the following convex optimisation problems

110
00:07:44,860 --> 00:07:49,140
and all this is assuming that the data is linearly separable which which is assumption

111
00:07:49,140 --> 00:07:50,500
of fixed later

112
00:07:50,520 --> 00:07:52,060
and this

113
00:07:52,070 --> 00:07:54,750
but this is the south african people back in

114
00:07:54,760 --> 00:07:56,590
they was

115
00:07:57,000 --> 00:08:01,930
and so with this optimisation problem given a training set

116
00:08:01,950 --> 00:08:05,580
this will on

117
00:08:06,560 --> 00:08:09,660
yes the after all margin

118
00:08:09,670 --> 00:08:12,890
classifier for the dataset that maximizes

119
00:08:12,910 --> 00:08:17,350
the geometric margin for your training examples

120
00:08:19,730 --> 00:08:23,920
and so the previous lecture we also derive the dual of this problem

121
00:08:23,940 --> 00:08:28,450
which was to maximize the

122
00:08:28,630 --> 00:08:41,620
right of

123
00:08:44,340 --> 00:08:53,540
and this is the dual

124
00:08:55,000 --> 00:09:00,220
as the optimisation problem where here on using the piece of angle brackets denote inner

125
00:09:00,220 --> 00:09:05,770
products of this is just you know it's i transposing exchange

126
00:09:05,780 --> 00:09:07,980
the vectors exciting

127
00:09:08,120 --> 00:09:13,040
we also worked out

128
00:09:13,050 --> 00:09:17,780
the weights w with the given by some of i alpha i y i x

129
00:09:19,820 --> 00:09:24,600
and therefore when you need to make predictions of a classification time

130
00:09:24,620 --> 00:09:27,010
you need to compute

131
00:09:27,080 --> 00:09:31,870
the value of the hypothesis is applied to input that which is actually the second

132
00:09:31,870 --> 00:09:33,450
suppose would be

133
00:09:33,570 --> 00:09:38,390
right which is that threshold function developers plus one minus one

134
00:09:38,510 --> 00:09:43,990
and so this is key of rye rye

135
00:09:44,000 --> 00:09:56,790
so that can also be written in terms of

136
00:09:56,810 --> 00:10:01,390
in the products between input input vectors x

137
00:10:04,200 --> 00:10:07,670
what i want to do is not the idea of kernels which make use of

138
00:10:07,670 --> 00:10:12,300
this property that it turns out we can take on the only dependencies of the

139
00:10:12,300 --> 00:10:17,980
algorithm on you on x is through the inner product and they can write the

140
00:10:17,990 --> 00:10:23,170
entire algorithm without ever explicitly referring to expect that only

141
00:10:23,190 --> 00:10:25,100
and the other thing in the publicity

142
00:10:25,120 --> 00:10:28,270
input your input feature vectors

143
00:10:29,780 --> 00:10:33,390
and the idea behind kernels is the following

144
00:10:33,410 --> 00:10:36,870
let's say that you have some input attributes

145
00:10:36,890 --> 00:10:44,660
of course to say for now zero number maybe this is on the area of

146
00:10:44,660 --> 00:10:45,520
the whole

147
00:10:45,680 --> 00:10:49,650
the you trying to make some particularly whether it be so in the next six

148
00:10:50,810 --> 00:10:53,890
are quite often will take this

149
00:10:53,900 --> 00:10:56,440
feature x and one at

150
00:10:56,590 --> 00:11:02,680
two which is set of features for example we take a step back to these

151
00:11:02,680 --> 00:11:10,340
of full polynomial features and then we actually call this mapping phi so that five

152
00:11:10,340 --> 00:11:17,410
x denote the mapping from your original features to some high dimensional the features

153
00:11:17,420 --> 00:11:21,500
so if you do this and if you want to use the features five x

154
00:11:22,580 --> 00:11:28,280
all you need to do is go back to linear and everywhere you see i

155
00:11:28,410 --> 00:11:31,570
xj right we replace

156
00:11:32,530 --> 00:11:40,080
on the inner product between private five SG

157
00:11:40,130 --> 00:11:44,460
right so this response to running a support vector machine with the features given by

158
00:11:44,460 --> 00:11:49,430
five x rather than what you know your original maybe one dimensional input feature

159
00:11:51,930 --> 00:12:02,140
and this not i want to consider sometimes i fx will be very high dimensional

160
00:12:02,250 --> 00:12:07,500
and in fact sometimes five x seven example five x may contain very high degree

161
00:12:07,500 --> 00:12:09,080
polynomial features

162
00:12:09,090 --> 00:12:13,920
on sometimes five x actually even be an infinite dimensional vector of features

163
00:12:13,930 --> 00:12:17,600
and the question is if i have actually extremely high dimensional

164
00:12:17,800 --> 00:12:20,880
then you can't actually computed these inner products

165
00:12:20,900 --> 00:12:24,160
very efficient seems right because computers you to

166
00:12:24,180 --> 00:12:29,390
he represents an extremely high dimensional feature vector and taking the product in that would

167
00:12:29,390 --> 00:12:31,560
be computationally inefficient

168
00:12:31,570 --> 00:12:39,440
it turns out that in many important special cases we can write down on scholar

169
00:12:39,460 --> 00:12:40,640
kernel function

170
00:12:40,660 --> 00:12:42,180
denoted by k

171
00:12:42,200 --> 00:12:43,730
which will be

172
00:12:47,820 --> 00:12:53,960
we should be in the product between those feature vectors

173
00:12:54,030 --> 00:13:00,490
it turns out that important special cases where computing five axis is computationally very expensive

174
00:13:00,490 --> 00:13:04,920
maybe is impossible there is an infinite dimensional vector and you can't compute infinite dimensional

175
00:13:05,920 --> 00:13:10,240
but the important special cases with five x is very expensive to represent because it's

176
00:13:10,240 --> 00:13:11,480
so high dimensional

177
00:13:11,530 --> 00:13:14,120
but none the less you can actually compute the kernel

178
00:13:14,130 --> 00:13:17,740
between its and exchange can compute the inner product between these two vectors

179
00:13:18,740 --> 00:13:21,410
very inexpensively

180
00:13:21,430 --> 00:13:26,900
and so the idea of the support vector machine is that everywhere in the algorithm

181
00:13:26,900 --> 00:13:29,250
that you see these inner product

182
00:13:29,260 --> 00:13:34,260
we're going to replace it with the kernel function that you can compute efficiently

183
00:13:34,270 --> 00:13:36,580
and that lets you work in

184
00:13:36,590 --> 00:13:40,740
feature space five x even if i effects on

185
00:13:40,790 --> 00:13:42,520
very high dimensional

186
00:13:45,220 --> 00:13:47,850
let me also how that is done on

187
00:13:47,860 --> 00:13:53,050
and actually a little bit later today where she see some concrete examples of five

188
00:13:53,050 --> 00:13:58,030
x and the kernels on analysis think about constructing kernels explicitly

189
00:13:58,290 --> 00:14:02,840
three dispenses phase one example so

190
00:14:02,960 --> 00:14:06,640
let's say you have

191
00:14:06,650 --> 00:14:12,400
on two inputs x and z are normally actually write x i and x j

192
00:14:12,920 --> 00:14:15,960
but it's easy to say the writing

193
00:14:17,490 --> 00:14:25,610
let's say might know is k if x common e equals transpose where

194
00:14:28,180 --> 00:14:29,530
so this is

195
00:14:29,710 --> 00:14:46,170
right this extract fuzzy either to this thing here is xtrans lsi and distinct sections

196
00:14:46,170 --> 00:14:47,420
if you

197
00:14:47,430 --> 00:14:50,810
i want to add k nearest neighbors well what you have to do is

198
00:14:50,860 --> 00:14:52,500
you need to

199
00:14:52,550 --> 00:14:55,620
not just sort the arguments but then just pick

200
00:14:55,670 --> 00:14:58,980
the first k terms that's all

201
00:14:58,990 --> 00:15:01,860
the the code doesn't really change much

202
00:15:01,860 --> 00:15:05,040
and if you have a nearest neighbour regression

203
00:15:05,090 --> 00:15:08,550
the code actually gets easy you just wrote the song here

204
00:15:08,550 --> 00:15:11,680
that's basic nearest neighbors

205
00:15:11,730 --> 00:15:14,250
does it work

206
00:15:16,220 --> 00:15:17,510
is example

207
00:15:17,590 --> 00:15:19,670
on the previous studies

208
00:15:19,680 --> 00:15:21,180
and you can see that

209
00:15:21,180 --> 00:15:24,450
more or less kind of get it right i should tell you that the optimal

210
00:15:24,450 --> 00:15:26,790
separation of it's just a straight line

211
00:15:26,800 --> 00:15:29,430
is that just taking two normal distributions

212
00:15:29,470 --> 00:15:32,500
and so you will get the same variance

213
00:15:32,630 --> 00:15:34,420
the straight line here

214
00:15:35,510 --> 00:15:38,550
it's not quite straight well let's see what we can do

215
00:15:38,560 --> 00:15:40,640
this would be seven nearest neighbors

216
00:15:40,660 --> 00:15:44,920
and the shades of brown and violet seems to depend on

217
00:15:44,970 --> 00:15:47,130
how many of the neighbours are

218
00:15:47,140 --> 00:15:49,840
all right or all blue

219
00:15:49,930 --> 00:15:52,990
so that we all seven we also blue

220
00:15:52,990 --> 00:15:55,420
and these will be somewhere in between

221
00:15:55,430 --> 00:15:58,530
if you look at the decision boundary it's a little bit smarter

222
00:15:58,560 --> 00:16:00,920
so a bit like the coastline

223
00:16:02,890 --> 00:16:07,250
a lot a lot nicer than what we have here

224
00:16:08,490 --> 00:16:11,950
if you if you have more nearest neighbour need more data

225
00:16:12,000 --> 00:16:17,220
and so the convergence properties of the nearest neighbour classifier conferences like world

226
00:16:17,260 --> 00:16:22,810
the neighborhood needs increases my sample size increases but not directly

227
00:16:22,860 --> 00:16:23,910
to regression

228
00:16:23,920 --> 00:16:28,540
so that probably looks like assign right

229
00:16:28,550 --> 00:16:31,070
the one nearest neighbour regression

230
00:16:31,090 --> 00:16:33,130
looks god-awful

231
00:16:36,550 --> 00:16:40,230
well what the seven nearest neighbour regression like

232
00:16:40,280 --> 00:16:42,200
actually preclude right

233
00:16:42,220 --> 00:16:47,350
i mean i not very smooth but overall it actually broke it's quite

234
00:16:47,540 --> 00:16:51,920
this is the other thing i mean you don't necessarily care that much about the

235
00:16:51,920 --> 00:16:56,120
curve being pretty what you care about is it actually working with

236
00:16:56,140 --> 00:17:00,310
probably will work quite well in terms of their

237
00:17:01,490 --> 00:17:02,350
this was

238
00:17:02,360 --> 00:17:04,620
the simplest algorithm you could come up with

239
00:17:04,680 --> 00:17:06,800
it does work fairly well

240
00:17:06,860 --> 00:17:11,250
the nearest neighbour search will find the nearest guy predict the same label

241
00:17:11,300 --> 00:17:15,850
you can do that for regression it can classification it's trivial to implement supplies for

242
00:17:17,350 --> 00:17:19,290
you don't need any training

243
00:17:20,430 --> 00:17:24,610
doesn't work that brilliantly well but if you have any algorithm

244
00:17:24,660 --> 00:17:25,570
you want to

245
00:17:25,600 --> 00:17:29,800
find out what it's good it's probably a good idea to compare against nearest neighbour

246
00:17:29,850 --> 00:17:32,980
just in case it out was might actually do quite well

247
00:17:34,290 --> 00:17:37,010
from his safe

248
00:17:37,420 --> 00:17:40,370
now let's look at another problem

249
00:17:40,380 --> 00:17:42,480
looks quite unrelated initially

250
00:17:42,490 --> 00:17:45,350
estimating probabilities from data

251
00:17:45,430 --> 00:17:46,740
so here the goal is

252
00:17:46,790 --> 00:17:49,430
well we go and rolling dice

253
00:17:49,490 --> 00:17:53,440
and then i count how many times each site comes up

254
00:17:53,490 --> 00:17:56,490
i could just failed empirical probability

255
00:17:57,560 --> 00:18:01,140
according to wall the frequency of occurrence

256
00:18:01,840 --> 00:18:06,290
what if i have a coin toss it and made it a hundred times in

257
00:18:06,320 --> 00:18:10,490
four times he comes up eight six times it comes up tails i could say

258
00:18:10,490 --> 00:18:12,370
maybe the probability of

259
00:18:12,430 --> 00:18:17,800
his forty is point for probability of ten is point six

260
00:18:18,220 --> 00:18:22,190
so this will be actually what's also called the maximum likelihood estimation

261
00:18:22,240 --> 00:18:25,360
name and trying to find the most likely

262
00:18:25,410 --> 00:18:29,970
set of parameters that explain my daughter

263
00:18:33,100 --> 00:18:34,410
this is not this

264
00:18:34,420 --> 00:18:37,160
this is a good idea in every case

265
00:18:37,200 --> 00:18:39,910
made me give an example

266
00:18:39,940 --> 00:18:40,740
let's say

267
00:18:40,740 --> 00:18:45,550
a student comes to school and he said well i didn't do my homework

268
00:18:45,590 --> 00:18:47,600
the tests why

269
00:18:47,630 --> 00:18:48,860
students as well

270
00:18:48,860 --> 00:18:51,120
the dog the homework

271
00:18:51,120 --> 00:18:57,050
this is perfectly valid in the maximum likelihood seems to explain the outcome namely student

272
00:18:57,050 --> 00:18:58,720
doesn't have homework

273
00:18:58,740 --> 00:19:01,540
it could really happen that the dog at home

274
00:19:01,740 --> 00:19:07,620
but we know that dogs usually tend not to eat homework and maybe the more

275
00:19:07,680 --> 00:19:12,010
plausible explanation was well the student was elated by

276
00:19:13,620 --> 00:19:16,210
teacher will reject that hypothesis

277
00:19:16,230 --> 00:19:17,420
even so

278
00:19:17,440 --> 00:19:19,240
in the maximum likelihood scenes

279
00:19:19,320 --> 00:19:23,960
student being a lazy farmer the homework are two equally

280
00:19:24,110 --> 00:19:26,270
plausible scenarios

281
00:19:26,330 --> 00:19:29,700
so what if the teacher do is we use the priors

282
00:19:29,780 --> 00:19:34,080
because you know he's been probably teacher for a while so he knows what reason

283
00:19:34,260 --> 00:19:36,710
what are unreasonable explanations

284
00:19:36,780 --> 00:19:39,690
they basically use the posterior

285
00:19:39,700 --> 00:19:44,400
we had a prior on possible explanations how reasonable they are

286
00:19:44,410 --> 00:19:48,020
and then use that to perform inference

287
00:19:48,040 --> 00:19:50,620
if we just count the number of occurrences here

288
00:19:50,650 --> 00:19:52,340
you have number of trials

289
00:19:52,350 --> 00:19:55,480
which would be the most likely explanation of

290
00:19:57,090 --> 00:20:00,690
it would be falling into exactly the same trap of the homework

291
00:20:00,710 --> 00:20:04,770
i'll see afterwards how to fix it

292
00:20:04,830 --> 00:20:07,620
now let me show you what is a bad idea

293
00:20:07,750 --> 00:20:09,990
thank you for taking

294
00:20:10,040 --> 00:20:12,480
just a simple random number generator

295
00:20:12,490 --> 00:20:14,350
which was equal probabilities

296
00:20:14,360 --> 00:20:15,790
without it

297
00:20:15,830 --> 00:20:17,420
one and

298
00:20:17,490 --> 00:20:18,250
i've just

299
00:20:18,270 --> 00:20:20,030
both the accounts

300
00:20:20,080 --> 00:20:21,770
so that would be i think

301
00:20:21,780 --> 00:20:24,150
twelve observations twenty four

302
00:20:24,170 --> 00:20:27,190
sixty and one hundred twenty

303
00:20:27,240 --> 00:20:30,030
what you should be seeing a straight line

304
00:20:30,140 --> 00:20:34,350
what you are seeing is anything but the straight line see it appeared to here

305
00:20:34,440 --> 00:20:36,620
should be for their

306
00:20:37,460 --> 00:20:38,860
the team here

307
00:20:38,870 --> 00:20:41,080
and so on

308
00:20:41,790 --> 00:20:46,940
what's happening is that my empirical observations are not really that close to the true

309
00:20:49,410 --> 00:20:55,070
if i were just doing a maximum likelihood estimate i would say well i this

310
00:20:55,070 --> 00:20:58,080
is probably occurring with something like

311
00:20:59,070 --> 00:21:03,850
you know almost thirty percent probability is this is around fifteen

312
00:21:03,890 --> 00:21:08,230
whereas in reality although frequent occurrence of the machine

313
00:21:12,350 --> 00:21:14,990
before i tell you how to fix it let me first tell you if you

314
00:21:14,990 --> 00:21:17,270
could properly be

315
00:21:17,350 --> 00:21:19,620
there's something called have things by

316
00:21:19,690 --> 00:21:21,100
and this is the following

317
00:21:21,110 --> 00:21:22,770
but the probability

318
00:21:22,780 --> 00:21:25,740
that's an empirical estimate

319
00:21:25,760 --> 00:21:30,020
deviates from the true probability by more than epsilon

320
00:21:30,090 --> 00:21:31,650
is less than two

321
00:21:31,660 --> 00:21:33,240
e to the minus

322
00:21:33,240 --> 00:21:36,410
two in its long square

323
00:21:36,470 --> 00:21:40,740
in the summer observations it's like the confidence

324
00:21:40,750 --> 00:21:44,200
so in other words i have exponentially fast convergence

325
00:21:44,270 --> 00:21:46,030
to the answer

326
00:21:46,090 --> 00:21:48,510
for any given epsilon

327
00:21:48,570 --> 00:21:50,140
that's the good news

328
00:21:50,150 --> 00:21:51,730
the bad news is

329
00:21:51,760 --> 00:21:52,750
if i ask

330
00:21:53,480 --> 00:21:55,520
if i keep this term here fixed

331
00:21:55,530 --> 00:21:57,730
how does it scale within

332
00:21:59,620 --> 00:22:02,820
that's what you eat some scales like

333
00:22:02,830 --> 00:22:06,860
one of the square of number of observations

334
00:22:06,870 --> 00:22:09,120
this is really bad right

335
00:22:09,120 --> 00:22:10,200
was in order to

336
00:22:10,210 --> 00:22:14,200
get the ten fold reduction in my confidence improvement in my confidence

337
00:22:14,380 --> 00:22:15,990
i need

338
00:22:16,030 --> 00:22:19,600
well a hundred times as many observations

339
00:22:19,640 --> 00:22:23,360
this is really

340
00:22:23,410 --> 00:22:25,700
the only good thing about this is that

341
00:22:25,740 --> 00:22:29,010
this volunteer doesn't hold only for a single probability here

342
00:22:29,250 --> 00:22:32,810
holds uniformly over all of them

343
00:22:32,810 --> 00:22:36,770
so this is converging to be according the fixed point of t

344
00:22:37,930 --> 00:22:41,320
so this the about fixed point theorem for the contraction

345
00:22:41,390 --> 00:22:43,470
you're blind operator

346
00:22:43,480 --> 00:22:46,530
people applying in its convergence to the fixed

347
00:22:47,500 --> 00:22:54,390
since we have these inequities and should have the same inequality for fixed points

348
00:22:54,430 --> 00:22:57,330
continuity of complying lectures

349
00:22:58,210 --> 00:23:01,310
then i can take the maximum suspected p

350
00:23:01,310 --> 00:23:02,160
four pi

351
00:23:02,980 --> 00:23:07,980
so i think the maximum suspect apply so this holds in particular any state prize

352
00:23:07,980 --> 00:23:11,650
so we comparing these vectors is all compliance y so

353
00:23:11,690 --> 00:23:13,330
i can write

354
00:23:13,390 --> 00:23:18,320
this for an estate

355
00:23:19,410 --> 00:23:22,730
and then i can take

356
00:23:22,760 --> 00:23:24,480
these animated prove

357
00:23:24,500 --> 00:23:29,290
the maximum possible sites

358
00:23:29,300 --> 00:23:32,000
maxine you suspect pi

359
00:23:32,020 --> 00:23:33,610
what i had

360
00:23:33,640 --> 00:23:35,190
if i get here

361
00:23:35,210 --> 00:23:39,030
the optimal value function that

362
00:23:39,930 --> 00:23:42,960
so i got the optimal value function

363
00:23:43,160 --> 00:23:45,880
it cannot be larger than than the fixed point

364
00:23:47,850 --> 00:23:52,920
so that's so good so far that's hot off what we require because we want

365
00:23:52,920 --> 00:23:56,390
to conclude that i actually if

366
00:23:56,430 --> 00:24:02,910
so in order to see if the article that's become policy that greatest respectively

367
00:24:02,920 --> 00:24:04,670
so no

368
00:24:04,710 --> 00:24:06,040
maybe you should

369
00:24:06,090 --> 00:24:12,210
raise some alliance because maybe such a policy doesn't actually

370
00:24:12,220 --> 00:24:14,390
so i am assuming here

371
00:24:14,410 --> 00:24:16,900
maybe not finite MDP

372
00:24:16,910 --> 00:24:21,550
and so greedy policy we suspect two any value function always exists what do i

373
00:24:21,550 --> 00:24:22,640
mean by that

374
00:24:23,280 --> 00:24:24,280
you know

375
00:24:24,290 --> 00:24:27,810
these maxim is taken somewhere

376
00:24:27,850 --> 00:24:31,470
if if you if you have infinite action spaces

377
00:24:31,530 --> 00:24:37,230
this is not maybe you would need something like compact mass continuity about

378
00:24:37,310 --> 00:24:38,700
but if you

379
00:24:38,760 --> 00:24:45,390
have a nice small MDP everything's finite the maximum is always taken from there

380
00:24:46,000 --> 00:24:47,520
and if if it

381
00:24:47,550 --> 00:24:52,550
state access to maximize taken that action in one then i call

382
00:24:52,560 --> 00:24:53,670
i just

383
00:24:53,680 --> 00:24:54,610
call that

384
00:24:56,900 --> 00:24:58,380
so that be my policy

385
00:24:59,030 --> 00:25:00,220
the player

386
00:25:00,230 --> 00:25:04,580
the greedy policy always exists for suspected leaders of creation

387
00:25:04,640 --> 00:25:10,510
and so if you have that so definition for greedy policies that people buying

388
00:25:10,550 --> 00:25:13,090
applied to the

389
00:25:13,160 --> 00:25:17,540
is the same as he applied to be so we're taking the policy with respect

390
00:25:17,540 --> 00:25:19,520
to the fixed point

391
00:25:19,550 --> 00:25:22,110
but then you can keep applying t of

392
00:25:22,130 --> 00:25:25,540
both sides t to both sides

393
00:25:25,550 --> 00:25:28,890
well in part because here of the is the right so

394
00:25:28,930 --> 00:25:30,690
if you black p

395
00:25:30,730 --> 00:25:36,290
as many times as you want you always get the me is the fixed point

396
00:25:36,330 --> 00:25:39,550
you can apply to your reply

397
00:25:39,570 --> 00:25:40,550
and the

398
00:25:40,570 --> 00:25:41,770
on this side

399
00:25:42,890 --> 00:25:45,430
but then what do you get so this is the

400
00:25:45,440 --> 00:25:47,680
so we apply to game

401
00:25:47,680 --> 00:25:50,720
you get you keep eyes that the

402
00:25:50,730 --> 00:25:51,920
you get keep i

403
00:25:53,280 --> 00:25:54,580
but the five we

404
00:25:54,600 --> 00:25:58,780
what is that it's TV which is b

405
00:25:59,390 --> 00:26:02,810
so no matter how many times you applying here by

406
00:26:02,810 --> 00:26:03,810
two we

407
00:26:03,810 --> 00:26:06,460
you get we

408
00:26:06,470 --> 00:26:09,170
and we know that upon

409
00:26:09,360 --> 00:26:11,630
if you apply to to your

410
00:26:11,650 --> 00:26:14,530
indefinitely to some unusual factor

411
00:26:14,530 --> 00:26:16,120
then there is that the

412
00:26:16,140 --> 00:26:18,210
the process is going to come back

413
00:26:18,210 --> 00:26:21,310
to the value function and the output of policy pi

414
00:26:21,360 --> 00:26:22,510
so this

415
00:26:22,510 --> 00:26:24,590
this site here

416
00:26:24,650 --> 00:26:28,100
the parts are going to come actually apply

417
00:26:28,310 --> 00:26:31,010
so we have to buy it was we

418
00:26:31,020 --> 00:26:34,770
so we have the following the policy

419
00:26:34,800 --> 00:26:37,420
was value function equals the

420
00:26:37,470 --> 00:26:39,340
isn't that great

421
00:26:39,360 --> 00:26:41,200
yes it's great

422
00:26:42,360 --> 00:26:45,440
because we already concluded that these stars

423
00:26:45,450 --> 00:26:47,890
cannot be larger than than we

424
00:26:47,950 --> 00:26:50,820
but right now we have to follow the policy

425
00:26:50,880 --> 00:26:53,510
so that we BFI was the

426
00:26:53,550 --> 00:26:56,300
so we start has to be equal to

427
00:26:57,970 --> 00:27:01,560
because we just demonstrated the distance of the policy

428
00:27:01,650 --> 00:27:03,820
this value function is v

429
00:27:03,840 --> 00:27:08,860
so it cannot be the optimal value function is really smaller than b

430
00:27:08,870 --> 00:27:11,370
because of the existence of this policy

431
00:27:12,300 --> 00:27:14,050
so then we can conclude

432
00:27:14,060 --> 00:27:17,190
from this anecdote is that we equispaced

433
00:27:17,210 --> 00:27:19,480
that finishes approval so

434
00:27:19,490 --> 00:27:21,040
the fixed point

435
00:27:21,100 --> 00:27:26,780
the panel operator is the optimal value function OK and we get something more

436
00:27:26,790 --> 00:27:28,500
from this proof

437
00:27:28,540 --> 00:27:33,650
we got that the greedy policy we suspect these five or the fixed point of

438
00:27:34,730 --> 00:27:36,700
is an optimal policy

439
00:27:38,440 --> 00:27:39,840
because one

440
00:27:39,860 --> 00:27:43,050
because the value function of this is just the

441
00:27:43,050 --> 00:27:44,610
which is the on

442
00:27:44,700 --> 00:27:46,580
the optimal

443
00:27:49,550 --> 00:27:53,070
got it

444
00:27:53,100 --> 00:27:58,720
ask questions

445
00:28:02,400 --> 00:28:08,690
greedy policy with respect to

446
00:28:08,690 --> 00:28:11,850
we start with a fixed point of t

447
00:28:11,910 --> 00:28:14,630
and it's going to be up to that that policy is going to be able

448
00:28:16,570 --> 00:28:23,600
if you take a greedy policy suspected any value function that it could be rubbish

449
00:28:23,650 --> 00:28:26,390
so what does this mean so this means

450
00:28:26,440 --> 00:28:31,900
that it hurts four trying to compute the optimal value function because once you have

451
00:28:31,900 --> 00:28:35,320
all the optimal value function

452
00:28:35,340 --> 00:28:38,270
computing an optimal policy

453
00:28:38,330 --> 00:28:40,250
balance into their problem

454
00:28:40,310 --> 00:28:45,900
just have to solve this maximization problems you plug in the function here

455
00:28:46,850 --> 00:28:47,790
well OK

456
00:28:47,790 --> 00:28:52,620
you have this sound could be over very in a large number of states and

457
00:28:52,630 --> 00:28:57,090
that could be daunting process as it can be computationally difficult to do that

458
00:28:57,150 --> 00:29:00,360
but at least this is one way off for

459
00:29:00,370 --> 00:29:03,360
discovering optimal policies right

460
00:29:03,490 --> 00:29:06,940
so this is a very

461
00:29:06,990 --> 00:29:09,390
crucial fundamental activation

462
00:29:09,420 --> 00:29:12,490
of dynamic programming and many of them at once

463
00:29:12,500 --> 00:29:14,170
the VOC

464
00:29:14,190 --> 00:29:17,190
and hopefully right now you see why

465
00:29:17,200 --> 00:29:25,720
we had that slide that was all those abstract constructions was lift stuff

466
00:29:26,500 --> 00:29:28,940
so one the first

467
00:29:28,950 --> 00:29:31,060
are given that counts the mind

468
00:29:31,120 --> 00:29:35,060
so we know that these studies the fixed point contraction

469
00:29:35,060 --> 00:29:37,870
in april one

470
00:29:46,260 --> 00:29:49,370
you know i

471
00:29:53,800 --> 00:29:59,340
it's just a with to

472
00:30:01,280 --> 00:30:04,990
what are your

473
00:30:06,820 --> 00:30:09,280
more than thirty

474
00:30:16,070 --> 00:30:22,770
then of course this is not just

475
00:30:22,780 --> 00:30:28,090
so the single neuron

476
00:30:29,230 --> 00:30:32,540
this is my one

477
00:30:33,350 --> 00:30:36,090
one of the

478
00:30:41,590 --> 00:30:47,980
need to be wrong

479
00:30:47,990 --> 00:30:49,110
he made

480
00:30:49,130 --> 00:30:51,220
all right

481
00:30:54,810 --> 00:30:56,890
he may

482
00:31:07,910 --> 00:31:11,670
you like

483
00:31:19,420 --> 00:31:20,250
it is

484
00:31:20,260 --> 00:31:21,860
you get the ball

485
00:31:27,240 --> 00:31:29,380
and the right

486
00:31:31,420 --> 00:31:34,400
you can see what she

487
00:31:38,120 --> 00:31:43,560
due the

488
00:31:50,390 --> 00:31:53,740
now we all

489
00:31:57,700 --> 00:31:59,300
for each

490
00:32:04,510 --> 00:32:06,400
you know

491
00:32:06,420 --> 00:32:11,400
and i believe it should

492
00:32:18,670 --> 00:32:20,450
you need

493
00:32:23,410 --> 00:32:31,330
just because

494
00:32:42,380 --> 00:32:44,170
it was

495
00:33:05,950 --> 00:33:07,800
one of the

496
00:33:07,810 --> 00:33:12,770
so all

497
00:33:17,060 --> 00:33:25,610
you know it just

498
00:33:26,680 --> 00:33:27,610
he is

499
00:33:31,060 --> 00:33:36,500
can you give them

500
00:33:36,500 --> 00:33:41,350
it called if you

501
00:33:52,670 --> 00:33:55,000
it is

502
00:34:11,520 --> 00:34:18,390
what you need to be some

503
00:34:18,420 --> 00:34:25,000
the it is to see one

504
00:34:25,070 --> 00:34:30,520
some these people who

505
00:34:30,620 --> 00:34:37,310
i never had some

506
00:34:37,310 --> 00:34:38,950
all right

507
00:34:39,000 --> 00:34:42,630
in the right direction

508
00:34:42,630 --> 00:34:45,650
then the fixed point iteration based

509
00:34:50,480 --> 00:34:51,230
what does

510
00:34:51,250 --> 00:34:55,880
the impact of using this back trick that was to trick of computing this kronecker

511
00:34:55,880 --> 00:35:00,190
product in off into the three rather than off into the four

512
00:35:00,210 --> 00:35:02,440
on the run time

513
00:35:02,440 --> 00:35:03,840
as you can see here

514
00:35:03,840 --> 00:35:06,020
roughly one order of magnitude

515
00:35:06,040 --> 00:35:07,400
is gained by

516
00:35:07,420 --> 00:35:13,020
using this vector compared to the original way of directly computing

517
00:35:13,070 --> 00:35:14,290
the random walk

518
00:35:17,590 --> 00:35:22,500
so this was a technique by which monotone it out to speed up traffic computation

519
00:35:22,500 --> 00:35:25,190
but of course they did not deal

520
00:35:25,210 --> 00:35:27,270
if this problem of torturing

521
00:35:27,320 --> 00:35:28,320
which is

522
00:35:28,380 --> 00:35:30,520
depicted here

523
00:35:30,520 --> 00:35:33,880
as i said before walks allow for repetitions of notes

524
00:35:33,900 --> 00:35:39,250
what can visit the same cycle of notes all over again

525
00:35:39,250 --> 00:35:44,500
a kernel measure similarity between two graphs in terms of common walks

526
00:35:44,520 --> 00:35:45,840
hence small

527
00:35:45,840 --> 00:35:48,770
structural similarity can cause a huge

528
00:35:48,770 --> 00:35:50,360
kind of value

529
00:35:50,420 --> 00:35:51,920
as i said

530
00:35:51,960 --> 00:35:55,300
if you graph is undirected and it contains

531
00:35:55,320 --> 00:35:59,020
and if both graphs undirected and they contain

532
00:35:59,040 --> 00:36:02,250
two nodes a and b which are connected by an edge

533
00:36:02,270 --> 00:36:07,230
then you walk could go back and forth between these two notes all the time

534
00:36:07,270 --> 00:36:08,960
and that would be in

535
00:36:09,020 --> 00:36:11,040
a good example for torturing

536
00:36:11,060 --> 00:36:12,690
and it would completely

537
00:36:12,710 --> 00:36:15,670
mess up your draft value

538
00:36:15,690 --> 00:36:21,230
because it would lead to a huge controversy although it is only a tiny structure

539
00:36:21,270 --> 00:36:23,320
relationship between these two

540
00:36:28,020 --> 00:36:29,250
my hand i

541
00:36:29,270 --> 00:36:30,900
tackle this problem

542
00:36:30,900 --> 00:36:32,500
by transforming

543
00:36:32,500 --> 00:36:35,400
two input graphs and they transformed

544
00:36:35,460 --> 00:36:36,920
you know way

545
00:36:36,980 --> 00:36:39,060
that's four bits

546
00:36:39,110 --> 00:36:43,110
tottering between two nodes

547
00:36:43,130 --> 00:36:47,290
they increase the size of the original graph so they create

548
00:36:48,590 --> 00:36:52,480
conditions from graph not only for each node from the original graph in also for

549
00:36:52,480 --> 00:36:54,610
each edge

550
00:36:54,610 --> 00:36:55,360
from the

551
00:36:55,380 --> 00:36:56,770
the original graph so

552
00:36:58,460 --> 00:37:00,980
is increased tremendously

553
00:37:01,820 --> 00:37:06,960
then they create edges in this new transformed graph

554
00:37:07,000 --> 00:37:09,750
two connect nodes

555
00:37:09,790 --> 00:37:11,110
that represented

556
00:37:11,110 --> 00:37:13,770
nodes in the original graph and

557
00:37:13,790 --> 00:37:15,900
knows that represented edges

558
00:37:15,900 --> 00:37:19,530
in the original graph to each other and they do this in a way that

559
00:37:20,340 --> 00:37:21,840
makes it impossible

560
00:37:23,300 --> 00:37:28,650
back and forth between the same two nodes in two consecutive steps

561
00:37:28,690 --> 00:37:31,840
so you cannot go back to the same node that you

562
00:37:31,860 --> 00:37:33,130
it started from

563
00:37:34,340 --> 00:37:36,650
in two in two steps

564
00:37:36,690 --> 00:37:41,040
if you do it in three steps it's possible

565
00:37:41,090 --> 00:37:42,820
and that's actually one of them

566
00:37:42,880 --> 00:37:45,770
limitations of this approach

567
00:37:45,820 --> 00:37:49,400
namely this approach for bits

568
00:37:49,440 --> 00:37:56,800
tottering between two nodes but not tottering along any larger cycles within your two graphs

569
00:37:56,820 --> 00:37:59,440
and that might be the reason why my here

570
00:37:59,480 --> 00:38:06,380
could not really present experimental evidence showing uniform improvement of the classification accuracy based on

571
00:38:06,380 --> 00:38:09,880
these graph constant forbids torturing

572
00:38:09,900 --> 00:38:11,630
also and this is the

573
00:38:11,650 --> 00:38:14,170
move around ten problem of course if you

574
00:38:14,190 --> 00:38:16,940
create a node for each edge

575
00:38:16,960 --> 00:38:18,320
in your life

576
00:38:18,360 --> 00:38:22,250
original graphs then you increase your graphs arise from all the

577
00:38:22,270 --> 00:38:24,900
o of n two order of n squared

578
00:38:24,960 --> 00:38:26,980
and this is an address

579
00:38:27,040 --> 00:38:34,300
effect on kind of computation time

580
00:38:34,360 --> 00:38:36,210
my et observed that

581
00:38:36,210 --> 00:38:40,090
and therefore they came up with the strategy to speed up

582
00:38:40,090 --> 00:38:41,920
graph come computation

583
00:38:41,940 --> 00:38:43,070
and they did so

584
00:38:43,090 --> 00:38:45,440
by adding additional labels

585
00:38:45,460 --> 00:38:47,270
two the graphs

586
00:38:47,290 --> 00:38:50,920
they observe that the larger your product graph

587
00:38:50,980 --> 00:38:53,290
the slower year-round

588
00:38:53,340 --> 00:38:54,540
i should mention

589
00:38:54,590 --> 00:38:57,270
this year in

590
00:38:57,320 --> 00:38:59,000
the case where you have

591
00:38:59,020 --> 00:39:01,840
graphs of node labels

592
00:39:01,900 --> 00:39:03,540
your product graph

593
00:39:03,590 --> 00:39:06,070
only contains pairs of nodes

594
00:39:06,090 --> 00:39:08,730
that have identical labels

595
00:39:08,770 --> 00:39:10,020
i didn't

596
00:39:10,020 --> 00:39:11,400
describe that

597
00:39:11,440 --> 00:39:15,210
in the beginning because they're only the topology but if you have no neighbours in

598
00:39:15,210 --> 00:39:16,750
your graphs then

599
00:39:16,750 --> 00:39:20,730
your product graph only contest contains pairs of nodes

600
00:39:20,790 --> 00:39:24,380
this is identical node labels

601
00:39:24,400 --> 00:39:28,770
and of course if you're number of node labels of different node labels in your

602
00:39:28,770 --> 00:39:30,670
graphs is larger

603
00:39:31,560 --> 00:39:33,190
this will have effect

604
00:39:33,210 --> 00:39:34,710
that the product graph

605
00:39:34,820 --> 00:39:37,190
then be smaller

606
00:39:37,230 --> 00:39:41,670
and so the trick that my head i use is that they introduce new artificial

607
00:39:41,690 --> 00:39:42,900
node labels

608
00:39:42,920 --> 00:39:47,520
to reduce the size of the product graph

609
00:39:48,650 --> 00:39:53,290
they came up with the idea to use logical distributors of notes

610
00:39:53,290 --> 00:39:56,730
its natural extra labels of these nodes

611
00:39:56,750 --> 00:39:59,750
so to add an extra label to each node

612
00:39:59,790 --> 00:40:03,710
for instance counting the number of neighbors of the node

613
00:40:03,710 --> 00:40:06,710
i'm adding this additional labels

614
00:40:06,770 --> 00:40:09,610
they then reduced the size of the product graph

615
00:40:09,690 --> 00:40:13,190
which then has an effect on the run time that is necessary to commute across

616
00:40:15,630 --> 00:40:19,320
and this idea just described of counting

617
00:40:19,400 --> 00:40:23,190
the neighbours of the node in the graph is the so-called morgan index which is

618
00:40:23,190 --> 00:40:25,340
more than forty years of

619
00:40:25,340 --> 00:40:28,500
and it's based on the idea of counting the neighbours

620
00:40:28,540 --> 00:40:30,340
of nodes in the graph

621
00:40:30,360 --> 00:40:32,570
and doing this recursively

622
00:40:32,590 --> 00:40:35,690
it's you higher order morgan indices

623
00:40:35,690 --> 00:40:39,040
for your footnotes in your graph and this

624
00:40:39,110 --> 00:40:40,710
technique actually had

625
00:40:40,710 --> 00:40:42,320
a significant impact on

626
00:40:42,340 --> 00:40:48,940
and kind of computation around

627
00:40:48,980 --> 00:40:50,480
but still

628
00:40:50,500 --> 00:40:52,730
nine of his father i nor

629
00:40:52,770 --> 00:40:54,900
my here at our

630
00:40:54,920 --> 00:40:56,980
could completely remove this

631
00:40:57,000 --> 00:41:00,750
the problem of tottering even in my head as approach we still have the problem

632
00:41:00,750 --> 00:41:03,110
of tottering in larger size

633
00:41:07,190 --> 00:41:08,710
and natural

634
00:41:08,710 --> 00:41:10,210
extension or

635
00:41:10,210 --> 00:41:13,630
that way of dealing with this problem was to study

636
00:41:15,170 --> 00:41:17,650
looking at other substructures them

637
00:41:17,690 --> 00:41:23,190
walks could remove this problem of torturing and obviously if you do not allow for

638
00:41:23,190 --> 00:41:25,060
repetitions of nodes

639
00:41:25,070 --> 00:41:26,540
in your walks

640
00:41:26,630 --> 00:41:30,130
then you cannot suffer from torturing but then

641
00:41:30,150 --> 00:41:33,570
you're not commuting walks anymore your computing power

642
00:41:33,570 --> 00:41:35,190
in your two graphs

643
00:41:39,960 --> 00:41:41,880
we study the better

644
00:41:41,920 --> 00:41:46,300
it would be feasible to compare all have into graphs

645
00:41:46,320 --> 00:41:48,020
but of course

646
00:41:48,070 --> 00:41:49,540
it's well known that all

647
00:41:49,560 --> 00:41:54,800
to compute all pass into india in across is an NP hard problem

648
00:41:54,860 --> 00:41:57,980
the same holds for along path in your breath

649
00:41:57,980 --> 00:42:01,970
average value that you would get of the random variable in the limit that your

650
00:42:01,970 --> 00:42:04,420
sample size would be infinite

651
00:42:04,430 --> 00:42:05,660
that's the

652
00:42:05,670 --> 00:42:10,100
essentially what the expectation value is so if you have a finite sample with only

653
00:42:10,100 --> 00:42:16,450
n observations and somehow the obvious estimator to use is simply the arithmetic average

654
00:42:17,400 --> 00:42:21,110
the values of the random variable that you are observed

655
00:42:21,120 --> 00:42:24,060
right so i could write a new hat

656
00:42:24,430 --> 00:42:26,720
and that would be the the function of the data

657
00:42:26,800 --> 00:42:29,160
that i would use for that estimator

658
00:42:29,180 --> 00:42:33,390
that's a very important estimated that comes up all the time obviously sufficiently important to

659
00:42:33,390 --> 00:42:34,370
have its own

660
00:42:34,400 --> 00:42:38,040
notation sometimes we write x bar

661
00:42:38,180 --> 00:42:41,920
the bar looks like a something like a hat and it is sometimes called the

662
00:42:41,920 --> 00:42:45,700
sample mean which is a little bit confusing because the mean

663
00:42:45,720 --> 00:42:50,050
it is not an estimator of the mean is is is the parameter itself constant

664
00:42:50,150 --> 00:42:51,850
whereas the sample mean

665
00:42:51,870 --> 00:42:54,770
it is is an estimator for the mean

666
00:42:54,810 --> 00:42:59,160
OK so the two important parameters that we want to consider for every estimator for

667
00:42:59,170 --> 00:43:03,660
the bias and variance and in this particular case it happens that the bias is

668
00:43:03,660 --> 00:43:08,890
identically equal to zero so that that's nice and that's not always true for many

669
00:43:08,890 --> 00:43:11,280
estimators but in this case it is

670
00:43:11,310 --> 00:43:15,920
you can also look at the variance of new hat that's not too difficult to

671
00:43:15,920 --> 00:43:19,530
do i go through the calculation here but what you find is that it is

672
00:43:19,530 --> 00:43:23,330
equal to the variance of x the original random variable

673
00:43:23,360 --> 00:43:24,440
divided by

674
00:43:24,500 --> 00:43:26,660
the number of

675
00:43:26,660 --> 00:43:29,240
observations of x in your data sample

676
00:43:29,870 --> 00:43:34,510
so that leads to the well known result that the standard deviation of the estimator

677
00:43:34,510 --> 00:43:39,540
of the mean is equal to the standard deviation of the original random variable x

678
00:43:39,610 --> 00:43:42,710
divided by the square root of the number of measurements

679
00:43:42,760 --> 00:43:46,660
so you're probably familiar with that

680
00:43:46,680 --> 00:43:48,120
let's just consider one more

681
00:43:48,210 --> 00:43:51,290
the sort that is that the variance

682
00:43:52,840 --> 00:43:55,310
if that's the parameters that i'm interested in

683
00:43:55,320 --> 00:43:56,530
recall that the

684
00:43:56,550 --> 00:44:00,660
the meaning of that parameter one the expectation value of

685
00:44:00,680 --> 00:44:06,780
x minus mu square so it is the expectation value of the variable itself minus

686
00:44:06,780 --> 00:44:10,190
its own expectation value that quantity square

687
00:44:11,090 --> 00:44:16,060
clearly if you only have a finite data sample you can try to approximate by

688
00:44:16,060 --> 00:44:20,660
taking something like the average value of x minus its

689
00:44:20,660 --> 00:44:25,410
estimated expectation value square so that somehow seems like in natural

690
00:44:25,430 --> 00:44:28,130
a reasonably natural formula to write down

691
00:44:28,140 --> 00:44:30,410
to estimate the variance

692
00:44:30,430 --> 00:44:32,440
you might wonder

693
00:44:33,140 --> 00:44:37,400
why not just one over and y one over n minus one

694
00:44:37,410 --> 00:44:41,370
the story there is the following is that if you

695
00:44:41,420 --> 00:44:48,130
were to compute the variance sorry the bias of the estimator sigma had squared defined

696
00:44:48,130 --> 00:44:52,160
with the one over n minus one you find the bias is zero that is

697
00:44:52,160 --> 00:44:55,800
to say that the expectation value of that expression

698
00:44:55,820 --> 00:44:57,740
it is in fact equal to the

699
00:44:57,750 --> 00:44:59,200
the variance of

700
00:44:59,810 --> 00:45:03,770
if you need to consider any other factor here one over and one over n

701
00:45:03,780 --> 00:45:08,990
plus seventeen or some other factor in front then you would have had to have

702
00:45:09,010 --> 00:45:10,780
a bias

703
00:45:10,790 --> 00:45:12,990
i sometimes statistics books that

704
00:45:13,000 --> 00:45:17,000
that factor of one over n minus one is is explained in a slightly strange

705
00:45:17,670 --> 00:45:21,590
something about something about losing a degree of freedom when you when you put in

706
00:45:21,590 --> 00:45:22,450
the sample

707
00:45:22,480 --> 00:45:26,930
i mean there but really the story revolves around whether or not this estimator has

708
00:45:26,930 --> 00:45:28,700
a bias

709
00:45:29,430 --> 00:45:31,240
the bias itself

710
00:45:31,260 --> 00:45:35,260
this is zero you can work out then the variance of this estimator and it's

711
00:45:35,260 --> 00:45:36,840
this one

712
00:45:39,020 --> 00:45:43,990
but we sent our little note to nature pointing it out in input with a

713
00:45:43,990 --> 00:45:47,920
verbal argument and they just back so we had to do with simulation model to

714
00:45:47,920 --> 00:45:49,360
convince people

715
00:45:49,540 --> 00:45:52,080
so the

716
00:45:52,100 --> 00:45:56,260
the deal here is that by using

717
00:45:56,300 --> 00:46:04,640
artificial neural networks by using evolutionary search algorithms because the kind of the complexity of

718
00:46:04,640 --> 00:46:08,600
the simulation model that happening was constructed kind of propelled them to the results but

719
00:46:08,600 --> 00:46:13,640
they were hoping for without really understanding the

720
00:46:13,720 --> 00:46:17,090
but the effect but they were looking at

721
00:46:17,160 --> 00:46:22,580
and we had dave and i were forced to write a whole new simulation model

722
00:46:23,780 --> 00:46:25,420
to debunk it

723
00:46:26,880 --> 00:46:31,680
i never gets cited and no one knows about it is very irritating

724
00:46:31,860 --> 00:46:37,200
so here's another reason why you might resort to simulation which is maybe more

725
00:46:37,320 --> 00:46:39,480
less negative

726
00:46:39,540 --> 00:46:43,550
so math models often encourage us to embrace the limits this is infinity

727
00:46:43,640 --> 00:46:50,580
often it makes sense or it makes a model tractable to consider infinite populations

728
00:46:50,600 --> 00:47:00,620
individuals the interact over infinite time frictionless systems well mixed populations kinds of idealised asian

729
00:47:00,620 --> 00:47:06,460
that that

730
00:47:06,500 --> 00:47:11,120
the theory might lead us to believe that those idealise asians that we would be

731
00:47:11,120 --> 00:47:14,100
better off if we didn't have to make them

732
00:47:14,180 --> 00:47:17,960
some of the organisations often

733
00:47:18,020 --> 00:47:19,360
need to be made

734
00:47:19,880 --> 00:47:22,500
so what they they don't leave much room for

735
00:47:22,960 --> 00:47:28,760
transience isolation between parts of the system delays and lags locality

736
00:47:30,960 --> 00:47:36,720
it's a new kind of maybe sum up

737
00:47:36,740 --> 00:47:39,020
this the rest of logistics

738
00:47:39,300 --> 00:47:45,040
so who does what to whom where when and for how long the individual level

739
00:47:45,080 --> 00:47:46,600
in factors

740
00:47:46,780 --> 00:47:49,440
which are averaged out

741
00:47:49,480 --> 00:47:55,880
when you consider that when you consider the system in the limit

742
00:47:57,000 --> 00:48:00,660
the notion here is that it's these things are not good just because they're more

743
00:48:00,660 --> 00:48:05,300
realistic we're not going to want to include them in our model just because it

744
00:48:05,300 --> 00:48:09,920
takes us closer to the real world we have to have some theoretical motivation for

745
00:48:09,920 --> 00:48:14,420
thinking that these are important in our understanding of the system was all all no

746
00:48:14,420 --> 00:48:17,270
system is infinite in any respect

747
00:48:18,220 --> 00:48:22,100
so it's always if we were just motivated by realism we would always want to

748
00:48:22,100 --> 00:48:27,640
get rid of these things but sometimes they useful organisations theoretically motivated idolization

749
00:48:27,920 --> 00:48:30,600
so an example of that

750
00:48:31,480 --> 00:48:34,320
this good work on the piper cycles

751
00:48:34,480 --> 00:48:40,640
which is nice and simple two percent

752
00:48:40,740 --> 00:48:43,980
a big mess of chemical reactions

753
00:48:44,000 --> 00:48:47,060
if you set them up

754
00:48:47,140 --> 00:48:53,620
in the right way you'll find cycles of catalytic reactions reactions to catalyze each other

755
00:48:53,620 --> 00:48:56,020
in a mutually self-reinforcing way

756
00:48:56,060 --> 00:48:58,290
i cycles

757
00:48:59,680 --> 00:49:06,880
they can be vulnerable to parasites reactions which don't take part in the cycle which

758
00:49:06,890 --> 00:49:11,420
use up some of the the the catalysts some of the products

759
00:49:11,920 --> 00:49:15,940
well some of the the chemicals that take part in the reactions

760
00:49:16,300 --> 00:49:21,500
and those who wrote guide the cycles and

761
00:49:21,540 --> 00:49:26,620
and destroy them eventually so when systems like this

762
00:49:26,640 --> 00:49:29,360
are embedded in space so instead of considering

763
00:49:29,830 --> 00:49:34,940
big bag of chemicals to be well mixed solution a jar where every particle is

764
00:49:34,940 --> 00:49:38,850
is is as likely to interact with every other particle

765
00:49:39,360 --> 00:49:43,660
what if you spread out your chemicals over space in the system i'm talking about

766
00:49:43,660 --> 00:49:49,800
you you see spirals of chemical reactions on these spirals resist parasitic invasion so

767
00:49:50,340 --> 00:49:55,240
whereas in the well mixed population you don't see how cycles because they get destroyed

768
00:49:55,240 --> 00:50:00,980
by these parasitic chemical reactions before they can get started once you distribute the system

769
00:50:00,980 --> 00:50:02,460
in space you do see them

770
00:50:02,600 --> 00:50:08,840
so this is what by fourteen who will break i'm sure i'm not pronouncing those

771
00:50:08,840 --> 00:50:11,660
names correctly

772
00:50:11,700 --> 00:50:15,940
the hold about and they don't pronounce the g end of

773
00:50:16,480 --> 00:50:22,900
i'm going to try to find the it's impossible to pronounce dutch names when

774
00:50:22,920 --> 00:50:28,840
somebody tried to convince me what dennis bergkamp is called in in holland there's just

775
00:50:28,840 --> 00:50:31,420
nothing to this nothing like way it's written down

776
00:50:31,440 --> 00:50:34,720
you start off with a well mixed

777
00:50:34,780 --> 00:50:39,050
the distribution of different chemicals

778
00:50:39,520 --> 00:50:44,860
and the system evolves under a set of rules that govern which chemicals can interact

779
00:50:44,900 --> 00:50:48,820
and when interact what they give rise to what things catalyze that interaction

780
00:50:49,160 --> 00:50:51,780
and very rapidly the system organizers into

781
00:50:52,690 --> 00:50:57,360
spiral wave fronts which propagate across the space

782
00:50:57,460 --> 00:51:04,100
because there's a nice there's the rules imply sort of chain reactions and so green

783
00:51:04,100 --> 00:51:08,500
gives rise to yellow which gives rise to orange which gives rise to read to

784
00:51:08,500 --> 00:51:10,720
pink and that's why see these

785
00:51:10,920 --> 00:51:17,000
waves and for some reason the waves are not just

786
00:51:17,040 --> 00:51:19,180
horizontal parallel bars

787
00:51:19,220 --> 00:51:20,040
but they

788
00:51:20,040 --> 00:51:22,840
our initial ice from the labelled examples

789
00:51:22,860 --> 00:51:26,060
and so if there are several different clustering is that might come out from different

790
00:51:26,060 --> 00:51:30,240
random initial the stations of the clustering algorithm those

791
00:51:30,280 --> 00:51:33,800
supervised labelled examples will assure that

792
00:51:33,820 --> 00:51:35,900
we at least pick one that's

793
00:51:35,980 --> 00:51:40,400
pretty consistent with these labels so then that becomes the nationalization

794
00:51:40,600 --> 00:51:47,160
but getting intuition right

795
00:51:47,240 --> 00:51:50,280
OK so any comments

796
00:51:50,320 --> 00:51:55,360
suggestions clarifications on this

797
00:51:55,460 --> 00:51:58,300
what of

798
00:51:58,840 --> 00:52:02,080
how use this

799
00:52:03,710 --> 00:52:09,860
well um

800
00:52:09,880 --> 00:52:11,480
i don't i

801
00:52:11,500 --> 00:52:15,780
short answer is i don't think anybody knows how the brain works but there's is

802
00:52:15,780 --> 00:52:19,940
plenty that but it but there's just no way on

803
00:52:19,960 --> 00:52:24,620
the question was how does this correspond with what we might know about what happens

804
00:52:24,620 --> 00:52:26,520
in the brain

805
00:52:26,540 --> 00:52:30,340
because we don't know what happens exactly in the brain but it's interesting that there's

806
00:52:34,590 --> 00:52:37,000
pretty active

807
00:52:37,040 --> 00:52:41,000
so the research group in

808
00:52:42,620 --> 00:52:48,140
cognitive science and also in developmental psychology that's

809
00:52:50,920 --> 00:52:55,580
very interested in how is it that humans can learn so much

810
00:52:55,600 --> 00:52:58,240
it's so rapidly as children

811
00:52:58,260 --> 00:53:02,960
from apparently not mostly on labelled example

812
00:53:03,080 --> 00:53:05,960
mostly labelled examples

813
00:53:08,400 --> 00:53:10,860
so it the question that is

814
00:53:10,900 --> 00:53:16,440
considerable interest there my own hunch is that well i don't know if this stuff

815
00:53:16,440 --> 00:53:17,400
has any

816
00:53:17,420 --> 00:53:21,040
is an appropriate model for that are not my guess is that the second method

817
00:53:21,040 --> 00:53:25,520
we're going to talk about is a much more appropriate model for

818
00:53:25,540 --> 00:53:28,980
how humans get away with using of labelled data

819
00:53:29,000 --> 00:53:32,860
that's what

820
00:53:33,300 --> 00:53:43,100
you know you are set you know of

821
00:53:43,120 --> 00:53:45,160
so i was

822
00:53:46,310 --> 00:53:50,260
from despair hearts

823
00:53:50,280 --> 00:53:51,740
so i

824
00:53:52,000 --> 00:53:56,630
yeah i think there's it does top out somewhere below a hundred percent for a

825
00:53:56,630 --> 00:54:01,920
couple reasons one is that in some senses the classes are inherently ambiguous and so

826
00:54:01,920 --> 00:54:05,520
for example even something apparently obvious like spam

827
00:54:05,600 --> 00:54:10,380
what's spam to my not overlap perfectly with what's spam to me

828
00:54:10,400 --> 00:54:14,700
although probably it overlaps ninety percent of the guess

829
00:54:14,940 --> 00:54:18,540
but even if if we had two independent people label spam documents they don't agree

830
00:54:18,540 --> 00:54:20,900
completely so there is

831
00:54:20,940 --> 00:54:24,680
that kind of ambiguity in the labels to begin with the second is that there

832
00:54:24,700 --> 00:54:28,420
are some really severe assumptions in the naive bayes

833
00:54:28,440 --> 00:54:34,220
model for text classification the of words model so it's like saying

834
00:54:34,380 --> 00:54:39,360
the sentence machine learning is great is the same as saying the sentence

835
00:54:39,380 --> 00:54:42,230
learning great areas fishing

836
00:54:43,380 --> 00:54:46,560
and to people they don't seem like i'm saying the same thing but to this

837
00:54:46,560 --> 00:54:51,660
program they of this is john bellingham altogether and ignoring

838
00:54:51,760 --> 00:54:55,260
the content so there are some modelling problems here to

839
00:54:58,560 --> 00:55:00,600
OK so let's move on to the

840
00:55:08,530 --> 00:55:13,100
i thought hard about how to use my limited sixty minutes here

841
00:55:13,340 --> 00:55:16,780
and what you should know is that there's actually quite a bit this is a

842
00:55:16,780 --> 00:55:20,100
fairly hot topic in

843
00:55:20,140 --> 00:55:25,620
of statistical language analysis right now how do we use on labelled data to improve

844
00:55:25,620 --> 00:55:26,780
on our learning

845
00:55:28,920 --> 00:55:32,060
in sixty minutes i thought the most useful thing we can do

846
00:55:32,100 --> 00:55:35,140
was cover what seemed to me to be the two

847
00:55:35,160 --> 00:55:40,720
he types of problem structure that people are web urging with these algorithms

848
00:55:40,740 --> 00:55:41,920
and so

849
00:55:41,960 --> 00:55:45,860
i can show you a lot more algorithms for leveraging

850
00:55:45,880 --> 00:55:51,020
that first type of information that first type of problem structure but i'm not going

851
00:55:51,020 --> 00:55:55,280
to because i think the the most important thing to get out of this are

852
00:55:55,330 --> 00:56:01,180
talk is from that first part is all i see is

853
00:56:01,220 --> 00:56:08,000
documents naturally clusters into groups based on the words that currently in and if those

854
00:56:08,000 --> 00:56:12,920
groups have been actually clustering to happen to align with the class labels that i

855
00:56:12,920 --> 00:56:14,660
want my classify to assign

856
00:56:15,480 --> 00:56:19,620
then it's probably a good idea for me to be using on labelled data augment

857
00:56:21,960 --> 00:56:27,520
and furthermore i could probably test for that by running an unsupervised clustering algorithm and

858
00:56:27,520 --> 00:56:31,700
then seeing to what degree those natural clusters do

859
00:56:31,720 --> 00:56:33,630
a line with class labels

860
00:56:34,020 --> 00:56:38,100
and if you get that idea then i'm happy and you could probably yourself often

861
00:56:38,100 --> 00:56:41,500
think of better algorithms than the one we just talked about before we talked about

862
00:56:41,500 --> 00:56:46,610
it's pretty easy to to grok and it falls right on the naive bayes quest

863
00:56:47,050 --> 00:56:48,880
just talked about

864
00:56:48,880 --> 00:56:50,080
this one

865
00:56:50,100 --> 00:56:51,000
and so on

866
00:56:51,010 --> 00:56:53,350
max intl

867
00:56:53,370 --> 00:56:54,790
new coefficient

868
00:56:54,800 --> 00:56:57,830
the problem

869
00:56:59,250 --> 00:57:01,240
this is so

870
00:57:01,250 --> 00:57:04,960
let's call recursively

871
00:57:04,970 --> 00:57:14,170
because the one plus w one will be the two

872
00:57:14,200 --> 00:57:15,610
what does that mean

873
00:57:16,530 --> 00:57:18,680
my claim is that

874
00:57:18,790 --> 00:57:23,850
if i get it right

875
00:57:23,860 --> 00:57:26,860
basic scale

876
00:57:26,890 --> 00:57:30,630
automatically got it right and all higher scales

877
00:57:31,990 --> 00:57:34,610
these function functions of t

878
00:57:34,620 --> 00:57:36,050
or two t

879
00:57:37,040 --> 00:57:40,880
this seems to be to to to the stated that

880
00:57:40,880 --> 00:57:43,760
you've got the right scale

881
00:57:43,770 --> 00:57:44,580
but now

882
00:57:44,590 --> 00:57:46,700
look how we got here

883
00:57:46,720 --> 00:57:50,020
the zero plus w zero

884
00:57:50,100 --> 00:57:51,760
three one

885
00:57:51,810 --> 00:57:54,800
so suppose i plug in

886
00:57:54,850 --> 00:57:57,010
said one

887
00:57:58,300 --> 00:58:02,870
bringing everyone down into

888
00:58:03,040 --> 00:58:04,960
these are all

889
00:58:05,010 --> 00:58:06,740
plus w zero

890
00:58:06,750 --> 00:58:08,380
plus w one

891
00:58:11,590 --> 00:58:14,550
that's our government

892
00:58:14,730 --> 00:58:19,370
that's the multiscale here's the averages

893
00:58:19,410 --> 00:58:25,650
here is the course details

894
00:58:25,680 --> 00:58:28,250
here is the fine details

895
00:58:28,400 --> 00:58:30,920
i guess i'm realizing

896
00:58:37,090 --> 00:58:38,100
these are always

897
00:58:38,130 --> 00:58:40,490
to give this whole lecture

898
00:58:40,510 --> 00:58:44,050
would be to bring in matlab toolbox

899
00:58:44,060 --> 00:58:46,160
wavelet toolbox

900
00:58:46,170 --> 00:58:47,700
and see

901
00:58:47,720 --> 00:58:49,740
these pieces

902
00:58:49,750 --> 00:58:52,540
so i don't know what

903
00:58:55,150 --> 00:58:57,740
know you meant of course

904
00:58:57,780 --> 00:58:59,840
there are all these

905
00:59:04,520 --> 00:59:07,740
well what i

906
00:59:07,810 --> 00:59:09,630
what i get that

907
00:59:09,640 --> 00:59:13,410
OK i will that toolbox you

908
00:59:13,420 --> 00:59:16,630
this is probably is

909
00:59:20,970 --> 00:59:24,710
so here i am a mathematician

910
00:59:24,730 --> 00:59:29,200
so i was tired talking microphone

911
00:59:29,250 --> 00:59:35,110
ah well i could see the done so often

912
00:59:35,120 --> 00:59:38,330
usually by my colleague however

913
00:59:38,430 --> 00:59:40,340
and i could probably do it

914
00:59:40,850 --> 00:59:42,940
really so maybe

915
00:59:42,940 --> 00:59:48,400
let me check offline after this lecture what might be feasible

916
00:59:48,410 --> 00:59:54,820
but what what what we say hypothetical in in this for this additional image

917
00:59:55,700 --> 00:59:59,990
what matlab toolbox the matlab wavelet toolbox

918
01:00:00,010 --> 01:00:03,200
it's one of the many toolbox

919
01:00:03,240 --> 01:00:07,810
one of the many toolboxes by the way

920
01:00:07,870 --> 01:00:12,660
so matlab course is the product of works in

921
01:00:12,670 --> 01:00:16,350
but you know

922
01:00:16,370 --> 01:00:18,050
right toolboxes

923
01:00:18,060 --> 01:00:21,490
the individual

924
01:00:21,510 --> 01:00:23,720
scientists around the world

925
01:00:23,730 --> 01:00:26,240
well thank you know what

926
01:00:26,290 --> 01:00:29,680
i know how to use matlab

927
01:00:29,680 --> 01:00:31,660
wavelet theory

928
01:00:31,710 --> 01:00:34,020
alright toolbox

929
01:00:34,250 --> 01:00:41,140
and then they mathworks is the authors which in this case were four

930
01:00:41,240 --> 01:00:43,840
four additional french

931
01:00:47,530 --> 01:00:49,670
i want to mention

932
01:00:49,670 --> 01:00:56,390
they created the toolbox for the gooey the license user interface

933
01:00:57,330 --> 01:00:59,300
so that you could call it

934
01:00:59,360 --> 01:01:01,230
copper signal

935
01:01:01,230 --> 01:01:03,090
what's the signal itself

936
01:01:03,120 --> 01:01:08,180
well it's discrete actually looks like a continuous function

937
01:01:10,800 --> 01:01:13,250
but the toolbox to work

938
01:01:13,260 --> 01:01:14,920
and you say OK

939
01:01:14,930 --> 01:01:18,430
figure out it's

940
01:01:20,910 --> 01:01:23,360
this level

941
01:01:23,390 --> 01:01:25,840
it's averages

942
01:01:26,050 --> 01:01:30,910
he so so you would

943
01:01:30,930 --> 01:01:33,530
here's the five we can say here is the

944
01:01:33,550 --> 01:01:35,210
here's the function and now

945
01:01:35,210 --> 01:01:38,340
the toolbox would break into pieces

946
01:01:38,360 --> 01:01:43,460
so it would have these that was very smooth

947
01:01:44,080 --> 01:01:45,640
he was

948
01:01:45,660 --> 01:01:46,910
four of these

949
01:01:46,920 --> 01:01:48,890
is this

950
01:01:49,670 --> 01:01:53,820
catching some consolation that would be w zero is

951
01:01:55,790 --> 01:01:58,740
smaller probably much smaller

952
01:02:00,260 --> 01:02:03,530
but possibly more rapidly taking

953
01:02:03,540 --> 01:02:07,070
other isolation would be the w one part

954
01:02:07,120 --> 01:02:10,410
and we could have like five scales are we just

955
01:02:10,410 --> 01:02:13,050
well the toolbox how many skills we want

956
01:02:13,070 --> 01:02:15,930
four five would be to

957
01:02:17,730 --> 01:02:23,270
the rest these functions together and of course back to the right

958
01:02:24,310 --> 01:02:27,260
what i'm saying is the toolbox

959
01:02:27,280 --> 01:02:31,670
i'm writing things at the level of the whole space of functions

960
01:02:31,680 --> 01:02:34,690
but what that means is for each function

961
01:02:34,700 --> 01:02:39,870
just like for each function this weight function as a fourier series it's going away

962
01:02:39,980 --> 01:02:42,480
the series

963
01:02:42,540 --> 01:02:45,390
and the box with computer

964
01:02:45,450 --> 01:02:50,000
so what would the wavelet series b we use these functions

965
01:02:50,010 --> 01:02:54,180
we use these

966
01:02:54,200 --> 01:02:55,480
these are

967
01:02:57,260 --> 01:02:59,580
w was at all scales so

968
01:02:59,600 --> 01:03:01,720
so the the way

969
01:03:01,770 --> 01:03:03,730
expansion of function

970
01:03:03,900 --> 01:03:07,220
compare it with the fourier expansion

971
01:03:07,220 --> 01:03:08,800
four vertices

972
01:03:08,820 --> 01:03:11,090
and obviously has five edges

973
01:03:11,100 --> 01:03:13,080
its minimum valence

974
01:03:13,090 --> 01:03:15,780
here the religious history

975
01:03:17,160 --> 01:03:19,530
two so the minimum balances two

976
01:03:19,550 --> 01:03:22,710
and the maximum analysis three

977
01:03:22,760 --> 01:03:24,350
and as i mentioned earlier

978
01:03:24,380 --> 01:03:27,570
the matrices aren't

979
01:03:27,600 --> 01:03:29,340
the graph invariant

980
01:03:29,390 --> 01:03:32,780
although if you take the spectrum of the matrix of the

981
01:03:33,030 --> 01:03:36,930
adjacency matrix it is graph

982
01:03:37,800 --> 01:03:43,140
what is the spectrum it's the collection of of hiking VAT so the idea that

983
01:03:43,140 --> 01:03:45,010
these are graphic variants

984
01:03:45,020 --> 01:03:47,320
they are independent of the limitations of

985
01:03:47,430 --> 01:03:51,290
simultaneous politicians frozen sums rows and columns of

986
01:03:52,080 --> 01:03:55,160
adjacency matrix

987
01:03:55,360 --> 01:03:58,030
any questions

988
01:04:00,030 --> 01:04:03,630
let's go to subgraphs and contributing

989
01:04:03,650 --> 01:04:07,030
if we have a graph and if we

990
01:04:07,050 --> 01:04:08,230
that's the

991
01:04:08,430 --> 01:04:11,110
and select two subset

992
01:04:12,300 --> 01:04:14,120
there the subset of the vertex set

993
01:04:14,140 --> 01:04:17,260
and f which is the subset of the answer

994
01:04:17,280 --> 01:04:22,080
then it may happen that this pair you have is

995
01:04:22,110 --> 01:04:24,530
a graph or it's not

996
01:04:24,550 --> 01:04:26,490
if it is a graph

997
01:04:26,500 --> 01:04:28,630
then it is called the subgraph

998
01:04:31,060 --> 01:04:34,800
but if it's not the graph then of course that the sum

999
01:04:34,820 --> 01:04:38,790
so for instance

1000
01:04:38,800 --> 01:04:42,010
so let me just give you a little example

1001
01:04:43,800 --> 01:04:48,040
or random graph here

1002
01:04:48,100 --> 01:04:56,400
and if we select this edge here

1003
01:04:56,410 --> 01:04:59,070
and this vertex here

1004
01:04:59,080 --> 01:05:00,760
this is not something

1005
01:05:00,780 --> 01:05:03,830
because together with this edge

1006
01:05:03,860 --> 01:05:07,510
you have to select the two endpoints

1007
01:05:07,550 --> 01:05:12,780
if you select the three vertices and and one edge then that would be so

1008
01:05:15,270 --> 01:05:16,140
of course

1009
01:05:16,160 --> 01:05:17,660
if you equals v

1010
01:05:17,660 --> 01:05:21,680
then we say that h is a spanning subgraph so this means

1011
01:05:21,700 --> 01:05:26,690
we just remove some of the edges of the original graph in order to get

1012
01:05:26,700 --> 01:05:27,870
the subgraph

1013
01:05:28,930 --> 01:05:30,460
the other extreme

1014
01:05:30,560 --> 01:05:31,910
it is

1015
01:05:31,960 --> 01:05:33,660
if you have

1016
01:05:33,700 --> 01:05:37,730
if you have a graph and you just select

1017
01:05:39,370 --> 01:05:43,210
of vertices

1018
01:05:43,240 --> 01:05:46,630
that this collection of vertices

1019
01:05:46,650 --> 01:05:50,900
somehow defines the maximal possible

1020
01:05:52,150 --> 01:05:54,290
on that induced on that

1021
01:05:54,380 --> 01:05:58,530
set of this is this means that you get all the edges that belong to

1022
01:05:58,530 --> 01:06:05,760
that have both endpoints in this subset then we are talking about the use of

1023
01:06:06,640 --> 01:06:13,060
as we said here in this graph is completely determined by the set of this

1024
01:06:13,080 --> 01:06:15,500
here are

1025
01:06:15,520 --> 01:06:17,120
three weeks examples

1026
01:06:18,010 --> 01:06:19,510
the very first

1027
01:06:19,540 --> 01:06:22,850
graph is k four here is an example that is

1028
01:06:22,890 --> 01:06:26,630
the graph of the subgraph that is neither induced or spending

1029
01:06:26,650 --> 01:06:30,900
a spanning subgraph and that's continuous

1030
01:06:30,910 --> 01:06:37,780
so what is the walk

1031
01:06:37,790 --> 01:06:39,150
a walk

1032
01:06:39,160 --> 01:06:40,370
in our case

1033
01:06:40,390 --> 01:06:41,330
in the graph

1034
01:06:41,360 --> 01:06:45,250
this is just the sequence of vertices

1035
01:06:45,260 --> 01:06:46,200
the zero

1036
01:06:46,250 --> 01:06:49,230
v one up to k

1037
01:06:49,250 --> 01:06:50,930
with the property

1038
01:06:50,950 --> 01:06:52,810
that's too

1039
01:06:53,940 --> 01:06:55,310
but this is

1040
01:06:55,370 --> 01:06:56,600
in that sequence

1041
01:06:56,620 --> 01:07:00,650
at adjacent vertices in the graph

1042
01:07:00,660 --> 01:07:03,210
and in this case

1043
01:07:03,270 --> 01:07:05,360
we can talk about

1044
01:07:05,370 --> 01:07:07,780
the initial vertex v zero

1045
01:07:07,790 --> 01:07:08,410
tell me

1046
01:07:08,430 --> 01:07:13,020
vertex vk and we talk about the walk from the zero to vk or between

1047
01:07:13,020 --> 01:07:15,080
these are on the k

1048
01:07:15,100 --> 01:07:19,610
these are maybe also called the beginning of the walk and became the end of

1049
01:07:19,610 --> 01:07:20,460
the war

1050
01:07:20,520 --> 01:07:21,420
and k

1051
01:07:21,430 --> 01:07:25,980
the number k is called the length of the war w

1052
01:07:26,000 --> 01:07:29,530
so there are some special cases of walks

1053
01:07:29,600 --> 01:07:34,060
if i may just draw like this so walk looks like this

1054
01:07:34,070 --> 01:07:39,200
but it may itself so it may go

1055
01:07:39,210 --> 01:07:40,190
so once you

1056
01:07:40,210 --> 01:07:43,780
make a picture of what you don't see what was going on just

1057
01:07:44,910 --> 01:07:47,010
we were walking like this so here

1058
01:07:47,030 --> 01:07:48,390
here here

1059
01:07:48,450 --> 01:07:52,910
here and then back here here here here

1060
01:07:54,610 --> 01:07:56,000
so i walk

1061
01:07:56,010 --> 01:07:57,790
it's called a closed walk

1062
01:07:57,800 --> 01:08:01,230
is that if the terminal vertex is identical

1063
01:08:01,290 --> 01:08:05,850
the initial to lock so we start walking and get to the point

1064
01:08:05,860 --> 01:08:07,490
so that would not be

1065
01:08:07,510 --> 01:08:10,280
close what would be an open book

1066
01:08:12,690 --> 01:08:16,480
a very special case of the walk is a path

1067
01:08:16,770 --> 01:08:20,000
that's just to walk

1068
01:08:20,020 --> 01:08:23,890
which belongs to an

1069
01:08:23,900 --> 01:08:28,380
whose underlying subgraphs is upon

1070
01:08:28,410 --> 01:08:30,390
as we defined it so

1071
01:08:30,400 --> 01:08:34,390
all vertices in the work must be distinct

1072
01:08:34,400 --> 01:08:37,160
and special cases of cycle

1073
01:08:37,200 --> 01:08:39,890
where again all of this is a listing

1074
01:08:39,920 --> 01:08:42,890
except for the very first and the last which would be the same so it's

1075
01:08:42,900 --> 01:08:44,070
a close to walk

1076
01:08:45,030 --> 01:08:46,920
this is thing

1077
01:08:46,940 --> 01:08:53,070
so path can be also used as the subgraph of g isomorphic and that's also

1078
01:08:53,070 --> 01:08:56,300
the same

1079
01:08:57,880 --> 01:08:59,550
there is

1080
01:08:59,550 --> 01:09:06,430
well so f is proportional to weight so f diverges if inverse power

1081
01:09:06,470 --> 01:09:07,760
goes to zero

1082
01:09:07,760 --> 01:09:13,270
that's infinite redshift survey infinite redshift that's where the more items it corresponds to an

1083
01:09:13,270 --> 01:09:19,140
equal to zero now you can compute the radius of the sphere there it seems

1084
01:09:19,140 --> 01:09:24,510
to actually go to zero but that's true because there's also left one here and

1085
01:09:24,510 --> 01:09:28,490
if you compute it you will see that they able to use is finite and

1086
01:09:28,510 --> 01:09:31,820
the area is exactly two pi squared

1087
01:09:31,840 --> 01:09:36,680
our one five p one of these this constant here

1088
01:09:36,700 --> 01:09:41,090
and you can also put in the value of the newton's constant which is again

1089
01:09:41,090 --> 01:09:45,550
given exactly by string theory so these are numbers you get from the string theory

1090
01:09:46,720 --> 01:09:47,660
and then

1091
01:09:47,680 --> 01:09:50,470
the analogue of the bacon steinhardt in

1092
01:09:50,490 --> 01:09:53,050
calculation is simple problem

1093
01:09:53,120 --> 01:09:57,910
is been giving you these very simple answer to the entropy of the black hole

1094
01:09:57,930 --> 01:10:01,800
his area of corsi's over four times g new

1095
01:10:01,840 --> 01:10:05,050
i forgot to nominate biarritz put equal to one

1096
01:10:05,070 --> 01:10:08,610
and this is too by squirrel and one n five p

1097
01:10:08,800 --> 01:10:12,890
so that's doing the analogue of the same classical hot

1098
01:10:12,930 --> 01:10:19,640
calculation in this slightly more complicated one

1099
01:10:19,640 --> 01:10:24,890
now can we find the same result by counting whether the answer is yes somehow

1100
01:10:24,930 --> 01:10:27,510
here is how it goes

1101
01:10:27,550 --> 01:10:30,090
so here is our three charge black hole

1102
01:10:30,110 --> 01:10:34,970
we look at it through a loop remember and it is a very complicated beast

1103
01:10:34,970 --> 01:10:39,530
that's the analogue of having this being description of the potomac

1104
01:10:39,550 --> 01:10:45,870
yeah its brains little open strings that run along the brain and carry momentum

1105
01:10:45,930 --> 01:10:50,990
and actually there is some effective low energy theory of the brains which i will

1106
01:10:50,990 --> 01:10:55,390
not try to describe in great detail here that's being

1107
01:10:55,430 --> 01:10:59,840
necessary but if some of you really want to get into details here is for

1108
01:10:59,840 --> 01:11:01,700
instance in the u

1109
01:11:01,720 --> 01:11:05,680
let me just say the words the the effective low energy theory of the brain

1110
01:11:05,680 --> 01:11:12,530
this is actually a supersymmetric yang mills theory again with you and one because five

1111
01:11:12,530 --> 01:11:19,320
gauge groups this is exactly like the constructions of brain one the official yesterday

1112
01:11:19,350 --> 01:11:24,070
then there are open strings that go from d one to d five so like

1113
01:11:24,090 --> 01:11:29,740
the matter fields of yesterday and here they correspond to hyper multiplets of you have

1114
01:11:30,050 --> 01:11:35,340
suppressed marksmanship semantic theory there is a certain number of them

1115
01:11:35,390 --> 01:11:39,550
now you can try to analyse what the entropy of this theory is

1116
01:11:39,570 --> 01:11:44,720
and there is one very simple even one of the calculation is trivial counting problem

1117
01:11:44,720 --> 01:11:52,200
again i'll show you this

1118
01:11:52,260 --> 01:11:56,950
so what we need to do remember is counts the number of states with the

1119
01:11:56,950 --> 01:12:00,200
total number of momentum in p

1120
01:12:00,240 --> 01:12:05,180
all of this configuration of n one n five brains

1121
01:12:05,220 --> 01:12:11,370
this boils down in the simplest case counting the number of ways of distributing the

1122
01:12:11,370 --> 01:12:12,870
total momentum

1123
01:12:12,890 --> 01:12:15,430
among the and one in five massless

1124
01:12:15,450 --> 01:12:20,340
open strings that stretched between the ones thousand the fives

1125
01:12:20,390 --> 01:12:24,820
well that's a very simple combinatorial problem you can actually do it

1126
01:12:24,820 --> 01:12:28,120
by using what is called the generating function

1127
01:12:28,180 --> 01:12:31,950
there are four and one in five of these rings

1128
01:12:31,990 --> 01:12:36,370
each one y four one four because that's the number of

1129
01:12:36,450 --> 01:12:39,780
it was only come from you on being the degrees of freedom in one of

1130
01:12:39,780 --> 01:12:46,390
these hypermarket let's that some supersymmetry multiply so far and one five and each one

1131
01:12:46,390 --> 01:12:52,180
of them can carry some momentum little n there are boards and some fermions these

1132
01:12:52,180 --> 01:12:57,840
out of the three boys and three FIRM you want to partition functions just compute

1133
01:12:59,340 --> 01:13:04,180
he called the there are more chozick coefficient q to the total momentum p

1134
01:13:04,180 --> 01:13:09,260
is a very simple asymptotic formula on the scottish formula that gives you the answer

1135
01:13:09,260 --> 01:13:14,120
and that's exactly what we found was talking

1136
01:13:14,180 --> 01:13:16,840
so the two results agree exactly

1137
01:13:16,850 --> 01:13:23,180
but actually when you think about this calculation a bit more intensely

1138
01:13:23,220 --> 01:13:28,140
you say why should they agree since after all we have done very little two

1139
01:13:28,140 --> 01:13:31,240
very different calculations here

1140
01:13:31,280 --> 01:13:35,700
it's as if i had done the calculation for the free spins in may

1141
01:13:35,740 --> 01:13:40,430
more than the very start and i told you that the result is exactly correct

1142
01:13:40,430 --> 01:13:41,550
in the limit of

1143
01:13:41,590 --> 01:13:43,780
extremely strong coupling reaction

1144
01:13:43,800 --> 01:13:47,820
you wouldn't believe me there because there would be no reason why the two should

1145
01:13:48,720 --> 01:13:53,410
and something analogous is happening here and the reason is the following

1146
01:13:53,430 --> 01:13:57,180
in doing the gravity calculation of heart beating

1147
01:13:57,220 --> 01:14:01,640
i have shown that there was a nice large most geometry

1148
01:14:01,660 --> 01:14:02,610
now this

1149
01:14:02,620 --> 01:14:07,780
boils down to to saying that the various length scales that appeared in the geometry

1150
01:14:07,780 --> 01:14:10,260
namely our one five

1151
01:14:10,280 --> 01:14:13,510
the volume of the ford taurus and so on

1152
01:14:13,530 --> 01:14:14,350
at all

1153
01:14:14,370 --> 01:14:21,030
larger than both the characteristic string scale and the planck length if this wasn't true

1154
01:14:21,030 --> 01:14:24,200
then i had no right to use simply

1155
01:14:24,260 --> 01:14:30,090
today about the einstein theory i i would have to use the full quantum gravity

1156
01:14:30,090 --> 01:14:35,180
or the full string theory in doing this calculations that would be corrections to this

1157
01:14:36,300 --> 01:14:41,910
analogous to the finite volume corrections that i mentioned before so our results there was

1158
01:14:41,910 --> 01:14:45,390
only valid because they shown that this

1159
01:14:45,700 --> 01:14:52,300
scales are very large compared to the fundamental microscopic scale so the theory

1160
01:14:52,350 --> 01:14:57,970
but if you look back at the expressions for the scales

1161
01:14:57,990 --> 01:14:59,260
here they are

1162
01:14:59,280 --> 01:15:02,140
and if you translate what they mean

1163
01:15:02,140 --> 01:15:05,950
you will see that the mean for instance that the number of strings are five

1164
01:15:05,950 --> 01:15:11,610
brains and the string coupling is very very big much bigger than one

1165
01:15:11,660 --> 01:15:17,240
and now the effective coupling of young mills theory on the d brains is exactly

1166
01:15:17,240 --> 01:15:19,140
this one will see

1167
01:15:19,220 --> 01:15:22,410
the moment why the more precisely

1168
01:15:22,450 --> 01:15:29,680
therefore when the gravity calculations justified the young miscalculation is not at all justified

1169
01:15:29,740 --> 01:15:34,760
and conversely one of the young miscalculation is justified and that's what they here the

1170
01:15:34,760 --> 01:15:37,510
gravity calculation is not justified

1171
01:15:37,550 --> 01:15:43,510
these are two strong weak coupling regime in which you have the calculation again as

1172
01:15:43,510 --> 01:15:50,010
if we consider free spins very strongly interacting spins for month

1173
01:15:50,030 --> 01:15:54,390
so it's not clear why the two should have agreed that actually in this particular

1174
01:15:54,390 --> 01:16:00,200
case that they did easily saved by supersymmetry game namely

1175
01:16:00,200 --> 01:16:05,800
what we are really counting here it turns out of supersymmetric ground states in a

1176
01:16:07,030 --> 01:16:12,850
chad sector or as they are called one needs replacement black holes

1177
01:16:12,950 --> 01:16:19,120
and because of this is a number of topological index remember which modulo some mild

1178
01:16:19,120 --> 01:16:24,390
assumptions does not change as you very confused parameters

1179
01:16:24,410 --> 01:16:30,680
we could actually extrapolate the weak coupling resolved to infinitely strong coupling it hasn't changed

1180
01:16:30,720 --> 01:16:33,300
and i got exactly the same month

1181
01:16:33,320 --> 01:16:38,140
she says you may say we just got like people do this calculation it can

1182
01:16:38,140 --> 01:16:43,670
theta is quantity of of interest and x represents all the things that we know

1183
01:16:43,680 --> 01:16:48,280
about theta so far so all the data we have collected and other information we

1184
01:16:48,280 --> 01:16:52,070
have that we describe are uncertainty

1185
01:16:52,120 --> 01:16:57,270
on a theta by this conditional probability p theta given all the things we know

1186
01:16:57,270 --> 01:17:00,070
so far summarise by x

1187
01:17:00,090 --> 01:17:04,930
and we'll call that the prior probability and as we observe some new data x

1188
01:17:05,900 --> 01:17:10,390
then we can absorb the evidence from x had by computing this quantity which is

1189
01:17:10,390 --> 01:17:14,550
the likelihood function and we think this is the function of theta

1190
01:17:14,550 --> 01:17:16,230
so p of x given theta

1191
01:17:17,220 --> 01:17:18,010
in which

1192
01:17:18,780 --> 01:17:21,540
x that is set to the observed values

1193
01:17:21,580 --> 01:17:25,170
we look at this is the function of features we evaluate for all values theta

1194
01:17:25,170 --> 01:17:27,190
we multiply that by the prior

1195
01:17:27,250 --> 01:17:31,570
and up to normalisation constant to get the posterior distribution

1196
01:17:31,700 --> 01:17:35,500
so it using bayes theorem to absorb the effect that new evidence

1197
01:17:35,510 --> 01:17:40,200
so we've gone from the distribution before we observed that had the distribution after we

1198
01:17:40,200 --> 01:17:45,970
observe that will see lots of examples of that as we go through

1199
01:17:46,000 --> 01:17:51,520
and the other important idea is to make predictions we marginalize so

1200
01:17:51,530 --> 01:17:52,970
in this simple case

1201
01:17:52,980 --> 01:17:55,460
we wish to predict quantity y

1202
01:17:56,550 --> 01:17:58,410
we will assume that the

1203
01:18:00,020 --> 01:18:03,680
depends on x only through theta so once we know feat

1204
01:18:03,690 --> 01:18:04,620
p of y

1205
01:18:04,640 --> 01:18:07,010
doesn't depend on x

1206
01:18:07,050 --> 01:18:07,970
and so

1207
01:18:08,000 --> 01:18:10,370
from the sum and product rules

1208
01:18:10,380 --> 01:18:11,270
we want to

1209
01:18:11,290 --> 01:18:13,220
predict p of y given

1210
01:18:14,240 --> 01:18:17,520
that is obtained by taking p of y given theta if theta given x and

1211
01:18:17,520 --> 01:18:24,190
marginalizing so making predictions usually use each and every possible value for theta weighted by

1212
01:18:24,220 --> 01:18:27,890
the current distribution of hysteria distribution

1213
01:18:27,900 --> 01:18:32,170
another thing about this of course is that this is now effectively if we observe

1214
01:18:32,170 --> 01:18:36,780
it more information this is now our prior to the next application of bayes theorem

1215
01:18:36,950 --> 01:18:41,650
to bayesian inference is intrinsically a sequential process as you have more information people knew

1216
01:18:41,650 --> 01:18:43,900
about what the the likelihood function

1217
01:18:43,920 --> 01:18:48,060
multiplied by the current distribution which is the prior to get the new distribution which

1218
01:18:48,060 --> 01:18:50,750
is the posterior

1219
01:18:53,590 --> 01:18:56,180
why is prior knowledge important so i said the

1220
01:18:56,190 --> 01:19:00,650
the interesting thing about the third generation framework is that we can incorporate rich prior

1221
01:19:00,650 --> 01:19:06,160
knowledge into a machine learning model someone's prior knowledge important so cartoon sketch of what

1222
01:19:06,160 --> 01:19:10,040
might be important so miserable x and the variable y

1223
01:19:10,050 --> 01:19:14,810
and we we collected some data should pairs of values of x and y

1224
01:19:14,830 --> 01:19:18,430
and we want to make predictions we want to know what value is why did

1225
01:19:18,430 --> 01:19:22,390
it take for this new value of x is not been observed previously

1226
01:19:22,400 --> 01:19:24,900
and as things stand it's actually quite quite hard to

1227
01:19:25,300 --> 01:19:26,930
to know what we should say about why

1228
01:19:26,940 --> 01:19:30,200
not clear what value we should choose

1229
01:19:30,220 --> 01:19:32,300
and if i tell you that

1230
01:19:32,350 --> 01:19:33,770
the values of y

1231
01:19:33,800 --> 01:19:39,310
were generated from the values of x by a straight line with some guassian noise

1232
01:19:39,310 --> 01:19:41,730
added and i tell you the variance of the noise i give you a lot

1233
01:19:41,730 --> 01:19:47,010
of prior knowledge about the way the data generated using a much better position to

1234
01:19:47,010 --> 01:19:49,450
make predictions about the value of y

1235
01:19:49,470 --> 01:19:53,540
and in fact of course without any prior knowledge you can't really say anything there

1236
01:19:53,540 --> 01:19:57,190
are there are many many different ways in which white could have been generated from

1237
01:19:57,190 --> 01:20:01,470
x is a huge range of possibilities and the more prior knowledge you have the

1238
01:20:01,470 --> 01:20:06,670
more constrained range approx possibilities and therefore the better predictions you can make actually prior

1239
01:20:06,670 --> 01:20:14,440
knowledge is very central to machine learning really learn anything just from data alone

1240
01:20:15,430 --> 01:20:16,900
graphical models

1241
01:20:16,910 --> 01:20:23,550
kenny and we had to be already use graphical models in your research

1242
01:20:23,670 --> 01:20:26,140
about half the who

1243
01:20:26,160 --> 01:20:31,170
OK great so what i'm going to do is to

1244
01:20:31,660 --> 01:20:35,400
as i said skin lightly over the topic of graphical models and try to motivate

1245
01:20:35,400 --> 01:20:39,680
them is even is going to give two entire talks on the details of graphical

1246
01:20:40,900 --> 01:20:42,470
so i shall just give

1247
01:20:42,520 --> 01:20:47,500
a brief introduction for those of you who are not already familiar with the field

1248
01:20:47,510 --> 01:20:49,640
so the idea of graphical models is

1249
01:20:49,650 --> 01:20:53,600
to take probability theory in other words the sum and product probability and combine them

1250
01:20:53,600 --> 01:20:57,190
with some diagrams and pictures some graphs

1251
01:20:57,250 --> 01:20:59,720
this brings lots of nice

1252
01:21:01,660 --> 01:21:06,500
it gives insights into existing models models which already just specified mathematically and i see

1253
01:21:06,500 --> 01:21:09,930
them pictorially alliances to understand

1254
01:21:09,940 --> 01:21:13,120
but for most of us certainly for me allows us to understand the structure of

1255
01:21:13,120 --> 01:21:17,670
the model is much more clearly many many people myself included find it much easier

1256
01:21:17,670 --> 01:21:21,050
to look at the picture and save the structure than to look at the big

1257
01:21:21,130 --> 01:21:23,750
page of mathematics

1258
01:21:23,800 --> 01:21:27,630
it also provides a framework for designing new models once we see the structure of

1259
01:21:27,630 --> 01:21:32,090
the model graphically we can also the model if we wish by adding links to

1260
01:21:32,090 --> 01:21:35,190
the graph or removing links or adding new nodes and so on so we can

1261
01:21:35,190 --> 01:21:38,810
design are model by designing the diagram

1262
01:21:38,830 --> 01:21:43,550
but it's actually much better than that because we can also do calculations using

1263
01:21:43,560 --> 01:21:47,480
using the diagrams we can do things like read off

1264
01:21:47,540 --> 01:21:52,800
conditional independence properties simply by looking at separation properties directly on the graph

1265
01:21:52,800 --> 01:21:54,810
thank you

1266
01:21:54,930 --> 01:21:59,140
so in the feature selection challenge

1267
01:21:59,150 --> 01:22:04,150
so it's nice thousand three and it made a mistake in the future stations and

1268
01:22:04,150 --> 01:22:10,060
there were five datasets one is a mass spectrometry data it's the task of separating

1269
01:22:10,060 --> 01:22:12,300
cancer versus normal

1270
01:22:12,410 --> 01:22:19,580
dexter text processing dataset extracted from the reuters corpus

1271
01:22:19,590 --> 01:22:22,780
the idea is drug discovery data sets

1272
01:22:22,800 --> 01:22:28,050
corresponding to predicting the molecular activity of some compounds

1273
01:22:28,060 --> 01:22:34,820
she it is handwritten digit recognition and man with an artificial dataset and what you're

1274
01:22:34,830 --> 01:22:36,650
looking at here is the distribution

1275
01:22:37,070 --> 01:22:41,580
of the results of the participants in the competition

1276
01:22:41,590 --> 01:22:47,260
so what we measured results in terms of balanced error rate which is the average

1277
01:22:47,260 --> 01:22:51,200
of the error rate of the positive class the negative class it's a measure of

1278
01:22:51,200 --> 01:22:52,870
performance that is a good

1279
01:22:53,740 --> 01:22:59,060
when you have unbalanced classes it averages the those and avoids that you will always

1280
01:22:59,060 --> 01:23:02,650
for the most abundant class for which the error to be able to give you

1281
01:23:02,650 --> 01:23:09,110
the good the good result even though you make too trivial

1282
01:23:09,120 --> 01:23:10,300
i guess

1283
01:23:10,420 --> 01:23:16,050
so in that case you can see that on some datasets there is a broad

1284
01:23:16,050 --> 01:23:23,150
distribution so people had anywhere from from as in people have anywhere from ten fifteen

1285
01:23:23,150 --> 01:23:27,940
percent error to almost fifty percent error

1286
01:23:28,030 --> 01:23:35,320
for the next it was more peaked around the five percent balanced error rate for

1287
01:23:35,320 --> 01:23:37,510
the termites is very flat

1288
01:23:37,550 --> 01:23:41,660
so people can get you know very good results in very bad results project is

1289
01:23:41,660 --> 01:23:45,320
much more peaked format along it's very interesting that you have

1290
01:23:45,370 --> 01:23:49,390
a bimodal distribution so people are either going to try to go it alone

1291
01:23:49,410 --> 01:23:53,800
and knowing how you know the data set was designed causing artificial data it's no

1292
01:23:53,800 --> 01:23:59,370
surprise he was very linear problem can never met at x or problem where i

1293
01:23:59,370 --> 01:24:05,120
had put in a five dimensional space clusters of the two classes are arranged on

1294
01:24:05,130 --> 01:24:10,220
hypercube so if you try the linear model you would miserably fails so that's probably

1295
01:24:10,230 --> 01:24:11,090
in the

1296
01:24:11,090 --> 01:24:14,170
twenty percent are of the linear model people

1297
01:24:14,190 --> 01:24:19,350
and this was the best course of feature selection so we ask people both to

1298
01:24:19,590 --> 01:24:25,080
get good balanced error rate and small feature sets

1299
01:24:25,090 --> 01:24:31,160
so the class you know focused on on feature extraction feature selection but at the

1300
01:24:31,160 --> 01:24:35,050
same time people had to learn about you know all the

1301
01:24:35,100 --> 01:24:39,550
tools of machine learning some of them had taken classes in machine learning before but

1302
01:24:39,550 --> 01:24:44,870
i discovered i had enough to review again a lot of the basic concepts of

1303
01:24:45,510 --> 01:24:50,180
they don't about all the basic tools again

1304
01:24:50,210 --> 01:24:55,650
and what's amazing about this class is the distance considerably

1305
01:24:56,430 --> 01:25:00,090
i exceeded my makes speculations

1306
01:25:01,420 --> 01:25:08,430
i have to say that my colleagues and i who organised the challenge we tried

1307
01:25:08,430 --> 01:25:12,830
very hard to match the performance of the participants

1308
01:25:12,840 --> 01:25:14,700
and it was not trivial

1309
01:25:15,100 --> 01:25:19,160
and we did not succeed actually imagine this performance even though we had in all

1310
01:25:19,160 --> 01:25:24,490
the results of the participants so it's not so easy to what i did is

1311
01:25:24,490 --> 01:25:26,480
that we created

1312
01:25:26,570 --> 01:25:29,470
the toolkit based on the

1313
01:25:29,490 --> 01:25:36,490
spider it implemented in matlab spider is a machine learning package that was implemented at

1314
01:25:36,850 --> 01:25:42,100
the max plank institute in tuning in

1315
01:25:42,100 --> 01:25:47,190
and it consists of a matlab learning objects so we built on top of that

1316
01:25:47,190 --> 01:25:52,130
and added methods that were successful in in the challenge and i provided to the

1317
01:25:52,130 --> 01:25:54,460
student baseline methods

1318
01:25:54,520 --> 01:25:57,850
so make sure you know that the methods i had one of the best i

1319
01:25:57,850 --> 01:26:01,610
could i could get because i want to know that they would have the potential

1320
01:26:01,610 --> 01:26:05,520
to improve a little bit on what they did but then i also try to

1321
01:26:05,520 --> 01:26:12,160
give them baseline methods that going about the tenth percentile of the best methods secession

1322
01:26:12,220 --> 01:26:17,210
basically what the best i could do myself either within the ten percent i thought

1323
01:26:17,210 --> 01:26:19,870
okay they will maybe improve a little bit and so on

1324
01:26:19,910 --> 01:26:25,630
but they ended up all to really outperform the baseline methods and to even outperform

1325
01:26:25,630 --> 01:26:31,180
the best performers in the challenge and they were motivated to do that because i

1326
01:26:31,180 --> 01:26:36,680
give them more points if they would outperform the best results in the challenge the

1327
01:26:36,680 --> 01:26:40,650
total number of point to get the maximum grade they could get the total amount

1328
01:26:40,650 --> 01:26:44,200
of points to get the maximum grade without out of performing the best results in

1329
01:26:44,200 --> 01:26:47,330
the challenge season even if you know the exceeded

1330
01:26:47,370 --> 01:26:54,240
the baseline performance with a significant margin they would get the maximum point but they

1331
01:26:54,270 --> 01:26:58,650
get extra points you know if they are they exceeded the

1332
01:26:58,670 --> 01:27:02,890
first results of the challenge and so they all worked extremely hard

1333
01:27:02,900 --> 01:27:07,210
two to be that and that's that's what they did so what you see on

1334
01:27:07,210 --> 01:27:11,870
this graph the balanced error rate

1335
01:27:12,180 --> 01:27:15,350
eight percent of the baseline method

1336
01:27:15,410 --> 01:27:18,400
and then the challenge

1337
01:27:18,420 --> 01:27:22,410
the best results with the bar

1338
01:27:22,420 --> 01:27:27,460
and then the results of the students except for the durability editor dataset they always

1339
01:27:27,460 --> 01:27:33,350
going to propagate at different speeds depending on the kind of media is propagating through

1340
01:27:35,020 --> 01:27:40,480
the way we models is mathematically is the acoustic wave propagation

1341
01:27:40,490 --> 01:27:41,970
so imagine

1342
01:27:41,990 --> 01:27:46,100
first second that we're just have only one access to make things simple

1343
01:27:47,150 --> 01:27:51,310
things are propagating in the z direction here that

1344
01:27:51,830 --> 01:27:55,650
so how does this work so we have one t wave propagation

1345
01:27:55,690 --> 01:27:57,410
where we have the

1346
01:27:57,900 --> 01:28:05,650
acoustic wave equations that depends on this coefficients that are medium dependent easy

1347
01:28:05,660 --> 01:28:06,830
OK so you

1348
01:28:06,920 --> 01:28:11,760
is the unknown the particle displacement at time t and that's the

1349
01:28:11,780 --> 01:28:16,270
t in this equation is time and the is that

1350
01:28:16,280 --> 01:28:21,370
and so this equation is the wave equation which is telling you how particles get

1351
01:28:21,370 --> 01:28:23,030
displaced at time t

1352
01:28:23,050 --> 01:28:25,140
and that the

1353
01:28:25,140 --> 01:28:31,140
became an easy is the acoustic impedances what you wish to recover part of these

1354
01:28:31,140 --> 01:28:35,760
experiments because if you could recover this profile easy

1355
01:28:35,810 --> 01:28:41,220
then you would know as this is the corresponding to waters's corresponding to send sanitise

1356
01:28:41,220 --> 01:28:44,710
corresponding to of oil and so you would know that that that's

1357
01:28:44,720 --> 01:28:48,470
there is already a new start drilling prism

1358
01:28:50,070 --> 01:28:54,260
so the way seismologists probes the media

1359
01:28:54,300 --> 01:28:55,950
is to probe the

1360
01:28:55,990 --> 01:28:59,970
the the system by a wavelet what they call the wave which has nothing to

1361
01:28:59,970 --> 01:29:03,990
do with the way that people use in other fields the college wavelets f of

1362
01:29:04,930 --> 01:29:09,020
try to is the idea of making up his explosion of the surface

1363
01:29:09,040 --> 01:29:13,710
and so what this exit six the boundary condition which said that the derivative of

1364
01:29:13,710 --> 01:29:14,840
the field

1365
01:29:14,890 --> 01:29:16,570
along the z direction

1366
01:29:16,590 --> 01:29:22,240
at the surface z equals zero is whatever i put in which is way fifty

1367
01:29:25,040 --> 01:29:26,160
of course

1368
01:29:26,160 --> 01:29:30,250
this boathouse trailing some microphones that are going to collect

1369
01:29:30,270 --> 01:29:34,440
the article that is you use and this pressure wave and then there's an echo

1370
01:29:34,440 --> 01:29:36,250
that comes back

1371
01:29:36,270 --> 01:29:40,020
and the echo comes back because you have this discontinuity in the medium so it's

1372
01:29:40,020 --> 01:29:41,340
time the wave

1373
01:29:41,350 --> 01:29:44,940
which is the discontinuity a fraction of the way will come back

1374
01:29:45,410 --> 01:29:51,900
so what you collecting with microphones and trailing behind the trailing boat is essentially the

1375
01:29:51,900 --> 01:29:54,010
pressure with that comes back

1376
01:29:54,040 --> 01:29:55,180
all right

1377
01:29:55,190 --> 01:29:55,980
and so

1378
01:29:55,990 --> 01:30:03,060
what the reflection seismic grammar seismic traces recording is the surface particle velocity which mathematically

1379
01:30:03,060 --> 01:30:08,810
is the derivative of this function you with respect to time at the surface

1380
01:30:08,830 --> 01:30:10,530
and this is

1381
01:30:10,550 --> 01:30:14,140
object that we call going to call HST

1382
01:30:14,140 --> 01:30:19,750
OK so i excised the system was the big hammer i created my way fifty

1383
01:30:19,780 --> 01:30:21,810
i measure the the response

1384
01:30:21,820 --> 01:30:26,160
which we're going to call each of t and the problems that seismologists face is

1385
01:30:26,160 --> 01:30:32,220
to find the coefficients to differential equation given what you put in and what the

1386
01:30:32,330 --> 01:30:33,950
call you get

1387
01:30:33,960 --> 01:30:34,740
right so

1388
01:30:34,760 --> 01:30:37,800
the exciting thing you you hear the echo

1389
01:30:37,810 --> 01:30:40,580
and out of the article you're trying to fear

1390
01:30:40,600 --> 01:30:43,820
the coefficients of p

1391
01:30:43,830 --> 01:30:45,980
that makes sense

1392
01:30:45,990 --> 01:30:49,260
lots of fact animals which uses

1393
01:30:49,270 --> 01:30:55,370
for example the bat who is very poor as very for visual system

1394
01:30:55,380 --> 01:30:59,330
the ways about inferiors geometry nick cave is vice

1395
01:30:59,390 --> 01:31:02,630
meaning church then bounce off the walls

1396
01:31:02,640 --> 01:31:07,470
and then about processes information to reconstruct the geometry of

1397
01:31:07,630 --> 01:31:08,710
of the room

1398
01:31:08,720 --> 01:31:13,250
and about so good at it that it can also crosses the reflection due to

1399
01:31:13,260 --> 01:31:18,980
kind of tiny insect that it needs to eat while in flight

1400
01:31:18,990 --> 01:31:22,490
OK so this is a small it is it is something that is a bit

1401
01:31:22,520 --> 01:31:25,430
similar to what that

1402
01:31:25,480 --> 01:31:27,750
the proba system and out of it

1403
01:31:27,780 --> 01:31:29,510
by collecting the echo

1404
01:31:29,520 --> 01:31:32,720
the figure out what the earth is made up

1405
01:31:34,000 --> 01:31:38,190
alright so the way people approached this problem is

1406
01:31:38,200 --> 01:31:44,010
it's in very non-linear problem to actually determine the impedance from

1407
01:31:44,040 --> 01:31:47,410
this input data and output data

1408
01:31:47,450 --> 01:31:50,310
this is a canadian ice problem to make it much simpler

1409
01:31:51,170 --> 01:31:52,230
OK so

1410
01:31:52,240 --> 01:31:56,410
the thing is that we can assume that the impedance is a constant plus

1411
01:31:56,480 --> 01:32:02,060
a tiny perturbation and then we go through our method that is similar in applied

1412
01:32:02,060 --> 01:32:09,010
maths we're gonna linearized equations by expressing the velocity field you as the field which

1413
01:32:09,010 --> 01:32:14,130
would correspond to a fixed impedance equal to one plus the tiny perturbation which is

1414
01:32:14,130 --> 01:32:16,220
due to this perturbation alpha

1415
01:32:16,230 --> 01:32:20,870
so you know is the solution to the wave problem with the constant medium and

1416
01:32:20,870 --> 01:32:23,120
you one is a small perturbations

1417
01:32:23,150 --> 01:32:24,210
induced by

1418
01:32:25,730 --> 01:32:27,650
all fuzzy

1419
01:32:27,910 --> 01:32:31,070
and so we know as the problem and i think i'm going to skip

1420
01:32:31,090 --> 01:32:35,650
the details so you just write down the linearized equation you you drop all the

1421
01:32:35,650 --> 01:32:40,500
terms that are quadratic in the perturbation you only keep the first order terms and

1422
01:32:40,500 --> 01:32:41,750
at the end of the day

1423
01:32:41,770 --> 01:32:46,960
you give you get a new problem which asks you to find this perturbation alpha

1424
01:32:46,960 --> 01:32:49,910
fuzzy from the input data

1425
01:32:50,960 --> 01:32:53,050
the output data which is now

1426
01:32:53,190 --> 01:32:55,710
what the echo plus what you put in

1427
01:32:55,730 --> 01:32:58,690
and this problem has a very simple form

1428
01:32:58,690 --> 01:33:00,070
because it's actually

1429
01:33:00,070 --> 01:33:03,900
in isomap the distances are the shortest distance between two points in the graph so

1430
01:33:03,900 --> 01:33:05,460
that's sort of how they are related

1431
01:33:05,900 --> 01:33:07,020
but i'm short on time

1432
01:33:09,420 --> 01:33:11,420
i think we've seen how laplacian i can maps

1433
01:33:12,500 --> 01:33:14,880
is related which is not fitting the parameters the class

1434
01:33:15,570 --> 01:33:16,690
but the cool thing is

1435
01:33:17,210 --> 01:33:21,030
how locally linear embedding is related and this is the last thing i really wanna

1436
01:33:22,030 --> 01:33:22,960
spend a little time on

1437
01:33:24,480 --> 01:33:27,300
so the question needs to be constrained positive definite

1438
01:33:28,000 --> 01:33:32,480
that's not always easy to do with this is my favorite trick for constraining a matrix positive definite

1439
01:33:33,630 --> 01:33:35,360
just write in the form two

1440
01:33:36,280 --> 01:33:40,260
other mixes hear this is a low rank this is in my head but what

1441
01:33:40,260 --> 01:33:44,260
it could be is sparse and what we also can do is in order constraint

1442
01:33:44,360 --> 01:33:47,760
develop and we can force em transpose one to be zero

1443
01:33:49,650 --> 01:33:54,800
so this is just a way of constraining the any u model so that the question is positive definite

1444
01:33:56,940 --> 01:33:57,780
and once we've done there

1445
01:33:59,110 --> 01:34:02,900
we end up with this result the diagonal must equal the sum of the off-diagonals

1446
01:34:04,630 --> 01:34:09,570
now locally linear embeddings then specific case the maximum entropy unfolding with percival

1447
01:34:10,150 --> 01:34:13,550
you constrain the diagonal sums am i i to one

1448
01:34:14,440 --> 01:34:18,320
remember summer w was gonna receptor one instead allowing it very

1449
01:34:19,360 --> 01:34:23,840
hand you find the model parameters by maximizing the pseudo likelihood of the data not

1450
01:34:23,840 --> 01:34:29,760
the real likelihood that's typically a fast way fitting calcium random field in fact pseudolikelihood

1451
01:34:29,760 --> 01:34:34,900
originally proposed by people work in spatial statistics with fitting calcium random fields

1452
01:34:38,170 --> 01:34:39,110
the proof about

1453
01:34:39,860 --> 01:34:44,980
quickly sketched is that fry unit diagonals we have enemies i minus w that's the

1454
01:34:44,980 --> 01:34:46,980
form that you get in locally linear embedding

1455
01:34:48,130 --> 01:34:51,420
and the off-diagonal sparsity w matches end so you end up

1456
01:34:52,500 --> 01:34:53,380
without being true

1457
01:34:53,820 --> 01:34:58,590
which is true in early as well and says that you should look for the smallest igon

1458
01:34:59,110 --> 01:35:00,520
vectors of this guy

1459
01:35:01,960 --> 01:35:03,500
which is equal to atlassian

1460
01:35:04,150 --> 01:35:06,130
so that's exactly the same as what we suggest

1461
01:35:07,340 --> 01:35:08,820
the classical multidimensional scaling

1462
01:35:10,440 --> 01:35:13,130
and it's like i can i laplacian i can maps as well

1463
01:35:14,250 --> 01:35:17,360
the other portion is the pseudolikelihood approximation

1464
01:35:19,210 --> 01:35:20,550
and that's says that these

1465
01:35:20,900 --> 01:35:26,940
joint distribution is approximately equal to the product of the conditional distributions in the graph

1466
01:35:27,630 --> 01:35:28,940
so these are all the conditionals

1467
01:35:29,550 --> 01:35:33,820
now the true likelihood is proportional to the but it's got normalization which is the

1468
01:35:34,110 --> 01:35:36,550
in the gas in case the determinant of the covariance

1469
01:35:37,440 --> 01:35:40,090
and pseudolikelihood normalisation is ignored

1470
01:35:41,210 --> 01:35:43,440
this is the pseudo likelihood in this model

1471
01:35:44,460 --> 01:35:49,500
now if i if you cross out the and look at the logo this that's exactly the objective

1472
01:35:50,960 --> 01:35:52,070
locally linear embeddings

1473
01:35:52,610 --> 01:35:54,250
the new there is this term here

1474
01:35:56,190 --> 01:36:01,340
doesn't appear locally linear embeddings and that's the precision of each of these calcium fits

1475
01:36:03,320 --> 01:36:06,610
so locally linear embedding fits them with precision one

1476
01:36:08,070 --> 01:36:12,250
basically pre describes what should be hear back comes out of the mass as well

1477
01:36:14,980 --> 01:36:15,380
okay so

1478
01:36:15,920 --> 01:36:19,500
alleles approximation to maximum likelihood in this new model i show

1479
01:36:20,320 --> 01:36:23,960
i hands but i mean important point about allele

1480
01:36:24,520 --> 01:36:29,210
it is actually the sparsity pattern because the sparsity is done in w not in

1481
01:36:29,210 --> 01:36:33,980
the overall matrix you don't get the same neighborhood graph in the final graph laplacian

1482
01:36:34,210 --> 01:36:38,630
and in fact is entropy hard to work out what the sparsity pattern in w

1483
01:36:38,630 --> 01:36:39,070
should be

1484
01:36:39,530 --> 01:36:41,960
in order to get the desired sparsity pattern in al

1485
01:36:42,670 --> 01:36:43,860
which is kind of annoying

1486
01:36:46,230 --> 01:36:48,320
so you always end up with slightly different neighborhoods

1487
01:36:49,150 --> 01:36:49,690
so here's the

1488
01:36:50,230 --> 01:36:51,940
point which really illustrates

1489
01:36:53,590 --> 01:36:55,420
there's something funny was going on in early

1490
01:36:56,260 --> 01:36:57,900
which turned out to be the pseudo likelihood

1491
01:36:59,000 --> 01:37:02,070
so locally linear embedding is motivated by considering

1492
01:37:02,710 --> 01:37:07,460
locally linear embeddings the data but interestingly as we increase the neighborhood size k eh

1493
01:37:07,570 --> 01:37:10,400
ten minus one we can recover piece you eh

1494
01:37:11,650 --> 01:37:15,500
so if you're going from a local to a global linear embedding you would expect

1495
01:37:15,500 --> 01:37:17,760
a cover piece e eight because its optimal

1496
01:37:18,590 --> 01:37:19,960
but in lowell you don't

1497
01:37:19,960 --> 01:37:23,670
between compression artifacts right

1498
01:37:23,870 --> 01:37:25,980
so by the

1499
01:37:26,060 --> 01:37:31,560
the relative height of the speaker from this week you can learn about the relative

1500
01:37:32,210 --> 01:37:37,100
importance of body of course called that matter to baryonic matter

1501
01:37:37,160 --> 01:37:40,930
so there's a lot of you can learn from the detailed statistical properties of this

1502
01:37:42,190 --> 01:37:45,310
and so this is an animation

1503
01:37:45,320 --> 01:37:51,070
tools see for example how you can determine the geometry

1504
01:37:51,110 --> 01:37:55,590
so you have a standard ruler there and if the geometry of the universe is

1505
01:37:55,590 --> 01:37:58,470
flat it would appear to you

1506
01:37:58,490 --> 01:38:00,710
on the map and

1507
01:38:00,790 --> 01:38:01,860
given angle

1508
01:38:01,870 --> 01:38:04,750
but if the geometry is not flat

1509
01:38:04,780 --> 01:38:05,980
then c

1510
01:38:05,990 --> 01:38:10,470
it's possible to occur then this bot would appear to be good and if it's

1511
01:38:10,480 --> 01:38:14,440
negative because this spot would appear to small

1512
01:38:14,490 --> 01:38:19,460
so you go into the measurement and when it turns out that

1513
01:38:19,480 --> 01:38:21,940
sports are of the right side

1514
01:38:22,130 --> 01:38:25,390
the universe is flat

1515
01:38:25,420 --> 01:38:28,630
good so we started off from here

1516
01:38:29,940 --> 01:38:31,890
o point seven billion years ago or so

1517
01:38:31,910 --> 01:38:35,960
and we end up here how did you get from here to there

1518
01:38:35,980 --> 01:38:41,340
so to do that is the subject of the next class which

1519
01:38:41,390 --> 01:38:59,010
i have to go and the out

1520
01:38:59,030 --> 01:38:59,780
and so

1521
01:38:59,790 --> 01:39:01,690
we've seen this before

1522
01:39:01,700 --> 01:39:04,260
one concert they will need now

1523
01:39:04,290 --> 01:39:05,820
is that the

1524
01:39:06,580 --> 01:39:10,940
if you have a decelerated expansion which is what you have when you have a

1525
01:39:10,940 --> 01:39:14,010
universe that is dominated by martin remember

1526
01:39:14,030 --> 01:39:15,140
this class

1527
01:39:15,160 --> 01:39:16,710
dominated by matter

1528
01:39:16,710 --> 01:39:20,240
a acceleration equation a double that is negative

1529
01:39:20,330 --> 01:39:23,440
then there is an expand

1530
01:39:23,480 --> 01:39:25,570
and you can see it in this animation

1531
01:39:25,590 --> 01:39:27,320
so there is a dog

1532
01:39:27,330 --> 01:39:32,710
that sounds like polls which is represented by the green here and there is another

1533
01:39:32,710 --> 01:39:40,210
dog and the pattern that expands present the space that is expanding and this expansions

1534
01:39:40,210 --> 01:39:42,250
is decelerating

1535
01:39:42,280 --> 01:39:46,430
this function is decelerating you would see that at the beginning of animation the second

1536
01:39:46,430 --> 01:39:51,410
step outside of that called and by the end of the animation dating site that's

1537
01:39:51,410 --> 01:39:56,550
it could represent a eyes and right because it's fast any kind of information light

1538
01:39:56,550 --> 01:40:01,200
can travel and so what we have learned that in decelerating expansion the size of

1539
01:40:01,200 --> 01:40:02,470
the horizon

1540
01:40:03,980 --> 01:40:09,130
so you can get things that were outside your eyes and entering your right

1541
01:40:11,760 --> 01:40:19,980
we've seen this before and i'm not going to repeat sole

1542
01:40:19,990 --> 01:40:25,610
today we have an extremely successful cosmological model think about the friedmann equation we can

1543
01:40:25,610 --> 01:40:30,660
use a few parameters remember omega martyr automata body omega called that matter

1544
01:40:30,670 --> 01:40:34,260
the amount of curvature radiation

1545
01:40:34,340 --> 01:40:39,230
the cosmological constant this equation of state which if is the cosmological constant is minus

1546
01:40:39,230 --> 01:40:42,690
one if not to be something else and they have all around

1547
01:40:42,730 --> 01:40:44,830
so these few parameters

1548
01:40:44,860 --> 01:40:50,210
describe the background evolution the expansion history of the union

1549
01:40:50,860 --> 01:40:55,990
now we've added fluctuations so we may need a couple of more parameters to describe

1550
01:40:55,990 --> 01:41:01,500
also the prose statistical properties of those perturbations and that these two parameters were met

1551
01:41:01,500 --> 01:41:06,630
them before one is the primordial power spectrum which is mostly like a power law

1552
01:41:06,680 --> 01:41:09,990
and amplitude and aspect personal

1553
01:41:10,000 --> 01:41:13,210
with this parameters you go everywhere

1554
01:41:13,220 --> 01:41:17,810
the name of the game is to determine these parameters and for that to be

1555
01:41:17,820 --> 01:41:19,190
people are investing

1556
01:41:19,210 --> 01:41:21,290
billions of dollars in expenses

1557
01:41:21,310 --> 01:41:25,640
OK so you start off from here you want to end up with something like

1558
01:41:25,640 --> 01:41:28,580
that this is actually real data is the simulation

1559
01:41:28,630 --> 01:41:33,300
you just put this kind of initial conditions in computer you give the computer the

1560
01:41:33,300 --> 01:41:38,980
kind of composition of baryonic dark matter except except that and let it evolve under

1561
01:41:38,980 --> 01:41:43,460
gravity in the computer will tell you that statistically matter will be distributed in this

1562
01:41:43,460 --> 01:41:47,980
way and galaxies formed their x x

1563
01:41:49,230 --> 01:41:53,200
how the structure formation work and what we can we learn from it so let's

1564
01:41:53,200 --> 01:41:55,430
imagine did you start off

1565
01:41:55,440 --> 01:42:00,210
the morning where you have the surface of the sea the sea is very calm

1566
01:42:00,330 --> 01:42:04,760
there are only very tiny ripples on the surface of the sea

1567
01:42:04,860 --> 01:42:08,940
so this is your universe as you see in the cosmic rays back

1568
01:42:08,980 --> 01:42:12,870
you switch on gravity that is the wind picks up

1569
01:42:12,880 --> 01:42:15,450
and the the day goes on

1570
01:42:15,450 --> 01:42:18,350
and the fluctuation start growing

1571
01:42:18,380 --> 01:42:24,530
there growing start growing until they become so nonlinear to conserve them

1572
01:42:24,580 --> 01:42:29,410
the same kind of thing happened with cosmological perturbation you start off with small perturbation

1573
01:42:29,550 --> 01:42:32,970
you see them in the cosmic microwave background they grow on the gravity until they

1574
01:42:32,970 --> 01:42:38,090
become galaxies and planets except accepts and we want to write down the equations but

1575
01:42:38,090 --> 01:42:41,110
they are very similar equation for waves as

1576
01:42:41,170 --> 01:42:44,700
they are four perturbations cosmological perturbation observations

1577
01:42:45,890 --> 01:42:51,550
there is an impressive agreement between the data and this handful of parameters and physics

1578
01:42:51,550 --> 01:42:55,060
that is based on a couple of friends of equation that we

1579
01:42:55,060 --> 01:42:57,230
the basic colour

1580
01:42:57,280 --> 01:42:58,390
so this

1581
01:42:58,490 --> 01:43:04,300
the data and the theory and this again for cosmic background in this for the

1582
01:43:04,300 --> 01:43:09,400
class and property the power spectrum of the larger-scale structure this the data this is

1583
01:43:09,430 --> 01:43:10,730
a game theory

1584
01:43:10,900 --> 01:43:15,090
and the interesting thing here is because this is very new and hard to so

1585
01:43:15,100 --> 01:43:19,920
i'm going to mention is that this theory look i got some small we got

1586
01:43:19,920 --> 01:43:24,590
here and this is more we're going remind you a lot of this week

1587
01:43:24,640 --> 01:43:28,520
what do they come from this advanced topic but just because it's hard going to

1588
01:43:28,520 --> 01:43:29,310
tell you

1589
01:43:29,320 --> 01:43:33,680
so remember that they told you there is this dark matter distribution in the body

1590
01:43:33,680 --> 01:43:36,920
of the when they get released from the fort and they see another density of

1591
01:43:36,920 --> 01:43:41,220
dark matter in this world labour to fall into the because and i feel gravity

1592
01:43:41,260 --> 01:43:46,420
but if the body one-seventh of the amount of dark matter and matter must also

1593
01:43:46,420 --> 01:43:50,270
thing the same right that matter is sitting there and there is a little bit

1594
01:43:50,270 --> 01:43:52,740
is what does that mean what i just means that

1595
01:43:52,820 --> 01:43:54,190
on average

1596
01:43:54,200 --> 01:43:57,890
over all the possible function on average

1597
01:43:57,940 --> 01:44:02,880
independent of x i haven't i mean mean value of zero

1598
01:44:02,890 --> 01:44:06,860
and then you can sort of see that just by symmetry arguments as many functions

1599
01:44:06,870 --> 01:44:10,400
positive for some particular argument if they are negative

1600
01:44:10,410 --> 01:44:13,180
on average over this distribution functions

1601
01:44:13,230 --> 01:44:16,650
never going to be the new function is going to fail

1602
01:44:16,660 --> 01:44:19,070
all right so what about the grants function

1603
01:44:19,130 --> 01:44:20,780
again we just do the same thing

1604
01:44:20,830 --> 01:44:21,980
just plugin

1605
01:44:21,990 --> 01:44:26,250
what is the covariance of the covariance the definition of koreans expectations

1606
01:44:27,070 --> 01:44:30,370
the point x

1607
01:44:30,410 --> 01:44:33,120
minus its mean which is zero

1608
01:44:34,750 --> 01:44:36,830
the value at different points in time

1609
01:44:36,880 --> 01:44:39,340
and again at next prime and out

1610
01:44:39,430 --> 01:44:43,570
big promises in expectations or anything

1611
01:44:44,030 --> 01:44:45,120
plug that in

1612
01:44:45,400 --> 01:44:46,350
we get this

1613
01:44:46,360 --> 01:44:49,270
the interval of talking to me

1614
01:44:50,410 --> 01:44:53,630
we can pull these apart into three different

1615
01:44:56,750 --> 01:44:57,640
so here we can

1616
01:44:57,660 --> 01:45:00,980
well i expect prime they don't have to do with a and then we just

1617
01:45:00,980 --> 01:45:01,710
get the

1618
01:45:01,790 --> 01:45:07,080
the integral is where it which is the variance of a which alpha

1619
01:45:08,190 --> 01:45:09,480
get it over here

1620
01:45:12,870 --> 01:45:15,710
that means that this

1621
01:45:15,720 --> 01:45:19,800
the distribution over functions that we find out here

1622
01:45:19,850 --> 01:45:24,430
is aghast process with the mean function of zero and the covariance function which is

1623
01:45:24,430 --> 01:45:27,760
given by the i'm peter

1624
01:45:28,730 --> 01:45:32,460
this is a very complicated way of doing you know model

1625
01:45:32,510 --> 01:45:34,800
you can write them down process the

1626
01:45:36,880 --> 01:45:40,770
and two in front in the corresponding out

1627
01:45:40,910 --> 01:45:45,210
OK so this is this is a slightly more examples like that but two and

1628
01:45:45,960 --> 01:45:48,400
a more interesting example

1629
01:45:48,420 --> 01:45:49,170
so here's

1630
01:45:49,220 --> 01:45:51,250
it's more interesting

1631
01:45:51,300 --> 01:45:52,920
now let's consider

1632
01:45:53,510 --> 01:45:56,890
the function again

1633
01:45:56,940 --> 01:45:58,210
which is going to be

1634
01:45:58,260 --> 01:46:00,170
just a linear combination

1635
01:46:00,990 --> 01:46:03,440
a bunch of little girls in

1636
01:46:04,380 --> 01:46:07,220
so i have to find some girls involved the

1637
01:46:07,230 --> 01:46:12,880
a little beta minus the square between x and this variable

1638
01:46:12,890 --> 01:46:14,730
i over and

1639
01:46:14,780 --> 01:46:20,170
five little else in the next and have the coefficient here the coefficient is going

1640
01:46:20,170 --> 01:46:21,620
to be a random variable

1641
01:46:22,270 --> 01:46:23,380
the gamma i

1642
01:46:24,360 --> 01:46:26,780
i just

1643
01:46:26,800 --> 01:46:30,520
this debate according to the gulf distribution

1644
01:46:30,570 --> 01:46:32,620
have these little

1645
01:46:32,630 --> 01:46:37,940
the coefficient times these little girl from

1646
01:46:37,990 --> 01:46:40,940
now it's some up a bunch of these things and then i look at what

1647
01:46:40,940 --> 01:46:42,160
happened in the limit

1648
01:46:42,210 --> 01:46:45,220
where have an infinite number of

1649
01:46:45,460 --> 01:46:48,870
but that the centre of everywhere

1650
01:46:51,650 --> 01:46:52,780
don't worry

1651
01:46:52,790 --> 01:46:58,850
too much about them and maybe they could have done a little bit more serious

1652
01:46:58,870 --> 01:47:02,180
so let's look at what happens in the limit

1653
01:47:02,220 --> 01:47:05,120
well we have a limit of one over and times the

1654
01:47:05,160 --> 01:47:07,030
and some of the things that

1655
01:47:07,040 --> 01:47:10,170
i mean it turns into an interval

1656
01:47:10,220 --> 01:47:12,110
we have now

1657
01:47:12,160 --> 01:47:13,230
and now i right

1658
01:47:13,280 --> 01:47:16,700
gamma i here just the function here

1659
01:47:16,710 --> 01:47:20,190
gamma i gamma zero function of

1660
01:47:20,370 --> 01:47:23,900
you promised

1661
01:47:23,950 --> 01:47:24,770
and the

1662
01:47:24,780 --> 01:47:27,110
and gamma use are all independent

1663
01:47:27,120 --> 01:47:30,360
but the normal year one distribution

1664
01:47:30,410 --> 01:47:33,040
that means the gamma sigma here is

1665
01:47:33,890 --> 01:47:35,530
white noise

1666
01:47:35,580 --> 01:47:37,940
all the gamma completely independent of each other

1667
01:47:37,990 --> 01:47:39,940
OK and you have an infinite number of them

1668
01:47:39,990 --> 01:47:42,610
and they sit in front of these

1669
01:47:42,810 --> 01:47:45,300
gaston but they're all

1670
01:47:45,310 --> 01:47:47,760
on the corresponding value you

1671
01:47:49,730 --> 01:47:54,590
don't think of this is being sort of the limit the limiting behavior of

1672
01:47:54,600 --> 01:47:56,560
regression model where you have

1673
01:47:56,610 --> 01:47:59,620
a bunch of galveston but you see what happens when

1674
01:48:00,570 --> 01:48:04,370
i think you get more and more from the pond and and

1675
01:48:05,510 --> 01:48:10,040
and then we have to normalized by and you want to go and explode

1676
01:48:10,240 --> 01:48:17,160
this is this defined some distribution over functions and it's going to be a process

1677
01:48:17,200 --> 01:48:20,630
because the coefficient

1678
01:48:20,680 --> 01:48:24,030
i was trying to work out again the mean function

1679
01:48:24,380 --> 01:48:26,230
and the covariance function

1680
01:48:26,380 --> 01:48:27,650
two again to buy

1681
01:48:27,700 --> 01:48:28,960
just plugging in

1682
01:48:30,100 --> 01:48:31,310
the question here

1683
01:48:32,350 --> 01:48:33,310
but the

1684
01:48:33,320 --> 01:48:35,790
i mean function it station

1685
01:48:35,840 --> 01:48:41,160
overall the way that use

1686
01:48:42,400 --> 01:48:45,410
so you can pull the the that problem

1687
01:48:45,460 --> 01:48:50,220
indigo went into the thing here can you can separate the two intervals and the

1688
01:48:50,220 --> 01:48:51,400
last thing to go here

1689
01:48:51,780 --> 01:48:53,600
you know

1690
01:48:53,680 --> 01:48:56,260
just because of again

1691
01:48:56,330 --> 01:48:58,420
the mean function is zero

1692
01:48:58,430 --> 01:49:00,710
OK there is no great surprise by the

1693
01:49:00,720 --> 01:49:02,990
the coefficients that we multiply the thing with

1694
01:49:03,000 --> 01:49:07,850
as likely to be positive and negative one ever going to have

1695
01:49:08,500 --> 01:49:12,090
and then we can do the same thing for the covariance function

1696
01:49:12,140 --> 01:49:14,640
but again i have

1697
01:49:14,690 --> 01:49:18,710
i've written up here and the simple

1698
01:49:21,030 --> 01:49:22,920
the worry about exactly how to do it

1699
01:49:23,020 --> 01:49:26,570
but look at the results instead of that

1700
01:49:26,570 --> 01:49:29,850
i think actually on the tags

1701
01:49:29,910 --> 01:49:34,140
we had in the flickr dataset and this is the distribution that you get for

1702
01:49:34,150 --> 01:49:37,420
the social correlation coefficient out

1703
01:49:37,460 --> 01:49:43,340
and you see is its largest considerably larger something like the peak is around between

1704
01:49:43,340 --> 01:49:44,370
one and two

1705
01:49:44,390 --> 01:49:47,220
this this is the city

1706
01:49:47,260 --> 01:49:51,930
so there is definitely some element of social class social correlation if you have a

1707
01:49:51,930 --> 01:49:53,820
friend that using one tag

1708
01:49:53,840 --> 01:49:57,730
if any more likely to use the

1709
01:49:59,720 --> 01:50:02,130
in the question who distinguish

1710
01:50:02,150 --> 01:50:08,340
this element of social influence from other types of other types of social correlation

1711
01:50:08,350 --> 01:50:10,490
remember that we had a graph g

1712
01:50:10,490 --> 01:50:13,510
and we denote the set of nodes that are active at the end of the

1713
01:50:13,510 --> 01:50:14,930
observation by

1714
01:50:16,270 --> 01:50:21,200
now i want to somehow model but the model for non influence models

1715
01:50:21,700 --> 01:50:26,990
remember that there are two different types of

1716
01:50:26,990 --> 01:50:32,530
car caused by elements that influence one must lawfully which was basically the idea was

1717
01:50:32,530 --> 01:50:38,130
that the set w of active nodes are people first and then the graph g

1718
01:50:38,160 --> 01:50:41,930
comes from a distribution that depends on the this is like a pure form of

1719
01:50:44,330 --> 01:50:49,680
i confounding factors think that officially bought this graph g and also the w

1720
01:50:49,700 --> 01:50:54,490
are picked from distributions that depend on another and the value another of unobserved and

1721
01:50:54,580 --> 01:50:57,720
my vote which is denoted by x

1722
01:50:57,760 --> 01:51:06,050
so more generally we consider this correlation model that encapsulate for some of the day

1723
01:51:06,050 --> 01:51:07,990
we assume that this graph g

1724
01:51:07,990 --> 01:51:12,390
and the set w coming from a joint distribution

1725
01:51:12,430 --> 01:51:16,660
so they both can depend on each other in number of ways but important assumption

1726
01:51:16,660 --> 01:51:17,950
here is that

1727
01:51:18,470 --> 01:51:21,640
after we know which agents are likely to be active

1728
01:51:21,680 --> 01:51:26,930
the time at which they become active actually i escaped from some distribution

1729
01:51:26,990 --> 01:51:29,240
and zero and t

1730
01:51:29,970 --> 01:51:32,720
independent and i call

1731
01:51:32,720 --> 01:51:37,450
that's that's the main assumption everything basically the

1732
01:51:38,390 --> 01:51:44,760
that beauty the activation time for it in the

1733
01:51:44,780 --> 01:51:48,720
it's so now

1734
01:51:48,740 --> 01:51:52,160
that that actually gives an idea for testing

1735
01:51:52,200 --> 01:51:55,890
the influence like the point is that even though

1736
01:51:55,950 --> 01:52:00,890
an agent's probability of becoming active can depend on friends but the timing is that

1737
01:52:00,890 --> 01:52:02,600
it's not going to be

1738
01:52:02,600 --> 01:52:05,910
so the idea is that if you have two friends and if they are both

1739
01:52:05,910 --> 01:52:08,450
influenced by some outside factor

1740
01:52:08,490 --> 01:52:12,450
that causes them to become active

1741
01:52:12,490 --> 01:52:15,350
the probability that a becomes active before b

1742
01:52:15,430 --> 01:52:18,700
is the same as if he becomes active before

1743
01:52:18,740 --> 01:52:23,740
whereas if you have a social influence phenomena you expect this thing to essentially spread

1744
01:52:23,740 --> 01:52:28,070
throughout this is going to see the pattern

1745
01:52:30,080 --> 01:52:33,780
that's that's the intuition behind this is that trying to run

1746
01:52:34,260 --> 01:52:38,370
basically we are going to shuffle the time stamp of all actions

1747
01:52:38,410 --> 01:52:43,870
trying to keep everything the same essentially especially noting that if the social network is

1748
01:52:43,870 --> 01:52:48,370
going over find the area that you're observing it

1749
01:52:48,370 --> 01:52:51,970
we're going to try to keep everything the same so so that the number of

1750
01:52:51,970 --> 01:52:56,970
people to become active in this time of saying but you're shopping

1751
01:52:56,990 --> 01:53:02,870
and then after shuffle them going to estimate the social correlation coefficient alpha again

1752
01:53:02,890 --> 01:53:04,760
compared to the original one

1753
01:53:04,800 --> 01:53:08,760
and the idea is that if this if these two coefficients that you get a

1754
01:53:08,810 --> 01:53:14,450
not very different that's indicative of social influence not

1755
01:53:14,450 --> 01:53:18,640
the other way around if the if they are different social influence is the likely

1756
01:53:18,640 --> 01:53:19,640
factor but

1757
01:53:19,640 --> 01:53:24,620
obviously we can show

1758
01:53:24,640 --> 01:53:29,030
OK so and the other thing that i'm going to show some simulation results on

1759
01:53:29,030 --> 01:53:31,870
the edge of what is basically it's

1760
01:53:31,930 --> 01:53:37,700
it's from the obesity study the idea is that if you switch to direction of

1761
01:53:38,270 --> 01:53:42,160
if the model is coming only from the social correlation and not implies that should

1762
01:53:42,160 --> 01:53:50,740
not change the value of that you get the social correlation coefficient which

1763
01:53:50,870 --> 01:53:54,050
so now let me give you

1764
01:53:54,180 --> 01:53:59,850
because justifications for us what we can prove that if you actually looking at one

1765
01:53:59,850 --> 01:54:00,800
of these

1766
01:54:00,810 --> 01:54:03,260
models of correlation

1767
01:54:03,260 --> 01:54:05,510
that does not include influence

1768
01:54:05,550 --> 01:54:11,050
if the graph is large enough time shuffle test is going to allow the rule

1769
01:54:11,050 --> 01:54:13,830
out influences the likely factor in this model

1770
01:54:13,850 --> 01:54:18,680
the intuition is clear the point is that in the current model that defines

1771
01:54:18,680 --> 01:54:20,370
the time stamps or not

1772
01:54:20,370 --> 01:54:25,220
depend each other and therefore the distribution and again after shuffling is the same as

1773
01:54:25,280 --> 01:54:32,350
distribution that had initially the thing that's more challenging the goal is to prove concentration

1774
01:54:32,350 --> 01:54:36,280
the fact that we have the good ones that look this why this is same

1775
01:54:36,280 --> 01:54:37,950
as the original one

1776
01:54:38,010 --> 01:54:41,720
but the idea of the proof i'm not going to pull the idea is to

1777
01:54:41,720 --> 01:54:46,830
use martingale inequality in particular what quality to show that y and they are actually

1778
01:54:48,280 --> 01:54:51,720
and then all of these probabilities are all that

1779
01:54:52,080 --> 01:54:53,930
the shuffling of times

1780
01:54:53,950 --> 01:54:57,950
and then after we showed us the news that lamont dimension to show that the

1781
01:54:57,950 --> 01:55:03,550
maximum likelihood estimator continuous function as the result of should change very which

1782
01:55:03,550 --> 01:55:07,510
and the ticket actually the second part of

1783
01:55:07,510 --> 01:55:12,580
OK so let me

1784
01:55:14,550 --> 01:55:19,830
they're from the test on the number of randomly generated instances

1785
01:55:19,830 --> 01:55:22,430
and also you work for

1786
01:55:22,430 --> 01:55:27,510
all of the randomly generated inferences are based on the data from the flickr for

1787
01:55:27,530 --> 01:55:33,470
keeping the social network because constant and you regenerate their tagging data the action

1788
01:55:33,510 --> 01:55:38,960
according to one of these models the baseline models no correlation model basically to ignore

1789
01:55:38,960 --> 01:55:42,450
it and that's what we're generating the tagging events

1790
01:55:42,470 --> 01:55:47,350
according to the distribution of matter one of the facts but essentially the identities of

1791
01:55:48,200 --> 01:55:51,220
at random at the top the network

1792
01:55:51,220 --> 01:55:55,010
another model the influence model which is basically the same as the one that they

1793
01:55:55,050 --> 01:55:59,490
described in every time step each person contact with some probability

1794
01:55:59,570 --> 01:56:04,870
that's given by that function p of a and the variety of parameters is used

1795
01:56:06,070 --> 01:56:08,300
and finally we have the correlation models

1796
01:56:08,350 --> 01:56:12,580
which is the first american like this we can about

1797
01:56:12,600 --> 01:56:15,810
and the centre in this network in the social network

1798
01:56:15,870 --> 01:56:20,470
and let w be the union of balls of radius two around these centers

1799
01:56:21,030 --> 01:56:23,220
there is an effect to here is that

1800
01:56:23,240 --> 01:56:26,410
graph is actually very well connected so if you go further than two

1801
01:56:26,430 --> 01:56:28,600
each ball is going to be very large

1802
01:56:28,620 --> 01:56:31,680
and if you go below two it's going to be just one person and their

1803
01:56:31,680 --> 01:56:37,350
neighbours to tools into the right and notice that so this model essentially the set

1804
01:56:37,350 --> 01:56:43,600
w going to have exhibits some level of correlation because people are within this w

1805
01:56:43,600 --> 01:56:46,870
are much more likely to be friends

1806
01:56:48,530 --> 01:56:52,430
yes so have to repeat this w then after that you're making the

1807
01:56:52,430 --> 01:56:56,810
time stamps in the same way that the data based on model basically looking following

1808
01:56:56,810 --> 01:57:00,530
one of the tags on the radio later the pattern of growth of that tag

1809
01:57:00,640 --> 01:57:06,330
but the time that are picked independently

1810
01:57:06,330 --> 01:57:08,460
it ideas and grab sharks

1811
01:57:08,480 --> 01:57:12,960
two different so aggregate fluctuations

1812
01:57:13,060 --> 01:57:17,350
the result of the summation of individual fluctuations

1813
01:57:18,670 --> 01:57:19,960
of course

1814
01:57:19,980 --> 01:57:29,710
i don't know why we have this this this stuff you of course supposing that

1815
01:57:29,710 --> 01:57:32,290
it's for me is sufficiently small

1816
01:57:32,350 --> 01:57:39,390
so that each each firm accounts for one over and over the total

1817
01:57:39,410 --> 01:57:42,390
and sharks find violence

1818
01:57:42,440 --> 01:57:45,350
then the large number applies

1819
01:57:45,750 --> 01:57:47,670
just to give you an example

1820
01:57:47,690 --> 01:57:54,370
supposing that he has four hundred fifty sector of which is basically the number of

1821
01:57:54,370 --> 01:58:01,120
sectors the four-digit level for the united states and its sector is characterized by volatility

1822
01:58:01,120 --> 01:58:04,270
of terms of standard deviation

1823
01:58:04,500 --> 01:58:06,270
six percent

1824
01:58:06,290 --> 01:58:12,460
then we should end up with the volatility is equal to the point fifteen percent

1825
01:58:12,480 --> 01:58:14,270
which is why

1826
01:58:14,290 --> 01:58:21,770
order of magnitude lower than what we observe in real data for the activity to

1827
01:58:22,520 --> 01:58:24,040
while these

1828
01:58:24,060 --> 01:58:27,430
it is precisely the the average

1829
01:58:27,440 --> 01:58:30,620
of the value for the four hundred sixty six

1830
01:58:30,710 --> 01:58:33,120
OK so this problem

1831
01:58:36,410 --> 01:58:38,830
possible answers given by by a

1832
01:58:38,850 --> 01:58:40,330
i bet

1833
01:58:43,500 --> 01:58:49,080
you also want to appeal to possible answer is given by kebab

1834
01:58:49,080 --> 01:58:51,750
just slow down their applications the

1835
01:58:51,790 --> 01:58:56,810
of the rule number by assuming that firms are followed distributed

1836
01:58:56,830 --> 01:59:03,890
but even the firms are not power law distributed we can have an aggregate volatility

1837
01:59:03,910 --> 01:59:09,230
in line with what we observe a real that if shocks are relatively stable distributed

1838
01:59:09,230 --> 01:59:12,000
with infinite variance

1839
01:59:12,000 --> 01:59:22,730
why because by the property of under convolution we have this result

1840
01:59:22,750 --> 01:59:25,640
very briefly this means

1841
01:59:25,670 --> 01:59:31,330
that as soon as and the number of firms so the number of sectors affected

1842
01:59:31,330 --> 01:59:40,390
by IID sharks levy stable distributed with infinite variance increases

1843
01:59:41,730 --> 01:59:47,410
aggregate fluctuations decays with them

1844
01:59:47,410 --> 01:59:51,540
at this rate

1845
01:59:51,600 --> 01:59:56,440
this rate which is much lower than than the then the rate for the standalone

1846
01:59:58,620 --> 02:00:01,160
so it it's it's not

1847
02:00:01,160 --> 02:00:07,120
necessary to introduce the assumption that from power law distributed in terms of size

1848
02:00:07,160 --> 02:00:11,580
we can have a really competitive market

1849
02:00:11,620 --> 02:00:18,540
which firms accounting for a negligible amount of the total but it suffices that they

1850
02:00:18,540 --> 02:00:25,120
are affected by latest stable distributed shocks to the opportunity to have fluctuations which are

1851
02:00:25,120 --> 02:00:28,310
in line with what we observe real

1852
02:00:28,330 --> 02:00:30,140
this is the main results

1853
02:00:31,270 --> 02:00:36,850
another possible application

1854
02:00:36,910 --> 02:00:40,440
tell me when i'm going out of time

1855
02:00:40,460 --> 02:00:44,730
well how much

1856
02:00:44,750 --> 02:00:46,460
ten minutes

1857
02:00:46,480 --> 02:00:49,350
OK maybe fifteen minutes for for

1858
02:00:49,500 --> 02:00:54,350
and here another possible example

1859
02:00:54,370 --> 02:00:57,890
of application i'm going to go

1860
02:00:57,890 --> 02:01:04,230
very very quick about this is about diversification diversification is of course quite well known

1861
02:01:05,410 --> 02:01:10,540
you know it's not better to put all your eggs in the same on back

1862
02:01:10,540 --> 02:01:11,770
basically k

1863
02:01:13,560 --> 02:01:18,520
what's what's the problem suppose that we are measuring the value of the portfolio on

1864
02:01:18,520 --> 02:01:23,600
the occasion and these are the number of tasks in the portfolio

1865
02:01:23,600 --> 02:01:26,160
understand the assumptions

1866
02:01:26,210 --> 02:01:28,140
densification works

1867
02:01:28,190 --> 02:01:33,580
so as we increase the number of items in inside our portfolio the value of

1868
02:01:33,580 --> 02:01:39,140
the portfolio tend to increases which means that the risk of the total for you

1869
02:01:39,160 --> 02:01:41,000
was that

1870
02:01:41,020 --> 02:01:41,930
if we are

1871
02:01:41,930 --> 02:01:43,210
that's a

1872
02:01:43,230 --> 02:01:46,770
we are able to to to diversify our risk based

1873
02:01:46,790 --> 02:01:48,960
but this is not true

1874
02:01:53,290 --> 02:01:57,600
is characterized by stable distributions with alpha

1875
02:01:57,620 --> 02:02:00,480
lower than one

1876
02:02:00,500 --> 02:02:02,690
it has been shown

1877
02:02:02,690 --> 02:02:10,410
well this was this picture has been taken by a paper by ibragimov jaffe and

1878
02:02:12,020 --> 02:02:16,660
just published journal of financial economics but these results can be found even in some

1879
02:02:16,660 --> 02:02:20,390
papers by different

1880
02:02:20,410 --> 02:02:24,190
well what happens

1881
02:02:24,210 --> 02:02:26,790
is that

1882
02:02:26,850 --> 02:02:31,350
this is the case for the levy distribution which means that all physical two

1883
02:02:31,370 --> 02:02:33,160
o point five

1884
02:02:33,190 --> 02:02:35,100
as increase

1885
02:02:35,120 --> 02:02:37,100
the number of assets

1886
02:02:37,140 --> 02:02:39,520
inside your portfolio

1887
02:02:39,540 --> 02:02:40,770
the total risk

1888
02:02:43,390 --> 02:02:46,170
instead of the decreasing

1889
02:02:47,750 --> 02:02:49,620
it's better not to do this for

1890
02:02:49,690 --> 02:02:52,520
as soon as risks of distributed in this way

1891
02:02:52,520 --> 02:02:57,580
in this case is four alpha equal to one that is for the cushy distribution

1892
02:02:57,600 --> 02:03:04,940
in this case is to it's interesting to note that supposing that this is the

1893
02:03:04,940 --> 02:03:06,710
number of people who

1894
02:03:06,730 --> 02:03:14,710
i want to buy insurance and insurance firm work so can make profits just because

1895
02:03:14,710 --> 02:03:16,670
they are collecting customers

1896
02:03:16,690 --> 02:03:25,190
so the higher the number of customers the best batters the degree of densification standard

1897
02:03:25,310 --> 02:03:31,100
assumption but the problem is that if you are in this range

1898
02:03:31,120 --> 02:03:38,210
you have a sort of coordination failure because as soon as the number of people

1899
02:03:38,210 --> 02:03:47,230
who want to ensure that sound is higher than seventy they insurance markets works

1900
02:03:47,290 --> 02:03:52,480
as long as the number of people is lower the seventeenth century markets doesn't work

1901
02:03:52,480 --> 02:03:58,140
OK so the number of people who want to buy insurance council what

1902
02:03:58,190 --> 02:04:02,350
and in this case but the main idea is that as soon as we want

1903
02:04:02,350 --> 02:04:04,210
to study

1904
02:04:04,250 --> 02:04:07,440
diversification we must estimate

1905
02:04:08,600 --> 02:04:13,060
the alpha of the stable distribution because if we are in presence of shots which

1906
02:04:13,060 --> 02:04:18,350
are distributed with an awful lot diversification theory doesn't work

1907
02:04:18,390 --> 02:04:22,710
this is the result in finance is possible to apply another in other fields as

1908
02:04:24,190 --> 02:04:28,120
well according to mean but not only to me

1909
02:04:29,770 --> 02:04:31,870
there's an idea

1910
02:04:31,890 --> 02:04:36,560
which has been propagated by a guy who is called robert shaw

1911
02:04:36,580 --> 02:04:41,290
and the idea of ursula is that why why why we are not using fire

1912
02:04:41,580 --> 02:04:48,850
financial markets to to provide well-being to people

