1
00:00:00,000 --> 00:00:05,170
mu sigma squared data to be minus one half the square would see was with

2
00:00:05,390 --> 00:00:08,410
then g of to this expression here

3
00:00:10,650 --> 00:00:13,690
it looks like we're going to absolutely nothing

4
00:00:13,700 --> 00:00:18,710
because i've turned a perfectly good and well known expression which is this one here

5
00:00:18,930 --> 00:00:21,400
into something absolutely atrocious

6
00:00:21,450 --> 00:00:22,600
and there

7
00:00:22,600 --> 00:00:26,820
just for the sake of algebraic reformulations

8
00:00:26,830 --> 00:00:29,620
well actually have gained something

9
00:00:29,640 --> 00:00:32,040
because if i later on

10
00:00:32,040 --> 00:00:36,700
i want to make this expression here depending on the location or something similar

11
00:00:36,720 --> 00:00:39,850
like what you would do great as good as regression

12
00:00:39,850 --> 00:00:40,850
the in

13
00:00:40,850 --> 00:00:46,850
this function will be extremely handy because it will be convex intellectual army nice optimization

14
00:00:46,860 --> 00:00:51,050
and dreaming up that function well i couldn't remember up in my worst nightmares

15
00:00:51,090 --> 00:00:52,570
so you will

16
00:00:52,580 --> 00:00:54,900
in this way in a systematic way

17
00:00:54,940 --> 00:01:01,740
stumble across a nice convex optimization problems that otherwise you wouldn't find

18
00:01:01,780 --> 00:01:06,900
and but some others that is the physicians before have tried designing such estimators and

19
00:01:07,110 --> 00:01:10,960
they didn't follow this track they dreamed up the wrong functions and the problems when

20
00:01:10,960 --> 00:01:12,040
the convex

21
00:01:12,830 --> 00:01:15,410
it's actually quite convenient you know why

22
00:01:15,440 --> 00:01:20,700
you have to pay a price but afterwards also the implementation get nicer

23
00:01:21,370 --> 00:01:25,460
anybody seen in almost division

24
00:01:25,780 --> 00:01:28,340
multinomial distributions

25
00:01:28,390 --> 00:01:30,320
well you've seen that those two

26
00:01:30,330 --> 00:01:32,990
so that's basically for instance if we have

27
00:01:33,000 --> 00:01:37,290
this joint events like in different one click if i toss the dice and then

28
00:01:37,310 --> 00:01:39,350
will be six

29
00:01:39,400 --> 00:01:42,800
which only occur with certain probability

30
00:01:42,810 --> 00:01:47,000
now i could go through this entire expression expression like what we have for the

31
00:01:47,280 --> 00:01:51,780
binomial distribution and then we derive it or i could just tell you the answer

32
00:01:51,790 --> 00:01:54,330
so let's the pain

33
00:01:54,580 --> 00:01:58,040
and what i'm going to tell you is that this map here

34
00:01:58,050 --> 00:01:59,350
which maps x

35
00:01:59,370 --> 00:02:02,330
into the victory x

36
00:02:02,380 --> 00:02:07,310
is the right one to give us a multinomial distribution

37
00:02:07,350 --> 00:02:09,080
but what does this mean

38
00:02:09,220 --> 00:02:12,100
well the inner product between EU x theta

39
00:02:12,290 --> 00:02:16,110
just takes out the corresponding in training data

40
00:02:16,120 --> 00:02:17,830
so for instance he three

41
00:02:17,830 --> 00:02:22,250
in proc with theta would pick up the the three

42
00:02:22,260 --> 00:02:27,520
so in other words i will get into the three minus the normalisation

43
00:02:27,570 --> 00:02:30,490
and that's exactly what i have here g you theta

44
00:02:30,790 --> 00:02:35,590
is the law of the sum of e to the theta i's

45
00:02:36,010 --> 00:02:38,850
and that the game

46
00:02:38,850 --> 00:02:41,910
is an exponential family distribution

47
00:02:41,920 --> 00:02:48,780
the advantage of doing it this way is that we have now covered

48
00:02:48,780 --> 00:02:51,760
a rather wide range of distributions

49
00:02:51,780 --> 00:02:53,620
which all can be dealt with

50
00:02:53,640 --> 00:02:55,820
with the same machinery afterwards

51
00:02:55,820 --> 00:03:00,300
so for instance if i have some convex the some optimizer it will not care

52
00:03:00,300 --> 00:03:05,290
so much about which particular distribution and i'm putting into it

53
00:03:05,300 --> 00:03:07,150
well it's just different

54
00:03:07,170 --> 00:03:08,640
feature map and all that

55
00:03:08,650 --> 00:03:12,310
and afterwards i want to tell my soul what the form of g if theta

56
00:03:12,310 --> 00:03:15,460
is interest happy optimise away at it

57
00:03:15,470 --> 00:03:18,700
so i can basically build one optimizer

58
00:03:18,750 --> 00:03:23,800
i can deal with multiclass with regression with normal distributions with

59
00:03:26,590 --> 00:03:29,920
two different events and will see a lot more others

60
00:03:29,950 --> 00:03:32,080
in the same framework

61
00:03:32,140 --> 00:03:34,120
this is great if you're

62
00:03:34,130 --> 00:03:37,350
writing software

63
00:03:38,060 --> 00:03:41,110
the distribution over ten different events OK

64
00:03:54,060 --> 00:03:56,900
it depends only in certain cases

65
00:03:56,920 --> 00:04:01,740
even for a normal distribution you want to do you they be this form

66
00:04:01,860 --> 00:04:12,820
yes i think that if you want

67
00:04:13,250 --> 00:04:21,390
not always made it it is really very much on which probably trying to solve

68
00:04:23,300 --> 00:04:24,680
so what you need

69
00:04:24,710 --> 00:04:27,610
here is that in certain cases

70
00:04:28,810 --> 00:04:30,360
they well defined

71
00:04:30,450 --> 00:04:35,440
i mean do not lead to well-defined normalizations g of state over the entire domain

72
00:04:35,780 --> 00:04:40,610
like for instance normal distributions with negative variance don't make sense

73
00:04:40,780 --> 00:04:44,780
or so so so that that's the type of thing that you want to avoid

74
00:04:44,940 --> 00:04:50,640
well up class distributions where the number of atoms keeps on increasing doesn't make sense

75
00:04:50,660 --> 00:04:51,920
but other than that

76
00:04:51,930 --> 00:04:55,440
it's just fine

77
00:04:56,930 --> 00:04:59,350
now let's look at some others

78
00:04:59,370 --> 00:05:01,900
let's look at the price of distribution

79
00:05:02,860 --> 00:05:06,560
plus all distributions look very much like a plus distribution

80
00:05:06,600 --> 00:05:07,680
in fact

81
00:05:07,790 --> 00:05:13,200
well the sufficient statistics of those

82
00:05:13,210 --> 00:05:17,250
are exactly the same ones as what we had for the past distribution just go

83
00:05:23,900 --> 00:05:31,960
solve five fix here was one of six

84
00:05:32,350 --> 00:05:37,130
and he thought ethics explic which is also called minus six and

85
00:05:37,140 --> 00:05:38,880
flip the sign of data

86
00:05:38,930 --> 00:05:43,140
but there's one key difference between the

87
00:05:43,200 --> 00:05:45,570
what's on the class distribution

88
00:05:45,570 --> 00:05:50,470
with the price of distribution at the discrete domain

89
00:05:50,530 --> 00:05:52,410
so it's not just the

90
00:05:52,440 --> 00:05:53,580
positive half

91
00:05:54,880 --> 00:05:59,610
it's just the set of all integers critical in here

92
00:05:59,610 --> 00:06:02,450
and what this does is it leads to

93
00:06:02,460 --> 00:06:06,250
and in addition to that they also have different measure here on the domain which

94
00:06:07,350 --> 00:06:10,210
and this one here

95
00:06:10,240 --> 00:06:13,150
so i don't count every observation the same

96
00:06:13,180 --> 00:06:14,870
the count them

97
00:06:14,880 --> 00:06:18,450
well count the small times higher than the

98
00:06:18,590 --> 00:06:19,940
larger terms

99
00:06:19,960 --> 00:06:24,440
so this is exactly where i told you initially that well you can hide away

100
00:06:24,760 --> 00:06:28,050
p zero europhysics in any way that you want as long as you can solve

101
00:06:28,050 --> 00:06:29,610
the integral

102
00:06:29,610 --> 00:06:31,770
that's one of those cases

103
00:06:31,780 --> 00:06:33,540
it just leads the in

104
00:06:33,590 --> 00:06:39,340
two quite different normalisation is the normalisation is e to the the theta

105
00:06:39,540 --> 00:06:44,170
but before we had a lot of state it was me and minus log of

106
00:06:44,170 --> 00:06:47,590
data it's e to the data

107
00:06:47,600 --> 00:06:49,910
so depending on how you

108
00:06:49,930 --> 00:06:51,240
define domain

109
00:06:51,280 --> 00:06:55,300
you know you play with the normalisation

110
00:06:55,340 --> 00:07:02,190
i mean so how you play with the measure you will get quite different normalizations

111
00:07:02,220 --> 00:07:03,830
that's probably the most

112
00:07:03,870 --> 00:07:09,260
relevant thing in this context

113
00:07:10,120 --> 00:07:11,990
this point here

114
00:07:12,040 --> 00:07:14,920
play around with the measure

115
00:07:14,940 --> 00:07:17,580
so what does it look like this one here

116
00:07:17,580 --> 00:07:22,180
almost looks like in the most efficient

117
00:07:22,200 --> 00:07:28,650
it's essentially the coolant from the most efficient for the discrete case

118
00:07:28,650 --> 00:07:31,600
you know if you wanted to harvard square and you see an out-of-work harvard grad

119
00:07:31,600 --> 00:07:34,710
the handing out examples of square roots to give you an example and you can

120
00:07:35,520 --> 00:07:40,330
see is the square root of two one point four one five two nine whatever

121
00:07:40,370 --> 00:07:44,870
and you get left harvard jokes strong stop in the second here right

122
00:07:44,960 --> 00:07:48,510
right so one of my trying to say here it doesn't

123
00:07:48,630 --> 00:07:53,530
yeah exactly were staying away from that really quickly especially with the cameras rolling what

124
00:07:53,570 --> 00:07:57,420
i try to say it tells you how you might test something but it doesn't

125
00:07:57,420 --> 00:07:59,680
tell you how to

126
00:07:59,690 --> 00:08:01,650
and that's what imperative knowledge is

127
00:08:01,650 --> 00:08:03,450
parrot of knowledge is

128
00:08:03,470 --> 00:08:06,650
a description of how to do something

129
00:08:06,690 --> 00:08:09,730
so let me give an example of a piece of imperative knowledge

130
00:08:09,750 --> 00:08:13,410
all right is actually a very old piece of imperative knowledge for computing square roots

131
00:08:13,410 --> 00:08:15,690
is attributed to heron of alexandria

132
00:08:15,740 --> 00:08:20,420
although i believe that the babylonians are i i suspected of knowing beforehand

133
00:08:20,430 --> 00:08:24,100
here is a piece of imperative knowledge

134
00:08:24,150 --> 00:08:25,800
i'm going to start

135
00:08:25,830 --> 00:08:28,020
with the gas

136
00:08:28,110 --> 00:08:30,860
the ecology

137
00:08:30,860 --> 00:08:32,890
and then i want to say if g

138
00:08:35,460 --> 00:08:37,190
is close to x

139
00:08:38,730 --> 00:08:41,960
in return g

140
00:08:42,000 --> 00:08:43,690
good enough answer

141
00:08:48,020 --> 00:08:49,420
to get new gas

142
00:08:49,420 --> 00:08:51,120
we're taking g

143
00:08:51,130 --> 00:08:53,130
x over g

144
00:08:53,180 --> 00:08:54,100
adding them

145
00:08:54,200 --> 00:08:58,960
dividing by two take the average of gnx g don't worry about how came about

146
00:08:58,960 --> 00:09:00,330
here and found this

147
00:09:00,440 --> 00:09:02,260
that gives me new guess

148
00:09:02,330 --> 00:09:05,130
and then repeat

149
00:09:10,290 --> 00:09:11,930
a recipe

150
00:09:12,030 --> 00:09:15,140
as the description of a set of steps

151
00:09:15,180 --> 00:09:17,850
notice what has it has a bunch of nice things that we want to use

152
00:09:18,480 --> 00:09:21,950
it's a sequence of specific instructions

153
00:09:22,000 --> 00:09:24,030
that i do in order

154
00:09:24,040 --> 00:09:27,940
along the way i have some tests depending on the value that test i may

155
00:09:27,950 --> 00:09:29,320
change where

156
00:09:29,390 --> 00:09:31,850
i am in that sequence of instructions

157
00:09:31,860 --> 00:09:35,490
it has an intense something tells me when i'm done what the answer is

158
00:09:35,550 --> 00:09:36,860
this tells you

159
00:09:36,900 --> 00:09:40,860
how to find square roots is how to knowledge is imperative not

160
00:09:41,070 --> 00:09:43,450
that's what computation

161
00:09:43,540 --> 00:09:47,330
basically it's about we want to have ways of capturing

162
00:09:47,330 --> 00:09:49,100
this process

163
00:09:49,200 --> 00:09:52,930
o canal is now to an interesting question which would be how to y

164
00:09:52,950 --> 00:09:55,620
build a mechanical process

165
00:09:55,660 --> 00:09:56,580
the capture

166
00:09:56,590 --> 00:09:58,640
that sort of computation

167
00:09:58,650 --> 00:10:03,330
so i'm going to suggest that there's an easy way to do it

168
00:10:03,350 --> 00:10:06,390
i realized that the boards in the wrong order here

169
00:10:06,440 --> 00:10:08,920
one of the ways i could do it as you could imagine building a little

170
00:10:08,920 --> 00:10:10,880
circuit to do this

171
00:10:10,920 --> 00:10:13,980
i had a couple of elements stored values in that i had some wires to

172
00:10:13,980 --> 00:10:16,620
move things around i had a little thing to do additional little thing to do

173
00:10:18,200 --> 00:10:21,000
it's something to do the testing i could build a little circuit that would actually

174
00:10:21,000 --> 00:10:23,100
do this computation

175
00:10:26,370 --> 00:10:31,650
strange as it sounds is actually an example of the early computers early computers

176
00:10:31,690 --> 00:10:33,460
well we call

177
00:10:33,500 --> 00:10:37,250
fixed programme

178
00:10:41,380 --> 00:10:49,220
meaning that they had to pieces circuitry designed to do a specific computation

179
00:10:49,260 --> 00:10:50,480
that's what they do

180
00:10:50,530 --> 00:10:53,240
we do that specific computation

181
00:10:53,290 --> 00:10:56,740
you've seen these alot right a good example of this

182
00:11:00,840 --> 00:11:02,960
it's basically an example of a fixed

183
00:11:03,030 --> 00:11:04,880
program computer

184
00:11:04,920 --> 00:11:06,690
does arithmetic

185
00:11:06,740 --> 00:11:09,170
to play video games on good luck

186
00:11:09,170 --> 00:11:13,070
i want to do word processing on good luck is designed to do

187
00:11:13,130 --> 00:11:16,540
specific things are fixed programme computer

188
00:11:16,690 --> 00:11:20,790
a lot of the other really interesting early ones similar have the flavor give an

189
00:11:22,660 --> 00:11:24,490
you never know how to pronounce this

190
00:11:24,520 --> 00:11:25,800
and as i

191
00:11:25,850 --> 00:11:30,610
nineteen forty one one of the earliest computational things with the thing designed by finding

192
00:11:30,610 --> 00:11:34,450
that as of basically solve linear equations

193
00:11:34,460 --> 00:11:38,530
and the thing to do if you're doing it you know one

194
00:11:38,590 --> 00:11:42,530
right or eighteen o six or whatever you want to do those things in

195
00:11:42,550 --> 00:11:45,070
all i could do that was all those equations

196
00:11:45,070 --> 00:11:48,980
one of my favourite examples of an early computer

197
00:11:49,040 --> 00:11:50,760
was done by alan turing

198
00:11:50,840 --> 00:11:54,290
the great computer scientists of all time called the bones

199
00:11:54,350 --> 00:11:56,880
which was designed to break codes

200
00:11:56,930 --> 00:11:59,860
actually used during world war two to break german enigma code and what it was

201
00:11:59,860 --> 00:12:00,930
designed to do

202
00:12:00,980 --> 00:12:04,230
was to solve that specific problem

203
00:12:04,280 --> 00:12:07,190
o point in trying to make is fixed programme computers is where we started but

204
00:12:07,190 --> 00:12:10,020
it doesn't really gets to where we like to be able to capture this idea

205
00:12:10,020 --> 00:12:11,690
of problem solving

206
00:12:11,710 --> 00:12:14,580
so let's see how we get there

207
00:12:14,580 --> 00:12:17,850
so even within this framework of given a description

208
00:12:17,860 --> 00:12:20,640
of of of of computation is a set of steps

209
00:12:20,690 --> 00:12:23,230
and the idea that i could build a circuit to do let me suggest for

210
00:12:23,290 --> 00:12:26,520
what would be a wonderful circuit build

211
00:12:26,520 --> 00:12:28,350
taking care of another way

212
00:12:28,360 --> 00:12:33,290
but i'm going to cook up the domain and interpretation out of gamma

213
00:12:33,300 --> 00:12:35,760
in this

214
00:12:35,820 --> 00:12:43,940
in this way under the assumption that gamma is consistent with that

215
00:12:43,960 --> 00:12:45,170
it's going to be the

216
00:12:45,180 --> 00:12:49,400
the but it's just such a proof

217
00:12:59,970 --> 00:13:01,470
start with the

218
00:13:03,140 --> 00:13:05,330
easy bit

219
00:13:05,340 --> 00:13:07,430
let's start with the additional

220
00:13:07,440 --> 00:13:09,330
calculus so less

221
00:13:09,340 --> 00:13:11,820
forget quantifiers forget variables

222
00:13:11,830 --> 00:13:14,970
forget evaluations

223
00:13:14,970 --> 00:13:16,600
OK forget the mine

224
00:13:18,470 --> 00:13:22,440
we take simple case

225
00:13:22,450 --> 00:13:24,750
we just have axioms

226
00:13:24,810 --> 00:13:31,450
one to three

227
00:13:31,470 --> 00:13:34,560
and this what the interpretations come to

228
00:13:34,640 --> 00:13:38,710
it's just good old truth tables

229
00:13:40,850 --> 00:13:42,180
which is

230
00:13:42,670 --> 00:13:47,130
incorporated in the other more elaborate definition

231
00:13:47,230 --> 00:13:50,620
the semantics of first-order logic

232
00:13:50,630 --> 00:13:57,240
so and usual truth

233
00:14:01,770 --> 00:14:07,290
so OK

234
00:14:07,300 --> 00:14:09,810
now what i'm going to do

235
00:14:11,320 --> 00:14:13,190
start with

236
00:14:22,810 --> 00:14:25,140
and we use the consistent

237
00:14:25,200 --> 00:14:34,540
and we want to find

238
00:14:34,580 --> 00:14:36,200
and interpret it

239
00:14:36,330 --> 00:14:41,730
of the language

240
00:14:41,780 --> 00:14:45,740
which can everything in gamma comes out true

241
00:14:45,820 --> 00:14:47,070
just thinking about

242
00:14:47,120 --> 00:14:50,550
propositional logic for them

243
00:14:52,060 --> 00:14:53,130
do we do

244
00:15:00,660 --> 00:15:07,850
we construct some sets

245
00:15:07,870 --> 00:15:14,210
we construct a big set

246
00:15:14,290 --> 00:15:15,810
a set of formulae

247
00:15:17,360 --> 00:15:19,860
in the following way

248
00:15:19,910 --> 00:15:24,050
it's the union of a sequence of sets

249
00:15:24,090 --> 00:15:26,670
delta zero

250
00:15:30,300 --> 00:15:34,370
there are different ways of starting this you can take it the empty set gamma

251
00:15:34,420 --> 00:15:38,850
has taken to be

252
00:15:38,860 --> 00:15:40,580
to be the

253
00:15:40,590 --> 00:15:43,260
he said

254
00:15:43,260 --> 00:15:44,240
which is

255
00:15:51,350 --> 00:15:55,440
we go through the action at each ten is that we have constructed delta psi

256
00:15:56,260 --> 00:15:59,610
and we're going to construct delta psi by plus one

257
00:15:59,640 --> 00:16:01,340
how however what has are going to do

258
00:16:01,380 --> 00:16:04,330
well we're gonna make reference to

259
00:16:04,370 --> 00:16:07,560
an enumeration

260
00:16:07,610 --> 00:16:09,800
a one a two

261
00:16:15,180 --> 00:16:22,840
in you know the formula

262
00:16:22,850 --> 00:16:25,760
it doesn't have to be a lot

263
00:16:25,820 --> 00:16:30,330
structural properties just every formula comes up exactly once

264
00:16:30,340 --> 00:16:31,440
it's a

265
00:16:31,460 --> 00:16:36,630
infinitely long but that's all right

266
00:16:36,630 --> 00:16:40,980
hundred number of clusters say okay

267
00:16:41,630 --> 00:16:44,880
before we take this k goes to infinity limit

268
00:16:44,880 --> 00:16:49,530
we can as imagine that there's kind of a very large value for k

269
00:16:51,070 --> 00:16:55,490
so in particular if k is very very large then it will be

270
00:16:55,510 --> 00:16:58,480
much larger than n but

271
00:16:58,510 --> 00:17:03,780
n here is gonna be our data set size basically the only the n

272
00:17:03,780 --> 00:17:09,210
data items in our data sets and if k is much larger than n then we

273
00:17:09,210 --> 00:17:14,400
know that there are at most n clusters which will be occupied in the

274
00:17:14,400 --> 00:17:17,570
sense that n clusters are gonna be as

275
00:17:18,070 --> 00:17:21,420
associated with the n data items okay

276
00:17:21,420 --> 00:17:24,960
typically there will be less than n clusters associated with our data

277
00:17:24,960 --> 00:17:29,570
but there can be at most n because every data item can only be assigned

278
00:17:30,400 --> 00:17:32,240
at most one cluster

279
00:17:32,280 --> 00:17:35,480
right so

280
00:17:35,490 --> 00:17:38,030
we know that because if

281
00:17:38,150 --> 00:17:42,900
k is much larger than n most of these k clusters will be basically empty right

282
00:17:42,900 --> 00:17:48,800
they they won't be assigned any data items at all right so they they're empty

283
00:17:48,800 --> 00:17:51,460
and we can kind of lump them all together

284
00:17:51,460 --> 00:17:54,710
okay just to make our competitions more efficient

285
00:17:55,010 --> 00:18:01,090
and if you do that then the conditional distributions for collapsed Gibbs sampler will look like this

286
00:18:01,090 --> 00:18:06,380
so for a cluster k which does have some data item assigned to it

287
00:18:06,380 --> 00:18:11,650
then n k will be positive and the conditional probability of that i equals to

288
00:18:11,650 --> 00:18:16,940
k is gonna be again this conditional prior this is unchanged times the

289
00:18:17,710 --> 00:18:20,340
conditional likelihood of x i

290
00:18:20,340 --> 00:18:23,920
given all the other data items assigned to cluster k

291
00:18:25,280 --> 00:18:30,480
we'll also have the conditional probability of

292
00:18:30,490 --> 00:18:33,460
data item i being assigned to some cluster

293
00:18:33,460 --> 00:18:34,990
which is not

294
00:18:35,010 --> 00:18:41,320
associated with any other data item and you can compute that by basically looking at basically

295
00:18:41,320 --> 00:18:45,730
summing over all of this empty clusters so for each of the empty clusters as we're

296
00:18:45,730 --> 00:18:49,490
gonna have a conditional prior of alpha divided by k

297
00:18:49,610 --> 00:18:50,990
divided by

298
00:18:51,030 --> 00:18:54,280
n minus one plus alpha and we have a conditional

299
00:18:54,280 --> 00:18:56,110
likelihood which is basically

300
00:18:56,240 --> 00:18:59,630
the probability of x i given

301
00:18:59,650 --> 00:19:03,480
that there are no other data items being assigned to that cluster yes

302
00:19:03,600 --> 00:19:08,440
k star is gonna be the number of clusters which

303
00:19:08,480 --> 00:19:10,670
have some data item

304
00:19:11,590 --> 00:19:13,090
assigned to them okay

305
00:19:13,380 --> 00:19:18,480
so these are the non empty clusters so that k minus k star is the number of

306
00:19:18,480 --> 00:19:22,990
empty clusters right so when you sum over those probably these you're gonna get k minus k star

307
00:19:22,990 --> 00:19:25,590
here yes

308
00:19:26,090 --> 00:19:31,610
I'll come to that later actually yeah so basically alpha is gonna be related to

309
00:19:31,610 --> 00:19:37,380
what you expect the the number of clusters you expect to see in your data set okay

310
00:19:37,440 --> 00:19:39,760
I'll come to that in I don't know ten

311
00:19:39,880 --> 00:19:42,090
slides or something

312
00:19:44,670 --> 00:19:49,550
the larger alpha is the the more clusters we we tend to expect to

313
00:19:49,550 --> 00:19:50,340
see okay

314
00:19:52,420 --> 00:19:56,820
we know that k star is the number of occupied clusters and it

315
00:19:56,820 --> 00:20:01,490
will be at most n and n is gonna be much smaller than k if

316
00:20:01,490 --> 00:20:06,280
key is very very large so now we can take k to go to infinity

317
00:20:06,280 --> 00:20:11,920
because in these two equations here when k goes to infinity we see that alpha divided

318
00:20:11,920 --> 00:20:14,420
by k is gonna go to zero

319
00:20:14,510 --> 00:20:15,900
so that's gonna go to zero okay

320
00:20:17,170 --> 00:20:21,550
k minus k star divided by k is gonna go to one

321
00:20:21,590 --> 00:20:26,880
because k star ha is upper bounded by n and k is going to infinity right so this

322
00:20:26,880 --> 00:20:31,050
sta here this ratio here is gonna go to one so that's gonna go to

323
00:20:31,050 --> 00:20:33,360
one alpha times one is just alpha okay

324
00:20:33,360 --> 00:20:35,280
so this is

325
00:20:35,820 --> 00:20:39,070
the infinite limit of the collapsed Gibbs sampler

326
00:20:39,090 --> 00:20:43,940
and basically this gives us a collapsed Gibbs sampler for an infinite mixture model with

327
00:20:43,940 --> 00:20:45,940
an infinite number of clusters okay

328
00:20:46,710 --> 00:20:50,660
and you you can actually implement this and it'll it'll

329
00:20:50,700 --> 00:20:54,660
it'll work very well okay

330
00:20:54,670 --> 00:20:59,870
but actually it kind of doesn't make sense okay as the model itself doesn't make sense

331
00:20:59,870 --> 00:21:03,280
and the reason is because if you look at any particular

332
00:21:03,280 --> 00:21:07,400
cluster in here it'll be assigned basically a

333
00:21:09,860 --> 00:21:13,590
prior probability of zero being

334
00:21:13,710 --> 00:21:15,360
there'll be a

335
00:21:15,360 --> 00:21:20,540
basically so the probability of that particular cluster being assigned to

336
00:21:20,580 --> 00:21:24,280
explain a particular data item will be basically zero okay

337
00:21:25,080 --> 00:21:29,040
and basically the reason is because if you look at the prior the prior

338
00:21:29,150 --> 00:21:35,330
is a is a is a symmetric Dirichlet so the prior probability of cluster k being

339
00:21:36,360 --> 00:21:40,360
particular data item will be just one over big k right

340
00:21:40,400 --> 00:21:44,860
and over big k is gonna go to zero as big k goes to infinity okay

341
00:21:45,630 --> 00:21:48,580
and basically

342
00:21:48,580 --> 00:21:50,250
the wh

343
00:21:54,160 --> 00:21:58,320
the math that we kind of have to go through to actually

344
00:21:58,330 --> 00:22:03,780
derive the Dirichlet process are basically kind of better ways of kind of making this

345
00:22:03,780 --> 00:22:05,480
infinite limit

346
00:22:05,480 --> 00:22:10,540
construction precise what what does it mean what does this infinite limit mean when we

347
00:22:10,540 --> 00:22:14,750
take big k to go to the infinity okay and these two ways

348
00:22:14,790 --> 00:22:15,780
are basically

349
00:22:16,080 --> 00:22:22,200
different ways of looking at a Dirichlet process kind of better ways of making this

350
00:22:22,330 --> 00:22:24,200
precise okay

351
00:22:24,410 --> 00:22:29,490
and basically we can think of a Dirichlet process as a basically infinite dimensional Dirichlet

352
00:22:29,490 --> 00:22:35,040
distribution basically is the think of it as the prior as the limit of the

353
00:22:35,040 --> 00:22:36,790
Dirichlet prior

354
00:22:36,830 --> 00:22:38,940
in our

355
00:22:39,050 --> 00:22:44,040
over the mixi mixing proportins as the number of clusters goes to infinity

356
00:22:44,090 --> 00:22:48,940
okay so I guess I'll move on

357
00:22:49,050 --> 00:22:52,740
to define what is a Dirichlet process okay

358
00:22:52,750 --> 00:22:55,590
before I do so

359
00:22:55,750 --> 00:23:00,330
perhaps I would actually spend two slides on some probability theory

360
00:23:00,330 --> 00:23:03,770
i took in the literature for different nuclei

361
00:23:03,790 --> 00:23:09,610
you see that all these processes are competitions in all these different nuclei have different

362
00:23:09,610 --> 00:23:11,630
intensity different lifetimes

363
00:23:11,660 --> 00:23:15,540
for all these particles decay

364
00:23:15,540 --> 00:23:20,320
because when you want to know is the lifetime of the nucleus

365
00:23:20,380 --> 00:23:25,080
you have to consider for example the least one million two hundred twenty six he

366
00:23:25,080 --> 00:23:28,310
can indicate either by any from capture

367
00:23:28,320 --> 00:23:34,010
alpha particle emission of becoming stick with different like here partial

368
00:23:34,020 --> 00:23:37,070
you don't think the biggest one or you don't take

369
00:23:37,080 --> 00:23:41,940
the some of this life the relation relationship like that so so you have to

370
00:23:41,940 --> 00:23:45,320
add the inverse of the lifetime to deter mine

371
00:23:45,330 --> 00:23:48,580
the last term of the nuclei

372
00:23:48,580 --> 00:23:51,460
so these are some examples of life lies

373
00:23:52,190 --> 00:23:55,870
just said that they can span many orders of magnitude

374
00:23:55,890 --> 00:24:01,990
so included sixteen if you think the well-known carbon fourteen which is

375
00:24:02,030 --> 00:24:04,140
used a lot for the patient

376
00:24:04,970 --> 00:24:06,930
is thousands of years

377
00:24:07,080 --> 00:24:13,080
to tell you for example which is then for twenty eight years

378
00:24:16,650 --> 00:24:19,890
now that i explain what are stability i

379
00:24:19,900 --> 00:24:21,530
what is artificial

380
00:24:21,790 --> 00:24:25,990
which which are rejected which began with one

381
00:24:26,010 --> 00:24:28,810
this one so i have two questions

382
00:24:28,820 --> 00:24:32,800
do you know why be stable nuclei

383
00:24:32,800 --> 00:24:34,610
he knew law

384
00:24:34,630 --> 00:24:38,580
not for all of these and you will be line

385
00:24:38,600 --> 00:24:42,960
why do you have more reach a neutron rich nuclei then

386
00:24:43,630 --> 00:24:47,020
it's beautiful young men and d

387
00:24:47,040 --> 00:24:52,390
and so second question why do we search for new nuclei

388
00:24:53,740 --> 00:25:00,080
concerning one stable nuclei do not follow this line is because of the coulomb repulsion

389
00:25:00,080 --> 00:25:01,850
which is later iterations

390
00:25:01,880 --> 00:25:07,120
so it's not you lose energy when you had a proton because of distribution

391
00:25:07,140 --> 00:25:09,400
so now why don't you have only

392
00:25:10,810 --> 00:25:15,930
neutron neutron rich and reach it because i told you that the proton neutron interaction

393
00:25:17,110 --> 00:25:19,420
then neutron neutron or proton proton

394
00:25:19,440 --> 00:25:21,230
so that's why have in

395
00:25:21,240 --> 00:25:22,410
brien between

396
00:25:22,430 --> 00:25:28,570
nuclear proliferation and proton neutron interaction which is bigger than your

397
00:25:28,580 --> 00:25:31,920
and why do we search for new nuclei

398
00:25:31,930 --> 00:25:35,980
so there are many reasons two of them are

399
00:25:35,980 --> 00:25:37,040
because of the

400
00:25:37,060 --> 00:25:39,910
the nucleon nucleon interaction that we we don't know

401
00:25:40,840 --> 00:25:45,580
we have it in order to have some input some

402
00:25:45,620 --> 00:25:49,790
new discoveries or to know that the behaviour of the

403
00:25:49,890 --> 00:25:52,980
interaction for example for very heavy nuclei

404
00:25:52,990 --> 00:25:54,810
or how it is in

405
00:25:54,820 --> 00:26:01,940
neutron rich we had we need all this new experimental data this is for example

406
00:26:01,940 --> 00:26:05,780
is to see how the spin the evolves when you add new transit is something

407
00:26:07,650 --> 00:26:12,310
the other region is related to the nucleosynthesis pass

408
00:26:12,320 --> 00:26:14,480
because if you are

409
00:26:14,480 --> 00:26:20,860
goulding was astrophysics are stars and you know that many processes

410
00:26:20,880 --> 00:26:24,320
i've been a a few processes been

411
00:26:24,330 --> 00:26:26,780
invoked to understand

412
00:26:26,790 --> 00:26:31,720
these nucleosynthesis and for example you have here is that process

413
00:26:31,740 --> 00:26:33,270
which is capture

414
00:26:33,290 --> 00:26:37,520
of the rapid capture of neutrons and then you have k

415
00:26:37,530 --> 00:26:43,710
this it goes through very exotic and very neutron rich nuclear so if you want

416
00:26:43,710 --> 00:26:50,190
to understand this nucleosynthesis you have in fact very precisely all these exotic nuclei

417
00:26:50,220 --> 00:26:51,650
which had

418
00:26:51,680 --> 00:26:54,250
played which had played a role in the past

419
00:26:54,340 --> 00:26:59,260
for creation of structures and things like that

420
00:26:59,280 --> 00:27:01,580
so where do we look for

421
00:27:01,600 --> 00:27:04,580
all these exotic nuclei

422
00:27:05,230 --> 00:27:09,400
all around the world for example here in san is like these

423
00:27:09,470 --> 00:27:14,580
so they do a lot of experiments i will show you examples of measurement friday

424
00:27:14,590 --> 00:27:15,840
of nuclear i

425
00:27:15,850 --> 00:27:22,260
and many experiment concerning these neutral reject the dignity you have also stipulated to win

426
00:27:22,340 --> 00:27:23,170
which we

427
00:27:24,160 --> 00:27:25,740
in in

428
00:27:26,600 --> 00:27:28,610
you have change then

429
00:27:28,640 --> 00:27:32,990
the project in the US so you have many many

430
00:27:32,990 --> 00:27:40,870
so you have some some facilities that are working on that are under construction

431
00:27:42,990 --> 00:27:45,990
how do we experimentally study a nucleus

432
00:27:46,010 --> 00:27:48,460
so you have different ways

433
00:27:48,480 --> 00:27:50,300
this is one so

434
00:27:50,320 --> 00:27:57,060
soon after what i've presented first concerning the rutherford experiment so elastic and inelastic scattering

435
00:27:57,060 --> 00:27:58,710
you have the project

436
00:27:58,720 --> 00:28:02,440
whatever it is electron positron proton neutron

437
00:28:02,470 --> 00:28:04,730
or maybe i and

438
00:28:04,740 --> 00:28:10,010
that's and then you can detect these particles here

439
00:28:10,020 --> 00:28:15,570
and you have your spectrum depending on the excitation energy and you see some people

440
00:28:16,800 --> 00:28:22,500
that do in fact correspond to the excited levels the state of these new

441
00:28:22,520 --> 00:28:27,260
then you can determine the momentum transfer if you have the energy in different angles

442
00:28:27,260 --> 00:28:29,820
you can determine this momentum transfer

443
00:28:29,940 --> 00:28:32,540
and then you have the form factor

444
00:28:32,550 --> 00:28:37,660
close to be intensity using these different but

445
00:28:37,690 --> 00:28:40,260
you do like for you transform

446
00:28:40,260 --> 00:28:42,140
the last part of this

447
00:28:44,700 --> 00:28:47,230
is related to this

448
00:28:47,240 --> 00:28:53,950
collaborative filtering in transferring but here we try to use a different source data

449
00:28:54,000 --> 00:28:57,220
instead of using the same

450
00:28:57,230 --> 00:29:00,480
collaborative filtering domain reading more

451
00:29:00,610 --> 00:29:02,420
binary data

452
00:29:03,380 --> 00:29:04,810
try to use

453
00:29:04,950 --> 00:29:09,810
wikipedia for example as the source domain and the target domain is the same is

454
00:29:09,810 --> 00:29:15,720
still reading them and and we asked whether it's possible to transfer some knowledge again

455
00:29:15,770 --> 00:29:20,510
to help with the data sparsity problem OK and this is the work done together

456
00:29:20,510 --> 00:29:23,450
with my student change you

457
00:29:23,460 --> 00:29:28,020
now so let's look at some details of this

458
00:29:29,190 --> 00:29:34,460
and this paper is also included in the workshop proceedings so everybody can

459
00:29:34,860 --> 00:29:38,730
already get that from the web

460
00:29:38,730 --> 00:29:41,110
OK so the motivation of

461
00:29:41,800 --> 00:29:43,380
is the following OK

462
00:29:43,410 --> 00:29:51,220
suppose we are interested in helping with recommendation doing better recommendation even one hour reading

463
00:29:51,230 --> 00:29:53,190
data is very sparse

464
00:29:53,800 --> 00:29:56,840
but suppose we don't have

465
00:29:56,880 --> 00:30:02,670
another reading domain to help us with so let's turn to something that everybody has

466
00:30:02,690 --> 00:30:04,850
OK which is the wikipedia

467
00:30:04,880 --> 00:30:06,520
now on the wikipedia

468
00:30:06,540 --> 00:30:07,750
we have

469
00:30:07,770 --> 00:30:14,050
a lot of data which we can model as a bipartite graph of users

470
00:30:14,060 --> 00:30:15,990
and product

471
00:30:16,030 --> 00:30:20,140
the products can be anything can be a document can be an event and in

472
00:30:20,160 --> 00:30:25,110
this particular war we focus on movies OK because them we can we can look

473
00:30:25,110 --> 00:30:29,900
at moving so so we have movies here in the target domain and we look

474
00:30:29,900 --> 00:30:32,270
at the subset of wikipedia

475
00:30:32,290 --> 00:30:37,390
web pages that are about movies and each of these pages

476
00:30:37,420 --> 00:30:43,630
i have a number of editors and these editors are people who have spent their

477
00:30:43,630 --> 00:30:49,610
time and effort in describing this movie in commenting on this movie so you know

478
00:30:50,300 --> 00:30:52,980
they have given some readings

479
00:30:53,020 --> 00:30:54,060
on this movie

480
00:30:54,080 --> 00:30:55,920
and another way

481
00:30:55,920 --> 00:30:57,070
we can look at

482
00:30:57,090 --> 00:31:00,790
whether the same user commented twenty from movies

483
00:31:00,810 --> 00:31:05,950
and use this knowledge to link similar movies together

484
00:31:05,950 --> 00:31:12,680
OK so then intuition becomes this even though we have various bars user movie rating

485
00:31:13,730 --> 00:31:19,780
now it's almost like using wikipedia we can relate some movies together and we can

486
00:31:19,780 --> 00:31:27,300
relate some users together OK so symmetrically the movies link different users together now however

487
00:31:27,310 --> 00:31:31,810
we cannot guarantee that these user and these users are the same but we can

488
00:31:32,740 --> 00:31:39,080
these movies and these movies together so let's focus on using the relationship between movies

489
00:31:39,130 --> 00:31:45,760
so it's so think of it as a consisting of a taxonomy ontology that really

490
00:31:45,770 --> 00:31:51,400
movies so as a result maybe we can do better recommendation even though the data

491
00:31:51,400 --> 00:31:54,000
is very sparse OK so this is the intuition

492
00:31:54,010 --> 00:31:55,420
we had to work

493
00:31:56,160 --> 00:32:01,940
if you look at wikipedia pages about movies we can see many many of these

494
00:32:02,300 --> 00:32:06,460
examples like kung-fu panda there's already a page there

495
00:32:06,470 --> 00:32:08,510
this is a popular movie

496
00:32:08,540 --> 00:32:10,150
in some less popular

497
00:32:10,160 --> 00:32:14,930
i don't know but maybe so there there are other links

498
00:32:14,970 --> 00:32:17,210
OK there are other things

499
00:32:17,220 --> 00:32:19,010
all right so

500
00:32:19,040 --> 00:32:22,840
we can represent as i said users

501
00:32:22,850 --> 00:32:30,670
and editing information as zero one so one means this this editor edited something about

502
00:32:30,670 --> 00:32:32,400
this movie

503
00:32:32,410 --> 00:32:34,320
and we have this matrix here

504
00:32:35,550 --> 00:32:41,170
the experiment we have here is really focused on a simple question so we call

505
00:32:41,170 --> 00:32:42,260
this system

506
00:32:42,260 --> 00:32:44,980
call it can call it

507
00:32:47,510 --> 00:32:51,880
the name is rather long so you can read the paper about it but generally

508
00:32:52,020 --> 00:32:57,210
it means if we have the extra information about

509
00:32:57,260 --> 00:33:02,530
the wikipedia so x is the original data then we can append a part of

510
00:33:02,530 --> 00:33:07,670
this matrix to this so you can see the the users are different but the

511
00:33:07,670 --> 00:33:09,280
movies are

512
00:33:09,300 --> 00:33:12,800
aligned because they have the same name and so on

513
00:33:12,820 --> 00:33:18,260
so when we do this matrix factorisation hopefully this is better than not using this

514
00:33:18,260 --> 00:33:21,200
however the states along this path of type

515
00:33:21,240 --> 00:33:29,790
and i want i think that's in that this brain might be successful for all

516
00:33:29,790 --> 00:33:31,500
branches of successful

517
00:33:33,750 --> 00:33:35,110
this goes

518
00:33:37,140 --> 00:33:40,220
that's ok your infinitely often

519
00:33:43,840 --> 00:33:45,950
i know it's all for you

520
00:33:46,030 --> 00:33:51,450
but there is still play the game with the example

521
00:33:56,720 --> 00:33:58,870
if i want my branch

522
00:34:02,120 --> 00:34:04,020
because you

523
00:34:04,220 --> 00:34:08,770
so far the branch opened

524
00:34:10,890 --> 00:34:13,930
my computer city

525
00:34:16,030 --> 00:34:17,560
so imagine

526
00:34:17,680 --> 00:34:20,930
here is the branch

527
00:34:23,220 --> 00:34:24,130
there's no way

528
00:34:26,190 --> 00:34:30,130
you will have here branch was just q a

529
00:34:30,500 --> 00:34:31,460
and she

530
00:34:33,660 --> 00:34:34,420
the column

531
00:34:34,430 --> 00:34:36,330
twenty one

532
00:34:36,370 --> 00:34:39,090
it would be one

533
00:34:42,490 --> 00:34:46,170
this input

534
00:34:48,470 --> 00:34:51,110
classified and a

535
00:34:53,340 --> 00:34:54,680
like you

536
00:35:00,420 --> 00:35:05,560
boxcar that when i mean topics canvassing with a topless women talk

537
00:35:08,020 --> 00:35:12,040
does that nokia infinitely often in this set

538
00:35:16,170 --> 00:35:17,870
this is even

539
00:35:17,880 --> 00:35:20,030
so it will be accepting here

540
00:35:23,630 --> 00:35:25,120
is not successful

541
00:35:25,170 --> 00:35:26,780
the entire cover

542
00:35:29,020 --> 00:35:32,030
so actually what does this automatically

543
00:35:32,080 --> 00:35:34,990
portraits in this automaton accepts

544
00:35:37,400 --> 00:35:39,140
it will accept

545
00:35:41,340 --> 00:35:43,910
which is a branch of the tree

546
00:35:44,060 --> 00:35:46,150
you will not

547
00:35:48,570 --> 00:35:49,850
he was it to be

548
00:35:49,960 --> 00:35:52,590
which means that this is the one

549
00:35:54,680 --> 00:35:58,740
you will not be even and you will a o

550
00:36:01,200 --> 00:36:07,360
with this notion of contracts make you another example

551
00:36:11,930 --> 00:36:14,190
so this is exactly what we said

552
00:36:14,200 --> 00:36:15,360
for example

553
00:36:17,730 --> 00:36:22,240
that you will eventually hit a while labels

554
00:36:24,890 --> 00:36:25,760
this is

555
00:36:25,810 --> 00:36:29,680
all successor remained that really have to fulfil

556
00:36:31,880 --> 00:36:33,580
part of the formula

557
00:36:33,660 --> 00:36:38,290
but does in this set but does it to find out what has happened to

558
00:36:44,360 --> 00:36:49,840
the events related opportunities to this

559
00:37:00,560 --> 00:37:02,110
for me here

560
00:37:02,310 --> 00:37:04,000
OK let's do this

561
00:37:04,030 --> 00:37:08,650
this article so i want an automaton

562
00:37:15,040 --> 00:37:19,850
of trees and what is it with infinite one is enough

563
00:37:22,270 --> 00:37:24,070
at least one

564
00:37:26,930 --> 00:37:30,830
so let's see what with

565
00:37:33,120 --> 00:37:34,730
i have three states

566
00:37:34,750 --> 00:37:36,910
they'll q

567
00:37:39,290 --> 00:37:40,590
it's it's

568
00:37:40,910 --> 00:37:45,040
the thing so whichever state in which is not

569
00:37:49,630 --> 00:37:51,550
if is a

570
00:37:59,660 --> 00:38:09,470
well i'm happy actually i would like to see more often

571
00:38:17,770 --> 00:38:19,860
i can get

572
00:38:21,890 --> 00:38:23,210
you have a choice

573
00:38:23,210 --> 00:38:24,820
one of the automaton

574
00:38:26,730 --> 00:38:28,680
on the same thing

575
00:38:28,710 --> 00:38:32,970
because nondeterministic it to be accepting

576
00:38:34,920 --> 00:38:36,990
why it is accompanied features

577
00:38:39,130 --> 00:38:42,070
this transition of this transition

578
00:38:42,070 --> 00:38:44,240
the following when i

579
00:38:46,120 --> 00:38:48,000
meat and a

580
00:38:52,620 --> 00:38:54,760
in the past where

581
00:38:57,240 --> 00:38:58,500
b either

582
00:38:58,620 --> 00:39:01,770
in the left subtree of

583
00:39:04,290 --> 00:39:08,650
so if my guess

584
00:39:11,370 --> 00:39:12,420
so that

585
00:39:12,680 --> 00:39:17,590
i want it with this

586
00:39:22,450 --> 00:39:26,320
the square to go and do away with his q

587
00:39:29,470 --> 00:39:31,700
q eight expect right OK

588
00:39:35,930 --> 00:39:45,660
i got to say state q

589
00:39:48,360 --> 00:39:51,790
how to tell me that they have met b

590
00:39:57,420 --> 00:40:01,470
interested in exhibiting one best

591
00:40:04,180 --> 00:40:05,470
so at this step

592
00:40:05,500 --> 00:40:07,550
when i mean to choose

593
00:40:09,850 --> 00:40:12,620
by choosing this side of this size

594
00:40:13,270 --> 00:40:21,980
continues on the left to right subtree

595
00:40:24,810 --> 00:40:38,950
so imagine my my my input is correct there is that there is the best

596
00:40:38,950 --> 00:40:43,950
for each training point we will lear in the combination of training points that created

597
00:40:43,950 --> 00:40:48,070
the weight vector so there's one alpha for each training point and

598
00:40:48,070 --> 00:40:51,970
so this is now I M M I'm using to denote the size of

599
00:40:51,970 --> 00:41:01,930
the training set and X X primed is is also an M by M matrix so now we can actually write the dual solution it's a it's a

600
00:41:01,930 --> 00:41:04,950
a very similar problem to solve it's a set of linear

601
00:41:04,950 --> 00:41:09,950
equations we need to solve but but as I said before it's actually of

602
00:41:09,950 --> 00:41:13,230
different dimensions to the the set of equations we were

603
00:41:13,230 --> 00:41:18,350
solving for W and now if we want regress on a new point

604
00:41:18,350 --> 00:41:24,010
what do we do okay we we take X primed W but now W

605
00:41:24,020 --> 00:41:27,630
of course is X capital X primed times alpha that's how it's

606
00:41:27,630 --> 00:41:33,370
expressed so if we now write that out explicitly

607
00:41:33,370 --> 00:41:37,030
sum alpha rhi xi by the linearity of the inner

608
00:41:37,030 --> 00:41:43,130
product we can bring the sum outside and we actually express this G of X as the sum

609
00:41:43,130 --> 00:41:46,990
weighted sum of the inner products between the test point and

610
00:41:47,550 --> 00:41:53,610
the training data okay so now the key observation here is

611
00:41:53,610 --> 00:41:57,270
that we actually only need to compute inner product between

612
00:41:57,270 --> 00:42:01,330
test points in training data and this matrix actually only

613
00:42:01,330 --> 00:42:05,030
involves inner products between training data the entry

614
00:42:05,030 --> 00:42:08,230
the I J entry in this matrix is just the inner product

615
00:42:08,230 --> 00:42:12,890
between the I and J training points that's

616
00:42:12,900 --> 00:42:20,290
because remember the rows of X were the training data therefore the columns of X prime to the training data so

617
00:42:20,290 --> 00:42:24,750
the I J entry in this matrix is the I throw in a product

618
00:42:24,750 --> 00:42:27,430
with the J column which is just the inner product

619
00:42:27,430 --> 00:42:31,860
between the I and J training points in the input space

620
00:42:31,950 --> 00:42:43,150
the way I've done it okay so the key ingredients are you now I'm using this K a little bit sort of a suggestively

621
00:42:43,150 --> 00:42:48,470
here kernel to denote this because as I said the K I J

622
00:42:48,470 --> 00:42:51,890
entry is just the inner product between the I J training

623
00:42:51,890 --> 00:42:57,190
points the evaluation of a new point is again now the

624
00:42:57,330 --> 00:43:07,210
weighted weighting of these inner products with new test point and the training data weighted according to this alpha rhi that we got by

625
00:43:07,210 --> 00:43:11,750
solving this set of equations here so the key observation

626
00:43:11,750 --> 00:43:18,830
here is that both steps this step here and this step here only involve inner

627
00:43:18,830 --> 00:43:26,430
products between input data points okay here we have X the test point and the training data and here we just have inner

628
00:43:26,430 --> 00:43:29,630
products between training data so remember what I said

629
00:43:29,630 --> 00:43:34,570
before the kernel provides a short cut to compute the inner

630
00:43:34,570 --> 00:43:38,390
product between projections of data points in to a complex feature

631
00:43:38,390 --> 00:43:46,890
space and then take the inner products okay cool well here if we just substitute in some

632
00:43:46,890 --> 00:43:51,490
feature vectors phi of X I phi of X J and similarly here phi

633
00:43:51,490 --> 00:43:58,790
of X phi of X I some feature vector then all we're needing here is the kernel function to

634
00:43:58,790 --> 00:44:02,350
compute that inner product it's an inner product of projections

635
00:44:02,350 --> 00:44:10,770
into a feature space of input data and so we can substitute a kernel function here I suggested by

636
00:44:10,780 --> 00:44:14,590
this K and similarly here we have a kernel function

637
00:44:14,590 --> 00:44:26,310
between the test point and the training data so this is the key idea so it only involves

638
00:44:26,310 --> 00:44:28,470
occurrences of the inner product a kernel function

639
00:44:28,470 --> 00:44:32,210
computes that inner product in the kernel defined feature

640
00:44:32,210 --> 00:44:36,250
space and so we can apply the ridge regression algorithm in

641
00:44:36,250 --> 00:44:38,670
that high-dimensional potentially high-dimensional feature

642
00:44:38,670 --> 00:44:45,050
space by just substituting the kernel function for

643
00:44:45,020 --> 00:44:47,770
the computation of this matrix here the I J entry of this

644
00:44:47,770 --> 00:44:51,190
matrix here and the kernel function for the evaluation of

645
00:44:51,190 --> 00:44:54,270
this new point so it's sort of a little bit a magic

646
00:44:54,270 --> 00:44:58,110
you kind of apparently operating in this very high-

647
00:44:58,110 --> 00:45:02,070
dimensional space without actually explicitly computing in

648
00:45:02,070 --> 00:45:08,890
it by using this this trick okay well you may say do

649
00:45:08,890 --> 00:45:12,670
such functions exist I mean give me an example I

650
00:45:12,670 --> 00:45:15,510
mean maybe this kernel function is gonna be just as

651
00:45:15,500 --> 00:45:18,590
difficult to compute as actually computing these explicit

652
00:45:18,830 --> 00:45:21,650
vectors and taking an inner product so what I wanna do now is show

653
00:45:21,650 --> 00:45:27,510
you an example very simple example just to convince you that yes you know there are

654
00:45:27,510 --> 00:45:30,850
very natural ways of using defining kernel functions

655
00:45:31,030 --> 00:45:34,310
that do correspond very interesting high-dimensional

656
00:45:34,310 --> 00:45:38,270
feature spaces so here's the first sort of nontrivial

657
00:45:38,270 --> 00:45:44,090
example of a kernel function it's the so-called quadratic kernel so what you do

658
00:45:44,090 --> 00:45:46,650
is you simply take the inner product between the feature

659
00:45:46,650 --> 00:45:50,390
vectors in the input space and then you just square that

660
00:45:50,390 --> 00:45:57,110
number okay so it's one extra computation one extra

661
00:45:57,110 --> 00:46:02,000
jeff questions about you know products

662
00:46:02,970 --> 00:46:08,140
so this is the first very very very simple classifier

663
00:46:08,160 --> 00:46:12,460
so imagine that you have two sets of points

664
00:46:12,470 --> 00:46:14,350
two sets of labeled points

665
00:46:14,360 --> 00:46:19,520
from each year from four from from which you're going to to learn classifiers so

666
00:46:19,520 --> 00:46:22,420
you have the blue points here

667
00:46:22,440 --> 00:46:24,490
the red one here

668
00:46:24,530 --> 00:46:28,420
those are your training points and those are the points that you're going to use

669
00:46:28,430 --> 00:46:31,020
to construct your model

670
00:46:31,040 --> 00:46:38,460
and don't forget the details for not just what the very simple thing as i

671
00:46:38,460 --> 00:46:40,520
say that i'm going to take

672
00:46:40,520 --> 00:46:41,600
in the middle of the

673
00:46:41,680 --> 00:46:45,590
red points here in the middle of the the blue points here

674
00:46:45,600 --> 00:46:49,800
i'm going to draw a line between those two in the middle points and i

675
00:46:49,800 --> 00:46:52,310
say that i'm going to say that

676
00:46:52,380 --> 00:46:53,430
if the point

677
00:46:53,440 --> 00:46:54,850
it's closer to

678
00:46:54,890 --> 00:47:00,070
c-minus than c plus then i'll say that these points should be classified as an

679
00:47:00,110 --> 00:47:02,320
as minus

680
00:47:02,340 --> 00:47:08,600
as it has a negative point otherwise if a point is closer to c plus

681
00:47:08,600 --> 00:47:12,750
than c-minus then i'll say that it should be classified as a plus

682
00:47:12,760 --> 00:47:14,340
it is very very simple

683
00:47:14,370 --> 00:47:19,530
there's nothing to do it and you'll see that the the

684
00:47:20,440 --> 00:47:23,410
mathematics is also very simple so

685
00:47:23,430 --> 00:47:26,730
here you have the

686
00:47:26,770 --> 00:47:28,270
centre of mass

687
00:47:28,280 --> 00:47:30,520
it should be something like that

688
00:47:30,520 --> 00:47:34,940
the centre of mass of the price point the positive points and the centre of

689
00:47:34,940 --> 00:47:37,210
mass the mass of the negative points

690
00:47:37,260 --> 00:47:44,090
so yes he plus that is just the mean of all the exercise

691
00:47:44,820 --> 00:47:46,000
you see plus

692
00:47:46,000 --> 00:47:47,910
seen minus which is

693
00:47:47,920 --> 00:47:51,690
the mean or the average points of all

694
00:47:51,700 --> 00:47:54,800
the minus points

695
00:47:55,000 --> 00:47:57,520
you have

696
00:47:57,560 --> 00:48:01,760
the middle of c-minus and c plus which is just

697
00:48:02,400 --> 00:48:05,650
one half of c plus plus c minus

698
00:48:05,690 --> 00:48:10,020
OK this is something that i told told you about just before if you want

699
00:48:10,020 --> 00:48:13,900
to to compute the mean of two points you just add the two vectors and

700
00:48:13,910 --> 00:48:16,690
you divided by two and you have made of the point

701
00:48:16,700 --> 00:48:22,670
and the vector that i'm going to use to to do classification

702
00:48:22,720 --> 00:48:24,490
is called w

703
00:48:26,250 --> 00:48:29,990
i think i'm consistent with that are always call

704
00:48:30,760 --> 00:48:33,330
classification vector w

705
00:48:33,380 --> 00:48:36,070
one of the the slide

706
00:48:36,780 --> 00:48:40,300
and so the decision function is very simple

707
00:48:40,320 --> 00:48:45,340
i said that i would classify points test points x according to which of the

708
00:48:45,350 --> 00:48:49,760
two class means c plus or c one c minus is closed

709
00:48:49,780 --> 00:48:53,660
so it's very easy to do that

710
00:48:53,700 --> 00:48:57,950
you just take the inner product between

711
00:48:57,960 --> 00:48:59,770
w here

712
00:48:59,790 --> 00:49:01,880
and the vector x

713
00:49:03,800 --> 00:49:06,590
again if those two vectors

714
00:49:06,610 --> 00:49:08,790
o point to the same direction

715
00:49:08,850 --> 00:49:15,110
then you're going to say that that the inner product is positive and you're going

716
00:49:15,110 --> 00:49:19,560
to say that the the point x is positive or otherwise it's just the other

717
00:49:19,560 --> 00:49:25,600
way around if the the inner product is negative then it means that x minus

718
00:49:25,600 --> 00:49:28,010
cnw point two opposite directions

719
00:49:28,400 --> 00:49:30,040
and you're going to classify

720
00:49:31,130 --> 00:49:32,530
as minus point

721
00:49:32,540 --> 00:49:34,950
but then again if

722
00:49:34,960 --> 00:49:40,330
and the classifier is just these very simple function here

723
00:49:40,470 --> 00:49:45,050
so it's based on these on these real valued function

724
00:49:45,110 --> 00:49:47,200
which is a measure of x

725
00:49:47,200 --> 00:49:51,650
which is exactly equal to w inner product with x minus c

726
00:49:51,660 --> 00:49:56,760
and you want to predict the class just take the sign of these these values

727
00:49:56,760 --> 00:50:00,960
these inner product if the sign is positive if the if the inner product is

728
00:50:00,960 --> 00:50:05,120
positive then you say that x x should be classified as positive example

729
00:50:05,170 --> 00:50:08,350
if the inner product is negative and you say

730
00:50:08,370 --> 00:50:15,610
that the the test point should be classified as as negative as the negative point

731
00:50:15,650 --> 00:50:22,480
and the decision surface is induced by these decision rules

732
00:50:22,520 --> 00:50:24,220
it is just

733
00:50:24,260 --> 00:50:27,100
these hyperplane

734
00:50:27,120 --> 00:50:32,720
so remember that what what something told you

735
00:50:32,730 --> 00:50:34,710
just a

736
00:50:34,750 --> 00:50:42,270
she told you that hyperplane in the dimension d dimensional space just a d minus

737
00:50:43,100 --> 00:50:45,050
the national dimensional space

738
00:50:45,060 --> 00:50:50,100
so here it is the two dimensional space hyperplane is just like

739
00:50:50,120 --> 00:50:54,820
in three dimensional space hyperplane is

740
00:50:54,870 --> 00:50:56,010
the plane

741
00:51:05,040 --> 00:51:12,340
now we're we're are going to go into the details of computing hf x

742
00:51:12,380 --> 00:51:15,510
again this very easy to be freaked out

743
00:51:16,600 --> 00:51:19,460
here if you compute edge effects

744
00:51:19,460 --> 00:51:24,770
i say that this is the inner product between w and x minus c

745
00:51:26,320 --> 00:51:29,430
what i do here is that i place

746
00:51:29,430 --> 00:51:31,780
one more

747
00:51:34,190 --> 00:51:45,790
because of my view or

748
00:51:51,940 --> 00:51:58,520
and some of them

749
00:51:58,540 --> 00:52:01,090
i mean

750
00:52:05,080 --> 00:52:09,790
in the area

751
00:52:32,870 --> 00:52:40,520
what we're seeing or be or

752
00:53:01,420 --> 00:53:06,340
we're i going why to be

753
00:53:06,350 --> 00:53:13,750
one of the problem users

754
00:53:13,770 --> 00:53:27,170
we are not

755
00:53:38,800 --> 00:53:45,780
what we

756
00:54:06,200 --> 00:54:12,250
you one

757
00:54:16,460 --> 00:54:22,310
so that they are

758
00:54:22,360 --> 00:54:28,710
some of

759
00:54:28,710 --> 00:54:34,710
the phone content is provided by MIT opencourseware under a creative commons license

760
00:54:34,830 --> 00:54:43,700
additional information about relations and MIT opencourseware in general is available OCW MIT the EU

761
00:54:43,710 --> 00:54:46,800
the molecular orbital theory

762
00:54:49,690 --> 00:54:52,700
remember what the key idea was there

763
00:54:52,750 --> 00:54:54,850
the key idea

764
00:54:54,870 --> 00:54:58,140
let's take the wave functions

765
00:54:58,150 --> 00:55:06,630
atomic wavefunctions wavefunctions on the atoms that are going to form the chemical plant

766
00:55:06,640 --> 00:55:10,610
take those functions and let them overlap

767
00:55:10,660 --> 00:55:12,030
let them

768
00:55:12,040 --> 00:55:17,220
constructively and destructively interfere a rather ways

769
00:55:19,110 --> 00:55:23,720
they can constructively and destructively interfere

770
00:55:23,730 --> 00:55:25,370
we saw last time

771
00:55:25,620 --> 00:55:31,000
when these atomic wavefunctions constructively interfere

772
00:55:31,010 --> 00:55:37,490
the result was a molecular wavefunctions that was a bonding

773
00:55:37,500 --> 00:55:39,400
one way function

774
00:55:39,420 --> 00:55:42,950
consequently antibonding state

775
00:55:42,970 --> 00:55:47,620
so for example in the case of oxygen

776
00:55:47,630 --> 00:55:48,590
we hear

777
00:55:49,840 --> 00:55:56,220
two s wavefunctions i need to be isolated oxygen atoms when they

778
00:55:57,730 --> 00:56:01,730
they can constructively to tivoli interfere

779
00:56:01,740 --> 00:56:05,850
to form a sigma two as finding wavefunctions

780
00:56:05,900 --> 00:56:09,410
and consequently sigma two s

781
00:56:09,490 --> 00:56:11,130
finding date

782
00:56:11,160 --> 00:56:14,160
and there are two electrons in there

783
00:56:14,170 --> 00:56:18,270
and you remember that we label the molecular state

784
00:56:18,280 --> 00:56:23,600
with respect to the symmetry of the wave function around the molecular axis

785
00:56:23,640 --> 00:56:26,030
that way function with sigma

786
00:56:26,060 --> 00:56:28,140
if that symmetry was

787
00:56:28,150 --> 00:56:32,500
cylindrical around the molecular axis

788
00:56:32,510 --> 00:56:34,400
it was part i

789
00:56:34,420 --> 00:56:38,410
here in this case from molecular oxygen

790
00:56:38,460 --> 00:56:40,770
when the weight function

791
00:56:40,820 --> 00:56:46,790
it was not cylindrically symmetric around the bond axis

792
00:56:46,810 --> 00:56:52,130
and we also saw what happened when these two wave functions come together they overlap

793
00:56:52,130 --> 00:56:56,910
and they destructively interfere when they destructively interfere

794
00:56:56,930 --> 00:56:59,850
we perform what we call the nancy bonding

795
00:56:59,860 --> 00:57:01,330
the weight function

796
00:57:01,370 --> 00:57:04,810
given designations

797
00:57:04,880 --> 00:57:10,580
in the case here the two as electrons are away functions and oxygen this form

798
00:57:10,600 --> 00:57:17,770
the sigma two s wavefunction the antibonding wave function its energy

799
00:57:17,820 --> 00:57:19,360
is higher

800
00:57:19,370 --> 00:57:22,810
than that of the

801
00:57:22,850 --> 00:57:24,890
atomic state

802
00:57:24,930 --> 00:57:31,360
for the two separated oxygen atoms bonding state is lower in energy into finding state

803
00:57:31,360 --> 00:57:34,460
is higher in energy

804
00:57:34,510 --> 00:57:38,480
and so this was for the two s electrons

805
00:57:38,520 --> 00:57:40,270
well all this

806
00:57:40,280 --> 00:57:42,500
right lower energy

807
00:57:42,510 --> 00:57:47,390
because the more strongly bound are though one as atomic states

808
00:57:47,410 --> 00:57:50,150
and the molecular states formed by the

809
00:57:50,190 --> 00:57:55,650
constructive and destructive interference with the one as wavefunctions i didn't draw there

810
00:57:55,670 --> 00:57:57,980
US that higher energy

811
00:57:58,060 --> 00:58:04,280
and finally here here the two p states that are coming together and molecular oxygen

812
00:58:04,290 --> 00:58:05,830
forming a sigma

813
00:58:05,840 --> 00:58:07,690
two PC

814
00:58:07,710 --> 00:58:09,810
and then two

815
00:58:09,820 --> 00:58:15,580
molecular states pi two PY and applied to px

816
00:58:15,600 --> 00:58:18,750
and then in molecular oxygen we also saw

817
00:58:18,770 --> 00:58:21,780
that there was one electron each

818
00:58:21,800 --> 00:58:23,220
here in the US

819
00:58:23,290 --> 00:58:30,540
hi star two px i start PY

820
00:58:30,550 --> 00:58:31,620
OK now

821
00:58:31,630 --> 00:58:34,830
i just wanted to say a few words here about

822
00:58:34,880 --> 00:58:39,470
the physical significance of these energies here

823
00:58:39,490 --> 00:58:42,190
so this is the molecular orbital

824
00:58:42,200 --> 00:58:45,330
in the case of molecular oxygen here this is the

825
00:58:45,410 --> 00:58:49,280
molecular state that is highest occupied

826
00:58:49,290 --> 00:58:54,160
right what we mean by highest occupied is that it is the state

827
00:58:54,230 --> 00:58:57,230
that is the highest flying in energy

828
00:58:57,250 --> 00:59:02,380
is an electron in the highest flying in energy

829
00:59:02,390 --> 00:59:05,320
what this binding energy means

830
00:59:06,240 --> 00:59:08,760
what this energy means

831
00:59:08,760 --> 00:59:14,440
so you can prove this by induction could prove this by induction saying well

832
00:59:14,490 --> 00:59:18,430
base cases one equals or one OK that's true

833
00:59:18,440 --> 00:59:23,090
OK and the induction step is well if i know that and minus one so

834
00:59:23,090 --> 00:59:26,370
let's suppose that and minus one is order one

835
00:59:27,870 --> 00:59:29,220
that implies

836
00:59:30,040 --> 00:59:31,940
which is an minus one

837
00:59:31,940 --> 00:59:33,220
plus one

838
00:59:33,270 --> 00:59:35,930
well that this is order one

839
00:59:35,970 --> 00:59:40,120
and this one is order one so the whole thing is order one and and

840
00:59:40,120 --> 00:59:43,710
that's true if you knew that and minus one was order one n one is

841
00:59:43,710 --> 00:59:47,540
that one then their some is also order one but this is the false proof

842
00:59:47,540 --> 00:59:51,710
you can't you can inductive abigail's what's going on here is that the concepts that

843
00:59:51,710 --> 00:59:57,170
are implicit in here changing here you bigger one areas in big o one you're

844
00:59:57,180 --> 01:00:01,000
probably doubling the constant in that every time you do this relation if you have

845
01:00:01,000 --> 01:00:04,500
a finite number of doubling of constance no big deal just the concept to the

846
01:00:04,500 --> 01:00:09,220
power number of dublin's but here you're doing n dumplings and that's not good

847
01:00:09,230 --> 01:00:11,010
the constant is getting

848
01:00:11,050 --> 01:00:15,390
the cost is now depending on and so we're avoiding this kind of problem by

849
01:00:15,390 --> 01:00:19,900
writing out the constant we have to make sure that cost doesn't change

850
01:00:19,930 --> 01:00:21,070
so good

851
01:00:21,090 --> 01:00:25,340
OK so now written not the costs and i should be safe is what what

852
01:00:25,340 --> 01:00:29,090
i need to prove this is assuming it for all k less than them and

853
01:00:29,090 --> 01:00:31,520
i have to prefer k equal there

854
01:00:31,530 --> 01:00:32,820
so i'm going to take

855
01:00:34,640 --> 01:00:39,790
and i just expanded can going do the obvious thing i had this recurrence how

856
01:00:39,790 --> 01:00:43,310
to expand here than that of most even over two and i know

857
01:00:43,330 --> 01:00:47,470
some facts about human over two because to two is less than n

858
01:00:48,380 --> 01:00:52,720
six pantera and is four times the of two

859
01:00:52,760 --> 01:00:54,560
was and

860
01:00:54,590 --> 01:00:57,030
and now i have an upper bound on this thing

861
01:00:57,030 --> 01:00:58,960
from the induction hypothesis

862
01:00:58,970 --> 01:01:01,760
so this is

863
01:01:04,580 --> 01:01:05,800
time c

864
01:01:05,820 --> 01:01:11,530
times the argument q plus and

865
01:01:14,530 --> 01:01:18,970
i have a feeling

866
01:01:18,990 --> 01:01:22,040
by the end of last lecture

867
01:01:22,070 --> 01:01:24,350
his blackboard so

868
01:01:26,100 --> 01:01:30,530
what's gonna

869
01:01:32,440 --> 01:01:38,000
OK how have board

870
01:01:38,040 --> 01:01:47,270
i'd like to say my super-human strength but i believe it's from the quality of

871
01:01:47,290 --> 01:01:49,480
the blackboard

872
01:01:49,490 --> 01:01:55,550
the hinge is OK so continuing on here let's expand this little bit

873
01:01:55,580 --> 01:01:58,000
we have

874
01:02:00,070 --> 01:02:05,620
OK so we end up with nq over two q and to keep his eight

875
01:02:05,620 --> 01:02:07,840
for eight years i have

876
01:02:07,850 --> 01:02:10,450
so we have to have

877
01:02:10,460 --> 01:02:12,480
times c

878
01:02:12,490 --> 01:02:17,130
time and q class and

879
01:02:17,150 --> 01:02:19,430
and what i'd like this to be

880
01:02:19,450 --> 01:02:24,600
is the bottom would like to go is that this is at most c times

881
01:02:24,600 --> 01:02:27,110
and q

882
01:02:27,130 --> 01:02:31,930
that's what i'd like to prove to establish re-established induction hypothesis for and

883
01:02:31,930 --> 01:02:35,020
so what i'll do in order to see when that's the case is just write

884
01:02:35,340 --> 01:02:40,290
this as what i want to this sort of the desired

885
01:02:40,370 --> 01:02:46,030
value see times and q minus whatever i don't want this

886
01:02:46,050 --> 01:02:48,200
called the residual

887
01:02:52,720 --> 01:02:56,640
now i have to actually figure this out c where c and q but only

888
01:02:56,640 --> 01:03:01,620
have cm cube here so i need to subtract off half c q

889
01:03:01,650 --> 01:03:05,180
to get that we term correct and i have plus and a minus here says

890
01:03:07,450 --> 01:03:12,030
OK and that's the residual in order for this to be announced this i knew

891
01:03:12,040 --> 01:03:16,720
that the residual is nonnegative this is if

892
01:03:16,730 --> 01:03:20,790
and the residual part

893
01:03:20,820 --> 01:03:23,690
is greater than equal to zero

894
01:03:23,700 --> 01:03:27,560
which is pretty easy to do because here i have control over c

895
01:03:27,580 --> 01:03:30,030
i get to pick c to be whatever i want

896
01:03:30,040 --> 01:03:33,440
and as long as c is at least to

897
01:03:34,880 --> 01:03:37,270
and this is o or one

898
01:03:37,280 --> 01:03:40,900
at least i have an cubes should be greater or equal to n and that's

899
01:03:40,900 --> 01:03:42,040
always the case

900
01:03:42,090 --> 01:03:45,350
so this is for example

901
01:03:47,450 --> 01:03:49,020
this is true if c

902
01:03:49,030 --> 01:03:51,010
is it atleast one

903
01:03:51,020 --> 01:03:54,480
and i don't think it matters what n is let's say

904
01:03:54,500 --> 01:03:57,090
and there's atleast one just four

905
01:03:58,780 --> 01:04:00,890
OK so we get

906
01:04:01,310 --> 01:04:05,810
what we done is prove that he then is the most some constant times and

907
01:04:05,810 --> 01:04:10,470
q then the costs is like one

908
01:04:10,480 --> 01:04:15,050
so that's an upper bound is not tight upperbound we actually believe that m squared

909
01:04:15,050 --> 01:04:16,050
and it is

910
01:04:16,110 --> 01:04:18,610
but you can still means here have to be a little careful this is not

911
01:04:18,630 --> 01:04:23,470
the the answer is and just means it's most mksp government q

912
01:04:23,480 --> 01:04:26,810
and this was approved by induction now technically i should have put the base case

913
01:04:26,810 --> 01:04:30,930
in this section is a little bit missing the base case is pretty easy because

914
01:04:30,930 --> 01:04:33,790
t want some constants

915
01:04:33,820 --> 01:04:38,180
but it will sort of influence things the base case

916
01:04:40,490 --> 01:04:45,370
if you want some concepts and what we need is that is at most c

917
01:04:45,370 --> 01:04:48,260
times one q which is c

918
01:04:48,270 --> 01:04:53,010
and that will be true as long as you choose c to be sufficiently large

919
01:04:53,030 --> 01:04:59,030
so this is true if c is chosen

920
01:04:59,050 --> 01:05:00,510
sufficiently large

921
01:05:00,520 --> 01:05:03,110
now we don't care about constants but

922
01:05:03,130 --> 01:05:05,370
the point is just to be a little bit careful

923
01:05:05,390 --> 01:05:07,080
it's not true

924
01:05:07,130 --> 01:05:11,500
the teevan then is at most one times m squared even if you're all we

925
01:05:11,500 --> 01:05:14,520
need is this is at least one for the base case to work c actually

926
01:05:14,520 --> 01:05:18,100
might have to be a hundred or whatever t one s

927
01:05:18,110 --> 01:05:19,290
this would be a little bit

928
01:05:19,290 --> 01:05:21,530
careful the

929
01:05:21,580 --> 01:05:25,270
it doesn't really affect the answer usually won't but because we have very simple base

930
01:05:25,290 --> 01:05:27,230
cases here

931
01:05:27,810 --> 01:05:29,490
so let's try to prove

932
01:05:29,500 --> 01:05:31,700
the tight bound

933
01:05:31,790 --> 01:05:36,710
order and squares

934
01:05:36,720 --> 01:05:42,230
i'm not going to prevent make about where you can prove in america and spread

935
01:05:42,240 --> 01:05:44,360
and as well using substitution method

936
01:05:46,010 --> 01:05:47,960
this be satisfied for now

937
01:05:50,170 --> 01:05:52,340
the upper bound and square

938
01:05:52,360 --> 01:05:54,710
so let's try

939
01:05:54,710 --> 01:05:59,870
to prove that here this the the same

940
01:05:59,890 --> 01:06:01,710
recurrence i want that

941
01:06:04,560 --> 01:06:06,220
so i'm going to

942
01:06:07,050 --> 01:06:12,770
do the same thing already bit faster consists basically copying

943
01:06:16,480 --> 01:06:18,790
so now instead of three i have two

944
01:06:18,800 --> 01:06:21,710
so then i have you then

945
01:06:21,720 --> 01:06:27,140
four times tumour number two was and i expand

946
01:06:27,170 --> 01:06:33,010
this team in order to this is that most four times see times and over

947
01:06:33,010 --> 01:06:38,820
two square loss and and now instead of having to queue to have two squared

948
01:06:38,820 --> 01:06:41,480
which is only four force council

949
01:06:41,490 --> 01:06:42,640
i can see

950
01:06:42,670 --> 01:06:45,460
times and squared plus and

951
01:06:45,480 --> 01:06:49,530
and if you prefer to write as desired minus residual then i have c and

952
01:06:49,530 --> 01:06:53,920
squared minus negative and and what i need is that i want

953
01:06:53,940 --> 01:06:56,500
this to be nonnegative

954
01:06:56,630 --> 01:07:03,490
it is damn hard for minus ten to be nonnegative is zero or happy version

955
01:07:03,510 --> 01:07:07,000
this is an induction on it's got hold for all n greater than equal to

956
01:07:10,290 --> 01:07:11,340
this is

957
01:07:12,340 --> 01:07:13,770
less than or equal to

958
01:07:13,770 --> 01:07:15,130
c and

959
01:07:15,220 --> 01:07:16,900
notice the temptation

960
01:07:16,920 --> 01:07:21,330
it is to write that this equals big open square

961
01:07:21,350 --> 01:07:23,530
which is true for this one step

962
01:07:23,540 --> 01:07:25,320
c times m squared minus

963
01:07:25,340 --> 01:07:27,650
mines and what i mean these are both order and

964
01:07:27,670 --> 01:07:31,420
this is order and disorder and spread certainly this thing is order and square that's

965
01:07:31,420 --> 01:07:35,530
true but it's not completing the induction to complete the and actually have to prove

966
01:07:35,530 --> 01:07:38,710
thank you

967
01:07:52,250 --> 01:08:02,130
four of

968
01:09:54,120 --> 01:09:58,480
five for while

969
01:11:30,690 --> 01:11:33,100
o five

970
01:12:28,670 --> 01:12:36,950
well here

971
01:13:27,610 --> 01:13:37,170
he also worked on the

972
01:13:37,170 --> 01:13:42,880
thing here where we've found over x we can basically bring the summation

973
01:13:42,910 --> 01:13:47,430
inside here and just some you want over

974
01:13:52,970 --> 01:13:54,600
equation two

975
01:13:54,630 --> 01:13:57,520
and divided by equation three

976
01:13:57,560 --> 01:14:00,370
on the left-hand side what you get is

977
01:14:01,700 --> 01:14:06,780
joint probability of x y and be divided by the probability of y and b

978
01:14:06,810 --> 01:14:08,540
and we know that

979
01:14:09,010 --> 01:14:11,420
by the definition of

980
01:14:11,470 --> 01:14:14,410
conditional probability that the probability of x

981
01:14:14,420 --> 01:14:18,210
given y and b dividing this by this

982
01:14:19,200 --> 01:14:23,370
on the right-hand side when we divide this by this is what we get when

983
01:14:23,370 --> 01:14:26,210
we get whatever that cancelling

984
01:14:26,260 --> 01:14:30,310
we get this factor g cancelling

985
01:14:30,370 --> 01:14:33,730
and so we get this expression

986
01:14:33,770 --> 01:14:36,990
so what was the point of all that

987
01:14:37,040 --> 01:14:40,440
well the interesting thing about that expression is

988
01:14:42,390 --> 01:14:45,430
the right hand side of this expression

989
01:14:45,530 --> 01:14:49,350
is no longer functionally dependent on y

990
01:14:51,240 --> 01:14:52,920
this doesn't depend on y

991
01:14:52,930 --> 01:14:55,030
and from that it follows that

992
01:14:56,830 --> 01:14:59,050
it is independent of y

993
01:14:59,060 --> 01:15:01,100
given the

994
01:15:02,630 --> 01:15:05,780
this thing here

995
01:15:07,350 --> 01:15:12,310
no longer depends on y and so on

996
01:15:12,320 --> 01:15:14,570
we can

997
01:15:14,580 --> 01:15:16,840
basically defined that to be

998
01:15:16,860 --> 01:15:20,270
the probability of x given the

999
01:15:20,280 --> 01:15:23,430
and if we were to compute the probability that can be directly we get we

1000
01:15:23,430 --> 01:15:27,260
could write this equation is well from that

1001
01:15:27,270 --> 01:15:28,760
and therefore

1002
01:15:28,770 --> 01:15:35,300
the fac torisation we had in equation two implies the conditional independence in

1003
01:15:35,320 --> 01:15:37,840
in this statement one

1004
01:15:37,920 --> 01:15:43,460
right this is how we generally get from factorizations the probability distributions statements about conditional

1005
01:15:43,460 --> 01:15:48,170
independence and we we can apply this simple logical argument too

1006
01:15:48,170 --> 01:15:50,180
basically proof

1007
01:15:50,240 --> 01:15:53,980
essentially all the claims i'm going to make about conditional independence for

1008
01:15:54,230 --> 01:15:59,700
factor graphs directed graphs undirected graphs that in the same kind of reasoning

1009
01:16:00,530 --> 01:16:02,970
like we did in the previous slide

1010
01:16:03,030 --> 01:16:05,130
all right

1011
01:16:05,160 --> 01:16:09,550
so that was factor graphs let me now talk about undirected graphical models which

1012
01:16:09,680 --> 01:16:16,030
are older but very similar to factor that

1013
01:16:16,070 --> 01:16:18,690
so in an undirected graphical model

1014
01:16:18,760 --> 01:16:22,010
the joint probability over all variables can be written

1015
01:16:22,030 --> 01:16:24,080
in factored form

1016
01:16:24,100 --> 01:16:28,440
so we say the joint probability of all the variables represent that is the vector

1017
01:16:29,230 --> 01:16:31,660
going from one xk

1018
01:16:32,130 --> 01:16:37,190
is some normalisation constant times the product over j

1019
01:16:37,200 --> 01:16:39,260
of factors j

1020
01:16:39,280 --> 01:16:42,350
where each factor is just the function of

1021
01:16:43,510 --> 01:16:45,600
subset j

1022
01:16:45,620 --> 01:16:50,050
of the set of all variables one

1023
01:16:50,070 --> 01:16:53,770
so are subsets of the set of all variables

1024
01:16:53,770 --> 01:16:55,360
and the notation

1025
01:16:55,380 --> 01:16:57,140
x with

1026
01:16:57,190 --> 01:16:58,030
with the

1027
01:16:59,370 --> 01:17:02,000
it's one of the subset is just

1028
01:17:02,050 --> 01:17:08,310
think of it as the vector of all the axes that belong in that

1029
01:17:08,450 --> 01:17:12,250
that's what except cj

1030
01:17:12,250 --> 01:17:15,510
so here's how you specify a graph

1031
01:17:15,520 --> 01:17:17,820
an undirected graph starting from

1032
01:17:18,920 --> 01:17:22,460
knowledge that you're trying to publish crucial factors in this way

1033
01:17:22,950 --> 01:17:27,090
you create a node for each variable

1034
01:17:27,100 --> 01:17:31,500
and then you connect nodes i and k

1035
01:17:31,500 --> 01:17:33,620
if they exist

1036
01:17:33,670 --> 01:17:38,020
sorry if there exists some set cj such that both

1037
01:17:38,030 --> 01:17:43,880
i cj and k is change in other words if i and k participate in

1038
01:17:44,020 --> 01:17:50,530
common factor you connect directly to each other so in undirected graphical model doesn't have

1039
01:17:50,540 --> 01:17:52,730
two kinds of nodes is just

1040
01:17:52,780 --> 01:17:56,000
you know like we saw in the first slide

1041
01:17:56,070 --> 01:17:59,470
the nodes with undirected links between them

1042
01:18:00,380 --> 01:18:05,560
all of the variables in some subset will be connected to each other

1043
01:18:05,580 --> 01:18:09,710
so all the variables of the cj for example the connected to each other

1044
01:18:09,750 --> 01:18:13,720
and that means that these that are formed

1045
01:18:13,740 --> 01:18:18,020
cliques of the graph in other words they form a fully connected subgraph

1046
01:18:18,030 --> 01:18:20,760
of the whole graph

1047
01:18:21,410 --> 01:18:26,280
undirected graphical models are also called markov networks still hear that term

1048
01:18:26,330 --> 01:18:32,480
markov got his name associated with too many things i think so let's call the

1049
01:18:32,480 --> 01:18:35,620
undirected graphical models is the more descriptive than

1050
01:18:35,660 --> 01:18:37,260
markov network

1051
01:18:40,380 --> 01:18:42,950
so here's an undirected graphical model

1052
01:18:45,700 --> 01:18:49,660
the undirected graph here basically a third that

1053
01:18:49,700 --> 01:18:53,680
the joint probability of these variables factors into the product of

1054
01:18:53,730 --> 01:18:55,280
a function

1055
01:18:55,310 --> 01:19:03,030
this is clearly a c that simply connected subgraph this clique PCD another point directly

1056
01:19:03,030 --> 01:19:05,420
so i'll i give you a hint

1057
01:19:05,510 --> 01:19:08,320
so imagine that had vectors and

1058
01:19:08,410 --> 01:19:10,800
and that is that was wrong from

1059
01:19:10,840 --> 01:19:13,400
from the normal distribution

1060
01:19:13,440 --> 01:19:16,950
we here the identity matrix so we know how to sample that guy because we

1061
01:19:16,950 --> 01:19:21,190
just repeatedly sample of value for of its dimensions OK

1062
01:19:22,200 --> 01:19:27,220
if i if i multiply guy by matrix face a times OK and i call

1063
01:19:27,220 --> 01:19:28,450
this vector

1064
01:19:30,770 --> 01:19:36,720
OK what is the covariance matrix of u

1065
01:19:40,940 --> 01:19:44,410
so the covariance matrix of u

1066
01:19:44,450 --> 01:19:50,600
is equal to the times transpose OK

1067
01:19:52,610 --> 01:19:57,110
if that's the case now i almost know how to sample from from a multivariate

1068
01:19:57,110 --> 01:19:58,600
gaus right

1069
01:20:01,430 --> 01:20:06,860
indeed any two i need to sort of decompose my covariance matrix

1070
01:20:06,890 --> 01:20:10,710
in two into something that looks like

1071
01:20:10,950 --> 01:20:15,080
times and transpose OK the sort of the first

1072
01:20:15,100 --> 01:20:19,480
that's sort of the first step so which which the compositions could i take of

1073
01:20:19,480 --> 01:20:22,580
of the covariance matrix that would help me

1074
01:20:22,640 --> 01:20:24,510
as one of the slide

1075
01:20:24,560 --> 01:20:27,440
so i could take the i can the composition right but is there any other

1076
01:20:27,440 --> 01:20:43,740
thing i could do

1077
01:20:43,760 --> 01:20:46,490
i could take the cholesky decomposition for example

1078
01:20:46,510 --> 01:20:47,760
or i could take

1079
01:20:47,770 --> 01:20:49,280
some new form of

1080
01:20:49,300 --> 01:20:54,810
the matrix square root or basically anything that that the composer sigma into the product

1081
01:20:54,810 --> 01:20:55,670
of two

1082
01:20:56,960 --> 01:20:59,700
matrices modulo transposed right

1083
01:21:02,170 --> 01:21:06,700
the interesting thing so it's OK so let's just this is costly right i i

1084
01:21:06,700 --> 01:21:10,640
do this i i take the i can value decomposition right

1085
01:21:10,680 --> 01:21:12,540
and i take sort of

1086
01:21:12,560 --> 01:21:16,600
sort of half of it so to say right so i take the

1087
01:21:16,660 --> 01:21:20,300
i can vectors and i scaled them by

1088
01:21:20,340 --> 01:21:22,420
the square root of the eigen values

1089
01:21:22,430 --> 01:21:27,160
right and then what you get is again is orthogonal basis OK and now that

1090
01:21:27,160 --> 01:21:31,530
bayes actually is the base that is going to rotate my

1091
01:21:31,540 --> 01:21:32,590
sort of

1092
01:21:33,030 --> 01:21:34,920
my my round

1093
01:21:34,940 --> 01:21:41,400
is going to scale and rotated basically

1094
01:21:41,410 --> 01:21:44,020
so the interesting thing is that

1095
01:21:44,060 --> 01:21:45,220
this is actually

1096
01:21:46,530 --> 01:21:48,280
very related to PCA

1097
01:21:51,880 --> 01:21:56,050
so in which ways is related to PCA world

1098
01:21:56,060 --> 01:21:57,390
if we were to walk

1099
01:21:57,420 --> 01:22:02,420
backwards in a certain sense so now we're starting with the covariance matrix before we

1100
01:22:02,420 --> 01:22:05,440
see the data right was sort of say well that's the covariance matrix that i

1101
01:22:05,440 --> 01:22:09,050
want because this is the gas distribution i want to sample from right

1102
01:22:09,090 --> 01:22:13,310
but now things could be things could be different right i can sort of say

1103
01:22:13,350 --> 01:22:15,580
o i've seen as in data

1104
01:22:15,590 --> 01:22:19,460
OK now i compute the empirical covariance matrix of these data

1105
01:22:20,430 --> 01:22:23,540
and you know that this is the first that when you do PCA right this

1106
01:22:23,540 --> 01:22:26,640
second set when you PCA is you go on and do this

1107
01:22:27,770 --> 01:22:29,170
you give yourself

1108
01:22:29,220 --> 01:22:33,080
you sort of learn with the directions of maximum variance our

1109
01:22:33,090 --> 01:22:36,430
OK so this is like

1110
01:22:36,440 --> 01:22:46,990
using PCA backwards to sample from gauss

1111
01:22:54,020 --> 01:22:59,960
sampling from a gaussian is is nice it's it's exciting perhaps perhaps not but the

1112
01:22:59,960 --> 01:23:03,010
interesting thing is if we want to model data

1113
01:23:03,180 --> 01:23:08,310
what we want to do really is we're gonna see some data OK and when

1114
01:23:08,510 --> 01:23:12,930
we're going to ask ourselves what is the mean and the covariance matrix

1115
01:23:12,940 --> 01:23:17,160
one of the gaussians the generated that this data OK

1116
01:23:18,820 --> 01:23:21,230
you can you can simply

1117
01:23:21,260 --> 01:23:23,630
you can simply write the probability

1118
01:23:23,680 --> 01:23:24,770
of the data

1119
01:23:24,790 --> 01:23:29,730
given the specific choice of parameters for multivariate gaussians already the if we did this

1120
01:23:29,730 --> 01:23:33,230
and we saw three datasets and we tried three different values we can go in

1121
01:23:33,230 --> 01:23:36,600
and evaluate the likelihood

1122
01:23:36,630 --> 01:23:39,410
on the specific choice of parameters right

1123
01:23:39,420 --> 01:23:42,510
and now if we did this in this case we can see that

1124
01:23:42,560 --> 01:23:46,150
the model that has the highest likelihood is actually the one over there

1125
01:23:46,170 --> 01:23:48,780
it's not very surprising right

1126
01:23:48,840 --> 01:23:54,730
so using the likelihood function

1127
01:23:54,740 --> 01:23:58,030
what we can do

1128
01:23:58,070 --> 01:24:00,260
actually this

1129
01:24:02,570 --> 01:24:05,730
again in this case if you if you were to assume

1130
01:24:05,790 --> 01:24:08,410
that the samples have been drawn independently

1131
01:24:08,420 --> 01:24:11,310
you can just take

1132
01:24:13,770 --> 01:24:18,520
that of being wrong

1133
01:24:18,530 --> 01:24:21,760
and i started from section to section right

1134
01:24:21,780 --> 01:24:23,090
wise our

1135
01:24:23,110 --> 01:24:27,560
d dimensional vectors

1136
01:24:27,600 --> 01:24:33,760
so what would you do if it if that thing was an i i d

1137
01:24:33,760 --> 01:24:37,720
and what we want to learn is the properties of the function

1138
01:24:37,730 --> 01:24:40,710
OK so learning corresponds to finding

1139
01:24:40,720 --> 01:24:42,760
a good covariance function

1140
01:24:42,770 --> 01:24:47,120
this a little bit different from the way people talk about learning support vector machines

1141
01:24:47,160 --> 01:24:51,260
we think about learning support vector machines acted find the support vectors find a separating

1142
01:24:52,220 --> 01:24:53,730
for fixed

1143
01:24:55,130 --> 01:24:59,620
again this in this scenario in mind not finding the kernel

1144
01:24:59,900 --> 01:25:05,870
OK so so how would we actually do that so let's say we have covariance

1145
01:25:05,870 --> 01:25:10,450
functions that have some parameters and show them on the next slides if we had

1146
01:25:10,450 --> 01:25:14,890
that we have a bunch of parameters that just try to optimize the marginal likelihood

1147
01:25:14,890 --> 01:25:16,770
with respect the from

1148
01:25:16,790 --> 01:25:17,890
the computers

1149
01:25:19,220 --> 01:25:21,620
don't worry about it

1150
01:25:22,590 --> 01:25:28,350
let's try this example so you have the covariance function

1151
01:25:28,520 --> 01:25:32,910
which now has some promises and say that the green function here is it has

1152
01:25:32,910 --> 01:25:34,320
the same form

1153
01:25:34,330 --> 01:25:38,980
but i introduced a bunch of problems that is l

1154
01:25:38,980 --> 01:25:41,060
which is called the lengthscale

1155
01:25:41,080 --> 01:25:46,020
now also in the interest of the media was related to the amplitude of the

1156
01:25:46,020 --> 01:25:48,760
of the of the thing

1157
01:25:48,780 --> 01:25:50,930
now the covariance function

1158
01:25:50,970 --> 01:25:51,830
tells you

1159
01:25:51,850 --> 01:25:55,840
what is the covariance between the values

1160
01:25:55,850 --> 01:25:59,960
as a function of the input the function of the axis

1161
01:25:59,970 --> 01:26:02,750
what it tells you what the covariances between the

1162
01:26:05,140 --> 01:26:09,690
the function that we've been using so far as it has this is the squared

1163
01:26:09,690 --> 01:26:10,610
exponential form

1164
01:26:10,950 --> 01:26:14,750
the idea is that if x is close to each other

1165
01:26:14,770 --> 01:26:18,230
then the corresponding values should have high compare

1166
01:26:18,270 --> 01:26:20,720
it should have something to do with each other

1167
01:26:20,720 --> 01:26:23,020
whereas if the axes are often each other

1168
01:26:23,080 --> 01:26:26,700
then the the the covariance here is very close to zero

1169
01:26:27,150 --> 01:26:29,140
the corresponding eigenvalues

1170
01:26:29,170 --> 01:26:30,750
i don't have much to do with each other

1171
01:26:30,810 --> 01:26:34,020
could be independent

1172
01:26:34,700 --> 01:26:35,940
we haven't really

1173
01:26:35,950 --> 01:26:39,020
i he said well what do we mean by close what do we mean by

1174
01:26:39,020 --> 01:26:41,140
far away

1175
01:26:41,160 --> 01:26:45,880
and so i introduced from l here which called the lengthscale which tells me exactly

1176
01:26:45,890 --> 01:26:47,970
what do i mean by far away

1177
01:26:48,020 --> 01:26:50,150
so tell me scale

1178
01:26:50,150 --> 01:26:51,830
the on which to measure

1179
01:26:51,910 --> 01:26:55,240
distances between x and x y

1180
01:26:57,300 --> 01:26:58,970
so by doing this

1181
01:26:58,970 --> 01:27:02,250
i can see i can now say to my my learning algorithm i can say

1182
01:27:02,250 --> 01:27:07,330
to my process well i think underlying is my underlying function is small

1183
01:27:07,390 --> 01:27:10,700
but i'm not so sure exactly on what scale

1184
01:27:10,750 --> 01:27:12,770
this is something i would like to know

1185
01:27:12,780 --> 01:27:15,010
well this is something that we want to learn about

1186
01:27:15,060 --> 01:27:17,170
l three parameter here

1187
01:27:17,200 --> 01:27:19,700
and we want to do inference for

1188
01:27:19,850 --> 01:27:23,950
so similar to this problem is less interesting this just gives you it's me the

1189
01:27:23,950 --> 01:27:26,310
overall magnitude of the covariance

1190
01:27:26,330 --> 01:27:34,650
it's actually gives it may be the variance of signal so whereas the problem here

1191
01:27:34,750 --> 01:27:38,530
sort of structures the the the the x-factor the b

1192
01:27:38,640 --> 01:27:41,340
values of structures the y

1193
01:27:43,150 --> 01:27:44,820
let's try this out

1194
01:27:44,840 --> 01:27:47,220
let's try out doing inference

1195
01:27:47,220 --> 01:27:49,250
on a particular dataset twenty

1196
01:27:49,530 --> 01:27:50,990
data point here

1197
01:27:51,000 --> 01:27:55,030
now i just tried to do inference and i just want the mean function

1198
01:27:55,640 --> 01:27:57,250
the posterior mean function

1199
01:27:57,260 --> 01:28:00,920
based on the data points for different values of l

1200
01:28:00,920 --> 01:28:02,510
and that he will happen

1201
01:28:02,530 --> 01:28:05,510
OK so let's let me try first to take very short

1202
01:28:05,520 --> 01:28:10,030
thanks so else be a very small number

1203
01:28:10,050 --> 01:28:12,220
i forget exactly what number is

1204
01:28:12,220 --> 01:28:13,160
then i get

1205
01:28:13,170 --> 01:28:17,040
the following the following me but i get the red line

1206
01:28:17,190 --> 01:28:19,270
the red line kind of

1207
01:28:19,280 --> 01:28:21,850
kind of looks good from the point of view that it fits the data very

1208
01:28:22,510 --> 01:28:23,410
the goes

1209
01:28:23,820 --> 01:28:27,760
almost through every data point is very close to every data point

1210
01:28:27,810 --> 01:28:30,660
but i i and we might be worried about

1211
01:28:31,380 --> 01:28:36,110
about whether this is actually a good fit to the data might be that this

1212
01:28:36,130 --> 01:28:40,040
is actually it's actually picking up a lot of the noise in the data set

1213
01:28:40,190 --> 01:28:45,920
maybe we don't really believe that the thing is sensible

1214
01:28:47,250 --> 01:28:50,180
you might be happy about the fact that it cannot actually represent

1215
01:28:50,190 --> 01:28:52,510
something which is quite complicated

1216
01:28:52,530 --> 01:28:56,450
and that's why to pick out to be very long

1217
01:28:56,490 --> 01:28:58,610
we have very long lengthscale

1218
01:28:58,630 --> 01:29:00,180
in that case we get a little bit

1219
01:29:00,220 --> 01:29:04,470
the problem with the properties that doesn't really reflect the

1220
01:29:04,480 --> 01:29:08,690
the distribution of the data very well because it forces things that are quite far

1221
01:29:09,400 --> 01:29:10,710
to be highly correlated

1222
01:29:10,750 --> 01:29:14,420
because i think that things should be really far apart before the correlation

1223
01:29:14,490 --> 01:29:19,150
go away in that case itself struggling to model the data

1224
01:29:19,190 --> 01:29:22,730
and there's a good length scale the the green line here

1225
01:29:23,260 --> 01:29:25,090
this is actually the length scale

1226
01:29:25,180 --> 01:29:29,660
that optimizes that has the maximum value of the marginal likelihood

1227
01:29:29,660 --> 01:29:34,140
and that has that gives some of them more visually and more

1228
01:29:35,340 --> 01:29:37,750
it's the data

1229
01:29:37,770 --> 01:29:42,410
this comes close to something we're talking about our yesterday

1230
01:29:42,420 --> 01:29:46,550
that the mere fact that you can fit training data you could get too excited

1231
01:29:46,550 --> 01:29:47,770
about that

1232
01:29:47,790 --> 01:29:50,300
the red line actually fits much better

1233
01:29:50,310 --> 01:29:52,330
the marginal likelihood doesn't really like

1234
01:29:52,340 --> 01:29:55,330
but in favour that as well

1235
01:29:56,150 --> 01:30:00,280
but there's something wrong with your complexity in that

1236
01:30:00,400 --> 01:30:02,460
let's fine fine look at that

1237
01:30:04,310 --> 01:30:10,490
a little bit more than happy so again this plot showed before sure yesterday

1238
01:30:11,350 --> 01:30:12,530
in this case

1239
01:30:12,590 --> 01:30:14,410
yes it does

1240
01:30:14,970 --> 01:30:17,500
much more intuitive

1241
01:30:17,880 --> 01:30:23,130
nation of what these functions look like so again i have a much like here

1242
01:30:24,010 --> 01:30:29,490
i have all possible datasets on the x axis and today and even more abstract

1243
01:30:29,490 --> 01:30:33,070
representation because it's all possible

1244
01:30:33,540 --> 01:30:36,290
observations you could have for any function

1245
01:30:36,300 --> 01:30:38,450
this is that this is pretty weird

1246
01:30:38,640 --> 01:30:39,420
o thing

1247
01:30:39,470 --> 01:30:43,630
but the interesting thing is that the marginal likelihood is the probability distribution over the

1248
01:30:43,650 --> 01:30:47,110
data that the data sets of size

1249
01:30:47,170 --> 01:30:48,110
now the

1250
01:30:48,690 --> 01:30:50,940
the the two simple

1251
01:30:51,060 --> 01:30:54,430
the function here correspond to

1252
01:30:54,440 --> 01:30:59,730
situation where you can only model that very small subset of the data

1253
01:31:00,680 --> 01:31:05,470
for example happens for the line here so long lengthscale

1254
01:31:05,490 --> 01:31:10,900
somehow corresponds to a very small model class but you know it can only model

1255
01:31:11,390 --> 01:31:14,430
but extremely small things

1256
01:31:14,650 --> 01:31:20,010
whereas the red line here and do

1257
01:31:20,030 --> 01:31:24,090
and how can model of a lot of different functions can agree with allowing spend

1258
01:31:24,090 --> 01:31:26,780
a lot of different

1259
01:31:27,570 --> 01:31:28,570
in this blog

1260
01:31:28,610 --> 01:31:32,880
the columns don't match i realized that this party

1261
01:31:32,890 --> 01:31:35,240
it's a very weak line from before

1262
01:31:35,290 --> 01:31:36,260
and actually gives

1263
01:31:36,910 --> 01:31:41,970
and significantly nonzero marginal likelihood to lots and lots of different

1264
01:31:42,010 --> 01:31:43,490
but because of that

1265
01:31:43,510 --> 01:31:47,320
the the the actual value of this thing because it has to integrate to one

1266
01:31:47,380 --> 01:31:48,440
actually quite low

1267
01:31:48,480 --> 01:31:51,540
the when are of particular data set up here

1268
01:31:51,570 --> 01:31:54,090
the marginal likelihood is not that high

1269
01:31:54,090 --> 01:31:55,340
thank you guys

1270
01:31:55,400 --> 01:31:58,590
this is

1271
01:31:58,760 --> 01:32:02,640
joint work with alex is and higher jonathan gross and markus schulz

1272
01:32:04,080 --> 01:32:06,650
carnegie mellon

1273
01:32:06,660 --> 01:32:10,180
i'll start by

1274
01:32:12,770 --> 01:32:14,700
explained very

1275
01:32:14,710 --> 01:32:16,410
if you very simple

1276
01:32:16,570 --> 01:32:18,290
o thing we all know

1277
01:32:18,300 --> 01:32:20,340
we will have encountered

1278
01:32:20,360 --> 01:32:23,270
o finite support signals

1279
01:32:24,300 --> 01:32:31,360
our experience and we know that we can compute the discrete time fourier transform

1280
01:32:31,430 --> 01:32:33,030
of those

1281
01:32:34,210 --> 01:32:36,320
this formula right here

1282
01:32:36,390 --> 01:32:40,150
that is we can actually compute

1283
01:32:40,210 --> 01:32:46,970
this value by some polynomial whose

1284
01:32:46,990 --> 01:32:52,670
quite efficient are exactly the entries in the signal by developing this polynomial over the

1285
01:32:52,670 --> 01:32:54,560
unit circle

1286
01:32:55,560 --> 01:33:00,570
a finite version a finite representation of the

1287
01:33:01,480 --> 01:33:02,390
can be

1288
01:33:02,670 --> 01:33:06,120
computed by evaluating the same polynomial

1289
01:33:06,170 --> 01:33:08,870
only in the roots of unity

1290
01:33:09,480 --> 01:33:10,320
that's the

1291
01:33:10,340 --> 01:33:12,120
the discrete fourier transform

1292
01:33:12,480 --> 01:33:16,090
o of the signal

1293
01:33:16,140 --> 01:33:21,610
OK we can say that as the length of the signal goes to infinity the

1294
01:33:21,610 --> 01:33:27,200
discrete fourier transform approaches the discrete time fourier transform by the fact that

1295
01:33:27,210 --> 01:33:32,260
the sampling of the serious is finer and finer

1296
01:33:32,310 --> 01:33:33,900
OK but that's not all

1297
01:33:33,900 --> 01:33:36,670
usually when we talk about

1298
01:33:36,730 --> 01:33:43,150
four year transforms of the signals we assumed implicitly that

1299
01:33:43,170 --> 01:33:46,420
the signal is extended periodically

1300
01:33:46,430 --> 01:33:51,120
and as a result we have the circular convolution

1301
01:33:51,140 --> 01:33:56,400
and this in turn corresponds to see click

1302
01:33:56,420 --> 01:33:59,030
the cyclic boundary conditions so

1303
01:33:59,200 --> 01:34:03,970
this is exactly what's depicted in this picture so if you were to

1304
01:34:03,970 --> 01:34:07,740
find out what is the next value of the

1305
01:34:07,780 --> 01:34:11,490
o of the signal we could just turn back to

1306
01:34:11,500 --> 01:34:15,530
the left and the whole thing is being

1307
01:34:15,560 --> 01:34:16,590
OK so

1308
01:34:16,590 --> 01:34:20,430
the question that we're this here is how can we design transforms

1309
01:34:20,490 --> 01:34:22,340
that imply more general

1310
01:34:22,380 --> 01:34:26,550
other than periodic signal extensions of boundary conditions and

1311
01:34:26,650 --> 01:34:31,090
we can do that using the

1312
01:34:32,080 --> 01:34:36,900
the framework of algebraic theory of signal processing

1313
01:34:36,910 --> 01:34:42,060
and we will now show here how we can design a large class of such

1314
01:34:42,060 --> 01:34:43,370
transforms that

1315
01:34:43,400 --> 01:34:50,680
corresponding to non circular notion of coalition and singer extension

1316
01:34:53,130 --> 01:34:55,120
as the signal

1317
01:34:55,120 --> 01:34:57,810
o thing infinity they will approach the

1318
01:34:57,830 --> 01:34:59,430
discrete time free transfer

1319
01:34:59,490 --> 01:35:04,150
and they afford fast implementation or light blue

1320
01:35:04,160 --> 01:35:06,190
square them and as

1321
01:35:06,210 --> 01:35:10,300
we know they could even go to from the

1322
01:35:10,310 --> 01:35:11,740
classic free

1323
01:35:11,780 --> 01:35:13,180
OK so

1324
01:35:13,210 --> 01:35:18,160
what is this algebraic theory of signal processing

1325
01:35:19,490 --> 01:35:24,460
has been explained introduced in a paper large paper by by mark mark this challenge

1326
01:35:24,460 --> 01:35:26,740
is more in two thousand six

1327
01:35:26,750 --> 01:35:31,050
and now it has been published in the united drapery transactions of signal processing and

1328
01:35:31,050 --> 01:35:31,900
it has

1329
01:35:31,900 --> 01:35:33,000
the central

1330
01:35:33,300 --> 01:35:35,780
elements central concept

1331
01:35:35,840 --> 01:35:37,430
a set of filters

1332
01:35:39,500 --> 01:35:42,550
linear systems which form an algebra

1333
01:35:42,560 --> 01:35:47,810
now to run by the way is a vector space which is also a weak

1334
01:35:47,830 --> 01:35:50,210
so you can use the same

1335
01:35:50,210 --> 01:35:53,930
o operations in the UK you can use the addition

1336
01:35:55,690 --> 01:35:59,590
vector space and you can also multiply

1337
01:36:00,900 --> 01:36:05,520
and the other a central concept is the set of signals

1338
01:36:05,560 --> 01:36:07,810
which is an a module

1339
01:36:07,830 --> 01:36:10,060
and that means

1340
01:36:10,080 --> 01:36:11,550
it's also

1341
01:36:11,570 --> 01:36:18,340
the vector space but we can afford we can have an external operations from the

1342
01:36:18,340 --> 01:36:19,350
elements of

1343
01:36:19,480 --> 01:36:21,410
eight to the elements of m

1344
01:36:22,140 --> 01:36:23,520
so what are the

1345
01:36:23,530 --> 01:36:25,450
the notions

1346
01:36:25,460 --> 01:36:27,910
in signal processing

1347
01:36:27,940 --> 01:36:32,530
what is the correspondence with his algebraic setting

1348
01:36:32,530 --> 01:36:34,090
well start with

1349
01:36:34,150 --> 01:36:36,130
with the signal

1350
01:36:36,170 --> 01:36:42,760
namely for this module which is a vector space we can pick a basis

1351
01:36:42,800 --> 01:36:44,200
and we can see

1352
01:36:44,210 --> 01:36:48,020
signal as a linear combination of

1353
01:36:48,280 --> 01:36:50,450
elements in the basis and

1354
01:36:50,490 --> 01:36:53,290
we can actually

1355
01:36:53,400 --> 01:36:54,830
find more

1356
01:36:54,840 --> 01:37:00,350
the more concrete representation foresight to signal by taking the set of coefficients

1357
01:37:01,520 --> 01:37:05,580
so here the set of indices could be finite or infinite

1358
01:37:05,660 --> 01:37:08,710
this the filters

1359
01:37:08,770 --> 01:37:11,480
the elements of the algebra as i said before

1360
01:37:11,530 --> 01:37:15,080
and in coordinates there will be represented as matrices

1361
01:37:15,110 --> 01:37:19,980
and this makes sense is consistent with the fact that applying the filter two

1362
01:37:20,610 --> 01:37:25,340
signal you get back signal so applying matrix the vector given

1363
01:37:27,520 --> 01:37:28,940
the impulse

1364
01:37:30,340 --> 01:37:32,130
impulse response

1365
01:37:32,140 --> 01:37:34,490
our own

1366
01:37:34,540 --> 01:37:39,400
trivial you define what's more interesting is the notion of fourier transform

1367
01:37:39,400 --> 01:37:40,410
OK so

1368
01:37:40,590 --> 01:37:42,940
our module as

1369
01:37:43,470 --> 01:37:46,090
vector space can be written

1370
01:37:46,090 --> 01:37:47,840
as the direct sum

1371
01:37:48,700 --> 01:37:52,380
small dimensional vector spaces

1372
01:37:52,480 --> 01:37:54,340
and they are indexed by this

1373
01:37:54,360 --> 01:37:55,940
coming my here

1374
01:37:55,940 --> 01:38:01,020
lots of tables ready then we expect to see even more tables in the future

1375
01:38:01,020 --> 01:38:11,360
so so you can also work out the EPP F of the of

1376
01:38:11,360 --> 01:38:20,160
this two parameters CRP yes it has to be between zero and one we

1377
01:38:20,160 --> 01:38:26,180
know that if it's greater than one then this thing may not be well

1378
01:38:26,180 --> 01:38:40,680
defined if there's only one customer at the table yeah yeah

1379
01:38:40,680 --> 01:38:45,580
you can do that except that the model that you would get would not be

1380
01:38:45,580 --> 01:38:52,020
exchangeable anymore it was to be projected because this is a sequential process but

1381
01:38:52,020 --> 01:38:56,160
it won't be exchangeable and if you want exchangeability then so far as we

1382
01:38:56,160 --> 01:39:00,220
know this is probably the most general form that people have worked with anyways there

1383
01:39:00,220 --> 01:39:05,780
is kind of a generalization of this Pitman Yor process called species sampling models which the

1384
01:39:05,780 --> 01:39:10,060
probability theories have looked at and that gives us that gives you a bit more

1385
01:39:10,060 --> 01:39:28,480
flexibility so we can show that the EPP of this two parameters CRP is of this form

1386
01:39:28,480 --> 01:39:32,880
and you can also see that it will be projective because it's sequential and it

1387
01:39:32,880 --> 01:39:39,660
will be exchangeable because this EPPF doesn't depend on the way that the customers

1388
01:39:39,660 --> 01:39:42,920
are labelled it only depends on the sizes of the clusters and the number of

1389
01:39:42,920 --> 01:39:49,890
clusters and again because is projective and exchangeable we can apply De Finetti's theorem and

1390
01:39:50,140 --> 01:39:55,800
De Finetti0s measure that corresponds to this process will be the Pitman Yor process and this

1391
01:39:55,800 --> 01:40:04,000
is a generalization of the Dirichlet process I guess the idea of power-law properties was

1392
01:40:04,010 --> 01:40:12,820
really mentioned a little bit yes yes for clustering

1393
01:40:13,060 --> 01:40:18,780
type of problems it seems that you probably do want to use a

1394
01:40:18,780 --> 01:40:22,800
Dirichlet process over Pitman Yor process unless you do believe that there are huge number

1395
01:40:22,800 --> 01:40:32,880
of very small clusters they do have

1396
01:40:32,880 --> 01:40:38,900
different effects so alpha just changes the overall number of clusters the D parameter

1397
01:40:38,900 --> 01:40:44,000
we'll come to this actually in the next two slides will affect the ratio of

1398
01:40:44,000 --> 01:40:48,760
the number of big clusters to the number of small clusters so the larger D is

1399
01:40:48,760 --> 01:40:55,280
the more there will be very small clusters there will be lots of very small clusters yes

1400
01:40:55,520 --> 01:41:06,740
sorry think of D as a regulizer I tend to think of

1401
01:41:07,340 --> 01:41:15,860
D as a way of putting a prior over the kind of over the data basically

1402
01:41:15,860 --> 01:41:20,980
you're kind of saying that you expect your data to x exhibit power-law properties and

1403
01:41:21,000 --> 01:41:26,260
D actually controls the power-law the structure of the power-law so I tend to think

1404
01:41:26,270 --> 01:41:32,960
of it as a prior instead of a regularization right so it's again

1405
01:41:32,970 --> 01:41:37,880
related to this idea that we have a rich richer property of the Pitman

1406
01:41:37,880 --> 01:41:43,680
Yor and a property that if we have lots of tables then we tend to

1407
01:41:43,680 --> 01:41:48,440
see more new tables okay so both of his properties if you put them together you

1408
01:41:48,440 --> 01:41:52,860
find that there will be lots of tables with a very small number of customers

1409
01:41:52,860 --> 01:41:57,000
because if you have lots of tables then you have to have necessary lots of small tables

1410
01:41:57,000 --> 01:42:03,960
beca you because you only have a fixed number N of customs and it turns

1411
01:42:03,960 --> 01:42:09,320
out that if you combine these two to together then you get actually a power-law property

1412
01:42:09,320 --> 01:42:13,400
of Pitman Yor processes okay and the nice thing with this is that because there's lots

1413
01:42:13,400 --> 01:42:19,740
of natural phenomena that has power-law properties it's makes it quite a natural model to use

1414
01:42:19,740 --> 01:42:24,420
so far as I know I think people mentioned that they hadn't really explored models

1415
01:42:24,460 --> 01:42:31,300
for that have this sort of power-law property prior to this this rediscovery of the Pitman

1416
01:42:31,300 --> 01:42:39,320
Yor process Dirichlet learning community so here's kind of a demonstration of the

1417
01:42:39,320 --> 01:42:45,100
power-law property so this is the original Dirichlet process where D equals to zero

1418
01:42:45,100 --> 01:42:51,100
and we see that the number of tables grows quite slowly okay here we have

1419
01:42:51,100 --> 01:42:55,190
a Pitman Yor process where the D parameter's set to a half and alpha is

1420
01:42:55,190 --> 01:43:01,420
set to one and we see that the number of tables grows quite quickly

1421
01:43:01,420 --> 01:43:06,440
compared to the a number of set alpha to be one here otherwise

1422
01:43:06,440 --> 01:43:09,100
it would have blown up to be a few thousand tables instead of a few

1423
01:43:09,100 --> 01:43:14,560
hundred tables okay and we see that there's lots of this tables out here where they

1424
01:43:14,560 --> 01:43:18,240
are very sparse and what that says is that the tables has very small number

1425
01:43:18,240 --> 01:43:18,830
i think we should

1426
01:43:19,430 --> 01:43:21,220
we try to prove this one as well

1427
01:43:22,760 --> 01:43:24,190
this one hand

1428
01:43:25,490 --> 01:43:25,820
this one

1429
01:43:26,430 --> 01:43:26,860
so let's

1430
01:43:28,610 --> 01:43:32,070
well it's actually if you want to control them but maybe down here gets a

1431
01:43:32,070 --> 01:43:34,070
bit more missus and this one is relatively difficult

1432
01:43:36,160 --> 01:43:38,420
this one is and what will the mathematicians

1433
01:43:40,040 --> 01:43:43,660
this is easy this is easy but so let's say try to

1434
01:43:44,580 --> 01:43:45,290
to some of those

1435
01:43:46,560 --> 01:43:47,070
up to here

1436
01:43:47,790 --> 01:43:49,130
the interplay between the definition

1437
01:43:49,780 --> 01:43:51,310
end i have some hints here

1438
01:43:53,130 --> 01:43:54,770
maybe for the first one i don't give you a hint

1439
01:43:55,660 --> 01:43:57,770
therefore the second one had this hint written here

1440
01:43:58,710 --> 01:44:01,800
so the hint is that you have to write down the ground matrix remember the

1441
01:44:01,800 --> 01:44:04,290
gram matrix or kernel matrix is the matrix of

1442
01:44:04,930 --> 01:44:06,970
the dot product between all training points

1443
01:44:07,610 --> 01:44:10,920
he the training points on the x and ex-prime minister two by two matrix

1444
01:44:11,500 --> 01:44:12,370
and then think about

1445
01:44:12,950 --> 01:44:18,410
the above properties this this matrix sorry determinant have in the case that the matrix is positive definite

1446
01:44:22,190 --> 01:44:22,910
the hint is

1447
01:44:23,340 --> 01:44:26,830
you can prove this one by using this one so it has proved this one

1448
01:44:29,470 --> 01:44:30,980
these to our basic elementary

1449
01:44:31,550 --> 01:44:33,920
by substituting into the definition so i'll give you

1450
01:44:34,390 --> 01:44:37,960
another few minutes and and play around a bit between it and then we'll see

1451
01:44:58,690 --> 01:45:02,390
maybe taking how many of you have solved three or more problems

1452
01:45:04,580 --> 01:45:05,810
how many have two or more

1453
01:45:07,870 --> 01:45:08,450
one or more

1454
01:45:14,960 --> 01:45:15,670
zero more

1455
01:45:18,730 --> 01:45:21,500
you're not answering my questions i

1456
01:45:23,530 --> 01:45:25,780
okay i give it to my minutes and then we'll do that here

1457
01:45:34,090 --> 01:45:38,030
looking at that's the first one and i want ask a question i wanted

1458
01:45:45,190 --> 01:45:50,490
and so the case that in all these cases are positive case have enough positive definite kernels

1459
01:45:53,030 --> 01:45:54,160
so that's the assumption

1460
01:45:55,450 --> 01:45:56,890
it's a sequence of all the

1461
01:45:58,140 --> 01:45:59,420
problem of positive definite

1462
01:46:00,330 --> 01:46:01,930
and if the limit exists but

1463
01:46:02,480 --> 01:46:03,970
i mean you don't have to prove this one

1464
01:46:04,390 --> 01:46:08,740
if you if you go up real it's enough and we went gonna need this one still here

1465
01:46:10,780 --> 01:46:13,210
so that's the first one the first and we're actually going to need

1466
01:46:14,530 --> 01:46:16,000
someone have an idea how to do it

1467
01:46:17,280 --> 01:46:19,510
you can either do it he'll just tell me and i'll do it

1468
01:46:21,680 --> 01:46:23,400
i'm sure some people have solved the first one

1469
01:46:31,950 --> 01:46:32,690
you speak louder

1470
01:46:40,850 --> 01:46:42,940
okay so i think i i think you're saying

1471
01:46:43,920 --> 01:46:45,650
one of is to be one and end

1472
01:46:46,170 --> 01:46:46,970
everything else zero

1473
01:46:48,000 --> 01:46:49,840
more equivalently you could say

1474
01:46:50,380 --> 01:46:50,760
you just

1475
01:46:51,240 --> 01:46:52,580
i want to take one data point

1476
01:46:54,080 --> 01:46:55,150
right because here

1477
01:46:56,800 --> 01:46:57,630
and we say that

1478
01:46:59,760 --> 01:47:00,940
with the laser pointer again

1479
01:47:01,590 --> 01:47:05,450
any set of training points so in particular it could be said that only is one element

1480
01:47:06,990 --> 01:47:07,840
so that's the that's

1481
01:47:11,980 --> 01:47:15,360
so we take only one element so the sum goes only from one to one

1482
01:47:15,890 --> 01:47:16,870
we can remove the sum

1483
01:47:17,720 --> 01:47:19,440
we only have one way we set it to one

1484
01:47:19,990 --> 01:47:22,980
so we get one times one times payoff

1485
01:47:23,910 --> 01:47:24,390
x one

1486
01:47:24,950 --> 01:47:25,410
x one

1487
01:47:27,520 --> 01:47:28,110
it's positive

1488
01:47:29,020 --> 01:47:29,850
so we done

1489
01:47:30,890 --> 01:47:32,410
so the first one was easy it

1490
01:47:32,930 --> 01:47:36,240
and it means that the diagonal elements of such income

1491
01:47:37,030 --> 01:47:40,310
and of course also for kernel matrix are always non-negative

1492
01:47:41,240 --> 01:47:44,510
so if you have negative diamonds you know it's not a positive definite kernel

1493
01:47:45,410 --> 01:47:48,560
so let's look at the second one which is a generalization of the

1494
01:47:48,980 --> 01:47:52,260
cushy schwartz inequality that you probably know from dot products

1495
01:47:52,840 --> 01:47:55,120
so any ideas how to prove the second one

1496
01:48:00,140 --> 01:48:02,440
this fact

1497
01:48:09,650 --> 01:48:11,630
so that you construct the gram matrix four

1498
01:48:12,710 --> 01:48:13,430
the training points

1499
01:48:15,810 --> 01:48:16,640
x x prime

1500
01:48:19,990 --> 01:48:21,670
so let's write down a grammar tricks

1501
01:48:33,170 --> 01:48:33,710
okay and now

1502
01:48:36,890 --> 01:48:38,760
okay gram matrix is positive definite

1503
01:48:44,500 --> 01:48:45,390
is non-negative front

1504
01:48:46,360 --> 01:48:50,320
so the gram matrix is positive definite in our notation

1505
01:48:50,950 --> 01:48:55,010
which still includes i value zero therefore the determinant is non-negative

1506
01:48:57,400 --> 01:48:59,910
straight line is that the term is nonnegative and then

1507
01:49:00,710 --> 01:49:02,140
we calculate the determinant

1508
01:49:03,720 --> 01:49:06,350
so the determinant is this times this

1509
01:49:20,390 --> 01:49:20,900
anything else

1510
01:49:25,300 --> 01:49:28,720
this is greater than zero and now we have to use the fact that case symmetric

1511
01:49:29,880 --> 01:49:30,820
so this is actually

1512
01:49:33,550 --> 01:49:35,170
okay x x prime squared

1513
01:49:36,170 --> 01:49:38,430
and now if we take a shot look at

1514
01:49:39,020 --> 01:49:40,010
what we have here

1515
01:49:40,650 --> 01:49:41,020
it should be

1516
01:49:41,480 --> 01:49:41,800
the same

1517
01:49:42,800 --> 01:49:44,390
so we move this to the other side

1518
01:49:45,140 --> 01:49:48,100
and then we have the upper bound which is this thing here

1519
01:49:49,170 --> 01:49:49,840
it's quite okay

1520
01:49:50,420 --> 01:49:51,070
any questions

1521
01:49:53,190 --> 01:49:54,640
so let's do the next one

1522
01:49:56,380 --> 01:49:57,770
any ideas hard so the next one

1523
01:50:01,700 --> 01:50:02,870
also from the determinant

1524
01:50:03,560 --> 01:50:04,230
so you are

1525
01:50:06,270 --> 01:50:09,590
we want to compute the determinant or you want to what you want to use this one

1526
01:50:11,510 --> 01:50:12,780
you can use the same one right you can

1527
01:50:14,400 --> 01:50:15,010
so you can

1528
01:50:16,240 --> 01:50:16,950
you can simply say

1529
01:50:19,100 --> 01:50:20,820
i subset i write this one down

1530
01:50:21,310 --> 01:50:23,360
well maybe i don't well let's let's write it

1531
01:50:24,210 --> 01:50:26,430
so in the general case we have we know

1532
01:50:27,150 --> 01:50:27,990
this is true now

1533
01:50:38,240 --> 01:50:39,270
and we know that this is true

1534
01:50:40,410 --> 01:50:43,390
now if it's the case that cave x x is zero for all x

1535
01:50:43,900 --> 01:50:46,010
in particular is also easier for x x prime

1536
01:50:46,510 --> 01:50:48,490
it doesn't matter anyway this thing is zero

1537
01:50:53,520 --> 01:50:56,880
so the we have an upper bound on the square thing which is zero

1538
01:50:57,360 --> 01:50:59,730
therefore the thing itself is also zero

1539
01:51:10,150 --> 01:51:11,410
should we do they other ones as well

1540
01:51:14,510 --> 01:51:17,080
not x i think it might not be using them but

1541
01:51:18,080 --> 01:51:20,300
relatively easy so we can briefly to them

1542
01:51:21,800 --> 01:51:22,880
i five times kate

1543
01:51:24,400 --> 01:51:25,140
how do we prove that's

1544
01:51:29,490 --> 01:51:31,080
you know

1545
01:51:31,980 --> 01:51:32,940
is okay

1546
01:51:35,370 --> 01:51:35,520
with the

1547
01:51:40,320 --> 01:51:41,090
we you

1548
01:51:42,410 --> 01:51:46,350
okay so he's saying it's easier to do all this for visualizing the archaeologists

1549
01:51:46,810 --> 01:51:48,050
they follow directly so

1550
01:51:48,830 --> 01:51:52,300
there's a few additional within the arcade maybe that's the easiest way

1551
01:51:54,210 --> 01:51:56,210
only one one note of caution

1552
01:51:56,210 --> 01:51:59,070
radiation will go through beautifully

1553
01:51:59,070 --> 01:52:03,330
and this will then be the value of c

1554
01:52:03,380 --> 01:52:04,840
the value of k

1555
01:52:05,870 --> 01:52:07,450
is negotiable

1556
01:52:07,470 --> 01:52:09,380
that value for kx

1557
01:52:09,390 --> 01:52:13,530
must be the value that i had earlier where is it that must be by

1558
01:52:13,570 --> 01:52:15,550
divided by

1559
01:52:15,560 --> 01:52:17,890
eight let me write it down again

1560
01:52:18,030 --> 01:52:20,570
so for any equals one

1561
01:52:20,630 --> 01:52:22,670
k of x

1562
01:52:22,720 --> 01:52:24,250
must be by

1563
01:52:24,300 --> 01:52:27,360
so far what this crazy

1564
01:52:27,380 --> 01:52:29,420
that just itself

1565
01:52:29,440 --> 01:52:33,290
sort it meets the boundary conditions kx

1566
01:52:33,320 --> 01:52:37,200
does not just itself kx has no choice

1567
01:52:37,220 --> 01:52:41,630
kx must meet the boundary conditions so that the vector

1568
01:52:41,630 --> 01:52:44,390
vanishes is actually closer when x equals

1569
01:52:44,460 --> 01:52:45,640
and so

1570
01:52:46,420 --> 01:52:51,160
solution must lie on this curve and so it kesey that pays the price

1571
01:52:51,210 --> 01:52:52,140
and that

1572
01:52:52,150 --> 01:52:54,590
just itself

1573
01:52:54,620 --> 01:52:59,130
so keep in mind that all radiation with frequency above this value you

1574
01:52:59,140 --> 01:53:01,130
i can go through this gap

1575
01:53:01,180 --> 01:53:06,470
there is often a misunderstanding among students they think that they are resonance frequencies that

1576
01:53:06,470 --> 01:53:08,300
i know resonance frequencies

1577
01:53:08,340 --> 01:53:11,600
any frequency above that value can go

1578
01:53:11,610 --> 01:53:13,960
through the get

1579
01:53:13,980 --> 01:53:16,130
you can immediately see here

1580
01:53:16,130 --> 01:53:19,310
that's the phase velocity is larger than c

1581
01:53:19,320 --> 01:53:22,870
because the phase velocity is only god divided by kesey

1582
01:53:22,940 --> 01:53:25,080
so when i draw this line

1583
01:53:25,090 --> 01:53:27,870
then this slope here

1584
01:53:27,930 --> 01:53:32,810
omega divided by cases that slope is larger than this flow

1585
01:53:32,870 --> 01:53:34,070
and that

1586
01:53:34,120 --> 01:53:35,500
correspond to

1587
01:53:35,530 --> 01:53:36,830
speed c

1588
01:53:36,840 --> 01:53:38,870
so you see things this slope

1589
01:53:38,920 --> 01:53:40,360
this deeper

1590
01:53:40,400 --> 01:53:43,020
the phase velocity is larger than c

1591
01:53:43,060 --> 01:53:46,910
and you see that when you read the situation that casey becomes zero

1592
01:53:46,940 --> 01:53:48,030
that then

1593
01:53:48,030 --> 01:53:49,300
you have

1594
01:53:49,350 --> 01:53:54,780
this is only god divided omega a case zero and so when you reach

1595
01:53:54,830 --> 01:53:55,980
this crazy

1596
01:53:56,000 --> 01:53:58,470
cutoff frequency the phase velocity

1597
01:53:58,520 --> 01:54:00,420
goes to infinity

1598
01:54:00,590 --> 01:54:03,620
also notice that the tangent here

1599
01:54:03,630 --> 01:54:05,800
is the group velocity

1600
01:54:05,880 --> 01:54:07,940
that is the only god k

1601
01:54:07,960 --> 01:54:11,230
and notice that this slope is smaller than this one

1602
01:54:11,310 --> 01:54:15,290
that is why the group velocity is lower than c

1603
01:54:15,300 --> 01:54:18,410
and also notice that when you reach this kind of frequency

1604
01:54:18,430 --> 01:54:20,450
that the tangent is like this

1605
01:54:20,490 --> 01:54:23,320
that means the group velocity is zero

1606
01:54:23,350 --> 01:54:24,580
nothing can go

1607
01:54:25,250 --> 01:54:32,960
the gap anymore

1608
01:54:32,970 --> 01:54:36,670
so i now want to write down for you

1609
01:54:36,690 --> 01:54:41,820
something that may help you later if you want to

1610
01:54:41,820 --> 01:54:45,200
i understand you notes

1611
01:54:45,220 --> 01:54:47,370
i'll put this back up again

1612
01:54:47,400 --> 01:54:51,180
i'm going to do the following experiment always you in my head

1613
01:54:51,220 --> 01:54:54,530
i started with a certain frequency omega

1614
01:54:54,570 --> 01:54:55,880
and i'm going to

1615
01:54:55,900 --> 01:55:00,780
lower that frequency i do in my habits duncan experience going to write down step-by-step

1616
01:55:00,780 --> 01:55:02,870
what's going to happen

1617
01:55:02,880 --> 01:55:06,570
this point will slow to go down and so we will come down this line

1618
01:55:08,000 --> 01:55:12,370
and finally we which here

1619
01:55:12,420 --> 01:55:15,130
i probably can write it yet it's is nicer for you

1620
01:55:15,210 --> 01:55:18,110
because then you have it all together

1621
01:55:18,190 --> 01:55:22,810
so i started his omega which is larger than omega c that's how i start

1622
01:55:22,890 --> 01:55:24,500
to give

1623
01:55:24,560 --> 01:55:27,170
and i'm going to law omega

1624
01:55:27,190 --> 01:55:29,410
the next thing that i'm going to do

1625
01:55:29,420 --> 01:55:32,830
it's going to look at the moment

1626
01:55:32,830 --> 01:55:35,190
when when you lonely guy

1627
01:55:42,200 --> 01:55:44,870
it must remain quiet over a

1628
01:55:44,880 --> 01:55:48,850
because it must meet the boundary conditions

1629
01:55:48,850 --> 01:55:51,160
therefore case gets smaller

1630
01:55:51,300 --> 01:56:00,080
going down this line indicates he gets more

1631
01:56:00,100 --> 01:56:02,550
only guys casing

1632
01:56:02,600 --> 01:56:04,800
always k time c

1633
01:56:04,810 --> 01:56:07,660
so clearly if only guy comes down

1634
01:56:07,720 --> 01:56:09,640
OK must come down

1635
01:56:09,680 --> 01:56:14,350
and that means kesey comes down in this case because kx is not going to

1636
01:56:14,350 --> 01:56:16,310
give up

1637
01:56:16,380 --> 01:56:19,660
and so came gets more

1638
01:56:19,820 --> 01:56:25,370
and if k get smaller

1639
01:56:25,380 --> 01:56:26,450
then of course

1640
01:56:31,870 --> 01:56:36,090
let those two pi over k

1641
01:56:37,720 --> 01:56:39,370
disaster strikes

1642
01:56:39,630 --> 01:56:42,850
o god becomes omega c

1643
01:56:42,850 --> 01:56:45,110
and when you reach that point

1644
01:56:45,120 --> 01:56:47,430
casey has become zero

1645
01:56:47,450 --> 01:56:50,480
and kx is still built by over a

1646
01:56:50,560 --> 01:56:52,020
OK places

1647
01:56:52,030 --> 01:56:53,130
there is now zero

1648
01:56:53,140 --> 01:56:54,300
and kx

1649
01:56:54,470 --> 01:56:56,530
still pi over a

1650
01:56:56,600 --> 01:56:58,610
so now k itself

1651
01:56:58,670 --> 01:57:00,820
this kx

1652
01:57:00,860 --> 01:57:02,560
and several other now

1653
01:57:02,580 --> 01:57:03,610
is two pi

1654
01:57:03,620 --> 01:57:04,870
divided by

1655
01:57:04,890 --> 01:57:07,550
that value kx

1656
01:57:07,660 --> 01:57:08,670
and that means

1657
01:57:08,720 --> 01:57:12,530
two pi divided by kx means that lambda them now

1658
01:57:12,540 --> 01:57:13,420
he calls

1659
01:57:13,440 --> 01:57:14,530
two a

1660
01:57:14,600 --> 01:57:24,380
it means if i have a different frequencies

1661
01:57:24,380 --> 01:57:28,610
which we do have here which i cannot change i have ten gigahertz

1662
01:57:28,700 --> 01:57:33,180
and get pending covers has the wavelengths of three centimetres in fact

