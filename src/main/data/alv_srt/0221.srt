1
00:00:00,000 --> 00:00:03,720
so now what is the definition of a gas process so here's the definition of

2
00:00:03,720 --> 00:00:09,160
gas across a collection of random variables any finite number of which have guassian distributions

3
00:00:09,160 --> 00:00:12,170
OK so this is the sort of mathematics mathematicians

4
00:00:12,270 --> 00:00:17,500
definition here to mathematicians always very cautious when we talk about infinite things here so

5
00:00:17,500 --> 00:00:21,380
they don't actually say it's an infinite collection of random variables that have joint council

6
00:00:21,400 --> 00:00:25,870
distribution maybe that doesn't exist whatever that might mean right they just any finite number

7
00:00:25,870 --> 00:00:30,610
of them should have gotten distributions and then there's OK so there there there

8
00:00:30,620 --> 00:00:37,620
there was a bit fidity when when it comes to to infinity right yes

9
00:00:38,540 --> 00:00:43,990
i don't think it matters i think we we don't need to be i'm not

10
00:00:43,990 --> 00:00:47,660
going to look into the to the mathematical details i'm trying to get give you

11
00:00:48,050 --> 00:00:51,900
a high-level intuition here right i think it's uncountably infinite

12
00:00:52,160 --> 00:00:56,070
is is it probably right the right answer

13
00:00:56,170 --> 00:00:58,570
OK so gas industry gas and process

14
00:00:58,670 --> 00:01:03,630
this is somewhat mysterious object right just think of it as being very large gaussians

15
00:01:03,660 --> 00:01:06,380
OK now analogy would be good enough

16
00:01:06,390 --> 00:01:08,890
OK so let's see what happens

17
00:01:08,990 --> 00:01:11,950
so whereas the gas in distribution

18
00:01:12,000 --> 00:01:14,290
is fully specified by a mean vector

19
00:01:14,290 --> 00:01:18,220
and the covariance matrix so it's it's like this so you have the gas in

20
00:01:18,500 --> 00:01:23,900
a joint council distribution over over d dimensional object are like this

21
00:01:24,060 --> 00:01:28,080
we have the mean vector and covariance matrix in the mean vector will have the

22
00:01:28,080 --> 00:01:33,870
same length as the number of of dimensions of and the covariance matrix will be

23
00:01:33,870 --> 00:01:37,060
a matrix which has size d by d

24
00:01:37,060 --> 00:01:39,190
in that d dimensional space

25
00:01:39,370 --> 00:01:44,370
so what happens if we tried to lift this up to infinite dimensional thing

26
00:01:45,020 --> 00:01:48,840
now what is the vector what is the mean vector turn into if you want

27
00:01:48,840 --> 00:01:52,050
to try to follow this analogy

28
00:01:52,950 --> 00:01:56,740
it turns into function right you need an infinitely long

29
00:01:56,750 --> 00:02:00,720
i mean vector and we think of infinitely long vectors is being functions right so

30
00:02:00,730 --> 00:02:05,170
the mean of the gas and process is a mean function

31
00:02:05,180 --> 00:02:12,420
and how the covariance matrix

32
00:02:12,430 --> 00:02:16,140
that's right it's the function is a function of two variables right

33
00:02:16,140 --> 00:02:20,740
so just like you normally think of this group of covariance matrix is being indexed

34
00:02:20,740 --> 00:02:24,980
by the dimension now this is just doing exactly the same thing but the index

35
00:02:24,980 --> 00:02:30,180
set here is the other values of x which happens to be the inputs to

36
00:02:30,940 --> 00:02:34,610
regression thing we that we're trying to do

37
00:02:34,660 --> 00:02:35,880
OK so

38
00:02:35,880 --> 00:02:37,930
so so so maybe

39
00:02:38,220 --> 00:02:41,750
i want to try to prove this that the this thing actually really exist but

40
00:02:41,750 --> 00:02:45,900
if it exists it should definitely have this kind of four right should be you

41
00:02:45,900 --> 00:02:50,000
know very long mean on here in this this two-dimensional

42
00:02:50,670 --> 00:02:54,430
the function of two arguments

43
00:02:55,970 --> 00:02:57,230
OK so

44
00:02:58,690 --> 00:03:01,580
let's worry a little bit about this thing about it being infinitely big so we

45
00:03:01,580 --> 00:03:02,610
can write it down

46
00:03:05,130 --> 00:03:09,810
so we might this might be totally impractical but it turns out that it's not

47
00:03:09,810 --> 00:03:14,400
totally impractical and the reason for that is because of the marginalisation property so remember

48
00:03:14,400 --> 00:03:16,680
that if you have a joint gaussians

49
00:03:16,700 --> 00:03:21,070
then you find the marginal distribution for some of the variables by just integrating out

50
00:03:21,080 --> 00:03:22,920
the ones that you're not interested in

51
00:03:22,940 --> 00:03:28,210
in particular if i had joined get joint distribution between x's and y is here

52
00:03:28,210 --> 00:03:30,970
and the and had

53
00:03:30,980 --> 00:03:34,980
and the mean for four x would be a and the covariance of of x

54
00:03:34,980 --> 00:03:40,420
would be capital and the cross covariance between x and y is here so i

55
00:03:40,430 --> 00:03:44,500
think of this is being let's think of this as being being used in the

56
00:03:44,500 --> 00:03:46,110
case where the

57
00:03:46,120 --> 00:03:48,880
i mean vector is infinitely long line

58
00:03:48,900 --> 00:03:52,710
if i now happen to be interested in only a finite number of these entries

59
00:03:53,190 --> 00:03:56,820
that could just rearrange this thing to say well let's put the a finite number

60
00:03:56,820 --> 00:04:00,630
of indexes up here and call them x and then

61
00:04:01,240 --> 00:04:04,910
infinitely many other indexes that i'm not interested in i put them down to be

62
00:04:06,630 --> 00:04:11,420
say well what's the distribution over x well distribution axis is just this

63
00:04:11,970 --> 00:04:14,660
good fashioned gas in distribution

64
00:04:14,690 --> 00:04:17,730
it only depends on things that are

65
00:04:18,130 --> 00:04:23,160
that that i find in particular if you had the entries here then this would

66
00:04:23,160 --> 00:04:24,720
be one by d

67
00:04:24,730 --> 00:04:26,930
that in nineteen

68
00:04:26,950 --> 00:04:30,310
covariance matrix

69
00:04:30,320 --> 00:04:33,850
OK so this is essentially why it's possible

70
00:04:33,860 --> 00:04:39,470
two to work with these things without using infinitely many

71
00:04:40,060 --> 00:04:42,430
in fact the infinite amounts of memory

72
00:04:42,450 --> 00:04:45,350
OK so let's look at an example of one of these things what these things

73
00:04:45,350 --> 00:04:47,650
actually look like so here's

74
00:04:47,690 --> 00:04:53,160
is it is it is the one-dimensional gaussianprocess so x is the one-dimensional they're all

75
00:04:53,160 --> 00:04:57,430
here so i choose the gas process the distribution over these functions so i have

76
00:04:57,670 --> 00:04:59,150
a probability distribution over functions now

77
00:04:59,540 --> 00:05:03,400
which is gaussianprocess here and have a mean function

78
00:05:03,420 --> 00:05:05,160
and i'm just going to use the function zero

79
00:05:05,160 --> 00:05:10,990
good news that throughout but there's no reason why you should always do that but

80
00:05:10,990 --> 00:05:13,960
that's the simplest thing you can choose and here's is the covariance function

81
00:05:13,970 --> 00:05:19,070
and the covariance function i'm going to use the this function here

82
00:05:19,070 --> 00:05:24,420
so you sample entries uniformly at random from the and MSP the matrix and then

83
00:05:24,420 --> 00:05:27,430
we can show that with very high probability

84
00:05:27,450 --> 00:05:32,540
your matrix which satisfy which satisfy that published OK

85
00:05:32,550 --> 00:05:38,430
this would be true if n is bigger than the constant time scale p

86
00:05:38,450 --> 00:05:42,550
OK so here is kind of funny that if you give me an x i

87
00:05:42,550 --> 00:05:44,840
cannot say whether it's going to work on that

88
00:05:44,840 --> 00:05:53,090
very few simple x then it's going to work because the interesting

89
00:05:53,090 --> 00:05:58,130
and of course what you would like is to relax conditions into a convex optimisation

90
00:05:59,060 --> 00:06:03,340
essentially if you can compute is also possible values maybe you can relax and in

91
00:06:03,340 --> 00:06:04,080
the way

92
00:06:04,090 --> 00:06:10,190
that would be computable so there is also a small industry of of relaxing the

93
00:06:10,270 --> 00:06:17,100
types of problem into a convex optimisation also semidefinite programming and essentially is a sparse

94
00:06:17,100 --> 00:06:18,310
they can use

95
00:06:18,340 --> 00:06:23,020
and here if you look at all those references you can see this work but

96
00:06:24,120 --> 00:06:24,950
still again

97
00:06:24,960 --> 00:06:27,180
OK so you can take the condition

98
00:06:27,190 --> 00:06:31,020
four which again is is of the order of canada p

99
00:06:31,040 --> 00:06:35,130
but if you relax the condition becomes very fact that you can verify it

100
00:06:35,180 --> 00:06:37,100
but the right so we can

101
00:06:37,150 --> 00:06:39,020
so right now there is no way

102
00:06:39,070 --> 00:06:43,210
of getting an assumption that you can verify

103
00:06:43,230 --> 00:06:47,090
four which we get the rights of the in

104
00:06:48,480 --> 00:06:51,790
so to be some of the other problem but is it is possible

105
00:06:51,790 --> 00:06:54,540
so people will be able to design

106
00:06:54,590 --> 00:06:57,990
so it's of sufficient conditions which allow

107
00:06:58,010 --> 00:07:01,410
to be verified which was scaling all is impossible

108
00:07:01,430 --> 00:07:07,430
so this is kind of interesting problems

109
00:07:07,450 --> 00:07:13,550
common extensions which i won't go into much into that often people complain about the

110
00:07:13,550 --> 00:07:16,460
less so because it will we have some bias

111
00:07:16,460 --> 00:07:19,430
seems to need long big to remove viable

112
00:07:19,450 --> 00:07:22,870
to set to zero sum available to the end your estimate

113
00:07:22,880 --> 00:07:26,740
on the active variables are shrunk towards zero

114
00:07:26,850 --> 00:07:29,750
one way to remove that is two

115
00:07:29,760 --> 00:07:34,220
keep the active set and they do not give rise estimation

116
00:07:34,230 --> 00:07:39,160
and this you can get better theoretical bounds but i think it's

117
00:07:39,170 --> 00:07:43,510
not separable because if you describe his your active set then you will go back

118
00:07:43,910 --> 00:07:46,880
you may have to many levels and you will need

119
00:07:46,880 --> 00:07:50,660
to regularize and if you don't get overfitting

120
00:07:50,670 --> 00:07:53,890
another thing is to use the elastic net

121
00:07:53,910 --> 00:07:58,030
which is replacing the one known by the one norm plus a bit

122
00:07:58,040 --> 00:08:03,040
of the square there two known so that's two effects he says is the effect

123
00:08:03,040 --> 00:08:07,970
of making the problem strongly convex so with a unique solution to the nice and

124
00:08:07,970 --> 00:08:11,820
also in hollows better behavior with heavily correlated variables

125
00:08:11,830 --> 00:08:16,010
so this is a very important point is that went to have also very close

126
00:08:16,010 --> 00:08:18,390
to the last so we choose one of them

127
00:08:18,450 --> 00:08:24,500
but depending on the depending on the case my my my change this idea we

128
00:08:24,510 --> 00:08:28,220
this idea reason for your problem OK you add a bit of nice he might

129
00:08:28,220 --> 00:08:32,190
select another via OK if you have two which are very close it will always

130
00:08:32,380 --> 00:08:35,850
be david in the two indices so might not deserve

131
00:08:35,850 --> 00:08:40,810
and this is the elastic net in this sense of that a bit and i

132
00:08:40,880 --> 00:08:46,320
refer you to the people of minister hasty manner

133
00:08:46,330 --> 00:08:50,540
so now what about the relevance of the theoretical results so first of all the

134
00:08:50,540 --> 00:08:55,980
results are for the square loss this is by no way limitation

135
00:08:55,980 --> 00:09:00,450
all the results of four and one can arise are this is by no means

136
00:09:00,510 --> 00:09:05,690
limitation as well and i will present some results later in the tutorial for not

137
00:09:06,000 --> 00:09:07,350
beyond everyone on

138
00:09:07,380 --> 00:09:10,730
so black is the things that we can we can do it

139
00:09:10,730 --> 00:09:12,660
in red this is

140
00:09:12,670 --> 00:09:18,030
the interesting stuff so first in the condition and coalitions is is very very restrictive

141
00:09:18,380 --> 00:09:20,760
very far from could get

142
00:09:20,780 --> 00:09:24,690
with the basic ninety presented so there is a huge gap between what we would

143
00:09:24,690 --> 00:09:27,540
like to know what we can do that then it may be

144
00:09:27,570 --> 00:09:30,760
true that there is a such that there is no way of closing the gap

145
00:09:30,760 --> 00:09:31,470
you know

146
00:09:31,510 --> 00:09:37,720
first of all the work is considering non sparse generating vector is very sparse uniting

147
00:09:37,720 --> 00:09:40,120
the task is is only to work

148
00:09:40,170 --> 00:09:46,380
which will study all business when you want alternating the is far from being sparse

149
00:09:46,390 --> 00:09:51,110
and often and of course is always the same problem how do you get london

150
00:09:51,110 --> 00:09:53,040
that's what you usually do

151
00:09:53,090 --> 00:09:57,600
in different you know what it's exact or approximate inference typically is officially convert the

152
00:09:57,600 --> 00:10:01,960
bayes nets to markov networks and then you do inference over them so in some

153
00:10:01,960 --> 00:10:03,850
sense a lot of the because the more

154
00:10:03,980 --> 00:10:07,480
fundamental notion then then the innovation network

155
00:10:07,480 --> 00:10:10,390
so how to do inference in markov networks

156
00:10:11,230 --> 00:10:17,280
our goal is to compute marginal and conditional probabilities over any variables in the network

157
00:10:17,290 --> 00:10:19,770
given in the other variables

158
00:10:19,790 --> 00:10:22,380
so is the expression

159
00:10:22,450 --> 00:10:27,060
unfortunately doing exact inference of these is the shark to complete problems so not only

160
00:10:27,060 --> 00:10:28,740
able to do it exactly

161
00:10:29,490 --> 00:10:33,780
one thing that is easy to do is to condition on the markov blanket variables

162
00:10:33,780 --> 00:10:37,810
so computing the probability of variable given its neighbors is actually very simple

163
00:10:37,830 --> 00:10:42,390
it's just this you know the normalisation constant cancels between the numerator denominator

164
00:10:42,430 --> 00:10:44,320
and it's just the exponentiated some

165
00:10:44,400 --> 00:10:46,500
of the weights of the features that are true

166
00:10:46,500 --> 00:10:49,310
when the variable has a the state one

167
00:10:49,360 --> 00:10:52,460
versus the sum of that for the states to industry one

168
00:10:52,990 --> 00:10:56,700
the variable has more rose just more turns here but the main point is that

169
00:10:56,700 --> 00:11:00,630
this is very quick and very simple to do so we can do is exploit

170
00:11:01,230 --> 00:11:05,890
and you know style exploits this is called gibbs sampling and it's and it's very

171
00:11:05,890 --> 00:11:07,040
simple over the

172
00:11:07,060 --> 00:11:08,730
here's what it does

173
00:11:08,750 --> 00:11:13,430
we just start with the random assignment of values to the variables

174
00:11:13,460 --> 00:11:15,820
i'm going to assume that there a billion so

175
00:11:15,830 --> 00:11:19,390
the truth assignments and then what i do the following

176
00:11:19,400 --> 00:11:23,570
for some number of steps until some convergence criterion is met

177
00:11:23,580 --> 00:11:28,880
i i repeatedly sample each variable according to its neighbors so i getting new state

178
00:11:28,880 --> 00:11:31,540
and then all i do is i count

179
00:11:31,540 --> 00:11:33,260
what fraction of the time

180
00:11:33,290 --> 00:11:37,080
whatever variable i'm interested in was true and that's going to be an estimate of

181
00:11:37,080 --> 00:11:38,230
the probability

182
00:11:38,280 --> 00:11:43,130
and you can show that you know under certain benign conditions this actually converges to

183
00:11:43,130 --> 00:11:48,120
the true probabilities if you take enough samples so simple algorithm but very general so

184
00:11:48,120 --> 00:11:50,300
very very widely used

185
00:11:50,310 --> 00:11:51,650
there's many others

186
00:11:51,950 --> 00:11:55,620
in particular there's lots and lots of variations of markov chain monte carlo gibbs sampling

187
00:11:55,620 --> 00:11:59,830
is just the simplest version of the there's also a family of algorithms that go

188
00:11:59,830 --> 00:12:03,820
by the name of belief propagation in particular this is what's called a sum product

189
00:12:03,820 --> 00:12:09,120
from the publication because what it's really doing is computing is sum of products

190
00:12:09,130 --> 00:12:13,820
there's also variational approximation and and various exact methods

191
00:12:13,820 --> 00:12:17,720
which are historically the earliest but they tend not to scale to the kinds of

192
00:12:17,720 --> 00:12:20,990
the things that we're going to be interested in your so we want to focus

193
00:12:20,990 --> 00:12:23,320
on that much

194
00:12:23,320 --> 00:12:24,280
so the other

195
00:12:24,280 --> 00:12:28,590
type of inference that we often want to do is what's called any

196
00:12:28,610 --> 00:12:30,310
four MPE inference

197
00:12:30,310 --> 00:12:36,430
which is the following problem given some evidence x these differences arose i know

198
00:12:36,460 --> 00:12:40,530
compute the most likely state of the world meaning the other variables y which we

199
00:12:40,530 --> 00:12:42,330
can call the clear variables

200
00:12:42,480 --> 00:12:46,230
so if you're familiar with hidden markov models this is for example of the viterbi

201
00:12:46,230 --> 00:12:47,360
algorithm does

202
00:12:47,380 --> 00:12:50,060
and this is like the journal version of that problem

203
00:12:50,110 --> 00:12:55,080
notice that the most likely joined state of the variables is not necessarily the most

204
00:12:55,080 --> 00:12:57,310
likely state of each of them individually

205
00:12:57,360 --> 00:13:00,810
because the most likely state for a and b is not necessarily the most likely

206
00:13:00,810 --> 00:13:04,840
state of a and the most likely to be so these are different problems at

207
00:13:04,840 --> 00:13:08,230
the level of and this is the question that we want to answer for example

208
00:13:08,230 --> 00:13:12,300
to the collective classification you know classifier whole bunch of web pages taking their links

209
00:13:12,300 --> 00:13:14,200
into account what we want to do

210
00:13:14,210 --> 00:13:17,550
is is compute the most

211
00:13:17,560 --> 00:13:21,670
the the maximum a posterior of or most probable explanation

212
00:13:21,680 --> 00:13:24,280
of of the data that we've seen

213
00:13:24,320 --> 00:13:27,490
and so what algorithms can be used for this purpose

214
00:13:27,500 --> 00:13:33,640
well the oldest and simplest one is something called iterated conditional modes which has limited

215
00:13:33,650 --> 00:13:34,250
the effect

216
00:13:34,270 --> 00:13:36,140
gibbs sampling you just to the following

217
00:13:36,170 --> 00:13:40,520
i repeatedly go each variable and i said it's to it's most likely value given

218
00:13:40,520 --> 00:13:44,720
its neighbors i yesterday to its conditional and then iterates

219
00:13:44,720 --> 00:13:48,060
the disadvantage of this of course is that it gets its it can get stuck

220
00:13:48,060 --> 00:13:49,260
in local optima

221
00:13:49,310 --> 00:13:54,180
so to avoid that we can produce simulated annealing which historically is you know the

222
00:13:54,180 --> 00:13:58,720
standard methods for for doing inference in markov networks

223
00:13:58,740 --> 00:14:04,170
it can be pretty slow another alternative more recent that that can be quite fast

224
00:14:04,170 --> 00:14:07,060
is to use graph algorithms

225
00:14:07,070 --> 00:14:09,980
what i do is i represent the problem is the graph and to try to

226
00:14:09,980 --> 00:14:13,180
find you know the minimum set the least cost set of edges that i can

227
00:14:13,180 --> 00:14:17,040
cut such that say one side is labeled positive and the other side is the

228
00:14:17,950 --> 00:14:21,750
and we can also use belief propagation in this case a max

229
00:14:21,790 --> 00:14:28,510
product version because we're trying to do is find the maximum of the product

230
00:14:28,550 --> 00:14:31,580
OK so let's talk a little bit about learning now

231
00:14:31,610 --> 00:14:34,640
so how do we how do we learn these models

232
00:14:34,650 --> 00:14:36,390
well there's two parts

233
00:14:36,400 --> 00:14:38,210
there's one in the parameters

234
00:14:38,210 --> 00:14:39,520
i e the weights

235
00:14:39,560 --> 00:14:42,130
and learning the structure i e the features

236
00:14:42,150 --> 00:14:46,820
the parameters could be learnt generatively are discriminatively and we'll will look at both

237
00:14:46,870 --> 00:14:50,830
in this tutorial i'm going to assume that we have complete data

238
00:14:50,850 --> 00:14:55,220
if we don't have complete data we need to use versions of these arguments and

239
00:14:55,220 --> 00:14:58,670
these are available in some of the software that i'll be talking about later but

240
00:14:58,670 --> 00:15:01,970
in this tutorial for simplicity i'm just going to assume that the data is always

241
00:15:03,600 --> 00:15:06,080
so how do we do generative weight learning

242
00:15:06,120 --> 00:15:12,230
well you just the standard you can do for example maximum likelihood learning or

243
00:15:12,830 --> 00:15:19,470
maximum posterior probability learning or full bayesian learning just as you could for simple models

244
00:15:19,580 --> 00:15:22,660
the one thing that

245
00:15:22,670 --> 00:15:27,040
not so fortunate about these models is that you cannot find the weights in closed

246
00:15:28,720 --> 00:15:31,800
like seen in bayesian networks however

247
00:15:31,820 --> 00:15:35,760
yeah it is a it's a convex problem so there a single global optimum and

248
00:15:35,760 --> 00:15:36,840
you can find it

249
00:15:36,880 --> 00:15:42,760
efficiently without of falling into local optima using something like gradient descent

250
00:15:42,760 --> 00:15:47,970
in the simplest case or using a second order technique likewise i'm looking or conjugate

251
00:15:47,970 --> 00:15:51,520
gradient methods all of which are pretty well understood

252
00:15:51,570 --> 00:15:53,610
and here is the form

253
00:15:53,630 --> 00:15:59,430
the derivative of the log likelihood with respect to weight the gradient is just the

254
00:15:59,430 --> 00:16:02,290
vector of these it's actually very intuitive

255
00:16:02,290 --> 00:16:04,120
and it's the following

256
00:16:04,140 --> 00:16:07,470
the derivative of the log likelihood with respect to weight is

257
00:16:07,480 --> 00:16:12,100
the number of times that the corresponding features is true in the data

258
00:16:13,210 --> 00:16:17,050
the expected number of times according to the model

259
00:16:17,060 --> 00:16:20,630
so what happens is that if the model predicts that this features is true less

260
00:16:20,630 --> 00:16:24,130
often than it really is then its we needs to go up

261
00:16:24,130 --> 00:16:28,260
however in the process i introduced ranking

262
00:16:29,010 --> 00:16:30,610
he that's what so

263
00:16:31,150 --> 00:16:37,170
this one this one i have decrease drag performance and in terms of means i

264
00:16:37,170 --> 00:16:43,750
have improved probability that just to show that these things are not necessarily

265
00:16:43,750 --> 00:16:45,920
here is a similar example

266
00:16:45,940 --> 00:16:48,610
with respect to ranking

267
00:16:49,290 --> 00:16:53,520
when you're all right leg he is on the

268
00:16:53,540 --> 00:16:57,430
before the least likely to be if i throw themselves behind each

269
00:16:57,810 --> 00:16:58,980
since time

270
00:16:58,990 --> 00:17:01,810
actually this is a random random

271
00:17:02,620 --> 00:17:07,170
distinguish between the positive and negative this

272
00:17:07,190 --> 00:17:11,180
it was before

273
00:17:11,240 --> 00:17:12,700
ranking errors

274
00:17:12,730 --> 00:17:18,560
and you can use ranking errors that's exactly because the top because the positive and

275
00:17:19,900 --> 00:17:24,940
it's not the whole ranking error because the negative not from positive with some kind

276
00:17:24,980 --> 00:17:31,210
of this kind of one another way to understand this for the same way

277
00:17:31,230 --> 00:17:38,950
in the united three of these are actually made no ranking errors in the brain

278
00:17:40,750 --> 00:17:42,150
as a classifier

279
00:17:42,190 --> 00:17:45,040
i have to say this ranking somewhere

280
00:17:45,060 --> 00:17:48,060
and you will immediately see what i do that

281
00:17:48,070 --> 00:17:51,800
for the classification errors when

282
00:17:51,810 --> 00:17:52,820
so it's

283
00:17:52,830 --> 00:17:55,370
the random

284
00:17:56,500 --> 00:17:58,450
now this one

285
00:17:59,940 --> 00:18:02,680
is one that make more errors

286
00:18:02,690 --> 00:18:07,580
of course each of these and this is two positive

287
00:18:07,600 --> 00:18:11,310
that this expression has been going on for ranking

288
00:18:11,330 --> 00:18:12,990
six ranking

289
00:18:13,000 --> 00:18:18,360
however the best class the best accuracy that can achieve here is the fact that

290
00:18:19,200 --> 00:18:23,120
i correctly classified correctly classified minuses

291
00:18:23,130 --> 00:18:25,760
and i only make two plus

292
00:18:25,770 --> 00:18:29,980
so again we have similar phenomena we can

293
00:18:30,100 --> 00:18:32,880
decrease ranking performance

294
00:18:32,900 --> 00:18:39,630
one so the relationship between all these things is perhaps last

295
00:18:39,650 --> 00:18:41,150
that's the thing

296
00:18:41,160 --> 00:18:46,200
and they were allowed to go with his friend because

297
00:18:46,210 --> 00:18:51,570
in one of the reasons for CSS

298
00:18:51,590 --> 00:18:53,990
exactly this thing isn't good

299
00:18:54,000 --> 00:18:57,400
probability estimates are necessarily a good ranker

300
00:18:57,420 --> 00:18:59,580
and so the answer

301
00:18:59,590 --> 00:19:00,750
the answer is

302
00:19:00,790 --> 00:19:07,200
is might all this depends on how you measure the quality of the probability is

303
00:19:07,200 --> 00:19:09,760
that we want to be addressed

304
00:19:09,870 --> 00:19:12,340
this is also called tony

305
00:19:13,050 --> 00:19:17,960
do you have sex with a woman depends on how you define set

306
00:19:19,080 --> 00:19:27,000
OK so this is the problem of finite part the main part

307
00:19:27,010 --> 00:19:27,950
i was talk about

308
00:19:27,960 --> 00:19:30,510
building models

309
00:19:30,930 --> 00:19:34,640
and will be treated based models models

310
00:19:34,650 --> 00:19:35,770
well you

311
00:19:35,780 --> 00:19:41,790
also analyse process of these models have been built and we can do with them

312
00:19:42,130 --> 00:19:48,870
that look at the relationship between the national tracking and the by the relationship between

313
00:19:48,870 --> 00:19:54,110
the ranking and probability estimation here is a brief executive summary for

314
00:19:57,380 --> 00:20:02,160
now before i thought it might be a good idea for the health warning here

315
00:20:02,170 --> 00:20:03,660
because i know

316
00:20:03,680 --> 00:20:10,170
that some people in this audience will get very uneasy with something that's what i

317
00:20:10,730 --> 00:20:12,640
these are mostly talk

318
00:20:12,660 --> 00:20:14,890
as you were

319
00:20:14,900 --> 00:20:16,350
training set

320
00:20:16,370 --> 00:20:21,150
and from experience again ran before and this is another

321
00:20:21,160 --> 00:20:25,050
you know this is like a red rag to all people

322
00:20:25,080 --> 00:20:31,410
i think it is justified but not so much this talk about the great

323
00:20:31,430 --> 00:20:35,120
model talking about so

324
00:20:35,130 --> 00:20:36,470
you know if somebody

325
00:20:36,490 --> 00:20:38,750
training decision tree

326
00:20:38,750 --> 00:20:41,070
and they will use training data

327
00:20:41,240 --> 00:20:45,460
are you would say that you can't do that sort of thing you need to

328
00:20:45,460 --> 00:20:48,850
use the validation set that's what the training set for

329
00:20:48,890 --> 00:20:54,610
so i'm believe here is that process and

330
00:20:54,620 --> 00:20:58,620
let's see what happens when we train to model so this is not to say

331
00:20:58,620 --> 00:21:05,670
that generalisation and overfitting issues just say i don't have to be a clever solutions

332
00:21:05,670 --> 00:21:11,410
for in this also applies here as much as the rest of it is the

333
00:21:11,410 --> 00:21:15,850
real issue is whether the stronger than can

334
00:21:15,870 --> 00:21:22,960
generalise from training set test set but that's not something that concerns me very much

335
00:21:22,960 --> 00:21:24,710
during this time

336
00:21:24,720 --> 00:21:25,910
OK so

337
00:21:25,970 --> 00:21:32,620
the first term is about building models i we consider that model

338
00:21:32,780 --> 00:21:39,410
the first very well the decision trees and a long we have been used in

339
00:21:39,410 --> 00:21:46,660
the paper this conference it's all let's try the lexicographic ranking which is

340
00:21:47,820 --> 00:21:50,770
has strictly stronger by the naive bayes

341
00:21:51,550 --> 00:21:56,430
that they do not want to go down the price increases

342
00:21:56,460 --> 00:21:57,460
if you go down

343
00:21:57,580 --> 00:21:59,780
since the degrees

344
00:21:59,870 --> 00:22:06,290
what do these models have in common well actually you understand all the basic models

345
00:22:06,450 --> 00:22:10,920
may not be entirely obvious if you think about naive bayes friends because that's not

346
00:22:10,920 --> 00:22:16,390
how i used to think of ways but this will become clear from abroad

347
00:22:16,400 --> 00:22:21,550
so let's look at the core the of city is kind of running example

348
00:22:21,920 --> 00:22:23,430
the first

349
00:22:23,460 --> 00:22:26,020
so i have to binary

350
00:22:26,040 --> 00:22:27,620
and use a and b

351
00:22:27,620 --> 00:22:31,240
value zero one

352
00:22:31,250 --> 00:22:33,230
so for instance here

353
00:22:33,240 --> 00:22:40,770
for a zero one and five of the six negative right one four

354
00:22:41,080 --> 00:22:45,070
five is for so long

355
00:22:45,840 --> 00:22:50,850
relatively easy maybe kind of it's certainly not linearly separable

356
00:22:50,860 --> 00:22:52,140
it's something

357
00:22:52,150 --> 00:22:58,150
decision tree and is like an axe or kind of concepts mostly plus their their

358
00:22:58,220 --> 00:23:00,160
minds that that's so

359
00:23:00,170 --> 00:23:04,280
i wouldn't be surprised if you try to decision tree and then you get something

360
00:23:04,280 --> 00:23:07,400
which is a complete decision trees users

361
00:23:07,420 --> 00:23:08,760
due to all the

362
00:23:10,120 --> 00:23:12,630
so we first on b

363
00:23:12,650 --> 00:23:16,220
and it's pretty good but it's not a we get here

364
00:23:16,230 --> 00:23:20,350
the joint distribution danny hillis

365
00:23:20,590 --> 00:23:21,930
so if you want to

366
00:23:22,070 --> 00:23:23,760
classifier will do

367
00:23:23,780 --> 00:23:28,480
we need a decision rule that only leaves the classes

368
00:23:29,150 --> 00:23:34,510
all these possibilities using the majority class get in that case

369
00:23:34,530 --> 00:23:40,230
you might want to apply the majority class you want your mind

370
00:23:40,280 --> 00:23:45,230
during plus an instance based that's what it looks like so you know it's kind

371
00:23:45,230 --> 00:23:47,160
of bimodal

372
00:23:47,180 --> 00:23:48,420
x four

373
00:23:48,420 --> 00:23:54,320
so that was d

374
00:24:01,720 --> 00:24:03,300
i believe that's it

375
00:24:03,320 --> 00:24:07,220
so that was definitely the end of that round

376
00:24:07,240 --> 00:24:11,400
i too because we just looked at the edge

377
00:24:11,440 --> 00:24:15,780
and she and jack indeed that's the last thing that happens we can check the

378
00:24:15,780 --> 00:24:17,240
couple of outgoing edges

379
00:24:17,250 --> 00:24:22,230
from d because that's the only one that just whose value just changed and there

380
00:24:22,230 --> 00:24:24,160
are no more relaxation possible

381
00:24:24,180 --> 00:24:25,860
so i was in two rounds

382
00:24:25,880 --> 00:24:28,190
claims we got all the shortest path weights

383
00:24:28,630 --> 00:24:31,540
the algorithm what actually loop four times

384
00:24:31,550 --> 00:24:37,130
to guarantee correctness because we have five vertices here one must not so in fact

385
00:24:37,130 --> 00:24:42,460
in the execution here there are two more blank rounds at the bottom nothing happens

386
00:24:43,460 --> 00:24:45,590
what the hell

387
00:24:45,610 --> 00:24:51,820
OK so that is the bellman ford i mean it's certainly not doing anything wrong

388
00:24:51,820 --> 00:24:55,890
the question is why does it converge wiser guaranteed to converge in the minus one

389
00:24:55,890 --> 00:24:59,280
steps unless there is a negative weight cycle question

390
00:24:59,310 --> 00:25:02,330
don't think is that you don't

391
00:25:06,350 --> 00:25:13,490
so that's optimization if if you discover whole round nothing happened so you could keep

392
00:25:13,490 --> 00:25:15,540
track of the album then you can stop

393
00:25:15,560 --> 00:25:19,150
in the worst case it will make a difference but in practice you probably want

394
00:25:19,150 --> 00:25:20,580
to do that

395
00:25:20,610 --> 00:25:22,710
good question

396
00:25:23,100 --> 00:25:25,130
all right

397
00:25:25,150 --> 00:25:29,940
so some simple observations i mean we're only doing relaxation so we can use a

398
00:25:29,940 --> 00:25:34,710
lot of our analysis from before in particular the d values are only decreasing monotonically

399
00:25:34,710 --> 00:25:40,400
as we cross values are always making smaller arches good another nifty thing about this

400
00:25:40,400 --> 00:25:42,080
algorithm is you can run it

401
00:25:42,080 --> 00:25:47,180
even in a distributed system if this is some actual network some some computer network

402
00:25:47,180 --> 00:25:50,750
and these are machines and they're communicating by these links

403
00:25:50,770 --> 00:25:54,780
i mean it's it's purely local thing relaxations the local thank you don't need any

404
00:25:54,780 --> 00:25:57,840
global strategy and you're asking about can we

405
00:25:57,870 --> 00:25:59,090
do different order

406
00:25:59,110 --> 00:26:03,370
in each step or you could just keep relaxing edges and keep relaxing edges and

407
00:26:03,370 --> 00:26:07,050
just keep going is for the entire lifetime of the network and eventually you will

408
00:26:07,050 --> 00:26:12,090
find shortest path so this algorithm is guaranteed to finish the rounds in a distributed

409
00:26:12,090 --> 00:26:15,980
system might be more asynchronous and so little harder to analyse but it will still

410
00:26:15,980 --> 00:26:19,330
work eventually it's guaranteed to converge

411
00:26:19,420 --> 00:26:24,330
as of bellman ford is is lot like the internet for finding shortest path

412
00:26:24,340 --> 00:26:29,320
so let's finally proved that it works

413
00:26:29,340 --> 00:26:34,230
so it should only take a couple aborts

414
00:26:39,100 --> 00:26:45,900
so i suppose we have a graph some tweets that have no

415
00:26:45,920 --> 00:26:47,340
negative weight cycles

416
00:26:53,560 --> 00:26:59,350
claim is that we turn it with the correct answer

417
00:26:59,370 --> 00:27:03,580
bellman ford

418
00:27:04,560 --> 00:27:06,980
with all of these two new values

419
00:27:07,000 --> 00:27:11,880
set to delta

420
00:27:18,880 --> 00:27:20,390
for every vertex

421
00:27:23,320 --> 00:27:25,560
proof is going to be pretty immediate

422
00:27:28,070 --> 00:27:33,630
using elements that we had from before you remembered

423
00:27:34,580 --> 00:27:38,490
we're just gonna look at every vertex separately

424
00:27:38,610 --> 00:27:46,730
so called vertex the claim is that this holds by the end of the algorithm

425
00:27:46,780 --> 00:27:50,980
so remember what we need to prove is that at some point dv equals delta

426
00:27:50,980 --> 00:27:55,430
that's kind of because we know it decreases monotonically and we know that it never

427
00:27:55,430 --> 00:27:59,090
gets any smaller than the correct guy cos relaxations always safe

428
00:27:59,110 --> 00:28:02,030
so we just need to show some point this holds and that it will hold

429
00:28:02,030 --> 00:28:03,400
at the end

430
00:28:03,420 --> 00:28:04,670
so that's by

431
00:28:04,690 --> 00:28:18,550
by monotonicity

432
00:28:18,560 --> 00:28:20,470
of the devalues

433
00:28:20,490 --> 00:28:22,930
and my correctness part one

434
00:28:26,130 --> 00:28:27,290
which was

435
00:28:27,300 --> 00:28:33,730
that's the these are always greater than or equal to delta

436
00:28:33,890 --> 00:28:37,770
we only need to show

437
00:28:38,080 --> 00:28:40,480
but at some point

438
00:28:40,480 --> 00:28:48,030
we have a quality

439
00:28:54,230 --> 00:29:06,530
research goal

440
00:29:06,550 --> 00:29:09,780
so we're going to do is just look at the

441
00:29:09,800 --> 00:29:14,240
and the shortest path to and see what happens to the algorithm relative to that

442
00:29:14,250 --> 00:29:20,350
so i'm say i'm going to name the scarlet he starts to vertex easier just

443
00:29:20,660 --> 00:29:21,970
one p two

444
00:29:22,010 --> 00:29:25,480
and you can

445
00:29:25,490 --> 00:29:30,360
and this is not just any shortest path but it's one that started as

446
00:29:30,390 --> 00:29:32,070
these areas as

447
00:29:32,110 --> 00:29:34,340
and it ends up be

448
00:29:34,370 --> 00:29:37,550
so i'm going to give a couple of names destined bb so i can talk

449
00:29:37,550 --> 00:29:40,000
about the past more uniformly

450
00:29:40,090 --> 00:29:43,940
so this is a shortest path

451
00:29:47,910 --> 00:29:51,810
s two

452
00:29:51,820 --> 00:29:59,120
now i also want to be not just any shortest path from s to be

453
00:29:59,220 --> 00:30:03,310
but among all shortest paths from s to v i want to have to be

454
00:30:03,310 --> 00:30:07,610
one with the fewest possible and

455
00:30:13,460 --> 00:30:19,840
OK so shortest here in terms of the total weight of the path subject to

456
00:30:19,840 --> 00:30:23,830
being shortest and weight i want to also be shortest in the number of edges

457
00:30:23,850 --> 00:30:30,680
and the reason i want that is to be able to conclude peace

458
00:30:30,680 --> 00:30:36,810
how many if you wanted to call the random variables in a nice way so

459
00:30:36,810 --> 00:30:40,950
that the outcome that are more probable

460
00:30:40,970 --> 00:30:45,410
we have the shortest code and the outcome is that that are less probable to

461
00:30:45,430 --> 00:30:46,620
have longer

462
00:30:46,680 --> 00:30:47,870
could war

463
00:30:47,890 --> 00:30:50,910
then the entropy is is that the

464
00:30:50,950 --> 00:30:56,660
major you'd use to know exactly how many how many bits you should call each

465
00:30:56,660 --> 00:30:58,620
of the codewords of of

466
00:30:58,620 --> 00:31:03,060
each of the outputs of of your random bible

467
00:31:04,160 --> 00:31:09,240
so all of these these these notions i have to do with how are you

468
00:31:09,240 --> 00:31:11,160
going to be able to call it

469
00:31:12,430 --> 00:31:20,100
your your random even seen nice way if you know something about the problem before

470
00:31:20,100 --> 00:31:23,350
so we should be

471
00:31:23,350 --> 00:31:27,390
closer to

472
00:31:27,450 --> 00:31:32,330
statistical learning or problems of classification

473
00:31:32,370 --> 00:31:38,810
so right now let's assume that we have a set of i i d samples

474
00:31:38,890 --> 00:31:43,080
x i y i where x a year is an example and y is it

475
00:31:43,080 --> 00:31:44,770
is its label

476
00:31:44,770 --> 00:31:52,160
and what you want to do is to find a classifier our hypothesis f

477
00:31:52,180 --> 00:31:55,810
so that f of x

478
00:31:56,990 --> 00:31:58,600
the label of x

479
00:31:59,640 --> 00:32:05,100
and so you will look for the hypothesis that minimizes y

480
00:32:05,120 --> 00:32:06,370
that at least

481
00:32:06,430 --> 00:32:10,120
the empirical risk

482
00:32:10,140 --> 00:32:15,470
or you could look for i i processes that that actually minimising the whole risk

483
00:32:15,470 --> 00:32:20,160
which is the expectation of the sort of

484
00:32:20,200 --> 00:32:22,040
f of x y

485
00:32:23,490 --> 00:32:26,350
since you don't have access to really two

486
00:32:26,350 --> 00:32:31,470
through to the fall the full risk

487
00:32:31,490 --> 00:32:34,330
you're you're only have access to

488
00:32:34,390 --> 00:32:40,010
you bring up the empirical risk that can measure from from the data eg

489
00:32:40,120 --> 00:32:42,470
x y y i

490
00:32:43,610 --> 00:32:46,990
so i think

491
00:32:49,010 --> 00:32:53,830
one think you need to do then is to control

492
00:32:53,850 --> 00:32:59,140
how far is the empirical loss from that rule is that you're trying to

493
00:32:59,290 --> 00:33:00,700
to minimize

494
00:33:01,990 --> 00:33:03,490
so to do so

495
00:33:05,180 --> 00:33:06,040
let's say a

496
00:33:07,990 --> 00:33:09,410
what you want to do

497
00:33:09,430 --> 00:33:11,770
then is to be able to

498
00:33:12,270 --> 00:33:14,120
control tower

499
00:33:14,140 --> 00:33:18,910
the empirical mean of the sum of i i d valuable

500
00:33:18,970 --> 00:33:19,910
which is

501
00:33:19,930 --> 00:33:24,270
it corresponds to the empirical loss there

502
00:33:24,290 --> 00:33:28,740
is close to its true meaning which is the true

503
00:33:30,120 --> 00:33:31,250
and we see

504
00:33:31,290 --> 00:33:36,600
here we are going to see two or three ways to

505
00:33:39,060 --> 00:33:46,220
get closer to controlling how the empirical loss is close to the true loss

506
00:33:46,240 --> 00:33:50,830
so here i three different quick inequalities

507
00:33:52,350 --> 00:33:59,080
time to take a break it and

508
00:33:59,100 --> 00:34:00,010
all right

509
00:34:00,020 --> 00:34:05,600
so i i think still

510
00:34:05,640 --> 00:34:09,720
go to this slide and then the end of last in the next slide before

511
00:34:09,720 --> 00:34:11,270
we actually think every

512
00:34:18,470 --> 00:34:25,220
so the markov inequality the chebychev inequality and change changing the deep certain of adding

513
00:34:25,220 --> 00:34:27,350
inequalities are going to help you

514
00:34:27,370 --> 00:34:32,080
you know how much how far is the empirical mean for

515
00:34:32,120 --> 00:34:34,660
i i d simple to whom

516
00:34:34,660 --> 00:34:41,930
so there with be for a bit here exercises are i i d variables that

517
00:34:43,540 --> 00:34:45,270
i equivalent to

518
00:34:45,290 --> 00:34:48,620
two one another access

519
00:34:48,640 --> 00:34:51,620
i consider as an which is the

520
00:34:51,640 --> 00:35:01,870
the one over and some of the size and know that the

521
00:35:01,950 --> 00:35:07,430
the median of as and in exactly the mean of x OK

522
00:35:07,450 --> 00:35:13,080
what do

523
00:35:13,390 --> 00:35:15,870
and then in expectation then

524
00:35:15,890 --> 00:35:21,970
if exactly the same it's not exactly the same expedition as any of the exciting

525
00:35:21,990 --> 00:35:26,270
which is x OK and i want to know how how close it is

526
00:35:26,330 --> 00:35:31,020
the expectation of and to the expectation of

527
00:35:31,040 --> 00:35:33,160
so i think

528
00:35:37,010 --> 00:35:42,450
so we look for quantities and this one we want to know whether as an

529
00:35:42,450 --> 00:35:47,080
is close to the to the true mean

530
00:35:47,080 --> 00:35:48,850
or not and so

531
00:35:49,040 --> 00:35:50,040
it's a

532
00:35:50,060 --> 00:35:51,450
we want to know

533
00:35:51,870 --> 00:35:56,160
if if sn is not close to the true mean then

534
00:35:56,330 --> 00:35:59,770
this probability is going to be bigger then some excellent

535
00:36:00,490 --> 00:36:03,290
so to achieve that you may use

536
00:36:03,290 --> 00:36:04,470
things like

537
00:36:04,470 --> 00:36:14,730
the reason

538
00:36:16,030 --> 00:36:18,930
so much

539
00:36:28,640 --> 00:36:40,760
this year

540
00:36:41,550 --> 00:36:42,530
you see that

541
00:36:49,060 --> 00:36:51,590
this picture was

542
00:36:58,920 --> 00:36:59,920
let's see

543
00:37:11,360 --> 00:37:15,390
you can see what

544
00:37:33,030 --> 00:37:34,230
you know

545
00:37:34,240 --> 00:37:35,920
so what

546
00:37:37,960 --> 00:37:39,360
there no longer

547
00:37:39,400 --> 00:37:42,050
he three

548
00:37:42,700 --> 00:37:46,310
the same is true here

549
00:37:50,760 --> 00:37:56,900
so three

550
00:38:07,720 --> 00:38:19,320
to ensure

551
00:38:26,410 --> 00:38:28,630
so so

552
00:38:35,570 --> 00:38:36,930
then i

553
00:39:01,620 --> 00:39:03,910
this is

554
00:39:12,840 --> 00:39:19,470
one the

555
00:39:48,720 --> 00:39:54,410
i want to that

556
00:39:59,800 --> 00:40:03,260
i think it

557
00:40:03,330 --> 00:40:07,430
look for me

558
00:40:07,440 --> 00:40:09,520
we want them

559
00:40:09,550 --> 00:40:14,970
what do you want to

560
00:40:14,990 --> 00:40:21,460
and what should

561
00:40:21,810 --> 00:40:24,370
you know

562
00:40:25,220 --> 00:40:30,060
recognition model

563
00:40:44,500 --> 00:40:50,520
that was

564
00:40:50,530 --> 00:40:56,040
we the

565
00:40:56,060 --> 00:40:59,620
sealed bid

566
00:41:02,380 --> 00:41:03,630
the a

567
00:41:05,180 --> 00:41:11,780
actually is

568
00:41:11,870 --> 00:41:14,010
one the

569
00:41:16,650 --> 00:41:23,920
one more

570
00:41:25,650 --> 00:41:28,010
and your

571
00:41:41,410 --> 00:41:44,440
if you can

572
00:41:44,480 --> 00:41:46,720
so it is

573
00:42:01,720 --> 00:42:02,880
and tom

574
00:42:19,700 --> 00:42:21,650
the world

575
00:42:27,380 --> 00:42:32,060
is very no

576
00:42:43,720 --> 00:42:45,590
if you need

577
00:42:53,180 --> 00:42:56,150
and so

578
00:42:57,170 --> 00:43:00,050
what we do is just

579
00:43:00,050 --> 00:43:03,760
positive current

580
00:43:04,070 --> 00:43:07,570
the charge here on this right plate

581
00:43:07,590 --> 00:43:10,550
i will call q

582
00:43:10,630 --> 00:43:12,970
and therefore by that definition

583
00:43:14,280 --> 00:43:17,170
is then dqtq

584
00:43:17,210 --> 00:43:20,240
signed sensitive

585
00:43:20,280 --> 00:43:25,190
i called the potential difference over this capacitor in going from the right side

586
00:43:25,240 --> 00:43:27,150
to the left side

587
00:43:27,170 --> 00:43:28,670
i call that

588
00:43:28,900 --> 00:43:29,880
of c

589
00:43:29,900 --> 00:43:32,240
that then is q

590
00:43:32,240 --> 00:43:33,880
divided by c

591
00:43:33,900 --> 00:43:39,470
all of that is signed sensitive

592
00:43:39,530 --> 00:43:41,610
i go around the circle

593
00:43:41,630 --> 00:43:45,970
and i want to calculate the closed loop in the goal of each LTL

594
00:43:46,010 --> 00:43:50,760
and that close to the integral of e dot

595
00:43:50,840 --> 00:43:53,820
it is not zero which many books

596
00:43:53,860 --> 00:43:55,220
i tell you

597
00:43:55,340 --> 00:43:57,470
many professors tell you

598
00:43:57,470 --> 00:43:59,090
it is not zero

599
00:44:00,150 --> 00:44:01,530
it is minus

600
00:44:01,570 --> 00:44:03,990
the fighting

601
00:44:04,030 --> 00:44:05,900
this is filed is law

602
00:44:07,610 --> 00:44:10,090
our economy

603
00:44:10,090 --> 00:44:12,800
because of the magnetic flux change

604
00:44:12,800 --> 00:44:14,610
in closed loops

605
00:44:14,670 --> 00:44:17,260
we can generate induced EMF

606
00:44:17,320 --> 00:44:18,920
we run our economy

607
00:44:18,920 --> 00:44:21,720
look at the lights

608
00:44:21,740 --> 00:44:26,880
luckily this is not zero

609
00:44:26,900 --> 00:44:29,030
this phi is the magnetic flux

610
00:44:29,050 --> 00:44:32,940
go through the surface any surface you can that

611
00:44:32,940 --> 00:44:36,510
that is close to

612
00:44:36,530 --> 00:44:40,380
so i've done this before i can do with a little faster i go from

613
00:44:40,380 --> 00:44:41,900
here to here

614
00:44:41,900 --> 00:44:44,780
but that is i are

615
00:44:44,880 --> 00:44:50,280
there is no electric field inside the ideal self inductance because superconducting wire

616
00:44:50,300 --> 00:44:51,950
not be in the field

617
00:44:52,950 --> 00:44:55,570
is zero going from here to there

618
00:44:55,610 --> 00:44:57,650
when i go over the capacitor

619
00:44:57,710 --> 00:45:00,300
i get my VC

620
00:45:00,320 --> 00:45:01,380
and here

621
00:45:01,440 --> 00:45:03,780
depending upon

622
00:45:04,690 --> 00:45:05,780
if i zoom

623
00:45:05,780 --> 00:45:09,030
this plot and this miners which you can reverse that

624
00:45:09,110 --> 00:45:10,800
then i will get

625
00:45:10,840 --> 00:45:12,630
when i walk into this direction

626
00:45:12,690 --> 00:45:14,880
i would get minus the zero

627
00:45:17,820 --> 00:45:21,300
but if you feel like reverting that i have no problem with that

628
00:45:21,320 --> 00:45:23,820
that's just a matter of on the a degree phase

629
00:45:23,840 --> 00:45:25,300
no different physics

630
00:45:25,320 --> 00:45:29,800
and this now equals minus the phi

631
00:45:29,840 --> 00:45:34,620
the only thing where you apply fire these laws you should always integrate in the

632
00:45:34,620 --> 00:45:36,320
direction that you have your

633
00:45:36,320 --> 00:45:37,630
current so

634
00:45:37,670 --> 00:45:39,740
then is miners LTI

635
00:45:39,760 --> 00:45:41,990
if you do the opposite direction

636
00:45:42,010 --> 00:45:43,970
and this plus LTI

637
00:45:43,990 --> 00:45:47,170
i have learned to certain discipline in my life

638
00:45:47,900 --> 00:45:50,110
many years so you have a long way to go

639
00:45:50,130 --> 00:45:51,240
and i always go

640
00:45:51,260 --> 00:45:55,450
in the direction of y so i never have to think this is minus al

641
00:45:55,510 --> 00:45:56,590
the idea

642
00:45:56,630 --> 00:46:00,150
is now

643
00:46:00,170 --> 00:46:04,360
covers the minus five so now what i do i bring the in

644
00:46:04,400 --> 00:46:08,170
and i take one more time derivatives

645
00:46:08,220 --> 00:46:10,090
so i get l

646
00:46:10,130 --> 00:46:12,400
times i double got

647
00:46:12,420 --> 00:46:13,800
players are

648
00:46:13,800 --> 00:46:15,650
times i dot

649
00:46:16,570 --> 00:46:21,090
BC but i take the time derivative sort cute becomes i

650
00:46:21,090 --> 00:46:22,630
so i got i

651
00:46:22,670 --> 00:46:25,090
divided by c

652
00:46:25,110 --> 00:46:26,880
and that now becomes

653
00:46:26,920 --> 00:46:29,920
the time derivative of this function

654
00:46:29,990 --> 00:46:31,860
but it goes to the right side

655
00:46:31,860 --> 00:46:34,380
which make the minus sign plus

656
00:46:34,400 --> 00:46:37,780
but when i take the derivative of cosine omega t

657
00:46:37,800 --> 00:46:40,030
get the miners omega

658
00:46:40,030 --> 00:46:41,970
so i get your minus

659
00:46:43,420 --> 00:46:44,720
times omega

660
00:46:45,970 --> 00:46:49,320
of omega t

661
00:46:49,340 --> 00:46:51,360
this is the differential equation

662
00:46:51,380 --> 00:46:53,690
it has to be solved

663
00:46:53,740 --> 00:46:55,740
and i will divide this

664
00:46:55,740 --> 00:46:57,530
out by

665
00:46:57,610 --> 00:46:59,280
l i will take

666
00:46:59,300 --> 00:47:01,510
provide everything by l

667
00:47:01,570 --> 00:47:04,760
i put to sea a little higher

668
00:47:04,760 --> 00:47:07,280
so we are

669
00:47:07,300 --> 00:47:08,670
over l

670
00:47:09,840 --> 00:47:12,090
and we only zero grant

671
00:47:12,090 --> 00:47:14,440
he was one of LC

672
00:47:15,210 --> 00:47:16,990
it comes then

673
00:47:17,050 --> 00:47:18,630
i i

674
00:47:18,630 --> 00:47:20,320
double dot

675
00:47:20,400 --> 00:47:22,150
as gamma

676
00:47:22,380 --> 00:47:23,900
i doubt

677
00:47:23,970 --> 00:47:27,650
because omega zero screen

678
00:47:27,690 --> 00:47:29,590
times i

679
00:47:29,670 --> 00:47:32,530
that now equals minus

680
00:47:32,570 --> 00:47:33,720
the zero

681
00:47:33,760 --> 00:47:35,280
divided by l

682
00:47:40,570 --> 00:47:42,150
here you see

683
00:47:42,210 --> 00:47:45,490
differential equation

684
00:47:45,550 --> 00:47:48,170
that differential equation

685
00:47:48,220 --> 00:47:51,400
looks amazingly similar

686
00:47:51,420 --> 00:47:53,970
for this one

687
00:47:54,030 --> 00:47:56,940
so you should be able to solve that

688
00:47:56,990 --> 00:48:00,660
for in fact you wouldn't even want to solve it you can write down immediately

689
00:48:00,660 --> 00:48:02,740
the answer

690
00:48:02,840 --> 00:48:05,170
very together and i

691
00:48:05,190 --> 00:48:08,130
which is and i zero it is

692
00:48:08,130 --> 00:48:09,860
take place of that

693
00:48:09,900 --> 00:48:14,720
eight areas of steady state solution go for steady-state solution

694
00:48:14,800 --> 00:48:17,070
by the sign

695
00:48:17,170 --> 00:48:19,220
of omega t

696
00:48:19,240 --> 00:48:21,340
minus delta

697
00:48:21,400 --> 00:48:22,880
no adjustable

698
00:48:22,880 --> 00:48:24,860
constant steady state

699
00:48:24,860 --> 00:48:28,220
the solution that i have

700
00:48:28,220 --> 00:48:29,760
steady state

701
00:48:29,760 --> 00:48:32,070
and i'll leave you

702
00:48:32,090 --> 00:48:33,720
finally i zero

703
00:48:33,760 --> 00:48:34,880
and you

704
00:48:34,940 --> 00:48:37,950
and work out delta is that is part of your

705
00:48:38,010 --> 00:48:39,740
problem set then you

706
00:48:41,050 --> 00:48:43,990
with the knowledge that you have you could write it down

707
00:48:44,050 --> 00:48:48,880
in a matter of seconds

708
00:48:48,880 --> 00:48:52,150
minimizes the cale divergence between

709
00:48:53,630 --> 00:48:54,480
star that's

710
00:48:55,320 --> 00:48:56,780
and pee and the likelihood

711
00:48:57,300 --> 00:48:59,090
copy of x given theta

712
00:48:59,940 --> 00:49:03,000
okay so it will find the nearest

713
00:49:04,420 --> 00:49:05,670
in kehl sense

714
00:49:06,130 --> 00:49:07,110
two peace star

715
00:49:10,980 --> 00:49:14,820
which in fact is the maximum likelihood model so my poster will converge

716
00:49:15,320 --> 00:49:18,550
around the maximum likelihood parameter setting

717
00:49:19,550 --> 00:49:23,320
subject to all these regularity conditions in the finite dimensional case

718
00:49:25,530 --> 00:49:26,530
in the limit okay

719
00:49:28,400 --> 00:49:32,400
so at least this is giving you some links between stuff that happens from the

720
00:49:32,400 --> 00:49:35,150
bayesian posterior stuff that would happen in the classical

721
00:49:35,760 --> 00:49:36,530
statistics setting

722
00:49:37,710 --> 00:49:38,880
okay questions about them

723
00:49:40,170 --> 00:49:41,440
so here's what happens

724
00:49:42,070 --> 00:49:44,090
this is asymptotic consensus

725
00:49:45,750 --> 00:49:48,690
this is what happens when you have multiple bayesians

726
00:49:50,820 --> 00:49:53,210
so imagine you have two different

727
00:49:53,760 --> 00:49:57,530
bayesians with different priors p e one theta and p e to data

728
00:49:58,090 --> 00:50:00,020
but they observe the same data

729
00:50:04,130 --> 00:50:04,860
here is the key

730
00:50:05,530 --> 00:50:10,960
assume both bayesians agree on the set of possible and impossible values of theta

731
00:50:11,840 --> 00:50:12,690
in other words

732
00:50:13,110 --> 00:50:14,630
this set theta

733
00:50:16,960 --> 00:50:18,460
that pe one assigns

734
00:50:18,960 --> 00:50:20,900
positive probability to

735
00:50:23,110 --> 00:50:27,250
is the same as the set date other people to science positive probability to

736
00:50:30,090 --> 00:50:33,750
everybody happy with this so this is basically we wanted people to

737
00:50:34,210 --> 00:50:39,230
have the same universal state is that they are considering but they just spread the mass out in different ways

738
00:50:40,190 --> 00:50:40,840
over the universe

739
00:50:42,480 --> 00:50:43,500
then in the limit

740
00:50:43,980 --> 00:50:51,150
the posteriors pe oneof data given indiana peter data given the end will converge to eachother

741
00:50:51,820 --> 00:50:53,960
this is what is called asymptotic consensus

742
00:50:54,570 --> 00:50:55,670
given the same data

743
00:50:56,360 --> 00:50:57,420
am assuming

744
00:50:58,630 --> 00:51:00,500
there are priors are not mutually

745
00:51:02,710 --> 00:51:06,280
incoherent with eachother all bayesians conversion the same

746
00:51:09,980 --> 00:51:11,530
so this is comforting as well

747
00:51:12,070 --> 00:51:16,110
don't worry so much about the pryr just worry about putting

748
00:51:16,880 --> 00:51:17,980
probability mass

749
00:51:18,480 --> 00:51:21,570
overall the reasonable parameter values then you can think of

750
00:51:22,440 --> 00:51:26,730
you will converge into the same thing of course you can reanalyzed priors if you want

751
00:51:28,820 --> 00:51:30,440
and in finite in

752
00:51:31,050 --> 00:51:32,550
the finite data case

753
00:51:33,000 --> 00:51:38,860
of course the answers we different but in the limit will converge under those same regularity conditions before yeah

754
00:51:45,820 --> 00:51:49,190
abs absolutely there's a lot of more theory

755
00:51:49,670 --> 00:51:52,090
there's asymptotic normality results

756
00:51:54,230 --> 00:51:55,460
and convergence rates

757
00:51:56,230 --> 00:52:01,030
there's a lot of work on rates which are not going to talk about but that's a more model specific

758
00:52:02,570 --> 00:52:04,400
under live conditions you can get

759
00:52:04,980 --> 00:52:06,130
you know optimal rates

760
00:52:07,210 --> 00:52:09,960
you can also find conditions where you rates are not optimal

761
00:52:10,590 --> 00:52:10,940
there is

762
00:52:11,360 --> 00:52:14,650
this is this is something i'm gonna get to my last slides

763
00:52:15,230 --> 00:52:16,150
a this talk

764
00:52:16,570 --> 00:52:19,800
i will talk about the formal results but this is kind nice interface

765
00:52:20,360 --> 00:52:22,820
between bayesian modeling

766
00:52:24,170 --> 00:52:27,130
at frequentist are classical analysis

767
00:52:27,840 --> 00:52:28,630
bayesian methods

768
00:52:29,050 --> 00:52:30,260
so i see bayesian

769
00:52:30,730 --> 00:52:33,320
methods as a way of of developing models

770
00:52:33,880 --> 00:52:40,250
but i can use classical frequentist tools to analyse the properties and my bayesian procedure

771
00:52:41,480 --> 00:52:43,570
for example to prove convergence and things like that

772
00:52:44,110 --> 00:52:45,070
and rates yeah

773
00:52:50,940 --> 00:52:51,610
by prior

774
00:52:54,780 --> 00:52:58,360
to put zero mass somewhere here you say okay well

775
00:53:03,090 --> 00:53:03,550
i think

776
00:53:06,110 --> 00:53:08,730
it depends on how you parameterize your space

777
00:53:12,130 --> 00:53:13,130
i'll be talking

778
00:53:13,570 --> 00:53:18,360
and peter orbanz as well will be talking quite a lot about bayesian nonparametrics

779
00:53:18,860 --> 00:53:23,710
where you have an infinite dimensional parameter space and then it's it's even hard to know

780
00:53:24,230 --> 00:53:25,690
exactly where are your pudding

781
00:53:26,280 --> 00:53:28,420
zero mass a non-zero mass on

782
00:53:29,630 --> 00:53:31,460
in in simple cases

783
00:53:31,920 --> 00:53:35,030
they knew you'd do wanna spread years apart out just in case

784
00:53:43,750 --> 00:53:45,480
so here's a quiz question by the way

785
00:53:48,530 --> 00:53:48,860
do you know

786
00:53:48,860 --> 00:53:50,190
it's in the plane

787
00:53:53,760 --> 00:53:55,930
applications so

788
00:53:55,950 --> 00:54:02,500
let's start with the function f estimation account like links to the all articles in

789
00:54:02,500 --> 00:54:04,840
processes that the colour

790
00:54:04,860 --> 00:54:06,720
as has to about

791
00:54:07,520 --> 00:54:12,620
let's start with the help of her parametric approach to function of estimation

792
00:54:12,640 --> 00:54:17,430
so this is used in things like regression and classification and we assume that they

793
00:54:17,430 --> 00:54:19,500
consist of two sets of points

794
00:54:19,680 --> 00:54:24,650
the axis which crosses want to predict the are called a covariance basically things which

795
00:54:25,460 --> 00:54:27,930
used to predict outputs y

796
00:54:28,530 --> 00:54:32,490
from acting to predict what what is the value of y

797
00:54:32,540 --> 00:54:36,030
and being parametric let's assume the model

798
00:54:36,280 --> 00:54:41,430
it is as follows we say that y is a function of x

799
00:54:41,470 --> 00:54:43,330
and parametrized by w

800
00:54:43,560 --> 00:54:47,140
maybe some noise in this case we assume that goes nine

801
00:54:47,210 --> 00:54:51,560
with zero mean and variance sigma work

802
00:54:51,610 --> 00:54:56,970
and being bayesian prior over w

803
00:54:57,800 --> 00:55:00,390
as you saw in college park

804
00:55:00,400 --> 00:55:01,270
you can do

805
00:55:01,600 --> 00:55:05,610
we can compute the posterior over

806
00:55:05,780 --> 00:55:10,170
over w five basically taking the prior multiplied in the likelihood

807
00:55:10,180 --> 00:55:11,440
and normalising

808
00:55:12,240 --> 00:55:16,720
he basically the marginal likelihood of y given x integrating out w

809
00:55:16,770 --> 00:55:20,490
and that of course the real with w

810
00:55:21,750 --> 00:55:25,590
that if given new has a point x bar

811
00:55:26,570 --> 00:55:27,960
our prediction

812
00:55:28,010 --> 00:55:30,300
well why that would be basically

813
00:55:30,350 --> 00:55:35,420
the distribution of y given x start as well as in a will be the

814
00:55:36,890 --> 00:55:41,860
probability of y given x and w so this is just

815
00:55:41,940 --> 00:55:43,640
that think of that

816
00:55:44,630 --> 00:55:46,960
integrated over w

817
00:55:47,180 --> 00:55:49,130
with respect to the posterior w

818
00:55:49,180 --> 00:55:50,910
so that's how the usual

819
00:55:50,920 --> 00:55:53,850
the bayesian approach to parametric estimation

820
00:55:53,900 --> 00:55:59,000
now moving on to in the normal way of doing mission in the in the

821
00:55:59,000 --> 00:56:00,170
bayesian approach

822
00:56:00,220 --> 00:56:04,610
we use gauss processes and the idea of processes that

823
00:56:04,630 --> 00:56:05,590
is there

824
00:56:05,640 --> 00:56:08,360
i have a parametric family of functions

825
00:56:08,410 --> 00:56:09,840
we simply say

826
00:56:09,890 --> 00:56:15,080
that we place our prior directly over the class of all possible functions one of

827
00:56:15,080 --> 00:56:15,970
which is about

828
00:56:16,030 --> 00:56:18,520
basically really large clusters of

829
00:56:18,530 --> 00:56:19,690
the class of functions

830
00:56:19,710 --> 00:56:22,030
so now we say that

831
00:56:22,040 --> 00:56:25,720
why are output is a function of

832
00:56:26,530 --> 00:56:28,170
last night

833
00:56:28,190 --> 00:56:32,610
and our prior over functions is to think of gaussians process which

834
00:56:32,620 --> 00:56:37,560
come to begin later but you should be familiar with the culture and

835
00:56:37,630 --> 00:56:41,390
will apply exactly the same missionary in which we

836
00:56:41,410 --> 00:56:46,030
can't you both the real function as well as the prediction

837
00:56:46,050 --> 00:56:47,510
integrating out

838
00:56:47,530 --> 00:56:50,440
all possible functions within the posterior

839
00:56:50,490 --> 00:56:54,540
so that's how they are four chan commission

840
00:56:55,200 --> 00:56:58,130
that's the estimation is really quite similar so

841
00:56:58,250 --> 00:56:58,860
we also

842
00:56:58,900 --> 00:57:01,670
she is a little

843
00:57:02,210 --> 00:57:05,000
like which i got from cult of which

844
00:57:07,600 --> 00:57:10,820
these samples of the form of prior

845
00:57:10,870 --> 00:57:12,430
and we observe

846
00:57:12,440 --> 00:57:16,060
input output points given by the cross is here

847
00:57:16,100 --> 00:57:18,900
and then we have a posterior which is count

848
00:57:18,950 --> 00:57:23,790
described in terms of the grey area and as the samples from the function

849
00:57:23,810 --> 00:57:27,400
so coming to that the commissioner so

850
00:57:27,450 --> 00:57:31,360
it's basically exactly the same way out how how you a little bit about

851
00:57:31,560 --> 00:57:33,850
the mission in the territory and what

852
00:57:33,920 --> 00:57:37,840
and then the machine in nonparametric and what the

853
00:57:37,890 --> 00:57:42,570
in the ordered the nature of data simply consists of all of the axis so

854
00:57:42,570 --> 00:57:48,110
this is basically unsupervised learning in the previous it supervised learning in which you

855
00:57:48,160 --> 00:57:52,760
you are given the label of the output which you want to predict

856
00:57:52,820 --> 00:57:56,400
in the case of the information you have simply given a set of data points

857
00:57:57,370 --> 00:58:02,320
you just want to model that has no output weight to predict

858
00:58:02,340 --> 00:58:05,270
so again we model the axis

859
00:58:05,280 --> 00:58:09,570
using a parametric family so again promise that by w

860
00:58:09,590 --> 00:58:11,860
so what this thing is saying is that

861
00:58:11,910 --> 00:58:17,200
the axes are distributed according to some function be solved

862
00:58:17,280 --> 00:58:22,180
distribution parametrized by w so this distribution of

863
00:58:22,500 --> 00:58:26,390
being distribution and the parameters w

864
00:58:26,400 --> 00:58:29,680
so again we place prior over parameters

865
00:58:29,700 --> 00:58:34,220
come do the first compute the posterior and they can do prediction again in exactly

866
00:58:34,220 --> 00:58:34,980
the same way

867
00:58:35,000 --> 00:58:39,180
of course the prediction is slightly different in that we are not given y we

868
00:58:39,190 --> 00:58:42,620
simply want to figure out what the probability of

869
00:58:42,640 --> 00:58:43,990
x are

870
00:58:44,000 --> 00:58:48,330
a new test point x what what the probability given all the training data

871
00:58:48,340 --> 00:58:51,290
and we are the in degree of w in the process

872
00:58:52,600 --> 00:58:57,130
how do we move from which is apparently approach nonparametric approach

873
00:58:57,180 --> 00:58:59,040
so the

874
00:58:59,060 --> 00:59:03,570
basic idea is exactly the same way of assuming a parametric

875
00:59:03,570 --> 00:59:13,940
so today

876
00:59:13,950 --> 00:59:16,850
no no concepts no new ideas

877
00:59:16,950 --> 00:59:18,570
relax a little bit

878
00:59:18,580 --> 00:59:21,960
and I want to discuss with you the connection between electric potential

879
00:59:23,140 --> 00:59:26,160
electric field

880
00:59:26,210 --> 00:59:30,960
imagine an electric field here in space

881
00:59:31,020 --> 00:59:32,770
and that

882
00:59:32,780 --> 00:59:34,360
I take a

883
00:59:34,370 --> 00:59:36,110
charge q in my pocket

884
00:59:36,180 --> 00:59:38,210
I start at position a

885
00:59:38,220 --> 00:59:40,250
and I walk around

886
00:59:40,250 --> 00:59:41,740
and I return

887
00:59:41,780 --> 00:59:42,900
at that point

888
00:59:44,680 --> 00:59:46,420
since these

889
00:59:46,470 --> 00:59:53,210
forces are conservative forces if the electric field is a static electric field there are no

890
00:59:53,210 --> 00:59:56,540
moving charges but that becomes more difficult

891
00:59:58,420 --> 01:00:02,580
the forces are conservative forces and so the work that i do

892
01:00:02,600 --> 01:00:06,000
well i march around the coming back for a must be

893
01:00:07,430 --> 01:00:11,320
it's clear when you look at the equation number three

894
01:00:11,370 --> 01:00:14,150
that the potential difference

895
01:00:14,210 --> 01:00:18,900
between point a and point eight is obviously zero i start point a and i

896
01:00:18,900 --> 01:00:20,930
and that point

897
01:00:20,950 --> 01:00:22,500
that is the

898
01:00:23,560 --> 01:00:26,730
in going from a back to point a

899
01:00:26,750 --> 01:00:29,090
of the of the

900
01:00:29,090 --> 01:00:34,730
and that then has to be zero and we normally indicate

901
01:00:34,730 --> 01:00:36,170
such an integral

902
01:00:36,200 --> 01:00:38,290
with a circle

903
01:00:38,340 --> 01:00:39,340
which means

904
01:00:39,340 --> 01:00:43,540
you end up where you started this is online now this is not a closed

905
01:00:44,370 --> 01:00:46,420
as we had in creation

906
01:00:46,430 --> 01:00:49,600
one closed line

907
01:00:49,620 --> 01:00:51,870
so whenever we deal

908
01:00:51,980 --> 01:00:54,480
static electric fields

909
01:00:54,500 --> 01:00:57,760
we can have no not equation if we like that

910
01:00:57,780 --> 01:01:03,200
and that is if we have a clothesline

911
01:01:03,250 --> 01:01:06,590
of you don't know

912
01:01:06,650 --> 01:01:08,700
so we end up where we started

913
01:01:08,750 --> 01:01:09,790
that then

914
01:01:09,820 --> 01:01:11,290
s two

915
01:01:11,420 --> 01:01:12,790
come here

916
01:01:12,850 --> 01:01:16,970
later in the course we'll see that there special situations

917
01:01:17,100 --> 01:01:19,160
we don't deal with static fields

918
01:01:19,170 --> 01:01:21,600
when we don't have conservative if

919
01:01:21,610 --> 01:01:24,580
but that's not the case anymore but for now

920
01:01:24,600 --> 01:01:27,190
it is

921
01:01:27,200 --> 01:01:30,010
so if we know electric fields everywhere

922
01:01:30,070 --> 01:01:31,220
then we

923
01:01:31,260 --> 01:01:34,660
you can see equation number two then we know the potential

924
01:01:36,190 --> 01:01:40,360
so if we turn it away around if we knew the potential everywhere

925
01:01:40,380 --> 01:01:43,720
you want to know what the electric field is that of course is possible if

926
01:01:43,720 --> 01:01:46,330
you look at equation two and three

927
01:01:46,350 --> 01:01:47,490
you see that the

928
01:01:47,510 --> 01:01:49,550
potential integral

929
01:01:49,570 --> 01:01:51,440
of the electric field

930
01:01:51,480 --> 01:01:53,170
so it is

931
01:01:53,230 --> 01:01:56,160
obvious that the field must be the derivative

932
01:01:56,200 --> 01:01:57,940
of the potential

933
01:01:57,950 --> 01:01:59,490
so when you have

934
01:01:59,510 --> 01:02:02,240
field being derivative of potentials

935
01:02:02,260 --> 01:02:05,630
you always have to worry about plus and minus signs

936
01:02:06,970 --> 01:02:10,330
you have to pay MIT twenty seven thousand dollars tuition

937
01:02:10,350 --> 01:02:11,360
b here

938
01:02:11,380 --> 01:02:16,510
or MIT pays you twenty seven thousand dollars tuition for coming here

939
01:02:16,580 --> 01:02:18,040
is only a difference of

940
01:02:18,050 --> 01:02:21,260
a minus sign but it's a big difference of course

941
01:02:21,300 --> 01:02:23,100
so let's work this out

942
01:02:23,110 --> 01:02:24,700
in some detail

943
01:02:24,740 --> 01:02:27,320
i have your chart plus q

944
01:02:27,320 --> 01:02:30,480
and it

945
01:02:30,490 --> 01:02:33,130
distance are

946
01:02:33,470 --> 01:02:35,540
it should be

947
01:02:35,570 --> 01:02:39,990
we know what the electric field is we found that the zillion times

948
01:02:40,010 --> 01:02:41,920
this is the unit vector

949
01:02:41,990 --> 01:02:44,660
in the direction from q to that point

950
01:02:44,700 --> 01:02:47,830
and we know that the electric field

951
01:02:47,850 --> 01:02:50,200
is pointing away from that charge

952
01:02:50,200 --> 01:02:53,100
and we know that the electric field

953
01:02:53,100 --> 01:02:55,920
we've seen that already first lecture

954
01:02:57,230 --> 01:02:59,290
divided by four by

955
01:02:59,330 --> 01:03:00,700
actually non-zero

956
01:03:00,700 --> 01:03:02,380
are square

957
01:03:02,390 --> 01:03:03,540
in the direction

958
01:03:03,550 --> 01:03:05,470
one of our world

959
01:03:05,510 --> 01:03:06,890
and last lecture

960
01:03:06,910 --> 01:03:08,730
we derived what the

961
01:03:08,760 --> 01:03:10,380
electric potential is

962
01:03:10,390 --> 01:03:12,050
at that location

963
01:03:12,140 --> 01:03:13,690
the electric potential

964
01:03:14,790 --> 01:03:16,380
divided by four i

965
01:03:16,390 --> 01:03:18,570
actually non-zero

966
01:03:20,100 --> 01:03:22,460
this is a vector

967
01:03:22,530 --> 01:03:28,730
is the scalar

968
01:03:30,670 --> 01:03:34,310
potential is the integral of the electric field along

969
01:03:34,330 --> 01:03:37,040
the line and now i want to try whether

970
01:03:37,810 --> 01:03:39,360
electric field

971
01:03:39,380 --> 01:03:41,330
can be written as the derivative

972
01:03:41,340 --> 01:03:43,480
of the potential

973
01:03:43,540 --> 01:03:48,800
so let's take the VDR

974
01:03:48,860 --> 01:03:51,540
let's see what we get

975
01:03:51,590 --> 01:03:55,470
if i take this DVD are i got minus

976
01:03:57,170 --> 01:03:59,540
divided by four pi

977
01:03:59,590 --> 01:04:00,920
actually non-zero

978
01:04:00,930 --> 01:04:04,230
or squared

979
01:04:04,330 --> 01:04:05,460
of course

980
01:04:05,490 --> 01:04:07,480
if i want to know what electric field is

981
01:04:07,490 --> 01:04:08,970
i need the vector

982
01:04:09,020 --> 01:04:13,630
so i will multiply both sides which is completely legal there's nothing illegal about that

983
01:04:13,680 --> 01:04:16,780
with the unit vector in the direction are

984
01:04:16,810 --> 01:04:19,350
but it turns into vectors

985
01:04:19,380 --> 01:04:21,910
and now you see that i'm almost there

986
01:04:21,920 --> 01:04:25,400
this is almost the same except for a minus sign

987
01:04:25,430 --> 01:04:27,420
so the derivative

988
01:04:27,470 --> 01:04:29,930
of the potential is minus the

989
01:04:29,940 --> 01:04:31,740
not close

990
01:04:31,750 --> 01:04:34,100
so i'll write that down here

991
01:04:36,330 --> 01:04:38,170
equals minus

992
01:04:38,230 --> 01:04:43,660
if you are

993
01:04:43,690 --> 01:04:44,750
so they are

994
01:04:44,800 --> 01:04:46,160
closely related

995
01:04:46,170 --> 01:04:47,470
if you know

996
01:04:47,520 --> 01:04:48,470
what the

997
01:04:48,480 --> 01:04:50,060
so i want i want

998
01:04:50,110 --> 01:04:52,100
this can be a vector

999
01:04:52,360 --> 01:04:54,040
here are rules

1000
01:04:54,050 --> 01:04:57,980
effect on the left side you must have an effect on the right side

1001
01:04:58,030 --> 01:05:02,020
so if you know the potential everywhere in space then you can retrieve the electric

1002
01:05:06,660 --> 01:05:10,350
i mentioned last time that the electric field vector

1003
01:05:10,410 --> 01:05:13,520
electric field lines are always perpendicular

1004
01:05:13,530 --> 01:05:17,980
to the equipotential surfaces

1005
01:05:18,030 --> 01:05:21,040
and that's always what it has to be the case

1006
01:05:21,060 --> 01:05:23,540
imagine that you are in

1007
01:05:23,600 --> 01:05:25,350
in space

1008
01:05:25,360 --> 01:05:27,470
and that you move

1009
01:05:27,520 --> 01:05:29,790
with the charge in your pocket

1010
01:05:31,550 --> 01:05:37,520
electric field lines so you purposely move only perpendicular to electric field lines

1011
01:05:37,530 --> 01:05:39,870
so that means that the force on you

1012
01:05:39,880 --> 01:05:44,120
and the direction in which you move always at ninety degree angles so you will

1013
01:05:44,120 --> 01:05:48,610
only move perpendicular to the field lines the field lines move like this

1014
01:05:48,630 --> 01:05:50,600
he said if you plan to move like this

1015
01:05:50,660 --> 01:05:52,300
so you have to do any work

1016
01:05:52,360 --> 01:05:55,580
because the dot product between the l and

1017
01:05:55,590 --> 01:05:56,810
it is zero

1018
01:05:56,840 --> 01:05:58,350
and if you don't do any work

1019
01:05:58,360 --> 01:06:01,330
the potential remains the same that's the definition

1020
01:06:01,340 --> 01:06:07,270
so you can see there for people tential surfaces must always be perpendicular to

1021
01:06:07,340 --> 01:06:11,600
field lines and field lines must always be perpendicular to the

1022
01:06:11,620 --> 01:06:12,330
you create

1023
01:06:15,340 --> 01:06:17,060
i'll show you again the

1024
01:06:17,160 --> 01:06:19,790
view graph the overhead

1025
01:06:19,790 --> 01:06:22,530
projection of the nice

1026
01:06:22,540 --> 01:06:24,420
growing by maxwell

1027
01:06:24,420 --> 01:06:27,480
plus four charge charging minus one charge

1028
01:06:27,550 --> 01:06:29,480
same one we saw last time

1029
01:06:29,530 --> 01:06:31,740
only to point out again

1030
01:06:32,580 --> 01:06:33,920
ninety degree angle

1031
01:06:33,920 --> 01:06:35,860
i discussed this in great detail

1032
01:06:35,880 --> 01:06:38,160
last lecture so i will do that

1033
01:06:38,160 --> 01:06:40,730
good morning presenting

1034
01:06:40,740 --> 01:06:46,350
and presented the nonparametric bayesian dictionary learning for sparse image representations this is joint work

1035
01:06:46,350 --> 01:06:49,690
was held in turn john paisley lu

1036
01:06:49,700 --> 01:06:53,400
and the matter why laws carrying the policy at duke university

1037
01:06:53,420 --> 01:06:55,890
and the gamma superior

1038
01:06:55,910 --> 01:07:00,030
the problem is the university of minnesota

1039
01:07:00,040 --> 01:07:01,820
first ever give introduction

1040
01:07:01,840 --> 01:07:04,050
so i will talk about efficiently

1041
01:07:05,290 --> 01:07:08,100
model introduced as inference

1042
01:07:08,110 --> 01:07:10,530
it's our show very encouraging results

1043
01:07:10,550 --> 01:07:16,320
although model is applied to image denoising and inpainting and compressive sensing

1044
01:07:16,370 --> 01:07:19,260
finally i will give the conclusion

1045
01:07:19,280 --> 01:07:25,030
sparse vegetation has drawn considerable attention has risen in recent years due to the fact

1046
01:07:25,030 --> 01:07:26,670
that encourages simple model

1047
01:07:26,680 --> 01:07:30,000
just overtraining can be avoided furthermore

1048
01:07:30,030 --> 01:07:33,450
this dictionary elements and sparse coefficients

1049
01:07:33,830 --> 01:07:38,900
generally can be maybe then interpreted in terms of biological of physical minutes

1050
01:07:38,910 --> 01:07:44,060
and the whole reason may be applied to image denoising inpainting and compressive sensing with

1051
01:07:44,060 --> 01:07:50,490
low income without you which of the shelf dictionary or basis that DCT fifty all

1052
01:07:50,510 --> 01:07:56,430
commonly used by the recent research shows that if the chain of incomplete over complete

1053
01:07:56,430 --> 01:07:57,470
dictionary on

1054
01:07:57,480 --> 01:08:02,320
on the master to the signal of test we may get improved performance and better

1055
01:08:04,590 --> 01:08:09,520
so it's sparse coding trying to solve the objective function that given dictionary d an

1056
01:08:09,530 --> 01:08:14,720
observation acts were trying to find the optimal trying to solve the objective function that

1057
01:08:14,740 --> 01:08:20,940
could minimize the along the sparse coefficient subject to that set out to of the

1058
01:08:21,350 --> 01:08:25,090
observation and the representation is smaller than constant

1059
01:08:25,140 --> 01:08:29,630
it's well known that is that the solution to this problem is NP hard problems

1060
01:08:29,710 --> 01:08:34,960
so many approximate solution has been proposed which can be generally divided into two categories

1061
01:08:34,980 --> 01:08:37,240
what can we use the greedy

1062
01:08:37,260 --> 01:08:41,100
algorithms you which supplement in pursuit is represented representative one

1063
01:08:41,110 --> 01:08:46,680
so i currently is the correct solution approaches you uris lossless who appears come has

1064
01:08:46,680 --> 01:08:49,030
been widely used

1065
01:08:49,050 --> 01:08:52,230
it has been shows that sparse representations

1066
01:08:52,240 --> 01:08:54,250
and a property dictionary

1067
01:08:54,270 --> 01:09:00,230
can be used to carry cover the original data based on noisy or incomplete observations

1068
01:09:00,230 --> 01:09:03,660
of compressed measurements

1069
01:09:03,680 --> 01:09:07,790
so what kind of dictionary can be considered part of the dictionary

1070
01:09:07,800 --> 01:09:12,790
based on my past experience is DFT DCT and we

1071
01:09:12,830 --> 01:09:18,270
this off-the-shelf bases and dictionary can be for any used to to sparsity represent imaging

1072
01:09:18,280 --> 01:09:22,650
acoustic and audio signals and if he had a simple and faster computations

1073
01:09:22,660 --> 01:09:24,620
however recent research shows

1074
01:09:24,630 --> 01:09:30,440
if we use dictionaries adapted the signal under test we may get improved performance and

1075
01:09:30,440 --> 01:09:33,280
better interpretations

1076
01:09:34,140 --> 01:09:37,850
how do we do with addition and the general approach is trying to solve the

1077
01:09:37,850 --> 01:09:43,710
global objective function that find a proper dictionary and the sparse coefficient that would minimize

1078
01:09:43,710 --> 01:09:49,760
the reconstruction subject to two for each of the asian this possible smaller last seven

1079
01:09:51,350 --> 01:09:55,950
and the company has two stages the first stage is the sparse coding stage you

1080
01:09:55,950 --> 01:09:57,690
which we fixed the dictionary

1081
01:09:57,710 --> 01:10:03,410
and we're trying to minimize trying to solve this objective function in which we use

1082
01:10:03,420 --> 01:10:08,810
is true that we know the law is ryan's oyster we know this person no

1083
01:10:08,830 --> 01:10:11,710
then we can use the orthogonal matching pursuit either

1084
01:10:12,300 --> 01:10:14,020
to solve this problem

1085
01:10:14,040 --> 01:10:18,130
and the laws and was possible is used as stopping criteria

1086
01:10:18,150 --> 01:10:24,050
and the second stage is trying to update the dictionary you the wrong way is

1087
01:10:24,050 --> 01:10:29,490
that we fix the sparse codes and use the ordinariness this collusion to

1088
01:10:29,510 --> 01:10:31,300
two is to find the

1089
01:10:31,310 --> 01:10:37,860
to find dictionary which is known as the most to more direction modi and the

1090
01:10:37,860 --> 01:10:41,750
other way we can use the case with the which have proved to give better

1091
01:10:41,750 --> 01:10:44,470
interpret better performance and the convergence

1092
01:10:44,940 --> 01:10:49,690
which is that while fixing the sparse codes we are fixing the sparsity pattern

1093
01:10:49,790 --> 01:10:57,220
and using the rank one approximation to simultaneously updates the dictionary and the sparse representations

1094
01:10:57,240 --> 01:11:03,370
but is this dictionary training algorithms has some restrictions the first most obvious restriction is

1095
01:11:03,370 --> 01:11:08,150
that the need to assume the laws were run source possible always no then can

1096
01:11:08,150 --> 01:11:13,470
then it can be used as a stopping criterion and restrictions that the size of

1097
01:11:13,480 --> 01:11:15,570
the dictionary the to be set of prior

1098
01:11:15,590 --> 01:11:20,240
and this sort restrictions that only point estimates can be provided which means you cannot

1099
01:11:20,240 --> 01:11:23,150
tell us what's to come region of the estimation

1100
01:11:23,170 --> 01:11:26,370
so how to relax these restrictions

1101
01:11:26,390 --> 01:11:31,400
here we introduce a long how much based on dictionary learning approaches this can use

1102
01:11:31,400 --> 01:11:37,290
this is a fact that model and in fact noting can be considered dictionary here

1103
01:11:37,330 --> 01:11:38,010
and there

1104
01:11:38,030 --> 01:11:39,190
back to school

1105
01:11:39,210 --> 01:11:41,690
can be considered as the coefficient here

1106
01:11:41,840 --> 01:11:47,080
so instead of directly enforcing that this coefficient has a certain sparse alone will assume

1107
01:11:47,080 --> 01:11:49,120
we know the law as well as in the data

1108
01:11:49,160 --> 01:11:55,570
we we introducing a sparsity promoting priors on this fact in skin on the back

1109
01:11:55,570 --> 01:12:01,380
to school which is the coefficient coefficient here also we percent we presented dictionary size

1110
01:12:01,400 --> 01:12:06,490
to large unless the data itself team for will support was dictionary size for this

1111
01:12:06,490 --> 01:12:08,460
given data

1112
01:12:08,470 --> 01:12:13,480
so the line form representation is opposition x i

1113
01:12:13,500 --> 01:12:18,670
is equal to DCI plus noise ten you which the is the dictionary which has

1114
01:12:19,350 --> 01:12:24,730
OK dictionary elements and each dictionary element has the same dimension as the observation x

1115
01:12:25,400 --> 01:12:29,690
and we impose that all x i

1116
01:12:29,710 --> 01:12:34,750
can be expressed as a linear combination of a small subset of the dictionary adamant

1117
01:12:34,920 --> 01:12:36,170
plus lost it

1118
01:12:37,180 --> 01:12:43,010
we hope that representations e i would be sparse here we use the beta process

1119
01:12:43,010 --> 01:12:46,000
formulation to to impose these two

1120
01:12:46,000 --> 01:12:47,740
all here

1121
01:12:56,110 --> 01:12:59,370
in one of

1122
01:13:05,040 --> 01:13:10,170
all right

1123
01:13:26,030 --> 01:13:27,430
you have to

1124
01:13:33,830 --> 01:13:36,000
what here

1125
01:13:51,680 --> 01:13:54,850
which is all

1126
01:13:54,860 --> 01:13:59,790
they say well

1127
01:14:20,400 --> 01:14:23,310
you are right

1128
01:14:23,430 --> 01:14:28,950
five out of

1129
01:14:44,450 --> 01:14:46,860
they always

1130
01:14:51,600 --> 01:14:56,830
how do you

1131
01:14:56,850 --> 01:14:57,470
you know

1132
01:15:01,900 --> 01:15:06,670
i i

1133
01:15:26,640 --> 01:15:30,170
at model

1134
01:15:59,350 --> 01:16:01,970
but as

1135
01:16:01,990 --> 01:16:04,170
you have long

1136
01:16:08,590 --> 01:16:10,940
we have

1137
01:17:27,930 --> 01:17:30,340
of course

1138
01:17:38,130 --> 01:17:39,970
moreover the action or

1139
01:18:08,760 --> 01:18:12,220
it will be

1140
01:18:43,170 --> 01:18:47,260
that's why i came here

1141
01:18:56,520 --> 01:19:00,850
well defined

1142
01:19:00,860 --> 01:19:02,760
these are

1143
01:19:02,780 --> 01:19:05,320
one year

1144
01:19:06,570 --> 01:19:07,910
this work

1145
01:19:45,450 --> 01:19:50,850
thing are

1146
01:19:50,850 --> 01:20:03,790
she she

1147
01:20:51,130 --> 01:20:54,170
it is threatened to be

1148
01:21:36,410 --> 01:21:40,070
and you that

1149
01:22:18,170 --> 01:22:22,060
you are right

1150
01:22:44,390 --> 01:22:47,320
one not

1151
01:23:24,030 --> 01:23:27,620
well that's

1152
01:23:38,190 --> 01:23:44,370
so the

1153
01:23:48,010 --> 01:23:51,370
they have

1154
01:23:51,380 --> 01:23:55,230
this is

1155
01:23:59,670 --> 01:24:07,880
that means

1156
01:24:11,150 --> 01:24:15,730
at the end

1157
01:24:49,860 --> 01:24:58,400
four three

1158
01:24:58,470 --> 01:25:02,490
the we

1159
01:25:08,470 --> 01:25:10,760
most people

1160
01:25:23,120 --> 01:25:26,730
the problem

1161
01:25:38,000 --> 01:25:40,180
three months

1162
01:25:44,290 --> 01:25:50,200
in one

1163
01:25:55,850 --> 01:25:58,690
well as you

1164
01:26:06,330 --> 01:26:13,440
fifty years

1165
01:26:52,270 --> 01:26:57,940
i but it is not

1166
01:27:14,030 --> 01:27:17,170
four hundred and which

1167
01:27:22,960 --> 01:27:32,040
you know at the

1168
01:27:50,070 --> 01:27:54,240
i think

1169
01:28:07,990 --> 01:28:16,530
and these are wrong

1170
01:28:32,410 --> 01:28:34,870
OK for

1171
01:28:34,880 --> 01:28:36,990
as of

1172
01:29:00,330 --> 01:29:03,170
so so on

1173
01:29:08,400 --> 01:29:12,210
are you know

1174
01:29:13,610 --> 01:29:17,170
you know by

1175
01:29:17,240 --> 01:29:21,170
three times

1176
01:29:21,200 --> 01:29:25,550
in the end these are

1177
01:29:34,350 --> 01:29:42,750
we had make the

1178
01:29:42,750 --> 01:29:43,780
since bizer

1179
01:29:43,800 --> 01:29:46,070
and to be able to do this kind of

1180
01:29:47,420 --> 01:29:49,690
nausium almost

1181
01:29:50,660 --> 01:29:53,280
o thing questions let me

1182
01:30:00,020 --> 01:30:01,180
so what

1183
01:30:01,200 --> 01:30:04,400
so now we have established is that

1184
01:30:04,440 --> 01:30:05,570
the one

1185
01:30:05,630 --> 01:30:08,210
the longest common subsequence of

1186
01:30:08,380 --> 01:30:10,850
at the two strings we

1187
01:30:10,850 --> 01:30:14,810
draw the two prefixes when we drop the last character

1188
01:30:16,840 --> 01:30:20,840
first we have a COI minus one

1189
01:30:20,880 --> 01:30:22,110
that's why

1190
01:30:22,120 --> 01:30:24,910
is equal to one

1191
01:30:24,930 --> 01:30:43,880
let's see if i missed one j minus one

1192
01:30:43,880 --> 01:30:46,190
that's one thank you were born

1193
01:30:51,270 --> 01:30:53,640
which implies

1194
01:30:53,640 --> 01:30:57,080
that seems like

1195
01:30:57,970 --> 01:31:00,810
i just called to say the line

1196
01:31:00,810 --> 01:31:02,970
one j minus one

1197
01:31:03,000 --> 01:31:09,910
plus one

1198
01:31:10,380 --> 01:31:15,640
it is straightforward if you think about what's going on there

1199
01:31:15,680 --> 01:31:21,960
now straightforward in some problems is for longest common subsequence ideas

1200
01:31:22,010 --> 01:31:22,700
you know

1201
01:31:22,710 --> 01:31:24,370
the the

1202
01:31:24,370 --> 01:31:32,820
well will on some of the other cases are similar

1203
01:31:32,870 --> 01:31:39,000
in fact we get on one

1204
01:31:39,020 --> 01:31:40,670
two hallmarks

1205
01:31:40,680 --> 01:31:42,340
dynamic programming

1206
01:31:42,370 --> 01:31:48,460
by hallmarks i mean when you see this kind of structure in a problem

1207
01:31:48,520 --> 01:31:52,140
there's a good chance the dynamic programming is going to work

1208
01:31:52,210 --> 01:31:53,980
OK as strategy

1209
01:31:55,170 --> 01:31:59,570
i programming for following

1210
01:31:59,610 --> 01:32:15,340
number one

1211
01:32:15,400 --> 01:32:18,360
is this property of the

1212
01:32:29,130 --> 01:32:31,370
this is is an optimal

1213
01:32:38,770 --> 01:32:43,050
this really mean problem instance get changes

1214
01:32:43,060 --> 01:32:44,990
the same problem in schools

1215
01:32:45,050 --> 01:32:47,370
the problem is generally in

1216
01:32:47,410 --> 01:32:50,300
computer science you as a

1217
01:32:50,910 --> 01:32:54,660
having infinite number of instances typically

1218
01:32:55,430 --> 01:32:58,640
we're just because the sorting is problem

1219
01:32:58,710 --> 01:33:01,610
sorting instances in particular in

1220
01:33:01,620 --> 01:33:06,270
so really talk about problem instances distance a problem

1221
01:33:08,280 --> 01:33:10,180
so when you

1222
01:33:10,180 --> 01:33:12,280
something like my

1223
01:33:12,310 --> 01:33:17,230
the most valuable and unique of human attribute lay beyond the reach of the scientific

1224
01:33:18,240 --> 01:33:21,740
to begin with than in this case with the natural scientific metal

1225
01:33:21,790 --> 01:33:26,030
and the could be approached only by the rational refraction

1226
01:33:26,040 --> 01:33:27,680
and this is the approach

1227
01:33:27,690 --> 01:33:30,770
the branch of

1228
01:33:30,780 --> 01:33:33,970
investigations like could also be

1229
01:33:34,020 --> 01:33:37,230
labour leader psychology has taken

1230
01:33:37,290 --> 01:33:42,770
you the rational refraction and try to guess how the mind works

1231
01:33:45,610 --> 01:33:48,130
neuroscientists including life

1232
01:33:51,730 --> 01:33:53,590
because was wrong

1233
01:33:53,600 --> 01:33:57,860
and our position of it are

1234
01:33:57,910 --> 01:33:59,720
express very

1235
01:34:01,300 --> 01:34:02,550
by this and other

1236
01:34:02,620 --> 01:34:03,670
part of

1237
01:34:03,720 --> 01:34:06,000
which was taken from a book

1238
01:34:06,050 --> 01:34:07,170
called the to only

1239
01:34:07,300 --> 01:34:10,660
but because nobody

1240
01:34:10,730 --> 01:34:13,380
francis crick who was one of the

1241
01:34:13,430 --> 01:34:17,180
two people discover the structure of DNA

1242
01:34:17,230 --> 01:34:22,570
so he states here stories hypothesis one is astonishing hypothesis

1243
01:34:22,610 --> 01:34:25,630
this is the first paragraph of book popular book

1244
01:34:25,650 --> 01:34:26,860
in the u

1245
01:34:26,860 --> 01:34:32,020
you're going to use all all your memories and the ambitions sense of

1246
01:34:32,030 --> 01:34:33,770
of identity

1247
01:34:33,780 --> 01:34:38,250
and the free will in fact more than the behaviour of the vast assembly line

1248
01:34:38,250 --> 01:34:42,660
of cells their associated molecules

1249
01:34:42,670 --> 01:34:46,820
so this is what we call monism as opposed to the car

1250
01:34:48,230 --> 01:34:50,230
where the body

1251
01:34:50,280 --> 01:34:52,670
the mind separated

1252
01:34:52,670 --> 01:34:58,190
and in this case one is we believe that the you unified mechanism

1253
01:34:58,210 --> 01:34:59,790
four explaining

1254
01:34:59,810 --> 01:35:03,360
so called the body functions but idea what functions

1255
01:35:03,380 --> 01:35:04,820
my functions

1256
01:35:04,840 --> 01:35:07,360
and of course mind functions

1257
01:35:07,410 --> 01:35:10,240
it is also based on the part of the body

1258
01:35:10,270 --> 01:35:11,970
according to this view

1259
01:35:11,980 --> 01:35:16,900
that's that's this one here human brain is made of one

1260
01:35:16,940 --> 01:35:21,480
a hundred billion specialized cells called neurons

1261
01:35:22,530 --> 01:35:24,050
ten times more

1262
01:35:24,210 --> 01:35:27,590
supporting cells grid cells and the song

1263
01:35:27,620 --> 01:35:31,730
now these cell phone gigantic networks

1264
01:35:31,770 --> 01:35:36,730
and here is the country of

1265
01:35:39,480 --> 01:35:42,250
three neurons connected

1266
01:35:42,270 --> 01:35:47,000
by very special structures called synapses

1267
01:35:47,670 --> 01:35:52,800
the brain receives input signals from the environment

1268
01:35:52,860 --> 01:35:57,520
through the visual system auditory the sensory system

1269
01:35:57,560 --> 01:35:59,680
this is a combative in the brain

1270
01:35:59,730 --> 01:36:03,900
in each of these sensory organs into electric signal

1271
01:36:04,050 --> 01:36:07,360
these selectors electric signal around through

1272
01:36:07,440 --> 01:36:11,050
these processes of neuron

1273
01:36:11,060 --> 01:36:18,520
and then one can lead to this is a very specialized collection called the synapse

1274
01:36:18,570 --> 01:36:22,790
it really see the small compounds called neurotransmitters

1275
01:36:24,190 --> 01:36:28,380
it received on the other side the other side of the connection

1276
01:36:28,710 --> 01:36:30,600
on on the second the cell

1277
01:36:30,660 --> 01:36:35,650
by specific proteins called neurotransmitters receptors and that binding

1278
01:36:35,660 --> 01:36:36,960
lead to

1279
01:36:37,010 --> 01:36:39,250
creation of new

1280
01:36:39,290 --> 01:36:41,820
electric signal in the second cell

1281
01:36:41,880 --> 01:36:43,850
and the song

1282
01:36:50,050 --> 01:36:54,150
we are interested in among the many many mine

1283
01:36:54,170 --> 01:36:56,400
functions we have

1284
01:36:56,460 --> 01:37:00,930
we can guess from the name of and

1285
01:37:00,990 --> 01:37:03,280
we are interested in the particularly

1286
01:37:03,310 --> 01:37:08,670
learning memory and associated cognitive functions such as a piece of consciousness

1287
01:37:08,820 --> 01:37:10,060
things like that

1288
01:37:10,180 --> 01:37:13,170
so here is the cover of the famous

1289
01:37:15,180 --> 01:37:21,400
columbia later we're gonna governorship marcus one hundred years of solitude i put this up

1290
01:37:23,520 --> 01:37:28,170
actually it is amazing if you read this book

1291
01:37:28,210 --> 01:37:32,420
this was published in nineteen eighty three or something

1292
01:37:32,430 --> 01:37:34,330
and you seen the in

1293
01:37:34,340 --> 01:37:35,590
in my colleague

1294
01:37:35,690 --> 01:37:37,460
he had already predicted

1295
01:37:37,520 --> 01:37:38,960
that's three

1296
01:37:38,970 --> 01:37:40,420
i will help you

1297
01:37:40,430 --> 01:37:42,520
to consolidate memory

1298
01:37:42,530 --> 01:37:46,450
you have already published nineteen forty that would unite

1299
01:37:46,600 --> 01:37:51,120
now the define neuroscientists have no really seriously working on that

1300
01:37:51,170 --> 01:37:52,420
but in any case

1301
01:37:53,920 --> 01:37:55,050
in this book

1302
01:37:55,060 --> 01:37:57,080
he describes a small

1303
01:37:57,090 --> 01:37:58,790
chromium village

1304
01:37:58,800 --> 01:38:02,790
with the villagers were afflicted by plug

1305
01:38:02,800 --> 01:38:07,270
they don't describe what what actually is medically

1306
01:38:07,310 --> 01:38:12,550
in such a way that everybody in so many spread in the village

1307
01:38:12,570 --> 01:38:14,300
and one more

1308
01:38:14,820 --> 01:38:18,670
people get affected by the chemistry

1309
01:38:18,670 --> 01:38:20,080
it don't need this week

1310
01:38:20,130 --> 01:38:27,020
and then initially thought it's great to see any more the interesting things

1311
01:38:27,130 --> 01:38:32,350
but then it comes out that many of these people develop memory

1312
01:38:32,420 --> 01:38:33,940
memory deficit

1313
01:38:34,010 --> 01:38:38,620
and this spread so my thoughts on this matter

1314
01:38:40,590 --> 01:38:42,370
rico carty

1315
01:38:43,760 --> 01:38:46,680
i was very concerned about the story so

1316
01:38:46,740 --> 01:38:48,400
and you can read here

1317
01:38:49,660 --> 01:38:54,070
one day people might forget what what what cost

1318
01:38:54,130 --> 01:38:56,560
collinson and what to do with them

1319
01:38:56,660 --> 01:38:58,450
so here go around

1320
01:38:58,500 --> 01:39:03,000
and put it in the middle of the nickel they recall

1321
01:39:03,040 --> 01:39:05,020
but the

1322
01:39:05,040 --> 01:39:07,050
with this statement

1323
01:39:07,090 --> 01:39:11,050
so this is the columns must be every morning before that she worked with your

1324
01:39:12,290 --> 01:39:16,130
and then it must be born in order to be mixed with the coffee to

1325
01:39:16,130 --> 01:39:17,890
make coffee

1326
01:39:19,210 --> 01:39:23,200
i don't have to i think that you i don't have to

1327
01:39:25,820 --> 01:39:27,670
the importance of the memory

1328
01:39:27,670 --> 01:39:29,760
for our daily lives

1329
01:39:29,770 --> 01:39:30,670
if you

1330
01:39:30,680 --> 01:39:32,790
just sit back and think about it

1331
01:39:32,830 --> 01:39:35,780
almost everything we were doing every moment

1332
01:39:35,790 --> 01:39:38,590
it's very much depending on

1333
01:39:38,600 --> 01:39:39,680
your memory

1334
01:39:39,720 --> 01:39:41,780
of one form or another

1335
01:39:41,790 --> 01:39:44,580
and that come back to that later

1336
01:39:44,580 --> 01:39:49,030
OK so let's let gamma priors

1337
01:39:54,410 --> 01:39:57,550
with each

1338
01:40:02,080 --> 01:40:04,320
i replaced

1339
01:40:08,540 --> 01:40:13,010
the CI

1340
01:40:14,260 --> 01:40:16,420
we have to argue

1341
01:40:16,480 --> 01:40:18,320
the gamma prime

1342
01:40:18,380 --> 01:40:20,790
is consistent

1343
01:40:24,080 --> 01:40:27,770
in our private schools

1344
01:40:28,310 --> 01:40:31,870
and the argument is similar

1345
01:40:34,190 --> 01:40:37,640
if gamma prime we're not consistent there would have to be a derivation of a

1346
01:40:37,640 --> 01:40:39,750
contradiction from it

1347
01:40:39,750 --> 01:40:42,040
wherever you use these

1348
01:40:42,050 --> 01:40:46,110
constants in the derivation you could have used the free variables

1349
01:40:46,130 --> 01:40:48,370
instead and that would have been the country

1350
01:40:48,380 --> 01:40:53,050
the derivation of contradiction from guam

1351
01:40:53,050 --> 01:40:57,650
so the upshot of that is

1352
01:40:57,680 --> 01:41:00,790
we can get rid of the free variables

1353
01:41:00,790 --> 01:41:02,910
in favour of constants

1354
01:41:03,530 --> 01:41:04,830
foreign concept that

1355
01:41:07,370 --> 01:41:08,520
all right

1356
01:41:08,530 --> 01:41:10,590
they don't do anything

1357
01:41:10,690 --> 01:41:12,620
four x

1358
01:41:12,630 --> 01:41:22,370
that couldn't be done by names

1359
01:41:28,600 --> 01:41:30,910
gamma prime

1360
01:41:30,920 --> 01:41:33,080
it's still not

1361
01:41:33,100 --> 01:41:37,120
good enough

1362
01:41:37,530 --> 01:41:47,150
we need to extend it some more

1363
01:41:47,160 --> 01:41:52,500
and this is where we're going to need to use this stuff

1364
01:41:52,540 --> 01:41:56,770
what we want in order to build up a big delta that treats the quantifiers

1365
01:41:56,770 --> 01:42:00,000
in the expected way

1366
01:42:00,100 --> 01:42:04,200
is it in particular wherever we've got an existential formula

1367
01:42:04,250 --> 01:42:05,790
in delta

1368
01:42:05,810 --> 01:42:08,560
we should have a weakness for it

1369
01:42:08,580 --> 01:42:12,440
OK if delta contains something that says there is enough

1370
01:42:12,520 --> 01:42:15,650
then we should we should have an example of such a thing

1371
01:42:15,680 --> 01:42:25,590
in delta should be dealt you say f of some something

1372
01:42:30,010 --> 01:42:34,580
consider an enumeration

1373
01:42:36,370 --> 01:42:37,860
enumeration of what

1374
01:42:40,430 --> 01:42:47,340
i guess of all the

1375
01:42:47,340 --> 01:42:52,780
all the form all the the stanshall

1376
01:43:08,850 --> 01:43:11,570
the language

1377
01:43:11,650 --> 01:43:20,600
we're intrin sign language were interested in

1378
01:43:25,090 --> 01:43:30,630
that connect sites particular variable case

1379
01:43:30,690 --> 01:43:33,150
let's say it's just a

1380
01:43:33,340 --> 01:43:36,160
some arbitrary variable

1381
01:43:36,170 --> 01:43:39,560
an arbitrary formula

1382
01:43:39,590 --> 01:43:48,670
we consider all of these

1383
01:43:57,590 --> 01:43:59,090
we're going to

1384
01:43:59,100 --> 01:44:02,280
we're going to

1385
01:44:05,280 --> 01:44:06,510
four now

1386
01:44:06,530 --> 01:44:19,320
a witness a formula that the forces eyewitness to exhibit

1387
01:44:30,100 --> 01:44:31,400
letting that

1388
01:44:32,870 --> 01:44:36,510
but that's just a formula can this this is an existential formula one of the

1389
01:44:36,510 --> 01:44:40,740
existential formula in the language

1390
01:44:40,780 --> 01:44:44,380
the the

1391
01:44:44,440 --> 01:44:46,450
this with

1392
01:44:46,490 --> 01:44:51,260
that the antecedent and the result of substituting a new constant

1393
01:44:51,270 --> 01:44:53,090
for the variable

1394
01:44:54,480 --> 01:44:57,370
foreign constant for the variable

1395
01:44:57,370 --> 01:45:06,350
zero zero one minus one

1396
01:45:06,370 --> 01:45:10,160
one minus one here this is the shooter

1397
01:45:10,160 --> 01:45:15,760
here's the going

1398
01:45:16,830 --> 01:45:19,330
if there's no deterministic

1399
01:45:19,390 --> 01:45:22,140
nash equilibrium here because

1400
01:45:22,990 --> 01:45:24,910
the shooter shoots

1401
01:45:24,910 --> 01:45:26,310
this way

1402
01:45:26,350 --> 01:45:27,700
the goalie e

1403
01:45:27,760 --> 01:45:30,510
she died this way

1404
01:45:30,530 --> 01:45:32,390
the goal is that in this way

1405
01:45:32,410 --> 01:45:34,950
the shooter would want to switch to shoot in this way

1406
01:45:34,950 --> 01:45:38,060
this addition this way only one of which died in this way

1407
01:45:38,100 --> 01:45:41,720
if the goal is diving this way the shooter was which in this way the

1408
01:45:41,720 --> 01:45:44,560
easiest way to go about doing this when there's no

1409
01:45:44,580 --> 01:45:47,200
deterministic equilibrium right so

1410
01:45:47,200 --> 01:45:49,160
this kind of behavior

1411
01:45:49,160 --> 01:45:52,700
leads to this kind of behavior should we want imagine a few minutes when the

1412
01:45:52,720 --> 01:45:56,260
going to want actually mean this should one

1413
01:45:56,280 --> 01:45:58,050
in this way the going

1414
01:45:58,100 --> 01:45:59,200
is no

1415
01:45:59,200 --> 01:46:01,200
deterministic equilibrium

1416
01:46:01,240 --> 01:46:02,740
but there is a

1417
01:46:02,760 --> 01:46:05,890
mixed right equilibrium namely you're go fifty fifty

1418
01:46:05,910 --> 01:46:10,010
going fifty there's no particular incentive to deviate

1419
01:46:10,030 --> 01:46:12,030
given the way the other players play

1420
01:46:12,060 --> 01:46:15,100
fifty fifty is illegal in fact

1421
01:46:15,120 --> 01:46:22,620
the existence of nash equilibria give the minimax theorem and we just proved core area

1422
01:46:22,640 --> 01:46:26,310
because you pick some nash if you think of a zero sum game

1423
01:46:26,330 --> 01:46:31,550
you pick some nash equilibria the VBE the value to the row player in that

1424
01:46:31,550 --> 01:46:33,120
nash equilibrium

1425
01:46:33,120 --> 01:46:34,560
the claim is

1426
01:46:34,620 --> 01:46:38,200
it's internationally calabria neither player

1427
01:46:38,240 --> 01:46:43,510
can do better even knowing the randomized strategy thereupon is planned to be taken actually

1428
01:46:43,510 --> 01:46:47,180
cleverer means that even you know even though the other guys doing you have no

1429
01:46:47,180 --> 01:46:49,350
answer to do anything else

1430
01:46:49,350 --> 01:46:53,910
so what that means is even in the con players doing there's nothing you could

1431
01:46:53,910 --> 01:46:57,470
do that we can do better than the physical ever even though when you're doing

1432
01:46:57,470 --> 01:46:59,450
something called layer could do

1433
01:47:00,910 --> 01:47:02,810
column player better than

1434
01:47:02,830 --> 01:47:04,990
the on the negative

1435
01:47:05,030 --> 01:47:06,870
even though when you do so

1436
01:47:06,870 --> 01:47:11,330
and actually calabria must be both players playing minimax optimal

1437
01:47:11,430 --> 01:47:13,370
there's there's no way neither one

1438
01:47:13,410 --> 01:47:17,450
even knowing what you guys doing to do better than this value

1439
01:47:17,490 --> 01:47:21,370
so actually the proof of nash equilibrium implies

1440
01:47:23,870 --> 01:47:28,830
but there is a difference which is that

1441
01:47:28,870 --> 01:47:33,100
OK so so so nash equilibrium in a zero sum game

1442
01:47:33,120 --> 01:47:36,950
both players happy playing minimax optimal strategy

1443
01:47:36,970 --> 01:47:40,660
if you want to play one of the players wasn't playing minimax optimal strategy then

1444
01:47:40,660 --> 01:47:43,450
you should be able to do better than that value

1445
01:47:43,450 --> 01:47:46,390
knowing what you guys do

1446
01:47:46,390 --> 01:47:53,370
unfortunately the existence of nash equilibrium is an unconstructive argument whereas for minimax we have

1447
01:47:53,370 --> 01:47:55,680
very constructively and our

1448
01:47:55,810 --> 01:48:00,530
but actually guaranteed to converge to released as well as minimax optimal we're better the

1449
01:48:00,530 --> 01:48:04,660
other player wasn't playing well so this the nice thing with this regret minimising algorithm

1450
01:48:04,720 --> 01:48:10,030
is the other guys playing optimally your approach minimax optimal value does not playing optimally

1451
01:48:10,080 --> 01:48:12,370
you develop that super

1452
01:48:12,560 --> 01:48:16,530
for nash equilibrium there is no known effective

1453
01:48:16,560 --> 01:48:19,950
no efficient algorithm for finding the nash equilibrium

1454
01:48:19,950 --> 01:48:23,550
the perfect is not constructive

1455
01:48:23,640 --> 01:48:27,260
so unlike the case is zero sum games we do not know any polynomial time

1456
01:48:27,260 --> 01:48:32,370
algorithm for finding nash equilibrium and by in general sum games

1457
01:48:32,370 --> 01:48:36,060
so for zero sum games we had these regret minimising algorithm or could use linear

1458
01:48:36,060 --> 01:48:37,700
programming solve

1459
01:48:38,890 --> 01:48:44,200
nash equilibria is known to be not NP hard but it's hard for some other

1460
01:48:44,200 --> 01:48:47,490
bizarre class called p pad

1461
01:48:47,530 --> 01:48:50,030
and so i believe

1462
01:48:51,160 --> 01:48:55,280
and i have polynomial time

1463
01:48:55,290 --> 01:49:00,410
well it's not i mean

1464
01:49:00,470 --> 01:49:03,740
in that sense i mean it would be a lot more money on the and

1465
01:49:03,910 --> 01:49:05,950
ninety point p versus the people

1466
01:49:05,990 --> 01:49:07,930
i would be

1467
01:49:08,760 --> 01:49:13,010
the difference is that these things exist it's not like the question of

1468
01:49:13,030 --> 01:49:15,160
from complete problems like existence

1469
01:49:15,160 --> 01:49:17,560
so far

1470
01:49:23,710 --> 01:49:25,440
kind of

1471
01:49:37,990 --> 01:49:41,990
one of

1472
01:49:51,390 --> 01:49:53,280
one so

1473
01:50:22,870 --> 01:50:29,640
i have not about

1474
01:50:29,680 --> 01:50:30,640
the last

1475
01:50:42,890 --> 01:50:49,420
o point five set of one

1476
01:50:49,500 --> 01:50:54,670
on the right side of

1477
01:51:21,820 --> 01:51:29,260
will tell you that i want

1478
01:51:29,520 --> 01:51:31,680
in thank you

1479
01:52:11,670 --> 01:52:14,130
one of the

1480
01:52:17,770 --> 01:52:20,390
on one of the

1481
01:52:59,460 --> 01:53:05,860
the world

1482
01:53:22,290 --> 01:53:27,310
what i want

1483
01:53:35,230 --> 01:53:36,410
the song

1484
01:53:37,890 --> 01:53:39,690
the people who

1485
01:53:52,670 --> 01:53:56,860
so the

1486
01:53:58,020 --> 01:54:03,940
if you really want

1487
01:54:09,230 --> 01:54:12,780
i want

1488
01:54:18,120 --> 01:54:21,380
in fact

1489
01:54:26,140 --> 01:54:32,670
the one source

1490
01:54:32,770 --> 01:54:36,260
so for the

1491
01:55:09,130 --> 01:55:10,730
o four four

1492
01:55:10,770 --> 01:55:12,970
in fact

1493
01:55:12,980 --> 01:55:17,840
the fact that i find it

1494
01:55:17,850 --> 01:55:20,960
there is a

1495
01:55:22,020 --> 01:55:25,670
now in the

1496
01:55:26,620 --> 01:55:29,900
the story

1497
01:55:29,900 --> 01:55:33,400
don't know

1498
01:55:33,600 --> 01:55:39,710
thank you for

1499
01:56:00,430 --> 01:56:05,920
four different very

1500
01:56:24,090 --> 01:56:27,850
that's what you

1501
01:56:33,250 --> 01:56:36,740
on the the other

1502
01:56:38,810 --> 01:56:41,380
one of course

1503
01:56:55,700 --> 01:56:58,350
think about

1504
01:57:07,120 --> 01:57:09,480
o five

1505
01:57:12,270 --> 01:57:15,570
actual care

1506
01:57:15,630 --> 01:57:20,820
all of the this

1507
01:57:32,100 --> 01:57:38,420
but also the

1508
01:57:38,450 --> 01:57:39,760
it's called

1509
01:57:44,920 --> 01:57:48,980
all of this

1510
01:57:48,980 --> 01:57:50,370
and then finally

1511
01:57:51,800 --> 01:57:52,900
things like

1512
01:57:52,920 --> 01:57:59,700
subgraph identification finding richer patterns and then metadata mapping so

1513
01:57:59,730 --> 01:58:01,710
a lot of the work and

1514
01:58:02,400 --> 01:58:12,170
ontology mapping ontology alignment can also be put into a framework that can make use

1515
01:58:12,170 --> 01:58:18,180
of some of the techniques and there's been some work in that area

1516
01:58:18,240 --> 01:58:23,870
so all of these are kind of higher level tasks hopefully

1517
01:58:23,920 --> 01:58:28,310
at least some of them seem compelling to you that you might have had an

1518
01:58:28,310 --> 01:58:34,250
information extraction system that would be useful to do a combination of doing

1519
01:58:34,280 --> 01:58:37,340
entity resolution may be predicate invention

1520
01:58:37,360 --> 01:58:41,820
and so on

1521
01:58:43,410 --> 01:58:51,040
the interesting thing is for all of these tasks

1522
01:58:51,040 --> 01:58:55,210
there's these kind of common things that come up

1523
01:58:55,220 --> 01:58:59,560
in terms of needing to collective classification needed to do

1524
01:58:59,790 --> 01:59:05,990
collective consolidation figuring out how to represent the relational structure the logical

1525
01:59:06,000 --> 01:59:10,240
dependence and statistical dependence in

1526
01:59:10,250 --> 01:59:15,250
a framework that can represent both of those being able to do the feature construction

1527
01:59:15,570 --> 01:59:19,590
selection aggregation

1528
01:59:19,650 --> 01:59:20,900
and figuring out

1529
01:59:20,910 --> 01:59:23,220
how to

1530
01:59:23,310 --> 01:59:28,390
can combine these things together ideally in a way that is flexible

1531
01:59:28,530 --> 01:59:33,520
and if possible decomposable

1532
01:59:33,560 --> 01:59:39,010
dealing with whether or not you act explicitly in your model refer to individuals

1533
01:59:39,100 --> 01:59:44,510
like a particular author that's very predictive or just at the class level

1534
01:59:44,510 --> 01:59:48,450
is labelled and unlabelled data

1535
01:59:48,500 --> 01:59:54,160
link prediction that kind of mention this but one of the things with link prediction

1536
01:59:54,180 --> 01:59:58,620
especially kid predicting the existence of the link is

1537
02:00:00,400 --> 02:00:03,980
prior probability of any link existing is just so

1538
02:00:07,100 --> 02:00:08,280
the kind of

1539
02:00:08,290 --> 02:00:11,310
confidence you can have in any of the

1540
02:00:11,340 --> 02:00:15,600
statistical inferences you get out can be very low and that's something you need to

1541
02:00:15,600 --> 02:00:19,310
be extremely careful about and then

1542
02:00:19,350 --> 02:00:22,820
making open world versus closed world assumptions

1543
02:00:22,870 --> 02:00:24,210
in your model

1544
02:00:27,230 --> 02:00:29,720
i think the interesting thing is

1545
02:00:29,770 --> 02:00:30,790
we met

1546
02:00:30,800 --> 02:00:36,450
no matter which these alphabet soup methods you end up using i think you end

1547
02:00:36,450 --> 02:00:38,610
up having to deal with

1548
02:00:38,630 --> 02:00:40,510
these challenges

1549
02:00:42,070 --> 02:00:47,100
not anything unique to a particular approach

1550
02:00:53,350 --> 02:00:54,720
i want to get into

1551
02:00:54,750 --> 02:00:58,450
some specific SRL approaches

1552
02:01:02,070 --> 02:01:04,510
categorization for approaches

1553
02:01:04,520 --> 02:01:07,050
that i'm giving it four approaches

1554
02:01:10,310 --> 02:01:11,790
it is based on

1555
02:01:12,480 --> 02:01:16,400
one set of them is based on underlying

1556
02:01:16,460 --> 02:01:20,400
semantics of directed probabilistic model

1557
02:01:22,900 --> 02:01:30,310
the underlying assumptions for the semantics of the probabilistic component is an undirected

1558
02:01:30,370 --> 02:01:31,890
the model

1559
02:01:33,240 --> 02:01:34,570
within these

1560
02:01:34,620 --> 02:01:36,100
the kind of logical

1561
02:01:36,120 --> 02:01:37,810
relational approaches

1562
02:01:41,450 --> 02:01:47,550
can be broken down into a rule based approaches versus more frame based approaches

1563
02:01:48,100 --> 02:01:50,410
and then actually there are

1564
02:01:50,450 --> 02:01:51,950
is a whole nother

1565
02:01:51,950 --> 02:01:54,090
category of methods

1566
02:01:54,150 --> 02:02:00,570
which much i chose the name programming languages approach that's probably not the greatest term

1567
02:02:00,910 --> 02:02:03,780
but there are approaches for example

1568
02:02:03,780 --> 02:02:07,010
based on stochastic context free grammars

1569
02:02:08,390 --> 02:02:14,370
probabilistic functional programming languages and so on and actually not going to go into

1570
02:02:14,380 --> 02:02:15,790
there's so much

1571
02:02:15,810 --> 02:02:19,930
but i think they have questions about them offline i'm happy to talk more about

1572
02:02:22,270 --> 02:02:23,710
so i'm going to focus

1573
02:02:24,640 --> 02:02:27,400
these four

1574
02:02:31,360 --> 02:02:35,880
like i said at the outset i think the thing is interesting

1575
02:02:35,900 --> 02:02:42,160
is that actually get very similar points at the end of starting with the rules

1576
02:02:42,160 --> 02:02:48,130
and adding directed models starting with systems are in directed models in but there's still

1577
02:02:48,130 --> 02:02:51,320
is something a little bit different amounts

1578
02:02:51,350 --> 02:02:52,990
can the mindset

1579
02:02:52,990 --> 02:02:56,600
during going in with these and so

1580
02:02:56,680 --> 02:02:58,070
the rule based

1581
02:02:58,070 --> 02:03:00,870
approaches tend to focus on

1582
02:03:00,880 --> 02:03:06,500
in fact what is true in the world and what depends on where

1583
02:03:06,540 --> 02:03:08,750
the frame based approaches

1584
02:03:08,760 --> 02:03:10,010
ten two

1585
02:03:10,700 --> 02:03:14,810
more focus on the modeling aspect

1586
02:03:14,860 --> 02:03:21,340
what are the objects in the world and how are they related to each other

1587
02:03:23,680 --> 02:03:24,560
and then

1588
02:03:24,560 --> 02:03:31,110
to contrast the directed approaches versus undirected approaches the director approaches

1589
02:03:31,180 --> 02:03:35,410
tend to focus more on kind of causal relationships

1590
02:03:36,130 --> 02:03:39,210
where as

1591
02:03:39,220 --> 02:03:42,520
undirected approaches focus on

1592
02:03:42,530 --> 02:03:44,760
things where there is a kind of symmetry

1593
02:03:44,810 --> 02:03:46,200
in the relationship

1594
02:03:46,250 --> 02:03:48,310
and this is the way that don't

1595
02:03:48,510 --> 02:03:50,760
so it can handle

1596
02:03:50,780 --> 02:03:52,730
non causal interactions

1597
02:03:52,750 --> 02:03:56,100
and then the programming alone

1598
02:03:56,120 --> 02:03:59,570
languages approaches tend to focus more on the process

1599
02:04:01,500 --> 02:04:05,490
is the world generated if this event happens then

1600
02:04:05,990 --> 02:04:11,970
how does that influence another that

1601
02:04:29,620 --> 02:04:36,530
so it's interesting because there certainly are

1602
02:04:36,570 --> 02:04:40,590
systems that allow you to make

1603
02:04:40,650 --> 02:04:45,070
directed and undirected but even then just the

1604
02:04:45,080 --> 02:04:48,570
propositional learning

1605
02:04:48,570 --> 02:04:50,240
there's then

1606
02:04:50,250 --> 02:04:52,390
certainly beautiful theory

1607
02:04:52,410 --> 02:04:54,610
developed for but

1608
02:04:54,660 --> 02:04:58,230
i haven't seen systems practically use

1609
02:05:00,740 --> 02:05:04,700
it is possible that people for some reason are doing it and

1610
02:05:04,710 --> 02:05:09,290
after a talk a little bit more about the two potential semantics we should

1611
02:05:09,350 --> 02:05:10,570
talk about

1612
02:05:10,580 --> 02:05:11,780
how we go

1613
02:05:13,060 --> 02:05:16,000
and i think then you might see the

1614
02:05:18,960 --> 02:05:22,860
even though it's not necessarily the most elegant way

1615
02:05:22,880 --> 02:05:24,950
you can kind of

