1
00:00:00,000 --> 00:00:05,230
and different filter designer might come along and say well the filter you want is

2
00:00:07,520 --> 00:00:10,090
what we want

3
00:00:10,170 --> 00:00:14,750
it is not going to be optimal in l two what measure would be

4
00:00:14,790 --> 00:00:18,560
another possible measure use

5
00:00:21,560 --> 00:00:26,860
how can i measure this error one way is to measure its energy l two

6
00:00:26,880 --> 00:00:31,090
but another measure of the areas

7
00:00:31,110 --> 00:00:35,400
it is the max difference right

8
00:00:35,420 --> 00:00:36,980
so the biggest

9
00:00:36,980 --> 00:00:39,750
because triple there is killing us

10
00:00:39,920 --> 00:00:41,590
and the max norm

11
00:00:41,610 --> 00:00:44,040
so the man in the maximum norm

12
00:00:44,080 --> 00:00:47,310
this would be no as we have a much higher

13
00:00:50,190 --> 00:00:52,110
this gives a number which is

14
00:00:52,130 --> 00:00:53,770
known to

15
00:00:53,770 --> 00:00:56,310
actually it's interesting i

16
00:00:56,330 --> 00:01:01,540
gibbs first paper is papers on this on his own phenomena

17
00:01:01,670 --> 00:01:04,980
like the first papers about the page and a half

18
00:01:05,080 --> 00:01:06,980
he gets it wrong

19
00:01:07,040 --> 00:01:09,460
he invested

20
00:01:09,480 --> 00:01:12,630
and then

21
00:01:12,630 --> 00:01:15,080
six months or so later

22
00:01:15,110 --> 00:01:19,190
of course a a correction was published

23
00:01:19,360 --> 00:01:21,960
there has only about ten lines

24
00:01:21,960 --> 00:01:26,790
where he says i just missed it there and then it tells us

25
00:01:26,850 --> 00:01:30,380
that goes up here and tells us what the number is

26
00:01:30,440 --> 00:01:33,920
it's some integral or something we find it

27
00:01:33,920 --> 00:01:35,810
two any number of places

28
00:01:36,860 --> 00:01:39,360
so it's sort of these

29
00:01:39,380 --> 00:01:43,730
you know it's the most famous of the phenomenon in signal processing

30
00:01:43,790 --> 00:01:47,920
in a tiny tiny paper there was a correction to

31
00:01:47,940 --> 00:01:49,420
quite sure

32
00:01:52,170 --> 00:01:54,810
if we looking at the max error

33
00:01:54,860 --> 00:01:56,020
we want to

34
00:01:56,040 --> 00:01:58,040
reduce it there

35
00:01:58,060 --> 00:02:04,790
at the cost of raising there so we can reduce the reduce it there but

36
00:02:04,810 --> 00:02:09,420
but then the report die office for

37
00:02:09,460 --> 00:02:11,880
and what's the optimum

38
00:02:11,900 --> 00:02:13,750
we're looking at max

39
00:02:15,750 --> 00:02:16,960
equally report

40
00:02:16,980 --> 00:02:19,940
so that the optimal

41
00:02:22,560 --> 00:02:25,150
the max norm

42
00:02:27,900 --> 00:02:29,480
frequency is

43
00:02:29,500 --> 00:02:30,770
we report

44
00:02:30,790 --> 00:02:32,540
what is we report

45
00:02:32,540 --> 00:02:37,730
i mean that the graph a polynomial of degree two hundred it's got two hundred

46
00:02:39,110 --> 00:02:41,040
and they all have the same height

47
00:02:43,710 --> 00:02:48,000
i guess in magnitude i can't go below zero here

48
00:02:51,400 --> 00:02:54,730
so that's it we report filter and that

49
00:02:54,850 --> 00:02:57,170
now how would you find the coefficients

50
00:02:57,190 --> 00:02:59,250
for that one

51
00:02:59,270 --> 00:03:06,920
so now this is this page hundred we report

52
00:03:06,940 --> 00:03:09,380
it's not so easy

53
00:03:09,400 --> 00:03:13,500
you're looking for the polynomial that

54
00:03:13,520 --> 00:03:18,630
even automatically clear that there is one that has equal height ripple

55
00:03:18,710 --> 00:03:23,710
all along

56
00:03:23,730 --> 00:03:28,380
of course there's got to be in error here actually take to get anywhere we

57
00:03:28,380 --> 00:03:31,690
really have to allow ourselves a little space

58
00:03:31,770 --> 00:03:35,730
between the end of the past and the start

59
00:03:35,750 --> 00:03:37,810
stop so let me

60
00:03:37,830 --> 00:03:39,670
so if i gave this

61
00:03:39,690 --> 00:03:41,790
these numbers to matlab

62
00:03:41,810 --> 00:03:45,650
matlab would want to numbers the matlab

63
00:03:45,670 --> 00:03:48,810
so what's the matlab command

64
00:03:48,830 --> 00:03:51,060
and as i think because it

65
00:03:51,090 --> 00:03:53,830
it is this guy's algorithm

66
00:03:53,880 --> 00:03:57,330
parameters start and will be the matlab command

67
00:03:57,350 --> 00:03:58,540
which would

68
00:03:58,560 --> 00:04:00,270
so what i do

69
00:04:00,290 --> 00:04:02,500
you would tell

70
00:04:02,540 --> 00:04:04,460
omega p

71
00:04:04,460 --> 00:04:05,560
don't make s

72
00:04:05,580 --> 00:04:08,810
so you inputs would be amazing if he

73
00:04:08,860 --> 00:04:10,560
and omega as

74
00:04:10,560 --> 00:04:12,310
and you could import

75
00:04:14,310 --> 00:04:16,650
one hundred

76
00:04:18,040 --> 00:04:20,940
stop s first stop

77
00:04:21,020 --> 00:04:23,750
what your choice

78
00:04:23,900 --> 00:04:28,830
it some distance here

79
00:04:29,860 --> 00:04:32,750
the more distance you give it

80
00:04:32,770 --> 00:04:33,750
but easier

81
00:04:33,770 --> 00:04:37,560
the design problem

82
00:04:37,580 --> 00:04:42,580
the lower the lower the reports that the closer you can come to ideal if

83
00:04:42,580 --> 00:04:45,350
we fix the degree

84
00:04:47,250 --> 00:04:52,150
if if i if you're asking this polynomial to rebel and then get down like

85
00:04:52,150 --> 00:04:54,210
mad rebel again if you

86
00:04:54,230 --> 00:04:58,500
titan that that will

87
00:04:58,500 --> 00:05:00,310
i don't have

88
00:05:00,350 --> 00:05:03,230
the general solution of the hand

89
00:05:03,250 --> 00:05:04,990
such situations

90
00:05:10,000 --> 00:05:12,660
it is

91
00:05:12,660 --> 00:05:17,950
as of four one one against one

92
00:05:20,060 --> 00:05:24,140
this is a

93
00:05:29,350 --> 00:05:32,890
is why is

94
00:05:34,160 --> 00:05:46,620
OK all so we said something about these people get so you four

95
00:05:46,830 --> 00:05:51,980
for the labelled as this data is really associated with is that you can define

96
00:05:51,980 --> 00:05:53,450
minus levels

97
00:05:53,520 --> 00:05:56,750
in the old days for a lot of other

98
00:05:56,750 --> 00:06:01,100
classifier which is which are related to you

99
00:06:01,120 --> 00:06:05,870
to this day the prediction is random

100
00:06:06,810 --> 00:06:11,600
so we all know these things using this one so when it is put into

101
00:06:11,600 --> 00:06:13,810
these decision function two

102
00:06:16,020 --> 00:06:19,520
then the prediction is somehow like random prediction

103
00:06:19,720 --> 00:06:24,290
so in the end the true case can minus levels

104
00:06:24,310 --> 00:06:30,220
and for all other cases make a k divided by him OK divided by two

105
00:06:30,220 --> 00:06:31,540
something like this

106
00:06:31,700 --> 00:06:37,580
so it is this that can minus one ceased to get true gives correct prediction

107
00:06:37,730 --> 00:06:40,950
that's the way how how this works

108
00:06:40,970 --> 00:06:42,310
this is a

109
00:06:44,790 --> 00:06:52,830
well as this is called

110
00:06:52,850 --> 00:06:54,790
o thing

111
00:06:54,810 --> 00:06:55,470
both the

112
00:06:55,470 --> 00:06:59,910
problem and trying to find a good source of energy

113
00:07:03,680 --> 00:07:11,140
that's why the actually hard quantities this we using this try to

114
00:07:11,270 --> 00:07:13,950
you can use decision as that

115
00:07:13,970 --> 00:07:21,120
but this is more like to of doing that whether or not there those some

116
00:07:21,120 --> 00:07:22,640
research work

117
00:07:22,660 --> 00:07:23,490
how to

118
00:07:23,540 --> 00:07:25,850
how want to list all these kind of things

119
00:07:25,870 --> 00:07:31,310
but in practice you really would like to do prediction this misfolding

120
00:07:31,330 --> 00:07:34,330
already works quite well

121
00:07:34,350 --> 00:07:37,700
what i

122
00:07:37,780 --> 00:07:40,950
this one

123
00:07:40,970 --> 00:07:42,120
o thing

124
00:07:42,160 --> 00:07:52,870
one of the problem that the probability of the positive problem

125
00:07:59,310 --> 00:08:00,560
so here

126
00:08:00,580 --> 00:08:07,950
but as of deletion anything about probability of it's so hard to generate probability of

127
00:08:07,970 --> 00:08:10,250
for is is an

128
00:08:10,270 --> 00:08:20,720
so it's just is a whole other a whole different issue but some research on

129
00:08:20,930 --> 00:08:22,290
the web

130
00:08:22,450 --> 00:08:24,600
OK so the first thing is you

131
00:08:24,600 --> 00:08:29,870
you need to generate the probability of rules for the two classes situation so you

132
00:08:29,970 --> 00:08:35,180
mention that OK so you know i only had two classes how to generate the

133
00:08:35,180 --> 00:08:41,830
probability outputs so was produced the state using the first could as possible probability you

134
00:08:41,830 --> 00:08:43,430
in a second place

135
00:08:43,910 --> 00:08:46,870
of course that must be related to PCA invectives

136
00:08:47,060 --> 00:08:52,730
lin lin that

137
00:08:52,750 --> 00:08:57,020
now we are doing you know for example one against terrorists were one against one

138
00:08:57,060 --> 00:08:58,970
so hopefully combine

139
00:08:59,000 --> 00:09:03,270
those probability values for any four

140
00:09:03,290 --> 00:09:09,540
very two classes yes as soon as a whole estimates for four

141
00:09:09,810 --> 00:09:12,250
for k classes as the average

142
00:09:12,250 --> 00:09:18,100
what is going to be very complicated in this talk about that here

143
00:09:18,100 --> 00:09:21,660
you can see for more information

144
00:09:25,850 --> 00:09:30,870
for more details

145
00:09:31,810 --> 00:09:33,580
support vector machines

146
00:09:34,730 --> 00:09:37,770
we use more complicated forms

147
00:09:37,770 --> 00:09:39,220
so this is one is that

148
00:09:39,220 --> 00:09:43,730
so this is different from one against the rest or one against one

149
00:09:43,750 --> 00:09:47,990
we're trying to directly extend

150
00:09:48,000 --> 00:09:50,290
the SVM optimisation problem

151
00:09:50,290 --> 00:09:52,640
two multi class c

152
00:09:52,660 --> 00:09:53,910
that's the idea

153
00:09:53,930 --> 00:09:57,910
so instead of using only one big about

154
00:09:57,930 --> 00:10:00,540
now we can make a big difference

155
00:10:00,560 --> 00:10:05,500
so we have this summation over from one to k

156
00:10:05,640 --> 00:10:13,080
so each one is for one place in the important part is full of it

157
00:10:13,080 --> 00:10:16,250
this inequality is politics is actually

158
00:10:16,560 --> 00:10:24,640
a whale in one against the rest into an optimisation for

159
00:10:26,200 --> 00:10:27,180
so why

160
00:10:27,200 --> 00:10:32,750
he's true to exactly as well training so we know the true class label of

161
00:10:35,560 --> 00:10:37,700
four this a nice i

162
00:10:37,730 --> 00:10:40,200
and if we multiply we see

163
00:10:42,000 --> 00:10:43,520
the w

164
00:10:43,540 --> 00:10:47,160
as of the true class they should be going

165
00:10:47,970 --> 00:10:51,660
then they use when we multiply it

166
00:10:51,680 --> 00:10:52,910
it we

167
00:10:52,930 --> 00:10:56,540
the about all other labels

168
00:10:56,890 --> 00:11:00,580
so if are using the first past this is w one

169
00:11:00,580 --> 00:11:02,160
so w one times

170
00:11:02,160 --> 00:11:07,220
it i should be with the article about the full extent examples also called w

171
00:11:07,290 --> 00:11:09,620
three six

172
00:11:09,640 --> 00:11:11,080
of things

173
00:11:12,080 --> 00:11:13,330
so this is actually

174
00:11:13,350 --> 00:11:16,080
similar to one against the rest

175
00:11:16,100 --> 00:11:21,350
you want to learn the of the true colours of true class label is greater

176
00:11:21,370 --> 00:11:22,850
or equal to one of a

177
00:11:22,850 --> 00:11:26,310
of using other labels

178
00:11:26,330 --> 00:11:33,750
and we also need bicycle wheels by a large so so that's why we have

179
00:11:33,750 --> 00:11:36,720
these simulations of top transport stop

180
00:11:37,040 --> 00:11:42,750
we also because they variables of suicide it is the penalty parameter

181
00:11:44,990 --> 00:11:46,500
so this looks good

182
00:11:46,520 --> 00:11:53,890
this problem the problem is this is going to be more complicated optimisation problems

183
00:11:55,250 --> 00:12:00,300
OK OK i stress the number of training data

184
00:12:00,350 --> 00:12:03,050
in the case the number of classes

185
00:12:03,120 --> 00:12:05,040
so why is that

186
00:12:05,090 --> 00:12:07,000
is a village feast on

187
00:12:07,000 --> 00:12:08,680
we i then we

188
00:12:08,870 --> 00:12:16,430
so the exciting was just one hundred who one residential until one returning

189
00:12:16,730 --> 00:12:19,470
so that's all the time so that he

190
00:12:19,520 --> 00:12:26,290
quality is for each data instance therefore the number of total quality here is k

191
00:12:27,500 --> 00:12:32,920
so we can also derive the dual optimisation problem because we want to sell to

192
00:12:34,090 --> 00:12:37,180
so we can have

193
00:12:37,330 --> 00:12:39,810
so we can derive b

194
00:12:39,830 --> 00:12:42,810
that's only when we derive the

195
00:12:42,840 --> 00:12:46,290
maybe this is still very off the sizes

196
00:12:46,340 --> 00:12:52,300
it is the number of training data and there is also a member of inequalities

197
00:12:52,300 --> 00:12:54,500
in the primal SVM

198
00:12:54,500 --> 00:12:59,060
one of the interesting range

199
00:12:59,080 --> 00:13:03,830
OK so the question is the overall assumption i made on pretty much

200
00:13:03,830 --> 00:13:10,230
page one of this lecture was that encoding decoding for the error correcting code

201
00:13:10,250 --> 00:13:14,690
around the noisy channel is something we separate

202
00:13:14,710 --> 00:13:19,130
from the system the compression and decompression and i said it is conventional to do

203
00:13:19,130 --> 00:13:23,210
this and the question is is this optimal or could it be better to have

204
00:13:23,210 --> 00:13:32,790
a combined compressor encoder and i can combined decompressor to compressor and decode and

205
00:13:32,810 --> 00:13:37,790
it's very conventional to do things this way to completely separated because people didn't perceive

206
00:13:37,790 --> 00:13:41,130
to be an advantage nevertheless there do exist

207
00:13:41,130 --> 00:13:45,580
some codes of the type that you have in mind so for example if your

208
00:13:45,580 --> 00:13:48,060
source is the bent coin

209
00:13:48,100 --> 00:13:52,330
then there is a single encodings

210
00:13:57,390 --> 00:14:01,330
for distance in the car i neil actually neil's in radford neal who've been hearing

211
00:14:01,960 --> 00:14:07,770
a lot in one calendar things and there is a single decoder that simultaneously effectively

212
00:14:07,770 --> 00:14:12,690
does compression and error correction for the binary symmetric channel so your idea

213
00:14:12,730 --> 00:14:16,400
is conventional tall but there does exist the solution of exactly that for the binary

214
00:14:16,400 --> 00:14:21,250
symmetric channel all the gas in channel a variety of simple channels you can use

215
00:14:21,250 --> 00:14:23,080
the name encodes and

216
00:14:24,040 --> 00:14:28,400
it is possible to make combined systems and you can reach the shannon limit with

217
00:14:28,400 --> 00:14:31,150
with those interiors were

218
00:14:33,920 --> 00:14:37,440
i've shown you several codes and i told you they get within one the three

219
00:14:37,440 --> 00:14:40,480
which might sound really good and i'm showing you how to make it and then

220
00:14:40,480 --> 00:14:42,170
the question is are we done is that

221
00:14:42,190 --> 00:14:46,730
compression is that all you need to know and there's several reasons why this doesn't

222
00:14:46,730 --> 00:14:48,960
wrap up compression

223
00:14:49,190 --> 00:14:50,110
it is

224
00:14:50,130 --> 00:14:53,690
the case so we've got little symbol codes but we've only got optimal symbol codes

225
00:14:53,710 --> 00:14:57,130
and maybe we want something better than a simple code i'm going to show you

226
00:14:57,480 --> 00:15:01,880
that that is the case now by playing the game with you which you probably

227
00:15:01,900 --> 00:15:03,730
some of you come across before

228
00:15:03,750 --> 00:15:05,790
and it's called

229
00:15:05,810 --> 00:15:09,540
the guessing game i'm going to write down on the board

230
00:15:09,600 --> 00:15:12,810
headline but you have to guess every last one of the time

231
00:15:12,830 --> 00:15:16,150
the headlines a little sentence is going to appear on board is going to pair

232
00:15:16,150 --> 00:15:17,560
up here

233
00:15:17,610 --> 00:15:21,810
one time and every time you make an incorrect guess of the next letter which

234
00:15:21,810 --> 00:15:23,170
is drawn from the set

235
00:15:23,190 --> 00:15:27,500
i will write incorrect letters down here in the column until you first one right

236
00:15:27,500 --> 00:15:32,330
then our carry on right and the headline is going to be chosen by the

237
00:15:32,330 --> 00:15:33,980
headline generation

238
00:15:33,980 --> 00:15:35,290
just here

239
00:15:44,000 --> 00:15:45,770
start dancing

240
00:15:45,790 --> 00:15:53,190
so i think be

241
00:16:01,500 --> 00:16:04,480
w l

242
00:16:20,580 --> 00:16:27,250
no there's no quotation mark in alphabet

243
00:16:30,710 --> 00:16:33,810
three pad

244
00:16:37,650 --> 00:16:39,060
is is correct

245
00:16:41,650 --> 00:16:44,810
oh sorry that's correct

246
00:17:04,730 --> 00:17:07,080
so far

247
00:17:16,650 --> 00:17:21,360
space s

248
00:17:32,650 --> 00:17:36,730
the tail

249
00:17:45,940 --> 00:17:48,110
alice gone already

250
00:17:51,060 --> 00:17:53,610
the a

251
00:17:56,230 --> 00:17:57,650
and it's already on

252
00:18:12,190 --> 00:18:16,020
so instead are some

253
00:18:26,190 --> 00:18:31,540
so and

254
00:18:42,270 --> 00:18:44,770
he is OK

255
00:18:44,790 --> 00:18:46,650
we've got

256
00:18:46,650 --> 00:18:48,310
prostitutes is available

257
00:18:48,360 --> 00:18:52,000
right something to notice about this game

258
00:18:53,210 --> 00:18:56,610
the number of guesses it took you to get the next letter

259
00:18:56,630 --> 00:19:01,610
which was lot and in area of the team

260
00:19:03,420 --> 00:19:06,230
chair only for high

261
00:19:06,250 --> 00:19:14,710
one wanted for a long long long long panel on the long one of the

262
00:19:15,050 --> 00:19:22,790
two long long long long long warner in nineteen ninety one

263
00:19:24,690 --> 00:19:29,610
so that it changed sort of right is sometimes it was difficult

264
00:19:29,630 --> 00:19:33,770
and sometimes it difficult often at the beginning of words and sometimes it was really

265
00:19:33,770 --> 00:19:40,980
easy in the middle all in the middle phrase that's often occurs like appeal to

266
00:19:42,670 --> 00:19:47,250
it was the probability distribution of letters the same at every single step in that

267
00:19:47,250 --> 00:19:53,230
process for you the expert expert system will predictions the same at every know you

268
00:19:53,230 --> 00:19:58,480
weren't using that probability distribution from the x-factor which we had that here

269
00:19:58,480 --> 00:20:02,790
you are changing your probability distribution every single step in a way that depend on

270
00:20:02,790 --> 00:20:07,730
the context possibly some quite long way back you know if i started writing in

271
00:20:07,730 --> 00:20:11,330
french headline you would have learned of the french language model and or if you

272
00:20:11,360 --> 00:20:14,380
know french word started learning french because we went through the ahead

273
00:20:14,420 --> 00:20:18,480
and by the end of the headline you would have been using your gradually developing

274
00:20:18,480 --> 00:20:20,920
knowledge of french to make those predictions

275
00:20:22,270 --> 00:20:27,650
what's wrong with optional symbol codes is that just optimal symbol codes and

276
00:20:27,670 --> 00:20:32,290
there examples where the optimal symbol code is actually really lousy let's look at the

277
00:20:32,310 --> 00:20:35,520
typical case in here is a typical case might be

278
00:20:35,560 --> 00:20:40,630
while you're back to predict this thing here

279
00:20:40,650 --> 00:20:44,880
and i asked him what was your probability distribution what would you prediction be any

280
00:20:44,880 --> 00:20:50,560
problem and said well the probability of being the

281
00:20:50,580 --> 00:20:54,520
these are already small point along

282
00:20:54,540 --> 00:20:57,860
lots of very small numbers and point nine nine sitting here

283
00:20:58,610 --> 00:21:02,040
that was your probability distribution and it's often that way and if we were to

284
00:21:02,040 --> 00:21:03,690
use the symbol code

285
00:21:03,710 --> 00:21:06,670
working at the probability distribution on the fly

286
00:21:06,690 --> 00:21:11,460
working at the optimal for that distribution what would happen then well

287
00:21:11,480 --> 00:21:15,000
it might look like this here is an example

288
00:21:15,020 --> 00:21:17,000
of the little alphabet a to k

289
00:21:17,000 --> 00:21:18,080
where c

290
00:21:18,150 --> 00:21:22,330
has a probability of ninety nine percent

291
00:21:22,360 --> 00:21:28,610
and what the optimal simulcast run have algorithm combine these is that

292
00:21:28,630 --> 00:21:32,880
you run huffman algorithm and the very last step you combine the sea

293
00:21:32,900 --> 00:21:35,110
with all the other guys

294
00:21:35,110 --> 00:21:38,930
and the code words you get like this you have one of length one for

295
00:21:38,930 --> 00:21:42,580
probable guy nice and short but it's still one bit long

296
00:21:42,650 --> 00:21:47,580
and the actual entropy the expected information content the idea that we ought to be

297
00:21:47,580 --> 00:21:51,380
compressing down to of this particular distribution here point one one

298
00:21:51,400 --> 00:21:56,190
so the factor by which the huffman algorithm loses compared with all shannon says we

299
00:21:56,190 --> 00:21:59,940
ought to be doing is nine the ratio of this expected language has to be

300
00:21:59,940 --> 00:22:04,020
bigger than one person will code one point something to point one one about nine

301
00:22:04,020 --> 00:22:09,330
so for sources of the time we've just been discussing optimal symbol codes may be

302
00:22:09,330 --> 00:22:12,020
nice and simple but of course you have to change the probability on the fly

303
00:22:12,020 --> 00:22:16,790
and everything else that change probability change the tree and so forth and you still

304
00:22:16,790 --> 00:22:18,440
are only within a factor of nine

305
00:22:18,460 --> 00:22:21,500
of optimal distributions of this time

306
00:22:22,630 --> 00:22:24,880
the real probability distribution

307
00:22:24,920 --> 00:22:32,190
changes the context and simple codes suck when you have probability distributions like this

308
00:22:41,960 --> 00:22:49,290
i the question is what's going on and the answer is

309
00:22:49,330 --> 00:22:53,730
huffman codes give you optimal symbol code but we don't want to simple code

310
00:22:53,770 --> 00:22:57,190
OK so simple cosine

311
00:22:57,190 --> 00:22:59,270
we have the latest stable distribution

312
00:22:59,290 --> 00:23:09,810
with fine line with infinite lives and that's the the best results you can make

313
00:23:09,810 --> 00:23:13,020
use of several

314
00:23:13,210 --> 00:23:20,980
i agree them to estimate the parameters the quantile CFR algorithm due to the colour

315
00:23:21,670 --> 00:23:24,850
guy working in finance

316
00:23:24,870 --> 00:23:27,560
this algorithm

317
00:23:28,420 --> 00:23:35,210
of the middle of of the eighties but you can also occur to maximum likelihood

318
00:23:36,710 --> 00:23:41,960
and since we have a lot of data probably maximum likelihood estimation is the more

319
00:23:42,330 --> 00:23:52,750
formal estimate for this for this result we find that the parameter alpha

320
00:23:52,790 --> 00:23:58,230
is lower than two and significantly so

321
00:23:58,250 --> 00:24:04,310
so prove it if we believe that this type of distribution

322
00:24:04,310 --> 00:24:05,730
comes from

323
00:24:05,750 --> 00:24:06,730
the sum

324
00:24:09,000 --> 00:24:17,420
independent independently idea synchronic shocks independently distributed of synchronic shocks

325
00:24:17,440 --> 00:24:23,790
two several cycles of several firms which are not this interconnected between them

326
00:24:23,810 --> 00:24:29,120
we should end up with something from the lowlands number

327
00:24:29,170 --> 00:24:36,600
so if shocks characterised by finding bias we should have go normality if shocks

328
00:24:36,650 --> 00:24:42,310
not characterized by a finite variance we should have a stable distribution so that the

329
00:24:42,330 --> 00:24:48,080
the main motivation for playing this class of distributions that because here we have a

330
00:24:48,080 --> 00:24:49,870
large number of works

331
00:24:51,650 --> 00:24:59,120
and we found that if we believe that a large number is a good model

332
00:24:59,120 --> 00:25:06,000
to use then we have that this distribution is less stable distribution with alpha lower

333
00:25:06,000 --> 00:25:08,540
than two so we have fat tails

334
00:25:10,040 --> 00:25:14,980
the two days are power law distributed

335
00:25:15,000 --> 00:25:18,630
and the second moment doesn't exist

336
00:25:25,830 --> 00:25:31,250
this is a similar to what

337
00:25:31,960 --> 00:25:33,620
so we have a gaussian and c

338
00:25:34,670 --> 00:25:37,350
and the liver cells

339
00:25:39,120 --> 00:25:41,210
missing data

340
00:25:41,230 --> 00:25:42,960
is good

341
00:25:42,980 --> 00:25:47,560
because in the tails that tend to become more spots

342
00:25:47,560 --> 00:25:49,920
OK but if it is quite good

343
00:25:49,960 --> 00:25:51,500
thank you

344
00:25:51,520 --> 00:25:55,500
OK so

345
00:25:55,520 --> 00:26:02,440
let stable distributions are characterized by instability and the summation

346
00:26:02,460 --> 00:26:08,250
it means that if we start from different models

347
00:26:08,270 --> 00:26:14,040
distributed according to oliver distribution with the given off

348
00:26:14,150 --> 00:26:16,250
was some then we end up

349
00:26:16,310 --> 00:26:19,940
with another level distribution with the same of

350
00:26:19,960 --> 00:26:22,460
if the alphas are all the same

351
00:26:22,500 --> 00:26:27,620
otherwise we end up with a solution with the distribution which is not stable estimation

352
00:26:28,400 --> 00:26:30,750
so it's possible

353
00:26:30,770 --> 00:26:33,270
testing strategy

354
00:26:33,310 --> 00:26:38,330
in order to assess whether i'm on the one on the right

355
00:26:38,350 --> 00:26:39,630
is two

356
00:26:39,650 --> 00:26:41,650
consider that are

357
00:26:41,670 --> 00:26:43,460
in terms of sectors

358
00:26:43,480 --> 00:26:46,230
we have twenty sectors at two-digit

359
00:26:49,190 --> 00:26:51,750
and aggregate

360
00:26:51,770 --> 00:26:54,190
sectors of the four digit level

361
00:26:54,210 --> 00:27:00,730
the level of the digital the digital that sort of the digital level is just

362
00:27:01,270 --> 00:27:04,960
a measure of how does aggregated they industries

363
00:27:04,960 --> 00:27:16,150
so for example in the two sector in the two-digit sector for automotive

364
00:27:17,350 --> 00:27:24,150
you have several for digit sectors regarding two times

365
00:27:24,170 --> 00:27:25,710
sixty years

366
00:27:25,750 --> 00:27:27,190
two in genes

367
00:27:27,350 --> 00:27:32,520
so so you can use aggregate so the first picture that use the new so

368
00:27:32,540 --> 00:27:38,730
was the disaggregated force for these sectors if you aggregated over twenty digits sector you

369
00:27:38,730 --> 00:27:40,330
can estimate

370
00:27:40,500 --> 00:27:44,980
twenty different distributions

371
00:27:45,480 --> 00:27:50,770
since the the the sum of all sector must be the same that is the

372
00:27:50,770 --> 00:27:54,420
entire manufacturing sector of the united states

373
00:27:54,460 --> 00:28:00,350
we can try to assess whether it's sexy it's individual sample of the two digit

374
00:28:01,580 --> 00:28:06,850
is alpha stable distribution we can estimate

375
00:28:06,870 --> 00:28:11,900
the level of of the value of alpha and if the value of alpha this

376
00:28:11,900 --> 00:28:15,580
competition but with available for for the wall set

377
00:28:15,580 --> 00:28:22,110
we should try and you will hear sorry i

378
00:28:24,820 --> 00:28:28,810
OK so

379
00:28:50,520 --> 00:28:54,620
when i got there is the starting point to be

380
00:28:56,160 --> 00:28:58,060
this here

381
00:28:58,070 --> 00:29:02,310
right OK this is an starting with just this part

382
00:29:04,120 --> 00:29:05,320
i'm i'm going

383
00:29:05,320 --> 00:29:06,580
work with that

384
00:29:06,600 --> 00:29:12,150
so you have

385
00:29:12,220 --> 00:29:14,500
so there's that

386
00:29:15,600 --> 00:29:18,690
so the first thing we do is that the suit

387
00:29:19,860 --> 00:29:21,210
one over and

388
00:29:23,610 --> 00:29:25,230
sigma i

389
00:29:25,230 --> 00:29:28,680
h of said i told

390
00:29:31,130 --> 00:29:32,460
one over and

391
00:29:32,460 --> 00:29:33,910
so i minus

392
00:29:33,930 --> 00:29:36,000
one over and some

393
00:29:36,120 --> 00:29:38,320
sigma i h

394
00:29:38,440 --> 00:29:41,430
i said i

395
00:29:43,140 --> 00:29:46,770
and now what i'm going to say is that that is less than or equal

396
00:29:47,620 --> 00:29:48,390
the super

397
00:29:50,480 --> 00:29:52,230
one over and

398
00:29:52,270 --> 00:29:54,880
so CKY

399
00:29:54,910 --> 00:29:58,090
h z i until the

400
00:29:58,100 --> 00:29:58,880
for us

401
00:29:58,890 --> 00:30:00,760
one over and

402
00:30:03,210 --> 00:30:07,610
sigma pi h seven n nine

403
00:30:08,190 --> 00:30:11,420
and then i'm going to say that's less than or equal to

404
00:30:11,460 --> 00:30:13,600
i can choose the suit for

405
00:30:13,620 --> 00:30:16,930
individually this issue age age

406
00:30:17,210 --> 00:30:18,460
so this is

407
00:30:18,470 --> 00:30:22,260
one size but if i can choose the age for this part and for this

408
00:30:23,560 --> 00:30:27,880
separately i'm going to do better still this is less than or equal to suit

409
00:30:27,930 --> 00:30:29,860
h in h

410
00:30:30,420 --> 00:30:33,340
one over and

411
00:30:33,350 --> 00:30:37,780
some sigma i h that i feel that

412
00:30:37,820 --> 00:30:39,980
class c

413
00:30:40,030 --> 00:30:41,600
one of and

414
00:30:41,650 --> 00:30:43,980
some sigma i h

415
00:30:46,300 --> 00:30:47,890
h h

416
00:30:48,080 --> 00:30:51,380
that's just equal to twice suit

417
00:30:51,400 --> 00:30:53,220
h h

418
00:30:53,220 --> 00:30:57,570
sorry expectation that sorry i can't make this stuff up here but since these are

419
00:30:57,570 --> 00:31:01,030
the same you know generated by the same process

420
00:31:01,050 --> 00:31:03,650
the super the expectation of this

421
00:31:03,670 --> 00:31:05,820
and the expectation of this

422
00:31:07,500 --> 00:31:11,330
and so you just get twice the expectation is that

423
00:31:11,370 --> 00:31:13,060
is that OK

424
00:31:17,270 --> 00:31:22,750
i'll run through the steps again just sort of put it all together maybe

425
00:31:24,790 --> 00:31:26,920
if you favour so that then

426
00:31:26,940 --> 00:31:29,570
you know that then separates the south

427
00:31:29,570 --> 00:31:33,890
into one super this into the other side and because he got the expectations

428
00:31:33,920 --> 00:31:38,570
they essentially become independent and you just get twice the expectation over one of them

429
00:31:38,570 --> 00:31:39,870
because they are identical

430
00:31:40,660 --> 00:31:41,800
OK so

431
00:31:41,890 --> 00:31:44,570
now looks quite good we seem to have got quite far

432
00:31:44,580 --> 00:31:46,490
and then you know think well

433
00:31:46,510 --> 00:31:47,970
i've done enough for the day

434
00:31:47,980 --> 00:31:49,330
so i just say

435
00:31:49,350 --> 00:31:52,410
that's rademacher complexity and unfinished OK

436
00:31:52,540 --> 00:31:54,280
so OK

437
00:31:54,290 --> 00:31:58,900
so that's the definition rademacher complexity that is it

438
00:31:58,990 --> 00:32:00,630
OK so

439
00:32:00,780 --> 00:32:06,500
it doesn't seem particularly sensible thing to do this point it just seems like you

440
00:32:06,540 --> 00:32:08,980
run out of steam and you don't know what to do next so just make

441
00:32:08,980 --> 00:32:11,880
a definition

442
00:32:13,120 --> 00:32:17,890
but it amazingly seems a very sensible thing to do in the end you will

443
00:32:17,900 --> 00:32:21,910
see you know that this is actually but OK so if we just make the

444
00:32:21,910 --> 00:32:23,320
definition that is

445
00:32:23,340 --> 00:32:25,780
rademacher complexity

446
00:32:25,790 --> 00:32:27,010
there it is

447
00:32:27,750 --> 00:32:30,310
that's just the definition rademacher complexity

448
00:32:30,460 --> 00:32:34,000
i'll come back to talk about what it means and so on we can we

449
00:32:34,000 --> 00:32:38,320
get this right so let's just go back and see the stages we went through

450
00:32:38,340 --> 00:32:41,580
so if you remember we started by doing this

451
00:32:41,600 --> 00:32:45,310
so we had the that that was the empirical and then we had the same

452
00:32:46,500 --> 00:32:50,210
and we said the same thing was less than or equal to something

453
00:32:50,230 --> 00:32:53,550
plus this so going to have to keep this page

454
00:32:53,570 --> 00:32:58,540
but OK we're just going really replace this by this and and and in

455
00:32:58,550 --> 00:33:01,590
so what was this this was the thing that we essentially got down to the

456
00:33:01,590 --> 00:33:03,620
rademacher complexity in the end

457
00:33:03,680 --> 00:33:10,160
by definition and so we end up with a simply

458
00:33:10,260 --> 00:33:15,390
he all efforts and the expected value of this function is the empirical value plus

459
00:33:15,390 --> 00:33:20,320
the rademacher complexity class this extra term we had one begin again

460
00:33:20,330 --> 00:33:22,090
so that's basically it

461
00:33:24,920 --> 00:33:28,340
maybe now let's just look at this rademacher complexity

462
00:33:32,620 --> 00:33:37,630
maybe just go once more through the steps i mean the the critical step was

463
00:33:38,540 --> 00:33:41,020
i mean the first step was quite easy that was just the rademacher

464
00:33:41,520 --> 00:33:45,480
to get to the expected value once we got to the expected value we could

465
00:33:45,480 --> 00:33:46,570
apply this

466
00:33:46,610 --> 00:33:48,310
nice little

467
00:33:50,310 --> 00:33:54,280
of the sample to estimate p of h within the city

468
00:33:54,290 --> 00:33:56,020
that gave us this

469
00:33:56,030 --> 00:34:00,610
expectation over this guy in place of this kind just inequality

470
00:34:00,640 --> 00:34:02,070
because if you just

471
00:34:02,110 --> 00:34:08,080
work out this average and randomize over the choice of the samples you end up

472
00:34:08,080 --> 00:34:09,820
with the true estimation

473
00:34:10,240 --> 00:34:14,380
then we were able to bring this event outside

474
00:34:14,690 --> 00:34:16,310
and only increase

475
00:34:16,320 --> 00:34:21,070
and then we were able to

476
00:34:21,090 --> 00:34:22,790
that gave us that one

477
00:34:22,810 --> 00:34:29,030
then we were able to introduce the symmetrisation by this sigma five plus minus one

478
00:34:29,040 --> 00:34:32,000
swapping of those two in a very simple way

479
00:34:32,050 --> 00:34:33,610
became very simple

480
00:34:35,160 --> 00:34:36,840
it was listed here

481
00:34:36,870 --> 00:34:37,650
and then

482
00:34:38,380 --> 00:34:42,540
made those steps that i made that more explicit to get to this thing and

483
00:34:42,540 --> 00:34:46,440
then we said we done often that's rademacher complexity that's OK

484
00:34:46,530 --> 00:34:49,010
so that's basically it's

485
00:34:49,010 --> 00:34:50,150
is energy

486
00:34:50,170 --> 00:34:53,850
in the electric fields and there energy in the magnetic field you remember that from

487
00:34:53,850 --> 00:34:57,310
a door two

488
00:34:57,370 --> 00:35:00,680
and the energy density

489
00:35:00,700 --> 00:35:05,810
mind the units in terms of joules per cubic metre

490
00:35:05,820 --> 00:35:08,370
forty electric fields we

491
00:35:08,420 --> 00:35:10,840
right for that you

492
00:35:10,880 --> 00:35:12,630
equals one half

493
00:35:12,680 --> 00:35:16,460
actually non-zero east graphs

494
00:35:16,530 --> 00:35:18,850
there's no such thing as a free lunch

495
00:35:18,930 --> 00:35:23,230
you have to do work to create an electric field you have to assemble charges

496
00:35:23,230 --> 00:35:27,650
and bring them together that means work that creates an electric field

497
00:35:27,650 --> 00:35:29,040
and the same is true

498
00:35:30,150 --> 00:35:35,400
the magnetic field never sort you create a magnetic field inside the solenoid that cost

499
00:35:36,790 --> 00:35:38,400
and the energy

500
00:35:39,730 --> 00:35:44,560
my toward density this per cubic metre for magnetic fields

501
00:35:44,590 --> 00:35:45,980
is squared

502
00:35:45,990 --> 00:35:46,960
divided by

503
00:35:46,980 --> 00:35:50,400
two new zero

504
00:35:50,420 --> 00:35:54,730
now traveling wave traveling electromagnetic wave

505
00:35:54,770 --> 00:35:56,070
so this is now

506
00:35:56,070 --> 00:35:58,820
traveling wave

507
00:35:58,850 --> 00:36:01,850
we know that the magnitude of b

508
00:36:01,900 --> 00:36:07,230
it is the magnitude of e at any moment in time divided by c

509
00:36:07,270 --> 00:36:09,340
so i can write this

510
00:36:09,410 --> 00:36:12,120
as the squared

511
00:36:12,120 --> 00:36:15,210
divided by two museo

512
00:36:16,040 --> 00:36:19,680
c squid

513
00:36:19,730 --> 00:36:23,210
but c square is one of actually non-zero you zero

514
00:36:23,210 --> 00:36:25,730
so this is also one half

515
00:36:25,770 --> 00:36:28,730
absolutely zero these grants

516
00:36:28,740 --> 00:36:30,870
and what you see now is so wonderful

517
00:36:30,890 --> 00:36:34,240
so beautiful is symmetric in electromagnetic waves

518
00:36:35,120 --> 00:36:38,930
it's the same as the other one can not exist without the other and look

519
00:36:38,930 --> 00:36:43,580
at the energy density in the electric field of traveling wave is exactly the same

520
00:36:43,580 --> 00:36:45,090
as the energy density

521
00:36:45,140 --> 00:36:46,520
in the

522
00:36:46,560 --> 00:36:49,840
magnetic field

523
00:36:49,840 --> 00:36:51,210
and so the total

524
00:36:51,220 --> 00:36:53,550
energy density

525
00:36:53,580 --> 00:36:56,270
it is the sum of the two

526
00:36:56,340 --> 00:36:58,620
is absolutely zero

527
00:36:58,640 --> 00:37:02,840
thank you great but you can also write that for that

528
00:37:02,890 --> 00:37:05,770
epsilon zero times e times BC

529
00:37:07,590 --> 00:37:09,800
if you prefer that

530
00:37:09,810 --> 00:37:10,710
this of course

531
00:37:10,740 --> 00:37:14,090
is only true in infection

532
00:37:14,150 --> 00:37:16,330
when the speed of propagation

533
00:37:16,340 --> 00:37:20,180
is c

534
00:37:20,430 --> 00:37:21,910
is traveling wave

535
00:37:21,930 --> 00:37:26,060
and this travelling wave moves and so it carries energy with it

536
00:37:26,220 --> 00:37:28,180
now the question is

537
00:37:28,210 --> 00:37:32,490
how much energy flows through an area which is a one square metre

538
00:37:32,520 --> 00:37:33,490
every year

539
00:37:33,490 --> 00:37:37,720
perpendicular to the direction of propagation

540
00:37:37,770 --> 00:37:40,930
so suppose ever box here

541
00:37:40,930 --> 00:37:45,550
and this site is one square metre

542
00:37:46,430 --> 00:37:48,250
and i want to know

543
00:37:48,300 --> 00:37:51,810
how much radiation comes out of there in one second

544
00:37:51,810 --> 00:37:55,450
so the radiation is flowing in this direction with speed c

545
00:37:55,550 --> 00:37:58,140
so in one second this box

546
00:37:58,180 --> 00:38:00,960
it's quite a large box

547
00:38:01,050 --> 00:38:03,050
three times ten thirty eight meters

548
00:38:03,090 --> 00:38:05,900
and all that energy which is in there

549
00:38:05,990 --> 00:38:08,660
will flow through this one square metre

550
00:38:08,710 --> 00:38:11,490
in the time of one second

551
00:38:11,520 --> 00:38:12,430
and so the

552
00:38:12,430 --> 00:38:14,930
dimensions that we're talking about now

553
00:38:14,930 --> 00:38:16,270
these jewels

554
00:38:16,310 --> 00:38:17,690
per second

555
00:38:17,780 --> 00:38:19,340
per square metre

556
00:38:19,400 --> 00:38:20,580
which is also

557
00:38:20,590 --> 00:38:22,440
what's the square meter

558
00:38:22,460 --> 00:38:26,710
and that now is of course the total

559
00:38:26,720 --> 00:38:28,250
energy density

560
00:38:28,370 --> 00:38:30,640
time c

561
00:38:30,650 --> 00:38:32,330
the speed of light

562
00:38:32,340 --> 00:38:34,060
four which you can ride

563
00:38:34,110 --> 00:38:35,310
absolutely zero

564
00:38:37,340 --> 00:38:39,130
times b

565
00:38:39,140 --> 00:38:41,270
time series squared

566
00:38:41,330 --> 00:38:42,780
if you like that

567
00:38:42,840 --> 00:38:46,960
but you can also right-footed each be divided by museo

568
00:38:49,420 --> 00:38:54,830
c square is one of the apsidal zero mu

569
00:38:54,840 --> 00:38:57,890
this should remind you

570
00:38:57,920 --> 00:38:59,300
of something that

571
00:38:59,380 --> 00:39:00,640
is in your far

572
00:39:00,660 --> 00:39:02,410
distant past

573
00:39:02,420 --> 00:39:04,770
which is what we really have called

574
00:39:04,780 --> 00:39:09,880
in a o to the poynting vector in

575
00:39:09,960 --> 00:39:11,820
and the poynting vector

576
00:39:13,710 --> 00:39:14,650
was the

577
00:39:15,840 --> 00:39:21,160
divided by new zero and the units were watts per square meter which is exactly

578
00:39:21,160 --> 00:39:23,160
what this is

579
00:39:23,170 --> 00:39:28,570
and the reason why the cross disappears here is that the electromagnetic travelling waves he

580
00:39:28,640 --> 00:39:30,820
is always perpendicular to be

581
00:39:30,830 --> 00:39:31,700
so that

582
00:39:31,760 --> 00:39:33,770
takes care of the cross

583
00:39:33,780 --> 00:39:38,530
now both e and b are time variable

584
00:39:38,590 --> 00:39:41,920
and so the poynting vector but obviously also be

585
00:39:41,960 --> 00:39:43,690
time variable

586
00:39:45,090 --> 00:39:47,030
it's going to be proportional

587
00:39:47,770 --> 00:39:51,580
or you can write on you some is zero

588
00:39:51,590 --> 00:39:53,600
times cosine only get t

589
00:39:53,670 --> 00:39:54,880
and b

590
00:39:54,900 --> 00:39:56,650
is some be zero

591
00:39:56,650 --> 00:39:59,980
times cosine omega so in the poynting vector

592
00:40:00,080 --> 00:40:01,760
you get the

593
00:40:01,770 --> 00:40:04,880
cosine square of omega t

594
00:40:04,960 --> 00:40:09,540
but since we never interested in the poynting vector on the time scales smaller than

595
00:40:09,540 --> 00:40:12,820
the period of oscillations we want to know the average

596
00:40:12,860 --> 00:40:15,840
over many oscillations what matters there

597
00:40:15,880 --> 00:40:17,100
is the

598
00:40:17,150 --> 00:40:21,720
the average value of cosines square only getting one from the

599
00:40:21,770 --> 00:40:23,450
and the other one from the b

600
00:40:23,510 --> 00:40:27,360
and that is one half

601
00:40:27,410 --> 00:40:29,760
and so we can conclude that

602
00:40:29,890 --> 00:40:33,030
that the average value of the poynting vector

603
00:40:33,080 --> 00:40:35,150
time everest

604
00:40:35,200 --> 00:40:37,090
is one half that value

605
00:40:37,100 --> 00:40:40,890
one half EB

606
00:40:40,960 --> 00:40:43,340
divided by museo

607
00:40:43,400 --> 00:40:45,230
for which you can also right

608
00:40:45,280 --> 00:40:49,590
one half if you want to kill completely over by the way is is one

609
00:40:50,800 --> 00:40:51,860
he zero

610
00:40:51,880 --> 00:40:52,820
the zero

611
00:40:52,830 --> 00:40:57,390
it's important that you have the zero because the cosine square which is one half

612
00:40:57,400 --> 00:40:59,880
so he idea amplitudes

613
00:40:59,890 --> 00:41:02,730
so you can also write for that one half

614
00:41:03,820 --> 00:41:06,440
he zero scranton and so you're i don't for b

615
00:41:06,450 --> 00:41:07,770
divided by c

616
00:41:07,770 --> 00:41:10,080
and then you get downstairs

617
00:41:11,150 --> 00:41:13,030
divided by c

618
00:41:13,080 --> 00:41:15,960
and the reason why i write it in this form is that

619
00:41:15,960 --> 00:41:17,210
it tells you

620
00:41:17,220 --> 00:41:20,470
that if you know what the strength of the

621
00:41:20,500 --> 00:41:22,500
if you this that alone

622
00:41:22,520 --> 00:41:23,630
tells you then

623
00:41:23,650 --> 00:41:28,400
what the poynting vector is because b is coupled to e four maxwell's equations

624
00:41:28,460 --> 00:41:30,530
that we're busy over c

625
00:41:30,540 --> 00:41:32,380
and so all that matters then

626
00:41:32,390 --> 00:41:34,280
if you want to calculate what the

627
00:41:34,290 --> 00:41:36,040
the poynting vector is

628
00:41:36,070 --> 00:41:37,270
is is zero

629
00:41:37,300 --> 00:41:41,390
of course the zero would also be fine

630
00:41:41,400 --> 00:41:44,330
so let's take an example

631
00:41:44,340 --> 00:41:47,340
suppose we have an electromagnetic wave

632
00:41:47,350 --> 00:41:51,280
by easier one hundred volts per meter

633
00:41:51,400 --> 00:41:54,250
then i can calculate now

634
00:41:54,270 --> 00:41:57,330
and it isn't traveling electromagnetic wave

635
00:41:57,340 --> 00:42:01,470
i can calculate what the average value of the poynting vector is

636
00:42:01,480 --> 00:42:02,820
so that would become

637
00:42:03,000 --> 00:42:05,450
one half

638
00:42:05,460 --> 00:42:08,150
nine hundred

639
00:42:08,200 --> 00:42:10,320
divided by

640
00:42:10,330 --> 00:42:13,660
museo zero time c

641
00:42:13,770 --> 00:42:15,770
and if you do your homework on that

642
00:42:15,770 --> 00:42:19,820
you will find that it is thirteen watts per square meter

643
00:42:19,960 --> 00:42:25,890
now if you have exposed yourself

644
00:42:25,960 --> 00:42:29,530
thirteen watts per square meter visible light

645
00:42:29,570 --> 00:42:32,250
an infrared light you take all your clothes off

646
00:42:32,290 --> 00:42:35,890
you expose yourself to that your body will absorb that

647
00:42:35,890 --> 00:42:39,220
because we really collect everything from the golden of the

648
00:42:39,230 --> 00:42:40,760
first page

649
00:42:40,870 --> 00:42:45,970
i did before before the lecture so

650
00:42:45,980 --> 00:42:49,390
living in here in the middle so it appears

651
00:42:50,040 --> 00:42:54,840
all these articles and the second most frequent in this

652
00:42:54,860 --> 00:43:01,560
snippets basically creation when the world cup like you and so on so can select

653
00:43:01,560 --> 00:43:03,850
one of them and then we see

654
00:43:03,860 --> 00:43:07,090
this is positioned in the middle of the

655
00:43:08,790 --> 00:43:10,210
the centre of this

656
00:43:11,880 --> 00:43:16,310
and so on the right we see the other all other name entities which are

657
00:43:16,310 --> 00:43:18,420
common to most related to the

658
00:43:18,430 --> 00:43:22,060
the rest of the work is not just the name

659
00:43:24,890 --> 00:43:26,670
what we can do here we can

660
00:43:30,370 --> 00:43:34,950
the problem

661
00:43:34,950 --> 00:43:36,920
OK we can

662
00:43:36,930 --> 00:43:43,040
one on this into context so in the context context means that we on the

663
00:43:43,040 --> 00:43:45,820
world which the most related

664
00:43:45,850 --> 00:43:52,140
which are the most characteristic for the documents were both slovenia and lithuania in this

665
00:43:52,140 --> 00:43:55,560
case appear together are meaning creation so

666
00:43:55,640 --> 00:43:59,340
o thing in creation will be possible your country serbia whatever

667
00:43:59,360 --> 00:44:00,940
and with some other

668
00:44:00,940 --> 00:44:08,200
some other of this countries with some some some other words would be more relevant

669
00:44:08,210 --> 00:44:10,890
but if you were typing creation and then we'll see

670
00:44:10,920 --> 00:44:16,530
how crazy related to other but in this case just countries but injured could be

671
00:44:16,530 --> 00:44:18,990
any kind of name name is

672
00:44:19,010 --> 00:44:23,520
so if turn on this and then we can also wrote to second level of

673
00:44:23,520 --> 00:44:25,450
the sea

674
00:44:25,470 --> 00:44:27,230
interested in the middle

675
00:44:27,240 --> 00:44:30,820
first thirty seconds or so we see the whole context how

676
00:44:31,910 --> 00:44:34,000
these appear

677
00:44:34,010 --> 00:44:40,120
so you have a problem with this resolution otherwise it is a little bit more

678
00:44:41,820 --> 00:44:48,300
but value you've got the idea that you can show this to other functions which

679
00:44:48,300 --> 00:44:50,160
was supported by this itself so

680
00:44:50,170 --> 00:44:53,320
if we select two name entities we see

681
00:44:55,740 --> 00:44:57,990
we see that the graph how

682
00:44:58,010 --> 00:45:03,110
they are related between each other with other name and this

683
00:45:03,140 --> 00:45:05,800
and so one will be just simple

684
00:45:05,850 --> 00:45:08,940
visualisation which i showed before so weak

685
00:45:09,690 --> 00:45:13,950
graph class clustering can be the vision of the topics is graphs that is the

686
00:45:13,950 --> 00:45:14,760
way how

687
00:45:14,770 --> 00:45:17,760
you can pretty easy seem very short time

688
00:45:19,190 --> 00:45:26,250
what kind of topics are in this particular moment relevant for one in three people

689
00:45:29,640 --> 00:45:31,180
george bush probably

690
00:45:31,340 --> 00:45:41,050
so around the world and is not going to work

691
00:45:52,340 --> 00:45:54,920
now we've got it during bush

692
00:45:54,920 --> 00:45:58,720
and the most relevant topics on the name entities

693
00:45:58,890 --> 00:46:04,610
related to the invasion rock america public conversion motion so and so

694
00:46:07,420 --> 00:46:10,530
we could explore some more

695
00:46:11,530 --> 00:46:14,260
so much about this

696
00:46:14,270 --> 00:46:19,520
now just see

697
00:46:19,950 --> 00:46:25,420
so this was this context and last topic is basically visualizing text using

698
00:46:25,470 --> 00:46:33,200
a lot of structure so this is a bit less usual topic ban talking about

699
00:46:33,200 --> 00:46:35,080
that text vision the things we

700
00:46:35,110 --> 00:46:38,930
did some work in this direction so i thought it might be interesting to mention

701
00:46:39,860 --> 00:46:41,660
so first

702
00:46:47,490 --> 00:46:49,930
so what

703
00:46:50,160 --> 00:46:52,160
what is the structure in the

704
00:46:53,130 --> 00:47:01,330
so all the previous patients methods you don't know much about the actual structure so

705
00:47:01,330 --> 00:47:05,250
first we need to form of the documents including that

706
00:47:05,270 --> 00:47:06,770
structure and then we

707
00:47:06,770 --> 00:47:13,610
you are basically relying on this statistical car occurrences within the next next

708
00:47:13,630 --> 00:47:16,540
the moment the approach is expected to

709
00:47:16,930 --> 00:47:21,060
the the structure so it name it's it's pretty cheap to extract that's why it

710
00:47:21,110 --> 00:47:24,280
was kind of useful but now

711
00:47:24,340 --> 00:47:28,770
text has a lot of structure that the question how to extract

712
00:47:30,520 --> 00:47:32,260
of course the structure

713
00:47:32,300 --> 00:47:38,360
is he he's first linguistic structure in the linguistic structure others but on

714
00:47:38,370 --> 00:47:43,730
kind of its related to semantics says the semantics of the

715
00:47:46,770 --> 00:47:48,700
in this work which really it so

716
00:47:48,710 --> 00:47:55,430
you could possibly find can myself which we published this year beginning with a combination

717
00:47:55,430 --> 00:47:56,230
of this

718
00:47:56,240 --> 00:47:57,770
different approaches

719
00:47:58,820 --> 00:47:59,770
in this case

720
00:47:59,770 --> 00:48:02,040
in the case of these were for the purpose of

721
00:48:02,090 --> 00:48:05,460
summarisation of documents but since

722
00:48:05,460 --> 00:48:10,880
kind of side to side result was also musician that that's why i'm i'm showing

723
00:48:10,910 --> 00:48:12,260
OK just two

724
00:48:12,280 --> 00:48:16,340
show you one flight on how we can extract any more

725
00:48:16,730 --> 00:48:18,740
structured information from them

726
00:48:18,750 --> 00:48:20,530
so let me

727
00:48:20,600 --> 00:48:22,770
using microsoft's

728
00:48:25,590 --> 00:48:27,970
parts are called in the evening

729
00:48:27,980 --> 00:48:31,710
which is also in some of the versions used in

730
00:48:31,850 --> 00:48:38,420
microsoft office that he had access to this operation are educating

731
00:48:40,880 --> 00:48:46,340
here's one example so of one input units and michael so we put in on

732
00:48:46,340 --> 00:48:48,570
the input sentence like this

733
00:48:48,590 --> 00:48:52,220
on the output we get part three

734
00:48:52,230 --> 00:48:56,740
which will be something like this is actually the output of the parts

735
00:48:57,840 --> 00:49:03,600
send it the world past and then here we have some other

736
00:49:03,620 --> 00:49:07,630
you they use as an objective third person

737
00:49:08,080 --> 00:49:10,180
singular proper name

738
00:49:10,190 --> 00:49:12,990
in my likely case indirect

739
00:49:13,480 --> 00:49:18,250
think that but i think the proper name market this

740
00:49:18,260 --> 00:49:24,170
a letter is obvious that he was the first singular and

741
00:49:25,530 --> 00:49:30,790
information which we are able to extract out of its in the past basically

742
00:49:30,810 --> 00:49:34,900
take one sentence and gives this kind of part three of them so the question

743
00:49:34,900 --> 00:49:37,790
now i'll be able to use this force

744
00:49:37,800 --> 00:49:43,240
course think what we need to to do is to

745
00:49:43,780 --> 00:49:47,480
OK already in the second example

746
00:49:47,530 --> 00:49:49,660
so this is the whole procedure what we do

747
00:49:49,660 --> 00:49:53,550
so if we have a set of specs attacks the doctor in this case the

748
00:49:53,550 --> 00:49:59,120
document was is from simple example from three sentences on fire and found

749
00:49:59,140 --> 00:50:02,530
he met a friend was that so

750
00:50:02,530 --> 00:50:03,930
with the low capacity

751
00:50:03,950 --> 00:50:08,310
we're giving space as potentially lots of capacity but we've got to leave the controls

752
00:50:08,920 --> 00:50:12,760
and keeps it under control for the particular problem and of course

753
00:50:12,760 --> 00:50:17,030
the amount of capacity we use will depend on the problem was solved

754
00:50:17,040 --> 00:50:20,840
if the problems were easy we may get a very very large margin and we

755
00:50:20,840 --> 00:50:24,900
may have very low capacity effectively and get very very good generalisation

756
00:50:24,920 --> 00:50:31,950
if the problem is harder OK we may have to use more capacity and correspondingly

757
00:50:31,960 --> 00:50:34,340
more data effectively to get the same

758
00:50:34,360 --> 00:50:36,480
level of performance

759
00:50:36,500 --> 00:50:41,900
OK so that's all i can say about why large margin is good

760
00:50:42,460 --> 00:50:45,540
obviously you know if you're interested in that kind of thing

761
00:50:46,120 --> 00:50:47,840
one can look to

762
00:50:47,860 --> 00:50:55,120
more detailed analysis and statistics but there's a lot written about it in various cases

763
00:50:55,170 --> 00:51:01,590
now this very nice connection between what i've said about large margin and and controlling

764
00:51:01,590 --> 00:51:06,290
generalisation and something known as regularisation and i think actually it's probably better to think

765
00:51:06,910 --> 00:51:11,360
regularisation when you're thinking of controlling capacity in kernel methods

766
00:51:11,360 --> 00:51:14,620
so regularisation is

767
00:51:14,640 --> 00:51:17,200
something that has a an old knowledge history

768
00:51:17,810 --> 00:51:19,360
but the connection

769
00:51:19,390 --> 00:51:22,690
with regularisation comes through this observation

770
00:51:23,180 --> 00:51:25,780
and that is that keeping a large margin

771
00:51:26,080 --> 00:51:30,340
for classifier is equivalent to minimising the norm of the weight vector

772
00:51:30,360 --> 00:51:33,580
while keeping the outputs above a fixed value

773
00:51:33,600 --> 00:51:37,390
so if you think about the way that you measure the margin

774
00:51:37,440 --> 00:51:42,890
it's measured by taking the inner product with the

775
00:51:42,910 --> 00:51:45,220
perpendicular weight vector to the

776
00:51:46,460 --> 00:51:50,510
with that weight vector normalized to length one

777
00:51:50,510 --> 00:51:55,130
so if you just compute that inner product project effectively and that direction

778
00:51:55,190 --> 00:51:59,100
which is what you do if you take into product with no one weight vector

779
00:51:59,110 --> 00:52:02,200
that measures the distance from the hyperplane

780
00:52:02,220 --> 00:52:07,390
now if you now imagine that you scalar vector so that the distance of the

781
00:52:08,560 --> 00:52:13,060
two that sort of closest point is this is actually comes out in the product

782
00:52:13,060 --> 00:52:14,760
of one

783
00:52:14,810 --> 00:52:16,430
then the weight vector

784
00:52:16,480 --> 00:52:21,300
now the norm of the weight vector actually measures one over the marginal

785
00:52:22,040 --> 00:52:25,360
the business sort of inverse relation between the margin

786
00:52:25,370 --> 00:52:27,080
and the norm of the weight vector

787
00:52:27,090 --> 00:52:29,690
if you require that the

788
00:52:29,710 --> 00:52:32,390
in the products of the

789
00:52:32,410 --> 00:52:38,040
the weight vector with the training points all have norm because fixed values one

790
00:52:39,030 --> 00:52:43,360
so in other words if we take that as a sort of criterion we fix

791
00:52:43,520 --> 00:52:46,920
show you that you know how this works in the definition

792
00:52:46,980 --> 00:52:47,940
of the

793
00:52:47,960 --> 00:52:51,550
the optimisation but if you take the outputs in the highland have a fixed value

794
00:52:51,560 --> 00:52:52,440
say one

795
00:52:52,560 --> 00:52:55,450
then minimising the norm of the weight vector

796
00:52:55,480 --> 00:52:57,730
while satisfying this criterion

797
00:52:57,750 --> 00:53:01,690
is equivalent to maximizing the margin

798
00:53:01,710 --> 00:53:04,580
so suddenly you know you start to

799
00:53:04,830 --> 00:53:06,560
see that

800
00:53:06,610 --> 00:53:11,350
expanding the marginal controlling the capacity is equivalent to actually

801
00:53:11,360 --> 00:53:13,690
controlling the norm of the

802
00:53:13,720 --> 00:53:17,360
weight vector that you're using in this high dimensional space

803
00:53:17,360 --> 00:53:21,390
so controlling the norm of the weight that is often is also referred to as

804
00:53:22,900 --> 00:53:28,720
and in fact relates back to classical technique in neural networks known as weight decay

805
00:53:28,770 --> 00:53:30,920
so weight decay in neural networks

806
00:53:30,970 --> 00:53:34,140
is the equivalent if you like of

807
00:53:34,160 --> 00:53:35,840
maximizing the margin

808
00:53:35,850 --> 00:53:37,260
provided you

809
00:53:38,000 --> 00:53:40,480
the output separation

810
00:53:40,500 --> 00:53:43,080
at a fixed value which of course

811
00:53:43,270 --> 00:53:46,010
it wasn't always the case in your learning

812
00:53:47,260 --> 00:53:48,440
but often was too

813
00:53:48,470 --> 00:53:51,790
so in the way you want people already had this intuition

814
00:53:51,800 --> 00:53:56,760
the problem was that they're learning algorithms were much harder to

815
00:53:56,770 --> 00:54:00,780
to make effective

816
00:54:00,790 --> 00:54:02,620
i should just say

817
00:54:02,640 --> 00:54:04,610
there is quite a lot of

818
00:54:04,610 --> 00:54:06,970
misinterpretation of

819
00:54:07,000 --> 00:54:08,800
what this

820
00:54:10,630 --> 00:54:14,440
margin maximisation is doing and this

821
00:54:15,760 --> 00:54:19,300
it's written that it's structural risk minimisation

822
00:54:19,310 --> 00:54:22,890
structural risk minimization is where you take a fixed hierarchy of

823
00:54:22,890 --> 00:54:30,230
everything you might want to be screening people for risk of certain disease diagnosing disease

824
00:54:30,230 --> 00:54:34,950
or predicting the outcome of the treatment prognosis

825
00:54:34,970 --> 00:54:37,710
or discovering new drugs

826
00:54:37,750 --> 00:54:44,770
and in security area recently there has been a renewal of interest in machine learning

827
00:54:45,050 --> 00:54:53,350
because of security problems and their face recognition signature fingerprint or iris verification and DNA

828
00:54:53,350 --> 00:54:56,770
fingerprinting have been important applications of machine learning

829
00:54:56,850 --> 00:55:04,110
in the area of computer and the internet people have been using machine learning to

830
00:55:04,120 --> 00:55:13,350
design better computer interfaces including troubleshooting wizards and handwriting recognition interfaces or speech recognition interfaces

831
00:55:13,650 --> 00:55:21,290
and more recently even brainwaves mostly for handicapped people right now to control computers with

832
00:55:21,290 --> 00:55:24,250
their brainpower directly

833
00:55:24,310 --> 00:55:34,290
and india the number of internet applications including hit ranking spam filtering text categorisation text

834
00:55:34,290 --> 00:55:38,090
translation and recombination

835
00:55:38,150 --> 00:55:45,510
as i mentioned when i introduced myself i organise challenges and recently i've i've organised

836
00:55:45,510 --> 00:55:53,510
two changes for nips two-thousand three and two thousand six and for each challenge there

837
00:55:53,510 --> 00:55:59,610
has been five datasets so in total there are now ten datasets that of former

838
00:55:59,770 --> 00:56:01,330
in uniform way

839
00:56:01,510 --> 00:56:08,050
and the that are available to perform experiments and this span the whole spectrum of

840
00:56:08,060 --> 00:56:12,740
the difficulty in terms of number of inputs the number of examples is also a

841
00:56:12,740 --> 00:56:18,930
wide spectrum of types of applications so different types of inputs

842
00:56:18,950 --> 00:56:24,610
and you can see on this slide the variety of difficulty of tests from a

843
00:56:24,610 --> 00:56:28,690
different point of view from the point of view of the performance of the participants

844
00:56:28,690 --> 00:56:34,350
which you see here other he's diagrams of the performance of the participants some on

845
00:56:35,250 --> 00:56:40,350
x is you have the error rate on the test said it's the balanced error

846
00:56:40,350 --> 00:56:43,370
rate that is the average of the error rate of the positive class and of

847
00:56:43,370 --> 00:56:46,630
the negative class for two clusters education problems

848
00:56:46,850 --> 00:56:50,710
since all these problems are too classification problems

849
00:56:50,750 --> 00:56:59,590
and in some some in some cases like on sylva dataset you have

850
00:57:01,490 --> 00:57:04,900
peaked very peaked distribution towards the origin

851
00:57:04,970 --> 00:57:11,030
which indicates that the task is relatively easy everybody's doing well that task

852
00:57:11,050 --> 00:57:16,190
on the other datasets for example the dorothea dataset you see that there is very

853
00:57:16,410 --> 00:57:20,870
widespread distribution so some people are doing very well and some people are doing very

854
00:57:20,870 --> 00:57:25,970
bad on some other datasets you can see that there is a

855
00:57:25,990 --> 00:57:28,550
almost by model distribution

856
00:57:28,590 --> 00:57:32,650
they that most people do well that some people do bad

857
00:57:32,810 --> 00:57:40,750
actually on the metal dataset it's most apparent that you have a by move distribution

858
00:57:40,770 --> 00:57:46,410
so you'll be having the opportunity in the lab class to play with some of

859
00:57:46,410 --> 00:57:52,710
these datasets and compete with you know the best competitors trying to outperform their performance

860
00:57:53,310 --> 00:57:59,770
in class i taught a year ago in zurich i had the students try to

861
00:57:59,770 --> 00:58:05,570
outperform the participants of the first challenge to units thousand three challenge and they all

862
00:58:05,570 --> 00:58:10,930
did very well we only know outperformed or matched closely the performance of the best

863
00:58:10,930 --> 00:58:13,210
participants so this is doable

864
00:58:13,270 --> 00:58:21,120
and this slide i'm showing how different learning machines perform on different datasets

865
00:58:21,190 --> 00:58:26,890
so i'm very coarsely grouped the learning machines in into four categories the linear and

866
00:58:26,890 --> 00:58:32,270
kernel methods the neural that's the decision trees than random forest methods the naive bayes

867
00:58:32,830 --> 00:58:35,070
in these the ten datasets

868
00:58:35,090 --> 00:58:40,850
and as you can see in terms of you know relative balanced error rates these

869
00:58:41,220 --> 00:58:47,380
balanced error rate over the average balanced error rate of all the participants

870
00:58:47,450 --> 00:58:54,050
the best performers are doing very well in general with linear and kernel methods for

871
00:58:54,050 --> 00:58:59,770
the first two datasets but not not better than other methods on the last four

872
00:59:01,570 --> 00:59:06,870
and no networks can do very well in some cases but they have quite a

873
00:59:06,870 --> 00:59:13,590
bit of variance too on some datasets they don't do better than others and the

874
00:59:13,680 --> 00:59:19,030
decision trees well similarly also they have some datasets on which they don't do well

875
00:59:19,070 --> 00:59:26,730
and a bias which is a very simple method making independence assumptions about the inputs

876
00:59:26,730 --> 00:59:30,610
so it's kind of you know the baseline or reference methods for all the machine

877
00:59:30,610 --> 00:59:36,790
learning techniques can do well on on some datasets on which actually other ones do

878
00:59:36,790 --> 00:59:38,270
well also

879
00:59:38,350 --> 00:59:44,370
but generally performs worse so we'll see that you know one of the games in

880
00:59:44,370 --> 00:59:48,870
machine learning is to predict ahead of time which learning machine is going to be

881
00:59:48,870 --> 00:59:54,550
performing well before you've seen data or reserve or chunk of data eyes radiation to

882
00:59:54,550 --> 00:59:59,150
decide between various learning machines which when you think is going to be performing best

883
00:59:59,150 --> 01:00:00,440
in the future

884
01:00:00,550 --> 01:00:08,690
in this class the convention is going to be you that the data are going

885
01:00:08,690 --> 01:00:14,790
to be represented as the matrix the lines of which represent examples and the columns

886
01:00:14,790 --> 01:00:18,130
represent the features or variables

887
01:00:18,170 --> 01:00:23,290
so for example this would be patients in in lines and for each patient you

888
01:00:23,290 --> 01:00:28,990
would have a record of the age of the weight of the number of children

889
01:00:29,400 --> 01:00:34,450
and whatever information is really relevant about this patient

890
01:00:34,470 --> 01:00:39,260
in terms of learning machines a lot of the learning machines they way

891
01:00:39,490 --> 01:00:48,900
the the columns of the lines in order to determine the decision function that they're

892
01:00:48,900 --> 01:00:54,490
are going to be using to make predictions so if you if we're weighting the

893
01:00:55,630 --> 01:00:59,670
columns then we're going to be calling the weights w and if we're weighting the

894
01:00:59,670 --> 01:01:02,950
lines we're going to be calling the weights alpha

895
01:01:03,010 --> 01:01:08,440
and there is one special call at that time here separating from others which is

896
01:01:08,440 --> 01:01:15,010
the target so this is the quantity that you want to predict for example the

897
01:01:15,010 --> 01:01:18,830
health status of the patient is the patient healthy or diseased

898
01:01:18,850 --> 01:01:27,050
you can going i'm showing you know several instances of the same matrix

899
01:01:27,070 --> 01:01:34,850
in so-called unsupervised learning problems we are caring about with the structure of the is

900
01:01:34,850 --> 01:01:39,810
there are not structuring data on the problem being that we don't know what the

901
01:01:39,810 --> 01:01:44,910
target y is in in supervised learning we want to predict an outcome so an

902
01:01:44,910 --> 01:01:50,140
what's the matter computing the related is having an estimate which is good or related

903
01:01:50,160 --> 01:01:56,340
clients OK so the related via being defined essentially by divided by the square of

904
01:01:56,350 --> 01:02:01,400
the expectation OK so in this case what you find what you would find is

905
01:02:01,400 --> 01:02:07,220
that essentially the value store violence would be something which indeed decreasing in one of

906
01:02:07,220 --> 01:02:12,050
the capital and but basically one of pnx is extremely small so they lead to

907
01:02:12,050 --> 01:02:17,390
violence is really really huge so if you want to relate violence of saleable the

908
01:02:17,400 --> 01:02:18,850
ten minus two

909
01:02:18,860 --> 01:02:22,010
it means that essentially in the dimension twenty

910
01:02:22,040 --> 01:02:27,050
you will need like four billion samples OK on four and fourteen you in electing

911
01:02:27,060 --> 01:02:30,280
to the point four twenty sample OK so

912
01:02:30,300 --> 01:02:32,190
with the calorimeters

913
01:02:32,200 --> 01:02:35,290
nice internal rate of convergence but in the scenario

914
01:02:35,300 --> 01:02:40,000
he doesn't break the curse of dimensionality in this case it shouldn't be a kind

915
01:02:40,000 --> 01:02:44,270
of a mystery to you if you saw paul blind in this space when you

916
01:02:44,270 --> 01:02:48,760
try to do the sphere in using sample for that hypercube is really like finding

917
01:02:48,760 --> 01:02:53,110
a needle in a stack in such high dimension on this essentially such blind monte

918
01:02:53,110 --> 01:02:56,880
carlo methods will be a bit stupid so you just a bit of cautionary warning

919
01:02:57,140 --> 01:02:58,770
about semantic elements

920
01:02:59,700 --> 01:03:03,200
so now let's move to the light that's it for the really

921
01:03:03,220 --> 01:03:07,830
g introduction to model the calorimeter so let's move on to do prime trying to

922
01:03:07,830 --> 01:03:09,250
solve the case

923
01:03:11,030 --> 01:03:15,660
what i'm going to be interested in doing is computing essentially just picking coming up

924
01:03:15,660 --> 01:03:22,810
with estimates of expectation of function f with respect to apply distribution pyrex so typically

925
01:03:22,840 --> 01:03:26,290
was your latter point may not be initially posed this way that you have to

926
01:03:26,290 --> 01:03:31,280
write it as an expectation respect appointed distribution i assume for the time being that

927
01:03:31,280 --> 01:03:32,810
you have done the job for me

928
01:03:35,720 --> 01:03:40,170
basically he applies be essentially an arbitrary point p delta function

929
01:03:40,170 --> 01:03:45,360
i assume that were basically rnx but could be any space this knowledge if is

930
01:03:45,360 --> 01:03:50,740
that essentially nothing particularly been working with real numbers

931
01:03:52,180 --> 01:03:54,030
i will introduce some notation

932
01:03:54,050 --> 01:04:01,590
basically we use later on quite lot so i will introduce the delta function

933
01:04:01,590 --> 01:04:07,200
that x not which is basically it integrates with any function f respect to tell

934
01:04:07,310 --> 01:04:11,620
that you are function OK that's not equal to ethics and so i will choose

935
01:04:11,620 --> 01:04:13,720
merge authority notation

936
01:04:13,740 --> 01:04:15,530
in this set of lectures

937
01:04:15,550 --> 01:04:20,790
you could obviously to be completely rigorous obviously should write delta x not dx instead

938
01:04:20,790 --> 01:04:26,050
of using this was not quite rigorous but as the notation abducted not to overload

939
01:04:27,220 --> 01:04:30,950
OK so if you think i want to tell really doing

940
01:04:31,840 --> 01:04:36,600
it's important to give you an sample distributed according to pi

941
01:04:36,610 --> 01:04:42,530
it previously that was my drops that will distributed according to the uniform distribution then

942
01:04:42,530 --> 01:04:47,420
pbgd what you're doing what is the colour corresponds to a correspond to doing an

943
01:04:48,790 --> 01:04:55,900
of the initial distribution of interest approximating it by an empirical measure which is essentially

944
01:04:55,910 --> 01:05:00,810
basically as the re weighted sum of the mass located at the particle at the

945
01:05:00,810 --> 01:05:03,380
one ensembles location OK

946
01:05:03,380 --> 01:05:06,100
so monic element identity y

947
01:05:06,110 --> 01:05:11,120
basically it's not bad if that's stupid despite the fact that if you do things

948
01:05:11,120 --> 01:05:12,750
applies to efficient

949
01:05:12,830 --> 01:05:16,000
simply because essentially you approximate

950
01:05:16,030 --> 01:05:17,710
basically the

951
01:05:17,720 --> 01:05:22,150
target distribution of interest by x by simply so set of one the

952
01:05:22,280 --> 01:05:28,580
points which automatically concentrated then set the norwegian of put that's OK that's why it's

953
01:05:28,580 --> 01:05:30,210
quite clever

954
01:05:30,220 --> 01:05:35,390
so this is a bit funny way of reasoning because whereas when you're doing stuff

955
01:05:35,390 --> 01:05:40,190
the biometric statistics you like samples the data on you try to come up with

956
01:05:40,210 --> 01:05:44,430
function or foundation of your data so you try to approximate that go should have

957
01:05:44,430 --> 01:05:49,770
given me is when you're doing with the calendar year elsewhere you have typically

958
01:05:49,790 --> 01:05:54,510
expression for the target distribution of five on you come up we basically we've

959
01:05:55,520 --> 01:06:00,730
this the multicolour presentation of it from samples because of the kind of we have

960
01:06:00,780 --> 01:06:02,140
way of thinking

961
01:06:02,140 --> 01:06:07,200
OK so this is important for example so this is my mother multicolour approximation of

962
01:06:07,230 --> 01:06:08,800
univariate gaussian

963
01:06:08,830 --> 01:06:13,390
it's in OECD to make such approximation why dimension on it is full dimensional to

964
01:06:13,450 --> 01:06:20,510
element should be precluded but thought i'd mention essentially the concentration of essentially one sample

965
01:06:20,670 --> 01:06:24,670
original by quality mass going be quite actually want

966
01:06:25,250 --> 01:06:28,200
so montecarlo i'm just for doing very quickly

967
01:06:28,220 --> 01:06:29,400
the the

968
01:06:29,420 --> 01:06:35,080
benefits of basically this estimates assume you have evidence on paul this to be according

969
01:06:35,080 --> 01:06:39,100
to buy you want to approximate the expectation of a function f with respect by

970
01:06:39,120 --> 01:06:44,710
what you do very simple you simply substituted by the empirical measure of the sampled

971
01:06:44,710 --> 01:06:48,890
is always HC over lambda right electron is moving at the speed of light you

972
01:06:48,890 --> 01:06:53,910
has to have some other expression for the ratio of the Planck constant to its

973
01:06:53,910 --> 01:06:59,140
momentum and this is just the Newtonian expression of momentum the product of the mass

974
01:06:59,150 --> 01:07:01,990
of the electron times it's some philosopher

975
01:07:02,030 --> 01:07:02,670
actually said

976
01:07:03,560 --> 01:07:11,160
so what are the consequences that how can we go somewhere with their value recalled

977
01:07:11,180 --> 01:07:12,800
in them in

978
01:07:14,990 --> 01:07:22,510
the quantum condition more express the quantum conditioned by the angular momentum quantum condition in

979
01:07:22,510 --> 01:07:30,080
the following manner boss said that the angular momentum and the is quantized where n

980
01:07:30,080 --> 01:07:37,660
is this integer counter h over to apply this is the part proportionality that's multiplied

981
01:07:37,660 --> 01:07:45,230
by the so if if the correct we could then model the electrons in its

982
01:07:45,230 --> 01:07:50,860
orbit not moving as a particle but let's model it is a way

983
01:07:50,930 --> 01:07:55,900
so will come away visit is only 2 kinds of ways this traveling waves and

984
01:07:55,900 --> 01:08:00,380
the standing waves so this is stationary orbit so we need to have a standing

985
01:08:00,380 --> 01:08:01,470
wave so

986
01:08:02,030 --> 01:08:07,060
not to scale so let's imagine this is the electron

987
01:08:07,060 --> 01:08:09,750
in its orbit

988
01:08:09,800 --> 01:08:12,030
it's understanding a standing wave configuration

989
01:08:12,260 --> 01:08:17,120
all right so there's a geometric constraints right so

990
01:08:17,160 --> 01:08:22,910
In order to have an electron in a stationary orbit

991
01:08:22,940 --> 01:08:26,880
electron in stationary orbit

992
01:08:26,880 --> 01:08:34,120
this implies a standing wave a standing wave means there's a geometric constraints which is

993
01:08:34,120 --> 01:08:39,600
what I have to have a whole number of wavelengths to get around the circumference

994
01:08:39,600 --> 01:08:42,510
so the circumference is 2 pi R

995
01:08:43,600 --> 01:08:48,140
it is going to be a standing wave than this must be an integral number

996
01:08:48,140 --> 01:08:49,740
of wavelengths

997
01:08:49,750 --> 01:08:54,990
the boss told us that actually Debreu has told us that the wavelength is related

998
01:08:54,990 --> 01:09:00,530
to the incident velocity through this formula so let's let's substitute that will give us

999
01:09:00,530 --> 01:09:08,510
2 pi are those areas and age over and the I cross multiply here that

1000
01:09:08,510 --> 01:09:14,750
will give me and the r equals n h over 2 pi

1001
01:09:16,360 --> 01:09:17,840
the client condition of

1002
01:09:18,580 --> 01:09:26,620
falls out of three-line derivation if you accept the bride's hypothesis that the electron in

1003
01:09:26,630 --> 01:09:31,320
this set of circumstances can be modeled as a way

1004
01:09:33,660 --> 01:09:40,300
so he got his PhD thesis and in 1929 he gets a Nobel Prize he's

1005
01:09:40,310 --> 01:09:46,800
off to a flying start Einstein right the thesis love the loved what we care

1006
01:09:46,800 --> 01:09:53,620
what Einstein says Einstein's only if irritation and Abreu is theoreticians so 1 the retention

1007
01:09:54,060 --> 01:09:59,980
propping up another the retention is mutual admiration society we need lot

1008
01:09:59,990 --> 01:10:03,560
we need evidence we need experimental evidence

1009
01:10:03,560 --> 01:10:07,950
so let's go back to high school for a moment just by way of background

1010
01:10:08,320 --> 01:10:13,710
remember these water tanks we could put a vibratory water taken this is top view

1011
01:10:13,710 --> 01:10:18,060
of a water taken if you had a motor here that was causing a paddle

1012
01:10:18,070 --> 01:10:21,900
to move up and down you could cause waves to form at 1 end of

1013
01:10:21,900 --> 01:10:24,270
the tank and moved from left to right

1014
01:10:25,180 --> 01:10:29,500
and furthermore you could do little experiment so for example you could put on a

1015
01:10:29,500 --> 01:10:36,720
dam in the middle of the tank and if the distance between the wall

1016
01:10:37,020 --> 01:10:44,780
In the dam was large in comparison to the wavelength then what happens

1017
01:10:44,990 --> 01:10:49,300
if this distance is large in comparison to the wave like this is then simply

1018
01:10:49,300 --> 01:10:50,950
casts a shadow

1019
01:10:50,970 --> 01:10:58,230
this like like casting a shadow was so part of our approach so this is

1020
01:10:58,230 --> 01:11:10,730
simple when D is large in comparison to land they are obstacles obstacles cast shadows

1021
01:11:10,730 --> 01:11:18,580
and this is equivalent to modeling the wave as particles this is ray optics

1022
01:11:18,620 --> 01:11:22,640
this is the equivalent to ray optics isn't

1023
01:11:22,820 --> 01:11:27,800
you don't have to know anything about wave like behaviour you got front coming here

1024
01:11:28,140 --> 01:11:29,640
you've got an obstacle

1025
01:11:29,690 --> 01:11:32,120
where you can get by it moves through

1026
01:11:32,190 --> 01:11:38,560
but in the other case where the gaps

1027
01:11:39,190 --> 01:11:42,380
this is the spacing

1028
01:11:42,410 --> 01:11:52,320
when the the spacing was small in comparison to the wavelength the obstacles did not

1029
01:11:52,320 --> 01:11:57,170
cast a shadow the obstacles did not cast a shadow you see a tiny tiny

1030
01:11:57,170 --> 01:12:03,690
little stream coming out of each of these orifices instead you saw this recall

1031
01:12:03,690 --> 01:12:07,130
in this case just picking one feature at the time

1032
01:12:07,230 --> 01:12:10,670
and if that value of the feature was largely

1033
01:12:10,730 --> 01:12:14,820
reject we remove that that with high probability if it was small we kept do

1034
01:12:14,860 --> 01:12:17,860
with high probability of vice versa

1035
01:12:17,980 --> 01:12:21,980
and what you can see here is the test error

1036
01:12:23,460 --> 01:12:24,770
this solid green

1037
01:12:24,800 --> 01:12:25,960
dark green line

1038
01:12:25,980 --> 01:12:28,190
is what you do if you don't do any great what you get if you

1039
01:12:28,190 --> 01:12:30,340
don't do any correction

1040
01:12:32,550 --> 01:12:33,270
the mean

1041
01:12:33,270 --> 01:12:35,090
the middle bar is what you do

1042
01:12:35,110 --> 01:12:39,000
what you get if you actually knew the by some distribution

1043
01:12:39,020 --> 01:12:42,030
which in our case meaning you know because we have

1044
01:12:42,050 --> 01:12:44,610
explicit bias in one

1045
01:12:44,650 --> 01:12:47,070
the right bar is the are the ticket

1046
01:12:47,130 --> 01:12:51,840
if you use the moment matching four three waiting

1047
01:12:51,900 --> 01:12:54,790
they should be very surprised about the fact

1048
01:12:54,790 --> 01:12:58,070
that in all these cases the moment matching model

1049
01:12:58,130 --> 01:13:04,170
outperforms the importance sampler

1050
01:13:04,190 --> 01:13:08,360
even so for the moment matching and not assume that all that they know what

1051
01:13:08,400 --> 01:13:10,770
distribution looks like

1052
01:13:10,820 --> 01:13:14,690
not just recover the weighting coefficients

1053
01:13:14,710 --> 01:13:17,800
that sounds like magic doesn't it

1054
01:13:17,860 --> 01:13:19,590
well the reason why

1055
01:13:19,610 --> 01:13:24,770
it outperforms at least believe that this is the reason is because when making use

1056
01:13:24,770 --> 01:13:29,530
of the fact that you have you noticed started it explicitly disposition

1057
01:13:29,550 --> 01:13:31,980
so basically transduction

1058
01:13:32,030 --> 01:13:38,960
by more than what you have to pay for not knowing the distribution

1059
01:13:39,050 --> 01:13:42,860
and so you would think OK well that's just one dollar maybe that's just specific

1060
01:13:42,860 --> 01:13:46,440
to breast cancer

1061
01:13:46,440 --> 01:13:49,550
now actually what happened is that j one so she was the one who did

1062
01:13:50,570 --> 01:13:51,940
and it's something where

1063
01:13:51,960 --> 01:13:53,340
i initially thought well

1064
01:13:53,400 --> 01:13:57,020
stupid and if she had asked me whether this is a good idea would have

1065
01:13:57,020 --> 01:14:00,050
really try talking about this experiment

1066
01:14:00,130 --> 01:14:02,070
but you did it anyway

1067
01:14:02,090 --> 01:14:04,630
and it turned out to be really great

1068
01:14:04,670 --> 01:14:08,300
so what it she she thought you know i mean this passing has worked on

1069
01:14:08,300 --> 01:14:09,670
the docks

1070
01:14:09,690 --> 01:14:13,290
now let's do something that actually would break all of our assumption

1071
01:14:13,320 --> 01:14:16,650
namely he had have actually boss the labels

1072
01:14:16,670 --> 01:14:18,750
so the distribution

1073
01:14:18,750 --> 01:14:20,770
over sick and healthy patient

1074
01:14:20,770 --> 01:14:24,980
it would be different on between training and test it

1075
01:14:25,000 --> 01:14:30,360
so she basically artificially reweighted to taste it

1076
01:14:30,380 --> 01:14:33,520
which doesn't make any sense in the way i mean what the theory shouldn't really

1077
01:14:33,520 --> 01:14:34,840
work right

1078
01:14:34,880 --> 01:14:38,190
at least at the moment we don't have a good theory for it

1079
01:14:38,190 --> 01:14:41,440
but surprising thing was that actually did this

1080
01:14:41,460 --> 01:14:44,820
and she is just different degrees of boxing

1081
01:14:44,860 --> 01:14:49,050
the method still works really well

1082
01:14:49,190 --> 01:14:50,630
so this is probably

1083
01:14:50,650 --> 01:14:53,480
this is the big surprise came out of it

1084
01:14:53,610 --> 01:14:56,070
the plots he would kind of expect

1085
01:14:56,110 --> 01:14:57,710
that's when i was really

1086
01:14:57,800 --> 01:15:00,730
very surprised when i saw them

1087
01:15:02,940 --> 01:15:06,610
now you can have a look at the big table and we use different datasets

1088
01:15:06,610 --> 01:15:10,920
for regression classification what have you and we just

1089
01:15:10,940 --> 01:15:13,690
looked at what you do if you don't get

1090
01:15:13,750 --> 01:15:17,020
if you don't go to a correction

1091
01:15:17,070 --> 01:15:20,920
if you do the correction by knowing the true distribution

1092
01:15:21,750 --> 01:15:25,230
if you use the moment matching

1093
01:15:26,130 --> 01:15:31,020
in the majority of all cases the moment matching outperforms the importance sampling

1094
01:15:31,020 --> 01:15:38,920
in the cases where it doesn't outperform it it's actually fairly close

1095
01:15:38,940 --> 01:15:40,800
so for instance i mean

1096
01:15:40,840 --> 01:15:48,460
in this case well it's point o to one two point one three likewise here

1097
01:15:48,480 --> 01:15:51,650
OK that was maybe not so great but the variance is

1098
01:15:51,710 --> 01:15:56,270
a very large anyway but as you can see it actually doesn't really good job

1099
01:15:56,320 --> 01:16:01,630
so for practical purposes this is really as good as knowing the poisson distribution

1100
01:16:01,750 --> 01:16:03,670
and it's so easy to implement

1101
01:16:03,710 --> 01:16:08,750
so the credit program

1102
01:16:10,500 --> 01:16:11,500
so this is

1103
01:16:11,520 --> 01:16:13,440
basically an affliction number

1104
01:16:14,900 --> 01:16:20,460
and now we're going to look at distribution test for independence

1105
01:16:20,480 --> 01:16:24,770
now the time for questions

1106
01:16:24,770 --> 01:16:27,030
yet showing

1107
01:16:28,840 --> 01:16:33,380
is that we will be

1108
01:16:33,400 --> 01:16:36,500
close your

1109
01:16:36,500 --> 01:16:38,250
however how should we

1110
01:16:39,280 --> 01:16:40,820
match those

1111
01:16:43,310 --> 01:16:44,450
OK so

1112
01:16:44,460 --> 01:16:45,310
you may

1113
01:16:45,320 --> 01:16:46,150
you know

1114
01:16:46,160 --> 01:16:48,570
derive different matching

1115
01:16:48,580 --> 01:16:52,700
according to different background knowledge different countries

1116
01:16:52,760 --> 01:16:56,130
so this is another application

1117
01:16:56,140 --> 01:17:00,860
where uncertainty material

1118
01:17:05,610 --> 01:17:08,640
the next one is about search

1119
01:17:10,940 --> 01:17:11,910
for example

1120
01:17:12,340 --> 01:17:19,170
if you look at those literature on entity detection digital library so often they use

1121
01:17:19,170 --> 01:17:24,160
one example is a you search one chinese name way one which is really very

1122
01:17:24,160 --> 01:17:29,680
popular chinese name and then the competition is that how many way one can you

1123
01:17:31,780 --> 01:17:36,060
so if you google a while got the right along this of people differently once

1124
01:17:36,070 --> 01:17:39,270
you have no clue who is what

1125
01:17:39,310 --> 01:17:45,020
and you say because the web is so messy data so if you go to

1126
01:17:45,020 --> 01:17:47,160
some clean the data sources b

1127
01:17:47,170 --> 01:17:50,280
we may have a better situation in unfortunately

1128
01:17:50,290 --> 01:17:55,640
the answer is no because the BLP well to some states of really you know

1129
01:17:55,990 --> 01:18:00,090
about the core of better quality than just the rollback right however if you go

1130
01:18:00,090 --> 01:18:05,790
to the DBLP is still can find so many we once in that some entries

1131
01:18:05,790 --> 01:18:08,880
contain more than one way one

1132
01:18:08,900 --> 01:18:12,160
you have an impact in interview individuals

1133
01:18:12,210 --> 01:18:13,260
so you can see

1134
01:18:13,270 --> 01:18:17,280
if you want to have you know being information to individuals

1135
01:18:17,360 --> 01:18:18,900
by search

1136
01:18:18,910 --> 01:18:21,280
naturally you have the uncertainty

1137
01:18:24,270 --> 01:18:26,040
another example

1138
01:18:26,300 --> 01:18:33,070
suppose you want to fill up custom customer form in the web page so often

1139
01:18:33,070 --> 01:18:37,320
parties for example you want to apply for continuous bay or you want to apply

1140
01:18:37,710 --> 01:18:39,170
for confi lumber

1141
01:18:42,170 --> 01:18:43,090
when he

1142
01:18:43,110 --> 01:18:46,560
annoying they always as you for actors both a

1143
01:18:46,580 --> 01:18:50,740
or this person information and so many customers in they don't want to give the

1144
01:18:51,870 --> 01:18:55,830
they just want to go through the form as soon as possible and then

1145
01:18:56,080 --> 01:18:57,950
after service

1146
01:18:59,030 --> 01:18:59,840
what do you

1147
01:18:59,870 --> 01:19:01,650
often do

1148
01:19:01,660 --> 01:19:02,540
four b

1149
01:19:02,540 --> 01:19:07,230
at least i often quite user asked me for the state in i do not

1150
01:19:07,470 --> 01:19:11,280
believe in the united states but i still want to get the service what can

1151
01:19:11,280 --> 01:19:17,680
i do and i just get the job at least could the first one alabama

1152
01:19:18,280 --> 01:19:22,730
so many people use jennifer's as to their birthdays

1153
01:19:24,420 --> 01:19:27,210
when you look at this kind of data in that when you look at the

1154
01:19:27,210 --> 01:19:31,580
data you do it if you do not do any analysis you find the data

1155
01:19:31,660 --> 01:19:35,180
is screen is also a complete

1156
01:19:35,190 --> 01:19:36,710
nothing is missing

1157
01:19:37,910 --> 01:19:42,250
the concept really inherently the data is missing

1158
01:19:42,280 --> 01:19:46,110
OK so we call these as these guys missing data

1159
01:19:46,260 --> 01:19:50,130
and then discuss using a there is a very serious problem because if you do

1160
01:19:50,130 --> 01:19:54,590
not analyse the uncertainty of missing values here

1161
01:19:54,610 --> 01:20:00,170
you make very wrong business decision for example you find so many customers i in

1162
01:20:01,180 --> 01:20:04,480
then this go there for all a lot of sourced there

1163
01:20:04,520 --> 01:20:06,260
and there's a lot of money there

1164
01:20:06,300 --> 01:20:09,070
you see that was the result right

1165
01:20:11,000 --> 01:20:16,480
so this is disguised missing data and it also introduces the uncertainty the uncertainty here

1166
01:20:16,490 --> 01:20:22,910
is that was the view URL and more importantly whether these customer these you have

1167
01:20:22,910 --> 01:20:24,430
you value or not

1168
01:20:24,440 --> 01:20:29,050
this is absurd

1169
01:20:29,060 --> 01:20:35,340
then we talk about so many application examples of uncertainty

1170
01:20:35,430 --> 01:20:37,890
in and said in our data

1171
01:20:37,930 --> 01:20:40,980
and you may ask one question well

1172
01:20:40,990 --> 01:20:45,610
uncertainty sounds a very bad thing why do we still want to look at a

1173
01:20:45,630 --> 01:20:46,630
certain date

1174
01:20:46,640 --> 01:20:49,180
why and data is still useful

1175
01:20:49,870 --> 01:20:54,740
in fact all the data could be to some extent uncertain

1176
01:20:54,860 --> 01:21:00,760
but even if we can model the uncertainty pop three we still can derive some

1177
01:21:00,760 --> 01:21:04,530
useful and useful information from such data

1178
01:21:04,530 --> 01:21:10,740
for example for temperature sensor if the difference between the real temperature and sense temperature

1179
01:21:10,760 --> 01:21:12,560
follows a normal distribution

1180
01:21:12,710 --> 01:21:16,670
then if you have a question is that outside where the temperature is all the

1181
01:21:16,670 --> 01:21:17,870
forty degree

1182
01:21:17,890 --> 01:21:19,680
but this is uncertain

1183
01:21:19,720 --> 01:21:21,400
however if you want to say

1184
01:21:21,440 --> 01:21:22,850
what's the probability

1185
01:21:23,620 --> 01:21:24,700
this is

1186
01:21:24,700 --> 01:21:27,650
if you want to have situation where

1187
01:21:31,110 --> 01:21:35,680
therefore you as you can see when we analyse uncertain data one important thing we

1188
01:21:35,680 --> 01:21:38,030
need to do is that when we

1189
01:21:38,030 --> 01:21:39,670
the questions

1190
01:21:39,680 --> 01:21:45,930
but when we asked the questions the answers could should be probabilistic

1191
01:21:45,940 --> 01:21:48,350
and the answers should be

1192
01:21:48,370 --> 01:21:51,300
at some some sort of aggregate level

1193
01:21:51,340 --> 01:21:52,780
this is important

1194
01:21:52,830 --> 01:21:54,580
for example

1195
01:21:54,630 --> 01:22:00,780
these kind of questions how we can estimate the percentage of married vote voters supporting

1196
01:22:00,780 --> 01:22:05,220
obama from the way data these kind of questions is so popular so frequent in

1197
01:22:05,220 --> 01:22:06,970
these days

1198
01:22:06,980 --> 01:22:12,320
i guess and you know when verview see all kinds of such questions every week

1199
01:22:15,430 --> 01:22:20,230
given such answers and the answer has to come with some confidence

1200
01:22:20,270 --> 01:22:22,920
some quantity measures of the answer

1201
01:22:27,320 --> 01:22:31,680
you have to ask another question about uncertainty is somewhat look

1202
01:22:31,680 --> 01:22:32,380
can we

1203
01:22:32,400 --> 01:22:34,650
i removed and sent by some way

1204
01:22:34,840 --> 01:22:37,520
well there's something called the clinic

1205
01:22:37,550 --> 01:22:39,000
which may help

1206
01:22:39,070 --> 01:22:44,640
OK and when we just apply data cleaning to no remove all the uncertainty and

1207
01:22:44,640 --> 01:22:47,700
then we have cleaned and sorted data

1208
01:22:47,710 --> 01:22:51,170
just some serious problem the first problem is

1209
01:22:51,860 --> 01:22:54,010
how do i remove this uncertain data

1210
01:22:54,050 --> 01:22:57,670
one thing you can do is that well you can remove all the uncertain actually

1211
01:22:58,700 --> 01:23:01,840
and you can remove all the uncertain records

1212
01:23:02,640 --> 01:23:07,470
the problem here is that in fact you reduce the available data

1213
01:23:07,470 --> 01:23:11,130
for some applications you have you know enough data

1214
01:23:11,150 --> 01:23:12,930
band is so you could be OK

1215
01:23:12,940 --> 01:23:14,780
but for some application

1216
01:23:14,790 --> 01:23:18,230
you only have one of the very small amount of data

1217
01:23:18,270 --> 01:23:20,090
and the data

1218
01:23:20,180 --> 01:23:25,380
has the inherent concur that uncertainty then it is a problem let me give you

1219
01:23:25,380 --> 01:23:26,360
one example

1220
01:23:27,540 --> 01:23:31,300
medical even if metrics of health informatics

1221
01:23:31,380 --> 01:23:32,440
there's so many

1222
01:23:32,440 --> 01:23:34,000
kind of disease

1223
01:23:34,010 --> 01:23:38,060
in the whole world only collect very small number of cases

1224
01:23:38,080 --> 01:23:42,930
and for each case you may have hundreds of attributes

1225
01:23:42,970 --> 01:23:44,460
but only

1226
01:23:44,500 --> 01:23:48,960
but only few of them a complete most of them in that contain a lot

1227
01:23:48,960 --> 01:23:50,620
of uncertainties

1228
01:23:50,650 --> 01:23:54,390
then you can also just say i know audience the matters because in such a

1229
01:23:54,470 --> 01:23:57,760
if you do that you lose all the cases

1230
01:23:57,790 --> 01:23:59,360
OK so this is one

1231
01:24:01,170 --> 01:24:04,440
we review removing unsourced entries may not be

1232
01:24:04,750 --> 01:24:06,560
may not work

1233
01:24:06,590 --> 01:24:07,750
and we

1234
01:24:07,760 --> 01:24:12,320
you think about is the uncertainty in fact to some extent is related to data

1235
01:24:12,320 --> 01:24:13,740
generated by

1236
01:24:13,750 --> 01:24:18,900
for example if you don't tell me exactly which at the street edges of

1237
01:24:19,060 --> 01:24:23,320
of your home but you tell me the city or the state

1238
01:24:24,180 --> 01:24:26,220
how about we raise

1239
01:24:26,280 --> 01:24:27,960
the data and it is

1240
01:24:29,900 --> 01:24:30,780
to the level

1241
01:24:30,790 --> 01:24:33,330
things are certain

1242
01:24:33,340 --> 01:24:36,110
then we have we have a beautiful work

1243
01:24:37,330 --> 01:24:40,400
when you raise the analyse level

1244
01:24:40,400 --> 01:24:46,050
basically you have to do the same stuff as in normal mixture models you introduce

1245
01:24:46,050 --> 01:24:51,100
a hidden variable which year is one if the density at this point in space

1246
01:24:51,100 --> 01:24:56,160
x is explained by a certain component which is currently

1247
01:24:56,170 --> 01:25:00,980
well basically kernel offered offer certain data point and their else rising and indeed you

1248
01:25:00,980 --> 01:25:02,110
do the normal

1249
01:25:02,380 --> 01:25:03,260
am stuff

1250
01:25:03,290 --> 01:25:05,420
and you come up with this

1251
01:25:05,420 --> 01:25:06,990
e step

1252
01:25:07,960 --> 01:25:12,060
if you plug causes tetanus into the m step we get a nice from of

1253
01:25:12,090 --> 01:25:13,670
i've shown you before

1254
01:25:13,690 --> 01:25:20,050
so this shows that although many of maximizing

1255
01:25:20,050 --> 01:25:25,160
the complete light duty also maximizes the data likelihood which is basically in this case

1256
01:25:25,160 --> 01:25:29,370
the kernel density estimate and when starting the EM with

1257
01:25:29,400 --> 01:25:32,980
well the data points

1258
01:25:32,990 --> 01:25:35,970
you do the hill climbing for that data point

1259
01:25:36,010 --> 01:25:40,030
and so we we got nice formulation

1260
01:25:40,370 --> 01:25:42,220
four our dental organisms

1261
01:25:42,230 --> 01:25:49,360
and how much faster hit climbing procedure and then slide problems come into play and

1262
01:25:49,360 --> 01:25:54,480
now we have to identify the local maxima slightly differently as we did before the

1263
01:25:54,480 --> 01:26:01,240
EM algorithm iterates and iterate into and sometimes and from one point it converges

1264
01:26:01,250 --> 01:26:07,540
two two point detectors usually by this kind of this form you put threshold epsilon

1265
01:26:08,180 --> 01:26:12,120
if the target function doesn't change much then you stop

1266
01:26:12,130 --> 01:26:16,220
and because the end points reached by this iteration

1267
01:26:16,230 --> 01:26:23,370
x star t and we also recorded the size of the last step

1268
01:26:23,390 --> 01:26:26,140
if you converge to the to the fixed point here

1269
01:26:26,150 --> 01:26:29,650
steps to become closer to become smaller

1270
01:26:29,680 --> 01:26:34,610
and so you have a kind of sense of scale what you're dealing at sons

1271
01:26:34,630 --> 01:26:39,950
assumption we are taking to identify local maxima is that true local maxima is in

1272
01:26:39,960 --> 01:26:43,280
a ball around x star

1273
01:26:43,340 --> 01:26:46,370
and the ball has radius of st

1274
01:26:48,180 --> 01:26:52,950
we associate to point to be in the same cluster there

1275
01:26:52,960 --> 01:26:54,590
and point

1276
01:26:54,610 --> 01:26:57,640
if the distance between the endpoints smaller than the sun

1277
01:26:57,710 --> 01:27:03,830
between t and t prime and then we say the towards overlaps in the same

1278
01:27:03,840 --> 01:27:05,610
belong to the same maximum this

1279
01:27:05,650 --> 01:27:12,110
may create in some cases non unique assignment in this case which we rarely happens

1280
01:27:12,460 --> 01:27:13,820
you just to write a few

1281
01:27:14,930 --> 01:27:18,140
steps for them and then you're

1282
01:27:18,200 --> 01:27:22,390
x star converges to a bit closer to the local maximum and and then you

1283
01:27:22,390 --> 01:27:26,030
can resolve these non unique assignments

1284
01:27:26,040 --> 01:27:29,830
so far for the growth

1285
01:27:29,870 --> 01:27:34,020
the organisms still needs

1286
01:27:34,040 --> 01:27:38,370
in each iteration a sum over all data points which is quite expensive

1287
01:27:38,380 --> 01:27:44,610
so we look for some more general methods to speed up and one thing is

1288
01:27:45,020 --> 01:27:49,320
to do this piracy and this is the paper by neal and hinton

1289
01:27:49,330 --> 01:27:50,560
ninety nine

1290
01:27:50,570 --> 01:27:57,020
quite general paper and basically they say update only p percent of the point is

1291
01:27:57,020 --> 01:28:03,100
the largest posterior basically those point which has the largest kernel values

1292
01:28:03,110 --> 01:28:05,460
which produce large kind of a

1293
01:28:05,470 --> 01:28:07,400
with in one iteration

1294
01:28:07,410 --> 01:28:12,710
and this saves the rest of canada competition of the after the first iteration you

1295
01:28:12,830 --> 01:28:18,070
you need to do one iteration two to sort by the priors

1296
01:28:18,180 --> 01:28:21,500
but by the posterior and then you can

1297
01:28:21,530 --> 01:28:26,030
safe it and we did it in the following that we just identified a few

1298
01:28:26,030 --> 01:28:32,080
points which really contribute to the local maxima and then the

1299
01:28:32,090 --> 01:28:38,020
basically wanted the point is not we did not have the point this slide over

1300
01:28:38,060 --> 01:28:42,220
the deviation from the origin of approach but the intent and they also also update

1301
01:28:42,220 --> 01:28:47,390
this set but since they converge so fast it wasn't really necessary

1302
01:28:47,450 --> 01:28:49,700
to update the points in the set l

1303
01:28:49,720 --> 01:28:52,450
which is not so much contribute

1304
01:28:52,470 --> 01:28:58,110
and the other approach is much more cross we just we use dataset first step

1305
01:28:58,120 --> 01:29:01,140
very simplest two by random sampling

1306
01:29:01,150 --> 01:29:05,560
if you have an point to just simple p percent of as representative points or

1307
01:29:05,560 --> 01:29:12,120
you run k means on these points and take the centroids as the representative points

1308
01:29:12,760 --> 01:29:18,700
takes account of was also representative points so experiments

1309
01:29:18,710 --> 01:29:25,820
basically confirmed our search assumption that we need much less iterations number of iterations is

1310
01:29:25,820 --> 01:29:29,250
shown here four point if you have this

1311
01:29:29,290 --> 01:29:35,110
the prior previous approach of england one point this and here you have probably put

1312
01:29:35,110 --> 01:29:40,120
different base for epsilon we stop the EM iteration he put zero there

1313
01:29:40,230 --> 01:29:44,940
well it's not useful but you see it and how much more work but if

1314
01:29:44,940 --> 01:29:48,390
you put a reasonable value like this

1315
01:29:48,450 --> 01:29:50,230
you do much less iterations

1316
01:29:50,270 --> 01:29:52,970
and just to identify the correct clustering

1317
01:29:52,990 --> 01:29:56,940
this was artificial data here show you read it in a moment

1318
01:29:56,950 --> 01:30:01,490
to compare this to acceleration methods

1319
01:30:01,500 --> 01:30:04,780
ten in terms of speed this

1320
01:30:04,810 --> 01:30:09,310
sparsity and a bit more expensive since you have to do one full iteration then

1321
01:30:09,310 --> 01:30:17,320
you can reduce the problem reduces costs and here sampling approaches doesn't

1322
01:30:17,330 --> 01:30:20,020
what was the big differences in speech

1323
01:30:21,480 --> 01:30:27,830
suspect equality in this still artificial data measured graph clustering quality the suspect anomalous mutual

1324
01:30:30,340 --> 01:30:32,510
the few on the x axis

1325
01:30:32,520 --> 01:30:38,710
sample size so here we take full sample was not shown here eighty percent of

1326
01:30:38,710 --> 01:30:42,150
the data get twenty percent of the data and you see here for low dimensional

1327
01:30:43,110 --> 01:30:47,010
you need a identifier clustering with small error

1328
01:30:47,020 --> 01:30:50,890
but but you only twenty percent of your point

1329
01:30:51,000 --> 01:30:54,440
this is low dimensional data technique for sixteen

1330
01:30:54,450 --> 01:30:56,700
and if you make the higher

1331
01:30:56,730 --> 01:30:59,120
i give you one hundred twenty eight

1332
01:30:59,530 --> 01:31:06,900
this album is not so nicely shown more and you also need larger small things

1333
01:31:06,960 --> 01:31:14,030
for real data we compared it to k means and there is not a direct

1334
01:31:14,030 --> 01:31:16,610
methods which is here

1335
01:31:16,620 --> 01:31:18,970
provided this is a bit like self

1336
01:31:18,980 --> 01:31:22,020
step by step for suggesting

1337
01:31:22,150 --> 01:31:23,540
this has

1338
01:31:23,570 --> 01:31:29,610
similar based k means a bit better in terms of as which information

1339
01:31:29,660 --> 01:31:31,950
here are shown here

1340
01:31:32,160 --> 01:31:38,620
the size of the sample size and here's the standard deviation is i was

1341
01:31:38,630 --> 01:31:40,480
standard deviation

1342
01:31:40,490 --> 01:31:44,230
and you see here the sampling cost something

1343
01:31:44,330 --> 01:31:46,990
but not too much on real data

1344
01:31:47,000 --> 01:31:51,660
he also can compare but basically the cells that

1345
01:31:51,660 --> 01:31:51,970
that's all

1346
01:31:53,050 --> 01:31:54,100
and that's a law about

1347
01:31:54,530 --> 01:31:55,840
this bound becomes tight

1348
01:31:56,550 --> 01:31:58,540
becomes equality if and only if

1349
01:31:59,160 --> 01:32:02,150
your q distribution here matches peaked distribution

1350
01:32:02,550 --> 01:32:06,350
so if you're approximation is really really good that matches your true

1351
01:32:07,930 --> 01:32:09,090
then the bound becomes tight

1352
01:32:10,280 --> 01:32:13,560
right so now why do we have this bond well it turns out

1353
01:32:14,080 --> 01:32:15,380
that you can write it this way

1354
01:32:16,490 --> 01:32:18,560
and this term the log probabilities

1355
01:32:19,010 --> 01:32:20,100
is just linear in

1356
01:32:20,550 --> 01:32:23,370
this hidden variables and this is known as official about

1357
01:32:25,260 --> 01:32:26,300
so why is this

1358
01:32:26,890 --> 01:32:27,820
an interesting thing

1359
01:32:28,680 --> 01:32:32,530
the reason why this is interesting because if you look at these guys here this

1360
01:32:32,530 --> 01:32:36,950
is the normalizing constant is the partition function that we can compute that's where all

1361
01:32:37,690 --> 01:32:39,370
exponential computation goes into

1362
01:32:40,070 --> 01:32:41,360
everything else we can compute

1363
01:32:42,060 --> 01:32:45,080
and the beauty of this algorithm is that now i'm gonna try to say well

1364
01:32:45,100 --> 01:32:51,230
finding the distribution q such that this bound is despite this possible maximizes about

1365
01:32:51,750 --> 01:32:55,890
so try to me to be less close as possible to the true he but

1366
01:32:55,890 --> 01:32:58,760
not is that terms that depend on q they don't really

1367
01:32:59,180 --> 01:33:01,600
interact with this intractable partition function

1368
01:33:02,400 --> 01:33:06,110
so that's the beauty of the algorithm that basically says we don't really need to

1369
01:33:06,110 --> 01:33:10,730
find what the normalizing constant and what this partition function is we can actually solve

1370
01:33:10,730 --> 01:33:11,590
this problem without

1371
01:33:12,570 --> 01:33:13,500
using exponential

1372
01:33:16,070 --> 01:33:18,980
then we can view this thing is that you can actually write it as a

1373
01:33:18,980 --> 01:33:22,240
low probability minus something that's called tail divergence

1374
01:33:22,760 --> 01:33:25,980
cool but lieber divergence divergence between two distribution and he

1375
01:33:26,660 --> 01:33:29,860
and the tail that is essential measures the distance between two and

1376
01:33:30,530 --> 01:33:34,840
so it's kind of like saying by maximizing the bound you've basically trying to find

1377
01:33:34,850 --> 01:33:38,170
two such that it's as close as possible to the true

1378
01:33:40,430 --> 01:33:41,700
true posterior distribution

1379
01:33:42,100 --> 01:33:43,900
in terms of detail divergence sense

1380
01:33:46,100 --> 01:33:49,480
so now we have the initial about what we can do is we can choose

1381
01:33:49,690 --> 01:33:53,470
a fully factorized distribution that's the easiest thing to do is called mean field you're

1382
01:33:53,470 --> 01:33:56,100
basically saying all the links in my model are gonna be broken

1383
01:33:57,630 --> 01:33:59,960
and what the variational inference in this case is trying to do

1384
01:34:00,760 --> 01:34:05,180
is essentially trying to find a single mode in the posterior distribution

1385
01:34:06,090 --> 01:34:08,080
okay so if you have you know a few modes

1386
01:34:08,560 --> 01:34:12,360
additional inference will just find one mode and say that's my explanation of the data

1387
01:34:14,210 --> 01:34:18,090
and end it comes down to a set of nonlinear equations which are very easy to

1388
01:34:18,650 --> 01:34:21,200
so four you just iterating through these non linear set of

1389
01:34:21,860 --> 01:34:23,630
the question of deletion a fixed point

1390
01:34:24,030 --> 01:34:25,820
and that's gonna be your best to distribution

1391
01:34:26,890 --> 01:34:27,910
that that that you get

1392
01:34:28,670 --> 01:34:29,400
again it's

1393
01:34:29,890 --> 01:34:32,810
you know it's it's a little bit complex if you've never heard about variational inference

1394
01:34:32,810 --> 01:34:35,640
but it's sort of the idea that you have in mind is that you try

1395
01:34:35,660 --> 01:34:40,180
to approximate your complex distribution with a simpler distribution and there is subjective that you're

1396
01:34:40,180 --> 01:34:42,430
trying to make it as close as possible to the true

1397
01:34:43,360 --> 01:34:44,500
conditional distribution here

1398
01:34:45,650 --> 01:34:50,000
so now the way you want these models is that's you're doing variational inference

1399
01:34:52,000 --> 01:34:57,990
to estimate the first data and expectations and then you're running in same see stochastic approximation to estimate these

1400
01:34:59,290 --> 01:35:00,820
model's expected sufficient statistics

1401
01:35:01,320 --> 01:35:03,080
you can sort of show you know properly

1402
01:35:03,620 --> 01:35:08,030
show that you have all show convergence guarantees asymptotically stable point official about

1403
01:35:08,580 --> 01:35:12,200
you know basically it says that you know when you talk to you guys they're

1404
01:35:12,200 --> 01:35:14,830
happy with out because it says asymptotically everything is okay

1405
01:35:15,760 --> 01:35:16,820
right so now let's see

1406
01:35:18,050 --> 01:35:19,170
how it works in practice

1407
01:35:19,790 --> 01:35:24,090
now in general inference he is fast because we using mean field approximation so it

1408
01:35:24,130 --> 01:35:25,890
takes fraction of a second to infer

1409
01:35:26,450 --> 01:35:28,400
approximately the states of the latent variables

1410
01:35:28,900 --> 01:35:31,260
and learning she can scale to millions of examples

1411
01:35:33,380 --> 01:35:36,550
this is because we using stochastic approximation which doesn't depend on the data so you

1412
01:35:36,550 --> 01:35:38,870
can really make it make it very scalable

1413
01:35:39,500 --> 01:35:41,580
so one question is does it actually work

1414
01:35:43,370 --> 01:35:46,420
you know i think the article should be working is good but doesn't actually work

1415
01:35:46,420 --> 01:35:49,410
in practice so what i'm gonna do is i'm gonna let you judge whether it

1416
01:35:49,410 --> 01:35:50,390
works or not

1417
01:35:51,360 --> 01:35:53,530
so what i'm gonna show is i'm gonna show you two panels

1418
01:35:54,200 --> 01:35:56,650
on one panel i'm gonna show you the real data

1419
01:35:57,140 --> 01:35:59,360
on another initially simulated data

1420
01:36:00,890 --> 01:36:02,680
and you have to tell me which one is which

1421
01:36:04,490 --> 01:36:08,870
so again one panel shows real data data about shows simulated data

1422
01:36:09,380 --> 01:36:11,430
how many of you this was you know whether this was real

1423
01:36:13,940 --> 01:36:16,390
all right hand but this was real this was simulated

1424
01:36:17,500 --> 01:36:19,250
alright as i have and have perfect

1425
01:36:19,730 --> 01:36:20,420
so that's

1426
01:36:20,860 --> 01:36:22,830
that's good i'm always worried about you know

1427
01:36:23,430 --> 01:36:23,900
if you

1428
01:36:24,860 --> 01:36:27,150
if you get it right when you get right right

1429
01:36:29,060 --> 01:36:29,740
so if you look at

1430
01:36:31,400 --> 01:36:31,950
the data

1431
01:36:33,550 --> 01:36:36,190
this is simulated this is real

1432
01:36:38,620 --> 01:36:42,140
so a couple of things to notice the real data is much more diverse

1433
01:36:43,050 --> 01:36:46,510
right if you look at these handwritten characters across these different alphabets

1434
01:36:47,030 --> 01:36:49,380
you know this is all a bit more structure to the real data and now

1435
01:36:49,420 --> 01:36:53,400
has to do with the inability of the model to explore this exponential space

1436
01:36:53,940 --> 01:36:55,700
of all possible handwritten characters

1437
01:36:56,290 --> 01:36:57,300
if you also look at

1438
01:36:57,820 --> 01:36:58,890
you know something like this

1439
01:36:59,620 --> 01:37:02,390
right you can basically see that people probably wouldn't

1440
01:37:03,430 --> 01:37:06,210
draw this image this way right it's just you know they're like

1441
01:37:06,980 --> 01:37:11,010
pixel hanging out here in one pixel hang out here so if you actually steer

1442
01:37:11,150 --> 01:37:12,690
these images for quite some time

1443
01:37:13,110 --> 01:37:13,760
you'll see

1444
01:37:14,580 --> 01:37:18,740
but this is simulated this is the only method captures a lot of structure handwritten characters right

1445
01:37:19,280 --> 01:37:20,720
sort captures how how

1446
01:37:21,500 --> 01:37:22,700
what's the underlying structure

1447
01:37:29,320 --> 01:37:33,190
now if i look at some other datasets so for example if i look at the e

1448
01:37:33,420 --> 01:37:34,790
data set of handwritten digits

1449
01:37:36,410 --> 01:37:40,150
now there's another this is a commonly used is a simple dataset how many of

1450
01:37:40,150 --> 01:37:41,880
you think this is really simulated

1451
01:37:44,430 --> 01:37:46,920
right a few how many of you think this is real and this simulated

1452
01:37:48,970 --> 01:37:50,370
so that's a much easier test

1453
01:37:51,640 --> 01:37:55,500
this is indeed simulated so if you look at something like five over here right

1454
01:37:55,500 --> 01:37:59,470
the kind of the thickness or if the about average

1455
01:37:59,480 --> 01:38:05,790
solutions will probably survived the higher the fitness about the average to high the probability

1456
01:38:05,790 --> 01:38:11,960
of survival and of course the below average probably die out

1457
01:38:11,970 --> 01:38:16,200
OK i mentioned that this is done

1458
01:38:16,680 --> 01:38:20,710
that one way of implementing it is

1459
01:38:20,720 --> 01:38:23,300
the so-called roulette wheel

1460
01:38:23,310 --> 01:38:25,730
you can imagine the rule

1461
01:38:25,750 --> 01:38:26,820
and dr

1462
01:38:26,820 --> 01:38:29,940
the idea is

1463
01:38:29,970 --> 01:38:31,800
the following

1464
01:38:33,190 --> 01:38:37,790
this is the rule that then

1465
01:38:37,810 --> 01:38:42,180
the sections

1466
01:38:42,190 --> 01:38:44,550
correspond to the individuals

1467
01:38:44,570 --> 01:38:46,120
in such a way that

1468
01:38:46,130 --> 01:38:47,580
the larger this

1469
01:38:48,930 --> 01:38:54,790
i mean the higher the fitness the larger sections section belongs to one individual so

1470
01:38:54,790 --> 01:38:55,700
if you now

1471
01:38:55,720 --> 01:38:58,530
do this rule selections

1472
01:38:58,560 --> 01:39:02,120
if you turn this real and of course if you select

1473
01:39:02,150 --> 01:39:07,160
one thing that there is a high probability of selecting the larger one

1474
01:39:07,170 --> 01:39:11,530
so after doing this selection procedure and times

1475
01:39:11,550 --> 01:39:19,200
n is the number of these sections or individuals you get what

1476
01:39:19,210 --> 01:39:22,800
a certain way and copies

1477
01:39:22,810 --> 01:39:24,690
out of this population

1478
01:39:24,700 --> 01:39:26,650
where the most

1479
01:39:27,330 --> 01:39:30,590
it is the individual will probably be represented

1480
01:39:30,610 --> 01:39:35,270
a higher number high number of times then maybe the second one and so so

1481
01:39:35,270 --> 01:39:40,200
on but this is very bad individuals will probably never come

1482
01:39:40,210 --> 01:39:47,040
out of this selection procedure so this is one traditional approach of implementing this selection

1483
01:39:47,200 --> 01:39:50,060
that is called fitness proportionate

1484
01:39:50,080 --> 01:39:57,600
there are also alternative selection schemes now there's more popular like rank based when we

1485
01:39:57,620 --> 01:39:58,750
do not take

1486
01:39:58,770 --> 01:40:07,090
fitness values into core into considerations but simply their ranks to serve as witness then

1487
01:40:07,090 --> 01:40:07,930
there is

1488
01:40:07,940 --> 01:40:10,530
l selections when a certain number of

1489
01:40:10,540 --> 01:40:11,730
the best

1490
01:40:11,740 --> 01:40:12,920
solutions to go

1491
01:40:12,970 --> 01:40:17,380
into the next populations tournament selection when a certain subset

1492
01:40:17,440 --> 01:40:20,790
of solutions compete against each other and so on

1493
01:40:20,810 --> 01:40:27,280
these models are known to have certain properties which so some of these are more

1494
01:40:27,280 --> 01:40:33,820
convenient and there's about this is a very specific so

1495
01:40:33,820 --> 01:40:40,950
maybe we can mention some extensions of these algorithms and then show some additional examples

1496
01:40:40,950 --> 01:40:44,790
to know what is actually

1497
01:40:46,050 --> 01:40:48,320
what it can produce in practice

1498
01:40:48,330 --> 01:40:51,270
we said that

1499
01:40:51,280 --> 01:40:52,860
bit strings

1500
01:40:52,900 --> 01:41:01,090
are the most traditional way of encoding the solutions genetic algorithms later this is this

1501
01:41:02,320 --> 01:41:08,100
improved in the way that we are not that strict anymore it depends on the

1502
01:41:08,100 --> 01:41:11,100
problem is we are solving let's say

1503
01:41:11,120 --> 01:41:14,030
i don't know travelling salesman problem then

1504
01:41:14,040 --> 01:41:16,150
presenting solutions as

1505
01:41:16,190 --> 01:41:21,400
permutations will be the most natural way so we can do this with the genetic

1506
01:41:21,400 --> 01:41:24,670
algorithms also just needs to

1507
01:41:24,680 --> 01:41:28,420
some tailoring of these operators

1508
01:41:28,440 --> 01:41:34,470
then it can be also real vectors as we the evolution strategies arrays

1509
01:41:34,490 --> 01:41:36,350
it depends on the problem

1510
01:41:37,530 --> 01:41:39,860
the crossover operator is

1511
01:41:39,900 --> 01:41:45,710
also something that is implemented in twenty different ways of what we saw

1512
01:41:45,730 --> 01:41:52,070
with the simple crossover region is the most straightforward implementation

1513
01:41:53,130 --> 01:42:00,080
o variety of other implementations like multiple point cross-over which is cutting the strings at

1514
01:42:00,080 --> 01:42:04,040
several points and then exchange every second

1515
01:42:04,050 --> 01:42:05,320
a pair of

1516
01:42:07,540 --> 01:42:11,220
then it is uniform cross-over when

1517
01:42:11,270 --> 01:42:17,020
for each pair of components we decide separately by the two exchange them or not

1518
01:42:17,040 --> 01:42:25,870
then there is arithmetic crossover for real vectors when new components are calculated based on

1519
01:42:25,870 --> 01:42:28,610
the original ones and so on

1520
01:42:28,660 --> 01:42:32,630
four permutation problems we have specific operators and so on

1521
01:42:32,640 --> 01:42:34,810
as we did

1522
01:42:34,820 --> 01:42:37,580
evolution strategies we also how

1523
01:42:37,600 --> 01:42:41,320
advanced techniques genetic algorithms

1524
01:42:41,340 --> 01:42:46,680
like meta GA's which is a tuning

1525
01:42:48,170 --> 01:42:52,790
one of the problem or of the data on the high level and solving the

1526
01:42:52,790 --> 01:42:56,310
problem itself on on

1527
01:42:56,310 --> 01:42:57,340
lower level

1528
01:42:57,370 --> 01:43:00,510
then these algorithms can be paralyzed

1529
01:43:00,560 --> 01:43:07,590
that is implemented on parallel computers because of population based search

1530
01:43:07,610 --> 01:43:09,310
this is particularly

1531
01:43:09,310 --> 01:43:15,230
this is calculated by these microscopic approach the distance between the fragments so

1532
01:43:15,230 --> 01:43:20,460
and you can have this line here which are the prediction that we can make

1533
01:43:20,480 --> 01:43:23,700
if you compare with experimental data here

1534
01:43:23,720 --> 01:43:24,550
you see

1535
01:43:24,570 --> 01:43:29,460
but you have a global agreement with experimental data

1536
01:43:31,340 --> 01:43:36,300
these people and p here can be explained by the formation of the fragments so

1537
01:43:36,300 --> 01:43:40,010
with the hand when you have two different fragments

1538
01:43:40,020 --> 01:43:44,030
the distance between the centre of mass of these fragments is quite large

1539
01:43:44,050 --> 01:43:46,250
so then if the distance is lot

1540
01:43:46,250 --> 01:43:47,780
total kinetic energy

1541
01:43:47,800 --> 01:43:49,590
is small

1542
01:43:49,610 --> 01:43:53,600
so you you have the spherical here's the distances less

1543
01:43:53,620 --> 01:43:58,060
so it's a big east and say sorry

1544
01:43:58,080 --> 01:44:02,190
it's more dispensary small distance and then total kinetic energy

1545
01:44:02,830 --> 01:44:04,650
big and so on

1546
01:44:04,670 --> 01:44:08,060
the information can explain this by down here

1547
01:44:08,080 --> 01:44:13,420
so we have an overestimation to six percent so

1548
01:44:13,420 --> 01:44:16,840
as we don't have introduced a new phenomenological parameters

1549
01:44:16,840 --> 01:44:20,090
this is quite a good news

1550
01:44:20,130 --> 01:44:24,770
so we can calculate the quadrupole deformation of the fission fragments that solution

1551
01:44:24,790 --> 01:44:30,340
so this is what we have done for different fissioning system from diamond family

1552
01:44:30,360 --> 01:44:35,190
this is this information depending on the mass of the fraction of the fragments

1553
01:44:35,210 --> 01:44:40,420
and you see this sort of structure which is really do to be shell effects

1554
01:44:40,420 --> 01:44:43,050
for eighty two fifty so

1555
01:44:43,090 --> 01:44:46,940
we have these match minima for a eighty two

1556
01:44:46,960 --> 01:44:51,770
and one hundred thirty beats maximum here

1557
01:44:51,790 --> 01:44:57,320
what you can see also that this fragment deformation do not strongly depend upon the

1558
01:44:57,320 --> 01:45:02,380
fissioning system that you are looking at

1559
01:45:02,440 --> 01:45:07,270
when you have this information you can easily calculate the deformation energy

1560
01:45:07,320 --> 01:45:12,500
and then you can constitute can calculate the emission of neutrons so first when you

1561
01:45:12,500 --> 01:45:15,940
have excited fragmented we first meet some neutrons

1562
01:45:15,960 --> 01:45:20,690
so when you have the deformation energy you can calculate the emission of neutrons because

1563
01:45:20,690 --> 01:45:24,790
you just have to divide by the energy of the neutrons what is the energy

1564
01:45:24,790 --> 01:45:26,480
of the emitted neutrons

1565
01:45:26,500 --> 01:45:29,570
if the kinetic energy of the neutrons blows

1566
01:45:29,590 --> 01:45:33,800
the binding energy of the neutrons when he was in the

1567
01:45:33,820 --> 01:45:36,340
so that in this formula

1568
01:45:36,360 --> 01:45:42,880
number of neutrons emitted in the exhibition energy divided by the energy of the neutrons

1569
01:45:42,900 --> 01:45:47,670
which is the sum of the kinetic energy of the neutrons the separation energy of

1570
01:45:47,670 --> 01:45:50,520
the neutrons in the nucleus

1571
01:45:50,530 --> 01:45:55,320
so all it has been calculated by the previous calculations and then you see that

1572
01:45:56,250 --> 01:46:01,130
can compare with experimental data this is the neutron multiplicity depending on the mass of

1573
01:46:01,130 --> 01:46:02,460
the fragments

1574
01:46:02,480 --> 01:46:10,230
so open symbols are for experiment and this one black symbols from theory

1575
01:46:10,250 --> 01:46:12,710
so you see that the

1576
01:46:12,730 --> 01:46:16,320
we had good qualitative agreement so we have

1577
01:46:16,340 --> 01:46:20,980
i found these structure here the so structure

1578
01:46:21,020 --> 01:46:23,630
you see that there are some discrepancies

1579
01:46:23,650 --> 01:46:27,570
some can be explained by experiments

1580
01:46:27,590 --> 01:46:32,090
which are very difficult to obtain and some can be explained by different assumptions that

1581
01:46:32,090 --> 01:46:35,420
we have made one of the assumption that here

1582
01:46:35,440 --> 01:46:36,920
we had this

1583
01:46:36,940 --> 01:46:41,000
after approximation where we consider here

1584
01:46:41,020 --> 01:46:41,880
at that

1585
01:46:42,300 --> 01:46:45,650
energy of the fragment is only due to the information

1586
01:46:45,670 --> 01:46:49,900
but you can have all that kind of exciting action due to promotion of some

1587
01:46:49,920 --> 01:46:53,920
intrinsic citation indices neglected here

1588
01:46:53,920 --> 01:46:54,860
so that

1589
01:46:54,880 --> 01:46:59,150
is the way progress can be made now to better

1590
01:46:59,170 --> 01:47:05,360
i understand the process better describe their experimental results

1591
01:47:05,420 --> 01:47:10,130
so now the time dependent propagation so how do we do so with the hand

1592
01:47:10,150 --> 01:47:14,840
what we do in that need with an initial state here which represent the company

1593
01:47:14,840 --> 01:47:20,050
placed a new trying new stations of initial state here and then we have this

1594
01:47:20,110 --> 01:47:24,710
schrodinger equations time dependent so we propagate everywhere

1595
01:47:24,730 --> 01:47:29,250
it's quantum mechanical but you can imagine that what what somewhere and then you are

1596
01:47:29,250 --> 01:47:31,380
looking whereas the winters training

1597
01:47:31,420 --> 01:47:36,800
and then you can that flex along all station configurations and then you can have

1598
01:47:38,170 --> 01:47:40,800
time dependent territories so

1599
01:47:40,820 --> 01:47:43,070
even if it's the same effect with water

1600
01:47:43,090 --> 01:47:45,670
that and then it where

1601
01:47:45,800 --> 01:47:50,400
comes here and then calculate the different probabilities to have

1602
01:47:50,420 --> 01:47:51,520
what are here

1603
01:47:51,550 --> 01:47:55,270
different depending on the configuration

1604
01:47:55,300 --> 01:48:01,030
so and then we can calculate different properties like the mass distribution for example

1605
01:48:01,050 --> 01:48:07,820
and these are the results of value evaluation can consider experiments and are you experienced

1606
01:48:07,820 --> 01:48:09,170
and here

1607
01:48:09,190 --> 01:48:14,210
so this is a unit of fragments depending on one that

1608
01:48:14,210 --> 01:48:19,980
you have a static results so you can calculate from no dynamics no time evolution

1609
01:48:20,000 --> 01:48:21,020
you can

1610
01:48:21,210 --> 01:48:26,130
calculate is what we call one-dimensional results in right

1611
01:48:26,150 --> 01:48:28,380
and the new results out the blue

1612
01:48:28,400 --> 01:48:30,130
here here

1613
01:48:30,150 --> 01:48:34,150
so you see different points of first you have the same location of the maxima

1614
01:48:34,170 --> 01:48:36,190
so it means that in these

1615
01:48:36,210 --> 01:48:39,650
nucleus which is fissionable uranium two hundred thirty eight

1616
01:48:39,690 --> 01:48:41,570
this does not depend

1617
01:48:41,570 --> 01:48:45,480
too much on the national that really depend on the energy

1618
01:48:45,520 --> 01:48:48,340
but you see that you really improve here

1619
01:48:48,340 --> 01:48:50,610
we want to be able to

1620
01:48:50,630 --> 01:48:56,440
to build a company gener generalizations like that every metal has solvent

1621
01:48:56,450 --> 01:48:58,920
for every fish there's a bigger fish

1622
01:49:08,300 --> 01:49:11,420
every student is brighter than someone but we

1623
01:49:11,420 --> 01:49:15,140
we need to be able to say things like

1624
01:49:16,930 --> 01:49:18,270
and the technique that's

1625
01:49:18,310 --> 01:49:20,510
standard for

1626
01:49:22,330 --> 01:49:23,690
as you

1627
01:49:23,750 --> 01:49:26,140
i will be away

1628
01:49:26,190 --> 01:49:28,060
is to introduce

1629
01:49:34,960 --> 01:49:41,690
in the syntax variables look like names

1630
01:49:41,690 --> 01:49:44,330
so they begin with x something which means to

1631
01:49:47,700 --> 01:49:51,520
so they they they behave just just as though they were naming things

1632
01:49:51,950 --> 01:49:55,950
but they don't name any particular thing that's the intended semantics

1633
01:49:58,660 --> 01:50:00,650
variability over the domain

1634
01:50:00,660 --> 01:50:01,790
we consider

1635
01:50:01,810 --> 01:50:03,770
the sets of

1636
01:50:03,780 --> 01:50:06,190
when we when we look at the variable

1637
01:50:06,200 --> 01:50:09,530
and think about what it takes out well we consider

1638
01:50:09,540 --> 01:50:14,390
the set evaluations of that we get by letting it began different things like range

1639
01:50:14,390 --> 01:50:17,350
over variable well at the very

1640
01:50:17,350 --> 01:50:19,910
over the domain

1641
01:50:19,960 --> 01:50:25,450
variables in themselves are not that useful

1642
01:50:25,470 --> 01:50:29,760
to say things to to generalize they have to be bound

1643
01:50:29,770 --> 01:50:34,390
and they get bound by quantifiers

1644
01:50:40,920 --> 01:50:43,030
are expressions that say

1645
01:50:43,090 --> 01:50:45,510
what quantity of things

1646
01:50:45,560 --> 01:50:47,310
in the domain

1647
01:50:47,360 --> 01:50:51,590
satisfy some description

1648
01:50:51,630 --> 01:50:57,610
again in natural languages there are quite a lot of quantifiers

1649
01:50:57,620 --> 01:51:01,260
most of which don't have any very interesting logic

1650
01:51:03,670 --> 01:51:05,980
we consider expression like

1651
01:51:08,590 --> 01:51:13,980
most days of these

1652
01:51:14,590 --> 01:51:19,040
this is

1653
01:51:19,100 --> 01:51:21,470
quantifier it says

1654
01:51:21,490 --> 01:51:25,240
consider the things in here that satisfy the description i

1655
01:51:25,250 --> 01:51:29,520
most of them satisfy this other description b

1656
01:51:29,590 --> 01:51:33,470
and the way we see that in

1657
01:51:35,090 --> 01:51:38,990
logical notation will be something like for most

1658
01:51:46,700 --> 01:51:48,840
such that

1659
01:51:49,050 --> 01:51:56,160
but text justifies a

1660
01:51:56,200 --> 01:52:00,340
x satisfies b

1661
01:52:00,640 --> 01:52:01,950
and this

1662
01:52:01,980 --> 01:52:04,190
stuff here

1663
01:52:07,670 --> 01:52:12,310
like most quantifiers in natural language this is a binary

1664
01:52:12,360 --> 01:52:15,150
operation it takes two

1665
01:52:15,200 --> 01:52:17,380
predicates here

1666
01:52:17,440 --> 01:52:23,440
the variable standing here

1667
01:52:23,440 --> 01:52:25,450
in the places

1668
01:52:26,240 --> 01:52:30,080
the names of these objects are going to go on

1669
01:52:31,130 --> 01:52:35,790
and this kind of notation is capable of representing

1670
01:52:43,060 --> 01:52:44,430
first order logic

1671
01:52:44,430 --> 01:52:47,080
again we not

1672
01:52:47,090 --> 01:52:49,060
concerned to formalize

1673
01:52:49,120 --> 01:52:51,840
all of these we restrict ourselves to

1674
01:52:51,890 --> 01:52:54,960
to just a couple of the

1675
01:52:55,020 --> 01:52:59,900
namely a universal

1676
01:53:00,980 --> 01:53:04,160
but says everything in the domain satisfies

1677
01:53:07,450 --> 01:53:13,690
for all

1678
01:53:13,980 --> 01:53:16,210
and for that we

1679
01:53:16,260 --> 01:53:17,620
i'm going to use

1680
01:53:17,660 --> 01:53:19,830
this inverted a

1681
01:53:19,880 --> 01:53:23,690
with the very

1682
01:53:23,710 --> 01:53:24,430
and then

1683
01:53:24,430 --> 01:53:26,720
this formula here with

1684
01:53:26,740 --> 01:53:31,650
in the interesting case the variable occurs in it

1685
01:53:31,690 --> 01:53:36,330
and the existential quantifier

1686
01:53:41,210 --> 01:53:44,340
he says that something in the domain at least one thing

1687
01:53:44,390 --> 01:53:46,970
satisfies the condition

1688
01:53:46,990 --> 01:53:50,190
and actually right to reverse e

1689
01:53:50,580 --> 01:53:52,200
in the standard way

1690
01:53:52,230 --> 01:53:55,160
to represent that

1691
01:53:55,200 --> 01:53:58,860
so as is familiar

1692
01:53:58,880 --> 01:54:00,300
the logic was

1693
01:54:00,360 --> 01:54:05,230
with this vocabulary we can say things like all

1694
01:54:07,030 --> 01:54:13,510
by peds

1695
01:54:13,530 --> 01:54:15,950
otherwise they look ridiculous

1696
01:54:15,970 --> 01:54:21,050
take anything in the domain

1697
01:54:21,050 --> 01:54:27,590
this is the question in natural language so we can put any other questions so

1698
01:54:27,590 --> 01:54:30,280
with a simple interface we can form

1699
01:54:31,420 --> 01:54:33,010
this question in tool

1700
01:54:33,030 --> 01:54:38,840
query before the knowledge base which is already first order

1701
01:54:39,450 --> 01:54:41,800
kerry in this

1702
01:54:41,800 --> 01:54:45,110
much with the like this give the values of

1703
01:54:45,160 --> 01:54:46,490
the o station and

1704
01:54:46,490 --> 01:54:51,300
five such that the following is true so is a couple of

1705
01:54:51,610 --> 01:54:55,380
constraints which we provide in in the text

1706
01:54:55,470 --> 01:54:56,510
and the president

1707
01:54:56,530 --> 01:54:57,570
but then

1708
01:55:01,630 --> 01:55:04,340
we would get an answer so that

1709
01:55:04,380 --> 01:55:07,930
function five so this argument would be

1710
01:55:07,970 --> 01:55:10,650
the governor detection of an outlets

1711
01:55:10,660 --> 01:55:12,630
five rockets

1712
01:55:12,630 --> 01:55:16,570
and so on and the station is something west stations so this is now

1713
01:55:16,610 --> 01:55:21,430
taken straight from the knowledge base so there are obviously plenty of

1714
01:55:21,470 --> 01:55:25,300
a set of documents which had this pieces of

1715
01:55:25,340 --> 01:55:30,820
on information inside which were in one of the other way as large showing before

1716
01:55:30,820 --> 01:55:34,990
transferring to the first into knowledge base and now we can ask

1717
01:55:35,010 --> 01:55:37,860
about these facts which appeared in the knowledge base

1718
01:55:37,900 --> 01:55:40,050
so what's so

1719
01:55:40,070 --> 01:55:42,220
we came from the natural language

1720
01:55:42,220 --> 01:55:43,740
the first or the query

1721
01:55:43,760 --> 01:55:47,450
so the answer now we can also say well justified

1722
01:55:47,510 --> 01:55:50,900
justify this answer

1723
01:55:50,930 --> 01:55:55,110
and now this would be a justification

1724
01:55:55,110 --> 01:55:58,070
so up here

1725
01:55:58,090 --> 01:56:00,400
we have very which we

1726
01:56:00,470 --> 01:56:02,610
three with the beginning

1727
01:56:02,630 --> 01:56:04,880
the question here is of there

1728
01:56:04,930 --> 01:56:08,510
you see there are two two variables station in funk type so the answer was

1729
01:56:08,510 --> 01:56:11,820
this station something was stationed functi type is

1730
01:56:11,900 --> 01:56:14,510
whatever of it

1731
01:56:14,530 --> 01:56:19,090
and now this is justification for the answer it's a long one but this is

1732
01:56:19,090 --> 01:56:20,840
really the trace of cyc

1733
01:56:21,200 --> 01:56:27,320
it's like doing reasoning can connecting small pieces and very specific knowledge

1734
01:56:27,340 --> 01:56:32,800
and very generic knowledge now if let's see you try to come focus to be

1735
01:56:32,820 --> 01:56:34,720
from this justification so

1736
01:56:34,780 --> 01:56:36,240
this would be

1737
01:56:36,260 --> 01:56:39,300
this rule here is very specific

1738
01:56:42,180 --> 01:56:45,420
knowledge about these rockets

1739
01:56:45,420 --> 01:56:49,180
which again won gold much into the details but it if

1740
01:56:49,400 --> 01:56:55,260
facility cessation of that and that type and some type of situation and so on

1741
01:56:56,610 --> 01:57:01,070
we can conclude this and this so this would be very domain specific knowledge was

1742
01:57:01,200 --> 01:57:02,510
down here

1743
01:57:02,550 --> 01:57:08,940
using this reasoning car whether specific domain knowledge would be triggered as well as a

1744
01:57:08,940 --> 01:57:10,660
very generic knowledge so

1745
01:57:10,760 --> 01:57:15,130
it's a it's simple common sense rule in this particular case would be divide devices

1746
01:57:15,130 --> 01:57:19,700
cannot perform their primary function when they are broken so this is part of the

1747
01:57:20,700 --> 01:57:22,650
we a justification

1748
01:57:22,660 --> 01:57:26,970
for that particular relatively complex complex

1749
01:57:29,050 --> 01:57:33,070
this interface allows really cool for typically four

1750
01:57:33,130 --> 01:57:40,240
expert user or analyst to ask relatively complicated queries and then get this kind of

1751
01:57:40,530 --> 01:57:44,070
justification and especially in this kind of very

1752
01:57:44,090 --> 01:57:47,610
montreal domains so this kind of justification so very

1753
01:57:47,630 --> 01:57:50,220
helpful because

1754
01:57:50,240 --> 01:57:55,050
they are far from complete especially if the person is not very knowledgeable in the

1755
01:57:56,030 --> 01:58:00,900
so the second example which i have is an example of abductive reasoning which doesn't

1756
01:58:00,900 --> 01:58:08,930
work in general but works well in this particular case this is

1757
01:58:11,450 --> 01:58:15,470
i hope you know how much can you see this question so the question is

1758
01:58:15,510 --> 01:58:19,240
who has a motive for this is the nation of life and i think it

1759
01:58:19,240 --> 01:58:23,320
is a couple of years ago there was because the prime minister of lebanon which

1760
01:58:23,320 --> 01:58:26,340
got killed in some

1761
01:58:29,570 --> 01:58:33,800
now the question we're still i think but today is not clear who

1762
01:58:33,900 --> 01:58:38,570
who was the perpetrator of this event now

1763
01:58:38,610 --> 01:58:40,590
we can ask

1764
01:58:40,610 --> 01:58:42,550
so what's eichner about

1765
01:58:42,550 --> 01:58:45,380
terrorism so it knows a lot of facts

1766
01:58:45,400 --> 01:58:47,800
collected from if public sources

1767
01:58:48,510 --> 01:58:51,930
so i think this was one of the projects from sites so some other people

1768
01:58:51,930 --> 01:58:59,680
were day-by-day collecting pieces of information from public sources about the terrorist activities

1769
01:58:59,720 --> 01:59:02,130
now we can ask so

1770
01:59:02,160 --> 01:59:06,050
what who has a motive for this is the notion of but i think

1771
01:59:06,090 --> 01:59:09,610
so this question gets transferred of their into

1772
01:59:09,700 --> 01:59:14,490
the first order form so we would like to instantiate this variable who

1773
01:59:14,510 --> 01:59:17,550
so who has a motive for this is the national accrediting

1774
01:59:17,590 --> 01:59:23,400
and then the press this button for getting an answer we get the list so

1775
01:59:23,450 --> 01:59:27,650
it's much sort city OK the united states

1776
01:59:28,380 --> 01:59:32,900
so the difference is that we allow cycle in this particular case

1777
01:59:32,900 --> 01:59:37,170
and not only everybody right everybody right by a good amount by l one margin

1778
01:59:37,230 --> 01:59:40,570
a difference between this and the rest of point two

1779
01:59:41,210 --> 01:59:45,550
so that tells us least the thing that posting is searching for example

1780
01:59:45,610 --> 01:59:49,520
we don't tell us how to get the only access we have this matrix by

1781
01:59:49,520 --> 01:59:51,210
the weak learning algorithm

1782
01:59:51,270 --> 01:59:55,730
that you find data but lisa tells us that this weighted vote system there exists

1783
01:59:55,730 --> 02:00:01,750
a good way of a nice linear separation really defined over these hypotheses has a

1784
02:00:03,920 --> 02:00:09,170
l one margin alomar this means that this weighted by the weights

1785
02:00:09,250 --> 02:00:12,880
sum up their sum up to one

1786
02:00:12,920 --> 02:00:15,250
and the gap there's point

1787
02:00:15,500 --> 02:00:21,400
so this means the thing it was searching for we know it exists and of

1788
02:00:21,400 --> 02:00:24,110
course it was great because it gives you a way of getting at if the

1789
02:00:24,110 --> 02:00:27,380
only act if we don't so if you had with this thing is if we

1790
02:00:27,380 --> 02:00:31,460
had explicitly listed always hypothesis we would need to use a

1791
02:00:31,500 --> 02:00:33,290
we can define a linear separation

1792
02:00:33,290 --> 02:00:37,480
but if this is a huge set of hypotheses and the only access have is

1793
02:00:37,480 --> 02:00:41,270
by this algorithm a that goes to find one and it was just way of

1794
02:00:41,270 --> 02:00:43,630
doing that

1795
02:00:44,610 --> 02:00:47,290
this is just like adding an interesting way of thinking about what it was to

1796
02:00:54,750 --> 02:00:55,670
OK so now

1797
02:00:55,670 --> 02:00:59,800
a little bit about what happens if everyone's adapting

1798
02:01:05,020 --> 02:01:06,710
so what if

1799
02:01:12,590 --> 02:01:18,050
so we start by talking about algorithms that

1800
02:01:18,130 --> 02:01:21,630
you know the world is changing traffic is changing every you know an actual traffic

1801
02:01:21,630 --> 02:01:23,250
is likewise trafficking

1802
02:01:23,270 --> 02:01:28,110
because everybody else is changing the way they're acting based on what's going themselves and

1803
02:01:28,110 --> 02:01:32,900
you know so everybody's changing so what happens so the changing cost function we're experiencing

1804
02:01:33,400 --> 02:01:38,380
traffic is due to other players in the system optimizing for themselves using some i

1805
02:01:38,420 --> 02:01:40,320
regret minimising procedure

1806
02:01:40,880 --> 02:01:44,800
if you think about this idea of having lower right it can be viewed as

1807
02:01:44,800 --> 02:01:49,380
a nice definition of what it means to be acting reasonably of interest one right

1808
02:01:49,380 --> 02:01:52,380
so here you are you figure out how to drive to work and if some

1809
02:01:52,380 --> 02:01:54,920
other way to go much better than what you've been taking

1810
02:01:54,980 --> 02:01:59,230
you know you're not doing great job acting in the nicely self-interest away

1811
02:01:59,250 --> 02:02:04,860
but suppose everybody is able to have regret going to zero

1812
02:02:04,880 --> 02:02:07,300
it was to say about the overall system

1813
02:02:07,550 --> 02:02:11,730
so one thing is we saw the zero sum games

1814
02:02:11,730 --> 02:02:15,130
the empirical frequencies so if you have

1815
02:02:15,150 --> 02:02:19,730
zero sum game two players zero sum game and they're both minimizing regret then their

1816
02:02:19,730 --> 02:02:22,570
behaviors are going approach minimax optimal

1817
02:02:22,590 --> 02:02:26,480
OK so there are both minimizing regret and they they play you you look at

1818
02:02:26,480 --> 02:02:30,550
how many times they played to their options those distributions apply how many times they

1819
02:02:30,550 --> 02:02:34,250
played this first that is that those things are going for minimax optimal and you

1820
02:02:34,250 --> 02:02:37,790
can see that because if your empirical distribution of play wasn't

1821
02:02:37,800 --> 02:02:41,460
going towards minimax optimal if it was was far too was

1822
02:02:41,520 --> 02:02:43,030
very non optimal

1823
02:02:46,670 --> 02:02:51,000
minimax optimality guarantees you get released the value of the game is if you're quite

1824
02:02:51,000 --> 02:02:52,550
far from that if you only

1825
02:02:52,570 --> 02:02:57,690
you are praying distribution and only guarantees you so much lower numbers the opponent with

1826
02:02:57,690 --> 02:03:00,840
the will take advantage of that they would have to take advantage of your they're

1827
02:03:00,840 --> 02:03:02,630
running minimizing our

1828
02:03:04,920 --> 02:03:08,000
OK so i'm just arguing here is the two regret minimising i'm going to play

1829
02:03:08,000 --> 02:03:12,460
against each other they have to have the distributions approaching minimax optimality because if you

1830
02:03:12,460 --> 02:03:16,630
want approaching that would mean that there be some choice the other player could make

1831
02:03:16,960 --> 02:03:19,980
that would do much better than the value of the game v

1832
02:03:20,020 --> 02:03:23,270
they would have to do that because are minimizing regret and then you will have

1833
02:03:24,830 --> 02:03:27,790
so really they both have to be approach next

1834
02:03:27,840 --> 02:03:32,000
that's so we can understand two player zero-sum game

1835
02:03:33,800 --> 02:03:37,170
the natural thing to ask is in general sum games you start playing these guys

1836
02:03:37,170 --> 02:03:43,610
against each other will the behavior quickly or even all approach to nash equilibrium

1837
02:03:43,710 --> 02:03:48,130
i mean if you think about a nash equilibrium is exactly a set of distributions

1838
02:03:48,190 --> 02:03:51,170
have no regret with respect to each other

1839
02:03:51,270 --> 02:03:52,900
given the other guys doing this

1840
02:03:52,900 --> 02:03:57,020
there's nothing you read it or find there's no particular strategy that does much better

1841
02:03:57,320 --> 02:04:02,690
given what you're doing is no particular strategy guide your for your both feeling no

1842
02:04:02,730 --> 02:04:04,150
regret happy

1843
02:04:09,150 --> 02:04:14,170
in general sum games will these adaptive algorithms guarantee approach

1844
02:04:14,190 --> 02:04:17,960
unfortunately the answer is no

1845
02:04:22,980 --> 02:04:25,020
just to give an example

1846
02:04:25,130 --> 02:04:27,500
here's an example of the EM where they don't

1847
02:04:27,590 --> 02:04:28,730
it's one that

1848
02:04:28,750 --> 02:04:34,150
now they officially called this but i like to call rock paper scissors was born

1849
02:04:35,360 --> 02:04:37,590
there in the first part

1850
02:04:38,460 --> 02:04:43,440
so the first three rows and columns this game are what's called the shapley game

1851
02:04:43,550 --> 02:04:47,590
which is rock paper scissors ceremonies rock paper scissors beats

1852
02:04:47,690 --> 02:04:50,900
paper beats rock scissors paper

1853
02:04:51,130 --> 02:04:52,300
rock beats

1854
02:04:53,690 --> 02:04:56,340
what happens if you both play the same thing

1855
02:04:56,400 --> 02:04:58,070
and i mean high right

1856
02:04:58,130 --> 02:05:01,000
but in the shapley game you both

1857
02:05:01,050 --> 02:05:02,340
OK so

1858
02:05:05,290 --> 02:05:07,980
if the row player plays plays rock

1859
02:05:08,070 --> 02:05:09,750
tumblr page paper

1860
02:05:09,800 --> 02:05:12,610
OK then

1861
02:05:14,650 --> 02:05:17,190
paper wins so the row player plays

1862
02:05:17,250 --> 02:05:18,690
pays the fact that

1863
02:05:18,710 --> 02:05:20,290
the column player dollar

1864
02:05:20,340 --> 02:05:23,320
the other way around the money goes the other way if they both the same

1865
02:05:23,320 --> 02:05:27,520
thing about me and all of they both

1866
02:05:27,530 --> 02:05:30,960
so that the shapley game the sadly game is losing proposition

1867
02:05:31,020 --> 02:05:36,360
OK in the shapley game here

1868
02:05:36,440 --> 02:05:39,610
i have left the future slides

1869
02:05:39,710 --> 02:05:46,820
anyway so the shapley game got these three

1870
02:05:46,880 --> 02:05:52,170
places here many men the best that equilibrium is still one thirty one one just

1871
02:05:52,170 --> 02:05:54,420
like rock paper scissors

1872
02:05:54,480 --> 02:05:57,550
that's the best you can really do

1873
02:05:58,380 --> 02:05:59,980
but when you lose

1874
02:06:02,630 --> 02:06:08,130
we we we don't just change the battle to be the flu and so

1875
02:06:08,150 --> 02:06:10,570
actually you're expected

1876
02:06:10,940 --> 02:06:14,230
expected loss of one-third in this game because

1877
02:06:14,250 --> 02:06:16,940
one third chance to do the same thing in the guy that new

1878
02:06:16,940 --> 02:06:20,320
you lose two thirds chance

1879
02:06:20,400 --> 02:06:24,210
so we start with that will have a fourth action which is played foosball

1880
02:06:24,210 --> 02:06:25,690
in this fourth action

1881
02:06:25,730 --> 02:06:31,200
but the man slightly negative if the other papers players doing rock paper scissors but

1882
02:06:31,200 --> 02:06:34,710
positive person joined the and played football with you

1883
02:06:34,730 --> 02:06:38,360
so we have the fourth option there here

1884
02:06:38,380 --> 02:06:39,630
but it's going to be a plus

1885
02:06:39,670 --> 02:06:42,110
for about but this online says

1886
02:06:42,230 --> 02:06:44,800
up here in other players doing anything

1887
02:06:44,840 --> 02:06:49,170
so if you set this up right i'll go through the exact values you need

1888
02:06:49,250 --> 02:06:54,290
you can make it so the only nash equilibrium is to help both players playing

1889
02:06:55,360 --> 02:06:57,770
OK and the reason is that

1890
02:06:57,820 --> 02:07:00,110
this part is a losing proposition

1891
02:07:00,150 --> 02:07:04,420
so if you're instead having some probability and playing foosball and some probably rock paper

