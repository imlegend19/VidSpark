1
00:00:00,000 --> 00:00:01,050
flavor that

2
00:00:01,100 --> 00:00:03,710
research thing as well

3
00:00:04,600 --> 00:00:10,300
we talked about directed graphs and it's very intuitive to try to think of directed

4
00:00:10,300 --> 00:00:13,920
graphs in terms of causal relationships

5
00:00:13,980 --> 00:00:15,560
and you know

6
00:00:15,560 --> 00:00:18,170
inferring causal relationships

7
00:00:18,180 --> 00:00:23,230
this is the fundamental component of both cognition you know human beings and animals are

8
00:00:23,510 --> 00:00:25,630
constantly trying to figure out

9
00:00:25,640 --> 00:00:31,480
causal explanations for what's around them and also the science scientific discovery

10
00:00:31,520 --> 00:00:36,280
is all about discovering causes for facts

11
00:00:37,230 --> 00:00:43,430
so it's important to think about causality and phil david will be talking about that

12
00:00:43,440 --> 00:00:44,840
in the second week

13
00:00:44,850 --> 00:00:48,650
but just to give you kind of a little flavor

14
00:00:48,670 --> 00:00:55,270
although independence relationships might be identical there because of the difference between the graph that

15
00:00:55,270 --> 00:00:57,230
went from smoking

16
00:00:57,260 --> 00:00:58,530
the yellow teeth

17
00:00:58,550 --> 00:01:02,270
and got the win from yellow teeth smoking

18
00:01:02,280 --> 00:01:06,140
so in particular the way to think about these things think of these as binary

19
00:01:06,140 --> 00:01:07,970
that variables

20
00:01:12,070 --> 00:01:15,350
the key idea is to think about

21
00:01:15,390 --> 00:01:21,220
what would happen if rather than just observing these variables we were able to intervene

22
00:01:21,270 --> 00:01:26,010
to actually do something to the setting of the variable

23
00:01:26,020 --> 00:01:29,060
OK so the probability of smoking

24
00:01:29,070 --> 00:01:30,270
given then

25
00:01:30,270 --> 00:01:31,970
that the TV

26
00:01:31,980 --> 00:01:34,600
of the particular person are yellow

27
00:01:34,610 --> 00:01:38,020
it is not the same as the probability of smoking

28
00:01:38,020 --> 00:01:41,260
given that you painted somebody's teeth yellow

29
00:01:41,320 --> 00:01:44,600
OK these are different things so the intervention

30
00:01:44,640 --> 00:01:50,100
of course you know painting somebody's teeth yellow doesn't cause them to smoke i think

31
00:01:50,220 --> 00:01:54,340
my cousin to get upset you wouldn't cause smoke

32
00:01:54,390 --> 00:02:01,520
on the other hand if we think that smoking causes people's teeth to get yellow

33
00:02:01,680 --> 00:02:03,440
then the also

34
00:02:03,470 --> 00:02:07,500
unethical intervention of forcing somebody to smoke

35
00:02:07,500 --> 00:02:09,280
i would presume we

36
00:02:10,270 --> 00:02:14,780
their probability of teeth to be yellow and that

37
00:02:14,840 --> 00:02:17,610
that probability

38
00:02:17,670 --> 00:02:19,020
it would be

39
00:02:19,030 --> 00:02:21,970
the same as if they were

40
00:02:22,100 --> 00:02:24,470
smoking of their own free will

41
00:02:25,550 --> 00:02:30,810
this is a gross simplification but hopefully gives you a flavor for the difference between

42
00:02:31,020 --> 00:02:32,460
just observing

43
00:02:32,520 --> 00:02:37,730
variables and being able to intervene to infer causal relationships

44
00:02:37,770 --> 00:02:43,060
and the key difficulty in learning causal relationships is you might have a causing b

45
00:02:43,060 --> 00:02:46,720
or you may have to be causing a

46
00:02:46,770 --> 00:02:52,180
the problem is you might also have one or any number of hidden common causes

47
00:02:52,180 --> 00:02:53,640
causing both a and b

48
00:02:53,650 --> 00:02:55,850
so observing relationships between a and b

49
00:02:55,890 --> 00:02:57,890
it's very hard to figure out what the causal

50
00:02:57,930 --> 00:02:59,680
link is

51
00:02:59,960 --> 00:03:02,090
all right

52
00:03:03,850 --> 00:03:08,940
i'm just i think i'm just going to skip over well i'll just say a

53
00:03:08,940 --> 00:03:10,590
couple things about this

54
00:03:12,560 --> 00:03:16,730
we just talked about directed graphs what about undirected graphs

55
00:03:16,760 --> 00:03:21,100
so can we do parameter and structure learning in undirected graphs

56
00:03:21,180 --> 00:03:26,980
and the problem with undirected graphs and you know general factor graphs if you just

57
00:03:26,980 --> 00:03:30,860
write it out as the fact that it didn't come from a directed graph originally

58
00:03:30,940 --> 00:03:37,110
the problem is that there's this pesky little normalisation constant k and usually we just

59
00:03:37,110 --> 00:03:40,970
said well that's the normalisation constant maybe we you know we don't have to worry

60
00:03:40,970 --> 00:03:48,600
about it but in very already describes ways of trying to estimate normalisation constants which

61
00:03:48,600 --> 00:03:49,300
you know

62
00:03:49,310 --> 00:03:53,500
it's difficult computational problem to compute the normalisation constant

63
00:03:54,480 --> 00:03:57,710
the problem with parameter and structure learning

64
00:03:57,730 --> 00:04:03,180
is that if you're graph looks like this and each of your factors has some

65
00:04:05,260 --> 00:04:09,550
associated with the then you're normalisation constant

66
00:04:09,570 --> 00:04:11,400
is not constant

67
00:04:11,420 --> 00:04:14,130
with respect to the parameters right

68
00:04:14,170 --> 00:04:16,600
for every setting of

69
00:04:16,850 --> 00:04:23,470
these parameters this whole vector parameters this whole thing has to sum to one

70
00:04:23,470 --> 00:04:26,270
so the normalisation constant z

71
00:04:26,280 --> 00:04:33,210
it is actually potentially very complicated intractable function of your parameters

72
00:04:35,480 --> 00:04:40,770
computing the normalisation constant is computationally intractable usually

73
00:04:40,810 --> 00:04:43,680
for non tree structured graphs et cetera

74
00:04:44,310 --> 00:04:52,510
maximum likelihood methods and bayesian scoring of structures all that becomes formally computationally intractable

75
00:04:52,530 --> 00:04:54,050
and in fact

76
00:04:54,570 --> 00:04:58,810
it's you know there's sort of levels of computational intractability and this is

77
00:04:58,810 --> 00:05:00,510
something that is

78
00:05:00,520 --> 00:05:05,100
even more intractable than than some other problems but it doesn't mean that we can

79
00:05:05,100 --> 00:05:10,300
try to solve it so there are algorithms out there approximate algorithms are based on

80
00:05:10,300 --> 00:05:14,400
MCMC and other methods that are trying to solve these

81
00:05:15,310 --> 00:05:18,180
OK this is an active area of research that

82
00:05:18,240 --> 00:05:21,680
ian marie myself have been involved in

83
00:05:23,550 --> 00:05:30,210
i've done a very good job sort of using at the time without getting into

84
00:05:31,600 --> 00:05:34,420
one bit that i want to talk about that i'm going to

85
00:05:34,430 --> 00:05:37,800
use my last five minutes to go into that just to give you a flavor

86
00:05:40,730 --> 00:05:45,190
so that's the summary of what i talked about today

87
00:05:45,220 --> 00:05:47,020
and we have five minutes

88
00:05:47,050 --> 00:05:51,630
and here's what we're gonna do we have the benefit of videolectures

89
00:05:53,560 --> 00:05:55,280
you can go home

90
00:05:56,400 --> 00:05:59,170
play this

91
00:05:59,170 --> 00:06:00,800
slow down

92
00:06:01,440 --> 00:06:05,060
so i'm going to go through this really fast and to try to do this

93
00:06:05,060 --> 00:06:06,420
in five minutes

94
00:06:06,420 --> 00:06:09,350
and then you can have a coffee break to recover from it and for me

95
00:06:09,350 --> 00:06:10,780
to recover as well

96
00:06:11,520 --> 00:06:16,210
let me just tell you about something really fun it's work in progress we don't

97
00:06:16,210 --> 00:06:20,430
even have a paper on it but the reason i'm talking about it and first

98
00:06:20,430 --> 00:06:25,100
of all what i should say is that this is with co-authors ryan adams and

99
00:06:25,100 --> 00:06:28,210
hanna wallach who deserve all the credit OK

100
00:06:28,230 --> 00:06:32,720
and the reason i'm going to talk about this is that it's something that brings

101
00:06:32,720 --> 00:06:36,220
together a whole bunch of topics that are covered in the lectures

102
00:06:37,520 --> 00:06:39,310
in these two weeks

103
00:06:39,360 --> 00:06:44,960
so the topic is learning the structure of deep sparse directed graphical models

104
00:06:44,970 --> 00:06:46,690
so what is that

105
00:06:46,710 --> 00:06:49,630
well the reason i'm talking about is because

106
00:06:49,640 --> 00:06:51,960
it has to do with graphical models

107
00:06:51,970 --> 00:06:56,360
it's related to deep belief networks which have hinton is going to talk about next

108
00:06:57,320 --> 00:07:01,970
we use markov chain monte carlo methods and some nonparametric bayesian model

109
00:07:02,010 --> 00:07:06,600
so all these things come together very naturally to solve this problem

110
00:07:06,600 --> 00:07:08,980
it could be one

111
00:07:08,990 --> 00:07:12,340
the possible i can values are

112
00:07:13,270 --> 00:07:15,240
and minus one

113
00:07:17,100 --> 00:07:18,790
i n minus

114
00:07:19,520 --> 00:07:23,640
and no others right

115
00:07:23,650 --> 00:07:26,120
now other slice that this this is

116
00:07:26,150 --> 00:07:27,530
see why that is

117
00:07:27,610 --> 00:07:34,630
while suppose we have an organ value and i can vector suppose so really this

118
00:07:34,630 --> 00:07:36,290
is the reason that

119
00:07:36,990 --> 00:07:39,520
so why

120
00:07:39,530 --> 00:07:44,280
if if i have the will and x

121
00:07:44,290 --> 00:07:48,740
all apply here and again i would have square attacks

122
00:07:49,790 --> 00:07:54,630
what i what i learned from f squared x

123
00:07:54,640 --> 00:07:59,790
it would it would be lambda square right because it would be lambda fx but

124
00:07:59,790 --> 00:08:05,280
the fact is another landslide so i lambda square next me jumped after the four

125
00:08:05,290 --> 00:08:10,380
facts i would learn that that was planned for facts

126
00:08:10,390 --> 00:08:13,110
but now if f

127
00:08:13,130 --> 00:08:15,400
if i had this incredibly

128
00:08:15,420 --> 00:08:17,320
special situation

129
00:08:17,320 --> 00:08:21,630
where the fourth power has come around to the identity

130
00:08:21,670 --> 00:08:25,850
and this would be x so i learned that lambda therefore so

131
00:08:25,890 --> 00:08:28,950
lambda therefore it has to be one

132
00:08:28,970 --> 00:08:31,990
and if lambda fourth is one of those are the only

133
00:08:32,150 --> 00:08:34,280
four possible landing

134
00:08:38,180 --> 00:08:42,110
first of all we have a by a matrix are actually we have n by

135
00:08:42,110 --> 00:08:44,150
n matrix for every n

136
00:08:44,170 --> 00:08:48,060
the this is through these are the four organised

137
00:08:48,070 --> 00:08:48,930
so what

138
00:08:49,040 --> 00:08:51,970
what puzzle could remains here

139
00:08:52,000 --> 00:08:55,150
but i'm going to ask you about

140
00:08:58,200 --> 00:09:03,370
this is true for any n any and it's true for any n

141
00:09:03,980 --> 00:09:05,390
and that is

142
00:09:05,390 --> 00:09:07,940
i agree so that

143
00:09:07,950 --> 00:09:11,840
i did you might try to prove that

144
00:09:11,850 --> 00:09:12,830
i guess

145
00:09:13,470 --> 00:09:16,240
maybe have square is minus

146
00:09:16,280 --> 00:09:18,420
five right

147
00:09:18,550 --> 00:09:20,530
i remember i

148
00:09:20,580 --> 00:09:27,230
if f squared is minus side and then we're certainly got after fourth nailed

149
00:09:27,260 --> 00:09:30,350
i think maybe half squared is minus site

150
00:09:30,450 --> 00:09:32,710
and forces

151
00:09:32,760 --> 00:09:37,110
that would be that should be fun to

152
00:09:37,140 --> 00:09:39,420
to prove

153
00:09:41,740 --> 00:09:47,470
but it gets let's take that as something we can check

154
00:09:47,490 --> 00:09:51,320
and then that tells us what the possible organ values are so one of the

155
00:09:51,330 --> 00:09:55,060
questions could i

156
00:09:55,070 --> 00:09:58,890
i could ask about the multiplicity is exactly

157
00:09:58,910 --> 00:10:01,530
and that's there's an answer for that

158
00:10:01,540 --> 00:10:02,300
you know

159
00:10:03,040 --> 00:10:08,160
let's see if any a by a since forgoes evenly the weight i suspect that

160
00:10:08,160 --> 00:10:16,090
the the reach multiplicity two but my memory doesn't know this isn't the problem i

161
00:10:16,290 --> 00:10:18,420
really know about

162
00:10:19,390 --> 00:10:24,870
i think an otherwise there's you know then them

163
00:10:24,880 --> 00:10:26,710
if it's an by and

164
00:10:26,790 --> 00:10:28,780
and and and four doesn't go

165
00:10:28,790 --> 00:10:31,510
even in the way and then we've got some

166
00:10:31,540 --> 00:10:35,310
you know we've got to make the count right but it's no

167
00:10:35,320 --> 00:10:38,180
what is known

168
00:10:38,210 --> 00:10:42,110
in a satisfactory way at least as far as i can tell and i

169
00:10:42,160 --> 00:10:45,940
had just had occasion to look through various

170
00:10:46,980 --> 00:10:49,960
it is one of the eigen vectors

171
00:10:49,970 --> 00:10:53,140
we've got the i mean it seems amazing to me

172
00:10:53,150 --> 00:10:55,460
we've got the organ values

173
00:10:55,480 --> 00:10:59,080
we know the multiplicity and that is checked out

174
00:10:59,090 --> 00:11:05,880
but what watson nice description of but straightforward description of the eigen vectors

175
00:11:05,880 --> 00:11:07,500
we've chosen one

176
00:11:09,110 --> 00:11:10,460
in that basis

177
00:11:10,470 --> 00:11:12,040
we write

178
00:11:12,080 --> 00:11:16,330
we write the signal in that basis and that's what my lecture that's the math

179
00:11:16,330 --> 00:11:17,970
part of my lectures

180
00:11:18,030 --> 00:11:20,070
now here's the application part

181
00:11:20,130 --> 00:11:23,310
the next part is going to be the compression step

182
00:11:23,430 --> 00:11:28,150
and that's the last thing

183
00:11:28,170 --> 00:11:30,150
we're going to lose information

184
00:11:30,190 --> 00:11:34,520
and what what will actually happen at that

185
00:11:34,530 --> 00:11:36,030
well one

186
00:11:36,040 --> 00:11:41,040
o thing we could do is just throw away the small coefficients

187
00:11:41,050 --> 00:11:42,700
so that would be

188
00:11:42,720 --> 00:11:45,710
that's called threshold thresholding we set some threshold

189
00:11:45,770 --> 00:11:47,400
every coefficient

190
00:11:48,250 --> 00:11:53,780
basis vector that's not in their more than the a threshold value

191
00:11:53,800 --> 00:11:57,810
and we set some threshold so that are i can see the difference whether we

192
00:11:57,810 --> 00:11:59,460
throw away that

193
00:11:59,510 --> 00:12:04,090
our can hardly see the difference whether we throw away that little

194
00:12:04,110 --> 00:12:06,430
a bit of that basis vector keep it

195
00:12:06,480 --> 00:12:08,460
so this compression step

196
00:12:10,410 --> 00:12:15,230
compressed set of coefficients just keep going here

197
00:12:16,050 --> 00:12:22,570
so he was going this compression step produces some coefficients c that

198
00:12:22,630 --> 00:12:26,950
and with lots of with many zeros

199
00:12:27,140 --> 00:12:30,520
so that's where the compression can

200
00:12:30,590 --> 00:12:35,930
probably there is enough of this vector of all ones we we very seldom throw

201
00:12:35,930 --> 00:12:37,500
that away

202
00:12:37,510 --> 00:12:40,250
usually it's coefficient will be large

203
00:12:40,290 --> 00:12:43,610
but the coefficient of something like this

204
00:12:43,640 --> 00:12:46,460
that quickly alternating vector

205
00:12:46,520 --> 00:12:51,110
there's probably very little of that in any smooth signal i mean that's high frequency

206
00:12:51,110 --> 00:12:54,750
this is this is low frequency zero frequency

207
00:12:54,760 --> 00:12:58,710
this stuff is the highest frequency we could have an

208
00:12:58,770 --> 00:13:01,330
there's all the noise the

209
00:13:01,760 --> 00:13:03,580
gender is producing

210
00:13:03,600 --> 00:13:04,950
that sort of

211
00:13:04,960 --> 00:13:13,170
output but a small lecture like this one is very little of the highest frequency

212
00:13:13,180 --> 00:13:15,720
very little noise and this like OK

213
00:13:17,520 --> 00:13:22,210
we throw away whatever there's and we were left with just a few

214
00:13:22,230 --> 00:13:24,540
coefficients and then we

215
00:13:26,660 --> 00:13:29,230
a signal

216
00:13:29,260 --> 00:13:34,190
using those coefficients we take those coefficients times

217
00:13:34,220 --> 00:13:36,090
there basis vectors

218
00:13:37,470 --> 00:13:41,310
this some doesn't have sixty four terms anymore

219
00:13:41,310 --> 00:13:44,430
probably it has about two or three terms

220
00:13:44,460 --> 00:13:47,390
so that if we say it has three terms

221
00:13:47,440 --> 00:13:51,970
from sixty four down to three that's compression of twenty one to one

222
00:13:52,020 --> 00:13:55,050
that's the kind of compression you're looking for

223
00:13:56,250 --> 00:13:58,710
everybody is looking for that sort of compression

224
00:13:59,390 --> 00:14:04,040
let's see i guess i met the problem with the FBI

225
00:14:04,130 --> 00:14:06,260
and fingerprints

226
00:14:06,300 --> 00:14:10,290
so there's a whole lot of still images you know you with here some you

227
00:14:10,290 --> 00:14:11,220
make these

228
00:14:11,270 --> 00:14:13,190
inky mark

229
00:14:14,940 --> 00:14:16,460
go somewhere

230
00:14:16,480 --> 00:14:20,810
used to go to washington gets stored in a big five

231
00:14:20,820 --> 00:14:24,710
so washington header file of thirty million

232
00:14:24,750 --> 00:14:25,860
murder is

233
00:14:26,940 --> 00:14:30,700
cheetahs on with there's other stuff

234
00:14:31,820 --> 00:14:35,030
and actually

235
00:14:35,050 --> 00:14:40,130
there was no way to retrieve them in time like so so exposure at the

236
00:14:40,130 --> 00:14:41,190
police station

237
00:14:41,240 --> 00:14:42,730
it's OK this

238
00:14:42,750 --> 00:14:44,710
a person may have done this

239
00:14:44,750 --> 00:14:49,620
check with washington have they got are suffering his or fingerprints on file

240
00:14:50,560 --> 00:14:53,100
washington will know the answer

241
00:14:53,150 --> 00:14:54,520
within a week

242
00:14:54,550 --> 00:15:00,710
with if it's got filing cabinets full of fingerprints so of course the natural step

243
00:15:00,710 --> 00:15:03,490
is digitized

244
00:15:03,510 --> 00:15:06,980
so all fingerprints are now digitized

245
00:15:06,990 --> 00:15:09,900
so now it's is fully least electronic

246
00:15:09,960 --> 00:15:11,600
but still

247
00:15:11,630 --> 00:15:15,130
there's too much information in each

248
00:15:15,150 --> 00:15:18,730
each one i mean you can search through that many

249
00:15:21,120 --> 00:15:22,470
if you if

250
00:15:23,390 --> 00:15:26,580
the digital images five traf were

251
00:15:26,590 --> 00:15:29,580
by five elsewhere if it's that many pixels

252
00:15:30,390 --> 00:15:34,170
you get compressed so the FBI had to decide

253
00:15:34,220 --> 00:15:36,290
what basis to choose

254
00:15:36,300 --> 00:15:40,520
four compression of fingerprints and then they built a big new facility

255
00:15:40,530 --> 00:15:42,010
in west virginia

256
00:15:42,020 --> 00:15:43,800
and that's where

257
00:15:43,820 --> 00:15:46,380
fingerprint now suppose that so if you

258
00:15:46,390 --> 00:15:49,960
i think if you get your finger princeton now at the police station

259
00:15:50,000 --> 00:15:56,010
if it's an up-to-date police station it happens digitally and the signal is sent digitally

260
00:15:56,340 --> 00:16:00,860
and then west virginia to compress and index

261
00:16:00,920 --> 00:16:02,530
and then

262
00:16:02,540 --> 00:16:05,370
if they want to find you they

263
00:16:05,400 --> 00:16:06,990
i can do it within minutes

264
00:16:06,990 --> 00:16:09,000
that within a week

265
00:16:09,020 --> 00:16:11,810
OK so this compression is

266
00:16:11,830 --> 00:16:14,320
comes up or signal for images

267
00:16:14,340 --> 00:16:15,780
for video

268
00:16:15,780 --> 00:16:16,640
which is

269
00:16:16,660 --> 00:16:18,310
like these lectures

270
00:16:18,510 --> 00:16:20,660
there's another aspect

271
00:16:20,710 --> 00:16:24,540
you could treat the video ones still

272
00:16:24,570 --> 00:16:27,000
image after another

273
00:16:27,040 --> 00:16:29,160
and compress each one

274
00:16:29,210 --> 00:16:30,850
and then

275
00:16:30,880 --> 00:16:33,420
run them and make of it

276
00:16:33,470 --> 00:16:35,530
but that missus

277
00:16:35,540 --> 00:16:38,250
well you can see why that's not optimal

278
00:16:38,270 --> 00:16:39,710
in video

279
00:16:39,720 --> 00:16:41,540
so for instance in the video

280
00:16:41,560 --> 00:16:43,890
o thing you you you you have

281
00:16:44,220 --> 00:16:47,170
a sequence of images

282
00:16:47,230 --> 00:16:50,470
the video is really a sequence of images

283
00:16:54,620 --> 00:16:56,230
what about

284
00:16:56,270 --> 00:16:58,900
one image to the next image

285
00:16:58,920 --> 00:17:04,410
there are extremely correlated i mean that getting an image every split second

286
00:17:04,460 --> 00:17:08,820
and also i'm moving slightly that's what's producing the

287
00:17:09,190 --> 00:17:12,340
jumping motion on the video

288
00:17:13,890 --> 00:17:15,800
i'm not like

289
00:17:15,810 --> 00:17:21,540
you know each each image in the sequence is pretty close to the one before

290
00:17:21,620 --> 00:17:24,870
so you have to use like prediction

291
00:17:24,880 --> 00:17:27,490
and correction i mean you might

292
00:17:27,490 --> 00:17:28,880
the image of me

293
00:17:28,900 --> 00:17:34,600
one instance one time step later you would assume would be the same and then

294
00:17:34,600 --> 00:17:36,570
plus small corrections

295
00:17:36,610 --> 00:17:38,230
and you would only

296
00:17:38,240 --> 00:17:41,290
code and digitize the correction

297
00:17:41,300 --> 00:17:43,300
and compress the correction

298
00:17:43,910 --> 00:17:45,940
the sequence of images

299
00:17:45,940 --> 00:17:48,710
it's highly correlated and

300
00:17:50,900 --> 00:17:57,540
that problem and compression is always to use this correlation is fact that

301
00:17:57,550 --> 00:17:59,890
in time or in space

302
00:17:59,940 --> 00:18:02,550
things don't change instantly

303
00:18:03,640 --> 00:18:06,090
are very often smooth changes

304
00:18:06,100 --> 00:18:09,280
and you can predict

305
00:18:09,290 --> 00:18:11,510
one value from the previous round

306
00:18:11,530 --> 00:18:15,960
OK so that's the those are applications

307
00:18:16,050 --> 00:18:18,480
which are pure linear algebra

308
00:18:18,530 --> 00:18:23,580
i could maybe i maybe allow me to tell you the

309
00:18:25,010 --> 00:18:26,650
and the book describes

310
00:18:27,380 --> 00:18:29,120
new basis

311
00:18:30,510 --> 00:18:32,470
the competition for four

312
00:18:32,520 --> 00:18:37,780
so the competition for four e a is called wavelet

313
00:18:37,800 --> 00:18:42,990
and i can describe what the basis is like saying the eight by eight days

314
00:18:43,040 --> 00:18:45,670
so that a by a wavelet basis

315
00:18:45,710 --> 00:18:50,360
is the eight is the vector of all ones eight ones

316
00:18:50,460 --> 00:18:56,050
and the vector of four one and four minus one

317
00:18:56,090 --> 00:18:58,740
then the vector of

318
00:18:58,770 --> 00:19:02,470
two ones include minus one thousand four zeros

319
00:19:02,550 --> 00:19:07,510
and also all the vector of four zero

320
00:19:07,510 --> 00:19:09,370
good morning everybody

321
00:19:13,100 --> 00:19:16,670
you know what it's like when the conference organizer ask you to give the keynote

322
00:19:16,670 --> 00:19:20,480
and you say ok big honor like to do it and that i shall ask

323
00:19:20,480 --> 00:19:25,370
me all this is the tenth anniversary can you use that as part of the

324
00:19:25,370 --> 00:19:29,350
theme of your presentation and all by the way can you also give me a

325
00:19:30,080 --> 00:19:33,190
so by the time you only have the vaguest idea what you want to talk

326
00:19:33,190 --> 00:19:35,630
about you know you have to keep you happy so

327
00:19:35,690 --> 00:19:39,860
you give some title and then a few weeks later you get what the french

328
00:19:39,860 --> 00:19:45,860
call this as this clear which is the idea is you get on the stairs

329
00:19:45,860 --> 00:19:48,820
walking back home you know all the things you should have said

330
00:19:49,390 --> 00:19:52,360
and then i realized what the title should be

331
00:19:52,390 --> 00:19:57,710
so the title is not what it says your program something about universal patterns the

332
00:19:57,710 --> 00:19:59,130
title actually is

333
00:19:59,140 --> 00:20:01,060
ten years of semantic web

334
00:20:01,100 --> 00:20:02,710
does it work in theory

335
00:20:05,730 --> 00:20:09,030
so what i want to do in this talk

336
00:20:09,060 --> 00:20:10,500
is back

337
00:20:10,520 --> 00:20:12,600
at ten years of our work

338
00:20:12,620 --> 00:20:19,040
and i'm not going to celebrate our successes because they are so obvious they don't

339
00:20:19,040 --> 00:20:20,950
need to be stressed it's obvious

340
00:20:21,040 --> 00:20:25,670
because we have done a good job as engineers we have built

341
00:20:25,710 --> 00:20:32,040
really successful very large artifact but the question for this talk will be done any

342
00:20:32,980 --> 00:20:35,310
every besides building anything

343
00:20:35,320 --> 00:20:36,420
if we

344
00:20:36,450 --> 00:20:38,150
learned anything

345
00:20:39,040 --> 00:20:40,310
that is of

346
00:20:40,340 --> 00:20:42,870
permanent value

347
00:20:47,340 --> 00:20:51,320
this is going to be no by definition this is a pretentious talk right so

348
00:20:51,320 --> 00:20:53,480
it's always science conference

349
00:20:53,530 --> 00:21:00,820
it's always pretentious to talk about science instead of just talking science and

350
00:21:00,860 --> 00:21:05,560
with some trepidation i i started this talk and i'm not the only one so

351
00:21:06,670 --> 00:21:11,950
jeff norton who is a leading researcher in the database field

352
00:21:12,040 --> 00:21:15,700
last year i think it was that i see the two thousand ten he also

353
00:21:15,700 --> 00:21:16,730
gave a talk

354
00:21:16,730 --> 00:21:20,140
about science in this case it was about the scientific process

355
00:21:20,160 --> 00:21:23,820
and how we organize it and he started

356
00:21:24,250 --> 00:21:30,230
with an introductory slides that i thought was very appropriate so i for freely borrowed

357
00:21:30,230 --> 00:21:34,050
this slide from him so what he said was well when he announced that he

358
00:21:34,050 --> 00:21:37,550
was going to give this talk to a colleague so the first response he got

359
00:21:37,550 --> 00:21:41,260
from a colleague was called i i look forward to it

360
00:21:42,070 --> 00:21:45,750
he's said this to another colleague but then also explained what he was going to

361
00:21:45,750 --> 00:21:47,000
talk about

362
00:21:47,040 --> 00:21:51,420
and the response was oh how are you ever going to make this interesting

363
00:21:52,360 --> 00:21:53,410
and then you

364
00:21:53,420 --> 00:21:55,100
tried it on the third colleague

365
00:21:55,110 --> 00:21:57,980
and the response was well jeff

366
00:21:58,010 --> 00:22:02,000
you've now reached the age where you're old enough to scratch about in public so

367
00:22:02,040 --> 00:22:03,760
go ahead

368
00:22:04,020 --> 00:22:09,770
and maybe the most disheartening response was for it he announces plans to a senior

369
00:22:09,770 --> 00:22:13,720
colleague and emeritus and response was this don't do it

370
00:22:13,730 --> 00:22:18,980
now given the keynote anyway means you are a washed up has-been so

371
00:22:19,000 --> 00:22:22,890
but but notice the floor piece of logic in this right so

372
00:22:22,910 --> 00:22:26,540
even without giving the keynote you could still be washed up has-been

373
00:22:26,570 --> 00:22:30,010
so in that case you might as well give the keynote as well

374
00:22:30,040 --> 00:22:35,320
so that's the plan

375
00:22:35,950 --> 00:22:37,730
talking about

376
00:22:37,790 --> 00:22:43,390
generic los junior close science lots of computer science before i

377
00:22:43,450 --> 00:22:48,610
i can talk about generic laws of computer science i i need to make it

378
00:22:48,630 --> 00:22:52,980
philosophical confession right i need to explain to you where i stand on this idea

379
00:22:52,980 --> 00:22:56,760
of of of what science is

380
00:22:57,670 --> 00:23:00,250
so here's my view of science

381
00:23:01,550 --> 00:23:04,260
what's called the philosophy of scientific realists

382
00:23:04,290 --> 00:23:10,750
and i'm even naive scientific realist maybe i'm even happy naive scientific realists so what

383
00:23:10,820 --> 00:23:13,290
scientific realism

384
00:23:13,320 --> 00:23:16,970
so here's a quote from the stanford handbook of philosophy

385
00:23:17,630 --> 00:23:18,500
it says

386
00:23:19,600 --> 00:23:21,290
philosophical realism

387
00:23:21,300 --> 00:23:26,210
it is the belief that our reality is ontologically independent from what we think

388
00:23:26,210 --> 00:23:29,260
just to quickly summarize

389
00:23:29,270 --> 00:23:34,450
so we thought what concepts the empirical risk EX

390
00:23:34,460 --> 00:23:37,230
o point six functions

391
00:23:38,350 --> 00:23:40,240
the actual risk

392
00:23:40,260 --> 00:23:42,770
also to do this

393
00:23:42,820 --> 00:23:46,410
one of the principal one principle

394
00:23:46,620 --> 00:23:50,480
to estimate function which we have low actually

395
00:23:50,530 --> 00:23:53,490
they only knowing the training examples

396
00:23:53,510 --> 00:23:56,710
the principle of the minimum here

397
00:23:59,580 --> 00:24:02,420
just saying we would like to have a small risk

398
00:24:02,430 --> 00:24:04,680
because his

399
00:24:05,800 --> 00:24:12,110
we can compute the dangerous with it simply minimize our here over the last

400
00:24:12,300 --> 00:24:16,830
get a function which we hope will small actually

401
00:24:19,150 --> 00:24:21,660
i already told we minimize

402
00:24:21,780 --> 00:24:22,940
here it is

403
00:24:23,650 --> 00:24:30,090
what resulted was it depend on the size of

404
00:24:30,100 --> 00:24:33,280
first of all because the are lots of the function class the the more likely

405
00:24:33,280 --> 00:24:36,490
it is that we have to have more here

406
00:24:36,620 --> 00:24:39,480
from that point you would appear to be with

407
00:24:39,490 --> 00:24:43,400
and then you get with this small here

408
00:24:43,410 --> 00:24:49,500
but it turns out in the theory there's a lot of the more likely to

409
00:24:49,500 --> 00:24:52,880
cause but there's a lot of that

410
00:24:52,890 --> 00:24:57,080
the more likely it is that you have to choose one for which the difference

411
00:24:57,080 --> 00:25:00,930
between a terrorist activities is large

412
00:25:00,980 --> 00:25:05,860
you would like look at the same time would have liked

413
00:25:05,870 --> 00:25:08,470
it allows you to get a small

414
00:25:08,520 --> 00:25:10,290
at the same time

415
00:25:10,350 --> 00:25:17,050
he class which is small enough such that the rest of the challenge

416
00:25:17,290 --> 00:25:21,960
and now analyse how it works well because

417
00:25:21,980 --> 00:25:24,230
based on the law of large numbers

418
00:25:24,440 --> 00:25:28,630
recall that one function the members flows from process

419
00:25:28,670 --> 00:25:35,000
basically we to generalize this to many functions or even if many functions

420
00:25:35,050 --> 00:25:37,890
OK so the analysis works as follows

421
00:25:37,910 --> 00:25:39,020
first of all

422
00:25:39,100 --> 00:25:41,290
find this shorthand i

423
00:25:41,560 --> 00:25:43,710
for the zero one loss

424
00:25:43,730 --> 00:25:48,930
so i is the zero one loss of the training examples x i y i

425
00:25:48,970 --> 00:25:52,520
so whenever it is correctly classified

426
00:25:52,990 --> 00:25:55,400
this training example

427
00:25:55,460 --> 00:25:58,600
loss is zero whenever incorrect losses one

428
00:26:01,670 --> 00:26:04,580
now if of is fixed

429
00:26:04,600 --> 00:26:10,780
if all x i y i independently sampled from online distribution

430
00:26:10,820 --> 00:26:16,170
then the squid is phi and also independent of each other

431
00:26:16,180 --> 00:26:21,210
and they have another property of course also called the files because the values of

432
00:26:21,210 --> 00:26:22,760
plus minus one

433
00:26:22,780 --> 00:26:25,260
these are independent bernoulli trials

434
00:26:25,270 --> 00:26:26,380
so this is

435
00:26:26,430 --> 00:26:29,000
on the statistical point of view the same thing

436
00:26:29,010 --> 00:26:31,380
as a consequence

437
00:26:31,390 --> 00:26:34,270
potentially a biased coin something like that

438
00:26:34,320 --> 00:26:39,440
so it's a random experiment which is the result zero one

439
00:26:39,490 --> 00:26:42,260
the empirical means if you

440
00:26:43,200 --> 00:26:45,530
he was

441
00:26:49,900 --> 00:26:54,340
so why always plus one one

442
00:26:54,350 --> 00:26:57,520
you can also define last one of them

443
00:26:57,620 --> 00:26:59,650
in which case you would have a hard here

444
00:26:59,660 --> 00:27:02,770
so my question is that the plus one minus one

445
00:27:02,910 --> 00:27:07,200
must i would have so that this thing is zero

446
00:27:07,210 --> 00:27:09,080
so this is like a contrast

447
00:27:09,090 --> 00:27:10,980
so your fuzzy kind

448
00:27:11,000 --> 00:27:15,070
then you can do things like to compute the mean

449
00:27:15,090 --> 00:27:17,230
how do you get heads or tails

450
00:27:17,330 --> 00:27:20,840
over n coin tosses

451
00:27:20,890 --> 00:27:23,720
we consider the empirical mean of this

452
00:27:23,730 --> 00:27:26,470
quantity of this random variable

453
00:27:26,830 --> 00:27:29,960
then by definition we get a terrorist

454
00:27:29,980 --> 00:27:31,600
of the function f so

455
00:27:31,630 --> 00:27:32,550
live in

456
00:27:32,560 --> 00:27:34,980
this definition of course i here

457
00:27:35,090 --> 00:27:39,340
it exactly what you see in greece before

458
00:27:39,450 --> 00:27:43,140
likewise if we compute the expected value of this thing

459
00:27:43,150 --> 00:27:44,640
it was a

460
00:27:44,650 --> 00:27:46,340
many examples of the mean

461
00:27:46,360 --> 00:27:49,760
averaged over the unknown underlying distributions

462
00:27:49,780 --> 00:27:51,380
you get the

463
00:27:51,400 --> 00:27:54,340
for are

464
00:27:54,390 --> 00:27:59,010
all question of studying how the risk and the risk relate to each other

465
00:27:59,200 --> 00:28:02,490
is not transformed into the question of how

466
00:28:02,670 --> 00:28:04,140
empirical mean

467
00:28:04,180 --> 00:28:06,800
all this only variables

468
00:28:06,820 --> 00:28:09,570
converges to their expectation

469
00:28:09,710 --> 00:28:14,780
big the difference can be things like that

470
00:28:14,820 --> 00:28:18,120
others a classical bond in probability theory

471
00:28:18,130 --> 00:28:20,270
was proved by

472
00:28:20,320 --> 00:28:26,170
how much what still alive is a few years ago it was

473
00:28:26,180 --> 00:28:31,690
this is an example of what people call it a bond

474
00:28:32,210 --> 00:28:38,990
roughly speaking to because of the distribution of the

475
00:28:41,190 --> 00:28:43,480
if you for your kind

476
00:28:43,490 --> 00:28:47,200
many many times the distribution will be more and more people the point is if

477
00:28:47,200 --> 00:28:50,120
you have a fair coin toss your clients

478
00:28:50,130 --> 00:28:53,360
in the two thousand ten times it will get

479
00:28:53,380 --> 00:28:55,650
seven times and every time they

480
00:28:55,660 --> 00:28:58,060
if you toss it one thousand times

481
00:28:58,130 --> 00:29:01,860
you will not get seven hundred ten thousand three hundred maybe

482
00:29:01,870 --> 00:29:05,150
five hundred three and four hundred ninety seven or whatever so you will if it's

483
00:29:06,180 --> 00:29:11,530
the distribution will be more and more sharply concentrated around the expected value

484
00:29:11,540 --> 00:29:17,740
and the way to describe how concentrated distribution is around its expected value is is

485
00:29:17,740 --> 00:29:23,070
simply by giving up bound how likely tails are tails of the distribution of everything

486
00:29:23,070 --> 00:29:25,420
that's far away from the expected value

487
00:29:25,470 --> 00:29:26,950
we would like to

488
00:29:26,960 --> 00:29:29,520
limit the probability that

489
00:29:29,530 --> 00:29:31,330
you empirical mean here

490
00:29:31,350 --> 00:29:34,280
the expected the expected value

491
00:29:34,330 --> 00:29:36,540
by more than epsilon

492
00:29:36,550 --> 00:29:40,730
and telephone tells us that this probability is found

493
00:29:40,780 --> 00:29:43,890
by this exponential term here

494
00:29:43,900 --> 00:29:46,010
this is nice because you see

495
00:29:46,050 --> 00:29:50,940
the number of files goes to infinity then this is your very fast

496
00:29:50,950 --> 00:29:52,610
exponentially fast

497
00:29:52,660 --> 00:29:58,950
for each fixed epsilon because we can't put it down zero because otherwise the material

498
00:29:59,080 --> 00:30:03,490
we can put aside any small number that we like this thing to

499
00:30:03,500 --> 00:30:08,000
so this is one is quantification of all past

500
00:30:08,050 --> 00:30:10,580
large numbers works

501
00:30:10,630 --> 00:30:14,040
the first is this in your hand notes this slide

502
00:30:14,060 --> 00:30:16,680
mistakes like to tell me

503
00:30:16,700 --> 00:30:19,640
OK so

504
00:30:19,650 --> 00:30:24,790
OK maybe a little bit about the this notation so here i have put p

505
00:30:24,810 --> 00:30:26,710
which is also probability

506
00:30:26,720 --> 00:30:29,710
strictly speaking this is not the same

507
00:30:29,730 --> 00:30:31,720
probability that about four

508
00:30:31,770 --> 00:30:34,470
member of the board probability measures

509
00:30:34,480 --> 00:30:39,150
government the random experiment whole the x y here were generated

510
00:30:39,190 --> 00:30:42,090
because here is the probability measure over n

511
00:30:42,100 --> 00:30:45,710
here's of x y is a product measure

512
00:30:45,710 --> 00:30:50,310
it's interesting to note that the extreme this is an extreme terms way and according

513
00:30:50,310 --> 00:30:52,560
to entire according to pi star

514
00:30:52,600 --> 00:30:55,730
is an exchangeable at the current time according to

515
00:30:57,280 --> 00:30:58,960
so i'm going to have this

516
00:30:58,980 --> 00:31:01,780
this some

517
00:31:04,000 --> 00:31:05,760
this will be

518
00:31:05,770 --> 00:31:08,850
one of the elements the first element

519
00:31:09,810 --> 00:31:11,580
the first

520
00:31:12,400 --> 00:31:17,830
term of the sound and this will be the last element in the last term

521
00:31:17,870 --> 00:31:21,540
of the sound

522
00:31:21,550 --> 00:31:24,440
and in between i have all these things that i've inserted which is of the

523
00:31:25,310 --> 00:31:31,670
possibly money-spinner posse manor c

524
00:31:31,690 --> 00:31:32,880
OK and then

525
00:31:34,220 --> 00:31:35,760
what's going to happen is

526
00:31:35,760 --> 00:31:37,350
and note that

527
00:31:37,360 --> 00:31:39,430
and active according to high

528
00:31:39,460 --> 00:31:42,260
the first team minus one time steps

529
00:31:42,260 --> 00:31:44,340
in each of these

530
00:31:44,390 --> 00:31:46,920
two expectations

531
00:31:46,960 --> 00:31:48,350
that means that

532
00:31:48,380 --> 00:31:50,710
the difference

533
00:31:50,770 --> 00:31:55,210
in these two the expected sum it's going to be entirely due to how it

534
00:31:55,210 --> 00:31:59,590
behaves after the first human s one time step because

535
00:31:59,640 --> 00:32:03,200
expectations just equivalent to the first team at one time step straight so that means

536
00:32:03,200 --> 00:32:04,390
that can limit

537
00:32:04,400 --> 00:32:09,360
the range of the sound from t primality

538
00:32:09,380 --> 00:32:19,590
and then that's the definition of these it

539
00:32:28,920 --> 00:32:35,950
OK so you think about this

540
00:32:36,090 --> 00:32:39,880
one way to think about this is that kind of an unconstructive reduction of reinforcement

541
00:32:41,190 --> 00:32:45,360
it says that there exists some constant classification problems since the cell them well

542
00:32:45,370 --> 00:32:47,580
piece of reinforcement learning

543
00:32:47,970 --> 00:32:50,550
really tell you how to do it

544
00:32:52,440 --> 00:32:53,690
if you

545
00:32:53,700 --> 00:32:56,650
happen to have an optimal policy around at training time

546
00:32:56,690 --> 00:32:59,550
like maybe

547
00:32:59,580 --> 00:33:02,270
maybe alex smola is

548
00:33:02,390 --> 00:33:07,200
who started the machine learning summer schools knows how to get to the next

549
00:33:07,230 --> 00:33:11,560
so you can tell you how to get the next given your individual actions

550
00:33:15,140 --> 00:33:16,850
based upon your

551
00:33:16,890 --> 00:33:22,150
upon by his guidance you can derive a good policy

552
00:33:22,200 --> 00:33:24,120
the this this kind of approach

553
00:33:24,120 --> 00:33:26,140
before reaching the next

554
00:33:26,160 --> 00:33:31,280
OK and

555
00:33:33,500 --> 00:33:36,150
we need to think about what we can do in in kind of a constructive

556
00:33:36,150 --> 00:33:36,880
so this is

557
00:33:37,260 --> 00:33:38,050
this is

558
00:33:38,060 --> 00:33:43,120
it is maybe of inspirational says that there exists some constant the classification problems is

559
00:33:43,120 --> 00:33:47,200
the solved the of reinforcement learning but it we need to think about how to

560
00:33:47,200 --> 00:33:51,210
actually draw samples from these distributions nor two

561
00:33:51,230 --> 00:33:55,540
to construct this policy which will behave optimally

562
00:33:58,630 --> 00:34:00,270
i'm gonna go through

563
00:34:00,270 --> 00:34:02,960
a huge tricks people have

564
00:34:03,020 --> 00:34:05,800
people have thought about for trying to do this

565
00:34:07,440 --> 00:34:09,620
in general all this analysis

566
00:34:09,630 --> 00:34:11,940
it is going to apply to the fall

567
00:34:11,960 --> 00:34:14,450
reinforcement learning problem

568
00:34:14,500 --> 00:34:18,670
which is is it's good compared to what i talked about earlier right

569
00:34:18,700 --> 00:34:23,310
the earlier is talking about markov decision processes which is a very limited form

570
00:34:23,370 --> 00:34:28,830
however we can rely upon having a generative model for that trace my so the

571
00:34:28,830 --> 00:34:31,080
trace model

572
00:34:31,100 --> 00:34:36,800
what you do is you you just you're behaving you're acting directing and acting is

573
00:34:37,150 --> 00:34:38,820
a stream of experience

574
00:34:38,860 --> 00:34:40,310
the generative model

575
00:34:40,320 --> 00:34:44,350
you have got a more powerful mechanism for interacting with the world

576
00:34:44,360 --> 00:34:45,450
you can now

577
00:34:48,050 --> 00:34:52,140
you can now say suppose that i have some history of observations

578
00:34:52,240 --> 00:34:55,450
it takes an action then what happens next

579
00:34:55,460 --> 00:34:56,290
and you can say

580
00:34:56,290 --> 00:35:00,400
suppose i have the same history of interactions take some other action

581
00:35:00,400 --> 00:35:01,180
for the

582
00:35:01,230 --> 00:35:07,480
the general case you going get through square

583
00:35:07,520 --> 00:35:10,070
OK so

584
00:35:10,080 --> 00:35:14,360
so that that's that's the basic budget and hopefully it's simple enough that

585
00:35:14,420 --> 00:35:15,900
it's very clear now

586
00:35:15,910 --> 00:35:19,620
the bank several people have mentioned that there are sort of the limitations of one

587
00:35:19,620 --> 00:35:20,810
sort or another

588
00:35:22,610 --> 00:35:26,300
so the question is what's wrong with this

589
00:35:27,530 --> 00:35:29,210
what you mention out

590
00:35:29,410 --> 00:35:36,380
so you can reuse the previous state seems kind of weird

591
00:35:37,080 --> 00:35:39,990
we have the samples from this set the recursion and we just ignore them for

592
00:35:39,990 --> 00:35:41,410
the next recursion

593
00:35:41,610 --> 00:35:43,860
it seems to baltimore

594
00:35:44,030 --> 00:35:46,050
potentially suboptimal

595
00:35:46,060 --> 00:36:00,390
what else is wrong

596
00:36:00,410 --> 00:36:06,740
the computation is certainly as pretty substantial issues

597
00:36:06,890 --> 00:36:10,080
because we're sort of enumerating

598
00:36:10,120 --> 00:36:13,330
passes checking the upper and lower bound and

599
00:36:13,370 --> 00:36:14,810
this is a

600
00:36:14,830 --> 00:36:16,080
one issue

601
00:36:16,120 --> 00:36:18,030
what else is wrong

602
00:36:18,040 --> 00:36:25,060
it's actually amazing how many things are on the the side of the several more

603
00:36:36,340 --> 00:36:38,580
this is something that i had from you entirely

604
00:36:38,600 --> 00:36:40,810
which is the label complexity

605
00:36:41,760 --> 00:36:44,100
you have infinite labelled data

606
00:36:44,110 --> 00:36:46,600
to measure the disagreement

607
00:36:46,630 --> 00:36:49,640
regions size to infinite precision

608
00:36:49,650 --> 00:36:53,340
we can do is unlabelled data is just checking to see if there exist two

609
00:36:53,340 --> 00:36:55,290
parties disagree

610
00:36:55,290 --> 00:36:57,650
but what we can rely on

611
00:36:57,680 --> 00:37:00,060
an arbitrary precision

612
00:37:02,350 --> 00:37:04,430
computational issues

613
00:37:07,390 --> 00:37:12,370
several different able complexity issues we're throwing away examples is i pointed out

614
00:37:12,380 --> 00:37:18,130
what we didn't do very well when we want to

615
00:37:18,190 --> 00:37:18,870
have the

616
00:37:19,050 --> 00:37:20,930
so on

617
00:37:22,100 --> 00:37:24,340
with the best which is smaller than the

618
00:37:26,530 --> 00:37:27,900
it's kind of weird

619
00:37:28,300 --> 00:37:34,520
and then of course if you care about solving real-world problems often you care about

620
00:37:34,520 --> 00:37:36,690
things other than zero one loss

621
00:37:36,710 --> 00:37:39,140
the sort of the whole space of things

622
00:37:39,150 --> 00:37:46,850
which were not even addressing

623
00:37:46,870 --> 00:37:49,070
OK so to the that the

624
00:37:49,070 --> 00:37:53,830
almost all of these have been addressed

625
00:37:53,840 --> 00:37:59,090
so is the this paper is about generating generalization of power

626
00:38:00,600 --> 00:38:02,660
not notable case

627
00:38:02,690 --> 00:38:06,730
and there they deal with the label complexity problem it partly due to the computational

628
00:38:07,710 --> 00:38:12,460
this paper is a seat belt also deal the computational problems in a different way

629
00:38:15,320 --> 00:38:18,750
the the first label complexity problem turned out to be unreliable

630
00:38:18,780 --> 00:38:20,650
you can get rid of it

631
00:38:20,670 --> 00:38:23,270
under current assumptions

632
00:38:23,410 --> 00:38:28,870
so the lower bound saying you just can't get along the and so on are

633
00:38:28,870 --> 00:38:30,330
small compared to here

634
00:38:30,340 --> 00:38:34,890
human married

635
00:38:36,730 --> 00:38:40,610
the derivation of cal uses all the labelled examples

636
00:38:40,630 --> 00:38:42,080
which is nice

637
00:38:45,800 --> 00:38:49,630
one one issue which comes up mention is these

638
00:38:50,060 --> 00:38:53,140
upper bounds and lower bounds are often quite loose

639
00:38:53,930 --> 00:39:00,620
and people try to make algorithms based upon these bounds of it's it's not

640
00:39:00,630 --> 00:39:05,620
i mean you worked on the slide of course some work and

641
00:39:06,700 --> 00:39:10,910
you know something we succeed but to some extent they are still on the loose

642
00:39:11,460 --> 00:39:13,310
and we would like to do

643
00:39:13,310 --> 00:39:17,790
we mean what it means that we very pessimistic as for more labels into really

644
00:39:35,300 --> 00:39:39,210
i don't know about the model

645
00:39:39,230 --> 00:39:41,410
maybe something like that could work but

646
00:39:45,730 --> 00:39:47,670
OK so

647
00:39:47,680 --> 00:39:49,030
one way to

648
00:39:49,030 --> 00:39:53,600
deal with this what the signal in some sense the the that you can use

649
00:39:53,600 --> 00:39:55,080
importance weights

650
00:39:55,090 --> 00:39:58,410
and you can you can play games of importance weights you say the samples much

651
00:39:58,410 --> 00:40:02,610
more important than that one because i had a very low probability of a single

652
00:40:04,020 --> 00:40:07,620
and that will help address the smoothness and these bounds

653
00:40:07,670 --> 00:40:10,330
and then you can also you can also deal with

654
00:40:10,340 --> 00:40:13,880
other loss functions

655
00:40:14,040 --> 00:40:17,090
OK so

656
00:40:17,100 --> 00:40:22,230
you've already seen a calcined rhythm there's a generalization of it involves more analysis for

657
00:40:22,250 --> 00:40:24,090
nonseparable case

658
00:40:24,100 --> 00:40:26,230
so what i'm going to do is show you

659
00:40:26,250 --> 00:40:30,030
we discussed this this newspaper which uses

660
00:40:30,050 --> 00:40:33,330
the importance weights

661
00:40:33,380 --> 00:40:39,500
o but i guess first necessary about the lower bound

662
00:40:39,540 --> 00:40:40,410
OK so

663
00:40:40,440 --> 00:40:44,830
money had a lower bound which which we generalize a little bit so that had

664
00:40:45,030 --> 00:40:46,750
the VC dimension

665
00:40:46,750 --> 00:40:49,650
because the basic claim is

666
00:40:49,680 --> 00:40:53,140
for every hypothesis space with some VC dimension

667
00:40:57,120 --> 00:41:01,150
amount of regret and what you said about these

668
00:41:01,160 --> 00:41:04,570
for every minimum error

669
00:41:06,570 --> 00:41:09,190
among the regret is less than

670
00:41:09,240 --> 00:41:11,980
the minimum area divided by two

671
00:41:12,450 --> 00:41:15,650
you can construct a distribution d

672
00:41:15,710 --> 00:41:19,360
so is that all active learning algorithms require

673
00:41:19,380 --> 00:41:20,600
the times

674
00:41:20,600 --> 00:41:22,930
new squared over epsilon square

675
00:41:22,990 --> 00:41:24,490
labelled samples

676
00:41:25,280 --> 00:41:27,460
six you probably on with

677
00:41:27,520 --> 00:41:29,690
with reasonable probability

678
00:41:29,810 --> 00:41:37,510
OK so

679
00:41:37,540 --> 00:41:41,730
so this is to get an occasion

680
00:41:46,320 --> 00:41:48,580
there was the these calloused rhythms

681
00:41:48,600 --> 00:41:50,150
and one the claims

682
00:41:50,170 --> 00:41:54,730
for the non several were nonseparable version of the cap

683
00:41:54,730 --> 00:41:56,820
two thousand seven

684
00:41:56,880 --> 00:41:58,170
what is

685
00:41:58,210 --> 00:42:01,500
done most often is defined sliding windows

686
00:42:01,550 --> 00:42:03,710
last three hours

687
00:42:03,730 --> 00:42:09,000
and you can define landmark windows from september for first

688
00:42:09,000 --> 00:42:10,400
up to to now

689
00:42:10,590 --> 00:42:13,690
the window specification

690
00:42:13,840 --> 00:42:14,880
can be done

691
00:42:14,900 --> 00:42:17,670
i with physical type

692
00:42:17,710 --> 00:42:18,710
with the data

693
00:42:18,730 --> 00:42:20,750
we have logical type

694
00:42:20,800 --> 00:42:24,020
number of items

695
00:42:24,070 --> 00:42:25,480
so usually when

696
00:42:25,480 --> 00:42:28,900
you want to queries three new defined window on the

697
00:42:28,940 --> 00:42:32,650
and the second thing you have to do when you queries three

698
00:42:32,750 --> 00:42:36,300
it is to define when you want to have the reserve

699
00:42:36,340 --> 00:42:38,770
do you want to have the results

700
00:42:38,820 --> 00:42:44,340
every time one element and has one three or you want to have the reason

701
00:42:44,520 --> 00:42:47,380
that some regular intervals so this is the

702
00:42:47,820 --> 00:42:50,130
we fresh and refreshing

703
00:42:51,320 --> 00:42:54,380
so the rate of producing research

704
00:42:54,400 --> 00:42:58,380
every ten items or every minute every hour

705
00:43:00,300 --> 00:43:03,570
so here is an example of a sliding window

706
00:43:03,590 --> 00:43:05,650
so we suppose that

707
00:43:05,670 --> 00:43:08,280
this is the current time

708
00:43:08,300 --> 00:43:15,440
we have defined window to be the last three hours so we provide research

709
00:43:15,460 --> 00:43:20,230
and if there refreshment refreshment time is this

710
00:43:20,250 --> 00:43:22,690
then at

711
00:43:23,230 --> 00:43:29,540
plus the refreshment time we will provide the new uris but

712
00:43:29,590 --> 00:43:32,670
for this period which has been like

713
00:43:39,270 --> 00:43:41,570
now we know everything about

714
00:43:41,610 --> 00:43:44,770
structure data streams

715
00:43:46,210 --> 00:43:49,690
and we we can now talk about

716
00:43:49,690 --> 00:43:52,800
data stream management systems

717
00:43:54,020 --> 00:43:58,440
for data stream management systems

718
00:43:58,440 --> 00:44:03,020
and the outline will be the following so there will be two parts of the

719
00:44:03,020 --> 00:44:06,670
user point of view how do we use this system

720
00:44:06,690 --> 00:44:08,250
and the

721
00:44:08,270 --> 00:44:12,000
computer scientist point of view how is it

722
00:44:13,880 --> 00:44:15,520
and then i will finish by

723
00:44:15,540 --> 00:44:17,210
giving names of

724
00:44:17,380 --> 00:44:22,500
several existing system

725
00:44:22,550 --> 00:44:26,340
so the the definition of a DSMS

726
00:44:26,340 --> 00:44:31,320
it can be done in contrast with database management system

727
00:44:31,340 --> 00:44:36,050
in database management systems you have the data model or you can put what you

728
00:44:36,050 --> 00:44:40,800
want to know the relations in the database you have the structure

729
00:44:40,840 --> 00:44:46,300
data is stored in the on this and you can access data with language and

730
00:44:46,300 --> 00:44:49,210
usually the extra language

731
00:44:49,230 --> 00:44:53,400
and if ten shows you that you have good performance

732
00:44:53,400 --> 00:44:57,500
good response time even with large volume of data

733
00:44:57,520 --> 00:45:00,540
data stream management system

734
00:45:00,540 --> 00:45:03,730
we should be we provided data model

735
00:45:03,750 --> 00:45:09,480
it's going to to provide the same doing data model as a standard database management

736
00:45:11,320 --> 00:45:13,880
the between structure

737
00:45:13,940 --> 00:45:18,090
it's going to be able to store

738
00:45:18,150 --> 00:45:19,650
then the relation

739
00:45:19,670 --> 00:45:22,020
which are permanent

740
00:45:23,320 --> 00:45:28,730
these systems will be able to process streams on the fly

741
00:45:28,750 --> 00:45:32,320
i mean without storing all data arriving in the

742
00:45:32,380 --> 00:45:33,750
in the history

743
00:45:33,770 --> 00:45:35,320
they will provide

744
00:45:35,340 --> 00:45:40,480
and this fear language SQL like language which has been extended

745
00:45:41,230 --> 00:45:46,300
applied to streams and especially by

746
00:45:46,320 --> 00:45:48,090
windowing features

747
00:45:48,110 --> 00:45:53,230
the added to the standard SQL language

748
00:45:53,250 --> 00:45:54,940
we will see that

749
00:45:54,960 --> 00:45:56,690
queries some streams

750
00:45:56,710 --> 00:46:00,280
i will not be the same as querying

751
00:46:00,300 --> 00:46:01,750
prominent data

752
00:46:02,460 --> 00:46:10,110
a new concept has been introduced this is the concept of continuous queries

753
00:46:10,130 --> 00:46:18,190
in the data stream management systems you will find some tools for capturing input streams

754
00:46:18,210 --> 00:46:20,270
so usually input streams

755
00:46:20,280 --> 00:46:21,610
arrive in

756
00:46:22,550 --> 00:46:25,630
in internet ports in machine

757
00:46:25,650 --> 00:46:30,860
so the a data stream management systems provides connectors

758
00:46:31,590 --> 00:46:36,190
o ports and ways to decode the structural

759
00:46:36,210 --> 00:46:39,360
of data arriving in football

760
00:46:39,380 --> 00:46:45,070
into the structure which has been defined for the stream in the system

761
00:46:46,650 --> 00:46:48,110
it matters

762
00:46:48,130 --> 00:46:51,280
data stream management system as given

763
00:46:51,300 --> 00:46:53,400
good performance

764
00:46:53,420 --> 00:46:57,300
that's a demise in computer resources

765
00:46:57,320 --> 00:46:59,270
in the case of one three

766
00:46:59,300 --> 00:47:04,750
several streams several queries on several streams

767
00:47:04,750 --> 00:47:07,170
and last thing process you can

768
00:47:08,690 --> 00:47:16,520
is the ability to face variations in arrival rates without any crash so this would

769
00:47:16,520 --> 00:47:19,090
be important point because as you

770
00:47:19,130 --> 00:47:19,940
i said

771
00:47:19,940 --> 00:47:23,000
in the first slide we can not

772
00:47:23,050 --> 00:47:24,900
and control

773
00:47:25,480 --> 00:47:29,280
arrival rate of breakers in the system

774
00:47:34,860 --> 00:47:35,840
now we

775
00:47:35,860 --> 00:47:37,730
i'm going to talk about

776
00:47:37,770 --> 00:47:41,280
data stream management system data model or

777
00:47:41,340 --> 00:47:43,440
so in fact stand

778
00:47:43,440 --> 00:47:45,840
you will find

779
00:47:45,840 --> 00:47:51,950
i could you know i could run myself but then they would look horrible basically

780
00:47:51,950 --> 00:47:52,870
you know

781
00:47:52,890 --> 00:47:53,960
it's an icon

782
00:47:53,980 --> 00:47:55,700
thank you for your so

783
00:47:55,700 --> 00:47:59,020
right so to but in particular what we're trying to do here we're trying to

784
00:47:59,020 --> 00:48:03,310
represent a mapping from state and action to a probability distribution over states but were

785
00:48:03,310 --> 00:48:07,450
uncertain about the probability distribution so we need a distribution over the distribution and that's

786
00:48:07,450 --> 00:48:08,870
what they're actually does for us

787
00:48:08,910 --> 00:48:13,200
keeping track of this sort of stuff is actually keeping the posterior is actually very

788
00:48:13,200 --> 00:48:18,370
nice it just counts so that's good work by keeping this count we can recover

789
00:48:18,370 --> 00:48:21,930
how much uncertainty is still at any point in the learning

790
00:48:21,950 --> 00:48:26,330
which is good unlike example the distributions over an infinite set

791
00:48:26,350 --> 00:48:29,890
which you know turned out to be sort of nice you can still keep these

792
00:48:29,890 --> 00:48:34,430
probabilities but imagine doing that planning things that you should before we're now instead of

793
00:48:34,430 --> 00:48:37,580
having that little vector over

794
00:48:37,620 --> 00:48:40,600
the two numbers well i guess you can still keep it you can use either

795
00:48:40,600 --> 00:48:44,330
side represent as well so you can still plan out in the posterior using the

796
00:48:44,350 --> 00:48:46,000
dear so representation

797
00:48:46,020 --> 00:48:48,140
OK so

798
00:48:48,140 --> 00:48:51,730
that doesn't change things all that much and there's a bunch of bayesian reinforcement learning

799
00:48:51,730 --> 00:48:57,410
papers that that hard use this representation of the posterior

800
00:48:57,410 --> 00:49:01,290
there it it really hard to plan and the resulting space

801
00:49:01,310 --> 00:49:02,620
the best

802
00:49:02,620 --> 00:49:07,000
that i've seen out there is a relatively new to a couple years ago

803
00:49:07,060 --> 00:49:09,230
pascal poupart et all

804
00:49:09,290 --> 00:49:11,720
i have an when called beetle

805
00:49:11,720 --> 00:49:15,540
which tells all talk about a lot of people have tried to approximate the result

806
00:49:15,560 --> 00:49:17,790
of the value function for the resulting

807
00:49:17,810 --> 00:49:21,870
infinite dimensional MDP in various ways but this is this one seems to be the

808
00:49:21,870 --> 00:49:25,950
most successful so far so they basically use some of the latest ideas from solving

809
00:49:25,960 --> 00:49:29,830
continuous partially observable markov decision processes

810
00:49:29,830 --> 00:49:34,270
so if the if in the discrete model that i should in the beginning with

811
00:49:34,270 --> 00:49:37,960
the tree and so forth really what we're trying to do is planning to pompey

812
00:49:37,980 --> 00:49:42,620
we we had uncertain information about our true state what's the probability with which model

813
00:49:42,620 --> 00:49:46,180
we really and and we don't know so we have a distribution over that planning

814
00:49:46,180 --> 00:49:47,960
the probability space is what

815
00:49:48,000 --> 00:49:52,480
it what upon the model is about but since now we're working in a continuous

816
00:49:54,480 --> 00:49:58,790
the parameters of probability and we probably over the probability are actually talking about it

817
00:49:58,790 --> 00:50:01,310
you know to high dimensional continuous

818
00:50:01,310 --> 00:50:06,600
partially observable MDP soap on regular police family basically impossible to solve but then you

819
00:50:06,600 --> 00:50:11,750
can make and continuous

820
00:50:12,790 --> 00:50:17,600
what is a continuous over right so so

821
00:50:17,620 --> 00:50:23,640
in the standard palm BP have the discrete distribution over the like the world in

822
00:50:23,640 --> 00:50:26,830
the case that we've got its that's not in the real power and punjabi we

823
00:50:26,830 --> 00:50:30,910
do these estates of the world in this example we're actually talking about models which

824
00:50:30,910 --> 00:50:33,910
MDP or we can we don't know which MDP were and so we have a

825
00:50:33,910 --> 00:50:36,600
distribution over which MDP we might here

826
00:50:36,680 --> 00:50:39,980
one that's just the distribution over models but

827
00:50:40,100 --> 00:50:43,450
but that's only if you have a discrete set of models there's model a b

828
00:50:43,450 --> 00:50:49,180
c and d if if the general space of mdps are parameterized by continuous quantities

829
00:50:49,180 --> 00:50:52,480
which to say well what's the probability if i'm in the state i go to

830
00:50:52,480 --> 00:50:55,100
that state said writing down to model

831
00:50:55,120 --> 00:50:58,330
an MDP model requires writing down a bunch of real numbers

832
00:50:58,430 --> 00:50:59,600
and so now

833
00:50:59,620 --> 00:51:02,910
our uncertainty and that is the distribution over the real numbers

834
00:51:02,950 --> 00:51:05,870
so that's the sense in which it ends have been continuous

835
00:51:05,890 --> 00:51:15,500
there are i guess we're representing things dear size it's not we just you basically

836
00:51:15,500 --> 00:51:17,370
have like an m squared times

837
00:51:17,390 --> 00:51:20,660
a kind k kind of thing

838
00:51:20,680 --> 00:51:24,790
and i think we're keeping count anyway they're not actually continuous values that just

839
00:51:24,810 --> 00:51:28,370
there's numbers but the numbers without bound infinity

840
00:51:28,390 --> 00:51:31,930
it's still an infinite MDP just i guess the discrete infinite MDP

841
00:51:31,980 --> 00:51:33,250
countably infinite

842
00:51:33,290 --> 00:51:36,290
the head of the back

843
00:51:40,720 --> 00:51:43,750
it seems like you could use entropy is

844
00:51:43,770 --> 00:51:50,460
entropy is i tell which world tells how confused we are about world around them

845
00:51:50,480 --> 00:51:51,930
the entropy of one

846
00:51:59,450 --> 00:52:03,250
one of the actions decrease the entropy dramatically that we're saying

847
00:52:03,270 --> 00:52:06,020
so so right so a good heuristic is often

848
00:52:06,040 --> 00:52:09,540
if you've got time for it take some actions to reduce the entropy

849
00:52:09,600 --> 00:52:13,960
it's a heuristic it actually solve this problem but it but it is useful anyway

850
00:52:13,960 --> 00:52:16,960
and symmetry is very nice

851
00:52:16,980 --> 00:52:22,040
for calculations because when you have this symmetry can simplify your problem and divided by

852
00:52:22,040 --> 00:52:25,580
two you get only had and then you can use the other one

853
00:52:25,610 --> 00:52:28,360
by symmetry so if you break everything

854
00:52:28,380 --> 00:52:33,000
you have to calculate everything and it's quite difficult

855
00:52:33,040 --> 00:52:36,170
so these are one dimensional here

856
00:52:36,170 --> 00:52:40,090
but i presented but you can constraint as many called you can put as many

857
00:52:40,090 --> 00:52:41,670
constraints as you want

858
00:52:41,670 --> 00:52:46,770
this is for example of potential energy landscape for uranium two hundred thirty eight

859
00:52:46,770 --> 00:52:49,170
you have here is very city here in

860
00:52:49,170 --> 00:52:50,750
treat this elongation

861
00:52:50,750 --> 00:52:54,060
and here we previous this asymmetry

862
00:52:54,080 --> 00:53:00,850
you see here is the ground state for years isomeric well this symmetry here leading

863
00:53:00,880 --> 00:53:03,560
to two symmetric fragments

864
00:53:03,560 --> 00:53:06,850
but you see here that you have another value here

865
00:53:06,860 --> 00:53:11,060
that is developing and it's even lower in energy than it will end with a

866
00:53:11,060 --> 00:53:13,210
small from big one

867
00:53:13,230 --> 00:53:19,210
so you see here that we these degrees potential energy landscape

868
00:53:19,230 --> 00:53:20,610
you can explain

869
00:53:20,630 --> 00:53:23,500
while the nucleus is going to have

870
00:53:23,540 --> 00:53:30,270
asymmetric fission was a small fragmented way it's because of that results so it's lowering

871
00:53:30,270 --> 00:53:32,080
energy so these

872
00:53:32,110 --> 00:53:37,060
but here is much more favor than this one

873
00:53:37,090 --> 00:53:38,230
so we it

874
00:53:39,540 --> 00:53:44,860
constrained the calculation you can explain many features that are sears

875
00:53:44,880 --> 00:53:47,480
in the UK

876
00:53:47,500 --> 00:53:52,170
of course when you have deformation you had this thing about is that the way

877
00:53:52,190 --> 00:53:57,960
so this is the story cities so this is an instance died schematic

878
00:53:58,020 --> 00:54:01,880
but this is very simply so you have generate c and then when you increase

879
00:54:01,880 --> 00:54:03,380
the formation of the

880
00:54:03,400 --> 00:54:09,000
then the liberals to go down so they completely make

881
00:54:09,020 --> 00:54:11,270
so it's magic numbers here

882
00:54:13,690 --> 00:54:18,480
explain that recently so the difference in energy between two level

883
00:54:18,500 --> 00:54:20,980
so they become negligible

884
00:54:21,000 --> 00:54:26,080
and sometimes you have new gaps that do appear so here not so

885
00:54:26,090 --> 00:54:29,170
but you have some time to get

886
00:54:30,270 --> 00:54:35,480
these are some real calculations from microscopic calculations populations four

887
00:54:35,500 --> 00:54:37,730
seven million one hundred fifty four

888
00:54:37,730 --> 00:54:41,630
and you see that here you have no doubt that do appear for

889
00:54:41,630 --> 00:54:44,770
so this is elongation some deformations

890
00:54:44,790 --> 00:54:50,290
so that's why tomorrow i will talk about exotic nuclei as search the search sorry

891
00:54:50,290 --> 00:54:52,730
for new

892
00:54:52,750 --> 00:54:55,860
quite magic nuclei so very stable nuclei

893
00:54:56,040 --> 00:54:58,060
due to these properties

894
00:54:59,460 --> 00:55:04,790
the spherical cap disappeared when you have some fun here you can find new gaps

895
00:55:04,790 --> 00:55:05,940
of new

896
00:55:05,960 --> 00:55:09,560
big stability for new nuclei

897
00:55:09,590 --> 00:55:10,630
of course

898
00:55:10,670 --> 00:55:15,850
with this deal with this mean field can go beyond introduce some exhibitions

899
00:55:15,920 --> 00:55:17,900
there are two types of approaches

900
00:55:17,920 --> 00:55:20,250
if you want to consider only these

901
00:55:20,270 --> 00:55:23,290
individual excitations so you promotes some nuclear

902
00:55:23,310 --> 00:55:24,810
it is or what is done

903
00:55:24,830 --> 00:55:26,610
is what we call a p

904
00:55:26,690 --> 00:55:28,650
you can introduce these

905
00:55:28,670 --> 00:55:31,500
large amplitude correlations so

906
00:55:31,520 --> 00:55:33,790
especially when you have constraint

907
00:55:34,360 --> 00:55:39,420
care it's very easy to introduce these vibrations

908
00:55:39,440 --> 00:55:45,230
so then you have access to to individual excitations and collective states

909
00:55:45,250 --> 00:55:50,000
and this is the results obtained with these the all mean field calculations

910
00:55:50,020 --> 00:55:55,290
where what is calculated is the first collective state rotational vibrational

911
00:55:55,290 --> 00:55:59,420
positive states and you see you have different states here

912
00:55:59,440 --> 00:56:03,560
which have been calculated

913
00:56:05,000 --> 00:56:09,110
no question why in the nuclear shape pertinent

914
00:56:10,250 --> 00:56:12,670
how can it help for

915
00:56:13,590 --> 00:56:16,330
comparison between theory and experiment

916
00:56:16,350 --> 00:56:19,440
it's because you have some more than others

917
00:56:19,460 --> 00:56:24,540
which explained that for a spherical nuclei you

918
00:56:24,580 --> 00:56:31,150
so second i cannot rotate because you don't have

919
00:56:31,170 --> 00:56:36,190
because of quantum mechanics you you need to have differently i too have excited states

920
00:56:36,190 --> 00:56:37,270
with rotations

921
00:56:37,290 --> 00:56:41,060
so it's very good media it's vibrational spectrum which is

922
00:56:41,130 --> 00:56:44,440
proportional to all these

923
00:56:44,460 --> 00:56:47,110
two these

924
00:56:47,150 --> 00:56:52,130
i'm government to sorry exhibition energy proportional to the angular momentum so you have equally

925
00:56:54,190 --> 00:56:55,790
for the former nuclei

926
00:56:55,810 --> 00:56:59,130
you have rotational spectra perpendicular to z

927
00:56:59,940 --> 00:57:04,540
to the axis of the main axis of the nucleus

928
00:57:04,590 --> 00:57:09,160
evolution axis and you have the spectra which is proportional to that so you have

929
00:57:09,160 --> 00:57:12,770
this because he's not equally spaced at all

930
00:57:12,770 --> 00:57:17,270
so if i take back to two examples of last time less course

931
00:57:17,350 --> 00:57:20,830
these were some i one hundred forty eight and gadolinium

932
00:57:20,850 --> 00:57:23,360
one hundred sixty you see that here

933
00:57:23,380 --> 00:57:26,150
pos and you you have equally spaced

934
00:57:27,330 --> 00:57:32,380
so these we'll be associated to spherical nuclei

935
00:57:32,380 --> 00:57:36,080
four gadolinium you have very nice professional so

936
00:57:36,110 --> 00:57:37,480
jim jeffries one

937
00:57:41,940 --> 00:57:44,270
this is these are

938
00:57:44,290 --> 00:57:48,020
the most famous example is very passionate about rational

939
00:57:48,040 --> 00:57:54,350
in many many nuclei you have something mixed between repression rotation and vibration

940
00:57:54,360 --> 00:57:58,710
but can help for example this is a summary of isotopic chain

941
00:57:58,750 --> 00:58:02,250
from one hundred forty eight so the one that presented before

942
00:58:02,270 --> 00:58:04,230
to one hundred fifty four

943
00:58:04,250 --> 00:58:07,610
and you see that you have to change between the

944
00:58:07,630 --> 00:58:12,420
vibrational spectra to repression picked from here

945
00:58:12,420 --> 00:58:14,060
in this case here

946
00:58:14,080 --> 00:58:15,360
and you see that

947
00:58:15,380 --> 00:58:16,540
with very

948
00:58:16,540 --> 00:58:21,080
you can reproduce here that it's very cool and then you have for this one

949
00:58:21,080 --> 00:58:22,230
hundred fifty

950
00:58:22,250 --> 00:58:24,980
the name which is a little bit different

951
00:58:24,980 --> 00:58:28,900
and deformations in creating really really crazy

952
00:58:28,900 --> 00:58:34,310
all these states are located in this way so there really are different of the

953
00:58:34,310 --> 00:58:36,830
correspond to fashion brand

954
00:58:36,880 --> 00:58:39,960
so all these effects are very

955
00:58:39,980 --> 00:58:44,810
this structure is very very important because imagine here

956
00:58:44,810 --> 00:58:48,960
you only ten six k and so you have

957
00:58:49,000 --> 00:58:53,480
around one hundred fifteen pounds chance sixty and you have something

958
00:58:53,500 --> 00:58:54,960
completely different

959
00:58:54,960 --> 00:58:59,470
to keep multiple parts trees for partial plans and they use the same knowledge in

960
00:58:59,470 --> 00:59:01,450
this bottom-up fashion

961
00:59:01,990 --> 00:59:05,870
what that is it what you believe in good ways of using the site but

962
00:59:05,870 --> 00:59:10,020
i said in in one kind of conjures up the idea of hierarchy that actually

963
00:59:10,020 --> 00:59:14,070
just kind of the knowledge the idea of grammar and if you're learning that you

964
00:59:14,070 --> 00:59:16,630
might want to learn hierarchical task networks

965
00:59:16,680 --> 00:59:20,420
when you look at lands and you see this regularity all the plan that people

966
00:59:20,420 --> 00:59:23,990
seem to be doing it and travel plans they don't seem to keep on making

967
00:59:23,990 --> 00:59:28,480
cycles there's just one more or less go straight and so you would generate hierarchical

968
00:59:28,480 --> 00:59:32,120
task networks which will ensure that such plans be

969
00:59:32,490 --> 00:59:36,300
you are you could try to learn the grammar of the correct solution solutions and

970
00:59:36,300 --> 00:59:39,140
that's not possible

971
00:59:39,170 --> 00:59:45,100
then i said is and the idea called shop which is some sort of how

972
00:59:45,100 --> 00:59:50,150
to cope plan but it's quite different from the sort of constant reduction schemas what

973
00:59:50,150 --> 00:59:52,810
they want to be doing is not only do this say

974
00:59:52,820 --> 00:59:54,180
not only do the same

975
00:59:54,210 --> 00:59:55,630
how do you

976
00:59:55,630 --> 00:59:59,880
he and commit action and between primitive actions they also value

977
00:59:59,890 --> 01:00:03,120
if you have this cool first applied this

978
01:00:03,130 --> 01:00:04,920
if it works then try this

979
01:00:04,930 --> 01:00:08,030
if it doesn't work then try this one so they giving and bad policy there's

980
01:00:08,030 --> 01:00:10,440
no search left anymore the search is now

981
01:00:10,490 --> 01:00:15,270
but in the program that the shop land right so here is actually a shop

982
01:00:15,420 --> 01:00:20,460
land about travelling and you know it is that is that it did not travel

983
01:00:20,460 --> 01:00:24,140
by bus only if going by that doesn't work

984
01:00:24,150 --> 01:00:28,130
so it's been set up front so you almost giving much more a

985
01:00:28,130 --> 01:00:32,670
much more of a controlled the the search for your neurons search for

986
01:00:32,700 --> 01:00:36,300
so think of this as you know if you don't like c like shop you

987
01:00:36,300 --> 01:00:39,070
write your plans planner in shock

988
01:00:40,520 --> 01:00:45,130
surprisingly shop planets do better than normal HTML elements

989
01:00:45,130 --> 01:00:51,140
because i'm giving even more probable planning to my so sampling the planet one

990
01:00:51,250 --> 01:00:55,530
if you don't like extended knowledge to

991
01:00:55,530 --> 01:00:58,370
that in some of which also can be learned it in remember the reason i'm

992
01:00:58,370 --> 01:01:01,500
telling you all this stuff is all the things that you can learn

993
01:01:01,520 --> 01:01:03,720
and you can start thinking in the back of your hand how do i learn

994
01:01:03,730 --> 01:01:07,020
this sort of knowledge you know plan examples

995
01:01:07,030 --> 01:01:11,040
OK so you could learn how to hierarchical schema you can learn invariance you could

996
01:01:11,040 --> 01:01:15,770
say well in this domain are you know if took a truck is the locations

997
01:01:15,770 --> 01:01:20,900
one and i cannot be that simply cannot be location this may not be present

998
01:01:20,900 --> 01:01:24,180
in the domain physics but you look at the the main i mean you look

999
01:01:24,180 --> 01:01:27,390
at the deductive closure of the domain and figured out that no such state at

1000
01:01:27,390 --> 01:01:31,220
what seems to be reachable from any legal state

1001
01:01:31,280 --> 01:01:35,000
OK you might want to learn and that's a useful thing because then in in

1002
01:01:35,230 --> 01:01:38,640
if you are doing planning especially in the backward direction

1003
01:01:38,660 --> 01:01:42,700
you might find that you have a state which actually is not it will never

1004
01:01:42,700 --> 01:01:46,910
get the initial state because it assuming that well i want because of this section

1005
01:01:46,910 --> 01:01:51,180
i want to be location one at the same time on the the location and

1006
01:01:51,180 --> 01:01:55,220
the search happily expanded by the end of that and take their child and you

1007
01:01:55,220 --> 01:01:58,030
know it's better to cut your losses and get revenge

1008
01:01:58,140 --> 01:01:59,750
and if you have this knowledge

1009
01:01:59,800 --> 01:02:04,630
can you can learn rules about optimality saying do not have package two

1010
01:02:04,770 --> 01:02:06,210
at the same location

1011
01:02:06,240 --> 01:02:10,350
you could learn i mean fact saying learn what people write in the knowledge base

1012
01:02:10,370 --> 01:02:15,410
planning black people actually write these things for each you can then apply these kinds

1013
01:02:15,410 --> 01:02:19,340
of simplifying assumptions saying one step back is lauded it should immediately move

1014
01:02:19,340 --> 01:02:23,190
nothing really makes that happen in the domain physics you know you can load the

1015
01:02:23,190 --> 01:02:24,430
park and

1016
01:02:24,450 --> 01:02:27,680
keep bi together that's what you know it it seems like a good idea to

1017
01:02:27,680 --> 01:02:28,530
move it

1018
01:02:28,570 --> 01:02:32,300
you know i remember that when you do this you are cracking down some solutions

1019
01:02:32,320 --> 01:02:35,420
we just solutions in the most recent syntactic sense if you do this you get

1020
01:02:35,430 --> 01:02:39,670
to the goal said but you don't like it similarly similar to my saying about

1021
01:02:39,680 --> 01:02:44,180
it and i really don't like walking presented even though that's a syntactically correct

1022
01:02:45,410 --> 01:02:49,960
so once again these kinds of you know actually knowledge communities

1023
01:02:49,970 --> 01:02:55,120
this is the other one that i mentioned earlier you could write rules about

1024
01:02:55,130 --> 01:02:58,060
state sequences that are not good

1025
01:02:58,070 --> 01:03:00,240
this sequence of that

1026
01:03:00,300 --> 01:03:05,350
in particular you really want to write articles about state sequences that are not

1027
01:03:05,400 --> 01:03:09,860
OK so that if you have progression plan and if you find that a particular

1028
01:03:09,940 --> 01:03:15,790
class particular brand seems to have a set of six states which winds up violating

1029
01:03:15,790 --> 01:03:17,210
a rule already

1030
01:03:17,230 --> 01:03:18,620
then you can

1031
01:03:18,630 --> 01:03:23,120
OK to do that in a sense you end up writing these rules about state

1032
01:03:23,120 --> 01:03:28,160
sequences in this case in a linear temporal logic habits for example here is and

1033
01:03:28,170 --> 01:03:32,290
that it is an example of this knowledge if you e on

1034
01:03:32,320 --> 01:03:36,220
then please wait until on citizenship

1035
01:03:36,220 --> 01:03:40,260
just increasing the size by looking at the bigger balls or is it because there

1036
01:03:40,280 --> 01:03:43,280
is some interesting property within those borders

1037
01:03:43,790 --> 01:03:47,790
and you not of justify prior

1038
01:03:47,810 --> 01:03:50,780
just get guidance

1039
01:03:55,730 --> 01:03:58,540
i mean this was a bit of repetition of

1040
01:03:58,560 --> 01:04:01,130
what i said before so maybe a

1041
01:04:03,170 --> 01:04:05,100
and you will see the slides if you want to

1042
01:04:05,140 --> 01:04:06,310
read them

1043
01:04:06,320 --> 01:04:11,490
i went into the kind of important messages and then we try to save

1044
01:04:11,500 --> 01:04:14,570
some time for questions and also to give you but the thing that they have

1045
01:04:14,570 --> 01:04:16,700
taken in the first lecture

1046
01:04:19,990 --> 01:04:25,120
OK so it's here i cannot help justify our prior what can we do

1047
01:04:25,140 --> 01:04:28,850
this is the problem right

1048
01:04:28,900 --> 01:04:29,810
the whole

1049
01:04:29,810 --> 01:04:32,710
the game of machine learning is

1050
01:04:32,790 --> 01:04:36,140
you have you are given the problem that you have to solve this is to

1051
01:04:36,140 --> 01:04:38,960
find a good classifier are good predictor

1052
01:04:39,100 --> 01:04:40,950
good model for that problem

1053
01:04:40,970 --> 01:04:43,370
and the whole idea is to find

1054
01:04:43,380 --> 01:04:47,640
i mean OK if you are bayesian you have to encode your knowledge input prior

1055
01:04:47,650 --> 01:04:50,390
and if you're not the bayesian it's a matter of you know

1056
01:04:50,500 --> 01:04:54,680
trying to figure out how best to use the knowledge that you

1057
01:04:54,760 --> 01:04:58,930
because of course if you don't know anything about the problem the problem can be

1058
01:04:59,860 --> 01:05:01,820
anything and

1059
01:05:01,870 --> 01:05:06,100
i don't know algorithm which allows you can try your

1060
01:05:06,100 --> 01:05:09,600
the favourite algorithm can fail SVM with gaussian kernel

1061
01:05:09,620 --> 01:05:15,670
and that might work but it does work it the reason why would work is

1062
01:05:16,840 --> 01:05:20,020
your problem magically has the right property of

1063
01:05:20,090 --> 01:05:20,980
you know

1064
01:05:21,070 --> 01:05:26,410
the target function most with respect to the euclidean distance because in the past colony

1065
01:05:26,410 --> 01:05:30,030
of being in this so if it happens that

1066
01:05:30,040 --> 01:05:36,040
the have this example again

1067
01:05:37,180 --> 01:05:40,170
if you happen to have a target function which is

1068
01:05:40,220 --> 01:05:41,360
very close

1069
01:05:41,410 --> 01:05:45,040
then of course if you use the gaussian kernel which is trying to and you

1070
01:05:45,050 --> 01:05:50,500
maximize the margin which means minimising the norm in the RKHS and of course

1071
01:05:50,500 --> 01:05:52,050
if you have the time

1072
01:05:52,090 --> 01:05:53,080
on that

1073
01:05:53,100 --> 01:05:54,420
target function

1074
01:05:54,530 --> 01:05:57,710
and you try to fit these data with this was function

1075
01:05:57,770 --> 01:05:59,580
you might get to the right

1076
01:05:59,600 --> 01:06:03,410
but what if your target function

1077
01:06:03,460 --> 01:06:07,820
is not small with respect to this euclidean distance but smooth with respect to just

1078
01:06:07,820 --> 01:06:09,630
another that you don't know

1079
01:06:09,640 --> 01:06:10,900
right let's say

1080
01:06:10,950 --> 01:06:17,410
that same year function is most with respect to the hamming distance

1081
01:06:17,430 --> 01:06:23,130
in the binary expansion

1082
01:06:23,180 --> 01:06:24,290
of the exit

1083
01:06:24,330 --> 01:06:25,850
right so you take x

1084
01:06:25,870 --> 01:06:30,040
you might be these binary representation

1085
01:06:30,050 --> 01:06:36,160
let's so you're between zero and one in particular the decimal expansion in binary

1086
01:06:36,190 --> 01:06:38,310
if you xis and then

1087
01:06:38,350 --> 01:06:44,020
you're looking at the hamming distance between two axes and that's your notion of similarity

1088
01:06:44,030 --> 01:06:46,890
and you have a target function which happens to be small

1089
01:06:46,910 --> 01:06:48,360
with respect to

1090
01:06:48,400 --> 01:06:51,500
this is the which means like if you change just one between this expansion is

1091
01:06:51,510 --> 01:06:53,140
a change in a bit

1092
01:06:53,860 --> 01:06:57,800
the times the value of the function does not change much

1093
01:06:57,900 --> 01:07:01,360
this is the case then you would have a target function which

1094
01:07:01,410 --> 01:07:02,160
you know

1095
01:07:02,170 --> 01:07:03,940
it looks like this

1096
01:07:03,980 --> 01:07:05,420
and practical

1097
01:07:05,470 --> 01:07:06,640
and i think

1098
01:07:06,690 --> 01:07:11,730
so of course if we use some data

1099
01:07:11,780 --> 01:07:14,970
from that

1100
01:07:15,030 --> 01:07:19,260
and you try to that was function you don't get the right thing

1101
01:07:19,270 --> 01:07:22,340
you just from that no matter

1102
01:07:22,350 --> 01:07:26,310
even the tomorrow almost they you get what i mean eventually you might want to

1103
01:07:26,310 --> 01:07:31,440
because you know the gaussians can approach arbitrary functions one the

1104
01:07:32,040 --> 01:07:34,520
i think of

1105
01:07:34,890 --> 01:07:37,700
whereas if you knew the right

1106
01:07:37,760 --> 01:07:39,760
the notion of was that if you knew this

1107
01:07:39,780 --> 01:07:42,910
that you should be looking at the binary expansion

1108
01:07:42,970 --> 01:07:46,390
and you can call that into your prior or in your kernel

1109
01:07:46,400 --> 01:07:50,120
then you might immediately with very few points find the right thing

1110
01:07:52,390 --> 01:07:56,590
how can you choose between those two how can you like if you come

1111
01:07:56,720 --> 01:07:59,980
one comes with the problem how can you tell where this is the i mean

1112
01:07:59,980 --> 01:08:02,370
which kernel is the best

1113
01:08:02,440 --> 01:08:06,840
well you can't apart from real trying things and playing around or

1114
01:08:06,850 --> 01:08:10,210
asking the expert about what he knows about the problem in this kind of thing

1115
01:08:10,360 --> 01:08:14,120
so that's why i think it's more an art than a science you can't really

1116
01:08:14,170 --> 01:08:16,010
prove anything or justified

1117
01:08:16,100 --> 01:08:18,130
i mean is some

1118
01:08:18,150 --> 01:08:21,080
bayesian with the if your rationale

1119
01:08:21,080 --> 01:08:23,320
you can do it in the sense that

1120
01:08:23,370 --> 01:08:24,500
rationale meaning

1121
01:08:24,510 --> 01:08:27,310
there is some kind of

1122
01:08:27,320 --> 01:08:29,500
maybe even a single way

1123
01:08:29,510 --> 01:08:31,020
to go from what you know

1124
01:08:32,120 --> 01:08:34,010
the writing the right prior

1125
01:08:34,020 --> 01:08:35,070
that's fine but

1126
01:08:35,080 --> 01:08:38,500
depends on what you know

1127
01:08:42,100 --> 01:08:47,160
maybe another way to say that is when you want to design an algorithm

1128
01:08:47,190 --> 01:08:48,330
two steps

1129
01:08:48,340 --> 01:08:51,080
the first one is to choose this preference for this

1130
01:08:51,120 --> 01:08:54,340
the notion of your prior similarity measure whatever

1131
01:08:56,190 --> 01:08:59,340
you know there is no guidance i mean is there's is no

1132
01:08:59,350 --> 01:09:04,250
true but you can get some guidance maybe you're looking at about that

1133
01:09:04,300 --> 01:09:07,460
but you know some people get very intrusion from

1134
01:09:07,520 --> 01:09:08,900
there're some people

1135
01:09:08,920 --> 01:09:12,250
i get the impression from practice and just that's fine

1136
01:09:12,610 --> 01:09:15,340
here is not only way

1137
01:09:15,390 --> 01:09:17,830
and then you have to exploit

1138
01:09:18,620 --> 01:09:24,080
reference for different and that's where you can maybe formalise a little bit more

1139
01:09:24,080 --> 01:09:25,780
and you know

1140
01:09:25,800 --> 01:09:28,780
improve results of the four OK within that's

1141
01:09:28,790 --> 01:09:30,120
common within that

1142
01:09:30,140 --> 01:09:34,290
choice that i made because i choose the margin as my criterion or because i

1143
01:09:34,290 --> 01:09:36,890
was the kernel of something that is

1144
01:09:36,900 --> 01:09:39,990
i can prove that what i'm doing is not too far off from what i

1145
01:09:39,990 --> 01:09:42,710
would do in if i am

1146
01:09:42,760 --> 01:09:46,600
seeing the whole distribution that's the only thing that can say and you can also

1147
01:09:46,840 --> 01:09:50,720
see how these things are different like what you're doing on the empirical data and

1148
01:09:50,720 --> 01:09:54,190
what you would do if you want full knowledge of the distribution and get from

1149
01:09:54,190 --> 01:09:56,510
their sum from

1150
01:09:58,640 --> 01:10:03,470
one thing which is important is also the computational cost i will not enter into

1151
01:10:03,520 --> 01:10:05,220
for this but

1152
01:10:05,230 --> 01:10:10,110
you know that that is really an important part of algorithm design you may

1153
01:10:10,110 --> 01:10:12,780
OK so where we want to go

1154
01:10:14,560 --> 01:10:16,470
OK when left

1155
01:10:16,800 --> 01:10:21,060
wait wait don't go up again you just said well

1156
01:10:38,190 --> 01:10:39,330
right right

1157
01:10:39,330 --> 01:10:45,340
right right it's gone but did you have thought before it was gone

1158
01:10:45,350 --> 01:10:48,820
there was only structure to this crazy crazy world

1159
01:10:48,830 --> 01:10:52,380
circle seem to be plant friends are really was that try again

1160
01:10:56,010 --> 01:10:58,690
right well that's what

1161
01:10:58,740 --> 01:11:02,610
that's a lot

1162
01:11:04,940 --> 01:11:09,950
you have to get

1163
01:11:09,970 --> 01:11:13,000
down left

1164
01:11:13,040 --> 01:11:19,110
left left left left OK we hit the wall once in that trial and in

1165
01:11:19,110 --> 01:11:22,120
fact i suspect will never hit the wall more than once again

1166
01:11:22,120 --> 01:11:28,630
people thinking up

1167
01:11:32,260 --> 01:11:36,210
left left left i get what do you think what's happening on that we don't

1168
01:11:36,210 --> 01:11:38,610
have any

1169
01:11:38,620 --> 01:11:43,650
but it was probably the diamonds yeah

1170
01:11:43,700 --> 01:11:45,790
you don't know just

1171
01:11:45,910 --> 01:11:50,480
i had left and right columns because they put

1172
01:11:50,490 --> 01:11:55,830
it's safe

1173
01:11:57,900 --> 01:11:59,690
right right

1174
01:12:00,740 --> 01:12:03,150
OK so that was diamonds

1175
01:12:03,240 --> 01:12:10,120
well anyway we do a bunch of a bunch of these

1176
01:12:10,130 --> 01:12:14,720
and it turns out you can't even kill them they come back anyway

1177
01:12:16,670 --> 01:12:20,820
there's ultimately this ten of them and the last in the last one and i

1178
01:12:20,820 --> 01:12:25,370
think classes is the is the wall and the reason i tell you that is

1179
01:12:25,370 --> 01:12:30,220
because i done this in the following setting is the same series of mazes for

1180
01:12:30,220 --> 01:12:34,130
all the different people i tested on but i very

1181
01:12:34,130 --> 01:12:37,780
the wall rules in the three worlds so in the first world and we did

1182
01:12:37,780 --> 01:12:42,570
world two by the experiment experiment one every screen it was always plus that was

1183
01:12:42,570 --> 01:12:43,720
the wall

1184
01:12:43,750 --> 01:12:48,120
OK so you can imagine that you would have picked up on that right in

1185
01:12:48,120 --> 01:12:49,100
experiment two

1186
01:12:49,100 --> 01:12:52,950
it was always one shape was either plug this state being plus shaping circle the

1187
01:12:52,950 --> 01:12:57,130
shaping diamond and once you get the hang of that you only make one mistake

1188
01:12:57,130 --> 01:13:00,460
right once you realize what's going on you hit that shape and then you say

1189
01:13:01,200 --> 01:13:04,110
there are lot of the project and you see world laws are in the new

1190
01:13:04,110 --> 01:13:07,630
pottery at least that's what people in my study didn't look like that's what you're

1191
01:13:07,630 --> 01:13:10,160
doing is well experiment three was different

1192
01:13:10,170 --> 01:13:15,120
there was always some feature so it's either fish that would one of the particular

1193
01:13:15,120 --> 01:13:20,010
shapes one of the particular background colors are one of the particular foreground colours

1194
01:13:20,030 --> 01:13:24,120
OK so there's really twenty seven different combinations of cell types but what people and

1195
01:13:24,120 --> 01:13:26,780
that one realizes that is always

1196
01:13:26,840 --> 01:13:31,730
is the background color a specific value back cover color for color the shape and

1197
01:13:31,730 --> 01:13:35,410
so they will make maybe two mistakes over bump into something and they was know

1198
01:13:35,410 --> 01:13:38,570
was that because the color or the shape or the background color the thing that

1199
01:13:38,570 --> 01:13:42,100
i just here and the hit something else and noticed that there was something in

1200
01:13:42,100 --> 01:13:44,560
common between the two of them and then they see all the walls and they

1201
01:13:45,550 --> 01:13:48,090
thank you

1202
01:13:48,140 --> 01:13:52,700
the new ball is

1203
01:13:53,980 --> 01:13:55,880
to get i'm not as far as i know

1204
01:13:55,880 --> 01:14:00,450
i did hear someone say oh yeah i know the problem it can the wall

1205
01:14:00,460 --> 01:14:04,570
can be a circle on this because circle you on the goals would be unreachable

1206
01:14:04,610 --> 01:14:08,160
all the all the color of the patterns that are used it was the same

1207
01:14:08,160 --> 01:14:12,570
ten may sixteen ten pictures for all the people so they ended up usually having

1208
01:14:12,570 --> 01:14:15,450
with this one solution to one side the other that's why this two goals by

1209
01:14:15,540 --> 01:14:19,370
way so just one more

1210
01:14:22,990 --> 01:14:28,660
sure we can i think i think if we do we could turn this into

1211
01:14:28,660 --> 01:14:32,420
a group brainstorming session to make a sellable video game but the point of this

1212
01:14:32,420 --> 01:14:37,470
was something a little bit different which was that the last maze for all three

1213
01:14:37,470 --> 01:14:40,840
groups of subjects now i say subject that's really unfair because it was like my

1214
01:14:40,840 --> 01:14:45,840
kids into my students it was i got moderate amount experimental psychologist so i don't

1215
01:14:45,840 --> 01:14:48,830
know how to actually do this but what i do is i run you know

1216
01:14:48,840 --> 01:14:53,250
my friends on it and then i make generalizations the talks which is kind of

1217
01:14:53,250 --> 01:14:58,000
like the a really bad psychologist but what would be interesting thing was the last

1218
01:14:58,000 --> 01:15:01,850
maze for all the subjects it was always plus that was the wall right through

1219
01:15:01,850 --> 01:15:05,260
because the experiment when people had to be plus experiment two was always the shape

1220
01:15:05,260 --> 01:15:09,070
and close shape experiment rate was always some feature that feature

1221
01:15:09,080 --> 01:15:12,680
and what would happen is they make depending on what they maze that they saw

1222
01:15:12,700 --> 01:15:16,700
leading up to the last one they make no mistakes one mistake or two mistakes

1223
01:15:16,700 --> 01:15:23,380
say maze same population of people probably but different prior experience coming into k so

1224
01:15:23,380 --> 01:15:26,140
they can ever changing either

1225
01:15:26,160 --> 01:15:30,080
exploration behaviour depending on the kinds of worlds that they were expecting

1226
01:15:30,120 --> 01:15:33,420
and and i think that's the real thing in the real world that we

1227
01:15:33,460 --> 01:15:38,210
learn from past experience and applied to new tasks right and this is partly why

1228
01:15:38,210 --> 01:15:42,890
i think yesterday the taxi worldwide so well with the with the group because we've

1229
01:15:42,890 --> 01:15:45,660
seen things like that and we know what kind of things to expect we didn't

1230
01:15:45,660 --> 01:15:48,960
want to bump into the walls yes sure it's possible and in fact i could

1231
01:15:48,960 --> 01:15:52,650
give new game where the walls were the goal and eventually we got around to

1232
01:15:52,650 --> 01:15:55,630
checking that but that was when you check first you're going to check the things

1233
01:15:55,630 --> 01:15:59,070
that made sense and fit with your experience so really what i'm talking about here

1234
01:15:59,070 --> 01:16:02,600
is a kind of transfer learning is this idea that you can learn from some

1235
01:16:02,600 --> 01:16:07,410
examples to do to do better or faster on some new test problem

1236
01:16:07,420 --> 01:16:11,040
we're test problem here is an entire reinforcement learning environment

1237
01:16:11,070 --> 01:16:15,300
all right now what's interesting is that the reason that i bring this up as

1238
01:16:15,300 --> 01:16:19,600
a way of transitioning out of the quick work is because we can quickly learn

1239
01:16:19,600 --> 01:16:22,510
any of these classes any these three classes we can make a quick learner that

1240
01:16:22,510 --> 01:16:26,460
takes advantage of the structure in that class and learns effectively and would do the

1241
01:16:26,460 --> 01:16:30,510
same thing is the people here that if the class was you always are going

1242
01:16:30,510 --> 01:16:34,440
to see a plus then it would make no mistakes the last means if that

1243
01:16:34,440 --> 01:16:38,110
the class was euro should pay attention to shape it would make one mistake on

1244
01:16:38,110 --> 01:16:41,100
the last me and if it was feature one that would maybe maybe two mistakes

1245
01:16:41,100 --> 01:16:43,310
in the last maze but the thing is

1246
01:16:43,970 --> 01:16:48,730
i didn't tell you which class which experiment you're running even though there was such

1247
01:16:48,730 --> 01:16:54,180
thing as experiment you're kind of putting this together as you went flexibly and beyond

1248
01:16:54,350 --> 01:16:59,200
based on your past experience with other experiments other silly video game thanks to a

1249
01:16:59,250 --> 01:17:03,200
quick can't do that take the quick stuff really requires that you specify the hypothesis

1250
01:17:03,200 --> 01:17:07,800
class and then learned efficiently with respect to the hardest thing to learn that class

1251
01:17:07,800 --> 01:17:11,380
so if we tell well it's possible that the features can matter

1252
01:17:11,410 --> 01:17:14,510
then it was it's going to learn like it's in the last class is not

1253
01:17:14,510 --> 01:17:18,570
going to take advantage of the fact that there's a shortcut to commit

1254
01:17:18,590 --> 01:17:20,750
so what is that sort of lead us to

1255
01:17:20,750 --> 01:17:24,310
the following content is provided under creative commons license

1256
01:17:24,330 --> 01:17:30,680
your support will help MIT opencourseware continue to offer high quality educational resources for free

1257
01:17:30,690 --> 01:17:35,390
to make a donation or to view additional materials from hundreds of MIT courses

1258
01:17:35,410 --> 01:17:40,590
this MIT opencourseware OCW that MIT that EDU

1259
01:17:40,590 --> 01:17:42,270
we're going to do

1260
01:17:42,270 --> 01:17:43,590
the first of three

1261
01:17:43,610 --> 01:17:46,020
lectures on the last topic

1262
01:17:46,070 --> 01:17:49,450
talk about phase diagram starting today one day

1263
01:17:49,460 --> 01:17:51,610
and wrap it up on wednesday

1264
01:17:52,540 --> 01:17:55,950
phase diagrams is related to the question of stability

1265
01:17:55,970 --> 01:17:58,800
and sustaining the solid state

1266
01:17:58,800 --> 01:18:00,660
we've talked about

1267
01:18:00,670 --> 01:18:02,520
the behavior of solids

1268
01:18:02,580 --> 01:18:03,660
and we have used

1269
01:18:03,660 --> 01:18:05,120
solid in order to

1270
01:18:05,130 --> 01:18:08,780
teach the rudiments of chemistry but today i want to talk about the conditions under

1271
01:18:08,780 --> 01:18:10,700
which solids are stable

1272
01:18:12,240 --> 01:18:14,280
but on what conditions do

1273
01:18:14,340 --> 01:18:20,160
solids remain stable when they become unstable this important industry for example

1274
01:18:20,170 --> 01:18:22,670
running a castrato making auto parts

1275
01:18:22,690 --> 01:18:26,660
you want to know what this little vacation temperature is for particular alloy

1276
01:18:26,670 --> 01:18:30,170
it's important in failure analysis something like

1277
01:18:30,200 --> 01:18:35,030
the fall of the world trade center looking at the metal specimens to determine what

1278
01:18:35,030 --> 01:18:36,860
was the mode of failure

1279
01:18:36,870 --> 01:18:42,700
the temperature excursions leave signature thermal signature indicative of the history of what happened to

1280
01:18:44,830 --> 01:18:49,950
by determining whether something went above a certain phase transformation temperature we can

1281
01:18:49,970 --> 01:18:52,270
retrace reconstruct the

1282
01:18:52,280 --> 01:18:54,190
the incident

1283
01:18:54,200 --> 01:18:55,530
you might say well

1284
01:18:56,420 --> 01:18:58,690
what's the big deal i mean if you look on

1285
01:18:58,800 --> 01:19:04,330
look up in the tables everybody knows water boils at one hundred degrees centigrade

1286
01:19:05,090 --> 01:19:06,580
it's not so simple

1287
01:19:06,590 --> 01:19:08,770
let's say decide to realize your

1288
01:19:08,780 --> 01:19:10,720
life's ambitions go

1289
01:19:10,720 --> 01:19:14,950
climb mount everest so you plot down ten thousand dollars to get a permit from

1290
01:19:14,950 --> 01:19:16,360
the nepalese government

1291
01:19:16,440 --> 01:19:19,280
the next thing you know you're base camp twenty thousand feet

1292
01:19:19,330 --> 01:19:21,480
you have a hankering for soft-boiled egg

1293
01:19:21,500 --> 01:19:23,140
jupiter campfire

1294
01:19:23,220 --> 01:19:25,780
but in the action

1295
01:19:25,800 --> 01:19:29,530
five minutes ten minutes fifteen minutes apart you want hard-boiled eggs

1296
01:19:29,540 --> 01:19:30,970
hard world eggs

1297
01:19:30,990 --> 01:19:33,060
after fifteen minutes to opening eggs

1298
01:19:33,100 --> 01:19:34,710
still running

1299
01:19:34,720 --> 01:19:38,430
you really steam twenty minutes twenty five minutes

1300
01:19:38,440 --> 01:19:40,250
every time the yoke

1301
01:19:40,310 --> 01:19:45,070
it's still running the white convertibility OK still running what's going on while we talk

1302
01:19:45,090 --> 01:19:46,720
about the nature in

1303
01:19:46,780 --> 01:19:50,240
the white teenagers is sixty five degrees centigrade

1304
01:19:51,070 --> 01:19:56,660
twenty thousand feet the atmospheric pressure is reduced to the point that water boils at

1305
01:19:57,630 --> 01:20:00,940
the nature in temperature of the egg yolk and so you can boil until the

1306
01:20:00,940 --> 01:20:04,370
end of time you always have soft-boiled eggs if you like so well that you

1307
01:20:04,370 --> 01:20:05,690
can screw up

1308
01:20:05,750 --> 01:20:07,460
you can screw up

1309
01:20:08,180 --> 01:20:10,280
so what do we know from this little

1310
01:20:10,290 --> 01:20:11,490
and don't

1311
01:20:11,530 --> 01:20:14,470
this little anecdote is that the boiling point

1312
01:20:14,490 --> 01:20:17,090
the boiling point is a function of

1313
01:20:18,060 --> 01:20:21,990
it's not that simple let's go to another place let's go let's go under the

1314
01:20:21,990 --> 01:20:26,000
hood of the car and my favorite places OK so here's what's going on under

1315
01:20:26,000 --> 01:20:27,120
the hood of the car

1316
01:20:27,160 --> 01:20:29,430
here you've got the engine

1317
01:20:29,470 --> 01:20:34,560
and inside the engine we combustion the combustion gives off a huge amount of he

1318
01:20:34,620 --> 01:20:36,910
we have to dissipate that heat so

1319
01:20:36,910 --> 01:20:41,530
we have water channels running through the engine and out

1320
01:20:41,560 --> 01:20:43,220
the radiator

1321
01:20:43,240 --> 01:20:49,220
and the radiator cools the water before recirculated thanks to the action of either fan

1322
01:20:49,240 --> 01:20:53,690
or wind or some kind of movement of air

1323
01:20:53,710 --> 01:20:58,570
and so what's the principle here the principle here is that inside

1324
01:20:58,590 --> 01:21:03,490
the radiator we've got a solid this is the the rat

1325
01:21:04,380 --> 01:21:05,970
it's probably made of

1326
01:21:06,000 --> 01:21:08,880
in the old days and the amount of copper but nowadays making more and more

1327
01:21:08,880 --> 01:21:12,630
the amount of aluminum and we have liquid which is the cool

1328
01:21:12,660 --> 01:21:14,600
this is the coolant this water

1329
01:21:14,620 --> 01:21:18,130
and we have a big delta t here

1330
01:21:18,150 --> 01:21:19,090
right this is

1331
01:21:19,090 --> 01:21:20,750
hi this is cool

1332
01:21:20,750 --> 01:21:23,900
so we have a heat flux going in this direction

1333
01:21:23,940 --> 01:21:25,940
and this is working great because

1334
01:21:26,030 --> 01:21:28,030
the density water is high

1335
01:21:28,030 --> 01:21:30,630
and so therefore it's able to transfer heat

1336
01:21:30,690 --> 01:21:33,090
very efficiently so this is good

1337
01:21:33,100 --> 01:21:37,050
this is really good what can happen if things go out of control things go

1338
01:21:37,060 --> 01:21:39,770
out of control and we start to boil

1339
01:21:39,820 --> 01:21:43,490
you know the boiling point of water is more or less one hundred degrees centigrade

1340
01:21:43,490 --> 01:21:47,440
were down here we're not at the base camp on mount everest here's what happens

1341
01:21:47,440 --> 01:21:52,240
when things start to go to control now we start to get gas bubbles

1342
01:21:52,270 --> 01:21:56,140
these are the gas bubbles associated with the boiling of water and now the heat

1343
01:21:56,140 --> 01:21:58,440
transfer between a gas

1344
01:21:58,450 --> 01:22:01,140
and solid

1345
01:22:01,150 --> 01:22:04,450
between gas and solid is very poor

1346
01:22:04,480 --> 01:22:07,940
think about it what's transferring the heat is the atoms in the atom density in

1347
01:22:07,940 --> 01:22:10,020
gasses spas so

1348
01:22:10,050 --> 01:22:15,370
gases are very poor heat transfer medium so we must avoid boiling if we get

1349
01:22:15,370 --> 01:22:19,820
boiling then we get on with thermal runaway situation because now we got boiling in

1350
01:22:19,820 --> 01:22:23,560
the first place because the water was too high but now we're doing less efficient

1351
01:22:23,560 --> 01:22:25,380
job of cooling

1352
01:22:25,480 --> 01:22:29,640
and it's going to get worse and worse and worse until finally bowl

1353
01:22:29,670 --> 01:22:33,350
so what i want to do is i want to get my liquid range

1354
01:22:33,360 --> 01:22:37,440
i want to tailor the properties of the cool i want to call it that

1355
01:22:37,440 --> 01:22:38,430
will be

1356
01:22:38,440 --> 01:22:40,380
boiled over proof

1357
01:22:40,390 --> 01:22:44,380
so if i could raise the boiling point that would make the car safer to

1358
01:22:44,380 --> 01:22:49,130
drive under extreme conditions typically what happens is your zooming down the highway on a

1359
01:22:49,130 --> 01:22:53,900
hot summer's day and there's some reason to come to an abrupt stop and then

1360
01:22:53,900 --> 01:22:56,210
there's all that heat to be dissipated

1361
01:22:56,240 --> 01:22:59,880
no more you get the benefit of the motion of the cart marriages relying on

1362
01:22:59,880 --> 01:23:03,800
the fan so what can we do if we go to the top of everest

1363
01:23:04,020 --> 01:23:05,920
and the pressure goes down

1364
01:23:05,940 --> 01:23:08,000
and the boiling point falls

1365
01:23:08,000 --> 01:23:12,080
could i raise the boiling point by applying more pressure

1366
01:23:12,100 --> 01:23:17,430
yeah let's put pressure cap on the radiator let's keep the contents the radiator high

1367
01:23:17,430 --> 01:23:19,240
and if you read the talk to say

1368
01:23:19,240 --> 01:23:27,810
it is in an environment that provides students with interactive simulations for understanding mathematical function

1369
01:23:27,810 --> 01:23:32,580
this is the sample activity in this interactive simulations where for instance here this is

1370
01:23:32,580 --> 01:23:36,230
designed to help students understand the relationship between the function graph

1371
01:23:36,280 --> 01:23:41,030
and the functional equation and is activity this tuning can move the function around on

1372
01:23:41,030 --> 01:23:41,990
the screen

1373
01:23:42,040 --> 01:23:46,750
and on the other hand let's see what the changes the corresponding changes in the

1374
01:23:46,750 --> 01:23:51,150
creation of the modern or can change parameters in equation and see how that changes

1375
01:23:51,410 --> 01:23:53,150
the graph of the function

1376
01:23:53,190 --> 01:23:55,760
and what we've been doing in the context of

1377
01:23:55,780 --> 01:24:00,600
this interaction we have been trying to experiment with different sources of information on the

1378
01:24:00,600 --> 01:24:04,610
students' behaviour during the interaction to see

1379
01:24:06,160 --> 01:24:10,030
sources of information can generate can allow user model

1380
01:24:10,060 --> 01:24:13,990
to generate better predictions of how much the student has learned

1381
01:24:14,000 --> 01:24:15,390
from the given

1382
01:24:15,390 --> 01:24:16,950
the interaction section

1383
01:24:17,080 --> 01:24:23,090
and we we have experimented with a in just using basic actions so for instance

1384
01:24:23,090 --> 01:24:26,700
in the case of example that i showed you are often the student what kind

1385
01:24:26,700 --> 01:24:32,100
of parameters justin was changing how the student was moving around was moving the graph

1386
01:24:32,380 --> 01:24:34,210
on the panel

1387
01:24:34,290 --> 01:24:38,750
and this this gave to the system so using just information on the number of

1388
01:24:38,750 --> 01:24:40,470
and coverage of

1389
01:24:40,520 --> 01:24:43,660
actions that this is the student perform with the simulation

1390
01:24:43,760 --> 01:24:45,980
and we also try to see

1391
01:24:45,980 --> 01:24:54,570
if we can infer self explanation behavior by using as information only for how

1392
01:24:54,590 --> 01:24:58,900
what is the time that the students spend in between two options so time is

1393
01:24:58,900 --> 01:25:00,090
a measure of

1394
01:25:00,190 --> 01:25:05,090
i think the students taking more time from students reasoning about the outcome of the

1395
01:25:05,090 --> 01:25:09,490
action issues presenting and so is getting more information out of the simulation

1396
01:25:09,610 --> 01:25:11,320
and we also try to see

1397
01:25:11,340 --> 01:25:14,710
if adding information on

1398
01:25:14,710 --> 01:25:20,010
the students gaze patterns and she is working with interactive simulation could

1399
01:25:20,010 --> 01:25:26,380
increase mobility model accuracy and in particular for instance we have tracked

1400
01:25:26,400 --> 01:25:31,460
we have given the model information on whether the student is shifting attention from one

1401
01:25:31,460 --> 01:25:36,710
town to the other when she's making changes in one issue should look at

1402
01:25:36,760 --> 01:25:39,550
and the corresponding effects in the second

1403
01:25:39,590 --> 01:25:43,750
and what we have seen we have

1404
01:25:43,750 --> 01:25:46,110
compared to three versions of the models

1405
01:25:47,010 --> 01:25:48,550
they just use action

1406
01:25:48,550 --> 01:25:53,190
as a measure of activity but doesn't even try to capture whether students of explaining

1407
01:25:53,190 --> 01:25:58,010
or not one that uses tries to capture what the distance of explaining or not

1408
01:25:58,010 --> 01:26:01,340
and so by using information just last time

1409
01:26:01,420 --> 01:26:06,420
between two actions and one model that uses information on both

1410
01:26:06,440 --> 01:26:10,590
i tracking and time to assess whether some explanation is happening

1411
01:26:12,300 --> 01:26:16,110
we can see that if you use information in both time and i tracking we

1412
01:26:16,110 --> 01:26:22,440
have a significance in this case significantly higher precision in capturing self explanation the system

1413
01:26:22,440 --> 01:26:26,300
is better at predicting whether the student self explaining during the interaction

1414
01:26:26,320 --> 01:26:32,250
and there's also generates better model accuracy in predicting students' learning at the end of

1415
01:26:32,260 --> 01:26:34,550
the interaction

1416
01:26:36,380 --> 01:26:38,420
following these results we

1417
01:26:38,440 --> 01:26:42,280
i want to continue using eye tracking information because we believe that provides a very

1418
01:26:42,280 --> 01:26:47,690
good window on student could mission during the interaction with this particular

1419
01:26:47,710 --> 01:26:49,340
learning activities

1420
01:26:49,650 --> 01:26:56,110
so what we discussed so far as the possibility of having tutors that can help

1421
01:26:56,110 --> 01:27:00,190
students improve their method mitochondria behaviors what about

1422
01:27:00,230 --> 01:27:01,860
the other

1423
01:27:01,860 --> 01:27:04,820
research direction that i mentioned that is the ability

1424
01:27:04,900 --> 01:27:10,780
having tutors that can also take into account student emotional reaction during the learning process

1425
01:27:10,780 --> 01:27:11,940
and react to that

1426
01:27:12,200 --> 01:27:17,800
so that's what i'm going to the second project that i'm going to describe and

1427
01:27:17,860 --> 01:27:19,820
the project involves

1428
01:27:20,630 --> 01:27:25,940
aspects in educational games so it also relates to the research direction of having more

1429
01:27:25,940 --> 01:27:31,050
open-ended activities that might involve interaction in this case with an indication

1430
01:27:31,130 --> 01:27:35,840
we focus on educational games because the occasional games

1431
01:27:35,920 --> 01:27:42,920
our educational games are systems that are designed to trigger learning by again like activities

1432
01:27:43,280 --> 01:27:48,400
and this is becoming a very popular trend in education because we all know that

1433
01:27:48,400 --> 01:27:51,940
electronic games are very engaging and the idea is to try to see if anyone

1434
01:27:51,940 --> 01:27:52,500
in this case

1435
01:27:53,170 --> 01:27:59,520
five to ten times better than a bad recommendation and user can stay away longer on the website it generates

1436
01:28:00,540 --> 01:28:03,880
in the case of this website which has roughly ten ten million clicks per day

1437
01:28:05,210 --> 01:28:05,850
lots of reading

1438
01:28:06,620 --> 01:28:09,170
so this is kind of big data with real time element

1439
01:28:10,230 --> 01:28:12,040
so response time needs to be like

1440
01:28:12,770 --> 01:28:16,540
twenty to fifty milliseconds everything what's above hundred millisecond s

1441
01:28:17,170 --> 01:28:17,810
not useful

1442
01:28:20,850 --> 01:28:25,250
others keep what we track about a user here i mean the profile of each clique is

1443
01:28:26,480 --> 01:28:30,980
morning star heavily in along many dimensions center so that's why this

1444
01:28:31,940 --> 01:28:32,980
this is gonna be great

1445
01:28:36,960 --> 01:28:38,230
yeah for some

1446
01:28:38,770 --> 01:28:40,400
people which are charities that we have also

1447
01:28:43,250 --> 01:28:43,670
this will be

1448
01:28:44,100 --> 01:28:47,290
the rest we have history we have a couple of other things i mean this

1449
01:28:47,290 --> 01:28:49,600
is what everybody's doing so what's available we don't

1450
01:28:52,080 --> 01:28:53,230
we don't get any external

1451
01:28:54,170 --> 01:28:57,420
sources everything just things which are available through the

1452
01:28:58,000 --> 01:29:01,750
cookie tracking ant what's available from the log files sense

1453
01:29:02,330 --> 01:29:02,900
now that's it

1454
01:29:07,710 --> 01:29:08,940
similar setting was let's say

1455
01:29:10,560 --> 01:29:12,230
what we did far analysis on

1456
01:29:13,330 --> 01:29:14,060
the new york times

1457
01:29:14,670 --> 01:29:19,850
again each from each user is being threatened then we would like to come up

1458
01:29:19,850 --> 01:29:22,400
with in real time with the segments of users which

1459
01:29:24,580 --> 01:29:29,350
which appear so have a time so advertising campaigns can be more efficient so

1460
01:29:29,790 --> 01:29:31,560
the scale for instance here would be

1461
01:29:32,750 --> 01:29:39,460
roughly fifty giga bytes of uncompressed log files per day is limited to two hundred million clicks per day

1462
01:29:40,620 --> 01:29:41,500
let's say around

1463
01:29:41,960 --> 01:29:47,730
forty six depends on the day unique users send seven thousand pages are visited one

1464
01:29:47,750 --> 01:29:50,140
hundred times for so this is the distribution of

1465
01:29:54,290 --> 01:29:56,020
another example from british telecom

1466
01:29:56,460 --> 01:29:57,730
so here we observe

1467
01:30:00,170 --> 01:30:03,730
british telecom telecommunication internet telecommunication network like

1468
01:30:04,330 --> 01:30:06,920
twenty five thousand devices and technologies this

1469
01:30:07,810 --> 01:30:09,270
problem we are solving is

1470
01:30:10,250 --> 01:30:12,440
if a problem happens in this network so

1471
01:30:13,040 --> 01:30:17,080
device starts complaining can then all devices which are connected to this device

1472
01:30:17,620 --> 01:30:21,370
starts complaining and this is network effect now the problem is to find the root

1473
01:30:21,370 --> 01:30:23,580
cause the root cause the device which is really

1474
01:30:24,400 --> 01:30:24,850
the source

1475
01:30:25,790 --> 01:30:26,710
of problem so

1476
01:30:27,100 --> 01:30:30,940
the root cause analysis this is kind of probabilistic reasoning setting so this needs to

1477
01:30:30,940 --> 01:30:32,710
happen like you know second after

1478
01:30:33,170 --> 01:30:34,020
the failure happens

1479
01:30:43,330 --> 01:30:47,560
it was

1480
01:30:52,120 --> 01:30:58,620
this is more like the generic question about the clout on the well depends how you organize the

1481
01:30:59,480 --> 01:31:02,770
problem uh sorry for solving problem on

1482
01:31:03,810 --> 01:31:08,960
amazon yasuhiro if you do this in the wrong way then this can be

1483
01:31:10,750 --> 01:31:11,690
of course bottleneck

1484
01:31:13,080 --> 01:31:15,920
really depends on the problem some problems are more friendly

1485
01:31:16,960 --> 01:31:17,500
four hour

1486
01:31:17,980 --> 01:31:19,730
clouds some are less friendly

1487
01:31:22,150 --> 01:31:23,850
okay but let's continue here so

1488
01:31:24,380 --> 01:31:28,830
for instance in this case we are solving this root cause analysis and prediction what

1489
01:31:28,830 --> 01:31:33,210
might and anomaly detection what values smell the smoke but there's no fire so

1490
01:31:33,750 --> 01:31:35,250
and did this happens all the three

1491
01:31:37,350 --> 01:31:39,900
and they have like ten two hundred alarms percent

1492
01:31:42,520 --> 01:31:43,830
another example which

1493
01:31:44,880 --> 01:31:48,670
so this is just the pictures so for instance know the big data which is

1494
01:31:48,880 --> 01:31:52,150
stored in the context of planet daytime this couple other projects

1495
01:31:53,120 --> 01:31:56,710
so we are monitoring all mainstream news stand

1496
01:31:57,250 --> 01:31:58,500
blocks as well

1497
01:31:58,900 --> 01:31:59,670
from the whole world

1498
01:32:01,080 --> 01:32:03,230
like seventy five thousand in news

1499
01:32:07,900 --> 01:32:12,540
if you open this page with and clean them or you will see how these articles are

1500
01:32:13,020 --> 01:32:15,310
just rolling so this is kind good source of

1501
01:32:16,520 --> 01:32:17,830
of big data fore

1502
01:32:19,210 --> 01:32:25,100
and now each article which we crawled from these are assessed feeds gets also semantically enriched

1503
01:32:25,600 --> 01:32:26,460
the next

1504
01:32:27,670 --> 01:32:32,400
then complete semantic enrichment natural language processing linking to melody

1505
01:32:33,040 --> 01:32:35,420
and then this is the input so we have stream of

1506
01:32:36,120 --> 01:32:37,270
semantically annotated

1507
01:32:39,080 --> 01:32:42,120
which was published just in the last minutes pretty much

1508
01:32:44,230 --> 01:32:45,210
okay let's keep this

1509
01:32:45,750 --> 01:32:46,830
visualization parts

1510
01:32:49,640 --> 01:32:55,080
maybe just give you an example mining so huge social networks so this is work from

1511
01:32:55,900 --> 01:32:58,250
utilities send the equal rights

1512
01:32:58,850 --> 01:32:59,690
from microsoft's

1513
01:33:04,020 --> 01:33:07,440
there analyzing one month of microsoft messenger

1514
01:33:08,060 --> 01:33:09,270
social network

1515
01:33:11,310 --> 01:33:17,870
so for each person basically there was the log file for each person who logged into amazon messenger

1516
01:33:19,790 --> 01:33:20,210
they knew

1517
01:33:21,690 --> 01:33:22,810
gender language

1518
01:33:25,770 --> 01:33:30,580
end now the questions which appear to be aware what the structure this

1519
01:33:31,000 --> 01:33:32,080
communication effort so

1520
01:33:32,690 --> 01:33:35,600
is a little bit longer presentation these but just give you a couple of

1521
01:33:36,080 --> 01:33:36,870
slides on this

1522
01:33:37,560 --> 01:33:39,870
this was hundred fifty giga bytes per day

1523
01:33:42,020 --> 01:33:42,810
uncompressed was

1524
01:33:43,710 --> 01:33:45,100
like four five times bigger

1525
01:33:45,100 --> 01:33:48,510
because we have sorted array not simply sorted array

1526
01:33:48,550 --> 01:33:53,920
but so that arabi the radius much closer together as possible

1527
01:33:53,930 --> 01:33:58,560
so it's always divided by inter while it's beautiful compression

1528
01:33:58,580 --> 01:34:01,750
because we have very very different distribution here

1529
01:34:01,760 --> 01:34:06,500
they're only one which featured four horror insertion

1530
01:34:06,510 --> 01:34:09,790
and the problem was distortion in that

1531
01:34:09,790 --> 01:34:13,170
if you look into this picture we can see that there

1532
01:34:13,220 --> 01:34:18,140
it's full have information and we don't have placed inside information

1533
01:34:18,150 --> 01:34:21,800
and when they are going to use the information what

1534
01:34:21,840 --> 01:34:24,590
what we are doing this

1535
01:34:26,260 --> 01:34:28,100
we divide not into

1536
01:34:28,130 --> 01:34:29,750
two plaques

1537
01:34:29,770 --> 01:34:32,880
and it means that after the division

1538
01:34:32,880 --> 01:34:37,090
this is not you have empty cells

1539
01:34:37,140 --> 01:34:44,060
and the more in your smallest of value from this not to be true

1540
01:34:44,090 --> 01:34:44,670
to date

1541
01:34:45,430 --> 01:34:50,430
so the tree is different from ordinary tree that you can see in in in

1542
01:34:50,430 --> 01:34:51,520
the party

1543
01:34:51,580 --> 01:34:53,590
is that

1544
01:34:53,640 --> 01:34:56,170
it grows from

1545
01:34:56,180 --> 01:34:58,110
leaves to root

1546
01:34:58,150 --> 01:35:03,440
not from root to leaves but rather in opposite directions so always late

1547
01:35:03,470 --> 01:35:08,960
leave and this new nor does provide gating on level

1548
01:35:09,010 --> 01:35:11,270
and it means that also that

1549
01:35:11,290 --> 01:35:12,370
the tree

1550
01:35:12,380 --> 01:35:16,130
in stable situation when you have to have a lot of things

1551
01:35:16,180 --> 01:35:17,720
it's on average

1552
01:35:19,180 --> 01:35:24,470
has half all cells not use and this is bad

1553
01:35:26,260 --> 01:35:31,150
four using in memory you can use it especially compressed around but it's not so

1554
01:35:31,150 --> 01:35:36,890
good but on these and expression in database applications not so well

1555
01:35:38,210 --> 01:35:39,040
we have

1556
01:35:39,080 --> 01:35:44,100
this place is much cheaper and it down but i don't bother about this lies

1557
01:35:45,680 --> 01:35:47,430
the second

1558
01:35:49,730 --> 01:35:54,420
we already have when we had into this not we already have

1559
01:35:54,430 --> 01:35:58,300
and think cell don't do we don't need to divide it again and you and

1560
01:35:58,300 --> 01:35:59,500
you would fall

1561
01:35:59,510 --> 01:36:01,680
and therefore

1562
01:36:01,690 --> 01:36:05,890
what they don't we don't need a lot of usually usually this leads to provide

1563
01:36:05,890 --> 01:36:10,510
additional are extremely restoration in real

1564
01:36:10,550 --> 01:36:12,210
so if have

1565
01:36:12,250 --> 01:36:14,590
computer is small

1566
01:36:15,840 --> 01:36:17,890
what we have a collection

1567
01:36:19,050 --> 01:36:24,080
huge number of documents be in a lot of different

1568
01:36:24,090 --> 01:36:25,340
i don't know maybe

1569
01:36:25,340 --> 01:36:32,000
and in this example with our text documents there are a lot of names

1570
01:36:32,010 --> 01:36:34,460
and geographical and other names

1571
01:36:35,600 --> 01:36:38,670
for unknown reasons trying to do it

1572
01:36:39,750 --> 01:36:44,260
from nineteen use only one megabyte of memory

1573
01:36:44,270 --> 01:36:47,900
actually the beginning of nineteen

1574
01:36:48,510 --> 01:36:52,510
you don't have any any other choice than to put parts of dictionary to model

1575
01:36:55,630 --> 01:36:58,760
and in this case is the trees structure

1576
01:36:58,770 --> 01:37:03,420
so what we're going to do if you are in such limited situation

1577
01:37:03,550 --> 01:37:05,670
using a smaller dictionary

1578
01:37:05,690 --> 01:37:09,680
when venetian documents for example the hash hash

1579
01:37:09,720 --> 01:37:12,110
when to be done with a group of documents

1580
01:37:12,140 --> 01:37:14,670
really combined from there

1581
01:37:14,670 --> 01:37:18,250
and add them to this country and these

1582
01:37:18,300 --> 01:37:22,800
and that's all we have dictionary very good so we don't dictionary

1583
01:37:22,840 --> 01:37:27,590
we even consider a situation when you should edit conflicts and to men

1584
01:37:30,970 --> 01:37:35,970
then i think what was going on usually when creating search engine actually a haven't

1585
01:37:35,970 --> 01:37:36,970
done got

1586
01:37:37,000 --> 01:37:38,350
we already have

1587
01:37:38,400 --> 01:37:40,770
dictionary and the

1588
01:37:40,870 --> 01:37:46,550
the are doing indexing and maybe were can do some experiments with charge

1589
01:37:46,640 --> 01:37:48,250
is that

1590
01:37:48,930 --> 01:37:53,640
you are developing this is what you want before development

1591
01:37:53,650 --> 01:37:57,800
it's complicated you'll have

1592
01:37:57,800 --> 01:38:01,880
got additional information and now you know the

1593
01:38:01,930 --> 01:38:04,210
it's not available on this hardware

1594
01:38:04,220 --> 01:38:09,180
for example knowledge actually so the that it can feed into your even even on

1595
01:38:09,180 --> 01:38:10,610
the on your knees

1596
01:38:10,660 --> 01:38:12,150
so what you can do

1597
01:38:12,210 --> 01:38:14,710
it scaling

1598
01:38:14,730 --> 01:38:18,050
and basically you have to approach the scaling

1599
01:38:18,130 --> 01:38:23,980
the first approach is to simply move to a new computer as so you can

1600
01:38:23,980 --> 01:38:28,090
actually it was and saying something like this

1601
01:38:28,090 --> 01:38:34,840
and now we try to do this but we need more memory

1602
01:38:34,840 --> 01:38:36,460
what we need

1603
01:38:39,810 --> 01:38:45,850
let's look into a catalogue and know by this user from there

1604
01:38:45,890 --> 01:38:48,210
four times memory and the other

1605
01:38:48,230 --> 01:38:51,050
so this is vertical scanning and

1606
01:38:51,090 --> 01:38:53,960
the problem is article scaling is that

1607
01:38:53,960 --> 01:38:56,420
figure out where the climate is grey on

1608
01:38:56,500 --> 01:39:00,380
and in order to do that you have to find the path through this semantic

1609
01:39:01,250 --> 01:39:05,420
and you can use any of these sort of cross-town links

1610
01:39:05,460 --> 01:39:09,900
so you gotta to figure out when it's possible to sort of in first knowledge

1611
01:39:09,900 --> 01:39:14,080
over one of the sort solid links and when you're supposed to sort and negated

1612
01:39:14,080 --> 01:39:16,570
conclusion using one of these cross-town links

1613
01:39:16,650 --> 01:39:20,520
there's a lot of work in the literature trying to use things like how specific

1614
01:39:20,520 --> 01:39:25,650
links so if you've got a shorter path between the object in your conclusion

1615
01:39:26,840 --> 01:39:29,750
we should take part in preference to another part

1616
01:39:29,770 --> 01:39:33,130
so this part from clyde elephant to gray

1617
01:39:33,130 --> 01:39:38,980
overrides the path from clyde to federal control of to not great

1618
01:39:39,000 --> 01:39:43,480
just looking at that you know you're in trouble so a lot of the research

1619
01:39:43,480 --> 01:39:47,750
and definitions of you know this is what when you can draw conclusions and when

1620
01:39:47,750 --> 01:39:51,750
you can draw conclusions and so on and really it just became more and more

1621
01:39:51,750 --> 01:39:52,860
difficult to

1622
01:39:52,860 --> 01:39:58,540
trying justify why this particular why quite should be grains cases to

1623
01:39:58,610 --> 01:40:01,550
what should be done

1624
01:40:01,570 --> 01:40:05,420
OK a little bit later we had this idea of frames now frames approved probably

1625
01:40:05,420 --> 01:40:07,070
end up being quite

1626
01:40:07,090 --> 01:40:11,860
for me it is the computer scientists because it ends up being sort of precursor

1627
01:40:12,540 --> 01:40:15,460
so object oriented approach which is common in london

1628
01:40:15,460 --> 01:40:16,840
programming languages

1629
01:40:16,860 --> 01:40:21,070
and the basic idea is that the price of the frames gives you a typical

1630
01:40:21,070 --> 01:40:23,710
instance and you can fill in all the slots

1631
01:40:23,750 --> 01:40:26,110
so this is the frame for a trip

1632
01:40:26,150 --> 01:40:31,050
and there are some things in this trip true so number steps

1633
01:40:31,110 --> 01:40:36,070
and this step might have different attributes attributes be things like the beginning data

1634
01:40:36,070 --> 01:40:40,320
the end of the step how you managed to carry out step and so on

1635
01:40:42,070 --> 01:40:44,790
what you would said like this one

1636
01:40:44,880 --> 01:40:49,590
the only method of trying to represent knowledge and same usually have some pretty typical

1637
01:40:49,590 --> 01:40:54,770
frames which can be used to populate the frames and draw some you know read

1638
01:40:54,790 --> 01:40:57,630
initial knowledge and then of course you can come back and you can buy some

1639
01:40:57,630 --> 01:40:58,480
of the knowledge

1640
01:40:58,550 --> 01:41:01,610
and in fact you can have these procedures stored

1641
01:41:01,610 --> 01:41:05,070
in some of these attributes to work out the value of attributes from other attributes

1642
01:41:05,090 --> 01:41:06,360
and so on

1643
01:41:06,380 --> 01:41:10,110
so is one which is the procedure to work at the cost of the trip

1644
01:41:10,210 --> 01:41:15,500
so by looking at the individual

1645
01:41:15,540 --> 01:41:16,540
OK that's

1646
01:41:16,730 --> 01:41:21,250
another method might be trying to represent uncertainty so one of the more

1647
01:41:21,630 --> 01:41:25,270
so i think she gets to this point which was

1648
01:41:25,290 --> 01:41:30,130
well a lot of the work is done in the standard with standard sort of

1649
01:41:30,130 --> 01:41:34,570
probability axioms but in in case you see a lot of work also using the

1650
01:41:34,570 --> 01:41:36,820
possibilistic axioms which

1651
01:41:36,840 --> 01:41:40,770
differ sense when you come when it comes to combining knowledge

1652
01:41:41,210 --> 01:41:45,440
so for instance if you know to preserve sort of probabilities two propositions and you

1653
01:41:45,900 --> 01:41:49,190
you will look at the conjunction of the two

1654
01:41:49,210 --> 01:41:53,920
well the possibilistic act and the possibilistic actions usually take the maximum of the degrees

1655
01:41:53,960 --> 01:41:55,020
of belief in the

1656
01:41:55,070 --> 01:41:56,090
of the two

1657
01:41:56,110 --> 01:41:59,980
propositions but you can

1658
01:42:00,040 --> 01:42:03,840
OK then this an attempt to do things like in bayesian networks here we're essentially

1659
01:42:03,840 --> 01:42:04,670
what you're

1660
01:42:04,670 --> 01:42:06,520
trying to do

1661
01:42:06,940 --> 01:42:12,460
it is instead trying to give some sort of heuristic to evaluate the conditional probability

1662
01:42:12,460 --> 01:42:16,770
which doesn't require you to go through based formula and trying calculate all of the

1663
01:42:16,770 --> 01:42:22,480
conditional release by using the by using this network you can

1664
01:42:22,480 --> 01:42:25,110
determine the conditional probability without us having to

1665
01:42:25,150 --> 01:42:29,310
prior to this huge form

1666
01:42:29,340 --> 01:42:34,430
there are other methods like this to shape theory fuzzy logic which again you sort

1667
01:42:34,430 --> 01:42:35,520
of numeric

1668
01:42:35,570 --> 01:42:41,040
instead of using triple space using some sort of numeric assessment of the

1669
01:42:41,090 --> 01:42:45,960
the level of degree of belief in the proposition and then how you go about

1670
01:42:45,960 --> 01:42:47,380
combining those

1671
01:42:47,420 --> 01:42:49,920
great ability

1672
01:42:49,980 --> 01:42:53,730
it's essentially what differs between all these methods the way that you end up combining

1673
01:42:53,730 --> 01:42:58,810
the degrees of belief when you have more complex formulas

1674
01:42:58,810 --> 01:43:03,400
OK now one of the sort of hallmarks of logic in artificial intelligence is this

1675
01:43:03,400 --> 01:43:06,540
idea of what will nonmonotonic reasoning

1676
01:43:06,630 --> 01:43:12,090
now as i a nonmonotonic reasoning tries to capture this notion of common sense reasoning

1677
01:43:12,130 --> 01:43:16,210
so the argument would be that if we look at say classical logic be in

1678
01:43:16,340 --> 01:43:20,090
propositional form or in its first order form

1679
01:43:20,110 --> 01:43:22,290
that type of logic is good for

1680
01:43:22,340 --> 01:43:23,610
reasoning about

1681
01:43:23,630 --> 01:43:25,650
like mathematics for instance

1682
01:43:25,730 --> 01:43:28,610
reasoning about the true mathematical statements

1683
01:43:28,610 --> 01:43:30,020
where it's less

1684
01:43:30,020 --> 01:43:33,050
so in other words for everybody in population

1685
01:43:33,060 --> 01:43:36,030
this variable is instantiated two

1686
01:43:36,830 --> 01:43:39,140
person has hypertension

1687
01:43:39,190 --> 01:43:41,380
well that was the case

1688
01:43:41,400 --> 01:43:46,070
finasteride discounts hair loss just like i said before for

1689
01:43:46,070 --> 01:43:49,450
if we find that an individual uses finasteride

1690
01:43:49,510 --> 01:43:52,840
it makes it explains away the hypertension

1691
01:43:52,860 --> 01:43:54,630
making it less likely

1692
01:43:56,080 --> 01:44:00,850
so the people who use finasteride would have less hair loss

1693
01:44:00,880 --> 01:44:06,790
so selection bias is present in our sample can can screw up the

1694
01:44:06,790 --> 01:44:08,030
because the

1695
01:44:08,060 --> 01:44:10,080
causal markov assumption of bad

1696
01:44:10,130 --> 01:44:11,860
that is not yet

1697
01:44:11,870 --> 01:44:16,660
can to happen and that's not merely obviously problematic as they had in common cause

1698
01:44:16,660 --> 01:44:22,970
thing but is something you have to be careful

1699
01:44:22,990 --> 01:44:27,370
this is just last right there are not independent in our observed distribution although there

1700
01:44:29,160 --> 01:44:35,990
amongst all people did not independent the probability distribution that we observe

1701
01:44:36,090 --> 01:44:39,370
fourth one is kind of funny one

1702
01:44:39,410 --> 01:44:41,180
and this

1703
01:44:41,200 --> 01:44:45,550
in this domain there's always the population my examples of all the population has been

1704
01:44:46,800 --> 01:44:49,990
the population happens to be units of time

1705
01:44:50,010 --> 01:44:53,280
in nineteen ninety and we just take each day

1706
01:44:54,390 --> 01:44:58,780
if we looked at at the bottom of first the dow jones and should be

1707
01:45:00,120 --> 01:45:01,620
my here one

1708
01:45:01,640 --> 01:45:07,140
because all the nineties about average one up my harlan's going up the same time

1709
01:45:07,300 --> 01:45:11,510
but nobody would ever think about jones and which had a causal effect on my

1710
01:45:11,510 --> 01:45:13,720
carolina and vice versa so

1711
01:45:13,760 --> 01:45:15,280
so the causal

1712
01:45:15,280 --> 01:45:18,890
the relationship between these two variables is is

1713
01:45:18,890 --> 01:45:21,910
there is none yet they were correlated

1714
01:45:21,950 --> 01:45:26,470
in the last few weeks has not gone back down so that kind of

1715
01:45:26,510 --> 01:45:29,370
in the case because the relationship is not there

1716
01:45:29,390 --> 01:45:31,320
actually that was up again

1717
01:45:31,370 --> 01:45:32,680
those down again

1718
01:45:34,050 --> 01:45:38,450
school like this not

1719
01:45:38,590 --> 01:45:42,010
perhaps the condition that is most violated is that it can be no hidden common

1720
01:45:43,990 --> 01:45:47,910
i'll come back to this like i said i don't that assumption is isn't all

1721
01:45:47,910 --> 01:45:49,490
the reasonable

1722
01:45:50,910 --> 01:45:53,910
but it's easier to develop a theory if we take

1723
01:45:53,950 --> 01:45:56,220
so that from the start of making it

1724
01:45:57,430 --> 01:46:01,760
we need to go back

1725
01:46:01,760 --> 01:46:13,510
well if there is a causal feedback there would be correlated

1726
01:46:13,550 --> 01:46:14,680
but you couldn't

1727
01:46:14,740 --> 01:46:19,910
applied the theory because the theory requires that the adaptive causation only goes in one

1728
01:46:21,090 --> 01:46:27,280
so one is causation one is correlation and these two different concepts

1729
01:46:27,340 --> 01:46:29,780
OK to keep track of the time i realize

1730
01:46:32,800 --> 01:46:38,490
two thirty eight already is going to quickly so far

1731
01:46:38,700 --> 01:46:43,680
there's something different is the conditional dependency among these variables that is not entailed by

1732
01:46:43,680 --> 01:46:45,140
the markov condition

1733
01:46:45,180 --> 01:46:48,430
this occurs because comprehensive example involving

1734
01:46:48,450 --> 01:46:53,800
these variables that's the right as i told you before lowers DHT levels

1735
01:46:53,870 --> 01:46:55,910
that's actually how it

1736
01:46:56,760 --> 01:47:00,140
during baldness because DHT is male hormone that

1737
01:47:00,160 --> 01:47:03,780
makes hair fall out the lawyer DHT levels

1738
01:47:03,800 --> 01:47:06,390
actually the a lot of studies and if you do that around the age of

1739
01:47:06,390 --> 01:47:08,950
twenty you what was your hair

1740
01:47:08,970 --> 01:47:12,470
not sure but it's very likely will map

1741
01:47:12,510 --> 01:47:17,700
now it is a study in logan and i'll show DHT levels though

1742
01:47:18,030 --> 01:47:23,280
low you know cause low DHT levels cause erectile dysfunction

1743
01:47:23,340 --> 01:47:24,320
so given

1744
01:47:24,350 --> 01:47:30,740
this causal chain you would think the use of finasteride cause erectile dysfunction

1745
01:47:30,740 --> 01:47:32,930
so in march started

1746
01:47:33,300 --> 01:47:35,180
offering this medication

1747
01:47:36,590 --> 01:47:38,970
they fear decided

1748
01:47:38,990 --> 01:47:40,510
the form

1749
01:47:40,510 --> 01:47:46,640
it's going to be used because it out of the room that the termites and

1750
01:47:46,720 --> 01:47:49,870
and to the truth

1751
01:47:50,820 --> 01:47:52,740
what's going on here

1752
01:47:52,740 --> 01:47:57,280
because we want to take the medication right it's just that this happened well

1753
01:47:57,340 --> 01:47:59,070
actually in enough

1754
01:47:59,120 --> 01:48:02,760
in a very large controlled study

1755
01:48:02,820 --> 01:48:08,320
it was essentially found that even after independent there's some very small evidence that there

1756
01:48:08,320 --> 01:48:09,660
prior model

1757
01:48:09,680 --> 01:48:13,390
which is the same with logistic regression we also get

1758
01:48:13,410 --> 01:48:14,450
one model

1759
01:48:14,470 --> 01:48:20,680
so let's say one rule or whatever but here the idea is that you would

1760
01:48:21,800 --> 01:48:27,030
a model which can be let's say also transformed into the individual rules and that

1761
01:48:27,050 --> 01:48:31,990
you would kind of be able to understand the individual decisions based on the

1762
01:48:32,410 --> 01:48:34,130
pass which leads you

1763
01:48:34,150 --> 01:48:34,970
two there

1764
01:48:35,010 --> 01:48:37,780
the final decision

1765
01:48:37,800 --> 01:48:41,450
so also the approach

1766
01:48:41,550 --> 01:48:47,840
here is based on the most informative attributes which is put in the particular note

1767
01:48:47,840 --> 01:48:50,340
of the decision tree and the

1768
01:48:50,360 --> 01:48:56,380
way the most informative activities is determined is by the heuristic which is called the

1769
01:48:56,380 --> 01:49:04,200
so called information gain and that tries to mimic the

1770
01:49:04,450 --> 01:49:08,780
property known from information theory which is called entropy

1771
01:49:09,010 --> 01:49:13,430
which is used to characterize the impurity of the set

1772
01:49:13,450 --> 01:49:16,360
of instances which are in the particular note

1773
01:49:16,410 --> 01:49:22,050
so what when is the entropy in the of the decision tree the largest

1774
01:49:22,050 --> 01:49:27,720
the largest entropy will be when there are examples of different classes which are in

1775
01:49:27,720 --> 01:49:29,070
the set

1776
01:49:29,090 --> 01:49:34,160
so then the entropy is the largest then entropy is the smallest or equal zero

1777
01:49:34,590 --> 01:49:36,570
when all the examples

1778
01:49:36,610 --> 01:49:38,240
in a subset

1779
01:49:38,240 --> 01:49:41,360
are labeled with the same class label

1780
01:49:41,380 --> 01:49:46,740
so the idea is that in every note we will select such an attribute

1781
01:49:46,760 --> 01:49:49,070
we took all the

1782
01:49:49,070 --> 01:49:54,320
after splitting the note the entropy will in the sub notes will be the smallest

1783
01:49:54,320 --> 01:49:59,340
so that the intuition behind the decision tree learning algorithm

1784
01:49:59,550 --> 01:50:02,150
and if we formalize this

1785
01:50:02,160 --> 01:50:03,880
measure of entropy

1786
01:50:03,880 --> 01:50:06,110
of a set of examples s

1787
01:50:06,130 --> 01:50:09,410
so we have a training set set of training examples

1788
01:50:09,590 --> 01:50:14,880
suppose without labelled with different class labels in our example it was three classes

1789
01:50:15,070 --> 01:50:19,320
like it was two classes like play tennis yes and no

1790
01:50:20,340 --> 01:50:22,660
the entropy of

1791
01:50:22,680 --> 01:50:27,050
a set of examples s is the measure of the impurity of the of the

1792
01:50:27,050 --> 01:50:28,180
set as

1793
01:50:28,180 --> 01:50:33,910
and it is computed with the following formula so let's look at the simplified version

1794
01:50:34,750 --> 01:50:40,090
the case when we have just the binary class plus and minus entropy with computed

1795
01:50:40,090 --> 01:50:46,090
like minus probability of class plus times the logarithm of the probability of class plus

1796
01:50:46,110 --> 01:50:52,340
minus probability of class minus times the logarithm of the probability of class minus so

1797
01:50:52,340 --> 01:50:57,930
this looks pretty horrible but if we look at in the intuition behind this will

1798
01:50:57,930 --> 01:50:59,890
come become very clear

1799
01:51:00,630 --> 01:51:02,950
let's look at this formula

1800
01:51:04,300 --> 01:51:06,550
we want would like to have

1801
01:51:08,160 --> 01:51:09,380
such that

1802
01:51:09,450 --> 01:51:11,970
the entropy is the highest when

1803
01:51:12,010 --> 01:51:16,840
the in the in the set of examples we have representatives of both classes and

1804
01:51:16,840 --> 01:51:20,970
the entropy will be the highest when we have fifty percent of examples of one

1805
01:51:20,970 --> 01:51:24,650
class and fifty percent of examples of the other class so that's

1806
01:51:24,650 --> 01:51:31,530
totally that has the highest entropy and exactly here is the maximum of this function

1807
01:51:31,950 --> 01:51:36,490
which is the probability so if we plot the probability of

1808
01:51:36,550 --> 01:51:38,220
class plus here

1809
01:51:38,240 --> 01:51:40,840
at the scale of zero probability

1810
01:51:41,780 --> 01:51:45,110
one probability one so in this point

1811
01:51:45,130 --> 01:51:49,240
all the examples would be of a certain class plus

1812
01:51:49,260 --> 01:51:54,700
point all the examples will be of class miners and at this point there is

1813
01:51:54,700 --> 01:51:59,470
fifty percent of examples of class plus info fifty percent of class minus so in

1814
01:51:59,470 --> 01:52:02,180
this case the entropy is the highest

1815
01:52:02,200 --> 01:52:07,220
in this case the entropy equals zero and in this case all the examples belong

1816
01:52:07,220 --> 01:52:10,340
to class minus the entropy zero

1817
01:52:12,360 --> 01:52:17,110
if there is not fifty fifty distribution for instance if we have eighty percent of

1818
01:52:17,970 --> 01:52:23,200
class and twenty percent of class minus the entropy still relatively high

