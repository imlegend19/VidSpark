1
00:00:00,000 --> 00:00:07,150
you will grant equation for the dirichlet integral the continuum looks like this and for

2
00:00:07,220 --> 00:00:08,900
the graph problem

3
00:00:08,920 --> 00:00:11,420
it looks like that so that's equivalent

4
00:00:11,430 --> 00:00:16,470
o plus interview because you're the passing of exit

5
00:00:16,560 --> 00:00:20,710
you also the final boss in natively just based on the topology of the graph

6
00:00:20,720 --> 00:00:24,690
i don't think i really need to go through this for for this crowd and

7
00:00:24,690 --> 00:00:29,210
order to actually solve the problem with the boundary conditions you can decomposable policy in

8
00:00:29,210 --> 00:00:34,390
the block form introduction introduced boundary conditions in the form of the vector

9
00:00:34,440 --> 00:00:37,890
and then solve for the unknown

10
00:00:37,930 --> 00:00:40,610
potentials at the nodes

11
00:00:40,610 --> 00:00:43,250
and what you're left with a system of linear equations

12
00:00:43,280 --> 00:00:46,450
indeed so one system of linear equations per

13
00:00:46,470 --> 00:00:52,300
the label released if you k labels need to solve k minus one systems equations

14
00:00:53,530 --> 00:00:54,980
the weights

15
00:00:55,000 --> 00:00:57,360
again is an sure you all know

16
00:00:57,430 --> 00:01:03,750
take a function of the the image gradient or alternately any other feature that you

17
00:01:03,750 --> 00:01:06,140
had in mind texture color

18
00:01:06,150 --> 00:01:08,560
and the

19
00:01:08,560 --> 00:01:13,890
each pixel rich foxholes associated with one node in the graph

20
00:01:13,930 --> 00:01:17,630
and this representation shows the

21
00:01:17,640 --> 00:01:19,450
the edge weights

22
00:01:19,480 --> 00:01:24,450
represented by the thickness of the edges

23
00:01:24,470 --> 00:01:27,730
alternate way of viewing this

24
00:01:27,790 --> 00:01:32,820
random walker problems from the standpoint of circuit theory the same directional actually integral same

25
00:01:32,820 --> 00:01:34,650
combinatorial dirichlet integral

26
00:01:34,660 --> 00:01:37,350
it can be seen as the composition of the three main

27
00:01:37,390 --> 00:01:39,850
equations in circuit theory

28
00:01:39,880 --> 00:01:43,380
khrushchev's current location voltage law and ohms law

29
00:01:43,400 --> 00:01:45,200
if you put these three together

30
00:01:45,210 --> 00:01:46,400
you get this

31
00:01:47,370 --> 00:01:50,040
dissipation on the circuit and the

32
00:01:50,060 --> 00:01:54,360
system of distribution of potentials or probabilities

33
00:01:54,370 --> 00:01:56,550
is going to be equivalent to

34
00:01:56,560 --> 00:01:59,660
the solution of this circuit the problem

35
00:01:59,750 --> 00:02:02,710
from the standpoint of circuit theory you can think of what the organs doing is

36
00:02:02,710 --> 00:02:07,450
the following so we had a four by four image

37
00:02:07,460 --> 00:02:09,350
and we label three

38
00:02:10,460 --> 00:02:13,010
green yellow and blue

39
00:02:13,070 --> 00:02:16,160
if we want to compute the probabilities that

40
00:02:17,060 --> 00:02:23,290
note here centre of a random walker two first label

41
00:02:23,300 --> 00:02:27,880
before the other two from circuit standpoint we encode all the way to the conductance

42
00:02:27,880 --> 00:02:30,110
is the reciprocal the resistances

43
00:02:33,130 --> 00:02:38,260
ground labels two and three establishing a voltage source with label one

44
00:02:38,320 --> 00:02:45,390
and solve for the steady-state DC potentials these states potentials are exactly equal to the

45
00:02:45,390 --> 00:02:47,660
probability is that we're looking for

46
00:02:47,690 --> 00:02:52,310
and so therefore we get a distribution of probabilities at each node sends random walk

47
00:02:52,310 --> 00:02:53,460
the first label

48
00:02:53,470 --> 00:02:58,190
in order to compute the probabilities that a random walker center the second label we

49
00:02:58,190 --> 00:03:00,710
can ground labels one and three

50
00:03:00,730 --> 00:03:02,480
establishing folded source

51
00:03:02,630 --> 00:03:08,450
i was labelled to compute these probabilities and then likewise for label three

52
00:03:08,610 --> 00:03:11,290
for any given node

53
00:03:11,300 --> 00:03:16,720
if you some these probabilities across different labels you should get one another with thinking

54
00:03:16,720 --> 00:03:21,340
about why this should be true is from the superposition theorem circuit there

55
00:03:23,820 --> 00:03:28,470
original motivating random walker problem that we looked at it can also be

56
00:03:28,530 --> 00:03:30,290
alternately thought of

57
00:03:30,300 --> 00:03:35,560
as circuit theory problem for which it should be no surprise that there is a

58
00:03:35,560 --> 00:03:37,050
steady state

59
00:03:37,100 --> 00:03:40,120
deterministic solution

60
00:03:40,830 --> 00:03:41,860
to distill

61
00:03:41,870 --> 00:03:43,610
the last few slides

62
00:03:43,660 --> 00:03:46,410
here's how you operate the algorithm

63
00:03:47,330 --> 00:03:51,640
generate weights based on image intensities or any other feature

64
00:03:51,650 --> 00:03:56,370
the little plus in matrix you solve system of equations for each seed group each

65
00:03:57,280 --> 00:04:02,300
and then use on the pixels the label for which it has the highest probability

66
00:04:02,360 --> 00:04:07,060
it's about fifteen lines of matlab to do the whole thing

67
00:04:07,110 --> 00:04:12,760
and you can find those fifteen lines available on my webpage look at that way

68
00:04:13,560 --> 00:04:14,710
you can

69
00:04:14,730 --> 00:04:16,840
i interpret this algorithm

70
00:04:16,850 --> 00:04:20,600
from several different perspectives depending on what you're most comfortable with

71
00:04:20,650 --> 00:04:23,470
as i have been discussing there's this

72
00:04:23,480 --> 00:04:24,830
random walker

73
00:04:24,830 --> 00:04:28,540
interpretation this electrical potential interpretation

74
00:04:28,640 --> 00:04:32,390
from the standpoint of of continuous peet's you can think of it as the temperature

75
00:04:33,490 --> 00:04:38,620
could say your image was thought of as the a sheet of copper

76
00:04:39,650 --> 00:04:42,070
these edges

77
00:04:42,090 --> 00:04:44,860
could be thought of as a thermal installations

78
00:04:44,970 --> 00:04:51,150
not not allowing temperature heat to pass between them then you could view the distribution

79
00:04:51,150 --> 00:04:53,570
of potentials are probabilities

80
00:04:53,580 --> 00:04:54,970
as the

81
00:04:55,070 --> 00:04:57,820
the distribution of potentials if you were to set up

82
00:05:00,150 --> 00:05:06,650
a one degree celsius boundary here and is zero degrees celsius boundary condition there

83
00:05:06,680 --> 00:05:08,960
this would be the steady state of the art of

84
00:05:09,010 --> 00:05:16,200
temperatures across that piece of copper which would then be thresholded produce the segmentation

85
00:05:16,220 --> 00:05:19,410
another interpretation going back to the electrical circuit analogy

86
00:05:19,420 --> 00:05:24,650
it is in terms of effective resistance so given the same circuit set up

87
00:05:24,890 --> 00:05:28,720
we could compute the effective resistance between this point and blue

88
00:05:28,730 --> 00:05:30,880
or this point and red

89
00:05:30,880 --> 00:05:33,580
and then if the effective resistance with blue

90
00:05:33,590 --> 00:05:36,850
was smaller it would be assigned label blue

91
00:05:36,960 --> 00:05:42,140
an intuition of what factor resistance represents

92
00:05:44,570 --> 00:05:49,040
if there are a lot of parallel paths between this point blue defect resistance will

93
00:05:49,040 --> 00:05:49,940
be low

94
00:05:50,000 --> 00:05:53,160
but if all the parallel for the past had to go through certain

95
00:05:53,170 --> 00:05:54,090
o point

96
00:05:54,090 --> 00:05:58,570
or certain set of resistors and effective resistance will be larger

97
00:05:58,630 --> 00:06:05,670
a final interpretation is completely different and it's entirely in terms of

98
00:06:06,960 --> 00:06:10,160
and completely combinatorial it should be surprised

99
00:06:10,230 --> 00:06:15,110
surprising that there is a tree interpretation of this

100
00:06:15,120 --> 00:06:16,900
problems because

101
00:06:16,920 --> 00:06:22,690
ever since car shops original papers trees and circuits have really been identified

102
00:06:24,490 --> 00:06:26,840
what's the tree interpretation here

103
00:06:26,900 --> 00:06:29,260
imagine we have this graph

104
00:06:29,360 --> 00:06:31,490
and we see

105
00:06:31,530 --> 00:06:34,880
these two nodes is blue and red and we want to label this

106
00:06:34,890 --> 00:06:36,010
green node

107
00:06:36,010 --> 00:06:39,340
if you were to do the following

108
00:06:39,380 --> 00:06:44,510
you could obtain this labeling randomly choose the tree from this graph

109
00:06:44,530 --> 00:06:48,660
and then randomly removing edge from that tree

110
00:06:48,680 --> 00:06:52,410
and you would get for example something that looks like this

111
00:06:52,550 --> 00:06:54,820
in this case

112
00:06:54,860 --> 00:06:57,970
the node in question is attached to blue

113
00:06:57,990 --> 00:06:59,740
but not to read

114
00:07:01,050 --> 00:07:03,990
one vote

115
00:07:03,990 --> 00:07:05,320
based on the street

116
00:07:05,340 --> 00:07:07,030
i would send this

117
00:07:07,050 --> 00:07:10,840
notable blue if you to keep repeating this procedure

118
00:07:10,840 --> 00:07:15,190
you would you would only be able to conclude that things will happen in at

119
00:07:15,190 --> 00:07:16,670
most one over an hour

120
00:07:16,720 --> 00:07:19,360
at least one over and would be the probability

121
00:07:19,410 --> 00:07:23,760
that's also seems reasonable is only made and experiment

122
00:07:23,820 --> 00:07:25,250
how can you then be sure

123
00:07:25,290 --> 00:07:28,850
about outcomes have probability is lower than one already

124
00:07:28,860 --> 00:07:32,380
it doesn't seem you can really gain information about things that happened that infrequently if

125
00:07:32,380 --> 00:07:35,130
you want to make it and that we away

126
00:07:35,160 --> 00:07:39,880
so fix this kind of stuff

127
00:07:40,020 --> 00:07:44,290
so what do you do if you're it if you want to do things in

128
00:07:44,630 --> 00:07:45,650
the same way

129
00:07:47,410 --> 00:07:48,330
the bayesian

130
00:07:50,080 --> 00:07:55,470
thousands the more complicated than more ingredients in how to formalize your problems

131
00:07:55,970 --> 00:07:58,700
but but what i'm trying to show here

132
00:07:58,870 --> 00:07:59,630
is that

133
00:07:59,690 --> 00:08:01,970
in many cases the

134
00:08:03,010 --> 00:08:05,400
the outcome of your analysis

135
00:08:05,430 --> 00:08:08,860
and the actual procedure that you are a fine

136
00:08:08,880 --> 00:08:12,100
black is conceptually a lot simpler in this case

137
00:08:12,140 --> 00:08:13,540
let's let's try to do

138
00:08:13,580 --> 00:08:16,130
so again you have the likelihood function

139
00:08:16,230 --> 00:08:18,520
which is famous for the likelihood of the

140
00:08:18,560 --> 00:08:20,460
the probability of the observations

141
00:08:20,480 --> 00:08:21,780
given the problem

142
00:08:21,820 --> 00:08:27,520
and then you have a new thing here which called was of higher

143
00:08:27,610 --> 00:08:29,320
the prior is the

144
00:08:29,330 --> 00:08:33,000
the knowledge of the assumptions you make about the parameters

145
00:08:33,020 --> 00:08:35,090
before you make any observations

146
00:08:36,090 --> 00:08:39,460
this is called the prior p of high

147
00:08:39,820 --> 00:08:43,000
what you know about high before you start

148
00:08:43,050 --> 00:08:45,110
what you think about the properties of the point

149
00:08:46,510 --> 00:08:49,850
it's not really the observation that five of the priors

150
00:08:54,870 --> 00:08:56,670
i talked a lot more about

151
00:08:59,050 --> 00:09:02,170
of i thought i was a little bit

152
00:09:02,190 --> 00:09:03,460
so i'll talk about

153
00:09:03,510 --> 00:09:06,930
so in this case you have to say what you think about the before you

154
00:09:06,930 --> 00:09:10,770
start and people get people going but uneasy about this

155
00:09:10,780 --> 00:09:14,160
i say well i don't know anything about twenty four

156
00:09:14,230 --> 00:09:16,120
and then the bayesian with a well

157
00:09:16,200 --> 00:09:19,750
and then you can do inference no way

158
00:09:19,780 --> 00:09:20,730
but you can

159
00:09:21,210 --> 00:09:24,600
if you say anything could happen

160
00:09:24,610 --> 00:09:26,420
for aquinas list is a little bit

161
00:09:26,740 --> 00:09:28,820
it's a bit hard to imagine

162
00:09:28,860 --> 00:09:31,890
maybe it balances on the edge of something like

163
00:09:31,920 --> 00:09:37,400
it's a little bit but hard to imagine let's say you're using more complicated in

164
00:09:37,400 --> 00:09:41,720
the more complicated narrative you trading stocks for example is a well what do you

165
00:09:41,720 --> 00:09:44,100
think what do you think the properties of

166
00:09:44,110 --> 00:09:45,190
dogs are

167
00:09:45,250 --> 00:09:46,560
it's a oh i don't know

168
00:09:46,600 --> 00:09:48,580
i just want look at the data

169
00:09:48,630 --> 00:09:51,460
and that's problem in that right

170
00:09:52,190 --> 00:09:53,180
and of course

171
00:09:53,230 --> 00:09:54,870
stock prices are not

172
00:09:54,880 --> 00:09:58,430
and they can do just anything like if you could do just anything that it

173
00:09:58,430 --> 00:10:00,590
would be impossible to model what they were doing

174
00:10:00,640 --> 00:10:03,750
well stocks had a value of you know

175
00:10:05,110 --> 00:10:09,060
have added value of one hundred yesterday and today as the value of hundred one

176
00:10:09,060 --> 00:10:11,400
what think it has more

177
00:10:11,430 --> 00:10:12,930
can you can say well

178
00:10:12,970 --> 00:10:14,070
i have no idea

179
00:10:14,720 --> 00:10:15,810
good the value of b

180
00:10:15,810 --> 00:10:17,620
ten to twenty six

181
00:10:17,680 --> 00:10:19,940
probably not that seems unlikely

182
00:10:20,290 --> 00:10:23,930
we do know something i don't know exactly

183
00:10:23,950 --> 00:10:25,900
what this talk is going to be tomorrow

184
00:10:25,970 --> 00:10:28,290
if you do that you wouldn't be here

185
00:10:33,250 --> 00:10:36,750
the prior of what do we know about the problem before we start

186
00:10:36,790 --> 00:10:38,290
will get back to

187
00:10:38,310 --> 00:10:43,520
what kind of things you could know about it in another experiment a in that

188
00:10:43,520 --> 00:10:48,560
experiment is not so interesting tomorrow i'll talk about inference about functions that we have

189
00:10:48,580 --> 00:10:51,500
prior distribution over functions and things will becoming in

190
00:10:52,830 --> 00:10:59,750
OK and then then we what we will compute is known the posterior the posterior

191
00:11:00,960 --> 00:11:04,380
the probability distribution of the parameters

192
00:11:04,380 --> 00:11:08,290
after we made observations about the probability of high

193
00:11:09,540 --> 00:11:11,900
the observation that we made be

194
00:11:12,750 --> 00:11:14,830
i notice that the difference between the

195
00:11:16,980 --> 00:11:21,580
the posterior the likelihood is exactly the reverse

196
00:11:21,690 --> 00:11:24,980
the property rights like what you need given y

197
00:11:25,020 --> 00:11:27,350
and the posterior you high

198
00:11:27,350 --> 00:11:31,400
what that what the the likelihood and the prior in high

199
00:11:32,690 --> 00:11:35,790
the distribution over

200
00:11:36,150 --> 00:11:38,690
or upon

201
00:11:38,730 --> 00:11:43,380
and only when you want to plug into the bayes rule here by using

202
00:11:43,400 --> 00:11:47,230
this by swapping these these conditional probabilities

203
00:11:47,290 --> 00:11:48,230
to get the

204
00:11:48,230 --> 00:11:49,580
the ratio of the

205
00:11:49,580 --> 00:11:52,440
prior to the that prior divided by

206
00:11:52,500 --> 00:11:56,750
here pp here is just the probability of the data

207
00:11:57,670 --> 00:12:00,880
and refer to model likelihood of the data

208
00:12:02,250 --> 00:12:02,900
so now

209
00:12:02,940 --> 00:12:04,540
the bayesian inference

210
00:12:05,730 --> 00:12:09,230
so the computing what is the posterior probability

211
00:12:09,250 --> 00:12:13,730
start by some knowledge about the data which incorporated the higher

212
00:12:13,730 --> 00:12:15,310
the update that

213
00:12:15,310 --> 00:12:18,210
you draw the samples from this q and you evaluate

214
00:12:18,260 --> 00:12:19,840
o thing in here

215
00:12:20,640 --> 00:12:21,680
there some

216
00:12:21,800 --> 00:12:26,370
some technicalities about you know a lot about a zero sum of at the details

217
00:12:26,950 --> 00:12:33,200
they the concept whatever that means that you draw samples from some distribution q and

218
00:12:33,200 --> 00:12:34,950
then you evaluate the

219
00:12:34,990 --> 00:12:36,900
some of these things that the so-called

220
00:12:36,950 --> 00:12:42,420
this ratio is called the importance weights multiply these guys by the whole

221
00:12:42,440 --> 00:12:45,960
and of course if q is completely different from p

222
00:12:45,970 --> 00:12:50,080
then the importance weights will sometimes very large and very small that's not a good

223
00:12:51,240 --> 00:12:54,480
would be nicer if these things were often very close to one

224
00:12:54,620 --> 00:12:58,140
you're averaging together you get extra variance

225
00:12:58,150 --> 00:13:02,670
you have to get enough samples here i average band and p PS two are

226
00:13:02,670 --> 00:13:07,490
very different and that adds extra bang we have to get good important and we

227
00:13:07,490 --> 00:13:08,920
have to choose q

228
00:13:08,970 --> 00:13:12,380
that's as close as possible to p of positive sample from one

229
00:13:12,400 --> 00:13:14,980
so can just choose the largest

230
00:13:15,050 --> 00:13:19,850
equal to q so the

231
00:13:19,860 --> 00:13:24,390
OK now so the one one one

232
00:13:24,500 --> 00:13:29,050
objective come with here is well this actually frequencies procedure i just take just take

233
00:13:29,050 --> 00:13:30,000
a bunch of

234
00:13:31,120 --> 00:13:33,020
evaluation just hours together

235
00:13:33,030 --> 00:13:35,580
it's all i can be

236
00:13:35,590 --> 00:13:42,620
maximum likelihood estimates that look a little bit closer at what kind of things what

237
00:13:42,620 --> 00:13:44,020
kind of

238
00:13:44,040 --> 00:13:47,980
by the properties that kind of procedure might have

239
00:13:48,030 --> 00:13:49,160
here's an example

240
00:13:49,170 --> 00:13:53,620
where i've compared to have a function here

241
00:13:53,670 --> 00:13:57,930
i want to evaluate what the interval of this of this function is complicated functions

242
00:13:57,950 --> 00:13:59,700
and within school list

243
00:13:59,820 --> 00:14:01,670
i want to integrate with respect to

244
00:14:01,740 --> 00:14:06,120
from distributions p which is given by

245
00:14:06,140 --> 00:14:07,970
the red carpet

246
00:14:08,100 --> 00:14:14,210
so let's try this try important thing say i can't p because it has a

247
00:14:14,240 --> 00:14:19,580
looks scales here is not quite out so let's let me invent similar distribution which

248
00:14:19,580 --> 00:14:21,760
accounts for

249
00:14:21,770 --> 00:14:22,960
q one here

250
00:14:22,980 --> 00:14:23,990
or two

251
00:14:24,000 --> 00:14:25,060
and then

252
00:14:25,070 --> 00:14:27,730
now let me draw samples from the distribution

253
00:14:27,740 --> 00:14:30,460
and then evaluate the monte carlo estimate

254
00:14:30,480 --> 00:14:31,700
let's say i draw

255
00:14:31,710 --> 00:14:34,630
this report the point light

256
00:14:34,670 --> 00:14:36,180
it's three x values

257
00:14:36,540 --> 00:14:38,900
nine evaluate the function here

258
00:14:38,920 --> 00:14:42,610
now in this case my estimate

259
00:14:42,620 --> 00:14:46,840
it would depend on what the q function was

260
00:14:47,420 --> 00:14:49,370
although i draw the same point

261
00:14:49,380 --> 00:14:50,730
it's possible to draw

262
00:14:50,740 --> 00:14:54,940
these three points from both of these distributions are not that different

263
00:14:54,950 --> 00:14:57,250
and if i draw these three points

264
00:14:57,300 --> 00:14:58,540
then my

265
00:14:58,560 --> 00:14:59,860
estimate will be

266
00:15:00,520 --> 00:15:07,090
the weighted average of the of those function the weights will be different because the

267
00:15:07,090 --> 00:15:08,260
ratio between

268
00:15:08,310 --> 00:15:09,370
p and q

269
00:15:09,420 --> 00:15:10,840
the accusative

270
00:15:10,850 --> 00:15:12,210
this is an example

271
00:15:12,230 --> 00:15:16,840
where you can see that the procedure violated like principle depends on something which was

272
00:15:16,840 --> 00:15:19,390
our chosen arbitrarily

273
00:15:19,400 --> 00:15:22,040
but that does that mean something

274
00:15:22,090 --> 00:15:24,530
we're going on here

275
00:15:24,570 --> 00:15:29,310
there's another problem problems related to the fact that it does not take into account

276
00:15:29,310 --> 00:15:34,820
what the x values are just hours to go the value function for example here

277
00:15:34,900 --> 00:15:37,340
two almost identical values of x

278
00:15:37,350 --> 00:15:39,950
which we happen to choose random distribution

279
00:15:39,960 --> 00:15:42,180
but they still played exactly the same role

280
00:15:42,190 --> 00:15:45,150
in the value of the of the interval

281
00:15:45,270 --> 00:15:49,260
then we're right because they both basically carry the same information

282
00:15:49,320 --> 00:15:52,580
why should you take what you think you coming into account

283
00:15:52,620 --> 00:15:55,360
it needs to be down weighted get two samples that are

284
00:15:55,370 --> 00:15:58,500
close to each other

285
00:16:09,900 --> 00:16:16,270
OK there is no prior and here i'm just trying to evaluate this article

286
00:16:16,980 --> 00:16:18,940
and the piece given

287
00:16:18,950 --> 00:16:21,790
but we can sample from it f is given that this is just

288
00:16:21,800 --> 00:16:25,810
this is just the number

289
00:16:25,820 --> 00:16:28,780
we happen to know what the numbers but it's just the

290
00:16:28,830 --> 00:16:32,620
i did not know a priori

291
00:16:32,780 --> 00:16:38,470
that's one of the problems that we actually

292
00:16:38,480 --> 00:16:41,900
or one of the problems with the procedure that you don't actually say anything about

293
00:16:41,900 --> 00:16:45,760
what the function doing this was just about to illustrate here

294
00:16:45,810 --> 00:16:47,290
let's say you're doing an interval

295
00:16:48,070 --> 00:16:51,370
trying to evaluate what principles of this function

296
00:16:51,380 --> 00:16:54,330
but actually in the procedure you

297
00:16:54,340 --> 00:16:57,220
you actually you don't assume anything about the

298
00:16:57,270 --> 00:16:59,490
function that into matters well look like this

299
00:17:01,320 --> 00:17:03,140
the the somehow

300
00:17:03,160 --> 00:17:03,770
you know

301
00:17:03,870 --> 00:17:07,840
you thing to do this in integrating the green function to be a lot harder

302
00:17:08,380 --> 00:17:12,370
then integrating the function but actually there are treated exactly the same

303
00:17:12,470 --> 00:17:15,470
so somehow we're not using the fact that this is actually

304
00:17:15,490 --> 00:17:16,640
a nice function

305
00:17:16,650 --> 00:17:19,190
agreement not particularly nice

306
00:17:19,200 --> 00:17:23,470
a bit about how to proceed it doesn't use that information about

307
00:17:23,490 --> 00:17:28,890
o'hagan wrote a paper about this which is called monte carlo fundamentally unsound

308
00:17:29,020 --> 00:17:31,620
now the question is how we fix it

309
00:17:31,660 --> 00:17:33,090
we can we can we

310
00:17:33,600 --> 00:17:35,670
and then the procedure

311
00:17:35,680 --> 00:17:37,890
that would turn this into

312
00:17:37,910 --> 00:17:40,300
a proper bayesian inference problem

313
00:17:40,340 --> 00:17:44,780
now says yes nuts-and-bolts health property

314
00:17:44,780 --> 00:17:49,950
so this latent semantic indexing

315
00:17:49,950 --> 00:17:51,970
and maybe

316
00:17:51,980 --> 00:17:54,220
present semantic indexing here or

317
00:17:54,240 --> 00:17:56,920
some of the more recent

318
00:17:56,970 --> 00:18:04,070
world that is in a similar vein like the what you mention but i think

319
00:18:04,070 --> 00:18:05,240
the semantic indexing is

320
00:18:05,690 --> 00:18:06,780
the classic

321
00:18:06,800 --> 00:18:09,140
and and so and it's also i think

322
00:18:09,140 --> 00:18:11,480
nice because it has a very clean

323
00:18:11,540 --> 00:18:17,430
matrix formalisation for linear algebra i was

324
00:18:17,560 --> 00:18:21,650
so i would like see about

325
00:18:21,690 --> 00:18:25,300
the composition that the math behind the semantic index

326
00:18:25,700 --> 00:18:29,180
in what way can be used for dimensionality reduction

327
00:18:29,180 --> 00:18:35,830
and then the whole we use that as latent semantic indexing and information

328
00:18:35,850 --> 00:18:38,940
so we start again with this term document matrix

329
00:18:40,090 --> 00:18:44,900
this matrix is the basis for computing the similarity between documents and queries back in

330
00:18:44,900 --> 00:18:46,890
the vector space model here

331
00:18:47,350 --> 00:18:52,460
so there's this matrix and vector space model is the basis for computing the similarity

332
00:18:52,470 --> 00:18:56,000
between documents and queries because the theory would also be

333
00:18:56,030 --> 00:18:58,640
a column in this metric space

334
00:18:58,640 --> 00:19:03,160
now in this lecture goal is to transform this matrix so that we get a

335
00:19:03,200 --> 00:19:10,410
better measure of similarity between documents and queries

336
00:19:10,430 --> 00:19:15,370
so we want to compute transformation that gives us a better measure of similarity between

337
00:19:15,370 --> 00:19:18,350
doctor doctor appeared

338
00:19:18,370 --> 00:19:24,430
to do that will decompose the term document matrix into a product of matrices

339
00:19:25,600 --> 00:19:29,980
the decomposition uses the singular value decomposition

340
00:19:29,980 --> 00:19:31,240
and looks like this

341
00:19:31,250 --> 00:19:33,560
see the term document matrix

342
00:19:33,580 --> 00:19:38,700
is equal to the product of the matrices using my vt

343
00:19:38,720 --> 00:19:43,750
and then use the SVD to compute a new improved term document matrix which cause

344
00:19:43,810 --> 00:19:45,520
the prime

345
00:19:45,540 --> 00:19:49,830
and we'll get better similarity values of supply

346
00:19:49,850 --> 00:19:54,580
using SVD for this purpose is called latent semantic indexing

347
00:19:54,660 --> 00:19:59,290
i'll use the term document matrix c it's binary so it's not really

348
00:19:59,330 --> 00:20:01,450
the matrix use

349
00:20:01,470 --> 00:20:04,450
if you really wanted to do outside it's just simple

350
00:20:04,540 --> 00:20:09,620
to work with the binary matrix like that

351
00:20:09,640 --> 00:20:13,390
so this is a standard document for

352
00:20:13,390 --> 00:20:18,240
admittedly a very small collection six documents five terms so for example the document you

353
00:20:19,000 --> 00:20:23,660
has three times should or should one

354
00:20:23,680 --> 00:20:27,890
OK i'm not just going to go through the three matrices

355
00:20:27,890 --> 00:20:30,370
i get that compose this matrix c

356
00:20:30,370 --> 00:20:33,950
so i'm going to i decompose this matrix c e

357
00:20:34,000 --> 00:20:36,600
and these are three matrices u sigma v vt

358
00:20:36,620 --> 00:20:38,870
that i get out of the decomposition

359
00:20:38,870 --> 00:20:41,480
so the first matrix is you

360
00:20:41,500 --> 00:20:48,250
it's a square matrix times where n is the number of words in the vocabulary

361
00:20:48,290 --> 00:20:53,020
it's an orthonormal matrix what that means is that the rule that this year all

362
00:20:53,020 --> 00:20:54,350
have unit length

363
00:20:54,390 --> 00:20:59,330
that is each column is represented by a vector that has been

364
00:20:59,370 --> 00:21:04,080
and if any two distinct from vectors orthogonal to each other so we can think

365
00:21:04,080 --> 00:21:07,430
of this as an orthogonal basis for the space because

366
00:21:08,700 --> 00:21:15,180
the term vectors are orthogonal to each other

367
00:21:15,200 --> 00:21:19,910
and we can think of the dimensions as semantic dimensions that capture distinct topics like

368
00:21:19,910 --> 00:21:21,950
politics sports economics

369
00:21:21,980 --> 00:21:26,330
that's why it's called latent semantic indexing because the dimensions

370
00:21:26,350 --> 00:21:28,970
if it works semantic connections

371
00:21:28,970 --> 00:21:31,330
for example the dimension two years

372
00:21:31,410 --> 00:21:34,100
is the wall and dimension

373
00:21:34,120 --> 00:21:35,430
because the water

374
00:21:35,470 --> 00:21:37,270
should vote in ocean

375
00:21:37,290 --> 00:21:39,330
have negative values

376
00:21:39,350 --> 00:21:43,450
and i'm going to call this the land that could also have come from the

377
00:21:43,450 --> 00:21:45,240
world or

378
00:21:45,240 --> 00:21:50,950
a number of common the land grants what intrigued the positive values on this dimension

379
00:21:51,000 --> 00:21:55,020
so this has become the semantic dimensions that distinguishes

380
00:21:55,040 --> 00:21:59,350
whatever its inland ports

381
00:21:59,370 --> 00:22:03,290
and it's not energy in the matrix indicates how strongly related to my eyes to

382
00:22:03,290 --> 00:22:07,730
the topic represented by semantic dimension j that's what just said about these numbers these

383
00:22:07,730 --> 00:22:09,290
tell us

384
00:22:09,310 --> 00:22:13,910
in this case for the day or whatever it's that

385
00:22:13,970 --> 00:22:18,410
this is the second matrix of the decomposition sigma months

386
00:22:18,410 --> 00:22:23,830
as sigma is a square matrix of dimensionality many many times minimax

387
00:22:23,850 --> 00:22:28,750
that is the smaller of the two numbers documents and terms in this case that's

388
00:22:28,750 --> 00:22:31,680
five five five dimension

389
00:22:31,700 --> 00:22:35,220
the diet consists of the singular values of c

390
00:22:35,250 --> 00:22:38,330
these are the singular values

391
00:22:38,370 --> 00:22:44,640
the magnitude of the singular value measures the importance of the corresponding semantic dimensions so

392
00:22:44,680 --> 00:22:46,790
i add dimension

393
00:22:46,810 --> 00:22:51,250
has been problems one point five nine according to this

394
00:22:51,290 --> 00:22:53,080
and make use of

395
00:22:53,100 --> 00:22:55,180
these indicators to me

396
00:22:55,220 --> 00:22:57,740
an important dimensions

397
00:22:57,750 --> 00:23:02,250
so i will say this is an important we can

398
00:23:02,250 --> 00:23:08,420
symbolically described numerical data

399
00:23:12,920 --> 00:23:18,750
understanding of complex event processing what's become very clear over the years is

400
00:23:18,760 --> 00:23:21,220
no as using

401
00:23:21,240 --> 00:23:25,480
what we to call pattern mechanism it is now called machine learning and you have

402
00:23:25,480 --> 00:23:32,610
to say what high of data structure do i want to induce o

403
00:23:32,620 --> 00:23:34,120
so for example

404
00:23:35,420 --> 00:23:42,110
for example that that the hierarchical scene understanding system you might know that in fact

405
00:23:42,490 --> 00:23:46,640
the type of rules i have to that need to infer

406
00:23:46,660 --> 00:23:48,620
something being part of the structure

407
00:23:48,630 --> 00:23:54,850
is that there this attribute inside which is part of a given distances from

408
00:23:54,860 --> 00:24:01,450
others might have additional generation of for relational hashing techniques like for example if i

409
00:24:01,450 --> 00:24:02,950
have these attributes

410
00:24:02,970 --> 00:24:08,030
this parts of these attributes and we have to to this part two other

411
00:24:08,240 --> 00:24:13,930
the relationship no correlations example distances and angles between this part of the public sector

412
00:24:13,950 --> 00:24:17,070
such a problem then what

413
00:24:17,090 --> 00:24:18,860
so this

414
00:24:21,010 --> 00:24:25,410
balance on attributes of generalizations and attributes

415
00:24:25,470 --> 00:24:27,590
almost like being for example

416
00:24:28,000 --> 00:24:31,630
support vector machine or something like that you're going to use a mixture model

417
00:24:31,790 --> 00:24:37,220
but then just little little little estimation techniques controlled

418
00:24:37,240 --> 00:24:38,920
but the fact that got this

419
00:24:38,970 --> 00:24:41,740
data structure

420
00:24:41,750 --> 00:24:43,150
so we got this

421
00:24:43,160 --> 00:24:45,550
this control of numerical

422
00:24:45,570 --> 00:24:51,390
merge or numerical induction why this syntactic properties of the data structure you need to

423
00:24:52,470 --> 00:24:53,830
to guarantee

424
00:24:53,840 --> 00:24:57,030
you know the rules that describe the data

425
00:24:57,040 --> 00:25:00,590
in some forms meaningful for example

426
00:25:00,630 --> 00:25:06,200
the head consists of some eyes nose and mouth usually and you want to have

427
00:25:06,210 --> 00:25:11,150
a description because the the the police officer wants to type in something in english

428
00:25:11,150 --> 00:25:15,640
to clear the data bytes

429
00:25:15,660 --> 00:25:23,110
although many many numerical machine learning techniques are provably lovely

430
00:25:23,120 --> 00:25:25,770
they don't often dispose themselves or

431
00:25:25,780 --> 00:25:27,960
initially description

432
00:25:27,970 --> 00:25:29,970
in finite time

433
00:25:31,500 --> 00:25:35,930
so that's the this one trying to get out of this is the different literature

434
00:25:35,980 --> 00:25:38,130
and what you may think of it

435
00:25:38,150 --> 00:25:41,110
traditional machine learning traditional machine learning mean

436
00:25:41,120 --> 00:25:43,370
machine learning in the last fifteen years

437
00:25:43,380 --> 00:25:45,790
present US fifty four hundred years

438
00:25:47,980 --> 00:25:49,130
in traditional

439
00:25:49,370 --> 00:25:53,200
normalisation because you simply this is the type of real you you've probably heard of

440
00:25:53,910 --> 00:25:57,150
if is activities in certain classes

441
00:25:57,200 --> 00:25:58,710
the because

442
00:25:58,720 --> 00:26:00,910
and this is useless

443
00:26:00,920 --> 00:26:01,990
in most

444
00:26:02,040 --> 00:26:03,740
image understanding problems

445
00:26:03,750 --> 00:26:06,350
because when we come to classify

446
00:26:06,740 --> 00:26:11,200
my head all my body moving here in the complex in the what is missing

447
00:26:11,200 --> 00:26:12,390
data and all that stuff

448
00:26:12,430 --> 00:26:14,590
what's important

449
00:26:14,610 --> 00:26:20,370
all the data the thing that part of the class

450
00:26:22,270 --> 00:26:25,780
so i'm going to find a subset of data the classifier but how do i

451
00:26:25,780 --> 00:26:28,720
choose on my goodness is not going to work

452
00:26:28,740 --> 00:26:32,370
so this techniques great if i know what i'm looking for

453
00:26:32,460 --> 00:26:34,430
we have precompiled

454
00:26:34,510 --> 00:26:36,100
the search

455
00:26:36,900 --> 00:26:42,250
to emphasise again so once signing is think carefully about the data structure

456
00:26:42,250 --> 00:26:48,460
you want to infer about or learn about when you're trying to solve the problem

457
00:26:48,460 --> 00:26:52,330
of understanding and predicting about complex signals

458
00:26:52,370 --> 00:26:55,140
i'm going to talk about that more and more

459
00:26:55,160 --> 00:27:01,070
so that structures are not rich enough and generalizations are not relational

460
00:27:01,090 --> 00:27:05,400
my you are pushing extreme case because of my interest to do that cause many

461
00:27:05,400 --> 00:27:10,130
problems with the article was sold by activity induction whatever

462
00:27:10,170 --> 00:27:12,910
and in fact relevant to the previous lecture

463
00:27:14,610 --> 00:27:18,280
how much variance is explained by the first sort of statistics how much is explained

464
00:27:18,280 --> 00:27:22,410
by the second how much is explained by to probably lot

465
00:27:22,430 --> 00:27:26,870
i don't know i don't want to take this is an oversell fully aware

466
00:27:26,870 --> 00:27:33,360
the the first order information is often very very

467
00:27:33,380 --> 00:27:40,700
workflow in fact some professor of english at university newcastle who got towards simply showing

468
00:27:41,000 --> 00:27:48,500
the amount of prediction of of classical authors like actual photos or whatever

469
00:27:48,500 --> 00:27:54,090
milo completed simply by the frequency with

470
00:27:54,260 --> 00:27:59,090
i can't think of the binary is an old man to the freedom in the

471
00:28:00,750 --> 00:28:06,180
so is probably going to try and emphasise the going rate for all how to

472
00:28:06,180 --> 00:28:08,470
summarize relational data

473
00:28:08,500 --> 00:28:11,900
and so summarize what direction that i have to be able to sort of match

474
00:28:11,910 --> 00:28:13,950
things to summarisation

475
00:28:13,970 --> 00:28:16,800
the matching problem in trying to summarise

476
00:28:16,860 --> 00:28:20,300
so then i will probably to want to end induce over the past

477
00:28:21,490 --> 00:28:24,000
there's a problem with missing data

478
00:28:24,010 --> 00:28:27,170
this problem of by the way when you with learning

479
00:28:27,840 --> 00:28:29,730
the problem when you recognise

480
00:28:29,740 --> 00:28:31,950
in our case is usually

481
00:28:31,980 --> 00:28:34,160
what you recognizing even better

482
00:28:34,180 --> 00:28:39,670
in many many other things called the same

483
00:28:41,590 --> 00:28:45,160
it turns out this is the context of the work

484
00:28:45,180 --> 00:28:50,740
so it depends on problem's structure is attributed graphs

485
00:28:51,090 --> 00:28:53,910
this literature over the last fifty years

486
00:28:53,920 --> 00:28:55,120
the most common

487
00:28:55,130 --> 00:29:00,650
this representation of the data structure we work with is basically an attributed graph so

488
00:29:00,670 --> 00:29:05,980
or features you're playing with whether it's my or an edge something like this is

489
00:29:05,980 --> 00:29:08,450
like fifty vertex

490
00:29:08,460 --> 00:29:12,430
and the vertex set attributes for example orientation

491
00:29:13,220 --> 00:29:18,570
hyperspectral signature infrared heat when i was going to be

492
00:29:18,620 --> 00:29:23,090
can a whole lot attributes could including position if you like

493
00:29:23,110 --> 00:29:24,540
not necessarily

494
00:29:24,550 --> 00:29:27,960
so the sentence

495
00:29:27,960 --> 00:29:32,090
she gives a few moments for questions

496
00:29:32,780 --> 00:29:42,500
so what

497
00:29:45,320 --> 00:29:46,780
was this

498
00:29:54,090 --> 00:29:59,040
first thing called SIFT trace

499
00:29:59,050 --> 00:30:04,070
there are various things that can do the same as the last the trace has

500
00:30:04,070 --> 00:30:06,780
sort of teachers based on some of the work

501
00:30:06,800 --> 00:30:09,320
and the idea of what can happen when

502
00:30:09,360 --> 00:30:15,440
that stuff is mostly not yet in the mainstream can still argument going on about

503
00:30:15,440 --> 00:30:18,690
how it should really symbol table

504
00:30:19,770 --> 00:30:23,070
the rest of the

505
00:30:23,110 --> 00:30:26,110
there are various other traits tells

506
00:30:26,570 --> 00:30:30,650
there is a kernel debugger which

507
00:30:33,940 --> 00:30:39,860
this is very funny about dividing tells they tend to live in the moscow

508
00:30:39,860 --> 00:30:42,420
which is also very diverse the debugging anyway

509
00:30:42,440 --> 00:30:50,840
that's almost as easily the most useful kernel debugging tool we have these days for

510
00:30:50,840 --> 00:30:53,880
things that are not part of specific bugs

511
00:30:53,920 --> 00:30:56,630
if we go virtualization

512
00:30:56,670 --> 00:31:00,190
so you can run it can learn something like you have the impression that it

513
00:31:00,190 --> 00:31:03,190
crashes without revealing machine

514
00:31:03,190 --> 00:31:07,280
and you can benefit you get to make the emulator do strange things

515
00:31:07,420 --> 00:31:09,500
in certain cases

516
00:31:09,710 --> 00:31:16,460
people also depends on you've got a funny happening come already does very occasionally

517
00:31:16,480 --> 00:31:19,420
you can write small emulator

518
00:31:19,440 --> 00:31:21,570
from the drive into the emulation

519
00:31:21,730 --> 00:31:23,940
tracy when you press the button

520
00:31:23,940 --> 00:31:25,280
bad things happen

521
00:31:25,300 --> 00:31:30,900
press the button on the hard disk pretends to crash test support

522
00:31:30,960 --> 00:31:33,800
i told that this user but minutes

523
00:31:34,460 --> 00:31:37,150
because all of the

524
00:31:37,190 --> 00:31:41,630
well everything that runs in user space you can actually GDP

525
00:31:41,730 --> 00:31:46,940
so you get the single step a kernel new space running out of one of

526
00:31:46,960 --> 00:31:51,770
which is extremely useful certain kinds of things

527
00:31:53,150 --> 00:31:55,440
the the main thing

528
00:31:55,460 --> 00:32:04,520
the best thing i never driver debugging used area code and occasionally large quantities of

529
00:32:04,550 --> 00:32:07,920
sometimes it is necessary to get into the same state as the guy who designed

530
00:32:11,860 --> 00:32:14,670
the but are pretty much essential because

531
00:32:14,690 --> 00:32:19,880
the documentation for hardware really agrees with what they actually does

532
00:32:19,940 --> 00:32:23,690
so you get but you cannot possibly find any

533
00:32:23,750 --> 00:32:28,980
that's one of the cases where i use the kernel debugging which

534
00:32:29,000 --> 00:32:32,250
it's sometimes one cases where you specialization

535
00:32:32,380 --> 00:32:37,920
at times of even when using the program to just poke around in because you

536
00:32:37,920 --> 00:32:41,360
next PCA space from user space and CPU

537
00:32:41,420 --> 00:32:44,570
so you can see how to read and write it in user space

538
00:32:44,590 --> 00:32:49,320
these are used for delisting this in this space doesn't

539
00:32:49,340 --> 00:32:50,940
maybe try

540
00:32:50,960 --> 00:32:55,420
previously selected registered to get the right answer

541
00:32:55,480 --> 00:32:58,460
and some of it is just magic sorry

542
00:33:00,460 --> 00:33:04,090
we have many better way of doing it

543
00:33:26,040 --> 00:33:30,780
there are several things can not just specific process before

544
00:33:30,800 --> 00:33:33,480
the intel four six

545
00:33:34,440 --> 00:33:35,690
three six

546
00:33:35,730 --> 00:33:41,190
is the only case where you knew you may not have hardware floating point

547
00:33:41,210 --> 00:33:44,300
it also has some memory management differences to do

548
00:33:45,040 --> 00:33:49,590
whether the right protect networks from kernel space

549
00:33:49,650 --> 00:33:54,570
and intrinsic motivation very complicated particularly when threading

550
00:33:54,690 --> 00:33:57,920
the whole set of other cases to deal with things like this in user space

551
00:33:57,920 --> 00:34:01,460
memory which are really are

552
00:34:01,500 --> 00:34:05,880
so in theory you drop just three six four actually doing is dropping

553
00:34:05,940 --> 00:34:08,480
anything below four eight six

554
00:34:08,500 --> 00:34:11,710
and i believe also

555
00:34:12,540 --> 00:34:16,250
the next gen five eight six process

556
00:34:16,250 --> 00:34:21,150
which is so obscure i only ever had one but report

557
00:34:21,380 --> 00:34:22,800
every scene

558
00:34:23,130 --> 00:34:29,340
so it's not like it takes like twenty seconds before

559
00:34:30,670 --> 00:34:35,730
every other press out outlets is and is capable of pretending to to be

560
00:34:38,780 --> 00:34:46,320
well they will have to drop four six foot six love harder because the process

561
00:34:46,340 --> 00:34:50,050
today which are not quite twenty

562
00:34:50,050 --> 00:34:53,960
and possibly fixing the mistakes of the perceptron why not

563
00:34:54,010 --> 00:34:59,580
trying to make an update that the force is large marching on the current that

564
00:34:59,830 --> 00:35:02,910
but every time we make a mistake

565
00:35:02,920 --> 00:35:04,900
on the kind of the example

566
00:35:04,970 --> 00:35:07,660
we making up to that

567
00:35:07,680 --> 00:35:14,020
cost us who achieved the imagine a list of the one if that example comes

568
00:35:18,130 --> 00:35:19,960
so this is people

569
00:35:19,970 --> 00:35:23,930
this is of course i mean more aggressive updates

570
00:35:23,970 --> 00:35:25,830
and there

571
00:35:27,820 --> 00:35:32,240
so you might be interested to see whether it is because

572
00:35:32,260 --> 00:35:35,610
easy to compute the canonical solution

573
00:35:35,660 --> 00:35:36,820
in the

574
00:35:36,830 --> 00:35:40,040
we can view it as a sort of

575
00:35:40,050 --> 00:35:45,320
online SVM and says we are first-order theorem in the sense that we are

576
00:35:45,370 --> 00:35:51,340
imposing an SVM SVM band of solution on a on the single currency example and

577
00:35:51,340 --> 00:35:53,100
not the whole

578
00:35:53,160 --> 00:35:57,220
we're interested in getting incrementally patients

579
00:35:58,100 --> 00:36:00,810
let's see

580
00:36:00,900 --> 00:36:05,510
the as is the following

581
00:36:05,610 --> 00:36:08,110
or being

582
00:36:08,120 --> 00:36:13,440
a large margin

583
00:36:13,490 --> 00:36:15,180
on a single example

584
00:36:15,200 --> 00:36:17,650
so i suppose that

585
00:36:17,860 --> 00:36:22,350
we made a mistake

586
00:36:22,360 --> 00:36:24,100
the the current i

587
00:36:24,210 --> 00:36:26,820
without making assumptions right now

588
00:36:27,100 --> 00:36:36,630
the fact that this is actually what i wanted

589
00:36:36,670 --> 00:36:42,840
i will be correctly referred i said the wrong thing suppose that

590
00:36:46,130 --> 00:36:48,180
know it's it's correct

591
00:36:48,200 --> 00:36:51,840
we want to make it happen whenever the our lives in a small

592
00:36:51,850 --> 00:36:55,290
whenever they have more than one

593
00:36:55,310 --> 00:36:57,720
not whatever make mistakes

594
00:37:00,290 --> 00:37:03,020
which is what i was going to write

595
00:37:03,030 --> 00:37:09,210
this is the condition that triggers an update but now is not a mistake but

596
00:37:09,210 --> 00:37:12,710
if it was more like that we don't have enough

597
00:37:16,230 --> 00:37:19,510
on enough confidence on our classification

598
00:37:19,560 --> 00:37:24,890
this is now the new condition for an update not been made another mistake but

599
00:37:25,080 --> 00:37:27,200
will much more than one

600
00:37:27,250 --> 00:37:32,970
and then we would like to make an update that forces

601
00:37:33,240 --> 00:37:39,140
large and i think i think i

602
00:37:39,160 --> 00:37:41,890
but this is updated

603
00:37:43,240 --> 00:37:45,920
and this is the the

604
00:37:46,060 --> 00:37:47,860
the kind of examples

605
00:37:47,870 --> 00:37:51,040
and we want it because one

606
00:37:53,370 --> 00:37:58,730
this is equivalent to say in our vision that the loss

607
00:37:58,750 --> 00:38:04,290
all that we use the hinge loss of w one

608
00:38:04,300 --> 00:38:08,860
he began iraq

609
00:38:12,460 --> 00:38:13,650
how do we do this

610
00:38:13,660 --> 00:38:14,760
i mean there are

611
00:38:14,810 --> 00:38:17,940
one easy way to get the form of the solution i propose that we can

612
00:38:17,940 --> 00:38:21,030
write the solution of

613
00:38:21,080 --> 00:38:24,250
we can write the

614
00:38:24,300 --> 00:38:33,450
the updating viewers former

615
00:38:34,750 --> 00:38:39,280
we have to use such process update but we add the

616
00:38:39,300 --> 00:38:41,700
a learning rate it

617
00:38:41,710 --> 00:38:47,950
now suppose that we can write after this format let's see if we can

618
00:38:47,960 --> 00:38:52,600
if you can find in a closed form expression for it

619
00:38:53,300 --> 00:38:54,300
so now

620
00:38:54,320 --> 00:38:56,600
we now like this

621
00:38:56,610 --> 00:38:58,790
definition in this condition here

622
00:38:58,800 --> 00:39:01,900
and what we get is why the

623
00:39:01,910 --> 00:39:09,100
WP minus one plus it that they were x being

624
00:39:10,730 --> 00:39:14,230
this is a vector just being the bar

625
00:39:14,330 --> 00:39:18,000
because we are now

626
00:39:18,020 --> 00:39:21,490
and we want to say OK let's say that we want to achieve exactly matching

627
00:39:21,490 --> 00:39:23,950
one so we want to have

628
00:39:24,000 --> 00:39:26,530
the update that

629
00:39:26,540 --> 00:39:30,850
exactly brings the margin of one

630
00:39:30,860 --> 00:39:32,440
for the new vector

631
00:39:35,400 --> 00:39:39,920
so now this can be written equivalently

632
00:39:39,930 --> 00:39:41,990
many we can we can it

633
00:39:42,000 --> 00:39:48,910
at this point so we have the what i think that the minus one x

634
00:39:48,960 --> 00:39:51,720
but it that it

635
00:39:55,100 --> 00:39:58,860
what he and why the square this is one

636
00:39:58,910 --> 00:40:03,280
so we have to be more of x squared

637
00:40:03,370 --> 00:40:05,680
equal one

638
00:40:06,390 --> 00:40:07,580
which now

639
00:40:07,600 --> 00:40:10,730
there is also the solution for

640
00:40:10,820 --> 00:40:13,110
is exactly

641
00:40:13,590 --> 00:40:20,660
one minus y and w one minus one x by the by

642
00:40:22,000 --> 00:40:24,080
x where the

643
00:40:24,210 --> 00:40:27,760
since we make it happen whenever

644
00:40:27,820 --> 00:40:29,840
commission is true

645
00:40:29,890 --> 00:40:33,250
this equivalence can be written as it were

646
00:40:34,910 --> 00:40:36,120
l v

647
00:40:36,130 --> 00:40:38,270
all that matters

648
00:40:38,400 --> 00:40:41,360
by the by norm of x is squared

649
00:40:41,370 --> 00:40:42,570
so now

650
00:40:43,030 --> 00:40:47,010
you see that what she them and you know one

651
00:40:47,020 --> 00:40:49,270
on on the on the same example

652
00:40:49,320 --> 00:40:51,940
it's enough to make an update

653
00:40:51,990 --> 00:40:53,100
learning with which

654
00:40:53,110 --> 00:40:57,780
the perceptron update with the learning rate which is equal to the hinge loss

655
00:40:57,830 --> 00:40:59,580
of the past guy

656
00:40:59,630 --> 00:41:05,390
on the current example divided by the square of

657
00:41:08,500 --> 00:41:13,100
so what is this dramatically how can you help can view this

658
00:41:13,120 --> 00:41:14,920
it's very simple no

659
00:41:18,290 --> 00:41:19,890
you can

660
00:41:19,900 --> 00:41:23,550
we will we

661
00:41:23,560 --> 00:41:29,010
we'll see later in the more general think that this the solution

662
00:41:30,470 --> 00:41:33,290
is that it is up here with

663
00:41:33,310 --> 00:41:34,810
form of

664
00:41:34,820 --> 00:41:36,430
it is equivalent to the

665
00:41:36,550 --> 00:41:40,000
solution of the following

666
00:41:40,010 --> 00:41:42,130
one is that problem

667
00:41:42,150 --> 00:41:43,720
which is

668
00:41:43,730 --> 00:41:46,720
this so w

669
00:42:02,580 --> 00:42:05,570
that the man of one minus the real

670
00:42:06,820 --> 00:42:10,450
such that

671
00:42:10,460 --> 00:42:11,660
such that

672
00:42:11,710 --> 00:42:15,410
and then we read the model conditional on x

673
00:42:15,460 --> 00:42:17,390
which is why

674
00:42:18,760 --> 00:42:19,860
x b

675
00:42:19,910 --> 00:42:22,040
the other one

676
00:42:22,050 --> 00:42:24,420
but this is

677
00:42:24,430 --> 00:42:25,950
this is the

678
00:42:25,970 --> 00:42:32,280
first of all the one and so we have to way

679
00:42:32,280 --> 00:42:34,090
now we could actually

680
00:42:35,530 --> 00:42:38,300
remove that offset and actually

681
00:42:38,320 --> 00:42:42,680
i had one extra component in this vector which would be constant for all the

682
00:42:42,680 --> 00:42:48,220
training examples and then effectively that b could have been wrapped into the w here

683
00:42:48,280 --> 00:42:52,930
that would make a slightly different problem because the b noticed doesn't occur in the

684
00:42:52,970 --> 00:42:55,090
optimisation criterion

685
00:42:55,110 --> 00:43:00,070
but it will be a minor differences in the b would then be part of

686
00:43:00,070 --> 00:43:02,260
this w square here

687
00:43:02,660 --> 00:43:03,930
so the

688
00:43:03,950 --> 00:43:05,300
you know whether you

689
00:43:05,320 --> 00:43:08,920
it's a matter of choice as to whether you like to keep that b

690
00:43:09,150 --> 00:43:12,030
or not if you if you're keen on it

691
00:43:12,050 --> 00:43:14,970
it sort of feels right if you think you of the margins

692
00:43:15,760 --> 00:43:17,180
you end up

693
00:43:17,180 --> 00:43:18,650
with this additional

694
00:43:20,590 --> 00:43:23,280
if not then that constrain disappears

695
00:43:25,610 --> 00:43:32,240
when you solve this you know what the weight vector is some alpha i y

696
00:43:33,340 --> 00:43:34,800
five exercise

697
00:43:35,220 --> 00:43:38,420
you don't know what the threshold is the but you can work it out by

698
00:43:38,420 --> 00:43:43,630
looking at the border examples now i'll talk about the border examples a little later

699
00:43:43,630 --> 00:43:44,630
because i

700
00:43:44,650 --> 00:43:49,900
i don't want to throw too many sort of distractions at you but that

701
00:43:49,920 --> 00:43:52,400
you know you should give you feel that OK

702
00:43:52,430 --> 00:43:54,740
we can solve this problem

703
00:43:54,760 --> 00:43:57,720
by finding the alphas rather than finding the weights

704
00:43:58,420 --> 00:44:01,900
once we got the alphas we can do the classification because we know

705
00:44:01,930 --> 00:44:04,510
how to compute it we we actually

706
00:44:04,530 --> 00:44:05,740
did that

707
00:44:06,740 --> 00:44:09,090
some outcry

708
00:44:09,090 --> 00:44:13,240
you know in about five x five x OK so so for everything we've done

709
00:44:13,240 --> 00:44:15,430
is just rewritten

710
00:44:15,430 --> 00:44:16,130
you know

711
00:44:16,150 --> 00:44:18,740
the problem in terms of these alphas

712
00:44:18,760 --> 00:44:22,010
we haven't got around the problem of the high dimensional space yet

713
00:44:22,530 --> 00:44:26,300
OK so now the crunch comes that

714
00:44:26,320 --> 00:44:29,280
we can actually imagine a situation

715
00:44:30,320 --> 00:44:31,530
rather than

716
00:44:32,030 --> 00:44:37,430
making the projection explicitly of two points and computing the inner product we might have

717
00:44:37,430 --> 00:44:39,740
a shortcut functions

718
00:44:39,760 --> 00:44:42,800
so this might be a function that somehow cleverly

719
00:44:42,840 --> 00:44:49,180
computes that inner product of those two projections without actually doing them

720
00:44:49,200 --> 00:44:51,900
now if we had that

721
00:44:51,920 --> 00:44:55,130
we're going to call it a kernel function k

722
00:44:56,220 --> 00:45:01,380
suggestion that we might be able to find otherwise what we call common methods

723
00:45:02,340 --> 00:45:06,860
let's imagine we had one OK so how we use it OK so

724
00:45:07,360 --> 00:45:10,300
so it's wrong quite well

725
00:45:10,320 --> 00:45:14,420
here we have this inner product between the projection of x i and x j

726
00:45:14,420 --> 00:45:18,700
OK we just apply kernel function to exon xj we've got about

727
00:45:20,030 --> 00:45:22,130
so noticed by doing that we

728
00:45:22,150 --> 00:45:24,340
we shortcut it having to do

729
00:45:24,590 --> 00:45:26,720
and now when we want to evaluate

730
00:45:26,740 --> 00:45:30,930
same thing with just got to use be able to evaluate the inner product between

731
00:45:30,930 --> 00:45:34,130
x i the new example apply the kernel function

732
00:45:34,150 --> 00:45:39,320
we can compute that again without actually explicitly computing in the high dimensional space

733
00:45:39,340 --> 00:45:44,610
so basically we've got this trick if function exists we can now

734
00:45:45,570 --> 00:45:49,050
the support vector algorithm in the feature space

735
00:45:49,200 --> 00:45:51,780
in high dimensional feature space without ever

736
00:45:51,840 --> 00:45:54,400
actually computing a single vector

737
00:45:54,420 --> 00:45:55,610
in that space

738
00:45:55,630 --> 00:45:59,450
so it's sort of magic in the way i mean it is remarkable

739
00:45:59,450 --> 00:46:01,090
kind of

740
00:46:01,110 --> 00:46:04,260
trick you know you're working in this space without actually ever

741
00:46:04,280 --> 00:46:05,590
however using it

742
00:46:05,610 --> 00:46:07,050
i come

743
00:46:07,070 --> 00:46:11,780
one analogy i have not sure is good but anyway is this you know when

744
00:46:12,180 --> 00:46:15,800
someone who was sent to kill medusa and if you looked at

745
00:46:15,800 --> 00:46:20,160
the deal with all makes the hair and so on then you to understand so

746
00:46:20,630 --> 00:46:24,900
you can't actually work in this space because it's too complex you you're just going

747
00:46:24,900 --> 00:46:28,450
to be you know killed by the complexity but you can somehow look at it

748
00:46:28,450 --> 00:46:30,010
through a mirror and

749
00:46:30,070 --> 00:46:35,050
and the mirror is the kernel function somehow gives you enough information to work in

750
00:46:35,050 --> 00:46:36,570
space without actually

751
00:46:37,090 --> 00:46:39,970
you know getting bogged down

752
00:46:41,700 --> 00:46:45,550
so that's the you know what one would like to do and i'm going to

753
00:46:45,550 --> 00:46:48,470
give you an example to show that it does actually

754
00:46:48,510 --> 00:46:52,880
happened in the example we had OK case so here is this

755
00:46:52,920 --> 00:46:58,650
quadratic projection function that we were looking at which takes the vector and projected into

756
00:46:58,650 --> 00:46:59,950
these quadratic

757
00:46:59,970 --> 00:47:03,610
OK well let's look at what the inner product between two

758
00:47:03,650 --> 00:47:06,930
vectors projected into this space is

759
00:47:06,950 --> 00:47:08,510
OK so

760
00:47:08,530 --> 00:47:13,510
i've written it so this is a bit a sort of you know the kind

761
00:47:14,110 --> 00:47:19,680
the derivation but i mean ignore that this is just the outer product matrix of

762
00:47:19,680 --> 00:47:22,030
x next prime do you can think of that

763
00:47:22,050 --> 00:47:25,570
you know putting these into a matrix so x one squared x one x two

764
00:47:25,570 --> 00:47:27,280
and x two x one

765
00:47:27,300 --> 00:47:32,400
x two squared you know below it then that's that's matrix and similarly for z

766
00:47:32,550 --> 00:47:36,950
prime and then the inner product between these two vectors corresponds to the frobenius inner

767
00:47:36,950 --> 00:47:39,180
product between these two matrices

768
00:47:41,070 --> 00:47:43,990
just notation nothing nothing

769
00:47:44,010 --> 00:47:47,740
OK and the frobenius inner product can be written as the trace of

770
00:47:47,760 --> 00:47:51,550
the product of the two matrices while the transpose of one and the other

771
00:47:51,570 --> 00:47:54,570
and then when you're into traces you can

772
00:47:54,590 --> 00:47:59,590
movies around and take x to the end and you just have ex-prime z

773
00:47:59,610 --> 00:48:02,780
times the context so prime is the transpose

774
00:48:02,800 --> 00:48:05,180
so this is just the number of times the number

775
00:48:05,200 --> 00:48:07,530
so the trace is just

776
00:48:07,550 --> 00:48:11,110
traces just justice on the diagonal elements if you only have one by one matrix

777
00:48:11,130 --> 00:48:12,320
symbol number

778
00:48:12,340 --> 00:48:16,340
so this is just the inner product between x and z squared OK if you

779
00:48:16,340 --> 00:48:20,300
don't like the derivation you can write it out and you know just

780
00:48:20,320 --> 00:48:24,930
go through and it it comes out but basically what this means is that

781
00:48:24,950 --> 00:48:28,260
we can actually evaluate this inner product

782
00:48:28,280 --> 00:48:32,740
by evaluating the inner product in the input space and then just squaring

783
00:48:32,740 --> 00:48:36,830
so for mining

784
00:48:36,850 --> 00:48:39,380
thank you

785
00:48:39,400 --> 00:48:42,000
i want to know

786
00:48:49,670 --> 00:48:53,480
we might be better

787
00:48:54,930 --> 00:49:00,730
what is the correct functions are

788
00:49:00,740 --> 00:49:02,130
in cold

789
00:49:06,140 --> 00:49:07,800
and the figure out

790
00:49:08,950 --> 00:49:11,180
language crisp functions

791
00:49:11,930 --> 00:49:15,280
one to tool that is that function

792
00:49:18,120 --> 00:49:20,290
and when you use them all

793
00:49:21,370 --> 00:49:24,180
then you can optimize the

794
00:49:26,500 --> 00:49:29,900
you might not like the fact

795
00:49:34,720 --> 00:49:37,750
sort of

796
00:49:37,770 --> 00:49:40,560
agree to it's quite important

797
00:49:42,710 --> 00:49:45,250
now in its being done more

798
00:49:45,780 --> 00:49:48,680
it was a new for example where you know the

799
00:49:48,700 --> 00:49:50,260
it's right panel

800
00:49:50,270 --> 00:49:52,560
i think that

801
00:49:52,860 --> 00:49:53,600
i think

802
00:49:53,620 --> 00:49:57,920
something is happening in the future is that we have a lot more

803
00:49:57,940 --> 00:50:03,140
grand functions extract information to

804
00:50:03,160 --> 00:50:04,130
modeling work

805
00:50:06,880 --> 00:50:12,340
you don't show you more realistic really is

806
00:50:17,030 --> 00:50:19,630
so you can get

807
00:50:19,640 --> 00:50:20,710
out of the

808
00:50:20,720 --> 00:50:21,800
and you can get

809
00:50:21,820 --> 00:50:24,010
if different models are already known literature

810
00:50:24,650 --> 00:50:25,710
last words

811
00:50:25,860 --> 00:50:27,800
so the final

812
00:50:34,520 --> 00:50:42,380
it was time to get to the just talk about a few people on

813
00:50:45,020 --> 00:50:47,280
minimize the second half

814
00:50:50,860 --> 00:50:54,010
approximate inference for constable constellation

815
00:50:54,030 --> 00:50:57,040
the data on this very very beautiful

816
00:50:58,420 --> 00:51:00,330
for gases

817
00:51:00,440 --> 00:51:03,450
all this user goes what

818
00:51:05,100 --> 00:51:07,320
have of knowledge

819
00:51:11,100 --> 00:51:12,810
the b

820
00:51:12,830 --> 00:51:16,960
so what we're going to do is going to approximately by

821
00:51:16,970 --> 00:51:20,410
things are really think

822
00:51:20,420 --> 00:51:24,720
so this seems to me is trivial thing

823
00:51:24,900 --> 00:51:27,570
but it turns out that the nice

824
00:51:27,620 --> 00:51:29,590
you can

825
00:51:29,610 --> 00:51:31,010
twenty nine thousand

826
00:51:31,880 --> 00:51:36,340
in particular

827
00:51:36,670 --> 00:51:44,270
so there are lots of proposed for classification and what is called the expectation propagation

828
00:51:44,290 --> 00:51:47,480
what about to that this is

829
00:51:48,720 --> 00:51:52,210
there's not a lot of features

830
00:51:52,220 --> 00:51:53,260
is roughly

831
00:51:53,320 --> 00:51:57,390
our goal

832
00:52:00,110 --> 00:52:01,150
all right so quite

833
00:52:01,160 --> 00:52:03,900
question just two

834
00:52:03,910 --> 00:52:06,660
the company

835
00:52:06,890 --> 00:52:10,950
one house was relation to read it

836
00:52:11,940 --> 00:52:14,280
one slide show you

837
00:52:14,290 --> 00:52:18,640
construction which looks very much like this which network

838
00:52:18,650 --> 00:52:20,810
except that about everywhere

839
00:52:20,820 --> 00:52:22,460
not just the french

840
00:52:24,770 --> 00:52:26,910
i do the rest

841
00:52:26,930 --> 00:52:28,210
but model

842
00:52:28,270 --> 00:52:30,180
it's going to be useful in combination

843
00:52:30,190 --> 00:52:30,920
want to

844
00:52:30,930 --> 00:52:32,400
that's box

845
00:52:32,410 --> 00:52:34,730
he said

846
00:52:34,740 --> 00:52:35,960
i over

847
00:52:37,330 --> 00:52:39,930
grows have

848
00:52:40,670 --> 00:52:43,710
we're trying to model

849
00:52:45,220 --> 00:52:51,580
and if we get higher

850
00:52:51,600 --> 00:52:53,050
then he

851
00:52:53,070 --> 00:52:54,700
so what we get

852
00:52:54,710 --> 00:52:55,690
at the

853
00:52:57,330 --> 00:52:58,590
you can do little

854
00:52:58,730 --> 00:53:00,760
iteration to find

855
00:53:00,780 --> 00:53:02,970
well what would the what is the corresponding

856
00:53:02,990 --> 00:53:06,420
i mean what was

857
00:53:06,440 --> 00:53:08,560
four days before constructing

858
00:53:08,580 --> 00:53:10,280
this property

859
00:53:10,420 --> 00:53:12,280
and such

860
00:53:12,330 --> 00:53:15,490
it would take some here comes

861
00:53:16,730 --> 00:53:18,690
now i have these

862
00:53:18,870 --> 00:53:20,870
that is

863
00:53:23,200 --> 00:53:24,160
number of

864
00:53:24,370 --> 00:53:26,120
the next five years

865
00:53:26,130 --> 00:53:29,070
and i just prior in the sense that they all

866
00:53:29,090 --> 00:53:30,500
drawn independently

867
00:53:30,550 --> 00:53:32,550
from about

868
00:53:32,570 --> 00:53:35,690
this just like noise

869
00:53:38,850 --> 00:53:43,700
so i already the fact that the distribution functions

870
00:53:43,750 --> 00:53:45,940
which is defined by the city

871
00:53:47,030 --> 00:53:48,920
it's going to cost

872
00:53:48,970 --> 00:53:51,770
and then compute the mean function is great

873
00:53:52,660 --> 00:53:54,200
is distracted

874
00:53:54,880 --> 00:53:56,220
i mean function

875
00:53:56,230 --> 00:53:56,990
it is

876
00:53:57,010 --> 00:53:59,520
is the average of

877
00:53:59,650 --> 00:54:02,250
some particular location x

878
00:54:02,270 --> 00:54:04,100
so next year fixed

879
00:54:04,140 --> 00:54:08,270
the thing adding more is the process

880
00:54:08,290 --> 00:54:10,800
during the

881
00:54:13,530 --> 00:54:15,770
we have here

882
00:54:15,790 --> 00:54:18,280
so we need to ensure or

883
00:54:18,300 --> 00:54:24,250
that year

884
00:54:24,270 --> 00:54:26,070
so this part

885
00:54:26,090 --> 00:54:27,140
and and ongoing

886
00:54:30,970 --> 00:54:32,370
it is

887
00:54:32,390 --> 00:54:34,050
seventy five

888
00:54:34,070 --> 00:54:34,970
looking up

889
00:54:35,090 --> 00:54:36,490
i don't know

890
00:54:36,500 --> 00:54:38,750
it's just a call for

891
00:54:42,550 --> 00:54:43,720
that year

892
00:54:46,970 --> 00:54:49,780
one of gas but OK lot

893
00:54:50,170 --> 00:54:57,030
but just because symmetry this as much as other ways to make it work

894
00:54:57,110 --> 00:54:59,720
not surprised

895
00:54:59,760 --> 00:55:03,140
more interesting to try to compute the

896
00:55:03,580 --> 00:55:05,340
so this is now

897
00:55:05,530 --> 00:55:08,540
tation times some x

898
00:55:08,560 --> 00:55:12,120
this is the right here

899
00:55:12,160 --> 00:55:14,450
and in this situation now

900
00:55:14,580 --> 00:55:17,480
random fluctuations

901
00:55:17,490 --> 00:55:22,170
it's only too much time together to

902
00:55:22,180 --> 00:55:23,870
these two terms are

903
00:55:23,890 --> 00:55:25,780
because independence

904
00:55:25,820 --> 00:55:28,400
the that you

905
00:55:28,520 --> 00:55:31,780
the only contribution the same

906
00:55:32,090 --> 00:55:34,620
you need to know about this guy

907
00:55:34,660 --> 00:55:36,540
together with

908
00:55:36,550 --> 00:55:37,640
same for

909
00:55:39,700 --> 00:55:41,440
things that

910
00:55:41,480 --> 00:55:44,040
to do it

911
00:55:44,050 --> 00:55:46,360
garcia is

912
00:55:46,380 --> 00:55:47,710
and i just get a

913
00:55:50,010 --> 00:55:53,940
but the ratio of this

914
00:55:53,950 --> 00:55:56,900
but i was in school

915
00:55:56,920 --> 00:55:59,360
e to the process know

916
00:55:59,380 --> 00:56:01,120
and some at times

917
00:56:02,250 --> 00:56:03,590
x five

918
00:56:03,630 --> 00:56:07,960
i can be described

919
00:56:08,560 --> 00:56:12,980
the rest of the functions transferred to get

920
00:56:13,000 --> 00:56:14,000
when you

921
00:56:14,100 --> 00:56:16,690
i assume that have just about

922
00:56:16,690 --> 00:56:18,210
except that it

923
00:56:18,230 --> 00:56:20,960
i'm hearing way follows a little state

924
00:56:22,270 --> 00:56:24,020
depending on its current state

925
00:56:24,770 --> 00:56:26,620
read writes the current square

926
00:56:26,620 --> 00:56:28,460
possibly move to a new state

927
00:56:28,480 --> 00:56:31,230
possibly even shifts its position

928
00:56:31,250 --> 00:56:34,950
so this must mean in addition to the symbols on the type we've got a

929
00:56:34,950 --> 00:56:37,330
bunch of stats

930
00:56:37,350 --> 00:56:41,120
that's true his these states and the usual

931
00:56:41,180 --> 00:56:46,040
terminology the notation reason here's choose q

932
00:56:46,080 --> 00:56:47,520
so is that

933
00:56:47,680 --> 00:56:49,520
is the

934
00:56:49,520 --> 00:56:51,140
the states

935
00:56:51,190 --> 00:56:54,540
then this things that the little true machine can do

936
00:56:54,540 --> 00:56:58,060
move right one square left one square

937
00:56:58,120 --> 00:56:59,770
chinese symbol

938
00:56:59,810 --> 00:57:03,950
all depending on the state of its in

939
00:57:04,500 --> 00:57:07,370
if we had one of these things and we let loose on this on the

940
00:57:08,430 --> 00:57:12,930
it moves around changing symbols may be moving left and right

941
00:57:15,100 --> 00:57:17,770
for all i know it might stop

942
00:57:17,790 --> 00:57:20,890
not have any further states that can go into

943
00:57:20,950 --> 00:57:22,950
it finds itself in the states

944
00:57:22,960 --> 00:57:24,120
reading symbol

945
00:57:24,140 --> 00:57:26,560
somewhere other ones on the type

946
00:57:26,560 --> 00:57:28,710
and there is no further instructions

947
00:57:28,810 --> 00:57:30,040
we call that

948
00:57:30,080 --> 00:57:32,540
we say machine household cats

949
00:57:32,540 --> 00:57:36,060
so long-winded way of saying the machine has stopped

950
00:57:36,100 --> 00:57:38,890
hasn't done anything useful with than

951
00:57:40,950 --> 00:57:43,210
the turing machine device

952
00:57:44,330 --> 00:57:47,160
just continuing for as long as

953
00:57:47,180 --> 00:57:49,000
we look at

954
00:57:49,040 --> 00:57:52,100
might be something the machine is doing

955
00:57:52,120 --> 00:57:55,580
that it just keeps on the my keep moving left on the time it takes

956
00:57:56,960 --> 00:57:59,580
you don't have to go and stop we just looking

957
00:58:00,910 --> 00:58:04,330
we can tell from machines health we can't tell if it's going to hold in

958
00:58:05,160 --> 00:58:06,540
so find out

959
00:58:07,460 --> 00:58:08,790
maybe tomorrow

960
00:58:08,830 --> 00:58:11,060
we can tell general fitz can hold

961
00:58:11,160 --> 00:58:14,980
we certainly see it has hold that just interstate looking to sell

962
00:58:15,120 --> 00:58:17,770
this is not moment

963
00:58:17,770 --> 00:58:19,100
for his little

964
00:58:19,120 --> 00:58:21,160
standard little picture

965
00:58:21,160 --> 00:58:24,040
often jeffrey put little wheels on cell

966
00:58:24,080 --> 00:58:24,750
as if

967
00:58:25,540 --> 00:58:29,180
we really want to make this thing in your computer and

968
00:58:29,370 --> 00:58:31,680
it doesn't matter if it's got wheels

969
00:58:31,690 --> 00:58:35,100
it just cannot appear cell can only move from one cell to the left to

970
00:58:35,120 --> 00:58:36,680
right at the time

971
00:58:36,790 --> 00:58:39,910
but it doesn't happen

972
00:58:40,180 --> 00:58:43,250
in this particular conception turing machine

973
00:58:43,310 --> 00:58:46,480
OK most of finite number square not playing

974
00:58:46,520 --> 00:58:49,460
initially and later stages

975
00:58:49,480 --> 00:58:51,750
and the machine knows what reading

976
00:58:51,770 --> 00:58:55,540
and the initial configuration is part of the description

977
00:58:55,810 --> 00:59:00,460
is mention about stalinism state

978
00:59:00,460 --> 00:59:02,460
so these are the picture

979
00:59:02,560 --> 00:59:03,710
next slide

980
00:59:03,770 --> 00:59:04,410
is you

981
00:59:04,430 --> 00:59:08,660
just that much more mathematical formalisms this

982
00:59:08,710 --> 00:59:11,890
i think that this is just an example

983
00:59:11,910 --> 00:59:14,080
so here's little example running three

984
00:59:14,120 --> 00:59:16,580
simple ones on a blank tape

985
00:59:16,640 --> 00:59:17,620
there is there

986
00:59:17,660 --> 00:59:21,000
the head of the type if you like this we might allow is

987
00:59:21,120 --> 00:59:23,460
right simple one is left

988
00:59:23,520 --> 00:59:25,430
it's another similar left

989
00:59:25,480 --> 00:59:27,180
it's another symbol

990
00:59:27,190 --> 00:59:28,710
and then holds

991
00:59:28,790 --> 00:59:31,180
it doesn't have any work to do

992
00:59:31,230 --> 00:59:33,770
in its description

993
00:59:33,790 --> 00:59:35,120
so each machine

994
00:59:35,120 --> 00:59:39,020
computer programs specific that's a turing machine

995
00:59:39,020 --> 00:59:40,000
in each one

996
00:59:40,020 --> 00:59:42,580
there's some specific description what happened

997
00:59:42,620 --> 00:59:45,120
under certain conditions

998
00:59:46,270 --> 00:59:49,350
is my slides

999
00:59:49,390 --> 00:59:50,790
tried to fit song

1000
00:59:50,790 --> 00:59:53,980
it you may not thank me for this is that we try to do

1001
00:59:54,040 --> 01:00:00,520
those three things here three ways of describing the very same machine

1002
01:00:02,540 --> 01:00:07,160
what we trying to describe machine i think it's i think it's the machine right

1003
01:00:07,290 --> 01:00:11,250
three s one and then stopped on the left right time

1004
01:00:11,410 --> 01:00:13,770
because i think it is yes

1005
01:00:15,520 --> 01:00:19,750
one way to do this just be to put a table of strata title

1006
01:00:19,810 --> 01:00:22,540
there are three states q one q two c three

1007
01:00:22,680 --> 01:00:24,580
has two symbols

1008
01:00:24,640 --> 01:00:26,890
and we'll see what happens

1009
01:00:26,910 --> 01:00:29,040
under every circumstance

1010
01:00:29,100 --> 01:00:30,680
it under this

1011
01:00:30,730 --> 01:00:33,140
particular machine what's about to happen

1012
01:00:33,160 --> 01:00:36,750
so reading the first cell of that matrix

1013
01:00:36,790 --> 01:00:40,730
and the state q one reading symbol it's zero

1014
01:00:40,730 --> 01:00:44,290
it's supposed to write symbol this one

1015
01:00:45,040 --> 01:00:49,580
and then changed to a new state

1016
01:00:49,620 --> 01:00:51,460
that was the thing

1017
01:00:51,500 --> 01:00:53,520
looked like this

1018
01:00:53,580 --> 01:00:54,640
it's rating

1019
01:00:54,660 --> 01:00:58,330
it's not which is blank

1020
01:00:58,370 --> 01:01:00,980
it's supposed to write this one

1021
01:01:01,040 --> 01:01:02,960
and next instance time

1022
01:01:03,020 --> 01:01:05,310
it's still right here

1023
01:01:05,370 --> 01:01:07,040
now to change that

1024
01:01:07,040 --> 01:01:09,120
q one before

1025
01:01:09,180 --> 01:01:13,790
changes to q two

1026
01:01:13,890 --> 01:01:16,080
that's one cell

1027
01:01:16,120 --> 01:01:18,710
the other cells to what else can do it

1028
01:01:18,730 --> 01:01:21,310
one of the cells in the matrix is display

1029
01:01:21,350 --> 01:01:23,520
because it is not going to do anything there

1030
01:01:24,160 --> 01:01:25,580
should the machine error

1031
01:01:25,600 --> 01:01:27,580
reach the state q three

1032
01:01:27,600 --> 01:01:29,910
and happened to be reading as one

1033
01:01:29,980 --> 01:01:31,770
wouldn't have any way to go

1034
01:01:31,810 --> 01:01:34,350
i'm labouring the point because

1035
01:01:34,390 --> 01:01:37,290
we want to worry about with machine stop or not

1036
01:01:37,290 --> 01:01:38,930
so we just have to have

1037
01:01:39,060 --> 01:01:44,100
good for idea of what it means for machines to stop and all i mean

1038
01:01:44,120 --> 01:01:47,620
you can vary around just there's a hole in the matrix if you like

1039
01:01:47,680 --> 01:01:52,250
there's something it doesn't have does any more instructions

1040
01:01:52,310 --> 01:01:55,910
it it does it just stopped stone state q three

1041
01:01:55,960 --> 01:01:59,040
so what you can do this way can have a special state called the whole

1042
01:02:00,310 --> 01:02:02,370
moved into the whole state

1043
01:02:02,430 --> 01:02:05,060
the flag from

1044
01:02:05,080 --> 01:02:09,060
the second technique is the true machine flowgraph technique

1045
01:02:09,140 --> 01:02:12,230
which expresses i guess more

1046
01:02:13,390 --> 01:02:16,040
a flow chart sort of idea

1047
01:02:16,120 --> 01:02:18,060
which now

1048
01:02:18,120 --> 01:02:21,080
i guess people may be thought that was very modern

1049
01:02:21,100 --> 01:02:23,410
computing to the phi chapter

1050
01:02:23,460 --> 01:02:29,410
now we wouldn't necessarily described computer programs that sort of mechanism but certainly quite helpful

1051
01:02:29,410 --> 01:02:31,250
for our reasoning

1052
01:02:31,290 --> 01:02:35,730
and so this thing here says this is a little bit over here left

1053
01:02:35,730 --> 01:02:47,520
which seems which shows

1054
01:02:49,710 --> 01:02:55,040
you here is

1055
01:02:55,170 --> 01:02:59,820
only one of the

1056
01:03:32,730 --> 01:03:38,250
wish i it is transformed to a

1057
01:04:12,810 --> 01:04:19,540
he said that we it

1058
01:04:40,750 --> 01:04:43,710
and that is

1059
01:04:56,420 --> 01:05:01,320
so every time

1060
01:05:03,170 --> 01:05:19,400
of the

1061
01:05:51,060 --> 01:05:56,750
in fact

1062
01:05:57,040 --> 01:06:02,790
so i think

1063
01:06:10,840 --> 01:06:14,500
it's just

1064
01:06:14,630 --> 01:06:17,820
and they are

1065
01:06:45,090 --> 01:06:50,000
the same

1066
01:07:00,710 --> 01:07:06,790
and if you want

1067
01:09:21,000 --> 01:09:26,380
so i mean

1068
01:09:26,400 --> 01:09:31,250
proposition one

1069
01:09:31,250 --> 01:09:37,450
this kind are satisfied that means that the score of the correct is the most

1070
01:09:37,450 --> 01:09:43,670
of the case is larger than the colorful and correct ones

1071
01:09:43,680 --> 01:09:49,400
but if one want to impose if we believe that this kind of thing like

1072
01:09:49,400 --> 01:09:55,140
this should be bigger than this kind of on correct one we could either constrained

1073
01:09:55,150 --> 01:09:57,840
to that quadratic programming problem

1074
01:09:57,840 --> 01:09:59,820
we can solve it by name

1075
01:09:59,850 --> 01:10:01,540
iterative algorithms

1076
01:10:01,560 --> 01:10:04,500
in a similar way to previous approaches

1077
01:10:04,530 --> 01:10:09,840
and eventually in that case when the data is not linearly separable we can relax

1078
01:10:09,870 --> 01:10:13,630
constraints by adding slack variables

1079
01:10:14,230 --> 01:10:18,360
there are going to be in that case would be nice so we have basically

1080
01:10:18,360 --> 01:10:22,760
force that in the first step we compute the moment in the second step we

1081
01:10:22,760 --> 01:10:28,360
maximize the call without any constraint then we entering the main loop

1082
01:10:28,360 --> 01:10:34,370
and for each input output pair in the training set we compute we identified the

1083
01:10:34,370 --> 01:10:36,960
most violated constraints and

1084
01:10:38,380 --> 01:10:45,670
as karl suggested to there is a big is larger than the score of the

1085
01:10:45,670 --> 01:10:52,370
correct input output pairs we add to the set of constraints constraint that imposed the

1086
01:10:52,370 --> 01:10:58,120
discovery of the input the of the car the input output pairs should be given

1087
01:10:58,140 --> 01:11:00,670
and so we solve them

1088
01:11:01,140 --> 01:11:05,540
we maximize the discourse subject to that constraint

1089
01:11:05,540 --> 01:11:12,290
and it would be these until this set of course trains does not change in

1090
01:11:12,290 --> 01:11:13,770
the iteration

1091
01:11:13,770 --> 01:11:20,170
finally show some experimental results related to the south african to the problem score approach

1092
01:11:20,170 --> 01:11:26,810
and to this core approach with constraints i will

1093
01:11:26,830 --> 01:11:32,350
briefly show that our method performs better than the order in case of four

1094
01:11:32,390 --> 01:11:39,870
i level of noise is what we are doing here is generated random sequences by

1095
01:11:40,430 --> 01:11:46,600
starting from around the vector of parameters for the chain conditional random field with HMM

1096
01:11:46,600 --> 01:11:48,810
features we consider two

1097
01:11:48,830 --> 01:11:53,440
kind of conditional random field one with this when the the size of the

1098
01:11:53,460 --> 01:11:57,730
observation alphabet is four and the side of the the ninth of it is to

1099
01:11:57,810 --> 01:11:59,040
another one

1100
01:11:59,040 --> 01:12:03,730
when the size of the observation alphabet is five on the side of the you

1101
01:12:03,750 --> 01:12:05,540
know enough about you three

1102
01:12:05,560 --> 01:12:11,980
the sequence are the length of the length of fifty and the training set size

1103
01:12:11,980 --> 01:12:17,580
is made by ten to face side is made by one hundred pages or the

1104
01:12:17,580 --> 01:12:19,460
results generated by

1105
01:12:20,540 --> 01:12:23,330
iterating the process of for one

1106
01:12:23,350 --> 01:12:25,410
under the

1107
01:12:25,480 --> 01:12:33,790
and we compare our approach with the previous approaches and we plot the there either

1108
01:12:33,790 --> 01:12:39,690
and the number of incorrect labels that dividing level of noise and as you can

1109
01:12:39,690 --> 01:12:43,940
see our approach are better in case of large p

1110
01:12:43,960 --> 01:12:48,540
and p represents the the probability

1111
01:12:49,770 --> 01:12:57,770
a mirror represented the level of noise that we generated by flipping even labour in

1112
01:12:57,790 --> 01:13:01,830
the training set

1113
01:13:01,850 --> 01:13:05,270
and the problem is that the

1114
01:13:05,750 --> 01:13:08,580
that we a we consider the learning curve

1115
01:13:08,580 --> 01:13:09,790
for them

1116
01:13:09,810 --> 01:13:16,230
a second graph here when the level of noise is the zero point two

1117
01:13:16,370 --> 01:13:18,940
with the land enquiry

1118
01:13:18,960 --> 01:13:24,560
for a or the algorithms that we can see that i did not show any

1119
01:13:24,690 --> 01:13:31,390
are the discourse in the performance of are very similar to sort out and

1120
01:13:31,560 --> 01:13:37,350
what we have is that our approach performs better than conditional random fields and the

1121
01:13:37,350 --> 01:13:44,940
perceptron and is very competitive some are better than the SVM high so that we

1122
01:13:45,520 --> 01:13:48,210
use and i mean lots of similar to

1123
01:13:48,250 --> 01:13:51,750
may i got the presented this morning by PASCAL

1124
01:13:51,770 --> 01:13:53,810
and in this case

1125
01:13:54,350 --> 01:13:58,600
we see that the performance in terms of these that very similar

1126
01:13:58,620 --> 01:14:05,410
with as VMI's about we analyse the computational cost of learning we see that our

1127
01:14:05,410 --> 01:14:08,750
approach is much faster especially for

1128
01:14:08,770 --> 01:14:11,890
when the size of the training set

1129
01:14:11,910 --> 01:14:15,270
becomes larger

1130
01:14:15,290 --> 01:14:22,120
another spearmint means that we need all again with artificial that is that we consider

1131
01:14:22,120 --> 01:14:30,290
a chain conditional random field with HMM features and the side of the international but

1132
01:14:30,290 --> 01:14:37,460
is the the size of the observation alphabet changes saying experiment we want to show

1133
01:14:37,830 --> 01:14:44,350
that with something approaches we can obtain good results so basically it

1134
01:14:44,440 --> 01:14:51,830
we compare how we use our approach by computing matrix with dynamic programming about computing

1135
01:14:51,830 --> 01:15:03,640
matrix with random with estimates of the matrix we approximate sampling on fifty parts and

1136
01:15:03,640 --> 01:15:04,940
two hundred parts

1137
01:15:04,960 --> 01:15:09,140
and we plot the results in time of this and we see that

1138
01:15:09,160 --> 01:15:14,270
one of the four methods are more or less equivalent but of course using sampling

1139
01:15:14,290 --> 01:15:19,770
the time is the much the computational cost and the time for learning is much

1140
01:15:19,770 --> 01:15:23,620
more reduced

1141
01:15:23,640 --> 01:15:29,410
simulation that we need to get it to featured that is to show

1142
01:15:29,410 --> 01:15:34,570
the second condition is that w does not have control converging arrows along the path

1143
01:15:34,620 --> 01:15:39,320
so you go this way or that way and w is observed

1144
01:15:40,310 --> 01:15:45,320
so i find these two conditions are satisfied then vd separates

1145
01:15:45,330 --> 01:15:47,200
x from y

1146
01:15:47,300 --> 01:15:50,350
and if you haven't seen this before i don't expect you to look at this

1147
01:15:50,350 --> 01:15:54,550
and say aha that immediately makes sense takes a little bit of time to think

1148
01:15:54,550 --> 01:15:57,610
about it i'm not going to go into details because i want to be able

1149
01:15:57,610 --> 01:16:02,040
to get into the propagation algorithms in the learning but there's a lot of good

1150
01:16:02,040 --> 01:16:05,080
material that tries to explain this

1151
01:16:05,100 --> 01:16:09,200
including longer tutorials of this kind

1152
01:16:09,240 --> 01:16:10,350
that might be online

1153
01:16:10,420 --> 01:16:11,830
so the kalahari

1154
01:16:11,840 --> 01:16:15,100
four bayesian networks are directed graphs is

1155
01:16:17,560 --> 01:16:23,330
the markov boundary for text that is the set of variables that make x independent

1156
01:16:23,330 --> 01:16:25,570
of all other variables

1157
01:16:26,390 --> 01:16:30,670
the parents of x the children of x and the parents of the children of

1158
01:16:30,670 --> 01:16:35,450
x so given those variables x is independent of everything else

1159
01:16:35,490 --> 01:16:37,310
that's similar to what we had

1160
01:16:37,360 --> 01:16:39,860
in factor graphs

1161
01:16:43,180 --> 01:16:48,080
so let's let's look at some examples

1162
01:16:48,130 --> 01:16:49,930
if we have this graph

1163
01:16:49,980 --> 01:16:52,740
a is independent of b

1164
01:16:54,100 --> 01:16:56,150
marginally independent since

1165
01:16:56,200 --> 01:17:00,280
a a c b is blocked by c

1166
01:17:00,310 --> 01:17:04,400
and a CD b is blocked by d et cetera

1167
01:17:04,410 --> 01:17:07,870
so all the paths between a and b are blocked because

1168
01:17:07,880 --> 01:17:09,570
nothing is observed

1169
01:17:09,580 --> 01:17:15,100
none of these variables have been observed is not true that a is independent of

1170
01:17:15,920 --> 01:17:17,290
given c

1171
01:17:17,300 --> 01:17:21,350
since a c b is not blocked so given c

1172
01:17:21,400 --> 01:17:24,790
a and b are dependent on each other

1173
01:17:24,790 --> 01:17:28,100
and this is a very important concept

1174
01:17:29,740 --> 01:17:33,060
given the particular symptom

1175
01:17:33,070 --> 01:17:37,910
the probability of having this disease or that this is the kind of course that

1176
01:17:39,300 --> 01:17:42,950
those probabilities are going to be correlated

1177
01:17:42,960 --> 01:17:43,890
OK the

1178
01:17:43,910 --> 01:17:45,530
joint probability is

1179
01:17:45,530 --> 01:17:49,460
it's not going to be independent even if getting this disease and getting that this

1180
01:17:49,460 --> 01:17:52,320
is going to happen with independent probabilities

1181
01:17:52,340 --> 01:17:57,110
and and furthermore we can look at a few other things like a is independent

1182
01:17:57,110 --> 01:17:59,420
of the given b and c

1183
01:17:59,470 --> 01:18:03,620
a and d are independent given b and c on this graph

1184
01:18:03,640 --> 01:18:05,830
it's all paths are blocked

1185
01:18:08,450 --> 01:18:11,330
so here are some of

1186
01:18:11,380 --> 01:18:16,790
useful notation that again you'll see in a lot of papers

1187
01:18:16,830 --> 01:18:19,080
you see this these

1188
01:18:19,090 --> 01:18:22,100
boxes around some notes with the little

1189
01:18:23,420 --> 01:18:26,840
later in the quarter this cold plate notation

1190
01:18:26,850 --> 01:18:31,300
and it represents repetitions of some

1191
01:18:31,300 --> 01:18:36,210
very so consider n data points generated from the gaussians

1192
01:18:36,340 --> 01:18:40,280
we have x one through x and let's say that the calcium has mean mu

1193
01:18:40,280 --> 01:18:42,950
and standard deviation sigma

1194
01:18:43,980 --> 01:18:48,650
could write down a little probabilistic model where we have p of mu

1195
01:18:48,700 --> 01:18:50,600
being the prior on mu

1196
01:18:50,620 --> 01:18:53,700
p of sigma being the prior on sigma and then

1197
01:18:53,700 --> 01:18:56,280
the likelihood

1198
01:18:57,640 --> 01:19:00,400
the corresponding to each of the data points

1199
01:19:00,410 --> 01:19:02,810
this is what the graph would look like

1200
01:19:02,850 --> 01:19:05,230
for this factorisation right

1201
01:19:05,230 --> 01:19:10,290
each node given its parents is represented like this but these are these guys are

1202
01:19:10,390 --> 01:19:12,680
are replicated n times

1203
01:19:12,740 --> 01:19:17,260
so to make the graph little prettier and easier to look at we can put

1204
01:19:17,260 --> 01:19:21,650
a box around one of these nodes and put an index here and that just

1205
01:19:21,650 --> 01:19:23,530
means x and

1206
01:19:23,590 --> 01:19:26,850
goes from any cause one to big

1207
01:19:28,140 --> 01:19:30,310
that's plate notation

1208
01:19:30,310 --> 01:19:33,740
all right so that's the summary of the first many lecture

1209
01:19:33,740 --> 01:19:40,230
topping it up into lots of little bits i introduced different kinds of graphical models

1210
01:19:40,230 --> 01:19:46,990
focusing on factor graphs and directed graphs i got the marginal and conditional independence markov

1211
01:19:46,990 --> 01:19:52,610
boundaries and the idea of the separation and placement notation

1212
01:19:54,900 --> 01:19:59,150
any questions well i look the next

1213
01:19:59,200 --> 01:20:07,500
built up

1214
01:20:15,420 --> 01:20:22,770
all right so we introduce graphical models very quickly now let's talk about inference and

1215
01:20:22,770 --> 01:20:25,070
propagation algorithms

1216
01:20:28,810 --> 01:20:34,360
so what is this inference problem what are we talking about consider the following graph

1217
01:20:34,420 --> 01:20:37,160
which represents joint distribution

1218
01:20:37,170 --> 01:20:40,240
that we had in the previous

1219
01:20:41,570 --> 01:20:48,830
that inference is the problem of evaluating the probability distribution for some set of variables

1220
01:20:48,840 --> 01:20:52,410
given the values of some other set of variables

1221
01:20:52,460 --> 01:20:54,110
so for example

1222
01:20:54,130 --> 01:20:59,420
how can we compute the probability distribution over a given that c takes on the

1223
01:20:59,420 --> 01:21:01,460
value will see

1224
01:21:01,470 --> 01:21:03,640
right and you could imagine

1225
01:21:03,670 --> 01:21:05,600
if you have a big graph

1226
01:21:05,980 --> 01:21:08,310
corresponding to

1227
01:21:08,360 --> 01:21:14,670
nuclear power plant are big medical diagnosis system you input your sensor readings or your

1228
01:21:14,670 --> 01:21:20,550
symptoms and you want to find out the probability that there's a meltdown or the

1229
01:21:20,550 --> 01:21:24,190
let's proceed so basically we can also write these

1230
01:21:25,940 --> 01:21:27,650
in two different places

1231
01:21:27,760 --> 01:21:32,020
the camera is equality like this

1232
01:21:32,020 --> 01:21:36,360
because based theorem gives us these

1233
01:21:36,420 --> 01:21:38,750
troops in bayes theorem

1234
01:21:38,800 --> 01:21:39,980
before in

1235
01:21:40,480 --> 01:21:44,610
in life it certainly but in the in the court about

1236
01:21:46,110 --> 01:21:47,880
and this is also true here

1237
01:21:47,900 --> 01:21:50,150
we can apply bayes theorem

1238
01:21:50,230 --> 01:21:54,420
with these other two vials as well the bayes rule whatever you call

1239
01:21:56,550 --> 01:21:59,070
so i mean basically these three

1240
01:21:59,150 --> 01:22:06,000
are statements are equivalent statements

1241
01:22:06,050 --> 01:22:07,340
so let me just

1242
01:22:07,380 --> 01:22:09,710
discover we discover that

1243
01:22:09,710 --> 01:22:11,900
a given

1244
01:22:20,170 --> 01:22:23,500
i will apply

1245
01:22:23,570 --> 01:22:27,050
some type of factorizations

1246
01:22:27,050 --> 01:22:31,460
for the joint probability distribution which is not unique by the way

1247
01:22:31,460 --> 01:22:33,570
i can factorize these way

1248
01:22:33,570 --> 01:22:35,070
these way

1249
01:22:35,090 --> 01:22:36,340
all these way

1250
01:22:36,360 --> 01:22:37,880
and these three

1251
01:22:37,900 --> 01:22:44,320
factorizations will give rise to precisely the same conditional independence

1252
01:22:46,110 --> 01:22:48,300
that's important as well

1253
01:22:49,460 --> 01:22:52,780
o input was the conditional independence the output

1254
01:22:52,860 --> 01:22:57,630
is apparently a set of possible factorizations

1255
01:23:01,770 --> 01:23:03,650
so we are starting to

1256
01:23:07,320 --> 01:23:09,840
more into the

1257
01:23:09,880 --> 01:23:11,280
the details

1258
01:23:11,440 --> 01:23:14,020
OK so let's try to

1259
01:23:14,070 --> 01:23:16,230
navigate slowly here and try to

1260
01:23:16,270 --> 01:23:17,320
get all the

1261
01:23:20,820 --> 01:23:23,090
so what seems to be happening is that

1262
01:23:23,110 --> 01:23:30,840
the conditional independence statements they seem to generate factorisation for joint probability that's what seems

1263
01:23:30,840 --> 01:23:34,400
to be happening have studied this for a very simple case of

1264
01:23:34,400 --> 01:23:35,570
three viable

1265
01:23:35,570 --> 01:23:38,190
probabilistic models

1266
01:23:38,540 --> 01:23:42,960
now let's see

1267
01:23:43,000 --> 01:23:45,150
what are we going

1268
01:23:45,210 --> 01:23:48,940
by using this graphical model in terms of computational

1269
01:23:51,960 --> 01:23:52,770
let's see

1270
01:23:52,780 --> 01:23:56,610
an example of how expensive it is to compute

1271
01:23:56,610 --> 01:24:00,670
p of x two for example

1272
01:24:01,650 --> 01:24:04,400
well first let's be naive

1273
01:24:04,460 --> 01:24:06,840
as we saw in the beginning

1274
01:24:06,880 --> 01:24:10,170
let's compute pfx two by some

1275
01:24:10,190 --> 01:24:11,110
the joint

1276
01:24:11,130 --> 01:24:14,750
distribution on x one and x three

1277
01:24:19,340 --> 01:24:20,670
for every

1278
01:24:20,690 --> 01:24:23,550
possible value of x two

1279
01:24:23,630 --> 01:24:25,820
i have to go through all the

1280
01:24:25,840 --> 01:24:27,050
that is thick

1281
01:24:27,050 --> 01:24:29,250
three and x one

1282
01:24:29,300 --> 01:24:30,590
compute the

1283
01:24:30,630 --> 01:24:34,040
and then to go to the next value of x two

1284
01:24:34,040 --> 01:24:35,480
they need to fill out

1285
01:24:35,500 --> 01:24:36,780
all these

1286
01:24:38,400 --> 01:24:41,500
for the complexity of running this thing your goals

1287
01:24:41,500 --> 01:24:44,650
could be basically if you think that the size of three

1288
01:24:44,710 --> 01:24:49,570
sample space is the same this could become the size of examples

1289
01:24:54,710 --> 01:24:57,690
we use the factorisation

1290
01:24:57,710 --> 01:24:59,800
again we have now

1291
01:24:59,800 --> 01:25:05,910
in linear order so that means it converges linearly so that means

1292
01:25:06,090 --> 01:25:08,790
the objective function at the tth iteration

1293
01:25:08,800 --> 01:25:10,500
minus the objective function

1294
01:25:10,510 --> 01:25:12,230
the best function

1295
01:25:12,600 --> 01:25:16,180
this difference goes down like c

1296
01:25:16,200 --> 01:25:22,490
the two part minus t so the c depends on the function

1297
01:25:22,500 --> 01:25:27,140
OK so it goes exponentially down

1298
01:25:27,220 --> 01:25:29,850
so i mean of course i would like to have this as close as possible

1299
01:25:29,870 --> 01:25:32,290
to do

1300
01:25:40,130 --> 01:25:44,050
so she sees less than one

1301
01:25:44,060 --> 01:25:48,000
these less one so therefore this converges to zero

1302
01:25:48,050 --> 01:25:49,990
these changes

1303
01:25:55,690 --> 01:25:56,700
you right

1304
01:25:56,780 --> 01:26:01,160
OK so these mistakes of making making negative plus you

1305
01:26:01,770 --> 01:26:07,160
thank you

1306
01:26:07,210 --> 01:26:11,110
so in practice of course i mean when you would like to apply this bound

1307
01:26:11,120 --> 01:26:13,410
so then this c

1308
01:26:13,490 --> 01:26:15,470
it's very close to one

1309
01:26:15,490 --> 01:26:22,390
OK so it's maybe one minus ten to the point something so i mean this

1310
01:26:23,390 --> 01:26:29,470
huey so and maybe maybe in practice it would help to show really fast converging

1311
01:26:29,480 --> 01:26:33,020
so the other bones which have shown you a much much stronger support this this

1312
01:26:33,020 --> 01:26:35,510
shows you in principle these are doesn't converge

1313
01:26:36,190 --> 01:26:43,130
OK as a corollary we we can show that adaboost logistic regression least square boost

1314
01:26:43,140 --> 01:26:45,270
all these things converge

1315
01:26:45,290 --> 01:26:47,670
linearly so with this performance part

1316
01:26:47,690 --> 01:26:52,920
so this was not known before so those are only known when the data is

1317
01:26:53,820 --> 01:26:59,490
so there we had this PAC boosting property that we have the fast convergence here

1318
01:26:59,710 --> 01:27:03,990
with much better constants and here so here the optimisation of in the

1319
01:27:04,030 --> 01:27:05,040
proof i mean

1320
01:27:05,180 --> 01:27:08,770
the techniques are more difficult into the

1321
01:27:08,780 --> 01:27:13,110
bones are not as tight

1322
01:27:13,140 --> 01:27:15,550
OK so this is

1323
01:27:15,550 --> 01:27:19,880
this was condescend so other questions for that

1324
01:27:19,900 --> 01:27:22,550
one more question

1325
01:27:22,650 --> 01:27:27,910
OK so now i go to the dual domain again so highly optimized and alpha

1326
01:27:27,930 --> 01:27:33,260
domain so we chose one final optimized so now we go back to the d

1327
01:27:33,260 --> 01:27:35,800
domain so that's you domain

1328
01:27:35,850 --> 01:27:41,320
we optimize these the weights on examples of course OK

1329
01:27:41,370 --> 01:27:48,260
so we have essentially seems like so in each iteration we update this deed deep

1330
01:27:48,270 --> 01:27:52,500
distribution here and this is the solution of of an optimisation problem here

1331
01:27:52,530 --> 01:27:56,100
so this is a generalized projection onto a hyperplane

1332
01:27:59,430 --> 01:28:01,830
so you can generalize this this one as well

1333
01:28:02,550 --> 01:28:08,900
you can define a generalized distance it's not the distance but so it's called a

1334
01:28:08,900 --> 01:28:14,410
distance so it it is called the divergence to bregman divergence a monthly has to

1335
01:28:14,410 --> 01:28:16,390
fight to find that

1336
01:28:16,450 --> 01:28:22,020
and so the distance between the bregman divergence between dm dt

1337
01:28:22,030 --> 01:28:24,320
for a function g

1338
01:28:24,370 --> 01:28:26,610
is it just

1339
01:28:26,620 --> 01:28:27,730
the function g

1340
01:28:27,730 --> 01:28:32,700
at t minus the linear approximation of the function kx in this picture in slide

1341
01:28:35,550 --> 01:28:37,410
so and

1342
01:28:37,550 --> 01:28:44,510
if you use GSM s i think the time slot the then this is just

1343
01:28:44,520 --> 01:28:48,720
that it was setting so special case of this article

1344
01:28:48,760 --> 01:28:53,880
OK so and not most one leveraging would find the closest point to the closest

1345
01:28:53,880 --> 01:28:56,890
dt so the point close to dt

1346
01:28:57,180 --> 01:28:59,370
which is on the hyperplane so that means

1347
01:28:59,480 --> 01:29:01,990
this is again the hyperplane

1348
01:29:02,000 --> 01:29:04,490
and now we do this project here

1349
01:29:05,070 --> 01:29:06,760
so the back projection

1350
01:29:06,760 --> 01:29:11,460
if you put the project with euclidean distance they will get this point and here

1351
01:29:11,460 --> 01:29:16,840
within it really depends on on the function g

1352
01:29:17,770 --> 01:29:21,720
leveraging the essentially doing this projection step

1353
01:29:21,770 --> 01:29:26,540
in every iteration

1354
01:29:26,850 --> 01:29:32,840
and there's something which is called the great along with that's from things from nineteen

1355
01:29:32,840 --> 01:29:39,110
the two for this work the children identified among which of these are

1356
01:29:39,170 --> 01:29:40,420
which of the right

1357
01:29:40,440 --> 01:29:46,690
classes whether partially with purely derived or partially derived

1358
01:29:46,710 --> 01:29:50,750
so we should still concentrate on all the three classes

1359
01:29:50,750 --> 01:29:54,070
you can fix all the purely derived classes and start to fix

1360
01:29:57,130 --> 01:29:58,670
partially derived classes

1361
01:29:58,730 --> 01:30:01,730
OK back to hands on

1362
01:30:01,880 --> 01:30:14,250
so what you see

1363
01:30:14,280 --> 01:30:15,820
here then is

1364
01:30:15,860 --> 01:30:18,440
the exploration workbench

1365
01:30:18,570 --> 01:30:25,400
we've got a justification selected this is a i students

1366
01:30:25,420 --> 01:30:29,090
and on the right side we see

1367
01:30:29,190 --> 01:30:33,070
just we see a justification for the entailment over here

1368
01:30:33,070 --> 01:30:37,860
this is this justification has been computed for entailments see here

1369
01:30:37,880 --> 01:30:43,230
the number of justifications for entailment is shown in this column here

1370
01:30:43,260 --> 01:30:45,730
you can see that some of the the

1371
01:30:45,860 --> 01:30:50,380
figures are filled enough because they have been computed you can ask the system to

1372
01:30:50,380 --> 01:30:54,610
compute all of the justifications for entailment in this list

1373
01:30:54,630 --> 01:30:59,710
so we select these and computes these other justifications as well

1374
01:31:00,300 --> 01:31:02,860
you can go down to the bottom here

1375
01:31:02,880 --> 01:31:08,360
you'll notice that this lecture taking four courses has three justifications

1376
01:31:08,380 --> 01:31:10,530
all the other entailments have one

1377
01:31:10,550 --> 01:31:18,480
so what we're going to do is take a look at some example justifications

1378
01:31:18,520 --> 01:31:22,210
so let's take a look at this first one i students

1379
01:31:23,360 --> 01:31:24,800
no way to read this

1380
01:31:24,820 --> 01:31:31,690
is that the first axiom here so you can see such image larger

1381
01:31:31,710 --> 01:31:37,420
so it's OK the first scene in this list is that posted entailments i students

1382
01:31:37,440 --> 01:31:42,190
opossum nothing the actual entailment in this column we just paying the the consistent with

1383
01:31:42,190 --> 01:31:46,300
the rest interface we just coughing that we don't bother to show that suppose nothing

1384
01:31:47,530 --> 01:31:51,800
below we got four axioms in this justification

1385
01:31:51,820 --> 01:31:57,190
and there's some presentation going on here so we all the axioms using certain heuristics

1386
01:31:57,190 --> 01:32:01,940
we also intends from anything easier to beautifully two this one we see that AI

1387
01:32:01,980 --> 01:32:05,230
students of class of things

1388
01:32:05,250 --> 01:32:11,820
least one professor in HCI as advisors should just point out this is the manchester

1389
01:32:11,820 --> 01:32:15,000
syntax by default prior to show uses manchester syntax which

1390
01:32:15,630 --> 01:32:20,630
is a slightly different from the description logic syntax but hopefully it shouldn't be too

1391
01:32:20,630 --> 01:32:25,500
difficult for you to know the correspondence interviews push before you be used to it

1392
01:32:25,530 --> 01:32:30,590
so we see i students subclass of has advisers and professor in HCI

1393
01:32:31,630 --> 01:32:37,280
we also see that professor in HCI

1394
01:32:37,280 --> 01:32:40,550
is we also see that the inverse property

1395
01:32:40,570 --> 01:32:44,050
of this has advised property is advice evolve

1396
01:32:44,070 --> 01:32:46,960
and then we see that professor in a CIA i

1397
01:32:46,980 --> 01:32:49,130
it is advisable of

1398
01:32:49,150 --> 01:32:55,090
advisor of only eighty i student so this means we can conclude that since his

1399
01:32:56,610 --> 01:33:02,840
local administration this means that anything that is advised by one professor in HCI

1400
01:33:02,860 --> 01:33:07,300
or a i must be in HCI students and then we see that a i

1401
01:33:07,300 --> 01:33:12,940
student disjoint a shy student and this is one reason why this class is unsatisfiable

1402
01:33:12,960 --> 01:33:17,050
any questions

1403
01:33:17,070 --> 01:33:21,710
take a look at another one

1404
01:33:21,730 --> 01:33:25,630
here we've got CS department which is unsatisfiable

1405
01:33:25,650 --> 01:33:26,780
the game

1406
01:33:26,840 --> 01:33:30,440
the tell shown in the first show here

1407
01:33:30,440 --> 01:33:35,050
i we say that CS department is a subclass of things that

1408
01:33:35,070 --> 01:33:38,110
affiliated with least once slightly

1409
01:33:38,130 --> 01:33:43,280
and then we've also got another restriction here that cf library is affiliated with some

1410
01:33:44,780 --> 01:33:49,020
and the key thing is that it's affiliated property here is transitive

1411
01:33:49,070 --> 01:33:54,400
so this means that RCS department must be affiliated with e library

1412
01:33:54,420 --> 01:33:58,090
and then we say anything that was also said that it was affiliated with RTE

1413
01:33:58,190 --> 01:34:00,130
library must be

1414
01:34:00,130 --> 01:34:01,980
and the department

1415
01:34:02,000 --> 01:34:05,920
so this means CS department must be in e department

1416
01:34:05,940 --> 01:34:08,170
and these things are disjoint

1417
01:34:08,190 --> 01:34:13,340
well the whole point about user interface here is that you're doing uses is based

1418
01:34:13,340 --> 01:34:15,840
on heuristics so it is not

1419
01:34:16,380 --> 01:34:20,500
it's essentially the best thing all the time

1420
01:34:20,530 --> 01:34:23,250
so you might like to change that you can do that you can have a

1421
01:34:23,250 --> 01:34:27,480
play about a new vaccines and justification around if you press control

1422
01:34:27,500 --> 01:34:30,400
and then use the arrow keys to do this you know

1423
01:34:30,570 --> 01:34:34,340
the up and down arrow keys move the axiom of down

1424
01:34:34,480 --> 01:34:37,940
and also to press the left and right community

1425
01:34:37,960 --> 01:34:41,840
x around so it might actually be preferential here that we have

1426
01:34:41,940 --> 01:34:46,260
these axioms close together so we can see that this chain along

1427
01:34:46,280 --> 01:34:56,320
and then the inference due to the transitive property

1428
01:34:56,340 --> 01:35:03,190
so any questions so far

1429
01:35:11,250 --> 01:35:13,650
a little bit about how we actually

1430
01:35:13,690 --> 01:35:17,760
compute these things

1431
01:35:17,780 --> 01:35:23,970
various ways that we can do it but broadly speaking the way computing committees were

1432
01:35:23,970 --> 01:35:26,980
divided into two broad categories

1433
01:35:27,000 --> 01:35:32,670
and so do the services are generally implemented as either glass box services all black

1434
01:35:32,670 --> 01:35:35,000
box services

1435
01:35:39,070 --> 01:35:43,300
you have go

1436
01:35:43,320 --> 01:35:44,610
OK OK

1437
01:35:44,670 --> 01:35:51,300
so if just go over possible services first glassbox implementations first

1438
01:35:51,320 --> 01:35:56,360
and these implementations are specific to a particular reason

1439
01:35:56,400 --> 01:35:58,360
so if you want to

1440
01:35:58,420 --> 01:36:03,610
this can tracing to compute justifications to these now

1441
01:36:03,630 --> 01:36:08,070
you need to an existing these now you need to implement some kind of tracing

1442
01:36:08,150 --> 01:36:12,760
and this just to say this requires usually if there are non trivial modification of

1443
01:36:12,760 --> 01:36:14,360
the reasoner internals so

1444
01:36:15,260 --> 01:36:19,400
if you want to take something like five plus which doesn't currently support

1445
01:36:19,480 --> 01:36:22,130
this tracing but it will do

1446
01:36:22,150 --> 01:36:26,550
it takes quite a lot of effort track which axioms used because there's all sorts

1447
01:36:26,550 --> 01:36:29,090
normalisation going on inside the reason

1448
01:36:30,570 --> 01:36:32,050
and so is nontrivial

1449
01:36:32,050 --> 01:36:34,630
and examples of this palette

1450
01:36:34,820 --> 01:36:38,650
what do you can ask how is gone interface you can ask how reason why

1451
01:36:39,130 --> 01:36:42,340
for exploration last entailment you asked about

1452
01:36:42,380 --> 01:36:43,820
and also the sole reason here

1453
01:36:43,840 --> 01:36:47,460
for so reason can compute all justifications for

1454
01:36:47,480 --> 01:36:53,900
and then tell us how just computes what one justification using this glass box technique

1455
01:36:53,960 --> 01:36:57,480
and then use a hybrid technique with black partition going to come on next to

1456
01:36:57,500 --> 01:36:59,880
computer remained justifications

1457
01:36:59,900 --> 01:37:05,960
advantages of this approach is glassbox approach is that it can be really fast because

1458
01:37:05,960 --> 01:37:10,020
usually the tracing techniques used just almost come for free

1459
01:37:10,030 --> 01:37:14,630
in the case of pollock for example and i don't require any extra resources is

1460
01:37:14,630 --> 01:37:16,000
almost noticeable

1461
01:37:16,590 --> 01:37:20,360
but it requires actually sources in terms of time

1462
01:37:20,380 --> 01:37:21,400
and memory so

1463
01:37:21,420 --> 01:37:26,750
the advantage is that when you actually computing

1464
01:37:26,750 --> 01:37:29,840
where the sum is satisfiable or not

1465
01:37:29,900 --> 01:37:35,280
we also automatically get justification computed so it's very fast and almost cancer-free

1466
01:37:35,300 --> 01:37:37,550
on onto black box

1467
01:37:37,610 --> 01:37:39,960
so as i mentioned

1468
01:37:39,980 --> 01:37:44,320
the with glass box in crisis there are non trivial modification of these entails it's

1469
01:37:44,320 --> 01:37:48,530
quite difficult if you want to use it with your favourite reasonable or in your

1470
01:37:48,530 --> 01:37:52,430
it's just a tiny detail and

1471
01:37:53,310 --> 01:37:59,570
i will call those points margin errors that have a positive side i

1472
01:37:59,580 --> 01:38:02,890
remember kci was the thing that is

1473
01:38:03,120 --> 01:38:05,100
making this constraint easier

1474
01:38:05,100 --> 01:38:06,990
you can already

1475
01:38:06,990 --> 01:38:11,830
imagine if that's the point which lies in the mountain or even on the wrong

1476
01:38:11,830 --> 01:38:16,530
side so the constraint is violated this so i will start growing but since the

1477
01:38:16,530 --> 01:38:20,870
site penalized in the objective function and will grow more than necessary so will only

1478
01:38:22,180 --> 01:38:26,800
just enough to make this constraint satisfied as inequality

1479
01:38:26,810 --> 01:38:30,240
so for all points which have a non-zero XII

1480
01:38:30,260 --> 01:38:36,580
those points would be support vectors in the sense of will lie on the edge

1481
01:38:36,950 --> 01:38:43,580
so one can prove this formally that from the KKT conditions follows that all margin

1482
01:38:43,580 --> 01:38:45,430
errors are support vectors

1483
01:38:45,450 --> 01:38:48,390
so margin errors are subset of the support vectors

1484
01:38:48,410 --> 01:38:52,530
but there could be more support vector so there could be some support vectors that

1485
01:38:52,530 --> 01:38:53,240
are not

1486
01:38:53,240 --> 01:38:54,410
margin errors

1487
01:38:54,410 --> 01:38:57,700
but they only support vectors

1488
01:38:57,700 --> 01:39:01,410
which are not margin errors are those

1489
01:39:02,950 --> 01:39:05,990
the only support vectors which are not margin errors

1490
01:39:06,010 --> 01:39:11,080
all those that lie exactly on the edge of the margin

1491
01:39:13,680 --> 01:39:18,600
so so we have two sets here we have the set of support vectors

1492
01:39:18,620 --> 01:39:23,030
we have a set of margin errors the set of support vectors is a subset

1493
01:39:23,030 --> 01:39:28,130
of the set of marginals is larger and one can prove that the size of

1494
01:39:28,130 --> 01:39:31,760
the set of support vectors is lower bound by this parameter new and the size

1495
01:39:31,760 --> 01:39:37,070
of the set of margin errors is upper bounded by this from to new

1496
01:39:37,070 --> 01:39:42,680
moreover one can prove that asymptotically these two inequalities will become quality so this parameter

1497
01:39:42,700 --> 01:39:45,580
new which is parameters set by the user

1498
01:39:45,720 --> 01:39:50,800
before running the optimisation problem controls the fraction of margin errors and the fraction of

1499
01:39:50,800 --> 01:39:59,910
support vectors so it's you know if you want to physically meaningful parameter

1500
01:39:59,930 --> 01:40:09,280
so if we compute the tools of these two problems have already mentioned to you

1501
01:40:09,910 --> 01:40:11,450
two of the sea

1502
01:40:11,510 --> 01:40:15,800
soft margin support vector machine is

1503
01:40:15,800 --> 01:40:20,430
the one the same as the tool of the hard margin SVM with one modification

1504
01:40:20,430 --> 01:40:22,280
that we now have an upper bound

1505
01:40:22,280 --> 01:40:24,100
on the other five

1506
01:40:24,100 --> 01:40:30,370
the dual of the new political machine on the other hand is a little bit

1507
01:40:30,370 --> 01:40:36,180
different so it consists of maximizing this function which is non homogeneous quadratic form

1508
01:40:36,200 --> 01:40:40,870
a little bit simpler than this one which also had this linear term simple quadratic

1509
01:40:40,870 --> 01:40:42,100
term is the same

1510
01:40:42,120 --> 01:40:44,550
however this

1511
01:40:44,570 --> 01:40:47,870
i find some of the finest which was part of the

1512
01:40:47,910 --> 01:40:53,430
objective function is now appearing in constraint so we have one more constrained

1513
01:40:53,450 --> 01:40:57,620
we still have this lower and upper bounds and five so these are sometimes called

1514
01:40:57,620 --> 01:41:04,660
box constraints we still have this equality constraint but now we also have this constraint

1515
01:41:04,660 --> 01:41:06,990
involving u

1516
01:41:07,010 --> 01:41:09,800
in both cases we get the same decision function

1517
01:41:09,810 --> 01:41:16,370
so let me say a little bit about how support vector machines are trained so

1518
01:41:16,370 --> 01:41:21,680
if you try to support vector machine you in some problem you are likely to

1519
01:41:21,680 --> 01:41:27,950
simply download and SVM package nowadays there so many different packages you can get links

1520
01:41:27,950 --> 01:41:34,220
to them on the kernel machines website i think i have appointed down here

1521
01:41:34,240 --> 01:41:38,390
you probably will download package but nevertheless it's good to at least have an idea

1522
01:41:38,390 --> 01:41:43,140
of what's behind these approaches and this is that the one slide summary of support

1523
01:41:43,140 --> 01:41:44,370
vector training

1524
01:41:46,910 --> 01:41:49,970
we would like to maximize this thing here

1525
01:41:50,430 --> 01:41:56,830
maybe this thing without the subject to certain constraints and if you naively plug this

1526
01:41:58,080 --> 01:42:03,610
and optimize from the matlab optimiser you will find that it scares approximately with the

1527
01:42:03,610 --> 01:42:06,850
power of the training set size

1528
01:42:07,810 --> 01:42:08,660
so that's

1529
01:42:08,680 --> 01:42:13,330
essentially because matrix that you have to

1530
01:42:13,330 --> 01:42:16,910
plug into you optimise it will be the whole kernel matrix m by m where

1531
01:42:16,910 --> 01:42:21,620
m is the number of training examples so it would be fairly expensive this method

1532
01:42:21,640 --> 01:42:25,350
and also you will need a lot of memory to plug in this matrix so

1533
01:42:25,350 --> 01:42:30,450
if you have more than ten thousand points it's not really feasible on your PC

1534
01:42:30,450 --> 01:42:31,180
could be

1535
01:42:32,510 --> 01:42:35,640
so the first idea for how to improve this

1536
01:42:35,660 --> 01:42:40,870
the situation is that only the support vectors are actually relevant so that i mentioned

1537
01:42:40,870 --> 01:42:44,050
before if we knew what are the non support vectors we could just throw them

1538
01:42:44,050 --> 01:42:48,330
out and only trained on the support vectors and we would get the same solution

1539
01:42:48,350 --> 01:42:53,100
so if you have a problem with the support vectors are small subset in

1540
01:42:53,120 --> 01:42:57,430
that would be very attractive

1541
01:42:57,450 --> 01:43:01,160
so you don't know beforehand but what you can do something in between you can

1542
01:43:01,160 --> 01:43:02,600
in turn

1543
01:43:02,710 --> 01:43:06,690
three quotients and numerous cancelled

1544
01:43:06,690 --> 01:43:09,000
and we clearly got that

1545
01:43:09,020 --> 01:43:16,670
so why do we bother to do this

1546
01:43:16,690 --> 01:43:20,850
well once we will start to the right of the bar

1547
01:43:20,870 --> 01:43:23,140
then we can

1548
01:43:23,170 --> 01:43:27,580
three can play that

1549
01:43:27,620 --> 01:43:28,390
so now

1550
01:43:28,400 --> 01:43:34,310
we're estimating the probability of these three forces being in the respective positions product

1551
01:43:34,310 --> 01:43:36,730
of three independent productions

1552
01:43:36,750 --> 01:43:39,460
what's the probability epitaph will be in third place

1553
01:43:39,500 --> 01:43:43,020
that will be in second place the river will be in the first place and

1554
01:43:43,020 --> 01:43:48,250
we pretend these are all independent other

1555
01:43:48,250 --> 01:43:51,330
is that correct or they in fact

1556
01:43:51,370 --> 01:43:56,330
know how do you know there the cap

1557
01:43:56,350 --> 01:44:02,120
for almost get

1558
01:44:02,850 --> 01:44:08,640
because they can be first year so once once we know the valentine epitaph for

1559
01:44:08,640 --> 01:44:12,400
in second and third place that increases the chance of being in the first place

1560
01:44:12,660 --> 01:44:15,460
so crossing these things are probably actually did

1561
01:44:16,670 --> 01:44:19,250
probably actually did reduce the probability

1562
01:44:19,270 --> 01:44:22,440
from what would have been if we kept

1563
01:44:22,460 --> 01:44:28,140
but you know this but this is not totally crazy assumption in this particular case

1564
01:44:28,140 --> 01:44:29,890
we might be able to prove it

1565
01:44:31,290 --> 01:44:34,540
this is sort of a general pattern of

1566
01:44:34,540 --> 01:44:40,670
how you can estimate lots of things happening once is used channel bodies often decide

1567
01:44:40,670 --> 01:44:41,980
what to ignore

1568
01:44:42,040 --> 01:44:48,810
so maybe in this case these worth ignoring let me give you the terminology

1569
01:44:48,830 --> 01:44:53,830
by crossing these of of the assumption that we're making is the

1570
01:44:53,850 --> 01:45:00,940
whether revere wins is independent of whether valentine places and protection

1571
01:45:01,000 --> 01:45:04,250
not quite independently but conditionally

1572
01:45:04,270 --> 01:45:07,060
so always said if the weather is clear

1573
01:45:07,080 --> 01:45:11,350
there are independent of each other that whether you're wins is unaffected

1574
01:45:11,350 --> 01:45:13,190
by these two events

1575
01:45:13,210 --> 01:45:15,250
if the weather

1576
01:45:15,270 --> 01:45:19,020
so we might say it's completely different about it

1577
01:45:19,060 --> 01:45:20,670
i'm sorry

1578
01:45:20,690 --> 01:45:22,900
completely different if the weather is not clear

1579
01:45:22,920 --> 01:45:25,830
then suddenly the data coupled

1580
01:45:25,850 --> 01:45:30,480
but there are independent if the web so that's why we say conditionally

1581
01:45:30,480 --> 01:45:35,100
i and this conditional independencies in engineering assumption right

1582
01:45:35,120 --> 01:45:39,060
so we decided what we're willing to you can test against

1583
01:45:39,060 --> 01:45:43,180
well if you enough data to be able to find out whether there something like

1584
01:45:46,890 --> 01:45:47,420
in in

1585
01:45:47,440 --> 01:45:50,000
practise this is something we do when we don't have enough data to get what

1586
01:45:50,000 --> 01:45:52,620
we really want to look

1587
01:45:52,640 --> 01:45:57,580
i carla into joke which is that the guys as well

1588
01:45:57,770 --> 01:46:04,830
founder twenty dollars sixty sensible

1589
01:46:04,850 --> 01:46:08,170
OK so let's get back to language identification

1590
01:46:08,190 --> 01:46:13,810
so here we have a real science courses and workshops around the curriculum

1591
01:46:13,830 --> 01:46:17,080
and the only reason i knew how to pronounce that was i was doing language

1592
01:46:17,080 --> 01:46:19,140
idea pieces of the set

1593
01:46:19,160 --> 01:46:28,310
and anybody know who were savages

1594
01:46:28,440 --> 01:46:30,640
got invented polish notation

1595
01:46:30,790 --> 01:46:33,980
was not prefix notation and since this is hard to pronounce

1596
01:46:34,000 --> 01:46:37,670
people actually that also have a slash across

1597
01:46:37,690 --> 01:46:43,900
people call police station and if you if you have an HP calculator uses reverse

1598
01:46:43,900 --> 01:46:48,520
polish notation rewrite three four class instead of three miles four

1599
01:46:49,250 --> 01:46:54,310
so we have to decide we have take a single language

1600
01:46:54,330 --> 01:46:58,020
for the whole to what we think the language is

1601
01:46:58,120 --> 01:47:01,330
well there are some idea of using and models so do we think that this

1602
01:47:01,330 --> 01:47:02,850
is good english

1603
01:47:02,870 --> 01:47:05,520
do we think that english dies

1604
01:47:05,540 --> 01:47:09,020
will be likely to generate a a sentence or do we think that polish dies

1605
01:47:09,330 --> 01:47:11,310
would be likely to generate the

1606
01:47:11,330 --> 01:47:16,670
maybe instead standardisation talk about monkeys typing randomly right except of english monkeys and bullish

1607
01:47:17,870 --> 01:47:22,850
and there are many many of the monkeys maybe the keyboard type typing

1608
01:47:22,870 --> 01:47:27,290
so you know if you're banging this kind of keyboard you're likely to get you're

1609
01:47:27,290 --> 01:47:34,940
now the importance for this just is huttenlocher case is the complexity speed up so

1610
01:47:34,980 --> 01:47:38,960
searching here is a search the complexities into the p

1611
01:47:38,980 --> 01:47:43,630
once you get down to a star model military model then the

1612
01:47:43,650 --> 01:47:48,040
both the training and in recognition search is much faster so n here

1613
01:47:48,110 --> 01:47:49,750
the number of detections

1614
01:47:49,770 --> 01:47:53,340
p and the number of parts so the government of the p twenty

1615
01:47:53,360 --> 01:47:54,590
two n times p

1616
01:47:54,590 --> 01:47:57,070
much much faster search

1617
01:47:57,070 --> 01:48:02,060
that means that both the number of detections number of reasons we we detect in

1618
01:48:02,060 --> 01:48:05,400
the first place and the number of parts can be increases of values in six

1619
01:48:05,400 --> 01:48:10,570
parts and maybe only twenty five to thirty detections now we can start increasing the

1620
01:48:10,570 --> 01:48:12,460
number of detections two

1621
01:48:12,900 --> 01:48:18,800
hundreds of thousands and this is very important where you've got texture in the background

1622
01:48:18,800 --> 01:48:20,190
and the foreground

1623
01:48:20,210 --> 01:48:23,820
because you have to detect many many reasons to be sure that all you need

1624
01:48:23,820 --> 01:48:26,940
is a lots regions on the order interested in

1625
01:48:31,170 --> 01:48:34,210
it's christmas party

1626
01:48:35,980 --> 01:48:38,750
that's learned

1627
01:48:38,750 --> 01:48:40,560
that's it

1628
01:48:40,560 --> 01:48:45,540
there's a slight loss because the inclusion isn't that was quite so well in this

1629
01:48:46,290 --> 01:48:50,170
so in the fully connected to any part can be

1630
01:48:50,170 --> 01:48:55,520
you learn how model any in recognition and in learning any part can be included

1631
01:48:55,540 --> 01:48:59,400
in this case if it doesn't have any

1632
01:48:59,400 --> 01:49:02,980
the anchor parts included the model can't be recognised but any of these other parts

1633
01:49:02,980 --> 01:49:03,800
can be

1634
01:49:04,150 --> 01:49:09,110
included so i mean this is technical detail but that's that's where we will use

1635
01:49:09,110 --> 01:49:11,360
the admin intervention

1636
01:49:17,800 --> 01:49:20,320
it's just a

1637
01:49:20,410 --> 01:49:26,590
it's true essentially so for each

1638
01:49:26,920 --> 01:49:30,730
each detected feature we've got a set of parts as to be assigned

1639
01:49:30,750 --> 01:49:32,710
but it it's

1640
01:49:32,730 --> 01:49:35,290
it's the same reason any to this tree search

1641
01:49:36,440 --> 01:49:38,250
can be done efficiently that you

1642
01:49:38,270 --> 01:49:44,590
you have to consider the pairwise relationships to given the way these things always work

1643
01:49:44,590 --> 01:49:48,840
is given the choice of one you can work out in advance which is the

1644
01:49:48,840 --> 01:49:53,360
best fit of all the other part of that pair that's way these things work

1645
01:49:53,360 --> 01:49:55,170
but could not the case in here

1646
01:49:55,210 --> 01:49:57,670
you have to do everything else

1647
01:49:57,690 --> 01:49:59,090
when you're working at a cost

1648
01:49:59,310 --> 01:50:01,210
the first one

1649
01:50:03,630 --> 01:50:12,690
so you're saying time relationship between the past and the future

1650
01:50:12,790 --> 01:50:15,540
so it's

1651
01:50:17,730 --> 01:50:21,090
the ricci flow constrained

1652
01:50:21,170 --> 01:50:22,980
it just

1653
01:50:27,340 --> 01:50:33,860
so this is the case those who have shown that the world is

1654
01:50:36,270 --> 01:50:39,790
that you going to solve this problem

1655
01:50:39,800 --> 01:50:44,320
one them does answer question about but this template or not OK so you're right

1656
01:50:44,340 --> 01:50:49,920
is the worst case complexity but you say the structure model is very very loose

1657
01:50:49,980 --> 01:50:52,690
then you really can't jump out and see if you think the the case of

1658
01:50:52,690 --> 01:50:57,270
the spotted cats this the gaussians are very very loose you don't you really don't

1659
01:50:57,270 --> 01:51:02,690
have a a a ten place in that case the parts can move very unrestrained

1660
01:51:02,690 --> 01:51:05,630
see in fact you in that case you can't jump out of the a star

1661
01:51:05,630 --> 01:51:07,300
search very quickly

1662
01:51:07,320 --> 01:51:10,090
so that in in the

1663
01:51:10,150 --> 01:51:17,190
in other cases the the appearance term might not be very strong so it

1664
01:51:17,210 --> 01:51:19,500
even though in some cases might be like templated the

1665
01:51:19,690 --> 01:51:24,770
structure term is very very tight in other cases not and correspondingly sometimes appearance is

1666
01:51:24,770 --> 01:51:26,190
very very loose

1667
01:51:26,210 --> 01:51:29,790
so yes this is the worst case complexity is but

1668
01:51:29,840 --> 01:51:32,150
that's all i can say on that

1669
01:51:32,150 --> 01:51:35,250
now the question

1670
01:51:35,300 --> 01:51:40,790
right and his example of stalking even number of parts small

1671
01:51:40,790 --> 01:51:45,610
but increasing the number of detections is recognised spotted cats is a significant improvement

1672
01:51:45,630 --> 01:51:49,460
as i said that's because you've got more chance of finding

1673
01:51:49,650 --> 01:51:53,500
is species section which is sort of the features section in this case if you

1674
01:51:53,500 --> 01:51:58,460
have more detections much more likely that the foreground object cat in this case is

1675
01:51:58,460 --> 01:52:00,750
going to have detections on it

1676
01:52:01,170 --> 01:52:04,670
we're seeing here all these pink dots actions

1677
01:52:07,020 --> 01:52:10,900
well you can see the hundreds of these detections now whereas before the twenty five

1678
01:52:11,650 --> 01:52:16,880
because that does give performance improvement

1679
01:52:16,900 --> 01:52:21,290
second realization is so far we just talked about one type of pageant and in

1680
01:52:21,290 --> 01:52:24,020
fact all the models received so far the the

1681
01:52:24,070 --> 01:52:27,020
the parts of corresponded to parents

1682
01:52:27,060 --> 01:52:29,880
it has been

1683
01:52:29,900 --> 01:52:33,520
after capturing the geometry of the object in the part

1684
01:52:33,540 --> 01:52:36,020
i mean the spatial configuration

1685
01:52:36,020 --> 01:52:40,440
so one judgement straighter ways to go

1686
01:52:40,460 --> 01:52:44,960
four curves which more directly go to the geometry of the part so

1687
01:52:46,000 --> 01:52:48,480
in this case they wanted recognise this guitar

1688
01:52:48,500 --> 01:52:51,130
we can find the outlines objects

1689
01:52:51,150 --> 01:52:52,230
in parts

1690
01:52:52,600 --> 01:52:56,400
you know no one in this case one could find the curve or the objective

1691
01:52:56,570 --> 01:53:01,960
cut on the background because like get broken but using a canny operator some good

1692
01:53:01,960 --> 01:53:07,840
edge detector one can find the boundary subjects we find a scale

1693
01:53:07,860 --> 01:53:12,380
scale invariant representation of is not so important but that's done using by tangent points

1694
01:53:12,380 --> 01:53:17,000
to segment out this region like is the scale of this curve and then becomes

1695
01:53:17,290 --> 01:53:18,360
a part

1696
01:53:18,380 --> 01:53:23,800
just on the same standing is the appearance based image fragments we've seen so far

1697
01:53:23,820 --> 01:53:28,130
so in fact we're going to have three types apart now we're going to have

1698
01:53:28,690 --> 01:53:33,770
the circular salient regions detected by the media and radio operator

1699
01:53:36,610 --> 01:53:38,000
around the

1700
01:53:38,400 --> 01:53:43,980
you can see very well but because around the fuselage of the aircraft and effect

1701
01:53:44,040 --> 01:53:45,540
for type will be

1702
01:53:45,590 --> 01:53:47,360
these multiscale harris

1703
01:53:47,520 --> 01:53:51,130
detect something called it little about this yesterday as well

1704
01:53:51,420 --> 01:53:57,020
OK so we can build in any any type part which for which the detector

1705
01:53:57,020 --> 01:53:59,400
gives a position and scale

1706
01:53:59,460 --> 01:54:01,710
OK that's all required

1707
01:54:04,000 --> 01:54:07,610
when we come to the learning we have to decide which parts should be there

1708
01:54:07,630 --> 01:54:10,490
so that the learning data then we put we pull out a six to the

1709
01:54:10,490 --> 01:54:16,400
data for validation and on the validation data we compare various models with different

1710
01:54:17,000 --> 01:54:20,610
blends of these various types of parts curves or

1711
01:54:20,610 --> 01:54:28,790
patches and determine which blend of parties best design best performance

1712
01:54:29,270 --> 01:54:31,020
so on motorbikes

1713
01:54:32,440 --> 01:54:34,190
this curve is recursive here

1714
01:54:34,190 --> 01:54:36,590
is for heterogeneous

1715
01:54:36,610 --> 01:54:41,460
model which has three parts which the patches the captain brady detector patches in three

1716
01:54:41,460 --> 01:54:43,730
parts which are curves

1717
01:54:45,590 --> 01:54:48,540
showing the model so the that's

1718
01:54:48,570 --> 01:54:53,110
patch kirkpatrick et cetera

1719
01:54:53,650 --> 01:54:59,000
the system is here the here the because being found on the outline of the

1720
01:54:59,040 --> 01:55:00,420
of the wheel

1721
01:55:00,440 --> 01:55:03,320
OK so this heterogeneous

1722
01:55:03,400 --> 01:55:07,420
and this is a increases performance

1723
01:55:07,500 --> 01:55:13,730
for sparse spotted cats the passage which first the recursion here are both patch like

1724
01:55:13,730 --> 01:55:19,250
regions it's the multiscale harris and the guardian brady not because not important for spotted

1725
01:55:19,250 --> 01:55:22,150
cats saw texture blobs

1726
01:55:22,170 --> 01:55:24,610
OK listen recognition of those

1727
01:55:25,920 --> 01:55:28,250
questions and that come to

1728
01:55:30,920 --> 01:55:34,900
right so let's see what we've seen what can we do now in terms of

1729
01:55:36,790 --> 01:55:41,670
what you see in the game again dealing effectively with one aspect of an object

1730
01:55:41,670 --> 01:55:45,420
the from frontal the rear face from the front all profiles

1731
01:55:45,440 --> 01:55:48,070
that's that's what the stage now

1732
01:55:48,090 --> 01:55:53,800
when combining learning techniques with these bitola structures we can effectively do this for maybe

1733
01:55:53,800 --> 01:55:58,150
ten types of object classes and this is not the number of course is going

1734
01:55:58,150 --> 01:55:59,520
to increase

1735
01:55:59,570 --> 01:56:06,980
you we can do translation and scale invariance partial occlusions OK background clutter is OK

1736
01:56:06,980 --> 01:56:09,770
both recognition and in training

1737
01:56:09,770 --> 01:56:13,120
so if you if you find lucene is

1738
01:56:13,160 --> 01:56:14,910
almost what you want but not quite

1739
01:56:14,930 --> 01:56:17,750
then when you have to consider thinking about

1740
01:56:17,810 --> 01:56:21,350
adding something that that's that's the way it works

1741
01:56:21,680 --> 01:56:24,930
it becomes what people want to become

1742
01:56:24,980 --> 01:56:26,100
so not

1743
01:56:26,140 --> 01:56:27,850
is a

1744
01:56:27,890 --> 01:56:32,540
another project that i started apache

1745
01:56:32,560 --> 01:56:37,700
it's as opposed to just a library for doing search

1746
01:56:37,720 --> 01:56:41,410
it's it's a full-on applications

1747
01:56:41,450 --> 01:56:44,520
and the forward search so it's a crawler

1748
01:56:45,040 --> 01:56:48,270
it has tools to maintain the link graph

1749
01:56:48,350 --> 01:56:50,180
which is

1750
01:56:50,250 --> 01:56:56,660
use the search for doing like analysis things like pagerank computations

1751
01:56:56,750 --> 01:57:01,500
as well as a source for getting anchortext for

1752
01:57:02,100 --> 01:57:03,330
the page

1753
01:57:03,370 --> 01:57:05,100
it's very useful to know

1754
01:57:05,230 --> 01:57:08,810
all of the links that point to that page with the text of the anchors

1755
01:57:08,810 --> 01:57:10,100
of those links was

1756
01:57:10,120 --> 01:57:13,310
so you need to be able to maintain this link graph

1757
01:57:14,430 --> 01:57:15,890
it also does

1758
01:57:15,910 --> 01:57:19,770
document format detection passing so they can extract

1759
01:57:19,830 --> 01:57:22,950
tag has been abducted by forgetting

1760
01:57:23,040 --> 01:57:25,560
turning documents into

1761
01:57:25,680 --> 01:57:30,270
text properties that might index

1762
01:57:30,350 --> 01:57:38,080
so working on PDF and RTF and ten hml and given the name name word

1763
01:57:38,160 --> 01:57:44,870
but also language and characters detection are there are

1764
01:57:44,980 --> 01:57:46,500
not things that

1765
01:57:46,520 --> 01:57:50,390
unable to match but finding them in a form that is appropriate for web search

1766
01:57:50,460 --> 01:57:51,850
what was something we

1767
01:57:51,870 --> 01:57:54,430
couldn't find on the shelf so we need to to build these

1768
01:57:54,430 --> 01:57:57,120
generic tools

1769
01:57:57,200 --> 01:57:59,480
and everything is

1770
01:57:59,560 --> 01:58:01,810
is extensible pretty much

1771
01:58:01,870 --> 01:58:07,430
it lists around the perimeter down so that the crawler is extensible in lots of

1772
01:58:07,430 --> 01:58:11,480
ways of passing is extensible what gets index for document

1773
01:58:11,870 --> 01:58:17,580
and how you can how queries are transformed into searches it is also

1774
01:58:18,830 --> 01:58:21,000
so we go into a little more detail

1775
01:58:21,350 --> 01:58:24,100
not uses lucene

1776
01:58:24,140 --> 01:58:26,540
as its underlying search platform

1777
01:58:26,580 --> 01:58:30,230
by default it stores

1778
01:58:30,270 --> 01:58:35,230
these fields in these fields for documents among among others

1779
01:58:35,350 --> 01:58:40,810
so was fields have these properties is also on the property is the basis for

1780
01:58:40,980 --> 01:58:43,910
the core one so it in match

1781
01:58:44,000 --> 01:58:47,100
there a URL field which is stored

1782
01:58:47,180 --> 01:58:51,520
and in the scene story field is one that

1783
01:58:51,580 --> 01:58:53,330
he returned with it

1784
01:58:53,350 --> 01:58:57,560
the one with the actual the literal values stored in the index

1785
01:58:57,620 --> 01:59:01,890
it's index which you can do searches of URL

1786
01:59:01,910 --> 01:59:06,810
of the URL field is analyse which means that we break the urals into words

1787
01:59:06,810 --> 01:59:11,890
fact analyse your also saying we analyse in text the same with the same the

1788
01:59:11,890 --> 01:59:16,060
same tokenizers you talk a little bit more about that

1789
01:59:16,080 --> 01:59:20,000
we have a feel anchors services all the anchor text that is found that associated

1790
01:59:20,000 --> 01:59:20,810
with links

1791
01:59:20,930 --> 01:59:22,310
to each page

1792
01:59:22,330 --> 01:59:25,250
we don't store that we don't need to return with him

1793
01:59:25,270 --> 01:59:27,790
we do index we need to organize it into words

1794
01:59:28,620 --> 01:59:31,100
similarly for the content

1795
01:59:31,160 --> 01:59:36,930
this site pages on this is used by various things so we have that the

1796
01:59:36,930 --> 01:59:38,160
language that's

1797
01:59:38,250 --> 01:59:41,640
the page is written in his also

1798
01:59:41,700 --> 01:59:43,640
the index can be search

1799
01:59:43,660 --> 01:59:47,200
and these are some of these are these are literals or not

1800
01:59:47,220 --> 01:59:49,960
not word strings that

1801
01:59:51,330 --> 01:59:54,600
and the

1802
01:59:55,450 --> 02:00:00,140
it is this

1803
02:00:00,750 --> 02:00:05,080
for showing snippets is a yes much

1804
02:00:05,140 --> 02:00:09,660
much i think at this point has its own snippet library there's is one included

1805
02:00:09,660 --> 02:00:11,270
with seen as well

1806
02:00:11,290 --> 02:00:15,500
so several available to do to exhibit ourselves

1807
02:00:19,290 --> 02:00:20,430
the ten axis

1808
02:00:20,500 --> 02:00:22,330
basically trying to match

1809
02:00:22,330 --> 02:00:25,080
what is the standard for web search

1810
02:00:25,390 --> 02:00:32,200
but providing this in in open source form so that convention is that all query

1811
02:00:32,200 --> 02:00:35,310
terms that users and are required to have to occur somewhere

1812
02:00:35,350 --> 02:00:41,660
in a matching document or an anchortext for matching documents but have always had URL

1813
02:00:41,660 --> 02:00:44,370
the projects and the content of the page

1814
02:00:44,410 --> 02:00:49,460
and there is the reward for terms being closer together

1815
02:00:49,480 --> 02:00:51,700
for proximity

1816
02:00:51,720 --> 02:00:56,410
so the way that this is implemented

1817
02:00:56,460 --> 02:01:01,810
is when a user enters and match two word query like search engine is expanded

1818
02:01:03,580 --> 02:01:04,830
five class

1819
02:01:04,830 --> 02:01:08,040
the same query where

1820
02:01:08,120 --> 02:01:11,080
there's too required clauses which is

1821
02:01:13,730 --> 02:01:15,310
the word search

1822
02:01:15,330 --> 02:01:19,850
has to be in one of your all core content and

1823
02:01:19,890 --> 02:01:22,480
the word engine has to be in one of

1824
02:01:22,500 --> 02:01:23,850
the same field

1825
02:01:23,870 --> 02:01:28,360
so that's sort of the basic building requirements in order to call imagine has had

1826
02:01:28,360 --> 02:01:32,000
both of those in one field or another and then we also

1827
02:01:32,450 --> 02:01:33,680
look for places

1828
02:01:33,700 --> 02:01:40,080
we're reward more these are either phrase searches the telly is its proximity operator

1829
02:01:40,080 --> 02:01:46,040
there were more when the closer together as well as with these these three classes

1830
02:01:46,180 --> 02:01:47,450
do now

1831
02:01:48,770 --> 02:01:50,060
i charles size

1832
02:01:50,100 --> 02:01:51,580
things x y c

1833
02:01:51,660 --> 02:01:56,480
BBC MPEG one or more parameters which

1834
02:01:56,520 --> 02:01:57,540
can be tuned

1835
02:01:57,540 --> 02:02:03,230
now there their default values there but they haven't extensively and that's something we do

1836
02:02:03,230 --> 02:02:05,660
need to work on this is to try to improve the quality

1837
02:02:05,750 --> 02:02:10,310
by by tuning all these the up arrow was a

1838
02:02:10,330 --> 02:02:15,660
in scenes query syntax is boost operator so whatever number we put here this is

1839
02:02:15,660 --> 02:02:19,390
a number which is multiplied into the score of this component

1840
02:02:19,750 --> 02:02:21,160
this clause that it's

1841
02:02:21,200 --> 02:02:22,870
it's combined into scores

1842
02:02:22,890 --> 02:02:25,730
something that has caused this

1843
02:02:25,750 --> 02:02:27,580
so if you put two

1844
02:02:27,580 --> 02:02:33,520
if you made actually will two then matches on the URL will be rewarded twice

1845
02:02:33,520 --> 02:02:36,910
as strongly that matches on anchors kind

