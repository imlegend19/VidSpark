1
00:00:00,000 --> 00:00:05,040
local supervised learning in other words learn

2
00:00:05,090 --> 00:00:06,000
each level

3
00:00:06,010 --> 00:00:11,000
for initializing each level using an unsupervised learning algorithm that takes as input and transforms

4
00:00:11,000 --> 00:00:12,980
it into a new

5
00:00:13,030 --> 00:00:14,320
o space

6
00:00:14,330 --> 00:00:16,570
tries to factor

7
00:00:17,210 --> 00:00:19,830
because of of of the rhine

8
00:00:20,660 --> 00:00:25,650
you can think of what each level is doing his job isation of PCA ICA

9
00:00:25,700 --> 00:00:27,660
but with PCR ICA

10
00:00:27,670 --> 00:00:32,400
when we try to stack these things at multiple levels we don't gain anything here

11
00:00:32,410 --> 00:00:34,850
you can use something like an RBM

12
00:00:34,860 --> 00:00:38,620
restricted boltzmann machine which are going to explain which is going to extract a bunch

13
00:00:38,620 --> 00:00:39,910
of numbers

14
00:00:39,950 --> 00:00:41,430
from my input

15
00:00:41,520 --> 00:00:47,920
in these numbers turn out to be useful at explaining the variations in the input

16
00:00:47,960 --> 00:00:51,060
so once you've trained model of the input distribution

17
00:00:51,070 --> 00:00:55,700
what you get is a side product is a new representation for your input just

18
00:00:55,700 --> 00:00:58,170
like you would get PCA

19
00:00:58,210 --> 00:01:01,420
and what you can do is take this new representation

20
00:01:02,880 --> 00:01:05,530
input data for training the second level

21
00:01:05,580 --> 00:01:07,780
so now you transform the

22
00:01:07,790 --> 00:01:10,950
this new representation h one into reservation h two

23
00:01:11,060 --> 00:01:13,920
you can do that for many levels

24
00:01:15,180 --> 00:01:19,210
it turns out that stacking all these one w of the other actually makes sense

25
00:01:19,210 --> 00:01:21,710
from a mathematical point of view

26
00:01:21,750 --> 00:01:25,790
and the next thing you need to be doing typically is taking all these levels

27
00:01:25,790 --> 00:01:27,200
of representation and

28
00:01:27,260 --> 00:01:33,940
using them as initialisation for a supervised task you care about

29
00:01:33,990 --> 00:01:37,240
you could ask well why would we just trying that

30
00:01:37,370 --> 00:01:40,540
model supervised learning from scratch

31
00:01:40,550 --> 00:01:43,370
well it turns out that doing this treaty

32
00:01:43,380 --> 00:01:45,800
layer wise unsupervised pre training

33
00:01:45,820 --> 00:01:49,950
makes a big difference somehow but the parameters place

34
00:01:49,990 --> 00:01:53,280
before you do gradient based optimisation of the criterion you care

35
00:01:55,000 --> 00:01:57,410
there's a difficult optimisation problem somehow

36
00:01:57,420 --> 00:02:01,340
and by doing this supervised initialisation level by level

37
00:02:01,360 --> 00:02:02,850
somehow our able

38
00:02:03,750 --> 00:02:05,870
to get better results

39
00:02:05,880 --> 00:02:10,710
so the job goes like this we train the kind of unsupervised feature extraction like

40
00:02:10,720 --> 00:02:13,630
the RBM auto encoders i'll talk about

41
00:02:13,680 --> 00:02:16,960
that maps inputs to some representation

42
00:02:17,010 --> 00:02:20,760
that hopefully captures try to capture the activation in the input

43
00:02:20,770 --> 00:02:22,500
you get a new representation

44
00:02:23,360 --> 00:02:25,750
then you trying second level exactly

45
00:02:25,760 --> 00:02:26,850
as before

46
00:02:26,860 --> 00:02:29,210
using this representation as input and you do that

47
00:02:29,250 --> 00:02:30,860
so the number of times

48
00:02:30,920 --> 00:02:32,110
let's say you want to do

49
00:02:33,210 --> 00:02:36,230
you want to predict some y given x y you can take the

50
00:02:36,250 --> 00:02:39,590
the last level observations input to some classifier

51
00:02:39,640 --> 00:02:42,180
and train the classifier

52
00:02:42,230 --> 00:02:43,230
what you could do

53
00:02:43,490 --> 00:02:47,550
that seems to help in most cases actually is once you've

54
00:02:47,560 --> 00:02:51,970
put this classifier on top which might be an SVM logistic regression or something

55
00:02:51,980 --> 00:02:56,360
you can fine-tune all the parameters not just of the logistic regression but also all

56
00:02:56,360 --> 00:02:58,400
these representations

57
00:02:58,450 --> 00:03:00,490
with respect to the supervised objective

58
00:03:00,540 --> 00:03:01,550
and that

59
00:03:01,570 --> 00:03:05,340
give the best results that see in the experiments

60
00:03:05,420 --> 00:03:07,290
so not all of them now

61
00:03:07,300 --> 00:03:10,700
was i didn't say what the feature extractor should be

62
00:03:10,930 --> 00:03:14,920
and i'm going to explain that the but

63
00:03:14,930 --> 00:03:17,300
interestingly you can replace

64
00:03:17,310 --> 00:03:20,450
are the abuse with many other things and get good results

65
00:03:20,600 --> 00:03:23,610
maybe not always as good as IBM's but still

66
00:03:23,660 --> 00:03:26,380
there's something more general going on

67
00:03:26,390 --> 00:03:28,200
what is an RBM

68
00:03:28,250 --> 00:03:31,030
IBM research was machine is a probabilistic

69
00:03:34,190 --> 00:03:36,890
which is essentially a markov random field

70
00:03:37,760 --> 00:03:38,890
some of the

71
00:03:38,900 --> 00:03:41,990
random variables related in some are observed

72
00:03:42,010 --> 00:03:46,810
so the joint distribution of the observed x in late h

73
00:03:46,880 --> 00:03:49,620
it is just a normalized exponential

74
00:03:49,630 --> 00:03:54,600
where in the exponential we have the quadratic polynomial with a particular structure

75
00:03:54,620 --> 00:03:59,200
such that there are no terms for the interactions between

76
00:04:00,130 --> 00:04:04,870
and no term for the interactions between observed variables but only terms for the interactions

77
00:04:04,910 --> 00:04:08,660
between the hidden and and verbal and observe

78
00:04:11,790 --> 00:04:14,420
it turns out that this particular structure

79
00:04:14,480 --> 00:04:15,560
makes four

80
00:04:16,690 --> 00:04:20,180
inference and training

81
00:04:20,220 --> 00:04:24,420
although as i said a lot of variations have been proposed

82
00:04:24,470 --> 00:04:28,700
that's that's that's the european model we can train and by like you we can

83
00:04:28,710 --> 00:04:33,040
compute the exact gradient but we can approximate it

84
00:04:35,700 --> 00:04:38,850
the approximation is is based on gibbs sampling

85
00:04:38,890 --> 00:04:42,090
in in those models so

86
00:04:42,130 --> 00:04:45,120
i won't have time to go through the details of this but the gradient of

87
00:04:45,120 --> 00:04:46,700
the log like likelihood gradient

88
00:04:46,710 --> 00:04:48,530
it involves

89
00:04:48,870 --> 00:04:50,270
a term that

90
00:04:50,460 --> 00:04:54,790
simple and says we want to be

91
00:04:57,680 --> 00:04:59,300
polynomial here to be

92
00:04:59,330 --> 00:05:01,510
large for the observed

93
00:05:01,520 --> 00:05:06,270
x and we wanted to be small for all possible or other possible axes

94
00:05:06,280 --> 00:05:10,200
and in proportion to how the model gives in probability and to get

95
00:05:10,240 --> 00:05:14,360
the gradient respect all possible axes is kind of difficult because we would like to

96
00:05:14,420 --> 00:05:16,990
some of all possible configurations of input

97
00:05:17,080 --> 00:05:22,040
so we use sampling techniques to get these negative examples

98
00:05:22,060 --> 00:05:26,190
and because of the particular structure of the model

99
00:05:26,200 --> 00:05:27,660
gibbs sampling

100
00:05:27,740 --> 00:05:31,650
is very convenient in these models very easy to do you can sample the hidden

101
00:05:31,650 --> 00:05:33,820
variables given the inputs

102
00:05:33,850 --> 00:05:36,930
and then you can sample the inputs given the hidden variables

103
00:05:36,940 --> 00:05:41,950
and those each of those steps is easy because those conditional distributions

104
00:05:43,280 --> 00:05:47,740
meaning that given city input each of the hidden variables are

105
00:05:52,710 --> 00:05:57,410
so number of algorithms have been proposed to train these are RBN's

106
00:05:58,020 --> 00:06:02,610
the first one and still the most commonly used is called contrastive divergence

107
00:06:02,610 --> 00:06:06,660
and the idea is that we're going to start

108
00:06:06,830 --> 00:06:08,600
a chain

109
00:06:08,640 --> 00:06:10,870
to obtain these negative examples

110
00:06:10,930 --> 00:06:12,990
he's one negative example

111
00:06:13,050 --> 00:06:16,920
but instead of running the chain to convergence which would give you

112
00:06:17,500 --> 00:06:18,950
and by artists

113
00:06:18,950 --> 00:06:23,140
estimating of the gradient we just a few steps in fact once that looks very

114
00:06:23,140 --> 00:06:25,710
fast training is very fast with these things

115
00:06:25,730 --> 00:06:29,290
now even though it's unbiased estimate of the gradient

116
00:06:29,300 --> 00:06:32,210
in practice it has been found to work very well

117
00:06:32,220 --> 00:06:36,250
and one reason why it works well that we found is that

118
00:06:37,890 --> 00:06:39,910
the gradient estimate we get

119
00:06:39,920 --> 00:06:43,030
divergence even though it is not the right

120
00:06:43,030 --> 00:06:44,870
but but use like

121
00:06:44,900 --> 00:06:48,160
it's actually sexually mature

122
00:06:51,110 --> 00:06:55,240
and so the cues we find beautiful are cues to that large eyes

123
00:06:55,290 --> 00:06:56,390
full lips

124
00:06:56,410 --> 00:06:58,110
smooth tight skin

125
00:06:58,120 --> 00:07:00,320
signal something else

126
00:07:00,370 --> 00:07:03,930
the reason marker for health

127
00:07:03,940 --> 00:07:06,370
and so what we find beautiful

128
00:07:06,380 --> 00:07:09,920
things like the absence of deformities clear ice

129
00:07:09,940 --> 00:07:12,320
unblemished skin intact e

130
00:07:12,330 --> 00:07:13,370
very big

131
00:07:13,390 --> 00:07:17,120
and an average face

132
00:07:17,120 --> 00:07:19,860
that last part might seem a little bit strange

133
00:07:19,880 --> 00:07:23,090
what would be so good about an average face

134
00:07:23,100 --> 00:07:26,510
and there's different answers to that one answer is

135
00:07:26,530 --> 00:07:28,340
on average based on

136
00:07:29,320 --> 00:07:31,780
it should be considered attractive

137
00:07:31,860 --> 00:07:37,220
because any sort of deformities are variations from the average

138
00:07:37,220 --> 00:07:40,300
and if you average every face together

139
00:07:40,320 --> 00:07:43,660
you get face that were nothing bad has happened to it

140
00:07:43,700 --> 00:07:50,550
there's no distortions there's no deviations as one gets older face gets less symmetrical

141
00:07:50,570 --> 00:07:51,510
and so on

142
00:07:51,530 --> 00:07:57,220
aborigines seems to factor out all the bad things that can happen

143
00:07:57,260 --> 00:07:59,660
good theory how do we know it's true

144
00:08:01,740 --> 00:08:03,860
there's a photo raster

145
00:08:03,910 --> 00:08:07,200
that comment that i have access to for this class so i can look at

146
00:08:07,200 --> 00:08:08,680
each of your pictures

147
00:08:09,340 --> 00:08:12,570
i will make you bet about who has the most beautiful face

148
00:08:12,570 --> 00:08:13,660
in this course

149
00:08:14,050 --> 00:08:16,410
that is it would be

150
00:08:16,430 --> 00:08:18,240
all of you

151
00:08:20,430 --> 00:08:23,340
well to be funded by showed somebody may

152
00:08:25,570 --> 00:08:29,320
if i did and you know a

153
00:08:29,340 --> 00:08:31,340
i don't have the energy to do this and b

154
00:08:31,360 --> 00:08:35,700
it would probably violate four hundred different privacy laws or whatever but if i took

155
00:08:35,720 --> 00:08:38,340
all those places and more them together

156
00:08:38,390 --> 00:08:41,740
i would get a very pretty face

157
00:08:41,780 --> 00:08:46,180
and how do we know this is what people done this event were sold interfaces

158
00:08:47,610 --> 00:08:49,550
here to here

159
00:08:49,570 --> 00:08:51,630
and if you're like most people

160
00:08:51,640 --> 00:08:53,780
you see as you're going to the right

161
00:08:53,890 --> 00:08:56,160
looking better and better and better

162
00:08:57,530 --> 00:08:59,570
but it's actually not so subtle

163
00:08:59,590 --> 00:09:02,030
that babies don't notice

164
00:09:02,050 --> 00:09:07,320
the same researchers are constructed this these spaces average caucasian faces

165
00:09:07,360 --> 00:09:08,740
male and female

166
00:09:08,760 --> 00:09:13,050
i have shown in faces the babies and find babies prefer to look at average

167
00:09:14,360 --> 00:09:19,430
suggest that our preference for averaging is not the product of culture rather is to

168
00:09:19,430 --> 00:09:22,360
some extent hardwired

169
00:09:22,370 --> 00:09:24,610
these people don't exist

170
00:09:24,610 --> 00:09:26,390
the computer composites

171
00:09:28,070 --> 00:09:29,820
a heavily average

172
00:09:29,820 --> 00:09:31,010
male face

173
00:09:31,010 --> 00:09:36,180
and the heavily average female figures both from application data sample

174
00:09:36,220 --> 00:09:37,860
they don't right

175
00:09:37,930 --> 00:09:42,530
the faces they don't seem to care for instance is is identical so don't you

176
00:09:42,530 --> 00:09:44,360
can use here

177
00:09:44,360 --> 00:09:46,950
but pretty attractive

178
00:09:46,990 --> 00:09:50,890
but the story of attractiveness does not end there

179
00:09:50,910 --> 00:09:54,700
how do you get a better than average face what can you do spaces and

180
00:09:54,760 --> 00:09:56,700
traces and make them work

181
00:09:56,740 --> 00:09:58,340
even better

182
00:09:58,360 --> 00:10:00,700
well the vote

183
00:10:00,740 --> 00:10:01,760
was pretty

184
00:10:01,820 --> 00:10:04,700
who says the one on the right

185
00:10:04,720 --> 00:10:06,970
this is the one on the left

186
00:10:07,030 --> 00:10:08,910
left is average face

187
00:10:08,930 --> 00:10:12,890
and there may be variation in this cluster definitely variations what you will be able

188
00:10:12,890 --> 00:10:15,680
to this is a feminized version

189
00:10:15,700 --> 00:10:20,740
of the average face where certain prototype features were made more feminine than average

190
00:10:22,780 --> 00:10:24,260
as more sexual

191
00:10:25,780 --> 00:10:29,180
this is more complicated

192
00:10:40,680 --> 00:10:42,070
the interface

193
00:10:42,140 --> 00:10:45,870
o thing space a is more attracted

194
00:10:45,930 --> 00:10:50,340
the main phase b is more attractive

195
00:10:51,740 --> 00:10:54,030
most people

196
00:10:55,340 --> 00:10:56,870
face b

197
00:10:56,930 --> 00:10:59,240
the exception is

198
00:10:59,240 --> 00:11:02,800
and this has been statistically replicate i think now in three laps

199
00:11:03,110 --> 00:11:08,450
phase a is preferred by women who are violating

200
00:11:08,470 --> 00:11:11,840
and then the story about why

201
00:11:11,870 --> 00:11:13,840
is it is

202
00:11:13,840 --> 00:11:18,510
complicated and will take is beyond the scope of this class but the idea that

203
00:11:18,510 --> 00:11:24,390
that is really handsome guy he's young he's healthy he looks strong good provider

204
00:11:24,430 --> 00:11:26,390
this guy's really high

205
00:11:28,390 --> 00:11:32,010
he may not be a good provider and everything but i'm sure is wonderful genes

206
00:11:32,220 --> 00:11:35,220
so the idea is that once you have sex with him

207
00:11:35,280 --> 00:11:37,910
and then have him raise the kids

208
00:11:38,010 --> 00:11:48,300
we've talked so far about things about about sex and sexual attractiveness largely from a

209
00:11:48,300 --> 00:11:50,450
biological perspective

210
00:11:50,470 --> 00:11:54,700
looking at universal's and in fact there are some universals in what men and women

211
00:11:54,700 --> 00:11:58,240
have in common and what distinguished men and women

212
00:11:59,490 --> 00:12:03,240
and some of the sex differences particularly related to aggression

213
00:12:04,200 --> 00:12:05,890
and preference

214
00:12:05,910 --> 00:12:10,200
seem to be universal they seem to show up to some extent across every culture

215
00:12:10,200 --> 00:12:11,540
they have to explain

216
00:12:12,510 --> 00:12:13,990
many more data

217
00:12:14,010 --> 00:12:15,640
in this calculation

218
00:12:15,700 --> 00:12:18,100
same time penalizes simple models

219
00:12:18,110 --> 00:12:20,300
because by the introduction of the noise

220
00:12:20,310 --> 00:12:21,780
the error model

221
00:12:21,830 --> 00:12:28,010
they have to also explain alot more data

222
00:12:29,010 --> 00:12:34,720
everyone happy with that so far

223
00:12:34,790 --> 00:12:37,490
that's just setting the scene so what i want to do the rest of the

224
00:12:38,220 --> 00:12:40,250
maybe will creep balance the next

225
00:12:40,630 --> 00:12:42,430
looking very simple problem in

226
00:12:42,490 --> 00:12:44,290
data modeling just modelling

227
00:12:44,300 --> 00:12:46,470
univariate real valued data

228
00:12:47,620 --> 00:12:51,400
hopefully the and will arrive back at this point where we can look at

229
00:12:51,490 --> 00:12:52,810
potential models

230
00:12:52,870 --> 00:12:54,390
sequence of models

231
00:12:54,400 --> 00:12:59,350
see this effect to work again

232
00:12:59,380 --> 00:13:00,500
so here's a

233
00:13:00,530 --> 00:13:03,510
simple modelling problem

234
00:13:03,650 --> 00:13:07,200
so we have a set of supposedly mystery data we don't really know what's come

235
00:13:08,150 --> 00:13:12,260
imagine as we go along really don't know anything about it but the truth is

236
00:13:12,300 --> 00:13:14,630
the purposes of this lecture

237
00:13:14,640 --> 00:13:19,390
and many of them for years that generated the data right again

238
00:13:20,210 --> 00:13:23,050
there are actually fifteen samples generated from

239
00:13:23,110 --> 00:13:24,780
so i wavefunctions

240
00:13:24,840 --> 00:13:28,670
with semantic as you know is standard deviation of twenty

241
00:13:28,760 --> 00:13:30,760
and just in terms of notation

242
00:13:30,800 --> 00:13:33,030
are you call the input x

243
00:13:33,040 --> 00:13:35,150
of functional call y

244
00:13:35,160 --> 00:13:39,400
and our observations of the target variables are called t

245
00:13:39,520 --> 00:13:45,650
got fifteen of these fifty inputs fifteen corresponding target

246
00:13:45,660 --> 00:13:47,310
kind of models gonna look at

247
00:13:48,330 --> 00:13:49,340
because you want to

248
00:13:49,350 --> 00:13:51,140
have some

249
00:13:51,140 --> 00:13:54,290
transparent analysis of what we're doing we're going to give her

250
00:13:54,310 --> 00:13:56,180
almost the simplest form of model

251
00:13:56,300 --> 00:13:59,730
those models that are linear in the parameters

252
00:13:59,770 --> 00:14:02,770
so all prediction is simply going to be weighted sum

253
00:14:02,810 --> 00:14:03,750
of some

254
00:14:03,760 --> 00:14:05,760
fixed basis functions

255
00:14:05,820 --> 00:14:07,400
the basis functions

256
00:14:07,790 --> 00:14:09,290
could be linear but

257
00:14:09,350 --> 00:14:13,760
typically we use nonlinear functions in this example using gas

258
00:14:13,840 --> 00:14:17,160
data center basis functions

259
00:14:17,170 --> 00:14:20,890
so predictable be non-linear but of course the key thing is called a linear model

260
00:14:20,920 --> 00:14:23,670
linear in the parameters model because the parameters

261
00:14:23,750 --> 00:14:26,120
only appear linear

262
00:14:26,150 --> 00:14:27,340
and so

263
00:14:27,400 --> 00:14:31,970
and optical problem we've got fifteen these gaussians centered on

264
00:14:32,010 --> 00:14:33,240
on the x axis

265
00:14:33,250 --> 00:14:34,930
on each data point

266
00:14:35,000 --> 00:14:41,790
so are prediction model is a linearly weighted sum of those fifteen get

267
00:14:41,810 --> 00:14:43,690
so somehow we want to

268
00:14:43,690 --> 00:14:47,230
find a good model to find good values space and

269
00:14:47,370 --> 00:14:49,310
built in weight

270
00:14:51,490 --> 00:14:55,160
the range approaches but perhaps the most simple simply to say

271
00:14:55,170 --> 00:14:56,740
let's try to fit the data

272
00:14:56,770 --> 00:14:58,340
by defining an error measure

273
00:14:58,420 --> 00:15:01,900
sure you're all familiar with this idea of least squares

274
00:15:01,910 --> 00:15:04,330
so we define the relation least squares error

275
00:15:04,440 --> 00:15:06,650
is simply the sum squared deviation

276
00:15:06,770 --> 00:15:09,790
the data from all predictions

277
00:15:09,810 --> 00:15:13,550
and this is the population for different reasons but one obvious one is

278
00:15:14,900 --> 00:15:16,060
because the weights appear

279
00:15:17,050 --> 00:15:19,030
we can minimize it analytically

280
00:15:19,090 --> 00:15:20,450
and if we defined

281
00:15:20,500 --> 00:15:22,230
the design matrix phi

282
00:15:22,240 --> 00:15:23,050
as the

283
00:15:23,070 --> 00:15:25,360
matrix of the output of every basis functions

284
00:15:25,370 --> 00:15:26,760
for every data point

285
00:15:26,770 --> 00:15:29,040
we can have this analytic

286
00:15:29,070 --> 00:15:31,710
they expect to expression that gives us the solution

287
00:15:31,730 --> 00:15:35,800
the set of weights that minimizes

288
00:15:35,850 --> 00:15:39,500
now of course we know in this case i think you probably expecting we've got

289
00:15:39,500 --> 00:15:41,920
fifteen basis functions

290
00:15:41,950 --> 00:15:43,580
fifteen data points

291
00:15:43,640 --> 00:15:46,420
we should be able to fit the data

292
00:15:46,430 --> 00:15:47,920
and that's what we see

293
00:15:47,930 --> 00:15:52,710
so that's the least squares fit using RBF model with some gasoline basis functions

294
00:15:52,760 --> 00:15:54,830
noisy data

295
00:15:54,840 --> 00:15:55,940
in principle

296
00:15:55,980 --> 00:15:57,300
this is the idea of it

297
00:15:58,070 --> 00:15:58,830
we know

298
00:15:58,850 --> 00:16:01,510
the underlying generator was assigned from

299
00:16:01,510 --> 00:16:04,450
so this is the sideline

300
00:16:05,870 --> 00:16:07,230
an obvious question is

301
00:16:07,230 --> 00:16:11,230
which of these models is actually better

302
00:16:11,250 --> 00:16:13,870
so we get to a sort of philosophical point

303
00:16:14,340 --> 00:16:16,910
it's important to realize

304
00:16:17,010 --> 00:16:20,770
without any further assumptions without knowing where the data came from

305
00:16:20,820 --> 00:16:22,000
we can't judge

306
00:16:22,010 --> 00:16:24,260
between these two models

307
00:16:24,270 --> 00:16:25,910
we don't know

308
00:16:25,910 --> 00:16:32,460
whether the data really came from this more complex function with no noise

309
00:16:32,510 --> 00:16:35,050
or whether it came from a simple function

310
00:16:35,120 --> 00:16:36,600
with noise

311
00:16:36,730 --> 00:16:39,930
we just can't tell

312
00:16:39,940 --> 00:16:45,290
without any further information we can't make any further progress modelling this data

313
00:16:45,340 --> 00:16:48,180
we must accept

314
00:16:48,180 --> 00:16:52,110
we're going to have to introduce some prior knowledge or belief or understanding to this

315
00:16:54,270 --> 00:16:55,860
so we don't give up

316
00:16:55,920 --> 00:16:58,060
because we're fairly happy to make any assumptions

317
00:16:58,190 --> 00:17:02,190
in the real world and the kind of data modelling problems that we tackle

318
00:17:02,210 --> 00:17:05,170
the underlying functions that we expect to find

319
00:17:05,250 --> 00:17:08,340
probably smooth and probably

320
00:17:08,340 --> 00:17:12,560
people can get and get to know each other a bit better while the quality

321
00:17:12,560 --> 00:17:15,550
is not very good and

322
00:17:15,570 --> 00:17:18,650
maybe can install this here

323
00:17:18,820 --> 00:17:28,740
OK i'll not use the microphone because

324
00:17:29,970 --> 00:17:31,400
and i have to focus this

325
00:17:39,280 --> 00:17:41,110
actually trying to break

326
00:17:41,110 --> 00:17:44,350
i tried break but the light was still on

327
00:18:16,600 --> 00:18:17,820
so just before

328
00:18:17,840 --> 00:18:22,910
me i'll switch the lights because maybe it will make things easier

329
00:18:22,920 --> 00:18:29,590
i don't like this

330
00:18:29,740 --> 00:18:35,240
OK now you see quite a few face detections lighting up

331
00:18:35,250 --> 00:18:38,930
some of them are faces some of them are not faces

332
00:18:38,940 --> 00:18:45,910
but of course it's it's a pretty poor quality camera and a fairly cluttered scenes

333
00:18:46,290 --> 00:18:50,590
which again

334
00:18:50,640 --> 00:18:53,880
so what you're seeing here is

335
00:18:53,920 --> 00:18:57,730
of course support vector machines that some of you

336
00:18:57,820 --> 00:18:59,420
some of you may know

337
00:18:59,440 --> 00:19:04,500
some of you may know and others maybe not it's kind of classifier let's train

338
00:19:04,510 --> 00:19:09,630
on data in this case the data are positive and negative examples

339
00:19:09,660 --> 00:19:17,010
so the examples of faces in examples of nonfaces so background clutter

340
00:19:17,030 --> 00:19:23,190
and the classifiers trained on a few thousand such examples and after which can automatically

341
00:19:23,190 --> 00:19:25,760
recognise faces from non faces

342
00:19:25,920 --> 00:19:31,260
and this this is the problem of binary pattern recognition of binary classification with two

343
00:19:31,260 --> 00:19:33,960
different classes of objects and we would like to find

344
00:19:34,100 --> 00:19:39,650
general rule which will assign new inputs to one of these two classes based on

345
00:19:39,650 --> 00:19:43,610
one face

346
00:19:46,170 --> 00:19:48,320
so let me start with some

347
00:19:48,330 --> 00:19:51,400
it informal thoughts on machine learning

348
00:19:51,450 --> 00:19:54,730
so in my lecture will be fairly

349
00:19:54,740 --> 00:20:02,010
none theoretical because there are other lecturers lectures for in statistical learning theory that will

350
00:20:02,010 --> 00:20:04,660
afterwards provide you with the

351
00:20:04,670 --> 00:20:10,280
statistical basis for what i'm talking about so i will not talk much about statistics

352
00:20:10,290 --> 00:20:15,750
talk about some informal ideas in machine learning a little bit about functional analysis and

353
00:20:15,750 --> 00:20:19,950
statistics because of lecturers will supply you with that later on

354
00:20:19,970 --> 00:20:22,800
so for my purposes

355
00:20:22,820 --> 00:20:29,660
motivation learning is quite simple so we have some data taken from input and output

356
00:20:29,660 --> 00:20:34,210
domain x and y so for instance they could be a class of all images

357
00:20:34,210 --> 00:20:35,680
of a certain size

358
00:20:35,700 --> 00:20:38,190
it contains faces among faces

359
00:20:39,440 --> 00:20:40,540
and why

360
00:20:40,560 --> 00:20:44,490
could be a plus or minus one plus one

361
00:20:44,530 --> 00:20:48,530
meaning face minus one million on so it's just as

362
00:20:48,600 --> 00:20:51,180
set containing two elements

363
00:20:51,940 --> 00:20:58,000
when i trained classifier on given training set consists of males x and y

364
00:20:58,670 --> 00:21:02,130
some of them places some of the nonfaces and

365
00:21:02,180 --> 00:21:07,430
in machine learning people often talk about something called generalisation given

366
00:21:07,500 --> 00:21:11,240
a number of examples you would like to make a prediction on the new example

367
00:21:11,240 --> 00:21:14,300
so you would like to find the general rule that will generalize to new examples

368
00:21:14,840 --> 00:21:20,380
that you haven't seen before so in particular given a previously unseen x

369
00:21:20,390 --> 00:21:23,360
given a new face on one face patch

370
00:21:23,470 --> 00:21:26,970
i would like to assign it right as we would find to predict that you

371
00:21:26,970 --> 00:21:30,340
would like to predict the right output y for it

372
00:21:30,540 --> 00:21:36,350
and one way to think about this would be to say well in some sense

373
00:21:36,350 --> 00:21:39,150
my prediction why would be reasonable if the new pair

374
00:21:39,530 --> 00:21:43,740
x and y so i'm given x and i want to find the corresponding y

375
00:21:43,830 --> 00:21:47,100
is somewhat similar to the ones in the training set

376
00:21:47,110 --> 00:21:49,190
this is the training set here

377
00:21:49,240 --> 00:21:52,700
and then of course the question comes up how do we measure similarity

378
00:21:54,510 --> 00:21:57,090
for outputs people usually

379
00:21:57,110 --> 00:22:01,840
use something which is called the loss function to measure similarity so for instance if

380
00:22:01,840 --> 00:22:07,000
we have labels plus or minus one we could just say almost zero whenever the

381
00:22:07,000 --> 00:22:12,290
labels correct so whenever we say face it's really if a is almost zero whenever

382
00:22:12,290 --> 00:22:16,250
we say non face and it's really an en face or losses zero likewise

383
00:22:16,350 --> 00:22:21,210
but for the inputs it's a little bit less trivial hard to measure similarity

384
00:22:21,230 --> 00:22:26,490
between different inputs and that's where concept comes up which is called kernel

385
00:22:26,690 --> 00:22:28,380
which is

386
00:22:28,420 --> 00:22:29,720
basic four

387
00:22:29,730 --> 00:22:34,090
a number of different learning algorithms nowadays i would like to spend some time on

388
00:22:37,420 --> 00:22:43,070
so what i will consider this possible kernels are symmetric functions

389
00:22:43,270 --> 00:22:50,710
so defined on x times x so i have two inputs x prime could be

390
00:22:50,710 --> 00:22:54,670
two faces so they could be computer on nonfaces of one phase one and phase

391
00:22:54,680 --> 00:23:00,620
and i would like to assign a kernel value k of x can prime

392
00:23:00,670 --> 00:23:02,190
two these

393
00:23:07,000 --> 00:23:10,190
one example of such kernels this this function k

394
00:23:10,200 --> 00:23:11,460
it's called the kernel

395
00:23:13,310 --> 00:23:16,540
this function of capture some all notion of similarity

396
00:23:16,550 --> 00:23:17,490
for the moment

397
00:23:17,500 --> 00:23:22,620
in one example that we could use it for input domain is a vector space

398
00:23:22,660 --> 00:23:27,860
at the end we could just consider what's called the canonical dot product between the

399
00:23:27,870 --> 00:23:29,970
two vectors x and x prime

400
00:23:30,200 --> 00:23:33,680
so this is the sum over the i th coordinate of x

401
00:23:33,710 --> 00:23:36,570
times the qualitative

402
00:23:36,870 --> 00:23:38,010
now if

403
00:23:38,080 --> 00:23:40,320
x is not a dot product space

404
00:23:40,360 --> 00:23:43,520
and it's not obvious why it should be the product space me even in the

405
00:23:43,520 --> 00:23:44,370
case of images

406
00:23:44,910 --> 00:23:47,560
you first have to represent them as vectors

407
00:23:47,580 --> 00:23:51,410
before you can take the product but if it's not the products based on we

408
00:23:51,410 --> 00:23:52,770
will assume

409
00:23:52,780 --> 00:23:58,640
that's at least k has a represen tation as a dot product space has a

410
00:23:58,640 --> 00:24:02,330
dot product in some of of space which is the products

411
00:24:02,560 --> 00:24:07,580
so we're assuming that if x and x prime are not living in the product

412
00:24:07,580 --> 00:24:11,610
space then at least they can be mapped to some dot product space using a

413
00:24:11,610 --> 00:24:12,660
mapping phi

414
00:24:12,670 --> 00:24:16,890
but we can think of is all representation sometimes is also called the feature map

415
00:24:17,640 --> 00:24:21,060
so we assume we have nothing to them of space which is the product space

416
00:24:21,060 --> 00:24:25,070
and then we have the the products k corresponds to the product in some other

417
00:24:25,070 --> 00:24:26,600
space at least

418
00:24:26,680 --> 00:24:31,990
so in that case we can think of all the inputs or sometimes they called

419
00:24:31,990 --> 00:24:36,560
patterns the axis of the inputs we can think of them as actually five x

420
00:24:36,560 --> 00:24:37,850
five x y

421
00:24:37,900 --> 00:24:41,630
and then in that of a space is not the product space we can do

422
00:24:41,630 --> 00:24:48,840
whatever we can do based on dot products we can carry out geometric algorithms

423
00:24:48,840 --> 00:24:54,240
the first question we want to discuss has to do with the possibility of my

424
00:24:54,240 --> 00:24:59,850
surviving my death is there life after death is the possibility that i might still

425
00:24:59,850 --> 00:25:03,590
exist to survive after my death

426
00:25:04,910 --> 00:25:07,970
at first glance in fact i think it's that england is going to turn out

427
00:25:07,970 --> 00:25:08,440
to be true

428
00:25:08,930 --> 00:25:13,850
you might think that the answer to this question would depend on two basic issues

429
00:25:13,850 --> 00:25:17,860
you do i survived my death do we survive death

430
00:25:17,880 --> 00:25:20,820
you think first we have to get clear on is

431
00:25:20,830 --> 00:25:24,040
well what i am i what kind of

432
00:25:24,050 --> 00:25:28,960
o thing and i or generalizing the kind of thing is the person what are

433
00:25:28,960 --> 00:25:34,520
we made of what are our parts it seems plausible to think that

434
00:25:34,540 --> 00:25:36,880
before we can answer the question

435
00:25:37,170 --> 00:25:41,180
do i survived we need to know how i'm built

436
00:25:41,190 --> 00:25:42,090
and so

437
00:25:42,110 --> 00:25:45,000
the first thing we're going to spend a fair bit of time on is trying

438
00:25:45,000 --> 00:25:49,650
to get clear on what that person what are the fundamental to the fundamental building

439
00:25:49,650 --> 00:25:54,280
blocks of a person second question that you might think we want to get clear

440
00:25:54,280 --> 00:25:59,020
on is what's the idea of what the concept of surviving

441
00:25:59,040 --> 00:26:01,860
so we need to get it to ask how do i survive we need to

442
00:26:01,860 --> 00:26:03,920
get clear on what and why

443
00:26:03,970 --> 00:26:08,330
and what is it to survive what is it for something that exists in the

444
00:26:08,330 --> 00:26:10,920
future to be me

445
00:26:10,960 --> 00:26:18,030
now this question can be discussed philosophically quite general terms what the nature of persistence

446
00:26:18,030 --> 00:26:24,760
of identity over time but since we are especially interested in beings like us

447
00:26:25,960 --> 00:26:31,190
this topic this sort of some specialized version of the question of identity gets discussed

448
00:26:31,190 --> 00:26:33,510
under the rubric of the topic

449
00:26:33,520 --> 00:26:38,980
personal identity what's the key or the nature of the basis of personal identity as

450
00:26:38,980 --> 00:26:42,710
we might put it what is it for somebody who's here

451
00:26:42,740 --> 00:26:46,060
next week to be the same person

452
00:26:46,070 --> 00:26:47,800
as me

453
00:26:47,820 --> 00:26:52,460
what's the nature personal identity so as i say at first glance you might think

454
00:26:52,480 --> 00:26:56,040
but to get clear on the answer to do y

455
00:26:56,050 --> 00:27:00,870
well my dir could i survive my death we need to know

456
00:27:00,910 --> 00:27:06,050
what am i what the person what's the metaphysical composition of people on the one

457
00:27:06,050 --> 00:27:10,000
hand and we need to get clear on the nature of identity or persistence or

458
00:27:10,000 --> 00:27:14,710
more specifically personal identity now as i say i believe that when push comes to

459
00:27:16,020 --> 00:27:19,200
we do need to get clear about both those questions and so that's going to

460
00:27:19,200 --> 00:27:22,500
take that for several weeks of the class were going to spend a couple of

461
00:27:22,500 --> 00:27:26,130
weeks talking about what that person

462
00:27:26,150 --> 00:27:31,150
and going spend several weeks at least weaker so talking about the nature of personal

463
00:27:32,740 --> 00:27:38,870
but before we can even get started there are a question really an objection to

464
00:27:38,870 --> 00:27:39,680
the whole

465
00:27:39,710 --> 00:27:44,120
enterprise so about to spend a lot of time asking the philosophical question is there

466
00:27:44,120 --> 00:27:49,220
life after death could be life after death might have survived my death

467
00:27:49,260 --> 00:27:53,940
but there is no philosophical objection to the entire question and the objection is fairly

468
00:27:53,940 --> 00:28:01,620
simple it says the whole question is misconceived it's based on a confusion

469
00:28:01,630 --> 00:28:04,760
once we see the confusion

470
00:28:04,830 --> 00:28:10,190
we can see what the answer to our questions could i survive my death the

471
00:28:10,190 --> 00:28:14,750
answers back to be this is what the objections says has got to be obviously

472
00:28:14,750 --> 00:28:17,420
not hard to hear his objection

473
00:28:17,730 --> 00:28:21,760
i should mention that the very first reading that you're going to be doing a

474
00:28:21,760 --> 00:28:28,510
couple of pages from j rosenberg a contemporary philosopher and he gives us a version

475
00:28:28,510 --> 00:28:30,250
of this objection

476
00:28:30,260 --> 00:28:34,790
so i'll give you one version you have another version in your readings

477
00:28:34,810 --> 00:28:38,130
the objection basically says what is it

478
00:28:38,180 --> 00:28:42,550
i mean to say that somebody's died you know asking is there life after death

479
00:28:42,550 --> 00:28:48,390
what does it mean to say that somebody has died one natural definition of death

480
00:28:48,390 --> 00:28:51,740
might be something like the end of life

481
00:28:51,760 --> 00:28:54,780
all right so then if that's the right thing to ask is there life after

482
00:28:54,780 --> 00:29:00,920
death is just asking is there life after the end of a lot

483
00:29:00,930 --> 00:29:04,730
and the answer to that would be pretty obvious well

484
00:29:04,780 --> 00:29:08,170
obviously the answer to that is no

485
00:29:08,190 --> 00:29:12,360
after all if we say once you've run out of life

486
00:29:12,370 --> 00:29:14,240
is there any more life

487
00:29:15,600 --> 00:29:20,200
so that's that's that's like asking when i've eaten up all the food on my

488
00:29:20,200 --> 00:29:22,660
plate is there any food left on my plate

489
00:29:23,710 --> 00:29:26,920
what happens in the movie after the movie ends

490
00:29:26,940 --> 00:29:28,490
these are

491
00:29:28,500 --> 00:29:35,520
stupid questions because what you understand what they're asking the answer just building it follows

492
00:29:35,520 --> 00:29:39,820
trivially so although it has seemed to people over the age is that the question

493
00:29:40,040 --> 00:29:43,450
is there life after death is one of the great mysteries one of the great

494
00:29:43,450 --> 00:29:45,630
philosophical things to ponder

495
00:29:45,680 --> 00:29:50,850
the objection says that's the kind of illusions in fact what you think about it

496
00:29:50,850 --> 00:29:54,750
and not all that long you can see the answer is going to be no

497
00:29:54,750 --> 00:30:00,800
they couldn't possibly be life after death couldn't possibly be like after the end of

498
00:30:02,010 --> 00:30:04,660
or suppose we ask the question a slightly different way

499
00:30:04,680 --> 00:30:09,330
my i survived my death

500
00:30:09,350 --> 00:30:12,540
well what's the word survive me

501
00:30:12,550 --> 00:30:18,090
well survivors something like we say that somebody is a survivor if something's happening they

502
00:30:18,090 --> 00:30:22,010
haven't having died is still alive

503
00:30:22,030 --> 00:30:26,790
you know when a car accident you ask you know so-and-so died so and so

504
00:30:26,800 --> 00:30:31,710
survived you know this person survives to say they survived is just saying that there

505
00:30:31,780 --> 00:30:33,380
still alive

506
00:30:35,010 --> 00:30:40,010
my i survived my dad is like asking

507
00:30:40,030 --> 00:30:43,560
might i still be alive after

508
00:30:43,600 --> 00:30:46,400
well what's death death is the end of life so

509
00:30:46,410 --> 00:30:49,340
might i still be alive after i

510
00:30:49,360 --> 00:30:50,530
stop living

511
00:30:50,540 --> 00:30:54,650
might i be one of the people who didn't die when i died

512
00:30:54,670 --> 00:30:57,020
that's the answer that is again

513
00:30:57,040 --> 00:30:58,520
i don't know

514
00:30:58,540 --> 00:31:03,250
you could possibly survive you're death given the very dif with this in mind as

515
00:31:05,080 --> 00:31:10,220
and this reminds me of this joke you probably told seemed hysterical when you're were

516
00:31:10,220 --> 00:31:12,030
seven right

517
00:31:12,050 --> 00:31:20,610
plane crashes are exactly on the border of canada and the united states exactly on

518
00:31:20,610 --> 00:31:27,460
the border with and the dead people everywhere where they bury the survivors

519
00:31:27,530 --> 00:31:32,550
the answer is you don't vary the survivors so we use everything i don't know

520
00:31:32,550 --> 00:31:38,020
any better than it can be very america right it has no bearing the survivors

521
00:31:38,520 --> 00:31:42,210
because survivors are people they haven't yet

522
00:31:42,820 --> 00:31:48,540
can i survive my death is like asking could i not have yet died after

523
00:31:48,540 --> 00:31:50,010
the answer is

524
00:31:50,020 --> 00:31:53,840
of course you have to have died a few died and you have survived if

525
00:31:53,840 --> 00:31:55,070
you died

526
00:31:55,110 --> 00:31:59,830
right so the question cannot even get off the ground that this is how the

527
00:31:59,830 --> 00:32:01,610
objection goes

528
00:32:01,630 --> 00:32:06,100
now i don't mean to be utterly dismissive of the objection that's why i spend

529
00:32:06,100 --> 00:32:08,400
a couple of minutes trying to spell it out

530
00:32:08,420 --> 00:32:11,430
but i think there's a way to

531
00:32:11,450 --> 00:32:13,910
respond to it

532
00:32:13,920 --> 00:32:19,080
we just have to get clear about what precisely the question is that we're trying

533
00:32:19,080 --> 00:32:22,250
to ask and this is something that rosenberg try to get clear out as well

534
00:32:22,250 --> 00:32:24,160
so here's my attempt to

535
00:32:24,180 --> 00:32:28,730
make the question both a bit more precise and question that's an open question a

536
00:32:28,730 --> 00:32:31,990
question we can legitimately raise

537
00:32:31,990 --> 00:32:37,710
in the online setting so in the the set up for that is we have

538
00:32:37,720 --> 00:32:39,270
we have

539
00:32:39,320 --> 00:32:41,580
we want to at every iteration

540
00:32:41,590 --> 00:32:44,490
we have some mahalanobis matrix

541
00:32:45,460 --> 00:32:50,110
at step step t say and we receive a pair of points

542
00:32:50,130 --> 00:32:52,710
and we predict the distance between those

543
00:32:52,720 --> 00:32:58,260
those two points and and and then we receive the true distance and incur some

544
00:32:58,260 --> 00:33:03,050
loss for how how far off we were from the from the true distance

545
00:33:03,150 --> 00:33:04,750
then we update our

546
00:33:04,770 --> 00:33:07,670
mahalanobis metric for the next iteration

547
00:33:07,770 --> 00:33:11,500
and what we want to do is we want to minimize the total loss the

548
00:33:11,500 --> 00:33:13,620
sum of the losses that

549
00:33:13,640 --> 00:33:18,610
at all iterations and what's typically done in online learning is you you want to

550
00:33:18,610 --> 00:33:21,680
prove some sort of regret bound that is you want to show that your you

551
00:33:21,690 --> 00:33:25,260
don't do much worse than the best possible

552
00:33:25,270 --> 00:33:27,820
offline algorithms

553
00:33:28,060 --> 00:33:32,820
so we were able to prove the bound so al amal is the loss of

554
00:33:32,910 --> 00:33:38,260
online metric learning algorithm and l a stars the loss of the best offline metric

555
00:33:39,270 --> 00:33:40,970
and so

556
00:33:40,980 --> 00:33:46,680
note that alistar is not the the loss of of like are off offline

557
00:33:46,700 --> 00:33:53,940
information theoretic metric learning algorithm is like the best possible offline algorithm that could exist

558
00:33:53,940 --> 00:33:55,840
in theory

559
00:33:56,040 --> 00:34:01,950
so in in a few minutes i have left out just present some

560
00:34:01,970 --> 00:34:06,290
empirical results of of this approach

561
00:34:06,310 --> 00:34:14,270
what we did is we set up we did your typical k nearest neighbour classifier

562
00:34:14,290 --> 00:34:20,430
so we have a training data set we we generate constraints from the training dataset

563
00:34:20,520 --> 00:34:25,460
we learn the metric and then we do k nearest neighbour over the

564
00:34:25,480 --> 00:34:27,980
the test data

565
00:34:27,990 --> 00:34:31,390
and some of the details are written up on the slide

566
00:34:31,400 --> 00:34:34,120
we compare against the number of different

567
00:34:34,190 --> 00:34:41,200
existing approaches including some baseline metrics like the squared euclidean distance and using the inverse

568
00:34:44,730 --> 00:34:48,130
four year mahalanobis distance

569
00:34:48,140 --> 00:34:53,530
so first we look at some of the standard UCI datasets that everybody looks at

570
00:34:53,530 --> 00:35:00,480
the leftmost side of the leftmost bar is ITML maxent which is information theoretic metric

571
00:35:00,480 --> 00:35:05,270
learning with a not equal to the identity matrix

572
00:35:05,280 --> 00:35:08,290
what we see is that in general that

573
00:35:08,310 --> 00:35:12,310
that algorithm is performing quite comparably to

574
00:35:12,330 --> 00:35:17,600
two other methods and and

575
00:35:17,620 --> 00:35:20,150
it is often is

576
00:35:20,190 --> 00:35:27,020
is within the confidence intervals is always for performing the best

577
00:35:27,030 --> 00:35:30,730
we looked at application of metric learning to

578
00:35:30,750 --> 00:35:32,800
nearest neighbor software support

579
00:35:32,850 --> 00:35:38,370
so there's this system developed at UT called clarify the way it works is every

580
00:35:38,370 --> 00:35:41,430
execution of a program is

581
00:35:41,450 --> 00:35:45,710
it's like one data point and it and monitors things like function calls and so

582
00:35:45,710 --> 00:35:49,880
forth and so if if you run your program and it crashes you might want

583
00:35:49,880 --> 00:35:51,960
to say oh you know what other

584
00:35:52,000 --> 00:35:57,640
programme executions are similar to this letter from say some training sets so i can

585
00:35:57,640 --> 00:35:59,250
figure out now

586
00:35:59,270 --> 00:36:05,460
why my program crashed so we looked at so so we looked at metric learning

587
00:36:05,460 --> 00:36:09,540
in this context and looked at

588
00:36:09,590 --> 00:36:15,750
a few different programs like attack and an MP three player database manager

589
00:36:15,760 --> 00:36:19,220
and this linux kernel applications

590
00:36:19,230 --> 00:36:23,340
and then again we found that that we do quite quite well as compared to

591
00:36:23,340 --> 00:36:27,600
the other methods especially on on the lot tech data set where we seem to

592
00:36:27,600 --> 00:36:31,310
be doing very well

593
00:36:31,340 --> 00:36:34,750
in terms of timing i don't have the results here but we're also

594
00:36:34,770 --> 00:36:35,840
very very

595
00:36:35,850 --> 00:36:41,330
fast so in general i think where the fastest of of the methods that we

596
00:36:41,330 --> 00:36:44,170
looked at

597
00:36:44,180 --> 00:36:47,570
so to to wrap up we

598
00:36:47,590 --> 00:36:49,220
so the

599
00:36:49,230 --> 00:36:55,050
the formulation for this problem is that of minimising the differential relative entropy of two

600
00:36:55,050 --> 00:36:57,070
gaussians corresponding to

601
00:36:57,640 --> 00:37:04,350
mahalanobis distance DNA in my home in the mahalanobis distance d and not

602
00:37:04,410 --> 00:37:08,670
and that's that straightforward of set up

603
00:37:08,720 --> 00:37:11,410
has a lot of nice advantages we can

604
00:37:11,430 --> 00:37:13,370
we have a very simple

605
00:37:13,400 --> 00:37:17,220
and scalable algorithm based on logdet projections

606
00:37:17,230 --> 00:37:23,200
and it includes able to incorporate many different constraints we can catalyze that we can

607
00:37:23,200 --> 00:37:32,470
prove regret bounds and empirically it seems to be quite competitive with with existing techniques

608
00:37:32,480 --> 00:37:34,430
OK thanks

609
00:37:34,470 --> 00:37:43,500
have to complete

610
00:37:43,640 --> 00:37:46,160
it's like

611
00:37:46,190 --> 00:37:49,660
you have a a lot of

612
00:37:49,680 --> 00:37:52,330
at the end of the

613
00:37:52,350 --> 00:37:53,470
the above

614
00:37:53,520 --> 00:37:59,080
ah i don't know if you can have banned said and so how would you

615
00:37:59,160 --> 00:38:03,810
so that know this is the data the gun

616
00:38:03,830 --> 00:38:11,140
the god of love from the log will go the likelihood of the data to

617
00:38:11,150 --> 00:38:13,510
get this

618
00:38:13,510 --> 00:38:18,250
it would be all here this this and this means that there will be

619
00:38:18,680 --> 00:38:21,390
and an edge from

620
00:38:23,140 --> 00:38:26,070
six four to six

621
00:38:26,240 --> 00:38:28,430
this is an events

622
00:38:28,450 --> 00:38:31,670
events description of the temporal

623
00:38:37,910 --> 00:38:41,320
it's also it also can be an event

624
00:38:47,430 --> 00:38:50,500
of course what you can get this also

625
00:38:50,520 --> 00:38:52,030
from the

626
00:38:52,040 --> 00:38:55,720
from the internet just click here and you'll get

627
00:38:55,730 --> 00:38:59,150
this is the the same data set that i

628
00:38:59,150 --> 00:39:01,630
so this is just

629
00:39:01,670 --> 00:39:03,420
just to show how

630
00:39:03,460 --> 00:39:05,090
things can be done then

631
00:39:05,100 --> 00:39:11,910
nicer and the visualisation can be really nice when using it when using piped and

632
00:39:12,000 --> 00:39:17,990
we're going to look at this temporal network from september eleventh well the network was

633
00:39:17,990 --> 00:39:21,020
what network the data were

634
00:39:21,110 --> 00:39:24,510
together by steve corman with collaborators

635
00:39:24,520 --> 00:39:29,520
and they were you they used the writers news

636
00:39:29,530 --> 00:39:33,720
in sixty six days and about september eleventh

637
00:39:34,770 --> 00:39:39,170
they were they get big idea articles and then

638
00:39:39,190 --> 00:39:41,430
the collected works

639
00:39:41,440 --> 00:39:46,690
different words that appear in those articles and

640
00:39:46,710 --> 00:39:48,670
the line between two

641
00:39:48,670 --> 00:39:50,040
but this is too

642
00:39:50,690 --> 00:39:58,230
what would that mean that there is that they appeared in the same text in

643
00:39:58,230 --> 00:39:59,410
the same article

644
00:40:00,480 --> 00:40:05,730
what would this mean that the thickness of these edges would then mean that they

645
00:40:05,730 --> 00:40:10,600
appeared so many times in so many different articles

646
00:40:12,950 --> 00:40:15,840
by i also kenneth export

647
00:40:15,840 --> 00:40:17,510
there are

648
00:40:20,020 --> 00:40:27,900
drawings to SVG that's a scalable vector format and i'm going to show you are

649
00:40:27,910 --> 00:40:30,230
here how this looks like

650
00:40:30,430 --> 00:40:33,270
this is the whole network

651
00:40:33,290 --> 00:40:35,820
and this is just the first day

652
00:40:35,830 --> 00:40:37,820
so in the first day there's

653
00:40:37,840 --> 00:40:38,750
a lot

654
00:40:38,780 --> 00:40:45,960
about a lot written about twenty our world trade centre new york pentagon then also

655
00:40:45,960 --> 00:40:48,380
attacked to the united states

656
00:40:48,390 --> 00:40:50,430
then the sea territories

657
00:40:53,070 --> 00:40:54,910
on the right

658
00:40:54,930 --> 00:40:58,910
well this is a low-resolution that's why i have two

659
00:40:59,280 --> 00:41:02,710
go from left to right we have also the

660
00:41:02,740 --> 00:41:05,330
the weight of the lines

661
00:41:05,540 --> 00:41:09,260
and if we for example want to just see

662
00:41:11,690 --> 00:41:15,730
to see what the lines that appeared at least thirty

663
00:41:15,750 --> 00:41:16,860
thirty one

664
00:41:16,870 --> 00:41:18,500
times so

665
00:41:18,560 --> 00:41:23,700
the the words that were thirty five one times in the same appeared in the

666
00:41:23,700 --> 00:41:27,130
same articles we would click

667
00:41:28,110 --> 00:41:30,850
and what we get is

668
00:41:30,860 --> 00:41:32,340
just the

669
00:41:32,590 --> 00:41:38,230
the vertices and lines with the highest weight

670
00:41:38,510 --> 00:41:41,230
but if we so then we can go forward

671
00:41:41,240 --> 00:41:43,070
four weekdays

672
00:41:43,110 --> 00:41:44,100
the next day

673
00:41:49,290 --> 00:41:53,950
a bit more diverse for example here we have like that

674
00:41:54,090 --> 00:41:55,560
conference used

675
00:41:55,580 --> 00:41:58,960
o and then rescue rescue workers

676
00:42:00,460 --> 00:42:04,250
and of course we can again look at that and try to

677
00:42:04,260 --> 00:42:07,780
i'll get some useful information about that

678
00:42:08,080 --> 00:42:11,500
and the

679
00:42:11,710 --> 00:42:14,370
sorry i

680
00:42:14,380 --> 00:42:15,440
but then

681
00:42:15,500 --> 00:42:17,320
what's interesting here

682
00:42:19,060 --> 00:42:25,670
that if for example could the thirty nine

683
00:42:25,720 --> 00:42:27,410
we get a lot less

684
00:42:27,410 --> 00:42:30,530
written about that but the

685
00:42:30,600 --> 00:42:37,150
the weights change two different subjects so here as opposed to worker the anthrax

686
00:42:38,280 --> 00:42:40,520
skin and so on so

687
00:42:40,570 --> 00:42:42,000
these are two

688
00:42:42,040 --> 00:42:46,360
the main topics that were to be the the the article's talk about

689
00:42:46,400 --> 00:42:47,340
on the

690
00:42:47,360 --> 00:42:50,350
thirty nine thirty nine days

691
00:42:50,360 --> 00:42:52,700
and about afghanistan and so on

692
00:42:52,720 --> 00:42:54,630
so it's

693
00:42:56,590 --> 00:42:57,760
they were

694
00:42:57,820 --> 00:43:01,130
there can be a lot of things said about it so

695
00:43:02,440 --> 00:43:07,920
you can of course get to go to the article about the september eleven

696
00:43:07,930 --> 00:43:10,080
it's also in attendance

697
00:43:14,480 --> 00:43:16,670
is that a slide show

698
00:43:18,970 --> 00:43:25,740
the data from cat some from then county kansas event data set data service that

699
00:43:25,750 --> 00:43:26,450
it is

700
00:43:26,530 --> 00:43:32,840
presented like like this how they are recoded to

701
00:43:32,920 --> 00:43:34,430
five format

702
00:43:34,440 --> 00:43:38,140
and how they well this is like

703
00:43:38,140 --> 00:43:39,270
the first

704
00:43:39,290 --> 00:43:43,150
number here is the date so this would be in the

705
00:43:43,190 --> 00:43:49,670
the year nineteen eighty nine in the fourth month and in the second day then

706
00:43:49,890 --> 00:43:51,600
these two countries

707
00:43:53,060 --> 00:43:54,250
had this

708
00:43:55,160 --> 00:43:57,190
activity or this

709
00:43:57,220 --> 00:44:00,170
relations so the it's right

710
00:44:00,180 --> 00:44:02,490
here and that's how it's

711
00:44:02,500 --> 00:44:05,340
code so we will look at the

712
00:44:05,340 --> 00:44:10,610
the first country in the second country here vertices then we can have all the

713
00:44:10,610 --> 00:44:15,290
are we have all the arts and all the relations here so that's how they

714
00:44:15,290 --> 00:44:21,650
are just sort it will just listed down all the different relations and then at

715
00:44:21,650 --> 00:44:22,560
the end

716
00:44:22,600 --> 00:44:27,840
we have the number of the relation and the numbers of two countries that one

717
00:44:27,840 --> 00:44:31,150
of two two states that shared this relation

718
00:44:31,170 --> 00:44:36,220
what we tend to be and the idea and there's

719
00:44:36,240 --> 00:44:42,020
the time event over time to time

720
00:44:42,150 --> 00:44:46,880
timestamp when this happens so this happen in time step for what we have to

721
00:44:46,880 --> 00:44:49,850
be careful here is that we have also

722
00:44:49,950 --> 00:44:54,560
this third number and this is the weight of the

723
00:44:54,580 --> 00:44:56,610
i think even if

724
00:44:56,620 --> 00:44:58,920
the weight is always one

725
00:44:58,930 --> 00:45:00,210
that's how did

726
00:45:00,230 --> 00:45:03,180
five will now

727
00:45:03,850 --> 00:45:08,510
so this is the end of the first part of the talk that was about

728
00:45:08,510 --> 00:45:13,390
networks what kind of network what kind of data sets were are used in pike

729
00:45:13,470 --> 00:45:19,820
and will start with doing some more analysis some more statistics so the first thing

730
00:45:19,820 --> 00:45:26,110
which will be what kind of input data we have and what kind of statistic

731
00:45:26,140 --> 00:45:28,290
we can use statistics we can do

732
00:45:28,310 --> 00:45:31,940
we have no merits numeric

733
00:45:31,990 --> 00:45:35,010
data that would vectors

734
00:45:35,030 --> 00:45:38,170
and in the values of lines

735
00:45:38,180 --> 00:45:43,530
then we have or not this could be one k one k one

736
00:45:43,540 --> 00:45:48,610
possibility of station and then nominal data of course to the

737
00:45:48,610 --> 00:45:51,990
clustering or the partition and the

738
00:45:51,990 --> 00:45:54,430
good afternoon everybody

739
00:45:54,430 --> 00:45:55,840
my name is

740
00:45:55,870 --> 00:45:59,270
new shoes for microsoft research asia

741
00:45:59,280 --> 00:46:02,610
and i worked in the hardware computing group

742
00:46:02,610 --> 00:46:05,940
and this is a joint work with you i want to see

743
00:46:05,950 --> 00:46:08,900
the department for electronics engineering

744
00:46:08,920 --> 00:46:10,970
and the high tories

745
00:46:10,990 --> 00:46:18,380
pg based mapreduce framework for machine learning so this is something different for the case

746
00:46:18,540 --> 00:46:20,330
should based

747
00:46:20,420 --> 00:46:24,480
for the machine learning approach i will illustrate more about

748
00:46:24,540 --> 00:46:26,690
why we select this approach for

749
00:46:26,690 --> 00:46:29,580
efficient computing machine learning

750
00:46:29,680 --> 00:46:33,500
so quick so it who has no

751
00:46:33,540 --> 00:46:35,180
the PG

752
00:46:35,330 --> 00:46:37,340
over the head

753
00:46:37,370 --> 00:46:40,080
it was pretty much sex

754
00:46:40,090 --> 00:46:44,250
here the offline d

755
00:46:44,260 --> 00:46:46,790
first i will introduce the motivation

756
00:46:46,830 --> 00:46:49,070
and the proposed solution

757
00:46:49,080 --> 00:46:56,130
and then i will introduce the case study we have implemented we implement

758
00:46:56,510 --> 00:46:58,630
it was named rankboost

759
00:46:58,660 --> 00:46:59,790
and i will

760
00:46:59,800 --> 00:47:03,050
present the results and in the summary

761
00:47:03,290 --> 00:47:07,440
this is a very popular figure

762
00:47:07,520 --> 00:47:09,100
and the

763
00:47:10,430 --> 00:47:13,370
general purpose CPU has the

764
00:47:13,600 --> 00:47:17,300
the power the power limit be column it

765
00:47:17,320 --> 00:47:20,570
and people have to go into the parallel

766
00:47:22,010 --> 00:47:30,150
as we know them and who has utilized many many motions in these data centre

767
00:47:30,190 --> 00:47:39,930
this the have the parallel computing power for their web applications but is still

768
00:47:39,940 --> 00:47:42,870
requires a power

769
00:47:42,910 --> 00:47:46,210
and you can see that the

770
00:47:46,270 --> 00:47:50,320
the is they have and the annual

771
00:47:52,040 --> 00:47:59,720
for the the cost for the power consumption and the queen will cover the hardware

772
00:48:00,930 --> 00:48:02,550
for each year

773
00:48:02,680 --> 00:48:06,840
so we can see that you large problems you when we go into the lead

774
00:48:07,800 --> 00:48:10,310
processing with many commodity motions

775
00:48:10,330 --> 00:48:15,150
so here's the challenge for general purpose CPU architecture

776
00:48:16,050 --> 00:48:17,210
the memory wall

777
00:48:17,210 --> 00:48:20,420
because if you that past member benefits to slow

778
00:48:20,460 --> 00:48:25,300
and we also has a very large area in the cheap

779
00:48:25,300 --> 00:48:26,490
for the cash

780
00:48:26,520 --> 00:48:31,400
and only part of the areas for the computation logic

781
00:48:31,400 --> 00:48:36,840
and also many areas for the normal arithmetic operation like

782
00:48:36,870 --> 00:48:40,980
old for order for the prediction speculative

783
00:48:41,080 --> 00:48:42,180
address the

784
00:48:42,230 --> 00:48:43,780
to again try to guess

785
00:48:43,800 --> 00:48:47,310
what's the next state is what the structure is so

786
00:48:47,330 --> 00:48:49,030
that is not efficient

787
00:48:49,230 --> 00:48:53,590
OK this is the architecture is here and also

788
00:48:53,610 --> 00:48:55,140
for the

789
00:48:55,210 --> 00:48:57,680
parallel programming we also

790
00:48:57,740 --> 00:49:03,950
i need to manage the concurrency explicitly that means we have created to create the

791
00:49:04,390 --> 00:49:06,460
threats and created the

792
00:49:06,490 --> 00:49:09,150
synchronisation manually and two

793
00:49:09,170 --> 00:49:12,950
get it work we have a long block two

794
00:49:12,960 --> 00:49:14,650
make it work and when we

795
00:49:14,670 --> 00:49:19,810
how bad hardware we want to increase the scalability of this has to go through

796
00:49:19,810 --> 00:49:24,230
many many motions we may want to retool in the parameters

797
00:49:24,270 --> 00:49:29,830
so we have at architecture and the program intended is

798
00:49:29,840 --> 00:49:32,280
so what's the

799
00:49:32,300 --> 00:49:33,650
the solution to that

800
00:49:33,670 --> 00:49:40,710
this is our proposed solution is called cost of the mosfet computing for motion and

801
00:49:40,730 --> 00:49:43,020
there are some

802
00:49:43,020 --> 00:49:45,370
interesting fact before the fact

803
00:49:45,390 --> 00:49:48,400
here is the primary goal of this project

804
00:49:48,420 --> 00:49:50,300
we want to automatically

805
00:49:50,300 --> 00:49:53,140
but this expression for z

806
00:49:53,180 --> 00:49:57,110
is monotonically increasing in epsilon t

807
00:49:57,110 --> 00:50:00,140
and the range that we care about between zero and a half

808
00:50:00,270 --> 00:50:05,640
so we're minimizing epsilon t is the same as minimizing this expression and so is

809
00:50:05,650 --> 00:50:08,710
the same as minimizing zt

810
00:50:08,720 --> 00:50:11,410
so what we see coming out of that proof

811
00:50:11,430 --> 00:50:13,500
is that both

812
00:50:13,520 --> 00:50:16,200
adaboost which is choosing alpha t

813
00:50:16,230 --> 00:50:22,520
and the weak learner which is choosing ht are working together so as to minimize

814
00:50:23,630 --> 00:50:28,800
this normalisation factor on every round of boosting

815
00:50:28,860 --> 00:50:35,290
so that's equivalent to saying that they're working to greedily minimizes the product of the

816
00:50:36,860 --> 00:50:41,200
so they minimize if ct separately so they are trying to minimise the product of

817
00:50:41,210 --> 00:50:45,270
the it is but they're doing it in a greedy fashion greedy in the sense

818
00:50:45,270 --> 00:50:47,370
that you're just doing doing it on round t

819
00:50:47,790 --> 00:50:52,110
you just minimizing zt without looking at what effect it will have on the later

820
00:50:54,850 --> 00:50:56,340
so apparently

821
00:50:56,360 --> 00:50:58,410
adaboost is

822
00:50:58,470 --> 00:51:03,470
would seem to be a greedy procedure for minimizing this product of busy tees

823
00:51:03,480 --> 00:51:06,500
so what is the product of the is

824
00:51:06,500 --> 00:51:09,940
well again if you go to the back to the training year proof

825
00:51:10,120 --> 00:51:14,710
we show that the product of the ETS is equal to one over times the

826
00:51:15,690 --> 00:51:17,820
of this exponential function

827
00:51:17,830 --> 00:51:19,840
we're half again is this

828
00:51:19,890 --> 00:51:21,350
weighted sum

829
00:51:21,360 --> 00:51:23,330
of the weak classifiers

830
00:51:23,340 --> 00:51:27,800
so this is a loss function this has come to be called the exponential

831
00:51:27,820 --> 00:51:29,070
loss functions

832
00:51:29,120 --> 00:51:33,010
so what we've argued is that adaboost is apparently trying to greedily

833
00:51:33,010 --> 00:51:38,650
minimize the exponential loss function so that would seem to be the loss function associated

834
00:51:38,650 --> 00:51:41,320
with adaboost

835
00:51:41,390 --> 00:51:43,790
so why exponential loss

836
00:51:43,790 --> 00:51:46,200
well intuitively

837
00:51:46,260 --> 00:51:50,070
exponential loss strongly favours f of x

838
00:51:50,080 --> 00:51:51,890
having the same sign

839
00:51:51,910 --> 00:51:57,230
graph of x i having the same signers why because if they have opposite signs

840
00:51:57,270 --> 00:52:02,410
then this exponent will be positive and we take the exponential of that you get

841
00:52:02,410 --> 00:52:04,150
a very large number

842
00:52:04,170 --> 00:52:07,120
so trying to make this loss function small

843
00:52:07,130 --> 00:52:08,880
has the effect of

844
00:52:09,960 --> 00:52:11,300
they bring their

845
00:52:11,310 --> 00:52:13,300
predictions f of x y

846
00:52:13,310 --> 00:52:15,570
having the same sign as one

847
00:52:15,620 --> 00:52:18,610
which is just what we want in adaboost

848
00:52:18,620 --> 00:52:22,460
another reason which is kind of what we used in the training year proof is

849
00:52:22,460 --> 00:52:24,950
that this exponential loss function

850
00:52:25,020 --> 00:52:28,030
is an upper bound on the usual training here

851
00:52:28,040 --> 00:52:32,320
so the story goes well we really like to minimize the training air

852
00:52:32,390 --> 00:52:37,640
but the training area is this nasty function it's not

853
00:52:37,660 --> 00:52:41,620
smooth it's not convex it's not even continuous

854
00:52:41,660 --> 00:52:47,180
so instead of minimizing will minimize this upper bound on the training year which is

855
00:52:47,180 --> 00:52:50,380
smooth and convex and continuous

856
00:52:50,430 --> 00:52:54,230
but it is a very loose upper bound on the training year

857
00:52:54,250 --> 00:53:01,880
so those are two motivations for why this exponent exponential loss function might make sense

858
00:53:02,000 --> 00:53:07,100
so how does adaboost go about minimizing exponential loss so if we try to abstract

859
00:53:07,100 --> 00:53:11,480
away from adaboost and viewed as a numerical method for minimizing the loss function what's

860
00:53:13,120 --> 00:53:16,720
well again let's let g one through g and be the space of all of

861
00:53:16,720 --> 00:53:18,730
our weak classifiers

862
00:53:18,760 --> 00:53:20,850
so we can take half

863
00:53:20,860 --> 00:53:23,860
which is this linear combination of weak classifiers

864
00:53:23,890 --> 00:53:28,870
and rewrite it in terms of the entire space of weak classifiers

865
00:53:28,890 --> 00:53:32,730
so in that case nearly all of these lambda is these parameters will be zero

866
00:53:32,730 --> 00:53:34,500
so be very sparse

867
00:53:34,500 --> 00:53:38,190
combination of this large space of weak classifiers

868
00:53:38,220 --> 00:53:43,620
and then loss function can be rewritten in this way we replaced f of x

869
00:53:43,620 --> 00:53:47,510
with this linear combination over the entire space weak classifiers

870
00:53:47,580 --> 00:53:50,070
and now we just have this function

871
00:53:50,090 --> 00:53:55,340
of these parameters lambda one two lambda and that we're trying to minimize so adaboost

872
00:53:55,340 --> 00:54:02,890
is just trying to minimize find these parameter values to minimize this function here

873
00:54:03,030 --> 00:54:07,720
so how does it do it well adaboost is actually doing it kind of coordinate

874
00:54:07,720 --> 00:54:10,800
descent on this optimisation problem

875
00:54:10,810 --> 00:54:14,930
so if you think about it in these abstract terms what it's doing is that

876
00:54:14,930 --> 00:54:18,080
initially setting all the parameter values to zero

877
00:54:18,210 --> 00:54:22,880
and then on every round it's choosing one corner lambda j just one of them

878
00:54:22,880 --> 00:54:24,160
to update

879
00:54:24,210 --> 00:54:28,890
in its updating it by incrementing by some amount alpha t

880
00:54:28,910 --> 00:54:31,120
choosing coordinate lambda j

881
00:54:31,140 --> 00:54:34,550
corresponds to choosing one of these weak classifiers

882
00:54:34,550 --> 00:54:38,970
which is exactly what is happening on every round we're choosing an HTN every round

883
00:54:39,170 --> 00:54:42,670
which has to be one of these weak classifiers gj

884
00:54:43,700 --> 00:54:49,870
we're just using a single coordinate work updating that one coordinate we're doing it in

885
00:54:49,870 --> 00:54:55,140
such a way so as to cause the biggest decrease in the loss function

886
00:54:55,160 --> 00:54:58,130
so this is called coordinate descent it's an approach that

887
00:54:58,140 --> 00:55:02,110
goes back a long time before adaboost

888
00:55:02,120 --> 00:55:08,200
and so it seems to be very slow slow compared say to gradient descent which

889
00:55:08,200 --> 00:55:11,080
might be a more natural way of minimising

890
00:55:11,390 --> 00:55:14,030
some function

891
00:55:14,120 --> 00:55:18,210
but it does have the advantage that that it's very convenient when you're working over

892
00:55:18,210 --> 00:55:22,960
a huge space of weak classifiers are huge space of functions

893
00:55:22,980 --> 00:55:27,840
because you just updating one of these on each round c and up with something

894
00:55:27,840 --> 00:55:32,330
which is sparse and you often can use some kind of work the boosting case

895
00:55:32,330 --> 00:55:38,050
the weak learning algorithm to look for the one to date

896
00:55:38,090 --> 00:55:39,530
OK here's another

897
00:55:39,540 --> 00:55:41,860
the of boosting which is as

898
00:55:44,110 --> 00:55:47,440
for doing something called functional gradient descent

899
00:55:47,720 --> 00:55:51,860
so instead of viewing the loss as a function of those parameters we can view

900
00:55:51,860 --> 00:55:55,360
the loss as a function of the function f

901
00:55:56,350 --> 00:56:00,910
so here again is our loss function which is now a function of the function

902
00:56:02,430 --> 00:56:05,700
so it's a function of functions sort of functional

903
00:56:05,720 --> 00:56:09,110
so the loss is now functional

904
00:56:09,120 --> 00:56:13,580
and we're looking for the function as which minimizes this

905
00:56:13,580 --> 00:56:17,620
so if we had current estimate for half

906
00:56:17,640 --> 00:56:20,860
one natural thing you might want to do is to use gradient descent which means

907
00:56:20,860 --> 00:56:22,220
to take a stab

908
00:56:22,230 --> 00:56:25,050
in the direction of steepest descent

909
00:56:25,050 --> 00:56:28,610
two basically is now from a logical point of view i suppose you might have

910
00:56:28,610 --> 00:56:33,500
a third possible view know we've got them monasteries has only and those bodies but

911
00:56:33,500 --> 00:56:38,790
there's no souls you could imagine somebody who says there are stalls but there are

912
00:56:38,790 --> 00:56:41,210
no bodies

913
00:56:41,270 --> 00:56:46,360
this would roughly be view according to which their minds but there are really physical

914
00:56:46,360 --> 00:56:50,910
objects physical objects are kind of illusion perhaps we fall into

915
00:56:50,910 --> 00:56:56,130
or thinking about the materialistic terms might be greatly confused or mistaken

916
00:56:56,140 --> 00:57:00,300
this view is sometimes known in philosophy as idealism

917
00:57:00,420 --> 00:57:03,970
all that exists some mines in their ideas

918
00:57:04,000 --> 00:57:08,390
physical objects is just the way of talking about the ideas the mind has or

919
00:57:08,390 --> 00:57:10,490
something like that

920
00:57:10,510 --> 00:57:15,150
idealism is the position that's got a very long history and philosophy and you know

921
00:57:15,150 --> 00:57:18,510
for many classes would be worth taking you know a fair bit of time to

922
00:57:18,510 --> 00:57:22,580
consider more carefully but for our purposes i think it's not a contender so i'm

923
00:57:22,580 --> 00:57:25,470
just going to put aside

924
00:57:25,470 --> 00:57:29,610
the positions that i'm going to enter other possibilities as well

925
00:57:29,620 --> 00:57:34,060
you know there there are views where the mind and body are just two different

926
00:57:34,060 --> 00:57:38,340
ways of looking at the same underlying reality

927
00:57:38,360 --> 00:57:42,910
with the underlying reality is neither physical nor mental

928
00:57:42,930 --> 00:57:47,730
is also worth taking seriously in metaphysics class but for our purposes

929
00:57:47,750 --> 00:57:52,650
i mentioned it put aside the two views we're going to focus on are on

930
00:57:52,650 --> 00:57:55,410
the one hand the dualistic view

931
00:57:55,410 --> 00:57:57,150
people have

932
00:57:57,160 --> 00:57:59,480
souls as well as bodies

933
00:57:59,530 --> 00:58:01,070
and the physical is view

934
00:58:01,090 --> 00:58:07,460
all we have all we are are bodies

935
00:58:07,460 --> 00:58:13,650
well let me say something more than about that are dualistic position

936
00:58:13,670 --> 00:58:16,030
according to journalist

937
00:58:16,070 --> 00:58:20,340
that the mind is is immaterial substance and we could we could call it by

938
00:58:20,340 --> 00:58:24,270
different names we could no harm would be done if we call it to mind

939
00:58:24,340 --> 00:58:29,200
though the reason i will typically talk about us all is to try to flag

940
00:58:29,200 --> 00:58:34,830
the crucial point do all this view the mind is based in or even just

941
00:58:34,830 --> 00:58:38,810
is something non-physical something on material

942
00:58:38,820 --> 00:58:43,490
we could call mind called so called psyche is a are typically try to call

943
00:58:43,490 --> 00:58:49,660
it the soul the soul can direct and give orders to

944
00:58:49,670 --> 00:58:50,560
the body

945
00:58:50,580 --> 00:58:52,000
on the one hand

946
00:58:52,020 --> 00:58:54,420
on the other hand the body

947
00:58:54,430 --> 00:58:56,330
sort of

948
00:58:57,990 --> 00:59:00,240
input that eventually gets

949
00:59:00,250 --> 00:59:03,330
since or felt by the

950
00:59:03,340 --> 00:59:08,840
you get you take up in and you stick it through my flesh of my

951
00:59:10,280 --> 00:59:12,990
i feel pain

952
00:59:13,000 --> 00:59:13,790
in my

953
00:59:14,590 --> 00:59:17,030
in my mind

954
00:59:17,740 --> 00:59:24,170
two way interaction and is always in with philosophy there's there's more complicated versions of

955
00:59:25,480 --> 00:59:29,240
we are maybe the interaction of we're both ways but let's just

956
00:59:29,290 --> 00:59:32,720
limit ourselves to good old fashion

957
00:59:32,750 --> 00:59:36,910
two way interaction this dualism

958
00:59:37,020 --> 00:59:37,780
so my

959
00:59:37,790 --> 00:59:40,610
my mind control of my body

960
00:59:40,610 --> 00:59:45,930
my body can affect my mind in various ways but for that there are separate

961
00:59:47,690 --> 00:59:53,970
still there is this very tight connection

962
00:59:56,160 --> 01:00:01,430
we sometimes put it the cells in the body though of course talking about spatial

963
01:00:01,430 --> 01:00:05,410
location here maybe maybe some metaphorically

964
01:00:05,460 --> 01:00:10,920
intended is that as we think that if start opening up the body finally find

965
01:00:10,920 --> 01:00:16,310
the particular spot OK here's the place where the souls located

966
01:00:16,340 --> 01:00:21,130
well it does seem from this dual is perspective is all souls are located i'm

967
01:00:21,140 --> 01:00:23,470
sort of viewing the world from here

968
01:00:23,480 --> 01:00:27,250
just like each of you is viewing in the world in particular locations so maybe

969
01:00:27,250 --> 01:00:32,380
your soul is located more or less in the vicinity of your body

970
01:00:32,430 --> 01:00:36,990
the crucial point of course the the attraction of the dualistic view

971
01:00:37,050 --> 01:00:40,000
so far point of view is that

972
01:00:40,000 --> 01:00:45,340
autoregressive models they was how random elements to them so the prediction is itself a

973
01:00:45,340 --> 01:00:50,590
random variable so we simply sampling fairly from the distribution of the random variable to

974
01:00:51,240 --> 01:00:56,380
st anne's and finally the like weights by TNA

975
01:00:56,390 --> 01:00:58,080
are simply

976
01:00:58,090 --> 01:01:03,920
as before likelihoods evaluated at the positions of the new particles and for the new

977
01:01:05,600 --> 01:01:08,290
here's a picture of the same thing

978
01:01:08,370 --> 01:01:15,290
articles depicted along a horizontal line which is the one-dimensional depiction of the state space

979
01:01:15,300 --> 01:01:20,100
and the positions of the particles along the line of values as in the size

980
01:01:20,100 --> 01:01:22,420
of the particle depicts its weight

981
01:01:22,430 --> 01:01:27,150
and so you sample from each particle

982
01:01:27,160 --> 01:01:29,260
with probability that is proportional to the

983
01:01:29,280 --> 01:01:32,990
the weight on the particles so you see some large particles will get chosen more

984
01:01:32,990 --> 01:01:37,420
than once in some small particles may not get chosen at all and if we

985
01:01:37,420 --> 01:01:42,270
split up the prediction into its deterministic path and you know as in the autoregressive

986
01:01:42,270 --> 01:01:46,260
process and is random part you see what happens is that initially when i choose

987
01:01:46,260 --> 01:01:49,550
this big particles several times all of the new

988
01:01:51,370 --> 01:01:52,540
drawn from that

989
01:01:52,550 --> 01:01:56,650
i have exactly the same value because so far have used only the deterministic part

990
01:01:56,650 --> 01:02:01,990
of the process now i we'll in the random part of the process and these

991
01:02:02,270 --> 01:02:06,980
duplicate particles freedom sitting on top of of one another here split according to

992
01:02:07,410 --> 01:02:12,670
draw independent draws of the random variables and i've got a nice have non degenerate

993
01:02:12,820 --> 01:02:18,270
distribution of particles along the line and this this particle set equally weighted forms the

994
01:02:18,310 --> 01:02:26,120
prediction distribution finally particle gets passed under their likelihood curve this is the curve says

995
01:02:26,120 --> 01:02:27,850
to what degree

996
01:02:27,920 --> 01:02:32,250
a particular value in state space agrees with the observations

997
01:02:32,260 --> 01:02:36,730
and so if the particle happens to sit on the peak of the likelihood function

998
01:02:36,740 --> 01:02:39,600
then it will get given the largest weight

999
01:02:39,650 --> 01:02:43,960
a light weight and values and sit on the floor often like a function will

1000
01:02:43,960 --> 01:02:46,150
get given small weight and so on

1001
01:02:46,170 --> 01:02:49,580
i've got an animation this maybe it's going too far

1002
01:02:49,630 --> 01:02:53,050
well the animation

1003
01:02:53,160 --> 01:02:56,330
bit of animation

1004
01:02:56,350 --> 01:02:58,160
they are particles

1005
01:02:59,490 --> 01:03:04,210
drawn and this is the first time step of the particle filter

1006
01:03:04,230 --> 01:03:07,230
so some samples are drawn according to their weights

1007
01:03:07,240 --> 01:03:10,900
at the previous timestep and they get

1008
01:03:10,910 --> 01:03:15,480
reweighted according to this thread likelihood curve and a got particles of different sizes some

1009
01:03:15,480 --> 01:03:19,040
of them almost disappeared because they had such low likelihoods

1010
01:03:20,420 --> 01:03:23,650
that's just the first time step and the particle filter and now we take

1011
01:03:23,670 --> 01:03:27,520
that that particles that is being the prior for the next time step and so

1012
01:03:27,520 --> 01:03:28,530
on the

1013
01:03:28,580 --> 01:03:33,270
possible selection process is repeated

1014
01:03:37,950 --> 01:03:41,260
i guess this slide is just to emphasise

1015
01:03:41,310 --> 01:03:43,150
how much we need this

1016
01:03:43,170 --> 01:03:44,510
mechanism actually

1017
01:03:45,120 --> 01:03:48,270
when particle filtering was invented in nineteen three

1018
01:03:48,290 --> 01:03:50,430
people used it for

1019
01:03:50,480 --> 01:03:56,620
linear filtering problems but you know what i would call not particularly interesting nonlinearities where

1020
01:03:56,670 --> 01:04:01,660
for example bearings only tracking is one example where you have particles in the plane

1021
01:04:01,660 --> 01:04:07,110
but you're only able to measure the angle so the the arctan of y one

1022
01:04:07,220 --> 01:04:12,040
x is you're nonlinearity in the observation function which sort of mild on long nonlinearity

1023
01:04:12,440 --> 01:04:13,430
what my

1024
01:04:13,450 --> 01:04:17,680
colley is art and i realized was that they were much more

1025
01:04:17,690 --> 01:04:24,070
pressing nonlinearities that arise in problems of tracking in cluster and so he that picture

1026
01:04:24,070 --> 01:04:25,950
again that we had before the normal

1027
01:04:25,960 --> 01:04:27,550
with the responses on

1028
01:04:27,570 --> 01:04:28,730
the normal

1029
01:04:28,740 --> 01:04:31,540
and features found along each normal

1030
01:04:31,550 --> 01:04:35,450
and you expect multiple features and so for each normal

1031
01:04:35,470 --> 01:04:39,750
you're going to get the likelihood function may be something like this where there is

1032
01:04:39,750 --> 01:04:44,180
a peak in the function corresponding to each response of your low level feature detector

1033
01:04:44,440 --> 01:04:46,930
but the and

1034
01:04:46,970 --> 01:04:50,810
i won't go through this argument here but using the probability of association which is

1035
01:04:50,810 --> 01:04:56,850
the standard way of modelling these things between features and events we can build up

1036
01:04:56,850 --> 01:05:03,130
a mixture distribution the likelihood is naturally expressed as a mixture distribution with a again

1037
01:05:03,160 --> 01:05:08,190
let's gasoline component sitting on each of these observations but the point is that this

1038
01:05:08,200 --> 01:05:14,980
very this multimodal and therefore the nongaussian distribution is what you get sitting on one

1039
01:05:15,000 --> 01:05:21,050
normal curve now if you refer those observation distributions into the state space using our

1040
01:05:21,060 --> 01:05:27,550
beloved h matrix remember the tube of the the two on the on the trumpet

1041
01:05:27,600 --> 01:05:32,180
we refer all of those observation distributions state space now

1042
01:05:32,200 --> 01:05:36,680
you've got the combination of the modes from each of those individual lines or if

1043
01:05:37,010 --> 01:05:42,510
state space ECM massive the multimodality problem is once you aggregate all of these are

1044
01:05:42,520 --> 01:05:46,510
the observations associated with one curve and c in state space so this is a

1045
01:05:46,510 --> 01:05:49,400
much more extreme form of

1046
01:05:50,250 --> 01:05:54,490
nonlinearity that arises i say with class and to me

1047
01:05:54,500 --> 01:05:55,860
it seems that the real

1048
01:05:57,390 --> 01:05:59,870
who of particle filters has been in there

1049
01:05:59,890 --> 01:06:03,320
the ability to do filtering class i think this is whether really important

1050
01:06:04,310 --> 01:06:07,780
and a depiction of the remember the picture of the kalman filter we had an

1051
01:06:08,720 --> 01:06:10,110
electoral two back

1052
01:06:10,930 --> 01:06:14,280
the idea is that you take guassian distribution

1053
01:06:14,330 --> 01:06:20,000
and in the prediction step it drifts and gets diffused and then the observations that

1054
01:06:20,010 --> 01:06:26,810
peaks in the distribution are emphasized again correspond to observations and what this particles that

1055
01:06:26,810 --> 01:06:30,050
is modelling in this admittedly somewhat degenerate way

1056
01:06:30,250 --> 01:06:33,870
rather the mixture of delta functions is just the same kind of process but now

1057
01:06:33,870 --> 01:06:35,750
we nongaussian distribution

1058
01:06:38,910 --> 01:06:41,580
of course it may be that the complexity

1059
01:06:41,590 --> 01:06:46,130
you can have the nonlinearity either the likelihood model or in the predictive model boats

1060
01:06:46,130 --> 01:06:56,030
OK so

1061
01:06:58,350 --> 01:07:01,710
i started the talk a few announcements to make

1062
01:07:05,300 --> 01:07:19,490
so remember there will be talks and after knowing he is studying the store clerk

1063
01:07:19,490 --> 01:07:23,240
there there's some very nice stocks so

1064
01:07:23,790 --> 01:07:26,820
that would be a good place to go to

1065
01:07:26,830 --> 01:07:35,030
there is also sold

1066
01:07:37,390 --> 01:07:40,980
i don't remember there it's under

1067
01:07:41,010 --> 01:07:43,280
you can go to the website

1068
01:07:47,180 --> 01:07:48,330
so i

1069
01:07:48,390 --> 01:07:52,090
way should come and find out

1070
01:07:59,860 --> 01:08:07,490
so maybe it would be a good idea to print out the schedule for the

1071
01:08:07,490 --> 01:08:09,270
afternoon talks

1072
01:08:09,320 --> 01:08:16,190
so that's one so that talks in the afternoon then they will be

1073
01:08:22,100 --> 01:08:24,640
three fifteen

1074
01:08:24,650 --> 01:08:28,510
and i'm this is log

1075
01:08:28,530 --> 01:08:31,030
and password

1076
01:08:31,110 --> 01:08:34,180
if t i

1077
01:08:34,230 --> 01:08:37,580
c underscore zero methods

1078
01:08:37,600 --> 01:08:41,410
and what

1079
01:08:41,420 --> 01:08:45,260
what i'm going to talk in the second part of introduction i will talk about

1080
01:08:46,540 --> 01:08:49,940
so i will talk about kernel based algorithms

1081
01:08:49,960 --> 01:08:56,780
and so the kernel based algorithms it's it's a large family of algorithms and it

1082
01:08:56,780 --> 01:09:03,380
has been quite popular in the last perhaps fifteen years or so and that's probably

1083
01:09:03,380 --> 01:09:08,920
was could is considered to be one of the biggest developments in machine learning in

1084
01:09:08,920 --> 01:09:11,820
particular associated with support vector machine

1085
01:09:11,890 --> 01:09:13,950
but there are

1086
01:09:14,030 --> 01:09:19,350
other algorithms like regularized this question which actually built up quite a bit and the

1087
01:09:19,350 --> 01:09:24,030
kernel principal component analysis and there are many other algorithms

1088
01:09:24,050 --> 01:09:27,940
and basically the problems this algorithms address

1089
01:09:27,960 --> 01:09:31,260
the usual problems were interested in machine learning

1090
01:09:31,280 --> 01:09:33,680
which is classification regression

1091
01:09:33,690 --> 01:09:36,790
density estimation and there is another thing

1092
01:09:39,090 --> 01:09:42,800
outlier detection for example and

1093
01:09:42,840 --> 01:09:45,490
other things also

1094
01:09:45,500 --> 01:09:47,520
OK so

1095
01:09:47,530 --> 01:09:50,670
this algorithms in this talk i will

1096
01:09:50,690 --> 01:09:55,310
more mausoleum talk about regularized least squares and support

1097
01:09:55,400 --> 01:09:57,980
vector machines

1098
01:09:57,990 --> 01:10:03,260
OK so

1099
01:10:03,300 --> 01:10:06,990
so what is the problem that remind you what the problem is

1100
01:10:07,010 --> 01:10:08,300
of course you

1101
01:10:08,310 --> 01:10:10,360
the standard in pakistan but

1102
01:10:10,410 --> 01:10:12,470
i think it's good to say that again

1103
01:10:12,480 --> 01:10:16,150
so regression and classification

1104
01:10:16,190 --> 01:10:21,980
and here is the problem of regression the fundamental problem of regression is to estimate

1105
01:10:21,980 --> 01:10:27,440
a function from some space let's just say are and for simplicity into our

1106
01:10:27,450 --> 01:10:29,990
so you have a bunch of

1107
01:10:30,030 --> 01:10:33,770
they for example switch which x y

1108
01:10:34,460 --> 01:10:38,520
axes and around one thousand and you want to estimate

1109
01:10:38,540 --> 01:10:40,750
a function from this example so

1110
01:10:40,770 --> 01:10:43,720
is to make the function that given in your example you will be able to

1111
01:10:43,720 --> 01:10:46,340
tell what the well and there are many

1112
01:10:49,070 --> 01:10:50,840
cases in which this is very

1113
01:10:53,040 --> 01:10:56,070
for example you know there is

1114
01:10:56,120 --> 01:10:59,640
maybe you have a house and you want to estimate the price of house depending

1115
01:10:59,640 --> 01:11:04,720
on various parameters of the house on the market and so on so that one

1116
01:11:04,720 --> 01:11:09,180
example maybe you have a patient and you want to find

1117
01:11:09,350 --> 01:11:16,320
the probability that they will die depending on various parameters of their health

1118
01:11:16,370 --> 01:11:20,380
and so if the insurance company very interested in this question

1119
01:11:20,500 --> 01:11:25,220
estimate lunch and you know if you are

1120
01:11:25,240 --> 01:11:28,600
trying to if a few have rocket and you're

1121
01:11:28,620 --> 01:11:32,040
going into space maybe you would like to know what your rocket is going one

1122
01:11:32,040 --> 01:11:35,540
to five and in what is going to do and say

1123
01:11:35,550 --> 01:11:38,460
two months from now and it's going to end up

1124
01:11:38,510 --> 01:11:41,480
so that's estimating

1125
01:11:41,500 --> 01:11:45,310
some sort of four actually position of course is not one real numbers three real

1126
01:11:45,310 --> 01:11:48,030
numbers but

1127
01:11:48,040 --> 01:11:51,290
OK so the classification is

1128
01:11:51,340 --> 01:11:55,970
very similar but you have minus one in one labels and typically you want to

1129
01:11:56,490 --> 01:11:58,360
say separate

1130
01:11:58,380 --> 01:12:01,050
images of one from images of

1131
01:12:01,290 --> 01:12:04,160
images of digits know five from

1132
01:12:04,170 --> 01:12:08,110
digit nine or perhaps you want to separate

1133
01:12:08,120 --> 01:12:11,080
grammatical sentences from ungrammatical

1134
01:12:11,090 --> 01:12:13,050
so this a classification

1135
01:12:13,060 --> 01:12:15,290
and basically this

1136
01:12:15,310 --> 01:12:19,750
in something quite similar and what's interesting is that actually

1137
01:12:19,880 --> 01:12:25,230
but really does classification people what people the people typically estimator real valued function

1138
01:12:25,240 --> 01:12:28,410
and then the threshold of real valued functions

1139
01:12:28,430 --> 01:12:29,540
like this

1140
01:12:29,550 --> 01:12:34,010
so if basically say if it's greater than zero then its class one

1141
01:12:34,030 --> 01:12:38,940
so this is the two class classification just to classes in reality you might want

1142
01:12:38,990 --> 01:12:43,160
to have more than you have to stay in character recognition you want to

1143
01:12:43,160 --> 01:12:44,030
we can put a name to

1144
01:12:44,990 --> 01:12:49,310
so let's think about it all is happening on facebook is once you go past

1145
01:12:49,310 --> 01:12:50,940
a hundred and fifty will you actually doing

1146
01:12:51,630 --> 01:12:56,670
is adding these outer layers and which is just a part of normal everyday life anyway there's nothing special

1147
01:12:59,640 --> 01:13:01,710
what is it surprises in these data

1148
01:13:02,640 --> 01:13:03,480
is that

1149
01:13:03,980 --> 01:13:09,410
networks spent roughly fifty fifty within a hundred and fifty into family and friends we

1150
01:13:09,410 --> 01:13:14,620
had assumed the kingship family with something only happened in traditional small-scale societies and

1151
01:13:15,140 --> 01:13:15,660
what these

1152
01:13:17,360 --> 01:13:21,170
what we're all our data come from either britain and belgium are two big data

1153
01:13:21,940 --> 01:13:23,360
both pretty much identical

1154
01:13:24,340 --> 01:13:28,760
we still find that about half the people including your network actually family

1155
01:13:29,770 --> 01:13:31,440
and only about half a friends

1156
01:13:32,170 --> 01:13:35,160
places are on the left it just gives you a sense see

1157
01:13:35,570 --> 01:13:36,620
interaction rate

1158
01:13:37,830 --> 01:13:43,130
he intensity as social investment in the different layers you can see huge proportion your

1159
01:13:43,130 --> 01:13:48,380
social time is devoted to the unit circle a five andafter the at a time

1160
01:13:49,880 --> 01:13:52,110
two people in the different layers is is just

1161
01:13:52,780 --> 01:13:55,100
and dropping away very very fast anyway

1162
01:13:56,000 --> 01:13:57,010
friendship and kinship

1163
01:13:57,420 --> 01:14:00,120
turned out to be completely have completely different dynamics

1164
01:14:00,850 --> 01:14:05,160
so these data from an eighteen month study we did the following through

1165
01:14:05,680 --> 01:14:07,310
the changes in people's networks

1166
01:14:07,810 --> 01:14:08,950
after they moved away

1167
01:14:10,690 --> 01:14:11,530
from the original

1168
01:14:13,030 --> 01:14:13,830
places that were

1169
01:14:15,570 --> 01:14:20,910
and what you can see is and with kind of interested in the pressures that physical distance with put on

1170
01:14:21,590 --> 01:14:23,270
these equality as a relationship

1171
01:14:24,810 --> 01:14:26,210
what you can see is fact

1172
01:14:27,110 --> 01:14:29,650
the average emotional closeness which is the wire axis

1173
01:14:31,760 --> 01:14:36,640
is very stable over eighteen months with respect to q and if anything goes up a bit

1174
01:14:37,110 --> 01:14:39,250
almost as the absence really doesn't make the heart

1175
01:14:41,590 --> 01:14:44,990
and that this is not just you know your brothers and sisters and your mum

1176
01:14:44,990 --> 01:14:49,560
and dad says you your entire extended kinship this is the average for the entire

1177
01:14:49,620 --> 01:14:52,410
extended kinship groups but look at the contrast the friends

1178
01:14:53,270 --> 01:14:58,140
so this is this is the emotional recorded emotional closeness that's these three different points

1179
01:14:58,140 --> 01:15:02,380
in time for the original set of friends that they started out with at the

1180
01:15:02,380 --> 01:15:06,540
beginning of the study about sarah it just plummets and particularly permits away

1181
01:15:07,150 --> 01:15:08,820
in the first nine months

1182
01:15:12,490 --> 01:15:16,680
one of the things we are interested in this figure studies how digital technology allows

1183
01:15:16,680 --> 01:15:19,760
you to slow down the rate of decay of friendship

1184
01:15:20,630 --> 01:15:21,770
so this is what happens

1185
01:15:23,000 --> 01:15:25,600
and what turned out there was a surprising

1186
01:15:26,220 --> 01:15:27,640
sex difference in how

1187
01:15:28,790 --> 01:15:31,690
stop relationships and this area is just friendship so

1188
01:15:33,680 --> 01:15:35,420
on the left-hand side we need each

1189
01:15:36,070 --> 01:15:37,000
the two graphs

1190
01:15:38,220 --> 01:15:39,390
on the left hand side

1191
01:15:39,850 --> 01:15:45,970
we have the change in emotional closeness to a friend as a result of reduced

1192
01:15:46,840 --> 01:15:48,360
interaction with them in some way

1193
01:15:48,770 --> 01:15:50,290
over i think this is over

1194
01:15:50,900 --> 01:15:55,390
nine months the first nine months in the middle here interacting with them we enter

1195
01:15:55,390 --> 01:15:57,040
the nine months about the same is true

1196
01:15:57,660 --> 01:16:01,420
beginning and on the right-hand side view interacting with the more

1197
01:16:03,170 --> 01:16:06,110
and on of these two graphs the one on the right here

1198
01:16:06,920 --> 01:16:07,690
is essentially

1199
01:16:08,350 --> 01:16:11,530
frequency and talking to them either by phone or in person

1200
01:16:12,250 --> 01:16:14,340
in some way or even perhaps by email

1201
01:16:15,210 --> 01:16:18,900
on the left hand side is doing stuff together a catalog things you might do

1202
01:16:19,590 --> 01:16:25,210
hand how often have you done been shopping parties holiday to help them move house engage

1203
01:16:25,880 --> 01:16:27,040
by stuff would have been

1204
01:16:30,310 --> 01:16:31,840
basically the story is

1205
01:16:32,410 --> 01:16:33,820
if you don't do stuff

1206
01:16:35,380 --> 01:16:36,610
with somebody with a friend

1207
01:16:37,270 --> 01:16:39,860
the relationship deteriorates over time very quickly

1208
01:16:40,680 --> 01:16:44,360
the way to stop relationship deteriorating particular when the physical space

1209
01:16:45,240 --> 01:16:47,940
it built and put into in between

1210
01:16:48,410 --> 01:16:48,910
the menu

1211
01:16:50,110 --> 01:16:54,430
to keep doing more as well but there's a massive sex difference

1212
01:16:59,250 --> 01:17:00,040
what stops

1213
01:17:00,870 --> 01:17:01,570
the goals

1214
01:17:02,010 --> 01:17:04,500
friendships deteriorating is essentially

1215
01:17:05,330 --> 01:17:06,090
talking together

1216
01:17:08,700 --> 01:17:12,920
what stops the boys friendships deteriorating is banging their heads together

1217
01:17:13,980 --> 01:17:14,470
doing stuff

1218
01:17:15,520 --> 01:17:18,150
and in fact doing stuff has a negative effect on girls they

1219
01:17:18,150 --> 01:17:20,300
the next thing about rape laws

1220
01:17:20,310 --> 01:17:22,110
is that the overall

1221
01:17:22,140 --> 01:17:25,510
order of the reaction is simply the sum

1222
01:17:25,550 --> 01:17:31,520
of the exponent so the sum of all the individual orders

1223
01:17:31,640 --> 01:17:34,460
so if you're rate law

1224
01:17:34,480 --> 01:17:37,150
it was you rate constant

1225
01:17:37,180 --> 01:17:40,840
and it's a two to the two

1226
01:17:40,890 --> 01:17:45,330
so that's what water in a

1227
01:17:45,350 --> 01:17:46,520
second order

1228
01:17:46,520 --> 01:17:48,780
and what order and b

1229
01:17:48,790 --> 01:17:50,210
first order

1230
01:17:50,240 --> 01:17:54,420
and your overall order is going to be

1231
01:17:56,340 --> 01:17:58,750
so the overall to word order

1232
01:17:58,780 --> 01:18:02,550
second order in a first order and b

1233
01:18:02,560 --> 01:18:03,730
and overall

1234
01:18:05,350 --> 01:18:09,380
so it always breaks my heart on the test when people get this part right

1235
01:18:09,380 --> 01:18:12,620
and they say what the overall order the reaction

1236
01:18:12,640 --> 01:18:16,320
and they had one in the two to get great so

1237
01:18:16,330 --> 01:18:21,750
remember that part that that's like give me point on an exam

1238
01:18:21,860 --> 01:18:26,370
to be able to add these very small numbers and come up with the song

1239
01:18:26,630 --> 01:18:28,710
so remember how to do that

1240
01:18:28,760 --> 01:18:36,250
so overall it's just some of the individual orders

1241
01:18:36,270 --> 01:18:38,370
OK so here's the bad news

1242
01:18:38,400 --> 01:18:39,650
and that is

1243
01:18:39,660 --> 01:18:43,600
of the unit for the rate constant k

1244
01:18:45,100 --> 01:18:47,480
so the units are going to have to do

1245
01:18:48,620 --> 01:18:53,560
you know what units you have concentration and rates and what terms are in there

1246
01:18:53,600 --> 01:18:57,500
so there's no one unit that you can memorize and we know that that's what

1247
01:18:57,500 --> 01:18:58,370
it's going to be

1248
01:18:58,380 --> 01:19:03,020
so you have to actually think about what the unit for the rate constant or

1249
01:19:03,020 --> 01:19:09,240
so that's a little bit trickier this one much easier just that so but the

1250
01:19:09,240 --> 01:19:13,230
units are little more complicated here

1251
01:19:13,240 --> 01:19:17,160
all right so that you need to know about great loss so it's it's not

1252
01:19:17,160 --> 01:19:20,660
not too much

1253
01:19:20,690 --> 01:19:23,370
all right so here are some of the problem

1254
01:19:23,390 --> 01:19:25,830
with these kinds of experiments

1255
01:19:25,830 --> 01:19:29,230
some of the data they might get the you're trying to analyse may not be

1256
01:19:29,230 --> 01:19:33,770
too perfect usually if it's in the textbook it's pretty perfect data but in general

1257
01:19:33,770 --> 01:19:36,000
these things can be tricky to do

1258
01:19:36,020 --> 01:19:39,760
and they can be tricky to get the good data to figure out what the

1259
01:19:39,770 --> 01:19:42,260
order of the reactions are

1260
01:19:42,270 --> 01:19:44,750
because you're often measuring

1261
01:19:44,770 --> 01:19:51,020
really small changes in concentration of material over brief intervals of time so that can

1262
01:19:51,020 --> 01:19:53,210
be technically challenging to do

1263
01:19:53,270 --> 01:20:00,900
and so the approach that uses that is to use these integrated rate laws

1264
01:20:00,910 --> 01:20:07,300
and so here you can express concentrations directly as a function of time

1265
01:20:07,330 --> 01:20:13,380
and this makes it easier to get at the information that you want

1266
01:20:13,400 --> 01:20:18,780
so let's look at a first order reactions you know what first order means now

1267
01:20:18,790 --> 01:20:23,170
so we just have one thing going to another thing in this particular example here

1268
01:20:23,170 --> 01:20:25,320
first order for a

1269
01:20:25,340 --> 01:20:29,860
i rate of the reaction could be written the rate expression can be written in

1270
01:20:29,860 --> 01:20:31,690
terms of the disappearance

1271
01:20:31,840 --> 01:20:34,550
of a over time

1272
01:20:34,550 --> 01:20:38,340
and you know its first order told us first order

1273
01:20:38,360 --> 01:20:40,280
so you can also right

1274
01:20:40,280 --> 01:20:41,990
the rate law

1275
01:20:42,010 --> 01:20:46,000
so the rate law for first order reaction in a would just be the rate

1276
01:20:46,000 --> 01:20:50,820
constant times the concentration of that so and this one here

1277
01:20:51,900 --> 01:20:53,450
so now

1278
01:20:53,460 --> 01:20:57,700
we can derive the first order integrated rate law

1279
01:20:57,710 --> 01:21:04,370
from this simple expression right here

1280
01:21:04,380 --> 01:21:06,530
so what we're going to do

1281
01:21:06,540 --> 01:21:08,280
is we're going to take

1282
01:21:08,290 --> 01:21:10,320
the concentration terms

1283
01:21:10,340 --> 01:21:12,530
and put them on one side

1284
01:21:12,550 --> 01:21:13,880
of the expression

1285
01:21:13,900 --> 01:21:20,710
and we're going to take time terms and put them on the other side

1286
01:21:20,730 --> 01:21:23,380
so we're going to bring

1287
01:21:23,450 --> 01:21:28,320
the concentration of a here over to the side of the expressions there it is

1288
01:21:28,320 --> 01:21:32,600
one of the concentration of a we have dt

1289
01:21:32,610 --> 01:21:34,420
as well

1290
01:21:34,430 --> 01:21:36,740
and as we move that

1291
01:21:36,780 --> 01:21:38,760
and we have k

1292
01:21:38,880 --> 01:21:43,980
and the negative sign over here and we've moved the change in time

1293
01:21:44,050 --> 01:21:48,290
over to the other side so we have all the concentration terms on one side

1294
01:21:48,290 --> 01:21:53,650
we have a rate terms in time turns on the other side

1295
01:21:53,730 --> 01:21:56,370
and now we can integrate

1296
01:21:56,390 --> 01:21:58,130
and we want to consider

1297
01:21:58,840 --> 01:22:05,780
change from the initial concentration of a to o for all original or to the

1298
01:22:05,780 --> 01:22:11,980
zero so the original amount that you have of a to the concentration of a

1299
01:22:11,980 --> 01:22:13,600
you have at time t

1300
01:22:13,700 --> 01:22:20,510
so we have our our concentration expression in terms of time we're looking from zero

1301
01:22:20,510 --> 01:22:23,940
time to time t

1302
01:22:23,960 --> 01:22:25,830
and now we can take this

1303
01:22:25,830 --> 01:22:28,560
and solve this expression

1304
01:22:28,570 --> 01:22:31,190
so this is the same one and it's moving up to the top of the

1305
01:22:34,370 --> 01:22:38,140
so now we can

1306
01:22:38,200 --> 01:22:41,650
we can express this in terms of natural log

1307
01:22:41,670 --> 01:22:43,590
so the natural log

1308
01:22:43,610 --> 01:22:46,840
of the concentration of a at time t

1309
01:22:46,870 --> 01:22:53,600
minus the natural log of the original concentration of a equals minus k

1310
01:22:55,830 --> 01:22:59,310
and we can reorder that put a box around it which means is going to

1311
01:22:59,310 --> 01:23:00,800
be important later on

1312
01:23:00,840 --> 01:23:06,740
so we can rearrange this in terms of the natural log of the concentration at

1313
01:23:06,740 --> 01:23:07,630
time t

1314
01:23:07,700 --> 01:23:11,820
minus katie plus the natural log of the initial

1315
01:23:11,820 --> 01:23:16,490
concentration of a and so we come back to this in a few minutes we

1316
01:23:16,490 --> 01:23:20,290
can also keep going and rearrange this in other ways

1317
01:23:20,330 --> 01:23:28,830
so we can rewrite this term instead of by subtracting them rewrite as this division

1318
01:23:28,840 --> 01:23:35,210
so natural log of the concentration of a at time t over the original concentration

1319
01:23:35,280 --> 01:23:38,580
equals minus katie

1320
01:23:38,590 --> 01:23:43,050
and then we can take the inverse natural log

1321
01:23:43,060 --> 01:23:48,620
and so then on the other side we have e race to the minus eighty

1322
01:23:48,640 --> 01:23:52,900
and we can write this in this expression again box is the way that you

1323
01:23:52,900 --> 01:23:54,140
often see

1324
01:23:54,150 --> 01:23:56,600
the first order integrated rate law

1325
01:23:56,600 --> 01:24:01,280
where solving for the concentration of a at time t

1326
01:24:01,310 --> 01:24:06,980
is equal to the original concentration of a times e to the minus katie

1327
01:24:07,020 --> 01:24:11,440
so how the concentration of a changes depends on

1328
01:24:11,460 --> 01:24:18,460
the the rate constant and the time that has elapsed

1329
01:24:18,510 --> 01:24:23,340
so this is often the way that the integrated first-order rate laws written sometimes you

1330
01:24:23,340 --> 01:24:27,550
may see a little bit different expressions but usually this is the final expression that

1331
01:24:27,550 --> 01:24:29,740
you see

1332
01:24:29,750 --> 01:24:31,020
and then

1333
01:24:31,040 --> 01:24:33,560
is an equation for straight line

1334
01:24:33,580 --> 01:24:40,030
and that's why it was box because it how did is going to be plotted

1335
01:24:40,040 --> 01:24:44,770
so if you're going to use that now plot data

1336
01:24:44,830 --> 01:24:46,670
you can plot data

1337
01:24:46,690 --> 01:24:48,040
the whole act

1338
01:24:48,060 --> 01:24:53,520
so the concentration of a is changing sphere measuring how the concentration of a is

1339
01:24:53,520 --> 01:24:56,830
changing versus time

1340
01:24:56,830 --> 01:25:00,330
then rather than focusing on getting concentrated on the

1341
01:25:01,020 --> 01:25:02,380
the centre of this triangle

1342
01:25:03,300 --> 01:25:09,010
the probability mass starts going towards the edges and that the more you decrease the

1343
01:25:10,460 --> 01:25:11,170
o from one

1344
01:25:12,150 --> 01:25:16,480
the more concentrated around the edges that become so this is an interesting property

1345
01:25:18,930 --> 01:25:19,830
this the distribution

1346
01:25:20,460 --> 01:25:22,500
ants this is basically

1347
01:25:24,280 --> 01:25:27,190
these are basically what it looks looks like when you have

1348
01:25:27,720 --> 01:25:29,170
different values far

1349
01:25:29,810 --> 01:25:31,700
he landed on test match increases but

1350
01:25:32,200 --> 01:25:34,840
the special doesn't really want you to look at very long

1351
01:25:36,220 --> 01:25:38,040
you can you can play around with the

1352
01:25:38,580 --> 01:25:40,730
this their distributions by using the

1353
01:25:43,880 --> 01:25:45,020
there are density that's

1354
01:25:45,640 --> 01:25:48,320
you don't need to that now but this is basically

1355
01:25:48,770 --> 01:25:50,570
going to show you what happens when you do

1356
01:25:51,140 --> 01:25:53,410
input different parameter values and so on

1357
01:25:56,360 --> 01:25:58,360
so that usually distribution again

1358
01:26:00,070 --> 01:26:01,960
an interesting property right

1359
01:26:02,580 --> 01:26:06,230
if you combine entries of additional distributed probability vector

1360
01:26:07,070 --> 01:26:10,300
you will still end up with there's a distributed vectors

1361
01:26:10,880 --> 01:26:13,220
but with less number of dimensions this time

1362
01:26:13,630 --> 01:26:16,040
so if we have key dimensional

1363
01:26:16,890 --> 01:26:17,760
vector pi

1364
01:26:18,400 --> 01:26:23,920
that is usually distributed with parameters alpha one two alpha complicate ants if we simply

1365
01:26:23,920 --> 01:26:26,690
sum up the first two entries of this

1366
01:26:28,230 --> 01:26:31,810
similarity sum up the first two parameters of this distribution

1367
01:26:32,790 --> 01:26:33,650
this is what we get

1368
01:26:34,770 --> 01:26:37,890
the vector with one less dimensions is still

1369
01:26:38,370 --> 01:26:40,620
this distributed with this the shape this time

1370
01:26:41,100 --> 01:26:44,820
all the parameters the same except first planets are being some of the

1371
01:26:46,170 --> 01:26:47,310
first parameters there

1372
01:26:48,920 --> 01:26:54,140
and this is not specific to of course the first national first dimensions only you can do it or mentions

1373
01:26:54,540 --> 01:26:56,740
and the colors of this is also true

1374
01:26:57,460 --> 01:26:58,750
so you can have

1375
01:27:02,350 --> 01:27:03,480
of kate dimension

1376
01:27:04,600 --> 01:27:05,040
if i

1377
01:27:05,540 --> 01:27:10,210
being distributed with alpha one t alpha cape ann if your sample

1378
01:27:11,370 --> 01:27:16,830
these two one to two values from a two-dimensional dish which is also called the

1379
01:27:17,870 --> 01:27:19,750
but a function the distribution

1380
01:27:20,940 --> 01:27:22,250
and then you multiply

1381
01:27:22,810 --> 01:27:25,340
the corresponding entries with the corresponding

1382
01:27:26,820 --> 01:27:27,190
well you

1383
01:27:27,760 --> 01:27:29,110
like this by one times

1384
01:27:29,660 --> 01:27:35,720
the tau one and pi one time start to similar to offer one times but

1385
01:27:35,840 --> 01:27:38,030
one and alpha one one times

1386
01:27:38,630 --> 01:27:39,380
but the two

1387
01:27:40,340 --> 01:27:41,580
i think there's a typo there

1388
01:27:43,610 --> 01:27:49,220
making sure that this one plus with better to is equal to one then u get additional distribution

1389
01:27:54,610 --> 01:27:55,130
you can run

1390
01:27:55,590 --> 01:27:56,400
deep you generate

1391
01:28:01,090 --> 01:28:02,240
to see that we can

1392
01:28:02,790 --> 01:28:05,300
actually draw from additional process by

1393
01:28:05,700 --> 01:28:06,780
using this property

1394
01:28:07,960 --> 01:28:08,680
we start with

1395
01:28:09,110 --> 01:28:10,640
uh uniform distribution

1396
01:28:11,100 --> 01:28:12,410
all zero ten

1397
01:28:13,630 --> 01:28:15,710
and therefore did this is it's a o point one

1398
01:28:16,200 --> 01:28:18,550
and for keep refining by drawing

1399
01:28:19,050 --> 01:28:21,660
from this better distribution we keep refining

1400
01:28:22,110 --> 01:28:23,700
ah that this space

1401
01:28:25,210 --> 01:28:28,000
like this for example so i mean first row from the

1402
01:28:28,790 --> 01:28:30,280
but the distribution it

1403
01:28:30,760 --> 01:28:31,930
devices into two

1404
01:28:34,040 --> 01:28:34,660
two parts

1405
01:28:35,710 --> 01:28:37,800
and the height of that is proportional to the

1406
01:28:38,350 --> 01:28:39,410
values that we draw

1407
01:28:40,130 --> 01:28:42,250
and then we will keep rehearsing on

1408
01:28:42,650 --> 01:28:47,520
each of these helps like this like this and if you do it long enough

1409
01:28:49,320 --> 01:28:50,820
we're going to have something like this so

1410
01:28:51,790 --> 01:28:54,210
you actually you're going to have different personal

1411
01:28:56,250 --> 01:28:57,730
infinitesimal intervals

1412
01:28:58,130 --> 01:28:59,470
some of which will have

1413
01:29:00,170 --> 01:29:05,460
huge spikes and some of the more most of which will have a tiny spikes so

1414
01:29:06,070 --> 01:29:08,450
this is a very easy and fun way of

1415
01:29:10,450 --> 01:29:13,370
draw from the dish that this deletion process actually

1416
01:29:14,320 --> 01:29:18,720
it shows to that you can get it as the infinite limit of the addition distribution

1417
01:29:20,060 --> 01:29:23,120
there are different ways where you can define the distance

1418
01:29:25,120 --> 01:29:28,760
you can construct samples from the show there's a process or you can sample

1419
01:29:29,300 --> 01:29:35,140
from the process and one way to do it is by using the stick breaking construction

1420
01:29:35,560 --> 01:29:39,260
so if you go to the court the speed generate that's

1421
01:29:39,630 --> 01:29:40,940
code is going to

1422
01:29:42,690 --> 01:29:43,720
fixed four-year

1423
01:29:44,630 --> 01:29:45,950
so these are going to be

1424
01:29:46,400 --> 01:29:47,260
sticks that are

1425
01:29:47,830 --> 01:29:49,580
that i've presented a of the

1426
01:29:50,450 --> 01:29:51,120
none of these

1427
01:29:52,120 --> 01:29:53,740
rates forward additionally

1428
01:29:55,700 --> 01:29:57,880
parameter values so the sticks

1429
01:29:58,290 --> 01:30:03,150
i'm going to look like this because they are defined using this recursive formula

1430
01:30:04,460 --> 01:30:08,250
you is drawn from the better one of distribution

1431
01:30:10,110 --> 01:30:11,750
in expectation they are going to be

1432
01:30:12,190 --> 01:30:16,490
exponentially decreasing so the this red parts artistic lens of the

1433
01:30:17,100 --> 01:30:17,850
this process

1434
01:30:19,010 --> 01:30:20,910
these are going to be exponentially

1435
01:30:21,520 --> 01:30:23,050
decreasing in expectation

1436
01:30:23,510 --> 01:30:23,950
there fore

1437
01:30:23,950 --> 01:30:25,290
here was in one

1438
01:30:25,300 --> 01:30:29,250
black and white male female

1439
01:30:29,260 --> 01:30:31,350
interested not interested in this course

1440
01:30:32,110 --> 01:30:33,780
so that got me

1441
01:30:35,780 --> 01:30:38,910
and i'm going to go through the whole procedure of

1442
01:30:38,960 --> 01:30:41,920
standard CS

1443
01:30:41,980 --> 01:30:42,980
from a to c

1444
01:30:42,990 --> 01:30:44,400
well even beyond

1445
01:30:44,460 --> 01:30:46,330
before a and b and c

1446
01:30:46,360 --> 01:30:49,070
it is easy to side

1447
01:30:49,120 --> 01:30:51,490
they're going to develop the second technique

1448
01:30:51,490 --> 01:30:54,010
which is called an because he stands for

1449
01:30:54,020 --> 01:30:56,260
ninety value because you

1450
01:30:56,310 --> 01:31:01,050
it is simply an extension of course

1451
01:31:01,480 --> 01:31:05,030
meaning to still

1452
01:31:05,040 --> 01:31:07,540
i have clear categories in the data

1453
01:31:07,620 --> 01:31:09,980
but more than two

1454
01:31:10,140 --> 01:31:12,040
let's say

1455
01:31:12,090 --> 01:31:15,570
if you're working with social classes and you have

1456
01:31:15,640 --> 01:31:17,210
three qualitatively

1457
01:31:18,860 --> 01:31:23,260
subgroups that the working class middle class and upper class and you cannot three dichotomized

1458
01:31:23,970 --> 01:31:26,140
well then you can use any see

1459
01:31:26,160 --> 01:31:30,700
you have three values instead of two

1460
01:31:30,710 --> 01:31:32,440
there are

1461
01:31:32,450 --> 01:31:38,030
pros and cons i mean strength and weaknesses in each of these approaches techniques

1462
01:31:38,110 --> 01:31:40,590
one difficulty with images here is that

1463
01:31:40,600 --> 01:31:44,050
when used a lot of MV multi value

1464
01:31:44,130 --> 01:31:45,780
terrible conditions

1465
01:31:45,780 --> 01:31:46,610
you will

1466
01:31:46,740 --> 01:31:48,550
this page expand

1467
01:31:48,640 --> 01:31:51,920
because property space there will be a lot of diversity in the data and therefore

1468
01:31:51,920 --> 01:31:55,690
you will lose parsimony

1469
01:31:55,730 --> 01:31:59,230
you see as one of the core issues going to develop in the course

1470
01:31:59,240 --> 01:32:03,060
there's always a very important tension between

1471
01:32:03,300 --> 01:32:05,900
complexity in parsimony

1472
01:32:05,950 --> 01:32:09,220
every time they make a choice from complexity

1473
01:32:09,310 --> 01:32:12,980
well you you are bound to lose parsimony india

1474
01:32:13,830 --> 01:32:17,170
and the whole thing is weighing the pros and cons of

1475
01:32:17,310 --> 01:32:19,290
keeping the complexity

1476
01:32:19,380 --> 01:32:23,970
all going for parsimony i would like to be i will advocate quite strongly in

1477
01:32:23,970 --> 01:32:25,080
the first week

1478
01:32:25,090 --> 01:32:29,260
the the usefulness of a crisp set to see the basic one

1479
01:32:29,320 --> 01:32:31,140
zeros and ones that got them is

1480
01:32:33,520 --> 01:32:35,080
the shocking to some of you

1481
01:32:35,170 --> 01:32:38,380
because you seem to be losing so much information

1482
01:32:38,390 --> 01:32:39,310
if you

1483
01:32:39,310 --> 01:32:43,670
restrict if you summarize information that could in fact academic say if you have

1484
01:32:43,760 --> 01:32:45,540
information about the richness

1485
01:32:45,550 --> 01:32:48,210
affluence right in euros dollars

1486
01:32:48,250 --> 01:32:48,940
and you

1487
01:32:48,950 --> 01:32:53,290
you have very fine grained data and summarized as a dichotomy

1488
01:32:53,330 --> 01:32:56,720
you seem to a lot of information but then gives you a lot of leverage

1489
01:32:56,720 --> 01:32:57,720
on the data

1490
01:32:57,770 --> 01:33:02,280
because you know you will achieve a lot of applications in the casinos

1491
01:33:02,290 --> 01:33:04,250
it's a trade

1492
01:33:04,380 --> 01:33:08,620
so the chance cast and schneider in the second week

1493
01:33:08,670 --> 01:33:13,210
we actually made some counter-arguments and argue why it is interesting to use

1494
01:33:13,310 --> 01:33:17,490
fuzzy sets which is much more fine grained

1495
01:33:17,540 --> 01:33:19,100
and you will be able to choose

1496
01:33:19,200 --> 01:33:21,970
what to make arbitration of

1497
01:33:22,010 --> 01:33:24,090
your own way

1498
01:33:24,160 --> 01:33:28,450
we also in the first week do some refinements

1499
01:33:28,450 --> 01:33:32,110
and makes me more advanced things but it was the first big movie

1500
01:33:32,130 --> 01:33:35,280
kind of more basic

1501
01:33:35,380 --> 01:33:38,110
the second we will be more advanced

1502
01:33:38,120 --> 01:33:40,990
and then cast list neither will present

1503
01:33:41,050 --> 01:33:43,590
what is called viscous you

1504
01:33:43,650 --> 01:33:47,600
stands for fuzzy sets because you

1505
01:33:48,330 --> 01:33:52,990
it is not an extension of course it is see this kind of another approach

1506
01:33:53,030 --> 01:33:54,880
it starts from fuzzy sets

1507
01:33:54,960 --> 01:33:55,710
which is of

1508
01:33:55,750 --> 01:33:57,840
whole of the set of literature

1509
01:33:57,900 --> 01:33:59,810
fuzzy sets

1510
01:33:59,820 --> 01:34:01,870
are another way to envisage

1511
01:34:06,330 --> 01:34:07,520
let me say

1512
01:34:07,560 --> 01:34:09,270
a few words about this in a minute

1513
01:34:09,520 --> 01:34:11,710
in a few words

1514
01:34:14,620 --> 01:34:17,500
the functionalities begins by

1515
01:34:17,500 --> 01:34:19,820
defining pure set

1516
01:34:19,840 --> 01:34:22,570
pure set is like it to public property

1517
01:34:23,620 --> 01:34:25,540
some given kind for instance say a

1518
01:34:25,540 --> 01:34:27,060
you could define

1519
01:34:27,080 --> 01:34:29,250
theoretically or actually

1520
01:34:29,310 --> 01:34:33,970
pure set of what a rich country with b

1521
01:34:34,020 --> 01:34:38,710
purely rich country with a country where everyone is a millionaire in euros or dollars

1522
01:34:41,310 --> 01:34:44,130
and then you are the empirical world

1523
01:34:44,140 --> 01:34:46,530
and you will be able to two

1524
01:34:46,530 --> 01:34:50,270
determine to what extent each case observation

1525
01:34:50,280 --> 01:34:54,420
he belongs to that pure set

1526
01:34:54,850 --> 01:34:57,200
it's called fuzzy set membership scores

1527
01:34:57,280 --> 01:34:58,230
forget about it

1528
01:34:58,240 --> 01:35:00,330
it is here

1529
01:35:00,390 --> 01:35:03,220
for instance maybe

1530
01:35:03,520 --> 01:35:07,300
the the sultanate of of oman

1531
01:35:07,310 --> 01:35:09,310
before the crisis

1532
01:35:10,320 --> 01:35:11,550
it would have been

1533
01:35:11,610 --> 01:35:18,760
nineteen ninety nine percent inside this because most people over there are millionaires in dollars

1534
01:35:19,080 --> 01:35:20,610
hi t

1535
01:35:20,630 --> 01:35:23,050
we probably zero point zero one

1536
01:35:23,050 --> 01:35:24,820
compensate for perturbations

1537
01:35:26,720 --> 01:35:28,840
says look at learning the target territory first

1538
01:35:30,880 --> 01:35:35,960
difficult specified by hand because helicopter dynamics is complicated we tried we tried very hard

1539
01:35:36,090 --> 01:35:39,020
and we didn't succeed and prosper about a year year and a half

1540
01:35:39,470 --> 01:35:42,080
resulting in the video you saw the helicopter going through the trees

1541
01:35:42,550 --> 01:35:43,450
we read a lot of books

1542
01:35:44,200 --> 01:35:45,010
tried a lot of things

1543
01:35:46,200 --> 01:35:48,390
that was said this by the water readings

1544
01:35:48,730 --> 01:35:51,310
this is too hard for us maybe we can just have a machine learning

1545
01:35:52,670 --> 01:35:55,290
our solution was to collect demonstrations of desired maneuvers

1546
01:35:56,140 --> 01:35:56,900
and somehow

1547
01:35:57,380 --> 01:35:58,740
extract the queen trajectory from

1548
01:35:59,630 --> 01:36:03,010
the challenge here is that the pilot even if he's an expert pilot will not

1549
01:36:03,010 --> 01:36:05,750
do the same thing over and over and we ask him to do demonstrations

1550
01:36:06,360 --> 01:36:09,720
so getting rid of the de-noising problem you get a budget demonstrations

1551
01:36:10,150 --> 01:36:11,820
some i wanna get a clean version out

1552
01:36:12,240 --> 01:36:14,010
and that's going to be a target trajectory

1553
01:36:17,120 --> 01:36:17,540
show u

1554
01:36:18,670 --> 01:36:20,840
the title demonstrations were getting from our pilot

1555
01:36:22,690 --> 01:36:23,800
are you see here is

1556
01:36:25,260 --> 01:36:26,310
six demonstrations

1557
01:36:26,940 --> 01:36:28,110
all the same air show

1558
01:36:28,670 --> 01:36:29,270
it's a sequence

1559
01:36:29,630 --> 01:36:30,940
aggressive aerobatic maneuvers

1560
01:36:32,290 --> 01:36:38,480
we have very sort of sensors on the helicopter on the ground track position and orientation reasonably accurately

1561
01:36:39,700 --> 01:36:42,250
and then we see here is a graphical replay

1562
01:36:42,680 --> 01:36:45,350
of was actually flown up to some reasonable accuracy

1563
01:36:46,280 --> 01:36:48,690
look at this these demonstrations are very similar

1564
01:36:49,100 --> 01:36:51,250
this outcomes are going through the same sequence of maneuvers

1565
01:36:52,740 --> 01:36:56,830
they're not doing it at the same time not the same place now what exactly the same orientation

1566
01:36:57,750 --> 01:36:59,900
so we want to clean this up and get out one

1567
01:37:01,210 --> 01:37:02,660
that's the best abilities

1568
01:37:03,860 --> 01:37:05,600
and better than the best evolves

1569
01:37:09,360 --> 01:37:12,820
you work in machine learning and natural thing to do is to think of these as well

1570
01:37:13,560 --> 01:37:16,860
looks like in its human rights in the sequence of states they you try to

1571
01:37:16,860 --> 01:37:19,610
recover the end you get observations over time

1572
01:37:20,230 --> 01:37:21,670
so that's all we started out with

1573
01:37:22,360 --> 01:37:25,960
the sequence of states we don't know what they are but we're going to recover them

1574
01:37:27,490 --> 01:37:28,370
the dynamics model

1575
01:37:29,030 --> 01:37:31,340
but a helicopter can be plugged into the transition model

1576
01:37:33,100 --> 01:37:35,720
the demonstrations are observations so we get to see

1577
01:37:36,290 --> 01:37:37,750
so the helicopter states

1578
01:37:38,770 --> 01:37:41,060
because the timing isn't exactly the same in all these

1579
01:37:41,710 --> 01:37:42,840
we don't know how to come up

1580
01:37:43,560 --> 01:37:44,370
it is in a state

1581
01:37:44,900 --> 01:37:46,940
but if we knew how to come up with average

1582
01:37:49,900 --> 01:37:50,710
how to align these

1583
01:37:52,300 --> 01:37:57,440
before an iterative procedure so the first thing is to say well let's just randomly initializes initial trajectory

1584
01:37:58,220 --> 01:38:00,910
let's say with one demonstrations are some

1585
01:38:02,380 --> 01:38:03,100
often makes them

1586
01:38:03,820 --> 01:38:08,750
we come up with by hand usually we take the one has the average duration of all demonstrations about five

1587
01:38:09,130 --> 01:38:09,680
they going with the

1588
01:38:10,330 --> 01:38:11,270
middle the median

1589
01:38:11,680 --> 01:38:13,670
duration but in the hidden trajectory

1590
01:38:15,220 --> 01:38:16,010
was about in there

1591
01:38:16,570 --> 01:38:17,970
we can do pairwise alignment

1592
01:38:18,610 --> 01:38:21,260
so alignment is expensive from multiple sequences

1593
01:38:21,660 --> 01:38:23,130
but you just have to it's fairly

1594
01:38:23,670 --> 01:38:24,250
efficient do

1595
01:38:24,940 --> 01:38:26,390
so we live demo one

1596
01:38:28,520 --> 01:38:31,600
an introductory that we now have an estimate we line them up to

1597
01:38:33,530 --> 01:38:34,210
now in a state

1598
01:38:35,820 --> 01:38:36,610
state sequence

1599
01:38:37,200 --> 01:38:38,080
now we're going to recover

1600
01:38:38,680 --> 01:38:39,860
and a set of observations

1601
01:38:40,310 --> 01:38:41,060
two and each time

1602
01:38:44,100 --> 01:38:44,490
we run

1603
01:38:45,020 --> 01:38:46,540
it's meant forward backward here

1604
01:38:47,030 --> 01:38:52,270
is a continuous state space be extended kalman filter we run forward backward it's the same idea

1605
01:38:53,050 --> 01:38:54,290
standard probabilistic inference

1606
01:38:55,380 --> 01:39:00,630
right backward gets city-state estimate sound and this is our first with the hidden trajectory could be

1607
01:39:01,830 --> 01:39:04,790
was the first one we iterate so now we have a better assessment

1608
01:39:05,340 --> 01:39:07,030
we disconnect these observations

1609
01:39:07,770 --> 01:39:11,620
we get dynamic time warping which is standard already from the speech literature

1610
01:39:12,190 --> 01:39:14,030
and the same are enemies is known as minimum wage

1611
01:39:14,470 --> 01:39:16,470
in the biological sequence alignment literature

1612
01:39:17,040 --> 01:39:17,860
so run down

1613
01:39:18,310 --> 01:39:18,920
the real line

1614
01:39:21,280 --> 01:39:23,100
then rerun the forward backward

1615
01:39:24,350 --> 01:39:25,330
in new trajectory out

1616
01:39:25,610 --> 01:39:27,540
keep repeating this until this converges

1617
01:39:28,300 --> 01:39:30,080
using this takes about five iterations

1618
01:39:30,080 --> 01:39:31,270
x y

1619
01:39:31,300 --> 01:39:33,520
we have actually

1620
01:39:33,550 --> 01:39:35,320
i'm family

1621
01:39:35,370 --> 01:39:38,950
so that it decomposes show

1622
01:39:39,700 --> 01:39:40,930
some these

1623
01:39:40,950 --> 01:39:42,640
which far from

1624
01:39:42,650 --> 01:39:44,530
unknown to him

1625
01:39:44,590 --> 01:39:46,520
the british

1626
01:39:46,550 --> 01:39:48,800
of time and

1627
01:39:53,480 --> 01:39:59,420
in a way that it is the last station

1628
01:39:59,750 --> 01:40:03,990
it also allows to capture on the frequencies

1629
01:40:04,000 --> 01:40:05,880
but also by the way she

1630
01:40:07,060 --> 01:40:11,690
it's all these basis functions that depend on one another

1631
01:40:11,710 --> 01:40:12,960
have one

1632
01:40:12,980 --> 01:40:15,670
what to this

1633
01:40:15,720 --> 01:40:17,330
the basic

1634
01:40:17,340 --> 01:40:20,520
and it is the basis functions

1635
01:40:20,580 --> 01:40:28,910
and based on these models we can all these are just performing she scaling of

1636
01:40:28,910 --> 01:40:30,420
the model

1637
01:40:30,450 --> 01:40:33,520
so we have article actually is called scale

1638
01:40:33,530 --> 01:40:36,760
and applying this scaling by my mother

1639
01:40:37,440 --> 01:40:38,830
we cannot

1640
01:40:39,320 --> 01:40:43,070
all the basis and that they can be infinite

1641
01:40:43,110 --> 01:40:46,100
because we can apply this scaling

1642
01:40:46,150 --> 01:40:50,330
in thing that's mars but usually is

1643
01:40:52,190 --> 01:40:53,830
several hours

1644
01:40:53,840 --> 01:40:55,910
on the be

1645
01:40:55,960 --> 01:40:59,400
and after that can try to come

1646
01:40:59,410 --> 01:41:00,670
i don't

1647
01:41:01,710 --> 01:41:05,230
into as the new system of

1648
01:41:05,250 --> 01:41:11,410
what can we do not

1649
01:41:11,460 --> 01:41:16,930
and i understand why know here functions when

1650
01:41:18,720 --> 01:41:22,870
you can imagine that in which it is to the actual function

1651
01:41:22,870 --> 01:41:23,640
so i

1652
01:41:23,660 --> 01:41:28,910
we have the value function just intended to land

1653
01:41:28,990 --> 01:41:30,010
the point

1654
01:41:30,020 --> 01:41:30,960
x y

1655
01:41:30,980 --> 01:41:32,130
so it's the

1656
01:41:32,130 --> 01:41:35,120
ordinary dimensions described one

1657
01:41:35,140 --> 01:41:36,480
and we can apply

1658
01:41:36,510 --> 01:41:40,240
this could transform this function

1659
01:41:40,260 --> 01:41:41,470
and the two of them

1660
01:41:41,750 --> 01:41:43,400
in new way

1661
01:41:44,420 --> 01:41:47,670
look and is now in which one to

1662
01:41:47,670 --> 01:41:50,960
but you can see that these

1663
01:41:52,260 --> 01:41:54,730
works actually like he does

1664
01:41:54,750 --> 01:41:57,910
because when we project

1665
01:41:57,920 --> 01:42:00,910
our portal because it's a white card

1666
01:42:01,760 --> 01:42:03,560
the car according to be

1667
01:42:03,600 --> 01:42:05,850
we know how many

1668
01:42:06,860 --> 01:42:10,900
in our and how we know the great white fleet

1669
01:42:10,920 --> 01:42:14,300
it's the same with these functions so we know

1670
01:42:14,300 --> 01:42:15,570
the first was it

1671
01:42:15,620 --> 01:42:17,280
basis functions

1672
01:42:17,300 --> 01:42:19,690
and we actually project our

1673
01:42:19,810 --> 01:42:23,120
six function of the bones of function

1674
01:42:23,150 --> 01:42:28,120
and the because there's lot the because there will be one of the coordinate x

1675
01:42:29,170 --> 01:42:31,570
that means that the best of

1676
01:42:31,590 --> 01:42:35,780
this one is this really is the basis functions

1677
01:42:35,940 --> 01:42:39,070
and now we can make

1678
01:42:40,240 --> 01:42:41,400
another way

1679
01:42:41,420 --> 01:42:46,690
so that the function is usually like two thousand images this

1680
01:42:47,550 --> 01:42:52,110
only ever basis functions kind of it and therefore

1681
01:42:52,110 --> 01:42:55,990
one of the which was always the first

1682
01:42:56,010 --> 01:42:57,630
so actually performed

1683
01:42:57,630 --> 01:42:58,930
the formation

1684
01:42:59,240 --> 01:43:01,180
the formation of our

1685
01:43:01,190 --> 01:43:02,590
you need to function

1686
01:43:02,590 --> 01:43:03,870
which is that the

1687
01:43:03,960 --> 01:43:07,530
then mention what we wait

1688
01:43:07,560 --> 01:43:09,680
this debate function

1689
01:43:09,740 --> 01:43:11,590
one of them

1690
01:43:11,600 --> 01:43:13,820
and you

1691
01:43:13,960 --> 01:43:19,140
the late energy so we don't really exist some way

1692
01:43:19,160 --> 01:43:20,520
of this form

1693
01:43:20,550 --> 01:43:22,260
the because the

1694
01:43:22,320 --> 01:43:24,200
the method the function

1695
01:43:24,280 --> 01:43:27,310
which is one of the soul

1696
01:43:27,320 --> 01:43:28,960
it is more like this

1697
01:43:31,270 --> 01:43:35,340
it's the basic idea of all these wavelet based

1698
01:43:35,340 --> 01:43:37,450
the station

1699
01:43:37,510 --> 01:43:42,920
there are a lot of weight so women can be complex

1700
01:43:42,930 --> 01:43:44,920
we can be

1701
01:43:45,850 --> 01:43:49,360
like the three women the same way of life

1702
01:43:49,380 --> 01:43:53,150
generally the transform applied to the one way

1703
01:43:53,170 --> 01:43:55,050
is that they are

1704
01:43:55,060 --> 01:43:59,940
this kind of formation become just so well we're usually called life

1705
01:44:02,170 --> 01:44:03,900
so this

1706
01:44:03,940 --> 01:44:05,670
basis functions

1707
01:44:05,690 --> 01:44:07,150
which of scale

1708
01:44:07,150 --> 01:44:09,980
just an assumption right you may not believe it or maybe you believe it to

1709
01:44:09,980 --> 01:44:13,670
some degree are not that the degree of anything i definitely want the function which

1710
01:44:13,670 --> 01:44:17,610
is global is most to some degree i don't want to for suppose my unlabelled

1711
01:44:17,610 --> 01:44:21,680
data it's crazy i don't want to function which you know will fit very well

1712
01:44:21,680 --> 01:44:28,130
and whatever the crazy stuff and unlabelled data but doesn't somehow respect global

1713
01:44:28,140 --> 01:44:30,710
niceness of the function

1714
01:44:32,110 --> 01:44:36,060
for example the line the manifold assumption might be completely false

1715
01:44:36,070 --> 01:44:40,890
right you it may be that actually the correct but first

1716
01:44:40,930 --> 01:44:45,200
then why would i want to you know keep that up

1717
01:44:45,250 --> 01:44:48,440
i don't know in real life it seems to help so it seems that helps

1718
01:44:48,450 --> 01:44:51,280
usually set this up

1719
01:44:51,290 --> 01:44:53,640
to be a little bit

1720
01:44:54,780 --> 01:45:00,240
because it somehow allows you to control this extreme sick niceness of the function which

1721
01:45:00,240 --> 01:45:01,670
you get

1722
01:45:01,710 --> 01:45:08,570
and in some sense it's it's sort of denoising if you wish

1723
01:45:08,710 --> 01:45:10,460
OK so

1724
01:45:10,470 --> 01:45:15,340
but in this case i don't think it makes a difference

1725
01:45:19,570 --> 01:45:24,820
OK so here are some experiments

1726
01:45:24,840 --> 01:45:30,680
basically blue is using unlabelled data and RAD is not using unlabelled data those guys

1727
01:45:30,820 --> 01:45:32,040
error rates

1728
01:45:32,110 --> 01:45:34,990
and there are some digits dataset

1729
01:45:35,110 --> 01:45:39,170
so you can see that using unlabelled data helps

1730
01:45:39,210 --> 01:45:43,740
that's the main point and there are some comparisons which i think i

1731
01:45:46,270 --> 01:45:48,980
but effectively

1732
01:45:48,980 --> 01:45:54,240
so OK so here you have rigorously square and here you have to pour support

1733
01:45:54,240 --> 01:45:59,470
vector machine so support vector machines with laplacian penalty versus support vector machines and he

1734
01:45:59,470 --> 01:46:01,750
regularly nicely

1735
01:46:01,770 --> 01:46:05,930
versus regularized least squares a lot less in

1736
01:46:05,940 --> 01:46:10,710
so if you are first it's interesting to note to the basically support vector machines

1737
01:46:10,710 --> 01:46:13,380
and regularized least squares actually work about the same

1738
01:46:13,400 --> 01:46:16,880
which is the case in many real applications

1739
01:46:16,900 --> 01:46:22,590
now while when you add unlabelled data i think this is roughly ten times more

1740
01:46:22,590 --> 01:46:26,420
ten to twenty maybe twenty times more unlabelled data unlabelled data

1741
01:46:26,500 --> 01:46:30,050
you get pretty dramatic drops in there

1742
01:46:30,070 --> 01:46:32,610
so actually does

1743
01:46:34,230 --> 01:46:36,440
this is the same for sphere SVM

1744
01:46:36,440 --> 01:46:40,050
and and this is something that comparisons to something

1745
01:46:40,070 --> 01:46:44,710
to actually vapnik for support

1746
01:46:44,730 --> 01:46:46,690
and again

1747
01:46:46,760 --> 01:46:49,300
there are some more comparisons

1748
01:46:53,070 --> 01:46:57,610
i would like to ignore the various other things which we can compare but i

1749
01:46:57,610 --> 01:47:00,050
think the most important point here to make

1750
01:47:00,070 --> 01:47:03,210
events for this support vector machines

1751
01:47:03,230 --> 01:47:07,000
using just to label points

1752
01:47:07,010 --> 01:47:08,280
and this is

1753
01:47:08,610 --> 01:47:11,110
support vector machine

1754
01:47:11,170 --> 01:47:15,230
using all points so you you know you in principle you know the labelled of

1755
01:47:15,230 --> 01:47:18,070
unlabeled points because the cell standard datasets

1756
01:47:18,090 --> 01:47:21,190
so this is using all points

1757
01:47:21,230 --> 01:47:22,710
this is using just

1758
01:47:23,380 --> 01:47:28,110
and labeled points and this is using labelled plus unlabeled combination

1759
01:47:28,130 --> 01:47:31,800
so you can see the three percent this is a this is five so that's

1760
01:47:31,800 --> 01:47:33,880
in between here

1761
01:47:33,880 --> 01:47:37,500
you actually get a zero if you point however if you don't know them you

1762
01:47:37,500 --> 01:47:41,440
get pretty awful right twenty four percent when you add unlabelled data you drop from

1763
01:47:41,440 --> 01:47:43,710
twenty four to three percent

1764
01:47:43,710 --> 01:47:45,590
you have

1765
01:47:45,610 --> 01:47:48,150
the true error rate three

1766
01:47:48,150 --> 01:47:51,900
this is readily outside was only inside

1767
01:47:51,920 --> 01:47:54,770
that's what the rainbows

1768
01:47:54,770 --> 01:47:59,060
so i i you know that if you look at the right there

1769
01:47:59,110 --> 01:48:01,810
like for the directions

1770
01:48:01,860 --> 01:48:03,710
all of the

1771
01:48:03,810 --> 01:48:06,270
all right

1772
01:48:06,330 --> 01:48:08,880
here is my shape parameter

1773
01:48:11,440 --> 01:48:14,380
i can that all the light through now in the middle

1774
01:48:14,420 --> 01:48:16,400
and i'm going to rotate it

1775
01:48:16,440 --> 01:48:18,650
now i kill that life

1776
01:48:18,670 --> 01:48:21,560
that is convincing proof that light

1777
01:48:21,650 --> 01:48:23,600
highly polarized

1778
01:48:23,630 --> 01:48:25,540
almost hundred percent

1779
01:48:25,540 --> 01:48:27,560
i can do the same here

1780
01:48:27,610 --> 01:48:30,270
i let the light through this parameter

1781
01:48:30,330 --> 01:48:32,310
this is apparently for the sheet

1782
01:48:32,330 --> 01:48:34,000
a direction

1783
01:48:34,080 --> 01:48:35,440
order was asian

1784
01:48:35,460 --> 01:48:40,230
i rotated very slowly gets darker and darker now

1785
01:48:40,250 --> 01:48:44,500
britain dark almost nothing goes through in

1786
01:48:44,560 --> 01:48:46,460
you see this

1787
01:48:48,210 --> 01:48:52,730
highly polarized also the white light inside there

1788
01:48:52,830 --> 01:48:54,610
just below the

1789
01:48:54,630 --> 01:48:55,690
white light

1790
01:48:55,710 --> 01:48:57,560
we also see that in nature

1791
01:48:57,560 --> 01:48:58,940
that light is also

1792
01:48:59,960 --> 01:49:05,480
nearly hundred percent polarized because the white light also goes away

1793
01:49:08,790 --> 01:49:10,560
from now on

1794
01:49:10,600 --> 01:49:12,540
you will always

1795
01:49:12,560 --> 01:49:14,210
carry on you

1796
01:49:14,290 --> 01:49:16,690
linear form

1797
01:49:16,710 --> 01:49:17,690
you simply

1798
01:49:17,770 --> 01:49:19,980
cannot go out on the street

1799
01:49:20,020 --> 01:49:22,270
without a linear

1800
01:49:22,290 --> 01:49:24,210
i always have my wallet

1801
01:49:24,230 --> 01:49:26,830
please one million

1802
01:49:26,880 --> 01:49:29,020
and whenever you see a rainbow

1803
01:49:29,060 --> 01:49:32,210
you take the linear polarizer and you make sure

1804
01:49:33,210 --> 01:49:34,540
as per

1805
01:49:34,540 --> 01:49:36,380
maybe they will come

1806
01:49:36,440 --> 01:49:39,000
that is not polarized in which case

1807
01:49:39,000 --> 01:49:40,020
you should call me

1808
01:49:40,040 --> 01:49:41,750
right away

1809
01:49:41,770 --> 01:49:43,520
i always carry on you

1810
01:49:43,540 --> 01:49:47,730
a linear flow

1811
01:49:47,790 --> 01:49:50,540
now i come to the third

1812
01:49:50,580 --> 01:49:53,830
the way last part that we can

1813
01:49:53,880 --> 01:49:55,960
well i like

1814
01:49:56,040 --> 01:49:59,810
not by means of reflection but remember i mentioned

1815
01:49:59,860 --> 01:50:02,830
fine dust particles

1816
01:50:02,880 --> 01:50:06,380
light strikes fine dust particles

1817
01:50:06,440 --> 01:50:09,860
it doesn't reflect nice way

1818
01:50:09,880 --> 01:50:14,520
but it's getting all directions

1819
01:50:14,540 --> 01:50:19,020
it goes off in this direction something this interaction we give them different would recall

1820
01:50:19,020 --> 01:50:23,080
that scan

1821
01:50:23,130 --> 01:50:24,000
and when

1822
01:50:24,020 --> 01:50:25,900
fine dust

1823
01:50:25,960 --> 01:50:29,330
can also become

1824
01:50:29,380 --> 01:50:31,040
one percent

1825
01:50:31,060 --> 01:50:32,900
paul this

1826
01:50:34,480 --> 01:50:36,770
do we just the right way

1827
01:50:36,770 --> 01:50:40,250
however the light coming from below

1828
01:50:40,250 --> 01:50:43,080
later that's where it will come up

1829
01:50:45,710 --> 01:50:47,360
beam of light will trade

1830
01:50:47,580 --> 01:50:53,500
and dust particles in here

1831
01:50:53,560 --> 01:50:55,400
and those the article

1832
01:50:55,420 --> 01:50:57,310
next before

1833
01:50:57,360 --> 01:51:00,520
which we will use cigarettes much as i hate to smoke

1834
01:51:00,580 --> 01:51:02,940
after the war

1835
01:51:03,730 --> 01:51:06,650
well the article

1836
01:51:06,670 --> 01:51:11,460
and some of the slides

1837
01:51:11,500 --> 01:51:13,520
which comes up

1838
01:51:13,520 --> 01:51:17,330
changes the direction not as nice as it will do of glass

1839
01:51:17,380 --> 01:51:21,290
reflects some of it will go in this direction some in this direction some in

1840
01:51:21,290 --> 01:51:27,190
this election some industry and some other countries

1841
01:51:27,210 --> 01:51:29,920
some of the world

1842
01:51:30,000 --> 01:51:31,960
if this slide

1843
01:51:31,960 --> 01:51:33,940
in this way

1844
01:51:33,980 --> 01:51:38,420
is scattered over an angle of ninety degrees

1845
01:51:42,670 --> 01:51:45,020
that that's not what i just want to show

1846
01:51:45,130 --> 01:51:46,730
this four

1847
01:51:46,790 --> 01:51:50,980
how can we be sure that you see is getting over ninety degrees well

1848
01:51:51,020 --> 01:51:53,840
all of you will almost perfect conditions

1849
01:51:53,860 --> 01:51:56,250
because the light comes up like this

1850
01:51:56,360 --> 01:51:58,400
and so it goes in this direction

1851
01:51:58,420 --> 01:52:02,130
no matter what goes there were there were there were there

1852
01:52:02,150 --> 01:52:06,250
the change in direction is almost always no matter where you sit in the

1853
01:52:06,270 --> 01:52:07,340
ninety degrees

1854
01:52:07,360 --> 01:52:08,980
we follow that goes from this

1855
01:52:09,020 --> 01:52:10,610
this that's ninety degrees

1856
01:52:10,630 --> 01:52:13,400
this this is like three this is

1857
01:52:13,520 --> 01:52:16,630
to all of you are almost in an ideal position

1858
01:52:16,690 --> 01:52:20,420
you see that light that was kind of my small part

1859
01:52:20,540 --> 01:52:22,600
and it will be polarized

1860
01:52:22,600 --> 01:52:24,520
but also circles

1861
01:52:25,630 --> 01:52:26,520
that's all

1862
01:52:26,540 --> 01:52:29,650
alright like this right in front

1863
01:52:29,670 --> 01:52:31,040
all rights like this

1864
01:52:31,060 --> 01:52:35,560
a little bit there will be more like this we would this

