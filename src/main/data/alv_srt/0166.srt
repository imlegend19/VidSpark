1
00:00:00,000 --> 00:00:04,880
to create value by doing a structured and unstructured dance

2
00:00:04,900 --> 00:00:10,840
now here's the game for example and going give you real travel allow what I

3
00:00:10,840 --> 00:00:14,460
really it is like I love travel 1 I also love knowledge collective not so

4
00:00:14,460 --> 00:00:18,180
I wonder below thing that you know we we wanna go on a trip somewhere

5
00:00:18,180 --> 00:00:23,960
you ask your friends even in there will so the say let's do that on

6
00:00:23,960 --> 00:00:28,240
the scale of earth specialist at acidity just anyone in the world has ever been

7
00:00:28,240 --> 00:00:30,940
to any place in the world you want to go out and get the best

8
00:00:30,940 --> 00:00:36,800
knowledge of all local hotels places to go things to do and so on so

9
00:00:36,810 --> 00:00:43,780
simple concept but it's interesting pommel for collect analysis there's just a quick screenshot newcomer

10
00:00:43,800 --> 00:00:47,280
Carey most of these words but this is the bits a blog sites essentially a

11
00:00:47,280 --> 00:00:51,920
vertical blog sites where we collect when people come write their blogs they share their

12
00:00:51,920 --> 00:00:55,920
stories just like you shirt photos of the creature stories with that would look like

13
00:00:55,920 --> 00:01:00,500
a magazine articles of it out pretty anyway and they're all connected together so here's

14
00:01:00,500 --> 00:01:04,240
a bunch this is the Bangkok Bank of Thailand is a bunch of blogs about

15
00:01:04,240 --> 00:01:09,000
Bangkok along all rank ordered by user redeems the fault

16
00:01:09,940 --> 00:01:17,840
now what's interesting about this is why does the blogger a writer and will 1

17
00:01:17,850 --> 00:01:20,780
is that we give them a better place like showed flier but you know what

18
00:01:20,780 --> 00:01:25,580
I gallery era of we give among space of free free host space to publish

19
00:01:25,900 --> 00:01:30,040
and we have really nice designs and so on so that it looks Mitchell and

20
00:01:30,040 --> 00:01:35,020
making sure their friends and family but as a second-order effects which is now while

21
00:01:35,020 --> 00:01:39,960
that of my photos and flicker likable among Kodak Gallery of flickers a community and

22
00:01:39,960 --> 00:01:46,460
part of a larger community connect with other people along interest lines so that's part

23
00:01:46,460 --> 00:01:49,800
of the game for the blogger now we can give them all the other blogs

24
00:01:49,800 --> 00:01:53,980
in Bangkok although blogs around the same time the all the other blogs and by

25
00:01:53,980 --> 00:01:59,240
you know all kinds of dimensions and other photos of the same place that you

26
00:01:59,240 --> 00:02:02,890
were at the same time the other side this is weird to give the reader

27
00:02:02,940 --> 00:02:06,920
something to do here that and the readers in fact really all they care about

28
00:02:06,920 --> 00:02:12,740
is given a destination what can I do that so we basically take this bloggers

29
00:02:12,740 --> 00:02:16,360
daily which is basically it was left to was own be these isolated murals although

30
00:02:16,510 --> 00:02:21,400
universe aggregating together and index here

31
00:02:21,660 --> 00:02:27,820
so this a technology technique that's familiar themselves share but now it's something co-opted advised

32
00:02:27,820 --> 00:02:34,140
browsing arises where you can't have it along we picture again you can pivot like

33
00:02:34,140 --> 00:02:39,900
you go down to the Bangkok you can go up to Thailand over 2 photos

34
00:02:39,900 --> 00:02:45,520
down the hotel reviews by some blogger like fold-out blogger down at threat then pick

35
00:02:45,520 --> 00:02:49,800
up another thing and destination of England somewhere else and so what is it just

36
00:02:49,800 --> 00:02:55,420
like you do want inflicted serious indeed or even wikipedia lost not hypertext there but

37
00:02:55,420 --> 00:03:01,440
flickers actually indexical where hypertext sort of normal data but in this case third dimension

38
00:03:01,500 --> 00:03:09,280
you like these tag dimensions time dimensions redeem dimensions but you can serve now what's

39
00:03:09,280 --> 00:03:13,040
the backbone of this it wouldn't have worked if I just Sony's into a big

40
00:03:13,040 --> 00:03:18,060
search engine the only way the real opposes off is because we have a very

41
00:03:18,060 --> 00:03:24,560
strong backing structure which is the destination hierarchy and turns that's as our problem there

42
00:03:24,790 --> 00:03:29,340
that millions of needed in cities like things in the world and is a big

43
00:03:29,340 --> 00:03:34,080
number and make their in always languages and there's tons of redundancy and it's a

44
00:03:34,080 --> 00:03:39,400
really nasty hierarchy and just try to be England right we have to roll things

45
00:03:39,400 --> 00:03:44,240
the UK as a country half-dozen doesn't England's and is not you know it's crazy

46
00:03:44,360 --> 00:03:48,480
and the CIA has a database out there but it's been it's a full blood

47
00:03:48,500 --> 00:03:53,640
and tests and budgeted for waste anyway it's a problem so I'll get back to

48
00:03:53,640 --> 00:03:57,610
what that kind of thing but once we got the destination there's sort of hammered

49
00:03:57,610 --> 00:03:59,000
into shape

50
00:03:59,160 --> 00:04:03,160
but you don't want to amount of work on the data mining data management and

51
00:04:03,160 --> 00:04:08,140
then we do program we got now if use the structured data as a backup

52
00:04:08,140 --> 00:04:13,700
to all kinds inferential services so this is not very deep inference but if I

53
00:04:13,700 --> 00:04:19,520
have a blog about Bangkok guess what it's also about Thailand an inference users something

54
00:04:19,620 --> 00:04:24,000
that we use just put it in the Bangkok spots and you can also say

55
00:04:24,130 --> 00:04:27,120
by the way if I have a photos in a blog about Bangkok it's also

56
00:04:27,190 --> 00:04:31,940
photo about Thailand's so I can circle of opposing in Thailand and so on all

57
00:04:31,970 --> 00:04:34,980
by the way I can also look up the Geo coordinators and popular and so

58
00:04:34,980 --> 00:04:43,520
cool map or indeed whole rout out all this stuff follows from the structured background

59
00:04:43,540 --> 00:04:48,820
on away from the play other players ecosystem like advertisers they like to target based

60
00:04:48,820 --> 00:04:54,400
on where you're going how do you say we're going the strange I mean yes

61
00:04:54,400 --> 00:04:59,570
it's true to say PARIS they're probably disintegrated to be PARIS France and so Hurst

62
00:04:59,570 --> 00:05:04,280
Texas but you know sometimes they go wrong and they don't like in arms across

63
00:05:04,290 --> 00:05:08,980
the money anyway is sort what it might look like in this case the photos

64
00:05:08,990 --> 00:05:14,840
you might see if typed in Thailand for photos nobody organized these photos by Thailand

65
00:05:14,850 --> 00:05:19,760
before Connecticut the photo and you can serve through all of in Thailand nobody organized

66
00:05:19,760 --> 00:05:24,450
into that group emerged from a combination of people stick in a blogs in a

67
00:05:24,450 --> 00:05:28,340
little piece of the treated they live in Bangkok residents and all the everything else

68
00:05:28,340 --> 00:05:34,700
inherited down is 1 of a piece of this sort of multidimensional proposal which is

69
00:05:34,700 --> 00:05:36,360
testament to play

70
00:05:36,560 --> 00:05:40,690
now we do that people act and to be honest with you with the common

71
00:05:40,700 --> 00:05:44,880
is mixed success it turns out that like all things in this world the social

72
00:05:44,880 --> 00:05:50,290
world people only do things for enlightened self-interest and most of the time and tagging

73
00:05:50,290 --> 00:05:54,700
world the self-interest is kind of bizarre kind of me depends on where in some

74
00:05:54,700 --> 00:06:00,500
cases like people like analyze canister Dyson chases photos and flicker and there's this sort

75
00:06:00,500 --> 00:06:07,040
of obsessive-compulsive woman comes in mind tags off with 4 is a symbiotic relationship and

76
00:06:07,040 --> 00:06:13,200
they have different motives and so it tagging is often different know who knows why

77
00:06:13,200 --> 00:06:17,880
this case at any a real compelling reason the accurately side of things so people

78
00:06:17,880 --> 00:06:21,520
mostly down but some of them do but what we did is we say look

79
00:06:21,520 --> 00:06:25,200
it good Taggart all we're going to use statistics to tell you which are the

80
00:06:25,200 --> 00:06:30,120
most useful wants to use the words and so it's a example we call snap

81
00:06:30,120 --> 00:06:35,280
to grid said they would destinations you start typing if you as a destination police

82
00:06:35,280 --> 00:06:39,860
on the destination in this was likely to be completing the watching match so snaps

83
00:06:39,860 --> 00:06:45,960
you into the grid of structured data not tags early structure but the discrete now

84
00:06:45,960 --> 00:06:48,760
it turns out have all these tags running around and this is somewhat of a

85
00:06:48,760 --> 00:06:58,040
controlled vocabularies suggested cannibal warmly controlled softly controlled vocabulary then you have this nice combination

86
00:06:58,040 --> 00:07:03,080
of structure construction indices so grass example of you could back entirely new you go

87
00:07:03,090 --> 00:07:08,100
here all the tags about Thailand notice they organized by Medicare worries that wasn't users

88
00:07:08,100 --> 00:07:12,780
doing that that was us doing that but we didn't take us long plywood ties

89
00:07:12,790 --> 00:07:17,750
people care about and of course it to snap into the grid and you not

90
00:07:17,800 --> 00:07:21,920
and you know pluralization all kind of stuff is not we still have the folk

91
00:07:21,920 --> 00:07:25,740
song we still we have things organizers and the weights and I can go as

92
00:07:25,740 --> 00:07:31,070
a mother well you know there's the beach tag what's there the beach tag and

93
00:07:31,070 --> 00:07:32,520
for example

94
00:07:32,530 --> 00:07:33,900
each segment type

95
00:07:33,910 --> 00:07:35,440
has the typical length

96
00:07:35,450 --> 00:07:39,610
for example wall segments tend to be longer than four segments each segment type has

97
00:07:39,610 --> 00:07:41,120
the typical dept

98
00:07:41,140 --> 00:07:43,230
relative to the newest line

99
00:07:43,240 --> 00:07:45,150
one of them to be close to that have

100
00:07:45,320 --> 00:07:47,230
dorset to be deep

101
00:07:48,230 --> 00:07:50,770
here's another neighboring segments

102
00:07:50,770 --> 00:07:52,120
there are aligned

103
00:07:52,140 --> 00:07:56,320
the line basically means that the angle is below some threshold and neighbouring means that

104
00:07:56,320 --> 00:08:01,190
you know the distance it wasn't actually tend to be of the same type

105
00:08:01,220 --> 00:08:05,820
notice that this is this is a collective inference rule like this is what propagates

106
00:08:05,830 --> 00:08:09,300
labels between neighboring nodes

107
00:08:09,320 --> 00:08:13,580
and here's another one it says that if the segment has no previous line segment

108
00:08:13,690 --> 00:08:16,470
in this part of the line then it's the start of the line

109
00:08:16,480 --> 00:08:17,900
this makes sense

110
00:08:17,910 --> 00:08:21,740
if think is the start of the line then the x y coordinates are the

111
00:08:23,310 --> 00:08:26,770
if you think start of line then it's starting point should be the starting point

112
00:08:26,770 --> 00:08:30,440
of the line but again it's not exactly the starting point because when i the

113
00:08:30,440 --> 00:08:36,230
line it couldn't manage exactly the borders allows is girl some slack right this

114
00:08:36,240 --> 00:08:40,080
this equation is we've got slack so i'm going to try to make them the

115
00:08:40,080 --> 00:08:42,570
same but not too much

116
00:08:42,570 --> 00:08:46,220
and now he is the key rule is the one that says the following

117
00:08:46,220 --> 00:08:50,540
if the segment is part of the line then the slope of the segment should

118
00:08:50,540 --> 00:08:52,610
be the same as the flow

119
00:08:52,700 --> 00:08:53,610
the line

120
00:08:53,640 --> 00:08:57,040
from the starting point of the line to the starting point of the same

121
00:08:57,070 --> 00:08:59,300
but there should be no in between

122
00:08:59,330 --> 00:09:03,940
and i have the same rules for ending segments and so what this rule is

123
00:09:03,940 --> 00:09:04,900
going to do

124
00:09:04,950 --> 00:09:07,890
is this collective you know is this inference

125
00:09:07,890 --> 00:09:12,110
we're and simultaneously trying to find the best fit line to all the segments and

126
00:09:12,110 --> 00:09:15,450
trying to decide which segments belong to the one in which don't

127
00:09:15,700 --> 00:09:20,230
so if this sentence works well what's going to happen is that the door segments

128
00:09:20,240 --> 00:09:21,230
are going to be excluded

129
00:09:21,740 --> 00:09:24,980
and i'm going i'm gonna wind up averaging only

130
00:09:24,980 --> 00:09:28,730
the line only the wall segments to the side walls

131
00:09:28,760 --> 00:09:30,010
and we've done this

132
00:09:30,030 --> 00:09:31,430
and it works well

133
00:09:31,490 --> 00:09:36,610
and it's another successful joint inference because what happens is even if even you know

134
00:09:36,610 --> 00:09:39,300
the originally like for example this paper by

135
00:09:39,320 --> 00:09:40,690
link it kind of

136
00:09:40,720 --> 00:09:43,980
all i was trying to do was labelled the segments

137
00:09:44,030 --> 00:09:47,910
no we don't we doing this during inference between the segments and figuring out what

138
00:09:47,910 --> 00:09:49,430
the lines are

139
00:09:49,470 --> 00:09:54,350
and doing this in addition to telling us more also there's labeling of the segments

140
00:09:54,370 --> 00:09:57,690
trying to figure out what the lines are jointly with leaving the segments of this

141
00:09:57,720 --> 00:09:59,900
label the segments that

142
00:09:59,900 --> 00:10:00,780
OK so

143
00:10:01,770 --> 00:10:06,400
let me mention i could use in milan's for things like planning

144
00:10:08,220 --> 00:10:14,180
well classical planning is easy to write because you can formulate as the satisfiability problem

145
00:10:14,220 --> 00:10:16,140
and solve in the usual way

146
00:10:16,160 --> 00:10:20,890
satisfiability is the state-of-the-art methods for solving planning problems and we've done that with things

147
00:10:20,890 --> 00:10:24,930
like the blocks world and as i mentioned earlier you know we can scale to

148
00:10:24,930 --> 00:10:26,890
much larger problems than before

149
00:10:26,900 --> 00:10:28,930
but now you know what happens

150
00:10:28,940 --> 00:10:31,900
when actions have certain effects right this is the kind of thing that we know

151
00:10:31,900 --> 00:10:33,120
want to be able to do well

152
00:10:33,150 --> 00:10:37,260
all we need to do to have certain effects for the actions is

153
00:10:38,080 --> 00:10:41,010
given finite which the action axioms

154
00:10:41,030 --> 00:10:43,740
x x that things like if the block is on the table and then i

155
00:10:43,740 --> 00:10:45,890
pick it up in the block is in my hand

156
00:10:45,910 --> 00:10:49,190
in classical planning this is deterministic

157
00:10:49,200 --> 00:10:52,480
in real planning sometimes you know the block slips

158
00:10:52,530 --> 00:10:55,940
so what i need to do is only to give that of finite weights

159
00:10:55,950 --> 00:11:01,480
and that allows me to have action on certain actions with uncertain effects i can

160
00:11:01,480 --> 00:11:05,620
also has sensing actions which again can be noisy what i need to do is

161
00:11:05,640 --> 00:11:11,320
i need to add clauses relating sensor states to the sensor readings to world states

162
00:11:11,360 --> 00:11:17,940
and again i for the world state in perfectly from the sensors using probabilistic inference

163
00:11:17,940 --> 00:11:22,820
how might i relational markov decision processes this is a very active area of research

164
00:11:22,820 --> 00:11:29,240
these days trying to combine classical planning with mdps and people have proposed many approaches

165
00:11:29,800 --> 00:11:34,010
one that we haven't tried but you know are going to try shortly is

166
00:11:34,020 --> 00:11:38,650
again using markov logic this is very straightforward we can do is we can add

167
00:11:38,650 --> 00:11:41,330
to the weights to the clauses

168
00:11:41,360 --> 00:11:44,480
with the following semantics is this clause true

169
00:11:44,490 --> 00:11:47,180
then i have to switch to my two

170
00:11:47,180 --> 00:11:51,290
that would give us a hundred thousand

171
00:11:51,300 --> 00:11:57,930
and then this would be sort of like the fourier sine series of the wind

172
00:11:57,990 --> 00:11:59,830
but if i choose

173
00:11:59,850 --> 00:12:03,410
that if i make that is that a good choice

174
00:12:03,440 --> 00:12:07,710
the take those functions sin x sin to accept assign a hundred that's what i'm

175
00:12:07,710 --> 00:12:10,000
going to do then i going up like this

176
00:12:10,060 --> 00:12:11,360
into the

177
00:12:14,770 --> 00:12:18,870
the point that into the minimum things and minimize

178
00:12:18,890 --> 00:12:22,660
but we minimize i've only got

179
00:12:22,720 --> 00:12:24,660
these are my unknowns now

180
00:12:24,710 --> 00:12:26,240
the weight

181
00:12:26,320 --> 00:12:27,650
the unknown

182
00:12:27,720 --> 00:12:31,390
and unknown

183
00:12:34,330 --> 00:12:37,970
i've got one hundred and no

184
00:12:37,990 --> 00:12:42,420
i'm not too crazy about the choice of sin x sin to accept assigned one

185
00:12:42,420 --> 00:12:45,330
hundred x one not

186
00:12:45,370 --> 00:12:49,620
because the calculations would take a long time

187
00:12:49,630 --> 00:12:50,790
i have to

188
00:12:50,810 --> 00:12:55,430
be able to do it to compute this integral for functions like SIN one hundred

189
00:12:57,850 --> 00:13:02,530
and then minimize over this hundred dimensional space

190
00:13:02,540 --> 00:13:07,070
and the idea of finite elements was the better choice

191
00:13:07,120 --> 00:13:10,630
instead of using sines and cosines

192
00:13:10,650 --> 00:13:13,310
what's what's said better choice

193
00:13:13,310 --> 00:13:15,640
of trial function

194
00:13:17,130 --> 00:13:18,630
you could say OK

195
00:13:18,650 --> 00:13:20,200
like those guys in

196
00:13:20,250 --> 00:13:24,680
but still you have these integrals

197
00:13:24,700 --> 00:13:29,240
show when galerkin

198
00:13:29,300 --> 00:13:32,670
suggested this idea

199
00:13:32,730 --> 00:13:35,080
well the computer of course didn't exist

200
00:13:35,650 --> 00:13:37,540
and and was three

201
00:13:39,350 --> 00:13:40,630
probably too

202
00:13:42,790 --> 00:13:45,170
in other words so galerkin

203
00:13:45,240 --> 00:13:48,240
what i spent a lot of time

204
00:13:48,250 --> 00:13:52,530
trying to get because he's only choosing to functions he would try to get to

205
00:13:52,530 --> 00:13:54,140
functions that were pretty

206
00:13:54,200 --> 00:13:56,790
close to the right to the real when

207
00:13:56,830 --> 00:14:00,890
course the real winner is unknown but he would he would work hard to try

208
00:14:00,890 --> 00:14:04,300
to find functions are close to it

209
00:14:04,350 --> 00:14:05,710
what if we

210
00:14:05,730 --> 00:14:08,900
if we deal with the fourier series and we take a hundred or a thousand

211
00:14:08,900 --> 00:14:12,580
terms well we don't have to worry or we could have we got a good

212
00:14:12,580 --> 00:14:16,130
approximation we can expect yes we probably have

213
00:14:16,180 --> 00:14:17,660
if if our

214
00:14:17,680 --> 00:14:21,180
if we're taking a thousand for eight terms

215
00:14:21,230 --> 00:14:22,900
and we find the best

216
00:14:22,950 --> 00:14:27,830
thousand weights we can probably come down close to the true value

217
00:14:27,870 --> 00:14:33,730
that but my complaint about the fourier choice was the cost of

218
00:14:36,000 --> 00:14:40,630
all right so let me let's invent the finite element method here

219
00:14:41,950 --> 00:14:46,000
give me a suggest some other functions

220
00:14:51,770 --> 00:14:56,820
so that i could take the functions one x x squared xq about x to

221
00:14:56,820 --> 00:15:00,630
the ninety nine

222
00:15:00,690 --> 00:15:05,980
that would be their myspace would be all the polynomials of degree ninety nine

223
00:15:05,990 --> 00:15:08,430
that hundred dimensional space

224
00:15:08,440 --> 00:15:11,520
i could minimize

225
00:15:13,870 --> 00:15:16,210
OK you it's OK so

226
00:15:16,230 --> 00:15:19,680
of with with that polynomial what do you think

227
00:15:19,940 --> 00:15:23,380
is that is that a good choice

228
00:15:23,390 --> 00:15:25,430
i think

229
00:15:25,460 --> 00:15:29,280
not the best OK so you have another

230
00:15:29,320 --> 00:15:35,340
so you're going to take functions

231
00:15:35,600 --> 00:15:40,570
now you are going the right direction OK so you're going to take functions that

232
00:15:40,610 --> 00:15:43,930
i this the kind of function you're putting in

233
00:15:43,950 --> 00:15:46,380
OK so this is what waltz

234
00:15:47,280 --> 00:15:49,390
so this is the wall street

235
00:15:52,060 --> 00:15:54,130
sorry that somebody

236
00:15:54,150 --> 00:16:02,470
well will add your name also walls why are OK

237
00:16:02,520 --> 00:16:09,920
are but this you're you're absolutely right can i am i allowed function

238
00:16:10,020 --> 00:16:12,900
the derivative is the problem

239
00:16:13,520 --> 00:16:15,910
you're you caught me out actually

240
00:16:15,970 --> 00:16:18,180
i said any function

241
00:16:18,200 --> 00:16:20,630
but i really overdid it

242
00:16:20,680 --> 00:16:24,720
because that because the step function

243
00:16:24,740 --> 00:16:29,210
have the derivative is the delta and here we squaring the darned thing and we

244
00:16:29,210 --> 00:16:30,720
would get infinite

245
00:16:31,810 --> 00:16:34,560
so i really should have

246
00:16:34,610 --> 00:16:40,110
requiring that the function that i need the thing to state finite

247
00:16:41,200 --> 00:16:43,080
now you're all year

248
00:16:43,080 --> 00:16:46,830
take one more step and you've invented by

249
00:16:48,670 --> 00:16:51,780
so instead of piecewise constant

250
00:16:51,790 --> 00:16:54,000
whichever jump

251
00:16:54,030 --> 00:16:56,290
and the derivative is

252
00:16:56,310 --> 00:17:00,610
is delta which was really blows us up

253
00:17:00,860 --> 00:17:03,500
what should i take now

254
00:17:03,530 --> 00:17:05,190
p fries and you

255
00:17:05,210 --> 00:17:06,720
these piecewise

256
00:17:06,740 --> 00:17:08,130
so now

257
00:17:08,140 --> 00:17:10,280
switch to

258
00:17:10,280 --> 00:17:14,680
linear finite elements they people piecewise linear

259
00:17:14,750 --> 00:17:17,570
linear pieces linear elements

260
00:17:17,570 --> 00:17:20,360
that's where the word elements coming in

261
00:17:20,370 --> 00:17:23,840
OK so my my functions are

262
00:17:24,680 --> 00:17:29,400
the functions i'm allowing manner or any piecewise linear

263
00:17:29,420 --> 00:17:30,890
right so

264
00:17:30,900 --> 00:17:31,630
and now

265
00:17:31,630 --> 00:17:33,520
everybody sees

266
00:17:33,570 --> 00:17:36,570
that we made our life

267
00:17:36,620 --> 00:17:41,500
much easier as far as calculating these intervals

268
00:17:41,520 --> 00:17:46,180
as we got a pretty darn simple functions which we can integrate

269
00:17:46,210 --> 00:17:47,660
a piece of the time

270
00:17:47,660 --> 00:17:51,160
b given that we know the value of c

271
00:17:51,220 --> 00:17:52,500
and finally

272
00:17:52,520 --> 00:17:57,790
there's the computational reason why graphical models are very exciting

273
00:17:58,560 --> 00:18:00,470
allow us to define

274
00:18:00,480 --> 00:18:03,720
general message passing algorithms

275
00:18:03,730 --> 00:18:06,930
to implement probabilistic inference efficiently

276
00:18:06,940 --> 00:18:12,040
so if we want to answer a question like what is the probability distribution over

277
00:18:12,040 --> 00:18:17,100
a given that we know very c takes on some value little c

278
00:18:17,360 --> 00:18:23,370
we can do this by sending messages on the graph so when we implement things

279
00:18:23,370 --> 00:18:28,100
on the computer we can do kind of object oriented implementation where each node sends

280
00:18:28,100 --> 00:18:33,050
messages to its neighbors and after a bunch of messages get past we can compute

281
00:18:33,050 --> 00:18:36,290
all the right probabilities that we're interested in

282
00:18:36,300 --> 00:18:45,060
so graphical models really combined ideas from statistics from graph theory and computer science

283
00:18:47,610 --> 00:18:53,140
even if you even if you don't use these message passing algorithms you'll find the

284
00:18:53,140 --> 00:18:58,540
graphical models all over papers in machine learning and pattern recognition these this is important

285
00:18:58,540 --> 00:19:01,040
to know what exactly they mean

286
00:19:01,150 --> 00:19:08,160
OK so fundamentally graphical models represent conditional independence and we going to use the following

287
00:19:10,120 --> 00:19:14,390
no conditional independence this as access is

288
00:19:14,430 --> 00:19:16,840
are independent of y

289
00:19:18,790 --> 00:19:23,780
and what that means is that the probability distribution over x

290
00:19:23,790 --> 00:19:26,640
given y and v

291
00:19:26,690 --> 00:19:30,430
is equal to the probability distribution over x given v

292
00:19:31,370 --> 00:19:33,550
you know in other words why

293
00:19:33,600 --> 00:19:38,870
it doesn't really affect the distribution of x once we know the value of v

294
00:19:39,310 --> 00:19:43,470
and this is just the technical conditions so that these things are well defined these

295
00:19:43,470 --> 00:19:45,310
probabilities are well defined

296
00:19:45,430 --> 00:19:51,740
another way of thinking about this is x is independent of y given v

297
00:19:51,750 --> 00:19:57,230
if the joint distribution of x and y given v factors into this tradition of

298
00:19:57,230 --> 00:20:01,640
x given b and the distribution of y given t

299
00:20:01,900 --> 00:20:06,600
and we can think of conditional independence between sets of variables in a completely analogous

300
00:20:07,480 --> 00:20:13,160
a bunch of variables x and bunch variables y are going to be conditionally independent

301
00:20:13,160 --> 00:20:14,640
given v if the

302
00:20:14,660 --> 00:20:20,840
joint distribution factors in this way and the more traditional notion of independence which is

303
00:20:20,840 --> 00:20:23,610
marginal independence we can write like this

304
00:20:23,670 --> 00:20:26,190
it just says x is independent of y

305
00:20:26,210 --> 00:20:30,900
or in other words x is independent of y given the empty set of variables

306
00:20:30,910 --> 00:20:36,160
if you want to write in this conditional independence form that just says the joint

307
00:20:36,160 --> 00:20:42,160
distribution of x and y factors into the distribution of x the distribution of y

308
00:20:42,160 --> 00:20:44,740
OK so

309
00:20:44,750 --> 00:20:48,030
you know we can think of a bunch of examples in the real world where

310
00:20:48,030 --> 00:20:51,690
you might have conditional or marginal independence

311
00:20:51,710 --> 00:20:53,230
for example

312
00:20:53,240 --> 00:20:59,040
the amount of speeding fine that you get a

313
00:20:59,080 --> 00:21:04,950
should be independent of the type of car you are driving given the speed that

314
00:21:04,950 --> 00:21:06,740
your car driving

315
00:21:06,790 --> 00:21:09,560
otherwise it wouldn't seem very fair

316
00:21:12,120 --> 00:21:15,160
you know whether you have lung cancer

317
00:21:15,160 --> 00:21:18,590
it is independent of whether you have teeth

318
00:21:18,600 --> 00:21:20,860
yellow teeth sorry

319
00:21:20,920 --> 00:21:27,920
given that you're smoking although smoking can cause both lung cancer in your teeth to

320
00:21:27,920 --> 00:21:29,580
turn yellow

321
00:21:30,560 --> 00:21:36,570
etcetera so you know we can we can imagine lots of different examples of marginal

322
00:21:36,570 --> 00:21:38,890
and conditional independence

323
00:21:39,550 --> 00:21:44,470
and i will go through all of them but it's it's a little subtle to

324
00:21:44,470 --> 00:21:49,720
think about these things because sometimes things can be marginally independent like for example if

325
00:21:49,720 --> 00:21:55,470
you have two teams playing each other with players chosen at random from a lot

326
00:21:55,470 --> 00:22:00,010
of a large pool players then the ability of team a the ability of team

327
00:22:00,010 --> 00:22:02,510
b might be marginally independent

328
00:22:03,000 --> 00:22:05,800
variables by

329
00:22:06,190 --> 00:22:11,940
given that you know the outcome of the game versus a versus b then these

330
00:22:11,940 --> 00:22:17,940
are no longer marginally independent so for example if we know that a b b

331
00:22:18,000 --> 00:22:23,160
then you would infer that the ability of team a is more is likely to

332
00:22:23,800 --> 00:22:26,000
greater than the ability of team b

333
00:22:26,040 --> 00:22:27,250
right there

334
00:22:27,260 --> 00:22:29,660
no longer remain independent

335
00:22:29,760 --> 00:22:35,200
OK so graphical models are way of representing conditional and marginal independence

336
00:22:35,210 --> 00:22:38,270
and i'm going to start just by talking about factor graphs

337
00:22:38,300 --> 00:22:41,290
in fact graphs you have

338
00:22:41,290 --> 00:22:46,910
the notes only have two types of nodes circles represent random variables and these all

339
00:22:46,910 --> 00:22:50,840
filled dots represent factors in the joint distribution

340
00:22:50,890 --> 00:22:53,900
so for example this factor graph here

341
00:22:53,920 --> 00:22:56,710
represents the fact that the probability distribution over

342
00:22:56,760 --> 00:23:02,370
a through e factors into the product of a function of

343
00:23:02,380 --> 00:23:08,150
a and c that's this represented by this not here a function of b c

344
00:23:08,150 --> 00:23:12,950
and d represented by this and the function of c d and e and this

345
00:23:12,950 --> 00:23:16,890
one over z is simply a normalisation constant

346
00:23:16,900 --> 00:23:24,050
this factor graph here on this side represents this factorisation of the joint distribution

347
00:23:24,060 --> 00:23:32,340
and the normalisation constant is simply the sum over all the possible settings of all

348
00:23:32,340 --> 00:23:36,240
the variables of the products of the factors

349
00:23:38,200 --> 00:23:42,460
it's just what you need so that the probabilities sum to one

350
00:23:44,210 --> 00:23:47,500
what these graphs represent is that

351
00:23:47,510 --> 00:23:50,790
is a family of probability distributions so

352
00:23:50,790 --> 00:23:57,090
this graph here represents all probability distributions over

353
00:23:57,150 --> 00:23:58,840
five variables

354
00:23:58,860 --> 00:24:01,590
that can be written in this form

355
00:24:03,500 --> 00:24:04,500
and it

356
00:24:04,510 --> 00:24:07,670
in the graph we can talk about things

357
00:24:07,710 --> 00:24:11,830
like neighborhood relationships between nodes and these are going to be important to figure out

358
00:24:11,830 --> 00:24:18,470
what the independence relationships are between the random variables so two nodes are neighbors in

359
00:24:18,470 --> 00:24:23,900
a factor graph if they share a common factor so a and c are neighbours

360
00:24:23,900 --> 00:24:27,260
but a and d are not neighbors

361
00:24:30,900 --> 00:24:32,660
this is just a recap

362
00:24:32,660 --> 00:24:36,130
what i said on the previous slide

363
00:24:36,160 --> 00:24:39,640
and now we can talk about how these graphs

364
00:24:39,640 --> 00:24:44,310
so i'll give you five minutes

365
00:27:49,500 --> 00:27:55,690
so we know time of the solution or

366
00:27:55,730 --> 00:27:57,060
most people who work

367
00:27:57,080 --> 00:28:01,280
how many of the people have solved the problem already

368
00:28:03,310 --> 00:28:08,280
then they give it another two minutes

369
00:29:19,440 --> 00:29:22,960
two q one to solve

370
00:29:23,030 --> 00:29:27,620
i'm not used

371
00:30:12,830 --> 00:30:15,850
so i can knew the solution the whiteboard but

372
00:30:15,860 --> 00:30:19,650
maybe someone is considered he has found the right solution she is on the right

373
00:30:19,650 --> 00:30:22,430
solution and i would like to to present

374
00:30:22,500 --> 00:30:25,990
i should say we will have a prior for the people who find the most

375
00:30:27,720 --> 00:30:30,380
it's a very rare item

376
00:30:31,150 --> 00:30:35,070
one day in the future command high prices if you sell it on ebay

377
00:30:35,300 --> 00:30:40,240
we know is when we printed these teachers you have this little girl

378
00:30:40,250 --> 00:30:41,570
there here

379
00:30:41,900 --> 00:30:44,960
one of the teachers upside down

380
00:30:45,040 --> 00:30:47,010
i don't know why

381
00:30:47,160 --> 00:30:52,730
but we believe mean something so we have to do do something with this tissue

382
00:30:52,830 --> 00:30:58,330
so i will have occasional a little exercise like this in my lectures and

383
00:30:58,340 --> 00:31:00,380
whoever solves most of them

384
00:31:00,700 --> 00:31:04,120
get the t-shirt maybe also some problems in the election

385
00:31:06,130 --> 00:31:08,080
because you have to prove that use of

386
00:31:08,130 --> 00:31:11,260
not enough you tell us you will have to do it here

387
00:31:11,330 --> 00:31:12,470
now help you

388
00:31:12,510 --> 00:31:14,330
if you want

389
00:31:15,790 --> 00:31:18,210
to convince anyone

390
00:31:18,210 --> 00:31:23,160
maybe not in the very first lecture

391
00:31:23,210 --> 00:31:25,290
she so many signs

392
00:31:27,040 --> 00:31:30,350
OK i'll do it

393
00:31:48,110 --> 00:31:51,760
so we have courageous volunteer

394
00:32:08,200 --> 00:32:10,330
direct everything within

395
00:32:11,050 --> 00:32:14,160
i just want to be true

396
00:32:14,180 --> 00:32:16,040
c four

397
00:32:18,220 --> 00:32:20,830
OK in particular

398
00:32:20,860 --> 00:32:22,280
like just

399
00:32:22,550 --> 00:32:25,120
he sits on the one you

400
00:32:25,210 --> 00:32:30,310
so you get something like this

401
00:32:30,340 --> 00:32:32,390
but with the opposite sign

402
00:32:32,600 --> 00:32:38,970
with the opposite sign you can use instead of writing in terms of

403
00:32:41,330 --> 00:32:43,750
in terms of the dot product between two

404
00:32:43,780 --> 00:32:49,260
the point is you go OK just show us right

405
00:32:49,270 --> 00:32:51,410
do you want the microphone

406
00:32:51,720 --> 00:32:56,290
can you can you hear him without microphone

407
00:32:56,290 --> 00:32:59,120
the values

408
00:33:04,220 --> 00:33:05,660
first of all way it to

409
00:33:11,840 --> 00:33:14,290
this formulation is just

410
00:33:15,650 --> 00:33:19,460
people that one slide

411
00:33:19,540 --> 00:33:26,050
so here we have

412
00:33:26,070 --> 00:33:28,720
the one sided between

413
00:33:28,730 --> 00:33:30,050
my c

414
00:33:43,890 --> 00:33:44,900
so what

415
00:33:52,110 --> 00:33:53,470
right is

416
00:34:00,760 --> 00:34:02,040
this is

417
00:34:02,730 --> 00:34:05,390
to see why

418
00:34:05,480 --> 00:34:10,330
my da my

419
00:34:10,380 --> 00:34:11,820
like this

420
00:34:12,790 --> 00:34:15,160
doesn't seem like

421
00:34:18,080 --> 00:34:19,230
see my

422
00:34:19,230 --> 00:34:24,210
so it's really hard to be here today in two

423
00:34:24,210 --> 00:34:30,390
there should be some of the changes taking place in the world to talk about

424
00:34:30,390 --> 00:34:34,510
a fundamental shift is happening in the world of advertising

425
00:34:34,510 --> 00:34:36,440
also advertising

426
00:34:36,440 --> 00:34:37,920
so when this

427
00:34:38,100 --> 00:34:42,030
i spent some time period

428
00:34:43,330 --> 00:34:45,100
imagine the second order

429
00:34:45,100 --> 00:34:48,630
this hundreds of millions of dollars spent every

430
00:34:49,960 --> 00:34:52,450
and the statistics back

431
00:34:52,560 --> 00:34:59,310
providing such decisions about how to effectively space

432
00:34:59,340 --> 00:35:02,310
it's actually the content was missing for

433
00:35:02,450 --> 00:35:05,490
first produced

434
00:35:05,700 --> 00:35:11,060
it's essentially consumers get messages to talk about

435
00:35:11,070 --> 00:35:13,810
the size of the data

436
00:35:13,880 --> 00:35:15,170
in fact

437
00:35:15,180 --> 00:35:21,220
it's beginning to rapidly replaced traditional forms that size it introduces

438
00:35:21,560 --> 00:35:23,970
the complete paradigm shift in terms of

439
00:35:24,020 --> 00:35:28,610
they have to be interpreted and analyzed and speed which that analysis needs to take

440
00:35:28,610 --> 00:35:31,550
place the advertising to be effective

441
00:35:31,880 --> 00:35:36,280
in some sort of shared experience in terms of

442
00:35:36,330 --> 00:35:39,310
organisation scientists try

443
00:35:39,330 --> 00:35:42,100
it allows them to develop new models

444
00:35:42,110 --> 00:35:44,890
to assess those models together models into production

445
00:35:44,940 --> 00:35:51,220
without having to worry continuously about the underlying deficit necessary to manage petabytes of data

446
00:35:51,640 --> 00:35:56,390
and and you know these runtime environments in operation worldwide basis

447
00:35:56,410 --> 00:36:00,690
i'd also like to use today to introduce a visiting scholar program talk about what

448
00:36:00,710 --> 00:36:03,220
more about that at the end of the day

449
00:36:03,330 --> 00:36:08,950
so i'm going to touch upon the business of internet advertising just provided a brief

450
00:36:08,950 --> 00:36:12,710
primer on on how it works and ultimately

451
00:36:12,720 --> 00:36:17,830
the data that generates one of the things about digital advertising compared to sort traditional

452
00:36:18,360 --> 00:36:24,370
broadcast print advertising is the very process of delivering the advertising creates data record

453
00:36:24,420 --> 00:36:29,550
in fact enormous quantities of data produced every day and this data contains rich consumer

454
00:36:29,550 --> 00:36:34,210
experience that we can start to mine to understand patterns and build predictive models

455
00:36:34,260 --> 00:36:40,620
ultimately these models are about understanding consumers that understanding the interesting affinities consumers so we

456
00:36:40,620 --> 00:36:44,570
can ensure they receive more relevant content lasted until a little bit about how we

457
00:36:44,580 --> 00:36:49,010
organised company the way we solve these problems of of making sort of science is

458
00:36:49,010 --> 00:36:53,710
effective when there's lots of other things to take care as well

459
00:36:57,260 --> 00:37:00,430
we're in the midst of the biggest shift in the history of media

460
00:37:00,490 --> 00:37:05,980
media is transitioning from a one-size-fits-all model of broadcast model

461
00:37:05,990 --> 00:37:11,670
so the dynamic real-time environment where consumers can choose whatever they want anytime and content

462
00:37:11,670 --> 00:37:14,140
can be selected for now

463
00:37:14,170 --> 00:37:20,090
media has historically always been about broadcast what that means is that everyone who consumed

464
00:37:20,110 --> 00:37:25,800
everyone who receive the magazine or watching television show would see the same content the

465
00:37:25,800 --> 00:37:30,070
same same media content and the same advertising content

466
00:37:30,080 --> 00:37:34,950
and in those environments everything we did was about to understand the audience and that

467
00:37:34,990 --> 00:37:39,180
we need to understand every individual is reading a magazine which is that to understand

468
00:37:39,240 --> 00:37:44,890
aggregate what was the characteristics of the entire population of people to read the magazine

469
00:37:44,890 --> 00:37:49,010
anyone in tune into the television show we see the same advertisement

470
00:37:50,050 --> 00:37:53,710
but our computers are digital devices and mobile phones are i had

471
00:37:53,730 --> 00:37:59,550
these are effectively blank slide nothing is broadcast them brothers delivered individually to them based

472
00:37:59,550 --> 00:38:03,620
on request that we can in turn out browsers may

473
00:38:03,640 --> 00:38:08,930
and the way it works is when we go to a particular website a request

474
00:38:08,950 --> 00:38:12,290
is made about websites of potentially many services

475
00:38:13,110 --> 00:38:15,640
content is returned it is rendered

476
00:38:15,640 --> 00:38:17,020
by browser

477
00:38:17,040 --> 00:38:20,550
and of course that content could be the same for everyone

478
00:38:20,570 --> 00:38:23,110
you know the server may be set up with a simple set of rules to

479
00:38:23,110 --> 00:38:25,070
return the same content to everyone

480
00:38:25,480 --> 00:38:29,520
but in fact that server can decide based on whatever logic our algorithms are present

481
00:38:29,520 --> 00:38:32,020
in any data that has available

482
00:38:32,040 --> 00:38:37,040
to provide a different message has the has the power to actually serve different communications

483
00:38:37,040 --> 00:38:40,840
to each individual and everything that takes place

484
00:38:41,780 --> 00:38:43,440
behind your browser

485
00:38:43,470 --> 00:38:46,230
is the online advertising ecosystem

486
00:38:46,260 --> 00:38:49,100
all of the different parties that enable

487
00:38:49,120 --> 00:38:51,140
buyers and sellers media

488
00:38:51,150 --> 00:38:54,930
two interact that enable content to be distributed to

489
00:38:54,940 --> 00:39:00,820
two and machines enable us to respond all sit the next cloud behind computers

490
00:39:02,330 --> 00:39:03,810
that ecosystem

491
00:39:03,830 --> 00:39:09,200
fundamentally changes the way we need to think about making decisions

492
00:39:09,250 --> 00:39:13,760
because the geometric expansion the number decisions need to be made we move away from

493
00:39:13,770 --> 00:39:15,190
whole audiences

494
00:39:15,210 --> 00:39:16,450
and to individuals

495
00:39:16,460 --> 00:39:21,440
we're we're not making decisions about an entire website but rather making decisions about individual

496
00:39:22,400 --> 00:39:27,730
on top of that we have fragmentation of media consumption we can watch the same

497
00:39:28,460 --> 00:39:31,700
across many different screens at different times

498
00:39:31,730 --> 00:39:33,080
from different places

499
00:39:33,090 --> 00:39:38,040
and all of these additional considerations in terms of what content may be relevant

500
00:39:38,140 --> 00:39:43,780
now alongside this massive expansion and the amount of data we have enormous compression at

501
00:39:43,780 --> 00:39:46,980
the time frames that are available for us to make decisions

502
00:39:47,130 --> 00:39:52,730
you know when making a decision about which advertisement which magazine to advertise a product

503
00:39:53,430 --> 00:39:56,020
we might spend many weeks making that decision

504
00:39:56,020 --> 00:39:57,720
we have one insertion order

505
00:39:57,750 --> 00:40:01,760
and every single copy of the magazine shows the same advertisement

506
00:40:01,820 --> 00:40:04,120
well the internet because in real time

507
00:40:04,140 --> 00:40:08,930
and the time that the service had to make a decision to consumers experience isn't

508
00:40:08,930 --> 00:40:11,910
isn't degraded is measured in milliseconds

509
00:40:12,980 --> 00:40:17,850
an enormous exponential number increasing in the amount of decisions huge compression in the time

510
00:40:17,850 --> 00:40:19,080
we have available to them

511
00:40:19,090 --> 00:40:22,870
these problems can only be solved algorithmically

512
00:40:22,890 --> 00:40:25,180
this is a big shift in terms of the way

513
00:40:25,200 --> 00:40:28,900
media media buying occurred and it's the shift that we that is difficult for the

514
00:40:28,900 --> 00:40:34,760
industry to move through because they have such established processes but ultimately a significant portion

515
00:40:34,760 --> 00:40:37,520
you know i would wager majority proportion

516
00:40:37,700 --> 00:40:45,830
of all advertising dollars will actually be decided in real time algorithmically

517
00:40:45,850 --> 00:40:49,840
just to give you a feel for the online advertising ecosystem will start with the

518
00:40:49,870 --> 00:40:51,020
he entities

519
00:40:51,030 --> 00:40:52,620
all of the

520
00:40:52,630 --> 00:40:55,570
all of the inventory all of the audiences

521
00:40:55,580 --> 00:41:01,560
are created by publishers media companies that produce content and make their content available on

522
00:41:01,560 --> 00:41:06,410
their website typically the content is made available free of charge and the way in

523
00:41:06,410 --> 00:41:08,060
which these

524
00:41:08,080 --> 00:41:13,020
media companies are able to produce that content is of course by selling advertising their

525
00:41:13,130 --> 00:41:14,840
businesses that support

526
00:41:14,850 --> 00:41:16,400
all of the model

527
00:41:16,410 --> 00:41:21,850
in the advertising ecosystem flows from advertising markets people that have products they want to

528
00:41:21,850 --> 00:41:26,050
his out of work that it's just a question of time and more lost

529
00:41:26,070 --> 00:41:31,660
and it's it will happen so i think it's over really for human computer go

530
00:41:32,180 --> 00:41:37,300
but i like i'm trying to be strong to bear

531
00:41:37,330 --> 00:41:41,280
you can ask them to

532
00:41:41,300 --> 00:41:43,660
david about that

533
00:41:43,720 --> 00:41:48,120
OK so let's go to the last idea bootstrapping

534
00:41:48,120 --> 00:41:53,030
so bootstrapping i mean things like the bellman equation like temporal difference learning relating the

535
00:41:53,030 --> 00:41:55,930
value of state to state that falls may just when you thought i would start

536
00:41:55,930 --> 00:42:00,210
with i think it is a key a key idea so let's let's look at

537
00:42:00,210 --> 00:42:01,100
a little bit

538
00:42:01,180 --> 00:42:05,510
so we can basically think that is an alternative way of defining value

539
00:42:05,530 --> 00:42:07,870
so we can define value as i did before

540
00:42:07,870 --> 00:42:10,890
we call this extensive definition i don't know what to call this i want to

541
00:42:10,890 --> 00:42:15,310
contrast it with the sort of the bellman equation bootstrapping definition

542
00:42:15,330 --> 00:42:19,830
so this is the bellman equations says the values state should in expectation because the

543
00:42:19,830 --> 00:42:21,240
value the first reward

544
00:42:21,300 --> 00:42:26,760
because the discounted value for the next state OK that's the bellman equation and just

545
00:42:26,760 --> 00:42:29,700
and just think from all this is really a definition

546
00:42:29,720 --> 00:42:31,350
of what VE is

547
00:42:31,390 --> 00:42:36,030
this definition is a definition two of course same

548
00:42:37,870 --> 00:42:42,010
the same if we have them exactly solved but if we're going to approximate that

549
00:42:42,030 --> 00:42:43,580
they're not going to say

550
00:42:43,600 --> 00:42:48,390
and in everything about approximation we don't have to take this is the truth value

551
00:42:48,410 --> 00:42:50,810
we could take this is the true

552
00:42:50,830 --> 00:42:54,070
it's not really sense which one is true and the other is not true

553
00:42:54,120 --> 00:42:56,780
and but which one would be better

554
00:42:56,800 --> 00:43:00,120
and what will happen we think about which which might be better what might that

555
00:43:00,140 --> 00:43:06,410
we would be better to think of things of bootstrapping sensor earning extensive roll-out sense

556
00:43:06,430 --> 00:43:09,550
so one can see right away that the bootstrapping

557
00:43:09,600 --> 00:43:12,080
one way in which the bootstrapping things better

558
00:43:12,140 --> 00:43:16,080
it is if you can make an argument going to work with in in time

559
00:43:16,120 --> 00:43:17,080
it's better

560
00:43:17,100 --> 00:43:19,160
because to do this

561
00:43:19,180 --> 00:43:21,780
if you don't worry about learning

562
00:43:21,780 --> 00:43:25,100
but let's just worry about it verifying

563
00:43:25,120 --> 00:43:27,720
we have we have a theory about what the values how can we tell if

564
00:43:27,740 --> 00:43:33,070
that value that serious corrective our values are correct and just look at it over

565
00:43:33,070 --> 00:43:36,800
time so you think that the state has some number

566
00:43:36,830 --> 00:43:40,100
and now to see if it's correct you have to see the rewards that come

567
00:43:40,100 --> 00:43:42,800
and you have to see all of them out you know the still discount is

568
00:43:43,950 --> 00:43:47,200
so it's all incoming because you getting a different estimate every time you get all

569
00:43:47,200 --> 00:43:50,510
these estimates for every time you making them and then you for all those you

570
00:43:50,510 --> 00:43:54,530
have to look at all the future awards and scans make sure they line up

571
00:43:54,550 --> 00:43:57,280
it's it's awkward it's inconvenient

572
00:43:57,300 --> 00:44:01,870
was this thing you have you estimate the state u a one one step you

573
00:44:01,870 --> 00:44:05,410
see the reward you see next guess the same they're not the same

574
00:44:05,430 --> 00:44:08,430
and you done then you can you you repeat

575
00:44:08,450 --> 00:44:13,470
the news the new state it's much more convenient so that's one way of bootstrapping

576
00:44:13,720 --> 00:44:18,140
is better OK now let's look at a couple other things the other things happened

577
00:44:18,160 --> 00:44:20,810
with you know approximations

578
00:44:20,830 --> 00:44:22,160
it's also why

579
00:44:22,160 --> 00:44:27,080
one might values be approximately might be approximate because you've got the metadata

580
00:44:27,100 --> 00:44:31,050
there might be approximate because you have limited function approximation you only have so many

581
00:44:31,050 --> 00:44:35,220
parameters function approximation of course that's really going to happen all the time both is

582
00:44:35,220 --> 00:44:39,600
going to happen all the time axis state space the state space is

583
00:44:39,850 --> 00:44:47,370
unimaginably huge going to solve the potentiality not by getting the right value at every

584
00:44:47,370 --> 00:44:48,930
state because

585
00:44:48,950 --> 00:44:52,240
the state space is exponential

586
00:44:52,330 --> 00:44:56,660
but we're going to do it by by doing well without OK so let's look

587
00:44:56,660 --> 00:45:00,450
at these two first we might have to approximate because we have limited data so

588
00:45:00,450 --> 00:45:04,890
here's the classic reason why bootstrapping might be better

589
00:45:07,280 --> 00:45:11,660
it's an example i think this is game looking back at minor or we have

590
00:45:11,660 --> 00:45:15,030
stayed here which we think is good because we have lots of experience with that

591
00:45:15,300 --> 00:45:18,200
and ninety percent of the time in which that state go on to win

592
00:45:18,330 --> 00:45:21,120
and sometimes from we reach that state would going to last but most of the

593
00:45:21,830 --> 00:45:23,490
we go on to win

594
00:45:23,490 --> 00:45:27,550
so that's our past experience and we see a new state

595
00:45:27,600 --> 00:45:32,080
and we want to ask what value should it have well our experience with this

596
00:45:32,080 --> 00:45:36,050
new state is it goes on to the good state and then goes on this

597
00:45:36,050 --> 00:45:37,890
time to los

598
00:45:37,910 --> 00:45:42,970
what should we could conclude well

599
00:45:42,990 --> 00:45:45,600
excuse me

600
00:45:45,680 --> 00:45:46,990
excuse me

601
00:45:46,990 --> 00:45:49,850
j and

602
00:45:49,870 --> 00:45:54,450
once OK what value should the unknown state be given

603
00:45:54,660 --> 00:45:56,680
and we can use to our two things

604
00:45:56,680 --> 00:46:03,240
extensive approximation bootstrapping approx extensive approximation says OK unknown state i end up losing it

605
00:46:03,260 --> 00:46:05,330
returns were zero

606
00:46:05,350 --> 00:46:06,580
one minus one

607
00:46:06,580 --> 00:46:08,620
OK bad state

608
00:46:08,620 --> 00:46:13,310
bootstrapping definition says i saw the state took me to a good state by chance

609
00:46:13,310 --> 00:46:17,370
they this time i lost from that could stay but it needs that state basically

610
00:46:17,510 --> 00:46:18,660
it should be good

611
00:46:19,390 --> 00:46:23,600
so these are are different and i think we can imagine we can we can

612
00:46:23,600 --> 00:46:27,120
imagine cases where each each one of these answers would be would be

613
00:46:27,140 --> 00:46:29,410
would be better than one of these might be better

614
00:46:29,490 --> 00:46:34,080
but it in particular there are cases of the world's markov where this is

615
00:46:34,080 --> 00:46:36,950
this is the right conclusion bootstrapping is the right conclusion

616
00:46:37,010 --> 00:46:39,300
so that's why

617
00:46:39,310 --> 00:46:42,970
that's why we might want to approximate what that might be better

618
00:46:43,260 --> 00:46:48,740
bootstrapping definition so here's some empirical results this is old stuff but

619
00:46:48,780 --> 00:46:54,640
we have four different problems and we are varying the degree of bootstrapping this simple

620
00:46:54,640 --> 00:47:00,490
lambda parameter lambda is the books that were just applying size lambda

621
00:47:00,950 --> 00:47:05,780
the the y axis is a performance measure lower is better these are kind of

622
00:47:05,780 --> 00:47:09,600
like steps perhaps open mountain car

623
00:47:09,660 --> 00:47:14,600
what is the cost per episode in cardiff also in our case one to be

624
00:47:14,600 --> 00:47:19,640
low in all cases the right most data point the red data points

625
00:47:19,640 --> 00:47:24,080
islamic is one which has no bootstrapping so that's an extensive

626
00:47:24,100 --> 00:47:29,930
so the notion of accuracy and the leftmost endpoint is full bootstrapping

627
00:47:30,050 --> 00:47:34,760
and in between is in between so the striking thing and then this is users

628
00:47:34,760 --> 00:47:37,550
a different kind to different kinds of traits but so just look at one of

629
00:47:37,550 --> 00:47:42,850
these is is typical that we get a that's a good performance at what we

630
00:47:43,570 --> 00:47:45,030
for bootstrapping

631
00:47:45,050 --> 00:47:48,390
and there's there's an intermediate place where

632
00:47:48,410 --> 00:47:53,490
the the best degree bootstrap and but what happens if we go to one

633
00:47:54,350 --> 00:47:58,870
a full extensive definition with no bootstrapping is perform skits gets

634
00:47:58,890 --> 00:48:00,700
rapidly much worse

635
00:48:00,780 --> 00:48:01,780
much worse

636
00:48:01,780 --> 00:48:06,310
can't paul it can even have data point because it just doesn't happen it's too

637
00:48:07,430 --> 00:48:12,430
now this is these are all small problems but i think the representative of what

638
00:48:12,450 --> 00:48:14,070
what we think

639
00:48:14,070 --> 00:48:19,240
is the correct conclusion that bootstrapping in practice

640
00:48:19,260 --> 00:48:21,640
helps we to bootstrap at least a little bit

641
00:48:21,700 --> 00:48:24,180
so i think is the key idea reinforced

642
00:48:24,200 --> 00:48:28,240
OK second

643
00:48:28,260 --> 00:48:31,740
the reason we might want to approximate because we have limited function approximation

644
00:48:31,800 --> 00:48:37,100
so you don't have flexibility parameters can get exactly right all states this is we

645
00:48:37,100 --> 00:48:41,120
should consider this to be the normal case and so again we have to answer

646
00:48:41,120 --> 00:48:43,640
the question if we can get exact

647
00:48:43,640 --> 00:48:45,010
you know that

648
00:48:45,040 --> 00:48:48,120
usually computers pretty fast they

649
00:48:48,220 --> 00:48:51,080
operated by

650
00:48:51,100 --> 00:48:53,200
and variable white

651
00:48:53,200 --> 00:48:55,350
is example such calls

652
00:48:55,410 --> 00:48:57,950
and the idea is extremely simple

653
00:48:57,990 --> 00:48:59,390
and everybody

654
00:48:59,410 --> 00:49:01,970
you use one beat

655
00:49:02,020 --> 00:49:08,000
to indicate is the last byte of this number of this is not last but

656
00:49:08,000 --> 00:49:09,740
in this number

657
00:49:09,740 --> 00:49:14,160
so for example if you want to encode fifteen

658
00:49:14,200 --> 00:49:15,760
so we are using

659
00:49:15,780 --> 00:49:17,410
one by

660
00:49:17,430 --> 00:49:22,180
and they put zero in the last beat of this by to indicate that we

661
00:49:22,180 --> 00:49:24,660
in court everything in one

662
00:49:24,680 --> 00:49:27,310
in this in this number

663
00:49:28,720 --> 00:49:33,970
ft into into one but you use two bytes and videos topped

664
00:49:34,040 --> 00:49:37,740
largest paedophiles quite indicating that

665
00:49:37,760 --> 00:49:40,850
the next byte is also in use

666
00:49:40,930 --> 00:49:45,520
and the same situation is if you want to include the next night

667
00:49:45,580 --> 00:49:48,310
and this causes so-called

668
00:49:48,310 --> 00:49:53,060
then i actually went in court in ordinary text collection

669
00:49:53,080 --> 00:49:55,510
compression is very very

670
00:49:55,560 --> 00:50:01,830
close to what we can get with these the most optimise i never called or

671
00:50:01,830 --> 00:50:03,870
rice golomb code

672
00:50:03,870 --> 00:50:06,410
it's very very close the first

673
00:50:06,490 --> 00:50:10,100
second implementation is extremely

674
00:50:11,180 --> 00:50:15,330
you can write it in any programming language in one

675
00:50:15,390 --> 00:50:20,350
one hour is the bargaining unit testing and know stuff

676
00:50:20,450 --> 00:50:26,160
and it wasn't a lot of experiments that showed that the results of compression and

677
00:50:26,160 --> 00:50:29,660
file in this this very simple approach

678
00:50:29,700 --> 00:50:36,260
is maybe only a number of course and different from any types of wire compression

679
00:50:36,260 --> 00:50:38,390
that using beats

680
00:50:38,450 --> 00:50:40,160
so it's very good calls

681
00:50:40,160 --> 00:50:44,930
and i recommend you if you are developing search engines you this called because it

682
00:50:44,930 --> 00:50:47,760
satisfies all our demands

683
00:50:47,810 --> 00:50:53,810
and actually all open source project that we discuss not only most of them

684
00:50:53,850 --> 00:50:59,350
i using variable byte this and different material things yes

685
00:51:00,120 --> 00:51:02,310
it's very good call

686
00:51:02,330 --> 00:51:04,200
but sometimes

687
00:51:04,220 --> 00:51:08,120
in pretoria situations we need to compress better

688
00:51:08,160 --> 00:51:12,430
and actually we can do it we can compress better than all the previous calls

689
00:51:12,430 --> 00:51:15,510
but usually the loss performance

690
00:51:16,890 --> 00:51:17,910
i want to

691
00:51:17,930 --> 00:51:22,640
described this because this is interesting topic that you cannot read in most of books

692
00:51:22,640 --> 00:51:25,490
about information retrieval index

693
00:51:25,510 --> 00:51:28,450
so the idea is that

694
00:51:28,580 --> 00:51:33,490
we assume that we can compress better than all calls that we described before

695
00:51:33,560 --> 00:51:36,510
and the intuition behind this is

696
00:51:36,560 --> 00:51:38,410
if you have this forced

697
00:51:38,410 --> 00:51:43,220
posts ideas in this post this have ideas of documents

698
00:51:43,220 --> 00:51:47,100
in our previous approach to compression

699
00:51:47,280 --> 00:51:53,160
the was look if we're looking only on the previous number and called the

700
00:51:53,160 --> 00:51:59,990
maybe a better idea is to look not not only on the previous number

701
00:52:00,010 --> 00:52:03,120
but maybe we can take a block of numbers

702
00:52:03,140 --> 00:52:05,810
and if you have a lot of numbers

703
00:52:05,810 --> 00:52:10,330
we have we can we can get more statistics about this number

704
00:52:10,370 --> 00:52:14,470
number and based on the statistics can create better compression

705
00:52:14,470 --> 00:52:16,280
because we can look into

706
00:52:16,330 --> 00:52:19,120
into the future can compress better

707
00:52:19,140 --> 00:52:23,520
actually if you are talking about the idea of compression

708
00:52:23,580 --> 00:52:25,140
blog called it

709
00:52:25,140 --> 00:52:28,330
so block compression is a big area

710
00:52:29,290 --> 00:52:31,540
text it's well known

711
00:52:35,490 --> 00:52:37,450
for example

712
00:52:37,520 --> 00:52:43,850
but easy easy to run on unix compress that implementing this algorithm

713
00:52:43,870 --> 00:52:49,140
this rigorous and they the same so if you have a block of data we

714
00:52:49,140 --> 00:52:53,450
can look here can take stock uses from all these block and we can compress

715
00:52:53,450 --> 00:52:55,390
the for but

716
00:52:55,410 --> 00:52:59,020
so how we can use the according to here

717
00:52:59,100 --> 00:53:03,950
first of all we divide our least into some blog

718
00:53:03,950 --> 00:53:07,680
and then we can encode every block independently

719
00:53:07,700 --> 00:53:13,200
and for every this everyblock going to

720
00:53:14,310 --> 00:53:18,540
you use of questions will combine them together

721
00:53:18,540 --> 00:53:23,810
and how we can divide course there are two approach first approach we can simply

722
00:53:23,830 --> 00:53:27,680
get all always some fixed number of points for example we

723
00:53:27,700 --> 00:53:28,910
i can say that

724
00:53:30,390 --> 00:53:34,040
two hundred fifty five forcing is one block

725
00:53:34,060 --> 00:53:35,450
and become president

726
00:53:35,470 --> 00:53:41,290
it's usually better it's better approach implementing implementing compression

727
00:53:41,290 --> 00:53:42,960
OK you already have that

728
00:53:42,980 --> 00:53:45,360
OK small

729
00:53:45,390 --> 00:53:50,390
for all zeros for instance and long for a ridiculous things

730
00:53:50,450 --> 00:53:52,110
this definition is

731
00:53:52,110 --> 00:53:54,410
nearly independent of the choice

732
00:53:57,270 --> 00:54:07,380
yes that's right

733
00:54:07,420 --> 00:54:09,090
OK the sense

734
00:54:10,330 --> 00:54:11,930
the sense in which it is

735
00:54:11,950 --> 00:54:13,230
complex it

736
00:54:13,250 --> 00:54:19,100
it is normally not easy to find from the data the random number generator

737
00:54:23,100 --> 00:54:24,650
i won't go into that

738
00:54:26,050 --> 00:54:31,740
there are versions time bond versus common more complexity to take into account

739
00:54:31,760 --> 00:54:35,650
so that one there one of the great take into account complexity is to generate

740
00:54:35,650 --> 00:54:39,650
the sequence and there's one which takes into account how complex is fine

741
00:54:39,700 --> 00:54:42,380
the shortest

742
00:54:42,390 --> 00:54:48,160
because you here on the foundations you know i will

743
00:54:48,190 --> 00:54:52,840
mostly ignore computational issues so if you know computational issues of student more

744
00:54:52,850 --> 00:54:56,800
sequence is very simple i mean to figure out to number generator and then you

745
00:54:57,500 --> 00:54:58,760
the program

746
00:54:58,810 --> 00:55:02,690
but there is very little information

747
00:55:04,000 --> 00:55:06,460
OK it's nearly independent of the choice of u

748
00:55:06,510 --> 00:55:10,380
the first thing i could write a program in c or price programming lists

749
00:55:10,410 --> 00:55:12,050
and they get different answers

750
00:55:13,040 --> 00:55:16,880
if i have my short program written in c

751
00:55:16,890 --> 00:55:20,170
and i one of the list program what they can do is just right in

752
00:55:20,170 --> 00:55:23,600
enlist c interpreter or compiler

753
00:55:23,650 --> 00:55:27,710
and then append c programs so the novelist program

754
00:55:27,750 --> 00:55:30,940
which computes my sequence and the length of this program

755
00:55:31,080 --> 00:55:35,690
it's just the c program plus a little bit of overhead which is the compiler

756
00:55:35,940 --> 00:55:40,870
the overhead is independent of complex along the string is and this is what i

757
00:55:40,870 --> 00:55:42,910
mean nearly independent

758
00:55:43,850 --> 00:55:47,230
i think everything from

759
00:55:47,390 --> 00:55:50,140
so what you have is the k of you have access

760
00:55:50,180 --> 00:55:52,650
user compare two

761
00:55:53,830 --> 00:55:54,870
new prime

762
00:55:54,890 --> 00:55:58,350
so different universal language

763
00:55:59,460 --> 00:56:02,910
it is bounded by a constant that depends on the new york times

764
00:56:02,920 --> 00:56:08,710
what is independent of x

765
00:56:10,460 --> 00:56:13,150
normally what you write is

766
00:56:13,230 --> 00:56:17,350
then because additive constants appear everywhere discuss them

767
00:56:17,420 --> 00:56:19,080
very and a little bit

768
00:56:19,170 --> 00:56:21,460
right is equal to and you put plus

769
00:56:21,480 --> 00:56:24,460
that means equal apart from some constant

770
00:56:24,770 --> 00:56:26,620
these concepts are annoying

771
00:56:28,310 --> 00:56:31,710
some believe that because of this constant the whole series crash

772
00:56:36,170 --> 00:56:40,540
probably don't have time but in my book and in the book of leviticus standard

773
00:56:40,560 --> 00:56:46,060
of chemical complexity they find arguments for why you can live with the consequences of

774
00:56:46,080 --> 00:56:48,040
all they not

775
00:56:48,080 --> 00:56:50,210
many ships in many cases

776
00:56:51,540 --> 00:56:59,230
another thing is so k satisfies most properties informationmeasures satisfy in particular

777
00:56:59,560 --> 00:57:02,890
the shannon entropy which probably about

778
00:57:02,960 --> 00:57:05,980
which also makes the information content in

779
00:57:06,000 --> 00:57:08,600
in in in sequences

780
00:57:08,640 --> 00:57:12,330
and it's actually closely related so many of the theorems

781
00:57:12,330 --> 00:57:15,270
probably most theorems in shannon entropy have

782
00:57:15,290 --> 00:57:18,850
barry on a lot in common complexity

783
00:57:18,920 --> 00:57:21,980
but the kolmogorov complexity is superior to shannon entropy

784
00:57:22,040 --> 00:57:26,000
in various ways for instance shannon entropy will never catch

785
00:57:26,040 --> 00:57:30,460
the regularity in the data is of high

786
00:57:32,710 --> 00:57:36,830
OK that was the greatest i mean one of the bodies of kolmogorov complexity that

787
00:57:37,730 --> 00:57:40,480
all regularities well effective regulators

788
00:57:40,480 --> 00:57:43,230
the major disadvantage is that it's not computable

789
00:57:43,270 --> 00:57:46,020
but the purely theoretical concepts

790
00:57:46,080 --> 00:57:48,350
you can prove nice theorems

791
00:57:48,390 --> 00:57:51,600
but if you want to do something in practice you have to proximity somehow

792
00:57:51,600 --> 00:57:55,020
and ever come to that later

793
00:57:55,080 --> 00:58:00,020
OK i should mention i mean there are many mathematical constructs which is so high

794
00:58:00,020 --> 00:58:03,000
in the arithmetical hierarchy that this sort of you know

795
00:58:04,440 --> 00:58:05,830
you know

796
00:58:05,850 --> 00:58:07,060
anything reasonable

797
00:58:07,080 --> 00:58:10,690
but this is actually quite low i mean this list and sigma one so you

798
00:58:10,690 --> 00:58:15,420
can compute computed from above approximate from about future is the number of programmes let

799
00:58:15,420 --> 00:58:19,440
them run this one as some programs stops to look for the compute right answer

800
00:58:19,460 --> 00:58:22,960
and if yes progress of mankind a bit shorter than the previous one to keep

801
00:58:22,980 --> 00:58:26,980
it can approximate this function from above

802
00:58:27,020 --> 00:58:31,250
unfortunately if you do it and there's no way you can also show that the

803
00:58:31,250 --> 00:58:34,270
convergence is

804
00:58:34,310 --> 00:58:36,890
you write on any anything

805
00:58:36,940 --> 00:58:39,580
computable functions

806
00:58:39,640 --> 00:58:42,900
which is supposed to be the convergence rate and then you can show that convergence

807
00:58:42,900 --> 00:58:50,170
slower than the function so so it's not not a very good way to approximate

808
00:58:51,710 --> 00:58:53,620
OK that was the one slide

809
00:58:53,690 --> 00:58:56,270
introduction to come work complexity

810
00:58:57,690 --> 00:59:02,420
the summary is that it is an excellent universal complexity measure and if you come

811
00:59:02,420 --> 00:59:05,350
across the country

812
00:59:05,350 --> 00:59:06,580
OK so

813
00:59:07,370 --> 00:59:11,440
i can't compute it i can plot a graph of it

814
00:59:13,310 --> 00:59:16,620
expected graph sketch of this function

815
00:59:16,640 --> 00:59:20,620
so first what you have is i mean your x

816
00:59:20,620 --> 00:59:23,690
and i identify you natural numbers with

817
00:59:23,730 --> 00:59:25,080
the strings

818
00:59:26,000 --> 00:59:28,620
a natural number x

819
00:59:28,670 --> 00:59:31,020
i can cope with log x bits

820
00:59:31,080 --> 00:59:32,960
you all know that

821
00:59:33,670 --> 00:59:36,330
and kolmogorov complexity the shortest code

822
00:59:36,330 --> 00:59:38,830
so it can't be larger than log

823
00:59:38,890 --> 00:59:42,980
it is a little bit larger because they have to talk about the coat i

824
00:59:42,980 --> 00:59:45,850
don't want to go into that so you can show that is bounded by log

825
00:59:45,870 --> 00:59:50,980
explores two log log x sort of marking the end of the of the bits

826
00:59:51,020 --> 00:59:53,710
OK so it's always smaller than that

827
00:59:53,790 --> 00:59:55,600
and then OK so

828
00:59:55,620 --> 01:00:01,520
asymptotic killing you can also show that must be larger than log x foremost text

829
01:00:03,830 --> 01:00:06,440
it's roughly because you cannot have i mean

830
01:00:06,480 --> 01:00:10,290
if you have a class things not all of these things can have short called

831
01:00:10,290 --> 01:00:12,850
most of them need to have long long called

832
01:00:12,870 --> 01:00:14,830
so there's a lot about you

833
01:00:14,850 --> 01:00:18,270
but occasionally function drop down if x is very simple

834
01:00:18,330 --> 01:00:20,520
so for instance if x is

835
01:00:20,580 --> 01:00:24,810
in talking about numbers now is two to the power to to the power of

836
01:00:26,440 --> 01:00:30,100
a large number but has a very simple description was brought down

837
01:00:30,150 --> 01:00:34,580
so then in these cases kolmogorov complexity dogs

838
01:00:34,620 --> 01:00:40,350
well that's roughly how it looks like

839
01:00:40,370 --> 01:00:43,640
you can even prove some solo

840
01:00:43,650 --> 01:00:48,330
continuity for this common complexity which is quite interesting so it is in a certain

841
01:00:48,330 --> 01:00:50,500
sense are continuous functions

842
01:00:50,560 --> 01:00:52,770
apart from additive constants

843
01:00:52,790 --> 01:00:53,910
so there's a lot of

844
01:00:53,920 --> 01:00:56,650
very beautiful theorems and useful theorems onto

845
01:00:56,690 --> 01:00:59,520
and at least one i will

846
01:00:59,730 --> 01:01:01,850
exploit later

847
01:01:01,920 --> 01:01:04,750
so now we have quantified complexity

848
01:01:04,770 --> 01:01:08,460
four strings and we have some coding for other objects

849
01:01:09,330 --> 01:01:10,460
what we need now

850
01:01:10,480 --> 01:01:15,370
we started this hypothesis and i want a prior over the set policies occam's razor

851
01:01:15,370 --> 01:01:19,710
tells me that the price depends on of the complexity of this is

852
01:01:19,730 --> 01:01:24,540
so the complexity of this hypothesis that can measure commonwealth complexity so remember these are

853
01:01:24,540 --> 01:01:28,390
probability distributions so these functions

854
01:01:28,480 --> 01:01:33,120
but the kolmogorov complexity of function is simply i mean the shortest programme which computes

855
01:01:33,250 --> 01:01:34,690
this function

856
01:01:38,080 --> 01:01:42,170
and what you can show is the right way to define the prior is now

857
01:01:42,190 --> 01:01:45,270
as a tool to the mines the kolmogorov complexity

858
01:01:45,270 --> 01:01:48,770
so what does it mean if you have a policies which is very simple so

859
01:01:48,770 --> 01:01:52,940
probability distribution is extremely simple in the short program

860
01:01:52,980 --> 01:01:54,040
let's compute

861
01:01:54,100 --> 01:01:56,790
you know the probabilities of samples from this distribution

862
01:01:56,830 --> 01:02:00,490
so this would be a small number two to the miners came to be a

863
01:02:00,490 --> 01:02:02,230
this like technology triggers

864
01:02:03,510 --> 01:02:05,310
so here we have everything from

865
01:02:06,660 --> 01:02:11,350
quantum computing to all the most hype stuff apparently these days would be

866
01:02:13,040 --> 01:02:17,570
cloud augmented reality which is going out social analytics and so on so big data

867
01:02:17,580 --> 01:02:20,860
is coming according to gartner going up

868
01:02:23,950 --> 01:02:27,800
what's interesting so here we have also different signals symbols

869
01:02:28,480 --> 01:02:29,970
the triangles which are

870
01:02:30,880 --> 01:02:32,500
like ten years to the markets

871
01:02:33,630 --> 01:02:36,070
are ten years to the sort of maturity but

872
01:02:38,240 --> 01:02:39,060
that's a big data

873
01:02:40,490 --> 01:02:41,670
has a circle so it's

874
01:02:42,390 --> 01:02:44,000
only two to five so it's really

875
01:02:44,500 --> 01:02:49,950
upcoming technologies so which will have an impact pretty soon not as much as

876
01:02:50,380 --> 01:02:51,040
maybe some other

877
01:02:54,330 --> 01:02:55,620
again this is just another view

878
01:02:56,960 --> 01:02:59,690
putting big data in the context of other technologies which are

879
01:03:00,300 --> 01:03:00,450
out there

880
01:03:02,070 --> 01:03:03,040
okay what are

881
01:03:03,680 --> 01:03:04,490
key enablers

882
01:03:05,120 --> 01:03:05,790
for big data

883
01:03:07,810 --> 01:03:10,380
all three i guess are obvious but still

884
01:03:11,260 --> 01:03:11,970
let's mention them

885
01:03:12,370 --> 01:03:14,500
so one is increase of storage capacities

886
01:03:15,470 --> 01:03:20,040
increase of processing power and availability of data so without

887
01:03:20,670 --> 01:03:23,510
any of these three big data wouldn't be possible in such a form

888
01:03:24,040 --> 01:03:25,350
as we are discussing these days

889
01:03:27,150 --> 01:03:30,970
ok, let's check each of those and

890
01:03:33,300 --> 01:03:34,870
so here we are in

891
01:03:35,360 --> 01:03:37,040
1986 basically

892
01:03:37,610 --> 01:03:40,790
back then there's almost no data

893
01:03:42,360 --> 01:03:44,050
we didn't have capacity really

894
01:03:44,990 --> 01:03:48,110
but now this just exploded in the last ten years

895
01:03:49,000 --> 01:03:49,490
is going up

896
01:03:51,120 --> 01:03:51,640
another view

897
01:03:52,670 --> 01:03:53,880
it's more like relative

898
01:03:54,440 --> 01:03:57,110
comparing analog versus digital data

899
01:03:57,620 --> 01:03:59,720
see back in 1986 it was

900
01:04:00,140 --> 01:04:03,300
ninety nine percent everything was still analog so we had just this tiny

901
01:04:03,890 --> 01:04:05,890
little disks so two hundred

902
01:04:06,990 --> 01:04:07,860
fifty K

903
01:04:10,860 --> 01:04:12,050
those guys remember

904
01:04:13,690 --> 01:04:14,480
and then this was

905
01:04:15,120 --> 01:04:16,570
growing growing growing and now

906
01:04:16,750 --> 01:04:17,950
nowadays analog

907
01:04:18,470 --> 01:04:19,090
part is

908
01:04:19,580 --> 01:04:23,520
definitely ways smaller and everything is digital so this certainly is one

909
01:04:25,820 --> 01:04:26,670
important enabler

910
01:04:35,930 --> 01:04:36,770
exponential curve

911
01:04:37,680 --> 01:04:39,620
almost nothing back then this

912
01:04:40,470 --> 01:04:44,620
so called we didn't even use supercomputers back then but this huge

913
01:04:45,320 --> 01:04:46,590
server rooms are

914
01:04:47,140 --> 01:04:48,580
even the word server was not there

915
01:04:49,180 --> 01:04:50,080
back then so

916
01:04:50,710 --> 01:04:53,160
these huge computers are

917
01:04:53,520 --> 01:04:55,960
were as powerful as mobile phones nowadays

918
01:04:56,470 --> 01:04:57,460
and now this went up

919
01:04:57,890 --> 01:04:59,460
now it's interesting if

920
01:05:01,230 --> 01:05:03,270
we check distribution of computing power

921
01:05:05,400 --> 01:05:07,410
different types of devices so we have

922
01:05:08,500 --> 01:05:12,670
pocket calculator calculators which were well based on the amount of them

923
01:05:13,340 --> 01:05:15,720
back then they are pretty significant

924
01:05:18,420 --> 01:05:23,530
servers mobile phones video games and personal computers and see how these proportions are

925
01:05:24,420 --> 01:05:25,110
changing so

926
01:05:25,840 --> 01:05:27,120
personal computers are

927
01:05:28,960 --> 01:05:30,000
pretty big still

928
01:05:30,690 --> 01:05:32,420
but obviously there are in decline

929
01:05:33,070 --> 01:05:34,050
so this would be

930
01:05:37,060 --> 01:05:37,840
clouds I guess

931
01:05:39,990 --> 01:05:41,530
mobile is moving

932
01:05:43,290 --> 01:05:47,260
or maybe I miss some of the colours basically these distributions are changing and

933
01:05:48,330 --> 01:05:49,660
it's important somehow to

934
01:05:50,660 --> 01:05:51,980
to to be aware that

935
01:05:53,630 --> 01:05:57,950
this is growing can also types are types of machinery which we're using nowadays are

936
01:06:01,710 --> 01:06:03,690
data availability was the search enabler

937
01:06:05,680 --> 01:06:06,900
here's an interesting graph

938
01:06:07,620 --> 01:06:12,690
so all these graphs I am taken from mckinsey report which I will reference which you can

939
01:06:12,690 --> 01:06:14,490
get for free from the uh web

940
01:06:16,230 --> 01:06:16,800
so these are

941
01:06:18,750 --> 01:06:19,590
industry sectors

942
01:06:21,800 --> 01:06:22,210
so this is

943
01:06:22,690 --> 01:06:26,710
and the estimated stored data in 2009 in

944
01:06:27,100 --> 01:06:28,170
petabytes per

945
01:06:29,020 --> 01:06:29,930
sector so

946
01:06:31,110 --> 01:06:31,750
see we have

947
01:06:32,270 --> 01:06:33,600
manufacturing government

948
01:06:34,080 --> 01:06:36,760
communication media and so on banking would be here

949
01:06:37,230 --> 01:06:38,120
and is dropping down

950
01:06:38,960 --> 01:06:39,600
what's interesting

951
01:06:42,190 --> 01:06:42,910
how much data

952
01:06:43,920 --> 01:06:45,270
is stored per company

953
01:06:46,870 --> 01:06:49,200
per company with more than thousand employees

954
01:06:49,590 --> 01:06:50,930
in 2009 you see that

955
01:06:52,410 --> 01:06:53,830
it's not unusual to have

956
01:06:54,870 --> 01:06:56,870
most of the companies would have somewhere in the

957
01:06:56,870 --> 01:07:00,790
so unless there's other questions on move on to the second part

958
01:07:15,540 --> 01:07:23,370
well we treat the two projects pretty independently and then that should be

959
01:07:23,390 --> 01:07:25,650
more men

960
01:07:27,070 --> 01:07:34,330
that we we do them completely independently we take the method and we get data

961
01:07:34,330 --> 01:07:35,990
from the medical setting and we

962
01:07:36,000 --> 01:07:41,320
learn the policy estimation the errors for that setting and completely separately we do

963
01:07:41,360 --> 01:07:48,500
estimation method is the same but otherwise the models and estimation are completely separate

964
01:07:48,630 --> 01:07:51,370
OK so the search for the second part

965
01:07:51,390 --> 01:07:54,330
is a little bit different

966
01:07:54,420 --> 01:07:57,140
we're going to complicate things into is

967
01:07:57,160 --> 01:08:02,120
the first one that we assume that we don't have the state labels

968
01:08:02,130 --> 01:08:04,330
this is a lot more realistic

969
01:08:05,550 --> 01:08:07,130
and the second one

970
01:08:07,160 --> 01:08:11,860
is that we're going to assume we have control over how the data is collected

971
01:08:11,880 --> 01:08:16,610
a lot of robotics domain this is perfectly fine in the medical domain this is

972
01:08:16,610 --> 01:08:17,740
not fine

973
01:08:17,750 --> 01:08:22,010
in terms of certainly no one is offering to let me run randomized clinical trial

974
01:08:22,010 --> 01:08:26,490
anytime soon and even in the epilepsy case i'm not going to collect data on

975
01:08:26,500 --> 01:08:31,670
humans but as i mentioned for epilepsy we have a nice rat model where we

976
01:08:31,670 --> 01:08:36,850
have collaborators which allow us to come in with our time DP based decision maker

977
01:08:36,850 --> 01:08:41,530
and plug it into the rat brain and let us collect these there however we

978
01:08:41,530 --> 01:08:45,760
want and then we doing this sort of on a bi-weekly basis so we have

979
01:08:45,760 --> 01:08:49,770
this kind of set up right now where we don't have the state labels we

980
01:08:49,770 --> 01:08:54,150
really don't know what's the state of the neurons any time in point or even

981
01:08:54,150 --> 01:08:57,210
some indicators of epilepsy

982
01:08:57,230 --> 01:09:00,920
but we do get control over how the data is collected this is both

983
01:09:00,940 --> 01:09:04,110
a blessing and a curse it's great that we have to control but we need

984
01:09:04,110 --> 01:09:07,530
to use it wisely we need to take action to make sense otherwise

985
01:09:07,570 --> 01:09:11,980
the point of having that control

986
01:09:13,130 --> 01:09:15,160
the online case

987
01:09:16,570 --> 01:09:20,970
well also going to add up to bayesian framework in this case rather than frequentist

988
01:09:21,190 --> 01:09:23,620
framework and the

989
01:09:23,680 --> 01:09:25,910
general bayesian framework is

990
01:09:25,920 --> 01:09:27,860
i'm sure familiar to everyone here

991
01:09:27,880 --> 01:09:32,920
we assume we have some prior distribution over are unknown parameters

992
01:09:32,940 --> 01:09:37,280
so we still assume that the model of the transitions in the observation is not

993
01:09:37,280 --> 01:09:42,440
known a priori but we have some notion of what might be good parameters

994
01:09:42,460 --> 01:09:47,240
we're going to be the posterior via bayes rule as experience is acquired and the

995
01:09:47,240 --> 01:09:48,980
more interesting part is

996
01:09:48,990 --> 01:09:53,210
how do we pick our actions with respect to the posterior

997
01:09:53,480 --> 01:09:57,050
so standard bayesian framework but in the probably be setting

998
01:09:57,190 --> 01:10:01,770
and the advantage from our perspective is that it gives us a nice way to

999
01:10:01,770 --> 01:10:05,010
include prior knowledge explicitly

1000
01:10:05,020 --> 01:10:10,460
the epilepsy domain we really have very little prior knowledge so we pick fairy agnostic

1001
01:10:10,470 --> 01:10:15,820
priors in the dialogue management problem we have better priors we have some sense of

1002
01:10:15,830 --> 01:10:20,430
when the robot says something the person is going to reply something specific but there's

1003
01:10:20,430 --> 01:10:23,710
some noise factor but we still have some sense of what the model might look

1004
01:10:25,540 --> 01:10:28,130
so this framework lets us include that knowledge

1005
01:10:28,190 --> 01:10:32,790
it lets us perform learning as necessary because when the online setting where we get

1006
01:10:32,790 --> 01:10:37,380
to pick actions so we get the advantage of that in the last point which

1007
01:10:37,380 --> 01:10:43,290
is one that i think is useful really allows us to explicitly consider the uncertainty

1008
01:10:43,310 --> 01:10:46,170
during planning to take actions

1009
01:10:46,230 --> 01:10:48,270
yes that will give us good foreward

1010
01:10:48,290 --> 01:10:51,520
and yes i will try to figure out what the state of the system is

1011
01:10:51,520 --> 01:10:55,770
but in particular that will trade-off between when i need to learn more about my

1012
01:10:55,770 --> 01:10:59,360
model one they need to be pretty confident about what i know and pick the

1013
01:10:59,360 --> 01:11:03,500
strategies which i think are good

1014
01:11:03,520 --> 01:11:08,150
so just a quick recall for those of you with a short memory post food

1015
01:11:08,150 --> 01:11:12,420
induced coma about the problem the model the same set up as before

1016
01:11:12,460 --> 01:11:16,540
and the question here is really how should we choose actions if parameters

1017
01:11:16,590 --> 01:11:19,000
t and o are uncertain

1018
01:11:19,020 --> 01:11:22,900
we assume for the purpose of today's talk about the reward function is known

1019
01:11:22,920 --> 01:11:25,960
i did that for the first half i'm doing that again

1020
01:11:27,110 --> 01:11:31,000
techniques can be generalized quite easily to the case where the reward is not

1021
01:11:31,020 --> 01:11:36,560
known but it keeps things cleaner and simpler for today and so on

1022
01:11:36,590 --> 01:11:42,610
this notion of bayesian learning has been explored in the MDP set up in the

1023
01:11:42,610 --> 01:11:45,250
case where you know the state of the system

1024
01:11:46,750 --> 01:11:51,460
but after a lot of work in france

1025
01:11:51,480 --> 01:11:55,480
and that set up the idea is quite simple it's a little bit like the

1026
01:11:55,480 --> 01:11:59,040
frequentist idea you can the number of times

1027
01:11:59,060 --> 01:12:04,420
but you transition from state s to s five upon taking action a

1028
01:12:04,440 --> 01:12:09,170
so you just counting as we did before but you start from some prior

1029
01:12:09,190 --> 01:12:13,520
and that prior you can think about imagined experiences that i'm pretty sure that transition

1030
01:12:13,520 --> 01:12:16,860
is going to happen it would be like seeing it ten times and then also

1031
01:12:16,860 --> 01:12:19,880
counting how many more times i see it

1032
01:12:19,920 --> 01:12:23,860
so these counts define the dirichlet distribution

1033
01:12:24,000 --> 01:12:28,810
over the transition model assumes that people are familiar with the dirichlet distribution

1034
01:12:28,840 --> 01:12:32,980
the good thing is that the nice closed

1035
01:12:32,980 --> 01:12:38,940
closed form update for upon seeing the information and so these counts

1036
01:12:38,960 --> 01:12:42,790
are sufficient statistics for defining the dirichlet

1037
01:12:42,810 --> 01:12:46,790
posterior so it's easy to update and now all we need to do is planned

1038
01:12:48,440 --> 01:12:49,940
these counts

1039
01:12:49,960 --> 01:12:52,590
so you can formulate a new MDP model

1040
01:12:53,210 --> 01:12:56,460
the state space is defined as the original physical state

1041
01:12:56,500 --> 01:13:00,210
and the information state meaning or count vectors

1042
01:13:00,230 --> 01:13:05,380
and now the probability depends on both being in state having a certain count

1043
01:13:05,400 --> 01:13:10,210
seeing a certain action will take you to a new state and set of counts

1044
01:13:10,210 --> 01:13:12,230
so the count the count part

1045
01:13:12,250 --> 01:13:15,690
is deterministic and the state to state part

1046
01:13:15,710 --> 01:13:17,880
can be probabilistic

1047
01:13:18,880 --> 01:13:22,440
so once you've set up this extended MDP

1048
01:13:22,440 --> 01:13:27,310
can solve it just to standard MDP accounts discrete here stated discrete you can run

1049
01:13:27,310 --> 01:13:32,570
your favourite MDP solver and things are all well and good

1050
01:13:32,610 --> 01:13:34,340
in finite pomdps

1051
01:13:34,380 --> 01:13:37,880
we can extend the same framework

1052
01:13:37,900 --> 01:13:40,840
instead of having only counts over transition

1053
01:13:40,840 --> 01:13:43,330
now we need the help over transition

1054
01:13:43,330 --> 01:13:47,700
i wrote the third chapter of the book and the book that was you going

1055
01:13:48,540 --> 01:13:50,960
it was written by somebody else

1056
01:13:50,990 --> 01:13:57,300
basically because we are trying to understand it and then to construct my bit

1057
01:13:57,310 --> 01:14:00,310
but initially we try to get over what the

1058
01:14:00,330 --> 01:14:02,830
basic ideas and statistics are

1059
01:14:02,850 --> 01:14:05,810
have statisticians approach data analysis problems

1060
01:14:05,830 --> 01:14:11,920
and then we look in the second half in chapter three it will be

1061
01:14:11,940 --> 01:14:18,530
about specific techniques that statisticians use specific models use specific approaches

1062
01:14:18,980 --> 01:14:20,960
so so first is more

1063
01:14:20,990 --> 01:14:24,850
theoretical and the second will be more

1064
01:14:24,860 --> 01:14:27,280
what people do

1065
01:14:28,130 --> 01:14:29,880
figure four

1066
01:14:31,170 --> 01:14:34,170
the point is

1067
01:14:35,340 --> 01:14:37,990
she is

1068
01:14:38,030 --> 01:14:39,130
well we have

1069
01:14:39,160 --> 01:14:40,130
one of the

1070
01:14:40,170 --> 01:14:45,060
section numbers here you find the section numbers don't make sense in terms of

1071
01:14:45,060 --> 01:14:48,660
the individual the talk section was taken from the book

1072
01:14:48,670 --> 01:14:52,360
so if i missed the section of the book out then that section

1073
01:14:52,410 --> 01:14:54,190
in this slide

1074
01:14:54,210 --> 01:14:55,240
OK so

1075
01:14:55,250 --> 01:14:59,520
a quick skim over what had introducing what was introduced and was one of the

1076
01:14:59,520 --> 01:15:04,560
basic statistical ideas the details of this are in the book basically and that's what

1077
01:15:04,560 --> 01:15:08,660
i'm assuming you have access to some stage

1078
01:15:08,670 --> 01:15:10,350
so i would take

1079
01:15:11,210 --> 01:15:15,660
the fundamental idea is statistics is that we think of uncertainty

1080
01:15:15,670 --> 01:15:23,170
as based on a probability problems but if you can explain in terms of exactly

1081
01:15:23,170 --> 01:15:24,800
how it works

1082
01:15:24,830 --> 01:15:26,190
we will

1083
01:15:26,220 --> 01:15:28,470
think of it as

1084
01:15:28,520 --> 01:15:32,720
it's just random chance so probability is an integral

1085
01:15:32,720 --> 01:15:34,950
element statistics because what we're doing

1086
01:15:34,970 --> 01:15:39,890
in statistics is well we did probability say this is how the world is what

1087
01:15:39,890 --> 01:15:43,990
should happen if i do such and such a thing to do was some sort

1088
01:15:45,220 --> 01:15:48,420
what would you statistics go the other way and say

1089
01:15:48,440 --> 01:15:51,820
this is what i did this is one of the data i got what does

1090
01:15:51,820 --> 01:15:57,150
that mean about the world so so one is what is called induction and is

1091
01:15:57,170 --> 01:16:00,040
what i called the deduction

1092
01:16:00,170 --> 01:16:05,710
right so source the simplest thing about probability is

1093
01:16:05,770 --> 01:16:10,860
was called the random experiment or statistical experiment and it's

1094
01:16:10,890 --> 01:16:13,950
as a trial so we do something

1095
01:16:13,960 --> 01:16:19,170
and the result is random so we've got examples here but you must see before

1096
01:16:19,170 --> 01:16:21,200
rolling the die

1097
01:16:21,270 --> 01:16:22,930
and what's wrong with this case

1098
01:16:24,420 --> 01:16:30,370
so rolling the die is one thing of course so in this case and i

1099
01:16:30,390 --> 01:16:34,730
got six possible outcomes are possible results and

1100
01:16:34,770 --> 01:16:36,370
the random editor

1101
01:16:36,390 --> 01:16:38,770
the problem

1102
01:16:42,420 --> 01:16:44,120
we're going to stand

1103
01:16:55,830 --> 01:17:02,080
so so one of the world's largest an example we're rolling the die

1104
01:17:02,140 --> 01:17:03,890
and you've got six

1105
01:17:03,890 --> 01:17:08,670
possibilities and they're all equally likely and the result you don't know what the results

1106
01:17:08,670 --> 01:17:12,830
going to be is randomly determined we don't know

1107
01:17:12,860 --> 01:17:16,570
if you have a a lot of physics and you could observe the di very

1108
01:17:16,570 --> 01:17:17,680
very closely

1109
01:17:17,700 --> 01:17:20,610
you might be able to work out was going to happen but that we're assuming

1110
01:17:20,860 --> 01:17:25,320
that we haven't got that sort of information and it's pure chance that turns it

1111
01:17:25,610 --> 01:17:30,080
is the same tossing the columns you've got two comes to a head or tail

1112
01:17:30,110 --> 01:17:33,370
and again it's totally determined

1113
01:17:33,400 --> 01:17:38,070
statisticians use a bit further from that this is the origin of probability statistics that

1114
01:17:38,070 --> 01:17:41,840
gambling games basically people

1115
01:17:41,900 --> 01:17:46,890
employed people mathematicians statisticians to

1116
01:17:46,890 --> 01:17:52,180
i will have rowed in the game against richmond who wanted to gamble pays people

1117
01:17:52,180 --> 01:17:58,330
like me to work out the probabilities of various things happening a long time ago

1118
01:17:58,370 --> 01:18:04,750
but statisticians take this further take it will be on the government going to say

1119
01:18:04,750 --> 01:18:06,390
something like that

1120
01:18:06,410 --> 01:18:12,220
this example here which you see what chosen an example later but

1121
01:18:12,250 --> 01:18:13,060
if you

1122
01:18:13,070 --> 01:18:16,770
so something biological you plant a field turnips

1123
01:18:16,770 --> 01:18:20,600
you find the turnips will grow differently

1124
01:18:20,620 --> 01:18:24,900
and you find if you do the same thing with the turnips on two different

1125
01:18:24,900 --> 01:18:29,950
occasions you get different results so there's a random element to that as well so

1126
01:18:30,410 --> 01:18:31,980
so the yield

1127
01:18:32,060 --> 01:18:35,080
the weight of the term it comes that will move

1128
01:18:36,710 --> 01:18:44,640
and the results looking yet we about the call those outcomes or events sometimes and

1129
01:18:44,640 --> 01:18:45,460
this just a

1130
01:18:45,500 --> 01:18:48,770
in some notation all probability this is one

1131
01:18:48,780 --> 01:18:55,810
there very few politicians one view probability based on the frequentist view

1132
01:18:55,820 --> 01:18:59,180
you do lots of these trials

1133
01:18:59,200 --> 01:18:59,840
in fact

1134
01:18:59,870 --> 01:19:01,010
you do

1135
01:19:01,060 --> 01:19:02,580
and trials

1136
01:19:02,600 --> 01:19:11,120
and we have an outcome results of the results of this event the here a

1137
01:19:11,210 --> 01:19:16,140
it might be something like the score on the die is even

1138
01:19:16,160 --> 01:19:21,960
for example for trivial example of so what's the probability that happening well

1139
01:19:21,970 --> 01:19:26,840
we can work out it's just half but in

1140
01:19:26,850 --> 01:19:30,310
and on not the

1141
01:19:30,310 --> 01:19:36,720
and this indicates once again that somehow one's exposure to infectious agent historical exposure has

1142
01:19:36,720 --> 01:19:42,540
a dramatic effect on one subsequent susceptibility to the

1143
01:19:42,560 --> 01:19:47,690
the two a virus and in the case of polio virus we're dealing here with

1144
01:19:47,690 --> 01:19:48,870
an agent

1145
01:19:48,960 --> 01:19:54,860
which is much simpler than the retroviruses we talked about last time like RSV poliovirus

1146
01:19:54,860 --> 01:19:58,030
also has a single stranded RNA genome

1147
01:19:58,040 --> 01:20:02,180
and that encapsulated in a protein asia's cold

1148
01:20:02,190 --> 01:20:07,100
which is made up of only of viral proteins so it's very simple single stranded

1149
01:20:09,090 --> 01:20:11,670
protein h is called the single stranded RNA

1150
01:20:11,680 --> 01:20:15,880
it is actually polyadenylation at the three prime and so it can serve as a

1151
01:20:15,880 --> 01:20:17,550
messenger RNA

1152
01:20:17,560 --> 01:20:21,970
it's all the same polarity that is it has a plus polarity which means that

1153
01:20:21,970 --> 01:20:24,170
it can be translated immediately

1154
01:20:24,190 --> 01:20:28,390
i'm distinguishing that from the other flavors single stranded RNA which could be of the

1155
01:20:28,390 --> 01:20:33,850
complementary strand which could exist because one can make double stranded RNA which obviously cannot

1156
01:20:33,850 --> 01:20:39,590
be translated so this can serve as a messenger RNA and just as an aside

1157
01:20:39,600 --> 01:20:44,750
the way the polio virus replicates is totally different from that of retroviruses what happens

1158
01:20:44,750 --> 01:20:50,050
is the polio virus makes its own plimer is you go from single stranded RNA

1159
01:20:50,070 --> 01:20:53,290
two double stranded RNA

1160
01:20:53,300 --> 01:20:58,410
i e the complementary copy is is made and that in turn is used as

1161
01:20:58,410 --> 01:21:01,540
a template for making more single stranded RNA

1162
01:21:01,560 --> 01:21:03,030
progeny RNA

1163
01:21:03,050 --> 01:21:07,160
and note here that there is no DNA and all involved in fact poliovirus can

1164
01:21:07,160 --> 01:21:12,400
grow inside the cell that has been deprived of its nucleus moreover if one stops

1165
01:21:12,430 --> 01:21:17,220
nuclear RNA nuclear DNA dependent transcription that has no effect on this

1166
01:21:17,270 --> 01:21:19,680
but you'll you'll notice correctly

1167
01:21:19,690 --> 01:21:22,240
the these these kind of steps here

1168
01:21:22,260 --> 01:21:25,930
involved RNA dependent RNA polymerase is

1169
01:21:25,950 --> 01:21:28,370
and in the normal physiology cells

1170
01:21:28,390 --> 01:21:34,750
that such a plumber such an enzyme never operates and therefore poliovirus must make among

1171
01:21:34,750 --> 01:21:40,370
its other proteins and RNA dependent RNA polymerase that can mediate the steps making a

1172
01:21:40,370 --> 01:21:45,540
complimentary copy of this strand get to get to the complementary strand in the making

1173
01:21:45,740 --> 01:21:48,770
progeny plus strand RNA is

1174
01:21:48,820 --> 01:21:53,980
again it has a very simple capsid of several viral proteins which rapid

1175
01:21:54,870 --> 01:21:56,640
now if one wants to assess

1176
01:21:56,650 --> 01:22:00,600
the potency of poliovirus RNA one can grow in a petri dish

1177
01:22:00,650 --> 01:22:05,840
one way of figuring out how much poliovirus RNA is in the solution is to

1178
01:22:05,840 --> 01:22:09,130
make a monolayer of cells like this

1179
01:22:09,180 --> 01:22:13,960
and then take various di lutions of virus document one talks about a virus type

1180
01:22:13,960 --> 01:22:18,390
one talks about a solution of virus particles one can really see them and even

1181
01:22:18,390 --> 01:22:21,700
if one could see the money electron microscope one wouldn't really know

1182
01:22:21,710 --> 01:22:26,080
what fraction of the ones who were seeing were actually biologically competent

1183
01:22:26,100 --> 01:22:32,920
but most interestingly you can take poliovirus solution of poliovirus particles place on a monolayer

1184
01:22:32,920 --> 01:22:34,120
of cells here

1185
01:22:34,170 --> 01:22:36,910
which can be infected by polio virus

1186
01:22:36,930 --> 01:22:41,080
in contrast to what i told you last time about

1187
01:22:41,100 --> 01:22:46,740
we're infected cell can tolerate the continued presence of the viral infection without dying

1188
01:22:46,790 --> 01:22:53,510
poliovirus is a highly psychopathic virus and by that i mean it replicated inside cells

1189
01:22:53,520 --> 01:22:57,160
and it kills them during the course of replicating inside the cells

1190
01:22:57,340 --> 01:23:01,420
so here what one has the polio virus will infect the cell here

1191
01:23:01,470 --> 01:23:02,540
and then

1192
01:23:02,560 --> 01:23:06,410
it will begin to infer spread from that cell to neighbouring cells in the

1193
01:23:06,420 --> 01:23:08,410
in the surroundings

1194
01:23:08,420 --> 01:23:09,770
and in so doing

1195
01:23:09,810 --> 01:23:12,920
it will create it we wrote a whole

1196
01:23:12,970 --> 01:23:13,840
right here

1197
01:23:13,860 --> 01:23:15,980
in the model layer

1198
01:23:16,020 --> 01:23:19,540
whose presence a so-called black signifies

1199
01:23:19,560 --> 01:23:24,530
the fact that there was initially infecting virus particle there it spreads intrinsically that spread

1200
01:23:24,530 --> 01:23:27,660
outward from the initially infected cells

1201
01:23:27,670 --> 01:23:31,650
in detail if you want to do this experiment really nicely what you do have

1202
01:23:31,660 --> 01:23:36,130
few affect the initial monolayer cells which i show here and in section is you

1203
01:23:36,130 --> 01:23:37,610
put on a layer of

1204
01:23:37,660 --> 01:23:42,210
the other rows or something above the infected monolayer and that ensured so that if

1205
01:23:42,210 --> 01:23:46,090
there's any viral spread it will be from cell to cell spread in a certain

1206
01:23:46,090 --> 01:23:52,650
neighborhood random virus particles getting into solution and swimming around all over the place infecting

1207
01:23:52,800 --> 01:23:54,830
cells helter-skelter

1208
01:23:54,880 --> 01:23:58,790
so here we might want to infect initial cell right here the monolayer and now

1209
01:23:58,790 --> 01:24:05,390
this grows will confine the the subsequent spread of progeny virus two adjacent cells in

1210
01:24:05,390 --> 01:24:08,000
with a ternary numeral

1211
01:24:08,020 --> 01:24:12,620
and what might happen in the real arithmetic the real numbers the ticket not the

1212
01:24:12,620 --> 01:24:14,890
actual numbers

1213
01:24:14,940 --> 01:24:18,390
so we have these these particular constants

1214
01:24:18,410 --> 01:24:21,810
and we have particular functions and successor function

1215
01:24:22,910 --> 01:24:25,410
and that sort of thing

1216
01:24:25,460 --> 01:24:33,210
this allows us to build up a little algebra inside of formal logic of times

1217
01:24:34,190 --> 01:24:36,810
inside terms these are all still have formal

1218
01:24:36,850 --> 01:24:39,750
mechanisms therefore methods

1219
01:24:39,810 --> 01:24:41,080
we still have

1220
01:24:41,370 --> 01:24:45,710
some predicates and variables but we now everybody in terms

1221
01:24:45,750 --> 01:24:47,940
here are the successors zero

1222
01:24:47,960 --> 01:24:50,670
successor's successor of zero

1223
01:24:50,730 --> 01:24:53,850
which often enough issue or not

1224
01:24:53,870 --> 01:25:00,960
have created to that kind of thing

1225
01:25:01,870 --> 01:25:04,480
sometimes in whatever in the future

1226
01:25:04,480 --> 01:25:07,100
these things would be called numerals

1227
01:25:08,040 --> 01:25:12,660
and the underlying thing at the neural spines to represent is the number

1228
01:25:12,710 --> 01:25:16,020
so this is supposed to represent two

1229
01:25:16,060 --> 01:25:18,940
but it's not it's this isn't just in your

1230
01:25:18,940 --> 01:25:22,440
referring to two we had in in a good model

1231
01:25:23,600 --> 01:25:25,710
the distinction between the natural model

1232
01:25:25,710 --> 01:25:27,040
in which

1233
01:25:27,060 --> 01:25:30,310
in which the the term zero prime prime

1234
01:25:30,330 --> 01:25:32,120
actually does refer to two

1235
01:25:32,140 --> 01:25:34,600
from some of the models

1236
01:25:34,600 --> 01:25:38,410
so this is kind of relevant to this kind of exercise because we trying to

1237
01:25:38,410 --> 01:25:39,350
build up

1238
01:25:39,370 --> 01:25:42,790
very complicated terms

1239
01:25:42,810 --> 01:25:44,580
build up with numbers

1240
01:25:44,660 --> 01:25:48,080
with the idea that there's this mapping to form

1241
01:25:48,620 --> 01:25:52,620
in the next couple of pages show you two ways that this was done

1242
01:25:52,660 --> 01:25:56,330
this is not a great import these days because people tend to believe that can

1243
01:25:56,330 --> 01:25:56,980
be done

1244
01:25:57,120 --> 01:25:59,930
they i have to be too big a deal about

1245
01:25:59,980 --> 01:26:04,440
it certainly is important as the first you cannot proceed unless you can show that

1246
01:26:04,440 --> 01:26:08,000
the model that the mapping is unique and so on

1247
01:26:09,350 --> 01:26:12,310
cutting method is going to assign the close to

1248
01:26:12,370 --> 01:26:14,520
the variables x and y

1249
01:26:14,520 --> 01:26:15,790
that can be

1250
01:26:15,790 --> 01:26:17,940
this kind of thing in principle

1251
01:26:17,940 --> 01:26:19,940
in our official notation

1252
01:26:19,960 --> 01:26:21,430
and that

1253
01:26:21,430 --> 01:26:26,120
it's going to be related to a formula in the language

1254
01:26:28,020 --> 01:26:32,690
so we call it and conventionally this thing

1255
01:26:32,870 --> 01:26:38,310
we all use that notation yesterday the standard description machine for instance

1256
01:26:38,310 --> 01:26:39,790
this is the same idea

1257
01:26:39,890 --> 01:26:42,350
now it's time to use more formally

1258
01:26:42,370 --> 01:26:44,160
so this is supposed to be

1259
01:26:44,160 --> 01:26:47,640
a particular terminal language two

1260
01:26:47,690 --> 01:26:49,100
formula five

1261
01:26:49,120 --> 01:26:52,770
and we need to see a couple of ways in which we do that

1262
01:26:52,790 --> 01:26:54,810
not only that

1263
01:26:54,810 --> 01:26:55,830
what girls

1264
01:26:55,830 --> 01:26:58,730
considered more than other people have found

1265
01:26:58,750 --> 01:27:01,390
easier or more or less

1266
01:27:01,440 --> 01:27:06,890
difficult anyway ways to a number of things is not just formulas derivations

1267
01:27:06,910 --> 01:27:08,730
after all the derivation

1268
01:27:08,770 --> 01:27:11,310
informal logic is supposed to be

1269
01:27:11,310 --> 01:27:13,100
a sequence of

1270
01:27:13,160 --> 01:27:15,440
the formulas

1271
01:27:16,140 --> 01:27:20,600
each formula which is either an axiom or follows from the preceding ones by the

1272
01:27:20,600 --> 01:27:22,160
rules that kind

1273
01:27:22,210 --> 01:27:23,830
and it's supposed to be

1274
01:27:25,080 --> 01:27:27,080
the last thing in the sequence

1275
01:27:28,140 --> 01:27:29,270
that's one

1276
01:27:29,290 --> 01:27:32,500
that same the introducing the concept of the derivation

1277
01:27:32,520 --> 01:27:34,480
we can cut that is the number

1278
01:27:34,480 --> 01:27:37,250
by just giving a certain number comma

1279
01:27:37,290 --> 01:27:39,690
we can decide that

1280
01:27:39,690 --> 01:27:40,690
this thing here

1281
01:27:40,890 --> 01:27:42,120
can be

1282
01:27:42,350 --> 01:27:44,080
as well as

1283
01:27:44,180 --> 01:27:46,330
telling the formulas

1284
01:27:46,350 --> 01:27:47,890
these axes

1285
01:27:47,910 --> 01:27:50,290
zero successor any other

1286
01:27:50,290 --> 01:27:55,060
function symbols or predicate symbols quantify whatever are around

1287
01:27:55,080 --> 01:27:58,620
because it allows up into this thing we can also

1288
01:27:59,830 --> 01:28:02,770
and so we can express their relations that way

1289
01:28:02,770 --> 01:28:06,210
so we could and we might get to that so much in the beginning but

1290
01:28:06,210 --> 01:28:12,640
we assign a closed term to each derivation which after sequence

1291
01:28:12,750 --> 01:28:14,960
now if the language in question

1292
01:28:14,980 --> 01:28:19,040
is the language of formalised arithmetic which is what i've talking about here

1293
01:28:19,080 --> 01:28:21,040
because will be in your

1294
01:28:21,080 --> 01:28:22,730
it will be like this

1295
01:28:24,460 --> 01:28:27,560
big formulas

1296
01:28:31,410 --> 01:28:34,060
just keep going here i guess i'll just show you the

1297
01:28:34,080 --> 01:28:35,390
the cleaning e

1298
01:28:36,710 --> 01:28:39,940
this is sorry the bill as jeffrey approach

1299
01:28:39,960 --> 01:28:41,830
in the bill of jeffrey

1300
01:28:41,910 --> 01:28:45,730
scheme for doing this sort of stuff

1301
01:28:45,750 --> 01:28:48,120
this is not the my golden goal

1302
01:28:48,120 --> 01:28:52,190
we started out just like this one both else on board

1303
01:28:52,210 --> 01:28:54,960
let's bring this is going to get the number one

1304
01:28:55,040 --> 01:28:57,100
rockets get the number two

1305
01:28:57,160 --> 01:28:59,580
the ampersand the wage

1306
01:29:00,330 --> 01:29:02,230
i think the logical conjunction

1307
01:29:02,270 --> 01:29:04,080
gets number three

1308
01:29:04,850 --> 01:29:06,690
exists gets four

1309
01:29:06,710 --> 01:29:07,980
the first

1310
01:29:08,020 --> 01:29:11,790
variable whatever it is gets five

1311
01:29:12,890 --> 01:29:14,830
this scheme for extending

1312
01:29:14,830 --> 01:29:17,930
if we assign five to the exterior

1313
01:29:17,940 --> 01:29:19,690
wilson five nine

1314
01:29:19,710 --> 01:29:23,910
x one sin five nine nine to x two

1315
01:29:23,960 --> 01:29:25,960
and so on

1316
01:29:27,040 --> 01:29:31,120
for the function symbols if there are any we'll use the six series

1317
01:29:31,160 --> 01:29:32,730
so function symbol

1318
01:29:32,750 --> 01:29:34,640
the zero function was

1319
01:29:34,660 --> 01:29:36,910
zero arguments

1320
01:29:36,930 --> 01:29:40,540
is so these constants get sick so

1321
01:29:40,640 --> 01:29:44,230
six years can extend to is k

1322
01:29:46,620 --> 01:29:49,870
we've got six o six nine six nine nine

1323
01:29:49,890 --> 01:29:52,580
in the course excited states

1324
01:29:52,600 --> 01:29:56,250
that's now it and logical symbols

1325
01:29:58,690 --> 01:30:01,430
we've done constants and function symbols

1326
01:30:01,430 --> 01:30:04,330
now it is predicate symbols

1327
01:30:05,160 --> 01:30:09,230
so the predicate symbols get the seven series

1328
01:30:09,320 --> 01:30:15,120
we've just associated with each particular predicate symbol in the language whatever it might be

1329
01:30:15,170 --> 01:30:19,850
seven seven i seven i had a section

1330
01:30:19,870 --> 01:30:23,910
that's a little example

1331
01:30:23,930 --> 01:30:27,500
which issues discussed so far completely mechanical

1332
01:30:27,560 --> 01:30:30,620
that course was an important point for these guys

1333
01:30:30,750 --> 01:30:32,440
for lexicons x

1334
01:30:32,460 --> 01:30:35,710
x equals x is coded by this scheme two

1335
01:30:35,730 --> 01:30:37,940
let me say wastrel

1336
01:30:37,980 --> 01:30:40,830
it's forty nine case

1337
01:30:40,960 --> 01:30:45,370
we start with forty nine that we have to have a left parenthesis

1338
01:30:45,390 --> 01:30:50,850
by the time of the state

1339
01:30:50,890 --> 01:30:52,310
i have

1340
01:30:52,370 --> 01:30:55,290
so that should be for nine

1341
01:30:55,310 --> 01:31:01,640
one five on sorry for nine five years sorry then left prints several x four

1342
01:31:01,640 --> 01:31:03,870
ninety five

1343
01:31:03,930 --> 01:31:06,120
four nine

1344
01:31:07,140 --> 01:31:09,310
references one

1345
01:31:09,310 --> 01:31:12,370
and so on

1346
01:31:12,480 --> 01:31:14,560
that's the mapping

1347
01:31:14,580 --> 01:31:17,960
so if i gave you formulate certainly produce the number by

1348
01:31:17,980 --> 01:31:19,250
phone around

1349
01:31:19,250 --> 01:31:20,540
what what what

1350
01:31:20,600 --> 01:31:23,500
is more complicated but

1351
01:31:23,520 --> 01:31:24,980
we need to proof is

1352
01:31:24,980 --> 01:31:26,540
if i give you my number

1353
01:31:26,560 --> 01:31:29,020
can you figure out the formula

1354
01:31:29,140 --> 01:31:31,160
if i give you the number

1355
01:31:31,230 --> 01:31:34,480
nine by itself

1356
01:31:34,480 --> 01:31:37,620
that number there is no formula correspond to

1357
01:31:37,620 --> 01:31:41,490
which touches x x five x six hundred thirty three

1358
01:31:41,990 --> 01:31:47,340
please we saw here in between these separator potentials

1359
01:31:47,350 --> 01:31:52,000
that connect and containing the variables so x one x two x three

1360
01:31:52,010 --> 01:31:56,720
shares x three with this week so we have the junction tree

1361
01:31:56,730 --> 01:31:59,520
and now we can run the junction tree algorithm

1362
01:32:00,740 --> 01:32:01,890
and the junction

1363
01:32:01,900 --> 01:32:03,690
basically contains a

1364
01:32:03,710 --> 01:32:08,860
the public distribution and send messages around which everybody is consistent

1365
01:32:08,900 --> 01:32:11,220
and we might consist is

1366
01:32:11,230 --> 01:32:14,150
i want to very tables in the junction tree

1367
01:32:17,120 --> 01:32:18,990
so each one of these please

1368
01:32:19,000 --> 01:32:25,190
we saw earlier as a function of its variables and nobody's these articles so functional

1369
01:32:25,190 --> 01:32:29,480
registered to the functional groups one x two x three i want all these functions

1370
01:32:29,480 --> 01:32:31,950
to on the variables they share

1371
01:32:31,960 --> 01:32:33,830
OK if i asked this function

1372
01:32:33,850 --> 01:32:36,410
what is the likely value for x two

1373
01:32:36,440 --> 01:32:37,490
next week

1374
01:32:37,500 --> 01:32:38,900
true or false

1375
01:32:38,920 --> 01:32:43,220
well this one like extra is true but this one like sixteen false and i'm

1376
01:32:43,310 --> 01:32:44,390
going to be

1377
01:32:44,410 --> 01:32:45,320
structure out the

1378
01:32:45,330 --> 01:32:46,640
you just all the

1379
01:32:48,290 --> 01:32:53,520
by sending messages from each clean to its neighboring saying this is what i think

1380
01:32:53,610 --> 01:32:56,580
then marginalize over the shared variable

1381
01:32:56,600 --> 01:33:01,130
so a b will send a message to saying if i some of my two

1382
01:33:01,130 --> 01:33:03,860
dimensional table entries over the

1383
01:33:03,870 --> 01:33:06,520
here's what i think should be

1384
01:33:06,540 --> 01:33:11,480
and then you get that message then update stable so it was about see you

1385
01:33:12,740 --> 01:33:14,290
table from b

1386
01:33:14,320 --> 01:33:18,580
so basically get invited to the two one dimensional

1387
01:33:18,600 --> 01:33:22,570
two by one table two by two table and some will stable give you the

1388
01:33:22,570 --> 01:33:24,440
same value something on the table

1389
01:33:24,450 --> 01:33:25,780
over c

1390
01:33:25,810 --> 01:33:29,790
so that's what i mean by message passing and so that's ever gets updated many

1391
01:33:29,810 --> 01:33:34,880
up typically with this message be the same for the first method the ball potentials

1392
01:33:34,880 --> 01:33:35,960
are really

1393
01:33:35,980 --> 01:33:38,250
we have a consistent distribution

1394
01:33:40,030 --> 01:33:44,410
there's something very valuable about that because once the judge in other that consistency and

1395
01:33:44,410 --> 01:33:46,440
we have a marginal distributions are

1396
01:33:47,530 --> 01:33:50,690
and how to treat the junction tree algorithm is guaranteed

1397
01:33:50,710 --> 01:33:54,660
it was two steps you initialise all you need to feel

1398
01:33:54,720 --> 01:33:58,740
and separate potentials you pull messages up towards the roof

1399
01:33:58,750 --> 01:34:01,570
everybody updates

1400
01:34:01,590 --> 01:34:06,560
that's upstream like it's in this room everybody is their tables and the reluctant to

1401
01:34:06,690 --> 01:34:09,790
send messages back downstream of individually

1402
01:34:11,340 --> 01:34:13,100
he's done some these

1403
01:34:13,120 --> 01:34:15,640
it's sum product messages

1404
01:34:15,650 --> 01:34:18,250
also called the sum product algorithm because it

1405
01:34:18,270 --> 01:34:22,820
at the very end when you read three tables your potential tables

1406
01:34:22,840 --> 01:34:28,880
you separate tables they all become potentials they become marginals conditionals over the hidden variables

1407
01:34:28,880 --> 01:34:30,620
given the data

1408
01:34:30,630 --> 01:34:34,240
so that's what we want for inference of each one of these

1409
01:34:34,240 --> 01:34:38,380
clearly now becomes the probability of the variables

1410
01:34:38,390 --> 01:34:43,860
it's describing given all the observed variables that you've actually observed before started the junction

1411
01:34:45,330 --> 01:34:49,360
so we have all these marginals telling you what configurations are like for the hidden

1412
01:34:49,360 --> 01:34:52,500
variable here this invaluable in the parent variables

1413
01:34:52,510 --> 01:34:54,990
as a two dimensional table

1414
01:34:55,000 --> 01:34:59,360
and also the normalized every potential functions is the likelihood of the data

1415
01:34:59,400 --> 01:35:02,390
the problem is that there were some of the hidden variable

1416
01:35:02,400 --> 01:35:05,310
so for basically one one

1417
01:35:05,320 --> 01:35:09,030
very efficient algorithm on it reaches the collective distributed

1418
01:35:09,460 --> 01:35:11,030
we have

1419
01:35:11,060 --> 01:35:16,160
all these interesting marginals conditionals over hidden variables given the data then the likelihood of

1420
01:35:16,160 --> 01:35:18,210
the data given the model

1421
01:35:18,250 --> 01:35:23,130
OK so that steps one two that we would like to do with

1422
01:35:23,150 --> 01:35:25,290
with that bayes net

1423
01:35:25,310 --> 01:35:30,110
and then another step is maximizing like so i can evaluate that like but i

1424
01:35:30,110 --> 01:35:31,900
also want to increase by

1425
01:35:31,910 --> 01:35:33,610
we only some parameters

1426
01:35:33,620 --> 01:35:35,840
i just think my tables

1427
01:35:35,850 --> 01:35:38,950
or just some jealousy means and covariances

1428
01:35:38,970 --> 01:35:41,690
so we'd like to maximize the likelihood

1429
01:35:41,700 --> 01:35:43,700
by summing over the hidden variables

1430
01:35:43,720 --> 01:35:47,820
basically the probability that the XV given the data

1431
01:35:47,840 --> 01:35:50,100
these are rights age disappears

1432
01:35:50,120 --> 01:35:51,980
so that's what that's like

1433
01:35:51,980 --> 01:35:57,560
able to learn from the data the worst-case predictions from learning theory would would apply

1434
01:35:58,310 --> 01:36:02,370
but in practice they don't because real data is correlated like this is a simple

1435
01:36:02,370 --> 01:36:06,690
correlation and we're gonna talk about this correlation first how you fit models that assume

1436
01:36:06,690 --> 01:36:07,340
this correlation

1437
01:36:09,300 --> 01:36:13,110
that's not invalid lesson doesn't mean that you don't have to worry about the curse

1438
01:36:13,110 --> 01:36:16,840
dimensionality because it actually associated with your model not your data

1439
01:36:18,770 --> 01:36:23,590
we were sampling from model had this independence characteristic if you make such a model

1440
01:36:23,590 --> 01:36:28,090
that has this independence characteristic your model is covering a vast space data can be

1441
01:36:28,090 --> 01:36:32,540
difficult to extract information from the data so there's something wrong with models like back

1442
01:36:33,860 --> 01:36:39,020
i think it's endemic in machine learning and statistics to claim that this is a

1443
01:36:39,020 --> 01:36:43,750
characteristic of the space is a characteristic of the model plus the space yet it

1444
01:36:43,750 --> 01:36:46,980
gives the worst case thing that can happen in the space and then if someone

1445
01:36:46,980 --> 01:36:48,110
brought to the data

1446
01:36:48,570 --> 01:36:52,730
which has after characteristic you really wanna be asking them what rubbish if you just

1447
01:36:52,730 --> 01:36:56,500
give me there's no information in this data because there really is and that's what

1448
01:36:56,670 --> 01:36:57,750
these lessons are about

1449
01:36:58,290 --> 01:37:04,230
um if this information is regularity in the data that you can extract those things will be true okay so

1450
01:37:05,420 --> 01:37:07,690
one thing i to do is put somebody

1451
01:37:08,860 --> 01:37:09,340
i mean because

1452
01:37:10,590 --> 01:37:13,980
i mean i just think it's in a larger sense is a myth a lot

1453
01:37:13,980 --> 01:37:17,670
of the people think people extract from it but not touched on earlier saying you

1454
01:37:17,670 --> 01:37:21,360
should expand things in the dimensions that's what the statistics say

1455
01:37:21,770 --> 01:37:22,770
because it's a bad idea

1456
01:37:23,190 --> 01:37:27,840
by making a feature size and infinity that's always gonna problems no no it's not

1457
01:37:27,840 --> 01:37:32,290
because the structure the model because the capacity control that's the way but talking about

1458
01:37:32,360 --> 01:37:35,900
yeah i'm saying the same thing is about what you know about your you'll

1459
01:37:37,040 --> 01:37:41,790
dataset size is a very bizarre thing if you think about it for someone to come to you

1460
01:37:42,400 --> 01:37:46,630
you're going be model and someone's bought u on you know his your client he's

1461
01:37:46,630 --> 01:37:48,790
walk into a room and he says that the data set

1462
01:37:49,320 --> 01:37:52,630
i found out hundred things about these fifty people

1463
01:37:53,090 --> 01:37:54,320
please can you fit your model

1464
01:37:54,860 --> 01:37:56,340
thank you think it that's difficult because

1465
01:37:56,800 --> 01:37:58,800
the zpi is large and in a smallish

1466
01:37:59,250 --> 01:38:02,750
but are trying to model and the guy goes all just i can tell you

1467
01:38:02,750 --> 01:38:05,210
one hundred more things about those people would you like to know

1468
01:38:05,900 --> 01:38:10,190
and then you should know don't tell me because then you have an even larger peak

1469
01:38:11,770 --> 01:38:15,690
i don't know anymore because it will mess with me the data as well as

1470
01:38:16,170 --> 01:38:18,230
how can that be true because it's not true

1471
01:38:19,690 --> 01:38:23,300
those hundred things will not be independent the previous hundred things you'll get richer model

1472
01:38:23,300 --> 01:38:24,690
the person so and so on so on

1473
01:38:25,340 --> 01:38:27,190
okay to be more stuff about that later on

1474
01:38:29,750 --> 01:38:30,150
okay so

1475
01:38:30,610 --> 01:38:35,340
where does this low rank abundance come from it comes from a low dimensional approximation

1476
01:38:35,340 --> 01:38:40,710
for the dataset that's the connection with dimensionality reduction simple low dimensional approximation linear low

1477
01:38:40,710 --> 01:38:41,690
dimensional approximation

1478
01:38:42,090 --> 01:38:47,040
if we made this e w w transpose dusty i made indeed sigma squared i before

1479
01:38:47,610 --> 01:38:52,840
then i would be known as factor analysis on the former used is known as probabilistic piece here

1480
01:38:54,400 --> 01:38:59,110
probabilistic principal component analysis and is a linear mapping from q-dimensional latent space

1481
01:38:59,860 --> 01:39:07,190
because i'm to build probabilistic models i think the low dimensional thing as latent variables unobserved things to appeal dimensional

1482
01:39:08,190 --> 01:39:08,770
data space

1483
01:39:09,980 --> 01:39:13,250
and then we can mappings we do a linear mapping from the then we corrupted

1484
01:39:13,250 --> 01:39:18,130
by independent gaussian noise noise is high-dimensional that's something we can't i mean that's almost

1485
01:39:18,130 --> 01:39:22,070
like this one way of thinking about noise if the nasty difficult stuff that we

1486
01:39:22,070 --> 01:39:26,960
can extract information from the people who define noise were people like the class and

1487
01:39:26,960 --> 01:39:29,480
gauss who looking a planetary motions through

1488
01:39:30,460 --> 01:39:34,670
the atmosphere and telescope and they said these things like on account four

1489
01:39:35,650 --> 01:39:40,590
let's assume independent random things being added to each other observation

1490
01:39:41,000 --> 01:39:42,040
arm bands

1491
01:39:42,650 --> 01:39:47,000
they're unstructured something they can't deal with the structure structure noise is very interesting but

1492
01:39:47,110 --> 01:39:48,670
it's not something we're assuming hit

1493
01:39:49,610 --> 01:39:53,670
and then what we do is we marginalize these latent variables using against him prior

1494
01:39:53,690 --> 01:39:55,520
that's how we do probabilistic piece you so

1495
01:39:55,980 --> 01:39:57,520
just remind on the notation so

1496
01:39:58,070 --> 01:40:02,070
two dimensional latent space pure dimensional data space and the number of data points why

1497
01:40:02,070 --> 01:40:04,020
is data in this end by p

1498
01:40:04,610 --> 01:40:06,250
matrix that's important because

1499
01:40:06,860 --> 01:40:10,290
it turns out if i write why why transpose that's an inner product matrix

1500
01:40:11,250 --> 01:40:15,710
center data is like data with the mean subtracted of so that's something we use

1501
01:40:15,710 --> 01:40:19,150
a lot and the latent variables are x and then we can use this mapping

1502
01:40:19,150 --> 01:40:21,920
matrix which maps repeat dimension to q-dimensional

1503
01:40:22,380 --> 01:40:25,150
data space so in fact when i go from

1504
01:40:25,150 --> 01:40:27,810
measure the similarity between the patterns

1505
01:40:27,830 --> 01:40:31,410
and if you're interested you can check our KDD that was on the

1506
01:40:31,410 --> 01:40:36,410
file page we describe these two cases

1507
01:40:36,460 --> 01:40:43,130
OK so far i just discussed as techniques to interpret

1508
01:40:43,140 --> 01:40:46,760
those discovered patterns

1509
01:40:46,770 --> 01:40:48,960
i haven't touched the second problem

1510
01:40:49,020 --> 01:40:53,210
if we set the threshold the low likely we would generate an exponential

1511
01:40:53,300 --> 01:40:55,240
set of patterns

1512
01:40:55,260 --> 01:40:59,580
and in some cases it is really time consuming so

1513
01:40:59,590 --> 01:41:03,310
researchers propose ideas for example closed graph patterns

1514
01:41:03,320 --> 01:41:08,130
and the maximum graph pattern to generate those graph patterns that are not redundant

1515
01:41:08,180 --> 01:41:09,880
so here is the concept

1516
01:41:09,890 --> 01:41:12,130
of closed frequent patterns

1517
01:41:12,180 --> 01:41:15,140
and the maximal frequent patterns

1518
01:41:15,190 --> 01:41:19,030
so let me show you some examples of closed frequent patterns

1519
01:41:19,860 --> 01:41:21,370
a graph pattern g

1520
01:41:21,390 --> 01:41:26,810
is closed if there exists no supergraph of g that carries the same support.

1521
01:41:26,810 --> 01:41:31,410
of which is obvious if some of G's subgraphs have the same support then it

1522
01:41:31,410 --> 01:41:37,990
is not necessary to output G subgraph. G-subgraph is just redundant subgraph

1523
01:41:37,990 --> 01:41:40,690
if we compare them with G.

1524
01:41:40,750 --> 01:41:44,450
and here is the definition of maximal frequent graph pattern

1525
01:41:44,510 --> 01:41:49,060
a frequent graph pattern g is maximal if there exists no supergraph of g is that

1526
01:41:49,060 --> 01:41:50,050
is frequent

1527
01:41:50,050 --> 01:41:54,380
so basically suppose we have a large set of frequent patterns we would like to

1528
01:41:54,380 --> 01:41:56,530
pick maximum

1529
01:41:56,550 --> 01:42:01,110
maximum patterns and output patterns to the user

1530
01:42:01,170 --> 01:42:02,810
so by doing this too

1531
01:42:02,810 --> 01:42:08,710
actually we can afford to shrink the size of frequent pattern set

1532
01:42:08,740 --> 01:42:09,580
here is

1533
01:42:09,590 --> 01:42:16,130
number comparison between frequent graph pattern and the closed frequent graph patterns

1534
01:42:16,190 --> 01:42:21,360
here is the axis is the minimum support threshold is the y axis

1535
01:42:21,380 --> 01:42:25,640
is the number of patterns

1536
01:42:25,650 --> 01:42:29,460
so as you can see that the number of closed frequent graph patterns

1537
01:42:29,540 --> 01:42:33,160
is significantly less than the number of frequent graph patterns

1538
01:42:33,220 --> 01:42:35,040
here i do not show the

1539
01:42:35,050 --> 01:42:39,310
number of maximal frequent graph patterns actually will be

1540
01:42:39,360 --> 01:42:44,210
much small than this number

1541
01:42:44,250 --> 01:42:48,570
and and you want to find those call graph patterns we developed some new

1542
01:42:48,570 --> 01:42:54,100
method which enabled which is able to find these patterns efficiently so we do not

1543
01:42:54,100 --> 01:42:59,340
want to develop some algorithm. OK in in the first step we generate all of the frequent patterns

1544
01:42:59,340 --> 01:43:02,000
and the second step is then we just do the for

1545
01:43:02,160 --> 01:43:06,380
post-process to generate those close closed graph patterns. this is not a

1546
01:43:06,990 --> 01:43:12,620
we would like to propose some pattern mining algorithm which can further speed up the mining

1547
01:43:12,620 --> 01:43:16,510
process without generating is intermediate

1548
01:43:16,520 --> 01:43:21,720
frequent graph patterns so here is our method the intuition

1549
01:43:21,780 --> 01:43:28,100
so suppose we extend a new pattern candidate from the previously discovered patterns

1550
01:43:28,110 --> 01:43:29,330
with k edge

1551
01:43:29,340 --> 01:43:33,600
and we would like to find some conditions that we can stop the search of

1552
01:43:33,610 --> 01:43:35,360
all the super

1553
01:43:36,350 --> 01:43:38,280
so here is a condition

1554
01:43:38,280 --> 01:43:42,460
if we have two graphs g and g prime and both of them are frequent

1555
01:43:42,460 --> 01:43:45,280
g is a subgraph of g prime

1556
01:43:45,380 --> 01:43:46,840
if in any

1557
01:43:46,850 --> 01:43:49,110
part of the one tgraph in the dataset

1558
01:43:49,190 --> 01:43:50,790
where g occurs

1559
01:43:50,790 --> 01:43:52,870
g-prime prime also occurs

1560
01:43:52,890 --> 01:43:57,130
then we actually need not grow g any-more since none of G's subgraph will

1561
01:43:57,130 --> 01:44:01,370
be closed except those of g prime

1562
01:44:01,380 --> 01:44:05,440
so this is very intuitive basically we can skip some search space

1563
01:44:05,450 --> 01:44:10,280
however there is a very tricky cases we should pay attention to in the graph

1564
01:44:10,280 --> 01:44:14,240
pattern mining here is example suppose we have two graphs

1565
01:44:14,240 --> 01:44:15,720
in the database

1566
01:44:15,750 --> 01:44:17,690
and we find that

1567
01:44:18,180 --> 01:44:19,850
edge a and edge b

1568
01:44:19,860 --> 01:44:24,190
always are coupled together

1569
01:44:24,200 --> 01:44:26,640
OK then we can find that's OK

1570
01:44:26,750 --> 01:44:28,260
with our

1571
01:44:28,270 --> 01:44:32,910
early termination condition we may need to expand patterns that contains

1572
01:44:32,930 --> 01:44:35,780
edge a and b together

1573
01:44:35,820 --> 01:44:38,270
however we recognise that

1574
01:44:38,310 --> 01:44:42,460
this part and this is part and actually this one across at least in both

1575
01:44:42,470 --> 01:44:43,900
of these graphs

1576
01:44:43,960 --> 01:44:46,670
but this part can not be generated by

1577
01:44:46,680 --> 01:44:51,430
this pattern a and b that means we have to design some specific

1578
01:44:51,430 --> 01:44:55,770
heuristicus to avoid the generation of such cases

1579
01:44:55,780 --> 01:45:05,590
so in our closed graph minimg algorithms we develop some mechanism to do this

1580
01:45:05,660 --> 01:45:07,520
so so far i have discussed

1581
01:45:07,520 --> 01:45:14,170
techniques about frequent graph pattern mining which is just related with the frequency criteria actually

1582
01:45:14,170 --> 01:45:16,250
many application scenarios

1583
01:45:16,300 --> 01:45:20,300
there as an interesting objective function so that we can use

1584
01:45:20,350 --> 01:45:25,620
for example let's take a second problem setting is example suppose we have two sets

1585
01:45:25,620 --> 01:45:28,550
of graphs with positive label and the negative labels

1586
01:45:29,260 --> 01:45:31,820
there are many interesting

1587
01:45:31,870 --> 01:45:33,100
objective functions

1588
01:45:33,110 --> 01:45:38,020
popularly used in data mining and machine learning society for example contrast ratio

1589
01:45:38,070 --> 01:45:41,740
let p and q be the frequency of subgraph g in the positive

1590
01:45:41,740 --> 01:45:46,150
dataset and the negative dataset so we can derive contrast ratio

1591
01:45:46,170 --> 01:45:48,590
we all can derive G-test scores

1592
01:45:48,600 --> 01:45:53,470
to measure the significance of this graph part and we can measure information gain of

1593
01:45:53,470 --> 01:45:59,300
these graph pattern we can do the cosine measure etcetera

1594
01:45:59,390 --> 01:46:02,950
unfortunately except as the frequency management as we

1595
01:46:02,970 --> 01:46:07,120
have seen before that frequency measurement actually is anti-monotonic

1596
01:46:07,140 --> 01:46:09,570
to the size of the graph pattern

1597
01:46:09,600 --> 01:46:15,410
here i should use the value change of these objective functions with respect to the graph

1598
01:46:15,410 --> 01:46:20,090
size of all supergraphs of a given graph pattern

1599
01:46:20,140 --> 01:46:21,530
as you can see that

1600
01:46:21,530 --> 01:46:22,880
the frequency

1601
01:46:22,900 --> 01:46:27,320
this measure actually always goes start with the growing of the graph size

1602
01:46:27,360 --> 01:46:29,780
however all of the other three

1603
01:46:29,850 --> 01:46:32,350
objective functions i listed here

1604
01:46:32,380 --> 01:46:37,520
i encourage you G-test score, cosine, the information gain all of them are not correlated with

1605
01:46:37,520 --> 01:46:39,010
the size of graph

1606
01:46:39,050 --> 01:46:40,290
what this means

1607
01:46:40,300 --> 01:46:44,780
this means that you if you want to find the graph patterns

1608
01:46:45,650 --> 01:46:46,670
have the

1609
01:46:47,820 --> 01:46:49,100
objective scores

1610
01:46:49,110 --> 01:46:53,700
you have to enumerate all of the possible subgraph in the database and then check

1611
01:46:53,720 --> 01:46:57,910
their objective score one by one to find those acquired patterns with a high

1612
01:46:59,520 --> 01:47:01,160
certainly it is not

1613
01:47:01,180 --> 01:47:05,830
good solution because it's not affordable in many applications

1614
01:47:05,870 --> 01:47:11,110
so researchers resort to some intermediate approximate solutions

1615
01:47:11,280 --> 01:47:15,670
here is

1616
01:47:15,680 --> 01:47:18,110
frequent pattern based mining framework

1617
01:47:18,110 --> 01:47:22,380
OK in this framework in the first step we can first generate those frequent

1618
01:47:22,380 --> 01:47:24,820
graph patterns and in the second step

1619
01:47:24,860 --> 01:47:28,500
so we can check their objective score go one by one try to find

1620
01:47:28,500 --> 01:47:32,880
those graph patterns with the highest objective score

1621
01:47:32,900 --> 01:47:34,690
however this mining

1622
01:47:35,470 --> 01:47:39,620
pipeline is approximate and also it has its own bottleneck

1623
01:47:39,770 --> 01:47:45,380
in the first step sometimes we might generate millions or even billions of patterns

1624
01:47:45,400 --> 01:47:49,460
here's the reason if we use that to the minimum support threshold too high

1625
01:47:49,500 --> 01:47:52,390
likely you will miss some important patterns

1626
01:47:52,440 --> 01:47:55,640
if you use that as the minimum support threshold is very low

1627
01:47:55,650 --> 01:47:59,350
likely the mining process will be stuck in the first step

1628
01:47:59,400 --> 01:48:03,470
and the second is has no guarantee of the quality because there is no correlation

1629
01:48:03,470 --> 01:48:09,110
between this objective functions with frequency

1630
01:48:10,110 --> 01:48:16,250
so we recognise this problem so rather than actually generating the complete set of frequent patterns

1631
01:48:16,270 --> 01:48:22,150
we are thinking that it could be more appealing to generate graph patterns

1632
01:48:22,170 --> 01:48:23,240
that has

1633
01:48:23,240 --> 01:48:28,170
in our work well with the case of lot marketing when we had the recommendations

1634
01:48:28,170 --> 01:48:33,070
between people and the other one is blogs have citations between blog posts

1635
01:48:33,130 --> 01:48:36,550
OK and the questions we ask in this part was

1636
01:48:36,600 --> 01:48:39,960
what kind of cascades arise in real life so so basically we would like to

1637
01:48:39,960 --> 01:48:41,750
see what are the common

1638
01:48:41,770 --> 01:48:45,500
the shapes of the cascades what got what the graph that commonly occur in the

1639
01:48:45,500 --> 01:48:49,770
network right so could be out of three stars and so on the question is

1640
01:48:50,400 --> 01:48:55,710
what is the distribution of this over this cascades and then

1641
01:48:55,730 --> 01:48:58,950
in case of fire marketing you could also go and just ask

1642
01:48:58,960 --> 01:49:02,840
how how what is your probability of going and buying a product if somebody makes

1643
01:49:02,840 --> 01:49:06,390
a recommendation in the more recommendations you get more likely to buy

1644
01:49:06,390 --> 01:49:09,330
if you do get tired of communications and just

1645
01:49:09,340 --> 01:49:12,890
develop some kind of resistance to that particular problem

1646
01:49:12,920 --> 01:49:14,390
so the first

1647
01:49:14,420 --> 01:49:15,920
case is from the

1648
01:49:16,000 --> 01:49:17,310
like marketing

1649
01:49:17,310 --> 01:49:21,950
well the idea is that we have descendants centers and followers of recommendations and they

1650
01:49:21,950 --> 01:49:25,650
all all get discount there is some kind of incentive for people to do to

1651
01:49:25,650 --> 01:49:29,830
do good recommendations and here's what's going on so we have a person does the

1652
01:49:29,830 --> 01:49:30,850
by the book

1653
01:49:30,910 --> 01:49:35,170
was the what when they are checking out they get an option to send recommendations

1654
01:49:35,200 --> 01:49:37,660
two to friends you may

1655
01:49:37,670 --> 01:49:38,640
so let's say

1656
01:49:38,680 --> 01:49:43,390
the person decided to send the recommendation now this this the the receiver gets a

1657
01:49:44,170 --> 01:49:45,640
two to buy the book

1658
01:49:45,650 --> 01:49:50,840
the same product and they get ten percent discount because they react upon the recommendation

1659
01:49:50,840 --> 01:49:53,950
and the centre also gets ten percent credit

1660
01:49:53,970 --> 01:49:57,820
because they they made good recommendation right and everyone is happy

1661
01:49:57,830 --> 01:49:59,290
and the process continues

1662
01:50:00,000 --> 01:50:03,250
and the data we were using is from are large

1663
01:50:03,770 --> 01:50:09,060
four vep store that the case that was doing this they have a big so

1664
01:50:09,060 --> 01:50:13,380
that our readers at least three million people sixteen million recommendations on half a million

1665
01:50:13,380 --> 01:50:18,880
products and we had like four categories of products sold books dvds music and videos

1666
01:50:19,630 --> 01:50:23,130
and the questions here is how hot what can we say about the propagation of

1667
01:50:23,130 --> 01:50:28,740
these conditions over the network so here's here's an example of the product recommendation network

1668
01:50:28,750 --> 01:50:31,940
so i think this is the product recommendation network for some movie

1669
01:50:33,070 --> 01:50:38,900
the blue nodes people that just like bot purchase the product without any external or

1670
01:50:38,900 --> 01:50:43,390
without any influence that we know of and then once once the blown-out purchases the

1671
01:50:43,390 --> 01:50:48,520
product it makes recommendations right it makes recommendations to all the black nodes

1672
01:50:50,040 --> 01:50:52,620
some of the black nodes

1673
01:50:52,630 --> 01:50:55,430
hard to say

1674
01:50:55,490 --> 01:50:58,680
the to the recombination and go go by the product so this would be to

1675
01:50:58,680 --> 01:51:04,040
read and write and also when of the cases where the accommodation was successful the

1676
01:51:04,080 --> 01:51:07,970
person went to purchase the item and the think propagate and you can see that

1677
01:51:08,180 --> 01:51:11,100
a lot of a lot of the graphs just a very small but then we

1678
01:51:11,100 --> 01:51:11,930
have this slide

1679
01:51:11,980 --> 01:51:15,420
there is a connected graph where you can find some red nodes

1680
01:51:15,850 --> 01:51:19,770
when they were recommendations recommendation sexually propagate

1681
01:51:19,810 --> 01:51:21,880
right in the first question to ask is

1682
01:51:21,900 --> 01:51:26,320
however this propagation that's how the cascades in real life and what you can find

1683
01:51:26,960 --> 01:51:30,800
you can find many many times there are just starts right somebody buys recommends and

1684
01:51:30,800 --> 01:51:32,150
nothing happens

1685
01:51:32,160 --> 01:51:33,400
nobody follows

1686
01:51:33,440 --> 01:51:35,330
then what also

1687
01:51:35,380 --> 01:51:39,930
happens is that people have common friends so two to people so first person buys

1688
01:51:39,930 --> 01:51:44,990
makes accommodation and somebody else buys and makes recommendations to the same set of people

1689
01:51:44,990 --> 01:51:50,400
like here or largest subset of in the other case that i know what the

1690
01:51:50,400 --> 01:51:55,200
the crossing from this one is when this person buys makes all the recommendation and

1691
01:51:55,200 --> 01:51:56,800
then one of the people

1692
01:51:57,780 --> 01:51:59,620
after the combination again

1693
01:51:59,620 --> 01:52:01,890
make it makes recommendations for

1694
01:52:01,910 --> 01:52:05,830
so this is for example what what we can see and this makes alot of

1695
01:52:05,830 --> 01:52:09,960
sense if you think of people have friends and friends of friends and so on

1696
01:52:09,980 --> 01:52:13,360
and then the second thing that we can go and examine is what are the

1697
01:52:13,360 --> 01:52:18,350
cascade the sizes right so how big this propagations of recommendations so basically we can

1698
01:52:18,350 --> 01:52:24,270
go how many people participate in one one cascade and you can plot the distribution

1699
01:52:24,270 --> 01:52:26,520
and again i'm showing the

1700
01:52:26,520 --> 01:52:30,600
cascade size so the number of people people in the propagation graph which is the

1701
01:52:32,130 --> 01:52:35,420
and i blocked on log log scales

1702
01:52:35,450 --> 01:52:39,860
so this means it follows a power law that has a very steep drop-off

1703
01:52:39,860 --> 01:52:43,320
so basically most of the cascades are very small radius stand

1704
01:52:43,370 --> 01:52:46,450
while while there are a few that are very large

1705
01:52:46,470 --> 01:52:50,890
so this would mean that that was the propagation of recommendations were like hundred people

1706
01:52:50,890 --> 01:52:52,130
it follows that

1707
01:52:52,160 --> 01:52:57,180
and what what is interesting from this is that if you would have some kind

1708
01:52:57,180 --> 01:52:59,880
of a simple branching process than this

1709
01:52:59,940 --> 01:53:01,880
it wouldn't generate URI

1710
01:53:02,890 --> 01:53:04,580
distribution like this

1711
01:53:05,390 --> 01:53:10,080
what i was promising before is we can go and just as query simply what

1712
01:53:10,080 --> 01:53:13,820
is the probability of buying a product as one gets more and more recommendations

1713
01:53:14,830 --> 01:53:19,290
and so we are putting than the number of incoming recommendations which is the probability

1714
01:53:19,290 --> 01:53:20,160
of buying

1715
01:53:20,180 --> 01:53:22,880
four books and four dvds separately

1716
01:53:22,890 --> 01:53:26,800
and for books you can see that after recommendations people

1717
01:53:27,700 --> 01:53:33,150
don't don't want to buy anymore while for dvds which are to be different

1718
01:53:33,250 --> 01:53:35,830
the common the probability or the

1719
01:53:35,830 --> 01:53:38,930
the likelihood of purchasing saturates at around

1720
01:53:38,950 --> 01:53:41,640
ten to twenty recommendations

1721
01:53:41,650 --> 01:53:42,690
and this

1722
01:53:42,700 --> 01:53:48,990
make some implications for viral marketing practitioners to on deciding which people to target and

1723
01:53:48,990 --> 01:53:51,810
how much how much spent send to them

1724
01:53:52,920 --> 01:53:57,140
and see if we can do similar things on the blogosphere now we are observing

1725
01:53:57,140 --> 01:54:01,390
blogs that what you will know slashdot and blogs like that right so the idea

1726
01:54:01,390 --> 01:54:07,050
is look these websites where people can post their diet is of course and so

1727
01:54:07,050 --> 01:54:10,650
on so basically the idea is you have this website that is composed of posts

1728
01:54:10,650 --> 01:54:13,630
in each post is time stamped and it's i

1729
01:54:13,670 --> 01:54:18,570
so hypertext because hyperlinks to other interesting things on the web

1730
01:54:18,580 --> 01:54:19,980
other post and so on

1731
01:54:20,000 --> 01:54:22,920
so this is how we can be represented as a graph

1732
01:54:22,940 --> 01:54:26,050
we have a huge box represents a blog

1733
01:54:26,060 --> 01:54:28,920
each blogs has a set of time stamped posts

1734
01:54:28,950 --> 01:54:31,130
posts and his posts just like

1735
01:54:31,130 --> 01:54:32,660
to be embedded

1736
01:54:33,680 --> 01:54:37,160
the difference being that the algorithm that it's going to guide your search going to

1737
01:54:37,160 --> 01:54:38,260
tell you which

1738
01:54:38,260 --> 01:54:40,970
feature to try next

1739
01:54:40,990 --> 01:54:46,140
so one particular example of that is the backward elimination

1740
01:54:46,160 --> 01:54:50,430
algorithm are the that was designed for support vector machines

1741
01:54:50,510 --> 01:54:55,530
it is very simple you start with all features then you're trying to learning machine

1742
01:54:55,530 --> 01:55:00,260
on the current subset of features by minimizing a given risk functional

1743
01:55:00,360 --> 01:55:02,720
then for each of the remaining features

1744
01:55:02,740 --> 01:55:06,300
you estimate that you've used select the feature

1745
01:55:08,570 --> 01:55:16,900
in the results in the least increase of the cost function or other decrease of

1746
01:55:16,900 --> 01:55:18,130
the cost function

1747
01:55:18,180 --> 01:55:21,410
so use these degraded or to improve

1748
01:55:21,410 --> 01:55:23,800
your your performance

1749
01:55:23,800 --> 01:55:27,470
as as measured by your cost function

1750
01:55:27,490 --> 01:55:30,010
and then you remove that features

1751
01:55:30,110 --> 01:55:32,910
that improves on is degree j

1752
01:55:32,930 --> 01:55:35,180
and to start over again

1753
01:55:35,200 --> 01:55:39,680
you try again with the remaining features and it right

1754
01:55:40,680 --> 01:55:45,570
this kind of embedded methods of work for the full kernel methods and for neural

1755
01:55:50,180 --> 01:55:57,930
instead of abruptly removing features in these degrees or or or adding features in this

1756
01:55:57,930 --> 01:56:00,180
research methods

1757
01:56:00,200 --> 01:56:05,510
what you can do for embedded methods is to use scaling factors in this opens

1758
01:56:05,510 --> 01:56:10,570
you the door to a wide variety of algorithm for feature selections

1759
01:56:10,590 --> 01:56:15,530
that combined with training

1760
01:56:15,550 --> 01:56:20,910
so before you know you know enormous space of all feature subsets we had one

1761
01:56:20,910 --> 01:56:25,010
for selecting the feature and zero for not selecting it

1762
01:56:25,030 --> 01:56:28,640
this corresponds to this vector of sigma

1763
01:56:28,660 --> 01:56:32,320
there is one for each feature one or present presence of the future and zero

1764
01:56:32,320 --> 01:56:33,840
absence of the future

1765
01:56:33,860 --> 01:56:39,050
so these are discrete indicators so you can replace the discrete indicators by

1766
01:56:39,070 --> 01:56:40,950
continuous scaling factors

1767
01:56:40,970 --> 01:56:46,110
that are now between zero and one instead of belonging to just zero one

1768
01:56:46,130 --> 01:56:47,340
sorry this

1769
01:56:47,340 --> 01:56:51,010
type here so that this is meant to be between zero and one

1770
01:56:51,030 --> 01:56:54,760
so the advantage of doing that

1771
01:56:54,780 --> 01:57:02,410
is that now we can perform a gradient descent on the scaling factors

1772
01:57:02,430 --> 01:57:08,320
sources do it we take a break in the middle so

1773
01:57:08,320 --> 01:57:12,720
so i think this would be a good time to break to make you present

1774
01:57:12,720 --> 01:57:17,840
in it and and the remaining part of the talk showing you how you can

1775
01:57:17,840 --> 01:57:19,610
design your own

1776
01:57:19,630 --> 01:57:23,970
algorithm for feature selection with your own training methods

1777
01:57:23,990 --> 01:57:29,590
using the scaling factors and then we'll be talking about causality

1778
01:57:29,610 --> 01:57:31,510
so in general

1779
01:57:31,530 --> 01:57:32,860
if you only care

1780
01:57:32,900 --> 01:57:37,860
to make predictions we don't care to know whether the variables cause the target or

1781
01:57:37,880 --> 01:57:39,910
the consequences of the target so

1782
01:57:39,930 --> 01:57:46,220
it's force predicted for disease to find genes that are

1783
01:57:46,240 --> 01:57:53,820
that have mutated and causing or disease or to find proteins that was concentration varies

1784
01:57:53,820 --> 01:57:58,200
in serum as a consequence of the this is like for example immune reactions

1785
01:57:59,090 --> 01:58:01,780
in some cases we will see when we care about

1786
01:58:01,800 --> 01:58:04,140
performing actions on the system

1787
01:58:04,160 --> 01:58:05,030
and c

1788
01:58:05,070 --> 01:58:08,660
what is going to be the results of our actions like for example delivering drugs

1789
01:58:08,660 --> 01:58:13,090
to a patient with respect to q and in that case we care about causality

1790
01:58:13,090 --> 01:58:17,380
and with the features are not closely related to the target so we'll talk to

1791
01:58:18,240 --> 01:58:21,740
but both of these problems in the second part of the presentation

1792
01:58:21,820 --> 01:58:23,450
a five minute break

1793
01:58:23,470 --> 01:58:32,780
so will come back and thank you for coming back after the test the the

1794
01:58:32,800 --> 01:58:34,070
first r

1795
01:58:34,360 --> 01:58:41,630
first like you to take questions if you have some questions on the first part

1796
01:58:41,640 --> 01:58:43,860
i've taken some questions offline

1797
01:58:45,200 --> 01:58:47,610
welcome to you know

1798
01:58:47,610 --> 01:58:49,380
as the more here

1799
01:59:21,430 --> 01:59:24,010
yes that's great one twenty

1800
01:59:24,070 --> 01:59:28,880
to details but i briefly spoke about the problem of multiple testing and the bonferroni

1801
01:59:28,880 --> 01:59:30,910
correction but a few

1802
01:59:30,930 --> 01:59:36,260
go the way of estimating the false discovery this becomes you whatever you can only

1803
01:59:36,970 --> 01:59:40,240
was the whole discovery

1804
01:59:40,260 --> 01:59:46,240
i didn't also give you know many details about this problem as the

1805
01:59:46,240 --> 01:59:48,910
the difference with the using

1806
01:59:49,220 --> 01:59:51,320
tabulated distributions

1807
01:59:51,340 --> 01:59:53,630
but there are some

1808
01:59:54,030 --> 01:59:58,800
delicate aspects about this whole mess and

1809
01:59:58,800 --> 02:00:03,630
in some experiments performed we not get always the same results in estimating the false

1810
02:00:03,630 --> 02:00:08,050
discovery rate is the probability distribution of the four methods

1811
02:00:08,070 --> 02:00:11,910
because in the sense the null distribution that you assume

1812
02:00:11,910 --> 02:00:23,370
this presentation is delivered by the stanford center for professional development

1813
02:00:23,430 --> 02:00:25,060
we are on the air

1814
02:00:26,280 --> 02:00:29,840
welcome one and all

1815
02:00:29,850 --> 02:00:32,590
and as i said on the TV when you're walking in but just to make

1816
02:00:32,590 --> 02:00:37,550
sure everybody knows this is to sixty one three transform and its applications

1817
02:00:37,560 --> 02:00:42,640
really transforms at all

1818
02:00:43,880 --> 02:00:53,630
and my name is brett good

1819
02:00:53,680 --> 02:00:58,530
circulating around are two

1820
02:00:58,570 --> 02:01:03,250
documents that give you information about the class there's a general description of the class

1821
02:01:03,250 --> 02:01:08,280
course information how we're going to proceed some basic bookkeeping

1822
02:01:08,370 --> 02:01:11,540
items out a little bit more about than just the second and also it's still

1823
02:01:11,540 --> 02:01:15,500
business schedule analysis a bit more about that in a second let me introduce our

1824
02:01:15,500 --> 02:01:17,600
partners in crime in this course

1825
02:01:17,610 --> 02:01:19,660
we have three courses fence

1826
02:01:19,710 --> 02:01:24,690
thomas john thomas was standard

1827
02:01:24,740 --> 02:01:32,530
this time serial regime aggarwal is that right

1828
02:01:32,570 --> 02:01:34,640
rushing enough

1829
02:01:36,380 --> 02:01:42,090
and the communities

1830
02:01:42,110 --> 02:01:44,270
OK so far

1831
02:01:44,290 --> 02:01:46,770
and correct

1832
02:01:48,840 --> 02:01:51,240
that's what i mean is everybody

1833
02:01:51,370 --> 02:01:55,820
thank you are now

1834
02:01:55,830 --> 02:02:00,160
and we will be setting up a time so the review sessions and so on

1835
02:02:00,160 --> 02:02:03,700
are also that will be that will be forthcoming we have a web page for

1836
02:02:03,700 --> 02:02:06,010
the course some of you may have already visited that but let me give the

1837
02:02:06,020 --> 02:02:08,950
and it's the addresses on the one that she one of the sheet is being

1838
02:02:08,950 --> 02:02:14,570
passed around but let me right now so you can be sure to visit and

1839
02:02:14,570 --> 02:02:17,830
register for the class because it is on the web page that you will find

1840
02:02:18,020 --> 02:02:22,710
a course handouts course information i will email people v of the web page or

1841
02:02:22,980 --> 02:02:24,210
have to be registered

1842
02:02:24,370 --> 02:02:27,250
i five to seven announcement of the class

1843
02:02:27,270 --> 02:02:30,700
post announcements in an email then that will be done through the web page and

1844
02:02:30,700 --> 02:02:33,780
you have to be registered on the web page in order to get those emails

1845
02:02:33,780 --> 02:02:37,460
i won't be doing it through access rights so it is at like many of

1846
02:02:37,460 --> 02:02:44,150
the other class http slash slash you were calling go

1847
02:02:44,160 --> 02:02:45,250
are there

1848
02:02:45,260 --> 02:02:49,150
class but stanford edu you can find it very easily

1849
02:02:49,350 --> 02:02:57,500
he knew slash each sixty one OK

1850
02:02:57,510 --> 02:03:02,970
go there if you have not already and registers of the class

1851
02:03:02,980 --> 02:03:07,250
right now let say little about the information that you have one is a little

1852
02:03:07,250 --> 02:03:15,070
bit more about the mechanics of talk more about the content and just the second

1853
02:03:15,090 --> 02:03:19,990
the little bit about the soldiers and schedule and the course through the syllabus is

1854
02:03:20,180 --> 02:03:23,040
as i said on the on the top in outline what we're going to be

1855
02:03:23,040 --> 02:03:27,550
doing a fairly accurately awarded we're going to be doing but it's not a contract

1856
02:03:27,550 --> 02:03:28,430
right so

1857
02:03:28,440 --> 02:03:31,810
there will be a natural inflow of the course as well as things go along

1858
02:03:31,970 --> 02:03:35,800
and when we get particular material or what we cover in order to do this

1859
02:03:35,800 --> 02:03:40,310
is more less say accurate but it is not written in stone

1860
02:03:40,360 --> 02:03:45,400
what you should use it for however is to play your reading so i things

1861
02:03:45,400 --> 02:03:48,130
will be much better for all of us if you

1862
02:03:48,140 --> 02:03:54,150
read along with the material as the syllabuses scheduled basically outlines are because there's there

1863
02:03:54,150 --> 02:03:57,990
are times when i want want to skip around little bit there are times and

1864
02:03:57,990 --> 02:04:00,320
i'm going to derive things there are times when i'm not going to derive things

1865
02:04:00,320 --> 02:04:03,920
and you'll get much more out of a lecture our time together if you've read

1866
02:04:03,920 --> 02:04:07,860
the material thoroughly before you come to class so that's one thing i ask you

1867
02:04:07,870 --> 02:04:12,940
to do we have two exams scheduled we have midterm exam and finally exam i'm

1868
02:04:12,940 --> 02:04:17,070
going to get the midterm asymmetry sami's is already actually on here

1869
02:04:17,080 --> 02:04:20,710
at least tentatively sort of toward the end of october

1870
02:04:20,750 --> 02:04:25,990
we'll have outside of class where that is will be down regular exam

1871
02:04:26,040 --> 02:04:29,850
but i want to do it for ninety minutes rather than fifteen fifteen minutes just

1872
02:04:29,850 --> 02:04:34,520
too short of time for class for material like this so it be nineteen examine

1873
02:04:34,520 --> 02:04:39,210
all schedule at several sessions outside class this is way usually that has many problem

1874
02:04:39,980 --> 02:04:43,610
has worked out or if everybody so have an alternate times and so on and

1875
02:04:43,610 --> 02:04:47,220
the final exam is scheduled by the registrar's office do not come to me right

1876
02:04:47,220 --> 02:04:50,920
before the final exam saying all i scheduled trip out of town i hope that

1877
02:04:50,920 --> 02:04:52,410
sort of problem

1878
02:04:52,420 --> 02:04:55,140
right you know what the dates are ahead of time

1879
02:04:55,180 --> 02:04:58,780
but also regular problem sets none of these things i'm saying should be new you've

1880
02:04:58,780 --> 02:05:02,580
been through the drill many times the first part of the problem sets are going

1881
02:05:02,580 --> 02:05:06,520
to be i i had of startling innovation last out of course

1882
02:05:06,530 --> 02:05:09,630
where i had about the problem set on monday and had to do the following

1883
02:05:09,630 --> 02:05:12,870
wednesday actually had like we can have to do the problem sets and there is

1884
02:05:12,870 --> 02:05:17,330
overlap between the two and people thought that was just a brilliant idea

1885
02:05:17,350 --> 02:05:20,350
so we can do that again this year except for the first problem so i

1886
02:05:20,350 --> 02:05:23,370
decided was not good policy to handle the very first problems at the very first

1887
02:05:23,370 --> 02:05:24,470
day of class

1888
02:05:24,510 --> 02:05:28,840
so i'll hand out on wednesday and post that also really suppose sure handed out

1889
02:05:29,000 --> 02:05:32,700
it will be available on wednesday and will be the following wednesday and again these

1890
02:05:32,700 --> 02:05:35,910
sorts of things are pretty routine for you and i'm sure even through many times

1891
02:05:36,360 --> 02:05:38,320
it will be practice

1892
02:05:38,330 --> 02:05:42,590
r although again not necessarily every time

1893
02:05:42,640 --> 02:05:47,810
without fail to have matlab problems on the homework one or two malapropism problems in

1894
02:05:47,810 --> 02:05:52,040
the homework so i'm going to the assumption that people have some experience with using

1895
02:05:52,040 --> 02:05:56,550
matlab that doesn't have to be terribly advanced and also accessed using matlab so if

1896
02:05:56,550 --> 02:06:00,700
you don't have experience using matlab and you have access to to matlab get some

1897
02:06:00,700 --> 02:06:04,520
experience and get some access

1898
02:06:04,530 --> 02:06:05,420
will be hard

1899
02:06:07,150 --> 02:06:10,770
we sell soul little bit about some things this is the course reader for the

1900
02:06:10,770 --> 02:06:14,210
course it's available at the bookstore

1901
02:06:14,340 --> 02:06:19,620
and also available on the course website doesn't have the problems and it has the

1902
02:06:19,860 --> 02:06:21,830
material that going to be covering class now

1903
02:06:22,150 --> 02:06:26,720
this is basically as it together set of lecture notes that have been using for

1904
02:06:26,720 --> 02:06:29,080
a number of years in the class nice tinker with it

1905
02:06:29,090 --> 02:06:30,670
every time i teach the class

1906
02:06:30,680 --> 02:06:34,690
but because it is stitched together set of lecture notes there the organisation sometimes a

1907
02:06:34,690 --> 02:06:37,810
little bit odd like you have an appendix in the middle of the chapter

1908
02:06:37,820 --> 02:06:40,370
and what that means is it was used to be an appendix to set to

1909
02:06:40,410 --> 02:06:42,890
particular lecture that went on that particular day

1910
02:06:42,910 --> 02:06:46,340
and there never got moved to anywhere else right so the organisation can be a

1911
02:06:46,340 --> 02:06:51,050
little bit funny you can help on this sorry that is if you find typos

1912
02:06:51,050 --> 02:06:56,520
if you find errors if you find things that are less than clear in their

1913
02:06:56,520 --> 02:06:58,960
in their wording if you want to if you if you have some other ideas

1914
02:06:58,960 --> 02:07:03,090
for example there are other explanations please tell me i'm all i'm working on this

1915
02:07:03,360 --> 02:07:07,320
i have to say that the whole because these these are written in this election

1916
02:07:08,110 --> 02:07:11,670
these are meant to be a good and i hope helpful companion for the class

1917
02:07:11,670 --> 02:07:15,080
that is they're meant to be read and they're meant to be used so i

1918
02:07:15,210 --> 02:07:18,890
you can help as generations of students in the past have helped try to refine

1919
02:07:18,890 --> 02:07:23,380
these and turn them into something that's really good accompaniment to the classes we go

1920
02:07:25,850 --> 02:07:29,660
one other thing that special this quarter is

1921
02:07:29,710 --> 02:07:34,470
the class is as always taped to an election the lectures are going to be

1922
02:07:34,470 --> 02:07:37,140
available to everybody but this time for the first time which is going to be

1923
02:07:37,140 --> 02:07:39,380
available to the world

1924
02:07:39,390 --> 02:07:44,580
i stanford is assigned an experimental bases which are competing with MIT here i think

1925
02:07:44,590 --> 02:07:48,330
to try to make some classes some of the materials for some classes available to

1926
02:07:48,330 --> 02:07:50,770
the world war i so the election is going to be everything's gonna be done

1927
02:07:50,770 --> 02:07:51,900
through the website

1928
02:07:51,920 --> 02:07:54,600
but instead of being stanford i d two

1929
02:07:54,620 --> 02:07:58,750
view the tape lectures i think anybody in the world can do these lectures of

1930
02:07:58,790 --> 02:08:01,220
the daunting have to watch my language

1931
02:08:01,420 --> 02:08:02,790
address well

