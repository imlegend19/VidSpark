1
00:00:00,000 --> 00:00:05,630
OK good morning ladies and gentlemen my name is craig right and this is listening

2
00:00:05,630 --> 00:00:10,870
to music the most basic course at the department of music has to offer its

3
00:00:10,870 --> 00:00:13,990
aim is to teach you how to listen to music

4
00:00:14,010 --> 00:00:17,370
wait a minute you say that's preposterous

5
00:00:17,380 --> 00:00:22,820
i listen to music all the time i got what my ipod i downloading mp

6
00:00:22,820 --> 00:00:25,840
three files continually swapping files

7
00:00:25,850 --> 00:00:28,010
i've got my car

8
00:00:28,030 --> 00:00:31,920
what we call the city will be aware is that tape that you can take

9
00:00:31,920 --> 00:00:36,350
your ipod plugged into the here this is serious system in your car got that

10
00:00:36,350 --> 00:00:40,650
i was musically dorm of my computer and bookstore wherever i bet i listened to

11
00:00:40,870 --> 00:00:44,520
a lot more music than you do you all go

12
00:00:44,530 --> 00:00:48,100
and you're right you probably do

13
00:00:49,350 --> 00:00:53,880
what kind of music are you listening to well probably pop and that's fine that's

14
00:00:53,880 --> 00:00:59,360
OK fair enough pop music but are you getting the most out of this particular

15
00:00:59,360 --> 00:01:04,930
experience are you getting the most out of your listening experience

16
00:01:04,950 --> 00:01:10,680
i contend that perhaps were not the gene that you are not maximizing the time

17
00:01:10,680 --> 00:01:13,170
using time most profitable

18
00:01:13,780 --> 00:01:17,770
how do i know that's what makes you think that you are not getting as

19
00:01:17,770 --> 00:01:22,820
much as you possibly can out of your music well experience to some degree but

20
00:01:22,820 --> 00:01:28,860
also experiment i just last weekend i have four children the last of the four

21
00:01:28,860 --> 00:01:31,810
has now turned seventeen so i said

22
00:01:31,850 --> 00:01:33,600
last weekend criticism

23
00:01:33,660 --> 00:01:36,900
always with the chris what you're listening to

24
00:01:36,920 --> 00:01:38,400
go with your body

25
00:01:38,470 --> 00:01:41,260
running my life so

26
00:01:41,440 --> 00:01:45,360
but come on let me listen to this i mean this is when you listen

27
00:01:45,450 --> 00:01:49,290
to so i listen to it so i hear you listen to this and tell

28
00:01:49,290 --> 00:01:50,790
me what you're hearing

29
00:01:50,800 --> 00:01:54,900
and what what was the tracking he was tracking the text he was tracking the

30
00:01:54,900 --> 00:01:56,770
beat of the peak

31
00:01:56,790 --> 00:01:59,810
i ask what what's the mode of the piece

32
00:01:59,860 --> 00:02:04,410
what's the metre the piece was the based you follow baseline you can you identify

33
00:02:04,690 --> 00:02:11,020
any chords in this particular piece nothing zero and this from a reasonably sophisticated kids

34
00:02:11,020 --> 00:02:14,990
had twelve years of serious cello lessons

35
00:02:15,000 --> 00:02:20,190
and that brings up i suppose a a point

36
00:02:20,210 --> 00:02:24,610
that although i don't know much about your music i think i can teach you

37
00:02:24,620 --> 00:02:30,410
a great deal about your music by using the paradigms of classical music so we

38
00:02:30,420 --> 00:02:35,620
talked a lot about classical music and his moats by beethoven it will be the

39
00:02:35,620 --> 00:02:40,770
locus of our cause how many of you already listen to classical music raise your

40
00:02:40,770 --> 00:02:45,460
hand OK great a lot of and that's wonderful i'd be interested to know gentlemen

41
00:02:45,460 --> 00:02:49,900
down here how do you do this is it's streaming off of your computer

42
00:02:49,940 --> 00:02:53,360
are you downloading mp three files and saving them how to tell me how do

43
00:02:53,360 --> 00:02:55,510
you do

44
00:02:55,570 --> 00:02:59,170
if you go to youtube are very interesting i should have known that but i

45
00:02:59,170 --> 00:03:02,240
didn't you go to youtube and you listen there anybody else do it in a

46
00:03:02,240 --> 00:03:04,620
different way

47
00:03:05,480 --> 00:03:10,370
on the radio OK that's interesting will come will come back to that point anything

48
00:03:10,370 --> 00:03:13,530
else anybody

49
00:03:13,570 --> 00:03:17,920
OK apparently these records that's wonderful sort of the old technology here but some of

50
00:03:17,920 --> 00:03:24,640
those are all recordings might be very very very good now here's a question for

51
00:03:24,640 --> 00:03:28,370
you know why would we want to listen to classical music

52
00:03:28,420 --> 00:03:29,140
why do

53
00:03:29,150 --> 00:03:33,170
why it move just answer the question from here those folks to raise your hand

54
00:03:33,320 --> 00:03:38,060
what to do when you're going to appear you're my sacrificial lamb this morning why

55
00:03:38,060 --> 00:03:42,990
do you like to listen why would you want to listen to classical music

56
00:03:43,040 --> 00:03:49,050
OK very interesting national public radio asked exactly this

57
00:03:49,070 --> 00:03:53,880
this question in a survey a year or so ago and they got the following

58
00:03:53,880 --> 00:03:55,930
principle responses that

59
00:03:55,940 --> 00:03:58,450
why people listen to classical music one

60
00:03:58,470 --> 00:04:00,370
it helps them relax

61
00:04:00,410 --> 00:04:05,520
and relieve stress so that this is this is but perhaps the principle which two

62
00:04:05,540 --> 00:04:12,140
helps is centre the mind allowing the listener to concentrate three classical music provides a

63
00:04:12,140 --> 00:04:18,190
vision of a better world vision of a better world refugee beauty

64
00:04:18,240 --> 00:04:23,520
of majesty perhaps of even of of love and sometimes at least for me personally

65
00:04:23,520 --> 00:04:26,810
it suggests that there might be something out there

66
00:04:26,870 --> 00:04:33,130
god or whatever bigger than ourselves and it as is the think sometimes

67
00:04:33,180 --> 00:04:34,320
think about

68
00:04:34,340 --> 00:04:41,960
the that's what i think these great fine arts do great literature poetry painting music

69
00:04:42,010 --> 00:04:42,730
they show

70
00:04:43,500 --> 00:04:48,010
human beings can be the capacity of the human spirit

71
00:04:48,020 --> 00:04:52,720
this suggests to us as indicated maybe there is something a larger spirit out there

72
00:04:52,720 --> 00:04:57,410
than ourselves and they get us to think me to think frequently about what i'm

73
00:04:57,410 --> 00:04:59,660
doing on this earth

74
00:04:59,680 --> 00:05:01,790
what are you doing this

75
00:05:01,810 --> 00:05:07,330
don't answer that one of my living on this earth with regard to this particular

76
00:05:07,330 --> 00:05:12,710
cause what am i trying to accomplish in here will

77
00:05:12,760 --> 00:05:18,420
maybe two things one change your personality

78
00:05:18,450 --> 00:05:23,800
change your personality i want to make you a richer person broader personal

79
00:05:23,840 --> 00:05:28,790
by instilling you with india

80
00:05:28,810 --> 00:05:34,870
a deep and abiding understanding of classical music so that's part of this not just

81
00:05:34,870 --> 00:05:40,820
here for you but for your life after yale i would hope that how you

82
00:05:40,820 --> 00:05:45,050
believe your life ten years now twenty years now thirty years now would have been

83
00:05:45,050 --> 00:05:50,440
significantly influenced by this particular experience in this cause

84
00:05:50,450 --> 00:05:57,040
and secondly if i'm successful in my teaching i will accomplish this second in here

85
00:05:57,350 --> 00:06:01,900
i will party you a love of classical music

86
00:06:03,520 --> 00:06:08,410
later on after you know your attendance at concerts by in one fashion or another

87
00:06:08,410 --> 00:06:14,240
downloading mp three files of items or whatever happens to be maybe being members of

88
00:06:14,240 --> 00:06:18,570
the local symphony board opera company something like that may be giving music lessons to

89
00:06:18,570 --> 00:06:21,420
your children you will become

90
00:06:21,470 --> 00:06:27,120
the pervez areas of classical music thereafter you the intelligentsia the next generation will be

91
00:06:27,120 --> 00:06:31,130
those that preserve this great treasures

92
00:06:31,210 --> 00:06:37,850
of western culture and it is a great treasure of western culture OK how we

93
00:06:37,850 --> 00:06:41,600
going to do all of this how we can accomplish these two things on our

94
00:06:41,750 --> 00:06:43,450
list of agenda here

95
00:06:43,460 --> 00:06:47,540
what are the mechanics of course to do all get the syllabus

96
00:06:47,590 --> 00:06:52,530
everybody's got syllabus i should have grown up here but i didn't doesn't matter first

97
00:06:52,530 --> 00:06:57,110
three four weeks so i'll be following the elements of music rhythm

98
00:06:57,120 --> 00:06:59,130
melody and harmony

99
00:06:59,150 --> 00:07:00,370
and then test

100
00:07:00,380 --> 00:07:06,280
next we will deal with what's the what arguably the the single most important thing

101
00:07:06,280 --> 00:07:10,660
when we listen to any piece of music and that is its musical form

102
00:07:10,670 --> 00:07:14,500
here's a question for i was thinking about this the other day so a preparing

103
00:07:14,500 --> 00:07:20,490
a lecture for today what's the most common type of musical form in pop music

104
00:07:20,490 --> 00:07:25,000
we can have not just objects but unknown relations as well

105
00:07:25,020 --> 00:07:28,920
as long as we're willing to go to second order markov logic

106
00:07:28,950 --> 00:07:33,200
in second order markov logic we allow predicates that we all variables that range over

107
00:07:33,200 --> 00:07:37,770
predicates as well as variables that range over objects

108
00:07:37,780 --> 00:07:38,780
and without

109
00:07:38,850 --> 00:07:42,060
in this in essence just by reading the same from as that we had before

110
00:07:42,150 --> 00:07:47,960
know they get grounded with predicates as always with objects and i can actually resolved

111
00:07:47,980 --> 00:07:52,030
relations like for example in my text i could have and is the friend of

112
00:07:52,030 --> 00:07:57,580
bob cannonball bob who are friends and her boyfriend and strand but these are all

113
00:07:57,580 --> 00:07:59,210
instances of the simulation

114
00:07:59,230 --> 00:08:01,390
and that could result

115
00:08:01,410 --> 00:08:03,530
and i won't go into the details of that here

116
00:08:03,540 --> 00:08:07,890
but there's actually a paper that stanley kok is going to be presenting tomorrow cost

117
00:08:07,890 --> 00:08:12,040
it good predicate invention and so you're more than welcome to go there to to

118
00:08:12,040 --> 00:08:13,600
see in more detail how

119
00:08:13,640 --> 00:08:15,560
know the things they can do

120
00:08:15,610 --> 00:08:19,250
with with second order markov logic

121
00:08:19,270 --> 00:08:23,580
so it's getting pretty sure let me just mention the last couple of things

122
00:08:23,640 --> 00:08:28,850
he his his another even more ambitious application which is robot that

123
00:08:28,860 --> 00:08:34,240
anderson continues to illustrate very briefly i can handle continuous domains in markov logic which

124
00:08:34,240 --> 00:08:36,230
is something that we haven't seen yet

125
00:08:36,570 --> 00:08:39,610
so we know about nothing as many of you probably know

126
00:08:39,950 --> 00:08:44,780
the goal is to build a map of the environment that the robot is moving

127
00:08:45,360 --> 00:08:49,950
so the inputs typically is laser range finder segments

128
00:08:50,700 --> 00:08:53,350
given by the start and end point coordinates

129
00:08:53,360 --> 00:08:57,850
and the outputs are labels of the segments like wall door there

130
00:08:58,280 --> 00:09:01,240
and we also want to assign wall segments to towards

131
00:09:01,250 --> 00:09:04,210
and also to figure out what the real position of the walls is so we

132
00:09:04,210 --> 00:09:07,460
have a bunch of noisy segments that don't quite aligned we want to figure out

133
00:09:07,460 --> 00:09:11,910
exactly where the wall is so here's his the real example of

134
00:09:11,920 --> 00:09:14,480
publicly available datasets for robot mapping

135
00:09:14,490 --> 00:09:19,140
you can see there a corridor here and corridor here the red segments of walls

136
00:09:19,160 --> 00:09:20,890
the green ones outdoors

137
00:09:21,490 --> 00:09:24,990
the ones others and so our goal is to actually figure out that this is

138
00:09:24,990 --> 00:09:25,890
the environment that we're in

139
00:09:26,200 --> 00:09:30,740
just from the coordinates of the of the range finder segments

140
00:09:30,900 --> 00:09:32,640
so what we do this

141
00:09:33,650 --> 00:09:34,940
in order to do this

142
00:09:34,950 --> 00:09:39,160
we need to talk about numeric properties of objects and numeric features

143
00:09:39,190 --> 00:09:42,830
which is something that we haven't done so far because after all logic is just

144
00:09:42,830 --> 00:09:47,140
about boolean variables and building features so how can we do that

145
00:09:47,150 --> 00:09:49,990
well it's very simple at least syntactically

146
00:09:50,120 --> 00:09:53,790
because first order logic already has numerous times in the syntax for can have a

147
00:09:53,790 --> 00:09:58,020
function that's numeric order constant or variable that's numeric so that's not a big deal

148
00:09:58,270 --> 00:10:00,740
so all i need to do is to allow

149
00:10:00,750 --> 00:10:04,110
the numeric properties to be nodes like for example i could have no that's the

150
00:10:04,110 --> 00:10:07,070
length of x or the distance between x and y

151
00:10:07,150 --> 00:10:12,410
and then i will allow numeric terms in the standard syntax of first-order logic

152
00:10:12,420 --> 00:10:17,580
maybe with some extensions like infix notation and whatnot as features like for example

153
00:10:17,600 --> 00:10:22,070
i could have the feature minus the length of x minus five square

154
00:10:22,120 --> 00:10:27,980
this feature is basically saying that link has a gas distribution with mean five

155
00:10:27,990 --> 00:10:31,230
and variance one over two times the weight of this form

156
00:10:31,990 --> 00:10:35,870
because if you think about it if you exponentially this times away what you have

157
00:10:35,870 --> 00:10:38,100
is exactly agustin distribution

158
00:10:38,700 --> 00:10:42,310
and now we can allow a very useful piece of syntactic sugar

159
00:10:42,360 --> 00:10:46,530
which is to just right off equals better where alpha and beta are arbitrary numeric

160
00:10:46,530 --> 00:10:50,980
expressions as a shorthand for minus of minus better square

161
00:10:50,990 --> 00:10:56,270
for example i can think of x equals five as a short for this

162
00:10:56,290 --> 00:11:00,480
and what this is saying is that the length of an object is typically five

163
00:11:00,490 --> 00:11:03,780
they can vary from it with the gas distribution

164
00:11:03,790 --> 00:11:06,770
and you don't have to explicitly said that the distribution is gas we just like

165
00:11:06,780 --> 00:11:07,690
you do this

166
00:11:07,700 --> 00:11:10,540
because this is what you're going to be doing ninety percent of the time is

167
00:11:10,960 --> 00:11:14,740
putting us in distributions of various things and combinations of things we just like you

168
00:11:14,740 --> 00:11:16,160
write in this way

169
00:11:16,190 --> 00:11:19,100
OK so using this syntax and now

170
00:11:19,110 --> 00:11:22,120
what is the set of this sets up

171
00:11:22,140 --> 00:11:26,940
where some of the variables are numeric some of the features are numeric but everything

172
00:11:26,940 --> 00:11:31,780
else goes through you need to extend the inference algorithms they the numeric case but

173
00:11:31,780 --> 00:11:35,360
the weight learning is still the same the former seven change so again at least

174
00:11:35,360 --> 00:11:38,190
a high level this is a straightforward extension

175
00:11:38,270 --> 00:11:40,000
so here's another way

176
00:11:40,160 --> 00:11:43,310
that the to the state of the art in robot mapping

177
00:11:43,330 --> 00:11:47,900
that we've recently developed this just as a bunch of things there a natural

178
00:11:47,900 --> 00:11:50,700
and this functions need to be small

179
00:11:50,710 --> 00:11:57,750
with respect to the underlying geometry of our data

180
00:11:58,420 --> 00:12:02,190
what does that mean so this is just on the probabilistic setting

181
00:12:02,200 --> 00:12:05,890
we want to learn so why is the space of all labels so we can

182
00:12:05,890 --> 00:12:08,880
just thing minus one and one for simplicity

183
00:12:09,380 --> 00:12:16,180
probability with some probability distribution on x y pairs so each x some probability and

184
00:12:16,180 --> 00:12:21,390
given the probability there is some probability that the blue and some probability that's right

185
00:12:21,400 --> 00:12:24,150
and the same for regression except to say really

186
00:12:29,700 --> 00:12:32,300
we have the conditional distributions

187
00:12:32,310 --> 00:12:37,110
p of y given x as i said before the probability of blue given the

188
00:12:37,110 --> 00:12:40,790
point sources probability of red given the point and i don't know if you can

189
00:12:40,790 --> 00:12:42,640
see that and put it in

190
00:12:43,590 --> 00:12:47,130
and probability

191
00:12:48,020 --> 00:12:55,180
and the marginal distribution which is somehow the distribution of access when you forget about

192
00:12:55,200 --> 00:13:00,640
the labels so the marginal distribution is the distribution of all access and the conditional

193
00:13:00,640 --> 00:13:02,710
distribution given x

194
00:13:03,670 --> 00:13:04,780
the label

195
00:13:04,790 --> 00:13:07,830
what is the distribution of labels

196
00:13:07,840 --> 00:13:08,520
OK so the

197
00:13:08,610 --> 00:13:12,340
two important objects and the assumption this

198
00:13:12,450 --> 00:13:16,130
if that the conditional

199
00:13:16,170 --> 00:13:23,290
if most respected the marginal so the conditional probability distribution does not change between points

200
00:13:23,290 --> 00:13:34,090
does not change too much between points which calls on the manifold itself

201
00:13:34,100 --> 00:13:36,190
and if you go back here

202
00:13:36,200 --> 00:13:38,890
you will see that

203
00:13:38,940 --> 00:13:41,600
it is much more likely that this red

204
00:13:41,640 --> 00:13:44,320
because really we don't want to class

205
00:13:44,390 --> 00:13:48,370
labels to change and it would have to go to blue and black to red

206
00:13:49,210 --> 00:13:53,680
and this is somehow much sharper transition in carrying out of this thread mostly turns

207
00:13:53,680 --> 00:13:57,920
it into blue here

208
00:13:58,890 --> 00:14:02,940
so now now what is small for now i said that i want my class

209
00:14:02,940 --> 00:14:05,440
conditional to be

210
00:14:05,470 --> 00:14:09,730
most business back to the marginal to with respect to the structure of the data

211
00:14:09,820 --> 00:14:11,410
now what does this mean

212
00:14:11,430 --> 00:14:17,390
so this one can say the following so when what for example given the function

213
00:14:17,450 --> 00:14:22,900
and it's just assume material for well functions when this function nice and small well

214
00:14:22,900 --> 00:14:24,630
the function is nice and small

215
00:14:26,860 --> 00:14:28,060
when you

216
00:14:28,080 --> 00:14:33,630
take all possible values around this function and you look at all differences and this

217
00:14:34,480 --> 00:14:36,090
this quantity

218
00:14:36,150 --> 00:14:38,070
the integral over a little sphere

219
00:14:38,080 --> 00:14:40,120
of the difference is not too large

220
00:14:40,190 --> 00:14:44,470
so for each point i want to the point i want the values of the

221
00:14:44,470 --> 00:14:49,250
function around this points to be pretty similar to the values of the function at

222
00:14:49,260 --> 00:14:50,880
the point at

223
00:14:50,890 --> 00:14:55,850
so this is somehow an intuitive notion of most is i don't want to change

224
00:14:55,900 --> 00:15:00,170
so locally i don't want to function to change globally i don't care

225
00:15:00,220 --> 00:15:04,270
and i integrated of all points so if function is nice and smooth if it

226
00:15:04,270 --> 00:15:07,330
doesn't change locally for each point

227
00:15:07,340 --> 00:15:11,430
and i have to do with way this problem which from and this is basically

228
00:15:11,430 --> 00:15:16,430
the same as the gradient the limit of this some constant from is actually the

229
00:15:17,760 --> 00:15:20,560
so the gradient is a measure of smoothness

230
00:15:20,610 --> 00:15:23,080
as part the size the previous talk

231
00:15:23,120 --> 00:15:27,290
and this is closely related to the laplace operator

232
00:15:27,350 --> 00:15:30,540
fuller plus and this quantity

233
00:15:32,200 --> 00:15:36,580
this is in the product on health tools of this is integral

234
00:15:36,730 --> 00:15:44,560
times LPF the LP the plus measures how smooth functions

235
00:15:47,530 --> 00:15:49,920
and the manifold assumption

236
00:15:49,940 --> 00:15:52,010
can be said that

237
00:15:52,020 --> 00:15:55,360
class conditional p one given x

238
00:15:55,370 --> 00:15:58,310
if most in this sense so when you

239
00:15:58,370 --> 00:16:02,500
integrate the gradient of class conditional you get the smaller

240
00:16:02,510 --> 00:16:07,180
that that's all the rest

241
00:16:07,230 --> 00:16:09,610
OK so that's the key

242
00:16:09,620 --> 00:16:12,420
that's the key somehow idea

243
00:16:12,510 --> 00:16:17,290
the integral of class can of gradient of course conditional has to be small

244
00:16:23,360 --> 00:16:27,230
effect can be pretty much any probability distribution of course

245
00:16:27,280 --> 00:16:31,270
this quantity is need to make sense of some conditions on it but

246
00:16:31,290 --> 00:16:35,840
i mean i'm not putting any strong conditions

247
00:16:35,890 --> 00:16:39,920
so p of x you have some distribution of you have some joint distribution of

248
00:16:40,090 --> 00:16:44,090
digits and then for each thing you know if it looks like this it's very

249
00:16:44,090 --> 00:16:47,520
unlikely to be in line so that zero but there are some things which you

250
00:16:47,890 --> 00:16:52,820
like before you can imagine for nine days sometimes extremely close so you really cannot

251
00:16:53,360 --> 00:16:57,310
so that presumably the fifty fifty

252
00:16:57,360 --> 00:17:02,660
OK so now let me just remind you about the graph plus and so i

253
00:17:02,660 --> 00:17:06,910
will describe the algorithmic framework and

254
00:17:06,960 --> 00:17:11,840
in the graph the algorithmic framework is based on the notion of the graph laplacian

255
00:17:12,280 --> 00:17:16,510
graph laplacian is the following think of this summer from graph which has nothing to

256
00:17:16,510 --> 00:17:17,910
do with

257
00:17:19,070 --> 00:17:20,370
to give you an example

258
00:17:20,370 --> 00:17:22,800
did you deduced from the feature mapping phi

259
00:17:22,810 --> 00:17:28,570
and the metrics this is cleavage the metrics

260
00:17:28,570 --> 00:17:32,190
so suppose there actually prelearned classifiers if p

261
00:17:32,200 --> 00:17:36,700
and actually if you can learn about adding technique using the

262
00:17:36,720 --> 00:17:42,640
trinidad from either the author domain all the time dimension of from both domains

263
00:17:42,650 --> 00:17:45,340
so in the experiments way

264
00:17:45,390 --> 00:17:50,820
the if piece learning by this man with the labelled training data from both domains

265
00:17:50,880 --> 00:17:54,330
and we propose the target decision function like this

266
00:17:54,340 --> 00:17:55,520
in this equation

267
00:17:55,550 --> 00:18:00,450
because is linear combination coefficients and i don't i fix is called the the perturbation

268
00:18:00,450 --> 00:18:05,620
function and then the target decision function the first term can be considered as prior

269
00:18:07,150 --> 00:18:12,870
and because of the good interpretability of the multiple kernel learning we model the perturbation

270
00:18:12,870 --> 00:18:17,860
function derived by using multiple kernel kernel learning so you might you might have occurred

271
00:18:17,860 --> 00:18:23,490
our kernel learning MKL the kernel is assumed to be linear combination of multiple base

272
00:18:23,800 --> 00:18:25,000
kernels k

273
00:18:25,090 --> 00:18:30,130
and came is deduced from the feature map phi and so was MKL we can

274
00:18:30,130 --> 00:18:34,700
we can rewrite the in the distance as a function omega d which equals two

275
00:18:34,700 --> 00:18:37,120
the transpose of h times

276
00:18:37,130 --> 00:18:38,210
that d

277
00:18:38,240 --> 00:18:45,550
so here the method is composed of the linear combination coefficients of the countries

278
00:18:45,570 --> 00:18:51,240
so finally we propose our adaptive multiple kernel learning has the following of optimisation problem

279
00:18:51,280 --> 00:18:53,370
so in the objective function

280
00:18:53,390 --> 00:18:54,470
that's two terms

281
00:18:54,500 --> 00:18:59,300
the first term is and determine it minimizes the mismatch between the two domains

282
00:18:59,340 --> 00:19:02,200
and the second term is the structural risk functional

283
00:19:02,210 --> 00:19:06,610
and in the traditional MKL the training data are assumed to be from the same

284
00:19:06,610 --> 00:19:09,070
domain however our method

285
00:19:09,120 --> 00:19:14,660
the training data from different domains and our method is actually attempted adapted from one

286
00:19:14,660 --> 00:19:17,160
domain to the other

287
00:19:17,180 --> 00:19:22,660
and to solve this optimisation problem and the structural risk functional jt is really retains

288
00:19:22,660 --> 00:19:23,820
dual form

289
00:19:23,830 --> 00:19:26,830
and for that we can we get to the

290
00:19:26,860 --> 00:19:32,500
that might have occurred algorithm iteratively solves the linear combination coefficients d

291
00:19:32,950 --> 00:19:37,410
and the dual variables alpha in the dual form of g

292
00:19:37,440 --> 00:19:43,250
so here it is several existing customer methods the first one is the future applications

293
00:19:43,260 --> 00:19:45,360
so far it simply

294
00:19:45,390 --> 00:19:47,030
augments the features

295
00:19:47,060 --> 00:19:51,760
and second method according to the domain transfer SVM DTSVM and this is our previous

296
00:19:51,760 --> 00:19:56,210
work by using this method there's no prior information used

297
00:19:56,240 --> 00:20:01,120
and the third method is adaptive SVM and this method is similar to ours except

298
00:20:01,120 --> 00:20:02,870
for two major differences

299
00:20:02,950 --> 00:20:08,190
the first one that the linear combination quite coefficient gamma p these predefined in that

300
00:20:09,380 --> 00:20:14,120
and the provision function is model by a simple MKL

301
00:20:14,130 --> 00:20:16,980
the symbol is mentioned

302
00:20:16,990 --> 00:20:21,790
so here comes the experiment parts for the that it we use one hundred ninety

303
00:20:21,790 --> 00:20:27,080
five consumer videos and in one hundred six it connected by our ourselves and also

304
00:20:27,080 --> 00:20:30,060
from kodak consumer video benchmark that's it

305
00:20:30,080 --> 00:20:36,390
there are six events that wedding birthday picnic parade show and sports

306
00:20:36,390 --> 00:20:41,960
and for training data we read simple three media was prevented from consumer videos and

307
00:20:41,960 --> 00:20:46,960
all the web videos also considered as the training and the rest consumer videos are

308
00:20:46,960 --> 00:20:49,770
considered as the test data

309
00:20:49,790 --> 00:20:55,620
and the experiments we use two types of features that time feature on same feature

310
00:20:55,620 --> 00:21:00,140
and we also use four types of kernels that does in that that's an inverse

311
00:21:00,140 --> 00:21:02,530
square distance and inverse distance

312
00:21:02,590 --> 00:21:09,750
and we compare our aligned space time pyramid matching with the loudest whisper matching

313
00:21:09,940 --> 00:21:15,800
this two tables tables show the results for some features and space time features respectively

314
00:21:15,840 --> 00:21:21,410
from the table the observed that the space time pattern matching is better than the

315
00:21:21,460 --> 00:21:24,430
loudest whisper matching label one for every

316
00:21:25,420 --> 00:21:31,220
so we call that this is our proposed to tackle decision function

317
00:21:31,230 --> 00:21:35,750
in nineteen dollars payments we use it base kernels in total

318
00:21:35,810 --> 00:21:41,040
there are from two pyramid levels two types of features five kernels parameters and four

319
00:21:41,040 --> 00:21:43,080
types of kernels

320
00:21:43,100 --> 00:21:49,370
and the way we treat eighty base classifiers based on this data base kernels and

321
00:21:49,370 --> 00:21:54,730
for each level and each type of feature web content based classifiers learned by experience

322
00:21:54,850 --> 00:22:00,110
and then we reached this content based classifiers to get the average cost for a

323
00:22:00,110 --> 00:22:05,210
total there are four average classifiers and then use this average cost per to construct

324
00:22:05,210 --> 00:22:07,240
the prelearned classifiers p

325
00:22:07,250 --> 00:22:14,740
now here we compare our method with south across the minimum methods three settings

326
00:22:14,760 --> 00:22:17,430
you said a usage SIFT features

327
00:22:17,440 --> 00:22:23,160
basically we will use space time features a post that both features i used

328
00:22:23,760 --> 00:22:25,620
this figure shows the

329
00:22:25,630 --> 00:22:31,710
average precision of all methods for each sheet for each event

330
00:22:33,460 --> 00:22:39,500
and do you have any sitting c which uses both features our method outperforms the

331
00:22:39,500 --> 00:22:42,170
other methods in four out of six events

332
00:22:42,270 --> 00:22:44,720
for example taking parade for example

333
00:22:44,730 --> 00:22:48,280
our method which is seventy five point seven percent

334
00:22:48,290 --> 00:22:54,490
average precision compared with the second best result sixty two point six fifty application

335
00:22:54,630 --> 00:23:01,660
which is thirteen thirteen point five percent improvement

336
00:23:01,760 --> 00:23:06,110
and were use mean average precision in the final evaluation

337
00:23:06,150 --> 00:23:11,560
we observe that in the multiple kernel learning based methods the performance performance of city

338
00:23:12,300 --> 00:23:15,940
it is much better than the then those of sitting in b

339
00:23:15,970 --> 00:23:21,690
so this might have occurred in the current learning based methods can actually better fuse

340
00:23:21,690 --> 00:23:26,230
SIFT features and the space time features and they can also handle the noise in

341
00:23:26,230 --> 00:23:29,550
the loose labels to some extent

342
00:23:29,620 --> 00:23:35,110
and for each method we find the best result among all the three settings and

343
00:23:35,110 --> 00:23:40,380
based on the best results we compared our method is all other methods and here

344
00:23:40,380 --> 00:23:45,760
the relative improvement of our method over the other methods we can see that

345
00:23:45,860 --> 00:23:51,970
methods significantly outperform outperforms the other methods

346
00:23:51,990 --> 00:23:53,230
so in conclusion

347
00:23:53,240 --> 00:23:58,620
we propose a new event recognition framework for consumer videos by leveraging a large number

348
00:23:58,620 --> 00:24:01,050
of loosely labeled web videos

349
00:24:01,080 --> 00:24:05,000
and we develop a new aligned space time pyramid matching method

350
00:24:05,010 --> 00:24:06,410
and also we her

351
00:24:06,480 --> 00:24:12,490
present a new cross domain learning method adaptive multiple kernel learning which handles the mismatch

352
00:24:12,490 --> 00:24:13,270
between the data

353
00:24:13,290 --> 00:24:18,600
distributions of the consumer video domain and the web video domain

354
00:24:18,610 --> 00:24:20,650
and that's all for my presentation thank you

355
00:24:26,010 --> 00:24:30,240
other questions

356
00:24:30,250 --> 00:24:34,260
yes please go to like if you can

357
00:24:34,430 --> 00:24:41,290
so it seems to me that approach two components the first component is carried matching

358
00:24:41,340 --> 00:24:44,100
the second one is a political party

359
00:24:44,120 --> 00:24:49,080
so how do these two points i mean in the first component the matching score

360
00:24:49,290 --> 00:24:55,680
the right and then what features to use for the multiple kernel and actually

361
00:24:56,610 --> 00:24:57,890
we just use that

362
00:24:57,890 --> 00:25:04,420
limit theorem intuitions stuff now all these properties of correlation got by

363
00:25:04,510 --> 00:25:10,080
lots of methods but we we saw this volatility clustering means immediately that what we

364
00:25:10,080 --> 00:25:14,830
read about in fowler where events are

365
00:25:14,920 --> 00:25:16,190
are really

366
00:25:16,360 --> 00:25:18,920
stationary usually

367
00:25:18,940 --> 00:25:24,530
cannot be used to we need other techniques into common technique study trended fluctuation analysis

368
00:25:25,660 --> 00:25:30,890
which is the site and over eight hundred times is used in many fields

369
00:25:30,940 --> 00:25:33,500
and the newer method bicarbonate

370
00:25:33,510 --> 00:25:39,380
in our collaborators could be in training moving average the essence of which is here

371
00:25:39,420 --> 00:25:42,470
the the jagged curve is the stock price

372
00:25:42,490 --> 00:25:43,410
and the

373
00:25:43,430 --> 00:25:49,580
smooth curve is just moving average average over the previous in events and when you

374
00:25:49,580 --> 00:25:54,820
look at the at the little sausages that are formed like this red sausage

375
00:25:55,870 --> 00:26:03,120
bush there was like a train the worst there formed you abide by the intersection

376
00:26:03,120 --> 00:26:05,930
of the moving average and the stock price itself

377
00:26:05,990 --> 00:26:11,770
you have some objects that contain information important things in these little force

378
00:26:11,780 --> 00:26:17,470
good information and the two is used to measure the things are the lifetime that

379
00:26:17,900 --> 00:26:24,290
how long this along this forced is and its total area by how much how

380
00:26:24,290 --> 00:26:25,540
many calories

381
00:26:25,650 --> 00:26:31,070
in his first and and both of these scaling ways to give the critical exponents

382
00:26:31,110 --> 00:26:32,290
OK so

383
00:26:32,320 --> 00:26:37,160
all of this is the end if i'm going to talk about with making log

384
00:26:37,160 --> 00:26:38,210
log plots

385
00:26:38,230 --> 00:26:41,680
and talk about something different now

386
00:26:41,680 --> 00:26:45,610
and something that economies has apparently due respect

387
00:26:45,650 --> 00:26:49,120
and that's in part because they didn't already doing

388
00:26:49,140 --> 00:26:52,940
and the reason i do it this is a method random matrix theory that was

389
00:26:52,940 --> 00:26:55,840
popularized in physics by eugene victor

390
00:26:55,850 --> 00:27:02,080
from budapest great nobel laureate in physics not the colours the very and then he

391
00:27:02,080 --> 00:27:05,740
was confronted in the thirties with some puzzles in nuclear physics

392
00:27:05,770 --> 00:27:10,330
and he formulated an approach to the puzzle

393
00:27:10,330 --> 00:27:14,250
using random matrix theory what is random matrix theory in this problem

394
00:27:14,270 --> 00:27:19,940
it is to take the cross correlations of the change of price every pair stocks

395
00:27:19,940 --> 00:27:21,770
so this is coca-cola

396
00:27:21,930 --> 00:27:27,420
this is pepsi-cola and they are generally correlated one goes up the other goes up

397
00:27:27,630 --> 00:27:29,490
i do we have a positive

398
00:27:29,530 --> 00:27:30,810
elements of the

399
00:27:30,810 --> 00:27:35,950
matrix element that has coca-cola because the classical the right

400
00:27:35,980 --> 00:27:41,660
you do this false a thousand stocks whatever thousands of a good number include

401
00:27:41,710 --> 00:27:44,710
almost every company that has a lot of data

402
00:27:44,730 --> 00:27:50,480
and then you get a big thousand five thousand matrix diagonalizes find i can values

403
00:27:50,720 --> 00:27:55,460
the graph the histogram of the eigen values and what you find is that ninety

404
00:27:55,460 --> 00:27:57,710
eight percent of the site and values

405
00:27:59,870 --> 00:28:01,600
under the curve

406
00:28:01,610 --> 00:28:04,080
this exactly what eugene victor

407
00:28:04,180 --> 00:28:10,420
derived would occur if the correlations were purely random man and that nothing more than

408
00:28:10,420 --> 00:28:13,160
random there will always be apparent correlation right

409
00:28:13,190 --> 00:28:15,860
if i leave this room in the same time

410
00:28:15,870 --> 00:28:21,280
you shlomo leaves this rule we could say we are leaving because we're correlated question

411
00:28:21,280 --> 00:28:25,130
what happened to collaborate or we could say is just sheer chance and there's no

412
00:28:25,130 --> 00:28:26,100
way to know

413
00:28:26,120 --> 00:28:31,930
almost no way to know but statistically you can by forming this large matrix of

414
00:28:32,250 --> 00:28:37,200
everyone is leaving the room together and you find ways that most things are pure

415
00:28:37,200 --> 00:28:43,420
chance so the fluctuations and stocks almost all pure chance but there are these twenty

416
00:28:43,440 --> 00:28:46,640
o two thousand DVD i get values

417
00:28:46,640 --> 00:28:50,780
and what can we say about what every igon value has an i can factor

418
00:28:50,820 --> 00:28:54,900
and we can look at the components of that i mean vector here is just

419
00:28:54,900 --> 00:28:56,480
the right and that

420
00:28:56,510 --> 00:28:59,380
her so this is just the

421
00:28:59,380 --> 00:29:06,130
the twenty deviating eigen values studied by vessel key player rules and go because standards

422
00:29:06,130 --> 00:29:07,250
obviously got there

423
00:29:07,270 --> 00:29:10,460
and and their collaborators

424
00:29:10,540 --> 00:29:13,140
and the remarkable thing is that

425
00:29:15,090 --> 00:29:21,170
i can vectors are dominated by stocks from individual business sectors

426
00:29:21,200 --> 00:29:23,160
which means that the

427
00:29:23,160 --> 00:29:27,210
the problem that we physicists always have an economics we don't know what the three-letter

428
00:29:28,440 --> 00:29:33,420
except that you decided that we believe in bank of america i can never remember

429
00:29:33,440 --> 00:29:37,930
and all the little ones we don't know but if we came from mars we

430
00:29:37,930 --> 00:29:42,690
did nothing other than this we will find exactly what they are because you would

431
00:29:42,690 --> 00:29:45,200
be all code dealing with transportation

432
00:29:45,210 --> 00:29:49,180
this all paper this with drugs and so forth and so on so i mean

433
00:29:49,190 --> 00:29:53,760
that there is a means there is a way of finding coal movement of

434
00:29:53,780 --> 00:29:58,430
of stocks to belong to the same sector and that in turn leads to practical

435
00:29:59,390 --> 00:30:01,920
that i'm trying to sell to bloomberg

436
00:30:01,980 --> 00:30:08,530
not the mayor bloomberg company bloomberg new york which as you probably know makes it

437
00:30:08,530 --> 00:30:13,330
he he's the richest man in new york and made his many billions of dollars

438
00:30:13,920 --> 00:30:20,700
trivial sort of physicist dream idea putting software on every traders that with click on

439
00:30:20,700 --> 00:30:25,500
the keyboard can tell you anything but he doesn't do this now we can diagonalizable

440
00:30:25,500 --> 00:30:30,560
matrices so fast you can make a graph of the companies and when stock price

441
00:30:30,560 --> 00:30:33,640
that deviate from where it belongs

442
00:30:34,090 --> 00:30:35,880
the trader comes out of

443
00:30:35,890 --> 00:30:37,710
it has to be a reason for that

444
00:30:37,770 --> 00:30:40,540
because this is not just random

445
00:30:40,560 --> 00:30:45,620
and the trader can take appropriate action may be sounded by more try to figure

446
00:30:45,620 --> 00:30:46,590
out why

447
00:30:47,510 --> 00:30:50,160
now now we come to the last thing

448
00:30:50,310 --> 00:30:55,680
which is i introduced by showing you the slide thomas looks

449
00:30:55,760 --> 00:31:00,990
showed yesterday which is the phase transition

450
00:31:01,000 --> 00:31:08,220
where it starts the idea that you should somebody mumbles that you you might want

451
00:31:08,220 --> 00:31:10,500
to buy the stock of access

452
00:31:10,510 --> 00:31:14,810
and i guess in this case it looks like they want to sell it to

453
00:31:14,820 --> 00:31:16,670
sell the stock xl

454
00:31:16,670 --> 00:31:22,680
and then this amplifies cooperative way just like the first order phase transition metastability

455
00:31:22,700 --> 00:31:27,820
and then everybody cells and cells and cells and cells and goodbye goodbye goodbye and

456
00:31:27,820 --> 00:31:29,740
then some little creepy guy

457
00:31:30,150 --> 00:31:35,900
he says by it and that amplifies and everyone buys so this is the cart

458
00:31:35,920 --> 00:31:40,690
and nothing more of it is inaccurate cartoon roughly what happened so we have to

459
00:31:40,690 --> 00:31:47,120
take seriously we physicists being so simple-minded are allowed to take simple things seriously

460
00:31:47,130 --> 00:31:50,280
we have to take my what does this tell us

461
00:31:50,320 --> 00:31:53,420
what is the time i have no idea for sure but i would like it

462
00:31:53,430 --> 00:31:59,290
to be as priests price tobias price spent part of his phd with us

463
00:31:59,310 --> 00:32:04,770
and he came not only with his good brains but it came with

464
00:32:04,790 --> 00:32:07,600
he came with the database

465
00:32:07,610 --> 00:32:10,010
and the database he brought

466
00:32:10,030 --> 00:32:14,370
classes good brain is we're that tell about a couple of minutes

467
00:32:14,510 --> 00:32:21,670
and the question that we ask is this kind of cartoon show to sort of

468
00:32:21,680 --> 00:32:28,670
critical point light phenomenon namely the amplification of fluctuations as if as if we're giving

469
00:32:28,670 --> 00:32:32,110
you a critical point of course is not a critical point but it is a

470
00:32:32,110 --> 00:32:34,760
kind of switch point based which when i mean

471
00:32:34,820 --> 00:32:39,760
trend in the market which is going one way suddenly switches to going the other

472
00:32:39,760 --> 00:32:43,150
way and experiment every one of us can do if you have your laptop on

473
00:32:43,150 --> 00:32:45,610
data volumes have grown fifty four

474
00:32:48,520 --> 00:32:49,450
you know

475
00:32:49,460 --> 00:32:53,720
we are able to observe something like ten billion new

476
00:32:53,740 --> 00:32:57,950
media consumption events every day ten billion those web records

477
00:32:57,970 --> 00:32:59,780
every single day

478
00:32:59,830 --> 00:33:03,080
this is because we have participation from a very broad range

479
00:33:03,290 --> 00:33:08,620
of media companies around around the world in millions of small sites many of the

480
00:33:08,620 --> 00:33:12,610
largest sites in the world rely on our service to understand their audiences to make

481
00:33:12,610 --> 00:33:18,440
editorial decisions and to develop better package those audiences monetize them and therefore create more

482
00:33:18,460 --> 00:33:20,440
content for those consumers

483
00:33:21,980 --> 00:33:26,180
this is a very data rich environment this is one of the sort of

484
00:33:26,200 --> 00:33:31,950
one of the fascinating things about the online advertising online content area we have we

485
00:33:31,950 --> 00:33:35,070
have you know it was possible almost too much data

486
00:33:35,100 --> 00:33:40,260
just incredible volumes of data every month we have well in excess of a quarter

487
00:33:41,410 --> 00:33:45,000
new records of media consumption covering over a billion

488
00:33:45,110 --> 00:33:48,270
global internet users across millions of sites

489
00:33:48,280 --> 00:33:49,790
on peak days

490
00:33:49,800 --> 00:33:52,510
we have about fifteen terrabytes of new data

491
00:33:52,510 --> 00:33:57,870
and within that we have a set of

492
00:33:57,920 --> 00:33:59,180
pretty interesting

493
00:33:59,200 --> 00:34:04,300
data mining challenges and ongoing just touch on one of these in detail today first

494
00:34:04,300 --> 00:34:04,970
of all

495
00:34:05,020 --> 00:34:11,050
got this fundamental challenge audience estimation using reference data to pieces of information that you

496
00:34:11,050 --> 00:34:15,530
know how high confidence in the first small number of people in a small number

497
00:34:15,530 --> 00:34:16,830
of websites

498
00:34:16,850 --> 00:34:21,870
infer the attributes demographics business cycle graphics

499
00:34:21,880 --> 00:34:23,660
but and otherwise

500
00:34:23,700 --> 00:34:26,280
the audience of all sites

501
00:34:26,300 --> 00:34:31,110
so starting with this a small subset of things you know about using inference mechanism

502
00:34:31,110 --> 00:34:34,530
to understand everything now the counter to that is

503
00:34:34,550 --> 00:34:36,910
using media consumption records

504
00:34:36,930 --> 00:34:38,670
an audience estimates

505
00:34:38,690 --> 00:34:43,670
determine the characteristics of an internet user across arbitrary dimensions and the reason is going

506
00:34:43,670 --> 00:34:47,950
to be arbitrary in different markets are very different things they're interested in

507
00:34:48,240 --> 00:34:54,820
now we actually developed a pretty inevitable novel approach to actually combine these two together

508
00:34:54,930 --> 00:35:00,180
the way we do is we build a graph theoretic representation of internet consumption we

509
00:35:00,180 --> 00:35:05,130
call it the visit graph so there are media assets and there are internet users

510
00:35:05,130 --> 00:35:10,320
the consumers media assets based on common media consumption you can start to connect these

511
00:35:10,320 --> 00:35:16,520
things together we've been able to develop an iterative machine learning approach that actually passes

512
00:35:16,520 --> 00:35:21,040
information around this model and as it converges can be cross validated against pieces of

513
00:35:21,040 --> 00:35:24,860
data that we know that we have chance to people more than one time to

514
00:35:24,860 --> 00:35:26,380
get into it today

515
00:35:26,460 --> 00:35:30,220
i will talk a little bit about look selection today is really key to what

516
00:35:30,220 --> 00:35:34,440
to what we do is key to making advertising successful the idea of look like

517
00:35:34,450 --> 00:35:38,090
selection is if you have a small number of people that have exhibited some behavior

518
00:35:38,090 --> 00:35:39,380
this desire

519
00:35:39,410 --> 00:35:41,660
for example they bought to given product

520
00:35:41,680 --> 00:35:43,770
you have this depend their

521
00:35:43,790 --> 00:35:45,560
find the features

522
00:35:45,580 --> 00:35:48,480
the best discern those people

523
00:35:48,510 --> 00:35:52,510
to everyone else and then find more people like that's ultimately what advertising is all

524
00:35:53,580 --> 00:35:55,210
the last piece

525
00:35:55,260 --> 00:36:00,380
which is something that is completely unique to the online advertising ecosystem is live traffic

526
00:36:00,380 --> 00:36:04,690
model is not enough just have static models these models have to be applied in

527
00:36:04,690 --> 00:36:06,600
real time literally in

528
00:36:06,630 --> 00:36:08,270
eleven seconds two

529
00:36:08,310 --> 00:36:13,870
thousands tens even hundreds of thousands of requests every second and actually the information that

530
00:36:13,870 --> 00:36:15,770
is available in those requests

531
00:36:15,800 --> 00:36:19,510
can make the difference between the campaign succeeding or failing

532
00:36:19,510 --> 00:36:22,720
so given time limitations them into touch upon

533
00:36:22,740 --> 00:36:25,940
on one of these and that's look-alike selection

534
00:36:25,950 --> 00:36:32,240
the idea behind collection selection is that there may be a set of users archetype

535
00:36:32,330 --> 00:36:37,740
done something interesting there one of the really interesting things about about the digital landscape

536
00:36:37,760 --> 00:36:39,770
is more than ever before

537
00:36:39,820 --> 00:36:44,420
markets are actually in possession of data about their own customs historically they often rely

538
00:36:44,700 --> 00:36:50,450
on whatever third-party to provide them with but increasingly markets operate websites that people interacting

539
00:36:50,450 --> 00:36:55,090
with the content of those people might actually be purchasing online for example they might

540
00:36:55,090 --> 00:37:00,370
they might go to the commons product online and obviously advertising is all about getting

541
00:37:00,370 --> 00:37:05,850
people buy products is the ultimate evil advertising even though what's called the purchase funnel

542
00:37:05,850 --> 00:37:07,750
may take some time to get

543
00:37:07,750 --> 00:37:10,680
the purchase purchases the very in the final before you can purchase you've got to

544
00:37:10,680 --> 00:37:12,020
be interested in the product

545
00:37:12,040 --> 00:37:15,120
four you're interested in the product you've got to be where the product even exists

546
00:37:15,290 --> 00:37:20,920
advertising is the process of introducing people to product making them consider that process highly

547
00:37:20,920 --> 00:37:22,800
and getting them to actually buy it

548
00:37:22,950 --> 00:37:27,040
and even if people can't buy the product line they can still exhibit behavior which

549
00:37:27,040 --> 00:37:32,160
is a good proxy to an offline purchase if go to the the automotive website

550
00:37:32,160 --> 00:37:33,550
they can configure the car

551
00:37:33,570 --> 00:37:38,010
they can download coupons to get money of detergent in store

552
00:37:38,030 --> 00:37:42,540
so the idea of lookalikes elections to take some archetype of a few thousand maybe

553
00:37:42,540 --> 00:37:46,510
a few hundred people that have done something that is interesting and valuable to that

554
00:37:46,510 --> 00:37:54,650
particular market and then find the feature set the best separates them from people not

555
00:37:54,860 --> 00:38:01,470
these features can be positive or negative indicators of content relevant to consumer this piece

556
00:38:01,570 --> 00:38:05,150
often makes people to think about one of the things said someone's going to do

557
00:38:06,240 --> 00:38:12,290
actually advertising not targeting is just as valuable as targeting in terms of the return

558
00:38:12,290 --> 00:38:14,690
on investment market has

559
00:38:14,710 --> 00:38:16,800
finally one should develop this model

560
00:38:16,820 --> 00:38:19,850
the task is to find more people like them

561
00:38:22,440 --> 00:38:26,350
different features different behaviours interested consumers

562
00:38:26,380 --> 00:38:30,410
can predict the relevance of particular messages to them what we have here are two

563
00:38:30,410 --> 00:38:33,610
now it is a very short summary you

564
00:38:33,620 --> 00:38:37,700
to sentences like the one i just showed

565
00:38:39,850 --> 00:38:45,200
that is the find a single words and then you build a graph to the

566
00:38:45,220 --> 00:38:46,510
right and

567
00:38:46,510 --> 00:38:49,140
you have to also like start

568
00:38:49,170 --> 00:38:56,290
and in nature between two works represents it uses information so you see the world

569
00:38:56,880 --> 00:39:01,340
if you ever since ABC then in this graph from a to b from b

570
00:39:01,340 --> 00:39:02,090
to c

571
00:39:02,100 --> 00:39:07,490
and from such a procedure and so this is just consisting of three words in

572
00:39:10,500 --> 00:39:15,790
i'll illustrate method very simple example so that's a false sentences

573
00:39:15,810 --> 00:39:20,820
they have short hillary clinton wanted to visit china last month but postponed her plans

574
00:39:20,820 --> 00:39:22,570
to one day last week

575
00:39:22,580 --> 00:39:26,530
you're going to need is visit to the people's republic of china monday

576
00:39:26,850 --> 00:39:32,690
final last week the secretary of state missus clinton visited chinese officials is

577
00:39:32,720 --> 00:39:34,340
so what happens is the following

578
00:39:34,410 --> 00:39:36,510
you take the first sentence you know

579
00:39:36,510 --> 00:39:39,920
all top descendants built just

580
00:39:39,950 --> 00:39:46,000
so graph just a stream of words so that humans we wanted to visit china

581
00:39:46,000 --> 00:39:51,530
last longer than a very short for readability but postponed her plans to one day

582
00:39:51,530 --> 00:39:53,390
last week and then

583
00:39:53,410 --> 00:39:58,060
so this is just an abbreviation fact there are more

584
00:39:58,060 --> 00:39:59,840
note somewhere in between

585
00:39:59,880 --> 00:40:07,180
and what you know about this is the first word for example six string representation

586
00:40:07,420 --> 00:40:11,330
you know the part of speech is now you can get part of speech tags

587
00:40:11,340 --> 00:40:12,850
and you know the better

588
00:40:13,130 --> 00:40:15,230
since the first position

589
00:40:15,240 --> 00:40:19,200
so this is the information you have not only for the first one actually for

590
00:40:19,200 --> 00:40:21,650
everyone you know it's all lowercase

591
00:40:21,680 --> 00:40:23,900
from you know it's part of speech know

592
00:40:23,910 --> 00:40:27,050
its position in the sentence

593
00:40:27,050 --> 00:40:30,570
so now what happens next the following you take

594
00:40:30,580 --> 00:40:32,740
four percent from your cluster

595
00:40:34,290 --> 00:40:36,590
we had work there was you

596
00:40:36,860 --> 00:40:39,130
people's republic of china

597
00:40:39,150 --> 00:40:41,400
and you have

598
00:40:41,420 --> 00:40:44,950
words sentences one after another but

599
00:40:45,040 --> 00:40:50,290
not necessarily in this order but you that those words they considering free

600
00:40:50,310 --> 00:40:52,330
group supports so first

601
00:40:52,340 --> 00:40:53,760
works picture

602
00:40:54,620 --> 00:40:57,800
well compared with what you already have the right

603
00:40:57,860 --> 00:41:03,160
and you have some those still word you just create a new node with m

604
00:41:03,160 --> 00:41:07,420
edges connecting them then you get ambiguous words

605
00:41:08,470 --> 00:41:12,740
UTC mean words for which you have

606
00:41:12,760 --> 00:41:15,110
two nodes in the graph british

607
00:41:15,840 --> 00:41:18,130
given that this word appears

608
00:41:18,150 --> 00:41:19,790
so what is this

609
00:41:19,800 --> 00:41:25,450
so i want to again it's not clear which of the two

610
00:41:25,470 --> 00:41:28,680
and you have this would appear in the records in the which would you want

611
00:41:28,680 --> 00:41:30,360
to happen there exists

612
00:41:30,610 --> 00:41:32,890
so for example if you

613
00:41:32,910 --> 00:41:38,320
the with clinton twice hillary clinton and bill clinton in one sense there was just

614
00:41:38,380 --> 00:41:40,580
before the wife of the former president

615
00:41:40,600 --> 00:41:44,850
bill clinton hillary clinton and given that i would like to hear from

616
00:41:44,860 --> 00:41:50,200
so you don't have both words on but instead look for local context and see

617
00:41:50,200 --> 00:41:55,610
that in the sentence were you we can invent in front of the second term

618
00:41:55,630 --> 00:42:00,950
is going to end the director of the world killer the map

619
00:42:00,970 --> 00:42:04,530
the ones which is similar to what happened

620
00:42:04,580 --> 00:42:09,570
in the graph and then finally you all this before so he doesn't any syntactic

621
00:42:09,570 --> 00:42:12,320
information what you do is

622
00:42:12,340 --> 00:42:17,560
the words into separate words spoken words on the board

623
00:42:17,580 --> 00:42:22,270
and this is the time for many languages it's much easier to to get

624
00:42:22,340 --> 00:42:24,120
syntactic information

625
00:42:24,920 --> 00:42:26,360
for example

626
00:42:26,380 --> 00:42:27,190
this is a

627
00:42:27,290 --> 00:42:30,170
what happens when you have sex with

628
00:42:30,190 --> 00:42:35,050
you may have the word hillary into the work here so it's when we sex

629
00:42:35,730 --> 00:42:38,980
you updated information so you know the people

630
00:42:39,940 --> 00:42:44,560
but his speech otherwise they wouldn't stand up to get them you know now that

631
00:42:44,570 --> 00:42:49,550
this particular it was not only in the first and but also the second

632
00:42:49,550 --> 00:42:53,260
important sentences it's the first position

633
00:42:55,640 --> 00:42:56,930
for example

634
00:42:57,720 --> 00:42:59,640
the word is is not enough to the

635
00:42:59,640 --> 00:43:03,050
note this it because the different parts of speech so here is it

636
00:43:03,720 --> 00:43:05,680
the thing here is unknown

637
00:43:05,680 --> 00:43:07,620
so they don't end up together

638
00:43:07,620 --> 00:43:13,740
this is the third we bring together with which really is what about the same

639
00:43:13,740 --> 00:43:16,800
things non different

640
00:43:16,820 --> 00:43:21,100
so in the end you get in there and when you merge all four sentences

641
00:43:22,080 --> 00:43:25,370
we are making some parts of the

642
00:43:25,390 --> 00:43:27,280
it's good for

643
00:43:27,320 --> 00:43:28,510
but is it

644
00:43:28,530 --> 00:43:34,410
so what you see now that you have that it corresponds to from this that

645
00:43:34,780 --> 00:43:39,680
you can get to the first words of sentences had one cluster and to today

646
00:43:39,680 --> 00:43:42,510
and you can get from all the words

647
00:43:42,530 --> 00:43:43,820
with which

648
00:43:43,840 --> 00:43:46,200
those sentences and their

649
00:43:46,220 --> 00:43:48,570
so this image of the story

650
00:43:48,620 --> 00:43:53,220
you start this is that you want to get to the end and we just

651
00:43:53,240 --> 00:43:54,160
don't what

652
00:43:54,180 --> 00:43:55,450
on the edge of

653
00:43:55,470 --> 00:44:00,120
so what you can get a human being to want to visit china

654
00:44:02,550 --> 00:44:04,530
monday last week

655
00:44:05,220 --> 00:44:10,850
while the former US president bill clinton missus clinton visited china on monday so you

656
00:44:10,850 --> 00:44:15,760
can work well in that have space to get to the to some settlements

657
00:44:15,760 --> 00:44:18,910
but that the intuition is that if you follow

658
00:44:18,930 --> 00:44:21,200
the right if you follow

659
00:44:21,200 --> 00:44:25,050
the words which occur in many sentences that what you get is nice

660
00:44:25,050 --> 00:44:26,930
compression of their

661
00:44:27,490 --> 00:44:30,260
asked so if you go over that hillary clinton

662
00:44:30,280 --> 00:44:34,010
is a child last monday jumped and try to wait

663
00:44:34,030 --> 00:44:38,740
what you get is actually the just a sentence so this is what

664
00:44:38,760 --> 00:44:40,640
four sentences were

665
00:44:40,660 --> 00:44:44,990
now the question so he wouldn't have any syntactic information on

666
00:44:45,010 --> 00:44:46,930
part of speech tags makes

667
00:44:46,950 --> 00:44:51,640
the game is to get the data on the percentage information

668
00:44:51,640 --> 00:44:55,780
so we you have very few sentences it's unlikely that you have something meaningful but

669
00:44:55,780 --> 00:44:56,300
given that

670
00:44:56,530 --> 00:45:00,990
this is a fair chances are high that you will find it

671
00:45:01,010 --> 00:45:02,260
a good shot at

672
00:45:02,280 --> 00:45:08,600
so you find a path which is short for each sentence and which will give

673
00:45:08,600 --> 00:45:09,410
you you

674
00:45:09,490 --> 00:45:10,350
a good

675
00:45:10,370 --> 00:45:14,030
compression summarisation percent expressed

676
00:45:14,050 --> 00:45:17,990
OK this is what is happening now the question is how can we find a

677
00:45:21,390 --> 00:45:27,660
most of the background to computer science and the show

678
00:45:27,890 --> 00:45:32,070
it's not neutral so this is how it can be used for

679
00:45:32,470 --> 00:45:37,300
sentence compression so in this is the shortest

680
00:45:37,300 --> 00:45:42,780
the reason for this is important them that it's not hard to find the shortest

681
00:45:42,800 --> 00:45:46,970
in terms of nodes travel changes in terms of edges

682
00:45:46,970 --> 00:45:52,500
of a certain number function in this case is this exponential function

683
00:45:52,520 --> 00:45:58,780
so from this thing you can can reconstruct all moments of a random variable off

684
00:45:58,780 --> 00:46:03,300
its distribution and therefore you you know the whole distribution

685
00:46:03,310 --> 00:46:07,120
in our case rather than having these exponential quantity here

686
00:46:07,140 --> 00:46:09,530
we have a strict we have

687
00:46:09,540 --> 00:46:14,840
a certain strictly positive definite kernels is even so-called universal kernels

688
00:46:14,860 --> 00:46:19,510
and this thing here is actually an example of the universe can but they are

689
00:46:19,510 --> 00:46:21,000
the university council

690
00:46:21,040 --> 00:46:23,090
a slightly more general than

691
00:46:23,110 --> 00:46:25,880
this is a classical concept

692
00:46:25,900 --> 00:46:32,250
another interesting question that i'm not going to talk about is how to to study

693
00:46:32,900 --> 00:46:41,010
this empirical mean will approximate the actual mean over the distribution that generated the data

694
00:46:41,040 --> 00:46:47,840
this is something that can be studied using methods from statistical learning theory and then

695
00:46:47,850 --> 00:46:52,970
we will find that subject to certain conditions we get a nice convergence with the

696
00:46:52,970 --> 00:46:58,540
rate of one over the square root of the number of observations that we seen

697
00:46:58,550 --> 00:47:03,550
so that's nice because in this case we can we can do things like testing

698
00:47:03,570 --> 00:47:06,140
for equality of distributions

699
00:47:06,160 --> 00:47:12,440
based on empirical samples so this problem called the two sample problem the two sample

700
00:47:12,440 --> 00:47:18,400
problem amounts to testing whether a given two samples x and y so given two

701
00:47:18,400 --> 00:47:23,070
sets of observations from two different distribu of from two distributions p and q

702
00:47:23,090 --> 00:47:28,360
so we only given the observations and we are supposed to decide whether p and

703
00:47:28,360 --> 00:47:32,270
q are identical different based on these observations

704
00:47:32,280 --> 00:47:33,660
and you can imagine that

705
00:47:33,970 --> 00:47:40,590
so we can do we can compute this kind of difference for the samples and

706
00:47:40,590 --> 00:47:44,000
you can imagine that if we notice that for the samples x y for these

707
00:47:44,590 --> 00:47:49,530
sets if it turns out that they have a very large different these two empirical

708
00:47:50,390 --> 00:47:57,260
then with certain probably certain probability we can also say that the actual distributions are

709
00:47:57,260 --> 00:48:02,320
different and therefore we can decide this two sample problem so that's also related to

710
00:48:02,660 --> 00:48:06,250
existing theory and there's a lot of things that i really don't have time to

711
00:48:06,250 --> 00:48:10,950
talk about now but maybe kanji will get back to this problem

712
00:48:14,070 --> 00:48:17,400
so there's a second area

713
00:48:17,450 --> 00:48:21,000
but actually i'm also not going to talk about this instead i would use the

714
00:48:21,250 --> 00:48:26,810
the remaining five minutes maybe to tell you something about the so-called representer theorem which

715
00:48:26,810 --> 00:48:29,050
is also basic results

716
00:48:29,070 --> 00:48:31,020
from the kernel methods

717
00:48:31,040 --> 00:48:36,210
and then maybe we have a few minutes for questions before play maximal announcements about

718
00:48:36,210 --> 00:48:37,020
the summer school

719
00:48:37,350 --> 00:48:41,080
so what's the representer theorem

720
00:48:41,140 --> 00:48:46,530
the representer theorem tells us something about the solution of certain optimization problems that come

721
00:48:46,530 --> 00:48:47,780
up when you train

722
00:48:47,790 --> 00:48:49,890
kernel machines

723
00:48:50,000 --> 00:48:54,800
and here says assuming we are given a positive definite kernel

724
00:48:54,810 --> 00:48:57,100
we still call our inputs x

725
00:48:57,120 --> 00:49:00,570
and we have a training set of size m

726
00:49:00,590 --> 00:49:06,380
suppose we have a strictly monotonic increasing real valued function lambda which we use over

727
00:49:07,460 --> 00:49:10,760
and an arbitrary cost function

728
00:49:12,130 --> 00:49:18,270
so what's the cost function does is given are labeled points x i y i

729
00:49:18,280 --> 00:49:23,330
and given the predictions that all function makes on these lines cost function will assign

730
00:49:23,330 --> 00:49:27,600
some cost values so alex when i was talking this morning about the square error

731
00:49:28,020 --> 00:49:32,740
the square there will be a special case of this kind of course function

732
00:49:32,750 --> 00:49:36,770
so we have but here we have a fairly general class function that given some

733
00:49:36,770 --> 00:49:40,590
candidate solution to the problem f would tell us what's the cost on the training

734
00:49:41,520 --> 00:49:47,290
so let's look at this function part and this is what people call the regularizer

735
00:49:47,290 --> 00:49:49,870
so overall this thing is called an objective function

736
00:49:50,250 --> 00:49:54,720
and a lot of learning problems consists of minimising objective functions

737
00:49:54,730 --> 00:50:00,250
so basic probably almost all the machines are like this you have some objective function

738
00:50:00,250 --> 00:50:05,750
and you want to find a function f from a certain class of function minimizing

739
00:50:05,750 --> 00:50:09,780
this objective function and points out function in this case is the reproducing kernel hilbert

740
00:50:09,780 --> 00:50:16,120
space associated with our concave so we have this problem and it turns out that

741
00:50:16,120 --> 00:50:19,660
the solution under these conditions the solution f

742
00:50:19,680 --> 00:50:22,370
of this problem

743
00:50:22,420 --> 00:50:25,500
admits a representation of this form

744
00:50:25,510 --> 00:50:29,480
which is the kind of form that we've been talking about all the time when

745
00:50:29,480 --> 00:50:34,470
we talk about the space weather surprising thing about this expansion is that these kernels

746
00:50:34,470 --> 00:50:37,430
are centered only on the training points

747
00:50:37,490 --> 00:50:43,900
so here we have an general optimisation problems with the property that

748
00:50:43,910 --> 00:50:48,330
empirical loss of the training hours only evaluated on the training points that makes sense

749
00:50:48,330 --> 00:50:53,640
because it's the training error and here the second term is something related to the

750
00:50:53,640 --> 00:50:58,520
norm of all function in the reproducing kernel hilbert space to monotonic transformation of but

751
00:51:00,280 --> 00:51:04,120
so if this is our cost function then the solution

752
00:51:04,150 --> 00:51:08,350
lives in the span of the kernel sitting on the training points so it's surprising

753
00:51:08,350 --> 00:51:13,370
because we are we working in potentially infinite dimensional hilbert space we only find many

754
00:51:13,370 --> 00:51:17,500
training points and then i'm telling you the solution actually lived in the finite dimensional

755
00:51:17,500 --> 00:51:22,700
subspace that's nice because it means that this kind of optimisation problem to find the

756
00:51:22,700 --> 00:51:25,490
function that minimizes the objective function

757
00:51:25,510 --> 00:51:31,420
this kind of optimisation problem infinite dimensional space can be reduced to an optimisation problem

758
00:51:31,420 --> 00:51:34,210
where we optimize only all functions of this form

759
00:51:34,220 --> 00:51:40,240
so that's quite useful in many domains and that's why will briefly going to look

760
00:51:40,240 --> 00:51:40,980
at it

761
00:51:45,580 --> 00:51:47,840
let's take a look at the proof

762
00:51:47,850 --> 00:51:49,430
so to prove it

763
00:51:49,500 --> 00:51:54,320
we first decompose maybe eventually here on the screen

764
00:51:54,370 --> 00:51:55,640
we decompose

765
00:51:55,650 --> 00:52:01,750
are an arbitrary functions of hilbert space into two parts one part is supposed to

766
00:52:01,750 --> 00:52:03,600
be in the span of the

767
00:52:03,610 --> 00:52:06,490
kernels sitting on the training points

768
00:52:06,540 --> 00:52:09,450
in one part which is orthogonal to it

769
00:52:09,460 --> 00:52:11,760
we can always do this because it's a hilbert space

770
00:52:11,770 --> 00:52:16,360
so this is the first part of its in the span of the training points

771
00:52:16,360 --> 00:52:18,680
this is orthogonal and because of orthogonal

772
00:52:19,040 --> 00:52:20,680
it's not all

773
00:52:20,700 --> 00:52:26,670
kernel and on training points to this equation here is true now let's apply this

774
00:52:26,670 --> 00:52:29,200
function to an arbitrary

775
00:52:29,280 --> 00:52:30,840
o point six

776
00:52:30,850 --> 00:52:33,060
see what happens

777
00:52:33,080 --> 00:52:35,960
so if we apply this function takes a

778
00:52:36,020 --> 00:52:39,700
we point evaluate it we can do that by taking into product

779
00:52:39,890 --> 00:52:42,810
because we know the curve represents point evaluation

780
00:52:43,770 --> 00:52:48,830
so we take the dot product between this function and k sitting j

781
00:52:48,840 --> 00:52:50,350
we use the fact

782
00:52:50,370 --> 00:52:56,240
the first substitute is and we use the fact that the product ontology engineering

783
00:52:56,260 --> 00:52:57,720
so the first

784
00:52:57,730 --> 00:52:59,340
the product this

785
00:52:59,410 --> 00:53:00,670
and this thing here

786
00:53:00,690 --> 00:53:04,690
the second term b dot product between this thing and this but we know that

787
00:53:04,690 --> 00:53:08,880
that's zero from here so the second term vanishes so what we're left with is

788
00:53:08,880 --> 00:53:10,720
this going to be here

789
00:53:10,770 --> 00:53:14,940
the crucial thing is that this quantity is independent of the particular part of the

790
00:53:14,940 --> 00:53:16,530
function f

791
00:53:16,550 --> 00:53:20,310
so first so that means if if we start with an arbitrary functions and we

792
00:53:20,310 --> 00:53:26,360
want to evaluate its cost according to this objective function then if we were only

793
00:53:26,360 --> 00:53:30,850
interested in the first part whatever is orthogonal to the training points we could just

794
00:53:30,850 --> 00:53:33,200
as well throw away doesn't make a difference

795
00:53:33,540 --> 00:53:36,170
OK so now let's look at the second part

796
00:53:36,190 --> 00:53:39,620
so the rewards of

797
00:53:39,630 --> 00:53:42,620
so let's look at this this quantity you

798
00:53:42,620 --> 00:53:51,100
is it

799
00:54:05,270 --> 00:54:09,490
you can say

800
00:54:35,130 --> 00:54:36,700
to say

801
00:54:43,480 --> 00:54:47,240
and this is

802
00:55:05,480 --> 00:55:07,420
just right

803
00:56:07,770 --> 00:56:18,050
OK thank you can hear me

804
00:56:18,890 --> 00:56:21,160
thank you very much for having me here

805
00:56:21,180 --> 00:56:24,050
i think this is my first time at ICML i've course many ice in all

806
00:56:24,050 --> 00:56:28,330
papers but i don't think i've actually been to the conference before so it's great

807
00:56:28,330 --> 00:56:29,190
to be here

808
00:56:29,540 --> 00:56:36,390
let me just start off by acknowledging a bunch of lab members including one former

809
00:56:36,390 --> 00:56:38,760
lead member tom griffiths who is now

810
00:56:38,820 --> 00:56:43,250
faculty of berklee i just want to all of these people have made some contribution

811
00:56:43,250 --> 00:56:45,120
to the research i'm going to talk about here

812
00:56:45,160 --> 00:56:49,910
i want to highlight the charles kemp over there in red because the most of

813
00:56:49,910 --> 00:56:53,880
the good ideas and hard work in one minute talk about come from charles' thesis

814
00:56:53,880 --> 00:56:57,280
czech he's just about to graduate and then he will be starting a faculty position

815
00:56:57,290 --> 00:56:58,250
at CMU

816
00:56:58,340 --> 00:57:04,420
so if you if you like these ideas you might look

817
00:57:04,470 --> 00:57:08,520
now i don't i don't think i need to tell this audience about the probabilistic

818
00:57:08,520 --> 00:57:12,790
revolution in machine learning and AI more generally across

819
00:57:12,810 --> 00:57:17,300
pretty much all subfields of a i may most prominently in machine learning but including

820
00:57:17,300 --> 00:57:23,750
vision robotics natural language expert systems reasoning planning

821
00:57:23,760 --> 00:57:29,660
the field's been revolutionized by the adoption of sophisticated mathematical techniques that give principles and

822
00:57:29,930 --> 00:57:34,530
in some cases efficient and certainly effective methods for dealing with uncertainty in particular for

823
00:57:34,760 --> 00:57:40,020
trying to make inductive inferences from very very ill posed problems very an ambiguous data

824
00:57:40,310 --> 00:57:44,190
but if you look at what you know i i would say what sort of

825
00:57:44,190 --> 00:57:48,450
the informal survey of the leading practitioners of the state of the art probabilistic AI

826
00:57:48,540 --> 00:57:50,730
machine learning most of them would say

827
00:57:50,810 --> 00:57:53,760
but there is that they don't think there's any necessary connection between the math these

828
00:57:53,760 --> 00:58:00,120
probabilistic methods and the way the human brain actually learns and increasingly as statistical machine

829
00:58:00,120 --> 00:58:02,200
learning has become more

830
00:58:02,210 --> 00:58:06,500
mathematically sophisticated in these up until a few years ago there's been less and less

831
00:58:06,500 --> 00:58:09,740
of connection between the field of machine learning in the field of human learning to

832
00:58:09,740 --> 00:58:13,980
me that's too bad because someone who sits between these fields have been was saying

833
00:58:13,990 --> 00:58:17,620
i think we still have a lot to learn from each other i hope i'm

834
00:58:17,620 --> 00:58:21,180
not the only one i i i want to start here with quote that

835
00:58:21,190 --> 00:58:22,720
zoom in rome over email

836
00:58:22,730 --> 00:58:26,520
when he was inviting me to speak and we talking about topics he said many

837
00:58:26,520 --> 00:58:29,360
people in machine learning get into the field because they are interested in how humans

838
00:58:29,360 --> 00:58:33,880
learn rather than how convex functions are optimized and how we can get machines to

839
00:58:33,880 --> 00:58:38,290
be more like humans so i hope that he was representative of these much of

840
00:58:38,290 --> 00:58:41,600
the audience here and i hope that there will be some interesting insights to be

841
00:58:41,660 --> 00:58:44,040
to be had from this interdisciplinary discussion

842
00:58:44,140 --> 00:58:47,880
now the particular problems that we're working on here are what we like to call

843
00:58:47,880 --> 00:58:50,950
everyday inductive leaps these are problems where

844
00:58:51,310 --> 00:58:55,450
sort of this essential core problems of human learning where people seem to learn so

845
00:58:55,450 --> 00:59:00,150
much about the world from such that apparently such limited evidence

846
00:59:00,300 --> 00:59:03,600
classic examples that have been studying for a while as you even mentioned are basically

847
00:59:03,600 --> 00:59:09,100
problems of concept learning in categorisation so to take a paradigmatic example consider child learning

848
00:59:09,390 --> 00:59:13,670
their first words like this is sort of rough reconstruction of my daughter learning the

849
00:59:14,620 --> 00:59:18,590
when she was a year and a half old we went out to the country

850
00:59:18,740 --> 00:59:24,550
for thanksgiving western massachusetts and from just a few experiences like this scene of course

851
00:59:24,550 --> 00:59:28,420
paired with an excited label from one of her parents horse

852
00:59:28,430 --> 00:59:32,190
she was able to grasp this concept and pick out

853
00:59:32,390 --> 00:59:36,020
and if not perfectly almost perfectly other horses you know she might be confused about

854
00:59:36,020 --> 00:59:38,190
the occasional kaur

855
00:59:38,270 --> 00:59:43,260
four camel but more less got this concept from just one or a few examples

856
00:59:43,280 --> 00:59:46,940
if you want to think about it just for the simple mathematical abstraction

857
00:59:46,960 --> 00:59:51,090
there are some you know basically infinite set of objects out there in the world

858
00:59:51,100 --> 00:59:54,970
it really is infinite if you think of all the horses you could draw

859
00:59:55,010 --> 00:59:59,330
and if of course this concept is going to generalize to draw on horses as

860
01:00:00,460 --> 01:00:02,220
so this infinite set of

861
01:00:02,240 --> 01:00:03,660
all objects you could

862
01:00:03,670 --> 01:00:06,340
preceding an infinite subset of that

863
01:00:06,380 --> 01:00:10,080
which you could say is the subset of horses and somehow child learning word like

864
01:00:10,080 --> 01:00:14,020
this is able to grasp more less the boundaries of that set from just one

865
01:00:14,020 --> 01:00:14,780
or a few

866
01:00:14,830 --> 01:00:19,040
randomly chosen points with that's a stunning achievement and we want to understand how it

867
01:00:19,040 --> 01:00:20,230
could work

868
01:00:20,250 --> 01:00:24,400
in case you forgot what it's like to be a child learning word let me

869
01:00:24,400 --> 01:00:27,600
give you an example of a sort of a demo of the experiment was done

870
01:00:27,600 --> 01:00:30,700
in our lab where we try to put human adult

871
01:00:30,720 --> 01:00:34,850
in the sixteen kind of situation so you're on the planet because you

872
01:00:34,880 --> 01:00:37,750
these are some of the because you being objects and we're going to give you

873
01:00:38,060 --> 01:00:41,930
a few examples of various words in the gives language

874
01:00:41,940 --> 01:00:43,470
so let's

875
01:00:43,520 --> 01:00:47,040
show you a few two first here now even

876
01:00:47,050 --> 01:00:50,580
i think if i think if i'd only just given you one example and certainly

877
01:00:50,580 --> 01:00:54,520
giving you three examples here but you've never seen the word to for before seeing

878
01:00:54,520 --> 01:00:57,120
most of these kinds of objects before you have a pretty good idea

879
01:00:57,170 --> 01:00:59,110
of what things are too busy which are

880
01:00:59,160 --> 01:01:03,930
so let's try this out for the audience participation part of the lecture

881
01:01:03,940 --> 01:01:12,420
just yell out is that the two for yes and no

882
01:01:13,320 --> 01:01:22,020
a little more and certainly there

883
01:01:22,030 --> 01:01:24,380
that's good because our model predicts that

884
01:01:24,430 --> 01:01:26,160
so you get the idea

885
01:01:27,260 --> 01:01:32,260
the the problem of generalizing for a few examples isn't just about learning words and

886
01:01:32,260 --> 01:01:35,610
concepts all that's one of the main places where comes up but also comes up

887
01:01:35,610 --> 01:01:39,950
and we studied in other contexts such as learning causal relations relation between cause and

888
01:01:41,820 --> 01:01:46,880
with what kind of scientists call theory of mind learning about the internal mental states

889
01:01:46,880 --> 01:01:53,310
beliefs goals plans of other agents from observing their behaviour or for example learning intuitive

890
01:01:53,350 --> 01:01:59,030
learning intuitive theories that govern our psychological world social world social rules and conventions into

891
01:01:59,030 --> 01:02:00,040
the physics

892
01:02:00,060 --> 01:02:03,340
all of these are versions of this kind of problem just to give one another

893
01:02:03,340 --> 01:02:07,380
example which will come back to time permitting at the end learning cause-and-effect right so

894
01:02:07,390 --> 01:02:09,670
we all learned in our

895
01:02:09,700 --> 01:02:15,800
most basic statistics classes that you can't infer necessarily causation from observing the correlation right

896
01:02:15,890 --> 01:02:21,650
could be confounders and hidden variables and things like that directionality isn't so clear with

897
01:02:21,650 --> 01:02:26,120
with more recent developments in causal bayes nets argu there are some cases now you

898
01:02:26,120 --> 01:02:28,310
can infer causation from correlation

899
01:02:28,700 --> 01:02:31,970
and there are modern kinds of techniques for controlled experiments which allow you to do

900
01:02:31,970 --> 01:02:36,780
this under intervention but think about the case of the child learning causal relation or

901
01:02:36,780 --> 01:02:41,430
or inferences we make all the time we're often just one or a few examples

902
01:02:41,560 --> 01:02:47,130
did with the right kind of pattern of suspicious coincidence can strongly suggests causal relation

903
01:02:47,390 --> 01:02:50,490
when again from just one or a few data points you couldn't even computer reliable

904
01:02:50,490 --> 01:02:56,290
correlation right so the problem isn't going from correlations to causation but rather observe are

905
01:02:56,300 --> 01:03:00,240
inferring causal structure when you when you have so little data you can't buy these

906
01:03:00,240 --> 01:03:04,300
conventional statistical means even computer reliable correlation

907
01:03:04,310 --> 01:03:07,200
so how can you solve this problem well

908
01:03:07,290 --> 01:03:10,250
in some sense the answer is pretty simple and we all know that is right

909
01:03:10,250 --> 01:03:13,740
if you're learning a lot from little data it must be because in some sense

910
01:03:13,740 --> 01:03:17,540
you already knew alot you must have some strong prior knowledge is an inductive bias

911
01:03:17,560 --> 01:03:21,800
this is the lesson that the field of machine learning has learned and taught quite

912
01:03:21,800 --> 01:03:22,890
well right

913
01:03:22,960 --> 01:03:27,920
pretty much any paradigm for machine learning has some version of the idea that there

914
01:03:27,920 --> 01:03:31,560
there i think that

915
01:03:31,560 --> 01:03:33,500
if you really

916
01:03:34,040 --> 01:03:38,990
i know this entry is i've seen and i've seen that is whether conditions lighting

917
01:03:38,990 --> 01:03:44,350
and there's also contains sliding we also try back is all the time

918
01:03:48,020 --> 01:03:48,770
you know

919
01:03:48,790 --> 01:03:55,390
just to summarise the textual portions of the talk page about what function and to

920
01:03:55,390 --> 01:03:59,710
me was surprising lessons on working everybody's last few years is that it's actually really

921
01:03:59,710 --> 01:04:05,770
hard to specify reward functions on i think it's important demanded enforcement to encompass have

922
01:04:05,770 --> 01:04:06,100
as well

923
01:04:06,560 --> 01:04:10,730
as well as ideas of learning dynamic small and i actually see a lot of

924
01:04:10,730 --> 01:04:16,440
problems in robotics especially when you can't come over the dynamics model

925
01:04:16,440 --> 01:04:19,870
but somehow humans can learn to do it and then after alpha able to do

926
01:04:19,870 --> 01:04:24,520
so to by some way on and you know a recurring theme in how we

927
01:04:24,520 --> 01:04:29,330
do a lot of these has turned out to be prince

928
01:04:32,120 --> 01:04:35,020
there other articles out there that

929
01:04:35,040 --> 01:04:36,520
i guess it in there

930
01:04:36,540 --> 01:04:41,640
sort of encouraged me to put it back on so kind personal thoughts and then

931
01:04:41,640 --> 01:04:45,850
you know how home show you the story of how i to work on robotics

932
01:04:47,020 --> 01:04:49,900
i used to do fairly theoretical work and

933
01:04:49,960 --> 01:04:56,080
because that was a paper that still and jitendra malik pointed me to that strongly

934
01:04:56,080 --> 01:04:59,440
influenced my life and the way i think the research and the way i transforms

935
01:04:59,440 --> 01:05:04,120
whereupon my wife show this to you just as the personal story and on them

936
01:05:04,120 --> 01:05:08,020
just as they have and definitely not suggesting that anyone else should think the same

937
01:05:08,020 --> 01:05:12,480
way as i do not recommend want be bad idea everyone wants to know if

938
01:05:12,480 --> 01:05:15,560
you have all the messages from say well this is how i tend to do

939
01:05:15,560 --> 01:05:18,770
is just one piece of work is strongly influenced my thinking

940
01:05:19,520 --> 01:05:24,370
this is based on property who's writing about databases but that story to two to

941
01:05:25,850 --> 01:05:30,210
how cars and a high rates you know someone has a bad idea of building

942
01:05:30,230 --> 01:05:35,100
mail delivery robot right so that's something we all agree once we have the robot

943
01:05:35,100 --> 01:05:40,120
to deliver the mail into offices every day this this could have also time so

944
01:05:40,120 --> 01:05:43,890
on their papers on these twenty five years old i imagine very much all the

945
01:05:43,890 --> 01:05:46,600
one on the on trying to build robot

946
01:05:48,100 --> 01:05:52,480
the nature center prague as is then you know what happens we now on to

947
01:05:52,480 --> 01:05:55,140
figure out what i think that what we need to build them out of the

948
01:05:55,140 --> 01:06:00,940
robot well obviously any obstacle avoidance because you will navigate the hallways of office buildings

949
01:06:00,940 --> 01:06:06,190
to deliver mail not in objects and you need robot motivation so the robot can

950
01:06:06,190 --> 01:06:09,500
pick up me on those and drop them off in your office

951
01:06:09,520 --> 01:06:15,560
and i'm going to alice like that on to know that you know these circles

952
01:06:15,580 --> 01:06:19,940
are done in support of the males are well so each of these circles to

953
01:06:19,960 --> 01:06:25,560
represent one research paper or one research idea one research community was sometimes entire research

954
01:06:25,560 --> 01:06:28,040
conferences on that is how

955
01:06:28,040 --> 01:06:30,100
science progresses right

956
01:06:32,060 --> 01:06:38,690
to do obstacle avoidance on we obviously need object detection to detect the obstacles in

957
01:06:38,690 --> 01:06:43,770
the path of the robot and we need navigation the robot navigate around obstacles the

958
01:06:43,790 --> 01:06:46,670
saxons is actually how we do it but i mean i my since i we

959
01:06:46,670 --> 01:06:48,850
and we do this on our robots today

960
01:06:51,750 --> 01:06:53,390
do object detection

961
01:06:53,400 --> 01:06:57,580
you know on many of my on on many robots around the world cameras are

962
01:06:57,580 --> 01:07:03,210
by far the cheaper sensors and so on what you like is on and what

963
01:07:03,210 --> 01:07:07,370
the problem even in the environment is that the lighting often changes right so if

964
01:07:07,440 --> 01:07:11,940
lights on after the sun is different place on you like colin variance the robot

965
01:07:11,940 --> 01:07:16,320
can still recognise it as podiums same thing even those who makes all the different

966
01:07:16,520 --> 01:07:21,060
so one thing that really holds object detection of what is going to be a

967
01:07:21,060 --> 01:07:23,940
little bit in invariant to the to the illumination

968
01:07:26,080 --> 01:07:30,480
so it is that well on his love everything work recently on similarity learning in

969
01:07:30,480 --> 01:07:36,940
which i would like to understand given to RGB vectors of the the pixels are

970
01:07:36,940 --> 01:07:39,460
these the same colour not so can

971
01:07:39,520 --> 01:07:43,920
you correctly recognised the same object even though the illumination has changed little since the

972
01:07:43,920 --> 01:07:48,250
learning free dissimilarity i things very promising approach is

973
01:07:49,230 --> 01:07:52,500
to really understand freely similarity learning

974
01:07:52,600 --> 01:07:57,660
on i things most as a were is on understanding the differential geometry of the

975
01:07:57,660 --> 01:08:02,480
manifold sitcom on developed the most probable ck sound algorithms to

976
01:08:02,540 --> 01:08:06,140
analyzer can make sure the conversion of all these good things

977
01:08:06,810 --> 01:08:08,500
and you can tell is going

978
01:08:09,520 --> 01:08:10,890
and eventually the guy

979
01:08:10,940 --> 01:08:15,940
working on i mean that i have no idea what that is and you know

980
01:08:15,960 --> 01:08:20,060
this guy sitting out there in some you know isolated lab in the middle of

981
01:08:20,060 --> 01:08:27,310
nowhere on thinking that maybe is hoping on to military robots the reality on the

982
01:08:27,310 --> 01:08:32,060
guy working on this slide began work that's is actually not relevant she thinks is

983
01:08:32,060 --> 01:08:36,260
applied to that there really is not doesn't quite apply that special case of quite

984
01:08:36,260 --> 01:08:40,850
apply and you kind of years maybe holds the whole system is also there made

985
01:08:40,850 --> 01:08:43,350
of stories and so

986
01:08:44,670 --> 01:08:47,690
and that's not because i think real theory i think of all the most high

987
01:08:47,690 --> 01:08:49,060
impact work in

988
01:08:49,060 --> 01:08:51,610
if if you

989
01:08:51,640 --> 01:08:53,540
limit yourself to trees

990
01:08:53,570 --> 01:08:57,030
where the edges all on one side of the of the

991
01:08:57,070 --> 01:09:00,500
the words and don't cross each other that's called the called projective trees i can

992
01:09:00,500 --> 01:09:04,040
give you more formal definition but that's sort of the simplest

993
01:09:04,040 --> 01:09:05,460
we say it

994
01:09:05,470 --> 01:09:09,240
the graph the graph is restricted to be on one side it's completely plainer

995
01:09:09,370 --> 01:09:13,250
the edges never cross and this actually works very well for english you can explain

996
01:09:13,250 --> 01:09:16,140
most of english this way but

997
01:09:16,180 --> 01:09:20,040
occasionally we really want crossing are so this is an example of a non projective

998
01:09:20,040 --> 01:09:22,900
tree a talk is scheduled on cats ears today

999
01:09:22,950 --> 01:09:25,490
perfectly good english

1000
01:09:25,490 --> 01:09:29,500
nobody's going to jack because this is the linguistic stock

1001
01:09:29,670 --> 01:09:35,730
on cats ears is attacked that phrase is attached through on to talk causing a

1002
01:09:35,790 --> 01:09:37,950
crossing link or nonprojective arc

1003
01:09:38,910 --> 01:09:42,580
nonprojective seems to be much more important in some languages than others

1004
01:09:42,640 --> 01:09:46,470
choosing not to be not to use it not to model it is perfectly OK

1005
01:09:46,470 --> 01:09:48,640
but you might sacrifice and accuracy

1006
01:09:48,650 --> 01:09:51,860
in doing so

1007
01:09:52,770 --> 01:09:56,140
so we've been doing sort of progressively as i've shown more and more these problems

1008
01:09:56,140 --> 01:10:00,240
is we started the top and talked about words which you can mostly c

1009
01:10:00,250 --> 01:10:03,820
now you should be skeptical of course you can completely see them but mostly and

1010
01:10:03,820 --> 01:10:06,970
we just move down we've been moving down to deeper and deeper structure so now

1011
01:10:06,970 --> 01:10:08,050
i'm going to i'm gonna

1012
01:10:08,080 --> 01:10:10,670
got a little bit lower

1013
01:10:10,720 --> 01:10:14,050
i'm gonna show picked my version of what's called linguistic pipeline and what i want

1014
01:10:14,050 --> 01:10:16,280
you to be aware of is that as we go deeper and deeper and try

1015
01:10:16,280 --> 01:10:20,660
to analyse more and more of the structure of language we're getting what we're getting

1016
01:10:20,660 --> 01:10:24,680
further and further away from the particulars of the language itself and more into physical

1017
01:10:24,680 --> 01:10:30,050
and cultural nonlinguistic things that language is really about

1018
01:10:30,060 --> 01:10:33,510
OK so this is this is sort of breakdown maybe in the field of linguistics

1019
01:10:33,510 --> 01:10:39,000
plus plus a little extra so up at the top just imagine the iceberg superimposed

1020
01:10:39,000 --> 01:10:43,060
on how to do that later but imagine the iceberg superimposed over top of this

1021
01:10:43,060 --> 01:10:47,700
at the top we have the the acoustics the the phonetics which are the physical

1022
01:10:47,700 --> 01:10:51,430
properties of speech the sound waves that are moving from my mouth into your ear

1023
01:10:51,810 --> 01:10:55,930
or the the orthography units of writing that i that i put down the pagerank

1024
01:10:55,950 --> 01:10:59,180
slide that you can read and then we start getting to things that are harder

1025
01:10:59,180 --> 01:11:03,130
to see and we have to reason about more carefully like the units of sound

1026
01:11:03,170 --> 01:11:06,880
the structure of words the structure of sentences morphology and syntax so what i've been

1027
01:11:06,880 --> 01:11:11,050
talking about mostly so far and then we get down into this thing called meaning

1028
01:11:11,050 --> 01:11:14,830
what is what is a sequence of words actually mean what is referring to in

1029
01:11:14,830 --> 01:11:19,820
the world what is it's what its literal meaning or what the active communication which

1030
01:11:19,820 --> 01:11:21,410
are intended

1031
01:11:21,430 --> 01:11:23,920
when i say can you pass the salt

1032
01:11:23,980 --> 01:11:27,910
and as you literally a question about whether you're capable of passing sort

1033
01:11:28,100 --> 01:11:31,000
i want you to do something and i'm conveying it to you in a polite

1034
01:11:31,000 --> 01:11:34,740
way that you're not meant to interpret literally and further we go down there's there's

1035
01:11:34,750 --> 01:11:38,370
discourse which is i say something you say something in this case you sit there

1036
01:11:38,370 --> 01:11:45,050
instantly awkwardly because this lecture scenario there's there you can pass this course which just

1037
01:11:45,050 --> 01:11:46,760
one person speaking as well

1038
01:11:46,770 --> 01:11:50,060
as we move down we're getting farther and farther from the things that are easy

1039
01:11:50,060 --> 01:11:52,100
to observe

1040
01:11:52,120 --> 01:11:53,970
OK so i'm going to go a little bit further

1041
01:11:54,220 --> 01:11:56,800
and you start to see everything come apart at the seams

1042
01:11:57,820 --> 01:12:02,080
the problem of meaning would be to convert descends into some kind of canonical meaning

1043
01:12:02,080 --> 01:12:06,680
representation language so if i get robin swam across the river and delivered the message

1044
01:12:06,770 --> 01:12:12,880
i might pass this into two sort of predicates like swimming delivered there were two

1045
01:12:12,880 --> 01:12:17,400
events one in which somebody was swimming who swimming a around which is the agent

1046
01:12:17,450 --> 01:12:21,780
and she's which is when should swim through she one through the river that's medium

1047
01:12:21,820 --> 01:12:24,930
and i'm sort of making this up as i go the agent is the technical

1048
01:12:24,930 --> 01:12:29,180
term but you can you can sort of assign these semantic rules to the arguments

1049
01:12:29,180 --> 01:12:30,370
of each predicate

1050
01:12:30,400 --> 01:12:33,800
you can convert this into some kind of first order logic i took a stab

1051
01:12:33,800 --> 01:12:35,250
at it you might disagree

1052
01:12:35,310 --> 01:12:38,570
with exactly how to represent this in first order logic there are a lot of

1053
01:12:38,570 --> 01:12:43,920
things in natural language that you can represent this way but nobody quite agrees and

1054
01:12:43,920 --> 01:12:45,450
nobody feels like this

1055
01:12:45,470 --> 01:12:50,200
these representations are quite everything we need to represent the meaning of language

1056
01:12:50,220 --> 01:12:53,310
so i'm going to say first required there's no consensus yet

1057
01:12:53,340 --> 01:12:57,500
it's good exercise coming in finding the semantic structure we don't know there there are

1058
01:12:57,510 --> 01:12:58,980
a lot of things on the table

1059
01:12:58,980 --> 01:13:03,540
the semantic roles are one that's getting a lot of attention this year was part

1060
01:13:03,540 --> 01:13:08,870
of the college conl shared task at the natural language learning conference first order logic

1061
01:13:08,870 --> 01:13:12,250
expressions have always been around a case where there was a little bit of a

1062
01:13:12,250 --> 01:13:17,350
resurgence there are a few recent papers out of MIT and then there are other

1063
01:13:17,350 --> 01:13:21,910
things that people have proposed for very specific problems that often what you have is

1064
01:13:22,180 --> 01:13:25,480
if you have a particular problem you want to solve people will design a problem

1065
01:13:25,480 --> 01:13:30,080
specific meaning language so for example i had a student who is interested in automating

1066
01:13:30,080 --> 01:13:31,970
understanding of cooking recipes

1067
01:13:31,980 --> 01:13:35,200
so it took us about three months and we sat down and we designed the

1068
01:13:35,540 --> 01:13:39,420
meaning language for cooking recipes which is a fairly low dimensional space you have to

1069
01:13:39,420 --> 01:13:42,420
talk about everything in the world only a few things in the kitchen and we

1070
01:13:42,420 --> 01:13:44,830
came up with the primitives and we were able to do it without a whole

1071
01:13:44,830 --> 01:13:46,010
lot of work

1072
01:13:46,060 --> 01:13:48,400
but it took three months and then we had to annotate

1073
01:13:48,420 --> 01:13:51,740
and people don't always agree about what a recipe means

1074
01:13:51,810 --> 01:13:55,260
so so you can see why this is not getting quite as much attention it's

1075
01:13:58,060 --> 01:14:01,710
maybe a little less hard is figuring out which real world entities are being mentioned

1076
01:14:01,710 --> 01:14:03,220
in text and where

1077
01:14:03,270 --> 01:14:09,740
so here's here's a piece of text from the news recently you can guess who

1078
01:14:09,740 --> 01:14:12,910
it's about might be a little bit more helpful

1079
01:14:12,970 --> 01:14:17,220
to understand if i highlight the things that are being referred to

1080
01:14:17,220 --> 01:14:20,790
OK so you made all the policies right

1081
01:14:23,900 --> 01:14:39,020
so exploration is a different subject so we are not

1082
01:14:39,030 --> 01:14:44,420
doing and is and any learning here we are not exploring it adjust their mathematical

1083
01:14:44,420 --> 01:14:46,080
problem right now

1084
01:14:48,510 --> 01:14:53,140
i want to know the existence of an object there exists an optimal policies so

1085
01:14:53,140 --> 01:14:56,080
this is independent of any learning procedure

1086
01:15:03,690 --> 01:15:08,890
that is not shown to be non that it's it's known for the mathematician right

1087
01:15:10,060 --> 01:15:15,270
this is the definition given in this environment the expectation is assessed with respect to

1088
01:15:15,270 --> 01:15:17,090
the environment

1089
01:15:20,980 --> 01:15:24,520
no just at least one

1090
01:15:24,520 --> 01:15:33,270
that's enough

1091
01:15:49,950 --> 01:15:59,100
OK so what's certificate

1092
01:15:59,170 --> 01:16:03,330
so that that would not be any difficulty OK so there are two difficulties one

1093
01:16:03,330 --> 01:16:07,710
difficulty is that if the maximum is taken here on

1094
01:16:07,770 --> 01:16:12,580
and let's sidestep difficult to so let's assume that the maximum mistake

1095
01:16:12,580 --> 01:16:13,950
at every

1096
01:16:13,960 --> 01:16:15,620
state x

1097
01:16:16,390 --> 01:16:19,900
so that's what the what does it mean doesn't mean that there access the policy

1098
01:16:19,900 --> 01:16:22,470
that that is optimized

1099
01:16:22,520 --> 01:16:26,580
it just means so far that given stem state

1100
01:16:26,630 --> 01:16:31,540
you can find the policy that gives you the optimal value from that state

1101
01:16:32,660 --> 01:16:37,430
then how does it follow that full that there exists the a policy that she

1102
01:16:38,360 --> 01:16:41,100
the optimal value at every state

1103
01:16:41,150 --> 01:16:45,050
so there is a difference between assuming the value at a single state

1104
01:16:45,060 --> 01:16:48,360
initiating value at risk

1105
01:16:53,220 --> 01:16:57,090
this is what i wanted to emphasise

1106
01:16:57,100 --> 01:17:00,500
so the optimal policy has to be science that

1107
01:17:00,510 --> 01:17:04,470
it achieves the optimal value at any stage

1108
01:17:04,480 --> 01:17:07,190
no matter you start

1109
01:17:07,250 --> 01:17:09,160
so that

1110
01:17:09,270 --> 01:17:10,940
the ideas

1111
01:17:13,410 --> 01:17:15,890
OK so maybe we should

1112
01:17:28,660 --> 01:17:30,170
is it true

1113
01:17:39,050 --> 01:17:42,030
OK so this is very simple

1114
01:17:42,070 --> 01:17:46,600
because what you have to do is is that you want to have a policy

1115
01:17:46,610 --> 01:17:51,430
that's optimally at the state and you have policies that are optimized for any single

1116
01:17:52,650 --> 01:17:56,380
and you just combine them together holding the combined them together what will be the

1117
01:17:56,380 --> 01:17:57,740
resulting policy

1118
01:17:57,770 --> 01:18:00,890
so there is result does the resulting policy act

1119
01:18:00,940 --> 01:18:05,050
the resulting policy activism histories say that OK i mean the state

1120
01:18:05,060 --> 01:18:09,240
i my coming from i'm coming from the initial state over there

1121
01:18:09,260 --> 01:18:12,870
the optimal policy corresponding to that state was this and that

1122
01:18:12,910 --> 01:18:14,590
how does it act

1123
01:18:14,640 --> 01:18:17,050
i just follow that policy

1124
01:18:17,970 --> 01:18:20,480
so if you remember your start position

1125
01:18:20,490 --> 01:18:22,550
you can use that knowledge to

1126
01:18:25,340 --> 01:18:31,250
there exists an orbital to my policy policy that's optimum for every state

1127
01:18:32,650 --> 01:18:33,980
so that's easy

1128
01:18:34,050 --> 01:18:39,100
at least

1129
01:18:39,110 --> 01:18:40,070
OK so

1130
01:18:40,100 --> 01:18:45,170
before going go further into detail of terry e

1131
01:18:45,230 --> 01:18:47,610
some applications so

1132
01:18:47,620 --> 01:18:52,770
i don't have much time but not the less so

1133
01:18:52,790 --> 01:18:53,590
there are

1134
01:18:53,600 --> 01:18:58,300
a very large number of applications mdps

1135
01:18:58,370 --> 01:19:05,690
the main fields that people are studying and the operations research econometrics you know studying

1136
01:19:05,690 --> 01:19:07,010
economy and

1137
01:19:07,040 --> 01:19:09,910
maybe money investment and stuff like that

1138
01:19:09,930 --> 01:19:15,230
conference that the stakes in our various applications and games in the iron i listed

1139
01:19:15,270 --> 01:19:17,870
some of these applications here

1140
01:19:19,970 --> 01:19:21,540
there are many more

1141
01:19:23,500 --> 01:19:29,750
come to that

1142
01:19:29,790 --> 01:19:31,260
the OK OK

1143
01:19:34,880 --> 01:19:36,940
this is what started

1144
01:19:37,570 --> 01:19:41,490
we said that mission that the status of the book but we see that we

1145
01:19:41,490 --> 01:19:45,100
do learning we don't really need assumption or we are not going to make that

1146
01:19:46,890 --> 01:19:50,800
but you need something close close enough to that still

1147
01:19:50,810 --> 01:19:52,390
the OK so

1148
01:19:53,270 --> 01:19:54,630
by the way

1149
01:19:56,520 --> 01:19:59,020
applications not like

1150
01:19:59,990 --> 01:20:05,690
i just possibilities applications that people have already tried and the bees and

1151
01:20:05,750 --> 01:20:09,890
various learning methods and they were pretty successful

1152
01:20:09,970 --> 01:20:13,630
and so i i not listing here and the success stories but but there are

1153
01:20:13,630 --> 01:20:20,230
quite a few success stories or the of suffering first

1154
01:20:21,390 --> 01:20:25,860
you want to know that there are many variants of mdps so we consider discounted

1155
01:20:25,860 --> 01:20:29,770
and it is but you could have and on this continent and the

1156
01:20:29,810 --> 01:20:33,280
it was just thought i expected

1157
01:20:33,340 --> 01:20:39,020
return on this continent returned like in the navigation problem

1158
01:20:39,040 --> 01:20:40,410
if you

1159
01:20:40,420 --> 01:20:44,740
modify the problem in such a way that was the goal state is reached

1160
01:20:44,750 --> 01:20:45,900
the agent

1161
01:20:45,910 --> 01:20:50,490
stays that no matter what happens so there are elections left

1162
01:20:50,500 --> 01:20:55,100
phase there and doesn't receive any reward and this is again about defined problem

1163
01:20:55,150 --> 01:21:00,240
if you want this is just a generalisation of the shortest path problems

1164
01:21:00,270 --> 01:21:03,500
maybe to infinite state spaces and

1165
01:21:03,510 --> 01:21:06,380
the problem was stochastic as well so

1166
01:21:06,390 --> 01:21:10,520
for this reason this is called the stochastic shortest path problem

1167
01:21:10,550 --> 01:21:15,040
there are other criteria like the average reward criterion when about up this con thing

1168
01:21:15,050 --> 01:21:17,730
you just

1169
01:21:17,730 --> 01:21:18,890
in addition to this trick

1170
01:21:19,030 --> 01:21:23,590
we're using local connections which removed a lot of parameters that we didn't know how to train

1171
01:21:24,060 --> 01:21:26,460
we also use trick called weight-tying

1172
01:21:26,800 --> 01:21:28,740
as yet another scheme

1173
01:21:28,880 --> 01:21:31,950
for removing additional parameters that we don't have to train

1174
01:21:32,150 --> 01:21:35,910
so if we don't have a lot of data we have very few parameters representing the

1175
01:21:35,930 --> 01:21:36,650
neural network

1176
01:21:37,770 --> 01:21:39,770
and the basic the idea is

1177
01:21:40,070 --> 01:21:44,670
that we want to constrain or whole bunch of elements in w to be equal

1178
01:21:44,950 --> 01:21:47,740
so rather than having a hundred three parameters

1179
01:21:47,970 --> 01:21:53,160
maybe we could have just ten but those ten parameters show up at multiple locations in the matrix

1180
01:21:55,090 --> 01:21:59,620
the reason that this works is because images for example tend to be stationary

1181
01:21:59,820 --> 01:22:04,030
so the features that we think are going to be good for recognizing things on the left side of the

1182
01:22:04,630 --> 01:22:08,090
are probably going to be good for recognizing things on the right side of the image

1183
01:22:08,280 --> 01:22:11,360
and it turns out that you're working on an application that's not true

1184
01:22:11,680 --> 01:22:15,290
if you're working on application where you know that the left and right side of the images are

1185
01:22:15,310 --> 01:22:16,140
very different

1186
01:22:17,320 --> 01:22:18,780
then you should be wary

1187
01:22:18,980 --> 01:22:20,660
of using these tricks blindly

1188
01:22:23,070 --> 01:22:24,400
the basic idea is

1189
01:22:24,550 --> 01:22:26,680
going back to that sort of one example

1190
01:22:26,910 --> 01:22:30,910
before what i had was have all these neurons are arranged

1191
01:22:31,050 --> 01:22:31,850
or in this

1192
01:22:32,320 --> 01:22:33,880
spatial direction

1193
01:22:34,130 --> 01:22:35,360
and each neuron

1194
01:22:35,500 --> 01:22:40,510
for each location in the input i've got several neurons looking at that at

1195
01:22:40,710 --> 01:22:41,550
that location

1196
01:22:41,700 --> 01:22:46,300
and only going to do is say that the parameters for this neuron over here that specify

1197
01:22:47,650 --> 01:22:51,560
how it depends on this input window are going to be required to

1198
01:22:51,940 --> 01:22:55,490
are going to constrain them to be equal to the parameters for these neurons next door

1199
01:22:57,830 --> 01:23:02,510
even if we have a constraint we can still have a whole bunch of different kinds of features

1200
01:23:02,660 --> 01:23:07,170
because you have many different neurons looking at the same locations and still have different parameters for

1201
01:23:08,400 --> 01:23:10,730
so this is sometimes called convolutional

1202
01:23:10,940 --> 01:23:11,760
neural network

1203
01:23:12,010 --> 01:23:16,280
because if you set the step size between these windows one

1204
01:23:16,620 --> 01:23:17,880
you can think of it is being

1205
01:23:18,020 --> 01:23:24,410
taking that filter that represents a neuron parameters sort of convulsing with your image to get all the neuron outputs

1206
01:23:24,550 --> 01:23:26,570
so we see a convolutional neural network

1207
01:23:26,760 --> 01:23:27,990
you can just think that

1208
01:23:28,530 --> 01:23:30,030
as a regular neural network

1209
01:23:30,200 --> 01:23:34,100
with these local connectivity and weight-tying tricks at idea

1210
01:23:34,620 --> 01:23:36,720
it's not it's architecture necessarily

1211
01:23:38,940 --> 01:23:44,050
so some other things that you'll definitely come across that make a big difference

1212
01:23:44,370 --> 01:23:45,530
things pooling

1213
01:23:45,760 --> 01:23:51,960
so if you work on computer vision systems you probably you've probably work with pooling architectures

1214
01:23:51,980 --> 01:23:53,500
like a bag of visual words

1215
01:23:55,180 --> 01:23:57,420
and we can basically build these

1216
01:23:57,600 --> 01:24:02,360
as just another functional unit just another one of those black rock on black box modules

1217
01:24:02,620 --> 01:24:03,990
into our neural networks

1218
01:24:04,260 --> 01:24:05,680
are so for example

1219
01:24:06,450 --> 01:24:08,820
we could have some pool neuron here

1220
01:24:08,970 --> 01:24:13,390
represented by h and it's again just going to look at some local window inputs

1221
01:24:13,620 --> 01:24:15,510
like the neurons from the last few slides

1222
01:24:15,720 --> 01:24:18,340
but instead of computing say linear function

1223
01:24:18,690 --> 01:24:21,310
and it's going to compute something like maybe two norm

1224
01:24:21,450 --> 01:24:23,970
or max norm of the things in the window

1225
01:24:24,390 --> 01:24:29,770
and if you take the max of all these neuron responses then you can represent invariant feature

1226
01:24:30,230 --> 01:24:32,260
because if one of these neurons turns on

1227
01:24:32,540 --> 01:24:35,060
regardless of which one this will output

1228
01:24:35,320 --> 01:24:36,760
the value of the largest and if

1229
01:24:37,900 --> 01:24:40,490
thing that you're recognizing activates different

1230
01:24:40,610 --> 01:24:41,720
neurons in this group

1231
01:24:41,970 --> 01:24:44,060
this neuron at the top will still stay on

1232
01:24:45,050 --> 01:24:48,110
and as a whole bunch of work trying to figure out what are the best kinds

1233
01:24:48,130 --> 01:24:50,170
of pooling functions how do we train them

1234
01:24:50,480 --> 01:24:51,500
for example

1235
01:24:51,760 --> 01:24:56,770
when you first start out it's worth recognizing that you can just fix most of these things

1236
01:24:56,980 --> 01:24:58,920
to pool over a fixed set of

1237
01:25:00,050 --> 01:25:01,440
a fixed set of neurons

1238
01:25:01,860 --> 01:25:05,970
but there are also algorithms actually learn how those should be connected up and so forth

1239
01:25:07,400 --> 01:25:08,550
as well

1240
01:25:08,670 --> 01:25:13,280
contrast normalization turns out to be a pretty handy trick so again

1241
01:25:13,440 --> 01:25:15,160
and the computer vision literature

1242
01:25:15,280 --> 01:25:18,430
turns out if you contrast normalized things can be very helpful

1243
01:25:18,590 --> 01:25:19,880
because sometimes you have

1244
01:25:20,180 --> 01:25:22,050
inputs that differ in scale

1245
01:25:22,210 --> 01:25:26,130
or in contrast and it's just not useful and can confusion algorithm

1246
01:25:27,480 --> 01:25:29,110
again even though this is sort of

1247
01:25:29,280 --> 01:25:33,320
computer vision centric idea turns out to be useful in a whole bunch of places

1248
01:25:33,490 --> 01:25:36,240
and you can think of it as just another black-box module

1249
01:25:36,410 --> 01:25:40,700
as long as we can compute gradients are of the output with respect to the input

1250
01:25:40,770 --> 01:25:42,830
we can just drop this in any of our networks

1251
01:25:44,830 --> 01:25:45,790
so it turns out

1252
01:25:45,910 --> 01:25:49,750
that i think i told you so far so the back-propagation

1253
01:25:50,060 --> 01:25:53,140
a bunch of different tricks for making the optimizer work backprop

1254
01:25:53,330 --> 01:25:54,060
and these

1255
01:25:54,250 --> 01:25:57,550
two or three different components that are more specific computer vision

1256
01:25:58,050 --> 01:25:59,720
are enough to build

1257
01:25:59,930 --> 01:26:03,040
which is currently one of the sort of state of the art systems

1258
01:26:03,240 --> 01:26:06,330
in not only in the deep learning world in computer vision world

1259
01:26:06,630 --> 01:26:10,160
so alex krizhevsky sutskever and hinton have this

1260
01:26:10,440 --> 01:26:11,970
pretty amazing neural network

1261
01:26:12,190 --> 01:26:13,740
trained on gp use

1262
01:26:13,880 --> 01:26:16,570
from millions of images on imagenet

1263
01:26:16,720 --> 01:26:20,170
and is currently the top performer in the imagenet challenge

1264
01:26:20,300 --> 01:26:22,460
and the main components of that

1265
01:26:22,660 --> 01:26:24,520
modulo few details is

1266
01:26:24,730 --> 01:26:28,640
convolutional neural network using this local connectivity with weight-tying

1267
01:26:28,920 --> 01:26:29,980
we just talked about

1268
01:26:30,270 --> 01:26:32,930
max-pooling to give you some invariance

1269
01:26:33,860 --> 01:26:36,010
to represent invariant features

1270
01:26:36,380 --> 01:26:38,440
using these rectified linear units

1271
01:26:38,740 --> 01:26:41,250
that i showed you earlier just a non-linear function

1272
01:26:42,060 --> 01:26:45,520
contrast normalization and the local connectivity i said before

1273
01:26:45,740 --> 01:26:50,840
and it's really just tall neural network trained with back-propagation momentum these tricks i talked about

1274
01:26:50,840 --> 01:26:56,200
according to to independence assumption the product of the individual

1275
01:26:56,220 --> 01:27:03,430
probably it

1276
01:27:19,780 --> 01:27:21,840
so for each class

1277
01:27:21,870 --> 01:27:24,340
so we have an object

1278
01:27:24,350 --> 01:27:27,760
which is described by number of features

1279
01:27:27,780 --> 01:27:33,340
and we compute the probability that the object belong to the class

1280
01:27:33,350 --> 01:27:36,930
so we do actually we

1281
01:27:36,950 --> 01:27:38,740
this probability

1282
01:27:38,990 --> 01:27:42,890
the highest so we do actually

1283
01:27:42,950 --> 01:27:46,550
so this is likely cause

1284
01:27:46,580 --> 01:27:49,200
it doesn't affect your

1285
01:27:49,220 --> 01:27:50,800
thank you

1286
01:27:50,800 --> 01:27:53,410
because the time only the best

1287
01:27:53,430 --> 01:27:57,140
last point may be the two best classes

1288
01:27:57,510 --> 01:28:03,010
so we don't take it for you just call

1289
01:28:11,450 --> 01:28:17,840
right image page and also with small zero probabilities

1290
01:28:17,850 --> 01:28:23,700
but usually used in practical implementation we also for rank

1291
01:28:23,720 --> 01:28:25,340
we can

1292
01:28:25,910 --> 01:28:28,820
the location of the value

1293
01:28:28,850 --> 01:28:31,840
and of rules can be

1294
01:28:32,080 --> 01:28:41,220
well it's translated into the local of the some of the company so we use

1295
01:28:41,240 --> 01:28:45,390
the equations that implementation

1296
01:28:45,390 --> 01:28:46,870
now we can also

1297
01:28:46,910 --> 01:28:49,840
have a normal life forms

1298
01:28:50,160 --> 01:28:57,600
the other thing is equation

1299
01:28:57,660 --> 01:29:02,700
i integration not press somehow

1300
01:29:03,320 --> 01:29:06,620
would be microsoft powerpoint

1301
01:29:06,660 --> 01:29:08,510
i tried to get it down

1302
01:29:08,530 --> 01:29:12,100
i didn't see equation so there

1303
01:29:12,100 --> 01:29:13,450
the value

1304
01:29:13,470 --> 01:29:15,950
can actually be normalized

1305
01:29:15,970 --> 01:29:17,600
so taking into

1306
01:29:18,640 --> 01:29:20,280
well computing

1307
01:29:20,340 --> 01:29:23,240
this probability for each class

1308
01:29:23,260 --> 01:29:24,740
and why

1309
01:29:24,740 --> 01:29:32,410
of the class so we normally you might say that the range of the key

1310
01:29:37,680 --> 01:29:40,050
we have well

1311
01:29:40,070 --> 01:29:44,780
the probability that a certain class of critics prior probabilities

1312
01:29:44,780 --> 01:29:46,660
we have to estimate the

1313
01:29:46,680 --> 01:29:53,390
and also we have to estimate the conditional probability that feature words maybe our case

1314
01:29:53,390 --> 01:29:54,320
given us

1315
01:29:54,410 --> 01:29:57,010
so there are two months

1316
01:29:57,240 --> 01:30:00,030
a binomial of model

1317
01:30:00,030 --> 01:30:01,010
and we

1318
01:30:03,300 --> 01:30:04,280
you know by

1319
01:30:05,950 --> 01:30:07,240
we just

1320
01:30:07,260 --> 01:30:10,720
consider the option

1321
01:30:10,760 --> 01:30:14,550
whether or not feature of curves in the

1322
01:30:14,570 --> 01:30:16,660
one of

1323
01:30:17,410 --> 01:30:20,890
one of our feature vector by

1324
01:30:20,910 --> 01:30:22,800
the future of course not

1325
01:30:22,870 --> 01:30:27,240
among we take into account

1326
01:30:27,720 --> 01:30:33,640
the fraction of time diffusion occurs across all ages

1327
01:30:33,660 --> 01:30:37,100
which means that if we take it would cause

1328
01:30:37,100 --> 01:30:40,160
i find in an object in the document

1329
01:30:40,180 --> 01:30:41,850
some information

1330
01:30:41,870 --> 01:30:48,050
we take into account that occurs five times that we say well we consider the

1331
01:30:48,050 --> 01:30:52,200
yorkurbandb that feature across all the edges

1332
01:30:52,260 --> 01:30:58,100
so which we have here that this position independence assumption

1333
01:30:58,120 --> 01:31:04,260
because we not take into account the position of

1334
01:31:04,280 --> 01:31:08,240
so here i don't know if you can read the

1335
01:31:11,180 --> 01:31:13,910
but i will go

1336
01:31:15,200 --> 01:31:16,620
so this is my

1337
01:31:16,680 --> 01:31:20,660
it's a very simple implementation for multinomial model

1338
01:31:20,760 --> 01:31:26,260
training and testing now this is not your mother if you do information extraction

1339
01:31:26,300 --> 01:31:29,010
they were quite well

1340
01:31:29,200 --> 01:31:30,780
so that's why

1341
01:31:30,800 --> 01:31:33,580
i told him that are

1342
01:31:33,600 --> 01:31:37,620
so we have

1343
01:31:37,640 --> 01:31:39,490
so please don't you

1344
01:31:39,510 --> 01:31:44,870
which i would have to classify the document d

1345
01:31:44,930 --> 01:31:47,740
so the class p and the document d

1346
01:31:47,740 --> 01:31:50,530
so we have a look at the end

1347
01:31:50,530 --> 01:31:54,140
fact vocab from document set

1348
01:31:54,160 --> 01:32:00,410
this of course comes from and you look from information that people might have mind

1349
01:32:01,640 --> 01:32:10,390
it designed for classification of documents based on the terms that occur in the document

1350
01:32:10,410 --> 01:32:11,160
so you

1351
01:32:11,160 --> 01:32:13,340
so that's why i had

1352
01:32:13,350 --> 01:32:14,550
feature that

1353
01:32:14,620 --> 01:32:18,720
it's called the vocabulary of like items

1354
01:32:18,740 --> 01:32:20,550
consider not

1355
01:32:20,600 --> 01:32:26,970
so you've got to number of documents which gives you

1356
01:32:26,970 --> 01:32:29,740
and then for each club

1357
01:32:29,930 --> 01:32:33,530
you come the number of documents

1358
01:32:33,870 --> 01:32:37,550
in that same class which is called the

1359
01:32:37,570 --> 01:32:40,220
and have a

1360
01:32:40,220 --> 01:32:43,740
if we are trying to estimate the number of

1361
01:32:43,950 --> 01:32:46,850
consequently classified by the number

1362
01:32:50,260 --> 01:32:51,740
and that's what

1363
01:32:53,300 --> 01:32:57,820
you compute the conditional probability of that term

1364
01:32:57,840 --> 01:33:00,820
it's only one

1365
01:33:00,820 --> 01:33:04,080
they also talk about them mean the paper on drug networks

1366
01:33:04,100 --> 01:33:09,830
the same conception most method allows classification of drugs by their mechanism of actions based

1367
01:33:09,830 --> 01:33:13,510
only on the properties of their mutual interactions between drugs

1368
01:33:13,560 --> 01:33:15,820
so all of those problems

1369
01:33:15,900 --> 01:33:19,930
the real problems on very large scale in which you can

1370
01:33:20,420 --> 01:33:23,740
you can model of them as problems of a bicluster

1371
01:33:27,020 --> 01:33:28,350
i want to do it

1372
01:33:28,430 --> 01:33:29,520
that's why

1373
01:33:29,570 --> 01:33:34,150
by perspective of the world not to develop algorithms but

1374
01:33:35,220 --> 01:33:36,610
prove theorems

1375
01:33:36,630 --> 01:33:40,740
i mean so that was trying to do and if you want to prove theorems

1376
01:33:40,740 --> 01:33:44,760
the first thing you have to do is to have some precise definition of what

1377
01:33:44,780 --> 01:33:47,170
is the goal of this by class

1378
01:33:47,200 --> 01:33:51,060
it turns out that in many of these applications no such

1379
01:33:52,180 --> 01:33:56,230
objective function have been formulated so

1380
01:33:56,240 --> 01:33:58,520
here is a sum

1381
01:33:58,570 --> 01:34:01,690
potential candidates

1382
01:34:01,710 --> 01:34:02,990
because functions

1383
01:34:03,010 --> 01:34:05,110
objective function that you may

1384
01:34:06,070 --> 01:34:08,640
so is the formalisation of the problem

1385
01:34:09,630 --> 01:34:12,830
our input is a matrix

1386
01:34:12,860 --> 01:34:17,770
of entries in a lot of size n times and we can also consider matrices

1387
01:34:17,770 --> 01:34:20,680
with real valued

1388
01:34:20,700 --> 01:34:23,770
input of a discrete set

1389
01:34:23,800 --> 01:34:26,390
other than zero one but let's for completeness

1390
01:34:26,400 --> 01:34:28,510
just concentrate on c one

1391
01:34:28,620 --> 01:34:33,480
and we have two parameters which can which indicate the number of clusters that we

1392
01:34:33,480 --> 01:34:36,680
expect to see in the rows and the number of clusters that we want to

1393
01:34:36,680 --> 01:34:38,350
create in the columns

1394
01:34:39,390 --> 01:34:44,090
what do we want to output we want to output a partition of a set

1395
01:34:44,280 --> 01:34:49,090
of columns and the partition of the set of roles

1396
01:34:49,120 --> 01:34:52,310
now the question is how do we define

1397
01:34:52,330 --> 01:34:58,090
the quality of such partition of such a pair of partitions of set biclustering

1398
01:34:58,110 --> 01:35:02,860
and there are several ways of defining quality and here is one of them which

1399
01:35:02,860 --> 01:35:04,600
we call the minority cost

1400
01:35:04,620 --> 01:35:06,780
and what we do here

1401
01:35:06,790 --> 01:35:09,010
in this course is

1402
01:35:09,030 --> 01:35:12,200
we sum over all blocks

1403
01:35:12,350 --> 01:35:16,560
and what do we do in every block i look at all the entries

1404
01:35:16,570 --> 01:35:20,320
that belong to this block so i'm looking at the i j

1405
01:35:21,950 --> 01:35:23,840
all the entries in this blog

1406
01:35:23,860 --> 01:35:25,130
such that

1407
01:35:25,150 --> 01:35:26,170
the label

1408
01:35:26,180 --> 01:35:27,560
in this entry

1409
01:35:27,570 --> 01:35:32,350
it is different than the majority label of this blog

1410
01:35:32,370 --> 01:35:35,310
so kind of time counting as

1411
01:35:35,330 --> 01:35:39,970
any entry in the block which is different in the majority of the block

1412
01:35:43,540 --> 01:35:52,080
if i have nice matrix

1413
01:35:52,090 --> 01:35:53,410
or something

1414
01:35:55,050 --> 01:35:58,050
and now i partition my column

1415
01:35:58,070 --> 01:36:01,020
and rose into blocks and say that

1416
01:36:01,030 --> 01:36:07,370
these are my blog so i account i have no what it is

1417
01:36:07,380 --> 01:36:09,220
on this blog is if i have

1418
01:36:09,240 --> 01:36:15,130
equality between zero and one then i consider half of them as errors i mean

1419
01:36:15,440 --> 01:36:19,910
i just look at one labelled and ask yourself if a kind of this block

1420
01:36:19,940 --> 01:36:23,850
is once how many entries are going to be

1421
01:36:23,910 --> 01:36:26,220
miss classified

1422
01:36:26,330 --> 01:36:31,390
and in this not have too many quality blocks so let's make it like this

1423
01:36:31,480 --> 01:36:34,260
so into this block the majorities one

1424
01:36:34,280 --> 01:36:36,620
so i have to have somebody this block

1425
01:36:36,640 --> 01:36:39,680
i have two LC one l here

1426
01:36:39,700 --> 01:36:41,720
so it in

1427
01:36:41,770 --> 01:36:46,780
i some the number of entries which are in the minority value of the blogs

1428
01:36:46,860 --> 01:36:50,430
and take the ratio of this the total number of entries

1429
01:36:50,570 --> 01:36:55,130
so my goal is to make my blocks as emerging as possible and this is

1430
01:36:55,130 --> 01:36:57,710
one way of formulating this

1431
01:36:59,680 --> 01:37:02,740
so that's the minority cost

1432
01:37:02,750 --> 01:37:04,920
there are others

1433
01:37:05,010 --> 01:37:07,590
a possible reason below

1434
01:37:07,720 --> 01:37:15,680
objective function so that to because it is much genius covers maximal margin covered what

1435
01:37:15,680 --> 01:37:19,940
you want to do is you want to partition it in such a way that

1436
01:37:19,960 --> 01:37:25,380
the maximum number of entries is covered by purely homogeneous

1437
01:37:26,560 --> 01:37:31,140
so this is a good block and all the other blogs and the question is

1438
01:37:31,210 --> 01:37:35,450
how can i arrange my block such that the size of the total size of

1439
01:37:35,450 --> 01:37:37,800
the good blocks is maximized

1440
01:37:37,850 --> 01:37:39,990
and they treat all the genius

1441
01:37:40,010 --> 01:37:43,750
on the non which is blocked as if they have background noise

1442
01:37:43,760 --> 01:37:48,090
so you want to try to explain as much of the matrix as you can

1443
01:37:48,230 --> 01:37:50,240
we appeal

1444
01:37:50,390 --> 01:37:54,290
blocks and the rest of it due to noise and you want to maximize

1445
01:37:54,300 --> 01:37:59,600
the size of the explained a portion of course we can take some kind of

1446
01:38:01,250 --> 01:38:03,610
middle ground between these two

1447
01:38:04,190 --> 01:38:06,330
types of requirements

1448
01:38:06,380 --> 01:38:13,200
and the other one is the mean minimum weighted variance and this is mainly applies

1449
01:38:13,240 --> 01:38:17,610
two matrices with real valued entries

1450
01:38:17,620 --> 01:38:19,270
and in late like the

1451
01:38:19,350 --> 01:38:20,990
the gene expression make

1452
01:38:21,000 --> 01:38:23,890
so what you want is for in every blocks so now we don't have his

1453
01:38:23,890 --> 01:38:27,230
it was once they have written and so

1454
01:38:28,040 --> 01:38:33,020
in every block i can look at the value and

1455
01:38:33,140 --> 01:38:36,450
you can look at the variance of this blog

1456
01:38:36,490 --> 01:38:40,710
and i want to minimize the but the talk to the sum of the variances

1457
01:38:40,710 --> 01:38:41,760
of the blocks

1458
01:38:41,770 --> 01:38:46,320
i want to have my blocks as centered around the

1459
01:38:46,440 --> 01:38:48,910
average as possible

1460
01:38:48,920 --> 01:38:51,390
so far we invited mattresses consider this

1461
01:38:51,410 --> 01:38:55,260
and when i say weighted variance a way the violence of each block by the

1462
01:38:55,260 --> 01:38:57,200
size of the block

1463
01:38:57,210 --> 01:38:59,730
and they want to minimize the

1464
01:39:01,080 --> 01:39:04,330
a weighted sum of the variances in the blogs

1465
01:39:04,340 --> 01:39:06,550
so all of those

1466
01:39:06,560 --> 01:39:12,750
reasonable size objective functions they we consider

1467
01:39:12,800 --> 01:39:16,350
we can view it i mean there are several ways of viewing what are you

1468
01:39:16,350 --> 01:39:20,370
doing here but one interesting way of doing it is the only the compression you

1469
01:39:20,370 --> 01:39:23,200
have initially

1470
01:39:23,220 --> 01:39:26,540
the matrix has size

1471
01:39:26,560 --> 01:39:31,360
n times and you have intense in many entries once rearrange

1472
01:39:31,360 --> 01:39:36,120
the good charlotte

1473
01:39:36,140 --> 01:39:37,570
it's already going

1474
01:39:37,590 --> 01:39:38,570
so you see

1475
01:39:40,060 --> 01:39:41,120
we have the current

1476
01:39:41,140 --> 01:39:42,370
and vertically

1477
01:39:42,380 --> 01:39:44,160
we have the voltage

1478
01:39:44,170 --> 01:39:46,160
and so it takes about the second

1479
01:39:46,210 --> 01:39:47,520
go from zero

1480
01:39:49,340 --> 01:39:54,000
this goes from zero to four vols you see that the current is beautifully

1481
01:39:54,120 --> 01:39:59,270
i'm looking down on my reflection that's interesting

1482
01:39:59,280 --> 01:40:01,510
well doesn't allow for that

1483
01:40:01,560 --> 01:40:04,070
so you see how beautiful many that so now

1484
01:40:04,090 --> 01:40:06,480
you may have great confidence

1485
01:40:06,490 --> 01:40:09,080
in ohms law

1486
01:40:09,140 --> 01:40:13,810
i don't have any confidence in ohms law

1487
01:40:13,820 --> 01:40:15,610
the conductivity

1488
01:40:17,880 --> 01:40:18,990
is a

1489
01:40:19,020 --> 01:40:22,070
strong function of the temperature

1490
01:40:22,090 --> 01:40:24,560
if you increase the temperature

1491
01:40:24,560 --> 01:40:27,340
then the time tall

1492
01:40:27,430 --> 01:40:28,930
between collisions

1493
01:40:28,950 --> 01:40:32,450
goes down because the speed of these free electrons goes up

1494
01:40:32,510 --> 01:40:33,980
very strong functions

1495
01:40:34,070 --> 01:40:35,390
of temperature

1496
01:40:35,410 --> 01:40:37,480
and so if top

1497
01:40:37,480 --> 01:40:39,820
goes down

1498
01:40:39,880 --> 01:40:41,370
then clearly

1499
01:40:41,380 --> 01:40:43,720
what will happen is that the

1500
01:40:43,770 --> 01:40:45,850
computing will go down

1501
01:40:45,870 --> 01:40:46,920
and that means

1502
01:40:46,970 --> 01:40:48,320
road will go up

1503
01:40:48,420 --> 01:40:50,520
so you get more resistance

1504
01:40:50,550 --> 01:40:52,900
so when you read out the substance

1505
01:40:52,960 --> 01:40:55,550
the resistance goes up higher temperature

1506
01:40:55,560 --> 01:40:58,070
higher resistance

1507
01:40:58,130 --> 01:41:01,770
sort of moments that the resistance are

1508
01:41:01,820 --> 01:41:03,570
becomes a function

1509
01:41:03,580 --> 01:41:07,080
of the temperature i call the total breakdown

1510
01:41:08,640 --> 01:41:10,560
because i are

1511
01:41:10,560 --> 01:41:13,920
a total breakdown of law

1512
01:41:13,980 --> 01:41:15,560
if you look in your book

1513
01:41:15,570 --> 01:41:18,410
they say oh no no no no no breakdown

1514
01:41:18,470 --> 01:41:20,770
you just have to just the

1515
01:41:20,780 --> 01:41:23,250
resistance four different temperatures

1516
01:41:24,710 --> 01:41:29,510
that's incredible performance way of saving all that

1517
01:41:29,520 --> 01:41:30,770
very badly

1518
01:41:31,650 --> 01:41:35,250
the temperature itself is a function of current high and the current i in the

1519
01:41:36,580 --> 01:41:38,360
and so now you get ratio

1520
01:41:38,380 --> 01:41:40,350
the divided by night

1521
01:41:40,370 --> 01:41:42,790
which is no longer constant

1522
01:41:42,860 --> 01:41:46,140
it becomes a function of the current

1523
01:41:46,180 --> 01:41:48,840
that's the end of vauxhall

1524
01:41:48,850 --> 01:41:50,560
so i want to show you

1525
01:41:50,620 --> 01:41:54,070
but if i do the same experiment i did here

1526
01:41:54,210 --> 01:41:57,100
but i've replaced biological

1527
01:41:57,150 --> 01:42:00,030
o fifty ohms it's a very small light bulb

1528
01:42:00,920 --> 01:42:02,530
when it is hot

1529
01:42:02,570 --> 01:42:03,790
fifty ohms

1530
01:42:03,800 --> 01:42:05,340
when it is cold

1531
01:42:07,160 --> 01:42:10,400
so are called the light bulb

1532
01:42:10,520 --> 01:42:12,900
roughly seven hours i believe

1533
01:42:12,920 --> 01:42:15,260
but i know that when it is hot

1534
01:42:15,300 --> 01:42:17,990
it's very close to fifty ohms

1535
01:42:18,040 --> 01:42:20,310
because of the low

1536
01:42:20,380 --> 01:42:22,610
what do you expect no

1537
01:42:23,700 --> 01:42:24,810
you expect now

1538
01:42:24,840 --> 01:42:27,370
the resistance is low in the beginning

1539
01:42:27,420 --> 01:42:28,790
you get this

1540
01:42:28,870 --> 01:42:29,580
and then

1541
01:42:29,590 --> 01:42:31,270
and the resistance goes out

1542
01:42:31,310 --> 01:42:33,510
you you get this

1543
01:42:33,510 --> 01:42:36,970
i may end up a little higher current because i think the resistance the load

1544
01:42:37,040 --> 01:42:38,750
fifty o

1545
01:42:38,760 --> 01:42:42,620
and if you see curve like this that's nowhere near any more

1546
01:42:42,670 --> 01:42:43,950
that's the end

1547
01:42:43,950 --> 01:42:46,490
of the well

1548
01:42:46,500 --> 01:42:49,030
and that's what i want to show you know

1549
01:42:49,080 --> 01:42:52,600
so all i do is you have this little light those of you sit close

1550
01:42:52,600 --> 01:42:55,480
they can actually see the light bulb

1551
01:42:55,520 --> 01:42:58,800
glowing but that's not important i really wanted to see that

1552
01:42:58,820 --> 01:43:03,290
the first is i is no longer linear there you go

1553
01:43:03,300 --> 01:43:05,960
you see every time this is libel going

1554
01:43:06,000 --> 01:43:09,160
it's up and running the heating up

1555
01:43:10,450 --> 01:43:12,520
the resistance increases

1556
01:43:12,600 --> 01:43:16,560
and the end of small for this libel but will find

1557
01:43:16,560 --> 01:43:19,860
forty other sister but it was not find

1558
01:43:19,920 --> 01:43:25,960
for this libel

1559
01:43:27,980 --> 01:43:32,760
there's another way

1560
01:43:32,770 --> 01:43:34,480
but i can show you

1561
01:43:36,410 --> 01:43:37,980
was law

1562
01:43:37,980 --> 01:43:42,400
it's not always doing so well

1563
01:43:42,460 --> 01:43:45,800
i have on the twenty five vols

1564
01:43:45,830 --> 01:43:47,750
i was applied

1565
01:43:47,830 --> 01:43:51,470
these hundred and twenty five goals

1566
01:43:51,520 --> 01:43:55,290
the potential difference

1567
01:43:55,340 --> 01:43:57,260
and i have like

1568
01:43:57,260 --> 01:43:59,460
you see here

1569
01:43:59,530 --> 01:44:00,530
the light bulb

1570
01:44:00,530 --> 01:44:04,140
happened is that the first the first nine people would say that sees the the

1571
01:44:04,320 --> 01:44:08,280
the closest mainland and then the last person would say same

1572
01:44:08,330 --> 01:44:09,450
which is again like

1573
01:44:09,460 --> 01:44:17,630
the pressure sort of time

1574
01:44:17,680 --> 01:44:27,860
i think they would be because becomes because they must undergo going long period of

1575
01:44:27,860 --> 01:44:30,860
time by

1576
01:44:33,430 --> 01:44:36,790
so these are like the flavor of the studies that that that are going on

1577
01:44:36,790 --> 01:44:37,100
and now

1578
01:44:37,980 --> 01:44:38,850
i want to

1579
01:44:38,950 --> 01:44:42,930
just a bit of of labour so for example a very basic question from model

1580
01:44:43,020 --> 01:44:48,460
this phenomenon is what's the probability of of adopting new behavior

1581
01:44:48,510 --> 01:44:51,460
i know as more of your friends

1582
01:44:51,500 --> 01:44:56,670
i don't so basically how does this influence propagation on you and there there are

1583
01:44:57,270 --> 01:45:02,510
the hypothesis that could could make right please the most evident ones are first one

1584
01:45:02,510 --> 01:45:06,000
would be like diminishing returns type of typeofgood right so if all x x is

1585
01:45:06,000 --> 01:45:10,320
a lot the number of friends i know quality on an ipod and y axis

1586
01:45:10,320 --> 01:45:14,540
is the probability that i go and buy and i i do something right and

1587
01:45:14,670 --> 01:45:18,570
the idea would be the more friends more friends of the more likely i am

1588
01:45:18,610 --> 01:45:20,690
to go and buy but

1589
01:45:20,710 --> 01:45:24,750
as as the number of the increases sort of the probability that in some parts

1590
01:45:24,750 --> 01:45:28,640
of the city for example one one hypothesis the hypothesis would be would be the

1591
01:45:29,040 --> 01:45:32,900
critical mass of some kind of this kind of behaviour you would say i don't

1592
01:45:32,900 --> 01:45:34,870
really care when only a few people

1593
01:45:34,890 --> 01:45:38,930
i don't own and i i i phone or ipod but once i know some

1594
01:45:38,930 --> 01:45:42,860
critical mass in the homes of the things i'm going i would also like to

1595
01:45:42,870 --> 01:45:44,730
go out of their behavior some

1596
01:45:45,470 --> 01:45:47,340
so these are these are two

1597
01:46:08,260 --> 01:46:15,230
i notified him of densities but

1598
01:46:15,280 --> 01:46:19,830
the only the only recently we were able to start to measure this so until

1599
01:46:19,850 --> 01:46:21,790
i two years ago

1600
01:46:21,810 --> 01:46:24,970
the sample size for which you can produce

1601
01:46:24,990 --> 01:46:28,430
you need to be the sample size to be able to produce this curve i

1602
01:46:28,430 --> 01:46:33,020
know small bars and only recently been able to start doing so and there are

1603
01:46:33,020 --> 01:46:37,440
a lot of questions that i mean i mean touch and the later on the

1604
01:46:37,460 --> 01:46:41,690
there's a lot more time behind but this is this is like a very basic

1605
01:46:41,690 --> 01:46:45,190
question about what if you want to have a model of how the whole something

1606
01:46:45,190 --> 01:46:48,820
spread over the network should be able to say how likely am my

1607
01:46:48,840 --> 01:46:53,130
to become idol to adopt some behavior or something as as more and more of

1608
01:46:53,130 --> 01:46:54,270
my friends

1609
01:46:54,290 --> 01:47:01,640
about the behaviour and

1610
01:47:21,950 --> 01:47:29,090
either the

1611
01:47:29,270 --> 01:47:39,210
it will be all over the world

1612
01:47:39,230 --> 01:47:43,420
or one

1613
01:47:44,660 --> 01:47:49,160
so i think this is a general philosophical question there is always this discrepancy between

1614
01:47:49,160 --> 01:47:53,630
machine learning models where i know all we care about is precision recall or whatever

1615
01:47:53,630 --> 01:47:57,400
you like that that these physics style models well

1616
01:47:57,420 --> 01:47:59,230
you want to know some

1617
01:47:59,240 --> 01:48:03,960
large-scale property to be modeled by by small and i think that's the question the

1618
01:48:03,960 --> 01:48:07,810
same question you are asking right i can always throw an SVM agreed and say

1619
01:48:07,810 --> 01:48:09,960
this is my position but

1620
01:48:10,000 --> 01:48:12,840
use SEM the model from life

1621
01:48:12,850 --> 01:48:15,680
explaining the process SVM is not the model even though

1622
01:48:15,840 --> 01:48:19,090
we have one hundreds of parameters and should work well

1623
01:48:19,090 --> 01:48:22,950
so i think it's here is the same as you can see what i mean

1624
01:48:22,950 --> 01:48:26,720
by by models here is something that may be that you would be able to

1625
01:48:26,720 --> 01:48:31,290
generalise across domains but then if you really want to go into other solid particulate

1626
01:48:31,290 --> 01:48:34,080
prediction tasks or a particular model does

1627
01:48:34,090 --> 01:48:36,130
mention should would go to more

1628
01:48:36,190 --> 01:48:39,380
i machine learning data mining type of models

1629
01:48:39,420 --> 01:48:40,910
is that

1630
01:48:42,210 --> 01:48:43,880
so all i want to say here is

1631
01:48:43,890 --> 01:48:47,280
there are these this let's say competing hypotheses and this will have a big impact

1632
01:48:47,280 --> 01:48:51,760
on how think about processes spreading over the networks and how to model

1633
01:48:51,770 --> 01:48:55,750
and now i want to touch on some of this money she's right there

1634
01:48:55,800 --> 01:49:00,280
two flavors of types of questions that people ask when they think about diffusion one

1635
01:49:00,290 --> 01:49:03,620
is about the virus propagation models where we want to see

1636
01:49:03,640 --> 01:49:06,870
two when we ask ourselves how how does it the

1637
01:49:06,870 --> 01:49:10,180
the this is spread over the network and the the

1638
01:49:10,780 --> 01:49:14,980
most basic or the most interesting question there is this what they call the wind

1639
01:49:14,980 --> 01:49:18,140
in the wires they taken so basically

1640
01:49:18,390 --> 01:49:25,390
so this is this is what are interested in whether the particular what that has

1641
01:49:25,640 --> 01:49:29,500
really take over the network or then there are two ways to describe what this

1642
01:49:29,500 --> 01:49:34,600
is or the basic ways one is the SIS model where which is something like

1643
01:49:34,600 --> 01:49:40,100
if write your subset susceptible because you become infected or in fact and after a

1644
01:49:40,100 --> 01:49:44,720
while he loved but as you like you can get infected right so this would

1645
01:49:44,720 --> 01:49:47,650
be so this is one model and the other one one is something like a

1646
01:49:47,690 --> 01:49:48,850
school essay

1647
01:49:48,860 --> 01:49:55,730
SAR where you become susceptible infected and you are covered remove so chickenpox or play

1648
01:49:56,010 --> 01:50:00,250
would be something like this right place the outcomes fatal and you removed from the

1649
01:50:00,250 --> 01:50:04,500
network with chickenpox is sort of become immune to the disease and these are the

1650
01:50:04,500 --> 01:50:08,090
two most basic models that people think about them

1651
01:50:08,110 --> 01:50:13,500
i will also talk about the diffusion models where i will touch on two most

1652
01:50:13,500 --> 01:50:16,340
classical of the and what people who

1653
01:50:16,360 --> 01:50:21,000
do that they are more interested in when does a particular person about the behavior

1654
01:50:21,000 --> 01:50:25,980
and so so so in the diffusion you sort of have control of over yourself

1655
01:50:25,980 --> 01:50:29,100
one virus propagation sort because you don't

1656
01:50:29,150 --> 01:50:30,940
let's go about

1657
01:50:30,950 --> 01:50:35,420
the virus propagation so basically the question is how do viruses propagate over the network

1658
01:50:36,350 --> 01:50:40,250
what will look at very very briefly is like from like while wires and the

1659
01:50:40,250 --> 01:50:42,160
question is whether you like

1660
01:50:42,180 --> 01:50:45,370
staying the network forever only to become extinct

1661
01:50:45,420 --> 01:50:49,560
and there would be two parameters with the reduced to

1662
01:50:49,560 --> 01:50:51,110
to describe

1663
01:50:51,150 --> 01:50:55,810
the devices so we have the virus but which would be like a probability that

1664
01:50:56,020 --> 01:50:58,890
a particular infected anybody affects the other

1665
01:50:58,940 --> 01:51:03,310
so here i have a small social networking let's say that disputed guys are infected

1666
01:51:03,310 --> 01:51:06,170
so with probability based on each one of the effects the

1667
01:51:06,610 --> 01:51:07,170
it is

1668
01:51:07,170 --> 01:51:10,700
but i haven't put in the story so i'm not gonna talk about that's all

1669
01:51:10,700 --> 01:51:15,750
these iterative methods multidimensional scaling what i want to talk about the is the machine

1670
01:51:15,750 --> 01:51:20,280
learning communities contributions and how they relate to what i've talked about so here's an

1671
01:51:20,280 --> 01:51:21,930
idea to get a nonlinear algorithm

1672
01:51:22,930 --> 01:51:24,820
so compute the distances

1673
01:51:26,310 --> 01:51:29,480
in a space that is non linearly related to the original data

1674
01:51:31,620 --> 01:51:34,830
we can use the distance we like so why don't we map to a non

1675
01:51:34,830 --> 01:51:39,630
linear space compute the distances there and then use the linear algorithm so this is

1676
01:51:39,630 --> 01:51:44,920
like the citric the banner was talking about where u nonlinear eyes and then you

1677
01:51:44,920 --> 01:51:47,780
do something linear and non linear space same trick it

1678
01:51:48,680 --> 01:51:49,440
i'm going to introduce

1679
01:51:50,510 --> 01:51:52,780
it's not always the same trick in the way it's being applied

1680
01:51:53,740 --> 01:51:57,540
although the kernel people would tell you know the nice thing is bernard's not his

1681
01:51:57,540 --> 01:52:01,330
so i can be very rude about can appreciate as long as the cameras not

1682
01:52:01,340 --> 01:52:06,230
switched off dialog and we wrote about sick is very important algorithm i

1683
01:52:08,540 --> 01:52:10,260
and it doesn't rely on this idea

1684
01:52:10,710 --> 01:52:13,440
but here is my interpretation of what's going on with it

1685
01:52:13,840 --> 01:52:15,780
so it should be i and after

1686
01:52:18,180 --> 01:52:22,190
let's nonlinearly map the data to a new space and compute the distances there and

1687
01:52:22,190 --> 01:52:25,970
we're gonna do this using basis function models i think these very nice simple models

1688
01:52:25,970 --> 01:52:27,290
and i'll show an illustration of this

1689
01:52:27,710 --> 01:52:30,510
so i provided moment so we may apply

1690
01:52:31,070 --> 01:52:35,070
a new representation and why i and we set up by the function why i

1691
01:52:35,070 --> 01:52:40,380
such that is a linear sum of some basis functions computed each why i and

1692
01:52:40,380 --> 01:52:44,350
the end basis functions and this is then the squared distance between that should be

1693
01:52:44,350 --> 01:52:45,450
enough i and j

1694
01:52:48,010 --> 01:52:49,510
absolutely late-night notation change

1695
01:52:53,460 --> 01:52:57,110
i have a common basis functions used is this what i like to call

1696
01:52:57,620 --> 01:52:59,130
an exponentiated quadratic

1697
01:53:00,000 --> 01:53:02,840
because it's not a guassian is also not squared exponential

1698
01:53:03,330 --> 01:53:06,940
and some people don't like radial basis function but it's got all these names

1699
01:53:07,700 --> 01:53:10,330
and it's just as we'll see in a moment and a little blob

1700
01:53:11,290 --> 01:53:13,300
now and then we've got these elements over

1701
01:53:14,210 --> 01:53:18,120
these basis functions which we can write as a matrix and we tend to write

1702
01:53:18,120 --> 01:53:21,020
these sort of things in the form of the design matrix for the set of

1703
01:53:21,030 --> 01:53:22,150
vectors so each

1704
01:53:23,240 --> 01:53:23,950
column of this

1705
01:53:24,920 --> 01:53:29,740
design matrix gives you evaluation of one of the basis functions all the data different

1706
01:53:29,740 --> 01:53:31,870
data points and error and columns

1707
01:53:32,350 --> 01:53:33,240
one column for each

1708
01:53:34,040 --> 01:53:38,510
location when computing the basis function so there's this location parameter u mu

1709
01:53:39,000 --> 01:53:43,430
which you have to make a decision about this which the basis functions so for the moment

1710
01:53:44,640 --> 01:53:51,900
we're gonna decided distribute basis functions uniformly along a one dimensional input we cannot assume that the data

1711
01:53:52,360 --> 01:53:54,850
as i mentioned reducing is one dimensional

1712
01:53:55,400 --> 01:53:57,800
i know that's a bit strange that that makes it easier to port

1713
01:54:00,910 --> 01:54:03,260
in matrix notation as i said all vector notation

1714
01:54:03,720 --> 01:54:05,220
we have this that they are

1715
01:54:05,960 --> 01:54:10,390
function of the data is given by w transpose phi i where this is

1716
01:54:13,450 --> 01:54:14,170
rho from thee

1717
01:54:16,190 --> 01:54:20,480
from this design matrix is so that's one of those rose and w is the weighting in vitro

1718
01:54:21,260 --> 01:54:22,600
okay so we're going to do this

1719
01:54:23,060 --> 01:54:27,510
mapping of the basis functions but where we now introduce a set new parameters which

1720
01:54:27,510 --> 01:54:28,850
is a little bit annoying because

1721
01:54:29,830 --> 01:54:32,890
what parameters should be used for computing this distance well

1722
01:54:34,120 --> 01:54:40,710
let's just generate random functions and look at the expectation that these random function so we're not gonna specify

1723
01:54:45,300 --> 01:54:48,010
values w is gonna specify probability density

1724
01:54:48,830 --> 01:54:52,640
and then we compute the expected squared distance is now the nice thing is that

1725
01:54:52,720 --> 01:54:54,690
if i minus the jay squared

1726
01:54:54,950 --> 01:54:56,120
is equal to this

1727
01:54:56,540 --> 01:54:59,110
so this is the basis function transposed w minus

1728
01:54:59,820 --> 01:55:02,120
the basis functions computed for the i th data point

1729
01:55:03,460 --> 01:55:07,540
in a product with w minus the basis functions computed the jay data point in

1730
01:55:07,610 --> 01:55:09,740
product w just using the notation there

1731
01:55:10,340 --> 01:55:11,860
multiply broken out

1732
01:55:12,870 --> 01:55:14,530
we can see it in this form

1733
01:55:14,950 --> 01:55:19,630
and now we take expectation of these square distances on the pier w

1734
01:55:22,840 --> 01:55:27,320
let's just assume that second moment this distribution we're not saying anything about the distribution

1735
01:55:27,320 --> 01:55:32,500
of haven't said calcium haven't said it's anything all i'm saying far second moment exists

1736
01:55:33,360 --> 01:55:35,580
and the second moment distribution is gonna be

1737
01:55:36,560 --> 01:55:37,620
the identity matrix

1738
01:55:38,520 --> 01:55:42,560
so the expected squared distances on the these random functions with projecting

1739
01:55:45,530 --> 01:55:46,280
is equal to this

1740
01:55:48,740 --> 01:55:53,990
basis functions minus about computer i minus the basis functions computed jay in the product

1741
01:55:54,910 --> 01:55:55,640
with themselves

1742
01:55:57,020 --> 01:56:02,240
if we also said that the mean this thing with zero the covariance of this distribution would be zero

1743
01:56:03,210 --> 01:56:05,950
now if you could think about it if you like but i haven't said

1744
01:56:09,380 --> 01:56:11,840
it could be anything at this stage actually show you

1745
01:56:12,300 --> 01:56:17,200
so illustrations i'm going to assume it's calcium describe the sample from something but it doesn't have to be gaussian

1746
01:56:19,460 --> 01:56:21,090
so is a set of basis functions

1747
01:56:21,590 --> 01:56:25,930
we think which one hand location parameter minus one zero one one

1748
01:56:27,220 --> 01:56:29,510
so that's the design matrix is

1749
01:56:30,150 --> 01:56:35,250
being computed now this is a visualization of the design matrix for every point between minus two into yeah

1750
01:56:36,400 --> 01:56:41,850
so what i'm saying i'm saying well to compute the distances we're gonna sample weight

1751
01:56:41,850 --> 01:56:46,240
vector three dimensional weight vector in this case it's going away that's a plus that's

1752
01:56:46,270 --> 01:56:48,050
plus back with some calcium waiting

1753
01:56:48,500 --> 01:56:52,090
and now give us a function and we compute distances on flat function

1754
01:56:54,340 --> 01:56:57,510
that's what that looks like so here's our input this is the original why

1755
01:56:58,200 --> 01:56:59,790
and where mapping from out why

1756
01:57:00,380 --> 01:57:02,190
two example one w

1757
01:57:02,560 --> 01:57:03,980
and it's given me this function here

1758
01:57:05,230 --> 01:57:07,410
and the distance between these two is given by the fact

1759
01:57:08,270 --> 01:57:12,050
and what we what we've written down is what the expectation that the square error

1760
01:57:12,060 --> 01:57:14,480
that's because we are interested in squared distances

1761
01:57:15,870 --> 01:57:16,910
so a mapping from

1762
01:57:17,390 --> 01:57:18,000
input why

1763
01:57:18,410 --> 01:57:20,680
two and not in a nonlinear way some space

1764
01:57:21,800 --> 01:57:23,340
given by some function space

1765
01:57:24,620 --> 01:57:28,830
okay so that's one sample but we're gonna do over multiple samples now look at

1766
01:57:28,830 --> 01:57:32,800
this was nice is that if these because this is a smooth function if these

1767
01:57:32,860 --> 01:57:34,430
inputs are close together here

1768
01:57:35,120 --> 01:57:37,560
they're actually gonna be close together in there as well

1769
01:57:39,480 --> 01:57:40,730
if there were further apart

1770
01:57:41,460 --> 01:57:42,120
so here

1771
01:57:42,810 --> 01:57:43,820
if they were the

1772
01:57:43,950 --> 01:57:48,360
zero and minus two then the distance without much greater not always at much greater

1773
01:57:48,360 --> 01:57:50,540
because it could be the but biggest by randomness

1774
01:57:51,190 --> 01:57:55,550
it could be closer on average the distances will be that much greater yeah

1775
01:57:56,280 --> 01:57:57,680
so makes sense clear

1776
01:57:59,040 --> 01:58:02,330
so doesn't seem like a good idea and will compute distances and that's space

1777
01:58:05,690 --> 01:58:06,800
so there is some other samples

1778
01:58:08,750 --> 01:58:10,060
again it seems like quite a good idea

1779
01:58:10,520 --> 01:58:12,570
i quite like the idea so we're gonna then do

1780
01:58:13,100 --> 01:58:15,460
classical multidimensional scaling on that's space

1781
01:58:15,860 --> 01:58:19,630
there is a slight problem with the idea because we located basis functions

1782
01:58:20,100 --> 01:58:24,620
minus one zero one so can tell me what's gonna happen if i have data

1783
01:58:24,620 --> 01:58:27,330
points which is why value is minus four and four

1784
01:58:31,590 --> 01:58:34,870
yeah will be zero apart when the same small distance apart

1785
01:58:35,360 --> 01:58:37,180
that's gonna be pretty bad so

1786
01:58:38,580 --> 01:58:39,070
here you go

1787
01:58:40,220 --> 01:58:43,440
if we got point just outside the span of basis functions

1788
01:58:44,030 --> 01:58:45,370
this doesn't work quite so well

1789
01:58:46,280 --> 01:58:48,450
well part but they'll always be zero

1790
01:58:50,030 --> 01:58:51,510
distance apart in the new space

1791
01:58:52,550 --> 01:58:53,130
she's annoying

1792
01:58:55,380 --> 01:59:01,450
so that's slightly problematic so this is very small despite the data being far apart so there's an issue there

1793
01:59:01,910 --> 01:59:05,070
and this is a side effect of bad basis function placement basically if you put

1794
01:59:05,070 --> 01:59:07,770
your basis functions over where your data was this would happen

1795
01:59:07,770 --> 01:59:09,790
zero then of course

1796
01:59:10,130 --> 01:59:15,490
why is it is also the things you see so these zero is is a

1797
01:59:15,490 --> 01:59:18,470
very simple visible point

1798
01:59:18,520 --> 01:59:24,670
so in this decomposition procedure usually is start from from from the zero vector is

1799
01:59:24,670 --> 01:59:29,490
you intend maintaining feasibility so so from one iteration

1800
01:59:29,500 --> 01:59:34,640
two another iteration your solutions always feasible

1801
01:59:36,320 --> 01:59:39,130
so we start from zero vector

1802
01:59:39,150 --> 01:59:41,010
and if this happens in the end

1803
01:59:41,020 --> 01:59:42,270
so you have to optimize

1804
01:59:42,280 --> 01:59:47,380
a lot of support i mean the number of support vectors uses all sorts of

1805
01:59:49,850 --> 01:59:53,140
only very few of length they are not zeros

1806
01:59:53,150 --> 01:59:56,420
everything is zero then

1807
01:59:56,440 --> 02:00:01,680
in this situation the composition is they work very well so we

1808
02:00:01,700 --> 02:00:05,740
we need to we need to have i want to be to select you stop

1809
02:00:05,770 --> 02:00:12,200
changing several components OK so maybe the first iteration change some of the second problem

1810
02:00:12,240 --> 02:00:17,660
here the end of the because up only a few are nonzero so it's very

1811
02:00:17,660 --> 02:00:20,370
possible that loss

1812
02:00:20,390 --> 02:00:25,070
but as you victories in front of using many components and they were changed

1813
02:00:25,100 --> 02:00:26,770
so you only touch this

1814
02:00:26,780 --> 02:00:28,880
the subset of components

1815
02:00:29,020 --> 02:00:35,700
in optimisation procedure but in a situation that this decomposition is going to be first

1816
02:00:36,150 --> 02:00:39,650
to this is an example

1817
02:00:39,650 --> 02:00:47,910
machine learning knowledge affects the optimisation procedure because it is an optimisation decision they

1818
02:00:48,030 --> 02:00:52,230
i don't think according to the loss minimisation is is good

1819
02:00:53,160 --> 02:00:55,790
but for this one

1820
02:00:56,000 --> 02:00:58,470
they have certain properties for sale

1821
02:00:58,490 --> 02:01:02,550
so that this is a practical procedure

1822
02:01:02,560 --> 02:01:04,200
so i can show you an example

1823
02:01:04,220 --> 02:01:12,430
so we training fifty thousand instances so you can see in some way

1824
02:01:12,530 --> 02:01:20,560
so we have a lot of iterations time for so many iterations it tough but

1825
02:01:20,570 --> 02:01:22,520
the total number of support vectors is

1826
02:01:22,970 --> 02:01:27,540
resulted of his own is much smaller than the fifty so this this is a

1827
02:01:27,540 --> 02:01:33,450
very good situation so very likely that you start from the zero vector and many

1828
02:01:33,450 --> 02:01:36,060
components and they were changed

1829
02:01:37,190 --> 02:01:40,260
so so these this takes five minutes

1830
02:01:41,860 --> 02:01:43,870
just go to the top

1831
02:01:43,940 --> 02:01:49,720
well you go go looking at the whole matrix q OK fifty by fifty seven

1832
02:01:49,720 --> 02:01:52,480
fifty thousand

1833
02:01:52,490 --> 02:01:57,750
just just just means calculations that may already know already take more than five minutes

1834
02:01:57,770 --> 02:02:02,520
so the reason is because number support vectors is much smaller than the number of

1835
02:02:02,520 --> 02:02:04,150
training instances

1836
02:02:04,150 --> 02:02:06,440
but of course it is

1837
02:02:06,610 --> 02:02:15,390
these may not always have have evidence in some situations

1838
02:02:15,400 --> 02:02:19,470
there are lots of issues

1839
02:02:19,480 --> 02:02:22,170
about this decomposition method

1840
02:02:22,220 --> 02:02:27,470
first we need to know how to select working set so that we should use

1841
02:02:27,480 --> 02:02:31,140
what is to be decide the size of the working set sort things at each

1842
02:02:31,140 --> 02:02:36,910
iteration you change how many variables we can change you only made as i said

1843
02:02:36,920 --> 02:02:42,020
there are only two variables but you can change more of course if you if

1844
02:02:42,020 --> 02:02:46,520
you change more than the course at each iteration is going to be hired by

1845
02:02:46,530 --> 02:02:50,410
the number of iterations may be small you only

1846
02:02:50,450 --> 02:02:56,010
there are a few very few components then the cost per iteration is it is

1847
02:02:56,010 --> 02:02:59,640
all about that number three since is quite high

1848
02:02:59,640 --> 02:03:01,470
i would be very tired

1849
02:03:01,530 --> 02:03:03,060
i think we would all agree

1850
02:03:03,090 --> 02:03:05,930
and if i stand here twenty four hours like this

1851
02:03:05,960 --> 02:03:07,780
but i would get very tired

1852
02:03:07,830 --> 02:03:10,560
i haven't done any work i might have reported here that

1853
02:03:10,610 --> 02:03:11,670
the table just

1854
02:03:11,680 --> 02:03:12,830
hold that

1855
02:03:12,860 --> 02:03:16,970
briefcase from so it's clear that you can get very tired

1856
02:03:16,980 --> 02:03:18,470
without having done

1857
02:03:18,530 --> 02:03:19,920
any work

1858
02:03:19,930 --> 02:03:22,310
what is the the way we define work

1859
02:03:22,360 --> 02:03:23,870
in physics

1860
02:03:23,920 --> 02:03:26,620
now let's go from one dimensions two

1861
02:03:26,650 --> 02:03:32,480
three dimensions is not very much difference as you will see

1862
02:03:32,540 --> 02:03:36,780
i go in three dimensions from point a to point

1863
02:03:37,890 --> 02:03:39,550
and i now have a force

1864
02:03:42,950 --> 02:03:46,950
could be pointing not just along the x direction but in general

1865
02:03:46,990 --> 02:03:48,730
in all the actions now the

1866
02:03:48,760 --> 02:03:51,910
work that the force is doing going from a to b

1867
02:03:53,580 --> 02:03:56,920
after dot dr

1868
02:03:56,970 --> 02:03:59,350
are is the position

1869
02:03:59,390 --> 02:04:04,840
in three-dimensional space where the forces at that moment and the are is small

1870
02:04:07,790 --> 02:04:10,080
this is from a

1871
02:04:10,140 --> 02:04:11,530
to be

1872
02:04:13,390 --> 02:04:16,300
the are here to go in this direction

1873
02:04:16,360 --> 02:04:18,700
this will be little fact dr are

1874
02:04:18,830 --> 02:04:20,680
and here

1875
02:04:20,740 --> 02:04:22,910
that would be an ineffective dr

1876
02:04:22,950 --> 02:04:24,840
and the force itself

1877
02:04:24,900 --> 02:04:26,110
could be like this

1878
02:04:26,930 --> 02:04:28,160
in the force

1879
02:04:28,200 --> 02:04:30,260
could be like is there

1880
02:04:30,300 --> 02:04:33,700
the force can obviously change

1881
02:04:34,660 --> 02:04:36,640
this past

1882
02:04:36,650 --> 02:04:38,310
so that the force

1883
02:04:40,490 --> 02:04:44,160
f of x

1884
02:04:44,280 --> 02:04:47,620
x roof was ever y

1885
02:04:47,700 --> 02:04:48,910
my rule

1886
02:04:48,960 --> 02:04:51,500
was f of c

1887
02:04:51,610 --> 02:04:53,050
zero of

1888
02:04:53,070 --> 02:04:56,000
a movie is a up a little

1889
02:04:56,010 --> 02:04:56,750
but it here

1890
02:04:56,760 --> 02:04:58,980
and that dr

1891
02:04:59,030 --> 02:05:02,900
general notation for vector dr

1892
02:05:02,910 --> 02:05:04,400
equals dx

1893
02:05:04,450 --> 02:05:06,650
x rule

1894
02:05:06,730 --> 02:05:08,230
plus the y

1895
02:05:09,200 --> 02:05:12,210
it was easy to you

1896
02:05:12,260 --> 02:05:14,800
cannot be any more general

1897
02:05:14,840 --> 02:05:18,390
so the work that this force is doing

1898
02:05:18,440 --> 02:05:22,300
when it moves from a to b

1899
02:05:22,450 --> 02:05:24,880
in the world

1900
02:05:29,980 --> 02:05:31,510
let's first

1901
02:05:32,380 --> 02:05:35,800
small displacement over the are

1902
02:05:35,820 --> 02:05:37,560
then i get the w

1903
02:05:37,620 --> 02:05:38,760
that is simply

1904
02:05:41,230 --> 02:05:44,160
dx scalar

1905
02:05:44,200 --> 02:05:46,060
this is the dot product

1906
02:05:46,120 --> 02:05:48,460
thus f y

1907
02:05:50,300 --> 02:05:52,670
was at seen

1908
02:05:54,200 --> 02:05:56,030
that is

1909
02:05:56,040 --> 02:05:59,310
the amount of work if the

1910
02:05:59,370 --> 02:06:01,710
four is this place over

1911
02:06:01,800 --> 02:06:03,340
distance dr

1912
02:06:03,390 --> 02:06:05,560
now i have to do the integrals

1913
02:06:06,340 --> 02:06:07,710
the entire

1914
02:06:07,760 --> 02:06:09,910
pass to get w

1915
02:06:10,030 --> 02:06:13,650
a to b

1916
02:06:13,660 --> 02:06:16,860
the integral going from a to b

1917
02:06:16,870 --> 02:06:20,380
the integral going from a to b

1918
02:06:20,400 --> 02:06:24,040
i don't need it anymore

1919
02:06:26,840 --> 02:06:28,910
in going from a to b

1920
02:06:30,560 --> 02:06:33,160
in going from a to b

1921
02:06:33,200 --> 02:06:35,150
now we home free

1922
02:06:35,160 --> 02:06:36,660
because we already that this

1923
02:06:36,670 --> 02:06:39,070
this is the one-dimensional problems

1924
02:06:39,080 --> 02:06:42,590
and the one-dimensional problem we already know the outcome

1925
02:06:45,550 --> 02:06:46,660
we found

1926
02:06:46,750 --> 02:06:48,360
it is one half

1927
02:06:48,370 --> 02:06:53,110
the squared minus the squared which in this case is obviously

1928
02:06:53,120 --> 02:06:57,440
the velocity in the x direction because this is the one dimensional problem and the

1929
02:06:57,440 --> 02:07:01,780
one dimensional problem indicates that the velocity that i'm dealing with is the component in

1930
02:07:01,780 --> 02:07:03,030
this direction

1931
02:07:03,080 --> 02:07:04,390
so we have

1932
02:07:04,440 --> 02:07:05,380
this is

1933
02:07:05,390 --> 02:07:06,740
one half

1934
02:07:09,670 --> 02:07:10,980
the squared

1935
02:07:11,000 --> 02:07:12,800
and this is the x component

1936
02:07:12,860 --> 02:07:18,560
minus the a squared and that's the x component is also one dimensional problem except

1937
02:07:18,560 --> 02:07:22,960
that now i deal with the component is the y component of the velocity so

1938
02:07:22,960 --> 02:07:24,560
i get one half and

1939
02:07:25,280 --> 02:07:26,540
time VB

1940
02:07:26,610 --> 02:07:30,020
y squared minus

1941
02:07:33,050 --> 02:07:36,780
y squared plus one half and

1942
02:07:37,760 --> 02:07:43,050
the square minus the a z

1943
02:07:44,540 --> 02:07:46,290
and our home free

1944
02:07:46,300 --> 02:07:49,930
because what you see here is you see these great

1945
02:07:49,940 --> 02:07:54,740
in the x direction create y component discrete component and if you add those three

1946
02:07:55,450 --> 02:07:56,690
you get exactly

1947
02:07:57,330 --> 02:08:00,990
the square of the velocity you get the square of the speed

1948
02:08:01,000 --> 02:08:03,880
so if you add up the three terms

1949
02:08:03,890 --> 02:08:06,440
you get up you get

1950
02:08:06,460 --> 02:08:08,960
the square

1951
02:08:09,020 --> 02:08:11,810
a lost more than put my hand in there

1952
02:08:11,870 --> 02:08:13,520
one half

1953
02:08:13,530 --> 02:08:16,380
time the squared

1954
02:08:16,410 --> 02:08:17,890
and here you see

1955
02:08:17,900 --> 02:08:19,570
a x created

1956
02:08:19,580 --> 02:08:21,680
it was created a c squared

1957
02:08:22,530 --> 02:08:24,480
the squared

1958
02:08:24,530 --> 02:08:28,050
and you get exactly the same we felt that you had before

1959
02:08:28,060 --> 02:08:29,750
namely that the work done

1960
02:08:29,810 --> 02:08:30,860
is the difference

1961
02:08:30,880 --> 02:08:32,180
in connecticut

1962
02:08:33,270 --> 02:08:34,910
you can always think of these

1963
02:08:37,830 --> 02:08:39,670
well as it is created

1964
02:08:39,760 --> 02:08:41,860
the speed is the magnitude squared

1965
02:08:41,880 --> 02:08:45,760
of the velocity

1966
02:08:45,770 --> 02:08:46,980
all right

1967
02:08:47,020 --> 02:08:50,540
i'd like to return to gravity

1968
02:08:51,560 --> 02:08:56,280
work on the three-dimensional situation

1969
02:08:56,280 --> 02:08:57,260
we have here

1970
02:08:57,280 --> 02:09:01,560
that is bx

1971
02:09:02,450 --> 02:09:05,390
and is busy

1972
02:09:05,430 --> 02:09:08,990
and is here this is the increasing value of y

1973
02:09:09,010 --> 02:09:13,150
and as you point eight in three dimensions

1974
02:09:13,260 --> 02:09:14,750
like this

1975
02:09:14,790 --> 02:09:16,210
and there is here

1976
02:09:16,240 --> 02:09:17,810
o point b

1977
02:09:17,990 --> 02:09:23,180
so you get a rough idea about this

1978
02:09:23,190 --> 02:09:25,890
three dimensions and y of b

1979
02:09:25,920 --> 02:09:31,880
miners while because h is to give to a high difference between eight and the

1980
02:09:35,060 --> 02:09:38,900
there's a gravitational force the object moves from a to b

1981
02:09:38,910 --> 02:09:41,070
i suppose it moves in some crazy way

1982
02:09:41,090 --> 02:09:43,930
because gravity alone could not do that there has to be to force if it

1983
02:09:43,930 --> 02:09:45,660
goes in a strange way

1984
02:09:45,810 --> 02:09:49,920
but i'm only calculating now the work is going to be done by gravity

1985
02:09:50,030 --> 02:09:52,820
other forces i ignore for now only want to know

1986
02:09:52,870 --> 02:09:54,960
the work the gravity is doing

1987
02:09:55,020 --> 02:09:57,000
you all that has a mass and

1988
02:09:57,040 --> 02:09:58,410
and so

1989
02:09:58,490 --> 02:10:01,210
there is a force and g

1990
02:10:02,210 --> 02:10:04,550
i can write down

1991
02:10:04,670 --> 02:10:07,980
force in vector notation it's in this direction

1992
02:10:08,030 --> 02:10:09,280
so now

1993
02:10:09,330 --> 02:10:11,580
i notice that there is only

1994
02:10:12,660 --> 02:10:14,730
value for f of y

1995
02:10:14,740 --> 02:10:20,760
but there is no value for f of x there is no value for

1996
02:10:20,800 --> 02:10:22,530
next day is zero

1997
02:10:22,570 --> 02:10:24,460
so f of y

1998
02:10:24,490 --> 02:10:26,000
equals minus gl

1999
02:10:26,610 --> 02:10:30,010
so if i calculate how to work

2000
02:10:30,020 --> 02:10:32,080
in going from a to b

