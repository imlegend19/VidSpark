1
00:00:00,000 --> 00:00:04,680
four members of

2
00:00:04,690 --> 00:00:08,420
because of

3
00:00:16,010 --> 00:00:18,010
this is intended to be

4
00:00:18,140 --> 00:00:20,390
the action

5
00:00:31,400 --> 00:00:35,370
it is

6
00:00:45,390 --> 00:00:47,510
he was

7
00:02:12,210 --> 00:02:19,150
one is

8
00:02:20,820 --> 00:02:24,870
o was

9
00:02:24,880 --> 00:02:25,970
it is

10
00:02:25,990 --> 00:02:31,690
how hard is o

11
00:02:31,930 --> 00:02:35,590
all these

12
00:03:23,440 --> 00:03:29,500
if you

13
00:03:40,450 --> 00:03:51,600
first of all

14
00:03:51,700 --> 00:04:01,460
through the world

15
00:04:01,560 --> 00:04:05,900
the first

16
00:04:46,930 --> 00:04:48,370
this something that

17
00:05:47,470 --> 00:05:51,320
here is the

18
00:05:51,340 --> 00:05:53,870
june cohen was all

19
00:06:01,380 --> 00:06:03,780
the answer is

20
00:06:03,780 --> 00:06:07,710
p people making a difference between the functionality are

21
00:06:07,770 --> 00:06:09,860
of some system and the

22
00:06:09,890 --> 00:06:12,440
parameters of the system but

23
00:06:14,260 --> 00:06:19,410
just the same thing and we become networks just like bits in this laptop here

24
00:06:20,000 --> 00:06:24,640
all parts of the functionality and of the parameters in a certain sense all the

25
00:06:24,640 --> 00:06:29,630
ways in which we can network which is the general computer which can compute anything

26
00:06:29,630 --> 00:06:34,340
this laptop can compute because this laptop essentially is we can work with a particular

27
00:06:34,380 --> 00:06:37,670
activation function for the continent

28
00:06:37,740 --> 00:06:46,670
then you don't have really this difference between functionality and parameters and what you can

29
00:06:46,670 --> 00:06:50,860
do with these we can networks is that you can

30
00:06:50,860 --> 00:06:57,400
use them as predictors of time series for example of histories of actions and

31
00:06:57,420 --> 00:07:00,080
and sensory inputs and you can

32
00:07:00,090 --> 00:07:06,670
you can

33
00:07:06,670 --> 00:07:12,280
compute the difference between what an the network is predicting and what really happens and

34
00:07:12,280 --> 00:07:17,010
this is what you want to minimize the cost and and then you can compute

35
00:07:17,010 --> 00:07:23,070
a gradient algorithm space in progress based essentially because there is a way of changing

36
00:07:23,070 --> 00:07:30,130
the voting system such that you minimize that you that you find

37
00:07:30,180 --> 00:07:32,650
that you compute the first derivative of the

38
00:07:32,670 --> 00:07:36,930
i wish that you have namely minimizes error with respect to the problem that is

39
00:07:36,930 --> 00:07:41,110
running on the weekend which is just as its weight matrix so this very nice

40
00:07:41,110 --> 00:07:42,840
you have

41
00:07:42,860 --> 00:07:44,200
a space

42
00:07:44,220 --> 00:07:46,880
of programs and in that space

43
00:07:46,910 --> 00:07:53,050
you find another reason you find gradient which leads to a better from

44
00:07:53,090 --> 00:07:56,930
now in the nineteen nineties and neural networks of this type so we can attract

45
00:07:57,260 --> 00:08:04,650
they were considered by some exciting and very general but it's in practical applications they

46
00:08:04,670 --> 00:08:11,200
just scale up well that was all toy problems but the situation has totally changed

47
00:08:11,220 --> 00:08:17,380
has totally changed now and is just one recent example that is by the way

48
00:08:17,380 --> 00:08:22,970
that's alex graves who used to be my phd student and now a poster community

49
00:08:22,990 --> 00:08:31,030
and he is now beating every other algorithm and connected handwriting recognition connected handwriting recognition

50
00:08:31,030 --> 00:08:33,820
is much more difficult than

51
00:08:33,880 --> 00:08:36,090
single digit recognition

52
00:08:36,090 --> 00:08:38,510
which we had before a couple of slides ago

53
00:08:38,610 --> 00:08:42,700
because there is no teacher tells you where and does

54
00:08:42,700 --> 00:08:47,130
this individual let's start here and what is it and all you have if you're

55
00:08:47,130 --> 00:08:49,930
lucky you have a set of

56
00:08:49,930 --> 00:08:54,200
training sequences and teacher told you what kind of

57
00:08:55,740 --> 00:08:57,220
letter labels

58
00:08:57,240 --> 00:09:01,820
this corresponds to but even the teacher doesn't know is that the latter here is

59
00:09:01,820 --> 00:09:06,300
that part of letters that just an accident is that

60
00:09:06,400 --> 00:09:09,680
is that the beginning of some letter at the end of it of another level

61
00:09:09,680 --> 00:09:13,430
and so on but what you can do is we you can do

62
00:09:14,990 --> 00:09:17,740
things just maximize the probability

63
00:09:18,490 --> 00:09:20,050
the probability is

64
00:09:20,090 --> 00:09:22,930
label sequences

65
00:09:22,950 --> 00:09:26,630
which in this case for example he'll blank b

66
00:09:27,040 --> 00:09:31,470
billowing blanket he old blank AG

67
00:09:31,470 --> 00:09:35,240
and you you want to match the somehow to this real valued input string they

68
00:09:35,240 --> 00:09:41,340
get here and you can maximize the probability of this label sequence given the input

69
00:09:42,990 --> 00:09:48,050
go all the training sequences and said and it turns out that if you do

70
00:09:48,050 --> 00:09:52,650
that right if you use the right type of networks you must use long short

71
00:09:52,650 --> 00:09:57,320
term memory recurrent networks which don't have all the problems that the original become networks

72
00:09:57,320 --> 00:10:02,860
have in particular they can deal with really long time lags between relevant events what

73
00:10:02,860 --> 00:10:08,070
the traditional african networks cannot do this and this is enough to know when all

74
00:10:08,110 --> 00:10:17,610
the handwriting competitions alex for example also achieve the best results and in around handwriting

75
00:10:17,630 --> 00:10:20,550
recognition contest

76
00:10:20,550 --> 00:10:22,590
although he doesn't speak a word of arabic

77
00:10:22,610 --> 00:10:31,150
now you can use these these compression machines of these really we can networks as

78
00:10:31,150 --> 00:10:35,050
prediction machines and therefore compression machines

79
00:10:37,280 --> 00:10:42,840
and it's very natural for such networks and to develop various sets of neurons that

80
00:10:42,840 --> 00:10:45,490
always get active when a certain type of

81
00:10:46,610 --> 00:10:52,070
spatio temporal sequences active is present and then store the memory from one

82
00:10:52,090 --> 00:10:55,880
because if you want to predict the next thing given the previous things the best

83
00:10:55,880 --> 00:11:00,570
thing you can do is to to encode the stuff that frequently occurs again and

84
00:11:00,570 --> 00:11:07,260
again by just a few things a few neurons that somehow correspond to a prototype

85
00:11:07,300 --> 00:11:11,930
a prototype of this object which is that an object is just something that frequently

86
00:11:12,280 --> 00:11:13,110
comes up

87
00:11:13,150 --> 00:11:15,260
and so

88
00:11:18,170 --> 00:11:24,320
clearly defined groups of neurons or little complex things that stand for

89
00:11:24,410 --> 00:11:31,650
absolutely that stand for complex input sequences and compact call them i just a natural

90
00:11:31,650 --> 00:11:37,380
byproduct of compression of this time we cannot work like that those trying to predict

91
00:11:37,410 --> 00:11:42,880
data sequences is compressing all the time and the way compressing is use the same

92
00:11:42,880 --> 00:11:47,910
units again and again for different but similar

93
00:11:47,950 --> 00:11:49,760
input sequence

94
00:11:49,820 --> 00:11:51,590
so the best thing to do

95
00:11:51,610 --> 00:11:54,340
to deal with environment like ours is

96
00:11:54,360 --> 00:11:58,550
just invent a couple of prototypes

97
00:11:58,590 --> 00:12:04,880
that reflect what's often be occurring and then on top of it

98
00:12:04,900 --> 00:12:08,510
just after tx is that you need to to describe the

99
00:12:08,700 --> 00:12:11,970
special instances and that's the way you can save a lot of

100
00:12:13,900 --> 00:12:19,650
and i never understood this difference between sub symbolic and symbolically

101
00:12:19,650 --> 00:12:24,150
obviously you get stuff that is very much like symbolic computation just as a product

102
00:12:24,150 --> 00:12:25,400
of these

103
00:12:25,400 --> 00:12:31,470
of these training algorithms for we can facts and of course as you

104
00:12:31,490 --> 00:12:35,820
interacting with the world as the agent that is

105
00:12:37,180 --> 00:12:41,030
all kinds of events but some of them are related to the sun goes up

106
00:12:41,030 --> 00:12:44,570
every single day and you have to be three times a day and all these

107
00:12:46,320 --> 00:12:50,930
there is one thing of course that is that whenever the agent interacts with the

108
00:12:50,930 --> 00:12:54,610
one that situation itself so it's the most natural thing to have a couple of

109
00:12:54,610 --> 00:12:59,010
neurons that somehow represent this prototype of the agents of that will be something like

110
00:12:59,010 --> 00:13:05,320
itself some more people often discussing consciousness and in all these things in writing books

111
00:13:05,320 --> 00:13:09,570
about it it seems to me was just a very simple by-product of compression it's

112
00:13:09,570 --> 00:13:14,570
just sufficient to have a couple of years dedicated for modeling the things that frequently

113
00:13:14,570 --> 00:13:16,360
appear including

114
00:13:16,380 --> 00:13:20,970
yourself we can take one of these we can use networks that are modelling probability

115
00:13:20,970 --> 00:13:27,570
distributions over all future possible events and london and we can then

116
00:13:27,570 --> 00:13:29,100
and that's why the function doesn't

117
00:13:29,110 --> 00:13:34,230
doesn't exactly the data but that's actually very smooth underlying function

118
00:13:34,300 --> 00:13:35,570
and the useful thing

119
00:13:35,580 --> 00:13:37,250
about the marginal likelihood is

120
00:13:37,260 --> 00:13:40,980
it doesn't just care about how well you fit the training data

121
00:13:41,030 --> 00:13:42,820
it also cares about

122
00:13:42,840 --> 00:13:45,230
how complex models

123
00:13:45,270 --> 00:13:47,080
it has a very specific way

124
00:13:47,100 --> 00:13:48,280
of measuring

125
00:13:48,310 --> 00:13:50,360
exactly how that complexity

126
00:13:52,580 --> 00:13:54,540
and that's given by

127
00:13:54,550 --> 00:13:56,320
by this expression up here

128
00:13:56,330 --> 00:13:59,040
which came out automatically out of the framework

129
00:13:59,040 --> 00:14:04,320
that automatically this term is telling you how well fitting the data in this term

130
00:14:04,370 --> 00:14:06,620
is the complexity

131
00:14:09,440 --> 00:14:14,220
the complexity penalty is not lambda times the complexity the way you have to set

132
00:14:15,370 --> 00:14:19,040
i think that this is the scale that you need to measure these things

133
00:14:19,930 --> 00:14:21,700
and there's no knobs to tweak

134
00:14:21,890 --> 00:14:26,860
so it so it's completely automatic

135
00:14:27,030 --> 00:14:29,180
so here's a picture

136
00:14:31,570 --> 00:14:33,860
o thing which known as occam's razor

137
00:14:33,870 --> 00:14:39,920
tries to illustrate how it can be that this very useful property comes comes about

138
00:14:40,000 --> 00:14:44,840
so in this plan here i have on the on this axis here have an

139
00:14:44,840 --> 00:14:48,460
abstract representation of all possible data sets

140
00:14:48,750 --> 00:14:53,610
normally you can order datasets let's say all this is of particular size

141
00:14:53,680 --> 00:14:55,390
it's no you can order

142
00:14:55,450 --> 00:14:58,960
sets of data so this is abstract in some sense

143
00:14:58,990 --> 00:15:03,690
and on this axis here you have the value of the marginal likelihood

144
00:15:03,740 --> 00:15:06,360
OK let's examine what happens

145
00:15:06,380 --> 00:15:12,750
with when we look at different models that have different types of complexity

146
00:15:12,750 --> 00:15:16,280
let's start with agreement has been was very simple models that could be that it

147
00:15:16,280 --> 00:15:18,220
so let's say the regression model

148
00:15:18,270 --> 00:15:20,120
and the green model corresponds to

149
00:15:20,230 --> 00:15:21,860
linear model

150
00:15:21,870 --> 00:15:23,100
so the linear model

151
00:15:23,120 --> 00:15:24,650
can only fit

152
00:15:25,330 --> 00:15:32,380
a certain subset of the data the data that agrees roughly with and what that

153
00:15:32,380 --> 00:15:36,870
means that it only takes significantly nonzero values on a very limited

154
00:15:36,980 --> 00:15:40,130
a number of the dataset it's only for these data sets

155
00:15:40,270 --> 00:15:42,770
it models things well

156
00:15:43,690 --> 00:15:47,550
i've also drawn to a more complex model here

157
00:15:47,580 --> 00:15:49,080
so this this model here

158
00:15:49,100 --> 00:15:51,050
four four large number of

159
00:15:51,070 --> 00:15:55,550
datasets that states polynomials of degree of two hundred or something like that

160
00:15:55,650 --> 00:15:57,580
can really you know model

161
00:15:57,590 --> 00:16:01,010
almost any dataset you could think of

162
00:16:01,020 --> 00:16:05,820
so so so that has significantly nonzero value for lots and lots of data sets

163
00:16:06,880 --> 00:16:10,430
because these things are probability distributions

164
00:16:10,440 --> 00:16:12,390
they have to normalize to one

165
00:16:13,100 --> 00:16:14,300
that means that

166
00:16:14,950 --> 00:16:18,000
the value of the of the

167
00:16:18,050 --> 00:16:22,240
the marginal likelihood can get much larger for the very simple model

168
00:16:22,240 --> 00:16:24,370
so if you have a data set

169
00:16:24,400 --> 00:16:29,450
that that is actually well modeled by by simple model the marginal likelihood for model

170
00:16:29,450 --> 00:16:32,680
with a much higher than for the very complex

171
00:16:32,720 --> 00:16:34,780
so what happens in this

172
00:16:34,800 --> 00:16:39,490
in this case selection scenarios that now you get a particular observations

173
00:16:39,590 --> 00:16:41,100
this particular observation

174
00:16:41,150 --> 00:16:45,570
doesn't fit very well into this is very simple model the marginal likelihood for model

175
00:16:45,630 --> 00:16:50,320
this is low that corresponds to the rector they

176
00:16:50,340 --> 00:16:52,280
the two simple model

177
00:16:52,310 --> 00:16:53,210
in the other

178
00:16:53,210 --> 00:16:54,500
now the

179
00:16:54,510 --> 00:16:58,070
these colors don't don't match is this is this very simple model

180
00:16:59,350 --> 00:17:03,090
this does this very simple ball doesn't fit very well

181
00:17:03,620 --> 00:17:10,560
but the very complex models correspond to red curve almost exactly reproduce the training data

182
00:17:11,360 --> 00:17:13,470
it is also not very likely

183
00:17:14,690 --> 00:17:16,740
a function that flutter is that much

184
00:17:16,770 --> 00:17:21,120
could have explained almost anything so we shouldn't be too surprised that also explains our

185
00:17:21,290 --> 00:17:23,800
twenty data points in that case

186
00:17:23,860 --> 00:17:27,970
and the exactly right model is the one which is just complex enough

187
00:17:27,970 --> 00:17:31,590
to capture the data like

188
00:17:31,640 --> 00:17:34,720
this but perhaps the most important slide i will talk about you can forget about

189
00:17:34,720 --> 00:17:36,800
casting process if you understand

190
00:17:43,290 --> 00:17:46,830
that's right

191
00:17:47,060 --> 00:17:52,950
so the question the question is you know why is the complexity

192
00:17:53,000 --> 00:17:55,860
penalty the determinant of this matrix

193
00:17:56,100 --> 00:18:02,480
so now the the process remember is a distribution over functions right in and if

194
00:18:02,480 --> 00:18:05,510
you look at the the value of those functions

195
00:18:05,560 --> 00:18:08,050
at those particular locations where we have data

196
00:18:08,060 --> 00:18:10,400
then you get this covariance matrix y

197
00:18:10,480 --> 00:18:13,780
it's hard to think about because it lives in this very high dimensional space

198
00:18:13,830 --> 00:18:15,720
but basically you can think of this

199
00:18:15,760 --> 00:18:16,330
you know

200
00:18:17,670 --> 00:18:21,000
that's how this listeners very high dimensional space right

201
00:18:21,080 --> 00:18:23,670
and what it says here is that the penalty

202
00:18:23,670 --> 00:18:25,110
depends on

203
00:18:25,170 --> 00:18:30,450
on how big that ellipses like this is actually the volume of the lips i

204
00:18:31,540 --> 00:18:33,210
so the determinant here

205
00:18:33,270 --> 00:18:37,110
is the product of the ideas values and i can values tells you the variance

206
00:18:37,110 --> 00:18:38,580
in the different directions

207
00:18:39,630 --> 00:18:42,270
so this is unlikely the hyper volume

208
00:18:42,270 --> 00:18:44,170
of this thing right as a half

209
00:18:44,250 --> 00:18:46,240
because these variances

210
00:18:46,250 --> 00:18:50,240
and you the volume of the thing is the this is the length

211
00:18:50,250 --> 00:18:55,140
the product of the lengths writing it into half outside the lover so it's exactly

212
00:18:55,140 --> 00:18:56,310
the volume

213
00:18:56,330 --> 00:18:58,750
of the right and that tells you

214
00:18:58,750 --> 00:19:00,440
how many different functions

215
00:19:00,450 --> 00:19:02,030
where possible

216
00:19:02,630 --> 00:19:06,710
and the more quickly the functions are the more possible functions there and this is

217
00:19:06,710 --> 00:19:09,580
exactly the way you value

218
00:19:09,590 --> 00:19:15,540
so it's also it's it's also completely analogous to the to the entropy of the

219
00:19:17,180 --> 00:19:21,020
measuring the same thing

220
00:19:21,090 --> 00:19:23,010
OK so

221
00:19:24,010 --> 00:19:27,980
so it seems that there is something that something extremely nice about this this quantity

222
00:19:27,980 --> 00:19:31,590
that sort of popped out of the of the framework

223
00:19:31,610 --> 00:19:34,930
we might be a little bit skeptical about whether this is

224
00:19:34,980 --> 00:19:37,680
but this is exactly right but it seems a little bit too good to be

225
00:19:38,700 --> 00:19:40,070
so maybe i could

226
00:19:40,090 --> 00:19:41,850
this way you by giving

227
00:19:41,860 --> 00:19:43,520
you are much simpler example

228
00:19:43,520 --> 00:19:46,280
where you would be able to say if that's exactly right

229
00:19:46,330 --> 00:19:52,740
so i tried to do that

230
00:19:52,740 --> 00:19:56,050
so here's an example is a different kind of question

231
00:19:56,110 --> 00:19:58,570
which will shed some light on that a lot of

232
00:19:58,620 --> 00:20:00,540
so let's say i give you a task

233
00:20:00,550 --> 00:20:02,310
i give you a bunch of observations

234
00:20:02,320 --> 00:20:07,510
and i say these observations come from a from a unit dimensional gaussians

235
00:20:07,540 --> 00:20:09,400
which has a mean of zero

236
00:20:09,510 --> 00:20:10,770
and you should tell me

237
00:20:10,780 --> 00:20:13,090
what is the variance

238
00:20:13,110 --> 00:20:16,230
OK so i give you a bunch of hundred points points

239
00:20:16,270 --> 00:20:17,730
and you should tell me

240
00:20:17,900 --> 00:20:20,500
and i tell you they come from gauss drawn at random from gauss and we

241
00:20:20,500 --> 00:20:21,570
have zero mean

242
00:20:21,600 --> 00:20:23,240
you know what is the variance

243
00:20:24,110 --> 00:20:28,330
but probably pretty good idea about how to go about this let's just try to

244
00:20:28,330 --> 00:20:31,150
try to make a pictorial representation of or you could try to do the other

245
00:20:31,150 --> 00:20:33,150
points that were run

246
00:20:33,200 --> 00:20:37,150
so let's try to entertain three different crosses one is that they came from the

247
00:20:37,170 --> 00:20:39,780
gaussians with a narrow variance

248
00:20:39,800 --> 00:20:44,200
OK one is that they came from gauss and with somewhat larger variance and and

249
00:20:44,230 --> 00:20:47,480
now houses the camp came gas and from a very large

250
00:20:47,480 --> 00:20:51,020
well scoring technique doesn't give you an intuition whatsoever

251
00:20:51,170 --> 00:20:54,150
what do we mean exactly by this

252
00:20:54,250 --> 00:20:55,610
from these data

253
00:20:55,610 --> 00:20:57,070
we were learning

254
00:20:57,090 --> 00:20:58,610
y and x

255
00:20:58,630 --> 00:21:00,790
y one given x one x y y

256
00:21:00,880 --> 00:21:02,880
one given x equal to

257
00:21:02,880 --> 00:21:04,380
so when x is one

258
00:21:04,400 --> 00:21:07,360
although here when x is one

259
00:21:07,380 --> 00:21:10,110
why is one half the time when x is to

260
00:21:10,130 --> 00:21:12,520
why is one half the time

261
00:21:12,520 --> 00:21:16,420
i only show a data item five a lot more daylight that

262
00:21:16,480 --> 00:21:21,610
i will learn that another thing that x and y are independent

263
00:21:22,820 --> 00:21:24,480
the first thing you do is

264
00:21:24,570 --> 00:21:25,520
you take

265
00:21:25,540 --> 00:21:29,980
all the data and learn your conditional independencies

266
00:21:30,090 --> 00:21:35,360
and then from the conditional independencies you learn the causal DAG

267
00:21:35,380 --> 00:21:38,170
it's going press wrong thing

268
00:21:38,250 --> 00:21:41,900
how much data do we need

269
00:21:41,920 --> 00:21:46,400
now in one thousand assume that we know the independence it's right men but

270
00:21:46,400 --> 00:21:48,880
the map is only as good as

271
00:21:48,900 --> 00:21:53,290
if the dependencies how much data do we need to reliably learn anything

272
00:21:53,290 --> 00:21:55,840
i'll come back to this right

273
00:21:55,860 --> 00:22:02,090
but then again assume for now that we learn the conditional independencies for certain

274
00:22:02,150 --> 00:22:06,110
so can an example here

275
00:22:06,150 --> 00:22:07,880
i suppose

276
00:22:07,880 --> 00:22:09,480
vehicles x y

277
00:22:09,500 --> 00:22:12,170
these are some of the observed variables x and y

278
00:22:12,210 --> 00:22:15,190
and we learned are independent

279
00:22:16,380 --> 00:22:18,340
we're starting out with this knowledge

280
00:22:18,360 --> 00:22:22,500
production wire independent

281
00:22:22,560 --> 00:22:26,070
we cannot have the causal DAG in a and b we could attempt making the

282
00:22:26,070 --> 00:22:32,670
causal faithfulness assumption x could not cause a widening like can not convex

283
00:22:32,730 --> 00:22:39,690
because if i applied the markov condition to this DAG i don't get that independence

284
00:22:39,730 --> 00:22:43,380
i would like to get the wise independent of x and this then y is

285
00:22:43,380 --> 00:22:47,150
independent of its non descendants conditional on x

286
00:22:47,190 --> 00:22:52,790
so his day doesn't say y and x are independent neither does this one

287
00:22:52,820 --> 00:22:53,840
right so

288
00:22:53,860 --> 00:22:56,300
there's an independency that is not

289
00:22:56,360 --> 00:23:00,500
shown that that was which screws up faithfulness

290
00:23:00,560 --> 00:23:02,590
so we must have the causal DAG

291
00:23:02,610 --> 00:23:04,170
and c

292
00:23:04,230 --> 00:23:06,340
that's kind of a no-brainer

293
00:23:06,360 --> 00:23:09,690
you have to the rebels and you find independent news and they don't have an

294
00:23:09,690 --> 00:23:10,980
idea what causes it

295
00:23:11,880 --> 00:23:14,190
you have come to this talk figure that out it

296
00:23:16,150 --> 00:23:18,060
so now the simplest example

297
00:23:19,460 --> 00:23:20,840
i suppose

298
00:23:20,860 --> 00:23:23,400
this is a set of conditional independencies

299
00:23:25,250 --> 00:23:28,880
we have no independencies in other words the dependent

300
00:23:28,940 --> 00:23:33,060
so when i say this set of conditional independencies is the empty set

301
00:23:33,110 --> 00:23:37,150
i mean there are dependent

302
00:23:37,210 --> 00:23:40,400
we cannot have the causal DAG in a

303
00:23:40,460 --> 00:23:43,900
because if i applying the markov condition to that

304
00:23:43,960 --> 00:23:46,650
its axis is independent of y

305
00:23:47,710 --> 00:23:50,520
and that the dependency is not observed

306
00:23:50,570 --> 00:23:53,090
so this with the causal DAG

307
00:23:54,380 --> 00:23:59,920
causal markov assumption will not hold and we're assuming it does hold

308
00:23:59,980 --> 00:24:03,500
so we must have the causal DAG in b or in c

309
00:24:03,520 --> 00:24:06,520
x causes why why cause effect

310
00:24:06,570 --> 00:24:10,570
now you may ask why why could we have a hidden common cause because before

311
00:24:10,570 --> 00:24:14,480
in common cause with we correlate them with another right now i'm assuming that you

312
00:24:14,480 --> 00:24:16,750
don't have common causes

313
00:24:16,800 --> 00:24:18,790
so they make that assumption

314
00:24:18,800 --> 00:24:20,630
which i can make you want to write

315
00:24:20,630 --> 00:24:25,610
but of course i can conclude that there is a hidden common cause to assume

316
00:24:25,610 --> 00:24:27,960
there is no such thing

317
00:24:28,130 --> 00:24:31,920
so another if you want to have to be able to find the neck

318
00:24:31,960 --> 00:24:33,400
that there are

319
00:24:33,460 --> 00:24:38,270
correlated one causes the other you know which one

320
00:24:38,290 --> 00:24:42,320
i suppose we have three variables now nonuniform we get things that are little more

321
00:24:43,520 --> 00:24:46,770
and we know that x and y are independent

322
00:24:46,820 --> 00:24:53,060
so the only independent sees the x and y are independent

323
00:24:53,070 --> 00:24:56,610
there can be no edge between x and y

324
00:24:56,630 --> 00:25:00,070
owing to the reason i just gave an example one of the edge from x

325
00:25:00,070 --> 00:25:01,250
and y

326
00:25:01,270 --> 00:25:04,690
the causal markov assumption we're not say the independent

327
00:25:04,730 --> 00:25:06,250
right because

328
00:25:07,340 --> 00:25:12,190
OK then example one so there can be an edge

329
00:25:12,230 --> 00:25:15,590
there must be an edge between x and c and y and z owing to

330
00:25:15,590 --> 00:25:17,960
the reason given an example two

331
00:25:18,000 --> 00:25:21,730
once is no edge between x and y there's is no edge between x and

332
00:25:23,190 --> 00:25:26,570
and the markov assumption with x the

333
00:25:26,610 --> 00:25:28,250
are independent

334
00:25:28,270 --> 00:25:31,880
and that's not one of our observed independencies

335
00:25:31,940 --> 00:25:35,360
so the markov essentially be violated

336
00:25:35,360 --> 00:25:36,640
and for girls process you can

337
00:25:37,250 --> 00:25:38,200
that looks like this

338
00:25:40,380 --> 00:25:42,390
if i have a picture here which is not

339
00:25:43,130 --> 00:25:44,530
sorry this is not a visible

340
00:25:44,950 --> 00:25:45,480
as i

341
00:25:46,650 --> 00:25:47,380
yeah it should be

342
00:25:48,390 --> 00:25:52,030
i hate it when people say that in talks but sorry just happened to be so

343
00:25:52,470 --> 00:25:52,900
this is

344
00:25:53,630 --> 00:25:57,270
i've taken this picture from from the pitch you see this option and williamson who is

345
00:25:57,670 --> 00:25:59,830
was the inspiration student and what you see here is

346
00:26:00,890 --> 00:26:02,160
here we have a gaussianprocess

347
00:26:03,540 --> 00:26:05,310
this is a function drawn from a gaussian process

348
00:26:05,800 --> 00:26:08,010
this function has the larger length scales

349
00:26:08,730 --> 00:26:12,780
reproduce function by sampling from a gaussian process which has a large bandwidth parameter the

350
00:26:12,780 --> 00:26:17,650
kernel parameterized have large bandwidth the functions that we draw from it oscillate slow

351
00:26:19,760 --> 00:26:24,400
and this function has zero right so it it it oscillates slowly

352
00:26:26,160 --> 00:26:27,880
they axis as its me

353
00:26:30,740 --> 00:26:31,520
right so that would be

354
00:26:31,910 --> 00:26:33,570
the function we draw from this model here

355
00:26:34,430 --> 00:26:37,280
and now we plug this function into another girls process

356
00:26:39,070 --> 00:26:40,210
and use that as the mean

357
00:26:40,700 --> 00:26:42,170
the function of the girls in process

358
00:26:42,930 --> 00:26:46,730
and we give the girls in process smaller length scales something that we sample from

359
00:26:46,730 --> 00:26:49,860
the output will oscillate at a at a higher rate

360
00:26:50,670 --> 00:26:54,420
but what will oscillate around not exist anymore but rather around this function

361
00:26:56,170 --> 00:27:01,200
so we get something that looks like these these small weekly functions here you these

362
00:27:01,630 --> 00:27:02,710
the red and green functions

363
00:27:03,380 --> 00:27:04,220
so that way we can

364
00:27:04,640 --> 00:27:07,480
we can model the problem where we have to timescales

365
00:27:08,350 --> 00:27:08,910
something that

366
00:27:09,610 --> 00:27:12,090
something that oscillates on two timescales ones

367
00:27:13,890 --> 00:27:17,120
end and then larger time scale and we can split these up

368
00:27:23,490 --> 00:27:24,090
now the um

369
00:27:27,220 --> 00:27:28,160
the most widely used

370
00:27:28,720 --> 00:27:32,490
hierarchical model and bayesian nonparametrics i suppose the hierarchical dirichlet process

371
00:27:35,940 --> 00:27:39,510
the model works like this so we we remember when we sample from a dirichlet

372
00:27:39,510 --> 00:27:43,450
process you this this was the formulation of the order parameter iterative process we saw

373
00:27:44,410 --> 00:27:45,480
we have this parameter alpha

374
00:27:46,130 --> 00:27:47,930
it was that the parameter which control

375
00:27:49,340 --> 00:27:51,120
how probable it is to produce a new cluster

376
00:27:52,200 --> 00:27:52,940
and then we have this

377
00:27:53,630 --> 00:27:58,470
parameter gene knock here which is the distribution of these other atom locations right of the cluster parameters

378
00:28:02,770 --> 00:28:03,530
we now sample

379
00:28:04,970 --> 00:28:06,570
the idea is now that we sample that

380
00:28:07,650 --> 00:28:10,440
the measure which controls the location of atom locations

381
00:28:11,950 --> 00:28:13,130
from indirect process

382
00:28:13,800 --> 00:28:15,630
can usually this is a parameter that we choose

383
00:28:16,590 --> 00:28:19,230
but now is we we do this hierarchical you we

384
00:28:19,920 --> 00:28:22,450
sample this measure itself from a dirichlet process

385
00:28:23,760 --> 00:28:25,230
and i've drawn over here so you see

386
00:28:26,450 --> 00:28:30,260
but in this picture here this is a sample from all represents a sample from

387
00:28:30,260 --> 00:28:32,220
a dirichlet process are all a part of the sample

388
00:28:32,920 --> 00:28:34,420
namely finally many clusters

389
00:28:35,040 --> 00:28:36,350
finally many atoms

390
00:28:37,530 --> 00:28:39,750
the locations here on the on the interval

391
00:28:40,430 --> 00:28:44,420
these are the locations of of the atoms what i call theta yesterday right so

392
00:28:44,420 --> 00:28:46,920
in this discrete random measures the location of the direct spikes

393
00:28:47,530 --> 00:28:47,980
and the height

394
00:28:48,650 --> 00:28:49,060
of these

395
00:28:49,450 --> 00:28:50,480
of these bars here

396
00:28:51,370 --> 00:28:52,030
are the weights

397
00:28:52,630 --> 00:28:55,320
which correspond to the cluster sizes are the cluster probabilities

398
00:28:58,220 --> 00:28:58,610
these are

399
00:28:59,080 --> 00:29:04,420
locations of cluster parameters all values of cluster parameters and probabilities of the corresponding clusters

400
00:29:08,000 --> 00:29:10,450
all right now we we take this discrete measure here

401
00:29:11,080 --> 00:29:14,080
we plug it into a dirichlet process and we sample from there

402
00:29:15,320 --> 00:29:18,450
what's create happen if we do that each time we produce a new cluster

403
00:29:19,530 --> 00:29:20,360
be drawn from

404
00:29:20,830 --> 00:29:25,130
the measuring given here and that is itself a discrete metric so we will always

405
00:29:25,300 --> 00:29:27,060
when we produce a new class that we will always

406
00:29:27,830 --> 00:29:30,940
produce a point that we have already seen in this matter

407
00:29:32,110 --> 00:29:36,210
we have restricted the possible about the values of cluster parameters to these values

408
00:29:37,910 --> 00:29:38,950
and so you see that

409
00:29:39,440 --> 00:29:41,100
these are two different samples from there

410
00:29:41,770 --> 00:29:44,620
and you see that the locations you exactly the same as up here

411
00:29:46,850 --> 00:29:49,830
but what's the point that nothing changes was something changes and those are

412
00:29:50,390 --> 00:29:50,870
the weights

413
00:29:52,430 --> 00:29:55,830
because we we sample from this but we sample from the random

414
00:29:56,240 --> 00:29:59,160
and these are not these are not the number of points that we actually see

415
00:29:59,160 --> 00:30:03,510
these are the probabilities right so sample from that you get different we different weights

416
00:30:08,620 --> 00:30:10,470
why would you why would you want to do something like that

417
00:30:12,300 --> 00:30:13,680
and application of this is

418
00:30:15,180 --> 00:30:15,970
a nonparametric

419
00:30:16,640 --> 00:30:18,340
version of latent dirichlet allocation

420
00:30:19,070 --> 00:30:22,150
and which is which is a topic model and i don't expect everybody you know

421
00:30:22,150 --> 00:30:25,770
what they can allocation is i suppose lot people probably do but

422
00:30:26,410 --> 00:30:27,950
basically what you wanted to hear is

423
00:30:29,710 --> 00:30:30,550
in topic modeling

424
00:30:31,100 --> 00:30:32,100
have that i guess that

425
00:30:34,050 --> 00:30:35,740
the basic idea of topic modeling is

426
00:30:36,310 --> 00:30:37,550
that if you want to

427
00:30:38,960 --> 00:30:39,270
if you do

428
00:30:40,350 --> 00:30:43,010
natural language processing you want to represent a topic

429
00:30:43,680 --> 00:30:46,780
good way to do that is has a distribution over words

430
00:30:47,610 --> 00:30:51,630
so that's the basic assumption of topic modeling topics are distributions over words

431
00:30:53,160 --> 00:30:53,740
that means u

432
00:30:54,600 --> 00:30:57,010
when you analyse a big corpus of text data

433
00:30:57,540 --> 00:30:59,890
you basically extract all the words that occur in the data

434
00:31:00,550 --> 00:31:02,330
and then you record how many times

435
00:31:02,850 --> 00:31:07,540
does this but doesn't each word occur and that gives you a probability estimate of a probability distribution

436
00:31:08,100 --> 00:31:09,130
and the idea is now that

437
00:31:09,720 --> 00:31:11,860
remember yesterday we talked about this power laws

438
00:31:12,370 --> 00:31:15,560
he said well in in language there are some words that occur all the time

439
00:31:16,110 --> 00:31:18,040
and then there are some words which occur only rarely

440
00:31:19,630 --> 00:31:24,230
and many of these terms are specialized now a few if you focus on a specific topic

441
00:31:24,730 --> 00:31:29,420
right if you have some biological terms it overall in the language occur rarely but

442
00:31:29,480 --> 00:31:32,020
in the in the context of biology it may be more frequent

443
00:31:33,260 --> 00:31:36,970
and the idea is that you can use these frequencies to characterize

444
00:31:37,920 --> 00:31:39,640
is a complete characterization of the talk

445
00:31:41,040 --> 00:31:42,100
so the topic

446
00:31:42,960 --> 00:31:47,800
in technical terms what does it mean the topic is a vector that represents a probability

447
00:31:48,920 --> 00:31:52,640
it's a probability over all right so it's it's a very large very long vector

448
00:31:52,650 --> 00:31:54,660
each entry corresponds to one possible route

449
00:31:55,810 --> 00:31:56,520
but it is finite

450
00:31:57,370 --> 00:31:57,950
finite length

451
00:31:58,540 --> 00:32:00,020
and the entries are numbers

452
00:32:00,650 --> 00:32:02,580
nonnegative numbers with which sum up to one

453
00:32:03,190 --> 00:32:06,640
so it's a probability distribution over a finite number of events and these are the words

454
00:32:09,480 --> 00:32:10,770
and then when you sample from that

455
00:32:12,240 --> 00:32:13,590
use that has been

456
00:32:13,900 --> 00:32:17,070
the parameter vector of multinomial distribution and sample from the nineteen

457
00:32:21,180 --> 00:32:21,430
and now

458
00:32:22,330 --> 00:32:24,270
one way that you can that you can

459
00:32:27,110 --> 00:32:29,330
so in topic modeling if you want to represent

460
00:32:31,260 --> 00:32:34,230
you represent then in terms of mixture models and that's

461
00:32:34,960 --> 00:32:36,310
that was an idea that started

462
00:32:37,280 --> 00:32:39,300
in the late nineteen nineties with the moral code

463
00:32:39,880 --> 00:32:41,840
probabilistic latent semantic analysis

464
00:32:42,370 --> 00:32:45,690
and that's very simply it's it's a mixture model of multinomials

465
00:32:47,130 --> 00:32:48,080
and the way it works is

466
00:32:48,620 --> 00:32:49,440
we first sample

467
00:32:51,370 --> 00:32:53,270
the first sample the index of a topic

468
00:32:54,070 --> 00:32:55,620
the topic is characterized

469
00:32:57,850 --> 00:32:59,270
probability distribution over words

470
00:33:00,040 --> 00:33:02,810
and then you sample the document from the probability distribution

471
00:33:04,650 --> 00:33:06,670
so that's a mixture model representing documents

472
00:33:08,280 --> 00:33:12,460
this more latent dirichlet allocation was introduced with based on the idea that there

473
00:33:14,080 --> 00:33:17,840
using a simple mixture model is very restrictive because what is the assumption in their

474
00:33:18,120 --> 00:33:19,790
this assumption that mixture model is

475
00:33:21,410 --> 00:33:23,860
every data point belongs to one cluster right

476
00:33:23,860 --> 00:33:26,330
people control the actions they can see

477
00:33:26,340 --> 00:33:27,420
what you added

478
00:33:27,450 --> 00:33:31,240
look at what i wanted to look at home in back

479
00:33:31,250 --> 00:33:33,980
so if i showed confidence and at

480
00:33:34,000 --> 00:33:37,910
you simply choose to ignore the focus on the one hand i'm sure

481
00:33:37,910 --> 00:33:39,110
many of us have some

482
00:33:39,120 --> 00:33:42,250
training and we just ignore it

483
00:33:42,280 --> 00:33:43,310
now this is

484
00:33:43,330 --> 00:33:48,390
almost instinctively when you look at the search results page i just social problems

485
00:33:48,400 --> 00:33:50,430
and you know as sure

486
00:33:50,450 --> 00:33:52,240
so in this sort of thing

487
00:33:52,270 --> 00:33:53,120
even though

488
00:33:53,150 --> 00:33:57,830
that has been shown to the users not making an impression users users are not

489
00:33:57,830 --> 00:33:59,730
seen that

490
00:33:59,740 --> 00:34:01,930
so in this sort of thing

491
00:34:02,800 --> 00:34:05,060
what think well justified

492
00:34:05,090 --> 00:34:06,790
will be at on

493
00:34:06,800 --> 00:34:08,390
well what users actually

494
00:34:08,400 --> 00:34:09,800
c had

495
00:34:09,810 --> 00:34:10,970
and look at me

496
00:34:11,150 --> 00:34:15,310
and understanding of get trapped products

497
00:34:15,340 --> 00:34:18,410
following online advertising work when users

498
00:34:18,420 --> 00:34:20,350
do not in fact

499
00:34:20,360 --> 00:34:22,080
have had control

500
00:34:23,830 --> 00:34:26,490
and the answer to this street before

501
00:34:26,520 --> 00:34:28,160
the first part of the answer

502
00:34:28,170 --> 00:34:31,550
is the massive scale at which online advertising history

503
00:34:31,600 --> 00:34:32,810
it's essentially

504
00:34:33,060 --> 00:34:36,650
percent to serve as the millions of users

505
00:34:36,730 --> 00:34:40,900
thousands of years but as having billions of such transactions

506
00:34:40,960 --> 00:34:45,910
between users as over the over the course of a month

507
00:34:45,920 --> 00:34:46,960
it's essentially

508
00:34:47,000 --> 00:34:53,000
destroying as many many times many many different users in the hope that something somewhere

509
00:34:53,080 --> 00:34:55,390
some users are going to CBS

510
00:34:55,620 --> 00:34:58,560
we have a good track record from source

511
00:34:58,590 --> 00:34:59,920
so it has to work

512
00:34:59,930 --> 00:35:01,650
he is a massive schemes

513
00:35:01,680 --> 00:35:05,160
in in addition to have very low marginal cost

514
00:35:05,180 --> 00:35:08,450
so the cost of serving any one given

515
00:35:08,490 --> 00:35:10,460
one is that in one

516
00:35:10,470 --> 00:35:11,700
people setting

517
00:35:11,720 --> 00:35:13,900
in terms of one particular on

518
00:35:13,910 --> 00:35:15,340
has to very

519
00:35:16,270 --> 00:35:17,200
it has to be there

520
00:35:18,110 --> 00:35:21,520
of the two thousands of minutes against showing that

521
00:35:21,530 --> 00:35:24,930
plus you should not be way too much competition for

522
00:35:24,960 --> 00:35:28,780
because if you spend too much effort to come up with the best possible at

523
00:35:29,100 --> 00:35:33,160
for this content and the user simply chooses to ignore

524
00:35:33,160 --> 00:35:39,540
and this is you have become a very low margin cost ways of showing that

525
00:35:39,560 --> 00:35:45,170
and many many different actually many many different so massive scale and the marginal cost

526
00:35:45,300 --> 00:35:48,520
kind of like this in this but then

527
00:35:48,540 --> 00:35:52,240
this means that you cannot have human in the

528
00:35:52,270 --> 00:35:55,640
it's just too costly and too slow humans

529
00:35:55,730 --> 00:35:59,340
do any sort of data processing or even online

530
00:35:59,350 --> 00:36:01,280
thinking of of

531
00:36:01,970 --> 00:36:03,810
all of this has to be done

532
00:36:03,850 --> 00:36:05,530
automatically on the fly

533
00:36:05,550 --> 00:36:07,990
using very little computational effort

534
00:36:08,220 --> 00:36:09,980
and the key here is to one

535
00:36:10,650 --> 00:36:13,200
even a little bit even a little bit more

536
00:36:13,230 --> 00:36:18,450
if we can come up with something that just barely crossed dependencies doing this can

537
00:36:18,450 --> 00:36:20,940
translate into millions of dollars month

538
00:36:20,960 --> 00:36:24,680
consider the massive scale which this process is that

539
00:36:24,700 --> 00:36:28,560
and now there are many different ways we can think of quite one

540
00:36:28,670 --> 00:36:29,870
the state of minas

541
00:36:29,890 --> 00:36:31,610
one of we have

542
00:36:31,620 --> 00:36:33,480
is to learn from

543
00:36:33,490 --> 00:36:34,810
so typically

544
00:36:34,820 --> 00:36:42,240
any any computation everything online advertising system which generate huge historical logs so lots of

545
00:36:42,260 --> 00:36:47,520
the form that show this particular with this particular user who come from this particular

546
00:36:47,560 --> 00:36:52,470
demographic when the user was actually looking at this point on this particular page and

547
00:36:52,470 --> 00:36:53,770
so on and then

548
00:36:53,780 --> 00:36:57,640
i showed this user either clicks on the not

549
00:36:57,670 --> 00:36:59,870
billions of such transactions

550
00:36:59,890 --> 00:37:04,990
this this this is starting to very noisy signals

551
00:37:05,000 --> 00:37:06,470
it's very weak

552
00:37:06,500 --> 00:37:10,360
but if we can extract some information on this

553
00:37:10,370 --> 00:37:15,860
this this one based on the fact that we lot what is good at this

554
00:37:17,100 --> 00:37:19,260
and this will lead us

555
00:37:19,280 --> 00:37:20,900
in many games

556
00:37:20,970 --> 00:37:22,300
even look

557
00:37:22,320 --> 00:37:25,210
in terms of millions of dollars

558
00:37:25,440 --> 00:37:27,870
just more satisfied users

559
00:37:27,890 --> 00:37:33,120
and this is part of the reason why online advertising is becoming less important feasts

560
00:37:34,200 --> 00:37:39,500
slowing economy is because there are lots of interesting open problems in a

561
00:37:39,510 --> 00:37:42,270
and this has led to the birth of the new distance for

562
00:37:42,280 --> 00:37:44,600
called computational

563
00:37:44,640 --> 00:37:46,640
but computational

564
00:37:46,650 --> 00:37:49,100
it's not a completely new

565
00:37:49,850 --> 00:37:54,370
it's really more of an amalgam of these problems we have already seen so far

566
00:37:54,430 --> 00:37:58,090
just with special add constraints so for instance

567
00:37:58,100 --> 00:38:02,130
if i want to show that actually users initially and have to look at the

568
00:38:02,250 --> 00:38:05,400
end of the set of all possible subsets of

569
00:38:05,460 --> 00:38:07,350
text processing

570
00:38:07,360 --> 00:38:10,960
maybe natural language processing and the forgotten what it's about

571
00:38:10,970 --> 00:38:13,780
so the text analysis simple

572
00:38:13,810 --> 00:38:16,730
i have restored is in some sense it is

573
00:38:16,740 --> 00:38:20,890
so that can quickly retrieve the when it becomes the best for this

574
00:38:20,920 --> 00:38:23,360
for some given content in some sense

575
00:38:23,420 --> 00:38:27,680
so i to be able to do some large-scale information retrieval

576
00:38:27,760 --> 00:38:30,620
have a good model of

577
00:38:30,680 --> 00:38:34,460
what match what pages want ask

578
00:38:34,470 --> 00:38:36,710
match the use queries and so on

579
00:38:36,750 --> 00:38:40,170
so this request looking at all all sorts of historical data and coming up with

580
00:38:40,170 --> 00:38:41,720
a good more

581
00:38:41,730 --> 00:38:46,210
this is exactly what we do in statistical modeling machine learning models and so on

582
00:38:46,280 --> 00:38:49,650
again optimisation problems become more for instance

583
00:38:49,680 --> 00:38:51,690
one example would be that

584
00:38:51,710 --> 00:38:54,190
instead of showing just one i want to show

585
00:38:54,210 --> 00:38:59,080
two at the top of the words on the side so for that school

586
00:38:59,090 --> 00:39:02,610
so how to optimize for this in a set of four hours

587
00:39:02,620 --> 00:39:07,320
but these sort of according it each i want to show as diverse as possible

588
00:39:07,650 --> 00:39:12,940
but also acts that individuals good so on so how to optimize for this same

589
00:39:12,970 --> 00:39:16,600
set of facts i think is just picking up four

590
00:39:16,620 --> 00:39:19,170
in this whole severity of

591
00:39:19,170 --> 00:39:20,670
let's suppose that the result

592
00:39:20,680 --> 00:39:24,940
of the measurement of each event is some collection of numbers some vector of number

593
00:39:24,940 --> 00:39:26,430
that i'll call x

594
00:39:26,430 --> 00:39:27,050
which with

595
00:39:27,060 --> 00:39:31,410
and components but now the that the

596
00:39:31,420 --> 00:39:35,600
elements of this vector can be completely different things so x one might represent the

597
00:39:35,600 --> 00:39:40,580
number of neurons x two might represent the PT of of subjects x three might

598
00:39:40,580 --> 00:39:43,960
be missing energy and so forth i just want to say that for each event

599
00:39:43,960 --> 00:39:46,610
i measure some some vector of

600
00:39:48,300 --> 00:39:51,720
now depending on what classifiers and i'm talking about

601
00:39:51,770 --> 00:39:57,550
this vector x will follows some n dimensional joint pdf so if i'm if i

602
00:39:57,550 --> 00:39:58,720
want to somehow

603
00:39:58,770 --> 00:40:03,890
assume i'm only looking at pp goes to safety bar then x will fall some

604
00:40:03,890 --> 00:40:08,280
joint pdf if i have a proton proton goes to gluino paris you will be

605
00:40:08,280 --> 00:40:09,720
some different media

606
00:40:09,730 --> 00:40:12,150
so for each reaction

607
00:40:12,160 --> 00:40:16,140
i will get out label that some hypotheses with the hype what i mean here

608
00:40:16,140 --> 00:40:20,260
by hypothesis is something that specifies the probability law

609
00:40:20,290 --> 00:40:21,260
four x

610
00:40:21,270 --> 00:40:26,640
so usually use sort of statisticians notation f of x given h zero

611
00:40:26,680 --> 00:40:31,400
four is the null hypothesis or f of x given h one or some alternative

612
00:40:31,400 --> 00:40:36,450
hypothesis those would be the terms that's a statistician would use in particle physics usually

613
00:40:36,450 --> 00:40:37,400
we we say

614
00:40:37,430 --> 00:40:40,930
that might be the type of event i'm interested in so i call that the

615
00:40:40,930 --> 00:40:42,520
signal hypothesis

616
00:40:42,530 --> 00:40:47,560
and the other events that i'm not interested in that's the background hypothesis so sort

617
00:40:47,560 --> 00:40:53,740
of flip back and forth between using signal background and h zero h one

618
00:40:53,770 --> 00:40:59,160
the important point being that the that specifies the probability law four

619
00:40:59,170 --> 00:41:02,330
the observable

620
00:41:03,940 --> 00:41:05,530
suppose you have

621
00:41:05,540 --> 00:41:09,790
some data sample with two kinds of events corresponding to say two hypotheses h two

622
00:41:09,790 --> 00:41:10,780
and h one

623
00:41:10,820 --> 00:41:14,690
and suppose you want to select events of the type h one so you want

624
00:41:14,690 --> 00:41:18,100
to somehow test h zero and rejected so as to retain

625
00:41:18,110 --> 00:41:19,860
h one

626
00:41:20,520 --> 00:41:25,170
here just symbolically are two dimensions of the state space x i and x j

627
00:41:25,190 --> 00:41:28,510
i mean it could be many dimensions but there's just two dimensions and suppose that

628
00:41:28,570 --> 00:41:29,480
the density

629
00:41:29,490 --> 00:41:30,780
it is implied by

630
00:41:30,790 --> 00:41:32,480
the hypothesis h zero

631
00:41:32,510 --> 00:41:37,070
and that's the density that are implied by the hot hypothesis h one

632
00:41:37,120 --> 00:41:39,190
right so how do you select the events

633
00:41:39,230 --> 00:41:42,660
well what people usually start off by doing is just making some sort of cuts

634
00:41:42,660 --> 00:41:47,700
in x space so what you mean by a cut based analysis is that

635
00:41:47,710 --> 00:41:52,320
the decision boundary the the selection boundary in this space is just a set of

636
00:41:52,320 --> 00:41:58,050
rectangular cut so it's it's a rectangular or in n dimensions i suppose hyper rectangular

637
00:41:58,200 --> 00:41:59,430
decision boundary

638
00:41:59,440 --> 00:42:03,680
but you can see that's not really necessarily optimal decision boundary here certainly not the

639
00:42:04,580 --> 00:42:08,210
something similar might be a linear decision boundary you might say all right let's just

640
00:42:08,550 --> 00:42:12,450
somehow try to separate them with the liner with with the hyperplanes

641
00:42:12,510 --> 00:42:16,930
also in this case because this structure is kind of curves that wouldn't be the

642
00:42:16,930 --> 00:42:21,260
optimal does a boundary to separate them you might want to have some sort of

643
00:42:21,300 --> 00:42:23,310
a curve decision boundary

644
00:42:23,340 --> 00:42:28,270
so that's the basic problem that we have to deal with in it in

645
00:42:28,280 --> 00:42:34,890
event classification or more generally in setting up some kind of statistical test

646
00:42:34,900 --> 00:42:37,570
now that surface that decision boundary

647
00:42:37,590 --> 00:42:41,680
is some surface and in dimensional space so what kind of an equation describes the

648
00:42:41,680 --> 00:42:43,910
surface in an dimensional space

649
00:42:43,950 --> 00:42:46,280
if i have some scalar function

650
00:42:46,290 --> 00:42:48,970
of that vector x equals the constant

651
00:42:48,980 --> 00:42:53,810
that will define that defines some surface in the n dimensional space right

652
00:42:53,820 --> 00:42:58,920
in that regard whatever that function is that describes the surface in question i can

653
00:42:58,950 --> 00:43:03,390
i can use that function as a new random variable i call that year test

654
00:43:04,600 --> 00:43:07,600
so that will be some given function

655
00:43:07,660 --> 00:43:12,400
of the variables x and also a in just a few minutes some some strategies

656
00:43:12,400 --> 00:43:14,890
for how to construct such functions

657
00:43:14,930 --> 00:43:18,060
for the moment suppose that i've specified function

658
00:43:18,070 --> 00:43:21,970
well specified function of random variables then you can work out its PDF so if

659
00:43:21,970 --> 00:43:25,800
i if i know what the PDF of x is given ages

660
00:43:25,840 --> 00:43:27,570
i specify that function

661
00:43:27,650 --> 00:43:29,450
and i can work out the pdfs of

662
00:43:30,220 --> 00:43:33,070
given h zero in the p fifty given h one

663
00:43:33,160 --> 00:43:34,410
and so on

664
00:43:34,420 --> 00:43:38,080
i might wind up with some pdf that look like this is something that piqued

665
00:43:38,090 --> 00:43:39,170
over here

666
00:43:39,190 --> 00:43:41,560
so assuming the hypothesis h one

667
00:43:41,570 --> 00:43:46,110
something that's peaked over here is in the hypothesis h zero in each one

668
00:43:46,180 --> 00:43:47,640
the two different things

669
00:43:47,660 --> 00:43:51,030
so now this this decision boundary in this and dimensional space

670
00:43:51,050 --> 00:43:52,890
has now been reduced to a single

671
00:43:53,540 --> 00:43:58,560
in this one dimensional space so from the one in the one dimensional problem the

672
00:43:58,560 --> 00:44:03,700
n dimensional problem somehow maps down onto a corresponding one-dimensional problem

673
00:44:03,710 --> 00:44:08,710
and and this division of the x space or the t space into two parts

674
00:44:09,100 --> 00:44:11,400
defines a statistical test

675
00:44:11,440 --> 00:44:15,800
right so the idea is that the one half in which you would say

676
00:44:15,840 --> 00:44:21,350
except the hypothesis h one and reject the hypothesis h is zero that's what's called

677
00:44:21,350 --> 00:44:22,590
the critical region

678
00:44:22,600 --> 00:44:24,320
the test of h there

679
00:44:24,340 --> 00:44:27,800
so then you look at the data you you get your point in x space

680
00:44:27,800 --> 00:44:31,630
that corresponds to a certain point the space and it falls in the region over

681
00:44:32,320 --> 00:44:36,230
you would reject the hypothesis h zero in favor of the alternatives

682
00:44:36,430 --> 00:44:38,100
h one

683
00:44:44,800 --> 00:44:48,240
when i say that you will accept and reject different hypotheses there are certain things

684
00:44:49,510 --> 00:44:51,940
you obviously don't always get the right answer

685
00:44:51,950 --> 00:44:57,940
it can happen for example that the you reject the hypothesis h zero

686
00:44:57,990 --> 00:45:00,310
but in fact that turned out to be the true

687
00:45:00,320 --> 00:45:02,850
the hypothesis that was the real nature of the event

688
00:45:02,850 --> 00:45:08,030
this is in the plane version of our algorithm or considering the local structure

689
00:45:08,090 --> 00:45:13,480
only this is in the adjacent versions of our algorithm so the real whether do

690
00:45:13,490 --> 00:45:17,540
one or the other is used to imply that you operations have to be added

691
00:45:17,540 --> 00:45:20,250
at the end of the computation

692
00:45:20,270 --> 00:45:25,910
so consequently we noticed simon and the implied edge assignment found by our algorithm

693
00:45:25,930 --> 00:45:30,730
and need not necessarily correspond to the exact graph edit distance

694
00:45:30,740 --> 00:45:32,830
so the crucial question is now

695
00:45:32,880 --> 00:45:37,920
whether or not the accuracy of the also optimal distance that we found here remains

696
00:45:37,920 --> 00:45:44,360
sufficiently accurate for pattern recognition application this is exactly what you want to find out

697
00:45:44,360 --> 00:45:46,870
in our experiments

698
00:45:46,880 --> 00:45:49,620
so here's the experimental setup

699
00:45:49,640 --> 00:45:55,280
we use the knn classifier in conjunction with graph edit distance

700
00:45:55,330 --> 00:46:00,070
so what demographics distance or exact graph edit distance and and then we address

701
00:46:00,120 --> 00:46:02,060
classification problems

702
00:46:02,070 --> 00:46:07,630
so we think that this classification accuracy given by the knn classifiers

703
00:46:07,640 --> 00:46:12,280
is a good indicator of how good or bad our approximation of the exact graph

704
00:46:12,280 --> 00:46:13,900
edit distances

705
00:46:13,950 --> 00:46:18,410
so we had several meta parameter values that we have to

706
00:46:18,420 --> 00:46:23,740
that we have to determine entities and we use the validation set and the results

707
00:46:23,740 --> 00:46:29,490
that are reported x are applied on independent test set

708
00:46:29,500 --> 00:46:34,360
so we use three other systems to compare our new algorithm with the first of

709
00:46:34,360 --> 00:46:40,420
all of course we use an optimal tree search based on a star implementation in

710
00:46:40,420 --> 00:46:43,650
industry search and then we have also true

711
00:46:43,660 --> 00:46:47,790
suboptimal modifications of the standard tree search algorithm

712
00:46:47,830 --> 00:46:52,980
which we refer to as beamsearch and pathlength sunbeam search which we are basically doing

713
00:46:52,980 --> 00:46:59,080
is we're pruning the tree search and path claims we making reweighting of danger it

714
00:46:59,080 --> 00:47:02,200
will pass in the tree

715
00:47:02,250 --> 00:47:06,470
OK so he was the first data set is used

716
00:47:06,490 --> 00:47:08,160
this is the letter dataset

717
00:47:08,170 --> 00:47:13,880
which consists of graphs representing capital letter line drawings fifteen classes

718
00:47:13,890 --> 00:47:17,850
so to construct these graphs we first manually

719
00:47:17,870 --> 00:47:21,340
draw some prototype line drawings of each letter

720
00:47:21,360 --> 00:47:26,870
and then we apply to different distortion operators on this prototype line drawings and so

721
00:47:26,870 --> 00:47:29,620
we can obtain arbitrarily many

722
00:47:31,450 --> 00:47:37,650
for each letter in arbitrarily high distortion levels you see we have applied quite high

723
00:47:37,650 --> 00:47:43,240
distortion low so it's get it gets harder and harder to predict the real class

724
00:47:43,240 --> 00:47:50,280
and finally to convert drawings into graphs we represent lines by edges and ending point

725
00:47:50,380 --> 00:47:52,750
lines by not

726
00:47:52,760 --> 00:47:55,220
so the results that you see here are

727
00:47:55,260 --> 00:47:57,760
chief on high distortion levels

728
00:47:57,770 --> 00:48:02,080
so first of all you see that our algorithms both the plain version and also

729
00:48:02,080 --> 00:48:06,920
the adjacent version are very fast and they are much faster than the exact

730
00:48:08,480 --> 00:48:12,010
and but they are also faster than the two suboptimal

731
00:48:12,030 --> 00:48:15,030
modifications beamsearch and pathlength

732
00:48:15,320 --> 00:48:20,200
so and when we are looking at the classification accuracy we see that the classification

733
00:48:20,200 --> 00:48:22,520
of course is not negatively affected

734
00:48:22,530 --> 00:48:24,820
so it is even increased

735
00:48:24,830 --> 00:48:29,240
and it is statistically significantly better so when i first saw this we were very

736
00:48:29,240 --> 00:48:34,090
surprised and thought maybe something wrong here and but we

737
00:48:34,100 --> 00:48:38,370
i think we have found a good explanation for this phenomenon

738
00:48:38,420 --> 00:48:40,310
so in this diagram here

739
00:48:40,330 --> 00:48:45,010
you see dots and he starts represents the graph edit distance

740
00:48:45,060 --> 00:48:46,720
between two graphs

741
00:48:46,800 --> 00:48:49,050
so in the x axis you see

742
00:48:49,160 --> 00:48:53,560
the found exact edit distance between two graphs and the y axis you see the

743
00:48:53,560 --> 00:48:55,600
suboptimal distance found

744
00:48:55,980 --> 00:48:57,750
between two graphs

745
00:48:57,760 --> 00:49:00,930
so as you see we distinguish between entrance

746
00:49:01,210 --> 00:49:06,820
distance is the blue dots and these are distances found between two graphs out of

747
00:49:06,820 --> 00:49:12,700
the same class and we have interclass distance is the order of the red dots

748
00:49:12,700 --> 00:49:18,050
and these dots represent distances found between graphs out of

749
00:49:18,060 --> 00:49:20,310
different classes

750
00:49:20,330 --> 00:49:25,970
so what we see is that the blue dots the intra class distances are low

751
00:49:25,980 --> 00:49:29,810
so they are low in exact version but also the suboptimal version and what is

752
00:49:29,810 --> 00:49:35,980
very important they are not increased these distances by our suboptimal version

753
00:49:36,030 --> 00:49:40,550
but when we are looking at the intercluster distances they are quite high

754
00:49:40,570 --> 00:49:45,760
so this is all right and they are even increased by the suboptimal version

755
00:49:45,780 --> 00:49:51,430
OK so to summarise this means that the graphs in the domain of graphs i

756
00:49:51,430 --> 00:49:54,290
re-arranged in respect to each other

757
00:49:55,040 --> 00:49:59,580
and to simplify this we can say that in the

758
00:49:59,590 --> 00:50:03,170
the classes are pulled apart from each other

759
00:50:03,210 --> 00:50:07,870
and this is a welcome effect regarding the classification task so we do not claim

760
00:50:07,870 --> 00:50:11,030
that this happens on every data set but it happens here

761
00:50:11,100 --> 00:50:16,360
so choose to conclude the suboptimality of our algorithm here mainly leads to an increase

762
00:50:16,650 --> 00:50:22,950
of inter class distances while most of the intra class distances are not strongly affected

763
00:50:22,960 --> 00:50:23,730
the disease

764
00:50:23,770 --> 00:50:26,440
this is a good effect

765
00:50:26,440 --> 00:50:29,420
so we have tested our algorithm on two

766
00:50:29,470 --> 00:50:36,860
other datasets the first the image datasets which represents which consists of graphs representing images

767
00:50:36,860 --> 00:50:39,240
out of five classes

768
00:50:39,250 --> 00:50:44,930
to convert these images into graphs we first segment down into regions then we eliminate

769
00:50:44,930 --> 00:50:50,100
at hand if you ever complex cost function usually it's hard to optimize might have

770
00:50:50,100 --> 00:50:55,080
many local minimiser good recipes from all over the world how to improve that and

771
00:50:55,080 --> 00:50:59,420
in the end you are not really sure what what you should pursue an intern

772
00:50:59,430 --> 00:51:04,650
in real life in this in the spectrum of of of complicated and simple models

773
00:51:04,650 --> 00:51:14,060
and introspective problems with with the image interpretation one particular simple example is that you

774
00:51:14,060 --> 00:51:17,580
take a cost function and it's just the some

775
00:51:17,770 --> 00:51:27,540
of individual contributions from object i and this contribution this course contribution is is captured

776
00:51:27,540 --> 00:51:34,210
here by the functional described by the function psi i conditioned on x y

777
00:51:34,260 --> 00:51:42,880
that is actually sort of a cost function which which calculates the

778
00:51:42,900 --> 00:51:50,680
clustering solution or the the segmentation solutions separately for every object and then it's up

779
00:51:51,140 --> 00:51:52,110
to two

780
00:51:52,140 --> 00:51:56,510
to get the course for the complete configuration

781
00:51:58,350 --> 00:52:05,300
extensions incorporate neighborhood information in cluster assignments it's fairly clear that when you have a

782
00:52:05,300 --> 00:52:10,270
situation like this you know these spirals there are already colour coded for you so

783
00:52:10,270 --> 00:52:16,180
that you know what i mean by groups here then you want to have some

784
00:52:16,180 --> 00:52:27,250
some some more refined properties which or a sort of more sophisticated more sophisticated criterion

785
00:52:27,250 --> 00:52:32,100
to really define the group and it's no longer closing in the embedding space in

786
00:52:32,110 --> 00:52:37,830
the two d space it sort of being being in a high density area that

787
00:52:37,830 --> 00:52:43,540
high-density area is sort of a part of the spiral then you want to have

788
00:52:43,770 --> 00:52:47,580
one of these arms of the spiral denoted one of them is one of the

789
00:52:49,630 --> 00:52:55,120
here this this this example is is chosen to make clear that an object wise

790
00:52:55,120 --> 00:53:01,370
evaluation of course is not sufficient you have to have the more sophisticated cost function

791
00:53:01,980 --> 00:53:07,700
to handle a situation like this and actually we have we have proposed to model

792
00:53:08,200 --> 00:53:14,750
for these for the spiral example a couple of years ago and you see there

793
00:53:14,760 --> 00:53:20,650
are still some errors in here so so one of these green dots you actually

794
00:53:20,650 --> 00:53:23,500
do laser into here

795
00:53:23,520 --> 00:53:35,770
so one of the the green dots belongs to the spiral over there and it's

796
00:53:35,790 --> 00:53:50,680
OK OK i mean this i mean this data point here you would probably say

797
00:53:50,680 --> 00:53:52,230
this should be blue

798
00:53:52,280 --> 00:54:00,750
it's the popularity of that model that screen but don't worry about it

799
00:54:00,760 --> 00:54:06,220
so two two two two sort of set the landscape of way where we position

800
00:54:06,220 --> 00:54:12,730
the segmentation problems in the classroom problems let me distinguish between two types of sort

801
00:54:12,730 --> 00:54:21,360
of extreme two types of problems learning problem supervised and unsupervised problems in supervised problems

802
00:54:21,390 --> 00:54:26,420
you have methods which are trained on standard of labelled examples so at least on

803
00:54:27,130 --> 00:54:32,370
special cases you know what should come out you train that you have the problem

804
00:54:32,370 --> 00:54:36,090
of noise in your data and then you have to make a trade of how

805
00:54:36,090 --> 00:54:42,660
much you believe the examples in your in your training set and how much you

806
00:54:42,660 --> 00:54:46,020
want to generalize to new cases

807
00:54:46,050 --> 00:54:54,510
clustering belongs to different standing in clustering nobody tells you what the correct segments on

808
00:54:54,510 --> 00:54:59,440
it depends on your application and you have to specify sort of a cost function

809
00:54:59,730 --> 00:55:06,580
which which gives you a guide installed solution and this cost function h is is

810
00:55:06,580 --> 00:55:11,880
is the input which distinguishes between good clustering solutions and then once in a sense

811
00:55:11,880 --> 00:55:14,700
you have different cost function

812
00:55:14,710 --> 00:55:19,700
for the supervised methods in the unsupervised methods and at that level of abstraction i

813
00:55:19,700 --> 00:55:26,620
think one can compare and see what is really necessary in order to learn something

814
00:55:28,540 --> 00:55:36,820
the problem specification what is given in data clustering i said already in classification you

815
00:55:36,820 --> 00:55:47,660
partition your object space and you have these training objects from the supplies and the

816
00:55:47,660 --> 00:55:52,330
key problem is how to generalize the partition to new objects so it's fairly clear

817
00:55:52,330 --> 00:55:57,350
what you would like to achieve in in classification you would be you want to

818
00:55:57,350 --> 00:56:02,360
make correct classification of future future examples

819
00:56:02,360 --> 00:56:03,890
so in clustering

820
00:56:03,910 --> 00:56:06,240
it's not so clear how you actually

821
00:56:06,270 --> 00:56:11,950
how do you actually pursue these training these testing step here

822
00:56:12,390 --> 00:56:19,800
you've unsupervised partitions of objects or the objects based on the quality criteria and how

823
00:56:19,800 --> 00:56:22,350
often do you optimize the

824
00:56:22,390 --> 00:56:30,230
cluster criterion that is sort of the challenge she but there is also a generalization

825
00:56:30,250 --> 00:56:36,250
issue in unsupervised learning problems and i will come later to that now

826
00:56:36,280 --> 00:56:43,700
you have these notions which are clearing classification is underfitting sort of a step in

827
00:56:43,700 --> 00:56:46,320
between and is overfitting

828
00:56:46,320 --> 00:56:54,970
and when i when i alluded to these notions of of of of classification then

829
00:56:54,970 --> 00:57:01,390
what i basically meant is that on the training data the blue and the red

830
00:57:01,390 --> 00:57:07,380
dots here this complicated boundary looks fairly OK you know none of the blues on

831
00:57:07,380 --> 00:57:12,540
the wrong side and on right on the side however you you would not bet

832
00:57:12,540 --> 00:57:19,110
your personal money on this classifier i'm sure otherwise let me know after that against

833
00:57:22,290 --> 00:57:25,680
so so this is the overfitting situation

834
00:57:25,690 --> 00:57:31,600
i make too much i extract too much information from my data and i will

835
00:57:31,600 --> 00:57:36,700
be penalized on future tests of that of that classifier

836
00:57:37,220 --> 00:57:41,860
and i guess it's fair to say that this is the key problem in machine

837
00:57:41,860 --> 00:57:48,400
learning and it's sort of relates what i believe two of the of of the

838
00:57:48,400 --> 00:57:57,380
most the issues in in computer science there is the computational complexity and involved which

839
00:57:57,380 --> 00:58:03,640
has been put on the on the sort of the research landscape by people from

840
00:58:03,640 --> 00:58:08,960
computer science in particular value and and and and the called community that so that

841
00:58:08,960 --> 00:58:14,340
this computational learning theory on one hand and you have the statistical issues the statistical

842
00:58:14,340 --> 00:58:20,290
learning theory problems how much can you say on the basis of noisy data and

843
00:58:20,290 --> 00:58:26,780
how fast can you or how efficiently can you say that and the relation here

844
00:58:26,830 --> 00:58:34,150
is not very well understood so far and i think this key problem in machine

845
00:58:34,150 --> 00:58:42,080
learning in classification will come up also in applications like object recognition like segmentation but

846
00:58:42,090 --> 00:58:45,030
it comes up in a more complex way

847
00:58:45,040 --> 00:58:50,250
and we learn a lot if we start identifying what is the generalisation problem in

848
00:58:50,550 --> 00:58:53,140
in vision in the oceans

849
00:58:53,150 --> 00:58:56,420
OK now let me see

850
00:58:56,800 --> 00:59:03,000
i have to rush little bit otherwise the seventy five not be reached the structure

851
00:59:03,030 --> 00:59:06,940
is i i talked a little bit about this basic concepts and then i will

852
00:59:06,940 --> 00:59:15,400
go through a number of of of clustering approaches all segmentation approaches and

853
00:59:16,260 --> 00:59:22,400
i will start to say a little bit about optimisation much less than what i

854
00:59:22,620 --> 00:59:27,650
presented at CVPR titterrell and i would like to spend a little bit more time

855
00:59:27,710 --> 00:59:32,030
on validation of clustering solutions because i think the validation issue

856
00:59:32,030 --> 00:59:39,270
GEM this is the stick breaking construction so it's the

857
00:59:39,270 --> 00:59:46,190
distribution over probability vector where the first entry is gonna be drawn from a beta one

858
00:59:46,190 --> 00:59:51,530
alpha and then the second entry is gonna be taking the the rest of the

859
00:59:51,530 --> 00:59:56,370
probability mass and then break at some point which is drawn from a beta one alpha

860
00:59:56,370 --> 01:00:08,950
and thus repeating this infinitely often what's the form stand for Griffiths Engen and McCloskey

861
01:00:08,950 --> 01:00:15,940
basically the three authors actually they have kind of independently discovered this distribution and they

862
01:00:15,940 --> 01:00:20,550
kind of explored it and then later on I forgot who was it like eitherAldez or Pittman

863
01:00:20,550 --> 01:00:34,990
who decided to name it name this distribution after them yeah okay so in the

864
01:00:34,990 --> 01:00:42,790
last little bit I'd like to tell a bit on the somewhat different

865
01:00:42,790 --> 01:00:51,830
topic which is basically constructing random trees okay from basically coagulations and

866
01:00:51,830 --> 01:00:59,750
fragmentations okay and trees are quite useful I guess we'll see a number

867
01:00:59,750 --> 01:01:05,050
of this sort of trees in like things like hierarchical bayesian models where you have

868
01:01:05,050 --> 01:01:12,190
where the model is defined on the tree okay but in in this case

869
01:01:12,190 --> 01:01:18,940
what I mean is actually somewhat different is basically one where we'd like to learn

870
01:01:19,110 --> 01:01:25,630
the tree structure from data that typically lives on lives on the leaves of the

871
01:01:25,630 --> 01:01:33,650
tree okay so basically this is a problem of hierarchical clustering or hierarchical partitioning

872
01:01:33,650 --> 01:01:39,070
and the way we're gonna do it is by using fragmentation and coagulation and

873
01:01:39,070 --> 01:01:44,030
this view is quite nice and there is a quite is a unifying view for

874
01:01:44,110 --> 01:01:50,710
a variety of bayesian nonparametric models for random trees okay as as we'll see

875
01:01:50,710 --> 01:01:59,690
okay so let's start off by to by looking at random trees and trees

876
01:01:59,690 --> 01:02:03,730
are basically so in in this context what we would like to do is to learn

877
01:02:04,490 --> 01:02:15,410
to learn or to infer some tree from some data okay so in philogenetics you

878
01:02:15,410 --> 01:02:20,650
might be given data on the various species and you like to learn about the

879
01:02:20,650 --> 01:02:26,190
evolutionary history that relates one species to another species and we know that I

880
01:02:26,190 --> 01:02:34,670
guess if you if you I presume that everybody here accepts the idea of

881
01:02:34,670 --> 01:02:45,910
evolution if you accept evolution that to basically first approximation then the species

882
01:02:45,910 --> 01:02:53,370
that we see nowadays as you look back into the evolutionary

883
01:02:53,370 --> 01:03:00,150
history is related on this basically a philogenetic tree okay where the species

884
01:03:00,310 --> 01:03:06,130
are kinda like much into the same species as you go back in time so

885
01:03:06,130 --> 01:03:12,230
we have you know mammals out here and then much down to with reptiles and fish

886
01:03:12,230 --> 01:03:17,190
and so forth and with other plants with plants and have very

887
01:03:17,190 --> 01:03:22,510
far back in time long long time ago so this is a problem from

888
01:03:22,510 --> 01:03:30,550
biology learning trees is also important for people who for psychologists who are interested in learning

889
01:03:30,550 --> 01:03:37,170
how is it that we are able to construct hierarchies of categories of things like

890
01:03:37,170 --> 01:03:43,990
everyday objects or of words of in general of concepts okay and so here we have concepts

891
01:03:43,990 --> 01:03:49,930
which correspond to everyday objects that people might know about maybe children might know about

892
01:03:49,930 --> 01:03:55,630
and we'd like to learn sums of hierarchy or a tree that that kind of

893
01:03:55,630 --> 01:04:00,830
groups the different concepts together so here we might group that all the animals should

894
01:04:00,830 --> 01:04:07,830
be together you know duck and chicken should be much together seal and dolphin should

895
01:04:07,830 --> 01:04:14,110
be together seal and dolphin are kind of like you know sea animals here we have tiger and lion which are quite

896
01:04:14,410 --> 01:04:19,850
different from the other animals because they are dangerous animals okay or these people perceive them as

897
01:04:19,850 --> 01:04:24,850
being dangerous animals so they're not the same as cows and sheeps and pigs here

898
01:04:24,850 --> 01:04:31,790
we have things like edible vegetables fruits you know like

899
01:04:31,790 --> 01:04:40,330
various tools that people might use and and down here we have vehicles basically like

900
01:04:40,330 --> 01:04:43,490
trains and jets and cars and vans and so forth okay so we would like

901
01:04:43,490 --> 01:04:49,870
to understand how is that people can learn this sort of hierarchies in their head from

902
01:04:49,870 --> 01:04:58,190
features of this everyday objects and again we'd like to infer this tree over this objects

903
01:04:58,290 --> 01:05:05,740
there's of course been lots of ways of building these trees from

904
01:05:05,740 --> 01:05:09,850
from data okay so I guess probably one of the first things that you might learn

905
01:05:09,860 --> 01:05:15,090
in the introductory machine learning course would be things like the linkage algorithms where it's

906
01:05:15,090 --> 01:05:21,410
basically a hierarchical clustering algorithm where you start off with every object in its

907
01:05:21,410 --> 01:05:26,850
own cluster and there it iteratively find the two most similar clusters and merge

908
01:05:26,850 --> 01:05:33,790
merging them together and that merging operation basically forms a tree so those are linkage

909
01:05:33,790 --> 01:05:43,810
algorithms are I guess what are called algorithmic approaches basically somebody just tells you here

910
01:05:43,810 --> 01:05:52,690
is an algorithm for constructing a hierarchy okay so that's really no no model behind this

911
01:05:52,690 --> 01:05:54,040
and the q table

912
01:05:54,060 --> 01:05:59,560
as the one rule for each state here and has one column for each action

913
01:05:59,590 --> 01:06:04,460
we assume that there are only three actions in this particular example and quite a

914
01:06:04,460 --> 01:06:07,700
few different states here

915
01:06:09,820 --> 01:06:16,350
what happens is you

916
01:06:16,370 --> 01:06:21,760
however given state and in this particular case this guy

917
01:06:21,770 --> 01:06:22,890
it is in this

918
01:06:23,240 --> 01:06:29,840
state is standing on the ground and the distance between the two opponents is say

919
01:06:29,850 --> 01:06:32,830
three feet so that's why we're in this role here

920
01:06:32,880 --> 01:06:40,110
and here are some exemplary q values for this role and

921
01:06:43,380 --> 01:06:48,900
we choose the action with the highest state action value of course

922
01:06:50,590 --> 01:06:53,260
well that's what happens

923
01:06:53,280 --> 01:06:57,350
this guy grabs his opponent and

924
01:06:57,360 --> 01:07:03,000
and now comes the reward for their reward this is the health bar of the

925
01:07:03,000 --> 01:07:07,410
opponent and this is the health bar of our guys these the acting guide big

926
01:07:07,410 --> 01:07:10,400
guy the other guys the victims

927
01:07:10,420 --> 01:07:17,810
and you see no good i think other people's health was down quite a bit

928
01:07:17,810 --> 01:07:23,820
more so quite resistant and easier the health bar went down a bit and the

929
01:07:23,820 --> 01:07:30,150
rewards that of obviously translates to a positive reward for this guy here and so

930
01:07:30,150 --> 01:07:31,260
now we can

931
01:07:31,280 --> 01:07:33,190
observe the

932
01:07:33,220 --> 01:07:37,830
next state now the distance maybe five feet in this guy's not so we're in

933
01:07:37,830 --> 01:07:40,910
this role here now

934
01:07:44,450 --> 01:07:50,830
now we look at in the school an example we take the best state action

935
01:07:50,830 --> 01:07:56,380
value here which was the six this was the largest value of these and now

936
01:07:56,380 --> 01:08:00,520
we have everything that we need for updating you need are we know our initial

937
01:08:00,520 --> 01:08:02,760
state and q value

938
01:08:02,780 --> 01:08:07,320
we know the new state represented by this and we know the reward

939
01:08:07,340 --> 01:08:10,590
so now we can use this human learning equation

940
01:08:10,600 --> 01:08:17,950
and what essentially happens is that the observation is OK in my subsequent state if

941
01:08:17,950 --> 01:08:19,850
i choose the best action

942
01:08:21,410 --> 01:08:25,690
that that will be worth six units

943
01:08:25,710 --> 01:08:30,270
now i get an immediate reward for of ten units because i was a bit

944
01:08:30,280 --> 01:08:35,880
too around the head of my opponent into the earth and so

945
01:08:35,900 --> 01:08:42,160
they did this together gives ten plus six is sixteen so this is essentially evaluates

946
01:08:42,170 --> 01:08:43,410
to sixteen

947
01:08:43,430 --> 01:08:47,380
but this thing is starting point two so we will have an up to date

948
01:08:47,890 --> 01:08:52,010
in the positive direction for this towards the sixteen

949
01:08:52,020 --> 01:08:54,220
that's the whole idea of of

950
01:08:54,240 --> 01:08:57,160
this update scheme

951
01:08:57,170 --> 01:09:04,500
any questions about that

952
01:09:04,510 --> 01:09:06,250
well don't be afraid it's

953
01:09:06,260 --> 01:09:11,380
it's just a game so

954
01:09:11,380 --> 01:09:18,420
what we actually did was to use the different representation of the q function

955
01:09:18,480 --> 01:09:23,290
but that we first describe what kind of features we used so one thing that's

956
01:09:23,290 --> 01:09:27,870
obviously important is the features the separation between the two players

957
01:09:27,970 --> 01:09:32,550
because if far away then something different is a good move for

958
01:09:32,600 --> 01:09:35,750
then if they are close for example when you close you might want to punch

959
01:09:36,170 --> 01:09:39,190
while when you for part you may want to kick or you may want to

960
01:09:39,190 --> 01:09:40,410
move closer

961
01:09:41,280 --> 01:09:47,000
so another interesting feature is the previous action taken

962
01:09:47,000 --> 01:09:54,580
and we probably took this also to encourage the rhythm to show some continuity

963
01:09:58,470 --> 01:10:02,680
so the this is pretty much the previous action of the opponent

964
01:10:02,680 --> 01:10:07,970
so then there are different modes in which you can be you can be standing

965
01:10:07,970 --> 01:10:11,270
firmly on the ground you could be in the air because you just do the

966
01:10:11,280 --> 01:10:15,620
fancy karate kick or you could be knocked on the floor in which case you

967
01:10:15,620 --> 01:10:20,760
always has to be careful that unfair opponent may not kick you down on the

968
01:10:22,050 --> 01:10:27,440
and another very interesting thing is the proximity to obstacle because in these games you

969
01:10:27,440 --> 01:10:32,870
have certain world if you like a room training room and it is can be

970
01:10:32,870 --> 01:10:36,290
quite unpleasant to be in the corner of the room with the back to the

971
01:10:36,290 --> 01:10:43,000
corner and being punish they'd like boxing so there's definitely an interesting feature now we

972
01:10:43,000 --> 01:10:48,970
selected among different actions there nineteen there were nineteen aggressive actions like kicks and punches

973
01:10:49,580 --> 01:10:52,810
ten defensive actions like blocking or lunging

974
01:10:52,850 --> 01:10:56,820
and there were some some neutral actions were just you know moving about stuff like

975
01:10:56,820 --> 01:10:58,430
he added that

976
01:10:58,450 --> 01:11:02,670
checking that when antibiotics the condition is satisfied

977
01:11:02,780 --> 01:11:07,950
at the end of this process this process will end up with the maximum wage

978
01:11:07,960 --> 01:11:10,650
which is not necessarily max actually

979
01:11:10,700 --> 01:11:14,790
finding the maximum clique in graph is and and then you have problem

980
01:11:15,330 --> 01:11:17,580
and p hard even to approximate

981
01:11:17,630 --> 01:11:18,670
it's very very different

982
01:11:19,490 --> 01:11:25,710
but finding a maximum clique it's the simple problem actually come from

983
01:11:25,880 --> 01:11:27,860
so instead of

984
01:11:29,100 --> 01:11:34,590
i really like i was taught standard algorithms for finding dominance

985
01:11:35,190 --> 01:11:39,530
we follow quite different we transform

986
01:11:39,610 --> 01:11:46,850
the combined total problem the purely combinatorial problem of finding dominant sets

987
01:11:46,860 --> 01:11:50,200
into purely confuse optimisation

988
01:11:50,220 --> 01:11:56,700
and once we done that we can exploit the full arsenal continuous optimisation technique is

989
01:11:56,700 --> 01:12:03,210
available in a position to actually our problems will be a quadratic optimisation problem

990
01:12:03,240 --> 01:12:09,060
and there are the zillions of algorithms for solving quadratic optimisation actually use a very

991
01:12:09,060 --> 01:12:12,940
simple one is five from evolution

992
01:12:13,020 --> 01:12:14,400
OK so

993
01:12:14,450 --> 01:12:18,220
so now we are just seeing how can i transform

994
01:12:18,450 --> 01:12:22,440
the know how can i characterize the the notion of dominance in terms of continuous

995
01:12:25,590 --> 01:12:28,070
let's take an edge weighted graph g and that's

996
01:12:29,410 --> 01:12:30,910
the the

997
01:12:30,930 --> 01:12:33,680
the adjacency matrix of this graph

998
01:12:33,740 --> 01:12:37,690
so let's consider this problem the problem of finding the maximum

999
01:12:37,730 --> 01:12:40,960
of this quadratic function this is linear in

1000
01:12:40,980 --> 01:12:43,860
metrics for if you live

1001
01:12:43,900 --> 01:12:46,450
i can just say that

1002
01:12:46,460 --> 01:12:49,990
from ways to express a x

1003
01:12:50,030 --> 01:12:51,310
just so

1004
01:12:51,320 --> 01:12:52,340
four all

1005
01:12:52,360 --> 01:12:54,880
so all j

1006
01:12:55,020 --> 01:12:56,120
i j

1007
01:12:56,140 --> 01:12:57,940
x i x j

1008
01:12:58,090 --> 01:13:02,620
if you actually can also ride this in a different way

1009
01:13:02,660 --> 01:13:04,550
right this way

1010
01:13:05,550 --> 01:13:07,700
one of we are talking about with

1011
01:13:07,800 --> 01:13:09,990
i was sure this differently

1012
01:13:09,990 --> 01:13:13,110
later on we could go back to that

1013
01:13:14,840 --> 01:13:19,070
in this case it's much is symmetric canonical

1014
01:13:19,140 --> 01:13:20,720
with the zero that

1015
01:13:20,740 --> 01:13:26,160
OK so consider the problem of finding the maximum this quadratic polynomial homogeneous polynomial over

1016
01:13:26,160 --> 01:13:32,860
the standard simplex the standard simplex is a very simple geometric structure it's it's the

1017
01:13:33,950 --> 01:13:35,300
of the plane

1018
01:13:35,320 --> 01:13:37,140
of equation

1019
01:13:37,140 --> 01:13:39,950
some exciting equal to one this

1020
01:13:39,950 --> 01:13:43,390
it means someone will excite equal to one

1021
01:13:43,390 --> 01:13:47,800
so the play with that equation with the positive or of space

1022
01:13:47,800 --> 01:13:52,760
so we require that all x times greater than or equal to zero

1023
01:13:54,510 --> 01:13:59,090
this is an illustration of the stands in between and is equal to treat just

1024
01:13:59,090 --> 01:14:00,410
try and

1025
01:14:01,840 --> 01:14:03,840
consider this problem

1026
01:14:03,890 --> 01:14:06,720
and let me just point out that

1027
01:14:06,760 --> 01:14:10,180
other approaches to pairwise clustering notably

1028
01:14:10,220 --> 01:14:16,840
one hundred of years ago by and also by perronnin freeman ECCV

1029
01:14:16,860 --> 01:14:19,570
these european conference on computer vision

1030
01:14:19,660 --> 01:14:25,280
please do quite similar but fundamentally different quadratic optimisation

1031
01:14:25,390 --> 01:14:29,550
the basic idea in this approach was actually the sort of spectral class

1032
01:14:29,700 --> 01:14:30,700
these two

1033
01:14:31,470 --> 01:14:33,740
find the largest eigenvalue

1034
01:14:33,740 --> 01:14:39,090
and then the corresponding eigenvectors of the agency of the adjacency matrix a in this

1035
01:14:40,720 --> 01:14:45,470
finding the largest eigen value and the corresponding eigenvectors

1036
01:14:45,510 --> 01:14:49,820
they found by maximizing x transpose x

1037
01:14:49,840 --> 01:14:53,590
the standard simplex on the sphere

1038
01:14:53,620 --> 01:14:59,680
another thing is that this problem is computationally much simpler than that that problem if

1039
01:14:59,680 --> 01:15:02,760
of the probability function

1040
01:15:02,760 --> 01:15:07,110
and then we're going to introduce new variables v which is an velocities

1041
01:15:07,160 --> 01:15:09,700
the game to

1042
01:15:09,950 --> 01:15:11,680
the vectors that

1043
01:15:11,720 --> 01:15:15,640
like the velocity of the physical ball have affected position and then you can have

1044
01:15:15,660 --> 01:15:20,120
the velocity which is the speed in which is moving in the direction

1045
01:15:20,180 --> 01:15:21,590
and then we define

1046
01:15:21,610 --> 01:15:25,050
what we're going to do is find a distribution over both of these quantities

1047
01:15:25,070 --> 01:15:27,640
x b

1048
01:15:29,530 --> 01:15:31,950
i'm deliberately gonna skip through this two

1049
01:15:31,990 --> 01:15:34,530
quickly just to give you a flavor but i don't expect it to be able

1050
01:15:34,530 --> 01:15:38,530
to implement this and understand fully from this exposition say a

1051
01:15:38,610 --> 01:15:41,820
the joint distribution over x and b is going to be the distribution we had

1052
01:15:43,010 --> 01:15:46,910
multiplied by something related to the velocity which is going to be it's going to

1053
01:15:46,910 --> 01:15:48,860
turn out to be gas distribution

1054
01:15:48,860 --> 01:15:51,680
the joint distribution has the marginal

1055
01:15:51,680 --> 01:15:56,800
the distribution we care about and have another distribution to do with the velocity

1056
01:15:56,800 --> 01:16:02,880
then there's is a bunch of physics and actually is a bit subtle to see

1057
01:16:02,880 --> 01:16:06,700
exactly why it's valid that allows you to evolve

1058
01:16:06,720 --> 01:16:08,950
your position and your velocities

1059
01:16:08,990 --> 01:16:12,190
and the physical analogy that you want to have in your head is that you've

1060
01:16:12,190 --> 01:16:16,550
got a hilly landscape where low areas are areas of high probability both like to

1061
01:16:16,550 --> 01:16:19,180
roll downhill and go to areas of high probability

1062
01:16:19,200 --> 01:16:24,760
and you start the ball somewhere in the landscape and you allow it to roll

1063
01:16:24,760 --> 01:16:28,720
around so then it's going to want to roll down to areas of high probability

1064
01:16:28,780 --> 01:16:32,860
one symbols got up speed when it's going downhill will also go up to areas

1065
01:16:32,860 --> 01:16:36,490
of low probability again so is not an optimizer is not going to end up

1066
01:16:36,490 --> 01:16:41,860
just sticking made distribution

1067
01:16:41,880 --> 01:16:43,410
so a

1068
01:16:43,430 --> 01:16:44,620
this details

1069
01:16:44,660 --> 01:16:48,140
and i'm sorry but i want to cover all the things they

1070
01:16:48,160 --> 01:16:51,070
i just want you to i wanted to mention that even though i don't have

1071
01:16:51,070 --> 01:16:54,840
time to properly because it's very important algorithm

1072
01:16:55,430 --> 01:16:59,180
some of the biggest hardest problems so in the nineties there was a great advances

1073
01:16:59,200 --> 01:17:04,490
in bayesian neural networks which have thousands of parameters very high dimensional probability spaces with

1074
01:17:04,490 --> 01:17:10,270
lots of nasty ridges and multiple maintenance and using gradient information able to get very

1075
01:17:10,270 --> 01:17:15,880
very competitive performance of based network by using hamiltonian dynamics

1076
01:17:15,930 --> 01:17:20,240
using a gradient information for the toy picture of the sort of trajectories that you

1077
01:17:20,240 --> 01:17:25,320
might get so the pictures in lecture one doing metropolis and gibbs sampling on by

1078
01:17:25,320 --> 01:17:30,340
very gaussians and they all did very slow diffusions because they're constrained to me by

1079
01:17:30,340 --> 01:17:33,390
lengthscales the width of the gaussians

1080
01:17:33,450 --> 01:17:37,700
methods based on hamiltonian dynamics on a physical system are able to just

1081
01:17:37,740 --> 01:17:42,140
mean persistently along the distribution of made much faster progress

1082
01:17:42,180 --> 01:17:44,140
the disadvantages are that

1083
01:17:44,180 --> 01:17:47,910
you have to derive gradients you have to set various parameters to do the dynamics

1084
01:17:47,910 --> 01:17:51,240
of the system and it's more work and i know some of the people in

1085
01:17:51,240 --> 01:17:54,620
the room of use this in their research there found in a very well when

1086
01:17:54,620 --> 01:17:57,010
you get to work

1087
01:17:57,070 --> 01:18:02,120
OK so

1088
01:18:02,140 --> 01:18:05,140
this is just sort of an idea that is

1089
01:18:05,140 --> 01:18:09,280
this is something where some of the advanced research research machine and

1090
01:18:09,280 --> 01:18:12,450
markov chain methods is going to be you need to look at

1091
01:18:12,450 --> 01:18:16,240
clever ways of introducing new variables that will do different things we might make it

1092
01:18:16,240 --> 01:18:22,120
easier to complete your probability theory might make it easier to move quickly very different

1093
01:18:22,120 --> 01:18:25,470
advantages and this is where the bulk of my research has been one of the

1094
01:18:25,470 --> 01:18:28,510
i haven't i'm not going to talk about my research at all in this tutorial

1095
01:18:28,510 --> 01:18:34,720
but several of my research projects have been different ways of introducing auxiliary variables and

1096
01:18:34,720 --> 01:18:36,110
that allows you to do

1097
01:18:36,110 --> 01:18:39,340
all sorts of different things with markov chains

1098
01:18:39,360 --> 01:18:42,410
i'm going to take a break for

1099
01:18:42,470 --> 01:18:47,780
three minutes so document itself before it the question

1100
01:18:48,050 --> 01:18:51,970
that's the next section

1101
01:18:52,010 --> 01:18:53,990
that's what uncovering next after the break

1102
01:18:55,080 --> 01:19:00,010
so a quick break again out of my comfort viewing of conformal questions like you

1103
01:19:01,510 --> 01:19:05,910
OK i apologize the slightly whirlwind tour i'm really trying to give you just the

1104
01:19:05,910 --> 01:19:09,160
flavour for the things that are out there

1105
01:19:09,200 --> 01:19:12,120
the last section i'm going to spend talking about

1106
01:19:12,140 --> 01:19:14,410
one particular problem because

1107
01:19:14,430 --> 01:19:17,360
it ends up throwing up a lot of interesting issues which

1108
01:19:17,410 --> 01:19:23,340
i have a whole new area of MCMC research and the problem of finding normalising

1109
01:19:23,340 --> 01:19:25,090
constants and it's not

1110
01:19:25,090 --> 01:19:27,520
we call zero significance

1111
01:19:33,270 --> 01:19:36,480
the user has

1112
01:19:36,500 --> 01:19:38,300
so that

1113
01:19:40,170 --> 01:19:46,920
OK let's have this

1114
01:19:47,070 --> 01:19:56,560
prompted request

1115
01:19:57,570 --> 01:20:02,040
like so

1116
01:20:05,400 --> 01:20:07,210
she said she

1117
01:20:13,500 --> 01:20:15,340
or is there

1118
01:20:16,770 --> 01:20:24,210
it's exactly it's actually so the cross sections of the different physics processes

1119
01:20:24,270 --> 01:20:29,290
skate differently with centre of mass energy so for instance the higgs cross section

1120
01:20:29,290 --> 01:20:33,670
the blue blue express section increases by a factor of forty if you go from

1121
01:20:33,670 --> 01:20:35,900
tiverton to LHC

1122
01:20:35,920 --> 01:20:38,790
and the background only by a factor of ten

1123
01:20:38,840 --> 01:20:43,840
so that's for the WWD kitchen your better at the LHC is of any processes

1124
01:20:43,840 --> 01:20:49,960
that blue initiated you better at the LHC that the temperature on for processes like

1125
01:20:49,960 --> 01:20:56,210
whlch those o'clock initiated because if q q by going to w

1126
01:20:56,230 --> 01:21:00,310
and not only increases by a factor of ten this again related to the patton

1127
01:21:00,310 --> 01:21:04,500
flax for glance it increases a lot more about the lh

1128
01:21:04,520 --> 01:21:06,270
then for quarks

1129
01:21:06,290 --> 01:21:08,190
compared to the collider so

1130
01:21:08,210 --> 01:21:13,590
so only getting factor ten and the second but the background to wh production are

1131
01:21:13,590 --> 01:21:14,770
partially again

1132
01:21:14,790 --> 01:21:19,730
one could dominate anyway to the background are rising faster than the signal in that

1133
01:21:19,730 --> 01:21:22,590
case so it really depends on how

1134
01:21:22,630 --> 01:21:26,790
your signature changes with centre of mass energy and how your background changes and that

1135
01:21:26,790 --> 01:21:29,560
it is driven by the pattern flux

1136
01:21:34,680 --> 01:21:37,400
i have to question all

1137
01:21:37,460 --> 01:21:39,690
this is the

1138
01:21:39,710 --> 01:21:42,040
well it's

1139
01:21:42,060 --> 01:21:43,590
over the of

1140
01:21:51,110 --> 01:21:55,770
get your so called friend

1141
01:21:56,920 --> 01:21:57,880
that same

1142
01:21:57,900 --> 01:21:59,420
quantity occurs in like

1143
01:21:59,710 --> 01:22:02,110
bayesian approach as well yes i was doing it frequentist

1144
01:22:05,340 --> 01:22:07,400
if you want to say

1145
01:22:08,860 --> 01:22:10,590
so what

1146
01:22:12,170 --> 01:22:15,650
all these

1147
01:22:20,270 --> 01:22:21,880
she would

1148
01:22:22,440 --> 01:22:24,560
it's got to do

1149
01:22:27,860 --> 01:22:35,460
in the particular example that i think we're talking about where you're selecting events that

1150
01:22:35,460 --> 01:22:39,340
the hypothesis is the label which

1151
01:22:39,380 --> 01:22:43,340
tells you whether it was say an event of type h zero or an event

1152
01:22:43,340 --> 01:22:49,730
of type h one single event backgrounds and normally in the frequentist approach we don't

1153
01:22:49,730 --> 01:22:53,980
talk about the probability of the hypothesis you wouldn't talk about the probability that a

1154
01:22:53,980 --> 01:22:56,250
specific event is

1155
01:22:56,290 --> 01:22:59,980
signal event in the sense that you wouldn't talk about the degree of belief

1156
01:23:00,000 --> 01:23:02,130
that that is the single event

1157
01:23:02,270 --> 01:23:06,750
but notice in this case we're observing many events so this label or hypothesis is

1158
01:23:06,750 --> 01:23:12,820
in fact the outcome of repeatable observation so this is one very interesting example where

1159
01:23:12,820 --> 01:23:14,980
the same application of bayes theorem

1160
01:23:15,020 --> 01:23:16,070
makes sense

1161
01:23:16,070 --> 01:23:20,940
in both the frequentist context and in a bayesian context with only slightly subtle

1162
01:23:22,630 --> 01:23:27,000
in the interpretation of the results in one case if i say that the probability

1163
01:23:27,000 --> 01:23:29,270
that the event is a signal event is

1164
01:23:29,270 --> 01:23:32,360
such and such i mean if i look at many events

1165
01:23:32,400 --> 01:23:34,170
with similar data

1166
01:23:34,190 --> 01:23:37,380
and the fraction of those events that would be signal events would be

1167
01:23:37,570 --> 01:23:41,020
a certain amount i think it's exactly it's the same formula that i partially obscured

1168
01:23:41,040 --> 01:23:43,420
here but it's basically just based there

1169
01:23:43,980 --> 01:23:49,250
whereas bayesian would say no i have a single event in the data is fixed

1170
01:23:49,270 --> 01:23:53,730
now in my degree of belief that this event is the single event is given

1171
01:23:53,730 --> 01:23:54,820
by base there

1172
01:23:54,820 --> 01:23:58,710
but that's actually this is an example of events selection as is the case where

1173
01:23:58,710 --> 01:24:13,980
you have a perfectly frequentist application of those there

1174
01:24:14,170 --> 01:24:18,650
tomorrow tomorrow i'll talk about using bayes theorem in parameter estimation and then really the

1175
01:24:18,650 --> 01:24:21,040
distinction between the two philosophies becomes

1176
01:24:23,250 --> 01:24:25,670
a person

1177
01:24:26,730 --> 01:24:29,040
one way which

1178
01:24:31,110 --> 01:24:36,250
well i mentioned this OK so what i mentioned was the simple

1179
01:24:36,590 --> 01:24:42,230
multiplicative linear congruent shield generator and that was a very simple algorithm

1180
01:24:42,360 --> 01:24:47,860
you recall that the state of the generator was basically encoded by a single number

1181
01:24:47,860 --> 01:24:50,230
which there was just an update rule for that number

1182
01:24:50,230 --> 01:24:57,980
the periodicity the period of that sequence is basically close to roughly equal to the

1183
01:24:57,980 --> 01:24:59,960
the largest number that you can store

1184
01:24:59,960 --> 01:25:03,310
or law of of nature is a software program

1185
01:25:03,350 --> 01:25:08,710
and it valid if it if when you calculate the to the universe of the

1186
01:25:08,710 --> 01:25:11,790
experimental data at the time evolution of the physical system

1187
01:25:11,800 --> 01:25:16,190
you get exactly what you observe i mean what the reality is something the observations

1188
01:25:16,190 --> 01:25:20,840
are correct by the way i'm assuming there's no noise i'm assuming a deterministic world

1189
01:25:21,770 --> 01:25:23,800
and the thing about quantum mechanics

1190
01:25:24,630 --> 01:25:27,090
so the idea so in a way what you have here if you go back

1191
01:25:27,090 --> 01:25:33,430
to leiden it's the idea that god is the most perfect would be that here

1192
01:25:33,430 --> 01:25:36,130
you have the whole pattern evolution of the world

1193
01:25:36,210 --> 01:25:39,200
from the creation of the world until

1194
01:25:39,250 --> 01:25:42,600
now until you know the end of the world

1195
01:25:43,840 --> 01:25:46,850
the idea is that

1196
01:25:46,870 --> 01:25:50,130
that there is a simple program

1197
01:25:50,280 --> 01:25:54,700
that enables you to calculate this which would be the the beauty of the ideas

1198
01:25:54,700 --> 01:25:58,020
from the the idea the elegance of the idea that god used to create the

1199
01:25:58,020 --> 01:26:01,610
world or another way to put it would be the laws of physics

1200
01:26:01,660 --> 01:26:06,060
OK so the the assertion that the world is comprehensible understandable means that all of

1201
01:26:06,080 --> 01:26:07,210
this diversity

1202
01:26:07,260 --> 01:26:12,310
actually come from a small piece of software which would have the initial conditions plus

1203
01:26:13,530 --> 01:26:14,990
one of the laws

1204
01:26:14,990 --> 01:26:18,930
so then you you integrate to go forward from initial conditions calculating the time evolution

1205
01:26:18,930 --> 01:26:21,710
of the world in the case of the general model

1206
01:26:21,760 --> 01:26:23,350
to be more precise

1207
01:26:23,380 --> 01:26:27,970
it is i have a computer and what i explained to be a string of

1208
01:26:27,970 --> 01:26:32,350
zeroes and ones it will be a finite sequence of zeros and ones

1209
01:26:32,370 --> 01:26:39,920
and the explanation of the theory that explains these facts even if

1210
01:26:40,060 --> 01:26:46,340
the explanation will also be a string of bits

1211
01:26:46,350 --> 01:26:51,290
because you know a computer programming computer the spread of this

1212
01:26:51,340 --> 01:26:52,720
it was in one

1213
01:26:55,800 --> 01:26:57,350
let's look at this idea

1214
01:26:57,370 --> 01:27:02,550
that if a if a if the theory is our if laura theory can be

1215
01:27:02,550 --> 01:27:08,810
arbitrarily complicated than an ocean of theory becomes meaningless because there is always a lot

1216
01:27:09,610 --> 01:27:13,720
what this in this in this in this domain it's very

1217
01:27:13,760 --> 01:27:15,750
it's very clear what that means

1218
01:27:15,830 --> 01:27:17,060
because take a look

1219
01:27:17,120 --> 01:27:21,330
if you have a set of facts and you have and bits of facts

1220
01:27:21,330 --> 01:27:23,810
there is always

1221
01:27:23,860 --> 01:27:26,460
and then bit theory

1222
01:27:26,470 --> 01:27:30,620
there's always the trivial theory which says that these are the facts

1223
01:27:30,660 --> 01:27:32,280
you know it just put it just

1224
01:27:32,290 --> 01:27:36,310
so that's not about the theory that's not a valid explanation that is not a

1225
01:27:36,310 --> 01:27:39,020
law this does not have any structure

1226
01:27:39,100 --> 01:27:42,990
when does the facts have structure what is the pattern

1227
01:27:43,030 --> 01:27:46,510
it if the theory is much less than this

1228
01:27:46,530 --> 01:27:49,400
much less than this

1229
01:27:49,470 --> 01:27:53,950
much much worse than that

1230
01:27:56,160 --> 01:27:59,950
there's this idea of occam's razor right which is the simplest theory is best known

1231
01:27:59,950 --> 01:28:03,690
in this domain would say that the smallest program you measure the size of the

1232
01:28:03,690 --> 01:28:06,160
program the complexity of the program and that

1233
01:28:06,200 --> 01:28:09,840
and he would say that the most concise program the most compact program is the

1234
01:28:09,840 --> 01:28:11,500
best theory

1235
01:28:11,500 --> 01:28:13,680
that so called occam's razor

1236
01:28:13,780 --> 01:28:16,460
and i don't know how you spell that you know a lot of ways to

1237
01:28:16,460 --> 01:28:19,160
spell it

1238
01:28:19,170 --> 01:28:20,570
the cancer is

1239
01:28:22,190 --> 01:28:26,920
like it's of course knew about occam's razor but blade is saying something more crucial

1240
01:28:27,000 --> 01:28:29,110
what limits saying

1241
01:28:29,140 --> 01:28:30,090
it is

1242
01:28:30,100 --> 01:28:33,810
but there is always a lot of you know if you have and that there's

1243
01:28:33,810 --> 01:28:36,610
always a trivial theory

1244
01:28:36,660 --> 01:28:40,010
where is no compression where you just basically say this is it this is what

1245
01:28:40,010 --> 01:28:40,820
i saw

1246
01:28:40,900 --> 01:28:45,190
but that doesn't count because that's always that that's the way of saying

1247
01:28:45,240 --> 01:28:51,750
that's not a lot that's not structure that a pattern

1248
01:28:51,830 --> 01:28:54,310
you see

1249
01:28:54,350 --> 01:28:57,970
i mean something is always itself but that doesn't that doesn't but if you have

1250
01:28:57,970 --> 01:29:00,340
a much more so let me put it another way

1251
01:29:00,430 --> 01:29:02,030
another way to put this

1252
01:29:02,030 --> 01:29:04,190
and so the the

1253
01:29:04,220 --> 01:29:08,110
i'm losing you folks i can see the patient is

1254
01:29:08,210 --> 01:29:09,260
is dying

1255
01:29:09,290 --> 01:29:13,640
so let's put it another way how can we measure pattern with go back to

1256
01:29:13,640 --> 01:29:15,850
how can we measure

1257
01:29:15,900 --> 01:29:17,700
so i'm talking about

1258
01:29:17,720 --> 01:29:20,610
pattern in our toy environments

1259
01:29:20,610 --> 01:29:23,820
you folks are concerned with all kinds of patterns but i'm just going to talk

1260
01:29:23,820 --> 01:29:26,740
about the pattern in a string of bits OK i

1261
01:29:26,850 --> 01:29:28,750
a finite string

1262
01:29:29,520 --> 01:29:32,680
the question the way i measure patterns like this you have a computer and you

1263
01:29:32,680 --> 01:29:36,830
want to calculate this as the output is the result of the computation and you

1264
01:29:36,830 --> 01:29:41,240
ask what is the smallest program i don't care about time

1265
01:29:41,340 --> 01:29:46,100
i just care about the size of the program so i want the smallest program

1266
01:29:46,160 --> 01:29:49,510
the smallest program

1267
01:29:49,510 --> 01:29:53,990
that calculates that specific object that i'm interested in i what i'm

1268
01:29:55,790 --> 01:29:59,280
the idea is let's look at the size of the program OK so people right

1269
01:29:59,280 --> 01:30:02,240
that is the absolute value of p of p is the the program and i

1270
01:30:02,240 --> 01:30:05,000
compare that with the size in bits of the

1271
01:30:05,060 --> 01:30:08,560
of the bit string that i'm interested in

1272
01:30:08,680 --> 01:30:12,840
the question of whether this bit string has pattern or not how much pattern doesn't

1273
01:30:12,840 --> 01:30:17,220
have the ideas you compare these two numbers the number of bits in the most

1274
01:30:17,220 --> 01:30:21,890
concise program you compare that calculates that particular bit string you compare that with the

1275
01:30:21,890 --> 01:30:24,510
number of bits in that bit strings

1276
01:30:25,400 --> 01:30:28,790
the number of bits in the in the smallest program the carcass that the string

1277
01:30:28,790 --> 01:30:32,920
is actually much smaller than this has a lot of pattern if the number of

1278
01:30:32,920 --> 01:30:34,610
bits in a bit string is

1279
01:30:34,670 --> 01:30:36,320
essentially the same

1280
01:30:36,470 --> 01:30:38,740
the number of bits in this and there is no pattern

1281
01:30:38,780 --> 01:30:42,280
so what you the way to measure the amount of patterns

1282
01:30:42,360 --> 01:30:43,570
is you

1283
01:30:43,590 --> 01:30:47,630
subtract from the number of it in the bit string

1284
01:30:47,660 --> 01:30:51,800
you subtract the number of bits in the smaller

1285
01:30:52,610 --> 01:30:55,460
to calculate for that bit string of for that

1286
01:30:55,490 --> 01:31:00,470
a bit strange

1287
01:31:00,560 --> 01:31:05,630
and this number measured in bits the amount of pattern

1288
01:31:05,710 --> 01:31:08,620
so this number will be is essentially zero

1289
01:31:09,500 --> 01:31:13,280
the most concise program is the same size

1290
01:31:13,350 --> 01:31:14,700
but if

1291
01:31:14,740 --> 01:31:17,670
if the program is very very small

1292
01:31:17,720 --> 01:31:23,580
if the string can have in the in the in the extreme case as measured

1293
01:31:23,580 --> 01:31:25,910
in bits the amount of pattern can have

1294
01:31:25,960 --> 01:31:28,870
the minimum is zero and the maximum is the number of bits the and the

1295
01:31:28,870 --> 01:31:30,440
bit string so let me give you

1296
01:31:30,540 --> 01:31:35,330
two examples of this let's look at a bit string which has not out which

1297
01:31:35,330 --> 01:31:37,710
has maximum amount of pattern

1298
01:31:37,870 --> 01:31:40,240
a string of zero

1299
01:31:40,280 --> 01:31:42,750
you know and zeros

1300
01:31:42,770 --> 01:31:45,560
and then bit string consisting entirely of zero

1301
01:31:45,580 --> 01:31:48,280
has and bits of patterns

1302
01:31:48,340 --> 01:31:49,790
pretty much

1303
01:31:49,870 --> 01:31:52,380
because in fact

1304
01:31:52,720 --> 01:31:56,210
to be able to calculate this from my computer that i really only about log

1305
01:31:56,210 --> 01:31:58,350
two n bits telling me how many

1306
01:31:58,400 --> 01:32:00,250
how many there are

1307
01:32:00,320 --> 01:32:04,390
and if i o that's essentially very small compared with them

1308
01:32:04,500 --> 01:32:09,200
so the amount of patterns and minus log two n which is essentially an

1309
01:32:09,380 --> 01:32:13,110
which is essentially the length of the bit string so this between has a lot

1310
01:32:13,110 --> 01:32:14,860
of patterns

1311
01:32:14,900 --> 01:32:18,810
now the other extreme is you toss the coin independent also the fair coin and

1312
01:32:18,810 --> 01:32:22,530
you get something well here's the way let's look at intermediate cases

1313
01:32:22,830 --> 01:32:25,030
how about

1314
01:32:25,070 --> 01:32:27,820
how about a bit strange

1315
01:32:27,870 --> 01:32:30,940
well how about a bit string where bit is repeated

1316
01:32:31,020 --> 01:32:34,870
so you have you know you have one one zero zero every every

1317
01:32:34,950 --> 01:32:37,150
every two-bit so they have to be the same

1318
01:32:37,160 --> 01:32:39,610
but otherwise you have complete freedom

1319
01:32:39,610 --> 01:32:41,900
to immortality and it was like to get to

1320
01:32:41,990 --> 01:32:44,900
so neutrinos actually are happening all around us

1321
01:32:44,920 --> 01:32:46,930
and they are

1322
01:32:46,950 --> 01:32:49,830
less familiar than other particles because they are so

1323
01:32:50,580 --> 01:32:53,570
and reluctant to reveal themselves but it is possible

1324
01:32:55,250 --> 01:33:00,720
how we do that is something which we'll talk about the things that we know

1325
01:33:00,720 --> 01:33:02,650
about neutrinos

1326
01:33:02,780 --> 01:33:07,740
how do we know indeed neutrino is produced in beta decay well the answer is

1327
01:33:07,740 --> 01:33:10,220
that neutral state say his

1328
01:33:10,230 --> 01:33:11,370
beta decay

1329
01:33:11,390 --> 01:33:16,080
ten to proton emitting electrons going one way and the neutrino the other

1330
01:33:16,240 --> 01:33:20,820
and then upstream the target waiting and

1331
01:33:20,850 --> 01:33:22,600
neutrino hits the target

1332
01:33:24,070 --> 01:33:25,770
the reverse of beta decay

1333
01:33:25,780 --> 01:33:27,610
and turns back into

1334
01:33:27,630 --> 01:33:28,850
the electron

1335
01:33:28,860 --> 01:33:31,490
so you see that this sort of

1336
01:33:31,490 --> 01:33:33,610
the energy that sort disappeared

1337
01:33:33,640 --> 01:33:35,390
in neutrino here

1338
01:33:35,460 --> 01:33:40,990
reappears the balanced downstream so something has gone from here to here you can see

1339
01:33:40,990 --> 01:33:45,290
it directly but if you believe in the conservation of energy and momentum everything balances

1340
01:33:45,290 --> 01:33:50,060
that some unseen neutral object was transported from creation to

1341
01:33:50,060 --> 01:33:53,950
hitting something in these experiments we now know

1342
01:33:53,980 --> 01:33:58,890
that is not just electrons but there are two heavy versions of the electron called

1343
01:33:58,890 --> 01:34:00,260
the me one in the town

1344
01:34:00,300 --> 01:34:02,640
the same amount of electric charge

1345
01:34:02,670 --> 01:34:04,860
the same properties

1346
01:34:04,880 --> 01:34:08,090
in all other respects the best experiments we can measure

1347
01:34:08,090 --> 01:34:13,860
except that the me on is about two hundred times more massive than electrons and

1348
01:34:13,860 --> 01:34:15,900
the tower is

1349
01:34:16,180 --> 01:34:18,600
thousands of times more massive

1350
01:34:18,660 --> 01:34:22,490
the two thousand times more massive than electrons four thousand times the mass of the

1351
01:34:22,490 --> 01:34:24,810
problem as stated the same

1352
01:34:24,820 --> 01:34:27,350
the electrons

1353
01:34:27,380 --> 01:34:28,520
has this

1354
01:34:28,520 --> 01:34:30,070
neutral counterparts

1355
01:34:30,080 --> 01:34:36,830
called the electron type neutrino with a new subscript e the neuron has an analysis

1356
01:34:36,830 --> 01:34:39,600
one new subscript

1357
01:34:39,620 --> 01:34:43,530
the town has an analogous one new subscript ten

1358
01:34:43,540 --> 01:34:47,790
three times of neutrino feature which matches with the three times that

1359
01:34:47,890 --> 01:34:50,170
how do we know that

1360
01:34:50,190 --> 01:34:51,440
the question

1361
01:34:51,530 --> 01:34:55,700
i removed the slide to tell having that but i think now i didn't here

1362
01:34:55,780 --> 01:34:59,290
we know that because experiments

1363
01:34:59,300 --> 01:35:01,830
like the ability to get the nucleus

1364
01:35:02,720 --> 01:35:05,490
an electron positrons produced

1365
01:35:05,510 --> 01:35:07,090
the neutrino is long

1366
01:35:07,110 --> 01:35:11,390
and it produces electrons or positrons not neurons or towns

1367
01:35:11,930 --> 01:35:15,010
there are experiments for example

1368
01:35:15,020 --> 01:35:16,900
the primary so it can decay

1369
01:35:16,920 --> 01:35:18,660
into a me one

1370
01:35:18,670 --> 01:35:25,060
and a neutrino that neutrino hits something upstream it turns back into a mu r

1371
01:35:25,060 --> 01:35:30,090
and we know from experience that originates from eleven states that there are

1372
01:35:30,250 --> 01:35:35,360
so is much harder to detect and measure for technical reasons but otherwise pretty much

1373
01:35:35,360 --> 01:35:36,070
the same

1374
01:35:36,220 --> 01:35:39,760
where the decay of a massive particle can turn into

1375
01:35:39,770 --> 01:35:46,490
a tau particle neutrino tau particle neutrino and if the neutrino hits something upstream it

1376
01:35:46,490 --> 01:35:48,410
turns back into attack

1377
01:35:48,470 --> 01:35:51,490
so these neutrinos are clearly different they

1378
01:35:51,500 --> 01:35:56,250
how a memory of how they were born which they transport them and reveal

1379
01:35:56,260 --> 01:36:01,110
later on but precisely what the nature of that is we don't know

1380
01:36:03,740 --> 01:36:08,330
so here is the summary of where we are with the fundamental

1381
01:36:08,340 --> 01:36:10,030
particles of matter

1382
01:36:10,880 --> 01:36:14,920
the electron as it has to have versions mu and tau

1383
01:36:14,940 --> 01:36:17,650
there are three varieties of neutrino

1384
01:36:17,660 --> 01:36:18,690
one for each

1385
01:36:18,700 --> 01:36:23,040
and this little family we call the leptons so lepton

1386
01:36:23,070 --> 01:36:24,570
is a generic words

1387
01:36:25,440 --> 01:36:28,050
particles like the electron and neutrino

1388
01:36:28,050 --> 01:36:29,360
it's been half

1389
01:36:29,380 --> 01:36:33,840
they feel the electromagnetic and weak forces

1390
01:36:33,880 --> 01:36:37,510
should say that more carefully they feel the electroweak force

1391
01:36:37,520 --> 01:36:43,260
neutrinos which have no charge do not feel the electromagnetic they feel weak

1392
01:36:44,880 --> 01:36:47,500
the london towers feel the full

1393
01:36:47,510 --> 01:36:50,500
panoply of electromagnetic and weak

1394
01:36:50,510 --> 01:36:52,940
they are all called leptons

1395
01:36:52,970 --> 01:36:56,000
they do not feel the strong force is the

1396
01:36:56,020 --> 01:36:59,830
forces that originate in the color charge of quarks carry

1397
01:36:59,880 --> 01:37:02,220
anything which fills the strong force

1398
01:37:02,220 --> 01:37:04,560
which is what's been half is the clerk

1399
01:37:04,570 --> 01:37:08,260
the up and down movement they live inside neutrons and protons

1400
01:37:08,280 --> 01:37:12,890
matters we know it they have heavier versions

1401
01:37:12,990 --> 01:37:15,880
the one with charge positive two-thirds

1402
01:37:15,900 --> 01:37:20,370
has a heavy version called charm the heavier version called top

1403
01:37:20,420 --> 01:37:23,820
identical in all respects except mass

1404
01:37:23,820 --> 01:37:25,240
pretty much

1405
01:37:25,250 --> 01:37:29,580
the down quark with a negative charge one for also has to have versions called

1406
01:37:29,580 --> 01:37:31,850
strange and bottom

1407
01:37:32,040 --> 01:37:35,750
o strange particles that i mentioned at the end of the first lecture discovered back

1408
01:37:35,750 --> 01:37:40,860
in nineteen forty seven in cosmic rays we now know are strange because they contain

1409
01:37:40,910 --> 01:37:44,280
strange quarks together

1410
01:37:44,300 --> 01:37:47,890
it was not formats

1411
01:37:47,920 --> 01:37:49,850
talk would be

1412
01:37:49,870 --> 01:37:51,170
pretty much the same

1413
01:37:51,220 --> 01:37:55,990
down strange in boston pretty much the same electron muon tau pretty much the same

1414
01:37:56,000 --> 01:38:00,790
but it's mass discriminates this is the beginning of the idea that i mentioned at

1415
01:38:00,790 --> 01:38:03,840
the end of the first talk but it is mass

1416
01:38:03,860 --> 01:38:04,990
which is the

1417
01:38:06,480 --> 01:38:10,750
inside the present standard model of particles if it wasn't for mass

1418
01:38:10,770 --> 01:38:11,840
they would all be

1419
01:38:12,590 --> 01:38:13,530
much the same

1420
01:38:13,550 --> 01:38:17,330
what it is there are three varieties and not more

1421
01:38:17,330 --> 01:38:18,640
from source

1422
01:38:19,380 --> 01:38:20,600
terminal then

1423
01:38:20,610 --> 01:38:22,610
you don't have been looking at this

1424
01:38:22,620 --> 01:38:24,380
more mathematically

1425
01:38:24,610 --> 01:38:26,760
later on

1426
01:38:26,770 --> 01:38:31,800
we iterate until all the least one saturated age no more

1427
01:38:31,910 --> 01:38:34,270
past the remaining

1428
01:38:34,290 --> 01:38:38,040
up along edges which is actually

1429
01:38:38,100 --> 01:38:40,030
that's the match max flow

1430
01:38:40,060 --> 01:38:41,130
i mean come

1431
01:38:41,180 --> 01:38:42,960
algorithms and

1432
01:38:43,010 --> 01:38:44,630
the set of talk about that

1433
01:38:44,660 --> 01:38:46,810
more rigorously

1434
01:38:51,760 --> 01:38:53,740
well as i said in

1435
01:38:53,750 --> 01:38:57,750
incidents in nineteen sixty two also probably the least then

1436
01:38:57,790 --> 01:39:01,440
so the idea is to use now by boykov and kolmogorov

1437
01:39:01,520 --> 01:39:04,960
efficient algorithms that vision researchers and

1438
01:39:04,970 --> 01:39:06,520
perhaps we use because

1439
01:39:06,540 --> 01:39:08,580
is freely implement freely

1440
01:39:11,150 --> 01:39:12,950
which i'm sure you'll find a few

1441
01:39:13,000 --> 01:39:16,860
look for the names boykov and common graphs

1442
01:39:16,870 --> 01:39:19,960
and we use is typically in graph cuts off

1443
01:39:22,590 --> 01:39:26,520
there also this one goal which i don't know much about

1444
01:39:26,680 --> 01:39:31,530
parallel in power law

1445
01:39:32,220 --> 01:39:36,450
examples of graph cuts in vision i'm sure this is a very incomplete graph cuts

1446
01:39:36,450 --> 01:39:38,940
is used all over the vision

1447
01:39:38,990 --> 01:39:40,120
these days

1448
01:39:40,350 --> 01:39:44,120
a number of things that i am able to show that it's useful in

1449
01:39:44,150 --> 01:39:48,210
it's just the fraction

1450
01:39:50,320 --> 01:39:52,250
one of them is another example

1451
01:39:54,150 --> 01:39:56,190
we looked at just

1452
01:39:56,230 --> 01:39:57,760
static textures

1453
01:39:57,810 --> 01:39:59,740
this interesting work on

1454
01:39:59,850 --> 01:40:02,210
video textures

1455
01:40:02,250 --> 01:40:05,390
by various people and i also name

1456
01:40:05,430 --> 01:40:09,470
rick szeliski as one of the people who has worked on this and also

1457
01:40:15,760 --> 01:40:18,780
the idea being you have video

1458
01:40:18,830 --> 01:40:19,570
and you

1459
01:40:19,580 --> 01:40:20,920
videos of them

1460
01:40:21,690 --> 01:40:27,420
so this is just an example flame or something like that and you want to

1461
01:40:28,930 --> 01:40:31,260
video just goes on forever

1462
01:40:32,420 --> 01:40:34,000
so you

1463
01:40:34,070 --> 01:40:36,160
could stacks together

1464
01:40:37,300 --> 01:40:38,950
taken taker

1465
01:40:38,950 --> 01:40:40,160
a clip

1466
01:40:40,210 --> 01:40:41,560
it represented by

1467
01:40:41,580 --> 01:40:44,350
the time dimension being the horizontal dimension here

1468
01:40:44,360 --> 01:40:46,930
put another one and then work out where can i cut

1469
01:40:46,930 --> 01:40:48,330
between these

1470
01:40:48,350 --> 01:40:50,960
two are

1471
01:40:51,000 --> 01:40:53,170
if the most

1472
01:40:53,250 --> 01:40:59,200
the way that use these notice then you describe this as an example graph cuts

1473
01:40:59,220 --> 01:41:00,460
i looked

1474
01:41:02,050 --> 01:41:05,590
so has this he says three d generalization image cool

1475
01:41:05,610 --> 01:41:07,320
it is

1476
01:41:07,340 --> 01:41:08,910
very good

1477
01:41:08,920 --> 01:41:10,690
project or

1478
01:41:10,700 --> 01:41:11,860
result from

1479
01:41:12,570 --> 01:41:16,310
michael code as i mentioned before from microsoft

1480
01:41:16,320 --> 01:41:21,900
well which included next thing which see graph paper on this where he took a

1481
01:41:22,690 --> 01:41:27,580
if one of make panoramic from images right so the question is a digital camera

1482
01:41:27,580 --> 01:41:31,370
you put things up and make a large image what about

1483
01:41:33,390 --> 01:41:36,850
so he did the single video panoramas where

1484
01:41:37,540 --> 01:41:39,620
take a video try

1485
01:41:41,460 --> 01:41:43,970
things are moving around in course

1486
01:41:43,980 --> 01:41:45,000
and he makes

1487
01:41:48,390 --> 01:41:54,090
continuing the you can just look around in this panorama and they always be

1488
01:41:54,110 --> 01:41:58,950
something moving in that area will go on forever so for instance one example is

1489
01:41:58,980 --> 01:42:04,700
what i haven't got here friday show examples of wavelets beech trees

1490
01:42:05,730 --> 01:42:07,050
flags waving

1491
01:42:07,070 --> 01:42:09,220
in the harbour at to

1492
01:42:11,700 --> 01:42:14,280
it's very impressive the way it's done

1493
01:42:14,980 --> 01:42:16,360
by doing this

1494
01:42:16,410 --> 01:42:21,660
making cuts in two dimensions going the spatial dimension of the image also time dimensions

1495
01:42:21,660 --> 01:42:23,020
working at high

1496
01:42:23,040 --> 01:42:26,660
these things together because it can't be done

1497
01:42:26,660 --> 01:42:30,190
perfectly all the time you have someone walk into this room

1498
01:42:30,200 --> 01:42:31,760
and out again

1499
01:42:31,770 --> 01:42:37,750
show that to forever doesn't look persons continually walking and out of the city but

1500
01:42:37,760 --> 01:42:43,030
it's a continuous action like waves of trees waving you don't notice anything artificial about

1501
01:42:43,040 --> 01:42:46,050
the whole thing looks like

1502
01:42:46,050 --> 01:42:48,130
so you lose b

1503
01:42:48,950 --> 01:42:55,390
it may not be a very accurate approximation of classification accuracy but it is also

1504
01:42:55,400 --> 01:42:57,720
so it is easier to talk to

1505
01:42:58,770 --> 01:43:12,350
so such functions you use the gradient hypothesis or use some global optimisation techniques

1506
01:43:12,670 --> 01:43:16,210
so we have to use gradient this

1507
01:43:16,240 --> 01:43:19,660
you can somehow be

1508
01:43:19,800 --> 01:43:25,760
well of course you want it to be differentiable but even that is not possible

1509
01:43:25,760 --> 01:43:35,650
for it to be differentiable almost everywhere then you finances optimisation techniques to minimize it

1510
01:43:35,670 --> 01:43:37,690
but you can also use

1511
01:43:37,710 --> 01:43:42,870
two days but the problem is if you many parameters where it is created what

1512
01:43:42,950 --> 01:43:49,900
is goal of this technique actually don't need to use these functions can directly what

1513
01:43:51,680 --> 01:43:57,890
you just try different parameter sets and she classification accuracy

1514
01:43:58,200 --> 01:44:05,570
user things like genetic reasons to mutation crossover to go to the next parameter set

1515
01:44:05,930 --> 01:44:10,970
created in this world but source code

1516
01:44:11,030 --> 01:44:13,530
is usually i don't know

1517
01:44:13,590 --> 01:44:15,760
and although these kind of feature scaling

1518
01:44:15,780 --> 01:44:18,720
the reason is

1519
01:44:20,070 --> 01:44:21,240
if we

1520
01:44:21,260 --> 01:44:26,030
if we do this kind of issues scale which is more complicated kernel so so

1521
01:44:26,300 --> 01:44:29,510
we are going to want to spend more inference

1522
01:44:29,550 --> 01:44:36,630
then the situation using only one single got to know your way to work

1523
01:44:36,640 --> 01:44:41,700
but really we we don't know we we never where they were given that the

1524
01:44:41,700 --> 01:44:43,700
performance is

1525
01:44:44,110 --> 01:44:50,090
it just as i said earlier this morning as so in designing machine learning and

1526
01:44:50,090 --> 01:44:55,720
reasoning you always searching for the better than simplicity and performance so

1527
01:44:55,820 --> 01:44:57,950
but you know what we are doing this

1528
01:44:58,010 --> 01:45:00,760
we try to use the more complicated

1529
01:45:00,910 --> 01:45:05,490
no we will never know if it performance is going to be better for that

1530
01:45:05,840 --> 01:45:08,930
this is a lot more studies to which

1531
01:45:09,280 --> 01:45:14,700
so we have to collect a out of bag benchmark datasets and try to try

1532
01:45:14,700 --> 01:45:16,160
to come to see

1533
01:45:16,550 --> 01:45:22,890
if using only one single kernel parameters go into the surfaces seriously of

1534
01:45:23,050 --> 01:45:25,160
feature scaling then will

1535
01:45:25,160 --> 01:45:30,910
we will one using feature scale is very significant better so we have that stated

1536
01:45:30,910 --> 01:45:35,240
and we are sure that maybe this time you have to do it by using

1537
01:45:35,240 --> 01:45:38,220
only one single got universities fine form

1538
01:45:38,220 --> 01:45:39,410
plus you want to try it

1539
01:45:39,430 --> 01:45:44,320
one is not is not very easy

1540
01:45:44,390 --> 01:45:49,370
there's another people kernel combination

1541
01:45:53,200 --> 01:45:57,530
so the question is how see something like this

1542
01:45:57,660 --> 01:46:00,780
so we we are considering

1543
01:46:01,050 --> 01:46:02,430
i kernels

1544
01:46:02,450 --> 01:46:04,870
you may seen is

1545
01:46:04,890 --> 01:46:09,260
obviously this going to be you know

1546
01:46:11,430 --> 01:46:13,390
o which is a plus sign

1547
01:46:17,010 --> 01:46:22,990
the combination so you with constant here that those coefficients this is like a linear

1548
01:46:22,990 --> 01:46:25,240
combination of several kernels

1549
01:46:25,410 --> 01:46:31,010
and there are ways of of such accommodation and assimilation p one

1550
01:46:31,030 --> 01:46:39,410
so using such things as well if you a k one k the other kernels

1551
01:46:39,840 --> 01:46:43,240
that is a linear combination is still OK

1552
01:46:43,950 --> 01:46:46,740
then the the

1553
01:46:46,760 --> 01:46:48,820
i want to say that this is actually

1554
01:46:48,840 --> 01:46:54,430
related to parameter selection and we just discussed

1555
01:46:54,450 --> 01:47:02,200
you for example you can see that at the kernels with different kernel parameters here

1556
01:47:02,220 --> 01:47:05,590
that this is you can always

1557
01:47:05,610 --> 01:47:07,130
parameter gamma one

1558
01:47:07,140 --> 01:47:12,200
these are you could always cover with parameter gamma i try to do a a

1559
01:47:12,200 --> 01:47:13,990
linear combination of

1560
01:47:16,390 --> 01:47:22,820
well so this is like to select the common parameter is you've got what is

1561
01:47:22,820 --> 01:47:26,610
good then this kind of constant then

1562
01:47:26,870 --> 01:47:34,090
in which we identify this stewardship because one another he eyes they should be close

1563
01:47:34,110 --> 01:47:34,990
to zero

1564
01:47:35,010 --> 01:47:37,550
so that's why is it so so that's why

1565
01:47:37,570 --> 01:47:43,140
this kind of companies using this is somehow related to parameter selection

1566
01:47:43,410 --> 01:47:51,410
so the reason why they should be scrapped kernel combination technique is used to the

1567
01:47:51,510 --> 01:47:58,550
machine learning community quite a few people work is one of the newspapers today

1568
01:47:58,570 --> 01:48:04,990
the idea by forming this kind of things into a convex function with conflicts with

1569
01:48:05,070 --> 01:48:06,840
his earlier music

1570
01:48:06,870 --> 01:48:09,860
we talk about parameters actions

1571
01:48:09,860 --> 01:48:13,760
this functional parameters you usually can

1572
01:48:13,780 --> 01:48:19,340
but this

1573
01:48:19,370 --> 01:48:23,280
for this kind of kernel combination which they can be formulated

1574
01:48:23,510 --> 01:48:28,010
only function of those you want to you

1575
01:48:28,110 --> 01:48:33,410
it is a convex function when c is fixed so still not good as the

1576
01:48:33,410 --> 01:48:40,160
sequence where penalty promises based on this function is a function that says may be

1577
01:48:40,160 --> 01:48:42,970
easier to be minimized by

1578
01:48:43,570 --> 01:48:45,450
it is of course the idea

1579
01:48:45,470 --> 01:48:53,300
programming problem optimisation so problem is a convex problem but the computational cost is also

1580
01:48:53,410 --> 01:48:58,430
quite high it's not very easy problem to be solved so it is clear

1581
01:48:58,780 --> 01:49:02,700
this kind of approach is going to be going to be very useful in practice

1582
01:49:03,320 --> 01:49:08,490
i think we need more empirical studies just like what i said earlier for

1583
01:49:08,490 --> 01:49:10,760
to feature scale

1584
01:49:10,910 --> 01:49:16,680
we may to collect also so as system and there was serious comp sch this

1585
01:49:17,450 --> 01:49:21,590
the second is the common kernel combinations

1586
01:49:21,760 --> 01:49:23,700
is come to regular

1587
01:49:23,700 --> 01:49:25,660
just prior to selection

1588
01:49:25,810 --> 01:49:28,630
to the average

1589
01:49:28,680 --> 01:49:29,910
many more

1590
01:49:30,950 --> 01:49:33,450
a lot

1591
01:49:38,740 --> 01:49:43,720
so i think that the number of support vectors are same

1592
01:49:43,740 --> 01:49:45,510
well it could be done

1593
01:49:47,680 --> 01:49:51,160
so you only uses one another

1594
01:49:51,180 --> 01:49:52,220
well i don't

1595
01:49:52,240 --> 01:49:54,280
clear it is that you one

1596
01:49:54,300 --> 01:49:58,030
so it is not only k one only needed

1597
01:49:58,030 --> 01:50:04,510
so the percentage of instances your support vectors it is increasing use of combinations where

1598
01:50:04,510 --> 01:50:10,260
this is going to be up or down the i don't know and i don't

1599
01:50:10,260 --> 01:50:14,160
decent work in this class and most of you have the ability to do good

1600
01:50:14,160 --> 01:50:16,870
work and some of you have a fair chunk of you have the ability to

1601
01:50:16,880 --> 01:50:21,230
do excellent work but may take some work on your part to get to that

1602
01:50:22,690 --> 01:50:27,070
the last thing i should say about the grades is why do i do this

1603
01:50:28,690 --> 01:50:32,570
really i try to do it as a sign of respect

1604
01:50:32,570 --> 01:50:35,160
for you and that may seem like a surprising to say what it just sort

1605
01:50:35,160 --> 01:50:38,150
of going on my little gleeful manner but i'm going to fail all of you

1606
01:50:38,910 --> 01:50:39,590
i mean

1607
01:50:39,600 --> 01:50:43,520
it's worth i think you guys are so

1608
01:50:44,940 --> 01:50:46,360
you're so

1609
01:50:47,960 --> 01:50:49,190
you've got and sold

1610
01:50:49,320 --> 01:50:50,390
so far

1611
01:50:50,460 --> 01:50:52,650
on your ability

1612
01:50:53,540 --> 01:50:56,020
many of you have learned to coast

1613
01:50:56,040 --> 01:51:03,520
and it's not doing you any kind of service to let you continue coasting my

1614
01:51:03,520 --> 01:51:06,090
goal here is to be honest with you

1615
01:51:07,170 --> 01:51:10,170
are smart enough probably most if you pull some sort of be without a lot

1616
01:51:10,170 --> 01:51:13,310
of without breaking into a sweat is not of significance sweat

1617
01:51:13,320 --> 01:51:14,810
so be it

1618
01:51:14,870 --> 01:51:19,110
but it's just lying due to pretend that excellence

1619
01:51:19,120 --> 01:51:20,500
in philosophy

1620
01:51:20,500 --> 01:51:24,110
so what i want to do in this class is be honest with you and

1621
01:51:24,110 --> 01:51:29,780
tell you you really don't work to be extraordinarily proud of yourself

1622
01:51:29,800 --> 01:51:34,340
first all you've done something OK or what you've done good work admittedly it's not

1623
01:51:34,340 --> 01:51:37,220
great but you don't good work

1624
01:51:37,290 --> 01:51:41,370
that seventy five percent of the great is the papers the remaining twenty five percent

1625
01:51:41,370 --> 01:51:43,900
grade is based on discussion section

1626
01:51:43,920 --> 01:51:47,850
now that's a lot of your great to turn on discussion section and so the

1627
01:51:47,850 --> 01:51:51,530
first thing i need to tell you is i really mean it

1628
01:51:51,530 --> 01:51:56,480
if you block discussion section your grade will suffer

1629
01:51:56,500 --> 01:51:59,910
so it's worth knowing in a general way what you need to do

1630
01:51:59,930 --> 01:52:04,510
there good grade discussion section and here answer is perhaps the obvious one

1631
01:52:04,520 --> 01:52:05,620
you need to

1632
01:52:07,320 --> 01:52:12,470
you need to come to discussion sections having thought about the lectures

1633
01:52:12,470 --> 01:52:17,030
having done the readings having thought about the questions that they raise

1634
01:52:17,040 --> 01:52:20,990
and you need to calm discussion section then prepared to

1635
01:52:22,920 --> 01:52:30,550
this week's set of issues you need to listen to what your classmates are saying

1636
01:52:30,580 --> 01:52:31,530
and say

1637
01:52:31,530 --> 01:52:35,600
why you disagree with them and not just that you disagree with them but to

1638
01:52:35,600 --> 01:52:37,810
raise an objection or

1639
01:52:37,830 --> 01:52:42,380
why you agree with them and when somebody else attacks and said i think that

1640
01:52:42,380 --> 01:52:45,310
what john was saying was a good point here is how i think he should

1641
01:52:45,310 --> 01:52:49,670
have defended his position or what have you you need to engage in

1642
01:52:49,740 --> 01:52:51,990
philosophical discussion

1643
01:52:52,000 --> 01:52:56,050
if you're not participating in discussion section

1644
01:52:56,110 --> 01:53:00,990
you're not doing what this section is there for

1645
01:53:01,270 --> 01:53:06,670
philosophers love to talk and we love to argue and the way to get better

1646
01:53:07,540 --> 01:53:09,650
thinking about philosophy

1647
01:53:09,650 --> 01:53:10,420
it is by

1648
01:53:10,440 --> 01:53:15,630
talking about philosophy and so i put my money where my mouth is i'm saying

1649
01:53:15,630 --> 01:53:19,520
look that an important part of the class so important that it's going to be

1650
01:53:19,520 --> 01:53:21,980
worth twenty five percent of the great

1651
01:53:22,000 --> 01:53:25,720
again it doesn't mean this is slightly different from the papers it doesn't mean that

1652
01:53:25,720 --> 01:53:31,060
you've got to be brilliant philosophically to get in a rather you've got to be

1653
01:53:31,060 --> 01:53:33,790
a wonderful class citizens

1654
01:53:33,880 --> 01:53:38,000
to get an a for discussion sections and so they put it back that input

1655
01:53:38,000 --> 01:53:42,500
is when the syllabus sort of participation and here i mean sort of you know

1656
01:53:42,580 --> 01:53:47,540
respectful participation not hogging the limelight but the participation

1657
01:53:47,560 --> 01:53:51,670
you can improve your grade but will lower your grade

1658
01:53:51,690 --> 01:53:57,290
non-participation or not being there that will lower your participation grade in any question about

1659
01:54:00,110 --> 01:54:05,110
i'm sorry to have sort of along gloom-and-doom but it seems that it's only fair

1660
01:54:05,650 --> 01:54:08,820
to let you know what you're getting into one of the remark about the discussion

1661
01:54:08,820 --> 01:54:15,310
sections the way i think of it is like the conversation hour for your foreign

1662
01:54:15,310 --> 01:54:16,670
language class

1663
01:54:16,690 --> 01:54:21,170
many of you have heard of what class before

1664
01:54:21,190 --> 01:54:23,110
it affects you

1665
01:54:23,150 --> 01:54:27,170
maybe fifteen percent of maybe twenty percent in most of you have not

1666
01:54:27,190 --> 01:54:33,230
that's pretty normal i don't go in the discussion section thinking all i can talk

1667
01:54:33,230 --> 01:54:37,460
i don't have any background philosophy never done this sort of thing before that's true

1668
01:54:37,480 --> 01:54:38,960
for most of you

1669
01:54:38,960 --> 01:54:42,130
the way you get better is by

1670
01:54:42,190 --> 01:54:46,040
talking philosophy

1671
01:54:48,210 --> 01:54:52,400
next remark this this is just one last connection with god great this is the

1672
01:54:52,400 --> 01:54:57,270
initial philosophy class the crucial point about intro is it means first class in philosophy

1673
01:54:57,540 --> 01:55:03,460
does presuppose any background in philosophy doesn't necessarily mean easy

1674
01:55:03,520 --> 01:55:07,400
some of this material for some of you is going to be very very difficult

1675
01:55:07,480 --> 01:55:12,400
and although the number of pages that you'll have to read are not there's not

1676
01:55:12,400 --> 01:55:13,360
a lot

1677
01:55:13,520 --> 01:55:16,770
probably typical b fifty pages may be less

1678
01:55:16,860 --> 01:55:20,770
it's from anything that fight dense material and

1679
01:55:20,860 --> 01:55:24,880
i don't really have the fantasy that many of you will read this stuff twice

1680
01:55:24,880 --> 01:55:29,000
if you had the time to do it that would be a wonderful thing to

1681
01:55:29,630 --> 01:55:35,290
philosophy is hard stuff to read

1682
01:55:35,290 --> 01:55:40,190
a remark about this being into into classes is that its introductory and that the

1683
01:55:40,190 --> 01:55:43,690
issues that we're talking about our kind of first

1684
01:55:43,690 --> 01:55:45,250
run through

1685
01:55:45,270 --> 01:55:51,940
every single thing that we discuss here could be pursued at greater depth so for

1686
01:55:53,580 --> 01:55:57,860
will spend whatever it is maybe we can have talking about the nature personal identity

1687
01:55:57,860 --> 01:55:58,840
two weeks

1688
01:55:58,880 --> 01:56:05,840
but one could easily spend an entire semester thinking about that question alone so don't

1689
01:56:05,840 --> 01:56:09,500
come away thinking that whatever it is that we're talking about here lecture is the

1690
01:56:09,500 --> 01:56:16,080
last word on the subject rather it's something more like the first words

1691
01:56:16,090 --> 01:56:21,170
actually one of the work about what what about the readings and the lectures

1692
01:56:21,170 --> 01:56:28,460
of the characteristic function so well sorry and the moment generating functions

1693
01:56:28,480 --> 01:56:35,510
and this is this is simply due to the fact that you know the sum

1694
01:56:35,530 --> 01:56:39,530
the density of the sum is equal to the convolution

1695
01:56:39,550 --> 01:56:41,770
all of the of the densities

1696
01:56:41,770 --> 01:56:47,070
and now in the fourier transformations convolutions are transformed into products

1697
01:56:47,090 --> 01:56:53,070
so that's why in the domain of characteristic function so a moment sorry moment generating

1698
01:56:53,070 --> 01:56:58,650
functions is this some which implies the convolution in the density domain is simply a

1699
01:57:00,170 --> 01:57:03,190
and of course if we have a product then for

1700
01:57:03,360 --> 01:57:08,440
means that that is for the cumulant generating function we have a sample

1701
01:57:10,800 --> 01:57:15,210
it is the sum and because the information is simply a linear thing you do

1702
01:57:15,210 --> 01:57:18,210
it separately for each them into some

1703
01:57:18,270 --> 01:57:22,480
that's why you will see that became as the sum some

1704
01:57:22,500 --> 01:57:24,150
sorry commitment

1705
01:57:24,170 --> 01:57:29,440
the sum will be equal to the sum of the components

1706
01:57:30,480 --> 01:57:34,000
and the point is that in the second when n is equal to

1707
01:57:34,010 --> 01:57:39,250
for the second well actually well start when n any equals one

1708
01:57:39,250 --> 01:57:44,070
the first derivative so that this is simply to me

1709
01:57:44,070 --> 01:57:46,780
and when it was sold

1710
01:57:46,940 --> 01:57:51,130
what we get is the variance so expectation of

1711
01:57:51,150 --> 01:57:54,090
x squared minus

1712
01:57:54,110 --> 01:57:58,170
the expectation it says what it and now what happens is that if you can

1713
01:57:58,170 --> 01:58:03,800
equals if any course three well in that case actually so this is in if

1714
01:58:03,800 --> 01:58:04,460
we have

1715
01:58:04,500 --> 01:58:07,440
if you have a zero mean

1716
01:58:07,500 --> 01:58:12,860
zero mean variables and we only get the expectation of the of the pilots that

1717
01:58:12,860 --> 01:58:18,940
moment and now when we can equals four then we have so that's idea the

1718
01:58:18,940 --> 01:58:22,030
and then the fourth order cumulants equals

1719
01:58:23,570 --> 01:58:26,570
as written down there

1720
01:58:26,590 --> 01:58:28,710
but it's is all about this idea of

1721
01:58:28,730 --> 01:58:30,840
of characteristic functions

1722
01:58:30,860 --> 01:58:36,400
that the characteristic function of a sum being the product of characteristic functions so taking

1723
01:58:36,440 --> 01:58:42,030
organisms it's some of kind of some of the logarithms of characteristic functions and so

1724
01:58:42,030 --> 01:58:46,380
when you when you then you can make any kind of linear transformations and you

1725
01:58:46,380 --> 01:58:51,340
still have this amazing property in particular can take you can take the derivative and

1726
01:58:51,380 --> 01:58:53,530
evaluate them at any point

1727
01:58:53,550 --> 01:58:58,400
well you develop them at point zero because that's the brightly by far the simplest

1728
01:58:58,400 --> 01:59:01,940
thing to do if you tried to evaluate that at any other point you will

1729
01:59:01,940 --> 01:59:03,800
get into a lot of trouble

1730
01:59:08,730 --> 01:59:10,920
the reason why

1731
01:59:12,880 --> 01:59:16,530
one city

1732
01:59:16,550 --> 01:59:18,590
well in

1733
01:59:18,590 --> 01:59:25,440
it's just that too many distributions in practice are symmetric so it uses skewness will

1734
01:59:25,440 --> 01:59:29,610
not work when you when you happen to have a symmetric distribution

1735
01:59:29,920 --> 01:59:34,280
so we can we can we there's no problem we can we can develop a

1736
01:59:34,280 --> 01:59:39,590
method for ICA estimation where we play everything where i want to talk about the

1737
01:59:39,590 --> 01:59:43,960
kurtosis we can always replace it with skewness but the problem is that those things

1738
01:59:44,150 --> 01:59:51,090
those methods only work when you have non symmetric densities and we fortunately unfortunately we

1739
01:59:51,090 --> 01:59:55,570
have quite open symmetric densities and so we want everyone to have a general purpose

1740
01:59:55,570 --> 02:00:00,770
method we cannot really use skewness at least we cannot use skewness alone we could

1741
02:00:00,770 --> 02:00:06,340
use some kind of a combination of skewness kurtosis

1742
02:00:07,070 --> 02:00:12,480
so as i mentioned that his skewness because skewness is zero

1743
02:00:12,480 --> 02:00:16,110
so then now you might ask that's how do we know that the process is

1744
02:00:16,110 --> 02:00:18,130
not zero

1745
02:00:18,150 --> 02:00:22,340
maybe bigger well we will see later that then with we have is given as

1746
02:00:22,360 --> 02:00:27,570
as zero then we get into kind of then the midst where we just you

1747
02:00:27,570 --> 02:00:32,880
know that generates objective functions well it's just that

1748
02:00:32,880 --> 02:00:34,110
it's just kind of

1749
02:00:34,420 --> 02:00:40,860
empirical observation that most often when you have nongaussian random variables they have non zero

1750
02:00:40,860 --> 02:00:44,480
skewness is that every non-zero characteristic

1751
02:00:44,590 --> 02:00:48,840
it not necessary to show you can construct some kind of

1752
02:00:49,130 --> 02:00:54,110
non gaussian random variables that have exactly zero kurtosis but we just kind of hope

1753
02:00:54,360 --> 02:00:57,570
we don't need that kind of area

1754
02:01:00,150 --> 02:01:04,090
these properties here are basically true for any cumulants

1755
02:01:04,110 --> 02:01:10,880
well this property is true for any equivalent and then this simpler homogeneity property is

1756
02:01:10,880 --> 02:01:15,380
true for any component but then this exponent here will be also the

1757
02:01:15,420 --> 02:01:19,090
order of that commitment

1758
02:01:19,130 --> 02:01:26,730
OK do you have any questions on this cumulants stuff and so on

1759
02:01:29,920 --> 02:01:36,630
so actually much before actually is a very old concept it it has been useful

1760
02:01:36,650 --> 02:01:39,780
for almost perhaps perhaps even one hundred years before

1761
02:01:39,800 --> 02:01:41,500
the invention of ICA

1762
02:01:42,960 --> 02:01:46,840
one of the reasons is that it's kind of it people have used this isn't

1763
02:01:46,840 --> 02:01:53,710
some kind of intuitive characterization of the nongaussianity of distributions depict people talk about the

1764
02:01:54,070 --> 02:02:01,130
book of distributions which have positive kurtosis and negative kurtosis having kind of the typical

1765
02:02:01,130 --> 02:02:05,250
forms of pdf so when you if you measure the kurtosis of the distribution and

1766
02:02:05,250 --> 02:02:11,400
you see that it negative then you intuitively understand something about this here you have

1767
02:02:11,400 --> 02:02:14,500
some basic examples

1768
02:02:14,920 --> 02:02:15,940
so this

1769
02:02:15,960 --> 02:02:17,940
all this

1770
02:02:17,960 --> 02:02:19,980
probability density

1771
02:02:20,420 --> 02:02:27,480
is called the laplacian probability density but it's actually write down the

1772
02:02:27,500 --> 02:02:30,920
the distribution function perhaps

1773
02:02:30,940 --> 02:02:33,610
so it is also called the double exponential

1774
02:02:33,630 --> 02:02:39,400
density because it is simply the exponential density like on two different on two sides

1775
02:02:39,780 --> 02:02:41,800
so the pdf is given by

1776
02:02:41,820 --> 02:02:47,610
some normalisation constant times the exponential of a function of

1777
02:02:49,610 --> 02:02:52,440
the absolute value of x

1778
02:02:52,460 --> 02:02:56,900
and here we go well here we can have some kind of variance

1779
02:02:56,940 --> 02:03:00,210
variance too and this is what is that what is the scale

1780
02:03:00,400 --> 02:03:04,250
but the point is that this is like in the same vein gaussian case we

1781
02:03:04,250 --> 02:03:08,380
have square but here we have just the absolute value so that's why that's why

