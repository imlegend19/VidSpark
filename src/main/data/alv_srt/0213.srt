1
00:00:00,000 --> 00:00:03,020
it's definitely theorem something called came in theorem shows that

2
00:00:03,520 --> 00:00:05,420
if you have an exchangeable random partition

3
00:00:06,750 --> 00:00:07,290
then it comes

4
00:00:07,710 --> 00:00:10,000
essentially then it comes from a random discrete measures

5
00:00:10,810 --> 00:00:12,590
it's newspaper in discrete manner in this way

6
00:00:14,150 --> 00:00:15,040
if and only if can

7
00:00:16,730 --> 00:00:18,500
can agree not going to dwell on this today

8
00:00:19,090 --> 00:00:20,520
i might come back to it tomorrow

9
00:00:22,770 --> 00:00:26,060
vectors is induced random partitions so we have the dirichlet process

10
00:00:26,710 --> 00:00:27,570
and induces

11
00:00:29,400 --> 00:00:30,790
on random partitions and this

12
00:00:31,250 --> 00:00:34,170
distribution on random partitions is called the chinese restaurant process

13
00:00:35,650 --> 00:00:37,560
so you probably have heard both terms before

14
00:00:38,770 --> 00:00:41,250
it certainly will if you go to machine learning conferences days

15
00:00:41,880 --> 00:00:45,660
and the connection between the two is the directly process is a distribution on a

16
00:00:45,660 --> 00:00:47,310
random discrete measures on the mixing action

17
00:00:48,440 --> 00:00:52,810
the random partition that's induced this has a distribution which is called the chinese restaurant process

18
00:00:56,860 --> 00:00:57,360
there's this

19
00:00:58,340 --> 00:00:59,130
u u often

20
00:01:00,770 --> 00:01:05,770
basically any paper that relates to the chinese restaurant process you see a picture like this

21
00:01:07,690 --> 00:01:09,110
the idea is that these are tables

22
00:01:09,940 --> 00:01:11,090
andy these are

23
00:01:13,960 --> 00:01:16,840
what it very simply means customers are observations

24
00:01:18,150 --> 00:01:21,480
and the numbers are the indices of the observation points come in

25
00:01:22,130 --> 00:01:25,340
and the tables are clusters so they are the blocks of the partition

26
00:01:26,460 --> 00:01:31,040
and then the customers sitting at a table simply another way of saying the data points in the cluster

27
00:01:31,520 --> 00:01:32,770
are belongs to a certain block

28
00:01:36,460 --> 00:01:40,880
i would quickly like to edit his trickery markets originally the chinese restaurant process

29
00:01:41,290 --> 00:01:42,420
was not introduced has

30
00:01:43,090 --> 00:01:46,070
has a distribution on partitions but on random permutations

31
00:01:48,210 --> 00:01:49,250
i'm doing some pitman

32
00:01:50,190 --> 00:01:50,730
use this

33
00:01:51,460 --> 00:01:53,400
describe something like this where they

34
00:01:56,270 --> 00:01:57,900
start points on these tables here

35
00:01:58,420 --> 00:01:59,420
to generate random

36
00:02:00,750 --> 00:02:01,440
random partition

37
00:02:02,810 --> 00:02:03,830
a random permutation

38
00:02:04,380 --> 00:02:06,090
and this random permutation is infinite

39
00:02:07,000 --> 00:02:10,500
and that's pretty remarkable thing was not clear at the time you could do something

40
00:02:10,500 --> 00:02:13,560
like this and the described this very simple idea with which you can generate this

41
00:02:14,480 --> 00:02:19,650
and that's why by people in probability theory and more theoretical talk it's got excited about this

42
00:02:20,110 --> 00:02:21,650
but basically what you do then is

43
00:02:23,130 --> 00:02:28,060
in when you generate permutations the order in which people sit at these tables matters

44
00:02:29,420 --> 00:02:32,730
if you want to generate a partition the order doesn't matter when he belongs who

45
00:02:32,730 --> 00:02:34,590
is it only matters who is in which block

46
00:02:36,500 --> 00:02:41,230
you get the distribution you get a distribution on partitions by throwing away the order

47
00:02:41,340 --> 00:02:45,060
forget the order the solution and permutations you forget the order you get a distribution

48
00:02:45,060 --> 00:02:45,690
on partitions

49
00:02:47,060 --> 00:02:49,150
that's then also called the chinese restaurant process

50
00:02:50,270 --> 00:02:52,880
in a way much simpler objects that's what we always use in

51
00:02:59,020 --> 00:03:00,940
i would i would like to make a personal appeal you

52
00:03:01,610 --> 00:03:02,670
if you write papers

53
00:03:03,480 --> 00:03:05,230
on some bayesian nonparametric model

54
00:03:05,840 --> 00:03:07,940
please don't call a chinese restaurant something

55
00:03:09,230 --> 00:03:11,630
it's just not funny really it's

56
00:03:12,420 --> 00:03:13,310
it has gone out of hand

57
00:03:14,860 --> 00:03:16,000
it's an industry don't do it

58
00:03:16,670 --> 00:03:17,360
don't participate

59
00:03:19,460 --> 00:03:20,330
built these models but

60
00:03:21,110 --> 00:03:22,360
call them something reasonable

61
00:03:28,340 --> 00:03:32,330
when we look at these exchangeable random partitions these these are these are the objects

62
00:03:32,330 --> 00:03:33,960
that you want to use a clustering now

63
00:03:34,590 --> 00:03:36,960
want to clustering in a nonparametric bayesian way

64
00:03:40,210 --> 00:03:41,770
unless something is is very odd

65
00:03:42,340 --> 00:03:43,670
what we want to have is

66
00:03:44,690 --> 00:03:46,770
an exchangeable random partition has a prior

67
00:03:47,170 --> 00:03:49,630
and so there are exchangeable random partition the parameter

68
00:03:50,150 --> 00:03:54,150
so basically what we have to ask is what distributions other an exchangeable random partitions

69
00:03:57,960 --> 00:03:58,650
ten years ago

70
00:03:59,190 --> 00:04:02,480
basically the only one that we knew we are the only one at least they

71
00:04:02,480 --> 00:04:05,190
had already made to machine learning was dirichlet process

72
00:04:05,860 --> 00:04:07,400
and today we have a much better picture

73
00:04:07,980 --> 00:04:11,730
of what this is the landscape of these models looks like which models are possible

74
00:04:13,170 --> 00:04:15,070
also to some degree which properties they have

75
00:04:15,460 --> 00:04:17,270
even though there's quite a lot of work still to be done

76
00:04:18,250 --> 00:04:18,860
and then and

77
00:04:19,330 --> 00:04:21,060
quickly want to tell you a few words about the

78
00:04:26,630 --> 00:04:29,400
here's a diagram of certain families of

79
00:04:31,230 --> 00:04:32,690
of exchangeable random partitions

80
00:04:33,110 --> 00:04:35,440
and we've seen a few of these so we've seen the dirichlet process

81
00:04:36,480 --> 00:04:39,230
we've seen the pitman yor process and there's

82
00:04:39,750 --> 00:04:41,570
an arrow here which indicates that

83
00:04:42,070 --> 00:04:43,460
this is a special case of this

84
00:04:44,150 --> 00:04:47,880
you can obtain the dirichlet process from the manual by certain parameters

85
00:04:49,540 --> 00:04:50,590
okay we've also seen

86
00:04:51,210 --> 00:04:53,290
the bayes mixture model where the distribution on

87
00:04:54,130 --> 00:04:54,420
on the

88
00:04:55,000 --> 00:04:58,960
the cluster sizes is simply directly distribution finite distribution

89
00:04:59,420 --> 00:05:00,810
and so that's parametric

90
00:05:01,310 --> 00:05:02,290
but it is exchangeable

91
00:05:02,770 --> 00:05:04,690
so that is also an exchangeable random partition

92
00:05:05,250 --> 00:05:09,940
and then we can generalize that and take a mixture of finite directly so in

93
00:05:09,940 --> 00:05:11,270
the prior put in a mixture

94
00:05:11,790 --> 00:05:12,940
and he remains exchangeable

95
00:05:13,670 --> 00:05:16,380
okay so there is also a way of creating gender factor

96
00:05:17,170 --> 00:05:20,020
so basically the ones i said in blue here are the ones we've seen before

97
00:05:20,420 --> 00:05:22,290
and this one you are going to see on the next slide

98
00:05:23,150 --> 00:05:27,420
and then there's a bunch of others you so there's is a large class of so-called crosslinking models

99
00:05:28,920 --> 00:05:30,330
this is really not

100
00:05:33,230 --> 00:05:38,130
i think this has really made it into statistics and machine learning this so far only studied in probability theory

101
00:05:38,920 --> 00:05:42,710
then there's something called normalized completely random measures which is the very last class large

102
00:05:42,730 --> 00:05:44,170
class of models and very useful

103
00:05:44,920 --> 00:05:47,210
but it turns out that in a certain sense these

104
00:05:47,860 --> 00:05:49,020
keep stivers might be

105
00:05:49,460 --> 00:05:50,000
more useful

106
00:05:52,230 --> 00:05:54,610
but most of this is very current research so on

107
00:05:55,460 --> 00:05:58,750
this is all stuff you that was developed over the past five years

108
00:06:02,330 --> 00:06:05,380
all right and you can see that these gives type measures you basically have errors

109
00:06:05,400 --> 00:06:07,360
to to everything down here right

110
00:06:07,940 --> 00:06:09,630
so that's a nice generalization

111
00:06:15,210 --> 00:06:15,790
let me give you

112
00:06:17,610 --> 00:06:18,730
heuristic description

113
00:06:19,940 --> 00:06:24,310
description by circumstantial evidence what these gives type which not actually going to define them

114
00:06:24,900 --> 00:06:27,540
because that would get a bit technical but i want to tell you what they

115
00:06:27,540 --> 00:06:29,110
are and how we can think about

116
00:06:31,770 --> 00:06:35,230
i will follow this classification here which has been suggested by

117
00:06:36,040 --> 00:06:37,290
but you start with some

118
00:06:38,330 --> 00:06:39,480
mathematics statistician

119
00:06:39,960 --> 00:06:42,900
and this is this is very recent so you have an you know he he

120
00:06:43,420 --> 00:06:43,980
has shown this

121
00:06:44,860 --> 00:06:49,710
he using its talks but i think the paper is not even out it's just very current research

122
00:06:50,400 --> 00:06:52,810
but i think this this classification you know

123
00:06:53,730 --> 00:06:55,560
since all this stuff is ongoing work

124
00:06:56,020 --> 00:06:59,380
i can't tell you whether this is the right classification might be that in ten

125
00:06:59,380 --> 00:07:01,340
years we just understand things better

126
00:07:01,730 --> 00:07:04,270
and we have a better one but this is i think currently

127
00:07:04,770 --> 00:07:07,480
our best possible guess what things are going to look like

128
00:07:10,750 --> 00:07:12,360
what would see is the following so he

129
00:07:13,670 --> 00:07:14,980
he classifies

130
00:07:15,400 --> 00:07:17,170
these models according to

131
00:07:18,520 --> 00:07:20,920
the probability of creating a new cluster

132
00:07:22,590 --> 00:07:24,960
and that is really a very neat idea because

133
00:07:26,830 --> 00:07:29,360
because we're interested in the distribution on the cluster sizes

134
00:07:30,020 --> 00:07:34,150
and that is controlled by how we create new clusters right so we can look at this probability

135
00:07:35,880 --> 00:07:37,790
and use it to classify these models

136
00:07:39,940 --> 00:07:41,690
he classifies them according to

137
00:07:42,190 --> 00:07:43,920
what this probability depends on

138
00:07:44,500 --> 00:07:47,540
so what could possibly depend on it could depend on

139
00:07:47,980 --> 00:07:49,110
the number of clusters

140
00:07:49,110 --> 00:07:51,760
you you need some time if you have corpus

141
00:07:51,770 --> 00:07:54,260
you want to pass all these sentences

142
00:07:54,260 --> 00:07:56,800
this stick sometimes particles flow

143
00:07:56,850 --> 00:07:59,150
so maybe you can research

144
00:07:59,170 --> 00:08:00,990
these expensive features

145
00:08:01,020 --> 00:08:03,230
only four examples

146
00:08:04,940 --> 00:08:06,480
well that may be

147
00:08:06,490 --> 00:08:10,180
ambiguous way of current classification is not so

148
00:08:10,180 --> 00:08:14,310
not so clear about it so that only a limited set of your

149
00:08:14,320 --> 00:08:19,300
examples have to be passed but i show you an example of late so

150
00:08:19,340 --> 00:08:20,760
make small clean

151
00:08:20,840 --> 00:08:24,720
and finally we used techniques of active learning

152
00:08:24,730 --> 00:08:27,420
to reduce the number of annotations

153
00:08:27,430 --> 00:08:29,980
so active learning is a technique where

154
00:08:30,520 --> 00:08:32,140
the system

155
00:08:32,190 --> 00:08:37,520
two sons automatically examples from an unlabeled set

156
00:08:37,840 --> 00:08:40,260
to be annotated by humans

157
00:08:40,270 --> 00:08:42,390
so in the assumptions

158
00:08:43,300 --> 00:08:48,710
by having them good examples labeled you can reduce the manual

159
00:08:48,730 --> 00:08:52,150
annotation work

160
00:08:52,170 --> 00:08:55,920
so this is why it's not it's not very clear sorry

161
00:08:55,970 --> 00:09:01,360
maybe if few and large print out a slightly larger

162
00:09:01,390 --> 00:09:02,900
it becomes clear

163
00:09:02,920 --> 00:09:03,770
so we

164
00:09:03,810 --> 00:09:06,480
just to the cascade approach

165
00:09:07,210 --> 00:09:10,490
we had three levels in our cascade

166
00:09:10,690 --> 00:09:13,730
in the first level

167
00:09:13,740 --> 00:09:15,380
we were interested

168
00:09:15,390 --> 00:09:18,230
in through its remember we have to classify

169
00:09:18,230 --> 00:09:22,070
positive new positive negative and neutral

170
00:09:22,090 --> 00:09:26,670
so in the first level usually there are many more neutral examples that

171
00:09:26,720 --> 00:09:32,570
then positive and negative effects so in the first level we want to look for

172
00:09:32,690 --> 00:09:34,230
neutral examples

173
00:09:34,270 --> 00:09:36,760
can be of course nice sentences

174
00:09:36,780 --> 00:09:40,850
with no feeling at all towards a certain product

175
00:09:40,930 --> 00:09:45,800
but it can also be genk if you download from the webpage

176
00:09:46,220 --> 00:09:49,300
you might also have lots of

177
00:09:49,310 --> 00:09:51,680
but you cannot screen perfectly

178
00:09:51,690 --> 00:09:55,380
o it's quite difficult to clean

179
00:09:55,560 --> 00:09:59,010
maybe you know of of good clean of web pages

180
00:09:59,020 --> 00:10:02,270
but at least the data that we receive from

181
00:10:02,310 --> 00:10:03,800
from ten

182
00:10:03,820 --> 00:10:09,100
they contain sometimes fragments of menus of also quite checks sentence

183
00:10:10,550 --> 00:10:13,220
so maybe i forgot to tell first of all

184
00:10:13,270 --> 00:10:15,550
we selected sentences

185
00:10:15,550 --> 00:10:18,050
that contains search terms

186
00:10:18,070 --> 00:10:21,550
that contain card names and moving

187
00:10:21,550 --> 00:10:27,100
so we have a list of chord names that were search for list of movie

188
00:10:27,140 --> 00:10:31,270
names that are search for in all these data in this dataset

189
00:10:31,300 --> 00:10:35,400
and then we replaced the name of the current name of the move we just

190
00:10:35,400 --> 00:10:40,760
in the sentence by the general called so we had more three examples

191
00:10:40,810 --> 00:10:42,310
so we had to set

192
00:10:42,360 --> 00:10:43,550
and then we did

193
00:10:43,550 --> 00:10:50,390
ten fold cross validation or sometimes the averaged over five splits usually fall

194
00:10:50,440 --> 00:10:52,560
cross validation

195
00:10:52,570 --> 00:10:55,430
so we have data set of sentences

196
00:10:55,480 --> 00:10:59,010
we knew that in the sentence

197
00:10:59,050 --> 00:11:05,180
the entity is mentioned so we suppose that the and it is correctly recognised

198
00:11:05,230 --> 00:11:10,260
so this is a task somehow escaped for real that task text

199
00:11:10,380 --> 00:11:15,930
because you know named entity recognition might take only eighty percent of the curacy for

200
00:11:15,930 --> 00:11:18,850
these brands or maybe eighty five

201
00:11:18,900 --> 00:11:23,900
so this is the difficulty which we already skipped here in this exercise

202
00:11:24,930 --> 00:11:27,600
first of all

203
00:11:28,100 --> 00:11:31,180
in the first level

204
00:11:31,860 --> 00:11:33,300
try to

205
00:11:34,010 --> 00:11:35,760
we try to

206
00:11:35,770 --> 00:11:38,590
it's too remove to throw out

207
00:11:38,660 --> 00:11:39,990
o neutral

208
00:11:40,020 --> 00:11:43,340
as many as possible future examples

209
00:11:43,340 --> 00:11:47,150
so we had to have an aggregation of tree classifiers

210
00:11:47,170 --> 00:11:51,730
which all had to be very certain about the classification

211
00:11:55,590 --> 00:11:59,800
it's about the

212
00:11:59,850 --> 00:12:01,680
if they were

213
00:12:02,420 --> 00:12:08,100
so about the classification of the future example it is true and so this is

214
00:12:08,180 --> 00:12:09,670
the idea

215
00:12:09,720 --> 00:12:11,950
in the second level we

216
00:12:11,970 --> 00:12:17,280
so here is an aggregation of tree classifiers so these examples of the all the

217
00:12:17,300 --> 00:12:24,130
definitely classified in the second level we have a classification in three

218
00:12:24,140 --> 00:12:27,470
in the two groups neutral positive and negative

219
00:12:27,510 --> 00:12:33,600
but then for the examples where to this classifier was not certain about we

220
00:12:33,650 --> 00:12:35,140
given we

221
00:12:35,140 --> 00:12:38,340
input into it to love

222
00:12:38,350 --> 00:12:40,380
and these were examples

223
00:12:40,390 --> 00:12:42,110
maybe when you have

224
00:12:42,710 --> 00:12:45,300
an opinion expressed

225
00:12:45,310 --> 00:12:46,950
two towards different

226
00:12:48,270 --> 00:12:50,590
different entities

227
00:12:50,600 --> 00:12:53,950
or or or an opinion expressed

228
00:12:54,230 --> 00:12:59,600
of the opinion expressed were different entities for instance so it could be a negative

229
00:12:59,600 --> 00:13:03,210
feelings for BMW but the positive feeling for

230
00:13:03,270 --> 00:13:04,730
proceedings of so

231
00:13:04,770 --> 00:13:09,430
but of course we are interested in the feeling for see this in this case

232
00:13:09,430 --> 00:13:15,060
this not the the case and be able to build a small crater from

233
00:13:15,110 --> 00:13:18,280
it's hard

234
00:13:19,790 --> 00:13:24,700
we tend to all the sort who is

235
00:13:24,710 --> 00:13:28,470
are very control

236
00:13:28,490 --> 00:13:31,360
to a lesser extent also

237
00:13:32,360 --> 00:13:36,390
and all that generates those

238
00:13:39,590 --> 00:13:40,920
the first thing

239
00:13:41,630 --> 00:13:46,000
one of the more on you the

240
00:13:48,030 --> 00:13:51,740
mike are for

241
00:13:51,780 --> 00:13:52,950
it's very

242
00:13:53,000 --> 00:13:56,180
we want to write

243
00:13:56,210 --> 00:13:58,240
programme which primarily

244
00:13:59,480 --> 00:14:01,210
o the

245
00:14:01,430 --> 00:14:10,450
so her but really everything from scratch is a really bad idea and not only

246
00:14:16,920 --> 00:14:19,470
and also

247
00:14:23,990 --> 00:14:27,930
programs are going to communicate each other

248
00:14:27,950 --> 00:14:33,710
a lot of people

249
00:14:33,730 --> 00:14:35,180
because they have to look for

250
00:14:36,600 --> 00:14:40,930
we need more might want to read what people write for readers

251
00:14:45,260 --> 00:14:48,200
there's one

252
00:14:51,820 --> 00:14:53,270
get some of the

253
00:14:53,270 --> 00:14:55,060
the first people to

254
00:14:55,110 --> 00:14:56,570
but this is about

255
00:14:56,580 --> 00:15:01,080
reading and reading more than right because i actually read much more

256
00:15:01,110 --> 00:15:05,030
many more intelligence than the average bits

257
00:15:05,070 --> 00:15:07,000
because usually hoping people

258
00:15:07,010 --> 00:15:09,580
with some of the getty supposed to build the

259
00:15:12,580 --> 00:15:15,430
so that's why

260
00:15:17,290 --> 00:15:23,090
one of them not all i've seen lots of success here

261
00:15:28,810 --> 00:15:30,830
so people say

262
00:15:31,700 --> 00:15:35,570
and the thing to me but i don't think about it though

263
00:15:36,310 --> 00:15:43,730
the two opposing the term name or what

264
00:15:43,730 --> 00:15:44,650
or four

265
00:15:44,670 --> 00:15:45,950
email address

266
00:15:45,960 --> 00:15:52,550
but you know that's like saying one thing wrong

267
00:15:57,140 --> 00:16:03,550
i was doing

268
00:16:08,180 --> 00:16:10,510
o point

269
00:16:10,510 --> 00:16:18,710
all you can ask

270
00:16:18,980 --> 00:16:24,140
six schools really also to two people

271
00:16:26,830 --> 00:16:29,430
what is it

272
00:16:34,160 --> 00:16:36,300
i understand that you can

273
00:16:36,310 --> 00:16:40,480
create the service requires a lot

274
00:16:40,790 --> 00:16:44,590
you know what things to read so that's a good thing

275
00:16:44,610 --> 00:16:45,810
so the

276
00:16:45,830 --> 00:16:48,150
it's not this

277
00:16:48,210 --> 00:16:56,520
are we can change things change the more

278
00:16:58,610 --> 00:17:03,520
all this is very important

279
00:17:03,530 --> 00:17:05,590
it's a

280
00:17:08,360 --> 00:17:14,540
it uses all and we told that stays

281
00:17:31,310 --> 00:17:36,920
one more

282
00:17:41,730 --> 00:17:43,400
four or less

283
00:17:43,420 --> 00:17:49,000
three fixed exchange

284
00:17:49,010 --> 00:17:53,530
if you can create an effective close

285
00:17:54,390 --> 00:17:55,810
right so

286
00:17:55,840 --> 00:17:58,930
that's one thing we're going to be learning how to do it

287
00:18:00,050 --> 00:18:01,710
also mention

288
00:18:01,720 --> 00:18:04,390
the idea

289
00:18:09,410 --> 00:18:12,310
so much

290
00:18:12,450 --> 00:18:13,440
the tower

291
00:18:13,450 --> 00:18:15,740
seven hours were

292
00:18:25,160 --> 00:18:29,200
the things he's trying to figure out

293
00:18:29,330 --> 00:18:32,700
all right well there's no right

294
00:18:36,910 --> 00:18:38,780
i thought it was like this

295
00:18:38,810 --> 00:18:44,490
let me skip to this one for

296
00:18:44,500 --> 00:18:45,860
we did this

297
00:18:45,910 --> 00:18:49,350
should change which was to survey

298
00:18:50,070 --> 00:18:51,360
how to solve

299
00:18:51,680 --> 00:18:58,910
well there's always this

300
00:19:01,300 --> 00:19:04,140
i had a good

301
00:19:04,160 --> 00:19:07,030
all right

302
00:19:07,030 --> 00:19:08,760
the company would been persuaded

303
00:19:10,190 --> 00:19:12,090
maybe we would've killed enough people

304
00:19:12,650 --> 00:19:15,900
that they would have been persuaded not to run this next trial

305
00:19:16,820 --> 00:19:21,510
so it's it's a very dicey question i mean and make it is as simple as

306
00:19:22,150 --> 00:19:22,730
i perhaps

307
00:19:24,230 --> 00:19:25,030
but but

308
00:19:25,690 --> 00:19:30,690
frank giles was persuaded i see no rationale to further delay moving to these designs

309
00:19:31,780 --> 00:19:35,760
said doctor giles who is currently involved in eight bayesian based

310
00:19:36,360 --> 00:19:38,460
leukemia studies they are more ethical

311
00:19:39,670 --> 00:19:40,820
more patient-friendly

312
00:19:41,460 --> 00:19:45,170
more conserving resources and more statistically desirable

313
00:19:49,280 --> 00:19:51,110
my quest talked about simulations

314
00:19:51,800 --> 00:19:55,340
um christian talked about simulations

315
00:19:56,070 --> 00:19:56,740
we do this

316
00:19:58,380 --> 00:19:59,940
simulations out the wazoo

317
00:20:00,590 --> 00:20:01,610
you're gonna hate this

318
00:20:02,230 --> 00:20:05,550
but one of the things that we do is we calculate type one error

319
00:20:06,150 --> 00:20:08,840
so we can we have a very complicated design

320
00:20:10,070 --> 00:20:11,460
but we generate

321
00:20:11,880 --> 00:20:13,550
under the null hypothesis

322
00:20:14,230 --> 00:20:16,070
to show what the type one error rate is

323
00:20:16,610 --> 00:20:20,920
and sometimes if this is the registration trial-to-trial it's gonna go the fifty eight

324
00:20:21,340 --> 00:20:23,280
we have to modify the design

325
00:20:24,710 --> 00:20:29,110
get the type one error rate less than point of o two five

326
00:20:30,130 --> 00:20:31,260
what that means is

327
00:20:32,260 --> 00:20:36,610
that we've gone through this bayesian machinery we built this wonderful clever design

328
00:20:37,030 --> 00:20:39,110
but then we made it frequentist

329
00:20:43,260 --> 00:20:45,550
so other things as well

330
00:20:47,150 --> 00:20:47,760
to recent

331
00:20:48,710 --> 00:20:49,530
get rid of it

332
00:20:50,860 --> 00:20:51,190
what is it

333
00:20:52,880 --> 00:20:54,400
okay i

334
00:20:54,420 --> 00:20:56,030
two recent clinical trials

335
00:20:56,490 --> 00:20:58,320
with smaller sample size

336
00:20:59,840 --> 00:21:03,630
one year they both use bayesian predictive probabilities actually

337
00:21:04,650 --> 00:21:06,630
and they both use longitudinal modeling

338
00:21:07,490 --> 00:21:09,590
but just to high profile

339
00:21:11,460 --> 00:21:12,340
bayesian trials

340
00:21:13,010 --> 00:21:14,690
this one again published in jama

341
00:21:15,460 --> 00:21:17,360
journal american medical association

342
00:21:18,090 --> 00:21:18,880
ah bee

343
00:21:20,420 --> 00:21:24,150
abstract says a prospective multi-center randomized to one

344
00:21:24,740 --> 00:21:26,710
unblinded bayesian design

345
00:21:27,420 --> 00:21:31,340
study conducted in nineteen hospitals one hundred sixty seven patients

346
00:21:32,150 --> 00:21:32,940
so this was

347
00:21:33,440 --> 00:21:36,210
about a third less than the maximum

348
00:21:36,840 --> 00:21:38,210
the company approached us

349
00:21:39,740 --> 00:21:41,150
because the crew was slow

350
00:21:41,990 --> 00:21:46,210
anne it was take forever that for them to finish the trial

351
00:21:46,840 --> 00:21:48,780
could we converted into a bayesian

352
00:21:49,300 --> 00:21:52,920
trial and we did and we did longitudinal modelling

353
00:21:53,740 --> 00:21:55,170
together early information

354
00:21:55,740 --> 00:22:02,010
and we did predictive probabilities such that if we predicted that he it's always going to be clear

355
00:22:02,900 --> 00:22:06,380
with additional follow-up we stop study at that point

356
00:22:09,090 --> 00:22:09,530
and we get

357
00:22:10,880 --> 00:22:11,960
submitted it to

358
00:22:12,530 --> 00:22:13,300
this is the journal

359
00:22:14,210 --> 00:22:15,490
and the journal said

360
00:22:17,710 --> 00:22:19,150
did a bayesian approach

361
00:22:19,880 --> 00:22:22,260
you've heard the word bayesian in the abstract

362
00:22:22,710 --> 00:22:25,570
we don't use the word bayesian are abstracts

363
00:22:28,650 --> 00:22:29,990
now you can imagine

364
00:22:31,190 --> 00:22:32,670
how i was

365
00:22:35,320 --> 00:22:38,050
and i said maybe you don't understand

366
00:22:38,800 --> 00:22:42,460
this trial was designed by a from a bayesian perspective

367
00:22:43,440 --> 00:22:45,460
the protocol was bayesian

368
00:22:48,550 --> 00:22:52,510
it was designed in coordination with the e fifty eight

369
00:22:53,300 --> 00:22:55,050
it was approved for marketing

370
00:22:55,550 --> 00:22:58,780
by the year after year in the basisof an expert panel

371
00:23:01,470 --> 00:23:04,050
u have an obligation as a medical

372
00:23:06,610 --> 00:23:10,570
to adhere to the bible medicine which is the protocol

373
00:23:11,740 --> 00:23:13,300
therefore you must use

374
00:23:13,820 --> 00:23:16,510
the bayesian approach in in the calculations

375
00:23:16,940 --> 00:23:18,110
in abstract

376
00:23:19,900 --> 00:23:21,970
because i thought i was whistling dixie that's

377
00:23:23,010 --> 00:23:24,740
i thought it was you know what

378
00:23:25,440 --> 00:23:27,320
i embarrassed enough so that there is

379
00:23:28,610 --> 00:23:33,070
jamie used the word bayesian and also the primary analysis are all bases

380
00:23:33,630 --> 00:23:38,360
well we did they did ask us to say what would be analog in the frequentist be

381
00:23:39,710 --> 00:23:42,960
so we can communicate better to our audience

382
00:23:43,490 --> 00:23:47,820
so we did have some analogs but you know going in the other direction taking

383
00:23:47,820 --> 00:23:51,570
the data and doing a bayesian approach if the data were collected as a frequent

384
00:23:51,570 --> 00:23:52,380
this is easy

385
00:23:52,840 --> 00:23:54,690
going in the other direction is not so easy

386
00:23:55,490 --> 00:23:56,280
because you don't know

387
00:23:56,740 --> 00:23:57,110
what they

388
00:23:58,900 --> 00:24:04,970
you know what the conference what he actual design the experiment was it was incorporated into the

389
00:24:06,030 --> 00:24:08,780
frequentist approach so we did nominal things

390
00:24:10,740 --> 00:24:13,550
don't i mentioned oncology perhaps the hardest

391
00:24:16,170 --> 00:24:19,190
in oncology to study using an adaptive

392
00:24:22,650 --> 00:24:25,530
early stage breast cancer and elderly women

393
00:24:26,130 --> 00:24:27,760
because it tends to be

394
00:24:28,530 --> 00:24:32,130
disease that innocuous in fact most if it is over diagnosed

395
00:24:32,800 --> 00:24:33,690
patient would never

396
00:24:36,210 --> 00:24:40,710
find out that she had the disease if you didn't do mammograms and sort harkened back

397
00:24:43,090 --> 00:24:45,710
andy so these events are not very long in coming

398
00:24:46,590 --> 00:24:48,050
i mean very long in coming

399
00:24:49,900 --> 00:24:53,090
it took me a while to build this design because

400
00:24:53,670 --> 00:24:56,090
these and see i said you've got to do

401
00:24:57,110 --> 00:24:59,210
trial with two thousand patients

402
00:25:00,170 --> 00:25:04,070
and i said we don't have two thousand patients will never finish the trial

403
00:25:04,490 --> 00:25:09,320
if we have two to two thousand patients there was a long time and i finally had this inspiration

404
00:25:09,860 --> 00:25:11,670
that i s okay his will do

405
00:25:12,210 --> 00:25:13,800
we've got a six hundred patients

406
00:25:14,740 --> 00:25:17,530
we'll do bayesian predictive calculations

407
00:25:18,670 --> 00:25:20,510
if with additional follow-up

408
00:25:21,240 --> 00:25:23,650
the probability of having a conclusion

409
00:25:24,820 --> 00:25:25,360
yeah nay

410
00:25:26,710 --> 00:25:30,320
in terms of these objective the trial

411
00:25:30,970 --> 00:25:31,840
will stop the cruel

412
00:25:32,320 --> 00:25:34,760
will follow the patients for the period time

413
00:25:35,230 --> 00:25:37,240
and then published the results and

414
00:25:37,990 --> 00:25:42,280
so they finally agreed that we could do that and so in clinical trials covered

415
00:25:42,280 --> 00:25:47,550
said under sample size it said this is usually look it says it's a two

416
00:25:47,550 --> 00:25:48,550
thousand patient trial

417
00:25:48,970 --> 00:25:51,840
one other ones five hundred patient trial this one said

418
00:25:53,050 --> 00:25:56,110
the sample size is six hundred ten eighteen hundred

419
00:25:58,570 --> 00:26:01,650
surprise when we looked at six six hundred

420
00:26:02,190 --> 00:26:06,070
the bayesian predictive calculation was such that we were going have an answer

421
00:26:06,570 --> 00:26:08,230
we stopped the crows six hundred

422
00:26:08,650 --> 00:26:11,340
final total was six hundred thirty six i think

423
00:26:12,550 --> 00:26:15,470
we followed additional two years and published in

424
00:26:17,320 --> 00:26:19,440
the new england journal positive results

425
00:26:23,730 --> 00:26:24,460
are to have

426
00:26:25,460 --> 00:26:26,730
to call on my favourite

427
00:26:26,730 --> 00:26:32,480
well you

428
00:26:32,560 --> 00:26:39,980
i think all right

429
00:26:55,060 --> 00:26:56,930
around it

430
00:27:33,800 --> 00:27:41,950
here we go

431
00:27:42,000 --> 00:27:47,450
and this

432
00:27:47,450 --> 00:27:51,390
no there are no

433
00:27:57,890 --> 00:27:59,880
here we will

434
00:28:15,610 --> 00:28:17,820
how right

435
00:28:35,090 --> 00:28:40,030
it was very small you know

436
00:28:40,050 --> 00:28:42,200
in fact

437
00:28:42,220 --> 00:28:46,760
was the very

438
00:29:03,860 --> 00:29:07,990
i have

439
00:29:25,070 --> 00:29:35,220
show me

440
00:29:52,590 --> 00:29:59,660
are you

441
00:30:16,930 --> 00:30:19,430
of my life

442
00:30:41,220 --> 00:30:45,380
because we want go

443
00:30:48,470 --> 00:30:53,430
the term

444
00:31:04,530 --> 00:31:07,090
from is

445
00:31:37,660 --> 00:31:39,950
right so

446
00:31:39,950 --> 00:31:44,300
the posterior you evaluated first and use that

447
00:31:44,340 --> 00:31:48,730
as it were to mix over the predicted distributions given theta

448
00:31:48,760 --> 00:31:50,610
and you get exactly the same answer

449
00:31:52,950 --> 00:31:56,910
so the idea of bayesian statistics is

450
00:31:56,950 --> 00:32:04,910
is that magnified is to use we're gonna use probability theory consistently to evaluate uncertainty in in all sorts of

451
00:32:04,910 --> 00:32:08,910
situations we're we're interested in the given data

452
00:32:08,950 --> 00:32:15,300
and we're now to do this because the bayesian view is everything is a random variable

453
00:32:16,470 --> 00:32:23,930
every possible variable in your system observed or unbserved observable unobservable things that are you think

454
00:32:23,930 --> 00:32:30,890
are parameters things that states of nature thing you know everything hidden visible missing

455
00:32:30,930 --> 00:32:33,540
whatever it is they're all random variables

456
00:32:36,560 --> 00:32:38,130
now that's not to say that

457
00:32:39,430 --> 00:32:43,630
bayesians think they're all the same philosophically they don't they understand those are different

458
00:32:44,540 --> 00:32:47,110
the future's different from the past bla bla bla

459
00:32:47,110 --> 00:32:53,800
but they're just treated uniformly for mathematical purposes so we treat them as random variables

460
00:32:54,370 --> 00:33:00,740
and it's just quickly is worth reminding ourselves that there's different sorts of randomness and indeed

461
00:33:00,820 --> 00:33:05,240
the distinctions can be can can get quite blurred so

462
00:33:05,280 --> 00:33:09,820
we sometimes distinguish between epistemological randomness and aleatory randomness

463
00:33:11,010 --> 00:33:16,350
and the first of those is concerned with lack of knowledge things you don't know

464
00:33:16,350 --> 00:33:20,570
and the second is to do with things that are inherently random

465
00:33:21,230 --> 00:33:22,820
so I'm

466
00:33:22,870 --> 00:33:28,390
indebted to my friend David Spiegelhalter how can I of this

467
00:33:28,430 --> 00:33:32,690
and a problem is that you're euro coins don't have

468
00:33:35,030 --> 00:33:38,410
don't have heads on them but

469
00:33:38,610 --> 00:33:45,320
one of the several things wrong with euro OK I'll jump ahead to the

470
00:33:45,330 --> 00:33:47,840
second step so

471
00:33:48,020 --> 00:33:54,370
I I'm going to I'll hold up a coin imagine I haven't thrown it I'll hold it up

472
00:33:54,380 --> 00:33:59,170
and I ask the audience what's the chance that this

473
00:33:59,170 --> 00:34:02,520
will come up heads

474
00:34:02,580 --> 00:34:04,080
OK what's the answer

475
00:34:08,280 --> 00:34:12,670
we're not worried about silly things like coin being biased

476
00:34:12,800 --> 00:34:18,480
and we're not worried about euros not having heads I then toss the coin flip it on the back

477
00:34:18,480 --> 00:34:20,670
of my hand and I don't reveal it

478
00:34:20,670 --> 00:34:22,060
I don't look at it or anything

479
00:34:23,240 --> 00:34:29,410
I need to flip that I now say what's the probability that this is heads

480
00:34:31,310 --> 00:34:39,660
OK so you're not quite nobody nobody's saying anything different from the half yet but you're a bit

481
00:34:39,660 --> 00:34:42,130
a bit slower to say so

482
00:34:43,290 --> 00:34:50,540
now I look at it OK and I know now what's the what's the chance

483
00:34:56,110 --> 00:35:01,730
yeah OK I know it my chances are now zero or one what are

484
00:35:01,780 --> 00:35:04,580
your chances

485
00:35:06,670 --> 00:35:11,650
so something you know uncertainties are moved around

486
00:35:11,670 --> 00:35:13,910
you are still getting fifty fifty I think

487
00:35:14,150 --> 00:35:16,800
they take you longer to think to say so but

488
00:35:16,820 --> 00:35:19,630
you know you don't you still don't have any evidence for one or the other

489
00:35:20,630 --> 00:35:24,630
but something rather strange has happened because it's no longer random

490
00:35:25,950 --> 00:35:30,650
what's happened is we've moved from it was tale by the way we've moved from something that was purely

491
00:35:30,650 --> 00:35:35,410
aleatory it was determined by something in the future that was genuinely chance to

492
00:35:35,410 --> 00:35:37,340
something that

493
00:35:37,400 --> 00:35:41,060
is only epistemological it's just to do with something you don't know

494
00:35:41,110 --> 00:35:45,060
and you and I think differently about it you know you thought at certain stage in

495
00:35:45,060 --> 00:35:48,950
that experiment you had one view and I'd another so is in the

496
00:35:48,950 --> 00:35:52,190
eyes of the beholder and so on

497
00:35:52,230 --> 00:35:57,370
so the the the uncertainty can be of different kinds but we can

498
00:35:57,370 --> 00:36:00,150
use the laws of probability to talk about both kinds

499
00:36:02,480 --> 00:36:06,430
the different kinds can be perceived differently by different by different observers

500
00:36:07,230 --> 00:36:12,090
so bayesians understand all that but they're still willing to use probability

501
00:36:12,090 --> 00:36:16,590
for both purposes they use it uniformly and treat everything as a random variable

502
00:36:16,690 --> 00:36:22,450
so the idea is to work throughout with the joint probability distribution you

503
00:36:24,300 --> 00:36:29,340
you you put together in one huge model the joint distribution of everything you don't

504
00:36:29,340 --> 00:36:36,730
know and everything you will know whatever their status where does that come from well it's not defined

505
00:36:36,730 --> 00:36:40,230
it's not there's not a right answer it's something that's assessed you you

506
00:36:40,230 --> 00:36:44,910
come to a scientific judgment about it you'll since

507
00:36:44,970 --> 00:36:48,890
the result has to obey the laws of probability you will be guided by

508
00:36:48,890 --> 00:36:50,960
the laws of probability in doing that

509
00:36:51,610 --> 00:36:54,890
and then at some point we have some data you know we looked at the

510
00:36:54,890 --> 00:37:00,650
coin or whatever it was and we model that process of of observation

511
00:37:00,650 --> 00:37:04,230
by conditioning this joint distribution on the things we've observed

512
00:37:05,690 --> 00:37:12,240
and we than use that condition distribution for everything else we want to know for our inference for our

513
00:37:12,240 --> 00:37:17,980
predictions for our decisions and so on the probability distribution of the hidden variables given

514
00:37:18,000 --> 00:37:22,170
given the visible ones so it's completely clean and

515
00:37:22,240 --> 00:37:26,980
or let's say straightforward is philosophically straightforward but

516
00:37:27,020 --> 00:37:31,210
you know it's not there's no particularly difficult philosophical ideas at that level

517
00:37:32,210 --> 00:37:35,030
now I've got quite a long way

518
00:37:35,520 --> 00:37:39,470
without mentioning likelihoods and priors don't think

519
00:37:39,480 --> 00:37:44,480
and in a sense likelihoods and priors are a little bit

520
00:37:45,530 --> 00:37:45,910
you know

521
00:37:46,040 --> 00:37:49,890
they're not really the the front rank of the theory the the main thing we

522
00:37:49,890 --> 00:37:53,040
need is this is this joint distribution

523
00:37:53,340 --> 00:37:55,680
of observables and unobservables

524
00:37:57,300 --> 00:38:02,470
but typically of course that Joint Distribution comes about by combining likelihood and prior

525
00:38:02,470 --> 00:38:05,730
so the standard situation a sort of vanilla

526
00:38:06,910 --> 00:38:13,650
is you got a parameter vectors theta you got some data Y you would have some numbers

527
00:38:13,650 --> 00:38:18,520
and there are the two theta and Y they may be big vectors but they're just two objects we care about

528
00:38:18,630 --> 00:38:22,540
and we gotta make inference about theta given Y

529
00:38:23,210 --> 00:38:28,170
and as I've already said we gonna use we gonna use the conditional distribution of

530
00:38:28,170 --> 00:38:32,040
theta given Y to make to make that inference and the key thing of course

531
00:38:32,040 --> 00:38:33,210
is that

532
00:38:33,230 --> 00:38:37,610
it's the joint distribution of theta and Y divided by the marginal distribution of Y

533
00:38:37,910 --> 00:38:42,000
and that's as far as theta's concerned is proportional in theta

534
00:38:42,020 --> 00:38:47,300
so the conditional distiribution we want to use for inference is just proportional to the joint distirbution

535
00:38:47,300 --> 00:38:49,170
that you

536
00:38:49,170 --> 00:38:50,240
decided on

537
00:38:53,370 --> 00:38:54,540
and and

538
00:38:54,630 --> 00:38:58,020
you know so we could just start with the joint distribution but but in fact we almost always

539
00:38:58,020 --> 00:39:01,930
construct that joint distribution by combining the likelihood and the prior so so it

540
00:39:01,930 --> 00:39:03,650
is it it is

541
00:39:03,650 --> 00:39:14,790
considerable control or particle size means precipitation my conclusions in such rhetoric from the Pacific

542
00:39:14,790 --> 00:39:22,730
nation of 5 sites where you find drop its effect is facing death and surfactants

543
00:39:22,730 --> 00:39:24,770
so worst license

544
00:39:24,810 --> 00:39:34,590
which are distributed in place so chemical reactions are limited to small specialty license and

545
00:39:34,630 --> 00:39:40,450
the size of the property plotting yeah depends primarily on the size of the monsters

546
00:39:40,450 --> 00:39:51,300
who just wanted to determine the particular water to affect the size of the uses

547
00:39:51,300 --> 00:39:55,410
just changing my conclusion nation bearing

548
00:39:55,450 --> 00:40:01,270
traditional reactions diesel

549
00:40:01,310 --> 00:40:04,390
so by changing market

550
00:40:04,430 --> 00:40:14,350
the composition and temperature of serious novel often plays a variety of different sizes and

551
00:40:14,420 --> 00:40:17,630
that they don't want to go

552
00:40:17,670 --> 00:40:27,650
With sizes approximately between on the parliamentary bloc party has the support Parliament met however

553
00:40:27,730 --> 00:40:35,230
articles in general had no monetization so much lower to what we would produce a

554
00:40:35,350 --> 00:40:44,430
take it to consideration only suggesting that the articles have different structures to what seemed

555
00:40:46,850 --> 00:40:58,930
on high-resolution shown with crystalline Lionel particles of course thinking considerations that sell barometer of

556
00:40:58,930 --> 00:41:03,950
spina Vichy surrounds you . 8 not only does it is difficult to walk talk

557
00:41:03,950 --> 00:41:14,990
about recently the smallest number of articles for each the shell slice one-and-a-half not only

558
00:41:14,990 --> 00:41:25,990
does justice to set parameters approximately 1000 km such articles on the use of high-resolution

559
00:41:25,990 --> 00:41:28,190
mutual most of us

560
00:41:28,210 --> 00:41:31,610
material follow own distinct areas

561
00:41:31,650 --> 00:41:37,180
regular reviews so to individual

562
00:41:37,630 --> 00:41:41,950
PDS really you know what the show

563
00:41:42,110 --> 00:41:50,730
the presence of all Wilfried constituting silencing approximately the relationship probes the corresponds to starting

564
00:41:50,730 --> 00:41:58,810
position so just a warning to the electorate prosecutors would not say why somewhere where

565
00:41:58,830 --> 00:42:07,170
properties so had to go a step further by using X-ray absorption techniques like scenes

566
00:42:07,170 --> 00:42:18,330
of Matisse's show variations manganese they achieve a suggesting that

567
00:42:18,410 --> 00:42:24,870
the latest average dependency of manganese and ceramics found to be 2 points that

568
00:42:24,910 --> 00:42:31,030
I'm not partly because the disease was estimated to be the last for the largest

569
00:42:31,030 --> 00:42:39,380
state-mandated sized worldwide and on 5 and 3 bus water on a small amount of

570
00:42:39,410 --> 00:42:47,090
articles suggesting combination of manganese free bus in many ways for lots cooperation of lenders

571
00:42:47,090 --> 00:42:52,040
full blast into the speed spiniferus structures and Marion you would want to use bop

572
00:42:52,040 --> 00:42:57,010
material so the structure

573
00:42:58,390 --> 00:43:04,570
the structure of particles have been started using excess and its views

574
00:43:04,650 --> 00:43:16,150
the new distribution of next at a restaurant in while we would hear the suspected

575
00:43:17,390 --> 00:43:29,550
the it reminded me of the firearms in the summer of the was like that

576
00:43:29,550 --> 00:43:39,290
its oxygen at the center of the next 2 weeks and the desert is favored

577
00:43:39,290 --> 00:43:44,610
to those use of

578
00:43:44,800 --> 00:43:58,670
function of the site where they cooperated with the marked a significant recuperation of selected

579
00:43:58,670 --> 00:44:05,670
decides he wants to use the operation

580
00:44:05,970 --> 00:44:12,530
but if we compare the of Bob

581
00:44:14,190 --> 00:44:22,190
talking we could see a decrease in the on a peak was formed to stick

582
00:44:22,190 --> 00:44:24,080
and how do you choose that

583
00:44:24,080 --> 00:44:28,020
a large number but the larger is the worse it gets

584
00:44:28,430 --> 00:44:32,290
let me first rate algorithm analysis should be obvious why it is the case

585
00:44:32,340 --> 00:44:34,570
i draw sample

586
00:44:34,630 --> 00:44:37,570
both sides a bit low

587
00:44:37,730 --> 00:44:41,620
if we could just raise it for the time being

588
00:44:41,630 --> 00:44:52,550
i draw samples from q effect

589
00:44:52,590 --> 00:44:58,180
from the blue distribution is sample number from gauss in distribution so that is just

590
00:44:58,180 --> 00:45:00,130
excise it got around then

591
00:45:00,180 --> 00:45:01,850
one common one

592
00:45:01,850 --> 00:45:04,050
and then you multiply that by hand

593
00:45:07,270 --> 00:45:10,740
then what i drew that sample

594
00:45:10,860 --> 00:45:14,150
i'm going to think i'm going to take an interval

595
00:45:15,890 --> 00:45:17,620
the zero value

596
00:45:17,630 --> 00:45:22,370
and the value of the function and time timescale effects

597
00:45:22,380 --> 00:45:27,220
so this hide here

598
00:45:27,320 --> 00:45:29,190
would be and

599
00:45:30,850 --> 00:45:33,230
of x

600
00:45:33,290 --> 00:45:37,600
so you have to evaluate we basically evaluating the cows in at that point

601
00:45:38,650 --> 00:45:40,060
but you can evaluate

602
00:45:40,060 --> 00:45:41,830
q effects

603
00:45:42,030 --> 00:45:47,380
in some problems in econometrics that's not true you sometimes you can't evaluate the topic

604
00:45:48,890 --> 00:45:51,590
in machine learning often that's true

605
00:45:52,460 --> 00:45:57,850
you have that we draw a random number uniformly between here and here

606
00:45:57,890 --> 00:46:02,000
the way you do that is you just wrote a number between zero and one

607
00:46:02,050 --> 00:46:03,520
ex aequo

608
00:46:05,000 --> 00:46:08,070
and then you must then you evaluate

609
00:46:08,100 --> 00:46:09,640
and the scale factor

610
00:46:09,660 --> 00:46:14,280
and then you multiply that times that number that you should number there

611
00:46:14,320 --> 00:46:16,810
and the vertical axis

612
00:46:16,830 --> 00:46:22,330
if the number is above the red line

613
00:46:22,370 --> 00:46:26,070
if you is about the red line you throw it away

614
00:46:26,080 --> 00:46:29,820
if it's below you accept it

615
00:46:29,830 --> 00:46:31,440
so that when we throw away

616
00:46:31,470 --> 00:46:32,780
we pick another one

617
00:46:32,780 --> 00:46:34,040
o it's inside

618
00:46:34,050 --> 00:46:35,730
so we accept it

619
00:46:35,790 --> 00:46:38,960
o it's inside we accept it if you keep doing that

620
00:46:38,990 --> 00:46:40,940
it's sort of should work

621
00:46:42,880 --> 00:46:44,270
where this is high

622
00:46:44,270 --> 00:46:47,080
be high probability of these guys being accepted

623
00:46:47,120 --> 00:46:51,040
as long as i have a sensible q effects

624
00:46:51,320 --> 00:46:58,630
and i'm going to give you

625
00:46:58,670 --> 00:47:08,290
should a demo of that

626
00:47:08,300 --> 00:47:27,300
so in matlab

627
00:47:27,640 --> 00:47:31,600
you need to specify the number of iterations

628
00:47:33,220 --> 00:47:34,980
god this was long time ago

629
00:47:34,990 --> 00:47:38,710
the mean the number of bands the mean and the variance is if the proposal

630
00:47:38,710 --> 00:47:46,100
distribution and so on but let me move to the algorithm

631
00:47:46,200 --> 00:47:48,310
again the algorithm is just this

632
00:47:48,310 --> 00:47:52,850
you generate a random number between zero and one that to you

633
00:47:52,860 --> 00:47:56,290
you generate this proposal distribution from q

634
00:47:56,310 --> 00:47:58,100
from the gods and distribution

635
00:47:58,110 --> 00:48:01,160
q has mean q various sq

636
00:48:01,220 --> 00:48:04,630
and this is just agustin number

637
00:48:04,730 --> 00:48:07,650
the from gauss and distribution

638
00:48:07,700 --> 00:48:13,000
you evaluate the target distribution we usually talk about the distribution of interest is the

639
00:48:13,000 --> 00:48:14,580
target distribution

640
00:48:14,580 --> 00:48:16,230
the target is just

641
00:48:16,600 --> 00:48:19,600
by model gauss and distribution

642
00:48:19,640 --> 00:48:22,140
here it's got some distribution

643
00:48:22,160 --> 00:48:24,980
with the mean it's zero with white point three

644
00:48:24,980 --> 00:48:27,770
and then there's another cousin distribution

645
00:48:28,080 --> 00:48:30,370
with mean ten

646
00:48:30,380 --> 00:48:34,880
and white zero point seven so it's sort of a bimodal distribution just like an

647
00:48:34,900 --> 00:48:37,140
example is on

648
00:48:37,140 --> 00:48:41,920
the proposal a single galaxy which has mean and q so these match

649
00:48:41,940 --> 00:48:45,150
it has variance is q

650
00:48:45,150 --> 00:48:48,850
standard deviation q

651
00:48:48,900 --> 00:48:52,250
and then what we do is we evaluate

652
00:48:52,310 --> 00:48:57,710
the effects of the and q effects like i had before

653
00:48:57,750 --> 00:48:59,420
the acceptance

654
00:48:59,440 --> 00:49:04,170
one e to the minus nineteen nineties because unfortunately computers are discrete device and continues

655
00:49:04,190 --> 00:49:07,120
devices they have to deal with condition

656
00:49:07,120 --> 00:49:10,650
and then if

657
00:49:10,650 --> 00:49:13,060
if you is less than alpha

658
00:49:13,120 --> 00:49:15,850
in other words if you is less than that height

659
00:49:15,900 --> 00:49:17,040
you accept

660
00:49:17,100 --> 00:49:18,520
if it's not

661
00:49:18,520 --> 00:49:19,730
you throw it away

662
00:49:19,730 --> 00:49:25,900
this ensures that it's between the blue line and the red line in the what

663
00:49:26,120 --> 00:49:28,920
if you rejected do nothing

664
00:49:28,940 --> 00:49:35,440
and then the rest is just a lot

665
00:49:35,480 --> 00:49:43,940
and if you run this

666
00:49:43,980 --> 00:49:47,690
you draw sample that's not a good one

667
00:49:47,750 --> 00:49:51,120
i actually that's a good one that's below

668
00:49:51,170 --> 00:49:55,190
on the first you pick on the x axis then you pick a uniform number

669
00:49:55,210 --> 00:49:57,250
you rejected it

670
00:49:57,270 --> 00:49:59,810
and then that when you accept

671
00:49:59,830 --> 00:50:04,870
so now you've got the approximation of the posterior distribution

672
00:50:04,920 --> 00:50:09,650
the target distribution the posterior distribution is bimodal thing

673
00:50:09,690 --> 00:50:11,480
and you have access the right thing

674
00:50:11,500 --> 00:50:15,900
and this is the approximation so with two samples we have agreed creamy lousy approximation

675
00:50:15,900 --> 00:50:17,120
of it

676
00:50:17,680 --> 00:50:25,600
and then you continue doing this thing that that that that that that that

677
00:50:30,060 --> 00:50:35,100
just OK i'm doing this on purpose his rejection sampling is inefficient

678
00:50:37,310 --> 00:50:51,620
and if comparing the actions that were

679
00:50:51,690 --> 00:50:55,350
slightly faster and see it

680
00:50:55,420 --> 00:51:00,370
we help before them it away

681
00:51:00,420 --> 00:51:01,690
bit by bit

682
00:51:01,690 --> 00:51:04,640
in the magic works

683
00:51:04,690 --> 00:51:07,100
and you get an approximation

684
00:51:07,210 --> 00:51:10,960
for one hundred samples starts looking a lot better

685
00:51:11,000 --> 00:51:12,560
starts looking like a

686
00:51:12,710 --> 00:51:16,060
good approximation of the posterior distribution

687
00:51:16,100 --> 00:51:25,730
why is this bad

688
00:51:25,850 --> 00:51:28,980
for cooking

689
00:51:29,020 --> 00:51:38,770
what's the obvious problem with this approach

690
00:51:38,770 --> 00:51:42,800
OK for maybe just enough product spaces may

691
00:51:42,930 --> 00:51:45,510
did this last week OK

692
00:51:45,520 --> 00:51:48,640
so if you have some vector space x

693
00:51:48,670 --> 00:51:50,410
for instance of those areas

694
00:51:50,420 --> 00:51:57,680
is it is an inner product space is that these two real values symmetry two

695
00:51:58,490 --> 00:52:02,670
function exists his gets

696
00:52:02,720 --> 00:52:05,980
that satisfies

697
00:52:07,340 --> 00:52:09,160
scally x

698
00:52:10,170 --> 00:52:13,260
always because it is only

699
00:52:13,270 --> 00:52:16,670
and if you have

700
00:52:16,730 --> 00:52:20,530
the enough project is treat if you have this property

701
00:52:20,580 --> 00:52:24,310
this is the scalar product of x and it's equals zero if and only if

702
00:52:24,310 --> 00:52:26,320
x against could

703
00:52:26,320 --> 00:52:32,330
and the once you have an idea for that so symmetric bilinear

704
00:52:33,360 --> 00:52:35,060
so which

705
00:52:35,070 --> 00:52:40,290
check which satisfy parity to two

706
00:52:40,480 --> 00:52:42,560
that's because it

707
00:52:42,570 --> 00:52:44,350
dimension the two

708
00:52:45,880 --> 00:52:49,930
then you can also decided on which is this is

709
00:52:50,520 --> 00:52:54,280
square root of this kind of product of x and x OK this is the

710
00:52:54,300 --> 00:52:58,160
classic definition of inner product space

711
00:52:58,180 --> 00:53:01,810
but then fact is it is more than that

712
00:53:01,810 --> 00:53:08,310
and despite its strict inner product space itself is was kind of a product of

713
00:53:08,330 --> 00:53:09,470
x by x

714
00:53:09,490 --> 00:53:12,330
which is zero then you have

715
00:53:12,340 --> 00:53:15,610
you have the property of x equals zero

716
00:53:15,680 --> 00:53:18,670
but more you the political properties

717
00:53:18,680 --> 00:53:25,030
so to full of topological properties one is separated to is completeness

718
00:53:25,070 --> 00:53:29,360
so completeness we suggest the following property

719
00:53:30,030 --> 00:53:31,570
if you have some

720
00:53:31,580 --> 00:53:32,990
caution sequence

721
00:53:33,000 --> 00:53:36,070
so remember because she sequence is such

722
00:53:37,110 --> 00:53:38,830
you have

723
00:53:38,840 --> 00:53:40,360
if you take two

724
00:53:41,960 --> 00:53:44,460
so test is to take you

725
00:53:44,480 --> 00:53:48,490
the difference between two

726
00:53:48,500 --> 00:53:51,250
the value of fuel course sequence

727
00:53:51,300 --> 00:53:54,000
it takes norm you look at the

728
00:53:54,010 --> 00:53:56,040
this set

729
00:53:58,070 --> 00:54:04,900
so the maximum of this and if and then it goes to towards still when

730
00:54:04,900 --> 00:54:06,770
n goes to

731
00:54:06,780 --> 00:54:11,190
two at infinite speed that's the difference between two values

732
00:54:11,480 --> 00:54:13,510
this is the

733
00:54:13,530 --> 00:54:14,680
it is increasing

734
00:54:14,710 --> 00:54:21,750
the completeness research to the following facts should get some

735
00:54:21,750 --> 00:54:26,440
some courses sequence when it converges

736
00:54:26,530 --> 00:54:28,660
it converges in

737
00:54:28,710 --> 00:54:30,770
i suspect set h

738
00:54:30,780 --> 00:54:33,740
OK so if you have this completeness properties

739
00:54:33,790 --> 00:54:40,060
you have some cushy sequence it converges if it converges you it converges

740
00:54:40,990 --> 00:54:44,010
the intercept

741
00:54:44,010 --> 00:54:45,810
intersect match so here

742
00:54:45,820 --> 00:54:48,170
is this space because we have decided

743
00:54:49,480 --> 00:54:51,910
so you keep

744
00:54:52,020 --> 00:54:55,960
the cause the convergence to the limit it was

745
00:54:55,980 --> 00:54:57,100
so in that space

746
00:54:57,260 --> 00:55:03,020
more value as properly it's so severity was supposedly if there is a countable set

747
00:55:03,020 --> 00:55:09,270
of element h one h i if h is a space such search

748
00:55:09,270 --> 00:55:11,160
that for all h

749
00:55:11,180 --> 00:55:12,130
small h

750
00:55:12,150 --> 00:55:17,790
in which there is i so there is some elements here

751
00:55:17,790 --> 00:55:19,240
which is

752
00:55:19,260 --> 00:55:20,570
so sorry i forgot

753
00:55:23,570 --> 00:55:25,000
which is the

754
00:55:25,660 --> 00:55:31,360
which can be closed

755
00:55:31,380 --> 00:55:34,650
enough and good enough to pitch

756
00:55:35,800 --> 00:55:37,070
so if you are

757
00:55:39,210 --> 00:55:40,570
you can find

758
00:55:40,580 --> 00:55:46,810
sign the conclave and that's fine but the accountable set of elements in h such

759
00:55:47,510 --> 00:55:52,930
you can always fully function phi functions

760
00:55:52,990 --> 00:55:55,110
among these elements which is

761
00:55:56,300 --> 00:55:59,340
closed as much as possible so we have to

762
00:55:59,360 --> 00:56:03,530
the that's part of h then you have to provide

763
00:56:03,530 --> 00:56:05,030
OK so you must

764
00:56:05,040 --> 00:56:10,580
you must be careful that respects is not only the scalar product it also

765
00:56:10,610 --> 00:56:14,340
the political properties

766
00:56:14,360 --> 00:56:17,570
so i'm going back to my

767
00:56:17,580 --> 00:56:19,980
you that space

768
00:56:19,980 --> 00:56:28,240
so the one

769
00:56:29,630 --> 00:56:35,420
so we have this kind of for the and now

770
00:56:35,490 --> 00:56:38,470
we have this function in space

771
00:56:39,930 --> 00:56:40,990
in fact

772
00:56:41,000 --> 00:56:44,560
he look at the property of the space you will get

773
00:56:44,570 --> 00:56:46,510
an interesting properties

774
00:56:46,520 --> 00:56:49,300
which is called in fact were produced

775
00:56:51,070 --> 00:56:53,060
so what you are

776
00:56:53,100 --> 00:56:56,800
if you have a function f in HK

777
00:56:56,830 --> 00:56:59,010
at some point x

778
00:56:59,030 --> 00:57:01,010
in fact it

779
00:57:01,060 --> 00:57:05,320
it can be written as the scalar product of the function f

780
00:57:05,330 --> 00:57:07,180
we the function

781
00:57:09,330 --> 00:57:13,370
six to at some point as the same point x

782
00:57:13,390 --> 00:57:16,490
and especially

783
00:57:16,500 --> 00:57:21,190
for the can nuances or to work for this important

784
00:57:21,220 --> 00:57:23,530
if you want to compute the image

785
00:57:23,550 --> 00:57:25,690
also x x prime

786
00:57:25,720 --> 00:57:26,850
using k

787
00:57:26,870 --> 00:57:30,390
then you can use the scanner in

788
00:57:30,390 --> 00:57:34,030
so it's not really the theory we want but let's take a look at it

789
00:57:34,030 --> 00:57:35,870
for a moment

790
00:57:35,910 --> 00:57:37,040
now there is

791
00:57:37,060 --> 00:57:42,430
let's try to understand a bit more this correspondence so there's something that was noticed

792
00:57:42,430 --> 00:57:45,760
by a general call for a long time ago

793
00:57:45,760 --> 00:57:48,260
which is that if you have any theory

794
00:57:48,260 --> 00:57:53,310
whose interactions are meant to matrix like as in young means

795
00:57:53,350 --> 00:57:58,620
there is a very nice different way of reorganizing feynman diagrams by using what is

796
00:57:58,620 --> 00:58:01,490
called the double line of topological expansion

797
00:58:01,510 --> 00:58:02,740
so the idea

798
00:58:02,760 --> 00:58:07,240
it is the following all your particles say the young man gauge bosons

799
00:58:07,260 --> 00:58:09,310
can be thought of as

800
00:58:09,330 --> 00:58:12,390
little dipole source strings with two

801
00:58:12,410 --> 00:58:15,310
all the charges have the twins

802
00:58:15,370 --> 00:58:21,100
technically that's because they belong to the so-called joint representation and all the interactions are

803
00:58:21,100 --> 00:58:28,240
matrix multiplication like so here is a vertex four four gauge bosons here is a

804
00:58:28,240 --> 00:58:33,060
cubicle vertex for for you was on some stories here and there

805
00:58:33,060 --> 00:58:37,120
and now when you write your feynman diagrams instead of using

806
00:58:37,140 --> 00:58:38,740
g young means

807
00:58:38,760 --> 00:58:42,660
to organize the expansion into groups as we usually do

808
00:58:42,700 --> 00:58:47,740
you can try to use instead in times g squared young males this is called

809
00:58:47,740 --> 00:58:52,660
the top of the coupling parameter and the number of colors

810
00:58:52,700 --> 00:58:53,640
and you see

811
00:58:53,640 --> 00:58:59,100
that that is at each given order in and there is now of course series

812
00:58:59,100 --> 00:59:01,330
in powers of that

813
00:59:01,350 --> 00:59:02,800
but this

814
00:59:02,810 --> 00:59:06,220
diagrams that but given up fixed them out

815
00:59:06,410 --> 00:59:09,990
characterized by the fact that they can be thrown

816
00:59:10,030 --> 00:59:16,180
on the surface of fixed topology here is the simplest case no powers of n

817
00:59:16,200 --> 00:59:19,060
i can do all this diagram on this

818
00:59:19,080 --> 00:59:24,870
i can easily drawn from and its first director miro has alarmed before because you

819
00:59:24,870 --> 00:59:28,660
see that the article vertex that g square

820
00:59:28,680 --> 00:59:35,950
another g and g that she seeks another forty one g eight hundred forty one

821
00:59:37,240 --> 00:59:41,030
you know that is correct

822
00:59:41,030 --> 00:59:45,530
no g six g seven g eight country have three quartic and two cubic so

823
00:59:45,530 --> 00:59:47,260
that g eight

824
00:59:47,330 --> 00:59:50,200
so i have indeed g a

825
00:59:50,200 --> 00:59:55,160
a factor let's count the numbers of fan when the end comes from the number

826
00:59:55,160 --> 01:00:00,160
of colors that can be done in this little loops now there are four

827
01:00:00,180 --> 01:00:04,430
triangular loops here that's some into the four so that g eight and to the

828
01:00:04,430 --> 01:00:08,260
four and that's exactly happened to the forum there is no one's

829
01:00:08,280 --> 01:00:09,530
left of

830
01:00:09,580 --> 01:00:13,680
but if i try to write almost the same diagram but with

831
01:00:13,680 --> 01:00:17,240
these different running of the quark lines

832
01:00:17,310 --> 01:00:22,260
what you see this quiet trying now crosses over to the other side instead of

833
01:00:24,470 --> 01:00:27,760
try getting it game aged fifty eight

834
01:00:27,780 --> 01:00:30,640
but now if you count the number of

835
01:00:30,660 --> 01:00:33,990
loops here you will find that there is only one loop so there is only

836
01:00:33,990 --> 01:00:35,510
one hour

837
01:00:37,100 --> 01:00:42,890
and this gives you sorry if you are very short story one

838
01:00:42,930 --> 01:00:46,280
two QB two particle that this is that g

839
01:00:46,330 --> 01:00:50,350
therefore two cubic that's due to this six

840
01:00:50,390 --> 01:00:53,810
there is one power of and so the whole thing is due to the six

841
01:00:53,810 --> 01:00:56,640
and that's lambda cube over one

842
01:00:56,700 --> 01:00:58,930
and all squared

843
01:00:58,950 --> 01:01:01,930
and now this one about an inch where the

844
01:01:01,990 --> 01:01:06,800
is actually telling me that they cannot draw the surface any more on this i

845
01:01:06,800 --> 01:01:12,180
need the disk with handle because this thing has to go around handle otherwise it

846
01:01:12,180 --> 01:01:14,160
would intersect days

847
01:01:14,160 --> 01:01:15,760
so there is this

848
01:01:15,810 --> 01:01:22,370
different expansion of government theory which was not like of this actually makes it look

849
01:01:22,370 --> 01:01:26,760
a bit more like string theory

850
01:01:26,810 --> 01:01:31,800
so because of these theorists have long suspected that the large n limit of game

851
01:01:32,810 --> 01:01:35,530
is a weakly coupled string theory

852
01:01:36,220 --> 01:01:41,410
the APS if the correspondence has made this hypothesis very sharp now

853
01:01:41,430 --> 01:01:47,490
and here is a very precise but partial dictionary of how you would associate quantities

854
01:01:47,490 --> 01:01:49,080
from the two sides

855
01:01:49,100 --> 01:01:51,100
the power of copying lambda

856
01:01:51,100 --> 01:01:53,850
the and i cant create good enough

857
01:01:54,040 --> 01:01:58,910
function and binary think it's too long and it's the slow again

858
01:02:00,810 --> 01:02:03,270
and if you are really smart guy's

859
01:02:03,290 --> 01:02:06,690
what every programmer who are living in the

860
01:02:06,710 --> 01:02:11,350
twenty one century should do in this twenty first century should do it

861
01:02:11,410 --> 01:02:13,650
second there is this moment of time

862
01:02:13,670 --> 01:02:14,350
you should

863
01:02:14,370 --> 01:02:16,460
look for an open source

864
01:02:17,850 --> 01:02:21,290
that means you are the station

865
01:02:21,620 --> 01:02:27,330
there are a huge number of this project and that's good enough and that things

866
01:02:27,330 --> 01:02:31,600
that don't have any any other questions to discuss

867
01:02:32,600 --> 01:02:35,120
you can take the same as

868
01:02:35,170 --> 01:02:36,960
the most popular ones

869
01:02:36,980 --> 01:02:39,290
and you have four j

870
01:02:39,310 --> 01:02:42,140
this is a last

871
01:02:42,150 --> 01:02:43,770
i believe five

872
01:02:43,870 --> 01:02:47,870
and it does everything for you

873
01:02:49,690 --> 01:02:53,730
OK so at this moment in the game situation where

874
01:02:53,730 --> 01:02:55,000
we decided

875
01:02:55,060 --> 01:02:57,460
i can be a year

876
01:02:57,480 --> 01:02:59,920
we give up

877
01:02:59,940 --> 01:03:03,560
and you will take of this package

878
01:03:03,580 --> 01:03:07,500
and all of them are implemented inverted file

879
01:03:07,500 --> 01:03:12,500
all of them are implemented all this ranking algorithms and everything is like a series

880
01:03:12,500 --> 01:03:14,390
of this

881
01:03:16,060 --> 01:03:22,230
but if you are really interested in into in development of information retrieval systems

882
01:03:22,290 --> 01:03:25,790
and if you go

883
01:03:25,850 --> 01:03:31,100
one to look into the wreckage how they organize journal

884
01:03:31,150 --> 01:03:32,890
or you feel

885
01:03:32,910 --> 01:03:36,460
already using these packages and you want to understand

886
01:03:36,620 --> 01:03:40,650
scalability limits of this for example can i take it scene

887
01:03:40,940 --> 01:03:47,330
download the seen from watching or an index of the internet

888
01:03:47,350 --> 01:03:50,250
good i don't know we need to

889
01:03:50,270 --> 01:03:54,600
to look forward and to analyse policy to

890
01:03:54,620 --> 01:03:57,640
we need to develop rather so OK

891
01:03:57,650 --> 01:04:04,330
well all my people after this can stop listening to my lecture using this site

892
01:04:04,370 --> 01:04:07,230
using this powerful and very good too

893
01:04:08,710 --> 01:04:11,190
stupid and naive

894
01:04:11,210 --> 01:04:15,460
i think that you are going for going to implement some

895
01:04:15,520 --> 01:04:16,620
but before

896
01:04:16,640 --> 01:04:20,560
the next step of implementation we want to map

897
01:04:20,600 --> 01:04:22,230
this is very good

898
01:04:23,390 --> 01:04:25,670
if you are a developer

899
01:04:27,640 --> 01:04:30,670
you're going to implement some

900
01:04:30,690 --> 01:04:35,480
before implementing so during all implementation you should do evaluation

901
01:04:35,520 --> 01:04:37,080
in addition to that

902
01:04:37,140 --> 01:04:39,730
i'm not talking about boring unit

903
01:04:39,750 --> 01:04:41,750
that are also unavoidable

904
01:04:41,830 --> 01:04:44,690
but then talking about testing the whole system

905
01:04:46,190 --> 01:04:48,520
you don't know what is

906
01:04:48,540 --> 01:04:52,640
gordon what where we don't know in what direction your morning you need to

907
01:04:52,640 --> 01:04:54,870
how this time so you need some

908
01:04:54,910 --> 01:04:59,080
characteristics of your of your search engine

909
01:04:59,100 --> 01:05:05,690
and i don't know why but the main characteristics for the most people are church

910
01:05:05,750 --> 01:05:07,020
maybe because

911
01:05:07,080 --> 01:05:08,730
hard drives so cheap

912
01:05:09,730 --> 01:05:14,170
and people are not thinking about the the search let's start from

913
01:05:14,190 --> 01:05:18,640
is from speed if if it's so important for us

914
01:05:18,690 --> 01:05:19,790
so we have

915
01:05:19,790 --> 01:05:23,480
different size of collection

916
01:05:23,540 --> 01:05:30,440
and he had shows whedon judged it's simply a number of

917
01:05:30,480 --> 01:05:34,230
query that our system can process during the

918
01:05:34,250 --> 01:05:36,370
some period of time

919
01:05:36,420 --> 01:05:40,640
of course it's not so everything is not so simple because

920
01:05:40,650 --> 01:05:44,540
different areas some queries about some queries go

921
01:05:44,560 --> 01:05:47,500
to be or not to be the but we're because a a lot of a

922
01:05:47,500 --> 01:05:50,790
lot of things to analyse it all common words

923
01:05:50,810 --> 01:05:55,190
and the very very rare words it's very good

924
01:05:55,210 --> 01:05:57,690
but we assume that we have

925
01:05:57,750 --> 01:06:00,390
gordon makes a collection of various

926
01:06:02,830 --> 01:06:08,080
queries from real users are going to use this collection or maybe maybe ought be

927
01:06:08,080 --> 01:06:12,500
doing to record i mean i know you already

928
01:06:12,540 --> 01:06:15,310
have all the square

929
01:06:17,600 --> 01:06:21,120
for small collection naive kind this is the

930
01:06:21,120 --> 01:06:23,940
because to one

931
01:06:24,000 --> 01:06:30,040
because you don't need to analyse many structured only to search in an addiction

932
01:06:30,100 --> 01:06:33,420
simply analysed document

933
01:06:33,460 --> 01:06:39,170
then you will see that signature file is faster because the signature file again you

934
01:06:39,170 --> 01:06:43,420
don't need to search in any dictionaries simply looking into this table

935
01:06:43,420 --> 01:06:45,600
that's good

936
01:06:45,600 --> 01:06:47,480
and for foreign inverted while

937
01:06:48,210 --> 01:06:52,580
feel to be the slowest one because we need to search in big dictionary even

938
01:06:52,580 --> 01:06:57,460
if you if you can see it in the dictionary file still searching the dictionary

939
01:06:57,730 --> 01:07:02,190
and then on you get this fourth-placed and doing something is for this to get

940
01:07:04,100 --> 01:07:08,520
so you see this very ground this recurrence

941
01:07:08,540 --> 01:07:11,540
o sixty eight

942
01:07:13,920 --> 01:07:17,100
thinking about what will be the next

943
01:07:17,120 --> 01:07:18,310
you see that

944
01:07:18,330 --> 01:07:20,500
for all all approach

945
01:07:20,520 --> 01:07:24,730
steed is reducing pretty pretty fly

946
01:07:24,750 --> 01:07:28,190
we can we can do more detailed analysis about the things that we simply don't

947
01:07:28,230 --> 01:07:30,100
don't have time for

948
01:07:31,710 --> 01:07:36,710
on what this speed depends on what we can do

949
01:07:36,710 --> 01:07:40,960
the most smart idea is to change the

950
01:07:41,000 --> 01:07:45,330
because without change or if you can change this

951
01:07:46,210 --> 01:07:53,310
significant you can do with some small modifications without change varies but you see that

952
01:07:53,410 --> 01:07:55,140
there is defined

953
01:07:55,150 --> 01:07:58,310
four this car powered band

954
01:07:58,350 --> 01:07:59,350
so we need

955
01:07:59,370 --> 01:08:01,310
i can look into forest

956
01:08:01,310 --> 01:08:04,790
another way the size of the habits of self

957
01:08:04,830 --> 01:08:08,980
it depends on size of databases graphs all

958
01:08:09,040 --> 01:08:12,000
going down some the

959
01:08:12,040 --> 01:08:14,190
currently there still

960
01:08:14,210 --> 01:08:17,580
because you don't see any numbers the

961
01:08:17,600 --> 01:08:19,540
because it sets to graphs

962
01:08:19,560 --> 01:08:25,560
actually it's not clear i i didn't do this experiment when i draw the graphs

963
01:08:25,580 --> 01:08:27,890
you've graphical package so

964
01:08:27,940 --> 01:08:30,270
it's it's from the top of my head

965
01:08:30,310 --> 01:08:35,560
and i i believe that are very good computer you have much

966
01:08:35,560 --> 01:08:37,480
because he and on

967
01:08:38,420 --> 01:08:40,350
IBM XT

968
01:08:40,350 --> 01:08:45,690
what sort of been talking about so far you can think of

969
01:08:45,690 --> 01:08:47,850
i was assuming that you knew the model

970
01:08:47,850 --> 01:08:52,730
right i was assuming that you know you're given a fixed graph structure you knew

971
01:08:52,730 --> 01:08:56,250
those functions that were sitting on the edges are on the cliques those are somehow

972
01:08:56,250 --> 01:08:57,190
given to you

973
01:08:57,210 --> 01:08:58,850
and all you're trying to do is

974
01:08:59,250 --> 01:09:04,040
maybe someone giving noisy observations you are trying to combine your data maybe future data

975
01:09:04,040 --> 01:09:06,730
to make predictions are inferences

976
01:09:06,920 --> 01:09:10,740
but in practice it's it's very rare that someone is going to come along and

977
01:09:10,740 --> 01:09:15,560
hand you model it's not given two and a silver platter in practice you have

978
01:09:15,560 --> 01:09:19,230
to actually estimating both the parameters of the model that is

979
01:09:19,600 --> 01:09:24,850
what the form of those compatibility functions are potential should be and also there's this

980
01:09:24,850 --> 01:09:29,310
sort of more general issues actually estimating the graph structure

981
01:09:31,790 --> 01:09:33,140
so really

982
01:09:33,180 --> 01:09:38,160
this is the typical setting applications that you really have to think that both the

983
01:09:38,160 --> 01:09:42,580
structure of the model and the parameters are unknown and

984
01:09:42,600 --> 01:09:46,810
to believe in trying to estimate them on the basis of available data

985
01:09:46,830 --> 01:09:50,850
so for instance if i was doing image processing

986
01:09:50,890 --> 01:09:56,830
say i wanted to compute disparity maps or i want to denoise or segment images

987
01:09:56,960 --> 01:10:02,540
typically what you do of course is collect some large database of images and from

988
01:10:02,540 --> 01:10:06,330
that database you're trying to learn something about the structure images you try and fit

989
01:10:06,330 --> 01:10:10,810
your your model to the particular structure of that class of images

990
01:10:10,850 --> 01:10:16,420
as much as you can do that you know derive power from your model

991
01:10:18,810 --> 01:10:25,290
sam spoke about these problems but the problem of parameter estimation on one hand is

992
01:10:25,290 --> 01:10:27,120
pretty easy to state

993
01:10:27,390 --> 01:10:30,620
in certain cases it's quite easy to solve but actually in general will see it's

994
01:10:30,620 --> 01:10:32,620
pretty difficult to solve as well

995
01:10:32,620 --> 01:10:38,180
there's a bit of a chicken and egg problem here that all mention that

996
01:10:38,210 --> 01:10:42,730
essentially in order to to solve this in general you actually need to perform inference

997
01:10:42,730 --> 01:10:44,310
or marginalisation

998
01:10:44,410 --> 01:10:48,060
for instance within the ECM algorithm you need to do that is an inner step

999
01:10:48,190 --> 01:10:49,140
so it's

1000
01:10:49,190 --> 01:10:54,830
you really can just escape the complexity of performing these summations are marginalizations even when

1001
01:10:54,830 --> 01:10:57,730
you're doing parameter estimation in general

1002
01:10:57,870 --> 01:11:05,120
so just briefly maximum likelihood is to classical principles statistics

1003
01:11:05,140 --> 01:11:08,870
i would tell you that i suppose i give you a set of samples these

1004
01:11:08,870 --> 01:11:10,440
would be let's assume there would be

1005
01:11:10,910 --> 01:11:13,520
independent identically distributed

1006
01:11:13,520 --> 01:11:19,210
the imagine your model is parameterized by some class of those compatibility functions

1007
01:11:19,230 --> 01:11:24,890
this could be a text database be image database we you'd like to do somehow

1008
01:11:24,890 --> 01:11:29,910
choose those compatibility functions to maximize the likelihood of seeing that data

1009
01:11:29,910 --> 01:11:35,660
it's certainly to do the reasonable thing to do and under reasonable conditions it's it's

1010
01:11:35,660 --> 01:11:40,370
asymptotically consistent in the sense that if you were given infinite data

1011
01:11:40,370 --> 01:11:44,210
and you solve this problem you would get the right model in the end so

1012
01:11:44,330 --> 01:11:48,120
if you someone is kept giving you data more and more samples

1013
01:11:48,210 --> 01:11:52,210
in the end this procedure would give you back the right model that's that's what

1014
01:11:52,210 --> 01:11:54,190
consistency is

1015
01:11:54,230 --> 01:12:01,810
so as i mentioned in in fully observed up models this is actually very simple

1016
01:12:01,830 --> 01:12:08,230
let me just rewrite the compatibility functions in a certain way and this is gonna

1017
01:12:08,350 --> 01:12:13,350
come up later it's convenient to start writing things in terms of exponentials you sort

1018
01:12:13,350 --> 01:12:17,750
of have a weighted sum over configurations he has some indicator functions and these are

1019
01:12:17,830 --> 01:12:23,730
parameters the status sort of given one number for every configuration within the clique that

1020
01:12:23,730 --> 01:12:28,350
you can take what you're trying to do is is that these numbers essentially

1021
01:12:28,410 --> 01:12:31,420
so if you just had a for instance a binary image you can think about

1022
01:12:32,540 --> 01:12:36,660
in a two by two matrix i'm just writing this exponential of a set of

1023
01:12:36,660 --> 01:12:38,290
four numbers like this

1024
01:12:38,310 --> 01:12:40,440
those be my parameters

1025
01:12:40,500 --> 01:12:44,680
the reason i'm doing this in this special case it turns out that the log

1026
01:12:44,680 --> 01:12:49,520
likelihood takes a very nice form you end up just with a linear function of

1027
01:12:49,520 --> 01:12:51,910
your parameters times

1028
01:12:51,920 --> 01:12:55,920
basically it what known as empirical marginals are sort of you be taking counts of

1029
01:12:55,920 --> 01:12:58,350
the data figuring out how many

1030
01:12:58,350 --> 01:13:03,960
sometimes the data assumes a particular configuration and you get away to a certain weight

1031
01:13:03,960 --> 01:13:07,410
it's normalized for each one of those

1032
01:13:07,540 --> 01:13:13,310
so it's very simple the thing that it's also important is that this the here

1033
01:13:13,310 --> 01:13:18,810
this was that normalisation constant remember that thing that was just making the distribution sum

1034
01:13:18,810 --> 01:13:19,850
to one

1035
01:13:19,890 --> 01:13:25,040
remember i said before that pops up it appears all over and so here it's

1036
01:13:25,040 --> 01:13:28,680
appearing in this is the function that you'd like to optimize

1037
01:13:28,690 --> 01:13:31,080
c is fixed

1038
01:13:31,100 --> 01:13:33,440
you'd like to optimize over the parameters

1039
01:13:33,460 --> 01:13:38,310
and one thing that's coming up is that you need to optimize function involves log

1040
01:13:38,330 --> 01:13:41,940
see if this you need to be able to for instance compute this or derivatives

1041
01:13:41,940 --> 01:13:45,890
of that

1042
01:13:45,910 --> 01:13:53,580
so this is actually very simple as mentioned that it the optimality conditions for given

1043
01:13:53,580 --> 01:13:55,910
an optimisation problem

1044
01:13:55,960 --> 01:14:00,390
the first thing you would have learning calculus is that you would take derivatives to

1045
01:14:00,390 --> 01:14:04,920
try and find where the derivative is zero you've got signed words flat

1046
01:14:04,940 --> 01:14:09,290
so you take gradients you set them to zero this this simple problem i'm looking

1047
01:14:09,290 --> 01:14:10,910
at it is unconstrained

1048
01:14:11,000 --> 01:14:14,960
and if you do that it turns out that

1049
01:14:15,000 --> 01:14:18,940
the optimality conditions what they end up saying is that you should take the empirical

1050
01:14:18,940 --> 01:14:21,270
marginals accounts from the data

1051
01:14:22,640 --> 01:14:26,190
when you take the derivative of this function has a very nice property that what

1052
01:14:26,190 --> 01:14:29,940
it outputs is what marginal distributions the

1053
01:14:29,940 --> 01:14:33,310
makes it easier for us to assume that

1054
01:14:33,330 --> 01:14:37,060
probabilities are always positive in particular from microphone fields

1055
01:14:37,120 --> 01:14:39,460
but you see that a requirement

1056
01:14:39,560 --> 01:14:43,350
o so that later today approach from probably one

1057
01:14:43,750 --> 01:14:48,020
probably only to more

1058
01:14:48,020 --> 01:14:50,960
so this can be done always right

1059
01:14:50,960 --> 01:14:54,150
in this first line there is absolutely no assumptions

1060
01:14:54,210 --> 01:14:57,140
well let's go ahead

1061
01:14:57,140 --> 01:15:01,250
let's just keep on doing this procedure

1062
01:15:02,250 --> 01:15:06,290
because here we have a joint distribution in very much the same way as you

1063
01:15:06,290 --> 01:15:08,600
know we have drawn

1064
01:15:09,770 --> 01:15:13,940
well let's just do exactly the same process here

1065
01:15:13,980 --> 01:15:16,100
and people goal

1066
01:15:16,100 --> 01:15:18,500
at the end of the day will reach these

1067
01:15:18,500 --> 01:15:20,290
but people at all

1068
01:15:20,440 --> 01:15:25,540
will be able to represent his joint probability distribution exclusively

1069
01:15:25,540 --> 01:15:28,040
by conditional probabilities

1070
01:15:28,060 --> 01:15:30,210
so that's the trick

1071
01:15:31,670 --> 01:15:34,250
we represent is joined by conditional

1072
01:15:34,270 --> 01:15:37,100
and we still have a marginal probability

1073
01:15:37,210 --> 01:15:39,060
that year

1074
01:15:45,120 --> 01:15:50,140
i'll just for simplicity of representation here

1075
01:15:50,140 --> 01:15:56,350
let's just defined is smaller than i as the set of all is

1076
01:15:56,400 --> 01:15:58,500
j more than a

1077
01:15:59,870 --> 01:16:05,420
so it's more than five years one two three four

1078
01:16:05,460 --> 01:16:07,880
and then they can just write these

1079
01:16:08,020 --> 01:16:11,520
expression in this way is the product

1080
01:16:11,560 --> 01:16:13,270
from i two n

1081
01:16:13,330 --> 01:16:19,540
of p of x i given x smaller than

1082
01:16:20,440 --> 01:16:27,400
because all these files here that after the conditions by

1083
01:16:27,460 --> 01:16:28,250
they have

1084
01:16:28,270 --> 01:16:34,560
a smaller index then the wild the is on the left hand side

1085
01:16:35,790 --> 01:16:39,600
because i chose to do that in this way because i'm allowed to do

1086
01:16:39,600 --> 01:16:43,370
because of this

1087
01:16:47,270 --> 01:16:54,670
but notes that there's nothing really very special about this particular label

1088
01:16:56,060 --> 01:16:59,940
you agree with me that a campaign is labeled as a which

1089
01:16:59,960 --> 01:17:01,310
i mean

1090
01:17:01,350 --> 01:17:07,830
my choice of these living was completely out you know

1091
01:17:07,830 --> 01:17:12,210
i contribute them as i wish and will still be true

1092
01:17:12,250 --> 01:17:13,650
so you should know by

1093
01:17:13,710 --> 01:17:17,000
i hear permutation of this label

1094
01:17:18,880 --> 01:17:20,540
such that pi j

1095
01:17:20,560 --> 01:17:26,120
is smaller than pi i for all i and j

1096
01:17:26,150 --> 01:17:27,580
belonging to

1097
01:17:32,480 --> 01:17:36,140
so in this case we have phi is equal just one

1098
01:17:36,190 --> 01:17:38,190
this will be the permutation the

1099
01:17:38,210 --> 01:17:43,330
let's say that can only temptation whatever you call

1100
01:17:43,350 --> 01:17:45,520
you can certainly write the following

1101
01:17:45,540 --> 01:17:47,420
we can write

1102
01:17:47,440 --> 01:17:48,940
this as

1103
01:17:48,940 --> 01:17:51,060
p of x phi i x

1104
01:17:51,080 --> 01:17:52,420
smaller than

1105
01:17:59,250 --> 01:18:12,580
which not these ones

1106
01:18:16,210 --> 01:18:25,560
the belonging to less than nine i find it less denies this

1107
01:18:25,880 --> 01:18:33,190
i'm sorry this is not really less that this is just the motivation for this

1108
01:18:34,540 --> 01:18:37,120
but but that i

1109
01:18:37,120 --> 01:18:41,460
i wanted to write this explicitly here to avoid any possible

1110
01:18:41,480 --> 01:18:42,790
question so

1111
01:18:42,790 --> 01:18:47,710
this is just the patient will the the set of indices

1112
01:18:47,790 --> 01:18:52,100
where the elements are all the natural numbers

1113
01:18:52,100 --> 01:18:54,330
apart from his mother

1114
01:18:54,750 --> 01:19:03,170
so basically i mean the point is that they can write any probability distribution

1115
01:19:04,440 --> 01:19:05,460
in this way

1116
01:19:05,460 --> 01:19:08,460
where i have positive probability OK

1117
01:19:08,460 --> 01:19:10,870
just like this

1118
01:19:11,050 --> 01:19:13,710
if i can write in this way

1119
01:19:13,710 --> 01:19:16,060
so i'm making no

1120
01:19:16,060 --> 01:19:17,170
assumptions here

1121
01:19:17,190 --> 01:19:18,920
it will be useful c

1122
01:19:20,460 --> 01:19:27,040
can you see why this will be used

1123
01:19:27,140 --> 01:19:30,560
i mean we've seen conditional independence already right

1124
01:19:30,580 --> 01:19:36,140
and remember that when we were studying conditional independence basically what we had

1125
01:19:36,140 --> 01:19:37,670
is that

1126
01:19:37,710 --> 01:19:41,710
we had that condition in bar here in some reliable

1127
01:19:41,810 --> 01:19:46,290
and then suddenly we have a set of three variables we have two articles on

1128
01:19:46,290 --> 01:19:48,500
the other side

1129
01:19:49,170 --> 01:19:51,370
also condition

1130
01:19:52,870 --> 01:19:54,500
so we made

1131
01:19:54,540 --> 01:19:57,560
yes that seems to have a factor here

1132
01:19:57,560 --> 01:20:01,850
factorizations this represents any graphical models

1133
01:20:01,940 --> 01:20:06,190
this is how have factorizations be possibly the factorizations

1134
01:20:07,690 --> 01:20:15,560
which is given in in terms of conditional independence maybe if we have the right

1135
01:20:15,600 --> 01:20:17,790
conditional independence statements

1136
01:20:17,870 --> 01:20:20,650
we'll just feet

1137
01:20:20,690 --> 01:20:27,580
some of these factors and defectors will suddenly shrink

1138
01:20:29,980 --> 01:20:32,120
that's exactly what we are going to see

1139
01:20:32,230 --> 01:20:40,870
so any probability distribution can be written in this work

1140
01:20:40,880 --> 01:20:42,290
let's assume

1141
01:20:42,290 --> 01:20:43,920
the following

1142
01:20:43,980 --> 01:20:48,650
conditional independence statements all now we are going to make an assumption so far there

1143
01:20:48,650 --> 01:20:49,810
is no such

1144
01:20:49,880 --> 01:20:52,400
now we are going to make this

1145
01:20:52,460 --> 01:20:54,190
we're going to assume

1146
01:20:54,210 --> 01:20:57,750
that these conditional independence statements hold

1147
01:20:57,790 --> 01:21:09,120
remember that when we have a conditional independence statement

1148
01:21:09,170 --> 01:21:11,150
there are several ways right

1149
01:21:11,350 --> 01:21:14,460
conditional independence statements at a

1150
01:21:14,460 --> 01:21:17,730
well the other one which is known only to reach which

1151
01:21:17,750 --> 01:21:20,480
actually tries to understand the text a little bit more

1152
01:21:20,480 --> 01:21:27,440
which performs live some kind of form semantic analysis and then it represents meaning and

1153
01:21:27,440 --> 01:21:33,020
other that generates a smaller version of the same content some of this is a

1154
01:21:33,020 --> 01:21:36,060
little bit harder problem now let's

1155
01:21:36,080 --> 01:21:37,440
see what's techniques

1156
01:21:37,460 --> 01:21:40,380
first we have this selection based

1157
01:21:40,670 --> 01:21:43,230
there are usually has very three

1158
01:21:43,620 --> 01:21:45,980
phases and the all

1159
01:21:46,000 --> 01:21:49,860
three quite simple so first we take text analyse it

1160
01:21:50,270 --> 01:21:56,100
so this analyzing means just splitting down into the basic units which are sentences then

1161
01:21:56,100 --> 01:21:58,560
determining its important units

1162
01:21:58,580 --> 01:22:04,060
this would be synthesis or pieces parts of sentences by some kind of simple

1163
01:22:04,060 --> 01:22:09,850
four men usually and then we are in synthesizing the appropriate output and this is

1164
01:22:09,850 --> 01:22:11,730
this would be a kind of formula

1165
01:22:11,750 --> 01:22:12,980
which is often

1166
01:22:12,980 --> 01:22:20,580
used in determining its important points so

1167
01:22:21,380 --> 01:22:23,460
weight or importance of

1168
01:22:23,460 --> 01:22:28,850
sentence or parts of the part of the sentence is determined by a couple of

1169
01:22:28,880 --> 01:22:31,830
attributes i location in the text some

1170
01:22:31,850 --> 01:22:36,210
q phrases of this special words which are used in in the sentence that either

1171
01:22:36,230 --> 01:22:41,420
sentence starts with it's important to and of course then the system says well must

1172
01:22:41,420 --> 01:22:47,920
be important and that's included in summary a couple of statistic statistical information mobility the

1173
01:22:48,000 --> 01:22:53,290
this TFIDF weights and so on which mentioned before and maybe some

1174
01:22:53,330 --> 01:22:55,190
additional heuristics which

1175
01:22:55,230 --> 01:23:00,460
authors include and that's it and this formula

1176
01:23:00,460 --> 01:23:04,150
produce some kind of lumber and then all the sentences in the document i just

1177
01:23:04,150 --> 01:23:09,330
sorted by the weights and measures the top most sentences are

1178
01:23:09,620 --> 01:23:15,270
written output that's it so you feel

1179
01:23:15,440 --> 01:23:21,130
this is the screen from microsoft word so here have the threshold

1180
01:23:21,150 --> 01:23:24,560
and by moving this threshold physically you're

1181
01:23:24,560 --> 01:23:29,000
you get different sentences from the text and that

1182
01:23:31,080 --> 01:23:35,230
in the sentence appears at at the beginning of the paragraph then it's more important

1183
01:23:35,270 --> 01:23:43,790
to consider this one of these five factors from this formula giving some results

1184
01:23:47,060 --> 01:23:48,650
the other

1185
01:23:48,670 --> 01:23:55,920
technique or set of techniques i so-called knowledge reach some summarisation very said somehow here

1186
01:23:55,920 --> 01:24:02,860
we try to understand the text a little bit more and

1187
01:24:02,920 --> 01:24:07,480
so i will present here one example one which we

1188
01:24:07,500 --> 01:24:15,810
a with colleagues from microsoft research other basically the idea was the following that cities

1189
01:24:15,810 --> 01:24:18,330
are then step procedure what we did

1190
01:24:18,350 --> 01:24:23,750
again the beginning we have a document which is split into sentences then we perform

1191
01:24:23,750 --> 01:24:29,600
departing of this document and the next step is name entity disambiguation so

1192
01:24:29,630 --> 01:24:32,520
each different forms of

1193
01:24:32,540 --> 01:24:37,600
the name of the person company or place which appear in documentary tried to consolidate

1194
01:24:37,600 --> 01:24:43,560
in the same form so that george bush bush or president

1195
01:24:43,580 --> 01:24:48,460
fall in the same category here then we perform anaphora solution which means that all

1196
01:24:48,460 --> 01:24:52,270
the pronouns like he she would be replaced with the proper name

1197
01:24:52,310 --> 01:24:55,650
and then we extract subject predicate object triples

1198
01:24:55,670 --> 01:24:57,310
and the generated graph

1199
01:24:57,310 --> 01:25:00,580
and then by having such a graph then we learn

1200
01:25:00,670 --> 01:25:05,060
which parts of the graph more relevant and which are less

1201
01:25:05,080 --> 01:25:10,170
proceedings from the by simply by machine learning techniques so we have

1202
01:25:10,190 --> 01:25:17,000
the corpus of documents and summaries another this corpus we learn

1203
01:25:17,020 --> 01:25:18,920
how summaries

1204
01:25:18,940 --> 01:25:24,920
look like in graphs and by having such a model that we can extract also

1205
01:25:25,190 --> 01:25:31,170
important parts of this semantic graphs also unseen documents so here's an example so if

1206
01:25:31,210 --> 01:25:34,730
have the document

1207
01:25:34,750 --> 01:25:39,100
tom went to town in a bookstore he bought a large book so this bomb

1208
01:25:39,100 --> 01:25:40,860
would get replaced here

1209
01:25:40,880 --> 01:25:44,730
is he will get replaced and then we would extract all

1210
01:25:44,790 --> 01:25:50,900
three puts out of it and then the more likely to generate such a graph

1211
01:25:50,900 --> 01:25:55,650
can be learned such graph so this would be an example

1212
01:25:55,690 --> 01:25:58,270
so this is the full document and then

1213
01:25:58,270 --> 01:26:04,660
summarisation sexual extracting just the most relevant parts of this graph and this set a

1214
01:26:04,660 --> 01:26:12,080
set so this works quite well actually it it's not really or performing but still

1215
01:26:12,420 --> 01:26:18,480
it this is an interesting alternative representation of the content of cork on canonical because

1216
01:26:18,480 --> 01:26:20,500
of the content of the document

1217
01:26:20,550 --> 01:26:24,900
and this is another example so if this is the full

1218
01:26:24,940 --> 01:26:27,020
text this is a summary

1219
01:26:27,040 --> 01:26:32,520
so this would be the summary graph of such documents show now short them on

1220
01:26:32,650 --> 01:26:35,600
how we can

1221
01:26:35,620 --> 01:26:45,850
play the content of document

1222
01:26:45,880 --> 01:26:55,560
so this is a news documents from

1223
01:26:56,790 --> 01:26:58,230
l use or something

1224
01:26:58,230 --> 01:27:01,560
from nineteen two to this was the test corpus

1225
01:27:01,850 --> 01:27:05,810
chris the

1226
01:27:05,850 --> 01:27:09,920
so this is the full documents now if e

1227
01:27:09,960 --> 01:27:14,830
reduce association here we it all the learning and the model is behind for each

1228
01:27:14,830 --> 01:27:18,390
the following content is provided under creative commons license

1229
01:27:18,410 --> 01:27:24,720
your support will help MIT opencourseware continue to offer high quality educational resources for free

1230
01:27:24,740 --> 01:27:29,540
to make a donation or to view additional materials from hundreds of MIT courses

1231
01:27:29,600 --> 01:27:35,310
his MIT opencourseware OCW MIT that EDU

1232
01:27:35,330 --> 01:27:37,310
to get started

1233
01:27:38,210 --> 01:27:41,400
one announcement and that is that

1234
01:27:41,490 --> 01:27:45,740
reminder about the celebration of learning on wednesday

1235
01:27:45,780 --> 01:27:47,030
october second

1236
01:27:47,070 --> 01:27:48,420
test two

1237
01:27:48,470 --> 01:27:51,180
it will have coverage

1238
01:27:51,210 --> 01:27:55,780
spanning lecture seven three seventeen seven three seventy

1239
01:27:55,800 --> 01:27:56,780
so the

1240
01:27:56,780 --> 01:28:00,310
material that the new material that i'm going to introduce today will not be

1241
01:28:00,320 --> 01:28:03,990
covered but certainly this little review is worthwhile

1242
01:28:04,030 --> 01:28:08,600
and going back to seven with the beginning avionics

1243
01:28:09,380 --> 01:28:11,670
what i wanted to do today was used

1244
01:28:11,990 --> 01:28:15,930
this set the stage briefly review where we were last day when we looked at

1245
01:28:15,930 --> 01:28:17,490
the emission spectrum

1246
01:28:17,550 --> 01:28:20,810
the target an x-ray tubes are plotted

1247
01:28:20,910 --> 01:28:27,280
the intensity versus wavelength so long wavelength is low energy energy is increasing from

1248
01:28:27,350 --> 01:28:28,880
right to left

1249
01:28:28,890 --> 01:28:31,080
and what we see is the family of

1250
01:28:31,080 --> 01:28:32,560
nested curves

1251
01:28:33,360 --> 01:28:35,580
each taking a different plate voltage

1252
01:28:35,600 --> 01:28:37,050
low voltage gives us

1253
01:28:37,100 --> 01:28:42,080
low-intensity high-voltage gives high intensity so we see this whale shaped

1254
01:28:42,080 --> 01:28:47,440
background which we refer to as the continuous spectrum abrams' strong and then once we

1255
01:28:47,440 --> 01:28:49,500
reach a critical voltage

1256
01:28:49,500 --> 01:28:52,890
beyond which we start to see the second set

1257
01:28:53,600 --> 01:28:54,860
o lines here

1258
01:28:54,880 --> 01:28:56,240
which have indicated in

1259
01:28:57,420 --> 01:29:02,860
and this is the characteristic spectral and it's quantized it's not continuous we have discrete

1260
01:29:02,880 --> 01:29:04,500
values of

1261
01:29:04,630 --> 01:29:10,670
wavelength associated with energy transitions within the target nodes transitions are prompted by

1262
01:29:10,720 --> 01:29:15,690
the ejection of inner shell electrons and that doesn't occur until you get to a

1263
01:29:15,690 --> 01:29:22,030
certain critical applied voltage which translates into a certain critical ballistic and energy on the

1264
01:29:22,030 --> 01:29:24,170
part of the incident electrons

1265
01:29:24,300 --> 01:29:27,450
and there are few features here that we can point out

1266
01:29:27,460 --> 01:29:29,890
the various discrete

1267
01:29:29,920 --> 01:29:34,740
wavelengths of the characteristic spectrum are calculable

1268
01:29:34,820 --> 01:29:36,930
the KL for the l alpha

1269
01:29:36,950 --> 01:29:40,890
o lines are calculable by mostly slot which is given here

1270
01:29:40,920 --> 01:29:44,050
were the NF in and i are associated with the

1271
01:29:44,090 --> 01:29:49,990
spring a particular transition for k lines it's the transition from two to one

1272
01:29:49,990 --> 01:29:52,520
and screening factor is one

1273
01:29:52,550 --> 01:29:56,290
for the k alpha transition for the ll transition

1274
01:29:56,340 --> 01:30:03,140
it's from three to two and screening factor in that case is seven point four

1275
01:30:03,200 --> 01:30:07,950
so we can calculate these discrete values and the other point that we can calculate

1276
01:30:07,950 --> 01:30:12,270
on this curve is the leading edge of the continuous spectrum

1277
01:30:12,270 --> 01:30:14,860
the leading edge of continuous spectrum is the

1278
01:30:14,870 --> 01:30:20,050
designated here is the shortest wavelength which obviously is the highest energy and that derives

1279
01:30:20,050 --> 01:30:25,920
from total transfer of incident ballistic energy of the electron to

1280
01:30:27,170 --> 01:30:31,770
and capture the electron and that's the shortest wavelength given by the doing hunt law

1281
01:30:31,790 --> 01:30:35,270
which is twelve thousand four hundred over the plate voltage

1282
01:30:35,270 --> 01:30:36,960
giving you value

1283
01:30:36,980 --> 01:30:39,140
of wavelength and angstroms

1284
01:30:39,140 --> 01:30:41,180
so i think we're ready now

1285
01:30:41,830 --> 01:30:47,080
probe atomic arrangement with x-rays and in order to do so we're going to rely

1286
01:30:47,080 --> 01:30:48,300
on two

1287
01:30:50,490 --> 01:30:53,800
factors first of all we're going to model

1288
01:30:53,890 --> 01:30:56,890
we're going to model the crystal

1289
01:30:56,930 --> 01:30:59,290
as an array of mirrors

1290
01:30:59,300 --> 01:31:01,270
in other words we're going to look at

1291
01:31:01,330 --> 01:31:07,240
the atoms in the crystal and consider each other as acting as a mirror

1292
01:31:08,450 --> 01:31:09,330
this will

1293
01:31:09,340 --> 01:31:10,340
give us

1294
01:31:11,490 --> 01:31:14,520
that we can exploit in a very simple model

1295
01:31:14,520 --> 01:31:16,920
so let's model atoms

1296
01:31:16,950 --> 01:31:19,550
model atoms as mirrors

1297
01:31:19,550 --> 01:31:27,210
and that's going to allow us to use the laws of specular reflection

1298
01:31:27,230 --> 01:31:30,870
the laws of specular reflection

1299
01:31:30,880 --> 01:31:32,210
and the one

1300
01:31:32,270 --> 01:31:36,690
most critically rely on is that the angle of incidence

1301
01:31:36,750 --> 01:31:38,160
equals the angle

1302
01:31:38,200 --> 01:31:39,890
of reflection

1303
01:31:39,900 --> 01:31:44,790
the incidence equals the angle of refraction so that's the first thing that we've

1304
01:31:45,770 --> 01:31:50,540
and the second thing that we use is interference criteria

1305
01:31:50,600 --> 01:31:52,760
we're going to apply

1306
01:31:52,900 --> 01:31:56,810
interference criteria

1307
01:31:56,820 --> 01:31:59,730
so let's take a look at what i mean by that

1308
01:32:00,830 --> 01:32:02,690
we have two possibilities

1309
01:32:02,700 --> 01:32:05,020
we can have multiple beams

1310
01:32:05,030 --> 01:32:10,190
coming in phase or we have multiple beams coming out of phase and so here's

1311
01:32:10,200 --> 01:32:15,560
where we talk about i'm going to show will sketch were plotting amplitude

1312
01:32:15,660 --> 01:32:17,020
this is some

1313
01:32:17,030 --> 01:32:21,180
electromagnetic signal amplitude versus time

1314
01:32:21,210 --> 01:32:23,380
amplitude versus time so

1315
01:32:23,410 --> 01:32:25,400
let's look at two

1316
01:32:25,400 --> 01:32:30,200
we have some temperature and the child is basically the daily average temperature in the

1317
01:32:30,230 --> 01:32:34,750
form of a weather station and what we're trying to determine whether a full occurred

1318
01:32:35,080 --> 01:32:37,320
on this trace now

1319
01:32:37,340 --> 01:32:40,710
what we know is in this case has not been a full for the first

1320
01:32:41,150 --> 01:32:44,140
fifty days which is concerned the last

1321
01:32:46,620 --> 01:32:52,440
they fifty today seventy one interesting whether that forces that would build a good

1322
01:32:52,720 --> 01:32:57,240
profile of user behavior clearly this is not going to work because what the temperature

1323
01:32:57,240 --> 01:32:59,420
was two months ago is not going to be

1324
01:32:59,440 --> 01:33:02,880
very helpful predictive of what's going to happen on one particular day

1325
01:33:03,330 --> 01:33:05,350
two months later

1326
01:33:05,370 --> 01:33:09,580
but if you want to find the weather stations had to have the similar sort

1327
01:33:09,580 --> 01:33:14,740
of traces i don't have to be geographically locate similarly located could be examples

1328
01:33:14,780 --> 01:33:19,000
i have a jew on the cosine quite quite far away from each other just

1329
01:33:19,000 --> 01:33:24,100
as long as they recently had a similar behavior

1330
01:33:24,110 --> 01:33:26,750
it lies at the top

1331
01:33:27,270 --> 01:33:30,110
so we can say well this

1332
01:33:30,140 --> 01:33:34,690
look at the time fifty tonnes seventy in more detail

1333
01:33:34,740 --> 01:33:36,470
we can see at time

1334
01:33:36,480 --> 01:33:38,100
taken sixty eight

1335
01:33:42,580 --> 01:33:49,520
weather station has deviated quite greatly from its paper and indeed that constitute a fault

1336
01:33:49,520 --> 01:33:53,380
in the system

1337
01:33:53,400 --> 01:33:56,240
compare that to the the

1338
01:33:56,250 --> 01:33:59,040
four population of

1339
01:33:59,070 --> 01:34:04,240
of weather stations in the consequent density is in this but just basically overlaying on

1340
01:34:05,260 --> 01:34:08,710
on the whole population you can see this is the point here where

1341
01:34:08,750 --> 01:34:10,140
the folder

1342
01:34:11,070 --> 01:34:15,090
again is compatible with some genuine global outliers so

1343
01:34:15,110 --> 01:34:18,630
we could better to find these if we want to set a threshold

1344
01:34:18,650 --> 01:34:21,280
this sort distance was an outlier

1345
01:34:21,290 --> 01:34:32,700
then it will probably get an awful lot of false positives after this is

1346
01:34:33,420 --> 01:34:40,390
just those so simple examples is the basic that method again now page divided on

1347
01:34:40,400 --> 01:34:44,160
multivariate continuous data which is timeline basically

1348
01:34:44,170 --> 01:34:48,450
the crucial thing here is that what we'd like to know what other accounts doing

1349
01:34:48,450 --> 01:34:51,770
at the same time so this is what we need to solve the timeline the

1350
01:34:51,770 --> 01:34:54,830
concept so very and we do is

1351
01:34:54,850 --> 01:34:57,710
she have appeared when we know they are

1352
01:34:57,720 --> 01:34:58,700
we can just

1353
01:34:58,740 --> 01:35:01,640
basic at the moment the distance from p paper

1354
01:35:01,650 --> 01:35:06,100
of the target to the centre of the people who just take the mean of

1355
01:35:06,110 --> 01:35:11,450
the values and the current of vice versa matrix and discover distance

1356
01:35:11,460 --> 01:35:12,830
so what

1357
01:35:12,850 --> 01:35:15,740
what term

1358
01:35:15,760 --> 01:35:21,360
well the point is an outlier not we have extended set threshold was set distance

1359
01:35:21,370 --> 01:35:24,280
the mahalanobis distance is different too

1360
01:35:24,290 --> 01:35:29,560
including the generalisation euclidean distance and that the lack of points which are the same

1361
01:35:29,580 --> 01:35:34,850
distance from the distance from the the mean would be a circle in this case

1362
01:35:35,790 --> 01:35:38,990
let's distances the generalizes to an ellipse

1363
01:35:39,040 --> 01:35:43,810
so in this case the typical points which are

1364
01:35:43,820 --> 01:35:45,250
here the blue dots

1365
01:35:47,970 --> 01:35:52,620
what with a certain threshold which of two such as this ellipse here we can

1366
01:35:52,620 --> 01:35:57,110
see that the target value is now

1367
01:35:57,750 --> 01:36:02,570
basic some problems with using the this distance especially for building people

1368
01:36:03,210 --> 01:36:09,000
as with outliers occurring in the paper this can occur because we can we can

1369
01:36:09,000 --> 01:36:13,990
misfortune transactions and they can be then contaminating appeared

1370
01:36:14,660 --> 01:36:19,750
that's the types of issues that can occur outlier masking this is an example where

1371
01:36:19,750 --> 01:36:22,040
some outliers down here

1372
01:36:22,070 --> 01:36:26,640
has shifted the distance such that are

1373
01:36:26,650 --> 01:36:28,080
it lies now

1374
01:36:28,090 --> 01:36:30,380
basically on the on the special and in

1375
01:36:30,490 --> 01:36:32,100
that's what i'm asking this

1376
01:36:32,110 --> 01:36:33,530
the converse where

1377
01:36:33,540 --> 01:36:37,970
a data point which is strictly an in

1378
01:36:37,990 --> 01:36:43,510
then because of the defamation due to outlast comes in out south

1379
01:36:43,530 --> 01:36:45,570
so we need to consider

1380
01:36:45,610 --> 01:36:46,980
how to

1381
01:36:46,990 --> 01:36:52,640
robust cases of are some techniques for robustifying against

1382
01:36:52,760 --> 01:36:56,350
outliers from distance for the computation intensive

1383
01:36:56,360 --> 01:36:59,730
so we just use simple heuristic which was

1384
01:36:59,740 --> 01:37:03,720
an account that has deviated strongly from this paper of time t should not contribute

1385
01:37:03,720 --> 01:37:09,890
to any other people database one system twice once again said scores for all the

1386
01:37:10,940 --> 01:37:13,590
then for each

1387
01:37:13,630 --> 01:37:18,930
target can we look at its peak members and just select this percentage of those

1388
01:37:20,420 --> 01:37:24,110
we're trying to use the seventy percent of those just to help

1389
01:37:28,210 --> 01:37:32,030
another issue is that is not necessarily the case that you can deploy people last

1390
01:37:32,030 --> 01:37:37,610
on all accounts so is a simple matter of people qualities this quote this question

1391
01:37:37,610 --> 01:37:40,580
here is based at a certain time period

1392
01:37:40,580 --> 01:37:43,020
first one specific time points

1393
01:37:43,030 --> 01:37:44,530
which is measured the

1394
01:37:44,540 --> 01:37:45,980
the distance of

1395
01:37:46,040 --> 01:37:49,140
targeted to the centre of the

1396
01:37:49,200 --> 01:37:53,180
the mean of the period

1397
01:37:53,190 --> 01:37:58,130
a slight complications at different times of the

1398
01:37:58,140 --> 01:38:04,720
we need to know what i'd like to scale the data standardized so we'll standardised

1399
01:38:05,530 --> 01:38:07,100
and then some of

1400
01:38:07,210 --> 01:38:08,610
for example here

1401
01:38:08,620 --> 01:38:10,830
each time point

1402
01:38:10,910 --> 01:38:14,100
i would rather we basically is what in the data

1403
01:38:14,110 --> 01:38:17,130
and so here then it is in is commensurate

1404
01:38:17,150 --> 01:38:21,980
and it's it's standpoint in this sum is these quality values together

1405
01:38:22,030 --> 01:38:28,700
and so the smaller the quality the better the this is trapped this particular target

1406
01:38:28,750 --> 01:38:31,510
and so select the number of

1407
01:38:31,520 --> 01:38:39,450
but it tracked to target counts

1408
01:38:39,510 --> 01:38:45,440
after the building of peer groups so this is more of problem specific

1409
01:38:45,500 --> 01:38:50,200
it's possible already which what the people membership base an example would be

1410
01:38:50,220 --> 01:38:54,850
employed for detection so people with the same job description you could just put together

1411
01:38:54,860 --> 01:38:55,920
and see how they

1412
01:38:56,840 --> 01:39:01,580
if they based the north end of one of the d gates again that might

1413
01:39:01,580 --> 01:39:07,030
be considered unusual behaviour so with an example of this would be the

1414
01:39:07,080 --> 01:39:10,360
rbm forty management system

1415
01:39:10,450 --> 01:39:14,860
probably what's in healthcare reform in the united states

1416
01:39:14,860 --> 01:39:19,310
and what they do that you can just take they tend to some new york

1417
01:39:19,320 --> 01:39:25,290
and compare them all to see his claiming the most advanced on the health insurance

1418
01:39:28,770 --> 01:39:32,070
but what we'd like to do is to infer membership from the actually behave the

1419
01:39:32,080 --> 01:39:38,530
time series themselves what we need and this is different measure similarity between time series

1420
01:39:38,640 --> 01:39:43,630
and that again is essential problem specific thing and we'll discuss one example of what

1421
01:39:43,860 --> 01:39:46,470
a simple use

1422
01:39:49,120 --> 01:39:51,520
the dataset

1423
01:39:51,520 --> 01:39:54,350
in simple cases like

1424
01:39:54,660 --> 01:40:00,040
every single words and multiwordstring compoundstring but

1425
01:40:00,290 --> 01:40:04,810
when you have a relational nouns like mother temperature you have to

1426
01:40:04,890 --> 01:40:06,890
two classification

1427
01:40:09,180 --> 01:40:11,390
by hand so

1428
01:40:11,450 --> 01:40:12,870
mother denotes

1429
01:40:13,240 --> 01:40:16,310
mothers relational noun in the sense that

1430
01:40:16,350 --> 01:40:20,600
it relates to arguments someone and their mother temperature in

1431
01:40:20,680 --> 01:40:22,790
something and its temperature

1432
01:40:22,890 --> 01:40:24,810
and there are three

1433
01:40:24,810 --> 01:40:28,040
options you have for your relational nouns

1434
01:40:28,060 --> 01:40:31,890
there's the predicate denotes are going around which all go through

1435
01:40:31,970 --> 01:40:33,700
you can also search

1436
01:40:33,720 --> 01:40:36,040
the denotation

1437
01:40:36,040 --> 01:40:40,660
a certain make it the notation assertion in the relationparaphrasemt and then you can also

1438
01:40:40,660 --> 01:40:42,450
use nounsemtrans

1439
01:40:42,450 --> 01:40:46,540
so that will get us into the center some trans predicates they get to that

1440
01:40:47,100 --> 01:40:50,770
the nodes are going around

1441
01:40:52,180 --> 01:40:54,660
here's an example of it in

1442
01:40:56,220 --> 01:41:00,060
four other denotes the

1443
01:41:00,140 --> 01:41:03,750
relation mother

1444
01:41:06,830 --> 01:41:09,250
the relation of because mother

1445
01:41:09,330 --> 01:41:10,790
is a binary

1446
01:41:10,810 --> 01:41:13,740
relation holds between something and the mother

1447
01:41:13,830 --> 01:41:14,700
so the

1448
01:41:15,350 --> 01:41:16,520
term other

1449
01:41:16,540 --> 01:41:18,040
refers to the

1450
01:41:18,060 --> 01:41:20,040
the second argument

1451
01:41:26,520 --> 01:41:27,930
what that entails

1452
01:41:27,930 --> 01:41:31,510
is that

1453
01:41:31,520 --> 01:41:35,080
the denotation of mother theword

1454
01:41:35,240 --> 01:41:37,790
is the set of things

1455
01:41:37,870 --> 01:41:39,950
four which

1456
01:41:45,620 --> 01:41:47,470
they are the mother

1457
01:41:47,490 --> 01:41:49,930
for of things that are mothers basically

1458
01:41:52,510 --> 01:41:54,990
relation exists instance

1459
01:41:55,040 --> 01:41:58,970
that's what i call rule macro predicates that expands into

1460
01:41:59,830 --> 01:42:01,790
one thousand variables

1461
01:42:01,890 --> 01:42:03,240
what this means

1462
01:42:04,890 --> 01:42:09,350
such a thing

1463
01:42:10,750 --> 01:42:14,410
such that and is the mother of something

1464
01:42:14,580 --> 01:42:17,330
so the set of all mothers

1465
01:42:17,370 --> 01:42:21,120
that do not around most proper way too

1466
01:42:22,020 --> 01:42:23,270
let's apply

1467
01:42:23,270 --> 01:42:25,010
relational nouns

1468
01:42:25,160 --> 01:42:26,720
you can also just

1469
01:42:26,750 --> 01:42:33,020
flop over a relational nouns into the relation paraphrase MT where

1470
01:42:35,120 --> 01:42:39,470
directly connected to relations here the notation

1471
01:42:39,520 --> 01:42:45,600
and for some applications all you want is to know that there are some relationship

1472
01:42:45,600 --> 01:42:46,910
between the

1473
01:42:46,930 --> 01:42:48,830
the word and

1474
01:42:48,830 --> 01:42:50,450
the relation

1475
01:42:50,850 --> 01:42:58,240
you don't really care exactly which argument corresponds to another more proper way of

1476
01:42:58,310 --> 01:43:00,640
like find nouns

1477
01:43:00,660 --> 01:43:02,600
i don't know things with

1478
01:43:02,600 --> 01:43:03,970
more structure

1479
01:43:04,020 --> 01:43:08,620
it is using nounsemtrans so here i've given an example

1480
01:43:08,660 --> 01:43:12,560
and complicated example explain more about what's going on

1481
01:43:15,120 --> 01:43:18,520
the first arguement of the predicate nouns nounsemtrans

1482
01:43:20,180 --> 01:43:24,330
complex word

1483
01:43:24,410 --> 01:43:26,430
assassination was

1484
01:43:26,450 --> 01:43:28,180
the word assassinate

1485
01:43:28,200 --> 01:43:29,810
with the suffix

1486
01:43:34,520 --> 01:43:38,270
combined with this function wordwithsuffixfn

1487
01:43:38,290 --> 01:43:43,080
it takes a word and suffix and gives you another word

1488
01:43:43,120 --> 01:43:48,870
the second argument is a random number h is one hundred thirty two because that's

1489
01:43:48,870 --> 01:43:51,660
my favorite number because all

1490
01:43:51,740 --> 01:43:53,240
and the

1491
01:43:53,240 --> 01:43:55,620
third argument is

1492
01:43:55,740 --> 01:44:02,910
the frame the so that the syntactic frame

1493
01:44:02,950 --> 01:44:04,430
in which this

1494
01:44:04,580 --> 01:44:07,450
translation hold

1495
01:44:07,490 --> 01:44:08,990
it says

1496
01:44:09,270 --> 01:44:11,560
so generative frame

1497
01:44:11,580 --> 01:44:14,250
means either

1498
01:44:14,540 --> 01:44:15,600
is y

1499
01:44:15,600 --> 01:44:17,360
it works surprisingly well

1500
01:44:17,420 --> 01:44:20,980
at least that's what many people apart and i also tried to really seems to

1501
01:44:22,390 --> 01:44:25,800
i mean there well i mean i it works reason i mean nothing works always

1502
01:44:25,800 --> 01:44:28,810
in clustering but it's a reasonable way

1503
01:44:28,860 --> 01:44:32,880
but some it's also problematic because

1504
01:44:32,900 --> 01:44:36,900
i mean stability something like a necessary requirement but it's nothing which is

1505
01:44:36,940 --> 01:44:38,550
sufficient way so

1506
01:44:38,550 --> 01:44:43,580
you're not just say i is made all of group pretty is pretty stable but

1507
01:44:43,580 --> 01:44:47,860
not select the k which is most stable and this somehow

1508
01:44:47,920 --> 01:44:53,830
yeah OK can do it but it's not really clear whether this is really useful

1509
01:44:53,830 --> 01:44:56,180
OK so the way it usually works is

1510
01:44:56,200 --> 01:44:58,060
OK you get your dataset

1511
01:44:58,070 --> 01:45:00,390
and the clustering

1512
01:45:00,450 --> 01:45:01,520
and now

1513
01:45:01,530 --> 01:45:03,950
what you do is you take your dataset

1514
01:45:03,980 --> 01:45:08,630
you draw a subsample of the dataset say your dataset contains and sub thousand pointing

1515
01:45:08,630 --> 01:45:10,750
out of seven hundred points

1516
01:45:10,780 --> 01:45:13,770
from this data set a new clusterid

1517
01:45:13,780 --> 01:45:15,520
with a fixed number of k

1518
01:45:15,660 --> 01:45:20,110
you do this again and again so in this way for each fixed k

1519
01:45:20,120 --> 01:45:21,760
you generate

1520
01:45:21,770 --> 01:45:23,890
an ensemble of classes which

1521
01:45:23,910 --> 01:45:26,750
like they are always based on a slightly different samples

1522
01:45:26,790 --> 01:45:29,540
but the argument does the same

1523
01:45:30,340 --> 01:45:32,560
and then what you do is you compare all those

1524
01:45:32,570 --> 01:45:36,000
compare with the results you always get our model is stable

1525
01:45:36,020 --> 01:45:38,180
with a very a lot

1526
01:45:38,200 --> 01:45:42,120
to do this and we have we will go on how to do that

1527
01:45:42,160 --> 01:45:46,060
the first define some the distance between the clusterings who observed

1528
01:45:46,060 --> 01:45:49,840
and then we define based on this distance you get to define some of what

1529
01:45:49,850 --> 01:45:53,830
stability is and then choose the k they could do for different numbers of k

1530
01:45:53,900 --> 01:45:56,300
interested in and then you take the one which is

1531
01:45:56,340 --> 01:45:58,120
which gives the best stability

1532
01:46:01,190 --> 01:46:03,010
and if it and

1533
01:46:03,010 --> 01:46:04,180
people will

1534
01:46:04,200 --> 01:46:08,610
i pro stability they always show you figure like that to explain why so the

1535
01:46:08,610 --> 01:46:11,030
figures very simple assume

1536
01:46:11,030 --> 01:46:14,700
we have so so that the toy figure which would say something like this before

1537
01:46:14,720 --> 01:46:17,550
clusters in our dataset so each of the

1538
01:46:17,600 --> 01:46:21,890
lex service should represent one clustering or distribution some

1539
01:46:21,950 --> 01:46:26,320
now we sample from the distribution and we get those red dot

1540
01:46:26,320 --> 01:46:31,560
and so obviously in our case the true number of clusters should before because the

1541
01:46:31,560 --> 01:46:34,930
status it just contains those ninety four blocks

1542
01:46:34,950 --> 01:46:36,380
but now i tell thee

1543
01:46:36,390 --> 01:46:38,220
just these two clusters

1544
01:46:38,270 --> 01:46:40,050
what happens is

1545
01:46:40,060 --> 01:46:44,530
so now i do this like i sampled several times and was repeatedly

1546
01:46:44,540 --> 01:46:48,130
so maybe want to cluster them like this and the next time it just like

1547
01:46:48,860 --> 01:46:52,470
so all the result would be very unstable

1548
01:46:52,500 --> 01:46:56,670
the idea is now this will tell you that case the choice

1549
01:46:56,730 --> 01:46:59,250
the same happens if

1550
01:46:59,310 --> 01:47:02,520
which was too many data points to many clusters

1551
01:47:02,530 --> 01:47:03,750
then some more

1552
01:47:03,760 --> 01:47:05,370
it has two

1553
01:47:05,420 --> 01:47:09,790
split one of the clusters and can be very different which one can split

1554
01:47:09,830 --> 01:47:12,150
depending on your simple

1555
01:47:12,180 --> 01:47:16,160
and of course if you think i don't have here but if you take k

1556
01:47:16,160 --> 01:47:20,500
equal to four you will always get more or less the same as that state

1557
01:47:20,510 --> 01:47:24,640
the story somewhere

1558
01:47:24,660 --> 01:47:31,850
now if you want to do this in practice

1559
01:47:31,940 --> 01:47:35,860
we first have to define a distance between clusterings

1560
01:47:35,860 --> 01:47:40,930
because we went to want to measure how simple similarity similar the different clusterings are

1561
01:47:40,980 --> 01:47:43,850
and there again different ways of doing this

1562
01:47:43,860 --> 01:47:47,210
so we first start by saying i can assume we look at clusterings of the

1563
01:47:47,210 --> 01:47:48,970
same set of points

1564
01:47:49,050 --> 01:47:51,870
and then we can simply can't often

1565
01:47:51,920 --> 01:47:55,670
we look at each pair of points and contour of this is it in the

1566
01:47:55,670 --> 01:47:59,720
same cluster are in different clusters so here

1567
01:47:59,730 --> 01:48:02,860
this this number and zero zero one in one one

1568
01:48:02,970 --> 01:48:05,120
so something like

1569
01:48:05,470 --> 01:48:07,900
how many points pairs of points

1570
01:48:07,950 --> 01:48:09,680
are together in the

1571
01:48:15,860 --> 01:48:19,810
so for each two pairs of points you look whether there

1572
01:48:19,820 --> 01:48:23,900
ending up in the same cluster in different clusters according to clustering one or two

1573
01:48:23,900 --> 01:48:27,710
so and zero zero means i look at i can't how many points are in

1574
01:48:28,130 --> 01:48:31,900
the same cluster according to cluster one and in the same cluster according to cluster

1575
01:48:33,080 --> 01:48:38,080
and c and one says the pair in the same cluster according to cluster one

1576
01:48:38,080 --> 01:48:42,310
but the different classes according to cluster to some all you just compare with the

1577
01:48:42,310 --> 01:48:46,100
clusterings to the same or not the same on pairs of points and there are

1578
01:48:46,100 --> 01:48:50,170
lots of ways how you can generate the distance of the or similarity function that's

1579
01:48:51,330 --> 01:48:56,010
so the two most famous one another and make sure context which are

1580
01:48:56,020 --> 01:48:59,380
just some way so in the hamming distance is some of the opposite of the

1581
01:48:59,380 --> 01:49:01,550
random example but it's nothing

1582
01:49:01,560 --> 01:49:06,210
very fundamental you just count how often does the clustering to the same and different

1583
01:49:06,270 --> 01:49:10,870
pairs of points

1584
01:49:10,930 --> 01:49:12,430
now the

1585
01:49:12,450 --> 01:49:15,830
OK skip another problem is if you have

1586
01:49:15,830 --> 01:49:18,680
we know we had this subsampling scheme

1587
01:49:18,850 --> 01:49:23,320
i said OK we subsamples seven hundred five thousand data points in one cluster

1588
01:49:23,500 --> 01:49:27,420
and then we subsample another seven hundred points and of course

1589
01:49:27,470 --> 01:49:30,350
o seven hundred points will not be the same i mean there will be some

1590
01:49:30,350 --> 01:49:33,800
points which are in common but there will be other points which are not uncommon

1591
01:49:33,870 --> 01:49:38,020
in most cases clustering just give you reside on the point of set on the

1592
01:49:38,020 --> 01:49:41,140
set of points which gave it doesn't include

1593
01:49:41,150 --> 01:49:45,510
somehow there now there are two ways what you can do with it

1594
01:49:46,950 --> 01:49:49,080
so the first place we restrict

1595
01:49:49,080 --> 01:49:53,980
or the other way to extend so restriction in life

1596
01:49:54,000 --> 01:49:59,010
restriction means so you look at their first set of substance and and you're such

1597
01:49:59,100 --> 01:50:01,800
second set of subsamples

1598
01:50:01,820 --> 01:50:06,180
and then you just take the intersection between those and you just compare the clustering

1599
01:50:06,180 --> 01:50:10,380
based on the on the intersection of point so if you compare

1600
01:50:10,410 --> 01:50:13,320
so say the first clustering

1601
01:50:13,330 --> 01:50:16,190
well that can really draw this

1602
01:50:16,210 --> 01:50:20,120
so essentially you compare the clusterings on the points on which both of them are

1603
01:50:22,470 --> 01:50:25,090
so that's the restriction operator

1604
01:50:25,090 --> 01:50:27,130
the other way you can do this

1605
01:50:27,150 --> 01:50:29,660
it's an extension operator is just say OK

1606
01:50:29,670 --> 01:50:31,450
take the first clustering

1607
01:50:31,500 --> 01:50:32,550
and extent

1608
01:50:32,570 --> 01:50:36,340
the cluster label somewhere to the results of all the other data points

1609
01:50:36,350 --> 01:50:39,130
which are in the other subset

1610
01:50:41,090 --> 01:50:44,290
and then i have to find a clustering on the whole domain after extension and

1611
01:50:44,290 --> 01:50:46,120
then i can again compare it

1612
01:50:46,130 --> 01:50:49,120
and the way extend

1613
01:50:49,130 --> 01:50:53,760
so that there are different ways of working means for example it's very simple for

1614
01:50:53,760 --> 01:50:57,650
k means has the centre so you just get new data points just assign each

1615
01:50:57,650 --> 01:51:01,030
data point to the closest centre and and that it so it's very simple to

1616
01:51:02,410 --> 01:51:04,830
single linkage is also very simple

1617
01:51:04,840 --> 01:51:09,900
because it just nearest neighbour because it's more that's what single linkage does

1618
01:51:09,920 --> 01:51:13,640
for other algorithms it's much more complicated

1619
01:51:13,640 --> 01:51:14,120
something like that

1620
01:51:14,880 --> 01:51:19,070
good idea maybe there will also be ten or fourteen or

1621
01:51:20,220 --> 01:51:21,490
well ballpark numbers

1622
01:51:23,540 --> 01:51:25,870
there are some questions about this change

1623
01:51:26,920 --> 01:51:27,510
and others

1624
01:51:27,920 --> 01:51:28,670
so that's going

1625
01:51:29,390 --> 01:51:32,820
you can imagine likely twenty metres wide

1626
01:51:33,200 --> 01:51:36,600
i could perform as well you know what thing going

1627
01:51:37,040 --> 01:51:39,060
and moving the right or left

1628
01:51:41,340 --> 01:51:46,030
that the proposal moves when you come up with that will always in fact be

1629
01:51:46,250 --> 01:51:48,350
accepted and in the middle like that

1630
01:51:48,880 --> 01:51:50,860
because the probability ratio between any

1631
01:51:54,280 --> 01:51:58,660
and the q ratio what we want put that have come back

1632
01:51:59,260 --> 01:52:01,060
one when i the wall

1633
01:52:01,810 --> 01:52:02,690
if we propose

1634
01:52:04,070 --> 01:52:04,980
moving right

1635
01:52:05,510 --> 01:52:06,540
then i will be forbidden

1636
01:52:07,710 --> 01:52:09,000
and you'll get of the

1637
01:52:10,170 --> 01:52:11,070
in those cases

1638
01:52:11,900 --> 01:52:15,010
so the first twenty that's on the screen

1639
01:52:17,180 --> 01:52:19,760
james starting from the top of the

1640
01:52:20,430 --> 01:52:21,300
number replaced

1641
01:52:22,480 --> 01:52:24,120
the can i think about that

1642
01:52:24,630 --> 01:52:27,060
it went for the world

1643
01:52:27,940 --> 01:52:29,660
and that's twenty iteration so here

1644
01:52:29,940 --> 01:52:30,730
questions it

1645
01:52:32,240 --> 01:52:33,100
the first one

1646
01:52:37,360 --> 01:52:41,930
how long will it take until this chain generate a good sample from it

1647
01:52:51,830 --> 01:52:52,630
the change

1648
01:52:58,850 --> 01:52:59,860
sample from me

1649
01:53:05,010 --> 01:53:09,170
i haven't really defined good but i'd like the neighbour and

1650
01:53:11,380 --> 01:53:14,380
the question anyway in the real world people but

1651
01:53:15,510 --> 01:53:16,390
find things that

1652
01:53:18,100 --> 01:53:19,650
how long is it gonna have around what

1653
01:53:24,600 --> 01:53:29,080
andy struggling with the question is another question which is more precise

1654
01:53:29,770 --> 01:53:32,000
how long do you think is gonna be roughly

1655
01:53:37,930 --> 01:53:39,430
well one there

1656
01:53:42,550 --> 01:53:44,260
question in great how long until we

1657
01:53:45,760 --> 01:53:46,520
along you think be

1658
01:53:46,920 --> 01:53:47,750
with it but what

1659
01:53:58,060 --> 01:54:03,740
thank you questions during three are precisely nothing you are probability distribution and you can do it well

1660
01:54:04,360 --> 01:54:05,350
what directly

1661
01:54:06,450 --> 01:54:09,490
tell me what the probability distribution is how long

1662
01:54:10,140 --> 01:54:14,250
so one will out one or two and then you have think about whether

1663
01:54:15,460 --> 01:54:17,970
questions are relevant question one how long do

1664
01:54:18,370 --> 01:54:19,770
we need run the chain or

1665
01:54:20,310 --> 01:54:21,230
nothing wrong

1666
01:54:21,860 --> 01:54:22,920
say at the end

1667
01:54:24,210 --> 01:54:25,760
and so you think you getting a good

1668
01:54:29,910 --> 01:54:30,610
thank you neighbours

1669
01:54:35,780 --> 01:54:37,350
okay any thoughts on

1670
01:54:38,170 --> 01:54:39,370
any other questions

1671
01:54:46,800 --> 01:54:47,710
which questionnaire

1672
01:54:57,880 --> 01:54:59,920
together for example from is saying

1673
01:55:00,310 --> 01:55:01,670
it has to have a chance

1674
01:55:03,530 --> 01:55:04,490
getting quite a long way

1675
01:55:05,060 --> 01:55:08,390
and that's gonna be like wrapping manages to go a long way

1676
01:55:09,270 --> 01:55:10,770
they are suggestion was

1677
01:55:12,180 --> 01:55:13,370
once made about

1678
01:55:13,930 --> 01:55:15,520
ten unlikely things happen

1679
01:55:17,750 --> 01:55:19,090
maybe we'll up there

1680
01:55:19,820 --> 01:55:21,410
so what is our ten

1681
01:55:22,960 --> 01:55:25,970
problem is that after about one thousand that's

1682
01:55:26,580 --> 01:55:29,850
you know what you're saying then you might not be doing all right

1683
01:55:30,600 --> 01:55:33,070
people about ten like it

1684
01:55:33,760 --> 01:55:34,920
i mean interesting

1685
01:55:35,480 --> 01:55:38,720
training what may well be right any other way the

1686
01:55:39,680 --> 01:55:40,930
answering any questions

1687
01:55:42,530 --> 01:55:42,910
and one

1688
01:55:55,390 --> 01:55:55,670
i get

1689
01:55:57,910 --> 01:55:59,310
okay so you're saying we

1690
01:55:59,860 --> 01:56:04,100
have long enough hit a wall when it's probably long enough for example

1691
01:56:04,610 --> 01:56:06,450
all right how long is until we

1692
01:56:07,630 --> 01:56:09,820
what do you think this is the way international

1693
01:56:11,420 --> 01:56:13,430
about a hundred that's why about a hundred

1694
01:56:24,710 --> 01:56:27,310
distance together typical distance you don't

1695
01:56:27,620 --> 01:56:29,230
it's something like the square root

1696
01:56:30,030 --> 01:56:30,820
the number of times

1697
01:56:31,360 --> 01:56:32,800
more on what the let's

1698
01:56:34,740 --> 01:56:35,650
and even though

1699
01:56:37,090 --> 01:56:42,590
it only takes hands the inference operator will you be quite a lot and you

1700
01:56:42,600 --> 01:56:45,220
only that you want inference what you don't wait

1701
01:56:48,750 --> 01:56:49,040
you will

1702
01:56:49,970 --> 01:56:50,760
it will

1703
01:56:51,900 --> 01:56:55,680
so only that one but we're actually expected to take about a hundred

1704
01:56:56,430 --> 01:57:00,360
get a one wall and then you will get all the way and that's the al-nuaimi

1705
01:57:07,300 --> 01:57:11,470
maybe another four hundred five hundred they have a chance getting back

1706
01:57:12,230 --> 01:57:14,470
so let's look the reasoning so

1707
01:57:15,070 --> 01:57:18,790
let's define the distance that the random walk with goal

1708
01:57:20,470 --> 01:57:20,970
we delta

1709
01:57:20,970 --> 01:57:22,970
goes down

1710
01:57:23,940 --> 01:57:25,880
but if we use

1711
01:57:25,900 --> 01:57:27,490
now the combination

1712
01:57:27,500 --> 01:57:32,280
strategy so this is showing this plot showing

1713
01:57:32,330 --> 01:57:34,740
the game that we get full

1714
01:57:34,740 --> 01:57:37,490
four going from two classes to more

1715
01:57:37,540 --> 01:57:42,380
and it's at the same time it chose this is like thing how much we

1716
01:57:42,380 --> 01:57:47,750
can actually improve by using a combination of the combination strategy of using these independent

1717
01:57:47,750 --> 01:57:49,510
physiological features

1718
01:57:51,780 --> 01:57:52,600
so you see

1719
01:57:52,620 --> 01:57:57,350
basically that for most subjects

1720
01:57:57,370 --> 01:58:02,570
that have done more than two class experiments

1721
01:58:02,590 --> 01:58:04,470
first of all

1722
01:58:04,470 --> 01:58:06,930
you know three will improve

1723
01:58:06,980 --> 01:58:12,310
but typically only it only proves if we use the combination strategy on top

1724
01:58:13,210 --> 01:58:16,620
generally you see that that you know

1725
01:58:16,640 --> 01:58:17,370
we can

1726
01:58:17,380 --> 01:58:19,790
drastically improved performance

1727
01:58:19,790 --> 01:58:25,190
in terms of bit decisions so you know we can improve here by fifty percent

1728
01:58:26,280 --> 01:58:28,200
number it's the decision

1729
01:58:28,210 --> 01:58:29,290
i mean

1730
01:58:29,400 --> 01:58:32,840
absolute fifty percent

1731
01:58:32,860 --> 01:58:36,640
now if you if you digest on this plot a little bit longer than you

1732
01:58:36,640 --> 01:58:41,150
see that in most subjects it doesn't make sense to use more than three classes

1733
01:58:41,150 --> 01:58:45,690
because it's very hard for them to imagine more than three classes and it's very

1734
01:58:45,690 --> 01:58:50,840
hard to classify more than three classes in any case it's always good

1735
01:58:51,260 --> 01:58:54,930
and it depends on the subject whether or not

1736
01:58:54,940 --> 01:59:01,690
it makes sense to use a combination strategy between different physiological features all

1737
01:59:01,710 --> 01:59:03,530
or whether you know

1738
01:59:03,570 --> 01:59:04,850
you know where the game

1739
01:59:04,900 --> 01:59:09,500
almost nothing this not significant or whether you gain a lot

1740
01:59:11,450 --> 01:59:14,160
so this was done with

1741
01:59:14,220 --> 01:59:16,570
multiclass CSP

1742
01:59:16,630 --> 01:59:22,860
in there that i introduced with the simultaneous localization

1743
01:59:22,870 --> 01:59:26,640
and combining with with the readiness potential

1744
01:59:26,800 --> 01:59:28,950
and so on and so forth

1745
01:59:30,420 --> 01:59:35,040
what is beyond c CSP and this

1746
01:59:35,100 --> 01:59:39,100
recent work of steven lemm

1747
01:59:39,110 --> 01:59:45,800
that has appeared in i tripoli biomedical engineering so

1748
01:59:45,870 --> 01:59:51,120
one of the things that we ask ourselves in this in this work was well

1749
01:59:51,140 --> 01:59:55,040
CSP is somewhat

1750
01:59:55,050 --> 01:59:58,230
you know

1751
02:00:01,020 --> 02:00:02,450
because we are not

1752
02:00:02,460 --> 02:00:05,540
we just computing spatial filter that's it

1753
02:00:05,590 --> 02:00:08,530
and we doing that for certain frequency band

1754
02:00:08,530 --> 02:00:11,430
maybe it's better to actually

1755
02:00:11,440 --> 02:00:14,690
combine some spatial temporal filter

1756
02:00:14,700 --> 02:00:20,990
could actually gain some additional information this is actually a very simple idea

1757
02:00:21,000 --> 02:00:25,540
and the way this is done this is instead of using the xk and so

1758
02:00:25,540 --> 02:00:29,380
now you have all the mass on this slide in fact should have for the

1759
02:00:29,380 --> 02:00:31,040
order different things

1760
02:00:31,240 --> 02:00:35,110
this is just to confuse URI to keep you awake

1761
02:00:36,760 --> 02:00:38,590
access the data

1762
02:00:38,610 --> 02:00:40,330
OK and you know

1763
02:00:40,340 --> 02:00:44,740
you have some operator which is the shift operator so you have an axe

1764
02:00:44,750 --> 02:00:47,470
at this same time point but also

1765
02:00:48,880 --> 02:00:50,330
minutes tall

1766
02:00:50,360 --> 02:00:53,140
the shift operator for your bad today

1767
02:00:53,190 --> 02:00:58,800
data into into like in the non-linear dynamics sense

1768
02:00:58,800 --> 02:01:04,190
and because you hope that that there's something in the dynamics that you would like

1769
02:01:04,190 --> 02:01:06,240
to use for your filter

1770
02:01:06,240 --> 02:01:10,600
so it changes in the spatial filters slightly

1771
02:01:10,620 --> 02:01:11,970
because this

1772
02:01:12,010 --> 02:01:15,930
o tempo filter component two

1773
02:01:15,970 --> 02:01:22,100
so the idea is just to apply the CSP that that you've seen

1774
02:01:22,140 --> 02:01:26,810
these are multiclass two class whatever to the

1775
02:01:26,850 --> 02:01:28,530
the data that

1776
02:01:30,620 --> 02:01:33,910
and then this gives us

1777
02:01:33,910 --> 02:01:35,740
some projection w

1778
02:01:35,780 --> 02:01:40,450
into the state space because this is the state space expansion of the data

1779
02:01:41,580 --> 02:01:44,560
so in other words we can we can

1780
02:01:44,620 --> 02:01:48,780
basically decompose the projector into some

1781
02:01:48,810 --> 02:01:53,350
equal time project and some delay project

1782
02:01:54,700 --> 02:01:55,370
and so

1783
02:01:55,390 --> 02:01:57,930
if we now take just one

1784
02:01:59,220 --> 02:02:00,600
of this

1785
02:02:02,370 --> 02:02:05,910
applied to xk

1786
02:02:05,930 --> 02:02:08,080
then we can decompose this into

1787
02:02:08,100 --> 02:02:10,890
the spatial component and

1788
02:02:10,930 --> 02:02:13,640
temporal component

1789
02:02:13,680 --> 02:02:15,510
and we can see that

1790
02:02:15,530 --> 02:02:20,120
if we now spell it all out then this

1791
02:02:20,120 --> 02:02:23,140
some spatial filter here

1792
02:02:23,560 --> 02:02:30,740
and so just to get the scum i see is a combination between the w

1793
02:02:32,810 --> 02:02:37,560
and w told well this is some choice that we have to make

1794
02:02:37,600 --> 02:02:41,470
it's a reasonable choice for filtering

1795
02:02:43,390 --> 02:02:46,470
so this is normalized

1796
02:02:46,490 --> 02:02:50,410
and this is basically only containing the spectral filter

1797
02:02:50,410 --> 02:02:54,330
and so so if you if you look at this this is like if if

1798
02:02:54,330 --> 02:02:58,430
i have feel to wear at the zeroth position we have something and then we

1799
02:02:58,430 --> 02:03:03,260
have nothing for minus one steps and then we have something OK

1800
02:03:03,260 --> 02:03:06,280
so if you

1801
02:03:06,300 --> 02:03:08,640
rather like to think about

1802
02:03:10,120 --> 02:03:12,870
so now if if you

1803
02:03:12,890 --> 02:03:16,620
i will have this this quantity on my next slide is just the

1804
02:03:17,620 --> 02:03:19,620
basically the angle phase

1805
02:03:19,660 --> 02:03:23,890
these filters so

1806
02:03:23,910 --> 02:03:26,140
so these

1807
02:03:26,160 --> 02:03:29,030
this is just the way these filters look like

1808
02:03:29,120 --> 02:03:36,620
but maybe it's easier on nicer to look at them as spatial filter so if

1809
02:03:36,620 --> 02:03:39,100
we now to take some

1810
02:03:39,140 --> 02:03:41,990
projection of the class foot

1811
02:03:42,120 --> 02:03:43,910
this is the right hand

1812
02:03:44,700 --> 02:03:48,600
not sure whether you actually see this very well

1813
02:03:50,780 --> 02:03:52,370
so this is the usual

1814
02:03:52,390 --> 02:03:55,390
CSP protection which

1815
02:03:55,430 --> 02:03:56,850
you know gives you

1816
02:03:56,890 --> 02:03:58,350
something over the

1817
02:03:58,390 --> 02:04:01,910
overseas at some high activity

1818
02:04:01,950 --> 02:04:03,410
which is where the foot

1819
02:04:03,410 --> 02:04:06,040
it is located on the motor cortex

1820
02:04:06,100 --> 02:04:06,890
so this is

1821
02:04:06,930 --> 02:04:11,200
makes a lot of sense and we've seen that under a couple of slides before

1822
02:04:11,200 --> 02:04:12,910
but much more colourful

1823
02:04:13,700 --> 02:04:19,160
and this is so to say the second CSP projection because CSP gives you a

1824
02:04:19,160 --> 02:04:20,760
number of rejections

1825
02:04:21,990 --> 02:04:25,430
i kind of knowledge to

1826
02:04:25,450 --> 02:04:28,870
and and this is CSSP

1827
02:04:28,890 --> 02:04:30,760
so common

1828
02:04:30,800 --> 02:04:32,890
spatial spectral

1829
02:04:33,970 --> 02:04:39,010
and you see this is the spatial content which is very similar to the original

1830
02:04:39,010 --> 02:04:46,040
CSP but there's also spectral content which corresponds to the choice of a certain tau

1831
02:04:46,060 --> 02:04:48,220
and if you now look at

1832
02:04:48,280 --> 02:04:53,410
the classification rates began over many many experiments

1833
02:04:53,430 --> 02:04:55,560
then you see that

1834
02:04:55,560 --> 02:04:58,180
with the CSP projection

