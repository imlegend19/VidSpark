1
00:00:00,000 --> 00:00:04,850
generic statements which are generically true about the world now a lot of the power

2
00:00:04,850 --> 00:00:10,780
of unknown come from boring binary ground facts about this particular instance of the world

3
00:00:10,780 --> 00:00:12,740
thinks that just happen to be true

4
00:00:12,760 --> 00:00:16,550
and i think you could even make a point from cognitive science about this a

5
00:00:16,550 --> 00:00:21,040
lot of our intelligence comes not from being smart in general just come from knowing

6
00:00:21,040 --> 00:00:22,460
a lot of stuff

7
00:00:22,470 --> 00:00:25,560
a lot of these big graph

8
00:00:25,570 --> 00:00:30,840
and actually we can be a little bit more precise about the relationship between

9
00:00:30,880 --> 00:00:34,540
the small black circle and the and to be quite graph

10
00:00:34,550 --> 00:00:36,060
so here are

11
00:00:36,070 --> 00:00:39,270
some numbers that some of you may have seen on the screen yesterday

12
00:00:39,780 --> 00:00:45,020
when jacob or by gave his talk about a distributed backward chaining reasoning

13
00:00:45,040 --> 00:00:48,830
and he did the following experiment he took three of the largest datasets that we

14
00:00:49,610 --> 00:00:53,830
so LUBM is a synthetic dataset you can make arbitrarily large

15
00:00:53,870 --> 00:00:55,980
then linked life data

16
00:00:55,990 --> 00:00:58,850
which is about two and a half dozen

17
00:00:59,250 --> 00:01:04,270
the largest life science datasets semantically integrated

18
00:01:04,330 --> 00:01:06,590
and fact forge

19
00:01:06,620 --> 00:01:11,440
which is the PDF plus plus so it's a lot of encyclopedic knowledge to the

20
00:01:11,440 --> 00:01:18,160
pedia plus the internet movie database plus music plus etcetera etcetera and your geography names

21
00:01:18,160 --> 00:01:19,510
and so on

22
00:01:19,520 --> 00:01:21,960
and what you did

23
00:01:21,970 --> 00:01:24,080
it is said OK i'll take the schemas

24
00:01:24,100 --> 00:01:27,550
of those things so that the small black circle and i do the full complete

25
00:01:27,550 --> 00:01:28,730
forward closure

26
00:01:28,730 --> 00:01:33,280
i compute all the inferences and see how long it takes into doing the schema

27
00:01:33,280 --> 00:01:36,570
closure is measured in the order of seconds

28
00:01:37,230 --> 00:01:38,960
or a small number of minutes

29
00:01:38,970 --> 00:01:41,400
in the second experiment was

30
00:01:41,460 --> 00:01:43,620
not only take to the black

31
00:01:43,680 --> 00:01:47,650
the black circle but richard with the big graph and the closure of the whole

32
00:01:48,530 --> 00:01:52,800
and then suddenly the run time is measured in hours

33
00:01:52,820 --> 00:01:58,070
and and by the way notice that we take here somehow the measure of the

34
00:01:58,070 --> 00:02:03,830
size of the circles not just counting the triples but we're taking somehow how complex

35
00:02:03,830 --> 00:02:07,070
it is to do with right so how long does it take to do inference

36
00:02:07,080 --> 00:02:09,510
of i think it's more interesting

37
00:02:10,510 --> 00:02:15,410
of how large the circle should be drawn you see that there is one or

38
00:02:15,410 --> 00:02:18,020
two orders of magnitude in in

39
00:02:18,070 --> 00:02:20,420
the difference in size between

40
00:02:20,440 --> 00:02:23,370
how much it takes to do inference of the white graph or

41
00:02:23,380 --> 00:02:27,150
over the terminological knowledge only

42
00:02:27,290 --> 00:02:29,650
so if we go back to the previous slide

43
00:02:29,670 --> 00:02:31,290
i was like this

44
00:02:31,290 --> 00:02:32,980
these numbers tell us

45
00:02:33,030 --> 00:02:35,470
the slide should really look like this

46
00:02:36,830 --> 00:02:39,850
this is how small terminologies are

47
00:02:39,930 --> 00:02:42,360
compared to our factual knowledge

48
00:02:42,370 --> 00:02:45,470
and it's very easy to imagine that you would blow up the

49
00:02:45,480 --> 00:02:49,460
the factual knowledge that it's very easy to imagine that you make is why draft

50
00:02:50,620 --> 00:02:52,550
but it's much harder to think

51
00:02:52,550 --> 00:02:56,110
and how to arbitrarily make black don't make a right as i was only so

52
00:02:56,110 --> 00:03:03,630
much to acknowledge you could possibly want to write down

53
00:03:06,440 --> 00:03:15,860
so the slogan here is it's a box stupid

54
00:03:17,190 --> 00:03:19,310
that's where the power of our

55
00:03:19,320 --> 00:03:22,210
of our representations are

56
00:03:22,230 --> 00:03:24,750
and again understanding this law

57
00:03:24,760 --> 00:03:30,990
helps us to design artifacts so it's exactly this insight that helps us to do

58
00:03:30,990 --> 00:03:34,540
and distributed reasoning because we can see that

59
00:03:34,570 --> 00:03:37,940
because the closure is so small and the black dot is so small we can

60
00:03:37,940 --> 00:03:40,520
easily replicated across all computing nodes

61
00:03:40,610 --> 00:03:44,570
it's more you can keep it in memory everywhere but at crucial key insight from

62
00:03:44,570 --> 00:03:49,660
this law that you need to know in order to make this informed to design

63
00:03:54,050 --> 00:03:58,280
OK let's look a little bit more this notion of of how complex this terminological

64
00:03:58,280 --> 00:03:59,130
knowledge is

65
00:03:59,190 --> 00:04:04,310
in the next claim is of course to political knowledge is not only small

66
00:04:04,310 --> 00:04:05,550
there's little red

67
00:04:05,560 --> 00:04:09,050
compared to factual knowledge but it's also a very low complexity

68
00:04:11,840 --> 00:04:12,600
what we have

69
00:04:12,620 --> 00:04:14,330
looking at something like how

70
00:04:14,360 --> 00:04:16,810
right OWL is is

71
00:04:17,180 --> 00:04:25,940
it's it's it's strong highly expressive language but depending on where you look now we

72
00:04:25,940 --> 00:04:31,500
either use such as a big expressive language or

73
00:04:31,510 --> 00:04:34,810
and that was of course very often we only use a very small part of

74
00:04:34,810 --> 00:04:41,390
it but somehow there is this unreasonable effectiveness of low expressivity knowledge representation

75
00:04:41,420 --> 00:04:42,680
it is much

76
00:04:42,690 --> 00:04:48,100
more x effective then you would have expected based on on the on the history

77
00:04:48,100 --> 00:04:51,340
of of a research for example

78
00:04:51,400 --> 00:04:55,680
and of course it's nice that we have not only the small language that we

79
00:04:55,680 --> 00:04:56,590
use a lot of the time

80
00:04:57,060 --> 00:05:00,820
it's nice that we also have the expressive language is for the case when we

81
00:05:00,820 --> 00:05:01,710
need it

82
00:05:01,720 --> 00:05:03,620
and there are cases when we need it

83
00:05:03,650 --> 00:05:07,660
so it's not a one zero distribution in the way this is

84
00:05:07,680 --> 00:05:13,210
this is in no way zones if distribution that the vast majority of ontologies are

85
00:05:13,210 --> 00:05:21,570
very lightweight and is a tale of of heavyweight ontologies and it's nice that we

86
00:05:21,570 --> 00:05:24,380
can use these expressive parts if you want to and there's even a bit more

87
00:05:24,380 --> 00:05:27,660
good news there that even though these expressive

88
00:05:27,730 --> 00:05:31,250
languages have very scary complexity measures

89
00:05:31,270 --> 00:05:36,300
in practice though scary complexity measures those worst case complexity measures don't really hit this

90
00:05:36,300 --> 00:05:37,360
in practice

91
00:05:37,510 --> 00:05:43,610
so there was a beautiful talk yesterday by how by student from manchester difficult you

92
00:05:43,620 --> 00:05:52,130
name apologies about how to classify highly expressive ontologies very efficiently i anyway laughing at

93
00:05:52,130 --> 00:05:53,670
the worst case complexity to

94
00:05:54,060 --> 00:05:58,430
the real k the average case complexity is is lower and again this is something

95
00:05:58,430 --> 00:06:02,310
useful to know where you are informing your what something useful to know what you

96
00:06:02,310 --> 00:06:03,790
are designing your defects

97
00:06:03,890 --> 00:06:08,130
know in the worst case complexity bounds are just that worst case

98
00:06:08,150 --> 00:06:13,050
complexity bounds and apparently the information universe is not always worst case

99
00:06:17,250 --> 00:06:23,550
now of course is a key topic in our community is heterogeneity right and then

100
00:06:23,550 --> 00:06:24,800
we know

101
00:06:24,850 --> 00:06:27,230
that originate eighty is unavoidable

102
00:06:27,230 --> 00:06:30,280
it's for good reasons that shows the tower of babel

103
00:06:30,300 --> 00:06:32,780
the metaphor for the vocabulary

104
00:06:32,860 --> 00:06:34,460
that we write

105
00:06:34,490 --> 00:06:36,900
and i think where

106
00:06:37,220 --> 00:06:39,590
something new that we've discovered

107
00:06:39,590 --> 00:06:41,360
column two

108
00:06:41,380 --> 00:06:43,940
OK so this fish that screen

109
00:06:43,960 --> 00:06:46,410
and what is the a correlated equilibrium

110
00:06:46,410 --> 00:06:47,860
it's an equilibrium

111
00:06:47,870 --> 00:06:49,260
it is distribution

112
00:06:49,280 --> 00:06:51,350
or fish which

113
00:06:51,360 --> 00:06:54,010
where you're best off following the fish's advice

114
00:06:54,030 --> 00:06:57,160
so let's say we know the distribution of the fish using

115
00:06:57,190 --> 00:06:58,730
and the fish suggests that

116
00:06:58,740 --> 00:07:01,110
so just to play a role one

117
00:07:01,260 --> 00:07:04,830
now this distribution i've drawn in this picture so what is this game just explain

118
00:07:04,830 --> 00:07:08,060
this so this game would reach choosing

119
00:07:08,130 --> 00:07:10,590
a row and column numbers between one and three

120
00:07:10,670 --> 00:07:15,310
my goal is obviously to match your number if i matching number then i get

121
00:07:16,930 --> 00:07:22,420
your goal is to be one bigger than my number or if it's three wrap

122
00:07:22,420 --> 00:07:25,500
one so your goal is to be off by one from my number and if

123
00:07:25,500 --> 00:07:27,250
you're off by one one bigger

124
00:07:27,260 --> 00:07:29,430
then you get one i get nothing

125
00:07:30,110 --> 00:07:33,630
so this is a very competitive game it's not a zero-sum game

126
00:07:33,680 --> 00:07:37,490
but it's a competitive game where only one of us can win but sometimes need

127
00:07:37,490 --> 00:07:39,650
arose when

128
00:07:39,770 --> 00:07:44,790
OK so now let's say that the fisher suggested has actually picked from this table

129
00:07:44,790 --> 00:07:50,150
according to this distribution which is basically choosing an entry which is not zero zero

130
00:07:50,510 --> 00:07:52,950
making sure that one of us will win at random but not telling us which

131
00:07:54,250 --> 00:07:57,890
and he told me now that i should play a role one

132
00:07:58,010 --> 00:08:00,940
now given that he told me that i should play a role one

133
00:08:01,030 --> 00:08:03,910
knowing how this page works

134
00:08:03,940 --> 00:08:08,830
i see that that probably he told the opponent to play

135
00:08:08,830 --> 00:08:11,600
for sure he told appointed by the column one column two

136
00:08:11,980 --> 00:08:17,220
in fact if he's really being faithful to distribution then it is equally likely that

137
00:08:17,220 --> 00:08:20,600
he's playing column one column two

138
00:08:20,600 --> 00:08:22,400
given that he told me

139
00:08:22,420 --> 00:08:26,150
to play a role one easy and another opponent disposal is equally likely to play

140
00:08:26,480 --> 00:08:29,610
columns one column two

141
00:08:29,620 --> 00:08:33,680
i'm actually best off sticking with his advice

142
00:08:33,710 --> 00:08:37,170
o the point fifty fifty here if i play role one get half if i

143
00:08:37,170 --> 00:08:41,080
play o two are going to have a three again zero so

144
00:08:41,090 --> 00:08:43,280
it is a good idea to follow his advice

145
00:08:43,330 --> 00:08:47,430
this fish is called the correlation device and is helping us play the game and

146
00:08:47,430 --> 00:08:49,510
actually is you see we're getting more money

147
00:08:49,540 --> 00:08:51,920
then we would have had we just

148
00:08:51,930 --> 00:08:55,840
probably if we just play the game regularly to fish the only equilibrium is reached

149
00:08:56,020 --> 00:09:00,270
randomized in the third of the time of us make money so each only make

150
00:09:00,270 --> 00:09:02,680
money third in this case making have

151
00:09:02,690 --> 00:09:07,140
the fish can really help in this cause this idea of a correlation devices really

152
00:09:07,140 --> 00:09:10,450
there in the real world as well like a traffic signal

153
00:09:10,630 --> 00:09:13,140
presume we don't both want to go through the intersection

154
00:09:13,230 --> 00:09:15,970
if there were no traffic signal we would have to sort of

155
00:09:17,910 --> 00:09:21,380
we have to do something else but this traffic signatures that one of us because

156
00:09:21,590 --> 00:09:23,850
and the other doesn't

157
00:09:23,860 --> 00:09:26,640
in this idea is is by

158
00:09:26,660 --> 00:09:28,950
the great game named

159
00:09:30,960 --> 00:09:34,640
it's been around for well it's not quite as popular as nash equilibrium but has many

160
00:09:34,650 --> 00:09:36,850
nice properties that the nash equilibrium

161
00:09:40,770 --> 00:09:49,460
OK so again we have this notion of correlated with many questions about political events

162
00:09:50,490 --> 00:09:52,120
OK so now

163
00:09:54,170 --> 00:09:56,810
this internal regret condition

164
00:09:56,820 --> 00:09:59,630
if you have no internal regret that actually

165
00:09:59,650 --> 00:10:05,080
a statement about you playing a correlated equilibrium which is very surprising to me because

166
00:10:05,080 --> 00:10:08,970
there is no correlation device there's no fish were playing this game we're just playing

167
00:10:08,970 --> 00:10:12,620
a game i'm making actions you make it is officially what to do

168
00:10:12,630 --> 00:10:16,250
so let's think about the game played the sequence of rows and columns who knows

169
00:10:17,340 --> 00:10:22,180
in hindsight we have no internal regret which means that i wouldn't rather

170
00:10:23,850 --> 00:10:25,420
that means i don't regret

171
00:10:25,440 --> 00:10:28,640
changing all my ones two threes or vice versa

172
00:10:28,710 --> 00:10:30,860
any other pair of countries

173
00:10:30,880 --> 00:10:36,010
well you know if you think about this sequence not a sequence but is actually

174
00:10:36,010 --> 00:10:41,350
a distribution over cells in this game if you actually draw from a distribution

175
00:10:41,350 --> 00:10:44,000
need another letter for distribution

176
00:10:44,030 --> 00:10:46,900
now i ran out of letters i just took p

177
00:10:46,900 --> 00:10:53,960
the picture of p p so he's so if you have another distribution p

178
00:10:54,010 --> 00:10:58,960
which is the distribution over the cells in the game that we actually played

179
00:10:58,980 --> 00:11:03,370
and this will be correlated equilibrium if we have no internal regret

180
00:11:03,410 --> 00:11:07,780
because again no internal regret means that the thing about this is the distribution with

181
00:11:07,830 --> 00:11:10,980
mean distribution someone came and chosen random

182
00:11:11,000 --> 00:11:12,390
one of these

183
00:11:12,390 --> 00:11:13,580
random time

184
00:11:13,600 --> 00:11:15,260
between one of our

185
00:11:15,290 --> 00:11:19,500
play around in this game and passes to play according to that

186
00:11:19,510 --> 00:11:22,940
we have no internal regret means when when i'm advised to play

187
00:11:22,990 --> 00:11:28,090
column row two i will play to my best response

188
00:11:28,090 --> 00:11:32,860
and that's exactly the same as this correlated equilibrium

189
00:11:32,880 --> 00:11:38,460
so it turns out that there are very natural algorithms for getting no internal regret

190
00:11:38,510 --> 00:11:41,280
i don't have time to talk about them but you can use the weighted majority

191
00:11:41,360 --> 00:11:42,640
in combination with

192
00:11:42,690 --> 00:11:47,130
some tricks to get a no internal regret algorithm and this means that you end

193
00:11:47,130 --> 00:11:48,960
up playing a game

194
00:11:48,960 --> 00:11:52,060
only found that really is look up

195
00:11:52,540 --> 00:11:53,930
factor two

196
00:11:53,970 --> 00:11:55,700
no i

197
00:11:55,830 --> 00:11:59,090
also question answering in current

198
00:11:59,110 --> 00:12:02,940
can be classified in this category

199
00:12:02,950 --> 00:12:08,840
but of course more and more and we are dealing with information

200
00:12:08,850 --> 00:12:13,600
you can try to acquire the knowledge base knowledge of the

201
00:12:13,620 --> 00:12:15,760
from the from

202
00:12:15,790 --> 00:12:21,170
the kind of unstructured data sources such as text or other

203
00:12:21,280 --> 00:12:24,740
you can try to aggregate to compare

204
00:12:25,120 --> 00:12:29,180
sort of network analyzers you can change

205
00:12:29,220 --> 00:12:32,500
and eventually you can go much more

206
00:12:32,530 --> 00:12:37,080
you can have your system to predict recessions or so

207
00:12:37,150 --> 00:12:41,860
and really really perform intelligence functions like synthesis

208
00:12:41,870 --> 00:12:44,710
discovery predicting

209
00:12:45,000 --> 00:12:47,500
so for crossing

210
00:12:51,860 --> 00:12:53,760
but of course

211
00:12:53,790 --> 00:12:56,150
so far

212
00:12:56,220 --> 00:12:59,580
well we are all the machine

213
00:13:02,460 --> 00:13:07,300
automatic but that we might

214
00:13:07,310 --> 00:13:12,970
maybe i that is also the beginning of the first lecture

215
00:13:13,400 --> 00:13:14,860
why now

216
00:13:14,880 --> 00:13:18,800
this any information that is that

217
00:13:19,600 --> 00:13:23,570
with regard to information extraction and text mining

218
00:13:23,610 --> 00:13:26,150
well what is the claimed

219
00:13:26,190 --> 00:13:33,960
they are both immature information quite a long time also

220
00:13:33,980 --> 00:13:35,970
information extraction

221
00:13:35,980 --> 00:13:41,100
now we can see that might have established technologies

222
00:13:42,040 --> 00:13:45,070
why how can

223
00:13:46,860 --> 00:13:49,300
i think i

224
00:13:49,330 --> 00:13:51,220
it is compatible with it

225
00:13:51,240 --> 00:13:55,430
multimedia when try i also

226
00:13:57,740 --> 00:14:02,430
an object to recognise the information in the data

227
00:14:02,440 --> 00:14:05,540
might be person might be object

228
00:14:05,550 --> 00:14:08,900
might be more categories while some

229
00:14:08,920 --> 00:14:10,540
the classes

230
00:14:10,550 --> 00:14:16,580
it's like being on the other side of the north sea off birthday party

231
00:14:17,460 --> 00:14:18,040
so we

232
00:14:18,200 --> 00:14:19,870
get used

233
00:14:19,880 --> 00:14:27,210
semantically system for and this is also something we could do text why not

234
00:14:27,220 --> 00:14:29,360
also for question

235
00:14:30,400 --> 00:14:32,650
but sometimes it it's today

236
00:14:32,660 --> 00:14:35,830
and next to search

237
00:14:36,440 --> 00:14:40,710
well looking at kind of agenda setting

238
00:14:40,720 --> 00:14:42,570
what we've been looking at

239
00:14:44,910 --> 00:14:49,130
so will try to integrate information extraction

240
00:14:49,190 --> 00:14:51,920
taking into the true model

241
00:14:51,920 --> 00:14:53,130
the careful

242
00:14:53,210 --> 00:14:56,880
if the angle is positive i call this the positive angle

243
00:14:57,840 --> 00:14:59,930
the average velocity is positive

244
00:14:59,980 --> 00:15:01,320
but if i

245
00:15:01,330 --> 00:15:02,690
have negative angle

246
00:15:04,310 --> 00:15:07,380
average velocity would be negative for instance

247
00:15:07,420 --> 00:15:10,090
between t four and t five

248
00:15:10,180 --> 00:15:12,020
if i draw this line

249
00:15:12,090 --> 00:15:14,630
then this angle here

250
00:15:15,760 --> 00:15:18,940
so the average velocity between p four and p five

251
00:15:19,020 --> 00:15:20,300
is now

252
00:15:24,000 --> 00:15:27,310
again if i had to change the zero point

253
00:15:27,320 --> 00:15:31,130
you would have found the same values for the average velocity the only difference would

254
00:15:31,130 --> 00:15:32,000
have been

255
00:15:32,010 --> 00:15:33,960
the position of the curve

256
00:15:33,980 --> 00:15:34,770
in that

257
00:15:38,240 --> 00:15:41,400
there's a very big difference in physics between speed

258
00:15:41,420 --> 00:15:44,250
and velocity

259
00:15:44,300 --> 00:15:48,130
the average velocity between time t one and t five

260
00:15:48,170 --> 00:15:49,430
is zero

261
00:15:49,460 --> 00:15:51,000
with the average speed

262
00:15:51,010 --> 00:15:52,840
is not

263
00:15:52,860 --> 00:15:55,680
the average speed

264
00:15:55,750 --> 00:15:57,420
it is defined

265
00:15:58,740 --> 00:16:02,000
the distance travelled

266
00:16:02,070 --> 00:16:06,500
divided by the time that it takes to travel that distance

267
00:16:06,580 --> 00:16:08,000
one is the distance

268
00:16:08,050 --> 00:16:10,210
that the object travelled between

269
00:16:10,260 --> 00:16:13,580
time t one and time t five

270
00:16:13,630 --> 00:16:15,620
well the object

271
00:16:15,630 --> 00:16:19,760
i started at the position here on these x axis

272
00:16:19,770 --> 00:16:21,520
and then it went up

273
00:16:21,620 --> 00:16:23,750
reach the highest point year

274
00:16:23,790 --> 00:16:26,310
so i make a drawing for you here

275
00:16:26,370 --> 00:16:28,700
which the highest point here

276
00:16:28,770 --> 00:16:31,210
and it went down

277
00:16:31,300 --> 00:16:33,130
and then when it went here

278
00:16:33,180 --> 00:16:35,480
it went up again

279
00:16:35,590 --> 00:16:36,990
comes down again

280
00:16:37,000 --> 00:16:38,260
and its back

281
00:16:38,440 --> 00:16:40,740
in order to find the average speed

282
00:16:40,750 --> 00:16:43,950
you wouldn't have to know exactly what this distances

283
00:16:43,960 --> 00:16:45,770
and at this distance

284
00:16:45,820 --> 00:16:48,500
and at this distance and this this

285
00:16:48,550 --> 00:16:53,300
and if that distance altogether were for instance three hundred metres

286
00:16:53,310 --> 00:16:54,430
and if

287
00:16:54,440 --> 00:16:55,920
the time between

288
00:16:55,930 --> 00:16:57,310
one ninety five

289
00:16:57,320 --> 00:16:58,890
for three seconds

290
00:16:58,900 --> 00:17:01,940
then the average speed would be three hundred metres

291
00:17:01,940 --> 00:17:06,700
divided by three seconds that would be hundred meters per second so the average speed

292
00:17:06,700 --> 00:17:08,480
would be hundred meters per second

293
00:17:08,510 --> 00:17:11,190
yet the average velocity would be

294
00:17:14,520 --> 00:17:17,950
if you look at the location

295
00:17:19,140 --> 00:17:21,170
and t two

296
00:17:21,180 --> 00:17:24,310
and i bring the three closer and closer

297
00:17:26,760 --> 00:17:30,110
then this angle of all five will increase

298
00:17:30,200 --> 00:17:34,070
and i can go to the extreme that i bring t three

299
00:17:34,080 --> 00:17:36,580
almost right at t two

300
00:17:36,590 --> 00:17:38,840
the angle of of fire will then be

301
00:17:40,170 --> 00:17:43,180
to this point

302
00:17:43,190 --> 00:17:44,690
this will then be

303
00:17:44,750 --> 00:17:47,390
my angle of

304
00:17:47,440 --> 00:17:49,540
and now you will understand

305
00:17:49,550 --> 00:17:51,020
how we define

306
00:17:51,150 --> 00:17:53,330
instantaneous velocity

307
00:17:53,390 --> 00:17:54,710
at the time

308
00:17:55,590 --> 00:17:57,000
which is different from

309
00:17:57,050 --> 00:18:00,730
average velocity between two time intervals

310
00:18:04,890 --> 00:18:07,290
and i pick a random time t

311
00:18:07,900 --> 00:18:10,580
equals the limiting case

312
00:18:10,630 --> 00:18:11,740
four x

313
00:18:11,770 --> 00:18:14,800
measured at time t plus delta t

314
00:18:14,810 --> 00:18:16,630
minus x

315
00:18:16,680 --> 00:18:18,810
measured time t

316
00:18:18,840 --> 00:18:20,830
divided by delta t

317
00:18:20,880 --> 00:18:23,200
and i do that for delta t

318
00:18:24,070 --> 00:18:25,090
two zero

319
00:18:25,110 --> 00:18:26,170
think this

320
00:18:26,190 --> 00:18:30,170
think of this as being the three and this as a tool i bring t

321
00:18:30,950 --> 00:18:33,520
closer and closer and closer to t two

322
00:18:33,540 --> 00:18:36,440
and the time between them then goes to zero

323
00:18:36,490 --> 00:18:37,750
and this

324
00:18:37,840 --> 00:18:42,380
is something that you undoubtedly recognise that's the first derivative

325
00:18:42,380 --> 00:18:45,000
after position versus time

326
00:18:45,130 --> 00:18:46,790
and now comes in equation

327
00:18:46,810 --> 00:18:48,930
which is one of the very few

328
00:18:48,940 --> 00:18:50,950
that i want you to remember

329
00:18:50,960 --> 00:18:53,300
in fact in a one

330
00:18:54,770 --> 00:18:56,580
the x dt

331
00:18:56,590 --> 00:18:57,600
this is one

332
00:18:57,610 --> 00:18:58,410
thank you

333
00:18:58,430 --> 00:19:02,300
you must remember not only in a one but for the rest of your time

334
00:19:02,300 --> 00:19:03,390
at MIT

335
00:19:03,440 --> 00:19:05,320
and this could be larger than zero

336
00:19:05,330 --> 00:19:06,520
this could be zero

337
00:19:06,530 --> 00:19:08,640
and this could be smaller than zero

338
00:19:08,660 --> 00:19:10,440
if the angle of o five

339
00:19:10,510 --> 00:19:12,420
the tangential is positive

340
00:19:12,460 --> 00:19:16,180
then it is a positive value if it is negative however

341
00:19:16,190 --> 00:19:19,050
you here then it is negative philosophy

342
00:19:19,060 --> 00:19:21,440
and if the angle of alpha is zero

343
00:19:23,900 --> 00:19:25,650
the velocity is zero

344
00:19:25,730 --> 00:19:27,850
so if we now look at this plot

345
00:19:27,910 --> 00:19:31,930
we can search for the times that the velocity is zero

346
00:19:31,940 --> 00:19:36,090
so you have to look for the derivative being zero that means the

347
00:19:36,150 --> 00:19:37,930
angle of being zero

348
00:19:37,940 --> 00:19:39,510
clearly here

349
00:19:39,520 --> 00:19:41,280
the velocity is zero

350
00:19:41,360 --> 00:19:43,690
right here this turning point

351
00:19:43,780 --> 00:19:46,440
that means many objective here is zero

352
00:19:46,490 --> 00:19:50,430
when the object is here it is again zero at this moment in time

353
00:19:50,540 --> 00:19:52,390
then the angle is zero

354
00:19:52,430 --> 00:19:55,140
and it is again zero here so those are the times

355
00:19:55,180 --> 00:19:59,790
that's the velocity is zero one hundred times that the velocities positive

356
00:20:00,670 --> 00:20:02,020
positive here

357
00:20:02,300 --> 00:20:03,740
it is positive here

358
00:20:03,760 --> 00:20:06,920
still positive positive becomes negative

359
00:20:06,930 --> 00:20:11,280
negative positive zero and negative

360
00:20:11,310 --> 00:20:14,920
that's the definition of the instantaneous

361
00:20:17,730 --> 00:20:20,780
once the instantaneous speed

362
00:20:22,300 --> 00:20:24,940
b is not time-sensitive

363
00:20:24,990 --> 00:20:28,460
suppose that the velocity here

364
00:20:28,480 --> 00:20:30,340
just cycle that v one

365
00:20:30,390 --> 00:20:35,690
suppose that was class thirty meters per second i just grabbed this number

366
00:20:35,690 --> 00:20:36,840
out of the blue

367
00:20:36,850 --> 00:20:39,620
and suppose you somewhere

368
00:20:39,640 --> 00:20:41,720
it was i call that v two

369
00:20:41,730 --> 00:20:45,120
suppose that was minus one hundred meters per second

370
00:20:45,180 --> 00:20:46,270
this is negative

371
00:20:46,310 --> 00:20:49,830
and this is positive then we would have to say in physics whether you like

372
00:20:49,830 --> 00:20:53,450
it or not it's not very pleasing but you would have to say that this

373
00:20:53,450 --> 00:20:55,450
velocity is lower than that one

374
00:20:55,540 --> 00:20:57,100
because minus hundred

375
00:20:57,170 --> 00:20:58,770
is low and employs thirty

376
00:20:58,780 --> 00:21:00,570
what this means of course

377
00:21:00,580 --> 00:21:04,360
is higher here because the speed is the

378
00:21:04,370 --> 00:21:05,930
the magnitude of

379
00:21:06,530 --> 00:21:11,530
velocity and is not signed sensitive so this has the highest bid two hundred meters

380
00:21:11,530 --> 00:21:13,810
per second and this has a lower speed

381
00:21:13,850 --> 00:21:19,430
but this has the lowest velocity it just an algebraic game but very important when

382
00:21:19,430 --> 00:21:22,770
you make your calculations

383
00:21:22,810 --> 00:21:24,370
i have always

384
00:21:24,440 --> 00:21:25,840
i one wondered

385
00:21:25,850 --> 00:21:27,490
what the

386
00:21:27,550 --> 00:21:31,700
average speed only average velocity is of a bullet

387
00:21:31,760 --> 00:21:34,780
now i want you to realize i'm not a fan of guns at all

388
00:21:34,900 --> 00:21:36,840
but it always intrigued me

389
00:21:36,860 --> 00:21:38,300
how can i measure

390
00:21:38,310 --> 00:21:39,900
the average speed

391
00:21:39,910 --> 00:21:41,320
of a

392
00:21:42,370 --> 00:21:44,810
and i've discussed it with some people here

393
00:21:44,850 --> 00:21:46,800
and we came up with

394
00:21:46,810 --> 00:21:49,050
easy way to do

395
00:21:50,010 --> 00:21:51,070
had wire

396
00:21:52,060 --> 00:21:53,930
goes into the blackboards

397
00:21:53,960 --> 00:21:55,280
y one

398
00:21:55,290 --> 00:21:59,010
and we have another wind it goes into the black or white two

399
00:21:59,060 --> 00:22:00,810
and the separation

400
00:22:00,850 --> 00:22:01,690
it is

401
00:22:02,660 --> 00:22:04,700
i mean we have to measure that

402
00:22:04,800 --> 00:22:06,410
set up this year

403
00:22:06,440 --> 00:22:08,030
so this is

404
00:22:08,040 --> 00:22:09,910
why are number one

405
00:22:09,920 --> 00:22:13,240
and this is why are number two

406
00:22:13,240 --> 00:22:22,750
OK let's get started

407
00:22:31,060 --> 00:22:35,590
so welcome back everybody to the second part of my lecture on

408
00:22:35,670 --> 00:22:42,060
machine learning about magic i hope you enjoy the talk they had some might break

409
00:22:42,200 --> 00:22:46,670
out of the paper and now fresh water

410
00:22:46,680 --> 00:22:49,880
OK i would like to start recently

411
00:22:49,920 --> 00:22:57,850
are repeating that what happened lectures that many many things in between just repeatedly

412
00:22:57,860 --> 00:23:04,680
OK so therefore parts we can actually covered the hive introduction to bioinformatics

413
00:23:04,690 --> 00:23:09,640
and i started talking about being can today i'm going to talk about large scale

414
00:23:09,640 --> 00:23:12,000
data structures

415
00:23:12,180 --> 00:23:16,680
to to be used in conjunction with think about it which in data and what

416
00:23:16,680 --> 00:23:19,440
structure of learning so it's quite some

417
00:23:19,450 --> 00:23:21,800
topics covered

418
00:23:22,030 --> 00:23:28,000
OK so i guess the main point for the introduction to bioinformatics was that there

419
00:23:28,000 --> 00:23:33,190
are many different data types and at least four five of them i mentioned that

420
00:23:34,310 --> 00:23:36,560
protein networks

421
00:23:36,700 --> 00:23:42,960
and sequences genomic sequences population they expression data so they come in different data types

422
00:23:42,960 --> 00:23:45,170
depending on the ground

423
00:23:45,220 --> 00:23:47,960
speedy according to the coordinate

424
00:23:47,980 --> 00:23:52,270
in the end we evaluate metric and you would like to inference back to do

425
00:23:52,270 --> 00:23:54,060
in the top

426
00:23:54,080 --> 00:23:55,640
OK so this was

427
00:23:55,690 --> 00:23:57,600
one of the main point

428
00:23:57,690 --> 00:24:03,080
the main point was that something like called the central dogma i hope you remember

429
00:24:03,190 --> 00:24:10,830
something about that being a story told information almost all the information and then this

430
00:24:10,830 --> 00:24:17,640
is called transcription which copies from nineteen eight to them on a and then b

431
00:24:17,720 --> 00:24:22,480
some post postprocessing step of them on a and of like things

432
00:24:22,600 --> 00:24:29,250
and finally the which is called translation that produces the protein the protein are proteins

433
00:24:29,250 --> 00:24:33,650
are the active component in the cell detecting which it with each other

434
00:24:34,760 --> 00:24:37,530
on a projective plane

435
00:24:37,540 --> 00:24:42,070
i mean it acting with

436
00:24:42,100 --> 00:24:49,120
and then were you have to apply things like will in and then introduced

437
00:24:49,130 --> 00:24:53,340
not and and with eighty

438
00:24:53,350 --> 00:24:56,840
OK anybody what these movies

439
00:24:56,900 --> 00:25:01,380
OK me that it can

440
00:25:02,340 --> 00:25:08,810
it so i talked about at the running example recognition of but right

441
00:25:08,850 --> 00:25:14,220
and if you are interested in classifying sequences of this type so we have the

442
00:25:14,220 --> 00:25:19,540
a gene sequence in the middle this boundary between intron exon

443
00:25:19,630 --> 00:25:21,940
always having the ATT here

444
00:25:21,970 --> 00:25:26,410
and then we try to learn rules which can but i think the

445
00:25:26,460 --> 00:25:32,340
which of course by five other retirement i mean like one other appearances of AG

446
00:25:32,340 --> 00:25:33,530
which are not lie

447
00:25:34,340 --> 00:25:36,530
positive and negative

448
00:25:36,540 --> 00:25:42,440
and i guess today you have the introduction into the water pressure and you probably

449
00:25:42,440 --> 00:25:46,500
have seen pictures like this and i think i missed points

450
00:25:48,340 --> 00:25:53,650
first introduced this one two features the sequence and before and after the eighty and

451
00:25:53,650 --> 00:25:58,150
then came up with some non-linear decision boundary but then we discussed several approaches for

452
00:25:59,750 --> 00:26:05,030
more powerful decision boundary maybe not based on who features what features

453
00:26:05,100 --> 00:26:07,440
and we talked about it

454
00:26:07,460 --> 00:26:11,470
which used much more features than just two

455
00:26:11,510 --> 00:26:17,840
and most importantly the spectrum kernel spectrum kernel

456
00:26:18,380 --> 00:26:25,230
it's essentially looking at all puritans came appearing in the sequence OK and became stretch

457
00:26:25,250 --> 00:26:27,310
of sequence

458
00:26:28,310 --> 00:26:31,420
if you look at k nucleotides or

459
00:26:31,430 --> 00:26:36,220
OK i mean the role nancy which combination here in your

460
00:26:36,270 --> 00:26:37,540
what in sequence

461
00:26:37,550 --> 00:26:41,100
and to compute the kernel interested in all the pain

462
00:26:41,150 --> 00:26:45,580
when you the one who has been attributed the to c

463
00:26:49,180 --> 00:26:56,010
another important question would be great degree kernel has the

464
00:26:56,100 --> 00:27:00,740
it is different in the sense that it takes position positional information into account for

465
00:27:00,740 --> 00:27:04,970
the spectrum kernel not take position information it just takes

466
00:27:05,030 --> 00:27:09,020
it just makes it kind of statistics about all the payment which appear in the

467
00:27:09,020 --> 00:27:13,660
sequence but but it doesn't use the the information we have here different for the

468
00:27:13,660 --> 00:27:15,140
weighted degree kernel

469
00:27:15,170 --> 00:27:16,820
so o

470
00:27:17,050 --> 00:27:18,780
but we can out

471
00:27:18,780 --> 00:27:20,570
formulation which i talk about

472
00:27:20,610 --> 00:27:24,580
and then that we become which which allows some

473
00:27:24,590 --> 00:27:27,970
my notion of sequent against each other

474
00:27:29,700 --> 00:27:32,370
OK so i mean i showed you the text

475
00:27:32,390 --> 00:27:35,210
small simulation results where

476
00:27:35,640 --> 00:27:42,680
we compared what life recognition spectrum kernel to compute the area under the policy can

477
00:27:42,740 --> 00:27:43,960
be computed the

478
00:27:44,010 --> 00:27:50,300
four different ringling for up for the kernel and four different if we were to

479
00:27:50,330 --> 00:27:53,540
because of the computer the out

480
00:27:53,550 --> 00:27:58,100
and we've seen of course of the position can become much better than the position

481
00:27:58,100 --> 00:28:00,270
independent code for the

482
00:28:00,320 --> 00:28:01,720
purpose of life

483
00:28:01,740 --> 00:28:10,180
OK so i added to the side the book which i used to generate figure

484
00:28:10,200 --> 00:28:14,120
if we can just play around with that so that actually there is that there

485
00:28:14,140 --> 00:28:20,020
these which are used to generate interest i think that link on the wiki to

486
00:28:20,020 --> 00:28:24,340
some other page and download the data and

487
00:28:24,970 --> 00:28:29,780
so i had something more to play say about

488
00:28:29,830 --> 00:28:31,480
i would like to show you some

489
00:28:31,490 --> 00:28:33,760
i mean what impact any more now

490
00:28:34,410 --> 00:28:37,910
also some scripts which can also download from the wiki page

491
00:28:37,920 --> 00:28:39,930
OK so

492
00:28:40,230 --> 00:28:47,530
this is an artificial dataset i artificially generated hundred sequences each of length two hundred

493
00:28:49,140 --> 00:28:54,970
and if if think a hundred and fifty positive and fifty negative so the negative

494
00:28:54,970 --> 00:28:57,090
see what users actually do

495
00:28:57,100 --> 00:29:02,830
and then based on what we observe try to make additional recommendations try to do

496
00:29:03,340 --> 00:29:11,540
automatic personalisation of certain things you know two things specifically for that user to

497
00:29:11,560 --> 00:29:13,270
you know improved

498
00:29:13,280 --> 00:29:14,730
according to some

499
00:29:14,740 --> 00:29:19,900
measure right i mean of course and recommendation we would like to maybe recommend items

500
00:29:19,900 --> 00:29:23,550
that are valuable to the user in any common setting we might actually want to

501
00:29:23,550 --> 00:29:28,080
recommend things that users wanted to buy and so on and so forth so and

502
00:29:28,100 --> 00:29:31,990
the idea is that the user would not need to actually issuing a query right

503
00:29:31,990 --> 00:29:34,900
like in information retrieval the user wouldn't be

504
00:29:34,950 --> 00:29:38,240
it would be necessary to use as you know i'm looking for this and that

505
00:29:38,580 --> 00:29:42,850
but rather that we can do it somehow implicitly on what what we observe about

506
00:29:42,850 --> 00:29:46,920
that user and that would be good because the user doesn't have to specify it

507
00:29:46,920 --> 00:29:50,350
and also we can make a recommendation even if the user doesn't want them so

508
00:29:50,350 --> 00:29:53,100
that's also good e-commerce setting isn't it

509
00:29:53,970 --> 00:29:57,290
so that's the idea you know what what types types of things we can do

510
00:29:57,290 --> 00:30:03,590
without actually query and instead of using things like building user profiles

511
00:30:03,610 --> 00:30:07,960
so of course you know this is this is done any common setting you know

512
00:30:07,960 --> 00:30:10,990
you look at them as on CD now and so on and so forth in

513
00:30:10,990 --> 00:30:17,620
information retrieval you know we have pushed technology where people are not actively searching but

514
00:30:17,620 --> 00:30:22,350
rather information is brought to them in certain folders are agent technology and so on

515
00:30:22,350 --> 00:30:26,110
and so forth intelligent user interfaces and if you look at it of course you

516
00:30:26,110 --> 00:30:27,650
know and amazon on

517
00:30:29,190 --> 00:30:33,710
you see this in know this year you know hello such-and-such and we have recommendations

518
00:30:33,710 --> 00:30:38,950
for you this is where the recommender systems used or you know you have these

519
00:30:38,960 --> 00:30:45,530
very simple things like customer who bought a particular book prove in this case also

520
00:30:45,530 --> 00:30:50,050
bought some other stuff so these are all types of things

521
00:30:50,060 --> 00:30:54,640
well you haven't actually should query where the system kind of makes recommendations to you

522
00:30:54,650 --> 00:31:01,340
can and so the type of data that we might use for that

523
00:31:01,360 --> 00:31:06,600
i can be of you know different types we can actually use explicit feedback often

524
00:31:06,600 --> 00:31:10,090
so user ratings or user provides even review

525
00:31:10,110 --> 00:31:13,100
or you know

526
00:31:13,120 --> 00:31:18,010
it feels of the questionnaire and so on and so forth all we might also

527
00:31:18,010 --> 00:31:22,410
want to use implicit feedback like you know what the pages visited what have you

528
00:31:22,410 --> 00:31:26,590
purchased whatever you downloaded the we might even go to the level of clickstreams and

529
00:31:26,590 --> 00:31:27,710
so on and so forth

530
00:31:29,510 --> 00:31:30,920
OK and so

531
00:31:30,940 --> 00:31:34,380
if we want to do that there are no two ways that we can go

532
00:31:34,380 --> 00:31:39,730
one is content based filtering k is that somehow we learn about the

533
00:31:39,780 --> 00:31:42,140
we look at the properties of objects

534
00:31:42,160 --> 00:31:47,620
that seem to matter to you it would be more or less standard learning setting

535
00:31:47,620 --> 00:31:51,160
right you know we you have let's say the the things that you buy in

536
00:31:51,160 --> 00:31:54,180
the things that you're not buying and so we would like to find out what

537
00:31:54,180 --> 00:31:56,550
are the things that you buy as opposed to the ones that do not by

538
00:31:56,550 --> 00:32:01,050
and we try to to do that based on the attributes of objects OK based

539
00:32:01,050 --> 00:32:04,940
on the content of things well the problem is often that there might not be

540
00:32:04,940 --> 00:32:08,490
very good predictive attributes OK so we might think of think of movies and things

541
00:32:08,490 --> 00:32:12,840
like that it's very hard you know we might favour certain directors and actors but

542
00:32:12,840 --> 00:32:15,940
you know that might just be very weak predict actually of whether we like it

543
00:32:16,220 --> 00:32:19,210
or not the other idea in in

544
00:32:19,220 --> 00:32:24,680
recommender systems that has been quite successful is the idea of collaborative for social filtering

545
00:32:24,690 --> 00:32:28,070
which means that well look you know if i am is on i have hundreds

546
00:32:28,070 --> 00:32:31,710
of millions of customers right that come to my website

547
00:32:31,720 --> 00:32:32,710
i can not

548
00:32:32,720 --> 00:32:37,160
you know it's it's not just that i could treat every customer individually and try

549
00:32:37,160 --> 00:32:41,030
to learn this but i can use the information have from other customers right to

550
00:32:41,030 --> 00:32:46,840
make recommendations for a particular customer and the idea is that there is some similarity

551
00:32:46,900 --> 00:32:48,780
typically in the way

552
00:32:48,830 --> 00:32:52,390
you know the the types of interest that people have the type preference that people

553
00:32:52,390 --> 00:32:56,470
have that for instance if i would find you know a lot of customers that

554
00:32:56,470 --> 00:33:00,910
are very similar to a particular customer i could use the things that these customers

555
00:33:00,910 --> 00:33:03,660
this is why we are coming to an end

556
00:33:03,670 --> 00:33:06,730
techniques of the only write this

557
00:33:06,840 --> 00:33:08,960
as a learning problem so we have

558
00:33:08,980 --> 00:33:14,530
sometimes is that he takes i call signals x i have some involvement the which

559
00:33:14,540 --> 00:33:17,400
will have the loss function l

560
00:33:17,410 --> 00:33:20,640
which is the solution the last

561
00:33:20,650 --> 00:33:25,290
so what you want to do is to have phoenix finding trendsetters for at the

562
00:33:25,290 --> 00:33:28,340
centre of course

563
00:33:29,410 --> 00:33:31,910
we are in the world

564
00:33:31,910 --> 00:33:34,000
because of having these values

565
00:33:34,030 --> 00:33:35,600
very cheap

566
00:33:35,620 --> 00:33:39,480
like a before you take the picture is that it can i have to admit

567
00:33:39,480 --> 00:33:41,430
it but she so

568
00:33:41,460 --> 00:33:44,770
and then consider the case where n is very large

569
00:33:45,910 --> 00:33:50,400
rules you have like a pool to learn to show you the basis of natural

570
00:33:50,400 --> 00:33:51,750
images you can have

571
00:33:51,800 --> 00:33:54,520
question number of that so what we

572
00:33:54,540 --> 00:33:58,390
might want to instead of these policies

573
00:33:58,410 --> 00:34:01,110
there are two key made the thing because

574
00:34:03,040 --> 00:34:04,000
and this is

575
00:34:05,150 --> 00:34:08,810
something which can be analyzed using unlabelled technique

576
00:34:08,830 --> 00:34:11,200
so indeed when i know can

577
00:34:11,210 --> 00:34:14,000
handle potentially infinite that's

578
00:34:14,020 --> 00:34:16,470
it can also add up with a name change

579
00:34:16,480 --> 00:34:18,720
for instance if you have to do

580
00:34:19,350 --> 00:34:25,540
you might not have the future they may want to optimize dynamically your dictionary

581
00:34:25,800 --> 00:34:28,460
and what is not it's also

582
00:34:28,720 --> 00:34:32,780
in this case from faster than batteries

583
00:34:32,790 --> 00:34:34,110
so i think the

584
00:34:34,120 --> 00:34:38,360
but when was before that also immediately of working in this field

585
00:34:38,390 --> 00:34:40,530
so here is what i want

586
00:34:40,540 --> 00:34:46,520
what we do is we can we do it with a large number of iterations

587
00:34:46,540 --> 00:34:49,060
at each iteration the basic idea is really

588
00:34:49,090 --> 00:34:54,160
for one thing and one of the things which is why you c

589
00:34:54,220 --> 00:34:55,660
then propose

590
00:34:55,670 --> 00:35:01,720
it spent for new elements we compute the space program computing this and that he

591
00:35:01,740 --> 00:35:03,390
and using

592
00:35:03,410 --> 00:35:06,290
the quran which which is here the

593
00:35:06,300 --> 00:35:09,310
team one and that he

594
00:35:10,090 --> 00:35:14,600
the main purpose of

595
00:35:14,680 --> 00:35:16,460
so here

596
00:35:16,470 --> 00:35:19,050
two things to

597
00:35:19,120 --> 00:35:22,300
make decisions the first of all you have to be able to deal with this

598
00:35:22,300 --> 00:35:26,350
but i think that should be so here in case have small

599
00:35:26,390 --> 00:35:29,970
sparsity problem with very high quality

600
00:35:29,980 --> 00:35:32,180
like if

601
00:35:32,200 --> 00:35:33,850
that has like

602
00:35:33,870 --> 00:35:35,770
of size one former

603
00:35:35,790 --> 00:35:41,460
maybe had we can have ten or fifteen until it so in this situation happens

604
00:35:41,460 --> 00:35:44,510
that love is performing very well and what we have is

605
00:35:44,560 --> 00:35:45,650
for the dictionary

606
00:35:47,620 --> 00:35:51,940
it's a bit more tricky because it could look like a bigot easier for the

607
00:35:52,110 --> 00:35:54,360
purpose of each iteration

608
00:35:54,380 --> 00:35:59,490
but one that we had the black hole what we fix all the problems of

609
00:35:59,520 --> 00:36:00,900
the but once

610
00:36:00,930 --> 00:36:03,920
then we can have a closed form for updating the

611
00:36:03,940 --> 00:36:05,930
the kind of be

612
00:36:05,940 --> 00:36:08,050
and it turns out that this

613
00:36:08,070 --> 00:36:12,090
each musician according to the easier to

614
00:36:15,720 --> 00:36:17,570
we know how to

615
00:36:17,590 --> 00:36:20,980
the with this algorithm which is the way the world

616
00:36:21,740 --> 00:36:22,970
the function

617
00:36:22,980 --> 00:36:24,860
the real world

618
00:36:27,310 --> 00:36:31,570
and we see data which guarantees that

619
00:36:31,580 --> 00:36:33,990
so if you want you to say

620
00:36:34,100 --> 00:36:38,380
all this stuff and the said you can last for the

621
00:36:39,460 --> 00:36:43,110
user group which for the we want stop

622
00:36:46,600 --> 00:36:49,620
that minimizes to of the deep sea

623
00:36:49,630 --> 00:36:51,680
these very close to the one produced

624
00:36:51,750 --> 00:36:56,630
the oceans of the solution exhaustivity made one which

625
00:36:56,650 --> 00:36:59,850
it efficiently use the semantic web as well

626
00:37:00,240 --> 00:37:03,440
and then there is also something so can

627
00:37:03,480 --> 00:37:05,980
using the following single

628
00:37:05,990 --> 00:37:10,940
the training set you can make a small evaluation more like fifty eight months

629
00:37:10,950 --> 00:37:16,600
of the training set at each iteration this is the almost

630
00:37:16,640 --> 00:37:21,590
so i began to make a say the function of the nice

631
00:37:21,600 --> 00:37:23,840
when the the

632
00:37:23,850 --> 00:37:26,950
which we call here i t

633
00:37:26,960 --> 00:37:29,360
very nice

634
00:37:30,280 --> 00:37:38,100
so the sense of the word that is the basic poses which we want to

635
00:37:39,550 --> 00:37:44,220
that is there when t tends to infinity with probability one

636
00:37:44,230 --> 00:37:47,100
and then the the second guarantees that

637
00:37:47,100 --> 00:37:51,660
if we consider all the special of the musicians before and that we have beginning

638
00:37:52,600 --> 00:37:55,530
the distance between the seventeenth and the

639
00:37:55,550 --> 00:37:57,250
when t tends to to infinity

640
00:37:57,270 --> 00:37:58,100
and we we

641
00:37:59,110 --> 00:38:00,170
happy with this

642
00:38:02,390 --> 00:38:05,800
so now i going to the extent that

643
00:38:07,280 --> 00:38:12,840
for those who are familiar with the kind of those which can be

644
00:38:12,920 --> 00:38:15,260
finally on his skills as well

645
00:38:15,350 --> 00:38:16,740
so we

646
00:38:16,740 --> 00:38:19,970
there is a from of the problem

647
00:38:19,980 --> 00:38:23,270
and this will be

648
00:38:23,340 --> 00:38:24,990
most important

649
00:38:25,010 --> 00:38:30,660
for the second part when we study a case of instrumental variables because we use

650
00:38:30,660 --> 00:38:38,790
exactly this result and just replace this box and then at minus and by the

651
00:38:38,790 --> 00:38:41,380
instrument regression and the associated

652
00:38:41,390 --> 00:38:43,570
estimation ising contamination

653
00:38:43,640 --> 00:38:49,180
so what they did there is a kind of expansion sympathy expansion

654
00:38:49,200 --> 00:38:55,460
land and at minus lambda which is subject to study is exactly equal to the

655
00:38:55,460 --> 00:39:00,960
difference of these two functions he of men at nine minus and

656
00:39:00,980 --> 00:39:04,360
and the problem is that they don't know is apparent he

657
00:39:04,360 --> 00:39:05,480
so i just

658
00:39:05,490 --> 00:39:08,270
apply this

659
00:39:08,280 --> 00:39:11,260
i just differentiating

660
00:39:11,280 --> 00:39:12,490
this difference

661
00:39:15,990 --> 00:39:17,720
to obtain two parts

662
00:39:17,720 --> 00:39:19,210
the first part

663
00:39:19,210 --> 00:39:24,180
which is which would be much nice to study since it could no part

664
00:39:24,340 --> 00:39:28,550
in the difference and minus and plus the residual term

665
00:39:28,580 --> 00:39:32,770
so the idea that would be to study

666
00:39:32,780 --> 00:39:38,030
the first part and study the rate of convergence of the first part and try

667
00:39:38,040 --> 00:39:44,060
to control this was an attempt to get rid of this residual and the first

668
00:39:44,060 --> 00:39:47,640
result is that these

669
00:39:47,640 --> 00:39:52,430
this linear part can be written as the integral

670
00:39:52,940 --> 00:39:57,520
from zero to ics of the difference between them and that and n

671
00:39:57,540 --> 00:39:59,380
multiply by function

672
00:39:59,420 --> 00:40:06,000
gamma gamma extend i have haven't to explain what is exactly this function it's not

673
00:40:06,000 --> 00:40:11,760
really important the important thing is that doesn't depend on n

674
00:40:11,780 --> 00:40:19,180
so it would change the results are asymptotic results what is important is that

675
00:40:20,670 --> 00:40:21,940
i have i can

676
00:40:21,980 --> 00:40:28,720
right to difference lambda n minus that as an integral part part of an automated

677
00:40:28,930 --> 00:40:34,730
system and the main part of it means that if i can control the residual

678
00:40:34,730 --> 00:40:39,200
then i will be able to prove that i can indeed improve the rate of

679
00:40:39,200 --> 00:40:46,470
convergence since i will improve the regularity of of the initial estimate

680
00:40:52,010 --> 00:40:55,780
an idea of the proof of these results to do

681
00:40:58,390 --> 00:41:03,660
the starting point of all the asymptotic results show

682
00:41:03,670 --> 00:41:07,690
how to transform this

683
00:41:07,700 --> 00:41:14,190
nonlinear problem to me no one was it and exposure to the right of reply

684
00:41:14,200 --> 00:41:15,960
eminently n

685
00:41:15,980 --> 00:41:18,720
so the idea is use

686
00:41:18,720 --> 00:41:29,110
but we have a new place in diffusion of how into parameter lambda operator

687
00:41:29,120 --> 00:41:31,220
and any of them

688
00:41:31,270 --> 00:41:35,190
and we want to express lambda as a function of and so the idea is

689
00:41:35,190 --> 00:41:38,530
to to place the implicit functions so

690
00:41:38,550 --> 00:41:42,170
to derive solution lambda as a function of n

691
00:41:42,180 --> 00:41:44,090
so there is

692
00:41:44,120 --> 00:41:52,420
some of course some assumptions to to check on and around and and and

693
00:41:52,430 --> 00:41:53,920
under these assumptions

694
00:41:56,690 --> 00:41:58,420
we can prove that

695
00:41:58,470 --> 00:42:04,180
there exists an open subset around the true function and

696
00:42:04,200 --> 00:42:10,360
such that ended an open subset around the true function lambda such that f of

697
00:42:10,360 --> 00:42:16,450
ten lambda equal to zero is also satisfied so i can extend the

698
00:42:16,960 --> 00:42:21,300
this relation a and lambda equal to zero to some open subset of one the

699
00:42:21,310 --> 00:42:23,510
two functions and and

700
00:42:23,540 --> 00:42:25,620
and this is

701
00:42:26,140 --> 00:42:30,240
quite nice since i can then use

702
00:42:30,320 --> 00:42:32,720
this result these

703
00:42:32,730 --> 00:42:39,840
a second relation and differentiate this relation in this open since it to obtain to

704
00:42:39,870 --> 00:42:47,210
transform the initial nonlinear differential equations to solve it into a linear differential equation

705
00:42:47,220 --> 00:42:51,030
in a

706
00:42:51,080 --> 00:42:53,480
in the first derivative of ft

707
00:42:53,490 --> 00:42:57,820
played and then at and minus and and the good point in that i can

708
00:42:57,820 --> 00:43:03,890
solve the linear differential equation and then i can obtain an exact form for the

709
00:43:03,890 --> 00:43:07,210
solution of these linear differential equations

710
00:43:07,210 --> 00:43:10,380
so the the idea is that when you have

711
00:43:10,430 --> 00:43:14,980
nonlinear problem to solve is to try to transform it into a linear one to

712
00:43:14,980 --> 00:43:16,920
be able to solve it

713
00:43:17,170 --> 00:43:22,900
to some residual term that has to be controlled and in order to obtain an

714
00:43:22,900 --> 00:43:29,710
exact the exact dependence between a and the solution of the differential equation

715
00:43:31,630 --> 00:43:36,980
the first estimate of enactments and that's the main point

716
00:43:38,430 --> 00:43:43,000
so now using these

717
00:43:43,080 --> 00:43:45,690
this is

718
00:43:46,930 --> 00:43:54,320
the results asymptotic results can derive on the solutions lambda in at manus island

719
00:43:54,330 --> 00:44:00,720
so mainly what do we expect expect to improve the weights of conventions and we

720
00:44:00,720 --> 00:44:03,740
expect to improve the rate of convergence in two

721
00:44:03,770 --> 00:44:07,150
in two directions first

722
00:44:07,150 --> 00:44:13,500
so the differential equation so if assume that n is continuously differentiable all the to

723
00:44:13,510 --> 00:44:16,490
the solution lambda conditioning with b

724
00:44:16,520 --> 00:44:22,090
continuously differentiable of all the three so we haven't yet again in regularity of the

725
00:44:22,090 --> 00:44:27,720
into this parameter and also we have an overview again in dimension since the first

726
00:44:28,180 --> 00:44:33,030
function m was depending on two arguments and my solution and that depends on one

727
00:44:33,030 --> 00:44:35,660
argument so i have

728
00:44:35,700 --> 00:44:41,560
i expect gain in two dimensions and fortunate be able to

729
00:44:41,590 --> 00:44:45,290
check that we obtain these games

730
00:44:45,290 --> 00:44:51,390
is the use cutting plane the strategy that into the mind the parameters are fast

731
00:44:51,470 --> 00:44:56,010
but these properties have to go back to the the

732
00:44:56,020 --> 00:45:03,350
the principal drawback a that they're before the algorithm converges and some quite a lot

733
00:45:03,350 --> 00:45:09,220
of course trains need to be included and to include each of course strength that

734
00:45:09,220 --> 00:45:14,600
is it should be the most violated constraint that they have to compute dynamic programming

735
00:45:15,400 --> 00:45:21,850
and they have to be computed the by dynamic programming second drawback is the use

736
00:45:21,850 --> 00:45:27,760
of slag by was to relax this problem in a real life problem we have

737
00:45:28,210 --> 00:45:34,340
been asked part of what so we use another the main idea behind our approach

738
00:45:34,340 --> 00:45:42,910
is to find a global objective function that anyone can that to in

739
00:45:43,340 --> 00:45:50,240
so that we are able to warm satisfy must if not all the constraints is

740
00:45:50,240 --> 00:45:54,980
beneficial like a number of constraints that what i said before and we think that

741
00:45:54,980 --> 00:46:01,390
the with this approach naked that this approach is more not only adapted for non

742
00:46:01,390 --> 00:46:09,520
separable cases and and our approach basically maximise the discord into the body if i

743
00:46:09,520 --> 00:46:16,140
consider just one sequence a are in the training set that maximize score i have

744
00:46:16,150 --> 00:46:23,950
the of the optimality is marked by the separated of the optimal alignment is maximally

745
00:46:23,950 --> 00:46:31,640
separated for from the markov for correct alignment and in particular the distance between the

746
00:46:31,640 --> 00:46:33,910
sculpted must go on the mean of the story

747
00:46:34,060 --> 00:46:38,590
he is maximized in terms of the standard deviation

748
00:46:41,160 --> 00:46:48,690
then discover it's quite interesting also because it is a measure of ranking so basically

749
00:46:48,690 --> 00:46:54,710
maximizing the discourse we minimize the number of alignments that have score higher than the

750
00:46:54,710 --> 00:46:55,860
given one

751
00:46:55,880 --> 00:47:02,990
since in practical application and the inverse parametric sequence alignment problem we don't have just

752
00:47:02,990 --> 00:47:06,770
one pair of sequence but we have an entire article in fact we should define

753
00:47:07,170 --> 00:47:10,770
the scope of the training set and since

754
00:47:10,780 --> 00:47:16,030
we discourage the linear function and also the sum of the score will be a

755
00:47:16,030 --> 00:47:21,450
linear function also the mean this decide they all overall mean it will be a

756
00:47:21,450 --> 00:47:27,820
linear function similar reasoning can be adopted for the variance if we think that the

757
00:47:27,820 --> 00:47:34,920
contribution of each element in the training set is in that is independent from the

758
00:47:34,920 --> 00:47:40,040
order so we simply compute the overall be vector and all that i see that

759
00:47:40,040 --> 00:47:45,320
there are some you but they that each

760
00:47:45,330 --> 00:47:47,410
each element of

761
00:47:47,540 --> 00:47:49,510
teacher training points to

762
00:47:49,550 --> 00:47:57,730
so the problem is that we are dealing with these maximise of the overall length

763
00:47:57,870 --> 00:48:03,640
and this problem and mister three really can be trivially solved by a quadratic programming

764
00:48:04,650 --> 00:48:09,650
and we see that they are for that we obtain a such that the master

765
00:48:09,650 --> 00:48:16,490
of this potentially many linear constraints are already satisfied the if one wants to

766
00:48:16,540 --> 00:48:24,460
in doing impose that each of them is satisfied one can also include in the

767
00:48:24,460 --> 00:48:25,930
the musician

768
00:48:25,940 --> 00:48:32,730
and again we have a convex optimisation problem and we can develop an iterative algorithms

769
00:48:33,040 --> 00:48:38,920
eventually if we want we can relax constraints for nonseparable problem so basically we have

770
00:48:38,950 --> 00:48:46,040
to approach or we decided to take the the the the and for that we

771
00:48:46,040 --> 00:48:53,060
obtain just maximizing the or we can decide to the other day in the lot

772
00:48:53,360 --> 00:48:57,860
of the constraints basically we see that

773
00:48:57,880 --> 00:49:03,640
this approach has a lot of of ontologies and it works very well in most

774
00:49:03,640 --> 00:49:05,330
of the applications

775
00:49:06,330 --> 00:49:07,120
we need

776
00:49:07,130 --> 00:49:12,950
there were twenty seven approach and we see that basically

777
00:49:12,980 --> 00:49:18,410
and an iterative algorithm can be developed to solve approach and it is based on

778
00:49:18,410 --> 00:49:24,050
four steps in the first step the moment are computed in the second but the

779
00:49:24,050 --> 00:49:32,580
optimal vector five phi either an all day in some solving quadratic programming problem than

780
00:49:32,580 --> 00:49:36,760
we if we want we can entering the main local and for each element in

781
00:49:36,760 --> 00:49:44,940
the training set we can compute the demise violated constraint that is the feature vector

782
00:49:44,980 --> 00:49:52,800
correspond to the optimal alignment for the coordinates of parameters and if the score of

783
00:49:52,880 --> 00:49:58,320
society to read even bigger than the score of the optimal period then we can

784
00:49:58,320 --> 00:50:03,340
impose score of document pages should be big and we can find it in a

785
00:50:03,340 --> 00:50:09,500
new one for solving the quadratic programming problem subject to this constraint we continue with

786
00:50:09,500 --> 00:50:11,590
these until today

787
00:50:12,120 --> 00:50:15,160
effect of course trains does not change

788
00:50:15,180 --> 00:50:21,750
i was so now some experimental results the first experiment i want to show that

789
00:50:21,760 --> 00:50:24,450
with these approaches learning is effectively

790
00:50:24,570 --> 00:50:31,730
she would consider artificial p of random sequences were aligned with random set of parameters

791
00:50:32,110 --> 00:50:37,370
in particular with three parameters and two on the eleventh of this month and the

792
00:50:37,370 --> 00:50:42,510
training set of varying sizes while the past that is fixed to one and the

793
00:50:42,510 --> 00:50:48,520
sequence and we see that learning can converge to the the learning is achieved in

794
00:50:48,520 --> 00:50:50,050
the second

795
00:50:50,070 --> 00:50:59,610
experiments show that the number of constraints and is in the i show that considering

796
00:50:59,820 --> 00:51:06,810
were just one training in the training set to reconstruct the optimise set of parameters

797
00:51:07,120 --> 00:51:12,980
you must have the cases local strains are needed in another experiment i consider to

798
00:51:12,980 --> 00:51:20,180
be discovered optimisation without any constraint e i considered the two hundred eleven parameter model

799
00:51:20,540 --> 00:51:26,400
and compare the performance of the score with a generative model and we today and

800
00:51:26,400 --> 00:51:32,500
substitution matrix are computed by plus and estimates and then i can see that in

801
00:51:32,500 --> 00:51:38,830
the past that is saw with the performance of his car which are always better

802
00:51:38,940 --> 00:51:42,200
than the performance of the generative model

803
00:51:42,260 --> 00:51:46,410
you and he's an example of the

804
00:51:46,410 --> 00:51:51,220
so whenever you see convex functions you should think of a few things you should

805
00:51:51,220 --> 00:51:55,870
think about you get lower bounds from the tension approximation and you get upper bounds

806
00:51:55,870 --> 00:51:58,670
from something like jensen's inequality

807
00:51:58,690 --> 00:52:00,610
so we used the latter

808
00:52:00,650 --> 00:52:02,150
it's member i was

809
00:52:02,170 --> 00:52:06,650
somehow breaking the graph into combinations of trees this is what's going on here is

810
00:52:06,650 --> 00:52:07,960
a special case

811
00:52:12,120 --> 00:52:15,910
so you can imagine this original graph may be i'm trying to solve the quadratic

812
00:52:15,910 --> 00:52:16,830
programme that has

813
00:52:17,240 --> 00:52:21,250
let's make all the node weights zero to make it simple let's say i edge

814
00:52:21,260 --> 00:52:24,340
weights two three four one minus two at a bunch

815
00:52:24,350 --> 00:52:25,860
of of weights

816
00:52:25,890 --> 00:52:30,940
right so those are the weights theta st that are specifying my optimisation problem most

817
00:52:30,940 --> 00:52:32,940
came from your data

818
00:52:32,950 --> 00:52:38,580
one one

819
00:52:38,660 --> 00:52:44,620
yes so it state st for the function access times xt so

820
00:52:48,100 --> 00:52:50,020
in general you can have a whole matrix

821
00:52:50,030 --> 00:52:53,400
but but actually you can always specify with just one number if you do sort

822
00:52:53,400 --> 00:52:56,800
of you discount the degrees of freedom you only need one number per node and

823
00:52:56,800 --> 00:53:01,360
one number per edge this is this is what's called a minimal representation

824
00:53:01,370 --> 00:53:03,570
OK so

825
00:53:03,610 --> 00:53:06,950
those things are the weights on the edges that that's the problem that you're trying

826
00:53:06,950 --> 00:53:10,450
to solve its maximisation problem with these weights

827
00:53:10,470 --> 00:53:14,360
so what the method is doing is saying well i can solve that problem

828
00:53:14,440 --> 00:53:18,790
but it's a future up some edges maybe drop the surgeon you drop this edge

829
00:53:18,800 --> 00:53:22,710
you can put whatever weight you want on those edges i can solve that problem

830
00:53:22,710 --> 00:53:26,270
for you because that's the treaty rights you could run max product on that and

831
00:53:26,270 --> 00:53:28,040
solve it

832
00:53:28,120 --> 00:53:31,890
and i could take some other tree and i could drop some other edges this

833
00:53:31,890 --> 00:53:35,150
one and this one i could put some other weights on those edges i could

834
00:53:35,150 --> 00:53:39,830
also solve that problem very quickly with max product

835
00:53:39,940 --> 00:53:44,700
so that's the intuition you're breaking the problem into things that are easy

836
00:53:44,710 --> 00:53:47,550
in this case we're using trees but you can see the idea is more generally

837
00:53:47,550 --> 00:53:52,240
applicable you could have used other easy structures not just reason that that's indeed what

838
00:53:52,240 --> 00:53:54,390
many people have done in more recent work

839
00:53:54,400 --> 00:53:59,540
OK so how might using these weights what sort of the key property of these

840
00:54:01,470 --> 00:54:04,530
i need to make a convex combination of trees so i'm just going to do

841
00:54:04,530 --> 00:54:07,320
this simple thing i put weight half on each tree

842
00:54:07,340 --> 00:54:10,940
so i need to choose whatever weights i put on the two trees it has

843
00:54:10,940 --> 00:54:13,650
to be the case that one half times this number

844
00:54:13,660 --> 00:54:18,010
when you don't have an edge it's zero by default so one half times this

845
00:54:18,010 --> 00:54:21,580
number plus one half times this number should be that number

846
00:54:21,620 --> 00:54:25,870
so you guys can check me but i think this is sort of valid splitting

847
00:54:25,870 --> 00:54:28,520
of that of the problem into two trees

848
00:54:28,570 --> 00:54:36,600
correct me if i'm wrong

849
00:54:46,030 --> 00:54:47,350
yes it does matter

850
00:54:47,360 --> 00:54:49,700
so his point was in edges where

851
00:54:49,710 --> 00:54:53,390
you know this edges in both this tree industry right this search

852
00:54:53,780 --> 00:54:56,230
i chose to minus six

853
00:54:56,240 --> 00:54:58,440
times one half get minus two

854
00:54:58,460 --> 00:55:02,240
but there many possible splittings i could have done i this is random i just

855
00:55:02,240 --> 00:55:05,940
chose it we're going to see is that this algorithm is actually doing something quite

856
00:55:05,940 --> 00:55:11,010
clever this algorithm for a fixed set of these weights is optimizing over all possible

857
00:55:11,010 --> 00:55:14,190
splittings of trees and it's finding the best splitting

858
00:55:14,330 --> 00:55:18,900
so it's answering your question it's sort of the optimal splitting that we could do

859
00:55:18,920 --> 00:55:21,790
so we'll get to that you're sort of anticipating

860
00:55:21,850 --> 00:55:24,540
which is good

861
00:55:24,720 --> 00:55:27,650
OK so we have this convex combination so

862
00:55:27,650 --> 00:55:32,510
about the label what's really interesting is the author assumes that the reader hasn't taken

863
00:55:32,510 --> 00:55:38,550
309 1 and so the 1st 3 chapters take you through the history of chemistry

864
00:55:38,550 --> 00:55:40,690
starting with the market

865
00:55:40,700 --> 00:55:45,330
and take you through the aristotelian theory after you've had this these lectures will be

866
00:55:45,400 --> 00:55:50,160
a joy to read OK so now let's go inside the let's go inside the

867
00:55:50,160 --> 00:55:53,830
and try to understand what's going on and this is taken from your book Table

868
00:55:53,830 --> 00:55:58,550
1 . 3 and so these are the subatomic particles that we have to

869
00:55:58,900 --> 00:56:03,940
contend with with the electrons it so there's a negative charge in the value of

870
00:56:03,940 --> 00:56:08,580
Y minus 1 . 6 times 10 to the minus 19 coulombs this is the

871
00:56:08,940 --> 00:56:14,500
SI units the system International unit for electric charge that has a mass of 9

872
00:56:14,520 --> 00:56:20,050
. 1 1 times 10 the minus 31 kg so that the charge compensation comes

873
00:56:20,050 --> 00:56:25,200
out of the nucleus with the protons and it's a positive 1 . 6 times

874
00:56:25,200 --> 00:56:30,480
and was lighting cool but its mass is 1800 times that of the electron the

875
00:56:30,480 --> 00:56:35,140
electron just a little map in comparison to the proton and then there's this 3rd

876
00:56:35,140 --> 00:56:39,870
species the neutron which has no charge roughly equal in mass

877
00:56:39,880 --> 00:56:44,720
to that of the of the proton and so on

878
00:56:44,740 --> 00:56:51,030
what we do in order to designate the identity of an atom is to to

879
00:56:51,050 --> 00:56:57,090
the following let this be this is the atomic symbol atomic symbol which was given

880
00:56:57,100 --> 00:57:04,000
by the Brazilian this is the latter wanted to letter character and we began with

881
00:57:04,100 --> 00:57:10,330
the proton number the proton number is in the lower left corner is the proton

882
00:57:10,330 --> 00:57:16,110
number number of protons in the nucleus which in the neutral atom as the number

883
00:57:16,110 --> 00:57:22,370
of protons in the nucleus which in the neutral atom is equal to the number

884
00:57:22,370 --> 00:57:24,140
of electrons

885
00:57:24,890 --> 00:57:27,710
but thank you back

886
00:57:28,110 --> 00:57:31,930
and get the number of neutrons what we do is we go up here the

887
00:57:31,940 --> 00:57:39,330
upper left and this is called the atomic mass atomic mass and that's equal to

888
00:57:39,330 --> 00:57:44,290
the sum of the number of protons across the number of neutrons so we get

889
00:57:44,380 --> 00:57:48,790
the number of neutrons indirect because we know that proton number here this summer protons

890
00:57:48,830 --> 00:57:53,130
and neutrons and we don't put that number of all the mass of the electrons

891
00:57:53,300 --> 00:57:54,390
the so

892
00:57:54,440 --> 00:57:58,480
a light in comparison so this will give us the uh of the sum we

893
00:57:58,480 --> 00:58:02,860
need and at some level we we we really there some redundancy here so we

894
00:58:03,150 --> 00:58:11,480
right sodium for example sodium 23 11 while 11 means of sodium if you've got

895
00:58:11,480 --> 00:58:16,610
11 protons it must be sodium so we can get away with just 23 sodium

896
00:58:16,980 --> 00:58:25,180
if you want to be really really smart you could write sodium like this

897
00:58:25,870 --> 00:58:31,070
just a suggestion now Adams need not be neutral they need not be neutral they

898
00:58:31,070 --> 00:58:39,550
can be show charge deficient or charge excessive and so Faraday Michael Faraday coined the

899
00:58:39,550 --> 00:58:49,250
term iron coined the term iron atoms of net charge atoms and other species sharing

900
00:58:49,250 --> 00:59:04,270
net charge so for example the positive ions positive ions dessert electron-deficient we can't do

901
00:59:04,270 --> 00:59:11,430
anything about getting protons the bearing the nucleus but we can add or subtract electrons

902
00:59:11,470 --> 00:59:16,810
the protein the positive ions are electron deficient and these are known as kappa on

903
00:59:17,220 --> 00:59:22,920
their owners Catalans and you can use mnemonics to try to keep the straight ICT

904
00:59:22,940 --> 00:59:29,490
as sort of like plus sign and you also know that decadic comes from the

905
00:59:29,490 --> 00:59:34,730
fact that you know in electrochemical cell that of the cannons of those a reduced

906
00:59:35,330 --> 00:59:41,140
so you know from catastrophe captors Greek for down catastrophe the downturn so that's a

907
00:59:41,140 --> 00:59:44,490
cat that the negative alliances

908
00:59:44,550 --> 00:59:50,900
the negative ions these are electron rich but they have an excess of electrons and

909
00:59:50,900 --> 00:59:56,400
these are known as the and on the right idea that it's got that on

910
00:59:56,400 --> 01:00:02,000
which which might conjure up negative or on an island and minus both have 5

911
01:00:02,000 --> 01:00:09,550
letters you want to keep those straight away OK nominalism monarchs influence community right now

912
01:00:09,550 --> 01:00:13,770
the last thing is that because the neutron has no net charge we can change

913
01:00:13,800 --> 01:00:22,240
neutron number k neutron has moved charge so this means you can vary you can

914
01:00:22,240 --> 01:00:26,700
vary the neutron number without

915
01:00:26,830 --> 01:00:33,330
without changing the chemical identity because chemical identity is fixed by

916
01:00:33,370 --> 01:00:35,370
the proton

917
01:00:35,550 --> 01:00:42,380
this is you know the charge compensate we can add or subtract of neutrons at

918
01:00:42,380 --> 01:00:48,140
liberty and so you can look for example at the periodic table and if you

919
01:00:48,140 --> 01:00:55,000
look up the atomic mass of carbon you'll see that carbon comes at 12 .

920
01:00:55,000 --> 01:01:02,020
4 0 1 1 4 carbon 12 . 0 1 1 isn't this against Dalton

921
01:01:02,180 --> 01:01:07,980
this expose the whole number here if you look more carefully you'll find that this

922
01:01:07,980 --> 01:01:15,630
is a blend of 3 different forms of carbon we have carbon 12 only believe

923
01:01:15,630 --> 01:01:20,220
office 6 because we already know what's carbon has to have 6 protons so carbon

924
01:01:20,220 --> 01:01:26,270
12 we know that it's got the proton number by definition is sex and the

925
01:01:26,270 --> 01:01:31,940
neutron number of 6 from 12 6 so it's got 6 protons and neutrons and

926
01:01:32,000 --> 01:01:39,610
is the dominant form of carbon so its abundance its abundance in nature is 98

927
01:01:39,720 --> 01:01:45,530
. 8 9 2 % 98 . 9 2 per cent there is a carbon

928
01:01:45,530 --> 01:01:53,160
13 however so it actually matically must be 6 protons and gets 7 neutrons and

929
01:01:53,160 --> 01:01:58,020
this is found in the abundance about 1 . 1 over 80 per cent and

930
01:01:58,020 --> 01:02:03,400
then there's the last 4 carbon 14 which is a radioactive form and we'll talk

931
01:02:03,400 --> 01:02:10,270
about that later 309 1 using radiocarbon dating and carbon 14 6 again and it

932
01:02:10,270 --> 01:02:16,230
has and it is found in the teaching of small amounts to minus 12 or

933
01:02:16,230 --> 01:02:17,560
part per trillion

934
01:02:18,000 --> 01:02:21,630
part per trillion so if you had all these up then you end up with

935
01:02:21,630 --> 01:02:30,170
if i were to follow one

936
01:02:30,370 --> 01:02:35,390
really really want

937
01:02:36,620 --> 01:02:40,530
if you

938
01:02:40,560 --> 01:02:42,570
all i will be

939
01:02:42,580 --> 01:02:45,380
well known

940
01:02:48,660 --> 01:02:51,880
before we have

941
01:02:53,530 --> 01:03:01,870
and both of them

942
01:03:01,890 --> 01:03:05,990
i i

943
01:03:08,090 --> 01:03:11,580
now if you think b

944
01:03:18,740 --> 01:03:24,630
the amount of

945
01:03:24,650 --> 01:03:27,090
i mean

946
01:03:29,130 --> 01:03:35,260
with three really there more

947
01:03:56,350 --> 01:03:58,730
what were

948
01:03:58,820 --> 01:04:01,810
the main

949
01:04:08,220 --> 01:04:11,970
it will be

950
01:04:16,170 --> 01:04:21,070
and then he really

951
01:04:21,090 --> 01:04:26,710
and there are three

952
01:04:30,580 --> 01:04:33,260
and on

953
01:04:38,850 --> 01:04:48,780
well i don't think of

954
01:04:51,130 --> 01:04:58,870
one here

955
01:05:00,580 --> 01:05:03,470
he went

956
01:05:07,110 --> 01:05:08,990
so what want

957
01:05:11,350 --> 01:05:15,280
a u

958
01:05:25,660 --> 01:05:29,390
there are a lot of each one

959
01:05:38,880 --> 01:05:44,030
we were

960
01:05:50,020 --> 01:05:57,710
three right for all three hundred and u

961
01:05:57,730 --> 01:06:02,930
and and what you want

962
01:06:03,020 --> 01:06:08,520
and the

963
01:06:32,380 --> 01:06:35,020
well what

964
01:06:35,030 --> 01:06:38,870
why not

965
01:06:38,890 --> 01:06:44,060
they were

966
01:06:50,940 --> 01:06:56,130
i revised we see her

967
01:06:58,730 --> 01:06:59,610
one the

968
01:07:02,720 --> 01:07:06,780
all my name e

969
01:07:07,620 --> 01:07:10,910
what you want

970
01:07:16,580 --> 01:07:18,600
now of course

971
01:07:18,610 --> 01:07:20,710
o three one

972
01:07:28,040 --> 01:07:35,520
so hold

973
01:07:44,140 --> 01:07:47,620
three we only one

974
01:08:04,850 --> 01:08:07,780
well i mean

975
01:08:07,830 --> 01:08:11,800
one of the

976
01:08:14,690 --> 01:08:21,980
well one of robot

977
01:08:22,000 --> 01:08:28,450
because what the press and role models

978
01:08:28,480 --> 01:08:34,160
o ninety nine million dollars in debt to

979
01:08:34,180 --> 01:08:47,890
OK now what i'm doing my one now so what i want

980
01:08:57,340 --> 01:08:59,420
what here

981
01:08:59,450 --> 01:09:03,700
row three in the morning

982
01:09:10,920 --> 01:09:12,980
in some way

983
01:09:12,980 --> 01:09:16,590
so first of all i wanna acknowledge a student i'm going rising at university of

984
01:09:16,590 --> 01:09:19,130
toronto addition to style it's mainly his work

985
01:09:20,270 --> 01:09:24,930
in unfortunately you know up until last week he was practicing to give this talk

986
01:09:24,940 --> 01:09:29,050
but last week he was selected for a random background checks so

987
01:09:29,600 --> 01:09:30,740
he can get his visa so

988
01:09:32,290 --> 01:09:32,840
in any case

989
01:09:33,940 --> 01:09:35,620
so he's he's a real down here

990
01:09:36,220 --> 01:09:39,340
so i'm gonna talk about multimodal in deep boltzmann machines end

991
01:09:39,900 --> 01:09:44,100
you know data data now this is really coming from a collection of modalities right

992
01:09:44,510 --> 01:09:46,310
we have images text audio

993
01:09:47,230 --> 01:09:48,670
if you're looking at various

994
01:09:50,310 --> 01:09:54,810
the data that's coming to us from social networking that approach product recommendation systems again

995
01:09:54,810 --> 01:09:57,750
we see that is coming in multiple sources of modalities

996
01:09:58,310 --> 01:10:03,280
right robotics applications it's might have multiple senses and by the way i should point

997
01:10:03,280 --> 01:10:06,890
out this was the most exciting acts like in terms of visual presentation so

998
01:10:07,420 --> 01:10:08,180
i hope you enjoy that

999
01:10:11,320 --> 01:10:14,990
in terms of what we try to do to have a mental picture we have

1000
01:10:14,990 --> 01:10:18,250
what we call without a full representation so for example he will looking at images

1001
01:10:18,250 --> 01:10:18,850
so far

1002
01:10:19,040 --> 01:10:22,610
images and text and we wanna get is we're gonna get some notion of a

1003
01:10:22,610 --> 01:10:25,260
concept or with commonality free representation right

1004
01:10:26,150 --> 01:10:30,520
so how we gonna how gonna do that's well we can learn generative model

1005
01:10:31,450 --> 01:10:34,580
joint density model so we're gonna be one generative model of images and text

1006
01:10:35,070 --> 01:10:38,280
each here would represent the notion of latent

1007
01:10:39,290 --> 01:10:39,790
well hidden

1008
01:10:41,690 --> 01:10:44,770
and really we can think about this if used representation and we can use it

1009
01:10:44,770 --> 01:10:47,750
for classification or retrieval task you want to use support

1010
01:10:48,280 --> 01:10:49,860
now because it a generative model

1011
01:10:50,840 --> 01:10:55,570
we can do fun things like generated data from one modality condition about right so

1012
01:10:55,570 --> 01:10:57,340
for example for things like image annotation

1013
01:10:57,770 --> 01:11:01,330
condition on an image we can generate a distribution over possible

1014
01:11:01,880 --> 01:11:04,690
tags or we can go the other way around conditionally

1015
01:11:05,340 --> 01:11:09,660
text we can generate a possible distribution images and does things could be useful for

1016
01:11:09,660 --> 01:11:11,620
things like image annotation and image retrieval

1017
01:11:12,580 --> 01:11:15,030
right but there are several challenges

1018
01:11:16,160 --> 01:11:18,260
that we face when we started looking at this problem

1019
01:11:18,750 --> 01:11:20,380
the very first challenges that often

1020
01:11:20,880 --> 01:11:24,260
different data modalities have very different statistical properties right so be

1021
01:11:25,280 --> 01:11:28,410
have very different input representations so for example if you look at images

1022
01:11:28,860 --> 01:11:32,030
images are typically represented in terms of real valued ends

1023
01:11:34,200 --> 01:11:37,950
if the working attacks typically for using bag of words representation then

1024
01:11:39,920 --> 01:11:43,820
it's discrete spas so so you can see that these are very very different data

1025
01:11:43,820 --> 01:11:48,710
modalities and really it's very difficult to learn meaningful cross-model features from from these low

1026
01:11:48,710 --> 01:11:49,800
level representations

1027
01:11:50,350 --> 01:11:52,070
right so so how can we go but at

1028
01:11:52,880 --> 01:11:53,980
the second challenge is that

1029
01:11:54,420 --> 01:11:56,840
you know we dealing with the data that's often noisy

1030
01:11:57,220 --> 01:11:57,860
all missing

1031
01:11:58,420 --> 01:12:02,800
right so here's some examples from the flickr dataset you know we have images but

1032
01:12:02,800 --> 01:12:07,320
sometimes some of the text that we observe is not very meaningful and sometimes it's

1033
01:12:07,650 --> 01:12:12,360
not meaningful sometimes it's amazing sometimes if you look at this last figure here last

1034
01:12:12,360 --> 01:12:16,010
picture it's somewhat me meaningful right that tells tells us something that is

1035
01:12:16,690 --> 01:12:19,480
picture for nature but is what we actually do seeing the data

1036
01:12:20,810 --> 01:12:23,510
in terms of text generated by the model this is the kind of

1037
01:12:24,030 --> 01:12:27,010
text that our model can generate just a little bit of a little bit of

1038
01:12:27,010 --> 01:12:32,350
a preview so that's capture certain notion of of what you see in in those

1039
01:12:33,840 --> 01:12:38,200
right now before i get into specific details about the model in which the model

1040
01:12:38,200 --> 01:12:39,920
is let me just show you would demo

1041
01:12:41,080 --> 01:12:45,610
so here you can think of this model is a generative model conditionally image

1042
01:12:46,030 --> 01:12:49,420
we're going generate a distribution over possible worlds just let the model

1043
01:12:51,070 --> 01:12:56,220
decide what what distributions can generate right now what i'm doing here because the a

1044
01:12:56,220 --> 01:13:00,560
generative model and running what's called the markov chain monte carlo gibbs sampler here

1045
01:13:02,270 --> 01:13:06,710
here you just seeing what it doesn't after every every fifty gives up its

1046
01:13:07,110 --> 01:13:13,580
so notice that after about hundred steps it's already generating something meaningful right so you're looking at sea beach island

1047
01:13:14,780 --> 01:13:18,180
you know sometimes it generates canadian things cannot be see lake

1048
01:13:18,860 --> 01:13:22,140
it only water boat and such so it doesn't go through these

1049
01:13:22,930 --> 01:13:25,180
different modes and that's generate

1050
01:13:26,120 --> 01:13:30,120
reasonable things right now is how fast it it sort of converges to

1051
01:13:30,610 --> 01:13:31,100
what i would

1052
01:13:31,500 --> 01:13:34,130
i think it converges right generating meaningful things

1053
01:13:35,010 --> 01:13:35,620
now let me

1054
01:13:36,140 --> 01:13:37,120
get the specific

1055
01:13:38,050 --> 01:13:41,280
definition of what the model is so that i described what that is

1056
01:13:41,780 --> 01:13:46,910
the very first building block of our model is is a specific document called restricted boltzmann machines

1057
01:13:47,410 --> 01:13:49,690
i mean if you know about this is both which is just a just a

1058
01:13:49,690 --> 01:13:55,260
formalise things it's a markov random field you've stochastic binary visible units of stochastic binary

1059
01:13:55,260 --> 01:13:57,800
hidden units and you have bipartite connections

1060
01:13:58,500 --> 01:13:59,240
i u can

1061
01:13:59,830 --> 01:14:04,440
write the joint probability distribution this way and it's a very standard way of of

1062
01:14:04,440 --> 01:14:07,680
defining undirected graphical models have pairwise potentials you have

1063
01:14:08,680 --> 01:14:09,640
unit potentials

1064
01:14:10,280 --> 01:14:13,880
the most important thing here is that you can think of as stochastic hidden variables

1065
01:14:13,880 --> 01:14:16,040
is detecting certain patterns that you see in the data

1066
01:14:16,510 --> 01:14:22,350
and the conditional distribution here it is again taking a very simple form the product of conditionals which conditional picks

1067
01:14:23,360 --> 01:14:24,520
a logistic function

1068
01:14:25,320 --> 01:14:27,540
now when dealing with real valued data like

1069
01:14:29,530 --> 01:14:29,850
we can

1070
01:14:30,280 --> 01:14:35,450
make slight modification to the model and and work with a model called gas in burnley markov random field

1071
01:14:35,980 --> 01:14:38,980
right again the structure of the model remains pretty much the same

1072
01:14:40,110 --> 01:14:42,090
you have pairwise potentials potentials

1073
01:14:42,480 --> 01:14:48,630
and the conditional distributions of the observed data is given by the product of conditionals

1074
01:14:48,630 --> 01:14:50,370
wage conditional takes a gas distribution

1075
01:14:50,860 --> 01:14:52,700
and these kinds of models have been used fore

1076
01:14:53,230 --> 01:14:55,750
modeling images image patches and such

1077
01:14:57,350 --> 01:15:01,260
now at the final stage one a model discrete sparse data count data

1078
01:15:01,800 --> 01:15:06,060
like in the bag-of-words representation we again make a slight modification to the model and

1079
01:15:06,060 --> 01:15:09,840
this is what's called replicated softmax model it's an undirected topic model again

1080
01:15:10,440 --> 01:15:11,980
it has the similar structure

1081
01:15:12,570 --> 01:15:17,180
the most important thing is that the conditional distribution is given by the softmax distribution so if you think of

1082
01:15:18,850 --> 01:15:22,880
the bag of words representation you have a vocabulary of size kate and you have dy

1083
01:15:24,370 --> 01:15:25,560
then we can use the model

1084
01:15:26,270 --> 01:15:30,240
again the point i was make is is that we have this building block and we can use it

1085
01:15:30,720 --> 01:15:32,490
four modeling different kinds of

1086
01:15:33,850 --> 01:15:37,230
now one useful thing about this is that most machines is that it's very easy

1087
01:15:37,230 --> 01:15:40,880
to infer the states of the latent variables the states of the hidden variables

1088
01:15:41,420 --> 01:15:44,940
right and takes a closed form has a closed form solution so it's very important

1089
01:15:44,940 --> 01:15:50,590
for things like information retrieval classification wear perception is easy conditionally input i can instantly

1090
01:15:50,590 --> 01:15:50,960
tell u

1091
01:15:52,240 --> 01:15:53,910
what latent presentations are

1092
01:15:54,360 --> 01:15:56,510
and if you look at all of these different models

1093
01:15:56,930 --> 01:16:00,030
this essentially all have binary hidden variables and using them to model

1094
01:16:00,450 --> 01:16:01,300
different kinds of data

1095
01:16:02,630 --> 01:16:04,250
now you can think of a simple model

1096
01:16:05,200 --> 01:16:06,000
simple multimodal

1097
01:16:07,020 --> 01:16:09,330
model where you just using a joint binary hidden layer

1098
01:16:10,230 --> 01:16:13,540
now there are several problems with at a particular model in particular as i mentioned

1099
01:16:13,540 --> 01:16:18,240
before inputs have very different statistical properties and it's very difficult to learn meaningful model

1100
01:16:18,240 --> 01:16:23,160
i have this five by kind of twenty six or a mysterious past only has

1101
01:16:23,160 --> 01:16:24,720
like five five ones

1102
01:16:24,800 --> 01:16:26,490
that encodes

1103
01:16:26,510 --> 01:16:27,700
my wife

1104
01:16:32,820 --> 01:16:42,090
the other part need is now encodings are actually pairs of consecutive pairs

1105
01:16:42,160 --> 01:16:46,090
we're going to go with this i'm going to write down the objective function

1106
01:16:46,090 --> 01:16:49,660
instead of just in terms of why i'm going to read the terms of disease

1107
01:16:49,840 --> 01:16:52,160
because you know we're doing argmax over y

1108
01:16:52,180 --> 01:16:56,320
we do now argmaxa risi so one express the objective in terms of disease

1109
01:16:56,360 --> 01:16:58,800
so this helps me codes

1110
01:16:58,860 --> 01:17:00,180
why in this way

1111
01:17:00,200 --> 01:17:04,070
next step is going to care in this

1112
01:17:04,090 --> 01:17:07,090
sequence model of pairwise terms

1113
01:17:07,120 --> 01:17:11,860
so what i'm going code is well you know this this and this and one

1114
01:17:11,860 --> 01:17:13,090
matrix so sold

1115
01:17:13,180 --> 01:17:16,030
the pair is a whole baby

1116
01:17:16,050 --> 01:17:17,530
right so

1117
01:17:17,570 --> 01:17:21,300
there's going to be one position on this matter

1118
01:17:21,470 --> 01:17:23,410
so this is this

1119
01:17:23,430 --> 01:17:27,240
you can think of it as an indicator variable that says first position is a

1120
01:17:27,280 --> 01:17:29,720
in second position is b

1121
01:17:29,740 --> 01:17:33,300
and there's you know twenty six about twenty six and so there no

1122
01:17:35,070 --> 01:17:37,410
for this

1123
01:17:37,430 --> 01:17:39,430
and these four there's only

1124
01:17:39,450 --> 01:17:40,820
four wants to know

1125
01:17:43,030 --> 01:17:44,390
the discrete y

1126
01:17:44,410 --> 01:17:48,930
this is how the maps into this kind of these are one variables e

1127
01:17:48,930 --> 01:17:55,570
now we are ready to write down the words in terms of c

1128
01:17:55,590 --> 01:18:00,950
so what we do is basically take whatever it was was

1129
01:18:00,970 --> 01:18:03,590
whatever objective was hanging on particular

1130
01:18:03,620 --> 01:18:04,660
y j

1131
01:18:04,680 --> 01:18:07,280
we right now in terms of the j

1132
01:18:07,300 --> 01:18:11,340
because it is easier said is the position j

1133
01:18:11,370 --> 01:18:12,870
has label and

1134
01:18:12,890 --> 01:18:15,120
and so we take the score in

1135
01:18:15,140 --> 01:18:17,010
part of the potential that the

1136
01:18:17,010 --> 01:18:20,570
talks about and we that's the coefficient

1137
01:18:20,600 --> 01:18:25,300
max vision was used so these are when maximizing this is the constant

1138
01:18:25,300 --> 01:18:26,300
from me

1139
01:18:26,360 --> 01:18:30,490
same thing with the edge variables right so i visited variables atomic

1140
01:18:30,490 --> 01:18:32,890
and then these and this is the constant

1141
01:18:32,890 --> 01:18:34,120
with this when and they the

1142
01:18:35,470 --> 01:18:39,660
OK so now my objective this objective is the same as the objective with the

1143
01:18:39,660 --> 01:18:40,910
discrete y

1144
01:18:40,930 --> 01:18:42,700
but now

1145
01:18:42,720 --> 01:18:49,240
you don't need to actually give the meaning right otherwise this maximization is completely arbitrary

1146
01:18:49,410 --> 01:18:53,320
so what i want i mean this is have to be really zero one

1147
01:18:53,340 --> 01:18:56,530
if i if i remember zero one as an integer programme

1148
01:18:56,550 --> 01:18:57,720
which is you know

1149
01:18:57,720 --> 01:19:00,030
now what i want i want something

1150
01:19:00,050 --> 01:19:06,050
continuous so it turns out it's not the following it's enough to say the positive

1151
01:19:06,050 --> 01:19:10,510
to say that each one sums to one meaning that

1152
01:19:10,530 --> 01:19:15,160
you only one position are only one position is set in each

1153
01:19:15,180 --> 01:19:16,820
consecutive slots

1154
01:19:17,410 --> 01:19:19,870
so that we basically

1155
01:19:20,410 --> 01:19:25,050
yeah work including this thing and then we need to ensure that a

1156
01:19:25,070 --> 01:19:31,050
z jason case are consistent with each other rights that they they encode the same

1157
01:19:32,390 --> 01:19:38,740
right because there's cause decays into scorns sieges and those need to be coordinated

1158
01:19:38,740 --> 01:19:40,910
you know otherwise you can just you

1159
01:19:40,990 --> 01:19:44,720
maximize the it score separately from the notes core and then you don't have to

1160
01:19:45,180 --> 01:19:47,430
consistent while you're talking about

1161
01:19:47,800 --> 01:19:53,120
so in order to synchronize with e g t decays thinking was just think

1162
01:19:53,140 --> 01:19:58,070
it's enough to do the following terms is basically enforce agreement meaning so

1163
01:19:58,100 --> 01:20:00,660
i'm looking in particular jk

1164
01:20:00,660 --> 01:20:03,570
and there's some over one side

1165
01:20:03,640 --> 01:20:06,220
should get some here

1166
01:20:06,240 --> 01:20:08,660
this can marginalized one way

1167
01:20:08,720 --> 01:20:10,570
and this on the other way

1168
01:20:10,570 --> 01:20:12,890
i should also get the

1169
01:20:15,780 --> 01:20:17,450
if you think of this

1170
01:20:17,470 --> 01:20:22,780
kind of us in all pairwise presentation that barroso presentation has to agree with the

1171
01:20:22,870 --> 01:20:25,200
una representation

1172
01:20:25,200 --> 01:20:28,070
so the linear constraints and uneasy

1173
01:20:28,070 --> 01:20:31,050
this is sort of linear inequalities right so

1174
01:20:31,070 --> 01:20:32,640
we haven't said

1175
01:20:32,680 --> 01:20:34,450
we have made disease

1176
01:20:34,470 --> 01:20:38,660
the discrete we just mean that still continues

1177
01:20:38,660 --> 01:20:40,510
well it turns out that you can you can

1178
01:20:41,340 --> 01:20:45,700
that if original network is the chain or tree

1179
01:20:45,740 --> 01:20:50,090
the answer yet is always going to be integral

1180
01:20:50,100 --> 01:20:51,200
right so so

1181
01:20:51,200 --> 01:20:53,030
going from

1182
01:20:53,050 --> 01:20:57,300
the picture this is something like this if you have

1183
01:20:57,300 --> 01:21:00,180
in fact the learner knows that it can take advantage of it and just learn

1184
01:21:00,190 --> 01:21:04,300
one to transition probabilities that can be applied in both of these states and the

1185
01:21:04,310 --> 01:21:08,850
state and this state right to states have the same pattern of walls

1186
01:21:09,240 --> 01:21:13,690
at two broad hypothesis class would be one that says we don't pay attention to

1187
01:21:13,690 --> 01:21:17,950
learn about them separately to know what might be one this is yet the wall

1188
01:21:17,970 --> 01:21:21,470
learn from that there were great in this great but you can be there that's

1189
01:21:21,470 --> 01:21:23,330
not true and it falls apart

1190
01:21:23,340 --> 01:21:28,770
so rule what about a bayesian perspective what about instead of saying there's what's possible

1191
01:21:28,770 --> 01:21:32,380
and everything else is impossible what if we say there's likely

1192
01:21:32,390 --> 01:21:36,120
and other things might be less likely to things might be very unlikely and that

1193
01:21:36,120 --> 01:21:40,210
i think might be impossible OK now you can imagine learning algorithm will put energy

1194
01:21:40,210 --> 01:21:43,850
into learning the more likely cases but if they start to break down it's willing

1195
01:21:43,850 --> 01:21:48,150
to consider other possibilities as it goes not to mention the probability of the crossword

1196
01:21:48,150 --> 01:21:50,960
puzzles stuff but i did and that was really important thing in our past puzzle

1197
01:21:50,960 --> 01:21:54,440
work is not to say this would fit in the slot where doesn't but to

1198
01:21:54,440 --> 01:21:58,220
say this word is in good faith but not perfect one this one is less

1199
01:21:58,220 --> 01:22:02,240
good this was less good and have all the possibilities lying around so the solver

1200
01:22:02,240 --> 01:22:04,620
could put them together in other ways

1201
01:22:04,630 --> 01:22:07,830
but it's more likely to use the really good answers

1202
01:22:07,850 --> 01:22:10,850
so the bayes perspective would give us that kind of

1203
01:22:10,900 --> 01:22:14,670
that kind of you we start some kind of prior over the space transition probability

1204
01:22:14,670 --> 01:22:20,400
matrices and as we're learning as the ages learning environment it's maintaining a posterior well

1205
01:22:20,420 --> 01:22:23,310
here's what i originally thought this was likely but then i saw a bunch of

1206
01:22:23,310 --> 01:22:26,330
this data now i think this other thing is likely to

1207
01:22:26,350 --> 01:22:28,680
then we optimize

1208
01:22:28,690 --> 01:22:32,690
decision making for the probable instead of just possible so we don't have to all

1209
01:22:32,870 --> 01:22:36,120
the agent does not take this worst-case view of the rule

1210
01:22:36,130 --> 01:22:40,770
something really great under the stage but probably not so

1211
01:22:40,780 --> 01:22:42,370
why waste my time

1212
01:22:42,380 --> 01:22:44,390
all right

1213
01:22:44,410 --> 01:22:49,120
so in this particular case you could say similar states have similar dynamics probably

1214
01:22:49,140 --> 01:22:51,310
right the learner should be able to use that

1215
01:22:51,330 --> 01:22:55,070
and are faster than if it was using that information so this this kind of

1216
01:22:55,070 --> 01:22:59,020
bayesian view can actually drive exploration and result in much faster learning

1217
01:22:59,040 --> 01:23:01,510
so what's nice is there's a whole

1218
01:23:01,660 --> 01:23:03,570
field of

1219
01:23:03,830 --> 01:23:08,300
bayesian reinforcement learning see is very text was one way to get here with a

1220
01:23:08,300 --> 01:23:11,620
bayesian representation of models we can plan in the space of posteriors also

1221
01:23:11,630 --> 01:23:14,970
i don't want to get into this in detail but the most most of the

1222
01:23:14,970 --> 01:23:16,410
way that bayesian

1223
01:23:16,420 --> 01:23:20,620
modeling has been used in reinforcement learning is to say you know what this is

1224
01:23:20,620 --> 01:23:24,640
really cool if we think about instead of model there's there's you know there's the

1225
01:23:24,640 --> 01:23:28,020
one truth and here's what we know about so far but if instead we have

1226
01:23:28,030 --> 01:23:30,560
a probability distribution over models we can say

1227
01:23:30,960 --> 01:23:35,490
well if i take this step but this time my my distribution of models tells

1228
01:23:35,490 --> 01:23:39,320
me what might happen and also what i would learn from that and so i

1229
01:23:39,320 --> 01:23:43,460
can in my head start learning about start reasoning about what things i should do

1230
01:23:43,460 --> 01:23:46,890
in the world to learn the model and optimize the model the the same time

1231
01:23:47,520 --> 01:23:53,400
what's cool about that is races this this possibly artificial distinction between exploring and exploiting

1232
01:23:53,400 --> 01:23:58,500
or learning and acting it's just one big planning problem planned to get my information

1233
01:23:58,500 --> 01:24:01,900
state to be one where i can get higher reward

1234
01:24:02,580 --> 01:24:08,100
this really beautifully elegant from a mathematical standpoint and really hideously intractable in computational view

1235
01:24:08,340 --> 01:24:12,190
there are some cases that they can be solved if if there's no states just

1236
01:24:12,190 --> 01:24:16,070
there's one stay in actions don't change the state they give you reward or if

1237
01:24:16,070 --> 01:24:21,180
the horizon length is actually relatively short then you can actually reason about all possible

1238
01:24:21,180 --> 01:24:23,220
reachable belief states

1239
01:24:23,240 --> 01:24:26,070
states knowledge and and

1240
01:24:26,080 --> 01:24:29,740
optimized for that but in the general case this is actually pretty pretty horrible so

1241
01:24:29,740 --> 01:24:32,630
i'm not going to go through this concrete example but it just the idea here

1242
01:24:32,630 --> 01:24:36,900
is to showing that when you have a distribution over what the possible models are

1243
01:24:36,900 --> 01:24:40,440
sometimes it gets you to take actions that would actually cause it because you learn

1244
01:24:40,550 --> 01:24:43,550
like specifically the optimal thing to do here is to learn because that's going to

1245
01:24:43,550 --> 01:24:47,120
allow me to get higher reward in the future

1246
01:24:47,160 --> 01:24:51,280
so it's very pretty if you really doing this in in the full MDP etc

1247
01:24:51,280 --> 01:24:56,750
etc you probability distributions over probability distributions so all the various things that people have

1248
01:24:56,750 --> 01:24:57,970
been doing lately in

1249
01:24:58,000 --> 01:25:00,550
especially the NIPS community in terms of

1250
01:25:00,810 --> 01:25:08,150
displays and nonparametric bayes all fits very nicely into this kind of you

1251
01:25:08,290 --> 01:25:13,870
in fact people have made various kinds of approximations to get bayes optimal plans so

1252
01:25:13,870 --> 01:25:18,360
the idea of the bayes optimal plan is given you're given the agents uncertainty about

1253
01:25:18,360 --> 01:25:23,530
the model space what actions should take to maximize its expected reward expected reward is

1254
01:25:23,530 --> 01:25:28,910
over not just uncertain transitions in the probability sorry in the trajectory but also

1255
01:25:28,930 --> 01:25:31,400
uncertainty about what the true model is

1256
01:25:31,420 --> 01:25:36,230
that the whole expectation and getting high reward maximizing reward with respect to that sort

1257
01:25:36,230 --> 01:25:40,610
like search shows research bottom-up learning and so forth

1258
01:25:40,610 --> 01:25:43,750
so that's the learning

1259
01:25:43,750 --> 01:25:49,440
let me briefly mention the software in which all of this is available

1260
01:25:49,520 --> 01:25:51,480
it's called alchemy

1261
01:25:51,520 --> 01:25:54,040
it's open source

1262
01:25:54,060 --> 01:25:58,360
it allows you to write a new knowledge or captured from somewhere in the form

1263
01:25:58,360 --> 01:26:00,270
of a first order formulas

1264
01:26:00,580 --> 01:26:03,790
and then all the other things that i just described for learning and inference are

1265
01:26:04,920 --> 01:26:10,340
in in addition you have programming language features like you know file inclusion typing

1266
01:26:10,400 --> 01:26:14,540
syntactic sugar for the things that are most often is the ability to link in

1267
01:26:14,540 --> 01:26:17,080
external functions and predicates and so forth

1268
01:26:17,230 --> 01:26:21,040
the URL i'll put up again at the end

1269
01:26:21,110 --> 01:26:26,670
it's interesting to compare alchemy with the kinds of things that were available before

1270
01:26:26,810 --> 01:26:33,380
on both the logical side and statistical site so comparing alchemy prolog

1271
01:26:33,400 --> 01:26:36,190
in prolog the representation has horn clauses

1272
01:26:36,190 --> 01:26:40,480
and alchemy it's arbitrary first of the four most welcome is more flexible in that

1273
01:26:40,480 --> 01:26:44,380
respect in prolog inference is done by theorem proving which you know back in the

1274
01:26:44,380 --> 01:26:47,040
seventies was the fastest for different available

1275
01:26:47,110 --> 01:26:52,790
but you know these days you know the fastest inferences by model checking by satisfiability

1276
01:26:52,860 --> 01:26:53,940
that's what we do

1277
01:26:54,190 --> 01:26:58,980
but of course most importantly in alchemy you've learning and handling of uncertainty right out

1278
01:26:58,980 --> 01:27:01,610
of the box and prolog you don't

1279
01:27:02,000 --> 01:27:06,130
now comparing alchemy with sort like that

1280
01:27:06,150 --> 01:27:10,170
so after that exists out there for doing things like statistical inference in machine learning

1281
01:27:10,340 --> 01:27:12,290
is of course many different packages

1282
01:27:12,310 --> 01:27:16,420
no single one that's for everything but probably the most popular one by a good

1283
01:27:16,420 --> 01:27:18,520
margin is something called bugs

1284
01:27:18,540 --> 01:27:21,940
meaning you know the bayesian inference using gibbs sampling

1285
01:27:22,230 --> 01:27:28,060
so but these days and that the representation of the muses markov nets involves influences

1286
01:27:28,060 --> 01:27:31,540
them by gibbs sampling which we already saw can get very very slow if you

1287
01:27:31,540 --> 01:27:35,960
have strong dependencies in community have given sampling in a number of other things including

1288
01:27:36,210 --> 01:27:38,500
SAT which is most much faster

1289
01:27:38,520 --> 01:27:42,980
in but you can only the parameters in alchemy can also in structure but of

1290
01:27:42,980 --> 01:27:47,610
course the most important difference is that alchemy is relational in but there is no

1291
01:27:47,610 --> 01:27:51,710
simple general way to handle non IID data in alchemy is just saw this is

1292
01:27:51,710 --> 01:27:52,750
very easy

1293
01:27:53,170 --> 01:27:58,630
so i can be even looking only as logical erroneous probability is already in many

1294
01:27:58,630 --> 01:28:01,250
ways in advance over the state of the art but the nice thing of course

1295
01:28:01,250 --> 01:28:04,040
is that it seamlessly combines these two things

1296
01:28:04,060 --> 01:28:07,790
you have a problem that has both structured and unstructured aspects you don't have to

1297
01:28:07,790 --> 01:28:10,940
put together pieces you don't have to start with a prologue and try to make

1298
01:28:10,940 --> 01:28:15,770
a probabilistic or start with bugs and try to handle relational information it's all done

1299
01:28:15,770 --> 01:28:19,460
very uniformly smoothly in the language markov logic

1300
01:28:19,480 --> 01:28:24,270
so last part of the talk

1301
01:28:24,580 --> 01:28:28,150
let me mention some examples of what you can do with markov logic and in

1302
01:28:28,150 --> 01:28:33,230
some sense this is the payoff right so far been investing in understanding this representation

1303
01:28:33,230 --> 01:28:37,210
in developing the arguments for so what is it that you can do with this

1304
01:28:37,690 --> 01:28:39,770
is an awful lot it turns out

1305
01:28:39,790 --> 01:28:42,130
so here's here's the sample

1306
01:28:42,150 --> 01:28:47,710
some of the things that markov logic and alchemy in some cases other implementations of

1307
01:28:47,710 --> 01:28:52,040
markov logic that people have developed things that have been done

1308
01:28:52,040 --> 01:28:56,680
information extraction is one it's actually example that i'm going to go over here in

1309
01:28:56,680 --> 01:29:01,210
some detail to illustrate things is of course a very important problem

1310
01:29:01,230 --> 01:29:05,880
there was a system by reasonable enquiry that actually won the the triple l two

1311
01:29:05,880 --> 01:29:07,060
thousand five

1312
01:29:07,150 --> 01:29:12,460
information extraction competition using markov logic this was the goal of this was to extract

1313
01:29:12,500 --> 01:29:15,750
interactions from from medline abstracts

1314
01:29:15,770 --> 01:29:20,440
in two resolution another very important problem link prediction

1315
01:29:20,440 --> 01:29:25,400
collective classification things like classifying web pages using the links as well as the content

1316
01:29:26,040 --> 01:29:30,730
various kinds of web mining and natural language processing i just came from EMNLP where

1317
01:29:30,730 --> 01:29:36,540
we have the paper applying markov logic to natural language processing ontology refinement

1318
01:29:36,560 --> 01:29:41,250
another very important problem as i mentioned the best paper award can last year was

1319
01:29:41,650 --> 01:29:44,480
a paper by free will and weld

1320
01:29:44,500 --> 01:29:50,730
they use markov logic to learn and refine ontologies from from wikipedia and then other

1321
01:29:50,730 --> 01:29:53,670
things like on by of social network analysis

1322
01:29:53,690 --> 01:29:58,040
and so forth is a couple of examples i think worth mentioning site

1323
01:29:58,060 --> 01:30:01,940
which is some of you know is the world's largest knowledge base right but inexact

1324
01:30:02,000 --> 01:30:06,440
succeed because of the problems in first order logic the folks it's i caught him

1325
01:30:06,480 --> 01:30:09,650
began to transform psyche into into an MLN

1326
01:30:09,670 --> 01:30:13,630
by adding weight so therefore it was right to the very straightforward thing to do

1327
01:30:14,020 --> 01:30:15,360
and another one is

1328
01:30:15,710 --> 01:30:19,920
OK well the largest project in derby history which is trying to build this automated

1329
01:30:19,920 --> 01:30:25,670
personal assistant coach manages email and new meetings for you automatically fast things and folders

1330
01:30:25,670 --> 01:30:31,080
and what not the main inference engine that killer uses is is based on on

1331
01:30:31,080 --> 01:30:35,440
markov logic gates and there's more of these things which hopefully and again you can

1332
01:30:35,440 --> 01:30:39,290
go to the alchemy website and there's you know a bunch of pointers to papers

1333
01:30:39,290 --> 01:30:43,770
and applications and melons and databases and what not there but you know hopefully this

1334
01:30:43,770 --> 01:30:49,020
will precisely that this really is something that you can do state-of-the-art solutions for many

1335
01:30:49,500 --> 01:30:55,710
important problems in information and knowledge management today so for concreteness let me as the

1336
01:30:55,710 --> 01:30:57,310
last thing in this talk

1337
01:30:57,440 --> 01:31:00,000
before conclude just show you

1338
01:31:00,080 --> 01:31:04,560
how we can do something like information extraction in markov logic and i think this

1339
01:31:04,560 --> 01:31:09,420
is a nice illustration because information extraction is used with essential bridge between the structure

1340
01:31:09,420 --> 01:31:14,190
and the instructions right you start out with unstructured it's just text and you populate

1341
01:31:14,460 --> 01:31:19,230
database and i can do database inference and logical inference of

1342
01:31:19,980 --> 01:31:25,400
i'm going to use as an example the problem of extracting a database of citations

1343
01:31:25,420 --> 01:31:29,520
from the reference lists and papers right this is the problem that citeseer and google

1344
01:31:29,520 --> 01:31:31,000
scholar have to solve

1345
01:31:31,000 --> 01:31:34,790
very problem very popular problem among researchers today

1346
01:31:34,790 --> 01:31:36,920
so here's a very small example

1347
01:31:36,940 --> 01:31:38,580
and now

1348
01:31:38,600 --> 01:31:42,610
you need to do to make things information extraction the first one is what's called

1349
01:31:43,750 --> 01:31:47,250
i need to run over the text string and figure out which

1350
01:31:47,270 --> 01:31:49,500
fields of my database are

1351
01:31:49,520 --> 01:31:53,690
OK so for example proxy england competed ming authors

1352
01:31:53,710 --> 01:31:56,210
triple six is very new and so forth

1353
01:31:56,330 --> 01:31:59,150
so this is the first part started segmentation

1354
01:31:59,330 --> 01:32:00,710
the other part

1355
01:32:00,920 --> 01:32:02,730
is into resolution

1356
01:32:02,750 --> 01:32:04,900
i need to figure out there

1357
01:32:04,920 --> 01:32:09,400
tripoli IL six and the proceedings of the twenty first national conference on artificial intelligence

1358
01:32:09,400 --> 01:32:11,310
of the same things

1359
01:32:11,330 --> 01:32:14,810
and then i also to figure out that these two papers are the same thing

1360
01:32:14,810 --> 01:32:18,060
in this two papers the same and these are the results

1361
01:32:18,080 --> 01:32:22,810
otherwise likely my database i'm going to get duplicate results right which should probably seen

1362
01:32:22,810 --> 01:32:26,710
happen when you know the we could john mailings to you with different misspellings of

1363
01:32:26,730 --> 01:32:29,730
the name and so forth and if i try to do mining on top of

1364
01:32:29,730 --> 01:32:33,860
this and that is not properly resolved the results will be garbage so we need

1365
01:32:33,860 --> 01:32:35,650
to be able to do these two things

1366
01:32:35,710 --> 01:32:38,170
and the state of the art

1367
01:32:38,170 --> 01:32:40,170
prior diag

1368
01:32:40,240 --> 01:32:45,960
why is that then we could figure out what entry

1369
01:32:46,010 --> 01:32:48,200
it's a transpose

1370
01:32:48,240 --> 01:32:50,760
now if you remember this is

1371
01:32:51,020 --> 01:32:53,770
let's see what it looks like

1372
01:32:53,890 --> 01:32:56,590
suppose that was the identity for the moment

1373
01:32:56,600 --> 01:32:59,230
and the errors were white noise

1374
01:32:59,290 --> 01:33:04,260
you see that a transpose they will be blocked with the block diagonal

1375
01:33:04,340 --> 01:33:08,720
let's see what's going to be the shape of a transpose a what's the shape

1376
01:33:08,720 --> 01:33:09,980
of this matrix

1377
01:33:10,000 --> 01:33:12,610
blocks a three by three right

1378
01:33:12,620 --> 01:33:17,010
this is this is five by this is five five three block

1379
01:33:17,020 --> 01:33:18,710
five five five

1380
01:33:18,740 --> 01:33:20,640
block diagonal

1381
01:33:20,660 --> 01:33:24,480
three by five o'clock what counts as

1382
01:33:24,490 --> 01:33:28,830
OK so the result is a three by three block matrix

1383
01:33:31,290 --> 01:33:33,740
some three by three block matrix

1384
01:33:33,790 --> 01:33:37,510
and you know what we can figure out what it what it is but what

1385
01:33:37,510 --> 01:33:42,090
i want this block tried glaspie is crucial part is the

1386
01:33:42,100 --> 01:33:44,340
is the fact that zero there

1387
01:33:44,380 --> 01:33:47,480
why is that while those zero

1388
01:33:47,490 --> 01:33:51,240
you see that this role

1389
01:33:51,250 --> 01:33:52,430
this column

1390
01:33:52,460 --> 01:33:55,550
is perpendicular to the skull

1391
01:33:55,570 --> 01:33:56,620
right there is

1392
01:33:56,630 --> 01:33:58,020
that that's the point

1393
01:33:58,040 --> 01:33:59,960
we were built in total

1394
01:34:00,010 --> 01:34:05,000
that that each column has some overlap with the next one

1395
01:34:05,060 --> 01:34:06,720
so there's is

1396
01:34:06,770 --> 01:34:11,720
there's one diagonal away from the main died but then when we take the

1397
01:34:11,730 --> 01:34:14,700
when we take that as the role

1398
01:34:14,720 --> 01:34:16,920
into this is the columns

1399
01:34:16,930 --> 01:34:17,820
to get

1400
01:34:17,830 --> 01:34:20,580
this guy it's zero

1401
01:34:20,590 --> 01:34:22,860
so the this structure

1402
01:34:22,920 --> 01:34:27,120
of connecting each one the only the only its neighbour

1403
01:34:27,220 --> 01:34:30,160
is automatically it's are it's are

1404
01:34:31,290 --> 01:34:37,550
the appearing framework that produces try diagonal makers

1405
01:34:37,560 --> 01:34:41,450
here blocked right go because each of these is system OK

1406
01:34:41,490 --> 01:34:44,410
now then the point is that

1407
01:34:44,420 --> 01:34:49,210
blocked right diagonal matrices

1408
01:34:54,180 --> 01:34:58,140
as we again what we learn that very first lecture was doing elimination of the

1409
01:34:58,140 --> 01:35:00,340
things that the it's

1410
01:35:00,380 --> 01:35:04,880
it's a manageable process right i just i just have one step to do i

1411
01:35:04,880 --> 01:35:08,320
multiply that role by something to kill this

1412
01:35:08,330 --> 01:35:10,580
and then this rose ready to go

1413
01:35:10,590 --> 01:35:14,010
and i'm by the the new role by something to kill this

1414
01:35:14,040 --> 01:35:17,140
this is the natural recursion

1415
01:35:17,170 --> 01:35:20,130
and then what about back substitution

1416
01:35:20,190 --> 01:35:21,840
back substitution

1417
01:35:21,890 --> 01:35:25,130
remember uses the upper triangular form

1418
01:35:25,180 --> 01:35:26,980
to go back

1419
01:35:26,990 --> 01:35:31,850
the upper triangular form how many diagonals and the of the when i factor this

1420
01:35:32,740 --> 01:35:39,670
in two one l new

1421
01:35:39,720 --> 01:35:44,430
what's what's with and this is the l so it's certainly zero there

1422
01:35:45,750 --> 01:35:49,240
i don't need to do an elimination here it's zero here

1423
01:35:49,300 --> 01:35:50,700
so it's something

1424
01:35:50,710 --> 01:35:53,270
something something something something

1425
01:35:53,330 --> 01:35:56,770
and the u is the same in the opposite direction

1426
01:35:57,820 --> 01:36:00,350
just got

1427
01:36:00,470 --> 01:36:02,250
got zero there

1428
01:36:02,260 --> 01:36:05,450
because because the zero there never leaves

1429
01:36:05,460 --> 01:36:06,300
and it

1430
01:36:06,310 --> 01:36:11,590
upper triangular so there is the element there's the block and u

1431
01:36:11,630 --> 01:36:18,260
so the forward elimination is just one multiplier and every and every stage the back

1432
01:36:18,260 --> 01:36:22,110
substitution is just one substitution every state

1433
01:36:24,290 --> 01:36:27,080
and we can create formulas

1434
01:36:27,110 --> 01:36:30,480
so this is what the kalman filter has them out

1435
01:36:30,540 --> 01:36:34,100
the kalman filter has to amount to inform the formulas

1436
01:36:34,260 --> 01:36:35,090
for the

1437
01:36:35,120 --> 01:36:41,870
is for these steps forward this is forward this prediction using l

1438
01:36:41,930 --> 01:36:44,220
ali and this is important

1439
01:36:44,230 --> 01:36:47,460
when i better say what are the outputs one of the inputs and one of

1440
01:36:47,460 --> 01:36:49,020
the output

1441
01:36:49,080 --> 01:36:50,720
per step

1442
01:36:50,840 --> 01:36:52,490
first step OK

1443
01:37:00,140 --> 01:37:01,950
OK one of the input

1444
01:37:01,960 --> 01:37:03,960
there's a new a

1445
01:37:03,970 --> 01:37:05,310
a k

1446
01:37:07,260 --> 01:37:09,390
there's a new path

1447
01:37:09,440 --> 01:37:11,660
after k

1448
01:37:11,820 --> 01:37:17,460
well let me put the first because it kind of comes with the a

1449
01:37:17,510 --> 01:37:21,130
so there's the A's and the bees at the new steps

1450
01:37:21,180 --> 01:37:22,520
and then

1451
01:37:22,540 --> 01:37:26,820
and that's the measurements step but then there's also very

1452
01:37:26,870 --> 01:37:28,370
prediction step

1453
01:37:28,500 --> 01:37:30,900
is a we've got two kinds of things here

1454
01:37:30,950 --> 01:37:32,940
we've got the measurement equations

1455
01:37:33,900 --> 01:37:37,520
status quite change of state equations the prediction equation

1456
01:37:39,660 --> 01:37:42,000
so we could

1457
01:37:44,190 --> 01:37:45,750
and one of the output

1458
01:37:47,270 --> 01:37:50,720
i think the best way to do the right thing to do is to break

1459
01:37:50,720 --> 01:37:54,640
each step in the into the two pieces

1460
01:37:54,640 --> 01:37:56,200
the two pieces

1461
01:37:56,210 --> 01:37:59,960
so maybe maybe when i go to the new step

1462
01:38:00,010 --> 01:38:02,140
really the f comes before

1463
01:38:02,150 --> 01:38:06,010
the a and b the new step so i'm i'm better off if i write

1464
01:38:06,970 --> 01:38:11,890
systematically think of the after coming first

1465
01:38:11,940 --> 01:38:13,230
as as giving

1466
01:38:13,250 --> 01:38:17,370
prediction of the new state

1467
01:38:17,450 --> 01:38:19,050
based on the old one

1468
01:38:19,110 --> 01:38:19,730
the all

1469
01:38:19,750 --> 01:38:21,540
the old our oldest

1470
01:38:21,630 --> 01:38:24,500
and then we make some measurements at the new time

1471
01:38:24,550 --> 01:38:25,620
and that

1472
01:38:25,620 --> 01:38:27,990
this gives us an improved version

1473
01:38:29,480 --> 01:38:30,820
so like

1474
01:38:30,870 --> 01:38:33,110
then A's and B's

1475
01:38:33,160 --> 01:38:35,030
at the new time OK

1476
01:38:35,030 --> 01:38:37,670
my that be

1477
01:38:37,700 --> 01:38:40,160
i will go here

1478
01:38:40,220 --> 01:38:42,150
and my talk a

1479
01:38:42,160 --> 01:38:44,730
i will go above it

1480
01:38:45,470 --> 01:38:46,530
yeah that's

1481
01:38:46,570 --> 01:38:48,880
one unit

1482
01:38:48,890 --> 01:38:54,320
and i have a right angle so i can use the pythagorean feeling

1483
01:38:54,330 --> 01:38:57,280
to find that

1484
01:38:57,280 --> 01:39:00,960
the length a square of equal length

1485
01:39:00,970 --> 01:39:03,380
well at least

1486
01:39:04,530 --> 01:39:07,830
we are reduced to finding the length of the length of the we can again

1487
01:39:07,830 --> 01:39:12,550
find using the ethical and feeling in the x y plane because here

1488
01:39:12,570 --> 01:39:14,040
we have a right angle

1489
01:39:14,050 --> 01:39:17,880
we have three units and here we have two units

1490
01:39:17,890 --> 01:39:21,080
OK so if you do the calculations you will see

1491
01:39:22,150 --> 01:39:25,740
well length of b is square root of three square miles to square the thirty

1492
01:39:25,780 --> 01:39:29,030
square of thirty

1493
01:39:30,520 --> 01:39:38,300
and the length of

1494
01:39:38,460 --> 01:39:39,950
is about

1495
01:39:39,960 --> 01:39:42,540
length the squared plus one

1496
01:39:42,600 --> 01:39:44,740
square you want

1497
01:39:46,160 --> 01:39:52,370
it's going to be square root of thirty first one is twelve fourteen

1498
01:39:52,430 --> 01:39:54,830
and so on and to which

1499
01:39:54,840 --> 01:39:59,250
almost all of you gave

1500
01:39:59,350 --> 01:40:01,000
OK so the original formula

1501
01:40:01,020 --> 01:40:04,380
you know if you follow it with it in general

1502
01:40:05,100 --> 01:40:07,850
if we have

1503
01:40:09,340 --> 01:40:12,070
with components a one a two

1504
01:40:12,110 --> 01:40:13,210
a a

1505
01:40:13,220 --> 01:40:15,370
then the line

1506
01:40:15,390 --> 01:40:20,340
a is the square root of a one square loss it was squared

1507
01:40:20,380 --> 01:40:24,220
classification with

1508
01:40:24,240 --> 01:40:30,000
any questions about that

1509
01:40:31,480 --> 01:40:40,830
yes so in general we indeed can consider vectors in abstract spaces that have any

1510
01:40:40,830 --> 01:40:42,490
number of coordinates

1511
01:40:42,580 --> 01:40:44,710
and then you have more components

1512
01:40:44,720 --> 01:40:47,240
in this class will mostly see vectors

1513
01:40:47,250 --> 01:40:51,390
with two of three components only because easier to the whole and because a lot

1514
01:40:51,390 --> 01:40:55,570
of the method we'll see what exactly the same way with you have three variables

1515
01:40:55,570 --> 01:40:57,220
of million volumes

1516
01:40:57,270 --> 01:40:58,470
if we had

1517
01:40:58,480 --> 01:41:02,020
vector with smart components then we would have a lot of trouble following it

1518
01:41:02,100 --> 01:41:04,750
but we could still define its length in the same way

1519
01:41:04,760 --> 01:41:07,370
by summing the squares of the components

1520
01:41:09,200 --> 01:41:10,520
i'm sorry to say that here

1521
01:41:10,550 --> 01:41:11,140
you know

1522
01:41:11,160 --> 01:41:14,430
multivariable mostly women mostly to offer

1523
01:41:16,560 --> 01:41:20,660
be assured that it was just the same way you have ten thousand i

1524
01:41:20,750 --> 01:41:25,220
just calculations some longer

1525
01:41:28,940 --> 01:41:30,790
more questions

1526
01:41:33,390 --> 01:41:36,610
what else can we do with vector as well i think that i'm sure you

1527
01:41:36,610 --> 01:41:38,590
know how to do with vectors

1528
01:41:38,630 --> 01:41:40,670
is to add them

1529
01:41:40,690 --> 01:41:42,840
to scale

1530
01:41:47,500 --> 01:41:48,720
vector addition

1531
01:41:51,270 --> 01:41:53,540
so if you have two vectors

1532
01:41:53,550 --> 01:41:55,350
a and b

1533
01:41:55,360 --> 01:41:58,040
you can from the some some a

1534
01:41:58,090 --> 01:42:00,340
how do we do that well first

1535
01:42:00,510 --> 01:42:04,320
i tell you that does not have this double life there at the same time

1536
01:42:04,330 --> 01:42:07,740
geometric objects that we can like this some pictures

1537
01:42:07,990 --> 01:42:12,380
fair also computational objects but we can present by number

1538
01:42:12,390 --> 01:42:18,250
so every question about vectors will have two instance one geometric and one numerical

1539
01:42:18,260 --> 01:42:20,070
so let's start with the geometry

1540
01:42:20,100 --> 01:42:22,490
so let's say that they have two vectors

1541
01:42:22,510 --> 01:42:24,360
a and b given to me

1542
01:42:26,310 --> 01:42:30,660
let's say that i thought of going the same place to start

1543
01:42:30,830 --> 01:42:35,540
well to take this same what they should do is actually move b

1544
01:42:35,550 --> 01:42:38,070
so that it starts at the end of it

1545
01:42:38,140 --> 01:42:40,630
the head of

1546
01:42:40,760 --> 01:42:43,710
OK so this is again the

1547
01:42:43,720 --> 01:42:47,040
so so this forms no part of it

1548
01:42:48,250 --> 01:42:52,200
so this site is again picked

1549
01:42:52,210 --> 01:42:52,980
and now

1550
01:42:53,020 --> 01:42:57,940
if we take the diagonal of that dialogue

1551
01:42:57,960 --> 01:43:00,630
this is what we call

1552
01:43:03,880 --> 01:43:05,660
OK so the idea being that

1553
01:43:05,680 --> 01:43:08,590
to move along a plus b it's the same as to move first along a

1554
01:43:08,590 --> 01:43:10,300
and then along

1555
01:43:10,320 --> 01:43:15,020
but on b then along a plus b equals because

1556
01:43:15,080 --> 01:43:18,420
OK now if we do it

1557
01:43:18,430 --> 01:43:19,870
new nightly

1558
01:43:23,680 --> 01:43:26,890
all you do is you just add the first component of a with the first

1559
01:43:26,890 --> 01:43:28,550
component of the

1560
01:43:28,560 --> 01:43:32,890
the second with second and the third with the

1561
01:43:32,910 --> 01:43:34,560
say that a

1562
01:43:34,570 --> 01:43:35,860
it was

1563
01:43:36,680 --> 01:43:38,320
in two three

1564
01:43:38,480 --> 01:43:40,190
he was one

1565
01:43:42,460 --> 01:43:44,890
then you just add this week

1566
01:43:44,910 --> 01:43:47,380
so it's pretty straightforward

1567
01:43:47,490 --> 01:43:50,960
so for example

1568
01:43:51,000 --> 01:43:56,020
you know i say that neither of its components of three two one

1569
01:43:56,020 --> 01:43:59,590
so we talked about fitting a line to given a bunch of points

1570
01:44:00,340 --> 01:44:01,880
when you're doing that line fitting

1571
01:44:03,100 --> 01:44:08,470
you're actually assuming that the number of call it's you have the those excess let's say you have

1572
01:44:08,910 --> 01:44:10,790
you know features you are dealing with it

1573
01:44:11,640 --> 01:44:14,320
and the number of samples to have in the training set

1574
01:44:15,040 --> 01:44:17,730
which is the actual data points ten dimensions

1575
01:44:19,160 --> 01:44:25,090
so so u assume that the number of samples you have is larger than the dimensionality of the data

1576
01:44:27,350 --> 01:44:31,040
so so you've maybe a hundred samples in ten dimensions

1577
01:44:31,760 --> 01:44:36,210
diabetes dataset in see i probably the most widely used has a hundred and fifty

1578
01:44:36,740 --> 01:44:38,910
data points each in four dimensions

1579
01:44:39,450 --> 01:44:40,450
in those cases

1580
01:44:40,840 --> 01:44:43,090
the standard theory that we talked about walks

1581
01:44:45,440 --> 01:44:48,900
many of us are getting into problems when the number of samples

1582
01:44:49,320 --> 01:44:53,140
it is not only smaller but we're always smaller than the dimensions

1583
01:44:53,870 --> 01:44:55,820
so i have a hundred thousand

1584
01:44:57,430 --> 01:44:59,950
and i only have a few thousand data points

1585
01:45:01,640 --> 01:45:08,950
so climate data is an example i am may be maintaining measuring you know precipitation are cambridgeshire

1586
01:45:09,640 --> 01:45:14,020
in the month of june over the last hundred years from inception reanalysis or something

1587
01:45:14,020 --> 01:45:17,970
like that so i have a hundred observations over the last hundred years one for

1588
01:45:17,970 --> 01:45:18,370
each year

1589
01:45:19,660 --> 01:45:25,210
look at the number of features i have this this can easily go into tens of thousands

1590
01:45:25,730 --> 01:45:27,070
if you're trying to fit

1591
01:45:28,020 --> 01:45:32,260
if you're trying to do ordinary least squares using your favourite tool it break down

1592
01:45:33,070 --> 01:45:35,240
if it actually gives you an answer it is line

1593
01:45:37,090 --> 01:45:40,450
it has to involve rank-deficient matrix

1594
01:45:41,290 --> 01:45:44,220
which cannot be done so ordinary least squares doesn't work in this and

1595
01:45:44,860 --> 01:45:47,450
in fact many models don't work in this scenario

1596
01:45:48,280 --> 01:45:51,900
i will give you the sort of the keynote example which is in this story

1597
01:45:51,920 --> 01:45:55,820
scenario i give you one data point and ask you to draw line

1598
01:45:56,790 --> 01:45:57,950
that's what we're talking about

1599
01:45:58,520 --> 01:45:59,700
right is complete insanity

1600
01:46:00,120 --> 01:46:02,590
if i give you one data point to be

1601
01:46:03,200 --> 01:46:06,920
you cannot line when you can fit in finance many lights writes about this you

1602
01:46:06,920 --> 01:46:11,980
know you're talking about so so when we talk about the small and heipi regime

1603
01:46:12,380 --> 01:46:13,880
this is what are talking about

1604
01:46:14,350 --> 01:46:16,090
and i can actually over the next five

1605
01:46:16,690 --> 01:46:17,650
maybe ten years

1606
01:46:18,180 --> 01:46:20,180
we will hear a lot about this this

1607
01:46:20,610 --> 01:46:21,620
class of problems

1608
01:46:22,030 --> 01:46:25,840
because in many domains this is showing up we are working on climate data forest

1609
01:46:25,840 --> 01:46:30,180
ecology and other places this is the scenario so many of the standard tools have

1610
01:46:30,180 --> 01:46:31,130
to be thrown out the window

1611
01:46:34,660 --> 01:46:35,650
there is some hope

1612
01:46:36,120 --> 01:46:40,020
and i'm going to sort of show you one class of methods and one specific problem

1613
01:46:40,550 --> 01:46:43,950
that's you know this is i'm using this as a case study but there's quite

1614
01:46:43,950 --> 01:46:46,280
a bit of very interesting what's going on on this

1615
01:46:46,740 --> 01:46:50,920
and i think on friday a couple of thoughts which sort of using this flavor

1616
01:46:51,490 --> 01:46:56,950
aaron angrily text analysis one is network construction and so on using using sparsity

1617
01:46:59,670 --> 01:47:00,710
what we're trying to do

1618
01:47:01,670 --> 01:47:05,170
and in this is an illustrative example to make the point

1619
01:47:05,570 --> 01:47:10,170
is that we are trying to study the land variable interaction based on climate data

1620
01:47:10,600 --> 01:47:12,880
maybe in separate analysis it or something like that

1621
01:47:14,510 --> 01:47:18,250
we are using the monthly means and this is available for download anybody can go

1622
01:47:18,250 --> 01:47:21,490
on download it you know if you have really seven or eight

1623
01:47:23,360 --> 01:47:27,930
and we are looking at responses temperature and precipitation and i locations

1624
01:47:28,820 --> 01:47:30,670
i'm gonna show you the pictures so you know

1625
01:47:31,340 --> 01:47:33,710
so i'm going to look at implicit in brazil

1626
01:47:34,640 --> 01:47:35,270
monthly means

1627
01:47:35,960 --> 01:47:37,230
over the last sixty years

1628
01:47:38,090 --> 01:47:41,040
and i'm going to try to fix that's why this form that's my wife

1629
01:47:41,850 --> 01:47:44,020
and everything everywhere in the ocean

1630
01:47:44,720 --> 01:47:45,140
is x

1631
01:47:46,280 --> 01:47:46,650
right so

1632
01:47:47,090 --> 01:47:47,740
so so

1633
01:47:48,260 --> 01:47:52,090
the number of features i'm going to uses going to run into tens of thousands

1634
01:47:53,520 --> 01:47:57,680
the number of data points i have samples i have sixty because i'm just looking

1635
01:47:57,680 --> 01:48:00,120
at sort of past sixty years of data

1636
01:48:00,810 --> 01:48:03,480
and for each such location i'm going to do

1637
01:48:04,230 --> 01:48:08,500
i'm going to focus on temperature and i'm going to focus on precipitation precipitation is

1638
01:48:08,500 --> 01:48:11,410
always the more difficult ones far on learning in these days

1639
01:48:11,840 --> 01:48:12,980
temperature is much more stable

1640
01:48:13,710 --> 01:48:16,060
um but can also be case study

1641
01:48:17,030 --> 01:48:18,030
and as i said about

1642
01:48:18,630 --> 01:48:20,760
this high dimensional regression with a twist

1643
01:48:21,300 --> 01:48:23,860
that's we have locations on the oceans

1644
01:48:24,630 --> 01:48:29,740
at each location i have hidden variables these variables the sea level pressure precipitation sea

1645
01:48:29,740 --> 01:48:32,840
surface temperature relative humidity on the ocean right so

1646
01:48:33,240 --> 01:48:37,630
so not i have locations and a bunch of variables at each location

1647
01:48:38,340 --> 01:48:43,650
but the total number of sort of dimensionality is is a way higher with higher

1648
01:48:44,090 --> 01:48:45,900
then the number of samples i have

1649
01:48:51,490 --> 01:48:54,050
i'm not a climate scientist i'm not going to claim anything

1650
01:48:54,550 --> 01:48:59,590
about bad but there are considerations that we were asked to sort of put into our model is

1651
01:49:00,280 --> 01:49:04,760
not all locations are going to be relevant greenland probably doesn't affect brazil directly

1652
01:49:06,630 --> 01:49:08,260
and even if a location is relevant

1653
01:49:08,780 --> 01:49:12,720
not all variables in that location is going to be relevant so so maybe

1654
01:49:13,280 --> 01:49:16,530
but cambridgeshire land britain maybe sea surface temperature

1655
01:49:17,110 --> 01:49:22,840
and maybe you know some wind speeds are important but not pressure maybe something else

1656
01:49:23,010 --> 01:49:26,150
right so we don't know but we don't expect all of them to be relevant

1657
01:49:26,150 --> 01:49:26,900
at all locations

1658
01:49:27,690 --> 01:49:28,570
so what we did

1659
01:49:29,690 --> 01:49:31,440
is that we look at ordinary least squares

1660
01:49:32,590 --> 01:49:35,470
and we look at that you realize it's not going to work

1661
01:49:36,690 --> 01:49:41,090
so this is the location one i mean variables location whatever it was this location

1662
01:49:41,210 --> 01:49:44,590
airline babies are ocean locations this is my responsibility

1663
01:49:45,380 --> 01:49:49,650
now i i in principle know how to do this optimisation if i actually did

1664
01:49:49,760 --> 01:49:52,050
do it in no matter are what they are

1665
01:49:52,530 --> 01:49:55,050
are they in principle should complain

1666
01:49:55,820 --> 01:50:00,130
these guys are very smart so they'll give you the pseudoinverse and everything will look nice and rosy

1667
01:50:00,720 --> 01:50:01,820
i don't believe that answer

1668
01:50:02,110 --> 01:50:07,760
right so because in principle the series of audience squares entirely breakdown in this scenario

1669
01:50:09,130 --> 01:50:10,170
so what we did we

1670
01:50:10,880 --> 01:50:11,760
we basically

1671
01:50:12,440 --> 01:50:15,530
border cousin of last so this this past thing

1672
01:50:16,590 --> 01:50:22,510
where we want to keep the ordinary least square loss function but we had some regularizers

1673
01:50:23,030 --> 01:50:25,110
and there are types some regularizers we added

1674
01:50:25,780 --> 01:50:26,420
one his

1675
01:50:26,880 --> 01:50:28,030
the group sparsity

1676
01:50:28,820 --> 01:50:30,650
what that simply means is back

1677
01:50:31,110 --> 01:50:34,190
we know that there is a natural grouping among the variables everything

1678
01:50:34,920 --> 01:50:37,470
because latitude longitude i have eight variables

1679
01:50:37,900 --> 01:50:41,550
and that's a natural group right this is the sea surface temperature here's he's similar

1680
01:50:41,550 --> 01:50:45,710
pressure and so on and so forth so location automatically groups those variables

1681
01:50:46,360 --> 01:50:47,720
and this is simply saying

1682
01:50:48,260 --> 01:50:51,150
that's why some of those locations are important

1683
01:50:51,990 --> 01:50:55,780
this is how you can put in your domain knowledge to the regularizer

1684
01:50:56,400 --> 01:51:00,740
so this is what they wanted to regularize are and what they're doing it will make

1685
01:51:02,130 --> 01:51:03,880
groups of variables zeal

1686
01:51:06,240 --> 01:51:09,820
and the second one is that it usually lasts only the arise are and what

1687
01:51:09,820 --> 01:51:13,010
it does is to make some of the features zeal

1688
01:51:13,760 --> 01:51:17,670
so together they are this is going to select some locations

1689
01:51:18,170 --> 01:51:21,550
and then this is going to select some variables from those locations

1690
01:51:22,670 --> 01:51:23,590
and in principle

1691
01:51:24,090 --> 01:51:25,400
the schematic if you

1692
01:51:25,800 --> 01:51:30,240
one that gets selected audience where will put a non-zero weight it doesn't do feature

1693
01:51:30,240 --> 01:51:31,510
those counts

1694
01:51:31,650 --> 01:51:36,450
and for this little example that match would have actually given us three point five

1695
01:51:36,500 --> 01:51:40,190
so we in general in practice what we find empirically is that these guys will

1696
01:51:40,190 --> 01:51:41,910
be quite close

1697
01:51:41,990 --> 01:51:46,480
to this we've done a series of experiments where we take these sets of varying

1698
01:51:46,480 --> 01:51:51,770
sizes and compute the optimal partial matching between them and are matching score

1699
01:51:51,810 --> 01:51:55,610
so you're looking at for each of these points corresponds to

1700
01:51:55,620 --> 01:51:57,610
the output of matching like this

1701
01:51:58,380 --> 01:52:03,200
the point set sizes varying here from five to one hundred points and these events

1702
01:52:03,200 --> 01:52:06,280
sort each of these are sorted according to the optimal measure which is shown in

1703
01:52:06,280 --> 01:52:09,400
red and blue is the pyramid match

1704
01:52:09,410 --> 01:52:13,310
this is for dimensions ranging from ten to ninety that the dimension of the feature

1705
01:52:15,560 --> 01:52:18,220
so it's nicely approximating this measure

1706
01:52:19,660 --> 01:52:24,850
we may use of this kernel and a few different scenarios

1707
01:52:24,860 --> 01:52:30,450
the pyramid match doesn't actually farmers kernel and we used it with support vector classification

1708
01:52:30,450 --> 01:52:36,030
and regression for recognition tasks and pose estimation which i'm going to talk about today

1709
01:52:36,070 --> 01:52:39,840
and more recently we've been looking at using this to do some clustering using a

1710
01:52:39,840 --> 01:52:44,470
partial match over features to the unsupervised learning of categories from images

1711
01:52:44,490 --> 01:52:46,440
but first i would like to this

1712
01:52:46,460 --> 01:52:49,830
cover these results today

1713
01:52:49,850 --> 01:52:54,290
so for the object recognition tasks we take images and represent them by a set

1714
01:52:54,290 --> 01:53:01,060
of features formed using a particular detector that find salient points basically based on corner

1715
01:53:01,060 --> 01:53:03,550
texture features within an image

1716
01:53:03,560 --> 01:53:06,680
and then we use the descriptor based on

1717
01:53:06,960 --> 01:53:12,390
and image gradients and this gives invariance to rotation scale translation

1718
01:53:12,410 --> 01:53:17,050
and so the ETH eighty database which contains a different classes

1719
01:53:17,060 --> 01:53:19,330
we find that the match will get

1720
01:53:19,340 --> 01:53:22,480
competitive performance eighty four percent two

1721
01:53:22,600 --> 01:53:31,300
these two existing state-of-the-art set kernels but at a much lower computational cost

1722
01:53:31,380 --> 01:53:37,710
we also considered somewhat more challenging database that contains one hundred one categories

1723
01:53:37,720 --> 01:53:43,740
and there's a great deal of intra class appearance variation here to deal with

1724
01:53:43,750 --> 01:53:47,210
and also the large number of categories is challenging

1725
01:53:47,220 --> 01:53:51,460
and you're using a couple of different types of features we get forty three and

1726
01:53:51,460 --> 01:53:55,740
fifty two percent performance average over all the test images in this dataset contains about

1727
01:53:55,740 --> 01:54:01,220
nine thousand total images we change with about thirty thirty examples per class

1728
01:54:01,240 --> 01:54:04,950
so the numbers if you're not familiar with the database might seem

1729
01:54:04,970 --> 01:54:08,820
well there are low because of the charge and it in fact are in line

1730
01:54:08,820 --> 01:54:11,450
with the state of the art performance here

1731
01:54:11,460 --> 01:54:13,560
in fact the measure based on

1732
01:54:13,580 --> 01:54:19,260
the exact partial matching using the earthmover distance has reported performance about fifty three percent

1733
01:54:19,260 --> 01:54:23,760
so when i with this but much much more efficient takes about twelve seconds to

1734
01:54:23,760 --> 01:54:27,630
do a single matching here on average and this is where the feature set sizes

1735
01:54:27,630 --> 01:54:33,030
they have about six hundred features and can be as large as two thousand features

1736
01:54:33,040 --> 01:54:37,280
and that seconds actually twenty times faster than just the quadratic

1737
01:54:37,780 --> 01:54:42,820
suboptimal matching measure

1738
01:54:42,830 --> 01:54:49,270
and considering regression an important problem in vision is pose estimation rate estimate articulated pose

1739
01:54:49,270 --> 01:54:51,830
of the human and so

1740
01:54:51,840 --> 01:54:55,540
we would like to directly when the relationship between the shape of the person in

1741
01:54:55,540 --> 01:55:00,330
the image and the underlying posts so here we are taking so let's

1742
01:55:00,380 --> 01:55:04,160
and we want to estimate directly through the joint positions

1743
01:55:04,180 --> 01:55:06,820
to do this we use support vector regression

1744
01:55:06,840 --> 01:55:13,210
and we trained with synthetic dataset used we generated this with a computer graphics package

1745
01:55:13,490 --> 01:55:18,220
so we get using now you lots of data quite easily and we can get

1746
01:55:18,220 --> 01:55:23,530
precise always the person and very precise right positions as well

1747
01:55:23,540 --> 01:55:29,530
and you're constructing this two walking poses only so only walking motion

1748
01:55:29,540 --> 01:55:34,690
and the representational use is the shape context histograms

1749
01:55:34,700 --> 01:55:38,600
which i mentioned earlier and so this is the descriptor that locally describing the shape

1750
01:55:38,860 --> 01:55:43,480
and each of these points

1751
01:55:43,490 --> 01:55:49,060
and to test this what we've learned we will use a synthetic tests generated again

1752
01:55:49,060 --> 01:55:51,260
with that computer graphics package

1753
01:55:51,280 --> 01:55:56,990
to add some difficulty we can insert quarter or simulate clearly these blogs in the

1754
01:55:56,990 --> 01:56:01,560
segmentation mask that are going to be very common in practice as you can see

1755
01:56:01,560 --> 01:56:04,700
in these real images they get caused by

1756
01:56:05,240 --> 01:56:10,460
difficulties in doing this for an extraction and changes in the background and things like

1757
01:56:11,930 --> 01:56:17,390
also valid with some real images here we can only eyeball the results into qualitative

1758
01:56:19,810 --> 01:56:23,900
so with a separate test set of about three hundred images

1759
01:56:23,910 --> 01:56:27,930
here we were training with three thousand of these synthetic examples so now with the

1760
01:56:27,960 --> 01:56:35,470
synthetic tests with clean so it's meaning perfect segmentations we look at the performance comparing

1761
01:56:35,470 --> 01:56:41,050
our method the pyramid match with a method based on this representation that's been converted

1762
01:56:42,170 --> 01:56:43,590
vector by a

1763
01:56:43,610 --> 01:56:48,010
counting the number of occurrences of a set of prototype features so if had a

1764
01:56:48,010 --> 01:56:52,130
tiny vocabulary of features that are going to occur and then you map your input

1765
01:56:52,130 --> 01:56:56,200
features to the prototypes you can count them in the end up with the vector

1766
01:56:57,950 --> 01:57:02,450
so here we combine that with gaston RBF kernel

1767
01:57:02,460 --> 01:57:05,380
and so basically says that there is

1768
01:57:05,380 --> 01:57:12,030
some increase in performance with the pyramid match and was statistically significant using a paired

1769
01:57:12,030 --> 01:57:13,880
difference details but

1770
01:57:14,060 --> 01:57:17,750
what's really interesting is that when we add some clarity these images like this on

1771
01:57:17,750 --> 01:57:22,810
the previous findings these bodies problems in the segmentation pyramid match is better able to

1772
01:57:22,810 --> 01:57:26,090
handle that because we are explicitly account for the fact that we

1773
01:57:26,110 --> 01:57:35,440
don't need every feature to match some of the feature in the second image

1774
01:57:35,460 --> 01:57:41,770
and then we have also had enough application using documents with define task of taking

1775
01:57:41,770 --> 01:57:45,490
a research paper in turn ask me what year it was published

1776
01:57:45,600 --> 01:57:49,760
so here we are using a bag of words representation

1777
01:57:49,780 --> 01:57:54,460
and we're trying to do this with thirteen volumes of newspapers

1778
01:57:55,420 --> 01:57:56,860
the word from features

1779
01:57:56,870 --> 01:57:59,670
when using latent semantic analysis two

1780
01:57:59,680 --> 01:58:03,860
extract semantic space over the words were

1781
01:58:03,880 --> 01:58:08,510
with the idea that similar words should have similar vector representations which you find out

1782
01:58:08,510 --> 01:58:12,780
by looking at the co occurrence statistics over some corpus of data about which words

1783
01:58:12,780 --> 01:58:14,440
occur together

1784
01:58:14,450 --> 01:58:16,180
in different documents

1785
01:58:16,200 --> 01:58:18,890
so we call is the word meaning features

1786
01:58:18,890 --> 01:58:22,030
so now we have a set of these word meaning vectors

1787
01:58:23,770 --> 01:58:25,930
we did regression here we're using

1788
01:58:25,930 --> 01:58:30,300
testing with about four hundred images in training with about

