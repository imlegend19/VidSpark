1
00:00:00,000 --> 00:00:03,810
the following content is provided under creative commons license

2
00:00:03,830 --> 00:00:10,520
your support will help MIT opencourseware continue to offer high quality educational resources for free

3
00:00:10,530 --> 00:00:15,240
to make a donation or view additional materials from hundreds of MIT courses

4
00:00:15,270 --> 00:00:17,160
this MIT opencourseware

5
00:00:17,200 --> 00:00:21,320
OCW MIT that you

6
00:00:21,320 --> 00:00:25,790
so our first question here about limiting reactant to learn something new

7
00:00:25,800 --> 00:00:29,960
i will encounter in your review reading for the sections that kind of review what

8
00:00:29,960 --> 00:00:33,650
we hope you have picked up from high school will pick up quickly by doing

9
00:00:33,650 --> 00:00:34,700
some review

10
00:00:34,760 --> 00:00:38,940
so how about we have everyone he got ten more seconds on the clicker question

11
00:00:38,940 --> 00:00:59,090
get your final answer in here

12
00:00:59,150 --> 00:01:03,300
right so let's see what we have

13
00:01:03,350 --> 00:01:08,390
right now it looks like are showing percentages here by the looks like hopefully most

14
00:01:08,390 --> 00:01:13,160
of you are able to get the correct answer each two being the limiting reactant

15
00:01:13,390 --> 00:01:17,530
looks like we're still figuring out this room was just motivated we're still working out

16
00:01:17,530 --> 00:01:22,090
exactly how the electronics work so normally wealthier percentage of how many of you got

17
00:01:22,090 --> 00:01:25,580
it but i'm going to say was probably about ninety five percent confidence to write

18
00:01:25,710 --> 00:01:29,860
so good job there if you didn't get the answer right will send these questions

19
00:01:29,860 --> 00:01:34,030
UTA so any time you get the question wrong and you refused to bring it

20
00:01:34,030 --> 00:01:38,220
up in the next recitation section will be able to discuss it there so starting

21
00:01:38,220 --> 00:01:39,940
in we can switch over to the

22
00:01:39,940 --> 00:01:42,480
so there no now

23
00:01:42,500 --> 00:01:44,700
when we left on wednesday

24
00:01:44,720 --> 00:01:48,200
what we have really been doing is trying to give you an overview of all

25
00:01:48,220 --> 00:01:52,670
the different topics that we're going to be going over this semester and also

26
00:01:53,080 --> 00:01:55,440
to make a couple of those connections

27
00:01:55,440 --> 00:02:00,190
between the principles were learning and some of the exciting research that's going on at

28
00:02:00,190 --> 00:02:04,020
MIT in the chemistry department and also to give you the idea

29
00:02:04,090 --> 00:02:08,310
we are going to be trying to make these connections between the chemistry and things

30
00:02:08,310 --> 00:02:10,320
like human health or medicine

31
00:02:11,880 --> 00:02:16,910
now we get to actually take a step back and started the beginning because before

32
00:02:16,910 --> 00:02:23,060
we can talk about some of the more complex issues which involve interactions between molecules

33
00:02:23,380 --> 00:02:29,750
reacting or even when we're talking about individual molecules the bonds form between individual atoms

34
00:02:29,780 --> 00:02:34,090
before any of that we actually need to establish a way that we're going to

35
00:02:34,090 --> 00:02:37,310
describe and think about how

36
00:02:37,440 --> 00:02:42,340
an individual atom behaves and the way that will do this is starting with talking

37
00:02:42,340 --> 00:02:45,070
about the discovery

38
00:02:45,090 --> 00:02:47,780
of the electron and the nucleus here

39
00:02:47,780 --> 00:02:54,160
once we go through that we will be able to talk about describing an atom

40
00:02:54,160 --> 00:02:59,120
using classical physics so once we have an atom in the nucleus what will try

41
00:02:59,120 --> 00:03:00,000
to do is apply

42
00:03:01,210 --> 00:03:05,210
classical mechanics to explain how that behavior

43
00:03:05,320 --> 00:03:09,870
what will find that this fails and once this fails we're going to need another

44
00:03:10,660 --> 00:03:14,840
luckily for us we have quantum mechanics which will be talking about for the next

45
00:03:14,840 --> 00:03:19,640
few lectures and will dive into that we might get a chance introduced stable certainly

46
00:03:19,640 --> 00:03:22,970
in not class will be introducing this new kind of mechanics that's going to allow

47
00:03:22,970 --> 00:03:25,320
us to describe the behavior at all

48
00:03:25,340 --> 00:03:28,750
so i want to point out that it it makes a lot of sense for

49
00:03:28,750 --> 00:03:33,070
us to start with the discovery of the electron and the nucleus because it really

50
00:03:33,070 --> 00:03:37,900
highlights one of the big issues that comes up all chemistry research that you do

51
00:03:37,990 --> 00:03:41,340
and that is how do we actually study

52
00:03:41,350 --> 00:03:42,630
or in this case

53
00:03:42,740 --> 00:03:44,280
how do we discover

54
00:03:44,290 --> 00:03:48,810
adams or some particles that we actually can see it all

55
00:03:48,820 --> 00:03:53,120
and there are lots of solutions that cannot come up with there's always new techniques

56
00:03:53,120 --> 00:03:57,060
that allow us to do this and these are just some of the first and

57
00:03:57,060 --> 00:04:00,590
will go through them in a little bit of detail here

58
00:04:00,660 --> 00:04:04,760
so this all started in terms of putting it in its historical context at the

59
00:04:04,760 --> 00:04:08,540
turn of the century we that we start writing in the twentieth century and were

60
00:04:08,540 --> 00:04:12,280
chemistry was and where we were at the start of the twentieth century in the

61
00:04:12,280 --> 00:04:16,600
late eighty ninety is that we were at the place where there was great confidence

62
00:04:16,850 --> 00:04:22,560
in our understanding of the universe and understanding of how all matter worked so people

63
00:04:22,560 --> 00:04:27,060
in the chemistry community and in the physics community had this general feeling that the

64
00:04:27,060 --> 00:04:32,750
theoretical structure of the entire universe was pretty well understood and they had this feeling

65
00:04:32,750 --> 00:04:34,690
because they had just found this

66
00:04:34,710 --> 00:04:41,940
huge boom of discovery of scientific advances that included newtonian mechanics and included daltons atomic

67
00:04:41,940 --> 00:04:46,620
theory of matter also thermodynamics in classical electromagnetism

68
00:04:46,620 --> 00:04:50,380
so you can understand i really felt quite confident at the time that we can

69
00:04:50,380 --> 00:04:55,800
explain everything that was going on in fact really telling quote from the time

70
00:04:55,880 --> 00:05:00,890
with that by professor at the university of chicago and what he said is our

71
00:05:00,890 --> 00:05:06,210
future discoveries must be looked for in the sixth decimal place

72
00:05:07,250 --> 00:05:12,350
basically what he's saying here is that we pretty much understand what's going on there

73
00:05:12,350 --> 00:05:16,930
is nothing new to really discover all we need to do is measure things more

74
00:05:20,700 --> 00:05:26,260
that's not exactly the case and we're going to start in at the point where

75
00:05:26,260 --> 00:05:31,270
right around this time of great confidence of feeling all has been conquered there are

76
00:05:31,290 --> 00:05:37,070
some observations and discoveries that are made that completely break down these theories for example

77
00:05:37,070 --> 00:05:39,570
in terms of the atomic theory of matter

78
00:05:39,600 --> 00:05:41,770
at the time at the turn of the century

79
00:05:41,770 --> 00:05:47,730
you only allow binary relations and likewise probabilistic relational models are essentially markov logic we

80
00:05:47,730 --> 00:05:49,930
only allow binary relations

81
00:05:49,980 --> 00:05:53,800
and then we you know is in essence we do everything the same as bayes

82
00:05:53,800 --> 00:05:57,880
nets except that the difference is not the first arguement can vary

83
00:05:57,890 --> 00:05:59,790
right so should remember

84
00:05:59,800 --> 00:06:04,060
in bayes nets i always require the first arguement to be the same object

85
00:06:04,090 --> 00:06:06,460
the key thing in probabilistic relational models that

86
00:06:06,460 --> 00:06:09,500
this property of this object could depend on some of the property or some other

87
00:06:09,500 --> 00:06:13,760
object so like and have forms the talk about multiple objects at the same time

88
00:06:13,760 --> 00:06:17,020
and this is self-published question models generalize

89
00:06:17,040 --> 00:06:21,120
there's networks there more restrictive than than islands in the sense that the only allowed

90
00:06:21,120 --> 00:06:24,840
binary relations with markov logic allows any relations

91
00:06:24,890 --> 00:06:28,580
and then compare PRM would so yourself using KBMC

92
00:06:28,630 --> 00:06:31,180
we are somewhat orthogonal restrictions

93
00:06:31,300 --> 00:06:36,400
in terms of your binary relations in these other along articles

94
00:06:36,430 --> 00:06:39,310
OK so each one of them can do things that the other one can at

95
00:06:39,310 --> 00:06:43,790
some level but all of them can be done by markov logic

96
00:06:43,800 --> 00:06:46,810
so what about relational markov networks

97
00:06:46,830 --> 00:06:50,140
well if you think about relational markov networks

98
00:06:50,170 --> 00:06:53,000
somebody has a question

99
00:07:20,860 --> 00:07:24,640
what i mean by numbers in the parameters or the probabilities

100
00:07:49,600 --> 00:08:14,010
right if i understood the question correctly here's the answer

101
00:08:14,770 --> 00:08:16,310
first of all

102
00:08:16,330 --> 00:08:19,580
my answer is in the query space that i have set up

103
00:08:19,590 --> 00:08:21,390
but if i give the right

104
00:08:21,400 --> 00:08:23,680
truth assignments to those predicate signed

105
00:08:23,690 --> 00:08:27,000
now you're right that if i do this naively that's going to be a much

106
00:08:27,000 --> 00:08:30,890
more complex problem then seizing CKY

107
00:08:30,890 --> 00:08:34,430
because now i have to compute all these things that otherwise i wouldn't like to

108
00:08:34,430 --> 00:08:36,060
say to represent them

109
00:08:36,090 --> 00:08:39,510
what makes this efficient is lazy inference

110
00:08:39,520 --> 00:08:42,980
is that most of the places where the prose could be i never going to

111
00:08:42,980 --> 00:08:44,090
worry about

112
00:08:44,090 --> 00:08:46,420
i'm always been assumed that they're not there

113
00:08:46,420 --> 00:08:51,760
i only thought i only explicitly represent nounphrase from one to two

114
00:08:51,790 --> 00:08:55,510
if i have some evidence in the input string that there's a known first from

115
00:08:55,510 --> 00:08:56,470
one to two

116
00:08:56,550 --> 00:08:58,390
otherwise it never happens

117
00:08:58,400 --> 00:09:03,000
so the complexity is actually again ones are being quite nice

118
00:09:05,000 --> 00:09:06,830
we can talk about this

119
00:09:09,390 --> 00:09:11,890
so what about relational markov networks

120
00:09:13,230 --> 00:09:15,300
the query is that you have in mind

121
00:09:15,300 --> 00:09:18,770
in markov networks conjunctive SQL queries which

122
00:09:18,790 --> 00:09:22,480
you can equally well represented as datalog which of course is a special case of

123
00:09:22,480 --> 00:09:26,510
first order logic so it's clear how to do this we can write the queries

124
00:09:26,520 --> 00:09:29,080
that represent the cliques in

125
00:09:30,400 --> 00:09:36,190
first order logic and therefore with weights in markov logic now we need one clause

126
00:09:36,190 --> 00:09:37,960
for each state of a clique

127
00:09:37,970 --> 00:09:42,340
i remember what happens in our lenses that we're going to explicitly represent each state

128
00:09:42,340 --> 00:09:43,590
of the clique

129
00:09:43,640 --> 00:09:48,090
unless we have some additional language for representing the structure that clique which is of

130
00:09:48,090 --> 00:09:52,050
course with markov logic networks provide but if you don't have have such language then

131
00:09:52,050 --> 00:09:55,970
you need the clause for each state of a clique which could become cumbersome if

132
00:09:55,970 --> 00:09:59,840
you have a lot of variables in the clique we we have another nice piece

133
00:09:59,840 --> 00:10:00,380
of work

134
00:10:00,510 --> 00:10:05,860
syntactic sugar not be that facilitates which is you can request clauses with an asterisk

135
00:10:05,860 --> 00:10:07,400
instead of the negation

136
00:10:07,430 --> 00:10:12,300
but this means that you want to commit to create both versions of the class

137
00:10:12,300 --> 00:10:16,270
the version that was a little gain in the vision with a little not negated

138
00:10:16,330 --> 00:10:19,850
so if you have a cause three literals all of which have as risk then

139
00:10:19,850 --> 00:10:23,850
this automatically creates all the it causes k so this makes it very easy to

140
00:10:23,850 --> 00:10:28,440
implement something like like a relational markov network in markov logic

141
00:10:28,510 --> 00:10:33,330
again it's clear what the restriction is is the relational markov networks i'm not specifying

142
00:10:33,390 --> 00:10:36,180
what the structure of the features inside the quickest

143
00:10:36,500 --> 00:10:39,710
and as a result i pay pricing inefficiency

144
00:10:39,770 --> 00:10:42,090
what about bayesian logic

145
00:10:42,120 --> 00:10:46,970
well the logic is pretty language pretty complicated syntax which i will not going to

146
00:10:46,970 --> 00:10:51,390
hear the crucial thing about the nice thing about this logic is that it allows

147
00:10:51,390 --> 00:10:56,260
us to talk about domains well we don't know in advance what your edits

148
00:10:56,290 --> 00:10:59,590
but i see blips on the radar and there are some aircraft there but you

149
00:10:59,590 --> 00:11:03,310
know i don't know which aircraft which in some of the police might be nice

150
00:11:03,330 --> 00:11:07,180
so the question is how can we do something like that in markov logic actually

151
00:11:07,180 --> 00:11:09,010
more than one way but his wife

152
00:11:09,040 --> 00:11:12,020
an object is really a cluster

153
00:11:12,080 --> 00:11:14,890
of related or similar observations

154
00:11:14,930 --> 00:11:20,470
so for example in the resolution which selected canonical application for this an object is

155
00:11:20,470 --> 00:11:25,810
really a cluster of the various mentions that appeared the various corrupted strings and different

156
00:11:25,810 --> 00:11:28,020
ways of writing the referencing and what not

157
00:11:28,040 --> 00:11:32,840
and so what i need to have this in addition to observation constants i need

158
00:11:32,850 --> 00:11:35,540
to have object constants

159
00:11:35,560 --> 00:11:39,940
and once you have added constants this gives me the nice property that blog also

160
00:11:39,940 --> 00:11:44,250
has that more naive approaches like conditional random fields don't which is that i can

161
00:11:44,250 --> 00:11:47,060
talk about properties of the object

162
00:11:47,080 --> 00:11:49,810
in addition to properties of the strings

163
00:11:50,650 --> 00:11:53,840
being three letters long is the property of the string bob

164
00:11:53,850 --> 00:11:57,050
being male is the property of the object body

165
00:11:57,050 --> 00:12:00,470
and we don't reproduce the properties of the object body as many times as the

166
00:12:00,470 --> 00:12:04,340
citation strings about that appear right that would be a better model

167
00:12:04,430 --> 00:12:09,300
but if i have up to constants i can talk about objects in this we

168
00:12:09,310 --> 00:12:12,890
only get to produce as many times as the actual edits that exist

169
00:12:12,980 --> 00:12:16,090
no of course the next thing that we need to do is i need to

170
00:12:16,090 --> 00:12:19,710
have a predicate formulas that relate the two

171
00:12:19,720 --> 00:12:23,390
because this is what i see that this is my evidence this is what i'm

172
00:12:23,390 --> 00:12:25,050
going to query about

173
00:12:25,060 --> 00:12:27,330
i'm gonna cui what objects are there

174
00:12:27,340 --> 00:12:28,510
and one would have know what

175
00:12:28,540 --> 00:12:30,080
relations exist between them

176
00:12:30,150 --> 00:12:33,500
so in order to do that we can just use the no the predicate instance

177
00:12:33,500 --> 00:12:38,010
of which that this observation is an instance of this

178
00:12:38,010 --> 00:12:42,020
and now i can write clauses using this predic

179
00:12:42,050 --> 00:12:44,080
this clause also things like

180
00:12:44,090 --> 00:12:45,050
if this

181
00:12:45,060 --> 00:12:48,960
it happens then this string is another version of this object

182
00:12:48,980 --> 00:12:51,290
like for example if the string is j smith

183
00:12:51,360 --> 00:12:55,470
then this is the instance of the object you know john smith one

184
00:12:55,520 --> 00:12:59,720
and with that know what i do inference with

185
00:13:01,060 --> 00:13:03,140
in the evidence this in the query

186
00:13:03,150 --> 00:13:05,010
if instance of a

187
00:13:05,040 --> 00:13:07,000
is in the query this will

188
00:13:07,010 --> 00:13:12,380
tell me what are the observations are instances of objects if also happen to know

189
00:13:12,380 --> 00:13:15,930
this for some objects which again to block you can actually say for some of

190
00:13:15,930 --> 00:13:19,640
the time that these exist then those grounds of this project will be in the

191
00:13:19,640 --> 00:13:21,270
evidence either way

192
00:13:21,330 --> 00:13:25,710
the inference in in alchemy will tell you how to do it

193
00:13:25,840 --> 00:13:27,050
not only that

194
00:13:28,090 --> 00:13:30,430
in in in in markov logic

195
00:13:30,430 --> 00:13:33,790
small number of variables to particular

196
00:13:33,870 --> 00:13:38,430
looking at the marginal for w we can notice that

197
00:13:38,720 --> 00:13:42,350
the variable v appears only in this factor here so we can pull that out

198
00:13:42,350 --> 00:13:46,990
is the common factor and the summation of these separately from the remaining

199
00:13:47,140 --> 00:13:48,850
some nations

200
00:13:48,870 --> 00:13:55,370
and so this is a numerical algebraically equivalent but numerically but numerically computationally is more

201
00:13:56,930 --> 00:14:00,430
and the elegant thing we can do here is to express this in terms of

202
00:14:00,430 --> 00:14:02,790
local messages on the graph

203
00:14:02,810 --> 00:14:07,890
so this quantity is some function of w will think of this is the message

204
00:14:07,890 --> 00:14:13,060
that is sent from this fact to note to this variable node called f one

205
00:14:13,060 --> 00:14:15,290
to the user function w

206
00:14:15,310 --> 00:14:19,470
and similarly this remaining object will think of this is the message sent from here

207
00:14:19,470 --> 00:14:23,410
to here so each of these are sort of k dimensional vector

208
00:14:23,430 --> 00:14:25,720
and compute the marginal we simply have to

209
00:14:25,750 --> 00:14:27,140
multiply them

210
00:14:27,140 --> 00:14:30,080
point wise together so that's the order k

211
00:14:33,040 --> 00:14:35,790
computing this involves a little k by k

212
00:14:35,810 --> 00:14:38,100
matrix that sort of quadratic in k

213
00:14:38,120 --> 00:14:39,700
and we have to do k

214
00:14:39,730 --> 00:14:44,410
some feature the k values WC there's a little order k squared calculation to compute

215
00:14:44,410 --> 00:14:46,890
that message to can do that very efficiently

216
00:14:46,910 --> 00:14:52,470
and then the multiplication against the k dimensional multiplications again that's also

217
00:14:52,490 --> 00:14:57,200
that's also very efficient to compute this message efficiently we could do the whole thing

218
00:14:57,200 --> 00:15:00,120
in quadratic time sort of exponential time

219
00:15:23,600 --> 00:15:28,200
i i start by talking about directed graphs and maps onto factor graphs in which

220
00:15:28,200 --> 00:15:32,020
each factors of those conditional distributions in the sense that the factor graphs more generally

221
00:15:32,020 --> 00:15:36,060
don't have to be conditional distributions in the from the directed graph they could be

222
00:15:36,060 --> 00:15:40,870
derived from some undirected graph or or or something it's any factors asian any any

223
00:15:41,290 --> 00:15:42,720
situation in which the

224
00:15:42,750 --> 00:15:47,180
joint probability distribution of all the variables factorizes that's all the factor graph is is

225
00:15:47,180 --> 00:15:52,350
expressing so those factors may have interpretations conditional distributions but they may be something else

226
00:15:52,350 --> 00:15:53,410
as well

227
00:15:54,560 --> 00:15:58,310
now the questions

228
00:15:58,330 --> 00:16:00,500
OK so

229
00:16:00,520 --> 00:16:03,720
the remains then the issue to computing this

230
00:16:03,720 --> 00:16:07,000
message efficiently let's look at that message again we can do the same thing we

231
00:16:07,000 --> 00:16:09,520
can reorder the sums and products

232
00:16:11,000 --> 00:16:13,270
again we can interpret

233
00:16:13,560 --> 00:16:19,390
this quantity now see is a message from a variable nodes to factor nodes so

234
00:16:19,560 --> 00:16:23,640
go back a second this is to compute the marginal at this node take the

235
00:16:23,640 --> 00:16:28,830
product of the two incoming messages that about variable node

236
00:16:29,870 --> 00:16:33,180
this is telling us how to compute the

237
00:16:33,220 --> 00:16:38,350
the message from factor to variable node it says take the incoming message from the

238
00:16:38,450 --> 00:16:42,600
but all the other variable nodes into that factor nodes multiplied together anyone here to

239
00:16:42,600 --> 00:16:46,500
take that incoming message multiplied by the local factors and estimation

240
00:16:46,500 --> 00:16:50,040
again this is a k dimensional vector this little k by k matrix and there

241
00:16:50,040 --> 00:16:54,790
are k terms in that some social order k squared again so we have the

242
00:16:54,790 --> 00:16:58,810
order case that operation fauna got another or the case where the operation to do

243
00:16:58,830 --> 00:17:00,580
again we can take this

244
00:17:01,850 --> 00:17:03,410
and we can reorder it

245
00:17:03,430 --> 00:17:05,330
and interpret that as

246
00:17:05,370 --> 00:17:06,720
other messages

247
00:17:06,730 --> 00:17:10,750
so this is the message that i'm going from this variable nodes this fact and

248
00:17:10,750 --> 00:17:14,640
it just the product of the incoming messages on the other

249
00:17:14,680 --> 00:17:17,600
from the factor nodes and the

250
00:17:17,660 --> 00:17:19,330
this a lot more of this

251
00:17:19,350 --> 00:17:24,470
i think in in tom minka store where he talks about various methods for doing

252
00:17:24,470 --> 00:17:28,350
inference on graphs using local message passing

253
00:17:29,950 --> 00:17:34,410
each of these is a little quadratic operation and the number of them is just

254
00:17:34,410 --> 00:17:35,180
going to grow

255
00:17:35,200 --> 00:17:37,250
linearly with the size of the graph

256
00:17:37,290 --> 00:17:38,140
so now

257
00:17:38,160 --> 00:17:39,950
if i make the graph bigger

258
00:17:39,970 --> 00:17:45,870
the computational cost now grows linearly instead of exponentially to big computational savings

259
00:17:45,930 --> 00:17:52,470
if you use something other than that you would

260
00:17:52,470 --> 00:17:55,850
the the

261
00:17:55,870 --> 00:18:00,250
so that's it

262
00:18:00,250 --> 00:18:03,620
they have like

263
00:18:04,810 --> 00:18:08,100
yes i mean applies to all sorts give you insight into the fast fourier transform

264
00:18:08,100 --> 00:18:09,430
all sorts of other

265
00:18:09,890 --> 00:18:12,120
yes this is very very general

266
00:18:12,140 --> 00:18:17,720
exploitation factors asian yes is more general than just machine learning show

267
00:18:18,830 --> 00:18:20,270
the other

268
00:18:20,270 --> 00:18:24,530
so now we are very pleased to welcome

269
00:18:24,590 --> 00:18:29,200
professor anthony from the news to washington who is the founder and director of

270
00:18:29,260 --> 00:18:31,160
the newspapers during center

271
00:18:31,170 --> 00:18:37,950
his triple-a fellow and the founder of several successful start-ups including how much to buy

272
00:18:37,950 --> 00:18:44,080
or not to buy cigarettes and his interest in solving fundamental problems in the study

273
00:18:44,080 --> 00:18:47,180
of intelligence web search mission reading

274
00:18:47,240 --> 00:18:51,440
so thanks are referred to as

275
00:18:54,760 --> 00:18:58,550
thank you it's a pleasure to be here

276
00:18:58,580 --> 00:19:00,610
hamlet is not called for cast

277
00:19:00,610 --> 00:19:06,360
and if that's an improvement but i won't talk about that let me start here

278
00:19:06,370 --> 00:19:11,440
can anybody tell me what this is

279
00:19:11,490 --> 00:19:13,750
thank you to reject test

280
00:19:13,960 --> 00:19:19,360
what i've done is i've devised a rorschach test for computer scientists so if you

281
00:19:19,360 --> 00:19:20,220
look at this

282
00:19:20,230 --> 00:19:25,840
picture you have to ask yourself what's that is that a depiction of moore's law

283
00:19:25,870 --> 00:19:29,150
storage capacity number of web pages

284
00:19:29,420 --> 00:19:31,540
facebook users

285
00:19:31,560 --> 00:19:37,590
it's we live in in an exponential world and the training center

286
00:19:37,610 --> 00:19:42,510
it is meant to investigate a set of questions around search natural language in that

287
00:19:42,530 --> 00:19:46,200
context of just a couple words about the training center and then i get to

288
00:19:46,200 --> 00:19:48,420
the topic of my talk

289
00:19:48,450 --> 00:19:51,000
so we have three research foci

290
00:19:51,010 --> 00:19:55,920
the first one is to scale machine translation to the large number of languages on

291
00:19:55,920 --> 00:20:00,590
the order of seven thousand languages spoken on earth that means forty nine million language

292
00:20:01,420 --> 00:20:06,310
standards statistical machine translation technology doesn't scale

293
00:20:06,360 --> 00:20:09,620
so this number of language pairs the reason is that

294
00:20:09,620 --> 00:20:14,340
they rely on aligned corpora which are manually generated and you can't have

295
00:20:14,390 --> 00:20:16,500
this manual on corporate scale

296
00:20:16,510 --> 00:20:20,970
what we have done is looked at an easier problem lexical translation

297
00:20:20,980 --> 00:20:24,590
mapping words from one language to another with the idea that we could do that

298
00:20:24,590 --> 00:20:27,060
with for a much larger set of languages

299
00:20:27,100 --> 00:20:31,090
and so we build something we call the translation graph right now it has on

300
00:20:31,090 --> 00:20:31,990
the order of

301
00:20:32,030 --> 00:20:38,720
two point five million words nodes and edges indicate translation between one word to another

302
00:20:38,750 --> 00:20:43,850
in particular sense and the question that we ask on the project is the inference

303
00:20:43,850 --> 00:20:48,000
question is if you start in words in vietnamese and you can translate the french

304
00:20:48,320 --> 00:20:52,410
and from french let's say you can go to catalina well

305
00:20:52,430 --> 00:20:58,530
is that true translation path is that truth preserving in other words what's the probability

306
00:20:58,530 --> 00:21:02,340
that if you go from vietnamese all the way to catalan you end up with

307
00:21:02,340 --> 00:21:04,490
the same word that you started with

308
00:21:04,500 --> 00:21:09,590
so interesting problem let me just show you a quick search application built on top

309
00:21:09,590 --> 00:21:14,970
of this lexical translation one of the things we realised is that image search experience

310
00:21:14,970 --> 00:21:20,660
is not nearly as good if you speak a relatively small languages like turkish and

311
00:21:20,660 --> 00:21:25,000
hebrew or vietnamese compared to our experience in english

312
00:21:25,000 --> 00:21:29,440
simply the size of the web's there are smaller so actually makes sense to take

313
00:21:29,440 --> 00:21:34,200
a query let's say in turkish and translated to different language for free during the

314
00:21:34,200 --> 00:21:40,930
search so we built this engine called pan images that use this translation technology

315
00:21:40,940 --> 00:21:44,840
to do exactly that let me just give you a sense of how it works

316
00:21:44,840 --> 00:21:46,560
so do query

317
00:21:46,570 --> 00:21:48,690
in english

318
00:21:48,720 --> 00:21:54,900
is it so you can click translate and what it does is it uses this

319
00:21:54,900 --> 00:21:57,280
graph an inference over the graph

320
00:21:57,300 --> 00:22:02,430
in computer set of translations to different languages and you can see that the notion

321
00:22:02,430 --> 00:22:04,560
of breakfast across different cultures so

322
00:22:04,620 --> 00:22:08,280
japanese breakfast looks quite different

323
00:22:08,350 --> 00:22:13,790
then a typical american one so this is a course parallel to flickr and google

324
00:22:13,790 --> 00:22:18,340
or trying to be fair and you can see the images there so that's the

325
00:22:19,640 --> 00:22:24,220
for project at the training center and you can play with this offline it's at

326
00:22:24,320 --> 00:22:26,030
penn images that or

327
00:22:26,090 --> 00:22:30,690
a second project that has more than a i flavor is we're asking how do

328
00:22:30,690 --> 00:22:34,430
you accumulate massive amounts of knowledge from from the web

329
00:22:34,440 --> 00:22:37,700
o age-old AI problem how do you build

330
00:22:37,710 --> 00:22:42,770
and a i system with a an enormous amount of knowledge and our hypothesis is

331
00:22:43,070 --> 00:22:46,750
well if you can be extracted from the web that would be one way to

332
00:22:46,750 --> 00:22:51,320
build that presumably more efficient than say what's happening inside where you have

333
00:22:51,710 --> 00:22:55,310
the knowledge engineers hand entering that knowledge over

334
00:22:55,320 --> 00:22:58,290
what's been no more than twenty years

335
00:22:58,310 --> 00:23:03,050
what i want to talk about today though is are third effort which is to

336
00:23:03,050 --> 00:23:08,420
attempt to investigate and sketch out a new paradigm for for search

337
00:23:08,430 --> 00:23:11,990
so the outline of my talk i will talk about this new paradigm what i

338
00:23:11,990 --> 00:23:14,580
mean by that talk about our

339
00:23:14,620 --> 00:23:20,180
brand if you will of information extraction which we call open information extraction and and

340
00:23:20,180 --> 00:23:25,800
then i'll talk about the various ways we try to do inference over that extracted

341
00:23:26,870 --> 00:23:28,680
and i'll conclude

342
00:23:28,730 --> 00:23:32,700
so the question that we start with is what is web search going to look

343
00:23:33,460 --> 00:23:38,700
so far into the future further than i think most of us usually think so

344
00:23:38,730 --> 00:23:40,870
pick twenty twenty could be twenty thirty

345
00:23:40,880 --> 00:23:45,070
twenty eighteen but let's think farhad what's going to look like a we still going

346
00:23:45,070 --> 00:23:46,510
to be typing

347
00:23:46,540 --> 00:23:48,700
keywords into the search box

348
00:23:49,460 --> 00:23:52,360
is it going to be a social type of things the running on top of

349
00:23:52,360 --> 00:23:57,740
face because it can be something like my hollow structure that's our yahoo answers that's

350
00:23:57,940 --> 00:23:59,250
human powered

351
00:23:59,260 --> 00:24:01,110
four that social

352
00:24:01,120 --> 00:24:05,450
is it going to be the semantic web tim berners-lee vision are we going to

353
00:24:05,450 --> 00:24:06,270
get there

354
00:24:06,640 --> 00:24:12,970
but twenty twenty what about the technology exponentials in my little rorschach shocked test look

355
00:24:12,970 --> 00:24:18,520
at how they change the equation how do we build search systems that use exponentially

356
00:24:18,520 --> 00:24:19,850
more powerful

357
00:24:19,980 --> 00:24:22,870
discs and machines

358
00:24:22,920 --> 00:24:28,480
so these are good questions and i think the best answers to borrow case lines

359
00:24:28,750 --> 00:24:31,450
the best way to predict the future is to invent it and i think that's

360
00:24:31,450 --> 00:24:34,620
what most of us here if not all of us are trying to do

361
00:24:34,670 --> 00:24:39,110
and again i'm going to focus on our particular way of inventing the future so

362
00:24:39,110 --> 00:24:41,420
and we will

363
00:24:41,440 --> 00:24:43,520
expose an electron

364
00:24:43,560 --> 00:24:45,270
four that radiation

365
00:24:45,310 --> 00:24:46,610
and then we'll see

366
00:24:46,630 --> 00:24:50,170
what you like to do with the incoming traveling wave

367
00:24:50,290 --> 00:24:53,170
plane wave

368
00:24:53,190 --> 00:24:54,900
so here is an electron

369
00:24:55,000 --> 00:24:58,330
has charge q which is non negative it has mass m

370
00:24:58,380 --> 00:24:59,810
and from the left

371
00:24:59,810 --> 00:25:03,250
comes linearly polarized radiation that's what i have chosen

372
00:25:03,270 --> 00:25:05,790
he zero cosine only get

373
00:25:05,840 --> 00:25:10,230
so just comes into play wave infinite in size comes in like that and electoral

374
00:25:10,230 --> 00:25:12,310
begins to shake

375
00:25:12,380 --> 00:25:16,710
and the force that the electron will experience f

376
00:25:18,090 --> 00:25:20,000
a few times is zero

377
00:25:20,000 --> 00:25:21,880
cosine omega right

378
00:25:21,960 --> 00:25:24,290
that's the definition of electric field

379
00:25:24,310 --> 00:25:27,630
as it is connected to force

380
00:25:27,710 --> 00:25:29,130
now when you look all

381
00:25:29,150 --> 00:25:33,380
i do this for bonding electrons through these electrons are bound

382
00:25:34,270 --> 00:25:35,810
and and or

383
00:25:37,360 --> 00:25:40,130
or in the molecule sort of bonding electrons

384
00:25:40,170 --> 00:25:44,020
so when you try to move them away from equilibrium

385
00:25:44,060 --> 00:25:45,710
they don't like that

386
00:25:45,790 --> 00:25:49,360
they experience restoring force because of the

387
00:25:49,400 --> 00:25:53,130
coulomb interaction in the end and they want to go back

388
00:25:53,130 --> 00:25:57,570
so does the restoring force that acts to some degree like a brain

389
00:25:57,640 --> 00:26:03,340
and so it is also then a resonance frequency associated with that restoring force

390
00:26:03,380 --> 00:26:05,290
so the resonance frequency

391
00:26:05,310 --> 00:26:06,540
i will simply call

392
00:26:06,540 --> 00:26:12,090
omega zero square which and k over m and k is then the equivalent of

393
00:26:12,090 --> 00:26:14,840
the spring constant which now acts

394
00:26:15,250 --> 00:26:17,810
in the anime

395
00:26:17,860 --> 00:26:21,650
i will ignore purposely for the reason of simplicity

396
00:26:21,670 --> 00:26:23,520
any form of them

397
00:26:23,570 --> 00:26:26,860
so i'm going to write down on newton's second law

398
00:26:26,860 --> 00:26:29,020
newton's second law becomes now

399
00:26:29,040 --> 00:26:31,360
x double dot

400
00:26:32,070 --> 00:26:34,070
omega zero square

401
00:26:34,110 --> 00:26:35,480
times x

402
00:26:35,520 --> 00:26:38,480
he calls this driving force which is q

403
00:26:38,650 --> 00:26:39,730
is zero

404
00:26:39,750 --> 00:26:41,520
divided by n

405
00:26:41,570 --> 00:26:45,310
times the cosine only getting and the reason why there is an annual because i

406
00:26:45,310 --> 00:26:49,980
have already removed and here and i have removed

407
00:26:50,020 --> 00:26:53,830
so this is an equation that we've seen before

408
00:26:53,840 --> 00:26:56,690
when we were dealing with force oscillations

409
00:26:56,710 --> 00:27:01,440
and it has a very simple solution that a x i mean is eight times

410
00:27:01,440 --> 00:27:03,440
the cosine of omega t

411
00:27:03,480 --> 00:27:05,790
this is only of the drive

412
00:27:05,790 --> 00:27:07,770
and so the object will follow

413
00:27:07,770 --> 00:27:09,340
in steady state solution

414
00:27:09,340 --> 00:27:11,710
and there a is the amplitude

415
00:27:11,760 --> 00:27:14,610
and the amplitude i even remember

416
00:27:14,630 --> 00:27:17,960
if you ask me three months from now i will not remember any more

417
00:27:17,980 --> 00:27:20,960
this is always upstairs remember

418
00:27:20,980 --> 00:27:23,500
and then downstairs

419
00:27:23,500 --> 00:27:28,210
you get all that stuff omega zero squares miners omega squared pillars omega squid government

420
00:27:28,210 --> 00:27:30,480
square the gummi is zero

421
00:27:30,500 --> 00:27:32,060
so you have no damping

422
00:27:32,130 --> 00:27:35,420
so you get downstairs here omega zero

423
00:27:35,750 --> 00:27:38,090
omega square that is the amplitude

424
00:27:38,110 --> 00:27:39,170
of the

425
00:27:39,290 --> 00:27:41,880
oscillating electrons

426
00:27:41,880 --> 00:27:43,500
but now we want to know

427
00:27:43,500 --> 00:27:45,830
how much radiation is going to produce

428
00:27:45,840 --> 00:27:49,880
so now we have to calculate what x double but

429
00:27:49,920 --> 00:27:51,670
actually about

430
00:27:51,690 --> 00:27:54,420
is the acceleration of the electron

431
00:27:54,440 --> 00:27:58,420
and then we want to see how much radiation comes out

432
00:27:58,500 --> 00:27:59,610
and so on

433
00:27:59,710 --> 00:28:02,400
x both becomes miners

434
00:28:02,440 --> 00:28:04,670
only guess credit

435
00:28:04,790 --> 00:28:09,400
sachs was taking the second derivative right you've seen that before

436
00:28:09,420 --> 00:28:13,920
and so that becomes n minus q

437
00:28:13,960 --> 00:28:16,060
he zero

438
00:28:16,130 --> 00:28:18,310
divided by

439
00:28:18,310 --> 00:28:20,110
omega zero grant

440
00:28:20,150 --> 00:28:22,960
mine is only gets credit and they upstairs

441
00:28:22,960 --> 00:28:32,110
and omega square

442
00:28:32,210 --> 00:28:36,710
and that's the next generation of this electoral

443
00:28:36,730 --> 00:28:37,860
but now

444
00:28:37,920 --> 00:28:42,070
i know how much power goes into that field

445
00:28:42,110 --> 00:28:47,290
because the power goes into the field where do i have that i raise my

446
00:28:47,290 --> 00:28:48,440
the raise the

447
00:28:48,480 --> 00:28:54,090
lot work i will do that for that reason so that total politics radiated is

448
00:28:54,090 --> 00:28:57,710
proportional to a square help you remember that i raise the wrong part of the

449
00:28:58,750 --> 00:29:03,090
and so the a squared is what i really want to know

450
00:29:03,150 --> 00:29:05,650
and so the power

451
00:29:05,670 --> 00:29:08,980
that is radiated by the shaking electron

452
00:29:09,000 --> 00:29:12,110
is proportional to forget all the constant

453
00:29:12,150 --> 00:29:14,040
only got two to force

454
00:29:14,060 --> 00:29:16,480
divided by omega zero square

455
00:29:16,540 --> 00:29:18,920
mine is only because it's great

456
00:29:19,940 --> 00:29:23,610
forget all the rest all the rest is gone

457
00:29:23,630 --> 00:29:24,880
and now

458
00:29:24,880 --> 00:29:26,340
if omega

459
00:29:26,360 --> 00:29:28,420
is way below resonance

460
00:29:28,480 --> 00:29:32,570
this is proportional to omega three four

461
00:29:32,590 --> 00:29:35,210
that's a very famous result

462
00:29:35,310 --> 00:29:39,480
which is known in the literature as rayleigh scattering

463
00:29:39,500 --> 00:29:42,170
and the reason why we call it scattering

464
00:29:42,190 --> 00:29:44,750
rayleigh scattering

465
00:29:44,810 --> 00:29:48,230
that you must understand that the radiation that comes in

466
00:29:48,250 --> 00:29:50,560
has this frequency

467
00:29:50,650 --> 00:29:56,000
but the radiation that comes out has exactly the same frequency look that is the

468
00:29:56,000 --> 00:29:57,560
oscillation of the

469
00:29:58,980 --> 00:30:01,810
so there is no change in the frequency

470
00:30:01,860 --> 00:30:05,400
radiation comes in a certain color red light

471
00:30:05,440 --> 00:30:09,210
and then there is a black hole which starts to shake gets very nervous because

472
00:30:09,210 --> 00:30:14,290
of the field and it radiates again red light the color does not change but

473
00:30:14,290 --> 00:30:17,340
it can go off in different directions

474
00:30:17,380 --> 00:30:18,830
that's why we call this

475
00:30:18,880 --> 00:30:20,500
really scattering

476
00:30:20,520 --> 00:30:22,770
and if you take oxygen and nitrogen

477
00:30:22,790 --> 00:30:24,360
in our atmosphere

478
00:30:24,440 --> 00:30:26,330
then the resonance frequency

479
00:30:26,330 --> 00:30:27,830
it is way above

480
00:30:27,840 --> 00:30:29,860
the frequency of visible light

481
00:30:29,920 --> 00:30:33,290
the resonant frequency for optimizing the far UV

482
00:30:33,310 --> 00:30:35,290
and so we need to that condition

483
00:30:35,290 --> 00:30:42,040
that visible light strikes oxygen and nitrogen molecules and for that matter many other molecules

484
00:30:43,110 --> 00:30:45,630
relationship holds

485
00:30:45,690 --> 00:30:47,560
and what that tells you then is

486
00:30:47,570 --> 00:30:49,920
that blue lights

487
00:30:49,920 --> 00:30:52,880
has the way higher probability to be scanner

488
00:30:53,730 --> 00:30:55,340
moving electrons

489
00:30:56,250 --> 00:30:58,480
red light because the wavelength

490
00:30:58,500 --> 00:31:03,000
or the frequency i should say of blue light is substantially larger than the frequency

491
00:31:03,000 --> 00:31:04,880
of red light

492
00:31:04,920 --> 00:31:08,520
so i let me try to stay on the centre bought

493
00:31:08,540 --> 00:31:12,060
and show you what the differences in

494
00:31:14,270 --> 00:31:16,210
so only w

495
00:31:16,230 --> 00:31:18,650
divided by omega red

496
00:31:18,670 --> 00:31:23,070
it is approximately one point five taking side of the spectrum

497
00:31:23,130 --> 00:31:25,330
about sixty five hundred

498
00:31:25,340 --> 00:31:28,250
angstroms random forty five hundred angstroms

499
00:31:29,540 --> 00:31:34,500
and that means that one point five of the pol four is about five

500
00:31:34,540 --> 00:31:36,340
so the blue light

501
00:31:36,340 --> 00:31:41,660
so i am officially go from here on i will be presenting negating the descriptors

502
00:31:41,660 --> 00:31:47,370
into a compact image presentation so this is joint work resettlement they use on polish

503
00:31:47,650 --> 00:31:51,430
from going on but there is something you could

504
00:31:51,540 --> 00:31:54,090
what you want to address is the

505
00:31:54,980 --> 00:31:59,130
image search on large case we want to be able to retrieve is to images

506
00:31:59,130 --> 00:32:04,570
of the same scene or object can the default view viewpoint background so for instance

507
00:32:04,570 --> 00:32:10,310
the first korean bottom is the amount of the database image we also want to

508
00:32:10,310 --> 00:32:16,790
be able to handle copyright attacks such as the cropping editing the second example and

509
00:32:16,790 --> 00:32:23,600
the bottom these images database it has been printing scratch on this can

510
00:32:23,640 --> 00:32:28,120
this because we don't use a short response time from image the to image database

511
00:32:28,360 --> 00:32:29,750
two billions

512
00:32:29,750 --> 00:32:31,990
the images online machine

513
00:32:32,000 --> 00:32:38,180
so most of the related work depends bag of features framework could do get approach

514
00:32:38,180 --> 00:32:44,760
by city councilman as improvement includes is the use of large vocabularies semi pulls descriptor

515
00:32:44,760 --> 00:32:51,560
presentation so decision which has been integrating reasons index and some nice techniques from text

516
00:32:51,560 --> 00:32:56,700
that retrieval as being elected to the image search such as the quick functions

517
00:32:56,810 --> 00:33:02,150
however this approach is the main tractable for human use images only

518
00:33:02,170 --> 00:33:08,170
so efficiency may be improved by using min ash or decision extension or by directly

519
00:33:08,170 --> 00:33:11,650
compressing about of features presentations however

520
00:33:11,650 --> 00:33:16,860
still hundreds of bytes are required to obtain was the quality of the reason is

521
00:33:16,860 --> 00:33:20,370
that exactly as good as the features with large vocabularies

522
00:33:21,030 --> 00:33:26,510
an alternative which consists in using these descriptors combines percussion techniques

523
00:33:27,480 --> 00:33:33,760
the descriptor itself give only very limited invariance to scaling rotation of crops

524
00:33:33,760 --> 00:33:37,280
and is sufficient for our application

525
00:33:37,280 --> 00:33:41,670
so our aim is to optimize trade off between search quality search kind of course

526
00:33:42,090 --> 00:33:46,180
but also memory usage which is critical parameters if you want to be able to

527
00:33:46,180 --> 00:33:48,870
store is indexed memory four

528
00:33:48,880 --> 00:33:55,250
besides the databases approach licence during continuation of three stages

529
00:33:55,250 --> 00:34:00,590
so first assumes that some SIFT descriptors have already been extracted from the images so

530
00:34:00,590 --> 00:34:01,870
this is quite stunned

531
00:34:01,870 --> 00:34:03,810
this gives us and

532
00:34:03,830 --> 00:34:06,190
the vectors that to present image

533
00:34:06,220 --> 00:34:09,500
and now we are going to have make sure he is for us to look

534
00:34:09,500 --> 00:34:14,080
at this to aggregation which from a set of vectors will use a single vector

535
00:34:14,080 --> 00:34:20,970
representing the image vector is subsequently reduced using dimensionality reduction so in these cases is

536
00:34:20,970 --> 00:34:26,500
you know that dimensionality reduction context that it is not exactly stand on then you

537
00:34:26,500 --> 00:34:33,460
could use that is on could produce a code which is the final image presentation

538
00:34:33,500 --> 00:34:35,500
so let's start with the first stage

539
00:34:35,520 --> 00:34:38,680
so we want to be able to present an image by using a fixed size

540
00:34:38,680 --> 00:34:41,930
vector services applications so that they consider

541
00:34:42,060 --> 00:34:46,680
was the most popular idea consists in using the bag of features framework which is

542
00:34:46,680 --> 00:34:50,930
the case of large scale image search for use in specific fields

543
00:34:51,210 --> 00:34:56,830
which are either mentioned and we see in our experiments this idea dimensionality is not

544
00:34:56,840 --> 00:35:01,840
good because it is difficult to reduce their tools without introducing some loss

545
00:35:02,120 --> 00:35:07,650
on this nineteen ninety which is the earliest known which is official canon representation was

546
00:35:07,650 --> 00:35:08,530
very popular

547
00:35:08,590 --> 00:35:14,280
which produce some non sparse vectors these are less sparse on still provide excellent because

548
00:35:14,280 --> 00:35:19,060
it was limited victoria dimensionality so i mean so it can be seen as a

549
00:35:19,060 --> 00:35:21,900
simplification of this presentation

550
00:35:21,900 --> 00:35:28,270
it was aggregation stage it's school vlad which stands for vector of locally aggregated descriptors

551
00:35:28,280 --> 00:35:33,210
the learning stage is quite simple it's exactly the same as far as the features

552
00:35:33,220 --> 00:35:37,030
approach to his we just as the chemicals contained

553
00:35:37,030 --> 00:35:41,150
it's is quite simple now if you want to call six of presentation

554
00:35:41,300 --> 00:35:45,960
we first have to assign each SIFT descriptors to its there are some tree through

555
00:35:46,020 --> 00:35:48,960
centuries of of includes real world

556
00:35:49,300 --> 00:35:52,670
but instead of counting the number of occurrences of each

557
00:35:52,820 --> 00:35:58,880
if you are going to compute the difference between each descriptors on its central to

558
00:35:59,210 --> 00:36:00,960
produce a set of n

559
00:36:00,960 --> 00:36:07,510
victor of differences which are does concentrate so for each simtree we have sort victor

560
00:36:07,570 --> 00:36:10,320
of differences in some cases we have no reason

561
00:36:10,360 --> 00:36:12,150
we just have to send

562
00:36:12,200 --> 00:36:15,460
this also to produce a set of character

563
00:36:15,480 --> 00:36:18,650
you have the same dimensionality as SIFT descriptors

564
00:36:18,650 --> 00:36:21,800
on our final presentation is just a concatenation

565
00:36:21,920 --> 00:36:24,690
of victor companies which is a phenomenon

566
00:36:24,710 --> 00:36:29,900
two you might seems that the final dimensionality is much higher than for biographies them

567
00:36:29,900 --> 00:36:35,460
for but if you just because have this kid times dimensionality of the local descriptors

568
00:36:35,490 --> 00:36:39,090
but in practice it is not case because the typical value

569
00:36:39,100 --> 00:36:41,910
four numbers on sixty four

570
00:36:41,940 --> 00:36:46,840
to be compared two millions words about the features approaches to finally we have a

571
00:36:46,840 --> 00:36:51,850
vector which is only a few thousand companies

572
00:36:51,860 --> 00:36:56,220
so for those who are familiar with the picture i see for presentation

573
00:36:56,230 --> 00:37:01,800
know that you can use the same kind of presentation that is for each simtree

574
00:37:02,260 --> 00:37:07,090
we can depict the four times four grid of the energy of the gradient for

575
00:37:07,090 --> 00:37:13,330
each possible orientations of rotation the main difference in this case is that you do

576
00:37:13,330 --> 00:37:18,660
the difference between discrete on the century we might have some negative values these are

577
00:37:18,720 --> 00:37:24,510
presented here in this case against blue for positive and you cannot is that forces

578
00:37:24,590 --> 00:37:30,590
these two images which are quite similar we obtain comparable of presentation and activities because

579
00:37:30,590 --> 00:37:32,580
lecture in a completely different way

580
00:37:32,590 --> 00:37:37,380
there is no algorithm to decide in advance whether computer program will halt not you

581
00:37:37,380 --> 00:37:40,960
know no general algorithm which will work in all possible cases and he proved this

582
00:37:40,960 --> 00:37:42,590
using that method

583
00:37:42,590 --> 00:37:45,400
from set theory cantor's diagonal method

584
00:37:45,420 --> 00:37:46,920
this is this is

585
00:37:46,920 --> 00:37:51,300
slight somewhat hidden but it's very inter in nineteen thirty six

586
00:37:51,320 --> 00:37:52,920
the wonderful page

587
00:37:54,170 --> 00:37:56,030
what i wanna do is

588
00:37:56,050 --> 00:38:00,730
so you know that there is no general method there's is something fundamentally

589
00:38:00,800 --> 00:38:05,650
example is the difficulty in trying to decide whether individual programmes will hold for now

590
00:38:05,650 --> 00:38:11,510
if they hold there is no problem in deciding in discovering that because in because

591
00:38:11,510 --> 00:38:15,630
in theory does run the program and if you're patient eventually you'll see that holds

592
00:38:15,630 --> 00:38:19,230
the holds but in patient could be a billion years or ten to the ten

593
00:38:19,230 --> 00:38:20,440
to the ten years

594
00:38:20,440 --> 00:38:23,650
but in principle if you want to prove the problem hotel you have to do

595
00:38:23,650 --> 00:38:24,860
is run it

596
00:38:24,880 --> 00:38:28,170
and wait until it the real problems

597
00:38:28,170 --> 00:38:33,400
the tree discovered the fundamental problem that he chose is beyond the power of computational

598
00:38:33,400 --> 00:38:37,340
mathematics and also be on the power of formal axiomatic theory

599
00:38:37,360 --> 00:38:39,590
it is to show that program never holds

600
00:38:39,610 --> 00:38:41,880
to be able to do this in every possible case

601
00:38:41,900 --> 00:38:44,740
that is really a fundamentally

602
00:38:44,760 --> 00:38:46,460
impossible things

603
00:38:47,530 --> 00:38:51,400
so the question is looking at individual programmes and trying to decide whether or not

604
00:38:51,400 --> 00:38:54,400
they hold and the most interesting thing is to is to be able to show

605
00:38:54,400 --> 00:38:58,840
in in in general the program cannot hold

606
00:38:58,840 --> 00:39:02,070
a string points out you know he shows that there is no algorithm to decide

607
00:39:02,470 --> 00:39:04,530
in general whether program holton

608
00:39:04,550 --> 00:39:09,210
but he immediately deduces incompetent of the core area in his nineteen thirty six paper

609
00:39:09,320 --> 00:39:12,820
and the argument he makes is an argument that i give you this earlier in

610
00:39:12,820 --> 00:39:13,900
the first lecture

611
00:39:14,010 --> 00:39:17,940
the basic idea is that has is if you have a formal axiomatic theory you

612
00:39:17,940 --> 00:39:20,780
can in principle mechanically generate all the theorems

613
00:39:20,820 --> 00:39:24,300
so if you had a formal axiomatic theory they're always enable you to prove whether

614
00:39:24,300 --> 00:39:25,690
an individual program

615
00:39:25,740 --> 00:39:27,630
also fails to hold

616
00:39:27,650 --> 00:39:31,630
that's impossible because by running through all possible proves mechanically

617
00:39:31,650 --> 00:39:35,130
you could either we do that until you find the proof of the program also

618
00:39:35,130 --> 00:39:38,030
you find proof never holds so if you had if for

619
00:39:38,030 --> 00:39:42,920
mathematical theory that always enable you to show individual cases whether program alternating that will

620
00:39:42,920 --> 00:39:47,280
in fact give you an algorithm for always mechanically deciding whether problem tonight and he

621
00:39:47,280 --> 00:39:51,550
showed using kind of diagonal argument that that's impossible

622
00:39:51,570 --> 00:39:53,610
so he

623
00:39:53,670 --> 00:39:57,960
in nineteen thirty six to during the views of correlated from his

624
00:39:57,960 --> 00:40:00,420
uncomputability results in computers

625
00:40:00,440 --> 00:40:03,740
and i don't know this in my metamap but by the way

626
00:40:03,860 --> 00:40:09,570
that's a technical non-technical book it's it's a high level

627
00:40:09,570 --> 00:40:15,110
o two will go is as shown on the front say it's it's the highest

628
00:40:15,130 --> 00:40:19,650
it's an attempt to explain these ideas higher level and the journalist would explain it's

629
00:40:19,650 --> 00:40:21,360
actually a serious math book

630
00:40:21,380 --> 00:40:24,190
disguised as a popular science book

631
00:40:24,210 --> 00:40:27,340
and i don't know why the publisher published it

632
00:40:27,360 --> 00:40:30,190
they insisted on publishing this book for a general audience

633
00:40:30,210 --> 00:40:31,260
OK so

634
00:40:31,340 --> 00:40:36,300
so what is the halting probability omega instead of looking at individual programs i want

635
00:40:36,300 --> 00:40:39,670
to look at the ensemble of all possible computer programs

636
00:40:39,690 --> 00:40:42,630
and the idea is that you take back you close your eyes you take about

637
00:40:42,650 --> 00:40:46,530
you throw it all possible computer programs is an infinite set and these have to

638
00:40:46,530 --> 00:40:49,150
be self-limiting computer programs

639
00:40:49,170 --> 00:40:54,030
OK not ordinary computer programs this only works in the nineteen seventies version

640
00:40:54,050 --> 00:40:58,150
of this theory not in the nineteen sixties version so you close your eyes you

641
00:40:58,150 --> 00:40:59,300
shake the

642
00:40:59,320 --> 00:41:03,920
back will then you pick out one you ask what is the probability this program

643
00:41:04,960 --> 00:41:08,050
and that to find the whole thing probability now

644
00:41:08,070 --> 00:41:15,860
this only works because let me let me write it down it's defined like this

645
00:41:15,920 --> 00:41:19,030
so you fix the computer programming language

646
00:41:19,070 --> 00:41:21,030
and has to be self-limiting

647
00:41:21,970 --> 00:41:27,190
basically every car cable program also contributes one over to the kind of the halting

648
00:41:29,670 --> 00:41:30,920
this is

649
00:41:30,960 --> 00:41:33,880
this is the now with the old theory

650
00:41:33,900 --> 00:41:37,690
with the normal notion of the computer program where there are two the NMB programs

651
00:41:37,690 --> 00:41:40,210
this sum diverges to infinity

652
00:41:40,900 --> 00:41:42,650
this is not well defined

653
00:41:42,710 --> 00:41:45,130
to get this to be meaningful

654
00:41:45,130 --> 00:41:49,460
you need to have solved many computer programs and then this will be between zero

655
00:41:49,460 --> 00:41:50,960
and one like

656
00:41:50,960 --> 00:41:52,940
like any good probability

657
00:41:52,960 --> 00:41:56,800
you know one would be all programs all there would be no program salt and

658
00:41:56,800 --> 00:42:00,940
some do and some don't

659
00:42:02,920 --> 00:42:05,690
yes it is

660
00:42:05,760 --> 00:42:07,880
it's version

661
00:42:19,360 --> 00:42:21,470
no programs are my code

662
00:42:21,530 --> 00:42:27,150
this is for those of you who know of shannon information theory and coding theory

663
00:42:27,170 --> 00:42:31,550
these are this is this is a version of kraft's inequality it's a kind of

664
00:42:31,550 --> 00:42:34,530
an infinite version of kraft inequality no i agree

665
00:42:34,590 --> 00:42:36,630
this is prefix free set

666
00:42:36,650 --> 00:42:41,130
instantaneous code the set of all programs has to be instantaneous to prefix three

667
00:42:41,130 --> 00:42:43,170
yeah this that's why

668
00:42:43,170 --> 00:42:47,150
that's why this is between this is between zero and one

669
00:42:47,150 --> 00:42:51,180
now about five thousand nodes are connected directly to the nucleus

670
00:42:51,190 --> 00:42:52,810
and the class

671
00:42:52,880 --> 00:42:54,460
is connected to

672
00:42:54,480 --> 00:42:58,770
i don't know which you could also the fact that features but this is the

673
00:42:59,510 --> 00:43:04,930
part of the network but what is important is not actually very important for

674
00:43:04,940 --> 00:43:09,920
for in the sense that if you remove this notice for example the distance between

675
00:43:09,920 --> 00:43:14,690
any of between the and with depends almost doubles

676
00:43:14,710 --> 00:43:19,760
so this close is very important for the network and you can learn many many

677
00:43:19,760 --> 00:43:24,770
important properties on the network for using this is a statistical physics support which called

678
00:43:24,770 --> 00:43:26,270
conditional decomposition

679
00:43:27,840 --> 00:43:33,730
so in other networks in economic network from become notable for example similar phenomena that

680
00:43:33,730 --> 00:43:38,500
can better understand what is the the close what is the case in the economic

681
00:43:38,500 --> 00:43:44,260
system and this is important so this is another example of networks and show that

682
00:43:44,300 --> 00:43:49,790
there is an example which is related to climate which was just publishing two papers

683
00:43:49,790 --> 00:43:52,890
one in physical review letters and running europhysics letters

684
00:43:52,970 --> 00:43:58,390
so what we do is we do the following will come to get him

685
00:43:59,950 --> 00:44:06,530
and what we know that there is a factor in effect which is actually considered

686
00:44:06,530 --> 00:44:09,780
as the temperature increases in during the

687
00:44:09,830 --> 00:44:14,660
this is the between a a nineteen eighty in two thousand five

688
00:44:14,710 --> 00:44:19,660
and you can see the temperature increases dramatically which is majority in the pacific ocean

689
00:44:19,660 --> 00:44:24,860
in this range and the temperature increases above a certain value

690
00:44:24,900 --> 00:44:29,100
so for sure because it in effect so you can see the new comes you

691
00:44:29,210 --> 00:44:35,610
and you comes in very seriously in another effect and very important for climate i

692
00:44:35,610 --> 00:44:40,910
mean it's a economics also and we want to know but to understand you know

693
00:44:40,910 --> 00:44:42,200
what we do we set

694
00:44:42,730 --> 00:44:45,630
it can be into four separate networks

695
00:44:45,630 --> 00:44:51,780
this is one of which included another notable means the exact positions

696
00:44:51,810 --> 00:44:56,450
and the network in the US and another the cells and what we did is

697
00:44:57,460 --> 00:45:02,600
network which is the similarity of the cost the combination of this one is that

698
00:45:02,600 --> 00:45:06,370
we need some elements of between places in the network

699
00:45:06,400 --> 00:45:11,150
and in the same for each one of the networks and we measured

700
00:45:11,700 --> 00:45:16,120
the number of links that exist

701
00:45:16,170 --> 00:45:18,680
in the network during the time

702
00:45:18,700 --> 00:45:22,330
in the ordered which includes also in time

703
00:45:22,340 --> 00:45:27,300
and what you find that the it links actually many links disappear go down to

704
00:45:27,300 --> 00:45:33,130
see exactly the appears is this is number one and jimmy and you see when

705
00:45:33,130 --> 00:45:35,760
you get to the number of links that exist in the network

706
00:45:35,810 --> 00:45:40,030
and suddenly the neocons then there is a big dramatic change in the number of

707
00:45:40,030 --> 00:45:43,240
links again you know the number of links that goes down the number of links

708
00:45:43,440 --> 00:45:49,250
this is one but what is surprising is that that not only the network can

709
00:45:49,250 --> 00:45:54,970
this can identify individuals we know that the reason you actually this is the measurement

710
00:45:55,960 --> 00:46:00,680
average temperature in the ontology and once it's already in use it much better

711
00:46:00,770 --> 00:46:05,650
when we measure only locally but this is measuring but not on this energy we

712
00:46:05,650 --> 00:46:09,880
see that in fact which is not believed to be also affect other places this

713
00:46:09,880 --> 00:46:12,980
see it also in this achievement in this in is and you can see this

714
00:46:12,980 --> 00:46:16,660
picture this number two number two a think is this one

715
00:46:16,880 --> 00:46:21,080
and number two you seem the in number three and number four and not the

716
00:46:22,150 --> 00:46:26,540
i don't see the temperature to reduce it is average temperature but generally you but

717
00:46:26,540 --> 00:46:28,170
when you look at the talk

718
00:46:28,180 --> 00:46:30,950
you can see the temperature affects the you consider

719
00:46:31,310 --> 00:46:32,960
measuring the temperatures

720
00:46:32,970 --> 00:46:37,300
and making the temperature you see the links disappeared in

721
00:46:37,840 --> 00:46:42,460
in different places around the world which are not in the original film new and

722
00:46:44,050 --> 00:46:48,360
this is an house in how to five kilometres if you want to go and

723
00:46:48,360 --> 00:46:53,380
you get the results the patrol fluctuations in the density of most effective and you

724
00:46:53,660 --> 00:46:57,800
but effectively and you can see in the network what does it mean

725
00:46:59,590 --> 00:47:00,580
which is in

726
00:47:00,590 --> 00:47:05,210
the not all the interaction between the nodes may be more important than the notes

727
00:47:05,210 --> 00:47:09,990
themselves which makes it makes sense so i wanted to look at interaction and to

728
00:47:09,990 --> 00:47:16,510
see whether we can learn about a new so and they actually would be

729
00:47:16,530 --> 00:47:21,150
well since it is much more sensitive than the temperature we believe maybe it will

730
00:47:21,160 --> 00:47:25,000
be useful in the future we don't have yet to sort out to to endorsing

731
00:47:25,060 --> 00:47:29,840
the phenomenon that even before the effect and only a few days before this would

732
00:47:29,840 --> 00:47:35,350
be a prediction and it will be very important not only these we see also

733
00:47:36,200 --> 00:47:41,090
the link breaks and what we see in the blink surely this is the a

734
00:47:41,110 --> 00:47:43,040
typical links and you can see

735
00:47:43,090 --> 00:47:45,850
this is the actual this type of links

736
00:47:45,860 --> 00:47:49,220
this is one of the things that the this is the cross correlation between the

737
00:47:50,060 --> 00:47:54,470
and you can see that the link is a coarse correlated in the center some

738
00:47:54,470 --> 00:47:57,470
links are not in the centre of the time delay

739
00:47:57,520 --> 00:48:01,580
this is the time lag to cross correlation time so they have an effect after

740
00:48:01,580 --> 00:48:03,150
on the twenty days

741
00:48:03,160 --> 00:48:07,850
the maximum easier as the true course correlation which may

742
00:48:07,920 --> 00:48:14,690
represent meaning that using the word and look at the links which a very different

743
00:48:14,720 --> 00:48:20,090
you can see that the doing that such patterns but the link appears and disappears

744
00:48:20,090 --> 00:48:25,130
appear disappear and measure this you can see again exactly this so the patterns we

745
00:48:25,470 --> 00:48:28,610
look for patterns of plus minus plus appear disappeared

746
00:48:28,620 --> 00:48:34,040
and measure the particles you can see also that these patterns the exactly the appearance

747
00:48:34,260 --> 00:48:39,660
so we need to understand the disappear but we can see that they not completely

748
00:48:39,660 --> 00:48:43,700
disappeared but they appear and disappear during the mean it's

749
00:48:43,730 --> 00:48:48,510
so i think i'm you i was already almost time let me only summarize the

750
00:48:48,540 --> 00:48:51,240
aggression and gives some some challenges

751
00:48:52,040 --> 00:48:57,530
so my talk is wasn't feeling and applications so some utility companies and i didn't

752
00:48:57,530 --> 00:49:00,850
give it full network and

753
00:49:00,850 --> 00:49:02,680
four distances for example

754
00:49:03,160 --> 00:49:09,110
this is my time is longer and this is the famous from result look into

755
00:49:09,110 --> 00:49:11,730
small this executive separation but

756
00:49:11,750 --> 00:49:18,040
what wasn't known is that misty and this was using percolation using celia for physics

757
00:49:18,040 --> 00:49:21,520
one can find that if you take the most vulnerable

758
00:49:21,520 --> 00:49:25,910
any another kind of graph it becomes much longer it becomes all the into the

759
00:49:25,910 --> 00:49:33,040
ones so this is relatively new is that only three with four years of networks

760
00:49:33,040 --> 00:49:37,850
was studied classical metal was studied fifty years almost and another

761
00:49:37,890 --> 00:49:41,480
the result is that logan is the distance when your scale free

762
00:49:41,480 --> 00:49:45,180
and this little again becomes low again for scale free when you can look at

763
00:49:45,180 --> 00:49:48,310
the minimum spanning which means you look and optimize on the

764
00:49:48,330 --> 00:49:54,120
the minimum of the optimal but surely replications of network studies in understanding but also

765
00:49:54,120 --> 00:49:55,500
similar systems

766
00:49:55,640 --> 00:50:01,480
for for the network and metabolic networks and other biological network gene

767
00:50:01,500 --> 00:50:08,410
interaction networks social we've confined uses social behaviors which could not be found those you

768
00:50:08,410 --> 00:50:14,540
need a lot of statistical analysis showing recent searches so that you can find uses

769
00:50:14,560 --> 00:50:18,020
in their social networks social systems predicting

770
00:50:18,460 --> 00:50:25,620
epidemic spreading using elements maybe will be a future to to prevent epidemics maybe

771
00:50:25,660 --> 00:50:30,790
eventually can improve transport in weighted networks of size and

772
00:50:30,790 --> 00:50:34,510
but let's

773
00:50:37,070 --> 00:50:44,080
say well let me draw just a little bit so here is the horizontal let's

774
00:50:44,190 --> 00:50:48,870
introduce a coordinate system in was there's a horizontal and here is the best to

775
00:50:48,870 --> 00:50:53,010
indicate where I am with respect to the picture

776
00:50:54,110 --> 00:50:55,510
so here boat

777
00:50:56,370 --> 00:50:57,970
here the

778
00:51:01,840 --> 00:51:05,530
and the path of the boat is going to make a 45 degree angle with

779
00:51:05,660 --> 00:51:08,820
so this is the path

780
00:51:08,890 --> 00:51:10,340
that we're talking about

781
00:51:10,410 --> 00:51:12,840
and now let's label what I know

782
00:51:12,860 --> 00:51:16,120
well this angle is

783
00:51:16,160 --> 00:51:18,860
45 degrees

784
00:51:20,320 --> 00:51:27,770
this angle I don't know but of course I can calculated easily enough because it

785
00:51:27,770 --> 00:51:29,890
it has to do with

786
00:51:29,950 --> 00:51:34,720
if I know the coordinates of this point x y

787
00:51:34,780 --> 00:51:39,490
and of course that horizontal angle I know the slope of this line and that's

788
00:51:39,490 --> 00:51:43,800
the ad that I will be related to the cell so it's called itself

789
00:51:48,070 --> 00:51:48,780
and now

790
00:51:48,800 --> 00:51:50,320
what I want to know is

791
00:51:51,270 --> 00:51:53,950
but the slope of the whole

792
00:51:54,090 --> 00:52:02,950
have so why prime so let's call y equals y of x the unknown function

793
00:52:03,070 --> 00:52:08,360
whose path whose paragraph is going to be the of boats

794
00:52:09,350 --> 00:52:10,870
unknown graphs

795
00:52:11,650 --> 00:52:20,890
what's it's so well it's slope is the tangent of the sum of these 2

796
00:52:20,890 --> 00:52:24,630
angles alpha plus 45 degrees

797
00:52:24,690 --> 00:52:33,590
now what why not well I know that the tangent of alpha is how much

798
00:52:33,610 --> 00:52:37,740
that's why over x in other words if this is the point x over y

799
00:52:37,740 --> 00:52:41,740
this is the angle it makes with the horizontal if you think of it over

800
00:52:41,740 --> 00:52:43,160
here this

801
00:52:43,820 --> 00:52:48,490
also this angle is the same as that 1 and Y over its slope is

802
00:52:48,570 --> 00:52:52,140
of the line is white X so the tangent to the angle is Y over

803
00:52:52,140 --> 00:52:56,550
X about the tangent of 45 degrees that's 1

804
00:52:57,590 --> 00:53:03,120
and then there's a formula others the hard part or all of the knows of

805
00:53:03,140 --> 00:53:07,310
formula exists and then you look it up if you've forgotten it by relating the

806
00:53:07,310 --> 00:53:11,670
tangent giving you the tensions somewhat 2 angles and the you can if you like

807
00:53:12,250 --> 00:53:16,650
clever derived from the formula for the sign of and cosine of some 2 angles

808
00:53:16,650 --> 00:53:20,670
but 1 peak is worth a thousand finesses so it is

809
00:53:20,680 --> 00:53:22,070
the tangent of alpha

810
00:53:22,300 --> 00:53:28,860
what's the tangent of 45 degrees so we write out also tails divided by 1

811
00:53:28,950 --> 00:53:35,360
so you'll released when the formula 1 minus tangent alpha times 45 degrees this would

812
00:53:35,360 --> 00:53:40,110
work for the tangent to the sum of any 2 angles that's a formula so

813
00:53:40,110 --> 00:53:43,610
how does that I get that why prime is equal to

814
00:53:43,660 --> 00:53:50,700
the tangent of alpha which is why over x always like that combination task 1

815
00:53:50,720 --> 00:53:51,510
divided by

816
00:53:52,090 --> 00:53:55,890
1 minus Y over X times 1

817
00:53:56,570 --> 00:54:00,190
now there's no reason for doing anything to it but let's make a little little

818
00:54:00,190 --> 00:54:06,770
prettier and thereby make it less obvious is a homogeneous equation by it's by multiply

819
00:54:06,790 --> 00:54:10,050
top and bottom by x it looks pretty

820
00:54:11,360 --> 00:54:17,430
X plus Y over X minus Y equals y prime that's a differential equations

821
00:54:18,290 --> 00:54:23,170
but notice that last step to make it look pretty has on the on the

822
00:54:23,170 --> 00:54:29,370
good work is fine if you immediately recognise this as being a homogeneous equation because

823
00:54:29,370 --> 00:54:32,970
you can divide top and bottom by xxx but here it's a lot clearer that

824
00:54:32,970 --> 00:54:38,550
that's a homogeneous equation because it's already been written in the right form

825
00:54:38,610 --> 00:54:45,850
OK let's now since we know what to do without the use of the new

826
00:54:45,850 --> 00:54:51,210
variables equals Y over X and as I wrote up there z

827
00:54:51,220 --> 00:54:57,570
for why prime will substitute z prime X was

828
00:54:57,650 --> 00:54:59,990
and without let's

829
00:55:02,210 --> 00:55:16,190
but solving the equation becomes z primed X plus z is equal to

830
00:55:16,220 --> 00:55:20,120
z + 1 over 1 minus Z.

831
00:55:21,710 --> 00:55:25,130
what is the only other we want separated variables so you have to put all

832
00:55:25,150 --> 00:55:31,830
disease on 1 side and so this is going to be x d z dx

833
00:55:31,890 --> 00:55:35,450
he calls this thing minus z

834
00:55:35,450 --> 00:55:37,680
locality in the following sense

835
00:55:37,730 --> 00:55:41,450
that if x i and x j look close to each other in the high

836
00:55:41,450 --> 00:55:43,570
dimensional space

837
00:55:43,600 --> 00:55:47,520
the image is why i and y j should be close to each other in

838
00:55:47,520 --> 00:55:50,520
the low dimensional space

839
00:55:54,000 --> 00:55:58,150
in other words we're looking for this map

840
00:55:58,160 --> 00:56:02,230
because that will actually get to that locality preservation

841
00:56:02,230 --> 00:56:03,890
preservation but

842
00:56:03,890 --> 00:56:05,160
this particular

843
00:56:05,200 --> 00:56:06,310
notion of it

844
00:56:08,280 --> 00:56:13,320
small maps are going to be given by the actions of the classes

845
00:56:14,390 --> 00:56:16,720
what do you want to do is you want to minimize

846
00:56:17,430 --> 00:56:20,020
particular quadratic forms

847
00:56:20,030 --> 00:56:21,850
so notice that

848
00:56:21,860 --> 00:56:24,230
w i g

849
00:56:24,290 --> 00:56:26,230
is large

850
00:56:26,280 --> 00:56:31,440
if x i and x j are close to each other

851
00:56:33,200 --> 00:56:38,600
when w ij is large XII next year's clo are close to each other and

852
00:56:38,600 --> 00:56:42,360
this forces why i and y j to be close to each other in that

853
00:56:45,150 --> 00:56:49,480
if w i j is smaller

854
00:56:49,500 --> 00:56:54,100
that means i next year were not necessarily closely that each other

855
00:56:54,140 --> 00:56:57,890
then we don't care what why and YGR

856
00:56:57,910 --> 00:57:01,780
we only want to ensure that if points were close to each other

857
00:57:01,790 --> 00:57:03,820
in the high dimensional space

858
00:57:03,860 --> 00:57:09,560
they continued to remain close to each other in the low dimensional space

859
00:57:09,580 --> 00:57:11,320
so this is

860
00:57:12,440 --> 00:57:16,980
hi argument

861
00:57:17,030 --> 00:57:19,860
here here's the fundamental identity

862
00:57:19,900 --> 00:57:23,040
this is a few lines of linear algebra which

863
00:57:23,060 --> 00:57:24,990
so you are encouraged to skip

864
00:57:25,020 --> 00:57:29,270
we are trying to prove yourself but basically that quadratic form which i just wrote

865
00:57:29,270 --> 00:57:34,120
down why i minus white squares WHA ends up being just the same quadratic form

866
00:57:34,120 --> 00:57:37,900
is why transpose and why so minimizing that

867
00:57:37,910 --> 00:57:42,150
it is equivalent to taking the i can vectors

868
00:57:43,600 --> 00:57:46,080
and that's how the eigenvectors of the laplacian

869
00:57:46,110 --> 00:57:50,070
show up

870
00:57:59,780 --> 00:58:01,860
give you a picture first

871
00:58:01,970 --> 00:58:05,950
before i will tell you a little bit about what this might correspond to on

872
00:58:05,950 --> 00:58:12,910
the underlying manifold

873
00:58:12,940 --> 00:58:14,730
OK the picture is this

874
00:58:14,860 --> 00:58:19,120
remember the picture of vertical bars and horizontal bars

875
00:58:19,140 --> 00:58:21,640
OK so we have the set of all

876
00:58:21,690 --> 00:58:27,410
pictures of vertical bars on the the set of all pictures of horizontal bars and

877
00:58:27,410 --> 00:58:32,200
every picture of the vertical bars of point in high dimensional space and every picture

878
00:58:32,200 --> 00:58:36,910
of of horizontal bars of point in high dimensional space and so suppose i give

879
00:58:36,910 --> 00:58:39,530
you all these points

880
00:58:39,570 --> 00:58:41,230
OK i give you a whole

881
00:58:41,240 --> 00:58:42,680
a bunch of points

882
00:58:42,680 --> 00:58:46,480
some of those correspond to pictures of vertical bars and some of them correspond to

883
00:58:46,480 --> 00:58:48,370
pictures of horizontal bars in

884
00:58:48,400 --> 00:58:52,850
you don't know which is which is have a collection of points in high dimensional space

885
00:58:52,860 --> 00:58:57,230
and the only thing you know how to do is PCA

886
00:58:57,260 --> 00:58:59,000
and you apply PCA

887
00:58:59,020 --> 00:59:00,500
the picture on the right

888
00:59:00,530 --> 00:59:04,360
is the picture that he was c

889
00:59:07,910 --> 00:59:11,190
the reason you see the picture is as we just discussed

890
00:59:11,190 --> 00:59:12,870
the pick the set of all

891
00:59:12,890 --> 00:59:15,060
images of vertical bars

892
00:59:15,070 --> 00:59:18,610
i'm not a linear space the set of all images of horizontal bars are not

893
00:59:18,610 --> 00:59:20,810
a linear space in fact they are

894
00:59:21,570 --> 00:59:23,360
two-dimensional manifolds

895
00:59:23,360 --> 00:59:26,260
in the high high dimensional space

896
00:59:26,400 --> 00:59:33,180
and so trying to linearly fit something through it leads to disaster

897
00:59:35,390 --> 00:59:40,100
on the other hand if you applied plants and i can map stewart

898
00:59:40,110 --> 00:59:45,640
the picture in the middle is the picture that he was c

899
00:59:45,690 --> 00:59:49,230
you see two connected components in your data

900
00:59:49,280 --> 00:59:50,520
that you pick out

901
00:59:50,520 --> 00:59:54,320
and one connected component consists of all the blue curves

902
00:59:54,350 --> 00:59:59,150
and the other connected component blue dots in the other connected component consists of all

903
00:59:59,150 --> 01:00:03,560
the red dots red is not working on this particular version

904
01:00:03,560 --> 01:00:05,710
sort of finally get right so we

905
01:00:05,710 --> 01:00:11,560
we are considering these against six different features you know now we basically just go

906
01:00:11,580 --> 01:00:16,290
through it and we collect all the features of these con features that happen to

907
01:00:16,290 --> 01:00:18,940
be binary here because we don't have multiple

908
01:00:19,060 --> 01:00:22,370
events in the same sequence so basically you know

909
01:00:22,390 --> 01:00:25,670
x come field x come alive will have you know

910
01:00:25,670 --> 01:00:30,730
feature two three four and six active so we'll basically get this binary vector to

911
01:00:30,730 --> 01:00:33,920
represent that these are our first six features

912
01:00:33,940 --> 01:00:38,540
why prime has nothing active so it just be the zero vector

913
01:00:38,940 --> 01:00:42,440
why double prime you know has these three features act and the other three is

914
01:00:42,440 --> 01:00:48,410
not and the triple primarily has this first

915
01:00:49,890 --> 01:00:52,480
OK and and then basically

916
01:00:52,790 --> 01:00:56,920
just to give you a second example so you don't have only one example

917
01:01:01,670 --> 01:01:03,480
this one is useful

918
01:01:06,370 --> 01:01:10,290
all of these things cheaply or

919
01:01:12,150 --> 01:01:16,060
the goal of is all of which is land use during

920
01:01:19,330 --> 01:01:20,080
i mean

921
01:01:20,180 --> 01:01:26,040
so you should probably

922
01:01:27,390 --> 01:01:31,960
yeah definitely they they are talk right that's why why said right imagine that if

923
01:01:31,960 --> 01:01:36,350
you think is over over lunch break right coming up with these features so i

924
01:01:36,350 --> 01:01:40,980
think we need usually we need to distinguish i would say the features that just

925
01:01:40,980 --> 01:01:42,620
deal with the output part

926
01:01:42,640 --> 01:01:44,620
right and correlations there

927
01:01:44,640 --> 01:01:46,770
usually you might have

928
01:01:46,790 --> 01:01:51,230
we might have a domain knowledge that really tells you write what type of dependencies

929
01:01:51,230 --> 01:01:53,790
you want to capture what types of things to look for

930
01:01:53,810 --> 01:01:59,390
if we look at the input side right and how that correlates with with the

931
01:01:59,390 --> 01:02:03,790
outputs right so for instance which words are likely to occur within a particular segment

932
01:02:03,790 --> 01:02:06,790
right you could just take all possible worlds right that's why i said it could

933
01:02:06,790 --> 01:02:11,210
be like templates and then indeed you might need to use them

934
01:02:11,210 --> 01:02:16,180
feature induction of feature selection methods try it depending on how you actually training tried

935
01:02:16,180 --> 01:02:19,850
to basically prune back and limited to

936
01:02:19,870 --> 01:02:23,190
a smaller number of features but basically that

937
01:02:23,390 --> 01:02:26,850
so so here you know this is not the way

938
01:02:26,910 --> 01:02:32,170
as i said right you're not actually going through feature one by one so for

939
01:02:32,170 --> 01:02:36,270
instance you know exactly hand picking system and systems and so on and so forth

940
01:02:36,270 --> 01:02:40,690
right that's why i said this thing of the template right template that does that

941
01:02:40,710 --> 01:02:45,330
template would be a word occurring in the name of the three ties

942
01:02:45,350 --> 01:02:49,540
right this could be a template and maybe this for all words or maybe only

943
01:02:49,540 --> 01:02:53,770
four words that in your training data are frequent enough to actually show up there

944
01:02:53,770 --> 01:02:58,000
right so this would be a lot of pruning and

945
01:02:58,040 --> 01:03:02,600
a word occurring at the end of of of of the company name the tried

946
01:03:02,620 --> 01:03:05,230
the template might be that indeed

947
01:03:05,250 --> 01:03:08,370
you might want to have features that not just tell you in the name by

948
01:03:08,370 --> 01:03:13,540
the density and are at the beginning right because this can be important so so

949
01:03:13,540 --> 01:03:17,960
in that sense the template i think is designed manually and then use

950
01:03:17,980 --> 01:03:21,330
automatic techniques to prune it printed

951
01:03:21,350 --> 01:03:24,600
or in the example here

952
01:03:24,640 --> 01:03:26,440
so this is a

953
01:03:26,500 --> 01:03:28,960
this is for the passing case

954
01:03:29,020 --> 01:03:31,310
right where

955
01:03:31,770 --> 01:03:33,980
you can basically just take

956
01:03:34,000 --> 01:03:36,460
the problem the context free grammar

957
01:03:36,520 --> 01:03:38,910
with the rules that are given to you

958
01:03:38,920 --> 01:03:40,270
right and just

959
01:03:40,290 --> 01:03:46,040
directly translate that into a feature representation and then we want to do here is

960
01:03:46,040 --> 01:03:48,540
just you basically look at

961
01:03:48,600 --> 01:03:50,620
right so we need to

962
01:03:50,640 --> 01:03:52,810
we are given an x and we

963
01:03:53,520 --> 01:03:57,980
some y and we want to know what is the corresponding feature representation

964
01:03:58,100 --> 01:04:01,960
so what we could do is basically look at the number of times we've applied

965
01:04:01,960 --> 01:04:06,140
to certain productions so we've done as two NP VP

966
01:04:06,980 --> 01:04:11,060
so this would get a kind of one industry

967
01:04:11,080 --> 01:04:15,060
we would have a production rule let's say that says we would just over-priced that

968
01:04:15,060 --> 01:04:19,980
didn't apply so that the world is our production process that we're nounphrase

969
01:04:20,000 --> 01:04:22,310
goes becomes on

970
01:04:22,330 --> 01:04:25,770
OK so we see this here and here so we have twice

971
01:04:25,790 --> 01:04:29,750
and we have a verb phrase moving to the verb and the verb phrase so

972
01:04:29,750 --> 01:04:33,330
we see this one here and so on and so forth right so this is

973
01:04:33,330 --> 01:04:37,690
all the part that have to do with the production part against these are things

974
01:04:37,690 --> 01:04:40,770
that are independent of the important right they model

975
01:04:40,790 --> 01:04:45,080
like which types of trees are likely which types of productions like and then when

976
01:04:45,080 --> 01:04:48,420
you come to the level of where you actually

977
01:04:48,420 --> 01:04:49,920
go to the terminals

978
01:04:49,940 --> 01:04:55,770
here right the actual words that been with the representation becomes lexicalized phrase and probably

979
01:04:55,770 --> 01:04:59,370
much more dimensional because now you actually need to know

980
01:04:59,410 --> 01:05:01,140
right something like

981
01:05:01,150 --> 01:05:05,600
you know the probability to go from an on and replace it by some actual

982
01:05:05,600 --> 01:05:10,390
words like boeing right or have of seattle and so on and so forth right

983
01:05:10,480 --> 01:05:14,870
so you also can include here but again you can imagine that this is a

984
01:05:14,940 --> 01:05:19,540
a very high dimensional representation that you're getting their right

985
01:05:20,140 --> 01:05:21,810
the main idea

986
01:05:21,830 --> 01:05:25,810
so we know what this

987
01:05:29,190 --> 01:05:34,870
that is because these in my graph

988
01:05:34,890 --> 01:05:37,520
right so we will see later

989
01:05:37,540 --> 01:05:39,390
how that how the

990
01:05:39,640 --> 01:05:40,730
it comes in

991
01:05:40,790 --> 01:05:44,230
so here

992
01:05:44,230 --> 01:05:48,980
the the feature representation in principle you could just doing anything right it doesn't matter

993
01:05:50,690 --> 01:05:53,440
i mean but it makes sense or not right i guess here

994
01:05:53,460 --> 01:05:57,350
just the branching it just needs to work out such that you have a certain

995
01:05:57,350 --> 01:06:01,460
number of non terminals in the end that is really the length of the sentence

996
01:06:01,460 --> 01:06:03,150
that you're looking at right

997
01:06:03,170 --> 01:06:07,710
and and above is just you just need to apply the rules right but you

998
01:06:07,710 --> 01:06:11,560
could you could you know located you might think is an on and it was

999
01:06:11,560 --> 01:06:14,690
generated from an phrase or something like

1000
01:06:14,750 --> 01:06:19,390
so maybe not grammatically all of this of course is nonsensical but that's what you

1001
01:06:19,390 --> 01:06:21,310
want to look right what is

1002
01:06:21,330 --> 01:06:28,750
grammatical and what is not so so basically and if you allow right that

1003
01:06:28,830 --> 01:06:31,500
basically every

1004
01:06:31,580 --> 01:06:35,560
every word can really be you know if you if you have rules

1005
01:06:35,600 --> 01:06:39,100
we actually on the right-hand side here for the for the actual words right you

1006
01:06:39,100 --> 01:06:40,580
don't constrain it

1007
01:06:40,620 --> 01:06:43,150
then basically

1008
01:06:43,170 --> 01:06:46,520
you know if you have prior knowledge of course that you know that located is

1009
01:06:46,520 --> 01:06:49,540
you see that comes out is assigned sensitive

1010
01:06:49,600 --> 01:06:53,020
minus and pluses mind so i can do negative work

1011
01:06:53,060 --> 01:06:58,360
if the two don't have the same polarity

1012
01:06:58,410 --> 01:07:00,990
i want you to convince yourself

1013
01:07:01,860 --> 01:07:06,240
if i didn't come along the straight line from all the way from infinity

1014
01:07:06,290 --> 01:07:09,120
but i came in a very crooked way

1015
01:07:09,160 --> 01:07:11,720
finally ended up at point p

1016
01:07:11,770 --> 01:07:12,910
that point

1017
01:07:12,910 --> 01:07:16,980
that the amount of work that i have to do is exactly the same

1018
01:07:17,030 --> 01:07:21,650
you've seen the parallel with a one where we dealt with gravity

1019
01:07:21,680 --> 01:07:24,750
gravity is a conservative force

1020
01:07:24,850 --> 01:07:28,590
and when you do with conservative forces the work that has to be done in

1021
01:07:28,590 --> 01:07:32,400
going from one point to the other is independent of the path

1022
01:07:32,410 --> 01:07:34,850
that is the definition of conservative forces

1023
01:07:34,910 --> 01:07:37,390
electric force forces also conservative

1024
01:07:37,400 --> 01:07:39,280
so it doesn't make any difference

1025
01:07:39,290 --> 01:07:40,310
when i come

1026
01:07:40,340 --> 01:07:41,870
along the straight line

1027
01:07:41,890 --> 01:07:46,350
to this point whether i do that in an extremely crooked way

1028
01:07:46,350 --> 01:07:47,390
and and finally

1029
01:07:47,400 --> 01:07:48,420
and up here

1030
01:07:48,470 --> 01:07:52,230
that's the same amount of work

1031
01:07:52,310 --> 01:07:54,150
now if we do have a collection

1032
01:07:54,200 --> 01:07:55,560
of charges

1033
01:07:55,610 --> 01:07:58,660
so we have pluses and minuses charges

1034
01:07:58,720 --> 01:08:02,620
pluses and minuses classes minus classes pluses

1035
01:08:04,220 --> 01:08:04,850
you now

1036
01:08:04,860 --> 01:08:07,370
can calculate the amount of work

1037
01:08:07,410 --> 01:08:09,430
but i will tell you would have to do

1038
01:08:09,450 --> 01:08:13,480
in assembling that you bring one from infinity to you not to one another one

1039
01:08:13,770 --> 01:08:18,160
you add up all that work some work maybe positive some work may be negative

1040
01:08:19,220 --> 01:08:21,700
you arrive at the total amount of work

1041
01:08:21,730 --> 01:08:22,870
but you have to do

1042
01:08:22,900 --> 01:08:24,160
to assemble

1043
01:08:24,210 --> 01:08:26,330
these charges and that is the meaning

1044
01:08:26,350 --> 01:08:28,020
of capital

1045
01:08:30,240 --> 01:08:32,550
now i turn to

1046
01:08:32,590 --> 01:08:36,110
electric potential

1047
01:08:36,140 --> 01:08:37,910
and for that

1048
01:08:37,980 --> 01:08:40,270
i start off here

1049
01:08:40,340 --> 01:08:42,920
with a charge which i now call us

1050
01:08:42,930 --> 01:08:44,280
capital q

1051
01:08:44,310 --> 01:08:48,600
located here

1052
01:08:49,470 --> 01:08:51,060
at position p

1053
01:08:53,040 --> 01:08:54,410
edit distance

1054
01:08:54,470 --> 01:08:56,740
are way

1055
01:08:56,750 --> 01:08:58,400
i place

1056
01:08:58,470 --> 01:09:00,080
best charts

1057
01:09:00,100 --> 01:09:02,150
plus q

1058
01:09:02,230 --> 01:09:05,230
make it positive for now you can change it later

1059
01:09:05,300 --> 01:09:08,120
coming negative and so the

1060
01:09:08,170 --> 01:09:13,660
electrostatic potential energy we we know already we just calculated that would be q

1061
01:09:13,660 --> 01:09:14,870
times q

1062
01:09:14,990 --> 01:09:16,960
divided by four pi

1063
01:09:16,970 --> 01:09:18,310
actually non-zero

1064
01:09:19,220 --> 01:09:22,730
that's exactly the same way that we have

1065
01:09:22,740 --> 01:09:26,220
so we

1066
01:09:26,270 --> 01:09:29,970
electric potential electrostatic potential energy

1067
01:09:30,030 --> 01:09:33,150
is the work that i have to do

1068
01:09:34,420 --> 01:09:37,160
bring this charge here

1069
01:09:37,170 --> 01:09:39,100
now i'm going to introduce

1070
01:09:39,200 --> 01:09:41,730
electric potential

1071
01:09:43,860 --> 01:09:45,370
o council

1072
01:09:45,420 --> 01:09:51,230
and that is

1073
01:09:51,240 --> 01:09:54,670
the work per unit charge

1074
01:09:54,710 --> 01:09:56,490
but i have to do

1075
01:09:56,510 --> 01:09:58,450
to go from infinity

1076
01:09:58,460 --> 01:09:59,400
so that

1077
01:10:00,530 --> 01:10:03,240
so q doesn't enter into it anymore

1078
01:10:03,300 --> 01:10:06,390
it is the work

1079
01:10:07,850 --> 01:10:11,720
you know charge

1080
01:10:11,800 --> 01:10:13,560
to go from infinity

1081
01:10:13,570 --> 01:10:16,560
to let location

1082
01:10:16,570 --> 01:10:18,900
and so it is the work unit charge

1083
01:10:18,920 --> 01:10:20,820
is little q

1084
01:10:20,850 --> 01:10:22,300
this appears

1085
01:10:22,310 --> 01:10:26,810
so now we write down the at that location p

1086
01:10:26,850 --> 01:10:30,230
potential electric potential that location

1087
01:10:30,580 --> 01:10:32,270
is known need q

1088
01:10:32,300 --> 01:10:34,300
divided by four

1089
01:10:34,310 --> 01:10:35,870
epsilon zero i

1090
01:10:35,930 --> 01:10:40,070
the q has disappeared

1091
01:10:40,080 --> 01:10:42,850
it is also is scalar

1092
01:10:44,060 --> 01:10:46,690
as you know jewels

1093
01:10:46,700 --> 01:10:47,870
the unit here

1094
01:10:47,890 --> 01:10:51,090
is joules per coulomb i've divided out among charge

1095
01:10:51,100 --> 01:10:54,190
it worked for unit charge

1096
01:10:54,230 --> 01:10:56,060
no one would ever called these jewels

1097
01:10:56,080 --> 01:11:00,550
recall one call these vaults called after the grateful that with a lot of research

1098
01:11:00,550 --> 01:11:01,570
on this

1099
01:11:01,600 --> 01:11:05,010
we call this fault

1100
01:11:05,070 --> 01:11:06,230
but it's the same

1101
01:11:06,240 --> 01:11:07,340
as jules

1102
01:11:09,970 --> 01:11:13,520
if we have a very simple situation like we have here is that we only

1103
01:11:13,520 --> 01:11:15,970
have one charge

1104
01:11:16,010 --> 01:11:16,950
and this

1105
01:11:16,970 --> 01:11:18,440
is the potential

1106
01:11:19,420 --> 01:11:22,020
any distance you want from this charts

1107
01:11:22,030 --> 01:11:23,440
if i

1108
01:11:23,450 --> 01:11:25,430
goes up you further away

1109
01:11:25,440 --> 01:11:26,880
the potential

1110
01:11:26,930 --> 01:11:28,880
will become lower

1111
01:11:28,980 --> 01:11:31,120
this q is positive

1112
01:11:31,180 --> 01:11:33,750
the potential is everywhere in space

1113
01:11:34,820 --> 01:11:36,310
four singles chart

1114
01:11:36,440 --> 01:11:38,960
if this q is negative

1115
01:11:39,020 --> 01:11:42,020
everywhere in space the potential is negative

1116
01:11:42,030 --> 01:11:45,100
electoral elected static potential can be negative

1117
01:11:45,140 --> 01:11:46,630
the work that i do

1118
01:11:46,680 --> 01:11:50,890
the unit charge coming from infinity

1119
01:11:50,900 --> 01:11:54,250
the negative if that's the negative charge and

1120
01:11:54,310 --> 01:11:55,870
the potential

1121
01:11:55,880 --> 01:11:57,970
when i'm infinitely far away

1122
01:11:57,980 --> 01:12:00,860
but his arm becomes infinitely large

1123
01:12:03,050 --> 01:12:04,150
that's the way

1124
01:12:04,170 --> 01:12:07,680
we define our zero

1125
01:12:07,760 --> 01:12:13,450
so you can have positive potentials positive charge negative potentials and the negative charge

1126
01:12:13,490 --> 01:12:15,240
and if you very very far away

1127
01:12:16,500 --> 01:12:20,690
potential is zero

1128
01:12:20,700 --> 01:12:24,550
that's not true into our vendor graph

1129
01:12:24,560 --> 01:12:28,870
it's almost sphere

1130
01:12:28,930 --> 01:12:31,330
israelis are

1131
01:12:32,080 --> 01:12:34,400
about thirty centimetres

1132
01:12:34,430 --> 01:12:36,610
and i'm going to put on here

1133
01:12:36,630 --> 01:12:40,180
plus ten microkernel

1134
01:12:40,190 --> 01:12:44,270
it will distribute itself uniformly we will discuss the next time in detail

1135
01:12:44,300 --> 01:12:46,740
because it's a conductor

1136
01:12:46,810 --> 01:12:52,640
we already discussed last lecture that the electric field inside the sphere is zero

1137
01:12:52,740 --> 01:12:56,270
and that the electric field outside is not zero but we can think of all

1138
01:12:56,270 --> 01:13:00,250
the charges being at this point here

1139
01:13:00,450 --> 01:13:02,100
last stand

1140
01:13:02,180 --> 01:13:07,350
michael was all year as long as we want to know what electric field outside

1141
01:13:07,390 --> 01:13:10,600
so you can forget the fact that it is a sphere

1142
01:13:10,650 --> 01:13:12,420
so now i want to know

1143
01:13:12,470 --> 01:13:13,860
what the

1144
01:13:13,920 --> 01:13:15,890
electric potential is

1145
01:13:15,950 --> 01:13:19,020
at any point in space want know of this here

1146
01:13:19,080 --> 01:13:20,800
and i want to know what it is here

1147
01:13:20,850 --> 01:13:21,950
o point p

1148
01:13:21,960 --> 01:13:25,310
which is now distance are

1149
01:13:25,310 --> 01:13:26,880
when when you like

1150
01:13:28,040 --> 01:13:30,460
thousand different what the first

1151
01:13:30,530 --> 01:13:35,110
then you would need to compute for every test data point d compute housing can

1152
01:13:35,150 --> 01:13:36,860
and the newspapers

1153
01:13:36,860 --> 01:13:42,310
and i'm mainly talking about other people this competition

1154
01:13:42,320 --> 01:13:47,880
the main idea is that exploit the ten y of x five

1155
01:13:47,890 --> 01:13:52,220
OK and if he have a linear combination of higher taxes which are very high

1156
01:13:52,220 --> 01:13:58,180
dimensional and hide the linear combination is also going to be

1157
01:13:58,200 --> 01:14:02,930
OK so it's a linear combination of part back this again going to be sparse

1158
01:14:03,200 --> 01:14:06,420
if you not combining make

1159
01:14:06,470 --> 01:14:11,220
and that we approach is one things that i'm going to explain this

1160
01:14:11,270 --> 01:14:16,960
and also i think we try to every

1161
01:14:17,870 --> 01:14:19,110
OK so

1162
01:14:19,120 --> 01:14:21,360
maybe some general

1163
01:14:21,380 --> 01:14:23,580
so we

1164
01:14:23,630 --> 01:14:25,330
i assume that this

1165
01:14:25,340 --> 01:14:27,250
five x is very long

1166
01:14:27,270 --> 01:14:31,270
but the only thing which we need is

1167
01:14:31,290 --> 01:14:32,930
some operations which

1168
01:14:32,950 --> 01:14:35,600
x certain elements in the

1169
01:14:38,340 --> 01:14:40,300
we need some lookup operations

1170
01:14:40,350 --> 01:14:41,270
the base

1171
01:14:41,280 --> 01:14:47,710
may be very well so despite x is indexed by a certain substring so for

1172
01:14:47,710 --> 01:14:52,270
instance we have one element in this fire x for every subset what became

1173
01:14:52,330 --> 01:14:57,310
we can use it became a kind of address into the feature space

1174
01:14:57,350 --> 01:15:01,710
OK so now we have to do operations in the feature space

1175
01:15:01,770 --> 01:15:07,060
point you would like to change one element of the vector in feature space

1176
01:15:07,100 --> 01:15:11,370
so if you know there are only a few i mean very few non zero

1177
01:15:11,370 --> 01:15:16,210
element and we would like to modify these fuel elements and the idea is that

1178
01:15:16,210 --> 01:15:18,960
the only store elements which are nonzero

1179
01:15:18,980 --> 01:15:20,580
so we use

1180
01:15:20,580 --> 01:15:24,350
three ar what only the number of elements

1181
01:15:24,390 --> 01:15:29,650
and what we need to look at single element or to add something to an

1182
01:15:29,690 --> 01:15:32,640
twenty two operations between

1183
01:15:32,690 --> 01:15:38,150
it's very very efficient

1184
01:15:38,170 --> 01:15:40,940
OK two operations so

1185
01:15:40,980 --> 01:15:43,270
we have substring you would like to support

1186
01:15:43,330 --> 01:15:44,900
like the value

1187
01:15:44,900 --> 01:15:51,710
and you can use either stored or we can retrieve so and we've only

1188
01:15:51,730 --> 01:15:56,400
one great suffering we just need to store like where every substring

1189
01:15:56,720 --> 01:15:59,290
the way i mean it's almost

1190
01:15:59,310 --> 01:16:06,520
so the important point is that we can using these operations we can precompute lighting

1191
01:16:06,560 --> 01:16:11,920
linear combination of these i o x i so given a set of training examples

1192
01:16:12,100 --> 01:16:15,100
can put it the community

1193
01:16:15,140 --> 01:16:18,480
it just needed a perspective drawing the vector

1194
01:16:18,500 --> 01:16:23,060
and if we you have that can be computed in your community

1195
01:16:23,900 --> 01:16:28,140
this operation is to look at all substrings

1196
01:16:28,580 --> 01:16:30,500
the six x

1197
01:16:30,500 --> 01:16:33,500
and sum up all the elements in b

1198
01:16:34,210 --> 01:16:35,900
it would have been

1199
01:16:35,960 --> 01:16:37,190
and we take

1200
01:16:37,230 --> 01:16:39,640
we've looked into the back of the year

1201
01:16:39,850 --> 01:16:41,620
in that moment c

1202
01:16:41,730 --> 01:16:44,400
but this is something

1203
01:16:44,400 --> 01:16:48,330
if is a non-zero value

1204
01:16:50,150 --> 01:16:51,460
so this

1205
01:17:02,250 --> 01:17:05,100
so now the question is how can we do these two operations

1206
01:17:05,170 --> 01:17:07,750
look up and the adding operations

1207
01:17:07,830 --> 01:17:10,020
i don't have any suggestions

1208
01:17:10,120 --> 01:17:17,020
explicit maps you can just take like all the pain that you just have map

1209
01:17:17,020 --> 01:17:19,810
which is as big as the number of him

1210
01:17:20,420 --> 01:17:21,710
so it

1211
01:17:21,750 --> 01:17:25,040
sigma small k k is small

1212
01:17:25,060 --> 01:17:28,960
then this is feasible to have this much memory allocated

1213
01:17:28,960 --> 01:17:30,260
you're fairly neutral about them

1214
01:17:31,320 --> 01:17:32,700
ten u love them dearly

1215
01:17:33,120 --> 01:17:37,060
and when you plot these two against each other you get this negative relationship which

1216
01:17:37,060 --> 01:17:42,330
is extremely robust every single dataset we have we pick the same relationship of those

1217
01:17:42,330 --> 01:17:44,420
who love me still you see most of them

1218
01:17:45,440 --> 01:17:48,760
but if you're gonna look eyeball those data there's a sense that which

1219
01:17:49,380 --> 01:17:50,560
perhaps there are layers

1220
01:17:52,670 --> 01:17:53,470
and indeed when we

1221
01:17:53,920 --> 01:17:54,290
did some

1222
01:17:55,150 --> 01:17:57,040
serious cranking on these data

1223
01:17:58,250 --> 01:18:02,700
that's what we were able to show so these this is a really using the massive fractals

1224
01:18:03,160 --> 01:18:05,420
to look for a recurring patterns in the data

1225
01:18:06,510 --> 01:18:08,180
on the left is are

1226
01:18:08,820 --> 01:18:09,750
ethnographic social

1227
01:18:10,420 --> 01:18:14,880
groupings database census data based on the right down here is all

1228
01:18:15,380 --> 01:18:16,500
christmas card database

1229
01:18:17,780 --> 01:18:18,500
and the key

1230
01:18:18,970 --> 01:18:21,040
the issue is is that the peak here

1231
01:18:21,580 --> 01:18:25,880
that's the the fractal parameter the rest the harmonics and they're kind of interesting in some sense

1232
01:18:26,320 --> 01:18:27,810
but really just harmonics about

1233
01:18:28,520 --> 01:18:29,460
the position and those

1234
01:18:29,950 --> 01:18:34,730
the fact chromosome both these datasets in exactly the same place is almost exactly three

1235
01:18:35,640 --> 01:18:37,620
well that's telling you this is a recurring pattern

1236
01:18:38,870 --> 01:18:40,840
in the scaling ratio three

1237
01:18:41,280 --> 01:18:44,920
in these datasets and this is picked up the latest study by somebody else

1238
01:18:45,930 --> 01:18:49,030
question that it looked at and hunter-gatherer database

1239
01:18:50,600 --> 01:18:54,260
looking at the structural groupings and then they used an alternative

1240
01:18:54,680 --> 01:18:58,420
much simpler we had some astrophysicist to the masses so that

1241
01:18:58,890 --> 01:19:00,290
i think the most complicated way

1242
01:19:00,820 --> 01:19:01,450
actually do it

1243
01:19:01,920 --> 01:19:06,830
they was jumping anthropologists actually markets hamilton i think that the santa fe institute now

1244
01:19:07,880 --> 01:19:11,740
they is taught in order analysis which is very simple because it just simple graphical

1245
01:19:11,740 --> 01:19:13,400
techniques as the slope on the line

1246
01:19:14,070 --> 01:19:15,000
there is they

1247
01:19:15,480 --> 01:19:19,140
scaling ratio they came up with the scale ratio is about three point eight

1248
01:19:19,640 --> 01:19:22,810
but in fact they only get three point eight because they included individual

1249
01:19:23,270 --> 01:19:24,500
as a group so one

1250
01:19:25,040 --> 01:19:27,330
on the left hand side is the first warden order

1251
01:19:29,030 --> 01:19:30,670
if you exclude at

1252
01:19:31,490 --> 01:19:34,420
and just start the true group if you like which for them as a family

1253
01:19:35,360 --> 01:19:40,740
what having will be individual on the left-hand side sister paula slowed down

1254
01:19:41,390 --> 01:19:46,380
if you ignore the slight tilt slightly negative slope almost exactly three point two after this as well

1255
01:19:47,820 --> 01:19:48,690
there we've yet to tell

1256
01:19:50,590 --> 01:19:53,220
they're sold on the idea that the scaling which is actually four

1257
01:19:54,640 --> 01:19:58,920
but it's not and we pick up the same scalar ratio in species battles that

1258
01:19:58,920 --> 01:20:02,270
have multilevel social systems as well so it's quite universal

1259
01:20:02,730 --> 01:20:06,420
but what it means for use that's what your social world looks like they the

1260
01:20:06,420 --> 01:20:10,930
centre again you're surrounded by a series a sort of layers so ripples on the

1261
01:20:10,930 --> 01:20:11,790
pond if you like

1262
01:20:12,260 --> 01:20:15,880
all relationships are different quality as you go out through these layers

1263
01:20:16,380 --> 01:20:17,600
the number people included

1264
01:20:18,070 --> 01:20:19,470
increases but the quality

1265
01:20:19,960 --> 01:20:25,690
emotional intensity relationship declines issue these lattices inclusive right the fifteen increases

1266
01:20:26,170 --> 01:20:28,200
o five fifty include fifteen so

1267
01:20:29,680 --> 01:20:34,610
we know they go right out fifteen hundred it goes beyond a hundred and fifty we can pick the signature

1268
01:20:35,180 --> 01:20:36,820
as far out as a hundred and fifty

1269
01:20:38,200 --> 01:20:41,060
fifteen hundred very but there's a big difference between

1270
01:20:42,540 --> 01:20:46,870
the people inside a hundred and fifty people outside people inside and fifty people have

1271
01:20:46,940 --> 01:20:50,220
real relationships with have relationships which have a history

1272
01:20:50,750 --> 01:20:53,520
relationships have obligation and trust and reciprocity

1273
01:20:54,230 --> 01:20:57,630
outside a hundred and fifty there essentially quite acquaintances

1274
01:20:58,240 --> 01:21:04,610
so you're most acquaintances probably in this out a five hundred let the fifteen hundred layers for people you know

1275
01:21:06,180 --> 01:21:08,460
but they don't know who u so i guess

1276
01:21:09,080 --> 01:21:11,510
the most diverse president obama said about here

1277
01:21:13,030 --> 01:21:14,480
i mean if you lost him

1278
01:21:15,540 --> 01:21:16,710
drop round arabia

1279
01:21:17,110 --> 01:21:17,980
on saturday night

1280
01:21:19,490 --> 01:21:21,680
first all he would look surprised second level

1281
01:21:22,370 --> 01:21:24,820
very large gentleman with bold gender here

1282
01:21:26,810 --> 01:21:27,650
but come and talk to you

1283
01:21:29,310 --> 01:21:32,250
so are people out there that we know by site that appears to be

1284
01:21:33,070 --> 01:21:37,660
the limit on the number people's faces that we can recognise that we can faces

1285
01:21:37,660 --> 01:21:40,350
closest to this

1286
01:21:40,390 --> 01:21:41,960
line to this model

1287
01:21:41,970 --> 01:21:47,650
i ask as well what about these not say OK it's red thing

1288
01:21:47,670 --> 01:21:49,510
threats OK

1289
01:21:49,550 --> 01:21:50,460
and then

1290
01:21:51,190 --> 01:21:55,160
we rebuild the model and we get the lines dividing line

1291
01:21:55,330 --> 01:21:57,220
a little bit higher

1292
01:21:57,230 --> 01:22:01,530
because it's on one side of the red one the other side was it is

1293
01:22:01,530 --> 01:22:03,830
the ones which are not we know about

1294
01:22:03,840 --> 01:22:05,730
again the machine selects

1295
01:22:05,750 --> 01:22:07,350
the question which is

1296
01:22:07,500 --> 01:22:09,980
close to the small planets

1297
01:22:10,090 --> 01:22:11,600
can read

1298
01:22:11,640 --> 01:22:13,290
and then

1299
01:22:13,300 --> 01:22:18,220
questions here

1300
01:22:18,240 --> 01:22:23,170
and would say so that there was one one so still and we have aligned

1301
01:22:23,170 --> 01:22:28,650
their on one side of the red one stands on the site only once and

1302
01:22:28,750 --> 01:22:31,130
so if they continue

1303
01:22:31,140 --> 01:22:34,850
so this continuous after twenty questions

1304
01:22:38,020 --> 01:22:39,210
hundreds so

1305
01:22:39,230 --> 01:22:45,410
one is one hundred questions and roughly the line but it was impossible somehow tool

1306
01:22:45,420 --> 01:22:47,160
tool of

1307
01:22:47,510 --> 01:22:51,930
distinguish of to me to build the straight line which would divide rule from red

1308
01:22:51,930 --> 01:22:56,650
ones but still blind more less converge here and now we have a model

1309
01:22:56,670 --> 01:23:00,670
say well everything what's on this site is most probably rats

1310
01:23:00,690 --> 01:23:06,030
of red color and everything was on this site is probably look all around

1311
01:23:06,050 --> 01:23:07,280
this is the

1312
01:23:07,300 --> 01:23:09,620
finally the final result

1313
01:23:09,640 --> 01:23:14,680
this is roughly the idea of what this active learning cycle thus

1314
01:23:14,700 --> 01:23:19,840
and so this is kind of that you can imagine instead of this maybe thousands

1315
01:23:19,890 --> 01:23:22,930
o point because a hundred thousand points and

1316
01:23:22,950 --> 01:23:29,820
maybe not just two dimensions hundred thousand dimensions and this is the process which can

1317
01:23:29,820 --> 01:23:33,040
save you a lot of money are is a lot of resources

1318
01:23:33,050 --> 01:23:36,510
when you're trying to take for label are

1319
01:23:38,240 --> 01:23:43,480
the query documents in whatever way

1320
01:23:45,690 --> 01:23:50,270
i have one more demo forgot to show it before which i think it's

1321
01:23:50,340 --> 01:23:53,810
also quite a good demonstration of

1322
01:23:58,440 --> 01:24:00,010
what learning does

1323
01:24:01,110 --> 01:24:06,340
before i mentioned is support vector machines and also for support vector machines

1324
01:24:07,650 --> 01:24:12,700
probably the most popular machine learning methods today because it has

1325
01:24:16,670 --> 01:24:21,500
and so it's so it's quite effective when it's

1326
01:24:21,520 --> 01:24:23,960
it is very hard to beat it

1327
01:24:24,010 --> 01:24:27,650
so this is the them all of

1328
01:24:28,220 --> 01:24:31,230
and how this method works

1329
01:24:32,680 --> 01:24:38,930
scenario the following again here we operate in two dimensions normally operate in

1330
01:24:38,950 --> 01:24:42,540
thousand or ten thousand or even more

1331
01:24:42,550 --> 01:24:47,080
but again algorithm is the same the whole situation is the same so this scenario

1332
01:24:47,080 --> 01:24:49,040
is the following so here we have

1333
01:24:50,040 --> 01:24:56,660
and so we have minds

1334
01:24:56,680 --> 01:24:58,830
and now we would like to

1335
01:24:58,840 --> 01:24:59,980
with the model

1336
01:25:00,000 --> 01:25:03,530
in this case the model is just a straight line

1337
01:25:03,570 --> 01:25:04,680
we see lights

1338
01:25:04,690 --> 01:25:08,220
this red ones from the ones

1339
01:25:08,230 --> 01:25:11,660
i think so in this case this was pretty easy because

1340
01:25:11,670 --> 01:25:12,920
we separate the them

1341
01:25:14,200 --> 01:25:16,070
now its inputs

1342
01:25:16,160 --> 01:25:20,410
a few more minor like this now and say well

1343
01:25:20,460 --> 01:25:26,070
right now it still this was pretty but let's see if we are evil

1344
01:25:29,230 --> 01:25:31,820
now we got confused already

1345
01:25:31,830 --> 01:25:35,800
what happened now

1346
01:25:35,850 --> 01:25:41,360
OK computer was really a lot i mean i know it's already confused because the

1347
01:25:41,360 --> 01:25:42,980
language is simply too

1348
01:25:42,990 --> 01:25:47,060
two simple i mean the language to express the model is to see how we

1349
01:25:47,060 --> 01:25:48,970
can increase the

1350
01:25:49,010 --> 01:25:53,520
power of the language expressivity of the language that will use the polynomial

1351
01:25:53,610 --> 01:25:57,890
languages instead of just a straight lines we will use the

1352
01:25:57,930 --> 01:26:01,080
well only polynomial curves

1353
01:26:01,100 --> 01:26:02,500
and while

1354
01:26:02,520 --> 01:26:04,270
this helped to bit

1355
01:26:04,290 --> 01:26:06,400
still i mean some

1356
01:26:06,410 --> 01:26:10,240
classes are some blue blue ones are on the

1357
01:26:10,260 --> 01:26:14,050
on the site where the red should be about small as it was able to

1358
01:26:14,050 --> 01:26:18,490
isolate reference from one pretty efficient

1359
01:26:18,540 --> 01:26:21,340
they can still be a little bit more

1360
01:26:24,830 --> 01:26:26,020
well mean

1361
01:26:26,060 --> 01:26:30,530
good job we can support some here

1362
01:26:30,550 --> 01:26:31,470
now it's

1363
01:26:31,490 --> 01:26:33,030
it's confused

1364
01:26:33,040 --> 01:26:34,920
but now we can increase the

1365
01:26:34,970 --> 01:26:39,850
the strength of the language in use this gulf kernels kernels are

1366
01:26:39,900 --> 01:26:41,120
just the way

1367
01:26:48,940 --> 01:26:52,360
how we represent internally the

1368
01:26:52,380 --> 01:26:53,490
space of the

1369
01:26:53,500 --> 01:26:58,610
possible concepts i wanna go too much into the details just the the

1370
01:26:58,620 --> 01:27:00,250
a different kind as we can

1371
01:27:00,300 --> 01:27:04,120
we can bring different optics

1372
01:27:04,130 --> 01:27:06,780
different optics into this high dimensional space

1373
01:27:09,140 --> 01:27:14,620
well this one was pretty good so this is basically a red ones are on

1374
01:27:16,310 --> 01:27:18,090
light sites and the

1375
01:27:18,100 --> 01:27:19,840
red ones are

1376
01:27:19,850 --> 01:27:22,330
and the blue ones are more less the dark side

1377
01:27:22,340 --> 01:27:27,230
let's make a a show a little bit more

1378
01:27:31,780 --> 01:27:33,200
this is

1379
01:27:33,290 --> 01:27:42,390
one typical even example which

1380
01:27:43,000 --> 01:27:47,450
obviously linear kernel cannot do much

1381
01:27:48,580 --> 01:27:53,400
also not much i think it's just something here

1382
01:27:53,400 --> 01:27:57,070
in your mission model based

1383
01:28:00,500 --> 01:28:03,420
if you have a partially constructed tree like this

1384
01:28:03,460 --> 01:28:05,070
so you have much

1385
01:28:05,150 --> 01:28:10,960
this data points into some cluster which those data points into some and also for

1386
01:28:11,070 --> 01:28:12,110
as well

1387
01:28:12,130 --> 01:28:15,190
what the what the model is saying is that

1388
01:28:15,190 --> 01:28:19,170
the data points within each connected component here

1389
01:28:19,190 --> 01:28:21,800
his model with a single gossip

1390
01:28:21,860 --> 01:28:23,170
so this

1391
01:28:23,230 --> 01:28:28,770
and of course the single girls is into is a very simple model probabilistic model

1392
01:28:28,770 --> 01:28:31,500
is simply a spherical shape

1393
01:28:33,820 --> 01:28:37,920
helen ghahramani zoubin is going to give a talk more but i'm not sure there

1394
01:28:38,290 --> 01:28:40,860
this thing but i really like model

1395
01:28:42,210 --> 01:28:47,440
what they say is that instead of assuming that each of the constructed subtrees here

1396
01:28:47,460 --> 01:28:49,190
is modeled by a single garcia

1397
01:28:49,210 --> 01:28:54,440
each other subtrees is in fact the corresponding itself to make sure model

1398
01:28:55,630 --> 01:29:00,130
for example this subtree right so has lead a b c and d

1399
01:29:00,130 --> 01:29:01,770
it's going to be model with

1400
01:29:01,800 --> 01:29:03,020
make sure

1401
01:29:04,060 --> 01:29:08,770
in one component all the data points belong to one calcium

1402
01:29:09,070 --> 01:29:13,070
and in another component is to data points belong to one goes in and those

1403
01:29:13,070 --> 01:29:15,400
two data points belong to one thousand

1404
01:29:15,460 --> 01:29:19,610
and then in another component this data point belongs to his own gaussians

1405
01:29:19,630 --> 01:29:21,710
the losses on goes

1406
01:29:21,710 --> 01:29:22,880
c and d

1407
01:29:22,880 --> 01:29:24,730
how much together

1408
01:29:24,820 --> 01:29:28,360
so this is what's called tree consistent partitions

1409
01:29:28,400 --> 01:29:29,960
and what this is basically

1410
01:29:29,980 --> 01:29:31,670
if you take this tree

1411
01:29:31,710 --> 01:29:33,250
and you come back

1412
01:29:33,290 --> 01:29:35,320
cut the line to this thing

1413
01:29:35,340 --> 01:29:40,880
then you get a one of this partitions of the data

1414
01:29:40,920 --> 01:29:46,790
we have or want to go into details because it's quite mathematical

1415
01:29:46,800 --> 01:29:49,400
but it turns out that the algorithm is

1416
01:29:49,440 --> 01:29:51,170
this is very similar

1417
01:29:51,170 --> 01:29:54,290
the probability of the data under a subtree

1418
01:29:56,290 --> 01:30:01,040
can still be computed recursively is not going to be the same as as before

1419
01:30:01,560 --> 01:30:04,590
in fact the probability of a subtree here

1420
01:30:04,650 --> 01:30:05,940
it's going to be

1421
01:30:05,960 --> 01:30:09,800
the sum of two terms one is the probability of the individual subtrees

1422
01:30:09,820 --> 01:30:13,070
multiply together with multiplied by some factor

1423
01:30:13,090 --> 01:30:16,500
and the other one the problem is the probability of

1424
01:30:16,560 --> 01:30:18,670
all the data points and both

1425
01:30:21,480 --> 01:30:27,560
and also this approach can be used to obtain a lower bound on the probability

1426
01:30:27,560 --> 01:30:29,790
of the dirichlet process mixture

1427
01:30:29,860 --> 01:30:31,940
so this just that

1428
01:30:32,000 --> 01:30:34,750
a throwaway statement

1429
01:30:37,460 --> 01:30:39,750
what are the pros and cons of probabilistic

1430
01:30:39,790 --> 01:30:41,440
hierarchical clustering

1431
01:30:41,500 --> 01:30:44,360
it is still in fact very easy and efficient

1432
01:30:45,440 --> 01:30:47,590
to implement as small so the algorithms

1433
01:30:47,610 --> 01:30:48,570
given to you

1434
01:30:49,440 --> 01:30:54,020
so the question is how do you define this problem probability of the much cluster

1435
01:30:54,040 --> 01:30:58,400
this is the probability of the individual cost

1436
01:30:58,730 --> 01:31:01,380
and you can compute the probability is efficiently

1437
01:31:01,400 --> 01:31:02,650
because you can

1438
01:31:02,690 --> 01:31:06,380
compute the inner because the fashion as well

1439
01:31:06,380 --> 01:31:08,110
it really

1440
01:31:08,110 --> 01:31:13,420
is basically the same framework as normal linkage algorithms that they use

1441
01:31:13,440 --> 01:31:14,590
to tell you about

1442
01:31:15,630 --> 01:31:17,000
in this sense is still

1443
01:31:17,000 --> 01:31:19,020
easy to describe this

1444
01:31:19,040 --> 01:31:22,040
the output of this algorithms to people because

1445
01:31:22,060 --> 01:31:25,670
it has to understand english english

1446
01:31:26,110 --> 01:31:32,440
so the probabilistic models are more into interpretable because it's basically gives generative process

1447
01:31:32,480 --> 01:31:34,110
and a tree

1448
01:31:34,110 --> 01:31:37,070
with high probability is simply

1449
01:31:39,210 --> 01:31:41,730
so basically the trees which

1450
01:31:42,130 --> 01:31:44,460
the algorithm returns to you

1451
01:31:44,490 --> 01:31:48,400
try to maximize the probably the probability of the data

1452
01:31:48,460 --> 01:31:52,730
but unfortunately

1453
01:31:52,750 --> 01:31:55,860
because this thing has to be probabilistic models

1454
01:31:56,420 --> 01:31:58,460
the in fact less flexible than

1455
01:31:58,570 --> 01:32:03,670
then distance metrics you can define any distance perfectly u one and u

1456
01:32:05,170 --> 01:32:09,250
but in terms of probabilistic models is important that you actually write down the probability

1457
01:32:09,250 --> 01:32:11,420
distribution has to sum to one

1458
01:32:11,440 --> 01:32:12,820
over all possible

1459
01:32:14,880 --> 01:32:17,500
so that gives you constraints

1460
01:32:17,650 --> 01:32:21,360
it also deals nicely with partially observed data because

1461
01:32:22,000 --> 01:32:25,900
the probability of the of the data

1462
01:32:26,210 --> 01:32:31,020
this problem this can be computed even if we don't observe

1463
01:32:31,020 --> 01:32:35,460
every entry in the data vector

1464
01:32:35,650 --> 01:32:40,060
and there is a coherent measure of goodness of fit for resulting

1465
01:32:40,110 --> 01:32:41,570
for the resulting tree

1466
01:32:42,150 --> 01:32:45,630
but there still no notion of uncertainty because it's still again

1467
01:32:47,290 --> 01:32:50,250
if you if you give it a data set it only returns to one single

1468
01:32:50,250 --> 01:32:54,270
tree it doesn't really tell you how much they actually the truth

1469
01:32:54,290 --> 01:32:56,960
is the is the true true

1470
01:32:57,000 --> 01:32:58,130
the true

1471
01:33:02,750 --> 01:33:07,340
just to reiterate what i said in the beginning there this in fact to these

1472
01:33:08,540 --> 01:33:10,440
ways in which you could believe

1473
01:33:10,440 --> 01:33:11,900
the at the

1474
01:33:11,900 --> 01:33:15,880
the there is in fact two distinct ways in which you could believe

1475
01:33:16,460 --> 01:33:17,590
the data is

1476
01:33:17,610 --> 01:33:21,730
generated because there's two possible generative processes for data

1477
01:33:21,750 --> 01:33:25,940
the first one is that we believe that the the data comes from

1478
01:33:27,520 --> 01:33:31,340
groups of sources so this is the mixture modeling approach

1479
01:33:31,440 --> 01:33:35,720
where you are in fact believe that the data points is heterogeneous has comes from

1480
01:33:35,720 --> 01:33:40,730
multiple groups within each group it's very simply doesn't have any internal structure analysis

1481
01:33:40,840 --> 01:33:43,650
one colour symbolism

1482
01:33:43,650 --> 01:33:48,880
and in this up in this approach we simply use hierarchical clustering as an efficient

1483
01:33:48,900 --> 01:33:50,960
so chris search procedure

1484
01:33:51,020 --> 01:33:52,630
to find a good way of

1485
01:33:52,650 --> 01:33:55,270
partitioning of the a into different groups

1486
01:33:55,340 --> 01:33:57,840
different sources

1487
01:33:58,230 --> 01:34:01,880
the other belief is that we might believe that the data in fact has an

1488
01:34:01,880 --> 01:34:03,320
underlying tree structure

1489
01:34:03,380 --> 01:34:06,980
so this is the case for example in in front of genetics

1490
01:34:07,040 --> 01:34:09,940
and i linguistics as well

1491
01:34:09,980 --> 01:34:11,190
so in this case

1492
01:34:11,230 --> 01:34:15,440
we want to use hierarchical clustering to in fact find the tree

1493
01:34:21,770 --> 01:34:24,540
the following algorithms out a about

1494
01:34:24,570 --> 01:34:27,730
so the previous algorithm which after about all

1495
01:34:29,440 --> 01:34:33,270
as the underlying model a mixture model so it in fact

1496
01:34:33,290 --> 01:34:36,380
corresponds to that particular to that belief

1497
01:34:36,440 --> 01:34:39,110
well the two algorithms which i tell you about

1498
01:34:39,170 --> 01:34:41,460
not too against the

1499
01:34:41,460 --> 01:34:43,880
one framework which i tell you what next

1500
01:34:43,880 --> 01:34:44,920
in fact

1501
01:34:45,110 --> 01:34:48,210
it follows the second

1502
01:34:48,280 --> 01:34:51,880
a set of beliefs that we believe that there is an underlying tree

1503
01:34:51,920 --> 01:34:55,520
and we want to find an underlying tree

1504
01:34:57,960 --> 01:34:59,710
we do believe that

1505
01:34:59,710 --> 01:35:03,570
OK so these

1506
01:35:03,680 --> 01:35:04,980
after the

1507
01:35:05,040 --> 01:35:11,240
no i meant specific point from the mouth time and that's the say that now

1508
01:35:11,320 --> 01:35:15,940
it's actually all on that problem he is given that each of those methods

1509
01:35:15,950 --> 01:35:22,410
supplement what does it mean that fact that it's going to some extent also practical

1510
01:35:22,430 --> 01:35:25,140
this is it possible

1511
01:35:25,160 --> 01:35:28,440
one the problem still had

1512
01:35:28,500 --> 01:35:31,400
low additional cost

1513
01:35:31,510 --> 01:35:37,740
perhaps so you could say that it is something which is one of the officials

1514
01:35:37,790 --> 01:35:40,410
but there is

1515
01:35:40,460 --> 01:35:42,320
the important point that

1516
01:35:42,380 --> 01:35:43,950
and you

1517
01:35:44,360 --> 01:35:48,460
what what the accuracy is that it is influenced by many many more

1518
01:35:48,470 --> 01:35:50,550
but there's sense of your

1519
01:35:50,570 --> 01:35:52,580
it's not feasible sets c

1520
01:35:52,590 --> 01:35:54,540
you may

1521
01:35:54,580 --> 01:35:57,280
one of the objective or

1522
01:35:57,300 --> 01:36:00,680
first part of the brain

1523
01:36:00,730 --> 01:36:05,310
in polynomial time and that's all those twenty is at

1524
01:36:05,430 --> 01:36:08,520
the company but there

1525
01:36:08,600 --> 01:36:09,980
that was the you

1526
01:36:09,990 --> 01:36:13,240
for much of the USL me my

1527
01:36:13,320 --> 01:36:19,390
functional level one or both of the policy

1528
01:36:19,440 --> 01:36:20,970
four methods

1529
01:36:21,020 --> 01:36:24,560
a lot but none of them so the

1530
01:36:24,630 --> 01:36:30,110
a most of the laws of the state of the art

1531
01:36:30,700 --> 01:36:36,060
that the actual accuracy while whatever located in percent of the optimal way

1532
01:36:37,000 --> 01:36:40,720
they are five percent of the optimal some of these

1533
01:36:40,730 --> 01:36:46,990
when you look like property will be made available on the same side of

1534
01:36:47,040 --> 01:36:49,220
it was a lot of course

1535
01:36:49,560 --> 01:36:51,640
the normalisation

1536
01:36:51,650 --> 01:36:56,550
is a normalisation of the show absolute purity right

1537
01:36:56,650 --> 01:36:59,870
the addition of europe for the

1538
01:37:03,010 --> 01:37:06,970
and was about you know absolute accuracy

1539
01:37:07,050 --> 01:37:15,030
it is look for is like the parliament classical optimal itself with the one you

1540
01:37:16,760 --> 01:37:19,760
the minus one but when you

1541
01:37:19,870 --> 01:37:25,210
normalized by the world looking for the normalized by the size of the department of

1542
01:37:25,210 --> 01:37:29,350
the size of the additional that continued world

1543
01:37:29,390 --> 01:37:31,600
for each u i mean when

1544
01:37:33,180 --> 01:37:36,520
doesn't actually want

1545
01:37:38,180 --> 01:37:42,620
the what the problem is going to be able to

1546
01:37:42,750 --> 01:37:44,510
and we'll see

1547
01:37:44,610 --> 01:37:48,260
OK so i put some

1548
01:37:48,360 --> 01:37:50,110
the first of those

1549
01:37:50,150 --> 01:37:52,490
so the goal

1550
01:37:52,580 --> 01:37:54,410
but there is a course for

1551
01:37:55,040 --> 01:37:57,760
methods in the first is that the

1552
01:37:58,580 --> 01:38:04,150
the question of what they sold by that point out the

1553
01:38:04,630 --> 01:38:08,860
problem when decision variables it was the

1554
01:38:08,960 --> 01:38:13,030
the idea was the first thing that came up with the u

1555
01:38:13,160 --> 01:38:14,800
the design dimension

1556
01:38:14,840 --> 01:38:18,630
unless you're being the past

1557
01:38:18,740 --> 01:38:21,730
one of

1558
01:38:21,790 --> 01:38:24,690
now the last thing you get

1559
01:38:24,930 --> 01:38:29,030
code problems with this is why

1560
01:38:29,080 --> 01:38:32,530
and that appointment book holding up

1561
01:38:34,270 --> 01:38:39,850
as of thousands of variables some you know

1562
01:38:39,900 --> 01:38:40,960
you know

1563
01:38:41,000 --> 01:38:42,840
variables and constraints

1564
01:38:42,850 --> 01:38:48,140
but but in typical only upon the province's precedence

1565
01:38:48,190 --> 01:38:49,900
the you first use

1566
01:38:49,910 --> 01:38:52,220
usually they also the

1567
01:38:52,230 --> 01:38:58,280
so that's a problem of the most of the people that lot of

1568
01:38:58,380 --> 01:39:00,240
one thousand your system

1569
01:39:05,960 --> 01:39:09,460
o uses and in addition is like the form

1570
01:39:09,720 --> 01:39:14,870
the this to all the most of the work that comes with all

1571
01:39:14,890 --> 01:39:20,020
the practical and a few hundred thousand of them

1572
01:39:20,140 --> 01:39:22,200
one the problem of

1573
01:39:22,250 --> 01:39:23,550
actually can

1574
01:39:23,630 --> 01:39:25,240
forget about

1575
01:39:25,820 --> 01:39:27,960
methods of a

1576
01:39:27,970 --> 01:39:31,070
but that his

1577
01:39:32,870 --> 01:39:35,080
really large scale problems

1578
01:39:35,090 --> 01:39:38,090
one of the last is

1579
01:39:38,140 --> 01:39:41,480
a cheap method

1580
01:39:41,530 --> 01:39:43,030
most of that

1581
01:39:43,080 --> 01:39:45,410
is it proper

1582
01:39:45,480 --> 01:39:47,590
there's an emotion

1583
01:39:47,630 --> 01:39:52,940
and what is the name of our lord the this it is the sum of

1584
01:39:53,020 --> 01:39:57,780
all of what one of the oldest

1585
01:39:58,040 --> 01:40:00,340
that is not good enough

1586
01:40:00,350 --> 01:40:01,750
and there could be

1587
01:40:01,760 --> 01:40:02,840
that what

1588
01:40:02,850 --> 01:40:05,780
and this he

1589
01:40:05,790 --> 01:40:07,550
but in one

1590
01:40:07,560 --> 01:40:08,390
but the

1591
01:40:08,440 --> 01:40:11,570
that at all

1592
01:40:11,590 --> 01:40:15,480
and if you are interested in more small organisation

1593
01:40:15,620 --> 01:40:19,230
OK not allow the

1594
01:40:19,610 --> 01:40:25,380
but i don't think this is you were a kid let's let's see what happens

1595
01:40:25,430 --> 01:40:29,830
the what's the matter so much of this state

1596
01:40:29,840 --> 01:40:31,360
but if you

1597
01:40:32,220 --> 01:40:35,480
i think in objective as a whole

1598
01:40:35,490 --> 01:40:38,890
or you thousand

1599
01:40:39,040 --> 01:40:41,090
but i will hold

1600
01:40:41,100 --> 01:40:44,690
subsequent formal constant musicians

1601
01:40:44,740 --> 01:40:50,090
the present level of well let's see what you end up with the model is

1602
01:40:50,090 --> 01:40:54,530
the only of some of the idea

1603
01:40:54,540 --> 01:40:59,170
what can you say about those workers from a convenience about you that you are

1604
01:40:59,170 --> 01:41:01,020
interested to minimize

1605
01:41:01,040 --> 01:41:06,460
convex function f of x all over on this one but still in that i

1606
01:41:06,970 --> 01:41:08,400
but this one

1607
01:41:10,550 --> 01:41:19,500
i love this one as it is the one as the maximum the was sponsored

1608
01:41:19,500 --> 01:41:20,190
of by

1609
01:41:20,240 --> 01:41:22,270
but the political

1610
01:41:22,940 --> 01:41:28,020
OK so and the builders assumed that this function is convex access

1611
01:41:28,020 --> 01:41:29,790
able to introduce

1612
01:41:29,850 --> 01:41:31,160
background dataset

1613
01:41:31,170 --> 01:41:32,580
in the first setting

1614
01:41:32,590 --> 01:41:36,680
for example we can use the random graph generators to generate a set of random

1615
01:41:36,680 --> 01:41:39,790
graphs and then use them as background dataset

1616
01:41:39,840 --> 01:41:42,230
so in the first that is the problem becomes

1617
01:41:42,230 --> 01:41:44,850
can we find some significant graph patterns

1618
01:41:44,860 --> 01:41:48,270
in this datasets such that it is seldom appear

1619
01:41:48,280 --> 01:41:50,370
in the random graph datasets

1620
01:41:50,380 --> 01:41:55,040
so this is just show you multiple graph setting scenario in graph data mining

1621
01:41:55,060 --> 01:41:59,530
i'm showing real applications so we have a single graph mining problems but today i

1622
01:41:59,530 --> 01:42:05,550
will focus on the multiple graph mining scenario because the much easier to to

1623
01:42:05,560 --> 01:42:07,650
to present

1624
01:42:07,670 --> 01:42:12,530
so when we talk about graph patterns the first question is what kind of

1625
01:42:12,790 --> 01:42:15,730
patterns might be interesting to our users

1626
01:42:15,920 --> 01:42:22,280
certainly the definition of interestingness is related to specific application scenarios

1627
01:42:22,330 --> 01:42:24,830
however there do exist general

1628
01:42:24,840 --> 01:42:27,810
definitions which is meaningful to many applications

1629
01:42:27,850 --> 01:42:32,480
here is an example frequent graph patterns

1630
01:42:32,490 --> 01:42:37,490
frequent graph patterns are those common sub-structures existing in a set of

1631
01:42:37,520 --> 01:42:39,100
of graphs

1632
01:42:39,120 --> 01:42:44,780
suppose we have a collection of active chemical compounds that are active to HIV

1633
01:42:45,710 --> 01:42:49,230
then frequent graph pattern mining can help us to find those

1634
01:42:49,250 --> 01:42:53,110
common sub-structures existing in these chemical compounds

1635
01:42:53,160 --> 01:42:59,730
certainly the discovery of such substructures will be very useful to drug design

1636
01:42:59,740 --> 01:43:01,610
we if we replace

1637
01:43:01,670 --> 01:43:03,480
the chemical compound dataset

1638
01:43:03,540 --> 01:43:05,030
with a program

1639
01:43:05,040 --> 01:43:06,780
sytem call graphs

1640
01:43:06,790 --> 01:43:08,870
the same concept can be applied

1641
01:43:08,920 --> 01:43:11,130
but for the discovery of full

1642
01:43:11,150 --> 01:43:13,310
routine system call behaviors

1643
01:43:13,370 --> 01:43:16,850
this routine system call behavior is actually can be used

1644
01:43:16,850 --> 01:43:21,090
to detector system anomalies or intrusions

1645
01:43:22,060 --> 01:43:26,420
frequent graph pattern mining can be applied to gene coexpression networks

1646
01:43:26,420 --> 01:43:30,920
for example to find those frequent dense graphs existing these genes

1647
01:43:30,980 --> 01:43:32,480
coexpression networks

1648
01:43:32,490 --> 01:43:35,540
these dense graphs can be taken as

1649
01:43:35,600 --> 01:43:38,240
function function modules

1650
01:43:38,270 --> 01:43:41,280
and actually we can use this problem

1651
01:43:41,290 --> 01:43:46,440
we can use the function modules to predict the functionality of unknown genes

1652
01:43:46,550 --> 01:43:50,040
by the functionality of known genes in the same modules

1653
01:43:50,050 --> 01:43:52,710
so you can see that for very simple

1654
01:43:52,730 --> 01:43:58,330
concept - frequent graph mining it actually can find a lot of interesting problems

1655
01:43:58,340 --> 01:44:00,620
besides the frequency as this criteria

1656
01:44:00,650 --> 01:44:02,680
actually we can come up with other

1657
01:44:02,720 --> 01:44:08,710
interesting criteria which is actually popularly used in the data mining and machine learning society

1658
01:44:08,720 --> 01:44:12,980
for example information gain suppose we are given two sets of graphs we would

1659
01:44:12,980 --> 01:44:15,430
like to find sub-graphs which

1660
01:44:15,480 --> 01:44:18,970
has the highest information gain for example fisher score

1661
01:44:18,980 --> 01:44:24,630
for example a significance test like a g-test score

1662
01:44:24,650 --> 01:44:30,940
so let me first introduced the very basic now for mining path frequent pattern mining

1663
01:44:30,940 --> 01:44:35,170
and later i will show you a direct connection between frequent graph pattern

1664
01:44:35,170 --> 01:44:38,810
and other graph patterns with respect to

1665
01:44:38,840 --> 01:44:42,880
user specified specified objective functions

1666
01:44:42,900 --> 01:44:47,050
so here is the definition of frequent graph patterns

1667
01:44:47,150 --> 01:44:52,250
the definition very simple suppose we are given a graph dataset D

1668
01:44:52,270 --> 01:44:55,880
and a user specify the threshold as theta that we would like to find

1669
01:44:55,880 --> 01:45:01,690
all of the subgraphs in our graph dataset with frequency greater than this

1670
01:45:01,850 --> 01:45:07,300
user specified threshold theta so here the frequency is defined as

1671
01:45:07,310 --> 01:45:11,940
the percentage of graphs in this dataset that contain

1672
01:45:11,960 --> 01:45:14,230
this subgraph g

1673
01:45:14,290 --> 01:45:17,640
so the concept like this very simple

1674
01:45:17,650 --> 01:45:21,320
so here is an example about frequent subgraph patterns

1675
01:45:21,350 --> 01:45:24,780
suppose we have three chemical compounds like these

1676
01:45:24,800 --> 01:45:26,580
and if we set to the minimum

1677
01:45:26,590 --> 01:45:29,460
frequency threshold for example fifty percent

1678
01:45:29,470 --> 01:45:32,690
so in this graph actually is frequent subgraph

1679
01:45:32,770 --> 01:45:35,870
because this graph appears at least

1680
01:45:35,980 --> 01:45:39,510
in this two chemical compounds

1681
01:45:39,560 --> 01:45:41,100
here is another example

1682
01:45:41,110 --> 01:45:46,210
we have three program call graphs extracted from program traces

1683
01:45:46,250 --> 01:45:48,930
each node represents function

1684
01:45:48,990 --> 01:45:51,150
if function a calls function b

1685
01:45:51,160 --> 01:45:52,480
then we will

1686
01:45:52,490 --> 01:45:57,270
build a directed edge between function and the function b so we can collect a lot

1687
01:45:57,270 --> 01:45:59,320
of system call graphs for

1688
01:45:59,330 --> 01:46:01,120
each programme run

1689
01:46:01,260 --> 01:46:05,410
and if we set the minimum threshold again for example at fifty percent

1690
01:46:05,610 --> 01:46:09,800
it by careful examination we find that both

1691
01:46:09,850 --> 01:46:11,690
these two

1692
01:46:11,700 --> 01:46:16,240
of program call symbol graphs actually frequent graph patterns

1693
01:46:16,250 --> 01:46:19,330
most of them appear in at least three two of these

1694
01:46:19,370 --> 01:46:20,850
programme call graphs

1695
01:46:20,860 --> 01:46:26,410
so you can see that frequent pattern actually is very relative concept it is related

1696
01:46:26,410 --> 01:46:27,140
to the

1697
01:46:28,340 --> 01:46:33,600
frequencies threshold setting later i will show you actually it is a bigger problem

1698
01:46:33,640 --> 01:46:36,350
so far i would like to present

1699
01:46:36,350 --> 01:46:43,140
some basic graph pattern mining algorithms which had been proposed in the past several years

1700
01:46:43,230 --> 01:46:46,660
so researchers have proposed many patterns

1701
01:46:46,730 --> 01:46:50,800
mining and especially frequent the while pattern mining algorithms

1702
01:46:50,810 --> 01:46:57,100
for example we can use inductive logic programming to represent graphs as datalog facts

1703
01:46:57,140 --> 01:47:01,760
and the using the logic logical programme method to generate those frequent graph patterns however

1704
01:47:01,760 --> 01:47:04,060
the process would be very slow

1705
01:47:04,080 --> 01:47:08,350
and the most of the existing graph pattern mining algorithms actually can be put into

1706
01:47:08,350 --> 01:47:12,080
the second category graph theory based approach

1707
01:47:12,120 --> 01:47:13,960
basically they use different

1708
01:47:14,230 --> 01:47:19,840
heuristics and come upon our method to generate to frequent graph patterns

1709
01:47:19,850 --> 01:47:22,110
so again in this category we can

1710
01:47:22,110 --> 01:47:23,990
divide this method into

1711
01:47:25,500 --> 01:47:30,860
one is apriori based approach the second is the pattern based pattern growth approach

1712
01:47:30,910 --> 01:47:36,280
i'm not going to show you the details of these algorithms one by one but

1713
01:47:36,280 --> 01:47:39,910
later i would just to show you is a high-level design principles

1714
01:47:39,930 --> 01:47:42,510
behind these algorithms

1715
01:47:42,570 --> 01:47:44,990
all of this algorithms actually show

1716
01:47:45,050 --> 01:47:49,770
a very basic rule being frequent pattern mining it is called apriori

1717
01:47:49,980 --> 01:47:53,220
property i think most of you probably already know this

1718
01:47:53,230 --> 01:47:55,500
so if a graph is frequent as in

1719
01:47:55,560 --> 01:48:01,020
all of its subgraphs will be frequent

1720
01:48:01,100 --> 01:48:05,110
this is a nice properties that means

1721
01:48:05,120 --> 01:48:10,450
basically you can enumerate this graph patterns from small size to large size

1722
01:48:10,510 --> 01:48:13,320
during the enumeration you can check their frequency

1723
01:48:13,350 --> 01:48:17,050
whenever you find out graph pattern that is not frequent

1724
01:48:17,100 --> 01:48:17,840
any more

1725
01:48:17,850 --> 01:48:20,340
then need not to extend

1726
01:48:20,350 --> 01:48:26,060
this graph part anymore because it's unlikely it's super graph will be frequent if it itself is

1727
01:48:26,060 --> 01:48:31,950
not frequent so here is the basic enumeration pipeline to generate these graph patterns

1728
01:48:31,960 --> 01:48:36,260
so start with a small graph pattern for example k edge graph pattern

1729
01:48:36,320 --> 01:48:40,730
we can generate k plus edge graph pattern candidates

1730
01:48:40,730 --> 01:48:44,170
diffusion losses you should also be assured of divergences because you can learn a lot

1731
01:48:44,170 --> 01:48:46,550
about loss functions by looking after

1732
01:48:46,560 --> 01:48:49,460
and vice versa

1733
01:48:50,380 --> 01:48:53,820
so one direction and this is it it is very easy in the other direction

1734
01:48:53,830 --> 01:49:00,200
is the core of our paper several pages of proofs of a little little challenge

1735
01:49:00,250 --> 01:49:05,250
that is the last theorem in this section of the talk and so i will

1736
01:49:05,250 --> 01:49:08,500
tell you how you prove it and so on but let's just assume it's true

1737
01:49:08,520 --> 01:49:13,090
alright analysis read as some easy consequences of that OK

1738
01:49:13,120 --> 01:49:17,700
so i suppose we start with zero one loss here we mapped over we expanded

1739
01:49:17,720 --> 01:49:21,020
we map back and now we get a whole bundle of loss functions there they're

1740
01:49:21,020 --> 01:49:25,630
all universal equivalent biosphere to zero one loss

1741
01:49:25,710 --> 01:49:31,310
i what me well that's good other loss functions andrew any probability distribution which all

1742
01:49:32,420 --> 01:49:36,410
the quantizer is the same as zero one loss ranks all zero one loss is

1743
01:49:36,410 --> 01:49:40,400
our goal here that's the overall loss function we're trying to optimize ranking the same

1744
01:49:40,430 --> 01:49:44,800
zero one loss consistent doing the right thing

1745
01:49:45,560 --> 01:49:47,820
but this is an if and only if there

1746
01:49:47,880 --> 01:49:52,410
which is which also says that you're not consistent zero one loss sorry if you're

1747
01:49:52,410 --> 01:49:58,510
not equivalent to zero one loss outside here another loss sitting out there

1748
01:49:58,550 --> 01:50:02,160
right that means you're not universally equivalent to zero one loss that means there exist

1749
01:50:02,170 --> 01:50:06,550
some problems which you get a different answer than zero one loss

1750
01:50:06,560 --> 01:50:09,050
which has been shown not consistent

1751
01:50:09,050 --> 01:50:12,770
there might be some problems which is that which give the right same answer

1752
01:50:12,820 --> 01:50:15,800
but that's not good enough for us we're trying to do general theorem which is

1753
01:50:15,800 --> 01:50:19,070
that you the same in general

1754
01:50:19,150 --> 01:50:21,770
OK so this reduces to a very simple problem now

1755
01:50:23,230 --> 01:50:29,250
loss functions are universal equivalent to to zero one loss

1756
01:50:29,270 --> 01:50:32,790
right that's easy because we know how do we go out after version space zero

1757
01:50:32,790 --> 01:50:36,700
one loss leads to variational distance

1758
01:50:36,720 --> 01:50:40,350
expand variational distance and you come back and you get a whole class of loss

1759
01:50:43,890 --> 01:50:47,330
in particular you get the hinge loss is one of them

1760
01:50:47,560 --> 01:50:51,480
plus you have to consider one so hinge loss i the support vector machine

1761
01:50:51,490 --> 01:50:56,580
leads to consistency you can join you can use the support vector machine formalism and

1762
01:50:56,580 --> 01:51:00,030
jointly optimize with respect to the to the discriminant which is what's usually done the

1763
01:51:00,030 --> 01:51:03,140
support vector machine and a quantizer

1764
01:51:03,200 --> 01:51:06,180
and he then had all the same kind of correctly then that will be that

1765
01:51:06,180 --> 01:51:07,900
will give the right answer

1766
01:51:07,920 --> 01:51:11,810
boosting is not

1767
01:51:11,850 --> 01:51:15,720
right boosting leads to another f divergence which is not the family

1768
01:51:15,730 --> 01:51:18,260
so if use the exponential loss behind boosting

1769
01:51:18,270 --> 01:51:20,850
then you're not equivalent you're not consistent

1770
01:51:20,880 --> 01:51:24,570
OK you can use that function or both of them in general

1771
01:51:24,630 --> 01:51:27,920
you there might be special cases which will work but not injured

1772
01:51:27,970 --> 01:51:32,990
OK so that's an interesting character a you know boosting SVM all these different losses

1773
01:51:32,990 --> 01:51:35,920
in this flat classification are all consistent

1774
01:51:35,980 --> 01:51:40,260
they lead to that's where the earlier we talked about it but when we don't

1775
01:51:40,260 --> 01:51:44,380
want is harder problem doing experiments line and the in the quanta and the classification

1776
01:51:44,780 --> 01:51:49,060
of some of the more and some more

1777
01:51:52,960 --> 01:52:02,660
yeah absolutely this function is also universal equivalent to zero one loss

1778
01:52:02,670 --> 01:52:06,790
and the square loss if you take the one that comes down and is

1779
01:52:06,820 --> 01:52:10,470
a flat after that is also

1780
01:52:11,000 --> 01:52:13,320
and i don't i don't have a kind of a list of all the others

1781
01:52:13,320 --> 01:52:16,630
it sort of effective beginning when i was never really done that kind of looking

1782
01:52:16,630 --> 01:52:18,060
at the others

1783
01:52:18,070 --> 01:52:20,320
at this point

1784
01:52:20,520 --> 01:52:23,260
i don't think it is actually

1785
01:52:24,950 --> 01:52:27,800
i have set down and work that out

1786
01:52:27,820 --> 01:52:31,710
we just got it's easy to work out you just

1787
01:52:31,730 --> 01:52:34,240
calculate the f you see if you can get it right here is that i

1788
01:52:34,240 --> 01:52:35,350
find related to the

1789
01:52:35,380 --> 01:52:37,470
i think is

1790
01:52:37,470 --> 01:52:43,410
OK so that's that story

1791
01:52:45,800 --> 01:52:48,110
so next sort of

1792
01:52:48,130 --> 01:52:52,380
application this that's one application is to consistency results and you can use this for

1793
01:52:52,460 --> 01:52:56,160
the kind of problems now that was for quantization from line we can also use

1794
01:52:56,160 --> 01:53:00,010
it for feature selection as another kind of experiment design problem how to find feature

1795
01:53:00,010 --> 01:53:06,370
selection stage together with a quanta with classifier discriminant but they jointly estimated together give

1796
01:53:06,370 --> 01:53:09,790
you consistency so you can use this for has been done but that's not something

1797
01:53:09,790 --> 01:53:12,760
that would be fun to do

1798
01:53:12,760 --> 01:53:17,080
here's another class of applications of this which has a with the estimation of divergences

1799
01:53:17,800 --> 01:53:22,220
you know think the kullback leibler divergence we managed to estimate the kullback leibler leiber

1800
01:53:22,220 --> 01:53:24,540
divergence from data

1801
01:53:24,740 --> 01:53:27,820
why would you want to do that well what's the reason sometimes we want to

1802
01:53:27,900 --> 01:53:31,830
optimize KL divergence is so you might be trying to build estimate if you expect

1803
01:53:31,830 --> 01:53:33,770
to you optimise it

1804
01:53:34,280 --> 01:53:36,220
special case killer is the entropy

1805
01:53:36,230 --> 01:53:39,710
well i interested in estimating interview from data

1806
01:53:40,630 --> 01:53:45,250
so there is for example the KL divergence somehow however how might we estimate that

1807
01:53:45,250 --> 01:53:48,600
from data well killer is based on two measures

1808
01:53:48,610 --> 01:53:49,900
p and q

1809
01:53:49,910 --> 01:53:52,970
so can get you have to assume you can get data from both p and

1810
01:53:54,160 --> 01:53:59,330
and then you know how in some sense will plug it into this formula get

1811
01:53:59,340 --> 01:54:00,970
some kind of an estimator

1812
01:54:01,060 --> 01:54:04,300
the problem here is that if you look at the key part you've got both

1813
01:54:05,380 --> 01:54:08,610
and log appearing together in the formula

1814
01:54:08,620 --> 01:54:12,500
and so if you think about things like kernel density estimation you have to develop

1815
01:54:12,500 --> 01:54:16,470
into a good bandwidth for estimating p part but also the log p and those

1816
01:54:16,470 --> 01:54:18,740
are incommensurate things to do

1817
01:54:18,740 --> 01:54:21,560
meaning that so this is supposed to be the arrow the cost

1818
01:54:21,560 --> 01:54:24,240
of respect to the parameter space on on the plane

1819
01:54:25,590 --> 01:54:28,020
meaning that you have directions where the

1820
01:54:28,610 --> 01:54:32,780
second derivatives of the cost are very large and directions where

1821
01:54:32,780 --> 01:54:35,580
the second derivative the costs of small and you know

1822
01:54:35,660 --> 01:54:38,910
this says slowing down training and so on

1823
01:54:39,140 --> 01:54:44,000
but in recurrent nets actually you might see things like this this

1824
01:54:44,000 --> 01:54:46,650
is actually the case with a single neuron with feedback

1825
01:54:46,650 --> 01:54:50,830
and you have these ridges so instead of things being these valleys

1826
01:54:50,840 --> 01:54:54,320
being symmetric you have very high derivatives on one side but

1827
01:54:54,320 --> 01:54:56,970
not this is the high derivatives on the other side

1828
01:54:56,970 --> 01:55:00,200
and here we have a nice slope going down in this direction you

1829
01:55:00,200 --> 01:55:02,730
actually would like to go down in this direction the problem as

1830
01:55:02,730 --> 01:55:07,460
you do that you approach this region which is close to probably

1831
01:55:07,470 --> 01:55:11,360
a boundary of basic attraction thought

1832
01:55:11,440 --> 01:55:14,720
this is in parameter space so it makes it makes it possible to

1833
01:55:14,720 --> 01:55:18,920
approach those boundaries that have very large gradients and near

1834
01:55:18,920 --> 01:55:21,860
those boundaries gradient become huge and kikuyu often you

1835
01:55:21,860 --> 01:55:26,270
throw off the learning so by simply scaling gradient or sometimes

1836
01:55:26,280 --> 01:55:29,090
just ignoring it and doing random move

1837
01:55:29,940 --> 01:55:34,040
you you just you know goal not too far away then continue and that

1838
01:55:34,050 --> 01:55:37,090
really the sole lot of these problems doesn't solve the

1839
01:55:37,090 --> 01:55:38,890
the long term dependencies problem

1840
01:55:38,890 --> 01:55:42,370
solve the problem coming from the large gradients so this is called

1841
01:55:42,370 --> 01:55:44,700
the gradient clipping is very very efficient

1842
01:55:44,870 --> 01:55:49,970
and you should all have in your encode right

1843
01:55:53,510 --> 01:55:58,700
so now i'm going tell you about approaches to try to deal with

1844
01:55:59,570 --> 01:56:03,430
this problem of long-term dependencies so in

1845
01:56:05,370 --> 01:56:08,530
in than ninety three paper that we wrote

1846
01:56:09,170 --> 01:56:10,860
about the long-term dependencies

1847
01:56:10,860 --> 01:56:13,980
so this was an early version of the joint paper ninety four

1848
01:56:14,140 --> 01:56:18,770
we a we proposed something that has the flavor of lstm is

1849
01:56:19,070 --> 01:56:21,670
is not the same thing but the idea was that

1850
01:56:22,160 --> 01:56:27,460
again we have this memory cell that at each state gets updated by

1851
01:56:27,470 --> 01:56:31,640
just copying itself and and we have some kind of decision that

1852
01:56:31,880 --> 01:56:34,830
gates that this that you know whether we copy

1853
01:56:35,010 --> 01:56:41,260
or all we don't and and we use some kind of heuristic to decide

1854
01:56:41,270 --> 01:56:45,700
how to provide a gradients but this idea that you should have this

1855
01:56:45,710 --> 01:56:49,670
kind of self-loop to keep information for a long time

1856
01:56:49,990 --> 01:56:56,240
was there was also a in ninety one in push writers thesis

1857
01:57:01,780 --> 01:57:05,730
yeah i actually tell you about lstm right now so

1858
01:57:08,580 --> 01:57:12,070
there are different variants of the lstm basically all of them the

1859
01:57:12,070 --> 01:57:16,350
central idea is there is this state variable

1860
01:57:17,990 --> 01:57:20,540
has a self-loop with itself another words

1861
01:57:21,630 --> 01:57:24,020
i call this the call the cell in the lstm

1862
01:57:24,230 --> 01:57:27,400
that the news state will be equal to the old state plus some

1863
01:57:27,400 --> 01:57:34,040
kind of update and we can control with a gate whether we actually

1864
01:57:34,050 --> 01:57:38,430
copy the old state to new state and whether we actually

1865
01:57:38,800 --> 01:57:41,990
know have an update coming so so the

1866
01:57:42,480 --> 01:57:45,020
the important one is the for the gate here which

1867
01:57:45,480 --> 01:57:51,160
which multiplies the self-loop path so it could kill off a self-loop

1868
01:57:51,170 --> 01:57:55,100
path and the reason to do that is that some if you didn't have that

1869
01:57:55,100 --> 01:57:57,840
any would just keep adding adding in the state would keep growing

1870
01:57:57,840 --> 01:58:04,150
and growing and yeah this might prevent you from forgetting things

1871
01:58:04,150 --> 01:58:07,050
sometimes you actually want to forget things because you have limited

1872
01:58:07,050 --> 01:58:12,250
memory and in the case of lstm there are also extra gates which

1873
01:58:12,250 --> 01:58:15,160
control whether we actually add something into the state so there's

1874
01:58:15,160 --> 01:58:19,490
the input but there's also gate which multiplies so it can kill off

1875
01:58:19,500 --> 01:58:23,030
the thing that will be added and they also have an output gate

1876
01:58:23,030 --> 01:58:26,380
which i think is useless but controls whether something is going

1877
01:58:26,380 --> 01:58:30,330
to be sent to the outside world and the gr you get a recurrent unit

1878
01:58:30,590 --> 01:58:36,330
has a slightly simpler architecture so it's but

1879
01:58:36,800 --> 01:58:41,850
it has the same self-loop and it works basically the same as the

1880
01:58:41,860 --> 01:58:45,290
cheaper to compute so several papers are using it

1881
01:58:45,900 --> 01:58:47,540
this kind of the same principle

1882
01:58:50,430 --> 01:58:54,390
so that was ninety seven lstm in ninety five

1883
01:58:58,080 --> 01:59:02,800
ninety five we the there's another paper that's missing

1884
01:59:02,990 --> 01:59:08,960
should have yeah we proposed in nips ninety five proposed to use

1885
01:59:09,250 --> 01:59:13,090
temple hierarchy so this is answering going as a suggestion

1886
01:59:13,520 --> 01:59:18,430
that maybe we could use multiple levels of time scales in

1887
01:59:18,440 --> 01:59:22,620
order to bypass this problem of having you know the multiplication

1888
01:59:22,630 --> 01:59:27,730
by many accordions so f what i mean here is here the bottom have

1889
01:59:27,730 --> 01:59:30,940
regualar regular recurrent net but at the next layer have recurrent

1890
01:59:30,940 --> 01:59:34,860
net that's updated less often somehow and then another one even

1891
01:59:34,870 --> 01:59:39,610
less often so here i double the period and so what happens is that

1892
01:59:39,620 --> 01:59:43,920
even though the path through the the the the the fine time scale

1893
01:59:43,930 --> 01:59:48,440
recurrent net may just make it impossible to learn long term dependencies

1894
01:59:48,660 --> 01:59:50,770
you could learn a long term dependencies

1895
01:59:51,250 --> 01:59:54,640
by having gradients propagate through the shorter path that goes

1896
01:59:54,640 --> 01:59:58,360
through the slow time scale so this is the general idea

1897
01:59:59,710 --> 02:00:04,390
and devil in the detail how do we know define when those upper one

1898
02:00:04,400 --> 02:00:08,650
should be updated and so on there you don't even have to have

1899
02:00:08,660 --> 02:00:12,460
a discrete updates you could have slowly changing units

1900
02:00:12,640 --> 02:00:15,230
higher up rather than you know i update every

1901
02:00:15,780 --> 02:00:19,920
ten timesteps i just update by copying miles value a little bit

1902
02:00:20,500 --> 02:00:22,620
and adding a little bit of the new value

1903
02:00:23,750 --> 02:00:26,350
another idea which came about the same time from

1904
02:00:26,870 --> 02:00:30,330
the giles in the top right what you see is you can have recurrent

1905
02:00:30,330 --> 02:00:32,080
net we skip connections over time

1906
02:00:33,390 --> 02:00:37,340
so again what you're doing is now you create those paths in the

1907
02:00:37,350 --> 02:00:41,240
graph which allowed to go sort of faster from the past the future

1908
02:00:41,440 --> 02:00:43,300
with less nonlinearities in between

1909
02:00:43,670 --> 02:00:46,190
and and that also works

1910
02:00:48,720 --> 02:00:53,620
so example of these these are hierarchies in recent work we did

