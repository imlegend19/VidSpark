1
00:00:00,000 --> 00:00:01,520
it was indicated have just

2
00:00:02,390 --> 00:00:03,100
sixty more

3
00:00:05,370 --> 00:00:06,980
and three of them are

4
00:00:08,620 --> 00:00:09,640
twenty five thousand

5
00:00:10,730 --> 00:00:12,140
and nowadays you give this

6
00:00:14,830 --> 00:00:16,140
impressive number of dental

7
00:00:21,910 --> 00:00:23,190
there are two important

8
00:00:23,580 --> 00:00:24,440
things in fact

9
00:00:26,870 --> 00:00:27,930
on the bus on the

10
00:00:28,960 --> 00:00:29,540
one hand

11
00:00:31,750 --> 00:00:35,940
this calculation started the of using computers recruitment zeros

12
00:00:38,540 --> 00:00:42,640
if it was done by children would be done by somebody else so it's not the main contribution

13
00:00:43,710 --> 00:00:44,480
what is important

14
00:00:45,040 --> 00:00:46,890
is the methods introduced by children

15
00:00:48,190 --> 00:00:49,000
and this message

16
00:00:49,910 --> 00:00:50,520
was used

17
00:00:51,600 --> 00:00:52,960
no consequent calculations

18
00:00:54,930 --> 00:00:57,460
nobody else was able to invent anything but

19
00:00:58,520 --> 00:00:59,430
so this message is

20
00:00:59,870 --> 00:01:00,520
and actually use

21
00:01:01,310 --> 00:01:01,710
up be

22
00:01:04,350 --> 00:01:05,230
i spoke about

23
00:01:05,960 --> 00:01:06,560
two papers

24
00:01:07,350 --> 00:01:07,930
published by

25
00:01:08,460 --> 00:01:09,850
student about their

26
00:01:10,750 --> 00:01:11,620
remains that the function

27
00:01:14,690 --> 00:01:17,770
repayments this talk i added a lot of papers about student

28
00:01:19,040 --> 00:01:22,190
and practically all of them say them for the same

29
00:01:24,270 --> 00:01:28,690
the legacy of q numbers it consisted of a number of unpublished manuscripts

30
00:01:29,440 --> 00:01:32,080
a number of that no other mathematicians

31
00:01:32,960 --> 00:01:34,390
and this manuscript sent letters

32
00:01:36,310 --> 00:01:37,100
and what ideas

33
00:01:38,020 --> 00:01:39,310
but was published

34
00:01:39,910 --> 00:01:41,190
not just these two papers

35
00:01:43,060 --> 00:01:43,790
but this is not

36
00:01:44,230 --> 00:01:44,640
is so

37
00:01:46,520 --> 00:01:47,640
there is a sort of labor

38
00:01:49,940 --> 00:01:51,730
there he did this that one function

39
00:01:54,100 --> 00:01:55,000
it means that the function

40
00:01:56,770 --> 00:01:57,270
but somehow

41
00:01:57,730 --> 00:01:58,560
this paper

42
00:01:59,640 --> 00:02:01,120
remains unknown numbers

43
00:02:01,980 --> 00:02:02,810
they don't cite it

44
00:02:03,640 --> 00:02:04,560
they don't know why

45
00:02:06,310 --> 00:02:09,640
maybe they don't appreciate the result obtained by consist paper

46
00:02:10,910 --> 00:02:12,930
or maybe they simply don't know the paper

47
00:02:13,940 --> 00:02:15,410
because it is difficult to guess

48
00:02:16,750 --> 00:02:18,000
from the title of the paper

49
00:02:24,390 --> 00:02:25,460
the paper was titled

50
00:02:26,190 --> 00:02:28,170
system of logic based on ordinals

51
00:02:30,000 --> 00:02:30,930
and text it was

52
00:02:31,500 --> 00:02:32,410
student dissertation

53
00:02:33,770 --> 00:02:34,850
done in princeton

54
00:02:41,940 --> 00:02:42,910
section city

55
00:02:44,020 --> 00:02:44,890
and his dissipation

56
00:02:45,540 --> 00:02:46,230
because the title

57
00:02:46,730 --> 00:02:48,000
number statistical science

58
00:02:52,890 --> 00:02:54,060
give a formal definition

59
00:02:54,770 --> 00:02:56,160
what you understand by

60
00:02:56,670 --> 00:02:57,710
numbers that it because you

61
00:02:59,580 --> 00:03:02,980
but numbers that because here we show them in a series of the form

62
00:03:04,170 --> 00:03:07,540
i think the effect vanishes for infinitely many natural numbers x

63
00:03:08,500 --> 00:03:11,000
they sit there is there a primitive recursive function

64
00:03:14,540 --> 00:03:15,870
okay she here's a footnote

65
00:03:17,020 --> 00:03:17,520
i believe

66
00:03:18,410 --> 00:03:21,370
there is no generally accepted meaning for this storm

67
00:03:22,040 --> 00:03:23,080
numbers that considered

68
00:03:24,020 --> 00:03:25,160
but it should be noticed

69
00:03:25,790 --> 00:03:26,810
that we are using it

70
00:03:27,190 --> 00:03:28,600
in that other restricted sense

71
00:03:31,620 --> 00:03:32,230
then he gives

72
00:03:32,940 --> 00:03:33,960
an equivalent definition

73
00:03:35,160 --> 00:03:37,430
an alternative form for number-theoretic theorems

74
00:03:39,210 --> 00:03:40,670
for each natural number x

75
00:03:41,660 --> 00:03:43,350
there exists a natural number right

76
00:03:43,910 --> 00:03:44,960
such that f of x

77
00:03:45,640 --> 00:03:46,120
when issues

78
00:03:46,830 --> 00:03:49,230
where f of x and it's been omitted because

79
00:03:52,770 --> 00:03:53,940
so numbers cities may

80
00:03:54,390 --> 00:03:56,310
we not that if i decide the finish and

81
00:03:56,640 --> 00:03:57,500
of what they're doing

82
00:03:58,580 --> 00:04:01,640
so the notion of primitive recursive function used in the definition

83
00:04:03,500 --> 00:04:05,980
but the same class of systems can be defined

84
00:04:07,410 --> 00:04:08,250
in a more arithmetic

85
00:04:10,810 --> 00:04:11,600
we can consider the

86
00:04:12,290 --> 00:04:13,960
well known as hierarchy

87
00:04:15,560 --> 00:04:16,160
the public

88
00:04:17,230 --> 00:04:18,210
and medical formulas

89
00:04:18,850 --> 00:04:20,480
they're all quantifiers are bounded

90
00:04:21,770 --> 00:04:24,640
and then recursively constructed the following levels

91
00:04:24,640 --> 00:04:28,390
to use a particular vocabulary for relations objects and so on in

92
00:04:28,660 --> 00:04:34,410
RDF graph you can define vocabulary in RDF schema you could not this figure vocabulary

93
00:04:34,420 --> 00:04:35,630
in a hierarchy

94
00:04:35,640 --> 00:04:39,550
OK so you can talk about persons you can talk about different types of persons

95
00:04:39,630 --> 00:04:43,880
persons that was readers and relationships between these two

96
00:04:44,030 --> 00:04:49,330
so the authors communicate to reader so this is a relationship with domain or the

97
00:04:49,330 --> 00:04:55,800
range reader around and then you can have instances so is a particular instance and

98
00:04:55,800 --> 00:05:00,370
here's another particular instance and they may have this particular relationship

99
00:05:00,450 --> 00:05:03,280
because she read the book written by

100
00:05:03,550 --> 00:05:09,300
and here is your comment about what your particular instances

101
00:05:09,470 --> 00:05:13,790
that are organised in this particular class

102
00:05:14,080 --> 00:05:18,730
OK so this everything there is to say that RDF schema

103
00:05:20,210 --> 00:05:21,610
well you should

104
00:05:21,650 --> 00:05:22,830
five is part

105
00:05:23,630 --> 00:05:27,930
and what you do with schema you can define classes subclasses in this hierarchy

106
00:05:28,310 --> 00:05:30,200
you can find properties

107
00:05:30,220 --> 00:05:35,930
between these classes and also for these properties can be defined hierarchy so fatherof subpropertyof

108
00:05:36,900 --> 00:05:41,900
in terms a few somebody's father always also some experience and these properties can have

109
00:05:41,900 --> 00:05:43,270
domains and ranges

110
00:05:43,290 --> 00:05:47,830
so it is to which the property applies in what the are the class of

111
00:05:50,090 --> 00:05:51,950
third element in the triple

112
00:05:51,960 --> 00:05:54,000
you could do little inferencing on this

113
00:05:54,740 --> 00:05:58,360
if i know that frank is an author i also know that the person

114
00:05:58,380 --> 00:06:02,610
and even if i didn't know notice frank was also

115
00:06:02,630 --> 00:06:05,230
what i that frank communicates to that

116
00:06:05,250 --> 00:06:09,880
and communicates to this is domain therefore frame must be also

117
00:06:09,890 --> 00:06:13,870
so we do this all the time i say

118
00:06:13,980 --> 00:06:17,080
marco is married to

119
00:06:17,100 --> 00:06:21,360
and then we have a lot of memory two is relationship between two people

120
00:06:21,390 --> 00:06:25,010
right so we mean even if you've never seen the pair of them before if

121
00:06:25,010 --> 00:06:29,110
i give you that statement you could you would have for most people

122
00:06:29,170 --> 00:06:37,860
it's not to get some householders so we do is inferring of of class membership

123
00:06:37,860 --> 00:06:42,000
based on relation instances we do it all the time and it's also a legal

124
00:06:42,000 --> 00:06:42,570
inference in

125
00:06:43,210 --> 00:06:47,030
in RDF schema so you see here are schema is a little logic

126
00:06:47,180 --> 00:06:50,550
in the sense that i give you some facts and you can derive some new

127
00:06:50,550 --> 00:06:53,980
facts and i can exactly predict which means that you're going to drive because they

128
00:06:53,980 --> 00:06:57,070
are exactly the derivation rules that are sanctioned by the

129
00:06:57,080 --> 00:07:06,320
but here is that it is time is IDF's way of spelling instance of this

130
00:07:06,320 --> 00:07:11,500
kind of of water or is it simply the two ways of looking at the

131
00:07:11,500 --> 00:07:16,040
domain and range definitions in RDF schema we look at them as sources for inference

132
00:07:16,040 --> 00:07:22,200
has overseen markers married and then i can infer the type in other paradigms you

133
00:07:22,310 --> 00:07:29,420
can use the main range is as real as constraints is instructions so if markers

134
00:07:29,430 --> 00:07:35,970
married and then i learned that that marko is not person but something else then

135
00:07:35,970 --> 00:07:42,810
this this could be used to constrain conflict in RDF they are not used such

136
00:07:42,810 --> 00:07:46,940
constraints they are only using the sources of inference

137
00:07:47,030 --> 00:07:48,260
so let's

138
00:07:48,280 --> 00:07:49,500
i could but

139
00:07:49,500 --> 00:07:50,960
it comes from this

140
00:07:50,970 --> 00:07:58,590
that's not discussed terminology this is just the magic words in the language which you

141
00:07:58,940 --> 00:08:03,860
also see is that it is because of these for example do not use this

142
00:08:03,860 --> 00:08:09,080
constraint is impossible to say anything inconsistent in RDF schema is simply to weak

143
00:08:09,190 --> 00:08:10,790
you can also

144
00:08:10,800 --> 00:08:12,800
they are not a

145
00:08:12,810 --> 00:08:14,950
because you could say anything not

146
00:08:14,960 --> 00:08:19,810
there's no indication in the language you cannot create such conflicts right

147
00:08:19,840 --> 00:08:22,730
having something in the class it shouldn't be because there is no such a notion

148
00:08:22,730 --> 00:08:27,140
of constraint so i was logic but it's very very simple if you use a

149
00:08:27,140 --> 00:08:31,890
very simple view of the world where on the one hand you have instances and

150
00:08:31,890 --> 00:08:36,400
on the other hand you class but RDF and RDF schema actually allowed

151
00:08:36,400 --> 00:08:40,820
these two parts of the world to be completely makes something can be both an

152
00:08:40,820 --> 00:08:42,960
instance and the class at the same time

153
00:08:42,970 --> 00:08:47,560
so you could have a class that in some instances

154
00:08:47,580 --> 00:08:51,260
and this is subclass of the class but it can be an instance of some

155
00:08:51,260 --> 00:08:56,200
of the class which suggested not only class because as instances but itself is an

156
00:08:56,200 --> 00:08:57,550
instance because

157
00:08:57,570 --> 00:08:58,730
members some the class

158
00:08:58,780 --> 00:09:05,310
so something can be both things and the class every calls properties like instances properties

159
00:09:05,590 --> 00:09:07,640
even worse in my view

160
00:09:07,690 --> 00:09:12,400
the classes can be members of other classes including themselves so class can be member

161
00:09:12,400 --> 00:09:17,090
of the low-level image features and then based on the clustering they might build

162
00:09:17,090 --> 00:09:22,100
some SVM on top of it so in this case you could again

163
00:09:22,100 --> 00:09:31,660
do nonparametric clustering using this DP mixture model and then kind of test it by kind of

164
00:09:31,670 --> 00:09:40,380
plugging it into some supervised learning algorithm and see whether that works well or not so if

165
00:09:40,380 --> 00:09:48,700
you have some external if you have some problem that you want to solve with

166
00:09:48,700 --> 00:09:55,680
some external measure of of of how well the model's doing than you could actually

167
00:09:55,680 --> 00:10:00,560
check that is doing well or is not doing well okay I'll move onto a Pitman

168
00:10:00,560 --> 00:10:08,490
Yor processes  this are basically generalizations of the of the  DP of

169
00:10:08,490 --> 00:10:13,360
the of the Dirichlet process and the easiest way was a Pitman Yor processes

170
00:10:13,360 --> 00:10:20,120
basically by this random partitioning by by these Chinese restaurant process right so we saw

171
00:10:20,120 --> 00:10:23,660
that we have this Chinese restaurant process where the probability of a customer sitting at

172
00:10:23,720 --> 00:10:28,080
table C is gonna be proportional to the number of customers already sitting there

173
00:10:28,080 --> 00:10:33,750
and the probability of  customers sitting  at a new table is gonna be proportional to alpha and it has

174
00:10:33,750 --> 00:10:42,760
this rich gets richer property there are two other properties which of these Chinese restaurant

175
00:10:42,760 --> 00:10:47,860
process which is I think quite important in terms of modelling and they are called projectivity

176
00:10:47,860 --> 00:10:54,840
and exchangeability so I'll just kind of give you a very simple example okay of projective

177
00:10:54,840 --> 00:10:59,120
exchangeable models of data imagine that our data set consists of this six data

178
00:10:59,120 --> 00:11:05,200
items and we're gonna label and it's a supervised earning problem classifier is  the

179
00:11:05,200 --> 00:11:11,960
data item in the blue clust or in the red clust suppose that that you will

180
00:11:11,960 --> 00:11:16,280
be tested on one  new test item but I don't tell you what it is yet

181
00:11:16,280 --> 00:11:23,780
should this change your predictions of should you should this change the decision boundary

182
00:11:24,060 --> 00:11:33,220
over your data set anybody would like to say something no okay yeah yes

183
00:11:33,360 --> 00:11:41,780
sorry you might have

184
00:11:41,780 --> 00:11:49,260
but if you don't really know where this item is then it's probably hard

185
00:11:49,260 --> 00:11:54,760
to optimize for that so at least the simplest thing to do would be to say

186
00:11:54,760 --> 00:12:08,240
that it actually doesn't matter one there being one  test item yes

187
00:12:08,240 --> 00:12:16,340
this is just a classification it is two  classification  so

188
00:12:16,340 --> 00:12:23,340
we don't really exp at least as a simplest approximation we it's

189
00:12:23,340 --> 00:12:28,600
probably easiest to think of this as not really changing our predictions whether we have one

190
00:12:28,600 --> 00:12:34,020
test data item or not especially if we don't actually know where this test

191
00:12:34,030 --> 00:12:38,280
data item is if we knew knew where it is the  that would actually tell us something about

192
00:12:38,280 --> 00:12:42,110
the data set and that might change our prediction but if we don't know where

193
00:12:42,110 --> 00:12:47,150
it is then that doesn't change it so if we have five additional test items than

194
00:12:47,150 --> 00:12:51,760
that shouldn't change our predictions either because we don't really know where they are so

195
00:12:51,760 --> 00:12:57,320
we might  as well work with the data that we have okay how about if we our

196
00:12:57,320 --> 00:13:01,150
item labels were permuted so if I take X one X three and X two and

197
00:13:01,150 --> 00:13:07,980
we premuted the labels should that change our prediction it shouldn't change either because is this idea

198
00:13:08,000 --> 00:13:12,800
I mean  the the labels actually doesn't tell you anything about the data

199
00:13:12,800 --> 00:13:17,580
set itself unless you do things that the labels actually tell you unless that

200
00:13:17,590 --> 00:13:21,940
you have some sort of time series data or sequential data in which case the

201
00:13:21,940 --> 00:13:25,440
which observe the data items does have an effects on the on

202
00:13:25,440 --> 00:13:29,500
the model but in this case if we assume that's  IID it shouldn't affect your

203
00:13:29,500 --> 00:13:39,080
predictions okay so models which are which don't really aren't affected by additional by

204
00:13:39,080 --> 00:13:44,130
right here

205
00:13:50,140 --> 00:13:53,400
o standing

206
00:14:07,610 --> 00:14:12,640
these are the things i mean all

207
00:14:13,640 --> 00:14:16,320
you can see

208
00:14:16,340 --> 00:14:17,010
all o

209
00:14:17,170 --> 00:14:26,920
there were

210
00:14:29,940 --> 00:14:33,140
how they on

211
00:15:08,010 --> 00:15:10,670
and it grows

212
00:15:10,800 --> 00:15:13,640
o o

213
00:15:14,150 --> 00:15:17,180
i mean

214
00:15:24,120 --> 00:15:30,140
i think i'm right

215
00:15:30,300 --> 00:15:38,780
they are

216
00:15:42,880 --> 00:15:45,920
i agree with

217
00:15:52,330 --> 00:15:54,970
will be

218
00:16:34,960 --> 00:16:39,250
all you do that

219
00:16:46,940 --> 00:16:50,200
the right

220
00:16:50,870 --> 00:16:54,820
in are

221
00:16:54,830 --> 00:16:57,660
these are called

222
00:17:33,530 --> 00:17:38,360
they are

223
00:18:00,100 --> 00:18:05,390
they are

224
00:18:07,360 --> 00:18:10,660
the plan being

225
00:18:49,610 --> 00:18:52,570
and there

226
00:18:52,720 --> 00:18:55,590
that we

227
00:18:55,590 --> 00:19:00,240
forty eight dimensional space but i rotated all rotations of the sixties and what we're

228
00:19:03,410 --> 00:19:08,890
three hundred sixty six is rotating at different points so

229
00:19:08,940 --> 00:19:11,060
the interesting question is well

230
00:19:11,070 --> 00:19:16,100
should we be trying to find these sort of space is not trying to find

231
00:19:16,170 --> 00:19:20,180
not trying to model in the original space and that's what motivates mensionality reduction in

232
00:19:20,190 --> 00:19:23,790
this case i would then say and it's clear that this dimensionality reduction has to

233
00:19:23,790 --> 00:19:27,760
be non linear because you can see that it's not a straight line so the

234
00:19:27,760 --> 00:19:33,540
relationship between the observed sixties and this thing here is this article has a circle

235
00:19:33,540 --> 00:19:39,920
because as to connect is also a little bit noisy because using these interpolation methods

236
00:19:39,920 --> 00:19:43,560
really do your image rotation you get a little bit of noise so i really

237
00:19:43,560 --> 00:19:48,620
think that's a great way of thinking about data that your data is really some

238
00:19:48,630 --> 00:19:50,430
corrupting process

239
00:19:50,460 --> 00:19:52,430
being applied to something

240
00:19:52,440 --> 00:19:58,160
and it's actually inherently low dimensional now obviously the cases six is pure rotation is

241
00:19:58,160 --> 00:19:58,870
just too simple

242
00:19:59,320 --> 00:20:03,460
you are you need several distortions so that it could be thinned and translated and

243
00:20:03,470 --> 00:20:05,420
rotated but

244
00:20:05,450 --> 00:20:08,810
i think it's always the definition if your data has any structure if it were

245
00:20:08,810 --> 00:20:13,230
true that your dataset was covering the three thousand six hundred forty eight dimensional space

246
00:20:13,700 --> 00:20:15,800
that's happened then

247
00:20:15,800 --> 00:20:19,510
just put your hand down and go home because you'll never model in that space

248
00:20:19,530 --> 00:20:23,830
so that's the curse ofdimensionality you one model in that space but that space as

249
00:20:23,830 --> 00:20:27,340
far as they concern is just noise the structure itself will be in a low

250
00:20:27,370 --> 00:20:31,130
dimensional space so we're interested in

251
00:20:31,150 --> 00:20:35,950
reducing the management of data the lasts of algorithm i want to briefly mention this

252
00:20:38,020 --> 00:20:40,780
OK so

253
00:20:40,820 --> 00:20:44,210
i don't know if i've got probably enough time to go this in detail so

254
00:20:46,510 --> 00:20:51,170
one way of representing the status of is unsupervised data now we just got why

255
00:20:51,190 --> 00:20:52,940
we don't have

256
00:20:52,970 --> 00:20:56,850
i know that was the label before we don't have anything but wise but why

257
00:20:56,850 --> 00:20:58,150
is now i data so

258
00:20:58,630 --> 00:21:02,400
one way of modelling the state and this comes from statistics is just to measure

259
00:21:02,410 --> 00:21:06,930
interpoint distances distances between each of these data points so

260
00:21:06,940 --> 00:21:08,850
we can represent our data

261
00:21:08,870 --> 00:21:13,880
by these distances so for any dataset we can then display that basis matrix d

262
00:21:13,880 --> 00:21:16,420
and what i wanted to show you is is that

263
00:21:16,440 --> 00:21:20,320
matrix has a lot of structure so this is just the distance between

264
00:21:20,360 --> 00:21:21,560
two points

265
00:21:21,570 --> 00:21:26,950
in of those rotated six status is being visualizes matrix so i did one degree

266
00:21:26,950 --> 00:21:30,660
rotations and the three hundred sixty data points what you should see

267
00:21:30,680 --> 00:21:34,960
is this interesting structure actually benefits to find the we should be able to might

268
00:21:34,960 --> 00:21:39,840
make out these boxes these boxes those rotations which are ninety degrees so that easy

269
00:21:39,840 --> 00:21:43,460
to do so there's no noise and you see that structure on here you see

270
00:21:43,460 --> 00:21:48,520
the fact that there's snow distance between three hundred sixty eight point and the zero

271
00:21:48,520 --> 00:21:52,970
point so blue is low distance radius high seas saying that structure and here so

272
00:21:53,010 --> 00:21:56,560
another way of representing data as the distance matrix

273
00:21:56,600 --> 00:21:57,550
so i want to do

274
00:21:57,580 --> 00:21:59,190
dimensionality reduction

275
00:21:59,220 --> 00:22:02,360
one thing i could do

276
00:22:02,360 --> 00:22:03,970
i really don't think would be

277
00:22:03,990 --> 00:22:08,290
OK so i want to is find a configuration of points

278
00:22:09,790 --> 00:22:13,100
that such that the distances between these axes

279
00:22:13,120 --> 00:22:14,450
look similar to this

280
00:22:14,470 --> 00:22:18,740
now really don't way of doing that would be to select a bunch of columns

281
00:22:18,740 --> 00:22:22,110
from y to just extract columns from y

282
00:22:22,120 --> 00:22:25,790
and say this is my reduced space so my features and now

283
00:22:25,800 --> 00:22:28,780
actual pixel columns from y

284
00:22:29,030 --> 00:22:33,800
and use that nomination follow that way of doing that and what it turns out

285
00:22:34,140 --> 00:22:37,190
is that to minimize

286
00:22:37,240 --> 00:22:41,860
this objective function which is just looking at the squared distances

287
00:22:41,860 --> 00:22:44,970
between each of those elements and measuring the absolute difference

288
00:22:44,990 --> 00:22:47,720
to minimize this objective function

289
00:22:47,740 --> 00:22:49,150
he y

290
00:22:49,170 --> 00:22:51,320
what we need to do is to retain

291
00:22:51,330 --> 00:22:53,780
for x the columns of y

292
00:22:53,790 --> 00:22:55,760
that have the largest variance

293
00:22:55,770 --> 00:23:01,010
so this is my pictorial motivation for that i have to give you any proof

294
00:23:01,010 --> 00:23:05,270
for why that's the case the pictorial motivation so here this is the two-dimensional y

295
00:23:05,280 --> 00:23:09,410
and we want to retain one of the columns of y which is either retained

296
00:23:09,410 --> 00:23:12,650
that directional that direction the distances

297
00:23:12,720 --> 00:23:16,240
the distances between all these points if we want to retain

298
00:23:16,260 --> 00:23:21,520
the distance is that most accurately reflect the original distance we retain the ones that

299
00:23:21,540 --> 00:23:25,400
are in the variance the highest variance direction which in this case is that direction

300
00:23:25,400 --> 00:23:28,860
so that's sort of lower variance than that

301
00:23:28,880 --> 00:23:32,760
because here we go beyond four now we're going nowhere near full so basically retained

302
00:23:32,760 --> 00:23:37,120
that and now we can compute interpoint distances between all these points

303
00:23:37,180 --> 00:23:41,100
so i want to show you is what that looks like for that data

304
00:23:41,100 --> 00:23:42,800
so if you retain

305
00:23:42,810 --> 00:23:45,900
two columns

306
00:23:45,920 --> 00:23:46,890
of y

307
00:23:47,860 --> 00:23:49,430
is the structure you get

308
00:23:49,450 --> 00:23:52,190
the approximate structure you get for all

309
00:23:52,210 --> 00:23:56,190
for that distance matrix we saw before this is our approximation so it's not very

310
00:23:56,190 --> 00:24:02,150
good some diagonal properties if we retain ten

311
00:24:02,190 --> 00:24:04,240
then you can see we get

312
00:24:04,320 --> 00:24:10,650
from the little bit better but we still mostly much structure is a hundred dimensions

313
00:24:10,680 --> 00:24:14,360
and this is a thousand dimensions of and only when you get two thousand dimensions

314
00:24:15,650 --> 00:24:17,170
that you start seeing

315
00:24:17,190 --> 00:24:21,720
this structure ninety degrees you can just about see that

316
00:24:21,860 --> 00:24:24,160
so this is obviously a really crap way

317
00:24:24,180 --> 00:24:26,310
doing dimensionality reduction

318
00:24:27,350 --> 00:24:29,780
we have a very simple

319
00:24:29,790 --> 00:24:34,670
extension which is even make it on we can see some dramatic effect so what

320
00:24:34,670 --> 00:24:39,420
i want to suggest is well we can do the same thing

321
00:24:39,510 --> 00:24:41,710
but before we look at the data

322
00:24:41,730 --> 00:24:46,070
before we look at the directions to retain we can rotate

323
00:24:46,110 --> 00:24:47,600
such that

324
00:24:47,620 --> 00:24:52,850
the columns of y are the largest variance here so basically before we actually look

325
00:24:52,850 --> 00:24:54,650
at the

326
00:24:54,670 --> 00:24:59,950
columns to extract we rotate the data set to align the largest variance directions with

327
00:24:59,950 --> 00:25:04,600
the axis so that would look like this first rotate the data right now notice

328
00:25:04,600 --> 00:25:09,760
rotating the data see any the interpoint distances changing is i'm rotating if anyone spots

329
00:25:09,760 --> 00:25:12,110
any changing tell me

330
00:25:12,110 --> 00:25:14,140
sampled from loveless distribution

331
00:25:14,150 --> 00:25:18,890
and let's make sure that the distributions have the same meaning in the same variance

332
00:25:18,890 --> 00:25:20,860
so we don't want to make it too simple

333
00:25:20,970 --> 00:25:24,240
so here's our plus in distribution

334
00:25:24,270 --> 00:25:27,800
here's the goal to have the same meaning in the same variance

335
00:25:27,820 --> 00:25:33,950
you can still see some differences here in the middle here on the sides probably

336
00:25:33,950 --> 00:25:35,200
also contains

337
00:25:35,300 --> 00:25:41,950
just small so these are two distributions and now we find that function that best

338
00:25:41,970 --> 00:25:44,810
can distinguish samples from these distributions

339
00:25:44,830 --> 00:25:48,350
because the people with with this function function that

340
00:25:48,350 --> 00:25:51,120
makes this quantity here extremal

341
00:25:51,140 --> 00:25:55,160
and we do this in particular

342
00:25:55,170 --> 00:25:59,660
was kind the space in our favour precision kind the space is one of those

343
00:25:59,660 --> 00:26:03,940
internal so we just do this in in the gaussian case and it turns out

344
00:26:04,790 --> 00:26:10,210
the shortest the function the best function with unit norm in that space to distinguish

345
00:26:10,210 --> 00:26:14,040
these two samples is that red function here

346
00:26:14,050 --> 00:26:18,660
so it's a function that whenever there is a sample of points coming up in

347
00:26:18,660 --> 00:26:20,010
the middle of

348
00:26:20,060 --> 00:26:22,620
that function gives it positive values

349
00:26:22,630 --> 00:26:27,750
if point comes up somewhere in the side negative values

350
00:26:27,770 --> 00:26:32,570
and it turns out that then in the end this function tends to have larger

351
00:26:32,570 --> 00:26:34,370
values larger values for

352
00:26:34,720 --> 00:26:38,990
plus in data are and small but goes in the task in that sense the

353
00:26:38,990 --> 00:26:43,570
function distinguishes goes in plot data from each other

354
00:26:43,630 --> 00:26:48,760
and this the best function in are given reproducing kernel hilbert space which was one

355
00:26:48,760 --> 00:26:52,520
of a gaussian kernel of some with that i don't remember

356
00:26:52,580 --> 00:26:56,360
if i had changed the width of that goes kernel would still find a function

357
00:26:56,360 --> 00:27:00,360
but the function would look a bit different so effectively if i had to those

358
00:27:00,360 --> 00:27:03,440
income that's why it is my reproducing kernel hilbert space

359
00:27:03,450 --> 00:27:08,370
the generating kernel i would find the function may be which is qualitatively similar but

360
00:27:08,370 --> 00:27:12,350
it would be a smooth function may be the pumps would be a bit smaller

361
00:27:12,470 --> 00:27:15,300
on the other hand if i had chosen a linear kernel

362
00:27:15,360 --> 00:27:17,200
just the standard product

363
00:27:17,220 --> 00:27:20,000
as i kernel then

364
00:27:20,020 --> 00:27:24,460
i cannot express this function because all the functions that can be expressed using a

365
00:27:24,460 --> 00:27:30,130
linear kernel are linear functions so i can write this red function which makes sense

366
00:27:30,130 --> 00:27:35,160
because we said before with the linear kernel we can only distinguish datasets that have

367
00:27:35,160 --> 00:27:38,640
different mean and here by construction i

368
00:27:38,660 --> 00:27:43,350
i've chosen distributions that have the same meaning say even the same variance so also

369
00:27:43,350 --> 00:27:45,750
with a polynomial kernel of degree two

370
00:27:45,770 --> 00:27:50,180
i should be able to do this because the data sets have the same variance

371
00:27:50,210 --> 00:27:53,040
and maybe you can imagine that it will be hard to write this with the

372
00:27:53,040 --> 00:27:55,620
problem of the two

373
00:27:55,640 --> 00:27:56,450
so let's

374
00:27:56,480 --> 00:27:59,330
let's move on to measure sralan functions

375
00:27:59,330 --> 00:28:02,410
we like like probability distributions of measures

376
00:28:03,540 --> 00:28:06,250
how do we generalize this

377
00:28:09,770 --> 00:28:14,660
we make certain technical assumptions about expectations of

378
00:28:14,680 --> 00:28:16,870
kernel functions being finite

379
00:28:16,890 --> 00:28:21,180
don't worry about this from now one

380
00:28:21,230 --> 00:28:26,580
condition that make sure this is true is that the kernel has finite norm

381
00:28:26,640 --> 00:28:31,540
they of these individual function find nominal space don't worry about that now

382
00:28:31,600 --> 00:28:37,370
but what really define is so before i remember we had this mapping of data

383
00:28:37,370 --> 00:28:39,810
points that it into account functions

384
00:28:39,870 --> 00:28:43,410
centered on those points that takes me so now

385
00:28:43,430 --> 00:28:45,160
we simply realize that

386
00:28:48,160 --> 00:28:54,700
rather than the set of points we have a distribution over probability measures and

387
00:28:54,750 --> 00:28:57,930
again we never to kernel functions but this time we don't take the average of

388
00:28:57,930 --> 00:29:03,410
the kernel functions but we take expectation with respect to the probability measure

389
00:29:03,450 --> 00:29:06,290
so that's also some function

390
00:29:06,310 --> 00:29:10,200
in all space in these conditions make sure that it's a function that lies in

391
00:29:10,200 --> 00:29:12,810
the same reproducing kernel hilbert spaces

392
00:29:13,180 --> 00:29:17,270
we we have mapped our probability measure to

393
00:29:17,310 --> 00:29:20,430
one point in all hilbert space

394
00:29:20,500 --> 00:29:24,560
one point in the input space is a function of the hilbert space of functions

395
00:29:25,370 --> 00:29:31,330
we get the same properties as before also these two properties this

396
00:29:31,370 --> 00:29:38,040
map mean represents the operation of taking the mean of the function

397
00:29:38,060 --> 00:29:40,770
and also a large distance

398
00:29:40,770 --> 00:29:45,560
of two sets of points of two means corresponds to

399
00:29:45,580 --> 00:29:49,060
finding a function that distinguishes the samples

400
00:29:49,080 --> 00:29:53,080
so now we have the same the mapped probability measure

401
00:29:53,100 --> 00:29:54,390
it represents

402
00:29:54,390 --> 00:29:57,750
the operation of taking expectation for function

403
00:29:57,770 --> 00:30:03,180
twenty representing the sense of this it gives us a linear operation dot product for

404
00:30:03,180 --> 00:30:05,700
which i

405
00:30:24,970 --> 00:30:26,370
that is

406
00:30:39,560 --> 00:30:42,870
so well

407
00:30:44,560 --> 00:30:47,260
and the

408
00:31:04,870 --> 00:31:10,600
when twenty

409
00:31:10,740 --> 00:31:14,450
so i

410
00:31:14,490 --> 00:31:17,740
and then we show that

411
00:32:45,810 --> 00:32:49,700
and i said

412
00:33:00,990 --> 00:33:04,740
they are now

413
00:33:22,350 --> 00:33:27,870
that was

414
00:33:34,760 --> 00:33:38,830
and the

415
00:33:42,240 --> 00:33:51,740
it's a

416
00:33:51,780 --> 00:34:00,760
we can say is

417
00:34:07,560 --> 00:34:13,680
so in

418
00:34:18,600 --> 00:34:20,910
on a mission

419
00:34:27,660 --> 00:34:31,510
you have a very

420
00:34:31,530 --> 00:34:40,010
so the last thing

421
00:34:58,200 --> 00:35:12,990
the way

422
00:35:27,430 --> 00:35:30,260
i mean it

423
00:35:41,450 --> 00:35:44,720
and when reach

424
00:36:05,350 --> 00:36:10,930
and the thing

425
00:37:25,620 --> 00:37:28,950
in fact

426
00:37:35,490 --> 00:37:37,760
that is

427
00:37:59,470 --> 00:38:01,830
we have

428
00:38:32,620 --> 00:38:37,620
we use your real name or

429
00:38:43,510 --> 00:38:44,890
they are saying

430
00:38:44,910 --> 00:38:54,540
and you can see the same thing is that she

431
00:38:54,540 --> 00:38:57,190
right so this is the formula going to put this

432
00:38:58,070 --> 00:39:01,660
box because this is by far

433
00:39:01,700 --> 00:39:04,090
the most important formula today which we

434
00:39:04,140 --> 00:39:06,700
used to derive pretty much everything else

435
00:39:06,750 --> 00:39:09,420
and this is the way that we're going to be able to

436
00:39:09,470 --> 00:39:17,350
compute these numbers

437
00:39:18,260 --> 00:39:37,520
let's let's do an example

438
00:39:37,570 --> 00:39:39,020
this example

439
00:39:39,070 --> 00:39:40,910
we call this example one

440
00:39:41,620 --> 00:39:46,350
i will take the function

441
00:39:47,370 --> 00:39:51,060
which is one over here

442
00:39:51,070 --> 00:39:54,790
that's sufficiently complicated to have an interesting answer

443
00:39:54,810 --> 00:39:58,330
and sufficiently straightforward that we can

444
00:39:58,380 --> 00:39:59,870
compute the derivative

445
00:39:59,880 --> 00:40:02,560
fairly quickly

446
00:40:03,860 --> 00:40:07,270
so what is it that we're going to do here

447
00:40:07,320 --> 00:40:09,200
all we're going to do

448
00:40:09,230 --> 00:40:12,330
it is we're going to

449
00:40:12,340 --> 00:40:14,670
plug in this formula here

450
00:40:14,720 --> 00:40:17,590
for that function that that's all we're going to do

451
00:40:17,600 --> 00:40:21,880
and visually what we're accomplishing is somehow

452
00:40:21,890 --> 00:40:22,800
to take

453
00:40:22,800 --> 00:40:24,430
the hyperbole

454
00:40:24,490 --> 00:40:28,980
take a point on the hyperbola and figure out

455
00:40:30,590 --> 00:40:35,460
tangent line all right that's what we're accomplishing when we do that so accomplishing this

456
00:40:35,460 --> 00:40:38,190
geometrically but will be doing it algebraically

457
00:40:38,230 --> 00:40:40,100
so first

458
00:40:40,110 --> 00:40:41,480
we consider

459
00:40:41,500 --> 00:40:42,890
this difference

460
00:40:42,900 --> 00:40:46,250
delta over delta

461
00:40:46,330 --> 00:40:50,160
right out its formula so i have to have a place so i'm going to

462
00:40:50,160 --> 00:40:51,730
make it again above

463
00:40:51,750 --> 00:40:57,030
this point x zero which is the general point will make the general calculation

464
00:40:57,030 --> 00:41:00,170
so the value of f at the top

465
00:41:00,220 --> 00:41:03,280
when we move to the right by f of x i just read off from

466
00:41:04,450 --> 00:41:07,220
read off from here

467
00:41:07,940 --> 00:41:09,630
the formula

468
00:41:09,670 --> 00:41:12,530
the first thing i get here is one

469
00:41:12,530 --> 00:41:14,820
zero one

470
00:41:15,990 --> 00:41:20,100
the left-hand turn minus one over

471
00:41:20,100 --> 00:41:25,380
the right term and i have to divide that by delta

472
00:41:25,450 --> 00:41:27,960
OK so here's our

473
00:41:29,140 --> 00:41:30,530
and by the way

474
00:41:30,540 --> 00:41:32,530
this has the name

475
00:41:32,580 --> 00:41:34,140
this thing is called

476
00:41:34,180 --> 00:41:35,880
a different

477
00:41:36,080 --> 00:41:46,030
it's pretty complicated because there's always the difference in in the numerator and in disguise

478
00:41:46,030 --> 00:41:49,520
the denominator is the difference because the difference between the value

479
00:41:49,630 --> 00:41:50,400
on the

480
00:41:50,420 --> 00:41:51,700
right side

481
00:41:51,710 --> 00:41:57,140
and the value on the left side here

482
00:41:58,270 --> 00:42:01,190
so now

483
00:42:01,210 --> 00:42:05,870
we're going to simplify it by some algebra

484
00:42:05,910 --> 00:42:09,070
so let's just take a look so this is equal to o

485
00:42:09,070 --> 00:42:11,140
continue on the next

486
00:42:11,150 --> 00:42:12,420
level here

487
00:42:12,460 --> 00:42:14,870
this is equal to one over delta

488
00:42:16,310 --> 00:42:20,310
all i'm going to do is put over common denominator

489
00:42:20,360 --> 00:42:22,530
so the the common denominator

490
00:42:22,610 --> 00:42:25,710
is x zero mostel to act

491
00:42:25,730 --> 00:42:27,610
times here

492
00:42:27,700 --> 00:42:32,020
so in the numerator for the first expression i have a zero

493
00:42:32,020 --> 00:42:35,490
for the second expression axial delta x

494
00:42:35,550 --> 00:42:37,790
so this is

495
00:42:37,830 --> 00:42:42,070
the same thing as i had in the numerator before factoring out the denominator

496
00:42:42,120 --> 00:42:45,200
and here i put that numerator into

497
00:42:46,880 --> 00:42:49,350
amenable will form and now

498
00:42:49,420 --> 00:42:50,140
there are

499
00:42:50,150 --> 00:42:55,100
two basic cancellations the first one is the x zero and axl zero council

500
00:42:55,130 --> 00:42:56,930
so we have this

501
00:42:57,140 --> 00:43:08,100
and then the second step is that these

502
00:43:08,820 --> 00:43:10,250
expressions can so

503
00:43:10,270 --> 00:43:14,700
right the numerator and denominator now we have a cancellation that we can make use

504
00:43:14,700 --> 00:43:16,240
of so

505
00:43:16,340 --> 00:43:19,910
all right that under here

506
00:43:19,960 --> 00:43:20,910
and this is

507
00:43:21,100 --> 00:43:25,240
equals minus one over a zero

508
00:43:26,980 --> 00:43:28,890
time zero

509
00:43:28,910 --> 00:43:31,190
and then the very last step

510
00:43:31,230 --> 00:43:33,280
is to take the limit

511
00:43:33,280 --> 00:43:36,640
as delta x tends to zero

512
00:43:39,870 --> 00:43:42,770
now we can do it before we couldn't do it y

513
00:43:42,780 --> 00:43:46,490
because the numerator and the denominator gave us zero over zero

514
00:43:46,490 --> 00:43:47,620
now the mean

515
00:43:47,640 --> 00:43:50,450
just this expression here

516
00:43:50,450 --> 00:43:52,390
so that's really just the

517
00:43:52,390 --> 00:43:55,030
that's the shorthand for saying well this is

518
00:43:55,070 --> 00:43:56,450
the vector

519
00:43:56,450 --> 00:43:59,220
of k of x sin x

520
00:43:59,280 --> 00:44:02,240
for all those locations here

521
00:44:03,010 --> 00:44:06,010
rip it out for one point in time

522
00:44:06,030 --> 00:44:08,760
the inverse covariance matrix

523
00:44:08,950 --> 00:44:10,590
my observations

524
00:44:10,700 --> 00:44:13,680
the thing is i can precompute this part

525
00:44:13,720 --> 00:44:16,660
and afterwards only have to take an inner product between

526
00:44:16,720 --> 00:44:18,640
the vector fixed

527
00:44:18,760 --> 00:44:21,660
something that depends on the location

528
00:44:21,680 --> 00:44:24,160
that's the expensive part all

529
00:44:24,160 --> 00:44:28,390
do not invert the matrix explicitly if you can avoid it or at least not

530
00:44:28,390 --> 00:44:31,010
if you've got problems lies in two thousand

531
00:44:31,070 --> 00:44:32,680
it will be slow

532
00:44:32,780 --> 00:44:35,570
is it something like conjugate gradient

533
00:44:36,720 --> 00:44:40,840
incomplete cholesky factorizations of things like that

534
00:44:40,840 --> 00:44:43,740
otherwise just wait expensive

535
00:44:43,740 --> 00:44:45,430
is the variance

536
00:44:45,490 --> 00:44:48,590
exactly the same expression is what we had before

537
00:44:48,660 --> 00:44:50,870
this the covariance matrix

538
00:44:50,890 --> 00:44:54,780
this is basically should compliment for scanner

539
00:44:54,820 --> 00:44:58,910
this we undermined and also we get k x x plus in this where

540
00:44:58,930 --> 00:45:01,240
it's just the additive noise component

541
00:45:01,240 --> 00:45:04,910
and here's my parents correction by having observed the rest

542
00:45:04,990 --> 00:45:08,930
now the interesting bit is this does not depend on the white

543
00:45:08,950 --> 00:45:10,700
this is actually a big criticism

544
00:45:10,700 --> 00:45:12,490
garcia processes

545
00:45:12,530 --> 00:45:14,370
custom process regression

546
00:45:15,220 --> 00:45:20,260
my actual observations will not change my view of how unreliable data is

547
00:45:20,340 --> 00:45:21,850
there are ways around this

548
00:45:21,870 --> 00:45:23,240
why essentially

549
00:45:23,390 --> 00:45:26,800
design heteroskedastic casting processes

550
00:45:26,840 --> 00:45:30,800
so if you want to know more about that this paper was if can use

551
00:45:31,090 --> 00:45:32,430
when we did this

552
00:45:32,430 --> 00:45:36,410
the optimisation is a lot less trivial so it's not just a simple matrix inversion

553
00:45:36,410 --> 00:45:40,470
anymore but it can be done

554
00:45:40,470 --> 00:45:41,870
the case

555
00:45:42,280 --> 00:45:44,640
if you do that

556
00:45:44,680 --> 00:45:46,910
it will typically actually work really well

557
00:45:47,030 --> 00:45:48,490
what you get is

558
00:45:53,510 --> 00:45:55,410
well first of all of course the point

559
00:45:55,490 --> 00:45:58,120
well typically by using the boundary

560
00:45:58,220 --> 00:46:03,510
but you can see all here we're not so much about my parents goes up

561
00:46:03,550 --> 00:46:07,740
this red line by the way the variance itself and also at the boundary

562
00:46:07,760 --> 00:46:09,910
right i don't have any further that over here

563
00:46:09,910 --> 00:46:11,990
i get some additional variance

564
00:46:12,030 --> 00:46:15,050
this is because while at the bottom line

565
00:46:15,120 --> 00:46:18,010
i don't know how to continue things properly

566
00:46:18,070 --> 00:46:20,840
and that's where the variance goes up

567
00:46:20,840 --> 00:46:22,660
now let's see how well we did

568
00:46:22,720 --> 00:46:25,120
OK with a fairly well so overall

569
00:46:25,140 --> 00:46:27,700
in most regions stayed within

570
00:46:27,760 --> 00:46:29,950
one thing single variant

571
00:46:30,030 --> 00:46:32,610
it's actually quite handy if you have two

572
00:46:32,640 --> 00:46:37,740
the practical implementation and you actually need to be able to make predictions of

573
00:46:39,390 --> 00:46:43,530
well at least it will tell you how uncertain things might be

574
00:46:43,550 --> 00:46:48,240
this is ok as long as surprise

575
00:46:49,550 --> 00:46:54,450
any questions so far

576
00:46:54,450 --> 00:47:00,010
here's another example

577
00:47:00,050 --> 00:47:03,140
and well OK sure it didn't work

578
00:47:03,200 --> 00:47:07,200
quite that will be found in any points here so you can see that

579
00:47:07,220 --> 00:47:10,390
the predictive variance here is really high

580
00:47:10,430 --> 00:47:14,180
all this casting process is simply saying this will happen but served only that the

581
00:47:14,180 --> 00:47:17,720
here so i really don't know what's going on

582
00:47:17,760 --> 00:47:19,620
rest over there

583
00:47:19,660 --> 00:47:21,340
lot of thought

584
00:47:21,350 --> 00:47:23,570
i know pretty well what's going on

585
00:47:23,620 --> 00:47:25,870
this is when you can see actually this

586
00:47:25,930 --> 00:47:27,890
that is fairly noisy but

587
00:47:28,450 --> 00:47:32,300
consideration of the various doesn't really change of for instance here i mean it should

588
00:47:32,300 --> 00:47:37,280
probably assuming that the points are really quite at the boundary it should actually probably

589
00:47:37,280 --> 00:47:39,890
increase the predictive variance estimates here

590
00:47:39,930 --> 00:47:40,780
but since

591
00:47:40,780 --> 00:47:42,600
the approximate answer

592
00:47:42,620 --> 00:47:45,390
this combination of

593
00:47:45,390 --> 00:47:48,280
a trial

594
00:47:49,890 --> 00:47:55,030
yes that's right it's nothing to do with that CMS would i be better i

595
00:47:55,030 --> 00:47:57,370
would be better to call these

596
00:47:57,410 --> 00:48:02,330
unknowns are that would actually be a lot better

597
00:48:02,490 --> 00:48:04,390
should i could i

598
00:48:04,450 --> 00:48:09,220
it would it would be a lot better is to be

599
00:48:09,220 --> 00:48:13,640
to be a for the u i to be there

600
00:48:13,660 --> 00:48:15,200
the weights

601
00:48:15,220 --> 00:48:16,410
of the feet

602
00:48:16,410 --> 00:48:17,760
why is that better

603
00:48:17,760 --> 00:48:20,800
because finite elements actually has this wonderful property

604
00:48:21,640 --> 00:48:23,410
these guys

605
00:48:24,200 --> 00:48:27,220
you would have been handled better there

606
00:48:28,910 --> 00:48:30,350
because those

607
00:48:30,390 --> 00:48:34,870
because use are actually the values of my approximate

608
00:48:34,930 --> 00:48:38,280
so you could say this is you the approximate guide

609
00:48:38,300 --> 00:48:41,070
and we hope it's true close to the

610
00:48:41,120 --> 00:48:43,120
to the real one

611
00:48:43,160 --> 00:48:47,620
you see what we did we minimize or we worked on over this finite dimensional

612
00:48:49,870 --> 00:48:52,680
so we arrive at a finite problem

613
00:48:52,680 --> 00:48:55,410
OK you weep left

614
00:48:55,430 --> 00:48:58,470
instead of differential equation

615
00:48:58,530 --> 00:49:03,720
instead of the differential equation we got to a finite matrix equation

616
00:49:03,720 --> 00:49:09,070
but the job was to figure out the matrix so that's the stiffness matrix

617
00:49:09,120 --> 00:49:11,760
and this is the load vector

618
00:49:12,680 --> 00:49:16,370
and to see what k is the the can i just say what

619
00:49:16,410 --> 00:49:21,100
what's the typical member of k

620
00:49:21,140 --> 00:49:22,300
and then

621
00:49:22,320 --> 00:49:24,070
you've really

622
00:49:24,120 --> 00:49:25,430
been a long

623
00:49:25,740 --> 00:49:27,870
they are not going to

624
00:49:27,870 --> 00:49:30,030
go over on this but what's that

625
00:49:30,070 --> 00:49:31,780
what's typical

626
00:49:31,800 --> 00:49:36,030
what's the i j entry of this matrix k

627
00:49:36,030 --> 00:49:37,910
it will be

628
00:49:37,930 --> 00:49:42,570
it comes from this is the angle of cmx

629
00:49:50,320 --> 00:49:55,120
so i j

630
00:49:55,140 --> 00:49:57,450
like into the weak form

631
00:49:57,470 --> 00:50:04,120
and the k part comes from the quadratic part what's the what's the load

632
00:50:07,450 --> 00:50:10,880
it would be the end will come from the linear park it will be in

633
00:50:10,880 --> 00:50:13,140
the middle of sign

634
00:50:15,970 --> 00:50:21,370
as is the side k

635
00:50:21,370 --> 00:50:23,090
yeah i think

636
00:50:23,100 --> 00:50:24,430
times f of x

637
00:50:31,780 --> 00:50:34,660
all i've done is plug in

638
00:50:34,680 --> 00:50:35,570
the u

639
00:50:39,160 --> 00:50:41,490
and the yves

640
00:50:46,510 --> 00:50:49,300
ah the article

641
00:50:49,430 --> 00:50:53,280
was these in rules that that are involved in assembling that

642
00:50:53,510 --> 00:50:57,970
in producing the main

643
00:50:57,990 --> 00:51:01,850
and hopefully those enrolled are not hard i mean how would you actually do this

644
00:51:03,030 --> 00:51:07,180
what's what is the fee i dx

645
00:51:07,200 --> 00:51:11,030
this is a piecewise constant just too little constant over a little piece of the

646
00:51:12,410 --> 00:51:14,320
and what's all

647
00:51:14,390 --> 00:51:19,030
what's what's the derivative let's take the feces and the size of the thing

648
00:51:19,050 --> 00:51:23,800
so this will be another piece wise constant

649
00:51:23,850 --> 00:51:26,470
OK so what's up

650
00:51:26,550 --> 00:51:30,050
what kind of of matrix k of going have here

651
00:51:30,090 --> 00:51:34,930
well why you got it you got it the first time

652
00:51:34,950 --> 00:51:39,260
absolutely i thought what i have great and i thought i haven't made clear enough

653
00:51:39,260 --> 00:51:40,680
but it

654
00:51:40,740 --> 00:51:44,830
it's true why is it right i go there i have the

655
00:51:44,950 --> 00:51:51,220
because they only ride this guy only overlaps with one if i is it different

656
00:51:51,220 --> 00:51:53,890
from today by more than one

657
00:51:53,950 --> 00:51:56,090
there's no overlap

658
00:51:56,140 --> 00:51:59,430
so that the local basis paid

659
00:51:59,450 --> 00:52:01,820
they were quite orthogonal

660
00:52:03,220 --> 00:52:05,180
this guy

661
00:52:05,200 --> 00:52:07,780
and this guy do overlap

662
00:52:07,800 --> 00:52:10,390
but it try diagonal and that

663
00:52:10,410 --> 00:52:11,720
that's fine

664
00:52:11,780 --> 00:52:16,550
i mean if everything was orthogonal then we whatever diagonal matrix

665
00:52:16,600 --> 00:52:19,320
because all the other products would give zero

666
00:52:19,330 --> 00:52:23,740
but that's try diagonal is just as good

667
00:52:24,050 --> 00:52:25,700
that's OK

668
00:52:25,700 --> 00:52:26,470
so that

669
00:52:26,470 --> 00:52:27,640
so please

670
00:52:27,640 --> 00:52:32,350
it's these calculations that are now

671
00:52:32,370 --> 00:52:39,260
what of what are we going to do with this effect

672
00:52:39,280 --> 00:52:42,870
are we can actually do the integral exactly

673
00:52:42,870 --> 00:52:44,100
probably not

674
00:52:44,120 --> 00:52:46,950
if c and if we do have some variable

675
00:52:48,600 --> 00:52:51,850
so the the of x really depends on the

676
00:52:51,870 --> 00:52:53,680
the conductivity or

677
00:52:53,680 --> 00:52:55,490
or whatever that

678
00:52:55,550 --> 00:53:00,740
physically represents if it's if it's changing continuously hey we're not going to we will

679
00:53:00,740 --> 00:53:05,010
approximate that by piecewise constant or piecewise linear some

680
00:53:05,050 --> 00:53:09,470
we all we want is a good approximation to those involved

681
00:53:09,470 --> 00:53:11,450
these and these

682
00:53:11,470 --> 00:53:16,180
then we would solve k equal f and we would have a darn good approximation

683
00:53:16,240 --> 00:53:18,300
that's the

684
00:53:19,470 --> 00:53:24,010
the whole idea of finite element was making this kind of a choice

685
00:53:25,070 --> 00:53:31,350
that's really real and then creating the

686
00:53:31,370 --> 00:53:34,350
a million dollar systems that would do it

687
00:53:35,010 --> 00:53:38,280
do all the calculations for

688
00:53:39,220 --> 00:53:40,350
can i

689
00:53:43,350 --> 00:53:49,640
ask you to move for these last move into two dimensions

690
00:53:49,660 --> 00:53:52,590
suppose we had a partial differential equation

691
00:53:52,640 --> 00:53:55,050
these are double integrals

692
00:53:55,070 --> 00:53:58,550
p x p y

693
00:53:58,550 --> 00:53:59,820
and there would be

694
00:54:00,240 --> 00:54:03,280
there will be a wide derivatives in there

695
00:54:03,300 --> 00:54:09,760
are functions are functions now of x and y

696
00:54:09,820 --> 00:54:12,450
can you invent finite element

697
00:54:12,510 --> 00:54:15,450
in two d

698
00:54:15,490 --> 00:54:22,100
what's good what's what's an invention of two-dimensional finite element these were

699
00:54:22,140 --> 00:54:25,200
great one-dimensional finite l

700
00:54:25,260 --> 00:54:29,070
this one the next one and the next one an extra now let me draw

701
00:54:29,070 --> 00:54:31,140
drawn to the region

702
00:54:33,890 --> 00:54:36,820
from some two miles from two dimensional region

703
00:54:36,850 --> 00:54:39,550
might be an air aircraft

704
00:54:39,570 --> 00:54:44,050
surface or something

705
00:54:46,350 --> 00:54:48,530
right here and here you would

706
00:54:48,530 --> 00:54:52,010
this you would now here's the beauty of it the

707
00:54:52,030 --> 00:54:56,260
i don't have the use equally spaced mesh orders

708
00:54:56,260 --> 00:54:58,160
really kind of mesh

709
00:54:58,180 --> 00:55:02,680
it it's the edge of the that's the surface of the plane and the nice

710
00:55:03,640 --> 00:55:08,160
or maybe where i expect shockwaves up put in a whole bunch of

711
00:55:08,240 --> 00:55:10,070
little element

712
00:55:10,070 --> 00:55:16,500
so this basically it's empirical result substantiating the theorem which is that for this parameter

713
00:55:16,500 --> 00:55:20,570
getting larger and larger and larger beyond some point is rescaled

714
00:55:20,650 --> 00:55:26,670
parameter and divided by something beyond some point the probability of support recovery being correct

715
00:55:26,750 --> 00:55:29,190
goes to one below that it goes to zero

716
00:55:30,070 --> 00:55:34,460
and this is done for number of dimensions growing and we see that bites by

717
00:55:34,460 --> 00:55:37,150
scaling things in this way is log of p

718
00:55:37,290 --> 00:55:41,590
basically the dependence vanishes we get these curves lie on top of each other

719
00:55:41,610 --> 00:55:46,360
so this shows that we actually have the right rescaling and log p is great

720
00:55:46,980 --> 00:55:49,540
he says that you know p can grow

721
00:55:49,560 --> 00:55:50,710
very quickly

722
00:55:50,730 --> 00:55:52,000
relative to an

723
00:55:52,000 --> 00:55:55,570
and you still get support recovery exact support recovery

724
00:55:58,440 --> 00:56:02,630
this very different so angle here is the

725
00:56:02,710 --> 00:56:06,290
angle this has to do with the support overlapped function

726
00:56:06,300 --> 00:56:11,020
so we did this under different regimes for that choice for the particular divine matrix

727
00:56:11,070 --> 00:56:14,000
and what is the support overlap function

728
00:56:14,020 --> 00:56:18,630
well what you do is you take it some facts about the the the the

729
00:56:18,630 --> 00:56:22,980
the parameter matrix OK so if you look at the

730
00:56:23,110 --> 00:56:25,710
the gradient of this

731
00:56:25,790 --> 00:56:28,210
l one l two norm on b

732
00:56:28,340 --> 00:56:31,690
you know you you use that to form a gradient matrix

733
00:56:31,730 --> 00:56:33,730
you normalized that

734
00:56:34,270 --> 00:56:40,570
or the equivalent and normalizing the former certain gram matrix where the gradient times that

735
00:56:41,230 --> 00:56:46,520
the covariance of the part of the x matrix that has nonzero coefficients

736
00:56:46,610 --> 00:56:52,960
and in this function i was talking about here is the maximum like value of

737
00:56:52,960 --> 00:56:55,270
of this matrix

738
00:56:55,290 --> 00:56:59,880
OK so it's as if the quantity can calculate in simulations and

739
00:56:59,940 --> 00:57:01,290
that's what it is

740
00:57:01,300 --> 00:57:03,960
OK so long if you want to spend more time with the paper you can

741
00:57:04,400 --> 00:57:08,000
do that we go through very special cases of calculating this and see how much

742
00:57:08,000 --> 00:57:11,110
overlap we get and therefore how much we're dividing that

743
00:57:11,150 --> 00:57:13,670
scaling factor by

744
00:57:13,710 --> 00:57:17,380
OK so i'm not going to get into the here just to get different examples

745
00:57:17,380 --> 00:57:22,230
of choosing this part just choosing coefficient matrices

746
00:57:22,250 --> 00:57:24,590
and getting different sparsity overlap

747
00:57:24,610 --> 00:57:31,670
so there some favorable cases where can have orthogonal situations if you have aligned regression

748
00:57:31,690 --> 00:57:35,150
where everybody has all the columns

749
00:57:35,170 --> 00:57:39,300
of this gradient matrix of the same you don't do any better than last so

750
00:57:39,440 --> 00:57:42,440
otherwise you do better than last shifted to the left feature

751
00:57:42,440 --> 00:57:46,920
here in a few data points to get the same problem of correct support recovery

752
00:57:46,920 --> 00:57:51,960
being correct and so and we have a simple situations in which you get you

753
00:57:51,960 --> 00:57:56,880
can get up to a factor of the number of regressions improvement

754
00:57:56,920 --> 00:57:58,730
so five k regressions

755
00:57:58,750 --> 00:58:00,020
i can actually

756
00:58:00,020 --> 00:58:04,460
i ni kefu are data points in the most favourable situations to be the the

757
00:58:04,460 --> 00:58:06,570
independent last

758
00:58:06,820 --> 00:58:11,300
we originally thought that we will never do worse than the actual as o independent

759
00:58:11,300 --> 00:58:14,590
so that turns out not to be the case so we have some kind of

760
00:58:14,610 --> 00:58:20,270
somewhat pathological situation we could do worse than so in general you improve

761
00:58:20,340 --> 00:58:26,270
OK that this is the statement that kind of care about how much you perform

762
00:58:26,290 --> 00:58:27,300
relative to the

763
00:58:27,400 --> 00:58:28,820
ordinary QP

764
00:58:28,820 --> 00:58:34,170
and arrested this particular situation where the covariance matrix of the

765
00:58:34,170 --> 00:58:38,210
that a fraction that portion of design matrix is the identity and we actually always

766
00:58:38,210 --> 00:58:42,590
improve over the independent cells but in general you that's not the case

767
00:58:44,610 --> 00:58:48,360
right the next few pages are proof sketch and i put them in the slides

768
00:58:48,360 --> 00:58:50,000
i'm not going to cover them but

769
00:58:50,070 --> 00:58:52,520
you know this is meant to be very talks if you will look through these

770
00:58:52,520 --> 00:58:57,040
slides received the techniques that have been used here it kind of is convex optimization

771
00:58:57,070 --> 00:59:03,000
meets sort of concentration inequalities that's really what's happening here gail kind gaussians

772
00:59:03,020 --> 00:59:09,360
concentration inequalities for some semantic projection arguments and again use large deviations to establish some

773
00:59:09,360 --> 00:59:13,070
kind of rate then you bring that into the optimisation story and

774
00:59:13,540 --> 00:59:17,170
well before but only other so let's skip all that

775
00:59:17,820 --> 00:59:22,590
OK so let me just briefly kind of again kind of running through all this

776
00:59:22,590 --> 00:59:27,770
just to give you a little flavor of kind of this these these ideas is

777
00:59:27,790 --> 00:59:31,610
a large literature also on sparse PCA so if we want to PCA but we

778
00:59:31,610 --> 00:59:34,710
make assumptions on the sparsity i vectors maybe we can

779
00:59:34,710 --> 00:59:38,800
use PCA in really high dimensional spaces into it probably

780
00:59:38,920 --> 00:59:42,560
so a lot of people are working on this in general if you don't have

781
00:59:42,560 --> 00:59:46,900
any sparsity assumptions you can't do it a classical result on that but if you

782
00:59:46,900 --> 00:59:52,250
have structured ensembles like sparse lichen vectors are sparse covariance matrices then you can actually

783
00:59:52,250 --> 00:59:57,290
get consistency even though he over and going to infinity and seemingly impossible but the

784
00:59:57,340 --> 00:59:59,570
true these kind of conditions

785
00:59:59,590 --> 01:00:04,290
so one of the theoretical situations where this study these were called spiked population covariance

786
01:00:04,290 --> 01:00:08,400
matrices our covariance matrix as a sum of

787
01:00:08,460 --> 01:00:13,630
rank one matrices so these are the late night i can vectors plus some

788
01:00:14,480 --> 01:00:18,570
matrix typically the identity matrix

789
01:00:18,610 --> 01:00:22,540
i then you look at different sparsity models either give hard sparsity where these i

790
01:00:22,610 --> 01:00:27,690
vectors have exactly k nonzero coefficients or you have some soft versions of that in

791
01:00:27,860 --> 01:00:30,340
you study these things theoretically

792
01:00:30,340 --> 01:00:32,840
OK so

793
01:00:32,880 --> 01:00:36,770
we have a project where we were interested in doing this first PCA using kind

794
01:00:36,770 --> 01:00:39,980
of sophisticated optimization tools implicate the SDP

795
01:00:40,000 --> 01:00:44,690
and this is kind of a pretty straightforward lactation you saw talk on convex optimisation

796
01:00:44,690 --> 01:00:49,210
you probably know how to do this to remind you remember that the igon values

797
01:00:49,400 --> 01:00:52,400
according to the core of fisher variational principles got by

798
01:00:52,460 --> 01:00:56,270
maximizing over the ball of this rail a kind of

799
01:00:57,300 --> 01:01:02,250
and you can rewrite that problem using the

800
01:01:02,650 --> 01:01:06,090
is it a semidefinite programme just using a linear

801
01:01:06,270 --> 01:01:09,270
expressionist traces EQ

802
01:01:09,270 --> 01:01:15,290
and you do that over all simple positive definite easy that have unit trace

803
01:01:15,320 --> 01:01:18,370
OK so you may be now know how to go from there there after the

804
01:01:18,370 --> 01:01:19,940
lecture the other day

805
01:01:19,980 --> 01:01:23,300
can use this idea now than to relax

806
01:01:24,070 --> 01:01:27,420
first PCA so there's the same component pulled down there

807
01:01:27,460 --> 01:01:31,210
all right then we want to sparse PCA so we have an l one norm

808
01:01:31,400 --> 01:01:36,920
to the objective function now we optimize that this is still a semidefinite program

809
01:01:39,840 --> 01:01:42,290
OK and now

810
01:01:42,440 --> 01:01:46,650
russian mini and martin wainwright to prove the theorem about this

811
01:01:46,670 --> 01:01:49,960
that year they have the kind of the same kind of classifier and we talked

812
01:01:49,960 --> 01:01:55,400
about for the other situation which is that you actually do recover under this is

813
01:01:55,570 --> 01:02:00,340
just a simple example of just to rank one is one i conduct an identity

814
01:02:00,340 --> 01:02:05,000
one large organ vector and identity

815
01:02:05,020 --> 01:02:07,020
despite covariance matrix

816
01:02:07,070 --> 01:02:11,400
the to get exact recovery you know here's the rate

817
01:02:12,090 --> 01:02:13,880
and in that situation

818
01:02:13,900 --> 01:02:15,900
using spp

819
01:02:15,940 --> 01:02:21,360
so that was they actually theoretical result that and there are some known results on

820
01:02:22,060 --> 01:02:28,110
thresholding estimators which achieve a slower rates if q is equal to zero it's the

821
01:02:28,110 --> 01:02:33,770
same rate as before but it's slower for q not equal to zero and so

822
01:02:33,770 --> 01:02:37,400
if you know this is for the first eigen vector but you will separate organisations

823
01:02:37,420 --> 01:02:40,560
get this for the first time that the whole was

824
01:02:40,560 --> 01:02:46,690
these change in weights telescope over any therefore

825
01:02:46,710 --> 01:02:49,110
i want to find shortest paths

826
01:02:49,170 --> 01:02:52,840
you just find shortest paths in this reweighted version and then you just change it

827
01:02:52,840 --> 01:02:54,460
by this one amounts

828
01:02:54,480 --> 01:02:58,150
you subtract off this amounts to an idea that will give you the shortest path

829
01:02:58,150 --> 01:03:01,040
weights in the original weights

830
01:03:02,000 --> 01:03:03,730
so this is the tool

831
01:03:03,770 --> 01:03:06,320
we now know how to change weights in the graph

832
01:03:06,360 --> 01:03:10,460
what we really want change weights in the graph so the weights all come out

833
01:03:10,460 --> 01:03:12,150
non negative

834
01:03:12,170 --> 01:03:17,170
how do we do that why in the world with the function h

835
01:03:17,190 --> 01:03:19,540
that makes all the weight of edge weights

836
01:03:19,540 --> 01:03:21,320
making it

837
01:03:21,340 --> 01:03:23,360
it makes sense

838
01:03:23,460 --> 01:03:29,020
it turns out we already know

839
01:03:36,360 --> 01:03:41,630
you should write down this consequence

840
01:03:52,150 --> 01:04:06,790
first order so in particular the shortest path changes by this amount

841
01:04:06,840 --> 01:04:09,520
and if you want to know this value just move the stuff to the other

842
01:04:09,520 --> 01:04:13,710
side so we can compute delta some h then we can compute delta

843
01:04:14,610 --> 01:04:19,110
the consequence here how many how many people here pronounce this word

844
01:04:21,130 --> 01:04:24,590
OK how many people pronounce the corollary

845
01:04:25,820 --> 01:04:27,840
four one can also

846
01:04:27,860 --> 01:04:33,940
usually at least one other student and there that which usually canadian or british or

847
01:04:33,940 --> 01:04:35,380
something i think that's the

848
01:04:35,440 --> 01:04:40,820
access i always avoid pronouncing this would also really think core

849
01:04:42,980 --> 01:04:45,230
we say zeenat our

850
01:04:45,250 --> 01:04:46,270
we have

851
01:04:46,320 --> 01:04:49,820
so what we want to do

852
01:04:50,000 --> 01:04:53,670
is defined

853
01:04:54,290 --> 01:04:59,250
one of these functions i mean this is right that what what we could hope

854
01:04:59,250 --> 01:05:05,270
to have we want to find winning h assigns weights to each vertex such that

855
01:05:05,270 --> 01:05:06,770
w seven h

856
01:05:08,230 --> 01:05:11,590
it is nonnegative that would be great

857
01:05:11,610 --> 01:05:13,130
for all edges

858
01:05:13,150 --> 01:05:19,750
on you can be

859
01:05:19,750 --> 01:05:22,630
OK then we can run dexter

860
01:05:22,670 --> 01:05:28,150
i could run by extracting the delta h is

861
01:05:28,250 --> 01:05:33,020
and then just undo the reweighting and get what we want

862
01:05:33,040 --> 01:05:35,090
and that johnson's army

863
01:05:35,250 --> 01:05:38,210
claims that this is always possible

864
01:05:38,230 --> 01:05:44,000
why should always be possible well let's look at this construct wsum h of UV

865
01:05:44,020 --> 01:05:46,230
that's w

866
01:05:46,250 --> 01:05:47,040
view v

867
01:05:47,070 --> 01:05:49,480
was each of you

868
01:05:49,540 --> 01:05:51,400
minus HIV

869
01:05:51,420 --> 01:05:53,860
should be nonnegative

870
01:05:53,920 --> 01:05:58,190
let me really want to redraw rewrite this little bit

871
01:05:58,210 --> 01:06:00,940
what i want

872
01:06:04,340 --> 01:06:07,590
i'm gonna put these guys over here

873
01:06:07,690 --> 01:06:12,940
o thing it should be mine is a agent of view is less than or

874
01:06:12,940 --> 01:06:16,440
equal to w of movie

875
01:06:16,500 --> 01:06:17,880
i look familiar

876
01:06:18,020 --> 01:06:26,690
you get it right

877
01:06:26,690 --> 01:06:30,460
the right

878
01:06:30,480 --> 01:06:37,520
anyone seen that inequality before

879
01:06:37,540 --> 01:06:44,110
yes correct answer OK where

880
01:06:44,150 --> 01:06:46,770
previous lecture

881
01:06:46,790 --> 01:06:51,380
in the previous lecture

882
01:06:51,400 --> 01:06:55,170
what is this called

883
01:06:55,170 --> 01:07:03,770
replace each with x

884
01:07:03,820 --> 01:07:07,710
charles was good

885
01:07:07,770 --> 01:07:11,860
anyone else remember all the way back to episode two

886
01:07:11,880 --> 01:07:15,500
there's a weekend

887
01:07:15,520 --> 01:07:20,650
it has this operator called

888
01:07:20,650 --> 01:07:24,540
on subtraction by

889
01:07:24,560 --> 01:07:29,020
the courage

890
01:07:30,500 --> 01:07:33,920
and i'll tell you

891
01:07:33,940 --> 01:07:36,070
the difference constraints

892
01:07:36,090 --> 01:07:38,440
this is the difference operator

893
01:07:39,230 --> 01:07:42,040
so good friend difference constraints

894
01:07:42,060 --> 01:07:45,840
so this is what we want to satisfy we have a system of difference constraints

895
01:07:45,840 --> 01:07:50,400
hv minus attribution be we want to find these these are on this subject to

896
01:07:50,400 --> 01:07:52,960
these constraints were given the W's

897
01:07:53,730 --> 01:07:54,670
we know

898
01:07:54,690 --> 01:07:57,770
when these difference constraints are satisfiable

899
01:07:57,790 --> 01:08:03,070
can someone tell me when the constraints are satisfiable

900
01:08:03,110 --> 01:08:06,150
we know exactly one

901
01:08:06,150 --> 01:08:08,600
and then he was

902
01:08:08,600 --> 01:08:10,580
denominal verbs which

903
01:08:10,630 --> 01:08:12,150
taken down

904
01:08:12,210 --> 01:08:16,600
and the

905
01:08:16,610 --> 01:08:20,980
no it's a

906
01:08:21,000 --> 01:08:23,690
but anyway

907
01:08:23,730 --> 01:08:28,230
we need to i

908
01:08:28,250 --> 01:08:29,940
but there

909
01:08:30,010 --> 01:08:31,670
one way

910
01:08:38,540 --> 01:08:43,790
typically derivational morphology changes the part of speech

911
01:08:44,380 --> 01:08:48,690
and derivational morphology is represented in cyc with reified suffixes

912
01:08:48,690 --> 01:08:50,460
and there's a rule

913
01:08:50,480 --> 01:08:51,810
that says

914
01:08:51,840 --> 01:08:55,730
the form of the derived word is drawn by

915
01:08:55,770 --> 01:08:57,840
attaching the suffix

916
01:08:57,920 --> 01:09:00,000
string to the base form

917
01:09:00,400 --> 01:09:01,880
so you have

918
01:09:01,900 --> 01:09:05,040
reified suffixes i o

919
01:09:05,060 --> 01:09:07,860
and it will attach to

920
01:09:07,880 --> 01:09:09,750
words like sesame

921
01:09:11,670 --> 01:09:12,420
give us

922
01:09:12,440 --> 01:09:14,690
this complex four

923
01:09:14,710 --> 01:09:16,230
it was shown earlier

924
01:09:16,250 --> 01:09:19,690
which can then be given semantic interpretation

925
01:09:22,130 --> 01:09:27,960
sometimes it can be hard to decide whether something is inflectional morphology or derivational morphology

926
01:09:28,360 --> 01:09:30,710
so comparatives and superlatives were

927
01:09:30,750 --> 01:09:34,210
initially represented in the KB as inflectional

928
01:09:34,250 --> 01:09:38,730
morphology but then we had to change that because of semantics which i think is

929
01:09:38,730 --> 01:09:40,860
kind of interesting because

930
01:09:40,940 --> 01:09:45,830
things are semantics can get rid off

931
01:09:46,100 --> 01:09:48,790
so inflectional analysis

932
01:09:50,130 --> 01:09:54,730
you have tall and presidency of tall taller tallest all being forms of the same

933
01:09:54,730 --> 01:09:58,710
word which will map the addition to

934
01:09:58,840 --> 01:10:02,270
the meaning of it all the time for senators

935
01:10:02,330 --> 01:10:03,330
total prime

936
01:10:03,340 --> 01:10:06,920
and also we don't need to tell us

937
01:10:07,000 --> 01:10:08,770
but if you

938
01:10:08,920 --> 01:10:11,610
variational analysis of two different words

939
01:10:11,650 --> 01:10:13,360
this will be from tall

940
01:10:14,020 --> 01:10:15,940
table b for this work

941
01:10:15,960 --> 01:10:16,690
and the know

942
01:10:16,750 --> 01:10:20,040
separate concepts which you need

943
01:10:20,060 --> 01:10:25,650
for interpreting queries like patients heavier than one hundred fifty pounds which we have this

944
01:10:26,880 --> 01:10:30,610
where doctors are trying to get access to these medical records and

945
01:10:31,880 --> 01:10:36,310
could well make this kind of query you know

946
01:10:36,400 --> 01:10:39,460
give me patients whose weight exceeds

947
01:10:39,600 --> 01:10:43,830
eighty pounds and we don't want to interpret that as

948
01:10:43,900 --> 01:10:45,920
given the patients who are heavy

949
01:10:46,040 --> 01:10:48,440
we want the comparative

950
01:10:49,460 --> 01:10:51,520
if there is a comparative analysis

951
01:10:52,020 --> 01:10:56,230
so we have to represent derivational morphology

952
01:10:56,710 --> 01:11:01,020
here's an example of the generative rules

953
01:11:01,040 --> 01:11:02,210
gives the

954
01:11:02,230 --> 01:11:04,360
semantics were compared

955
01:11:04,360 --> 01:11:07,440
give semantics for the actor

956
01:11:07,630 --> 01:11:12,400
unfortunately it doesn't have

957
01:11:12,400 --> 01:11:14,480
OK if the coverage is not

958
01:11:14,520 --> 01:11:17,830
very good this is sort of just the proof of concept is just we're just

959
01:11:17,830 --> 01:11:21,310
at the beginning stages of

960
01:11:21,400 --> 01:11:23,250
working on this kind of rule

961
01:11:23,270 --> 01:11:26,650
but the idea is that you take

962
01:11:26,670 --> 01:11:28,940
an adjective that means are very low

963
01:11:28,940 --> 01:11:30,960
the low amount of something

964
01:11:30,980 --> 01:11:33,330
and then

965
01:11:33,360 --> 01:11:37,420
the transfer translation of the derived adjective

966
01:11:37,440 --> 01:11:39,000
well mean

967
01:11:44,110 --> 01:11:45,750
i think

968
01:11:45,750 --> 01:11:48,340
maybe it's better to give an example

969
01:11:49,790 --> 01:11:53,860
so little earth

970
01:11:53,880 --> 01:11:55,150
it's something

971
01:11:55,230 --> 01:11:56,540
that means

972
01:11:56,710 --> 01:12:02,900
a little over x means

973
01:12:03,000 --> 01:12:04,880
so seven

974
01:12:04,920 --> 01:12:05,560
a little

975
01:12:05,610 --> 01:12:08,000
x littler then y

976
01:12:08,000 --> 01:12:11,060
it means that

977
01:12:11,170 --> 01:12:14,400
that's still not a good thing for variables

978
01:12:14,460 --> 01:12:18,730
all o

979
01:12:19,830 --> 01:12:21,730
a little earlier than b

980
01:12:21,830 --> 01:12:26,860
can see that there are some obvious that there is a value x

981
01:12:26,880 --> 01:12:30,940
which is less than the value denoted by the

982
01:12:31,000 --> 01:12:33,980
public object to the

983
01:12:34,000 --> 01:12:36,710
o thing in the van pp

984
01:12:36,730 --> 01:12:39,860
and the size parameter of the object is

985
01:12:39,900 --> 01:12:42,360
this value so in a

986
01:12:42,360 --> 01:12:43,420
so i'm trying to let's say

987
01:12:43,840 --> 01:12:48,740
try to figure out what this particularity is going to be annasee's coordinating so i'm

988
01:12:48,750 --> 01:12:52,630
subtracting the mean divided by a standard deviation so i'm handling looking at a real

989
01:12:52,630 --> 01:12:53,500
number over here

990
01:12:54,870 --> 01:12:56,070
so this is well

991
01:12:56,670 --> 01:13:02,880
if i'm doing a matrix factorization you know i'm going to factorize acts as you'll be transpose so

992
01:13:03,430 --> 01:13:07,670
the corresponding to be surrounded by a factor u i i its role

993
01:13:08,120 --> 01:13:10,790
and the effective immediately responding to this column

994
01:13:11,830 --> 01:13:13,490
my probabilistic model is

995
01:13:14,010 --> 01:13:14,960
that exciting

996
01:13:15,540 --> 01:13:17,920
is being sampled from a gaussian distribution

997
01:13:18,590 --> 01:13:21,100
was mean you why transpose vijay

998
01:13:21,530 --> 01:13:23,250
and with some standard deviation sigma squared

999
01:13:25,680 --> 01:13:28,330
um and then that's the more right so

1000
01:13:29,370 --> 01:13:31,460
and then i have a factor like this

1001
01:13:32,000 --> 01:13:33,010
four each wrong

1002
01:13:33,660 --> 01:13:40,630
and i'm assuming these factors you why themselves are being sampled from another multivariate gaussian distribution

1003
01:13:41,500 --> 01:13:46,990
with zero mean and some diagonal covariance and similarly the problem factors are being sampled

1004
01:13:46,990 --> 01:13:49,460
from a zero mean and diagonal covariance matrix

1005
01:13:50,220 --> 01:13:55,040
this is insanity right so you're taking a simple problem you're cooking things up on the fly

1006
01:13:55,540 --> 01:13:56,470
we love doing this

1007
01:13:57,830 --> 01:14:01,220
but you have to show that this walks and i can assure these are some

1008
01:14:01,220 --> 01:14:05,380
of the most popular methods we have got or disposal and this doesn't fit the

1009
01:14:05,380 --> 01:14:07,750
usual machine learning framework of where you're doing

1010
01:14:08,170 --> 01:14:09,330
clustering or classification

1011
01:14:11,250 --> 01:14:16,760
this is the graphical model structure is a generative model and this is how it so so they actually showed

1012
01:14:17,290 --> 01:14:18,450
fantastic results

1013
01:14:19,130 --> 01:14:20,750
back in two thousand six seven

1014
01:14:21,630 --> 01:14:24,090
in the netflix leaderboard some of these discussions

1015
01:14:24,620 --> 01:14:28,210
that they are doing pretty well to to register and said it was a different

1016
01:14:28,210 --> 01:14:30,260
one we just use gradient descent to do the

1017
01:14:31,130 --> 01:14:33,120
inference figuring out what those factors are

1018
01:14:33,920 --> 01:14:36,200
they made one more step this at all

1019
01:14:36,660 --> 01:14:38,120
i have i am using

1020
01:14:38,500 --> 01:14:43,320
a zero mean diagonal covariance gosh and why should i be doing that's should i

1021
01:14:43,320 --> 01:14:46,290
put some non zero mean and some general covariance

1022
01:14:46,750 --> 01:14:50,130
so that that's so this is the prior over the latent factors

1023
01:14:50,590 --> 01:14:56,710
well if you've observed are the small fraction of ratings in this matrix everything else

1024
01:14:56,800 --> 01:15:01,320
is latent variables and remember the example was that one latent variable heart disease

1025
01:15:01,830 --> 01:15:03,410
and this is pretty much everything is latent

1026
01:15:03,970 --> 01:15:09,870
so you you're putting russian distribution with mean mu and precision matrix lambda

1027
01:15:11,470 --> 01:15:17,450
and then putting a wishart prior gaussian wishart on top of these parameters that's that's it's one more layer

1028
01:15:19,080 --> 01:15:23,630
and this almost looks scary somebody who has not actually played with these models we're

1029
01:15:23,630 --> 01:15:27,870
starting with a sparsely filled matrix and we are talking about this story line on

1030
01:15:27,870 --> 01:15:28,450
each side

1031
01:15:28,930 --> 01:15:35,860
by putting latent factors being generated from russian distributions whose parameters are being generated from some unknown distribution

1032
01:15:36,380 --> 01:15:38,920
and so on but you know that's the route they took

1033
01:15:39,740 --> 01:15:41,830
and then i'm going to show you some results

1034
01:15:42,450 --> 01:15:44,460
so this is the sort of the full the full model

1035
01:15:47,140 --> 01:15:51,500
we we added a few things to it but it's not very major it's essentially

1036
01:15:51,670 --> 01:15:53,260
this model is actually very fast

1037
01:15:53,760 --> 01:15:57,330
well for the last model they uses embassy embassy which turned out to be extremely slow

1038
01:15:57,780 --> 01:15:58,220
and since

1039
01:15:58,670 --> 01:15:59,510
can forties

1040
01:16:00,080 --> 01:16:01,920
millions of data points problems

1041
01:16:02,330 --> 01:16:06,830
is there two models we sort of cooked up something in between which we call a parametric be map

1042
01:16:07,420 --> 01:16:10,490
not a large difference money to get into the details so

1043
01:16:11,290 --> 01:16:15,570
if you actually sort of see how they performed in comparison with something like this

1044
01:16:15,570 --> 01:16:19,170
is what the netflix baseline school this is the netflix put all the chance that

1045
01:16:19,170 --> 01:16:19,910
try to beat us

1046
01:16:20,370 --> 01:16:24,330
they can be re so there's no confusion about this this is what netflix used

1047
01:16:24,330 --> 01:16:25,540
to do the best they could have done

1048
01:16:26,090 --> 01:16:28,670
and this is sort of the four stock the pianist model

1049
01:16:29,550 --> 01:16:31,620
it's really actually starts overfitting

1050
01:16:32,320 --> 01:16:35,080
because because there is there is no regularisation

1051
01:16:35,580 --> 01:16:40,500
right there is no regularisation have a number of epochs number of passes over the data increases

1052
01:16:41,420 --> 01:16:44,580
it starts overfitting it fits the pocket sees well

1053
01:16:45,210 --> 01:16:47,820
it it fits the park it sees very well

1054
01:16:48,250 --> 01:16:50,590
so i'm seeing this five numbers and feeling it very well

1055
01:16:51,100 --> 01:16:54,580
but i'm messing up on the others and this is the lesson we have learned

1056
01:16:54,670 --> 01:16:56,670
with regularisation you should be trying to

1057
01:16:57,970 --> 01:17:01,750
the things you are observing and is very is fantastically well actually doing what he

1058
01:17:02,600 --> 01:17:05,300
warning regularisation start off and this is what a lot you'll see

1059
01:17:05,820 --> 01:17:06,420
as you go

1060
01:17:06,930 --> 01:17:09,960
but if you see the pianist models with more epochs being better

1061
01:17:11,220 --> 01:17:13,700
bayesian be map of course is a little bit more to come

1062
01:17:14,120 --> 01:17:18,870
imagine that this was a slightly better algorithm it's a little slower they have some

1063
01:17:19,100 --> 01:17:22,390
time numbers we have tried to improve them we have not managed to

1064
01:17:23,040 --> 01:17:25,100
so our model which is somewhere in between

1065
01:17:25,720 --> 01:17:27,420
um it's much faster

1066
01:17:28,080 --> 01:17:31,570
but we can outperform them but this is sort of i'm showing you a part

1067
01:17:31,570 --> 01:17:36,010
of the literature that slowly growing in them so this matrix some vision problems okay

1068
01:17:39,380 --> 01:17:40,670
we'll go back to overview

1069
01:17:42,360 --> 01:17:46,220
i give you a predictive model a lot of detail on graphical model trying to

1070
01:17:46,220 --> 01:17:49,820
convince you that these practical models which scales to millions of

1071
01:17:50,250 --> 01:17:51,720
sort of data points and features

1072
01:17:52,290 --> 01:17:55,670
and this is one of the things machine learners very excited about these days

1073
01:17:56,380 --> 01:17:58,680
other thing you really excited about is online learning

1074
01:18:00,160 --> 01:18:03,040
and this is how if you're trying to fit a support vector machine

1075
01:18:03,710 --> 01:18:05,000
on a billion points

1076
01:18:06,860 --> 01:18:09,780
you will have to use online learning there is really no other way i mean

1077
01:18:09,780 --> 01:18:12,900
sorry the number of views you

1078
01:18:13,770 --> 01:18:21,180
and yes in a still contrasted with the traditional combinations so basically whether it's combination

1079
01:18:21,180 --> 01:18:26,540
is that this guy here is just a course and which is the weight that

1080
01:18:26,540 --> 01:18:33,180
you give for the particle OK and here we have much richer structures and

1081
01:18:33,240 --> 01:18:37,740
OK so how are you are missing again you do maximum likelihood and we

1082
01:18:37,750 --> 01:18:43,910
yes you some bitterness and the priors in this case we seem to be working

1083
01:18:43,910 --> 01:18:48,370
in order to world and they know how to do inference we are gonna do

1084
01:18:48,370 --> 01:18:52,240
multiclass classification and the only thing that we use to maximize over the class and

1085
01:18:52,240 --> 01:18:56,850
then we get the class which has the maximum response

1086
01:18:56,860 --> 01:19:02,910
OK so let me go over the different examples that mario was working on this

1087
01:19:02,910 --> 01:19:04,860
so here is my area a few years ago

1088
01:19:04,880 --> 01:19:11,310
and so we were looking into these big disagreement dataset which disagreement in this case

1089
01:19:12,300 --> 01:19:14,530
let me explain what it is so

1090
01:19:14,530 --> 01:19:20,200
we have this scenario or which we have we ask questions to the set of

1091
01:19:20,200 --> 01:19:25,950
subjects and we record the review as well as the or the and the and

1092
01:19:26,010 --> 01:19:30,650
the view disagreement is the fact that you can actually respond visually saying this and

1093
01:19:30,650 --> 01:19:33,710
moving your head writer the two views which are going to be the only be

1094
01:19:34,280 --> 01:19:36,360
actually give information about this

1095
01:19:36,380 --> 01:19:40,890
the fact that suggests right you can yes cds but the move your hand in

1096
01:19:40,890 --> 01:19:42,020
that case the

1097
01:19:42,030 --> 01:19:48,230
it's actually corrupted innocence right the single view information that actually the saying yes and

1098
01:19:48,240 --> 01:19:50,950
the same thing can actually happen before the interview

1099
01:19:51,100 --> 01:19:56,150
right so in order to have a controller sitting we simulated is really this agreement

1100
01:19:56,240 --> 01:20:00,500
by have been broken motion in the case of a

1101
01:20:00,520 --> 01:20:04,170
in the case of the gesture and the the is for the case of the

1102
01:20:06,320 --> 01:20:13,040
OK so this are yes resource as a function of the view disagreement so much

1103
01:20:13,040 --> 01:20:19,110
of these nice days there so in that case that there is no disagreement there

1104
01:20:19,110 --> 01:20:22,330
then in this case one of the classifier is really good so you actually don't

1105
01:20:22,330 --> 01:20:26,430
even need to be of use to classify and you know the similarity the MKL

1106
01:20:26,430 --> 01:20:27,040
you do

1107
01:20:27,100 --> 01:20:31,530
these complications can occur in any combination you actually do fairly well and in the

1108
01:20:31,530 --> 01:20:36,580
case of you have a lot of disagreement dismissed seventy percent of the samples have

1109
01:20:36,580 --> 01:20:41,040
all these have one bucuresti OK either the all the all the review and in

1110
01:20:41,040 --> 01:20:44,900
this case you see that there is both the performance if you actually try to

1111
01:20:44,900 --> 01:20:48,870
capture the fact that some of these examples are corrupted which is basically what this

1112
01:20:48,870 --> 01:20:50,450
clustering is the

1113
01:20:50,460 --> 01:20:54,710
OK and here is again the the the the and the media which are the

1114
01:20:54,710 --> 01:20:57,350
the two modalities

1115
01:20:57,400 --> 01:21:02,500
so we try also this in the morris under the condition that the dataset again

1116
01:21:02,500 --> 01:21:05,830
is called the one one three seven hundred one categories with the same example social

1117
01:21:05,850 --> 01:21:11,400
before and here we are to compute multiple the occurrences by using different features that

1118
01:21:11,400 --> 01:21:14,600
people have billion the vision community

1119
01:21:14,620 --> 01:21:21,450
OK and so the good news is that actually we perform more you we might

1120
01:21:21,460 --> 01:21:24,160
to the state of the art of the time so this is one year later

1121
01:21:24,160 --> 01:21:28,140
so you see that is much much more people actually trying to combine different sources

1122
01:21:28,140 --> 01:21:33,510
of information now the the bad news again is that we get more or less

1123
01:21:33,510 --> 01:21:36,120
the same resources by the kind of communication

1124
01:21:36,150 --> 01:21:41,150
OK and the main research here is that actually having this richer structure

1125
01:21:41,190 --> 01:21:42,270
that's o

1126
01:21:42,280 --> 01:21:49,300
when you have an isolated or when you have this this locality vary in theme

1127
01:21:49,310 --> 01:21:53,130
actually we really mean is that same way that will have enough data to actually

1128
01:21:53,130 --> 01:21:57,110
capture these this variation

1129
01:21:57,180 --> 01:22:09,740
so the report was that with was in performance and we didn't get the the

1130
01:22:10,520 --> 01:22:13,390
so i've seen a version of the paper which the

1131
01:22:13,400 --> 01:22:16,030
they didn't have much less so than

1132
01:22:16,050 --> 01:22:17,580
what was the

1133
01:22:17,640 --> 01:22:25,060
what they did to make it two to work but here so here's we we

1134
01:22:25,060 --> 01:22:29,570
couldn't really make it to work and actually here is seen as an example as

1135
01:22:30,200 --> 01:22:35,260
a function of the number of clusters we really have flat performance

1136
01:22:35,280 --> 01:22:39,660
and now we similarly missing data and in that case you actually come up with

1137
01:22:39,800 --> 01:22:41,460
was the performance

1138
01:22:41,520 --> 01:22:44,100
respect to you simple MKL

1139
01:22:45,950 --> 01:22:48,040
so no

1140
01:22:48,060 --> 01:22:52,960
so here we should before was actually what clustering and having a simplified version of

1141
01:22:52,960 --> 01:22:56,770
this and now the

1142
01:22:56,780 --> 01:23:01,360
so what we try to do is so very preliminary cells into train to estimate

1143
01:23:01,370 --> 01:23:06,700
the full non parametric covariance another question if you want do this was actually the

1144
01:23:06,700 --> 01:23:11,640
regularizer avoid this massive overfitting then you would expect to have so we thought of

1145
01:23:11,640 --> 01:23:17,530
two possible regularizers which are pretty big operator is the laplacian regularizer will be so

1146
01:23:17,740 --> 01:23:19,210
some measure of

1147
01:23:19,240 --> 01:23:22,850
this distance similarity you have or

1148
01:23:22,870 --> 01:23:27,570
if you have some estimate of maybe what you know good estimation of this then

1149
01:23:27,570 --> 01:23:31,630
maybe you want to have the frobenius norm that minimize the distance to estimate the

1150
01:23:31,640 --> 01:23:35,810
you think it is possible for example this can be yes the fact that you

1151
01:23:35,810 --> 01:23:41,010
know the course and weights or yes if is the average cannot all once

1152
01:23:41,050 --> 01:23:44,950
and so we simple experiment were

1153
01:23:44,980 --> 01:23:50,030
and these are the only other result dataset and actually we got both of performance

1154
01:23:50,030 --> 01:23:56,140
if we learn the complicated matters is very interesting thing they call single training

1155
01:23:56,450 --> 01:24:01,540
which was developed by yuille NIPS a couple of years ago and the main idea

1156
01:24:01,570 --> 01:24:05,150
is this is going to be for active set in the main idea is that

1157
01:24:05,150 --> 01:24:09,850
you want to enforce an agreement prior in order to regularizer solutions so you're going

1158
01:24:09,850 --> 01:24:15,900
to have different views here you're gonna have to get some process function for a

1159
01:24:15,900 --> 01:24:19,790
review and then you've got some process priors on each of these guys and then

1160
01:24:19,790 --> 01:24:24,070
you have no gas generation this big o thing about using this is that you

1161
01:24:24,070 --> 01:24:29,210
can actually marginalizes in closed form and human the then you end up with a

1162
01:24:29,210 --> 01:24:35,760
gaussian process of business here which i covariance which is a combination of kernels in

1163
01:24:35,910 --> 01:24:41,840
innocent four so what you get is the members of these some of the inverses

1164
01:24:41,910 --> 01:24:46,740
of the kernel now the good thing about this is that

1165
01:24:46,800 --> 01:24:50,920
by doing the same you actually can use this in a semi supervised setting so

1166
01:24:50,920 --> 01:24:54,880
it's going to influence your to be the covariance therefore the covariance of the end

1167
01:24:55,180 --> 01:24:58,600
and the thing about this is that the masses of as are

1168
01:24:58,620 --> 01:25:01,980
you know i'm going to be pretty bad condition so you have to be very

1169
01:25:01,980 --> 01:25:03,620
careful when you when you do this

1170
01:25:03,630 --> 01:25:06,820
and so some i said he was interested in

1171
01:25:06,830 --> 01:25:11,050
this extend this to actually have

1172
01:25:11,180 --> 01:25:16,310
more complicated functions so is the case of catalyst first where you expect again the

1173
01:25:16,310 --> 01:25:22,510
nice vary across data points and the idea here is that

1174
01:25:22,570 --> 01:25:29,180
basically he again citizens scenarios before we have a subset of clusters and in this

1175
01:25:29,180 --> 01:25:32,170
so the assumption makes the observation true

1176
01:25:32,190 --> 01:25:35,960
and this action pressing the alarm signal button

1177
01:25:35,970 --> 01:25:37,860
it makes

1178
01:25:37,900 --> 01:25:39,400
the conclusion

1179
01:25:39,420 --> 01:25:42,340
of the maintenance school true

1180
01:25:42,380 --> 01:25:43,310
so this

1181
01:25:43,400 --> 01:25:48,880
inductive logic programming agent model has a very natural proof theoretic

1182
01:25:48,900 --> 01:25:55,660
operational semantics that looks like production systems combines production system functionality

1183
01:25:55,680 --> 01:25:57,970
with logic programming functionality

1184
01:25:57,980 --> 01:25:59,380
has meet

1185
01:25:59,390 --> 01:26:06,990
link to decision theory but has a shortcut to a stimulus response associations all of

1186
01:26:06,990 --> 01:26:14,720
which in turn have a connectionist implementation

1187
01:26:14,780 --> 01:26:17,790
so as i said before there is really no need to say it again

1188
01:26:17,800 --> 01:26:21,580
the agent's world is really a world of its own creations

1189
01:26:21,600 --> 01:26:22,880
for the most part

1190
01:26:22,900 --> 01:26:25,350
it's beliefs are entirely made up

1191
01:26:25,350 --> 01:26:28,080
they're all in the head of the agent

1192
01:26:28,100 --> 01:26:33,140
the only point of contact the agent has with the world are observations

1193
01:26:33,200 --> 01:26:37,420
and these observations must be made true by its assumptions about the world and by

1194
01:26:37,420 --> 01:26:41,780
the actions it performs and in addition to those observations

1195
01:26:41,860 --> 01:26:46,190
being true it also must fulfill its goals

1196
01:26:46,250 --> 01:26:49,110
so the actions of the agent embedded in the world

1197
01:26:49,150 --> 01:26:53,680
must be such that schools can be made true by the actions it chooses to

1198
01:26:56,720 --> 01:27:02,260
of course many different solutions exist there are many delta many different set of assumptions

1199
01:27:02,260 --> 01:27:06,460
about the world many different actions many different plans and achieve the same results

1200
01:27:06,540 --> 01:27:08,680
so it's not enough simply to say

1201
01:27:08,750 --> 01:27:12,590
to be justified simply to get along from day to day

1202
01:27:12,660 --> 01:27:15,880
the challenge is to find the best delta you can

1203
01:27:15,890 --> 01:27:16,640
the image does

1204
01:27:16,660 --> 01:27:19,070
most you can with your life

1205
01:27:19,090 --> 01:27:23,660
subject to your own capabilities to the computational resources the u

1206
01:27:23,680 --> 01:27:25,410
i have in particular

1207
01:27:25,430 --> 01:27:27,960
now in classical decision theory

1208
01:27:28,030 --> 01:27:31,160
the value of an action or course of action

1209
01:27:31,200 --> 01:27:36,930
is measured by the expected utility of its logical consequence is not simply the

1210
01:27:36,980 --> 01:27:39,780
cause and effect consequences but the logical

1211
01:27:41,320 --> 01:27:46,540
this expected utility as we know incorporates notions of probability

1212
01:27:46,580 --> 01:27:51,080
similarly in the philosophy of science the value of an explanation of an observation is

1213
01:27:51,080 --> 01:27:54,160
measured in terms of how likely

1214
01:27:54,190 --> 01:27:58,780
how likely a state of affairs in the world might be too

1215
01:27:58,780 --> 01:28:06,470
explain the observations for example tear gas is less likely explanation of smoke and fire

1216
01:28:06,540 --> 01:28:09,050
and explanatory power more generally

1217
01:28:09,060 --> 01:28:11,630
how many observations that those

1218
01:28:11,640 --> 01:28:15,130
and an explanatory hypothesis explains

1219
01:28:15,210 --> 01:28:20,640
the more observations explained by hypothesis the better in the same way the more goals

1220
01:28:20,640 --> 01:28:23,580
achieved by applying the better

1221
01:28:23,640 --> 01:28:27,080
the same criteria used in decision theory

1222
01:28:28,080 --> 01:28:31,460
help us choose between different courses of action

1223
01:28:31,640 --> 01:28:36,310
the virtually identical to the criteria used in philosophy of science

1224
01:28:36,310 --> 01:28:38,120
to identify

1225
01:28:38,130 --> 01:28:42,720
preferred explanations of observations

1226
01:28:42,780 --> 01:28:45,570
so an abductive logic programming

1227
01:28:45,670 --> 01:28:47,500
we can combine the two

1228
01:28:47,540 --> 01:28:48,860
we can combine

1229
01:28:48,880 --> 01:28:50,590
both measures of

1230
01:28:51,280 --> 01:28:53,540
the value of a solution

1231
01:28:53,550 --> 01:28:54,980
and use this

1232
01:28:55,010 --> 01:28:57,720
not only to help us make decisions

1233
01:28:57,760 --> 01:29:00,040
but also to choose

1234
01:29:00,040 --> 01:29:01,190
as we search

1235
01:29:01,210 --> 01:29:03,630
to search out better

1236
01:29:03,640 --> 01:29:05,620
part of the search space

1237
01:29:05,620 --> 01:29:08,370
more likely to achieve goals than others

1238
01:29:08,420 --> 01:29:13,130
more likely to achieve them to higher degree than other parts of the search space

1239
01:29:13,140 --> 01:29:16,840
so as i said and as you all know better than me for the most

1240
01:29:17,580 --> 01:29:20,350
life is uncertain

1241
01:29:20,370 --> 01:29:22,580
but what you may not appreciate

1242
01:29:22,590 --> 01:29:24,030
except for

1243
01:29:24,080 --> 01:29:27,730
if few words in the audience perhaps is that uncertainty

1244
01:29:27,790 --> 01:29:28,660
fits in

1245
01:29:28,680 --> 01:29:32,400
with logic logic and uncertainty go hand-in-hand

1246
01:29:32,410 --> 01:29:34,560
so in particular

1247
01:29:34,600 --> 01:29:38,970
it's quite natural for the state of the world such as

1248
01:29:38,990 --> 01:29:43,170
being rich to depend upon not only what you do

1249
01:29:43,210 --> 01:29:46,900
and actually of your own such as buying a lottery ticket but what the world

1250
01:29:48,040 --> 01:29:49,880
so you will be rich

1251
01:29:49,900 --> 01:29:52,580
if you buy lottery tickets and your number is chosen

1252
01:29:52,640 --> 01:29:55,010
and you can use this kind of belief

1253
01:29:55,030 --> 01:29:56,140
two are false

1254
01:29:56,170 --> 01:29:58,400
you can use this would be better if it's true

1255
01:29:58,410 --> 01:30:01,920
you can use this belief in order to guide your behavior

1256
01:30:01,940 --> 01:30:06,100
if your goal is to be rich if that's what makes you happy

1257
01:30:06,110 --> 01:30:08,910
then you can buy a lottery ticket

1258
01:30:08,970 --> 01:30:10,750
but the expected value

1259
01:30:10,870 --> 01:30:13,620
of that decision

1260
01:30:13,640 --> 01:30:17,650
has to be weighted by the probability that your number is chosen

1261
01:30:17,660 --> 01:30:21,030
so decision theory incorporates probability theory

1262
01:30:21,030 --> 01:30:25,900
and it is absolutely essential in any kind of agent whether it uses logic or

1263
01:30:26,600 --> 01:30:28,070
so logic alone

1264
01:30:28,080 --> 01:30:33,650
is nothing without probably not logic in a decision theoretic context in an agent

1265
01:30:34,080 --> 01:30:41,650
it doesn't survive without some capability for reasoning about uncertainty in in terms of probability

1266
01:30:41,650 --> 01:30:48,260
that depends on the size of the perturbation and depends on the sound speed

1267
01:30:48,350 --> 01:30:51,050
and sold for uconn

1268
01:30:51,070 --> 01:30:55,900
figure out to what is the time for pressure

1269
01:30:55,920 --> 01:31:03,020
if they equalised in time for the pressure is much smaller than the dynamical time

1270
01:31:03,050 --> 01:31:09,060
it means that whenever you create this perturbation like a sound wave this perturbation doesn't

1271
01:31:09,060 --> 01:31:13,880
grow and go crazy and create tornadoes but it remains like sound waves and eventually

1272
01:31:14,860 --> 01:31:20,180
OK so now we understand why there were no tornadoes in this room

1273
01:31:20,210 --> 01:31:26,040
but what are the implications for cosmology for these well the implications for cosmology that

1274
01:31:26,080 --> 01:31:29,050
the olive perturbation for weeks

1275
01:31:29,070 --> 01:31:32,060
the pressure time

1276
01:31:32,090 --> 01:31:38,530
is large it is larger done dynamical time can grow

1277
01:31:38,530 --> 01:31:42,440
but the west perturbation them growing your left with something like sound waves

1278
01:31:42,480 --> 01:31:45,280
so this defines a particular

1279
01:31:45,330 --> 01:31:47,250
size of perturbation

1280
01:31:47,250 --> 01:31:52,000
define the genes lens and deduced lens to define it when the dynamical time is

1281
01:31:52,000 --> 01:31:53,580
equal to the

1282
01:31:55,150 --> 01:31:58,020
depression time

1283
01:31:58,120 --> 01:32:02,290
below that the lens

1284
01:32:02,300 --> 01:32:04,940
o thing scanned

1285
01:32:05,870 --> 01:32:10,760
over the next region larger than the the influence can collapse about smaller than doctor

1286
01:32:10,760 --> 01:32:16,640
they come not to and therefore you just get the situation you get something like

1287
01:32:16,720 --> 01:32:20,870
and so when we write this equality

1288
01:32:20,930 --> 01:32:24,950
we can define these genes lands the genes lens for the area ten to the

1289
01:32:24,950 --> 01:32:27,890
five kilometres which means the sound waves

1290
01:32:27,950 --> 01:32:31,150
but any kind of wavelengths that you can use for hearing me

1291
01:32:33,940 --> 01:32:39,430
so what are the implications for cosmology well we can't go on substitute things in

1292
01:32:39,430 --> 01:32:43,820
figure out what is the typical size of the genes land the typical size of

1293
01:32:43,820 --> 01:32:47,990
the genes length depends on the dynamical time and the sound speed and the dynamical

1294
01:32:47,990 --> 01:32:53,070
time we can write it like this and remember this reminds you a lot of

1295
01:32:53,070 --> 01:32:54,260
the inverse of

1296
01:32:54,690 --> 01:32:57,070
the hubble parameter

1297
01:32:57,100 --> 01:32:59,250
so for the universe

1298
01:32:59,290 --> 01:33:00,920
we have a relation

1299
01:33:00,950 --> 01:33:09,630
the genes lance's factors of order unity the sound speed divided by the hubble parameter

1300
01:33:09,650 --> 01:33:13,900
so let's say to plug in some numbers in the radiation dominated i and the

1301
01:33:13,900 --> 01:33:17,290
sound speed it's very close to the speed of light is the speed of light

1302
01:33:17,290 --> 01:33:19,350
divided by the square root of three

1303
01:33:20,350 --> 01:33:28,170
and sold the genes length is three c divided by eight

1304
01:33:28,180 --> 01:33:37,210
but remember that this looks a lot like the size of the horizon

1305
01:33:37,210 --> 01:33:38,770
remember what was the

1306
01:33:38,800 --> 01:33:44,130
problems arise and how arise and was exactly these

1307
01:33:44,150 --> 01:33:48,040
so this is telling you that inside the eyes

1308
01:33:48,060 --> 01:33:51,240
during the radiation dominated there

1309
01:33:51,320 --> 01:33:55,720
perturbation unstable

1310
01:33:55,720 --> 01:34:00,030
in other words during the radiation dominated her if you have a perturbation they behave

1311
01:34:00,030 --> 01:34:01,980
like sound waves

1312
01:34:02,000 --> 01:34:06,340
you can't create galaxies stars or anything like that

1313
01:34:06,360 --> 01:34:07,520
and so

1314
01:34:07,530 --> 01:34:11,710
when you look at the universe and you get an image of the universe when

1315
01:34:11,710 --> 01:34:16,940
basie culley was very close to radiation dominated which is something that you see for

1316
01:34:16,940 --> 01:34:22,020
example the cosmic background this is what you expect to see

1317
01:34:22,030 --> 01:34:27,460
if there were some perturbations were not growing but they were behaving like acoustic waves

1318
01:34:27,460 --> 01:34:31,070
and the kind of mathematics you may actually need to describe this perturbation must be

1319
01:34:31,070 --> 01:34:35,200
very very similar to the kind of mathematics used to describe weights

1320
01:34:36,690 --> 01:34:44,360
one then when the universe started the bain dominated by matter instead of by radiation

1321
01:34:44,380 --> 01:34:50,520
so let's just prior to the sound speed for body and because we know how

1322
01:34:50,520 --> 01:34:55,610
to compute that and then we can write it in this way then we plug

1323
01:34:55,610 --> 01:34:59,480
in the numbers and we get the sound speed for body and is much much

1324
01:34:59,480 --> 01:35:05,360
smaller than the sound speed than than the speed of light

1325
01:35:05,380 --> 01:35:10,690
in other words when the universe passed goals from brain radiation dominated

1326
01:35:10,710 --> 01:35:16,110
two something that looks like body and dominated something that behaves like this

1327
01:35:16,110 --> 01:35:20,790
then the genes length decreases immediately like that

1328
01:35:20,820 --> 01:35:25,610
by a factor three ten to define

1329
01:35:25,630 --> 01:35:28,900
and that

1330
01:35:28,920 --> 01:35:34,000
same is well like that from then radiation dominated the brain dominated by something like

1331
01:35:34,730 --> 01:35:38,630
which means that the gene smartarse

1332
01:35:38,650 --> 01:35:40,300
decreases by

1333
01:35:40,320 --> 01:35:42,840
you know two times ten to the fourteenth

1334
01:35:43,630 --> 01:35:46,800
perturbation inside your eyes and back then

1335
01:35:46,820 --> 01:35:50,940
you know it's a scale that is really be could grow only perturbation outside items

1336
01:35:50,960 --> 01:35:54,050
imagine how big they had to be they couldn't go

1337
01:35:54,070 --> 01:35:56,270
but suddenly then they can

1338
01:35:56,270 --> 01:36:01,730
so suddenly after the universe becomes matter dominated then you get to that the mass

1339
01:36:01,730 --> 01:36:07,420
of the perturbation the constantly growing is about ten to the five solar masses

1340
01:36:07,480 --> 01:36:09,050
surprise surprise

1341
01:36:09,070 --> 01:36:13,020
this is the size of the globular cluster sizes more galaxy

1342
01:36:13,050 --> 01:36:18,610
and surprise surprise it turns out that globular clusters of the object in the universe

1343
01:36:18,610 --> 01:36:22,170
that cost the all this the stellar population

1344
01:36:22,190 --> 01:36:28,440
is almost like as soon as the universe cooled started growing its perturbations former globular

1345
01:36:28,440 --> 01:36:32,840
clusters and then globular clusters that are forming stars and the stars that we see

1346
01:36:32,840 --> 01:36:37,480
today in globular clusters have an age that is very very close to the to

1347
01:36:41,480 --> 01:36:43,480
could have

1348
01:36:45,050 --> 01:36:50,290
but the universe expands and so now we've done everything forgetting about this function of

1349
01:36:50,290 --> 01:36:53,750
the universe and this function of the universe must do something for these

1350
01:36:53,750 --> 01:36:58,690
right because you have this perturbation the start collapsing and wants to be crafted by

1351
01:36:58,690 --> 01:37:02,960
gravity but you have the fabric of space-time that tends to you know looted away

1352
01:37:03,380 --> 01:37:06,130
and so there is going to be some sort of

1353
01:37:06,150 --> 01:37:10,920
the tug-of-war between these two different forces and so we have to write that in

1354
01:37:10,920 --> 01:37:15,480
the equation and we can do that by following are very very similar calculation as

1355
01:37:15,480 --> 01:37:21,750
we did before but reminding ourselves that when we write down the other that city

1356
01:37:21,750 --> 01:37:26,250
the background density gets diluted and sing and if it is among the

1357
01:37:26,630 --> 01:37:30,520
then he gets they looted like the scale factor to the minus three

1358
01:37:30,520 --> 01:37:31,440
and now

1359
01:37:31,460 --> 01:37:35,500
i will not to walk you through the all the steps of the equation because

1360
01:37:36,050 --> 01:37:40,530
it's boring and because you can do it in the comfort of your own

1361
01:37:40,550 --> 01:37:46,250
room or your own homes but what you end up having is an equation that

1362
01:37:46,250 --> 01:37:50,590
looks like this remember before we had an equation where we didn't have this that

1363
01:37:50,590 --> 01:37:56,030
we didn't have decided to adopt we only have delta doubled up and out

1364
01:37:56,050 --> 01:37:59,400
but now we have this extra term here and this looks a lot like you

1365
01:37:59,400 --> 01:38:04,690
know forms a and and and are

1366
01:38:04,710 --> 01:38:06,460
acoustic oscillation right

1367
01:38:06,480 --> 01:38:10,020
and this term here it's called hubble drug

1368
01:38:10,020 --> 01:38:11,920
somebody might be too much

1369
01:38:11,940 --> 01:38:13,220
or to go

1370
01:38:13,270 --> 01:38:14,680
or smoke

1371
01:38:14,820 --> 01:38:18,380
trying to achieve satisfaction through the mouth

1372
01:38:18,400 --> 01:38:22,360
of the sort they didn't get in this very early stage of development but it

1373
01:38:22,360 --> 01:38:24,320
can also be more abstract

1374
01:38:24,670 --> 01:38:28,100
if your roommate is dependent need

1375
01:38:28,120 --> 01:38:31,980
you could then go to your remains a your oral person

1376
01:38:31,990 --> 01:38:36,690
the first year of your life did not go well

1377
01:38:36,700 --> 01:38:38,920
a phrase even more popular

1378
01:38:38,930 --> 01:38:40,770
is there a stage

1379
01:38:40,780 --> 01:38:43,060
and that happens after oral stage

1380
01:38:43,080 --> 01:38:45,970
and problems can emerge

1381
01:38:46,010 --> 01:38:49,300
if toilet training is not correctly

1382
01:38:49,320 --> 01:38:53,230
if you have problems during those years of life becomes an animal

1383
01:38:53,250 --> 01:38:59,730
personality according to freud and you make it say your problem is you too

1384
01:39:00,750 --> 01:39:07,040
according to freud literally meant you're unwilling to part with your own feces

1385
01:39:07,090 --> 01:39:11,000
it's written down here i know

1386
01:39:11,020 --> 01:39:14,370
and the way it manifests itself as you know from just now people to talk

1387
01:39:14,370 --> 01:39:16,020
is compulsive

1388
01:39:16,300 --> 01:39:18,560
queen used in g

1389
01:39:18,570 --> 01:39:20,810
this is the anal personality

1390
01:39:20,820 --> 01:39:23,840
then it gets a little bit more complicated

1391
01:39:23,890 --> 01:39:27,630
on the next stage of the phallic stage actually this is not much more complicated

1392
01:39:27,730 --> 01:39:29,250
the focus of pleasure

1393
01:39:29,270 --> 01:39:31,810
shifts to the general

1394
01:39:32,720 --> 01:39:35,460
and fixations

1395
01:39:35,480 --> 01:39:37,680
can lead to excessive masculinity

1396
01:39:37,690 --> 01:39:38,920
in females

1397
01:39:38,930 --> 01:39:41,460
or if you have it nails or

1398
01:39:41,480 --> 01:39:44,730
if female need for attention or domination

1399
01:39:44,750 --> 01:39:48,690
now at this point something really interesting happens

1400
01:39:48,690 --> 01:39:50,970
call at this complex

1401
01:39:50,980 --> 01:39:53,110
and this is based on the story

1402
01:39:53,120 --> 01:39:57,070
the mythical story became who killed his father

1403
01:39:57,070 --> 01:39:58,970
and marry his mother

1404
01:39:58,990 --> 01:40:03,070
and according to freud this happens to all of us

1405
01:40:03,080 --> 01:40:04,650
in this way

1406
01:40:04,700 --> 01:40:06,520
well all of us by all of us

1407
01:40:08,400 --> 01:40:12,090
so here's the idea

1408
01:40:12,350 --> 01:40:15,700
three or four years old during the phallic stage

1409
01:40:15,720 --> 01:40:18,870
so what are you interested in what you interested europeans

1410
01:40:18,870 --> 01:40:24,700
and then you seek external or object for a sort of vague about this but

1411
01:40:24,700 --> 01:40:25,820
you know

1412
01:40:25,930 --> 01:40:30,480
think some sort of satisfaction but who's out there will be sweet and kind and

1413
01:40:30,480 --> 01:40:31,950
loving and wonderful

1414
01:40:35,280 --> 01:40:39,960
so to thailand for inference model is not

1415
01:40:41,520 --> 01:40:44,310
so far so fast it is not crazy

1416
01:40:44,330 --> 01:40:48,290
well boy falling in love his mother

1417
01:40:53,040 --> 01:40:56,420
now this is going to get progressively worse but i want to say as the

1418
01:40:56,420 --> 01:41:02,250
father of two two sons both sides went two phase where they explicitly said they

1419
01:41:02,250 --> 01:41:04,690
wanted to marry mommy and me

1420
01:41:04,750 --> 01:41:07,690
if something bad happened to be that would be the worst thing in the world

1421
01:41:08,520 --> 01:41:10,290
so so does this

1422
01:41:10,300 --> 01:41:12,380
but now

1423
01:41:13,010 --> 01:41:16,680
it gets a little bit aggressive so so the idea is the child determines

1424
01:41:16,700 --> 01:41:19,190
that he's he's going to kill his father

1425
01:41:19,200 --> 01:41:22,550
every three and four with texas

1426
01:41:22,870 --> 01:41:28,170
but then because children according to freud don't have a good sense of

1427
01:41:28,220 --> 01:41:32,580
the boundary between or mine in the world which is this a problem the problem

1428
01:41:32,580 --> 01:41:35,550
is is they don't they think their father

1429
01:41:35,580 --> 01:41:39,720
i can tell you that you're plotting to kill

1430
01:41:39,790 --> 01:41:42,780
and the figure father is now angry at them

1431
01:41:42,800 --> 01:41:47,760
and then they ask themselves what's the worst thing that could do to me

1432
01:41:47,800 --> 01:41:49,200
and the answer is

1433
01:41:50,960 --> 01:41:56,750
so they come to the conclusion that her father is going to castrate because their

1434
01:41:56,780 --> 01:41:59,700
illicit love for their money

1435
01:41:59,720 --> 01:42:02,190
and then they say that

1436
01:42:02,200 --> 01:42:03,790
and then don't think about sex

1437
01:42:03,800 --> 01:42:05,880
for several years

1438
01:42:05,950 --> 01:42:11,390
and that the latency stage the latency stage is going to used a moment

1439
01:42:11,540 --> 01:42:13,110
for the love of my

1440
01:42:13,130 --> 01:42:15,170
i wanted to kill my father

1441
01:42:15,230 --> 01:42:17,310
that is going to castrate may

1442
01:42:17,310 --> 01:42:19,020
i fell of my

1443
01:42:19,040 --> 01:42:20,410
that is x business

1444
01:42:20,430 --> 01:42:25,290
and it's x repressed until you get to the general state and the general stages

1445
01:42:25,300 --> 01:42:27,110
this stage we are all in

1446
01:42:27,160 --> 01:42:28,220
the healthy

1447
01:42:28,230 --> 01:42:30,690
adult stage

1448
01:42:32,530 --> 01:42:36,380
now adults in going through all the developmental stages

1449
01:42:36,440 --> 01:42:40,200
would you stand in words yet

1450
01:42:41,590 --> 01:42:46,580
unconscious mechanisms are still even if you haven't got fixated or anything

1451
01:42:46,970 --> 01:42:50,820
there's still this dynamic going on all the time

1452
01:42:50,830 --> 01:42:53,280
with you in your ego in user prego

1453
01:42:53,290 --> 01:42:58,260
and the idea is to use super eagle remember super eagle was stupid

1454
01:42:58,970 --> 01:43:01,640
super is only telling you

1455
01:43:01,690 --> 01:43:04,040
not to do bad things

1456
01:43:04,050 --> 01:43:06,970
it's telling you not to think about things

1457
01:43:07,000 --> 01:43:10,010
so what's happening is your e

1458
01:43:10,030 --> 01:43:12,760
is sending out all of this weird six stuff

1459
01:43:12,770 --> 01:43:17,110
all of these crazy sexual and violent desires of killing

1460
01:43:17,120 --> 01:43:19,130
i'll have sex with that

1461
01:43:21,120 --> 01:43:23,540
i have actually helping some i deserve

1462
01:43:23,540 --> 01:43:24,870
and then

1463
01:43:24,890 --> 01:43:27,370
you're super is necessary to say no

1464
01:43:28,840 --> 01:43:33,540
and this stuff is repressed it doesn't even make it to consciousness the problem is

1465
01:43:33,540 --> 01:43:36,840
for the very hydraulic theory

1466
01:43:36,850 --> 01:43:39,060
of what goes on

1467
01:43:39,080 --> 01:43:41,590
and some other stuff slips out

1468
01:43:41,610 --> 01:43:45,310
and it shows up in dreams and it shows up on slips of the tongue

1469
01:43:45,320 --> 01:43:49,680
and in exceptional cases it shows up in certain clinical symptoms

1470
01:43:50,450 --> 01:43:52,100
so what happens is

1471
01:43:52,100 --> 01:43:54,650
freud described a lot of normal life

1472
01:43:54,750 --> 01:43:58,070
as in terms of different ways we use

1473
01:43:58,100 --> 01:44:02,950
to keep that horrible stuff from the is making its way to consciousness and he

1474
01:44:02,950 --> 01:44:06,170
called these defense mechanisms

1475
01:44:06,220 --> 01:44:10,540
you're defending yourself against all the horrible parts

1476
01:44:10,540 --> 01:44:12,190
of yourself

1477
01:44:12,220 --> 01:44:14,460
some of these make a little bit of sense

1478
01:44:14,470 --> 01:44:20,280
i mean one way to describe this in non-technical non freudian ways certain things about

1479
01:44:20,280 --> 01:44:22,160
ourselves rather not know

1480
01:44:22,180 --> 01:44:24,280
certain desires rather not know

1481
01:44:24,290 --> 01:44:26,610
and we have ways to hide

1482
01:44:26,660 --> 01:44:29,010
so for instance are sublimation

1483
01:44:29,020 --> 01:44:32,500
sublimation is you might have a lot of energy

1484
01:44:32,520 --> 01:44:34,930
maybe sexual energy are aggressive energy

1485
01:44:35,030 --> 01:44:39,250
but instead of turning into a sexual or aggressive target we do is you focus

1486
01:44:39,250 --> 01:44:40,880
in some other way

1487
01:44:40,900 --> 01:44:43,900
so you can imagine a great artists like castles

1488
01:44:43,900 --> 01:44:45,510
i i

1489
01:53:15,170 --> 01:53:19,370
we going to

1490
01:53:40,980 --> 01:53:42,920
one thing

1491
01:54:30,570 --> 01:54:39,870
much more

1492
01:56:06,300 --> 01:56:09,220
i mean

1493
01:56:11,130 --> 01:56:18,240
one year

1494
01:56:20,630 --> 01:56:23,080
good morning

1495
01:56:23,100 --> 01:56:25,830
all his

1496
01:56:31,600 --> 01:56:36,410
here we

1497
01:56:36,410 --> 01:56:39,940
before i move on i want to the to the junction tree

1498
01:56:40,340 --> 01:56:44,640
algorithm but but before before i backtrack a bit

1499
01:56:44,770 --> 01:56:49,310
judging from some questions bring the break let me go back to some of these

1500
01:56:50,670 --> 01:56:55,180
just to recap and sort of point out how these kinds of sum product and

1501
01:56:55,180 --> 01:56:59,740
max product algorithm is why these inference problems would actually be interesting in practice

1502
01:56:59,870 --> 01:57:04,710
right so let's go back to the stereo pair

1503
01:57:04,770 --> 01:57:08,730
member of what you're trying to do is to figure out an optimal shift between

1504
01:57:08,730 --> 01:57:12,640
these two guys to try and match them so you can detect that

1505
01:57:15,150 --> 01:57:17,860
what people actually do in practice is

1506
01:57:17,870 --> 01:57:24,090
they can model the pair image field and what the modelling is not the pixels

1507
01:57:24,100 --> 01:57:28,270
now what sort of trying to model is what's the shift the relative disparity shift

1508
01:57:28,270 --> 01:57:29,660
between the two images

1509
01:57:30,110 --> 01:57:32,530
so what you're trying to compute every point is

1510
01:57:32,540 --> 01:57:36,400
you've got to images and you're trying to compute for every point how much is

1511
01:57:36,400 --> 01:57:41,930
this point shifted relative to this point in in the two images how much is

1512
01:57:42,000 --> 01:57:46,940
this point here shifted relative the shift is going to change a lot depending on

1513
01:57:46,940 --> 01:57:50,850
the depth if you think about the geometry of it as the depth changes the

1514
01:57:50,850 --> 01:57:55,240
amount of shift is going to change so it's that shift called disparity that allows

1515
01:57:55,240 --> 01:57:59,280
for instance artificial system to recover a depth

1516
01:57:59,590 --> 01:58:03,080
and so in terms of the graphical model of message passing

1517
01:58:03,500 --> 01:58:06,800
we don't know quite how to solve this problem yet but this is really one

1518
01:58:06,800 --> 01:58:12,800
of those motor problems or most probable configuration what sort of thinking about his modelling

1519
01:58:12,800 --> 01:58:17,270
the set of possible disparities using one of these big lattice models will be much

1520
01:58:17,270 --> 01:58:18,740
bigger than this of course

1521
01:58:18,830 --> 01:58:22,970
and what you'd like to do is sort of search over all the disparities to

1522
01:58:22,970 --> 01:58:25,200
find the one that's most probable

1523
01:58:25,220 --> 01:58:30,170
what you're doing is sort of modeling the set of disparities in statistical or probabilistic

1524
01:58:30,170 --> 01:58:34,510
way that's what he is these compatibility functions here telling you this sort of telling

1525
01:58:34,510 --> 01:58:37,280
you how likely are you see certain relatives ships

1526
01:58:37,350 --> 01:58:38,500
and so

1527
01:58:38,520 --> 01:58:42,040
there's lot of heart of people in computer vision spent a long time figuring out

1528
01:58:42,300 --> 01:58:47,540
what the right functions here to use let's not worry about that but at a

1529
01:58:47,540 --> 01:58:51,410
high level the key thing is that this is exactly in in prince problem and

1530
01:58:51,410 --> 01:58:55,300
the interesting thing this is an inference problem that you are doing right now that

1531
01:58:55,380 --> 01:58:59,780
you know all mammals with the most members of bayan ocular visual systems are solving

1532
01:59:00,260 --> 01:59:04,230
implicitly this inference problems on the fly all the time

1533
01:59:04,260 --> 01:59:09,450
so it's it's interesting that were capable of doing it but currently computers were not

1534
01:59:09,450 --> 01:59:11,540
so good at it yet

1535
01:59:17,420 --> 01:59:21,480
so that's one example of inference let me maybe talk a bit about graphical codes

1536
01:59:21,480 --> 01:59:24,970
in and explain how this and other inference problem here

1537
01:59:24,970 --> 01:59:27,820
i didn't cover the slide before but

1538
01:59:27,870 --> 01:59:34,100
remember the coding problem was thinking about transmitting a message to your neighbor it could

1539
01:59:34,100 --> 01:59:37,950
be people speaking or could be computers talking to one another

1540
01:59:38,970 --> 01:59:44,140
if you're concerned about the fact that the channel over which was speaking has noise

1541
01:59:44,260 --> 01:59:47,500
so sent something it doesn't get received exactly

1542
01:59:47,510 --> 01:59:51,900
and you have to somehow protect you have to fight that noise

1543
01:59:51,910 --> 01:59:57,010
so the way graphical codes are built we now consider understand this picture but that

1544
01:59:57,010 --> 02:00:01,500
this is a factor graph and so what you want about the seven notes here

1545
02:00:01,500 --> 02:00:01,950
these are

1546
02:00:02,400 --> 02:00:05,650
it would be binary very labels they were telling you're going to be sending a

1547
02:00:05,650 --> 02:00:07,910
string of seven zeros or ones

1548
02:00:08,030 --> 02:00:12,060
so this would allow you to send a certain set of messages to your friend

1549
02:00:12,060 --> 02:00:15,530
you you know you might have messages that are text but you would sort of

1550
02:00:15,530 --> 02:00:18,690
map them to binary strings and that's what you mean to send

1551
02:00:18,710 --> 02:00:23,910
what's key here that you're not sending just any one of these strings you can

1552
02:00:23,910 --> 02:00:26,080
only send certain subsets

1553
02:00:26,130 --> 02:00:29,660
so that's the factor graph comes in this guy thing about him is that as

1554
02:00:29,660 --> 02:00:35,200
the policeman is going to check up on a bit one three five and seven

1555
02:00:36,270 --> 02:00:38,400
the way these particular graphs workers

1556
02:00:38,410 --> 02:00:43,630
what he each checks he checks the parity of those four bits so he wants

1557
02:00:43,630 --> 02:00:46,540
there to be an even number of ones

1558
02:00:46,550 --> 02:00:52,050
so this guy this guy's find is zero zero zero zero at those four positions

1559
02:00:52,050 --> 02:00:57,470
so zero wants this guy is bad he's got zero zero zero one so he's

1560
02:00:57,470 --> 02:01:01,890
not a codewords so this is something that doesn't get sent this is something that

1561
02:01:01,890 --> 02:01:03,280
you could send

1562
02:01:05,380 --> 02:01:10,370
the decoding problem say your wireless card is transmitting to the wireless base station

1563
02:01:10,470 --> 02:01:13,900
so your wireless card sending the strings of zeros and ones

