1
00:00:00,000 --> 00:00:01,040
and mutual funds

2
00:00:01,470 --> 00:00:02,050
and they laid

3
00:00:02,540 --> 00:00:06,700
i'll after of undersea overland

4
00:00:07,200 --> 00:00:09,070
fibre optic cable

5
00:00:11,730 --> 00:00:14,580
i don't tell this crowd about fiber-optic cable

6
00:00:16,710 --> 00:00:18,980
it's the gift that keeps on giving

7
00:00:20,660 --> 00:00:22,370
because once it's in the ground

8
00:00:23,250 --> 00:00:26,730
all you have to do is keep improving those which is at each end and

9
00:00:26,730 --> 00:00:29,040
new like more and more capacity

10
00:00:30,400 --> 00:00:35,320
and basically the goal of the massive glass fiber-optic cable

11
00:00:36,630 --> 00:00:41,130
allowed suddenly more people in more places to communicate

12
00:00:41,580 --> 00:00:42,600
virtually for free

13
00:00:43,190 --> 00:00:45,410
never before in the history of the planet

14
00:00:46,830 --> 00:00:52,700
so that's really is what the netscape flat was all about the third flatness simply called workflow

15
00:00:53,150 --> 00:00:54,710
and workflow is simply my

16
00:00:55,350 --> 00:00:57,940
catch off the software and all the standards

17
00:00:58,430 --> 00:00:59,970
then allowed work flow

18
00:01:01,000 --> 00:01:06,380
everything from microsoft word microsoft not mean told proprietary software that you're all right and

19
00:01:06,380 --> 00:01:08,220
work on in your daily work

20
00:01:09,000 --> 00:01:13,170
now again you go back to a mighty probably in the sometime in the eighties

21
00:01:13,170 --> 00:01:18,790
you know the admissions department probably running microsoft's bookkeeping department was running novell

22
00:01:20,100 --> 00:01:22,170
both departments are much more efficient because

23
00:01:23,130 --> 00:01:24,020
they had computers

24
00:01:25,460 --> 00:01:27,920
applications that connect applications

25
00:01:28,620 --> 00:01:32,970
now a lot of applications still don't connect applications we certainly know that's

26
00:01:33,310 --> 00:01:38,320
but over the nineties there was a revolution in software that's connected a lot more applications

27
00:01:38,880 --> 00:01:39,520
with a lot of

28
00:01:39,910 --> 00:01:41,370
more other applications

29
00:01:41,900 --> 00:01:45,070
and the net result of that it revolution

30
00:01:45,640 --> 00:01:48,440
from middleware right all kinds of transmission protocols

31
00:01:48,970 --> 00:01:49,900
the net result is there

32
00:01:51,060 --> 00:01:52,030
is that people

33
00:01:52,520 --> 00:01:55,050
are able to work together with other people

34
00:01:55,480 --> 00:01:56,470
on more stuff

35
00:01:56,990 --> 00:01:57,860
than everbefore

36
00:01:58,650 --> 00:02:00,140
when my applications

37
00:02:00,710 --> 00:02:02,330
can work with your applications

38
00:02:03,280 --> 00:02:05,290
virtually that we don't have to think about it

39
00:02:05,710 --> 00:02:08,650
it means that we can work together like never before

40
00:02:09,780 --> 00:02:13,360
and this i would argue some point in the mid nineteen nineties was what i

41
00:02:13,360 --> 00:02:15,970
would call the genesis moment the flat world

42
00:02:17,010 --> 00:02:22,010
because when you take the netscape revolution people able to connect with people

43
00:02:22,470 --> 00:02:24,390
over a network like never before

44
00:02:25,290 --> 00:02:32,460
and then you take the revolution in software more and more applications connecting to more other applications and everbefore

45
00:02:33,380 --> 00:02:34,480
what we created

46
00:02:34,870 --> 00:02:35,530
willy nilly

47
00:02:36,210 --> 00:02:37,750
was a global platform

48
00:02:38,320 --> 00:02:41,070
form multiple forms of collaboration

49
00:02:41,820 --> 00:02:43,210
suddenly more people

50
00:02:43,920 --> 00:02:45,960
could collaborate and connect

51
00:02:47,020 --> 00:02:48,190
on more stuff

52
00:02:48,800 --> 00:02:49,410
than ever

53
00:02:49,990 --> 00:02:51,780
before in the history of the world

54
00:02:52,300 --> 00:02:54,440
and that's to me was the genesis moment

55
00:02:54,900 --> 00:02:56,080
for the flattening the world

56
00:02:56,500 --> 00:02:57,920
now the next six latinas

57
00:02:58,510 --> 00:03:02,260
are the new forms of collaboration that's sprung from this platform

58
00:03:02,690 --> 00:03:04,920
and if come back to flatten the world even more

59
00:03:05,480 --> 00:03:08,130
go through them quickly the first courses outsourcing

60
00:03:09,100 --> 00:03:11,210
outsourcing is just a new form of collaboration

61
00:03:11,880 --> 00:03:17,210
empowered by this platform of people the people and application the application now a mighty

62
00:03:17,210 --> 00:03:22,070
if it wants to get outsource the whole bookkeeping department in north boston north dakota

63
00:03:22,070 --> 00:03:24,940
or north bong galore of this platform

64
00:03:25,660 --> 00:03:28,720
these one would be almost equally easy

65
00:03:29,320 --> 00:03:32,020
outsourcing a new form of collaboration

66
00:03:32,630 --> 00:03:37,660
the next to form of collaboration is offshoring ability to around data outsourcing is built

67
00:03:37,660 --> 00:03:42,700
from white to ke offshore is built around when china joins the world trade organization

68
00:03:43,120 --> 00:03:48,150
which takes offshoring to a whole new level now i take my factory from canton ohio

69
00:03:48,570 --> 00:03:50,290
and they move in the canton china

70
00:03:50,710 --> 00:03:54,470
and integrated into my global production system of short

71
00:03:55,370 --> 00:03:57,670
there are new form of collaboration is open sourcing

72
00:03:58,520 --> 00:03:59,420
open source to me

73
00:03:59,990 --> 00:04:03,920
is a it's open source is actually a new form of collaboration

74
00:04:04,360 --> 00:04:07,210
it's a bunch a geeks like half review sitting at home

75
00:04:07,710 --> 00:04:13,210
working on the linux operating system which today is fifteen percent the global awareness market

76
00:04:13,330 --> 00:04:17,080
brazil just announced a couple months ago it's converting the brazilian government

77
00:04:17,570 --> 00:04:18,300
until the next

78
00:04:18,750 --> 00:04:21,050
this is a whole new form of

79
00:04:21,500 --> 00:04:23,930
collaboration obviously and innovation

80
00:04:24,800 --> 00:04:28,470
and you know i mean how it's like to be microsoft's housing bill gates

81
00:04:29,380 --> 00:04:30,580
how would you like to be the same

82
00:04:33,580 --> 00:04:36,060
model is that any time anyone challenges u

83
00:04:36,880 --> 00:04:37,930
to undercut them on price

84
00:04:39,320 --> 00:04:40,110
and from

85
00:04:41,510 --> 00:04:42,420
it's hard to be free

86
00:04:43,190 --> 00:04:46,420
it's hard to beat free it's hard to beat a bunch a greek city at

87
00:04:46,420 --> 00:04:49,470
home writing the next operating system are the next

88
00:04:51,480 --> 00:04:53,970
microsoft word or adobe acrobat for free

89
00:04:54,560 --> 00:04:56,530
some doing it because they hate microsoft's

90
00:04:56,960 --> 00:04:59,990
let's be honest many doing it because they microsoft

91
00:05:00,660 --> 00:05:03,910
that's a question whether it be an open source movement if they work

92
00:05:05,680 --> 00:05:11,320
and many others do it for all the reasons you described understands best which is the peer reviewed science

93
00:05:12,460 --> 00:05:13,820
looking this algorithm

94
00:05:13,900 --> 00:05:14,570
idea about one

95
00:05:16,390 --> 00:05:19,470
what is that's this is so cool you try this

96
00:05:22,040 --> 00:05:27,970
but whatever reason they're are doing it is the only crowd in america who left the lot i

97
00:05:29,030 --> 00:05:29,420
i don't know

98
00:05:32,170 --> 00:05:34,070
he was that there was talking about

99
00:05:36,690 --> 00:05:38,270
but for whatever reason they're doing it

100
00:05:39,530 --> 00:05:44,580
this to me is a whole new form of collaboration firefox and firefox here's a

101
00:05:44,600 --> 00:05:48,300
web browser produced by nineteen year old stanford along with a twenty four year old

102
00:05:48,300 --> 00:05:49,060
and new zealand

103
00:05:50,030 --> 00:05:50,830
never met yet

104
00:05:52,120 --> 00:05:56,260
andy on one day in one month excuse me is downloaded ten million times takes

105
00:05:56,260 --> 00:05:58,970
five percent of microsoft's explores market in month

106
00:05:59,730 --> 00:06:03,850
so this is an incredibly powerful new form of collaboration

107
00:06:05,030 --> 00:06:09,500
and the open university that mighty is doing in which i had the pleasure learning

108
00:06:09,500 --> 00:06:12,110
all about today is just another expression this

109
00:06:12,800 --> 00:06:16,320
and it's gonna be i think an incredibly powerful forward innovation

110
00:06:16,770 --> 00:06:18,540
on the flatter the world against

111
00:06:18,970 --> 00:06:20,070
the fourth foremost

112
00:06:20,660 --> 00:06:21,730
collaboration on

113
00:06:22,210 --> 00:06:25,470
i call supply chain supply chain is what walmart

114
00:06:27,190 --> 00:06:32,330
and that's designed a global supply chain down to the last at some other efficiency

115
00:06:33,560 --> 00:06:38,670
so you take an item off-the-shelf and walmart in brookline and others immediately made in change in china

116
00:06:39,250 --> 00:06:42,760
wal-mart were a country it would be china largest trading partner the

117
00:06:43,480 --> 00:06:44,780
had canada and australia

118
00:06:46,180 --> 00:06:49,470
and of my friend you see sheffi use here has taught me

119
00:06:50,770 --> 00:06:54,040
something about supply chains and and he is a nice saying which step

120
00:06:54,640 --> 00:06:55,320
making stuff

121
00:06:56,380 --> 00:06:57,890
making stuff that's easy

122
00:06:58,610 --> 00:07:01,080
supply chain that's really hard

123
00:07:03,320 --> 00:07:04,240
think about walmart

124
00:07:05,130 --> 00:07:05,770
this is this

125
00:07:06,910 --> 00:07:08,570
what's the biggest company in america

126
00:07:09,660 --> 00:07:11,190
and they don't make anything

127
00:07:12,880 --> 00:07:17,520
just a supply chain in counterfeit last at of efficiency

128
00:07:18,210 --> 00:07:19,680
a whole new form of collaboration

129
00:07:20,870 --> 00:07:25,490
the fifth new form of collaboration article in sourcing in sources with dupuy yes that's

130
00:07:26,080 --> 00:07:29,040
you know the people in the funny brown trucks and the funny brown shorts

131
00:07:29,810 --> 00:07:31,300
if you think all they are doing

132
00:07:31,750 --> 00:07:32,790
is delivering packages

133
00:07:33,290 --> 00:07:33,890
u are

134
00:07:34,400 --> 00:07:35,810
not paying attention

135
00:07:36,960 --> 00:07:39,800
there's a little hint what they are up to its on the side the trucks

136
00:07:39,800 --> 00:07:43,230
now it simply says your world synchronized

137
00:07:43,230 --> 00:07:45,340
you know we can

138
00:07:45,360 --> 00:07:46,330
well it

139
00:07:46,330 --> 00:07:53,540
this friendly data representation clustering kind of similar sets and we would like to have

140
00:07:53,540 --> 00:07:58,200
some sort of brought in a bill to quantizer of compressed sensing

141
00:07:58,240 --> 00:08:02,850
the sort of intuition here intuition is the following

142
00:08:02,870 --> 00:08:03,960
we would like to

143
00:08:03,960 --> 00:08:08,760
and make sure that similar or to have similar right

144
00:08:08,850 --> 00:08:17,940
and that's the situation applies throughout all classification clustering and imprisoned but what the similar

145
00:08:17,940 --> 00:08:22,750
in sound to this one and this one is some of the time

146
00:08:22,820 --> 00:08:25,580
not necessary to look at

147
00:08:25,590 --> 00:08:26,050
but this

148
00:08:26,570 --> 00:08:29,970
this one and this one quite similar

149
00:08:30,350 --> 00:08:37,210
so so what's the notion of similarity so he actually is arguably was the geometry

150
00:08:37,210 --> 00:08:40,310
comes and one can actually say that

151
00:08:40,730 --> 00:08:44,190
so just to assumptions can be made about this

152
00:08:44,260 --> 00:08:47,540
and two assumptions can be made the following

153
00:08:47,550 --> 00:08:51,210
one what one may call the manifold assumption

154
00:08:51,350 --> 00:08:57,970
and now they've one michael in cluster assumption and other examples of what

155
00:08:57,980 --> 00:09:01,710
what do you know what they make this

156
00:09:01,720 --> 00:09:05,290
OK so let me start with the

157
00:09:06,560 --> 00:09:07,500
now suppose

158
00:09:07,510 --> 00:09:10,330
the simplest classification and that was the point

159
00:09:12,140 --> 00:09:16,240
and i want to build a classifier right classifierx from blue

160
00:09:16,250 --> 00:09:20,200
well i fall

161
00:09:20,210 --> 00:09:22,930
we all

162
00:09:22,950 --> 00:09:24,610
many of us agree

163
00:09:24,630 --> 00:09:26,960
this is a good classifier

164
00:09:29,440 --> 00:09:36,130
why is this a good classifier well simple classifier and somehow it seems to be

165
00:09:36,160 --> 00:09:37,690
the simplest possible

166
00:09:37,860 --> 00:09:44,060
the changes slightly suppose i'm doing you there are two labels points you hear about

167
00:09:44,070 --> 00:09:45,720
the underlying data

168
00:09:45,740 --> 00:09:46,870
like this

169
00:09:46,880 --> 00:09:48,590
those artists

170
00:09:49,560 --> 00:09:54,560
so i changed the problems like i tell you all the light and those are

171
00:09:55,760 --> 00:09:59,640
now what's it good classifiers

172
00:10:01,740 --> 00:10:03,030
again i don't know

173
00:10:03,040 --> 00:10:05,570
you may agree disagree with me but

174
00:10:05,590 --> 00:10:06,730
i say

175
00:10:06,740 --> 00:10:10,990
most reasonable people like agree that this one is about

176
00:10:11,110 --> 00:10:16,350
course it's

177
00:10:16,370 --> 00:10:20,830
so why is it that all lists are somehow intuitively this is that and why

178
00:10:20,830 --> 00:10:22,050
is this

179
00:10:22,070 --> 00:10:25,560
well because i actually believe that somehow

180
00:10:25,570 --> 00:10:26,890
this article

181
00:10:26,910 --> 00:10:32,220
and that's that to separate groups and the classification foundation go

182
00:10:32,280 --> 00:10:39,000
so clearly the geometry of the data on its own interest intuitive that effect on

183
00:10:39,030 --> 00:10:41,800
notion of similarity quite a bit

184
00:10:41,870 --> 00:10:47,220
now let's think about manifold assumption out

185
00:10:47,230 --> 00:10:51,910
i want to classify this work so well how the classify this point

186
00:10:52,140 --> 00:10:54,510
so if this is all i have

187
00:10:54,810 --> 00:10:59,160
i would think blue i can do several arguments why this should be too

188
00:10:59,180 --> 00:11:05,500
right now for example it's the closest point to the blue points well if linear

189
00:11:05,500 --> 00:11:10,410
classification this is on the same side the thing and so on

190
00:11:10,540 --> 00:11:15,120
however now it actually looks like this is all this

191
00:11:15,290 --> 00:11:16,410
well no

192
00:11:16,430 --> 00:11:17,290
it's a

193
00:11:17,310 --> 00:11:23,590
actually seems to be more likely to direct why i intuitions are falling

194
00:11:24,830 --> 00:11:28,850
it's not the distance which makes them much different in distance along the shore sort

195
00:11:28,850 --> 00:11:32,320
of genetic distance which makes a lot of

196
00:11:33,200 --> 00:11:37,830
please please ask questions and so on if you disagree with

197
00:11:41,670 --> 00:11:45,340
OK so this one might want to make a manifold assumptions sort of in the

198
00:11:46,280 --> 00:11:50,050
foreman usually look at the geodesic distance

199
00:11:50,070 --> 00:11:55,760
not them understand and then the distance is not very important agendas distances more

200
00:11:55,790 --> 00:11:56,800
in one

201
00:11:56,810 --> 00:12:00,070
well atleast administration

202
00:12:00,080 --> 00:12:04,460
here is another thing here is the cluster assumption again if you just have to

203
00:12:04,510 --> 00:12:10,080
point probably because black and blue right side to the new cluster that well all

204
00:12:10,540 --> 00:12:11,660
like this

205
00:12:11,680 --> 00:12:14,330
you can think of this being a mixture of gaussians

206
00:12:15,410 --> 00:12:18,030
the box here

207
00:12:18,050 --> 00:12:18,690
now there

208
00:12:18,690 --> 00:12:21,240
the first question is how do you choose the examples

209
00:12:21,280 --> 00:12:23,130
on each of these rounds

210
00:12:23,230 --> 00:12:27,240
and here we will do is what just concentrate on the hardest examples

211
00:12:27,260 --> 00:12:32,350
OK you always will be focusing on the hardest examples and hardest means examples which

212
00:12:32,350 --> 00:12:35,630
have most often been misclassified by the previous

213
00:12:35,680 --> 00:12:39,770
rules of thumb that you've already computed and that second question is how do you

214
00:12:39,770 --> 00:12:44,280
take all these rules of thumb and combine them into a single prediction rule

215
00:12:44,290 --> 00:12:48,520
and there will just be something really simple which is to take a majority vote

216
00:12:48,520 --> 00:12:52,910
or weighted majority vote of the rules of thumb that we computed

217
00:12:52,930 --> 00:12:54,690
so that in the challenge

218
00:12:54,700 --> 00:12:55,630
is how

219
00:12:55,640 --> 00:12:58,650
boosting works of course and going to go into more detail

220
00:13:00,790 --> 00:13:05,460
boosting is a general method of converting these rough rules of thumb into a highly

221
00:13:05,460 --> 00:13:10,620
accurate prediction rule has a said it does have more technical definition which is often

222
00:13:10,630 --> 00:13:12,980
overlooked in much of the literature

223
00:13:14,640 --> 00:13:19,910
boosting begins with an assumption any learning algorithm any learning method needs to begin with

224
00:13:19,910 --> 00:13:21,610
some kind of assumptions

225
00:13:21,650 --> 00:13:25,810
of some kind and really the study of boosting starts with

226
00:13:25,860 --> 00:13:28,140
kind of the difference

227
00:13:28,180 --> 00:13:32,460
kind of assumptions so we start with this assumption which says that we're assuming

228
00:13:32,470 --> 00:13:35,900
that we have available to us a weak learning algorithm

229
00:13:35,950 --> 00:13:38,990
this procedure that i was talking about for deriving

230
00:13:39,010 --> 00:13:40,860
rough rules of thumb

231
00:13:40,870 --> 00:13:46,810
and that's the procedure which can consistently find classifiers what i was calling earlier rules

232
00:13:46,810 --> 00:13:50,280
of thumb from now on i'll be calling them weak classifiers

233
00:13:50,330 --> 00:13:54,670
which are at least slightly better than random guessing so we always do a little

234
00:13:54,670 --> 00:13:57,230
bit better than random guessing

235
00:13:57,240 --> 00:13:59,850
so you have a two class problem

236
00:13:59,860 --> 00:14:01,640
and you guess

237
00:14:01,660 --> 00:14:06,330
at random you make predictions are random you'll be right exactly half the the time

238
00:14:06,350 --> 00:14:10,170
so we assume that this procedure is a little bit better than fifty percent it

239
00:14:10,170 --> 00:14:15,120
can come up with classifiers which have accuracy maybe fifty five percent

240
00:14:16,240 --> 00:14:21,290
OK so this is called the weak learning assumption and and it's the starting point

241
00:14:21,290 --> 00:14:23,470
for the study of boosting algorithms

242
00:14:23,480 --> 00:14:28,110
so given that assumption of boosting algorithm is one which given

243
00:14:28,130 --> 00:14:32,630
sufficient data whatever that means can provably construct

244
00:14:32,640 --> 00:14:34,380
a single classifier

245
00:14:34,390 --> 00:14:36,480
with very high accuracy

246
00:14:36,490 --> 00:14:40,810
maybe ninety nine percent c start with all these weak classifiers with that are close

247
00:14:41,580 --> 00:14:46,550
fifty five percent saying whose sum up to accuracy ninety nine percent

248
00:14:48,320 --> 00:14:51,960
OK so that's a very high level introduction to boosting

249
00:14:52,030 --> 00:14:55,280
and so what i'm actually going to talk about in this tutorial

250
00:14:55,300 --> 00:15:00,870
on going to start with some very brief background on boosting some very brief historical

251
00:15:02,110 --> 00:15:05,130
and they're going to talk about the basic boosting algorithm that we're going to look

252
00:15:05,130 --> 00:15:10,390
at which is the adaboost algorithm and the core theory surrounding adaboost

253
00:15:10,470 --> 00:15:14,520
then i'll talk about some the other ways of thinking about boosting

254
00:15:14,570 --> 00:15:19,940
and then finally we'll get to some experiments applications and extensions

255
00:15:19,960 --> 00:15:23,070
and at some point will take a break

256
00:15:23,090 --> 00:15:27,140
OK so some very brief historical background

257
00:15:27,920 --> 00:15:30,150
boosting has its roots

258
00:15:30,160 --> 00:15:32,890
in a learning model called the

259
00:15:32,930 --> 00:15:36,580
valiant or PAC learning model

260
00:15:36,590 --> 00:15:40,180
OK called the PAC learning model

261
00:15:40,190 --> 00:15:42,870
so in this model

262
00:15:42,890 --> 00:15:45,890
it's a very simple straightforward model of learning

263
00:15:45,900 --> 00:15:51,030
you seem that the learning algorithm has access to random examples which are coming from

264
00:15:51,030 --> 00:15:57,410
some unknown arbitrary distribution and one of the important things is that this distribution is

265
00:15:59,290 --> 00:16:02,930
so there are at least two kinds of learning algorithms that you might be interested

266
00:16:02,930 --> 00:16:04,990
in in this model the first is

267
00:16:05,000 --> 00:16:07,690
strong PAC learning model

268
00:16:07,700 --> 00:16:09,890
so strong PAC learning model

269
00:16:09,940 --> 00:16:12,700
sorry strong PAC learning algorithm

270
00:16:12,710 --> 00:16:14,900
is an algorithm such that for any

271
00:16:14,910 --> 00:16:17,690
distribution of these random examples

272
00:16:17,700 --> 00:16:23,060
with high probability one it's given polynomially many examples and it usually also required to

273
00:16:23,060 --> 00:16:25,030
run in polynomial time

274
00:16:25,050 --> 00:16:27,560
confined to a classifier with

275
00:16:27,580 --> 00:16:30,280
arbitrarily small generalisation error

276
00:16:30,290 --> 00:16:34,450
OK generalisation error is just you're expected test error

277
00:16:34,500 --> 00:16:36,220
so you can drive the

278
00:16:36,230 --> 00:16:37,300
air raids

279
00:16:37,350 --> 00:16:38,600
on the test set

280
00:16:38,610 --> 00:16:42,530
as small as you want in the strong PAC learning model

281
00:16:42,550 --> 00:16:47,350
in the weak PAC learning model or weak PAC learning algorithm is an algorithm

282
00:16:47,400 --> 00:16:50,520
that has all of these same requirements

283
00:16:50,540 --> 00:16:53,690
but the generalisation error doesn't have to be

284
00:16:53,710 --> 00:16:55,620
you don't have to be able to drive the

285
00:16:55,640 --> 00:16:58,980
generalisation there to be an arbitrarily small number

286
00:16:59,060 --> 00:17:02,740
the generalisation are only needs to be a little bit better than random guessing which

287
00:17:02,740 --> 00:17:04,070
means it has to be

288
00:17:04,080 --> 00:17:05,790
just a little bit better than

289
00:17:05,900 --> 00:17:08,800
half and the two class

290
00:17:09,680 --> 00:17:13,630
pac by the way stands for probably approximately correct

291
00:17:13,750 --> 00:17:19,640
because with high probability one a classifier which is approximately correct

292
00:17:19,750 --> 00:17:22,910
so current and valiant were the ones who asked

293
00:17:22,920 --> 00:17:26,470
does weak learnability imply strong learnability

294
00:17:26,510 --> 00:17:29,060
in other words if you have a week

295
00:17:29,080 --> 00:17:30,590
pac learning algorithm

296
00:17:30,600 --> 00:17:36,300
can you efficiently converted into a strong PAC learning algorithm

297
00:17:36,350 --> 00:17:41,800
so the first boosting algorithm with one that i came up with that

298
00:17:41,810 --> 00:17:43,790
a long long time ago

299
00:17:43,870 --> 00:17:49,470
so this was kind of this complicated recursive boosting algorithm where

300
00:17:49,700 --> 00:17:53,360
what do what you do is you would you've got this week learner this week

301
00:17:53,360 --> 00:17:57,880
learning algorithm and you would run it three times on three multiply distributions

302
00:17:57,890 --> 00:17:59,890
which would give you the small but

303
00:17:59,900 --> 00:18:05,190
a significant boost in accuracy and then you just apply that procedure recursively

304
00:18:05,200 --> 00:18:09,810
a year later you are trying to form up with a really nice algorithm called

305
00:18:09,810 --> 00:18:13,260
the boost by majority algorithm which is provably optimal

306
00:18:13,270 --> 00:18:15,670
in a very strong sense

307
00:18:15,690 --> 00:18:19,840
but both of these algorithms had strong practical drawbacks there were some

308
00:18:19,850 --> 00:18:22,370
early experiments with them but they were

309
00:18:22,380 --> 00:18:25,890
basically limited practicality these algorithms

310
00:18:26,070 --> 00:18:30,480
so the algorithm going to focus on in this talk is called the adaboost algorithm

311
00:18:30,560 --> 00:18:34,140
which has which has

312
00:18:34,150 --> 00:18:39,390
the boosting property but also has very strong practical advantages over the earlier algorithms

313
00:18:39,490 --> 00:18:43,040
and since it was introduced there's been a whole lot of work on adaboost which

314
00:18:43,040 --> 00:18:46,870
i obviously not going to talk about

315
00:18:46,890 --> 00:18:47,890
and all that

316
00:18:47,900 --> 00:18:50,570
so that's a little historical background so

317
00:18:50,590 --> 00:18:54,540
let's get to the adaboost algorithm itself so in this talk part

318
00:18:54,610 --> 00:18:58,290
going to talk about the basic algorithm which means the adaboost algorithm and

319
00:18:58,330 --> 00:19:02,280
there were really core important most important theory in my view

320
00:19:02,290 --> 00:19:03,630
of adaboost

321
00:19:03,650 --> 00:19:07,110
o thing going to talk about for sale introduced the algorithm

322
00:19:07,110 --> 00:19:09,920
and far more heated discussion

323
00:19:09,940 --> 00:19:13,100
but that one example so

324
00:19:13,110 --> 00:19:16,310
under the CPC setting if i had impressions

325
00:19:16,320 --> 00:19:19,470
and by becomes the expected number of clicks

326
00:19:19,570 --> 00:19:23,180
and this times the cost per click is the revenue

327
00:19:23,200 --> 00:19:25,600
but the cost cost per click

328
00:19:25,600 --> 00:19:29,570
it's been determined by some sort of option

329
00:19:29,590 --> 00:19:31,850
and then the CPS

330
00:19:31,870 --> 00:19:35,950
i have one extra factor and it was is this is the conversion

331
00:19:35,980 --> 00:19:37,550
what this is is

332
00:19:37,550 --> 00:19:41,880
what is the probability that a user was clicked on me as will actually go

333
00:19:41,880 --> 00:19:45,010
on and perform the action

334
00:19:45,030 --> 00:19:46,220
and this becomes

335
00:19:46,240 --> 00:19:49,720
very top big and say because

336
00:19:49,740 --> 00:19:54,430
this is not something that you can look because the action the user performs and

337
00:19:55,830 --> 00:19:58,170
it's very different and what it is

338
00:19:58,190 --> 00:20:03,000
actions that the user is doing on some other website but you have no control

339
00:20:03,020 --> 00:20:05,730
so it's very hard to estimate the conversion

340
00:20:05,750 --> 00:20:08,370
let's at is actually quite interesting

341
00:20:08,380 --> 00:20:12,810
so optimizing for CPA or whatever you want to see it becomes very hard problem

342
00:20:12,810 --> 00:20:14,620
for the have in this is why

343
00:20:14,660 --> 00:20:19,310
it because be the model the revenue model that used CPM CPC

344
00:20:19,320 --> 00:20:20,570
and slowly

345
00:20:20,650 --> 00:20:24,000
are moving towards CPM or but is still

346
00:20:24,030 --> 00:20:25,950
it states

347
00:20:26,880 --> 00:20:28,510
is one slide which

348
00:20:28,530 --> 00:20:30,780
essentially put of this context

349
00:20:30,800 --> 00:20:34,090
suppose i wanted to maximize revenue

350
00:20:34,110 --> 00:20:38,570
under CPM or cost per impression what i just want to maximize the number of

351
00:20:39,690 --> 00:20:42,480
which means that i want to maximize the website traffic

352
00:20:42,500 --> 00:20:44,820
on the of my access

353
00:20:44,830 --> 00:20:46,620
and the CPC more

354
00:20:46,630 --> 00:20:50,310
i need to maximise the number of clicks not just impressions so this means i

355
00:20:50,310 --> 00:20:52,780
have many impressions

356
00:20:52,840 --> 00:20:54,630
so high in the same attractor

357
00:20:54,650 --> 00:20:57,360
blast actually be relevant so that

358
00:20:57,410 --> 00:21:00,650
the users are induced to actually click on that

359
00:21:00,690 --> 00:21:03,890
so both website traffic and that the elements

360
00:21:03,900 --> 00:21:05,470
and the CPM or

361
00:21:05,490 --> 00:21:07,920
we need not only to actually

362
00:21:08,030 --> 00:21:12,910
but the advertiser's landing page quality on some matters it and was very messy very

363
00:21:13,110 --> 00:21:17,210
quickly then the user will probably not all that and in fact the page

364
00:21:17,250 --> 00:21:19,810
it's probably just leave that

365
00:21:19,810 --> 00:21:20,500
and so

366
00:21:20,530 --> 00:21:25,900
the network will need some estimate of not only will the user condescension

367
00:21:25,900 --> 00:21:30,420
will the user actually like the landing page of that it's which becomes an even

368
00:21:30,420 --> 00:21:32,430
more complicated question

369
00:21:32,460 --> 00:21:36,450
but on the other hand from the advertisers for you

370
00:21:36,470 --> 00:21:40,380
gentlemen increases as we move from CPM CPC

371
00:21:40,480 --> 00:21:42,380
CPA and the

372
00:21:42,390 --> 00:21:47,120
extreme skiing if the action if if i paid only for certain action and action

373
00:21:47,120 --> 00:21:52,620
is that the user must find the problem that is just bunch of because it

374
00:21:53,190 --> 00:21:55,100
guarantee that for every

375
00:21:55,100 --> 00:21:58,690
o dollars that's been amazing and make some extra

376
00:21:58,720 --> 00:22:00,910
by selling products

377
00:22:00,920 --> 00:22:02,330
and correspondingly

378
00:22:02,340 --> 00:22:04,940
the price is that is willing to be

379
00:22:05,000 --> 00:22:07,920
increases as we go from CPU to CPU

380
00:22:07,930 --> 00:22:13,270
but again the difficulty of picking the best at spatial also increases from CPM CPC

381
00:22:13,310 --> 00:22:17,410
so the job at it becomes much harder as see

382
00:22:17,420 --> 00:22:23,060
and typically CPM CPC at the smallest

383
00:22:23,110 --> 00:22:27,860
so we have looked at the revenue model essentially how these places are determined

384
00:22:27,880 --> 00:22:30,500
how much use and what's in it

385
00:22:30,530 --> 00:22:33,170
what does this advertising

386
00:22:33,190 --> 00:22:35,720
one from the users are from the

387
00:22:35,730 --> 00:22:40,410
the second question of the second dimension of online advertising is

388
00:22:40,420 --> 00:22:45,650
and what's it into a show my at what hope we expect users to interact

389
00:22:45,680 --> 00:22:46,830
with it

390
00:22:46,840 --> 00:22:49,560
and this we have to display advertising

391
00:22:49,580 --> 00:22:50,770
on that

392
00:22:50,830 --> 00:22:56,120
and sponsored search and essentially asking what we should do this what is the aim

393
00:22:56,120 --> 00:22:57,180
of me

394
00:22:57,210 --> 00:22:59,410
what they want

395
00:22:59,420 --> 00:23:02,870
the simplest is that of display advertising

396
00:23:03,900 --> 00:23:07,210
this is an an example of that is from new york times it's a health

397
00:23:08,560 --> 00:23:12,780
and then on the other hand that netflix that says well now you can try

398
00:23:12,780 --> 00:23:14,980
and fix fifty feet

399
00:23:15,010 --> 00:23:19,110
so netflix doesn't really have anything to do do with health is anything that see

400
00:23:19,110 --> 00:23:21,370
one of six once in this case

401
00:23:21,400 --> 00:23:24,020
is for users as many users as possible

402
00:23:24,050 --> 00:23:26,830
that metrics is going to promotion

403
00:23:26,840 --> 00:23:27,910
tries to flee

404
00:23:27,920 --> 00:23:32,060
well if you don't know about metrics i'm netflix i exist you can get movies

405
00:23:32,060 --> 00:23:33,800
from me and so

406
00:23:33,810 --> 00:23:36,510
so netflix simply find the

407
00:23:36,530 --> 00:23:41,190
there that i existed surprise increase in random

408
00:23:41,200 --> 00:23:45,360
and in this particular case one netflix wants is that the actually what should will

409
00:23:45,360 --> 00:23:47,980
show that as many users as possible

410
00:23:47,980 --> 00:23:56,940
we've seen the last time

411
00:23:56,980 --> 00:23:59,130
that using

412
00:23:59,140 --> 00:24:00,850
the islands of virus

413
00:24:02,350 --> 00:24:03,790
if you have the current

414
00:24:03,800 --> 00:24:08,800
going straight into the blackboard perpendicular to the blackboards

415
00:24:11,240 --> 00:24:13,620
we get the magnetic fields

416
00:24:13,630 --> 00:24:14,960
and the distance

417
00:24:16,780 --> 00:24:18,720
the magnetic field

418
00:24:18,740 --> 00:24:22,010
and you mentioned the the circle

419
00:24:22,020 --> 00:24:23,950
b here

420
00:24:25,860 --> 00:24:29,620
and that the strength of the magnetic fields

421
00:24:29,740 --> 00:24:31,650
he calls you zero

422
00:24:31,700 --> 00:24:33,330
times i

423
00:24:33,340 --> 00:24:38,400
divided by two pi are

424
00:24:38,500 --> 00:24:40,120
if you

425
00:24:40,190 --> 00:24:42,590
walk around this circle

426
00:24:42,610 --> 00:24:44,720
just walk around

427
00:24:44,720 --> 00:24:48,070
and you carve up this circle

428
00:24:48,080 --> 00:24:50,590
in little elements dl

429
00:24:56,400 --> 00:25:00,860
calculate the closed circle integral

430
00:25:00,870 --> 00:25:04,000
so the close circle

431
00:25:04,000 --> 00:25:05,260
of b

432
00:25:05,420 --> 00:25:07,810
adults dl

433
00:25:07,840 --> 00:25:09,860
so everywhere locally

434
00:25:09,920 --> 00:25:11,310
you've got to be ways

435
00:25:11,330 --> 00:25:15,890
the album being yellow exactly the same direction everywhere

436
00:25:15,920 --> 00:25:19,250
there you will find that is obviously is b

437
00:25:19,300 --> 00:25:22,360
i could buy are

438
00:25:22,400 --> 00:25:25,670
but the times two pi are

439
00:25:25,700 --> 00:25:27,610
he calls also

440
00:25:27,690 --> 00:25:32,390
new zero times by i

441
00:25:32,450 --> 00:25:35,360
this the al here

442
00:25:35,370 --> 00:25:39,110
has nothing to do with this dl you don't confuse the two

443
00:25:39,170 --> 00:25:40,370
this is the l

444
00:25:40,390 --> 00:25:44,730
is a small amount of length in the y goes into the blackboard which carries

445
00:25:44,730 --> 00:25:46,110
the current

446
00:25:46,110 --> 00:25:47,440
this is the l

447
00:25:47,470 --> 00:25:48,540
is simply

448
00:25:48,560 --> 00:25:51,260
you're dl when you walk around

449
00:25:52,590 --> 00:25:53,690
and why

450
00:25:53,700 --> 00:25:59,820
it doesn't matter what distance you walk around you always get new zero times i

451
00:25:59,860 --> 00:26:03,290
you see the right in front of your eyes because b is inversely proportional

452
00:26:03,300 --> 00:26:04,880
two are

453
00:26:04,890 --> 00:26:07,010
and it was and here

454
00:26:07,040 --> 00:26:09,270
who first to recognise

455
00:26:10,260 --> 00:26:12,960
you don't have to walk around in a circle

456
00:26:13,010 --> 00:26:15,230
to get the answer museo arrived

457
00:26:15,240 --> 00:26:17,740
but that you could walk around

458
00:26:18,860 --> 00:26:24,230
crew could pass as long as it is a closed class

459
00:26:24,290 --> 00:26:26,580
something like this

460
00:26:26,630 --> 00:26:29,010
and now

461
00:26:29,040 --> 00:26:31,640
we have here the local b

462
00:26:31,640 --> 00:26:34,260
which of course is perpendicular

463
00:26:35,210 --> 00:26:36,610
this radius

464
00:26:36,630 --> 00:26:40,580
and you you have your local dl

465
00:26:40,640 --> 00:26:44,040
and now you go around

466
00:26:44,930 --> 00:26:48,790
circle of any path doesn't have to be historical

467
00:26:48,800 --> 00:26:51,710
doctor dl

468
00:26:51,730 --> 00:26:52,570
that now

469
00:26:52,580 --> 00:26:57,960
becomes new zero times i which is known as an MP is law

470
00:26:58,040 --> 00:27:00,730
and i then is often

471
00:27:00,730 --> 00:27:02,390
given index

472
00:27:02,430 --> 00:27:03,450
and closed

473
00:27:03,460 --> 00:27:08,890
the current which is enclosed by their past

474
00:27:08,910 --> 00:27:11,910
it is actually easy to prove this using the o

475
00:27:11,920 --> 00:27:14,660
and survives formalism

476
00:27:14,690 --> 00:27:19,050
this is almost the third maxwell's equations we already had two out of four

477
00:27:19,080 --> 00:27:25,790
it almost number three not quite going to amend it in the future

478
00:27:25,800 --> 00:27:28,430
what is ill-defined a little bit in this

479
00:27:28,510 --> 00:27:29,540
equation is

480
00:27:29,570 --> 00:27:32,450
what we mean by and closed

481
00:27:32,610 --> 00:27:35,610
i'm going to define and also uniquely

482
00:27:35,630 --> 00:27:39,360
that there was never any misunderstanding

483
00:27:39,450 --> 00:27:41,140
if i have a

484
00:27:46,390 --> 00:27:50,850
close classifying chosen that's as i walk

485
00:27:50,890 --> 00:27:54,140
i have to attached to that closed loop

486
00:27:54,180 --> 00:27:57,790
the surface an open service that mandatory

487
00:27:57,800 --> 00:28:00,140
you can make it flat that's why

488
00:28:00,160 --> 00:28:01,580
you are free to choose it

489
00:28:01,600 --> 00:28:03,070
you can also make it

490
00:28:03,100 --> 00:28:07,090
one of the plastic bags open year put your hands in your

491
00:28:08,400 --> 00:28:11,960
like i had any services fine you must touch

492
00:28:12,920 --> 00:28:17,740
look a surface so you have some placid you could be walking

493
00:28:17,800 --> 00:28:20,070
and and this would be perfectly fine open

494
00:28:20,110 --> 00:28:21,380
over the surface

495
00:28:21,480 --> 00:28:22,520
b flat

496
00:28:22,560 --> 00:28:24,270
but it could also be open

497
00:28:24,280 --> 00:28:25,960
don't like that

498
00:28:25,970 --> 00:28:26,810
and now

499
00:28:26,900 --> 00:28:28,880
i can define uniquely

500
00:28:28,900 --> 00:28:30,030
what it means

501
00:28:30,670 --> 00:28:32,820
what i mean by this i closed

502
00:28:32,870 --> 00:28:35,000
because if not i have the current

503
00:28:35,020 --> 00:28:39,320
go through this surface and pokes out here

504
00:28:39,350 --> 00:28:41,100
then i have current

505
00:28:41,150 --> 00:28:44,850
penetrating the surface and that is uniquely defined

506
00:28:44,860 --> 00:28:49,300
and if i have another one coming in for the surface

507
00:28:49,310 --> 00:28:51,160
four i two

508
00:28:51,170 --> 00:28:52,720
this is penetrating

509
00:28:52,770 --> 00:28:57,500
that surface

510
00:28:57,540 --> 00:28:59,420
by convention

511
00:28:59,500 --> 00:29:00,920
if you go

512
00:29:04,150 --> 00:29:09,410
we follow the same notation that we had before the right-hand corkscrew notation

513
00:29:09,460 --> 00:29:13,090
the connection between magnetic fields and currents

514
00:29:13,110 --> 00:29:14,680
if you go around

515
00:29:14,730 --> 00:29:17,940
clockwise seen from this side

516
00:29:17,990 --> 00:29:20,060
so you go clockwise

517
00:29:20,110 --> 00:29:23,130
then i one as i have here

518
00:29:23,180 --> 00:29:26,110
new equation have to be larger than zero

519
00:29:26,120 --> 00:29:27,630
i two

520
00:29:27,650 --> 00:29:30,780
it is then smaller than zero

521
00:29:30,810 --> 00:29:33,350
but if you decide to go counter-clockwise

522
00:29:33,360 --> 00:29:34,960
which is perfectly fine

523
00:29:34,970 --> 00:29:36,070
and please law

524
00:29:36,090 --> 00:29:39,110
doesn't all dictating which direction you have to

525
00:29:39,110 --> 00:29:40,570
march around

526
00:29:40,600 --> 00:29:42,500
then i one

527
00:29:42,520 --> 00:29:43,800
it would be

528
00:29:43,800 --> 00:29:46,170
negative and then i two

529
00:29:46,220 --> 00:29:47,070
it would be

530
00:29:47,090 --> 00:29:48,730
positive so we follow the

531
00:29:48,810 --> 00:29:50,470
right hand corkscrew

532
00:29:51,440 --> 00:29:55,800
and so if you want to amend now

533
00:29:55,810 --> 00:29:58,350
p is well to do me a favor

534
00:29:58,400 --> 00:30:01,340
but you don't do books to favour because all the books use the word and

535
00:30:02,480 --> 00:30:04,000
i would like to see this

536
00:30:04,020 --> 00:30:06,040
replaced by

537
00:30:06,100 --> 00:30:11,290
penetration the penetration of the surface of the current that is uniquely defined

538
00:30:12,310 --> 00:30:14,670
the current enclosed by the loop

539
00:30:14,710 --> 00:30:17,850
is ill-defined

540
00:30:17,860 --> 00:30:21,220
because we possible when we apply MP's law

541
00:30:21,300 --> 00:30:22,500
real try

542
00:30:22,550 --> 00:30:27,350
to find easy passes around circle sometimes sometimes rectangles

543
00:30:27,400 --> 00:30:31,170
and since you are free to choose any surface that you would attach

544
00:30:32,070 --> 00:30:35,550
if you can get away with it use the flat surface

545
00:30:35,560 --> 00:30:37,060
but you can always get away

546
00:30:37,070 --> 00:30:39,340
with a flat surface

547
00:30:39,380 --> 00:30:42,130
so to wrap rapid recipes as follows you

548
00:30:42,160 --> 00:30:44,800
choose you closed loop

549
00:30:44,820 --> 00:30:46,570
any help is allowed

550
00:30:46,570 --> 00:30:52,710
around sixty people here were interviewed work shadow that are still in the surveys

551
00:30:52,830 --> 00:30:57,470
which gave us a bunch of requirements for both use cases

552
00:30:57,550 --> 00:31:00,970
so i'm not going to go through the use cases to the requirements but

553
00:31:01,530 --> 00:31:07,810
so the climbers are mainly stay around context modelling and

554
00:31:07,830 --> 00:31:16,150
process modelling and the second requirements are about how do we streamline the proposal development

555
00:31:16,150 --> 00:31:22,020
process with an very the technologies that are being developed in active so i last

556
00:31:22,020 --> 00:31:27,950
here was to sort of matter whether the area requirements and what is being developed

557
00:31:27,950 --> 00:31:31,570
in active and to assure that there is

558
00:31:31,790 --> 00:31:36,270
that's what is being developed in active is about it for the business for business

559
00:31:38,210 --> 00:31:40,900
so i talked about requirements gathering

560
00:31:42,810 --> 00:31:45,110
this figure shows

561
00:31:45,170 --> 00:31:51,690
what activities are we doing in scope of the project so we the input to

562
00:31:51,690 --> 00:31:56,350
the system is of course the requirements i just mentioned the second input is data

563
00:31:56,390 --> 00:32:01,990
so we concentrated then on data collection because this is the necessary part so should

564
00:32:01,990 --> 00:32:06,550
the the process modelling and mining the context modelling and mining and the children from

565
00:32:06,550 --> 00:32:11,190
knowledge integration and sharing which are all part of active actually have enough data to

566
00:32:11,190 --> 00:32:18,550
produce valid results a second that thing with that is that some of the requirements

567
00:32:18,570 --> 00:32:28,950
actually were not from the requirements were for context analysis application which means users wanted

568
00:32:28,970 --> 00:32:35,390
finer granularity content they wanted different this ways of displaying results they wanted to search

569
00:32:35,390 --> 00:32:40,910
for experts so we developed this content analysis components and reinvented all of that in

570
00:32:40,910 --> 00:32:48,630
our prototypes with the end goal to users and validates technologies

571
00:32:57,470 --> 00:33:04,590
having delayed time between switching my slides i'm sorry

572
00:33:04,630 --> 00:33:09,130
i would say

573
00:33:18,490 --> 00:33:26,870
the second activity was data collection and we started off with modifying our largest develop

574
00:33:27,130 --> 00:33:33,570
see bill application which is an application for search and browsing of large environments

575
00:33:33,650 --> 00:33:38,790
which i'll show demonstrate a bit later we modified it cynically log lots of actions

576
00:33:39,290 --> 00:33:44,130
we don't log only downloads of document but we log also what the user clicks

577
00:33:44,130 --> 00:33:50,210
on what he browses through what he downloads and

578
00:33:50,230 --> 00:33:56,450
we collected around seven hundred thousand records of the user activities then the second type

579
00:33:56,460 --> 00:34:03,090
of information that we we provided its skills so our employees are encouraged to make

580
00:34:03,090 --> 00:34:08,410
this listings of skills because this is what the company uses to find them and

581
00:34:08,410 --> 00:34:14,630
to staff them projects so it's in the interest of this structured database of skills

582
00:34:15,070 --> 00:34:19,610
so we had that and the third source of information that were provided was our

583
00:34:19,610 --> 00:34:26,530
common our document repository and what's very interesting about this repository is that it's accompanied

584
00:34:26,530 --> 00:34:32,930
by metadata this metadata is actually manually designed is based on nineteen optics on and

585
00:34:32,930 --> 00:34:35,870
here on the right-hand side you can see some of the categories and the values

586
00:34:35,870 --> 00:34:41,250
of the categories so pattern of information would be what industries is not document belong

587
00:34:41,250 --> 00:34:46,630
to what offering is it what type of document would be business functions are trying

588
00:34:46,640 --> 00:34:53,910
to accomplish so easy for learning for HR outsourcing is for accounting this is

589
00:34:53,930 --> 00:34:56,730
today there are people in accenture who actually higher

590
00:34:56,750 --> 00:35:01,160
i was main job is actually to upload these documents and to label them based

591
00:35:01,160 --> 00:35:02,950
on the in the tsunami

592
00:35:04,650 --> 00:35:08,110
so all of this was provided and then in the second year of the project

593
00:35:08,110 --> 00:35:13,190
we realised out of work with context mining and michael speaking about that i believe

594
00:35:13,210 --> 00:35:18,200
on monday we realize that it's not still not enough data to get useful context

595
00:35:18,220 --> 00:35:21,090
so we added

596
00:35:21,110 --> 00:35:27,710
some log information from our official enterprise search engine so and this is there is

597
00:35:27,710 --> 00:35:32,690
around seven point two million actions over two years now these actions were not as

598
00:35:32,690 --> 00:35:38,070
finely grained as we managed to do with our sable application but this means it

599
00:35:38,070 --> 00:35:43,170
was lot of data to do something to to find context to do work in

600
00:35:43,170 --> 00:35:45,270
the project

601
00:35:45,290 --> 00:35:48,110
but the

602
00:35:48,130 --> 00:35:52,350
OK constant

603
00:35:52,390 --> 00:35:54,470
still out

604
00:35:54,630 --> 00:36:00,050
the the first part of that we could activated sable

605
00:36:00,070 --> 00:36:05,190
and it integrates a number of components it integrates those content analysis components of about

606
00:36:05,190 --> 00:36:12,410
mentioned before which are basically expert graphics search and in it integrates context modelling context

607
00:36:12,410 --> 00:36:19,810
mining companies discovering and miseries asian switching so

608
00:36:19,910 --> 00:36:25,030
i'll go firstly through the content analysis components is is the first part of the

609
00:36:25,070 --> 00:36:28,720
first of components that we have developed in the first year so some of you

610
00:36:28,720 --> 00:36:32,950
who are here for the first active summer school might have heard about them but

611
00:36:32,950 --> 00:36:37,330
i'll just go quickly and then i'll talk a bit about the context modelling for

612
00:36:37,330 --> 00:36:40,350
the accenture case

613
00:36:40,390 --> 00:36:46,630
what we found out from my requirements is that no matter what process i users

614
00:36:46,960 --> 00:36:52,750
doing what tasks they're doing their daily work they rarely need whole documents what they

615
00:36:52,760 --> 00:36:54,670
need is to find people

616
00:36:54,770 --> 00:37:00,590
who have maybe worked on similar projects with they need sometimes in the find clients

617
00:37:00,590 --> 00:37:03,510
for which we have done similar where vendors

618
00:37:03,920 --> 00:37:08,800
who might be used for training or for whom we have useful similar work is

619
00:37:08,970 --> 00:37:13,850
that people so all of this and this this reusable pieces of information

620
00:37:13,850 --> 00:37:17,870
that this is what the islands are searching for

621
00:37:19,510 --> 00:37:23,800
in order to provide some of this we have developed two components one is around

622
00:37:23,800 --> 00:37:28,470
expert search and the other one is about graphic search explaining in while of it

623
00:37:28,480 --> 00:37:34,260
but they are still the expert search is actually

624
00:37:34,260 --> 00:37:36,270
the cosine

625
00:37:36,350 --> 00:37:38,630
of omega t

626
00:37:38,640 --> 00:37:41,730
and i'm going to use my same transmitter for that it's going to be a

627
00:37:45,020 --> 00:37:46,990
call this wire one

628
00:37:47,040 --> 00:37:50,370
and i call this y two

629
00:37:50,370 --> 00:37:52,440
regarding the wave

630
00:37:55,980 --> 00:37:59,170
o sinusoidal that going to propagate

631
00:37:59,200 --> 00:38:04,670
in these wires and what's going to happen is not what you think

632
00:38:04,710 --> 00:38:08,130
first of all

633
00:38:08,130 --> 00:38:10,500
there cannot be any electric field

634
00:38:10,560 --> 00:38:13,760
in the why is because it's a conductor

635
00:38:13,780 --> 00:38:15,350
so the electric field

636
00:38:15,350 --> 00:38:17,650
in this direction

637
00:38:17,670 --> 00:38:22,130
not exist so there is no potential difference from here to there

638
00:38:22,240 --> 00:38:26,620
but if there is any potential difference is going to be between here and there

639
00:38:26,620 --> 00:38:29,080
if i take this wire

640
00:38:29,100 --> 00:38:31,680
and i make a cross section of the wire

641
00:38:31,790 --> 00:38:33,170
this is why along

642
00:38:33,230 --> 00:38:36,020
and this is why i the any moment in time

643
00:38:36,030 --> 00:38:37,720
as the voltage wave

644
00:38:39,510 --> 00:38:41,000
this may be

645
00:38:43,790 --> 00:38:46,640
that is the role of us

646
00:38:46,670 --> 00:38:48,390
and this will be negative

647
00:38:48,410 --> 00:38:52,930
that's the role of s

648
00:38:53,030 --> 00:38:57,570
and the electric field lines that go like this

649
00:38:57,580 --> 00:39:01,860
so that means if this is at this moment in time positive and negative

650
00:39:01,910 --> 00:39:05,460
and the electric field in this direction between these two ideas

651
00:39:05,480 --> 00:39:09,200
not along the wire between the lines

652
00:39:09,250 --> 00:39:11,390
but then of course also going to be

653
00:39:11,420 --> 00:39:14,160
locations with this in mind in this class

654
00:39:14,180 --> 00:39:16,490
and then what this was this is minus

655
00:39:16,560 --> 00:39:21,160
so on an electric field is in this direction and electric fields in this direction

656
00:39:21,200 --> 00:39:25,670
so the potential difference between the EU is positive

657
00:39:25,780 --> 00:39:30,950
the potential difference between you and here negative because it's integral you don't know

658
00:39:31,170 --> 00:39:32,950
potential difference

659
00:39:34,430 --> 00:39:35,600
minus v two

660
00:39:35,630 --> 00:39:39,870
is going from one to two of

661
00:39:39,880 --> 00:39:44,770
so if the electric field is in this direction this has a higher potential than

662
00:39:45,910 --> 00:39:49,200
if it is in this direction this has a lot of potential so this is

663
00:39:49,200 --> 00:39:50,400
how this

664
00:39:50,440 --> 00:39:54,130
voltage wave propagates

665
00:39:54,160 --> 00:39:59,370
so thirty field is between the y is not along the wires

666
00:39:59,380 --> 00:40:01,000
is only electric

667
00:40:01,040 --> 00:40:04,600
feel potential between the two wires

668
00:40:04,690 --> 00:40:08,410
the oscillating surface charge densities

669
00:40:08,450 --> 00:40:13,440
in order to be those boundary conditions as the voltage pulse by so it's positive

670
00:40:13,440 --> 00:40:16,430
at this moment but later in time it will be negative and this will be

671
00:40:17,840 --> 00:40:20,440
and all that moves with the speed of light

672
00:40:20,520 --> 00:40:24,150
and then also oscillating surface current densities

673
00:40:24,190 --> 00:40:28,220
i j of s

674
00:40:28,240 --> 00:40:30,480
let's assume no

675
00:40:30,500 --> 00:40:33,170
i call this equals zero

676
00:40:33,310 --> 00:40:37,670
i'm going to short is out

677
00:40:37,690 --> 00:40:40,100
so i'm simply reporting there

678
00:40:40,150 --> 00:40:41,870
a wire

679
00:40:41,880 --> 00:40:44,940
totally shorted out

680
00:40:45,030 --> 00:40:50,240
that means there cannot be any potential difference between here and there

681
00:40:50,250 --> 00:40:56,760
because i make the resistance ohmic resistance and zero

682
00:40:56,770 --> 00:40:58,620
and what does that mean

683
00:40:58,670 --> 00:41:02,950
going to make drawing here

684
00:41:02,960 --> 00:41:07,510
otherwise i think it becomes too close

685
00:41:07,570 --> 00:41:11,430
so here is the situation

686
00:41:12,430 --> 00:41:15,540
the end of the

687
00:41:15,540 --> 00:41:16,750
o lines

688
00:41:16,940 --> 00:41:18,870
by the way is there

689
00:41:18,940 --> 00:41:19,980
at the end

690
00:41:20,080 --> 00:41:23,500
shorted out

691
00:41:23,500 --> 00:41:27,290
if it is shorted out it acts like a conducting walls just

692
00:41:27,330 --> 00:41:30,290
like which will be discussed before

693
00:41:30,310 --> 00:41:34,620
there cannot be any potential difference reunion because the e field in this direction must

694
00:41:34,620 --> 00:41:36,430
be zero

695
00:41:36,480 --> 00:41:39,910
so you get a similar situation that the reflectivity

696
00:41:39,940 --> 00:41:43,160
is minus one the the mountain comes back

697
00:41:43,170 --> 00:41:44,250
as the value

698
00:41:44,250 --> 00:41:46,890
in the field of inside wall

699
00:41:47,020 --> 00:41:49,190
and the field of the reflected light

700
00:41:49,190 --> 00:41:50,430
delete of

701
00:41:50,430 --> 00:41:53,390
exactly at this location

702
00:41:53,440 --> 00:41:55,980
that means you get standing waves

703
00:41:56,060 --> 00:42:01,710
so that means this is now notable line

704
00:42:01,870 --> 00:42:07,060
everywhere in this line the electric factor will always be here with all moments in

705
00:42:08,560 --> 00:42:11,770
but since i got standing wave here incident wave

706
00:42:11,790 --> 00:42:13,850
and reflected wave

707
00:42:13,890 --> 00:42:16,160
right here

708
00:42:16,210 --> 00:42:17,460
there is another

709
00:42:17,480 --> 00:42:18,830
no one

710
00:42:18,910 --> 00:42:20,660
and this distance then

711
00:42:20,660 --> 00:42:23,440
it's one level

712
00:42:23,460 --> 00:42:26,310
and right here

713
00:42:26,350 --> 00:42:30,790
there is another node line

714
00:42:30,830 --> 00:42:32,560
and the electric field here

715
00:42:32,600 --> 00:42:37,160
along this line is everywhere zero but it is not potential difference between y one

716
00:42:37,160 --> 00:42:42,580
and two two-year there's no potential difference here no potential difference here at all moments

717
00:42:42,580 --> 00:42:44,850
in time because it's standing wave

718
00:42:44,870 --> 00:42:46,350
in between

719
00:42:46,350 --> 00:42:51,350
x only takes an integer allies

720
00:42:51,390 --> 00:42:56,410
this is easy when you have to remember

721
00:42:56,430 --> 00:43:01,430
could derail

722
00:43:01,510 --> 00:43:06,550
i don't really know much about x except that it takes on integer values

723
00:43:06,570 --> 00:43:20,720
any suggestions on how i should expand the expectation of x

724
00:43:20,760 --> 00:43:23,470
how many people knows why are

725
00:43:24,240 --> 00:43:29,570
two reasons

726
00:43:29,620 --> 00:43:33,510
expedition something to probability

727
00:43:33,530 --> 00:43:37,340
so i should be looking at something like the probability that

728
00:43:37,390 --> 00:43:39,950
x equals some value x

729
00:43:39,950 --> 00:43:47,530
it would seem like a good thing to to do

730
00:43:47,570 --> 00:43:53,390
what else goes here

731
00:43:56,700 --> 00:44:01,260
well x can be somewhere between minus infinity and that's certainly true

732
00:44:01,320 --> 00:44:04,680
and you have some more

733
00:44:04,700 --> 00:44:06,200
o thing missing here

734
00:44:06,200 --> 00:44:08,070
what is this

735
00:44:08,160 --> 00:44:10,260
it doesn't come out

736
00:44:10,350 --> 00:44:13,820
for any random variable x takes on the value

737
00:44:15,910 --> 00:44:17,490
i need to add in

738
00:44:17,660 --> 00:44:20,490
something here namely

739
00:44:20,510 --> 00:44:24,180
that's the definition of the expectation

740
00:44:25,350 --> 00:44:29,160
the some of things where these

741
00:44:29,220 --> 00:44:32,070
coefficients sum to one

742
00:44:32,070 --> 00:44:33,600
looks an awful lot like

743
00:44:33,620 --> 00:44:35,620
the land that we just heard

744
00:44:35,660 --> 00:44:39,120
we prove that in the finite case it turns out to hold just as well

745
00:44:39,140 --> 00:44:40,600
if you take all

746
00:44:42,350 --> 00:44:44,180
i'm was going to assume that

747
00:44:44,200 --> 00:44:46,780
so i have these alpha these

748
00:44:46,800 --> 00:44:48,760
probabilities is alpha values

749
00:44:49,680 --> 00:44:51,820
to one

750
00:44:51,870 --> 00:44:54,470
therefore i can use this inequality

751
00:44:54,550 --> 00:44:56,700
this is the most

752
00:44:56,780 --> 00:44:58,340
it's right

753
00:44:58,350 --> 00:45:04,320
how help some x equals minus infinity to infinity

754
00:45:05,120 --> 00:45:10,680
the alphas which are probability capital x equals x

755
00:45:12,390 --> 00:45:14,680
for the value for

756
00:45:19,470 --> 00:45:21,700
OK so there it is

757
00:45:21,740 --> 00:45:22,780
here's the lemma

758
00:45:25,390 --> 00:45:28,850
raise the lemma

759
00:45:28,850 --> 00:45:37,050
i cheated by using the

760
00:45:37,110 --> 00:45:38,950
countable version of the lemma

761
00:45:38,990 --> 00:45:40,510
only proving the

762
00:45:40,570 --> 00:45:45,220
the finite case

763
00:45:45,320 --> 00:45:47,160
like in the lecture

764
00:45:49,720 --> 00:45:52,010
this is a lot

765
00:45:52,070 --> 00:45:59,100
now what i'd like to prove some blank space here is that this is the

766
00:46:01,510 --> 00:46:07,620
the summation is the most

767
00:46:07,660 --> 00:46:10,100
you've effects actually people

768
00:46:10,160 --> 00:46:12,070
you max

769
00:46:12,210 --> 00:46:18,070
really looks kind of equal rights got some some probabilities times that almost looks like

770
00:46:18,070 --> 00:46:19,390
the definition of the

771
00:46:19,490 --> 00:46:21,240
thanks but it isn't

772
00:46:21,300 --> 00:46:22,950
to be a little bit careful

773
00:46:23,050 --> 00:46:26,760
as you may have actually we should talk about the probability that f of x

774
00:46:26,760 --> 00:46:29,740
equals the particular value

775
00:46:29,780 --> 00:46:31,700
we can relate these

776
00:46:31,720 --> 00:46:33,530
as follows

777
00:46:33,570 --> 00:46:35,950
it's hard

778
00:46:36,010 --> 00:46:41,890
you can look at each

779
00:46:41,910 --> 00:46:43,780
value f takes on

780
00:46:43,840 --> 00:46:51,800
and then look at all the values can mapped to the

781
00:46:51,820 --> 00:46:55,890
thanks to all the is where f of k equals access

782
00:46:55,910 --> 00:46:58,780
the probability of x equals k

783
00:47:01,990 --> 00:47:03,490
there is another way of writing

784
00:47:03,490 --> 00:47:06,300
the probability that f of x

785
00:47:06,320 --> 00:47:07,470
he calls x

786
00:47:12,140 --> 00:47:16,140
in other words and grouping the terms in a particular way of saying well for

787
00:47:16,240 --> 00:47:18,820
next takes on various values

788
00:47:20,870 --> 00:47:22,370
however me

789
00:47:22,410 --> 00:47:23,490
to you switch

790
00:47:23,550 --> 00:47:26,120
i used to use cases and also

791
00:47:26,140 --> 00:47:29,490
i'm going to call it something else is called this why

792
00:47:30,760 --> 00:47:34,080
switch notation here

793
00:47:34,260 --> 00:47:43,620
accent probability measure the probability that x equals x

794
00:47:43,640 --> 00:47:46,600
so what i really care about is what this f of x value takes on

795
00:47:46,600 --> 00:47:50,620
this just call it why look at all values why the to take on the

796
00:47:52,140 --> 00:47:54,930
and then i look at all the different values of x

797
00:47:54,930 --> 00:47:56,510
where f of x equals y

798
00:47:56,530 --> 00:47:58,890
if i had those probabilities

799
00:47:58,890 --> 00:48:04,200
because these are different values of x the problem but those are some independent events

800
00:48:05,050 --> 00:48:05,950
so this

801
00:48:05,950 --> 00:48:11,890
in some asian will be the probability that f of x equals y

802
00:48:12,030 --> 00:48:13,780
capital tax

803
00:48:13,800 --> 00:48:15,410
it's little y

804
00:48:15,430 --> 00:48:18,950
and then if i multiply that by why i'm getting expectation

805
00:48:19,050 --> 00:48:20,300
the next

806
00:48:21,490 --> 00:48:23,600
think about this these two inequalities

807
00:48:23,660 --> 00:48:31,180
maybe a bit

808
00:48:31,240 --> 00:48:34,800
these are here because these sums are potentially infinite

809
00:48:34,820 --> 00:48:40,280
it's true

810
00:48:40,300 --> 00:48:46,050
OK this proves jensen's inequality so wasn't very hard just a couple aborts once we

811
00:48:46,050 --> 00:48:48,260
have this this powerful

812
00:48:48,280 --> 00:48:51,070
convexity lemma

813
00:48:51,080 --> 00:48:56,030
so we just use convexity we use the definition of x we use convexity that

814
00:48:56,030 --> 00:48:58,180
let's put the inside

815
00:48:58,200 --> 00:49:02,490
and we do this regrouping of terms and we think that's just give facts

816
00:49:02,490 --> 00:49:06,950
is it that when you minimize the l one norm we recover sparse signals and

817
00:49:06,950 --> 00:49:10,300
then i'm going try to draw an analogy so what you have is when you

818
00:49:10,300 --> 00:49:15,920
sort and one problem you recover sparse solution and wise's because what you say that

819
00:49:15,920 --> 00:49:19,460
they have a sparse vector which is here and is sparse because these colonies zero

820
00:49:19,460 --> 00:49:23,990
and this one is not zero and then you take one measurements i'd just make

821
00:49:23,990 --> 00:49:26,520
one measurement is vector and so we all know

822
00:49:26,580 --> 00:49:31,620
from middle school that well this gives equation of a line like this and then

823
00:49:31,620 --> 00:49:31,890
you say

824
00:49:32,510 --> 00:49:36,610
all right well if i were to finance his line point with minimum l two

825
00:49:36,610 --> 00:49:39,720
norm well i would get this thing

826
00:49:39,740 --> 00:49:41,260
which is not good

827
00:49:41,270 --> 00:49:45,080
so in high and low dimensional looks close but in high dimensions the disaster

828
00:49:45,130 --> 00:49:49,320
if i use the l one norm instead so this is the l two ball

829
00:49:49,320 --> 00:49:50,370
as we all know

830
00:49:50,380 --> 00:49:56,150
in one ball is like this as we shown

831
00:49:56,190 --> 00:49:59,260
so if i use the l one norm instead and so what's the point and

832
00:49:59,260 --> 00:50:00,880
is in one

833
00:50:00,890 --> 00:50:05,080
on this line has minimum in one norm while well so well i'm going to

834
00:50:05,080 --> 00:50:08,450
do is i'm going to grow this thing until it becomes tangent

835
00:50:08,460 --> 00:50:13,060
so the point that one ball tangent is this but its tangent at this vertex

836
00:50:13,060 --> 00:50:16,640
inside recovers sparse solution actually

837
00:50:16,650 --> 00:50:22,400
so this i show you that anyone actually sundays example one optimisation works and why

838
00:50:22,400 --> 00:50:25,480
is it it's because the and one ball is very pointy

839
00:50:27,250 --> 00:50:30,690
on sparse vectors for

840
00:50:30,810 --> 00:50:36,700
matrix completion something similar that occurs which is that this now we have the nuclear

841
00:50:36,700 --> 00:50:39,780
ball which is picture is represented by a cylinder

842
00:50:39,800 --> 00:50:43,760
so here you've got to set of two by two matrices which are symmetric because

843
00:50:43,760 --> 00:50:49,570
i can only drawing sometimes membrane drawings three d so here we have three parameters

844
00:50:49,570 --> 00:50:54,870
x y c we have a symmetric matrix and then this which have nuclear norm

845
00:50:54,870 --> 00:50:56,240
less than one

846
00:50:56,580 --> 00:51:03,160
and what happens is that the low rank matrices are extreme points of this convex

847
00:51:04,370 --> 00:51:08,550
and now we have are feasible set which is used dimensional space but it actually

848
00:51:08,550 --> 00:51:12,430
happens to be tangent to this very pointy nuclear ball

849
00:51:12,440 --> 00:51:16,600
because the nuclear bodies pointy at low rank solutions

850
00:51:16,620 --> 00:51:20,000
because this is a very quick

851
00:51:20,000 --> 00:51:25,390
geometric representation of what it is worth because of the point in this fuzzy nuclear

852
00:51:25,390 --> 00:51:27,460
norm or nuclear ball

853
00:51:27,810 --> 00:51:31,060
at low rank solution that all of a sudden there's many where you can have

854
00:51:31,060 --> 00:51:34,560
your a feasible set and maintain this tension property

855
00:51:34,640 --> 00:51:40,770
OK but of course this is not proof and i don't claim that it is

856
00:51:40,790 --> 00:51:46,270
OK so now of course you say well you know you can recover from the

857
00:51:46,290 --> 00:51:50,600
subset of entries can you recover from all the type of information and of course

858
00:51:50,600 --> 00:51:51,770
yes you can

859
00:51:51,790 --> 00:51:58,500
and now by now a whole theory which is being developed to show

860
00:51:58,520 --> 00:52:00,120
very precisely

861
00:52:00,120 --> 00:52:05,220
now you've got information about the matrix not given by

862
00:52:05,270 --> 00:52:10,830
revealing entries but by revealing coefficients so now i give you all the kind of

863
00:52:10,830 --> 00:52:15,540
linear information about the matrix of interest l so i give you inner products of

864
00:52:15,540 --> 00:52:20,560
before was giving you an inner product between the matrix l and e i e

865
00:52:21,640 --> 00:52:26,230
which essentially reveals that i j but now you can say well this matrices don't

866
00:52:26,230 --> 00:52:27,060
have to be

867
00:52:27,080 --> 00:52:31,040
the matrices full of zeros and ones somewhere just pick an element

868
00:52:31,040 --> 00:52:32,850
you can have anything you want

869
00:52:32,870 --> 00:52:34,500
and so

870
00:52:34,560 --> 00:52:38,910
now there's is this general theory of saying well you know now i have linear

871
00:52:38,910 --> 00:52:45,220
information about matrix linear information about a matrix of this kind find among all those

872
00:52:45,220 --> 00:52:50,700
matrices being these constraints that many with minimum l one norm and what i'm trying

873
00:52:50,700 --> 00:52:54,930
to say here is that if i have incoherence between the sensing matrices and the

874
00:52:54,930 --> 00:53:00,250
column space and the row space of the matrix everything should work and indeed it

875
00:53:00,250 --> 00:53:05,160
works and that's we've done a piece of it was then rushed and because of

876
00:53:05,160 --> 00:53:07,120
time maybe i'll skip this part

877
00:53:07,310 --> 00:53:13,770
but really is a big contribution is due to david grows who who explained and

878
00:53:13,770 --> 00:53:18,080
showed exactly what kind of coherence you need anyone one can you expect things to

879
00:53:18,930 --> 00:53:21,310
so i'm not going to go through the detail i'm just going to tell you

880
00:53:21,310 --> 00:53:23,580
that it's available and it's out there

881
00:53:23,620 --> 00:53:30,350
what's interesting about the contribution of the gross is that is the physicist quantum physicist

882
00:53:30,370 --> 00:53:34,350
and when he realizes that there are lots of problems in quantum mechanics that are

883
00:53:34,370 --> 00:53:38,020
matrix completion problem and so i'll just mention what it is

884
00:53:39,230 --> 00:53:45,680
so in quantum mechanics you have a system and quantum mechanically is if you have

885
00:53:45,730 --> 00:53:47,930
a case being one one-half system

886
00:53:47,950 --> 00:53:54,790
quantum mechanically is represented by a matrix which is called the density matrix the problem

887
00:53:54,790 --> 00:53:58,660
is as you know matthew you may not know is dimension in grows very quickly

888
00:53:58,660 --> 00:54:04,250
that the power squad quantum computing grows exponentially in the number of particles you have

889
00:54:04,250 --> 00:54:08,290
and so if you have a system we keep particles you've got dimension which is

890
00:54:08,310 --> 00:54:12,290
expansion of the exponential in the number of particles

891
00:54:12,310 --> 00:54:16,350
so perhaps without going into too too much into details try to explain what people

892
00:54:16,350 --> 00:54:17,730
try to do in this field

893
00:54:17,750 --> 00:54:20,700
so people prepare quantum states

894
00:54:20,720 --> 00:54:22,770
like the prepared quantum systems

895
00:54:22,830 --> 00:54:25,060
and then how do they know

896
00:54:25,060 --> 00:54:26,520
but what they prepare

897
00:54:26,560 --> 00:54:28,660
is what they thought they prepared

898
00:54:28,660 --> 00:54:32,220
how does a measure that what they the preparing the lab is actually what they

899
00:54:32,220 --> 00:54:34,890
thought they prepared they have to take measurements

900
00:54:34,910 --> 00:54:38,430
and what do you do when you take a measurement of a quantum system

901
00:54:38,430 --> 00:54:40,980
it destroyed money modify completely

902
00:54:42,720 --> 00:54:46,520
because you're going have to make a lot of measurements y because the number of

903
00:54:46,520 --> 00:54:47,850
degrees of freedom

904
00:54:47,910 --> 00:54:52,770
is a matrix which is an binds and quite huge and is exponential in the

905
00:54:52,770 --> 00:54:53,980
number of particles

906
00:54:54,000 --> 00:54:57,160
you can have to prepare a lot of quantum states

907
00:54:57,160 --> 00:55:01,450
now a lot of the state that these guys prepare actually low rank or proximity

908
00:55:01,460 --> 00:55:02,810
low rank

909
00:55:02,950 --> 00:55:05,520
so what the theory says is that

910
00:55:05,520 --> 00:55:09,390
and of course i'm here we have made a specific assumption about

911
00:55:09,400 --> 00:55:11,720
how the output interact with the input

912
00:55:11,940 --> 00:55:17,590
ok this kind of cartoon is typical of a discriminative models

913
00:55:17,740 --> 00:55:20,660
where you have the input sort of determining the output

914
00:55:20,750 --> 00:55:24,620
and some kind of sort of to close deterministic fashion

915
00:55:25,090 --> 00:55:28,930
but in general all we could have variables interacting in all

916
00:55:28,930 --> 00:55:31,970
kinds of ways case we could have many variables we could have

917
00:55:31,970 --> 00:55:34,440
arrows going forward and backward and so on

918
00:55:34,690 --> 00:55:37,650
and so you would get a lot more general polity by

919
00:55:38,110 --> 00:55:43,990
drawing models ok are there any questions or comments about this

920
00:55:44,040 --> 00:55:47,900
picture of a sort of this graphical models probabilistic view

921
00:55:52,230 --> 00:55:54,660
but is very quite suspicious

922
00:55:56,700 --> 00:56:02,370
ok so now let's talk a little bit about regular izations from this

923
00:56:02,380 --> 00:56:05,810
kind of perspective ok of of graphical models

924
00:56:07,440 --> 00:56:10,720
so regular decision is a weight of controlling

925
00:56:11,010 --> 00:56:15,850
the bias-variance trade-off in order to obtain a good value of the

926
00:56:15,860 --> 00:56:21,400
trade-off and it essentially sort of getting to the punchline

927
00:56:21,410 --> 00:56:26,040
to blows down to controlling this w here through some kind of

928
00:56:26,050 --> 00:56:31,700
prior ok that is going to help bias inject some bias but perhaps

929
00:56:31,710 --> 00:56:37,390
model way some of the variance ok now what is regular izations usually

930
00:56:37,400 --> 00:56:43,840
look like it looks like this we have initial objective which is jd

931
00:56:43,850 --> 00:56:48,110
of w that's to optimize something on the data either minimize

932
00:56:48,120 --> 00:56:52,010
squared error maximize likelihood or something like this

933
00:56:52,430 --> 00:56:57,910
and we add to it a penalty term ok and this penalty term depends

934
00:56:57,920 --> 00:57:01,430
on the weight vector and the penalty term essentially captures

935
00:57:01,440 --> 00:57:03,860
some kind of intuition that we have about

936
00:57:04,130 --> 00:57:08,480
priors on the hypothesis space what hypothesis are better than others

937
00:57:08,650 --> 00:57:11,770
in the absence of any data and the parameter lambda over

938
00:57:11,970 --> 00:57:15,550
controls the tradeoff so the make lambda the more we

939
00:57:15,650 --> 00:57:18,410
attention to the penalty the loan we make the lambda

940
00:57:18,560 --> 00:57:22,260
less engine to this penalty if we set on that zero then essentially

941
00:57:22,260 --> 00:57:25,290
we just try to optimize performance on the data and we don't

942
00:57:25,290 --> 00:57:30,530
care all about the actual prior on the hypothesis

943
00:57:30,830 --> 00:57:34,270
so used to this this this is called shrinkage in machine learning

944
00:57:34,270 --> 00:57:38,350
we call the regular sation and lambda here is called regulation

945
00:57:38,360 --> 00:57:43,780
musician coefficients and in principle you have

946
00:57:44,370 --> 00:57:47,750
criteria that can help you select as in practice very often

947
00:57:47,760 --> 00:57:50,720
people do cross validation and treat this as a

948
00:57:50,950 --> 00:57:52,800
as a sort of hyperparameter as well

949
00:57:54,200 --> 00:57:57,800
so now let's look a little bit at regular izations for linear models

950
00:57:57,800 --> 00:58:01,760
okay so here we have a linear model we have the error function

951
00:58:01,990 --> 00:58:06,840
and then we have here a term w transpose w

952
00:58:07,130 --> 00:58:10,500
that is what's called l to regular sation or what weight decay

953
00:58:10,770 --> 00:58:14,960
ok so what this is still it basically says that we would like

954
00:58:14,970 --> 00:58:17,160
our weight magnitude to be small

955
00:58:17,740 --> 00:58:21,570
okay because if we emphasize this term and the weight magnitude

956
00:58:21,570 --> 00:58:23,790
is high is going to make things look worse

957
00:58:24,000 --> 00:58:31,380
ok so that's why weight decay ok and the nice thing about this

958
00:58:31,380 --> 00:58:33,900
is that of course here the quadratic term in the front wheels

959
00:58:33,900 --> 00:58:36,440
out the quadratic terms so everything is nice and quadratic

960
00:58:36,440 --> 00:58:38,820
and we can take gradients and we can optimize

961
00:58:39,010 --> 00:58:42,400
things work very well in fact they work very well in closed form

962
00:58:42,400 --> 00:58:46,300
and we get this solution over here where the best weight vector

963
00:58:46,580 --> 00:58:51,530
has basically the same form as before except fight transpose far

964
00:58:51,540 --> 00:58:55,700
has a land times the data d matrix added to it ok or other words

965
00:58:55,710 --> 00:58:59,840
you're adding mass on the diagonal of this five transpose phi

966
00:59:00,140 --> 00:59:05,480
okay that helps to we do things ok helps you with making this a little

967
00:59:05,490 --> 00:59:09,460
bit better or better condition ok and again it's sort of drives

968
00:59:09,760 --> 00:59:11,080
your weights towards zero

969
00:59:14,130 --> 00:59:17,300
and of course if lambda is equal to zero you get exactly the

970
00:59:17,300 --> 00:59:20,690
same solution as for usual linear regression

971
00:59:21,150 --> 00:59:24,640
and if lambda those to infinity then the solution is just going to

972
00:59:24,730 --> 00:59:30,140
go to zero okay so this is called ridge regression there's a

973
00:59:30,140 --> 00:59:32,730
more general class of regularization which actually

974
00:59:32,730 --> 00:59:35,590
can tell you about really called cannot regular sation

975
00:59:36,040 --> 00:59:40,390
that has all this kind of four ok and

976
00:59:40,390 --> 00:59:44,460
the company to grow crops in that

977
00:59:44,480 --> 00:59:46,560
cross look people

978
00:59:46,580 --> 00:59:48,910
two two

979
00:59:48,910 --> 00:59:53,080
factories which so the country can stop them

980
00:59:53,120 --> 00:59:56,560
dependent on imported food and so on

981
00:59:59,310 --> 01:00:01,770
it would be

982
01:00:01,770 --> 01:00:02,810
he was

983
01:00:02,850 --> 01:00:08,180
you will probably get your goals

984
01:00:09,520 --> 01:00:13,330
for example the first the country's people

985
01:00:13,330 --> 01:00:15,370
the bridge the country

986
01:00:15,390 --> 01:00:18,910
the full independence u

987
01:00:18,930 --> 01:00:21,520
o the market they

988
01:00:22,370 --> 01:00:24,480
the great gas who

989
01:00:24,620 --> 01:00:27,140
made more than four

990
01:00:27,160 --> 01:00:29,200
we all know

991
01:00:32,020 --> 01:00:37,540
the european commission for the topic

992
01:00:39,020 --> 01:00:41,660
so we try

993
01:00:41,660 --> 01:00:44,410
he is just as possible

994
01:00:44,430 --> 01:00:48,230
they're not going to go

995
01:00:48,310 --> 01:00:50,890
we know that he

996
01:00:50,910 --> 01:00:54,230
you from

997
01:00:57,270 --> 01:01:00,120
the first of these is x

998
01:01:01,960 --> 01:01:04,060
out of the lake

999
01:01:04,080 --> 01:01:08,660
large complex of let the countries

1000
01:01:08,660 --> 01:01:13,390
two grow full or that you don't want to predict

1001
01:01:13,410 --> 01:01:15,200
five months ago

1002
01:01:15,230 --> 01:01:17,830
south korea

1003
01:01:18,080 --> 01:01:19,980
it's stated by

1004
01:01:20,090 --> 01:01:26,930
a network of the biggest food companies ball

1005
01:01:26,950 --> 01:01:30,520
around fifty percent of all

1006
01:01:31,950 --> 01:01:34,390
on the idea

1007
01:01:34,480 --> 01:01:40,220
the problem is that we're not going to grow food that we must grow grow

1008
01:01:40,230 --> 01:01:42,980
in that land and so on and so forth

1009
01:01:43,060 --> 01:01:44,890
even in the countries

1010
01:01:44,890 --> 01:01:49,120
when they have to let me usually

1011
01:01:49,140 --> 01:01:54,660
they are used for example in some pretty

1012
01:01:55,430 --> 01:01:58,750
so i think that

1013
01:01:59,910 --> 01:02:03,140
o this seems going on

1014
01:02:04,270 --> 01:02:06,660
weak it's a good

1015
01:02:06,680 --> 01:02:09,310
that would be the only through

1016
01:02:10,850 --> 01:02:14,390
the question is

1017
01:02:14,520 --> 01:02:17,720
it should be remained for you say

1018
01:02:18,350 --> 01:02:20,520
well lead to more human here and there

1019
01:02:20,520 --> 01:02:21,790
or there

1020
01:02:21,790 --> 01:02:23,790
one is that we call

1021
01:02:23,910 --> 01:02:26,250
weak oppose

1022
01:02:26,290 --> 01:02:29,980
which can be solved within the norm for

1023
01:02:32,770 --> 01:02:34,700
i think there are

1024
01:02:35,390 --> 01:02:36,620
four such

1025
01:02:36,620 --> 01:02:40,720
it's not in the field

1026
01:02:40,790 --> 01:02:45,700
the first is that you know he's about to go

1027
01:02:45,770 --> 01:02:47,750
in this image

1028
01:02:47,770 --> 01:02:51,060
i know i know state college

1029
01:02:51,160 --> 01:02:54,480
proof from eastern europe countries

1030
01:02:54,540 --> 01:03:00,000
state regulation would even want to know the united labor market

1031
01:03:00,120 --> 01:03:04,870
he also you know should use the

1032
01:03:04,980 --> 01:03:07,310
we try to undermine

1033
01:03:07,310 --> 01:03:09,500
because i think it again

1034
01:03:09,500 --> 01:03:11,720
no from the

1035
01:03:11,930 --> 01:03:16,680
plus you think that if they are to be the short

1036
01:03:16,680 --> 01:03:18,460
not all in

1037
01:03:18,480 --> 01:03:22,830
i can only imagine you give it a little bit of new

1038
01:03:23,460 --> 01:03:25,540
new york new demand

1039
01:03:25,560 --> 01:03:27,450
but what

1040
01:03:27,480 --> 01:03:29,890
what i think is

1041
01:03:31,330 --> 01:03:32,790
market works

1042
01:03:33,500 --> 01:03:36,100
competition this

1043
01:03:36,120 --> 01:03:39,830
but the threats facing the

1044
01:03:41,460 --> 01:03:47,060
with global warming you can say OK let's see if you think that so we

1045
01:03:47,060 --> 01:03:49,830
we don't have a chance because if it fails

1046
01:03:49,870 --> 01:03:53,600
this article

1047
01:03:57,540 --> 01:03:58,410
we have

1048
01:04:00,270 --> 01:04:02,160
we have

1049
01:04:03,040 --> 01:04:04,720
the problem of

1050
01:04:04,830 --> 01:04:09,080
intellectual property and i spoke economists were his

1051
01:04:09,080 --> 01:04:12,370
conservative comes in need that

1052
01:04:12,410 --> 01:04:13,660
maybe this should be

1053
01:04:14,430 --> 01:04:15,960
of those things

1054
01:04:15,980 --> 01:04:17,850
everybody thought of

1055
01:04:17,850 --> 01:04:22,930
that is not in the body mainly in the very far off

1056
01:04:22,950 --> 01:04:27,600
so so-called intellectual property knowledge reached this

1057
01:04:27,600 --> 01:04:29,960
four of private property

1058
01:04:30,250 --> 01:04:33,000
on the market

1059
01:04:33,060 --> 01:04:36,000
the market is becoming irrational

1060
01:04:36,640 --> 01:04:39,370
but i think it would be

1061
01:04:39,390 --> 01:04:39,700
a b

1062
01:04:39,870 --> 01:04:44,980
how can you become twenty years twenty five from the

1063
01:04:45,000 --> 01:04:49,520
who is hoping to get some things you all that

1064
01:04:49,520 --> 01:04:51,720
when estimating the one

1065
01:04:51,730 --> 01:04:56,910
you can also even in my you can see it a

1066
01:04:56,930 --> 01:05:02,910
who did what were all this

1067
01:05:02,910 --> 01:05:05,930
the problem is

1068
01:05:05,930 --> 01:05:07,120
this is the like

1069
01:05:07,120 --> 01:05:14,770
there's a lot of other commodities you you know you produced shows that god that

1070
01:05:14,830 --> 01:05:17,660
the prices belies not you

1071
01:05:17,700 --> 01:05:18,560
you don't know

1072
01:05:18,620 --> 01:05:19,980
well for example

1073
01:05:20,140 --> 01:05:23,330
you can help in any way

1074
01:05:23,330 --> 01:05:26,100
round or connect the price of

1075
01:05:26,160 --> 01:05:27,890
windows broken

1076
01:05:27,950 --> 01:05:30,680
to some market movements

1077
01:05:30,700 --> 01:05:34,410
a lot of economics

1078
01:05:34,410 --> 01:05:36,140
i think the

1079
01:05:36,140 --> 01:05:43,310
it was

1080
01:07:03,190 --> 01:07:04,760
you all

1081
01:08:31,240 --> 01:08:35,060
i think

1082
01:09:15,550 --> 01:09:16,590
one of

1083
01:09:16,590 --> 01:09:21,210
so considering one variable at a time it's it's two dimensional problem that we can

1084
01:09:21,210 --> 01:09:23,920
project in one dimension

1085
01:09:23,960 --> 01:09:28,710
and if we do so if we project everything onto the age variables

1086
01:09:28,760 --> 01:09:30,320
we see that h

1087
01:09:30,340 --> 01:09:34,070
reasonably well separates the two populations

1088
01:09:34,130 --> 01:09:39,490
if we project here on say wait minute making some arbitrary example

1089
01:09:39,550 --> 01:09:44,380
then we see that there is overlap between the two distribution and this variable here

1090
01:09:44,380 --> 01:09:47,710
alone by itself is not a good choice

1091
01:09:47,730 --> 01:09:52,280
to separate the two population where this variable here is a good choice

1092
01:09:52,300 --> 01:09:57,250
this is what would be the result of the univariate feature selection we decide to

1093
01:09:57,340 --> 01:09:58,480
pick this variable

1094
01:09:58,490 --> 01:10:00,530
throw away this one

1095
01:10:00,570 --> 01:10:04,440
but now if you look in two dimensions you can see that

1096
01:10:04,440 --> 01:10:08,960
there is about separation of the two populations if you add this variable

1097
01:10:09,010 --> 01:10:10,380
this variable

1098
01:10:10,480 --> 01:10:11,820
which alone

1099
01:10:11,840 --> 01:10:13,260
is irrelevant

1100
01:10:13,280 --> 01:10:15,610
when taken with another one

1101
01:10:15,630 --> 01:10:19,800
i can help you perform a better separation

1102
01:10:19,860 --> 01:10:23,030
now consider the second example

1103
01:10:23,050 --> 01:10:25,250
here it is even more dramatic

1104
01:10:25,380 --> 01:10:29,960
because now you have two variables that alone

1105
01:10:30,010 --> 01:10:32,340
do not separate the data

1106
01:10:32,340 --> 01:10:34,800
but when you consider them together

1107
01:10:34,860 --> 01:10:37,880
they give you a very good separation of the

1108
01:10:37,900 --> 01:10:41,510
even though it's not the linear separation in this case it's the non linear separation

1109
01:10:41,510 --> 01:10:42,860
because you have

1110
01:10:43,630 --> 01:10:48,960
class is composed of two clusters and their range during this very particular way which

1111
01:10:48,960 --> 01:10:50,610
is known you know and as the

1112
01:10:51,230 --> 01:10:56,860
chessboard problem it's one of you know the next year machine learning example that's always

1113
01:10:56,860 --> 01:11:01,260
quarter because when you're looking projection

1114
01:11:01,300 --> 01:11:02,940
you can't separate

1115
01:11:02,960 --> 01:11:06,730
the examples

1116
01:11:06,730 --> 01:11:07,710
and so

1117
01:11:07,730 --> 01:11:10,630
multivariate methods are going to allow us

1118
01:11:10,650 --> 01:11:19,380
to identify features that together are predictive but individually or not

1119
01:11:23,940 --> 01:11:28,050
we're now posing filters versus wrappers

1120
01:11:28,070 --> 01:11:34,380
the main goal is going to be ranking subsets of useful features

1121
01:11:34,400 --> 01:11:36,860
the filter approach consistent

1122
01:11:36,900 --> 01:11:38,550
considering all the features

1123
01:11:38,570 --> 01:11:44,030
putting them into a box called filter out putting some subset of good features and

1124
01:11:44,030 --> 01:11:46,710
then using them for making predictions

1125
01:11:46,730 --> 01:11:50,010
whereas the wrapper methods consider the features

1126
01:11:50,030 --> 01:11:54,090
generates some subset of candidate features

1127
01:11:54,110 --> 01:11:55,900
buses them to the predictor

1128
01:11:55,960 --> 01:12:02,050
the predictor you know training and we compute the prediction power of the feature subset

1129
01:12:02,990 --> 01:12:06,170
and then eventually we guide the search

1130
01:12:06,170 --> 01:12:11,480
for a new feature subset and we tried many times until we find an optimum

1131
01:12:11,510 --> 01:12:16,840
feature subset

1132
01:12:18,760 --> 01:12:19,820
what connects

1133
01:12:19,840 --> 01:12:24,400
this problem with the problem of the previous lecture is that there is a danger

1134
01:12:26,610 --> 01:12:30,380
if you're considering many subsets of features

1135
01:12:30,420 --> 01:12:34,710
it is like we had in the previous lecture one we considered many

1136
01:12:34,730 --> 01:12:39,900
learning machines so we considered a highly complex learning machines

1137
01:12:39,920 --> 01:12:43,800
the complexity of the learning problem in that case is going to be related to

1138
01:12:43,800 --> 01:12:48,840
the number of feature subsets that we're considering our search

1139
01:12:48,840 --> 01:12:54,300
so we run at risk of overfitting

1140
01:12:54,340 --> 01:12:59,800
in the chapters from the book there are many search strategies that are considered mentioning

1141
01:12:59,800 --> 01:13:04,840
here if you just to give you an or a broad overview

1142
01:13:04,880 --> 01:13:06,070
there are

1143
01:13:06,070 --> 01:13:11,860
so called forward and backward methods so forward selection methods consistent starting with an empty

1144
01:13:12,650 --> 01:13:18,440
a feature and progressively adding features backward elimination to start with the full set then

1145
01:13:18,440 --> 01:13:20,320
you personally eliminates

1146
01:13:20,360 --> 01:13:26,490
and then there's the search where you consider path and keeping all the time kick

1147
01:13:26,490 --> 01:13:28,860
and features

1148
01:13:28,920 --> 01:13:32,960
there is the generalized sequential forward selection

1149
01:13:32,980 --> 01:13:34,650
one and minus square

1150
01:13:34,670 --> 01:13:36,510
and landscape features are

1151
01:13:36,530 --> 01:13:41,730
i left sorry there's

1152
01:13:42,170 --> 01:13:44,260
there is no

1153
01:13:44,960 --> 01:13:47,780
the problem in the typesetting here

1154
01:13:48,760 --> 01:13:52,170
anyway that it requires

1155
01:13:52,440 --> 01:13:54,420
and minus k

1156
01:13:54,480 --> 01:14:01,130
two the g choose n minus could you think you can clearly see well training

1157
01:14:01,130 --> 01:14:09,360
so more trainings are done each step but fewer steps than in the simple

1158
01:14:09,380 --> 01:14:11,340
sequential forward selection

1159
01:14:11,510 --> 01:14:13,710
and then there is the PTA methods

1160
01:14:13,760 --> 01:14:15,360
so it's

1161
01:14:15,380 --> 01:14:17,380
well you you put out

1162
01:14:17,380 --> 01:14:22,370
and take away are so you you go for a certain number of steps l

1163
01:14:22,370 --> 01:14:24,650
and then you go back through certain number of of

1164
01:14:24,760 --> 01:14:29,320
steps and then finally there's the floating search which i think is the most popular

1165
01:14:29,320 --> 01:14:32,480
search methods right now for

1166
01:14:32,530 --> 01:14:37,280
and what you do is that you are you start going forward or start going

1167
01:14:37,280 --> 01:14:40,190
back your search and

1168
01:14:40,230 --> 01:14:42,130
you go for it until

1169
01:14:42,150 --> 01:14:46,320
you're not improving anymore and then you start going back heard until you not improving

1170
01:14:46,320 --> 01:14:51,110
anymore and so on and so forth new oscillates between going forward and backward

1171
01:14:51,150 --> 01:14:53,960
in your search

1172
01:14:54,230 --> 01:14:58,440
and of course and i didn't mention it but you could also do exhaustive search

1173
01:14:58,440 --> 01:15:05,010
so you could generate all possible subsets of variables and invested in them all and

1174
01:15:05,010 --> 01:15:08,190
you can do genetic algorithms you can do

1175
01:15:08,210 --> 01:15:09,090
and any

1176
01:15:09,150 --> 01:15:12,510
type of search you want

1177
01:15:12,670 --> 01:15:17,110
the problem being that if you search harder

1178
01:15:17,130 --> 01:15:21,380
you're going to incur some complexity penalty

1179
01:15:21,400 --> 01:15:27,420
in this graph i'm representing the case in which you have only four features

1180
01:15:27,420 --> 01:15:32,510
so these are represented by these four numbers here

1181
01:15:32,530 --> 01:15:37,460
and this is the state space of all possible feature subsets

1182
01:15:37,480 --> 01:15:42,590
so one means the future is present then zero the the future is absent

1183
01:15:42,590 --> 01:15:49,260
well i don't have to run meaning in a nice way and this is able

1184
01:15:49,260 --> 01:15:53,020
to around the parameters in

1185
01:15:53,030 --> 01:15:59,580
in some interactive way when you you make succession steps you can cannot parameters to

1186
01:15:59,580 --> 01:16:03,430
learn the parameters of q SVM so on

1187
01:16:03,440 --> 01:16:07,140
so i didn't talk about that i understand that i wanted to to tell you

1188
01:16:10,960 --> 01:16:11,840
of course

1189
01:16:11,860 --> 01:16:16,760
and i will talk about this tomorrow there is a big

1190
01:16:16,810 --> 01:16:19,690
big problem the challenge here

1191
01:16:19,690 --> 01:16:23,430
in some problems you have no idea of the k

1192
01:16:23,440 --> 01:16:25,980
i would you would like to learn

1193
01:16:25,990 --> 01:16:30,600
either in a nonparametric weights we build a function such that

1194
01:16:30,630 --> 01:16:36,700
it would be can then all you want to find some to combine different can

1195
01:16:36,710 --> 01:16:38,360
and learn to weight

1196
01:16:38,390 --> 01:16:40,580
so that there are many world

1197
01:16:40,600 --> 01:16:43,940
have been done by a long creating

1198
01:16:43,940 --> 01:16:47,430
two so then in one thousand seven for bioinformatics

1199
01:16:47,490 --> 01:16:51,470
so steve you are really a lot of stuff to do

1200
01:16:51,480 --> 01:16:52,590
we cameras

1201
01:16:52,610 --> 01:16:57,900
especially in the case when you want to

1202
01:16:59,400 --> 01:17:02,470
before talking about turning can and

1203
01:17:02,490 --> 01:17:07,440
we can try to define some applications you can try to define you can cannot

1204
01:17:07,450 --> 01:17:08,580
because you know

1205
01:17:08,630 --> 01:17:12,480
the data you know if object for instance you know your images

1206
01:17:12,490 --> 01:17:17,700
you know your documents and you have some idea what you can do

1207
01:17:18,880 --> 01:17:21,250
the first on easy

1208
01:17:22,190 --> 01:17:23,880
i would say very simple way

1209
01:17:23,890 --> 01:17:27,380
is just to find explicitly the feature map phi

1210
01:17:27,400 --> 01:17:29,750
of course if you

1211
01:17:29,760 --> 01:17:35,490
if you have feature map phi find sentences indicator for you can then when you

1212
01:17:35,490 --> 01:17:39,860
can you can use it and then you can but usually

1213
01:17:40,770 --> 01:17:43,990
we really want to benefit from the geometric

1214
01:17:44,030 --> 01:17:46,660
and we don't all we got

1215
01:17:46,670 --> 01:17:49,500
define very explicitly the feature map

1216
01:17:49,520 --> 01:17:55,010
so the second way is to use the building blocks i described yesterday

1217
01:17:55,060 --> 01:17:57,950
is only so the closure property so

1218
01:17:57,950 --> 01:18:00,720
if you combine to count as you

1219
01:18:00,740 --> 01:18:06,450
i have another can then if you took an exponential of again and you have

1220
01:18:06,450 --> 01:18:10,260
again and again and so you can of course play with it

1221
01:18:10,290 --> 01:18:12,060
as go

1222
01:18:12,980 --> 01:18:14,880
there are also

1223
01:18:14,890 --> 01:18:17,770
different ways to

1224
01:18:17,880 --> 01:18:19,820
to solve this

1225
01:18:19,850 --> 01:18:21,570
cameron definition

1226
01:18:21,630 --> 01:18:26,260
you can use graphical with that so you can take the data

1227
01:18:26,360 --> 01:18:31,060
you can estimate the parameters of a graphical

1228
01:18:31,110 --> 01:18:34,520
and then you have what we call fusion camera

1229
01:18:34,540 --> 01:18:37,170
it's so weighting factor defines

1230
01:18:37,180 --> 01:18:42,850
closing into the parameters of the graphic and that's the way to define canon between

1231
01:18:43,630 --> 01:18:45,290
two complex objects

1232
01:18:45,320 --> 01:18:47,250
and we have also

1233
01:18:47,260 --> 01:18:52,490
also can isolate convolution can they can bayes and syntax

1234
01:18:52,570 --> 01:18:55,090
you have a lot of possible

1235
01:18:55,140 --> 01:18:58,950
so today i'm going to work to describe

1236
01:19:00,550 --> 01:19:04,890
two way of defining canale and i've chosen applications

1237
01:19:06,560 --> 01:19:08,090
which we have

1238
01:19:09,880 --> 01:19:11,690
document and text mining

1239
01:19:11,690 --> 01:19:16,790
and the first one is closely linked to this building block and the second one

1240
01:19:16,790 --> 01:19:18,730
is closely to

1241
01:19:18,740 --> 01:19:19,980
the fisher king

1242
01:19:25,460 --> 01:19:28,990
can we talk a little bit about that

1243
01:19:32,130 --> 01:19:36,270
just to what about that if you structured data

1244
01:19:36,290 --> 01:19:39,320
your choices are not limited to this but

1245
01:19:39,340 --> 01:19:41,130
at least you can use this list

1246
01:19:41,130 --> 01:19:43,190
you can make this fish cannot

1247
01:19:43,210 --> 01:19:43,970
you can

1248
01:19:43,990 --> 01:19:47,950
try to use the syntax of your structure object

1249
01:19:47,960 --> 01:19:54,720
and you can also be found for instance if your your data are nodes in

1250
01:19:54,720 --> 01:19:56,710
a graph you can use

1251
01:19:56,850 --> 01:19:59,710
the family of diffusion can

1252
01:19:59,720 --> 01:20:06,530
OK just now and come back so my first example of building again

1253
01:20:06,550 --> 01:20:09,230
will be used it with some

1254
01:20:09,230 --> 01:20:10,920
implicit structure

1255
01:20:11,790 --> 01:20:14,880
so friends so we're going to talk about text

1256
01:20:14,920 --> 01:20:20,920
and index to the problem you are you one of the sequence of symbols the

1257
01:20:20,920 --> 01:20:23,770
world's but these

1258
01:20:23,840 --> 01:20:27,450
sequence is not as

1259
01:20:27,450 --> 01:20:29,160
and i was so it's

1260
01:20:29,210 --> 01:20:34,740
structure the sequence but more and more of you have an implicit structure

1261
01:20:34,750 --> 01:20:36,310
with the world

1262
01:20:36,350 --> 01:20:39,430
in fact you have the semantics of the world

1263
01:20:39,430 --> 01:20:41,610
okay welcome to lecture number nine

1264
01:20:42,060 --> 01:20:47,840
today our main topic is going to be inference inferring parameters for example to do

1265
01:20:47,840 --> 01:20:50,060
any science experiment and you want to infer

1266
01:20:50,580 --> 01:20:51,190
the parameters

1267
01:20:51,800 --> 01:20:52,440
that underlie

1268
01:20:53,360 --> 01:20:54,800
these data you've got

1269
01:20:55,250 --> 01:20:56,860
but before we do that i want to

1270
01:20:57,580 --> 01:20:59,330
o point out to a little gem

1271
01:20:59,770 --> 01:21:01,060
information theory that

1272
01:21:02,520 --> 01:21:03,660
you may not have noticed

1273
01:21:05,120 --> 01:21:06,740
i'd like just a start by

1274
01:21:07,120 --> 01:21:07,740
thinking about

1275
01:21:09,400 --> 01:21:10,850
the binary erasure channel

1276
01:21:11,500 --> 01:21:18,160
what we did in the last couple lectures was re-established shannon's noisy channel coding theorem that says any channel

1277
01:21:18,590 --> 01:21:21,700
you can work out its capacity by maximizing mutual information

1278
01:21:22,480 --> 01:21:28,740
anne then it's possible to make an encoder and the decoder that communicate reliably over the channel

1279
01:21:30,820 --> 01:21:34,880
reliably meaning you can get the error probability as close as you like to zero

1280
01:21:35,110 --> 01:21:38,250
but you can always communicated any rate up to the capacity

1281
01:21:39,540 --> 01:21:42,280
so what's the capacity of the binary erasure

1282
01:21:42,790 --> 01:21:43,950
channel please

1283
01:21:44,340 --> 01:21:45,640
i have a quick chat to your neighbour

1284
01:21:47,220 --> 01:21:47,780
is a question

1285
01:21:49,000 --> 01:21:50,970
all goes all the way over here and just one

1286
01:21:57,540 --> 01:22:00,530
there's always two ways you can solve these questions

1287
01:22:01,380 --> 01:22:02,080
you can see the

1288
01:22:03,050 --> 01:22:04,940
look at the mutual information this way around

1289
01:22:08,070 --> 01:22:09,320
people without this way around

1290
01:22:15,340 --> 01:22:17,350
and often one way it's easier than ever

1291
01:22:19,720 --> 01:22:20,530
so let's to be

1292
01:22:21,650 --> 01:22:24,000
binary erasure channel both ways around

1293
01:22:24,670 --> 01:22:26,070
how generalize a little bit

1294
01:22:26,850 --> 01:22:27,770
so that we're putting in

1295
01:22:29,220 --> 01:22:34,740
he zero and be one our pretend we don't know the peopple optimal input distribution

1296
01:22:34,880 --> 01:22:36,310
of course have to be symmetric

1297
01:22:38,720 --> 01:22:40,110
and i'll call these probabilities

1298
01:22:41,380 --> 01:22:42,050
one myself

1299
01:22:43,140 --> 01:22:46,020
so if we do it this way the first term

1300
01:22:46,630 --> 01:22:48,580
is binary and they are not

1301
01:22:49,780 --> 01:22:53,910
and the second term we need to think through all the different ways that could

1302
01:22:53,910 --> 01:22:56,720
happen that's a work we need to think about three things

1303
01:22:57,280 --> 01:23:00,790
figure out how probable they are then work out what the entropy of x given

1304
01:23:00,790 --> 01:23:05,360
why is quite a lot of work except almost all that is easy if you

1305
01:23:05,360 --> 01:23:06,690
get a zero you know that

1306
01:23:07,020 --> 01:23:08,530
input was a zero so

1307
01:23:10,040 --> 01:23:11,290
there's no entropy in case

1308
01:23:12,280 --> 01:23:14,030
similarly if there is a one

1309
01:23:15,020 --> 01:23:17,240
then the input doesn't have any entropy anymore

1310
01:23:17,610 --> 01:23:19,390
it's only a question mark comes out

1311
01:23:19,830 --> 01:23:21,590
which happens with probability at

1312
01:23:22,340 --> 01:23:23,390
one by by

1313
01:23:25,790 --> 01:23:28,500
you still don't know what the input was and you've got no information at all

1314
01:23:28,500 --> 01:23:32,410
about what input was he posterior distribution of the input is still

1315
01:23:33,140 --> 01:23:35,050
the same as it was when we started said

1316
01:23:37,060 --> 01:23:37,350
you know

1317
01:23:39,470 --> 01:23:40,150
which is

1318
01:23:43,340 --> 01:23:43,840
one minus

1319
01:23:45,360 --> 01:23:47,500
maximize with respect to p e naught

1320
01:23:48,330 --> 01:23:49,870
you end up with pino equals a half

1321
01:23:50,650 --> 01:23:51,310
if you want

1322
01:23:53,420 --> 01:23:53,820
one was there

1323
01:23:59,440 --> 01:24:02,680
usually doing it this way round is a bit hard because you have to think

1324
01:24:02,680 --> 01:24:07,160
about lots of inferences but in this case the three inferences fairly easy

1325
01:24:07,630 --> 01:24:08,700
this is a nice route

1326
01:24:09,740 --> 01:24:10,320
for solving this

1327
01:24:11,710 --> 01:24:14,140
alternatively we could work at the end of beer wine

1328
01:24:15,000 --> 01:24:18,860
means you need to work at the end of the probability of why

1329
01:24:20,930 --> 01:24:21,770
being zero

1330
01:24:22,600 --> 01:24:24,280
why being question mark

1331
01:24:25,710 --> 01:24:26,850
why being

1332
01:24:27,570 --> 01:24:30,330
one and you could laboratory work out those three

1333
01:24:31,090 --> 01:24:33,850
numbers the probability of getting a zero out

1334
01:24:36,910 --> 01:24:39,500
the probability that put in a zero multiplied

1335
01:24:43,490 --> 01:24:46,830
etcetera so you can work out the three numbers then you have a big

1336
01:24:47,260 --> 01:24:51,500
massive stuff to put into the entropy computation if qpi log on people must be

1337
01:24:51,550 --> 01:24:53,870
log on people about three terms

1338
01:24:54,310 --> 01:24:56,270
so that's looks like a terrible later work

1339
01:24:57,100 --> 01:24:59,180
then secondly we wanted to know why

1340
01:24:59,740 --> 01:25:00,670
given x

1341
01:25:01,390 --> 01:25:04,690
that's easier because if you condition on a particular

1342
01:25:05,360 --> 01:25:09,740
x going in and then they and why is just a choice between zero or

1343
01:25:09,750 --> 01:25:13,300
question mark or one question mark in the entropy of the distribution is the same

1344
01:25:13,440 --> 01:25:15,010
way around is just age

1345
01:25:16,110 --> 01:25:20,120
that's that's an easy thing this is looking horrendous but i want to introduce you

1346
01:25:21,270 --> 01:25:21,740
a little

1347
01:25:22,950 --> 01:25:27,360
property of the entropy which we call the decomposability over and it

1348
01:25:31,120 --> 01:25:33,470
this is not the jam sorry we're getting to the genome

1349
01:25:34,530 --> 01:25:36,330
the decomposability end he says

1350
01:25:36,900 --> 01:25:41,170
you can if you want to work out the entropy of a distribution over several

1351
01:25:41,860 --> 01:25:43,070
characters in the alphabet

1352
01:25:45,780 --> 01:25:50,220
imagining that u learn the character in a sequence of steps and add up how

1353
01:25:50,220 --> 01:25:52,850
much entropy you get on average a teacher those steps

1354
01:25:54,050 --> 01:25:55,700
first you can imagine finding out

1355
01:25:57,240 --> 01:25:59,630
the character that came out a question mark or not

1356
01:26:00,030 --> 01:26:00,610
and we know

1357
01:26:02,150 --> 01:26:04,210
fraction question that's coming out is always at

1358
01:26:04,750 --> 01:26:06,590
it doesn't matter what input distribution you four

1359
01:26:08,650 --> 01:26:09,060
we can say

1360
01:26:11,010 --> 01:26:12,070
was a question mark

1361
01:26:12,540 --> 01:26:12,820
and then

1362
01:26:12,820 --> 01:26:15,590
completely separately optimized reward functions

1363
01:26:15,850 --> 01:26:20,260
the state a separate optimisation problems after optimisation problems about the model is a learning

1364
01:26:21,040 --> 01:26:26,680
OK let's keep going to use more interesting one let's take the reward

1365
01:26:26,700 --> 01:26:31,250
just the best internal reward just before twenty five thousand

1366
01:26:31,270 --> 01:26:34,040
and the best award just after twenty five thousand

1367
01:26:34,370 --> 01:26:38,300
and if look at the behavior and then run does not learning

1368
01:26:38,390 --> 01:26:43,090
and show what happens to the fitness obtained by the agent with those two reward

1369
01:26:47,250 --> 01:26:51,680
best the reward function that that before twenty five thousand

1370
01:26:52,500 --> 01:26:56,610
is the rule of it basically line between one and two

1371
01:26:56,630 --> 01:26:58,590
just keep eating one

1372
01:26:58,760 --> 01:27:02,840
the agent the best reward function after twenty five thousand

1373
01:27:02,850 --> 01:27:06,790
last week we should take some time to learn to eat fish about twelve thousand

1374
01:27:07,470 --> 01:27:12,060
twenty five thousand time steps before and efficiently learning to eat fish

1375
01:27:12,070 --> 01:27:15,650
overcomes the disadvantage of not having eaten

1376
01:27:16,900 --> 01:27:19,560
that's that explains the twenty five thousand

1377
01:27:19,570 --> 01:27:22,060
because now here's more interesting results

1378
01:27:22,100 --> 01:27:25,130
look at the best internal reward functions

1379
01:27:25,470 --> 01:27:28,600
the best internal reward function

1380
01:27:28,850 --> 01:27:32,390
before you can ever learn to fix this was the need

1381
01:27:32,400 --> 01:27:37,320
at twelve thousand of clustering as

1382
01:27:37,330 --> 01:27:41,830
two thousand writers and the that even the best one hundred six about two thousand

1383
01:27:41,830 --> 01:27:46,740
time steps learn to eat fish so the time step it doesn't really matter what

1384
01:27:46,740 --> 01:27:50,630
reward function it is within the reward function as long as they don't reward functions

1385
01:27:50,630 --> 01:27:52,630
as good to eat bait

1386
01:27:52,640 --> 01:27:56,710
don't get distracted by fish but you know it's good to be experimental results between

1387
01:27:56,720 --> 01:28:01,190
two thousand twenty five thousand with the best internal reward function is overturning the thickness

1388
01:28:01,800 --> 01:28:04,240
it's saying you know what

1389
01:28:04,300 --> 01:28:06,680
eating bridges carols

1390
01:28:06,700 --> 01:28:11,090
because you have the opportunity to get distracted by fish don't like you can your

1391
01:28:11,870 --> 01:28:12,950
in your life

1392
01:28:12,960 --> 01:28:14,580
to learn to to eat fish

1393
01:28:14,590 --> 01:28:19,130
to learn fast enough to exploit to do to overcome

1394
01:28:19,150 --> 01:28:21,960
just having even ones all the time

1395
01:28:22,000 --> 01:28:26,680
right so so this is reversal that's the really interesting part that could explain some

1396
01:28:26,680 --> 01:28:30,640
of the things that are happening in in the sort of things that that the

1397
01:28:30,640 --> 01:28:36,400
talk about that the best adapted internal reward could be quite different from from what

1398
01:28:36,420 --> 01:28:39,150
the fitness might be the fitness function might

1399
01:28:42,220 --> 01:28:44,720
let me say one more thing i was going to be on time

1400
01:28:45,660 --> 01:28:50,470
i am today all my ninety minutes by the way just because

1401
01:28:50,480 --> 01:28:51,560
so can make get

1402
01:28:51,690 --> 01:29:00,440
so so the idea that lots of experiments there's lots of experiments showing cute things

1403
01:29:00,440 --> 01:29:04,740
about internal reward being different from fitness and advantages and so on and it turns

1404
01:29:04,740 --> 01:29:06,080
out there are

1405
01:29:06,120 --> 01:29:11,810
there are probably properties of environments that correspond to good internal reward functions we we

1406
01:29:11,810 --> 01:29:16,270
solve the optimisation problem but there is good reason to believe that the principles of

1407
01:29:16,290 --> 01:29:18,230
internal reward they can emerge

1408
01:29:18,250 --> 01:29:21,940
there are in the interesting and show you one

1409
01:29:21,950 --> 01:29:23,670
such principled in manner

1410
01:29:23,680 --> 01:29:26,880
so here's a strong conjecture we have

1411
01:29:26,900 --> 01:29:32,840
strong conjecture we have is that internal reward mitigates agent about this

1412
01:29:33,230 --> 01:29:35,950
four let's say that in the opposite way

1413
01:29:35,990 --> 01:29:39,520
if some agents were completely unbalanced

1414
01:29:39,540 --> 01:29:41,560
all powerful

1415
01:29:42,520 --> 01:29:44,650
nine to break the contract

1416
01:29:45,750 --> 01:29:48,470
the data and is just one powerful

1417
01:29:48,490 --> 01:29:50,220
so might as well just give my

1418
01:29:50,220 --> 01:29:54,590
as a designer i get the fitness is an award because it is all powerful

1419
01:29:54,590 --> 01:29:56,570
right just do the right thing

1420
01:29:56,720 --> 01:29:58,790
is because it is limited

1421
01:29:58,810 --> 01:30:03,240
in various kinds of ways either because the finite lifetime as using slow learning algorithm

1422
01:30:03,240 --> 01:30:06,210
as was the case before or because of limited memory

1423
01:30:06,220 --> 01:30:08,340
or because it is limited no

1424
01:30:08,350 --> 01:30:15,230
common resources any any sort right it's because of that i think internal rewards

1425
01:30:15,240 --> 01:30:18,340
are that you want to break contact

1426
01:30:18,350 --> 01:30:20,540
in fact we believe that there are probably

1427
01:30:20,550 --> 01:30:28,100
principles that map forms of boundedness two forms of total

1428
01:30:28,120 --> 01:30:29,650
OK that's

1429
01:30:29,670 --> 01:30:31,440
that's all

1430
01:30:33,310 --> 01:30:37,540
i get what i mean mitigation our internal rewards mitigate about this i mean the

1431
01:30:37,540 --> 01:30:40,790
bounded agent with confounded reward will do

1432
01:30:40,800 --> 01:30:42,150
say so

1433
01:30:42,180 --> 01:30:45,250
the unbounded agent

1434
01:30:45,290 --> 01:30:48,480
with confounded reward will will

1435
01:30:49,290 --> 01:30:51,120
i will do whatever do

1436
01:30:51,190 --> 01:30:53,040
the gap between these two

1437
01:30:53,060 --> 01:30:54,390
the point is

1438
01:30:54,400 --> 01:30:58,040
the same married agent but given the internal reward

1439
01:30:58,050 --> 01:31:00,430
and mitigate some of that gap that the

1440
01:31:00,450 --> 01:31:01,370
that's the picture

1441
01:31:03,450 --> 01:31:09,380
trying to do experiment so all this foraging domain in the form of boundedness is

1442
01:31:09,380 --> 01:31:12,970
the agent has incomplete information about the state

1443
01:31:13,080 --> 01:31:16,710
you can't really see where the where the world is and so

1444
01:31:16,730 --> 01:31:20,650
it got planned to read the warming doesn't know where it is it turns out

1445
01:31:20,650 --> 01:31:22,110
that a

1446
01:31:22,130 --> 01:31:23,870
internal rewards that

1447
01:31:24,220 --> 01:31:29,800
that said that encourages exploration going to places you have been to before

1448
01:31:29,800 --> 01:31:31,230
in andromeda

1449
01:31:31,320 --> 01:31:32,780
to construct physics

1450
01:31:32,780 --> 01:31:34,490
you need some unit of length

1451
01:31:34,510 --> 01:31:37,920
some unit of time and some unit of mass of three things

1452
01:31:37,960 --> 01:31:41,880
and playing was very excited by the idea that now

1453
01:31:43,670 --> 01:31:46,070
with his constant

1454
01:31:46,110 --> 01:31:47,820
you could join

1455
01:31:47,860 --> 01:31:53,590
the speed of light which appears in maxwell's equations and newton's gravitational constant g to

1456
01:31:53,590 --> 01:31:56,730
make one two three

1457
01:31:56,730 --> 01:32:01,440
quantities that appear fundamental with units that appear in fundamental laws of physics

1458
01:32:01,510 --> 01:32:05,800
so that by combining these with different powers you could construct

1459
01:32:05,840 --> 01:32:09,360
three things the length unit of length the unit of mass and unit of time

1460
01:32:09,650 --> 01:32:11,690
and have something you could transmit

1461
01:32:11,730 --> 01:32:14,960
to have something you could describe without transmitting

1462
01:32:15,050 --> 01:32:21,840
two how inhabitants of andromeda

1463
01:32:21,860 --> 01:32:24,030
since then

1464
01:32:24,090 --> 01:32:27,550
planks intuition has as

1465
01:32:28,510 --> 01:32:32,840
elevated almost into a principle

1466
01:32:32,860 --> 01:32:38,980
that it's the stakes have become much higher for this kind of numeral or numerology

1467
01:32:39,030 --> 01:32:40,210
it really

1468
01:32:40,230 --> 01:32:41,340
it is

1469
01:32:41,360 --> 01:32:43,170
probably the

1470
01:32:43,230 --> 01:32:44,550
the only road

1471
01:32:44,590 --> 01:32:46,010
two realizing

1472
01:32:46,250 --> 01:32:49,760
a much older vision that goes back to factories

1473
01:32:49,780 --> 01:32:53,610
i've tried to explain everything about the world in terms of numbers

1474
01:32:53,610 --> 01:32:58,210
now of course you can't really explain everything about the world in terms of numbers

1475
01:32:58,250 --> 01:33:02,110
because numbers don't have units

1476
01:33:02,130 --> 01:33:05,530
so ultimately you have to have some way of smuggling in units

1477
01:33:05,590 --> 01:33:08,090
and together with pure numbers

1478
01:33:09,030 --> 01:33:12,110
c g and h give us a beautiful way

1479
01:33:12,150 --> 01:33:15,130
such a beautiful way of smuggling in units

1480
01:33:15,150 --> 01:33:18,820
but i like to call them honorary numbers

1481
01:33:18,840 --> 01:33:23,980
because they are so deeply embedded in fundamental principles of

1482
01:33:24,030 --> 01:33:28,670
deep lies deep theories of physics that you can do without them in some form

1483
01:33:28,690 --> 01:33:30,630
so specifying

1484
01:33:30,650 --> 01:33:32,440
that they exist

1485
01:33:32,490 --> 01:33:36,300
it is part of specifying the theory

1486
01:33:36,340 --> 01:33:36,940
let me

1487
01:33:36,960 --> 01:33:38,440
be a little more specific

1488
01:33:38,460 --> 01:33:41,990
in this in the theory of relativity and special relativity

1489
01:33:42,010 --> 01:33:45,510
you learn that there's the symmetry between space and time

1490
01:33:45,550 --> 01:33:48,630
that one looks like an interval of space to you might look like an interval

1491
01:33:48,630 --> 01:33:51,420
of time to me from moving by

1492
01:33:51,510 --> 01:33:54,530
but of course space and time are measured in different units

1493
01:33:54,570 --> 01:33:59,260
so to make that notion incoherent there has to be some conversion factor between the

1494
01:33:59,260 --> 01:34:01,840
unit of length in the unit of time

1495
01:34:01,880 --> 01:34:03,510
and that's the role of c

1496
01:34:03,530 --> 01:34:05,900
the theory of special relativity

1497
01:34:06,980 --> 01:34:09,880
in the theory of general relativity

1498
01:34:09,880 --> 01:34:10,840
you learn

1499
01:34:10,900 --> 01:34:16,440
that spacetime curvature is caused by the local density of energy momentum

1500
01:34:16,480 --> 01:34:18,460
but those are measured in different units

1501
01:34:18,460 --> 01:34:21,730
so you have to have some conversion factor to make the local here to make

1502
01:34:21,730 --> 01:34:25,380
it makes sense and that's the role of newton's constant g

1503
01:34:25,420 --> 01:34:27,190
since in general relativity

1504
01:34:28,130 --> 01:34:31,010
in quantum mechanics and modern quantum mechanics

1505
01:34:32,150 --> 01:34:35,510
you've wave particle duality or things like

1506
01:34:35,530 --> 01:34:39,710
the relationship between energy and frequency of the photon

1507
01:34:39,730 --> 01:34:43,360
which is the embodiment of an aspect the wave particle duality

1508
01:34:43,420 --> 01:34:48,420
but energy and frequency are measured in different units you need conversion factor and that's

1509
01:34:48,420 --> 01:34:51,260
the role of playing seconds

1510
01:34:51,280 --> 01:34:54,840
so to make any of those theories cohen and you have to introduce these conversion

1511
01:34:55,800 --> 01:34:59,730
just like converting enters into yards

1512
01:34:59,780 --> 01:35:03,900
because there unavoidable relationships you need in the theory

1513
01:35:03,960 --> 01:35:06,190
and that's what i call the unary numbers

1514
01:35:06,190 --> 01:35:08,650
course plankton only

1515
01:35:08,650 --> 01:35:12,340
so it's quite remarkable that he stumbled into this nevertheless

1516
01:35:12,400 --> 01:35:16,320
we call them units and this core planks vision

1517
01:35:16,320 --> 01:35:18,830
on the other hand if you these two events

1518
01:35:18,890 --> 01:35:22,430
those that do not of course then the probability that the alarm is sounding is

1519
01:35:22,430 --> 01:35:24,950
very low and

1520
01:35:25,010 --> 01:35:30,670
there similarly for this that time follow entry we we can be able to specify

1521
01:35:30,670 --> 01:35:31,740
all these

1522
01:35:31,760 --> 01:35:37,160
the distribution setting and then the remaining tables

1523
01:35:37,170 --> 01:35:39,890
the hard the probability of our rules

1524
01:35:39,910 --> 01:35:45,810
here which are very very low the probability of a bimodal and we very low

1525
01:35:45,910 --> 01:35:48,220
up here we all know

1526
01:35:48,280 --> 01:35:49,400
if now

1527
01:35:49,410 --> 01:35:53,450
know that dalai is something we can ask what is the probability

1528
01:35:53,470 --> 01:35:55,840
that the bible was

1529
01:35:55,870 --> 01:35:58,190
in the aisles given that

1530
01:35:58,210 --> 01:36:06,880
alarm is sounding and we can find out that the probability is very high

1531
01:36:06,890 --> 01:36:08,040
if we now have

1532
01:36:08,050 --> 01:36:10,310
some additional evidence

1533
01:36:10,330 --> 01:36:16,350
that the radio broadcaster and that was warming then we can do a similar calculation

1534
01:36:16,590 --> 01:36:18,180
to find out that

1535
01:36:18,200 --> 01:36:26,150
the probability now that there wasn't bargain is very low so we started from

1536
01:36:26,160 --> 01:36:32,460
quite high probability and then this additional evidence enable us to

1537
01:36:32,490 --> 01:36:33,900
to understand the

1538
01:36:33,910 --> 01:36:36,600
the probability that the virus was in their houses

1539
01:36:36,620 --> 01:36:40,690
very very small

1540
01:36:40,710 --> 01:36:48,690
now i have said that a graphical models are useful because they neighbors to understand

1541
01:36:48,720 --> 01:36:53,300
dependencies independencies relationship by just looking at the graph in the next few slides i

1542
01:36:53,300 --> 01:36:58,890
will explain to you how to do that let's consider a very simple case of

1543
01:36:58,890 --> 01:37:00,970
which we have three noughts

1544
01:37:01,000 --> 01:37:03,490
three random variables a b and c

1545
01:37:03,600 --> 01:37:07,330
and if we create the link between all the variables that we can all see

1546
01:37:07,330 --> 01:37:11,380
anything about independence bowl despite so we have to draw

1547
01:37:11,400 --> 01:37:16,640
some link in the graph let's drop the link for example from a to b

1548
01:37:16,650 --> 01:37:22,200
and consider all the possible configurations all the possible really which

1549
01:37:22,220 --> 01:37:28,150
results from the different the orientation of the world i have listed the for all

1550
01:37:28,170 --> 01:37:31,580
four of them here if you look at the first three

1551
01:37:31,600 --> 01:37:33,290
what we can

1552
01:37:33,300 --> 01:37:35,760
in fact is that

1553
01:37:35,780 --> 01:37:40,820
the valuable and b are conditionally independent

1554
01:37:40,840 --> 01:37:48,150
conditionally independent given the right we see that i'm here indicate the yellow color

1555
01:37:48,190 --> 01:37:51,330
on the other hand if i look at this graph here

1556
01:37:51,350 --> 01:37:53,690
a and b are

1557
01:37:53,750 --> 01:37:56,680
not conditionally independent given

1558
01:37:56,690 --> 01:38:00,300
c was first allowed to ensure that

1559
01:38:00,320 --> 01:38:04,540
for the first three of should they be an independent given c well i can

1560
01:38:04,540 --> 01:38:09,520
simply write down for example for the first release that

1561
01:38:10,580 --> 01:38:13,470
distribution of a and b given c which is

1562
01:38:13,500 --> 01:38:17,170
great tenacity show of the joint distribution maybe c

1563
01:38:17,190 --> 01:38:18,040
given by

1564
01:38:18,040 --> 01:38:22,630
divided by the distribution of seeing i can use the full authorization

1565
01:38:22,650 --> 01:38:24,390
the theory about from

1566
01:38:24,400 --> 01:38:30,700
from the movement or see as not science also have to put it down here

1567
01:38:30,760 --> 01:38:32,370
p of c then

1568
01:38:33,310 --> 01:38:35,300
b as this apparent c

1569
01:38:35,300 --> 01:38:38,080
that's why i put that there appear to be c

1570
01:38:38,090 --> 01:38:44,670
and a fiancee so i needed there given see now we can see that these

1571
01:38:44,670 --> 01:38:45,670
two terms

1572
01:38:45,670 --> 01:38:47,270
cancel out

1573
01:38:47,280 --> 01:38:49,620
and therefore i o thing the

1574
01:38:50,720 --> 01:38:52,310
like can you can show

1575
01:38:52,320 --> 01:38:53,660
for the other two

1576
01:38:53,670 --> 01:38:58,330
i believe that or misdemeanour similar way why they are independent

1577
01:38:58,340 --> 01:38:59,910
and similar with

1578
01:38:59,920 --> 01:39:04,770
similar reasoning that we can use social that a and b are not independent

1579
01:39:04,790 --> 01:39:06,080
given c

1580
01:39:06,090 --> 01:39:09,130
in this sense that in

1581
01:39:10,830 --> 01:39:13,090
did this associated distributions

1582
01:39:13,120 --> 01:39:19,560
o four for most of the associated distribution a and b

1583
01:39:19,950 --> 01:39:24,920
will be independent but there might be cases in which of the particle distribution for

1584
01:39:24,920 --> 01:39:29,840
which a and b would be in the independent given to this is the kind

1585
01:39:29,840 --> 01:39:33,770
of us up the point that we have to understand the world graphical models we

1586
01:39:33,770 --> 01:39:36,630
can really for our independence

1587
01:39:36,800 --> 01:39:38,770
both random variable

1588
01:39:38,930 --> 01:39:40,530
but we

1589
01:39:40,540 --> 01:39:41,900
so we can not

1590
01:39:43,120 --> 01:39:44,460
fact true

1591
01:39:44,470 --> 01:39:49,680
two dependencies we can just say that a and b are not

1592
01:39:49,690 --> 01:39:53,060
independent given c

1593
01:39:53,600 --> 01:40:00,260
now if i consider the case we can reach i don't condition of c

1594
01:40:02,060 --> 01:40:07,000
this in the the first three cases a and b become

1595
01:40:07,010 --> 01:40:09,970
dependent in the sense this plane before

1596
01:40:10,030 --> 01:40:13,650
why in the forties a and b

1597
01:40:13,670 --> 01:40:17,850
become independent

1598
01:40:17,880 --> 01:40:19,310
basically what they have

1599
01:40:19,320 --> 01:40:21,790
in this slide is that

1600
01:40:21,800 --> 01:40:23,490
if c

1601
01:40:23,510 --> 01:40:26,470
as more than one incoming link

1602
01:40:26,500 --> 01:40:28,490
then a and b will be in the

1603
01:40:29,800 --> 01:40:34,420
but they will be independent when the conditional sitting on the other hand

1604
01:40:34,440 --> 01:40:39,950
if c has at most one incoming link then a and b would be

1605
01:40:39,960 --> 01:40:46,670
conditionally independent given c but they will be marginally dependent so in the first case

1606
01:40:46,670 --> 01:40:51,360
here is called collider in the second case is called and non collider so the

1607
01:40:51,360 --> 01:40:53,560
collider collider seem to

1608
01:40:53,640 --> 01:40:55,510
they behave in the kind of

1609
01:40:58,080 --> 01:41:00,140
wait so let them down

1610
01:41:00,160 --> 01:41:05,710
look at some x complicated example in which we have both collided and in all

1611
01:41:05,720 --> 01:41:08,540
collider in in the part between

1612
01:41:08,540 --> 01:41:12,300
our body will of interest so here for example we have

1613
01:41:12,310 --> 01:41:14,140
four variables

1614
01:41:14,160 --> 01:41:17,820
so we want to see if a and b and d are independent

1615
01:41:17,840 --> 01:41:19,820
given bscl always

1616
01:41:19,850 --> 01:41:25,320
indicates that i'm conditions over these five or

1617
01:41:25,500 --> 01:41:29,800
since there is a call there in the path

1618
01:41:29,810 --> 01:41:35,950
and conditioning on made the means that i i maintain dependencies and i i expressed

1619
01:41:35,950 --> 01:41:36,970
that by

1620
01:41:36,970 --> 01:41:42,300
let's look at

1621
01:41:42,300 --> 01:41:44,780
this next little piece of code

1622
01:41:46,130 --> 01:41:49,590
binding easy to be some value and then i'm going to run this

1623
01:41:49,630 --> 01:41:56,610
well list is run and see what it does

1624
01:41:58,130 --> 01:42:01,440
OK so what does look at what i'm doing a test there to say

1625
01:42:01,440 --> 01:42:04,990
if the string x is less than the value of the

1626
01:42:05,010 --> 01:42:09,240
next not appear before b is strings then i was going to do

1627
01:42:09,300 --> 01:42:10,940
a couple of things

1628
01:42:10,940 --> 01:42:13,420
because there at the same block level

1629
01:42:13,470 --> 01:42:15,260
given that wasn't true

1630
01:42:15,380 --> 01:42:16,720
did nothing

1631
01:42:16,820 --> 01:42:19,590
now wait a minute you say where's the else clause

1632
01:42:19,610 --> 01:42:21,780
and the answer is i don't need one

1633
01:42:21,860 --> 01:42:25,260
right if this is purely a test of this is true do this otherwise i

1634
01:42:25,260 --> 01:42:26,030
don't care

1635
01:42:26,090 --> 01:42:28,530
i don't need the else clause in to identify

1636
01:42:30,320 --> 01:42:33,630
the second thing i want to look at is suppose i compare that to the

1637
01:42:33,630 --> 01:42:36,090
one below it

1638
01:42:36,150 --> 01:42:39,970
ips i don't want to do

1639
01:42:39,990 --> 01:42:42,990
comment that out

1640
01:42:43,050 --> 01:42:48,340
it's uncommon this

1641
01:42:48,360 --> 01:42:50,340
still binding for

1642
01:42:50,340 --> 01:42:52,610
in the same test but notice now

1643
01:42:52,750 --> 01:42:56,320
but the two same commands but they have different indentation

1644
01:42:56,340 --> 01:43:03,320
in this case in fact i do get a different behavior

1645
01:43:04,920 --> 01:43:07,550
because that block identifies

1646
01:43:07,570 --> 01:43:10,610
a set of things i'm going to do if the test is true

1647
01:43:10,610 --> 01:43:14,610
the test was not true notice that last comment for print is now back at

1648
01:43:14,610 --> 01:43:19,150
the same level of the so what this says is the if does the test

1649
01:43:19,190 --> 01:43:22,070
having done the test tested decide not going to do anything in the block below

1650
01:43:22,070 --> 01:43:25,800
it i'm going to skip down therefore to the next instruction at the same level

1651
01:43:25,800 --> 01:43:26,800
as the if

1652
01:43:26,840 --> 01:43:30,190
which gets me to the second print statement

1653
01:43:30,240 --> 01:43:33,050
OK so now we're seeing some of these variations

1654
01:43:33,090 --> 01:43:35,940
let's see what else can we do here still image to try something a little

1655
01:43:35,940 --> 01:43:41,240
more interesting and then we'll get to writing some simple programs with common those out

1656
01:43:41,240 --> 01:43:45,090
let's go down to

1657
01:43:45,110 --> 01:43:46,650
this piece of code

1658
01:43:46,690 --> 01:43:48,360
and commented

1659
01:43:48,400 --> 01:43:53,570
uh yes that was brilliant let's try this again

1660
01:43:53,610 --> 01:43:56,650
uncommon that

1661
01:43:56,740 --> 01:43:59,570
one uncommon to the game

1662
01:43:59,700 --> 01:44:02,420
so here's a little piece of code is going to print out

1663
01:44:02,440 --> 01:44:03,880
the smallest value

1664
01:44:05,400 --> 01:44:08,280
notice what this is showing is that these can be nested

1665
01:44:08,360 --> 01:44:11,340
it is i look that is see x is what sorry if x is less

1666
01:44:11,340 --> 01:44:12,820
than y then

1667
01:44:12,880 --> 01:44:15,400
check to see if x is less than c

1668
01:44:15,460 --> 01:44:16,760
and if that's true

1669
01:44:16,820 --> 01:44:18,920
the x is the smallest

1670
01:44:19,010 --> 01:44:22,490
notice the structure the if it's not going to go to the next else

1671
01:44:22,510 --> 01:44:24,190
print of disease models

1672
01:44:24,240 --> 01:44:27,780
if the first test wasn't runescape that whole block and just go down to print

1673
01:44:27,780 --> 01:44:29,650
out the y was small

1674
01:44:29,820 --> 01:44:33,090
notice the nesting i can flow my way through how those tests are actually going

1675
01:44:33,090 --> 01:44:34,820
to take place

1676
01:44:34,860 --> 01:44:41,200
right so let's run this and see what happens

1677
01:44:42,220 --> 01:44:45,130
why smalls

1678
01:44:45,150 --> 01:44:49,550
OK the co correct

1679
01:44:49,650 --> 01:44:51,240
ten of hand back there

1680
01:44:51,240 --> 01:44:57,990
it's not doing all of the comparisons

1681
01:44:58,050 --> 01:45:00,240
i mean this is check this out because i want to make a point to

1682
01:45:00,260 --> 01:45:02,800
this let's go back and do the following

1683
01:45:02,840 --> 01:45:05,070
let's take y

1684
01:45:05,090 --> 01:45:07,630
changes to thirteen

1685
01:45:07,740 --> 01:45:11,050
to run

1686
01:45:12,650 --> 01:45:14,860
so what did i miss you

1687
01:45:14,900 --> 01:45:16,590
two important points

1688
01:45:16,630 --> 01:45:20,090
the first one when i write a piece of code especially code that has branches

1689
01:45:20,090 --> 01:45:22,610
in one i design test case

1690
01:45:22,630 --> 01:45:24,200
for that piece of code

1691
01:45:24,220 --> 01:45:29,190
i should try and have a specific test case for each possible path through the

1692
01:45:30,460 --> 01:45:33,170
by just doing that is part of the about here

1693
01:45:33,220 --> 01:45:35,740
the but was in my thinking i did not look for all the test so

1694
01:45:35,740 --> 01:45:38,400
the way i can fix that is

1695
01:45:38,460 --> 01:45:40,400
comment tips

1696
01:45:40,400 --> 01:45:44,170
overall so that does look like a rich set of vectors

1697
01:45:45,910 --> 01:45:46,610
for we in

1698
01:45:47,070 --> 01:45:49,290
look at the cardinality of the set of vectors

1699
01:45:50,490 --> 01:45:55,570
so the more advanced branches of learning theory did not get geometry of this set and how the

1700
01:45:56,880 --> 01:45:58,830
also the details with the internet

1701
01:45:59,640 --> 01:46:01,940
we got the cardinality of the set of vectors

1702
01:46:02,380 --> 01:46:03,210
and if we didn't take

1703
01:46:03,690 --> 01:46:04,850
expectation this

1704
01:46:06,090 --> 01:46:07,810
the logarithm of this county

1705
01:46:09,220 --> 01:46:12,690
through we were interested whether this thing grows exponentially so with the

1706
01:46:14,820 --> 01:46:16,900
if we take expectation in the logarithm

1707
01:46:17,460 --> 01:46:22,040
we get something called the we see entropy of picture when interbreed

1708
01:46:23,210 --> 01:46:25,070
end of determining true

1709
01:46:27,270 --> 01:46:31,780
a necessary and sufficient condition for uniform convergence of risk

1710
01:46:32,540 --> 01:46:34,470
so the uniform convergence of risk

1711
01:46:35,190 --> 01:46:37,860
well remembered one sided uniform convergence risk

1712
01:46:38,740 --> 01:46:40,920
was equivalent to a nontrivial consistency

1713
01:46:41,440 --> 01:46:45,130
we were saying it's difficult to check this kind of property for learning machines so we need

1714
01:46:45,830 --> 01:46:49,340
other properties not determining the problem equivalent

1715
01:46:50,070 --> 01:46:51,400
uniform convergence is

1716
01:46:52,610 --> 01:46:53,100
two sided

1717
01:46:53,680 --> 01:46:54,640
but similar

1718
01:46:56,680 --> 01:46:57,410
the fact that this

1719
01:46:58,110 --> 01:46:58,900
we see entropy

1720
01:46:59,540 --> 01:47:00,830
grows sublinearly

1721
01:47:01,980 --> 01:47:03,430
so we seem to be happening

1722
01:47:04,540 --> 01:47:05,640
if this zero

1723
01:47:07,560 --> 01:47:08,500
we have consistency

1724
01:47:11,120 --> 01:47:11,670
it depends

1725
01:47:12,260 --> 01:47:13,330
there's an expectation here

1726
01:47:14,660 --> 01:47:16,100
and is again over the

1727
01:47:16,320 --> 01:47:18,210
generation of training samples from all

1728
01:47:18,690 --> 01:47:22,360
unknown probability distribution so that depends on the problem that we are

1729
01:47:23,180 --> 01:47:23,690
dealing with

1730
01:47:26,510 --> 01:47:28,030
this further concepts

1731
01:47:28,490 --> 01:47:31,410
the the wall course a sense the first one is

1732
01:47:32,180 --> 01:47:38,730
if we exchange the expectation in the logarithm remember is a convex function or concave depending which way round to

1733
01:47:39,290 --> 01:47:39,830
define it

1734
01:47:40,930 --> 01:47:44,980
reduces inequality is we exchange the expectation

1735
01:47:45,370 --> 01:47:46,110
end lower

1736
01:47:48,140 --> 01:47:48,720
if we do that

1737
01:47:49,650 --> 01:47:51,570
we get a quantity which is

1738
01:47:52,640 --> 01:47:53,730
the annealed entropy

1739
01:47:54,880 --> 01:47:57,740
and if the annealed entropy now zero

1740
01:47:58,190 --> 01:47:59,080
one divided by and

1741
01:47:59,940 --> 01:48:02,600
it's a slightly stronger condition because this inequality

1742
01:48:03,530 --> 01:48:08,100
and this condition can be equivalent to exponentially fast uniform conversions

1743
01:48:08,880 --> 01:48:12,940
so it turns out in the former case we could have uniform conversions that's arbitrarily slow

1744
01:48:13,360 --> 01:48:13,920
in this case

1745
01:48:14,470 --> 01:48:15,530
we get the normal rate

1746
01:48:18,290 --> 01:48:19,850
great like in town of

1747
01:48:21,150 --> 01:48:22,570
if we go one step further

1748
01:48:23,580 --> 01:48:25,890
instead of an expectation take maximum

1749
01:48:26,670 --> 01:48:27,930
that again football

1750
01:48:29,500 --> 01:48:31,460
we get something which is called the growth function

1751
01:48:33,170 --> 01:48:34,440
and the growth function

1752
01:48:36,030 --> 01:48:39,910
is related to what i told you about before we this shattering coefficient

1753
01:48:41,960 --> 01:48:43,140
in the growth function

1754
01:48:44,810 --> 01:48:47,100
is basically the logarithm of this shattering coefficient

1755
01:48:48,270 --> 01:48:48,670
and the

1756
01:48:49,960 --> 01:48:50,570
the growth function

1757
01:48:51,720 --> 01:48:54,050
if the growth function divided by and goes to zero

1758
01:48:54,800 --> 01:48:58,580
we have an exponential convergence for all underlying probability distributions

1759
01:48:59,100 --> 01:48:59,590
actually i

1760
01:49:00,100 --> 01:49:01,960
is something well before the shattering required

1761
01:49:02,540 --> 01:49:06,410
someone image sharing coefficient depends on the distribution of course it doesn't

1762
01:49:08,210 --> 01:49:09,730
we were not written on the slides

1763
01:49:10,190 --> 01:49:11,160
so the growth function

1764
01:49:11,740 --> 01:49:12,570
going to zero

1765
01:49:15,280 --> 01:49:16,150
divided by and

1766
01:49:16,570 --> 01:49:16,820
is now

1767
01:49:18,280 --> 01:49:21,110
necessary and sufficient condition for exponential convergence

1768
01:49:21,590 --> 01:49:23,780
independent of the underlying distribution

1769
01:49:24,940 --> 01:49:27,220
and the growth function is something that is very close to

1770
01:49:28,610 --> 01:49:31,180
tell when is not mentioned that you probably all the

1771
01:49:32,380 --> 01:49:33,760
the growth function and

1772
01:49:34,830 --> 01:49:36,300
has a certain type of behavior

1773
01:49:36,780 --> 01:49:37,140
which is

1774
01:49:37,990 --> 01:49:40,330
the characterized by dimension

1775
01:49:41,260 --> 01:49:44,230
so we are interested in cases where the growth function

1776
01:49:44,680 --> 01:49:46,720
grows sublinearly in it

1777
01:49:47,160 --> 01:49:47,690
it turns out

1778
01:49:48,210 --> 01:49:51,250
the typical situation will be that the growth function at the beginning

1779
01:49:51,760 --> 01:49:54,000
if you have a small numbers of training examples

1780
01:49:54,820 --> 01:49:56,850
grows linearly so typically

1781
01:49:57,570 --> 01:49:59,790
will start with behavior like this

1782
01:50:00,480 --> 01:50:02,720
which means that all possible separations

1783
01:50:03,150 --> 01:50:03,810
of the two

1784
01:50:04,650 --> 01:50:08,910
all possible separations of training data can be realized using functions of

1785
01:50:12,570 --> 01:50:15,330
end suddenly some and it would typically

1786
01:50:15,880 --> 01:50:17,070
we no longer the case

1787
01:50:18,220 --> 01:50:18,900
a little bit slower

1788
01:50:19,840 --> 01:50:20,690
so as long as this

1789
01:50:21,330 --> 01:50:22,470
equality here is true

1790
01:50:23,340 --> 01:50:24,920
so the growth function grows linearly

1791
01:50:25,760 --> 01:50:26,260
this means

1792
01:50:27,360 --> 01:50:28,050
it no

1793
01:50:28,740 --> 01:50:29,480
all these and

1794
01:50:30,670 --> 01:50:32,470
all this can be generated

1795
01:50:34,410 --> 01:50:36,710
in the end points can be chosen such that

1796
01:50:38,180 --> 01:50:41,520
by using functions that the learning machine they can be separated in all possible ways

1797
01:50:42,140 --> 01:50:43,500
which we call chattering before

1798
01:50:45,260 --> 01:50:46,490
the slide is you

1799
01:50:47,540 --> 01:50:48,840
and it turns out that the

1800
01:50:49,290 --> 01:50:50,480
this is also something that would

1801
01:50:50,930 --> 01:50:52,470
turning seven fold proven

1802
01:50:53,670 --> 01:50:56,510
in nineteen sixty eight it was later proven also by

1803
01:50:56,930 --> 01:50:57,510
some of

1804
01:50:58,210 --> 01:51:02,430
scientists which is what is sometimes known as the cells was

1805
01:51:03,250 --> 01:51:05,070
approved by someone by the name of the

1806
01:51:06,230 --> 01:51:08,480
it was also proved by sheila

1807
01:51:10,420 --> 01:51:11,370
sometimes called the

1808
01:51:12,220 --> 01:51:14,230
ten minutes so of the last them on

1809
01:51:15,140 --> 01:51:16,180
combinatorial among

1810
01:51:17,020 --> 01:51:18,470
which says that this function

1811
01:51:19,590 --> 01:51:23,240
and was also once poses a question by elders to someone

1812
01:51:24,070 --> 01:51:26,860
and less complicated story about the history of this

1813
01:51:29,720 --> 01:51:31,910
but so basically the result that

1814
01:51:32,780 --> 01:51:33,410
the either

1815
01:51:33,610 --> 01:51:35,330
the function class grows linearly

1816
01:51:36,530 --> 01:51:37,130
four and

1817
01:51:38,430 --> 01:51:39,520
it is a form of

1818
01:51:40,010 --> 01:51:42,470
it starts like that and so much more than

1819
01:51:43,410 --> 01:51:44,330
the following is true

1820
01:51:45,420 --> 01:51:47,220
this is the end called dimension

1821
01:51:48,320 --> 01:51:49,790
and once we passed and

1822
01:51:51,140 --> 01:51:52,670
the growth function suddenly grows

1823
01:51:53,260 --> 01:51:54,390
only logarithmically

1824
01:51:56,510 --> 01:52:00,180
so that's kind of a surprising result so it staff linearly

1825
01:52:02,290 --> 01:52:02,780
and remember

1826
01:52:03,430 --> 01:52:08,900
algorithm and the definition of the growth function the growth function grows logarithmically related to the shattering coefficient

1827
01:52:09,570 --> 01:52:11,760
remember the shattering coefficient was reduced

1828
01:52:12,790 --> 01:52:15,420
most exponentially the growth function most in

1829
01:52:16,930 --> 01:52:19,390
so he really really grows linearly is the

1830
01:52:19,810 --> 01:52:20,820
massively rich one

1831
01:52:22,540 --> 01:52:24,440
famously wrote can stop

