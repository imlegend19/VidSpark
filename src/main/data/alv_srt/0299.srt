1
00:00:00,000 --> 00:00:04,600
to find points that match each other one configuration to the other in

2
00:00:04,600 --> 00:00:09,020
this notation that saying Xi J equals I K A that is they both correspond

3
00:00:09,020 --> 00:00:15,880
to the same hidden point that's the here is the formulation so this this sketch

4
00:00:15,880 --> 00:00:20,860
shows you what we're trying to do the anything's we know are the red points and the green ones

5
00:00:20,860 --> 00:00:28,810
X and Y drawn for emphasis in different spaces we're trying to infere

6
00:00:28,890 --> 00:00:36,300
rest so it looks like quite a heroic thing to do so we're inventing this new process and

7
00:00:36,300 --> 00:00:41,320
the points generated by selection from that and adding noise there's the Xs in the

8
00:00:41,320 --> 00:00:46,920
case of the Ys selection from them adding noise and then transforming through this transformation

9
00:00:46,920 --> 00:00:52,300
A in case we're trying to make an inference about everything except the the data

10
00:00:52,300 --> 00:00:58,680
which are the red and green points now there's a what we're gonna do is start

11
00:00:58,680 --> 00:01:04,960
getting rid of some of the sup you know superfluity redundancy in this formulation

12
00:01:04,960 --> 00:01:08,660
the first thing will do is notice that really the the indexing of the hidden

13
00:01:08,660 --> 00:01:13,780
points is arbitrary and if we don't observe anything based and all we really care about

14
00:01:13,780 --> 00:01:18,960
is this matching matrix one oh zero depending on whether the J F X point matches the

15
00:01:18,960 --> 00:01:25,200
K F Y point so that who M J K so this is a binary matrix there

16
00:01:25,200 --> 00:01:30,660
is most one one in each row and column because you can't have multiple matches and

17
00:01:30,660 --> 00:01:36,020
the total of all of the entries is the number of matches and that's of course

18
00:01:36,020 --> 00:01:40,960
so unknownso this this M is a key objective inference this this is this provides

19
00:01:40,960 --> 00:01:47,680
the matching we want and we want a prior distribution for that and that's set up in this

20
00:01:47,680 --> 00:01:54,300
way we're going to suppose the hidden points which are you know our our creation

21
00:01:54,390 --> 00:02:01,040
it's our model I'm gonna suppose they form a homogeneous poisson process so that's the point secure completely

22
00:02:01,130 --> 00:02:09,260
random at a rate lambda per unit volume of a certain volume V and then we need

23
00:02:09,260 --> 00:02:13,580
a model for whether or not these hidden points are actually observed in X all

24
00:02:13,590 --> 00:02:19,280
Y or both and what we do then is set is set independently for each point

25
00:02:19,520 --> 00:02:25,880
that those hidden points can give rise to either no observations account

26
00:02:25,880 --> 00:02:30,000
be seen in either the data sets they can be seen only in X only in Y

27
00:02:30,000 --> 00:02:35,360
or in both so it's four possibilities so what we're thinking basically with each of the hidden

28
00:02:35,360 --> 00:02:42,660
point we're gonna through a four-sided coin to decide which of those possibilities happens and

29
00:02:42,660 --> 00:02:46,520
if we assume those two things together with a third point that all

30
00:02:46,540 --> 00:02:51,760
matching matrices with the same number of matches have the same probability and that's simply

31
00:02:51,760 --> 00:02:59,220
action exchangeability assumption saying I think equally about any ordering of the Xs or

32
00:02:59,220 --> 00:03:06,140
or the Ys I think is equally probable so under those three assumptions we we can construct simple probability

33
00:03:06,140 --> 00:03:12,080
theory you can construct a prior to the matching matrix and it has this geometric form which is slightly

34
00:03:12,160 --> 00:03:21,240
misleading cause of course there's a set different numbers of matrices with the same total match count so

35
00:03:21,240 --> 00:03:25,660
those are prior to M and if you I'm not going for this now cause I want to mention

36
00:03:25,660 --> 00:03:31,340
a little more detail on the slides about a detail how you work that out it basically corresponds to the

37
00:03:31,340 --> 00:03:39,240
to the well-known fact that if you randomly thin a poisson process you still get a poisson process I

38
00:03:39,240 --> 00:03:46,320
now we're gonna think about the data given the hidden points and this

39
00:03:46,320 --> 00:03:52,180
matching information what's the likelihood of the data itself you can you have to

40
00:03:52,180 --> 00:03:57,360
do this separately for each possible class of transformation so I'm gonna do need do a simple case

41
00:03:57,360 --> 00:04:07,900
for now I'm gonna take in fact throughout today I'm gonna take an affine transformation A is a norm singular matrix kept

42
00:04:08,010 --> 00:04:15,260
his rotation scaling and possibly shear and there's a translation as well and it turns out

43
00:04:15,330 --> 00:04:22,220
that under that that setting together with the the assumption that we've made about the poisson process

44
00:04:22,220 --> 00:04:28,080
for the hidden points it turns out the actual locations of the hidden points

45
00:04:28,080 --> 00:04:37,180
can also be integrated out okay you just you can just do these integrals analytically and

46
00:04:37,180 --> 00:04:42,400
very often and not absolutely always it's a good idea to remove variables analytically from

47
00:04:42,400 --> 00:04:47,230
your model if you can so that's a slow at the dimension of the space the

48
00:04:47,240 --> 00:04:53,800
things we don't know and then we have that fairly explicit form of of likelihood

49
00:04:53,810 --> 00:04:56,660
then again the slides is a bit more detail about that where

50
00:04:56,670 --> 00:05:01,920
that arises so in terms of this cartoon we took this picture before we observe the

51
00:05:01,920 --> 00:05:04,780
for a the other way

52
00:05:04,810 --> 00:05:06,190
that is the

53
00:05:07,100 --> 00:05:08,560
is feasible

54
00:05:08,570 --> 00:05:11,060
then we can prove it by

55
00:05:11,100 --> 00:05:13,850
describing a convex combination

56
00:05:13,850 --> 00:05:21,850
this well describe a finite set that i said earlier to had these characterizations we

57
00:05:21,870 --> 00:05:25,950
it's important that we have a easy description

58
00:05:25,960 --> 00:05:28,240
of the system of inequalities

59
00:05:28,250 --> 00:05:32,350
and we also have an easy description

60
00:05:32,360 --> 00:05:35,210
of a set of points whose whole

61
00:05:35,220 --> 00:05:37,220
is that so

62
00:05:37,250 --> 00:05:41,550
i think i neglected to mention we want an easy description of the point who's

63
00:05:41,550 --> 00:05:46,440
also by having both we have one

64
00:05:46,450 --> 00:05:49,200
kind of the characterization just described

65
00:05:49,230 --> 00:05:51,130
and of course we be

66
00:05:51,140 --> 00:05:52,830
there is

67
00:05:53,880 --> 00:05:56,220
we've got an axe

68
00:05:56,230 --> 00:05:57,650
we want to know

69
00:05:58,920 --> 00:06:00,600
does not

70
00:06:00,600 --> 00:06:03,530
maximize cx subject to that

71
00:06:03,590 --> 00:06:06,120
x px means

72
00:06:06,130 --> 00:06:09,270
cx does not maximize

73
00:06:09,280 --> 00:06:12,070
cx subject to that

74
00:06:12,090 --> 00:06:15,720
well it's a it's that's

75
00:06:15,810 --> 00:06:19,370
and that's in the knowledge that doesn't it is so one

76
00:06:19,380 --> 00:06:23,800
you show that you have shown exit make cx bigger

77
00:06:24,030 --> 00:06:28,320
but that and

78
00:06:28,340 --> 00:06:30,280
and then of course

79
00:06:30,300 --> 00:06:33,370
we have an easy description that enables us to

80
00:06:33,380 --> 00:06:35,210
that we're showing

81
00:06:35,220 --> 00:06:37,420
x bigger and still feasible

82
00:06:38,840 --> 00:06:44,320
and yes if the negation of the predicate is true that is that is actually

83
00:06:44,320 --> 00:06:46,750
does maximize cx

84
00:06:47,670 --> 00:06:52,980
because we have an easy description of these inequalities we can

85
00:06:53,030 --> 00:06:54,810
we can prove that

86
00:06:54,940 --> 00:06:58,460
x optimizes by displaying

87
00:06:58,480 --> 00:07:01,610
a dual solution

88
00:07:03,240 --> 00:07:07,950
well so i got this idea for a couple years before his photos

89
00:07:08,010 --> 00:07:09,730
papers that

90
00:07:09,740 --> 00:07:12,000
i just showed you get these ideas

91
00:07:12,340 --> 00:07:19,360
this this philosophy couple years before i had any examples of it all

92
00:07:19,420 --> 00:07:23,320
and i started looking for examples of it

93
00:07:23,360 --> 00:07:25,130
and the

94
00:07:25,140 --> 00:07:32,220
and submodularity polymatroids is is one of the things i found when i was explicitly

95
00:07:34,810 --> 00:07:38,240
examples of

96
00:07:44,850 --> 00:07:48,340
so let's see other places

97
00:07:48,420 --> 00:07:52,060
i recall automate radius

98
00:07:52,510 --> 00:07:58,270
by a scene outcomes from the columns of the matrix are

99
00:07:58,350 --> 00:08:01,530
finite set of vectors in a vector space

100
00:08:01,550 --> 00:08:05,720
you know you certainly learn in your first out of course

101
00:08:05,730 --> 00:08:09,970
and every for finite set of vectors in a vector space

102
00:08:09,980 --> 00:08:11,560
for matrix

103
00:08:11,570 --> 00:08:19,220
areas that called that americans every maximal linearly independent of columns is the same size

104
00:08:19,260 --> 00:08:23,470
and that's rather magic is and you know you do your girl seen that in

105
00:08:23,470 --> 00:08:26,930
any way at all and you end up with

106
00:08:26,940 --> 00:08:28,820
with the degree of zeros

107
00:08:28,830 --> 00:08:33,510
and so on that day but it always sends out the same number

108
00:08:34,130 --> 00:08:35,280
that it

109
00:08:36,220 --> 00:08:37,770
and this is called the

110
00:08:37,780 --> 00:08:40,550
this gives you the so-called rank of the matrix

111
00:08:40,600 --> 00:08:44,350
the largest linearly independent that well

112
00:08:44,380 --> 00:08:48,250
may try to simply an abstract statement of the same thing

113
00:08:48,260 --> 00:08:49,850
you've got to say

114
00:08:49,890 --> 00:08:53,980
the models the set of vectors

115
00:08:53,980 --> 00:08:57,830
and so called independent subsets

116
00:08:57,880 --> 00:08:59,050
so that

117
00:08:59,990 --> 00:09:03,000
any subset of the

118
00:09:03,010 --> 00:09:08,310
well first of course every subset the seventies and then but for every subset s

119
00:09:09,520 --> 00:09:12,270
every basis of st

120
00:09:12,350 --> 00:09:13,750
is the same size

121
00:09:13,770 --> 00:09:14,720
and so on

122
00:09:14,720 --> 00:09:18,280
every basis of s is the largest

123
00:09:18,340 --> 00:09:21,750
and then it's up that's and that's called the rank

124
00:09:21,800 --> 00:09:24,310
so there is a direct reflection

125
00:09:24,580 --> 00:09:26,370
i mean

126
00:09:27,710 --> 00:09:36,120
a linear algebra there now i don't like the existing books called matrix theory

127
00:09:36,220 --> 00:09:42,340
because this is a they stressed pushing snacks themes around

128
00:09:42,390 --> 00:09:48,480
there are close to this definition and

129
00:09:49,810 --> 00:09:55,720
winners and they try and given by over by matrix over a field

130
00:09:55,790 --> 00:10:02,390
and things like that all that i don't dislike that

131
00:10:02,390 --> 00:10:08,910
reach this point here and that gives you the final evaluation and here's the recursive definition

132
00:10:08,910 --> 00:10:18,450
of how you evaluate so the the zero row is set to one for

133
00:10:18,450 --> 00:10:23,730
M greater or equal to zero and then the X is equal to zero for

134
00:10:23,730 --> 00:10:28,410
the if your M is less than the you know if you haven't had a

135
00:10:28,410 --> 00:10:33,010
chance to get down that far because you've only seen X features then this

136
00:10:33,010 --> 00:10:37,250
must be zero all that corresponds to is  you know things out here can't

137
00:10:37,310 --> 00:10:46,910
be can't be zero nonzero and then there's a recursive way in which you evaluate the next

138
00:10:47,150 --> 00:10:54,110
sorry the next node here say from the values here and here so you can

139
00:10:54,110 --> 00:10:59,650
evaluate those by just adding the two paths that you can reach that vertex from the

140
00:10:59,650 --> 00:11:05,190
left or from below with the extra feature add it in and the final kernel is that

141
00:11:05,190 --> 00:11:10,610
bottom right-hand corner value here

142
00:11:10,730 --> 00:11:16,170
D is a user-specified number you know you say I'm interested in using up to

143
00:11:16,170 --> 00:11:23,750
find features from my hundred dimensional feature set and it will select all possible combinations

144
00:11:23,750 --> 00:11:32,090
of Y features so you might I mean you could of course if you wanted

145
00:11:32,090 --> 00:11:37,010
combine you know essentially reading off here you've got DD minus one up to so

146
00:11:37,090 --> 00:11:41,510
could combine those if you wanted to have a sort of spread of different combinations

147
00:11:41,510 --> 00:11:48,450
add these together and you get a kernel combining those different amounts of features that you

148
00:11:48,450 --> 00:11:49,870
might want to include

149
00:11:49,990 --> 00:11:54,070
so again you I'm just trying to give you this flavour that you could move away

150
00:11:54,070 --> 00:12:02,550
from sorry not moving away from microphone move away from these sort of very clean like

151
00:12:02,550 --> 00:12:07,150
the  polynomial kernel it's a slightly more complex combinations of features you can sort of sculpt

152
00:12:07,150 --> 00:12:13,350
your feature space to the particular knowledge you have of your particular feature set you

153
00:12:13,350 --> 00:12:17,550
may know that there are certain features that should be combined certain that shouldn't and you

154
00:12:17,550 --> 00:12:25,010
can set up graphs even basically define your own graph any directed acyclic graph

155
00:12:25,010 --> 00:12:31,110
in which you have a source and a sink and your edges are labelled with with

156
00:12:31,110 --> 00:12:39,310
your features you can define it and impose your understanding of the way the

157
00:12:39,310 --> 00:12:44,530
features should be combined essentially every path through that graph will correspond to a combined feature

158
00:12:44,710 --> 00:12:49,310
the product to the features on that path will make up that feature

159
00:12:49,310 --> 00:12:54,070
and every you know you just have to think of how all of those paths

160
00:12:54,080 --> 00:13:00,010
should be defined in order to create the feature space that you think captures what

161
00:13:00,010 --> 00:13:03,830
what your data's about now I you know this is coming to the the crux

162
00:13:03,830 --> 00:13:07,330
of kernel methods I mean it's sort of sounds too good to be true and

163
00:13:07,330 --> 00:13:11,670
of course in a way it is  it all comes back to defining the

164
00:13:11,670 --> 00:13:17,770
kernel you know you're essentially building in your prior knowledge of your data into the

165
00:13:17,770 --> 00:13:23,430
kernel in such a way that it will contain enough information for you to do

166
00:13:23,430 --> 00:13:29,210
the job you're interested in doing and and will  present it in the right way

167
00:13:29,210 --> 00:13:34,290
so at this point it would really be useful  and probably I should've actually

168
00:13:34,290 --> 00:13:39,770
described the the bayesian view of what a kernel is basically is a

169
00:13:39,770 --> 00:13:49,110
covariance function over the expected covariance over a a distribution over the possible functions and so

170
00:13:49,110 --> 00:13:54,110
you can see it is encoding a prior belief of the possible functions you might

171
00:13:54,110 --> 00:14:01,450
be choosing for your learner and so points that are close in a kernel

172
00:14:01,450 --> 00:14:09,450
space in the kernel measurement are points that are similarly classified or similarly have similar

173
00:14:09,450 --> 00:14:16,540
outputs in the or likely to have similar outputs in this distribution over functions so

174
00:14:16,540 --> 00:14:20,670
that's a way the bayesian view a kernel code as a covariance function I think

175
00:14:20,670 --> 00:14:25,830
it's a very useful way of viewing it when we come to thinking about defining

176
00:14:25,830 --> 00:14:31,570
kernel functions they are our way of encoding our prior knowledge about the likely functions we're

177
00:14:31,570 --> 00:14:40,970
going to see  in in in in our learning that said you know I

178
00:14:40,970 --> 00:14:46,550
think that it's often easier to think of them as a similarity measure and to

179
00:14:46,550 --> 00:14:51,290
define I mean you know by defining that similarity measure we are defining a

180
00:14:51,290 --> 00:14:57,130
prior over those functions but in a way if you're thinking from a user's point

181
00:14:57,130 --> 00:15:02,630
of view defining a similarity between inputs is something they can really relate to

182
00:15:02,630 --> 00:15:05,690
if you go back to neural net days you'd ask the user will how many

183
00:15:05,690 --> 00:15:10,850
hidden notes should I use you know that's you know he doesn't even have any

184
00:15:10,930 --> 00:15:16,110
interest in in in how your  classifier's constructed so here with sort of I

185
00:15:16,110 --> 00:15:25,190
think making a more user-friendly way of defining your prior knowledge about the particular

186
00:15:25,190 --> 00:15:30,050
domain but but it is not as friendly as it might be because part of

187
00:15:30,050 --> 00:15:36,110
it is having to define these ways of computing these efficiently but also having to

188
00:15:36,330 --> 00:15:42,850
make sure that the function satisfies this rather arcane property of positive semi definiteness so in

189
00:15:42,850 --> 00:15:48,050
the general case of acyclic graph we just need to have this numbering so that

190
00:15:48,050 --> 00:15:53,010
the it's compatible with the acyclicity in other words if U Y has an

191
00:15:53,010 --> 00:15:59,300
edge to U J then I should be less than J so the ordering is compatible with the acyclicity

192
00:15:59,310 --> 00:16:04,830
of the graph and then we just run this dynamic programming algorithm initialize these source

193
00:16:04,840 --> 00:16:12,450
node to one and this is the sort of iteration that node I you look at all

194
00:16:12,450 --> 00:16:17,470
the nodes before it that are connected to it put in the kernel on those

195
00:16:17,470 --> 00:16:22,050
nodes I mean this is you know you  could component kernels on your features

196
00:16:22,050 --> 00:16:26,700
does the berkeley passed through and the boundary passes through something like water and to

197
00:16:26,700 --> 00:16:30,870
the demise one boxes then we say that this is a d minus one dimensional

198
00:16:30,870 --> 00:16:34,200
curve or box higher dimension is d minus one

199
00:16:35,120 --> 00:16:40,530
so everybody understand that idea it's a way of characterizing the fact that the boundary

200
00:16:40,530 --> 00:16:45,680
is one dimension lower than the ambient feature space

201
00:16:45,700 --> 00:16:50,700
so it's a pretty mild assumption something probably holds in most practical problems

202
00:16:50,760 --> 00:16:52,470
and further refine

203
00:16:52,490 --> 00:16:58,890
the attention to such subsets of this d box the classes

204
00:16:58,910 --> 00:17:00,200
learning problems

205
00:17:00,240 --> 00:17:02,280
when you also index with can

206
00:17:02,330 --> 00:17:07,850
and kappa guinness's transitions from this we're showing people one where we have chance

207
00:17:07,870 --> 00:17:11,990
between the between the black and white area kappa

208
00:17:12,060 --> 00:17:15,850
greater than one where there are some sort of smoothness so kappa greater than one

209
00:17:15,850 --> 00:17:20,350
means we have some smooth derivatives saying that transition region

210
00:17:20,370 --> 00:17:25,180
and the smoothness of the transition region affects how well we can approximate

211
00:17:25,950 --> 00:17:27,820
decision boundary

212
00:17:27,830 --> 00:17:31,950
if we look at different resolutions over partitions so i'm just looking at histogram like

213
00:17:31,950 --> 00:17:37,950
partitions this is the approximation of that decision function the decision functions above if i

214
00:17:37,950 --> 00:17:38,890
look at

215
00:17:38,970 --> 00:17:43,550
resolution of two the four to the minus five into the nineteen sixty you see

216
00:17:43,550 --> 00:17:49,080
as we increase resolution we get better and better approximation to the bayes decision boundary

217
00:17:49,120 --> 00:17:54,010
and what happens is if you actually look at the difference between the probability of

218
00:17:54,010 --> 00:17:58,350
error of approximation of the bayes decision boundary it will take a liking to the

219
00:17:58,350 --> 00:18:03,300
minus cap so the larger kappa is the more smooth transition is the more we

220
00:18:03,300 --> 00:18:08,180
can be kind crude rough about we're reporting are approximated boundary and will still get

221
00:18:08,200 --> 00:18:11,010
small difference between the probability of air

222
00:18:11,030 --> 00:18:13,140
so that they everybody

223
00:18:13,180 --> 00:18:14,430
so this and to

224
00:18:14,450 --> 00:18:19,930
kappa decay approximation something that's important in world war i use that later when we

225
00:18:19,930 --> 00:18:23,720
look at some of these rates of convergence but the main ideas again as kappa

226
00:18:23,720 --> 00:18:26,680
gets larger i can be more sloppy so for example

227
00:18:26,740 --> 00:18:29,320
if i had equal to say

228
00:18:29,330 --> 00:18:31,320
which is maybe what this case notes

229
00:18:31,320 --> 00:18:35,850
the difference between this approximation and the idea of bayes classification

230
00:18:35,910 --> 00:18:40,080
boundary which would look like the curve up there would be smaller

231
00:18:40,140 --> 00:18:46,470
we have is equal to the main campus is one

232
00:18:46,490 --> 00:18:51,580
so the way that you approach this problem is using something we call dyadic decision

233
00:18:51,580 --> 00:18:55,550
trees and i just want to be clear about what we're talking about we have

234
00:18:55,550 --> 00:18:57,160
labelled training data

235
00:18:57,160 --> 00:19:00,280
there are some bayes decision boundary that would like to learn

236
00:19:00,330 --> 00:19:04,260
we first small partition the space into lots of tiny boxes

237
00:19:04,260 --> 00:19:09,180
and then we prune that partition that based on the data to obtain some approximation

238
00:19:09,180 --> 00:19:13,330
to the bayes decision boundary and the main the reason that this is called a

239
00:19:13,330 --> 00:19:20,140
dyadic tree is because although the cells have dyadic sizes size like one by one

240
00:19:20,270 --> 00:19:22,080
and one-half i one half

241
00:19:22,140 --> 00:19:24,830
one four five one four and so on and so forth so we're just doing

242
00:19:25,100 --> 00:19:30,600
dyadic cuts partition of this feature space we can dyadic partition

243
00:19:30,640 --> 00:19:31,760
two very fine

244
00:19:31,800 --> 00:19:33,990
level and then we can prune back to

245
00:19:34,260 --> 00:19:36,370
something adapted show here

246
00:19:36,370 --> 00:19:39,820
and everything that will be talking about the rest of this talk is joint work

247
00:19:39,820 --> 00:19:43,160
with my former student clay scott

248
00:19:43,220 --> 00:19:45,330
so the partition

249
00:19:45,410 --> 00:19:48,740
we would take majority vote on each cell that gives us the decision trees i

250
00:19:48,740 --> 00:19:50,660
talked about before

251
00:19:50,680 --> 00:19:55,780
OK so what time try to walk you through is the following

252
00:19:55,800 --> 00:19:59,990
if we look at this class of problems that i was considering

253
00:20:00,660 --> 00:20:05,700
in i mean focus on the case where you have question back there

254
00:20:05,780 --> 00:20:08,260
and so on

255
00:20:08,260 --> 00:20:10,180
why is binary

256
00:20:14,510 --> 00:20:17,760
what we actually do is we divide

257
00:20:17,780 --> 00:20:23,080
one coordinate time in practice and you can either do that in a cyclic fashion

258
00:20:23,080 --> 00:20:28,800
ranges serve cycle through coordinates or you can do it more in you even more

259
00:20:28,800 --> 00:20:33,030
flexibility where you choose which one axis to split on each time so we really

260
00:20:33,030 --> 00:20:37,350
are just using binary trees as opposed to larger is when we do this and

261
00:20:37,350 --> 00:20:41,210
that one of the reasons for that gives us more adaptability and it also allows

262
00:20:41,210 --> 00:20:45,170
us to tackle higher dimensional space is divided the dimensional space and i was doing

263
00:20:45,170 --> 00:20:46,840
a d dimensional split

264
00:20:46,910 --> 00:20:50,660
i go from one box into two to the d boxes and of course to

265
00:20:50,670 --> 00:20:51,880
deal with ten

266
00:20:51,880 --> 00:20:52,660
that would be

267
00:20:52,670 --> 00:20:56,660
pretty problematic so we split one point access is that at the time so i

268
00:20:56,660 --> 00:21:01,840
was really thinking about binary trees so the progression things

269
00:21:01,870 --> 00:21:05,750
so i'm going to focus on the case where the boundary smoothness is is equal

270
00:21:05,750 --> 00:21:11,060
to one and also more about that later it's tricky problem to handle smoother boundaries

271
00:21:12,340 --> 00:21:14,160
don't worry about it right now

272
00:21:14,180 --> 00:21:18,380
kappa is the transition smoothness will throw either be from a job at that level

273
00:21:18,380 --> 00:21:21,130
set one half to some smooth transition

274
00:21:21,130 --> 00:21:25,280
and d is the dimension of the problem so for these situations we know that

275
00:21:25,280 --> 00:21:28,870
the rate at which we can learn as a function of the number of data

276
00:21:28,870 --> 00:21:33,460
can decay faster than and to them cap over two kappa let's d minus two

277
00:21:33,460 --> 00:21:35,700
that's the lower bound we

278
00:21:35,700 --> 00:21:40,430
no from the theory was developed by them in same coffin what i'll show is

279
00:21:40,430 --> 00:21:46,320
that by using these dyadic decision trees and with appropriate pruning scheme we can actually

280
00:21:46,320 --> 00:21:48,890
get a similar error decay rate

281
00:21:48,920 --> 00:21:55,790
within a logarithmic factor just another little notational practice quickly lesson equals means i'm ignoring

282
00:21:55,790 --> 00:21:59,880
the constants that might be from just looking at the rate of convergence as a

283
00:21:59,880 --> 00:22:02,670
function of the training set size

284
00:22:02,760 --> 00:22:07,410
so that's what i mean try to explain to you in the next

285
00:22:07,530 --> 00:22:10,010
forty minutes or so

286
00:22:10,030 --> 00:22:11,180
OK so

287
00:22:11,180 --> 00:22:14,170
well the drug was introduced to them

288
00:22:17,610 --> 00:22:22,900
drug users from the previous year and so on and so we we had this

289
00:22:22,910 --> 00:22:26,780
for five years and of course we can begin to do

290
00:22:26,780 --> 00:22:28,460
a lot of analysis

291
00:22:28,540 --> 00:22:35,870
well in by could we have two different possibilities of how to describe those temporal

292
00:22:35,870 --> 00:22:39,060
networks because

293
00:22:39,090 --> 00:22:44,250
one time the one is more suitable any other time the second is more suitable

294
00:22:44,250 --> 00:22:46,050
for example if we look at this one

295
00:22:46,050 --> 00:22:52,420
we have three vertices and for vertex a for example is present at timepoint tend

296
00:22:52,420 --> 00:22:58,130
to timepoint time point five to ten point ten and then into timepoints eleven it's

297
00:22:58,130 --> 00:23:04,130
not present but it's still it's again present from time point fourteen twenty twelve two

298
00:23:04,130 --> 00:23:05,630
time points fourteen

299
00:23:05,650 --> 00:23:11,660
this one here means that e is present for from time point four

300
00:23:11,670 --> 00:23:14,950
to the end of time points

301
00:23:14,950 --> 00:23:16,520
and four edges

302
00:23:16,540 --> 00:23:20,970
cause this one or this one is present just in time point seven but we

303
00:23:20,970 --> 00:23:25,450
have to be careful that link is present only if both and vertices this is

304
00:23:25,530 --> 00:23:29,660
also present by also controls for that

305
00:23:31,110 --> 00:23:34,800
if we have the second description this would be

306
00:23:34,800 --> 00:23:40,990
with events well here we have a table of all the events that are possible

307
00:23:41,000 --> 00:23:45,570
and how we can well then

308
00:23:45,600 --> 00:23:51,800
this is the description of this temporal network for example we start with time point

309
00:23:51,800 --> 00:23:55,560
one week and we add vertex two

310
00:23:55,590 --> 00:23:56,980
we a b

311
00:23:56,980 --> 00:24:00,070
then the time point three and

312
00:24:00,090 --> 00:24:02,030
and we hide vertex two

313
00:24:02,030 --> 00:24:04,290
that point four star

314
00:24:04,340 --> 00:24:06,000
we add more vertex three

315
00:24:06,000 --> 00:24:11,310
that that has name e and the time point five starts

316
00:24:11,320 --> 00:24:13,400
we and vertex one

317
00:24:13,460 --> 00:24:20,050
with the navy and so on course or this for example here we delete edge

318
00:24:20,760 --> 00:24:24,730
vertex one to vertex a vertex two and so on this how

319
00:24:24,750 --> 00:24:28,340
he goes OK well i have also

320
00:24:28,410 --> 00:24:33,640
one example that we can looking pi

321
00:24:33,660 --> 00:24:36,320
and we're going to his friends

322
00:24:36,320 --> 00:24:38,260
don team network what

323
00:24:39,310 --> 00:24:41,350
so we go to far

324
00:24:41,370 --> 00:24:44,910
time events networks read

325
00:24:44,920 --> 00:24:47,970
we have this team

326
00:24:48,010 --> 00:24:50,240
OK we opened it

327
00:24:50,250 --> 00:24:54,870
and we get this our network in this

328
00:24:54,890 --> 00:24:57,180
in this network

329
00:24:57,200 --> 00:24:58,970
part of the pipe

330
00:25:00,430 --> 00:25:02,080
of course

331
00:25:02,100 --> 00:25:06,320
what what we have to do is still if we just draw this

332
00:25:06,370 --> 00:25:07,760
this network

333
00:25:07,830 --> 00:25:10,700
we get all the lines and in

334
00:25:10,830 --> 00:25:12,540
that are in the

335
00:25:12,580 --> 00:25:18,060
the network at any of the time steps so what we have to do

336
00:25:18,070 --> 00:25:21,250
is we have to generate

337
00:25:22,030 --> 00:25:23,110
the network

338
00:25:23,970 --> 00:25:25,220
time steps

339
00:25:25,240 --> 00:25:28,920
and we start from time one to time step

340
00:25:28,950 --> 00:25:32,030
eleven because i know that there are just too many times

341
00:25:32,050 --> 00:25:34,730
and we're going to go back to to step one

342
00:25:34,740 --> 00:25:36,990
so we will get

343
00:25:37,010 --> 00:25:38,050
now we get

344
00:25:38,910 --> 00:25:41,120
different networks

345
00:25:41,130 --> 00:25:45,560
and if we just go to the first network

346
00:25:45,560 --> 00:25:47,630
we see that we just had

347
00:25:47,650 --> 00:25:49,830
the vertices so in the first

348
00:25:49,880 --> 00:25:52,740
in the first time step there just

349
00:25:52,810 --> 00:25:56,100
eight vertices and three

350
00:25:56,120 --> 00:25:57,260
go next

351
00:25:57,260 --> 00:25:58,870
in the next time step

352
00:25:59,910 --> 00:26:02,910
more well there's means in this

353
00:26:04,630 --> 00:26:07,040
network and then this

354
00:26:07,040 --> 00:26:10,620
start to change well what's the difference between the

355
00:26:10,640 --> 00:26:12,760
black and blue lines

356
00:26:12,800 --> 00:26:18,560
the blue line describes an edge and the black is an art

357
00:26:18,620 --> 00:26:22,630
OK so that's how we go

358
00:26:22,640 --> 00:26:24,610
tell time

359
00:26:24,660 --> 00:26:28,570
eleven which is the last one

360
00:26:34,730 --> 00:26:36,260
edge we

361
00:26:36,300 --> 00:26:40,060
and and are the leagues

362
00:26:40,110 --> 00:26:43,590
sorry i don't i i

363
00:26:43,590 --> 00:26:45,650
it contains for a

364
00:26:47,950 --> 00:26:50,840
so once you try this disguise

365
00:26:51,090 --> 00:26:57,110
you can compute some statistics and we will need some notation you are to be

366
00:26:57,110 --> 00:26:58,910
able to do that

367
00:26:59,000 --> 00:27:04,300
so in all the time timestamp mind you have tried

368
00:27:04,310 --> 00:27:09,110
the arms eight times the number of times we have tried

369
00:27:09,130 --> 00:27:12,740
an online after this time for

370
00:27:12,780 --> 00:27:14,890
so this capital t one

371
00:27:14,910 --> 00:27:17,620
at t minus one

372
00:27:17,630 --> 00:27:18,650
so four

373
00:27:18,660 --> 00:27:20,840
this the same for two

374
00:27:20,840 --> 00:27:21,880
and so

375
00:27:21,940 --> 00:27:26,290
our choices are going to to be made by carefully one can two and so

376
00:27:31,090 --> 00:27:33,980
what the because there an extant right

377
00:27:33,990 --> 00:27:36,330
we have tried one two three four

378
00:27:40,490 --> 00:27:45,120
so far relating the program you define this loss so was this

379
00:27:45,140 --> 00:27:48,580
so you could check if you could take the optimal are don't know which which

380
00:27:48,580 --> 00:27:52,660
one of these two answers to optimize denote its indexed by k such so that's

381
00:27:52,660 --> 00:27:54,350
an unknown quantity

382
00:27:56,040 --> 00:27:58,550
what you want to compare this

383
00:27:58,600 --> 00:27:59,530
is the

384
00:27:59,530 --> 00:28:01,730
support turn

385
00:28:01,750 --> 00:28:03,880
received that are

386
00:28:04,580 --> 00:28:07,260
imagine that you have for this

387
00:28:07,300 --> 00:28:11,530
during the first cabinet he tries so what would be the return receipt so that

388
00:28:11,580 --> 00:28:13,100
this one here

389
00:28:13,200 --> 00:28:15,080
so this is our baseline

390
00:28:15,080 --> 00:28:17,390
and what do you get

391
00:28:17,400 --> 00:28:19,370
this quantity here so

392
00:28:19,380 --> 00:28:21,420
at times the

393
00:28:21,440 --> 00:28:24,250
he you've chosen a after he

394
00:28:24,260 --> 00:28:28,410
and this is the number of times that are was chosen

395
00:28:28,470 --> 00:28:33,070
so far and so this is really what you are going to receive the funds

396
00:28:33,780 --> 00:28:36,090
you compute your

397
00:28:36,390 --> 00:28:38,030
the actual return

398
00:28:38,160 --> 00:28:41,870
and compare it with the proper that returned that you could have received if you

399
00:28:42,930 --> 00:28:44,230
the optimal

400
00:28:44,240 --> 00:28:54,470
are from the first times that

401
00:28:54,610 --> 00:28:57,370
minimizes the grand prize

402
00:28:57,390 --> 00:28:58,930
the scale

403
00:29:13,480 --> 00:29:19,170
these are on the road we're going to take expectations and that takes care

404
00:29:20,640 --> 00:29:23,690
you can compute the high profile

405
00:29:24,730 --> 00:29:27,890
this is the loss that we want to control

406
00:29:28,540 --> 00:29:32,740
so we can conclude this loss on in retrospect so once to learned

407
00:29:32,790 --> 00:29:37,610
the identity of the right so then then you could compute the laws but nevertheless

408
00:29:37,610 --> 00:29:39,850
we want to design and target and that

409
00:29:39,870 --> 00:29:41,530
that makes this laws

410
00:29:41,540 --> 00:29:45,930
not increase too fast

411
00:29:45,950 --> 00:29:49,330
right so i think about it it's like that not dividing by

412
00:29:49,340 --> 00:29:53,030
by capital so this is not the average loss per time step so the average

413
00:29:53,030 --> 00:29:57,830
loss per timestep should idea decreased is towards the right

414
00:29:57,840 --> 00:29:59,590
but if you're not dividing by

415
00:29:59,620 --> 00:30:03,050
cavity then this loss is going to grow

416
00:30:03,060 --> 00:30:04,930
maybe at a slower rate

417
00:30:05,030 --> 00:30:09,270
but there is nothing that would prevent it to grow

418
00:30:09,280 --> 00:30:13,500
so what it would mean that if this loss would stop growing

419
00:30:13,510 --> 00:30:17,190
it would mean that after a certain number of times that

420
00:30:17,220 --> 00:30:21,240
this hundred person accuracy you have identified the optimal are

421
00:30:21,380 --> 00:30:22,380
you do that

422
00:30:22,400 --> 00:30:23,890
maybe not

423
00:30:23,900 --> 00:30:28,070
OK so internet this loss is going to grow his capability

424
00:30:28,150 --> 00:30:29,870
and what you want to do

425
00:30:29,890 --> 00:30:32,750
is to limit the rate of growth

426
00:30:34,150 --> 00:30:35,840
so what's a good rate

427
00:30:35,840 --> 00:30:41,260
something that sublinear in t something that grows smaller and g

428
00:30:43,310 --> 00:30:47,270
because if if you have such loss then if you divide by t and

429
00:30:47,290 --> 00:30:50,180
the loss per time step is decreasing prices

430
00:30:50,200 --> 00:30:52,440
and this is what want

431
00:30:53,980 --> 00:30:55,490
very simple idea

432
00:30:55,500 --> 00:30:59,040
it is to compute the action values

433
00:30:59,060 --> 00:31:00,780
for the arms

434
00:31:00,830 --> 00:31:01,880
that you have

435
00:31:01,940 --> 00:31:05,760
just to pick the best so that would be exploitation all the time

436
00:31:05,840 --> 00:31:15,910
and why is this a bad idea

437
00:31:30,530 --> 00:31:33,840
what is the a certain it can happen if you are not

438
00:31:36,390 --> 00:31:38,080
what is that is a star

439
00:31:38,080 --> 00:31:42,500
that can happen

440
00:31:46,120 --> 00:31:47,380
exactly so

441
00:31:47,400 --> 00:31:52,740
you could have you can be unlucky because of similar at the beginning right

442
00:31:52,780 --> 00:31:55,720
if you're lucky is the optimal arm to estimate the

443
00:31:55,730 --> 00:31:59,170
its value is less than the value of the other and so if you just

444
00:31:59,170 --> 00:32:00,820
exploit all the time

445
00:32:00,830 --> 00:32:04,760
we are going to stick is that are which is the salt of our and

446
00:32:04,760 --> 00:32:06,940
your loss is going to grow in our

447
00:32:06,970 --> 00:32:11,640
so you want to explore

448
00:32:14,240 --> 00:32:15,480
in many ways

449
00:32:15,490 --> 00:32:19,300
most of these methods what they do is they just compute the action that all

450
00:32:19,300 --> 00:32:23,530
the computer action bias value basically take average

451
00:32:23,530 --> 00:32:28,500
standard planning computational tools to actually use policy search reinforcement learning inside

452
00:32:28,550 --> 00:32:33,550
the box so here again expert prioritize the space of possible policies how do you

453
00:32:33,550 --> 00:32:36,290
want to say in response to this kind of state how do you want to

454
00:32:36,290 --> 00:32:41,420
send control action and then there's an offline search just goes on and on

455
00:32:41,450 --> 00:32:46,340
some building some places grinds away and finds an excellent policy for the model not

456
00:32:46,340 --> 00:32:49,310
for the real world but the model close enough to the real world so that

457
00:32:49,310 --> 00:32:52,970
when they take this policy actually put it in a helicopter well

458
00:32:52,990 --> 00:32:55,590
i have to show you if you think a lot of people have seen these

459
00:32:55,590 --> 00:32:57,370
before i apologize but

460
00:32:57,380 --> 00:33:02,880
there there wicked call bear watching multiple times

461
00:33:02,880 --> 00:33:06,580
so there's a helicopter on the ground making lots of dust in the second test

462
00:33:06,580 --> 00:33:08,160
at this point that's taking off the

463
00:33:08,700 --> 00:33:09,630
upside down

464
00:33:09,640 --> 00:33:13,220
right now it's about flying

465
00:33:13,240 --> 00:33:16,710
this can have an around i think even that hard but compared to these other

466
00:33:16,710 --> 00:33:18,830
stuff doesn't look are so

467
00:33:18,840 --> 00:33:25,450
first start

468
00:33:25,460 --> 00:33:29,740
he so-called split so it's diving and then twisting in diving

469
00:33:29,750 --> 00:33:31,210
and then going around the other way

470
00:33:31,210 --> 00:33:34,750
and then going upside-down that's still starkest alterna

471
00:33:35,700 --> 00:33:37,160
the helicopter

472
00:33:37,170 --> 00:33:38,830
that's supposed to be able to do that

473
00:33:38,850 --> 00:33:42,010
there are there are people that can do the stunts but they didn't that's not

474
00:33:42,010 --> 00:33:43,090
how they trained

475
00:33:43,120 --> 00:33:46,880
big just train the flying normal and then maybe little upside down i guess and

476
00:33:48,350 --> 00:33:51,880
told what the what they want distance to look like that was actually falling backwards

477
00:33:51,880 --> 00:33:52,990
tail first

478
00:33:53,100 --> 00:33:56,330
it's now flying backwards in the inner loop

479
00:33:58,220 --> 00:33:59,210
this crazy

480
00:33:59,210 --> 00:34:02,930
i don't want to be in and out of this model helicopters by the way

481
00:34:02,930 --> 00:34:05,750
so these are these are like you know you a big appeal big enough that

482
00:34:05,750 --> 00:34:08,450
you want to get hit by the blades but not big enough that you have

483
00:34:08,450 --> 00:34:10,250
any threat of writing it

484
00:34:10,560 --> 00:34:13,960
stationary roles it's just sitting there flipping over on its back

485
00:34:15,210 --> 00:34:16,200
and flips so

486
00:34:16,210 --> 00:34:18,910
so tailor overhead head of retail

487
00:34:18,960 --> 00:34:21,120
one not moving otherwise

488
00:34:21,120 --> 00:34:24,120
the tax it's acting like a pendulum

489
00:34:27,050 --> 00:34:30,950
what you know

490
00:34:30,960 --> 00:34:33,380
no they told to do this

491
00:34:33,380 --> 00:34:35,120
twenty minutes

492
00:34:37,380 --> 00:34:40,720
why does it never fall down

493
00:34:45,210 --> 00:34:49,880
it's throwing itself in the ground and missing no idea why does it never fall

494
00:34:49,880 --> 00:34:54,540
down because it's it's controlling itself right so it's it's very carefully adjusting the angle

495
00:34:54,540 --> 00:34:58,200
of the rotor is the speed of the back the speed of the top thing

496
00:34:58,200 --> 00:34:59,870
so that it's it's

497
00:34:59,970 --> 00:35:02,960
fuselage goes through that series of positions

498
00:35:02,990 --> 00:35:06,050
so it's very carefully flying

499
00:35:06,090 --> 00:35:10,830
so those purely because there's been a

500
00:35:10,840 --> 00:35:11,950
the estimate the

501
00:35:11,960 --> 00:35:13,810
the this is

502
00:35:13,830 --> 00:35:17,390
eight of the high

503
00:35:17,410 --> 00:35:20,920
you know so it knows yes that's right it's trying to it when i was

504
00:35:20,920 --> 00:35:24,470
told to do the stationary roll it with the goal of the way that the

505
00:35:24,470 --> 00:35:28,330
reward function was specified is you should stay at all

506
00:35:28,350 --> 00:35:31,330
you're delta altitude should be close to zero

507
00:35:31,330 --> 00:35:35,800
but your feet should be where head is and then the other one so they

508
00:35:35,800 --> 00:35:37,960
specify the reward function that way and then

509
00:35:37,970 --> 00:35:42,180
through the control policy has to figure out what controls are going to cause me

510
00:35:42,180 --> 00:35:45,050
to go through that series of

511
00:35:46,250 --> 00:35:50,080
so it's it's pretty wacky

512
00:35:50,090 --> 00:35:56,430
they were the what they are i mean they have to have pretty good sensors

513
00:35:56,430 --> 00:36:00,750
on it so they can detect position pretty well and

514
00:36:00,760 --> 00:36:04,510
o yeah they treated completely as an MDP not upon the first when you said

515
00:36:04,510 --> 00:36:08,410
how noisy was michael the people are wearing headphones some pretty sure it's loud

516
00:36:09,060 --> 00:36:14,170
yes they know that this is this work is all done assuming that the state

517
00:36:14,170 --> 00:36:16,310
information is accurate

518
00:36:16,340 --> 00:36:20,550
right so that was that was pretty impressive here's some other some other core results

519
00:36:20,550 --> 00:36:24,770
just kind of powering through here you can also combine

520
00:36:24,800 --> 00:36:28,640
this is again the sort of nested idea where you learn model and then based

521
00:36:28,640 --> 00:36:32,630
on the model you learn value function based on the value function you act so

522
00:36:33,210 --> 00:36:34,660
this is some work

523
00:36:34,660 --> 00:36:47,180
like to thank the organisers or is this i added to the particle momentum modeling

524
00:36:47,180 --> 00:36:48,730
and applications

525
00:36:48,740 --> 00:36:50,640
meaning that

526
00:36:50,690 --> 00:36:55,870
i will say much about theory of convex optimization and not about the word for

527
00:36:55,880 --> 00:36:59,230
solving convex optimisation problem

528
00:36:59,280 --> 00:37:04,920
and this is based on work with stephen boyd this book and courses that have

529
00:37:04,920 --> 00:37:10,650
been teaching at UCLA and and

530
00:37:10,690 --> 00:37:15,470
so the the the the three lecture the first of those being introduction some background

531
00:37:15,470 --> 00:37:18,270
on convex sets and functions

532
00:37:18,280 --> 00:37:24,740
part two the talk about the standard problem classes and some recent problem classes semidefinite

533
00:37:24,740 --> 00:37:29,910
programming second order cone programming have been studied recently

534
00:37:29,950 --> 00:37:35,410
and then the third lecture you look at applications and three

535
00:37:35,430 --> 00:37:38,470
classes of applications

536
00:37:38,520 --> 00:37:43,970
been the focus of most of the recent interest in light of the decision problem

537
00:37:44,070 --> 00:37:46,250
three of the

538
00:37:46,270 --> 00:37:48,090
areas where people

539
00:37:48,100 --> 00:37:50,810
i have been very active

540
00:37:53,510 --> 00:37:57,830
so start with an introduction on

541
00:37:57,850 --> 00:38:00,410
the power lectures

542
00:38:01,870 --> 00:38:09,020
be talking about mathematical optimisation problems minimizing nonlinear cost function subject to linear equality inequality

543
00:38:09,020 --> 00:38:14,120
constraints vector variable x

544
00:38:14,130 --> 00:38:20,040
and that's the general framework of the nonlinear optimization problem so obviously it's very useful

545
00:38:21,960 --> 00:38:26,240
throughout engineering statistics and

546
00:38:26,250 --> 00:38:28,850
estimation numerical analysis

547
00:38:29,410 --> 00:38:33,770
and the most important thing to to note first that this problem is actually extremely

548
00:38:33,770 --> 00:38:35,310
hard in general

549
00:38:35,370 --> 00:38:41,360
the complex is very much dependent on the properties of the function involved

550
00:38:41,400 --> 00:38:44,360
inequality and ecological function

551
00:38:44,410 --> 00:38:45,620
and actually

552
00:38:45,670 --> 00:38:52,830
that's what i think textbooks optimistic optimisation iterative scanning

553
00:38:52,930 --> 00:38:57,450
we don't have good methods for solving the the general linear optimisation problem

554
00:38:57,500 --> 00:39:01,660
so that means in practice you have to make some compromises

555
00:39:01,670 --> 00:39:06,210
and usually that means that you resort to local optimisation

556
00:39:06,260 --> 00:39:11,350
which can be done efficiently but you find only in suboptimal local solution to the

557
00:39:12,650 --> 00:39:19,830
unless you're willing to do global optimisation for most applications that do

558
00:39:19,850 --> 00:39:22,660
now there are important exceptions

559
00:39:22,730 --> 00:39:25,610
there that's the situation is much better

560
00:39:25,630 --> 00:39:29,620
there you can find the global optimum is very reliably

561
00:39:29,670 --> 00:39:34,490
and the best known exceptions only scratching the programming

562
00:39:37,200 --> 00:39:43,130
also convex optimisation is actually more less the most general class of problems that can

563
00:39:45,700 --> 00:39:51,710
and that's something that people didn't appreciate very much until the last fifteen years

564
00:39:51,750 --> 00:39:54,990
the full expression in a programming this has been very well known for a long

565
00:39:55,920 --> 00:39:57,860
convex optimisation problems

566
00:39:57,910 --> 00:40:01,630
it's more recent

567
00:40:01,640 --> 00:40:05,310
so let's take something about these linear programming

568
00:40:06,070 --> 00:40:09,760
contrast that with a convex optimisation

569
00:40:09,780 --> 00:40:13,680
so what you are familiar with these squares of minimising the norm of x minus

570
00:40:13,680 --> 00:40:16,080
the euclidean norm

571
00:40:16,130 --> 00:40:17,470
and as a user

572
00:40:17,850 --> 00:40:20,570
of these stress

573
00:40:20,580 --> 00:40:21,610
in practice

574
00:40:21,630 --> 00:40:24,940
the most important things to note probably the only thing you need to know what

575
00:40:24,970 --> 00:40:26,070
the user

576
00:40:26,120 --> 00:40:30,060
is that there is a formula for an analytic solution for the solution of the

577
00:40:31,220 --> 00:40:35,540
the rank of a it's for people the number of columns

578
00:40:35,560 --> 00:40:43,560
it can be solved efficiently the very good model software just three took back in

579
00:40:43,740 --> 00:40:48,540
the solution is really no need to know how the solution is obtained

580
00:40:48,560 --> 00:40:54,100
and the computation time is linear it's polynomial is linear in the number of rows

581
00:40:54,100 --> 00:40:58,700
of a and about in the column

582
00:40:58,750 --> 00:41:00,680
and that's

583
00:41:00,700 --> 00:41:05,040
very efficient and for senior large a you need to look into you may need

584
00:41:05,040 --> 00:41:11,720
to look into exploiting structure to do it more efficiently for most problems is sufficient

585
00:41:11,740 --> 00:41:16,140
and if you want to use the best in practice formerly practical problem is the

586
00:41:16,150 --> 00:41:17,640
scratch problem

587
00:41:17,660 --> 00:41:23,910
then there are actually very easy to recognise because it's also very specialized problem minimizing

588
00:41:23,910 --> 00:41:26,790
the sum of squares of linear functions of x

589
00:41:26,810 --> 00:41:28,650
take an extremely

590
00:41:28,700 --> 00:41:31,020
specific class of problems

591
00:41:31,030 --> 00:41:33,730
but they are also easy to recognise

592
00:41:33,780 --> 00:41:38,870
people have a few standard tricks to increase the flexibility

593
00:41:38,880 --> 00:41:40,490
for example you can handle

594
00:41:40,500 --> 00:41:44,630
constraints to some extent by adding weight

595
00:41:44,650 --> 00:41:48,210
and the quadratic penalty on the interaction

596
00:41:48,260 --> 00:41:51,550
by solving a weighted least squares problem you can

597
00:41:51,690 --> 00:41:55,250
handle constraints to some extent

598
00:41:55,260 --> 00:42:02,030
but it's basically very limited and only a few a few standard techniques that increase

599
00:42:03,610 --> 00:42:10,270
applications a little bit

600
00:42:10,320 --> 00:42:15,600
linear programming is this problem that we minimize the objective function linear in subject to

601
00:42:15,600 --> 00:42:17,500
linear inequality constraints

602
00:42:17,520 --> 00:42:20,350
and they have the same

603
00:42:20,400 --> 00:42:21,170
kind of

604
00:42:21,250 --> 00:42:25,400
properties least stress the only difference is that there is no analytical formula for the

605
00:42:26,750 --> 00:42:30,500
but that doesn't make a big difference in practice because you solve this problem by

606
00:42:30,500 --> 00:42:36,490
computer anyway so the fact that you have formula for the solution is really

607
00:42:36,540 --> 00:42:38,220
that much

608
00:42:38,330 --> 00:42:44,210
there is also very easy very easily obtained and reliable software

609
00:42:44,220 --> 00:42:47,300
and computation is roughly proportional to

610
00:42:47,310 --> 00:42:48,680
and square m

611
00:42:48,700 --> 00:42:52,330
and n is the number of variables and m is the number of inequality

612
00:42:52,390 --> 00:42:56,620
basically that the cost of an interior point method because of one iteration in the

613
00:42:56,680 --> 00:42:57,860
point that it would be

614
00:42:57,910 --> 00:42:59,700
and square

615
00:42:59,770 --> 00:43:01,440
and the number of iterations

616
00:43:01,630 --> 00:43:04,470
roughly constant as a function of and

617
00:43:04,620 --> 00:43:09,290
get this for dense problem

618
00:43:09,360 --> 00:43:12,310
so again as a user that's all we need to know about the algorithms for

619
00:43:13,410 --> 00:43:18,570
and then if you want to pose a practical problem as a linear programming problem

620
00:43:18,640 --> 00:43:22,440
then there is a little bit harder to recognise the least first problems because problems

621
00:43:22,540 --> 00:43:27,360
always come in the form you need to do some of you some reformulation of

622
00:43:27,510 --> 00:43:31,150
information to get exactly in standard form

623
00:43:31,190 --> 00:43:36,920
but after a few examples you know all the tricks people use to

624
00:43:36,940 --> 00:43:38,430
four minutes

625
00:43:38,480 --> 00:43:41,420
problem as a linear programming problem

626
00:43:41,430 --> 00:43:45,320
and after a few examples that come very easy to recognise

627
00:43:45,370 --> 00:43:49,750
so for example typical example that give is actually piecewise linear function minimizing piecewise linear

628
00:43:49,750 --> 00:43:51,410
function of x

629
00:43:51,430 --> 00:43:54,790
it is a problem that can be solved NLP because it's easily written as an

630
00:43:54,790 --> 00:43:56,910
LP can be solved

631
00:43:57,100 --> 00:43:58,750
the programming problem

632
00:43:58,830 --> 00:44:03,020
other problems involving one or infinity norm

633
00:44:03,070 --> 00:44:08,110
it can also be written as a linear programming problem

634
00:44:08,150 --> 00:44:12,510
and finally for a convex optimization problem is so we say much more about this

635
00:44:12,510 --> 00:44:15,380
this is not actually a very good explanation of the data because

636
00:44:15,400 --> 00:44:20,560
in something happened here which according to the model should happen with almost zero probability

637
00:44:20,810 --> 00:44:22,460
it did happen

638
00:44:22,470 --> 00:44:24,320
that seems a little odd

639
00:44:24,460 --> 00:44:29,320
the probability of the data under the scouts in isn't very good

640
00:44:29,870 --> 00:44:32,420
so things are looking a little bit better here

641
00:44:32,770 --> 00:44:37,150
so what if we make me if you make catcalls very wide

642
00:44:37,180 --> 00:44:38,260
then of course

643
00:44:38,260 --> 00:44:40,010
initially things look even better

644
00:44:41,650 --> 00:44:45,540
all of the data points actually lie relatively speaking quite close to the mean of

645
00:44:48,020 --> 00:44:50,900
because the gaussians was extremely wide

646
00:44:50,960 --> 00:44:53,000
again it's the same argument right

647
00:44:53,040 --> 00:44:56,460
the gas an extremely wide so it could account for any dataset

648
00:44:56,520 --> 00:44:59,810
so we shouldn't be too surprised that also accounted for this one

649
00:45:00,800 --> 00:45:04,550
and actually it's likely the right solution here would be to write down

650
00:45:04,570 --> 00:45:07,440
the likelihood function to maximize likelihood

651
00:45:07,460 --> 00:45:11,040
one of the white likelihood function look like well if this is a gas distribution

652
00:45:11,050 --> 00:45:12,440
centre and zero

653
00:45:12,490 --> 00:45:13,410
then the

654
00:45:13,440 --> 00:45:17,670
the likelihood function which is the gauss in the log likelihood

655
00:45:17,680 --> 00:45:21,720
with the with the zero oil with me knew was chosen to be zero

656
00:45:21,770 --> 00:45:25,140
it's just the log of the gas in and that the log of the

657
00:45:25,200 --> 00:45:29,420
exponentiated term the longer the normalisation constant

658
00:45:30,400 --> 00:45:33,740
this is exactly the same expression is read before

659
00:45:34,410 --> 00:45:36,070
it's a data fit term

660
00:45:36,070 --> 00:45:39,150
otherwise should lie close to the mean

661
00:45:39,210 --> 00:45:40,650
as measured by the red

662
00:45:40,740 --> 00:45:41,540
and also

663
00:45:41,580 --> 00:45:45,060
the volume should be small this the data complexity term at the same time as

664
00:45:45,060 --> 00:45:48,030
before except that the one-dimensional version of that

665
00:45:48,040 --> 00:45:49,530
there are also the same

666
00:45:50,620 --> 00:45:52,310
nine cost the

667
00:45:54,090 --> 00:45:55,790
so the marginal likelihood that i

668
00:45:55,810 --> 00:45:57,350
giving the process

669
00:45:57,370 --> 00:45:59,740
it's just the multivariate version of this

670
00:46:01,340 --> 00:46:09,240
please you believe this then you should also believe the of

671
00:46:09,250 --> 00:46:10,520
it tells you

672
00:46:10,530 --> 00:46:15,330
it it tells you exactly why this overfitting problem is happening right if you only

673
00:46:15,330 --> 00:46:16,850
if you're only interested

674
00:46:16,870 --> 00:46:19,610
in the observations lying close to the mean

675
00:46:19,620 --> 00:46:21,640
as measured by the variance

676
00:46:21,700 --> 00:46:25,440
then this can be done trivially by just turning off the variance

677
00:46:25,570 --> 00:46:28,680
this is this is just the sort of the data for term the data for

678
00:46:28,700 --> 00:46:30,010
term in itself

679
00:46:30,060 --> 00:46:31,280
it's also

680
00:46:31,360 --> 00:46:33,580
you can just maximize

681
00:46:33,590 --> 00:46:37,350
the only way you could do that if you are working somehow constrained class model

682
00:46:37,360 --> 00:46:42,190
then it might work like

683
00:46:42,550 --> 00:46:44,610
all right so

684
00:46:44,650 --> 00:46:49,470
but this is another way of of of thinking of these girls in process

685
00:46:49,890 --> 00:46:55,550
so tried to try to just use to slice through two to look at different

686
00:46:56,140 --> 00:46:58,330
of thinking about golf courses

687
00:46:58,330 --> 00:46:59,170
the notice

688
00:46:59,220 --> 00:47:00,680
this sort of

689
00:47:00,690 --> 00:47:04,330
it's unusual to think about distributions of functions right

690
00:47:04,330 --> 00:47:09,100
so probably most of you are a little bit uncomfortable this still housing uncomfortable with

691
00:47:09,110 --> 00:47:10,100
it for years

692
00:47:10,330 --> 00:47:13,540
also i didn't have any which i to ask you can ask me for to

693
00:47:13,570 --> 00:47:15,370
you're in a good position

694
00:47:15,670 --> 00:47:20,670
so the point here is a construction that comes from that goes from a parametric

695
00:47:20,670 --> 00:47:22,270
model which were used to

696
00:47:22,300 --> 00:47:24,120
to the corresponding isoprofit

697
00:47:24,170 --> 00:47:28,130
let's first two in a very simple case that's doing it into interesting case on

698
00:47:28,130 --> 00:47:29,430
the next slide

699
00:47:29,510 --> 00:47:30,590
here we go to now

700
00:47:30,600 --> 00:47:31,740
we look at

701
00:47:32,420 --> 00:47:35,760
possibly now functions functions are extra p

702
00:47:36,410 --> 00:47:38,830
but now we think of the coefficients

703
00:47:38,870 --> 00:47:41,000
as being random

704
00:47:41,550 --> 00:47:43,630
if the coefficients here are randomly

705
00:47:43,690 --> 00:47:46,900
and that somehow implies also that the distribution

706
00:47:46,950 --> 00:47:49,070
but is the distribution or

707
00:47:49,110 --> 00:47:50,460
somehow the distribution

708
00:47:50,510 --> 00:47:52,770
overall mean function

709
00:47:52,770 --> 00:47:54,090
let's say that the

710
00:47:54,100 --> 00:47:56,140
these these are random

711
00:47:56,170 --> 00:47:59,390
they have distributions that gauss distributions

712
00:47:59,400 --> 00:48:00,710
which is zero mean

713
00:48:00,720 --> 00:48:05,150
and the distribution for a has the variance of the distribution for p has to

714
00:48:07,930 --> 00:48:12,430
now this distribution over functions is actually about process

715
00:48:12,440 --> 00:48:15,300
we can compute which counts processes

716
00:48:15,330 --> 00:48:16,690
by just computing

717
00:48:17,610 --> 00:48:20,120
the mean function and covariance function

718
00:48:20,140 --> 00:48:21,890
OK so what is the mean function

719
00:48:21,910 --> 00:48:23,180
the mean function

720
00:48:23,220 --> 00:48:25,020
is the average

721
00:48:25,020 --> 00:48:29,760
of the of the of f evaluated at any particular x

722
00:48:30,860 --> 00:48:31,650
so now

723
00:48:31,660 --> 00:48:33,310
in this expectation

724
00:48:33,350 --> 00:48:34,800
x is fixed

725
00:48:34,820 --> 00:48:38,530
and it's a and b that around

726
00:48:38,610 --> 00:48:42,160
this is an expectation over the distribution of a and b

727
00:48:42,170 --> 00:48:44,500
for any fixed x

728
00:48:44,540 --> 00:48:46,620
OK so we just plug in to the

729
00:48:46,650 --> 00:48:49,860
formula for expectation so that the integral of f of x

730
00:48:50,850 --> 00:48:52,880
the joint distribution of a and b

731
00:48:52,930 --> 00:48:55,250
i happen to be independent in this case

732
00:48:55,410 --> 00:48:57,150
and integrating out

733
00:48:57,170 --> 00:48:58,800
a and b

734
00:48:58,800 --> 00:48:59,840
OK so if we

735
00:48:59,850 --> 00:49:01,970
so we just plug the value for

736
00:49:02,030 --> 00:49:07,110
for the for the function of here we get any intercourse part so this one

737
00:49:07,110 --> 00:49:10,580
term that depends on a in the long-term depends on the

738
00:49:10,630 --> 00:49:12,660
this can be written on this

739
00:49:13,570 --> 00:49:14,630
seven tentacles

740
00:49:14,650 --> 00:49:18,970
in both these integrals have value of zero just because of symmetry

741
00:49:19,720 --> 00:49:22,350
so this is just the average of the

742
00:49:22,460 --> 00:49:25,000
these b has average to zero

743
00:49:25,040 --> 00:49:26,160
this thing here

744
00:49:26,640 --> 00:49:30,790
it's a x can go outside into google and then just the average of a

745
00:49:30,790 --> 00:49:31,960
times x

746
00:49:32,000 --> 00:49:33,300
which is also zero

747
00:49:35,620 --> 00:49:40,720
this is not so strange just saying if you take an average over all functions

748
00:49:40,740 --> 00:49:41,860
within this class

749
00:49:41,880 --> 00:49:44,420
then the function value on average

750
00:49:44,420 --> 00:49:49,610
i this has done lots of people enjoy exploring the slightly

751
00:49:49,610 --> 00:49:57,070
complications or ramifications of very

752
00:49:57,080 --> 00:50:00,880
hearing this that so

753
00:50:00,900 --> 00:50:07,190
OK i'm several extreme but i really like

754
00:50:07,400 --> 00:50:12,200
having the discipline of adhering to a simple thing and making sure we've said

755
00:50:12,230 --> 00:50:15,160
all the important things about that simple thing rather than

756
00:50:15,170 --> 00:50:19,800
you know making more papers by coming up with special cases like special case of

757
00:50:19,810 --> 00:50:24,240
the the main cases hard enough and rich enough that we should be able to

758
00:50:24,240 --> 00:50:25,830
address such OK

759
00:50:25,830 --> 00:50:30,010
so anyway you don't have to do that but i i want to put this

760
00:50:30,010 --> 00:50:34,230
on the table you to say that

761
00:50:34,290 --> 00:50:38,060
reward is the is the basic things that

762
00:50:39,180 --> 00:50:40,230
this key two

763
00:50:40,250 --> 00:50:43,560
the the the impact of our field that simplicity

764
00:50:43,570 --> 00:50:48,200
and this is a clear statement OK so in particular was a little more this

765
00:50:48,490 --> 00:50:53,620
we get autonomous the the other half of reward with we we don't know the

766
00:50:53,620 --> 00:50:59,000
reward and value i haven't done that i want to talk about

767
00:50:59,030 --> 00:51:03,000
why is it so why is it so important one reason is we don't have

768
00:51:03,000 --> 00:51:09,390
to teachers to desired things we can have things we learn for self reward let's

769
00:51:10,550 --> 00:51:14,470
so if you're playing so here the examples TD gammon appointed again as i said

770
00:51:14,480 --> 00:51:18,250
you can play the game against yourself and see if you win or lose you

771
00:51:18,250 --> 00:51:21,010
get the outcome in the end you did when you did lose you can learn

772
00:51:21,010 --> 00:51:24,940
from that you can learn to play better than any human player you don't require

773
00:51:24,950 --> 00:51:29,900
teacher you can just get that if you if you're flying helicopter you can fly

774
00:51:29,900 --> 00:51:33,070
helicopter perhaps in simulation and crash or not

775
00:51:34,640 --> 00:51:38,130
and you can you can learn from that you can verify whether or not you

776
00:51:38,130 --> 00:51:41,760
have a good policy someone do the robocup dogs is just one example of this

777
00:51:41,760 --> 00:51:46,210
issue this kind of kind of clear in the power of being it's what's really

778
00:51:46,210 --> 00:51:51,730
almost like the same thing in genetic algorithms that can run many many simulations and

779
00:51:51,730 --> 00:51:56,140
to find something which really good and have been there is the power of search

780
00:51:56,330 --> 00:52:01,220
being mechanistically tell whether or not you have a good policy so this has been

781
00:52:01,220 --> 00:52:04,830
done in robocup soccer where they have these robot dogs that have to learn to

782
00:52:04,830 --> 00:52:10,810
play soccer in this is this is the learning center the reinforcement learning center they

783
00:52:10,810 --> 00:52:14,160
interview to play soccer it's really important you could always win actually if you can

784
00:52:14,160 --> 00:52:17,530
run faster than the other guy you can do quite well

785
00:52:17,610 --> 00:52:22,100
and so there very concerned making them run faster and what we have is the

786
00:52:22,110 --> 00:52:27,140
field where we put these robots in they they can see these pencils

787
00:52:27,150 --> 00:52:31,190
and they basically walk back and forth they try one style walking towards to the

788
00:52:31,190 --> 00:52:34,950
pedestal and they they have the clock see long takes and they turn around a

789
00:52:34,990 --> 00:52:38,140
they run back and they can they can they can be used for weeks and

790
00:52:38,140 --> 00:52:40,850
weeks they can figure out a good way

791
00:52:43,780 --> 00:52:47,230
to walk so here's

792
00:52:47,260 --> 00:52:49,180
one the sound

793
00:52:49,350 --> 00:52:54,140
just a little bit so here they're just walking back and forth you diseases the

794
00:52:56,490 --> 00:52:58,200
clicks stopwatch

795
00:52:58,210 --> 00:53:02,240
turned around and walked back forth and they do this for a long time

796
00:53:03,340 --> 00:53:04,350
its key

797
00:53:04,360 --> 00:53:08,550
they you know they can figure it out they can try different ways walking

798
00:53:09,140 --> 00:53:13,380
takes and try different ways one day sockets this

799
00:53:13,400 --> 00:53:17,970
and you can use to this over and over again and find a better way

800
00:53:18,050 --> 00:53:20,740
better way to to what

801
00:53:20,760 --> 00:53:25,170
so that's the power being verify for yourself and so they did this and then

802
00:53:25,400 --> 00:53:29,260
after a while they end up with the best way there's anyone had ever found

803
00:53:29,260 --> 00:53:32,700
to walk so here's the robot from

804
00:53:35,080 --> 00:53:37,030
here's me

805
00:53:39,620 --> 00:53:41,640
from the initial this is the before

806
00:53:41,670 --> 00:53:43,110
video so

807
00:53:43,130 --> 00:53:46,900
you program them to the variety of ways walking up be very good

808
00:53:46,930 --> 00:53:52,100
and this was a very good one and experiment to try different things

809
00:53:52,110 --> 00:53:54,980
and then after after awhile after

810
00:53:55,000 --> 00:53:59,950
a few weeks they end up having the fastest way than anyone had ever find

811
00:53:59,950 --> 00:54:04,010
bound to have these robots walk not you can walk and zealous

812
00:54:04,030 --> 00:54:05,930
who'd have thought

813
00:54:06,050 --> 00:54:11,530
that turns but also knows he's gone kind of faster

814
00:54:11,540 --> 00:54:15,810
it's going to be play faster than before and so this is just the power

815
00:54:15,820 --> 00:54:20,260
we'll tell for yourself whether which you can you can save

816
00:54:20,290 --> 00:54:25,680
human programmers of time and you can get up with much better results than previously

817
00:54:25,680 --> 00:54:30,940
possible and in fact all the robot dogs pretty much that there is robocup cup

818
00:54:30,940 --> 00:54:34,320
soccer now he's learned

819
00:54:34,330 --> 00:54:36,150
policies for walking

820
00:54:36,210 --> 00:54:38,950
OK a little bit more reward

821
00:54:39,050 --> 00:54:41,120
because from reward

822
00:54:41,160 --> 00:54:43,240
follows the idea of value

823
00:54:43,300 --> 00:54:45,440
given equation is this later

824
00:54:46,120 --> 00:54:51,060
the value function you all know is just expected some of the future discounted reward

825
00:54:51,240 --> 00:54:57,390
OK so that something is really simple theory that value long-term goal is maximizing the

826
00:55:00,900 --> 00:55:06,000
the short-term goal and the value function hypothesis is that can use all again because

827
00:55:06,000 --> 00:55:07,130
i want to be

828
00:55:07,140 --> 00:55:08,680
the strong

829
00:55:08,700 --> 00:55:13,850
all efficient methods for solving sequential decision problems

830
00:55:13,860 --> 00:55:19,560
estimate as intermediate step is made value functions as an intermediate computation towards

831
00:55:19,580 --> 00:55:22,340
to find a good policy finding good solution

832
00:55:22,410 --> 00:55:23,600
i think

833
00:55:23,710 --> 00:55:25,570
this what i feel we learn

834
00:55:25,580 --> 00:55:27,690
the last

835
00:55:27,710 --> 00:55:30,140
and know how many years of

836
00:55:30,140 --> 00:55:37,140
the field that we need value functions and this is this comes from when reward

837
00:55:37,140 --> 00:55:40,370
you're in the world of value functions pushing those from

838
00:55:40,380 --> 00:55:42,370
from optimal control from forever

839
00:55:42,400 --> 00:55:45,680
and and really

840
00:55:45,740 --> 00:55:47,360
we're just carrying this further

841
00:55:47,360 --> 00:55:51,610
and relating to different fields OK so i'd better move on to some other ideas

842
00:55:52,960 --> 00:55:56,610
but want to mention so really the whole computational theory all the film it's really

843
00:55:56,610 --> 00:56:02,910
follow what you reward after value functions and natural models and temple different areas dynamic

844
00:56:02,910 --> 00:56:05,460
programming and much of much of things

845
00:56:05,550 --> 00:56:08,600
OK the idea certain idea

846
00:56:08,600 --> 00:56:10,590
so the idea is sampling

847
00:56:10,600 --> 00:56:17,620
time reward now sampling so i sampling i mean first sampling as well trial and

848
00:56:17,620 --> 00:56:24,010
error learning like a rat in maze tries things samples different actions also as in

849
00:56:24,010 --> 00:56:26,600
and is

850
00:56:26,620 --> 00:56:31,870
you see them on

851
00:56:52,710 --> 00:56:55,390
well and we

852
00:56:55,410 --> 00:57:07,000
he also

853
00:57:40,430 --> 00:57:47,680
yes the fire

854
00:58:30,420 --> 00:58:37,250
o thing

855
00:58:37,280 --> 00:58:39,430
you mean

856
00:59:43,270 --> 00:59:45,790
it red

857
00:59:55,440 --> 00:59:58,280
our vision

858
01:00:00,720 --> 01:00:08,630
you are

859
01:00:08,640 --> 01:00:11,050
the problem

860
01:00:25,530 --> 01:00:30,060
so far can be

861
01:00:48,550 --> 01:00:56,390
here we are

862
01:01:10,340 --> 01:01:15,910
all right

863
01:01:19,610 --> 01:01:22,650
so that

864
01:01:29,480 --> 01:01:31,690
well i i

865
01:01:31,710 --> 01:01:36,090
after all

866
01:01:50,430 --> 01:01:54,200
it used

867
01:02:24,320 --> 01:02:27,950
you know

868
01:02:49,980 --> 01:02:53,450
get lot

869
01:02:55,930 --> 01:03:00,100
they are

870
01:03:00,100 --> 01:03:05,080
and now we're going to discuss how to take the lousy this right actually flipping

871
01:03:05,080 --> 01:03:09,500
ten percent of bits at encoding and decoding systems to deliver

872
01:03:09,520 --> 01:03:11,000
this sort of performance

873
01:03:11,010 --> 01:03:16,070
and three months eighty ten minus fifty

874
01:03:16,110 --> 01:03:20,910
OK so what we want to do

875
01:03:20,920 --> 01:03:24,880
it is come up with some ideas about what this encoder and decoder going to

876
01:03:24,880 --> 01:03:29,970
be to start off with the anyone have any suggestions of an example

877
01:03:30,000 --> 01:03:34,930
of the encoder way of adding redundancy in the way that we think might be

878
01:03:35,960 --> 01:03:39,430
an example of adding redundancy

879
01:03:39,470 --> 01:03:41,970
duplicate repeat

880
01:03:43,030 --> 01:03:45,230
if the source file

881
01:03:45,280 --> 01:03:46,650
has got a one

882
01:03:46,650 --> 01:03:47,580
in it

883
01:03:47,600 --> 01:03:51,350
we duplicate some number of times

884
01:03:51,360 --> 01:03:52,390
how many

885
01:03:52,400 --> 01:03:54,110
the team

886
01:03:54,120 --> 01:03:55,770
OK so

887
01:03:55,870 --> 01:04:01,690
so everyone gets turned into three once a non-zero gets turned into three zero so

888
01:04:01,740 --> 01:04:03,810
the sort of thing you had in mind

889
01:04:03,830 --> 01:04:06,040
OK but possibly more

890
01:04:06,060 --> 01:04:07,430
let's start with a simple one

891
01:04:07,430 --> 01:04:10,580
i'm going to call this code are three

892
01:04:10,680 --> 01:04:15,100
for repeat three times so when i say repeat three times the total number of

893
01:04:15,100 --> 01:04:19,280
times the bit that sent is three times already it's not four times that would

894
01:04:19,280 --> 01:04:22,180
be another convention for what we mean by repetition

895
01:04:22,200 --> 01:04:24,300
so this is the source

896
01:04:24,330 --> 01:04:27,710
and this is the transmitted we read the source one but of time and we

897
01:04:27,710 --> 01:04:29,430
use this encoding rule

898
01:04:29,440 --> 01:04:31,260
so for example

899
01:04:31,320 --> 01:04:33,180
if the source

900
01:04:33,180 --> 01:04:40,670
the entire source firewall were zero one one zero one then we transmit

901
01:04:40,750 --> 01:04:43,470
zero zero zero one one one

902
01:04:43,570 --> 01:04:48,330
zero zero zero ongoing online

903
01:04:52,520 --> 01:04:55,240
binary symmetric channel we can describe the noise

904
01:04:55,300 --> 01:04:57,460
that knox things up

905
01:04:57,470 --> 01:05:00,990
as being a binary string as well as a list of zeros and ones with

906
01:05:00,990 --> 01:05:04,780
zero showing the bits that luckily didn't get flip

907
01:05:04,800 --> 01:05:09,110
and one showing bits that did get linked so is the noise factor

908
01:05:09,110 --> 01:05:10,290
might occur

909
01:05:10,470 --> 01:05:18,040
right so that means the receiver that would be

910
01:05:18,050 --> 01:05:23,410
now we add modulo two along has zero zero zero one one zero

911
01:05:23,410 --> 01:05:24,730
one zero one

912
01:05:24,890 --> 01:05:30,150
one one one one one one one one one

913
01:05:30,190 --> 01:05:32,950
OK that's an example of going

914
01:05:32,970 --> 01:05:36,960
this far we've encoded and we've gone through the channel tunnel received signal now we

915
01:05:36,960 --> 01:05:42,700
need to invent an appropriate decoder for the right hand side his slide showing what

916
01:05:42,700 --> 01:05:44,020
we've done so far

917
01:05:44,030 --> 01:05:49,250
i've actually reordered the bit so you can see the three repetitions spread out in

918
01:05:49,250 --> 01:05:51,900
time the conceptual it's just the same thing

919
01:05:53,230 --> 01:05:55,710
a way of implementing this

920
01:05:55,720 --> 01:05:56,570
it would be

921
01:05:56,580 --> 01:05:58,380
to buy

922
01:05:58,380 --> 01:06:02,690
to sell sorry when you're selling it destroys actually sell three of them inside the

923
01:06:03,670 --> 01:06:07,500
OK and every time you store file if you put the bit on strike one

924
01:06:07,500 --> 01:06:11,470
two and three and was one destroyer in here

925
01:06:11,520 --> 01:06:15,750
but we need to decode

926
01:06:18,300 --> 01:06:20,980
that is that it goes here

927
01:06:21,080 --> 01:06:27,470
what's the decoder going to do what what is the right decoder habitats to your

928
01:07:11,520 --> 01:07:21,200
what's that he carried out his various ways of saying and say yeah

929
01:07:21,210 --> 01:07:25,430
OK so this is just the code is

930
01:07:25,480 --> 01:07:30,080
read that the number of the middle and ignore the rest OK

931
01:07:30,100 --> 01:07:31,480
so this would

932
01:07:31,480 --> 01:07:36,830
one one you want to take one zero one one zero one

933
01:07:36,850 --> 01:07:40,520
two zero and zero zero zero

934
01:07:40,560 --> 01:07:42,870
he z

935
01:07:43,790 --> 01:07:47,410
the suggestions on the

936
01:07:52,390 --> 01:07:55,850
so majority vote

937
01:07:55,850 --> 01:08:00,270
you go to which a is

938
01:08:00,310 --> 01:08:03,250
the most common sense

939
01:08:03,270 --> 01:08:04,770
we say

940
01:08:04,830 --> 01:08:09,970
one one here one or disagrees and analyst

941
01:08:09,980 --> 01:08:13,120
zero that everyone understand what we mean by

942
01:08:13,180 --> 01:08:14,910
majority vote

943
01:08:16,830 --> 01:08:21,350
you suggesting a different account of what you think of the choice between these

944
01:08:21,350 --> 01:08:30,350
di very good i think it's a really good way to learn is by examples

945
01:08:30,350 --> 01:08:31,680
but some sort of

946
01:08:31,730 --> 01:08:35,250
some form of overfitting may have happened

947
01:08:36,600 --> 01:08:45,370
what's another way of motivating decoder you said majority vote decoder only came up with

948
01:08:45,370 --> 01:08:51,310
this idea of how could you have derived the decoder what does

949
01:08:52,370 --> 01:09:00,680
estimation all i'd say inference inferences voting would say how probable is that the

950
01:09:00,700 --> 01:09:02,080
source signal

951
01:09:04,470 --> 01:09:08,330
i mean for example

952
01:09:08,330 --> 01:09:10,660
take the zero one one

953
01:09:10,700 --> 01:09:14,480
so given that you've got received signal like zero one one

954
01:09:14,540 --> 01:09:16,080
how probable is it

955
01:09:16,270 --> 01:09:17,580
it was zero

956
01:09:17,600 --> 01:09:18,540
and we use

957
01:09:18,540 --> 01:09:22,060
based there because that's the right way to do inference it works with the red

958
01:09:22,060 --> 01:09:27,540
comes it works for any well-defined inference problem so we can use bayes theorem write

959
01:09:27,540 --> 01:09:30,250
down probability of getting zero one

960
01:09:30,310 --> 01:09:35,450
given as zero find the probability that this is zero

961
01:09:35,500 --> 01:09:41,540
normalized by dividing by the other explanation and this one is

962
01:09:41,600 --> 01:09:43,270
the first step

963
01:09:43,290 --> 01:09:49,540
and the other term that could have been upstairs is everything with s replaced zero

964
01:09:49,540 --> 01:09:53,790
replaced by s is one one

965
01:09:53,810 --> 01:09:56,620
one given as equals one

966
01:09:56,620 --> 01:09:59,620
and this is an example for some values

967
01:10:00,190 --> 01:10:02,270
so when LP

968
01:10:02,370 --> 01:10:05,480
and this is also the nominal value of R LP

969
01:10:07,040 --> 01:10:10,270
if I now assume that

970
01:10:10,290 --> 01:10:14,960
a normal vectors of each of these inequalities is a random vector

971
01:10:15,300 --> 01:10:19,000
with the symetric distribution normal distribution than

972
01:10:19,870 --> 01:10:24,100
and I have a point X that's somewhere on a hyperplane for example this hyperplane

973
01:10:26,330 --> 01:10:28,750
for the nominal value or the mean

974
01:10:28,770 --> 01:10:33,870
of the normal vector A I then that vector will also in fifty per cent of the

975
01:10:33,870 --> 01:10:37,560
cases with fifty per cent probability will still be

976
01:10:38,920 --> 01:10:41,500
even if I

977
01:10:41,540 --> 01:10:43,850
if A varies in this distribution

978
01:10:44,690 --> 01:10:46,140
so if you

979
01:10:46,190 --> 01:10:48,940
require this with fifty per cent probability

980
01:10:49,280 --> 01:10:55,310
the robust or the solution set for this feasible set for these M chance constraints

981
01:10:55,620 --> 01:10:58,940
it's still the original nominal value

982
01:11:00,600 --> 01:11:03,940
but if you take a different value of fifty per cent it's very different so if

983
01:11:03,940 --> 01:11:11,270
I require that the inequality is satisfied with the probability of the ninety per cent then these

984
01:11:11,770 --> 01:11:17,560
lines become curved so this instead of a hyperplane now I have this curve

985
01:11:17,810 --> 01:11:23,370
which is actually the level curve where the probability of satisfying at inequality is exactly ninety

986
01:11:23,370 --> 01:11:26,190
per cent and

987
01:11:26,540 --> 01:11:31,290
so we have one of these for each of the inequalities and the feasible set shrinks

988
01:11:31,310 --> 01:11:34,040
to something that's smaller and convex

989
01:11:35,710 --> 01:11:41,540
that's the feasible set for these M chance constraints inequalities if eta is

990
01:11:41,540 --> 01:11:42,830
ninety per cent

991
01:11:43,330 --> 01:11:48,580
if eta is less than fifty per cent it's actually the other way around the we get a larger

992
01:11:48,630 --> 01:11:49,840
feasible set

993
01:11:50,300 --> 01:11:54,460
and these level curves of consonant probability

994
01:11:54,500 --> 01:11:57,090
curve and the other have the other orientation

995
01:11:57,540 --> 01:12:00,840
and then the solution set becomes concave nonconvex

996
01:12:01,130 --> 01:12:04,050
and this problem becomes very difficult

997
01:12:04,090 --> 01:12:06,270
because if you have a solution set like this

998
01:12:06,550 --> 01:12:11,220
even if the curvature is not very pronounced then you can easily have and you

999
01:12:11,220 --> 01:12:18,850
minimize a linear function over this set then you can easily have multiple local minimum that are not global

1000
01:12:19,160 --> 01:12:25,850
so we see something interesting here that if you have this chance constraint problems if

1001
01:12:25,850 --> 01:12:30,590
eta is great than fifty per cent we have a convex feasible set if it's less

1002
01:12:30,590 --> 01:12:34,500
than fifty per cent it's not convex

1003
01:12:34,540 --> 01:12:40,690
so what we can actually in this case if you assume that A Is are normal

1004
01:12:40,710 --> 01:12:45,080
we know the mean and covariance we can actually easily calculate this probability

1005
01:12:46,550 --> 01:12:48,260
probability theory

1006
01:12:48,480 --> 01:12:50,500
and you can write this

1007
01:12:50,690 --> 01:12:54,850
constraint and this form so you have the nominal value of the

1008
01:12:54,890 --> 01:12:57,690
inequality that's A I bar

1009
01:12:58,240 --> 01:13:03,510
and then this extra term involves the cumulative density function of the gaussian

1010
01:13:03,540 --> 01:13:06,300
this function phi of T between zero and one

1011
01:13:06,310 --> 01:13:09,260
and then that's multiplied with the

1012
01:13:09,310 --> 01:13:14,220
two norm of the square of the covariance times X

1013
01:13:14,220 --> 01:13:17,590
so this does actually this makes sense because it tells you that if eta

1014
01:13:18,660 --> 01:13:22,500
is large is close than ninety percent then

1015
01:13:22,510 --> 01:13:25,580
In order to make X feasible it at high probability

1016
01:13:25,580 --> 01:13:31,510
it's not sufficient to satisfy the inequality for the nominal value of A I

1017
01:13:31,510 --> 01:13:34,580
but it has to satisfy with a certain slack or a certain margin

1018
01:13:34,620 --> 01:13:39,340
that depends and the value of the margin the required margin depends on the value

1019
01:13:39,340 --> 01:13:40,340
of eta

1020
01:13:40,430 --> 01:13:43,540
and on the covariances and actually more

1021
01:13:43,540 --> 01:13:46,890
precisely the product between X and the covariance

1022
01:13:47,770 --> 01:13:52,880
and this also explains why you have this transition between eta at fity per cent

1023
01:13:53,340 --> 01:13:58,690
because this coefficient here the inverse of eta and then this cumulative density function

1024
01:13:58,690 --> 01:14:03,260
changes sign for fifty per cent if eta is greater than fity per cent then

1025
01:14:03,260 --> 01:14:04,690
that's a positive coefficient

1026
01:14:05,090 --> 01:14:10,390
we get a convex constraint because we have a norm with a positive coefficients on the less

1027
01:14:10,390 --> 01:14:14,730
than an equal side of inequality if eta is less than

1028
01:14:14,770 --> 01:14:21,180
fifty percent then this coefficient becomes negative and the sinus's wrong we have the

1029
01:14:21,190 --> 01:14:24,730
negative term on the wrong side of the inequality

1030
01:14:25,050 --> 01:14:30,180
so that's a stochastic formulation of the robust LP

1031
01:14:30,390 --> 01:14:36,420
we can actually give a deterministic interpretation to exactly the same

1032
01:14:36,430 --> 01:14:41,880
problem so in deterministic interpretation

1033
01:14:42,200 --> 01:14:48,010
we make we don't make any assumptions on the distribution of AI but we impose a certain

1034
01:14:48,010 --> 01:14:49,680
set of allowed

1035
01:14:49,690 --> 01:14:55,550
allowable values of AI so in this case we assume that AIs are unknown better

1036
01:14:55,550 --> 01:15:00,850
known to be in a certain ellipsoids with a center AI bar and then a certain shape

1037
01:15:00,850 --> 01:15:03,730
that is determined by this matrix PI

1038
01:15:04,430 --> 01:15:08,550
and then the robust version of this inequality will be that we require that X

1039
01:15:08,550 --> 01:15:13,920
is feasible for all values of AI

1040
01:15:13,920 --> 01:15:15,300
actually we can

1041
01:15:16,050 --> 01:15:22,000
so if you work this out in order to write this more explicitly you will have to

1042
01:15:22,000 --> 01:15:26,800
find a maximum of AI transpose X for a given X over this ellipsoid and then you

1043
01:15:26,800 --> 01:15:30,390
find an expression that's exactly similar to this previous one

1044
01:15:30,880 --> 01:15:32,340
so this

1045
01:15:32,340 --> 01:15:34,220
inequality is satisfied

1046
01:15:34,460 --> 01:15:40,730
for all values of AI and the ellipsoid if X satisfies this second-order cone constraint

1047
01:15:40,850 --> 01:15:45,620
so again the first term is just a nominal value says that X satisfies it for the center point

1048
01:15:45,680 --> 01:15:52,310
of the ellipsoid and then we have euclidean norm term that involves the product of X and

1049
01:15:52,310 --> 01:15:53,670
PI transpose

1050
01:15:53,720 --> 01:15:56,350
so mathematically it's exactly similar to this

1051
01:15:56,810 --> 01:16:00,760
constraint but with a different interpretation

1052
01:16:00,770 --> 01:16:05,970
and again we see that in the robust version of the LP

1053
01:16:05,970 --> 01:16:11,250
it's not sufficient for X to be feasible for the nominal value of AI you add an extra margin

1054
01:16:11,620 --> 01:16:15,500
and the value of that margin actually depends on the size of X

1055
01:16:15,840 --> 01:16:20,260
and more precisely the product of X and PI

1056
01:16:20,260 --> 01:16:22,610
what's going on whether it can be

1057
01:16:22,650 --> 01:16:24,340
defeated somewhere

1058
01:16:24,360 --> 01:16:25,970
and once again create

1059
01:16:25,990 --> 01:16:28,990
a lot of controversy

1060
01:16:31,150 --> 01:16:32,420
a few people

1061
01:16:32,430 --> 01:16:34,110
speaking very loudly

1062
01:16:34,150 --> 01:16:35,930
said that is not possible

1063
01:16:36,010 --> 01:16:38,470
lots of other people i guess someone

1064
01:16:38,630 --> 01:16:43,220
just going to keep working in seeing what we can do

1065
01:16:43,220 --> 01:16:44,630
right so

1066
01:16:44,650 --> 01:16:47,280
and we get out of the question

1067
01:16:49,550 --> 01:16:51,200
m earlier

1068
01:16:51,280 --> 01:16:52,530
so this is

1069
01:16:52,630 --> 01:16:54,950
a famous quote from

1070
01:16:55,050 --> 01:16:58,880
i guess is philosopher

1071
01:16:58,930 --> 01:17:03,030
mean mister good the british philosopher

1072
01:17:03,380 --> 01:17:06,610
about an ultraintelligent machine

1073
01:17:06,670 --> 01:17:10,630
and this is kind of scary because this is something we need to think about

1074
01:17:10,740 --> 01:17:13,880
OK so

1075
01:17:13,980 --> 01:17:18,550
eleven altamont ultraintelligent machine be defined the machine that can never occurred

1076
01:17:18,550 --> 01:17:23,880
so far surpass all the intellectual activities many many

1077
01:17:23,920 --> 01:17:27,760
since the design of machine one of these intellectual activities

1078
01:17:27,820 --> 01:17:31,740
an ultraintelligent machine could design better machines

1079
01:17:31,740 --> 01:17:35,280
then there would unquestionably be an intelligence explosion

1080
01:17:35,380 --> 01:17:38,670
intelligence and be left behind

1081
01:17:38,670 --> 01:17:44,010
that's the first ultraintelligent machine is the last invention meant

1082
01:17:44,010 --> 01:17:47,150
provide the machine epsilon

1083
01:17:47,220 --> 01:17:51,610
tell us how to keep it under control

1084
01:17:51,650 --> 01:17:52,720
OK so

1085
01:17:53,340 --> 01:17:55,200
in nineteen sixty five

1086
01:17:59,300 --> 01:18:01,700
inciteful remark because

1087
01:18:01,780 --> 01:18:04,260
this is something to be said for this

1088
01:18:05,450 --> 01:18:10,400
maybe this one more slide

1089
01:18:12,570 --> 01:18:16,780
as i said we know it in the realm of science fiction

1090
01:18:16,800 --> 01:18:19,010
although not everyone agree with that

1091
01:18:19,030 --> 01:18:23,820
the concept of singularities of intelligence explosion is now going to form online if you

1092
01:18:23,820 --> 01:18:25,150
look on the web you find this

1093
01:18:25,220 --> 01:18:27,530
there singularity organizations

1094
01:18:27,630 --> 01:18:32,590
and there are some pretty woman people that subscribe to view

1095
01:18:32,590 --> 01:18:38,650
so the idea is that

1096
01:18:38,690 --> 01:18:41,650
through intelligent machines

1097
01:18:41,700 --> 01:18:45,530
technology will advance more and more quickly

1098
01:18:45,590 --> 01:18:49,860
advances in night more and more quickly

1099
01:18:49,900 --> 01:18:50,990
and that

1100
01:18:51,010 --> 01:18:53,050
the horizon

1101
01:18:53,070 --> 01:18:54,880
we can see to the end of the

1102
01:18:54,900 --> 01:18:58,880
the idea what's going to happen is getting shorter and shorter

1103
01:18:58,900 --> 01:19:00,450
and there's a lot to be said for this

1104
01:19:00,470 --> 01:19:02,070
i mean if you draw graph of

1105
01:19:02,130 --> 01:19:05,630
human advancement in technology at the time

1106
01:19:05,670 --> 01:19:08,970
it's basically flat four billion years

1107
01:19:09,090 --> 01:19:11,110
perhaps being warned and so on

1108
01:19:11,170 --> 01:19:13,320
the start of the run

1109
01:19:13,360 --> 01:19:15,920
that's right pretty rapidly

1110
01:19:15,970 --> 01:19:17,950
but even in my lifetime

1111
01:19:17,990 --> 01:19:20,150
i've seen this

1112
01:19:20,240 --> 01:19:21,530
when i was

1113
01:19:21,590 --> 01:19:24,800
lecture back in the problem with this chap on the frontier

1114
01:19:26,650 --> 01:19:28,420
twenty five years ago

1115
01:19:28,840 --> 01:19:31,340
things happen very slowly compared to what they do now

1116
01:19:31,380 --> 01:19:36,050
i mean the b-side was that the seventies cycle avionics upgrade some got worse

1117
01:19:36,280 --> 01:19:40,720
but i mean basically things happen incredibly slowly in the whole universe has been upgraded

1118
01:19:40,720 --> 01:19:42,300
every i mean

1119
01:19:42,320 --> 01:19:46,420
witness the fact that schools are schools that affected these colleges

1120
01:19:46,430 --> 01:19:49,530
it really is very nice just in my lifetime

1121
01:19:51,400 --> 01:19:54,610
this kind of effect is not question all

1122
01:19:54,690 --> 01:19:56,670
but israel

1123
01:19:57,490 --> 01:20:00,650
the question is whether you can take the files with the tag so the idea

1124
01:20:00,650 --> 01:20:01,840
is this

1125
01:20:01,900 --> 01:20:04,970
that in competitive small number of years

1126
01:20:05,010 --> 01:20:08,300
and depends who you talk to just ten years it might be

1127
01:20:08,720 --> 01:20:11,030
the shortest estimates

1128
01:20:12,740 --> 01:20:16,030
the order thirty years

1129
01:20:16,090 --> 01:20:18,470
the things will advance so quickly

1130
01:20:18,530 --> 01:20:21,450
we just have no conception of what can happen beyond that

1131
01:20:21,470 --> 01:20:23,550
it's like disappearing into black hole

1132
01:20:23,610 --> 01:20:26,200
first first first

1133
01:20:27,050 --> 01:20:28,920
so there this this this graph

1134
01:20:28,930 --> 01:20:31,170
started to climb

1135
01:20:31,170 --> 01:20:32,730
and including michael room will give

1136
01:20:33,210 --> 01:20:37,130
one the talks in djibouti sessions so i wanted to much there but

1137
01:20:38,380 --> 01:20:39,840
what it is doing is

1138
01:20:40,650 --> 01:20:42,510
using one can another

1139
01:20:43,050 --> 01:20:44,420
the basic cameras indicator

1140
01:20:45,340 --> 01:20:47,800
is pretty similar of pi of

1141
01:20:48,530 --> 01:20:51,670
given each of of war using the simulations

1142
01:20:52,590 --> 01:20:53,860
and therefore that explains

1143
01:20:55,150 --> 01:20:55,740
all that

1144
01:20:56,130 --> 01:20:56,590
the air

1145
01:20:57,490 --> 01:20:59,380
can be evaluated namely z

1146
01:21:00,840 --> 01:21:03,490
the the range over here is for the bias

1147
01:21:04,190 --> 01:21:06,050
in terms of of these square

1148
01:21:06,490 --> 01:21:06,920
you've been

1149
01:21:07,260 --> 01:21:09,460
is is that the tyrant

1150
01:21:10,050 --> 01:21:11,630
and the violence is one of be

1151
01:21:13,550 --> 01:21:19,260
and so the deed is here is an acting upon the violence and there's not much we can do

1152
01:21:20,420 --> 01:21:21,920
i increases in

1153
01:21:23,400 --> 01:21:24,510
except seems easier

1154
01:21:25,050 --> 01:21:25,900
increasing as well

1155
01:21:29,280 --> 01:21:30,320
okay so these are skipped

1156
01:21:31,840 --> 01:21:34,010
it's hard to skip and seems to be that well

1157
01:21:35,210 --> 01:21:37,010
two to move after to the last part

1158
01:21:39,650 --> 01:21:43,190
and to go it's more literature than than anything

1159
01:21:43,880 --> 01:21:45,130
i knew but

1160
01:21:46,010 --> 01:21:47,340
i'm trying to replace maybe see

1161
01:21:48,550 --> 01:21:51,340
fuzzy inference words rather than the computing world

1162
01:21:55,070 --> 01:22:00,050
you may have seen this is trans this slide a few minutes ago i'm just repeating the same thing

1163
01:22:02,380 --> 01:22:02,880
so four

1164
01:22:04,090 --> 01:22:06,630
and we cannot get much farther ahead

1165
01:22:07,920 --> 01:22:08,300
maybe see

1166
01:22:08,760 --> 01:22:10,190
has been justified has

1167
01:22:11,210 --> 01:22:16,570
the con convergence methods are in some cases has been justified as a convergence message

1168
01:22:18,630 --> 01:22:19,420
other simulation

1169
01:22:19,920 --> 01:22:23,530
approximation methods it requires further simulations to

1170
01:22:24,260 --> 01:22:30,610
two given up to give evolution the approximation except when we start looking at the nonparametric perspective in which case

1171
01:22:31,320 --> 01:22:32,150
we don't need to run

1172
01:22:32,740 --> 01:22:33,590
more simulations

1173
01:22:34,780 --> 01:22:36,460
now what being pragmatic

1174
01:22:37,690 --> 01:22:41,570
it relies on a lot of calibration issues we have to pick

1175
01:22:42,320 --> 01:22:46,820
the summary statistics and from there we have to pick the distances and also the tolerance

1176
01:22:47,740 --> 01:22:50,760
and in connection with the nonparametric perspective

1177
01:22:51,920 --> 01:22:55,380
and the stressed in in the recent white paper by paul van had in

1178
01:22:55,990 --> 01:22:56,590
the new strangle

1179
01:22:57,470 --> 01:23:01,300
pushing epsilon exactly two zero or as close as possible to zero

1180
01:23:01,780 --> 01:23:02,780
in that's respect

1181
01:23:03,170 --> 01:23:03,630
may not be

1182
01:23:04,570 --> 01:23:05,990
the optimal ninety because

1183
01:23:06,800 --> 01:23:07,400
we are losing

1184
01:23:08,760 --> 01:23:09,260
in tatively

1185
01:23:09,760 --> 01:23:13,030
a certain amount of simulations that are also increasing

1186
01:23:14,010 --> 01:23:15,840
the vines by trying to reduce

1187
01:23:18,900 --> 01:23:20,880
so if you're using maybe see maybe

1188
01:23:21,570 --> 01:23:22,610
dangerous to you are

1189
01:23:23,590 --> 01:23:25,010
maybe not enough but

1190
01:23:28,300 --> 01:23:29,380
warnings forum

1191
01:23:29,990 --> 01:23:31,860
the general surgeons or whatever

1192
01:23:33,470 --> 01:23:37,780
so let me through a few points of few papers that relates to society

1193
01:23:39,920 --> 01:23:42,690
one important paper in this respect is

1194
01:23:43,110 --> 01:23:44,970
the pianist favorite there only it

1195
01:23:45,240 --> 01:23:46,570
redmond courses

1196
01:23:48,940 --> 01:23:50,670
because epsilon is

1197
01:23:51,280 --> 01:23:52,090
both essential

1198
01:23:52,610 --> 01:23:53,800
and the trouble nuisance

1199
01:23:54,320 --> 01:23:57,190
because it doesn't appear in georgia inferential problem

1200
01:23:57,590 --> 01:23:59,440
they include itself into the picture

1201
01:23:59,880 --> 01:24:01,550
by drawing inference not only

1202
01:24:02,300 --> 01:24:02,840
on feature

1203
01:24:03,130 --> 01:24:03,740
but also

1204
01:24:04,130 --> 01:24:04,970
on epsilon

1205
01:24:06,240 --> 01:24:07,820
and said that they used are

1206
01:24:10,110 --> 01:24:13,170
but approach to to do that by putting a prior epsilon

1207
01:24:14,490 --> 01:24:16,590
and transforming the soul

1208
01:24:17,840 --> 01:24:21,710
likelihood into a function on its side so absent became part

1209
01:24:22,210 --> 01:24:25,690
partly the air on the methods and partly

1210
01:24:26,880 --> 01:24:27,710
the violation

1211
01:24:28,130 --> 01:24:29,800
of the around today

1212
01:24:30,940 --> 01:24:31,970
case because this is not

1213
01:24:32,760 --> 01:24:33,650
exactly available

1214
01:24:34,530 --> 01:24:38,150
there's an apprenticeship approximation involved at this level as well

1215
01:24:42,320 --> 01:24:44,940
and then the technique don't go the technique

1216
01:24:46,240 --> 01:24:47,150
for lack of time but

1217
01:24:48,690 --> 01:24:49,670
it's it's using

1218
01:24:51,300 --> 01:24:56,010
and since e two to move around with this trick that's that's all tricks the

1219
01:24:56,010 --> 01:24:57,940
first one is that the distances do not

1220
01:24:59,670 --> 01:25:03,550
summarized into a one dimensional object any longer

1221
01:25:04,010 --> 01:25:05,960
and that's the strings all the methods are many

1222
01:25:08,710 --> 01:25:10,990
and so epsilon becomes a multidimensional object

1223
01:25:11,460 --> 01:25:16,190
the distances can be negative which is interesting for resistances but that's another positive point

1224
01:25:16,900 --> 01:25:17,800
being negative right

1225
01:25:18,340 --> 01:25:21,570
and the last point is that because there are so many

1226
01:25:22,590 --> 01:25:23,090
that's all

1227
01:25:26,070 --> 01:25:26,990
the distribution on

1228
01:25:28,190 --> 01:25:30,550
directions are summarized into a minimum

1229
01:25:30,990 --> 01:25:32,670
which is another unusual objects

1230
01:25:33,170 --> 01:25:34,470
into into the picture

1231
01:25:35,690 --> 01:25:36,260
and from

1232
01:25:38,050 --> 01:25:42,610
intuition of of new objects they get away to compare models as well

1233
01:25:43,690 --> 01:25:47,730
because each other directions so this is stolen from the paper

1234
01:25:48,690 --> 01:25:49,300
always permission

1235
01:25:51,150 --> 01:25:53,400
from each other directions of errors

1236
01:25:53,960 --> 01:26:01,710
you can build a distribution prostitution absolutist institutions this errors and compare what happens in different models to spot

1237
01:26:02,320 --> 01:26:02,740
the model

1238
01:26:04,320 --> 01:26:04,920
is behaving

1239
01:26:05,440 --> 01:26:07,740
strangely namely is avoiding

1240
01:26:08,490 --> 01:26:09,360
it's a zero value

1241
01:26:10,630 --> 01:26:12,460
relates to are absent equal zero

1242
01:26:12,920 --> 01:26:14,280
justification of

1243
01:26:17,460 --> 01:26:20,530
kids out and we had a few questions that ask integrations

1244
01:26:22,320 --> 01:26:24,300
now another approach that's

1245
01:26:26,360 --> 01:26:28,690
with rich allusions and in internet

1246
01:26:29,670 --> 01:26:33,530
that has been found in many papers subsequently he's

1247
01:26:34,240 --> 01:26:34,970
the fact at

1248
01:26:35,710 --> 01:26:36,070
so now

1249
01:26:37,010 --> 01:26:39,320
maybe she can be inter turn into

1250
01:26:40,300 --> 01:26:41,780
gone exact obesity

1251
01:26:42,880 --> 01:26:44,800
although it's not exactly an exact pieces

1252
01:26:45,650 --> 01:26:46,130
and next actually

1253
01:26:48,490 --> 01:26:49,340
you're looking at

1254
01:26:52,940 --> 01:26:55,740
that is connected with the prod product or simulations

1255
01:26:56,130 --> 01:26:58,900
we're assuming from from pi cedar from part

1256
01:26:59,400 --> 01:27:00,630
then assuming to there

1257
01:27:01,650 --> 01:27:03,110
from the true likelihood

1258
01:27:04,190 --> 01:27:04,610
and then

1259
01:27:05,010 --> 01:27:06,070
because we cannot wait

1260
01:27:06,650 --> 01:27:07,650
we have this weight

1261
01:27:08,240 --> 01:27:10,740
that is zero on security more maybe see

1262
01:27:11,210 --> 01:27:12,300
or is it is a kernel

1263
01:27:12,920 --> 01:27:14,570
of are any flavor

1264
01:27:15,280 --> 01:27:16,760
in in the job market

1265
01:27:17,190 --> 01:27:19,150
we use tolerance epsilon

1266
01:27:20,360 --> 01:27:22,760
so when you look at things from from that's where you

1267
01:27:25,590 --> 01:27:27,170
looks like true posture

1268
01:27:27,800 --> 01:27:28,760
but from

1269
01:27:29,190 --> 01:27:30,740
another distribution then

1270
01:27:31,110 --> 01:27:35,150
the true distribution namely the convolution between enough then

1271
01:27:35,690 --> 01:27:35,970
the key

1272
01:27:37,440 --> 01:27:42,630
can we get enough others even see the time scale of one minus e has

1273
01:27:44,490 --> 01:27:45,300
the new distribution

1274
01:27:47,130 --> 01:27:48,070
i mean that's

1275
01:27:49,050 --> 01:27:52,210
maybe she is exact but for the wrong problem

1276
01:27:54,760 --> 01:27:57,280
is asking the wrong question but then we get the right solution

1277
01:27:58,860 --> 01:27:59,900
so if we do that's

1278
01:28:01,260 --> 01:28:02,400
it's as if

1279
01:28:02,730 --> 01:28:04,710
we were getting observations

1280
01:28:05,210 --> 01:28:05,550
the john

1281
01:28:05,990 --> 01:28:06,960
noises noisy

1282
01:28:07,590 --> 01:28:11,990
in the true observation is whited out from have a weighted a given scene

1283
01:28:12,510 --> 01:28:12,880
but then

1284
01:28:14,030 --> 01:28:16,780
because we don't know what to do with the white noise

1285
01:28:17,730 --> 01:28:21,990
it's a sign from cape silent we do get this convolution

1286
01:28:23,920 --> 01:28:25,280
term into the posterior

1287
01:28:25,960 --> 01:28:27,320
okay so in that respect

1288
01:28:28,670 --> 01:28:30,240
maybe see is an exact

1289
01:28:30,820 --> 01:28:33,490
vision competition but following noisy

1290
01:28:35,210 --> 01:28:38,510
causes has drawbacks has a perspective because

1291
01:28:39,490 --> 01:28:40,470
wrote something that's

1292
01:28:40,470 --> 01:28:46,970
solids have a random arrangement an asterisk next to random because it's not totally random

1293
01:28:47,260 --> 01:28:50,060
there's some local water but there's no long range order

1294
01:28:50,520 --> 01:28:59,090
so the order is short range and what we call such solids that ordered solids

1295
01:28:59,100 --> 01:29:02,340
recalled crystalline solids and we playing

1296
01:29:02,840 --> 01:29:07,560
word everyday word for such an ordered solid it's called a crystal and I've been

1297
01:29:07,560 --> 01:29:08,840
using that term alive

1298
01:29:09,090 --> 01:29:15,700
I talked about Davison radiating and nickel crystal glass to distinguish the material for something

1299
01:29:15,700 --> 01:29:21,590
that's disorders this disorder we say that the atomic arrangement is amorphous and as a

1300
01:29:21,590 --> 01:29:28,230
simple anglo-saxon word for amorphous solid glass glasses an amorphous solid you might think glass

1301
01:29:28,710 --> 01:29:34,760
has to do with whether something is transparent and I wanted disabuse you that we've

1302
01:29:34,760 --> 01:29:40,320
talked about transparency of talked about transparency and what we know about transparency if we

1303
01:29:40,320 --> 01:29:44,190
want to talk about transparency of something is transparent

1304
01:29:45,530 --> 01:29:51,470
transparent what do we do we ask ourselves as the band gap and that material

1305
01:29:51,520 --> 01:29:56,580
greater than 3 electron volts if the answer is yes it's transparent

1306
01:29:56,770 --> 01:29:59,640
it's less than 3 electron volts that's an absorbed

1307
01:29:59,840 --> 01:30:02,260
so what is glossary

1308
01:30:02,270 --> 01:30:05,340
glass glass means no

1309
01:30:05,450 --> 01:30:13,060
long range work on a long range order will give you some examples what we

1310
01:30:13,060 --> 01:30:14,180
could say

1311
01:30:14,230 --> 01:30:23,370
for example we've got diamond a diamond diamond is critical dynamism is crystal and yet

1312
01:30:23,370 --> 01:30:32,790
it's transparent it's transparent wiser transparent band gap about 5 . 4 electron volts obsidian

1313
01:30:32,940 --> 01:30:34,140
of cities

1314
01:30:34,320 --> 01:30:36,450
this is a glossary

1315
01:30:38,100 --> 01:30:39,910
glassy rock

1316
01:30:39,970 --> 01:30:42,210
it's got no long range order

1317
01:30:42,230 --> 01:30:49,950
it's an amorphous material and it is opaque obsidian is so here's an example of

1318
01:30:49,950 --> 01:30:55,080
an opaque glass in a transparent crystal so as of now I wanna make sure

1319
01:30:55,080 --> 01:31:01,330
that nobody in 309 1 from this day forward ever ever says glass is transparent

1320
01:31:01,340 --> 01:31:06,320
I will show you before the end of the month metallic glass metallic glass how

1321
01:31:06,320 --> 01:31:11,180
can it be a Madeleine glassy well it must mean that there are metallic bodice

1322
01:31:11,180 --> 01:31:16,350
metallic bonding operative but the patterns are not in regular arrangement and that material was

1323
01:31:16,350 --> 01:31:22,660
not transparent visible light because you know that metals have no band gap whatsoever so

1324
01:31:22,660 --> 01:31:28,710
something isn't something important something important here to retain so

1325
01:31:30,060 --> 01:31:34,160
this parents will now learn what the students have learned when we go to a

1326
01:31:34,160 --> 01:31:38,160
new unit we begin with a history lesson so let's go back in the way

1327
01:31:38,160 --> 01:31:44,580
Back machine will go back to early efforts and trying to describe ordered solids

1328
01:31:44,590 --> 01:31:49,850
so that's why crystallography is the study of ordered solids crystal comes from the greek

1329
01:31:49,940 --> 01:31:57,160
crystal comes from the greek and personal which is 1 of the top terms that

1330
01:31:57,370 --> 01:32:01,680
might refer to isis certain former Vice but anyways it's

1331
01:32:01,730 --> 01:32:04,920
that's the origin so let's go back to the 1st

1332
01:32:04,940 --> 01:32:08,320
marking point in history of robin hood

1333
01:32:08,420 --> 01:32:14,470
Robert look back in the 1616 was studying cannonballs they were doing military research are

1334
01:32:14,630 --> 01:32:18,970
factor when you think about it and he posited that a crystal almost all its

1335
01:32:18,970 --> 01:32:23,110
regular shape to the packing of spherical particles

1336
01:32:23,130 --> 01:32:28,820
packing of spherical particles so Tupac regularly you'll get long-range order then about a decade

1337
01:32:28,820 --> 01:32:33,870
later there was Neal Stephenson who was a danger spent most of his scientific career

1338
01:32:33,870 --> 01:32:37,700
in Italy and he observed that quartz crystal

1339
01:32:38,340 --> 01:32:43,960
had the same angles between corresponding faces regardless of the size

1340
01:32:43,970 --> 01:32:47,420
the see what they were looking for is trying to make a connection between the

1341
01:32:47,420 --> 01:32:52,830
macroscopic and the atomic world in other words here is a simple question if I

1342
01:32:52,830 --> 01:32:58,830
see something has a macroscopic shape very regular and cubic can i infer from that

1343
01:32:58,960 --> 01:33:03,630
that if I divide divided by divide divide if I get down to atomic dimensions

1344
01:33:03,630 --> 01:33:09,560
there will be some cubic repeat unit yes-or-no that's the question and then around the

1345
01:33:09,560 --> 01:33:16,990
end of the century Christian wagons in 1690 studying calcite crystals made drawings of atomic

1346
01:33:17,250 --> 01:33:19,400
packing and ball shape

1347
01:33:19,660 --> 01:33:24,100
and here's a drawing from his book in 1690 and time if I could have

1348
01:33:24,100 --> 01:33:27,960
that they could switch the video over to the euro

1349
01:33:28,080 --> 01:33:36,400
document projector calcite crystal here here's the calcite crystals

1350
01:33:37,330 --> 01:33:46,110
right and no of focus of focuses on our focus is on OK so now

1351
01:33:46,110 --> 01:33:50,040
you can see this is to look at the regularity of this imagine you're looking

1352
01:33:50,040 --> 01:33:54,090
at this is all you have to work with there's no scanning electron microscope there's

1353
01:33:54,090 --> 01:33:59,280
no meaning national stalled nothing that's you got to work with and when he comes

1354
01:33:59,280 --> 01:34:04,440
up with time if we can go back to the computer video please so starting

1355
01:34:04,440 --> 01:34:08,610
with the crystal this is what he posits was not that remember this wagons is

1356
01:34:08,610 --> 01:34:13,470
the same 1 that gave us optics you see these people were polymaths narrow specialist

1357
01:34:13,470 --> 01:34:14,680
they were generally

1358
01:34:14,970 --> 01:34:20,590
the substantial contributions in more than 1 field Tom let's go back to the to

1359
01:34:20,590 --> 01:34:25,470
the document camera was good but the document camera with so here's here's a piece

1360
01:34:25,480 --> 01:34:31,950
of 10 right so if I had to draw this piece of 10 I might

1361
01:34:31,950 --> 01:34:35,630
draw a different drawing from 1 avoidance true because you can see the angles of

1362
01:34:35,650 --> 01:34:40,940
different here look at these right

1363
01:34:40,950 --> 01:34:42,780
well here's a piece of their own

1364
01:34:43,040 --> 01:34:51,610
this is a beryllium aluminum silicate litter that that's a hexagonally and almost so

1365
01:34:52,200 --> 01:34:59,560
what's going on here is a piece a cry like

1366
01:34:59,630 --> 01:35:01,950
some important dropped

1367
01:35:02,080 --> 01:35:10,340
the bad data measured from city to have will of the samples cutting edge research

1368
01:35:10,620 --> 01:35:12,370
on the

1369
01:35:12,420 --> 01:35:14,510
OK Tom let's go back to the

1370
01:35:15,610 --> 01:35:20,230
computer before I digress right so here we are in the here's a here's another

1371
01:35:20,230 --> 01:35:23,300
example I could bring this 1 and is a big chance of being sold off

1372
01:35:23,300 --> 01:35:29,440
the coast of Iceland and you can see they have they have others forms

1373
01:35:29,700 --> 01:35:34,580
so so far so good now let's go into the about a hundred years later

1374
01:35:34,900 --> 01:35:40,920
over to france this is running is used only he's studying at Sorbonne and he

1375
01:35:40,920 --> 01:35:45,580
studied the cleavage of calcite basically what he does he comes into work every day

1376
01:35:45,720 --> 01:35:49,970
breaks things in any breaks the cleveland breaks the calcite and he looks at the

1377
01:35:49,970 --> 01:35:55,850
little pieces of calcite and what he observes is that all shards around does matter

1378
01:35:55,850 --> 01:36:02,400
how small he breaks the calcite crystals down he always gets from medieval shots so

1379
01:36:02,400 --> 01:36:09,280
then he says I'm gonna posit that that's the basic unit of calcite and furthermore

1380
01:36:09,750 --> 01:36:15,180
because he's a Frenchman and the steeped in mathematical tradition he decides to to model

1381
01:36:15,180 --> 01:36:20,060
this mathematically says OK we we all know that if we wanted to fill 3

1382
01:36:21,250 --> 01:36:26,040
with the same unit we could choose a number of shapes right we could start

1383
01:36:26,040 --> 01:36:29,900
with acute and I could fill this role of cubes of identical size I could

1384
01:36:29,900 --> 01:36:31,300
fill this room with

1385
01:36:31,340 --> 01:36:36,740
server or that boxes and also of rectangular but square cross section I could still

1386
01:36:36,740 --> 01:36:41,750
have something is rectangular with a rectangular cross-section I could do something goes wrong behavior

1387
01:36:41,750 --> 01:36:48,350
hard-liner look at cross-licensing holes here so what what did all he did is he

1388
01:36:48,350 --> 01:36:54,970
said mathematically I want to determine what is the maximum number of distinguishable shapes that

1389
01:36:54,970 --> 01:37:00,450
and so this turns out to be equivalent to minimisation over a finite dimensional space

1390
01:37:00,450 --> 01:37:06,310
of dimension n so practice and is not obvious but this is what was

1391
01:37:06,370 --> 01:37:11,140
like you said support vector machines remember that your decision was was over

1392
01:37:11,180 --> 01:37:14,680
victor i found that mention the number of points this is the same thing here

1393
01:37:14,680 --> 01:37:18,470
OK and this would be true for all kind of falconer method

1394
01:37:18,490 --> 01:37:21,780
as an argument about this thing you know that the solution will be indexed by

1395
01:37:21,870 --> 01:37:26,350
numbers so you can write this as a minimisation of fractal dimension n

1396
01:37:26,430 --> 01:37:28,430
and then you need to minimize this

1397
01:37:29,240 --> 01:37:32,240
they want to minimize the depends on the loss function

1398
01:37:33,430 --> 01:37:36,970
and convex loss function then

1399
01:37:36,990 --> 01:37:41,760
this is be a convex problem min this can be solved by and

1400
01:37:41,850 --> 01:37:44,330
i mean that

1401
01:37:44,350 --> 01:37:47,050
OK so so scenario of this

1402
01:37:47,200 --> 01:37:49,450
is the following

1403
01:37:50,180 --> 01:37:54,930
i define you know we started from the idea of what is can positive can

1404
01:37:54,930 --> 01:38:01,430
also you can actually can ask to properties has to be symmetric and positive definite

1405
01:38:01,450 --> 01:38:05,510
and when you give you can also wants to able to give you a

1406
01:38:05,530 --> 01:38:08,660
but if you define function in the set on any set that you want to

1407
01:38:09,510 --> 01:38:13,800
then in fact you comes with many things the first thing is that it comes

1408
01:38:13,800 --> 01:38:16,800
with some implicit mapping of your initial

1409
01:38:16,930 --> 01:38:20,070
space to human space so implicitly defined

1410
01:38:20,080 --> 01:38:25,140
this tendency is to define inner products your initial space thanks to the kernel

1411
01:38:25,160 --> 01:38:30,120
of course you don't you don't want again you don't assume anything before that

1412
01:38:30,140 --> 01:38:33,850
OK the space can be do whatever you want to overcome all you have implicitly

1413
01:38:33,850 --> 01:38:37,200
embed it into deep space

1414
01:38:37,510 --> 01:38:41,140
the second thing that comes with the RKHS and so we saw that can be

1415
01:38:41,140 --> 01:38:44,260
methods that the public commissions and many others in fact

1416
01:38:44,330 --> 01:38:49,100
use the RKHS because they do some of the musicians RKHS and they put it

1417
01:38:49,100 --> 01:38:52,430
but used in on the RKHS which would be

1418
01:38:52,430 --> 01:38:56,350
thought of as a measure of smoothness with respect to the metric induced by the

1419
01:38:57,640 --> 01:39:00,410
so the which

1420
01:39:00,470 --> 01:39:06,260
that we try to to study the in the rest of the lectures

1421
01:39:06,390 --> 01:39:10,930
is soul so that there was a a question for as several years so is

1422
01:39:10,930 --> 01:39:12,640
what is it that we should use

1423
01:39:12,660 --> 01:39:16,180
OK even for pictorial structure is very not can answer those question

1424
01:39:16,280 --> 01:39:21,370
how do i put all the put whatever it turns out that you see that

1425
01:39:21,370 --> 01:39:24,780
when you change now you can get some additional what happens when you change we

1426
01:39:24,780 --> 01:39:27,180
can all change our only

1427
01:39:27,200 --> 01:39:30,970
you can see that changing the way you embed your your point into the space

1428
01:39:31,470 --> 01:39:34,700
you can also see that changing the way you measure the smoothness

1429
01:39:35,870 --> 01:39:37,430
for instance in the

1430
01:39:37,490 --> 01:39:41,010
you can tell you can only done here

1431
01:39:41,010 --> 01:39:45,620
get the camera is not here not in the first that can only appears in

1432
01:39:45,640 --> 01:39:48,990
the norm of the RKHS so when you change the kernel

1433
01:39:49,010 --> 01:39:50,550
you can see that changing

1434
01:39:50,550 --> 01:39:54,140
the number RKHS where you define a function of most

1435
01:39:54,160 --> 01:39:59,510
and iteratively change canon is that you will focus on different functions of focus in

1436
01:39:59,510 --> 01:40:00,990
the sense a bit of you

1437
01:40:01,050 --> 01:40:04,870
bayesian prior like you need to say in which part of the space you want

1438
01:40:04,870 --> 01:40:05,680
to look at

1439
01:40:05,700 --> 01:40:07,720
here you will look at this function

1440
01:40:07,720 --> 01:40:12,100
and tracing can amounts to define what you mean by small

1441
01:40:12,120 --> 01:40:15,830
so it seems some examples that when you as for some reason if you have

1442
01:40:15,830 --> 01:40:18,200
some prior knowledge that the ocean you looking for

1443
01:40:18,240 --> 01:40:20,070
should be smooth

1444
01:40:20,080 --> 01:40:23,550
with some notion you have then you it may be interesting to find it can

1445
01:40:23,550 --> 01:40:28,050
also that this was this measure but can all is equivalent to what you think

1446
01:40:28,050 --> 01:40:30,120
should be

1447
01:40:33,160 --> 01:40:37,240
so it's probably quite a bit fast and i hope i can use to many

1448
01:40:37,240 --> 01:40:38,390
people here but

1449
01:40:38,470 --> 01:40:42,660
what's important is that we know what to with the topic and just focus given

1450
01:40:42,660 --> 01:40:44,950
his background on how to define kernels

1451
01:40:44,970 --> 01:40:49,030
not vectors but things which are strings which are crossed

1452
01:40:49,030 --> 01:40:53,240
but before this i can take any question they are

1453
01:40:56,890 --> 01:41:00,530
this is

1454
01:41:00,620 --> 01:41:07,330
what's the

1455
01:41:11,100 --> 01:41:16,030
OK so and so this is a

1456
01:41:16,120 --> 01:41:21,470
good and difficult question because there's no simple answer so for that is the same

1457
01:41:21,470 --> 01:41:23,430
as you as you

1458
01:41:23,430 --> 01:41:27,300
so the question is not so much an argument also like other which was seen

1459
01:41:27,300 --> 01:41:33,100
as you well i mean in theory you can have some theoretical ideas on how

1460
01:41:33,300 --> 01:41:34,640
that could be taken

1461
01:41:34,660 --> 01:41:35,850
for instance

1462
01:41:36,450 --> 01:41:40,990
here typically long that should decrease with n because most of the reasons for the

1463
01:41:40,990 --> 01:41:45,570
number of points increases you know that you have to register position of consistency is

1464
01:41:45,570 --> 01:41:50,580
that there are so you have some ideas there's about that but in practice i

1465
01:41:50,580 --> 01:41:52,030
am not aware of any

1466
01:41:52,050 --> 01:41:54,410
a good rule of thumb

1467
01:41:54,490 --> 01:41:58,330
is better world than just train does

1468
01:41:58,350 --> 01:42:00,510
evaluating the performance by

1469
01:42:00,510 --> 01:42:03,510
what you are supposed to be here is is because the shows could be whatever

1470
01:42:03,510 --> 01:42:06,700
you want and then make it shows choice

1471
01:42:06,720 --> 01:42:08,390
after that

1472
01:42:08,410 --> 01:42:10,430
so typically lambda is

1473
01:42:10,640 --> 01:42:13,930
of course there is the choice of the council on the twenty canisters and all

1474
01:42:13,930 --> 01:42:16,220
the chemicals have only one parameter

1475
01:42:16,240 --> 01:42:17,390
this and

1476
01:42:17,470 --> 01:42:23,160
and there is no automatic ways what to truth that there are many heuristics can

1477
01:42:23,180 --> 01:42:30,350
the books but i don't have any better accuracy than trying different values

1478
01:42:30,490 --> 01:42:38,740
so the exact meaning is

1479
01:42:38,790 --> 01:42:47,030
so probably to be precisely equal to to one over another

1480
01:42:47,100 --> 01:42:49,970
OK because you can just right

1481
01:42:50,360 --> 01:42:55,240
and so minimizing this is the same as removing designed writing one of our land

1482
01:42:55,260 --> 01:42:58,700
and and in the information you minimize

1483
01:42:58,720 --> 01:43:03,330
the norm so usually is just an number the w same as foreign fourteen functions

1484
01:43:03,550 --> 01:43:07,070
that the number of the new list see time

1485
01:43:07,080 --> 01:43:09,970
the set five which is what in the

1486
01:43:10,010 --> 01:43:14,080
as you because of the sea is one over one other than that

1487
01:43:14,100 --> 01:43:19,550
so we were walking to school large seemingly small

1488
01:43:20,140 --> 01:43:21,450
here here the

1489
01:43:21,470 --> 01:43:26,200
for more on regulations although that means you regularizer

1490
01:43:26,260 --> 01:43:29,470
so we have a very small function and this is the kind of small c

1491
01:43:29,720 --> 01:43:32,310
in as

1492
01:43:32,330 --> 01:43:34,530
other questions

1493
01:43:34,800 --> 01:43:39,680
so this is what you're saying she

1494
01:43:39,700 --> 01:43:42,350
you know

1495
01:43:44,490 --> 01:43:51,030
chris to do the same

1496
01:43:51,070 --> 01:43:58,470
the reason for this is the reason

1497
01:43:58,470 --> 01:44:01,530
and this

1498
01:44:01,550 --> 01:44:06,740
this is some sort of sense

1499
01:44:06,850 --> 01:44:10,700
because this is certainly

1500
01:44:14,220 --> 01:44:16,640
good news

1501
01:44:16,660 --> 01:44:17,740
well is

1502
01:44:17,910 --> 01:44:21,570
this is the

1503
01:44:21,570 --> 01:44:26,260
so that that's the by the size and shape of the world's most and the

1504
01:44:26,260 --> 01:44:28,660
best one to the what was that is user

1505
01:44:29,680 --> 01:44:35,830
are you can also gaussian come off because interestingly we will see that the function

1506
01:44:35,870 --> 01:44:42,330
is smooth if you don't have to consist now i decided to use the smoothness

1507
01:44:42,330 --> 01:44:47,980
so my what the way i'm doing it i going think the average case so

1508
01:44:48,360 --> 01:44:52,860
when i say i mean when i say i want to optimize the expected reward

1509
01:44:52,870 --> 01:44:56,560
that that everything works out if i would be working on some kind of adversarial

1510
01:44:56,560 --> 01:44:58,980
setting then things get more tricky

1511
01:45:00,360 --> 01:45:04,730
OK so this is this is a good news right so

1512
01:45:04,770 --> 01:45:07,730
something that we can't solve exactly now we know that we can sort of of

1513
01:45:07,730 --> 01:45:11,180
solve it by paying only a constant factor of

1514
01:45:11,190 --> 01:45:15,840
and know i can just briefly tell you why why our functions such someone to

1515
01:45:16,000 --> 01:45:17,440
right so we have this structure

1516
01:45:17,480 --> 01:45:21,880
so basically the adding benefit of adding a sensor to a small placement is larger

1517
01:45:21,880 --> 01:45:26,070
than the benefit of adding set in the same sense of to the larger placement

1518
01:45:27,380 --> 01:45:32,370
and so the question is why why does our problem have structure so what do

1519
01:45:32,370 --> 01:45:35,680
we know about submodular functions the first thing that we know is that if we

1520
01:45:35,680 --> 01:45:39,000
have a set of all it is submodular functions and we just create a linear

1521
01:45:39,000 --> 01:45:43,320
combination of them this new function will be also submodular and then second is this

1522
01:45:43,320 --> 01:45:48,350
natural natural example of a submodular functions right when we say we have these sets

1523
01:45:48,360 --> 01:45:51,790
and the reward function is the size of the union of the sets

1524
01:45:53,250 --> 01:45:57,580
so this is as sets of sets a and this is now the

1525
01:45:57,590 --> 01:46:00,960
the set of sets and now we can ask how much this

1526
01:46:00,970 --> 01:46:04,760
how much how much benefit to get from place in this new new setbacks and

1527
01:46:04,760 --> 01:46:08,470
again right we can clearly see that when adding it to the to to this

1528
01:46:08,470 --> 01:46:09,610
set of sets

1529
01:46:09,620 --> 01:46:14,010
the benefit we are getting of place index is much larger than when when we

1530
01:46:14,010 --> 01:46:18,980
would place it to a larger sets of sets so and this is clearly the

1531
01:46:18,980 --> 01:46:21,180
case we have no problem

1532
01:46:21,840 --> 01:46:25,860
so the first thing and i won't go into too much detail is that so

1533
01:46:25,860 --> 01:46:29,750
this objective functions that i was talking about what we find is that of the

1534
01:46:29,750 --> 01:46:35,750
water sensor networks which is a competition organised by i think US environmental protection agency

1535
01:46:35,750 --> 01:46:41,260
or something and they find these three objective functions and first thing so that the

1536
01:46:41,260 --> 01:46:45,610
detection like how long does it take to detect a contamination detection likelihood means how

1537
01:46:45,610 --> 01:46:50,280
many contaminations we detect out of all possible contamination that could happen and population affected

1538
01:46:50,290 --> 01:46:54,980
is how many people drank contaminated water before we detect and we show that all

1539
01:46:54,980 --> 01:47:00,530
these nodes are submodular so all these different objective functions have this property of diminishing

1540
01:47:04,640 --> 01:47:05,900
as i told you

1541
01:47:05,910 --> 01:47:10,790
that greedy hill climbing algorithm can be constant factor approximation i can just show you

1542
01:47:10,790 --> 01:47:13,890
what i mean by hill climbing right so the idea is that if i had

1543
01:47:13,890 --> 01:47:19,380
my water distribution network i e i would go on forever is for every possible

1544
01:47:19,380 --> 01:47:23,390
location we would consider how much you want to get right now we will behave

1545
01:47:23,390 --> 01:47:28,700
greedily which means we select the the sense that give was the most rewards what

1546
01:47:28,710 --> 01:47:31,960
we do next is basically we need to go

1547
01:47:31,980 --> 01:47:36,820
and evaluate the benefits of having

1548
01:47:36,840 --> 01:47:40,790
of adding additional sensors to something that you already placed right in this would be

1549
01:47:41,110 --> 01:47:44,860
this gives us a new set of rewards and again the maximum

1550
01:47:44,900 --> 01:47:48,750
we would need now to again going to evaluate all this simply the highest one

1551
01:47:48,750 --> 01:47:52,360
and added to the place and united have this is what i mean by future

1552
01:47:52,360 --> 01:47:58,080
planning so just like greedy algorithm and as old as i said we have constant

1553
01:47:58,080 --> 01:48:04,410
factor approximation so this is the worst case sixty three percent of optimal but there

1554
01:48:04,410 --> 01:48:08,250
are two parts to the story first but is that this only works for unit

1555
01:48:08,250 --> 01:48:12,790
cost case so the court the case where all the locations have the same cost

1556
01:48:12,810 --> 01:48:17,480
OK so we don't have different costs for placing sensors in different locations and the

1557
01:48:19,220 --> 01:48:22,780
two point seven

1558
01:48:22,810 --> 01:48:29,690
and then the other thing is that this climbing algorithm is slow and the reason

1559
01:48:29,690 --> 01:48:34,110
why it slows because for whenever places as we have two in the re-evaluate all

1560
01:48:34,110 --> 01:48:38,320
other sort of marginal benefits you would get from adding additional sensors again we need

1561
01:48:38,320 --> 01:48:41,210
to place the document one and then the

1562
01:48:41,220 --> 01:48:45,850
the one the highest one again re-evaluate everything right so so this is this is

1563
01:48:45,850 --> 01:48:47,290
scale where

1564
01:48:48,820 --> 01:48:52,090
so how can we go and solve this problem so

1565
01:48:52,480 --> 01:48:56,910
they said if if if i would just use hill climbing in in order to

1566
01:48:56,910 --> 01:49:01,020
cost so just pretend that there is no cost and try to the best sensors

1567
01:49:01,290 --> 01:49:05,480
during the cost to sort of try to respect the budget then this will fail

1568
01:49:05,480 --> 01:49:11,280
arbitrarily badly and the reason why this will fail is that we will always prefer

1569
01:49:11,280 --> 01:49:16,070
more more expensive sensitivity with the reward are i know also much cheaper sensor because

1570
01:49:16,160 --> 01:49:18,650
reward our mindsets now and now

1571
01:49:18,660 --> 01:49:23,200
this can get arbitrarily bad if we ignore the cost so and here's the here's

1572
01:49:23,210 --> 01:49:25,650
the idea that what we can do to

1573
01:49:25,660 --> 01:49:29,160
but what what one could do to to solve better so what if we instead

1574
01:49:29,160 --> 01:49:33,700
of just optimizing the reward optimize these benefit cost ratio right so we would go

1575
01:49:34,010 --> 01:49:40,200
and everything with the sensor that gives us best best increase in the in the

1576
01:49:40,200 --> 01:49:45,230
war and divided by the cost of that sense so we would sort of sense

1577
01:49:45,230 --> 01:49:48,190
so the distribution is going to result from this

1578
01:49:48,190 --> 01:49:52,480
right if you places into the expression of the milan is one the the exponential

1579
01:49:53,210 --> 01:49:54,750
a time c

1580
01:49:54,770 --> 01:49:58,050
right this is true is going to be added otherwise it's not

1581
01:49:58,230 --> 01:49:59,530
plus the

1582
01:49:59,550 --> 01:50:01,670
o b i f i c

1583
01:50:02,480 --> 01:50:06,570
these are the features if i see that i find these other weights

1584
01:50:06,590 --> 01:50:09,030
so this is the

1585
01:50:09,190 --> 01:50:13,130
this is the of what is the walls of the class going to be

1586
01:50:13,150 --> 01:50:17,150
well it's going to be the log of the ratio between p of c

1587
01:50:17,170 --> 01:50:20,320
he calls one of the few sequels era

1588
01:50:20,320 --> 01:50:25,320
when p once sequel zero this becomes and becomes zero so this is exponential of

1589
01:50:26,210 --> 01:50:31,400
which is why when sequels one well at times one is a and b i

1590
01:50:31,650 --> 01:50:33,480
times one is just by five

1591
01:50:33,500 --> 01:50:36,630
so i get exponential of a plus b i five k

1592
01:50:36,630 --> 01:50:40,210
so now this is one of the two councils have some expression that have up

1593
01:50:42,150 --> 01:50:45,230
so this is a very simple example why

1594
01:50:45,340 --> 01:50:47,610
ML and can represent

1595
01:50:47,650 --> 01:50:49,770
one of these statistical models

1596
01:50:49,790 --> 01:50:51,530
OK now here

1597
01:50:51,550 --> 01:50:53,880
i used conjunction features

1598
01:50:53,900 --> 01:50:57,650
which if that comes in statistical modeling and in the maximum entropy models and so

1599
01:50:57,650 --> 01:51:00,730
forth is probably what you'll find most intuitive

1600
01:51:00,800 --> 01:51:03,340
if you on in logical in ILP

1601
01:51:03,360 --> 01:51:08,150
you probably find this for more intuitive which is the implication for

1602
01:51:08,170 --> 01:51:13,020
the application forms to write that for each feature that that the future implies the

1603
01:51:14,670 --> 01:51:18,520
so like the natural language descriptions this is well if the page and has this

1604
01:51:18,520 --> 01:51:23,020
feature then she has or does not have breast cancer for example

1605
01:51:23,380 --> 01:51:26,210
and the reason can do things in this way

1606
01:51:26,250 --> 01:51:28,250
is the forward

1607
01:51:28,300 --> 01:51:29,090
it is

1608
01:51:29,110 --> 01:51:30,690
if this is evidence

1609
01:51:30,710 --> 01:51:32,500
then this reduces to the class

1610
01:51:32,550 --> 01:51:34,000
just like this

1611
01:51:35,250 --> 01:51:37,480
so even though these formulas are different

1612
01:51:37,500 --> 01:51:40,940
one to using these as evidence that become the same

1613
01:51:40,960 --> 01:51:43,070
the deep reason is the following

1614
01:51:43,070 --> 01:51:44,300
is there

1615
01:51:44,340 --> 01:51:47,360
all i'm using these features for

1616
01:51:47,360 --> 01:51:48,480
it is to build

1617
01:51:48,500 --> 01:51:49,380
the model

1618
01:51:49,380 --> 01:51:52,000
for the joint distribution of the class and the variables

1619
01:51:52,000 --> 01:51:56,000
and it turns out that if i have the unit clauses as well

1620
01:51:56,000 --> 01:52:00,420
any of these binary features we can construct like this conjunction of this implication or

1621
01:52:00,420 --> 01:52:05,480
any of the other things like conductors is sufficient to completely model distribution

1622
01:52:05,500 --> 01:52:09,400
so the weights will be different but we will have three parameters the weight of

1623
01:52:09,400 --> 01:52:12,570
the joint feature the way to the unit clauses

1624
01:52:12,630 --> 01:52:16,230
it is it's easy to see if they can represent any joint distribution of those

1625
01:52:16,230 --> 01:52:20,650
to available so which this you want to use it's really a matter of taste

1626
01:52:20,730 --> 01:52:23,340
here i'm going to use the implications since

1627
01:52:23,360 --> 01:52:27,900
it seems more intuitive in terms of what it's saying that if you prefer conjunctions

1628
01:52:27,900 --> 01:52:31,520
you know just replaced the implications by conventions throughout

1629
01:52:32,670 --> 01:52:35,400
here's a simple concrete applications

1630
01:52:35,400 --> 01:52:38,820
let's suppose we want to do text classification

1631
01:52:38,840 --> 01:52:42,500
OK here's an email in the test specification

1632
01:52:42,520 --> 01:52:46,630
you have three types page from one to the number of pages were which are

1633
01:52:46,630 --> 01:52:50,020
all the words in the language and topic which are the topic so you want

1634
01:52:50,020 --> 01:52:54,630
to classify pages into like politics sports science whatever

1635
01:52:54,670 --> 01:52:56,650
and they have two predicates

1636
01:52:56,710 --> 01:52:59,440
you have the predicates topic page topic

1637
01:52:59,460 --> 01:53:04,530
the next mission like this means that each page can has exactly one topic

1638
01:53:05,250 --> 01:53:08,710
now in some cases the page could have more than one topic in which case

1639
01:53:08,750 --> 01:53:10,590
you can remove the explanation

1640
01:53:10,590 --> 01:53:13,820
here i'm just going to look at the simple case where the topics are mutually

1641
01:53:13,820 --> 01:53:17,960
exclusive and exhaustive and then i just try to get has word page one which

1642
01:53:17,960 --> 01:53:20,150
means that this page is this one

1643
01:53:20,820 --> 01:53:22,940
so these all the predicates i need

1644
01:53:22,940 --> 01:53:25,210
now here's my complete mln

1645
01:53:25,250 --> 01:53:26,750
i have a unit clause

1646
01:53:26,770 --> 01:53:30,570
that says that by default the page does not have the topic which is true

1647
01:53:30,610 --> 01:53:34,210
most pages are not on most topics that i have

1648
01:53:34,250 --> 01:53:35,770
a single

1649
01:53:35,920 --> 01:53:37,590
for the cells

1650
01:53:37,610 --> 01:53:41,650
has p plus w implies topic people st

1651
01:53:42,270 --> 01:53:45,500
what is this doing since i have a plus

1652
01:53:45,520 --> 01:53:47,050
here in the past year

1653
01:53:47,070 --> 01:53:51,400
i'm going to grounding into this formula for every word topic here

1654
01:53:51,400 --> 01:53:57,290
so essentially in the matrix of parameters that's a highly indicative of each topic each

1655
01:53:58,730 --> 01:54:04,380
one hundred years precisely as we just saw is the logistic regression with across the

1656
01:54:04,380 --> 01:54:08,250
top topic of the page and the features are the words it contains

1657
01:54:09,230 --> 01:54:10,360
so doing standard

1658
01:54:10,380 --> 01:54:13,290
classifier in in

1659
01:54:13,320 --> 01:54:18,500
alchemy is really just writing to find in fact if you omit the things that

1660
01:54:18,500 --> 01:54:22,360
are optional it's really just this

1661
01:54:22,380 --> 01:54:24,340
OK so you can input this

1662
01:54:24,360 --> 01:54:25,440
two of me

1663
01:54:25,460 --> 01:54:26,880
with the database

1664
01:54:26,900 --> 01:54:29,820
of text into give a text classifier

1665
01:54:29,860 --> 01:54:31,550
just click to predicate

1666
01:54:31,570 --> 01:54:33,190
and the way formula

1667
01:54:33,190 --> 01:54:35,320
the need that by default

1668
01:54:36,000 --> 01:54:38,250
so this becomes very very simple

1669
01:54:38,270 --> 01:54:42,880
well now let's look at a more complex example which is to do hypertext classification

1670
01:54:42,900 --> 01:54:45,820
this is what you make the jump from i i d

1671
01:54:45,840 --> 01:54:52,110
well classifying each page separately to non i i d we're letting the the

1672
01:54:52,860 --> 01:54:56,550
topic of the page influence the topics of the page that links to

1673
01:54:56,570 --> 01:54:59,800
this is something that people for

1674
01:55:00,340 --> 01:55:03,900
know when the when to call was that if you actually used hyperlinks

1675
01:55:03,920 --> 01:55:08,860
to to help classify pages in addition to the words you get much better results

1676
01:55:08,920 --> 01:55:12,530
in fact even if you have only the labels for very few pages you could

1677
01:55:12,530 --> 01:55:16,000
get very good results on this because such thing for me

1678
01:55:16,090 --> 01:55:19,590
so in order to protect specification

1679
01:55:19,590 --> 01:55:20,500
in order to me

1680
01:55:20,520 --> 01:55:25,520
all we need to do is add one more predicate predicate links page page with

1681
01:55:25,520 --> 01:55:30,520
the obvious meaning one more formula which is topic pt

1682
01:55:30,520 --> 01:55:33,900
and links pp prime implies topic people ninety

1683
01:55:33,920 --> 01:55:36,110
this is

1684
01:55:36,190 --> 01:55:40,670
one pages on one topic on that page points to another page then that page

1685
01:55:40,670 --> 01:55:43,020
is probably on the same topic

1686
01:55:43,030 --> 01:55:47,250
OK and has some very that is will be reflected in the way that you

1687
01:55:47,250 --> 01:55:49,130
want for this formula from there

1688
01:55:49,130 --> 01:55:54,090
so nodes we can build a complete

1689
01:55:54,110 --> 01:55:55,790
hypertext classifiers

1690
01:55:55,840 --> 01:55:58,860
no i i d model in just two forms

1691
01:55:58,880 --> 01:56:00,960
contrast this with this paper

1692
01:56:00,980 --> 01:56:03,920
that was almost ten years ago when this was like you know twelve page two

1693
01:56:03,920 --> 01:56:07,690
column small paper about how to solve this problem

1694
01:56:07,770 --> 01:56:10,030
there's been a lot of work to do this

1695
01:56:10,090 --> 01:56:14,360
but with the now that we have all the bases incorporated into something like me

1696
01:56:14,400 --> 01:56:16,480
you know doing this complex

1697
01:56:16,480 --> 01:56:20,840
or apparently complex and the problem becomes a very very simple matter of writing down

1698
01:56:20,860 --> 01:56:22,730
a couple of points

1699
01:56:23,670 --> 01:56:27,050
well you can imagine using something called for information retrieval

1700
01:56:27,190 --> 01:56:32,380
of course this examples just for illustration purposes or can be doesn't scale to websites

1701
01:56:32,380 --> 01:56:37,860
that somehow the diameter of the influence of these models can have is limited to

1702
01:56:37,860 --> 01:56:41,820
be limited by the degree of conductivity in you know this this is said again

1703
01:56:41,820 --> 01:56:45,300
and again in vision committee still being said the people say well we've done a

1704
01:56:45,410 --> 01:56:49,490
low order markov models where the connections are short range now we should go to

1705
01:56:49,490 --> 01:56:55,410
higher order markov models because we want to get influences over the bigger diameter and

1706
01:56:55,430 --> 01:56:58,190
it may well be the case that one ought to go to higher order markov

1707
01:56:58,190 --> 01:57:01,380
models and the benefits from doing it but not for that reason it's not it's

1708
01:57:01,380 --> 01:57:07,030
not good enough to say that that we need to get influence over larger and

1709
01:57:07,360 --> 01:57:11,650
larger distance it must be something much more details about exactly the nature of that

1710
01:57:11,650 --> 01:57:13,430
in that influence

1711
01:57:13,450 --> 01:57:18,680
this is the

1712
01:57:20,010 --> 01:57:22,800
i can do i mean it will

1713
01:57:22,820 --> 01:57:26,050
you know i can actually what you know what you can do is set up

1714
01:57:26,050 --> 01:57:32,740
against markov random field where these were all the probability the gaussians and the the

1715
01:57:32,760 --> 01:57:38,610
the functions that think you know this is jumping ahead of it but the functions

1716
01:57:38,610 --> 01:57:39,320
that link the

1717
01:57:39,780 --> 01:57:44,470
adjacent nodes i guess you know that may not mean too much the materials used

1718
01:57:44,470 --> 01:57:47,740
and see what it means and everything about it is you know this the whole

1719
01:57:47,740 --> 01:57:51,610
thing in fact is a great big joining guassian and if now i solve for

1720
01:57:51,610 --> 01:57:55,510
the most probable state of such a model then it turns out the solution comes

1721
01:57:55,510 --> 01:58:00,630
out in bessel functions and the rational functions are asymptotically exponential so yes

1722
01:58:00,630 --> 01:58:05,610
it can it can work out the the the the the case exponential but

1723
01:58:05,630 --> 01:58:09,990
i don't think you could say you know in the case that said discrete model

1724
01:58:09,990 --> 01:58:14,260
that you very hard to i don't know if anyone can put their hand on

1725
01:58:14,260 --> 01:58:17,590
my heart and say that the decays exponentially

1726
01:58:17,610 --> 01:58:21,840
the wonderful if you could say that there is no

1727
01:58:21,840 --> 01:58:23,990
in fact this

1728
01:58:25,760 --> 01:58:27,280
the band

1729
01:58:38,010 --> 01:58:40,380
i don't completely understand the question

1730
01:58:56,800 --> 01:59:01,900
john you know i call you know that's a higher order models like i could

1731
01:59:01,900 --> 01:59:06,300
draw a graph like this one and i could make linkages for example the obvious

1732
01:59:06,300 --> 01:59:09,490
thing to do next would include the diagonal linkages and then i can make it

1733
01:59:09,490 --> 01:59:13,150
jump over one pixel like with without predictive text model and you know one could

1734
01:59:13,150 --> 01:59:15,300
certainly do that but

1735
01:59:15,320 --> 01:59:18,550
i guess what i want to say is that the effect of making those longer-range

1736
01:59:18,550 --> 01:59:23,760
linkage is on the range of correlations if you like on the on the distance

1737
01:59:23,760 --> 01:59:28,180
over which decisions then becoming joint is a non-obvious

1738
01:59:28,200 --> 01:59:34,820
linkage it's not just the case that by extending these conditional probability range conditional probabilities

1739
01:59:34,820 --> 01:59:40,820
the explicit you will then automatically extract exchange extends the diameter of the decision making

1740
01:59:40,840 --> 01:59:44,720
but i'm sure i can make a go backwards you know like yes i'm completely

1741
01:59:44,930 --> 01:59:48,510
like omega backwards that is i could take a model extend the range of the

1742
01:59:48,510 --> 01:59:52,820
links and make the diameter of which decisions are made this exponential fall for example

1743
01:59:53,110 --> 01:59:55,030
i can make it smaller

1744
02:00:01,110 --> 02:00:02,610
OK how do we

1745
02:00:02,990 --> 02:00:07,800
do this how we actually set up these models well

1746
02:00:07,840 --> 02:00:11,340
you know now look at the time has come actually to give an explicit algebraic

1747
02:00:11,340 --> 02:00:16,610
form for this joint probability distribution so here's the same graph again here's the conductivity

1748
02:00:16,610 --> 02:00:19,610
that said i was going to have and

1749
02:00:19,820 --> 02:00:25,410
you know in terms of conditional independence and so now i can write down the

1750
02:00:25,430 --> 02:00:30,130
joint probability distribution of this kind which is just a giant product over all these

1751
02:00:30,130 --> 02:00:34,170
possible pairs that have been marked by blue lines here

1752
02:00:34,180 --> 02:00:39,360
product over all of those guys and you know with some function which i haven't

1753
02:00:39,360 --> 02:00:42,720
specified yet so this is

1754
02:00:42,760 --> 02:00:44,030
this is going to be

1755
02:00:44,030 --> 02:00:49,630
a real valued function of two binary variables in the case of my segmentation problem

1756
02:00:49,630 --> 02:00:54,400
because you remember the binary variables in the second just foreground orbackground could represent was

1757
02:00:54,400 --> 02:00:56,410
ones or zeroes

1758
02:00:56,430 --> 02:00:58,490
or they will be an

1759
02:00:58,530 --> 02:01:03,510
other problems will come to later where there are where there's greater range for the

1760
02:01:03,510 --> 02:01:08,760
variables to cover for example i might want the variables to represent colours in an

1761
02:01:08,760 --> 02:01:11,720
image and you know there's a palette of two hundred fifty six

1762
02:01:11,720 --> 02:01:15,720
q colours and so maybe these labels will be the future activities q to them

1763
02:01:16,010 --> 02:01:20,130
or else i might want to explicitly to make the labels real numbers i might

1764
02:01:20,130 --> 02:01:24,180
be sort of because example that's just you color really is there is there is

1765
02:01:24,180 --> 02:01:27,430
the set of real numbers not not quantized thing so as this is this sort

1766
02:01:27,430 --> 02:01:31,630
of underlying model the unobserved part of the model you might say well you know

1767
02:01:31,650 --> 02:01:35,970
colours not really quantized there really real value so you know you can imagine wanting

1768
02:01:35,970 --> 02:01:37,860
these things to be real numbers but for now

1769
02:01:37,880 --> 02:01:43,550
we're going to restrict ourselves to having the hidden variables these these axes as

1770
02:01:44,320 --> 02:01:46,650
binary variables just zero or one

1771
02:01:46,680 --> 02:01:51,700
and i've got a proposal for what we might choose for these g functions that

1772
02:01:52,430 --> 02:01:57,590
neighboring pixels in the proposal is supposed to be something that captures this idea of

1773
02:01:57,590 --> 02:02:02,610
encouraging the neighbouring pixels to have the same labeling in this foreground background labeling problem

1774
02:02:02,820 --> 02:02:06,550
so here's my proposal that let's make

1775
02:02:08,260 --> 02:02:12,030
this function equal to one when the two neighbours have the same

1776
02:02:12,070 --> 02:02:17,670
label either background or foreground but we make it something less than one when they

1777
02:02:17,680 --> 02:02:20,840
have different labels so then the joint density

1778
02:02:20,840 --> 02:02:26,110
for this to this distribution will go down because the labels are different and the

1779
02:02:26,110 --> 02:02:34,240
extent to which goes down will be specified by this constant k

1780
02:02:35,030 --> 02:02:37,950
so now i could simulate from such a model

1781
02:02:37,950 --> 02:02:43,030
using a particular kind of MCMC simulated that sufficient for this this kind of thing

1782
02:02:43,030 --> 02:02:49,220
and i'm going to simulation for different values of k where i'm

1783
02:02:49,280 --> 02:02:51,470
changing the degree of penalty

1784
02:02:52,450 --> 02:02:57,650
allowing different labels adjacent pixels so here's the love value penalty and an increasing the

1785
02:02:58,010 --> 02:03:01,990
value of the penalty and you see now there's more encouragement for adjacent pixels to

1786
02:03:01,990 --> 02:03:06,170
take the same level labels of the blobs in the simulation is a typical sample

1787
02:03:06,170 --> 02:03:10,280
from the joint density is getting bigger and now increases even more

1788
02:03:10,300 --> 02:03:12,990
and the blobs are getting still because

1789
02:03:13,010 --> 02:03:17,650
and so you know i got a one parameter model here with some control over

1790
02:03:17,650 --> 02:03:20,820
the degree of coherence not to say that you know these things are looking very

1791
02:03:20,820 --> 02:03:23,490
much like typical objects are not going to go so far as to say that

1792
02:03:23,740 --> 02:03:28,070
you know captured the nature of the objects in the natural world with this very

1793
02:03:28,070 --> 02:03:31,630
simple one parameter model but you know may be doing something helpful in the way

1794
02:03:31,630 --> 02:03:36,320
of tipping the prior towards coherent and it turns out you can do a surprising

1795
02:03:36,320 --> 02:03:37,650
amount with this model

1796
02:03:37,680 --> 02:03:40,450
OK so now what i want to do is to take the model and do

1797
02:03:40,450 --> 02:03:45,050
like the speech people i want to embed it in a hidden markov model

