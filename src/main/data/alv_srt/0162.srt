1
00:00:00,000 --> 00:00:01,070
it's trying to do

2
00:00:01,090 --> 00:00:03,110
believe this is easy

3
00:00:03,120 --> 00:00:04,770
no hidden problems

4
00:00:04,790 --> 00:00:08,860
maximisation using derivatives and set to zero

5
00:00:08,900 --> 00:00:13,090
given that most likely set but instead there's a there are

6
00:00:13,100 --> 00:00:15,270
some issues of variables

7
00:00:15,300 --> 00:00:16,730
these theories are

8
00:00:16,740 --> 00:00:22,830
analytic candidate closed form so we apply the expectation maximisation algorithm

9
00:00:22,840 --> 00:00:27,880
which means maximize the lower bound on the log like sort of maximizing that maximise

10
00:00:27,890 --> 00:00:31,220
the log likelihood that's the case the exact same problem

11
00:00:31,290 --> 00:00:32,240
we apply

12
00:00:32,260 --> 00:00:34,430
jensen's inequality to rewrite

13
00:00:34,450 --> 00:00:36,520
this is the expected

14
00:00:36,550 --> 00:00:38,030
complete likelihood

15
00:00:38,040 --> 00:00:40,370
but some KL divergence between this

16
00:00:40,370 --> 00:00:44,530
proposal distribution and the posterior over the hidden variables given some

17
00:00:44,540 --> 00:00:49,080
so this turns out to be a lot easier to work with then this this

18
00:00:49,080 --> 00:00:53,120
is a function or some proposal distribution q when he very

19
00:00:53,130 --> 00:00:54,910
that i want

20
00:00:54,920 --> 00:01:00,010
maximize and some say that this is a function of two variables q theta which

21
00:01:00,010 --> 00:01:01,810
is always less than the original one

22
00:01:02,140 --> 00:01:03,720
the data

23
00:01:03,740 --> 00:01:06,850
so instead of maximizing one thousand

24
00:01:06,850 --> 00:01:10,700
or the fate of the log likelihood of data maximize this

25
00:01:10,730 --> 00:01:13,570
pounds which has two parameters

26
00:01:13,570 --> 00:01:18,790
this proposal distribution of what my guess is the distribution of the hidden variables

27
00:01:18,840 --> 00:01:21,580
that's the q and then the parameters for this

28
00:01:21,610 --> 00:01:23,470
lower bound

29
00:01:23,490 --> 00:01:25,030
so this looks like this

30
00:01:25,060 --> 00:01:28,670
axis parallel optimisation we maximize over q

31
00:01:28,700 --> 00:01:33,550
that pushes lowerbound maximization the we keep pushing upward slowly in

32
00:01:33,550 --> 00:01:36,310
increase by iteratively increasing this thing

33
00:01:36,320 --> 00:01:39,220
every set up here we first maximize over q

34
00:01:39,280 --> 00:01:43,980
this gives us the best rescue the best fit the best universities in order

35
00:01:45,560 --> 00:01:49,820
and this is easy to maximize their own easy to maximize here except

36
00:01:49,870 --> 00:01:54,110
i can get stuck in local minima so we you with the standard guess

37
00:01:54,130 --> 00:01:55,130
and know this

38
00:01:55,140 --> 00:02:00,140
q this status this each that is the best here

39
00:02:00,160 --> 00:02:04,140
the start of the place to get to the global optimum

40
00:02:04,150 --> 00:02:07,920
and what's going on in the e step for maximizing this functional

41
00:02:09,230 --> 00:02:12,050
distributions of hidden variables and parameters

42
00:02:12,060 --> 00:02:16,260
and it turns that the most likely the highest value of this function is one

43
00:02:16,370 --> 00:02:19,890
that the q distribution equal to the

44
00:02:19,920 --> 00:02:26,710
posterior distribution over the hidden variable given the observed variables and current guest tomorrow

45
00:02:26,720 --> 00:02:27,890
so the stakes

46
00:02:27,900 --> 00:02:29,790
the lower bound

47
00:02:29,790 --> 00:02:34,140
and maximize such that his log we surface

48
00:02:34,170 --> 00:02:35,070
this is the right

49
00:02:35,090 --> 00:02:36,370
functions as l

50
00:02:36,400 --> 00:02:37,870
the capital of functions

51
00:02:37,880 --> 00:02:43,430
it touches makes this were more than just the log likelihood and then we maximize

52
00:02:43,430 --> 00:02:46,800
i say that these are the people or lower bound

53
00:02:47,830 --> 00:02:50,420
maybe the best model

54
00:02:50,440 --> 00:02:52,030
see it is that

55
00:02:52,030 --> 00:02:52,730
and then you

56
00:02:52,760 --> 00:02:56,920
train your model so that's all there is to be seen as and the end

57
00:02:56,920 --> 00:02:58,960
of the day doesn't three he's that

58
00:02:58,980 --> 00:03:02,310
you need to do no matter how hard the network gets

59
00:03:02,340 --> 00:03:05,960
fundamentally maximizing this and maximizing this

60
00:03:05,990 --> 00:03:08,340
and running just to show them because

61
00:03:08,340 --> 00:03:10,890
we need xh given xt

62
00:03:10,950 --> 00:03:14,320
and that was the marginals conditionals we got a

63
00:03:16,100 --> 00:03:16,610
so that's the

64
00:03:17,090 --> 00:03:22,530
all closed world that we need in terms of bayes next tools obviously there's extra

65
00:03:24,110 --> 00:03:26,960
so we dynamic bayes nets will

66
00:03:26,980 --> 00:03:31,430
just to keep things simple that these are just things that are unrolled in time

67
00:03:31,440 --> 00:03:34,220
so of the network of static variables

68
00:03:34,230 --> 00:03:35,840
you've got this

69
00:03:35,850 --> 00:03:38,030
timestamp for every single variable

70
00:03:38,050 --> 00:03:41,940
and this is time people zero articles one to three was three

71
00:03:41,970 --> 00:03:43,620
and really was one

72
00:03:43,630 --> 00:03:49,850
something is happening in time represented as a random variable you know

73
00:03:49,860 --> 00:03:51,640
the probability distribution

74
00:03:51,670 --> 00:03:53,490
and this can continue indefinitely

75
00:03:53,580 --> 00:03:55,600
it's a really long time series

76
00:03:55,650 --> 00:03:56,530
so the

77
00:03:56,550 --> 00:04:02,620
two classical example of dynamic bayes nets are the hidden markov model over here

78
00:04:02,630 --> 00:04:04,420
this is an yellow means

79
00:04:04,450 --> 00:04:06,320
this is in state

80
00:04:06,330 --> 00:04:08,390
that involve markov dynamic

81
00:04:08,410 --> 00:04:11,650
so the current state depends on my pasta

82
00:04:11,660 --> 00:04:16,140
once on the current state i generate an observation relating to my

83
00:04:16,150 --> 00:04:17,320
current state

84
00:04:17,470 --> 00:04:19,370
so imagine this somebody

85
00:04:19,370 --> 00:04:23,290
we cannot directly observe the happy sad control but we can look at how much

86
00:04:23,290 --> 00:04:25,320
they're smiling down here

87
00:04:25,330 --> 00:04:31,030
and so to evolved from happy to neutron from trolls and jump back and forth

88
00:04:31,110 --> 00:04:32,590
between extremes

89
00:04:32,610 --> 00:04:35,170
and so there is some kind of

90
00:04:35,230 --> 00:04:38,060
my assumption here can have evolved to be

91
00:04:38,180 --> 00:04:42,700
s one s two to describe the state transition matrix given s two of how

92
00:04:43,630 --> 00:04:46,650
person smiling and then as three persons

93
00:04:46,670 --> 00:04:49,120
frowny surfaces are observations

94
00:04:49,120 --> 00:04:50,940
but but but that's a very much

95
00:04:50,960 --> 00:04:55,940
so while providing is concerned so for most of its history mathematics has been explicitly

96
00:04:57,080 --> 00:05:00,710
so for example you could geometry there are two types of theorems there i mean

97
00:05:00,730 --> 00:05:03,110
you know sometimes are called on to show that

98
00:05:03,120 --> 00:05:09,690
geometric configuration certain properties other times are called on to construct an explicit symmetrical object

99
00:05:09,760 --> 00:05:13,880
but the proofs and show you how to construct it similarly now to bring out

100
00:05:13,880 --> 00:05:20,420
rules current concerned with solving equations analysis and probability you're trying to calculate passing trajectories

101
00:05:20,420 --> 00:05:25,800
and probabilities and so on i was only developments in the nineteenth century that inaugurated

102
00:05:25,800 --> 00:05:32,380
methods that that you know we now characterize being infinitary or abstract conceptual nonconstructive set

103
00:05:32,380 --> 00:05:38,860
theoretic or predicate of and so on and whatever they do one side effect of

104
00:05:38,860 --> 00:05:45,720
these methods is that they tend to suppress or sometimes completely eliminate quantitative quite computational

105
00:05:47,110 --> 00:05:48,620
and i was program

106
00:05:48,680 --> 00:05:54,010
again on the surface was folks questions as the consistency of the new methods but

107
00:05:54,010 --> 00:05:57,470
i think it's better to view it more broadly is trying to

108
00:05:57,590 --> 00:06:03,340
address these these these dramatic changes in nineteenth century mathematics and try to reconcile sort

109
00:06:03,340 --> 00:06:04,990
of the abstract infantry

110
00:06:05,010 --> 00:06:09,510
view of mathematics with with the feeling of mathematics should have

111
00:06:09,530 --> 00:06:12,090
concrete computational content

112
00:06:12,110 --> 00:06:14,990
and so you can view the program is trying to

113
00:06:15,070 --> 00:06:18,930
recapture explicit computation so you want to have your cake and eat it too you

114
00:06:18,930 --> 00:06:21,010
want to allow the the set theoretic

115
00:06:21,010 --> 00:06:23,200
methods but

116
00:06:23,700 --> 00:06:27,120
but have some kind of concrete understanding or interpretation

117
00:06:27,160 --> 00:06:29,610
so it's not surprising that

118
00:06:29,700 --> 00:06:34,970
that help its program in the twentieth century was largely a matter of foundational reduction

119
00:06:35,200 --> 00:06:39,840
initiative in principle if you can prove something using non constructive or infantry methods you

120
00:06:39,840 --> 00:06:44,450
know in principle you can reduce it to two explicit constructive reasoning

121
00:06:44,470 --> 00:06:48,550
proof mining today aims to put these insights to good use so as we heard

122
00:06:48,550 --> 00:06:54,740
in or call box talk so for example extracting explicit quantitative information from abstract

123
00:06:54,780 --> 00:07:01,950
four infinitary nonconstructive arguments or as low as five extracting metamathematical metamathematical useful uniformities in

124
00:07:05,930 --> 00:07:10,550
you can also be the program is interested in developing infinitary methods that are better

125
00:07:10,550 --> 00:07:11,680
suited to

126
00:07:11,740 --> 00:07:17,220
to addressing finitary combinatorial or analytic you find problems

127
00:07:17,220 --> 00:07:21,380
and finally

128
00:07:21,390 --> 00:07:27,780
you can also view the program what interested the programs as well as developing means

129
00:07:27,780 --> 00:07:31,620
of interpreting nonconstructive developments for example in computer

130
00:07:31,640 --> 00:07:36,510
commutative algebra described his talk in algorithmic and computational terms

131
00:07:36,570 --> 00:07:39,860
and so along the lines of the delhi concerns that you know like to sort

132
00:07:39,860 --> 00:07:44,660
of call your attention to the the the issues and big questions that are being

133
00:07:45,680 --> 00:07:52,120
so harold edwards once once quipped to me that is so some that was remarkable

134
00:07:52,120 --> 00:07:57,180
person who was very much influenced by nineteenth century sensibility anyone script to me that

135
00:07:57,200 --> 00:07:59,890
will you wouldn't have to mine proves

136
00:07:59,910 --> 00:08:05,410
if you know for these this explicit quantitative information if only we just did your

137
00:08:05,490 --> 00:08:10,950
proofs in such a way that you preserve that that explicit quantitative information all along

138
00:08:10,970 --> 00:08:16,530
but of course the mathematics today we don't whatever reason we we find it useful

139
00:08:16,530 --> 00:08:17,180
to us

140
00:08:17,200 --> 00:08:19,820
to use abstract nonconstructive methods

141
00:08:19,840 --> 00:08:24,740
and so i think so for mining i think you can view it as a

142
00:08:24,740 --> 00:08:30,180
way to sort of understand what is lost when we go to abstract infinitary methods

143
00:08:30,470 --> 00:08:33,200
and at the same time you know what is is that we gain

144
00:08:33,200 --> 00:08:38,760
from the use of those methods and finally you want to sort of minimizing losses

145
00:08:38,760 --> 00:08:43,030
and maximizing game so the question is you one so even if we allow ourselves

146
00:08:43,030 --> 00:08:48,550
to use infantry methods proof mining aims to see if we can recapture some of

147
00:08:48,570 --> 00:08:50,610
the quantitative expose information

148
00:08:50,640 --> 00:08:52,610
that's lost

149
00:08:52,620 --> 00:08:59,450
so no topic on discussed briefly is formal verification and automated reasoning so the phrase

150
00:08:59,450 --> 00:09:03,240
is used most broadly to describe a branch of computer science that uses formal methods

151
00:09:03,240 --> 00:09:04,740
to verify correctness

152
00:09:04,780 --> 00:09:10,660
and verify correctness in practice can be one of two things so one is verifying

153
00:09:10,660 --> 00:09:15,910
the correctness of hardware and software to design relative to specifications to have specification you

154
00:09:15,910 --> 00:09:20,090
have a description of the circuit and you want to prove or verify the circuit

155
00:09:20,090 --> 00:09:24,640
does what it's supposed to do which is something that is all the more important

156
00:09:24,640 --> 00:09:28,640
if the circuit is you know running a nuclear reactor flying an airplane so you

157
00:09:28,640 --> 00:09:33,340
want to have some kind of formal guarantees that your designers is correct but it

158
00:09:33,340 --> 00:09:38,050
can also has been taken to mean verifying the correctness of a mathematical

159
00:09:39,030 --> 00:09:45,780
and so today interactive proof assistants have been developed that really do both

160
00:09:45,950 --> 00:09:49,740
there's there's much more common between those two

161
00:09:49,740 --> 00:09:51,760
that then you might think

162
00:09:51,780 --> 00:09:57,220
and so you focus most in the mathematical side but the ideas

163
00:09:57,240 --> 00:10:02,220
formal verification is not supposed to eliminate mathematical understanding or take the place of ordinary

164
00:10:02,220 --> 00:10:08,390
mathematical proof or anything like that but rather you only in recent years there has

165
00:10:08,390 --> 00:10:15,140
been increasing reliance on computation and mathematical arguments and there also have been i to

166
00:10:15,140 --> 00:10:19,840
say getting there are there proofs rely on a good deal of very difficult and

167
00:10:20,610 --> 00:10:22,320
and delicate calculations

168
00:10:22,340 --> 00:10:26,360
and so you think you want to think of formal verification is a tool to

169
00:10:26,360 --> 00:10:30,010
help ensure the correctness of the is as approved check

170
00:10:30,030 --> 00:10:35,160
so again not science applied mathematics but but but in so far has have incorrect

171
00:10:35,160 --> 00:10:40,930
arguments is important mathematics having computational systems to verify correctness is is a good thing

172
00:10:40,930 --> 00:10:46,590
and the idea is what sitting in interactive proof assistants are actually constructing a formal

173
00:10:46,590 --> 00:10:48,760
axiomatic proof as you go

174
00:10:49,280 --> 00:10:52,910
somewhere in the bowels of the computer there's actually a data structure that you can

175
00:10:52,910 --> 00:10:57,180
you unwrap and find yourself facing formal axiomatic proof

176
00:10:57,240 --> 00:11:00,860
and what the computer does is also first one has to keep track of your

177
00:11:00,880 --> 00:11:04,200
that is your axioms and the rules and checking applying them correctly but keep track

178
00:11:04,200 --> 00:11:07,110
of what you've proved and what you in your definitions

179
00:11:07,120 --> 00:11:10,240
so one line computers much as you can do that

180
00:11:10,240 --> 00:11:15,640
and then and then performing detailed calculations you fully automatically but then you want to

181
00:11:15,640 --> 00:11:17,780
know you i don't want to sort of way program and run it you want

182
00:11:17,780 --> 00:11:21,950
to have some kind of verification program is the the calculation it's doing is is

183
00:11:21,950 --> 00:11:23,950
giving you the result you think it is

184
00:11:23,970 --> 00:11:28,470
and all of these i maintain require a better understanding of mathematical

185
00:11:28,490 --> 00:11:30,490
language that

186
00:11:32,140 --> 00:11:36,010
let me let me emphasise that technology is not ready for prime time it still

187
00:11:36,010 --> 00:11:42,640
takes as suggested still takes much more work than one would like to check even

188
00:11:42,640 --> 00:11:45,410
you know straightforward mathematical inferences

189
00:11:45,410 --> 00:11:48,620
but i think the time will come when it will when formal verification will be

190
00:11:48,620 --> 00:11:53,570
commonplace in mathematics and in the near future as i said there are arguments that

191
00:11:53,570 --> 00:11:59,140
are very fiddly calculations i or arguments that rely on a good deal of kind

192
00:11:59,160 --> 00:12:00,510
of computation

193
00:12:00,570 --> 00:12:06,780
we're already formal verification is is starting to show

194
00:12:06,820 --> 00:12:11,570
its utilities and so on so on number of interesting things have been formalized date

195
00:12:11,570 --> 00:12:14,610
when i let me just mention that there are a number of interesting projects along

196
00:12:14,610 --> 00:12:18,260
the way so she was going to a is is right now within a few

197
00:12:18,260 --> 00:12:22,610
months certainly within the year of the verification of the feit thompson which first or

198
00:12:22,610 --> 00:12:28,590
purposes was a landmark in early an important landmark step in the classification of finite

199
00:12:28,590 --> 00:12:36,200
simple groups tom townhouses similarly pretty close to having full verification of this proof of

200
00:12:36,200 --> 00:12:41,820
the kepler conjecture ministerial mentioned in this talk about but sky has launched an interesting

201
00:12:42,840 --> 00:12:49,360
to develop new foundations for algebraic topology and perhaps beyond relying on the on the

202
00:12:49,360 --> 00:12:54,590
simple idea that you can view dependent type theory as a calculus for reasoning about

203
00:12:54,590 --> 00:12:57,950
topological spaces and continuous maps between them

204
00:12:57,950 --> 00:13:01,720
and then you could model that we're in hmm

205
00:13:01,860 --> 00:13:08,320
now in multimodal inter interactions and interfaces you in general have more than one of

206
00:13:08,320 --> 00:13:13,470
such streams to model at the same time so we need another notation

207
00:13:13,470 --> 00:13:17,530
so the notation here is to introduce another viable which

208
00:13:18,150 --> 00:13:19,990
you have now

209
00:13:21,490 --> 00:13:24,200
a set of sequences and each

210
00:13:24,300 --> 00:13:25,990
a set of sequence

211
00:13:25,990 --> 00:13:31,650
it is and you may have for each of them and so such so if

212
00:13:31,650 --> 00:13:36,180
we take an example of one set of sequence could be this presentation regarded by

213
00:13:36,180 --> 00:13:41,400
that guy he recorded the audio and video so you have two streams of information

214
00:13:41,410 --> 00:13:42,400
for this

215
00:13:42,700 --> 00:13:48,800
particle and then in the other room there is another one recording another event we've

216
00:13:48,800 --> 00:13:53,910
also audio and video so that next to a set of training examples

217
00:13:53,930 --> 00:13:58,990
and so these are the two labels here this is the evidence and this is

218
00:13:58,990 --> 00:14:01,180
the kind of tree we have

219
00:14:01,240 --> 00:14:02,400
audio video

220
00:14:03,820 --> 00:14:10,380
that's why we need so many indices

221
00:14:10,410 --> 00:14:14,630
and in the same way as for HMM GMM is we need to

222
00:14:14,630 --> 00:14:19,320
to define what will be the task in past will always be basically the same

223
00:14:19,320 --> 00:14:22,170
we want to compute the likelihood of our data

224
00:14:22,180 --> 00:14:24,840
this is often called the inference problem

225
00:14:24,860 --> 00:14:26,340
and we want to train

226
00:14:26,360 --> 00:14:29,700
this model that's going to maximize

227
00:14:29,720 --> 00:14:30,990
that likely would

228
00:14:31,010 --> 00:14:34,780
over the training set of a set of strings

229
00:14:34,970 --> 00:14:40,010
so this is the likelihood so it's going to be the product all are sequences

230
00:14:40,010 --> 00:14:42,180
and these are the set of sequences

231
00:14:42,200 --> 00:14:43,320
for all the

232
00:14:44,130 --> 00:14:51,220
and here training is to search for the best parameters of that guy

233
00:14:55,360 --> 00:14:57,130
what are the

234
00:14:57,220 --> 00:15:00,320
the options that we have the simplest one

235
00:15:00,340 --> 00:15:06,030
which many people use because really it's very simple is called what we call early

236
00:15:06,030 --> 00:15:10,570
integration hmms so in in fact is going to be a simple HMM so we

237
00:15:10,570 --> 00:15:13,470
all know how to train an HMM is very and it has been used for

238
00:15:13,630 --> 00:15:18,720
twenty years we know how it works for all these reasons might be a good

239
00:15:18,720 --> 00:15:20,380
choice for you

240
00:15:20,380 --> 00:15:23,150
and but you need to make some assumptions

241
00:15:23,170 --> 00:15:26,010
the basic assumption is that is that

242
00:15:26,110 --> 00:15:28,740
you are going to assume that all

243
00:15:28,760 --> 00:15:30,740
so for instance for this evident

244
00:15:30,760 --> 00:15:32,990
he's regarding your doing the video

245
00:15:33,010 --> 00:15:35,700
you're going to assume that you're doing the video are

246
00:15:35,720 --> 00:15:41,110
i have the same frame rate how they are recorded at the same time case

247
00:15:41,170 --> 00:15:43,150
but you meet you're going to need to

248
00:15:43,610 --> 00:15:48,800
extract them at the same frame rate this is not often very good because we

249
00:15:48,800 --> 00:15:50,970
know that the old information

250
00:15:51,030 --> 00:15:57,860
it follows something much quicker than the video information but let's suppose that you agree

251
00:15:57,860 --> 00:16:02,550
with that anyway then in that case you decide that you are one frame of

252
00:16:02,550 --> 00:16:05,340
information contained both the audio

253
00:16:05,380 --> 00:16:07,800
and the video and basically we

254
00:16:07,820 --> 00:16:12,720
we call that early integration because we're going to integrate the audio and video

255
00:16:13,150 --> 00:16:15,760
before using them into the model

256
00:16:15,800 --> 00:16:20,450
so there we decide before when we extract the data that they are all going

257
00:16:21,070 --> 00:16:24,360
come from the same model doing the video

258
00:16:24,380 --> 00:16:28,910
so in that case we have a single emission distribution and the transition distribution and

259
00:16:28,910 --> 00:16:30,930
we have our HMM and we work

260
00:16:30,950 --> 00:16:31,680
with it

261
00:16:31,700 --> 00:16:34,450
and we're happy

262
00:16:36,130 --> 00:16:39,610
this is the first solution that the solution is

263
00:16:39,630 --> 00:16:43,170
so called multi stream HMM

264
00:16:43,180 --> 00:16:48,670
which has been used to also in speech and in many other cases in that

265
00:16:49,490 --> 00:16:54,150
if we have node u in video source what you would do is that you

266
00:16:54,150 --> 00:16:58,130
would train an HMM for each of your sources separately

267
00:16:58,130 --> 00:17:03,510
assuming that because they are they are not related to each other during training

268
00:17:03,510 --> 00:17:05,930
you're going to have to hmms

269
00:17:05,950 --> 00:17:09,180
one for the old you one for the video

270
00:17:09,200 --> 00:17:14,320
and separately you're going to maximize the parameters of the reduced sets of sequence and

271
00:17:14,320 --> 00:17:16,240
a set of video sequences

272
00:17:16,780 --> 00:17:20,010
without knowledge of each other

273
00:17:20,220 --> 00:17:22,260
into this training

274
00:17:22,320 --> 00:17:24,670
and while

275
00:17:26,950 --> 00:17:31,220
during decoding you are now going to assume that they come from the same

276
00:17:31,280 --> 00:17:35,340
hmm so you're going to create the kind of but the

277
00:17:35,340 --> 00:17:37,110
we're because you

278
00:17:37,240 --> 00:17:41,390
in order for this to work you need to have the same kind of insurance

279
00:17:41,390 --> 00:17:45,320
so if you have a free state here going to have to restate here

280
00:17:45,340 --> 00:17:49,220
and during decoding you're going to force them to go into the same state your

281
00:17:49,260 --> 00:17:54,930
new information in the video information are going to go into the same state

282
00:17:54,950 --> 00:17:56,740
during the decoding so that their

283
00:17:56,760 --> 00:17:57,740
they are

284
00:17:57,760 --> 00:18:01,490
this is very simple to train because during training you have

285
00:18:01,490 --> 00:18:03,780
so you have an separate

286
00:18:03,780 --> 00:18:08,410
problems which you can solve using each of them in hmm but

287
00:18:11,220 --> 00:18:14,130
during training this is simply the complexity

288
00:18:14,150 --> 00:18:15,760
of the normal HMA

289
00:18:15,840 --> 00:18:19,340
times the number of strings you have and we know how to do that very

290
00:18:19,340 --> 00:18:23,880
if you consider that you can have these two levels if you do these pure

291
00:18:25,140 --> 00:18:27,410
telling OK to pure this

292
00:18:27,460 --> 00:18:32,220
seven pure p three has you find these values here

293
00:18:32,240 --> 00:18:36,570
when you compare with experiments it seems to be that is close to these ones

294
00:18:36,610 --> 00:18:41,160
so under that the differences do due to the mixing of the river

295
00:18:41,190 --> 00:18:46,080
so that's why you can say that you have a skin like that either seven

296
00:18:46,080 --> 00:18:47,060
health minus

297
00:18:47,140 --> 00:18:50,940
and once that's three have minus so it's not like that you have

298
00:18:51,500 --> 00:18:55,680
isomers here runs state here so that means

299
00:18:55,690 --> 00:19:01,330
depending on the nucleus you're you're studying you can really had different features

300
00:19:01,330 --> 00:19:07,140
and that was not expected by the calculation before the experiment

301
00:19:07,160 --> 00:19:10,050
so now concerning shape and fission isomers

302
00:19:10,080 --> 00:19:17,190
in the eighties and nineties there were extensive search for all these shape isomers

303
00:19:19,460 --> 00:19:24,230
because of two reasons because first you had a very clear signal so it was

304
00:19:24,230 --> 00:19:26,220
very clear experimentally

305
00:19:26,270 --> 00:19:27,890
the second point

306
00:19:27,940 --> 00:19:32,830
is that you have something collective so that you can really compare with

307
00:19:33,180 --> 00:19:37,350
approaches where you treat only collective motion to to really

308
00:19:37,370 --> 00:19:39,750
look at this part of

309
00:19:39,770 --> 00:19:43,550
of nuclear feature

310
00:19:43,640 --> 00:19:49,270
and what people have observed for example in this region of mercury and lead

311
00:19:49,280 --> 00:19:51,540
is that you had some bands

312
00:19:51,550 --> 00:19:54,280
so exciting bands very regular

313
00:19:54,300 --> 00:19:58,920
and it was interpreted in terms of these men based on the shape isomers here

314
00:19:58,980 --> 00:20:01,180
and then you have a professional bands

315
00:20:01,200 --> 00:20:05,100
if you calculate the deformation you really have stupid information

316
00:20:05,100 --> 00:20:08,880
so that means that you have the big axes which is twice

317
00:20:08,900 --> 00:20:11,720
so small axis here

318
00:20:11,740 --> 00:20:16,030
you see that that's what the first picture presented in the first lecture where you

319
00:20:16,030 --> 00:20:19,770
have the detected photons depending on the energy here

320
00:20:19,780 --> 00:20:23,590
and you see that it is the signal is very clear you had this speaks

321
00:20:23,630 --> 00:20:29,030
very intense because it's it's a collective and so you really have favored the case

322
00:20:29,030 --> 00:20:35,650
in into inside these bands so you don't have to continue rates going everywhere you

323
00:20:35,650 --> 00:20:37,880
really have these band that

324
00:20:37,910 --> 00:20:40,070
that comes out here

325
00:20:40,080 --> 00:20:44,720
you see that you can produce up to sixteen spain sixty age for so very

326
00:20:44,720 --> 00:20:48,910
excited states with high speed

327
00:20:48,940 --> 00:20:53,900
so this is represented like that you have a rotating nucleus excited repeating it is

328
00:20:53,900 --> 00:21:00,170
that the we need some photos to go back to its ground state

329
00:21:00,180 --> 00:21:06,830
fission isomer so this shape isomers that are observed in actinides many calculations have been

330
00:21:08,200 --> 00:21:13,150
and what is here in the difference between the state and this one

331
00:21:14,290 --> 00:21:17,790
i minus ground state here

332
00:21:17,810 --> 00:21:24,500
this is bloody this difference here depending on different nuclei from w to nobility

333
00:21:24,530 --> 00:21:30,170
you have only a few experimental data piece open symbols here

334
00:21:30,200 --> 00:21:34,210
you have a few in uranium petroleum helium no here

335
00:21:34,350 --> 00:21:39,150
the point black point are for theoretical prediction

336
00:21:39,200 --> 00:21:40,670
so you can see

337
00:21:40,910 --> 00:21:45,040
two things first agreement is not about here so you can think that the here

338
00:21:45,040 --> 00:21:47,670
you can make reliable predictions

339
00:21:47,690 --> 00:21:52,540
also you have these decrees here of the energy of the isomer when you increase

340
00:21:52,540 --> 00:21:54,070
the mass number

341
00:21:54,080 --> 00:21:57,660
and you see here three points where you have negative energy

342
00:21:57,680 --> 00:22:02,420
so negative energy mean that the state is buildings one

343
00:22:02,430 --> 00:22:05,330
so i mean that this is the ground state because the ground state is the

344
00:22:05,340 --> 00:22:07,800
state that has the minimum free energy

345
00:22:07,810 --> 00:22:10,550
so it mean that what you think what the eyes and there is in fact

346
00:22:10,550 --> 00:22:12,090
the ground state

347
00:22:12,600 --> 00:22:15,140
people were really interested in that

348
00:22:16,640 --> 00:22:18,440
and that

349
00:22:18,460 --> 00:22:23,140
there have calculated people have calculated the lifetime of the state because

350
00:22:23,140 --> 00:22:26,040
you need to know the light and in fact it seems to be that it's

351
00:22:26,040 --> 00:22:28,970
not about bound state budgets on the reason and

352
00:22:28,980 --> 00:22:32,210
because if you continue sufficient barrier

353
00:22:32,230 --> 00:22:35,990
courting insufficient value it goes like that are not like that

354
00:22:36,690 --> 00:22:41,140
you can go here very easily so it will go through fission

355
00:22:41,150 --> 00:22:46,250
so this is only a reason and and and then you can use with vision

356
00:22:46,270 --> 00:22:50,870
but many many papers were published containing that point because people have made the same

357
00:22:50,870 --> 00:22:53,420
calculation for very high them

358
00:22:53,490 --> 00:22:56,160
z equals great hundred two hundred two

359
00:22:56,180 --> 00:22:59,710
very very ABC's them and they have found this feature

360
00:22:59,730 --> 00:23:05,520
and so it was very important because when people experimentally want to study and to

361
00:23:06,610 --> 00:23:11,360
heavy elements what they are looking at it the alpha decay

362
00:23:11,380 --> 00:23:14,920
so they do fusion and that is supposed to have populated

363
00:23:14,950 --> 00:23:17,410
created a very heavy system

364
00:23:17,430 --> 00:23:21,080
and then it will emit alpha alpha alpha alpha two and known

365
00:23:21,380 --> 00:23:25,880
then the and then you reconstruct that and then you say OK if

366
00:23:25,900 --> 00:23:27,140
created these

367
00:23:27,450 --> 00:23:29,210
very ABC stem

368
00:23:29,230 --> 00:23:30,020
but then

369
00:23:30,020 --> 00:23:34,500
if you have a superheavy that goes from fission and of course we have then

370
00:23:34,500 --> 00:23:37,430
it completes the agenda where you want to

371
00:23:37,460 --> 00:23:41,410
two to study them to prove that you have created this

372
00:23:41,420 --> 00:23:43,140
OK so that's why

373
00:23:43,140 --> 00:23:45,450
it was really interesting

374
00:23:45,460 --> 00:23:49,350
and it seems to be the same that

375
00:23:49,360 --> 00:23:54,020
the states is not do not have sufficient lifetime

376
00:23:54,030 --> 00:23:58,460
to be considered as ground state and the reason and even

377
00:23:58,480 --> 00:24:03,740
in these superheavy nuclei

378
00:24:03,750 --> 00:24:10,500
concerning these fission isomers is also very important for applications when you want to know

379
00:24:10,500 --> 00:24:12,680
very precisely the cross section

380
00:24:12,690 --> 00:24:14,340
so cross section

381
00:24:16,600 --> 00:24:20,320
what happened to this reaction when you change for example the the energy of the

382
00:24:20,320 --> 00:24:23,950
projectile this is the section for neutron induced

383
00:24:23,990 --> 00:24:27,210
fission on uranium two hundred thirty eight

384
00:24:27,220 --> 00:24:31,680
and you see here that you have the first when two different states here two

385
00:24:31,680 --> 00:24:33,610
different isomeric well

386
00:24:33,650 --> 00:24:38,400
when you have seen the the labels that correspond so that do have the same

387
00:24:39,640 --> 00:24:42,250
you have structure the cross section

388
00:24:42,280 --> 00:24:45,660
so that means that the process is more favored are in favour of when you

389
00:24:45,660 --> 00:24:49,100
have live inside the same energy and it's really changed

390
00:24:49,350 --> 00:24:50,630
the cross section

391
00:24:50,650 --> 00:24:55,040
so when you want to have a evaluating data very precisely you need to know

392
00:24:55,040 --> 00:24:59,820
precisely the position of these things

393
00:24:59,820 --> 00:25:02,330
i going out to shape coexistence

394
00:25:03,040 --> 00:25:06,640
shape coexistence is when you have

395
00:25:06,670 --> 00:25:09,980
people have talked about strictly exist and when they have observed

396
00:25:10,230 --> 00:25:15,370
sierra vista second requested that at almost the same energy as the ground state

397
00:25:16,250 --> 00:25:20,620
it was an expected if you look only and the single particle levels so

398
00:25:20,620 --> 00:25:25,690
this is all you should have quite existence of the phenomenon that you don't know

399
00:25:26,520 --> 00:25:31,080
they have been discovered that when you look at the single particle levels you see

400
00:25:31,080 --> 00:25:32,650
and if that's not enough

401
00:25:32,670 --> 00:25:36,190
it turns out the mean is also involved in

402
00:25:36,190 --> 00:25:37,400
working memory

403
00:25:37,730 --> 00:25:43,960
dealing with novel situations in attention deficit hyperactive disorder and schizophrenia

404
00:25:43,960 --> 00:25:49,730
in depression anything you want basically anything that people care about in the brain dopamine

405
00:25:49,730 --> 00:25:51,440
is part of it

406
00:25:51,500 --> 00:25:53,150
and this may

407
00:25:53,150 --> 00:25:56,170
it sounds crazy but actually the brain only has

408
00:25:56,190 --> 00:25:59,060
four different neuromodulators there

409
00:25:59,080 --> 00:26:03,580
neurotransmitters which are ways in which are molecules by which one neuron

410
00:26:03,790 --> 00:26:09,770
communicates with another neuron but the four neuromodulators are molecules are sent all over the

411
00:26:09,770 --> 00:26:12,400
place and you might think if there are only four of these in the brain

412
00:26:12,400 --> 00:26:17,730
they must have really really important widespread functions and effect

413
00:26:17,810 --> 00:26:21,270
a lot of processing in the brain so it's maybe not so surprising the dopamine

414
00:26:21,270 --> 00:26:24,130
is related to all these things but still

415
00:26:24,170 --> 00:26:27,360
it makes neuroscientists really want to understand

416
00:26:27,420 --> 00:26:29,920
how does what it does and how we can cure

417
00:26:30,000 --> 00:26:34,750
the situations where does the wrong thing

418
00:26:34,810 --> 00:26:38,460
so as there are many functions are many hypotheses or i should say as there

419
00:26:38,460 --> 00:26:41,750
are many scientists there are many hypotheses

420
00:26:42,080 --> 00:26:45,750
and we'll all go through all of them for lack of

421
00:26:47,210 --> 00:26:51,000
only talk about the first two

422
00:26:51,060 --> 00:26:52,290
but just to

423
00:26:52,310 --> 00:26:55,790
put these on the board and so you know that although i'm going to tell

424
00:26:55,790 --> 00:26:59,600
you that i think this one is true it doesn't mean that it

425
00:26:59,610 --> 00:27:01,610
but there are others to consider

426
00:27:02,380 --> 00:27:04,290
so originally in the eighties

427
00:27:04,540 --> 00:27:08,130
the first part consists about dopamine will not the first

428
00:27:08,150 --> 00:27:14,060
the very influential hypothesis was called the anhedonia hypothesis

429
00:27:15,560 --> 00:27:17,750
and the idea is

430
00:27:17,770 --> 00:27:19,080
the inability

431
00:27:19,080 --> 00:27:25,960
to experience positive emotional states so usually if you eat something nice or you

432
00:27:26,210 --> 00:27:32,440
i have some sort of positive experience with another person or whatever you feel good

433
00:27:32,440 --> 00:27:36,290
about it and only at this stage where you don't feel good about things that

434
00:27:36,290 --> 00:27:39,560
used to make you feel good

435
00:27:40,480 --> 00:27:42,730
what i saw is that

436
00:27:42,750 --> 00:27:49,080
neuroleptics which are drugs that are antagonists to dopamine so they stop the function of

437
00:27:49,080 --> 00:27:53,830
dopamine in the brain these drugs cause anhedonia so cause people to not enjoy things

438
00:27:53,830 --> 00:27:57,940
that they used to enjoy life

439
00:27:57,980 --> 00:27:59,630
and so that

440
00:28:01,290 --> 00:28:02,810
that was

441
00:28:04,110 --> 00:28:07,110
the reason is to suggest that

442
00:28:07,130 --> 00:28:08,560
i don't mean

443
00:28:08,610 --> 00:28:13,080
maybe it has something to do with the dynamics are with enjoying pleasure in the

444
00:28:15,480 --> 00:28:19,210
perhaps the last movie know there will be another movie later

445
00:28:26,520 --> 00:28:30,940
is a movie from peter shizgal concordia here in montreal

446
00:28:30,960 --> 00:28:35,230
and what you're seeing here is the rat that is less pressing but not for

447
00:28:35,230 --> 00:28:39,330
food as you see there is no food magazine here whatever he presses the lover

448
00:28:39,330 --> 00:28:41,310
he gets as that of

449
00:28:42,480 --> 00:28:46,940
i electrical current straight to the dopamine area of his brain

450
00:28:47,000 --> 00:28:48,290
and he loves it

451
00:28:48,330 --> 00:28:50,400
he really really wants to

452
00:28:50,560 --> 00:28:54,480
get this you can see how he doesn't want to live or to go away

453
00:28:54,480 --> 00:28:57,060
he's really trying to coax it out

454
00:28:57,100 --> 00:28:58,710
because every time

455
00:28:58,730 --> 00:29:02,750
he gets this that in his brain it just makes you feel really good this

456
00:29:02,750 --> 00:29:04,900
is called

457
00:29:04,960 --> 00:29:07,130
brain self stimulation reward

458
00:29:07,130 --> 00:29:09,360
and it it

459
00:29:09,360 --> 00:29:13,520
was studied extensively

460
00:29:13,520 --> 00:29:18,630
at the time when people realize the dopamine has something to do with reward

461
00:29:18,670 --> 00:29:21,340
and till today

462
00:29:22,480 --> 00:29:26,920
in general the idea was OK there are rewards in the real world you know

463
00:29:28,710 --> 00:29:32,730
cake money et cetera how does the brain know

464
00:29:33,440 --> 00:29:38,020
i got the reward well you might know that god rewards through this molecule through

465
00:29:38,020 --> 00:29:42,920
this through dopamine dopamine scored all over the place telling the brain a good thing

466
00:29:42,920 --> 00:29:47,100
now happened and that that was wise anhedonia hypothesis

467
00:29:47,110 --> 00:29:50,670
and this explains well first of all it explains

468
00:29:51,060 --> 00:29:54,520
electrophysiological recordings that people had

469
00:29:54,540 --> 00:29:56,460
none of the time

470
00:29:56,500 --> 00:29:59,460
so what you see here is

471
00:29:59,860 --> 00:30:03,670
these recordings from the diplomatic neurons

472
00:30:04,630 --> 00:30:06,020
a monkey

473
00:30:06,040 --> 00:30:09,250
every dot here is when the neuron fires

474
00:30:09,340 --> 00:30:10,540
and every row

475
00:30:10,560 --> 00:30:11,920
is one trial

476
00:30:12,020 --> 00:30:13,650
and every trial

477
00:30:13,670 --> 00:30:18,020
sometimes faster then the monkey god it's got some two cities mouth this is the

478
00:30:18,290 --> 00:30:19,610
thirsty monkey

479
00:30:19,650 --> 00:30:21,770
so this is how one that the jews

480
00:30:21,790 --> 00:30:23,830
and then some more time passed

481
00:30:23,840 --> 00:30:27,150
what you see is this is the sum of all these that of all this

482
00:30:27,150 --> 00:30:28,480
raster plot

483
00:30:28,500 --> 00:30:34,440
and what you see is these the manager on fire right after reward

484
00:30:34,440 --> 00:30:37,100
and then after a while ago back to baseline

485
00:30:37,130 --> 00:30:41,520
so the idea is they signal to the brain that are reward happened to the

486
00:30:42,310 --> 00:30:44,540
i was obtained

487
00:30:44,560 --> 00:30:46,610
this of course explains why

488
00:30:46,630 --> 00:30:47,920
the rat would one

489
00:30:48,040 --> 00:30:52,500
that in his brain to the dopamine urticaria because that's exactly like getting real rewards

490
00:30:52,500 --> 00:30:54,170
in the world

491
00:30:54,190 --> 00:30:59,730
and explains why he blocked mean animals stop learning so if you give neuroleptics and

492
00:30:59,730 --> 00:31:00,960
if i had to do

493
00:31:00,980 --> 00:31:05,310
conditioning studies they don't learn to level press and they don't have to leverpress for

494
00:31:05,310 --> 00:31:12,690
food they don't work for normally words in the same way

495
00:31:14,670 --> 00:31:18,920
this story would have been simple and nice and we would have not been reinforcement

496
00:31:18,920 --> 00:31:21,520
learning and the whole thing would have and except the

497
00:31:21,560 --> 00:31:24,690
in the nineties schulz wolfram schultz who also

498
00:31:26,040 --> 00:31:27,790
these recordings

499
00:31:27,810 --> 00:31:33,750
started making his experiments little bit more complex by adding the stimulus before the reward

500
00:31:33,790 --> 00:31:35,250
and what he size

501
00:31:35,270 --> 00:31:37,600
when a stimulus

502
00:31:37,610 --> 00:31:42,170
it shown here in the reward is obtained two seconds later this is again recordings

503
00:31:42,190 --> 00:31:44,440
in monkeys the monkeys are getting

504
00:31:44,580 --> 00:31:48,670
jews reward after seeing a visual stimulus on the screen

505
00:31:48,670 --> 00:31:49,870
from the cluster

506
00:31:50,100 --> 00:31:53,490
sure the good representation is simple algorithms like these

507
00:31:53,970 --> 00:31:55,610
what works quite well

508
00:31:55,660 --> 00:31:58,070
and they have been a number of

509
00:31:59,270 --> 00:32:01,530
modifications to the e

510
00:32:01,620 --> 00:32:06,510
thanks for heuristic which have been added to it how celibacy points

511
00:32:06,520 --> 00:32:09,220
how to learn the distance metric

512
00:32:09,260 --> 00:32:16,110
bisecting the sex these constraints in these to handle large amount of data but the

513
00:32:16,110 --> 00:32:18,270
basic idea is essentially

514
00:32:18,330 --> 00:32:20,740
the same

515
00:32:20,750 --> 00:32:22,640
but not of what has gone on

516
00:32:22,650 --> 00:32:25,650
after two weeks

517
00:32:27,170 --> 00:32:29,970
in two thousand six two thousand seven

518
00:32:29,980 --> 00:32:34,830
just looking at your machine learning conferences only ICML ECML

519
00:32:36,230 --> 00:32:38,620
one hundred fifty five people

520
00:32:38,650 --> 00:32:43,370
in the machine learning is this conference pointed out that

521
00:32:43,410 --> 00:32:44,940
not quite limited

522
00:32:46,100 --> 00:32:53,690
we the policy about fifteen people were thought the last thing to of seven long

523
00:32:53,710 --> 00:32:56,920
so why might have so many different

524
00:33:00,090 --> 00:33:04,540
the choice of the objective function generative models in states

525
00:33:04,550 --> 00:33:06,740
and he thought

526
00:33:06,750 --> 00:33:08,880
selecting one before

527
00:33:08,900 --> 00:33:10,900
heuristic own preferred

528
00:33:10,910 --> 00:33:15,000
preferred generative model and and you will come up with

529
00:33:15,000 --> 00:33:16,160
different outcome

530
00:33:16,860 --> 00:33:18,280
which may be admissible

531
00:33:18,290 --> 00:33:20,840
but there is no good which is optimal

532
00:33:20,850 --> 00:33:23,420
this is the knowledge

533
00:33:23,450 --> 00:33:24,880
in a

534
00:33:24,890 --> 00:33:29,190
but as you can expect that the starting point is poisson

535
00:33:29,260 --> 00:33:30,400
point seven

536
00:33:30,420 --> 00:33:34,690
the status of and by similarity matrix

537
00:33:34,700 --> 00:33:39,820
there's only so much information you can extract some grouping them so it's not surprising

538
00:33:41,030 --> 00:33:44,550
the complete based on some kind of entertainment

539
00:33:44,630 --> 00:33:49,040
from the neighbouring distances but you know the difference

540
00:33:49,050 --> 00:33:51,380
mathematical models available

541
00:33:51,390 --> 00:33:55,530
the information bottleneck using information theoretic idea

542
00:33:55,550 --> 00:34:03,000
matrix factorizations and recent work has been on an some of overlapping maximum margin

543
00:34:05,240 --> 00:34:06,960
so i want to the

544
00:34:07,020 --> 00:34:10,510
sounds to to all these different methods but i just try to

545
00:34:10,530 --> 00:34:13,020
hopefully emphasise few these methods

546
00:34:13,030 --> 00:34:15,140
so in spite of so many

547
00:34:16,740 --> 00:34:20,500
clustering algorithm from somebody who wants to use it

548
00:34:20,550 --> 00:34:23,900
data clustering algorithm for a given application

549
00:34:23,910 --> 00:34:25,240
so this is is

550
00:34:25,250 --> 00:34:29,120
these problems what features normalisation schemes used

551
00:34:29,140 --> 00:34:31,640
i personally view this as a not a

552
00:34:31,700 --> 00:34:34,280
not a problem

553
00:34:34,370 --> 00:34:38,230
of the choice of listening and but the domain knowledge

554
00:34:39,210 --> 00:34:42,270
you need to have some domain knowledge in order to figure out what is the

555
00:34:43,000 --> 00:34:47,400
representation the features that should be like not

556
00:34:47,460 --> 00:34:50,110
how to define the pairwise similarity

557
00:34:50,120 --> 00:34:52,070
how many clusters

558
00:34:52,080 --> 00:34:54,860
but let's think about how to choose the output

559
00:34:54,890 --> 00:34:58,000
parameters and we have to that you have

560
00:34:58,040 --> 00:34:59,690
needs to

561
00:34:59,700 --> 00:35:05,240
and which is quite critical of the spectral out the minimum spanning tree algorithm is

562
00:35:05,240 --> 00:35:07,870
a heuristic which was to specify

563
00:35:07,900 --> 00:35:10,660
a particular parameter

564
00:35:10,670 --> 00:35:15,750
does it you have any clustering tendency what happens if i generate random data are

565
00:35:15,750 --> 00:35:18,140
uniform in five dimensions

566
00:35:18,150 --> 00:35:23,260
if i applied it to the first five five five clusters if not so be

567
00:35:23,260 --> 00:35:27,990
it does not have any clustering tendency so this is an important point here is

568
00:35:27,990 --> 00:35:29,210
a lot of time

569
00:35:29,220 --> 00:35:33,560
and clustering it doesn't really doesn't have much less tendency

570
00:35:33,590 --> 00:35:36,740
i discovered clusters and partition

571
00:35:36,740 --> 00:35:40,270
again it depends on what is the purpose of the last thing about the first

572
00:35:40,270 --> 00:35:42,960
thing is to simply organised data

573
00:35:43,240 --> 00:35:49,490
in a way of efficiently computing lets us know that doesn't really matter

574
00:35:49,500 --> 00:35:53,730
but in other situations trying to learn the the structure of the data

575
00:35:53,730 --> 00:35:57,630
then the list of entities that are extremely important

576
00:35:57,640 --> 00:36:03,530
and how to visualize the red and brown clusters

577
00:36:03,600 --> 00:36:07,070
so what is the a cluster one as i already mentioned the set of entities

578
00:36:07,070 --> 00:36:08,220
which satellite

579
00:36:08,260 --> 00:36:12,710
and entities from different clusters are not like that but the real problem is how

580
00:36:12,710 --> 00:36:14,760
do we define the notion of light

581
00:36:14,770 --> 00:36:15,970
and like

582
00:36:15,970 --> 00:36:21,160
well one thing we can say is cluster can be defined as within cluster distance

583
00:36:21,160 --> 00:36:25,410
that is a pair of points that fall in the same cluster distance between them

584
00:36:25,410 --> 00:36:26,500
is less

585
00:36:26,520 --> 00:36:27,190
the tree

586
00:36:27,190 --> 00:36:28,590
most this

587
00:36:28,650 --> 00:36:34,060
but it turns out there many useful clusters not satisfy certain requirements

588
00:36:35,010 --> 00:36:38,760
you know the cluster like this and the looks like this perhaps will satisfy this

589
00:36:38,780 --> 00:36:42,440
requirement but not necessarily all types of plants

590
00:36:42,440 --> 00:36:47,140
then the clusters have been defined based on connected clusters within cluster one activity should

591
00:36:47,140 --> 00:36:48,460
be people

592
00:36:48,470 --> 00:36:49,750
one activity

593
00:36:49,770 --> 00:36:54,420
but we we all know that an idea of cluster is compact and isolated is

594
00:36:54,460 --> 00:36:58,310
within cluster distance is much smaller than between cluster distance

595
00:36:58,330 --> 00:37:04,770
and again it depends on the representation of the book representation likely to have compact

596
00:37:04,770 --> 00:37:09,200
what the really result of the game is going to be

597
00:37:11,730 --> 00:37:18,950
the actually we don't have that

598
00:37:18,990 --> 00:37:23,970
so the interesting question now is and this will be the interesting question again and

599
00:37:23,970 --> 00:37:27,210
again and again is how do we represent

600
00:37:27,260 --> 00:37:32,560
the state of the board whatever the state of the world and it is our

601
00:37:32,570 --> 00:37:35,570
chose a representation of this new network which

602
00:37:35,620 --> 00:37:37,300
in a way is

603
00:37:37,300 --> 00:37:41,490
very suitable in very well crafted for neural networks but on the other hand is

604
00:37:42,790 --> 00:37:44,260
primitive in

605
00:37:44,280 --> 00:37:49,010
in a different point of view that makes explain so there were one hundred ninety

606
00:37:49,010 --> 00:37:52,060
eight input units of the neural network

607
00:37:52,070 --> 00:37:56,320
and in particular you for each color and for each of the twenty four points

608
00:37:56,320 --> 00:37:59,550
on the board you four units for

609
00:37:59,560 --> 00:38:04,570
neurons in your input layer before inputs so

610
00:38:04,600 --> 00:38:09,170
let's fall in the number of checkers on the point then

611
00:38:09,470 --> 00:38:13,670
if there were no checkers then all units would be zero for the input if

612
00:38:13,670 --> 00:38:17,170
there was one checker then only the first unit with be one

613
00:38:17,180 --> 00:38:20,860
there were two checkers the first and second unit with be one if there were

614
00:38:20,870 --> 00:38:25,180
three checkers the first three units would be one and for more than three checkers

615
00:38:25,180 --> 00:38:30,140
the force units would be set to represent essentially how many more than three there

616
00:38:33,530 --> 00:38:39,740
the interesting thing is that some backgammon knowledge enters into this because these first three

617
00:38:40,470 --> 00:38:45,750
i really kind of very different cases is something is empty that's very distinct case

618
00:38:45,760 --> 00:38:50,950
from there being one check in particular because one checker can be hit

619
00:38:51,000 --> 00:38:56,080
right and that's really weakness having one checker on the point is bad for your

620
00:38:56,140 --> 00:39:02,560
game in many situations so distinguishing this is very important now in this case where

621
00:39:02,610 --> 00:39:07,260
n equals two is an entirely different matter now from one check which is the

622
00:39:07,260 --> 00:39:13,140
vulnerability if you add the second checkers suddenly you're controlling that point

623
00:39:13,170 --> 00:39:14,860
so the opponent cannot

624
00:39:14,870 --> 00:39:20,060
entry at that point or cannot go to that point anymore so suddenly this becomes

625
00:39:20,060 --> 00:39:23,950
the strength and that i think that justifies another unit

626
00:39:24,740 --> 00:39:28,020
this thing here represents the case we have three

627
00:39:28,040 --> 00:39:34,410
three checkers on the point that can be particularly good because now you could even

628
00:39:34,410 --> 00:39:39,640
move checker away and would still retain control and this extra check is referred to

629
00:39:39,640 --> 00:39:43,610
as the builder because it can help you build other points to block the board

630
00:39:43,620 --> 00:39:47,960
even more and so this report just represents the rest

631
00:39:47,970 --> 00:39:50,950
so this is pretty clever

632
00:39:50,960 --> 00:39:52,260
now in addition

633
00:39:52,270 --> 00:39:54,600
we need a unit

634
00:39:54,620 --> 00:39:58,260
for each color to indicate the number of pieces on the bar

635
00:39:58,270 --> 00:40:02,450
so if you appears on the bar that means that your first obligation

636
00:40:02,470 --> 00:40:06,900
according to the rules is to get them back into the game game so it's

637
00:40:06,900 --> 00:40:11,670
very bad to have checkers on the bar in fact it can happen that you

638
00:40:11,670 --> 00:40:16,740
have checkers on the bar the opponent has blocked out during the entire field in

639
00:40:16,740 --> 00:40:21,480
which you may enter and then you're totally start you cannot move and whatever you

640
00:40:21,490 --> 00:40:29,680
throwing dice will be wasted called dancing on the bar

641
00:40:29,700 --> 00:40:33,750
so that's one thing and the other thing of course it has to be represented

642
00:40:33,750 --> 00:40:36,540
as how many of you are still once you've already out

643
00:40:36,740 --> 00:40:40,960
because that's the ultimate game and if this number here is the number of stones

644
00:40:40,960 --> 00:40:46,210
out reaches fifteen born borne numbers on support of then you win

645
00:40:46,210 --> 00:40:50,450
also we need an indicator of whose move it is

646
00:40:50,460 --> 00:40:59,170
what you have essentially is the standard feedforward neural network with a sigmoidal activation function

647
00:40:59,730 --> 00:41:03,480
here the weights and these are the inputs and that that was just a standard

648
00:41:03,480 --> 00:41:06,290
two layer network

649
00:41:06,310 --> 00:41:12,310
what i mean by well clever representations well i think he did use of that

650
00:41:13,200 --> 00:41:18,090
naive knowledge if you like about backgammon to build this input representation and it was

651
00:41:18,090 --> 00:41:23,750
advisable to think very hard about about which properties of the problem you can get

652
00:41:23,750 --> 00:41:28,480
into your presentation because that makes life for your learning algorithm much easier

653
00:41:28,500 --> 00:41:33,860
on the other hand consider this we see this board as humans and we know

654
00:41:33,870 --> 00:41:37,180
there are points next to each other and they are next to each other right

655
00:41:37,200 --> 00:41:40,240
we know that there are some spatial relationship

656
00:41:40,250 --> 00:41:45,390
the neural network has no idea about this the input representation is a vector and

657
00:41:45,390 --> 00:41:50,750
the vectors could just as well be totally scrambled it has no idea whatsoever that

658
00:41:50,750 --> 00:41:56,350
we're talking about a continuous line of twenty four points here

659
00:41:56,380 --> 00:42:00,740
so in that respect that's a very primitive representation but i've hardly seen any progress

660
00:42:00,740 --> 00:42:03,840
in investor representations

661
00:42:03,890 --> 00:42:07,920
so here's the TD learning rule that was used

662
00:42:09,910 --> 00:42:15,300
the parameter vector of the neural network is updated using the standard

663
00:42:15,300 --> 00:42:17,950
rule four td lambda

664
00:42:17,970 --> 00:42:23,290
in particular with this pt which is the eligibility trace which is updated like this

665
00:42:23,290 --> 00:42:28,380
you have this dk here you have the gradient of the value of the neural

666
00:42:28,380 --> 00:42:33,880
network with regard to the parameters here but with the loss so this this of

667
00:42:33,880 --> 00:42:37,340
course depends on the neural network representation of your value

668
00:42:38,540 --> 00:42:44,830
you can use backprop to train that you initialise the weights with small values

669
00:42:45,770 --> 00:42:51,100
how do you generate the sequence is through the state space and

670
00:42:51,300 --> 00:42:55,950
quite amazingly you can do that by self play and that's probably why this whole

671
00:42:55,950 --> 00:43:02,570
approach was is appears to be so elegant because you don't need any external interference

672
00:43:02,590 --> 00:43:07,340
this system is good generating its own training data so far given position where you

673
00:43:07,340 --> 00:43:10,200
start you find all the legal moves

674
00:43:10,210 --> 00:43:12,560
to determine the subsequent positions

675
00:43:12,570 --> 00:43:17,820
he use the neural network to evaluate these positions then you use

676
00:43:17,960 --> 00:43:21,360
then you choose to move that leads to the position with the highest probability of

677
00:43:22,160 --> 00:43:26,400
and this gets you through the game you have two of these neural networks essentially

678
00:43:26,400 --> 00:43:35,330
playing against each other and after each move you make TD update

679
00:43:35,350 --> 00:43:39,720
and in particular since you the dice rolls you don't even need a lot of

680
00:43:39,740 --> 00:43:46,860
epsilon greedy exploitation or stuff like that exploration because the dyes give users the stochasticity

681
00:43:46,860 --> 00:43:50,860
to explore the space

682
00:43:53,650 --> 00:43:59,530
after three hundred thousand training games TD gammon well had the same strength as a

683
00:43:59,530 --> 00:44:03,070
handcrafted not such true as this

684
00:44:03,110 --> 00:44:10,060
as as the program that use supervised learning to learn backgammon and this was neural

685
00:44:10,060 --> 00:44:14,840
gamini incidentally it was by the same authorities are

686
00:44:14,850 --> 00:44:19,080
but i mean that country half compared against the best thing that was available at

687
00:44:19,080 --> 00:44:23,160
OK so it tells you what exactly is documents

688
00:44:23,260 --> 00:44:24,910
so this is informative

689
00:44:24,930 --> 00:44:28,830
some can be of very different kind

690
00:44:28,900 --> 00:44:31,390
they can be just as short as this

691
00:44:31,400 --> 00:44:35,000
they can because it's so obvious short snippets

692
00:44:35,140 --> 00:44:41,770
they can belong to scientific abstracts they can be just single answers for all its

693
00:44:41,770 --> 00:44:44,620
provided keywords also standardisation

694
00:44:45,730 --> 00:44:46,710
they can be

695
00:44:46,720 --> 00:44:50,320
as i said earlier derived from a single documents for

696
00:44:50,330 --> 00:44:56,450
document collections so the former case speaking about the transition that it is about what

697
00:44:58,280 --> 00:45:02,930
there are a couple of sentences such paper all proceedings of the conference

698
00:45:02,950 --> 00:45:07,780
in their content they can be very engineering so you get document you get a

699
00:45:07,780 --> 00:45:15,030
summary or the summarisation the query based you provide we only

700
00:45:15,070 --> 00:45:18,570
information relating to the risk factors such

701
00:45:18,600 --> 00:45:24,060
it can also be used focus so that you can give me about what you

702
00:45:24,060 --> 00:45:28,060
know with respect to that to get summary

703
00:45:28,080 --> 00:45:31,600
in the approach that can be extracted for

704
00:45:31,610 --> 00:45:33,670
so performance better extract

705
00:45:33,680 --> 00:45:43,060
such extreme document this these phrases clauses sentences paragraphs or can be extracted

706
00:45:43,070 --> 00:45:45,260
so the structure is that what

707
00:45:45,260 --> 00:45:47,390
people usually do

708
00:45:47,420 --> 00:45:52,760
percent imagine to summarise text documents and what you do is would create those documents

709
00:45:52,770 --> 00:45:55,320
think about for a while

710
00:45:55,330 --> 00:45:58,220
immediately right and you know what the documents

711
00:45:58,220 --> 00:45:59,890
but then just right

712
00:45:59,920 --> 00:46:01,270
january new

713
00:46:01,340 --> 00:46:04,490
summary your understanding of the documents break

714
00:46:04,500 --> 00:46:09,540
but surprisingly to you perhaps but this is not really it's station works at the

715
00:46:09,540 --> 00:46:13,820
moment because there are problems with semantic so there's no

716
00:46:13,840 --> 00:46:16,130
a unified approach to

717
00:46:17,560 --> 00:46:21,200
the meaning of complete documents can be extracted from text

718
00:46:21,210 --> 00:46:27,480
so there are some theoretical difficulties and also even for existing theories many of them

719
00:46:27,530 --> 00:46:29,560
how to implement had to

720
00:46:29,630 --> 00:46:33,300
applied to real world problems

721
00:46:33,860 --> 00:46:40,000
what it means is that all systems especially all systems extract and extract sentences and

722
00:46:40,000 --> 00:46:41,090
this is

723
00:46:41,120 --> 00:46:44,630
local solution which works quite well sometimes

724
00:46:44,680 --> 00:46:47,900
first it's not going to what you get out and

725
00:46:47,910 --> 00:46:49,540
thanks here in summary

726
00:46:52,090 --> 00:46:56,400
so this is what all systems do you get a collection of documents for example

727
00:46:56,470 --> 00:47:00,560
is multiple musician those documents in

728
00:47:00,560 --> 00:47:06,870
and every document is split into sentences so here i don't have anything to think

729
00:47:06,870 --> 00:47:10,290
i don't have anything to point you can use it

730
00:47:10,400 --> 00:47:14,050
so we to split every document sentences

731
00:47:14,060 --> 00:47:15,020
green dot

732
00:47:15,040 --> 00:47:18,380
and then you cluster similar sentences together

733
00:47:18,410 --> 00:47:25,070
c interface inheritance after this sentences is all about the same what happened when this

734
00:47:25,070 --> 00:47:31,610
is business and what a certain person set of sentences which was

735
00:47:32,330 --> 00:47:38,620
consequences of the events so the clusters and what all systems to

736
00:47:38,640 --> 00:47:41,990
given the clusters they bring

737
00:47:42,000 --> 00:47:42,800
according to

738
00:47:42,870 --> 00:47:47,150
what and so that is something like we are very interested in what happened when

739
00:47:47,150 --> 00:47:49,370
not really interested in what

740
00:47:49,380 --> 00:47:50,980
this doesn't set

741
00:47:51,840 --> 00:47:53,990
clustering clusters

742
00:47:53,990 --> 00:47:56,800
well there probably ought to and from every cluster

743
00:47:56,820 --> 00:48:01,800
just one single sentence is extracted and put into the summary

744
00:48:01,800 --> 00:48:07,470
so this is a very reasonable approach of course there's no guarantee that what you

745
00:48:07,470 --> 00:48:08,320
get is

746
00:48:08,420 --> 00:48:11,720
because you're doing something right

747
00:48:11,720 --> 00:48:13,900
so for example imagine that

748
00:48:13,950 --> 00:48:16,580
you have a cost of those two sentences

749
00:48:16,590 --> 00:48:22,660
which are to the old i will read the first killing the syrian foreign minister

750
00:48:22,660 --> 00:48:26,260
today condemned the killing of eight civilians in the u s three as an act

751
00:48:26,260 --> 00:48:31,770
of criminal and terrorist aggression so this is the sentence comes out which was published

752
00:48:31,770 --> 00:48:32,890
by the guardian

753
00:48:32,890 --> 00:48:38,900
thanks for reminding me this is going to be based on joint work with artists

754
00:48:38,900 --> 00:48:40,530
and actors

755
00:48:40,550 --> 00:48:46,420
i always have pronouncing the name of probably not in a quarter and ravi kumar

756
00:48:47,000 --> 00:48:50,270
which will appear in KDD two thousand eight

757
00:48:50,280 --> 00:48:52,340
OK so

758
00:48:52,430 --> 00:48:57,030
let me start with an introduction to social network that's what metric is a graph

759
00:48:57,030 --> 00:49:01,890
that represents relationships between independent agents independent individual

760
00:49:01,930 --> 00:49:06,930
and we see them every day there's every everywhere especially nowadays with the advent of

761
00:49:07,040 --> 00:49:13,350
online social networks and they are making a big difference in everybody's life there are

762
00:49:13,350 --> 00:49:18,930
offline social networks social networks in the real world like network of professional contacts

763
00:49:18,970 --> 00:49:22,430
the been observed to make a big difference in

764
00:49:22,560 --> 00:49:27,760
for example when people try to find jobs or in the salary that people receive

765
00:49:27,900 --> 00:49:31,280
their location in the social networks like to make a difference

766
00:49:31,280 --> 00:49:34,280
o also for example network of colleagues in

767
00:49:34,280 --> 00:49:39,400
every profession for for example physicians or

768
00:49:40,590 --> 00:49:45,170
there's been a lot of research on how these networks affect how people learn new

769
00:49:45,170 --> 00:49:48,110
techniques for example

770
00:49:48,120 --> 00:49:54,010
use of new drugs use of new agricultural techniques and so on

771
00:49:54,010 --> 00:49:59,710
and also recently we've seen a lot of social networks on the web

772
00:49:59,760 --> 00:50:03,670
with the advent of this notion of web two point o which is essentially

773
00:50:03,700 --> 00:50:05,590
you can think about the social web

774
00:50:05,760 --> 00:50:11,140
you see a lot of examples like online social networks to explicitly expressed by social

775
00:50:11,450 --> 00:50:14,730
a social network for example facebook myspace orkut

776
00:50:14,780 --> 00:50:16,650
instant messaging software

777
00:50:16,720 --> 00:50:18,140
link in which is more

778
00:50:18,500 --> 00:50:21,950
the repertoire professionals and so on

779
00:50:21,950 --> 00:50:27,790
and there are also social networks that are a bit more hidden content sharing sites

780
00:50:27,790 --> 00:50:30,670
like flickr delicious youtube weblogs

781
00:50:30,710 --> 00:50:36,560
and finally there are social networks that are used in order to create content that

782
00:50:36,560 --> 00:50:43,730
wikipedia is essentially a social collaborative system that people

783
00:50:43,760 --> 00:50:48,150
so the point of this slide actually social networks are everywhere using them over there

784
00:50:48,150 --> 00:50:50,550
and so it's very natural to

785
00:50:50,570 --> 00:50:56,240
study the objects and try to understand them

786
00:50:56,260 --> 00:51:01,080
and as i mentioned during the past few years there has been an essentially an

787
00:51:01,080 --> 00:51:07,630
online revolution people are sitting more and more of their interactions social interactions online war

788
00:51:07,640 --> 00:51:12,110
a lot of people are keeping track of their friends online

789
00:51:13,190 --> 00:51:16,850
this is this is a lot of facts and essentially the structure of the society

790
00:51:16,850 --> 00:51:22,050
and also on social science it is pushing the number of contact person can keep

791
00:51:22,050 --> 00:51:27,130
track of the list and don't know exactly what the effect of this and so

792
00:51:27,270 --> 00:51:34,320
on this side there's been a classical result in social essentially

793
00:51:34,470 --> 00:51:38,980
look at the number of people that a person can keep track of and see

794
00:51:38,980 --> 00:51:43,020
that this this number for humans apparently this is something called to a hundred and

795
00:51:43,830 --> 00:51:46,890
and this has a large impact on the structure of society

796
00:51:46,910 --> 00:51:50,850
for example for other primates this number is much lower than that the structure of

797
00:51:50,850 --> 00:51:56,070
society is much simpler and what you see in online world is that this number

798
00:51:56,070 --> 00:51:58,850
is increasing are increasing with the help of

799
00:51:58,860 --> 00:51:59,760
on line

800
00:51:59,790 --> 00:52:01,320
social network

801
00:52:01,350 --> 00:52:04,350
and also another impact of this is that the

802
00:52:04,350 --> 00:52:09,670
we're seeing the definition of the notion of privacy with the new social network

803
00:52:09,720 --> 00:52:12,020
but there

804
00:52:12,080 --> 00:52:19,950
and finally from the perspective of social sciences this is an ideal experiment experiment lab

805
00:52:20,010 --> 00:52:23,640
it allows us to to measure and record all activities

806
00:52:23,730 --> 00:52:26,020
many very large social networks

807
00:52:26,040 --> 00:52:34,240
and the resulting get massive datasets in order to study and understand social dynamics of

808
00:52:34,260 --> 00:52:40,450
previously traditionally in social sciences datasets that they used to work with her

809
00:52:40,470 --> 00:52:43,510
very small size fifty people are so because they're all

810
00:52:43,520 --> 00:52:47,390
gather using survey data

811
00:52:48,370 --> 00:52:53,670
now the notion is going to be talking about this show social correlation the fact

812
00:52:54,460 --> 00:53:01,710
the social high play an important role role in shaping the behavior of users online

813
00:53:01,710 --> 00:53:06,110
and offline for example there there's been a lot of papers

814
00:53:06,110 --> 00:53:12,170
document in different contexts there is the paper by back from and others that look

815
00:53:12,170 --> 00:53:18,960
at the moment of joining a community on livejournal it's essentially blogging community

816
00:53:18,990 --> 00:53:23,880
and they show that actually graph in the next slide show that the probability of

817
00:53:23,880 --> 00:53:26,240
joining a community increase

818
00:53:26,270 --> 00:53:30,220
with the number of friends that you have in that community

819
00:53:30,270 --> 00:53:32,740
this is actually the graph from there

820
00:53:32,750 --> 00:53:34,060
so you see

821
00:53:34,090 --> 00:53:36,850
and in fact that this is the number of friends who already have in this

822
00:53:36,850 --> 00:53:39,750
community and the y axis you see

823
00:53:39,830 --> 00:53:44,090
the probability of you joining the same company i see that this problem is is

824
00:53:45,540 --> 00:53:48,810
at the beginning very fast and slower but still it's increasing with the number of

825
00:53:48,810 --> 00:53:51,000
from a model that that gets

826
00:53:51,370 --> 00:53:53,770
elimination independent representation of face

827
00:53:54,600 --> 00:53:56,500
and then recognise what what you seeing

828
00:53:58,240 --> 00:54:01,080
and in particular here what you can do is you can use

829
00:54:02,190 --> 00:54:03,970
prior of how images are formed

830
00:54:04,560 --> 00:54:07,560
and this is the pride that comes to us from the computer graphics community

831
00:54:08,100 --> 00:54:11,100
and the idea is that you know the way that images of form that is

832
00:54:11,100 --> 00:54:13,840
an image albedo is a light source surface normal

833
00:54:14,380 --> 00:54:17,820
it's a very very simple model of how images are formed by low so if

834
00:54:17,820 --> 00:54:21,500
you're ever played computer graphics games you know these kinds of models are used for

835
00:54:21,500 --> 00:54:22,700
generating the world

836
00:54:23,560 --> 00:54:27,480
right and we can take this prior knowledge and embedded into these

837
00:54:27,980 --> 00:54:32,820
systems in particular we can say well there's a light source is a surface normals initial beta

838
00:54:33,490 --> 00:54:35,860
and effectively we multiply three of them together

839
00:54:36,760 --> 00:54:39,360
and now we're gonna put as a prior the queen face

840
00:54:39,770 --> 00:54:43,440
elimination interface is gonna be used would be using a divorce which in this case

841
00:54:43,440 --> 00:54:45,010
we using gauss enables mission because

842
00:54:45,560 --> 00:54:47,040
the face itself is real valued

843
00:54:47,860 --> 00:54:49,380
we're gonna use the surface normal

844
00:54:49,900 --> 00:54:54,490
we also gonna be using gauss machine is a light source that specifies the direction of the light

845
00:54:56,750 --> 00:54:59,510
um is being inferred from the data you know thee

846
00:55:00,490 --> 00:55:05,330
he surface normal image shall the light source so everything is being inferred from the data based on

847
00:55:06,280 --> 00:55:06,950
observed images

848
00:55:07,340 --> 00:55:10,570
and again inference can be done with variational inference learning can be done with stochastic

849
00:55:10,570 --> 00:55:13,080
approximation because tools can be really quite

850
00:55:13,520 --> 00:55:14,920
for modeling these images

851
00:55:15,370 --> 00:55:18,540
and then there is a little bit of transfer learning where we have a set

852
00:55:18,540 --> 00:55:22,020
of you have a set of clean faces can build a generative model that fits

853
00:55:25,110 --> 00:55:28,830
you know so here's one particular example had thirty eight subjects you have forty five

854
00:55:28,950 --> 00:55:35,450
images different illumination conditions twenty eight subjects for training test for them for testing

855
00:55:35,980 --> 00:55:39,580
and this is what happens that's that's also show you this particular image

856
00:55:40,470 --> 00:55:44,070
this is being third elimination independent representation of the image

857
00:55:44,500 --> 00:55:46,080
and then you can do define things like like

858
00:55:46,560 --> 00:55:50,270
second sample different lighting directions in just like the fix

859
00:55:53,390 --> 00:55:54,720
and in terms of recognition

860
00:55:55,090 --> 00:55:57,100
just based on a single test example

861
00:55:57,540 --> 00:56:02,760
you can do fairly well in terms of recognizes compared to you know deep belief networks and such

862
00:56:03,260 --> 00:56:05,700
and then once you have six or seven different images and models

863
00:56:06,270 --> 00:56:09,450
like so singular value decomposition the fact that we ignore the prior

864
00:56:12,110 --> 00:56:16,400
but there's an interesting task where you do put some prior knowledge into these models right

865
00:56:16,880 --> 00:56:20,270
and you might want you might wonder why do sort of recognize faces and the

866
00:56:20,270 --> 00:56:24,500
difference between winning condition it's kind of interesting like for example companies like samsung

867
00:56:25,170 --> 00:56:26,750
interested in this kind of stuff because

868
00:56:27,370 --> 00:56:30,440
you know they also speak like cameron committee so you're walking

869
00:56:31,150 --> 00:56:31,510
your room

870
00:56:32,120 --> 00:56:35,450
and you have a real lighting and that if you can recognize who you are

871
00:56:35,450 --> 00:56:37,650
and basically you know recommend watching stuff

872
00:56:38,350 --> 00:56:39,530
right or maybe like

873
00:56:39,940 --> 00:56:40,640
show you and then

874
00:56:41,330 --> 00:56:42,160
which is more likely

875
00:56:44,520 --> 00:56:49,420
so what about dealing with occlusions or structured noise right this is another particular example let's say

876
00:56:49,880 --> 00:56:52,580
you get to see observed image but there is a structured noise

877
00:56:54,110 --> 00:56:55,340
covering part of the input

878
00:56:56,010 --> 00:56:56,990
right and by the way what we do

879
00:56:57,440 --> 00:57:02,580
everything here can be applied in a lot of different application domains there's nothing specific in these models

880
00:57:03,860 --> 00:57:04,710
what you'd like to do

881
00:57:05,770 --> 00:57:08,960
is you'd like to essentially build the model we have a clean face

882
00:57:10,260 --> 00:57:12,150
you're recovering the shape of the quota

883
00:57:12,670 --> 00:57:16,960
right so he can use against billion or dibble's mission to model the face

884
00:57:17,560 --> 00:57:21,960
you actually have a binary idea modeling the mask and then you have to putting

885
00:57:21,960 --> 00:57:25,060
a prior how these two different sources interact together

886
00:57:25,500 --> 00:57:27,100
we actually produce the observed image

887
00:57:28,610 --> 00:57:31,240
and there is a little bit of gas in noise now the details are not

888
00:57:31,240 --> 00:57:34,570
important but what is important is that you basically having a model that models the

889
00:57:36,140 --> 00:57:41,260
model that models the shape of the kluwer and then you combining these two together see putting some prior knowledge

890
00:57:41,830 --> 00:57:43,060
about how shape

891
00:57:43,500 --> 00:57:45,970
as well as the object itself should be separated

892
00:57:47,220 --> 00:57:51,870
right one interesting thing about this model is that if you look at the distribution of this data

893
00:57:52,350 --> 00:57:53,850
given these latent features

894
00:57:54,460 --> 00:57:57,450
it turns out that this distribution has a heavy tailed distribution

895
00:57:58,090 --> 00:58:01,000
right and any time you see a heavy tailed or robust

896
00:58:01,510 --> 00:58:04,120
the heavy tailed distribution should sort of should bring about

897
00:58:05,290 --> 00:58:07,370
right anytime you sort of try to deal with noise

898
00:58:08,000 --> 00:58:12,190
you should be putting heavy tailed distributions that can capture they they're not structure

899
00:58:13,190 --> 00:58:13,630
but here's

900
00:58:14,050 --> 00:58:15,060
you know what the model can do

901
00:58:15,620 --> 00:58:18,720
so this is what happens during the training so this is the case

902
00:58:19,340 --> 00:58:19,730
and then

903
00:58:20,270 --> 00:58:24,350
what you're inferring is inference phase as well as the glasses a particular shape

904
00:58:25,020 --> 00:58:28,230
the shape of the inclusion and this is what happens if the test is a

905
00:58:28,230 --> 00:58:30,050
test subject and this is the first phase

906
00:58:30,670 --> 00:58:33,160
now obviously this is not true inference phase

907
00:58:33,780 --> 00:58:38,010
this is the phase that the mobilization look like the generative process if you stimulate

908
00:58:38,070 --> 00:58:40,390
more you'll probably have a slightly different face

909
00:58:41,300 --> 00:58:43,880
have a whole distribution over what it should look like

910
00:58:45,330 --> 00:58:47,400
the same force car

911
00:58:47,800 --> 00:58:52,720
and then one interesting thing about these more by just putting this prior knowledge you can do quite well

912
00:58:53,170 --> 00:58:54,560
right so for example here

913
00:58:57,350 --> 00:59:01,770
you know with robust multi can get eighty percent recognition varies at the pixel level

914
00:59:01,770 --> 00:59:04,160
you know you're sitting at seventy percent fifty percent right

915
00:59:04,560 --> 00:59:09,660
so it's quite a dramatic increase by just putting a little bit of prior knowledge into these models

916
00:59:10,310 --> 00:59:13,450
right or modeling the source of noise corruption order

917
00:59:14,090 --> 00:59:14,760
missing data

918
00:59:16,300 --> 00:59:21,120
the same thing goes for speech recognition that's another big application of of these models where you have

919
00:59:23,560 --> 00:59:25,880
you're looking at twenty five millisecond frames new

920
00:59:26,690 --> 00:59:28,520
extract phonetic labels

921
00:59:29,150 --> 00:59:30,800
and this is one particular example aware

922
00:59:32,520 --> 00:59:37,650
you know u u basically trying to the spoken query detection this is the case where r

923
00:59:37,920 --> 00:59:39,570
you know you have a bunch of keywords

924
00:59:40,040 --> 00:59:40,560
and you're trying to

925
00:59:41,170 --> 00:59:44,560
detect whether you're observing this keywords when somebody box

926
00:59:45,210 --> 00:59:48,730
you know you can imagine it's very useful thing for like that's your forms right

927
00:59:49,210 --> 00:59:51,730
so talk on the phone and he says particular words

928
00:59:52,650 --> 00:59:54,090
the system can catch it right

929
00:59:56,790 --> 00:59:59,960
you know so far for these kinds of things when using these models

930
01:00:01,710 --> 01:00:02,090
you can get

931
01:00:02,610 --> 01:00:09,790
fairly good results compared to traditional speech recognition systems speech recognition systems that using gas mixture models

932
01:00:10,630 --> 01:00:11,400
so there's a lot of

933
01:00:12,270 --> 01:00:16,960
there's a pretty big discrepancies a pretty big gap from sixteen percent delta nine percent

934
01:00:17,480 --> 01:00:18,300
right before the war

935
01:00:19,020 --> 01:00:19,750
four using these

936
01:00:20,860 --> 01:00:21,340
these models

937
01:00:23,440 --> 01:00:23,880
let me now

938
01:00:24,420 --> 01:00:28,940
switch gears a little bit and talk about a slightly different but related problem which

939
01:00:28,940 --> 01:00:33,230
is also interesting problem in learning community people trying to look at

940
01:00:34,020 --> 01:00:36,500
the idea of transfer learning one shot learning

941
01:00:38,710 --> 01:00:41,400
so let just illustrate why this is an important problem

942
01:00:42,340 --> 01:00:45,770
suppose i show you this handwritten characters and i tell you that that's an image as art

943
01:00:47,540 --> 01:00:49,230
how many of you think this is an image as art

944
01:00:50,570 --> 01:00:51,170
what about this one

945
01:00:53,230 --> 01:00:54,020
do you think this is arc

946
01:00:55,630 --> 01:00:57,110
yeah what about what about this one

947
01:00:59,440 --> 01:01:00,590
yes what about this one

948
01:01:03,170 --> 01:01:04,170
what's happening here

949
01:01:04,820 --> 01:01:07,800
what's happening here is that you just seen one example

950
01:01:08,380 --> 01:01:09,630
right and you're able to generalize

951
01:01:11,170 --> 01:01:13,150
right what so far we've seen

952
01:01:13,630 --> 01:01:18,170
is we've seen sixty thousand examples are images of tools and then the model says

953
01:01:18,610 --> 01:01:21,750
finally understand what makes it to a to and i can recognise things

954
01:01:23,210 --> 01:01:27,040
obviously you know same goes vision if i show you a single image overseas where

955
01:01:27,040 --> 01:01:29,230
you recognise what is and can generalize

956
01:01:30,860 --> 01:01:36,560
so one of the fundamental problems in machine learning is is how can we learn

957
01:01:36,570 --> 01:01:40,290
new concept which is a high-dimensional statistical object from few examples

958
01:01:41,270 --> 01:01:46,320
and obviously humans can do that's can we teach our machines to do that's very challenging problem

959
01:01:47,400 --> 01:01:49,270
right so if you look at the traditional

960
01:01:49,270 --> 01:01:52,200
OK so

961
01:01:52,280 --> 01:01:53,700
before i do

962
01:01:53,720 --> 01:01:56,650
junction tree which is here

963
01:01:56,660 --> 01:02:00,590
the key is one of the main

964
01:02:00,640 --> 01:02:05,280
algorithms for exact inference is limited notation just easier to work with so

965
01:02:05,300 --> 01:02:11,220
OK so i have a vector x right it's don't know random variables the

966
01:02:11,240 --> 01:02:15,480
joint probability function or density function

967
01:02:15,490 --> 01:02:19,370
is some subset so if i say x of today i mean

968
01:02:19,400 --> 01:02:23,770
just a subset of variables in x that are in a right so

969
01:02:23,820 --> 01:02:26,080
and if i have a x one

970
01:02:26,090 --> 01:02:30,370
through x and is is two

971
01:02:30,390 --> 01:02:34,470
so and then it's x two and x

972
01:02:34,560 --> 01:02:37,010
so x a is this

973
01:02:37,770 --> 01:02:42,450
so that the examples and so

974
01:02:42,460 --> 01:02:45,910
right and then

975
01:02:45,970 --> 01:02:48,400
in this notation for four

976
01:02:49,820 --> 01:02:55,080
it's OK so that and then we need additional so for

977
01:02:57,190 --> 01:03:02,220
assignments threats to these kind of false i'm afraid so an assignment to a set

978
01:03:02,220 --> 01:03:02,980
of these

979
01:03:02,990 --> 01:03:06,060
very what value assigned to try

980
01:03:06,140 --> 01:03:11,090
so what about the same variables and then

981
01:03:11,100 --> 01:03:13,910
you know it's a with this sort of

982
01:03:13,950 --> 01:03:18,800
will face kind of x is the set of all possible assignments and then we

983
01:03:18,800 --> 01:03:22,800
need to do is look at kind of subset of scientific assignment to before one

984
01:03:22,800 --> 01:03:27,530
through and i may be asking about what is the assignment to variable two and

985
01:03:27,610 --> 01:03:31,440
sent to the same trick kind of indexing it by

986
01:03:31,450 --> 01:03:32,500
this subset

987
01:03:32,550 --> 01:03:37,250
sort of on the subscript means the subset of the assignment editor but right now

988
01:03:37,250 --> 01:03:39,120
those things are in the

989
01:03:39,170 --> 01:03:43,570
and if i do union then this is sort of a joint

990
01:03:43,590 --> 01:03:45,060
or if you take the

991
01:03:45,080 --> 01:03:46,530
what you says

992
01:03:46,540 --> 01:03:50,880
to assign and with this assigned to get so this notation but

993
01:03:50,890 --> 01:03:53,070
it's important to

994
01:03:53,080 --> 01:03:54,510
you get lost so

995
01:03:54,530 --> 01:04:00,160
and it so any in function of users just it's the function of the variables

996
01:04:02,690 --> 01:04:07,580
OK so

997
01:04:07,600 --> 01:04:11,720
just in this notation now we can can restate some things right so have the

998
01:04:11,730 --> 01:04:15,680
entire set so this is a

999
01:04:15,700 --> 01:04:21,140
interested in and i am interested in public particularly

1000
01:04:21,190 --> 01:04:25,430
so to get this i marginalise out the entire distribution so i think the

1001
01:04:25,530 --> 01:04:29,380
union with u where u is in x

1002
01:04:29,390 --> 01:04:35,130
seven skip a bar meaning to a bar is the complement of a some sort

1003
01:04:35,130 --> 01:04:41,490
of all possible node an extra bar x a bar means the

1004
01:04:41,500 --> 01:04:46,300
so the rest of the possible assign because the morals innovation

1005
01:04:46,350 --> 01:04:47,930
there exist

1006
01:04:47,940 --> 01:04:50,710
additional thing

1007
01:04:50,720 --> 01:04:54,850
and then you know he is this the right now like this so basically an

1008
01:04:54,850 --> 01:04:56,180
assignment here

1009
01:04:56,190 --> 01:05:00,940
this function only looks the part of the assignment cases function and just cares about

1010
01:05:00,940 --> 01:05:02,410
the part

1011
01:05:02,430 --> 01:05:04,220
that's very intuitive

1012
01:05:05,560 --> 01:05:11,040
OK so with that hand

1013
01:05:11,050 --> 01:05:14,980
the inference is that we have a vector variable that we care about this is

1014
01:05:15,340 --> 01:05:20,130
all the way over density that written as the product so we met business into

1015
01:05:20,140 --> 01:05:21,440
the formal

1016
01:05:22,790 --> 01:05:26,570
there is some evidence assignment that will incorporate into their

1017
01:05:26,620 --> 01:05:31,850
and there are some query variables denoted by x q q is a subset of

1018
01:05:32,960 --> 01:05:36,970
OK so we have the p of q given

1019
01:05:37,100 --> 01:05:42,260
all possible variables and w is the assignment to yi

1020
01:05:42,280 --> 01:05:45,960
how to incorporate evidence

1021
01:05:45,980 --> 01:05:51,190
so the probability of everything but each given e

1022
01:05:51,210 --> 01:05:54,470
is a commune in the end

1023
01:05:54,490 --> 01:05:56,630
divided by p

1024
01:05:56,650 --> 01:05:58,530
he said the w

1025
01:05:58,540 --> 01:06:00,590
which is basically the product of that

1026
01:06:00,600 --> 01:06:05,470
this is the full joint writing and here station is OK well i think the

1027
01:06:05,470 --> 01:06:10,330
part that they need from from you think about the domain from w in each

1028
01:06:10,330 --> 01:06:13,360
clique have some variables assigned someone not

1029
01:06:13,380 --> 01:06:18,400
and then divided by the probability of evidence so

1030
01:06:18,430 --> 01:06:20,300
we can essentially

1031
01:06:23,870 --> 01:06:27,260
absorb those factors into the

1032
01:06:27,280 --> 01:06:29,980
and calling things the prime right so he

1033
01:06:30,000 --> 01:06:33,900
e w goes into the prime and also

1034
01:06:33,910 --> 01:06:38,890
whatever is observed here i just instantiate the value and kind of collapse the potential

1035
01:06:38,910 --> 01:06:41,000
i just worry about things that unobserved

1036
01:06:41,060 --> 01:06:45,320
there is that they can sugar but basically

1037
01:06:45,340 --> 01:06:49,440
when do want to get evidence i think the potentials instantiate the values that of

1038
01:06:49,440 --> 01:06:53,600
the evidence that rings the potentials and now only a function of the subset of

1039
01:06:53,600 --> 01:06:55,790
the variables that are unobserved

1040
01:06:55,800 --> 01:07:00,180
and and then what the probability of evidence just goes into the partition function anyway

1041
01:07:00,570 --> 01:07:05,460
so for example for business the partition function was one right because we had everything

1042
01:07:05,460 --> 01:07:07,000
was normalised so

1043
01:07:07,850 --> 01:07:11,660
this is actually becomes exact probabilities evidence

1044
01:07:11,670 --> 01:07:13,960
so this is

1045
01:07:13,970 --> 01:07:19,000
what i said words

1046
01:07:19,200 --> 01:07:22,470
so now we can just reset the problem is finding

1047
01:07:22,480 --> 01:07:23,640
he view

1048
01:07:23,650 --> 01:07:27,180
people use this computer marginal the

1049
01:07:27,200 --> 01:07:31,980
and so this is already instantiated evidence right we to here we forgot about and

1050
01:07:31,980 --> 01:07:33,210
it's already

1051
01:07:33,230 --> 01:07:34,940
inside the inside

1052
01:07:34,950 --> 01:07:37,410
these guys

1053
01:07:37,420 --> 01:07:41,460
and so it's just what i need to do to compute this thing is to

1054
01:07:41,460 --> 01:07:45,100
some out everything that's not in q

1055
01:07:45,120 --> 01:07:49,930
with this partition function and you now is split into things that are in q

1056
01:07:49,930 --> 01:07:51,830
and things that are

1057
01:07:56,000 --> 01:07:57,120
OK so

1058
01:07:57,140 --> 01:07:58,760
now let's look at the

1059
01:07:58,810 --> 01:08:00,440
variable elimination in this

1060
01:08:02,690 --> 01:08:06,310
notation so as additional function i'm something out overall user

1061
01:08:06,320 --> 01:08:08,490
that are in the complement of q

1062
01:08:08,510 --> 01:08:11,100
all the different assignments and this is the

1063
01:08:11,160 --> 01:08:13,090
the actual private care about

1064
01:08:13,090 --> 01:08:19,560
so i think that the evidence that there

1065
01:08:19,580 --> 01:08:22,800
scotland backing what that incorporated

1066
01:08:28,740 --> 01:08:34,390
basically what this is what was talking about is when

1067
01:08:34,400 --> 01:08:38,080
so we have

1068
01:08:38,330 --> 01:08:40,830
evidence nodes

1069
01:08:42,350 --> 01:08:47,720
he said i is my evidence nodes one i

1070
01:08:47,820 --> 01:08:53,380
eliminate those i create new nation cliques which is all of the things that that

1071
01:08:53,400 --> 01:08:54,360
had that

1072
01:08:55,070 --> 01:08:56,530
the that peak

1073
01:08:58,900 --> 01:09:03,520
and then we have a q this this is the product of a potential that

1074
01:09:04,520 --> 01:09:06,190
only or

1075
01:09:06,200 --> 01:09:07,690
or q

1076
01:09:09,570 --> 01:09:12,960
so i mean that we're going to get you out in the but to get

1077
01:09:12,960 --> 01:09:14,520
these things we need to sum up

1078
01:09:14,530 --> 01:09:19,070
everything else it's not any the evidence and not a q

1079
01:09:19,120 --> 01:09:20,770
OK so

1080
01:09:20,800 --> 01:09:24,530
and variable elimination and we just you know because variables that we are interested in

1081
01:09:24,530 --> 01:09:28,620
we send them out and it was very specific to the actual

1082
01:09:28,620 --> 01:09:34,100
crew were asking right so once we we will be asking about probability of one

1083
01:09:34,100 --> 01:09:38,210
his for example so let's say we have some data points

1084
01:09:39,060 --> 01:09:42,790
so we're going to blue points here i don't know whether you can see that

1085
01:09:42,960 --> 01:09:47,850
so their points and their circles around these points what the size of the circle

1086
01:09:48,230 --> 01:09:50,210
is proportional to the

1087
01:09:50,270 --> 01:09:53,790
in proceedings of the past so this is why

1088
01:09:53,790 --> 01:09:55,040
y t here

1089
01:09:55,040 --> 01:09:57,750
OK so there are some points which are the boundary

1090
01:09:57,750 --> 01:10:01,680
which are very grandchildren at some points outside the chart position

1091
01:10:01,730 --> 01:10:05,000
this is the case without noise

1092
01:10:05,180 --> 01:10:09,750
let's say we use adaboost now we have a few examples where we added noise

1093
01:10:09,750 --> 01:10:11,580
for instance this one here

1094
01:10:11,620 --> 01:10:13,660
so the mislabeled example

1095
01:10:13,680 --> 01:10:17,480
there was here so another one here and one here

1096
01:10:17,660 --> 01:10:21,660
so the influence of those examples increases drastically so this is the example which has

1097
01:10:21,660 --> 01:10:22,730
the largest

1098
01:10:22,750 --> 01:10:27,350
paid so the queen is about decision boundary which is generated by adaboost

1099
01:10:27,370 --> 01:10:29,930
OK and he changes of quite a bit

1100
01:10:30,040 --> 01:10:32,790
if you know use

1101
01:10:33,050 --> 01:10:39,210
and drake with this definition of the inference then somehow

1102
01:10:39,230 --> 01:10:45,330
this example is weighted down because the artificially increase the margin therefore the influence of

1103
01:10:45,330 --> 01:10:47,620
the example in the next iteration goes down

1104
01:10:47,620 --> 01:10:52,580
it's essentially the examples was the most with the smallest margin given the most weight

1105
01:10:52,580 --> 01:10:56,750
so it can be artificially increase the weight of the model the example for the

1106
01:10:56,750 --> 01:10:59,890
weight goes down and the decision boundary is much much better

1107
01:11:01,120 --> 01:11:04,770
so it it doesn't have this standing here

1108
01:11:11,330 --> 01:11:16,600
the positive thing about this i was it really improve the classification performance of adaboost

1109
01:11:16,810 --> 01:11:20,040
it really helped in making the most much more robust

1110
01:11:20,180 --> 01:11:24,640
it was one of the first things addressing the overfitting problem in boosting

1111
01:11:25,270 --> 01:11:31,000
the negative thing about this article was that it was essentially modification one algorithmic level

1112
01:11:31,000 --> 01:11:33,000
so we couldn't you couldn't prove it

1113
01:11:33,040 --> 01:11:38,370
anything about this we could show that is converging to some minimum which was meaningful

1114
01:11:38,370 --> 01:11:42,950
something was more like modification of some quantity inside the

1115
01:11:42,960 --> 01:11:46,870
we couldn't show that to solve some optimisation problems

1116
01:11:46,950 --> 01:11:50,370
so and of course we didn't have generalization results

1117
01:11:50,390 --> 01:11:53,640
so and it took me a while to understand what what what i should do

1118
01:11:53,640 --> 01:11:58,640
that i mean i was quite happy with i wouldn't but you couldn't show anything

1119
01:11:58,640 --> 01:12:05,770
and essentially the idea was that we understand first what's happening in other in adaboost

1120
01:12:05,790 --> 01:12:10,540
i mean what optimisation problem is which is optimized and then really try to understand

1121
01:12:10,540 --> 01:12:15,580
the techniques of optimization then we go back to the optimisation problem changed optimisation problem

1122
01:12:15,910 --> 01:12:20,310
and then apply all the techniques to solve this better optimisation from this fuzzy

1123
01:12:20,330 --> 01:12:21,930
the best strategy

1124
01:12:22,020 --> 01:12:28,290
then we can use the convergence results for leveraging and apply the margin bound that's

1125
01:12:28,290 --> 01:12:31,390
what i'm showing you know

1126
01:12:31,480 --> 01:12:35,500
OK so you have seen the maximum margin problems so no

1127
01:12:35,520 --> 01:12:38,620
this is an extension of the maximisation problem

1128
01:12:38,620 --> 01:12:43,120
so the idea is that you so using maximum margin constraints the margin of the

1129
01:12:43,120 --> 01:12:45,230
example and should be greater than role

1130
01:12:45,330 --> 01:12:51,310
but now you have this additional variables science this is often to select variables

1131
01:12:51,390 --> 01:12:56,600
so for some of these constraints you not enforcing that this one is larger than

1132
01:12:56,680 --> 01:13:02,460
three allow some of those science to be positive so this constraints relaxed

1133
01:13:03,160 --> 01:13:08,660
and then you penalizes the some of these clients so you maximizing role

1134
01:13:08,660 --> 01:13:12,080
and subtracting the sum of basic science

1135
01:13:12,600 --> 01:13:14,160
this is soft much

1136
01:13:14,190 --> 01:13:18,190
so again we have the l one norm of the office equal to one and

1137
01:13:18,190 --> 01:13:22,660
this is very similar to the soft margin SVM

1138
01:13:22,680 --> 01:13:25,120
so this is the so-called new

1139
01:13:25,140 --> 01:13:28,390
formulation because we are using it part of it and you here

1140
01:13:28,520 --> 01:13:33,040
and it turns out i mean there's some results which shows that the fraction of

1141
01:13:33,040 --> 01:13:38,910
points which this science are not equal to zero is essentially the margin error

1142
01:13:38,910 --> 01:13:43,680
o is bounded by the new in some way is related to the new

1143
01:13:43,750 --> 01:13:47,660
so the empirical margin error is about a new service called the new to the

1144
01:13:47,660 --> 01:13:49,980
new trick is

1145
01:13:53,370 --> 01:13:56,980
the question

1146
01:13:57,000 --> 01:13:59,250
yes it should

1147
01:13:59,950 --> 01:14:02,230
maybe at this history is like

1148
01:14:02,390 --> 01:14:08,450
so it's not negative then

1149
01:14:08,520 --> 01:14:10,210
it's happening

1150
01:14:10,230 --> 01:14:16,040
i mean really have complementation closed closest sets rose always on it

1151
01:14:22,710 --> 01:14:26,120
so now we can actually with this i mean

1152
01:14:26,140 --> 01:14:32,080
i give you this optimisation problem might so new is essentially the maximum margin margin

1153
01:14:32,140 --> 01:14:34,810
now we can actually go back to the boundary which have shown you the margin

1154
01:14:36,560 --> 01:14:40,560
this was stating that the expected error is smaller or equal

1155
01:14:40,890 --> 01:14:43,330
to the margin

1156
01:14:43,390 --> 01:14:45,810
and this is actually related to two new

1157
01:14:45,830 --> 01:14:51,250
OK so this is the new parliament this was the primary of plus something

1158
01:14:51,270 --> 01:14:55,140
which depends essentially on the basis of set but also on the margin

1159
01:14:55,140 --> 01:15:00,660
OK so what what this last optimisation problem i solving was essentially try to maximize

1160
01:15:00,680 --> 01:15:02,810
the margin

1161
01:15:02,830 --> 01:15:09,060
much was founded by this new variables right so this parameter new

1162
01:15:09,080 --> 01:15:11,000
OK essentially

1163
01:15:11,000 --> 01:15:16,810
bb six this this we say you should be no larger than this

1164
01:15:16,830 --> 01:15:21,120
this much should not be large and this then the maximize role

1165
01:15:21,140 --> 01:15:26,270
what he so that means minimizing this bound

1166
01:15:28,770 --> 01:15:32,680
so this this optimisation problems really motivated from

1167
01:15:32,730 --> 01:15:37,430
from this the margin bound

1168
01:15:37,430 --> 01:15:44,230
OK so how do we get a regularized version of boosting

1169
01:15:44,480 --> 01:15:49,680
so first we have to define the optimisation problem so this might be the new

1170
01:15:50,140 --> 01:15:53,210
LP problem which i've just shown you

1171
01:15:53,290 --> 01:15:56,100
or you can also do regression i'm showing one example

1172
01:15:56,210 --> 01:16:02,100
and then having the optimisation problem we just have to go back and reconstructed posting

1173
01:16:02,100 --> 01:16:07,910
on from that so we can use essentially this leveraging technique we can use the

1174
01:16:07,960 --> 01:16:13,730
various technique or this current generation techniques of people's like with

1175
01:16:13,730 --> 01:16:18,660
so and then we have an article which rigid optimise this optimisation problem which can

1176
01:16:18,660 --> 01:16:23,540
solve optimisation problems using the base learner to call several times to generate constraint and

1177
01:16:23,540 --> 01:16:25,770
so on and so were gradient

1178
01:16:25,770 --> 01:16:31,810
and then you can use essentially the convergence theorems which which i've shown you to

1179
01:16:31,810 --> 01:16:34,710
prove convergence for for this particular i

1180
01:16:34,890 --> 01:16:38,040
essentially this has been done in several races so

1181
01:16:38,080 --> 01:16:45,180
i mean you different ways so is more the original boosting type techniques very methods

1182
01:16:45,290 --> 01:16:52,770
the column generation methods and essentially for linear programming classification there different versions there there

1183
01:16:52,830 --> 01:16:58,680
some names for the new LP classification there are some some other names of algorithms

1184
01:16:58,680 --> 01:17:02,580
and they're all the papers which can read

1185
01:17:02,640 --> 01:17:04,330
for regression this sum

1186
01:17:04,350 --> 01:17:06,980
for one class classification is also one

1187
01:17:07,120 --> 01:17:12,460
and for multiclass i think there's also some people about that

1188
01:17:12,480 --> 01:17:17,450
and essentially when you have some other optimization problem which some changed optimisation problem and

1189
01:17:17,450 --> 01:17:22,600
then you can just go take these techniques and improve you the convergence and the

1190
01:17:22,600 --> 01:17:24,060
director of

1191
01:17:24,250 --> 01:17:30,310
OK i would like to illustrate this one case

1192
01:17:30,310 --> 01:17:34,790
also you have also heard called models for minimal

1193
01:17:34,810 --> 01:17:37,860
unsatisfiability preserving seventy boxes

1194
01:17:37,880 --> 01:17:40,320
that's what came out of

1195
01:17:40,650 --> 01:17:45,840
stephan slovaks worked on the navy's from funds work which is

1196
01:17:45,880 --> 01:17:48,690
minimal like minimal axiom sets

1197
01:17:48,710 --> 01:17:56,560
so just to give you more proper definition

1198
01:17:56,590 --> 01:18:02,310
if we have an ontology which is really just a set of axioms

1199
01:18:02,320 --> 01:18:04,270
and we have some entailments

1200
01:18:04,440 --> 01:18:10,810
eta which is entailed by the ontology

1201
01:18:10,840 --> 01:18:15,320
then our justification is just a subset of the ontology

1202
01:18:15,340 --> 01:18:19,860
it's just a set of axioms

1203
01:18:19,880 --> 01:18:21,960
go is a subset of the ontology

1204
01:18:21,980 --> 01:18:26,750
and importantly the entailment holds in this subset of the ontology and a set of

1205
01:18:26,750 --> 01:18:28,520
axioms as well

1206
01:18:28,540 --> 01:18:31,840
and the key point is that is the minimal set so

1207
01:18:31,980 --> 01:18:35,420
for any proper subset that would take for justification

1208
01:18:36,090 --> 01:18:37,560
the subset

1209
01:18:37,560 --> 01:18:42,500
well it's intelligence helmet here

1210
01:18:42,520 --> 01:18:44,420
so one way

1211
01:18:45,130 --> 01:18:47,060
if we just look at this the kind of more

1212
01:18:47,060 --> 01:18:48,540
pictorial sense

1213
01:18:48,560 --> 01:18:51,000
if you imagine it to be ontology

1214
01:18:52,130 --> 01:18:56,790
these blocks here represent axioms

1215
01:18:57,020 --> 01:19:00,320
then if we have some entailment that we're interested in and we want to know

1216
01:19:00,320 --> 01:19:03,820
the reason why we might have a hell of a lot of axioms search you

1217
01:19:03,820 --> 01:19:06,250
consider something like the NCI ontology

1218
01:19:06,310 --> 01:19:11,810
which contains one thousand victims of matter which also contains hundreds of thousands of axioms

1219
01:19:12,020 --> 01:19:14,480
you could have a lot of action searches so

1220
01:19:14,650 --> 01:19:19,090
my justification pulls out and isolates the axioms

1221
01:19:19,190 --> 01:19:23,090
responsible for the entailment and these may only be a small subset of the actual

1222
01:19:25,060 --> 01:19:27,980
so we can focus our attention on to this

1223
01:19:29,840 --> 01:19:33,340
if we remove one axiom from the justification

1224
01:19:34,230 --> 01:19:39,520
it's no longer justification an entailment doesn't hold in this is this subset of axioms

1225
01:19:39,590 --> 01:19:44,040
and this is the way we go about paleontology

1226
01:19:44,060 --> 01:19:47,290
and now it's important to notice that the

1227
01:19:47,360 --> 01:19:53,130
keep something project it's important to notice that there may be more than one justification

1228
01:19:53,150 --> 01:19:55,400
for a given entailment

1229
01:19:55,420 --> 01:19:57,630
and also

1230
01:19:57,650 --> 01:20:04,150
these justifications can overlap so we can get actually being shared between separate justifications but

1231
01:20:04,150 --> 01:20:06,770
if we want to understand the reasons for entailment

1232
01:20:06,810 --> 01:20:09,920
and we need to look at all of these justifications and if you want to

1233
01:20:09,920 --> 01:20:11,540
repair the ontology

1234
01:20:11,560 --> 01:20:18,110
we need to move one axiom from each other justifications if if this is justification

1235
01:20:18,110 --> 01:20:20,840
four unsatisfiable class

1236
01:20:20,840 --> 01:20:23,820
then we can turn the class of fireball

1237
01:20:23,840 --> 01:20:27,270
by removing the axioms that define class to take one out from each of these

1238
01:20:27,270 --> 01:20:35,690
justifications here

1239
01:20:35,710 --> 01:20:41,380
so just to reiterate important points and maybe multiple justifications for an entailment

1240
01:20:41,420 --> 01:20:47,380
the kind of a lot and removing one axiom from each justification breaks the justifications

1241
01:20:47,420 --> 01:20:50,750
that entailment is no longer supported by the remaining axioms and if we do that

1242
01:20:50,750 --> 01:21:00,960
for all justifications for a given entailment and this is what we call the pa

1243
01:21:00,980 --> 01:21:06,540
and now we know what justification so we can make description of written sets for

1244
01:21:06,540 --> 01:21:10,170
root unsatisfiable classes slightly more formal

1245
01:21:11,250 --> 01:21:16,980
the class is a derived unsatisfiable class if it has a justification that's super set

1246
01:21:17,040 --> 01:21:22,210
of the justification for some other unsatisfiable class so we've got a set of axioms

1247
01:21:22,210 --> 01:21:25,080
that say what classes is unsatisfiable

1248
01:21:25,080 --> 01:21:28,610
and if there's some other unsatisfiable class and we have a small set contained in

1249
01:21:28,610 --> 01:21:32,170
the set of axioms that explain why so the classes unsatisfiable

1250
01:21:32,190 --> 01:21:36,230
then we know that are first class is a derived unsatisfiable class

1251
01:21:36,230 --> 01:21:43,400
and any unsatisfiable class that is not derived unsatisfiable class is a region satisfiable class

1252
01:21:43,400 --> 01:21:58,730
so this is that none of these justifications contains the justification of another unsatisfiable class

1253
01:21:58,770 --> 01:22:02,880
any questions up until now

1254
01:22:02,880 --> 01:22:12,440
we can further split down into a partially derived unsatisfiable classes in purely derived unsatisfiable

1255
01:22:12,440 --> 01:22:18,080
classes partially derived unsatisfiable classes

1256
01:22:18,080 --> 01:22:23,270
a derived unsatisfiable fireball classes for which there is at least one justification

1257
01:22:23,340 --> 01:22:28,610
that is not super set of justifications for other unsatisfiable classes

1258
01:22:28,790 --> 01:22:35,460
purely derived unsatisfiable classes unsatisfiable classes for which all of the justifications are supersets of

1259
01:22:35,460 --> 01:22:41,270
justifications for other unsatisfiable classes now the ramifications of this is that if we want

1260
01:22:41,270 --> 01:22:42,900
to repair the ontology

1261
01:22:42,940 --> 01:22:45,250
we can affect clusters are partially

1262
01:22:45,270 --> 01:22:48,040
derived unsatisfiable class

1263
01:22:48,090 --> 01:22:54,310
then we can start to fix it by looking at the onset satisfiable classes and

1264
01:22:55,110 --> 01:22:56,520
actions then

1265
01:22:56,540 --> 01:22:59,940
but they will will play have to do some iterations

1266
01:23:01,090 --> 01:23:07,190
the same justifications here that don't have axioms in the overlap with the solution size

1267
01:23:07,190 --> 01:23:09,380
five classes

1268
01:23:09,380 --> 01:23:11,820
is not active at times

1269
01:23:11,840 --> 01:23:16,920
here she has exactly a friend a active friends at the beginning of the times

1270
01:23:16,940 --> 01:23:21,610
and she becomes active in this step so this is the official FIFA and similarly

1271
01:23:21,620 --> 01:23:23,360
can define and a

1272
01:23:23,360 --> 01:23:27,530
exactly the same way except that this is the number of pairs

1273
01:23:27,580 --> 01:23:30,980
the user does not become active in times

1274
01:23:33,830 --> 01:23:36,950
if alpha and beta are given the probability that

1275
01:23:37,010 --> 01:23:37,860
we get

1276
01:23:37,910 --> 01:23:43,070
this scenario we get this data set can be given by this expression products

1277
01:23:43,070 --> 01:23:45,520
p eight our y

1278
01:23:45,680 --> 01:23:51,050
one minus the power and a therefore for for solving the maximum likelihood problem essentially

1279
01:23:51,060 --> 01:23:53,930
we want to maximize this objective function

1280
01:23:53,950 --> 01:23:57,340
find alpha beta maximize this function

1281
01:23:57,380 --> 01:24:04,360
and you can do this using a number offers thank you very much

1282
01:24:04,400 --> 01:24:10,430
also for convenience both for theoretical results and also for implementation capping the number of

1283
01:24:10,540 --> 01:24:13,140
active from the value or which is

1284
01:24:13,180 --> 01:24:14,760
o thing and

1285
01:24:14,840 --> 01:24:17,860
basically the point is that more than that

1286
01:24:17,910 --> 01:24:20,000
rarely happens anyway so

1287
01:24:20,670 --> 01:24:24,040
it's not going to estimate much but it's going to

1288
01:24:24,050 --> 01:24:25,960
help in

1289
01:24:26,000 --> 01:24:30,370
increasing concentration reduced the right

1290
01:24:30,410 --> 01:24:34,580
OK so the maximum likelihood problem one lemon it's going to be handed later on

1291
01:24:34,580 --> 01:24:39,070
that there is a unique solution alpha but unlike problem

1292
01:24:39,120 --> 01:24:43,570
this is interesting because this function this

1293
01:24:43,590 --> 01:24:48,210
likelihood function that we have here is NOT concave functions should when you have unique

1294
01:24:48,210 --> 01:24:50,200
this is because we have concavity

1295
01:24:50,230 --> 01:24:52,900
but even though you don't have concavity

1296
01:24:52,950 --> 01:24:55,030
can show that if you have two

1297
01:24:55,040 --> 01:24:57,370
o point for maximizing the function

1298
01:24:57,400 --> 01:25:02,300
there is a path between them such that the function is concave along that particular

1299
01:25:02,300 --> 01:25:05,680
at the the result that gives a contradiction so you can have two

1300
01:25:07,480 --> 01:25:11,450
and the reason that this lemma is going to be useful

1301
01:25:11,470 --> 01:25:16,540
in now to have gone analysis that essentially the same or some local used to

1302
01:25:16,540 --> 01:25:17,720
show that

1303
01:25:17,730 --> 01:25:20,890
the observation that we estimate from max

1304
01:25:20,900 --> 01:25:29,620
the problem is going to be a continuous function of these parameters why and

1305
01:25:29,630 --> 01:25:31,100
so now

1306
01:25:31,110 --> 01:25:35,920
let me get what they decide that you're using in the flickr dataset

1307
01:25:35,950 --> 01:25:39,870
flickr photo sharing website

1308
01:25:39,880 --> 01:25:42,620
this is the shell of the first

1309
01:25:42,640 --> 01:25:47,510
they have the data the using with for sixteen month period

1310
01:25:47,550 --> 01:25:50,880
and the number of users who by it

1311
01:25:50,900 --> 01:25:53,290
show you a graph

1312
01:25:53,290 --> 01:25:58,550
during this time period eventually the number of users was something like a hundred thousand

1313
01:25:58,560 --> 01:26:04,320
and about three hundred and forty thousand of these users have used the tagging feature

1314
01:26:04,320 --> 01:26:06,710
that this is going to focus on

1315
01:26:06,770 --> 01:26:11,870
and there's also a social network people can specify contacts in this

1316
01:26:11,880 --> 01:26:15,520
and the number of edges that you get the more accurate and then you get

1317
01:26:15,750 --> 01:26:18,470
something like two point eight million

1318
01:26:18,520 --> 01:26:23,930
twenty eight point five percent of them are not neutral so that a twenty eight

1319
01:26:23,930 --> 01:26:29,930
point five percent of your friends people that you may not friends have not named

1320
01:26:30,000 --> 01:26:32,380
and the size of the giant component

1321
01:26:32,410 --> 01:26:35,640
in this graph is something like a hundred and sixty k

1322
01:26:35,660 --> 01:26:43,370
so this is like being captured by profile they carry enough pictures here and

1323
01:26:43,410 --> 01:26:46,150
you can have all the stream and on each photo

1324
01:26:46,210 --> 01:26:50,760
you can that people can comment on it and also you have

1325
01:26:51,080 --> 01:26:52,760
feel that describe the

1326
01:26:53,660 --> 01:26:55,260
and usually you

1327
01:26:55,260 --> 01:26:59,250
specify these stacks other people can tag your photos as well but almost all of

1328
01:26:59,290 --> 01:27:05,120
the tags to the user specified by the person was also the form

1329
01:27:05,180 --> 01:27:06,350
and also

1330
01:27:06,350 --> 01:27:08,480
so like if you look at the prose

1331
01:27:08,540 --> 01:27:15,460
you know more complex have seventy five

1332
01:27:15,520 --> 01:27:20,510
and this is a graph showing the number of growth in the number of users

1333
01:27:20,510 --> 01:27:23,330
or the time period during the

1334
01:27:23,340 --> 01:27:27,100
and as you see the to very actually this is the number of users have

1335
01:27:27,100 --> 01:27:33,300
been using the tax school forty dots

1336
01:27:35,290 --> 01:27:39,210
to keep up with the tradition of showing paul offit for the in degree and

1337
01:27:39,210 --> 01:27:44,430
out degree this is thing out of the graph this is how you would expect

1338
01:27:44,430 --> 01:27:46,960
from an intersection

1339
01:27:48,710 --> 01:27:52,670
i mean if you think about the tags there are about ten thousand tags that

1340
01:27:52,750 --> 01:27:57,690
are used to describe photos in the dataset had and we focused on the side

1341
01:27:57,690 --> 01:28:00,070
of a hundred

1342
01:28:00,180 --> 01:28:06,610
seventeen hundred pounds or in some experiments even less than that surround is these attacks

1343
01:28:06,610 --> 01:28:09,980
that are used more often and there are different types of attacks there are some

1344
01:28:09,980 --> 01:28:15,730
of the members like keywords like halloween or katrina hurricane katrina

1345
01:28:15,760 --> 01:28:18,640
there are some that goal

1346
01:28:18,670 --> 01:28:22,320
and more small like landscape or black and white

1347
01:28:22,360 --> 01:28:25,320
and there are some that are periodic more

1348
01:28:25,370 --> 01:28:28,130
you see you get peaks at the time of the

1349
01:28:30,650 --> 01:28:34,880
and for each had essentially defining in action

1350
01:28:34,920 --> 01:28:38,810
that corresponds to using the tag for the first time noticed that we don't care

1351
01:28:38,810 --> 01:28:44,280
about their future plans to using the tag because the second using the tag is

1352
01:28:44,280 --> 01:28:50,210
probably not about social influence you want use the want that

1353
01:28:50,280 --> 01:28:55,140
so the act response you're using it

1354
01:28:56,360 --> 01:29:01,310
again back to this discussion of the maximum likelihood problem coefficient of

1355
01:29:01,340 --> 01:29:06,310
the run this maximum likelihood problem on a number of

1356
01:29:06,310 --> 01:29:12,290
Ruth Halaban, who was a  pioneering AI researcher, sequencing things by hand, figured out that

1357
01:29:12,290 --> 01:29:18,070
C kit was a mutation that drove some small number of melanomas she did that work at

1358
01:29:19,560 --> 01:29:21,550
so that was nineteen ninety one

1359
01:29:21,630 --> 01:29:26,770
in nineteen ninety eight, excuse me,

1360
01:29:26,770 --> 01:29:32,460
a group run run run by Brian Druker

1361
01:29:32,470 --> 01:29:37,050
created a drug at the time it was called SDI four seventy one,

1362
01:29:37,050 --> 01:29:37,910
I believe,

1363
01:29:37,930 --> 01:29:41,040
a drug  that was later to become gleevec

1364
01:29:41,050 --> 01:29:44,400
was designed to

1365
01:29:44,820 --> 01:29:48,540
address a particular different mutation that was called it was

1366
01:29:48,540 --> 01:29:52,710
a fusion gene that happened when two pieces of a chromosome broke off and came together

1367
01:29:52,710 --> 01:29:57,460
and created an oncogene, a gene that causes cancer. and this drug

1368
01:29:58,050 --> 01:30:02,590
was very effective against that  fusion gene so there's nothing Ruth Halaban could do in

1369
01:30:02,590 --> 01:30:05,650
nineteen ninety one, there was no drug, there was nothing anyone could do in nineteen ninety

1370
01:30:05,650 --> 01:30:09,240
eight, there was a drug for this fusion gene called BCR ABL.

1371
01:30:09,290 --> 01:30:10,580
but then

1372
01:30:11,470 --> 01:30:13,280
two thousand

1373
01:30:13,290 --> 01:30:15,000
or thereabouts Druker

1374
01:30:15,010 --> 01:30:21,390
the same group working in a different cancer gastrointestinal stromal tumors discovered that

1375
01:30:21,440 --> 01:30:24,760
this drug for some reason happened to also bind to

1376
01:30:24,780 --> 01:30:26,930
and inhibits C kit.

1377
01:30:26,940 --> 01:30:30,930
well, there were still some dots to be connected because it wasn't till three years

1378
01:30:30,930 --> 01:30:37,550
later that a group in italy, inspired by a letter to the editor in an obscure italian

1379
01:30:37,550 --> 01:30:42,380
journal that no one had ever seen that said, you know, maybe we should try

1380
01:30:42,600 --> 01:30:48,660
gleevec on the melanomas that express C kit, he connected the few dots in his head and so that led to

1381
01:30:48,660 --> 01:30:53,010
the work that Stephen Hodi finally published in two thousand and eight at asco. so nearly

1382
01:30:53,010 --> 01:30:57,470
two decades went by to connect three dots

1383
01:30:57,480 --> 01:31:02,010
now this is something, if you're serendipitous enough you overhear something in that cocktail lounge or something

1384
01:31:02,010 --> 01:31:06,750
conference, science works this way but it is unacceptable for it to work

1385
01:31:06,750 --> 01:31:11,320
this way in cancer, there are too many lives at stake and it's easy enough

1386
01:31:11,320 --> 01:31:15,280
to connect three dots but this is the size of the knowledge base that we're

1387
01:31:15,280 --> 01:31:19,830
dealing with, these happen to be some of the databases that people involved in the

1388
01:31:19,830 --> 01:31:25,370
semantic web project are trying to connect up the ones and magenta here are the ones

1389
01:31:25,370 --> 01:31:28,500
specifically relevant to this project

1390
01:31:28,520 --> 01:31:29,600
that I'm trying to do.

1391
01:31:29,720 --> 01:31:34,370
and as I said just one of them, pubmed has twenty million abstracts in it

1392
01:31:34,380 --> 01:31:37,960
so we need some AI here.

1393
01:31:38,030 --> 01:31:42,960
it's not that hard to do some to to capture some low-hanging fruit here, back in

1394
01:31:42,960 --> 01:31:48,540
two thousand and three Alan Rappaport of who  did a  company called

1395
01:31:48,550 --> 01:31:53,040
they're on data which some of you may remember from earlier days in AI, was

1396
01:31:53,040 --> 01:31:57,360
a student and I had a different company called medstory,

1397
01:31:57,380 --> 01:32:00,350
put together a simple application for asco

1398
01:32:00,370 --> 01:32:05,050
in which we got people submitting abstracts in breast cancer to fill out a form

1399
01:32:05,050 --> 01:32:10,120
something like this, which is akin to the reference model what subtype of breast cancer

1400
01:32:10,120 --> 01:32:14,660
were  there and there were only three subtypes back then,  it wasn't very complicated but based on the

1401
01:32:14,660 --> 01:32:19,990
author filling this out then when a when a researcher or a doctor came in and said what trials are

1402
01:32:19,990 --> 01:32:24,260
relevant to my patient instead of having to  thumb through four thousand pages of bible

1403
01:32:24,260 --> 01:32:29,220
paper, you know one page per four thousand ab each of four  thousand abstracts, they could

1404
01:32:29,230 --> 01:32:32,800
look it up and this was we actually ran a clinical trial on whether people

1405
01:32:32,800 --> 01:32:37,140
would use this and ninety five percent of the authors voluntarily filled this out when

1406
01:32:37,140 --> 01:32:41,440
they were submitting the application and seventy percent of people

1407
01:32:41,450 --> 01:32:45,280
looking up trials used it because they felt it would be helpful. this was a  big success

1408
01:32:45,290 --> 01:32:48,720
and I I'm basically building on this now

1409
01:32:48,720 --> 01:32:53,860
for as the human interface, the user interface for cancer commons. so if you can

1410
01:32:53,860 --> 01:32:57,430
imagine just adding some buttons to the side of this where you fill out whatever

1411
01:32:57,430 --> 01:33:00,780
it is about your patient and then click a button and be able to find

1412
01:33:00,780 --> 01:33:04,440
out what the treatment guidelines are and  where in that  big decision tree

1413
01:33:04,440 --> 01:33:09,830
square root of two x one time six two in x two square

1414
01:33:09,940 --> 01:33:14,880
these are three dimensional vector and you see that if you take the inner product

1415
01:33:14,880 --> 01:33:18,580
between five UN five v

1416
01:33:18,600 --> 01:33:21,130
this is just the inner product of these

1417
01:33:21,130 --> 01:33:25,150
three dimensional vector with these three dimensional vector

1418
01:33:25,420 --> 01:33:31,400
then you see that it's equal to OK it's maybe a bit quick but

1419
01:33:31,420 --> 01:33:36,270
if you do the math you compute everything you'll see that it is just

1420
01:33:36,290 --> 01:33:40,750
u one v one plus u two v two to the square squared

1421
01:33:40,770 --> 01:33:42,440
which is exactly

1422
01:33:42,770 --> 01:33:46,310
the inner product between u and v square

1423
01:33:46,350 --> 01:33:52,290
and is that the kernel that you you started with

1424
01:33:52,310 --> 01:33:58,380
meaning that you can you can see it like as as if i'm trying to

1425
01:33:58,380 --> 01:34:00,420
say that if you use this curve

1426
01:34:00,480 --> 01:34:07,730
then it's precisely as if you were trying to use

1427
01:34:07,750 --> 01:34:09,810
in formation of degree two

1428
01:34:09,850 --> 01:34:13,900
our if if you wanted to to use

1429
01:34:13,900 --> 01:34:17,730
quadratic information about your coordinates

1430
01:34:18,690 --> 01:34:22,600
it's very easy for instance if you use this kernel

1431
01:34:22,650 --> 01:34:32,790
to derive we've to derive using it for a linear algorithm to derive

1432
01:34:32,810 --> 01:34:35,980
this user face that are like ellipses

1433
01:34:35,980 --> 01:34:38,330
high probability high power

1434
01:34:38,350 --> 01:34:39,460
i possible

1435
01:34:39,500 --> 01:34:40,920
if it's

1436
01:34:40,960 --> 01:34:42,460
actually work

1437
01:34:42,460 --> 01:34:48,170
and and and things like that

1438
01:34:48,210 --> 01:34:54,920
and now we're back we're the very simple kernelized classifier

1439
01:34:55,920 --> 01:35:01,750
so the one with the average plus points in the average negative points

1440
01:35:03,600 --> 01:35:09,290
again we have the value of our far we have the value of b

1441
01:35:10,730 --> 01:35:12,210
i use

1442
01:35:12,270 --> 01:35:13,670
garson kernel

1443
01:35:14,310 --> 01:35:19,420
when you use gaussian curve you have these parameter sigma that you have to deal

1444
01:35:21,170 --> 01:35:25,690
i'm not going to tell you how to deal with this guy because it's

1445
01:35:25,730 --> 01:35:27,620
it's a pain actually

1446
01:35:27,630 --> 01:35:33,150
but i'm going to show you what kind of decision surface you can have using

1447
01:35:33,690 --> 01:35:38,250
an our RBF kernel is very simple classifier that i i showed you

1448
01:35:38,310 --> 01:35:42,020
on linear classification problems

1449
01:35:42,080 --> 01:35:45,630
so here is the training data

1450
01:35:47,360 --> 01:35:50,420
the blue points that are positive point

1451
01:35:50,420 --> 01:35:55,850
the red one there are negative points and as you can see

1452
01:35:55,900 --> 01:35:57,020
there is no

1453
01:35:57,020 --> 01:36:01,920
no lying no hyperplane that could could separate

1454
01:36:01,960 --> 01:36:05,210
blue points from breakpoints

1455
01:36:07,650 --> 01:36:11,880
i use very simple classifier that i showed you we've

1456
01:36:11,900 --> 01:36:18,670
v v gulshan with garson kernel of different with the in the parameters is called

1457
01:36:18,690 --> 01:36:22,920
the width of the of the kernel then i using missing equal to zero points

1458
01:36:22,920 --> 01:36:25,940
are five zero point one zero point two

1459
01:36:25,960 --> 01:36:30,440
and four here is zero point zero five here is the point

1460
01:36:31,270 --> 01:36:32,730
here's point two

1461
01:36:32,750 --> 01:36:39,020
why it's those pictures show

1462
01:36:39,020 --> 01:36:41,380
it is the value of h

1463
01:36:41,380 --> 01:36:44,190
given the location where it's computed

1464
01:36:44,330 --> 01:36:47,670
the only thing that you have to remember to see that

1465
01:36:47,690 --> 01:36:49,630
the red

1466
01:36:49,650 --> 01:36:51,020
corresponds to the blue

1467
01:36:52,770 --> 01:36:55,040
here the red here

1468
01:36:55,060 --> 01:36:57,710
it is like the rule here

1469
01:36:57,710 --> 01:37:00,580
and the yellow here

1470
01:37:00,580 --> 01:37:04,770
or even the blue here is like the right here

1471
01:37:07,080 --> 01:37:10,520
nice choice

1472
01:37:10,690 --> 01:37:13,670
i didn't realize that book it's OK

1473
01:37:13,710 --> 01:37:18,420
so the only thing that you have to remember is that i use a very

1474
01:37:18,420 --> 01:37:20,400
simple algorithm

1475
01:37:20,420 --> 01:37:28,020
where i can analytically compute the advice i can out analytically compute b

1476
01:37:28,080 --> 01:37:30,040
be the value of b

1477
01:37:30,130 --> 01:37:35,400
and i just place garson kernel into this very simple algorithm

1478
01:37:35,460 --> 01:37:41,230
i run it with these nonlinearly separable training data and i get i get

1479
01:37:41,250 --> 01:37:47,150
those non-linear decision surface because you can imagine that the

1480
01:37:48,440 --> 01:37:51,650
you can imagine i don't know if you can imagine but here

1481
01:37:51,670 --> 01:37:53,900
there is a

1482
01:37:53,940 --> 01:37:56,860
the war between the red and the yellow

1483
01:37:56,920 --> 01:38:00,190
these are the the location where you

1484
01:38:00,250 --> 01:38:01,690
you're going to say

1485
01:38:01,710 --> 01:38:05,040
everything that is in this part is going to be

1486
01:38:05,060 --> 01:38:08,580
positive and everything that is going to take that is in this part is going

1487
01:38:08,580 --> 01:38:09,960
to be negative

1488
01:38:12,710 --> 01:38:17,670
we use the very simple linear algorithm we used do we just picked

1489
01:38:17,730 --> 01:38:21,790
a kernel that is well known we would be the gas kernels

1490
01:38:21,790 --> 01:38:22,790
we used it

1491
01:38:24,730 --> 01:38:26,690
we switched the colors

1492
01:38:26,710 --> 01:38:29,690
and we had these results

1493
01:38:34,170 --> 01:38:36,400
they should be very happy now

1494
01:38:36,400 --> 01:38:38,210
are you

1495
01:38:38,230 --> 01:38:41,210
yes nice

1496
01:38:50,310 --> 01:38:52,630
a small thing about kernels

1497
01:38:54,360 --> 01:39:00,400
as a glimpse of what is going to happen to you in in two hours

1498
01:39:00,480 --> 01:39:04,770
or never because i think that time a little behind on my schedule but

1499
01:39:04,790 --> 01:39:07,790
here's that the graph

1500
01:39:07,810 --> 01:39:13,000
the gram matrix of the kernel this is given an input set as

1501
01:39:13,000 --> 01:39:15,480
made of and data

1502
01:39:15,500 --> 01:39:20,100
x one to x and the grain the gram matrix s of

1503
01:39:20,790 --> 01:39:22,190
of of k

1504
01:39:22,210 --> 01:39:23,520
four s

1505
01:39:23,540 --> 01:39:28,310
is just the matrix made of all the

1506
01:39:28,360 --> 01:39:33,060
the dot product between x i x j a g

1507
01:39:33,080 --> 01:39:33,880
so this is

1508
01:39:33,880 --> 01:39:37,960
i obviously symmetric matrix because k of x

1509
01:39:38,270 --> 01:39:45,380
i xj is equal to k affects excite and these is a very nice property

1510
01:39:47,960 --> 01:39:53,210
of positive definite kernels is that

1511
01:39:53,270 --> 01:39:59,600
what ever the the training set or a set s that you choose you always

1512
01:39:59,600 --> 01:40:03,040
have this property that

1513
01:40:03,080 --> 01:40:05,480
that is this property here

1514
01:40:06,520 --> 01:40:07,460
you take

1515
01:40:07,460 --> 01:40:08,750
and he said

1516
01:40:08,750 --> 01:40:13,710
as of x one to x and you compute the green got the gram matrix

1517
01:40:14,330 --> 01:40:16,850
OK with respect to s

1518
01:40:16,860 --> 01:40:19,480
you'll see that

1519
01:40:19,500 --> 01:40:21,980
you will have this property

1520
01:40:22,020 --> 01:40:23,560
in other words

1521
01:40:23,580 --> 01:40:25,040
i can values

1522
01:40:25,730 --> 01:40:32,500
the gram matrix are all nonnegative

1523
01:40:32,500 --> 01:40:36,280
as these one and

1524
01:40:36,290 --> 01:40:39,720
i guess i'm repeating myself before they tend to be a little bit more process

1525
01:40:39,720 --> 01:40:41,180
oriented so

1526
01:40:41,210 --> 01:40:44,350
you either have functional programming

1527
01:40:44,370 --> 01:40:46,040
kind of style

1528
01:40:46,100 --> 01:40:48,050
or they have the

1529
01:40:48,060 --> 01:40:53,430
style you have prove so you have derivations and then you have

1530
01:40:53,450 --> 01:40:58,040
distributions over there those derivations based on that

1531
01:40:58,870 --> 01:41:02,710
distribution so you can make inferences

1532
01:41:19,100 --> 01:41:23,090
ah good question so

1533
01:41:23,120 --> 01:41:27,520
one question was how much knowledge engineering essentially

1534
01:41:27,560 --> 01:41:30,040
do you have to do and how much

1535
01:41:30,050 --> 01:41:35,160
data cleaning do you have to do and so on and definitely i would

1536
01:41:35,180 --> 01:41:39,060
imagine for most of you you guys realise that when you do a machine learning

1537
01:41:39,060 --> 01:41:40,490
method that

1538
01:41:40,550 --> 01:41:45,560
ninety percent of its missile aging the data and getting it into the right format

1539
01:41:45,560 --> 01:41:47,670
and so on

1540
01:41:47,680 --> 01:41:50,450
interestingly though

1541
01:41:50,460 --> 01:41:54,840
i personally have found with appearance which is one that i've worked with the most

1542
01:41:57,680 --> 01:42:00,120
actually a lot less

1543
01:42:01,870 --> 01:42:09,050
something where you're building traditional statistical models so

1544
01:42:09,060 --> 01:42:11,640
for example one of the demands that we

1545
01:42:11,650 --> 01:42:15,480
i did a lot of work with was tuberculosis dataset

1546
01:42:15,480 --> 01:42:20,290
where the tuberculosis dataset describes the

1547
01:42:20,350 --> 01:42:25,650
tuberculosis patients the people that they have been in contact with and the produce

1548
01:42:25,660 --> 01:42:33,460
particular tuberculosis train they were infected with and we were working with doctors at stanford

1549
01:42:33,470 --> 01:42:37,220
medical center and they said that they had given the data to

1550
01:42:37,610 --> 01:42:41,680
traditional statisticians and you know after six months

1551
01:42:41,700 --> 01:42:45,020
the statisticians came back with a couple

1552
01:42:45,160 --> 01:42:47,400
statistically significant

1553
01:42:47,410 --> 01:42:50,950
differences between things versus we

1554
01:42:50,960 --> 01:42:53,370
take the PRM stuff and applied it

1555
01:42:54,290 --> 01:42:57,040
we're able and we to give them

1556
01:42:57,090 --> 01:42:59,080
first off the picture

1557
01:42:59,090 --> 01:43:01,620
you know what they depend on one

1558
01:43:02,670 --> 01:43:05,100
from that

1559
01:43:05,110 --> 01:43:06,790
so they were impressed

1560
01:43:06,800 --> 01:43:10,660
and please with that and then they could go in and they can look

1561
01:43:10,660 --> 01:43:18,430
at that model and basically focusing on particular dependencies and do inference is using those

1562
01:43:18,430 --> 01:43:21,520
and then i would say that you should then go

1563
01:43:22,360 --> 01:43:24,810
traditional statistical methods so

1564
01:43:24,830 --> 01:43:31,290
it helps you to the bottom of understanding of what depends on what what doesn't

1565
01:43:31,290 --> 01:43:33,870
depend on y

1566
01:43:34,000 --> 01:43:38,730
to understand things

1567
01:43:38,780 --> 01:43:42,290
so one questions knowledge engineering so

1568
01:43:42,350 --> 01:43:43,080
is it

1569
01:43:43,090 --> 01:43:45,540
therefore it is no silver bullet

1570
01:43:45,620 --> 01:43:47,120
one point

1571
01:43:47,140 --> 01:43:51,550
like initially we thought well if you have a database and has the relational schema

1572
01:43:51,560 --> 01:43:54,470
well you just use that relational scheme

1573
01:43:54,480 --> 01:43:56,180
people to sign some

1574
01:43:56,200 --> 01:44:00,660
really not very good relational schemas so it may well be the case that you

1575
01:44:00,660 --> 01:44:01,540
need to

1576
01:44:01,550 --> 01:44:03,710
do some

1577
01:44:03,720 --> 01:44:05,120
massaging with that

1578
01:44:05,140 --> 01:44:11,930
so the second question was scalability

1579
01:44:14,210 --> 01:44:16,830
i think there's

1580
01:44:16,850 --> 01:44:21,100
different places where the scalability

1581
01:44:21,140 --> 01:44:23,280
comes and so first off

1582
01:44:23,300 --> 01:44:27,310
this allows you to

1583
01:44:31,490 --> 01:44:33,200
really huge

1584
01:44:33,210 --> 01:44:35,420
models if you're not careful

1585
01:44:35,430 --> 01:44:37,210
and if you're doing inference

1586
01:44:37,220 --> 01:44:40,850
so the inference is one place where the scaling

1587
01:44:41,980 --> 01:44:45,510
be an issue

1588
01:44:45,530 --> 01:44:47,280
but for learning

1589
01:44:47,300 --> 01:44:49,600
learning actually

1590
01:44:49,650 --> 01:44:53,150
it's surprisingly

1591
01:44:53,940 --> 01:45:00,930
not as bad as i think most people think because the fact that you're getting

1592
01:45:00,930 --> 01:45:04,200
all this parameter sharing actually

1593
01:45:05,150 --> 01:45:06,620
help you

1594
01:45:10,720 --> 01:45:15,220
but that's kind of my experience my experience is

1595
01:45:15,240 --> 01:45:17,350
the the biggest

1596
01:45:22,440 --> 01:45:25,100
if you can get it to all fit in memory

1597
01:45:27,070 --> 01:45:30,560
you're OK if you're going to find out what fit in memory then there and

1598
01:45:31,320 --> 01:45:36,090
huge troubles so that that's

1599
01:45:36,180 --> 01:45:43,150
the biggest one but then there are certain kinds of steel stability issues as well

1600
01:45:47,480 --> 01:45:49,710
the third question was about

1601
01:45:49,750 --> 01:45:52,920
what places it's been applied in practice so

1602
01:45:52,930 --> 01:45:56,050
applied in practice so what we've done this

1603
01:45:56,090 --> 01:46:00,100
to regularize the status that was one of the more interesting ones we've done a

1604
01:46:00,110 --> 01:46:01,920
lot of things with

1605
01:46:06,430 --> 01:46:08,300
data as well

1606
01:46:09,480 --> 01:46:12,980
there's been a fair amount of work of applying this to

1607
01:46:12,990 --> 01:46:16,880
biological data in terms of the work that

1608
01:46:17,260 --> 01:46:20,350
near freeman and so on have done

1609
01:46:21,510 --> 01:46:23,180
but i think it's still

1610
01:46:23,190 --> 01:46:24,560
all right

1611
01:46:24,570 --> 01:46:26,210
very right very for

1612
01:46:29,770 --> 01:46:32,440
and that's fine up

1613
01:46:34,610 --> 01:46:36,840
OK so i do want to

1614
01:46:37,640 --> 01:46:41,200
something about

1615
01:46:41,240 --> 01:46:43,380
undirected approaches

1616
01:46:43,640 --> 01:46:46,090
i have used this definition

1617
01:46:46,130 --> 01:46:49,970
and i'm going to do a little bit

1618
01:46:49,980 --> 01:46:55,480
about frame based directed approaches and rule based on direct approach is the rule based

1619
01:46:55,480 --> 01:47:00,730
and direct approaches are the markov logic networks

1620
01:47:07,240 --> 01:47:09,060
first off

1621
01:47:09,120 --> 01:47:10,820
analogous to

1622
01:47:10,840 --> 01:47:13,350
the business

1623
01:47:13,390 --> 01:47:16,740
a little bit i'm going to say quickly a little bit about markov networks

1624
01:47:17,230 --> 01:47:20,150
how many people have

1625
01:47:20,200 --> 01:47:23,850
that was markov networks before

1626
01:47:23,850 --> 01:47:25,980
OK so there's a guest

1627
01:47:26,020 --> 01:47:31,700
lower percentage than the bayes nets but

1628
01:47:31,730 --> 01:47:33,650
the basic idea

1629
01:47:33,670 --> 01:47:35,850
is again it's

1630
01:47:38,490 --> 01:47:42,480
where the random the nodes represent random variables

1631
01:47:42,530 --> 01:47:45,460
and the edges represent

1632
01:47:49,330 --> 01:47:53,100
and here's a little example where i'm talking about the fame

1633
01:47:54,280 --> 01:47:58,350
four different authors

1634
01:48:00,670 --> 01:48:03,590
there there is not this encode search

1635
01:48:03,620 --> 01:48:05,580
conditional independence

1636
01:48:06,960 --> 01:48:10,460
these ones are ones that basically

1637
01:48:10,470 --> 01:48:12,710
if you look at some

1638
01:48:12,780 --> 01:48:15,610
set of nodes for example

1639
01:48:15,630 --> 01:48:17,630
a two and a three

1640
01:48:17,690 --> 01:48:20,180
if they separate

1641
01:48:20,220 --> 01:48:21,950
a one and a four

1642
01:48:22,740 --> 01:48:27,190
they're are conditionally independent

1643
01:48:31,680 --> 01:48:36,210
the parameterisation that goes along with a markov network

1644
01:48:38,730 --> 01:48:41,820
a set of factors the factors

1645
01:48:41,830 --> 01:48:48,050
are over subsets of the nodes in this case i have simple edge potential that

1646
01:48:48,050 --> 01:48:52,790
work in this area is the one in a thousand transactions are fraudulent

1647
01:48:53,480 --> 01:48:58,670
the delay in labelling talk about that and and often the classes mislabeled things label

1648
01:48:58,700 --> 01:49:02,440
fraud when not vice versa illustrate that

1649
01:49:02,460 --> 01:49:06,900
obviously the transaction times arrive according to some sort of stochastic process to arrive at

1650
01:49:06,900 --> 01:49:08,470
regular intervals

1651
01:49:08,480 --> 01:49:13,030
and this one is particularly important i think there's reactive population growth since the distributions

1652
01:49:13,030 --> 01:49:15,610
that we're dealing with change over the course of time

1653
01:49:15,630 --> 01:49:16,400
so well

1654
01:49:16,490 --> 01:49:19,230
illustrate all of those

1655
01:49:20,570 --> 01:49:24,380
here is an example of critical whenever you make credit card transactions

1656
01:49:24,430 --> 01:49:28,180
between seventy and eighty variables are recorded

1657
01:49:28,190 --> 01:49:30,270
every transaction you make

1658
01:49:30,290 --> 01:49:33,900
so the numbers the dimensionality is going up is not just how much is spent

1659
01:49:33,960 --> 01:49:35,610
when you spent

1660
01:49:35,630 --> 01:49:41,610
it's there's an identification number transaction type is an ATM cash withdrawal or a mobile

1661
01:49:41,610 --> 01:49:42,980
phone top-up or

1662
01:49:43,010 --> 01:49:45,340
credit card sale one

1663
01:49:45,400 --> 01:49:48,850
the date and time of transaction to the nearest second is the amount of the

1664
01:49:48,860 --> 01:49:56,250
currency local currency amount merchant category this is a categorical with around two thousand different

1665
01:49:56,250 --> 01:50:03,120
categories and saying what sort of merchant was the the supermarket was closed who bought

1666
01:50:03,130 --> 01:50:05,310
was in the betting shop whatever

1667
01:50:05,430 --> 01:50:08,700
tony the nature of the transaction

1668
01:50:08,750 --> 01:50:12,150
and there are a whole load of others

1669
01:50:12,220 --> 01:50:14,280
seventy to eighty

1670
01:50:14,300 --> 01:50:19,970
in each to each credit card transaction you make

1671
01:50:20,010 --> 01:50:24,210
that's just banking data in general now let's look at four days this is a

1672
01:50:24,210 --> 01:50:25,110
slide from

1673
01:50:25,210 --> 01:50:28,600
this you can stop it from the US patent office website

1674
01:50:28,650 --> 01:50:32,520
this is the pattern number if you're interested if you know anything about work in

1675
01:50:32,520 --> 01:50:34,120
this area you might well

1676
01:50:34,130 --> 01:50:35,110
i have an idea

1677
01:50:35,130 --> 01:50:39,670
what particular system it's an early version what particular system

1678
01:50:42,470 --> 01:50:45,110
and in the in the

1679
01:50:45,180 --> 01:50:49,170
in the pattern they listen these variables this doesn't look like too many but that's

1680
01:50:49,170 --> 01:50:52,550
what you can't see them but let's see what they are the first one says

1681
01:50:52,640 --> 01:50:56,030
customer usage pattern profiles representing time of day

1682
01:50:56,040 --> 01:50:58,080
and a week profiles and says

1683
01:50:58,110 --> 01:51:00,690
the the second expiration date for the credit card

1684
01:51:00,690 --> 01:51:05,540
the third one is a dollar amount spent in each ASIC

1685
01:51:05,880 --> 01:51:09,880
yes i see because american version of merchant category code set dollar amount spent in

1686
01:51:09,880 --> 01:51:11,950
each merchant category code

1687
01:51:11,960 --> 01:51:13,860
we have two thousand of those

1688
01:51:13,890 --> 01:51:15,430
during the current day

1689
01:51:15,470 --> 01:51:20,000
so potentially obviously this is sparse but potentially there are two thousand variables just for

1690
01:51:20,010 --> 01:51:21,820
the third one alone here

1691
01:51:21,840 --> 01:51:26,020
and it goes on like that was the fourth one is percentage dollars spent by

1692
01:51:26,020 --> 01:51:30,530
customer each ASIC much group during the current day and so on so although it

1693
01:51:30,530 --> 01:51:32,750
doesn't look as if there are many variables here

1694
01:51:32,800 --> 01:51:35,730
they blow up to high dimensional problems

1695
01:51:36,610 --> 01:51:39,240
as if that's not bad enough

1696
01:51:39,250 --> 01:51:45,130
this US pattern also says additional fraud related variables which may also be considered are

1697
01:51:45,130 --> 01:51:46,570
listed below

1698
01:51:46,590 --> 01:51:48,550
and then this these

1699
01:51:48,590 --> 01:51:49,810
and these

1700
01:51:49,820 --> 01:51:50,850
and these

1701
01:51:50,860 --> 01:51:55,880
so quite high dimensional problems

1702
01:51:55,890 --> 01:51:58,300
i'm gonna say about unbalanced classes

1703
01:51:58,310 --> 01:52:03,770
on monday a very clever gave almost exactly the same example is this

1704
01:52:03,780 --> 01:52:07,090
i'm going to very quickly repeated because i want to refer to it later so

1705
01:52:07,090 --> 01:52:10,400
just to remind you what it is

1706
01:52:10,420 --> 01:52:13,370
suppose we got detector which identifies

1707
01:52:13,390 --> 01:52:19,280
ninety nine hundred legitimate transactions correctly so it gets some right and also identifies ninety

1708
01:52:19,280 --> 01:52:23,920
nine hundred fraudulent transactions all the legitimate gets ninety nine percent right of the fraudulent

1709
01:52:23,930 --> 01:52:25,700
gets ninety nine percent right

1710
01:52:25,700 --> 01:52:28,050
superficially it looks as if that's pretty good

1711
01:52:28,080 --> 01:52:31,010
but as we all know from monday

1712
01:52:31,080 --> 01:52:34,120
we have a problem when the priors are very different than the class sizes are

1713
01:52:34,120 --> 01:52:37,530
very unbalanced and i think this differs from eric grelet ongoing

1714
01:52:37,540 --> 01:52:40,430
i'm going to take one in a thousand transactions

1715
01:52:40,480 --> 01:52:43,900
i was fortunate because as i've already said that's sort of ballpark figure what goes

1716
01:52:43,900 --> 01:52:44,770
on in this

1717
01:52:47,430 --> 01:52:51,060
so we've got this is the true class legitimate fraudulent

1718
01:52:51,080 --> 01:52:54,450
ninety nine percent are correctly predicted as legitimate

1719
01:52:54,500 --> 01:52:59,040
across fraudulent ninety nine percent correctly predicted this fraudulent

1720
01:52:59,060 --> 01:53:00,780
so it looks pretty good

1721
01:53:00,780 --> 01:53:05,040
and these are the actual numbers which are legitimate and which are fraud

1722
01:53:05,040 --> 01:53:07,300
OK now that's what these numbers through

1723
01:53:07,320 --> 01:53:11,540
ninety nine percent of these that's nine hundred ninety nine fall in that cell

1724
01:53:11,540 --> 01:53:15,310
nine point nineteen ninety nine point nine nine of legitimate are

1725
01:53:15,330 --> 01:53:20,740
predictors fraudulent only one percent similar sort of things here the other way round

1726
01:53:20,740 --> 01:53:24,670
but now let's look at the those which are predicted as fraudulent

1727
01:53:24,680 --> 01:53:28,610
nine point nine nine really legitimate point nine nine four

1728
01:53:28,630 --> 01:53:31,120
so we're only getting nine percent correct

1729
01:53:31,170 --> 01:53:33,760
of those we predict to be fraudulent

1730
01:53:33,780 --> 01:53:34,970
for every hundred

1731
01:53:34,970 --> 01:53:38,090
only nine really all

1732
01:53:39,320 --> 01:53:44,080
ninety one percent suspected frauds are in fact just now this matters in this context

1733
01:53:44,110 --> 01:53:45,470
a great deal because

1734
01:53:45,500 --> 01:53:49,490
operational decisions could be made you want to decide whether to stop the car stopped

1735
01:53:49,490 --> 01:53:51,820
the can drop the cart

1736
01:53:51,880 --> 01:53:56,000
and good customers must be irritated

1737
01:53:56,040 --> 01:53:57,450
if for every

1738
01:53:57,540 --> 01:54:00,880
ten customers you phone and say

1739
01:54:00,900 --> 01:54:05,000
i think this is fraud or every ten accounts you think before you start online

1740
01:54:05,120 --> 01:54:07,530
of those are going to be perfectly fits perfectly

1741
01:54:11,030 --> 01:54:15,280
things are more subtle as i illustrated with my sort of economic imperative social implications

1742
01:54:15,280 --> 01:54:17,060
stuff at the beginning

1743
01:54:17,080 --> 01:54:18,460
in general

1744
01:54:18,460 --> 01:54:21,610
customers quite pleased you you you ring them up and you say

1745
01:54:21,700 --> 01:54:24,290
did you really make this transaction

1746
01:54:25,100 --> 01:54:28,670
there are often quite pleased that you're not you can probably that's what it looks

1747
01:54:28,670 --> 01:54:31,520
like UK europe and really

1748
01:54:31,650 --> 01:54:35,040
it looks as if you can

1749
01:54:35,040 --> 01:54:36,270
two o point

1750
01:54:36,340 --> 01:54:38,560
they're not going to be too pleased if

1751
01:54:38,570 --> 01:54:42,190
you're there often getting phone calls from you you irritating them all the time so

1752
01:54:42,300 --> 01:54:45,810
did you make this transaction so there's a delicate balance here

1753
01:54:46,420 --> 01:54:48,480
are all sorts of other

1754
01:54:48,490 --> 01:54:54,580
factors influencing how these sorts of systems must operate in practice it's not just simple

1755
01:54:54,590 --> 01:54:56,230
mathematical statistics

1756
01:54:58,170 --> 01:55:00,230
and i think that sort of principle

1757
01:55:00,240 --> 01:55:04,400
there are a lot of other issues pervades all i'm going to be talking

1758
01:55:07,240 --> 01:55:12,740
the next thing i want to talk about was the delay in learning class labels

1759
01:55:12,800 --> 01:55:15,280
because it introduces subtle distortion in the data

1760
01:55:15,290 --> 01:55:20,780
if and when fraud alarm is relate is raised we think a transaction is suspicious

1761
01:55:20,790 --> 01:55:24,400
then you quickly find out the true clustering up the account holder did you make

1762
01:55:24,400 --> 01:55:25,610
this transaction

1763
01:55:25,620 --> 01:55:28,240
yes or no you find fraudulent or not

1764
01:55:28,240 --> 01:55:31,470
and that number of correlations that can model

1765
01:55:31,470 --> 01:55:34,130
is equal to k right so you say

1766
01:55:34,160 --> 01:55:36,110
there's a number of correlations of their

1767
01:55:36,110 --> 01:55:40,700
and and indeed this you're saying and noise reason i'm going to explicitly model

1768
01:55:40,720 --> 01:55:46,380
and separate the noise somehow

1769
01:55:51,450 --> 01:55:53,720
if i now had observed

1770
01:55:53,780 --> 01:55:58,160
a certain certain vector in my in my thousand dimensional space

1771
01:55:58,180 --> 01:55:59,360
if i want to

1772
01:55:59,380 --> 01:56:03,900
the projected onto my twenty dimensional space let's let's make twenty

1773
01:56:03,900 --> 01:56:06,390
this college to has to say

1774
01:56:06,450 --> 01:56:07,740
two so that we

1775
01:56:07,760 --> 01:56:11,530
because two is sort of the number we choose all three four using PCA to

1776
01:56:11,530 --> 01:56:15,180
look at our data right so you could also use this to actually look at

1777
01:56:15,180 --> 01:56:17,800
your data right now we know

1778
01:56:17,820 --> 01:56:19,990
everything about

1779
01:56:20,010 --> 01:56:22,780
bayesian mechanics

1780
01:56:22,800 --> 01:56:26,280
so we know that because we have a prior on x

1781
01:56:26,320 --> 01:56:29,050
right we also have

1782
01:56:29,070 --> 01:56:34,550
we also have likelihood right if we fix x if we say here's x then

1783
01:56:34,590 --> 01:56:38,130
then we know what the what the distribution of y is going to be conditioned

1784
01:56:38,130 --> 01:56:42,740
on the right and it's simply it is a distribution and not just not just

1785
01:56:42,740 --> 01:56:46,260
the number because we're adding noise right

1786
01:56:46,450 --> 01:56:49,990
so i have a prior have likelihood

1787
01:56:50,030 --> 01:56:52,660
i just want to crank and and i have a posterior right

1788
01:56:52,700 --> 01:56:55,760
so i can so i can for any

1789
01:56:55,820 --> 01:56:57,030
data points

1790
01:56:57,030 --> 01:57:02,760
in my thousand dimensions space i can find i can find the two dimensional representation

1791
01:57:02,760 --> 01:57:05,510
of it right so i can sort of project my

1792
01:57:05,510 --> 01:57:07,910
my vectors now

1793
01:57:07,950 --> 01:57:11,930
i get agustin distribution which is not a surprise because of multiply the some prior

1794
01:57:11,930 --> 01:57:14,510
with the agustin likelihood right

1795
01:57:14,510 --> 01:57:18,360
but let's look at the mean i i suppose it's more intuitive to look at

1796
01:57:18,360 --> 01:57:21,430
the mean right so if we just wanted to look at the means of of

1797
01:57:21,430 --> 01:57:26,200
the posteriors so think about it again for any possible data point in my data

1798
01:57:26,200 --> 01:57:30,220
space i get a little girl some posterior in my in my plane right so

1799
01:57:30,220 --> 01:57:32,860
we get all these little only is it goes in

1800
01:57:33,900 --> 01:57:35,900
if you will

1801
01:57:35,910 --> 01:57:39,860
so what are the means if i look at the other means well you see

1802
01:57:39,900 --> 01:57:43,300
you know this was linear model and

1803
01:57:43,360 --> 01:57:45,860
the design matrix was land

1804
01:57:45,910 --> 01:57:49,110
and then i had the noise term so for for you

1805
01:57:49,220 --> 01:57:52,240
every of you that has been working with animals this sort of

1806
01:57:52,240 --> 01:57:57,200
it sort of looks like the normal equations somehow read this sort of the projection

1807
01:57:57,200 --> 01:57:57,930
i would get

1808
01:57:57,990 --> 01:58:01,740
let's think a little bit about this about this projection term

1809
01:58:01,740 --> 01:58:02,720
so two

1810
01:58:02,740 --> 01:58:04,510
think about it

1811
01:58:05,530 --> 01:58:06,680
two things

1812
01:58:06,680 --> 01:58:08,010
what happens

1813
01:58:08,030 --> 01:58:12,550
if the noise starts getting large for specific dimension

1814
01:58:12,610 --> 01:58:16,070
OK so remember this is a

1815
01:58:16,130 --> 01:58:21,700
a thousand times a thousand diagonal matrix OK and it contains it contains one noise

1816
01:58:22,740 --> 01:58:26,470
for each of the of the dimensions in high dimensional space

1817
01:58:26,530 --> 01:58:31,340
alright so i'm not saying that noise has the same amplitude for all dimensions useful

1818
01:58:31,340 --> 01:58:34,570
thing here is that i'm a modelling noise

1819
01:58:34,610 --> 01:58:36,300
independently across

1820
01:58:36,300 --> 01:58:39,800
individually across each dimension OK so have

1821
01:58:39,820 --> 01:58:42,680
i have a thousand sort of noise variances that of learning

1822
01:58:42,700 --> 01:58:44,610
now you can sort of see

1823
01:58:44,660 --> 01:58:47,610
let's let's look at the the the size of the test so as so what

1824
01:58:47,610 --> 01:58:53,180
sites should be to be anyone

1825
01:58:56,450 --> 01:58:57,760
so bitter

1826
01:58:59,400 --> 01:59:05,470
what times what

1827
01:59:08,300 --> 01:59:13,030
none of it it has to have more because if you look about it it's

1828
01:59:13,320 --> 01:59:16,260
it is we need to get back the mean

1829
01:59:17,200 --> 01:59:21,260
of x right so what size it is x is just

1830
01:59:21,300 --> 01:59:22,740
take it slowly

1831
01:59:22,970 --> 01:59:26,640
so x is k times one right it's k times one vector and we've said

1832
01:59:26,640 --> 01:59:29,450
k could be too if i wanted to look at the data around

1833
01:59:30,410 --> 01:59:33,530
what sizes is why i

1834
01:59:33,550 --> 01:59:37,950
the times one right OK so now we know what size beta must be right

1835
01:59:38,010 --> 01:59:40,320
so taste k

1836
01:59:40,490 --> 01:59:41,640
times the

1837
01:59:41,660 --> 01:59:46,450
it's k times the projection matrix that brings me my dimensional data back into my

1838
01:59:46,450 --> 01:59:47,800
k dimensional space

1839
01:59:47,880 --> 01:59:50,760
OK fair enough for enough that's fine

1840
01:59:50,780 --> 01:59:55,280
now we can also guess what size is lambda actually we can guess what sizes

1841
01:59:55,280 --> 02:00:02,660
lambda because we know that there was this relation

1842
02:00:03,570 --> 02:00:04,900
so lambda

1843
02:00:04,930 --> 02:00:09,680
lambda better timescale right

1844
02:00:09,680 --> 02:00:11,430
which sort of makes sense

1845
02:00:12,070 --> 02:00:12,950
so now

1846
02:00:12,950 --> 02:00:14,840
let's try to be the test would beta

1847
02:00:14,860 --> 02:00:15,930
it is

1848
02:00:15,930 --> 02:00:17,700
a call

1849
02:00:17,780 --> 02:00:19,990
to this

1850
02:00:20,070 --> 02:00:26,610
now all lambda is transposed so it's k times this OK so short-sighted and these

1851
02:00:26,610 --> 02:00:28,280
along one

1852
02:00:28,510 --> 02:00:30,220
and this gets multiply

1853
02:00:32,840 --> 02:00:35,590
the square matrix here OK

1854
02:00:36,550 --> 02:00:37,800
so far so good

1855
02:00:38,640 --> 02:00:40,740
the simplest thing we can do is

1856
02:00:40,740 --> 02:00:42,910
pretend for a moment

1857
02:00:44,680 --> 02:00:46,820
the land that are orthogonal

1858
02:00:46,840 --> 02:00:49,090
if they orthogonal they don't need to be

1859
02:00:49,140 --> 02:00:53,030
the reason why i'm doing this is to help us think

1860
02:00:53,030 --> 02:00:55,280
let's just take them to be

1861
02:00:55,930 --> 02:00:59,910
the other thing i want now is i want to get diagonal matrix here because

1862
02:00:59,910 --> 02:01:03,610
if i have a diagonal matrix here what you what you see is that the

1863
02:01:03,610 --> 02:01:08,930
diagonal elements what they do is they scale the columns of of of this matrix

1864
02:01:08,930 --> 02:01:10,990
lambda right that's what they do

1865
02:01:10,990 --> 02:01:11,820
and so

1866
02:01:11,880 --> 02:01:14,970
what i want to give you is intuition of what happens even if a certain

1867
02:01:14,970 --> 02:01:19,140
noise term is very large so if i sort of discovered that

1868
02:01:19,160 --> 02:01:21,530
you know it's a site one like

1869
02:01:21,610 --> 02:01:25,780
the first dimension in my thousand dimensional space has a lot of noise OK

1870
02:01:25,800 --> 02:01:27,200
so what happens then

1871
02:01:29,090 --> 02:01:32,780
what happens if this term going very large is when i take the inverse

1872
02:01:32,820 --> 02:01:36,840
the one one element of the inverse is going to be very close to zero

1873
02:01:36,840 --> 02:01:40,240
reason to be very small so what happens then what happens in is one and

1874
02:01:40,240 --> 02:01:41,400
multiply these guy

1875
02:01:41,410 --> 02:01:45,610
this element is scaling this whole column right

1876
02:01:45,660 --> 02:01:47,240
and what does that mean

1877
02:01:47,340 --> 02:01:49,860
that means that when i be the projection

1878
02:01:49,880 --> 02:01:51,610
when i sort of take my

1879
02:01:51,630 --> 02:01:55,030
my my y data datapoint has a thousand dimensions

1880
02:01:55,110 --> 02:01:57,860
and i want to obtain my kahlil dimensions

1881
02:01:58,030 --> 02:02:01,510
there are going to be a linear combination of all the thousand dimensions and what

1882
02:02:01,590 --> 02:02:04,200
and what this is saying is that have not a lot of noise in one

1883
02:02:04,240 --> 02:02:06,970
of the dimensions i'm just going to switch their one off and not going to

1884
02:02:06,970 --> 02:02:08,260
listen to it too much

1885
02:02:09,090 --> 02:02:11,510
it's nice

1886
02:02:11,510 --> 02:02:13,360
all right

1887
02:02:13,570 --> 02:02:20,090
so that's

1888
02:02:20,110 --> 02:02:25,740
let's think about some other aspects of factor analysis another convenient thing in factor analysis

1889
02:02:25,740 --> 02:02:27,700
is that

1890
02:02:27,720 --> 02:02:28,610
the number

1891
02:02:28,610 --> 02:02:30,240
so thank you very much for

1892
02:02:30,310 --> 02:02:34,520
for me it's a great pleasure and

1893
02:02:35,940 --> 02:02:39,880
the named lecture is really cool

1894
02:02:39,930 --> 02:02:46,160
particularly one named after the greek heroes

1895
02:02:48,290 --> 02:02:50,090
and worried about the

1896
02:02:50,110 --> 02:02:54,580
timing of the election after all the food and alcohol

1897
02:02:54,750 --> 02:02:57,060
and not me

1898
02:02:57,140 --> 02:03:00,900
and i think the system but

1899
02:03:00,910 --> 02:03:06,840
you may know that alcohol has a biphasic effect on your whole level is increasing

1900
02:03:06,840 --> 02:03:08,640
you tend to be away

1901
02:03:08,660 --> 02:03:13,320
and at a certain point you start thinking it tends decline fall asleep

1902
02:03:13,340 --> 02:03:14,660
so i recommend

1903
02:03:21,200 --> 02:03:22,250
and gonna the

1904
02:03:22,640 --> 02:03:23,890
i'm talking about

1905
02:03:23,900 --> 02:03:26,790
applications of behavioral economics two

1906
02:03:26,830 --> 02:03:28,490
public policy

1907
02:03:28,570 --> 02:03:31,020
titles the economist as therapist

1908
02:03:31,040 --> 02:03:33,400
is that the title actually comes from

1909
02:03:33,410 --> 02:03:36,480
a paper by two princeton economist

1910
02:03:36,490 --> 02:03:39,450
nearest main goal and hasn't after

1911
02:03:40,130 --> 02:03:47,760
OK well they're going attack first against neuroeconomics

1912
02:03:48,880 --> 02:03:55,450
the current be pleased about that but i sort of think neuroeconomics has yet to

1913
02:03:55,470 --> 02:03:59,730
making any great discoveries but i think in the future we will and the other

1914
02:03:59,730 --> 02:04:01,230
thing that they attack

1915
02:04:01,250 --> 02:04:06,310
is what could be called the new paternalism in economics and i'm actually the reason

1916
02:04:06,310 --> 02:04:10,290
i didn't jump to the defence of neuroeconomics when you made that comment is really

1917
02:04:10,290 --> 02:04:13,470
much more excited about the second much more excited about

1918
02:04:13,480 --> 02:04:15,950
after applying behavioral economics

1919
02:04:17,100 --> 02:04:18,680
public policy

1920
02:04:18,700 --> 02:04:22,180
and that's when talk about

1921
02:04:22,320 --> 02:04:29,470
in the in the halls of course kind inquiry what happened to

1922
02:04:29,490 --> 02:04:30,610
and i can

1923
02:04:30,620 --> 02:04:35,020
but is think what i would say

1924
02:04:35,120 --> 02:04:39,500
when i started using powerpoint

1925
02:04:39,510 --> 02:04:43,640
i used to make mistakes pointing at the screen

1926
02:04:48,930 --> 02:04:51,360
thinking that i guess it

1927
02:04:52,250 --> 02:04:59,410
here i guess i'm phd on the lowest level of development the discrete just imagine

1928
02:04:59,420 --> 02:05:05,470
the are city as well

1929
02:05:05,490 --> 02:05:11,820
that's like in second slides kind of important so we work here

1930
02:05:11,880 --> 02:05:13,830
that's the first

1931
02:05:13,850 --> 02:05:17,260
this time

1932
02:05:17,280 --> 02:05:20,640
it's going to gradually is going to gradually come into focus

1933
02:05:23,170 --> 02:05:27,790
anyway when i was what i was walking in the halls of my university

1934
02:05:27,810 --> 02:05:30,000
university and became a poster

1935
02:05:30,020 --> 02:05:33,970
and this is the poster you might see

1936
02:05:34,020 --> 02:05:38,730
very you see what's behind this picture of the statue of liberty

1937
02:05:38,740 --> 02:05:43,840
and the reason you copied to come incompetent to know what's best for yourself

1938
02:05:44,910 --> 02:05:50,600
this is if you look at the small loads from the libertarian society that you

1939
02:05:51,370 --> 02:05:55,850
and so they seem to think that the answer is obvious answer the question is

1940
02:05:55,850 --> 02:06:00,540
obvious that is that we do know know what that what's best for themselves

1941
02:06:00,590 --> 02:06:04,140
well every time i see this poster i get plunged into

1942
02:06:04,150 --> 02:06:05,060
and questions

1943
02:06:05,170 --> 02:06:08,690
depression because the that isn't the answer i can

1944
02:06:08,710 --> 02:06:11,440
but you don't have to look at my behavior

1945
02:06:11,460 --> 02:06:14,820
if you look at the behaviour of americans and a lot of people in the

1946
02:06:15,570 --> 02:06:20,150
you see that there's a lot of people don't always know what's best for themselves

1947
02:06:20,160 --> 02:06:21,330
and even when they do

1948
02:06:21,370 --> 02:06:25,770
they often have trouble getting themselves to do in health behavior

1949
02:06:25,840 --> 02:06:32,020
between nineteen sixty and two thousand obesity in US adults increased from thirteen percent to

1950
02:06:32,050 --> 02:06:33,430
thirty one percent

1951
02:06:33,470 --> 02:06:36,500
and of course the the worldwide trend as well

1952
02:06:36,520 --> 02:06:38,550
in nineteen eighty five

1953
02:06:38,560 --> 02:06:41,460
it was not a single US state with obesity

1954
02:06:41,660 --> 02:06:46,590
right about fifty percent in two thousand five there's not a single state

1955
02:06:46,600 --> 02:06:51,150
and the country with obesity rate that's less than fifteen percent

1956
02:06:52,880 --> 02:06:57,000
a lot of if you look at diseases in the united states test

1957
02:06:57,110 --> 02:07:00,820
about a third of the deaths in the united states are caused by

1958
02:07:01,170 --> 02:07:06,450
like lifestyle diseases that are caused by behave behaviors that people do like tobacco alcohol

1959
02:07:08,160 --> 02:07:11,900
it's also the case that many of the medications

1960
02:07:12,290 --> 02:07:19,010
one of the most promising medications like medications to control blood pressure cholesterol avoid strokes

1961
02:07:19,180 --> 02:07:23,650
in fact many of the medications that are designed to deal with the lifestyle diseases

1962
02:07:23,670 --> 02:07:29,110
aren't really realizing their potential because people don't take them

1963
02:07:29,120 --> 02:07:30,140
and so

1964
02:07:30,160 --> 02:07:33,180
just one of many many striking examples

1965
02:07:33,200 --> 02:07:38,250
half of patients who have a heart attack stop taking cholesterol medication which is an

1966
02:07:38,250 --> 02:07:44,400
extremely effective remedy against having a second heart attack within the first year i think

1967
02:07:44,400 --> 02:07:47,050
it's pretty shocking is having a heart attack after all

1968
02:07:47,090 --> 02:07:48,410
this is very

1969
02:07:49,670 --> 02:07:56,320
and if you look at spending savings of course the the US is probably worse

1970
02:07:56,320 --> 02:08:00,280
than this but the average u s savings rate is minus one percent

1971
02:08:00,290 --> 02:08:04,110
we're just saving one percent of our world each year

1972
02:08:05,270 --> 02:08:10,190
and the media network in the year two thousand households was

1973
02:08:10,210 --> 02:08:12,010
thirteen thousand dollars

1974
02:08:12,670 --> 02:08:13,960
it's not very much

1975
02:08:13,980 --> 02:08:16,170
can you tell you that

1976
02:08:16,190 --> 02:08:19,270
so traditional economics

1977
02:08:19,280 --> 02:08:25,520
i think among friends here we all enjoy the expansion of traditional economics is not

1978
02:08:25,520 --> 02:08:27,990
well equipped to deal with these problems

1979
02:08:28,010 --> 02:08:32,530
because it assumes that people know what's best for themselves and their people to act

1980
02:08:32,530 --> 02:08:34,120
and understanding

1981
02:08:34,140 --> 02:08:38,610
as a result there is little or no need for intervention beyond the problem of

1982
02:08:38,610 --> 02:08:40,530
externalities that is the the

1983
02:08:40,540 --> 02:08:45,290
problems that people cars for other people but that there's no need to intervene to

1984
02:08:45,290 --> 02:08:46,360
help people

1985
02:08:46,380 --> 02:08:48,960
protect themselves against themselves

1986
02:08:48,980 --> 02:08:51,100
and so most of the policy

1987
02:08:51,110 --> 02:08:56,990
was the types of policies that were proposed by conventional economists focus on changing prices

1988
02:08:56,990 --> 02:09:01,070
or providing information to people

1989
02:09:01,080 --> 02:09:06,970
behavioral economics is very different big focus of behavioral economics is on the mistake people

1990
02:09:06,970 --> 02:09:12,050
make mistakes people make often we don't know what's best for themselves and even when

1991
02:09:12,050 --> 02:09:17,530
we don't know we often can do it and this this motivates interventions like paternalistic

1992
02:09:17,530 --> 02:09:20,420
interventions interventions designed

1993
02:09:20,430 --> 02:09:23,760
to help people protect people against themselves

1994
02:09:23,780 --> 02:09:25,320
very much like you might

1995
02:09:25,340 --> 02:09:27,570
help the child to

1996
02:09:27,580 --> 02:09:29,250
you might have

1997
02:09:29,260 --> 02:09:33,000
protect the child like try to get the child to eat healthy food something like

1998
02:09:35,250 --> 02:09:39,570
this insight that behavioral economics

1999
02:09:39,590 --> 02:09:43,190
mode can motivate new types into interventions

2000
02:09:43,200 --> 02:09:46,010
has inspired a new approach to public policy

2001
02:09:46,020 --> 02:09:50,970
which i reverted with the blanket term light paternalism

2002
02:09:50,990 --> 02:09:52,450
so the paternalism

2003
02:09:52,460 --> 02:09:56,250
we may not policies that are intended to benefit individuals

2004
02:09:56,270 --> 02:09:59,450
and they promised on the idea that people cannot be relied upon to pursue self

2005
02:10:00,940 --> 02:10:07,070
paternalism is distinct from regulations that again attempted to attempts to deal with externalities to

2006
02:10:07,240 --> 02:10:09,290
protect people from other people

