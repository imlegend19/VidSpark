1
00:00:00,000 --> 00:00:03,480
so this is this is pure covalency

2
00:00:04,000 --> 00:00:13,530
this is pure covalency this is polar covalent see what call a polar covalent because

3
00:00:13,530 --> 00:00:17,360
if you look at this molecule you say that well the electrons are a little

4
00:00:17,360 --> 00:00:22,880
bit closer to the right and the left so the charges and uniformly distributed its

5
00:00:22,880 --> 00:00:28,150
net neutral molecules that neutral but this right and there's a little bit more negative

6
00:00:28,260 --> 00:00:32,100
and the left hand is a little bit more positive and physics when fizzes was

7
00:00:32,110 --> 00:00:38,480
a little bit of citizens rights case delta that's physics talked for a little bit

8
00:00:38,650 --> 00:00:42,430
so this is a little bit negative this a little bit positive

9
00:00:42,450 --> 00:00:45,430
which means I could model this by

10
00:00:46,590 --> 00:00:48,750
a dipole

11
00:00:49,480 --> 00:00:50,830
this is a dipole

12
00:00:50,950 --> 00:00:56,410
it's got negative and the positive and and it has certain properties if we put

13
00:00:56,410 --> 00:01:00,740
a dipole that has the freedom to move we put the dipole in an electric

14
00:01:00,740 --> 00:01:06,690
field it will align itself with the field lines so because this is a dipole

15
00:01:06,690 --> 00:01:11,690
he chose the Pol part of dipole to give us polar covalent C and what's

16
00:01:11,690 --> 00:01:18,310
the ultimate what's the what's the extreme of of unequal sharing its its electron transfer

17
00:01:18,310 --> 00:01:22,430
so this is a perfect sharing unequal sharing and here

18
00:01:22,450 --> 00:01:27,290
the electrons are not actually donated so this becomes f minus this is the of

19
00:01:27,290 --> 00:01:31,620
an electron it's an a plus and so this 1 here I going say let's

20
00:01:31,620 --> 00:01:35,170
very than the longest 1 OK this is

21
00:01:35,190 --> 00:01:41,860
this is the ultimate in unequal sharing and this is ironic and this is ionic

22
00:01:43,740 --> 00:01:50,120
so you can see that polar covalent C is tendency towards ionic bonding and that's

23
00:01:50,120 --> 00:01:57,670
why pollen call this partial ionic character and universally quantified the quantified by this formula

24
00:01:58,050 --> 00:02:02,570
he said that the % ionic character and this is within a bond of a

25
00:02:02,570 --> 00:02:06,690
compound this is cent ionic character for

26
00:02:06,700 --> 00:02:08,120
a covalent bond

27
00:02:09,070 --> 00:02:15,630
for covalent bond he said that's going to equal one minus

28
00:02:15,650 --> 00:02:23,460
the exponential this is just basic natural logarithms e to the power of minus 1

29
00:02:23,460 --> 00:02:32,880
quarter times the difference in electronegativities squared so it's this thing again this universe electronegativity

30
00:02:33,190 --> 00:02:36,330
squaring multiplied by a quarter

31
00:02:36,350 --> 00:02:44,620
and raise that to the power of E subtract that from 1 multiplied by a

32
00:02:44,620 --> 00:02:48,670
hundred and you have something between 0 and 100 and

33
00:02:48,810 --> 00:02:51,700
I think there's some evidence of that here

34
00:02:52,090 --> 00:02:55,010
so this is right out of your periodic table if you look up in the

35
00:02:55,010 --> 00:02:59,370
corner your periodic table in 1 of the sides that they've actually tabulated this for

36
00:02:59,570 --> 00:03:05,850
so I've rewritten the formula here and these are the 2 different scales of ionic

37
00:03:05,850 --> 00:03:10,490
character 1 by piling actually there's more than 2 but in they show these 2

38
00:03:10,490 --> 00:03:14,410
there's polymers 1 Hannes Meyer and so here's pollen and

39
00:03:14,930 --> 00:03:17,120
we could we could do this for a

40
00:03:17,720 --> 00:03:24,240
or compound here HF for HF just for grins and chuckles let's do it the

41
00:03:24,360 --> 00:03:31,530
electronegativity of chlorine is 3 . 9 8 and electronegativity of H is 2 .

42
00:03:31,540 --> 00:03:37,380
2 all on this scale and so if you plug these values into this formula

43
00:03:37,380 --> 00:03:45,190
that will give you that the percentage of ionic character for the bond and see

44
00:03:45,190 --> 00:03:48,190
and I want to make sure that you don't think that this is the cent

45
00:03:48,190 --> 00:03:52,840
of character of the compounds on 1 of the really festivities here and indicate that

46
00:03:52,840 --> 00:03:58,600
I'm talking about the age of bond if you plug into this formula here you

47
00:03:58,600 --> 00:04:00,720
end up with a

48
00:04:00,740 --> 00:04:06,320
of about 50 60 % and that's already been tabulated up there in the normal

49
00:04:06,320 --> 00:04:11,950
graph see Figure that I think delta heroes of 1 . 5 7 8 which

50
00:04:11,950 --> 00:04:16,910
is roughly 1 . 0 in the using 56 now how does that just to

51
00:04:16,910 --> 00:04:22,170
give you a sense of what that means if the 264 figures pure covalency and

52
00:04:22,170 --> 00:04:29,000
the 344 as partial ionic character what fraction of the 608 344

53
00:04:29,060 --> 00:04:37,740
344 which is partial ionic character divided by 6 0 wait times 100 is 57

54
00:04:37,740 --> 00:04:42,170
% so it makes sense that makes sense

55
00:04:42,170 --> 00:04:47,100
that's good we can do all sorts of things the same site I indicated the

56
00:04:47,120 --> 00:04:53,320
molecule there's another of orthography that's used to indicate dipole you can use delta minus

57
00:04:53,320 --> 00:04:57,390
delta plus some people like to use an error and the arrow points in the

58
00:04:57,390 --> 00:05:00,890
direction of the more electronegative

59
00:05:01,110 --> 00:05:06,170
and when I remember that is little across here and I think that the pluses

60
00:05:06,170 --> 00:05:10,430
and you you can have your own mnemonic device to figure that out anyway so

61
00:05:10,430 --> 00:05:15,870
let's what's going to look at another 1 I think last professor Belanger did this

62
00:05:15,870 --> 00:05:21,910
only just hybridisation method right so there's nothing nothing looks like this with the SP

63
00:05:21,910 --> 00:05:27,150
three hybridization so let's let's look at that we see here we've got a dipole

64
00:05:27,150 --> 00:05:35,430
we only have 1 bond so the actual H. F. molecules HF molecule is polar

65
00:05:35,430 --> 00:05:39,130
HF molecule was polar hasn't died whereas H

66
00:05:39,830 --> 00:05:42,250
2 is non polar

67
00:05:42,390 --> 00:05:51,200
H 2 nonpolar how I know its charge distributions perfectly symmetric the electrons here are

68
00:05:51,200 --> 00:05:53,430
shared equally

69
00:05:53,450 --> 00:05:58,610
so that's non polar and likewise for fluorine F 2 so let's look at thing

70
00:05:59,200 --> 00:06:05,740
so methane we can draw it looks like this get carbon SP three hybridization right

71
00:06:05,760 --> 00:06:17,540
CH 4 years as 3 carbon hybridisation allows for monster form who has Sinaga hydrogens

72
00:06:17,540 --> 00:06:20,800
at the 4 corners of the tetrahedra

73
00:06:20,980 --> 00:06:24,540
so I wanna ask what's the nature the carbon hydrogen bonds

74
00:06:24,610 --> 00:06:29,170
was each carbon hydrogen bond and I look up and I see that the electronegativity

75
00:06:29,170 --> 00:06:33,700
of carbon is 2 . 5 5 and electronegativity of hydrogen and we already know

76
00:06:33,700 --> 00:06:39,780
is 2 . 2 0 so there's difference electronegativity since carbons electronegativity is higher than

77
00:06:39,780 --> 00:06:45,800
that of hydrogen which you'd expect from where carbon lies on the periodic table think

78
00:06:45,800 --> 00:06:51,060
about it and see if you know the periodic table then you know where relative

79
00:06:51,060 --> 00:06:56,060
elements where elements are relative to 1 another so at any moment you know in

80
00:06:56,060 --> 00:07:00,300
any bond which is the more electropositive

81
00:07:00,320 --> 00:07:03,910
electropositive is down to the left because that's where the metals

82
00:07:04,450 --> 00:07:09,090
so carbons right hydrogen this is more electronegative so let's look at this carbon hydrogen

83
00:07:09,090 --> 00:07:11,510
i mean was the forward

84
00:07:13,950 --> 00:07:19,520
the result will depend on the world but if we want to approximate

85
00:07:19,540 --> 00:07:21,330
the things

86
00:07:21,600 --> 00:07:23,730
o model

87
00:07:23,760 --> 00:07:25,430
you'll waste

88
00:07:25,470 --> 00:07:26,460
and that's it

89
00:07:26,460 --> 00:07:30,360
the importance of twenty one

90
00:07:30,380 --> 00:07:37,400
the assumption in the rich which is required to make the best of all is

91
00:07:37,400 --> 00:07:38,970
that you can compute

92
00:07:41,750 --> 00:07:42,960
you want

93
00:07:46,300 --> 00:07:48,440
in the simplest

94
00:07:48,550 --> 00:07:50,540
from this

95
00:07:50,560 --> 00:07:52,870
is the

96
00:07:52,940 --> 00:07:55,420
genetic sequence of points

97
00:07:55,430 --> 00:07:58,460
this is the way

98
00:07:58,510 --> 00:08:03,490
and sequence of approximate solutions to sort out point is generated

99
00:08:04,470 --> 00:08:06,230
they fought

100
00:08:06,240 --> 00:08:07,980
you can build this point this

101
00:08:08,000 --> 00:08:12,080
that might come about only by

102
00:08:12,130 --> 00:08:15,020
multiply by the

103
00:08:15,250 --> 00:08:17,340
and the problem of

104
00:08:17,360 --> 00:08:19,380
that intermediate points

105
00:08:19,430 --> 00:08:22,110
that in the same

106
00:08:22,310 --> 00:08:28,010
you take it do exactly the same but the

107
00:08:28,070 --> 00:08:34,290
the value of your data for five what the point is that in minus one

108
00:08:34,290 --> 00:08:37,290
of the

109
00:08:37,390 --> 00:08:43,410
OK but again you approximate the centre of this process like this in the u

110
00:08:43,450 --> 00:08:45,960
well you also

111
00:08:45,970 --> 00:08:50,070
oppose thomas is the best way

112
00:08:50,090 --> 00:08:53,410
OK that's it your search warrants

113
00:08:53,490 --> 00:08:55,650
well this is out that

114
00:08:55,880 --> 00:08:58,400
you've said that she's out

115
00:08:58,660 --> 00:09:02,520
some sort of

116
00:09:02,660 --> 00:09:06,510
and here is that all the results of the full

117
00:09:08,480 --> 00:09:09,940
assume that you're

118
00:09:09,990 --> 00:09:14,230
the gradient is called the continuous peninsula

119
00:09:16,520 --> 00:09:20,410
OK how do we measure the mass of in the distance between the

120
00:09:20,440 --> 00:09:22,960
the long

121
00:09:23,090 --> 00:09:25,690
and the distance between them

122
00:09:28,630 --> 00:09:30,480
because it was

123
00:09:34,070 --> 00:09:35,270
if you will

124
00:09:35,470 --> 00:09:41,520
sort of the wisdom of all of the with the problem is the last all

125
00:09:41,650 --> 00:09:42,300
i believe

126
00:09:42,370 --> 00:09:44,620
this is

127
00:09:44,790 --> 00:09:51,730
it is the maximum of during linear form the universe

128
00:09:51,740 --> 00:09:54,440
the normal operating

129
00:09:54,500 --> 00:09:57,920
it's where the nature of the phi actually world

130
00:09:57,970 --> 00:09:59,110
all right

131
00:09:59,120 --> 00:10:01,550
what is the evidence

132
00:10:01,970 --> 00:10:03,140
one of

133
00:10:03,140 --> 00:10:05,120
a of

134
00:10:05,390 --> 00:10:11,010
and that's what the explicit space policy differs according to the rate of convergence depends

135
00:10:11,010 --> 00:10:14,840
on boastful but only one

136
00:10:17,940 --> 00:10:20,820
this the need to be what you get

137
00:10:20,880 --> 00:10:24,010
most cases seem is the idea of the means

138
00:10:24,050 --> 00:10:25,740
say a the

139
00:10:26,380 --> 00:10:28,630
so what

140
00:10:28,650 --> 00:10:31,650
well known

141
00:10:31,760 --> 00:10:37,240
you're telling what you want to see what might

142
00:10:37,280 --> 00:10:42,130
what the problem of finding the right spot

143
00:10:42,800 --> 00:10:45,740
now as you may also want it means that your right

144
00:10:48,090 --> 00:10:53,260
OK and then you get this sort of components one two with both is one

145
00:10:53,360 --> 00:10:55,780
you get the convergence

146
00:10:55,860 --> 00:10:57,780
on the way the by biting

147
00:10:57,820 --> 00:11:00,260
this all forms that of the

148
00:11:00,360 --> 00:11:02,320
right course

149
00:11:02,340 --> 00:11:07,230
one is independent of the data and use it out that this is one of

150
00:11:07,260 --> 00:11:10,440
the first world war fall

151
00:11:12,050 --> 00:11:15,210
and you see how the world depends

152
00:11:20,670 --> 00:11:24,090
and also how it depends on the distance generate

153
00:11:24,170 --> 00:11:26,900
the goal was put out

154
00:11:26,920 --> 00:11:33,650
now can come up with just straight some of the best possible

155
00:11:33,730 --> 00:11:37,670
you know how to approximate this

156
00:11:37,780 --> 00:11:40,630
it was several situations where

157
00:11:40,760 --> 00:11:45,470
so what i did for the so-called untouchables what

158
00:11:45,470 --> 00:11:47,800
a point x is the same label

159
00:11:47,850 --> 00:11:49,820
it's good about

160
00:11:49,830 --> 00:11:51,340
seven gas

161
00:11:51,370 --> 00:11:55,530
and that's not the point of the opposite

162
00:11:55,570 --> 00:12:00,980
and this gap gamma right here defines wilderness survival given enough function

163
00:12:01,010 --> 00:12:03,400
this an example of a notion of

164
00:12:05,450 --> 00:12:09,660
so for instance said that the similarity between and two

165
00:12:09,700 --> 00:12:11,940
positive examples of police point two

166
00:12:11,950 --> 00:12:16,810
the similarity between and to negative that and examples still is o point on the

167
00:12:16,810 --> 00:12:21,780
dissimilarity between a positive example another example is the random number reminds one of

168
00:12:24,530 --> 00:12:27,850
on the left-hand side you need to point to the right hand side to be

169
00:12:28,690 --> 00:12:33,440
so this is satisfy notion flats along c and i don't want to just

170
00:12:33,510 --> 00:12:35,750
a simple example

171
00:12:35,770 --> 00:12:38,270
and actually if you think about it

172
00:12:38,300 --> 00:12:43,050
in space is large enough is known to get chances be similarity function will another

173
00:12:46,650 --> 00:12:49,590
so this looks like a very reasonable notion of what to ask

174
00:12:49,650 --> 00:12:51,140
but what about the function

175
00:12:51,150 --> 00:12:56,670
moreover if you had a similarity function satisfying these are very condition then also have

176
00:12:56,700 --> 00:12:57,510
a very

177
00:12:59,400 --> 00:13:01,230
efficient learning i

178
00:13:01,250 --> 00:13:04,800
so what all have to do in this case is to just throw a bunch

179
00:13:04,800 --> 00:13:10,310
of positive examples of of negative examples and then steamed out the classification rule

180
00:13:10,370 --> 00:13:13,390
that classifies an example x is positive

181
00:13:13,400 --> 00:13:14,310
on average

182
00:13:14,340 --> 00:13:16,470
most similar examples in this class

183
00:13:16,480 --> 00:13:20,330
this negative otherwise so this is not the definition is not good enough to get

184
00:13:20,330 --> 00:13:23,940
to what kind of functions are not just the simplest spot point

185
00:13:23,960 --> 00:13:25,170
OK so let me

186
00:13:25,180 --> 00:13:29,660
i'll come to the main notion beat

187
00:13:29,680 --> 00:13:32,150
i'm not claiming that we have this is

188
00:13:32,180 --> 00:13:36,580
not just kind of functions to see an example in sec

189
00:13:37,340 --> 00:13:41,150
so in that case so if we had a similarity function satisfying this condition

190
00:13:41,180 --> 00:13:45,260
there and we had to have

191
00:13:45,270 --> 00:13:47,900
we can use a simple and they start

192
00:13:48,050 --> 00:13:52,150
we draw a bunch of positive examples about negative examples and the classification rule that

193
00:13:52,150 --> 00:13:54,120
classifies an example x

194
00:13:54,130 --> 00:13:59,120
if it's an average more similar examples in this class the money and that he

195
00:13:59,120 --> 00:14:03,720
otherwise on the ground we get is that if set s plus in this minus

196
00:14:03,810 --> 00:14:08,280
has size at least one of them are square loss one the concepts of time

197
00:14:08,630 --> 00:14:15,560
then had privately someone that the classification with output has at most epsilon last at

198
00:14:15,560 --> 00:14:16,640
some time

199
00:14:16,660 --> 00:14:20,340
it's not easy to get an array to choose arbitrary close

200
00:14:20,350 --> 00:14:24,470
two the fraction of the examples it failed to satisfy this requirement

201
00:14:24,590 --> 00:14:30,010
that is more similar to examples of level that examples of it was OK this

202
00:14:30,010 --> 00:14:34,400
is the perfection is not very complicated and we go free

203
00:14:36,590 --> 00:14:41,970
it's actually very simple so first let's take a quick example at solar six an

204
00:14:41,970 --> 00:14:45,340
example that satisfies this condition here

205
00:14:45,400 --> 00:14:49,670
the band imagine something the census plus and minus

206
00:14:49,680 --> 00:14:52,590
then can show that the probability of error respect or even

207
00:14:52,820 --> 00:14:56,930
good example x is at most times sort of prime and this follows from the

208
00:14:56,930 --> 00:15:01,640
a simple application of standard station acquired like hoeffding inequality

209
00:15:01,710 --> 00:15:06,690
now since this is true for any given fixed it's a good example x but

210
00:15:06,690 --> 00:15:11,050
then is the expected error of our procedure the set of good examples that most

211
00:15:11,160 --> 00:15:12,980
of the times

212
00:15:13,030 --> 00:15:13,800
and so

213
00:15:13,810 --> 00:15:17,580
it means that the markov inequality together there is at most of the chance

214
00:15:17,600 --> 00:15:19,070
but the error rate

215
00:15:19,210 --> 00:15:22,700
over the set of with examples is greater than absolute right

216
00:15:22,760 --> 00:15:26,160
and then by adding back the fraction of examples

217
00:15:26,180 --> 00:15:30,260
failing to satisfy this inequality we get them to decide

218
00:15:30,300 --> 00:15:33,880
the error the classification of the output is at most epsilon

219
00:15:33,900 --> 00:15:34,920
that's it

220
00:15:34,930 --> 00:15:37,800
so if we had the same semantic funds signed this condition then you have to

221
00:15:37,800 --> 00:15:40,160
be able to learn that was

222
00:15:40,170 --> 00:15:44,670
however it turns out is you might expect that not all what kind of functions

223
00:15:44,670 --> 00:15:46,350
are going to satisfy this condition

224
00:15:47,310 --> 00:15:48,830
he would and

225
00:15:49,890 --> 00:15:54,180
to be hard to imagine what kind of functions satisfy this condition because otherwise it

226
00:15:54,180 --> 00:15:59,420
would be really interesting learning that was like is just use is trivial and does

227
00:15:59,420 --> 00:16:01,080
not one

228
00:16:01,150 --> 00:16:06,010
but here is the concrete examples showing that strange but not all what kind of

229
00:16:06,010 --> 00:16:08,800
functions are good similarity functions

230
00:16:08,800 --> 00:16:11,290
it is

231
00:16:12,170 --> 00:16:19,170
for this example imagine that the point without examples of point in the two dimensional

232
00:16:19,170 --> 00:16:20,380
euclidean space

233
00:16:20,450 --> 00:16:24,440
and that the similarity function but they can see that the kernel function to can

234
00:16:24,600 --> 00:16:28,070
that exist on the product is two-dimensional

235
00:16:28,080 --> 00:16:29,170
in space

236
00:16:29,180 --> 00:16:34,200
and imagine that there are distributions looks like this one here so the

237
00:16:34,340 --> 00:16:38,730
positive examples as being equally among upper left and upper right

238
00:16:38,760 --> 00:16:42,120
and the negative examples lie on here

239
00:16:43,090 --> 00:16:47,480
just for simplicity imagine that distribution is sixty feet so

240
00:16:47,500 --> 00:16:51,870
other examples have also been found examples and

241
00:16:51,910 --> 00:16:53,530
now if we

242
00:16:53,550 --> 00:16:57,290
consider an appropriate bank right here and particularly if to get an angle of thirty

243
00:16:58,830 --> 00:17:04,130
then clearly these kind of function similarity function can have a very large margins the

244
00:17:04,140 --> 00:17:06,640
margin going to be a constant going to be

245
00:17:10,220 --> 00:17:15,890
you can easily verify that this similarity function fails to satisfy this condition over here

246
00:17:15,920 --> 00:17:17,670
and it does so even before

247
00:17:17,910 --> 00:17:21,700
so long of course even for that long they one

248
00:17:24,480 --> 00:17:29,960
a large fraction of examples of the positive examples namely all the positive examples like

249
00:17:30,730 --> 00:17:32,940
however hard productive

250
00:17:33,410 --> 00:17:35,140
then the negative examples

251
00:17:35,150 --> 00:17:39,520
and they have on with the example particle the product of

252
00:17:39,540 --> 00:17:44,930
and the negative example is we can verify to half whether product or know

253
00:17:44,980 --> 00:17:48,540
a positive example of your half sample as a whole

254
00:17:48,590 --> 00:17:51,180
band hot water

255
00:17:51,200 --> 00:17:52,000
OK so

256
00:17:52,030 --> 00:17:54,080
this is an example where

257
00:17:54,110 --> 00:17:56,520
it was similarity function

258
00:17:56,640 --> 00:18:02,380
link function what kind of functions what kind of functions as the with large margin

259
00:18:02,780 --> 00:18:08,920
but if it fails to satisfy all symbols notion of was not much

260
00:18:09,970 --> 00:18:14,900
it is it is in this simple example right here we simply ignore all the

261
00:18:14,900 --> 00:18:17,510
positive examples here in the upper left

262
00:18:17,520 --> 00:18:21,150
where and competing expectations

263
00:18:21,160 --> 00:18:24,160
that's fine

264
00:18:24,180 --> 00:18:27,680
as of these that suggests trying to broaden the definition as follows

265
00:18:27,770 --> 00:18:32,910
we're going to say that all similarity function is what similarity function problem problem

266
00:18:32,920 --> 00:18:36,190
if exists a large enough set off

267
00:18:38,420 --> 00:18:39,880
reasonable points

268
00:18:39,900 --> 00:18:45,270
sergeant most of the examples acts on average more similar to reasonable points of overall

269
00:18:45,280 --> 00:18:48,770
labels then that resemble point of the opposite label

270
00:18:48,830 --> 00:18:50,920
OK now that you don't even need to

271
00:18:50,970 --> 00:18:53,280
no is set in advance

272
00:18:53,290 --> 00:18:54,780
we seem to have two

273
00:18:54,830 --> 00:18:56,680
i hope but it exists

274
00:18:56,740 --> 00:19:01,840
for services besides the our main definition does alter the protein have so

275
00:19:01,880 --> 00:19:02,820
for money

276
00:19:02,840 --> 00:19:05,080
we say that k

277
00:19:05,130 --> 00:19:10,200
assessment or similarity function is an absolute garment how

278
00:19:10,220 --> 00:19:15,390
good similarity function if there exists a set r of reasonable points

279
00:19:15,400 --> 00:19:17,900
so i most of examples x

280
00:19:17,910 --> 00:19:23,640
one is the fraction of examples x satisfy the property but the expected stimuli reasonable

281
00:19:23,640 --> 00:19:25,270
point of wrong labels

282
00:19:25,340 --> 00:19:27,680
is going to buy lease to certain get

283
00:19:27,680 --> 00:19:30,870
which if you think about it is a very strange thing right in your probability

284
00:19:30,870 --> 00:19:33,040
of smoking depends on the difference small

285
00:19:33,080 --> 00:19:34,990
you're probability of having the flu

286
00:19:35,020 --> 00:19:38,460
is not just a function of your symptoms it's also function of you know whether

287
00:19:38,590 --> 00:19:41,640
the flu epidemic going around and you have been in contact with somebody who has

288
00:19:41,650 --> 00:19:42,520
the full

289
00:19:42,540 --> 00:19:45,910
this is very easy to fix this is very difficult to express in this kind

290
00:19:45,920 --> 00:19:50,310
of models but very easy to express in markov logic as we just saw

291
00:19:51,060 --> 00:19:53,640
so this is sort of like the statistical site

292
00:19:53,660 --> 00:19:55,660
what about logical side

293
00:19:55,680 --> 00:19:58,890
well it's probably fairly intuitive there

294
00:19:58,910 --> 00:20:04,700
infinite weights give first order logic is a special case of markov logic civil at

295
00:20:04,700 --> 00:20:06,380
all no it's good infinity

296
00:20:06,390 --> 00:20:11,500
then you pay infinite penalty for violating formula and you retrieve back the first of

297
00:20:11,500 --> 00:20:15,630
the case and we have the theorem that says you can answer all entailment quiz

298
00:20:15,630 --> 00:20:20,520
in first order logic by computing conditional probabilities on the markov logic network that you

299
00:20:20,520 --> 00:20:24,420
get by taking the formulas and giving them infinite weights

300
00:20:24,470 --> 00:20:28,090
so this is nice though we first order logic is a special case but of

301
00:20:28,090 --> 00:20:31,110
course we really interested in the case where the weights are finite

302
00:20:31,130 --> 00:20:32,810
so what can we say there

303
00:20:32,950 --> 00:20:35,650
well if the knowledge base is satisfiable

304
00:20:35,670 --> 00:20:39,490
meaning it's possible to make all the formula is true at the same time

305
00:20:39,490 --> 00:20:41,500
and although it's are positive

306
00:20:41,510 --> 00:20:46,810
which you can always ensured by flipping by negating formulas and their weights

307
00:20:47,030 --> 00:20:51,110
the satisfying assignments are the most of the distribution

308
00:20:51,290 --> 00:20:55,570
which means that even in the finite noisy case

309
00:20:55,680 --> 00:21:01,100
the world that first-order logic likes this feeling that the modes of the distribution

310
00:21:01,120 --> 00:21:03,950
OK and we're gonna make very good use of this when it comes time to

311
00:21:03,950 --> 00:21:05,140
do inference

312
00:21:05,160 --> 00:21:09,530
but of course the most important thing about markov logic is that unlike in logic

313
00:21:09,530 --> 00:21:13,170
where if you have a contradiction in the in the knowledge base now pigs fly

314
00:21:13,170 --> 00:21:18,990
and anything can happen markov logic has no problem with contradictions between four

315
00:21:19,090 --> 00:21:22,970
if there is a contradiction markov logic just where the evidence on each side and

316
00:21:22,970 --> 00:21:25,110
produces the probability for the formula

317
00:21:25,140 --> 00:21:28,910
so now it's actually easy to build large knowledge bases because you have to look

318
00:21:28,920 --> 00:21:32,340
at least in this respect of course because you don't have to make sure that

319
00:21:32,340 --> 00:21:33,910
there no consistent anymore

320
00:21:33,930 --> 00:21:37,820
and more importantly it's easy to do things like the semantic web

321
00:21:37,820 --> 00:21:41,350
well you have contributions from many different people and there is no hope of ever

322
00:21:41,350 --> 00:21:44,860
making them more consistent but you still want to be able to reason with those

323
00:21:44,860 --> 00:21:51,790
contributions OK so markov logic it was at this level this this becomes pretty straightforward

324
00:21:51,920 --> 00:21:56,510
OK so let me talk a little bit about the inference and learning algorithms now

325
00:21:56,520 --> 00:21:57,390
of course

326
00:21:57,420 --> 00:22:01,840
this will be only of theoretical interest if we didn't have such efficient inference and

327
00:22:01,840 --> 00:22:03,380
learning algorithms enough

328
00:22:04,330 --> 00:22:07,400
you know it's a long way to go and make them more efficient but we've

329
00:22:07,400 --> 00:22:11,660
also come a very long way already and we can definitely handle a lot of

330
00:22:11,660 --> 00:22:15,540
rule real problems with what we have today so let's look at that a little

331
00:22:16,290 --> 00:22:18,610
let's start by looking at inference

332
00:22:18,630 --> 00:22:22,740
one of the most frequent and most useful types of inference that people want to

333
00:22:23,470 --> 00:22:28,310
is what's called MEP maximum upper styria or it or MP

334
00:22:28,330 --> 00:22:31,060
most probable explanation inference

335
00:22:31,100 --> 00:22:34,990
and this consists of a given some evidence

336
00:22:35,000 --> 00:22:36,800
what's called the next

337
00:22:36,820 --> 00:22:39,650
finding the most likely state of the world

338
00:22:39,680 --> 00:22:43,650
i e the most likely values of another set of variables y

339
00:22:44,290 --> 00:22:47,560
you can think of this as finding the most probable explanation for what you see

340
00:22:48,160 --> 00:22:52,420
here are the symptoms what is the most probable disease that explains the

341
00:22:53,490 --> 00:22:58,040
for those of you know for example by hmm this is what the viterbi algorithm

342
00:22:58,040 --> 00:22:59,010
is doing

343
00:22:59,100 --> 00:23:04,620
so what does this mean in markov logic well let's replace this p of y

344
00:23:04,620 --> 00:23:07,840
given x by the expression that we saw before right so we thought we want

345
00:23:07,840 --> 00:23:08,940
to find the white

346
00:23:08,960 --> 00:23:11,290
that maximizes this

347
00:23:11,510 --> 00:23:16,550
this is constant this is monotonically increasing function so to maximize this all have to

348
00:23:16,550 --> 00:23:19,420
maximize is what's inside here

349
00:23:21,350 --> 00:23:23,190
what you what are we doing here

350
00:23:23,210 --> 00:23:26,860
we were trying to find the assignment of values to the variables

351
00:23:26,860 --> 00:23:29,840
that maximizes the sum of the weights of the ground

352
00:23:29,840 --> 00:23:31,110
one was that you

353
00:23:31,850 --> 00:23:35,860
surprise surprise this is just the weighted maxsat problem

354
00:23:35,910 --> 00:23:40,820
right the satisfiability problem which is try to satisfy all formulas in a liquid essential

355
00:23:40,960 --> 00:23:45,870
NP complete problem the max that problem is trying to satisfy as many as possible

356
00:23:45,890 --> 00:23:49,310
the weighted next SAT problem is winning formula has the weight

357
00:23:49,310 --> 00:23:52,110
and you want to maximize the weight of satisfied formulas

358
00:23:52,460 --> 00:23:55,090
and the reason this is very nice us is that

359
00:23:55,100 --> 00:23:58,320
we don't have to invent anything new to do inference in markov logic we can

360
00:23:58,320 --> 00:24:01,240
just use an off-the-shelf which SAT

361
00:24:01,250 --> 00:24:03,360
like for example maxwalksat

362
00:24:03,380 --> 00:24:06,960
and we can take advantage of the last fifty years of intensive research on SAT

363
00:24:07,920 --> 00:24:12,060
but is sets of us can handle problems with millions of variables in minutes or

364
00:24:12,060 --> 00:24:16,240
seconds and are going to take advantage of that in all the future improvements of

365
00:24:16,240 --> 00:24:18,900
the continuing research on the subject

366
00:24:19,020 --> 00:24:20,350
and the number

367
00:24:20,360 --> 00:24:23,570
like very surprising and i think that falls out of this is that we start

368
00:24:23,580 --> 00:24:24,540
going in

369
00:24:25,460 --> 00:24:29,890
if you think about first order logic inference is not renowned for being fast

370
00:24:30,650 --> 00:24:34,130
and neither is probabilistic inference if you combine the two

371
00:24:34,140 --> 00:24:39,420
you probably have really be near the surprising thing is that it turns out that

372
00:24:39,420 --> 00:24:41,000
using this scheme

373
00:24:41,010 --> 00:24:44,420
we can actually do inference in markov logic faster

374
00:24:44,450 --> 00:24:46,380
then inference in first order logic

375
00:24:46,400 --> 00:24:49,580
and the reason i can do it faster is that as we will see shortly

376
00:24:49,580 --> 00:24:54,910
maxwalksat it's pretty much the exact same algorithm with very minor change as well you

377
00:24:54,910 --> 00:24:59,110
we is know process liability point one but owing to the

378
00:24:59,120 --> 00:25:02,500
if i did some of the constraints that were previously hard which should have been

379
00:25:03,710 --> 00:25:07,470
and make them soft and i have an easy problem to solve and therefore can

380
00:25:07,470 --> 00:25:08,860
solve it in a few

381
00:25:09,030 --> 00:25:10,180
fewer steps

382
00:25:10,260 --> 00:25:16,580
such systems are often when we make the first knowledge-based soft the inference on that

383
00:25:16,580 --> 00:25:19,850
we actually save time interest and in the process

384
00:25:19,890 --> 00:25:24,950
so for those of you not familiar with satisfiability solvers is is it actually is

385
00:25:24,950 --> 00:25:28,200
easy to explain something like walks in just one slide

386
00:25:28,210 --> 00:25:31,970
it's incredible how powerful it is given how simple it is so the goal of

387
00:25:32,100 --> 00:25:33,310
the walksat

388
00:25:33,340 --> 00:25:37,660
is to find an assignment of truth values to billion

389
00:25:37,680 --> 00:25:38,850
the labels

390
00:25:38,850 --> 00:25:41,710
it stays in this interval until t four

391
00:25:41,720 --> 00:25:43,980
and said OK

392
00:25:44,220 --> 00:25:48,710
let's say that the function is between two f one three at line

393
00:25:48,740 --> 00:25:54,960
over a region of size t two minus t one which is the size interval

394
00:25:54,970 --> 00:25:59,750
plus t four minus thirty three which is the size

395
00:25:59,770 --> 00:26:02,250
and instead of saying size

396
00:26:02,280 --> 00:26:06,720
he said size gets too confusing so i call measure instead

397
00:26:07,430 --> 00:26:11,900
and that was the beginning in measure theory essentially that other people talked about matter

398
00:26:11,900 --> 00:26:16,350
before the bag there are a lot of famous mathematicians who were all involved in

399
00:26:16,350 --> 00:26:17,470
doing this

400
00:26:18,320 --> 00:26:20,180
but but anyway

401
00:26:20,190 --> 00:26:22,400
so that's the basic idea

402
00:26:22,420 --> 00:26:26,850
what matters concerned with now for this curve here

403
00:26:26,900 --> 00:26:28,640
it's a nice smooth curve

404
00:26:28,670 --> 00:26:32,890
and you can almost see intuitively that you're going to get the same thing as

405
00:26:32,890 --> 00:26:34,480
looking at it this way

406
00:26:34,490 --> 00:26:36,830
looking at it this way

407
00:26:36,840 --> 00:26:38,370
and in fact you do

408
00:26:38,390 --> 00:26:45,180
and anyway what he finally wound up with this after saying what the matter was

409
00:26:45,180 --> 00:26:48,880
on each one of those little slices how much of the functional a in each

410
00:26:48,880 --> 00:26:52,130
one of those intervals would just add them all up

411
00:26:52,200 --> 00:26:56,490
he would add up how much of the function was any flies he would multiply

412
00:26:56,490 --> 00:26:58,850
how much the function within the slice

413
00:26:58,860 --> 00:27:01,870
but how high the place was up

414
00:27:01,960 --> 00:27:03,580
and then you get an answer

415
00:27:03,600 --> 00:27:05,990
one difference between what he did

416
00:27:06,000 --> 00:27:07,720
and what reminded

417
00:27:07,800 --> 00:27:10,800
was that he was going to lower bound and doing it this way if he

418
00:27:10,810 --> 00:27:13,700
was dealing with the non negative functions

419
00:27:13,710 --> 00:27:19,450
his approximation was always a little less than what the function was because anything that

420
00:27:19,450 --> 00:27:25,150
lay between two epsilon three excellent he would approximated this to work the line which

421
00:27:25,150 --> 00:27:26,400
is a little less

422
00:27:26,420 --> 00:27:31,440
then numbers between two epsilon three long so this is the lower bound

423
00:27:31,480 --> 00:27:37,360
whereas this is is whatever happens to be a however you decide to approximate the

424
00:27:37,360 --> 00:27:38,640
function here

425
00:27:38,670 --> 00:27:43,760
there are lots of ways of doing it

426
00:27:43,780 --> 00:27:48,650
OK i will prove any of these things i just want to point them out

427
00:27:48,670 --> 00:27:52,220
so that when you get frustrated with this

428
00:27:52,230 --> 00:27:54,760
you can always rely on this

429
00:27:54,940 --> 00:27:59,320
which says whenever they reminded the rule exists in other words whenever the integral if

430
00:27:59,320 --> 00:28:00,850
you're used to

431
00:28:02,580 --> 00:28:05,480
namely whenever it has meaning

432
00:28:05,490 --> 00:28:09,000
the integral gives you the same value

433
00:28:09,020 --> 00:28:12,100
OK in other words if you haven't lost anything

434
00:28:12,120 --> 00:28:16,850
by going from remind integration to the bank integration you can only gain you can't

435
00:28:17,940 --> 00:28:23,630
the familiar rules for calculating reminded of rules also apply for the bacon across

436
00:28:23,650 --> 00:28:27,690
OK remember what all those rules are you probably know all of them better

437
00:28:27,710 --> 00:28:32,350
better even fundamental definition of an integral which is split up the function at the

438
00:28:32,360 --> 00:28:34,000
time tiny little

439
00:28:34,080 --> 00:28:37,100
little increments because throughout

440
00:28:37,100 --> 00:28:42,930
all of the courses that you've taken learning about integration learning about differentiation learning about

441
00:28:42,930 --> 00:28:46,610
all of these things what you've done for the most part

442
00:28:46,670 --> 00:28:50,230
it's the go through exercises using these various rules

443
00:28:50,270 --> 00:28:55,130
so you know how how to integrate lots of lots of traditional functions you memorize

444
00:28:55,140 --> 00:28:57,350
what the role of many of them is

445
00:28:57,360 --> 00:29:00,850
you have many ways of combining them to find out what the in rule of

446
00:29:00,850 --> 00:29:02,620
many other functions is

447
00:29:02,660 --> 00:29:07,890
if you program a computer to calculate these integrals are computer can do it both

448
00:29:08,820 --> 00:29:12,030
you can either use all the rules to find out what the value of the

449
00:29:12,030 --> 00:29:17,340
integral is or can shop things that finally find out that way

450
00:29:17,380 --> 00:29:20,360
and as i said before

451
00:29:20,430 --> 00:29:21,820
if you

452
00:29:21,870 --> 00:29:27,620
if you think that being able to calculate integrals is what engineering is about think

453
00:29:27,620 --> 00:29:32,020
again i told you before you can be replaced by a digital computer

454
00:29:32,080 --> 00:29:34,360
it's worse than that

455
00:29:34,380 --> 00:29:40,370
you could be replaced by year by year handheld palm palm whatever it's called OK

456
00:29:40,370 --> 00:29:42,680
you can be replaced by anything

457
00:29:43,080 --> 00:29:48,250
after a while we're little things embedded in our body will tell us when we're

458
00:29:48,250 --> 00:29:53,270
second things like this you can be replaced by those things even

459
00:29:53,300 --> 00:29:54,830
OK so

460
00:29:55,210 --> 00:29:58,420
so you really want to learn more than just that

461
00:29:58,460 --> 00:30:03,860
OK for some very weird functions built by general exist but rely april doesn't exist

462
00:30:04,060 --> 00:30:08,550
why do we want to worry about weird functions

463
00:30:08,600 --> 00:30:12,550
i want to tell you two reasons for i told you about it last time

464
00:30:12,570 --> 00:30:14,600
one thing is

465
00:30:14,610 --> 00:30:18,000
awful lot of communication theory

466
00:30:18,070 --> 00:30:19,350
is concerned

467
00:30:19,360 --> 00:30:22,600
we're going back and forth between the time domain

468
00:30:22,620 --> 00:30:25,060
in the frequency domain

469
00:30:25,100 --> 00:30:28,320
when you talk about things which are

470
00:30:28,340 --> 00:30:31,350
straightforward in the time domain

471
00:30:31,400 --> 00:30:36,450
often they become very very weird in the frequency domain and vice versa

472
00:30:36,460 --> 00:30:39,520
i'm going to give you beautiful example of that

473
00:30:39,530 --> 00:30:44,510
probably the next probably class you can look at it

474
00:30:44,640 --> 00:30:47,940
it is in it is in in the appendix

475
00:30:48,140 --> 00:30:50,770
you see this absolutely weird functions

476
00:30:50,800 --> 00:30:53,630
which has the perfectly well-defined fourier transform

477
00:30:53,710 --> 00:30:59,600
and therefore the inverse fourier transform this nice looking thing is absolutely we're

478
00:30:59,940 --> 00:31:05,170
the other reason is even more important we have to deal with random processes we

479
00:31:05,170 --> 00:31:07,330
have to deal with noise functions

480
00:31:07,350 --> 00:31:10,950
which are continuously varying functions of time

481
00:31:10,970 --> 00:31:17,100
we have to deal with what we transmit which is continuously varying functions of time

482
00:31:17,130 --> 00:31:20,470
and just like when we were dealing with sources

483
00:31:20,480 --> 00:31:23,780
he said if we want to compress the source

484
00:31:23,810 --> 00:31:28,750
it's not enough to think about what one source sequences we have to think about

485
00:31:30,630 --> 00:31:32,430
namely we have to model

486
00:31:32,450 --> 00:31:33,820
these functions

487
00:31:33,830 --> 00:31:36,630
as as sample values

488
00:31:36,690 --> 00:31:37,760
what we call

489
00:31:37,780 --> 00:31:41,170
a stochastic process or random process

490
00:31:41,180 --> 00:31:48,710
originally start talking about that these functions which already look complicated just become sample points

491
00:31:48,740 --> 00:31:50,600
and this much bigger space

492
00:31:50,620 --> 00:31:53,120
we're dealing with random processes

493
00:31:53,140 --> 00:31:58,520
now when you're dealing with sample points of these random processes

494
00:31:58,550 --> 00:32:03,390
we are in this just crops up as a necessary part of all of this

495
00:32:03,400 --> 00:32:10,090
you can define a random process which consists of only non weird things

496
00:32:10,100 --> 00:32:13,860
if you do you get something that doesn't work very well it doesn't do much

497
00:32:13,860 --> 00:32:14,880
for more

498
00:32:14,940 --> 00:32:19,180
people do that all the time but you run out of steam with a very

499
00:32:19,180 --> 00:32:20,320
very quickly

500
00:32:20,360 --> 00:32:25,580
OK so for both reasons we have to be able to deal with where things

501
00:32:25,580 --> 00:32:29,600
the most ideal thing is to be able to deal with the weird things and

502
00:32:29,600 --> 00:32:33,280
get rid of them without even thinking about them

503
00:32:33,330 --> 00:32:36,450
what's the nice thing about the big theory

504
00:32:36,460 --> 00:32:41,140
it lets you get rid of all we're in this without even thinking about it

505
00:32:41,200 --> 00:32:44,430
but in order to understand to start with so we get rid of all that

506
00:32:44,430 --> 00:32:45,810
weird stuff

507
00:32:45,960 --> 00:32:50,740
we have to understand just a little bit about what it's about to start with

508
00:32:50,760 --> 00:32:55,110
OK so that's

509
00:32:55,170 --> 00:32:58,170
that's where we're going

510
00:32:59,350 --> 00:33:02,430
in this picture

511
00:33:02,530 --> 00:33:05,190
but the picture back out again

512
00:33:05,210 --> 00:33:12,140
i sort of left something very nice well-behaved function

513
00:33:12,200 --> 00:33:14,840
it's easy to find out what the

514
00:33:14,850 --> 00:33:16,400
what to measure

515
00:33:16,480 --> 00:33:22,350
of a the function is namely what what the measure is the set of values

516
00:33:22,590 --> 00:33:25,380
where you're in some tiny little interval

517
00:33:25,430 --> 00:33:28,250
and that's straightforward it was just the summary

518
00:33:28,260 --> 00:33:31,870
just the some of the bunch of intervals

519
00:33:31,920 --> 00:33:37,630
but now we come to the to the clincher which is we want to be

520
00:33:37,630 --> 00:33:40,330
knowledge based systems era

521
00:33:40,350 --> 00:33:41,960
which takes us up to about

522
00:33:42,510 --> 00:33:45,890
nineteen seventy nine

523
00:33:47,470 --> 00:33:48,910
we made into

524
00:33:48,990 --> 00:33:50,880
period where

525
00:33:50,920 --> 00:33:53,510
expert systems actually my people money

526
00:33:53,540 --> 00:33:55,620
so i guess the most famous one

527
00:33:55,670 --> 00:33:59,960
from from that period consistent with the deck which is

528
00:34:00,030 --> 00:34:05,580
dexter around this computer digital equipment corporation probably change something now something else now

529
00:34:05,620 --> 00:34:10,180
but they had an expert system they used to configure ordered that computers

530
00:34:11,150 --> 00:34:16,050
it was estimated that it was saying the company that forty million dollars a year

531
00:34:16,090 --> 00:34:18,840
there are they're related stories in the kind of things

532
00:34:21,690 --> 00:34:25,750
american military got similar kinds of stories that i i technology saving

533
00:34:25,760 --> 00:34:28,580
very large amounts of money

534
00:34:28,630 --> 00:34:30,650
so that was good

535
00:34:32,300 --> 00:34:37,050
in that same period there was a project started in japan called the fifth generation

536
00:34:38,400 --> 00:34:43,430
why fifth-generation while somehow the fall before the nodes of quite work that

537
00:34:43,520 --> 00:34:45,380
just what they were counting but anyway

538
00:34:45,460 --> 00:34:47,680
it was the fifth generation projects

539
00:34:48,190 --> 00:34:51,070
because in almost all the time

540
00:34:51,080 --> 00:34:52,850
started about nineteen eighty

541
00:34:52,870 --> 00:34:54,430
one also

542
00:34:54,490 --> 00:34:56,430
went into the nineties

543
00:34:56,490 --> 00:34:58,910
ten is one of the longer than that

544
00:34:58,930 --> 00:35:00,660
and it was

545
00:35:00,660 --> 00:35:04,400
it caused quite a stir because it was

546
00:35:04,410 --> 00:35:07,350
didn't follow the american style

547
00:35:07,410 --> 00:35:10,660
artificial intelligence which was based on this

548
00:35:11,680 --> 00:35:13,410
it is

549
00:35:13,410 --> 00:35:15,410
more european language prolog

550
00:35:15,410 --> 00:35:17,410
public based on logic

551
00:35:17,460 --> 00:35:20,290
this is based on a different kind of

552
00:35:20,350 --> 00:35:22,240
mathematical foundation

553
00:35:22,290 --> 00:35:26,190
this was the way americans did i i and they could imagine

554
00:35:26,410 --> 00:35:27,710
the other way

555
00:35:27,760 --> 00:35:30,070
and when the japanese chose problog

556
00:35:30,070 --> 00:35:32,680
the tracking of attention

557
00:35:33,850 --> 00:35:37,910
the first generation project was attempt to jump ahead

558
00:35:37,940 --> 00:35:39,210
and build

559
00:35:40,850 --> 00:35:44,020
they don't exactly talk about i have talked about solving

560
00:35:44,070 --> 00:35:47,820
i kinds of problems with new kinds of computers

561
00:35:47,880 --> 00:35:48,910
based on

562
00:35:48,970 --> 00:35:51,440
logical foundations

563
00:35:51,450 --> 00:35:53,980
and highly parallel

564
00:35:54,000 --> 00:35:55,440
so we went for

565
00:35:55,480 --> 00:35:58,040
about ten years ago but longer

566
00:35:59,130 --> 00:36:03,760
in fact i personally had a connection with i spent six weeks at tree in

567
00:36:03,780 --> 00:36:05,540
turkey which was

568
00:36:05,570 --> 00:36:08,360
an amazing experience

569
00:36:09,790 --> 00:36:12,290
the project itself

570
00:36:12,320 --> 00:36:14,040
on the whole

571
00:36:14,130 --> 00:36:18,360
it was an incredibly successful there remember now

572
00:36:18,410 --> 00:36:21,340
so at the beginning was better than the end

573
00:36:21,350 --> 00:36:24,630
in the end

574
00:36:24,630 --> 00:36:28,290
the a little less than other people which is that it's very hard to beat

575
00:36:28,350 --> 00:36:29,600
general purpose

576
00:36:30,920 --> 00:36:34,070
so the idea here is to build special purpose hardware

577
00:36:34,130 --> 00:36:39,630
does logical inference very rapidly and problem incredibly fast and power

578
00:36:39,660 --> 00:36:44,480
OK so the standard part meanwhile into was making making the general purpose

579
00:36:44,530 --> 00:36:47,690
o processors faster and faster

580
00:36:47,730 --> 00:36:54,640
by the end until we find just as fast as which is that the better

581
00:36:54,660 --> 00:36:57,590
so in fact trying to build special purpose troops

582
00:36:57,630 --> 00:37:01,190
this turned out not to be successful thing to do

583
00:37:01,200 --> 00:37:04,250
and forcing the japanese politics of that

584
00:37:04,290 --> 00:37:08,250
on the other hand many this got jobs fifth-generation project because a lot of money

585
00:37:08,250 --> 00:37:09,980
in the last round

586
00:37:09,980 --> 00:37:11,510
in the UK

587
00:37:11,510 --> 00:37:13,290
the project was started

588
00:37:13,320 --> 00:37:15,250
lots of jobs created

589
00:37:15,260 --> 00:37:20,340
and we shared some of that and the MCC which is the microelectronics and computer

590
00:37:20,340 --> 00:37:22,920
corporation was started in the US

591
00:37:22,940 --> 00:37:25,030
so called all the countries are very

592
00:37:25,070 --> 00:37:26,260
i guess

593
00:37:26,730 --> 00:37:29,850
motivated to try and match the fifth generation project

594
00:37:29,910 --> 00:37:31,100
and lots of

595
00:37:31,160 --> 00:37:33,810
it had lots of good side effects one of them was to get people to

596
00:37:33,820 --> 00:37:35,660
a i and to

597
00:37:35,660 --> 00:37:39,200
what programming and related areas

598
00:37:39,250 --> 00:37:42,630
OK so the next is

599
00:37:42,640 --> 00:37:46,810
the period of the nineteen eighties into the early nineteen nineties

600
00:37:46,850 --> 00:37:51,260
and members to about neurons before the very beginning mcculloch and pitts

601
00:37:51,310 --> 00:37:53,440
and then there was a period where

602
00:37:53,450 --> 00:37:54,480
it somehow

603
00:37:54,490 --> 00:37:57,790
funding died because it was some theorems prove said

604
00:37:57,840 --> 00:38:00,550
you know was could do certain things in fact it was pretty clear that this

605
00:38:00,550 --> 00:38:02,950
was not good objection anyway

606
00:38:04,370 --> 00:38:07,220
in the early eighties the number of people

607
00:38:07,230 --> 00:38:12,420
discovered what's called the backpropagation algorithm for neural networks so neural network is a collection

608
00:38:12,420 --> 00:38:14,030
of neurons

609
00:38:14,080 --> 00:38:15,720
and signals come in

610
00:38:15,720 --> 00:38:18,530
and the computer results

611
00:38:19,250 --> 00:38:19,900
the y

612
00:38:19,910 --> 00:38:22,400
and these things have to be trained

613
00:38:22,430 --> 00:38:26,040
and the training is in terms of weights between the different neurons

614
00:38:26,100 --> 00:38:27,930
so there's a whole bunch of parameters

615
00:38:27,950 --> 00:38:31,180
they can be hundreds and hundreds of parameters which these weights

616
00:38:31,230 --> 00:38:32,610
and you want to write weights

617
00:38:32,620 --> 00:38:34,670
the computed result

618
00:38:34,680 --> 00:38:36,140
so things how to train

619
00:38:36,170 --> 00:38:40,100
and the backpropagation algorithm was independently discovered by a whole bunch of people

620
00:38:40,140 --> 00:38:42,040
basically it's a way of

621
00:38:42,050 --> 00:38:46,240
pushing data through the neural network and then back propagating it to upgrade upgrade the

622
00:38:47,310 --> 00:38:51,490
and you do this ten thousand times you get a neural network that learns what

623
00:38:51,500 --> 00:38:53,530
you want to learn

624
00:38:53,540 --> 00:38:58,390
so that was anomalies enormously successful technology and very very widely used as to what

625
00:38:58,390 --> 00:39:00,430
is today the neural networks as

626
00:39:00,480 --> 00:39:02,310
probably still

627
00:39:02,610 --> 00:39:06,030
the most widely used machine learning technology

628
00:39:06,530 --> 00:39:12,080
so you're networks made a recovery point

629
00:39:12,110 --> 00:39:15,660
and then moving from the eighties into them say the early nineties

630
00:39:15,660 --> 00:39:18,340
and that's the forces

631
00:39:18,350 --> 00:39:24,410
the euclidean space is an example of symmetric positive definite by linear form semantic because

632
00:39:24,410 --> 00:39:28,740
if there's a competitiveness involves

633
00:39:29,530 --> 00:39:30,620
all rights

634
00:39:30,640 --> 00:39:36,460
so many men involved in the manifold is a differentiable manifold can reach each tangent

635
00:39:36,460 --> 00:39:44,720
space has an inner product so this is important we can define an inner product

636
00:39:45,570 --> 00:39:50,680
metric inducing in norm for the tangent vector so now we know which means that

637
00:39:50,680 --> 00:39:53,500
i can go to compute distances in that

638
00:39:55,020 --> 00:39:58,180
it's possible to define different metrics so

639
00:39:58,190 --> 00:40:00,510
are you end up

640
00:40:00,520 --> 00:40:03,730
the difference between different many manifolds

641
00:40:03,780 --> 00:40:09,140
this talk about to do this do is more

642
00:40:09,150 --> 00:40:15,730
curve that joins the points along the shortest spread on the manifold

643
00:40:15,780 --> 00:40:21,050
the way to do this is important because it use it as the minimum distance

644
00:40:21,050 --> 00:40:26,610
between two points on the manifold so you know there's a twice the shortest frying

645
00:40:26,940 --> 00:40:31,260
from boston to istanbul is not can conform the metadata

646
00:40:31,320 --> 00:40:35,690
line but it's kind of they opened this curve is the geodesic

647
00:40:35,710 --> 00:40:40,640
this measurement is not explored and shows great

648
00:40:40,650 --> 00:40:41,420
in the

649
00:40:41,480 --> 00:40:48,590
the together numbers it is what define now the explanation that the next the vector

650
00:40:49,280 --> 00:40:51,020
in the tangent space

651
00:40:51,030 --> 00:40:57,800
to the point on the manifold so the defining a mapping from the tangent spaces

652
00:40:57,800 --> 00:41:00,040
to back to the manifold

653
00:41:00,060 --> 00:41:05,880
and it is the property that a geodesic e

654
00:41:06,150 --> 00:41:11,440
reaches all of two units are for instance of

655
00:41:11,460 --> 00:41:13,130
the value that it

656
00:41:13,140 --> 00:41:13,970
i have

657
00:41:13,980 --> 00:41:19,010
o point is a point on the on y is equal to one of the

658
00:41:19,300 --> 00:41:25,820
by the victors velocities piecewise constant for geophysics but it's not going to give you

659
00:41:25,820 --> 00:41:28,670
that i mean to commodities arm

660
00:41:28,680 --> 00:41:33,970
under the exponential map to each of the zero zero tangent vector is the point

661
00:41:36,570 --> 00:41:41,840
it means that you are defining these explanations only for a specific point on the

662
00:41:41,840 --> 00:41:47,340
manifold which means that if i moved to another point when don't change

663
00:41:47,360 --> 00:41:51,850
of the four not change but can affect the result is going to change so

664
00:41:51,850 --> 00:41:55,400
these operators are defined point specific

665
00:41:55,420 --> 00:41:59,860
since the best stone to do do this it is constant length of the geodesic

666
00:41:59,860 --> 00:42:04,890
is given by the norm of the initial velocity and this is the kind norm

667
00:42:05,030 --> 00:42:07,540
on this issue

668
00:42:07,550 --> 00:42:14,090
all right for each point on the manifold we the exponential map is a diffeomorphism

669
00:42:14,090 --> 00:42:21,410
which means one old and differentiable mappings in both directions by the inverse mapping x

670
00:42:21,480 --> 00:42:28,680
is only defined in the neighborhood points and think about negative a real or complex

671
00:42:28,680 --> 00:42:29,840
numbers of

672
00:42:29,920 --> 00:42:35,760
one is kind of mapping contains the other one is only defined only on the

673
00:42:35,760 --> 00:42:44,010
neighborhood this like the one in small neighborhood are assumptions are going to be that

674
00:42:44,130 --> 00:42:49,430
this is kind of an illustration showing the geodesic between two points

675
00:42:49,460 --> 00:42:52,150
and a mapping from manifold to

676
00:42:52,270 --> 00:42:58,910
tangent space and i just remember this thing this still if you want to talk

677
00:42:58,910 --> 00:43:05,100
about manifold learning having to go from meaningful to tangent space

678
00:43:05,180 --> 00:43:11,000
because to complicated space between euclidean space which is just the logarithm operation

679
00:43:11,020 --> 00:43:17,390
and your application is what you need to look at spatial morgan i'm gonna can

680
00:43:17,730 --> 00:43:21,110
talk about the thing and it is also a

681
00:43:21,130 --> 00:43:26,140
making to go back to the manifold because you might be interested in the origin

682
00:43:26,140 --> 00:43:32,480
of this could like that this a structure of an object are so this

683
00:43:32,490 --> 00:43:38,750
these are the two things you may want to kind of keep in mind

684
00:43:38,760 --> 00:43:44,670
that is a couple more slides before the brain stars by the formal definition of

685
00:43:44,670 --> 00:43:49,680
duties against the explanation that the distance between two points on a manifold can be

686
00:43:49,680 --> 00:43:55,820
computed as the distance between the point on the manifold and kind of a is

687
00:43:56,230 --> 00:44:05,650
ten just space correspondence on the manifold a but as i mentioned the and it

688
00:44:05,660 --> 00:44:07,130
can be found in all

689
00:44:07,150 --> 00:44:12,920
and by on the tangent space and this makes sense from that point on the

690
00:44:12,940 --> 00:44:18,410
tangent space and two here is the same thing written because this is an inner

691
00:44:18,410 --> 00:44:22,210
product and this the thing i showed you before

692
00:44:22,210 --> 00:44:27,510
for many many false than during an inverse mapping can dive right these things

693
00:44:27,520 --> 00:44:31,100
which means that i'm gonna multiply left multiply

694
00:44:31,130 --> 00:44:36,410
one of the points on the manifold commercial the one of the points

695
00:44:36,460 --> 00:44:40,500
there are one this can be defined in distance

696
00:44:40,550 --> 00:44:45,620
all right well about these local and explanation thing right

697
00:44:45,620 --> 00:44:47,560
the talk has two parts

698
00:44:47,580 --> 00:44:49,390
the first part

699
00:44:49,400 --> 00:44:51,920
i will be by are to set

700
00:44:51,970 --> 00:44:56,730
another an introduction before and so to to kind of create to set up for

701
00:44:56,730 --> 00:44:59,210
people haven't seen this before

702
00:44:59,240 --> 00:45:03,890
i know will basically be talking about

703
00:45:03,910 --> 00:45:09,620
particle filtering for state space models and we will discuss some algorithms that we need

704
00:45:09,930 --> 00:45:13,430
a lot of us are very familiar with in this community

705
00:45:13,460 --> 00:45:15,620
but then he will introduce some

706
00:45:15,680 --> 00:45:21,730
recent new development that he's been working on with colleagues where he's looking at asia's

707
00:45:21,730 --> 00:45:26,860
of parameter learning not just filtering in state estimation but trying to go beyond due

708
00:45:26,860 --> 00:45:29,510
to tackle problems like smoothing

709
00:45:30,450 --> 00:45:32,160
and learning

710
00:45:32,300 --> 00:45:33,990
after that we

711
00:45:34,050 --> 00:45:38,230
this tutorial kind of log so we're going to have a short break

712
00:45:38,230 --> 00:45:43,010
and then i will look into particle filtering

713
00:45:43,010 --> 00:45:47,300
but i will consider the type of problems that we don't often

714
00:45:47,800 --> 00:45:49,050
the deal with

715
00:45:49,060 --> 00:45:55,580
things like igon value problems protein folding problems control problems

716
00:45:55,590 --> 00:45:57,750
and static distributions

717
00:45:57,780 --> 00:46:03,370
and so on and also some very interesting ways of learning ABC ABC is like

718
00:46:03,370 --> 00:46:05,090
this really cool

719
00:46:05,140 --> 00:46:11,160
bayesian technique where you can learn if you can imagine as basic so

720
00:46:11,170 --> 00:46:16,480
only these four topics were covered basically the end of the twentieth century

721
00:46:16,500 --> 00:46:19,170
and most of what we're going to talk about

722
00:46:19,530 --> 00:46:21,340
will be new things

723
00:46:21,360 --> 00:46:26,580
many many people in this community have worked in this problem

724
00:46:26,590 --> 00:46:30,670
and many communities to have attacked this problem

725
00:46:30,690 --> 00:46:36,720
people in control people in signal processing people and statistics

726
00:46:36,730 --> 00:46:41,610
physics and so on and they'll give them method different names and slightly different methodologies

727
00:46:41,810 --> 00:46:43,450
and we'll try to

728
00:46:43,470 --> 00:46:45,950
bring to your attention some of these

729
00:46:45,970 --> 00:46:50,810
in machine learning the method became popular with andrew blake michael is the computer vision

730
00:46:50,810 --> 00:46:54,950
applications a sort of those show you some examples

731
00:46:55,040 --> 00:47:00,010
many people have contributed to the method and now i have listed some here

732
00:47:00,020 --> 00:47:01,430
without mentioning

733
00:47:01,550 --> 00:47:05,800
just so already known face he's done a lot of work on this too and

734
00:47:05,830 --> 00:47:10,360
i'm sure i'm missing some of you in the audience

735
00:47:11,230 --> 00:47:15,760
so what's particle filtering so it's a technique for approximating

736
00:47:17,360 --> 00:47:20,980
and that evolve over time that have some sort of dynamics

737
00:47:21,020 --> 00:47:24,360
but it can also be used for static distributions is out of

738
00:47:24,360 --> 00:47:29,360
about the DNA full for now let's consider dynamic distributions

739
00:47:29,400 --> 00:47:32,740
the idea is we want to approximate the distribution

740
00:47:32,790 --> 00:47:37,080
and the way we do it is we draw samples from this thread distribution

741
00:47:37,100 --> 00:47:41,630
and then if we just count how many samples full image then we get histogram

742
00:47:41,630 --> 00:47:44,240
is approximation of this

743
00:47:44,290 --> 00:47:46,800
and we can give weight to these

744
00:47:47,070 --> 00:47:51,700
different samples which is what we call particles as well

745
00:47:51,730 --> 00:47:54,800
and that allows us to get a good representation of this

746
00:47:54,820 --> 00:47:59,230
and here we can see how these particles propagate over time

747
00:47:59,240 --> 00:48:01,110
and if they are doing well

748
00:48:01,110 --> 00:48:02,900
they get bigger weight

749
00:48:02,920 --> 00:48:06,680
so this sort of the survival of the fittest

750
00:48:06,700 --> 00:48:08,700
aspect to this

751
00:48:08,890 --> 00:48:10,570
and yet another

752
00:48:10,580 --> 00:48:16,080
picture of the same thing we you showed this to how the distribution is evolving

753
00:48:16,110 --> 00:48:17,760
over time

754
00:48:18,800 --> 00:48:23,710
you're able to in this case tracker target that actually has two

755
00:48:23,730 --> 00:48:26,670
possible hypotheses of what could be god

756
00:48:26,670 --> 00:48:30,450
and so you have to maintain a hypothesis

757
00:48:34,650 --> 00:48:36,300
so here's an example

758
00:48:39,610 --> 00:48:43,270
how you can use

759
00:48:45,390 --> 00:48:51,100
particles here just you trying to estimate the probability distribution over the location of these

760
00:48:51,270 --> 00:48:53,140
play is being tracked

761
00:48:53,150 --> 00:48:57,140
and each of these little boxes like in this layers of particles

762
00:48:57,150 --> 00:48:59,980
and then when you take the average of sort of ketamine

763
00:49:00,010 --> 00:49:02,890
track these things in video on the TV

764
00:49:02,900 --> 00:49:06,540
we then project them to the world using some basic geometry

765
00:49:06,570 --> 00:49:11,440
and then you have sort of trajectories you know what the person was doing

766
00:49:11,450 --> 00:49:13,740
in this case in the game

767
00:49:13,740 --> 00:49:18,780
OK i have added question

768
00:49:18,930 --> 00:49:21,480
for the first part

769
00:49:23,360 --> 00:49:25,190
here are some bigger

770
00:49:27,360 --> 00:49:29,040
the first is

771
00:49:29,060 --> 00:49:31,000
or coca cola

772
00:49:31,010 --> 00:49:36,300
during the forty four years fields

773
00:49:36,350 --> 00:49:42,710
in the from the need to be these aren't heal

774
00:49:42,760 --> 00:49:44,860
almost now

775
00:49:44,870 --> 00:49:50,750
here is the one over and times longer than which is the average growth rate

776
00:49:50,770 --> 00:49:53,700
you could see that the

777
00:49:53,750 --> 00:49:56,530
the average growth rate

778
00:49:56,570 --> 00:50:02,210
goldberg is fairly quickly

779
00:50:02,220 --> 00:50:05,000
plus number

780
00:50:05,010 --> 00:50:09,560
we which is about all all all

781
00:50:09,570 --> 00:50:12,330
o five or something like that

782
00:50:12,340 --> 00:50:16,230
that's a very small number

783
00:50:16,320 --> 00:50:23,500
and also one may ask that there

784
00:50:23,550 --> 00:50:28,180
the conditions on friday the eighth of the ideal assume that the

785
00:50:29,910 --> 00:50:36,580
the classes of three there is stationary stationary but it brought which means that the

786
00:50:36,590 --> 00:50:43,260
all the multidimensional distributions are invariant under the time changes

787
00:50:43,350 --> 00:50:46,500
so the distribution of the segment

788
00:50:46,510 --> 00:50:52,030
today is the same as the distribution of the segment of the same length

789
00:50:52,800 --> 00:50:55,380
three years later

790
00:50:55,390 --> 00:50:58,110
these figure

791
00:50:59,190 --> 00:51:01,730
does not contradict the

792
00:51:01,740 --> 00:51:05,260
could be the function

793
00:51:05,690 --> 00:51:08,930
and here is the histogram

794
00:51:09,000 --> 00:51:19,410
it is

795
00:51:19,460 --> 00:51:21,880
here you can see the

796
00:51:22,270 --> 00:51:27,180
fifty the normal that it would be his ograms

797
00:51:29,380 --> 00:51:32,720
it is it has the mean

798
00:51:32,730 --> 00:51:36,730
or or all peaks

799
00:51:36,780 --> 00:51:40,820
and the standard deviation of the group of the area

800
00:51:42,940 --> 00:51:49,450
twenty five times larger than these small holes the wall

801
00:51:49,590 --> 00:51:53,880
because it has a mean very close

802
00:51:53,930 --> 00:51:56,220
who who who won

803
00:51:56,300 --> 00:52:00,850
if scale is on the return and the

804
00:52:01,120 --> 00:52:03,770
in the region up on the log

805
00:52:03,910 --> 00:52:10,200
so the the mean is very very close to to one

806
00:52:10,210 --> 00:52:16,180
and interesting point is that it is not symmetric distribution

807
00:52:16,200 --> 00:52:22,850
so that the probability that is likely to be crazy

808
00:52:22,870 --> 00:52:28,280
is larger than the probability of life increase

809
00:52:28,300 --> 00:52:30,670
of the SS

810
00:52:30,720 --> 00:52:33,620
and the base sequence of three

811
00:52:33,630 --> 00:52:41,300
along and as that either very hard to predict essentially impossible to predict

812
00:52:41,310 --> 00:52:42,770
because of the

813
00:52:42,780 --> 00:52:48,570
very few large standard deviation because of the very large red area of

814
00:52:50,790 --> 00:52:56,100
back to the mean of the

815
00:52:56,150 --> 00:52:58,020
and feel

816
00:52:58,070 --> 00:53:03,120
using such read the sequence

817
00:53:03,170 --> 00:53:06,480
we we'll be very successful or

818
00:53:06,490 --> 00:53:08,000
the log optimum

819
00:53:08,010 --> 00:53:14,010
portfolio selection four and the goal of optimal portfolio selection

820
00:53:27,920 --> 00:53:29,650
this is one the

821
00:53:29,930 --> 00:53:36,360
property what i can extract from the the the good i don't know who much

822
00:53:36,360 --> 00:53:42,510
about the reasoning behind the cart

823
00:53:45,630 --> 00:53:54,430
on what

824
00:54:04,380 --> 00:54:06,110
we haven't made such

825
00:54:10,870 --> 00:54:12,450
this is the

826
00:54:12,460 --> 00:54:14,560
same for IBM

827
00:54:14,570 --> 00:54:20,520
and here the histogram has similar shape

828
00:54:20,520 --> 00:54:21,730
they are they

829
00:54:21,740 --> 00:54:25,240
the derivative of the probability that the problem

830
00:54:25,360 --> 00:54:27,950
so this parameter actually

831
00:54:27,990 --> 00:54:32,440
one when is sequence position is

832
00:54:32,490 --> 00:54:33,760
so that

833
00:54:33,870 --> 00:54:37,200
so essentially we replace i mean poor

834
00:54:37,210 --> 00:54:38,490
we have this

835
00:54:38,950 --> 00:54:43,520
during metric so the number of letters by the number of positions

836
00:54:43,540 --> 00:54:48,770
so there the derivative of that one is going to be all of europe except

837
00:54:49,010 --> 00:54:53,780
except the position where we have a letter in policy is just like a binary

838
00:54:53,780 --> 00:54:57,010
representation of secret

839
00:54:57,050 --> 00:55:01,300
and if you know take the perspective i mean take this magic make it

840
00:55:01,310 --> 00:55:04,870
and take this kind of approach this is going to be identical with the baby

841
00:55:07,000 --> 00:55:08,740
you can just

842
00:55:08,750 --> 00:55:12,580
one of the competition this year

843
00:55:12,590 --> 00:55:17,480
OK i there

844
00:55:25,550 --> 00:55:29,750
right the locked up here and then this

845
00:55:29,750 --> 00:55:31,800
in the summer right

846
00:55:31,830 --> 00:55:33,440
and then

847
00:55:33,480 --> 00:55:34,650
this mistake

848
00:55:35,670 --> 00:55:36,670
you should

849
00:55:36,720 --> 00:55:40,670
the log p and then the product becomes

850
00:55:40,710 --> 00:55:43,660
and then we get the new one

851
00:55:43,680 --> 00:55:46,030
thank you

852
00:55:59,740 --> 00:56:02,120
so it should be so

853
00:56:02,140 --> 00:56:07,790
questions but i always replace the fisher information metric with the like the identity matrix

854
00:56:07,800 --> 00:56:09,760
anything that i do

855
00:56:10,710 --> 00:56:15,500
i guess when i learn a linear classifier on top of this i mean the

856
00:56:15,500 --> 00:56:23,690
fisher information metric only like stretching dimensions and rotating in it's way for a kernel

857
00:56:23,690 --> 00:56:28,940
classifier doesn't really matter whether you rotate data maybe it matters whether stretch

858
00:56:28,990 --> 00:56:33,450
but rotating at least doesn't change your life

859
00:56:33,470 --> 00:56:39,520
so for when you when you learn SVM based on the future already

860
00:56:39,580 --> 00:56:41,160
make a big difference

861
00:56:42,530 --> 00:56:49,620
but i will usually ignore them because it's quite expensive to efficiently

862
00:56:54,340 --> 00:56:56,010
OK so

863
00:56:56,010 --> 00:56:58,450
this was one kernel which was

864
00:56:58,500 --> 00:57:03,150
pine based on probabilistic models and i give you two examples

865
00:57:03,170 --> 00:57:05,740
where we use similarity measure

866
00:57:07,040 --> 00:57:09,150
to define

867
00:57:09,230 --> 00:57:13,680
so this is one which is called the pairwise comparison

868
00:57:13,730 --> 00:57:17,190
so the general idea is that the

869
00:57:17,260 --> 00:57:24,060
program for computing similarity score between two sequence so this is an alignment program

870
00:57:24,120 --> 00:57:28,650
OK called block and that gives us a score for each

871
00:57:28,690 --> 00:57:30,830
possible health bring

872
00:57:31,050 --> 00:57:34,780
how can we use this string for this these alignments

873
00:57:34,830 --> 00:57:40,460
so we cannot directly use the similarity between the because that's not going to be

874
00:57:41,840 --> 00:57:45,430
what you can do it so we can take

875
00:57:45,440 --> 00:57:50,250
all the sequences but we have in the training set and compare all the examples

876
00:57:50,300 --> 00:57:51,440
each other

877
00:57:51,480 --> 00:57:52,910
then take

878
00:57:52,930 --> 00:57:55,050
like one goal of

879
00:57:55,440 --> 00:57:58,990
as the vector of features

880
00:58:00,140 --> 00:58:04,170
the problem is that we have to compare all sequences with each other and then

881
00:58:04,170 --> 00:58:11,300
you have to compute scalar product quite long only this leads to fight high computational

882
00:58:11,300 --> 00:58:16,180
costs in the u n is the number of training

883
00:58:18,910 --> 00:58:22,750
slightly better kernel is the local alignment kernel

884
00:58:22,810 --> 00:58:27,280
he is some more definitions

885
00:58:27,490 --> 00:58:29,720
OK so

886
00:58:30,620 --> 00:58:34,750
the idea of the local alignment is that you define

887
00:58:34,760 --> 00:58:39,670
based on the substitution method and and based on can be used to find score

888
00:58:39,710 --> 00:58:41,160
for certain alignment

889
00:58:41,620 --> 00:58:44,070
think like with you

890
00:58:44,090 --> 00:58:48,940
and sequence that the left image quite well

891
00:58:49,940 --> 00:58:53,570
i l i e

892
00:58:53,570 --> 00:58:55,390
this works as long as

893
00:58:55,390 --> 00:58:56,810
the speed

894
00:58:56,870 --> 00:58:58,400
is much smaller than

895
00:58:58,410 --> 00:59:00,490
the speed of light

896
00:59:00,570 --> 00:59:02,570
if that's no longer the case

897
00:59:02,650 --> 00:59:04,510
then we have to apply

898
00:59:04,520 --> 00:59:06,710
special relativity

899
00:59:06,720 --> 00:59:09,110
and that is not part of this course

900
00:59:09,150 --> 00:59:12,680
but i would like to briefly touched upon that today

901
00:59:12,690 --> 00:59:14,180
i can show you

902
00:59:14,220 --> 00:59:17,250
how things go sour

903
00:59:17,300 --> 00:59:19,320
because suppose we have a

904
00:59:19,440 --> 00:59:22,070
five hundred

905
00:59:22,070 --> 00:59:23,680
you know

906
00:59:24,710 --> 00:59:29,200
of electron

907
00:59:29,250 --> 00:59:32,930
four that means that in this equation here

908
00:59:32,950 --> 00:59:37,150
is five hundred thousand

909
00:59:37,160 --> 00:59:39,790
q is the charge of the electron

910
00:59:39,850 --> 00:59:42,180
and is now the mass of the electron

911
00:59:42,240 --> 00:59:44,230
and if i applied that equation

912
00:59:44,250 --> 00:59:47,300
i find that v is four point two

913
00:59:47,310 --> 00:59:49,130
times ten thirty eight

914
00:59:49,130 --> 00:59:50,480
beta the second

915
00:59:50,490 --> 00:59:51,910
and that is larger

916
00:59:51,960 --> 00:59:56,400
and the speed of light that's clearly not possible

917
00:59:56,460 --> 01:00:00,190
the actual speed if you make relativistic corrections

918
01:00:00,260 --> 01:00:02,610
is two point six

919
01:00:02,780 --> 01:00:04,870
ten thirty eight

920
01:00:04,900 --> 01:00:07,810
meters per second

921
01:00:07,840 --> 01:00:12,730
and although i don't expect you to be able to make those relativistic corrections

922
01:00:12,770 --> 01:00:14,750
i will make them today

923
01:00:14,800 --> 01:00:17,060
you will see why i have two

924
01:00:17,090 --> 01:00:18,550
and i want to show you

925
01:00:18,560 --> 01:00:19,940
that in fact

926
01:00:20,020 --> 01:00:21,800
this is not all that difficult

927
01:00:21,810 --> 01:00:24,530
even though i will not hold you responsible

928
01:00:24,540 --> 01:00:27,580
forty equations

929
01:00:27,630 --> 01:00:30,830
so what i have here

930
01:00:30,860 --> 01:00:33,530
is now

931
01:00:33,540 --> 01:00:35,030
kinetic energy

932
01:00:35,220 --> 01:00:36,530
again qv

933
01:00:36,540 --> 01:00:38,120
that's not changing

934
01:00:38,140 --> 01:00:40,750
it is no longer one half MV squared

935
01:00:40,760 --> 01:00:44,230
this gonna minus one times MC squared and gamma

936
01:00:44,410 --> 01:00:47,650
defined their called to lawrence factor

937
01:00:47,710 --> 01:00:50,720
so if you know now for the electron

938
01:00:50,760 --> 01:00:53,610
that capital these five hundred thousand

939
01:00:53,640 --> 01:00:56,800
you can calculate what organised from the first equation

940
01:00:56,850 --> 01:01:00,540
and then you go to the second equation you find speed is you'll see that

941
01:01:00,540 --> 01:01:02,680
that you never find this being larger

942
01:01:03,610 --> 01:01:05,840
the speed of light

943
01:01:05,890 --> 01:01:06,720
and so

944
01:01:06,730 --> 01:01:10,010
we now have to make a correction also

945
01:01:10,050 --> 01:01:12,100
forty eighty i

946
01:01:12,110 --> 01:01:14,760
and those corrections become

947
01:01:15,900 --> 01:01:17,810
relatively easy

948
01:01:17,870 --> 01:01:21,430
this now requires effective come and see that

949
01:01:21,440 --> 01:01:23,430
on the upper blackboard there

950
01:01:23,440 --> 01:01:25,710
and these two now

951
01:01:25,760 --> 01:01:29,310
it has to be replaced by government was one and then everything

952
01:01:29,340 --> 01:01:31,750
is OK

953
01:01:31,760 --> 01:01:33,990
so i don't expect you to know this

954
01:01:34,000 --> 01:01:37,340
but i don't want you to think that all these relative the corrections come out

955
01:01:37,340 --> 01:01:38,380
of the blue

956
01:01:38,390 --> 01:01:42,090
nor do i want you to think that is very difficult really is

957
01:01:42,100 --> 01:01:47,810
in equations are extremely straightforward

958
01:01:47,850 --> 01:01:49,850
i want to show you now the

959
01:01:49,890 --> 01:01:52,650
some of the results that we just

960
01:01:55,390 --> 01:01:58,760
the one MTV proton

961
01:01:58,760 --> 01:02:01,230
and the five hundred kv

962
01:02:02,260 --> 01:02:04,130
this is on the web

963
01:02:04,140 --> 01:02:06,990
you can click on lectures supplement and you can make yourself

964
01:02:07,090 --> 01:02:08,720
hard copy

965
01:02:08,760 --> 01:02:10,830
so you see the kinetic energy

966
01:02:10,880 --> 01:02:12,970
one of the proton

967
01:02:13,010 --> 01:02:18,500
notice the speed that we calculated their non relativistic government very close to one you

968
01:02:18,500 --> 01:02:20,340
don't have to make corrections

969
01:02:20,350 --> 01:02:22,080
and in one thousand light field

970
01:02:22,090 --> 01:02:24,380
you get a radius of fifteen centimetres

971
01:02:24,420 --> 01:02:26,660
we just calculated

972
01:02:26,670 --> 01:02:29,180
if you go to a fifty and the proton

973
01:02:29,240 --> 01:02:33,380
sort of on the borderline between relativistic and non relativistic

974
01:02:33,390 --> 01:02:36,110
still non relativistic enough

975
01:02:36,120 --> 01:02:38,310
and it is non relativistic

976
01:02:38,340 --> 01:02:40,030
you can clearly see here

977
01:02:40,050 --> 01:02:41,930
that's the radius

978
01:02:41,960 --> 01:02:45,500
goes with the square root of capitol hill

979
01:02:45,510 --> 01:02:47,130
and for fifty mv

980
01:02:47,130 --> 01:02:49,270
capital fierce fifty million

981
01:02:49,360 --> 01:02:52,510
and for one in the capital is one million

982
01:02:52,560 --> 01:02:56,300
and since it goes with the square root of the you expect roughly the radius

983
01:02:56,310 --> 01:02:59,650
to be square root of fifty times larger which is seven

984
01:02:59,670 --> 01:03:01,560
and indeed you see that

985
01:03:01,560 --> 01:03:03,630
that's my information

986
01:03:03,680 --> 01:03:09,270
in fact extraction so we continue with machinery

987
01:03:09,660 --> 01:03:16,190
towards the end will also speak somehow about future research things well

988
01:03:16,200 --> 01:03:18,330
research we would like to do

989
01:03:18,350 --> 01:03:20,990
which we are not aliens

990
01:03:21,080 --> 01:03:25,020
to execute but i think it is just

991
01:03:25,080 --> 01:03:28,040
because it might also like this for you to

992
01:03:28,040 --> 01:03:34,940
for doing research in this area

993
01:03:35,620 --> 01:03:40,010
we we thought that so yes

994
01:03:40,010 --> 01:03:43,410
somehow all

995
01:03:43,540 --> 01:03:48,220
the problem that

996
01:03:48,230 --> 01:03:51,400
when you communicate through text

997
01:03:51,430 --> 01:03:53,040
there are many many

998
01:03:53,360 --> 01:03:57,440
variables that are interdependent

999
01:03:57,530 --> 01:03:58,840
so actually

1000
01:03:58,890 --> 01:04:03,300
what we see in a text what we see and what right

1001
01:04:03,340 --> 01:04:09,950
of depends on the context so the words are not used in isolation

1002
01:04:09,970 --> 01:04:12,400
the used

1003
01:04:12,420 --> 01:04:17,120
in this system configuration in this country is in the

1004
01:04:18,080 --> 01:04:20,680
it supported the examples

1005
01:04:20,700 --> 01:04:23,470
if you are looking for

1006
01:04:23,480 --> 01:04:24,890
named entities

1007
01:04:24,900 --> 01:04:30,170
so you want to classify words such as persons locations

1008
01:04:30,180 --> 01:04:31,730
and you have to you

1009
01:04:31,730 --> 01:04:33,420
well it could be

1010
01:04:33,440 --> 01:04:38,810
a location but new york times text is an organisation

1011
01:04:38,870 --> 01:04:40,280
so how

1012
01:04:40,370 --> 01:04:45,370
well it's actually from the context from the rest of the round

1013
01:04:45,500 --> 01:04:46,420
new york

1014
01:04:46,450 --> 01:04:52,640
that you can infer from this that there is mounting location station

1015
01:05:00,980 --> 01:05:02,500
to summarize

1016
01:05:02,530 --> 01:05:07,590
communication that language has some structure and

1017
01:05:07,610 --> 01:05:15,310
exploring the structure also when using machine learning techniques for instance we have sequences

1018
01:05:15,340 --> 01:05:19,370
a sentence has worked in a certain sequence

1019
01:05:19,380 --> 01:05:24,630
and in some language is the position of the words in the sentence can make

1020
01:05:24,630 --> 01:05:26,650
a big difference between the two

1021
01:05:27,650 --> 01:05:29,220
i'd like see

1022
01:05:29,930 --> 01:05:31,800
but in english for instance

1023
01:05:31,810 --> 01:05:33,560
the subject is used

1024
01:05:34,470 --> 01:05:40,030
one of the first word the first nouns sentence followed by

1025
01:05:40,800 --> 01:05:45,300
and it follows that i object to that

1026
01:05:48,470 --> 01:05:49,680
so this

1027
01:05:50,960 --> 01:05:54,560
but also s communication is kind of

1028
01:05:54,570 --> 01:05:56,400
i structure

1029
01:05:56,440 --> 01:05:59,740
and this is more when we look into scores

1030
01:05:59,750 --> 01:06:03,060
let's just look at the table of contents in the book

1031
01:06:03,250 --> 01:06:09,930
it's structure you have some topics through topics which

1032
01:06:09,940 --> 01:06:14,620
find more detail topics as well by the search

1033
01:06:15,720 --> 01:06:34,500
so maybe we can exploit also this hierarchical structure

1034
01:06:34,520 --> 01:06:36,160
i mean

1035
01:06:36,180 --> 01:06:36,840
thank you

1036
01:06:36,850 --> 01:06:42,400
so we decided to

1037
01:06:43,440 --> 01:06:44,630
this course

1038
01:06:44,650 --> 01:06:45,340
it is the word

1039
01:06:45,370 --> 01:06:49,180
fact extraction so most of what we are

1040
01:06:50,810 --> 01:06:59,090
this course is in fact extraction fact extraction as i mentioned already is something you

1041
01:06:59,090 --> 01:07:04,340
be able to to do so we can extract facts certain from sentences

1042
01:07:04,350 --> 01:07:05,470
so we

1043
01:07:05,470 --> 01:07:07,250
this fact extraction

1044
01:07:07,250 --> 01:07:13,530
it could benefit from modeling in this context at least some slack

1045
01:07:13,560 --> 01:07:20,860
but i think it's fixed line should move beyond such fact extraction

1046
01:07:20,870 --> 01:07:24,660
remind you might be to

1047
01:07:27,230 --> 01:07:31,310
this is not substantial support from

1048
01:07:31,390 --> 01:07:33,350
so just write

1049
01:07:33,370 --> 01:07:37,120
but you can mention especially fields like

1050
01:07:37,140 --> 01:07:39,920
like this

1051
01:07:39,940 --> 01:07:42,180
especially for instance in LA

1052
01:07:42,200 --> 01:07:47,850
you seem many abstract concepts and maybe you also to

1053
01:07:47,870 --> 01:07:51,410
just maybe not only to expect certain facts

1054
01:07:51,420 --> 01:07:56,760
maybe along with you might have occurred in fact

1055
01:07:56,770 --> 01:08:01,580
the number of people can for instance

1056
01:08:01,600 --> 01:08:07,060
this can do it you might want to go beyond extracts

1057
01:08:07,080 --> 01:08:09,060
system concepts

1058
01:08:09,070 --> 01:08:11,570
hi level concepts from

1059
01:08:11,600 --> 01:08:14,510
the text the

1060
01:08:14,660 --> 01:08:19,830
and we want to integrate this context

1061
01:08:19,860 --> 01:08:24,210
and this is the actual fruitful life

1062
01:08:24,210 --> 01:08:29,980
on the first try to get right

1063
01:08:30,120 --> 01:08:36,730
first of all let's get

1064
01:08:41,780 --> 01:08:52,160
what turns have

1065
01:09:05,520 --> 01:09:08,180
the so

1066
01:09:24,320 --> 01:09:33,410
article as

1067
01:09:48,570 --> 01:09:54,060
there is a function of about

1068
01:09:54,310 --> 01:09:59,190
in fact the

1069
01:09:59,210 --> 01:10:05,890
i trust you

1070
01:10:14,550 --> 01:10:16,370
o of

1071
01:10:16,370 --> 01:10:23,910
one of the four

1072
01:10:27,460 --> 01:10:32,050
are around

1073
01:10:32,640 --> 01:10:36,230
part of lot of

1074
01:11:30,690 --> 01:11:35,420
in terms of

1075
01:11:35,590 --> 01:11:36,780
of the

1076
01:11:37,360 --> 01:11:43,940
one year on what you want to have

1077
01:11:48,900 --> 01:11:52,150
it not hard

1078
01:12:08,190 --> 01:12:11,300
thank you

1079
01:12:29,260 --> 01:12:34,780
the fact that he

1080
01:12:48,740 --> 01:12:52,240
for the rest

1081
01:12:52,490 --> 01:12:58,150
who are different

1082
01:13:18,780 --> 01:13:20,760
all right well

1083
01:13:23,270 --> 01:13:29,630
o to right

1084
01:13:31,070 --> 01:13:34,280
one of the structure of

1085
01:13:41,940 --> 01:13:44,760
the problem

1086
01:13:44,760 --> 01:13:46,010
on the average IQ

1087
01:13:46,400 --> 01:13:50,300
like the other half distance so you will definitely bring them

1088
01:13:52,240 --> 01:13:58,310
it is more many more

1089
01:13:58,780 --> 01:14:02,170
is that

1090
01:14:08,430 --> 01:14:12,230
what for example what is not good it's more and more

1091
01:14:13,030 --> 01:14:15,920
is this the right so here i have degree distribution right and i was just

1092
01:14:15,920 --> 01:14:20,070
showing you that it's the power right and this is not a power right

1093
01:14:20,090 --> 01:14:25,330
so what is so the thing is people usually just like go for four they

1094
01:14:25,330 --> 01:14:29,060
will discover something and then they would try to find the model that would explain

1095
01:14:29,060 --> 01:14:35,840
that so profoundly probability distributions and they said OK preferential then they found clustering this

1096
01:14:35,860 --> 01:14:38,730
triangles and they were saying oh how can we model that all here is the

1097
01:14:38,730 --> 01:14:40,500
small world model

1098
01:14:40,590 --> 01:14:45,420
the small world model won't get you won't get to power law degree distribution so

1099
01:14:45,420 --> 01:14:48,900
if you want one if you want more difficult one the other hand the other

1100
01:14:50,030 --> 01:14:54,990
OK this is an obviously would get about something a property that is the model

1101
01:14:54,990 --> 01:14:59,410
but get to that property but there is that was very few say what if

1102
01:14:59,660 --> 01:15:03,190
i want to all of them were can i find the generated throughout the can

1103
01:15:03,290 --> 01:15:04,220
all of them

1104
01:15:04,240 --> 01:15:06,020
and this is what i show

1105
01:15:11,120 --> 01:15:16,950
three days

1106
01:15:23,490 --> 01:15:24,590
so what

1107
01:15:24,610 --> 01:15:26,540
so what is the question

1108
01:15:26,550 --> 01:15:28,290
the question is this

1109
01:15:28,300 --> 01:15:31,380
maybe try

1110
01:15:31,560 --> 01:15:34,100
because we are talking about small world one

1111
01:15:34,260 --> 01:15:37,120
otherwise there is no

1112
01:15:37,180 --> 01:15:43,520
the clustering coefficient is defined by the clustering coefficient is calculated so it should be

1113
01:15:43,520 --> 01:15:48,070
defined not it's not of the clustering coefficient is that is the right you have

1114
01:15:48,070 --> 01:15:52,550
your clustering coefficient of of what it takes to be the case right

1115
01:15:52,580 --> 01:15:55,650
should be one or

1116
01:15:55,650 --> 01:16:01,290
right to exclude the k then you want to ask how many possible triangles that

1117
01:16:01,290 --> 01:16:07,020
which means which this is the number of possible triangles right

1118
01:16:07,270 --> 01:16:10,540
and here i call i say number of triangles

1119
01:16:11,300 --> 01:16:14,500
so what what he says if i have over six

1120
01:16:14,620 --> 01:16:17,500
like this

1121
01:16:17,520 --> 01:16:21,110
and i know something like this then i say how many possible triangles are that

1122
01:16:21,310 --> 01:16:22,580
it's cages to

1123
01:16:23,220 --> 01:16:27,500
so thank need to select two this is this is the total number of one

1124
01:16:27,500 --> 01:16:32,200
thank could you could be here here here here and so on

1125
01:16:32,210 --> 01:16:36,310
and i say OK there are two up the two out of whatever

1126
01:16:36,340 --> 01:16:38,240
five just OK

1127
01:16:38,280 --> 01:16:42,340
so this is clustering coefficient and then what you can do actually i should say

1128
01:16:42,350 --> 01:16:48,000
i should say this so this is what a particular x now ck just the

1129
01:16:48,000 --> 01:16:52,670
average over all its vertices where the of that particular degree

1130
01:16:52,710 --> 01:16:56,480
and this is now you can plot this called this distribution of the clustering coefficient

1131
01:16:56,480 --> 01:16:57,760
and then if you want to see

1132
01:16:57,830 --> 01:17:02,150
this is a single c that going again average or what right so there are

1133
01:17:02,430 --> 01:17:04,790
this is this is what

1134
01:17:04,800 --> 01:17:12,310
i think the important is that we do not

1135
01:17:12,360 --> 01:17:17,990
it it is connected component

1136
01:17:18,080 --> 01:17:22,360
i don't understand

1137
01:17:24,160 --> 01:17:27,150
i mean what it says right which makes sense is if that's me and i

1138
01:17:27,150 --> 01:17:31,060
have two friends and there is a higher probability of of the of the two

1139
01:17:31,060 --> 01:17:35,290
people knowing each other right and this is something that definitely true or real for

1140
01:17:35,290 --> 01:17:38,270
social networks like let's go back

1141
01:17:40,380 --> 01:17:42,480
OK the other thing for which

1142
01:17:42,480 --> 01:17:44,340
i don't know

1143
01:17:44,370 --> 01:17:47,820
i want to go into too much detail is that

1144
01:17:47,840 --> 01:17:51,450
there are also because study spectral properties of graphs which means that you can take

1145
01:17:51,450 --> 01:17:56,980
your graph adjacency matrix right where where you have one there an edge and you

1146
01:17:56,980 --> 01:18:00,560
have zero if there's no edge right and i can do if the graph is

1147
01:18:00,560 --> 01:18:05,650
undirected and you can look like a principal component analysis or you can do SVD

1148
01:18:05,690 --> 01:18:10,350
so that you get basically get arguments and i connect and if you plot the

1149
01:18:10,350 --> 01:18:13,250
distribution of this you'll find that it's cute

1150
01:18:14,520 --> 01:18:20,960
one interpretation is that they call it the spectral gap in the spectral gap measures

1151
01:18:20,970 --> 01:18:25,400
the the difference between first and second i can value and then that you how

1152
01:18:25,400 --> 01:18:30,950
how how nice the graph with cost which means home ice communities are there in

1153
01:18:30,950 --> 01:18:33,240
the cluster OK but that's

1154
01:18:33,260 --> 01:18:36,420
let's go on

1155
01:18:36,430 --> 01:18:41,570
so for example in them so this what this is all for static graphs for

1156
01:18:41,570 --> 01:18:46,010
temporal graph for temporal patterns that are too that one is

1157
01:18:46,020 --> 01:18:49,420
basic question but you want to ask what is the relation between the number of

1158
01:18:49,420 --> 01:18:51,270
nodes and the number of edges over time

1159
01:18:51,320 --> 01:18:52,720
right and

1160
01:18:52,760 --> 01:18:55,940
right so he would say n is the number of nodes at time t is

1161
01:18:55,940 --> 01:18:59,700
the number of edges at time t five w the number of nodes with the

1162
01:18:59,700 --> 01:19:01,310
number of edges also w

1163
01:19:01,370 --> 01:19:03,420
OK and

1164
01:19:03,470 --> 01:19:05,510
it turns out it will

1165
01:19:07,900 --> 01:19:10,100
it will more than doubled and

1166
01:19:10,310 --> 01:19:14,160
how you can reason about this is if you plotted on log so here the

1167
01:19:14,160 --> 01:19:15,760
number of nodes at time t

1168
01:19:15,770 --> 01:19:20,850
number of edges at time t and you blocked both on log log scale

1169
01:19:20,900 --> 01:19:24,810
and you will see it follows pretty much like right and if the relation with

1170
01:19:24,980 --> 01:19:28,120
thing many this line should have slope one

1171
01:19:28,150 --> 01:19:33,270
if the graph is fully connected or if it's core every node has like links

1172
01:19:33,270 --> 01:19:36,800
to the prove to the constant cost

1173
01:19:38,490 --> 01:19:43,810
part of the network then this should be to right so the

1174
01:19:43,870 --> 01:19:45,510
the exponent

1175
01:19:45,510 --> 01:19:49,540
this is a this slope should be between one and two one means you have

1176
01:19:49,540 --> 01:19:52,850
the linear growth of the number of edges right here

1177
01:19:52,880 --> 01:19:57,100
or you can have quadratic growth of the number of features OK so this is

1178
01:19:57,100 --> 01:20:01,200
what what has been found for foreground that evolve over time so that they are

1179
01:20:01,200 --> 01:20:02,990
basically getting down

1180
01:20:03,070 --> 01:20:07,820
OK and so this is what i also explained that if you

1181
01:20:07,870 --> 01:20:11,300
if you have a close one then you have the growth which

1182
01:20:11,320 --> 01:20:16,770
because the average degree the in the in the graph is constant and if you

1183
01:20:16,770 --> 01:20:22,180
have any questions that you then you have growth which means that

1184
01:20:22,220 --> 01:20:24,660
you're getting ready dense graphs right

1185
01:20:24,680 --> 01:20:28,720
because everyone is like connected to the constant fraction of the of the network

1186
01:20:28,820 --> 01:20:32,160
and this i will skip

1187
01:20:32,200 --> 01:20:38,350
and the other thing that is also surprising for evolving networks is a few

1188
01:20:38,370 --> 01:20:44,270
if you ask how the distances increase of my network gets louder and

1189
01:20:44,290 --> 01:20:48,680
no conventional wisdom or results in existing models tell you i would say that this

1190
01:20:48,680 --> 01:20:53,030
is slowly increase like log n log log n and things like that so if

1191
01:20:53,280 --> 01:20:56,020
the the and the number of nodes gets larger the

1192
01:20:56,480 --> 01:20:58,920
the diameter will slowly

1193
01:20:58,930 --> 01:21:03,030
this is what happens if you if you measure these things over time right so

1194
01:21:03,030 --> 01:21:07,300
here for example this is a citation after that from nineteen ninety two to two

1195
01:21:07,300 --> 01:21:08,810
thousand three

1196
01:21:08,830 --> 01:21:14,280
and diameter decreases right now the question is so i showed you all these

1197
01:21:15,590 --> 01:21:16,720
and so

1198
01:21:17,580 --> 01:21:20,240
this is just just a short list of

1199
01:21:20,280 --> 01:21:26,190
well of types of networks where these patterns of these properties can be found so

1200
01:21:26,190 --> 01:21:31,070
we would like when you have webpages hyper links online communities would be i know

1201
01:21:31,070 --> 01:21:36,340
friends and things like that who calls who is like and from making tea

1202
01:21:36,350 --> 01:21:41,420
you have four numbers in who course home of autonomous systems that are you can

1203
01:21:41,420 --> 01:21:42,750
think of them

1204
01:21:42,750 --> 01:21:46,270
you can think of them as the out or like IBM dot com is an

1205
01:21:46,270 --> 01:21:51,310
autonomous systems and make it is an autonomous systems all this autonomous systems like like

1206
01:21:51,310 --> 01:21:56,500
about packages between each other you can do the actual out

1207
01:21:56,520 --> 01:22:00,520
for the second one you can like bipartite graph like movie stock actors you can

1208
01:22:00,520 --> 01:22:05,110
system where we have some input on this thing called and

1209
01:22:05,140 --> 01:22:07,020
the output is determined by

1210
01:22:07,030 --> 01:22:09,730
weighting the input

1211
01:22:09,750 --> 01:22:14,550
this is very similar to the situation so that's what we have is when you're

1212
01:22:16,050 --> 01:22:21,260
in the perceptron and that's what we have

1213
01:22:23,550 --> 01:22:26,720
so what i did before which i

1214
01:22:26,990 --> 01:22:28,040
an output

1215
01:22:28,080 --> 01:22:29,500
as the principal component

1216
01:22:29,510 --> 01:22:33,170
and we're looking for it to have this property so so we can

1217
01:22:33,200 --> 01:22:35,030
david so that

1218
01:22:35,070 --> 01:22:38,380
these weights actually corresponds to this this

1219
01:22:38,580 --> 01:22:42,960
then we would have no network that was producing

1220
01:22:42,980 --> 01:22:47,840
the principal components for that will be the principal component score of x

1221
01:22:47,860 --> 01:22:49,400
it is one so

1222
01:22:49,410 --> 01:22:52,510
so it's obvious if we combine that w nicholson

1223
01:22:53,040 --> 01:22:58,570
then vehicle to one this was and sort

1224
01:23:00,890 --> 01:23:02,340
what we trying to do

1225
01:23:03,230 --> 01:23:04,630
here is

1226
01:23:08,750 --> 01:23:11,380
if i do the normalizing trade i did above

1227
01:23:11,400 --> 01:23:14,130
or just justice entering the of it

1228
01:23:16,250 --> 01:23:19,880
it turns out that the variance of y

1229
01:23:20,850 --> 01:23:23,410
just the one j squares of

1230
01:23:23,430 --> 01:23:25,740
divided by minus one

1231
01:23:25,750 --> 01:23:30,220
and also onto shared the top of the tree

1232
01:23:30,240 --> 01:23:35,380
so if i want to get your network to play principal components analysis then what

1233
01:23:35,380 --> 01:23:38,090
should be doing is making the squared output

1234
01:23:38,130 --> 01:23:40,390
the y

1235
01:23:42,210 --> 01:23:46,570
it was about and what the variance to maximize so this

1236
01:23:46,590 --> 01:23:51,150
so the output must once upon a time i y two

1237
01:23:51,210 --> 01:23:57,550
that i want to have the property that this is because it could be

1238
01:23:57,570 --> 01:23:59,960
but it still has to be worked out from

1239
01:23:59,970 --> 01:24:01,860
time the white

1240
01:24:01,880 --> 01:24:05,030
so g

1241
01:24:10,380 --> 01:24:16,710
is the obvious if i just by what is to just a double w

1242
01:24:18,250 --> 01:24:22,120
this just get bigger and bigger and bigger might double trouble and so on copenhagen

1243
01:24:22,120 --> 01:24:25,740
or not because it's quantity

1244
01:24:25,750 --> 01:24:30,210
so i just the by the way if technology is replaced with rule by three

1245
01:24:31,020 --> 01:24:33,860
then that once will be big

1246
01:24:34,020 --> 01:24:36,640
just keep getting higher high so so

1247
01:24:36,660 --> 01:24:40,270
i have to do something to stop the big big enough to choose the weights

1248
01:24:40,270 --> 01:24:42,570
in of the constraints of

1249
01:24:42,580 --> 01:24:44,430
so what do is

1250
01:24:45,390 --> 01:24:46,850
is this constraint

1251
01:24:46,870 --> 01:24:51,460
make the length of w is the vector w equal to one

1252
01:24:51,480 --> 01:24:54,150
so what's the squared length

1253
01:24:54,240 --> 01:24:56,200
it's the same

1254
01:24:57,720 --> 01:25:01,400
so that we together what was saying is this

1255
01:25:03,100 --> 01:25:05,290
maximize the squared outputs

1256
01:25:05,320 --> 01:25:07,710
so you see the whites

1257
01:25:07,720 --> 01:25:13,010
mean that constraint and we will get the first from school on the output

1258
01:25:15,510 --> 01:25:19,420
i don't know how to do this i don't think that's what i do try

1259
01:25:19,450 --> 01:25:25,970
to solve this by standard optimisation methods are on the ground in particular objective function

1260
01:25:25,990 --> 01:25:27,400
it is there

1261
01:25:28,460 --> 01:25:32,790
around the plane of one minus

1262
01:25:32,820 --> 01:25:34,490
the lead to one of

1263
01:25:34,500 --> 01:25:41,050
so that's the lagrangian and then if i can differentiate the find maximum then

1264
01:25:41,070 --> 01:25:47,640
but given the right answer because because self-justification to spread self-justification to try and it

1265
01:25:47,640 --> 01:25:52,510
was great this problem so that's we do try

1266
01:25:52,530 --> 01:25:54,700
to optimise this

1267
01:25:55,510 --> 01:25:58,210
first thing do is differentiation

1268
01:25:58,770 --> 01:26:01,500
then we set the derivative sequences it

1269
01:26:01,510 --> 01:26:07,510
so if it's necessary zero it turns out one of the most people two

1270
01:26:07,610 --> 01:26:09,430
i can't see their

1271
01:26:09,450 --> 01:26:12,700
that's just after all handwaving jogging

1272
01:26:12,710 --> 01:26:15,240
it is that what you do is you multiply

1273
01:26:15,250 --> 01:26:17,400
everything in sight by the way

1274
01:26:18,260 --> 01:26:21,520
what we see from the w three c from the

1275
01:26:23,040 --> 01:26:26,570
w transpose that is o

1276
01:26:26,590 --> 01:26:28,040
so that from

1277
01:26:28,750 --> 01:26:36,180
that becomes two lambda w c w w is one of

1278
01:26:36,210 --> 01:26:38,990
so i just end with lambda is equal to

1279
01:26:39,000 --> 01:26:41,200
OK quite sure that came from

1280
01:26:41,220 --> 01:26:46,880
so again and then also this back into the ground

1281
01:26:46,890 --> 01:26:54,450
by end up with half the derivatives of the lagrangian with respect to w is

1282
01:26:58,130 --> 01:27:00,050
if you look at the

1283
01:27:00,080 --> 01:27:01,580
no network

1284
01:27:01,590 --> 01:27:03,360
things in chapter eight

1285
01:27:03,380 --> 01:27:06,800
what you find is that they

1286
01:27:06,820 --> 01:27:09,530
hebbian learning says firstly

1287
01:27:09,540 --> 01:27:13,770
taking steps changing the w

1288
01:27:13,790 --> 01:27:16,140
five steps that x

1289
01:27:16,580 --> 01:27:19,370
minus one

1290
01:27:19,380 --> 01:27:22,580
some zones so what this is doing

1291
01:27:22,600 --> 01:27:28,180
what they don't work is doing is solving this problem

1292
01:27:28,200 --> 01:27:30,360
why gradient descent

1293
01:27:30,370 --> 01:27:34,300
but both density because that's so it is just like

1294
01:27:34,330 --> 01:27:37,400
so so you taking

1295
01:27:37,420 --> 01:27:40,020
iterations that's on you

1296
01:27:40,040 --> 01:27:44,480
this is talking about the gradients and you so far down the the gradient on

1297
01:27:45,120 --> 01:27:51,270
that's three components but seem fanciful so the stuff was going this is

1298
01:27:51,280 --> 01:27:57,870
why learning is related to principal component analysis can show but first analysis is about

1299
01:27:57,870 --> 01:28:00,200
solving that problem and this

1300
01:28:00,210 --> 01:28:04,120
is a gradient descent approach to find an optimum my problem

1301
01:28:04,380 --> 01:28:09,150
so that's what people going on about one so there's a link between principal components

1302
01:28:09,150 --> 01:28:15,110
analysis and know that OK

1303
01:28:17,370 --> 01:28:25,470
transc in correspondence analysis is the same idea as principal components analysis exception is thinking

1304
01:28:25,470 --> 01:28:30,470
of your observations is the rows or columns of the table so this is this

1305
01:28:30,520 --> 01:28:36,100
classical dataset from the book by one acres

1306
01:28:36,120 --> 01:28:43,470
it's about employees in south africa my and whether this meant how many of them

1307
01:28:43,470 --> 01:28:45,180
to do so

1308
01:28:45,230 --> 01:28:51,640
the idea in this is that we can simplify well

1309
01:28:51,650 --> 01:28:55,270
so far this instead of having five

1310
01:28:55,270 --> 01:28:56,200
all four

1311
01:28:56,220 --> 01:29:00,300
dimensions which are way you think about it i was being then you can simplify

1312
01:29:00,300 --> 01:29:03,230
into to one or two and

1313
01:29:03,240 --> 01:29:04,350
balls and two

1314
01:29:04,350 --> 01:29:07,400
the transformation of

1315
01:29:08,650 --> 01:29:10,110
and then

1316
01:29:10,130 --> 01:29:17,420
essentially just a principal component analysis fact mathematically doing principal component analysis of this so

1317
01:29:17,420 --> 01:29:21,440
which is in this for the memory free in this position he plays often deal

1318
01:29:21,440 --> 01:29:22,900
with places

1319
01:29:24,550 --> 01:29:26,110
and you can show that

1320
01:29:27,200 --> 01:29:29,230
the idea to come back here

1321
01:29:30,620 --> 01:29:32,940
o to come back here

1322
01:29:35,970 --> 01:29:39,850
the priority you always the priority zero

1323
01:29:39,860 --> 01:29:43,050
asian is the in the game matches the parity

1324
01:29:44,570 --> 01:29:46,350
of the state

1325
01:29:47,940 --> 01:29:50,140
you in so here

1326
01:29:50,150 --> 01:29:52,240
top thirty zero

1327
01:29:52,260 --> 01:29:57,460
he's qq had pretty easy and so on so just one of a number of

1328
01:29:57,460 --> 01:29:58,530
what concerns

1329
01:30:03,130 --> 01:30:09,440
possible transit transitions the end the game can be quite big it has

1330
01:30:13,450 --> 01:30:17,350
the possible labels a b that you can degrade with so you can think of

1331
01:30:17,470 --> 01:30:19,930
the size

1332
01:30:22,500 --> 01:30:26,130
there is that the

1333
01:30:27,550 --> 01:30:29,600
as i can

1334
01:30:29,630 --> 01:30:31,850
the winning strategy in this game

1335
01:30:33,240 --> 01:30:36,530
it means that whatever and tree

1336
01:30:37,630 --> 01:30:39,570
the trust will make

1337
01:30:39,600 --> 01:30:41,430
as the player zero

1338
01:30:41,450 --> 01:30:46,500
if i mean be i would choose the triangle which corresponds to one position with

1339
01:30:50,070 --> 01:30:53,750
it depends on the on the kernel in

1340
01:30:53,770 --> 01:30:55,300
so it make

1341
01:30:56,350 --> 01:31:00,070
the choice of the same triangle here

1342
01:31:00,240 --> 01:31:05,480
whatever and the tree

1343
01:31:05,500 --> 01:31:08,800
provided response to

1344
01:31:09,860 --> 01:31:13,330
i mean position in the game which is the same

1345
01:31:14,260 --> 01:31:18,910
so if you think of what i mean what is the consequence of this

1346
01:31:18,920 --> 01:31:20,120
so i think

1347
01:31:21,100 --> 01:31:22,540
similar choice

1348
01:31:22,550 --> 01:31:24,620
what i am in the tree

1349
01:31:24,620 --> 01:31:26,920
well i mean which two

1350
01:31:27,930 --> 01:31:33,870
i mean the current state of my automata which is the same transition from n

1351
01:31:34,940 --> 01:31:39,810
it may be true to what it means to be here is regular

1352
01:31:40,820 --> 01:31:45,000
so it means that the but the but this entry here

1353
01:31:46,100 --> 01:31:49,220
and the subtree here

1354
01:31:49,280 --> 01:31:52,250
this is isomorphic

1355
01:31:53,290 --> 01:31:54,640
same because

1356
01:31:54,640 --> 01:31:58,950
i mean QA always true because of one and q eight a shows i mean

1357
01:31:58,960 --> 01:32:02,290
the transition that i showed you here

1358
01:32:04,320 --> 01:32:06,260
and here are editors here so

1359
01:32:06,270 --> 01:32:08,670
that's what i've chosen here

1360
01:32:09,680 --> 01:32:13,830
if i change the values of course and and if i come back to here

1361
01:32:13,830 --> 01:32:18,540
i mean that we should and here i was which was the same transition CNT

1362
01:32:18,540 --> 01:32:20,950
here so the trophic

1363
01:32:22,790 --> 01:32:25,300
so what does that mean it means that

1364
01:32:25,320 --> 01:32:29,320
i can exhibit the tree which is the true is regular again represented with a

1365
01:32:29,320 --> 01:32:31,270
finite state object

1366
01:32:32,280 --> 01:32:34,180
i mean is that

1367
01:32:35,170 --> 01:32:36,870
so this is the claim here

1368
01:32:37,060 --> 01:32:38,470
when i say that

1369
01:32:38,540 --> 01:32:40,850
on top of the head the model

1370
01:32:41,950 --> 01:32:44,750
it means that if players he has a winning strategy

1371
01:32:44,780 --> 01:32:46,440
which is

1372
01:32:47,400 --> 01:32:51,440
you can also enforce the strategy to be memoryless

1373
01:32:52,700 --> 01:32:55,100
because you always have minor strategy

1374
01:32:55,110 --> 01:32:56,140
if you have

1375
01:32:56,150 --> 01:32:59,930
there were less winning strategy stages which means that you can build

1376
01:33:02,140 --> 01:33:07,580
model which is regular regularity in finite mathematics is the same

1377
01:33:08,960 --> 01:33:12,980
OK it's i skip but as you can imagine

1378
01:33:14,070 --> 01:33:15,370
i skipped because

1379
01:33:15,370 --> 01:33:19,920
they transform it into this input three transformation is polynomial

1380
01:33:20,970 --> 01:33:24,940
you build get the size of a prime and you saw the game

1381
01:33:26,250 --> 01:33:30,580
and solving the game is so this is the price to pay

1382
01:33:31,880 --> 01:33:36,270
to cheque non emptiness is the same price as the one you have to pay

1383
01:33:36,270 --> 01:33:38,610
to solve parity game

1384
01:33:39,940 --> 01:33:42,010
the parity games it's not really

1385
01:33:43,430 --> 01:33:45,660
very expensive is putting up

1386
01:33:45,680 --> 01:33:49,710
right so now membership

1387
01:33:50,910 --> 01:33:53,020
membership is open

1388
01:33:54,370 --> 01:33:56,050
the game is very easy

1389
01:33:56,110 --> 01:34:01,450
i show you

1390
01:34:03,030 --> 01:34:05,310
it's so i try to explain it

1391
01:34:06,750 --> 01:34:08,480
so what is given

1392
01:34:08,500 --> 01:34:10,130
four membership

1393
01:34:10,170 --> 01:34:11,190
the tree

1394
01:34:20,610 --> 01:34:26,510
and they give you a lot about the security of the math

1395
01:34:28,110 --> 01:34:29,680
so what

1396
01:34:31,060 --> 01:34:34,880
is that i would be

1397
01:34:36,210 --> 01:34:40,810
you're more than when i started with

1398
01:34:42,190 --> 01:34:43,900
the input tree t

1399
01:34:43,920 --> 01:34:46,460
and on the other side grow

1400
01:34:46,470 --> 01:34:50,130
of a

1401
01:34:51,330 --> 01:34:54,830
so what i did was that edition relations telling me look

1402
01:34:56,080 --> 01:34:58,710
when when i mean the node which is w

1403
01:34:58,780 --> 01:35:00,690
address the tree

1404
01:35:01,910 --> 01:35:04,790
in the binary tree when i mean no w

1405
01:35:04,810 --> 01:35:06,460
consider what

1406
01:35:07,710 --> 01:35:10,400
i consider

1407
01:35:10,430 --> 01:35:11,400
the late

1408
01:35:12,760 --> 01:35:14,700
in the state

1409
01:35:14,700 --> 01:35:19,010
and there are two problems with the first one is the node labeling right so

1410
01:35:19,010 --> 01:35:20,550
he if i have to grow

1411
01:35:20,550 --> 01:35:25,090
right they are the same i just labeled nodes different right so here i get

1412
01:35:25,090 --> 01:35:28,510
this is not number one two three four or five this way

1413
01:35:28,570 --> 01:35:31,950
OK i get two different ideas instrumentation

1414
01:35:32,030 --> 01:35:35,860
OK and now if i would just go and all that

1415
01:35:35,880 --> 01:35:37,680
this would be different in general

1416
01:35:38,450 --> 01:35:41,650
so this is one problem so i don't know

1417
01:35:41,670 --> 01:35:45,450
i don't know the node label right i don't know how to map the nodes

1418
01:35:45,450 --> 01:35:49,680
of this graph to the or how to make this adjacency matrix this adjacency matrix

1419
01:35:49,860 --> 01:35:53,890
so what i need to do in principle idea to consider or all node labelings

1420
01:35:53,890 --> 01:35:59,030
sigma OK so she might just the permutation basic how to label how to label

1421
01:35:59,030 --> 01:36:01,650
these nodes so that i can then

1422
01:36:01,680 --> 01:36:06,360
take this matrix they take that one can calculate that equation shown before OK so

1423
01:36:06,360 --> 01:36:09,240
the problem here is so what i would like to do is i would like

1424
01:36:09,240 --> 01:36:13,130
to average over all over all labelings

1425
01:36:13,150 --> 01:36:14,050
i mean

1426
01:36:14,150 --> 01:36:18,510
the number of permutations is like and victorian right so this will explode for more

1427
01:36:18,510 --> 01:36:22,780
than a graph of final note if you like and

1428
01:36:22,780 --> 01:36:27,530
the other assumption i'm making is that all labelings are a priori have the same

1429
01:36:27,530 --> 01:36:32,130
probability that they just something that comes at the end when the graph is created

1430
01:36:32,340 --> 01:36:35,200
and what they believe that is how to map

1431
01:36:35,240 --> 01:36:37,280
the rows and columns of here

1432
01:36:37,300 --> 01:36:40,680
two here so in principle

1433
01:36:40,680 --> 01:36:42,550
this is very bad news

1434
01:36:42,570 --> 01:36:45,360
OK the other

1435
01:36:45,380 --> 01:36:49,470
so let's see now that right so what i really calculating this very probability of

1436
01:36:49,470 --> 01:36:54,990
a graph the parameters that should be say sorry in this this is some notation

1437
01:36:55,010 --> 01:36:59,030
somebody told me how to do this map and right now i'm going to go

1438
01:36:59,030 --> 01:37:03,880
ahead just but i'm i'm i'm i know them around so this means that in

1439
01:37:03,900 --> 01:37:09,400
the u the the element that position you of this this permutation this work this

1440
01:37:09,400 --> 01:37:13,380
mapping and this is what i'm doing right and

1441
01:37:13,400 --> 01:37:18,300
the one problem with this equation is this again takes and and time ordered and

1442
01:37:18,300 --> 01:37:20,700
square because they have to go into the woods

1443
01:37:20,700 --> 01:37:23,220
every cell of this of this matrix

1444
01:37:23,340 --> 01:37:27,010
so i have to go so this this is the adjacency matrix of a graph

1445
01:37:27,010 --> 01:37:29,900
right and the adjacency matrix of a graph so what i need to do is

1446
01:37:29,900 --> 01:37:34,220
now the every cell of this matrix which means if i have a graph of

1447
01:37:34,220 --> 01:37:38,820
final size one hundred thousand this would never fit into memory or

1448
01:37:38,840 --> 01:37:41,030
the complexity of just evaluating

1449
01:37:41,050 --> 01:37:45,650
evaluating the probability for us in the data and the single mutation

1450
01:37:45,670 --> 01:37:48,240
we take and square brackets never

1451
01:37:49,300 --> 01:37:51,780
i mean they never able to calculate

1452
01:37:53,880 --> 01:37:57,200
so i can solutions for both OK this is good news so basically the bad

1453
01:37:57,200 --> 01:38:00,380
news is that if you need to create is then you have to average over

1454
01:38:00,380 --> 01:38:02,680
all permutations and you have to

1455
01:38:02,680 --> 01:38:07,380
the police every cell of your graph adjacency matrix or get the complexity would be

1456
01:38:07,380 --> 01:38:09,220
like in victoria land

1457
01:38:09,220 --> 01:38:12,030
and worst right and on

1458
01:38:12,050 --> 01:38:15,840
we can do this in linear time so we all in time for the right

1459
01:38:16,700 --> 01:38:19,130
time proportional to the number of features and there are two

1460
01:38:19,130 --> 01:38:24,950
two things will be using that we won't be able consider

1461
01:38:24,970 --> 01:38:32,130
all labelings but we do some like statistical simulation techniques like MCMC to sample permutations

1462
01:38:32,130 --> 01:38:35,320
from this distribution rights all now i know i think the graph and i think

1463
01:38:35,340 --> 01:38:39,670
the parameters and i want to say to sample

1464
01:38:39,680 --> 01:38:45,520
distribution are mutations of these so how you can think about this is that once

1465
01:38:45,520 --> 01:38:47,070
i think the graph in the

1466
01:38:47,090 --> 01:38:52,610
so let us be like the space of all permutations that so here you can

1467
01:38:52,610 --> 01:38:58,470
like think of them like i love being in some some lexical lexicographical order something

1468
01:38:58,470 --> 01:39:01,280
that so here is the permutation that like one two three four

1469
01:39:01,280 --> 01:39:04,610
and he's like and minus one and so

1470
01:39:05,860 --> 01:39:10,490
you can you think the distribution of the difference about some limitations that would that

1471
01:39:11,450 --> 01:39:15,450
but there was no really good and that would be a lot of bad permutations

1472
01:39:15,450 --> 01:39:19,570
so what we you want to do is we want to sample permutations from distribution

1473
01:39:21,030 --> 01:39:24,260
what we hope is that if we were to take a lot of samples

1474
01:39:24,280 --> 01:39:27,860
it won't have to consider everyone

1475
01:39:27,860 --> 01:39:32,990
but just sample many many times and this one and the other thing that using

1476
01:39:33,720 --> 01:39:37,880
since real graphs are sparse what i mean by that is that

1477
01:39:38,170 --> 01:39:41,470
right so i have to use the number of edges n is the number of

1478
01:39:41,470 --> 01:39:42,990
nodes in

1479
01:39:43,030 --> 01:39:46,400
in one was right in the worst case would

1480
01:39:46,400 --> 01:39:47,820
he could go like

1481
01:39:47,860 --> 01:39:50,630
and square right so is the number of cells

1482
01:39:51,360 --> 01:39:56,530
and these grows quadratically with which is the size of the matrix

1483
01:39:56,590 --> 01:39:59,930
but the the real graphs are sparse which means that majority

1484
01:39:59,950 --> 01:40:04,470
all elements in the in the matrix which is you know so what i can

1485
01:40:04,470 --> 01:40:09,950
do is i can quickly calculate probability that the graph is empty but the the

1486
01:40:09,950 --> 01:40:14,170
parameters i did not observe not even a single edge and now only going to

1487
01:40:14,170 --> 01:40:18,090
correct this for the edges that actually observed that we should be using

1488
01:40:18,090 --> 01:40:22,550
and now and this and this will now so creating this

1489
01:40:22,570 --> 01:40:24,450
and using some approximation will take

1490
01:40:24,820 --> 01:40:29,300
constantine so i will only go and have to correct

1491
01:40:29,320 --> 01:40:34,400
this probability for the edges that that really then OK now i'm going to be

1492
01:40:37,320 --> 01:40:41,930
what so what you really doing but

1493
01:40:41,950 --> 01:40:43,360
is we do

1494
01:40:43,380 --> 01:40:46,150
we've been doing gradient descent right so i want to have

1495
01:40:46,170 --> 01:40:50,650
the even q of the log likelihood with respect to my parameters to be some

1496
01:40:50,650 --> 01:40:56,470
kind of matrix of the which for every parameter so for every element of my

1497
01:40:56,470 --> 01:40:59,590
middle initial matrix and to calculate this

1498
01:40:59,650 --> 01:41:03,740
if you are to be clever you get something you get an expression that i

1499
01:41:03,740 --> 01:41:04,880
saw him again

1500
01:41:04,930 --> 01:41:07,510
averaging over all the permutations but it says

1501
01:41:07,530 --> 01:41:09,650
here is the probability of a permutation

1502
01:41:09,650 --> 01:41:10,950
and i think

1503
01:41:11,010 --> 01:41:12,340
what's have

1504
01:41:13,570 --> 01:41:15,030
i think i think the

1505
01:41:15,050 --> 01:41:19,950
the derivative of the likelihood but what is convenient now here is that i can

1506
01:41:19,950 --> 01:41:21,150
sample these

1507
01:41:21,150 --> 01:41:23,110
i can then evaluate these changes

1508
01:41:23,150 --> 01:41:26,410
output output which OK

1509
01:41:29,700 --> 01:41:35,340
so here here's here's what you're doing great so you want to

1510
01:41:35,360 --> 01:41:40,240
we want to sample permutations from distribution and what we are doing this

1511
01:41:40,260 --> 01:41:46,720
we started something with some limitations right let's just want to end and then the

1512
01:41:46,720 --> 01:41:48,510
two supporting this limitation

1513
01:41:48,550 --> 01:41:51,400
we solve the elements OK now

1514
01:41:51,410 --> 01:41:56,470
what i'm doing is just evaluating the ratio right so this this says if the

1515
01:41:58,090 --> 01:42:01,090
if the new one so this is the new to the new permutation i got

1516
01:42:01,090 --> 01:42:14,610
when we have

1517
01:42:16,410 --> 01:42:20,790
going through a wire like so

1518
01:42:20,900 --> 01:42:29,920
and we look at the magnetic field in the vicinity of the wire

1519
01:42:29,920 --> 01:42:32,310
and we know from experiment

1520
01:42:32,310 --> 01:42:34,580
if you put pieces of magnetite

1521
01:42:34,610 --> 01:42:35,960
round wire

1522
01:42:35,970 --> 01:42:37,440
they line up

1523
01:42:37,460 --> 01:42:39,280
in a circle

1524
01:42:39,330 --> 01:42:43,220
around like this

1525
01:42:43,230 --> 01:42:44,640
that circle

1526
01:42:44,680 --> 01:42:47,260
radius is are

1527
01:42:47,320 --> 01:42:49,030
then the magnetic field

1528
01:42:49,040 --> 01:42:51,220
that's an experimental fact

1529
01:42:51,260 --> 01:42:53,180
is proportional to the current

1530
01:42:53,220 --> 01:42:55,530
i is inversely proportional

1531
01:42:55,540 --> 01:42:59,140
with the radius of that circle

1532
01:42:59,150 --> 01:43:00,580
by convention

1533
01:43:01,360 --> 01:43:04,710
direction of the magnetic fields

1534
01:43:04,720 --> 01:43:07,690
given by the right-hand corkscrew

1535
01:43:07,730 --> 01:43:09,190
i rotate this way

1536
01:43:09,250 --> 01:43:13,120
current goes up

1537
01:43:13,170 --> 01:43:14,560
you've seen before

1538
01:43:14,560 --> 01:43:16,810
which electric charges

1539
01:43:16,860 --> 01:43:21,530
when you have a a wire which is uniformly distributed

1540
01:43:21,580 --> 01:43:23,450
so was positive charge

1541
01:43:23,500 --> 01:43:24,980
you've also seen

1542
01:43:25,120 --> 01:43:28,690
electric fields in the vicinity of that straight wire

1543
01:43:28,700 --> 01:43:30,730
four of us one of them are

1544
01:43:30,770 --> 01:43:33,420
for the direction is different than the magnetic field

1545
01:43:33,530 --> 01:43:36,090
but also falls off as one of or

1546
01:43:36,090 --> 01:43:38,260
and the reason is that

1547
01:43:38,310 --> 01:43:42,060
electric monopole individual charges

1548
01:43:42,090 --> 01:43:45,590
the electric field falls off as one of four square

1549
01:43:45,640 --> 01:43:51,090
so when you integrate out over straight you get the one of our fields

1550
01:43:51,110 --> 01:43:53,280
so by analogy

1551
01:43:53,290 --> 01:43:55,450
it would be it's

1552
01:43:55,450 --> 01:43:56,980
very plausible

1553
01:43:57,170 --> 01:43:59,620
if you took magnetic monopole

1554
01:43:59,670 --> 01:44:03,720
that the magnetic field would also fall off is one of four square

1555
01:44:03,730 --> 01:44:07,790
but the magnetic monopole as far as we know don't exist

1556
01:44:07,810 --> 01:44:10,660
in principle they could exist but we've never seen one

1557
01:44:10,690 --> 01:44:13,280
and if anyone of you ever find one

1558
01:44:13,300 --> 01:44:15,000
that would certainly be

1559
01:44:15,050 --> 01:44:18,840
the nobel prize it's by no means impossible

1560
01:44:18,870 --> 01:44:20,930
so the simple fact that the

1561
01:44:20,970 --> 01:44:22,930
the magnetic fields

1562
01:44:23,020 --> 01:44:25,500
around the current of all office

1563
01:44:25,530 --> 01:44:28,190
one of four our

1564
01:44:28,190 --> 01:44:30,030
it sort of suggests

1565
01:44:31,060 --> 01:44:32,490
if you carve this

1566
01:44:32,500 --> 01:44:36,100
why rob in the elements dl

1567
01:44:36,150 --> 01:44:40,520
at each one of those elements contributes to the magnetic fields

1568
01:44:40,590 --> 01:44:43,280
inverse are scramble

1569
01:44:43,320 --> 01:44:45,910
by integrating out over the whole wire

1570
01:44:45,930 --> 01:44:49,000
you don't get to one of our field

1571
01:44:49,020 --> 01:44:51,020
and this is behind the ideas

1572
01:44:52,320 --> 01:44:54,100
the formalism by the

1573
01:44:54,120 --> 01:44:56,090
so far

1574
01:44:56,120 --> 01:44:57,930
we introduce the idea here

1575
01:44:57,930 --> 01:45:03,740
if you have a little current element dl

1576
01:45:03,780 --> 01:45:06,770
the current is in this direction

1577
01:45:06,810 --> 01:45:08,120
and you want to know

1578
01:45:08,130 --> 01:45:13,560
what the magnetic field is a small contribution be

1579
01:45:13,620 --> 01:45:14,470
that little

1580
01:45:14,490 --> 01:45:16,490
current elements

1581
01:45:16,530 --> 01:45:19,430
and the distances are

1582
01:45:19,440 --> 01:45:21,400
and the unit vector

1583
01:45:21,460 --> 01:45:25,870
from the element bl to the point where you want to know the magnetic field

1584
01:45:25,880 --> 01:45:28,850
is i rules

1585
01:45:30,350 --> 01:45:33,070
the idea was that the b

1586
01:45:33,090 --> 01:45:38,530
current little bit of magnetic fields in this case it will be in the blackboards

1587
01:45:38,560 --> 01:45:41,090
because of the right hand corkscrew rule

1588
01:45:41,150 --> 01:45:43,940
currently in this direction so these elements

1589
01:45:43,960 --> 01:45:48,280
contribute to magnetic field in this direction perpendicular to the blackboard

1590
01:45:48,310 --> 01:45:50,780
is some constant

1591
01:45:50,810 --> 01:45:52,190
proportional to

1592
01:45:52,280 --> 01:45:54,620
current no doubt

1593
01:45:54,650 --> 01:45:55,590
and then

1594
01:45:55,620 --> 01:45:59,350
is proportional to the length of the element dl

1595
01:45:59,400 --> 01:46:00,960
if it's longer than

1596
01:46:01,020 --> 01:46:02,970
that you is larger

1597
01:46:03,030 --> 01:46:07,000
in order to get the direction right perpendicular to blackboard

1598
01:46:07,050 --> 01:46:08,780
we take the cross product

1599
01:46:08,820 --> 01:46:10,740
there is a unit vector are

1600
01:46:10,750 --> 01:46:12,220
the unit vector are

1601
01:46:12,220 --> 01:46:14,460
has length one

1602
01:46:14,500 --> 01:46:18,190
so you only do that in order to get the direction

1603
01:46:18,240 --> 01:46:19,410
and this

1604
01:46:19,460 --> 01:46:24,500
and that is inversely proportional to our script of course

1605
01:46:24,550 --> 01:46:27,160
and this is

1606
01:46:27,200 --> 01:46:31,080
the formalism by biau survive

1607
01:46:31,120 --> 01:46:33,080
and you can do experiments

1608
01:46:33,100 --> 01:46:38,000
and measure the magnetic fields in the vicinity of wires

1609
01:46:38,050 --> 01:46:40,180
in this formalism or works

1610
01:46:40,230 --> 01:46:45,140
so you can calculate the individual contributions of all these elements l

1611
01:46:45,160 --> 01:46:46,310
and then you

1612
01:46:46,430 --> 01:46:48,610
and integration

1613
01:46:48,660 --> 01:46:50,240
and this formalism works

1614
01:46:50,250 --> 01:46:52,120
you can then also

1615
01:46:52,140 --> 01:46:54,100
national c is

1616
01:46:54,180 --> 01:46:56,520
and as i units

1617
01:46:56,560 --> 01:46:58,930
these tend to remind seven

1618
01:46:59,100 --> 01:47:01,910
but we write for c

1619
01:47:01,930 --> 01:47:03,700
something quite peculiar

1620
01:47:03,710 --> 01:47:05,060
we write for c

1621
01:47:05,070 --> 01:47:08,540
museo divided by four by

1622
01:47:08,550 --> 01:47:10,310
we call these new zero

1623
01:47:10,420 --> 01:47:13,020
the permeability of free space

1624
01:47:13,040 --> 01:47:14,310
seen earlier

1625
01:47:14,310 --> 01:47:16,140
the school law

1626
01:47:16,200 --> 01:47:20,300
that this constant nine times ten to the ninth we call that one of four

1627
01:47:20,300 --> 01:47:23,060
pi actually non-zero what's in a name

1628
01:47:23,100 --> 01:47:24,690
so here

1629
01:47:24,700 --> 01:47:26,620
we call these museo

1630
01:47:26,620 --> 01:47:28,520
divided by

1631
01:47:28,610 --> 01:47:31,410
four pi

1632
01:47:31,430 --> 01:47:32,680
so now you can

1633
01:47:32,690 --> 01:47:35,830
apply the also fires law

1634
01:47:35,890 --> 01:47:41,420
and you can go to straight wire

1635
01:47:41,460 --> 01:47:43,600
you have a current i

1636
01:47:43,760 --> 01:47:45,430
suppose you want to know what to

1637
01:47:45,440 --> 01:47:48,850
magnetic field at that location is

1638
01:47:48,870 --> 01:47:51,810
and this in capitol are

1639
01:47:51,870 --> 01:47:53,920
so what you now have to do

1640
01:47:53,960 --> 01:47:56,310
if you carbon is up

1641
01:47:56,360 --> 01:47:59,810
in an infinite number of small

1642
01:47:59,820 --> 01:48:02,060
elements dl

1643
01:48:02,060 --> 01:48:03,360
and this

1644
01:48:03,440 --> 01:48:05,050
distances are

1645
01:48:06,170 --> 01:48:08,570
and the unit vector

1646
01:48:08,810 --> 01:48:10,680
like so

1647
01:48:10,730 --> 01:48:13,540
and you calculate the small amount of

1648
01:48:13,580 --> 01:48:15,880
semantic field due to this little element

1649
01:48:15,910 --> 01:48:19,110
you integrate is over the whole why mathematics

1650
01:48:19,160 --> 01:48:21,750
you've done it you've done it before

1651
01:48:21,800 --> 01:48:24,240
where we have uniformly

1652
01:48:24,290 --> 01:48:26,700
electric charge on the wire

1653
01:48:26,750 --> 01:48:30,440
so i'm not going to do this again for you very straightforward

1654
01:48:30,520 --> 01:48:32,000
is of mathematics

1655
01:48:32,010 --> 01:48:34,000
the magnetic field by the way

1656
01:48:34,040 --> 01:48:36,600
in this case will come out of the blackboard

1657
01:48:36,750 --> 01:48:39,040
the right-hand court school

1658
01:48:39,110 --> 01:48:41,260
what you find when you do this

1659
01:48:41,370 --> 01:48:44,110
you will find that the

1660
01:48:44,320 --> 01:48:45,910
was new zero

1661
01:48:45,920 --> 01:48:47,800
times i

1662
01:48:47,830 --> 01:48:50,700
divided by two pi are

1663
01:48:50,750 --> 01:48:52,070
this being are

1664
01:48:52,120 --> 01:48:54,710
so you need to see that the

1665
01:48:54,730 --> 01:48:57,230
inverse one of our

1666
01:48:57,270 --> 01:48:59,180
comes out

1667
01:48:59,210 --> 01:49:02,500
and so if you for instance take a radius

1668
01:49:04,690 --> 01:49:06,620
o point one metres

1669
01:49:06,630 --> 01:49:07,930
ten centimetres

1670
01:49:07,980 --> 01:49:09,880
and you have the current

1671
01:49:09,930 --> 01:49:12,750
why over one hundred and years

1672
01:49:14,020 --> 01:49:16,210
you would end up with b field

1673
01:49:16,320 --> 01:49:17,620
uses equations

1674
01:49:17,620 --> 01:49:22,650
OK so the similarity function so forgive sort of data given by documents i might

1675
01:49:22,660 --> 01:49:25,240
want them different similarity function and you

1676
01:49:25,250 --> 01:49:28,410
my one if we have different notions of the way

1677
01:49:28,420 --> 01:49:30,960
cluster numbers how u

1678
01:49:30,970 --> 01:49:35,120
and then given a certain property you what kind of algorithms that suggest that that's

1679
01:49:35,140 --> 01:49:37,740
the kind of question we're going to be looking at

1680
01:49:37,760 --> 01:49:42,140
so let's start with something simple here which is supposed by similarity function has the

1681
01:49:42,140 --> 01:49:43,960
the property that for any two

1682
01:49:43,970 --> 01:49:47,950
documents of the same topic there are similarities bigger than zero

1683
01:49:47,960 --> 01:49:53,070
and and for any two are topic document to different topic this similarity is less

1684
01:49:53,070 --> 01:49:54,450
than zero

1685
01:49:54,500 --> 01:49:58,110
i claim this to be a really easy thing to do

1686
01:49:58,160 --> 01:50:00,590
you give me you give me that

1687
01:50:00,630 --> 01:50:01,720
my algorithm

1688
01:50:01,730 --> 01:50:02,600
i came

1689
01:50:02,610 --> 01:50:07,140
what would you do your an algorithm given a similarity function with this property

1690
01:50:07,200 --> 01:50:08,690
when you do

1691
01:50:08,710 --> 01:50:18,910
it's just a character who thinks and acts although either begins zero that that's its

1692
01:50:19,880 --> 01:50:25,810
the side pick another x although i think that cluster very

1693
01:50:25,820 --> 01:50:27,610
right that's a bit much

1694
01:50:29,370 --> 01:50:34,810
unfortunately one problem is that we have no labeled data we get the following issue

1695
01:50:34,860 --> 01:50:38,980
the of its k clustering easy so let's try to weaken this little bit will

1696
01:50:38,980 --> 01:50:41,500
see something interesting happens

1697
01:50:41,970 --> 01:50:44,130
and then we have to deal with this thing in order to be able to

1698
01:50:44,130 --> 01:50:46,660
develop an interest in the theory

1699
01:50:46,670 --> 01:50:49,830
so i when we can just a little bit in the following way let's imagine

1700
01:50:49,830 --> 01:50:55,370
a similarity function has the property that all the examples are strictly more similar to

1701
01:50:55,390 --> 01:50:56,610
all the

1702
01:50:56,610 --> 01:51:01,390
other examples of their own topic then any example of the other

1703
01:51:01,440 --> 01:51:03,370
so all documents

1704
01:51:03,410 --> 01:51:09,330
are strictly more similar to other documents of their topic than any document so before

1705
01:51:09,330 --> 01:51:10,590
we had

1706
01:51:10,630 --> 01:51:14,910
zero as a fixed number bigger than year for your own last year for the

1707
01:51:14,920 --> 01:51:16,900
health now realize that fly

1708
01:51:16,910 --> 01:51:21,780
for any given example x if you're just sort all the others by similarity or

1709
01:51:21,790 --> 01:51:26,510
saying is that the more similar ones come first meets are the course sort we

1710
01:51:26,510 --> 01:51:30,160
sort them by similarity the ones in its own topic come first and then the

1711
01:51:30,160 --> 01:51:33,060
ones of the different topic come with that these

1712
01:51:33,110 --> 01:51:36,020
but still pretty strong condition

1713
01:51:36,100 --> 01:51:39,200
unfortunately the following thing can happen

1714
01:51:40,990 --> 01:51:45,780
there's a very strong condition but here's something could happen so

1715
01:51:45,830 --> 01:51:47,600
this is again USA century

1716
01:51:47,710 --> 01:51:54,440
in terms of the sports news but OK so the same similarity function could satisfy

1717
01:51:54,440 --> 01:51:59,400
this condition for two very different clusterings of the same for this is an example

1718
01:51:59,400 --> 01:52:04,540
imagine a document about four topics your baseball basketball math and physics

1719
01:52:04,600 --> 01:52:09,470
and let's imagine similarity function is some reasonable thing so that baseball documents are all

1720
01:52:09,470 --> 01:52:13,100
very similar to each other basketball is are all similar to each other the map

1721
01:52:13,110 --> 01:52:17,440
for each of the connection with each other and there's medium similarity between baseball and

1722
01:52:17,440 --> 01:52:23,400
basketball is medium similarity between metaphysics very low similarity

1723
01:52:23,650 --> 01:52:28,750
you have a physics based about it follows similarity

1724
01:52:28,760 --> 01:52:32,750
that should be good enough right big the problem is

1725
01:52:32,760 --> 01:52:35,490
the given information

1726
01:52:35,540 --> 01:52:39,720
it could be it could be that the clustering the person is thinking

1727
01:52:39,730 --> 01:52:43,310
it is in fact the clustering baseball basketball math physics that would be very nice

1728
01:52:43,310 --> 01:52:45,590
natural for clusters clustering

1729
01:52:45,600 --> 01:52:47,140
but it could also be

1730
01:52:47,190 --> 01:52:50,420
that the clustering the person is thinking of is that you know and i really

1731
01:52:50,420 --> 01:52:51,810
into sports

1732
01:52:51,830 --> 01:52:54,860
sports documents have math physics

1733
01:52:54,870 --> 01:53:00,350
so maybe the right answer is three clusters is mapped the physics in this work

1734
01:53:00,910 --> 01:53:01,710
or are

1735
01:53:01,710 --> 01:53:03,850
maybe the right answer

1736
01:53:03,870 --> 01:53:07,600
if you ask somebody outside of this room is the other way

1737
01:53:07,610 --> 01:53:10,220
the right answer is baseball

1738
01:53:10,220 --> 01:53:17,220
basketball and stuff with like we are the math symbols and

1739
01:53:17,280 --> 01:53:21,100
that's also a perfectly reasonable clustering and

1740
01:53:21,150 --> 01:53:22,730
the problem is

1741
01:53:24,470 --> 01:53:28,730
the same similarity functions satisfy

1742
01:53:28,780 --> 01:53:30,650
this property

1743
01:53:30,710 --> 01:53:34,120
all x are more similar to y in their own cluster than any

1744
01:53:34,140 --> 01:53:35,870
why primary other cluster

1745
01:53:35,880 --> 01:53:37,360
four above

1746
01:53:37,360 --> 01:53:39,210
this and this

1747
01:53:39,230 --> 01:53:41,100
OK so for instance here

1748
01:53:42,170 --> 01:53:46,980
here is strictly is more similar everybody in their own cluster than anybody

1749
01:53:47,060 --> 01:53:49,900
and these guys are more similar to

1750
01:53:50,110 --> 01:53:53,610
right so satisfies the properties so maybe they are thinking of the question

1751
01:53:53,660 --> 01:53:58,200
or maybe they are thinking of this clustering and they're both perfectly consistent with this

1752
01:53:58,200 --> 01:54:01,840
property so have my arguments will be able to figure out

1753
01:54:01,870 --> 01:54:04,420
so you have to somehow deal with

1754
01:54:04,610 --> 01:54:05,910
and so

1755
01:54:06,030 --> 01:54:10,070
there are several things one can do with the way women purpose of dealing with

1756
01:54:11,170 --> 01:54:16,170
is that so we can't even tell what someone this week and the goal

1757
01:54:16,440 --> 01:54:20,870
so how do we deal with we change the problem so so but we do

1758
01:54:20,870 --> 01:54:24,220
this in a way that i think is very weak and the goal building

1759
01:54:24,250 --> 01:54:28,830
so let's say that our algorithm rather than actually opening cost

1760
01:54:28,870 --> 01:54:31,980
it's allowed to produce a hierarchy

1761
01:54:33,790 --> 01:54:38,120
OK such that the correct answer is approximately some pruning of the tree

1762
01:54:38,150 --> 01:54:41,070
so what i mean is given this example

1763
01:54:41,100 --> 01:54:46,220
our algorithm could output the following three we could say that high-level is one big

1764
01:54:46,220 --> 01:54:51,980
cluster like this called dark and then i would support for society and also with

1765
01:54:51,980 --> 01:54:57,320
support from baseball basketball for science and the math first so instead of actually have

1766
01:54:57,560 --> 01:55:01,780
clustering my algorithm is going to help with the tree

1767
01:55:01,900 --> 01:55:05,500
and i'm going to say that this tree is going to define the error rate

1768
01:55:06,520 --> 01:55:11,520
say this tree is good if the cluster you're thinking of is approximately some pruning

1769
01:55:11,520 --> 01:55:14,510
of the tree so if you're thinking of the clustering

1770
01:55:14,520 --> 01:55:19,540
four math physics and this is a good tree thinking of this clustering baseball basketball

1771
01:55:19,540 --> 01:55:23,710
science good tree the area to my tree is going to be the error rate

1772
01:55:23,710 --> 01:55:26,290
with respect to the best proof

1773
01:55:26,310 --> 01:55:30,300
OK so i mean this defined based on whether we can our goal by things

1774
01:55:30,300 --> 01:55:32,370
that having to produce the right answer

1775
01:55:32,410 --> 01:55:34,790
the approximation to the right answer

1776
01:55:34,840 --> 01:55:38,590
and allow my producer tree and then

1777
01:55:38,610 --> 01:55:43,420
leave it up to the user to decide how specific they want to be

1778
01:55:43,440 --> 01:55:46,520
OK so let me the users job decide they they want to be

1779
01:55:46,860 --> 01:55:49,350
the claim is the reasonably natural thing

1780
01:55:49,390 --> 01:55:53,130
so so kind of a high-level picture here is if you try doing clustering you

1781
01:55:53,130 --> 01:55:55,560
can potentially

1782
01:55:55,570 --> 01:56:01,180
do better if you allow your algorithm to do this say if you're in a

1783
01:56:01,180 --> 01:56:02,270
and you put labels on here

1784
01:56:02,620 --> 01:56:04,520
and you say i will encode ayer zero

1785
01:56:04,890 --> 01:56:07,710
can be as one is not wonderful it's an optimal symbol code

1786
01:56:08,180 --> 01:56:09,240
it's expected length

1787
01:56:10,000 --> 01:56:10,580
it is one

1788
01:56:11,830 --> 01:56:12,990
and that is indeed

1789
01:56:13,430 --> 01:56:14,410
close to the entropy

1790
01:56:15,900 --> 01:56:16,220
it is

1791
01:56:16,790 --> 01:56:17,570
point zero eight

1792
01:56:19,520 --> 01:56:21,100
it's within one bit of entropy

1793
01:56:21,690 --> 01:56:22,830
but it's actually

1794
01:56:23,380 --> 01:56:26,070
that's about a factor of twelve bigger than the entropy

1795
01:56:26,600 --> 01:56:30,760
and it's not compressing it of course it's just taking a binary alphabet and encoding it directly

1796
01:56:31,390 --> 01:56:32,840
in the binary with no encoding

1797
01:56:33,690 --> 01:56:38,570
so that's a trivial example where symbol codes clearly aren't getting us where we really

1798
01:56:38,570 --> 01:56:42,270
want to go shannon says we should be able to achieve more than ten fold

1799
01:56:42,270 --> 01:56:44,860
compression about then calling file

1800
01:56:47,070 --> 01:56:49,750
so i'd like to now play a game with you

1801
01:56:54,610 --> 01:56:58,580
the situations in which we might be dissatisfied about this plus one

1802
01:56:59,230 --> 01:57:00,810
that's lurking in this

1803
01:57:05,960 --> 01:57:07,400
the game is called the guessing game

1804
01:57:08,310 --> 01:57:09,320
and what we're not going to do

1805
01:57:09,720 --> 01:57:11,740
is i'm going to write on the board

1806
01:57:12,640 --> 01:57:13,290
the headline

1807
01:57:14,620 --> 01:57:16,440
from a newspaper and i have a headline

1808
01:57:17,070 --> 01:57:17,670
written here

1809
01:57:18,550 --> 01:57:19,220
in front of me

1810
01:57:22,150 --> 01:57:23,570
and i'm going to write it with your help

1811
01:57:24,950 --> 01:57:28,360
because you're going to guess each letter a headline as we go along

1812
01:57:29,190 --> 01:57:32,820
and if you guess the first character letter correctly i'll write it down and i

1813
01:57:32,820 --> 01:57:35,820
will tell you it's correct and then we can move on the ground getting the

1814
01:57:35,820 --> 01:57:36,990
second letter

1815
01:57:37,620 --> 01:57:41,570
all the headline and me alphabet legal characters for this headline

1816
01:57:42,150 --> 01:57:43,400
is on the board data z

1817
01:57:44,270 --> 01:57:44,840
and space

1818
01:57:46,720 --> 01:57:49,370
when we finish the headline now i'll let you know so i'll supply

1819
01:57:50,560 --> 01:57:51,210
the full stop

1820
01:57:52,640 --> 01:57:53,550
in the the headline

1821
01:57:57,890 --> 01:58:02,960
you can start getting the first letter and i'll write it underneath this line all the guesses

1822
01:58:03,400 --> 01:58:07,070
that you've made that are not correct and when i right above the line it

1823
01:58:07,070 --> 01:58:08,460
tells you that you've got the right letter

1824
01:58:10,140 --> 01:58:10,830
guessing time

1825
01:58:14,350 --> 01:58:14,900
not to be

1826
01:58:16,350 --> 01:58:17,140
not i

1827
01:58:19,500 --> 01:58:20,620
this is not right

1828
01:58:30,030 --> 01:58:30,610
and is right

1829
01:58:31,170 --> 01:58:31,620
next letter

1830
01:58:35,720 --> 01:58:36,280
is wrong

1831
01:58:38,970 --> 01:58:40,280
do you say hey you

1832
01:58:49,160 --> 01:58:50,080
i is correct

1833
01:58:50,700 --> 01:58:51,100
next lecture

1834
01:58:54,990 --> 01:58:55,550
not tests

1835
01:58:57,870 --> 01:58:58,450
not take

1836
01:59:02,700 --> 01:59:03,420
it's not well

1837
01:59:03,830 --> 01:59:04,190
and i

1838
01:59:04,820 --> 01:59:05,330
so again

1839
01:59:07,340 --> 01:59:07,840
it's not you

1840
01:59:15,950 --> 01:59:17,730
i would say anything about ministers

1841
01:59:20,370 --> 01:59:20,700
not they

1842
01:59:34,670 --> 01:59:35,690
it's not less

1843
01:59:38,000 --> 01:59:38,370
not to

1844
01:59:43,140 --> 01:59:43,900
it's not less

1845
01:59:45,750 --> 01:59:46,230
not really

1846
01:59:48,080 --> 01:59:48,730
not enough

1847
01:59:52,270 --> 01:59:52,710
not see

1848
01:59:55,820 --> 01:59:56,970
property like you

1849
01:59:58,600 --> 01:59:59,170
now it

1850
02:00:01,620 --> 02:00:02,900
not not to

1851
02:00:11,460 --> 02:00:11,920
not only

1852
02:00:15,270 --> 02:00:16,100
no i

1853
02:00:32,700 --> 02:00:33,300
not then

1854
02:00:34,980 --> 02:00:35,300
not pay

1855
02:00:38,310 --> 02:00:38,710
not see

1856
02:00:39,990 --> 02:00:40,350
not be

1857
02:00:54,830 --> 02:01:04,110
thank you

1858
02:01:04,240 --> 02:01:04,810
it's not

1859
02:01:07,650 --> 02:01:08,330
no i

1860
02:01:10,930 --> 02:01:11,530
not w

1861
02:01:14,220 --> 02:01:14,390
is there

1862
02:01:14,830 --> 02:01:15,160
so again

1863
02:01:17,530 --> 02:01:17,960
not at

1864
02:01:20,030 --> 02:01:20,370
not you

