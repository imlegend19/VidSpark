1
00:00:00,000 --> 00:00:02,290
this is kind of sort of

2
00:00:02,300 --> 00:00:05,500
cognitive science perspective on active learning

3
00:00:05,520 --> 00:00:10,440
and the picture that you should keep in mind with this is traditionally you might

4
00:00:10,440 --> 00:00:14,930
think of a lot of such studies in cognitive science are done this way they

5
00:00:14,950 --> 00:00:20,990
present the subject to adjust stimulate and then the subject's response but the stimuli are

6
00:00:20,990 --> 00:00:25,500
presented kind of randomly and so forth and the person or subject is engaged are

7
00:00:25,500 --> 00:00:30,510
all in the process of selecting what stimuli getting but that's really not how we

8
00:00:30,510 --> 00:00:34,780
interact with the world at all so the idea here is to conserve closing the

9
00:00:34,780 --> 00:00:38,720
loop between the computing in this sense

10
00:00:38,730 --> 00:00:44,020
and a great example of this where there's a lot of work is in the

11
00:00:44,020 --> 00:00:51,400
area of visual perception people have estimated that our capacity

12
00:00:51,420 --> 00:00:55,490
there's about forty four bits per columns and they

13
00:00:55,550 --> 00:00:58,840
if you are interested in that that the authors going to a little bit more

14
00:00:59,180 --> 00:01:03,720
of a definition of what i mean by that but the takeaway messages that has

15
00:01:03,720 --> 00:01:08,230
worse as i'm looking around this room i'm really not taking in all that many

16
00:01:08,230 --> 00:01:13,820
bits of information and so how do we perceive reality from so few bits

17
00:01:13,830 --> 00:01:17,450
this is the idea of interactive vision

18
00:01:17,460 --> 00:01:24,680
so vision is exploratory and predictive we very selectively move our eyes around we're not

19
00:01:25,370 --> 00:01:31,350
like a a camera that takes everything in and so that's the interactive vision

20
00:01:31,440 --> 00:01:36,090
church remember trying to insert now nowitzki have some very nice work on that so

21
00:01:36,090 --> 00:01:40,640
the idea is when we look at the scene we actually very selectively scan that

22
00:01:40,640 --> 00:01:45,160
seem rather than just taking an image of it into our brains

23
00:01:46,150 --> 00:01:50,290
the philosopher andy clark has and i sort of metaphor for this he says we

24
00:01:50,290 --> 00:01:55,560
established visual contact with our world by an ongoing process of active exploration in which

25
00:01:55,560 --> 00:02:02,410
the world acts as a kind of reliable and terrible external memory and so

26
00:02:02,430 --> 00:02:06,180
i think of almost like a database out there that we can query is needed

27
00:02:06,190 --> 00:02:09,850
to help understand what's going on around us

28
00:02:09,870 --> 00:02:14,440
so this is a classic stay as a little bit about it but here's the

29
00:02:14,450 --> 00:02:19,510
painting and subject ax to look at this painting and what is typically observe when

30
00:02:19,510 --> 00:02:23,690
you use an eye tracking device to see where they focused is something like this

31
00:02:23,700 --> 00:02:27,790
so those red lines show where i moved around when the subject was shown this

32
00:02:27,790 --> 00:02:30,160
image on the computer screen and asked to

33
00:02:30,550 --> 00:02:33,830
a study in some way and depending on what the

34
00:02:33,850 --> 00:02:39,400
the goal is we get very different eye tracking results so here is a collection

35
00:02:39,400 --> 00:02:42,260
of results from this study

36
00:02:42,270 --> 00:02:46,580
so the same picture this is when the first subject is just shown

37
00:02:46,600 --> 00:02:48,800
the pictures that take a look at it

38
00:02:48,820 --> 00:02:53,100
and they just kind of skin around if you ask the person to estimate the

39
00:02:53,100 --> 00:02:58,390
material circumstances of the family you see this kind of scanning pattern and so what

40
00:02:58,420 --> 00:03:02,640
you might pick up is looking at the clothing some of the furniture maybe some

41
00:03:02,640 --> 00:03:05,690
of the decorations in the room if

42
00:03:05,700 --> 00:03:07,010
the subject is

43
00:03:07,040 --> 00:03:08,460
asked to

44
00:03:08,470 --> 00:03:10,250
given the ages of the people

45
00:03:11,520 --> 00:03:12,540
for me

46
00:03:12,550 --> 00:03:18,570
strong but you get this pattern and so the focus is really faces

47
00:03:18,580 --> 00:03:21,290
as you would expect if you want to know how to hold me as you

48
00:03:21,290 --> 00:03:27,310
may be what they are faced with a have appeared how tolerant and so forth

49
00:03:27,320 --> 00:03:31,200
if you want lies with the person is doing before they had expected this very

50
00:03:31,200 --> 00:03:35,960
kind of look at where they are positioned and so forth in the room remember

51
00:03:35,960 --> 00:03:40,930
the clothes they wore you see very strong focus on those clothing

52
00:03:41,190 --> 00:03:45,410
the wearing member people and objects in the room time looking over and so on

53
00:03:45,410 --> 00:03:47,460
and so forth so the point is that

54
00:03:47,510 --> 00:03:51,770
when you find the subject you say attention to this or that or the other

55
00:03:51,770 --> 00:03:53,760
thing how they

56
00:03:53,780 --> 00:03:59,370
interact and how they select information is very very different and this is a good

57
00:03:59,370 --> 00:04:02,590
example of active learning in humans

58
00:04:02,600 --> 00:04:09,670
here's another nice example recognizing faces so here's the face and and here's a typical

59
00:04:09,690 --> 00:04:13,730
scanned so this is how the eyes move about when you ask them to look

60
00:04:13,730 --> 00:04:17,420
at the face and so you see what's happening is there looking back and forth

61
00:04:17,420 --> 00:04:21,830
between the eyes down the nose at the mouth and we i mean i'm not

62
00:04:21,860 --> 00:04:23,060
expert in

63
00:04:23,070 --> 00:04:26,570
face recognition but i know that a lot of the things that people use are

64
00:04:26,960 --> 00:04:31,950
distances between prominent features like eisenhower and so forth you can see how perhaps this

65
00:04:31,950 --> 00:04:35,520
we see that

66
00:04:37,630 --> 00:04:39,340
so and we've got

67
00:04:40,670 --> 00:04:46,270
why i so we've got some conditional probabilities x given x i minus one

68
00:04:46,340 --> 00:04:48,910
also got some

69
00:04:48,940 --> 00:04:51,960
why i was given exercise

70
00:04:51,980 --> 00:04:54,980
now we

71
00:04:55,440 --> 00:04:56,970
we may want to

72
00:04:57,060 --> 00:05:02,830
express those distributions parametric models in general be some parameters theta governing these

73
00:05:02,840 --> 00:05:08,190
given these distributions are very often we want the data to be common to all

74
00:05:08,190 --> 00:05:12,590
of them in other words the the parameter itself is is not a function of

75
00:05:12,590 --> 00:05:14,040
time steps

76
00:05:14,110 --> 00:05:15,660
and so on

77
00:05:15,690 --> 00:05:21,280
the parameters shared across all of emission models in in almost all cases

78
00:05:21,330 --> 00:05:25,840
now suppose we want to adopt a bayesian treatment for bayesian treatment we put a

79
00:05:25,840 --> 00:05:30,400
prior distribution over theta so data is no longer parameter which we make a point

80
00:05:30,400 --> 00:05:32,200
estimate it's now

81
00:05:32,560 --> 00:05:34,840
the random variable and has

82
00:05:34,860 --> 00:05:40,660
prior probability distribution and that expressed graphically in this form so there's theta with a

83
00:05:40,660 --> 00:05:41,900
prior distribution

84
00:05:41,950 --> 00:05:43,910
and these conditional distributions

85
00:05:43,940 --> 00:05:48,220
four why i depend not only on x i but also on these

86
00:05:48,250 --> 00:05:51,770
we might do the same for all the transition probability so you might have some

87
00:05:51,770 --> 00:05:52,890
other products

88
00:05:52,900 --> 00:05:55,900
site and now points at

89
00:05:55,900 --> 00:05:59,900
these nodes here perhaps the the first initial state has some of the parameters and

90
00:05:59,900 --> 00:06:01,770
so on

91
00:06:01,820 --> 00:06:07,600
so we can very simply express these more complex naive bayesian models using is graphical

92
00:06:07,600 --> 00:06:12,940
representation very straightforwardly and we can make an interesting observation which will as we'll see

93
00:06:12,940 --> 00:06:16,250
later is very important in going from

94
00:06:16,260 --> 00:06:17,400
that model

95
00:06:18,170 --> 00:06:21,520
that model something important has happened to the original

96
00:06:21,530 --> 00:06:27,180
maximum likelihood kalman filter a hidden markov model is a tree structured graphs there are

97
00:06:27,180 --> 00:06:31,060
no loops in the graph and it will turn out that inference on tree structured

98
00:06:31,060 --> 00:06:33,340
graphs is very straightforward

99
00:06:33,490 --> 00:06:37,290
when they're all loops in the graph as we have here

100
00:06:37,300 --> 00:06:39,110
then inference becomes much

101
00:06:39,130 --> 00:06:40,410
more difficult

102
00:06:40,460 --> 00:06:43,070
and so for graphs like this

103
00:06:43,080 --> 00:06:47,980
exact inference could very quickly become intractable and we resort to approximations schemes in the

104
00:06:47,980 --> 00:06:51,360
one that will be discussing in detail is variational inference

105
00:06:51,370 --> 00:06:55,590
so i variational inference for example have been applied to bayesian state space models of

106
00:06:55,600 --> 00:07:00,960
the of exactly the kind of just wrong

107
00:07:00,970 --> 00:07:03,110
they all sorts of other games

108
00:07:04,300 --> 00:07:07,820
in this markov chain if this has n states and this has n states this

109
00:07:07,820 --> 00:07:11,670
is an m squared matrix of transition probabilities

110
00:07:11,710 --> 00:07:14,350
subject to the normalisation constraint

111
00:07:14,530 --> 00:07:19,140
we may wish to have a state space which has quite high dimensionality

112
00:07:19,160 --> 00:07:20,670
but without allowing

113
00:07:20,680 --> 00:07:21,930
all possible

114
00:07:22,380 --> 00:07:24,300
transitions to exist

115
00:07:24,340 --> 00:07:28,640
and one way to achieve this is to have multiple

116
00:07:28,660 --> 00:07:31,830
multiple chains so

117
00:07:31,840 --> 00:07:33,590
the nice thing is we can draw

118
00:07:33,610 --> 00:07:38,560
this particular graph and understand the probabilistic structure very readily be much less apparent i

119
00:07:38,560 --> 00:07:43,420
think we just wrote down the mathematical expressions for all conditional distributions that define the

120
00:07:43,430 --> 00:07:44,750
joint distribution

121
00:07:44,820 --> 00:07:46,330
so now we have multiple

122
00:07:46,340 --> 00:07:49,960
it chains and each of them can have

123
00:07:49,960 --> 00:07:55,680
it has a separate variables so if if all the variables had if this had

124
00:07:55,680 --> 00:08:01,980
in states that had and states madam states then there and q configurations

125
00:08:02,000 --> 00:08:03,750
of the

126
00:08:03,760 --> 00:08:05,350
of the hidden state

127
00:08:05,360 --> 00:08:07,100
at this time steps

128
00:08:09,990 --> 00:08:10,950
we could

129
00:08:10,960 --> 00:08:12,960
represent this equivalently

130
00:08:12,960 --> 00:08:15,710
as a sort of flat model with just a single

131
00:08:15,730 --> 00:08:17,500
markov chain down the middle

132
00:08:17,510 --> 00:08:22,910
in which each node has q possible states would have one variable that has

133
00:08:23,050 --> 00:08:26,420
exponentially many possible states but clearly that's going to get

134
00:08:26,460 --> 00:08:31,000
and we'll be very quickly whereas this allows us to have a very complex and

135
00:08:32,580 --> 00:08:34,580
in state space

136
00:08:34,590 --> 00:08:36,890
and you have a model which is

137
00:08:36,910 --> 00:08:41,260
although not exactly tractable very close to being tractable

138
00:08:41,360 --> 00:08:46,240
so this is the model which has been used and explored quite widely mention at

139
00:08:46,250 --> 00:08:49,320
the end i think we discuss variational inference looking at two

140
00:08:49,330 --> 00:08:52,850
treat the model of that kind variational

141
00:08:52,910 --> 00:08:56,570
so you can play lots of games with this kind of of those are all

142
00:08:56,570 --> 00:09:00,430
directed graphs to give you an example of an undirected graph this is probably the

143
00:09:00,430 --> 00:09:01,530
most common

144
00:09:01,550 --> 00:09:10,010
undirected example of markov random fields which is widely used in image processing attributed images

145
00:09:10,030 --> 00:09:13,640
so in images just the vector of pixel intensities but has

146
00:09:13,660 --> 00:09:19,150
the important property that nearby pixels tend to be very strongly correlated much more correlated

147
00:09:19,150 --> 00:09:22,460
the more distant pixels and we want to be able to capture that property in

148
00:09:22,460 --> 00:09:23,850
the probabilistic model

149
00:09:23,940 --> 00:09:25,760
and so we have two d spatial

150
00:09:25,790 --> 00:09:30,430
markov random fields which captures the property suitable for example would be

151
00:09:30,450 --> 00:09:35,660
the regions of images for example to measure have

152
00:09:35,670 --> 00:09:37,070
i have an image

153
00:09:37,080 --> 00:09:38,070
we might

154
00:09:38,080 --> 00:09:40,960
in the simple case divide the image up into

155
00:09:43,500 --> 00:09:45,680
let's say

156
00:09:45,690 --> 00:09:48,980
he show

157
00:09:49,070 --> 00:09:55,140
so we've got you know picture the house here or something

158
00:09:55,200 --> 00:09:56,200
and many this

159
00:09:56,210 --> 00:09:59,830
trees is the clouds and so on

160
00:09:59,850 --> 00:10:03,800
people stick with my day job she would become an artist

161
00:10:03,800 --> 00:10:08,530
well this is the architecture i don't know no don't cannot go much into the

162
00:10:08,530 --> 00:10:10,380
case yugoslav because of lack of time

163
00:10:10,400 --> 00:10:16,170
but everything is very much like saxex except for

164
00:10:16,340 --> 00:10:20,630
the but in the middle that states the annotation performance

165
00:10:20,690 --> 00:10:22,150
t one tn

166
00:10:22,170 --> 00:10:23,550
what is that

167
00:10:23,550 --> 00:10:25,030
for each

168
00:10:25,030 --> 00:10:26,530
OK case for example

169
00:10:26,550 --> 00:10:32,130
of music played by a human performance now what we have is twelve temples

170
00:10:32,130 --> 00:10:35,720
each example has been played at twelve different tempos

171
00:10:35,740 --> 00:10:40,150
in such a way that even when we have an input performance at it even

172
00:10:40,990 --> 00:10:45,010
anyone who desired temple dedicated temple

173
00:10:45,030 --> 00:10:50,990
the system will be able to retrieve some precedent where there is this source and

174
00:10:50,990 --> 00:10:55,550
target but which are close to the source and target them of the problem

175
00:10:55,590 --> 00:11:00,860
and what the system will do is will check for the differences between the annotated

176
00:11:00,860 --> 00:11:03,110
performances of the precedent case

177
00:11:03,220 --> 00:11:08,860
look at the differences what was different when the human players played that eighty or

178
00:11:08,860 --> 00:11:11,800
a hundred PM's and transfer

179
00:11:11,820 --> 00:11:19,260
again imitates transferred to the to the input of the input problem these differences and

180
00:11:19,260 --> 00:11:21,740
apply these differences to random

181
00:11:21,800 --> 00:11:25,920
these input problem at the target tempo

182
00:11:25,920 --> 00:11:28,240
so this is an annotation for example

183
00:11:28,300 --> 00:11:34,320
got associated with is one of these the labels is the saxex system

184
00:11:34,320 --> 00:11:37,530
so that means that the note in the score

185
00:11:37,590 --> 00:11:42,090
not just a sounding noting in the performance of the node really was something the

186
00:11:42,110 --> 00:11:43,110
performs maybe

187
00:11:43,150 --> 00:11:46,780
and in with some of the beach the direction

188
00:11:46,820 --> 00:11:52,360
the onset the brought together all these qualities all these values are associated which behind

189
00:11:52,360 --> 00:11:56,220
each one of these days but they wanted also to look more closely to the

190
00:11:56,220 --> 00:12:01,900
old and the indices because this was not things was not in saxex

191
00:12:01,920 --> 00:12:06,090
here we are no longer than just balance is just and this can be hard

192
00:12:06,090 --> 00:12:10,150
work can be people can be and in this type of this style of music

193
00:12:10,150 --> 00:12:14,920
in the style of just it is very common that in all these fragments

194
00:12:14,920 --> 00:12:18,360
or not is consolidated conservation means

195
00:12:18,360 --> 00:12:23,840
that's not that repeats like we see we listen to this like that but i'm

196
00:12:23,840 --> 00:12:29,670
here to be installed times may be consolidated into one single node in the performance

197
00:12:29,900 --> 00:12:32,530
and the fragmentation is the reverse

198
00:12:32,550 --> 00:12:36,070
they belong to what is fragmented into several

199
00:12:36,090 --> 00:12:40,970
types times the for the same not of the same beach right and all means

200
00:12:40,970 --> 00:12:46,630
ornamentation it is often that the amount of ornamental know is that that was not

201
00:12:46,630 --> 00:12:50,630
in the score so this is for for this the style of just these these

202
00:12:50,630 --> 00:12:52,650
are quite common resources

203
00:12:52,670 --> 00:12:56,820
so you can see is that she that for example you can

204
00:12:57,150 --> 00:13:04,490
for example all the nodes

205
00:13:15,380 --> 00:13:16,570
i mean this

206
00:13:16,590 --> 00:13:18,340
little notes that have

207
00:13:18,400 --> 00:13:20,320
very short duration but just

208
00:13:20,320 --> 00:13:26,170
right before and he also i will

209
00:13:26,190 --> 00:13:32,470
world because the environment is set to

210
00:13:32,490 --> 00:13:36,900
to the right it is this sound like one

211
00:13:36,900 --> 00:13:40,010
and this this and this sound like one single

212
00:13:40,110 --> 00:13:44,130
so as a set what does the system is well by

213
00:13:44,190 --> 00:13:49,800
several phrases that i will get into detail it ends up with some with at

214
00:13:49,800 --> 00:13:54,960
least some phrase to give someone just to the left box on the bottom

215
00:13:54,990 --> 00:14:00,650
the segments cities is a case that has already solution that mean that has been

216
00:14:00,650 --> 00:14:04,690
played at the two different at the two different tempos the target and the system

217
00:14:05,550 --> 00:14:11,440
that shown there and therefore we have the performance annotation of these two temples and

218
00:14:11,440 --> 00:14:14,570
at the bottom you see the inputs with TTT here

219
00:14:15,010 --> 00:14:17,220
but we have only performance for the

220
00:14:17,220 --> 00:14:22,280
so this temple of the system has to the performance annotation for the telling it

221
00:14:22,280 --> 00:14:24,040
which could be be it

222
00:14:24,100 --> 00:14:27,920
by covering all elements together

223
00:14:27,960 --> 00:14:33,300
so this is the first problem so there are some algorithms developed to do so

224
00:14:34,260 --> 00:14:38,050
there is another problem which is to maintain

225
00:14:38,100 --> 00:14:39,620
k class two

226
00:14:39,620 --> 00:14:42,430
but not only the centers of clusters

227
00:14:42,460 --> 00:14:44,680
but also information

228
00:14:44,700 --> 00:14:47,080
about the contents of the cluster

229
00:14:47,090 --> 00:14:49,670
number of individual

230
00:14:49,710 --> 00:14:54,160
the centroid and some information about the rate which is

231
00:14:54,240 --> 00:14:55,340
more rich

232
00:14:55,450 --> 00:15:00,950
reach fn and the first problem

233
00:15:00,990 --> 00:15:02,720
so the

234
00:15:02,780 --> 00:15:04,160
the problem

235
00:15:05,870 --> 00:15:07,700
of clustering

236
00:15:08,700 --> 00:15:13,180
which is also a problem for decision trees but PCA

237
00:15:13,240 --> 00:15:15,450
one important problem for

238
00:15:15,460 --> 00:15:17,240
data stream mining

239
00:15:17,290 --> 00:15:19,710
it is what we call concept three

240
00:15:19,720 --> 00:15:21,710
is that

241
00:15:22,810 --> 00:15:24,580
the distribution

242
00:15:24,580 --> 00:15:26,050
of elements

243
00:15:26,080 --> 00:15:28,600
is stationary

244
00:15:28,660 --> 00:15:30,240
it's OK

245
00:15:30,290 --> 00:15:32,990
if you have an incremental libraries

246
00:15:33,040 --> 00:15:35,590
it's the that very often

247
00:15:35,590 --> 00:15:37,700
the distribution of elements

248
00:15:37,710 --> 00:15:39,750
may change with time

249
00:15:39,760 --> 00:15:41,630
so that's what we call

250
00:15:41,670 --> 00:15:43,000
concept drift

251
00:15:43,040 --> 00:15:47,130
in this case

252
00:15:47,140 --> 00:15:50,200
one solution is window

253
00:15:50,240 --> 00:15:53,030
it's sliding window

254
00:15:53,040 --> 00:15:55,540
if you define the sliding window

255
00:15:55,550 --> 00:15:59,570
you make the assumption that it's stationary within

256
00:15:59,580 --> 00:16:01,530
the sliding window

257
00:16:01,570 --> 00:16:03,200
it's better than

258
00:16:03,210 --> 00:16:04,580
assuming that it

259
00:16:05,700 --> 00:16:10,180
from the beginning of the stream

260
00:16:10,200 --> 00:16:11,130
so the

261
00:16:12,180 --> 00:16:15,210
solution i'm going to present you

262
00:16:18,210 --> 00:16:19,380
solution apply

263
00:16:19,650 --> 00:16:22,570
which you can apply to data streams

264
00:16:22,580 --> 00:16:27,410
which we provide clusters with statistic on their come

265
00:16:27,420 --> 00:16:31,380
and as the solution to consider it

266
00:16:31,430 --> 00:16:32,340
it will

267
00:16:32,350 --> 00:16:35,630
provide it will enable the use

268
00:16:35,670 --> 00:16:36,970
to define

269
00:16:36,990 --> 00:16:38,870
any portion of the the stream

270
00:16:38,890 --> 00:16:42,830
in the past and provide the clustering

271
00:16:42,870 --> 00:16:44,310
as exactly

272
00:16:44,330 --> 00:16:48,290
as it it would have been done on that portion

273
00:16:48,300 --> 00:16:50,500
which is not defined

274
00:16:50,540 --> 00:16:55,290
at the beginning of the stream

275
00:16:55,300 --> 00:16:56,420
so this is the

276
00:16:56,430 --> 00:16:59,310
class garrison which had been designed by

277
00:16:59,330 --> 00:17:03,570
a guy while and it and in two thousand three

278
00:17:03,670 --> 00:17:09,040
it can be applied to numerical variable on the

279
00:17:09,040 --> 00:17:13,200
it has two phases

280
00:17:13,210 --> 00:17:14,710
the first phase

281
00:17:15,530 --> 00:17:17,070
and on phase

282
00:17:17,090 --> 00:17:19,030
so it received

283
00:17:19,040 --> 00:17:21,990
the elements of one stream

284
00:17:22,000 --> 00:17:23,890
and in this phase

285
00:17:23,910 --> 00:17:26,420
the idea is to maintain

286
00:17:26,450 --> 00:17:29,450
a large number of classes

287
00:17:30,380 --> 00:17:32,920
it's a large number of clusters we

288
00:17:32,930 --> 00:17:36,210
can consider that bizarre michael plastic

289
00:17:36,220 --> 00:17:38,710
large number typically

290
00:17:38,780 --> 00:17:41,750
several hundred or one thousand

291
00:17:41,810 --> 00:17:45,840
this micro clusters would be described by statistics

292
00:17:45,840 --> 00:17:48,600
on the context

293
00:17:48,660 --> 00:17:50,000
so this is the

294
00:17:50,010 --> 00:17:53,250
first and continues

295
00:17:53,290 --> 00:17:58,280
when the user needs a clustering of history

296
00:17:58,290 --> 00:17:59,640
very is

297
00:18:01,040 --> 00:18:03,120
phase which is done

298
00:18:04,870 --> 00:18:07,050
and the system uses a

299
00:18:07,070 --> 00:18:10,310
it uses the micro clusters produce

300
00:18:10,330 --> 00:18:14,250
the final cluster

301
00:18:14,300 --> 00:18:16,540
this is the first principal also

302
00:18:16,540 --> 00:18:21,280
i meant in terms of micro clusters in venues of these micro clusters

303
00:18:21,280 --> 00:18:22,080
to build

304
00:18:26,450 --> 00:18:29,030
in order to solve this problem

305
00:18:29,920 --> 00:18:31,790
giving the possibility

306
00:18:31,800 --> 00:18:32,990
to obtain

307
00:18:32,990 --> 00:18:36,140
the clustering of any past portion of the

308
00:18:36,960 --> 00:18:38,490
there is a mechanism

309
00:18:38,540 --> 00:18:41,120
to keep track of the story

310
00:18:41,130 --> 00:18:48,670
of micro clusters with time

311
00:18:48,670 --> 00:18:50,840
so as before

312
00:18:50,840 --> 00:18:55,580
the representation of micro clusters of course the system

313
00:18:55,630 --> 00:18:59,030
i cannot keep track of all elements belonging

314
00:18:59,040 --> 00:18:59,840
two each

315
00:18:59,870 --> 00:19:01,810
micro cluster

316
00:19:01,920 --> 00:19:03,920
elements are discarded

317
00:19:03,970 --> 00:19:06,160
so what is kept

318
00:19:06,340 --> 00:19:08,700
for each micro cluster

319
00:19:08,710 --> 00:19:10,590
it is

320
00:19:10,640 --> 00:19:15,740
what they called the cluster feature vector actually it was not introduced by

321
00:19:15,740 --> 00:19:19,090
i got what but by the previous algorithm

322
00:19:19,100 --> 00:19:21,390
called the dutch

323
00:19:21,430 --> 00:19:23,500
so for each micro cluster

324
00:19:24,930 --> 00:19:25,880
is kept

325
00:19:25,880 --> 00:19:29,450
and it's kind of summary of the micro cluster

326
00:19:29,450 --> 00:19:30,220
we have

327
00:19:31,410 --> 00:19:33,640
of elements in the micro cluster

328
00:19:33,750 --> 00:19:36,210
and for each variable

329
00:19:36,210 --> 00:19:38,370
the sum of values

330
00:19:38,380 --> 00:19:40,410
of the variables within the next

331
00:19:40,420 --> 00:19:43,830
course they get and the sum of squares

332
00:19:43,830 --> 00:19:46,040
chinese is an instance of language works

333
00:19:46,060 --> 00:19:50,810
park is actually working with the international linguistics community for fifteen years

334
00:19:51,540 --> 00:19:55,600
testing the theory that there could be a universal

335
00:19:55,620 --> 00:20:01,390
language platform that people can write their grammars knowledge for the different languages the first

336
00:20:01,490 --> 00:20:03,450
which is the world on top of

337
00:20:03,450 --> 00:20:08,870
and twice a year this helpful to come together and take a tackle on different

338
00:20:08,870 --> 00:20:12,790
kinds of projects ways of expressing things and at this point there are actually grammars

339
00:20:12,790 --> 00:20:19,450
in english french japanese german chinese norwegian and many other languages very diverse kind of

340
00:20:19,450 --> 00:20:21,890
languages and this appears to be working pretty well

341
00:20:22,140 --> 00:20:29,010
there's many aspects of algorithms and syntactic theories in represent the system and then the

342
00:20:29,240 --> 00:20:34,930
to draw community knowledge resources as we show and this technology can be applied to

343
00:20:34,930 --> 00:20:38,060
many different kinds of applications and the one we've been emphasizing here

344
00:20:38,100 --> 00:20:41,180
it's consumer search

345
00:20:41,200 --> 00:20:47,120
this is a little look at the kind of functional structure of the language is

346
00:20:47,120 --> 00:20:52,470
extracted from the text as we pass the web and this showing functional structure where

347
00:20:52,470 --> 00:20:55,310
you see the predicitions on the subject and the objects

348
00:20:55,310 --> 00:20:57,950
even down to

349
00:20:57,970 --> 00:21:02,890
you know that it's an active not passive type because

350
00:21:02,930 --> 00:21:05,540
this is the basic search architecture

351
00:21:05,540 --> 00:21:09,370
for natural language search so we begin with the open content

352
00:21:09,430 --> 00:21:16,660
pass it using the lexical functional grammar and XLE which is the park core technology

353
00:21:16,700 --> 00:21:20,950
combined with knowledge resources that are either manually or

354
00:21:20,990 --> 00:21:24,990
acquired to become miss machine learning

355
00:21:25,010 --> 00:21:26,640
semantic mapping

356
00:21:26,660 --> 00:21:28,830
that content is all indexed

357
00:21:28,850 --> 00:21:33,310
the semantics looks like i was showing you that the actual

358
00:21:33,330 --> 00:21:34,350
screens by

359
00:21:34,370 --> 00:21:37,430
so the kind is an index in a very large

360
00:21:37,450 --> 00:21:39,200
scale semantic index

361
00:21:39,220 --> 00:21:41,370
our semantic index draws on

362
00:21:41,370 --> 00:21:45,540
both the the large scale indexing techniques used in information retrieval

363
00:21:45,560 --> 00:21:52,220
and large scale search engines but it's actually also combines the semantic type of information

364
00:21:52,220 --> 00:21:58,790
representations so stored in one of the same index are words as vectors and where

365
00:21:58,790 --> 00:21:59,720
they occur

366
00:21:59,760 --> 00:22:06,080
and semantic concepts in thematic roles and their relationships

367
00:22:06,080 --> 00:22:10,310
and we put a lot of work into being able to essentially formulate these complex

368
00:22:10,310 --> 00:22:14,930
queries large scale and returned results in real time

369
00:22:14,950 --> 00:22:19,200
so then when the user as the query that query is parsed and create converges

370
00:22:19,200 --> 00:22:23,870
semantic map using the same processing has done for the text

371
00:22:23,890 --> 00:22:26,490
two ultimately get the semantics of the query

372
00:22:26,600 --> 00:22:28,850
which is then translated into

373
00:22:28,870 --> 00:22:35,890
a query execution plan thrown at the large-scale index retrieved and then we employ a

374
00:22:35,890 --> 00:22:40,930
set of ranking techniques ranking is certainly one of the major areas in search and

375
00:22:40,930 --> 00:22:45,100
our ranking takes advantage of the kinds of ranking the using search engines today

376
00:22:45,410 --> 00:22:50,540
and also takes advantage of all these new features information that's coming out of semantics

377
00:22:50,600 --> 00:22:56,220
finally the system that presents the results of the top ranked so objects or clusters

378
00:22:56,220 --> 00:23:00,410
and as you seen with some of the other demos i'm showing and because you

379
00:23:00,410 --> 00:23:04,410
have a whole new level of semantics and you have aligned elements of the query

380
00:23:04,410 --> 00:23:11,850
with elements of the corresponding semantic graphs there's a whole new opportunities for presentation

381
00:23:11,950 --> 00:23:14,950
and so is required this large a large scale

382
00:23:14,970 --> 00:23:21,180
is this really a lot of scalability according computing to thousands of machines a lot

383
00:23:21,180 --> 00:23:25,280
of work and system integration different kinds of data resources

384
00:23:25,330 --> 00:23:28,240
a lot of knowledge resources being pulled together

385
00:23:29,180 --> 00:23:32,580
and many acquisition strategies for the knowledge resources

386
00:23:32,600 --> 00:23:37,510
finally fair amount of work in user experience to set users expectations right because these

387
00:23:37,510 --> 00:23:42,640
systems are going to be useful but not perfect and may require some change behavior

388
00:23:42,640 --> 00:23:45,520
to get the most value system

389
00:23:45,580 --> 00:23:50,330
OK now i'm really reaching the end here and want to point two where i

390
00:23:50,330 --> 00:23:51,600
think that this

391
00:23:51,760 --> 00:23:53,060
all can lead

392
00:23:53,060 --> 00:23:58,350
we to start the talk by talking about the chicken-and-egg problem the semantic web

393
00:23:59,240 --> 00:24:04,200
now i think with these kinds of natural language search engines once becomes commonplace and

394
00:24:04,220 --> 00:24:10,600
you have a large scale commercial successful natural language search engines that do not require

395
00:24:10,600 --> 00:24:15,520
to be to be commercially deployed the semantic resource in the first place then you

396
00:24:15,520 --> 00:24:20,990
can really let the wisdom of crowds to accelerate the development and deployment semantic web

397
00:24:21,100 --> 00:24:25,390
some examples here as if i was in the crowd work so well is the

398
00:24:25,390 --> 00:24:26,790
current crop were today

399
00:24:26,790 --> 00:24:30,510
on search engines is through anchortext so

400
00:24:30,520 --> 00:24:34,290
everyone every person person writes a web page and links to the page puts in

401
00:24:34,290 --> 00:24:35,260
some text

402
00:24:35,260 --> 00:24:40,290
and that text is actually then used by the search engines and to know that

403
00:24:40,290 --> 00:24:41,890
something more about that page

404
00:24:41,930 --> 00:24:47,200
this is how a lot of the problem of navigation queries was solved search the

405
00:24:47,200 --> 00:24:51,060
IBM homepage may not even say international business machines anymore

406
00:24:51,100 --> 00:24:57,720
but because other people wrote nationally international business machines forty two IBM then the search

407
00:24:57,720 --> 00:25:02,010
engine knows that that content is actually it's actually on or about that page

408
00:25:02,720 --> 00:25:04,290
in a similar way now

409
00:25:04,310 --> 00:25:09,760
publishers who put alot of work into optimizing their pages to be linked in expert

410
00:25:09,760 --> 00:25:11,450
search engine so they can get traffic

411
00:25:11,560 --> 00:25:14,890
will be able to upload ontologies to get more traffic

412
00:25:14,910 --> 00:25:18,620
so you can imagine being a publisher of say sports

413
00:25:19,910 --> 00:25:24,680
and you already know all the different players and teams and owners and the rules

414
00:25:24,680 --> 00:25:26,200
and what they'll do

415
00:25:26,930 --> 00:25:31,540
if you upload that to this kind of natural language search engine then whenever users

416
00:25:31,540 --> 00:25:36,870
typing in more specific words vocabulary concepts to ruin OK could describe the player by

417
00:25:36,870 --> 00:25:40,310
the center field or something like that even the word may not have appeared on

418
00:25:40,310 --> 00:25:44,700
where you learn a density model for the positive density model for the negatives and

419
00:25:44,700 --> 00:25:48,710
then you compute for any new thing the posterior probability by comparing its relative densities

420
00:25:48,710 --> 00:25:53,680
under the positive model of the negative whereas this doesn't really look like that here

421
00:25:53,680 --> 00:25:58,760
i'm only giving you first of all three examples that it's hard to learn discriminating

422
00:25:58,760 --> 00:26:03,500
function for example you could learn something if it was very simple like linear but

423
00:26:03,500 --> 00:26:08,920
i'm not giving any negative examples so so naive discriminative approaches are you know most

424
00:26:08,920 --> 00:26:13,150
discriminative approaches even very not naive ones are not

425
00:26:13,180 --> 00:26:15,530
may even be applicable to this problem

426
00:26:15,900 --> 00:26:21,150
and even generative approaches where you say learned density model of the positive examples these

427
00:26:21,150 --> 00:26:25,150
are very complex objects what however gonna represent these it's seems like it might be

428
00:26:25,150 --> 00:26:29,310
a high dimensional space with three points you know what density model are you going

429
00:26:29,310 --> 00:26:31,900
to learn so it's kind of puzzle

430
00:26:31,930 --> 00:26:36,760
on the other hand you know it's not insignificant that i also give you all

431
00:26:36,760 --> 00:26:40,900
the other examples i call this the test set or we could call this semi

432
00:26:40,900 --> 00:26:45,210
supervised learning problem did you guys talk about that kind of semi supervised learning book

433
00:26:45,210 --> 00:26:47,880
about so that people know that terms

434
00:26:49,600 --> 00:26:54,850
so again excuse me if i'm going over ground you covered but there's a lot

435
00:26:54,850 --> 00:26:58,110
of interest recently in problems where you have a lot of unlabeled data and a

436
00:26:58,110 --> 00:27:03,630
few labeled examples so the unlabelled data make it like an unsupervised structure discovery problem

437
00:27:03,810 --> 00:27:07,260
but the actual task you to solve is defined by the small number of labeled

438
00:27:08,650 --> 00:27:15,840
and the the the the interesting computational challenges to figure out how to extract something

439
00:27:15,840 --> 00:27:20,600
from all the unlabelled examples that will help me generalize in a much more useful

440
00:27:21,190 --> 00:27:25,070
the concert and learning from these labelled examples and i think one way to approach

441
00:27:25,070 --> 00:27:29,290
this problem that takes is pretty quickly to the state of the art in machine

442
00:27:29,290 --> 00:27:33,270
learning is thinking of this as an interesting kind of semi supervised learning problem

443
00:27:33,290 --> 00:27:38,180
so we'll talk about talk about that but the particularly the bayesian approach to semi

444
00:27:38,180 --> 00:27:43,020
supervised learning that they were interested in really makes it an unsupervised learning problem it's

445
00:27:43,020 --> 00:27:47,110
really about it in the sense maybe my initial fame the problem was a bit

446
00:27:47,110 --> 00:27:50,770
of a cheat but it's really about what can we learn about the structure of

447
00:27:50,770 --> 00:27:52,420
the world from these data

448
00:27:52,430 --> 00:27:57,030
and then you know will use these labels and some

449
00:27:57,050 --> 00:28:01,440
a clever way to to governor but the real work is being done by coming

450
00:28:01,440 --> 00:28:05,350
up with the right hypothesis space in the sense from the unlabelled data now here's

451
00:28:05,350 --> 00:28:08,550
a very simple approach to solving this kind of problem

452
00:28:08,570 --> 00:28:12,350
just illustrated naively with the picture

453
00:28:12,350 --> 00:28:16,030
so let's say we've got it at you know our objects are points in a

454
00:28:16,030 --> 00:28:20,360
two dimensional space here so each of these data points is one of these objects

455
00:28:20,890 --> 00:28:24,300
and let's say you you

456
00:28:24,310 --> 00:28:27,960
so you get mostly unlabelled data and just one labelled example so that one here

457
00:28:27,960 --> 00:28:31,850
and i say that's applicable or to file here it's a blicket OK so which

458
00:28:31,850 --> 00:28:33,190
are the other blickets

459
00:28:33,220 --> 00:28:37,650
you tell me

460
00:28:37,810 --> 00:28:39,800
OK so this is a blicket

461
00:28:39,840 --> 00:28:41,980
that like it

462
00:28:42,000 --> 00:28:43,440
that wicket

463
00:28:43,470 --> 00:28:46,220
look at

464
00:28:46,980 --> 00:28:53,570
right good that was easy alright so what so what makes it easy well

465
00:28:53,590 --> 00:28:56,390
basically you look at this and you see

466
00:28:56,420 --> 00:29:00,890
these three clusters kind of pop out at you right and if you are able

467
00:29:00,890 --> 00:29:02,930
to identify the clusters

468
00:29:02,940 --> 00:29:07,180
from the unlabelled data which in this case you work and you but you have

469
00:29:07,180 --> 00:29:11,260
some prior abstract knowledge that tells you that were labels pick out clusters the clusters

470
00:29:11,260 --> 00:29:14,710
you can identify then all i need is one example

471
00:29:14,710 --> 00:29:17,960
right to tell you and only one possible example it's basically just telling you which

472
00:29:17,960 --> 00:29:19,440
is the right cluster label

473
00:29:19,470 --> 00:29:24,250
and versions of this approach have again been developed in machine learning the the first

474
00:29:24,250 --> 00:29:27,750
one that i know about was again worked zoubin did in grad school but a

475
00:29:27,750 --> 00:29:30,430
lot i get a lot of the work that we're doing is kind of footnotes

476
00:29:30,430 --> 00:29:32,350
and ghahramani

477
00:29:32,390 --> 00:29:37,290
but i that

478
00:29:37,300 --> 00:29:40,900
just kidding but but as you know zoom in

479
00:29:40,980 --> 00:29:45,190
i really like this technical report and i don't know if it was ever published

480
00:29:45,190 --> 00:29:50,010
outside of the ten but it's very t are maybe there was paper something that

481
00:29:50,350 --> 00:29:52,980
in which have been talked about i think it was the first paper i know

482
00:29:52,980 --> 00:29:54,530
to use

483
00:29:54,560 --> 00:30:00,220
you know gaussians mixture models are more or multinomial mixture to think about unsupervised categorisation

484
00:30:00,220 --> 00:30:05,850
and also supervising problems which are essentially the semi supervised problems in using EEN back

485
00:30:05,850 --> 00:30:06,920
when that was like

486
00:30:08,430 --> 00:30:14,610
so just making but still ian was the high and more recently you know there's

487
00:30:14,630 --> 00:30:19,630
a lot of interest in these non parametric mixture models so i think you i

488
00:30:19,630 --> 00:30:23,460
started to tell you about dirichlet process mixtures and so on and i think he

489
00:30:24,010 --> 00:30:27,310
he didn't quite finish telling you about them but i'm going to assume he told

490
00:30:27,310 --> 00:30:30,300
you about them but i think for the purposes of what i'm doing you probably

491
00:30:30,300 --> 00:30:31,810
know most of what you need to know

492
00:30:31,820 --> 00:30:35,900
but anyway the basic idea is these are models which unlike the finite mixture have

493
00:30:36,230 --> 00:30:38,010
actually infinite number of

494
00:30:38,010 --> 00:30:39,720
of of components

495
00:30:39,720 --> 00:30:46,300
but in a sense when you do posterior inference conditioned on some finite sample only

496
00:30:46,560 --> 00:30:51,140
of small finite number of those infinite components are effectively present the act effective number

497
00:30:51,150 --> 00:30:57,180
degrees of freedom is is much smaller and kind of always just the right size

498
00:30:57,180 --> 00:30:58,840
for the for the data you have

499
00:30:58,880 --> 00:31:00,380
you know something that the model

500
00:31:00,850 --> 00:31:06,600
predict the model assumptions expressed are appropriate so radford neal introduce some of these things

501
00:31:06,600 --> 00:31:13,130
but also karl rassmussen and in the starting about ten years ago

502
00:31:13,150 --> 00:31:17,570
so that's so that so this this is one place where according to scientists again

503
00:31:17,570 --> 00:31:22,180
have been inspired by the same kinds of ideas machine learning people have been developing

504
00:31:22,180 --> 00:31:27,140
with finite and infinite mixtures of an interesting thing is actually

505
00:31:27,140 --> 00:31:30,820
the psychologists thought these things first in a certain sense so there is this paper

506
00:31:30,820 --> 00:31:35,940
by friday in holyoke in the in the early eighties backwards overnight were like in

507
00:31:35,940 --> 00:31:40,520
high school or something in which they proposed it was a little bit less elegant

508
00:31:40,520 --> 00:31:45,710
but they propose to basically i e like algorithm for learning in the gaussians mixture

509
00:31:46,060 --> 00:31:48,340
that essentially was a model of

510
00:31:48,470 --> 00:31:55,510
unsupervised and semi supervised categorisation and then there's another psychologist john anderson proposed something which

511
00:31:55,510 --> 00:31:57,890
was mathematically equivalent to

512
00:31:57,930 --> 00:32:01,560
dirichlet process mixture but he had no idea what that was but he just kind

513
00:32:01,560 --> 00:32:05,050
of derived from first principles it's it's pretty remarkable he's smart guy

514
00:32:06,510 --> 00:32:11,140
and and he showed that can be also used to describe human learning others illustrate

515
00:32:11,140 --> 00:32:16,400
a little bit about how how this goes this is a typical extremely boring tutorial

516
00:32:16,470 --> 00:32:22,250
can be computationally trivial category learning problem that people have studied in scotland psychology going

517
00:32:22,250 --> 00:32:24,480
back to the fifties

518
00:32:24,500 --> 00:32:28,290
again this is this looks more like a traditional machine learning problem that there's a

519
00:32:28,290 --> 00:32:33,510
few positive examples of few negative examples so this is the category label here the

520
00:32:33,510 --> 00:32:38,670
training set and test set the actual stimuli in these experiments

521
00:32:38,680 --> 00:32:45,310
are formally represented by a vector of a few binary features

522
00:32:45,320 --> 00:32:48,300
what they actually look like in in the year in the light of versions including

523
00:32:48,300 --> 00:32:51,570
the first things which were done by jerome bruner and colleagues

524
00:32:51,750 --> 00:32:55,810
are these things which look like basically cards in the game is that people know

525
00:32:55,810 --> 00:32:58,000
the card game set right

526
00:32:58,100 --> 00:33:00,970
right now you raise your hand if you do i'm just curious

527
00:33:00,980 --> 00:33:04,890
OK i guess is more popular in the states

528
00:33:04,890 --> 00:33:08,840
it's a it's a game with

529
00:33:08,850 --> 00:33:13,630
basically these cards and they have one or two or three

530
00:33:13,680 --> 00:33:19,310
shapes and there either circles triangles that kind of thing actually ovals whatever they have

531
00:33:19,310 --> 00:33:25,510
different colors they could have different shading and there's some concept that defined in this

532
00:33:25,510 --> 00:33:26,880
low dimensional

533
00:33:26,890 --> 00:33:28,690
discrete vector space

534
00:33:28,710 --> 00:33:31,350
right so how you apply

535
00:33:31,350 --> 00:33:35,510
so because this is this is this is scalar so we can we can actually

536
00:33:35,510 --> 00:33:39,620
put this here and because the number of u is equal to one then these

537
00:33:39,620 --> 00:33:43,830
things cancel each other and what we are we can we can we can put

538
00:33:43,830 --> 00:33:47,040
the expectation operator in the middle

539
00:33:51,180 --> 00:33:53,040
and it's to

540
00:33:54,930 --> 00:34:01,490
and so now what the expectation thing here is nothing but stick the correlation matrix

541
00:34:04,100 --> 00:34:05,600
we've seen

542
00:34:05,620 --> 00:34:07,370
two things first of all

543
00:34:07,390 --> 00:34:12,790
principal component analysis basically boils down to looking at the correlation matrix

544
00:34:12,810 --> 00:34:18,240
of course the covariance matrix usually this cells

545
00:34:18,240 --> 00:34:20,310
and the second thing is that

546
00:34:20,350 --> 00:34:24,830
it boils down to something like an analysis of some kind of quadratic forms

547
00:34:26,580 --> 00:34:31,450
the next next slide is then how to maximise this kind of also actually we

548
00:34:31,450 --> 00:34:36,010
want to minimize the quadratic form a quadratic form

549
00:34:36,040 --> 00:34:39,540
now again let's try to understand this

550
00:34:39,540 --> 00:34:42,790
so what do we want to minimize this kind of a quadratic form under the

551
00:34:42,790 --> 00:34:47,470
constraint that the norm is equal to one another and with the gradient of this

552
00:34:48,810 --> 00:34:51,830
so this is the this is the the condition

553
00:34:51,830 --> 00:34:57,350
sorry i said to your school talk conversations but actually i think that's kind of

554
00:34:57,970 --> 00:35:04,060
exaggeration because that those are the more general conditions that are used in inequality constraints

555
00:35:04,060 --> 00:35:08,660
that this is just like the the classical like harsh conditions

556
00:35:08,680 --> 00:35:15,910
so we have the gradients mine is proportional i minus some constant times the gradient

557
00:35:15,910 --> 00:35:18,810
of the of the constant equals zero

558
00:35:19,120 --> 00:35:21,310
now it happens that the gradients

559
00:35:21,330 --> 00:35:25,390
well OK michael also use is not gradient but the

560
00:35:25,660 --> 00:35:30,120
that the transpose of the gradient which is more properly speaking the derivative but it

561
00:35:30,120 --> 00:35:36,280
doesn't make any difference and so that the data that is equal to its action

562
00:35:36,280 --> 00:35:38,490
is the corresponding linear function

563
00:35:38,510 --> 00:35:41,790
this is an interesting point that because the

564
00:35:41,790 --> 00:35:46,850
the gradient of the quadratic form is the corresponding linear function with same matrix multiplied

565
00:35:46,850 --> 00:35:47,950
by two

566
00:35:48,010 --> 00:35:53,780
and then he well it's had the same thing because it's also quadratic with an

567
00:35:53,780 --> 00:35:56,680
identity matrix in the middle

568
00:35:56,700 --> 00:36:01,160
so this this equations when we just put this that the tools out and with

569
00:36:01,160 --> 00:36:04,080
this you on the other side it gives you this

570
00:36:04,080 --> 00:36:08,560
and this is nothing but the definition of an idea value of the matrix

571
00:36:10,430 --> 00:36:14,850
this is actually a very general general phenomenon

572
00:36:14,850 --> 00:36:17,870
usually when we have some kind of a quadratic functions

573
00:36:17,890 --> 00:36:20,370
as a result of quadratic forms

574
00:36:20,390 --> 00:36:24,810
or to maximize the solutions will be given by some kind of like like and

575
00:36:27,540 --> 00:36:32,060
because you just get this centers kind of equations and here well then it depends

576
00:36:32,060 --> 00:36:36,760
on what depending on the situation it you have different i can values that you

577
00:36:36,760 --> 00:36:42,390
want sometimes you have the largest eigen values sometimes the smallest values sometimes something in

578
00:36:42,390 --> 00:36:42,660
the middle

579
00:36:43,060 --> 00:36:44,660
so what happens here

580
00:36:44,760 --> 00:36:46,140
is that

581
00:36:46,180 --> 00:36:49,780
we should actually find the

582
00:36:49,850 --> 00:36:52,680
the largest eigen value

583
00:36:52,720 --> 00:36:58,640
sorry in this case for you you should correspond to the smallest eigenvalue of the

584
00:36:58,640 --> 00:37:03,160
correlation matrix which is a bit confusing actually w what we actually wanted to find

585
00:37:03,160 --> 00:37:07,390
in the first place which is the origin of variable corresponds to the largest eigen

586
00:37:10,370 --> 00:37:13,620
what the point is that we can do PCA

587
00:37:13,640 --> 00:37:18,720
by taking the correlation matrix and then computing the nineteen values

588
00:37:18,740 --> 00:37:25,080
and this idea that the the core of the eigen vector with the largest eigen

589
00:37:25,080 --> 00:37:30,240
value when you ask this principal component

590
00:37:30,350 --> 00:37:34,560
and as i said in the beginning is basically recursive c so you first now

591
00:37:34,700 --> 00:37:37,850
have just shown how to compute one principal component

592
00:37:37,870 --> 00:37:43,220
but in in dimensional space you will you can define in in principle components

593
00:37:43,350 --> 00:37:44,310
so the first

594
00:37:44,310 --> 00:37:49,060
i define simply so that's not so that's

595
00:37:49,080 --> 00:37:52,220
so that we

596
00:37:52,240 --> 00:37:57,080
some tracks the produce projection that we had in the first place from the data

597
00:37:57,100 --> 00:38:00,430
so we reduce the dimension of the data by one and then do exactly the

598
00:38:00,430 --> 00:38:01,120
same thing

599
00:38:01,520 --> 00:38:03,620
so what we is that we

600
00:38:03,720 --> 00:38:07,160
well in this two-dimensional case it is not very

601
00:38:07,200 --> 00:38:14,930
very obvious but the international skating international case we are successively computing components that give

602
00:38:14,950 --> 00:38:17,140
us the best approximation

603
00:38:17,600 --> 00:38:22,810
the best approximation of the data using this limited number of components actually it can

604
00:38:22,810 --> 00:38:24,280
be proven that

605
00:38:24,290 --> 00:38:25,390
the first

606
00:38:26,010 --> 00:38:32,490
i k principal components give the best approximation of the data that is possible with

607
00:38:32,490 --> 00:38:35,680
a k dimensional subspace

608
00:38:51,200 --> 00:38:56,700
yes it is one problem with PCA is that's the result depends on the units

609
00:38:56,700 --> 00:38:58,790
of measurement

610
00:39:02,370 --> 00:39:10,810
yes again this is not about the problem and the matrix correlation coefficients as defined

611
00:39:10,810 --> 00:39:16,950
in statistics as correlation coefficient as defined here and signal processing has that had the

612
00:39:16,950 --> 00:39:21,720
same problem of of depending on the scale that it depends on scale but yes

613
00:39:21,850 --> 00:39:26,310
the classical way of getting rid of this problem is to simply define the scalar

614
00:39:26,370 --> 00:39:31,060
variables so that uses you say that the variance of each variable has to be

615
00:39:31,060 --> 00:39:32,790
equal to one

616
00:39:34,330 --> 00:39:40,060
yes i mean that that kind of cells the problem but then i mean it's

617
00:39:40,100 --> 00:39:43,810
unique answer to the problem but it may not be may not be the answer

618
00:39:43,810 --> 00:39:47,180
to the second question that you want to ask in the first place because in

619
00:39:47,180 --> 00:39:52,240
some cases the units units have an intrinsic meaning and you don't want to risk

620
00:39:54,510 --> 00:39:58,310
but it depends on the situation

621
00:39:58,310 --> 00:40:04,970
this but says this is the case first principal components give the best

622
00:40:05,350 --> 00:40:09,600
the approximation of the data in the sense that the new project the data onto

623
00:40:09,600 --> 00:40:15,790
the subspace defined by those principal components the me the expected squared error will be

624
00:40:15,790 --> 00:40:17,830
minimal so that's why

625
00:40:17,830 --> 00:40:20,850
PCA is used for dimensionality reduction

626
00:40:20,870 --> 00:40:25,950
so if you have data that has like ten thousand images for example

627
00:40:26,050 --> 00:40:31,390
and let's say for example that you want to do some computationally intensive methods that

628
00:40:31,390 --> 00:40:36,780
cannot really call ten thousand dimensions and you need to to reduce the dimension of

629
00:40:36,780 --> 00:40:38,580
the data to be able to use

630
00:40:38,600 --> 00:40:42,080
you may your favorite method then

631
00:40:42,100 --> 00:40:47,350
PCA is the classical way of reducing the dimensionality PCA you can reduce the dimension

632
00:40:47,350 --> 00:40:49,060
to save one hundred

633
00:40:49,120 --> 00:40:53,910
and you know that you have the best possible approximation of the original data that

634
00:40:54,850 --> 00:40:59,520
the best possible with one hundred dimensions

635
00:41:01,700 --> 00:41:03,040
no of course this is

636
00:41:03,910 --> 00:41:09,760
best is always extremely relative people always true some kind of optimality results for different

637
00:41:09,760 --> 00:41:14,660
kinds of methods but of course what this optimally of optimality is always in the

638
00:41:14,660 --> 00:41:16,770
well this year

639
00:41:16,810 --> 00:41:18,130
requires some

640
00:41:18,230 --> 00:41:22,040
my friend

641
00:41:25,950 --> 00:41:28,140
i was advised to

642
00:41:31,210 --> 00:41:32,210
it says

643
00:41:32,210 --> 00:41:33,460
met what

644
00:41:33,600 --> 00:41:35,030
how to make

645
00:41:35,050 --> 00:41:37,740
i will leave june twenty minutes

646
00:41:37,780 --> 00:41:41,330
i was advised to

647
00:41:41,350 --> 00:41:42,550
to clarify

648
00:41:42,560 --> 00:41:44,800
what i meant for that

649
00:41:47,820 --> 00:41:50,610
it is sort of a

650
00:41:50,650 --> 00:41:53,950
allowed title

651
00:41:54,010 --> 00:41:55,010
i wanted to

652
00:41:55,020 --> 00:41:55,820
to draw

653
00:41:58,700 --> 00:42:00,090
newspapers do

654
00:42:00,110 --> 00:42:04,580
at least here for a number of years now

655
00:42:07,050 --> 00:42:08,230
this method part

656
00:42:08,230 --> 00:42:10,870
it stands for meta programming

657
00:42:10,880 --> 00:42:11,680
i say

658
00:42:11,830 --> 00:42:15,050
meta programming is

659
00:42:15,090 --> 00:42:16,850
is a

660
00:42:16,850 --> 00:42:22,490
technique used frequently when one is trying to do what

661
00:42:22,540 --> 00:42:25,770
i'm about to present here

662
00:42:25,820 --> 00:42:28,910
and then you say that what

663
00:42:28,910 --> 00:42:29,350
i o

664
00:42:29,400 --> 00:42:32,600
that's why it's here

665
00:42:34,240 --> 00:42:35,880
the second part is definitely

666
00:42:37,320 --> 00:42:37,960
i mean

667
00:42:37,960 --> 00:42:40,290
how to make things language and twenty minutes

668
00:42:42,050 --> 00:42:43,470
strange funny

669
00:42:43,490 --> 00:42:45,960
i mean

670
00:42:46,020 --> 00:42:47,440
how can one expect to

671
00:42:47,460 --> 00:42:49,880
to make language twenty minutes

672
00:42:50,820 --> 00:42:53,940
well not very likely

673
00:42:55,930 --> 00:42:59,760
the best one could hope for is to

674
00:43:01,870 --> 00:43:03,820
make up several words

675
00:43:03,830 --> 00:43:07,470
make a dictionary of the future and then someone with

676
00:43:08,460 --> 00:43:11,770
large begin imagination imagination

677
00:43:11,820 --> 00:43:13,910
you can call it language

678
00:43:17,570 --> 00:43:20,470
this is actually called a domain specific language

679
00:43:20,470 --> 00:43:22,410
and this is what this

680
00:43:22,410 --> 00:43:26,590
but will be about

681
00:43:26,780 --> 00:43:29,790
there are

682
00:43:30,570 --> 00:43:32,660
many definitions of

683
00:43:32,700 --> 00:43:34,540
the main specific language

684
00:43:37,410 --> 00:43:41,230
like the one made by my father

685
00:43:41,290 --> 00:43:43,090
programming language

686
00:43:43,130 --> 00:43:46,410
it is it is a programming programming language

687
00:43:48,700 --> 00:43:51,100
i forgot

688
00:43:51,140 --> 00:43:51,940
how it was

689
00:43:51,950 --> 00:43:55,030
i started the reading

690
00:43:57,040 --> 00:44:01,950
it says here that its expressiveness is limited

691
00:44:02,010 --> 00:44:05,060
and it is focused on a particular domain

692
00:44:05,100 --> 00:44:08,590
and this is what it's all about

693
00:44:15,110 --> 00:44:16,820
this yesterday so

694
00:44:16,840 --> 00:44:20,100
i want to be

695
00:44:22,190 --> 00:44:24,660
a bit of explanation

696
00:44:24,670 --> 00:44:27,760
what the newspeak is actually

697
00:44:27,790 --> 00:44:30,660
it is a language that is based on english

698
00:44:30,670 --> 00:44:32,320
but has a

699
00:44:39,560 --> 00:44:42,860
and the simplified grammar

700
00:44:46,880 --> 00:44:49,720
now that is for four

701
00:44:49,730 --> 00:44:51,540
the people who have

702
00:44:51,640 --> 00:44:54,380
i read the book or seen the picture

703
00:44:54,380 --> 00:44:57,970
which was accidentally made in nineteen ninety four

704
00:44:58,000 --> 00:45:01,030
and start by

705
00:45:01,130 --> 00:45:02,690
john for that

706
00:45:02,700 --> 00:45:05,920
anyway this

707
00:45:05,940 --> 00:45:10,700
the purpose of these newspeak was too

708
00:45:12,420 --> 00:45:14,470
abolish all the words

709
00:45:14,470 --> 00:45:18,040
that was supposed to well that could inspire someone to

710
00:45:18,100 --> 00:45:20,100
think differently too

711
00:45:20,660 --> 00:45:21,970
well will

712
00:45:22,010 --> 00:45:25,140
so the words like freedom

713
00:45:27,570 --> 00:45:31,650
anyway i don't think this cell caused

714
00:45:33,680 --> 00:45:35,710
the authorities in

715
00:45:35,720 --> 00:45:38,850
as yet

716
00:45:40,030 --> 00:45:41,600
one can divide

717
00:45:45,330 --> 00:45:48,180
actually that this

718
00:45:48,180 --> 00:45:53,330
simple features vector contains the the number of matches the number of mismatches and the

719
00:45:53,330 --> 00:46:00,820
number of drops of alignment in predicting bioinformatics you can use many more complex model

720
00:46:00,870 --> 00:46:02,170
for example

721
00:46:02,180 --> 00:46:07,250
four parameter model what can distinguish between gap opening and up

722
00:46:07,300 --> 00:46:11,860
extensions so if the gap starts or he continued

723
00:46:12,290 --> 00:46:20,420
some more sophisticated more than is the one that parameter model where we compute if

724
00:46:20,590 --> 00:46:27,580
you think about forty just as you need put each possible of

725
00:46:28,540 --> 00:46:33,240
another example of a design features in sequence parting

726
00:46:33,250 --> 00:46:40,210
and even for example these three is possible and all the possible set of rules

727
00:46:40,210 --> 00:46:41,390
for three

728
00:46:41,640 --> 00:46:49,150
and in the future vector contain the affordances of features or lower in the tree

729
00:46:49,170 --> 00:46:53,960
for example that you look at one time

730
00:46:53,980 --> 00:46:57,140
so what i mean define features induce way

731
00:46:57,860 --> 00:47:02,640
such that they decompose well on this monograph we can

732
00:47:02,650 --> 00:47:09,560
b compute we can compute prediction very efficiently so solving there are lots of these

733
00:47:10,680 --> 00:47:15,690
for example for sequence labelling we can use the the and the first was alignment

734
00:47:15,710 --> 00:47:19,090
the need to my wish i could forty five

735
00:47:19,100 --> 00:47:22,860
the CYK algorithm

736
00:47:22,880 --> 00:47:23,900
i show

737
00:47:23,920 --> 00:47:30,280
the approach that we propose for solving the structure of output

738
00:47:31,730 --> 00:47:33,580
and later

739
00:47:33,600 --> 00:47:37,210
this is an extension of fisher discriminant analysis two

740
00:47:37,340 --> 00:47:44,310
learning in structured output and later the approach by georgians and others

741
00:47:44,350 --> 00:47:49,510
that is an extension of some a vector machine tools like about learning

742
00:47:49,520 --> 00:47:53,800
in our approach the idea is to

743
00:47:53,850 --> 00:47:54,940
and not

744
00:47:54,950 --> 00:47:59,840
an exponential number of possible output vector yk given an observation

745
00:47:59,850 --> 00:48:01,460
so how do we

746
00:48:01,600 --> 00:48:07,680
unable to to characterize the distribution of all possible scoring a back way

747
00:48:07,690 --> 00:48:12,910
we did we decided to put it simply the moment so simply the mean the

748
00:48:12,910 --> 00:48:15,370
violence of these distributions

749
00:48:15,390 --> 00:48:21,530
there's somebody i think we see that what they mean and dividing can be expressed

750
00:48:21,830 --> 00:48:24,480
in function of the weight vector

751
00:48:24,560 --> 00:48:29,080
and the mean vector and covariance matrix c

752
00:48:29,090 --> 00:48:34,150
can be computed efficiently with dynamic programming technique

753
00:48:35,180 --> 00:48:37,110
since we

754
00:48:37,150 --> 00:48:42,460
we approach we design our optimisation function for learning

755
00:48:42,510 --> 00:48:48,530
we decided to minimize the number of output vectors that that ever score higher than

756
00:48:48,550 --> 00:48:50,660
the score of the correct

757
00:48:50,800 --> 00:48:56,380
that we decided to what you might function the scarlet press

758
00:48:56,860 --> 00:49:02,050
this the parish want to separate as much this part of the carbon layer input

759
00:49:02,070 --> 00:49:08,320
about from the mean of the the distribution of my life was the standard deviation

760
00:49:08,340 --> 00:49:14,750
interestingly the discard can be expressed as a function of the parameters w

761
00:49:14,800 --> 00:49:18,240
we approach we got to this optimisation problem

762
00:49:18,260 --> 00:49:22,790
an equivalent optimisation problem is shown on the right side

763
00:49:22,790 --> 00:49:27,060
four in the case of one input output

764
00:49:27,080 --> 00:49:31,580
i don't know if you remember the formulation of fisher discriminant analysis that we saw

765
00:49:31,610 --> 00:49:33,930
this morning this is pretty much

766
00:49:33,950 --> 00:49:36,690
an extension of what we saw in the morning before

767
00:49:38,920 --> 00:49:43,660
replace the feature mapping by simply you

768
00:49:45,980 --> 00:49:51,960
from interacting point of view what we are doing is we're minimizing an upper bound

769
00:49:53,290 --> 00:49:55,310
what we call the ranking loss

770
00:49:55,330 --> 00:50:02,630
that counts the number of sequences the score i have been school of course the

771
00:50:02,630 --> 00:50:04,900
input output values

772
00:50:05,720 --> 00:50:13,050
so we're aware of the objective function is one of his last exactly as we

773
00:50:13,620 --> 00:50:18,550
this morning i mean the loss of the machine is an upper bound on the

774
00:50:18,550 --> 00:50:19,780
zero one loss

775
00:50:19,810 --> 00:50:25,020
we have the optimal lost that is the ranking loss is not

776
00:50:25,080 --> 00:50:27,510
differentiable so all we need to

777
00:50:27,540 --> 00:50:30,680
well defined enough about the we can three

778
00:50:30,690 --> 00:50:33,040
we do not support invasion

779
00:50:33,280 --> 00:50:35,860
i believe

780
00:50:35,860 --> 00:50:41,860
so this final opera optimisation problem that to get

781
00:50:41,880 --> 00:50:46,450
for the training set of input output pair and

782
00:50:46,460 --> 00:50:50,370
what we saw idea is

783
00:50:50,390 --> 00:50:52,640
maximizing relation

784
00:50:52,660 --> 00:50:58,600
because these approaches that about the discriminant finite and we solved by

785
00:50:58,650 --> 00:51:00,030
matrix inversion

786
00:51:00,050 --> 00:51:05,410
indeed since for practical applications since we had to deal with

787
00:51:05,420 --> 00:51:09,440
number of features we ought to develop a way to speed up

788
00:51:09,450 --> 00:51:13,910
these are the design approach because investing very large

789
00:51:13,960 --> 00:51:19,020
governance man this is not convenient

790
00:51:19,040 --> 00:51:22,460
of course what we need is not the only way to please

791
00:51:22,470 --> 00:51:26,960
factor about the learning that many of the possible approaches

792
00:51:26,960 --> 00:51:29,510
depending on the last that choose

793
00:51:29,620 --> 00:51:33,120
because we choose to minimize the number of

794
00:51:33,130 --> 00:51:37,740
you input output page that score higher than the score of the

795
00:51:37,760 --> 00:51:40,230
correct input output b

796
00:51:40,280 --> 00:51:47,170
but in some other people choose to minimize the number of uncorrect micro labels for

797
00:51:47,170 --> 00:51:51,300
example this is what is done by conditional fee

798
00:51:51,350 --> 00:51:52,970
and nice

799
00:51:53,030 --> 00:51:56,810
approach that things work well in tactics

800
00:51:56,830 --> 00:51:58,690
it is what

801
00:51:58,730 --> 00:52:00,880
has been proposed by task islands

802
00:52:00,880 --> 00:52:06,930
all the proteins inside mitochondria are imported from the cytoplasm i e

803
00:52:06,940 --> 00:52:09,410
these vestigial or bacteria

804
00:52:10,310 --> 00:52:15,630
rely on proteins coming made by the cell large the import into the mitochondria to

805
00:52:15,630 --> 00:52:21,740
supplement the small number of digital bacterial proteins which are still made here inside the

806
00:52:21,740 --> 00:52:26,240
mitochondria and used for central functions in energy production

807
00:52:26,260 --> 00:52:31,680
here's the golgi apparatus in the golgi apparatus appear is used for the production of

808
00:52:31,680 --> 00:52:35,300
membranes as one learned throughout the semester

809
00:52:35,320 --> 00:52:39,390
the membranes of the cell are in constant flux and are being pulled in and

810
00:52:39,390 --> 00:52:43,910
remodeled and regenerated the golgi apparatus is very important for that

811
00:52:43,950 --> 00:52:49,650
here's the rough endoplasmic reticulum that's important for the synthesis of proteins which are going

812
00:52:49,650 --> 00:52:53,110
to be displayed on the surface of cells you don't see them depicted here or

813
00:52:53,110 --> 00:52:56,030
going to be secreted into the extracellular space

814
00:52:56,100 --> 00:53:00,460
here the right songs which i might have mentioned briefly before and these writers some

815
00:53:00,540 --> 00:53:04,820
of the factories were protein are made again we're going to talk a lot about

816
00:53:06,200 --> 00:53:13,200
and finally several other aspects the cytoskeleton the physical integrity the architecture of the cell

817
00:53:13,200 --> 00:53:15,610
is maintained by a complex network

818
00:53:15,610 --> 00:53:20,780
of proteins which together are considered to be the cytoskeleton and they enable the cell

819
00:53:20,780 --> 00:53:22,450
to have some rigidity

820
00:53:22,460 --> 00:53:29,060
to resist tensile forces and actually to move cells can actually move from one place

821
00:53:29,060 --> 00:53:33,400
to the other they have motile properties are able to move from one location to

822
00:53:33,400 --> 00:53:39,180
another the process of cell motility if that's a word you'd like to learn

823
00:53:46,470 --> 00:53:51,500
here's what a prokaryotic cell looks like by contrast and i just want to give

824
00:53:51,500 --> 00:53:56,420
you feeling first of all it looks roughly like mitochondria and discussed before what you

825
00:53:56,420 --> 00:54:00,960
see there is the absence of a nuclear membrane there's the absence of the highly

826
00:54:01,980 --> 00:54:07,860
cycle architecture cycle always refers to cells there's is the absence of the complex of

827
00:54:07,860 --> 00:54:17,230
architecture that one associates with eukaryotic cells in fact all the a bacterium has

828
00:54:17,240 --> 00:54:21,000
what is this area in the middle it's called the nuclear oil

829
00:54:21,110 --> 00:54:25,710
a term which also will probably never here in your lifetime and represent simply an

830
00:54:25,710 --> 00:54:31,300
aggregate of the DNA of of the chromosomes of the bacteria and most bacteria

831
00:54:31,320 --> 00:54:35,860
the DNA comes is consists of only a single molecule of DNA

832
00:54:35,860 --> 00:54:41,640
which is responsible for carrying the genetic information of the bacteria there's no membrane around

833
00:54:41,640 --> 00:54:46,130
the nuclear oil and outside of area where the DNA is kept are largely right

834
00:54:46,200 --> 00:54:52,600
sums which are important for for protein synthesis there's not there's a membrane on the

835
00:54:52,600 --> 00:54:54,980
outside of is called the plasma membrane

836
00:54:55,000 --> 00:54:59,260
very similar to the plasma membrane of eukaryotic cells outside of that

837
00:54:59,310 --> 00:55:05,120
is a a mesh work that's called the the outer membrane sometimes called the cell

838
00:55:05,120 --> 00:55:09,870
wall of the bacterium which is simply there to impart structural rigidity to the bacteria

839
00:55:09,960 --> 00:55:16,090
making sure that it doesn't explode and holding it together and then there are other

840
00:55:16,420 --> 00:55:21,630
versions of eukaryotic cells here's what plant looks like and it's almost identical to the

841
00:55:21,630 --> 00:55:26,440
cells in our body except of for two major features

842
00:55:26,440 --> 00:55:28,840
for one thing it has chloroplast in it

843
00:55:28,850 --> 00:55:31,040
which are also one believes now

844
00:55:31,060 --> 00:55:37,730
the vestiges of parasitic bacteria that invaded into the cytoplasm of eukaryotic cells so additional

845
00:55:38,010 --> 00:55:39,680
condition in mitochondria

846
00:55:39,680 --> 00:55:45,460
which are responsible for energy production in all eukaryotic cells we have the chloroplast

847
00:55:45,480 --> 00:55:48,290
which are responsible for harvesting light

848
00:55:48,310 --> 00:55:54,130
and converting it into energy in plant cells the rest of the cytoplasm of the

849
00:55:54,150 --> 00:55:56,180
plan to looks pretty much the same

850
00:55:56,180 --> 00:56:01,680
one feature that i didn't really mention when i talked about an animal cell

851
00:56:01,700 --> 00:56:05,550
it is in the middle of the nucleus here you can see is a structure

852
00:56:05,550 --> 00:56:08,930
called the nuclear lists and nuclear less

853
00:56:08,930 --> 00:56:14,190
or the critically listen eukaryotic cell is responsible for making the large number of private

854
00:56:14,190 --> 00:56:20,880
songs which are exported from the nucleus into the cytoplasm and as i mentioned just

855
00:56:20,880 --> 00:56:25,250
before the rhizomes are responsible for protein synthesis

856
00:56:25,270 --> 00:56:29,240
it turns out this is a major synthetic effort on the part of most cells

857
00:56:29,280 --> 00:56:33,210
cells like our own have between five and ten million write the songs in the

858
00:56:33,210 --> 00:56:39,280
cytoplasm so it's an enormous amount of of biomass in the cytoplasm whose sole function

859
00:56:39,310 --> 00:56:41,740
is to synthesize proteins

860
00:56:41,760 --> 00:56:46,640
and we will learn also proteins that are synthesized by the riva sums don't sit

861
00:56:46,640 --> 00:56:52,250
around forever some proteins have long lives some proteins have lifetimes of fifteen minutes before

862
00:56:52,250 --> 00:56:55,270
the degraded before they are turned over

863
00:56:55,500 --> 00:57:00,890
one other distinction between ourselves that is the cells manzoor and modified are the cell

864
00:57:00,890 --> 00:57:06,400
walls analogous to the cell walls of bacteria this green thing on the outside as

865
00:57:06,400 --> 00:57:10,850
i said before we do not have cell walls around bacteria

866
00:57:10,860 --> 00:57:15,110
and we will have a semester goes on

867
00:57:15,130 --> 00:57:17,260
go into more and more details

868
00:57:17,310 --> 00:57:23,170
about different aspects of the site architecture during the first half a semester

869
00:57:23,170 --> 00:57:30,570
here for example is an artist's depiction of the endoplasmic reticulum why it has such

870
00:57:30,600 --> 00:57:35,380
complex name i can tell you but it does it's called the ER are

871
00:57:35,400 --> 00:57:40,920
in the past twelve street the you are in the endoplasmic reticulum is a series

872
00:57:40,920 --> 00:57:42,060
of membranes

873
00:57:42,090 --> 00:57:46,120
keep in mind not the only membrane the cell is the plasma membrane within the

874
00:57:46,120 --> 00:57:52,430
cytoplasm there are literally hundreds of membranes which are folded up in different ways you

875
00:57:52,430 --> 00:57:58,720
see them depicted in one set of these membranes often organised much like tools represents

876
00:58:00,420 --> 00:58:05,840
the membranes of the endoplasmic reticulum which either lacks write the songs attached to it

877
00:58:05,940 --> 00:58:10,230
or has these right prisms attached to it which causes this to be called the

878
00:58:10,230 --> 00:58:12,240
rough endoplasmic reticulum

879
00:58:12,270 --> 00:58:16,410
to refer to its rough structure which is created by the starting to write the

880
00:58:16,410 --> 00:58:20,310
songs on the surface as we will learn just trying to give you a feeling

881
00:58:20,310 --> 00:58:24,870
for the geography of what we're going to talk about this semester these riders sums

882
00:58:24,870 --> 00:58:29,750
on the surface of the endoplasmic reticulum are dedicated to the task of making highly

883
00:58:29,750 --> 00:58:35,190
specialized proteins which are either going to be dispatched to the surface of the cell

884
00:58:35,190 --> 00:58:40,920
where they will be to display and sell surface or actually secreted into the extracellular

885
00:58:41,790 --> 00:58:47,280
many of the proteins that are destined for our body are not kept within cells

886
00:58:47,280 --> 00:58:51,960
what are released into the extracellular space where they serve important functions

887
00:58:51,970 --> 00:58:54,100
and so

888
00:58:54,130 --> 00:58:58,420
we're going to focus very much on them here is actually what some of these

889
00:58:58,420 --> 00:59:03,610
things look like in the electron microscope to see whether we can either believe or

890
00:59:03,610 --> 00:59:07,620
fully discredit the imaginations of

891
00:59:07,720 --> 00:59:12,480
the the artist here's the rough endoplasmic reticulum i showed you in schematic form before

892
00:59:12,480 --> 00:59:16,900
and you can see what's called ross all these black dots are iverson's attached on

893
00:59:16,900 --> 00:59:22,280
because if you solve this using an Cholesky factorization method matrix

894
00:59:22,280 --> 00:59:28,620
with this dimension it'll scale as N cubed because A transpose A will be a matrix

895
00:59:28,620 --> 00:59:32,820
of size N by Nand if A is dense so it will be at least

896
00:59:32,820 --> 00:59:38,840
N cubed to solve this and that's what a general-purpose solver will do if you

897
00:59:38,840 --> 00:59:43,460
give this to CVX for example CVX will convert that to one of the

898
00:59:43,460 --> 00:59:50,120
standard forms and and that this matrix A transpose A now then problems like this where

899
00:59:50,120 --> 00:59:54,660
A isa very wide matrix so it has the number of rows is much

900
00:59:54,660 --> 01:00:00,860
smaller than the number of columns this is a lower rank matrix has rank M and

901
01:00:00,860 --> 01:00:05,660
you can use a trick to solve linear equations with this coefficient that's also used

902
01:00:05,660 --> 01:00:12,700
in or that's very common you can also rewrite an equation with coefficient matrix

903
01:00:12,720 --> 01:00:18,740
to get something in this form where the matrix in the coefficient is an

904
01:00:18,740 --> 01:00:25,000
identity matrix times A and then a second term A times diagonal matrix times

905
01:00:25,000 --> 01:00:29,760
A transpose and that's a trick that's actually also used in support vector machine training and

906
01:00:29,760 --> 01:00:36,580
that you may be familiar with it you have a linear kernel and the number of

907
01:00:36,580 --> 01:00:40,880
features is much less than the number of training points you get something similar in the

908
01:00:41,300 --> 01:00:48,020
standard a similar trick you can use in the Q P and the SVM training problem so but

909
01:00:48,020 --> 01:00:52,720
the point is that instead of this large matrix A transpose A of size M

910
01:00:52,730 --> 01:00:57,520
by N there N was the larger of the two dimensions you can rewrite the equations

911
01:00:57,520 --> 01:01:01,260
and reduce it to a smaller system where you have a matrix A times A

912
01:01:01,260 --> 01:01:06,020
transpose with the diagonal matrix in the middle but that doesn't make a big difference

913
01:01:06,020 --> 01:01:07,760
so now that's M by M

914
01:01:07,820 --> 01:01:12,760
and of course there's much reduce because the cost is basically a cost of

915
01:01:12,760 --> 01:01:18,280
forming this matrix at each iteration for a new diagonal matrix D that changes at

916
01:01:18,280 --> 01:01:25,340
each iteration so that takes M squared N operations to just multiply A with A transpose

917
01:01:25,400 --> 01:01:32,140
and then you will have another M cubed to actually solve this using Cholesky factorization so by

918
01:01:32,140 --> 01:01:38,440
just focusing on the linear algebra in each step and instead of solving the standard problem

919
01:01:38,440 --> 01:01:44,800
that the general-purpose solver would obtain as the Newton system in this case it can

920
01:01:44,800 --> 01:01:50,000
exploit this structure in the fact that A is a very wide matrix with M much less than N

921
01:01:50,000 --> 01:01:54,340
to do it much more efficiently and then if you use an interior point method

922
01:01:54,340 --> 01:01:59,150
and compare the the time it takes to solve with a general-purpose solver you see

923
01:01:59,200 --> 01:02:04,580
it actually scales much better for different phis of M and N this is MOSEC that's one

924
01:02:04,580 --> 01:02:10,620
of the best commercial quadratic programming solvers and this is just a custom solver that

925
01:02:10,620 --> 01:02:16,660
uses this trick to solve the newton system fast and then what's the limit of such

926
01:02:16,660 --> 01:02:21,060
a customized method well in this case for the problem we consider here at this

927
01:02:21,060 --> 01:02:28,260
regularized least-squares problem the limit will be your ability to form this matrix A A transpose

928
01:02:28,260 --> 01:02:33,120
a dense matrix of this size and then do a Cholesky factorization of it so that's

929
01:02:33,120 --> 01:02:41,140
probably around for M several thousands and so you can scale this quite far

930
01:02:41,150 --> 01:02:48,200
that when you reach ten thousands variables or matrix the dimension of that size

931
01:02:48,200 --> 01:02:52,760
this can become too expensive but certainly you can scale it much more or much better

932
01:02:52,780 --> 01:03:00,220
than a general-purpose solver by developing an custom solver so that's the model of this last

933
01:03:00,220 --> 01:03:14,940
section are there questions about this chapter on interior point methods sorry

934
01:03:14,940 --> 01:03:25,280
are there solvers which it's difficult to do it to automate it that's a very interesting question

935
01:03:25,280 --> 01:03:31,460
if for example a modeling tool would start with this it will be conceivable that it actually

936
01:03:31,460 --> 01:03:37,160
checks the dimensions of A and then chooses this or the other methods depending on

937
01:03:37,360 --> 01:03:43,740
the dimensions you're restricted because you have you're using a general-purpose solver but

938
01:03:43,740 --> 01:03:50,280
you can this is not the only quadratic programic formulation you could do something

939
01:03:50,280 --> 01:03:56,140
similar where you introduce equality constraint you call this is Y is A X minus B and then add

940
01:03:56,140 --> 01:04:03,460
equality constraints and then MOSEK will do a little better right so there's actually the formation we used already

941
01:04:03,460 --> 01:04:08,620
in the second column so there is some choices that a modeling tool can make and then

942
01:04:08,660 --> 01:04:14,140
can have a big influence on the efficiency but they would not be able to

943
01:04:14,140 --> 01:04:22,660
actually developed a custom newton equation solver right that's something that's difficult to automate

944
01:04:22,660 --> 01:04:44,280
yes a quality that's usually high but in the problem you have inequ equality constraints so is it possible to to problem where the equality constraints are formulating the dual form and inequalities and somehow some times depending on the methods it can be

945
01:04:44,280 --> 01:04:50,440
easier to solve the dual than the primal and these primal dual methods it doesn't make a

946
01:04:50,440 --> 01:04:55,500
big difference because they treat primal and dual symmetrically so actually a full fully symmetric primal

947
01:04:55,510 --> 01:04:59,760
dual method would be a method that's actually in variants respect to switching primal

948
01:04:59,760 --> 01:05:04,680
and dual so but depending on the algorithm used it can be more efficient to

949
01:05:05,040 --> 01:05:11,460
solve the dual and I think YALMIP supports has some support of that where

950
01:05:11,460 --> 01:05:20,160
you can actually automatically dualize a problem so that's nice a feature so then this

951
01:05:20,160 --> 01:05:24,280
is actually a good introduction for the last part of the tutorial so here what

952
01:05:24,650 --> 01:05:30,500
do when you s reach these limits than the linear algebra for an interior point method

953
01:05:30,500 --> 01:05:35,120
becomes so expensive because you have many ten thousand rows and columns in A so

954
01:05:35,120 --> 01:05:39,600
you can no longer solve these systems these newton systems then you have to go to

955
01:05:39,600 --> 01:05:51,540
first order methods and that's the topic of the last part of the

956
01:05:51,540 --> 01:05:58,040
tutorial so first order methods are so let's start with the and then we

957
01:05:58,040 --> 01:06:07,080
also look at some dual techniques that actually are one illustration of that question right

958
01:06:07,660 --> 01:06:12,480
so let's start with gradient methods so first order methods are basically the canonical

959
01:06:12,480 --> 01:06:16,620
or the simplest first order method of course is a simple gradient method that's

960
01:06:16,620 --> 01:06:23,520
the simplest oldest optimization method so at each iteration you compute the gradients of the function

961
01:06:23,520 --> 01:06:29,940
you minimize you figure out a good step size either a fixed step size of from

962
01:06:29,940 --> 01:06:35,460
some line search et cetera and then you make a step in that direction so

963
01:06:35,460 --> 01:06:40,750
that's a method with many advantages so every iteration is very inexpensive you don't have

964
01:06:40,760 --> 01:06:45,960
Newton equations to solve you don't have second derivatives to compute but the problems of

965
01:06:45,960 --> 01:06:52,160
this are also very well known I think so it's often very slow very sensitive

966
01:06:52,160 --> 01:06:57,500
to scaling of the problem and another problem is that it doesn't handle non-differentiable

967
01:06:57,500 --> 01:07:05,900
functions right and so that's something you probably seen in optimization courses if you have

968
01:07:05,900 --> 01:07:11,520
a simple quadratic problem and a parameter gamma here that controls the conditioning of the

969
01:07:11,520 --> 01:07:15,460
well what they are

970
01:07:15,490 --> 01:07:21,740
starting on we might be

971
01:07:21,740 --> 01:07:23,740
we find

972
01:07:23,760 --> 01:07:28,130
i don't like since i i would like to say

973
01:07:28,170 --> 01:07:31,310
when i started to ask the UN i

974
01:07:31,360 --> 01:07:34,290
we're doing poster for the

975
01:07:34,330 --> 01:07:39,000
one of the results that would still work with PCA

976
01:07:39,040 --> 01:07:40,260
which is

977
01:07:40,300 --> 01:07:44,470
that internationally well known

978
01:07:46,150 --> 01:07:47,580
i mean that doctor

979
01:07:48,000 --> 01:07:50,330
new models PCA so well

980
01:07:50,330 --> 01:07:51,330
along with my

981
01:07:54,540 --> 01:07:57,860
he so this is just one trick pony

982
01:07:57,900 --> 01:08:01,840
by that well with my soul

983
01:08:01,860 --> 01:08:04,280
six years eight

984
01:08:04,290 --> 01:08:07,680
coming up with you

985
01:08:07,730 --> 01:08:11,420
which is not the way

986
01:08:11,480 --> 01:08:13,420
signal processing

987
01:08:13,440 --> 01:08:16,890
and more like

988
01:08:16,970 --> 01:08:21,370
well that happen to know because he

989
01:08:21,420 --> 01:08:23,810
taught me on this equals

990
01:08:23,810 --> 01:08:27,290
well as

991
01:08:32,370 --> 01:08:39,780
you might

992
01:08:39,870 --> 01:08:42,970
thank you very much now for excessively kind introduction

993
01:08:43,510 --> 01:08:45,030
good morning everyone

994
01:08:45,040 --> 01:08:46,390
as it turned out

995
01:08:46,420 --> 01:08:50,860
i simplify so this is what i want to talk about today let me be

996
01:08:50,860 --> 01:08:53,870
brief outline the vision that means that he will get

997
01:08:53,900 --> 01:08:56,060
the rule is in the first half sets but

998
01:08:56,480 --> 01:08:59,590
this to atlanta too hard hopefully

999
01:08:59,640 --> 01:09:03,120
but i don't think we'll get all goes as far as we can

1000
01:09:03,200 --> 01:09:05,400
so i want to start off with

1001
01:09:05,500 --> 01:09:09,340
so the simple prolog if you like to set the scene that bayesian inference and

1002
01:09:09,340 --> 01:09:11,090
what might want to do it

1003
01:09:11,120 --> 01:09:13,120
so consider this

1004
01:09:13,120 --> 01:09:16,490
the sort of the sequence you get the opening of the bond film comes for

1005
01:09:16,490 --> 01:09:17,380
the credit

1006
01:09:17,440 --> 01:09:19,780
and it would be quite exciting

1007
01:09:19,800 --> 01:09:22,370
i just want to motivate what we're going to do that

1008
01:09:22,380 --> 01:09:25,960
and then i'll go back start looking into very simple problem and then we'll sort

1009
01:09:25,960 --> 01:09:28,090
of build up the treatment

1010
01:09:28,090 --> 01:09:31,750
this sort of linear a simple linear regression problem all start looking about how we

1011
01:09:31,750 --> 01:09:32,900
might tackle it

1012
01:09:32,940 --> 01:09:34,300
in a bayesian

1013
01:09:36,030 --> 01:09:39,680
and the idea of this talk is that eventually we get start off looking iteration

1014
01:09:39,680 --> 01:09:42,690
we get back to it in the end we may have to push this into

1015
01:09:42,690 --> 01:09:45,320
the second lecture

1016
01:09:45,370 --> 01:09:47,800
let's move on to the programme

1017
01:09:47,810 --> 01:09:49,810
so for

1018
01:09:49,870 --> 01:09:55,120
start staff with some minor interaction whose head of the idea of of arrays

1019
01:09:56,050 --> 01:09:58,250
most but not everyone

1020
01:09:58,280 --> 01:10:00,690
so some very brief history

1021
01:10:00,780 --> 01:10:04,720
there was a theologian and philosopher william of occam which is a village in surrey

1022
01:10:04,750 --> 01:10:09,990
in the fourteenth century is supposed to have said something to the effect of this

1023
01:10:11,000 --> 01:10:11,770
so what

1024
01:10:11,780 --> 01:10:14,900
that's my latin pronunciation but basically he said

1025
01:10:14,940 --> 01:10:19,840
when looking at solutions for problems the entity should not be multiplied unnecessarily

1026
01:10:21,190 --> 01:10:25,780
the in form of translation that could well just be down over complicate things

1027
01:10:25,840 --> 01:10:26,780
so his

1028
01:10:26,810 --> 01:10:30,520
he was thinking very much in the theological and philosophical context time

1029
01:10:30,770 --> 01:10:33,570
i'm sure you're familiar with some of the very complicated

1030
01:10:33,620 --> 01:10:37,060
explanations dictated by religion and theology

1031
01:10:37,150 --> 01:10:39,530
a certain physical phenomena such as the

1032
01:10:39,780 --> 01:10:41,310
going around the sun

1033
01:10:43,250 --> 01:10:48,620
he was basically talking about really we should be looking for solutions that the data

1034
01:10:48,620 --> 01:10:50,380
yet which are quite simple

1035
01:10:50,490 --> 01:10:52,220
and a modern context

1036
01:10:52,240 --> 01:10:55,840
this is still very much valid philosophy

1037
01:10:55,850 --> 01:11:00,300
with trying to solve data modelling problems and we have a number of potential solution

1038
01:11:00,340 --> 01:11:01,560
and ideally

1039
01:11:01,590 --> 01:11:04,750
we believe we should be choosing the one is the simplest we see no reason

1040
01:11:04,750 --> 01:11:06,620
to overcome

1041
01:11:06,630 --> 01:11:08,880
gets a little more difficult

1042
01:11:08,900 --> 01:11:12,060
let me start thinking about degree of solution

1043
01:11:12,070 --> 01:11:15,090
how good is the solution is not necessarily the case that we have

1044
01:11:15,130 --> 01:11:20,600
it's either right or wrong sometimes we have sort of degrees of accuracy

1045
01:11:20,680 --> 01:11:21,800
so in

1046
01:11:21,850 --> 01:11:24,660
monday data but we're interested in doing

1047
01:11:24,700 --> 01:11:27,060
want to find good solutions to problems

1048
01:11:27,180 --> 01:11:30,050
some trading off the

1049
01:11:30,080 --> 01:11:31,610
accuracy of the model

1050
01:11:31,680 --> 01:11:37,000
with its complexity and we'd like to do that and well principled and effective effective

1051
01:11:37,920 --> 01:11:39,930
and what i have shown this talk is that the

1052
01:11:39,940 --> 01:11:42,120
the ideas of bayesian inference

1053
01:11:42,130 --> 01:11:45,600
effectively embodied

1054
01:11:45,620 --> 01:11:48,620
so let me start off with just

1055
01:11:48,680 --> 01:11:51,110
kind of contrived example

1056
01:11:51,140 --> 01:11:52,850
sort of fairly simplistic

1057
01:11:52,890 --> 01:11:57,000
but it's intended to show the underlying mechanism at work

1058
01:11:57,060 --> 01:12:02,310
so imagine we have a binary communication system two symbols in theorem one

1059
01:12:02,360 --> 01:12:03,940
so we have to do

1060
01:12:04,520 --> 01:12:07,750
we have a way of sending messages is going to start off with taking an

1061
01:12:07,750 --> 01:12:11,010
arbitrary length string of zero

1062
01:12:11,060 --> 01:12:15,310
and then anyone that string at any particular place were allowed to place one of

1063
01:12:15,310 --> 01:12:16,790
these four symbols

1064
01:12:17,020 --> 01:12:21,490
we have a bit for bit two bit a single bit symbol

1065
01:12:21,500 --> 01:12:23,050
and we can construct a message

1066
01:12:23,060 --> 01:12:25,990
according to the system by placing those anywhere

1067
01:12:26,000 --> 01:12:29,990
even overlapping and then we simply or

1068
01:12:30,000 --> 01:12:32,320
we acknowledge the fact that in our system

1069
01:12:32,440 --> 01:12:36,620
we may have transmission errors which you will just say can be independent inversion of

1070
01:12:40,840 --> 01:12:42,210
imagine that

1071
01:12:42,290 --> 01:12:47,020
that we receive this particular sequence tendency

1072
01:12:47,040 --> 01:12:48,750
we're interested in knowing

1073
01:12:48,810 --> 01:12:50,290
what's the best decoding

1074
01:12:50,300 --> 01:12:54,170
i thought best in inverted commas because obviously that's a very complicated

1075
01:12:54,180 --> 01:12:57,200
we might need to be a little more formal and how we define

1076
01:12:57,250 --> 01:12:59,750
but for now we want to ask the question

1077
01:12:59,800 --> 01:13:01,290
what was the original

1078
01:13:01,320 --> 01:13:05,440
set of symbols generated the sequence received

1079
01:13:05,450 --> 01:13:07,550
so i think this

1080
01:13:07,560 --> 01:13:09,630
it's quite appealing intuitive one

1081
01:13:09,660 --> 01:13:13,080
obviously this is generated by four bit two bit symbol

1082
01:13:13,140 --> 01:13:16,910
the question is is that the most likely all the best decoding in any sense

1083
01:13:16,910 --> 01:13:19,250
can we show that form

1084
01:13:19,310 --> 01:13:21,440
so let's have a look

1085
01:13:21,490 --> 01:13:24,260
it's impossible to coding

1086
01:13:24,310 --> 01:13:25,680
so i think for

1087
01:13:25,690 --> 01:13:28,680
obviously that you can probably cook up hundreds

1088
01:13:28,730 --> 01:13:29,880
if you that knowledge

1089
01:13:29,940 --> 01:13:34,390
i pick for sort of have an obvious appeal to them in a certain sense

1090
01:13:34,440 --> 01:13:39,440
the model one is simply replace six bits required positions as we need

1091
01:13:39,490 --> 01:13:41,790
so if you like that's the most

1092
01:13:41,800 --> 01:13:44,270
of obvious model lisa

1093
01:13:44,290 --> 01:13:47,990
model two is that we place three three-team sequences

1094
01:13:48,000 --> 01:13:48,950
where we need

1095
01:13:48,960 --> 01:13:51,740
so that's the perfect depending in the data

1096
01:13:51,750 --> 01:13:53,370
all three which i hope

1097
01:13:53,450 --> 01:13:55,560
appeals to most intuitively

1098
01:13:55,620 --> 01:13:58,610
four bit symbol

1099
01:13:58,620 --> 01:14:03,100
so those three models are all perfectly coatings nothing

1100
01:14:03,110 --> 01:14:06,440
but also we might consider the factors we set the could be some noise so

1101
01:14:06,440 --> 01:14:09,660
let's just have a look at the model that requires some noise

1102
01:14:09,700 --> 01:14:11,630
in the decoding of the assumptions

1103
01:14:11,760 --> 01:14:16,180
this model just seems to single a bit sequence the most simplistic model

1104
01:14:16,250 --> 01:14:20,980
in that sense but it requires two in the these two bits are assumed to

1105
01:14:20,980 --> 01:14:23,750
be familiar

1106
01:14:24,360 --> 01:14:26,920
we're just looking at

1107
01:14:26,940 --> 01:14:29,800
some potential choices three which

1108
01:14:29,880 --> 01:14:32,130
became the sequence perfectly

1109
01:14:32,140 --> 01:14:33,830
and so forth

1110
01:14:33,850 --> 01:14:36,370
the simplest model

1111
01:14:36,410 --> 01:14:41,120
but it doesn't actually sequence precisely we have to make an assumption that night

1112
01:14:41,120 --> 01:14:44,200
and so if you take this point of view

1113
01:14:44,250 --> 01:14:48,450
the kind of algorithmic consequence of that things like supply lines

1114
01:14:48,470 --> 01:14:50,050
kernel methods

1115
01:14:50,060 --> 01:14:54,700
reproducing kernel hilbert spaces by the kernel k has certain smoothness properties

1116
01:14:54,700 --> 01:14:57,850
l two regularisation in subspaces and so on

1117
01:14:59,180 --> 01:15:04,450
these are the kinds of algorithms that arise out of this basic insight

1118
01:15:05,020 --> 01:15:06,890
now there's a second

1119
01:15:06,950 --> 01:15:09,320
o point of view

1120
01:15:09,370 --> 01:15:12,310
and the second point of view is the point of view not of smoothness in

1121
01:15:12,310 --> 01:15:13,880
the classical sense

1122
01:15:13,890 --> 01:15:17,150
but the point of view of sparsity

1123
01:15:18,410 --> 01:15:22,150
smoothness is has been known for a while i think at least since the sixty

1124
01:15:22,350 --> 01:15:28,510
so and sparsity the point of view is this that people realize that maybe the

1125
01:15:28,510 --> 01:15:32,850
function you're trying to learn is not more than any classical sense

1126
01:15:32,870 --> 01:15:35,710
but maybe it can be represented

1127
01:15:36,930 --> 01:15:39,960
in a sparse way in terms of

1128
01:15:39,970 --> 01:15:42,120
some basis functions

1129
01:15:42,140 --> 01:15:45,010
as a sparse combination of some basis functions

1130
01:15:45,010 --> 01:15:49,490
and in this setting too with might be possible to learn effectively

1131
01:15:49,530 --> 01:15:52,990
OK in other words this is the inside that the function depends on the few

1132
01:15:53,030 --> 01:15:55,350
relevant features perhaps

1133
01:15:55,400 --> 01:16:00,490
and this has an algorithmic consequence do if you take this point of view and

1134
01:16:00,490 --> 01:16:03,950
you think of things like wavelets

1135
01:16:03,970 --> 01:16:06,490
like l one regularisation

1136
01:16:06,500 --> 01:16:07,750
less so

1137
01:16:07,760 --> 01:16:09,210
compressed sensing

1138
01:16:09,220 --> 01:16:12,880
and these are algorithmic developments that somehow

1139
01:16:12,960 --> 01:16:17,330
the march out of this realization that sparsity is a good thing and can be

1140
01:16:18,700 --> 01:16:21,700
so that's the second point of view

1141
01:16:22,990 --> 01:16:25,100
and the point of view which has

1142
01:16:25,120 --> 01:16:28,070
the the the most recent of these

1143
01:16:28,090 --> 01:16:30,780
is the point of view of geometry

1144
01:16:30,800 --> 01:16:33,460
OK so that's what

1145
01:16:33,620 --> 01:16:35,720
try and explain what that means

1146
01:16:35,770 --> 01:16:40,520
and this point of view also has an outgrowth consequence

1147
01:16:40,530 --> 01:16:45,300
so just like small smoothness makes you think of supply lines and kernel methods and

1148
01:16:45,300 --> 01:16:49,940
l two regularisation and sparsity makes you think of anyone and compressed sensing less so

1149
01:16:49,940 --> 01:16:50,900
and so on

1150
01:16:50,910 --> 01:16:55,450
when you take the geometric point of view of the algorithms that the margin of

1151
01:16:55,450 --> 01:16:59,850
trying to exploit this point if you make use of the notion of graphs simplicial

1152
01:16:59,850 --> 01:17:02,940
complexes laplacians diffusions and so on

1153
01:17:02,950 --> 01:17:04,780
and we will see these objects

1154
01:17:04,790 --> 01:17:06,400
over the next

1155
01:17:06,410 --> 01:17:08,340
our so

1156
01:17:17,560 --> 01:17:19,910
OK so what is

1157
01:17:19,950 --> 01:17:21,990
the geometric point of view

1158
01:17:22,000 --> 01:17:23,530
so this is

1159
01:17:23,540 --> 01:17:25,290
somehow the

1160
01:17:25,920 --> 01:17:30,700
geometry pieces of data analysis in very high dimensional spaces OK

1161
01:17:31,120 --> 01:17:34,180
the central dogma is this

1162
01:17:34,200 --> 01:17:36,480
it's an argument in two parts

1163
01:17:36,490 --> 01:17:39,090
so the first part of the arguement says

1164
01:17:39,100 --> 01:17:40,270
that look

1165
01:17:40,280 --> 01:17:46,490
in very very high dimensional spaces the data is not going to be distributed uniformly

1166
01:17:46,540 --> 01:17:49,290
that's absurd

1167
01:17:49,310 --> 01:17:50,580
so in fact

1168
01:17:50,590 --> 01:17:54,800
the data will not be distributed uniform the distribution of the data will have some

1169
01:17:55,660 --> 01:17:58,720
if there is some shape perhaps there is some geometry to be done if you

1170
01:17:58,720 --> 01:18:00,310
want to understand the shape

1171
01:18:00,440 --> 01:18:05,540
maybe it concentrates around certain kinds of structures may be these structures are low dimensional

1172
01:18:05,790 --> 01:18:07,780
and this is going to be

1173
01:18:07,830 --> 01:18:12,070
general fact of natural data sets

1174
01:18:12,080 --> 01:18:15,020
the natural datasets are going to have

1175
01:18:15,070 --> 01:18:19,340
particular shapes in the high dimensional spaces

1176
01:18:19,350 --> 01:18:21,270
and that's based on the intuition

1177
01:18:21,290 --> 01:18:26,330
the natural data tends to be generated by systems may be physical systems are non

1178
01:18:26,330 --> 01:18:30,630
physical systems that have few underlying degrees of freedom

1179
01:18:30,910 --> 01:18:34,120
so that's the first part of the arguments of the data has some shape

1180
01:18:34,170 --> 01:18:38,420
and the second part of the arguement says that if the data has some shape

1181
01:18:40,030 --> 01:18:44,200
you can exploit the shape of the data the geometry of the data in some

1182
01:18:44,200 --> 01:18:49,430
sense to define suitable classes of functions with which to operate

1183
01:18:49,480 --> 01:18:54,530
the geometry motivated and adapted to the shape of the data

1184
01:18:54,570 --> 01:18:57,770
and developed representations

1185
01:18:57,780 --> 01:19:01,330
and develop algorithms that are adapted to the geometry of the data

1186
01:19:01,370 --> 01:19:03,560
and if this is done

1187
01:19:05,270 --> 01:19:07,500
this might allow us

1188
01:19:12,450 --> 01:19:14,200
the geometry of the data

1189
01:19:14,250 --> 01:19:16,700
to enable efficient learning

1190
01:19:24,650 --> 01:19:27,710
one is setting in which one can

1191
01:19:27,740 --> 01:19:30,480
explore this geometric teases

1192
01:19:30,500 --> 01:19:32,350
with some clarity

1193
01:19:32,410 --> 01:19:33,960
and i understand

1194
01:19:34,000 --> 01:19:37,750
what are the consequences of this teasers are

1195
01:19:37,780 --> 01:19:39,800
is the setting of the manifold

1196
01:19:39,820 --> 01:19:41,880
is the setting of manifold learning

1197
01:19:41,900 --> 01:19:47,940
so manifold learning is not a single problem

1198
01:19:47,950 --> 01:19:53,830
but rather it a collection of problems unified by some common assumptions

1199
01:19:53,990 --> 01:19:59,150
and that common assumption is this the data lives

1200
01:19:59,190 --> 01:20:02,880
on or near some low dimensional manifold

1201
01:20:02,940 --> 01:20:06,210
embedded in this high dimensional space

1202
01:20:06,270 --> 01:20:10,330
and we'll talk a little bit about how one can relax this

1203
01:20:10,340 --> 01:20:12,420
and generalize this further

1204
01:20:12,450 --> 01:20:16,980
but this is our first attempt for the first time the community has made to

1205
01:20:16,980 --> 01:20:19,730
try and make sense of the notion

1206
01:20:19,730 --> 01:20:24,990
so funny

1207
01:20:25,010 --> 01:20:27,920
chain closed set

1208
01:20:27,980 --> 01:20:32,500
take take

1209
01:20:32,510 --> 01:20:36,710
if x is in its then why is it that y i just

1210
01:20:36,730 --> 01:20:38,790
catch it does

1211
01:20:38,800 --> 01:20:44,140
so this is a big man to understand the tissue

1212
01:20:44,150 --> 01:20:48,960
this is for all n the man behind this for this as it is true

1213
01:20:48,960 --> 01:20:50,920
of all this stuff

1214
01:20:50,960 --> 01:20:54,480
these two answers that which are actually all the

1215
01:20:54,490 --> 01:20:57,260
all the positions that they can catch

1216
01:20:57,270 --> 01:21:00,080
starting from x closing

1217
01:21:01,760 --> 01:21:09,260
this is an example of what can be expressed or second OK and regarding labels

1218
01:21:10,920 --> 01:21:15,350
how it may so that i can for example

1219
01:21:15,390 --> 01:21:18,040
i else will be able to define that

1220
01:21:18,050 --> 01:21:24,010
so this is

1221
01:21:24,010 --> 01:21:27,430
just notation i don't think the

1222
01:21:27,440 --> 01:21:29,530
so in p

1223
01:21:36,010 --> 01:21:38,640
now x

1224
01:21:38,670 --> 01:21:44,310
the formulas say in my tree position x satisfies this formula

1225
01:21:44,330 --> 01:21:45,310
in terms of

1226
01:21:45,380 --> 01:21:49,190
to explain so this formula mean simply

1227
01:21:49,210 --> 01:21:51,510
from x is the path

1228
01:21:53,740 --> 01:21:57,350
come to a position where POV hold

1229
01:21:57,380 --> 01:21:59,640
soldiers later a

1230
01:22:00,930 --> 01:22:03,920
this can be expressed so i will not that it but

1231
01:22:03,940 --> 01:22:05,910
your first two

1232
01:22:05,970 --> 01:22:08,510
do this kind of tricks

1233
01:22:08,540 --> 01:22:09,840
you may try to play

1234
01:22:09,910 --> 01:22:13,980
because we can this is the part of my lectures so

1235
01:22:13,990 --> 01:22:17,590
four second talk with its two weeks if you want

1236
01:22:17,600 --> 01:22:22,190
but the idea is that you you have to express the first it's said that

1237
01:22:23,800 --> 01:22:28,670
think which was this equation we had and also to that is the least because

1238
01:22:28,720 --> 01:22:34,150
in m is easy to define inclusion in between

1239
01:22:34,160 --> 01:22:35,640
between sets

1240
01:22:35,680 --> 01:22:39,140
the popular actually this is the formula

1241
01:22:39,610 --> 01:22:44,880
but they can write if you prefer writing in this way the inclusion of x

1242
01:22:44,880 --> 01:22:46,580
in y

1243
01:22:46,590 --> 01:22:48,940
this is the formula one is it true

1244
01:22:49,000 --> 01:22:54,260
o and for every time i take x

1245
01:22:54,310 --> 01:22:55,880
well if

1246
01:22:55,880 --> 01:22:59,660
x is in x

1247
01:22:59,830 --> 01:23:04,930
in one

1248
01:23:05,000 --> 01:23:07,550
after a difficult

1249
01:23:07,580 --> 01:23:11,340
so that's because now what you have in so you have to kind of able

1250
01:23:11,370 --> 01:23:15,880
to have first of i've also reveals a second bibles

1251
01:23:15,890 --> 01:23:17,540
the sets of individuals

1252
01:23:17,560 --> 01:23:19,590
and you're able to define

1253
01:23:19,640 --> 01:23:23,190
formulas are in all of

1254
01:23:26,490 --> 01:23:30,370
so as you can see the inclusion you can talk about the list

1255
01:23:30,380 --> 01:23:32,850
set such that

1256
01:23:32,880 --> 01:23:34,080
and as i told you

1257
01:23:34,090 --> 01:23:38,260
the fixed point equation that we had the idea was to talk about that

1258
01:23:38,310 --> 01:23:41,880
set with given property and search one

1259
01:23:42,470 --> 01:23:44,540
combining then you can

1260
01:23:44,600 --> 01:23:46,600
you can see them so

1261
01:23:48,760 --> 01:23:52,580
express what we will be able to express the making

1262
01:23:53,270 --> 01:23:57,720
so what do we know about them so because it was questions

1263
01:23:57,760 --> 01:24:03,260
so it seems to be very very so the main base very very big results

1264
01:24:03,260 --> 01:24:04,330
about it

1265
01:24:04,500 --> 01:24:06,860
so the first big result is

1266
01:24:06,870 --> 01:24:08,190
OK in general

1267
01:24:08,230 --> 01:24:11,230
if you take out because structures

1268
01:24:11,250 --> 01:24:13,640
the vectors

1269
01:24:15,470 --> 01:24:17,880
like this

1270
01:24:17,980 --> 01:24:20,810
and you it is an MSO formula

1271
01:24:20,870 --> 01:24:23,420
but is interpretable in the structure mean

1272
01:24:24,390 --> 01:24:26,640
then you can not

1273
01:24:27,760 --> 01:24:31,120
because the site in terms of companies computational

1274
01:24:31,130 --> 01:24:32,500
i mean it is

1275
01:24:32,510 --> 01:24:37,080
we cannot compute whether this formula also the model

1276
01:24:37,130 --> 01:24:39,250
two what's and so

1277
01:24:39,260 --> 01:24:40,660
in general

1278
01:24:40,670 --> 01:24:43,660
so the big results like

1279
01:24:43,670 --> 01:24:51,620
characterisation of some classes of structures where it's footstool understandable it all the classes of

1280
01:24:51,630 --> 01:24:55,710
structures where you can inside and code have become really

1281
01:25:00,020 --> 01:25:01,590
so you have the domain

1282
01:25:01,640 --> 01:25:04,460
like and square

1283
01:25:04,560 --> 01:25:06,860
and you have relations like

1284
01:25:06,880 --> 01:25:09,180
i'm your less success all down

1285
01:25:09,190 --> 01:25:10,960
just to move along the way

1286
01:25:10,980 --> 01:25:13,700
and he would be undesirable for example

1287
01:25:15,600 --> 01:25:16,750
the good news to

1288
01:25:16,810 --> 01:25:18,220
is that

1289
01:25:18,360 --> 01:25:20,380
for this kind of structures

1290
01:25:20,390 --> 01:25:22,970
like the linear structure

1291
01:25:23,040 --> 01:25:24,900
and the binary tree

1292
01:25:26,270 --> 01:25:30,160
and so decided to go with

1293
01:25:30,220 --> 01:25:34,790
it's not the same as the women it's different logics one has only one of

1294
01:25:34,810 --> 01:25:36,530
unique success

1295
01:25:36,530 --> 01:25:39,740
and the other one has the notion of left and right to so you can

1296
01:25:39,740 --> 01:25:42,210
also have an arity

1297
01:25:43,970 --> 01:25:50,420
trees you need to know which one of kate exist relation what will

1298
01:25:50,430 --> 01:25:52,890
so for k

1299
01:25:52,900 --> 01:25:57,890
that's all it is the same means that you give me

1300
01:25:59,150 --> 01:26:03,130
it existed that is the blah

1301
01:26:07,930 --> 01:26:10,420
what else can i tell you this

1302
01:26:10,580 --> 01:26:11,880
there is also

1303
01:26:11,920 --> 01:26:17,640
and yes something really want to know is that the two big results very powerful

1304
01:26:17,640 --> 01:26:21,090
different ways you can think of an SVM is a big quadratic programming problem but

1305
01:26:21,090 --> 01:26:25,180
no one ever solves this like like plaits method or two

1306
01:26:25,220 --> 01:26:28,650
actually do that in in to make that

1307
01:26:28,670 --> 01:26:30,530
to be able to handle them properly in memory

1308
01:26:30,540 --> 01:26:34,170
so you can actually do a reasonable job with him and neural networks to some

1309
01:26:34,170 --> 01:26:39,770
degree and neural networks krause earlier on things but obviously it's better to do everything

1310
01:26:39,780 --> 01:26:43,390
batch because you can handle living at the same time but this is not feasible

1311
01:26:43,440 --> 01:26:49,460
and classifiers that are computationally expensive to do

1312
01:26:49,480 --> 01:26:51,570
exhaustively search

1313
01:26:51,580 --> 01:26:55,210
so the first successful one was done by

1314
01:26:55,670 --> 01:27:00,000
henry rally who he's google mail

1315
01:27:00,060 --> 01:27:03,150
and my boss moments ago canada

1316
01:27:03,180 --> 01:27:08,780
and they had a lot of success in applying artificial neural networks

1317
01:27:08,800 --> 01:27:10,170
essentially all the people

1318
01:27:10,180 --> 01:27:12,440
what's that

1319
01:27:12,490 --> 01:27:13,980
they went through

1320
01:27:14,030 --> 01:27:17,360
and there was looking for something in translation and scale

1321
01:27:18,170 --> 01:27:22,050
they say that every i six pixels in translation

1322
01:27:22,060 --> 01:27:27,830
they subsampled image rather than substance classifier this subsample the image

1323
01:27:27,880 --> 01:27:32,650
and the way they subsampled these in gases pyramids again to guard against using work

1324
01:27:32,710 --> 01:27:37,570
we touched upon before and what they do that they go through the candidate across

1325
01:27:37,570 --> 01:27:41,330
all those different kinds of you can think of essentially searching like a big

1326
01:27:41,340 --> 01:27:44,560
set of points in in in what space

1327
01:27:44,610 --> 01:27:47,750
i think that's a much more constructive kind of thinking all these pyramids and things

1328
01:27:47,840 --> 01:27:50,510
that sister what space on the sampling through

1329
01:27:50,550 --> 01:27:54,290
and in each of these individual points they have candidate image

1330
01:27:55,380 --> 01:28:00,370
and actually do some perhaps my stuff here the critical i think so take the

1331
01:28:00,370 --> 01:28:01,730
mean the going away

1332
01:28:01,820 --> 01:28:07,080
we also did some nice things actually where they fit in a least squares planes

1333
01:28:07,130 --> 01:28:11,320
so you can actually look like a plane across images of elimination is kind of

1334
01:28:11,380 --> 01:28:15,640
going like that you can actually fit leastsquares plane and remove some of the elimination

1335
01:28:15,650 --> 01:28:16,960
artifacts that way

1336
01:28:16,970 --> 01:28:22,590
then the histogram equalised essentially they put into collecting all school work and i think

1337
01:28:22,590 --> 01:28:25,000
it was three like everywhere

1338
01:28:27,740 --> 01:28:29,490
multilayer perceptron

1339
01:28:29,530 --> 01:28:32,640
and in situation that and they did some

1340
01:28:32,690 --> 01:28:36,780
they have some kind of funky stuff again because it was difficult to train the

1341
01:28:36,780 --> 01:28:42,880
classifiers the features they sometimes use like actual strawbs of images and things like that

1342
01:28:42,880 --> 01:28:46,980
rather than using each individual pixel things so they had a lot of ad hoc

1343
01:28:46,980 --> 01:28:51,790
measures in that but for the time know you know it was quite advanced and

1344
01:28:51,790 --> 01:28:54,970
the big thing with this and the big thing with a lot of these detectors

1345
01:28:54,970 --> 01:28:57,280
wasn't so much schooling to

1346
01:29:00,770 --> 01:29:06,110
i wasn't so much the learning to make it was really the data

1347
01:29:06,130 --> 01:29:09,470
and when you can look at these things whatever

1348
01:29:09,490 --> 01:29:13,190
four for a lot of these things it's really data specific if you've got a

1349
01:29:13,190 --> 01:29:15,010
really good data

1350
01:29:15,020 --> 01:29:16,720
a lot of the time you do better

1351
01:29:17,000 --> 01:29:20,690
rather than kind of perhaps playing too much with the learning algorithm when it comes

1352
01:29:20,690 --> 01:29:25,060
to this sort of stuff that's not rule one because obviously there there's has been

1353
01:29:25,060 --> 01:29:29,030
a lot of improvement in face detectors and things and there's a lot of kind

1354
01:29:29,030 --> 01:29:32,650
of several things that you can do but in general if you got a good

1355
01:29:32,650 --> 01:29:35,890
dataset you cannot do better than with the weak dataset so with these sort of

1356
01:29:35,890 --> 01:29:40,250
things that has benefited a lot from having thousands if not millions of images thrown

1357
01:29:40,530 --> 01:29:42,950
in general as well

1358
01:29:42,960 --> 01:29:47,760
and see this some examples is collected classic once again because it's not paper because

1359
01:29:47,760 --> 01:29:54,250
people can dress finally if and everyone space think got negative examples here

1360
01:29:54,320 --> 01:29:57,010
people walking off even this is kind of

1361
01:29:57,020 --> 01:30:00,570
i don't know if this is the result or bad found

1362
01:30:00,580 --> 01:30:04,100
i why they put in there but they have the found smiley face so i

1363
01:30:04,100 --> 01:30:06,410
guess i can find ronald mcdonald to one

1364
01:30:09,520 --> 01:30:14,790
the first jump into using support vector machines for this was by a sort of

1365
01:30:14,800 --> 01:30:18,990
freud in accuracy in ninety seven face detection

1366
01:30:19,000 --> 01:30:22,150
and i think is about this in this kind of

1367
01:30:22,200 --> 01:30:26,360
my editorial rather than fact i think so what it was a lot more elegant

1368
01:30:26,360 --> 01:30:31,870
because essentially instead of kind of choosing heuristic features choosing things hawkwind things that this

1369
01:30:31,870 --> 01:30:32,700
entry just

1370
01:30:32,720 --> 01:30:36,090
put the pixels in this put the pixels into the classifier and then led to

1371
01:30:36,090 --> 01:30:39,490
its business and as we know experience a lovely for this sort of stuff because

1372
01:30:39,510 --> 01:30:44,680
can deal with last mensional problems instead of being built by the cos dimensionality we

1373
01:30:44,680 --> 01:30:47,870
have this lovely margin thing become cyberspace and so

1374
01:30:47,960 --> 01:30:55,570
essentially they had a nice heuristic feature selection and obtained reasonable results but there regional

1375
01:30:55,570 --> 01:30:57,650
results for non linear SVM

1376
01:30:57,760 --> 01:31:01,700
now the problem with that is that obviously with the linear SVM i can kinda

1377
01:31:01,700 --> 01:31:02,810
sum up all my

1378
01:31:02,890 --> 01:31:07,410
support values and my support vectors and put into a single template which i can

1379
01:31:07,410 --> 01:31:13,160
then evaluate really quickly across the image but this is that non-linear computational costs go

1380
01:31:13,160 --> 01:31:16,700
up and this is the real problem with the with the first what

1381
01:31:16,800 --> 01:31:19,630
it was quite computationally expensive

1382
01:31:19,670 --> 01:31:24,790
as it could contain hundreds if not thousands of support

1383
01:31:27,790 --> 01:31:33,300
was proposed was cascade classifier cascade crest was not new but again

1384
01:31:33,960 --> 01:31:36,840
the big thing with it wasn't that is going to form any that it wasn't

1385
01:31:36,840 --> 01:31:40,230
going to be able to separate one class from another class the big thing was

1386
01:31:40,230 --> 01:31:46,060
the search was that can i still maintain reasonable performance while doing effective search

1387
01:31:46,060 --> 01:31:51,920
so essentially what a cascade classifier and the kind of course spoke about four

1388
01:31:51,980 --> 01:31:55,320
is that you can kind of a great deal so

1389
01:31:55,330 --> 01:32:01,070
you might be extract number of features from your face image things all you might

1390
01:32:01,070 --> 01:32:02,190
be able to say

1391
01:32:02,480 --> 01:32:09,080
to simplify is being same only two or three support vectors rather than or in

1392
01:32:09,080 --> 01:32:10,130
support vectors

1393
01:32:10,200 --> 01:32:14,920
and what we working threshold to decide these two three support vectors and i've got

1394
01:32:14,980 --> 01:32:17,550
threshold that no amount to classify say

1395
01:32:17,580 --> 01:32:19,160
ninety percent of all

1396
01:32:19,170 --> 01:32:23,730
why a training set reasonably well in the other ten percent i i can still

1397
01:32:23,860 --> 01:32:24,720
to do

1398
01:32:24,720 --> 01:32:26,100
this corresponds to

1399
01:32:26,280 --> 01:32:36,870
these this

1400
01:32:40,950 --> 01:32:43,720
this is the third operation

1401
01:32:43,720 --> 01:32:46,550
which is very important when you're looking at the

1402
01:32:46,600 --> 01:32:50,280
stability problem interesting graph relations

1403
01:32:50,300 --> 01:32:52,470
it's the

1404
01:32:52,490 --> 01:32:55,910
projection operations so

1405
01:32:55,970 --> 01:33:00,240
the idea in the project several operations that take

1406
01:33:00,280 --> 01:33:02,840
relation a binary relation to

1407
01:33:03,160 --> 01:33:05,510
what of the relations

1408
01:33:07,340 --> 01:33:08,430
make it

1409
01:33:08,470 --> 01:33:11,760
smaller by eliminating some of the columns

1410
01:33:11,820 --> 01:33:14,240
so the reason why you might be

1411
01:33:14,260 --> 01:33:18,280
interestingly in many columns is for example

1412
01:33:18,320 --> 01:33:24,490
in this case so if we are talking about the recently by two actions here

1413
01:33:24,550 --> 01:33:28,570
in this section but taking the section then possible

1414
01:33:28,590 --> 01:33:30,890
basically societies

1415
01:33:30,950 --> 01:33:36,620
maybe maybe we are not interested in the intermediate states or simply want to know

1416
01:33:37,820 --> 01:33:39,600
what states

1417
01:33:39,620 --> 01:33:42,800
here are examples from these states here

1418
01:33:43,490 --> 01:33:46,820
some unspecified intermediates

1419
01:33:47,490 --> 01:33:51,090
it is in terms of relational operations what we are interested in is the choice

1420
01:33:51,090 --> 01:33:56,180
of the projection operation relations so the project these

1421
01:33:56,220 --> 01:33:59,300
relation to columns zero cool

1422
01:33:59,360 --> 01:34:03,410
eliminating column y

1423
01:34:03,490 --> 01:34:09,970
other operators in the reasonably thing we will be using these

1424
01:34:09,990 --> 01:34:14,140
this is predictability

1425
01:34:14,160 --> 01:34:15,870
well it's a b

1426
01:34:15,870 --> 01:34:18,090
restrict to

1427
01:34:18,090 --> 01:34:20,740
he stays zero one

1428
01:34:20,760 --> 01:34:26,410
so this is like one joining his former this one here we

1429
01:34:27,200 --> 01:34:30,240
a zero and the zero

1430
01:34:30,240 --> 01:34:31,550
so we have

1431
01:34:31,590 --> 01:34:33,820
these relations here

1432
01:34:33,890 --> 01:34:38,280
and now we are interested in those states that can be reached from that initial

1433
01:34:38,280 --> 01:34:39,070
state by

1434
01:34:39,450 --> 01:34:40,890
these two actions

1435
01:34:40,910 --> 01:34:42,390
so be

1436
01:34:42,390 --> 01:34:49,930
project this relation to the third world eliminating the column zero or one

1437
01:34:49,930 --> 01:34:52,360
and what we get is the formula

1438
01:34:52,370 --> 01:34:53,840
that represents

1439
01:34:53,910 --> 01:34:54,600
these two

1440
01:34:55,700 --> 01:34:57,200
well yes

1441
01:34:57,240 --> 01:34:58,510
and that's the

1442
01:34:58,510 --> 01:35:03,950
set of states that can be reached from the initial state by taking those actions

1443
01:35:04,490 --> 01:35:06,700
it's about people with doing so

1444
01:35:06,740 --> 01:35:10,450
in some of what you will be doing elementary school

1445
01:35:10,470 --> 01:35:13,070
as i pointed out that corresponds

1446
01:35:13,390 --> 01:35:14,950
initial states

1447
01:35:15,010 --> 01:35:17,780
and then we'll have relations

1448
01:35:17,840 --> 01:35:20,860
that correspond to different actions

1449
01:35:20,870 --> 01:35:22,510
and we compute

1450
01:35:22,510 --> 01:35:25,050
all possible states that can be reached by

1451
01:35:25,070 --> 01:35:28,340
taking on one of the axioms in one step

1452
01:35:28,340 --> 01:35:32,200
then for the fourth step three steps forward and so on

1453
01:35:32,240 --> 01:35:34,740
and then interesting whether we have already

1454
01:35:34,760 --> 01:35:38,570
because this what we are simply interested in this sense of space that can be

1455
01:35:38,570 --> 01:35:42,280
reached by taking one two three four and so on

1456
01:35:43,470 --> 01:35:45,490
for these purposes we need

1457
01:35:45,550 --> 01:35:49,950
projection which

1458
01:35:49,990 --> 01:35:52,030
so the projection operation

1459
01:35:59,430 --> 01:36:00,930
eliminating call

1460
01:36:02,370 --> 01:36:04,510
that is the projection operation

1461
01:36:04,820 --> 01:36:06,820
these logical

1462
01:36:06,840 --> 01:36:09,870
o operation that does exactly that

1463
01:36:09,930 --> 01:36:12,760
so basically what happens is that

1464
01:36:12,820 --> 01:36:15,910
we have a formula phi

1465
01:36:16,620 --> 01:36:18,570
in this case

1466
01:36:21,090 --> 01:36:23,430
propositional variables is zero

1467
01:36:23,450 --> 01:36:26,760
baby zero a one b one

1468
01:36:26,820 --> 01:36:29,360
and we want to produce the formula

1469
01:36:29,360 --> 01:36:32,910
only says something about the second column here

1470
01:36:32,930 --> 01:36:37,070
and this will be a formula works variables one b

1471
01:36:38,890 --> 01:36:42,530
variables easier and easier

1472
01:36:46,050 --> 01:36:48,680
logical operators will be denoted by

1473
01:36:48,680 --> 01:36:50,620
exists are

1474
01:36:50,640 --> 01:36:52,260
where are you the

1475
01:36:52,280 --> 01:36:57,160
of frozen variables that are really for

1476
01:36:58,410 --> 01:37:02,370
it turned out that this is exactly corresponds to the

1477
01:37:02,390 --> 01:37:09,410
relations of production which

1478
01:37:11,430 --> 01:37:14,590
that is so simple that is almost ridiculous

1479
01:37:14,740 --> 01:37:19,990
so the elimination of one operation simply takes place by

1480
01:37:19,990 --> 01:37:23,870
making two copies of the formula

1481
01:37:23,910 --> 01:37:26,180
and one copy

1482
01:37:26,220 --> 01:37:28,910
replace the the variable y

1483
01:37:28,970 --> 01:37:33,840
because that's all i mean the article being replaced the former that problem that the

1484
01:37:33,840 --> 01:37:34,840
are able

1485
01:37:34,910 --> 01:37:39,410
why because falls and then you his former boss

1486
01:37:39,430 --> 01:37:40,660
from the fact

1487
01:37:41,390 --> 01:37:44,010
that's it

1488
01:37:44,010 --> 01:37:46,010
this and related operations

1489
01:37:46,180 --> 01:37:50,840
that that extends laterally from ocean there's also universal approach operation is the same except

1490
01:37:51,780 --> 01:37:56,010
is that is time to have contact something real

1491
01:37:56,030 --> 01:37:57,640
do not using this

1492
01:37:57,640 --> 01:37:59,640
in this

1493
01:38:02,740 --> 01:38:05,490
so this is eliminating one

1494
01:38:05,550 --> 01:38:07,620
variable formula

1495
01:38:07,790 --> 01:38:09,410
this notation here

1496
01:38:09,430 --> 01:38:14,360
we had a set of variables so the variable is eliminated by simply by simplified

1497
01:38:14,390 --> 01:38:16,570
and in doing so else

1498
01:38:16,620 --> 01:38:17,840
i mean

1499
01:38:20,280 --> 01:38:24,200
the result the same so essentially same this formula by

1500
01:38:26,840 --> 01:38:31,950
as many copies of this form of quite as they are the values of these

1501
01:38:31,950 --> 01:38:33,590
variables here

1502
01:38:33,600 --> 01:38:35,370
and in each copy

1503
01:38:36,550 --> 01:38:38,740
typical this one falls

1504
01:38:48,160 --> 01:38:50,410
this example

1505
01:38:50,410 --> 01:38:53,140
we have two implication a implies b

1506
01:38:53,160 --> 01:38:54,660
and the by c

1507
01:38:54,720 --> 01:38:56,160
everybody knows you

1508
01:38:57,100 --> 01:38:59,340
also also a policy

1509
01:38:59,340 --> 01:39:00,590
you can use that

1510
01:39:00,590 --> 01:39:01,920
now that i said that

1511
01:39:01,940 --> 01:39:07,900
i hope that's OK but i'm making it OK

1512
01:39:08,170 --> 01:39:13,570
anyway and so that they take a different tact which is and it's an interesting

1513
01:39:14,610 --> 01:39:16,360
where you say how

1514
01:39:16,380 --> 01:39:20,360
how positive is something how negative something how nature something as

1515
01:39:20,380 --> 01:39:25,360
OK so that's what i've the also

1516
01:39:25,380 --> 01:39:28,710
i'm going to point out of paper later that has the similar idea of having

1517
01:39:28,710 --> 01:39:32,710
fuzzy membership in these categories

1518
01:39:32,720 --> 01:39:40,150
there's many other ways many other related kinds of definitions and annotation schemes and just

1519
01:39:40,150 --> 01:39:46,650
for example you could do an motions is a lot of typologies of emotions appraisal

1520
01:39:46,780 --> 01:39:53,990
moods humor as are related you could have different structures and human how we were

1521
01:39:53,990 --> 01:39:55,110
interested in

1522
01:39:55,130 --> 01:40:00,190
not just the individual expressions of opinions but what is the reason that you have

1523
01:40:00,190 --> 01:40:04,130
an opinion you know a bigger structure coming back to the seventies and eighties where

1524
01:40:04,130 --> 01:40:10,090
you had like twenty minutes work on plot units had structured structured opinions in many

1525
01:40:10,090 --> 01:40:14,070
ways so there's a lot of other things you could do as well

1526
01:40:14,110 --> 01:40:19,190
my our focus is on the linguistic subjectivity and we would hope that would match

1527
01:40:19,190 --> 01:40:22,710
up with the kind of media with people that are

1528
01:40:22,720 --> 01:40:29,780
working on representing the actual opinion or private state that's being expressed also in and

1529
01:40:29,780 --> 01:40:33,650
we have a section enough to be able to go through and product review mining

1530
01:40:33,860 --> 01:40:40,010
and they often structure things specifically with respect to attribute and

1531
01:40:40,030 --> 01:40:43,300
the product and attitude towards the sun

1532
01:40:43,320 --> 01:40:48,240
you make point about gold standard

1533
01:40:48,260 --> 01:40:52,380
so gold standards meaning the thing you're using to evaluate system

1534
01:40:52,400 --> 01:40:58,150
so you can derive them from manually annotated data obviously and given annotation schemes you

1535
01:40:58,820 --> 01:41:05,070
back to sentence level say is there any subjectivity in a sentence or whatever also

1536
01:41:05,070 --> 01:41:08,820
very interestingly people have used a lot of what i would call found data

1537
01:41:08,880 --> 01:41:11,630
so the people

1538
01:41:11,650 --> 01:41:15,070
and by the way when i'm using these keep putting examples

1539
01:41:15,130 --> 01:41:17,260
and putting a citation these are not

1540
01:41:17,280 --> 01:41:20,590
an exhaustive list of citations i just kind of found one or two and put

1541
01:41:20,590 --> 01:41:21,880
them here

1542
01:41:22,170 --> 01:41:24,860
so blog tags

1543
01:41:24,960 --> 01:41:30,340
and all these websites for complaints political arguments and so on so

1544
01:41:31,760 --> 01:41:35,240
amazon dot com reviews

1545
01:41:35,260 --> 01:41:37,090
complaints that comes

1546
01:41:37,110 --> 01:41:40,720
there must be a lot of fun to read complaints that can

1547
01:41:40,740 --> 01:41:42,820
but you know their complaints

1548
01:41:44,190 --> 01:41:50,940
we have to learn to use these bitter lemons that com which is you know

1549
01:41:50,940 --> 01:41:54,070
when you get on there which is the

1550
01:41:54,090 --> 01:41:57,940
it's from the palestinian or israeli point of view so it's found in the sense

1551
01:41:57,940 --> 01:41:59,480
that the tag is given to you

1552
01:41:59,510 --> 01:42:00,840
right it does

1553
01:42:00,860 --> 01:42:04,980
so with humor and it doesn't interesting stuff like that so anyway there's a lot

1554
01:42:04,980 --> 01:42:08,690
of that and also people can

1555
01:42:08,720 --> 01:42:13,940
they often compare the results of their system two

1556
01:42:13,960 --> 01:42:18,490
word lists that are out there for example the journal inquirer mention the second so

1557
01:42:18,490 --> 01:42:22,860
there's different ways you can define the gold standard

1558
01:42:43,320 --> 01:42:51,630
now it's really interesting i don't know get so that's what will always contextual polarity

1559
01:42:51,630 --> 01:42:56,460
it's what we're talking about so we define prior polarity to be what you put

1560
01:42:56,460 --> 01:43:02,030
in the lexicon so without that's prior the prior probability without any evidence from any

1561
01:43:02,030 --> 01:43:07,320
text that is going to be that but in the context of the overtaxed many

1562
01:43:07,320 --> 01:43:13,090
different things can flip it around there's a really nice paper that a lot of

1563
01:43:13,900 --> 01:43:18,590
light in the spring symposium on attitude and the fact that we had in two

1564
01:43:18,590 --> 01:43:22,070
thousand four by pliny insane in

1565
01:43:22,070 --> 01:43:26,920
and they talked about polarity influencers all these different things in in context can flip

1566
01:43:26,920 --> 01:43:28,110
around the polarity

1567
01:43:28,110 --> 01:43:32,800
my guess i give this example because it was a real example that we found

1568
01:43:32,800 --> 01:43:35,820
is only they had failed to break their spirit

1569
01:43:35,840 --> 01:43:38,460
so spirit is good

1570
01:43:38,480 --> 01:43:40,090
making his spirit is

1571
01:43:41,150 --> 01:43:43,940
failing to break his spirit is good

1572
01:43:43,940 --> 01:43:44,840
but then

1573
01:43:44,860 --> 01:43:46,570
if only they had

1574
01:43:46,590 --> 01:43:51,650
it takes away that it even happened so it's really there's about two two groups

1575
01:43:51,650 --> 01:43:55,610
have done some kind of social experiments using lots of features

1576
01:43:55,650 --> 01:43:59,480
we did that we kind of got better than baseline but there's a lot of

1577
01:43:59,480 --> 01:44:01,170
work to do in that area

1578
01:44:01,170 --> 01:44:05,400
so although i would say in that case it's not half bad you know what

1579
01:44:05,400 --> 01:44:06,840
i would like to see

1580
01:44:06,860 --> 01:44:10,690
i'm not really talk about it but

1581
01:44:10,710 --> 01:44:14,580
so you have word senses as the extreme and then you have something like a

1582
01:44:14,580 --> 01:44:17,710
construction which is not necessarily

1583
01:44:17,720 --> 01:44:20,460
a fixed phrase that you put in wordnet

1584
01:44:20,460 --> 01:44:22,220
but it's a usage

1585
01:44:22,240 --> 01:44:23,190
and so

1586
01:44:23,210 --> 01:44:25,780
past that i would think you want to let you want to be able to

1587
01:44:25,780 --> 01:44:30,630
try to find that as the usage is that because it's not really completely open

1588
01:44:30,630 --> 01:44:33,780
to the context

1589
01:44:33,800 --> 01:44:38,070
but the and as usual one is that's good you know that and as usual

1590
01:44:38,070 --> 01:44:41,760
and as usual that seems really open so this is continuing from

1591
01:44:41,780 --> 01:44:45,280
the dictionary all the way to pure context

1592
01:44:45,320 --> 01:44:47,090
which is interesting

1593
01:44:47,090 --> 01:44:51,190
so lots of work to do on this

1594
01:44:56,340 --> 01:45:03,110
probably you know well

1595
01:45:03,130 --> 01:45:06,070
could use that made could use the positive

1596
01:45:06,070 --> 01:45:11,340
falls and this is

1597
01:45:11,340 --> 01:45:15,290
the wonderful world of independent component analysis

1598
01:45:15,300 --> 01:45:18,190
so as i mentioned yesterday

1599
01:45:18,210 --> 01:45:20,220
that kind of

1600
01:45:20,240 --> 01:45:25,700
historical background for ICA is this idea of blind source separation so we have a

1601
01:45:25,700 --> 01:45:31,280
number of sources sigma that emit some kind of signals and then these signals could

1602
01:45:31,290 --> 01:45:36,960
somehow mixed together in some medium and that we want to separate these sources

1603
01:45:37,270 --> 01:45:43,570
in a blind man we're blind means meaning minimum a prior information on on the

1604
01:45:43,570 --> 01:45:48,780
source signals minimum pricing information on the source signals or the mixing

1605
01:45:48,870 --> 01:45:52,820
so it's a very simple simulation example where we have these

1606
01:45:52,880 --> 01:45:59,180
also signals which are then mixed up together by random linear coefficients

1607
01:45:59,190 --> 01:46:04,330
and then using ICA we can then recover the original signals

1608
01:46:04,740 --> 01:46:10,930
as the independent components so how does this thing work

1609
01:46:11,060 --> 01:46:14,240
well what this works by defining

1610
01:46:14,290 --> 01:46:20,610
a linear latent variable model as you might call so x i

1611
01:46:20,620 --> 01:46:25,060
he's x dx i are random variables

1612
01:46:25,080 --> 01:46:30,210
there instance random variables and each of them is modeled as a linear sum of

1613
01:46:30,210 --> 01:46:34,260
certain hidden variables called as i j

1614
01:46:34,280 --> 01:46:40,630
and the point is that each of these observed variables is a linear combination with

1615
01:46:40,630 --> 01:46:46,390
one of these sort of these independent components with certain coefficients in the i j

1616
01:46:46,520 --> 01:46:52,620
so x i and they are random variables whereas the AIJ are parameters that give

1617
01:46:52,860 --> 01:46:57,910
us the properties that the unknown parameters which we don't know and the whole distance

1618
01:46:57,910 --> 01:47:01,240
estimates these unknown parameters a i j

1619
01:47:01,250 --> 01:47:04,500
and once we know that the parameters i j

1620
01:47:04,510 --> 01:47:12,750
we can then possibly kind inverts this linear equation and then of the independent components

1621
01:47:12,840 --> 01:47:14,480
so this is the case

1622
01:47:14,490 --> 01:47:17,210
his an unsupervised learning books

1623
01:47:17,390 --> 01:47:22,770
this the case in unsupervised learning which means that we are we only observe this

1624
01:47:23,080 --> 01:47:28,320
these variables x y or one of the random vector x i mean if this

1625
01:47:28,690 --> 01:47:32,590
if we for example observed both x and s

1626
01:47:32,610 --> 01:47:36,480
and the only that the only thing left would be to estimate the AIJ that

1627
01:47:36,480 --> 01:47:41,110
would be a classical case of supervised learning which people to solve by basically by

1628
01:47:41,110 --> 01:47:46,320
linear regression because this is completely linear equation

1629
01:47:46,430 --> 01:47:50,310
but we all know x and we have to recover both a and b both

1630
01:47:50,580 --> 01:47:52,620
a and b s

1631
01:47:57,530 --> 01:47:58,300
OK so

1632
01:47:58,320 --> 01:48:01,330
well this is of course not always possible

1633
01:48:01,350 --> 01:48:03,340
we have to make certain assumptions

1634
01:48:03,450 --> 01:48:08,040
well the fundamental assumption well for simplicity let me first say that i think this

1635
01:48:08,040 --> 01:48:13,360
this condition for simplicity we say that this matrix of the mixing coefficients the mixing

1636
01:48:13,360 --> 01:48:18,680
matrix a square but that's kind of just kind of simplifying assumption well what that

1637
01:48:18,680 --> 01:48:20,180
means is that

1638
01:48:20,230 --> 01:48:25,270
is that we can this that we can simply invert this linear equation so when

1639
01:48:25,270 --> 01:48:29,810
we know the i j we can then in the equation and redirect gets to

1640
01:48:29,810 --> 01:48:34,230
see the coefficients that we need to use to get the s s j

1641
01:48:34,250 --> 01:48:35,670
but then

1642
01:48:35,690 --> 01:48:37,470
these two

1643
01:48:39,440 --> 01:48:41,450
the statistical conditions are really

1644
01:48:41,460 --> 01:48:44,800
the whole point in this matter as well first of all as you may have

1645
01:48:44,800 --> 01:48:50,780
guessed from the name independent component analysis we assume that these component these source signals

1646
01:48:50,810 --> 01:48:59,130
are mutually independent mutually statistically independent so again statistical independence and not not for example

1647
01:48:59,130 --> 01:49:01,750
linear independence that we are talking about

1648
01:49:01,870 --> 01:49:08,030
but this is what this means basically is that this sort of thing that should

1649
01:49:08,030 --> 01:49:09,110
be somehow

1650
01:49:09,130 --> 01:49:12,480
create heat or indeed i mean this is the first

1651
01:49:12,490 --> 01:49:18,030
processes that create this sourcing should be somehow for example physically independent from each other

1652
01:49:18,030 --> 01:49:23,730
if we have like two different entities that produce these signals without any interaction physical

1653
01:49:23,730 --> 01:49:29,320
interaction with each other then we might assume that the signals are independent also statistically

1654
01:49:29,320 --> 01:49:31,800
independent but then what is really

1655
01:49:32,400 --> 01:49:35,620
extremely important here is this assumption

1656
01:49:35,630 --> 01:49:41,800
that's the the latent variables the independent components are non gaussian that is that distributions

1657
01:49:41,800 --> 01:49:42,530
are not

1658
01:49:42,540 --> 01:49:44,980
those gaussian are not normal

1659
01:49:45,000 --> 01:49:51,540
and this is really what makes the whole technique completely different from classical statistical models

1660
01:49:51,840 --> 01:49:55,100
i mean this this kind of this kind of linear models

1661
01:49:55,120 --> 01:49:59,860
well we only observe x i have been investigated in statistics for a very long

1662
01:49:59,860 --> 01:50:07,650
time actually i just attended a centennial meeting for factor analysis because section just nineteen

1663
01:50:07,650 --> 01:50:12,590
hundred o four spearman published the classical paper on factor analysis where

1664
01:50:12,600 --> 01:50:14,920
where we have very similar

1665
01:50:14,930 --> 01:50:17,830
very similar equation

1666
01:50:17,880 --> 01:50:24,420
but once once we say that these components are nongaussian everything changes and certain fundamental

1667
01:50:24,420 --> 01:50:27,150
limitations of factor analysis of PCA

1668
01:50:27,550 --> 01:50:32,150
completely vanish and we can actually properly do blind source separation

1669
01:50:32,170 --> 01:50:34,560
so some restrictions

1670
01:50:34,570 --> 01:50:40,490
some determinants have also been to be taken into account first of

1671
01:50:41,090 --> 01:50:45,830
this is i j only defined up to a multiplicative constant well that's quite simply

1672
01:50:45,830 --> 01:50:48,550
i guess i should have mention that we

1673
01:50:48,570 --> 01:50:54,810
conventionally defined the independent components have unit variance because the scale is completely undetermined so

1674
01:50:54,810 --> 01:51:00,270
we can define it however we want to this people's identity so what this means

1675
01:51:00,270 --> 01:51:05,810
is that it is sometimes a delay transpose equals identity thus eighty is

1676
01:51:05,820 --> 01:51:08,250
an orthogonal matrix

1677
01:51:08,260 --> 01:51:13,800
there's also a matrix has approximately half of the degrees of freedom

1678
01:51:13,820 --> 01:51:17,050
compared to the full matrix

1679
01:51:18,450 --> 01:51:24,090
well these not very simple complicated but it's something like one

1680
01:51:27,610 --> 01:51:29,300
know this

1681
01:51:29,310 --> 01:51:33,870
this logic also shows why we can do i see on gold standard why we

1682
01:51:33,870 --> 01:51:39,480
really have to assume that the independent components have non gaussian distributions

1683
01:51:39,500 --> 01:51:43,110
well this because as we saw in the last lecture

1684
01:51:43,130 --> 01:51:50,240
the multivariate gaussian distribution is completely determined by the covariance matrix and the mean mean

1685
01:51:50,240 --> 01:51:54,330
vector but the mean is assumed to be zero so basically is completely determined by

1686
01:51:54,330 --> 01:51:58,190
the covariance matrix well we so that the

1687
01:52:01,400 --> 01:52:06,520
four girls in distribution was some normalisation constant

1688
01:52:06,570 --> 01:52:14,190
times explanation of minus one half is

1689
01:52:15,790 --> 01:52:20,780
the times the inverse of the covariance times is

1690
01:52:20,800 --> 01:52:22,510
so once we know so

1691
01:52:22,520 --> 01:52:28,390
every covariance matrix corresponds to exactly one gaussian distribution assuming that the mean zero and

1692
01:52:28,390 --> 01:52:29,810
vice versa

1693
01:52:30,100 --> 01:52:32,870
so in the same distribution however

1694
01:52:32,890 --> 01:52:40,070
however many observations we may get we never get anything we either the there is

1695
01:52:40,070 --> 01:52:45,790
simply no more information in this distribution than what is given by the covariance matrix

1696
01:52:46,990 --> 01:52:51,880
for doing this kind of transformations as we see in this equation three doesn't really

1697
01:52:52,130 --> 01:52:56,740
give any new information these new covariance we don't have any new information actually they

1698
01:52:56,740 --> 01:53:03,750
would be a simple deterministic simple but they will be deterministic functions of the size

1699
01:53:03,780 --> 01:53:06,000
of the core of the covariance

1700
01:53:06,010 --> 01:53:11,750
and this is really the reason why

1701
01:53:11,760 --> 01:53:15,190
all the classical methods for estimating this

1702
01:53:15,240 --> 01:53:17,820
linear mixing model for

1703
01:53:17,840 --> 01:53:20,880
because there are simply they they use

1704
01:53:20,890 --> 01:53:23,190
factor analysis and PCA

1705
01:53:23,200 --> 01:53:28,540
they just use the covariance matrix and they simply don't have enough information

1706
01:53:28,550 --> 01:53:32,420
from which to infer the main mixing matrix a

1707
01:53:32,550 --> 01:53:34,340
in fact analysis people

1708
01:53:34,350 --> 01:53:35,580
talk about

1709
01:53:35,970 --> 01:53:41,000
the fact the rotation in deterministic

1710
01:53:41,010 --> 01:53:45,460
so that is that means it's just the fact that that there is a certain

1711
01:53:45,460 --> 01:53:46,910
kind of rotation

1712
01:53:47,010 --> 01:53:53,350
the this week given by this orthogonal mixing matrix a tilde which you cannot recover

1713
01:53:53,370 --> 01:53:56,940
well for various reasons people in fact others have

1714
01:53:56,980 --> 01:54:00,110
kind of learning to live with that problem and so on

1715
01:54:00,140 --> 01:54:04,450
they do fact quite happily and then they just three let's say that well we

1716
01:54:04,450 --> 01:54:08,630
have this factor analysis problem and then they figure out in different kinds of heuristic

1717
01:54:08,630 --> 01:54:12,220
methods of finding different kinds of rotations

1718
01:54:12,230 --> 01:54:16,550
but from the viewpoint of blind source separation for example

1719
01:54:18,020 --> 01:54:20,320
in factor rotation in the ministerial really

1720
01:54:20,350 --> 01:54:26,140
it's completely unacceptable if you take if you take some kind of source signals like

1721
01:54:26,150 --> 01:54:27,310
like this here

1722
01:54:27,350 --> 01:54:32,170
and if you make an orthogonal transformation on all the mixing of them it just

1723
01:54:32,170 --> 01:54:33,710
like you know

1724
01:54:33,720 --> 01:54:40,010
completely arbitrary mixture come it will complete garbage the fact that the mixing is orthogonal

1725
01:54:40,040 --> 01:54:50,580
doesn't really realistic restricting in any any intuitively comprehensible way

1726
01:55:00,550 --> 01:55:03,940
there is something wrong here

1727
01:55:04,000 --> 01:55:09,860
this PDF file doesn't have

1728
01:55:09,890 --> 01:55:14,570
the proper images notes

1729
01:55:14,820 --> 01:55:18,360
i realize that in you are in print out this there was something missing in

1730
01:55:18,360 --> 01:55:22,790
the figures but i thought that it was on independent process but it seems that

1731
01:55:22,820 --> 01:55:27,390
this file is corrupted in some way we should have backscatter from lots of different

1732
01:55:27,390 --> 01:55:29,230
distributions here

1733
01:55:29,970 --> 01:55:32,990
well well well

1734
01:55:47,160 --> 01:55:52,770
and yes i wonder how can that can be and

1735
01:55:58,880 --> 01:56:01,890
well i tried to get well perhaps i i

1736
01:56:04,490 --> 01:56:13,310
well maybe the problem isn't

1737
01:56:13,310 --> 01:56:17,960
so my name is caused by reason and phd student of professor bunker at the

1738
01:56:17,960 --> 01:56:19,390
university of bern

1739
01:56:19,430 --> 01:56:20,730
in switzerland

1740
01:56:20,750 --> 01:56:23,500
so this talk here considers

1741
01:56:23,510 --> 01:56:28,270
the general graph matching paradigm of graph edit distance

1742
01:56:28,280 --> 01:56:33,200
so first of all i will talk about graph edit distance as a graph matching

1743
01:56:34,180 --> 01:56:39,800
then i will briefly talk about standard procedure for computing the graph edit distance by

1744
01:56:39,800 --> 01:56:45,520
means of a tree search algorithm and then the next i will talk about munkres

1745
01:56:46,590 --> 01:56:53,030
which is in fact an algorithm for solving design problems in polynomial time complexity

1746
01:56:53,040 --> 01:56:58,750
and then our contribution here is that we adopt this algorithm for solving assignment problem

1747
01:56:58,750 --> 01:57:02,720
to the problem of graph edit distance so i will talk about this

1748
01:57:02,760 --> 01:57:07,500
then of course we will provide several experimental results we have achieved

1749
01:57:07,540 --> 01:57:10,050
with our new algorithm for computing

1750
01:57:10,070 --> 01:57:15,270
graph edit distance and finally how to draw conclusions

1751
01:57:15,280 --> 01:57:22,020
so the main contribution of this talk are work study provide very fast algorithm for

1752
01:57:22,020 --> 01:57:24,530
computing graph edit distance

1753
01:57:24,540 --> 01:57:29,230
and this algorithm is sub optimal in the sense of finding not necessarily

1754
01:57:29,240 --> 01:57:32,390
the exact graph edit distance

1755
01:57:32,400 --> 01:57:37,660
so suppose all of you know this a standard definition of graphs but nevertheless i

1756
01:57:37,660 --> 01:57:41,150
mention it here for the sake of completeness and there's one thing that i really

1757
01:57:41,150 --> 01:57:46,170
want to point out to you and it is this concerns so label alphabet

1758
01:57:46,220 --> 01:57:51,350
so as you see we have no restrictions concerning the label alphabet for both nodes

1759
01:57:51,350 --> 01:57:55,060
and edges so the labels can be added to the set of integers set of

1760
01:57:55,920 --> 01:57:59,060
it can be to real work to space it can be a set of discrete

1761
01:57:59,060 --> 01:58:04,640
symbols or it can even be a combination of two or more label alphabet

1762
01:58:04,650 --> 01:58:10,410
hence the parties are objects to be transformed into graph based representations are manifold and

1763
01:58:10,410 --> 01:58:16,900
they range from physical or logical networks two images and chemical structures

1764
01:58:16,910 --> 01:58:18,940
so this flexibility in

1765
01:58:19,060 --> 01:58:19,870
in our

1766
01:58:19,900 --> 01:58:21,670
definition here is based

1767
01:58:21,680 --> 01:58:24,740
in our graph matching pair than that we apply

1768
01:58:24,760 --> 01:58:27,800
and this is a graph edit distance

1769
01:58:27,850 --> 01:58:33,300
so in graph edit distance to the similarity of the distance of two graphs is

1770
01:58:33,300 --> 01:58:38,530
basically defined by the minimum amount of distortion that is needed to transform one graph

1771
01:58:38,530 --> 01:58:39,880
into another

1772
01:58:39,900 --> 01:58:46,020
so in fact these distortions are given by edit operations and these edit operations consists

1773
01:58:46,310 --> 01:58:51,250
of deletions insertions and substitutions of both nodes and edges

1774
01:58:51,270 --> 01:58:53,080
so here's an example

1775
01:58:53,090 --> 01:58:55,010
we have two graphs g one and g two

1776
01:58:55,020 --> 01:59:00,360
so we have now an added sequence which transformed she one completely into g two

1777
01:59:00,770 --> 01:59:01,830
and such

1778
01:59:01,840 --> 01:59:05,670
sequences commonly referred to as edit path so we

1779
01:59:05,680 --> 01:59:09,060
possibly delete some edges we do need some notes

1780
01:59:09,080 --> 01:59:15,400
three d insert notes reinserted and we substitute nodes and edges so finally we have

1781
01:59:15,400 --> 01:59:17,750
transformed she one completely

1782
01:59:17,830 --> 01:59:19,340
in to g two

1783
01:59:19,350 --> 01:59:25,360
with this edit so of course not only one edit path exists between two graphs

1784
01:59:25,360 --> 01:59:30,090
but the number of different edit past so for instance you can think of an

1785
01:59:30,090 --> 01:59:33,410
edit path you delete all the nodes edges

1786
01:59:33,450 --> 01:59:37,790
in g one and insert although it's the edges in g two this would be

1787
01:59:37,800 --> 01:59:39,480
possible and path through

1788
01:59:39,490 --> 01:59:41,660
so formally

1789
01:59:41,670 --> 01:59:45,300
the graph edit distance is defined as follows

1790
01:59:45,320 --> 01:59:49,800
let us assume we have two graphs given g one has the source graph and

1791
01:59:49,800 --> 01:59:54,640
g two s the target graph then the graph edit distance between the source and

1792
01:59:54,650 --> 02:00:01,520
the target graph is given by the minimum cost path between source and target so

1793
02:00:01,520 --> 02:00:07,160
obviously what we have to define here is a cost function which measures the strength

1794
02:00:07,170 --> 02:00:09,540
of a given edit operations

1795
02:00:09,590 --> 02:00:11,630
OK and this

1796
02:00:12,240 --> 02:00:15,730
is a cost function should of course

1797
02:00:15,740 --> 02:00:23,670
reflects the intuition that two nodes for instance with very different labels should

1798
02:00:23,720 --> 02:00:29,100
have a higher cost and to a substitution of two nodes with very similar

1799
02:00:30,980 --> 02:00:32,230
so we can

1800
02:00:32,260 --> 02:00:37,590
conclude that graph edit distance provides us with a very general dissimilarity model

1801
02:00:37,670 --> 02:00:42,800
for graphs which can handle any kind of label alphabet

1802
02:00:42,840 --> 02:00:47,920
so let's consider now the possibilities we have for edit distance based

1803
02:00:47,930 --> 02:00:53,180
classification and we distinguish two different ways of edit distance based

1804
02:00:53,190 --> 02:00:55,730
classification so first of all

1805
02:00:55,810 --> 02:01:00,640
of course you can apply classifiers directly in the domain of graphs but in fact

1806
02:01:00,970 --> 02:01:02,470
if you do so you are

1807
02:01:02,480 --> 02:01:08,780
you are restricted to this and then nearest neighbour classifiers

1808
02:01:08,790 --> 02:01:15,710
but however we can also define graph kernels based on the edit distance so in

1809
02:01:15,910 --> 02:01:16,860
year three

1810
02:01:16,880 --> 02:01:18,570
so for four

1811
02:01:18,580 --> 02:01:23,680
and since you can define a trivial graph kernels in conjunction with an SVM

1812
02:01:23,700 --> 02:01:26,210
so what you have to do is you have to change

1813
02:01:26,220 --> 02:01:32,060
to transform you dissimilarity of graph edit distance into a similarity measure

1814
02:01:32,070 --> 02:01:34,380
this is an example here

1815
02:01:34,480 --> 02:01:39,370
what you can also do is you can extend standard graph kernels for instance the

1816
02:01:39,370 --> 02:01:45,250
random walk kernel with edit distance only show the house as shown in his phd

1817
02:01:45,250 --> 02:01:47,840
that this is how this might be a good idea

1818
02:01:47,860 --> 02:01:51,280
for instance the random walk it could

1819
02:01:51,420 --> 02:01:55,180
and the third possibility you have this graph edit distance is that you have used

1820
02:01:55,200 --> 02:02:03,000
information about gravity for graph edit distance to embed your graphs explicitly in real vector

1821
02:02:03,010 --> 02:02:09,670
space so this will be the topic of my second talk on wednesday

1822
02:02:09,680 --> 02:02:13,890
so for those of you have already worked with gravity this and

1823
02:02:13,970 --> 02:02:17,690
distance know that its computational complexity

1824
02:02:17,700 --> 02:02:20,490
it is it's major drawback

1825
02:02:20,500 --> 02:02:24,120
so in contrast with exact graph matching algorithms

1826
02:02:24,630 --> 02:02:29,470
the nodes of the source graph can potentially be mapped to any node of the

1827
02:02:29,470 --> 02:02:30,830
the target graph

1828
02:02:30,880 --> 02:02:36,890
so this might be a problem and consequently the computational complexity for graph edit distance

1829
02:02:36,930 --> 02:02:40,770
is exponential in the number of nodes of the involved graphs

1830
02:02:40,810 --> 02:02:44,510
so it's well known that for graphs with unique no labels the complexity is linear

1831
02:02:44,810 --> 02:02:49,580
but they do not consider these graphs here we do consider general graphs

1832
02:02:49,630 --> 02:02:54,780
so the computation of gravity distance is usually carried out by means of a tree

1833
02:02:54,780 --> 02:02:56,980
search algorithm

1834
02:02:57,000 --> 02:03:02,690
so the idea is that you use the search tree as representation formalism for the

1835
02:03:02,690 --> 02:03:04,410
optimisation problem

1836
02:03:04,430 --> 02:03:05,450
so the

1837
02:03:05,460 --> 02:03:10,260
root note of this research represents the starting point of the research then in the

1838
02:03:10,260 --> 02:03:13,520
nodes represents partial solutions

1839
02:03:13,530 --> 02:03:21,730
and leave nodes represents complete solutions means complete edit path transforming one graph completely into

1840
02:03:21,760 --> 02:03:23,740
the graph

1841
02:03:24,980 --> 02:03:28,830
since the complexity of the exact graph

1842
02:03:28,840 --> 02:03:30,960
edit distance is

1843
02:03:30,990 --> 02:03:35,660
exponential in the number of nodes of the involved graphs it's a matter of fact

1844
02:03:35,660 --> 02:03:39,830
that exact graph edit distance is feasible for small graphs

1845
02:03:40,930 --> 02:03:45,430
so however in order to speed up graph edit distance and make it also applicable

1846
02:03:46,130 --> 02:03:52,100
larger graphs several suboptimal methods have been proposed and in this work here

1847
02:03:52,120 --> 02:03:57,450
the approach we introduce a new suboptimal methods for graph edit distance based on the

1848
02:03:57,480 --> 02:04:00,760
assignment problem

