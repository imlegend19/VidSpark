1
00:00:00,000 --> 00:00:08,130
we couple information which side of the dice showing up with the number of this

2
00:00:08,150 --> 00:00:13,270
has for example we might assume that the faces of the of the dice show

3
00:00:13,280 --> 00:00:18,380
one two three four five six ten twenty thirty sixty whatever so then this would

4
00:00:18,380 --> 00:00:23,860
be the numbers ten twenty thirty forty fifty sixty and this would still be just

5
00:00:23,860 --> 00:00:28,090
indicator variable indicating which of the sites showing one two

6
00:00:28,110 --> 00:00:32,960
four one two three four five six and and this is then again the noisy

7
00:00:32,960 --> 00:00:40,410
observations whose sometimes little more multimedia to work with because he uncoupled the value

8
00:00:40,420 --> 00:00:46,630
of the face with the indicator which faces showing up played by introducing the auxiliary

9
00:00:46,910 --> 00:00:51,770
variable z is also play a role in this process

10
00:00:51,790 --> 00:00:53,140
first later

11
00:00:53,280 --> 00:00:56,230
but essentially the same model c

12
00:00:56,310 --> 00:01:02,360
because now we say that the probability of observing that this hidden they was in

13
00:01:02,360 --> 00:01:08,800
a certain stage is exactly the case component of g again was going to say

14
00:01:08,800 --> 00:01:14,250
only that we had before here we have to have these here and we strongly

15
00:01:14,250 --> 00:01:22,540
couple disease will affect us all eastern state zk and the is that state

16
00:01:22,560 --> 00:01:28,950
i j then this is equal to one papers chaining zero otherwise so because these

17
00:01:28,950 --> 00:01:33,880
things are deterministically copied is exactly the same model as we had before

18
00:01:37,120 --> 00:01:41,510
i would like to do so

19
00:01:41,530 --> 00:01:45,410
you you have to look the latest version

20
00:01:45,430 --> 00:01:49,240
from the material from this

21
00:01:49,250 --> 00:02:02,660
another common is the said before the following use the collapsed gibbs sampling which means

22
00:02:02,660 --> 00:02:06,100
that we never explicitly said from g we sort of

23
00:02:06,120 --> 00:02:12,690
collapse model before we did simply integrating she this very elegant in particular if g

24
00:02:12,690 --> 00:02:20,480
is infinite dimensional but it has some problems because it gives them does not mix

25
00:02:20,480 --> 00:02:25,560
very well which means that consecutive samples are highly dependent

26
00:02:27,360 --> 00:02:28,310
in fact one

27
00:02:28,320 --> 00:02:30,470
o pins but sampling bias

28
00:02:30,490 --> 00:02:35,940
using the collection itself by sending explicit explicitly from g

29
00:02:35,960 --> 00:02:41,420
which in this model is also possible and if it has become somewhat different

30
00:02:41,430 --> 00:02:48,260
and the advantage is that given g we can now independently sample from the auxiliary

31
00:02:49,180 --> 00:02:53,410
in the block way we don't have to consider

32
00:02:53,420 --> 00:02:55,470
the other values of the other

33
00:02:55,570 --> 00:02:59,310
renovations of the when we separate from the given the

34
00:02:59,330 --> 00:03:01,810
as you have done in the previous approach

35
00:03:01,820 --> 00:03:07,080
remember the sculpting from disease and you see here the first update the g and

36
00:03:07,080 --> 00:03:12,010
then given g of the templates are independent and can be updated in a block

37
00:03:12,010 --> 00:03:16,750
in the block that's why this is also called

38
00:03:20,790 --> 00:03:28,000
is situation considered if g is known example of which is produced and these are

39
00:03:30,510 --> 00:03:33,950
among each other because they are blocked by the

40
00:03:34,320 --> 00:03:42,520
quality g

41
00:03:42,530 --> 00:03:46,930
so in the block itself that we have to send the from the negative labels

42
00:03:46,950 --> 00:03:51,800
the given the given data and then send the g

43
00:03:51,820 --> 00:03:57,930
given indicator variables which blocks away all the data the surveys so we just before

44
00:03:58,220 --> 00:04:06,260
so this is quite simple in this in the finite dimensional case to do and

45
00:04:06,280 --> 00:04:11,010
approximations possibly using the late and it is

46
00:04:11,120 --> 00:04:17,690
this is a slightly different model but also related models if you look at the

47
00:04:17,690 --> 00:04:19,350
situation over here

48
00:04:19,370 --> 00:04:27,100
the nice thing here was that we decouple the indicator here from the value of

49
00:04:27,100 --> 00:04:32,200
the dice now we can assume that this value might be unknown

50
00:04:32,210 --> 00:04:37,100
we are uncertain that we don't know exactly what to do

51
00:04:37,110 --> 00:04:39,070
well you will be of the dice

52
00:04:39,090 --> 00:04:40,890
it might be ten twenty

53
00:04:40,910 --> 00:04:44,990
forty fifty sixty nine people one hundred twelve whatever might be even in on an

54
00:04:44,990 --> 00:04:46,420
ordered set of numbers

55
00:04:46,430 --> 00:04:51,260
and big by the coupling this makes it makes us what we use it to

56
00:04:51,570 --> 00:04:59,590
also estimate these parameters which which which are shown in the face of the time

57
00:04:59,600 --> 00:05:07,070
in this land could closely related to a standard mixture model things graphically shown here

58
00:05:07,230 --> 00:05:09,150
so here we

59
00:05:09,160 --> 00:05:16,790
explicitly indicate that we we specify a prior distribution for the factors which might have

60
00:05:16,970 --> 00:05:22,500
some more parameters over here and that these parameters that are not appearing on but

61
00:05:22,520 --> 00:05:27,520
also estimated by observing the data and

62
00:05:28,320 --> 00:05:32,170
replace this g here by the pi which is

63
00:05:32,210 --> 00:05:35,190
also specify

64
00:05:35,200 --> 00:05:40,860
multinomial distribution and this is also usually process

65
00:05:40,960 --> 00:05:44,320
usually the distribution over here so

66
00:05:44,360 --> 00:05:47,790
a little bit has changed so it's not exactly the same thing is what we've

67
00:05:47,790 --> 00:05:48,930
done before

68
00:05:48,930 --> 00:05:51,740
of photons and some to be able to do anything useful

69
00:05:52,580 --> 00:05:56,840
or if the number of times small that means your kernel function does all the

70
00:05:56,850 --> 00:05:59,040
work and what is coming from words coming from

71
00:06:00,180 --> 00:06:05,560
so this kind of a bit of a conundrum here and you know you you would think that's

72
00:06:05,580 --> 00:06:07,830
some functions that conceptual systems

73
00:06:08,070 --> 00:06:09,260
have to

74
00:06:09,470 --> 00:06:14,870
compute are inherently sequential they require to have a set number of sequential operations

75
00:06:15,110 --> 00:06:16,010
for them to be

76
00:06:17,330 --> 00:06:17,790
a a

77
00:06:18,140 --> 00:06:21,660
computer always sort of a relatively small amount of resources

78
00:06:21,810 --> 00:06:23,960
there's a lot of complexity theorists in the room

79
00:06:24,100 --> 00:06:27,230
and people were studied sort of complexity i'm sure

80
00:06:27,360 --> 00:06:31,180
i'm know that's sure myself to a few layers competitions

81
00:06:31,410 --> 00:06:33,540
are you can gain or an exponential of

82
00:06:33,940 --> 00:06:35,040
factor on the

83
00:06:35,510 --> 00:06:38,120
can not hardware you need to compute particular function

84
00:06:38,530 --> 00:06:39,370
for example

85
00:06:39,570 --> 00:06:42,640
it's a basic example of you know

86
00:06:42,800 --> 00:06:45,290
a bit parity we know that we can do this with

87
00:06:45,580 --> 00:06:50,090
you know again stages or you know look like over two stages

88
00:06:50,230 --> 00:06:51,170
with x gates

89
00:06:52,500 --> 00:06:56,220
but if we force ourselves to do it into layers using the contracting

90
00:06:56,240 --> 00:06:58,810
form we might need an x natural number of minterms

91
00:06:59,370 --> 00:07:05,000
so that you know it's just a very simple question of exchange of complexity between space and time essentially

92
00:07:05,220 --> 00:07:07,640
that we deal with all the time we write programs

93
00:07:07,910 --> 00:07:12,060
for ourselves multiple stages then we might gain a huge factor on

94
00:07:12,370 --> 00:07:15,130
on the overall are not resources we need

95
00:07:15,550 --> 00:07:19,580
so so as a very intuitive you know i don't have easier to tell to

96
00:07:19,600 --> 00:07:21,040
to show you you guys other kinds

97
00:07:21,230 --> 00:07:21,960
of people to

98
00:07:23,260 --> 00:07:24,190
prove the theory as

99
00:07:24,360 --> 00:07:30,630
but the basic idea as a basic intuition of of why we need multiple layers is just

100
00:07:30,650 --> 00:07:32,470
that sort of complexity basically

101
00:07:34,780 --> 00:07:38,390
ok so this is a bit of a petition nightmare here which is that when we

102
00:07:38,400 --> 00:07:42,140
start having multiple layers in a system like a deep know that

103
00:07:42,440 --> 00:07:45,200
we try to train it using a supervised learning

104
00:07:45,690 --> 00:07:51,760
we have loss functions and being non-convex if you have non-convex loss functions and rely on things that

105
00:07:52,080 --> 00:07:53,750
you know like minimal

106
00:07:54,060 --> 00:07:56,570
but i do empirical musicians like that

107
00:07:56,880 --> 00:08:01,210
in all things go out the window because can prove anything about the you know you're you're model

108
00:08:01,630 --> 00:08:06,030
ok but you know they work it's actually worth maybe spending effort trying to figure out

109
00:08:06,100 --> 00:08:06,950
why they work

110
00:08:09,970 --> 00:08:13,700
so all bets are off non-convex losses but then again you know

111
00:08:13,980 --> 00:08:17,400
every speech system deployed as is non-convex optimization

112
00:08:18,280 --> 00:08:20,160
it's not because non-convex that don't work

113
00:08:24,120 --> 00:08:28,400
are you lead to some of us the only interesting learning actually is non-convex

114
00:08:32,720 --> 00:08:37,180
if you have a convex optimization to solve for learning the order in which

115
00:08:38,020 --> 00:08:39,440
you learn things doesn't matter

116
00:08:40,170 --> 00:08:41,940
we know that humans

117
00:08:42,540 --> 00:08:46,640
the order in which you learn things in humans matters this is called pedagogy

118
00:08:48,810 --> 00:08:51,740
can such that k as can start my talk with the last slide i think go

119
00:08:51,750 --> 00:08:55,120
backwards but i don't think you'll get the same state of the in if you do this

120
00:08:57,820 --> 00:09:02,540
in the aggregate statistics is the same so you know is

121
00:09:02,730 --> 00:09:05,610
the order in which you see things actually matters

122
00:09:05,960 --> 00:09:08,370
and in machine learnings actually matters

123
00:09:08,750 --> 00:09:11,410
and suggest the fact that whatever loss function

124
00:09:11,740 --> 00:09:15,050
you know that even write a minimizing given actually minimize loss function

125
00:09:15,260 --> 00:09:17,890
which is another question f has to be non-convex

126
00:09:18,170 --> 00:09:20,240
was the order in which we learn things with matter

127
00:09:23,080 --> 00:09:28,190
ok that doesn't mean that we can actually you know when i with a convex optimization but

128
00:09:29,830 --> 00:09:31,890
but is sort of features consideration perhaps

129
00:09:33,520 --> 00:09:35,510
ok the other theoretician 's nightmare

130
00:09:35,730 --> 00:09:37,330
you know if

131
00:09:37,960 --> 00:09:40,100
if we have all those things deep learning algorithms

132
00:09:40,410 --> 00:09:42,930
we don't have any good jun generalization bounds

133
00:09:43,400 --> 00:09:47,320
are actually not to know to take a big neural net actually has abound z down

134
00:09:47,990 --> 00:09:50,040
think because this dimension is finite

135
00:09:52,050 --> 00:09:53,240
but not tag bounce

136
00:09:53,460 --> 00:09:57,480
now i could be a problem except that i don't actually know anybody practice to use

137
00:09:57,500 --> 00:09:58,880
bound to model selection

138
00:10:01,080 --> 00:10:02,890
ok to see this for this about

139
00:10:05,030 --> 00:10:10,150
but that's true does anyone know a you know anyone who actually uses down in practice to

140
00:10:10,170 --> 00:10:12,360
model selection as opposed to a cross validation

141
00:10:12,940 --> 00:10:13,800
raise your hand

142
00:10:15,740 --> 00:10:17,490
yeah ok you

143
00:10:18,150 --> 00:10:23,410
know and ok i know but i know and to yeah

144
00:10:23,580 --> 00:10:26,080
ok yeah

145
00:10:27,700 --> 00:10:32,820
f so z some of my best friends use bounds yeah

146
00:10:34,600 --> 00:10:39,730
so you know it's hard to prove anything about deep learning systems but then again

147
00:10:39,750 --> 00:10:43,760
if we only study models for which we improve things in speech and writing object

148
00:10:43,780 --> 00:10:44,840
recognition systems today

149
00:10:45,580 --> 00:10:47,690
f ok

150
00:10:48,310 --> 00:10:49,070
on the other hand

151
00:10:49,510 --> 00:10:54,220
it's kind of smorgasbord of of stuff for theoreticians because there are so many things we don't

152
00:10:54,240 --> 00:10:57,590
know about are all methods are used in deep learning and i'll talk about them

153
00:10:57,820 --> 00:10:59,190
some of them as a group

154
00:10:59,610 --> 00:11:02,130
you know doing talk so

155
00:11:02,910 --> 00:11:08,710
you know deep learning is about representing high-dimensional data or representing a high-dimensional spaces

156
00:11:09,250 --> 00:11:12,720
and has to be interesting theoretical questions about this

157
00:11:12,980 --> 00:11:15,130
west geometry of natural signals

158
00:11:16,530 --> 00:11:19,850
you know is equal to a sequel learning theory for unsupervised learning

159
00:11:20,520 --> 00:11:23,300
of computational learning theory for that matter i mean i know there are

160
00:11:23,510 --> 00:11:25,460
efforts on those directions

161
00:11:26,060 --> 00:11:27,130
in this the

162
00:11:27,330 --> 00:11:31,090
what are good criteria on which to base unsupervised learning things so that

163
00:11:31,530 --> 00:11:36,810
you know questions had been answered by this committee for supervised learning which i think you know be

164
00:11:37,190 --> 00:11:38,800
we're thinking about for

165
00:11:40,270 --> 00:11:40,890
teacher learning

166
00:11:41,170 --> 00:11:41,790
which is

167
00:11:42,780 --> 00:11:43,950
sometimes unsupervised

168
00:11:49,350 --> 00:11:53,500
ok so let me talk about the state of the state of affairs for deep learning

169
00:11:55,730 --> 00:11:57,350
it's it's a one

170
00:11:57,510 --> 00:12:00,680
hottest topics in speech recognition for about the last two years

171
00:12:01,080 --> 00:12:04,680
the latest generation of commercial systems that are based on

172
00:12:04,840 --> 00:12:07,120
that speech recognition are all based on deep learning

173
00:12:07,650 --> 00:12:10,510
once from idea and google all microsoft

174
00:12:11,020 --> 00:12:15,350
know based on deep learning one of the phone which is actually directly produced by an

175
00:12:15,840 --> 00:12:17,130
idea and we know once

176
00:12:17,710 --> 00:12:19,100
they use deep neural nets

177
00:12:20,020 --> 00:12:25,600
and has been very very fast transition from gas mixture models to keep on its acoustic

178
00:12:26,830 --> 00:12:27,690
in the us to use

179
00:12:28,520 --> 00:12:35,210
is becoming the hottest topic in computer vision mostly because of the last imagenet competition conducted that

180
00:12:35,510 --> 00:12:37,650
and it's becoming a bit of

181
00:12:38,110 --> 00:12:41,260
interesting topic for natural language processing although it's not there yet

182
00:12:43,400 --> 00:12:46,640
and is a huge interest from the applied math community

183
00:12:46,970 --> 00:12:49,280
into things like where byzantine data

184
00:12:49,560 --> 00:12:50,770
in high-dimensional spaces

185
00:12:51,330 --> 00:12:53,490
people use how many canisius now

186
00:12:53,870 --> 00:12:55,050
sparse coding and stuff like that

187
00:12:56,450 --> 00:12:58,290
ok so

188
00:12:58,290 --> 00:13:00,380
so i said

189
00:13:11,640 --> 00:13:20,050
well done to all these

190
00:13:40,370 --> 00:13:44,980
see my

191
00:13:48,920 --> 00:13:53,790
the two hundred

192
00:13:53,860 --> 00:13:57,970
really to

193
00:14:06,350 --> 00:14:10,210
so we

194
00:14:16,010 --> 00:14:17,930
raise your

195
00:14:24,710 --> 00:14:30,080
smaller loss

196
00:14:30,340 --> 00:14:34,060
and this

197
00:14:37,690 --> 00:14:40,000
need to know

198
00:14:47,820 --> 00:14:49,240
well two

199
00:15:00,160 --> 00:15:03,340
so these are dimensions so

200
00:15:04,200 --> 00:15:06,640
we see

201
00:15:06,660 --> 00:15:10,480
we want to do

202
00:15:32,230 --> 00:15:34,960
this is the

203
00:15:44,640 --> 00:15:47,840
you want

204
00:15:50,920 --> 00:15:52,620
eighteen minutes

205
00:15:52,640 --> 00:15:56,190
well we

206
00:15:57,620 --> 00:16:00,960
and what that

207
00:16:16,500 --> 00:16:17,830
he was

208
00:16:23,270 --> 00:16:27,870
this is really

209
00:16:34,070 --> 00:16:36,020
it will

210
00:16:36,030 --> 00:16:42,340
let g

211
00:16:42,400 --> 00:16:45,980
so here we be

212
00:16:46,000 --> 00:16:50,200
just a

213
00:16:50,540 --> 00:16:54,070
class see you find

214
00:16:55,060 --> 00:17:05,080
one more relationship between the statistics

215
00:17:07,730 --> 00:17:09,360
vision problems

216
00:17:10,660 --> 00:17:13,490
no one was injured

217
00:17:28,050 --> 00:17:31,080
just to

218
00:17:32,060 --> 00:17:35,830
you might also want to be

219
00:17:36,300 --> 00:17:37,950
so what

220
00:17:41,360 --> 00:17:43,410
we i can

221
00:17:50,360 --> 00:17:55,420
all in all

222
00:17:55,440 --> 00:17:56,830
lot of us

223
00:17:56,840 --> 00:17:59,610
well known

224
00:17:59,670 --> 00:18:02,630
all this

225
00:18:18,740 --> 00:18:20,300
the british

226
00:18:28,840 --> 00:18:30,420
the use

227
00:18:30,430 --> 00:18:37,060
more to it for you

228
00:18:43,850 --> 00:18:45,530
sure the

229
00:18:45,540 --> 00:18:50,910
although well then it seems to me

230
00:19:00,770 --> 00:19:05,580
ray t

231
00:19:08,640 --> 00:19:14,000
it also we also

232
00:19:14,050 --> 00:19:17,220
if we went

233
00:19:17,220 --> 00:19:19,710
also can be done in closed form as two

234
00:19:19,720 --> 00:19:21,000
four groups

235
00:19:21,060 --> 00:19:25,420
this because the only non-linear part of the algorithm is to compute the polynomials and

236
00:19:25,420 --> 00:19:32,340
that can be done in closed form after the before

237
00:19:32,350 --> 00:19:34,480
maybe this is a good point to stop

238
00:19:34,650 --> 00:19:38,670
there are any questions so far

239
00:19:47,700 --> 00:19:52,080
but like i said it is nice these equal becomes approximately

240
00:19:52,140 --> 00:19:57,800
so so what to do is find the least squares solution to this linear system

241
00:19:57,830 --> 00:20:00,290
so you can always find this vectors

242
00:20:00,310 --> 00:20:07,500
coefficients no matter whether you have not

243
00:20:12,350 --> 00:20:21,030
OK that's a that's probably the only thing that fails you can know the rank

244
00:20:21,070 --> 00:20:23,150
the image is going to be full rank

245
00:20:23,150 --> 00:20:25,200
with noisy data for any degree

246
00:20:26,430 --> 00:20:27,420
and so

247
00:20:27,440 --> 00:20:31,080
the problem becomes the same question of well how do you know the dimension of

248
00:20:31,080 --> 00:20:33,670
the subspace in principal component analysis

249
00:20:33,690 --> 00:20:38,820
so what you do is essentially find a number of principal components that keeps ninety

250
00:20:38,820 --> 00:20:40,850
something percent of their energy

251
00:20:40,890 --> 00:20:44,220
in a sense and the way you do that is by looking at the singular

252
00:20:44,220 --> 00:20:47,470
values of the matrix and looking when is a big drop

253
00:20:47,490 --> 00:20:51,660
so the same thing can be done here you do it for degree one

254
00:20:51,660 --> 00:20:56,740
look at the plot of singular values and essentially it will be mostly flat

255
00:20:56,770 --> 00:21:00,140
you do for degree two and then will be decreased so you know the two

256
00:21:00,160 --> 00:21:04,370
is the right number but of course in doing that there is always some parameter

257
00:21:04,380 --> 00:21:05,670
that one has to set

258
00:21:05,680 --> 00:21:09,510
so this does not solve the problem of what the number of groups is it

259
00:21:09,510 --> 00:21:14,500
just gives you guidance that then can be used for initializing the more traditional techniques

260
00:21:14,500 --> 00:21:16,430
that are based on model selection

261
00:21:17,330 --> 00:21:20,950
solve simultaneously for the parameters and

262
00:21:20,950 --> 00:21:25,560
so the objective here is to have a quick simple the right technique it'll give

263
00:21:25,560 --> 00:21:29,480
you some idea of what the number of seats as well as well cluster centers

264
00:21:30,450 --> 00:21:33,320
and then you run e plus model selection

265
00:21:33,350 --> 00:21:35,990
to refine your

266
00:21:52,160 --> 00:21:59,480
two answers as the number one is

267
00:21:59,520 --> 00:22:01,100
who cares that much

268
00:22:01,220 --> 00:22:02,790
you just call the roots

269
00:22:02,820 --> 00:22:04,410
the function of matlab

270
00:22:04,410 --> 00:22:06,960
and it doesn't matter what degrees

271
00:22:06,980 --> 00:22:08,800
as the number two is

272
00:22:08,980 --> 00:22:16,750
as you can see moments that the actual technique for solving the case of subspaces

273
00:22:16,760 --> 00:22:20,760
in this case can also be so that way it by computing derivatives of the

274
00:22:20,760 --> 00:22:24,580
polynomial at a data point and that will give you some of the cluster center

275
00:22:24,600 --> 00:22:26,470
corresponding to that point

276
00:22:26,480 --> 00:22:28,020
and as such

277
00:22:28,030 --> 00:22:31,510
it's closed form no matter what the number of groups is

278
00:22:31,520 --> 00:22:37,620
because you'll never have to compute groups of an opponent

279
00:22:37,630 --> 00:22:41,290
in your question

280
00:22:54,830 --> 00:22:59,890
well i already answered that great

281
00:22:59,900 --> 00:23:02,980
the degree of the polynomial is the same as the number of groups

282
00:23:03,000 --> 00:23:05,720
and and so that you look at the

283
00:23:05,730 --> 00:23:09,560
you look at the singular values of this matrix

284
00:23:21,030 --> 00:23:25,000
yes but as i explained

285
00:23:25,290 --> 00:23:27,210
this matrix

286
00:23:27,220 --> 00:23:31,350
the number of columns of this matrix is essentially the degree of the polynomial plus

287
00:23:32,560 --> 00:23:37,370
and theoretically the degree that you should use the rank of this matrix

288
00:23:37,380 --> 00:23:40,970
so you can so if you have a technique for computing the rank of an

289
00:23:40,970 --> 00:23:42,310
ice matrix

290
00:23:42,330 --> 00:23:48,060
you have a technique for computing the degree of the polynomial and you should use

291
00:23:48,080 --> 00:23:50,800
so as i say explain what you do is that

292
00:23:52,070 --> 00:23:54,930
try to look at the singular values of the matrix

293
00:23:54,940 --> 00:23:57,830
whenever there is a big drop

294
00:23:57,850 --> 00:23:59,270
then you will

295
00:23:59,290 --> 00:24:00,950
see with the rank is

296
00:24:00,950 --> 00:24:05,260
and once you know the rank you know what the number of groups

297
00:24:07,660 --> 00:24:09,020
he was

298
00:24:21,200 --> 00:24:25,180
i would like to postpone the later because it will precisely show up and will

299
00:24:25,180 --> 00:24:27,700
be awaits later on

300
00:24:27,710 --> 00:24:30,160
i'll i'll point out twenty comes

301
00:24:30,220 --> 00:24:33,030
it would be about fifteen minutes i think

302
00:24:36,830 --> 00:24:40,580
this was my studies of dimension one pretty much useless

303
00:24:40,600 --> 00:24:47,040
although i'm sure that is actually useless itself the image segmentation problem dimension two

304
00:24:47,060 --> 00:24:48,600
i have

305
00:24:48,620 --> 00:24:50,500
cluster centers that look like this

306
00:24:52,890 --> 00:24:56,120
in this case you can still do the same thing actually

307
00:24:56,160 --> 00:24:58,240
you can take every data point

308
00:24:58,270 --> 00:25:01,950
and make it a complex number

309
00:25:02,000 --> 00:25:06,970
and now you just build your data matrix with complex numbers

310
00:25:07,700 --> 00:25:10,540
from the point of view of the algorithm to just treat them as real numbers

311
00:25:10,540 --> 00:25:14,270
so it doesn't really matter the change is going to be your data matrix is

312
00:25:14,270 --> 00:25:20,040
now complex vector of coefficients is going to be complex ones to compute the roots

313
00:25:20,040 --> 00:25:21,930
the roots are going to be complex

314
00:25:21,950 --> 00:25:26,310
but those complex which besides the clusters into the real part is the x coordinate

315
00:25:26,310 --> 00:25:29,970
the imaginary part is the white court so there is no change you can do

316
00:25:29,970 --> 00:25:32,080
it in the mentioned two as well

317
00:25:32,080 --> 00:25:36,260
look how they correlate with different sorts the cross or we can do that for

318
00:25:36,280 --> 00:25:39,030
completely novel objects with various features

319
00:25:39,120 --> 00:25:42,390
and we can also we can also use this i mean the we used this

320
00:25:42,390 --> 00:25:45,910
in machine learning setting with the conscious company is doing is basically using this for

321
00:25:46,220 --> 00:25:50,240
if you like collaborative filtering or sparse matrix completion now suppose you observe only a

322
00:25:50,240 --> 00:25:51,350
subset of these

323
00:25:51,370 --> 00:25:55,580
these values you want to infer the missing values of others so we can also

324
00:25:55,580 --> 00:25:59,930
do this sort of experiments on people were reintroduced let's say a novel feature it's

325
00:25:59,930 --> 00:26:03,120
like it's like in the netflix problem introducing a new

326
00:26:03,120 --> 00:26:07,370
movie or a new user like and that's like a new row and column here

327
00:26:07,370 --> 00:26:09,870
and you are just a few values and you want to say what are the

328
00:26:09,870 --> 00:26:11,410
values of the

329
00:26:11,410 --> 00:26:14,010
what the missing values and

330
00:26:14,310 --> 00:26:17,510
essentially you know with this model is able to do is to make a richer

331
00:26:17,510 --> 00:26:21,450
range of predictions by figuring out let's say for a new feature is one of

332
00:26:21,450 --> 00:26:23,100
this order is one of that sort

333
00:26:23,120 --> 00:26:28,100
and we can we can show with with in both

334
00:26:28,140 --> 00:26:31,350
one particular we can show with people but what we've also shown some versions of

335
00:26:31,350 --> 00:26:35,080
this just in pure machine learning setting we can do better with this more flexible

336
00:26:35,080 --> 00:26:39,680
model than say just with a flat single mixture model

337
00:26:41,350 --> 00:26:43,310
so let me

338
00:26:43,330 --> 00:26:44,740
talk about some other

339
00:26:44,760 --> 00:26:46,410
other parts of

340
00:26:46,430 --> 00:26:49,760
interestingly structured models

341
00:26:51,010 --> 00:26:53,990
there you again each of these you can see is ways of taking in a

342
00:26:53,990 --> 00:26:59,660
sense the idea of finding clusters and recognizing that in the kinds of mental models

343
00:26:59,660 --> 00:27:03,140
that humans build of the world there's more structure than just a single way of

344
00:27:03,310 --> 00:27:06,560
or you know just what we mean in our textbook notion of clustering so here

345
00:27:06,560 --> 00:27:10,990
that the additional structure is different ways of organizing the world to capture different aspects

346
00:27:10,990 --> 00:27:13,970
of things but another interesting dimension

347
00:27:13,990 --> 00:27:18,390
it is why well what people in machine learning have called learning from relational data

348
00:27:21,450 --> 00:27:25,810
it's really motivated from kind of science point of view about thinking about our intuitive

349
00:27:25,810 --> 00:27:31,030
theories of the world so you think about somebody who's so learning about the structure

350
00:27:31,030 --> 00:27:35,960
of an academic department this was i guess motivated by US department but probably in

351
00:27:35,960 --> 00:27:39,410
in european departments something similar

352
00:27:39,430 --> 00:27:42,660
you have a bunch of people and you could observe the interactions let's say you

353
00:27:42,660 --> 00:27:46,060
observe who gives advice to who and you might want to make sense of this

354
00:27:46,060 --> 00:27:48,140
by organizing the people into

355
00:27:48,990 --> 00:27:51,010
where the

356
00:27:51,060 --> 00:27:52,580
the relationships

357
00:27:52,640 --> 00:27:58,530
abstract relationships between the groups that govern how individuals interact so for example you might

358
00:27:58,530 --> 00:28:04,530
recognise three groups professors grad and undergrad and professors kind of give advice to everyone

359
00:28:04,530 --> 00:28:08,030
grad students give advice undergrad grad give advice to know what i mean that's a

360
00:28:08,030 --> 00:28:10,410
very rough approximation

361
00:28:10,430 --> 00:28:13,470
another sort of more physical intuitive theory

362
00:28:13,490 --> 00:28:16,600
one that i like law and kids enjoy the sort of thing is a learning

363
00:28:16,600 --> 00:28:17,720
about magnets

364
00:28:17,740 --> 00:28:21,160
so let's say i give you a bunch of metal objects let's you don't know

365
00:28:21,160 --> 00:28:22,140
anything about

366
00:28:22,160 --> 00:28:24,700
about magnets they're just these

367
00:28:24,760 --> 00:28:28,240
you know identical looking metal objects you play with them and you see what i

368
00:28:28,240 --> 00:28:32,080
want to bring these together they seem to interact in some way when i bring

369
00:28:32,080 --> 00:28:35,150
these other ones together they don't interact how do you make sense of that will

370
00:28:35,150 --> 00:28:40,100
again you might group these into categories let's say magnets magnetic objects like which i

371
00:28:40,100 --> 00:28:44,990
mean magnetically susceptible things which are not themselves permanent magnets no magnetic objects and we

372
00:28:44,990 --> 00:28:48,470
might say well the magnets interact with each other so it that bring together to

373
00:28:48,470 --> 00:28:49,970
manage their interact

374
00:28:50,060 --> 00:28:54,530
magnets and magnetic objects interact magnetic objects do interact with each other so to magnetic

375
00:28:54,530 --> 00:28:58,830
objects do anything and i'm not going to cut doesn't interact with anything

376
00:28:58,850 --> 00:29:02,390
so that's again the very first approximation to how

377
00:29:02,410 --> 00:29:07,620
metal magnetism works for metallic objects you could of course make this richer by distinguishing

378
00:29:07,620 --> 00:29:11,200
different kinds of north and south magnetic poles and all sorts of things like that

379
00:29:11,370 --> 00:29:13,510
and the the learning problem here

380
00:29:13,530 --> 00:29:17,010
is to observe a matrix of interactions like it might be social interactions are might

381
00:29:17,010 --> 00:29:21,970
be physical interactions and and reconstruct something like this kind of a theory of what's

382
00:29:21,970 --> 00:29:25,260
going on so it in particular again you can think of this as a kind

383
00:29:25,260 --> 00:29:29,370
of clustering problem but it's now this relational clustering problem so we might say well

384
00:29:29,370 --> 00:29:32,600
we observe interactions in these people might turn out we can organize into these three

385
00:29:32,600 --> 00:29:37,700
groups with one group was called the professors who don't give advice to each other

386
00:29:37,740 --> 00:29:40,760
they more let's give advice to everyone else this is the data is a little

387
00:29:40,760 --> 00:29:43,220
bit noisy so maybe

388
00:29:43,240 --> 00:29:44,220
in the summer

389
00:29:44,260 --> 00:29:49,240
more less professors give advice to the grounds and the undergrads grad gemelli advice undergraduate

390
00:29:49,560 --> 00:29:55,510
document also get don't give advice anyone so again it's it's just like a a

391
00:29:55,530 --> 00:30:00,430
i found a certain way of sorting the rows and columns that revealed this

392
00:30:00,450 --> 00:30:04,800
this simple relational theory and we propose to way of doing this which is again

393
00:30:04,800 --> 00:30:08,430
a nonparametric model to take into account the idea that we don't necessarily know in

394
00:30:08,430 --> 00:30:10,370
advance how many ways there are

395
00:30:10,390 --> 00:30:13,780
the were what's the right where there are some right finite number of

396
00:30:13,830 --> 00:30:18,970
auditions this is called the infinite relational model and we've we mostly published this this

397
00:30:18,970 --> 00:30:22,600
is in triple i and we and others have used this for various kinds of

398
00:30:22,600 --> 00:30:23,620
machine learning

399
00:30:23,640 --> 00:30:28,410
tasks but original motivation was trying to think about people's intuitive theories again i will

400
00:30:28,410 --> 00:30:32,680
really go into the mathematical details but basically there's one of these

401
00:30:32,700 --> 00:30:38,260
chinese restaurant process priors over ways of partitioning the objects into categories here i have

402
00:30:38,260 --> 00:30:43,220
three categories but in principle there could be an infinite number of them and their

403
00:30:43,220 --> 00:30:47,160
parameters which unlike in traditional mixture model

404
00:30:47,180 --> 00:30:51,450
the parameters describe the features of the objects here these parameters describe interactions between pairs

405
00:30:51,450 --> 00:30:56,100
of objects we're basically putting one queen wait one binomial

406
00:30:56,120 --> 00:31:01,470
distribution on each block of this matrix so this is sometimes called the matrix block

407
00:31:01,490 --> 00:31:06,490
parameters of the block matrix and the idea is you know here a weighted coin

408
00:31:06,490 --> 00:31:10,830
away point nine generates these bits but the point of weight point once a generates

409
00:31:10,830 --> 00:31:11,660
those bits

410
00:31:11,660 --> 00:31:15,690
so this is the best generative model there is one hundred digits

411
00:31:15,710 --> 00:31:19,140
i say that in every lecture given nobody's yet challenge

412
00:31:19,160 --> 00:31:22,000
and in fact same long enough to become true

413
00:31:22,310 --> 00:31:31,480
so i just changed label here

414
00:31:31,540 --> 00:31:33,410
and now will generate AIDS

415
00:31:33,410 --> 00:31:36,910
once he settled into the ravine in generate it's

416
00:31:36,930 --> 00:31:39,140
one way of thinking about this model

417
00:31:39,230 --> 00:31:43,980
is as a way of dealing with the fact that all real world data

418
00:31:44,000 --> 00:31:48,000
it lies on low dim lies on or close to low dimensional manifolds

419
00:31:48,000 --> 00:31:50,310
in the normal way to deal with that is to say

420
00:31:50,330 --> 00:31:53,710
well it's too slow on the manifold is about twelve dimensional

421
00:31:53,730 --> 00:31:57,140
inside my machine must have twelve members are represented two

422
00:31:57,160 --> 00:32:01,330
in this figure out how to get to these numbers may be less tricky maybe

423
00:32:03,000 --> 00:32:06,770
and we associate sort of twelve numbers with two

424
00:32:08,210 --> 00:32:11,790
the problem with that is if the two has a very long tail

425
00:32:11,810 --> 00:32:15,370
it's actually a lot more degrees of freedom causes or so social fourteen degrees of

426
00:32:15,370 --> 00:32:16,770
freedom is long tail

427
00:32:16,790 --> 00:32:21,310
and obviously somewhere in between it's got thirteen half degrees of freedom

428
00:32:22,430 --> 00:32:25,410
degrees of freedom until one of the things you can do with the two

429
00:32:25,980 --> 00:32:27,890
by making a very wobbly too

430
00:32:28,020 --> 00:32:29,890
making not quite so good too

431
00:32:29,910 --> 00:32:32,350
but none spans turning around the pixels

432
00:32:32,390 --> 00:32:36,620
so that's sort of degree of some freedom but you know it's a bit but

433
00:32:36,620 --> 00:32:38,350
to do that but not too bad

434
00:32:38,370 --> 00:32:39,410
what's more

435
00:32:39,430 --> 00:32:42,660
the data might have many these manifolds you don't know how many

436
00:32:42,710 --> 00:32:46,560
in fact he might be the those thirty eleven of these manifolds

437
00:32:46,580 --> 00:32:49,850
but if you go up in energy appeared that if you allow things to be

438
00:32:49,850 --> 00:32:51,870
not quite so good to the merge

439
00:32:51,890 --> 00:32:54,190
you might have sevens and cross sections

440
00:32:54,210 --> 00:32:57,390
but if you get a bit less sort discriminating they might be sort of

441
00:32:57,450 --> 00:32:59,370
morris in the same manifold

442
00:32:59,430 --> 00:33:03,310
and so rather than dimensionality dealing with low dimensional data

443
00:33:03,330 --> 00:33:04,980
by explicitly

444
00:33:04,980 --> 00:33:06,500
trying to represent

445
00:33:06,520 --> 00:33:08,540
where you are on the manifold

446
00:33:08,540 --> 00:33:12,160
a much better way to do it i think is too

447
00:33:12,210 --> 00:33:16,620
take your data that lies on these low dimensional manifolds

448
00:33:16,640 --> 00:33:19,870
blow it up into some high dimensional space

449
00:33:19,870 --> 00:33:22,190
and in the high dimensional space

450
00:33:22,230 --> 00:33:24,160
have an energy function

451
00:33:24,160 --> 00:33:28,330
start off with everything equal energy so what's the ratty zero

452
00:33:28,430 --> 00:33:31,060
and then gradually dig ravines

453
00:33:31,120 --> 00:33:32,830
low dimensional ravines so

454
00:33:32,830 --> 00:33:36,160
if we say will work in this time it is the five hundred dimensions here

455
00:33:36,250 --> 00:33:41,080
then in this five hundred dimensional space we can integrate out these units so we

456
00:33:41,080 --> 00:33:44,600
can get what's called a free energy for these five hundred motion vectors

457
00:33:44,660 --> 00:33:49,750
and that's going to be ten ravines each which is long and skinny

458
00:33:49,770 --> 00:33:53,640
and has a few degrees of freedom along the floor of the living

459
00:33:53,640 --> 00:33:56,750
and then lost the degrees of freedom the side of it

460
00:33:56,790 --> 00:33:59,180
and in particular the ravine for two years

461
00:33:59,250 --> 00:34:02,950
might in some places come quite close to the to three is the might be

462
00:34:02,950 --> 00:34:06,460
quite an energy barrier that because we expect low density regions of the high energy

463
00:34:06,480 --> 00:34:08,430
regions between

464
00:34:08,680 --> 00:34:10,180
two classes

465
00:34:10,250 --> 00:34:13,410
actually for forcing lines that's not true the ravine falls in the range from nine

466
00:34:13,410 --> 00:34:15,540
sort merge into one point

467
00:34:15,560 --> 00:34:17,460
more less

468
00:34:18,370 --> 00:34:22,430
what this model is learned by this sort of greedy learning in very fine tuning

469
00:34:22,450 --> 00:34:25,390
is to turn the pixels

470
00:34:25,390 --> 00:34:27,450
into these features

471
00:34:27,500 --> 00:34:31,910
and the features of the property that by using these hidden units

472
00:34:31,930 --> 00:34:33,580
to express

473
00:34:33,580 --> 00:34:36,620
sort of energies of combinations of features

474
00:34:36,640 --> 00:34:39,190
we can make ravines in this space

475
00:34:39,210 --> 00:34:42,310
the capture the manifolds if we try to do it by

476
00:34:42,350 --> 00:34:45,540
interactions between pixels we couldn't do it

477
00:34:45,540 --> 00:34:50,370
you just can't express what two is by markov random field on pixels

478
00:34:50,390 --> 00:34:54,250
it is the current in markov random field powers interesting pixels such that the only

479
00:34:54,250 --> 00:34:56,580
happy states two years

480
00:34:56,580 --> 00:34:59,310
but as we've seen we can do it if we use multiple less

481
00:34:59,330 --> 00:35:01,410
the only happy states of the system

482
00:35:01,460 --> 00:35:03,410
when you stick when you claim that label

483
00:35:06,330 --> 00:35:08,600
i do one more because i it so much

484
00:35:19,730 --> 00:35:21,710
you can't generate from

485
00:35:27,290 --> 00:35:31,980
repeat the question convolutional networks do the same thing

486
00:35:32,000 --> 00:35:35,160
well show very good recognizing digits

487
00:35:35,160 --> 00:35:38,610
the convolutional neural networks won't allow you to generate from the model because they don't

488
00:35:38,610 --> 00:35:40,140
have a generative model

489
00:35:40,180 --> 00:35:43,770
if you think of the classic ones from the late nineties from the nineties

490
00:35:43,790 --> 00:35:47,620
more recently he has been doing more complicated things we just combine this kind of

491
00:35:47,620 --> 00:35:49,480
learning convolutional nets

492
00:35:50,390 --> 00:35:53,680
so basically i've infected younes convolutional nets

493
00:35:53,730 --> 00:35:57,080
and he's not doing stuff like this as well

494
00:36:00,980 --> 00:36:19,310
so these are just some

495
00:36:19,350 --> 00:36:21,810
samples generated model where you

496
00:36:21,830 --> 00:36:26,960
run four thousand alternating gives great iterations for example so the independent

497
00:36:26,960 --> 00:36:31,510
whatever comes out of you will indicate your weight

498
00:36:32,490 --> 00:36:33,900
a of c is

499
00:36:33,920 --> 00:36:36,420
smaller than ten

500
00:36:36,540 --> 00:36:37,690
that's meaningless

501
00:36:37,700 --> 00:36:39,370
attention can never be

502
00:36:40,720 --> 00:36:43,510
the string was negative tension has no physical meaning

503
00:36:43,520 --> 00:36:46,940
what it means is that the bucket of water would never have made it to

504
00:36:46,940 --> 00:36:48,020
this point

505
00:36:48,070 --> 00:36:49,940
if you try to swing it up

506
00:36:49,980 --> 00:36:53,800
someone tried in the second lecture we didn't make it to that point the bucket

507
00:36:53,800 --> 00:36:54,740
of water

508
00:36:54,790 --> 00:36:56,340
i will just fall

509
00:36:56,410 --> 00:36:59,690
you end up with a mess with better detail

510
00:37:01,260 --> 00:37:03,890
the bucket of water

511
00:37:03,920 --> 00:37:05,080
when is here

512
00:37:05,100 --> 00:37:09,420
if the acceleration in the absence of the iteration were exactly ten meters per second

513
00:37:10,390 --> 00:37:13,640
then the bucket of water would be

514
00:37:16,910 --> 00:37:19,440
i said earlier that when you in freefall

515
00:37:19,450 --> 00:37:22,890
o object in free-fall i eyewitness

516
00:37:22,940 --> 00:37:28,740
it's like a spacecraft in orbit or the elevator was cut cable

517
00:37:28,750 --> 00:37:30,670
it also means

518
00:37:30,710 --> 00:37:33,230
that if i jump off the table

519
00:37:33,280 --> 00:37:34,610
that i'm weightless

520
00:37:34,620 --> 00:37:36,790
while i am

521
00:37:36,840 --> 00:37:38,300
in midair so to speak

522
00:37:38,360 --> 00:37:40,120
it means this tennis ball

523
00:37:40,170 --> 00:37:41,980
while it is in freefall

524
00:37:42,030 --> 00:37:43,470
it has no weight

525
00:37:43,480 --> 00:37:46,250
now it has weights

526
00:37:46,260 --> 00:37:49,150
now the way even higher because i'm accelerating it

527
00:37:49,150 --> 00:37:50,960
and now it has no weight

528
00:37:51,010 --> 00:37:52,300
and small

529
00:37:53,040 --> 00:37:55,800
right and i assume for now that air drag

530
00:37:55,810 --> 00:37:58,460
plays no role

531
00:37:58,610 --> 00:38:03,410
if i jump of the table

532
00:38:03,490 --> 00:38:06,020
i will be weightless for about half a second

533
00:38:06,090 --> 00:38:08,140
this is about one meter

534
00:38:08,140 --> 00:38:11,060
if i jump from the power which is hundred metres high

535
00:38:11,090 --> 00:38:13,790
i will be waiting list for four and a half seconds

536
00:38:13,840 --> 00:38:15,980
ignoring and

537
00:38:15,980 --> 00:38:18,750
i prefer today

538
00:38:18,760 --> 00:38:22,040
the harvest second

539
00:38:22,100 --> 00:38:24,020
i'm going to jump of this table

540
00:38:24,110 --> 00:38:29,790
with this one in my hands

541
00:38:29,850 --> 00:38:31,030
i tell you

542
00:38:31,040 --> 00:38:33,350
how i can convince you

543
00:38:33,540 --> 00:38:37,600
as i jumped that i will indeed be weightless

544
00:38:37,650 --> 00:38:40,350
here is the bot

545
00:38:40,370 --> 00:38:43,840
the gravitational force on the ball

546
00:38:43,920 --> 00:38:46,350
my hands are pushing up

547
00:38:46,410 --> 00:38:47,750
on this portal

548
00:38:47,830 --> 00:38:49,680
my hands are being

549
00:38:49,680 --> 00:38:52,460
about scale i feel

550
00:38:52,510 --> 00:38:55,230
in my muscles the need to push up

551
00:38:55,230 --> 00:38:58,260
in fact i might even be able to estimate the weight

552
00:38:58,290 --> 00:39:00,940
playing the role of the bathroom scale

553
00:39:00,940 --> 00:39:02,770
to get in the water

554
00:39:02,820 --> 00:39:08,660
it's about nine pounds

555
00:39:08,660 --> 00:39:10,500
now my own body

556
00:39:10,510 --> 00:39:12,270
gravity is acting only

557
00:39:12,290 --> 00:39:13,720
but i'm being pushed up

558
00:39:13,730 --> 00:39:17,460
right there

559
00:39:19,900 --> 00:39:22,330
we jumped

560
00:39:22,390 --> 00:39:23,930
there would be no pushing

561
00:39:23,960 --> 00:39:24,980
for me on the

562
00:39:24,990 --> 00:39:26,040
well anymore

563
00:39:26,060 --> 00:39:27,940
not pushing there

564
00:39:28,100 --> 00:39:28,930
on the

565
00:39:30,170 --> 00:39:32,800
only gravitational will act upon us

566
00:39:32,820 --> 00:39:35,040
and we would be weightless

567
00:39:35,090 --> 00:39:37,290
how can i show you that we await us

568
00:39:37,420 --> 00:39:41,900
well if i don't have to use my muscles to push on this bottle upwards

569
00:39:41,950 --> 00:39:46,490
i might as well with my hands on little bit doing this

570
00:39:46,500 --> 00:39:47,710
and you will see

571
00:39:47,760 --> 00:39:52,040
that the bot will just stay above my hands without my having to push up

572
00:39:53,240 --> 00:39:56,750
being the bathroom scale i no longer have to push on it

573
00:39:56,780 --> 00:39:58,010
i no longer

574
00:39:58,010 --> 00:39:59,920
my muscles don't feel anything

575
00:40:00,010 --> 00:40:03,010
and the model is therefore we

576
00:40:03,070 --> 00:40:04,400
but almost weightless

577
00:40:04,400 --> 00:40:05,390
when we jump

578
00:40:05,400 --> 00:40:13,000
i am weightless and even his beagle is weightless all weightless during harvest second

579
00:40:13,010 --> 00:40:15,300
there is no such thing in physics

580
00:40:15,360 --> 00:40:16,740
as a free lunch

581
00:40:16,800 --> 00:40:19,900
you have to pay a price for his half a second

582
00:40:19,940 --> 00:40:21,640
of weightlessness

583
00:40:21,690 --> 00:40:22,750
what happens

584
00:40:22,770 --> 00:40:24,240
when i hit the floor

585
00:40:24,240 --> 00:40:25,710
i think the floor with

586
00:40:25,740 --> 00:40:30,600
velocity in this direction which is about five meters per second you can calculate

587
00:40:30,750 --> 00:40:33,600
but a little later i've come to stop

588
00:40:33,650 --> 00:40:34,550
that means

589
00:40:34,570 --> 00:40:38,410
during impact must be acceleration upwards

590
00:40:38,450 --> 00:40:41,180
otherwise my velocity in this direction could never

591
00:40:41,230 --> 00:40:42,900
become zero

592
00:40:44,450 --> 00:40:49,840
i will weigh more doing this impact there's an acceleration in this direction

593
00:40:49,900 --> 00:40:51,950
the five meters per second

594
00:40:51,970 --> 00:40:53,730
goes to zero

595
00:40:53,770 --> 00:40:56,450
if i make the assumption that it takes two kinds of the second that's a

596
00:40:56,450 --> 00:40:58,770
very rough guess these impact time

597
00:40:58,820 --> 00:41:03,410
then the average acceleration will be five meters per second divided by o point two

598
00:41:03,510 --> 00:41:06,500
that is twenty five meters per second squared

599
00:41:06,590 --> 00:41:10,950
that means acceleration upwards two-and-a-half g

600
00:41:10,990 --> 00:41:14,120
that means i wait three-and-a-half times more

601
00:41:14,180 --> 00:41:18,040
remember a posting to a historian have to be up

602
00:41:18,070 --> 00:41:21,190
plus the gene that we already have the next three and a half

603
00:41:21,190 --> 00:41:26,250
she so instead of three hundred sixty five pounds i close to six hundred pounds

604
00:41:26,250 --> 00:41:28,310
for two sensible sector

605
00:41:28,420 --> 00:41:31,100
so we get for faces right now

606
00:41:31,120 --> 00:41:32,330
i'm a normal weight

607
00:41:32,340 --> 00:41:34,040
if i stand on a bathroom scale

608
00:41:34,100 --> 00:41:36,510
i john for half a second weightless

609
00:41:36,560 --> 00:41:37,690
but the floor

610
00:41:37,690 --> 00:41:39,510
for about two kinds of the second

611
00:41:39,560 --> 00:41:41,680
maybe close to six hundred pounds

612
00:41:41,680 --> 00:41:43,050
and then after that

613
00:41:43,150 --> 00:41:45,740
one of my normal weight again

614
00:41:45,740 --> 00:41:48,250
now you're going to have only half a second

615
00:41:48,270 --> 00:41:49,360
two c

616
00:41:49,370 --> 00:41:51,550
that this model as i j

617
00:41:51,600 --> 00:41:55,140
it's floating above my hands i will pull my hands off

618
00:41:55,180 --> 00:41:56,260
so you will see

619
00:41:56,430 --> 00:41:58,500
i no longer have to push

620
00:41:58,570 --> 00:41:59,850
that means

621
00:41:59,900 --> 00:42:02,770
it's weight was

622
00:42:02,850 --> 00:42:04,160
you're ready

623
00:42:04,160 --> 00:42:05,460
i'm ready

624
00:42:06,840 --> 00:42:08,540
one zero

625
00:42:08,550 --> 00:42:11,730
you see floating above my hands

626
00:42:11,770 --> 00:42:12,720
we were both

627
00:42:16,490 --> 00:42:17,870
i have been

628
00:42:17,930 --> 00:42:19,540
thinking about this

629
00:42:19,600 --> 00:42:21,430
for a long long time

630
00:42:21,470 --> 00:42:23,700
i have been thinking whether perhaps

631
00:42:23,710 --> 00:42:25,410
this could not be shown

632
00:42:25,450 --> 00:42:27,650
in a more

633
00:42:27,690 --> 00:42:30,020
dramatic perhaps even more

634
00:42:30,040 --> 00:42:31,620
convincing way

635
00:42:31,740 --> 00:42:33,310
and so i thought of the idea

636
00:42:33,330 --> 00:42:36,240
of food in the bathroom scale under my feet

637
00:42:36,240 --> 00:42:39,840
applying it very loosely so that wouldn't fall off when i john

638
00:42:39,860 --> 00:42:41,450
and then show you

639
00:42:41,460 --> 00:42:48,370
that's why i'm half a second in in fall the bathroom scale indeed indicates zero

640
00:42:48,430 --> 00:42:51,820
i don't think that i haven't tried to have tried many times as many bathroom

641
00:42:51,830 --> 00:42:53,790
skills made many jobs

642
00:42:53,840 --> 00:42:55,310
there's a problem

643
00:42:55,320 --> 00:42:58,550
and the problem is the bathroom scales by

644
00:42:58,690 --> 00:43:00,990
you normally get commercially

645
00:43:00,990 --> 00:43:05,040
they indeed wants to go to zero it takes a long time

646
00:43:05,070 --> 00:43:09,120
they have a lot of inertia the response time is slow

647
00:43:09,190 --> 00:43:12,960
even if they make it to zero by the time you get the floor

648
00:43:13,670 --> 00:43:15,640
immediately the

649
00:43:15,690 --> 00:43:19,470
weight increases become it before you wait comes up by three and a half time

650
00:43:19,470 --> 00:43:20,600
so i have

651
00:43:20,690 --> 00:43:24,550
things like classes and properties so i can kinda begin to build up schema that

652
00:43:24,550 --> 00:43:27,860
say things like person is class hascolleague the property

653
00:43:27,870 --> 00:43:30,060
professor subclassof person

654
00:43:30,220 --> 00:43:32,990
and this is kind of trying to represent some of the data that i talked

655
00:43:32,990 --> 00:43:33,980
about before

656
00:43:33,990 --> 00:43:39,050
and and specify some some kind of restrictions on the way in which are my

657
00:43:39,050 --> 00:43:41,400
range and domain should be interpreted

658
00:43:41,410 --> 00:43:45,570
OK and then then we have some some machinery some some RDF the

659
00:43:45,580 --> 00:43:48,710
schema so the distance liberality here

660
00:43:48,720 --> 00:43:50,600
OK so we have things like

661
00:43:50,610 --> 00:43:52,400
there's no

662
00:43:52,410 --> 00:43:56,190
distinction between my classes and instances OK so i could do lots of kind of

663
00:43:56,190 --> 00:43:59,640
quite powerful stuff in my in RDF and RDF schema

664
00:43:59,730 --> 00:44:05,270
so here i have this this kind of metamodelling notion that got liu's ally in

665
00:44:05,660 --> 00:44:09,690
line is a species and species class of this kind of species notion is so

666
00:44:09,690 --> 00:44:13,780
lions playing both the role of the kind of a collection of things the collection

667
00:44:13,780 --> 00:44:17,560
of all the lines and also as an individual which is itself species so i

668
00:44:17,560 --> 00:44:22,250
can do this kind of metamodelling notions RDF schema i could talk about the properties

669
00:44:22,800 --> 00:44:26,800
properties OK so i can in the class of properties but i can also to

670
00:44:26,820 --> 00:44:31,500
say some some rather strange stuff because my my RDF vocabulary is kind of available

671
00:44:31,500 --> 00:44:36,840
to make to the RDF resources there's no clear distinction made between language constructors and

672
00:44:36,840 --> 00:44:41,810
ontology vocabulary so i can potentially do some kind of quite strange stuff in in

673
00:44:41,910 --> 00:44:45,450
applying the constructors to themselves

674
00:44:45,450 --> 00:44:48,290
so in order to deal with this we have some semantics are not going to

675
00:44:48,290 --> 00:44:52,450
go into the details of this cave essentially this this kind of just tells you

676
00:44:53,190 --> 00:44:57,800
one should be interpreting the special built pieces of vocabulary into an describes the kind

677
00:44:57,800 --> 00:45:01,190
of inference is that you might expect to be able to draw from

678
00:45:01,230 --> 00:45:04,170
and among these are kind of the

679
00:45:04,180 --> 00:45:09,220
things that that kind of one would expect based here if i had some graph

680
00:45:09,220 --> 00:45:14,190
here i describe lectures being a subclass of academic academic is a subclass of person

681
00:45:14,190 --> 00:45:15,930
these things are all classes

682
00:45:15,940 --> 00:45:19,770
OK the RDF semantics will tell me that i can expect to find a triple

683
00:45:19,790 --> 00:45:22,350
it tells me that lectures across the person

684
00:45:22,370 --> 00:45:27,170
so essentially this is just telling me that subclasses some kind of transitive relationship

685
00:45:27,180 --> 00:45:30,830
OK this is what i would expect if i need to have a semantic machinery

686
00:45:30,830 --> 00:45:31,980
there to be able to

687
00:45:32,000 --> 00:45:33,210
kind of makes this

688
00:45:33,330 --> 00:45:35,790
make this inference

689
00:45:35,810 --> 00:45:43,160
and similarly if i got here and individual shown them saying sean lecturer lectures subclass

690
00:45:43,160 --> 00:45:46,870
of academic again i can make an inference here that i can i can deduce

691
00:45:46,870 --> 00:45:48,260
that there should be a triple here

692
00:45:48,270 --> 00:45:52,200
it tells the shown as an academic these are the ways that one would just

693
00:45:53,420 --> 00:45:57,540
subclass is an instantiation to work OK but we need to enshrine that within the

694
00:45:57,690 --> 00:46:00,530
machinery of language in order to be able to to actually make this

695
00:46:00,960 --> 00:46:04,680
make this happen and the inference happen for us

696
00:46:04,730 --> 00:46:07,720
so what does RDF schema give us well

697
00:46:07,730 --> 00:46:12,620
it gives us the ability to use some kind of simple schema vocabulary so i

698
00:46:12,620 --> 00:46:15,950
can begin to talk about classes of things i can begin to

699
00:46:15,950 --> 00:46:21,420
build this some sort of taxonomy of how i believe my domain fits together i

700
00:46:21,420 --> 00:46:25,830
get some and through that i can get some consistent vocabulary use case we can

701
00:46:25,830 --> 00:46:30,210
begin to share the classes that we use to describe our object

702
00:46:30,230 --> 00:46:33,470
i get some simple inference in terms of the

703
00:46:33,490 --> 00:46:36,740
the inference is that i just showed you

704
00:46:38,930 --> 00:46:40,940
there are some

705
00:46:42,300 --> 00:46:43,650
OK in terms of

706
00:46:43,690 --> 00:46:46,040
expressiveness OK so if i want to

707
00:46:46,100 --> 00:46:47,530
and if you if you remember

708
00:46:47,550 --> 00:46:53,120
i'm kind of living here and i'm talking largely in this kind of big world

709
00:46:53,120 --> 00:46:56,430
OK so that what kind of started to describe things

710
00:46:56,470 --> 00:47:00,020
in level of detail and i want to express in this to be able to

711
00:47:00,020 --> 00:47:03,260
come describe how i think my domain fits together

712
00:47:03,340 --> 00:47:07,300
so the problem with RDF schema is that it's really quite we in terms of

713
00:47:07,300 --> 00:47:10,430
its expressivity in terms of the things that i can see in the things i

714
00:47:10,430 --> 00:47:12,880
can describe classes that i have

715
00:47:12,880 --> 00:47:18,190
so for example i can express localised range and domain constraints

716
00:47:18,210 --> 00:47:22,010
things like if i have a published by

717
00:47:22,020 --> 00:47:25,380
OK so i journal is published by a publisher

718
00:47:25,390 --> 00:47:28,150
and the technical reports published by an institution

719
00:47:28,190 --> 00:47:31,220
OK so i can't there's nothing in RDF schema that will allow me to kind

720
00:47:31,220 --> 00:47:35,930
of essentially represent that kind of constraint OK so i want to know that this

721
00:47:35,930 --> 00:47:41,050
relationship kind behave differently when it when it's applied to different kinds of objects

722
00:47:41,110 --> 00:47:44,860
i can talk about existential or cardinality constraints

723
00:47:44,870 --> 00:47:48,880
OK so i can't say for example the papers must have off which is the

724
00:47:49,900 --> 00:47:53,880
for that you have got to have at least three reviewers there's no

725
00:47:53,890 --> 00:47:57,670
this is just no vocabulary that allows me to do that in RDF schema and

726
00:47:57,670 --> 00:48:01,860
similarly i can't talk about the characteristics of my properties so

727
00:48:01,920 --> 00:48:04,930
i want to say there is some of entropies transitive

728
00:48:04,940 --> 00:48:09,090
so any sub event subevents some events of the major events in the case of

729
00:48:10,450 --> 00:48:17,090
and nor can i say that the rules have inverses cases you always quite important

730
00:48:17,120 --> 00:48:21,190
my modelling want that hasrole is the inverse of a rule that i can again

731
00:48:21,190 --> 00:48:25,460
i can represent shooting final RDF schema

732
00:48:25,510 --> 00:48:30,960
so it's it's rather kind of weak language in terms of its expressivity however

733
00:48:30,980 --> 00:48:35,860
conversely it's actually quite is quite as a strong and rich language because of some

734
00:48:35,860 --> 00:48:37,990
of the things that i talked about earlier

735
00:48:38,000 --> 00:48:42,420
so for example because i can apply the RDF primitives to the to the the

736
00:48:42,420 --> 00:48:47,890
vocabulary itself OK we we need to have something we need to have machinery in

737
00:48:47,930 --> 00:48:52,820
semantics that allows us to deal with another resulted actually quite difficult to provide a

738
00:48:52,820 --> 00:48:56,220
full reasoning support for RDF to kind of cope with all of

739
00:48:56,240 --> 00:49:01,150
then that expressivity so one of can it's very expressive in some ways but it's

740
00:49:01,150 --> 00:49:03,980
also quite inexpensive and others when you want to talk about the kind of class

741
00:49:06,660 --> 00:49:07,940
so the solution

742
00:49:07,960 --> 00:49:11,810
OK so we're kind of this is this is kind of where we're all comes

743
00:49:11,810 --> 00:49:14,690
in is to provide a language that has

744
00:49:14,690 --> 00:49:20,770
sits within the standards OK so within RDF RDF schema and so on and the

745
00:49:20,770 --> 00:49:25,930
desire the one the desirable features it's easy to understand and use OK again rather

746
00:49:25,930 --> 00:49:28,620
dangerous tend to use objective here

747
00:49:29,180 --> 00:49:33,580
but of adequate expressive power so we were able to capture some more of the

748
00:49:33,580 --> 00:49:37,190
kind of things that we wanted to build models and also we have some formal

749
00:49:37,190 --> 00:49:41,830
specification case that we can provide some automated reasoning support this is where this is

750
00:49:41,830 --> 00:49:44,030
where outcomes in

751
00:49:44,080 --> 00:49:45,760
a little bit of of

752
00:49:45,780 --> 00:49:49,370
background all this is just some kind of historical stuff

753
00:49:49,370 --> 00:49:52,650
and so there were a couple of efforts that were going on i guess at

754
00:49:52,650 --> 00:49:54,280
the end of the

755
00:49:54,310 --> 00:49:56,340
in the nineties really so there was a

756
00:49:56,650 --> 00:50:00,850
there them along which was kind of language being developed under the daml program this

757
00:50:00,850 --> 00:50:07,000
was kind of taking pulling together RDF and RDF schema and frame based approaches at

758
00:50:07,000 --> 00:50:11,420
the same time over here in lies in europe was language called OIL which is

759
00:50:11,420 --> 00:50:16,700
being developed mainly under the auspices of the ontoknowledge project possible some other people

760
00:50:16,730 --> 00:50:21,470
and this was taking approach taking RDF RDF schema and kind of framework modelling approach

761
00:50:21,470 --> 00:50:27,410
and pulling in the formal semantics description logic world OK which we'll talk about later

762
00:50:27,530 --> 00:50:30,400
both these languages then fed into daml OIL

763
00:50:30,410 --> 00:50:34,270
OK and then down was used as the starting point essentially for the w three

764
00:50:34,270 --> 00:50:38,950
c activity they ended up with the yellow recommendation this is kind of where where

765
00:50:38,960 --> 00:50:42,450
some of the stuff has has come from

766
00:50:42,480 --> 00:50:46,670
so i mentioned description logics and i think i think jim used to at some

767
00:50:46,670 --> 00:50:52,100
point during his presentation so what are description logics DL so there

768
00:50:52,100 --> 00:50:56,390
their family of knowledge representation languages OK case there are kind of member of the

769
00:50:56,390 --> 00:50:59,660
DL learning on which construction is one employees and so on

770
00:50:59,670 --> 00:51:05,740
right to work in semantic networks and KL one the desire to try and formally

771
00:51:05,740 --> 00:51:10,000
specify what was going on in semantic networks and they describe domain in terms of

772
00:51:10,000 --> 00:51:15,750
concepts roles and individuals case fitting in with this paradigm i talked about before

773
00:51:15,750 --> 00:51:17,940
just using small positions

774
00:51:18,000 --> 00:51:22,150
we can prove that were purely random forest which is the reason i presented to

775
00:51:22,800 --> 00:51:26,990
which is exactly the reason for which is kind of

776
00:51:27,240 --> 00:51:30,170
i don't want somebody interested is consistent

777
00:51:30,180 --> 00:51:32,140
and the should

778
00:51:32,190 --> 00:51:34,990
but the distributions for one

779
00:51:35,040 --> 00:51:39,150
john roberts assumption kind of just got on me

780
00:51:39,190 --> 00:51:41,690
max vision to

781
00:51:41,750 --> 00:51:45,180
this can environment for some reason which is also

782
00:51:45,240 --> 00:51:47,430
so this is the first step

783
00:51:47,450 --> 00:51:50,500
and show that the very reason for

784
00:51:50,510 --> 00:51:52,780
some represent for

785
00:51:52,790 --> 00:51:54,460
university course

786
00:51:54,470 --> 00:52:06,070
that's correct

787
00:52:06,450 --> 00:52:09,840
so you can

788
00:52:09,860 --> 00:52:14,710
number of you but i will come back the original run we see

789
00:52:14,770 --> 00:52:16,240
we can get

790
00:52:16,250 --> 00:52:17,660
the original

791
00:52:18,410 --> 00:52:23,710
now what's interesting here is that the problem that randomized first system

792
00:52:23,740 --> 00:52:24,660
the average

793
00:52:24,700 --> 00:52:25,710
because it

794
00:52:25,800 --> 00:52:29,280
this is not the real question recreation is the converse is true

795
00:52:29,360 --> 00:52:32,620
i mean you have original bad guys

796
00:52:32,670 --> 00:52:37,240
can the candidate become better when get the question which are first in terms of

797
00:52:37,360 --> 00:52:38,970
the converse is not true

798
00:52:39,030 --> 00:52:43,210
the main message on my talk about randomized average

799
00:52:43,270 --> 00:52:48,280
well because the base classifiers into consistent ones

800
00:52:49,780 --> 00:52:52,870
i will show you how to run the first condition is that start with a

801
00:52:52,870 --> 00:52:54,560
very simple example which is

802
00:52:54,640 --> 00:52:57,320
an example which is inspired by that

803
00:52:57,500 --> 00:53:00,050
contraction of the navigation

804
00:53:00,060 --> 00:53:04,370
and the start with a very simple rule which we call a which is the

805
00:53:04,390 --> 00:53:07,430
one nearest which are going to school

806
00:53:07,450 --> 00:53:09,780
this is what we all the schools

807
00:53:09,790 --> 00:53:12,530
you want to make this school when don't know

808
00:53:12,560 --> 00:53:14,840
just look at the nearest neighbor

809
00:53:16,410 --> 00:53:18,910
one way

810
00:53:19,070 --> 00:53:20,970
it's cool but not in the know

811
00:53:21,080 --> 00:53:22,500
mathematical way away

812
00:53:22,610 --> 00:53:24,650
because this is not consistent

813
00:53:24,660 --> 00:53:30,650
of course just one nearest neighbor except a few regression some special and whatever is

814
00:53:30,650 --> 00:53:33,300
is not of course

815
00:53:36,770 --> 00:53:40,370
to make this request is so we can use the which just like very much

816
00:53:40,520 --> 00:53:42,130
to just go back

817
00:53:42,210 --> 00:53:46,070
back is also randomize reason randomisation machine

818
00:53:46,130 --> 00:53:49,640
by generating many bootstrap samples from the original

819
00:53:49,680 --> 00:53:53,770
and what we are going to do here is to apply back to the nearest

820
00:53:53,770 --> 00:53:55,090
neighbour algorithm reason

821
00:53:55,110 --> 00:54:00,700
just consider very simple model where each data pair just with some probability

822
00:54:00,750 --> 00:54:04,370
and we have some process and the sample

823
00:54:04,410 --> 00:54:08,170
it's that depend on each other so we should approach to it

824
00:54:08,180 --> 00:54:12,210
this way justifying randomized classifier which

825
00:54:12,310 --> 00:54:15,810
classifier which is of course some original

826
00:54:16,610 --> 00:54:18,310
of size and just fine

827
00:54:18,320 --> 00:54:24,400
so lets you picture with this

828
00:54:24,410 --> 00:54:28,910
so you have your original version of this room because samples to take charge separation

829
00:54:28,960 --> 00:54:33,290
with probability qn your don't subsampling which is final

830
00:54:33,340 --> 00:54:34,280
so i

831
00:54:34,290 --> 00:54:35,580
the original one

832
00:54:35,640 --> 00:54:37,090
the correct decision

833
00:54:37,150 --> 00:54:38,630
nearest neighbour decision

834
00:54:38,670 --> 00:54:40,890
so this is the decision of the first

835
00:54:43,650 --> 00:54:46,240
you do this one's enough to of course

836
00:54:46,290 --> 00:54:48,290
way you back

837
00:54:48,290 --> 00:54:54,700
the second term the difference is that some of their decision so this was once

838
00:54:54,700 --> 00:54:55,130
you want

839
00:54:55,790 --> 00:54:57,750
we get this

840
00:54:57,760 --> 00:54:59,900
and what we can prove

841
00:55:01,070 --> 00:55:03,390
that average one nearest neighbour classifier

842
00:55:03,400 --> 00:55:08,470
which is just an aggregation of many that five is consistent if and of course

843
00:55:09,710 --> 00:55:12,540
two and the the sampling rate

844
00:55:12,690 --> 00:55:15,170
and so

845
00:55:15,190 --> 00:55:17,670
it turns out that the proof

846
00:55:17,710 --> 00:55:20,720
it is based on how human by storms

847
00:55:20,740 --> 00:55:21,820
which proves

848
00:55:21,820 --> 00:55:24,180
we now have this knowledge

849
00:55:24,960 --> 00:55:26,460
it just makes

850
00:55:30,640 --> 00:55:33,720
in this

851
00:55:33,740 --> 00:55:40,550
i mean the difference here is

852
00:55:40,720 --> 00:55:45,350
the station is inserted before we had four

853
00:55:45,360 --> 00:55:50,050
the sum of of five i know as a no really

854
00:55:52,210 --> 00:55:55,680
every element of k which is now of

855
00:55:55,680 --> 00:56:01,670
below us so we have for each s makes waves are s

856
00:56:01,680 --> 00:56:06,700
the rate of each dimension h two mix is a two dimen two directors in

857
00:56:06,700 --> 00:56:08,590
each dimension

858
00:56:08,610 --> 00:56:10,230
so we have in two dimensions rich

859
00:56:10,250 --> 00:56:17,190
the relation next direction again the direction index dimensions and then twice and y dimensions

860
00:56:17,430 --> 00:56:20,770
this is a much smaller space because we have much more

861
00:56:20,780 --> 00:56:23,910
right now we can

862
00:56:23,910 --> 00:56:26,730
but the space we need for the sparse grid

863
00:56:26,750 --> 00:56:32,750
the reagan has age to make sense of this function is not responded and in

864
00:56:32,750 --> 00:56:37,090
some so we normally only takes in all secondary does not

865
00:56:37,100 --> 00:56:39,270
all the smaller ones as well

866
00:56:39,370 --> 00:56:46,790
what's quite this space it it just makes also tensor product structure so we can

867
00:56:46,790 --> 00:56:48,280
write this xmax

868
00:56:48,290 --> 00:56:52,790
just as the product of the one dimensional as HS spaces

869
00:56:52,970 --> 00:56:57,380
but there's also a reproducing kernel hilbert space by the way

870
00:56:57,510 --> 00:57:00,680
just because he h once the features

871
00:57:00,690 --> 00:57:11,860
OK just of the tensor product is one is what

872
00:57:11,880 --> 00:57:14,140
so let's assume we have

873
00:57:14,160 --> 00:57:15,960
the function f or of

874
00:57:15,990 --> 00:57:18,660
so outraged as

875
00:57:18,670 --> 00:57:22,860
zero makes means your bona fides because it makes all this stuff a bit easier

876
00:57:22,870 --> 00:57:27,100
to not consider the binary value but you can do it as well

877
00:57:29,500 --> 00:57:34,910
so you can easily show just a bit of calculations that

878
00:57:34,930 --> 00:57:38,940
these basis functions to normal bounded by this term

879
00:57:38,950 --> 00:57:40,660
cluster based calculation

880
00:57:43,210 --> 00:57:47,280
convention for the record the coefficients of the function of h two

881
00:57:48,070 --> 00:57:51,460
so to call freshened

882
00:57:51,460 --> 00:57:53,400
corresponding to the basis functions

883
00:57:53,410 --> 00:57:54,980
filer j

884
00:57:55,010 --> 00:58:00,100
this justice to try to prove was

885
00:58:00,950 --> 00:58:05,120
kind of simple proof and we can sometimes simply one dimensions

886
00:58:05,140 --> 00:58:09,100
so we take into google over omega field j

887
00:58:09,120 --> 00:58:10,710
second the rate of

888
00:58:10,740 --> 00:58:13,350
of a function

889
00:58:13,360 --> 00:58:19,020
so this is essentially since is a local business function we don't have to go

890
00:58:19,040 --> 00:58:23,490
over the support of his basis functions support goes for

891
00:58:23,500 --> 00:58:28,450
good point it excelled j mines h two could such plus h

892
00:58:28,490 --> 00:58:30,730
the media integration by parts

893
00:58:30,750 --> 00:58:33,060
so we move

894
00:58:33,070 --> 00:58:36,780
one partial derivative

895
00:58:37,050 --> 00:58:39,350
to the basis function and get

896
00:58:39,450 --> 00:58:42,030
this term and then we could also

897
00:58:42,050 --> 00:58:44,740
i wonder values

898
00:58:44,760 --> 00:58:47,840
could minus as well as we do is

899
00:58:47,840 --> 00:58:50,090
um to

900
00:58:50,100 --> 00:58:53,880
so we had a

901
00:58:55,670 --> 00:59:01,980
this we can just can just use effectively before that basis functions the value of

902
00:59:01,990 --> 00:59:06,190
the basis function is bounded between use

903
00:59:06,210 --> 00:59:08,050
it is the story

904
00:59:08,060 --> 00:59:11,500
i'm just takes one of which total set

905
00:59:11,520 --> 00:59:16,490
and the distance of the flow left and music can collect

906
00:59:16,500 --> 00:59:18,960
and this is just as before

907
00:59:18,980 --> 00:59:23,040
this is just a regular plus just off the tree

908
00:59:23,050 --> 00:59:26,150
o twice of the tree

909
00:59:26,160 --> 00:59:27,700
so this is

910
00:59:27,750 --> 00:59:30,460
for new moves

911
00:59:30,500 --> 00:59:32,940
age group mexico that's exactly

912
00:59:32,950 --> 00:59:36,030
what i wanted to show here

913
00:59:38,360 --> 00:59:41,770
see why we need to h two

914
00:59:41,810 --> 00:59:43,440
stuff because we

915
00:59:43,450 --> 00:59:44,270
to show

916
00:59:44,340 --> 00:59:46,040
but i have be able to do this

917
00:59:46,060 --> 00:59:48,000
and h two mix

918
00:59:48,020 --> 00:59:52,110
because if we do the tensor product we go to higher dimensions and we need

919
00:59:52,210 --> 00:59:57,110
a product of all these secondary so that's why was aged two mix

920
00:59:57,120 --> 00:59:57,900
comes from

921
01:00:02,100 --> 01:00:04,200
so now we have to record coefficients

922
01:00:12,930 --> 01:00:14,230
i have this

923
01:00:14,280 --> 01:00:15,820
and now we have to

924
01:00:15,840 --> 01:00:19,920
take it apart and a bit more so now we

925
01:00:19,940 --> 01:00:23,260
after the dimensional version we take just

926
01:00:23,280 --> 01:00:26,320
that's what had before you know it takes us out of this

927
01:00:27,050 --> 01:00:28,920
and this is just

928
01:00:30,010 --> 01:00:34,130
just the second derivative of the mexican directors on the support of this function

929
01:00:34,360 --> 01:00:37,010
and this we just cannot

930
01:00:37,030 --> 01:00:38,030
as before

931
01:00:38,090 --> 01:00:41,590
by above the system

932
01:00:41,610 --> 01:00:45,440
so we have seen of j is bounded by some constant which depends on the

933
01:00:45,440 --> 01:00:48,400
dimensions to the power

934
01:00:48,440 --> 01:00:50,740
of some term so level

935
01:00:50,760 --> 01:00:53,320
some of the scenes and support

936
01:00:53,630 --> 01:00:55,550
not support

937
01:00:55,570 --> 01:00:59,440
so each to mixed norm on the support of f on the support of field

938
01:01:00,150 --> 01:01:03,940
and we can send just just have to

939
01:01:03,960 --> 01:01:05,590
put all these

940
01:01:05,650 --> 01:01:08,630
local stuff together

941
01:01:09,530 --> 01:01:10,970
shows that c

942
01:01:10,990 --> 01:01:14,150
two norm of f

943
01:01:14,170 --> 01:01:16,360
can be

944
01:01:16,380 --> 01:01:23,170
founded on the bottom of the african represent f over f of all of these

945
01:01:23,170 --> 01:01:26,240
records places we can show that each

946
01:01:26,240 --> 01:01:28,030
component of set

947
01:01:28,050 --> 01:01:29,130
this despite

948
01:01:29,150 --> 01:01:33,280
but this term so we just fluxes in

949
01:01:33,340 --> 01:01:36,760
and use effect that effects from before that

950
01:01:39,670 --> 01:01:42,030
put all these

951
01:01:42,110 --> 01:01:44,670
local stuff together so that we get

952
01:01:44,670 --> 01:01:47,560
and then we need to be a reference ontology

953
01:01:47,570 --> 01:01:54,330
we will have a different locale ontologies for describing the different estimates sources and we

954
01:01:54,330 --> 01:02:00,450
need mapping between the local ontologies handed out of aces mappings between the local ontologies

955
01:02:00,450 --> 01:02:01,850
and the

956
01:02:01,870 --> 01:02:03,190
the reference

957
01:02:03,200 --> 01:02:06,550
the reference ontology so in that case i will present the

958
01:02:06,950 --> 01:02:12,570
yes an idea of how we have been affiliated network of ontologies in

959
01:02:12,610 --> 01:02:14,410
in this summer

960
01:02:14,440 --> 01:02:18,800
so the first thing is that for building the reference ontology so we have a

961
01:02:18,800 --> 01:02:21,920
using standard in the human resources manager

962
01:02:21,960 --> 01:02:23,450
we are using

963
01:02:23,500 --> 01:02:27,340
ontologies and ontology repositories and we

964
01:02:27,350 --> 01:02:33,700
our lives in the different scheme and that the sources for building these reference ontology

965
01:02:33,700 --> 01:02:39,930
so that the output will be the reference ontology and forbidding the local ontologies we

966
01:02:40,020 --> 01:02:45,860
take as input the the reference ontology and the different schema that the sources in

967
01:02:45,860 --> 01:02:52,860
order to produce several local ontologies one local ontologies for the employment a four four

968
01:02:52,860 --> 01:02:54,160
percent in the

969
01:02:55,560 --> 01:02:59,950
the sources of the different different employment services after that

970
01:03:00,010 --> 01:03:04,640
we will generated mappings between the pervasive

971
01:03:04,670 --> 01:03:05,640
the locale

972
01:03:05,650 --> 01:03:08,510
ontologies using our tool technology

973
01:03:08,520 --> 01:03:15,110
we will also using with some of the knowledge to represent the mapping between the

974
01:03:15,110 --> 01:03:21,090
local ontologies the reference ontology some based on that we create the mappings

975
01:03:21,640 --> 01:03:23,750
the at idea end what we have for

976
01:03:23,760 --> 01:03:27,410
knowledge resources the first one is the reference ontology

977
01:03:27,430 --> 01:03:31,850
four types of knowledge resources the first one is the reference ontology

978
01:03:31,900 --> 01:03:36,790
the second one is all the local ontologies the third one are the mappings between

979
01:03:36,790 --> 01:03:42,610
local ontologies that are based on the fourth one of the mappings between local ontologies

980
01:03:45,590 --> 01:03:46,890
so far below the

981
01:03:46,910 --> 01:03:48,360
the reference

982
01:03:51,540 --> 01:03:53,700
the first things we did is

983
01:03:53,760 --> 01:03:55,870
two to do the ontology

984
01:03:55,950 --> 01:03:58,860
the specification for that we used

985
01:03:58,900 --> 01:04:06,830
the makes named competency questions created by the building and weak focuses the competency questions

986
01:04:06,830 --> 01:04:11,910
related to in this domain to twelve what related to job seekers so in the

987
01:04:11,910 --> 01:04:17,160
competency questions about the article we have something like this he said educational level of

988
01:04:17,160 --> 01:04:20,700
for the job offer we have what are the required skills for the you will

989
01:04:20,700 --> 01:04:26,180
for the last half questions related with time and data management like when the job

990
01:04:26,180 --> 01:04:30,080
seeker completed completed his or her first

991
01:04:30,080 --> 01:04:31,890
with currencies and you know

992
01:04:31,910 --> 01:04:38,150
questions like given employ information economic activity of the employee employer and you will find

993
01:04:39,560 --> 01:04:41,870
what jobseekers had the most appropriate

994
01:04:41,890 --> 01:04:46,790
so from this question what we we do is to to get the vocabulary

995
01:04:46,800 --> 01:04:48,100
so we identified

996
01:04:48,110 --> 01:04:49,920
for instance that we

997
01:04:51,000 --> 01:04:54,770
we have the question we can identify that we need some terms like contract type

998
01:04:55,720 --> 01:04:58,590
what conditions difficult job offers

999
01:04:58,590 --> 01:05:05,040
from the and we we can get information about this is a part-time job or

1000
01:05:05,040 --> 01:05:08,790
the salary three thousand euros and based his work on the

1001
01:05:08,860 --> 01:05:12,520
that we identify from the questions and the answers to the compared to to the

1002
01:05:12,520 --> 01:05:14,060
question we

1003
01:05:14,210 --> 01:05:19,180
are able to identify the first set of terms to be included in the in

1004
01:05:19,200 --> 01:05:25,940
the ontology life on type compensation work or condition job seeker job offers had just

1005
01:05:25,940 --> 01:05:32,030
that everybody has compensation required for speedy name date is it OK so from this

1006
01:05:32,030 --> 01:05:36,550
question we can get the first list of the terminology two

1007
01:05:36,550 --> 01:05:38,270
to be included in the

1008
01:05:38,280 --> 01:05:40,270
in the in the ontology

1009
01:05:41,280 --> 01:05:46,850
so since we already know affairs since we already have first

1010
01:05:46,870 --> 01:05:51,700
least of the terms to be included in the ontology then this step should be

1011
01:05:51,750 --> 01:05:56,130
to go to to look for a system cover is in

1012
01:05:56,150 --> 01:05:59,760
internet i mean to know if there is some kind of i is always stand

1013
01:05:59,950 --> 01:06:05,530
or domain dependent book on very that has been previously every by by our community

1014
01:06:05,530 --> 01:06:09,090
because if they is that we will save a lot of time

1015
01:06:09,090 --> 01:06:17,030
when building our ontology so in that case using we search for the book cover

1016
01:06:17,030 --> 01:06:26,240
is related with occupation classification classification of economic activities currency classification geographic classification languages classification

1017
01:06:26,280 --> 01:06:31,560
so for on and so forth and we did some kind of assessment

1018
01:06:31,570 --> 01:06:33,640
activity in order to match

1019
01:06:33,660 --> 01:06:39,290
the terminology from the competency questions against the sound in order to know the degree

1020
01:06:39,290 --> 01:06:43,890
of coverage of the different in book of

1021
01:06:43,900 --> 01:06:45,530
so for this selection

1022
01:06:45,600 --> 01:06:47,660
which is the third stage

1023
01:06:49,000 --> 01:06:50,440
we have compared

1024
01:06:50,460 --> 01:06:54,270
as you can see in this table the different the different terms

1025
01:06:54,270 --> 01:06:58,520
well there's one thing which is sometimes sometimes this notion of expansion constant better than

1026
01:06:58,520 --> 01:06:59,600
you expect

1027
01:06:59,650 --> 01:07:03,020
so for example if you have this data set

1028
01:07:03,520 --> 01:07:08,620
it might actually be pretty small because

1029
01:07:08,690 --> 01:07:13,010
the expansion constant within these small poisson smaller

1030
01:07:13,020 --> 01:07:14,150
and this one

1031
01:07:14,390 --> 01:07:17,200
might be relatively small and then of course

1032
01:07:17,210 --> 01:07:19,220
for a larger radius is

1033
01:07:19,240 --> 01:07:23,750
if you don't you just kind of all the number of points

1034
01:07:23,770 --> 01:07:28,800
so you can kind of a lot points to be very close to some

1035
01:07:28,810 --> 01:07:32,370
in some ways and very far away from the same distance to have a small

1036
01:07:32,370 --> 01:07:35,670
expansion constant

1037
01:07:35,680 --> 01:07:39,670
an important simulation

1038
01:07:42,280 --> 01:07:46,340
when you throw in this structure the

1039
01:07:47,170 --> 01:07:51,080
so for the wall street there's been no analysis of the structure

1040
01:07:51,850 --> 01:07:53,840
it seems difficult to actually do it

1041
01:07:53,850 --> 01:07:59,760
but should mention that this is not really one of sort of a family about

1042
01:08:01,150 --> 01:08:02,920
for the moment

1043
01:08:07,480 --> 01:08:09,440
cardinal rule over them

1044
01:08:09,450 --> 01:08:14,140
certainly can you make insertion into destruction we log in time

1045
01:08:14,160 --> 01:08:19,210
you can construct by it insertion on log n time

1046
01:08:19,260 --> 01:08:22,040
he is in love again space

1047
01:08:22,050 --> 01:08:25,320
in a blog in time to remove

1048
01:08:25,340 --> 01:08:27,690
two on structure

1049
01:08:27,710 --> 01:08:34,440
in a blog in time the query this is the big one

1050
01:08:34,490 --> 01:08:36,410
so this going from

1051
01:08:36,450 --> 01:08:37,510
water and

1052
01:08:37,530 --> 01:08:40,840
water log and for some fixed c

1053
01:08:40,950 --> 01:08:44,970
yes you can actually be dependent upon the number of samples so it's not

1054
01:08:44,980 --> 01:08:49,490
so i really fixed parameter in some sense

1055
01:08:49,500 --> 01:08:51,670
so the navigating that

1056
01:08:51,690 --> 01:08:54,630
i would like to see if criminal was made this

1057
01:08:54,640 --> 01:08:55,580
in log in

1058
01:08:55,590 --> 01:08:59,020
space good in space

1059
01:08:59,040 --> 01:09:00,270
the cover tree

1060
01:09:00,280 --> 01:09:02,130
just makes good in space

1061
01:09:02,150 --> 01:09:07,250
you can see

1062
01:09:07,390 --> 01:09:12,270
we also work to enable them with exact constants were the constants here

1063
01:09:12,290 --> 01:09:15,740
well i just checked down once and failed

1064
01:09:18,550 --> 01:09:22,740
OK so

1065
01:09:22,790 --> 01:09:25,250
so maybe this is nice and since then

1066
01:09:25,270 --> 01:09:27,100
we have some

1067
01:09:27,120 --> 01:09:32,380
one of the members that both these are two c delta you this analysis

1068
01:09:32,390 --> 01:09:35,860
it says you do well constant small

1069
01:09:35,900 --> 01:09:38,610
and then you have

1070
01:09:42,660 --> 01:09:44,280
this analysis

1071
01:09:44,340 --> 01:09:47,090
which says you're going to be too bad in terms of

1072
01:09:48,420 --> 01:09:53,650
and a lot of time in this way to kind of advertisers away

1073
01:09:53,660 --> 01:09:58,990
so if you cover sort of an isolated and because it's not going to require

1074
01:09:58,990 --> 01:10:01,360
much more space than the brute force

1075
01:10:02,730 --> 01:10:04,780
and it's going to give you

1076
01:10:07,170 --> 01:10:08,130
as much

1077
01:10:08,150 --> 01:10:10,310
speedup with respect to

1078
01:10:10,330 --> 01:10:14,910
this expansion constant is this in any other algorithms

1079
01:10:14,920 --> 01:10:19,380
and it's also nice because it works pretty well in practice

1080
01:10:22,340 --> 01:10:23,570
what we did was we

1081
01:10:23,620 --> 01:10:26,320
did that's this is we compared

1082
01:10:26,850 --> 01:10:30,080
well we will never datasets

1083
01:10:30,100 --> 01:10:34,460
you can sort of bug bites right so the dimensionality now actually

1084
01:10:34,480 --> 01:10:39,340
this one for here what has very large dimensionality smaller points

1085
01:10:41,630 --> 01:10:45,150
and then what we did was we cover tree

1086
01:10:45,330 --> 01:10:48,020
from incoming optimise brute force

1087
01:10:48,120 --> 01:10:51,870
and for every point we ask for the

1088
01:10:51,880 --> 01:10:56,920
o point and said we ask for the one two three five ten years neighbours

1089
01:10:56,940 --> 01:11:00,980
so it's it's one two three four five ten

1090
01:11:01,000 --> 01:11:03,180
it's the different parts of

1091
01:11:03,730 --> 01:11:06,600
OK so

1092
01:11:09,560 --> 01:11:15,540
yes this compared is chemistry is divided by brute force optimise briffa's

1093
01:11:15,880 --> 01:11:20,560
so what you notice

1094
01:11:20,580 --> 01:11:25,220
so one thing which is nice is the performance except in

1095
01:11:25,230 --> 01:11:28,410
one very small case ten years they resort to personal data set

1096
01:11:28,420 --> 01:11:32,370
is good one

1097
01:11:32,390 --> 01:11:37,690
all these tests always datasets is that that we tested so i'm not holding anything

1098
01:11:41,560 --> 01:11:47,070
the time for cover trees also including the construction time

1099
01:11:47,640 --> 01:11:50,460
i'm trying to be as fair as possible here

1100
01:11:50,480 --> 01:11:54,530
how much of the speed of source very between

1101
01:11:54,610 --> 01:11:55,950
almost nothing

1102
01:11:55,960 --> 01:12:00,410
and you know all of about

1103
01:12:03,430 --> 01:12:04,810
the reason

1104
01:12:04,820 --> 01:12:11,130
some datasets we get such that the that's because of this expression because it

1105
01:12:11,880 --> 01:12:17,000
charlie parker with the more later but

1106
01:12:17,040 --> 01:12:23,340
what you can notice if you analyse data sets is not of expansion constant necessarily

1107
01:12:24,320 --> 01:12:25,460
but sort of the

1108
01:12:25,470 --> 01:12:27,490
average expansion constant

1109
01:12:27,520 --> 01:12:29,770
is small for the once we get a big speed

1110
01:12:29,790 --> 01:12:35,620
also he large data scenario big speed because of their independence and so these are

1111
01:12:35,630 --> 01:12:37,010
all small datasets

1112
01:12:37,100 --> 01:12:41,290
these are getting larger so this one happens to be smaller because it's four thousand

1113
01:12:44,310 --> 01:12:47,990
this is in this which is a hundred in direction recognition

1114
01:12:48,080 --> 01:12:51,740
and it's kind of cool to give much fewer you going you know

1115
01:12:51,870 --> 01:12:53,260
so in between

1116
01:12:53,270 --> 01:12:55,600
two and one

1117
01:12:55,650 --> 01:12:57,260
the average distances

1118
01:12:57,270 --> 01:12:58,600
we get the

1119
01:12:58,610 --> 01:13:00,030
now good

1120
01:13:00,050 --> 01:13:03,010
like all of the

1121
01:13:04,610 --> 01:13:05,660
what else

1122
01:13:05,670 --> 01:13:09,740
there's something is strange feature here which is that should have the spike for one

1123
01:13:09,740 --> 01:13:10,820
nearest neighbour

1124
01:13:10,870 --> 01:13:14,140
cimino one

1125
01:13:16,440 --> 01:13:18,310
it turns out that one nearest neighbour

1126
01:13:18,320 --> 01:13:20,730
as the degeneracy problem

1127
01:13:20,730 --> 01:13:24,740
online at some point right we can i think a little bit about you know

1128
01:13:25,060 --> 01:13:27,760
what sort of crazy things are people going to do with this

1129
01:13:27,810 --> 01:13:30,620
so we do have prior expectations about

1130
01:13:30,620 --> 01:13:34,550
the uses of this code that's probably the rational way of going about

1131
01:13:43,870 --> 01:13:47,300
this is

1132
01:13:47,430 --> 01:14:01,830
o lord even though like going into the work of the people

1133
01:14:01,850 --> 01:14:11,270
i mean you know there are ways of counting and they don't sound so horrible

1134
01:14:11,270 --> 01:14:14,630
that you could say i believe that the function is kind of smooth

1135
01:14:15,410 --> 01:14:19,290
and you know and

1136
01:14:19,350 --> 01:14:22,770
and you know that doesn't sound as scary as i believe that it comes from

1137
01:14:22,770 --> 01:14:25,010
a dozen process with this covariance

1138
01:14:25,060 --> 01:14:37,460
they were

1139
01:14:38,210 --> 01:14:39,860
you have

1140
01:14:44,250 --> 01:14:48,140
is there a way

1141
01:14:49,960 --> 01:14:54,020
all the way

1142
01:14:58,040 --> 01:15:00,930
obviously they have this year

1143
01:15:00,950 --> 01:15:05,520
we can see that

1144
01:15:09,810 --> 01:15:14,980
the huge are

1145
01:15:18,350 --> 01:15:21,750
i mean that's

1146
01:15:21,770 --> 01:15:25,250
but that's fine that's fine for the case study view where you're you're going to

1147
01:15:25,250 --> 01:15:28,770
peak at the data may be peak i mean i like the idea of peaking

1148
01:15:28,770 --> 01:15:30,560
at ten percent of the data saying

1149
01:15:30,560 --> 01:15:33,640
i think i see this sort of structure in here and then running on the

1150
01:15:33,640 --> 01:15:37,180
rest are something i mean people might object but

1151
01:15:37,390 --> 01:15:38,890
at least

1152
01:15:38,890 --> 01:15:42,270
but but that different from the black box view where you just put your code

1153
01:15:42,270 --> 01:15:46,310
online somebody else is running your code so i mean

1154
01:15:46,330 --> 01:15:48,720
god knows that they're going to do with it right

1155
01:15:48,730 --> 01:15:50,910
how to predict

1156
01:15:50,930 --> 01:15:56,950
you know that's the worst thing they do

1157
01:16:08,540 --> 01:16:13,700
you have already

1158
01:16:13,750 --> 01:16:16,700
and here is

1159
01:16:18,450 --> 01:16:37,250
so let's i mean this isn't unfortunately this and the only problem working limits

1160
01:16:37,270 --> 01:16:40,520
move on to other spiking neuron

1161
01:17:31,080 --> 01:17:34,600
all right

1162
01:17:48,870 --> 01:17:52,370
what you are

1163
01:17:55,520 --> 01:17:58,770
well first all

1164
01:18:10,040 --> 01:18:15,450
one of the

1165
01:18:17,700 --> 01:18:25,500
four in

1166
01:18:29,290 --> 01:18:35,810
i mean i think i think that's a valid point

1167
01:18:35,870 --> 01:18:36,730
in that

1168
01:18:36,830 --> 01:18:41,660
we we don't want to be just sort of blandly uniform over all possibilities we

1169
01:18:41,660 --> 01:18:45,600
want to learn from and i think as a community we do this we learn

1170
01:18:45,620 --> 01:18:50,580
from things that seem to work and then we develop new models

1171
01:18:50,640 --> 01:18:58,370
that have more structure maybe concentrate mass more interesting places where datasets might actually work

1172
01:18:58,930 --> 01:19:06,290
over the

1173
01:19:18,100 --> 01:19:20,000
so what you

1174
01:19:22,200 --> 01:19:23,500
this is about

1175
01:19:32,370 --> 01:19:36,910
the problem

1176
01:19:36,910 --> 01:19:39,910
have a problem

1177
01:19:39,910 --> 01:19:46,890
is the man

1178
01:19:53,000 --> 01:19:59,480
i mean the there

1179
01:19:59,500 --> 01:20:02,140
we would like to what we would like to do is to be able to

1180
01:20:02,140 --> 01:20:05,330
characterize the space of possible problems

1181
01:20:05,330 --> 01:20:07,370
so that nu

1182
01:20:07,430 --> 01:20:13,290
the method can look properties of the problem and no one kind of prior to

1183
01:20:13,290 --> 01:20:16,390
use in that case but let's just let's just move on because i think there

1184
01:20:16,390 --> 01:20:21,040
are other issues as well that that there were talking about

1185
01:20:23,060 --> 01:20:26,700
so one of them is this sort of nonparametric versus parametric

1186
01:20:31,120 --> 01:20:37,060
you know you just roughly speaking we can think of parametric models as

1187
01:20:37,120 --> 01:20:42,680
having a finite dimensional refining fixed number of parameters theta

1188
01:20:42,730 --> 01:20:43,580
and so

1189
01:20:43,600 --> 01:20:48,750
what happens is given those parameters the predictions are independent of the data

1190
01:20:48,810 --> 01:20:53,000
the parameters in the parametric model to capture

1191
01:20:53,060 --> 01:20:55,160
all the structure

1192
01:20:55,220 --> 01:21:00,250
in the data that is useful for doing predictions about new data points

1193
01:21:00,310 --> 01:21:04,330
OK we can think of this really is model based learning because you know you've

1194
01:21:04,330 --> 01:21:05,930
captured in that

1195
01:21:05,960 --> 01:21:10,290
finite dimensional parameter a model of the data

1196
01:21:10,330 --> 01:21:16,790
nonparametric models sort of roughly speaking allow the number of parameters to grow with the

1197
01:21:16,790 --> 01:21:21,120
data set size so the effective number of parameters grows with the data set size

1198
01:21:21,120 --> 01:21:24,200
or alternatively we can think of them as

1199
01:21:24,250 --> 01:21:28,870
having predictions that depend on the data

1200
01:21:28,910 --> 01:21:35,480
and possibly a usually small number of what we would probably call hyperparameters of well

1201
01:21:35,480 --> 01:21:39,580
OK so the predictions then depend on the data and these other

1202
01:21:39,640 --> 01:21:42,310
hyperparameters doesn't not like nonparametric models

1203
01:21:42,330 --> 01:21:43,310
don't have

1204
01:21:43,330 --> 01:21:44,770
any parameters in the

1205
01:21:44,770 --> 01:21:48,350
well this is of course you can do it by building a model of the

1206
01:21:48,350 --> 01:21:52,750
person in you know fitting the model to each point class but this is not

1207
01:21:52,750 --> 01:21:57,820
such an easy problem or you could you know manually sort of segmented at every

1208
01:21:57,820 --> 01:22:02,500
point again this is kind of painful fate so that an alternative solution to the

1209
01:22:02,500 --> 01:22:05,200
following its so well

1210
01:22:05,220 --> 01:22:06,870
a few more

1211
01:22:06,880 --> 01:22:08,310
what happens

1212
01:22:08,400 --> 01:22:12,800
when a person moves actually for the participants

1213
01:22:13,050 --> 01:22:14,890
this guy's

1214
01:22:14,900 --> 01:22:18,380
this our first

1215
01:22:18,390 --> 01:22:22,890
of this guy's body is about to fall into two dimensional manifold embedded in three

1216
01:22:22,890 --> 01:22:24,450
dimensional space

1217
01:22:24,450 --> 01:22:26,520
then the person moves

1218
01:22:26,570 --> 01:22:30,280
what happens well if you think about it

1219
01:22:30,310 --> 01:22:34,010
when you move your arm doesn't really stretch

1220
01:22:34,020 --> 01:22:39,530
it's actually a kind of rigid motion right your scheme so if i put markers

1221
01:22:39,580 --> 01:22:41,750
on the ski like here here

1222
01:22:41,760 --> 01:22:44,200
it doesn't matter what i do

1223
01:22:44,210 --> 01:22:49,560
the ambient space between this two

1224
01:22:49,580 --> 01:22:56,020
will of course change what like i'm would like this but the shortest geodesic distance

1225
01:22:56,020 --> 01:22:56,950
between them

1226
01:22:57,060 --> 01:22:59,190
will actually stays the same

1227
01:22:59,200 --> 01:23:04,260
o almost the same as the skin stretches a little bit of this so this

1228
01:23:04,260 --> 01:23:10,260
is an example of an isometric transformations that transformation which preserves in tornio

1229
01:23:13,020 --> 01:23:19,680
this persons on the manifold so to manifold isometrically equivalent

1230
01:23:19,710 --> 01:23:26,020
if there is a correspondence between them which preserves geodesic distances

1231
01:23:26,050 --> 01:23:31,260
so when you moved this is an isometric transformation of the surface of

1232
01:23:31,270 --> 01:23:52,180
now here's an interesting thing so let's see what

1233
01:23:52,250 --> 01:23:58,160
the new computer so this is this is a bunch of point practices data so

1234
01:23:58,160 --> 01:24:01,150
what happens when you compute

1235
01:24:01,170 --> 01:24:03,050
i can vectors

1236
01:24:03,070 --> 01:24:07,680
of the last operator of the laplacian matrix

1237
01:24:07,700 --> 01:24:10,170
it's part described in the

1238
01:24:10,190 --> 01:24:11,700
the first half of the dog

1239
01:24:12,080 --> 01:24:17,020
what happens when you computed on this day well each i can vector is a

1240
01:24:17,020 --> 01:24:18,960
function defined

1241
01:24:18,980 --> 01:24:22,580
all the surface of these guys but

1242
01:24:22,620 --> 01:24:24,860
so for each point

1243
01:24:24,920 --> 01:24:28,880
so if i take the furthest well the first thing that is constantly fighting the

1244
01:24:28,880 --> 01:24:30,650
second organ vector

1245
01:24:30,660 --> 01:24:34,970
this is going to be some function so so each point made point five you

1246
01:24:34,970 --> 01:24:37,410
may be minus point five and so on

1247
01:24:37,420 --> 01:24:41,710
now what is interesting about this is actually the third ranking function

1248
01:24:41,720 --> 01:24:45,020
if you can see the color

1249
01:24:45,050 --> 01:24:47,830
corresponds to the value of this function

1250
01:24:47,850 --> 01:24:49,700
so when the moving

1251
01:24:49,720 --> 01:24:53,070
the color actually is not changing

1252
01:24:53,080 --> 01:24:54,410
so that means

1253
01:24:54,450 --> 01:24:57,570
that these the well you here

1254
01:24:57,580 --> 01:25:04,200
this function was pointwise you know point t equals zero then five my second life

1255
01:25:04,200 --> 01:25:05,020
it's still

1256
01:25:05,040 --> 01:25:06,290
o point one

1257
01:25:06,300 --> 01:25:08,320
it didn't change at all

1258
01:25:08,330 --> 01:25:12,280
why is it useful well now

1259
01:25:12,290 --> 01:25:16,460
i actually have a function which doesn't change when a chair when they walk

1260
01:25:16,520 --> 01:25:20,080
so if i use this function of one function is not enough i want to

1261
01:25:20,080 --> 01:25:24,400
use several in general but if we use this function to to segment this guy's

1262
01:25:24,400 --> 01:25:28,670
body for example say that i'm only interested in the prox where this function is

1263
01:25:28,670 --> 01:25:30,100
bigger than point five

1264
01:25:30,120 --> 01:25:32,750
you can you can figure out how to do it

1265
01:25:33,540 --> 01:25:36,290
this segmentation will actually

1266
01:25:36,310 --> 01:25:40,810
go in time and state exactly the same and i will and feel exactly the

1267
01:25:40,810 --> 01:25:44,130
same in practice obviously not exactly the same but

1268
01:25:44,580 --> 01:25:48,930
but on the the set and the nice thing about it you don't actually you

1269
01:25:48,930 --> 01:25:52,420
just need to know this point clouds you don't at each time separately you don't

1270
01:25:52,420 --> 01:25:55,980
need to know anything about that so computed at one time

1271
01:25:56,040 --> 01:25:58,570
and you can see them throughout

1272
01:25:58,580 --> 01:26:00,710
the whole

1273
01:26:00,730 --> 01:26:05,070
time process that the whole time time scale

1274
01:26:05,080 --> 01:26:06,480
so that's

1275
01:26:06,490 --> 01:26:12,360
that that was kind of a nice application of this manifold i

1276
01:26:17,790 --> 01:26:22,690
this the what of neat examples i think you know what i'm going to do

1277
01:26:22,690 --> 01:26:25,820
i'm actually going to talk about

1278
01:26:26,080 --> 01:26:30,710
on how the heat equation

1279
01:26:31,590 --> 01:26:33,950
i think there were some more

1280
01:26:33,990 --> 01:26:42,080
this is closely related to graphics actually

1281
01:26:42,100 --> 01:26:46,040
let me tell you a little bit about the heat equation and the connection to

1282
01:26:46,040 --> 01:26:51,130
the because in a sense heat equation is the key object as i say

1283
01:26:51,340 --> 01:26:55,950
the laplace operator so the class separated in the heat equation that kind of go

1284
01:27:08,970 --> 01:27:12,430
keep question

1285
01:27:12,440 --> 01:27:14,320
the heat equation

1286
01:27:14,380 --> 01:27:17,020
it's basically

1287
01:27:17,060 --> 01:27:18,960
the following so

1288
01:27:19,020 --> 01:27:21,980
so let us form

1289
01:27:22,000 --> 01:27:27,460
let's for simplicity can be sourced considered an hour and a we actually interested in

1290
01:27:27,460 --> 01:27:33,830
the heat equation on the manifold but still get short so what happens in our

1291
01:27:33,850 --> 01:27:35,000
in our and

1292
01:27:35,030 --> 01:27:37,410
suppose you have some sort of

1293
01:27:37,450 --> 01:27:41,220
distribution in time zero u of x zero

1294
01:27:41,230 --> 01:27:43,470
so you have some key distribution

1295
01:27:46,530 --> 01:27:49,450
how does he evolved

1296
01:27:49,460 --> 01:27:51,820
so he actually evolve

1297
01:27:52,030 --> 01:27:55,070
according to to the what's called the heat equation

1298
01:27:55,230 --> 01:27:59,290
in the heat equation says that if you have a few of ecstasy

1299
01:27:59,300 --> 01:28:01,580
that's the amount of heat

1300
01:28:01,580 --> 01:28:04,400
at the point x at

1301
01:28:04,450 --> 01:28:06,080
the time t

1302
01:28:06,100 --> 01:28:09,050
then the lab class you fxt

1303
01:28:09,070 --> 01:28:14,650
is equal to the time derivative of you that and the heat equation

1304
01:28:14,690 --> 01:28:18,230
now how do you solve something like that well we have a good intuition about

1305
01:28:18,230 --> 01:28:23,810
how he goes well diffuses right it becomes kind of spread around

1306
01:28:23,880 --> 01:28:29,820
what does this intuition formerly well formerly in our and we actually can write the

1307
01:28:29,820 --> 01:28:32,520
solution of the heat equation

1308
01:28:32,610 --> 01:28:34,380
this integral

1309
01:28:34,470 --> 01:28:40,450
basically you FXT is the convolution of the he just of f of y which

1310
01:28:40,450 --> 01:28:42,530
is a new show he distribution

1311
01:28:42,530 --> 01:28:45,380
the shadows on the wall

1312
01:28:45,420 --> 01:28:46,590
in my hand

1313
01:28:46,630 --> 01:28:47,750
he was playing

1314
01:28:47,770 --> 01:28:50,610
and it is far from infinitely large

1315
01:28:50,610 --> 01:28:52,190
if i were this

1316
01:28:52,230 --> 01:28:55,130
so far away from its four centimetres

1317
01:28:55,190 --> 01:28:59,800
very close approximation will be infinitely large but if i'm here and there and that's

1318
01:28:59,800 --> 01:29:02,840
where i will be of course it is not

1319
01:29:02,860 --> 01:29:04,980
infinitely large anymore

1320
01:29:05,030 --> 01:29:07,520
so let's start then the graph

1321
01:29:07,520 --> 01:29:10,360
you can see but i turned it on

1322
01:29:10,380 --> 01:29:14,730
it's rotating now i have to put charts on the output should

1323
01:29:14,800 --> 01:29:17,770
was of and the graph

1324
01:29:18,670 --> 01:29:22,020
this is no charge has the same charges the plane

1325
01:29:22,090 --> 01:29:23,800
the plane is being charged

1326
01:29:23,860 --> 01:29:25,210
and here you see

1327
01:29:25,270 --> 01:29:28,380
the angle

1328
01:29:28,460 --> 01:29:30,920
try to remember that angle

1329
01:29:30,960 --> 01:29:34,320
hard to estimate the fifteen degrees you see the vertical

1330
01:29:34,380 --> 01:29:36,050
and if no i it's about

1331
01:29:36,230 --> 01:29:39,380
thirty centimetres away from the plane

1332
01:29:39,380 --> 01:29:41,860
and i go back

1333
01:29:41,920 --> 01:29:44,920
two fifty centimetres which is where i am now

1334
01:29:44,940 --> 01:29:47,440
c hasn't changed very much

1335
01:29:47,630 --> 01:29:50,050
go further out to sixty centimeters

1336
01:29:50,090 --> 01:29:52,770
the angle goes down a little of course it does

1337
01:29:52,790 --> 01:29:54,270
but not very much

1338
01:29:54,320 --> 01:29:57,690
and if i go far away all the way to mass avenue

1339
01:29:57,730 --> 01:29:59,070
of course

1340
01:29:59,110 --> 01:30:02,860
the force on this little object inversely are square because then

1341
01:30:02,880 --> 01:30:06,590
the whole plane would behave like points or

1342
01:30:06,610 --> 01:30:09,650
so i've shown you that very close to this plane

1343
01:30:10,790 --> 01:30:12,020
electric field

1344
01:30:12,050 --> 01:30:15,590
stays approximately constant

1345
01:30:15,670 --> 01:30:19,420
so if now we remove this

1346
01:30:19,440 --> 01:30:22,300
march because if you can you will have to

1347
01:30:22,550 --> 01:30:24,290
is also off

1348
01:30:24,360 --> 01:30:26,000
thank you very much

1349
01:30:26,020 --> 01:30:28,300
now we have the van der graaf along

1350
01:30:28,320 --> 01:30:31,050
so now we don't have the electric field falls off

1351
01:30:31,070 --> 01:30:34,090
as one of our square it's a very good approximation now

1352
01:30:34,090 --> 01:30:37,610
we can think of the charges being right at the center

1353
01:30:37,610 --> 01:30:42,670
i'll give it a little bit of charter and charts OK

1354
01:30:42,710 --> 01:30:45,110
so look at the projection

1355
01:30:48,190 --> 01:30:50,070
the balloon is now

1356
01:30:50,130 --> 01:30:53,550
or maybe thirty centimetres away from the center maybe forty

1357
01:30:53,590 --> 01:30:57,210
forty and is almost forty five degrees

1358
01:30:57,250 --> 01:31:00,340
now i go i double the distance i go to about

1359
01:31:00,340 --> 01:31:02,020
ninety centimeters

1360
01:31:02,110 --> 01:31:04,520
look at that angle theta

1361
01:31:04,570 --> 01:31:09,420
the angle theta is now down to o maybe ten degrees i will go back

1362
01:31:09,440 --> 01:31:10,480
where i was

1363
01:31:10,500 --> 01:31:13,570
this angle is about forty degrees

1364
01:31:13,670 --> 01:31:15,340
and now

1365
01:31:15,400 --> 01:31:16,440
very small

1366
01:31:16,460 --> 01:31:18,110
and when i go here

1367
01:31:18,150 --> 01:31:19,980
which is about

1368
01:31:20,020 --> 01:31:21,820
media and half

1369
01:31:21,880 --> 01:31:23,460
you can hardly see

1370
01:31:23,520 --> 01:31:24,800
there's any angle

1371
01:31:24,800 --> 01:31:26,340
only a few degrees

1372
01:31:26,360 --> 01:31:28,750
so i've shown you only qualitatively

1373
01:31:28,800 --> 01:31:33,610
that's the electric field falls off very rapidly

1374
01:31:33,650 --> 01:31:35,820
in the vicinity of a

1375
01:31:36,590 --> 01:31:39,190
uniformly charged sphere

1376
01:31:39,210 --> 01:31:41,520
and that it doesn't fall off

1377
01:31:41,530 --> 01:31:44,250
very fast if you are in the near vicinity

1378
01:31:44,290 --> 01:31:46,960
of a

1379
01:31:49,960 --> 01:31:52,070
the second thing i want to show you

1380
01:31:52,070 --> 01:31:54,500
has to do with the fact that the electric field

1381
01:31:56,020 --> 01:31:59,090
a uniformly charged sphere

1382
01:32:01,520 --> 01:32:04,050
here i have a sphere

1383
01:32:04,110 --> 01:32:06,320
which is not entirely closed

1384
01:32:06,380 --> 01:32:09,800
i can make it closed because i want to demonstrate to you that there is

1385
01:32:09,800 --> 01:32:13,610
no electric field inside when i charge is uniformly

1386
01:32:13,670 --> 01:32:17,050
and since i have to get inside an opening

1387
01:32:17,170 --> 01:32:20,840
nothing i can do about it since there is an opening

1388
01:32:20,900 --> 01:32:24,340
the electric field is not exactly zero inside

1389
01:32:24,380 --> 01:32:25,860
it's only true

1390
01:32:25,880 --> 01:32:30,290
if this is a complete closed surface and if the charge is uniformly distributed

1391
01:32:30,340 --> 01:32:35,420
but it's good approximation the opening is quite small

1392
01:32:35,480 --> 01:32:37,210
and what i'm going to do

1393
01:32:37,250 --> 01:32:39,300
is i'm going to try to this

1394
01:32:39,320 --> 01:32:41,840
the sphere and put in charge outside

1395
01:32:41,880 --> 01:32:43,130
user device

1396
01:32:43,130 --> 01:32:44,920
that we have not used before

1397
01:32:44,960 --> 01:32:46,190
but that's not so

1398
01:32:47,440 --> 01:32:49,300
he was now that

1399
01:32:49,340 --> 01:32:51,090
hollow sphere

1400
01:32:51,190 --> 01:32:53,250
going to put charge on their

1401
01:32:53,270 --> 01:32:55,920
let's suppose it is positive charge

1402
01:32:55,940 --> 01:32:58,420
so this will be positively charged

1403
01:32:58,480 --> 01:32:59,690
since it is is

1404
01:32:59,730 --> 01:33:02,150
the conductor as we will learn

1405
01:33:02,210 --> 01:33:08,320
i think the next lecture or things this week that the charge automatically distributes uniformly

1406
01:33:08,360 --> 01:33:10,520
only does that on the conductor

1407
01:33:10,520 --> 01:33:11,460
and now

1408
01:33:11,480 --> 01:33:15,400
to demonstrate to you that there is an electric field here

1409
01:33:15,440 --> 01:33:16,300
i will

1410
01:33:16,320 --> 01:33:19,360
i use induction

1411
01:33:19,420 --> 01:33:21,150
i have two

1412
01:33:21,190 --> 01:33:22,520
ping pong balls

1413
01:33:22,530 --> 01:33:25,480
i think it was conducting paint

1414
01:33:25,520 --> 01:33:28,070
they touch each other

1415
01:33:28,110 --> 01:33:30,440
on the influence of the electric field

1416
01:33:30,500 --> 01:33:34,530
this one will become negative and this one will become positive we've discussed at last

1417
01:33:34,530 --> 01:33:36,340
time you created dipole

1418
01:33:36,360 --> 01:33:38,500
the important it is the dipole

1419
01:33:38,550 --> 01:33:40,840
i separated two

1420
01:33:40,860 --> 01:33:44,110
i've negative charge the positive charge there

1421
01:33:44,130 --> 01:33:46,480
i'll touch

1422
01:33:46,500 --> 01:33:49,210
any one of these two balls it doesn't matter which one

1423
01:33:49,230 --> 01:33:51,610
with the electroscope and you will see

1424
01:33:51,710 --> 01:33:53,320
the discharge there

1425
01:33:53,380 --> 01:33:57,340
so i've demonstrated then the is an electric field outside

1426
01:33:57,360 --> 01:33:58,670
that sphere

1427
01:33:58,710 --> 01:34:01,650
now i will do exactly the same demonstration

1428
01:34:01,670 --> 01:34:02,380
but now

1429
01:34:02,380 --> 01:34:05,690
i put these two

1430
01:34:05,730 --> 01:34:08,300
conducting both inside

1431
01:34:08,320 --> 01:34:10,610
here they are

1432
01:34:10,630 --> 01:34:14,590
i parts that you just have to trust me that i really will touch them

1433
01:34:14,630 --> 01:34:16,790
and then i will take them out

1434
01:34:16,790 --> 01:34:20,440
are really satisfied with the actual performance that you're reaching

1435
01:34:20,440 --> 01:34:23,350
random search and try to sort of refine that a bit more

1436
01:34:23,650 --> 01:34:27,660
but that's that's a pretty solid baseline yeah

1437
01:34:29,470 --> 01:34:34,370
yes yeah that's another approach

1438
01:34:37,740 --> 01:34:41,570
works really well lab like me but yeah there's like hundreds of

1439
01:34:41,580 --> 01:34:44,620
students yeah less another yes

1440
01:34:56,050 --> 01:34:59,380
ok so actually that's a very good question any kind of ties and

1441
01:34:59,380 --> 01:35:02,780
so the question is about which should we use one regularization

1442
01:35:02,790 --> 01:35:07,010
or l to so i'm quickly start talk about

1443
01:35:07,210 --> 01:35:10,490
the problem of selecting the number of iterations for training

1444
01:35:10,490 --> 01:35:14,990
and building that l to one so there's this other hyperparameters

1445
01:35:14,990 --> 01:35:17,560
that barely mention which is the number of updates i'm going to

1446
01:35:17,560 --> 01:35:20,920
do typically we sort of think of it as the number of times doing

1447
01:35:20,920 --> 01:35:26,770
over the whole training set and one approach to be used specifically

1448
01:35:26,780 --> 01:35:30,940
for that hyperparameter is early stopping so what we do is that

1449
01:35:30,950 --> 01:35:35,440
we track at after every parts after every full pass over the how

1450
01:35:35,440 --> 01:35:39,190
are the training set going to measure the performance on the validation

1451
01:35:39,190 --> 01:35:41,990
set yeah just learning curve here i'm showing the training curve

1452
01:35:41,990 --> 01:35:45,780
as well as validation set after every epoch i'm reporting

1453
01:35:45,960 --> 01:35:47,860
what's the training sets performance

1454
01:35:47,860 --> 01:35:50,510
so as so the interesting for selecting hyperparameters but it's

1455
01:35:50,510 --> 01:35:54,330
a good metric to look at to debug and make sure you models training

1456
01:35:54,850 --> 01:35:59,560
but more crucially i'm going to measure the performance on the validation

1457
01:35:59,560 --> 01:36:02,840
set and i'm going to do that you know train another full epoch

1458
01:36:02,930 --> 01:36:05,900
and this again training a validation set performance

1459
01:36:06,530 --> 01:36:11,980
and what you expect this very basic you know

1460
01:36:12,340 --> 01:36:15,380
machine learning is that eventually the your underfitting

1461
01:36:15,380 --> 01:36:17,120
and there's a point in which the training

1462
01:36:17,120 --> 01:36:20,400
set performance starts continues getting better but actually

1463
01:36:20,400 --> 01:36:23,570
the performance on some new examples starts getting worse so

1464
01:36:23,570 --> 01:36:26,000
then you start entering the overfitting regime

1465
01:36:26,170 --> 01:36:29,500
the essential dynamic that's says going on here is that the gap

1466
01:36:29,510 --> 01:36:33,480
between these two curves we expected to becomes bigger and bigger

1467
01:36:33,490 --> 01:36:37,520
and bigger as you go and because this is you know usually

1468
01:36:37,890 --> 01:36:41,220
bounded by below needs to converge to some some i thought value

1469
01:36:41,220 --> 01:36:43,500
that it means eventually this curve goes up

1470
01:36:43,880 --> 01:36:46,740
so with thirty stopping which do you essentially track

1471
01:36:46,800 --> 01:36:50,640
the performance validation set and then once it starts going up

1472
01:36:50,640 --> 01:36:52,910
well you pretty much know you should stop training

1473
01:36:52,910 --> 01:36:56,450
so effectively you're not going to train to completion on your training

1474
01:36:56,450 --> 01:37:00,640
objective going to train until you measure some form overfitting

1475
01:37:00,650 --> 01:37:03,780
over the validation set in practice

1476
01:37:04,100 --> 01:37:07,540
the performance of validation set might be a bit noisy so usually

1477
01:37:07,540 --> 01:37:10,920
we leave some what i call look ahead that as you even though you

1478
01:37:10,920 --> 01:37:13,620
have an improved which affected the best performance it's

1479
01:37:13,620 --> 01:37:15,790
on the validation set so far for your run

1480
01:37:15,910 --> 01:37:18,610
you still keep going maybe ten iterations something

1481
01:37:18,870 --> 01:37:22,340
and then if after ten iterations that's a look ahead

1482
01:37:22,900 --> 01:37:27,880
number if it if you add an improver the best validation set

1483
01:37:27,880 --> 01:37:30,820
performance then you stop training go back maybe to your

1484
01:37:30,820 --> 01:37:37,080
best model and the reason i yeah regulation like one l two is

1485
01:37:37,090 --> 01:37:41,220
that effectively would this means is and you can show this like

1486
01:37:41,230 --> 01:37:45,310
more analytically and some work has been done and i are measuring

1487
01:37:45,320 --> 01:37:50,160
that or clarifying this link is because you start to weights usually

1488
01:37:50,170 --> 01:37:53,470
around zero effectively this is making its

1489
01:37:53,920 --> 01:37:57,250
this is retracting the model from going to far from its initial

1490
01:37:57,250 --> 01:38:00,320
as initialization so far from zero much like

1491
01:38:00,400 --> 01:38:03,890
l to regularization is doing much like one regularization is doing

1492
01:38:04,120 --> 01:38:08,260
so early stopping is actually good alternative to

1493
01:38:08,480 --> 01:38:11,650
regulation like one l to frankly in practice

1494
01:38:11,800 --> 01:38:16,200
i've not seen very frequently that l one oil to regularization

1495
01:38:16,200 --> 01:38:21,250
then it has to satisfy if you have a really basic properties which is called

1496
01:38:21,250 --> 01:38:23,260
reasonable right firstly

1497
01:38:23,260 --> 01:38:27,140
the length of empty subset is of course going to be zero

1498
01:38:27,140 --> 01:38:30,530
and if we have disjoint subsets

1499
01:38:30,550 --> 01:38:34,140
then the length of the union is going to be the sum of the lengths

1500
01:38:34,140 --> 01:38:35,910
of the individual subsets

1501
01:38:35,950 --> 01:38:37,240
this carries the multi

1502
01:38:37,240 --> 01:38:40,470
me and

1503
01:38:40,490 --> 01:38:43,300
a probability measure is simply one where

1504
01:38:43,320 --> 01:38:44,780
total length

1505
01:38:44,820 --> 01:38:46,760
of the whole space is going to be one

1506
01:38:46,800 --> 01:38:50,410
so the length now is going to be the probability of an event a subset

1507
01:38:50,510 --> 01:38:51,380
an event

1508
01:38:54,550 --> 01:38:56,200
the probability of

1509
01:38:56,220 --> 01:38:57,640
over the whole

1510
01:38:57,780 --> 01:39:01,780
event space is going to be one has has to indicate one is the probability

1511
01:39:03,990 --> 01:39:09,740
and it turns out that we can't really from most spaces that were interested in

1512
01:39:09,740 --> 01:39:15,550
for example the real line we can't actually define and measure over all possible subsets

1513
01:39:15,550 --> 01:39:16,640
of the real line

1514
01:39:16,680 --> 01:39:18,890
so we have to restrict ourselves

1515
01:39:18,930 --> 01:39:23,720
ourselves to count family of subsets that

1516
01:39:23,780 --> 01:39:25,160
terms measurable

1517
01:39:25,240 --> 01:39:31,930
and so that's what scholars sigma algebra so sigma algebra simply a family of subsets

1518
01:39:31,950 --> 01:39:33,410
of the set

1519
01:39:33,490 --> 01:39:36,200
OK theta such that

1520
01:39:36,220 --> 01:39:37,570
of course it cannot be empty we

1521
01:39:38,070 --> 01:39:41,090
there has to be some things for which you can measure

1522
01:39:41,110 --> 01:39:47,660
and if a subset is in our sigma algebra then it's complements has to be

1523
01:39:47,660 --> 01:39:49,280
in it as well

1524
01:39:49,300 --> 01:39:51,610
and then finally

1525
01:39:52,360 --> 01:39:54,740
we have a sequence of subsets

1526
01:39:54,780 --> 01:39:57,590
then the union is also going to be our

1527
01:39:57,590 --> 01:40:00,780
is also going to be measurable

1528
01:40:01,700 --> 01:40:05,720
the fact that this is sequences come important so you can't have

1529
01:40:05,820 --> 01:40:11,180
and countable union of things to any countable union of measurable sets is also measurable

1530
01:40:16,070 --> 01:40:17,860
and then finally

1531
01:40:17,910 --> 01:40:23,880
well it's not really about what's meant for a measurable function it in fact

1532
01:40:24,880 --> 01:40:29,530
say is that if you have two measurable spaces theta and delta

1533
01:40:29,550 --> 01:40:36,220
then a function is measurable if its inverse image is measurable for any measurable

1534
01:40:36,240 --> 01:40:42,550
subset a of the second of the of the output space

1535
01:40:45,880 --> 01:40:47,450
now let's us

1536
01:40:47,470 --> 01:40:50,890
history ourselves to probability measures and

1537
01:40:51,040 --> 01:40:54,360
think about what is meant by a random variable

1538
01:40:54,410 --> 01:40:58,140
and so that a random variable is actually not random at all

1539
01:40:58,180 --> 01:40:59,300
there is simply

1540
01:40:59,300 --> 01:41:01,260
a measurable function

1541
01:41:01,280 --> 01:41:03,180
OK so

1542
01:41:03,200 --> 01:41:06,660
the way you can think of this as the is is is as follows so

1543
01:41:07,430 --> 01:41:10,780
so how would you implement

1544
01:41:10,800 --> 01:41:17,110
a function in matlab c on c that in that just that generates say random

1545
01:41:17,110 --> 01:41:18,110
draws from

1546
01:41:18,450 --> 01:41:20,340
from a gamma distribution

1547
01:41:21,570 --> 01:41:24,490
so the way you would do is you would actually write down this function which

1548
01:41:25,240 --> 01:41:28,220
takes a sequence of

1549
01:41:28,240 --> 01:41:33,780
takes actually what we can measure how do you know what i wanted to

1550
01:41:34,050 --> 01:41:36,570
so his and his

1551
01:41:36,620 --> 01:41:39,950
and easier one so how would you generate from the galaxy and with the mean

1552
01:41:40,010 --> 01:41:42,640
of ten

1553
01:41:46,160 --> 01:41:48,760
tell me how you how would you implement civic

1554
01:41:52,620 --> 01:41:54,720
as of

1555
01:41:54,760 --> 01:42:00,970
OK yes so that's why so we assume that we have some random number generator

1556
01:42:00,970 --> 01:42:05,700
so in this case gulf a random number generator that generates zero mean gaussians and

1557
01:42:05,700 --> 01:42:08,910
then we simply add ten to it so if you think of the function itself

1558
01:42:08,910 --> 01:42:14,050
that you've just written is a deterministic function is or is not the this is

1559
01:42:14,140 --> 01:42:20,180
the function itself is fixed and it takes a random bits and they returns to

1560
01:42:20,180 --> 01:42:23,880
use random numbers from the distribution that you're interested in

1561
01:42:23,930 --> 01:42:27,660
and this is called the same way that you can think of a random variable

1562
01:42:27,680 --> 01:42:30,700
so the probability measure here

1563
01:42:30,740 --> 01:42:36,640
on this basis they turn common sigma sigma here's asking signature is going to be

1564
01:42:36,640 --> 01:42:38,160
our random number

1565
01:42:39,260 --> 01:42:43,180
and all random variable x is going to be a function which takes a random

1566
01:42:43,180 --> 01:42:50,050
number generator and spits out random variables random samples of this variable

1567
01:42:54,030 --> 01:42:57,640
and then finally a stochastic process is basically a collection of

1568
01:42:57,680 --> 01:43:02,860
and the variables so cx i one for each eye and the

1569
01:43:02,890 --> 01:43:04,880
the distinguishing property of

1570
01:43:04,910 --> 01:43:09,550
the stochastic process from say a graphical model is that

1571
01:43:09,590 --> 01:43:11,340
this index set

1572
01:43:12,320 --> 01:43:15,780
can be infinite and in fact can be uncountably infinite as we saw in the

1573
01:43:15,780 --> 01:43:18,010
case of the gaussian process

1574
01:43:18,800 --> 01:43:21,320
this turns out to be a tricky issues so

1575
01:43:22,700 --> 01:43:27,930
this is the how do even define this uncountably infinite number of random variables

1576
01:43:27,930 --> 01:43:31,590
how can even show that they exist and this is where

1577
01:43:31,590 --> 01:43:34,920
and there is a lot of which is what we try to deal with

1578
01:43:34,940 --> 01:43:38,080
we don't have to go into the system which is independent in fact if you

1579
01:43:38,080 --> 01:43:40,400
prefer to have

1580
01:43:40,680 --> 01:43:46,190
the citation should between the input and the output is that you is large enough

1581
01:43:46,210 --> 01:43:48,840
we just have that which which means that

1582
01:43:48,850 --> 01:43:55,340
again this then is only even by these external components out of the amplifiers which

1583
01:43:55,340 --> 01:43:59,460
means that the phi doesn't have to be perfect justice to have never again so

1584
01:43:59,470 --> 01:44:05,240
that this approximation again and it's much much simpler for instance to make

1585
01:44:05,280 --> 01:44:06,380
three sons

1586
01:44:06,520 --> 01:44:09,250
there exist also in very good for you

1587
01:44:09,260 --> 01:44:10,390
so the best

1588
01:44:10,400 --> 01:44:13,010
if a user is an amplifier which has

1589
01:44:13,020 --> 01:44:14,380
two inputs

1590
01:44:14,380 --> 01:44:17,170
and which of the game which is very large

1591
01:44:17,190 --> 01:44:21,420
which could be done which is very eyes which means that there is no current

1592
01:44:21,420 --> 01:44:25,040
flowing inside the amplifier

1593
01:44:25,050 --> 01:44:27,220
and again it's itself

1594
01:44:27,240 --> 01:44:30,520
so as i said it's high so you see a probably DB so it'd be

1595
01:44:30,530 --> 01:44:34,480
twenty log on the actual to put

1596
01:44:34,500 --> 01:44:37,750
one hundred and at the bottom

1597
01:44:37,760 --> 01:44:41,280
looks not to be so high that it in fact i was we start from

1598
01:44:41,290 --> 01:44:43,190
the idea and it's OK

1599
01:44:43,260 --> 01:44:47,110
and then how it works you just because it's on fire

1600
01:44:47,120 --> 01:44:51,440
you put a feedback network and it's so here i just want to is also

1601
01:44:51,440 --> 01:44:55,080
which means that i really enjoy this point

1602
01:44:55,090 --> 01:44:57,940
the fraction of the which

1603
01:44:58,000 --> 01:45:03,710
i would vote that and i will get an output voltage and then

1604
01:45:03,840 --> 01:45:07,260
if we make it a calculation the output voltage

1605
01:45:07,280 --> 01:45:11,360
you can start with the input voltage value for all this way

1606
01:45:11,370 --> 01:45:14,140
so the input voltage is just

1607
01:45:14,150 --> 01:45:15,850
this it's you

1608
01:45:15,870 --> 01:45:18,720
nine used up from the outside but i

1609
01:45:18,770 --> 01:45:23,520
the whole on the story also which is john for

1610
01:45:23,540 --> 01:45:26,000
the output voltage is equal to the

1611
01:45:26,010 --> 01:45:28,120
to this value minus

1612
01:45:28,150 --> 01:45:31,710
a similar but also if you fall of past

1613
01:45:31,730 --> 01:45:35,970
it's my it's a one plus two ten i

1614
01:45:35,980 --> 01:45:40,260
and then if you make the relationship between the two you find this relationship

1615
01:45:40,260 --> 01:45:45,540
you could use the feedback equation so in in the feedback equation with

1616
01:45:45,550 --> 01:45:48,230
you is the new

1617
01:45:48,250 --> 01:45:50,330
the values the function that

1618
01:45:51,730 --> 01:45:53,050
the input

1619
01:45:53,080 --> 01:45:55,620
this to just one

1620
01:45:55,630 --> 01:45:57,990
and then you have to find the

1621
01:45:58,010 --> 01:46:02,640
the same equation and what is the issue of the canadian

1622
01:46:02,650 --> 01:46:07,260
an ideal form new got to get which is infinity

1623
01:46:07,300 --> 01:46:11,010
and the washoe between the input and the output just

1624
01:46:11,020 --> 01:46:14,710
a short list of

1625
01:46:14,720 --> 01:46:16,880
so let's going to

1626
01:46:16,940 --> 01:46:19,110
the article itself

1627
01:46:19,210 --> 01:46:23,540
and coming back to some of the requirements of the

1628
01:46:23,550 --> 01:46:28,840
o which is in doing some child was very small signals and the order of

1629
01:46:28,840 --> 01:46:33,260
ten to cool summers and very cold so we don't tradition

1630
01:46:33,290 --> 01:46:34,460
she there

1631
01:46:34,460 --> 01:46:39,280
and then we have to make in this measurement of amplitude of time

1632
01:46:39,300 --> 01:46:43,170
and what is is more and more often in the in the detector now what

1633
01:46:43,170 --> 01:46:48,840
is is that element of signal so if you take big scientific dominated experiment it's

1634
01:46:48,840 --> 01:46:51,730
close to a hundred million spherical nature

1635
01:46:51,760 --> 01:46:55,470
state sequence to victories in several

1636
01:46:55,490 --> 01:46:57,130
it's between six

1637
01:46:57,140 --> 01:47:02,600
twenty million channel and holds that the genome it would be said well all the

1638
01:47:02,610 --> 01:47:04,680
cells the so

1639
01:47:04,690 --> 01:47:08,620
so when you have to do the given that you have to

1640
01:47:08,620 --> 01:47:12,210
to get lot of what are the public so

1641
01:47:12,240 --> 01:47:14,230
first of all we do you

1642
01:47:14,240 --> 01:47:17,340
however in the spatial environments such that you have to

1643
01:47:17,350 --> 01:47:21,630
to for instance if you are subject to additions and you have to be careful

1644
01:47:21,740 --> 01:47:23,160
very well

1645
01:47:23,180 --> 01:47:27,440
you you know it which is accessible on if it's not accessible to us to

1646
01:47:27,440 --> 01:47:28,870
be heavily

1647
01:47:30,390 --> 01:47:35,300
if the system is embedded inside the detector you want to have low if the

1648
01:47:35,300 --> 01:47:38,460
signal itself is very when you have to have a low noise electronics

1649
01:47:38,470 --> 01:47:41,180
if you put as much noise as in

1650
01:47:41,240 --> 01:47:44,370
it could be that you want to be fast all that you want to have

1651
01:47:44,390 --> 01:47:47,750
a lot of strange so for instance if you are not going to try to

1652
01:47:47,770 --> 01:47:50,020
understand the dynamic ranges

1653
01:47:50,030 --> 01:47:52,730
is one of the main issue of

1654
01:47:52,790 --> 01:47:54,850
together with no

1655
01:47:55,220 --> 01:47:58,540
if you want to talk to you want to have i also which means that

1656
01:47:58,540 --> 01:48:01,440
you have to to minimize ever

1657
01:48:01,470 --> 01:48:04,780
and of course you have to be low cost

1658
01:48:04,780 --> 01:48:06,800
so that they

1659
01:48:06,890 --> 01:48:09,960
you've got to be the cause and effect of the

1660
01:48:09,980 --> 01:48:14,310
a large variety of then it could be a small it's unlikely that it could

1661
01:48:14,310 --> 01:48:20,530
be a big big submodular set CMS pixel module it could be a full-time player

1662
01:48:20,540 --> 01:48:24,400
one of the football player of all time as you wrote about this experience

1663
01:48:24,410 --> 01:48:25,700
just before

1664
01:48:25,710 --> 01:48:27,530
so it could be

1665
01:48:27,540 --> 01:48:29,270
this big stuff which is

1666
01:48:29,280 --> 01:48:31,860
you could have required to get

1667
01:48:31,880 --> 01:48:35,640
so this would fill the

1668
01:48:35,700 --> 01:48:39,910
should almost the same always which is that you have got

1669
01:48:42,040 --> 01:48:43,460
the detector

1670
01:48:43,610 --> 01:48:50,070
and then we get some children that when a particular courses system so that you

1671
01:48:50,140 --> 01:48:52,810
that's the kind of course on the detector

1672
01:48:52,830 --> 01:48:55,730
it goes on point one people five

1673
01:48:55,750 --> 01:48:58,370
it said that all of the same size

1674
01:48:58,380 --> 01:48:59,460
fifty by

1675
01:48:59,480 --> 01:49:04,830
five formerly michael something like that so you have a very small capacitance i

1676
01:49:04,830 --> 01:49:07,610
two then people thought when you have

1677
01:49:07,630 --> 01:49:09,590
for subsequent questions

1678
01:49:09,610 --> 01:49:16,600
photomultiplier is depicted in the in the industry thirty people from organisations independent the john

1679
01:49:16,750 --> 01:49:20,360
but you can go from ten people thought that alpha so

1680
01:49:20,470 --> 01:49:29,390
that's the characteristic and it's important to know that the true density according to the

1681
01:49:30,190 --> 01:49:36,940
the altitude about one for year course on with michael to fully to to good

1682
01:49:36,950 --> 01:49:42,470
photomultiplier tubes which is wonderful to behold really get

1683
01:49:42,480 --> 01:49:43,930
and to the five to ten

1684
01:49:43,940 --> 01:49:46,990
seven hundred taught at the output

1685
01:49:47,000 --> 01:49:50,100
in shambles while so few thousand

1686
01:49:50,150 --> 01:49:51,060
you know

1687
01:49:51,080 --> 01:49:53,400
we can always move

1688
01:49:54,460 --> 01:49:56,230
signal by

1689
01:49:56,250 --> 01:49:57,990
by and process

1690
01:49:58,000 --> 01:50:01,780
source which is just a short play but are function

1691
01:50:01,820 --> 01:50:06,680
so i don't see anything on you don't see anything about i by as connectionist

1692
01:50:06,680 --> 01:50:13,050
eric thing in to not a they proposed the problem of of solving this as

1693
01:50:13,050 --> 01:50:16,690
an undirected and under sparse crowd

1694
01:50:16,700 --> 01:50:18,370
so you can actually post this

1695
01:50:19,320 --> 01:50:20,780
a graph property

1696
01:50:20,820 --> 01:50:23,460
so an undirected graph from and

1697
01:50:23,490 --> 01:50:27,560
the big problem but was that they and the big problem with this approach was

1698
01:50:27,560 --> 01:50:29,870
that if you view this PDF

1699
01:50:29,870 --> 01:50:31,060
as a graph

1700
01:50:31,170 --> 01:50:33,630
every node was interconnected with every other

1701
01:50:34,540 --> 01:50:36,800
there was no sparsity in the graph whatsoever

1702
01:50:36,810 --> 01:50:41,370
and so if anyone knows anything about the belief propagation lifted belief propagation behaves very

1703
01:50:41,370 --> 01:50:43,920
poor poorly we have a dense graph

1704
01:50:43,940 --> 01:50:50,070
so i propose technique to sparsify the graphs such as a kind of applied belief

1705
01:50:50,070 --> 01:50:51,970
propagation properly and

1706
01:50:52,010 --> 01:50:56,040
it works all right it's very slow because you've got another thing with loopy belief

1707
01:50:56,040 --> 01:50:59,810
propagation we don't really have any guarantees convergence

1708
01:51:00,760 --> 01:51:03,720
but all very interesting all very new

1709
01:51:03,880 --> 01:51:07,230
things so

1710
01:51:07,250 --> 01:51:12,490
the most straightforward approach another approach that could stated and we did initially was essentially

1711
01:51:12,490 --> 01:51:16,880
just basically find the maximum city for the maximum

1712
01:51:16,880 --> 01:51:19,680
for each patch so i've come to expect response here

1713
01:51:19,700 --> 01:51:22,420
i just look for the maximum point is

1714
01:51:22,430 --> 01:51:23,890
perhaps here

1715
01:51:25,210 --> 01:51:31,310
but if each patch and then i get maximum points

1716
01:51:31,350 --> 01:51:34,110
and then want to have these maximum points

1717
01:51:34,120 --> 01:51:38,390
i essentially just constraint that and i might have some wait for instance a beach

1718
01:51:38,400 --> 01:51:43,450
classifier got how strong the classifier responses at that point so that the case so

1719
01:51:43,450 --> 01:51:48,120
so around the are point WK might be very hard to i say around the

1720
01:51:48,120 --> 01:51:53,160
jaw point became of very low so i said just do a weighted least squares

1721
01:51:53,190 --> 01:51:59,150
estimate what appears like a piece that so essentially what i'm doing is that i'm

1722
01:51:59,150 --> 01:52:02,750
looking for the optimum position at each local patch

1723
01:52:02,760 --> 01:52:06,780
and then on this constraining to lie within the subspace so it's it's not really

1724
01:52:06,800 --> 01:52:11,180
good way to solve because i'm not solving talk cost function and canada's passing the

1725
01:52:11,180 --> 01:52:15,680
cost function into two steps and then solving but it's true it's and it's an

1726
01:52:19,010 --> 01:52:24,240
referred to this as exhaustive local search so now we had exhaustive search but exhaustive

1727
01:52:24,240 --> 01:52:25,820
local search

1728
01:52:26,680 --> 01:52:27,720
and so

1729
01:52:27,740 --> 01:52:29,910
but interesting enough

1730
01:52:30,010 --> 01:52:31,530
it doesn't work too badly

1731
01:52:31,550 --> 01:52:36,240
so this is a inverse dls so we can consider that i am

1732
01:52:36,260 --> 01:52:37,860
finding a lot of the time

1733
01:52:37,880 --> 01:52:40,410
so the jewel there

1734
01:52:40,450 --> 01:52:43,010
eleven illus sometimes does better

1735
01:52:43,050 --> 01:52:45,510
things like that but it really does worse

1736
01:52:45,530 --> 01:52:50,550
and it's it's it's getting a bit of a better generic performance

1737
01:52:50,570 --> 01:52:54,910
but it's of the suffering because we kind of got is really lousy way of

1738
01:52:54,910 --> 01:52:58,160
solving the cost function

1739
01:52:58,740 --> 01:53:02,930
we did some things and complexity o point touch too much of it but it

1740
01:53:02,930 --> 01:53:07,160
turns out that when we actually did analysis complexity you can think well exhaustive search

1741
01:53:07,300 --> 01:53:08,260
kind of slow

1742
01:53:08,280 --> 01:53:11,590
it turns out that we actually considerably

1743
01:53:13,050 --> 01:53:14,910
then some time ever

1744
01:53:14,930 --> 01:53:18,130
even though the gradient search but we still

1745
01:53:18,150 --> 01:53:21,320
can't slow the project so kind in between this somewhere

1746
01:53:21,340 --> 01:53:24,050
in our experiments we can get running about

1747
01:53:24,110 --> 01:53:25,570
eight to nine friends second

1748
01:53:25,590 --> 01:53:28,450
it's not about

1749
01:53:28,510 --> 01:53:31,220
even though we're doing exhaustive search

1750
01:53:31,610 --> 01:53:36,220
this is another point was that the nice thing that we have an axe to

1751
01:53:36,220 --> 01:53:40,550
explore this too much the nice thing is because we're treating each of these patches

1752
01:53:40,550 --> 01:53:47,180
separately and independently there is perhaps some scope here to do some parallel computing so

1753
01:53:47,180 --> 01:53:52,320
basically i could perhaps distributing exhaustive searches across a whole heap computers that need to

1754
01:53:52,320 --> 01:53:55,630
be done sequentially that can be done independently and they want to have the form

1755
01:53:55,700 --> 01:54:00,160
decisions confuse them back together and so we did some again we did some kind

1756
01:54:00,180 --> 01:54:04,340
of analysis and things and we can speculate that if we if we are able

1757
01:54:04,360 --> 01:54:08,840
to parallel compute we could actually perhaps get better the project

1758
01:54:08,840 --> 01:54:11,590
but again this to sort in theory

1759
01:54:11,590 --> 01:54:13,840
yes i complexity

1760
01:54:14,470 --> 01:54:19,220
and as we have some we want to we want to explore this more

1761
01:54:19,360 --> 01:54:24,030
so a major problem with the list is that finding in local minimum rather than

1762
01:54:24,030 --> 01:54:28,200
a single global minimum so it's far too computationally expensive

1763
01:54:28,260 --> 01:54:30,700
two exhaustive search for the global minimum

1764
01:54:30,720 --> 01:54:34,360
so one thing that we try to take advantage of and keep on going on

1765
01:54:37,430 --> 01:54:40,470
so obviously we kind of went ivory

1766
01:54:40,530 --> 01:54:45,430
but the nice thing about complexity and this is some pretty pretty pictures here

1767
01:54:45,430 --> 01:54:49,390
is that enforcing come convexity has a number of local is the properties like mean

1768
01:54:49,390 --> 01:54:50,970
like with global minimum

1769
01:54:50,970 --> 01:54:52,840
continuous and differentiable

1770
01:54:53,100 --> 01:54:58,680
allows me based methods so fast and this is the clincher summation of in convex

1771
01:54:58,680 --> 01:55:00,930
functions is a convex function

1772
01:55:00,970 --> 01:55:03,200
so if i can sure

1773
01:55:03,200 --> 01:55:04,890
pat responses

1774
01:55:05,160 --> 01:55:09,220
across my other sixty points are all convex and i sum them up the cost

1775
01:55:09,220 --> 01:55:10,360
function shows

1776
01:55:10,380 --> 01:55:14,800
my cost function will be convex and so i can solve

1777
01:55:14,820 --> 01:55:18,090
but the problem is that well at the moment my pet responses

1778
01:55:19,650 --> 01:55:22,030
is there a way that i can forces convexity

1779
01:55:22,090 --> 01:55:28,380
so that's what we did we basically tried to enforce convexity patch

1780
01:55:28,380 --> 01:55:30,410
so we basically tried to fit

1781
01:55:30,410 --> 01:55:32,070
convex quadratic

1782
01:55:32,090 --> 01:55:33,740
to each patch response

1783
01:55:33,760 --> 01:55:38,650
so this is what the visualization of it its

1784
01:55:38,650 --> 01:55:41,280
it's kind of looks like a smoothing but

1785
01:55:41,300 --> 01:55:46,780
it's not represented nonparametrically anymore it's actually represented parametrically is quadratic

1786
01:55:46,780 --> 01:55:49,340
and so this is how we represent

1787
01:55:51,110 --> 01:55:55,160
and this just through so these are the shifts so i've got a fifteen by

1788
01:55:55,160 --> 01:56:00,040
is just one of many papers by wainwright in short simoncelli where it's not just

1789
01:56:00,040 --> 01:56:02,860
positing some prior that you pull out of your head it's nice to work with

1790
01:56:02,860 --> 01:56:06,960
like gaussians but you can go out and measure the actual empirical statistics of natural

1791
01:56:06,960 --> 01:56:10,550
scenes and show that can be used to predict behavior in neural responses and you

1792
01:56:10,550 --> 01:56:15,050
might ask could we extend that kind of natural scene statistics approach to cognition

1793
01:56:15,100 --> 01:56:20,140
in some of the early work that was the a probabilistic cognitive science

1794
01:56:20,150 --> 01:56:23,040
the subfield subfields doing we do see that so here's an example of a study

1795
01:56:23,040 --> 01:56:26,850
that tom griffiths and i did where we give people these basically these textbook bayesian

1796
01:56:26,850 --> 01:56:30,720
statistics problems it's basically the problem of interval estimation

1797
01:56:30,730 --> 01:56:31,480
so you see

1798
01:56:31,600 --> 01:56:33,780
just to give you some these examples we set them in a kind of an

1799
01:56:33,780 --> 01:56:37,950
everyday setting where people might have wi-fi good intuitions so suppose you read about a

1800
01:56:37,950 --> 01:56:40,600
movie that has made sixty million dollars to date how much money will it make

1801
01:56:40,600 --> 01:56:43,710
in total or you see something in baking in the oven for thirty four minutes

1802
01:56:43,710 --> 01:56:46,720
how long until it's ready you meet someone who is seventy eight years old how

1803
01:56:46,720 --> 01:56:50,470
long will they live and so on each of these cases you encounter some phenomenon

1804
01:56:50,480 --> 01:56:55,250
has some unknown extent or duration teetotal at some random time or value t less

1805
01:56:55,250 --> 01:56:58,610
than teetotal and you have to guess the total from that one observation and we

1806
01:56:58,610 --> 01:57:01,440
can go and look at the natural scene statistics on the top here you see

1807
01:57:01,440 --> 01:57:06,230
the empirical statistics for these classes of events and then on the bottom you see

1808
01:57:06,230 --> 01:57:08,670
two things you see behavioral data the little dots

1809
01:57:08,680 --> 01:57:12,750
are are median subjects predictions of the the total duration as a function of the

1810
01:57:12,750 --> 01:57:17,000
given example and then you see the optimal bayesian predictions from the empirical statistics and

1811
01:57:17,000 --> 01:57:20,150
it's quite striking how how close this is i mean to get to get this

1812
01:57:20,150 --> 01:57:22,360
right here people have to do two things they have to be bayesian but they

1813
01:57:22,360 --> 01:57:25,130
also have to have the right priors and it's quite interesting but they seem to

1814
01:57:25,130 --> 01:57:28,720
be sensitive to the the different forms of the distributions for these different classes of

1815
01:57:28,720 --> 01:57:33,710
everyday events but this approach is going out and measuring statistics doesn't obviously extend to

1816
01:57:33,720 --> 01:57:37,210
the more cognitive problems that we interested in here so i can the case of

1817
01:57:37,210 --> 01:57:40,970
word learning what is the right prior what's the right hypothesis space it's not even

1818
01:57:40,970 --> 01:57:44,390
clear how to define that are going to measure in the world and not to

1819
01:57:44,390 --> 01:57:48,480
mention having a human learner brain autonomously figure that out

1820
01:57:48,490 --> 01:57:52,750
so we can look for inspiration to again classic cognitive psychology which suggests that natural

1821
01:57:52,750 --> 01:57:58,240
categories are nameable categories might be organised into something like a hierarchical tree data structure

1822
01:57:58,440 --> 01:58:03,010
saying with you no categories at different levels of abstraction so here you've canaries and

1823
01:58:03,010 --> 01:58:07,560
their bird bird and animal and so on but even evidence looking at population responses

1824
01:58:07,560 --> 01:58:11,350
from the level visual system the visual categories to organize that way

1825
01:58:11,370 --> 01:58:14,730
and we've been able to make models of of both adults and children were learning

1826
01:58:14,730 --> 01:58:18,550
with that same kind of idea so we can take for example these these alien

1827
01:58:18,550 --> 01:58:22,480
objects and come up with the tree structure that's shown here

1828
01:58:22,500 --> 01:58:26,550
such that the the the branch point the nodes of this tree correspond to hypotheses

1829
01:58:26,550 --> 01:58:30,220
of nameable categories and then we can define a prior based on the branch line

1830
01:58:30,220 --> 01:58:36,210
basically how perceptually distinct categories it's very similar to the probabilistic models using bayesian phylogenetics

1831
01:58:36,230 --> 01:58:42,130
so far from modeling distributions of properties arising from mutation and the likelihood just comes

1832
01:58:42,130 --> 01:58:46,510
from assuming the examples are random uniform sample from the labelled branch and then it's

1833
01:58:46,510 --> 01:58:49,210
pretty clear how to get this kind of behavior here where you see a few

1834
01:58:49,210 --> 01:58:52,950
examples that seem to clustering one distinctive branch and that tells you who for over

1835
01:58:52,950 --> 01:58:55,710
there in that part of the tree and we can use this model to make

1836
01:58:55,710 --> 01:58:59,580
quantitative predictions of people's judgments for one or a few examples and i'm showing you

1837
01:58:59,580 --> 01:59:04,170
just to see you can believe that we can make quantitatively accurate models people versus

1838
01:59:04,170 --> 01:59:08,430
this bayesian model for generalization of the new words from one or a few examples

1839
01:59:08,430 --> 01:59:11,350
the cluster different levels of the tree but i don't want to dwell too much

1840
01:59:11,350 --> 01:59:14,900
on the experiment because the interesting comes from machine learning are really how do we

1841
01:59:14,900 --> 01:59:18,650
get to the point where is this hypothesis space and prior come from

1842
01:59:19,550 --> 01:59:24,360
we built as modelers by asking subjects to judge in separate experiment to judge the

1843
01:59:24,360 --> 01:59:28,330
similarity of these objects and then doing hierarchical clustering but how does the brain do

1844
01:59:28,340 --> 01:59:32,150
it i mean in the sense that that's just cheating right it's we all know

1845
01:59:32,150 --> 01:59:35,810
that that is an infinite number of features you can compute from the visual image

1846
01:59:35,990 --> 01:59:38,810
and depending on which features you use or how you weight them you get very

1847
01:59:38,810 --> 01:59:42,100
different similarity metrics in the real question is basically how do people learn the right

1848
01:59:42,100 --> 01:59:46,100
notion of similarity or you might ask more deeply and you know it given the

1849
01:59:46,110 --> 01:59:49,330
similarity metric you can build the tree by hierarchical clustering but how do you know

1850
01:59:49,330 --> 01:59:53,200
usually building tree or other kinds of domains might be organised in other ways it's

1851
01:59:53,200 --> 01:59:56,930
not like everything in cognition history structure although it's amazing how often that seems to

1852
01:59:56,930 --> 02:00:00,210
come up with the natural representation so these are some of the questions that we

1853
02:00:00,220 --> 02:00:03,140
that we've been approaching and then you can think of them again this kind of

1854
02:00:03,140 --> 02:00:07,850
questions of learning to learn if you look at the literature and children's cognitive element

1855
02:00:07,850 --> 02:00:10,960
you can see the children actually do learn these things they learn what features of

1856
02:00:10,960 --> 02:00:15,450
objects to pay attention to forward learning and they learn to organize things into a

1857
02:00:15,450 --> 02:00:19,960
tree it's not there initially so for example in this method colleagues study the development

1858
02:00:19,960 --> 02:00:23,360
of the shape bias which is this which is this phenomenon you can see right

1859
02:00:23,360 --> 02:00:27,850
here you show kids say a two-year-old this novel object they haven't seen the backs

1860
02:00:28,110 --> 02:00:30,860
and which other one is that that's on the right bill pick up the one

1861
02:00:30,860 --> 02:00:34,690
with the same shape but novel texture material as opposed to say one matches in

1862
02:00:34,690 --> 02:00:39,620
texture material but not the same shape kids speaking english have this two years of

1863
02:00:39,620 --> 02:00:43,530
age but younger kids like stating models don't have this so somewhere in there they

1864
02:00:43,530 --> 02:00:44,340
learn it

1865
02:00:44,350 --> 02:00:47,740
and they learn other things a little bit older they learn different biases for different

1866
02:00:47,740 --> 02:00:50,400
kinds of words are entities like material bias

1867
02:00:51,510 --> 02:00:56,230
words for say non solid substances like learning toothpaste to the word honey or water

1868
02:00:56,230 --> 02:00:59,150
it's more about the texture properties of the material than any kind of shape for

1869
02:00:59,150 --> 02:01:03,030
these non solid substances so it's if you like two kids have to be able

1870
02:01:03,030 --> 02:01:06,850
to learn what counts for similarity but they have to learn different similarity metrics for

1871
02:01:06,850 --> 02:01:11,930
different kinds of concepts and again as i already referred to early on children in

1872
02:01:11,930 --> 02:01:16,140
learning words don't have a hierarchical structure categories but what's called mutual exclusivity this is

1873
02:01:16,170 --> 02:01:19,700
just one way to label each things like flat partition into categories and then only

1874
02:01:19,700 --> 02:01:24,110
later say by age age four supposed age two they start to learn that you

1875
02:01:24,110 --> 02:01:28,770
can have words like mammal animal living thing that refer to higher level categories

1876
02:01:28,800 --> 02:01:31,920
so in in some work that we did a few years ago with charles kemp

1877
02:01:31,920 --> 02:01:36,640
and amy perfors we need fairly straightforward simple models of these aspects of learning to

1878
02:01:36,640 --> 02:01:39,260
learn and apply them to carbon development but what i want to tell you guys

1879
02:01:39,260 --> 02:01:42,450
about this work that we've been doing which is more of the same kind of

1880
02:01:42,450 --> 02:01:46,050
idea but in the machine learning setting we can actually appreciate the value of this

1881
02:01:46,050 --> 02:01:50,470
this kind of cognitive learning to learn for the sorts of applications that that that

1882
02:01:50,470 --> 02:01:51,850
was interested

1883
02:01:51,850 --> 02:01:56,100
so here's some work that i've been doing with russ salakhutdinov who's post-doc at MIT

1884
02:01:56,100 --> 02:02:00,010
working with me and also with antonio torralba and we framed in the setting of

1885
02:02:00,010 --> 02:02:05,820
securing retrieving images from database but this isn't meant to be primarily of vision project

1886
02:02:06,050 --> 02:02:08,580
it's just that this is an application that many of us are familiar with and

1887
02:02:08,590 --> 02:02:12,690
work on the same idea could apply to speech motor action where you know we

1888
02:02:12,690 --> 02:02:15,690
want to do you want to be able to learn say a gesture from one

1889
02:02:15,690 --> 02:02:19,000
example or new speech sound or what someone's voice sounds like before you know just

1890
02:02:19,200 --> 02:02:22,560
you only have to hear someone talking for a short while to get their voices

1891
02:02:23,130 --> 02:02:26,930
so in the context of image database retrieval in standard problem is we want to

1892
02:02:26,930 --> 02:02:31,260
say curry with one image say however there and we want to we want to

1893
02:02:31,260 --> 02:02:34,850
search over many many images and return the other cows and not the other things

1894
02:02:35,100 --> 02:02:37,770
and what you see on the top is the kind of behaviour we'd like to

1895
02:02:37,770 --> 02:02:42,100
get in and that is the result of our our system's performance on that curry

1896
02:02:42,390 --> 02:02:45,890
but unfortunately if you just take a standard kind of naive similarity based approach what

1897
02:02:45,890 --> 02:02:48,470
you get something that looks like on the bottom you get some cows and some

1898
02:02:48,470 --> 02:02:52,480
chinese in some fields now why is that well you know the standard approach for

1899
02:02:52,480 --> 02:02:55,250
doing this was just one example fancy things you can do if you're allowed to

1900
02:02:55,250 --> 02:02:59,010
crew with multiple examples and learned discriminatively of just one example the best you can

1901
02:02:59,020 --> 02:03:02,350
do is something like similarity to you take some feature space here we have this

1902
02:03:02,350 --> 02:03:07,070
roughly fifty thousand dimensional texture texture feature space but also was introduced at NIPS in

1903
02:03:07,070 --> 02:03:10,730
the in the late nineties by devon in viola and this one works very well

1904
02:03:10,730 --> 02:03:14,650
in the more discriminative setting it's sort of the inspiration and basis for like the

1905
02:03:14,650 --> 02:03:18,530
viola jones face detector and so on but in this and it's sort of modelled

1906
02:03:18,530 --> 02:03:23,440
on the early structure of the visual system with recursively nested bore filters

1907
02:03:23,450 --> 02:03:27,600
in this setting though you have this fifty thousand dimensional feature space and an all

1908
02:03:27,600 --> 02:03:30,550
you can do is compute similarity to the query you don't really know which dimensions

1909
02:03:30,550 --> 02:03:33,960
count so something which match the grass in the background is almost as good as

1910
02:03:33,960 --> 02:03:38,220
something which actually matches the count another way to put this is at a minimum

