1
00:00:00,000 --> 00:00:01,470
so for instance

2
00:00:01,480 --> 00:00:02,980
if you have a case

3
00:00:02,980 --> 00:00:04,900
we're all the wife

4
00:00:05,570 --> 00:00:07,110
normally distributed

5
00:00:08,420 --> 00:00:09,980
and you have

6
00:00:10,000 --> 00:00:15,000
well a markov random field of not too long range interactions

7
00:00:15,750 --> 00:00:20,530
you get a covariance matrix which is not too wide and you can actually just

8
00:00:20,550 --> 00:00:25,980
two standard dynamic programming which in this case would just be gauss elimination through the

9
00:00:26,980 --> 00:00:31,680
so common filters will be doing something a little bit like that

10
00:00:33,070 --> 00:00:36,330
the main does not have to be finite in order to to allow you to

11
00:00:36,330 --> 00:00:37,980
do your summation

12
00:00:37,990 --> 00:00:39,520
but what it has to be

13
00:00:39,540 --> 00:00:43,330
is it has to allow you to actually carry out the summation

14
00:00:43,380 --> 00:00:47,440
having defined a small domain is sufficient condition

15
00:00:47,550 --> 00:00:51,960
but not a necessary one

16
00:00:52,010 --> 00:00:52,990
thank you

17
00:00:53,050 --> 00:00:56,770
and in some cases i mean when you don't have to find the main like

18
00:00:56,780 --> 00:01:00,520
or even you have five members just lot migrants images

19
00:01:00,570 --> 00:01:06,380
you may have to resort to other clever tricks on how to restricted domain

20
00:01:06,410 --> 00:01:09,540
one thing for instance you can do is i don't know who is

21
00:01:09,630 --> 00:01:13,810
seem to be the work of course let's say a across from CMU

22
00:01:13,820 --> 00:01:19,500
who's played around with the eliminate feature of

23
00:01:19,530 --> 00:01:24,150
the market actually say eliminated and just sample from the background

24
00:01:24,160 --> 00:01:26,440
in a way what he does is he

25
00:01:26,490 --> 00:01:28,440
restricts the main

26
00:01:28,480 --> 00:01:33,560
to a subset of packages from images that you've seen before

27
00:01:33,580 --> 00:01:37,840
and then samples from that made in or to fill in the remainder

28
00:01:37,860 --> 00:01:39,640
it's actually very simple

29
00:01:39,660 --> 00:01:42,150
model where you restrict the domain

30
00:01:42,200 --> 00:01:43,920
in a very clever way

31
00:01:43,960 --> 00:01:47,090
basically without any optimisation

32
00:01:47,120 --> 00:01:50,880
this is one way how you can cheat your way around

33
00:01:50,900 --> 00:01:53,280
and intractably large domain

34
00:01:53,300 --> 00:01:59,840
if you she cleverly enough it's just as good as well the real thing

35
00:02:00,860 --> 00:02:03,510
i mean that you seem to wear a lot of the art in graphical models

36
00:02:03,510 --> 00:02:09,440
papers comes in well how to deal cleverly with the main that first sight looks

37
00:02:15,480 --> 00:02:17,130
that's the the

38
00:02:17,150 --> 00:02:19,220
any other questions

39
00:02:19,340 --> 00:02:21,760
if one

40
00:02:28,240 --> 00:02:30,550
did which is what happens

41
00:02:35,800 --> 00:02:39,190
i mean to the two thousand eight he watches carefully

42
00:02:39,920 --> 00:02:42,880
so the first thing you can do it you can

43
00:02:42,920 --> 00:02:45,050
use the subspace representer theorem

44
00:02:45,070 --> 00:02:48,630
just expand in terms of all the terms they

45
00:02:48,670 --> 00:02:51,940
and for some problems that may be good enough

46
00:02:51,960 --> 00:02:54,320
so for instance if

47
00:02:54,550 --> 00:02:59,660
the exists if that space is too large like a few thousand and if the

48
00:02:59,660 --> 00:03:02,170
wiser only range over the set of

49
00:03:02,220 --> 00:03:04,320
two three cardinality

50
00:03:04,320 --> 00:03:07,090
and this may be perfectly fine

51
00:03:08,780 --> 00:03:10,420
problem is much larger

52
00:03:10,440 --> 00:03:15,550
then you will have to use some sparse approximate mean that onto which starts by

53
00:03:15,550 --> 00:03:19,570
the project is solution onto

54
00:03:19,630 --> 00:03:24,130
so that being just the standard problem of like what you will usually what happened

55
00:03:24,130 --> 00:03:30,070
gaussianprocess thing as opposed to classification that you can solve the problem for large datasets

56
00:03:30,070 --> 00:03:30,880
for you

57
00:03:30,920 --> 00:03:35,960
iteratively expands into a subspace and solve the problem there

58
00:03:35,960 --> 00:03:38,190
you will usually be able to go

59
00:03:38,240 --> 00:03:43,380
good performance guarantees and how how far you away from an optimal solution

60
00:03:43,400 --> 00:03:44,260
but you will

61
00:03:44,280 --> 00:03:47,070
of course not get the optimal solution

62
00:03:47,090 --> 00:03:49,340
so what you would do is we would then use

63
00:03:49,360 --> 00:03:53,740
indications of how large your duality gap is and so on in order to get

64
00:03:53,760 --> 00:03:57,070
any ideas on on how far you away from the optimal solution

65
00:03:59,510 --> 00:04:02,530
roughly the idea

66
00:04:02,530 --> 00:04:03,940
and again that's

67
00:04:03,940 --> 00:04:05,940
an active area of research

68
00:04:06,030 --> 00:04:10,670
and people are debating whether you should use the problem dual method for instance can

69
00:04:10,760 --> 00:04:15,190
be has done a lot of great work on the problem is that in such

70
00:04:15,210 --> 00:04:16,690
kind of problems

71
00:04:16,740 --> 00:04:19,920
there is no i i don't know how much you'll be able to download this

72
00:04:19,920 --> 00:04:21,280
recent pages but

73
00:04:21,340 --> 00:04:24,110
a lot of work on the

74
00:04:24,130 --> 00:04:28,760
source the paper made mangasarian on this but it doesn't talk about statistics at all

75
00:04:28,980 --> 00:04:34,960
in the other places along those lines

76
00:04:37,670 --> 00:04:45,360
let's just quickly connectors and compare it measurements

77
00:04:46,610 --> 00:04:51,670
in a conditional random field what we did is we say well there that the

78
00:04:51,670 --> 00:04:53,170
observation x

79
00:04:53,190 --> 00:04:56,320
you want to annotate it by labels y

80
00:04:56,340 --> 00:05:01,460
and this was essentially all dependencies structure in the white ivory dropped the dependencies and

81
00:05:01,480 --> 00:05:04,960
it's because they don't really matter

82
00:05:04,980 --> 00:05:11,280
so you would think oh and maximize p of y given x and theta

83
00:05:11,300 --> 00:05:15,400
now in hidden markov model what you typically do if you would say well actually

84
00:05:15,400 --> 00:05:18,170
this is hidden state why they want to infer

85
00:05:18,300 --> 00:05:22,320
but i only seventy x radiation and talking

86
00:05:22,320 --> 00:05:25,740
this is what you serving and this is what's going on in my mind and

87
00:05:25,740 --> 00:05:28,820
you're trying to infer that

88
00:05:30,360 --> 00:05:34,380
and what do they do is you were try to maximize

89
00:05:34,400 --> 00:05:35,550
in some way

90
00:05:35,550 --> 00:05:39,240
joint probability over this is unwise

91
00:05:39,300 --> 00:05:44,070
and for some parameter theta so you would actually have to model how my state

92
00:05:44,070 --> 00:05:45,740
of mind being

93
00:05:45,800 --> 00:05:49,070
in influences what i'm going to produce sound

94
00:05:49,150 --> 00:05:52,280
that has quite the big achilles heel in

95
00:05:53,300 --> 00:05:54,690
actually your model

96
00:05:54,740 --> 00:05:59,630
of what i'm saying needs to be pretty good in order to make any inference

97
00:05:59,650 --> 00:06:05,130
you know what you are optimizing the wrong criteria

98
00:06:05,150 --> 00:06:06,400
and so

99
00:06:06,420 --> 00:06:07,740
if you just

100
00:06:07,740 --> 00:06:10,170
try to to model both x and y

101
00:06:10,610 --> 00:06:15,740
well i think a lot of capacity that in the past

102
00:06:15,760 --> 00:06:20,260
on modeling the axis which are you know which you will never have to estimate

103
00:06:20,280 --> 00:06:25,420
but you wasting a lot of your computation on doing something on working on trying

104
00:06:25,420 --> 00:06:32,070
to answer the question that you will never be as

105
00:06:32,090 --> 00:06:35,940
now actually it turns out that there is an equivalence theorem saying that well

106
00:06:35,960 --> 00:06:37,800
here and hidden markov models

107
00:06:37,860 --> 00:06:39,320
you seem to be

108
00:06:39,340 --> 00:06:41,780
probability estimates

109
00:06:41,840 --> 00:06:46,690
if you're trying to hidden markov models in discriminative fashion

110
00:06:46,720 --> 00:06:50,260
and the set of functions is equally expressive

111
00:06:50,260 --> 00:06:53,570
so how do prove this well

112
00:06:53,590 --> 00:06:55,880
it just right after

113
00:06:55,920 --> 00:07:00,940
before the series the one for the hidden markov models and you show that they

114
00:07:00,940 --> 00:07:02,880
only differ in the normalisation

115
00:07:02,880 --> 00:07:04,960
problem that can be solved that

116
00:07:05,010 --> 00:07:10,330
suppose you want to minimize the maximum of linear functions of x the general piecewise

117
00:07:11,140 --> 00:07:12,540
the function of x

118
00:07:12,550 --> 00:07:18,360
then we already know that this is always convex because the maximum of family or

119
00:07:18,410 --> 00:07:24,110
the a convex function of x each of these separately protect the i convex

120
00:07:24,130 --> 00:07:26,620
so the maximum is convex

121
00:07:26,740 --> 00:07:30,790
if you're just interested in knowing the context of the

122
00:07:30,910 --> 00:07:32,030
the answer

123
00:07:32,080 --> 00:07:33,770
if you want to solve it then

124
00:07:34,530 --> 00:07:39,170
you have one difficulty that this problem is actually nondifferentiable

125
00:07:39,210 --> 00:07:43,650
you can just use of it by two method for example because

126
00:07:43,690 --> 00:07:46,020
because of the max it's not defensible

127
00:07:46,080 --> 00:07:48,510
it's underconstrained something sensible

128
00:07:48,520 --> 00:07:51,750
so the standard way of solving problems like this is to convert them into an

129
00:07:53,670 --> 00:07:56,460
and you do that by introducing new variables t

130
00:07:56,490 --> 00:07:58,010
scalar value would be

131
00:07:58,020 --> 00:08:03,270
and then you minimize the subject to these inequality for each of these inequalities says

132
00:08:03,270 --> 00:08:07,330
that he is an upper bound on the fact that he i

133
00:08:07,410 --> 00:08:11,270
so altogether means that he is an upper bound on the maximum of the function

134
00:08:11,710 --> 00:08:13,140
with a i

135
00:08:13,180 --> 00:08:15,290
if you minimize the subject to

136
00:08:15,340 --> 00:08:18,910
the constraint that is greater than the maximum that same minimizer

137
00:08:19,020 --> 00:08:26,790
so if you have know this information and you have no peace over you very

138
00:08:26,790 --> 00:08:27,990
efficiently solving

139
00:08:28,030 --> 00:08:32,940
piecewise linear decision problem

140
00:08:32,950 --> 00:08:38,780
another example is this but they completely described by inequalities

141
00:08:38,820 --> 00:08:43,210
and i ask are trying to find the centre of the village

142
00:08:43,260 --> 00:08:49,300
defined as the centre of the largest inscribed all inside the police

143
00:08:49,400 --> 00:08:54,030
it's called chebyshev actually maximizes like this

144
00:08:54,080 --> 00:08:57,110
two women and this is the boundary

145
00:08:57,160 --> 00:09:02,670
and is sometimes used as the initialisation of the centre of the body

146
00:09:02,680 --> 00:09:03,730
how do you

147
00:09:03,790 --> 00:09:07,690
i find this well the first condition is supposed to look at one of the

148
00:09:07,690 --> 00:09:13,420
neck the inequalities we just as the question is this given

149
00:09:15,230 --> 00:09:16,820
the centre xc

150
00:09:16,880 --> 00:09:19,040
and radius are

151
00:09:19,090 --> 00:09:22,770
contained in the half space defined by one

152
00:09:22,840 --> 00:09:26,190
pictures just one inequality for example

153
00:09:26,210 --> 00:09:31,410
and the question is does this all lies on the correct side of the inequality

154
00:09:32,860 --> 00:09:34,510
what if i know xy

155
00:09:34,550 --> 00:09:36,420
centre and the radio

156
00:09:36,430 --> 00:09:40,080
but if you look at the condition of it means that they i suppose x

157
00:09:40,080 --> 00:09:43,790
must be less than b i put all points and all

158
00:09:43,860 --> 00:09:49,540
the journal vol element in this all i can represent that exceed centre plus the

159
00:09:49,540 --> 00:09:51,690
vector normalized and are

160
00:09:51,700 --> 00:09:55,410
so the question is is this maximum overall

161
00:09:55,830 --> 00:09:58,680
you've written on as an artist and b

162
00:09:58,830 --> 00:10:04,160
this max something that they can work out easily get in the baltic sea

163
00:10:04,210 --> 00:10:08,510
and i think they maximize the maximum of it with you

164
00:10:08,530 --> 00:10:10,910
pro-eu bit normalized and are

165
00:10:10,910 --> 00:10:13,380
but you

166
00:10:13,420 --> 00:10:16,280
k two terms that's what c

167
00:10:16,410 --> 00:10:18,930
the maximum of its both thank you

168
00:10:18,950 --> 00:10:22,410
they take the maximum over all you with euclidean norm is an hour

169
00:10:22,520 --> 00:10:23,640
that simple

170
00:10:23,750 --> 00:10:29,340
the two normal the european on a high rate

171
00:10:29,400 --> 00:10:35,300
so the condition for this borderline as space is at a specific c

172
00:10:35,380 --> 00:10:38,730
this time euclidean norm a i b i

173
00:10:38,780 --> 00:10:43,320
look at this issue it's a linear inequality in xy and are

174
00:10:44,520 --> 00:10:49,310
use those variables at the midpoint of the year and radius

175
00:10:49,320 --> 00:10:52,480
then i get a linear inequality index you are

176
00:10:52,490 --> 00:10:57,440
and then i can find the maximum largest ball inscribed into politics by maximizing are

177
00:10:57,440 --> 00:10:59,530
subject to m of the inequality

178
00:10:59,540 --> 00:11:05,550
each of the inequality says that what lies on one of the as basis

179
00:11:06,410 --> 00:11:07,340
but this

180
00:11:07,400 --> 00:11:12,150
lp is an LP because your objective opinion on the variables are axioms are

181
00:11:12,160 --> 00:11:13,240
this year

182
00:11:13,250 --> 00:11:18,630
the conference here on

183
00:11:18,680 --> 00:11:25,940
so the and the next example actually familiar for people in machine learning

184
00:11:26,000 --> 00:11:28,980
but problem of separating two points by

185
00:11:30,780 --> 00:11:34,270
these are the points here and there

186
00:11:34,320 --> 00:11:37,320
invisible for other side

187
00:11:37,330 --> 00:11:42,160
so i tried to find has been hyperplane that we already at find function that

188
00:11:42,160 --> 00:11:47,130
separates the two sets so i want a sensible excited to be positive

189
00:11:47,140 --> 00:11:50,660
four sets in exile point onset

190
00:11:50,710 --> 00:11:54,910
and then it i suppose my ability to be negative with the other set

191
00:11:54,910 --> 00:11:58,310
and as a consequence the conditional on x a and b

192
00:11:58,320 --> 00:12:00,770
the parameters of the hyperplane that linear

193
00:12:01,160 --> 00:12:02,830
sixteen the of inequality

194
00:12:04,180 --> 00:12:09,730
and there is a small difference that we actually pulled the inequalities usually want non-stick

195
00:12:11,280 --> 00:12:12,790
and we can actually

196
00:12:14,060 --> 00:12:17,590
by noting that this this condition is homogeneous in a and b

197
00:12:17,780 --> 00:12:22,180
a and b satisfy these conditions that can multiply a and b then any

198
00:12:22,230 --> 00:12:24,630
number and they get

199
00:12:24,640 --> 00:12:25,930
our solution

200
00:12:25,940 --> 00:12:29,290
so can resolve scale everything so that the right hand side is greater than one

201
00:12:30,140 --> 00:12:32,280
and there's an minus one or the other

202
00:12:32,500 --> 00:12:37,400
and then you get this set of templates and linear inequalities in a b

203
00:12:37,410 --> 00:12:43,110
and then so that's four

204
00:12:43,310 --> 00:12:45,090
separate separable

205
00:12:45,910 --> 00:12:49,840
that's acceptable then the set of inequalities has solution

206
00:12:49,860 --> 00:12:52,140
if it's not try to find the

207
00:12:52,490 --> 00:12:58,820
if not separable try to find a hyperplane that separates them as well as you

208
00:12:59,920 --> 00:13:00,820
in one

209
00:13:00,820 --> 00:13:02,110
o point

210
00:13:03,720 --> 00:13:05,700
now i get o point

211
00:13:05,740 --> 00:13:09,220
now i am very close

212
00:13:09,260 --> 00:13:11,910
the maximum

213
00:13:13,280 --> 00:13:15,760
this is very convincing

214
00:13:16,070 --> 00:13:18,160
if you make your film

215
00:13:18,220 --> 00:13:19,950
this thing

216
00:13:19,990 --> 00:13:23,160
that it will look distinctly red

217
00:13:23,180 --> 00:13:24,860
maybe not proof read the

218
00:13:25,220 --> 00:13:28,180
i think the red color dominance

219
00:13:28,200 --> 00:13:29,200
the blue

220
00:13:29,220 --> 00:13:34,800
technically absence at all apart from the imbalance you may have

221
00:13:34,840 --> 00:13:36,360
the four one zero

222
00:13:36,380 --> 00:13:40,140
and the reports

223
00:13:40,160 --> 00:13:43,470
the question that i have asked over the years at the final exam for a

224
00:13:43,470 --> 00:13:44,490
two or three years

225
00:13:44,490 --> 00:13:46,950
whether the thin-film interference

226
00:13:46,970 --> 00:13:49,640
is the result of the difference

227
00:13:49,700 --> 00:13:50,610
the three

228
00:13:50,610 --> 00:13:52,510
the index of refraction

229
00:13:52,550 --> 00:13:56,300
between the different colors and is absolutely not

230
00:13:56,360 --> 00:14:01,660
the index of refraction i have taken the same for all for all practical purposes

231
00:14:01,700 --> 00:14:05,630
that this does not explain caused colors explain

232
00:14:05,680 --> 00:14:11,630
by the different path length divided by that that is why you see cause nothing

233
00:14:11,630 --> 00:14:12,320
to do

234
00:14:12,380 --> 00:14:18,220
with the index of refraction

235
00:14:18,260 --> 00:14:19,950
you can see here

236
00:14:20,010 --> 00:14:22,130
thin-film interference in soap

237
00:14:22,160 --> 00:14:25,800
now try to demonstrate that you can see it in oil spills on the road

238
00:14:25,860 --> 00:14:28,840
also unique example

239
00:14:30,060 --> 00:14:31,630
then films

240
00:14:31,640 --> 00:14:33,590
if you call

241
00:14:33,630 --> 00:14:37,360
i think films do not give you cause

242
00:14:37,360 --> 00:14:40,410
if you go through the exercise which i want to do

243
00:14:41,050 --> 00:14:42,200
to make the

244
00:14:42,260 --> 00:14:44,800
o point one million

245
00:14:44,840 --> 00:14:45,470
which is

246
00:14:45,470 --> 00:14:48,950
practical standard still very thin

247
00:14:49,030 --> 00:14:50,470
you will not see

248
00:14:50,530 --> 00:14:52,160
any cause

249
00:14:52,240 --> 00:14:53,840
and the reason is

250
00:14:53,880 --> 00:14:54,950
that if you

251
00:14:54,950 --> 00:14:57,800
go to this equation

252
00:14:57,800 --> 00:14:59,380
and you calculate

253
00:14:59,430 --> 00:15:01,930
four which colors you will see

254
00:15:01,930 --> 00:15:07,510
constructive that structure interference there are so many colours in the visible spectrum

255
00:15:07,530 --> 00:15:11,970
four which you get constructive interference so many you'll see

256
00:15:13,200 --> 00:15:15,860
your brains will tell you see white light

257
00:15:15,880 --> 00:15:18,530
it is not one uncommon the

258
00:15:18,590 --> 00:15:21,680
and values that you will need to buy

259
00:15:21,800 --> 00:15:25,450
i going to be very high in the range five hundred two

260
00:15:25,510 --> 00:15:27,180
seven hundred

261
00:15:27,360 --> 00:15:30,160
huge values of and are necessary

262
00:15:30,160 --> 00:15:31,320
in order to

263
00:15:32,530 --> 00:15:37,140
that's what it called interference go through this exercise on your own

264
00:15:37,160 --> 00:15:38,260
and you will see

265
00:15:38,280 --> 00:15:40,910
very quickly that so many colors

266
00:15:40,930 --> 00:15:42,640
constructively interfere

267
00:15:42,700 --> 00:15:47,220
that the film little white

268
00:15:47,280 --> 00:15:49,050
so the first thing that i want to do

269
00:15:49,240 --> 00:15:51,320
prize not so easy

270
00:15:51,380 --> 00:15:56,430
to make you see these callers was so we must have seen in fact if

271
00:15:56,450 --> 00:16:01,430
you just take a shower you soak yourself in the bubbles themselves in reflection already

272
00:16:02,130 --> 00:16:03,220
these collars

273
00:16:03,240 --> 00:16:04,700
i'll try to make one

274
00:16:05,010 --> 00:16:09,400
that is slightly larger in size do not always succeed

275
00:16:09,470 --> 00:16:11,840
let's try that

276
00:16:11,860 --> 00:16:14,490
so the idea is that one can see colours

277
00:16:15,430 --> 00:16:17,470
well maybe did but the

278
00:16:17,530 --> 00:16:20,240
there was a little quick

279
00:16:20,260 --> 00:16:32,700
now using collars

280
00:16:32,720 --> 00:16:49,990
so i wanted to say

281
00:16:51,400 --> 00:16:56,200
he said no about again i was

282
00:16:56,220 --> 00:16:58,030
i mean it want more

283
00:16:58,050 --> 00:17:13,700
well easy

284
00:17:13,760 --> 00:17:18,590
it's all right thank you

285
00:17:18,630 --> 00:17:21,260
OK so so bubbles give these course

286
00:17:21,260 --> 00:17:22,910
because they are exceedingly

287
00:17:22,930 --> 00:17:26,700
it also gives you have to

288
00:17:26,700 --> 00:17:31,610
really can be much bigger than a few times the wave lengths of light it

289
00:17:31,610 --> 00:17:35,660
becomes much thicker moved to call the reason that i just mentioned you

290
00:17:35,720 --> 00:17:38,950
that there are too many columns that constructive interference

291
00:17:39,010 --> 00:17:42,570
don't see

292
00:17:42,570 --> 00:17:46,880
now at demonstration which is

293
00:17:46,910 --> 00:17:49,090
very tricky

294
00:17:49,110 --> 00:17:54,010
it works most of the time but not all

295
00:17:54,030 --> 00:17:56,930
going to make it so film

296
00:17:57,010 --> 00:17:58,910
o eight

297
00:17:58,950 --> 00:18:00,140
metal frame

298
00:18:00,240 --> 00:18:02,410
we've done in this box

299
00:18:02,530 --> 00:18:07,280
the metal frame

300
00:18:07,340 --> 00:18:09,530
and we did but so

301
00:18:09,570 --> 00:18:12,410
and so you get soap

302
00:18:12,430 --> 00:18:13,800
and then gravity

303
00:18:13,860 --> 00:18:18,130
will make the film in the fall and figure out the ball

304
00:18:18,160 --> 00:18:20,910
so as you make a cross section through wear

305
00:18:22,220 --> 00:18:24,240
the film

306
00:18:24,340 --> 00:18:27,990
this is then so

307
00:18:27,990 --> 00:18:30,240
i can make this sort of

308
00:18:30,280 --> 00:18:36,360
is completely dark reflection that is what i promise you that d

309
00:18:36,410 --> 00:18:37,610
it's not zero

310
00:18:37,660 --> 00:18:40,430
but d divided by lambda

311
00:18:40,450 --> 00:18:44,410
so small maybe one twenty years of one thirty is of the the wavelength of

312
00:18:45,800 --> 00:18:48,030
it's all right

313
00:18:48,070 --> 00:18:49,530
and as this

314
00:18:50,410 --> 00:18:52,240
well suited doing

315
00:18:52,260 --> 00:18:54,340
the plant will show you can

316
00:18:54,380 --> 00:18:58,880
go through these phases of colors and then gets thinner and thinner and thinner

317
00:18:58,910 --> 00:19:02,970
and then it turns black

318
00:19:02,990 --> 00:19:05,930
i will show it to you upside down

319
00:19:05,970 --> 00:19:09,360
don't get too has to do with the way the project

320
00:19:09,360 --> 00:19:10,360
OK so

321
00:19:12,670 --> 00:19:14,660
in the demo that i just did

322
00:19:14,700 --> 00:19:17,960
whole is happening under the hood

323
00:19:18,570 --> 00:19:24,000
because now there a sort of a joint inference process to be done why is

324
00:19:24,010 --> 00:19:24,950
that well

325
00:19:25,000 --> 00:19:26,810
it's because

326
00:19:26,880 --> 00:19:33,800
we no longer have anything quite as rich as the try

327
00:19:33,810 --> 00:19:38,640
so we have a by map if you like the what the user did said

328
00:19:38,660 --> 00:19:43,350
there are some areas of the image with a very definitely background

329
00:19:43,840 --> 00:19:49,190
covers our

330
00:20:23,120 --> 00:20:24,350
go on

331
00:20:29,830 --> 00:20:32,910
so when you when you try rectangle around an object

332
00:20:32,930 --> 00:20:35,850
you've been explicit about what is in the background

333
00:20:35,920 --> 00:20:37,300
michael was of maps

334
00:20:38,010 --> 00:20:41,230
inside the square or you rectangle you said is

335
00:20:41,240 --> 00:20:44,330
there's a mixture of foreground star not programs

336
00:20:44,350 --> 00:20:45,770
so initially

337
00:20:45,780 --> 00:20:50,480
you have some evidence of training data to learn the background colour distribution but you

338
00:20:50,480 --> 00:20:50,830
have no

339
00:20:54,860 --> 00:20:56,850
data for learning the foreground

340
00:20:57,090 --> 00:20:58,460
four and mask

341
00:20:59,890 --> 00:21:01,110
so actually

342
00:21:01,120 --> 00:21:02,570
the now

343
00:21:02,580 --> 00:21:07,320
the whole inference process becomes an expectation maximisation process

344
00:21:08,220 --> 00:21:10,130
the maximisation is

345
00:21:10,160 --> 00:21:15,440
one round of optimisation of the trying that problem he like you know if you

346
00:21:16,490 --> 00:21:20,050
what the colour distributions were the foreground and background you could do a single shot

347
00:21:20,080 --> 00:21:22,860
of optimisation as we did before we the trial

348
00:21:22,900 --> 00:21:28,360
and get your inference but because we don't unambiguously no foreground and is

349
00:21:28,380 --> 00:21:30,250
the inference of the foreground

350
00:21:30,630 --> 00:21:34,590
the colour has to alternate with inference of the mask

351
00:21:34,730 --> 00:21:36,470
separate foreground background

352
00:21:36,490 --> 00:21:39,560
you know maximization step is the

353
00:21:39,570 --> 00:21:44,660
that we can perform the expectations that was that the re estimates the colour mixture

354
00:21:44,660 --> 00:21:46,140
components for the poor

355
00:21:46,180 --> 00:21:49,440
and you see this happening before your very eyes here

356
00:21:49,490 --> 00:21:56,480
with you initially in response that rectangle rather messy separation between foreground and background but

357
00:21:56,480 --> 00:21:58,360
iterating through

358
00:21:58,410 --> 00:22:03,820
sort of stable estimate the foreground and background colours

359
00:22:03,830 --> 00:22:07,790
and so there are lots of examples of you know cases where it does work

360
00:22:07,790 --> 00:22:09,620
in cases where it doesn't work

361
00:22:09,670 --> 00:22:12,130
these are nice separable cases

362
00:22:12,340 --> 00:22:15,840
you know it's not surprising that the

363
00:22:15,870 --> 00:22:19,030
the segmentation comes at night he

364
00:22:25,320 --> 00:22:28,830
here is here you know more difficult cases where

365
00:22:28,880 --> 00:22:31,830
you know this is like the newsreader effect

366
00:22:31,840 --> 00:22:32,610
i mean

367
00:22:32,800 --> 00:22:34,620
say the the

368
00:22:34,770 --> 00:22:37,640
and got these various bits of the background

369
00:22:37,650 --> 00:22:41,300
you know the computer county telepathy as far as he's concerned

370
00:22:41,400 --> 00:22:46,150
do you have been with in that means that we didn't tell the computer anything

371
00:22:46,150 --> 00:22:51,030
about that not being the case that was not unreasonable thing around assuming

372
00:22:51,050 --> 00:22:53,430
and so that's what it

373
00:22:53,450 --> 00:22:58,120
you you can't expect miracles and system similar to the three here

374
00:22:58,170 --> 00:23:00,070
out of all of this

375
00:23:00,710 --> 00:23:04,500
transparent stuff going on you know really we would have needed to solve

376
00:23:04,510 --> 00:23:05,660
the original problem

377
00:23:05,670 --> 00:23:09,390
to make much headway with this this one is always going to be difficult

378
00:23:09,400 --> 00:23:10,800
the binary

379
00:23:10,810 --> 00:23:12,590
the segmentation problem

380
00:23:12,600 --> 00:23:15,900
not everything that we can do

381
00:23:15,910 --> 00:23:18,040
this can be fixed

382
00:23:18,090 --> 00:23:19,960
there are low cost

383
00:23:20,000 --> 00:23:24,100
by going in as a user has an oracle and saying they you

384
00:23:24,120 --> 00:23:26,490
let me show you some of the things we got wrong

385
00:23:26,700 --> 00:23:28,850
you've got this one wrong for example

386
00:23:28,870 --> 00:23:31,530
and so you're going to use and

387
00:23:31,690 --> 00:23:33,000
that one

388
00:23:34,560 --> 00:23:37,020
OK the

389
00:23:46,340 --> 00:23:52,480
this one

390
00:23:52,490 --> 00:23:54,970
here is another one

391
00:23:55,020 --> 00:23:58,190
which i have had a

392
00:23:58,240 --> 00:24:00,470
and then try

393
00:24:00,660 --> 00:24:03,170
a croquet player

394
00:24:03,220 --> 00:24:06,970
or rather a like for you

395
00:24:07,020 --> 00:24:09,800
after the picture you but i

396
00:24:09,810 --> 00:24:12,870
the rectangle around there you have it

397
00:24:12,880 --> 00:24:15,140
well first

398
00:24:15,190 --> 00:24:18,180
it's got hole in that

399
00:24:18,190 --> 00:24:20,040
well yes

400
00:24:24,050 --> 00:24:26,370
we have

401
00:24:27,770 --> 00:24:33,400
so as a user i can go in and put in actually what i'm doing

402
00:24:33,400 --> 00:24:35,760
is putting in some hard constraints

403
00:24:35,800 --> 00:24:37,420
and the hard constraints

404
00:24:37,430 --> 00:24:42,390
get absorbed into the you know the optimisation is redone with the hard constraint actually

405
00:24:43,870 --> 00:24:44,980
explain how

406
00:24:44,990 --> 00:24:48,940
we make a small change like this the processing of the problem the optimisation can

407
00:24:48,940 --> 00:24:51,880
be done incredibly efficient you don't have do nearly as much work

408
00:24:51,900 --> 00:24:55,420
as you did it first time round and not just because of the

409
00:24:55,430 --> 00:24:59,480
iterative nature of expectation maximisation amount of work done

410
00:24:59,490 --> 00:25:02,940
when i make an edit is even less than one iteration

411
00:25:03,010 --> 00:25:05,860
you know even less than one inference that

412
00:25:05,910 --> 00:25:07,030
because of the

413
00:25:07,120 --> 00:25:09,250
clever thing going on in the algorithm

414
00:25:09,260 --> 00:25:13,010
and the issues well we probably want to

415
00:25:13,060 --> 00:25:13,860
we are

416
00:25:13,910 --> 00:25:16,210
might have got that shoot the price of issue

417
00:25:16,320 --> 00:25:18,270
but we do

418
00:25:18,320 --> 00:25:20,080
what to happen

419
00:25:20,130 --> 00:25:22,310
i quite like that what you think

420
00:25:22,330 --> 00:25:24,720
we could go on refining for example

421
00:25:26,310 --> 00:25:30,260
you might be able to tell me what's going on here the interesting

422
00:25:34,250 --> 00:25:38,970
actually sort of cutting corners in

423
00:25:39,020 --> 00:25:41,640
why do you think that might be happening

424
00:25:41,650 --> 00:25:44,730
what is known about the model that we have had

425
00:25:44,810 --> 00:25:50,700
that you know you would lead users yet i i knew that would happen

426
00:25:50,740 --> 00:25:52,170
the idea

427
00:25:52,210 --> 00:25:55,430
while you're thinking about it

428
00:25:55,440 --> 00:26:01,440
any other ideas

429
00:26:01,450 --> 00:26:05,560
the email by the way one

430
00:26:05,610 --> 00:26:08,090
now it is

431
00:26:08,140 --> 00:26:11,480
is it because need do

432
00:26:12,110 --> 00:26:16,070
somebody can tell me what it was like why does the

433
00:26:16,090 --> 00:26:22,600
interpreted conformal like to call it again down here it is not is not free

434
00:26:22,620 --> 00:26:28,250
that does have a bit of tendency to do

435
00:26:28,460 --> 00:26:33,850
well you have to take it homework in the county

436
00:26:33,850 --> 00:26:36,020
every search

437
00:26:38,900 --> 00:26:41,330
and element skip list

438
00:26:43,520 --> 00:26:45,790
cos or log in

439
00:26:46,590 --> 00:26:53,150
that's it there

440
00:26:53,150 --> 00:26:55,650
to define with high probability

441
00:26:55,710 --> 00:26:59,770
so with high probability

442
00:27:01,060 --> 00:27:07,230
and so we have a long phrase so

443
00:27:07,250 --> 00:27:11,210
then we will and you can abbreviated w h p

444
00:27:17,440 --> 00:27:21,270
if i had a random event and the random here is that every search

445
00:27:21,270 --> 00:27:23,920
in an element skip list costs log n

446
00:27:23,940 --> 00:27:27,130
i want to know what it means for the that events to occur with high

447
00:27:28,610 --> 00:27:42,060
this definition

448
00:27:42,210 --> 00:27:51,080
so the statement is if for any alpha

449
00:27:51,080 --> 00:27:53,270
greater than or equal to one

450
00:27:53,290 --> 00:27:56,600
there's a suitable choice of constant

451
00:28:07,190 --> 00:28:10,310
for which

452
00:28:10,310 --> 00:28:11,900
the event e

453
00:28:13,040 --> 00:28:15,330
with this probability actually mentioning

454
00:28:15,460 --> 00:28:26,420
the probability of least one minus

455
00:28:27,810 --> 00:28:30,100
one over and so the alpha

456
00:28:30,370 --> 00:28:39,790
so this is a

457
00:28:39,810 --> 00:28:44,330
a bit imprecise but it will suffice for our purposes you want the really formal

458
00:28:44,330 --> 00:28:46,480
definition you can read the lecture notes

459
00:28:46,520 --> 00:28:48,400
the special election of for this

460
00:28:49,170 --> 00:28:50,350
lecture on the

461
00:28:50,350 --> 00:28:52,150
so side

462
00:28:52,210 --> 00:28:54,980
is the powerpoint notes on some site

463
00:28:56,110 --> 00:29:00,060
but this this is right there's a bit of subtlety and the choice of constance

464
00:29:00,830 --> 00:29:03,500
there's the choice of this constant

465
00:29:03,520 --> 00:29:05,670
and this the trace of this concept

466
00:29:05,690 --> 00:29:07,480
these are related

467
00:29:07,500 --> 00:29:08,210
and this

468
00:29:08,630 --> 00:29:12,690
alpha which we get to say whatever we want but the bottom line is

469
00:29:12,690 --> 00:29:16,600
we get to choose what probability we want this to be true

470
00:29:16,650 --> 00:29:20,560
i wanted to be true with probability one minus one over into the hundreds

471
00:29:20,560 --> 00:29:22,400
i can do that

472
00:29:22,420 --> 00:29:24,060
i just said

473
00:29:24,440 --> 00:29:26,250
alpha two hundred

474
00:29:26,270 --> 00:29:30,020
and you know after to this will cost that's going to grow much slower

475
00:29:30,040 --> 00:29:31,480
and and to the alpha

476
00:29:31,520 --> 00:29:35,150
i get this i get the error probability so this thing is called the error

477
00:29:35,150 --> 00:29:38,480
probability the probability that i fail

478
00:29:38,610 --> 00:29:43,130
is polynomially small

479
00:29:43,150 --> 00:29:45,100
any polynomial i want

480
00:29:46,580 --> 00:29:49,580
is that with the same data structure to fix the data structure doesn't depend on

481
00:29:50,630 --> 00:29:54,230
anything you want any alpha value want

482
00:29:54,250 --> 00:29:55,630
this data structure

483
00:29:55,650 --> 00:29:59,880
will take water log n time now this concert will depend on alpha

484
00:29:59,920 --> 00:30:01,110
so you know you want

485
00:30:01,170 --> 00:30:04,130
error probably one over and one hundred is probably going to be like a hundred

486
00:30:04,980 --> 00:30:07,710
there is still stillorgan

487
00:30:07,730 --> 00:30:09,560
this is a very strong

488
00:30:09,580 --> 00:30:15,100
claim about the tail of the distribution of the running time the search

489
00:30:15,150 --> 00:30:16,710
very strong

490
00:30:16,730 --> 00:30:19,080
let me give you an idea how strong it is

491
00:30:24,330 --> 00:30:25,920
how many people know

492
00:30:26,040 --> 00:30:29,870
what fools inequality is

493
00:30:29,880 --> 00:30:34,750
how many people know what the union bound in probability

494
00:30:39,000 --> 00:30:42,250
in appendix c

495
00:30:42,310 --> 00:30:44,810
maybe you know it by

496
00:30:44,810 --> 00:30:51,190
there is going to know it by name so the linearity of expectation lot easier

497
00:30:51,190 --> 00:30:52,600
to it to someone

498
00:30:52,610 --> 00:30:58,730
linearity of expectation saying you know nothing more you sum up all the expectations of

499
00:30:58,730 --> 00:31:01,630
things and that the expectation the some

500
00:31:01,710 --> 00:31:05,400
what is are the same linearity of expectation

501
00:31:06,310 --> 00:31:09,210
let me crazy when different

502
00:31:11,290 --> 00:31:14,380
so if i take a bunch of events and i take the union this sort

503
00:31:14,900 --> 00:31:18,520
either this happens or this happens or so on so this is the

504
00:31:18,540 --> 00:31:20,810
inclusive or k events

505
00:31:20,830 --> 00:31:26,110
and instead of looking at the probability the sum of the probabilities of those

506
00:31:26,130 --> 00:31:32,850
easy question are these equal

507
00:31:33,630 --> 00:31:35,130
unless there independent

508
00:31:35,150 --> 00:31:38,380
what can i say anything about them

509
00:31:38,480 --> 00:31:40,420
any relation

510
00:31:44,350 --> 00:31:46,420
is less or equal to that

511
00:31:46,440 --> 00:31:47,790
there should be intuitive to you

512
00:31:47,810 --> 00:31:49,380
probability five here

513
00:31:49,380 --> 00:31:50,880
and look at the textbook

514
00:31:50,940 --> 00:31:52,630
very basic result

515
00:31:52,650 --> 00:31:56,060
trivial result from almost

516
00:31:56,060 --> 00:32:00,330
what does this tell us all suppose that he i

517
00:32:00,400 --> 00:32:04,080
is some kind of error that we don't want to happen

518
00:32:04,120 --> 00:32:06,100
and suppose

519
00:32:06,130 --> 00:32:10,560
so if you make some letters here suppose i have a bunch of events which

520
00:32:10,560 --> 00:32:13,270
occur with high probability

521
00:32:14,170 --> 00:32:18,480
call those by complement so suppose

522
00:32:19,350 --> 00:32:21,670
so this is the end of that segment the

523
00:32:24,600 --> 00:32:26,540
with high probability

524
00:32:26,670 --> 00:32:32,270
so then the probability of e i is very small

525
00:32:32,290 --> 00:32:37,040
polynomially small water and the alpha for any of fireworks

526
00:32:37,060 --> 00:32:40,370
now suppose i take a whole bunch of these events

527
00:32:40,380 --> 00:32:45,040
and so that k is polynomial in

528
00:32:45,100 --> 00:32:48,210
so i take a bunch of events which i'd like to happen they all occur

529
00:32:48,210 --> 00:32:52,020
with high probability is only polynomially many of them so let's say

530
00:32:52,080 --> 00:32:55,310
let me give this constant name let's call it c

531
00:32:55,330 --> 00:32:57,940
so take into the sea such events

532
00:32:57,980 --> 00:33:03,290
what's the probability the probability that all of those events occur together because they should

533
00:33:03,330 --> 00:33:05,810
most of the time occur together

534
00:33:05,830 --> 00:33:10,920
because they're each one occurs most of the time occurs with high probability

535
00:33:10,940 --> 00:33:15,040
so i want to look at one bar intersect

536
00:33:15,130 --> 00:33:16,790
each bar

537
00:33:16,790 --> 00:33:17,900
and so on

538
00:33:17,920 --> 00:33:23,440
each of these occurrences high probability was the chance that they all occurred

539
00:33:23,440 --> 00:33:25,790
also with high probability

540
00:33:25,790 --> 00:33:27,290
so when there a lot of

541
00:33:27,300 --> 00:33:29,090
theoretical calculations

542
00:33:29,210 --> 00:33:34,840
which are dealing with that and experiment also

543
00:33:35,750 --> 00:33:40,850
all the experiments are related to ternary fission so you don't have to fragments but

544
00:33:40,850 --> 00:33:46,250
you can handle so large light charged particles which is emitted also

545
00:33:46,750 --> 00:33:49,190
and it's quite rare process

546
00:33:49,200 --> 00:33:52,940
but they're not experiment expecially i ll

547
00:33:53,610 --> 00:33:56,090
really understand this make any sense

548
00:33:56,090 --> 00:34:01,320
what is looked also is sufficient fish for production of super heavy elements

549
00:34:01,640 --> 00:34:06,970
everything which is related to neutron emission because it's very important for all the

550
00:34:06,970 --> 00:34:08,640
applications so

551
00:34:08,660 --> 00:34:14,090
what measurement suffolk returned from ten delayed neutrons

552
00:34:14,110 --> 00:34:18,950
so now theoretical description so what are the progress has been made

553
00:34:18,970 --> 00:34:20,130
so first

554
00:34:20,160 --> 00:34:23,410
i will tell you that it's a real challenge to

555
00:34:23,430 --> 00:34:28,800
to describe the fission process so first because it appears

556
00:34:28,810 --> 00:34:32,540
as a large amplitude motion so what does it mean it means that you go

557
00:34:33,870 --> 00:34:40,060
deformed ground state to very very elongated shapes so you have to deal with shapes

558
00:34:40,060 --> 00:34:42,810
which are very far from equilibrium

559
00:34:42,820 --> 00:34:45,810
so that's something which is difficult to

560
00:34:47,180 --> 00:34:49,870
two constraint

561
00:34:49,930 --> 00:34:55,470
also you have a very big role played by the shell effects and i can

562
00:34:55,470 --> 00:35:01,840
explain that demonstrate that by two examples of the first example is quite old ones

563
00:35:01,850 --> 00:35:05,740
so you have sufficiently of the fragments mass distribution

564
00:35:05,790 --> 00:35:08,140
depending on the mass murder

565
00:35:08,170 --> 00:35:09,190
so you

566
00:35:09,200 --> 00:35:11,830
please look at this one first of

567
00:35:11,840 --> 00:35:13,370
and just neutrons

568
00:35:13,380 --> 00:35:14,610
so very

569
00:35:14,620 --> 00:35:17,090
low energetic neutrons

570
00:35:17,110 --> 00:35:21,290
so that means that the most probable fragmentation is around one hundred thirty five

571
00:35:21,990 --> 00:35:24,200
the complementary fragments here

572
00:35:24,210 --> 00:35:28,570
and you see that when you increase the energy of the projectile so then you

573
00:35:29,500 --> 00:35:31,330
something like that so it's

574
00:35:31,340 --> 00:35:37,570
very probable to have some fragments was two masses around one hundred twenty

575
00:35:37,580 --> 00:35:42,330
so you just increase the energy of the neutron and then it really changed

576
00:35:42,340 --> 00:35:46,340
this feature a mass distribution

577
00:35:46,350 --> 00:35:52,300
another striking feature is related to these fission fragment mass and charge distributions

578
00:35:52,320 --> 00:35:55,790
so this has been obtained energy side you have this

579
00:35:55,980 --> 00:36:01,380
charles distribution for different fissioning system different act and i put that in your dialect

580
00:36:01,630 --> 00:36:02,940
can you

581
00:36:02,950 --> 00:36:06,700
and you see that for example you go from here to here

582
00:36:06,720 --> 00:36:08,810
you change only one prototype

583
00:36:08,810 --> 00:36:14,830
magenta and threatened then completely changed all the features of this charge distribution so it's

584
00:36:14,830 --> 00:36:16,480
one proton among

585
00:36:16,540 --> 00:36:19,990
around two hundred and forty details

586
00:36:20,000 --> 00:36:21,060
on the one

587
00:36:21,080 --> 00:36:25,460
it's same if you go from here to here nugent one neutron of the fissioning

588
00:36:25,460 --> 00:36:30,010
system and you change all the features in one case you have two fragments with

589
00:36:30,010 --> 00:36:31,040
the same mass

590
00:36:31,060 --> 00:36:33,390
in the other case one big and one small

591
00:36:33,390 --> 00:36:40,700
so that's really very important shell effects that has to be reproduced

592
00:36:40,720 --> 00:36:45,000
all the problem is that we have to deal with different deformation at the same

593
00:36:45,000 --> 00:36:50,250
time and we don't know which one of the gene for the patient process

594
00:36:50,260 --> 00:36:51,390
and finally

595
00:36:51,400 --> 00:36:58,900
as usual we have these interaction between entrance ticket citation where you have the citation

596
00:36:58,900 --> 00:37:00,540
of the human clones

597
00:37:00,570 --> 00:37:03,950
it was collective motion of that's all these vibrations

598
00:37:03,960 --> 00:37:08,690
so that's because of all this reason that it's very difficult and challenging

599
00:37:08,710 --> 00:37:12,510
for theory

600
00:37:13,170 --> 00:37:17,190
that's why a lot of studies now undertaken two

601
00:37:17,220 --> 00:37:21,020
make progress on these related to this equations

602
00:37:21,040 --> 00:37:27,220
so i insist that because it's quite all process so why are we doing so

603
00:37:27,230 --> 00:37:29,470
fission studies

604
00:37:30,060 --> 00:37:36,120
of course of this fundamental questions but also because of all these new experimental results

605
00:37:38,510 --> 00:37:43,500
i geo site so there is a lot of experiments new experiment where now you

606
00:37:43,500 --> 00:37:46,760
have reasons for exotic nuclei efficient

607
00:37:46,770 --> 00:37:48,440
offer super heavy

608
00:37:48,440 --> 00:37:53,170
efficient so far different an engineer and so we no more restricted to the well

609
00:37:53,170 --> 00:37:57,880
known uranium two hundred thirty six five and eight

610
00:37:57,900 --> 00:38:02,330
and also because a lot of progress has been made these last decades

611
00:38:02,360 --> 00:38:04,250
for description of

612
00:38:04,270 --> 00:38:08,150
description sorry if structure so now we want to

613
00:38:08,190 --> 00:38:12,000
to make the same progress for nuclear reaction to try to

614
00:38:12,020 --> 00:38:15,660
they get data of all progress from the class structure

615
00:38:15,670 --> 00:38:16,980
to put that in

616
00:38:18,050 --> 00:38:21,180
time dependent mechanism

617
00:38:21,190 --> 00:38:27,490
and it's also because of his new super calculators to flow so now we really

618
00:38:27,520 --> 00:38:30,120
i can think of

619
00:38:30,140 --> 00:38:31,400
new approaches

620
00:38:32,370 --> 00:38:38,630
time consuming for CPU but that are now possible so that's really the reason why

621
00:38:38,650 --> 00:38:39,810
something new

622
00:38:39,830 --> 00:38:46,310
will be done concerning is now done will be done in the future concerning all

623
00:38:49,160 --> 00:38:53,040
so the goal of all these studies is to happen

624
00:38:53,060 --> 00:38:56,130
something which is as i mean this was possible

625
00:38:56,130 --> 00:39:01,690
so we don't want to introduce parameters that we cannot just we want first c

626
00:39:02,110 --> 00:39:03,860
what we obtain ways

627
00:39:03,870 --> 00:39:08,390
the most ab initio calculation that can do

628
00:39:08,430 --> 00:39:12,720
but also we want something that can be applicable to the different aspects of the

629
00:39:13,860 --> 00:39:18,710
so we want we want to be able to describe the structure of the fissioning

630
00:39:18,710 --> 00:39:21,210
system so these big actinides

631
00:39:21,230 --> 00:39:23,760
the structure of the fragments

632
00:39:23,770 --> 00:39:26,360
and the dynamical evolution in between

633
00:39:26,370 --> 00:39:31,120
so we should be able to have something very clearly around for all these different

634
00:39:33,020 --> 00:39:36,430
and then we want to have a quantitative results

635
00:39:36,440 --> 00:39:40,320
the challenge at with experimental data

636
00:39:40,350 --> 00:39:45,450
so that's why we have developed an approach which is career for all these properties

637
00:39:45,660 --> 00:39:49,240
quantum mechanical microscopic time dependent

638
00:39:49,270 --> 00:39:55,310
to describe also the structure and dynamics and the dynamics of the process

639
00:39:56,990 --> 00:39:58,200
the question so

640
00:39:58,200 --> 00:40:01,960
what are the progress that we have made so what was missing in this description

641
00:40:02,960 --> 00:40:06,230
so far what was missing was a real

642
00:40:06,270 --> 00:40:08,280
good description fully

643
00:40:08,280 --> 00:40:11,970
friends and you know they they can then somebody to can actually capture that in

644
00:40:11,970 --> 00:40:13,360
dodginess that

645
00:40:13,400 --> 00:40:15,340
in some kind of simple

646
00:40:15,360 --> 00:40:18,450
algorithmic way that allows nodes to create

647
00:40:18,460 --> 00:40:23,590
these networks artificially give some understanding of how we can do this not just relying

648
00:40:23,590 --> 00:40:24,730
on humans to

649
00:40:24,760 --> 00:40:26,550
to come to to to

650
00:40:26,570 --> 00:40:28,440
contribution to trust

651
00:40:28,450 --> 00:40:30,340
a priori

652
00:40:30,360 --> 00:40:33,960
i mean that could be useful for automatic peer-to-peer applications

653
00:40:35,660 --> 00:40:38,960
now i skip

654
00:40:39,020 --> 00:40:40,890
to something else

655
00:40:41,900 --> 00:40:43,340
going back to the

656
00:40:43,340 --> 00:40:46,650
this talk by another human

657
00:40:46,650 --> 00:40:53,730
he there was talking in his brief peter p section about how you can create

658
00:40:53,780 --> 00:40:59,310
effectively incentives for individuals to share things where they might not normally be incentives to

659
00:40:59,310 --> 00:41:02,860
do so and by the way did that by introducing money

660
00:41:02,900 --> 00:41:07,170
because when you introduce money that what you're doing effectively is then you have to

661
00:41:07,170 --> 00:41:11,130
assume the individuals rationally trying to maximize money

662
00:41:11,140 --> 00:41:15,200
you introduce money from outside the system need central payment system etcetera

663
00:41:15,280 --> 00:41:17,220
there are other ways of doing

664
00:41:17,240 --> 00:41:20,990
one of them is like by creating these changes will talk about it because the

665
00:41:20,990 --> 00:41:22,030
recent thing

666
00:41:22,040 --> 00:41:28,890
which is deployed peer-to-peer client which is based on bit torrent called tribal

667
00:41:28,900 --> 00:41:35,990
and this user's bandwidth this currency as a way of creating the kinds of incentives

668
00:41:35,990 --> 00:41:42,540
so you don't need necessary to import money into the system and and all that

669
00:41:42,540 --> 00:41:46,910
and assume rational incentives you can use other kinds of incentive mechanism in the basic

670
00:41:46,910 --> 00:41:52,020
idea is it's a bit like this kind of thing but instead of dollars you're

671
00:41:52,020 --> 00:41:57,360
talking about bandwidth that you're prepared to lend to a friend and then you would

672
00:41:57,360 --> 00:42:00,360
expect that to be reciprocated later

673
00:42:00,380 --> 00:42:05,520
so he so the idea of tribal ways you build what they called tribes which

674
00:42:05,520 --> 00:42:08,420
are essentially friendship networks

675
00:42:09,780 --> 00:42:16,100
based on persistent i persistent ID's where each node records how much bandwidth they contribute

676
00:42:16,110 --> 00:42:20,870
to their friend previously and balancing of the books of the time

677
00:42:20,990 --> 00:42:23,990
so it's a kind of

678
00:42:24,060 --> 00:42:26,880
kind of economy of bandwidth

679
00:42:26,890 --> 00:42:29,500
but currently is limited to

680
00:42:29,740 --> 00:42:32,500
disconnected tribes and again

681
00:42:32,510 --> 00:42:36,230
you have to supply you have to say this is my friend and he will

682
00:42:36,230 --> 00:42:39,980
help me download the file and i will help him so again this is why

683
00:42:40,030 --> 00:42:41,660
exhau generously done

684
00:42:44,130 --> 00:42:45,380
trying to learn

685
00:42:45,400 --> 00:42:47,690
tribal it all

686
00:42:47,700 --> 00:42:53,900
you can actually what's going on now so it bit promotion although and i've not

687
00:42:53,900 --> 00:42:57,900
been involved in this but i am and moving to the place that are involved

688
00:42:57,900 --> 00:43:02,400
in this there's is actually experiment i think still going on moment where you can

689
00:43:02,400 --> 00:43:05,650
download two different versions and if you go to the website you click and you'll

690
00:43:05,650 --> 00:43:08,940
get if it if experiment still running if you are still down to one of

691
00:43:08,940 --> 00:43:12,980
two versions but i'm going to tell you what the idea of experiments because it

692
00:43:12,980 --> 00:43:15,020
could it's your choice

693
00:43:15,120 --> 00:43:17,190
OK so

694
00:43:17,280 --> 00:43:22,170
basically what what i've been doing for a long time is looking at these new

695
00:43:22,170 --> 00:43:25,410
if you if you saw karl sigmund stock plenary talk

696
00:43:25,420 --> 00:43:30,920
he talked briefly about these new group selection models to create cooperation in evolutionary kinds

697
00:43:30,920 --> 00:43:34,360
of economic situations what i've been trying to do

698
00:43:34,400 --> 00:43:39,110
is get some of those models and get them into a network so that we

699
00:43:39,110 --> 00:43:41,560
can create these kinds of trust

700
00:43:41,630 --> 00:43:46,920
network spontaneously OK

701
00:43:46,930 --> 00:43:52,360
all these recent group selection models and i'm going to rush through mound i have

702
00:43:52,440 --> 00:43:58,720
OK so i'm going to go through quite quickly but the those references at the

703
00:43:58,720 --> 00:44:02,280
bottom you can look up if you're interested always group selection models are not really

704
00:44:02,280 --> 00:44:09,080
group selection in the traditional sense they're actually models of individual selection that create dynamically

705
00:44:09,080 --> 00:44:15,450
social structures which look like groups then constrain individual selection to produce group group level

706
00:44:15,450 --> 00:44:20,150
benefit benefits so said it did not the group itself is being reproduced of selected

707
00:44:20,150 --> 00:44:25,000
is that the individuals have been reproduced cited to create groups that maybe it's silly

708
00:44:25,010 --> 00:44:26,110
distinction ball

709
00:44:26,120 --> 00:44:29,650
biologists can get very upset when you talk about group selection if you don't mention

710
00:44:30,640 --> 00:44:35,990
OK the point about these models as they don't require reciprocity in this tit-for-tat

711
00:44:36,620 --> 00:44:40,200
well they could play social structures the kind of reciprocity

712
00:44:40,230 --> 00:44:43,230
i'll skip

713
00:44:43,250 --> 00:44:49,240
there was a whole being a whole set of models based on this tag concept

714
00:44:49,310 --> 00:44:58,880
by tag domain folksonomies can mean something totally different and tag models essentially create arbitrary

715
00:45:00,110 --> 00:45:02,040
which evolve over time

716
00:45:02,060 --> 00:45:07,480
eliminating free riders because groups have less free riders in them

717
00:45:07,500 --> 00:45:12,030
tend to contract and groups have more good guys in intend to expand so what

718
00:45:12,030 --> 00:45:16,990
you end up with this kind of dynamic ecology is groups in which individuals within

719
00:45:16,990 --> 00:45:22,780
the predisposed to be incentivized if you like to be good guys too

720
00:45:22,800 --> 00:45:28,970
to not to pay the debts owed to share bandwidth whatever application more recently there

721
00:45:28,970 --> 00:45:34,520
has been some some work presented here by by those that there's been a network

722
00:45:34,520 --> 00:45:36,160
rewiring approach

723
00:45:36,170 --> 00:45:38,820
which is where you take note and effectively

724
00:45:38,850 --> 00:45:40,480
really to put it

725
00:45:40,490 --> 00:45:44,920
in its simplest form nodes move from the neighborhood doing they drop links if they're

726
00:45:44,920 --> 00:45:46,300
not happy

727
00:45:46,310 --> 00:45:51,430
OK doing some interaction with their neighbours if they're not happy they dropped links and

728
00:45:51,430 --> 00:45:53,390
they moved to some of the random

729
00:45:53,400 --> 00:45:58,740
region of the network so kind of simple learning process what you find there is

730
00:45:58,900 --> 00:46:02,430
you get these groups of good guys forming bad guys

731
00:46:02,490 --> 00:46:06,670
dissolve that group came under certain conditions right

732
00:46:06,690 --> 00:46:09,740
the the more recent and different models

733
00:46:09,760 --> 00:46:15,810
like group merging well group splitting models where when group SSN inside it splits

734
00:46:15,870 --> 00:46:20,040
just another model so

735
00:46:20,100 --> 00:46:25,230
we designed this with called slack which is it can be proposed this recipe to

736
00:46:25,230 --> 00:46:29,830
be protocol but you can see as a simple mechanism that captures how you might

737
00:46:29,830 --> 00:46:33,050
create these chains

738
00:46:33,060 --> 00:46:35,380
that's basically algorithm

739
00:46:35,420 --> 00:46:42,510
right so the basic idea here is that you each node periodically is performing some

740
00:46:42,520 --> 00:46:46,850
activity which generates some utility this isn't not discussed it could be anything i'll show

741
00:46:46,850 --> 00:46:50,010
you what we go by the prisoner's dilemma which is a classic going to test

742
00:46:50,010 --> 00:46:54,060
cooperation but periodically nodes so random peer

743
00:46:54,120 --> 00:46:58,010
if they can if they see that p has a high utility and then find

744
00:46:58,650 --> 00:47:01,540
they dropped the current links they linked up here

745
00:47:01,550 --> 00:47:06,740
and they copy its strategy it's behaviour is this is an evolutionary approach OK

746
00:47:06,780 --> 00:47:09,950
now if you do algarve

747
00:47:10,010 --> 00:47:13,240
go through all this if you do this

748
00:47:13,250 --> 00:47:19,570
then in this case nodes are periodically playing the prisoner's dilemma with neighbours so that

749
00:47:19,570 --> 00:47:23,620
would in in a quite a way to defection in the prisoner's dilemma would be

750
00:47:23,620 --> 00:47:25,860
a bit like not paying your IOU's

751
00:47:25,900 --> 00:47:28,260
the similar kind of thing

752
00:47:28,260 --> 00:47:35,040
so the plan this hours to go back to static models dynamic models

753
00:47:35,060 --> 00:47:37,430
and is that the model

754
00:47:37,490 --> 00:47:41,940
there's a technique that works a lot better than

755
00:47:42,010 --> 00:47:45,240
the importance sampling called markov chain monte carlo

756
00:47:45,260 --> 00:47:46,120
and that's

757
00:47:46,160 --> 00:47:49,480
what began to lag

758
00:47:49,550 --> 00:47:51,140
by now

759
00:47:52,960 --> 00:48:01,150
the closest of manifestation of MCMC to you life google

760
00:48:01,160 --> 00:48:05,880
google is actually an example of a valid markov chain of

761
00:48:05,900 --> 00:48:10,200
a markov chain kernel that is used to find

762
00:48:10,220 --> 00:48:12,260
the probability vector over

763
00:48:12,270 --> 00:48:14,190
everyone every web page

764
00:48:14,230 --> 00:48:19,410
and how probable that thing is indicates how important

765
00:48:19,440 --> 00:48:22,860
or how relevant the web pages

766
00:48:23,180 --> 00:48:27,000
its surrounding integration you also have this

767
00:48:27,050 --> 00:48:28,380
matrix of

768
00:48:28,400 --> 00:48:32,530
essentially the web just you to think about this this agency matrix that says this

769
00:48:32,530 --> 00:48:35,260
user points these other users and so on

770
00:48:35,310 --> 00:48:38,390
and so the grids like kernel probabilistic

771
00:48:39,970 --> 00:48:41,720
and so are the ones of

772
00:48:41,760 --> 00:48:43,220
short fingers

773
00:48:43,230 --> 00:48:45,120
this running of kernels

774
00:48:45,550 --> 00:48:48,140
the first i can facts

775
00:48:48,200 --> 00:48:52,200
of google gives you the pagerank which is importance of each web page

776
00:48:52,280 --> 00:48:56,780
the second i can factor would tell you where the bottlenecks are on the web

777
00:48:56,780 --> 00:49:01,330
of information and this one with allied to split the web to cluster

778
00:49:01,370 --> 00:49:03,090
if you wanted to

779
00:49:03,090 --> 00:49:06,030
and if you look at successive i can values you would be able to actually

780
00:49:06,030 --> 00:49:09,890
find clusters on the web find some communities

781
00:49:09,900 --> 00:49:18,340
likewise schrodinger's equation of surprised the first eigen value i can function is all possible

782
00:49:18,340 --> 00:49:20,220
to its absolute value of the weight

783
00:49:20,220 --> 00:49:23,470
an electron is the absolute value of the weight of probability

784
00:49:23,470 --> 00:49:24,390
that's the model

785
00:49:24,420 --> 00:49:25,700
when the like

786
00:49:25,700 --> 00:49:30,280
and so the second one the second standing wave will have a negative

787
00:49:31,470 --> 00:49:34,060
so it's like standing waves space

788
00:49:34,120 --> 00:49:36,620
and what to compete standing waves

789
00:49:36,620 --> 00:49:40,960
and that actually complicates things because the moment this particles with the negative weights negative

790
00:49:40,960 --> 00:49:44,620
weight control positive weight you have be very careful in doing this

791
00:49:44,630 --> 00:49:47,870
physicist about a lot of effort to solve the problem

792
00:49:47,900 --> 00:49:50,190
it's called the sign problem in quantum

793
00:49:50,210 --> 00:49:52,400
one of the columns quantum physics

794
00:49:52,440 --> 00:49:56,910
if you can solve that that's also good phd

795
00:49:56,930 --> 00:49:59,720
possibly in world

796
00:49:59,720 --> 00:50:03,030
OK so

797
00:50:03,040 --> 00:50:07,530
here's the quick i'm going to give you a quick tour of markov chains then

798
00:50:07,530 --> 00:50:11,310
i'm going to introduce to you the most important algorithm which is the metropolis hastings

799
00:50:11,310 --> 00:50:16,090
algorithm every other method that i know the MCMC can be derived

800
00:50:16,120 --> 00:50:17,850
from metropolis hastings

801
00:50:17,910 --> 00:50:20,370
if you understand metropolis hastings here that

802
00:50:20,400 --> 00:50:21,630
the rest is just

803
00:50:21,720 --> 00:50:23,310
variational bayes

804
00:50:23,310 --> 00:50:26,240
including gibbs sampling government alone

805
00:50:26,250 --> 00:50:28,660
it separates

806
00:50:28,900 --> 00:50:33,520
these methods are very good

807
00:50:33,530 --> 00:50:37,090
markov chain monte carlo methods are very good especially when you have

808
00:50:37,120 --> 00:50:39,380
these graphical models

809
00:50:40,090 --> 00:50:45,130
where you can decompose in order to this divide and conquer

810
00:50:45,280 --> 00:50:47,720
this is a divide-and-conquer strategy

811
00:50:48,150 --> 00:50:52,280
there are also the method of choice when you have no idea how to design

812
00:50:52,280 --> 00:50:55,740
and any other algorithm to solve the problem

813
00:50:55,750 --> 00:51:00,120
these methods are except that summer how nasty your problem is throwing this method that

814
00:51:00,120 --> 00:51:01,520
your problem

815
00:51:01,530 --> 00:51:03,820
it is usually easy

816
00:51:04,020 --> 00:51:09,220
whereas if you try to come up with the variational approximations on that involves many

817
00:51:09,220 --> 00:51:11,750
pages of for example several papers

818
00:51:11,790 --> 00:51:14,650
and makes it hard for this features

819
00:51:14,690 --> 00:51:18,630
very easy to get off the ground doesn't mean that doing that extra workers not

820
00:51:18,630 --> 00:51:22,500
course might be to come up with approximation the work better than this

821
00:51:24,370 --> 00:51:26,840
but this is sort of a cheap way

822
00:51:26,840 --> 00:51:30,900
o doing something the methods of three generic

823
00:51:33,180 --> 00:51:35,910
at the same time the very computationally intensive

824
00:51:36,000 --> 00:51:37,490
so this is the sort of thing

825
00:51:37,530 --> 00:51:40,880
you don't do this in the front end of the search engine for example you

826
00:51:40,880 --> 00:51:45,740
would not be able to be running amok

827
00:51:45,780 --> 00:51:51,490
so what markov chain is considered discrete state space it has only three possible states

828
00:51:51,540 --> 00:51:55,340
one way to think of this as think of it as web pages web page

829
00:51:55,340 --> 00:51:57,950
one web page two and page three

830
00:51:57,990 --> 00:51:59,480
web page one

831
00:51:59,490 --> 00:52:02,220
o point two web page

832
00:52:04,330 --> 00:52:08,490
one hundred percent of the time then with say a hundred links

833
00:52:09,150 --> 00:52:14,430
web page three points the web page one sixteen point six

834
00:52:14,450 --> 00:52:19,140
in reality they only have one zero two or three legs but what the google

835
00:52:19,140 --> 00:52:23,590
guys do is they have a small number and they everything by the sun so

836
00:52:23,590 --> 00:52:25,650
that in the which you have these

837
00:52:25,660 --> 00:52:27,840
probability estimates

838
00:52:27,900 --> 00:52:29,920
that's how patron

839
00:52:31,730 --> 00:52:35,580
and if you have this graph then you can construct the matrix

840
00:52:36,530 --> 00:52:42,380
if you're building a search engine you don't construct matrix the matrix on the authorities

841
00:52:42,420 --> 00:52:46,270
it will be mostly spas

842
00:52:48,600 --> 00:52:50,410
and that's pretty much what amarkov

843
00:52:51,270 --> 00:52:55,390
how markov chain gets constructing these are the elements

844
00:52:55,470 --> 00:52:59,200
now the property of the markov chain is that the current value

845
00:52:59,300 --> 00:53:00,950
of the state

846
00:53:00,960 --> 00:53:05,420
only depends on the premise that it does not depend on the history

847
00:53:05,480 --> 00:53:10,240
with particle filters by the way just in in

848
00:53:10,290 --> 00:53:11,840
this a remark

849
00:53:11,900 --> 00:53:16,270
they don't need to be markov they can depend on the previous history

850
00:53:16,320 --> 00:53:18,360
the assumption is not needed

851
00:53:18,410 --> 00:53:20,360
i made that assumption actually

852
00:53:20,380 --> 00:53:23,390
because traditionally made but it's not

853
00:53:28,030 --> 00:53:31,640
so here's some properties of markov chains

854
00:53:31,670 --> 00:53:34,070
so t the matrix that we had before

855
00:53:34,650 --> 00:53:36,730
i'm just going to call the matrix d

856
00:53:36,780 --> 00:53:39,040
is the precision matrix the that

857
00:53:39,050 --> 00:53:43,230
web page one point to web page two it doesn't point to itself and it

858
00:53:43,230 --> 00:53:46,580
doesn't point to a page three from that graph together

859
00:53:46,590 --> 00:53:49,290
the transition matrix t

860
00:53:49,340 --> 00:53:51,170
he some properties

861
00:53:51,180 --> 00:53:54,220
one of the make

862
00:53:54,280 --> 00:53:58,470
this concept is an important periodicity

863
00:53:58,470 --> 00:54:00,960
and irreducibility

864
00:54:00,970 --> 00:54:02,780
a periodicity

865
00:54:02,820 --> 00:54:05,910
of come to explain the next slides but they are

866
00:54:05,970 --> 00:54:07,590
but if those conditions are set

867
00:54:10,290 --> 00:54:14,280
and if you take any arbitrary vectors whose entries sums to one

868
00:54:14,330 --> 00:54:17,920
then you multiply times the matrix many times

869
00:54:17,970 --> 00:54:21,290
o times cup the times coupling thames cup of tea and you do at the

870
00:54:22,150 --> 00:54:25,400
as t tends to infinity this multiplication

871
00:54:25,470 --> 00:54:28,030
always gives you the same effect

872
00:54:28,080 --> 00:54:32,820
that's why google works google starts with an arbitrary initial effect

873
00:54:33,530 --> 00:54:35,910
one of the n one of in one of our friends

874
00:54:35,930 --> 00:54:38,090
multiplies the

875
00:54:38,170 --> 00:54:40,970
and the transition matrix

876
00:54:40,970 --> 00:54:45,740
multiplies it again and again and again success of matrix multiplication

877
00:54:46,640 --> 00:54:48,290
you end up with this

878
00:54:48,300 --> 00:54:49,470
o thing

879
00:54:49,530 --> 00:54:53,490
and that's the same thing is what's known as schrodinger's equation you apply to kernel

880
00:54:53,490 --> 00:54:57,550
in other words you multiply by the time of the month multiple so we could

881
00:54:57,550 --> 00:54:59,600
do this with the article

882
00:55:00,670 --> 00:55:05,290
only to the particle filter if the matrix is like a gazillion by against million

883
00:55:05,300 --> 00:55:09,330
entries because otherwise is much more efficient numerical techniques

884
00:55:09,380 --> 00:55:12,510
so the argument applying the shopping your case

885
00:55:12,540 --> 00:55:15,530
because the matrix is infinity pi infinity

886
00:55:15,580 --> 00:55:18,400
but for finite matrices you can do it

887
00:55:18,420 --> 00:55:20,050
i've done it myself

888
00:55:20,050 --> 00:55:23,410
so i guess we'll get started

889
00:55:23,430 --> 00:55:27,780
perhaps will begin despite recapping a bit what we did last a

890
00:55:28,380 --> 00:55:31,610
it's probably a bit tougher to be inside day like today when the weather so

891
00:55:31,620 --> 00:55:33,540
nice and sunny outside

892
00:55:33,590 --> 00:55:37,310
thanks to all you for being here

893
00:55:37,320 --> 00:55:43,750
so last time we spoke about graphical models we spoke about the basics of graphical

894
00:55:45,370 --> 00:55:49,930
we spoke about undirected directed models factor graphs

895
00:55:49,940 --> 00:55:54,940
and we sort of concluded that for the purposes of computation we can from now

896
00:55:54,940 --> 00:55:58,200
on focus pretty much on on the undirected case

897
00:55:58,250 --> 00:56:01,230
so we had a graph and

898
00:56:01,280 --> 00:56:07,580
with undirected edges and we were interested in looking at factorizations of distributions

899
00:56:07,600 --> 00:56:13,050
as products of functions on cliques in this graph so to be looking at graphical

900
00:56:13,050 --> 00:56:15,860
models undirected ones that have this kind of form

901
00:56:15,910 --> 00:56:20,990
and today going be shifting more on to approximate methods for

902
00:56:21,040 --> 00:56:27,200
how to perform what's called inference how to compute things like local marginal distributions also

903
00:56:27,200 --> 00:56:30,580
things like modes or most likely configurations

904
00:56:30,620 --> 00:56:37,030
what we saw last time is the for trees things like chains and trees

905
00:56:37,070 --> 00:56:42,600
all of these computations are actually very simple there are linear time algorithms you sum

906
00:56:42,600 --> 00:56:46,240
product algorithm which we went over and also the max product algorithm which you mention

907
00:56:50,080 --> 00:56:53,470
so today we sort of want to move beyond a bit the case of histories

908
00:56:53,470 --> 00:56:56,650
and we'd like to focus on the following kinds of problems

909
00:56:56,670 --> 00:57:03,500
one is actually just computing the normalisation constant this fuzzy or log see that sits

910
00:57:03,500 --> 00:57:07,790
in front of the graphical model and makes it probability distribution

911
00:57:07,840 --> 00:57:12,820
we'll see what important it might not seem obvious but it's actually very important for

912
00:57:12,820 --> 00:57:16,790
instance in learning parameters model structure

913
00:57:16,800 --> 00:57:21,580
we spoke for a bit about this problem of computing local marginal distributions

914
00:57:21,630 --> 00:57:26,720
they will talk about that a bit more generally computing locals take statistics of some

915
00:57:28,000 --> 01:08:49,790
but at bahrain i

916
01:08:50,120 --> 01:09:10,370
some of the value and what the

917
01:09:10,380 --> 01:09:14,120
the random variable is also

918
01:09:14,180 --> 01:09:20,600
second moments

919
01:09:21,550 --> 01:09:34,230
the you have to get numbers

920
01:09:34,450 --> 01:09:48,130
so first one is you need to

921
01:09:53,390 --> 01:09:56,870
it's not

922
01:09:57,040 --> 01:09:59,550
not really

923
01:09:59,630 --> 01:10:17,860
this is the first one is

924
01:10:22,830 --> 01:10:25,070
these distribution e

925
01:10:25,430 --> 01:10:32,670
if you look at the distribution is all

926
01:10:32,750 --> 01:10:37,520
the second question

927
01:10:37,570 --> 01:10:41,870
you know if you find these things that

928
01:11:04,510 --> 01:11:09,320
as a concrete example of somebody told me that you had of one was equal

929
01:11:09,320 --> 01:11:10,390
to one

930
01:11:10,400 --> 01:11:12,980
and you had two is equal to two

931
01:11:13,030 --> 01:11:14,840
aside about equal to two

932
01:11:14,880 --> 01:11:28,450
are there any distributions satisfy those constraints

933
01:11:28,520 --> 01:11:44,590
these are questions

934
01:11:44,600 --> 01:11:49,330
what if you had of two is equal to minus two

935
01:11:49,410 --> 01:11:53,450
would that be OK or not

936
01:11:56,710 --> 01:12:06,250
right that would be OK because you're taking this function is always positive

937
01:12:06,260 --> 01:12:10,710
we know where to get the expectation of something strictly not are nonnegative to be

938
01:12:10,710 --> 01:12:11,750
minus two

939
01:12:11,760 --> 01:12:13,930
so this is not possible

940
01:12:13,940 --> 01:12:17,740
this is also not possible

941
01:12:17,760 --> 01:12:24,130
the reason is because the variance has to be

942
01:12:24,140 --> 01:12:27,670
there should be nonnegative

943
01:12:27,720 --> 01:12:36,820
right so the point is if i was fire people like to come along and

944
01:12:36,820 --> 01:12:41,910
give you pair choices you had for which there was no distribution network

945
01:12:41,920 --> 01:12:45,650
but if you measuring things from real data in the world if you didn't have

946
01:12:45,650 --> 01:12:49,520
some evil at doing this to the problems like that wouldn't happen you always have

947
01:12:49,520 --> 01:12:52,090
something that satisfies these constraints

948
01:12:52,140 --> 01:12:57,150
but that issue of whether or not the constraints can be satisfied whether these are

949
01:12:57,150 --> 01:13:02,390
called moment constraints in general because there is looking at the first and second moments

950
01:13:02,430 --> 01:13:06,490
this is going to be important later on so it's something to think about

951
01:13:06,520 --> 01:13:14,410
the other thing to think about is even when one does exist

952
01:13:14,420 --> 01:13:19,130
it's not in general going to be unique going to be many such distributions

953
01:13:19,150 --> 01:13:25,510
because all you're doing is is sort of giving particular constraints on the distribution year

954
01:13:25,580 --> 01:13:28,470
it's an underspecified problem in general

955
01:13:28,480 --> 01:13:34,530
so what maximum entropy says it's actually a very simple way of stating the density

956
01:13:34,560 --> 01:13:40,200
it's one that's quite controversial mainly for philosophical reasons for going to see that it

957
01:13:40,200 --> 01:13:45,020
really just is maximum likelihood so it shouldn't really be so controversial

958
01:13:45,040 --> 01:13:48,950
but what it says is that you should look at a certain kind of

959
01:13:48,990 --> 01:13:52,930
property of distribution this is this is called entropy

960
01:13:52,960 --> 01:13:57,400
and since the negative some of px log p of x

961
01:13:57,410 --> 01:14:03,000
so many people have seen entropy in some context before

962
01:14:03,030 --> 01:14:06,200
OK so a number of you might have seen in physics

963
01:14:06,220 --> 01:14:11,040
you might see in information theory and chemistry as well would play a role

964
01:14:11,060 --> 01:14:14,700
if you haven't seen it you just want to understand that this guy is in

965
01:14:14,700 --> 01:14:19,610
some sense a measure of the uncertainty of how spread out the distribution is

966
01:14:19,620 --> 01:14:23,190
it's like variance but it's it's a bit more refined

967
01:14:23,210 --> 01:14:27,070
so the natural thing to do and you can think about this as many names

968
01:14:27,190 --> 01:14:32,280
can think about is being faithful to your data while being otherwise as uncertain as

969
01:14:32,280 --> 01:14:35,010
possible you try to maximize the entropy

970
01:14:35,950 --> 01:14:40,350
imposing constraints that the data gives you given constraints from the data

971
01:14:40,350 --> 01:14:43,990
but otherwise you want to be as uncertain as possible

972
01:14:44,020 --> 01:14:49,840
so different names is called maximum entropy it's called the principle of least commitment

973
01:14:50,440 --> 01:14:53,010
and there's a big literature on this

974
01:14:53,040 --> 01:14:57,690
what's important for us is that if you go through a little argument with raj

975
01:14:57,690 --> 01:15:03,060
multipliers how many people have seen lagrange multipliers before

976
01:15:03,060 --> 01:15:07,320
OK let me ask about how many people have not seen lagrange multipliers before

977
01:15:07,640 --> 01:15:11,790
you have to raise your hand once because you have to some

978
01:15:11,810 --> 01:15:14,330
you either have really happened

979
01:15:15,320 --> 01:15:17,150
OK so some of your shy

980
01:15:17,150 --> 01:15:18,490
you don't want be a sea

981
01:15:21,480 --> 01:15:25,340
especially if you slip into bags for your order of stuff and you want removed

982
01:15:25,460 --> 01:15:28,360
the sea not get stuck in the middle i wanna be a bit you wanna

983
01:15:28,360 --> 01:15:28,840
be it

984
01:15:29,630 --> 01:15:34,340
the problem i'm i'm hip guys i did you take thing on southwestern i got

985
01:15:34,670 --> 01:15:38,990
to be w i airport ninety five minutes before my flight because i was going

986
01:15:39,050 --> 01:15:40,380
be in non gonna be

987
01:15:41,860 --> 01:15:47,550
so i got to the southwest airline machine stuck in my visa card and i my ticket and it said

988
01:15:50,090 --> 01:15:55,760
i sit side of this thing is fixed this is really this is worse than

989
01:15:55,760 --> 01:16:01,130
las vegas there's no way i'm gonna be i'm here ninety five minutes before this

990
01:16:01,130 --> 01:16:03,360
like there's no way i'm not really

991
01:16:07,150 --> 01:16:10,220
when i got my son upon and stood in the back of the field

992
01:16:15,760 --> 01:16:19,340
well i said there's stewing in the back with the beeline for an hour

993
01:16:20,510 --> 01:16:21,530
and then they called the flight

994
01:16:23,050 --> 01:16:23,880
and then i saw it

995
01:16:25,110 --> 01:16:25,990
all that is

996
01:16:26,800 --> 01:16:28,050
seem to be getting on

997
01:16:29,860 --> 01:16:35,150
what looked to me like crumpled pieces of whitehall printer paper

998
01:16:36,150 --> 01:16:42,420
has has though they had downloaded and printed out at home their own barcodes and

999
01:16:42,420 --> 01:16:46,440
boarding passes at twelve or one am the night before are

1000
01:16:48,170 --> 01:16:53,550
well i don't know it's thanks that first convergence southwest airlines began a program

1001
01:16:54,050 --> 01:16:56,530
we're all the customers could download print out

1002
01:16:56,980 --> 01:17:02,110
their own boarding passes twelve or one am the night before all friends i looked at the next

1003
01:17:04,070 --> 01:17:05,220
o twentieth century

1004
01:17:07,740 --> 01:17:11,210
u are so globalization to point out i mean really

1005
01:17:12,130 --> 01:17:15,630
i think about globalization one point o there was a ticket age and we go

1006
01:17:15,630 --> 01:17:20,110
down the case street washington one number standard is a physical person stood in front

1007
01:17:20,110 --> 01:17:20,320
of you

1008
01:17:20,860 --> 01:17:22,380
then there was an e ticket machine

1009
01:17:22,860 --> 01:17:23,820
we thought school

1010
01:17:25,490 --> 01:17:26,760
and while you were sleeping

1011
01:17:27,990 --> 01:17:28,960
while you were sleeping in

1012
01:17:29,380 --> 01:17:32,320
southwest airlines may due to the individual

1013
01:17:32,860 --> 01:17:33,420
your own

1014
01:17:36,420 --> 01:17:37,170
an excuse me

1015
01:17:38,760 --> 01:17:39,590
now we look at it

1016
01:17:40,110 --> 01:17:43,440
southwest airlines made u they are employed

1017
01:17:45,630 --> 01:17:46,880
an excuse me one more time

1018
01:17:47,780 --> 01:17:50,860
if you value times the twelve one am the night before

1019
01:17:51,630 --> 01:17:54,880
u are paying southwest airlines to be their employed

1020
01:18:03,130 --> 01:18:03,940
and that's

1021
01:18:06,960 --> 01:18:07,840
made possible

1022
01:18:08,420 --> 01:18:09,840
what southwest airlines did

1023
01:18:10,630 --> 01:18:13,340
and what i haven't done was horizontal as

1024
01:18:14,260 --> 01:18:14,940
i was still

1025
01:18:15,530 --> 01:18:17,610
interacting with southwest airlines

1026
01:18:19,400 --> 01:18:22,190
i was the more on going up to the e ticket machine

1027
01:18:23,820 --> 01:18:27,490
not the person downloading the take home so next time around

1028
01:18:28,010 --> 01:18:29,010
next time around

1029
01:18:29,820 --> 01:18:33,650
i will go online twelve one in the form i will go to work for

1030
01:18:33,650 --> 01:18:38,340
southwest airlines i will download as an individual my own boarding pass and barcode

1031
01:18:39,030 --> 01:18:41,150
and i will arrive be w airport

1032
01:18:41,650 --> 01:18:43,300
thirty five minutes before the flight

1033
01:18:44,360 --> 01:18:46,740
ninety five and i will capture

1034
01:18:47,210 --> 01:18:49,760
sixty minutes of productivity

1035
01:18:51,360 --> 01:18:55,820
all the changes that will be involved in recapturing those sixty minutes

1036
01:18:56,340 --> 01:19:01,650
we are just at the beginning of a million changes in our habits and business processes

1037
01:19:02,210 --> 01:19:04,900
around but i call horizontal rising ourselves

1038
01:19:05,300 --> 01:19:06,920
that's the second convergence

1039
01:19:07,570 --> 01:19:08,880
say we just the beginning

1040
01:19:09,880 --> 01:19:14,510
the third convergence is very simple just when the world went flat this we created the platform

1041
01:19:14,960 --> 01:19:17,070
from multiple forms of sharing knowledge and work

1042
01:19:17,720 --> 01:19:20,960
just we started to adapt our business processes what happens

1043
01:19:21,440 --> 01:19:26,760
three huge economies called india china and the former soviet empire open up

1044
01:19:28,010 --> 01:19:29,570
and three billion people

1045
01:19:30,630 --> 01:19:31,530
who are not in the game

1046
01:19:31,940 --> 01:19:33,590
walk onto the playing field

1047
01:19:34,190 --> 01:19:35,130
and when they arrive

1048
01:19:35,740 --> 01:19:37,340
just one then flattened

1049
01:19:37,780 --> 01:19:40,940
and just when they can plug-and-play compete connect and collaborate

1050
01:19:41,400 --> 01:19:42,380
with your kids and mine

1051
01:19:42,820 --> 01:19:46,960
more efficiently cheaply and effectively the everbefore in the history of the planet

1052
01:19:47,840 --> 01:19:48,510
yes i know

1053
01:19:49,610 --> 01:19:53,340
ten percent of those only ten percent of those three billion you can really plug-and-play

1054
01:19:53,720 --> 01:19:57,840
ten percent three billion zero here the one that's three hundred million people

1055
01:19:58,220 --> 01:20:01,170
that's exactly twice the size of the american workforce

1056
01:20:02,740 --> 01:20:04,460
it's that simple argument this book

1057
01:20:05,240 --> 01:20:11,340
that's it is the convergence of these ten flatness with these new business processes with these three billion new players

1058
01:20:12,110 --> 01:20:14,170
that's gonna shake the brief history

1059
01:20:14,740 --> 01:20:15,860
the twenty first century

1060
01:20:16,380 --> 01:20:17,260
now beforehand

1061
01:20:17,630 --> 01:20:19,440
let me just point out one last thing

1062
01:20:19,860 --> 01:20:20,940
one less coincidence

1063
01:20:22,860 --> 01:20:26,740
huge event this what i this triple convergence which i think over time

1064
01:20:27,300 --> 01:20:28,380
is going to change

1065
01:20:30,800 --> 01:20:32,650
we're going from vertical to horizontal

1066
01:20:33,490 --> 01:20:37,670
i think this is gonna be seen in time and it will take time to play out

1067
01:20:38,420 --> 01:20:41,010
as the mother of all inflection points

1068
01:20:42,460 --> 01:20:46,260
this is gonna be as big as goldenberg end the printing press

1069
01:20:47,990 --> 01:20:50,690
and just when we reach this incredible

1070
01:20:51,340 --> 01:20:52,320
inflection point

1071
01:20:53,710 --> 01:20:55,800
it's completely disguised

1072
01:20:56,510 --> 01:20:59,030
by what i would call a political perfect storm

1073
01:20:59,940 --> 01:21:02,400
and the political perfect storm is nine eleven

1074
01:21:02,980 --> 01:21:05,530
enron and the dot com bust

1075
01:21:06,440 --> 01:21:11,190
nine eleven completely distracts is a country from the president on down to the reporters

1076
01:21:11,840 --> 01:21:17,800
world focus on worlds run made every sio guilty until proven innocent so who wants

1077
01:21:17,980 --> 01:21:22,840
of them little and think about what they might need to actually compete and collaborate

1078
01:21:22,840 --> 01:21:26,920
more effectively in this world but they might need by way of tax incentives to

1079
01:21:27,570 --> 01:21:29,280
different right and intellectual

1080
01:21:29,710 --> 01:21:30,460
property loss

1081
01:21:32,480 --> 01:21:37,900
of course the com bust made a lot of people really silly it made them think that globalization was over

1082
01:21:38,900 --> 01:21:41,090
but in fact it was just being turbocharged

1083
01:21:43,010 --> 01:21:43,960
so right when

1084
01:21:44,510 --> 01:21:45,530
the world cup flat

1085
01:21:47,800 --> 01:21:49,820
we are looking totally otherwise

1086
01:21:50,610 --> 01:21:53,990
so doing this work was really strange experience

1087
01:21:54,570 --> 01:21:58,490
because they go around and interview all these ceos and sitios ceos and

1088
01:21:59,510 --> 01:21:59,860
and then

1089
01:22:00,760 --> 01:22:01,800
like part people

1090
01:22:02,920 --> 01:22:05,150
they're like pod people in the science fiction movie

1091
01:22:06,690 --> 01:22:08,170
they all know the secret

1092
01:22:09,050 --> 01:22:10,340
they're doing it like crazy

1093
01:22:10,960 --> 01:22:14,170
they all know what's going on everything i learned i learned from them

1094
01:22:15,130 --> 01:22:15,960
they all know

1095
01:22:16,510 --> 01:22:17,320
what's going on

1096
01:22:18,740 --> 01:22:19,880
but nobody has told

1097
01:22:20,240 --> 01:22:20,920
can it's

1098
01:22:23,710 --> 01:22:25,320
nobody's told

1099
01:22:26,090 --> 01:22:26,780
the kids

1100
01:22:27,920 --> 01:22:29,070
so we just had an election

1101
01:22:29,940 --> 01:22:32,300
right in the middle this triple convergence

1102
01:22:32,940 --> 01:22:36,460
with the democrats were debating whether nafta was a good idea

1103
01:22:37,860 --> 01:22:39,260
and the republicans

1104
01:22:39,690 --> 01:22:47,210
put duct tape over the mouth chief white house economist greg making when he said outsourcing makes a lotta sense

1105
01:22:47,710 --> 01:22:51,720
and they stashed in india c chain is basement

1106
01:22:55,380 --> 01:23:00,940
never to be heard from again has anybody seen greg q

1107
01:23:00,940 --> 01:23:04,670
and creating perceive gravity down

1108
01:23:04,720 --> 01:23:07,940
i'm not going to accelerate its in this direction

1109
01:23:08,970 --> 01:23:11,070
i'm going to do shortly

1110
01:23:11,110 --> 01:23:13,390
in the classroom

1111
01:23:13,400 --> 01:23:18,020
i'm going to accelerate all of us in this direction and

1112
01:23:18,080 --> 01:23:20,600
resurrection of the apple go

1113
01:23:20,650 --> 01:23:23,660
in which direction will the blue and gold

1114
01:23:23,670 --> 01:23:26,480
what do you think

1115
01:23:28,030 --> 01:23:31,430
i will go in direction that it proceeds gravity

1116
01:23:31,570 --> 01:23:33,430
after we go like this

1117
01:23:33,430 --> 01:23:35,250
i'll go like this

1118
01:23:35,260 --> 01:23:37,320
i want to go like this

1119
01:23:39,470 --> 01:23:42,150
o city opposite direction of gravity

1120
01:23:42,160 --> 01:23:45,660
so really goes in this direction in fact what you're doing is you're building an

1121
01:23:45,660 --> 01:23:50,140
atmosphere pressure p one here will be higher than the pressure p

1122
01:23:50,180 --> 01:23:51,570
two there

1123
01:23:51,670 --> 01:23:53,810
and wants to go in this direction

1124
01:23:53,860 --> 01:23:55,040
pressure here

1125
01:23:55,060 --> 01:23:58,410
the higher than the pressure there

1126
01:23:58,500 --> 01:24:02,230
larger than zero

1127
01:24:02,240 --> 01:24:04,400
if there is no nowhere in there

1128
01:24:04,440 --> 01:24:05,710
we would all fall

1129
01:24:05,760 --> 01:24:07,570
he would fall

1130
01:24:07,610 --> 01:24:10,020
the model for apple would fall

1131
01:24:10,040 --> 01:24:14,270
and i would fall

1132
01:24:14,350 --> 01:24:16,540
i have here

1133
01:24:16,590 --> 01:24:19,060
and appl

1134
01:24:19,070 --> 01:24:20,600
on the street

1135
01:24:20,610 --> 01:24:24,400
in the closed compartments not like

1136
01:24:24,460 --> 01:24:26,790
what we have there

1137
01:24:27,890 --> 01:24:29,280
i can take you out

1138
01:24:29,390 --> 01:24:31,830
an area where we have no gravity

1139
01:24:31,970 --> 01:24:34,720
here a closed compartment

1140
01:24:34,740 --> 01:24:38,120
here's the apple

1141
01:24:38,130 --> 01:24:40,180
yes gravity in this direction

1142
01:24:40,890 --> 01:24:44,980
he wants to fall in the direction of gravity if i cut the y

1143
01:24:45,000 --> 01:24:47,620
now i'm going to accelerate in this direction

1144
01:24:48,210 --> 01:24:49,820
when i do that

1145
01:24:49,820 --> 01:24:52,840
i had a perceived component of gravity

1146
01:24:52,850 --> 01:24:54,820
in the opposite direction

1147
01:24:54,890 --> 01:24:58,190
so perceived component of gravity this direction

1148
01:24:58,240 --> 01:25:02,400
so this apple wants to fall down because of the gravity that cannot afford

1149
01:25:02,440 --> 01:25:04,600
and want to fall in this direction

1150
01:25:04,640 --> 01:25:06,960
so what will the string do

1151
01:25:07,020 --> 01:25:08,170
very clear

1152
01:25:08,180 --> 01:25:11,360
very intuitive no one has any problem with that

1153
01:25:11,390 --> 01:25:14,540
this string will do this

1154
01:25:14,600 --> 01:25:15,990
now i have blues here

1155
01:25:22,650 --> 01:25:26,280
yes gravity direction that's why they will want to go out

1156
01:25:26,310 --> 01:25:27,770
well is gravity

1157
01:25:27,830 --> 01:25:30,620
i'm going to accelerate the car in this direction

1158
01:25:30,660 --> 01:25:33,280
i introduced perceive gravity in this direction

1159
01:25:33,380 --> 01:25:36,310
what does the building want to do it wants to go up against gravity

1160
01:25:36,320 --> 01:25:39,500
i build up and you must be closed compartment

1161
01:25:39,560 --> 01:25:40,580
i must

1162
01:25:40,620 --> 01:25:42,860
build up the pressure differential

1163
01:25:42,910 --> 01:25:46,430
and i want to fall in this direction i build up pressure here which is

1164
01:25:46,430 --> 01:25:50,910
larger than the pressure there that's why it has to be close

1165
01:25:50,920 --> 01:25:53,540
well over here in england do

1166
01:25:53,750 --> 01:25:55,120
like that

1167
01:25:55,170 --> 01:25:57,330
that is very similar to what

1168
01:25:57,330 --> 01:25:59,880
i accelerate scholars i will do

1169
01:25:59,980 --> 01:26:04,990
apple will go back which is completely consistent with our intuition that helium balloons

1170
01:26:05,030 --> 01:26:08,480
we'll go forward

1171
01:26:08,540 --> 01:26:10,630
the first was the apple

1172
01:26:10,700 --> 01:26:12,930
is totally consistent with anyone's

1173
01:26:14,810 --> 01:26:18,590
make sure that the apple is not

1174
01:26:18,680 --> 01:26:22,370
we need to watch

1175
01:26:22,370 --> 01:26:23,690
only happens

1176
01:26:23,710 --> 01:26:25,510
during acceleration

1177
01:26:25,590 --> 01:26:28,000
so there's only during very short questions

1178
01:26:28,060 --> 01:26:31,030
but i accelerate that you see apple go back and then of course it starts

1179
01:26:31,110 --> 01:26:33,340
swing forget that part

1180
01:26:33,390 --> 01:26:37,970
so what's closely or even more that i accelerate will come this way

1181
01:26:37,990 --> 01:26:41,640
goes into direction extract component of supergravity

1182
01:26:43,360 --> 01:26:45,670
point almost hit this class

1183
01:26:45,720 --> 01:26:47,560
everyone could see that right

1184
01:26:48,870 --> 01:26:51,740
now we're going to do visible

1185
01:26:51,760 --> 01:26:54,900
take this one up

1186
01:26:57,140 --> 01:27:09,570
and now let's take one of our beautiful problems

1187
01:27:09,630 --> 01:27:11,630
the following year

1188
01:27:11,640 --> 01:27:15,060
be closed compartments so that

1189
01:27:15,070 --> 01:27:18,180
air can build up the

1190
01:27:18,240 --> 01:27:27,500
pressure differential

1191
01:27:27,690 --> 01:27:31,100
this problem is static charges on these

1192
01:27:35,470 --> 01:27:41,620
only have to accelerate the balloon going forward direction to accelerate in this direction

1193
01:27:41,650 --> 01:27:44,120
what you're going to see is really very little to do with every time i

1194
01:27:44,120 --> 01:27:44,850
see it

1195
01:27:44,930 --> 01:27:47,590
i said to myself i can reason it

1196
01:27:47,640 --> 01:27:48,880
do i understand

1197
01:27:48,930 --> 01:27:52,380
i don't know what is the difference in reasoning and understanding there we go

1198
01:27:52,400 --> 01:27:56,230
and when this way

1199
01:27:56,280 --> 01:27:57,570
you can do this

1200
01:27:57,610 --> 01:27:59,430
your car your parents

1201
01:27:59,450 --> 01:28:01,060
really fun to do it

1202
01:28:01,080 --> 01:28:04,910
have string was an apple or something else and every helium balloon

1203
01:28:04,960 --> 01:28:07,170
close the windows

1204
01:28:07,210 --> 01:28:10,260
don't have to be exact totally closed more or less

1205
01:28:10,340 --> 01:28:13,810
and as you that you want to slam the brakes

1206
01:28:13,810 --> 01:28:17,240
first to apply this principle component analysis

1207
01:28:17,300 --> 01:28:19,470
you compute the principal component

1208
01:28:19,490 --> 01:28:24,450
and for each test point compute its projection on these components so basically you're converting

1209
01:28:24,450 --> 01:28:28,640
this does data according to this transformed space

1210
01:28:28,650 --> 01:28:30,940
and they are taking the first two components

1211
01:28:30,990 --> 01:28:32,320
so using this

1212
01:28:32,330 --> 01:28:33,980
formerly here

1213
01:28:34,070 --> 01:28:35,600
and if

1214
01:28:35,660 --> 01:28:39,590
this some here is higher than some

1215
01:28:39,590 --> 01:28:41,610
we specify threshold

1216
01:28:41,670 --> 01:28:44,550
data record can be considered as anomalous

1217
01:28:44,550 --> 01:28:47,900
again this is taking the first

1218
01:28:47,920 --> 01:28:54,150
q principal components so assuming that this first q principal component scale can represent the

1219
01:28:54,150 --> 01:28:56,540
normal numbers here which is very well

1220
01:28:56,550 --> 01:29:01,060
another but the opposite idea is basically to observe

1221
01:29:01,070 --> 01:29:04,610
the last few principal components and to compute the similar stuff

1222
01:29:04,620 --> 01:29:07,820
and then anomalies that show high value

1223
01:29:09,590 --> 01:29:12,550
this book of for example if this is

1224
01:29:12,600 --> 01:29:14,590
of the curve

1225
01:29:14,600 --> 01:29:18,670
that is beginning some pre specified threshold that can be also observed as anomalies

1226
01:29:18,690 --> 01:29:22,870
so sometimes this may lead to good results sometimes this may lead could to to

1227
01:29:22,870 --> 01:29:28,440
could but the main idea behind is basically saying that anomalies

1228
01:29:28,720 --> 01:29:30,160
captain the

1229
01:29:30,170 --> 01:29:38,690
this smaller principal components

1230
01:29:38,700 --> 01:29:41,290
OK so

1231
01:29:41,330 --> 01:29:44,610
so the set of

1232
01:29:44,670 --> 01:29:49,120
as i said few top principal components capture the variability of normally but the assumption

1233
01:29:49,120 --> 01:29:51,930
is the smallest principal components would also

1234
01:29:51,950 --> 01:29:54,860
here constant values for normal data so they

1235
01:29:54,860 --> 01:29:58,410
not important for this normal here but

1236
01:29:58,440 --> 01:30:04,030
outliers so anomalous here variability in the smallest principal components and for that reason this

1237
01:30:04,030 --> 01:30:07,490
smallest principal components of the most important that the undertaking

1238
01:30:10,160 --> 01:30:14,640
the city is applied in this paper here for network intrusion detection using principal component

1239
01:30:15,840 --> 01:30:17,210
and basically for

1240
01:30:17,230 --> 01:30:18,580
every time is the

1241
01:30:19,800 --> 01:30:22,490
the basic components for the specific

1242
01:30:22,550 --> 01:30:24,650
they can be taken the first one

1243
01:30:24,710 --> 01:30:29,620
yes taking all principal components over time to form a matrix

1244
01:30:29,650 --> 01:30:33,040
then we have forming the matrix of all this principal components

1245
01:30:33,090 --> 01:30:37,890
and then we are trying to applying this single about the composition of the matrix

1246
01:30:37,890 --> 01:30:39,110
to capture the

1247
01:30:39,660 --> 01:30:43,730
b here and right now we have the new data record

1248
01:30:43,790 --> 01:30:45,550
twenty basically new t

1249
01:30:45,550 --> 01:30:51,810
the angle between the new principal component and the singular vector for this step

1250
01:30:51,860 --> 01:30:55,510
can give the degree of anomaly the higher the and that's probably the

1251
01:30:55,530 --> 01:30:59,110
there are more than of the same so basically this new data record

1252
01:30:59,150 --> 01:31:00,640
it is actually

1253
01:31:00,650 --> 01:31:04,290
deviates more from that's what we have so far so that's

1254
01:31:04,300 --> 01:31:08,850
what people use full degree of anomaly

1255
01:31:10,480 --> 01:31:16,150
depending on the applications many researchers use different visualization techniques school

1256
01:31:16,160 --> 01:31:21,080
local all observers anomaly and people use different visualization tools

1257
01:31:21,170 --> 01:31:25,290
it to detect anomalies or to help them to understand what anomalies

1258
01:31:25,370 --> 01:31:26,950
o two

1259
01:31:26,970 --> 01:31:32,530
show what the evidence showing that so and they usually seems to usually dealing with

1260
01:31:32,530 --> 01:31:33,840
high dimensional

1261
01:31:33,880 --> 01:31:37,050
the states dataset

1262
01:31:37,100 --> 01:31:39,860
they are using different alternative fuels

1263
01:31:39,880 --> 01:31:42,790
how to you today how to visualize the data

1264
01:31:42,880 --> 01:31:47,780
obvious this advantage is that you have to keep the human in the loop so

1265
01:31:47,780 --> 01:31:51,350
humans have to verify that that's real anomaly

1266
01:31:52,450 --> 01:31:54,540
some drawbacks that's

1267
01:31:54,570 --> 01:31:56,140
this obviously works

1268
01:31:56,150 --> 01:31:57,860
full dimensional data we have

1269
01:31:57,870 --> 01:32:00,210
hundreds of the dimensions

1270
01:32:00,230 --> 01:32:01,130
it's quite

1271
01:32:01,160 --> 01:32:03,730
impractical for humans to look to all this

1272
01:32:11,150 --> 01:32:13,200
need not be

1273
01:32:13,220 --> 01:32:16,120
visible just in one dimension so sometimes

1274
01:32:16,130 --> 01:32:17,880
you have to combine two or three

1275
01:32:17,890 --> 01:32:19,600
the measures in order to see

1276
01:32:19,660 --> 01:32:21,180
so this anomalous

1277
01:32:21,240 --> 01:32:22,390
so that's why it's

1278
01:32:22,410 --> 01:32:24,070
difficult for humans too

1279
01:32:24,070 --> 01:32:28,130
visually the vision is that because if you introduce this

1280
01:32:28,200 --> 01:32:32,960
combination of of the parents of all the attributes the significant includes the number of

1281
01:32:32,960 --> 01:32:34,590
views that you have to

1282
01:32:34,630 --> 01:32:36,860
the ticket to look at

1283
01:32:36,870 --> 01:32:41,180
and it's not suitable for real time anomaly detection when you have real

1284
01:32:41,200 --> 01:32:42,650
data streams

1285
01:32:42,670 --> 01:32:44,640
so i'll just give you one example of

1286
01:32:44,640 --> 01:32:46,130
as probability

1287
01:32:46,140 --> 01:32:49,250
plus two times that's one

1288
01:32:49,270 --> 01:32:53,220
so we we shift to estimate the probability up but it turns upside

1289
01:32:53,240 --> 01:32:54,420
can shift

1290
01:32:54,460 --> 01:32:56,160
the probability of some other

1291
01:32:57,460 --> 01:33:00,770
right direction we set the probability of the correct thing

1292
01:33:01,220 --> 01:33:04,430
down but tons of songs

1293
01:33:04,450 --> 01:33:07,370
most of the problems of the wrong thing

1294
01:33:08,030 --> 01:33:10,300
but it and so on

1295
01:33:10,300 --> 01:33:11,790
and the vote goes the wrong way

1296
01:33:11,800 --> 01:33:13,200
and we get the wrong thing

1297
01:33:13,230 --> 01:33:14,430
so that

1298
01:33:14,630 --> 01:33:19,480
but by two because we can see if both these same time

1299
01:33:34,230 --> 01:33:35,310
so it tells us

1300
01:33:36,120 --> 01:33:37,580
time steps on

1301
01:33:37,630 --> 01:33:44,170
and then the score it is just is this query

1302
01:33:50,220 --> 01:33:53,680
raag again

1303
01:33:53,790 --> 01:33:56,940
so it is there

1304
01:33:56,960 --> 01:33:59,630
this discourages from probing

1305
01:33:59,990 --> 01:34:02,310
by the classifier

1306
01:34:04,600 --> 01:34:07,340
four years from now setting things up and down

1307
01:34:07,420 --> 01:34:09,710
now slip factor two

1308
01:34:09,730 --> 01:34:12,020
which comes in from that

1309
01:34:12,020 --> 01:34:15,920
from the exact analysis

1310
01:34:15,980 --> 01:34:20,890
screening is

1311
01:34:20,960 --> 01:34:23,520
OK so he says that

1312
01:34:23,630 --> 01:34:26,390
it says that there

1313
01:34:26,470 --> 01:34:30,600
for every classifier for every multiclass prediction problem

1314
01:34:30,670 --> 01:34:32,870
the multiclass regret is bounded by the

1315
01:34:32,910 --> 01:34:34,850
four times was carried out the

1316
01:34:34,850 --> 01:34:39,790
binary regret

1317
01:34:39,840 --> 01:34:44,050
we have had to martin

1318
01:34:53,890 --> 01:34:57,080
she was also or something cool

1319
01:34:57,120 --> 01:35:00,870
which is that you get a bonus round

1320
01:35:00,920 --> 01:35:03,550
you can do probabilistic multi class

1321
01:35:03,590 --> 01:35:05,690
prediction as well

1322
01:35:06,640 --> 01:35:11,300
maybe it's obvious if you go back and we think about the exact case

1323
01:35:11,340 --> 01:35:14,390
because they can

1324
01:35:14,430 --> 01:35:18,770
said that the individual probabilistic predictions work at that quantity

1325
01:35:20,500 --> 01:35:23,410
and then we can just invert trying to figure

1326
01:35:23,420 --> 01:35:26,690
there's a lot of the way to do that in the next one

1327
01:35:26,710 --> 01:35:29,680
that's supposed to be a good publicity guesstimate

1328
01:35:29,740 --> 01:35:31,340
whenever are

1329
01:35:31,340 --> 01:35:37,520
binary predictors are accurate

1330
01:35:39,280 --> 01:35:42,850
this list

1331
01:35:42,870 --> 01:35:44,280
if it things

1332
01:35:44,300 --> 01:35:47,100
according to this output process

1333
01:35:48,420 --> 01:35:50,000
very classifier

1334
01:35:50,030 --> 01:35:51,520
urbina crossfire

1335
01:35:51,600 --> 01:35:55,450
every multiclass distribution

1336
01:35:55,470 --> 01:35:58,120
the squared

1337
01:36:01,120 --> 01:36:03,860
one of our the true

1338
01:36:03,910 --> 01:36:05,880
multiclass probability

1339
01:36:05,920 --> 01:36:06,590
to the

1340
01:36:06,600 --> 01:36:09,220
predicted the class probability

1341
01:36:09,290 --> 01:36:10,770
it is bounded by

1342
01:36:10,790 --> 01:36:13,180
four times

1343
01:36:14,990 --> 01:36:23,080
binary regret

1344
01:36:23,120 --> 01:36:27,890
we haven't tried experiment as well

1345
01:36:29,670 --> 01:36:32,220
and kind of weird about this by the way

1346
01:36:32,270 --> 01:36:34,340
which is that this formula

1347
01:36:34,370 --> 01:36:35,880
can go above

1348
01:36:35,970 --> 01:36:38,680
one and below zero

1349
01:36:38,740 --> 01:36:42,290
kind of unfortunate

1350
01:36:42,300 --> 01:36:43,990
it is even if you click

1351
01:36:44,050 --> 01:36:46,150
meaning you just make sure things are

1352
01:36:46,210 --> 01:36:48,220
in the interval zero or one

1353
01:36:48,300 --> 01:36:54,810
the theorem is still valid

1354
01:36:57,720 --> 01:36:59,110
you know

1355
01:36:59,120 --> 01:37:02,850
we're working with imperfect ones as well

1356
01:37:02,900 --> 01:37:06,020
right so this is perfect and that this is imperfect

1357
01:37:07,050 --> 01:37:09,660
here i was thinking about

1358
01:37:10,390 --> 01:37:37,520
how we disturb the some of the largest and that

1359
01:37:37,540 --> 01:37:40,290
let me tell you

1360
01:37:40,330 --> 01:37:41,890
one of the analysis which

1361
01:37:41,890 --> 01:37:45,350
we should not do for recruiting codes

1362
01:37:45,400 --> 01:37:46,650
what you should not do

1363
01:37:46,650 --> 01:37:48,270
it is assumed that the

1364
01:37:48,290 --> 01:37:50,900
individual areas are independent

1365
01:37:50,920 --> 01:37:51,810
and then

1366
01:37:51,940 --> 01:37:57,060
multiply probability is an appropriate way

1367
01:37:57,120 --> 01:37:59,520
to to try to predict the class

1368
01:37:59,580 --> 01:38:02,230
the reason why you shouldn't do that is because

1369
01:38:02,230 --> 01:38:04,600
it really is the case in practice

1370
01:38:04,620 --> 01:38:06,420
that these

1371
01:38:06,830 --> 01:38:11,670
the individual binary problems

1372
01:38:11,690 --> 01:38:14,830
kind of in and around so

1373
01:38:14,900 --> 01:38:20,850
our approach is to are for each of the individual binary problems

1374
01:38:20,870 --> 01:38:24,170
or at least for a lot of them

1375
01:38:24,190 --> 01:38:27,210
that that that dependence

1376
01:38:27,230 --> 01:38:29,540
is there in practice in a big way

1377
01:38:29,600 --> 01:38:34,710
so you must take that into account in your analysis

1378
01:38:46,870 --> 01:38:51,210
right so

1379
01:38:51,230 --> 01:38:52,980
you analyse the running time

1380
01:38:52,980 --> 01:38:55,020
no i did not explicitly

1381
01:38:55,080 --> 01:38:59,330
i will tell you that turns out the running time this is similar to

1382
01:38:59,330 --> 01:39:02,770
so you might think that because you ran probing many different times

1383
01:39:03,270 --> 01:39:08,000
the costs you know according to be different from the programme which is a little

1384
01:39:11,400 --> 01:39:13,130
this trick

1385
01:39:13,230 --> 01:39:14,420
the trick is

1386
01:39:14,460 --> 01:39:17,150
let's go back to this

1387
01:39:21,750 --> 01:39:23,900
the trick is that this sum

1388
01:39:23,960 --> 01:39:25,580
can be thought of as an integration

1389
01:39:25,580 --> 01:39:29,330
then you can do inference using exactly the same markov chain going upwards

1390
01:39:29,380 --> 01:39:32,370
and that'll get you a sample from the posterior at every level,

1391
01:39:32,370 --> 01:39:35,780
and then learning should be easy

1392
01:39:35,780 --> 01:39:37,110
so in fact

1393
01:39:37,120 --> 01:39:39,590
if you remember the learning algorithm i had

1394
01:39:39,650 --> 01:39:43,870
for a directed belief net, let's think about learning a weight here

1395
01:39:43,920 --> 01:39:45,160
between unit j

1396
01:39:45,160 --> 01:39:46,190
in this layer,

1397
01:39:46,190 --> 01:39:48,250
and unit i in this layer.

1398
01:39:48,260 --> 01:39:50,290
the learning algorithm says,

1399
01:39:50,290 --> 01:39:52,440
get yourself a sample from the posterior.

1400
01:39:52,450 --> 01:39:55,960
well that's easy to do, we just go chunk.

1401
01:39:56,010 --> 01:39:58,870
and given a sample from the posterior

1402
01:39:58,940 --> 01:40:02,110
do learning that says, take the presynaptic activity

1403
01:40:02,120 --> 01:40:03,790
this guy

1404
01:40:03,840 --> 01:40:06,210
and multiply it by the difference between

1405
01:40:06,220 --> 01:40:08,580
what you actually had here

1406
01:40:08,590 --> 01:40:11,030
in the data, or the posterior if it was a hidden layer,

1407
01:40:11,120 --> 01:40:12,490
that's this guy

1408
01:40:12,490 --> 01:40:14,790
and the probability

1409
01:40:14,830 --> 01:40:18,660
that this sample from the posterior would have turned this guy on.

1410
01:40:18,700 --> 01:40:20,740
well what's the probability

1411
01:40:20,830 --> 01:40:23,290
that this stuff would turn that guy on?

1412
01:40:23,330 --> 01:40:27,340
instead of computing the probability, we're getting a sample with that probability,

1413
01:40:27,420 --> 01:40:29,910
so we can actually do an experiment. we're gonna take this and say do you

1414
01:40:29,910 --> 01:40:32,790
want to turn on? let's sample stochastically

1415
01:40:32,800 --> 01:40:35,250
and that's what i call this, right?

1416
01:40:35,320 --> 01:40:37,750
but notice we did that experiment already

1417
01:40:37,790 --> 01:40:40,220
because when we did inference for this layer,

1418
01:40:40,240 --> 01:40:42,010
we were exactly taking this sample

1419
01:40:42,030 --> 01:40:43,740
and going through the same weights the other way

1420
01:40:43,790 --> 01:40:46,040
and deciding whether this guy should be on or not.

1421
01:40:47,130 --> 01:40:49,410
this state here

1422
01:40:49,420 --> 01:40:53,360
is actually this the reconstruction right so the state here is actually

1423
01:40:53,410 --> 01:40:55,290
a sample

1424
01:40:55,290 --> 01:40:57,250
of this probability we need

1425
01:40:57,250 --> 01:40:59,960
so we can plug that state in instead

1426
01:41:00,010 --> 01:41:03,320
so that the learning rule for this connection looks like

1427
01:41:06,700 --> 01:41:09,120
the difference between

1428
01:41:09,170 --> 01:41:11,030
the actual state of si

1429
01:41:11,040 --> 01:41:13,460
in the data, si0,

1430
01:41:13,480 --> 01:41:16,840
and the top-down prediction from this

1431
01:41:16,900 --> 01:41:18,650
posterior state here

1432
01:41:18,670 --> 01:41:21,040
which is si1.

1433
01:41:21,080 --> 01:41:25,150
so that's the learning rule for a directed belief net.

1434
01:41:25,190 --> 01:41:27,820
for this weight here

1435
01:41:27,870 --> 01:41:30,450
but of course this weight is tied to all these other weights

1436
01:41:30,590 --> 01:41:34,040
so we need to apply the learning rule here too

1437
01:41:34,040 --> 01:41:36,420
the learning rule there looks like this

1438
01:41:36,440 --> 01:41:39,290
and the learning rule here looks like this

1439
01:41:39,340 --> 01:41:41,500
and if you look at all these things

1440
01:41:41,510 --> 01:41:46,000
that term, sj0 si0, that stays. but this term

1441
01:41:47,710 --> 01:41:48,870
si1 shows up

1442
01:41:48,870 --> 01:41:53,120
against there--again there. sj0 si1, but with the opposite sign

1443
01:41:53,160 --> 01:41:54,480
so that cancels

1444
01:41:54,620 --> 01:41:58,610
and this cancels and this cancels, it all cancels until you get down to infinity here.

1445
01:41:58,620 --> 01:42:00,420
and so we

1446
01:42:00,450 --> 01:42:02,010
get the idea that

1447
01:42:02,030 --> 01:42:04,580
if you take this directed belief net

1448
01:42:04,630 --> 01:42:06,160
and figure out the learning

1449
01:42:06,170 --> 01:42:09,870
rule, the learning rule says you get all these derivatives here

1450
01:42:09,870 --> 01:42:11,570
and they all cancel out

1451
01:42:11,580 --> 01:42:14,410
and you end up with

1452
01:42:14,450 --> 01:42:20,490
the correlation that you get here and a correlation you get at infinity there, and that's the boltzmann machine learning rule.

1453
01:42:20,490 --> 01:42:24,500
and then you can ask well what about that contrastive divergence approximation we were making,

1454
01:42:24,530 --> 01:42:27,400
when we didn't run the chain for very long?

1455
01:42:27,410 --> 01:42:29,510
well suppose these weights were small.

1456
01:42:29,570 --> 01:42:32,550
this chain would mix rapidly

1457
01:42:32,580 --> 01:42:34,820
and if the chain mixes rapidly

1458
01:42:34,820 --> 01:42:37,990
then when you do inference, it's also mixing rapidly

1459
01:42:38,050 --> 01:42:41,530
so by the time you've gone through a few steps

1460
01:42:41,530 --> 01:42:46,530
then you'll be sampling from the distribution the model believes in.

1461
01:42:46,540 --> 01:42:47,960
now if you take a model

1462
01:42:47,980 --> 01:42:51,950
generate samples from the distribution it believes in, and you ask it, how would you like to change

1463
01:42:51,950 --> 01:42:53,070
your parameters

1464
01:42:53,090 --> 01:42:57,150
you're gonna say, i don't want change parameters. my parameters fit this data really well

1465
01:42:57,160 --> 01:42:58,760
so we know that

1466
01:42:58,790 --> 01:43:00,040
if the weights are small

1467
01:43:00,050 --> 01:43:03,400
all these derivatives up here will add up to zero.

1468
01:43:03,410 --> 01:43:04,550
there'll be noise in them,

1469
01:43:04,570 --> 01:43:06,510
but they'll all come to zero on average,

1470
01:43:06,530 --> 01:43:08,320
so we can ignore them

1471
01:43:09,000 --> 01:43:11,880
we can say, let's ignore all these small ones, and let's just take this one and

1472
01:43:11,880 --> 01:43:12,830
this one

1473
01:43:12,840 --> 01:43:16,200
so we're taking the derivative here and the derivative here

1474
01:43:16,200 --> 01:43:17,780
so that gives us

1475
01:43:17,820 --> 01:43:19,950
just these first two terms here

1476
01:43:20,000 --> 01:43:21,380
which gives us

1477
01:43:21,400 --> 01:43:22,530
this correlation

1478
01:43:24,980 --> 01:43:28,960
si1 sj1. that's the contrastive divergence learning rule.

1479
01:43:28,980 --> 01:43:32,920
so it's equivalent to saying we're gonna learn this deep directed belief net by ignoring

1480
01:43:32,920 --> 01:43:35,150
the small derivatives you get up there,

1481
01:43:35,160 --> 01:43:36,380
which seems very sensible.

1482
01:43:36,550 --> 01:43:39,940
the best thing to do actually is, as the weights get bigger,

1483
01:43:39,950 --> 01:43:43,330
go for a few more steps, so run the Markov chain a little bit longer

1484
01:43:43,330 --> 01:43:46,200
and you can actually monitor, because you can look at the size of these derivatives and see

1485
01:43:46,590 --> 01:43:49,650
whether the ones you're ignoring are very small.

1486
01:43:49,910 --> 01:43:52,080
so that sort of

1487
01:43:52,090 --> 01:43:55,040
explained the relation between

1488
01:43:55,050 --> 01:43:57,690
the directed learning algorithm, this algorithm

1489
01:43:57,700 --> 01:44:00,040
and the boltzmann machine learning algorithm

1490
01:44:00,090 --> 01:44:09,160
for a net where you have these same weights at every layer, and it's an infinite net.

1491
01:44:09,160 --> 01:44:10,410
now of course

1492
01:44:10,410 --> 01:44:15,910
what we can do is we can say why don't i start off learning this

1493
01:44:15,940 --> 01:44:17,610
which is equivalent to learning

1494
01:44:17,660 --> 01:44:19,330
this. these are the same model,

1495
01:44:19,340 --> 01:44:21,620
because these are all the same weights.

1496
01:44:21,620 --> 01:44:24,510
once i've learned that,

1497
01:44:24,540 --> 01:44:28,170
i'm now going to freeze some weights

1498
01:44:28,220 --> 01:44:29,980
i'll freeze these weighs here.

1499
01:44:29,990 --> 01:44:32,240
i'll just keep this bit of the model.

1500
01:44:32,260 --> 01:44:36,160
i'll say, ok, now i have a model for how this stuff gets turned into that stuff

1501
01:44:36,160 --> 01:44:40,960
and i also know by using the weights the other way round, i can do accurate inference.

1502
01:44:40,990 --> 01:44:43,120
and now i'll take all these weights

1503
01:44:43,170 --> 01:44:46,400
untie them from these weights

1504
01:44:46,440 --> 01:44:48,420
so these weights are frozen.

1505
01:44:48,480 --> 01:44:49,690
these weights

1506
01:44:49,700 --> 01:44:52,080
start off from where they were already

1507
01:44:52,090 --> 01:44:54,300
so i've got the same model

1508
01:44:54,320 --> 01:44:59,420
this model is still the same as that model, the previous model, but now with

1509
01:44:59,420 --> 01:45:03,080
all these tied together i start learning these,

1510
01:45:03,090 --> 01:45:06,190
as a model of this stuff

1511
01:45:06,240 --> 01:45:09,990
so you can also think of this learning algorithm as, we're always going to have this

1512
01:45:09,990 --> 01:45:12,690
infintely deep directed belief net

1513
01:45:12,710 --> 01:45:15,620
we're gonna start off by learning all the weights at the same time, and all

1514
01:45:15,620 --> 01:45:18,050
layers at the same time

1515
01:45:18,080 --> 01:45:23,190
and gradually we're gonna untie the higher-level weights from the already learned lower-level weights, to give

1516
01:45:23,190 --> 01:45:25,760
ourselves more parameters

1517
01:45:25,780 --> 01:45:28,510
and we're going to get a better and better model.

1518
01:45:28,530 --> 01:45:31,620
so that's another way to justify what's going on in this deep learning. it's an

1519
01:45:31,620 --> 01:45:33,700
at least in principle is that they go

1520
01:45:33,790 --> 01:45:37,880
o all the all sets of five known so for example here's a set of

1521
01:45:37,880 --> 01:45:41,510
five nodes and ask OK what's it community score you know here's the school and

1522
01:45:41,510 --> 01:45:45,800
then i try some other set of nodes and you know this gives me

1523
01:45:45,820 --> 01:45:49,290
this gives you already have better scores of these school of point four and you

1524
01:45:49,290 --> 01:45:52,790
know if i now consider this set of five nodes you know this this has

1525
01:45:52,790 --> 01:45:54,340
absolutely best

1526
01:45:54,370 --> 01:45:56,710
this is absolutely the best

1527
01:45:56,760 --> 01:46:01,040
cluster of size five or on five nodes in this particular

1528
01:46:01,040 --> 01:46:04,870
right so the idea will be the idea is to find something that that we

1529
01:46:05,580 --> 01:46:12,140
the network community profile plot basically what i'm asking is for every k for every

1530
01:46:12,140 --> 01:46:16,780
size of the cluster was what is the score the conductance of

1531
01:46:16,820 --> 01:46:18,910
best such clustering the network

1532
01:46:20,290 --> 01:46:25,460
and then just putting k versus versus the school so the idea is the following

1533
01:46:25,460 --> 01:46:30,320
for example i fix k equal five i go over over all sets of five

1534
01:46:30,320 --> 01:46:32,390
nodes and i find the best

1535
01:46:32,430 --> 01:46:34,240
and now i increase my k

1536
01:46:34,260 --> 01:46:38,740
let's say go k equal seven and our goal sort of i go all possible

1537
01:46:38,740 --> 01:46:43,210
cuts in the network that cutaway seven nodes and i ask what is the best

1538
01:46:43,210 --> 01:46:48,130
one for example for seven notes this is the best one right now we're just

1539
01:46:48,160 --> 01:46:49,820
very can increase k

1540
01:46:49,820 --> 01:46:55,210
and you know i get i get this best best clusters of on k nodes

1541
01:46:55,220 --> 01:46:57,640
and this is the plot be creating

1542
01:46:58,470 --> 01:47:01,210
to show you a bit differently so here i am

1543
01:47:01,220 --> 01:47:03,080
the network

1544
01:47:03,080 --> 01:47:06,510
this is the size of the of the cluster the number of nodes in the

1545
01:47:07,300 --> 01:47:11,040
this is the quality of the cluster the conductance lower is better

1546
01:47:11,040 --> 01:47:14,180
basically every point here corresponds to a particular

1547
01:47:14,200 --> 01:47:16,030
cluster particular cut

1548
01:47:16,130 --> 01:47:19,530
this tells me how good is the cut in all of the interesting is how

1549
01:47:19,530 --> 01:47:24,000
does the quality of the cost of the cluster change with the size of the

1550
01:47:24,890 --> 01:47:28,330
so what i'm basically blocking is hard the

1551
01:47:28,330 --> 01:47:29,530
the score

1552
01:47:29,550 --> 01:47:34,500
the quality of the best cluster change as the cluster as the clusters get bigger

1553
01:47:38,240 --> 01:47:42,910
this calculating this is a bit tricky if i have time i'll go into why

1554
01:47:42,910 --> 01:47:47,470
is this the right i mean because in principle and basically going all possible subsets

1555
01:47:47,490 --> 01:47:50,720
one of four possible sizes and finding the best one

1556
01:47:50,750 --> 01:47:52,910
in reality not doing that

1557
01:47:52,950 --> 01:47:56,370
but i see that i have time to talk about that

1558
01:47:56,420 --> 01:48:00,630
so what's the idea that the idea is that by using the using this

1559
01:48:01,180 --> 01:48:05,660
approximation algorithms for NP hard problem of of graph partitioning

1560
01:48:05,670 --> 01:48:11,710
to somehow experimentally probe the structure of these networks and

1561
01:48:11,720 --> 01:48:13,260
there is a bunch of

1562
01:48:13,290 --> 01:48:18,710
different ways how to cook how to compute this these dots especially i can use

1563
01:48:18,710 --> 01:48:20,370
spectral methods

1564
01:48:20,380 --> 01:48:23,180
that have certain problems

1565
01:48:23,210 --> 01:48:27,880
basically they confuse long that's which sort of deep cuts i can use the flow

1566
01:48:27,880 --> 01:48:29,290
based IDS

1567
01:48:29,290 --> 01:48:32,700
in the past but have they have problems with expander graphs

1568
01:48:32,710 --> 01:48:37,340
i can use some heuristics that work really well like

1569
01:48:37,390 --> 01:48:40,500
i can use some post processing

1570
01:48:40,510 --> 01:48:44,910
to improve on the cards i can use this local spectral methods where i find

1571
01:48:44,910 --> 01:48:49,510
the cluster around a particular node and this is very scalable and this is basically

1572
01:48:49,510 --> 01:48:50,790
what i using

1573
01:48:50,830 --> 01:48:54,930
and i'll explain later what this means

1574
01:48:55,630 --> 01:49:00,380
so just to build some intuition what plotting you here is sort of the number

1575
01:49:00,380 --> 01:49:04,670
of nodes in the cluster which is the conductors the score lower is better

1576
01:49:04,710 --> 01:49:07,460
and here i'm showing

1577
01:49:07,470 --> 01:49:12,740
eight four different grades of different kinds of measures for the chain four greed like

1578
01:49:12,740 --> 01:49:16,920
two d mash for cube like a three d mesh and you can see so

1579
01:49:16,960 --> 01:49:20,990
what's the best way to cut agreed that the best way to come the to

1580
01:49:20,990 --> 01:49:23,790
cut the grid is exactly in the middle of the track i always have to

1581
01:49:23,790 --> 01:49:27,590
cut the same number of edges but as a move towards the half of the

1582
01:49:27,590 --> 01:49:32,370
grid getting more edges inside right so we find putting you sort of

1583
01:49:32,380 --> 01:49:36,050
the number of nodes in all in the smaller side of the bridge which is

1584
01:49:36,050 --> 01:49:39,280
the number of edges i have to cut the logarithmic scales then i'm getting the

1585
01:49:39,300 --> 01:49:44,130
surface to volume ratio so that's the reason why i get this straight lines

1586
01:49:44,180 --> 01:49:48,090
and the slope of the line is exactly one where the dimensionality of the great

1587
01:49:48,130 --> 01:49:52,930
OK and of course if i have a very dense random graph or clique

1588
01:49:52,970 --> 01:49:54,840
then all the cuts

1589
01:49:54,890 --> 01:49:57,540
very expensive so so

1590
01:49:57,960 --> 01:49:59,870
so i get this flat line

1591
01:50:00,780 --> 01:50:06,320
so what is the intuition here is that somehow if i have this nice geometric

1592
01:50:06,320 --> 01:50:11,420
things basically i get i get is downward curves where the

1593
01:50:11,430 --> 01:50:16,630
the slope of the curve somehow corresponds to the dimensionality of the object and cutting

1594
01:50:16,670 --> 01:50:18,220
OK that's the intuition

1595
01:50:18,240 --> 01:50:21,700
and for example if i take the road network in california

1596
01:50:21,710 --> 01:50:23,870
i see that the whole thing goes down

1597
01:50:24,170 --> 01:50:26,460
in the slope is

1598
01:50:26,490 --> 01:50:29,330
a bit more than two right and the reason why the slope is a bit

1599
01:50:29,330 --> 01:50:33,840
more than two is because we also have pages so this thing is not exactly

1600
01:50:33,850 --> 01:50:35,930
plane but it's almost plane

1601
01:50:36,800 --> 01:50:39,910
and if i continue for example if i think

1602
01:50:39,910 --> 01:50:42,630
like our hands they said that sort of

1603
01:50:42,680 --> 01:50:45,960
popular in manifold learning you know every

1604
01:50:45,960 --> 01:50:50,340
basically what i do is i think this every image of a hand is like

1605
01:50:50,410 --> 01:50:51,490
vector in

1606
01:50:51,500 --> 01:50:56,240
i think two hundred fifty five dimensional space and now is simply define the network

1607
01:50:56,260 --> 01:50:58,340
in this space to connect

1608
01:50:58,340 --> 01:51:00,380
pictures that are close

1609
01:51:00,390 --> 01:51:04,880
and if i try to partition such graph again i sort of see looks like

1610
01:51:05,090 --> 01:51:09,040
like great writers this nice structure where the best way to partition in half and

1611
01:51:09,040 --> 01:51:09,750
so on

1612
01:51:10,590 --> 01:51:14,160
so that's four like many for learning datasets

1613
01:51:14,160 --> 01:51:17,340
if i take OK again so i want you talk

1614
01:51:17,390 --> 01:51:21,410
so here is the

1615
01:51:21,420 --> 01:51:26,450
another little small social networks because it's nice clustering structure and if i blocked the

1616
01:51:26,450 --> 01:51:28,510
size of the cluster versus

1617
01:51:28,530 --> 01:51:32,620
the score of the cluster sort of the bigger the cluster the better and

1618
01:51:34,280 --> 01:51:39,930
this this letter here correspond to the clusters right so what he says is if

1619
01:51:39,930 --> 01:51:41,600
i cut away a

1620
01:51:41,620 --> 01:51:44,460
you know if i cut away this piece

1621
01:51:44,510 --> 01:51:48,000
this corresponds to hear support for example if i cut away the

1622
01:51:48,040 --> 01:51:49,460
this should be this piece

1623
01:51:49,460 --> 01:51:54,790
this is this is the score but if i combine c c and c together

1624
01:51:54,840 --> 01:51:56,280
i meaning if i put

1625
01:51:56,290 --> 01:51:59,740
this the together and got here you know this is the best of the best

1626
01:51:59,740 --> 01:52:02,780
way i can do it right so it says that if i i can sort

1627
01:52:03,630 --> 01:52:08,130
could put smaller clustering into bigger clusters and that pays off right because i sort

1628
01:52:08,140 --> 01:52:10,010
of get much more edges

1629
01:52:10,040 --> 01:52:12,130
inside and what they have to

1630
01:52:12,820 --> 01:52:19,130
to cut it away OK but again the observation is that again is downward shaping

1631
01:52:22,050 --> 01:52:25,700
similarly if i look at one of those

1632
01:52:25,700 --> 01:52:28,910
so this is this is the one we were looking before this is the hierarchical

1633
01:52:29,000 --> 01:52:34,000
random graphs right we have these hierarchically nested communities where for example the top level

1634
01:52:34,000 --> 01:52:38,700
i have the clusters and then each of these two clusters then internally is composed

1635
01:52:38,700 --> 01:52:42,240
of let's say you know kitty store is a few more and so on and

1636
01:52:42,240 --> 01:52:48,120
again if i do in partition such a network i get this downward downward slope

1637
01:52:48,140 --> 01:52:48,590
where is

1638
01:52:48,990 --> 01:52:53,040
bleeps basically correspond to cutting away clusters

1639
01:52:53,100 --> 01:52:57,210
right so for example this is the last one down here would correspond to cutting

1640
01:52:57,210 --> 01:53:00,090
the so i might get a lot of edges inside and i sort of got

1641
01:53:00,280 --> 01:53:01,410
just a few notes

1642
01:53:01,410 --> 01:53:02,840
just a few edges

1643
01:53:02,850 --> 01:53:06,500
and i know this one which probably corresponds to you know to cutting this

1644
01:53:06,540 --> 01:53:10,390
because here and many more edges and i get a smaller

1645
01:53:10,600 --> 01:53:15,000
so basically natural hypothesis now is that

1646
01:53:15,060 --> 01:53:18,060
when i when i taken at work i would get this

1647
01:53:18,060 --> 01:53:24,210
you could you just wave and if you want to speed up the that

1648
01:53:24,260 --> 01:53:26,120
don't be shy

1649
01:53:26,130 --> 01:53:32,490
the wave and if you want to slow down

1650
01:53:32,500 --> 01:53:36,060
and if you think this is about right

1651
01:53:36,070 --> 01:53:38,590
OK majority

1652
01:53:42,370 --> 01:53:48,780
many questions from what we have covered so far

1653
01:53:48,800 --> 01:53:50,910
it's took little bit about causality

1654
01:53:51,260 --> 01:53:53,150
if you go to the polls book

1655
01:53:53,190 --> 01:53:59,520
you will hear a lot about causality so the relationships and causality in directed graphs

1656
01:53:59,670 --> 01:54:04,360
to directed graphs rather good at expressing causal relationships and

1657
01:54:04,400 --> 01:54:06,560
it's often the case that we have

1658
01:54:06,570 --> 01:54:11,840
child variables which we are which we observe we want to infer the posterior distribution

1659
01:54:11,840 --> 01:54:15,960
of parent variables is a very common set up so something in the physical world

1660
01:54:15,960 --> 01:54:17,620
that we want to know about

1661
01:54:17,630 --> 01:54:22,530
we can't observe directly if we can probably solve the problem is we observed indirectly

1662
01:54:22,530 --> 01:54:27,430
via some other process and or some of the variables we we then condition on

1663
01:54:27,430 --> 01:54:31,270
the observed variable we want to find out about this this hidden variable so

1664
01:54:31,670 --> 01:54:35,490
a person may or may not have cancer we can't measure directly what they have

1665
01:54:35,490 --> 01:54:37,340
cancer or not precisely

1666
01:54:37,360 --> 01:54:39,140
we can do blood test

1667
01:54:39,150 --> 01:54:40,820
and the outcome of the blood test

1668
01:54:40,870 --> 01:54:44,600
will be related to have cancer but it may not be perfect test

1669
01:54:45,080 --> 01:54:47,590
it may be that the blood test is much more likely to be positive if

1670
01:54:47,590 --> 01:54:49,770
they have cancer than if they don't have cancer

1671
01:54:49,780 --> 01:54:54,070
nevertheless in idea stochastic relationship to observe blood test

1672
01:54:54,090 --> 01:54:58,350
outcome and then we want to find the posterior distribution of the have cancer condition

1673
01:54:58,390 --> 01:55:00,760
on that observed blood tests

1674
01:55:01,190 --> 01:55:05,380
as long as you take the trivial graph but will see similar structures in in

1675
01:55:05,400 --> 01:55:09,630
sort of a complicated graphs of practical interest

1676
01:55:09,680 --> 01:55:15,370
one might be tempted to go in the other direction one might be tempted to

1677
01:55:15,370 --> 01:55:20,910
try to infer causality from statistics you might observe that you know people have yellow

1678
01:55:20,910 --> 01:55:25,370
stains on fingers seem more likely to die of cancer

1679
01:55:25,390 --> 01:55:26,700
you might wonder whether

1680
01:55:26,720 --> 01:55:31,940
is it because they smoked cigarettes that's causing stains on fingers and smoking cigarettes causing

1681
01:55:32,930 --> 01:55:36,760
well all we have is the correlation between stained fingers and cancer

1682
01:55:36,770 --> 01:55:41,340
you know which is causing which and so you might be tempted to say well

1683
01:55:41,340 --> 01:55:46,030
we can we can try fitting different graphical models with is going in different directions

1684
01:55:46,250 --> 01:55:50,420
and try to see which fits best and then interpret that is causality that turns

1685
01:55:50,420 --> 01:55:53,530
out to be very subtle i don't consider myself an expert on it but it's

1686
01:55:53,530 --> 01:55:55,240
certainly not a trivial thing to do

1687
01:55:56,060 --> 01:56:00,620
it's extremely subtle really to get a good handle on causality you have to be

1688
01:56:00,620 --> 01:56:03,480
able to intervene in other words you have to be able to take the population

1689
01:56:03,480 --> 01:56:08,690
divided random and stop half of from you know stop people smoking cigarettes who otherwise

1690
01:56:08,690 --> 01:56:12,170
would have or wash their hands or something in order to find out what which

1691
01:56:12,170 --> 01:56:17,230
is causing which intervene you can infer causality but just correlation it's hard to get

1692
01:56:18,980 --> 01:56:21,410
so i mentioned it in passing i'm not going to say

1693
01:56:21,430 --> 01:56:24,020
a great deal about ability just show you

1694
01:56:24,400 --> 01:56:28,780
a little example which also illustrates another point which is to do with the ordering

1695
01:56:28,780 --> 01:56:32,470
of the variables and the fact that there an equivalent

1696
01:56:33,930 --> 01:56:38,030
so in the beginning we started off with the joint distribution of three variables completely

1697
01:56:38,030 --> 01:56:41,800
general one we wrote it very asymmetric way we have p of x p y

1698
01:56:41,800 --> 01:56:46,460
given x z given x and y and so there is an asymmetric relationship on

1699
01:56:46,460 --> 01:56:50,060
the right-hand side between these variables we could have done differently we could equally well

1700
01:56:50,060 --> 01:56:51,430
have written pz

1701
01:56:51,450 --> 01:56:54,640
and the y given z and p of x given y and z and that

1702
01:56:54,640 --> 01:56:59,390
would be different asymmetric relationship represented by different graphs still fully connected with the arrows

1703
01:56:59,390 --> 01:57:02,020
going in different directions so

1704
01:57:02,040 --> 01:57:06,590
this is to be lots of different ways of representing the same situation and here

1705
01:57:06,590 --> 01:57:10,890
are some graphs from a problem i may come from poles book it somewhere in

1706
01:57:10,890 --> 01:57:14,370
the in the literature from way back when

1707
01:57:14,570 --> 01:57:17,540
i won't go through the whole problem to boring these two graphs to illustrate this

1708
01:57:17,540 --> 01:57:22,800
particular point so there the problem to do with with your car will start or

1709
01:57:22,800 --> 01:57:24,320
not so

1710
01:57:24,780 --> 01:57:27,350
you know if your can't to start you have to have fuel

1711
01:57:27,370 --> 01:57:32,970
and battery has to be charged but you don't observe these unions of fuel gauge

1712
01:57:32,980 --> 01:57:36,870
the fuel gauge only works if electricity and it's a bit of an unreliable fuel

1713
01:57:36,870 --> 01:57:42,700
gauges well in this problem so these probabilistic relationships and and the engine maybe not

1714
01:57:44,940 --> 01:57:49,800
so binary variable five binary variables may the describe a joint probability distribution is the

1715
01:57:49,800 --> 01:57:54,370
table of numbers that add up to one that describe the all possible to the

1716
01:57:54,370 --> 01:57:55,610
power five

1717
01:57:55,620 --> 01:57:58,540
instantiations of those five binary variables

1718
01:57:58,550 --> 01:58:01,960
that the joint probability distribution is very special

1719
01:58:02,740 --> 01:58:06,860
lots of conditional independence properties things can be expressed in terms of the graph with

1720
01:58:06,860 --> 01:58:09,760
very few links and that's the graph

1721
01:58:09,780 --> 01:58:10,420
and that

1722
01:58:10,430 --> 01:58:14,740
the model is constructed to reflect this causal relationship we know that the

1723
01:58:15,770 --> 01:58:18,390
with the fuel gauge risk for empty

1724
01:58:18,430 --> 01:58:19,970
is determined by

1725
01:58:19,980 --> 01:58:23,780
whether the battery is charged a flat with the fuel tank is full or empty

1726
01:58:23,810 --> 01:58:30,460
so that those arrows represent the directions of causality we can equally well we decompose

1727
01:58:30,460 --> 01:58:35,740
the joint distribution into factors going in the direction we start with probability of start

1728
01:58:36,000 --> 01:58:38,530
me probably battery given start and so on

1729
01:58:38,540 --> 01:58:41,620
in this case the exact same joint distribution

1730
01:58:41,650 --> 01:58:44,560
is described by fully connected graph

1731
01:58:44,580 --> 01:58:47,990
so these are all equivalent to looking at that graph it tells you nothing

1732
01:58:48,010 --> 01:58:51,820
was that graph tells you a great deal because there are many missing links

1733
01:58:51,830 --> 01:58:56,120
so making two points here the first point is that if you change the ordering

1734
01:58:56,120 --> 01:58:57,400
of the variables

1735
01:58:57,410 --> 01:59:01,970
if you perfectly liberty to do that you end up with the different graph the

1736
01:59:01,970 --> 01:59:04,890
graph has a lot more links because a lot less useful

1737
01:59:05,020 --> 01:59:09,060
this is the absence of links that's the useful property of graphs

1738
01:59:09,150 --> 01:59:11,420
and the other point is that

1739
01:59:11,470 --> 01:59:13,790
the directions of the arrows

1740
01:59:13,810 --> 01:59:18,740
can reflect causality not necessarily but in situation where modeling the world in their causal

1741
01:59:18,740 --> 01:59:22,640
relationships the arrows follow the directions of causality

1742
01:59:25,800 --> 01:59:31,880
so up to now we've talked about graphs as expects as expressing the factorisation properties

1743
01:59:31,880 --> 01:59:36,920
of the joint distribution decomposing a probability distribution into a product of factors when i'm

1744
01:59:36,920 --> 01:59:38,420
going to talk about another

1745
01:59:38,430 --> 01:59:43,210
class and properties of joint distributions namely conditional independence properties

1746
01:59:43,220 --> 01:59:48,470
and then we'll see there an intimate relationship between conditional independence and factorisation of furthermore

1747
01:59:48,470 --> 01:59:53,480
that relationship can be understood graphically just by inspecting the diagram without having to grind

1748
01:59:53,480 --> 01:59:57,720
through lots of bayes theorem in mathematics

1749
01:59:57,740 --> 02:00:00,860
so the definition of this is the following imagine we've got

1750
02:00:00,880 --> 02:00:05,240
a joint probability distribution over three variables all three sets of variables x y and

1751
02:00:05,240 --> 02:00:11,940
z again this could be mixtures of combinations of discrete and continuous

1752
02:00:12,300 --> 02:00:16,350
then if the conditional distribution of x given y and z

1753
02:00:16,370 --> 02:00:18,390
doesn't depend on y

1754
02:00:18,410 --> 02:00:21,780
we say that the conditional independence property we say that

1755
02:00:23,830 --> 02:00:27,710
independent of y given z

1756
02:00:27,730 --> 02:00:32,820
so what that means that this whole for all possible choices the variable z so

1757
02:00:32,820 --> 02:00:37,380
imagine these with discrete variables should be just a big table of numbers giving the

1758
02:00:37,380 --> 02:00:38,600
value of p

1759
02:00:38,620 --> 02:00:40,130
for every possible

1760
02:00:40,150 --> 02:00:42,540
choice of the variables x y and z

1761
02:00:42,540 --> 02:00:47,490
one is used

1762
02:00:47,510 --> 02:00:48,680
so we have

1763
02:00:48,700 --> 02:00:49,310
it is this

1764
02:00:49,330 --> 02:00:50,200
good is that this

1765
02:00:50,220 --> 02:00:55,060
quite so to some discrete for this

1766
02:00:56,160 --> 02:00:59,390
and the question is how we do that

1767
02:00:59,410 --> 02:01:00,930
it's actually a lot

1768
02:01:04,010 --> 02:01:05,040
and i

1769
02:01:05,060 --> 02:01:08,830
which was

1770
02:01:10,560 --> 02:01:11,890
by far

1771
02:01:11,910 --> 02:01:20,640
by my actually was the first one was just simple a bright you

1772
02:01:24,320 --> 02:01:26,990
the students were well

1773
02:01:27,950 --> 02:01:31,140
human way to describe colors

1774
02:01:31,160 --> 02:01:32,200
we can think

1775
02:01:32,220 --> 02:01:34,930
the problem is that all line

1776
02:01:34,950 --> 02:01:39,120
we can say that you know if it is way you

1777
02:01:40,870 --> 02:01:43,970
also wish to

1778
02:01:45,470 --> 02:01:51,310
which is close to disagree with

1779
02:01:53,310 --> 02:01:55,560
because it's fun

1780
02:01:55,620 --> 02:01:59,310
after five color

1781
02:02:02,240 --> 02:02:05,720
so you want to make changes

1782
02:02:07,220 --> 02:02:11,060
transform which banned space

1783
02:02:12,870 --> 02:02:13,930
for example

1784
02:02:13,970 --> 02:02:17,330
but there is much more useful

1785
02:02:17,350 --> 02:02:22,600
in image processing because if we want

1786
02:02:24,890 --> 02:02:32,180
which is why we can somehow or other the known fact that we've just like

1787
02:02:32,450 --> 02:02:33,950
what i mean

1788
02:02:33,970 --> 02:02:42,350
so when we build this is the division one these we can make the

1789
02:02:42,450 --> 02:02:46,450
in the problem i mean

1790
02:02:46,470 --> 02:02:47,760
so here

1791
02:02:47,890 --> 02:02:54,240
just for the size of colors to choose a candidate for the

1792
02:02:54,260 --> 02:02:56,180
like like

1793
02:02:57,830 --> 02:02:59,640
so we have

1794
02:02:59,660 --> 02:03:02,510
x y and

1795
02:03:02,620 --> 02:03:06,490
because it is

1796
02:03:07,580 --> 02:03:09,370
this point

1797
02:03:09,370 --> 02:03:10,390
the most right

1798
02:03:11,100 --> 02:03:13,760
and the end the

1799
02:03:13,910 --> 02:03:17,370
describe the heel

1800
02:03:17,580 --> 02:03:19,580
o actually

1801
02:03:20,580 --> 02:03:25,370
they have different ways to calculate the amount

1802
02:03:26,180 --> 02:03:27,240
so four

1803
02:03:30,100 --> 02:03:32,580
but they

1804
02:03:32,600 --> 02:03:34,910
and the college

1805
02:03:36,510 --> 02:03:39,390
two crew

1806
02:03:46,760 --> 02:03:49,990
the last column

1807
02:03:50,030 --> 02:03:53,410
the thing

1808
02:03:56,040 --> 02:04:00,060
after the

1809
02:04:04,740 --> 02:04:07,180
this is called

1810
02:04:07,330 --> 02:04:09,580
it also

1811
02:04:09,620 --> 02:04:12,180
like called

1812
02:04:12,180 --> 02:04:14,580
the great using her

1813
02:04:16,140 --> 02:04:18,580
i for more actions

1814
02:04:18,830 --> 02:04:20,560
i think

1815
02:04:22,220 --> 02:04:24,620
and this is known to be

1816
02:04:24,620 --> 02:04:27,390
the most perception of colors

1817
02:04:27,450 --> 02:04:28,950
what does it mean

1818
02:04:28,950 --> 02:04:31,430
actually if you want to compare

1819
02:04:31,450 --> 02:04:32,830
two color

1820
02:04:32,830 --> 02:04:37,240
we want to be able to quickly because between columns and you want to use

1821
02:04:38,120 --> 02:04:41,870
correspond well to the human perception

1822
02:04:43,040 --> 02:04:45,760
for the you know

1823
02:04:45,760 --> 02:04:47,310
it's quite fast

1824
02:04:47,390 --> 02:04:52,390
the green line light quite close to each other so we need to

1825
02:04:52,410 --> 02:04:57,530
have a measure which this form of deception and then they

1826
02:04:57,580 --> 02:05:00,200
they are such

1827
02:05:00,200 --> 02:05:05,540
that means that euclidean distance in this space actually corresponds to

1828
02:05:06,660 --> 02:05:11,410
inputs for the because way to at go

1829
02:05:13,870 --> 02:05:15,580
it's not

1830
02:05:17,790 --> 02:05:19,740
but much more

1831
02:05:20,470 --> 02:05:23,260
one of our policies

1832
02:05:23,310 --> 02:05:26,310
it was proposed that the colors

1833
02:05:28,390 --> 02:05:30,330
like i

1834
02:05:31,450 --> 02:05:32,160
and the difference

1835
02:05:32,210 --> 02:05:34,910
a lot is the following

1836
02:05:34,910 --> 02:05:35,910
you can

1837
02:05:35,950 --> 02:05:38,770
that is like

1838
02:05:38,790 --> 02:05:43,930
you have to call the color me

1839
02:05:43,950 --> 02:05:45,580
criminal romantic

1840
02:05:46,470 --> 02:05:47,830
changes just

1841
02:05:47,850 --> 02:05:49,450
we need to

1842
02:05:49,470 --> 02:05:53,100
the guy who wrote the

1843
02:05:53,120 --> 02:05:55,350
you what are

1844
02:05:58,430 --> 02:06:00,970
if we

1845
02:06:01,180 --> 02:06:04,580
the point cloud

1846
02:06:04,600 --> 02:06:06,220
you may

1847
02:06:06,240 --> 02:06:08,970
just like

1848
02:06:08,990 --> 02:06:12,410
along this line again returned

1849
02:06:12,800 --> 02:06:15,870
let's try to colors

1850
02:06:15,970 --> 02:06:17,010
one of the problem

1851
02:06:17,040 --> 02:06:20,760
was situated about and actually we it

1852
02:06:20,770 --> 02:06:22,910
the human soul

1853
02:06:24,510 --> 02:06:28,740
if you measured usually by and the other o

1854
02:06:28,950 --> 02:06:30,620
the like x

1855
02:06:30,680 --> 02:06:34,200
if you think you can we just

1856
02:06:34,200 --> 02:06:38,090
if you do for the same

1857
02:06:38,120 --> 02:06:41,210
composition composite i'm thinking composing

1858
02:06:41,350 --> 02:06:45,030
what i mean is that i compose every function from the set here

1859
02:06:45,050 --> 02:06:46,890
with this fixed function here

1860
02:06:46,910 --> 02:06:49,010
where this fixed function means

1861
02:06:49,030 --> 02:06:51,780
essentially apply a try one

1862
02:06:51,790 --> 02:06:54,020
to the function

1863
02:06:54,980 --> 02:07:00,400
that information politics everyone happy with that

1864
02:07:02,320 --> 02:07:06,590
so this is just the application of rademacher complexity in our essentially to complete the

1865
02:07:07,650 --> 02:07:10,430
we need to get rid of this minus ones

1866
02:07:10,440 --> 02:07:15,140
we need to estimate this empirical value and we need to compute the rademacher complexity

1867
02:07:15,140 --> 02:07:16,470
and we done that's

1868
02:07:18,040 --> 02:07:21,300
let's let's do those steps

1869
02:07:21,320 --> 02:07:27,350
well the first thing to observe is that on the training set the

1870
02:07:28,010 --> 02:07:29,380
the value of this

1871
02:07:32,450 --> 02:07:34,650
threshold you know piecewise linear

1872
02:07:34,670 --> 02:07:37,210
approximation of the heaviside function

1873
02:07:37,230 --> 02:07:40,440
it's just the slack variable divided by gamma

1874
02:07:40,710 --> 02:07:42,930
one of the gamma comes from the slope

1875
02:07:42,950 --> 02:07:45,490
i is just the

1876
02:07:45,510 --> 02:07:48,870
the way we define psi i that's the way it comes out or other is

1877
02:07:48,880 --> 02:07:52,620
less than or equal to that it's it's in the region

1878
02:07:52,640 --> 02:07:57,020
less than zero then it's it's equal to that of course bigger than zero it

1879
02:07:57,020 --> 02:08:00,020
will keep going up straight so it's less than

1880
02:08:00,160 --> 02:08:03,910
so we can substitute that into here

1881
02:08:03,930 --> 02:08:07,400
and we get one of the m gamma something so i i

1882
02:08:07,450 --> 02:08:11,850
the minus ones from this one can the minus ones from this one so they

1883
02:08:11,850 --> 02:08:13,630
just go away

1884
02:08:13,650 --> 02:08:15,410
we just need the minus one

1885
02:08:15,430 --> 02:08:18,360
for you will see in a moment y

1886
02:08:18,420 --> 02:08:20,340
let me just go back

1887
02:08:20,420 --> 02:08:25,020
that minus one here which just spanish that minus one here because the expectations of

1888
02:08:25,020 --> 02:08:25,930
the sun

1889
02:08:25,950 --> 02:08:28,830
or the difference of two things is just expectations

1890
02:08:28,880 --> 02:08:34,670
that will just cancel this is just i over the empirical estimate this is just

1891
02:08:34,710 --> 02:08:37,110
XII over

1892
02:08:38,170 --> 02:08:40,750
so basically what we have is

1893
02:08:41,910 --> 02:08:44,670
of course i i some of the slack variables

1894
02:08:44,720 --> 02:08:47,130
which is the thing they were trying to minimize anyway

1895
02:08:47,150 --> 02:08:48,900
is one of the gamma

1896
02:08:48,950 --> 02:08:50,850
and this rademacher complexity

1897
02:08:52,460 --> 02:08:56,910
so it's all looking very good reason we had that minus one is that what

1898
02:08:56,910 --> 02:09:00,200
we're going to have to do to ban this rademacher complexity

1899
02:09:00,230 --> 02:09:01,770
is too

1900
02:09:01,790 --> 02:09:02,850
it is to appear

1901
02:09:03,140 --> 02:09:08,530
make general results about the composition of functions with a fixed function the function class

1902
02:09:08,530 --> 02:09:09,900
with a fixed function

1903
02:09:09,910 --> 02:09:12,440
and that will rely on the fact that the

1904
02:09:12,460 --> 02:09:16,060
function satisfies the effort of zero

1905
02:09:16,070 --> 02:09:20,470
is equal to zero so we have to subtract one to make the true

1906
02:09:20,490 --> 02:09:24,190
so what want to do now i'm i think is

1907
02:09:24,210 --> 02:09:26,320
just sketch the

1908
02:09:26,350 --> 02:09:30,680
steps stages i mean i want to look a little bit rademacher complexity

1909
02:09:30,700 --> 02:09:36,240
and show you a little bit of what's involved in this proof now

1910
02:09:36,260 --> 02:09:37,800
i think this is

1911
02:09:38,660 --> 02:09:40,810
quite an interesting case

1912
02:09:40,820 --> 02:09:42,780
or when

1913
02:09:45,420 --> 02:09:47,820
OK so this this is the proof of this

1914
02:09:49,620 --> 02:09:51,720
is alluded to in the paper by

1915
02:09:51,740 --> 02:09:55,340
mandelson bartlett and

1916
02:09:55,360 --> 02:09:56,960
it's part of

1917
02:09:56,980 --> 02:09:59,390
general sort of

1918
02:09:59,410 --> 02:10:03,610
remember in which there are several properties the rademacher complexity listed

1919
02:10:03,710 --> 02:10:06,730
most of which are fairly straightforward actually this one

1920
02:10:06,750 --> 02:10:12,360
the first two books by the do around and is really quite

1921
02:10:12,380 --> 02:10:15,440
if you really want to try and follow through all the steps in the proof

1922
02:10:15,440 --> 02:10:18,580
you have to delve into this rather

1923
02:10:18,810 --> 02:10:23,650
difficult text and you know it's really quite a lot of detail and that you

1924
02:10:23,650 --> 02:10:24,800
have to master

1925
02:10:26,890 --> 02:10:28,600
i think

1926
02:10:28,610 --> 02:10:32,340
it was sort of nice to try and get a direct proof so

1927
02:10:32,360 --> 02:10:34,360
what i want done with

1928
02:10:34,370 --> 02:10:37,700
collaborator random alliances come up with the

1929
02:10:37,710 --> 02:10:40,740
if you like an elementary proof of this fact

1930
02:10:40,750 --> 02:10:43,410
i was going to go through it and i don't think i've got time but

1931
02:10:43,410 --> 02:10:44,620
i've got slides

1932
02:10:44,630 --> 02:10:47,860
so there are no on the wiki or in your notes are you able to

1933
02:10:47,860 --> 02:10:51,240
look through if you're interested in but i think it's it's nice to see that

1934
02:10:51,240 --> 02:10:52,590
something that you know

1935
02:10:53,280 --> 02:10:55,730
he just break it down step by step actually

1936
02:10:55,750 --> 02:10:59,430
there is a quite straightforward sort of elementary proof of this fact

1937
02:10:59,440 --> 02:11:02,750
but i just wanted to sort of illustrate the kind of things and just to

1938
02:11:02,750 --> 02:11:05,830
give you a flavor of the kind of things that you can do with rademacher

1939
02:11:06,910 --> 02:11:11,040
and that hopefully will set you up to the truth if you're interested

1940
02:11:11,060 --> 02:11:13,070
in looking at it in detail

1941
02:11:15,020 --> 02:11:18,810
just the kind of first example of things you can get if you've got a

1942
02:11:18,920 --> 02:11:22,940
real number and you multiply all the functions in the function class f by the

1943
02:11:23,060 --> 02:11:24,000
real numbers

1944
02:11:24,050 --> 02:11:26,320
the rademacher complexity goes up by

1945
02:11:26,330 --> 02:11:27,300
a factor

1946
02:11:27,350 --> 02:11:32,380
the modulus of that and that's follows from the fact that if i have is

1947
02:11:32,380 --> 02:11:35,110
the function achieving the suit for some sigma

1948
02:11:35,120 --> 02:11:40,110
then for f then it will that will be true if and only if f

1949
02:11:40,110 --> 02:11:41,250
which is the suit

1950
02:11:41,290 --> 02:11:44,760
for the function class and so it's just a very straightforward

1951
02:11:44,780 --> 02:11:46,540
looking at the way the

1952
02:11:46,560 --> 02:11:50,390
choice of f in that suit is chosen and it's clear that

1953
02:11:50,590 --> 02:11:52,530
and we will do the job for you

1954
02:11:53,200 --> 02:11:57,870
if we were interested in bounding

1955
02:11:57,890 --> 02:12:01,740
the rademacher complexity this composition of

1956
02:12:01,820 --> 02:12:04,740
functions from f with a fixed function l

1957
02:12:04,760 --> 02:12:06,230
where l

1958
02:12:07,490 --> 02:12:10,720
a function that satisfies this lipschitz condition

1959
02:12:10,730 --> 02:12:14,830
that is that the amount the output of the function changes is bounded by some

1960
02:12:14,850 --> 02:12:17,440
constant times the difference between the inputs

1961
02:12:17,460 --> 02:12:18,570
in our case

1962
02:12:18,590 --> 02:12:21,660
the function was this piecewise linear function

1963
02:12:21,670 --> 02:12:25,570
and the species was one of the gamma because that was the slope

1964
02:12:25,580 --> 02:12:28,000
that section went up from zero to one

1965
02:12:28,010 --> 02:12:31,450
just before this year so the l in our case will be one of the

1966
02:12:32,580 --> 02:12:34,400
and we're playing that function

1967
02:12:35,330 --> 02:12:37,700
all the functions in the function class

1968
02:12:37,740 --> 02:12:41,410
and we require this property that allows zero zero that's why we have a minus

1969
02:12:41,410 --> 02:12:44,010
one can become complicated

1970
02:12:44,020 --> 02:12:48,500
and basically what one can show is that there less than two times

1971
02:12:48,510 --> 02:12:54,220
the lipschitz constant times the rademacher complexity of the base class

1972
02:12:54,230 --> 02:12:57,880
so that actually is the result we need

1973
02:12:57,900 --> 02:13:01,690
because now we're in a position to put the

1974
02:13:01,710 --> 02:13:03,030
bound together

1975
02:13:03,050 --> 02:13:04,720
now as i said i

1976
02:13:04,740 --> 02:13:08,650
i think i haven't got time because i'd like to talk about kernel PCA and

1977
02:13:08,720 --> 02:13:11,140
prove more interesting is another application

1978
02:13:11,180 --> 02:13:13,150
the rest of the slides here

1979
02:13:13,890 --> 02:13:18,280
just going through breaking down the problem proving it step by step and

1980
02:13:18,300 --> 02:13:21,780
you'll see that actually each step is really quite straightforward

1981
02:13:21,830 --> 02:13:25,010
so i'll leave that out

1982
02:13:26,130 --> 02:13:27,940
so that's the end

1983
02:13:27,950 --> 02:13:30,760
so here's the final bound we get out

1984
02:13:31,450 --> 02:13:38,240
remember the rademacher complexity of the basic cost was two over times this square roots

1985
02:13:38,260 --> 02:13:39,430
and the ship

1986
02:13:39,610 --> 02:13:42,640
constant was

1987
02:13:44,290 --> 02:13:50,180
one over gamma and we had to add to two times lipschitz constant for the

1988
02:13:50,180 --> 02:13:51,100
m gamma

1989
02:13:51,160 --> 02:13:52,230
times this

1990
02:13:53,050 --> 02:13:57,460
so that's the final bound you get out the probability of misclassification

1991
02:13:57,500 --> 02:13:59,870
this is given by this quantity here

1992
02:13:59,890 --> 02:14:04,720
so we've got a very nice involvement to the two parameters that were optimizing in

1993
02:14:05,720 --> 02:14:09,400
in the optimisation that we're trying to solve for the SVM

1994
02:14:09,410 --> 02:14:11,380
we've got began come in

1995
02:14:11,400 --> 02:14:14,340
inverse so obviously want to minimize gamma

1996
02:14:14,360 --> 02:14:16,460
and so maximise gamma

1997
02:14:16,470 --> 02:14:20,450
and we've got this some of the slack variables which we're trying to optimize

1998
02:14:20,460 --> 02:14:24,740
as the second moment coming in so it's very clear motivation

1999
02:14:26,310 --> 02:14:32,280
the optimisation of trying to solve furthermore has this nice property that we

2000
02:14:32,290 --> 02:14:34,050
actually measuring the

2001
02:14:34,050 --> 02:14:37,010
which guarantees that other satisfied

2002
02:14:37,030 --> 02:14:41,380
otherwise i flip available that maximizes the number of satisfied clauses OK so this is

2003
02:14:41,380 --> 02:14:43,010
the greatest

2004
02:14:43,030 --> 02:14:46,700
and i find lucky at some point in this process i will have a solution

2005
02:14:46,760 --> 02:14:50,150
if i'm like you have a solution and the disadvantage of these types of messages

2006
02:14:50,150 --> 02:14:52,610
that i never know

2007
02:14:53,300 --> 02:14:56,530
there really is no solution i are just didn't find one in the time that

2008
02:14:56,530 --> 02:14:57,610
i have available

2009
02:14:58,390 --> 02:15:01,010
so if you really want that and so if you want the negative answer you

2010
02:15:01,010 --> 02:15:04,860
need to use the backtracking methods the this advantage of this method are that it

2011
02:15:04,860 --> 02:15:06,610
tend to be a lot faster

2012
02:15:06,630 --> 02:15:10,360
OK so scale to really large problems which is what we want here walksat to

2013
02:15:10,360 --> 02:15:12,340
be very good

2014
02:15:12,360 --> 02:15:13,880
OK so

2015
02:15:13,890 --> 02:15:16,630
finally let's look at inductive logic programming

2016
02:15:16,660 --> 02:15:19,490
so the question i like how do we learn

2017
02:15:19,950 --> 02:15:22,740
knowledge bases in first order logic from data

2018
02:15:23,360 --> 02:15:28,050
and probably the best way to introduce this is to start by looking at propositional

2019
02:15:28,050 --> 02:15:29,050
rule induction

2020
02:15:29,070 --> 02:15:32,110
so rule induction over single table

2021
02:15:32,110 --> 02:15:36,180
so we have a set of positive and negative examples of some concept let's say

2022
02:15:36,180 --> 02:15:40,180
spam no spam emails and non spam emails

2023
02:15:40,240 --> 02:15:45,450
each example is a is is the concept of class y

2024
02:15:45,470 --> 02:15:49,910
i'm going to so its bullion and a set of attributes again i'm going to

2025
02:15:49,910 --> 02:15:53,530
some its boolean so why would be spam nonspam and x

2026
02:15:53,550 --> 02:15:57,430
to what extent would be the presence of different words like the word free for

2027
02:15:57,430 --> 02:16:01,950
example is probably a very good indication of that something is spam

2028
02:16:01,970 --> 02:16:04,340
i still

2029
02:16:04,360 --> 02:16:08,760
so the goal in rule induction is to induce a set of rules that cover

2030
02:16:08,760 --> 02:16:10,510
all positive examples

2031
02:16:10,530 --> 02:16:13,090
but none of the negative one

2032
02:16:13,090 --> 02:16:14,660
and the rule is just

2033
02:16:14,680 --> 02:16:16,700
a body implies the heads

2034
02:16:16,720 --> 02:16:18,740
otherwise known as horn clause

2035
02:16:18,760 --> 02:16:21,970
so the body is a conjunction of tests

2036
02:16:22,030 --> 02:16:24,380
which can be literals

2037
02:16:24,410 --> 02:16:28,590
meaning in all these values or their negations and the head is the class

2038
02:16:28,610 --> 02:16:33,340
and i'm going to say the rule covers an example if example satisfies the body

2039
02:16:33,340 --> 02:16:34,430
of r

2040
02:16:35,300 --> 02:16:38,970
and in order to the learning i'm going to need some evaluation measure for a

2041
02:16:38,970 --> 02:16:43,700
rule like you know its accuracy information gain coverage support you can use different things

2042
02:16:44,570 --> 02:16:48,360
so the way the way rules you know there are many algorithms for this but

2043
02:16:48,360 --> 02:16:52,090
i like the most widely used one that looks like this

2044
02:16:52,110 --> 02:16:55,780
i'm going to learn a single rule one antecedent at time

2045
02:16:55,800 --> 02:16:58,780
and then you know the rules set by learning one related time

2046
02:16:58,800 --> 02:17:00,680
OK so how do i learn a single rule

2047
02:17:00,700 --> 02:17:05,090
i started out with the heading the class and then antibody meaning that everything matches

2048
02:17:05,090 --> 02:17:05,930
the rule

2049
02:17:05,950 --> 02:17:08,130
and then what i do is

2050
02:17:08,150 --> 02:17:09,740
i look at every literal

2051
02:17:09,760 --> 02:17:13,780
i try adding it to the rule evaluate with the results and then i have

2052
02:17:13,800 --> 02:17:17,860
the literal that gives me the best results i added little to the rule and

2053
02:17:17,860 --> 02:17:23,320
i can repeat and another little to the rule until no literally improves the evaluation

2054
02:17:23,320 --> 02:17:25,240
and then i returned the all that i found

2055
02:17:25,260 --> 02:17:28,220
OK this not plugs into the larger with

2056
02:17:28,260 --> 02:17:30,800
often known as separate and conquer

2057
02:17:30,820 --> 02:17:34,430
which goes like this i start with an empty rule set

2058
02:17:34,430 --> 02:17:36,930
and with with my set s

2059
02:17:36,950 --> 02:17:39,550
which is all the all the examples

2060
02:17:39,570 --> 02:17:42,360
and i repeat the following single rule

2061
02:17:42,390 --> 02:17:44,070
the way that i just described

2062
02:17:44,090 --> 02:17:46,610
i have a rule to the rule set

2063
02:17:46,630 --> 02:17:50,300
and then i delete from the same as the positive examples that i discovered by

2064
02:17:50,320 --> 02:17:51,860
separate them out

2065
02:17:51,970 --> 02:17:54,030
because those those are accounted for

2066
02:17:54,030 --> 02:17:57,010
now and then i'm going to try to learn another rule to cover the remainder

2067
02:17:57,030 --> 02:17:58,660
of the positive examples

2068
02:17:59,390 --> 02:18:04,010
and i i keep on doing this until hopefully actually this

2069
02:18:04,010 --> 02:18:08,220
it's not until the cities states until the set of all incoming no positive examples

2070
02:18:08,220 --> 02:18:09,360
left to come

2071
02:18:13,650 --> 02:18:19,220
he lenses and we were sitting with this idea the most likely cause

2072
02:18:20,550 --> 02:18:25,910
and five that that's actually and so the question here is like how this rule

2073
02:18:25,930 --> 02:18:30,840
is this is similar to boosting this is actually a lot older than boosting decades

2074
02:18:31,510 --> 02:18:35,950
but there's an interesting relationship to fact you can use boosting to learn rule sets

2075
02:18:35,950 --> 02:18:39,550
by taking a single rule as the model that you posted that can give good

2076
02:18:40,490 --> 02:18:44,220
so in boosting in this instance what happens is that i don't completely take examples

2077
02:18:44,220 --> 02:18:48,410
i just don't like the so yes there is an interesting relation between the two

2078
02:18:48,430 --> 02:18:50,990
as i said this is only the most basic of them it's not in know

2079
02:18:51,030 --> 02:18:52,430
the most sophisticated

2080
02:18:52,570 --> 02:18:56,070
so not only the first of the rule induction

2081
02:18:56,070 --> 02:19:00,320
right now instead of just these boolean attributes i have arbitrary predicates with arguments

2082
02:19:00,340 --> 02:19:01,910
you know

2083
02:19:01,930 --> 02:19:05,930
thing not very complicated the good news is that we can actually take the time

2084
02:19:05,930 --> 02:19:06,930
that we just saw

2085
02:19:06,950 --> 02:19:10,970
and transferred to this more powerful language with only a couple of changes

2086
02:19:10,970 --> 02:19:16,030
so now i y index i have predicates with arguments a wise ancestor x y

2087
02:19:16,050 --> 02:19:20,200
so we want you know to learn the rules that pretty consistent x y

2088
02:19:20,220 --> 02:19:23,260
from things like you know parent x y

2089
02:19:24,160 --> 02:19:26,150
so now what's going to happen is that

2090
02:19:26,160 --> 02:19:31,720
when i grew a rule what i'm trying on is not attributes its literals

2091
02:19:31,740 --> 02:19:35,050
and again i can try adding all literals in the language

2092
02:19:35,130 --> 02:19:40,990
with one caveat which is actually make the restriction that i can only add literals

2093
02:19:40,990 --> 02:19:43,070
that share least one variable

2094
02:19:43,070 --> 02:19:45,550
with the literals that are already in the rule

2095
02:19:45,570 --> 02:19:48,910
because if they don't then basically the neutral says nothing about the things that i

2096
02:19:48,930 --> 02:19:52,050
think that i'm interested in in so i think this is reasonable

2097
02:19:52,070 --> 02:19:53,450
restriction to make

2098
02:19:53,470 --> 02:19:55,570
and the other thing that we need to two

2099
02:19:55,570 --> 02:19:59,860
what about is that adding a literal actually changes the number of groundings of the

2100
02:20:01,680 --> 02:20:03,550
in propositional learning

2101
02:20:03,590 --> 02:20:07,160
the space of examples that was learning rules over was always the same

2102
02:20:07,180 --> 02:20:08,820
it was the set of objects

2103
02:20:08,880 --> 02:20:12,590
that i have in mind here it's a little more subtle notice suppose that i'm

2104
02:20:12,590 --> 02:20:14,990
building a rule for ancestor x y

2105
02:20:15,110 --> 02:20:19,110
what is the set of things of this with the set of pairs of people

2106
02:20:19,820 --> 02:20:22,800
such that one could be the ancestor of the other

2107
02:20:22,800 --> 02:20:26,610
but if i now have this little parents see why not simon being that rule

2108
02:20:26,610 --> 02:20:30,860
here because it has one common with this guy now the set ground possible landings

2109
02:20:30,860 --> 02:20:34,240
of this rule is the set of triples x y z

2110
02:20:34,300 --> 02:20:38,260
so not just suddenly you know greatly expanded myspace possibilities

2111
02:20:38,300 --> 02:20:41,320
so to be easy to fool myself that i'm doing very well because i'm just

2112
02:20:41,320 --> 02:20:44,840
creating a lot of positive groundings but that doesn't actually mean anything

2113
02:20:44,840 --> 02:20:48,630
so what i need to do is i need to change my evaluation function to

2114
02:20:48,630 --> 02:20:50,930
take this effect into account

2115
02:20:50,950 --> 02:20:54,700
and again there's many ways to do this but one simple heuristic one that works

2116
02:20:54,700 --> 02:20:57,950
fairly well is i just take a vow

2117
02:20:57,970 --> 02:21:01,820
and i multiplied by the number of positive groundings of the original rule there are

2118
02:21:01,820 --> 02:21:04,890
still covered after adding the

2119
02:21:04,910 --> 02:21:08,320
and the idea behind this heuristic is that well that's what i care about

2120
02:21:09,240 --> 02:21:12,720
if the total number of things has gone up but but the number of positive

2121
02:21:12,720 --> 02:21:16,450
groundings has gone down actually and you know i'm more interested in the latter

2122
02:21:16,450 --> 02:21:21,470
what we do

2123
02:21:23,350 --> 02:21:26,800
you mean the qn

2124
02:21:26,820 --> 02:21:30,570
all or are in

2125
02:21:30,590 --> 02:21:34,970
PM right OK whether it's the bayesian idea that there's some

2126
02:21:34,990 --> 02:21:36,180
you know

2127
02:21:37,800 --> 02:21:44,320
belief about the likelihood that different functions will arise in a particular scenario that you're

2128
02:21:45,300 --> 02:21:49,620
so if you knew which function was gonna rise of machine learning to learn so

2129
02:21:49,640 --> 02:21:50,470
you're done

2130
02:21:50,470 --> 02:21:55,220
if you don't then you may have some belief about what functions are likely or

2131
02:21:55,220 --> 02:21:56,620
more likely to occur

2132
02:21:56,680 --> 02:21:59,180
and that is what pn is giving you

2133
02:21:59,200 --> 02:22:01,050
of course you know there are various

2134
02:22:01,070 --> 02:22:05,610
philosophical interpretations of what that belief means you know because you can't repeat the experiment

2135
02:22:05,610 --> 02:22:07,010
in some sense

2136
02:22:07,090 --> 02:22:10,340
that that you know i i wouldn't

2137
02:22:10,350 --> 02:22:14,990
be able to give very informed views about that but

2138
02:22:15,070 --> 02:22:25,800
yes exactly but this is the sorry the the idea is that function

2139
02:22:25,820 --> 02:22:30,120
that distribution is defined before you see the training example before you know when you

2140
02:22:30,120 --> 02:22:31,890
arrive and you say

2141
02:22:31,930 --> 02:22:34,140
i'm going to study i don't know

2142
02:22:34,220 --> 02:22:36,200
biological networks

2143
02:22:36,220 --> 02:22:40,260
you know then i have some prior knowledge that tells me the likelihood of certain

2144
02:22:40,260 --> 02:22:42,280
functions arising

2145
02:22:42,340 --> 02:22:45,640
and i have to think of all that and put it out there

2146
02:22:45,700 --> 02:22:47,140
ahead of time

2147
02:22:47,180 --> 02:22:50,320
and then i get my training sample try and

2148
02:22:50,550 --> 02:22:53,370
find the best function

2149
02:22:53,370 --> 02:22:57,160
biased without prior probability

2150
02:22:57,700 --> 02:23:00,720
that's not to use it

2151
02:23:03,570 --> 02:23:07,070
the only way that

2152
02:23:07,120 --> 02:23:09,660
shown in the

2153
02:23:09,680 --> 02:23:11,840
but given

2154
02:23:11,840 --> 02:23:13,870
the probability of mean

2155
02:23:13,930 --> 02:23:17,470
i mean what you know

2156
02:23:17,530 --> 02:23:23,410
of course it is

2157
02:23:24,800 --> 02:23:27,300
i think the

2158
02:23:27,320 --> 02:23:31,120
the assumption is that you know very little

2159
02:23:32,260 --> 02:23:36,870
i mean the assumption there is only the relationship between x and y that you're

2160
02:23:36,870 --> 02:23:41,370
assuming you know something about not assuming anything about the distribution

2161
02:23:41,390 --> 02:23:42,640
of x

2162
02:23:42,680 --> 02:23:45,410
so there i guess is way

2163
02:23:49,220 --> 02:23:53,570
the original set of these

2164
02:23:53,590 --> 02:23:57,470
that we can go away

2165
02:23:57,490 --> 02:23:58,740
air force

2166
02:23:58,800 --> 02:24:01,050
well because

2167
02:24:01,840 --> 02:24:06,470
and when he is wrong

2168
02:24:07,820 --> 02:24:12,430
it is not to say that prior

2169
02:24:12,610 --> 02:24:16,050
the prior was

2170
02:24:16,950 --> 02:24:17,780
the mall

2171
02:24:28,030 --> 02:24:31,930
i think it's only a prior over the relationship between x and y that peaceful

2172
02:24:31,930 --> 02:24:35,780
will satisfy are not an assumption about the

2173
02:24:35,800 --> 02:24:37,820
distributions over the

2174
02:24:37,840 --> 02:24:42,240
the marginal distribution of x there an assumption made about that

2175
02:24:42,300 --> 02:24:46,780
he said mean i mean what you're doing is saying is i'm expecting these types

2176
02:24:46,780 --> 02:24:50,300
of functions to rise those functions will tell us given the next how to get

2177
02:24:50,300 --> 02:24:51,620
the y out

2178
02:24:51,760 --> 02:25:00,450
one tell you anything about the distribution over acts independently of one

2179
02:25:00,450 --> 02:25:02,720
i mean that's the same answer just a moment ago

2180
02:25:02,780 --> 02:25:05,180
the idea

2181
02:25:05,200 --> 02:25:08,660
we can talk about offline

2182
02:25:08,660 --> 02:25:14,680
not convinced like so you can move on if you come back

2183
02:25:23,300 --> 02:25:27,200
right so

2184
02:25:27,220 --> 02:25:31,110
OK i'm fortunate have this you know what i would need is the bound with

2185
02:25:31,760 --> 02:25:35,930
training error in here but imagine is term in here with the training error

2186
02:25:35,950 --> 02:25:37,910
plus this quantity here

2187
02:25:38,590 --> 02:25:45,220
so now if i have a particular choice of q i will minimize this combination

2188
02:25:45,220 --> 02:25:49,340
and one choice of q will bias me toward using the functions so maybe i

2189
02:25:49,390 --> 02:25:53,140
two functions the roughly equivalent in terms of training error

2190
02:25:53,140 --> 02:25:55,240
and in one of the choices of q

2191
02:25:55,260 --> 02:25:59,370
one is is preferred and in the other choice of q the others preferred

2192
02:25:59,410 --> 02:26:02,620
so it would change that

2193
02:26:02,740 --> 02:26:04,870
what i need

2194
02:26:07,530 --> 02:26:09,530
it is

2195
02:26:15,050 --> 02:26:16,780
the best way

2196
02:26:22,120 --> 02:26:25,780
exactly those are regularized all in

2197
02:26:25,800 --> 02:26:27,370
last algorithms

2198
02:26:30,850 --> 02:26:36,970
well in that case it would simply be minimizing an algorithm that would minimize this

2199
02:26:38,120 --> 02:26:42,430
together with the training error so would find the function in the class

2200
02:26:42,430 --> 02:26:44,010
that would have

