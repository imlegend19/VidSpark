1
00:00:00,000 --> 00:00:05,050
we my normal hard margin where c is less so we can allow for some

2
00:00:05,050 --> 00:00:07,760
terms being non-zero

3
00:00:07,770 --> 00:00:10,040
it's like variables OK

4
00:00:10,080 --> 00:00:14,660
so that's my man my minimisation tasks slightly different from previously

5
00:00:14,700 --> 00:00:19,970
i have the constraints now slightly relaxed because i've got this term in there

6
00:00:20,030 --> 00:00:22,000
so it's been like before

7
00:00:22,010 --> 00:00:23,650
that's why l as before

8
00:00:23,670 --> 00:00:25,810
but i know these two terms

9
00:00:25,820 --> 00:00:28,640
these which come from my constraints

10
00:00:28,660 --> 00:00:29,870
and finally

11
00:00:29,890 --> 00:00:32,080
i have this additional term here

12
00:00:32,110 --> 00:00:34,040
because i do dictate

13
00:00:34,050 --> 00:00:35,750
that my

14
00:00:35,770 --> 00:00:41,440
xyz must be zero positive zero positive there is a constraint on there

15
00:00:41,500 --> 00:00:48,430
and that being the case any constraint must add corresponding lagrange multipliers so indeed do

16
00:00:49,170 --> 00:00:51,500
two types of controlled now

17
00:00:52,220 --> 00:00:56,290
so that comes about because my size must be positive OK

18
00:00:56,330 --> 00:00:57,950
i didn't have been positive

19
00:00:58,140 --> 00:01:04,290
b actually favoring points be messed up story basically so

20
00:01:04,300 --> 00:01:06,130
that's my l

21
00:01:06,170 --> 00:01:12,130
so i do my normal trickery of working out the one jewel dual formulation have

22
00:01:12,220 --> 00:01:13,990
got an extra condition here

23
00:01:14,000 --> 00:01:17,270
which is a of that is just as before

24
00:01:17,280 --> 00:01:18,950
but i have this extra

25
00:01:19,010 --> 00:01:24,930
xi separable floating around to deal with this i extra condition

26
00:01:25,750 --> 00:01:29,060
we substituting back in the first of those two equations

27
00:01:29,070 --> 00:01:32,370
i get the same objective function as before

28
00:01:33,450 --> 00:01:37,300
we must be positive it is the crunch multiply

29
00:01:37,310 --> 00:01:41,990
look this last equation i got to the bottom of the previous slide that must

30
00:01:41,990 --> 00:01:46,560
be positive which in turn dictates the alpha i i must be less like to

31
00:01:47,420 --> 00:01:52,360
in other words the previous constraint which i had is now replaced by this

32
00:01:52,990 --> 00:01:57,120
so i support formal proof that that is the only change i make for the

33
00:01:57,120 --> 00:01:58,170
l one

34
00:01:58,170 --> 00:01:59,710
error norm OK

35
00:01:59,770 --> 00:02:03,520
and there is one way and use the soft margin

36
00:02:03,560 --> 00:02:08,000
now i'd say there's a second we're doing it will just before doing just some

37
00:02:09,650 --> 00:02:12,370
some patterns will actually lie

38
00:02:12,440 --> 00:02:13,400
the alpha

39
00:02:13,410 --> 00:02:16,900
when i do the learning task alpha actually lies between the two

40
00:02:16,910 --> 00:02:22,160
our first these as non bound whereas patterns actually hit up against the constraints like

41
00:02:22,160 --> 00:02:23,700
sequence africa c

42
00:02:24,160 --> 00:02:29,180
i call them as found that found it might indicate that there are not like

43
00:02:29,410 --> 00:02:32,020
this these influence but it might be the case

44
00:02:32,110 --> 00:02:33,640
they are now

45
00:02:34,450 --> 00:02:36,950
now the optimal value c

46
00:02:39,150 --> 00:02:44,250
well i say can be found by using experimentation of the validation set

47
00:02:44,430 --> 00:02:47,890
she come back you can actually use the theorem to find out what the

48
00:02:47,920 --> 00:02:49,490
well see should be

49
00:02:49,510 --> 00:02:52,500
trivial a one-way finally what the sea should be

50
00:02:52,510 --> 00:02:55,690
will be as my previous parts the island fit data

51
00:02:55,700 --> 00:02:58,410
i simply have some holdout validation data

52
00:02:58,480 --> 00:03:01,830
i estimate the validation error

53
00:03:01,830 --> 00:03:04,590
on this date as a very c

54
00:03:04,600 --> 00:03:06,670
and i find where the minimum is

55
00:03:06,690 --> 00:03:11,160
and that's the minimum perceived value i would use with test data

56
00:03:14,680 --> 00:03:17,910
there is a variant on the

57
00:03:17,910 --> 00:03:19,660
l one error norm

58
00:03:19,670 --> 00:03:24,630
which might come across called the they called new SVM OK

59
00:03:24,630 --> 00:03:27,900
a different type of w

60
00:03:28,890 --> 00:03:33,670
this is a quadratic you notice a quadratic in alpha OK so is a convex

61
00:03:34,610 --> 00:03:39,410
the new SVM was invented by alex smola and i think the initial cough maniacs

62
00:03:40,100 --> 00:03:47,370
and different type of maximisation subject to different well some irish constraints OK

63
00:03:47,380 --> 00:03:49,090
this new

64
00:03:49,250 --> 00:03:53,140
must lie in the range lies between zero and one

65
00:03:53,150 --> 00:03:57,490
this variant on the l one error norm says the different type learning machines

66
00:03:57,510 --> 00:04:00,920
it was introduced because it does actually give you better

67
00:04:00,930 --> 00:04:04,950
interpretation of what the soft margin parameter should be

68
00:04:05,000 --> 00:04:10,170
c the fraction of training errors is upper bounded by the new which introduce

69
00:04:10,220 --> 00:04:14,190
and it also provides a lower bound on the fraction of support vectors

70
00:04:15,340 --> 00:04:20,320
in a somewhat improved interpretation of the soft margin parameter OK

71
00:04:20,340 --> 00:04:23,260
one of interpretation just now when i had to see

72
00:04:23,280 --> 00:04:24,640
it was not clear

73
00:04:24,640 --> 00:04:27,920
the game received value where the minimum is that what men

74
00:04:28,000 --> 00:04:30,520
but this had a little bit better interpretation

75
00:04:30,530 --> 00:04:31,970
this is just a variant

76
00:04:32,010 --> 00:04:34,730
it is worth noting is this

77
00:04:35,320 --> 00:04:37,640
the other type of

78
00:04:37,660 --> 00:04:43,480
o normal want to introduce is l two error norm and is assigned the same

79
00:04:44,350 --> 00:04:45,700
as l one

80
00:04:45,720 --> 00:04:46,780
it said

81
00:04:46,800 --> 00:04:50,900
rather than using a slack variable xi

82
00:04:50,910 --> 00:04:54,960
i have xi squared OK that's it was only difference

83
00:04:54,990 --> 00:04:56,170
now the words

84
00:04:56,210 --> 00:04:57,980
just now

85
00:04:58,030 --> 00:04:59,760
i had an equation

86
00:04:59,770 --> 00:05:02,970
one minus xi OK which

87
00:05:03,020 --> 00:05:04,830
and all these points here

88
00:05:04,830 --> 00:05:06,250
that was the l one

89
00:05:06,870 --> 00:05:09,100
l two is just square top

90
00:05:09,150 --> 00:05:12,260
that's it so that's the difference

91
00:05:12,860 --> 00:05:16,760
so the story for the l one l two norm

92
00:05:16,810 --> 00:05:18,970
pretty much the same as before

93
00:05:18,990 --> 00:05:23,060
so rather than linear inside it squared up

94
00:05:23,140 --> 00:05:26,220
and that's the only difference

95
00:05:26,280 --> 00:05:28,920
this equations got messed up here

96
00:05:28,930 --> 00:05:32,610
i think just are designed as before

97
00:05:33,350 --> 00:05:34,900
the same as before

98
00:05:34,920 --> 00:05:37,610
but i've got that different

99
00:05:39,080 --> 00:05:42,240
this was due to create court is i think actually

100
00:05:42,510 --> 00:05:44,330
it's a long time ago she

101
00:05:44,430 --> 00:05:47,810
note it is of minor comment in the paper

102
00:05:47,820 --> 00:05:49,900
right so we

103
00:05:49,910 --> 00:05:52,100
if i made a small change

104
00:05:52,110 --> 00:05:54,650
and is a little exercise you to do

105
00:05:54,660 --> 00:05:58,680
you would actually get w vlrtual which looks as before

106
00:05:58,730 --> 00:06:00,060
exactly as before

107
00:06:00,070 --> 00:06:02,720
but it's got this extra term tacked on

108
00:06:03,770 --> 00:06:08,970
that is the difference comes about when introduces i squared OK

109
00:06:10,030 --> 00:06:13,180
if i lambda you one a four c e

110
00:06:13,200 --> 00:06:15,780
i can actually introduced this to

111
00:06:15,830 --> 00:06:17,350
into here

112
00:06:17,360 --> 00:06:20,590
if i make a small adaptation of my

113
00:06:20,600 --> 00:06:24,430
OK if i shift my k value

114
00:06:24,480 --> 00:06:25,970
if you notice

115
00:06:25,980 --> 00:06:32,670
if the wise opossum minus one that obviously why i squared we pass one OK

116
00:06:32,710 --> 00:06:34,740
that's one further observation but

117
00:06:34,750 --> 00:06:37,310
i'm saying this term here

118
00:06:37,320 --> 00:06:40,170
can be sucked into this term here

119
00:06:40,230 --> 00:06:41,030
after all

120
00:06:41,690 --> 00:06:46,600
i'm go i j i like second in here

121
00:06:46,600 --> 00:06:50,420
you probably see if i just do one small change

122
00:06:53,050 --> 00:06:55,190
the kernel goes to the kernel

123
00:06:57,270 --> 00:07:00,700
where lambda is that she would be one on on to say

124
00:07:00,720 --> 00:07:05,210
OK the half talking about somewhere but that small change OK

125
00:07:05,220 --> 00:07:06,570
will give me the

126
00:07:06,980 --> 00:07:10,750
w had on my previous slide that change

127
00:07:10,800 --> 00:07:13,120
the that choice of lambda ix one

128
00:07:13,170 --> 00:07:14,660
two say would make

129
00:07:15,650 --> 00:07:16,710
equal to

130
00:07:16,730 --> 00:07:17,860
my normal

131
00:07:17,860 --> 00:07:19,130
w k

132
00:07:19,150 --> 00:07:21,960
that's the full proof of

133
00:07:22,050 --> 00:07:25,160
the this second type of soft margin

134
00:07:25,170 --> 00:07:28,420
OK so these the two types of soft margin i introduced

135
00:07:28,470 --> 00:07:32,140
to lessen the effect of outliers or noise in my data

136
00:07:32,160 --> 00:07:34,680
can typically i would you see

137
00:07:35,510 --> 00:07:38,740
one further comment phys so the leaf soft margins

138
00:07:39,000 --> 00:07:41,500
if i'm doing a typical problem

139
00:07:41,700 --> 00:07:44,870
i may have two types of errors could be

140
00:07:44,870 --> 00:07:48,450
words on the page classify and then

141
00:07:48,470 --> 00:07:52,190
we use only the labelled data for that by the way

142
00:07:52,210 --> 00:07:56,790
then will allow both of those quest to go through the of labelled data in

143
00:07:56,790 --> 00:08:02,050
pick just a small number of highly confident

144
00:08:02,100 --> 00:08:07,470
examples where it's highly confident of its prediction

145
00:08:07,510 --> 00:08:11,210
and then people get added to the self labelled examples

146
00:08:11,510 --> 00:08:14,030
and will iterate this process

147
00:08:14,050 --> 00:08:21,190
intuitively the reason this works well is that

148
00:08:21,230 --> 00:08:24,710
suppose this is our one of our first examples

149
00:08:25,430 --> 00:08:27,810
and we have let's say

150
00:08:27,850 --> 00:08:31,050
only a hundred million on labelled web pages

151
00:08:32,170 --> 00:08:38,250
even after this one example our hyperlink class the fire could become very confident that

152
00:08:38,290 --> 00:08:43,590
if it sees the hyperlinks that says exactly my advisor

153
00:08:43,610 --> 00:08:48,290
um then it's going to want to label the page that points two as a

154
00:08:48,290 --> 00:08:50,910
faculty page

155
00:08:51,020 --> 00:08:54,650
now when we search through the labelled data if the system happens to find one

156
00:08:54,650 --> 00:08:56,830
other my adviser link

157
00:08:57,130 --> 00:09:01,290
then that which is probably going to point two web page that looks nothing like

158
00:09:02,950 --> 00:09:07,230
and will be web page where the word base where the page classify will be

159
00:09:07,230 --> 00:09:08,990
very uncertain

160
00:09:09,270 --> 00:09:12,840
so part of what works in our favour here is that

161
00:09:12,910 --> 00:09:15,930
the hyperlinks that we've seen before

162
00:09:15,970 --> 00:09:20,710
in the labels that happen to a co-worker with web pages that we haven't seen

163
00:09:20,710 --> 00:09:24,470
before and vice versa

164
00:09:24,510 --> 00:09:28,670
and so the things that class of is very confident about tend to be things

165
00:09:28,670 --> 00:09:30,840
the quest for b

166
00:09:33,230 --> 00:09:38,290
and so they can both help each other in this kind of bootstrap

167
00:09:38,330 --> 00:09:40,250
so that's the idea of CO training

168
00:09:42,530 --> 00:09:44,670
this actually works

169
00:09:45,250 --> 00:09:49,840
for example here's the simple case where we had twelve labelled web pages

170
00:09:49,950 --> 00:09:55,610
a thousand on labelled pages and we iterate that simple quote training

171
00:09:55,670 --> 00:09:57,290
learning algorithm the error

172
00:09:57,590 --> 00:09:59,710
on the whole don't test said

173
00:09:59,730 --> 00:10:05,150
classification decreases from on the order of ten to fifteen percent down two

174
00:10:05,170 --> 00:10:07,410
five to ten percent

175
00:10:07,450 --> 00:10:11,510
so it's kind of practical advantage we get

176
00:10:11,550 --> 00:10:12,970
in fact

177
00:10:13,010 --> 00:10:15,090
since then i

178
00:10:15,130 --> 00:10:17,870
people have found

179
00:10:17,890 --> 00:10:22,190
stronger results but even here if you think about percentage-wise

180
00:10:22,210 --> 00:10:26,270
improvement in the air it's pretty substantial

181
00:10:26,470 --> 00:10:29,890
so let's spend a minute

182
00:10:29,910 --> 00:10:33,430
talking a little bit more

183
00:10:33,450 --> 00:10:36,730
looking in a little bit more detail at the

184
00:10:36,790 --> 00:10:45,970
real structure of this problem are because i think that'll let you actually see

185
00:10:48,370 --> 00:10:54,810
even in the case of this kind of page classification there will be problems with

186
00:10:54,810 --> 00:10:59,310
this works better works worse depending on some subtle piece of the structure

187
00:10:59,590 --> 00:11:07,900
OK so what's really going on there actually let me let me talk about it

188
00:11:07,900 --> 00:11:12,930
this way has the best way i know of explaining what's really going on with

189
00:11:12,950 --> 00:11:16,130
so i think of

190
00:11:16,150 --> 00:11:21,250
our document classification problem or the web if you like as a very very large

191
00:11:21,250 --> 00:11:23,230
by apartheid graph

192
00:11:23,250 --> 00:11:27,270
by partake just means there are two kinds of nodes in the connection go from

193
00:11:27,270 --> 00:11:29,460
one kind to the other kind

194
00:11:29,570 --> 00:11:30,720
in our case

195
00:11:30,870 --> 00:11:34,550
these nodes are web pages with words on

196
00:11:34,690 --> 00:11:37,350
these nodes are the hyperlinks

197
00:11:37,390 --> 00:11:39,130
like my adviser

198
00:11:39,350 --> 00:11:42,270
if you think drying the them

199
00:11:42,410 --> 00:11:45,980
think of representing the web this way or any collection of web pages that you

200
00:11:47,530 --> 00:11:51,510
and we'll put in the edge between

201
00:11:51,550 --> 00:11:53,090
a particular

202
00:11:53,110 --> 00:11:58,150
hyperlink and in particular web page just in case those co-workers an example so each

203
00:11:58,150 --> 00:12:01,670
edge here's an example

204
00:12:01,710 --> 00:12:04,730
of course some of those edges we'll have labels for

205
00:12:05,090 --> 00:12:10,190
somebody might tell us this is a positive example that web page in hyperlink pair

206
00:12:10,450 --> 00:12:14,090
other ones will be on labelled those are just the edges that have no

207
00:12:14,130 --> 00:12:15,390
labels on them

208
00:12:15,430 --> 00:12:18,490
others will be labelled as negative and so forth

209
00:12:18,550 --> 00:12:22,290
OK so you see the notation i'm using

210
00:12:23,670 --> 00:12:28,590
now let's think of how we would how the training process would work

211
00:12:28,630 --> 00:12:31,470
if we use the world's simplest

212
00:12:31,510 --> 00:12:36,090
learning algorithm what's too willing to train two rote learners

213
00:12:36,130 --> 00:12:38,550
one rote learner for the hyperlinks

214
00:12:38,570 --> 00:12:43,130
one rote learner for the web pages

215
00:12:43,230 --> 00:12:45,110
what happens

216
00:12:45,210 --> 00:12:49,470
well these are labelled edges i'll say are my on labelled examples

217
00:12:49,510 --> 00:12:53,870
in the labels they have pluses and minuses on the initially labelled subset of the

218
00:12:55,810 --> 00:12:57,410
so what's going to happen

219
00:12:57,610 --> 00:13:00,350
so on the first iteration my are

220
00:13:00,370 --> 00:13:05,710
well of pages will learn that this page is a positive example

221
00:13:05,860 --> 00:13:10,910
and are that this one's negative and this one's negative

222
00:13:10,950 --> 00:13:13,570
and then it gets to look for other

223
00:13:13,590 --> 00:13:17,810
examples that it can confidently label

224
00:13:17,830 --> 00:13:19,250
so here's one

225
00:13:19,370 --> 00:13:22,070
here's another

226
00:13:23,850 --> 00:13:25,310
page pair

227
00:13:25,350 --> 00:13:32,450
which involves the same page so it's going to label that here a positive example

228
00:13:34,630 --> 00:13:37,810
this hyperlink label will label this

229
00:13:37,830 --> 00:13:41,190
example is positive

230
00:13:43,590 --> 00:13:48,210
and the next iteration of the rote learner will learn that this page is also

231
00:13:48,210 --> 00:13:49,070
it turns out that

232
00:13:50,200 --> 00:13:51,320
four consistency to

233
00:13:51,980 --> 00:13:52,670
be true

234
00:13:53,080 --> 00:13:55,570
we need a uniform large numbers

235
00:13:56,320 --> 00:14:00,960
and that is uniform over all functions so here said that this is true for

236
00:14:00,960 --> 00:14:04,080
any fixed function bit more specific about later

237
00:14:04,750 --> 00:14:09,050
and we needed uniformly over all functions that the learning machine can implement

238
00:14:09,760 --> 00:14:11,960
so i sort of glossed over this before but

239
00:14:14,410 --> 00:14:15,460
i get to this nine

240
00:14:16,420 --> 00:14:19,090
so uniform convergence automatically learning machine

241
00:14:20,370 --> 00:14:21,160
and you have

242
00:14:21,570 --> 00:14:25,820
so the whole set of functions that can choose from a set of functions empirical observations now

243
00:14:26,340 --> 00:14:28,650
you would choose function based on your observations

244
00:14:29,790 --> 00:14:32,550
e to these functions will have a certain risk

245
00:14:33,540 --> 00:14:35,070
so the expected error

246
00:14:35,960 --> 00:14:41,570
theta functions would have a certain period research training error form of a given set of observations

247
00:14:42,400 --> 00:14:44,430
and the law of large numbers of classical

248
00:14:49,410 --> 00:14:50,790
for every fixed function as

249
00:14:51,380 --> 00:14:53,870
the training error will converge to the true error

250
00:14:54,380 --> 00:14:56,260
in probability so the probability of

251
00:14:57,090 --> 00:14:58,890
large deviation you get a zero

252
00:14:59,370 --> 00:15:00,670
actually exponentially fast

253
00:15:02,160 --> 00:15:02,720
it doesn't

254
00:15:03,330 --> 00:15:04,560
all this doesn't imply

255
00:15:06,730 --> 00:15:12,210
we keep increasing the observations sorry because observations parents are changes

256
00:15:12,730 --> 00:15:14,630
so this and running out of every here

257
00:15:15,490 --> 00:15:16,650
this curve you will change

258
00:15:18,490 --> 00:15:19,790
in this case readily

259
00:15:20,240 --> 00:15:20,670
in all

260
00:15:21,340 --> 00:15:23,130
points we get closer to this goal

261
00:15:23,670 --> 00:15:25,970
but this would imply that minimizes the

262
00:15:27,340 --> 00:15:29,650
most with minimalist surprisingly

263
00:15:31,080 --> 00:15:35,320
the reason is the difference between uniform convergence pointwise convergence end

264
00:15:36,070 --> 00:15:37,750
i think the mathematicians among you will

265
00:15:38,610 --> 00:15:40,480
immediately see why this is the case

266
00:15:44,120 --> 00:15:44,800
can explain now

267
00:15:45,390 --> 00:15:48,080
the go through more mathematics uh tree because this

268
00:15:48,680 --> 00:15:49,680
next just

269
00:15:50,190 --> 00:15:51,360
this is just to get a little bit

270
00:15:52,700 --> 00:15:56,120
in the first place so i guess you can imagine if we have a uniform convergence so this

271
00:15:56,700 --> 00:16:00,210
curve converges to this one at the same speed everywhere

272
00:16:01,710 --> 00:16:03,380
and also the minimum will converge

273
00:16:05,040 --> 00:16:06,210
which is basically what we want

274
00:16:08,060 --> 00:16:08,650
okay so

275
00:16:11,670 --> 00:16:12,560
so sometimes people

276
00:16:14,370 --> 00:16:16,340
how to choose that set of functions

277
00:16:16,940 --> 00:16:19,920
and that we should do or inference we

278
00:16:20,740 --> 00:16:23,990
so what kind of functions you are learning machine e

279
00:16:24,050 --> 00:16:25,090
able to implement

280
00:16:25,930 --> 00:16:29,240
and the things we may be don't know what is the problem we want to learn

281
00:16:30,200 --> 00:16:31,410
we might be tempted to say well

282
00:16:32,120 --> 00:16:38,210
how it taking all functions that take our input domain then domain and plus minus one

283
00:16:39,100 --> 00:16:41,420
and and that actually turns out to be impossible

284
00:16:42,580 --> 00:16:44,000
it is relatively easy to see

285
00:16:44,550 --> 00:16:46,440
once you think about the convergence

286
00:16:47,230 --> 00:16:48,220
and as follows

287
00:16:49,250 --> 00:16:49,630
so we have

288
00:16:50,260 --> 00:16:52,100
as we have a training set like before

289
00:16:52,980 --> 00:16:56,880
and in addition we already have that we have test inputs

290
00:16:57,990 --> 00:16:58,610
and explore

291
00:16:59,590 --> 00:17:00,570
and let's assume

292
00:17:01,000 --> 00:17:03,750
for simplicity disjoint from the training points

293
00:17:04,400 --> 00:17:05,080
so in a

294
00:17:05,130 --> 00:17:05,930
continuous problem

295
00:17:08,310 --> 00:17:08,690
the problem

296
00:17:09,910 --> 00:17:10,890
and this will be

297
00:17:11,900 --> 00:17:12,930
almost always true

298
00:17:13,450 --> 00:17:14,510
this probability

299
00:17:15,700 --> 00:17:17,650
it's not true with zero basically

300
00:17:18,210 --> 00:17:18,810
and then

301
00:17:19,360 --> 00:17:21,610
in this case is no overlap

302
00:17:23,150 --> 00:17:27,330
of course when you give me some function after give function every time they are

303
00:17:27,340 --> 00:17:29,150
in this is a good solution for problem

304
00:17:31,600 --> 00:17:34,680
so i looked at the training set and i came up with this function if

305
00:17:34,680 --> 00:17:35,910
i think this function will be good

306
00:17:36,820 --> 00:17:38,450
i can give you an star

307
00:17:39,810 --> 00:17:40,140
which is

308
00:17:40,670 --> 00:17:44,290
defined as follows or which is constructed as follows so far

309
00:17:44,710 --> 00:17:46,820
agrees with everyone all the training points

310
00:17:47,600 --> 00:17:47,960
but it

311
00:17:48,520 --> 00:17:51,990
disagrees with it on all the other points on the test points

312
00:17:52,670 --> 00:17:53,440
the joint

313
00:17:54,290 --> 00:17:55,350
so these joint

314
00:17:56,150 --> 00:17:57,090
one such function in

315
00:17:57,810 --> 00:17:59,640
no restriction on the class of functions

316
00:18:01,210 --> 00:18:06,040
so now based on the training set is no means of choosing which one of these two functions is better

317
00:18:07,190 --> 00:18:10,140
this is exactly the same that is that of results

318
00:18:11,500 --> 00:18:12,470
therefore if u

319
00:18:14,720 --> 00:18:15,840
if you go back to this picture

320
00:18:16,560 --> 00:18:20,120
these two functions will have the same appearance they have the same training error

321
00:18:21,250 --> 00:18:22,360
but that is

322
00:18:22,410 --> 00:18:24,490
but it could be arbitrarily different so they can

323
00:18:25,180 --> 00:18:26,480
very different characteristics

324
00:18:27,970 --> 00:18:29,300
if one of these functions

325
00:18:29,970 --> 00:18:32,370
nature is that was supposed be characteristic

326
00:18:32,910 --> 00:18:36,670
other one might have an actual risk that far away from the empirical risk

327
00:18:37,740 --> 00:18:39,690
we will never be the case that these curves

328
00:18:40,130 --> 00:18:41,840
are close together everywhere

329
00:18:42,570 --> 00:18:45,360
so in this case uniform convergence will be impossible

330
00:18:47,300 --> 00:18:47,540
and this

331
00:18:48,640 --> 00:18:50,410
kind of observation sometimes called

332
00:18:50,410 --> 00:18:53,420
f of

333
00:18:53,430 --> 00:18:55,970
y j

334
00:18:55,990 --> 00:18:59,170
we can also write it as the sum over

335
00:18:59,180 --> 00:19:02,280
i phi i p of

336
00:19:02,290 --> 00:19:03,340
x i

337
00:19:03,340 --> 00:19:06,090
so let's let's see what this is correct

338
00:19:06,100 --> 00:19:08,770
so if we substitute the f in here

339
00:19:10,240 --> 00:19:14,860
or let's let's do it here so remember remember g was

340
00:19:14,910 --> 00:19:16,500
sum over

341
00:19:16,530 --> 00:19:18,460
but they

342
00:19:18,480 --> 00:19:21,750
OK of dot com are y j

343
00:19:22,100 --> 00:19:25,890
nine the we substitute x i

344
00:19:25,940 --> 00:19:29,120
so we get this thing here and now we have some of the phi beta

345
00:19:29,140 --> 00:19:31,620
j k of x i y j

346
00:19:31,670 --> 00:19:33,330
which is exactly what we had here

347
00:19:33,350 --> 00:19:37,940
OK so we know we know this inequality is true to show this equality you

348
00:19:37,940 --> 00:19:42,640
do basically the same thing and also use the fact case symmetric place the same

349
00:19:44,810 --> 00:19:49,940
just by direct substitution you can prove that these two quantities are true

350
00:19:49,960 --> 00:19:51,890
so what does this tell us

351
00:19:51,910 --> 00:19:54,750
well first equality tells us

352
00:19:54,770 --> 00:19:58,890
that this quantity of this dot product here

353
00:19:58,910 --> 00:20:01,080
is independent of the of

354
00:20:01,080 --> 00:20:05,500
because here the f only appears directly expansion coefficients of f

355
00:20:05,580 --> 00:20:11,480
other fast and also the expansion points x either don't appear anymore

356
00:20:11,500 --> 00:20:13,060
so here we can see

357
00:20:13,080 --> 00:20:16,540
it is independent

358
00:20:16,620 --> 00:20:17,940
i phi

359
00:20:18,160 --> 00:20:19,660
x i

360
00:20:19,670 --> 00:20:22,290
likewise here we can see

361
00:20:22,310 --> 00:20:26,540
the dot product is independent of

362
00:20:26,560 --> 00:20:28,350
but i j

363
00:20:28,370 --> 00:20:30,850
and x sorry

364
00:20:30,850 --> 00:20:32,250
y j

365
00:20:32,330 --> 00:20:36,350
OK so this is something that mathematicians like to they like to show that these

366
00:20:36,350 --> 00:20:40,930
are well defined maybe you are not worried about this to begin with in which

367
00:20:40,930 --> 00:20:44,250
case i didn't want to get too worried but if you are worried about whether

368
00:20:44,250 --> 00:20:48,890
it's well-defined and hopefully you convinced that this is well defined and also note that

369
00:20:49,210 --> 00:20:51,520
you know order to show that this is well defined we didn't have to use

370
00:20:51,520 --> 00:20:56,020
that the kernel is positive definite so far we didn't we didn't use that at

371
00:20:57,120 --> 00:20:58,910
that's nice

372
00:20:58,910 --> 00:21:02,160
anyway so the next week going to show that so now we know that this

373
00:21:02,870 --> 00:21:04,190
it is well defined

374
00:21:04,190 --> 00:21:08,060
actually we also know if we look at it that it's by linear are because

375
00:21:08,060 --> 00:21:12,980
just by construction directly i mean if i had another term here

376
00:21:13,000 --> 00:21:17,390
in the number of kernels then it will just show up here is another and

377
00:21:17,390 --> 00:21:21,230
if you multiply with a constant it will just you will get this constant art

378
00:21:21,230 --> 00:21:25,830
and stuff like that so it's it's what people call it by linear forms

379
00:21:25,890 --> 00:21:30,140
and if the kernel is symmetric it is symmetric by now form the product is

380
00:21:30,140 --> 00:21:32,040
a little bit more than the body now home

381
00:21:32,140 --> 00:21:36,290
on a little bit more than symmetric body now form so it's also symmetric because

382
00:21:36,290 --> 00:21:41,060
the kernel is symmetric and here these coefficients of course i can also interchanges just

383
00:21:41,230 --> 00:21:42,790
a product products are symmetric

384
00:21:43,480 --> 00:21:46,830
so we know this is symmetric bilinear form

385
00:21:46,850 --> 00:21:49,330
now we have to prove that is the dot product and for that we are

386
00:21:49,330 --> 00:21:53,230
going to use that the kernel is positive definite

387
00:21:55,060 --> 00:22:00,290
so do this in two steps

388
00:22:00,370 --> 00:22:05,160
the first step is actually to prove that this kernel is

389
00:22:05,160 --> 00:22:08,210
well that this is the first step is to prove that this by linear form

390
00:22:08,210 --> 00:22:13,500
here so this thing that we've defined here is actually a positive definite kernel on

391
00:22:13,500 --> 00:22:15,870
the space of functions

392
00:22:16,000 --> 00:22:20,370
so how do we prove this well we are going to use are

393
00:22:20,560 --> 00:22:25,640
conditions that have to hold true for positive definite kernels so up there so this

394
00:22:25,640 --> 00:22:28,480
is a positive definite kernel the space of the exercise

395
00:22:28,500 --> 00:22:31,460
so i said no i'm going to show that this thing that we've just defined

396
00:22:31,460 --> 00:22:34,440
is a positive definite kernel on the space of functions

397
00:22:34,500 --> 00:22:39,020
so let's take some functions

398
00:22:39,040 --> 00:22:42,750
this figure could an expansion over the coefficients come

399
00:22:43,330 --> 00:22:44,410
goodman j

400
00:22:44,440 --> 00:22:47,170
and then functions fi fj

401
00:22:47,230 --> 00:22:52,060
and these all function functions of this form but they might have different expansions just

402
00:22:52,060 --> 00:22:55,580
some elements of all of all vector space

403
00:22:55,790 --> 00:23:00,370
OK so let's work this out what do we do about all symmetric bilinear form

404
00:23:00,440 --> 00:23:04,810
we know that symmetric by linear answer since by lee we can take sums in

405
00:23:04,810 --> 00:23:05,600
and out

406
00:23:05,620 --> 00:23:07,370
so that's what we're going to do

407
00:23:07,390 --> 00:23:13,520
as before

408
00:23:13,580 --> 00:23:19,310
OK so so far only use this by linear

409
00:23:19,330 --> 00:23:23,660
so now again the same triggers before we we see this is the function here

410
00:23:23,870 --> 00:23:29,170
is the same function as this one here let's just call this function f

411
00:23:29,190 --> 00:23:37,040
OK so now we have the by linear form between f itself that's not yet

412
00:23:37,040 --> 00:23:40,890
enough we would like to say that this is this is nonnegative we want to

413
00:23:40,890 --> 00:23:43,440
prove that this is lower bounded by zero

414
00:23:43,480 --> 00:23:48,330
if we knew that this thing where dot product then we could stop now because

415
00:23:48,330 --> 00:23:53,420
this direction

416
00:23:53,420 --> 00:23:56,330
but means

417
00:23:56,430 --> 00:23:58,640
the romans fours fl

418
00:23:58,690 --> 00:24:02,280
which in this case with the currents

419
00:24:02,330 --> 00:24:03,740
times the length

420
00:24:03,750 --> 00:24:04,740
of this

421
00:24:04,750 --> 00:24:07,500
are which is the length of this bar

422
00:24:07,560 --> 00:24:09,740
times b

423
00:24:09,740 --> 00:24:12,190
that is the force that i have to apply

424
00:24:12,290 --> 00:24:16,300
if i put it to right because that forces to the left

425
00:24:16,350 --> 00:24:18,670
so the force of waterloo in

426
00:24:18,730 --> 00:24:20,080
is the same

427
00:24:20,170 --> 00:24:21,600
but in this direction

428
00:24:21,650 --> 00:24:23,750
i have to overcome the force

429
00:24:23,760 --> 00:24:25,170
the romans force

430
00:24:25,210 --> 00:24:26,410
in this direction

431
00:24:26,500 --> 00:24:28,900
so it's clear that i have to do

432
00:24:30,820 --> 00:24:33,120
i have force in this direction

433
00:24:33,170 --> 00:24:34,920
and i moved within the direction

434
00:24:34,980 --> 00:24:37,510
so i do positive work

435
00:24:37,530 --> 00:24:40,160
what happened was that worked well that comes out

436
00:24:40,200 --> 00:24:41,990
in the four

437
00:24:42,000 --> 00:24:46,610
of heat in the resistance of this conductor i'm creating a map

438
00:24:46,670 --> 00:24:49,740
the current is going to flow

439
00:24:49,750 --> 00:24:51,930
and the power

440
00:24:51,980 --> 00:24:54,210
is the EMF five current

441
00:24:54,220 --> 00:24:58,970
i square are comes out in the form of heat

442
00:24:59,020 --> 00:25:01,680
if i change the direction when i push in

443
00:25:01,720 --> 00:25:04,640
velocity is now in this direction

444
00:25:04,650 --> 00:25:06,550
then clearly the current is going to

445
00:25:06,580 --> 00:25:08,920
change direction

446
00:25:08,920 --> 00:25:10,530
and so when i push in

447
00:25:10,590 --> 00:25:13,400
the lawrence force will also flip over

448
00:25:13,450 --> 00:25:17,780
so to forcefully will flip over so again i have to do positive work

449
00:25:17,830 --> 00:25:21,000
there's no such thing as a free lunch no matter what i do when i

450
00:25:21,000 --> 00:25:22,960
pull this way or pushing

451
00:25:23,010 --> 00:25:24,510
always have to do

452
00:25:24,520 --> 00:25:25,830
positive work

453
00:25:25,910 --> 00:25:28,340
and that work is always converted then

454
00:25:29,390 --> 00:25:34,760
in the resistance of that loop

455
00:25:34,800 --> 00:25:37,360
so the work that i do

456
00:25:37,470 --> 00:25:40,260
let me express it in terms of

457
00:25:40,270 --> 00:25:43,760
of power

458
00:25:43,790 --> 00:25:48,220
the power that i generate

459
00:25:48,290 --> 00:25:49,990
is my force

460
00:25:51,000 --> 00:25:58,430
dot product with high velocity remember from a total

461
00:25:58,490 --> 00:26:00,680
the work that i do

462
00:26:00,680 --> 00:26:02,160
is forests

463
00:26:02,180 --> 00:26:06,020
over a little element bx

464
00:26:06,080 --> 00:26:13,220
the power is work unit time so the x eighty becomes velocity

465
00:26:13,240 --> 00:26:17,770
and my force and my velocity always in the same direction when i push

466
00:26:17,790 --> 00:26:25,200
they're in this direction and when i pulled and is regularly do positive work

467
00:26:25,200 --> 00:26:27,450
and so the power

468
00:26:27,520 --> 00:26:28,930
that i

469
00:26:28,930 --> 00:26:32,740
generate is my force that's the magnitude of my force

470
00:26:32,740 --> 00:26:35,540
which is i l b

471
00:26:35,600 --> 00:26:37,330
times the velocity

472
00:26:37,390 --> 00:26:39,680
but that must also be the EMF

473
00:26:39,700 --> 00:26:43,910
times the current

474
00:26:43,950 --> 00:26:46,100
note is now

475
00:26:46,140 --> 00:26:48,410
that you may have therefore

476
00:26:52,040 --> 00:26:53,740
and b

477
00:26:53,740 --> 00:26:55,220
i'm sorry

478
00:26:55,290 --> 00:26:58,660
so now i have shown you that you may have

479
00:26:58,720 --> 00:27:02,520
is exactly what i found before in terms of magnitude

480
00:27:02,580 --> 00:27:04,970
but now i have not used for

481
00:27:05,020 --> 00:27:08,970
this is purely a derivation based of the work that i do

482
00:27:09,020 --> 00:27:11,260
and the work per unit time

483
00:27:11,260 --> 00:27:15,200
it's interesting that you can also think of it that way

484
00:27:15,270 --> 00:27:16,060
we check

485
00:27:17,490 --> 00:27:20,350
he i ask where i can live with that

486
00:27:21,580 --> 00:27:24,240
force dotted with the velocity

487
00:27:24,290 --> 00:27:26,580
i be it is the magnitude

488
00:27:26,660 --> 00:27:28,350
of the EMF

489
00:27:28,410 --> 00:27:37,700
and that's fine

490
00:27:38,720 --> 00:27:39,830
if i

491
00:27:39,890 --> 00:27:41,140
have a conducting

492
00:27:41,160 --> 00:27:43,240
in this case

493
00:27:43,270 --> 00:27:45,770
solid disc

494
00:27:45,790 --> 00:27:47,350
and i move that

495
00:27:47,370 --> 00:27:48,700
i try to

496
00:27:48,770 --> 00:27:53,080
moving through a magnetic field north or south pole

497
00:27:53,140 --> 00:27:55,240
this is the magnetic fields

498
00:27:55,290 --> 00:27:57,560
so there we go here

499
00:27:57,670 --> 00:27:58,950
little we there

500
00:27:59,060 --> 00:28:04,060
i movies it

501
00:28:04,100 --> 00:28:07,500
then the comes the time when this disk is here

502
00:28:07,560 --> 00:28:09,310
that magnetic field lines

503
00:28:10,160 --> 00:28:12,740
through this portion

504
00:28:12,790 --> 00:28:13,890
that means

505
00:28:13,910 --> 00:28:16,490
the magnetic flux through this surface

506
00:28:21,240 --> 00:28:23,100
lens doesn't like that

507
00:28:23,160 --> 00:28:25,560
prior the fire they doesn't like them

508
00:28:25,600 --> 00:28:30,180
so what's going to happen in the current is going to go around like this

509
00:28:30,220 --> 00:28:32,660
not so easy to precisely

510
00:28:32,790 --> 00:28:36,220
determine how to current exactly flows

511
00:28:36,220 --> 00:28:40,660
subject predicate object is just one way to write but you need to write them

512
00:28:40,700 --> 00:28:44,950
all in the same way but before going in that direction

513
00:28:45,010 --> 00:28:49,220
let's understand something more of what does it mean that i ever subject to predict

514
00:28:49,300 --> 00:28:53,180
help in logic it means that is true

515
00:28:53,280 --> 00:28:56,930
but these relationship among these three

516
00:28:56,970 --> 00:28:59,180
elements or

517
00:28:59,200 --> 00:29:04,030
OK so you can write to predicate let's say it's true also that me knows

518
00:29:04,030 --> 00:29:04,970
it in

519
00:29:05,910 --> 00:29:06,700
that's it

520
00:29:06,740 --> 00:29:08,280
nothing more than that so

521
00:29:08,330 --> 00:29:13,240
well also saying that when you have the tree when you have relational database the

522
00:29:13,260 --> 00:29:16,870
logical representation of it but brooks down everything in its true

523
00:29:17,320 --> 00:29:20,550
that the subject predicate object of related

524
00:29:20,660 --> 00:29:22,950
in this way

525
00:29:25,240 --> 00:29:28,910
everything is a URL or a uni actually

526
00:29:29,100 --> 00:29:32,390
which means that you can write them in all the possible ways is pieces belong

527
00:29:33,280 --> 00:29:37,740
this is the way to europe using the base URL and this one is using

528
00:29:37,740 --> 00:29:41,530
namespaces will always use this one because it's the one that i believe is more

529
00:29:41,530 --> 00:29:43,200
easy to read

530
00:29:43,950 --> 00:29:45,760
the values

531
00:29:45,780 --> 00:29:47,720
they this can be like a string

532
00:29:47,720 --> 00:29:53,260
you can type so you can write something like three dot fourteen is the floor

533
00:29:53,280 --> 00:29:58,350
and you can even use language that this stuff here is written in japanese

534
00:29:58,410 --> 00:30:02,990
OK so i mean it is nice way to represent the true in in the

535
00:30:02,990 --> 00:30:05,950
broad sense and you don't have to give

536
00:30:05,950 --> 00:30:09,910
in a matter every node so if you don't know how to name in order

537
00:30:09,930 --> 00:30:14,990
you can just leave it blank and the seat system will assign a unique identifier

538
00:30:14,990 --> 00:30:19,970
to heat normally use the square brackets to do that but you can also use

539
00:30:19,970 --> 00:30:22,100
this notation with an underscore and then

540
00:30:22,680 --> 00:30:26,260
then this underscore means you are free to assign

541
00:30:26,260 --> 00:30:32,050
i don't want to care about it

542
00:30:32,100 --> 00:30:33,140
for instance

543
00:30:33,160 --> 00:30:39,950
you know i'm declining all the namespaces so this is my name space which each

544
00:30:39,970 --> 00:30:40,820
points that you

545
00:30:41,330 --> 00:30:43,200
my website

546
00:30:43,200 --> 00:30:45,010
EV DVD

547
00:30:45,060 --> 00:30:47,970
is the name space but

548
00:30:48,050 --> 00:30:51,570
works for these four five RDF five

549
00:30:53,330 --> 00:30:58,430
RDF is the a name space of language for for is the namespace of these

550
00:31:00,320 --> 00:31:05,820
tell about ways friend of school so perhaps things like knows the name of the

551
00:31:09,600 --> 00:31:11,320
it is it right

552
00:31:13,530 --> 00:31:15,200
of type of some

553
00:31:15,950 --> 00:31:21,820
these oftype which is great in RDF can also be abbreviated using a since every

554
00:31:21,830 --> 00:31:24,450
moment my phone tripos these

555
00:31:24,510 --> 00:31:26,280
which means that me

556
00:31:26,350 --> 00:31:28,050
is a person

557
00:31:28,070 --> 00:31:30,390
OK just a shorter way to express

558
00:31:30,430 --> 00:31:36,200
and you're very easy way to show blank nodes so you're saying that to me

559
00:31:36,220 --> 00:31:37,830
know somebody

560
00:31:37,850 --> 00:31:42,800
and this somebody which i'm not even a uri as a name which is in

561
00:31:44,140 --> 00:31:48,990
so as you can see is not that there was an basically also easy

562
00:31:48,990 --> 00:31:50,140
one system

563
00:31:50,300 --> 00:31:51,640
could include

564
00:31:51,830 --> 00:31:55,700
so abbreviation

565
00:31:55,720 --> 00:32:00,350
when you have the same subset you can use the same color so you know

566
00:32:00,350 --> 00:32:05,410
you have a means of person and minos CNN which can be written like the

567
00:32:07,240 --> 00:32:08,780
it's a personal

568
00:32:08,820 --> 00:32:11,180
and no serious

569
00:32:11,280 --> 00:32:15,180
OK using the same because i will use this so mean this is the reason

570
00:32:15,180 --> 00:32:18,450
why i stress and when you are

571
00:32:18,470 --> 00:32:23,550
you have more people objects you can use the comma so you have to me

572
00:32:23,570 --> 00:32:26,620
no zero in there and minos w

573
00:32:26,760 --> 00:32:31,870
and is the way you can shorten it to me knows in and that you

574
00:32:31,870 --> 00:32:36,660
see the comma is a way to put multiple object of the same property

575
00:32:41,450 --> 00:32:43,100
so in the end

576
00:32:43,140 --> 00:32:45,620
if you do take is XML

577
00:32:45,620 --> 00:32:51,680
OK that talks about the band and artist saying that they be does is the

578
00:32:51,700 --> 00:32:58,830
band that they comes from musicbrainz and actually they perform three different styles british invasion

579
00:32:58,890 --> 00:33:00,390
rock and she for

580
00:33:00,390 --> 00:33:04,490
OK so this is the information that i got from music it is the way

581
00:33:04,490 --> 00:33:09,800
i can represent the same stuff using out here so i have the subject these

582
00:33:09,850 --> 00:33:12,890
URL coming from here

583
00:33:12,950 --> 00:33:15,870
and then seeing it's the band

584
00:33:15,890 --> 00:33:22,740
using the music namespaces and then saying that the label of this stuff is the

585
00:33:23,660 --> 00:33:26,620
i'm using these illiterate to the labels

586
00:33:26,620 --> 00:33:31,890
and i'm in saying that this ties that they perform these three different objects

587
00:33:33,300 --> 00:33:35,890
british invasion rock and

588
00:33:37,240 --> 00:33:38,680
this was to show

589
00:33:38,700 --> 00:33:40,140
but i mean

590
00:33:40,180 --> 00:33:44,030
if all of us got that this was useful

591
00:33:44,070 --> 00:33:48,660
and plenty of pages outside in the web but don't like these

592
00:33:48,760 --> 00:33:50,720
well i mean it's not that

593
00:33:51,430 --> 00:33:56,370
to to use a different representation which in an abstract one so is good for

594
00:33:56,430 --> 00:34:00,620
this layer up here

595
00:34:02,910 --> 00:34:04,390
that doesn't sound

596
00:34:04,410 --> 00:34:08,220
more complex or more variables than the original one

597
00:34:11,450 --> 00:34:15,870
if you want to read more input here the

598
00:34:15,890 --> 00:34:21,430
the links this life would be somewhere in the preceding i guess but if you

599
00:34:21,430 --> 00:34:26,760
like you can also download them from the from our website OK good i guess

600
00:34:26,820 --> 00:34:31,260
i mean you the to so you can click on them i guess an open

601
00:34:31,280 --> 00:34:33,620
the pages

602
00:34:34,260 --> 00:34:36,950
from RDF two

603
00:34:36,990 --> 00:34:40,530
the question of

604
00:34:42,140 --> 00:34:47,740
so this one is not to say a few role of the tree

605
00:34:47,740 --> 00:34:49,850
you don't need me to tell you

606
00:34:49,870 --> 00:34:53,380
that human civilization is very very old

607
00:34:53,390 --> 00:34:59,650
nevertheless our knowledge of the earliest stages of human civilization were quite limited for many

608
00:34:59,650 --> 00:35:05,770
centuries that is until the great archaeological discoveries of the nineteenth and twentieth centuries which

609
00:35:05,770 --> 00:35:10,570
honors trust the great civilizations of the ancient near east of which i have drawn

610
00:35:10,580 --> 00:35:12,390
remarkably lifelike map

611
00:35:12,410 --> 00:35:13,470
here on the board

612
00:35:13,480 --> 00:35:16,870
mediterranean we start with the mediterranean ocean

613
00:35:16,890 --> 00:35:18,290
and i'll river

614
00:35:18,340 --> 00:35:23,270
the tigers and the afraid is so the great civilizations of ancient egypt mesopotamia and

615
00:35:23,270 --> 00:35:25,790
the area we refer to as the fertile crescent

616
00:35:25,840 --> 00:35:29,570
of which little part here about the size of rhode island is came OK can

617
00:35:29,570 --> 00:35:31,240
and cannot

618
00:35:31,340 --> 00:35:36,500
and archaeologist in the nineteenth and twentieth centuries were stunned to find the ruins and

619
00:35:36,500 --> 00:35:42,920
the records of remarkable peoples and cultures massive complex empires in some cases the some

620
00:35:42,920 --> 00:35:46,250
of which had completely disappeared from human memory

621
00:35:46,270 --> 00:35:51,040
their newly uncovered languages had been long forgotten

622
00:35:51,050 --> 00:35:56,300
the rich literary and legal texts were now indecipherable that that soon changed

623
00:35:56,310 --> 00:36:01,610
but because of those discoveries we are now in a position to appreciate the monumental

624
00:36:01,610 --> 00:36:06,190
achievements of these early civilizations is the earliest civilizations

625
00:36:06,220 --> 00:36:11,470
so many scholars many people have remarked that it's not a small irony

626
00:36:11,480 --> 00:36:14,170
that the ancient near eastern people

627
00:36:14,180 --> 00:36:18,310
with one of the or perhaps the most lasting legacy

628
00:36:18,320 --> 00:36:24,060
it was not people that built and inhabited one of the great centres of engineer

629
00:36:24,070 --> 00:36:25,750
eastern civilization

630
00:36:25,790 --> 00:36:30,810
it can be argued that the ancient near eastern people with the most lasting legacy

631
00:36:30,890 --> 00:36:31,870
is the people

632
00:36:31,890 --> 00:36:33,780
they had an idea

633
00:36:33,820 --> 00:36:35,380
it was a new idea

634
00:36:35,490 --> 00:36:37,960
broke with the ideas of its neighbors

635
00:36:37,970 --> 00:36:40,230
and those people were the israelites

636
00:36:40,250 --> 00:36:45,750
and scholars have come to the realization that despite the bible's pretensions to the contrary

637
00:36:45,790 --> 00:36:47,660
the israelites were a small

638
00:36:47,670 --> 00:36:52,040
and i've actually over represented here i'm sure should be much smaller a small and

639
00:36:52,040 --> 00:36:56,090
relatively insignificant group for much of their history

640
00:36:56,100 --> 00:37:00,470
they did manage to establish the kingdom in the land that was known in antiquity

641
00:37:00,470 --> 00:37:01,580
as canaan

642
00:37:01,650 --> 00:37:08,210
around the year one thousand they probably succeeded in subduing some their neighbors collecting tribute

643
00:37:08,210 --> 00:37:12,750
there is some controversy about that but in about nineteen twenty two this kingdom divided

644
00:37:12,750 --> 00:37:16,090
into two smaller and lesser kingdoms fell in importance

645
00:37:16,250 --> 00:37:22,910
the northern kingdom which consisted of ten of the twelve israelite tribes and known confusingly

646
00:37:22,910 --> 00:37:23,980
as israel

647
00:37:24,000 --> 00:37:27,550
it was destroyed in seven twenty two by the assyrians

648
00:37:27,570 --> 00:37:32,100
the southern kingdom which consisted of two of the twelve tribes and known as judah

649
00:37:32,290 --> 00:37:37,040
managed to survive until the year five eighty six when the babylonians came in and

650
00:37:37,040 --> 00:37:38,980
conquered and sent the people

651
00:37:39,010 --> 00:37:42,610
into exile capital jerusalem fell

652
00:37:42,630 --> 00:37:49,230
conquest and exile were events that normally would spell the end of a particular ethnic

653
00:37:49,230 --> 00:37:51,230
and national groups particularly

654
00:37:51,280 --> 00:37:52,490
in antiquity

655
00:37:52,510 --> 00:37:58,680
conquered peoples would trade their defeated god for the victorious god of their conquerors and

656
00:37:58,680 --> 00:38:05,020
eventually there would be a cultural and religious assimilation intermarriage that people would disappear as

657
00:38:05,020 --> 00:38:08,580
a distinctive entity an effect that is what happened to the ten tribes of the

658
00:38:08,580 --> 00:38:12,170
northern kingdom to a large degree were lost to history

659
00:38:12,220 --> 00:38:15,470
this is not happen to those members of the nation of israel who lived in

660
00:38:15,470 --> 00:38:17,450
the southern kingdom of judah

661
00:38:18,400 --> 00:38:19,610
the demise

662
00:38:19,620 --> 00:38:23,940
of their national political base in five eighty six

663
00:38:23,950 --> 00:38:29,660
these rights along really among the many people who have figured in engineering eastern history

664
00:38:29,700 --> 00:38:35,850
the sumerian is the acadians the babylonians hittites the venetians the hoary in the canaanites

665
00:38:35,860 --> 00:38:38,840
they emerged after the death of their state

666
00:38:38,850 --> 00:38:41,690
producing a community and the culture

667
00:38:41,740 --> 00:38:44,220
they can be traced through various

668
00:38:44,230 --> 00:38:47,350
twists and turns and vicissitudes of history

669
00:38:47,370 --> 00:38:50,020
right down into the modern period

670
00:38:50,100 --> 00:38:54,050
that's a pretty unique claim and they carried with them

671
00:38:54,100 --> 00:38:54,960
the idea

672
00:38:54,970 --> 00:38:59,500
and the traditions that laid the foundation for the major religions of the western world

673
00:38:59,500 --> 00:39:02,610
judaism christianity and islam

674
00:39:02,650 --> 00:39:04,380
so what is this radical

675
00:39:04,400 --> 00:39:10,500
new ideas that shape the culture and enable its survival into into later antiquity and

676
00:39:10,500 --> 00:39:14,480
really right into the present day in some form

677
00:39:14,520 --> 00:39:19,790
well the conception of the universe that was widespread among ancient peoples is one that

678
00:39:19,790 --> 00:39:22,340
you're probably familiar with people regarded

679
00:39:22,350 --> 00:39:25,470
various natural forces

680
00:39:25,510 --> 00:39:29,880
as imbued with divine power as in some sense divinity themselves the earth was the

681
00:39:29,880 --> 00:39:34,430
divinity the guy was the divinity the water was the divinity had divine power in

682
00:39:34,430 --> 00:39:38,960
other words the gods are identical with or a minute in the forces of nature

683
00:39:38,980 --> 00:39:43,300
there are many gods no one single god was therefore all powerful

684
00:39:43,310 --> 00:39:45,740
there is very very good evidence

685
00:39:45,750 --> 00:39:48,020
to suggest that ancient israelites

686
00:39:48,040 --> 00:39:50,740
by and large share this worldview

687
00:39:50,750 --> 00:39:56,010
they participated at the very earliest stages in the wider religious called the culture of

688
00:39:56,010 --> 00:39:57,460
the ancient near east

689
00:39:58,750 --> 00:40:01,120
over the course of time

690
00:40:01,140 --> 00:40:05,960
some ancient israelites not all at once and not unanimously

691
00:40:06,010 --> 00:40:07,680
broke with this view

692
00:40:07,690 --> 00:40:13,090
an articulated different view that there was one divine power one god but much more

693
00:40:13,090 --> 00:40:14,750
important than number

694
00:40:14,760 --> 00:40:19,630
was the fact that this god was outside of and above nature this guy was

695
00:40:19,630 --> 00:40:26,470
not identified with nature he transcended nature and he wasn't known through nature or natural

696
00:40:27,580 --> 00:40:28,870
he was known through

697
00:40:30,860 --> 00:40:34,380
and the particular relationship with humankind

698
00:40:34,400 --> 00:40:39,170
an idea which seems simple at first and not so very revolutionary we'll see that's

699
00:40:39,170 --> 00:40:43,840
an idea that affected every aspect of israelite culture

700
00:40:43,850 --> 00:40:47,400
and in ways that will become clear as we move through the course to learn

701
00:40:47,400 --> 00:40:52,030
none of the bright light but in dark adapted conditions you actually have one photon

702
00:40:54,050 --> 00:40:58,610
very impressed

703
00:40:58,670 --> 00:41:02,480
under appropriate conditions like

704
00:41:02,490 --> 00:41:06,090
sound receptors

705
00:41:06,140 --> 00:41:09,960
you get sound receptors in your ear

706
00:41:09,980 --> 00:41:14,330
and there are beautiful rug talk about the meaning like but there's the there's little

707
00:41:14,330 --> 00:41:21,300
flappy these these little spiky things going along in your ear and they can translate

708
00:41:21,310 --> 00:41:23,380
vibrational energy

709
00:41:23,390 --> 00:41:28,090
coming from here including area here drum being translated into vibration in the fluid in

710
00:41:28,090 --> 00:41:30,490
your ear into a physical motion

711
00:41:30,500 --> 00:41:32,030
of of these

712
00:41:32,050 --> 00:41:35,400
little receptors there into an electrical motion

713
00:41:35,420 --> 00:41:39,330
into electrical signal that goes into year so all that

714
00:41:39,380 --> 00:41:41,230
o pretty impressive stuff

715
00:41:41,250 --> 00:41:44,020
we're not going to talk about the details of it but i invite some of

716
00:41:44,020 --> 00:41:48,740
you want to learn more about this particular MIT students taking find receptors to be

717
00:41:48,740 --> 00:41:54,830
really quite remarkable kinds of devices the focus more on this intermediate step here of

718
00:41:54,830 --> 00:41:56,970
a generic nor on

719
00:41:57,120 --> 00:41:59,890
so here's my generic neuron and the first thing that has to be said is

720
00:41:59,890 --> 00:42:03,490
there are no generic neurons neurons are all very specific

721
00:42:03,640 --> 00:42:06,750
but i mean just use the generic neuron for the moment

722
00:42:06,770 --> 00:42:10,250
and the important features of the generic neuron here

723
00:42:10,270 --> 00:42:15,250
it has these funny little processes

724
00:42:15,300 --> 00:42:18,380
on its cell body

725
00:42:18,430 --> 00:42:21,590
there are called dendrites

726
00:42:21,600 --> 00:42:23,840
that's where it's going to receive

727
00:42:23,960 --> 00:42:27,840
signals processes to the name for things that stick out

728
00:42:27,890 --> 00:42:31,220
so these processes are called dendrites

729
00:42:31,240 --> 00:42:38,310
here's our nucleus of ourselves here's our cell body with label this year cell body

730
00:42:38,320 --> 00:42:42,300
with the nucleus and

731
00:42:44,250 --> 00:42:47,050
we have this very long

732
00:42:50,450 --> 00:42:52,320
called it acts on

733
00:42:53,540 --> 00:42:57,370
the axis the wire here this region here

734
00:42:57,420 --> 00:43:00,850
is called the axon hillock

735
00:43:00,900 --> 00:43:05,500
and it looks like a little hill that's what hilo means

736
00:43:05,550 --> 00:43:09,490
so feel like region here and that's where all the electrical signals from the cell

737
00:43:09,490 --> 00:43:14,740
body or integrate and a quote decision is made about whether to fire

738
00:43:14,860 --> 00:43:18,460
and the normal then fire signal down the length of attacks on

739
00:43:18,470 --> 00:43:20,480
and then will get to the end here

740
00:43:20,490 --> 00:43:23,860
where we have these terminal processes

741
00:43:23,870 --> 00:43:27,240
there will be some saying on other cells here

742
00:43:27,360 --> 00:43:29,560
these are called nerve terminals

743
00:43:33,930 --> 00:43:37,750
we'll make connections either with other dendrites

744
00:43:37,760 --> 00:43:39,460
or maybe with the muscle

745
00:43:40,890 --> 00:43:45,620
you wouldn't see both of those occurring from destroying is for illustration purposes here and

746
00:43:45,820 --> 00:43:51,480
it will send the signal and these points of contact are called synapses

747
00:43:51,490 --> 00:43:53,380
so nerve or muscle

748
00:43:57,180 --> 00:44:00,380
that year general picture of the synapse apps

749
00:44:00,390 --> 00:44:04,740
which transmits this is an electrical signal

750
00:44:04,790 --> 00:44:07,990
this is typically a chemical signal

751
00:44:08,040 --> 00:44:09,740
and as you might imagine

752
00:44:09,750 --> 00:44:12,650
the presynaptic cell is this guy

753
00:44:12,660 --> 00:44:16,170
the postsynaptic cell that guy

754
00:44:16,180 --> 00:44:18,930
it's also presynaptic

755
00:44:19,110 --> 00:44:22,030
post synaptic

756
00:44:22,100 --> 00:44:26,910
and what nerve biologists want to do is understand how does this all work

757
00:44:27,070 --> 00:44:33,040
how does the initial signal impinge upon the dendrite from snaps how does that signal

758
00:44:33,040 --> 00:44:38,260
get collected integrated into decision about whether to fire an action potential that runs along

759
00:44:38,260 --> 00:44:42,220
the nerve and then how does that get transmitted from electrical signal back chemical signal

760
00:44:42,220 --> 00:44:42,910
which then

761
00:44:42,920 --> 00:44:47,330
restarts an electrical signal in the next cell et cetera et cetera et cetera

762
00:44:47,390 --> 00:44:51,580
those are some of the kinds of questions asked now i said generic neurons cause

763
00:44:51,580 --> 00:44:53,600
there's really nothing generic about

764
00:44:53,640 --> 00:44:56,770
some neurons are very tiny

765
00:44:56,810 --> 00:45:00,790
some neurons are very big but the typical size of the ordinary you carry out

766
00:45:00,790 --> 00:45:02,200
acts on liver cells

767
00:45:03,020 --> 00:45:04,660
the ballpark

768
00:45:04,660 --> 00:45:07,900
the minimizer of the regularized risk or rigorous here

769
00:45:07,940 --> 00:45:09,770
is close to the best

770
00:45:12,380 --> 00:45:18,230
and the only way you can make a meaningful relationship is by introducing restrictions or

771
00:45:18,280 --> 00:45:21,710
assumptions are you on this

772
00:45:21,760 --> 00:45:25,070
OK so there is no contradiction here is just that

773
00:45:25,120 --> 00:45:27,190
all these things are valid but they

774
00:45:27,200 --> 00:45:30,070
don't tell you that this quantity would be small

775
00:45:30,120 --> 00:45:33,480
just tell you that this quantity can be related to some of the point

776
00:45:34,260 --> 00:45:35,520
maybe large

777
00:45:38,090 --> 00:45:49,380
OK the question is what is what come under assumptions for respecting the probability distribution

778
00:45:49,550 --> 00:45:53,870
of one people one is to assume that there is no noise which means that

779
00:45:53,890 --> 00:45:56,190
at each point perfect lassification

780
00:45:56,210 --> 00:46:01,860
and you can in addition but this would not really help

781
00:46:01,870 --> 00:46:04,380
but in addition you may assume that

782
00:46:04,430 --> 00:46:06,070
the target function

783
00:46:06,730 --> 00:46:10,190
this this far right so he started the one that minimizes

784
00:46:13,050 --> 00:46:14,840
you would assume that the star

785
00:46:16,340 --> 00:46:19,310
to some class so maybe belongs to

786
00:46:19,410 --> 00:46:23,410
this is difficult or maybe some bigger plan

787
00:46:23,420 --> 00:46:26,390
and this would be i mean this is the

788
00:46:26,400 --> 00:46:30,350
standard type of restriction that to impose that you have some kind of like something

789
00:46:30,350 --> 00:46:31,980
that belongs to

790
00:46:32,030 --> 00:46:34,840
fixed class that you know in advance

791
00:46:34,850 --> 00:46:37,990
and the distribution is derived from that

792
00:46:38,040 --> 00:46:42,510
which means the labels are i mean the labels all thing by applying this

793
00:46:42,520 --> 00:46:47,280
but then there often you don't have really restriction of how the axes are distributed

794
00:46:47,280 --> 00:46:52,460
apart from you know they are in the involved

795
00:47:13,350 --> 00:47:18,390
OK the question is is that the same assuming that it belongs to the prime

796
00:47:18,400 --> 00:47:23,850
is the same as assuming that our as looking for g in of function

797
00:47:23,910 --> 00:47:26,790
i mean

798
00:47:26,800 --> 00:47:31,670
you have all sorts of results so you have results that are just like you

799
00:47:31,670 --> 00:47:33,540
you're working with the class g

800
00:47:33,590 --> 00:47:34,830
d and u

801
00:47:34,840 --> 00:47:37,670
i'm looking for these gn in the same class

802
00:47:37,680 --> 00:47:41,210
and you say well what if my true function belongs to this class like this

803
00:47:41,290 --> 00:47:43,580
these are the best areas

804
00:47:43,590 --> 00:47:45,000
well i assume that

805
00:47:45,010 --> 00:47:47,550
the best classifier is linear classifiers

806
00:47:47,600 --> 00:47:49,770
and let's look for

807
00:47:49,820 --> 00:47:52,500
one such a classifier and c

808
00:47:52,550 --> 00:47:53,900
how well we're doing there

809
00:47:53,910 --> 00:47:57,320
other cases is

810
00:47:57,660 --> 00:47:59,530
you look at

811
00:48:00,280 --> 00:48:02,930
prime that is that contains

812
00:48:04,780 --> 00:48:07,420
like you're trying to approximate c

813
00:48:07,430 --> 00:48:11,710
a polynomial of degree two ways

814
00:48:13,960 --> 00:48:19,550
with linear classifiers or well i mean if would look for something where g is

815
00:48:19,550 --> 00:48:24,840
dense in the prime of the deep discovered from the slightly larger class but then

816
00:48:24,840 --> 00:48:26,270
you hope that this

817
00:48:26,350 --> 00:48:30,540
these classes he can approximate arbitrarily well right

818
00:48:33,440 --> 00:48:35,520
there can be all OK

819
00:48:36,890 --> 00:48:40,250
and i think that the of question

820
00:48:45,370 --> 00:48:49,820
o back

821
00:48:52,140 --> 00:48:56,320
mentioned these three ways of using the bomb i either looking at the actual value

822
00:48:56,320 --> 00:48:57,410
of the bound

823
00:48:57,430 --> 00:48:59,560
are using the value of the bond

824
00:48:59,570 --> 00:49:02,120
choose between different models or

825
00:49:02,140 --> 00:49:06,310
thing to look at really in qualitative where the bomb

826
00:49:06,330 --> 00:49:10,110
the the the first case the problem is that

827
00:49:10,160 --> 00:49:11,280
you know

828
00:49:11,330 --> 00:49:15,210
it's not easy to capture what happens with all these fluctuations because we have a

829
00:49:15,210 --> 00:49:18,410
large class of functions we already see that

830
00:49:18,490 --> 00:49:22,420
you know we when we look at the binomial there's it's not an easy formula

831
00:49:22,420 --> 00:49:23,910
to work with

832
00:49:23,960 --> 00:49:28,410
and you imagine that you you know all functions would have added a slightly different

833
00:49:28,410 --> 00:49:31,970
behavior because they have different error

834
00:49:31,980 --> 00:49:34,280
and they are all interrelated

835
00:49:34,290 --> 00:49:37,010
in the sense that

836
00:49:37,070 --> 00:49:40,640
you know if you look at the function the errors that they make are not

837
00:49:40,640 --> 00:49:44,360
independent in the sense that you know the cause of they are the

838
00:49:44,410 --> 00:49:47,990
the same year the more similar the air

839
00:49:48,000 --> 00:49:49,270
that they will make

840
00:49:50,490 --> 00:49:55,740
two which means it's all right this is empirical process is very complicated is it's

841
00:49:55,740 --> 00:49:57,110
hard to

842
00:49:57,160 --> 00:49:58,990
really get something

843
00:49:59,010 --> 00:50:01,730
died or you know i have an accurate estimator

844
00:50:01,770 --> 00:50:03,520
the population so

845
00:50:03,570 --> 00:50:07,060
this is one reason for why the violence usually have large value and you know

846
00:50:07,060 --> 00:50:11,290
you lost large constant very hard follow

847
00:50:11,300 --> 00:50:17,430
the techniques that we use them in the only thing that we can say about

848
00:50:17,430 --> 00:50:20,760
them is that they are sharp in the asymptotic regime so we can prove that

849
00:50:21,090 --> 00:50:24,780
you know the square root of pain is part of n is the right rate

850
00:50:24,780 --> 00:50:26,580
asymptotically but

851
00:50:26,870 --> 00:50:29,310
four finance

852
00:50:29,320 --> 00:50:32,690
values of and we might have you know

853
00:50:32,710 --> 00:50:36,450
in the case of the training role which had told you is the right measure

854
00:50:36,460 --> 00:50:39,310
for capturing the the deviation of the inference process

855
00:50:39,440 --> 00:50:42,870
you think have some constant in front of the integral

856
00:50:42,920 --> 00:50:47,230
and this might be very big and also when you in when you compute the

857
00:50:47,270 --> 00:50:49,050
covering numbers

858
00:50:49,060 --> 00:50:53,600
computing the covering numbers is not an easy thing and also in this covering numbers

859
00:50:53,660 --> 00:50:59,790
you might have very large constant because you always are forced to kind of overestimating

860
00:51:03,730 --> 00:51:04,730
i mean there are

861
00:51:04,790 --> 00:51:10,380
slightly more precise techniques that than the ones i mentioned but again they are even

862
00:51:10,380 --> 00:51:11,900
more complicated

863
00:51:11,910 --> 00:51:13,030
we implement

864
00:51:18,200 --> 00:51:20,210
in the way you want to have

865
00:51:20,270 --> 00:51:22,300
there's this trade right you can have

866
00:51:22,350 --> 00:51:23,490
well that are

867
00:51:23,500 --> 00:51:28,610
very large and very nice to write like you know this kind of thing or

868
00:51:28,610 --> 00:51:30,960
you can have found that there bit smaller

869
00:51:30,960 --> 00:51:33,600
in fact the too margin point six

870
00:51:33,620 --> 00:51:38,750
OK so the problem is that even after this time and the reason is because

871
00:51:38,760 --> 00:51:40,850
at this is difficult to analyse

872
00:51:40,920 --> 00:51:45,870
and it's because usually what you do well the margin doesn't increased at every iteration

873
00:51:45,870 --> 00:51:49,410
a kind of bubbles up and down the visual tricks don't work is usually what

874
00:51:49,410 --> 00:51:52,840
you do is you add up the increases in the margin you know each iteration

875
00:51:52,840 --> 00:51:55,760
you see whether got to the maximum but in this case since it goes up

876
00:51:55,760 --> 00:51:57,780
and down you can do that

877
00:51:57,800 --> 00:52:00,260
so we really need some kind of new approach

878
00:52:00,670 --> 00:52:04,210
to understand the convergence of the algorithm

879
00:52:04,230 --> 00:52:07,410
and so i will give you in suspense any longer

880
00:52:07,420 --> 00:52:10,440
the answer is actually no

881
00:52:10,460 --> 00:52:13,040
it's exactly the opposite of what everyone thought

882
00:52:13,050 --> 00:52:17,570
in fact at this may converge to a margin that is significantly below the maximum

883
00:52:18,420 --> 00:52:23,950
and what's even more interesting is that russian actually were were

884
00:52:23,970 --> 00:52:27,900
they were even more right than they thought there are bound is actually tight

885
00:52:27,920 --> 00:52:31,140
so in other words not optimal adaboost

886
00:52:31,160 --> 00:52:34,420
you can you can actually get to converge to exactly the rebound

887
00:52:34,440 --> 00:52:38,400
so i can get this to converge to exactly and margin of wire rope in

888
00:52:38,420 --> 00:52:43,340
very specific limiting condition which i'll describe a little bit later on this theorem down

889
00:52:43,340 --> 00:52:46,950
here a specific is the more general theorem which i hope to be able to

890
00:52:46,950 --> 00:52:50,400
see later but what i'm going to do for the rest of my talk is

891
00:52:50,400 --> 00:52:52,680
i'm going to prove this here

892
00:52:52,700 --> 00:52:54,400
the question

893
00:52:54,420 --> 00:53:03,170
OK so i will introduce notation and image so be able to answer your questions

894
00:53:04,840 --> 00:53:07,900
so this is an overview of my talk the first thing i wanted to do

895
00:53:07,900 --> 00:53:11,820
was get the full history about the margin theory producing and i finished that know

896
00:53:11,820 --> 00:53:14,970
what i'm going to do is give introduction to adaboost i know you've already seen

897
00:53:14,970 --> 00:53:18,250
it but i know that a lot of you have haven't seen before yesterday so

898
00:53:18,250 --> 00:53:19,840
i'm going to go over it again

899
00:53:19,890 --> 00:53:22,660
and then i'm going to spend the rest of my talk giving the proof of

900
00:53:22,690 --> 00:53:27,110
the theorem that this doesn't always converge to the maximum margin solution and the proof

901
00:53:27,110 --> 00:53:32,120
of the theorem consists of reducing adaboost to a dynamical system in order to understand

902
00:53:32,120 --> 00:53:36,760
its convergence properties so in other words we have analysis problem we're trying to understand

903
00:53:36,760 --> 00:53:40,470
convergence of the algorithm what we did was we converted into a physics problem in

904
00:53:40,470 --> 00:53:42,710
order to solve it

905
00:53:42,730 --> 00:53:46,880
OK so so here is simple problem you can keep in mind just to have

906
00:53:46,880 --> 00:53:48,330
something in mind

907
00:53:48,340 --> 00:53:51,810
OK so what do you have a database of news articles and the articles are

908
00:53:51,810 --> 00:53:56,600
labeled plus one of the categories the articles entertainment and negative one otherwise

909
00:53:56,620 --> 00:54:01,060
and what did classification setting goals given a new article which is not in the

910
00:54:01,060 --> 00:54:04,930
database we want the computer to automatically be able to find label so to tell

911
00:54:04,930 --> 00:54:09,570
us whether legitimate or not entertainment so just you know wait for classification we have

912
00:54:09,740 --> 00:54:13,410
a bunch of algorithms first first you know following this kind of problems and we

913
00:54:13,410 --> 00:54:15,500
have boosting and

914
00:54:15,510 --> 00:54:18,700
listing can be used as i mentioned in two ways the optimal case in the

915
00:54:18,700 --> 00:54:22,440
non optimal case you can use that you can use the algorithm to do most

916
00:54:22,440 --> 00:54:23,630
of the work for you

917
00:54:23,700 --> 00:54:26,030
for example with this text

918
00:54:26,050 --> 00:54:29,880
you can be sort of very basic features and come up with some combination of

919
00:54:29,880 --> 00:54:33,630
some very basic features or you can use it as a wrapper for another algorithm

920
00:54:33,630 --> 00:54:37,580
you can plot but on top of the decision tree or neural network which would

921
00:54:37,580 --> 00:54:40,620
mean that you're probably using in the non optimal case

922
00:54:42,060 --> 00:54:43,720
see here

923
00:54:44,170 --> 00:54:48,110
so just to formalise the problem for you as usual we have this

924
00:54:48,190 --> 00:54:52,160
training data which is in the form of ordered pairs return to paris randomly from

925
00:54:52,160 --> 00:54:56,860
an unknown probability distribution on the space express negative one

926
00:54:56,870 --> 00:55:01,120
and you remember access the space of all possible news articles are negative one one

927
00:55:01,120 --> 00:55:02,490
of the labels

928
00:55:02,500 --> 00:55:04,830
and so you know given new article

929
00:55:04,840 --> 00:55:07,250
what's what's the label

930
00:55:07,300 --> 00:55:11,680
and how do we construct classifiers course we're going to do the usual thing which

931
00:55:11,680 --> 00:55:15,310
is we're going to divide our our space x into two sections based on the

932
00:55:15,310 --> 00:55:16,560
sign of the function

933
00:55:16,570 --> 00:55:20,260
and the decision boundary is going to be the zero level set of functions so

934
00:55:20,260 --> 00:55:22,940
what we're trying to do is we're trying to construct a function in a so

935
00:55:22,940 --> 00:55:27,280
that level that is in the right place to be in this decision boundary

936
00:55:27,370 --> 00:55:31,320
OK so now we're going to go into thing what they that we have a

937
00:55:31,320 --> 00:55:36,040
weak learning algorithm and remember learning algorithm produces weak classifiers and you should think about

938
00:55:36,040 --> 00:55:38,110
we classifier as a rule of thumb

939
00:55:38,130 --> 00:55:39,410
so for example

940
00:55:39,420 --> 00:55:45,670
if we're studying these articles into entertainment not entertainment we classifier might be too

941
00:55:45,680 --> 00:55:49,390
take a look at the article see whether it contains the term movie and classifier

942
00:55:49,390 --> 00:55:52,530
based on that so take a look at the article if it contains the term

943
00:55:52,530 --> 00:55:54,290
movie so you entertainment

944
00:55:54,310 --> 00:56:00,180
the content removing not entertainment and of course this is you know not a particularly

945
00:56:00,180 --> 00:56:04,630
good way to classify things but it gives us some information at least and so

946
00:56:04,630 --> 00:56:08,250
what we're going to do is to introduce a couple more weak classifiers they you

947
00:56:08,250 --> 00:56:10,020
could check for the term actor

948
00:56:10,040 --> 00:56:11,700
you could take for the term drama

949
00:56:11,740 --> 00:56:16,230
but instead of using any of these weak classifiers individually you want to combine them

950
00:56:16,230 --> 00:56:19,770
so that you can get something stronger and that's what that's why can use that

951
00:56:20,470 --> 00:56:24,920
because it's going to combine classifiers in a meaningful way we classifiers in a meaningful

952
00:56:24,920 --> 00:56:26,760
way so for example

953
00:56:26,770 --> 00:56:30,080
the function f is going to be the final point four times each one plus

954
00:56:30,080 --> 00:56:32,830
three ten sixty two plus point times age three

955
00:56:32,850 --> 00:56:37,170
so that if the article contains the term movie which means each one is one

956
00:56:37,180 --> 00:56:41,030
it contains the term drama which means village three is one but it doesn't contain

957
00:56:41,030 --> 00:56:43,620
the term actor which means should minus one

958
00:56:43,670 --> 00:56:48,900
then the value of this side point four minus three point three so it positive

959
00:56:48,900 --> 00:56:52,780
one so we say that this articles entertainment you know it contains two out of

960
00:56:52,780 --> 00:56:56,790
these three terms and even though it didn't contain the last term we think that

961
00:56:56,840 --> 00:57:01,280
you know contains two of these three terms it's an article about entertainment

962
00:57:02,290 --> 00:57:04,590
so what i'm really trying to say here

963
00:57:04,630 --> 00:57:08,540
is that you can think of boosting algorithm as doing the following

964
00:57:08,550 --> 00:57:11,090
you can think of it as taking in

965
00:57:11,110 --> 00:57:11,930
which is

966
00:57:11,940 --> 00:57:13,420
the weak learning algorithm

967
00:57:13,450 --> 00:57:17,020
which produces these weak classifiers h one h two and h three

968
00:57:17,040 --> 00:57:20,880
and it takes a large training database of labelled examples

969
00:57:20,900 --> 00:57:25,540
and outputs the coefficients of the weak classifiers to make the combined classifier

970
00:57:25,560 --> 00:57:29,210
so at which point four point three and point three

971
00:57:29,230 --> 00:57:33,280
OK and so just to go over what was that again

972
00:57:33,290 --> 00:57:35,370
so with adaboost

973
00:57:35,390 --> 00:57:38,930
what we do is we have these labelled training examples

974
00:57:38,950 --> 00:57:41,620
and we're going to assign an importance each

975
00:57:41,630 --> 00:57:43,340
training example

976
00:57:43,350 --> 00:57:47,520
and at the very beginning o and the way the weights on the training examples

977
00:57:47,520 --> 00:57:52,070
are going to tell the weak learning algorithm which examples are going to be important

978
00:57:52,150 --> 00:57:55,190
and at the very beginning we don't know which examples are more important than what

979
00:57:55,190 --> 00:58:00,420
other examples so we give all the training instances equal weights

980
00:58:00,440 --> 00:58:03,920
OK so then we take the weighted training examples and feed them into the weak

981
00:58:03,920 --> 00:58:06,440
learning algorithm and it gives us the weak classifiers

982
00:58:06,480 --> 00:58:08,560
and the class register also

983
00:58:08,570 --> 00:58:12,900
it's we week australia says he should be positive and should be negative

984
00:58:14,590 --> 00:58:17,630
so if you're wrong

985
00:58:17,880 --> 00:58:19,500
because there's only rule

986
00:58:19,510 --> 00:58:21,430
in fact gets one from

987
00:58:21,490 --> 00:58:24,630
so what we're going to do is say OK at the next round we got

988
00:58:24,630 --> 00:58:27,070
this wrong so we're going to make them more important

989
00:58:27,140 --> 00:58:31,340
and everybody else get there which produced because we got the correct OK so we

990
00:58:31,340 --> 00:58:36,650
repeat this over and over again we can take a weighted training examples to them

991
00:58:36,650 --> 00:58:39,870
into the weak learning algorithm receives a weak classifiers

992
00:58:39,880 --> 00:58:42,940
and it gets most of examples right but this is if you and so we're

993
00:58:42,940 --> 00:58:45,690
going to increase the weights on those examples and increase the weights and all the

994
00:58:45,690 --> 00:58:49,380
others and then we repeat this over and over again until we're completely blue in

995
00:58:49,380 --> 00:58:50,430
the face

996
00:58:50,450 --> 00:58:55,630
and the and we can only make a linear combination of the weak classifiers obtained

997
00:58:55,630 --> 00:58:56,850
all the iterations

998
00:58:56,870 --> 00:59:00,490
so our final combined classifier look something like this in this case get all the

999
00:59:00,490 --> 00:59:02,210
training examples correct

1000
00:59:02,220 --> 00:59:05,720
OK so the final combine clusters of this form it's

1001
00:59:05,720 --> 00:59:09,230
because this holds for all closeness et al so in particular it holds for the

1002
00:59:09,230 --> 00:59:12,490
courts which minimizes the particular sequence you get

1003
00:59:12,590 --> 00:59:17,400
so how can you construct such code well it's very simple

1004
00:59:19,080 --> 00:59:21,110
basically first

1005
00:59:21,130 --> 00:59:23,220
and called so you list

1006
00:59:23,230 --> 00:59:25,660
all the coast in here

1007
00:59:25,980 --> 00:59:29,830
let's say there of them sold in the list goes from one to n and

1008
00:59:29,830 --> 00:59:31,930
the first thing called the number

1009
00:59:31,960 --> 00:59:34,990
using the uniform code that takes two log n bits

1010
00:59:35,010 --> 00:59:37,980
and if you do with the way we call it a b c d here

1011
00:59:37,980 --> 00:59:41,060
with the same number of bits for each outcome you can coat

1012
00:59:41,070 --> 00:59:43,030
an element of the set was

1013
00:59:43,050 --> 00:59:45,260
m elements by log n bits of then

1014
00:59:45,280 --> 00:59:48,750
OK be look and where m is the number of elements

1015
00:59:48,770 --> 00:59:50,220
in the set curly l

1016
00:59:50,240 --> 00:59:53,610
and no matter what they do you get your overhead compared to the best code

1017
00:59:53,610 --> 00:59:58,070
for the data is logged the tools no matter how long the data sequences

1018
00:59:58,110 --> 01:00:03,560
so for long data sequences quite nice

1019
01:00:04,720 --> 01:00:07,880
now we make the crucial jump from universal codes

1020
01:00:07,940 --> 01:00:11,940
two what has been called universal models

1021
01:00:12,860 --> 01:00:15,820
suppose i have

1022
01:00:15,820 --> 01:00:20,620
a set of probability distributions now now we're getting a bit closer to statistics machine

1023
01:00:20,620 --> 01:00:25,600
learning a probabilistic model but for the moment we also assume that finite later we

1024
01:00:25,600 --> 01:00:28,390
will look at more realistic infinite models

1025
01:00:29,820 --> 01:00:31,440
using basic notation

1026
01:00:31,470 --> 01:00:34,690
i can describe more more like this it is

1027
01:00:35,590 --> 01:00:38,250
you don't want to see to so

1028
01:00:38,250 --> 01:00:42,790
we've just seen that there always exists a code

1029
01:00:42,800 --> 01:00:45,430
relative to a set of codes such that i have

1030
01:00:45,480 --> 01:00:48,100
overheads finite overheads

1031
01:00:48,120 --> 01:00:50,790
OK compared to the best coach in that sense

1032
01:00:50,790 --> 01:00:53,650
so now what i'm going to do is i'm going to turn this set of

1033
01:00:53,650 --> 01:00:56,560
distributions into set of codes

1034
01:00:56,580 --> 01:01:01,670
by the trick i described earlier so each of these distributions induces a cold

1035
01:01:01,680 --> 01:01:03,430
so for all outcomes

1036
01:01:03,570 --> 01:01:08,210
link is minus log the the probability of the outcome

1037
01:01:08,890 --> 01:01:12,860
this means that if i turn it into a set of codes

1038
01:01:12,870 --> 01:01:16,250
i can now construct a new coat

1039
01:01:17,030 --> 01:01:20,260
no matter what outcomes to get the number of bits and need is more equal

1040
01:01:20,270 --> 01:01:24,320
and minus log probability of the outcome according to any of these the test class

1041
01:01:24,320 --> 01:01:29,020
k where k will again be for example org

1042
01:01:33,320 --> 01:01:35,800
this is a universal code

1043
01:01:35,820 --> 01:01:39,100
and now i can also mapped back again to distribution

1044
01:01:39,120 --> 01:01:41,080
and that is called universal

1045
01:01:41,090 --> 01:01:44,850
model universal distribution in information theory literature

1046
01:01:44,870 --> 01:01:49,390
and what i do here is i met this back to distribution

1047
01:01:50,380 --> 01:01:54,180
because for and i can do that for any code there must also be distribution

1048
01:01:54,270 --> 01:01:57,770
such that for outcomes minus log probability of the outcome

1049
01:01:57,780 --> 01:02:03,510
small equal to minus log proactively outcome according to any theta plus the constant overhead

1050
01:02:03,530 --> 01:02:10,020
so if i exponentiate on both sides this means that this distribution is actually dominates

1051
01:02:10,030 --> 01:02:13,830
this set of distributions there are some constants such that no matter what outcomes i

1052
01:02:13,830 --> 01:02:18,660
get the probability assigned to this sequence is large equal the constant

1053
01:02:18,680 --> 01:02:23,030
times the probability i sin two theta for any time i model

1054
01:02:23,040 --> 01:02:27,550
and such a distribution which dominates the set of distribution is called universal model are

1055
01:02:27,550 --> 01:02:29,250
universal distribution

1056
01:02:29,260 --> 01:02:30,590
for the set

1057
01:02:30,960 --> 01:02:40,820
so now something like terminology small side so there's something confusing here because when i

1058
01:02:40,830 --> 01:02:45,980
i call this that curry and a model that's common terminology in statistics so model

1059
01:02:45,980 --> 01:02:50,420
is a family of distributions but information theory the word model is used to single

1060
01:02:50,440 --> 01:02:55,530
distribution and universal model is an information theoretic concepts so o

1061
01:02:55,530 --> 01:03:00,870
a universe so they are using information theoretic terminology so when i a universal model

1062
01:03:01,020 --> 01:03:03,260
we mean a single distribution

1063
01:03:03,270 --> 01:03:07,700
but this is a single distribution which acts as a representative for set of distributions

1064
01:03:07,700 --> 01:03:09,650
said that dominates it

1065
01:03:09,670 --> 01:03:15,400
and the set of distributions confusingly also called model

1066
01:03:15,420 --> 01:03:20,580
so now we've already seen one example of universal models this was basically is worked

1067
01:03:20,590 --> 01:03:23,650
if you look at it in terms of code by coding in two stages first

1068
01:03:23,650 --> 01:03:25,280
recorded index

1069
01:03:25,320 --> 01:03:29,490
of the set of codes and then you quote the data using

1070
01:03:29,500 --> 01:03:31,230
the code with the index

1071
01:03:31,260 --> 01:03:32,980
you just think of

1072
01:03:33,000 --> 01:03:34,870
but and this is crucial

1073
01:03:34,900 --> 01:03:36,290
inside again

1074
01:03:36,330 --> 01:03:40,070
there are many more universal models and that these two part way of coding is

1075
01:03:40,070 --> 01:03:44,210
just one way of doing it and actually often not the cleverest way

1076
01:03:44,230 --> 01:03:47,640
so another way which may be familiar to most of you

1077
01:03:47,670 --> 01:03:51,050
it's coding by using a bayesian mixture

1078
01:03:51,110 --> 01:03:54,800
so suppose i have some prior distribution over my set

1079
01:03:54,800 --> 01:03:57,360
of my model my set of distributions

1080
01:03:57,380 --> 01:04:01,070
then i can define the bayesian marginal distribution

1081
01:04:01,080 --> 01:04:04,440
the marginal likelihood in this case but the defines a distribution

1082
01:04:04,470 --> 01:04:08,430
because if you some over all sequences it becomes one so this is the weighted

1083
01:04:08,430 --> 01:04:12,710
average of the probability of the data according to see ten times the prior of

1084
01:04:12,710 --> 01:04:13,820
the heat

1085
01:04:13,850 --> 01:04:15,600
this is the common

1086
01:04:15,640 --> 01:04:19,270
bayesian way to define a distribution over

1087
01:04:19,270 --> 01:04:22,810
sequence of data

1088
01:04:24,300 --> 01:04:26,090
this distribution

1089
01:04:26,120 --> 01:04:30,020
defines a universal model relative to the set curly

1090
01:04:30,030 --> 01:04:34,650
because we have the following for all outcomes of all links

1091
01:04:34,670 --> 01:04:37,440
for all theta set career

1092
01:04:37,530 --> 01:04:41,730
we have the code length if you code to data using the code corresponding to

1093
01:04:41,730 --> 01:04:47,330
this base distribution so that minus log the probability of this distribution is by definition

1094
01:04:47,330 --> 01:04:50,860
minus love this sort of of this sum is efforts

1095
01:04:51,070 --> 01:04:55,040
there is some is larger than each of its terms someone's love of some smaller

1096
01:04:55,050 --> 01:04:56,880
the needs of its terms

1097
01:04:56,900 --> 01:05:01,660
so this must be smaller equal and minus log the probability of the data according

1098
01:05:01,660 --> 01:05:03,530
to particular seat

1099
01:05:03,540 --> 01:05:07,840
minus log the prior theta and this holds for all theta and you're set

1100
01:05:07,860 --> 01:05:11,320
so this means that the code length if you code using

1101
01:05:11,360 --> 01:05:14,400
the code corresponding to the base distribution

1102
01:05:14,410 --> 01:05:18,030
it's more equal than the code length corresponding to any individual

1103
01:05:18,030 --> 01:05:21,070
element in your set of distributions

1104
01:05:21,080 --> 01:05:24,800
plus a term which does not depend on and so this makes it universal again

1105
01:05:24,800 --> 01:05:27,470
because if n is large this remains constant

1106
01:05:27,480 --> 01:05:29,230
you see that's

1107
01:05:29,240 --> 01:05:32,230
in the end you're doing essentially as well as

1108
01:05:32,280 --> 01:05:35,390
the best distribution in your model because this holds for all theta

1109
01:05:35,410 --> 01:05:40,060
in particular the theta which featured data best which gives the highest probability

1110
01:05:40,130 --> 01:05:46,350
and therefore the smallest code with hindsight to data

1111
01:05:47,120 --> 01:05:49,400
let's compare the previous

1112
01:05:49,430 --> 01:05:51,080
universal codes we've seen

1113
01:05:51,090 --> 01:05:52,750
in which we will now

1114
01:05:59,210 --> 01:06:06,980
not really and saying that the goal here is to compress data as well as

1115
01:06:06,980 --> 01:06:09,210
good as possible compared to some set

1116
01:06:09,230 --> 01:06:10,420
of quotes

1117
01:06:10,420 --> 01:06:13,130
and i'm saying you can do this in in different ways

1118
01:06:13,150 --> 01:06:17,000
so one way to code in two stages first recorded index and then you could

1119
01:06:17,000 --> 01:06:21,270
data with the code corresponding to the index another way is to first define the

1120
01:06:21,270 --> 01:06:23,130
mixture distribution

1121
01:06:23,170 --> 01:06:27,960
so the first met the coats to distributions then define a mixture distributions and then

1122
01:06:27,960 --> 01:06:29,840
map that back to cope

1123
01:06:29,860 --> 01:06:33,630
that's another way of achieving the same goal and because

1124
01:06:33,650 --> 01:06:36,920
what what will happen now as we will see two more ways

1125
01:06:36,940 --> 01:06:40,420
of achieving the same goal and then we'll start comparing

1126
01:06:40,460 --> 01:06:44,980
so not comparing this bayesian method for constructing code is the two-part method we do

1127
01:06:44,980 --> 01:06:47,440
it in two stages

1128
01:06:47,440 --> 01:06:49,580
and so

1129
01:06:49,590 --> 01:06:52,840
course you can extend this two-part method also

1130
01:06:52,860 --> 01:06:55,670
so using a prior on the set of the task

1131
01:06:55,690 --> 01:07:00,340
because each distribution on the set of features and uses code for encoding the task

1132
01:07:00,520 --> 01:07:04,480
with code length minus log prior theta

1133
01:07:04,560 --> 01:07:08,190
so then if you could the data by first according to peter which maximizes the

1134
01:07:08,190 --> 01:07:10,210
probability of the data

1135
01:07:10,210 --> 01:07:16,990
world series from interacting with peter hunt group in auckland new zealand who were the

1136
01:07:16,990 --> 01:07:18,790
first start to

1137
01:07:20,110 --> 01:07:25,190
large anatomical models of the whole ventricle

1138
01:07:25,200 --> 01:07:31,660
part based course on experimental information and represented by tens of millions of grid points

1139
01:07:32,030 --> 01:07:34,370
into which was then possible

1140
01:07:34,400 --> 01:07:36,100
to introduce

1141
01:07:36,120 --> 01:07:43,000
the equations for electrical excitation that we developed set levels to produce impressive reconstructions in

1142
01:07:43,000 --> 01:07:48,160
this case is the spread of electrical excitation through the whole ventricle

1143
01:07:50,840 --> 01:07:51,930
and then later

1144
01:07:51,940 --> 01:07:56,270
two reconstructed with arrhythmias this is from the work of peter cole my group

1145
01:07:56,320 --> 01:07:58,310
get to tell you try another

1146
01:07:58,360 --> 01:08:03,480
in the united states in which they reconstructed the

1147
01:08:03,480 --> 01:08:06,200
fatal arrhythmia occurs

1148
01:08:06,250 --> 01:08:08,250
you had to be on the golf course

1149
01:08:08,270 --> 01:08:10,390
and the ball hits u

1150
01:08:10,440 --> 01:08:13,850
the chest just in the wrong place in just the wrong time during cardiac we

1151
01:08:13,850 --> 01:08:17,030
polarization they introduced french

1152
01:08:17,070 --> 01:08:24,100
activated channels which can be coates analyzed experimentally in the heart

1153
01:08:24,120 --> 01:08:26,190
to initiate

1154
01:08:26,220 --> 01:08:29,020
this re entrant a risk here in the

1155
01:08:29,080 --> 01:08:31,300
model of the whole ventricle

1156
01:08:31,310 --> 01:08:33,140
so what you're seeing here

1157
01:08:33,190 --> 01:08:37,460
is re-entry text citations and determined by

1158
01:08:37,530 --> 01:08:41,150
but the equations the cellular activity

1159
01:08:41,210 --> 01:08:47,290
the anatomical arrangement of those tens of millions of critical points in the anatomical model

1160
01:08:47,290 --> 01:08:52,530
and of course the incorporation of the stretch activated channels that enables the process in

1161
01:08:52,530 --> 01:08:53,890
this case to be

1162
01:08:55,530 --> 01:09:01,830
session panfilov later went on to have a look at the way in which such

1163
01:09:02,140 --> 01:09:07,520
as we enter into a written is to break down into the phenomenon of fibrillation

1164
01:09:07,550 --> 01:09:10,500
is every entrant arrhythmia now breaking down

1165
01:09:10,520 --> 01:09:16,700
into multiple wavelengths which the heart would no longer path

1166
01:09:16,710 --> 01:09:19,260
like this but but simply shim

1167
01:09:19,280 --> 01:09:25,440
and ceased pumping this recording by the koskie using voltage sensitive dyes

1168
01:09:25,450 --> 01:09:30,410
on animal park showing the same kind of phenomena

1169
01:09:30,440 --> 01:09:33,530
that works all being done

1170
01:09:34,520 --> 01:09:36,820
in animals recently with

1171
01:09:36,870 --> 01:09:38,830
and the group

1172
01:09:38,880 --> 01:09:42,290
sasha and often because they tend to share in

1173
01:09:42,290 --> 01:09:45,300
utrecht we collaborated and put together

1174
01:09:45,320 --> 01:09:47,600
humans model

1175
01:09:47,610 --> 01:09:50,550
and this nice simulation in the next slide

1176
01:09:50,560 --> 01:09:52,910
from august and ten two shoes works

1177
01:09:52,990 --> 01:09:55,480
simulating every entrant arrhythmia

1178
01:09:55,500 --> 01:09:57,240
in the human heart

1179
01:09:57,250 --> 01:10:04,250
and i notice incidentally was lovely abstract page sixty four abstract but by custom revealing

1180
01:10:04,270 --> 01:10:09,850
how modeling can help one to understand the differences between human and animal parts in

1181
01:10:09,850 --> 01:10:13,300
relation to the process of population

1182
01:10:13,330 --> 01:10:16,560
so the last two or three minutes

1183
01:10:16,570 --> 01:10:20,310
i want to give you the last three principles

1184
01:10:20,330 --> 01:10:28,750
no programs genetic programs no programs at any other level

1185
01:10:28,850 --> 01:10:31,030
if you like my

1186
01:10:31,040 --> 01:10:35,510
metaphore of the music of life then it doesn't have a conductor

1187
01:10:35,530 --> 01:10:38,620
i was delighted when oxford university press

1188
01:10:39,640 --> 01:10:40,800
this lovely

1189
01:10:41,660 --> 01:10:45,190
by the japanese paper artist he had and i don't

1190
01:10:46,840 --> 01:10:49,410
a violin butterfly

1191
01:10:49,430 --> 01:10:54,200
he's a brilliant paper artist so this is actually a cut-out and the shadows therefore

1192
01:10:54,280 --> 01:10:59,820
real shadows and you notice that the

1193
01:10:59,840 --> 01:11:01,280
violin butterfly

1194
01:11:01,300 --> 01:11:03,290
is being played

1195
01:11:03,300 --> 01:11:05,070
but there's no player

1196
01:11:06,600 --> 01:11:08,830
that's a lovely

1197
01:11:08,850 --> 01:11:13,510
illustration from michael because part of what i'm saying is of course the

1198
01:11:13,520 --> 01:11:20,200
organisations like doesn't have conducted if you like the orchestra analogy then it's like one

1199
01:11:20,200 --> 01:11:23,200
of these orchestras that can play effectively

1200
01:11:23,450 --> 01:11:30,120
without to conductor the review by thomas limberger in and reports that says more about

1201
01:11:30,120 --> 01:11:32,930
that which i think is full of insight

1202
01:11:32,950 --> 01:11:39,990
eight principle no programs at any other level including instant to break many people to

1203
01:11:39,990 --> 01:11:44,030
whom i i lectured on the subject and try to introduce some principles of systems

1204
01:11:44,030 --> 01:11:48,010
biology we tend to say to me then you may be right about viruses you

1205
01:11:48,010 --> 01:11:51,870
may be right about policy might even be right about simple animals but my goodness

1206
01:11:51,870 --> 01:11:55,080
is surely the programs here

1207
01:11:55,100 --> 01:12:00,800
well i think that concept is also incorrect and i explained why

1208
01:12:00,810 --> 01:12:03,240
in chapter nine

1209
01:12:03,240 --> 01:12:09,430
i got some very very distinguished opponents include francis crick

1210
01:12:09,450 --> 01:12:10,640
who once

1211
01:12:12,120 --> 01:12:15,260
there were poised for the greatest revolution of all

1212
01:12:16,560 --> 01:12:19,180
consciousness and it is

1213
01:12:19,200 --> 01:12:21,010
comments rededicate by

1214
01:12:21,030 --> 01:12:25,660
his colleague of course in the double helix work jim watson there are only molecules

1215
01:12:25,660 --> 01:12:30,310
everything else is sociology but tell you is that true then all i've been doing

1216
01:12:30,310 --> 01:12:33,220
for the last forty years is just sociology

1217
01:12:35,890 --> 01:12:38,180
it leads to very big mistake

1218
01:12:38,200 --> 01:12:42,450
and it's the one is illustrated by this last quote from create just before he

1219
01:12:43,350 --> 01:12:47,370
he said i think the secret of consciousness lies in the cluster

1220
01:12:47,390 --> 01:12:48,390
thank you

1221
01:12:48,410 --> 01:12:51,260
classroom is a thin layer

1222
01:12:51,600 --> 01:12:57,330
cells in the cortex and the reason is selected this was is connected to almost

1223
01:12:57,330 --> 01:12:59,660
everything else

1224
01:12:59,660 --> 01:13:05,370
there is no such location of consciousness there's no such location of the program of

1225
01:13:05,370 --> 01:13:06,430
that nature

1226
01:13:06,660 --> 01:13:11,950
and that itself is a fundamental mistake is a mistake in fact very similar to

1227
01:13:11,950 --> 01:13:15,470
the one that was initially made in western philosophy by

1228
01:13:15,530 --> 01:13:20,120
and they can be thought of course that there was an actual separate substance which

1229
01:13:20,120 --> 01:13:25,560
the set of of points into a kind of you have of planes if you want

1230
01:13:25,580 --> 01:13:26,310
you know f

1231
01:13:29,630 --> 01:13:33,500
so that the sum of the distance to the lines are and are being

1232
01:13:33,730 --> 01:13:35,420
the function we're looking for

1233
01:13:35,860 --> 01:13:39,100
i k means we'll just put a whole bunch of prototypes all around the surface

1234
01:13:39,320 --> 01:13:41,880
seems perfect but it does work in high dimension of course

1235
01:13:42,440 --> 01:13:44,610
ok so this concentrate on this idea sparsity

1236
01:13:45,070 --> 01:13:49,900
so fast learning is uses this energy function this type which has to

1237
01:13:49,920 --> 01:13:54,230
organize what is the input and the other one is a quote vector which is related to variables are

1238
01:13:54,270 --> 01:13:55,120
going to infer

1239
01:13:55,370 --> 01:13:56,600
and the energy function

1240
01:13:56,850 --> 01:14:00,710
is the square reconstruction error we multiply a vector by a matrix

1241
01:14:00,960 --> 01:14:04,970
code dictionary matrix and then we add or regularization term which is the sum of the two

1242
01:14:04,990 --> 01:14:06,580
values of the components of the

1243
01:14:07,000 --> 01:14:07,570
of the code

1244
01:14:07,780 --> 01:14:09,500
if we know a picture matrix

1245
01:14:09,810 --> 01:14:15,230
are given why we find z here that minimizes this energy on tion and gives us

1246
01:14:15,250 --> 01:14:15,940
feature vector

1247
01:14:16,270 --> 01:14:17,730
is going to be high dimensional how

1248
01:14:17,880 --> 01:14:18,730
shown sparse

1249
01:14:20,120 --> 01:14:24,910
so has a non-linear mapping one that maps y to the optimal z is on the mapping

1250
01:14:27,240 --> 01:14:30,600
but it's so to compute because you have to do this are in operation

1251
01:14:32,480 --> 01:14:37,780
you can learn addition a matrix of course with doesn't field told us to do this on

1252
01:14:37,800 --> 01:14:38,400
time ago

1253
01:14:38,650 --> 01:14:41,840
using just as he basically just as casagrande sat

1254
01:14:42,330 --> 01:14:48,230
to minimize this over the overall average energy of a collection of samples training samples

1255
01:14:48,540 --> 01:14:51,170
columns that you have to minimize this to work

1256
01:14:53,720 --> 01:14:58,630
and the manifold assumption as hidden behind the sparse coding is basically that the data

1257
01:14:58,810 --> 01:15:02,900
is fits sort of union of flow dimensional planes

1258
01:15:03,190 --> 01:15:04,510
with dimension image plane

1259
01:15:04,660 --> 01:15:07,590
corresponds to the number of components z that non-zero

1260
01:15:07,900 --> 01:15:09,240
once we do this musician

1261
01:15:13,070 --> 01:15:19,040
so in general though we have several ways of designing unsupervised learning algorithms one is

1262
01:15:19,280 --> 01:15:22,960
number one is to construct energy function so that the volume of stuff that takes energy

1263
01:15:23,290 --> 01:15:25,580
a is fixed because the problem we have is that

1264
01:15:25,960 --> 01:15:27,480
we have to build this energy function

1265
01:15:27,690 --> 01:15:32,460
so that has low energy on the points on the data points on a manifold higher energy everywhere else

1266
01:15:32,660 --> 01:15:33,900
and is easy enough to

1267
01:15:34,230 --> 01:15:37,680
treat the parameters of an energy function to take the values of the points ok just

1268
01:15:37,700 --> 01:15:38,320
to a

1269
01:15:38,450 --> 01:15:39,080
great descent

1270
01:15:39,680 --> 01:15:42,640
but how do you make sure the energy is high everywhere else

1271
01:15:43,160 --> 01:15:45,410
and that's basically it's basically the

1272
01:15:45,640 --> 01:15:48,910
the a partition function problem you know that people have been

1273
01:15:49,250 --> 01:15:52,740
getting in graphical models of luggage is very similar in spirit

1274
01:15:53,740 --> 01:15:59,020
your to construct well when method was strategies construct energy so that the volume of low energy statistics

1275
01:15:59,090 --> 01:16:02,820
so that if you give low energy some points the other points automatically have

1276
01:16:02,970 --> 01:16:03,840
a higher energy

1277
01:16:04,000 --> 01:16:05,870
ca one of those k-means is one of those

1278
01:16:06,370 --> 01:16:11,460
the second strategy is to push down the energy of the samples and push up on the energy of earth

1279
01:16:11,490 --> 01:16:14,790
else this is a strategy used by contrast divergence which is used to train

1280
01:16:15,130 --> 01:16:16,330
restricted most machines

1281
01:16:16,460 --> 01:16:20,320
also strategy used by maximum likelihood when you have a partition function

1282
01:16:20,510 --> 01:16:22,720
as contrasted term that pushes up on the energy of

1283
01:16:23,080 --> 01:16:24,910
all this stuff is very expensive to

1284
01:16:25,160 --> 01:16:27,520
if your partition function is not tractable

1285
01:16:27,990 --> 01:16:30,220
where does not tractable gradient

1286
01:16:30,920 --> 01:16:32,640
to use kernel methods stuff

1287
01:16:33,180 --> 01:16:37,480
and the third one is to use a regularizer to limit the volume of stuff that can take low

1288
01:16:37,500 --> 01:16:41,080
energy so basically building a neural in such a way that it has a regularizer

1289
01:16:41,350 --> 01:16:44,030
by minimizing a regularizer is sort of shrink wraps

1290
01:16:44,340 --> 01:16:50,460
the of the stuff of low energy into sort of a small value fuel and that's basically what's task

1291
01:16:50,490 --> 01:16:51,080
and does

1292
01:16:52,410 --> 01:16:53,450
sparsity term

1293
01:16:53,920 --> 01:16:57,030
limits the volume of stuff that is allowed to take low energy

1294
01:16:57,300 --> 01:16:58,520
that's essentially does

1295
01:16:59,570 --> 01:17:04,800
ok so this task learning method is or what we call the decoder only method goes from

1296
01:17:05,050 --> 01:17:08,160
has a simple function to compute goes from the code to the input

1297
01:17:08,410 --> 01:17:11,850
which means you have to run musician algorithm to figure out the optimal code

1298
01:17:11,870 --> 01:17:12,630
for a given input

1299
01:17:13,070 --> 01:17:14,820
so we're going to do is we're going to

1300
01:17:15,690 --> 01:17:20,280
to this a sort feed-forward function that goes from the input to the code

1301
01:17:20,540 --> 01:17:22,670
and we'll will tend to predict

1302
01:17:22,930 --> 01:17:24,460
what the optimal code is

1303
01:17:24,600 --> 01:17:26,660
this function would necessarily have to be non-linear

1304
01:17:29,060 --> 01:17:32,910
decay firstly we call this qz which means predictive sparse decomposition

1305
01:17:33,650 --> 01:17:39,340
which gives you know various architectures for this it's a simple neural net some kind

1306
01:17:39,860 --> 01:17:45,960
and so here is that the algorithm running this is the algorithm running on a set of natural

1307
01:17:45,980 --> 01:17:48,300
image patches what i'm representing here are

1308
01:17:48,600 --> 01:17:49,800
each square here

1309
01:17:50,220 --> 01:17:53,890
is a problem of the w matrix represented as

1310
01:17:54,090 --> 01:17:56,960
a as an image of the same size as the input

1311
01:17:57,270 --> 01:17:58,550
so you get

1312
01:17:58,840 --> 01:18:01,960
in the end of the training you get sort of going to give order to detectors

1313
01:18:01,980 --> 01:18:02,930
which is which you're x

1314
01:18:04,040 --> 01:18:04,920
more as expected

1315
01:18:08,060 --> 01:18:10,140
now to due to design this encoder

1316
01:18:10,710 --> 01:18:11,780
this g function

1317
01:18:12,040 --> 01:18:15,850
from oppose my progress is really brilliant idea

1318
01:18:15,980 --> 01:18:21,280
essentially iminating algorithm that we know will produce the optimal z cold fista or stuff

1319
01:18:21,540 --> 01:18:23,500
iterative shrinkage thresholding algorithm

1320
01:18:25,590 --> 01:18:32,210
and the algorithm basically is this kind of iteration where you take the input multiplied by input matrix and then you

1321
01:18:32,240 --> 01:18:34,240
as it were shrinkage shrinkage function

1322
01:18:34,430 --> 01:18:35,080
each component

1323
01:18:35,330 --> 01:18:38,950
multiplied by square matrix which is you could think of as a lateral inhibition matrix

1324
01:18:39,240 --> 01:18:42,030
added to the previous result you had iterate this group

1325
01:18:42,410 --> 01:18:44,420
so it's kind of this recurrence here

1326
01:18:44,840 --> 01:18:47,960
f what we're going to do is going to learn

1327
01:18:48,360 --> 01:18:50,380
instead of using

1328
01:18:51,590 --> 01:18:55,200
you know this definition for s and that definition for that you eat which is

1329
01:18:55,220 --> 01:18:56,770
to learn as and you eat

1330
01:18:57,340 --> 01:19:00,150
so you can think of this as a recurrent neural net some kind

1331
01:19:00,400 --> 01:19:03,090
which matrices w in s going to train this

1332
01:19:03,380 --> 01:19:07,910
recurrent neural net which has bounded o complexity to do the best approximation can

1333
01:19:08,110 --> 01:19:10,430
of the optimal task this works really well

1334
01:19:10,650 --> 01:19:12,370
interest of time and to skip a

1335
01:19:13,900 --> 01:19:17,750
just to say that we've apply this to kind of various things including one where

1336
01:19:17,770 --> 01:19:18,790
we have criteria

1337
01:19:19,170 --> 01:19:21,920
that combine this reconstruction sparsity as well as

1338
01:19:22,280 --> 01:19:24,060
the its prediction

1339
01:19:24,380 --> 01:19:27,020
and a recent paper by just roles

1340
01:19:27,150 --> 01:19:33,690
myself a clear yeah conference f and this produces very interesting results we have seen before in

1341
01:19:33,690 --> 01:19:38,090
sort of a s an alternative view of this ok

1342
01:19:38,350 --> 01:19:41,890
which is derived in the size but i'm really not going to go through

1343
01:19:41,890 --> 01:19:45,980
the derivation is the following we can think of this quite

1344
01:19:46,270 --> 01:19:49,910
and as saying we have some objective function here which is

1345
01:19:49,910 --> 01:19:52,560
the squared error ok we're trying to minimize that

1346
01:19:53,190 --> 01:19:56,630
and at the same time we want to keep the magnitude of the weights

1347
01:19:56,630 --> 01:20:00,960
bounded ok and this parameter data sort of the inverse of lambda

1348
01:20:01,300 --> 01:20:06,050
ok so what does this mean okay how do we visualize this

1349
01:20:06,700 --> 01:20:09,920
we basically we do the following in general will like the

1350
01:20:09,920 --> 01:20:12,890
weights to be close to zero okay so you have this red circle

1351
01:20:12,890 --> 01:20:15,820
over here around the origin that saying i on the weights to be

1352
01:20:15,820 --> 01:20:21,930
small okay and we have a sort of point over here that's the best

1353
01:20:21,970 --> 01:20:28,210
fit based on the data ok now what is the best solution according

1354
01:20:28,220 --> 01:20:32,820
to this quite and well it's where these circles intersect ok

1355
01:20:33,050 --> 01:20:35,900
and that's going to be on the sort of line that unites

1356
01:20:36,420 --> 01:20:40,320
these two centers or you know if your many dimensions can be hyper

1357
01:20:40,950 --> 01:20:43,730
so now what happens with the regular izations phi

1358
01:20:43,970 --> 01:20:48,610
ify make lambda very small ok data which is one over

1359
01:20:48,610 --> 01:20:51,720
does very big and the circle that's around the origin is going

1360
01:20:51,720 --> 01:20:54,930
to blow up ok and all solutions are acceptable essentially

1361
01:20:55,270 --> 01:20:59,400
ok otherwise when we shrink this circle

1362
01:20:59,550 --> 01:21:03,200
ok we really want the weights to be close to zero that's the emphasis

1363
01:21:03,430 --> 01:21:07,030
and so the weights are going to migrate away from the point that's

1364
01:21:07,030 --> 01:21:10,470
best for the data towards the point that's best for us this prior

1365
01:21:10,880 --> 01:21:12,510
ok that's to regulars h

1366
01:21:16,620 --> 01:21:21,130
so this is actually quite nice ok if you find a good value for the

1367
01:21:21,130 --> 01:21:24,150
regular zation parameter this helps you avoid overfitting

1368
01:21:25,540 --> 01:21:28,770
if there are irrelevant features in the input

1369
01:21:29,240 --> 01:21:32,680
then you're going to get a small weights for those features

1370
01:21:32,690 --> 01:21:37,920
typically ok without having to do some kind of complicated pre-selection

1371
01:21:37,920 --> 01:21:40,640
procedure where you just keep some features and so on

1372
01:21:40,640 --> 01:21:42,740
but their weights not then be quite zero

1373
01:21:42,740 --> 01:21:45,520
k the kind of stale little bit away from zero because there's nothing

1374
01:21:45,520 --> 01:21:48,730
really encouraging them to be all zero okay we're working with

1375
01:21:48,730 --> 01:21:52,170
the magnitude of the weight vector rather than particular weights

1376
01:21:53,450 --> 01:21:57,310
so there's a weight to to actually get these weights to be

1377
01:21:57,320 --> 01:22:00,450
equal to zero okay that's called one regular izations

1378
01:22:00,830 --> 01:22:04,050
ok and actually show you the picture first

1379
01:22:04,300 --> 01:22:08,010
ok the picture for one regularization is that instead of

1380
01:22:08,020 --> 01:22:11,200
using a circle around the origin we use a little diamond

1381
01:22:11,820 --> 01:22:17,850
ok and so what we require is for the weights to be inside of this

1382
01:22:17,860 --> 01:22:23,720
time and so now you have the circle here it's going to touch the

1383
01:22:23,730 --> 01:22:29,520
diamond somewhere along its edges ok but one can actually formally

1384
01:22:29,520 --> 01:22:32,220
show that you're more likely to touch at the corner

1385
01:22:32,470 --> 01:22:34,700
went to touch in the middle of on h ok

1386
01:22:35,080 --> 01:22:37,620
now that's actually kind of nice because here

1387
01:22:37,620 --> 01:22:40,400
if you're touching at the corner that means that one of your

1388
01:22:40,400 --> 01:22:43,720
weights is actually zero and only the other weight matters so

1389
01:22:43,730 --> 01:22:47,800
if you have irrelevant features then those irrelevant features essentially

1390
01:22:47,800 --> 01:22:49,760
will have zero weights will be thrown

1391
01:22:50,310 --> 01:22:52,760
so large class of methods called lasso

1392
01:22:53,160 --> 01:22:56,390
very popular statistics community works very well with linear

1393
01:22:56,400 --> 01:23:00,100
approximators doesn't works well with non linear approximators

1394
01:23:00,370 --> 01:23:03,950
because this kind of constraint is actually a little bit ugly

1395
01:23:04,230 --> 01:23:07,860
okay and optimization procedure is

1396
01:23:08,260 --> 01:23:12,460
significantly more elaborate ok the type of constraint that we have

1397
01:23:12,470 --> 01:23:16,340
here is that the sum of the absolute values of the weights has

1398
01:23:16,350 --> 01:23:20,120
to be bounded by parameter data and data again controls the size

1399
01:23:20,210 --> 01:23:25,730
of the time so now why is this more complicated well you can easily

1400
01:23:25,740 --> 01:23:29,440
see that if i take this absolute value and expanded and all

1401
01:23:29,450 --> 01:23:32,930
possible ways we get a lot of these constraints you still have

1402
01:23:32,930 --> 01:23:36,240
a sort of in principle nice optimization procedure but with lots

1403
01:23:36,240 --> 01:23:39,970
and lots of constraints and so where is this

1404
01:23:40,310 --> 01:23:43,930
useful where that depicts there are some things that are that going

1405
01:23:43,940 --> 01:23:48,830
between where people use sort of l one style regular sation some

1406
01:23:48,840 --> 01:23:51,750
parts of the space and i'm smoothly going to one l to

1407
01:23:51,950 --> 01:23:56,530
these are called huber arise losses some in some cases those work

1408
01:23:57,350 --> 01:24:01,860
better than either one l to the advantage of delta is that it eliminates

1409
01:24:01,870 --> 01:24:05,740
completely features that are irrelevant and so in some applications

1410
01:24:05,740 --> 01:24:09,440
that's actually useful for example if you work in medical applications

1411
01:24:09,570 --> 01:24:13,110
doctors really want to know what error the three different symptoms

1412
01:24:13,110 --> 01:24:15,890
that are determining the diagnosis sort of five different

1413
01:24:15,890 --> 01:24:18,410
is that determining the diagnosis and don't want everything

1414
01:24:18,410 --> 01:24:20,030
OK so we had

1415
01:24:22,510 --> 01:24:24,150
i see the person

1416
01:24:24,150 --> 01:24:25,970
viewpoint lighting

1417
01:24:26,000 --> 01:24:27,570
we have lots of axis here

1418
01:24:27,580 --> 01:24:30,890
the things my very on so you might say OK

1419
01:24:31,130 --> 01:24:35,680
the PCA is is too simple what i want is the multi access decomposition

1420
01:24:35,700 --> 01:24:39,980
but does a similar thing is linear decomposition but it will will divided that it

1421
01:24:39,990 --> 01:24:42,380
along several different axes and build character

1422
01:24:42,380 --> 01:24:43,770
characteristic models

1423
01:24:43,800 --> 01:24:44,780
in that way

1424
01:24:44,810 --> 01:24:48,030
so yes such techniques exist

1425
01:24:48,050 --> 01:24:51,330
equal to interfaces tensor analysis

1426
01:24:51,350 --> 01:24:57,900
so you can build things that have a particular characteristic directions for changing identity people

1427
01:24:57,900 --> 01:25:00,180
changing illumination these kinds of things

1428
01:25:00,200 --> 01:25:03,900
but still linear analysis still fairly simple minded

1429
01:25:04,220 --> 01:25:07,660
the way you do this is kind of iterative algorithm does this feeds along each

1430
01:25:07,660 --> 01:25:09,930
axis alternating right

1431
01:25:09,950 --> 01:25:13,110
so it's not very complicated to do

1432
01:25:13,110 --> 01:25:17,660
OK and then a whole slew of non linear dimensionality reduction techniques which tend to

1433
01:25:17,660 --> 01:25:19,620
get applied to vision problems but

1434
01:25:19,640 --> 01:25:21,440
i think in terms of faces

1435
01:25:21,460 --> 01:25:24,760
the more toy techniques and then the

1436
01:25:24,770 --> 01:25:26,260
really useful things

1437
01:25:26,270 --> 01:25:29,240
but certainly the first i submit papers the first

1438
01:25:29,260 --> 01:25:31,150
local linear embedding paper

1439
01:25:31,160 --> 01:25:33,100
her face that sits in the

1440
01:25:34,120 --> 01:25:37,580
people people have been using this and it's a good example because you can see

1441
01:25:37,580 --> 01:25:38,680
what's going on

1442
01:25:38,710 --> 01:25:40,270
when you're doing embedding

1443
01:25:40,370 --> 01:25:44,530
these techniques can be thought about of course is a whole slew of other

1444
01:25:46,300 --> 01:25:49,020
variance of these techniques these days

1445
01:25:53,410 --> 01:25:55,530
now on to some modelling

1446
01:25:56,340 --> 01:26:00,160
different ways the systems in different ways might model motion

1447
01:26:00,180 --> 01:26:03,000
so before we saw an example best humans data

1448
01:26:03,080 --> 01:26:05,230
OK you can do the same thing with motion

1449
01:26:05,250 --> 01:26:09,150
so if you take a training set of people in various composers

1450
01:26:10,400 --> 01:26:11,840
you have your detector

1451
01:26:11,860 --> 01:26:14,630
and if you can detect things like this

1452
01:26:14,650 --> 01:26:18,510
so the difference in the rates of the person being you can t into your

1453
01:26:18,510 --> 01:26:22,650
training set of motions and therefore is to make what the movement of the pose

1454
01:26:22,650 --> 01:26:24,030
of the person was

1455
01:26:24,300 --> 01:26:26,660
so this was an early paper that does

1456
01:26:26,880 --> 01:26:31,030
works actually with HMM based model

1457
01:26:31,050 --> 01:26:37,380
over the whole sequence some optimizations to try to pull out this this whole sequence

1458
01:26:37,380 --> 01:26:41,490
the motion suggesting that the to be start of algorithm to do that

1459
01:26:41,550 --> 01:26:45,820
and it was that reasonable estimates of motion the person

1460
01:26:47,930 --> 01:26:52,300
you can also then

1461
01:26:52,320 --> 01:26:54,780
and more with more linear methods priors

1462
01:26:54,780 --> 01:26:56,470
four motion

1463
01:26:56,490 --> 01:26:59,550
so i guess and regressive processes for example

1464
01:26:59,840 --> 01:27:04,050
here for dancing motion so this was all done essentially the learning was done in

1465
01:27:04,050 --> 01:27:05,340
three d

1466
01:27:05,680 --> 01:27:08,950
but again i got example kinds of things using

1467
01:27:08,990 --> 01:27:11,400
just was see a similar example

1468
01:27:11,400 --> 01:27:13,880
this is pure example of purely

1469
01:27:14,010 --> 01:27:17,760
learning motion models for the second c the kinds of tracking

1470
01:27:18,150 --> 01:27:25,800
the commonwealth

1471
01:27:25,880 --> 01:27:34,110
the school

1472
01:27:34,490 --> 01:27:40,990
as well to move

1473
01:27:41,010 --> 01:27:42,510
so this is two d

1474
01:27:42,530 --> 01:27:44,450
so it's time model

1475
01:27:44,470 --> 01:27:46,430
tracking down sequence

1476
01:27:46,720 --> 01:27:52,900
existing pretty well it's fairly stable tracking if you want to annotate these particular example

1477
01:27:53,130 --> 01:27:55,990
of three d poses and you can get out and this to the three d

1478
01:27:55,990 --> 01:27:59,260
pose which is rescued crew it is OK

1479
01:28:09,800 --> 01:28:14,840
you can do things like sports analysis with us analyse motions of people much the

1480
01:28:14,840 --> 01:28:16,200
same thing

1481
01:28:16,590 --> 01:28:21,470
we've we've just got been using exemplars here we've got an articulated model of the

1482
01:28:21,470 --> 01:28:25,490
person has been put into the exemplars by hand and it can reach say that

1483
01:28:25,490 --> 01:28:32,970
one of the things that season the image

1484
01:28:35,400 --> 01:28:37,700
even if you want to do sports analysis

1485
01:28:37,720 --> 01:28:42,470
of soccer players who are only a few pixels high twenty pixels high something

1486
01:28:42,490 --> 01:28:45,260
you can't really building articulated model

1487
01:28:45,280 --> 01:28:47,150
the person

1488
01:28:47,990 --> 01:28:51,130
no television is this of players just to

1489
01:28:51,820 --> 01:28:55,900
i don't really know what it is we can still build an example model of

1490
01:28:55,950 --> 01:28:59,820
in the signpost is an example of model which will only be approximate because you

1491
01:28:59,820 --> 01:29:04,550
can resolve the and some things you get approximate body pose and you can use

1492
01:29:04,550 --> 01:29:06,530
that for example the recent size

1493
01:29:06,570 --> 01:29:11,450
difference of the sequences in the original paper the guy used to replace the sock

1494
01:29:11,450 --> 01:29:14,050
also players on the team with himself

1495
01:29:14,110 --> 01:29:15,990
so it's quite bizarre

1496
01:29:16,010 --> 01:29:18,530
which we don't have the video so that

1497
01:29:18,650 --> 01:29:23,570
OK so another sample based methods that you can use

1498
01:29:23,610 --> 01:29:25,900
is that you can just record dynamics

1499
01:29:25,920 --> 01:29:30,150
so this a got some kind of three d motion capture system human movement the

1500
01:29:30,150 --> 01:29:33,530
kind of things they movies to capture three d motion

1501
01:29:35,440 --> 01:29:37,720
then you can record of the database that

1502
01:29:38,070 --> 01:29:42,130
and you've got some kind of reconstruction of some kind two d

1503
01:29:42,150 --> 01:29:45,320
that they can use that to t into the database

1504
01:29:45,430 --> 01:29:50,610
and if you find over short sequences time you've got a similar motion

1505
01:29:50,650 --> 01:29:54,280
you can easily predict the motions going to continue like that

1506
01:29:54,300 --> 01:29:58,340
so this is just a purely sampling based methods that looks into the database something

1507
01:29:59,160 --> 01:30:00,650
tries to reply that

1508
01:30:00,700 --> 01:30:04,200
but nevertheless it gives you quite good

1509
01:30:04,240 --> 01:30:10,380
it is also true for the video for that here

1510
01:30:12,990 --> 01:30:21,800
so the mouse stopped working his

1511
01:30:21,800 --> 01:30:25,570
so it's got

1512
01:30:25,570 --> 01:30:27,300
the type of motion is

1513
01:30:27,300 --> 01:30:28,740
coded by the colour

1514
01:30:28,760 --> 01:30:34,070
you can see that encoding fairly small fairly fairly good walking motion

1515
01:30:38,800 --> 01:30:43,990
so all of those models for purely exempt purely examples being used to

1516
01:30:44,010 --> 01:30:46,650
two to kind of reply

1517
01:30:46,660 --> 01:30:48,260
i think you see in the image

1518
01:30:48,260 --> 01:30:51,550
now we get onto most structural models

1519
01:30:51,550 --> 01:30:56,920
someone has gone and input in some kind of structure by hand and then of

1520
01:30:56,930 --> 01:31:00,450
often you have to learn some parts of the model

1521
01:31:00,470 --> 01:31:04,930
so here is what's called active appearance models or in particular

1522
01:31:04,970 --> 01:31:10,300
vision of an active shape models to typically will be control model the person with

1523
01:31:10,300 --> 01:31:13,010
an active appearance models also attached

1524
01:31:13,030 --> 01:31:16,740
model for the parents of the person those contours

1525
01:31:16,760 --> 01:31:19,150
and you're able to from that

1526
01:31:19,180 --> 01:31:21,220
to fit this parametric model

1527
01:31:21,220 --> 01:31:23,800
with much parents and parametric geometry

1528
01:31:23,820 --> 01:31:27,570
and then when she fitted the model you can do things like

1529
01:31:28,820 --> 01:31:34,380
six will be a the person one person would look like under certain circumstances the

1530
01:31:34,380 --> 01:31:39,040
if such object is in the image but we want to look for it and

1531
01:31:39,290 --> 01:31:46,800
she she component of this task is the localisation component so here we're looking for

1532
01:31:46,840 --> 01:31:51,580
or detecting cars and localizing where they are

1533
01:31:54,210 --> 01:31:57,180
another task would be identification

1534
01:31:57,190 --> 01:32:04,430
this will be related especially to the individualisation of objects within as a member of

1535
01:32:04,960 --> 01:32:10,550
of within the category category for example here we know there's

1536
01:32:10,560 --> 01:32:16,260
even if we identify this is a human face was still want individualized this particular

1537
01:32:16,260 --> 01:32:19,640
object we call it identification

1538
01:32:19,690 --> 01:32:27,350
going beyond identification we have the task of categorisation where this is more in a

1539
01:32:27,350 --> 01:32:34,980
what we have seen play has told us that different members of similar after different

1540
01:32:34,980 --> 01:32:40,740
members of the group participate together to form a class or category and we want

1541
01:32:40,750 --> 01:32:46,080
to categorize for example in the real world image all these different classes that are

1542
01:32:46,080 --> 01:32:49,720
present in this in this image

1543
01:32:49,740 --> 01:32:50,900
and of course

1544
01:32:53,730 --> 01:33:02,060
categories of objects or individual categories of just come together to describe visual scenes such

1545
01:33:02,060 --> 01:33:09,630
as sing sing club scene categories or even context or we can imagine moving beyond

1546
01:33:09,710 --> 01:33:14,400
our events and activities and so on especially towards

1547
01:33:14,410 --> 01:33:19,940
and towards the future of visual computer vision high-level vision research we would like to

1548
01:33:20,260 --> 01:33:28,470
write stories of images and videos so objects are very important building blocks for these

1549
01:33:31,880 --> 01:33:37,820
recognising objects are not easy and here is one of the tabbouleh some of the

1550
01:33:37,820 --> 01:33:44,490
challenges we all face and apparently human visual system have dealt with very well but

1551
01:33:44,490 --> 01:33:50,480
as designing robots and come machine vision algorithms we face a lot of of the

1552
01:33:50,480 --> 01:33:55,440
following problems one is viewpoint in fact variation

1553
01:33:57,790 --> 01:33:59,420
our physical three d

1554
01:34:01,840 --> 01:34:08,880
all other things and when you view it from different angles different perspective they render

1555
01:34:08,880 --> 01:34:16,100
very different images on the retina or on the computer on the camera image

1556
01:34:16,150 --> 01:34:18,740
so that's one big challenge

1557
01:34:18,750 --> 01:34:23,290
can you tell this is the same person

1558
01:34:23,370 --> 01:34:28,970
maybe some of you can buy trying to find some specific feature on the face

1559
01:34:29,340 --> 01:34:31,510
but just at a glance

1560
01:34:31,560 --> 01:34:33,790
it's not easy to tell at all

1561
01:34:34,010 --> 01:34:40,930
and this is because there's a lot of illumination changes in the world that rendered

1562
01:34:40,930 --> 01:34:45,800
the part our images and we have to deal with that in object recognition

1563
01:34:45,870 --> 01:34:54,210
in identification in categorisation we would like to tell you this is the same person

1564
01:34:55,610 --> 01:34:57,550
is another problem

1565
01:34:57,560 --> 01:35:00,010
objects don't exist

1566
01:35:00,020 --> 01:35:08,050
ten not exist in the world by itself completely clean and free environment of in

1567
01:35:08,050 --> 01:35:13,430
this world we have a lot of clutter and the objects occluded by other parts

1568
01:35:13,430 --> 01:35:19,150
of this this world and they can even self occluded when you have especially quite

1569
01:35:19,150 --> 01:35:25,840
articulated objects and so on so occlusion is the problem we can not only in

1570
01:35:25,840 --> 01:35:31,410
this room are algorithms can can recognise an object when the when the thing is

1571
01:35:31,410 --> 01:35:37,830
fully insight we would like to take care of recognition in under this under this

1572
01:35:39,500 --> 01:35:42,230
scale is another problem

1573
01:35:42,250 --> 01:35:47,940
the same class of object my have very different sizes

1574
01:35:48,440 --> 01:35:54,690
they might come into different physical sizes or they might just be geometrically rendered differently

1575
01:35:54,690 --> 01:36:00,660
when you're a far or near from these these objects but you would still like

1576
01:36:00,670 --> 01:36:04,060
to understand this is from the same class

1577
01:36:04,100 --> 01:36:13,030
deformation is another problem like i just mentioned especially articulated objects we have a lot

1578
01:36:13,030 --> 01:36:19,360
of it articulated objects in this world including our own human body animals and as

1579
01:36:19,360 --> 01:36:25,240
stringy objects and we would like to recognise that we would like to somehow express

1580
01:36:25,250 --> 01:36:29,610
these articulation and the deformation

1581
01:36:29,620 --> 01:36:35,220
background clutter looking for face may not be so simple here especially for any face

1582
01:36:35,220 --> 01:36:36,840
recognition algorithms

1583
01:36:36,920 --> 01:36:43,010
and it's because this world is very cluttered and it makes it interesting and fun

1584
01:36:43,040 --> 01:36:49,980
visually and beyond so we would like to recognise and categorize objects in in this

1585
01:36:53,710 --> 01:37:00,580
you know all this variations it was quite quite challenging task for the computer vision

1586
01:37:00,580 --> 01:37:07,020
scientists to deal with the recognition of objects and in terms of the history of

1587
01:37:07,020 --> 01:37:14,030
our field people started looking at this problem of cell is a single object recognition

1588
01:37:16,930 --> 01:37:26,440
especially those objects that are relatively plain relatively textured with a lot of with a

1589
01:37:26,440 --> 01:37:34,090
lot of the characteristic patch appearances and and some of the objects will have a

1590
01:37:34,090 --> 01:37:38,090
rather planner frossard and some of the

1591
01:37:38,180 --> 01:37:45,610
david lowe in nineteen ninety nine propose one of the first really quite successful

1592
01:37:45,620 --> 01:37:51,640
algorithm for single object recognition and from then on there was a whole series of

1593
01:37:51,640 --> 01:37:58,050
papers that have pushed this problem as far as as they can and it's been

1594
01:37:58,050 --> 01:38:03,640
very successful that some of them are even put into robots now

1595
01:38:03,690 --> 01:38:06,280
to recognise single objects

1596
01:38:08,140 --> 01:38:11,680
in terms of object classification we face

1597
01:38:11,690 --> 01:38:14,820
all the problems that single object

1598
01:38:15,220 --> 01:38:22,810
recognition interfaces from the viewpoint variables to illumination to scale and everything and on top

1599
01:38:22,810 --> 01:38:30,790
of that we're dealing with intra class variation and this is a typical

1600
01:38:30,850 --> 01:38:35,460
problem you will hear in computer vision how to recognise chairs

1601
01:38:35,470 --> 01:38:41,040
look at this i'm not so sure if humans can tell the or chairs and

1602
01:38:41,500 --> 01:38:43,010
this is the kind of

1603
01:38:43,020 --> 01:38:47,600
of course this is pushing the problem a little bit to the extreme but it

1604
01:38:47,600 --> 01:38:53,290
really demonstrate how tough we have to deal with the heart of this problem is

1605
01:38:53,290 --> 01:38:57,710
how do we deal with this intraclass variability so

1606
01:38:58,350 --> 01:39:07,040
are they object class categorisation and classification focused on some of the most

1607
01:39:07,090 --> 01:39:11,010
useful probably and

1608
01:39:11,010 --> 01:39:15,090
to have one note that misbehaves and violates running intersection

1609
01:39:15,120 --> 01:39:18,710
so this sort of tells us not aggressive junction trees this graph does not have

1610
01:39:18,710 --> 01:39:19,760
a junction tree

1611
01:39:19,780 --> 01:39:24,870
so we need to understand what types of graphs do have junction trees

1612
01:39:26,460 --> 01:39:30,100
because those are be the kinds of graphs that we can deal with

1613
01:39:30,140 --> 01:39:33,650
and in the end we can see that we can convert any graph into a

1614
01:39:33,650 --> 01:39:37,530
form in which it does have a junction tree by by a certain process of

1615
01:39:37,530 --> 01:39:41,090
adding edges and this is is actually bringing us useful full circle back to the

1616
01:39:41,090 --> 01:39:43,380
elimination story

1617
01:39:43,390 --> 01:39:45,700
OK so

1618
01:39:45,730 --> 01:39:50,750
what turns out to be key here is is the notion of triangulation

1619
01:39:50,760 --> 01:39:53,310
and again this is is a very intuitive notion

1620
01:39:53,350 --> 01:39:55,880
all you mean is that

1621
01:39:55,930 --> 01:40:00,030
the graph basically consists a bunch of triangles pasted together

1622
01:40:00,040 --> 01:40:03,810
so this graph is not triangulated whereas this one is

1623
01:40:03,870 --> 01:40:08,250
and a bit more precisely what it means is whenever you have a cycle of

1624
01:40:08,250 --> 01:40:11,660
length for a cycle of length four

1625
01:40:11,680 --> 01:40:17,400
or possibly longer what you need is you need to have watson's according need to

1626
01:40:17,400 --> 01:40:22,170
cut it into a triangle and keep doing that until you just get triangles everywhere

1627
01:40:22,210 --> 01:40:26,070
so here if i start cutting this and then cut that cut that i'm adding

1628
01:40:26,070 --> 01:40:27,320
edges here

1629
01:40:27,320 --> 01:40:31,690
number that's key when i add edges i am changing the complexity of the graph

1630
01:40:31,720 --> 01:40:36,810
but if i do that actually get triangulated graph where all of these four cycles

1631
01:40:36,810 --> 01:40:40,190
here have been cut by a court in half

1632
01:40:40,240 --> 01:40:44,510
so what's key here this is deep graph theoretic result we won't go into details

1633
01:40:46,780 --> 01:40:50,000
you can show that a graph has a junction tree if and only if

1634
01:40:50,060 --> 01:40:52,340
it is triangulated

1635
01:40:52,430 --> 01:40:57,780
so that sort of gives this intuition why we run into problems here

1636
01:40:57,820 --> 01:41:01,140
we ran into problems because that's the simplest example of a graph that is not

1637
01:41:03,180 --> 01:41:06,380
right there is this cycle that you can walk around the should be an edge

1638
01:41:06,420 --> 01:41:10,600
one of these edges should be there to make the triangles it's not there and

1639
01:41:10,600 --> 01:41:15,010
then lo and behold we ran into problems we failed running intersection when we tried

1640
01:41:15,010 --> 01:41:17,930
to make a junction tree

1641
01:41:17,940 --> 01:41:25,220
right so this is a very nice results because it has an important practical consequence

1642
01:41:25,240 --> 01:41:29,840
what it says well i can always make a junction tree from any graph this

1643
01:41:29,840 --> 01:41:31,900
graph does not have a junction tree

1644
01:41:31,910 --> 01:41:37,540
because it's not triangulated it's got these four cycles but this gives us a constructive

1645
01:41:37,540 --> 01:41:43,190
procedure in this this is a very practical method to actually make a junction tree

1646
01:41:43,200 --> 01:41:46,690
and this is what's known as the junction tree algorithm

1647
01:41:46,710 --> 01:41:53,190
so any questions about triangulation junction trees

1648
01:41:53,350 --> 01:41:56,350
sort of what we're after here before we jump into

1649
01:41:56,420 --> 01:41:59,670
the junction tree algorithm

1650
01:42:00,900 --> 01:42:02,510
you should doing more

1651
01:42:08,250 --> 01:42:11,480
that's a very good question no there is not

1652
01:42:13,400 --> 01:42:14,820
as i mentioned earlier

1653
01:42:14,820 --> 01:42:19,870
this is the problem of finding the best elimination ordering is NP hard

1654
01:42:19,880 --> 01:42:25,080
and this problem of finding an optimal triangulation is actually equivalent to that problem

1655
01:42:25,120 --> 01:42:28,800
this sort of these edges are being added are very much like the edges that

1656
01:42:28,800 --> 01:42:31,390
are being added during an elimination algorithm

1657
01:42:31,400 --> 01:42:32,900
that that's the connection

1658
01:42:32,910 --> 01:42:36,090
so in general finding the best triangulation

1659
01:42:36,370 --> 01:42:41,070
o come back and discuss in more detail later that's a hard problem but as

1660
01:42:41,070 --> 01:42:46,880
before they're very good heuristics that are used in practice for finding reasonable approximations to

1661
01:42:51,070 --> 01:42:57,630
so this is the junction tree algorithm

1662
01:42:57,640 --> 01:43:00,960
it's a very nice of them due to low attendance beagle halter

1663
01:43:00,980 --> 01:43:04,310
and it's quite simple

1664
01:43:04,320 --> 01:43:06,120
this is the high level it's quite simple

1665
01:43:06,140 --> 01:43:08,670
so there's four steps

1666
01:43:08,690 --> 01:43:12,360
you can be given as input some undirected graph

1667
01:43:12,360 --> 01:43:15,310
the graph may or may not be triangulated

1668
01:43:15,320 --> 01:43:19,440
if it's triangulated you're happy because you know that you can then go off and

1669
01:43:19,440 --> 01:43:22,300
make a junction tree at least in principle

1670
01:43:22,350 --> 01:43:25,870
if not then you have to go around and you have to add edges to

1671
01:43:25,870 --> 01:43:28,710
get rid of things like four cycles you have to make it look like a

1672
01:43:28,710 --> 01:43:30,630
triangle everywhere

1673
01:43:30,640 --> 01:43:35,850
right so going back to that question here on the same at the edges in

1674
01:43:35,850 --> 01:43:36,960
some way

1675
01:43:37,000 --> 01:43:40,030
it's going to be notion of the optimal way in which to do that but

1676
01:43:40,030 --> 01:43:43,690
let's not worry about that for the moment that's actually hard think about we add

1677
01:43:43,690 --> 01:43:47,560
edges in some way to make the graph triangulated

1678
01:43:47,560 --> 01:43:50,890
so let's

1679
01:43:50,900 --> 01:43:55,440
look at this example this is a pretty simple example of a sort of segment

1680
01:43:55,440 --> 01:43:57,380
of the lattice or grid model

1681
01:43:57,400 --> 01:44:03,030
and we can see this graph is untriangulated because we've got all these four cycles

1682
01:44:03,030 --> 01:44:06,090
so the first step would be triangulated

1683
01:44:06,140 --> 01:44:11,820
so i could for instance at edges here here here and here

1684
01:44:11,830 --> 01:44:18,430
so adding these four edges

1685
01:44:18,450 --> 01:44:23,250
for some reason i've also added another edge here between two and eight

1686
01:44:23,330 --> 01:44:25,890
what did i do that was necessary

1687
01:44:32,480 --> 01:44:35,300
so think about this graph if

1688
01:44:35,400 --> 01:44:38,880
suppose i deleted this edge here this arc

1689
01:44:38,920 --> 01:44:40,940
with that graph be triangulated

1690
01:44:40,980 --> 01:44:52,530
why why not

1691
01:44:52,610 --> 01:45:00,830
whereas the is not going to be triangulated

1692
01:45:04,860 --> 01:45:08,860
so without that edge you have to be careful there the cycle by adding edges

1693
01:45:08,860 --> 01:45:10,950
i made a new cycle two

1694
01:45:11,010 --> 01:45:13,440
august two four eight six two

1695
01:45:13,530 --> 01:45:15,680
that's the four cycle

1696
01:45:15,710 --> 01:45:19,370
it doesn't have a direct record if i don't have this arc i could have

1697
01:45:19,370 --> 01:45:21,670
to go to a node five that doesn't count

1698
01:45:21,780 --> 01:45:25,280
i need something directly that cuts it in half to make a triangle

1699
01:45:25,290 --> 01:45:27,610
so you have to add that extra art

1700
01:45:27,620 --> 01:45:31,140
so triangulation can be a bit tricky because when you add edges you can make

1701
01:45:33,670 --> 01:45:36,010
it's size in this example it's

1702
01:45:36,030 --> 01:45:39,700
toy but it's already getting a bit tricky

1703
01:45:39,730 --> 01:45:43,160
it's also worth thinking about this is this is not the best triangulation that i

1704
01:45:43,160 --> 01:45:48,100
did this is just a natural one but there is actually a better triangulation whereby

1705
01:45:48,100 --> 01:45:50,040
better i mean

1706
01:45:50,080 --> 01:45:52,490
adding fewer edges making the cliques

1707
01:45:52,510 --> 01:45:54,580
not grows quickly

1708
01:45:55,530 --> 01:45:56,860
OK so

1709
01:45:56,870 --> 01:46:01,170
we've done that step where we're triangulated and now our previous results is that we

1710
01:46:01,170 --> 01:46:03,470
all males together

1711
01:46:03,520 --> 01:46:06,000
confer conspiracy make you feel

1712
01:46:06,020 --> 01:46:08,200
everywhere inside zero

1713
01:46:08,250 --> 01:46:13,350
to nontrivial results

1714
01:46:13,440 --> 01:46:17,270
all right so now we know that the field inside is zero

1715
01:46:17,330 --> 01:46:19,300
so this is what are

1716
01:46:19,320 --> 01:46:20,760
smaller than are

1717
01:46:20,850 --> 01:46:23,340
let's not go are

1718
01:46:23,390 --> 01:46:24,510
larger than are

1719
01:46:24,620 --> 01:46:29,890
everything i told you hold for sphere which is outside

1720
01:46:29,930 --> 01:46:31,450
this hollow

1721
01:46:32,840 --> 01:46:37,240
everything all the field here must be the same everywhere on the surface the eighty

1722
01:46:37,250 --> 01:46:39,440
eight parallel and parallel

1723
01:46:39,450 --> 01:46:42,270
so i can write down again that four pi

1724
01:46:42,330 --> 01:46:44,710
or scratches the surface area

1725
01:46:44,860 --> 01:46:50,510
the electric field vector must be q inside divided by actually zero but this q

1726
01:46:50,570 --> 01:46:52,240
is that q

1727
01:46:52,320 --> 01:46:53,830
a lot of zero

1728
01:46:53,960 --> 01:46:55,520
this charge inside

1729
01:46:55,540 --> 01:46:56,400
so now

1730
01:46:56,400 --> 01:46:57,270
i know

1731
01:46:57,300 --> 01:46:59,490
the electric field e

1732
01:46:59,500 --> 01:47:01,200
in terms of its magnitude

1733
01:47:01,210 --> 01:47:02,390
is q

1734
01:47:02,430 --> 01:47:03,850
divided by four pi

1735
01:47:03,870 --> 01:47:05,850
are square

1736
01:47:05,860 --> 01:47:09,460
actually non-zero

1737
01:47:09,500 --> 01:47:10,900
and we know the direction

1738
01:47:10,910 --> 01:47:15,250
if it is positive of course it is radially outward and this is negative is

1739
01:47:15,260 --> 01:47:17,040
radially inwards

1740
01:47:17,100 --> 01:47:17,960
and this

1741
01:47:17,980 --> 01:47:19,550
is unknown

1742
01:47:19,680 --> 01:47:22,370
all result

1743
01:47:22,390 --> 01:47:24,620
we've seen this before

1744
01:47:24,640 --> 01:47:26,770
if i had put all the charge

1745
01:47:26,790 --> 01:47:30,940
right here at the middle of the centre we would have gotten exactly the same

1746
01:47:30,940 --> 01:47:33,180
and so we've seen that before

1747
01:47:33,330 --> 01:47:37,720
in other words where the charge is uniformly distributed over here

1748
01:47:37,890 --> 01:47:42,020
what are the chances all of it exactly at the centre of the sphere that

1749
01:47:42,020 --> 01:47:47,970
makes no difference for the field as long as you outside the sphere

1750
01:47:48,010 --> 01:47:50,540
if you plot the electric field

1751
01:47:51,530 --> 01:47:53,060
as a function of are

1752
01:47:53,080 --> 01:47:57,410
and here's capitol are

1753
01:47:57,420 --> 01:47:59,280
and this is the

1754
01:47:59,370 --> 01:48:01,560
the field strength

1755
01:48:01,600 --> 01:48:05,480
then you will get that the electric field is zero insight

1756
01:48:05,510 --> 01:48:07,950
john stewart maximum value

1757
01:48:07,980 --> 01:48:09,890
and this falls off

1758
01:48:09,930 --> 01:48:11,970
as one of our square

1759
01:48:11,980 --> 01:48:20,440
proportional to one of our script

1760
01:48:20,510 --> 01:48:23,740
if i go back to the situation that the

1761
01:48:23,780 --> 01:48:27,000
charged that the electric field inside

1762
01:48:27,060 --> 01:48:28,460
is zero

1763
01:48:28,460 --> 01:48:29,710
you may say

1764
01:48:29,720 --> 01:48:32,030
isn't that a little bit of a cheat

1765
01:48:33,240 --> 01:48:38,170
there is no charge inside but have you really use the charge outside

1766
01:48:38,240 --> 01:48:39,350
and if you have

1767
01:48:39,410 --> 01:48:42,310
use it how did you use it well i have used it

1768
01:48:42,360 --> 01:48:45,790
i use it for my symmetry arguments

1769
01:48:45,840 --> 01:48:48,000
the symmetry arguments

1770
01:48:48,050 --> 01:48:50,230
take into account

1771
01:48:50,260 --> 01:48:53,810
that the charge is uniformly distributed

1772
01:48:53,860 --> 01:48:58,690
if the charge on this if had not been uniformly distributed i could not use

1773
01:48:58,700 --> 01:49:00,480
the symmetry argument

1774
01:49:00,480 --> 01:49:05,370
and therefore the electric field inside would in fact not having zero if there is

1775
01:49:05,370 --> 01:49:10,010
more chance this here then there is there in the field inside the sphere is

1776
01:49:10,010 --> 01:49:11,270
not zero

1777
01:49:11,330 --> 01:49:13,310
so i have it was all the charge

1778
01:49:13,360 --> 01:49:14,820
by using my

1779
01:49:14,890 --> 01:49:17,320
symmetry arguement

1780
01:49:17,340 --> 01:49:19,370
god's law

1781
01:49:19,410 --> 01:49:23,180
and coulomb law in a way i the same law

1782
01:49:23,190 --> 01:49:25,640
they both link the electric field

1783
01:49:25,690 --> 01:49:28,470
with the charge q

1784
01:49:28,510 --> 01:49:30,000
he is the fact

1785
01:49:30,000 --> 01:49:31,970
that the electric force

1786
01:49:31,980 --> 01:49:33,220
falls off

1787
01:49:33,230 --> 01:49:36,080
as one of our square

1788
01:49:36,110 --> 01:49:40,670
if the electric field strengths did not as one of four square

1789
01:49:40,670 --> 01:49:43,640
goals law wouldn't even hold

1790
01:49:43,680 --> 01:49:45,760
and the electric field inside

1791
01:49:45,810 --> 01:49:49,770
this uniformly charged you wouldn't be zero

1792
01:49:49,820 --> 01:49:52,640
so what is the meaning of consequence of the fact

1793
01:49:54,030 --> 01:49:59,200
electric forces fall off as one of our square

1794
01:49:59,200 --> 01:50:01,740
gravitational forces also fall off

1795
01:50:01,750 --> 01:50:04,400
as one of four square

1796
01:50:04,440 --> 01:50:08,210
therefore if you take a planet if it existed

1797
01:50:08,220 --> 01:50:13,210
which is a hollow spherical a spherical planet with hollow inside

1798
01:50:13,220 --> 01:50:15,020
it means there would be no

1799
01:50:15,040 --> 01:50:19,390
gravitational field inside that whole planet

1800
01:50:19,390 --> 01:50:20,790
so if you were there

1801
01:50:20,810 --> 01:50:22,860
there will be no gravitational

1802
01:50:22,870 --> 01:50:24,740
force on new

1803
01:50:24,790 --> 01:50:26,300
david is spherical

1804
01:50:26,340 --> 01:50:28,350
if that planets were purely local

1805
01:50:30,910 --> 01:50:33,160
the gravitational field inside

1806
01:50:33,230 --> 01:50:34,660
i would not be

1807
01:50:38,240 --> 01:50:42,480
you say well big deal was a to one we always take planet

1808
01:50:42,490 --> 01:50:46,160
and then it sort of far outside the planet we put all the mass and

1809
01:50:46,160 --> 01:50:48,220
we consider it as the point

1810
01:50:50,060 --> 01:50:51,720
it's not a big deal for you

1811
01:50:51,750 --> 01:50:55,730
it's not a big deal for me but it was a big deal for notes

1812
01:50:55,740 --> 01:50:57,700
he intuitively senses

1813
01:50:57,710 --> 01:50:59,260
that it was correct

1814
01:50:59,320 --> 01:51:01,960
that if you have a planet

1815
01:51:02,000 --> 01:51:03,940
uniform mass distribution

1816
01:51:03,950 --> 01:51:07,050
that you can consider it as a point mass

1817
01:51:07,190 --> 01:51:09,360
long outside the planet

1818
01:51:09,410 --> 01:51:12,150
but it took him twenty years to prove it

1819
01:51:12,160 --> 01:51:14,430
and he finally published his results

1820
01:51:14,480 --> 01:51:16,280
it will take is now thirty seconds

1821
01:51:16,300 --> 01:51:18,660
he didn't have access to gauss law

1822
01:51:18,660 --> 01:51:21,090
came about a hundred years later

1823
01:51:21,120 --> 01:51:22,760
but the net result is

1824
01:51:22,760 --> 01:51:24,510
that you see in front of you

1825
01:51:24,530 --> 01:51:27,030
that if you have uniformly charged distribution

1826
01:51:27,080 --> 01:51:29,210
you can draw a parallel with gravity

1827
01:51:29,310 --> 01:51:32,950
that it's you get the same electric field outside

1828
01:51:32,990 --> 01:51:34,330
that you

1829
01:51:34,350 --> 01:51:36,760
i would have gotten if all the charge

1830
01:51:36,780 --> 01:51:39,710
is at one location

1831
01:51:39,710 --> 01:51:42,220
at the centre

1832
01:51:42,270 --> 01:51:43,190
this is

1833
01:51:43,190 --> 01:51:44,680
spherical symmetry

1834
01:51:44,690 --> 01:51:47,320
number one that's the easiest symmetry

1835
01:51:47,350 --> 01:51:49,260
that we have in

1836
01:51:49,270 --> 01:51:50,400
o eight o two

1837
01:51:50,440 --> 01:51:54,580
now i will present you with the second form of symmetry which is a flat

1838
01:51:54,580 --> 01:51:56,690
horizontal plane

1839
01:51:56,740 --> 01:51:59,370
and i want you to work out most of it would help you a little

1840
01:51:59,370 --> 01:52:02,020
bit to set up

1841
01:52:02,180 --> 01:52:05,070
suppose we have a plane

1842
01:52:05,140 --> 01:52:07,250
which is very very large

1843
01:52:07,260 --> 01:52:09,840
think of it for now is infinitely large

1844
01:52:09,860 --> 01:52:13,200
it doesn't exist of course infinitely large

1845
01:52:13,240 --> 01:52:14,950
and i put on this plane

1846
01:52:17,440 --> 01:52:19,530
and i put

1847
01:52:19,550 --> 01:52:21,350
a certain amount of

1848
01:52:21,420 --> 01:52:22,940
charge density

1849
01:52:22,950 --> 01:52:25,590
which i call sigma

1850
01:52:26,500 --> 01:52:28,680
is an amount of charge q

1851
01:52:28,700 --> 01:52:30,420
for every is

1852
01:52:30,430 --> 01:52:33,270
so only a certain number of cool

1853
01:52:33,290 --> 01:52:35,120
per square metre

1854
01:52:35,170 --> 01:52:40,790
energy uniformly distributed the whole plane everywhere has the same number of coulombs per square

1855
01:52:40,790 --> 01:52:46,190
metre micro cules will not cornwall's whatever you prefer

1856
01:52:46,350 --> 01:52:48,750
this is a play which is huge

1857
01:52:48,760 --> 01:52:50,280
you're being asked

1858
01:52:50,330 --> 01:52:53,300
what is the electric field anywhere in space

1859
01:52:53,310 --> 01:52:54,790
just like we before

1860
01:52:54,800 --> 01:52:59,570
we ask what is the electric field anywhere inside the sphere and anywhere outside the

1861
01:52:59,570 --> 01:53:03,640
sphere now i want to know what it is anywhere in the vicinity of this

1862
01:53:05,390 --> 01:53:07,000
it is now you pick

1863
01:53:07,010 --> 01:53:09,220
clever girl surface

1864
01:53:09,260 --> 01:53:10,830
the answer pops out

1865
01:53:10,840 --> 01:53:12,160
very quickly

1866
01:53:12,170 --> 01:53:14,710
if you choose this sphere

1867
01:53:14,750 --> 01:53:18,090
as you go service you dead in the water you get nowhere because there is

1868
01:53:18,090 --> 01:53:20,760
no spherical symmetry

1869
01:53:20,810 --> 01:53:22,800
i will define for you that

1870
01:53:22,840 --> 01:53:24,760
go surface

1871
01:53:24,760 --> 01:53:29,540
things can be brought to bear on actual behavioral data from human subjects to show

1872
01:53:29,550 --> 01:53:34,550
yes this really is a way to think about basic learning and inference behavior and

1873
01:53:35,390 --> 01:53:37,390
and then will

1874
01:53:37,440 --> 01:53:42,100
go to most more interesting problems like the problem i started with learning concepts from

1875
01:53:42,100 --> 01:53:46,500
examples which is an area where we're not we're going beyond just textbook bayesian statistics

1876
01:53:46,500 --> 01:53:50,570
basically to the frontiers of machine learning it's an area where machine learning has made

1877
01:53:50,570 --> 01:53:53,130
a lot of progress but where there's still a lot of progress to be made

1878
01:53:53,130 --> 01:53:55,940
in coming up with more human-like systems

1879
01:53:55,960 --> 01:54:00,340
and then probably some next time we'll go to these large scale systems of knowledge

1880
01:54:00,340 --> 01:54:02,170
like intuitive theories

1881
01:54:02,190 --> 01:54:06,400
OK and i understand people sometimes like to ask questions so feel free to

1882
01:54:06,420 --> 01:54:11,710
to jump in these lectures compared to the other lectures will probably be a little

1883
01:54:11,710 --> 01:54:12,520
bit less

1884
01:54:12,550 --> 01:54:16,940
for the technical specifics partly because i'm trying to build on on what i think

1885
01:54:16,940 --> 01:54:18,250
are technical

1886
01:54:18,270 --> 01:54:21,710
things you've already learned show you things you can do with them but you know

1887
01:54:21,710 --> 01:54:25,160
that feel free to ask jump if if there's something that seems like i'm not

1888
01:54:25,160 --> 01:54:30,320
being clear not sure exactly what technical idea i'm referring to

1889
01:54:30,330 --> 01:54:36,300
now these this idea of viewing cognition as probabilistic inference is much bigger than any

1890
01:54:36,300 --> 01:54:38,510
of the work that i have to tell you about here in the work that

1891
01:54:38,510 --> 01:54:43,020
i've been involved in this is side i prepared for NIPS tutorial a couple of

1892
01:54:43,020 --> 01:54:44,530
years ago on the same topic

1893
01:54:45,120 --> 01:54:49,310
and i was just you know taking all the different areas of cognitive science where

1894
01:54:49,670 --> 01:54:55,930
people have made progress recently by viewing them as as probabilistic inference i didn't have

1895
01:54:55,930 --> 01:54:58,950
time to update the slide since two thousand seven so apologies to those of you

1896
01:54:58,950 --> 01:55:01,820
who started working on these things since two thousand seven but i wouldn't have had

1897
01:55:01,820 --> 01:55:05,440
room for names on here anyway the point is this is an extremely active exciting

1898
01:55:05,440 --> 01:55:10,640
research area and it's only getting more active so it's good time to to get

1899
01:55:10,640 --> 01:55:12,090
interested in it

1900
01:55:12,100 --> 01:55:15,720
what i tell you about here just a couple of examples from work that i

1901
01:55:15,720 --> 01:55:21,550
did mostly with tom griffiths we which was which is again trying to look at

1902
01:55:21,590 --> 01:55:24,300
the basic cognitive capacities that can be

1903
01:55:24,360 --> 01:55:28,850
analyzed from the point of view of almost textbook bayesian statistics really really simple elementary

1904
01:55:28,850 --> 01:55:31,930
things that you from machine learning point of view they are that's trivial but there

1905
01:55:31,930 --> 01:55:35,710
are the place you have to start to show that these ideas actually apply

1906
01:55:36,520 --> 01:55:39,500
human cognition i won't go into some of the history here but there's a long

1907
01:55:39,500 --> 01:55:44,280
history in psychology of going back and forth between viewing the mind as a kind

1908
01:55:44,280 --> 01:55:49,700
of intuitive statistician and people being extremely skeptical of the idea so within cognitive psychology

1909
01:55:49,700 --> 01:55:53,380
there's a lot of work to be done initially to just established the groundwork that

1910
01:55:53,380 --> 01:55:56,710
that these ideas from from probability theory were

1911
01:55:56,740 --> 01:56:00,120
appropriate and useful for describing basic cognition

1912
01:56:00,240 --> 01:56:03,610
so i give a couple of examples on one will start off with an example

1913
01:56:03,610 --> 01:56:09,380
from from causal learning this is again a basic thing this isn't just about bayesian

1914
01:56:09,380 --> 01:56:15,340
statistics one of the basic things statisticians want to do is establish reliable relationships between

1915
01:56:15,340 --> 01:56:20,540
variables you might just be interested in the correlation although increasingly statisticians we don't just

1916
01:56:20,540 --> 01:56:24,570
want correlation they want to know is actually a causal relation cannot intervene and manipulating

1917
01:56:24,570 --> 01:56:26,160
make something happen

1918
01:56:26,170 --> 01:56:30,210
but you know here's again textbook problem you have data that comes in the form

1919
01:56:30,890 --> 01:56:31,860
two by two

1920
01:56:31,870 --> 01:56:33,820
set up like this there some

1921
01:56:33,870 --> 01:56:39,320
variable c and some other variable e there are binary and you observe

1922
01:56:39,340 --> 01:56:43,970
instances where c either happened or didn't happen either happen it didn't happen and you're

1923
01:56:43,990 --> 01:56:48,780
interested in whether or not see causes so you know example might be whether injecting

1924
01:56:48,780 --> 01:56:52,530
some chemical causes mice to express certain gene that's

1925
01:56:52,550 --> 01:56:57,240
semi scientific example but it the experiments that have been done with people are you

1926
01:56:57,240 --> 01:57:00,790
give them data sort of as if they were assigned to doing an experiment a

1927
01:57:00,890 --> 01:57:03,170
data coming in either in the table are online

1928
01:57:03,190 --> 01:57:07,540
and you basically ask them to make a judgement about that question on a scale

1929
01:57:07,540 --> 01:57:09,350
from zero to one hundred

1930
01:57:09,360 --> 01:57:12,590
traditionally people have been so clear to me if you if you want to know

1931
01:57:12,590 --> 01:57:16,040
what if you might think if you're thinking you look at the question does c

1932
01:57:16,040 --> 01:57:16,840
cause e

1933
01:57:16,960 --> 01:57:20,110
great judgement on a scale from zero to one hundred

1934
01:57:20,120 --> 01:57:23,070
and if your first thought as well i'm not sure exactly what you mean that

1935
01:57:23,070 --> 01:57:28,080
can mean different things that is part of the point that i'm trying to make

1936
01:57:28,080 --> 01:57:33,090
the psychologists weren't originally very clear about that but now that we've we've tried at

1937
01:57:33,090 --> 01:57:37,550
that the each component of Y as a poisson distribution the corresponding component of A

1938
01:57:37,550 --> 01:57:46,770
X has its mean and why is that well basically this represents attenuation and superposition that's

1939
01:57:46,770 --> 01:57:51,710
what's going on and that's captured exactly in that in that the model so providing your counters the the

1940
01:57:51,710 --> 01:57:58,290
whichever you are collecting the photons in they don't have any dead times and providing things stationary

1941
01:57:58,290 --> 01:58:03,950
and so on and this is a pretty pretty good model so we have

1942
01:58:03,950 --> 01:58:09,710
our X is doing two difference spaces here cause X is in the body so that'sthree-dimensional

1943
01:58:09,710 --> 01:58:15,250
coordinates within two- or three-dimensional cordinates within the body indexed by S and then

1944
01:58:15,250 --> 01:58:20,030
the array of detected phoco photons is measured in the space in which you are collecting the

1945
01:58:20,030 --> 01:58:27,210
it the the photons and I'll call that T so A T S  is the A T S is the mean amount

1946
01:58:27,240 --> 01:58:34,930
of sorry it's the mean number of counts detected at T from the

1947
01:58:34,930 --> 01:58:43,310
units source and the unit time at Xs so that's a precise physical interpretation capturing all

1948
01:58:43,310 --> 01:58:48,030
those effects and we've you know we've got some reasonably good models for what those

1949
01:58:48,030 --> 01:58:56,230
numbers are now how is it possible if you ever thought about tomography but I think this

1950
01:58:56,230 --> 01:59:00,510
cartoon might help a little bit this is a two-dimensional cross-section so this is

1951
01:59:00,510 --> 01:59:06,450
looking down on the patient's head if you like and this two hotspots different degree

1952
01:59:06,450 --> 01:59:13,530
of hotness and here's the gamma camera it's a one-dimensional camera in this case looking at

1953
01:59:13,530 --> 01:59:20,090
the patient collecting photons in these bins and you can see the pattern of intensity there

1954
01:59:20,630 --> 01:59:25,390
and what I what I've got in this model is that there's this essentially rectilinear

1955
01:59:25,390 --> 01:59:31,530
propagation together with some spread which is to do with the photons interacting with body

1956
01:59:31,530 --> 01:59:37,490
tissue and avoiding spread out so I get on the basis of that view of

1957
01:59:37,500 --> 01:59:41,170
the this body I get this and of course that's not enough to infer very much if

1958
01:59:41,300 --> 01:59:46,490
I stopped moving the camera I start building something up

1959
01:59:46,490 --> 01:59:53,590
okay so can see as I go around I'm getting a view from each direction

1960
01:59:53,590 --> 01:59:59,490
and the pattern I'm seeing is obviously a representation in a slightly strange space

1961
01:59:59,490 --> 02:00:05,130
of what is going on in here so these high spots here for example

1962
02:00:05,130 --> 02:00:09,770
came when the camera was over here somewhere and down here and these two

1963
02:00:09,770 --> 02:00:19,570
hotspots were exactly in line and the higher one was nearer so that's why it's hotter than there okay

1964
02:00:19,570 --> 02:00:23,810
so this is this is the data actually there's no I think there's no noise in

1965
02:00:23,810 --> 02:00:28,170
this but this is the the mean of the data this is a long-run data

1966
02:00:28,840 --> 02:00:38,470
what I wnat to do is to use these data to infer those those sources so clearly it's

1967
02:00:38,470 --> 02:00:43,950
not trivial but I hope it's believable that is possible because there is there is

1968
02:00:43,950 --> 02:00:48,070
a transformation from this space this is the X space to this space which is the

1969
02:00:48,070 --> 02:00:55,730
the Y space this is what they actually look like in reality this is a scan of apelvis and I've transpose

1970
02:00:55,730 --> 02:01:02,710
it so this is the angle and this is the linear dimension of the detector

1971
02:01:02,710 --> 02:01:10,350
and what you seen basically is superimposed sign waves and essentially as for a single point

1972
02:01:10,350 --> 02:01:14,350
source in the body you get an attenuated sign wave as you go as

1973
02:01:14,360 --> 02:01:20,890
you move the camera around and what you what you observe is a superposition in

1974
02:01:20,890 --> 02:01:24,590
all those time waves plus noise and this is this is an example so this is

1975
02:01:24,590 --> 02:01:32,540
called the signogram for obvious reasons the kind of priors we use well we've tried

1976
02:01:32,540 --> 02:01:37,870
various things and sometimes gaussian priors but this is quite a good choice this this

1977
02:01:37,870 --> 02:01:45,690
prior it's a sort of medium informative kind of prior which is encoding something

1978
02:01:45,690 --> 02:01:56,330
rather slightly unspecific about the smoothness of the truth if you had

1979
02:01:56,330 --> 02:02:00,680
is sort of this log cosh if you had a square then this would be a gaussian prior

