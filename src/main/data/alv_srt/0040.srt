1
00:00:00,000 --> 00:00:03,710
nine one

2
00:00:59,680 --> 00:01:01,720
there are

3
00:01:03,020 --> 00:01:07,280
are what they are

4
00:01:30,200 --> 00:01:36,720
very wrong

5
00:01:36,730 --> 00:01:41,070
thank you

6
00:02:00,590 --> 00:02:06,310
o is working

7
00:02:19,980 --> 00:02:24,470
there were one

8
00:02:24,480 --> 00:02:28,810
on you

9
00:02:38,760 --> 00:02:44,250
and all

10
00:02:58,310 --> 00:03:00,320
one year

11
00:03:44,730 --> 00:03:48,740
very good

12
00:04:06,850 --> 00:04:10,040
are you know

13
00:04:10,050 --> 00:04:15,270
but it is

14
00:04:32,520 --> 00:04:34,150
the worst case

15
00:05:27,180 --> 00:05:30,130
i think

16
00:05:30,180 --> 00:05:33,010
you using your

17
00:05:33,070 --> 00:05:35,600
here are

18
00:05:40,550 --> 00:05:45,200
you are

19
00:06:06,290 --> 00:06:10,440
three i four

20
00:06:10,480 --> 00:06:13,620
it's easy to see

21
00:06:16,380 --> 00:06:19,340
not all

22
00:06:19,350 --> 00:06:24,880
he right

23
00:06:36,330 --> 00:06:39,770
on very

24
00:06:39,950 --> 00:06:43,630
eight year

25
00:06:45,020 --> 00:06:47,410
need the

26
00:06:47,410 --> 00:06:54,050
the Dirichlet process is defined in this way right where given a partition of

27
00:06:54,050 --> 00:07:00,450
the space the the prior distribution of this random vector here is gonna be

28
00:07:00,450 --> 00:07:05,490
a Dirichlet distributed with the two parameters given by alpha and h let me see

29
00:07:05,490 --> 00:07:10,850
that h is the base distribution is basically the mean of the Dirichlet process

30
00:07:10,850 --> 00:07:17,300
while the alpha parameter's related to the in the inverse of the variance so

31
00:07:17,300 --> 00:07:22,290
the Dirichlet process is basically a defined in terms of the Dirichlet distribution which has

32
00:07:22,290 --> 00:07:29,170
this conjugacy property with the multinominal or the discrete distribution and this conjugacy actually carries over

33
00:07:29,170 --> 00:07:34,050
to the Dirichlet process and that's actually part of the reason why Ferguson actually

34
00:07:34,050 --> 00:07:39,190
was interested in the Dirichlet process to begin with basically it's this nice conjugacy property

35
00:07:39,190 --> 00:07:47,270
that you can calculate a lot of the the conditional distributions easily okay so and this

36
00:07:47,270 --> 00:07:52,730
property kind of basically goes as follows let's suppose that our random

37
00:07:52,730 --> 00:08:00,150
probabily the measure G is has a Dirichlet process prior given by alpha and h

38
00:08:00,150 --> 00:08:04,250
since G itself is a probability measure we can think of it as also a

39
00:08:04,250 --> 00:08:09,650
distribution over our space and since is that this is the distribution we can

40
00:08:09,650 --> 00:08:15,750
define random variables that are G distributers so each of them is gonna be sample

41
00:08:15,780 --> 00:08:21,490
from this G distribution right so let's call this theta I so theta I I from

42
00:08:21,490 --> 00:08:26,790
one to n each of them are gonna be IID draws from this G distribution

43
00:08:26,830 --> 00:08:32,470
okay so that's perfectly well defined now the question is what this is our

44
00:08:32,470 --> 00:08:36,850
prior this is our kind of a likelihood sort of term so the question is what is

45
00:08:36,850 --> 00:08:42,830
the posterior turns out the posterior is also gonna be a Dirichlet process it's a just

46
00:08:42,830 --> 00:08:46,110
as in the case of the a final mixture model we have a Dirichlet prior

47
00:08:46,110 --> 00:08:52,010
on missing proportions we have a likelihood term which is kind of conjugate to the Dirichlet

48
00:08:52,010 --> 00:08:55,630
and then the posterior's also gonna be a Dirichlet so in this case the posterior's also

49
00:08:55,630 --> 00:09:01,950
gonna be Dirichlet process okay so the posterior over G given theta one to theta n

50
00:09:01,960 --> 00:09:07,830
you can work out to be basically a Dirichlet process where the

51
00:09:07,850 --> 00:09:13,910
mass parameter we incremented by n basically n corresponds to the total number observations that

52
00:09:13,910 --> 00:09:21,430
we have here and the base measure is updated by basically adding atoms

53
00:09:21,430 --> 00:09:28,250
one corresponding to each observation and then normalizing okay so alpha H comes from the prior

54
00:09:28,250 --> 00:09:35,500
and then we add in N atoms each one located at theta I and

55
00:09:35,500 --> 00:09:40,970
then we had to normalize it by Alpha plus N so in this sense

56
00:09:40,970 --> 00:09:46,050
the this alpha parameter is a prior mass parameter right so the more data that we observe

57
00:09:46,070 --> 00:09:50,570
we simply add a larger N to this thing so if we can think of N

58
00:09:50,570 --> 00:09:57,750
here as the mass or the effects of the of our data this alpha here

59
00:09:57,750 --> 00:10:05,410
we can think of it as the total effects of our prior okay so the

60
00:10:05,410 --> 00:10:11,570
nice thing with this posterior Dirichlet processes is so one things that

61
00:10:11,570 --> 00:10:16,320
you could do is now you could define an iterative process defined in terms of

62
00:10:16,330 --> 00:10:21,310
this Dirichlet process so imagine that we have the following model again so we have

63
00:10:21,340 --> 00:10:25,750
G is Dirichlet process distributed and then we have a sequence now is could

64
00:10:25,750 --> 00:10:29,410
be an infinite sequence of theta one theta two and so forth each of them

65
00:10:29,410 --> 00:10:43,050
is going to be IID draws from G okay so the model looks like the following

66
00:10:43,050 --> 00:10:50,850
G goes to theta one to theta two theta three and so forth right and we can

67
00:10:50,850 --> 00:10:58,330
ask the following question what is the conditional distribution or the predictive distribution of

68
00:10:58,340 --> 00:11:05,890
the N plus first theta given the theta one to theta N alright so we have

69
00:11:05,910 --> 00:11:14,390
observed theta one until theta N and would like to compute the conditional distribution of

70
00:11:14,390 --> 00:11:23,210
the next one given the observed but marginalizing out G ok basically you know

71
00:11:23,290 --> 00:11:27,810
which is being base about this G is unobserved so we just integrate that up so we would like to compute

72
00:11:27,810 --> 00:11:35,170
conditional of theta N plus one given the first N thetas because the posterior distribution of

73
00:11:35,180 --> 00:11:39,870
G has a base measure which is given by this form here which is the

74
00:11:39,880 --> 00:11:45,590
mean of the of G is the basically the posterior mean of G we can work out

75
00:11:45,590 --> 00:11:50,630
that the predictive distribution of theta N plus one given theta one to theta N is gonna

76
00:11:50,630 --> 00:11:57,910
be given by this posterior mean okay and so if you look at this it's a

77
00:11:57,910 --> 00:12:04,250
goes as follows so it basically says that the probability theta N plus one

78
00:12:04,250 --> 00:12:11,370
being equal to theta I is gonna be one over Alpha plus N alright so that

79
00:12:11,370 --> 00:12:28,830
term here looks like okay and this term here is basically a degenerate probability distribution

80
00:12:28,830 --> 00:12:36,490
that gives probablity one to value given by theta I and probability zero everywhere else okay

81
00:12:36,490 --> 00:12:41,930
so with probability one over Alpha plus N theta N plus one is gonna take

82
00:12:41,930 --> 00:12:48,730
on this value theta I okay  and there's N of those and with probability alpha over

83
00:12:48,730 --> 00:12:53,430
alpha plus N theta N plus one is gonna be drawn from from the base

84
00:12:53,430 --> 00:13:04,090
distribution H yes yes that's right and there's actually the first simple

85
00:13:04,090 --> 00:13:08,510
result about the con convergence of the consistency of a Dirichlet process so if you

86
00:13:08,510 --> 00:13:14,020
use it as a prior to estimate a distribution then as we have an infinite amount of

87
00:13:14,030 --> 00:13:19,270
data then this term will dominate and this term goes to zero and so the

88
00:13:19,270 --> 00:13:28,550
do dominating term basically converges to the empirical distribution and that we know is consistent

89
00:13:28,570 --> 00:13:33,870
okay so and then this process here is called an urn scheme basically

90
00:13:33,880 --> 00:13:39,810
sometimes it's called polva urn scheme sometimes it's called Hopp or Hoppe urn scheme  sometimes it's called

91
00:13:40,010 --> 00:13:46,410
bčackwell macqueen urn scheme okay so they're basically the same thing and the metaphor

92
00:13:46,410 --> 00:13:51,410
is the following so we start off with an urn with alpha balls and this

93
00:13:51,410 --> 00:13:59,500
are of  a special color okay  and at each iteration of the of the scheme we're gonna

94
00:13:59,890 --> 00:14:04,670
reach our hand into the urn and pick up a ball at random oaky if the ball that

95
00:14:04,670 --> 00:14:09,390
we picked is a special color then we will make a new ball with the

96
00:14:09,400 --> 00:14:14,510
color sample from H and then we drop we know the color of the ball

97
00:14:14,510 --> 00:14:19,070
and then  we return  balls back into the urn both the special color ball and

98
00:14:19,070 --> 00:14:20,080
OK thanks

99
00:14:20,120 --> 00:14:23,160
all right thank you very much

100
00:14:23,180 --> 00:14:25,820
today i'm going to talk about on mining ending in game theory i think it

101
00:14:25,820 --> 00:14:31,000
would be a nice complementarities talk it's it's mostly theoretical work

102
00:14:31,780 --> 00:14:33,520
in experiments

103
00:14:33,550 --> 00:14:39,900
so how many of you are familiar with the weighted majority analysis

104
00:14:39,930 --> 00:14:43,470
OK i'm just kidding i'm not going to have a bunch of equations for the

105
00:14:43,470 --> 00:14:47,880
last talk of the the of the week so i figured i'd try to do

106
00:14:47,880 --> 00:14:52,190
the theoretical talk but i'm just gonna is mostly pictures and i'll see how well

107
00:14:52,190 --> 00:14:57,450
it works hopefully you get the big picture on the main ideas in some experience

108
00:14:57,450 --> 00:14:58,490
as well

109
00:14:59,090 --> 00:15:02,700
this is the talk about learning talk both about online learning and how it relates

110
00:15:02,700 --> 00:15:05,890
to gain game playing repeated game-playing

111
00:15:05,920 --> 00:15:08,560
one of the questions you know how do people learn how do

112
00:15:08,590 --> 00:15:10,270
what's the right model of learning

113
00:15:10,280 --> 00:15:12,480
from a theoretical point of view

114
00:15:13,690 --> 00:15:15,270
so one model of learning

115
00:15:15,300 --> 00:15:17,990
would be OK you just got a bunch of data

116
00:15:18,010 --> 00:15:21,100
that's the training data drawn from some distribution

117
00:15:21,120 --> 00:15:24,480
and the goal is to learn the function which is

118
00:15:24,520 --> 00:15:26,340
predicting y from x

119
00:15:27,840 --> 00:15:30,700
and that's the batch model right you get a bunch of data

120
00:15:30,790 --> 00:15:35,980
you have time to run your training algorithm get samples from this distribution

121
00:15:35,990 --> 00:15:40,820
and your goal is to output function that will predict well on future examples

122
00:15:40,820 --> 00:15:42,870
from the same distribution

123
00:15:42,880 --> 00:15:47,070
that's the setting that i guess which was working in and another model would be

124
00:15:47,070 --> 00:15:50,700
the online model which i think you've heard about already but i just described again

125
00:15:50,700 --> 00:15:53,230
we're it's more like a repeated game model where

126
00:15:53,240 --> 00:15:56,370
you have this child is sitting there gets an example yesterday side to do make

127
00:15:57,430 --> 00:16:01,820
they began writing together and he finds out the answer so each time if there's

128
00:16:01,820 --> 00:16:03,350
an example makes a prediction

129
00:16:03,400 --> 00:16:06,070
and finds out whether or not he was right

130
00:16:06,760 --> 00:16:10,870
and this is this is much more difficult setting here because we don't have a

131
00:16:10,880 --> 00:16:14,310
probability distribution is no guarantee that future examples

132
00:16:14,320 --> 00:16:17,630
coming from the same distribution as previous examples

133
00:16:17,650 --> 00:16:21,400
and this is the way many real world situations are

134
00:16:21,430 --> 00:16:24,980
we don't have this kind of guarantees

135
00:16:25,010 --> 00:16:27,940
t and we're going to see today is that there's a lot of similarities between

136
00:16:27,940 --> 00:16:32,380
these two settings in that lot of things we can prove that the same kind

137
00:16:32,380 --> 00:16:33,880
of rates happen in both

138
00:16:33,890 --> 00:16:36,510
the online and offline settings

139
00:16:36,530 --> 00:16:40,260
when i mentioned how to my slides

140
00:16:40,280 --> 00:16:43,730
lot this talk is not things that i did myself it's just the summary some

141
00:16:43,730 --> 00:16:48,130
of it is what i've done myself joint work with some crocodile

142
00:16:48,940 --> 00:16:52,850
and feel free to ask questions i want to go slowly make sure everyone understands

143
00:16:52,860 --> 00:16:55,880
and i don't mind if i don't get everything

144
00:16:55,950 --> 00:16:57,800
OK so

145
00:16:57,880 --> 00:17:02,440
well i'm going to talk about online versus batch learnability of functions

146
00:17:02,450 --> 00:17:06,510
and we're going to explain why not surprisingly online setting is harder than the batch

147
00:17:08,350 --> 00:17:11,660
i'll give an example of a simple case where you have just a small number

148
00:17:11,660 --> 00:17:14,980
of functions and you want to learn to do almost as well as the best

149
00:17:16,070 --> 00:17:17,850
and then i'll show

150
00:17:17,880 --> 00:17:19,830
why these two settings are

151
00:17:19,850 --> 00:17:21,760
roughly the same

152
00:17:21,760 --> 00:17:25,290
i mean in the second part of my talk about online learning in repeated games

153
00:17:25,290 --> 00:17:27,980
and i think the are pierre some of that is that right

154
00:17:28,040 --> 00:17:32,350
i was here first lecture but maybe go over that again and then i'll talk

155
00:17:32,350 --> 00:17:36,510
in for zero sum games he showed how you could use boosting to play games

156
00:17:36,570 --> 00:17:38,190
that right

157
00:17:38,200 --> 00:17:43,420
no maybe not OK well it would be nice and also general sum games where

158
00:17:43,510 --> 00:17:47,110
it's not the one player wins and the other places it's just a general game

159
00:17:47,300 --> 00:17:51,730
and this very nice algorithms for learning in those settings that could convert to something

160
00:17:51,730 --> 00:17:54,350
called correlated equilibrium

161
00:17:55,350 --> 00:18:00,550
so in the online setting again we have know example we have this adversary let's

162
00:18:01,100 --> 00:18:04,920
so make it really hard and he pick some points and we have to predict

163
00:18:04,920 --> 00:18:08,140
its label either plus or minus we make a prediction

164
00:18:08,160 --> 00:18:11,860
minus we find out the minus OK we got it right

165
00:18:11,920 --> 00:18:13,600
make another prediction

166
00:18:13,640 --> 00:18:17,450
we find out we we predict minus its plus we got it wrong

167
00:18:17,470 --> 00:18:20,100
and so on this repeats in an online fashion

168
00:18:21,850 --> 00:18:25,320
and it in the and how we can judge their performance well look at our

169
00:18:25,320 --> 00:18:28,350
air will simply count the number of mistakes we made

170
00:18:28,380 --> 00:18:31,450
our hundred OK

171
00:18:31,480 --> 00:18:33,850
and that this area is between zero and one

172
00:18:33,850 --> 00:18:36,610
and obviously one below

173
00:18:36,620 --> 00:18:38,060
in the batch setting

174
00:18:38,100 --> 00:18:41,850
it's easier setting we have with probability distribution

175
00:18:41,850 --> 00:18:43,220
because if the

176
00:18:43,290 --> 00:18:45,950
photons carry momentum

177
00:18:45,990 --> 00:18:47,720
and if the momentum

178
00:18:47,730 --> 00:18:52,370
photo photographi photo photo photo i feel the force

179
00:18:52,410 --> 00:18:54,990
you remember from eighty one force

180
00:18:55,050 --> 00:19:00,600
is ppt is the transfer of momentum

181
00:19:00,640 --> 00:19:03,980
if i throw rotten tomatoes that you

182
00:19:04,070 --> 00:19:06,380
then when you get these estimators on your face

183
00:19:06,430 --> 00:19:09,430
they you like this and then they go like this

184
00:19:09,530 --> 00:19:14,170
thirty come in a certain direction and they lose all their momentum in that direction

185
00:19:14,210 --> 00:19:17,230
that gives the force on new in this direction

186
00:19:17,290 --> 00:19:21,400
because that momentum in this direction is destroyed momentum is conserved

187
00:19:21,440 --> 00:19:23,720
and so you

188
00:19:23,740 --> 00:19:26,960
get that momentum

189
00:19:26,960 --> 00:19:28,430
if for instance

190
00:19:28,480 --> 00:19:31,210
i through one kilogram

191
00:19:31,310 --> 00:19:34,910
main you each second

192
00:19:34,910 --> 00:19:36,460
one after the

193
00:19:36,470 --> 00:19:41,530
all the time average one kilogram per second and each domain o

194
00:19:41,550 --> 00:19:44,390
had a speed of five meters per second

195
00:19:44,390 --> 00:19:47,650
and i call it the x direction

196
00:19:47,670 --> 00:19:49,730
here is your face

197
00:19:49,790 --> 00:19:52,460
they had to face and then they go

198
00:19:53,560 --> 00:19:57,270
so all the momentum in the x direction is destroyed

199
00:19:57,320 --> 00:20:00,910
then you will experience a force in the x direction

200
00:20:00,920 --> 00:20:02,910
is in the x direction

201
00:20:02,960 --> 00:20:07,760
which is one thousand five which is five militants

202
00:20:07,790 --> 00:20:10,260
this the time average force

203
00:20:10,310 --> 00:20:12,050
if for some reason

204
00:20:12,100 --> 00:20:17,970
the the two main of wood back to support the elastic collision like tennis ball

205
00:20:17,980 --> 00:20:19,880
then the

206
00:20:19,890 --> 00:20:22,300
momentum transfer is twice

207
00:20:22,320 --> 00:20:26,860
comes in with momentum and come back

208
00:20:26,890 --> 00:20:32,010
sort out momentum transfer is twice the number is not just destroyed no we first

209
00:20:32,050 --> 00:20:33,210
so in that case

210
00:20:33,260 --> 00:20:34,600
the force on your face

211
00:20:34,620 --> 00:20:36,080
would be doubled that

212
00:20:36,100 --> 00:20:39,110
would be ten newtons

213
00:20:39,220 --> 00:20:42,770
so we can now carry this further to our

214
00:20:42,870 --> 00:20:45,690
electromagnetic radiation

215
00:20:45,690 --> 00:20:49,450
and we can return to our poynting vector

216
00:20:49,500 --> 00:20:51,190
here i have

217
00:20:51,190 --> 00:20:55,020
one square metre

218
00:20:55,070 --> 00:20:58,420
radiation is coming in

219
00:20:58,430 --> 00:21:03,200
and i know exactly how much radiation is coming in for every square meter

220
00:21:03,240 --> 00:21:05,690
there is this value as

221
00:21:05,730 --> 00:21:07,380
so s

222
00:21:07,410 --> 00:21:11,750
which is an energy

223
00:21:11,850 --> 00:21:13,960
per square metre

224
00:21:13,990 --> 00:21:17,310
a second member that's

225
00:21:17,360 --> 00:21:21,870
but i mention of the poynting vector

226
00:21:21,910 --> 00:21:24,020
if i divided by c

227
00:21:24,080 --> 00:21:26,410
and i this by c

228
00:21:26,410 --> 00:21:27,950
you know what i have here

229
00:21:28,000 --> 00:21:30,770
i have energy divided by c

230
00:21:30,810 --> 00:21:34,250
the energy in electromagnetic radiation divided by c

231
00:21:34,310 --> 00:21:36,320
according to einstein

232
00:21:36,370 --> 00:21:37,870
is momentum

233
00:21:37,910 --> 00:21:42,230
and i don't care whether this is a hundred billions of individual photons

234
00:21:42,250 --> 00:21:43,940
that's okay with me

235
00:21:43,940 --> 00:21:46,380
that is the momentum

236
00:21:46,390 --> 00:21:50,180
and so i have no momentum

237
00:21:50,220 --> 00:21:51,660
unit time

238
00:21:51,660 --> 00:21:53,660
that makes it the fourth

239
00:21:53,710 --> 00:21:57,230
but the force per square metre makes it the pressure

240
00:21:57,250 --> 00:21:58,460
and so

241
00:21:58,500 --> 00:22:01,910
this is pressure

242
00:22:01,920 --> 00:22:03,660
and we call that

243
00:22:03,710 --> 00:22:05,930
radiation pressure

244
00:22:06,010 --> 00:22:07,980
this means that if you are

245
00:22:08,000 --> 00:22:11,180
exposed to bombardment of

246
00:22:11,180 --> 00:22:12,910
electromagnetic radiation

247
00:22:12,920 --> 00:22:14,420
and you absorb it

248
00:22:14,430 --> 00:22:16,100
that you feel the pressure

249
00:22:16,130 --> 00:22:19,310
relation comes to you and you being pushed backwards

250
00:22:19,370 --> 00:22:21,610
and this is the pressure

251
00:22:21,700 --> 00:22:28,610
that you will experience

252
00:22:28,620 --> 00:22:31,960
it makes a difference whether the radiation is absorbed

253
00:22:32,000 --> 00:22:36,890
or whether you're capable of of reflecting radiation just like receptor made of

254
00:22:36,930 --> 00:22:40,710
and so if i'm trying to be as general as i can

255
00:22:40,760 --> 00:22:42,140
let me

256
00:22:42,180 --> 00:22:43,000
first of all

257
00:22:43,020 --> 00:22:47,890
specify the mean value of as that's the only thing electromagnetic radiation with matter is

258
00:22:47,900 --> 00:22:50,010
the mean value

259
00:22:50,050 --> 00:22:52,480
so the mean value then

260
00:22:52,490 --> 00:22:55,190
if i divided by c

261
00:22:55,220 --> 00:22:59,110
and i multiplied by a factor of phi you'll come up short tell you of

262
00:22:59,250 --> 00:23:03,650
is that is now to radiation pressure

263
00:23:03,710 --> 00:23:05,410
and all five

264
00:23:05,460 --> 00:23:07,170
if alpha is one

265
00:23:07,180 --> 00:23:10,160
then i have fallen short

266
00:23:10,230 --> 00:23:13,260
all the radiation is absorbed

267
00:23:13,320 --> 00:23:18,020
if there's no absorption if it's completely transparent and there is some radiation that goes

268
00:23:18,020 --> 00:23:19,280
straight through you

269
00:23:19,300 --> 00:23:20,990
and also a zero

270
00:23:21,050 --> 00:23:27,050
but if there's hundred percent reflection

271
00:23:27,050 --> 00:23:29,980
sense to add levels to it is still

272
00:23:30,090 --> 00:23:35,340
we marginalize out the top of hierarchy are sold by some distribution at a particular

273
00:23:36,070 --> 00:23:39,460
so you know that you can make this go up

274
00:23:39,500 --> 00:23:43,990
infinitely long it's not really making

275
00:23:44,000 --> 00:23:47,460
but hierarchical principles often used define priors

276
00:23:47,460 --> 00:23:49,300
empirical priors

277
00:23:50,470 --> 00:23:55,720
essentially correspond to the idea that you want to learn some of the parameters of

278
00:23:55,720 --> 00:23:58,230
the prior from the data just because

279
00:23:58,270 --> 00:24:03,390
you know you're a bit nervous about setting up your whole prior entirely in japan

280
00:24:03,390 --> 00:24:07,570
and so a lot of people actually do that we call it empirical basis

281
00:24:08,770 --> 00:24:14,160
in some of the work that a lot of work people do few hyperparameters and

282
00:24:14,170 --> 00:24:15,840
you just optimize the

283
00:24:15,890 --> 00:24:20,290
fit the data and you cross your fingers in your that doesn't cost

284
00:24:20,310 --> 00:24:22,240
too much damage

285
00:24:23,520 --> 00:24:27,580
one thing that helps you with is that it makes things a bit more robust

286
00:24:27,580 --> 00:24:29,380
if data with something

287
00:24:29,410 --> 00:24:33,840
if you specify parameters are hyperparameters in your data is something that you really did

288
00:24:33,840 --> 00:24:35,140
not expect

289
00:24:35,190 --> 00:24:40,150
a new model might not perform very well whereas if you optimize the hyperparameters that

290
00:24:41,430 --> 00:24:46,360
you will make a bit more about at the cost of building a little bit

291
00:24:46,390 --> 00:24:49,220
and then finally subjective priors

292
00:24:49,270 --> 00:24:51,530
your prior to capture

293
00:24:51,540 --> 00:24:54,850
your beliefs as well as possible

294
00:24:54,900 --> 00:25:02,170
and it's important to to emphasise the priors these priors are subjective but not arbitrary

295
00:25:02,220 --> 00:25:03,530
it doesn't mean

296
00:25:03,550 --> 00:25:05,130
you know anything goes

297
00:25:06,270 --> 00:25:09,410
only when you actually believe it goes right

298
00:25:11,320 --> 00:25:14,450
you know if you if you don't actually capture things that you believe and then

299
00:25:14,450 --> 00:25:17,490
you're not being here in this

300
00:25:17,530 --> 00:25:21,400
OK and the dutch book theorem in this talk contains axioms

301
00:25:21,440 --> 00:25:24,380
i really about subjective priors

302
00:25:24,390 --> 00:25:25,920
i really about capturing

303
00:25:26,040 --> 00:25:31,590
police as possible but many people in statistics and machine learning just don't like the

304
00:25:31,590 --> 00:25:33,900
idea of thinking about all this stuff

305
00:25:33,950 --> 00:25:36,660
intersubjective framework

306
00:25:38,140 --> 00:25:42,720
so another distinction this sort of very important in the field

307
00:25:42,720 --> 00:25:44,730
is this idea that there are

308
00:25:44,750 --> 00:25:50,830
parametric models in nonparametric models let me try to waving and definition

309
00:25:51,680 --> 00:25:53,550
so parametric models

310
00:25:54,060 --> 00:25:57,090
you have a fixed number of parameters that

311
00:25:57,140 --> 00:26:01,830
finally the number of parameters data and regardless of the size of the dataset that

312
00:26:01,910 --> 00:26:04,180
sort of number of parameters that you have

313
00:26:04,230 --> 00:26:08,880
given data the predictions are going to be independent of the data so you parameters

314
00:26:08,880 --> 00:26:13,560
data are meant to capture everything about the data there is

315
00:26:13,580 --> 00:26:15,910
right or prediction

316
00:26:15,940 --> 00:26:20,770
so in the parameters are a finite summary of what the data is

317
00:26:20,830 --> 00:26:25,610
we can really had to call it the model based learning approach

318
00:26:26,290 --> 00:26:28,340
in nonparametric models

319
00:26:28,350 --> 00:26:33,150
in a sense you allow the number of parameters to grow with the data but

320
00:26:33,160 --> 00:26:37,140
in fact is usually there defined so the number of parameters was always kind of

321
00:26:37,140 --> 00:26:41,390
infinity and so the number of parameters that are getting used up

322
00:26:41,400 --> 00:26:45,680
grows the size the data but you know they're always sitting there

323
00:26:45,730 --> 00:26:51,650
alternatively we can think of the predictions as depending on the data and possibly usually

324
00:26:51,670 --> 00:26:57,500
small set of hyperparameters of we call that memory based learning and this is what

325
00:26:57,500 --> 00:27:03,780
nonparametric bayesian methods like gas processes and there's hypotheses which you hear about next are

326
00:27:03,780 --> 00:27:05,640
based on

327
00:27:07,270 --> 00:27:08,990
the argument is that

328
00:27:09,030 --> 00:27:12,930
we ought not to limit the complexity of the models that are a if we

329
00:27:12,930 --> 00:27:17,980
don't believe that the world the data in the real world was generated from a

330
00:27:17,980 --> 00:27:23,320
finite parametric model to the real world doesn't correspond to idealise physical models

331
00:27:23,330 --> 00:27:28,970
so we shouldn't say that the real world corresponds to a mixture of three gaussians

332
00:27:28,970 --> 00:27:31,590
that just doesn't seem to make sense

333
00:27:31,600 --> 00:27:35,180
so here are some common misconceptions about this is that

334
00:27:35,180 --> 00:27:38,080
if you choose the wrong prior you'll do poorly

335
00:27:38,910 --> 00:27:41,480
if you choose the daily prior you'll do poorly

336
00:27:41,480 --> 00:27:43,590
you don't need to to study this

337
00:27:44,270 --> 00:27:45,860
this thing is

338
00:27:45,880 --> 00:27:51,130
that we want to get close to get as close as possible to the unknown

339
00:27:51,130 --> 00:27:54,670
true covariance matrix so we want to minimize the norm

340
00:27:54,690 --> 00:27:56,130
of the hour

341
00:27:56,150 --> 00:27:58,790
estimation of the true covariance matrix

342
00:27:59,440 --> 00:28:00,460
so this

343
00:28:00,460 --> 00:28:05,580
looks like hopeless to calculate it because this sigma of the true covariance matrix is

344
00:28:07,500 --> 00:28:11,500
it turns out if we restrict the search space for this

345
00:28:13,440 --> 00:28:16,230
you can calculate optimum come in this

346
00:28:16,250 --> 00:28:19,060
calculation of the true covariance matrix

347
00:28:19,090 --> 00:28:20,750
kansas out

348
00:28:20,770 --> 00:28:24,590
the the optimal parameter gamma only depends on very

349
00:28:24,610 --> 00:28:27,000
simple value

350
00:28:27,040 --> 00:28:29,080
correlation coefficient

351
00:28:29,230 --> 00:28:34,670
the variance of correlation coefficients across tried

352
00:28:40,060 --> 00:28:42,040
after optima gamma

353
00:28:42,080 --> 00:28:43,360
for shrinkage

354
00:28:43,360 --> 00:28:48,580
and now if we use NDA restricted shows the covariance matrix we get

355
00:28:48,610 --> 00:28:53,880
drastically reduced the error rate for then only use four percent error

356
00:28:53,900 --> 00:28:55,040
so this

357
00:28:55,040 --> 00:28:56,820
you should decrease

358
00:28:56,880 --> 00:29:01,040
students range

359
00:29:01,150 --> 00:29:04,770
and in

360
00:29:04,790 --> 00:29:06,020
so in this

361
00:29:06,230 --> 00:29:08,150
that this works very well this

362
00:29:08,170 --> 00:29:11,520
not only for this particular dataset theory

363
00:29:11,540 --> 00:29:16,090
because you have some results for the classic matrix speller was setting

364
00:29:16,110 --> 00:29:18,000
participants who

365
00:29:19,540 --> 00:29:25,880
naive and so new to the paradigms not pre-selected and here's the classification accuracy for

366
00:29:26,900 --> 00:29:30,320
number of repetitions of the

367
00:29:32,540 --> 00:29:37,860
so it is accuracy in letter selection for the chance level three percent

368
00:29:37,880 --> 00:29:39,380
and you see that is only

369
00:29:39,440 --> 00:29:41,130
very few

370
00:29:41,150 --> 00:29:44,110
it intensification get very high

371
00:29:44,110 --> 00:29:46,560
classification accuracy so if we

372
00:29:46,580 --> 00:29:48,750
look at the mean across subjects

373
00:29:48,770 --> 00:29:52,020
you see that this for intensification

374
00:29:52,090 --> 00:29:54,860
the median across subjects percent

375
00:29:55,000 --> 00:29:59,520
so just this is simple techniques

376
00:29:59,880 --> 00:30:03,590
just introduced so if you want to see more details of the

377
00:30:03,630 --> 00:30:10,730
study see poster w seven hundred years later and other applications use this classification technique

378
00:30:10,920 --> 00:30:14,150
it to public settings and

379
00:30:14,170 --> 00:30:16,630
w ten mm on this

380
00:30:16,650 --> 00:30:24,060
so in summary this linear classification this trinket is very powerful

381
00:30:24,090 --> 00:30:26,860
the third and but of course

382
00:30:26,900 --> 00:30:29,960
linear methods enough depends on the

383
00:30:30,000 --> 00:30:32,630
kind of features extracted

384
00:30:32,650 --> 00:30:36,340
but if you can do it a linear methods and it's very nice because you

385
00:30:36,340 --> 00:30:39,440
can visualize what was known as agree

386
00:30:39,460 --> 00:30:45,090
so i suppose it's spatial temporal features for the classifier weight vector can be visualized

387
00:30:45,090 --> 00:30:47,270
as a sequence of

388
00:30:51,750 --> 00:30:54,320
so this not

389
00:30:54,340 --> 00:30:59,690
spatiotemporal features so no short section about spectral features

390
00:31:02,000 --> 00:31:04,290
so i was this was already

391
00:31:04,320 --> 00:31:06,400
introduce smallest by

392
00:31:06,400 --> 00:31:12,060
got be look either isn't over the sensorimotor area

393
00:31:12,060 --> 00:31:15,210
and then we see some some peaks in the spectrum

394
00:31:16,060 --> 00:31:19,460
and the rest for example you look at the electrodes

395
00:31:19,480 --> 00:31:21,460
fuses this peak

396
00:31:21,580 --> 00:31:23,170
the ideal result

397
00:31:23,190 --> 00:31:24,060
and if

398
00:31:24,190 --> 00:31:29,270
the movement is performed just dimensions and these peaks decreases

399
00:31:29,290 --> 00:31:33,360
local blocking of sensorimotor rhythm and

400
00:31:33,360 --> 00:31:35,500
this also called the

401
00:31:35,630 --> 00:31:39,230
gibbs then governments also

402
00:31:39,250 --> 00:31:43,170
i said that this is a very strong spatial smearing so here

403
00:31:43,170 --> 00:31:45,610
if calculated from one data

404
00:31:45,630 --> 00:31:51,320
correlation coefficient from the that to all other electrode and you see that

405
00:31:51,340 --> 00:31:53,270
correlation coefficient is

406
00:31:53,290 --> 00:31:58,020
almost point nine or about four nine four four electrodes

407
00:31:58,040 --> 00:31:59,860
so that's a very strong

408
00:31:59,860 --> 00:32:03,560
this page is smearing and that means if you look at the data you should

409
00:32:03,590 --> 00:32:05,150
first apply

410
00:32:05,150 --> 00:32:09,940
spatial it and this is especially true for the differences in

411
00:32:10,230 --> 00:32:12,820
spectral content

412
00:32:12,840 --> 00:32:16,480
we use the example it would illustrate the need

413
00:32:17,110 --> 00:32:21,150
this is for left hand motor imagery and try to motor imagery and we look

414
00:32:22,000 --> 00:32:25,860
electrode CP four and then we should

415
00:32:28,000 --> 00:32:29,040
decrease of

416
00:32:29,060 --> 00:32:31,540
vector energy for the left hand

417
00:32:32,230 --> 00:32:32,960
o train

418
00:32:33,020 --> 00:32:37,440
was the black and if you look at the ranch and then we don't see

419
00:32:37,440 --> 00:32:39,420
any difference at all

420
00:32:39,440 --> 00:32:42,900
to make a bipolar bipolar up

421
00:32:42,920 --> 00:32:46,520
if you have a little bit of common average references

422
00:32:46,540 --> 00:32:48,670
but only in the lab

423
00:32:48,690 --> 00:32:50,290
place in the

424
00:32:50,310 --> 00:32:52,650
you can see that the original features

425
00:32:52,650 --> 00:32:56,590
really composed of two peaks and in fact the lower peak is

426
00:32:56,770 --> 00:33:00,310
from visual for this has nothing to do with motor imagery

427
00:33:00,400 --> 00:33:02,090
and simulations

428
00:33:02,110 --> 00:33:03,480
happens only in the

429
00:33:03,480 --> 00:33:05,090
this offer p

430
00:33:07,380 --> 00:33:11,900
two to look at it you should always apply the best fit and

431
00:33:11,920 --> 00:33:16,690
then if you calculate CSP which introduced next which is the data driven method to

432
00:33:16,690 --> 00:33:21,150
something and not something else right this is the standard type of question we ask

433
00:33:21,760 --> 00:33:23,560
what's the posterior

434
00:33:23,610 --> 00:33:28,610
believe for distribution on my variable of interest or a set of variables given that

435
00:33:28,610 --> 00:33:29,410
i know

436
00:33:29,420 --> 00:33:33,130
this happens from this happened so these might be for example

437
00:33:33,150 --> 00:33:37,820
medical tests and this might be a disease

438
00:33:38,540 --> 00:33:42,040
it may be the other way this might be some sort of conditions that you

439
00:33:42,040 --> 00:33:46,490
have your asking what the probability of observing the symptoms

440
00:33:46,510 --> 00:33:53,080
so we can there is actually no back and forth in terms of actually

441
00:33:53,100 --> 00:33:55,830
show that you know

442
00:33:57,440 --> 00:34:01,690
it's nice to have the full joint distributions because we can ask all kinds of

443
00:34:01,690 --> 00:34:04,660
questions but of course we never get beyond

444
00:34:04,780 --> 00:34:06,610
five six ten

445
00:34:06,630 --> 00:34:13,640
we can really do anything that there are two big reasons discrete variables even binary

446
00:34:13,650 --> 00:34:15,340
we have variables we have

447
00:34:15,360 --> 00:34:17,450
two the ten possible

448
00:34:18,390 --> 00:34:19,870
we need to do ten

449
00:34:19,880 --> 00:34:21,880
minus one parameters

450
00:34:21,890 --> 00:34:23,020
minus one

451
00:34:23,040 --> 00:34:24,800
because this into one

452
00:34:26,220 --> 00:34:30,010
we can deal with these joints as is

453
00:34:30,460 --> 00:34:34,360
one with the objects we have to do something to

454
00:34:34,370 --> 00:34:42,120
reduce complexity and allow us to two to get the benefit of the jones without

455
00:34:42,130 --> 00:34:44,480
actually them explicitly

456
00:34:44,590 --> 00:34:50,060
there's a little graphical models two and there are named after these two guys

457
00:34:51,870 --> 00:34:59,000
so there's two kinds directed the bayes an undirected markov networks and other names for

458
00:34:59,000 --> 00:35:02,010
these things so these called belief nets

459
00:35:03,120 --> 00:35:05,070
directed graphical models

460
00:35:05,090 --> 00:35:08,690
these are called mrfs markov random field markov nets

461
00:35:08,700 --> 00:35:11,010
undirected graphical models

462
00:35:11,050 --> 00:35:18,050
often sometimes called log linear models or we signed in the tree i talked about

463
00:35:18,870 --> 00:35:21,670
when you cannot represent the log x

464
00:35:21,690 --> 00:35:24,580
something that's a log linear model

465
00:35:24,600 --> 00:35:30,900
and so these guys in the event but they had a lot to do with

466
00:35:31,260 --> 00:35:32,560
the mathematics of

467
00:35:32,710 --> 00:35:36,450
probability and many poems and conditional independence assumption

468
00:35:36,500 --> 00:35:41,290
this again if you can see that this is something that is

469
00:35:41,310 --> 00:35:42,980
you can see here

470
00:35:45,950 --> 00:35:52,040
it is

471
00:35:54,220 --> 00:35:55,790
it's just that are also

472
00:35:55,880 --> 00:36:00,170
judea pearl is the guy i think if we have to pick one guy who

473
00:36:00,170 --> 00:36:01,700
is responsible for

474
00:36:01,810 --> 00:36:05,460
making graphical models popular in machine learning

475
00:36:05,480 --> 00:36:08,840
the same year euro this kind of seminal book

476
00:36:08,850 --> 00:36:11,400
in the late eighties

477
00:36:11,420 --> 00:36:15,490
eventually turned the tide from more

478
00:36:15,500 --> 00:36:17,970
in logical approaches to AI

479
00:36:17,990 --> 00:36:22,860
two probabilistic and one of the major tools was was graphical models and so he

480
00:36:22,860 --> 00:36:27,820
did not like the theory for actually the semantics of graphical models you know what

481
00:36:28,210 --> 00:36:32,740
was the graph mean and what kind of axioms can cancer

482
00:36:32,760 --> 00:36:35,140
getting at the semantics

483
00:36:35,160 --> 00:36:40,290
it is a lot of work lately on causality using these kinds of models to

484
00:36:41,710 --> 00:36:49,020
think about causal kinds of events right so but you know the government has been

485
00:36:49,020 --> 00:36:53,290
around for awhile geneticists and statisticians use them in

486
00:36:53,330 --> 00:36:55,100
the nineteen thirties and

487
00:36:55,220 --> 00:36:59,740
vision research people

488
00:36:59,760 --> 00:37:03,900
used an even earlier than the pearl so in

489
00:37:03,910 --> 00:37:07,270
it's late seventies but really serves

490
00:37:07,290 --> 00:37:11,390
the momentum after today

491
00:37:11,480 --> 00:37:13,280
today really pushed hard for

492
00:37:14,640 --> 00:37:19,630
anyway so what they do the allows to play with you know who they are

493
00:37:19,710 --> 00:37:25,880
the joint and compute degrees of interest by representing them

494
00:37:25,900 --> 00:37:30,610
not as one big table but as sort of local

495
00:37:30,630 --> 00:37:31,860
easy to specify

496
00:37:32,890 --> 00:37:38,370
so what we do is we sort of variables that interact with directly and we

497
00:37:38,370 --> 00:37:44,310
specify how they interact and these local interactions if everything is set up properly will

498
00:37:44,310 --> 00:37:46,540
give rise to global interactions

499
00:37:46,570 --> 00:37:50,620
and cohen global distribution

500
00:37:50,630 --> 00:37:55,560
and that that satisfies our assumptions

501
00:37:57,110 --> 00:38:02,710
this one is of course an upper right so this is a model and if

502
00:38:02,720 --> 00:38:04,000
you can read about

503
00:38:04,010 --> 00:38:05,910
it's a model of

504
00:38:05,920 --> 00:38:07,760
car in our model

505
00:38:07,780 --> 00:38:14,810
where you alternate fan belt engine when car starts spark plugs it's about it and

506
00:38:14,810 --> 00:38:16,680
variables here

507
00:38:18,630 --> 00:38:22,400
any model of course is is wrong all models are wrong is is the question

508
00:38:22,400 --> 00:38:23,480
is whether they

509
00:38:23,520 --> 00:38:26,740
they capture enough for you to do something

510
00:38:27,720 --> 00:38:29,110
useful with right

511
00:38:29,120 --> 00:38:31,800
so we might not have every variable that we care about

512
00:38:31,810 --> 00:38:36,360
we not account for all the different possible interactions between variables that we have

513
00:38:36,400 --> 00:38:40,870
right there might be other constraints are we're not thinking about

514
00:38:40,890 --> 00:38:45,050
but the but when the goal is to capture interactions that are the most direct

515
00:38:45,050 --> 00:38:48,720
and are sort of our are not you know

516
00:38:48,730 --> 00:38:51,570
mediated by the any other interactions

517
00:38:51,620 --> 00:38:57,110
OK so to get into the more concretely and so given these models is going

518
00:38:57,110 --> 00:38:58,320
to show them but

519
00:38:58,330 --> 00:39:04,100
the wireless let me and maybe later i'll try to log in again this there's

520
00:39:04,100 --> 00:39:06,810
a job that allows you to to play with this

521
00:39:07,560 --> 00:39:08,910
model and other models

522
00:39:08,920 --> 00:39:11,580
so you know one thing we can do it for example

523
00:39:11,650 --> 00:39:17,630
say that you know the car doesn't start so start simple false and ask you

524
00:39:17,630 --> 00:39:21,100
know what's the probability that a battery each there

525
00:39:21,150 --> 00:39:22,020
it is

526
00:39:22,220 --> 00:39:26,350
all right so

527
00:39:26,360 --> 00:39:28,610
so actually the other end

528
00:39:28,890 --> 00:39:34,800
so this explanation so we have some some some some tape recorders and start many

529
00:39:34,800 --> 00:39:36,050
radio is dead

530
00:39:36,070 --> 00:39:41,300
and then we're asking given those two things with probability the battery which is

531
00:39:42,580 --> 00:39:45,740
OK so that's reasoning upward

532
00:39:45,790 --> 00:39:52,010
in this kind of if you think about directed direct causal reasoning prediction is kind

533
00:39:52,010 --> 00:39:54,780
it's because

534
00:39:54,840 --> 00:39:58,050
i tried to use it for

535
00:39:59,580 --> 00:40:01,040
so good morning everyone

536
00:40:01,050 --> 00:40:03,390
i'm going to present the group paper

537
00:40:03,390 --> 00:40:07,860
this is joint work with my advisor fairly

538
00:40:07,870 --> 00:40:12,060
given the human and object we want to recognise the interaction

539
00:40:12,370 --> 00:40:14,310
such as playing saxophone

540
00:40:14,330 --> 00:40:20,030
however this is not a trivial task because human object interaction cannot be fully understood

541
00:40:20,030 --> 00:40:25,100
by simply considering the co occurrence of this human and the object

542
00:40:25,110 --> 00:40:27,610
in this case it is possible that

543
00:40:27,630 --> 00:40:31,960
the human is holding the saxophone but not putting it

544
00:40:31,990 --> 00:40:36,350
recognizing human object interaction has a wide range of applications

545
00:40:36,360 --> 00:40:38,780
such as it sort

546
00:40:38,790 --> 00:40:41,750
such as teaching always hard to interact with objects

547
00:40:41,780 --> 00:40:43,770
automatic sports commentary

548
00:40:43,920 --> 00:40:48,130
two analysis for medical care and so on

549
00:40:48,140 --> 00:40:54,390
higher while recognition of both humans and generic object has been extensively studied

550
00:40:54,420 --> 00:40:59,120
the analysis of human object interaction has been mostly north

551
00:40:59,130 --> 00:41:04,840
although there are some works that use image context to improve the recognition performance

552
00:41:04,870 --> 00:41:07,680
the focus of this work is still

553
00:41:07,700 --> 00:41:12,950
human object recognition instead of the interaction between them

554
00:41:12,960 --> 00:41:14,040
in this talk

555
00:41:14,060 --> 00:41:18,010
let's look into human object interaction in detail

556
00:41:18,030 --> 00:41:19,710
this is all line

557
00:41:19,730 --> 00:41:21,620
firstly the intuition

558
00:41:21,620 --> 00:41:25,960
then we introduce the group the feature representation in the heart we could use it

559
00:41:25,960 --> 00:41:27,150
for recognition

560
00:41:27,180 --> 00:41:31,560
and then we show some experiment results and conclusions

561
00:41:31,570 --> 00:41:34,490
OK let's see our intuition

562
00:41:34,510 --> 00:41:38,950
recognizing human object interaction is a very challenging problem

563
00:41:38,980 --> 00:41:42,670
given this image about the human is playing saxophone

564
00:41:42,680 --> 00:41:46,340
the same interaction can be observed with different human pose

565
00:41:46,340 --> 00:41:49,010
lighting in the background

566
00:41:49,100 --> 00:41:53,390
different interactions such as playing bassoon always have humans

567
00:41:53,400 --> 00:41:57,150
you know very similar pose as playing saxophone

568
00:41:57,180 --> 00:42:03,390
also humans might interact very differently we start same object such as holding the saxophone

569
00:42:03,390 --> 00:42:05,750
but not playing it

570
00:42:05,850 --> 00:42:12,230
and given this is a very challenging problem is visual representations have some limitations

571
00:42:12,250 --> 00:42:17,800
for example given two images when humans playing and another playing saxophone

572
00:42:17,850 --> 00:42:23,550
bag of words or even a spatial pyramid which is very similar feature representation

573
00:42:23,570 --> 00:42:28,410
and the part based methods such as constellation model is also to cause

574
00:42:28,420 --> 00:42:31,850
to tell the difference between the two images

575
00:42:31,940 --> 00:42:36,700
the reason is that from the pixel labels that we images are very similar and

576
00:42:36,700 --> 00:42:39,100
only have some subtle difference

577
00:42:39,760 --> 00:42:46,700
to address this problem in this work we propose a novel feature representation called grouplet

578
00:42:46,720 --> 00:42:49,040
it is up on based feature

579
00:42:49,050 --> 00:42:52,160
and it explicitly considers the co occurrence

580
00:42:52,190 --> 00:42:54,520
of different image parts

581
00:42:54,540 --> 00:42:56,770
it is also a discriminative

582
00:42:56,790 --> 00:42:57,950
and the tenth

583
00:42:57,970 --> 00:43:03,130
feature representation so it is able to capture the subtle difference

584
00:43:03,190 --> 00:43:07,010
between different human object interactions

585
00:43:07,040 --> 00:43:11,600
let's look into the group the feature representation in detail

586
00:43:11,610 --> 00:43:14,260
given the image and the reference point

587
00:43:14,290 --> 00:43:20,500
the group considers the co occurrence of a set of highly related image patches

588
00:43:20,510 --> 00:43:23,230
the group here contains to image edges

589
00:43:23,260 --> 00:43:26,950
so we call it two grouplet

590
00:43:27,450 --> 00:43:31,050
the those image patches are encoded by feature units

591
00:43:31,070 --> 00:43:37,000
and each feature unit considers the specific visual appearance and image location information

592
00:43:37,010 --> 00:43:42,770
and if the appearance is encoded by visual codeword and the location is represented by

593
00:43:42,770 --> 00:43:45,540
a two d gaussian distributions

594
00:43:45,580 --> 00:43:48,300
and in the case of human object interactions

595
00:43:48,300 --> 00:43:53,750
fifty thousand words chosen a small so the hundred features usually anti-spam system

596
00:43:53,760 --> 00:44:00,060
let's see in the bayesian logistic regression implicit claim descent you get twenty percent test

597
00:44:00,060 --> 00:44:02,530
error which is which is unacceptably high

598
00:44:02,680 --> 00:44:05,670
so this is bayesian logistic regression

599
00:44:05,680 --> 00:44:11,990
this is just as the likelihood that with that additional long descriptive term and when

600
00:44:11,990 --> 00:44:18,030
maximizing rather minimizing was so so this miners lambda this was the first one is

601
00:44:19,830 --> 00:44:23,630
the question is on the semantic web page images aggression

602
00:44:23,750 --> 00:44:27,970
and you test on a test set on the set the high error so what

603
00:44:27,970 --> 00:44:28,920
you do next

604
00:44:32,580 --> 00:44:36,090
you know one thing could do is think about the ways you can improve this

605
00:44:36,090 --> 00:44:40,410
our this very what most people do said well let's sit down and think what

606
00:44:40,410 --> 00:44:43,790
could gone wrong and try to improve the article

607
00:44:43,800 --> 00:44:46,320
well obviously have more training data

608
00:44:46,330 --> 00:44:49,320
helps only just try to get more training examples

609
00:44:50,840 --> 00:44:54,830
maybe you suspect even one hundred features was too many seem like try to

610
00:44:54,830 --> 00:44:56,810
get smallest the features

611
00:44:56,810 --> 00:45:00,340
on the more common as you might suspect if you just like in enough the

612
00:45:00,340 --> 00:45:06,150
west byzantine the email headers see figure out better features for new finding spam emails

613
00:45:06,150 --> 00:45:07,400
or whatever

614
00:45:07,510 --> 00:45:14,430
all right and rights in just sit around the of the features as the even

615
00:45:16,690 --> 00:45:22,170
you also suspect that been descent having quite converge let's must five hundred descended belong

616
00:45:22,170 --> 00:45:24,990
to see that was clearly that can was

617
00:45:25,000 --> 00:45:31,120
so i'm going to set longer maybe remember you know you are hearing loss that

618
00:45:31,120 --> 00:45:34,200
may be newton's method converges bellies let's try that the

619
00:45:34,220 --> 00:45:37,410
so you want your value for lambda

620
00:45:37,470 --> 00:45:41,270
not sure that was right on or maybe even want to try and as you

621
00:45:41,560 --> 00:45:42,370
can media

622
00:45:42,420 --> 00:45:45,060
o thing and as you might work better which is actually

623
00:45:46,410 --> 00:45:50,620
i mean this eight things you can imagine if you actually sitting down building machine

624
00:45:50,620 --> 00:45:53,990
learning system options you and you can think of

625
00:45:53,990 --> 00:45:59,010
hundreds of ways to improve the learning system elements these things like we're getting which

626
00:45:59,010 --> 00:46:03,200
an example surely that's the whole so so that seems like a good user talk

627
00:46:04,470 --> 00:46:07,150
and it turns out that

628
00:46:07,180 --> 00:46:11,560
this approach in ways to improve the learning algorithm picking one thing going for it

629
00:46:11,900 --> 00:46:16,000
on my work in the sense that it may eventually get used to working system

630
00:46:16,400 --> 00:46:21,680
but often very time consuming and i think is often largely washed that to block

631
00:46:21,680 --> 00:46:23,880
what end fixing with the problems

632
00:46:23,900 --> 00:46:25,200
in particular

633
00:46:25,220 --> 00:46:28,820
he's eight improvements all fixed very different problems

634
00:46:28,900 --> 00:46:33,600
and some of them will be fixing problems the don't have on and if you

635
00:46:33,600 --> 00:46:35,140
can rule out

636
00:46:35,150 --> 00:46:40,260
six of eight these saying if i somehow looking at the problem or be you

637
00:46:40,260 --> 00:46:43,800
can figure out which one of these things is actually the right thing to do

638
00:46:43,810 --> 00:46:46,590
you can save yourself a lot of time

639
00:46:46,610 --> 00:46:49,050
so let's see how to go about doing that

640
00:46:52,110 --> 00:46:57,670
the people in industry and research that that i see very good will not

641
00:46:57,680 --> 00:47:00,550
going child change learning algorithm and

642
00:47:00,550 --> 00:47:05,560
lots of things that will put obviously improve learning algorithm but the process so many

643
00:47:05,560 --> 00:47:08,300
of them is hard to know what to do

644
00:47:08,320 --> 00:47:12,540
so people i know the really good ones that run various diagnostics to figure out

645
00:47:12,540 --> 00:47:13,850
what the problem is

646
00:47:13,870 --> 00:47:15,790
i think whether the problem

647
00:47:19,130 --> 00:47:23,740
for almost story right we said this is basically just a regression test error was

648
00:47:23,740 --> 00:47:27,880
twenty percent which was the theme on high on

649
00:47:27,900 --> 00:47:32,120
let's suppose you suspect that the problem is either overfitting

650
00:47:32,310 --> 00:47:36,740
so high bias or you suspect that you may be too few features across five

651
00:47:36,740 --> 00:47:39,860
spans on was his i think i

652
00:47:41,160 --> 00:47:42,560
wrote that's wrong

653
00:47:42,570 --> 00:47:48,000
this person has this forget get details services policy the high by the high variance

654
00:47:48,010 --> 00:47:50,160
in some of the text cosmic

655
00:47:50,180 --> 00:47:52,190
you want to know

656
00:47:52,200 --> 00:47:57,310
if all overfitting which should be high variance or get too few features features cos

657
00:47:57,310 --> 00:48:00,640
i spent high bias to sorry

658
00:48:02,070 --> 00:48:04,670
how figure out whether the problem

659
00:48:04,740 --> 00:48:09,080
is one of high by is or high variance

660
00:48:09,090 --> 00:48:14,790
so a simple diagnostic if you look at all tell you

661
00:48:14,830 --> 00:48:17,830
whether the problem is high bias and high variance

662
00:48:19,270 --> 00:48:24,480
if you remember the car to its increasing high variance problems when have high variance

663
00:48:24,500 --> 00:48:28,910
the training error will be much lower than the test error

664
00:48:28,920 --> 00:48:33,330
and a high variance problem that's when you fitting the training set very well that's

665
00:48:33,330 --> 00:48:37,160
when fitting ten for the polynomial to eleven data points

666
00:48:37,200 --> 00:48:41,610
that's when you just fitting that a very well the training of much more than

667
00:48:43,060 --> 00:48:45,750
and in contrast high by is

668
00:48:45,800 --> 00:48:48,410
that's when your training error will also be high

669
00:48:48,470 --> 00:48:50,830
that's when your data is quadratic say

670
00:48:50,870 --> 00:48:55,390
but fitting a linear function to it so you want to the training

671
00:48:56,520 --> 00:49:02,470
just in cartoons i guess this is that this is one of the typical learning

672
00:49:02,470 --> 00:49:07,890
curve for high variance looks like on on the horizontal axis imparting the training set

673
00:49:07,890 --> 00:49:09,050
size m

674
00:49:09,100 --> 00:49:12,050
and the graphs in some part the error

675
00:49:12,070 --> 00:49:13,220
and so

676
00:49:14,530 --> 00:49:16,930
let's see you know as increase

677
00:49:16,950 --> 00:49:21,680
if you have high variance problem of nodes as the training set size m increases

678
00:49:22,030 --> 00:49:25,050
you test error with people in decreasing

679
00:49:25,070 --> 00:49:29,660
so this suggest that what we can increase the training set size even further maybe

680
00:49:29,660 --> 00:49:36,360
believe that period

681
00:50:48,580 --> 00:50:50,970
you just

682
00:50:50,990 --> 00:50:54,840
that's all

683
00:50:57,970 --> 00:51:03,970
all right

684
00:51:08,960 --> 00:51:13,310
o who are

685
00:51:19,140 --> 00:51:26,200
and the more

686
00:52:35,390 --> 00:52:42,460
you are

687
00:53:03,180 --> 00:53:06,660
and if you want

688
00:53:24,810 --> 00:53:30,530
and he said

689
00:53:30,530 --> 00:53:34,990
at the distance from source to reflect to the destination

690
00:53:35,030 --> 00:53:38,910
and that gives us this propagation to like

691
00:53:38,970 --> 00:53:41,550
OK these are very with time

692
00:53:41,550 --> 00:53:46,910
but in our retracing approximation we assume that are independent of frequency

693
00:53:46,970 --> 00:53:50,720
i originally assumed that the antenna pattern was a function of frequency

694
00:53:50,760 --> 00:53:53,450
we don't want to say anything about that

695
00:53:53,470 --> 00:53:56,950
so we have a total of capital j pairs

696
00:53:56,970 --> 00:54:01,990
we put in input of cosine two pi after a what's going come out

697
00:54:01,990 --> 00:54:04,760
is an electromagnetic radiation

698
00:54:04,780 --> 00:54:07,050
which is the sum

699
00:54:07,100 --> 00:54:09,850
of these different attenuation factors

700
00:54:09,870 --> 00:54:12,390
times e to the two pi IFT t

701
00:54:13,780 --> 00:54:16,660
this propagation to lie

702
00:54:18,350 --> 00:54:23,120
so everything you can do with the ray tracing is included in this formula

703
00:54:23,140 --> 00:54:26,870
what you might be able to do in wireless system

704
00:54:26,950 --> 00:54:31,490
as you might by looking at the received waveform and knowing things about the transmitted

705
00:54:31,490 --> 00:54:36,950
waveform you might be able to figure out what these attenuation factors are and what

706
00:54:36,950 --> 00:54:40,740
these propagation delay factors are

707
00:54:40,780 --> 00:54:43,990
just like when we try to do frequency recovery

708
00:54:44,010 --> 00:54:47,780
we can find out what the transmitted frequency was we can do that we can

709
00:54:47,780 --> 00:54:52,050
play the same sorts of games here but the harder and we'll talk about that

710
00:54:53,410 --> 00:54:56,890
if you ever hear the term rake receiver

711
00:54:56,910 --> 00:55:01,280
rake receivers receiver that in fact measures all this stuff

712
00:55:01,300 --> 00:55:07,620
in response to what i will talk about that probably next monday

713
00:55:07,640 --> 00:55:11,330
OK if we want to look at the reflecting waters is an example of what

714
00:55:11,330 --> 00:55:13,410
this formula means

715
00:55:13,490 --> 00:55:17,450
beta one of two a mainly for the direct path

716
00:55:17,470 --> 00:55:19,330
we have an attenuation

717
00:55:19,350 --> 00:55:25,260
which is the magnitude of the antenna patterns divided by or zero plus b

718
00:55:25,300 --> 00:55:30,640
for the attenuation on the on the return path from the wall

719
00:55:31,550 --> 00:55:32,930
the same alpha

720
00:55:32,950 --> 00:55:36,120
why is it the same output because we assume that with the same outfit to

721
00:55:36,120 --> 00:55:42,780
make things simple for ourselves divided by two d minus or zero minus the today

722
00:55:42,820 --> 00:55:44,830
if you look at the

723
00:55:44,830 --> 00:55:47,450
that these propagation delay terms

724
00:55:47,470 --> 00:55:49,850
the propagation delay terms

725
00:55:50,490 --> 00:55:52,390
or zero plus the today

726
00:55:52,410 --> 00:55:56,930
divided by saying this gives us doppler shift the were interested in here

727
00:55:56,950 --> 00:56:00,120
we're also going to have an extra term here

728
00:56:00,140 --> 00:56:05,990
which is really caused by the phase change at the transmitting antenna in the phase

729
00:56:05,990 --> 00:56:10,890
change the receiving antenna and interface stranger to reflect their there's any there

730
00:56:10,910 --> 00:56:15,140
so we have the same sort of term in both of these places

731
00:56:15,180 --> 00:56:20,600
the reason but i talk about that if you look at the electromagnetic wave that

732
00:56:20,600 --> 00:56:25,870
you receive for this reflecting what problem we talked about a good deal

733
00:56:26,080 --> 00:56:32,430
the second term is there with the negative sign rather than a plus sign here

734
00:56:32,430 --> 00:56:35,410
everything is put in with plus signs

735
00:56:35,430 --> 00:56:39,470
you can create negative signs by phase changes

736
00:56:39,510 --> 00:56:40,680
of pi

737
00:56:40,700 --> 00:56:44,120
so the assumption is we put a phase change pi

738
00:56:44,120 --> 00:56:46,720
as part of this term here

739
00:56:47,970 --> 00:56:52,740
all these terms can be expressed in this general form here

740
00:56:52,970 --> 00:57:01,910
OK we said before you only have two choices with the cellular system

741
00:57:02,080 --> 00:57:03,740
you cannot

742
00:57:03,760 --> 00:57:07,830
solve the electromagnetic field problems at the self

743
00:57:07,930 --> 00:57:11,330
the person using the cell phone is not going to do with the cell phone

744
00:57:11,330 --> 00:57:14,700
is not going to do with the base station is not going to do it

745
00:57:14,700 --> 00:57:21,410
and you're not going to store all those changes because these radiations change remarkably within

746
00:57:21,410 --> 00:57:27,180
the period a small fraction of one meter and you have the coverage here

747
00:57:27,200 --> 00:57:29,780
which in fact

748
00:57:29,830 --> 00:57:32,600
it is at least area coverage

749
00:57:32,640 --> 00:57:35,970
it's a one kilometre times one kilometre

750
00:57:35,990 --> 00:57:39,300
and then the reflectors are going to be moving also

751
00:57:39,350 --> 00:57:43,580
so you can't deal with them very easily either so hope this problem to try

752
00:57:43,580 --> 00:57:48,140
to solve electromagnetic problem and stored someplace

753
00:57:50,030 --> 00:57:54,780
electromagnetism helps us to limit the range and the likelihood of choices

754
00:57:54,800 --> 00:57:57,490
but it doesn't help that fall detection

755
00:57:57,490 --> 00:58:01,350
so we're not going to deal with the kind of thing that we just talked

756
00:58:02,430 --> 00:58:05,680
which is the sort of a general expression

757
00:58:05,740 --> 00:58:07,530
four electric field

758
00:58:07,700 --> 00:58:12,720
in terms of of attenuation factors and phase changes

759
00:58:12,720 --> 00:58:14,060
as opposed to

760
00:58:15,330 --> 00:58:16,280
which is

761
00:58:16,300 --> 00:58:18,890
which is much more detail

762
00:58:18,910 --> 00:58:24,620
we're going to find channel system functions

763
00:58:24,850 --> 00:58:27,560
as just this sum

764
00:58:27,620 --> 00:58:33,660
of these attenuation terms times phase change terms OK the reason that we're doing this

765
00:58:33,660 --> 00:58:39,470
is that if we put in an input eighty two pi i have to say

766
00:58:39,490 --> 00:58:42,080
and then what

767
00:58:42,100 --> 00:58:43,660
and what we get

768
00:58:43,700 --> 00:58:46,100
is the system function here

769
00:58:46,120 --> 00:58:50,060
times access to a which is a to buy i have to say so we

770
00:58:50,060 --> 00:58:51,910
get this quantity here

771
00:58:52,220 --> 00:58:57,740
here's the minus two pi i f tell

772
00:58:57,740 --> 00:59:00,490
that term coming down there

773
00:59:00,530 --> 00:59:02,390
and here is the

774
00:59:02,390 --> 00:59:07,050
eta two pi i to coming down here so

775
00:59:07,100 --> 00:59:10,160
all of

776
00:59:10,180 --> 00:59:11,410
this term

777
00:59:11,430 --> 00:59:14,100
and this term are both included

778
00:59:14,120 --> 00:59:17,260
in this system response term

779
00:59:18,600 --> 00:59:23,870
OK this is linear also so we know what the responses to an exponential we

780
00:59:23,870 --> 00:59:24,830
put in

781
00:59:24,890 --> 00:59:30,350
mkt a little bit here by going from bi going from real

782
00:59:30,370 --> 00:59:32,720
by going from real to complex

783
00:59:32,820 --> 00:59:35,470
and they do that a little more carefully

784
00:59:37,740 --> 00:59:40,640
anyway we ought to be used to that now

785
00:59:40,680 --> 00:59:42,930
if i put in input

786
00:59:42,950 --> 00:59:48,390
x had that either the two pi ftbs and integrated in other words if i

787
00:59:48,390 --> 00:59:49,850
the common the normal thing to do

788
00:59:51,000 --> 00:59:57,900
so it's constructed from these two things that the the marginal distribution for the parameter is the prior

789
00:59:58,220 --> 01:00:03,000
the conditional distribution for the data given the parameter is the likelihood if we're modeling in this

790
01:00:03,000 --> 01:00:08,190
direction that's a generative model and then multiply it to give the joint distribution

791
01:00:14,820 --> 01:00:18,780
it is worth I'll just repeat that point that it's the joint distribution of theta and Y

792
01:00:18,780 --> 01:00:22,720
that's really the key idea and in fact if you started from that point you don't even need

793
01:00:22,720 --> 01:00:25,690
bayes theorem to do bayesian statistics

794
01:00:25,690 --> 01:00:28,100
which is a slightly scary thought

795
01:00:28,800 --> 01:00:33,720
now ) gonna talk quite a bit about priors later where do they come from but

796
01:00:33,740 --> 01:00:40,020
bearing in mind what I've said about two sorts of uncertainty typically

797
01:00:40,080 --> 01:00:45,970
those two factors the likelihood and the prior can be quite different in character

798
01:00:46,190 --> 01:00:51,830
the prior is often entirely subjective now subjective is a bit of a dirty word

799
01:00:51,830 --> 01:00:55,210
I I don't really like it because it has that pejorative

800
01:00:55,370 --> 01:01:00,000
connotation this is something somehow you just made up it'd be much better if we said

801
01:01:00,000 --> 01:01:05,970
scientific judgment I think it's much better way to think about subject subjectivity

802
01:01:06,690 --> 01:01:12,320
but the prior is very often based on on scientific judgment that's not hard you know isn't

803
01:01:12,320 --> 01:01:14,430
quantified in a particularly hard way

804
01:01:14,890 --> 01:01:20,350
whereas the likelihood may well be something as open to empirical evaluation

805
01:01:20,530 --> 01:01:26,060
you know in a lab you could perhaps create data and actually assess assess distributions directly

806
01:01:30,190 --> 01:01:34,370
now there are many many positive things that come out of using a probability theory

807
01:01:34,370 --> 01:01:38,300
consistently and this is just a very simple example of that and it's something

808
01:01:38,300 --> 01:01:42,890
we use all the time what happens when data acquire sequentially

809
01:01:43,000 --> 01:01:45,240
OK so every day you get and you

810
01:01:45,520 --> 01:01:52,020
every day you get a new set of weather and you know we we we we

811
01:01:52,020 --> 01:01:55,130
we want to update believes in the light of

812
01:01:55,210 --> 01:02:01,190
what we now know and so imagine there's this some sort of state of nature theta

813
01:02:03,640 --> 01:02:09,540
is our focus of our inference and that the different data we acquire are conditionally independent

814
01:02:09,540 --> 01:02:11,720
given that that that fact

815
01:02:11,780 --> 01:02:18,350
In that case the the joint distribution is just the product of the prior for theta

816
01:02:18,370 --> 01:02:21,670
multiplied by the likelihoods for all the individual

817
01:02:21,670 --> 01:02:25,020
data and that's

818
01:02:25,040 --> 01:02:30,450
you know that's the basic law of probability so we can infer from that that the conditional distribution

819
01:02:30,450 --> 01:02:33,910
we want is proportional for that product

820
01:02:33,970 --> 01:02:38,300
and notice that we can break off the last factor and we see that in fact what we're seeing is

821
01:02:38,310 --> 01:02:38,780
that the

822
01:02:39,080 --> 01:02:44,760
the the the the posterior after N data is is the product or proportional to the product

823
01:02:44,770 --> 01:02:49,820
of the posterior after N minus one data and the likelihood for the

824
01:02:49,870 --> 01:02:51,690
for the last observation

825
01:02:52,110 --> 01:02:55,870
so if you like the the the prior for the

826
01:02:55,870 --> 01:02:58,370
the last data point is the posterior

827
01:02:58,470 --> 01:02:59,870
from the step before

828
01:02:59,950 --> 01:03:02,890
very nice sequential

829
01:03:03,010 --> 01:03:08,540
sequential coherence and consistency of the distribution as you learn more stuff

830
01:03:09,430 --> 01:03:14,220
OK so I put I put prior and likelihood on the on the table for many people

831
01:03:14,230 --> 01:03:18,690
that's where we start but I was starting at slightly different place

832
01:03:18,720 --> 01:03:23,110
so is there anything else apart from prior and likelihood and and very importantly there is yes yes there

833
01:03:23,120 --> 01:03:27,130
is and it's not always a center stage

834
01:03:27,130 --> 01:03:31,410
and this concerns issues about utility and loss

835
01:03:34,540 --> 01:03:41,410
often we don't need to think very hard about that

836
01:03:41,410 --> 01:03:44,120
if you're contend

837
01:03:44,160 --> 01:03:49,980
you know to display or visualize a posterior distribution and that's it if that's the focus of your inference

838
01:03:49,980 --> 01:03:53,580
than don't need to think about anything else but if you wanted to make take

839
01:03:53,580 --> 01:03:58,480
decisions or test hypotheses if you wanted to do a serious job of estimation even

840
01:03:58,480 --> 01:04:02,450
you have to think about something else and that's that leads us into areas of

841
01:04:02,450 --> 01:04:04,120
decision theory

842
01:04:04,250 --> 01:04:08,970
so decision theory is a big subject a bit drier I would say actually but I just need

843
01:04:08,970 --> 01:04:10,140
a few key ideas from that

844
01:04:11,910 --> 01:04:15,850
there's a theory of utility which says you know what do we how do we

845
01:04:15,850 --> 01:04:22,270
evaluate uncertainty it tries typically it tries to evaluate the the cost to us

846
01:04:22,270 --> 01:04:29,730
as individuals of being uncertain in malatory terms so we talk about utility on a pseudo money scale

847
01:04:30,830 --> 01:04:35,590
but let's try and just formalize this process in a minute and we'll see we'll see some

848
01:04:35,590 --> 01:04:40,550
practical consequences in a second what's the sequence we observe data why we gonna make

849
01:04:40,550 --> 01:04:43,170
a decision on that data based on that data

850
01:04:43,180 --> 01:04:46,100
so if it's based on that data than our decision is a function of the

851
01:04:47,250 --> 01:04:51,560
so the decision is delta of Y and then we pay a price

852
01:04:51,730 --> 01:04:55,660
and the price we pay is a function of the decision we took on whatever the true

853
01:04:55,660 --> 01:04:57,230
state of nature was

854
01:04:57,230 --> 01:05:01,450
so that's what the loss function is it's a function that the decision you took and the true state

855
01:05:01,450 --> 01:05:02,540
of nature

856
01:05:02,750 --> 01:05:06,000
so it allows us to ask the question how bad is it to

857
01:05:08,150 --> 01:05:09,850
decide delta Y

858
01:05:09,850 --> 01:05:12,080
when theta is true

859
01:05:12,450 --> 01:05:16,450
and the idea we can use then is to try and minimize our losses

860
01:05:16,480 --> 01:05:20,080
and that gives us a means of deciding things

861
01:05:20,250 --> 01:05:24,870
well let's make this concrete the simplest possible thing you could do is decide between the

862
01:05:24,950 --> 01:05:26,060
the two

863
01:05:26,120 --> 01:05:28,060
hypotheses about a parameter

864
01:05:29,080 --> 01:05:35,580
suppose they're just two I've written it as a omega zero and omega one so it's theta and omega zero

865
01:05:35,580 --> 01:05:36,750
or it's theta and omega one

866
01:05:37,790 --> 01:05:43,630
we're gonna use data to decide which of those things is true so we can we can take a decision

867
01:05:43,630 --> 01:05:45,390
D zero

868
01:05:45,430 --> 01:05:46,410
or D one

869
01:05:46,470 --> 01:05:48,770
D zero is I think the theta

870
01:05:48,810 --> 01:05:49,230
is omega zero

871
01:05:51,520 --> 01:05:53,830
Ok so it's just a true or false situation

872
01:05:53,830 --> 01:05:58,180
I think it would be simpler if you get a right answer fine

873
01:05:58,200 --> 01:06:01,200
OK everyone's happy so there's no loss

874
01:06:01,850 --> 01:06:05,910
if you make a mistake then there's a loss and the losses might be

875
01:06:05,910 --> 01:06:11,470
different in the two cases so if you decide D I I zero one

876
01:06:11,480 --> 01:06:17,680
if you decide D I and you're wrong than the price you pay is A I A I is a positive number

877
01:06:19,060 --> 01:06:26,020
OK so that's the setup and we now observe why would what do you do or or or what's the what's the

878
01:06:26,060 --> 01:06:30,120
what's the decision you should take well if you knew theta of course there's no problem

879
01:06:30,120 --> 01:06:35,560
but you don't know theta you only know Y and a natural a natural thing to do

880
01:06:35,560 --> 01:06:40,450
and indeed is correct according to a sort of a meta theory

881
01:06:40,640 --> 01:06:45,910
axiomatic theory of utility is to minimize your expected loss

882
01:06:45,910 --> 01:06:51,850
So notice you minimize L of D theta is your loss and we

883
01:06:51,890 --> 01:06:55,790
integrate that with the spector posterior distribution but you don't know theta but you do have the

884
01:06:55,790 --> 01:07:01,700
posterior distribution that's the expected loss and you do the best you can and you can

885
01:07:01,700 --> 01:07:07,160
that work out very quickly the the the expected loss if you choose D I is A I times

886
01:07:07,160 --> 01:07:09,100
the probability that you're wrong

887
01:07:09,120 --> 01:07:11,580
and that's the posterior probability that you're wrong

888
01:07:12,310 --> 01:07:16,250
and so this amounts to essentially yes

889
01:07:24,020 --> 01:07:25,580
but you don't

890
01:07:38,160 --> 01:07:41,980
why can you just use the expectation well I think the simplest way to think about is just

891
01:07:41,980 --> 01:07:44,890
as a long run thing you know so

892
01:07:44,890 --> 01:07:49,250
this is not the only decision you gonna take in your life you gonna take lots let's suppose

893
01:07:49,250 --> 01:07:53,270
you know when you're a teenager you learn how to take decisions

894
01:07:53,270 --> 01:07:57,310
and you stick to that process but this is this is the law

895
01:07:57,310 --> 01:08:04,470
and pedro domingos from the university of washington and this is the on practical statistical

896
01:08:04,470 --> 01:08:06,000
relational learning

897
01:08:06,020 --> 01:08:08,300
i will start with

898
01:08:08,850 --> 01:08:15,810
make point to be usable which might be possible can you see this

899
01:08:18,700 --> 01:08:21,850
well i guess i have to do with that appointed them

900
01:08:23,840 --> 01:08:25,490
if anybody has a nice

901
01:08:25,510 --> 01:08:26,190
you know

902
01:08:26,200 --> 01:08:31,510
thick brightly visible pointer that i can of the grateful in the mean time i'll

903
01:08:31,510 --> 01:08:34,210
just what with what have so

904
01:08:34,220 --> 01:08:36,500
i will begin with some motivation

905
01:08:36,520 --> 01:08:40,690
and then the tutorial has three main parts

906
01:08:40,740 --> 01:08:41,830
the first part

907
01:08:42,650 --> 01:08:45,370
well you can think of is the core foundational areas

908
01:08:45,410 --> 01:08:49,220
all statistical relational learning mainly probabilistic inference

909
01:08:49,230 --> 01:08:52,080
statistical learning logical inference

910
01:08:52,090 --> 01:08:54,340
and inductive logic programming

911
01:08:54,360 --> 01:08:56,500
i will then

912
01:08:56,520 --> 01:08:59,630
let's see how this works

913
01:08:59,650 --> 01:09:01,090
this is better

914
01:09:01,110 --> 01:09:08,150
i will talk about how we put this all together to build statistical relational learning

915
01:09:08,150 --> 01:09:10,000
is that the second part

916
01:09:10,050 --> 01:09:11,930
you know

917
01:09:11,980 --> 01:09:15,160
this actually part is probably going to be long this because once we know the

918
01:09:15,160 --> 01:09:19,380
pieces putting them together is actually surprisingly maybe not that hard

919
01:09:19,400 --> 01:09:24,330
and finally i will spend a good chunk of time talking about various applications things

920
01:09:24,330 --> 01:09:28,120
that you can do with statistical relational learning today

921
01:09:28,130 --> 01:09:31,010
so let me begin with the motivation

922
01:09:31,030 --> 01:09:34,220
most machine learning algorithms assume

923
01:09:34,230 --> 01:09:36,320
that the data is i i d

924
01:09:36,340 --> 01:09:41,510
meaning that you objects are all independent of each other and you know they're all

925
01:09:41,510 --> 01:09:42,940
the same height

926
01:09:43,240 --> 01:09:45,510
they all have the same kind of distribution

927
01:09:45,530 --> 01:09:48,580
and of course this is very unrealistic but in the real world objects have all

928
01:09:48,580 --> 01:09:53,560
kinds of relationships between the properties of one object affects the properties of another object

929
01:09:53,560 --> 01:09:56,780
and you know we we would like to be able to model them

930
01:09:56,800 --> 01:10:00,610
we'll that we want to be able to model the means of multiple types of

931
01:10:00,610 --> 01:10:05,070
objects we want to be able to model relations between objects the dependencies of object

932
01:10:05,070 --> 01:10:09,820
attributes on the relations between the dependencies of relations on each other attributes and so

933
01:10:13,040 --> 01:10:19,400
i think it's pretty much enormous i think every domain with where we have always

934
01:10:19,400 --> 01:10:23,330
been assuming i did it i think it's the case that if you look closer

935
01:10:23,620 --> 01:10:25,950
there is not i i d

936
01:10:26,030 --> 01:10:30,440
here's is a random sampling of the mines web search is the famous example

937
01:10:30,490 --> 01:10:33,370
if you use links between pages you can get better

938
01:10:33,390 --> 01:10:35,490
page rankings than if you don't

939
01:10:35,500 --> 01:10:40,330
information extraction is another one if you do joint inference between the various things that

940
01:10:40,330 --> 01:10:42,810
you want expect various tests you better

941
01:10:42,820 --> 01:10:47,600
natural language processing is another example of the same kind more general perception

942
01:10:47,610 --> 01:10:49,710
in vision in speech

943
01:10:49,720 --> 01:10:53,410
it's true you have dependencies between the various objects that you're seeing

944
01:10:53,420 --> 01:10:58,260
medical diagnosis is another interesting example right the UCI datasets large number of them is

945
01:10:58,940 --> 01:11:02,920
medical diagnosis problems and of course the first thing you assume that all the patients

946
01:11:02,920 --> 01:11:07,820
are independent while in reality not because you know academics happen and if somebody has

947
01:11:07,820 --> 01:11:10,970
the flu you know the next person is more likely to have the flu and

948
01:11:10,970 --> 01:11:15,880
so forth another interesting examples computational biology

949
01:11:15,890 --> 01:11:19,020
it's one thing to model you know individual problems like you know is this the

950
01:11:19,020 --> 01:11:23,100
promoter region not what we really want to get into these believe is to build

951
01:11:23,100 --> 01:11:27,470
models of how the entire so works and that involves a lot of dependencies among

952
01:11:27,480 --> 01:11:33,570
you know proteins genes et social networks and other obvious example your preferences your habits

953
01:11:33,570 --> 01:11:36,930
the things you do are influenced by your friends your co-workers et cetera

954
01:11:36,940 --> 01:11:40,670
well we could computing is another important example sensor networks

955
01:11:40,810 --> 01:11:45,200
are very hot topic these days the readings that you get one sensor

956
01:11:45,440 --> 01:11:49,550
are you know not independent of the things that you get new sentences for example

957
01:11:49,680 --> 01:11:51,720
and many many more examples

958
01:11:51,900 --> 01:11:55,550
you know i could spend the rest of the afternoon just going over domains with

959
01:11:55,900 --> 01:11:59,040
modelling dependencies between objects is important

960
01:12:00,010 --> 01:12:04,780
what sort of statistical relational learning is really trying to do this trying to model

961
01:12:06,040 --> 01:12:12,120
domains where the objects are not independent because tribute the benefits of doing that well

962
01:12:12,930 --> 01:12:16,450
potential benefit is that we get better predictive accuracy

963
01:12:16,490 --> 01:12:20,410
because if there are dependencies between objects and we ignore the with trying out some

964
01:12:20,410 --> 01:12:24,640
of the information that we could use for prediction so that's one important potential benefit

965
01:12:24,730 --> 01:12:27,700
and that one is better understanding of domains

966
01:12:27,720 --> 01:12:31,290
if we if we ignoring all of the dependencies or the phenomena that are going

967
01:12:31,290 --> 01:12:35,370
on in the main probably won't understand very well as if we do

968
01:12:35,390 --> 01:12:41,100
and also more generally i think SRL offers growth path for machine learning from small

969
01:12:41,100 --> 01:12:46,870
isolated problems through solving the large big massive problems that real intelligence systems have to

970
01:12:46,870 --> 01:12:47,630
deal with

971
01:12:47,690 --> 01:12:50,200
i think we can grow progressively more ambitious

972
01:12:50,220 --> 01:12:51,640
in this way

973
01:12:51,690 --> 01:12:54,240
i took you to offset the

974
01:12:54,250 --> 01:12:56,870
benefits unfortunately there are also some costs

975
01:12:56,880 --> 01:13:01,250
one crosses that learning is not much much harder

976
01:13:01,270 --> 01:13:04,080
if you are dependencies between objects you know the

977
01:13:04,200 --> 01:13:08,270
the number of parameters of the model you have to potentially go up exponentially

978
01:13:08,280 --> 01:13:12,060
and this is going up exponential person was already exponentially large so

979
01:13:12,080 --> 01:13:15,630
you know building on eighty models can be really really hard problem

980
01:13:15,640 --> 01:13:18,270
another another issue is that

981
01:13:18,290 --> 01:13:21,610
you know when you're just doing something like living a classifier you can ignore the

982
01:13:21,610 --> 01:13:25,190
problem of inference because you know inferences just computing function

983
01:13:25,990 --> 01:13:29,510
you really getting inference any more inference is really crucial

984
01:13:29,520 --> 01:13:34,070
because you know no no different classes for example different articles each other you need

985
01:13:34,070 --> 01:13:39,040
to figure out how to compute the and indeed in a good chunk of

986
01:13:39,590 --> 01:13:42,520
this tutorial is going to be talking about

987
01:13:42,530 --> 01:13:47,020
the inference methods that are relevant for for statistical relational learning

988
01:13:47,060 --> 01:13:50,110
here's what is perhaps the biggest problems

989
01:13:50,120 --> 01:13:53,800
the biggest problem is that it's much more complex for the user

990
01:13:53,820 --> 01:13:56,180
two model one i i d domains

991
01:13:56,200 --> 01:13:59,370
this is probably the biggest thing that is holding us back and white people have

992
01:13:59,370 --> 01:14:03,830
always tended to assume i did there is that is just so much easier the

993
01:14:03,840 --> 01:14:06,520
number of options they have to contend with the number of things that you have

994
01:14:06,520 --> 01:14:10,100
to model the number decisions that you have to make you know all of this

995
01:14:11,010 --> 01:14:13,620
when you have a non IID data

996
01:14:13,640 --> 01:14:18,140
so those are the downside what i hope to persuade you today is that thanks

997
01:14:18,140 --> 01:14:19,610
to recent progress

998
01:14:19,620 --> 01:14:20,300
the the

999
01:14:20,310 --> 01:14:25,610
and if it's not the cost and necessarily something that you can do effectively in

1000
01:14:25,610 --> 01:14:30,700
practice today without a lot of investment on your part is the researcher or the

1001
01:14:32,180 --> 01:14:33,530
so our goal

1002
01:14:33,550 --> 01:14:35,250
stated in one sentence

1003
01:14:35,270 --> 01:14:38,580
is is to enable learning from non IID data

1004
01:14:38,620 --> 01:14:41,780
as easily as from i data

1005
01:14:41,790 --> 01:14:45,350
we would like to learning from inevitably to be is easier something like the playing

1006
01:14:45,350 --> 01:14:48,560
c four point five was cent classification problem

1007
01:14:48,570 --> 01:14:53,170
now they can never completely which disco because after all the problems are complex but

1008
01:14:53,170 --> 01:14:57,670
i think that you know we have a fairly close to the

1009
01:14:57,690 --> 01:15:01,640
there's been a lot of research on this problem in this year's lot progress

1010
01:15:01,700 --> 01:15:03,970
and i think we are at the point where we are close enough to the

1011
01:15:03,970 --> 01:15:09,230
goal that you know you can just give statistical analysis to these people and they

1012
01:15:09,230 --> 01:15:13,310
will be useful things with them and the that's experience and so the goal of

1013
01:15:13,370 --> 01:15:17,460
is to try to unite share this this knowledge

1014
01:15:17,480 --> 01:15:19,420
we have these techniques

1015
01:15:20,100 --> 01:15:24,350
that was developed in the field with all of

1016
01:15:24,370 --> 01:15:28,690
and will also point out that this is also symmetry in in the field work

1017
01:15:28,760 --> 01:15:32,540
there is easy to use open source software to do these things

1018
01:15:32,560 --> 01:15:35,450
as long as to do as i really need to go into your system and

1019
01:15:35,450 --> 01:15:38,130
we add a slope to y

1020
01:15:38,140 --> 01:15:42,760
with this slope is obtained by differentiating the corresponding component

1021
01:15:42,760 --> 01:15:45,800
at the solution to the corresponding solution

1022
01:15:45,840 --> 01:15:49,550
similarly for the

1023
01:15:49,600 --> 01:15:50,750
for the

1024
01:15:51,010 --> 01:15:53,800
in this is the in the linear i'd in this is

1025
01:15:53,810 --> 01:15:59,310
we just revert again we differentiation but we revert to reverse the order of differentiation

1026
01:15:59,310 --> 01:16:03,510
we differentiate the conjugate instead of the original

1027
01:16:03,560 --> 01:16:06,220
of course the subgradient duality

1028
01:16:06,240 --> 01:16:09,190
so you can reverse this universe that

1029
01:16:09,280 --> 01:16:11,700
if that's convenient algorithmically

1030
01:16:11,720 --> 01:16:24,870
and that's leaf

1031
01:16:27,070 --> 01:16:29,790
if you have both green and blue there no

1032
01:16:29,820 --> 01:16:31,100
there's is no

1033
01:16:32,070 --> 01:16:36,540
the there is no fixed there is no specified direction up if you have only

1034
01:16:36,540 --> 01:16:41,480
blue approximations and approximation below on the green approximations from above but if you have

1035
01:16:41,480 --> 01:16:43,670
a mixture there's nothing you can say

1036
01:16:43,960 --> 01:16:50,340
some problems for some problems it's easier to do but for some components of the

1037
01:16:50,340 --> 01:16:54,580
cost function it's easier to do in their out of the station

1038
01:16:54,730 --> 01:16:59,220
for example the simplest in cutting plane methods you may have a cost function

1039
01:16:59,290 --> 01:17:01,340
you become a constraints

1040
01:17:01,360 --> 01:17:03,620
and it may be easier to

1041
01:17:03,670 --> 01:17:09,390
just after the linear arise the cost function and in the linear the constraints k

1042
01:17:09,390 --> 01:17:13,170
may just be more convenient to do some extra flexibility

1043
01:17:13,220 --> 01:17:17,140
that you may or may not be able to use but it's there

1044
01:17:17,200 --> 01:17:22,870
and makes the brother

1045
01:17:22,920 --> 01:17:25,600
OK now let me explain the enlargement

1046
01:17:25,650 --> 01:17:27,620
the allotments step

1047
01:17:27,690 --> 01:17:30,390
remember we have this approximation

1048
01:17:30,470 --> 01:17:31,670
in we solve

1049
01:17:32,350 --> 01:17:36,480
and we find an extract by approximation

1050
01:17:37,320 --> 01:17:39,350
for every

1051
01:17:39,370 --> 01:17:40,470
out there

1052
01:17:40,470 --> 01:17:44,160
approximated function we want to add a new slow

1053
01:17:44,220 --> 01:17:47,740
so we have the corresponding component x

1054
01:17:47,760 --> 01:17:50,820
he is the corresponding slope wildcats

1055
01:17:50,870 --> 01:17:54,930
and then we go up here and we pick up a sub gradient of this

1056
01:17:55,950 --> 01:18:00,390
and it's not the flat subgradient OK it's just something that happens to be at

1057
01:18:00,390 --> 01:18:01,340
this point

1058
01:18:01,340 --> 01:18:02,650
and we have this

1059
01:18:02,660 --> 01:18:05,820
this two slopes

1060
01:18:05,830 --> 01:18:09,260
for the inner functions different kind of process

1061
01:18:09,310 --> 01:18:11,060
we find had

1062
01:18:11,070 --> 01:18:13,890
and we have the slope y

1063
01:18:13,910 --> 01:18:17,280
and these solve the approximated problem

1064
01:18:17,280 --> 01:18:18,290
and now

1065
01:18:18,300 --> 01:18:21,340
we find a new point x tilde

1066
01:18:21,360 --> 01:18:27,270
such that because the subgradient because the slope equal to one

1067
01:18:29,010 --> 01:18:37,300
and then you form this approximation of functions and then fit again and so on

1068
01:18:37,340 --> 01:18:42,870
it's reminiscent of simplicial decomposition if you can remember that picture

1069
01:18:43,060 --> 01:18:50,310
now let's talk about the duality here

1070
01:18:50,370 --> 01:18:52,010
we have this

1071
01:18:52,020 --> 01:18:54,330
approximation of the original problem

1072
01:18:54,390 --> 01:18:58,260
if we do realize that this is any MP because the dual and the dual

1073
01:18:58,260 --> 01:19:01,490
involves the conjugate of this f this blue

1074
01:19:01,490 --> 01:19:03,370
after this green

1075
01:19:03,420 --> 01:19:06,620
and the dual is denoted by h

1076
01:19:06,630 --> 01:19:10,020
the do of f i

1077
01:19:10,750 --> 01:19:16,330
h i but now becomes the green function because remember the conjugate of

1078
01:19:16,340 --> 01:19:18,080
over and out there

1079
01:19:18,120 --> 01:19:19,400
is in

1080
01:19:19,440 --> 01:19:21,330
the nb the

1081
01:19:21,330 --> 01:19:25,820
and similarly here the conjugate of a green becomes the blue

1082
01:19:27,080 --> 01:19:32,060
you can solve either this or that or solve them simultaneously whatever it's convenient for

1083
01:19:32,060 --> 01:19:34,840
you for a given problem it may be an algorithm

1084
01:19:35,010 --> 01:19:39,470
there may be an album that favors just solving this or that favors just solving

1085
01:19:40,090 --> 01:19:45,060
or an algorithm that solves both simultaneously

1086
01:19:45,100 --> 01:19:49,280
so in the dual the roles of in outer linear decision have been reserved the

1087
01:19:50,680 --> 01:19:54,670
and the choice of primary goal is just a matter of computational convenience does not

1088
01:19:54,670 --> 01:19:58,740
affect the output can be mathematically the two are the same

1089
01:19:58,810 --> 01:20:02,500
the primal and dual sequence was used exactly the same no matter which one you

1090
01:20:06,560 --> 01:20:10,050
so regarding the solution of the approximation

1091
01:20:11,090 --> 01:20:14,810
in some cases you may use an algorithm that simultaneously so the prime on the

1092
01:20:15,880 --> 01:20:17,680
in one traffic programming

1093
01:20:17,700 --> 01:20:19,650
almost all the algorithms

1094
01:20:19,660 --> 01:20:24,590
so some of them to the primary do network flow problems and that's certainly the

1095
01:20:27,840 --> 01:20:29,590
and then

1096
01:20:29,620 --> 01:20:34,620
you would have simultaneously at the end of hats and white hats

1097
01:20:34,670 --> 01:20:38,630
in other problems it may be preferable to focus on the solution of either the

1098
01:20:38,630 --> 01:20:40,430
primal or the dual

1099
01:20:40,480 --> 01:20:42,660
and then after solving the primal

1100
01:20:42,690 --> 01:20:48,770
you can define the approximation by differentiation you can go into the enlargements that differentiate

1101
01:20:48,770 --> 01:20:51,930
and this may be using it is easy in the cutting plane method you just

1102
01:20:52,980 --> 01:20:58,310
subgradient at the pattern many point but it may be non trivial which happens in

1103
01:20:58,310 --> 01:21:02,530
the simplest decomposition method we have sort a substantial problems

1104
01:21:02,580 --> 01:21:03,890
to find

1105
01:21:03,970 --> 01:21:08,450
to find the and not be allowed to be quite frank point and a new

1106
01:21:08,450 --> 01:21:15,070
extreme points defining extreme means solving a linear programme is not simplest differentiation

1107
01:21:15,080 --> 01:21:19,120
however you have the subgradient duality that may be useful

1108
01:21:19,120 --> 01:21:20,460
i can just jump

1109
01:21:20,480 --> 01:21:23,690
to the hospital so hyper media b

1110
01:21:24,720 --> 01:21:28,970
that's going so i can go back

1111
01:21:28,970 --> 01:21:31,640
some scaling back to

1112
01:21:31,680 --> 01:21:33,790
simple solution

1113
01:21:40,510 --> 01:21:44,430
one of

1114
01:21:46,430 --> 01:21:51,790
precision is the possibility for satisfiability for four seasons

1115
01:21:51,800 --> 01:21:53,340
that should be

1116
01:21:53,340 --> 01:21:58,970
the asian if you don't application you can get a contradiction can essentially

1117
01:21:59,000 --> 01:22:00,150
so you see

1118
01:22:00,310 --> 01:22:03,760
so in order to

1119
01:22:06,050 --> 01:22:13,380
in an essentially trivially satisfied are totally want to get rid of cattle

1120
01:22:14,470 --> 01:22:15,470
that's right

1121
01:22:15,560 --> 01:22:18,090
people tend not reliable

1122
01:22:18,090 --> 01:22:21,860
it's impossible anything in our or

1123
01:22:28,040 --> 01:22:31,470
well this is

1124
01:22:34,540 --> 01:22:38,800
here is an

1125
01:22:39,760 --> 01:22:43,800
because it's a very

1126
01:22:43,830 --> 01:22:44,960
right now

1127
01:22:44,970 --> 01:22:49,170
you're very little so so it's actually quite far

1128
01:22:50,680 --> 01:22:54,720
this is very

1129
01:22:56,720 --> 01:23:00,580
but really

1130
01:23:00,590 --> 01:23:02,860
one consequence

1131
01:23:02,890 --> 01:23:04,890
there's not

1132
01:23:04,920 --> 01:23:06,360
not has nine children

1133
01:23:06,620 --> 01:23:16,260
so we're not the only reason you

1134
01:23:16,300 --> 01:23:17,550
a search

1135
01:23:17,560 --> 01:23:23,050
ontology is is probably these are all almost all children of the pages they have

1136
01:23:23,050 --> 01:23:27,210
all sorts of whatsoever an easy way to test

1137
01:23:27,260 --> 01:23:28,680
is too

1138
01:23:34,580 --> 01:23:36,250
four nine

1139
01:23:36,260 --> 01:23:40,330
this is big enough radiation make the this everybody read in the back

1140
01:23:44,360 --> 01:23:47,620
who was me

1141
01:23:47,640 --> 01:23:50,420
would you like to think that's fine

1142
01:23:50,430 --> 01:23:56,330
i understand the words make it bigger don't understand

1143
01:23:56,330 --> 01:23:59,300
do good enough for OK

1144
01:24:02,350 --> 01:24:07,800
so you can see here we call

1145
01:24:07,810 --> 01:24:09,460
changes were

1146
01:24:11,310 --> 01:24:14,300
because have explicitly said

1147
01:24:14,300 --> 01:24:16,430
and they moved

1148
01:24:16,440 --> 01:24:23,340
right and that's the way it means that your definition

1149
01:24:23,350 --> 01:24:28,010
the world

1150
01:24:28,020 --> 01:24:29,810
it was

1151
01:24:29,920 --> 01:24:32,400
here's was

1152
01:24:43,500 --> 01:24:44,840
well the

1153
01:24:45,120 --> 01:24:47,380
from all

1154
01:24:47,390 --> 01:24:49,980
we know

1155
01:24:50,000 --> 01:24:52,080
you've plausibility to make

1156
01:24:52,090 --> 01:24:54,350
one more

1157
01:24:54,380 --> 01:25:00,550
strong support this by looking at what that means in terms of what what it

1158
01:25:00,550 --> 01:25:01,730
takes to get

1159
01:25:01,760 --> 01:25:02,970
we should get

1160
01:25:03,010 --> 01:25:06,050
some consolation that that's the goal

1161
01:25:07,010 --> 01:25:11,150
this is the name of

1162
01:25:11,180 --> 01:25:17,080
just follow the treatment but it's also possible to get more fine grained

1163
01:25:17,090 --> 01:25:20,470
the more detailed view of the structure of the ontology

1164
01:25:20,510 --> 01:25:23,960
for that reason visualizations

1165
01:25:23,980 --> 01:25:27,050
so this is still a very strong association

1166
01:25:27,210 --> 01:25:29,380
so she

1167
01:25:32,500 --> 01:25:35,380
in the case of nations since we're not

1168
01:25:35,390 --> 01:25:37,550
the reason this is just a certain

1169
01:25:39,090 --> 01:25:42,400
i can see a lot of

1170
01:25:43,460 --> 01:25:49,890
it means that you don't to explicitly asserted so this is just a

1171
01:25:54,500 --> 01:25:55,420
and the that

1172
01:25:56,750 --> 01:25:58,840
as of two possible ways

1173
01:25:58,850 --> 01:26:01,210
well it's possible

1174
01:26:10,680 --> 01:26:14,120
the idea is

1175
01:26:14,350 --> 01:26:16,750
in two thousand four

1176
01:26:16,810 --> 01:26:20,060
relation between all the nations

1177
01:26:20,220 --> 01:26:23,840
for example

1178
01:26:23,860 --> 01:26:29,840
the rear face and this is actually pretty pretty unusual

1179
01:26:29,890 --> 01:26:31,390
so i just to

1180
01:26:31,400 --> 01:26:34,620
do a little bit about visualizations

1181
01:26:34,630 --> 01:26:40,040
i'm not even remotely following the size and scope do pretty organically because

1182
01:26:40,130 --> 01:26:45,140
so last

1183
01:26:46,760 --> 01:26:48,500
he adores

1184
01:26:49,970 --> 01:26:54,360
you got

1185
01:26:54,390 --> 01:26:56,210
the name

1186
01:26:56,230 --> 01:26:57,790
so we go to advance

1187
01:26:57,800 --> 01:26:59,720
and you can fly mothership

1188
01:27:01,880 --> 01:27:04,080
this is of

1189
01:27:08,760 --> 01:27:14,470
this is probably more sensible but well so it is

1190
01:27:14,550 --> 01:27:17,840
it is an advanced features is an experimental feature

1191
01:27:18,180 --> 01:27:21,340
i should say that if you're interested in visualizations

1192
01:27:21,520 --> 01:27:29,010
for ontologies that shape and this is where telling myself pay for these

1193
01:27:29,050 --> 01:27:31,250
this association was

1194
01:27:34,010 --> 01:27:35,680
messy and so on

1195
01:27:41,500 --> 01:27:45,730
so we get a sense of the the overall structure of the ontology

1196
01:27:45,760 --> 01:27:50,340
nesting again this atrocity select something it will show up over there

1197
01:27:50,360 --> 01:27:56,100
you can drill down also the immediate children so animal which is this class here

1198
01:27:56,100 --> 01:27:57,960
we have for

1199
01:27:57,960 --> 01:27:59,170
also told that

1200
01:27:59,180 --> 01:28:02,760
the numbers of children which can be useful

1201
01:28:02,760 --> 01:28:05,420
if you select the children will

1202
01:28:05,430 --> 01:28:08,300
it will go through and you can and you can zoom up and down to

1203
01:28:08,800 --> 01:28:10,380
do what i call

1204
01:28:10,440 --> 01:28:12,710
the christmas tree lights

1205
01:28:12,720 --> 01:28:16,480
which i is kind just soothing and relaxing to be able to go back to

1206
01:28:16,480 --> 01:28:19,960
work but is also find it useful first rapidly scanning with the challenge of this

1207
01:28:19,960 --> 01:28:22,430
is equivalent to the energy industry

1208
01:28:23,340 --> 01:28:24,760
collapsing everything

1209
01:28:24,770 --> 01:28:27,390
only do that actually

1210
01:28:28,260 --> 01:28:31,210
it's actually really big controversy

1211
01:28:31,220 --> 01:28:34,210
what kind of visualization

1212
01:28:34,210 --> 01:28:37,780
systems in parallel across the part of speech

1213
01:28:37,900 --> 01:28:42,610
and his and in f one score for all of us all of task

1214
01:28:42,610 --> 01:28:46,280
and you can see that the performance is like you know

1215
01:28:46,300 --> 01:28:47,220
quite fair

1216
01:28:48,650 --> 01:28:49,590
and then

1217
01:28:49,590 --> 01:28:52,010
come on guys

1218
01:28:52,010 --> 01:28:52,920
i mean

1219
01:28:52,940 --> 01:28:58,420
when use the whole sentence rather than the words performance that still sucks

1220
01:28:58,490 --> 01:29:03,440
wasting my time i mean so you know i don't but the good results at

1221
01:29:03,440 --> 01:29:05,340
the beginning of the book so

1222
01:29:05,380 --> 01:29:08,590
actually says afterwards

1223
01:29:08,610 --> 01:29:12,110
i mean you're you're quite right it kind of sucks

1224
01:29:12,130 --> 01:29:15,440
even so we are like in you know the ballpark of

1225
01:29:15,470 --> 01:29:19,320
four years later with processing is sitting on the planet

1226
01:29:19,380 --> 01:29:23,470
but you might wonder why he sucks

1227
01:29:23,490 --> 01:29:28,280
actually if you think about it well the capacity of like all this work is

1228
01:29:28,280 --> 01:29:33,380
mainly in in the world for children right so the first question we we have

1229
01:29:33,380 --> 01:29:36,360
in mind is we training this one which is right

1230
01:29:38,820 --> 01:29:41,050
she was in is

1231
01:29:41,050 --> 01:29:44,380
because you know sometimes the cat sat on the mat

1232
01:29:44,400 --> 01:29:47,400
it should have the same kind of play than the feline sat on the mat

1233
01:29:47,570 --> 01:29:49,550
on the mat even sort of felines

1234
01:29:49,570 --> 01:29:50,670
you know it is

1235
01:29:50,690 --> 01:29:53,530
compared to cut

1236
01:29:53,550 --> 01:29:57,990
maybe it doesn't even exist actually in order in july

1237
01:30:00,650 --> 01:30:02,450
the ones on things

1238
01:30:02,470 --> 01:30:06,170
we're close to each other gap on filling close to each other in the what

1239
01:30:06,420 --> 01:30:11,670
is then it would be really great because they contradiction by continuity the tax which

1240
01:30:11,670 --> 01:30:13,240
would be the same right

1241
01:30:13,260 --> 01:30:17,360
so here i look like some random words in the dictionary

1242
01:30:17,360 --> 01:30:20,710
and as you have like

1243
01:30:20,720 --> 01:30:27,420
the range in frequency of the y so small smaller number means the world is

1244
01:30:27,420 --> 01:30:31,150
quite frequent in the last moments is quite well

1245
01:30:31,240 --> 01:30:38,010
and reported that encloses wall according to the euclidean distance of from the top one

1246
01:30:38,010 --> 01:30:41,570
hundred and see well at least according to the euclidean distance

1247
01:30:41,590 --> 01:30:44,400
and would seem to get anything

1248
01:30:44,420 --> 01:30:50,050
it's strictly not surprising because in fact in this region is about one million of

1249
01:30:50,300 --> 01:30:56,440
world's fifteen percent of the most frequent what IP like ninety percent of the time

1250
01:30:56,510 --> 01:31:01,240
so many what only like once or twice on ten times

1251
01:31:01,260 --> 01:31:05,450
so you cannot expect to train a fifty dimensional vector always like you know what

1252
01:31:05,450 --> 01:31:09,090
would happen after a few times

1253
01:31:09,110 --> 01:31:13,590
but fortunately we have like you know a lot of unlabelled data available on here

1254
01:31:13,590 --> 01:31:20,050
you just take the web and been infinite so in this part we are

1255
01:31:21,240 --> 01:31:22,950
two liver the

1256
01:31:24,110 --> 01:31:26,110
labelled data

1257
01:31:26,260 --> 01:31:31,010
in our system

1258
01:31:31,030 --> 01:31:32,360
if we have the books

1259
01:31:32,380 --> 01:31:34,570
i mean the black box

1260
01:31:34,950 --> 01:31:39,720
which was able to answer to the question sentences actually english or not

1261
01:31:39,740 --> 01:31:42,780
it would be really great because it it wouldn't like books

1262
01:31:42,860 --> 01:31:47,720
is able to capture you know the syntax and semantics of grammar which

1263
01:31:47,720 --> 01:31:55,380
so actually such a mean some people tried to of black books like you should

1264
01:31:55,380 --> 01:31:58,880
have been the resolution in two thousand

1265
01:31:59,490 --> 01:32:03,110
try to to make what is called a language model

1266
01:32:03,130 --> 01:32:05,400
this was

1267
01:32:05,420 --> 01:32:10,820
predicting the the probability of the next world in the sentence according to previous wars

1268
01:32:10,840 --> 01:32:12,780
in the sentence

1269
01:32:12,900 --> 01:32:17,900
if you know because we think that no dealing with politics it's so complicated you

1270
01:32:18,510 --> 01:32:25,490
no normalisation problems which are and actually the on to be greater than they use

1271
01:32:25,490 --> 01:32:31,450
tend to be the termite mainly by the most frequent phrases so

1272
01:32:33,240 --> 01:32:38,050
instead we prefer to use a ranking criterion

1273
01:32:38,050 --> 01:32:39,780
so if i consider like

1274
01:32:39,800 --> 01:32:41,590
network which is again

1275
01:32:41,610 --> 01:32:47,070
going to be all window approach network this network is going to give me a

1276
01:32:48,240 --> 01:32:53,050
school which states if my sentences in the charlotte

1277
01:32:54,220 --> 01:32:55,570
and i want

1278
01:32:55,590 --> 01:32:57,070
the school

1279
01:32:58,380 --> 01:33:02,210
window of text in

1280
01:33:02,220 --> 01:33:03,740
some copies

1281
01:33:03,740 --> 01:33:06,760
to be larger was the southern margin

1282
01:33:06,760 --> 01:33:12,800
then any source for the same window of text but where i replaced

1283
01:33:12,860 --> 01:33:14,210
the middle one

1284
01:33:14,220 --> 01:33:17,190
by some random walk so basically

1285
01:33:17,220 --> 01:33:22,570
the true piece of text coming from a corpus is my in a positive example

1286
01:33:22,590 --> 01:33:26,300
i might negative example it is going to be this you know

1287
01:33:26,320 --> 01:33:30,490
same window of text replaced in the middle by someone the more

1288
01:33:30,510 --> 01:33:37,970
gamma to train as is your language models

1289
01:33:37,970 --> 01:33:42,030
by stochastic gradient descent or ascent

1290
01:33:42,030 --> 01:33:46,130
and i'm going to say it on a lot of data

1291
01:33:46,150 --> 01:33:54,780
we actually first considered wikipedia which is which was about the time six hundred uniforms

1292
01:33:54,800 --> 01:34:00,760
then we like to consider an additional data set which is called reuters RCV one

1293
01:34:00,760 --> 01:34:02,590
which i did like

1294
01:34:02,610 --> 01:34:08,320
two hundred million of examples so it's a massive datasets you cannot expect you know

1295
01:34:08,990 --> 01:34:10,240
two two classical

1296
01:34:10,260 --> 01:34:14,880
training validation scheme like you know you train

1297
01:34:14,940 --> 01:34:20,420
you train on like margaret of parma does on trying to find the right ones

1298
01:34:20,590 --> 01:34:24,650
it's impossible it takes too much time so instead we did like a bit like

1299
01:34:24,650 --> 01:34:29,900
in biology you like a train new breed a couple of networks lines and you

1300
01:34:29,900 --> 01:34:31,940
know every everyday you check or

1301
01:34:31,950 --> 01:34:33,420
i was going

1302
01:34:34,900 --> 01:34:40,030
you try to make decisions according to some validation set too low

1303
01:34:40,050 --> 01:34:43,300
stub line or creating new ones

1304
01:34:43,300 --> 01:34:44,700
one of my preferred

1305
01:34:44,880 --> 01:34:49,590
problems in statistics is a mixture models are mixture model

1306
01:34:49,630 --> 01:34:50,690
is simply

1307
01:34:50,710 --> 01:34:51,900
the convolution

1308
01:34:51,920 --> 01:34:57,190
several simple model that can be written as as a weighted sum

1309
01:34:57,200 --> 01:34:59,990
of density so you have densities and j

1310
01:35:00,010 --> 01:35:05,380
and you say k densities these weights p one p two p q and the

1311
01:35:05,380 --> 01:35:08,170
sum of the weights is one

1312
01:35:08,190 --> 01:35:12,990
and that means that in terms of sampling you start by selecting one of the

1313
01:35:15,040 --> 01:35:17,560
with probability p one p two p k

1314
01:35:17,610 --> 01:35:22,740
and once you have selected from this density you generate from the identity

1315
01:35:23,280 --> 01:35:25,500
via value x

1316
01:35:26,590 --> 01:35:30,450
and so could could on of the selection mechanism

1317
01:35:30,820 --> 01:35:34,000
you get observations from this weighted sum

1318
01:35:34,020 --> 01:35:36,100
so that's something very simple

1319
01:35:36,180 --> 01:35:40,060
we would pick

1320
01:35:40,080 --> 01:35:44,490
the densities f one f two fk as parametrized entities and we're interested in the

1321
01:35:45,800 --> 01:35:47,470
of these entities

1322
01:35:47,490 --> 01:35:49,230
all right

1323
01:35:49,250 --> 01:35:50,040
we can

1324
01:35:50,450 --> 01:35:55,950
density is given in advance and we want to allocate the observations to each other

1325
01:35:55,950 --> 01:36:00,040
densities and that's more like clustering or classification

1326
01:36:02,480 --> 01:36:06,090
and when you write down the distribution of the sample

1327
01:36:06,100 --> 01:36:07,710
of size and

1328
01:36:07,720 --> 01:36:08,870
the this

1329
01:36:08,880 --> 01:36:12,190
what product of n terms which are all

1330
01:36:12,240 --> 01:36:17,600
fans of k terms so it's the complexity of the time and but as we

1331
01:36:17,600 --> 01:36:19,570
will see in a few slides

1332
01:36:19,590 --> 01:36:22,890
the complexity is more of order

1333
01:36:22,980 --> 01:36:28,200
k two c and which is the number of possibilities of allocating c and observations

1334
01:36:28,290 --> 01:36:29,380
to the k

1335
01:36:35,670 --> 01:36:40,260
in the simplest case it is like if you have a mixture of two normal

1336
01:36:40,280 --> 01:36:42,580
we've known weights point seven

1337
01:36:42,600 --> 01:36:48,100
and point three and non violence is one and one for you only have to

1338
01:36:48,100 --> 01:36:55,560
estimate the two main u one u two u shape

1339
01:36:55,580 --> 01:36:56,950
i have

1340
01:36:56,970 --> 01:37:01,130
the process of surface of the likelihood is about the same thing and they have

1341
01:37:01,130 --> 01:37:04,470
two modes which is if anything because in

1342
01:37:04,480 --> 01:37:09,400
actually it is the only identifiable problems they should have one of the forty to

1343
01:37:09,450 --> 01:37:13,480
five minute ovation you keep having two modes and

1344
01:37:13,500 --> 01:37:17,880
this is a complex problem to find the city mayor of new one u two

1345
01:37:17,880 --> 01:37:24,240
and for instance if you use the standard agrees to estimate both means depending on

1346
01:37:24,240 --> 01:37:28,570
where you start from that you can see really well from the back but the

1347
01:37:28,580 --> 01:37:32,110
part of the EM algorithm would depend on where you start from an out of

1348
01:37:33,020 --> 01:37:34,410
starting point here

1349
01:37:34,430 --> 01:37:37,070
two that two pairs

1350
01:37:37,090 --> 01:37:42,180
of the that stops at the wrong mode which doesn't mean anything in terms of

1351
01:37:42,180 --> 01:37:43,710
the real value

1352
01:37:43,720 --> 01:37:48,820
so that's already a complex problem even though you can write down italy

1353
01:37:50,660 --> 01:37:53,100
the light field

1354
01:37:53,120 --> 01:37:56,690
so the fish on in in addition to the bayesian statistician so

1355
01:37:56,840 --> 01:38:01,040
examples will also be mostly bayesian

1356
01:38:01,090 --> 01:38:04,140
and the basic is just

1357
01:38:04,190 --> 01:38:08,670
starting from regular statistics so you have a model with an unknown parameter

1358
01:38:09,540 --> 01:38:15,580
observations x from the distribution indexed with by these unknown parameter

1359
01:38:15,600 --> 01:38:18,110
and all you do is you

1360
01:38:18,120 --> 01:38:24,080
introduce two which is called the prior and which is usually a distribution probability distribution

1361
01:38:24,420 --> 01:38:28,930
on the parameter space and you start to work with both

1362
01:38:30,620 --> 01:38:32,290
and part

1363
01:38:32,300 --> 01:38:36,660
and the way you work with it is to use base and that is you

1364
01:38:36,680 --> 01:38:39,770
you think that my is the distribution of theory

1365
01:38:39,780 --> 01:38:44,340
and your observable come from the conditional distribution of x given c

1366
01:38:44,360 --> 01:38:49,220
you just apply the standard privileges herein called b here and that you can't use

1367
01:38:49,220 --> 01:38:52,440
the same distribution of seed given xt river

1368
01:38:52,450 --> 01:38:53,210
the other

1369
01:38:53,290 --> 01:38:59,800
of conditioning the the likelihood condition x on theta but no observed x u condition

1370
01:39:00,490 --> 01:39:03,160
on x there is so that's your

1371
01:39:03,170 --> 01:39:06,350
quantity you want to work with and

1372
01:39:06,370 --> 01:39:11,620
i know that the regular probability distribution can supply a given that is the basic

1373
01:39:11,620 --> 01:39:15,510
tool for doing bayesian inference

1374
01:39:15,550 --> 01:39:18,920
and so it's over in terms of formal

1375
01:39:18,950 --> 01:39:21,050
inference except that

1376
01:39:21,070 --> 01:39:24,970
of course we have trouble to use this posterior distribution

1377
01:39:24,980 --> 01:39:29,180
so would keep this this points which are more like advertising for

1378
01:39:29,200 --> 01:39:31,080
bayesian statistics

1379
01:39:31,100 --> 01:39:32,720
so in very

1380
01:39:32,740 --> 01:39:35,960
special conditions you can walk out

1381
01:39:35,980 --> 01:39:41,640
every single enclosed terms and i took this simple example where you have binomial

1382
01:39:42,060 --> 01:39:47,480
so if have the binomial BNP BNP you're unknown parameter

1383
01:39:47,580 --> 01:39:49,820
and if in addition on p

1384
01:39:49,840 --> 01:39:52,080
the probability between zero and one

1385
01:39:52,100 --> 01:39:54,700
using a beta distribution

1386
01:39:54,710 --> 01:39:57,680
which is just the poet distribution on the one

1387
01:39:57,700 --> 01:40:00,370
you can find your city mayor

1388
01:40:00,380 --> 01:40:01,380
of p

1389
01:40:01,390 --> 01:40:04,380
which is the posterior expectation of p

1390
01:40:04,390 --> 01:40:07,440
conditional next and so if you write down

1391
01:40:07,450 --> 01:40:09,500
what it means you take p

1392
01:40:09,510 --> 01:40:14,200
you take your question which is the product of your prior because n minus one

1393
01:40:14,200 --> 01:40:18,670
one minute speech to be minus one by the likelihood which is peter the x

1394
01:40:18,670 --> 01:40:24,640
y minute speeches he n minus text and bingo you get something close form which

1395
01:40:24,680 --> 01:40:30,680
looks rather like x over and your standard estimator except there is a kind of

1396
01:40:32,300 --> 01:40:35,310
term here experts a in a plus b

1397
01:40:35,330 --> 01:40:36,570
as an

1398
01:40:36,580 --> 01:40:42,260
but have a very special case which is called conjugate prior case

1399
01:40:42,280 --> 01:40:44,440
and conjugate priors

1400
01:40:44,460 --> 01:40:49,180
night because they give you closed form solutions but are terrible

1401
01:40:49,190 --> 01:40:50,770
because the rest effect

1402
01:40:50,790 --> 01:40:52,790
the use of prior information

1403
01:40:52,800 --> 01:40:56,140
they have offer specific forms so

1404
01:40:56,180 --> 01:41:00,110
they are too sensitive to the choice of the prior parameters

1405
01:41:01,480 --> 01:41:05,690
this is a bad situation that

1406
01:41:05,770 --> 01:41:10,250
was sold by new computational methods in

1407
01:41:10,310 --> 01:41:14,330
the the early nineties

1408
01:41:14,420 --> 01:41:18,300
we it when we get out of computational

1409
01:41:18,320 --> 01:41:19,740
computers simple

1410
01:41:19,760 --> 01:41:22,440
priors like conjugate priors

1411
01:41:22,460 --> 01:41:25,650
we have a range of difficulties

1412
01:41:27,680 --> 01:41:32,990
kind of blocking the the more widespread use of vision techniques

1413
01:41:33,000 --> 01:41:37,680
and here's a few examples of computational problems

1414
01:41:37,780 --> 01:41:39,040
the first

1415
01:41:39,050 --> 01:41:43,790
the parameter space may be constrained in very

1416
01:41:46,130 --> 01:41:47,460
as we will see later

1417
01:41:47,510 --> 01:41:48,910
for the area

1418
01:41:50,930 --> 01:41:54,020
the founding model itself the likelihood

1419
01:41:54,040 --> 01:41:59,310
may be complex enough so that we cannot write down we cannot compute explicitly the

1420
01:41:59,310 --> 01:42:01,250
likely to to point

1421
01:42:01,270 --> 01:42:05,380
and again in some missing problems like the stochastic volatility model

1422
01:42:05,440 --> 01:42:10,520
this is the case you cannot write down the likelihood of the data set itself

1423
01:42:10,520 --> 01:42:13,040
may be a problem that if you have so all

1424
01:42:13,060 --> 01:42:20,180
millions of observations the single computation of the likelihood may take too much time

1425
01:42:20,200 --> 01:42:21,400
and of course

1426
01:42:21,400 --> 01:42:26,470
it is clearly easy to do guassian process regression you've seen it done numerous times

1427
01:42:26,470 --> 01:42:30,780
now i think where you choose covariance function for the prior

1428
01:42:30,830 --> 01:42:33,780
and then the output is some fitted

1429
01:42:33,790 --> 01:42:39,120
some predictions but

1430
01:42:39,160 --> 01:42:40,780
i think what might and

1431
01:42:40,790 --> 01:42:44,160
i think one might prove difficult here if you want to apply this methodology is

1432
01:42:44,160 --> 01:42:46,940
that it's got to be done over time so not only have you got have

1433
01:42:46,940 --> 01:42:47,910
the prior

1434
01:42:47,970 --> 01:42:52,610
in the former guassian process which is sort of the input to you're simulation measurements

1435
01:42:52,610 --> 01:42:58,270
but the the posterior afterwards as also got big as impressed what you can come

1436
01:42:58,280 --> 01:43:01,100
you know reasonably propagate guassian prices that way

1437
01:43:01,240 --> 01:43:04,870
i'm not sure if you could do it the nice thing would be that you

1438
01:43:04,870 --> 01:43:05,650
could specify

1439
01:43:06,120 --> 01:43:10,970
get better gas progress is that one can do parametrically because it turns out that

1440
01:43:11,380 --> 01:43:18,190
para these parametric estimate we start with the gas in in in parameter space and

1441
01:43:18,210 --> 01:43:21,470
and then employ guassian the

1442
01:43:21,520 --> 01:43:28,610
guess in envelopes are not uniform is actually impossible to get a spatially invariant guassian

1443
01:43:28,610 --> 01:43:31,720
process i mean it would be nice to have one which is a guassian process

1444
01:43:31,720 --> 01:43:33,670
which is a

1445
01:43:33,740 --> 01:43:38,840
simply a function of the difference between and pride i'm sure you've seen that that

1446
01:43:38,850 --> 01:43:42,650
seem to be sensible property for africa so for example i want to start with

1447
01:43:42,650 --> 01:43:47,220
the circle and say OK what's my initial uncertainty position the circuit seem intuitive that

1448
01:43:47,220 --> 01:43:50,830
you ought to be allowed to say that the variance is uniformly distributed over the

1449
01:43:50,830 --> 01:43:53,340
circle and with my

1450
01:43:53,370 --> 01:43:58,970
formalism you can't do that if you look at you know whatever

1451
01:43:59,000 --> 01:44:05,440
gas distribution you specify over control points or over

1452
01:44:05,450 --> 01:44:09,750
over find parameters you tend to get sort of lumps in in the in the

1453
01:44:10,300 --> 01:44:14,160
uncertainty in the ground could you can't make a completely uniform

1454
01:44:17,940 --> 01:44:20,510
what's going to happen to think

1455
01:44:20,610 --> 01:44:22,090
just because

1456
01:44:22,210 --> 01:44:24,180
with any luck

1457
01:44:26,630 --> 01:44:29,600
cameron to do that again

1458
01:44:29,610 --> 01:44:32,890
is that what can to groups chris bishop two

1459
01:44:32,910 --> 01:44:35,510
so repeatable behavior across machine

1460
01:44:37,400 --> 01:44:43,440
put damp anyway

1461
01:44:43,460 --> 01:44:47,680
OK now we need to revisit the fusion process that we that we looked at

1462
01:44:48,610 --> 01:44:50,370
but now

1463
01:44:50,410 --> 01:44:55,460
done for age observations so you seen this picture before the normals cast out from

1464
01:44:55,470 --> 01:44:58,050
the our estimate

1465
01:44:58,120 --> 01:45:00,620
the of the console

1466
01:45:00,640 --> 01:45:02,300
and now we need to

1467
01:45:02,310 --> 01:45:07,400
few edged measurements remember these tangent measurements they're not not points like you do

1468
01:45:07,450 --> 01:45:09,530
with conventional regression they have to be

1469
01:45:09,540 --> 01:45:12,850
tangent so our the fusion work remember before

1470
01:45:12,860 --> 01:45:18,180
we had this simple picture shows what happens when you refuse one corner measurements and

1471
01:45:18,370 --> 01:45:22,090
that's the green distribution with the prior in blue and you get the red posterior

1472
01:45:22,100 --> 01:45:26,610
so how is this going to work for observations of tangents presumably

1473
01:45:28,800 --> 01:45:31,820
distribution is not going to look like this

1474
01:45:31,930 --> 01:45:36,830
green thing here because we observed

1475
01:45:36,870 --> 01:45:37,830
the tangent

1476
01:45:37,880 --> 01:45:40,050
and so you can replace that

1477
01:45:40,980 --> 01:45:45,700
observation likelihood by discrete observation like which is of the general thing it's a pair

1478
01:45:46,300 --> 01:45:49,960
parallel lines and

1479
01:45:50,010 --> 01:45:54,760
so it's covariance appears to be unbounded in one direction which is you thing was

1480
01:45:54,790 --> 01:45:58,270
messy thing to do deal with but if we think in terms of information the

1481
01:45:58,270 --> 01:46:01,680
inverse of the covariance that's not a problem so the

1482
01:46:01,700 --> 01:46:05,080
information matrix for this edge observation

1483
01:46:05,100 --> 01:46:07,250
perpendicular to the normal and

1484
01:46:07,280 --> 01:46:11,140
has a an information matrix which is just proportional to n then transpose and you

1485
01:46:11,140 --> 01:46:14,980
can see that some

1486
01:46:15,000 --> 01:46:17,860
only has one non-zero eigen value this

1487
01:46:17,880 --> 01:46:19,780
it has rank one matrix

1488
01:46:21,210 --> 01:46:22,910
so that deals with the

1489
01:46:22,960 --> 01:46:24,840
the edge observation

1490
01:46:24,940 --> 01:46:27,630
quite nicely now the posterior

1491
01:46:27,640 --> 01:46:31,730
well look something like this the posterior will have covariance because the prior

1492
01:46:31,820 --> 01:46:34,610
stabilise things

1493
01:46:37,030 --> 01:46:38,220
OK so now

1494
01:46:38,240 --> 01:46:43,560
the remember the principles of summation of information and the information weighted mean we ought

1495
01:46:43,570 --> 01:46:45,340
to be able to apply those now

1496
01:46:46,180 --> 01:46:50,260
absorbing new edge measurements into the

1497
01:46:50,280 --> 01:46:54,200
consul space into the shape space and so

1498
01:46:54,220 --> 01:46:57,930
what should i write down i could try something like this the new information is

1499
01:46:57,930 --> 01:46:58,830
you had

1500
01:46:58,880 --> 01:47:02,060
and if you look at this ratio that's enough to decide whether to take this

1501
01:47:02,060 --> 01:47:03,490
guy or not

1502
01:47:03,500 --> 01:47:06,470
but you need to normalize

1503
01:47:06,520 --> 01:47:08,700
my proposal to make this by

1504
01:47:08,750 --> 01:47:11,730
two to satisfy that

1505
01:47:11,980 --> 01:47:15,630
and we also see why these terms are necessary

1506
01:47:15,730 --> 01:47:19,790
but this is the acceptance probability so if you flip a coin

1507
01:47:19,800 --> 01:47:21,050
which is the number thing

1508
01:47:21,050 --> 01:47:23,980
so this will be between

1509
01:47:24,010 --> 01:47:26,220
zero and one

1510
01:47:26,440 --> 01:47:30,720
so if you if you flip coin and the coin is less than this

1511
01:47:30,740 --> 01:47:34,470
in other words if this guy is much better than this guy you take it

1512
01:47:34,480 --> 01:47:36,830
if you flip a coin and it's not

1513
01:47:36,850 --> 01:47:41,810
then the and essentially what you're doing is

1514
01:47:41,860 --> 01:47:45,040
what we're doing is something from a discrete distribution

1515
01:47:45,040 --> 01:47:49,020
just going to resampling trick we did yesterday because this is what we have here

1516
01:47:49,030 --> 01:47:52,440
is a and we have one minus a

1517
01:47:53,090 --> 01:47:55,110
this is when you know

1518
01:48:00,340 --> 01:48:02,180
accept new

1519
01:48:02,220 --> 01:48:03,250
next star

1520
01:48:03,290 --> 01:48:04,850
and this is when you

1521
01:48:10,130 --> 01:48:11,280
x i

1522
01:48:11,370 --> 01:48:14,320
so have a candidate

1523
01:48:14,370 --> 01:48:16,200
nature provides something for you

1524
01:48:16,210 --> 01:48:19,280
if it's much better to take if it's not

1525
01:48:19,300 --> 01:48:21,310
this is but you don't take it

1526
01:48:21,320 --> 01:48:22,540
if you're not sure

1527
01:48:22,540 --> 01:48:27,800
you do the flip of a coin and probability a you take that you can

1528
01:48:28,000 --> 01:48:32,480
and the way you choose a sort of detailed balance is satisfied

1529
01:48:33,010 --> 01:48:42,850
they next and gets update

1530
01:48:42,910 --> 01:48:44,960
welcome to that now

1531
01:48:49,450 --> 01:49:00,700
if the guy across the OK for you if the woman across the bottom looks

1532
01:49:00,700 --> 01:49:02,940
more or less like the one you have

1533
01:49:03,540 --> 01:49:07,530
you might not take too much if you're already with some hard bay and there's

1534
01:49:07,540 --> 01:49:09,960
a hot babe across the bar

1535
01:49:10,360 --> 01:49:12,450
so much effort to work

1536
01:49:54,060 --> 01:49:56,470
you guys guys guys well

1537
01:49:56,470 --> 01:50:02,570
you have to probabilities provide acceptance probability of rejection when something is much better

1538
01:50:02,630 --> 01:50:06,630
and this you term will dominate the bar will be much harder than the other

1539
01:50:06,630 --> 01:50:09,450
so quite often

1540
01:50:09,490 --> 01:50:11,310
it will be

1541
01:50:15,910 --> 01:50:22,180
of the original one when you don't have this q if q is symmetric if

1542
01:50:22,180 --> 01:50:27,140
this guy asymmetric in disguise disappear so the argument is when these guys disappear

1543
01:50:27,160 --> 01:50:30,340
because there is a symmetric so this is the same as the

1544
01:50:30,390 --> 01:50:32,980
then if this guy is much better than

1545
01:50:33,050 --> 01:50:36,390
if x diaries but the next slide is is going to be larger than one

1546
01:50:36,390 --> 01:50:39,880
so you will always accept

1547
01:50:39,930 --> 01:50:44,180
in general we do need to have the keys to satisfy the power so what

1548
01:50:44,180 --> 01:50:51,740
you really could do is to compute a in one minus a

1549
01:50:51,770 --> 01:50:55,610
that neither of what

1550
01:50:55,710 --> 01:51:02,770
now you will need it because there's going to be these cues

1551
01:51:12,120 --> 01:51:18,890
the club analogy problem but it was fun

1552
01:51:18,910 --> 01:51:20,790
but that's pretty much the idea

1553
01:51:20,790 --> 01:51:24,880
OK so maybe you the club have this special by is this going to be

1554
01:51:24,880 --> 01:51:26,620
no special by bias

1555
01:51:28,760 --> 01:51:31,270
the magic point that tells you want to

1556
01:51:31,350 --> 01:51:33,150
go for a hunt

1557
01:51:37,120 --> 01:51:45,570
good question so we have to question the it all the positions their annealed

1558
01:51:45,580 --> 01:51:51,950
now to that next

1559
01:51:51,960 --> 01:51:54,290
this is how the algorithm works in practice

1560
01:51:54,310 --> 01:51:57,100
after one hundred samples you have sort of

1561
01:51:57,150 --> 01:52:00,880
after five hundred you have to fit and i would run by the mathlet simulations

1562
01:52:00,880 --> 01:52:04,010
which show you that this happens this fast

1563
01:52:04,010 --> 01:52:07,700
and up to five thousand samples you have a pretty good approximation

1564
01:52:07,750 --> 01:52:13,000
of the target distribution

1565
01:52:13,050 --> 01:52:14,990
how do you choose that q

1566
01:52:15,010 --> 01:52:16,990
it's an important question

1567
01:52:17,000 --> 01:52:19,930
if q is just a random walk

1568
01:52:19,940 --> 01:52:23,140
and these are the things you have to keep in mind

1569
01:52:24,120 --> 01:52:27,540
if you start markov chain in one mode

1570
01:52:27,550 --> 01:52:29,090
the markov chain starts here

1571
01:52:29,090 --> 01:52:33,210
in the past winners we can delay the onset a little bit

1572
01:52:33,230 --> 01:52:38,610
and i told him with about cross validation expected look like

1573
01:52:38,640 --> 01:52:40,730
i think questions here

1574
01:52:40,950 --> 01:52:46,070
everything clear immediately that

1575
01:52:46,090 --> 01:52:53,350
well my master so frankly asleep

1576
01:52:53,390 --> 01:52:56,580
should not

1577
01:52:56,580 --> 01:52:58,510
what not looking good

1578
01:53:02,140 --> 01:53:04,290
well we can actually use it for a couple

1579
01:53:04,350 --> 01:53:06,170
useful practical things so

1580
01:53:06,220 --> 01:53:08,760
one with be novelty detection

1581
01:53:09,430 --> 01:53:12,900
the idea would be willing to find the least likely observations

1582
01:53:12,940 --> 01:53:16,420
and so what you do you could for instance perform with a system that in

1583
01:53:16,420 --> 01:53:17,390
the interest

1584
01:53:17,440 --> 01:53:21,010
without the observations events discourse lowest

1585
01:53:21,060 --> 01:53:24,490
and so what i could do is just compute the density estimate

1586
01:53:24,590 --> 01:53:27,150
this here

1587
01:53:27,170 --> 01:53:30,890
and then sort with respect to i and just

1588
01:53:30,890 --> 01:53:33,140
print according to the magnitude

1589
01:53:33,880 --> 01:53:35,710
what would you can

1590
01:53:35,770 --> 01:53:37,900
network intrusion detection

1591
01:53:37,910 --> 01:53:41,900
for instance you want to find out that something unusual is happening on the network

1592
01:53:41,910 --> 01:53:44,100
the jet engine might fail

1593
01:53:44,140 --> 01:53:48,730
you might want to cling it places like somebody inserted started with typos

1594
01:53:48,760 --> 01:53:52,920
mislabeled digital and on that photographs

1595
01:53:53,310 --> 01:53:55,400
fraud detection

1596
01:53:55,410 --> 01:53:57,800
as of collaborating alarm so

1597
01:53:57,840 --> 01:53:59,890
park your car somewhere in

1598
01:53:59,900 --> 01:54:04,220
you know sometimes it's very quiet back to street and sometimes it's a really nicely

1599
01:54:04,220 --> 01:54:07,170
highway and when he said the car alarms

1600
01:54:07,190 --> 01:54:08,320
you should the

1601
01:54:08,350 --> 01:54:11,010
i installed the city to the highway runs

1602
01:54:11,060 --> 01:54:13,410
otherwise it'll always go off the highway

1603
01:54:13,470 --> 01:54:17,000
but it might not be sensitive enough to detect the thief early on the back

1604
01:54:18,860 --> 01:54:24,810
so you want something that's calibration automatically

1605
01:54:26,860 --> 01:54:28,220
that one

1606
01:54:28,270 --> 01:54:31,860
this these are typical data

1607
01:54:31,860 --> 01:54:37,700
from these space so this is like the ones that i showed you

1608
01:54:37,760 --> 01:54:40,860
these will be outliers

1609
01:54:40,900 --> 01:54:43,470
the fairly of the most of them

1610
01:54:43,510 --> 01:54:45,680
at least i hope you agree

1611
01:54:45,730 --> 01:54:46,720
and that's just

1612
01:54:46,770 --> 01:54:49,040
from there in in decreasing

1613
01:54:49,080 --> 01:54:51,170
order this

1614
01:54:51,230 --> 01:54:52,870
you can see why

1615
01:54:52,900 --> 01:54:55,060
what this is anybody's guess

1616
01:54:55,070 --> 01:54:57,390
very badly segmented

1617
01:54:57,400 --> 01:54:59,700
this apparently suffice

1618
01:54:59,750 --> 01:55:01,660
it could have been an eight or nine

1619
01:55:01,670 --> 01:55:04,110
that i think is too

1620
01:55:04,150 --> 01:55:05,430
what that as well

1621
01:55:05,450 --> 01:55:07,290
probably something like

1622
01:55:07,340 --> 01:55:10,100
german for but badly written

1623
01:55:10,110 --> 01:55:13,210
what that if nobody knows i think it was six but

1624
01:55:13,260 --> 01:55:16,010
basically what it means is these are really

1625
01:55:16,190 --> 01:55:19,790
it it doesn't again guarantee that you'll find all the bad ones but you'll find

1626
01:55:19,790 --> 01:55:21,590
a lot

1627
01:55:21,640 --> 01:55:23,910
you'll see pretty pictures of outliers

1628
01:55:27,650 --> 01:55:31,510
now there is another useful trick that you might want to remember

1629
01:55:31,540 --> 01:55:35,850
it has to do with solar mass automatic adjustment rule

1630
01:55:36,430 --> 01:55:38,610
the problem is let's i have

1631
01:55:38,650 --> 01:55:40,360
pick some kernel with

1632
01:55:40,370 --> 01:55:43,390
and unfortunately doesn't really work well

1633
01:55:43,400 --> 01:55:44,070
it's say

1634
01:55:44,080 --> 01:55:47,330
that could lead to the final density

1635
01:55:47,380 --> 01:55:51,020
because what you would actually want to in regions of low density

1636
01:55:51,080 --> 01:55:55,860
because what coral person regions of high density you outcome

1637
01:55:55,870 --> 01:55:58,130
that's a bit of a technical problem right

1638
01:55:58,230 --> 01:56:00,970
because i'm trying to the density estimation

1639
01:56:00,980 --> 01:56:03,730
but actually it will need to have a good density estimate in order to the

1640
01:56:03,730 --> 01:56:06,090
a with density estimation

1641
01:56:06,920 --> 01:56:11,340
so how can we work around well we don't quite eustace this estimation but we

1642
01:56:11,340 --> 01:56:12,520
use a very very

1643
01:56:12,900 --> 01:56:17,200
sloppy scheme which will give us a rough idea of what's going on

1644
01:56:17,260 --> 01:56:21,830
so all i do is i just find out what the average distance of points

1645
01:56:21,850 --> 01:56:26,180
x i is to its nearest neighbour k nearest neighbour

1646
01:56:26,220 --> 01:56:27,600
and then using that

1647
01:56:27,600 --> 01:56:31,430
to adjust the range of radio system

1648
01:56:31,510 --> 01:56:33,330
and improve things quite a bit

1649
01:56:33,330 --> 01:56:35,250
so for instance you can pick

1650
01:56:36,050 --> 01:56:37,080
the kernel we

1651
01:56:37,160 --> 01:56:40,960
be something like point five that quantity

1652
01:56:40,980 --> 01:56:44,650
link is actually pretty good this estimation matter and you can use it for regression

1653
01:56:44,650 --> 01:56:46,700
and classification

1654
01:56:48,660 --> 01:56:51,990
so that would be exactly such events the idea here

1655
01:56:52,050 --> 01:56:56,590
so they feel there so you want to live out there and everyone over here

1656
01:57:00,540 --> 01:57:05,350
and if i pick just one scale not terribly good

1657
01:57:05,400 --> 01:57:07,940
i picked different scales

1658
01:57:07,980 --> 01:57:11,830
you can see it actually works very well

1659
01:57:15,670 --> 01:57:19,330
if you know how to implement this you'll will be able to get approval to

1660
01:57:19,340 --> 01:57:23,630
classify regression estimate already

1661
01:57:25,160 --> 01:57:28,740
the idea is well i've got past observations x and y

1662
01:57:28,800 --> 01:57:30,900
and it's a one plus one is one

1663
01:57:30,950 --> 01:57:35,190
i want to find some estimator for the conditional probability of y given x

1664
01:57:35,200 --> 01:57:36,190
why i can just

1665
01:57:38,220 --> 01:57:42,100
and i want to find p of y equals one given a

1666
01:57:42,100 --> 01:57:47,840
well that p of y equals one and x two are quite a few things

1667
01:57:47,850 --> 01:57:52,320
just worked this out the negative pressure

1668
01:57:52,370 --> 01:57:54,300
and whenever this racial here

1669
01:57:54,350 --> 01:57:57,250
less than one-half mile sites class minus one

1670
01:57:57,320 --> 01:58:00,290
underlie cell sites class

1671
01:58:02,160 --> 01:58:05,890
i can easily just say well look that we take the difference between the positive

1672
01:58:05,900 --> 01:58:07,830
and the negative class

1673
01:58:07,830 --> 01:58:09,610
and then they get this expansion here

1674
01:58:09,620 --> 01:58:10,910
which happens to be

1675
01:58:10,930 --> 01:58:14,970
essentially the same thing as the naive it's the incredible classifiers

1676
01:58:14,980 --> 01:58:17,740
that will be talking about much later

1677
01:58:17,790 --> 01:58:19,180
but basically

1678
01:58:19,210 --> 01:58:23,360
what you do is you just take a weighted combination of the labels

1679
01:58:23,360 --> 01:58:25,620
according to how close you are to them

1680
01:58:25,790 --> 01:58:29,140
and that will tell you tell you what you're label is

1681
01:58:29,210 --> 01:58:33,620
so this is very similar to what we had in our nearest neighbour remembered

