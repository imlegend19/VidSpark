1
00:00:00,000 --> 00:00:02,650
index will be on the fifteenth of august

2
00:00:02,670 --> 00:00:04,130
the limits of

3
00:00:04,130 --> 00:00:07,710
it's a more theoretical questions and more the questions but quite easy once i mean

4
00:00:07,710 --> 00:00:08,690
you know

5
00:00:08,750 --> 00:00:14,090
to process without the computer

6
00:00:20,860 --> 00:00:23,110
that's all

7
00:00:23,170 --> 00:00:26,190
that's that's later

8
00:00:27,190 --> 00:00:31,650
so i hope this gave you first let's say ten plates was ideas about what

9
00:00:31,650 --> 00:00:32,780
the cause is about

10
00:00:32,840 --> 00:00:34,550
this is about

11
00:00:34,610 --> 00:00:37,690
and i suggest to use the next ten minutes

12
00:00:38,940 --> 00:00:44,380
four open questions or further

13
00:00:44,440 --> 00:00:52,630
things you want to know about his see neuron is you

14
00:01:03,400 --> 00:01:07,480
so what is the difference between i'm saying because of the recording what's the difference

15
00:01:07,500 --> 00:01:08,730
or the advantage

16
00:01:08,750 --> 00:01:09,900
added value of

17
00:01:09,900 --> 00:01:15,130
q cv zv something such as cluster analysis or even just

18
00:01:15,170 --> 00:01:16,940
well actually there is

19
00:01:17,000 --> 00:01:18,670
there are also that's it

20
00:01:18,670 --> 00:01:21,250
there are some similar similarity right

21
00:01:21,250 --> 00:01:23,590
that you're able in both cases

22
00:01:23,630 --> 00:01:27,130
to cluster y cases in groups right

23
00:01:27,170 --> 00:01:29,570
or in types

24
00:01:31,920 --> 00:01:36,880
two main differences and it's not like an advantage or the density difference in nature

25
00:01:36,900 --> 00:01:41,110
the two main differences is first because you got to keep all the cases

26
00:01:42,420 --> 00:01:45,820
and in those cases which you mention you're going to have some some cases will

27
00:01:45,840 --> 00:01:46,980
be left out

28
00:01:47,940 --> 00:01:48,880
or less

29
00:01:48,880 --> 00:01:52,920
it was it not taken into account realistic and you can't some waiting for the

30
00:01:56,710 --> 00:01:59,190
doesn't have to be most often right

31
00:02:08,000 --> 00:02:11,030
so in the case that you have to in cases because he was in any

32
00:02:11,030 --> 00:02:12,280
case keep them all

33
00:02:12,300 --> 00:02:14,590
that's the first difference

34
00:02:14,690 --> 00:02:16,030
the other thing is that

35
00:02:16,110 --> 00:02:20,370
the two zero mentioning kind of more powerful

36
00:02:20,420 --> 00:02:25,080
as you gain a larger as you into larger and situations right

37
00:02:25,160 --> 00:02:28,460
and quite the contrary because you would be stronger when you go to smaller and

38
00:02:29,950 --> 00:02:32,960
so i would say that there is a kind of a continuum

39
00:02:33,000 --> 00:02:37,910
the first thing the difference is about the assumptions behind analysis

40
00:02:38,010 --> 00:02:40,950
in order to engage into their personalities

41
00:02:41,090 --> 00:02:43,780
you will have to take on board some basic assumptions

42
00:02:43,830 --> 00:02:46,410
the main statistical assumptions

43
00:02:46,450 --> 00:02:48,740
such as you know are

44
00:02:50,790 --> 00:02:52,400
i suppose

45
00:02:52,450 --> 00:02:54,660
you wouldn't in that case

46
00:02:54,700 --> 00:02:55,460
which is

47
00:02:55,490 --> 00:03:00,190
which is essentially taking processes

48
00:03:00,230 --> 00:03:10,320
so actually is in both cases the it is pacification to speak

49
00:03:10,320 --> 00:03:11,740
and you can use them

50
00:03:11,750 --> 00:03:15,450
also as say in dialogue we can see what you can get pretty we get

51
00:03:15,500 --> 00:03:20,160
processes and seen so much of his done using this done in the future

52
00:03:20,170 --> 00:03:21,080
crossing the two

53
00:03:21,080 --> 00:03:23,550
this analysis and

54
00:03:23,620 --> 00:03:24,460
this year

55
00:03:24,550 --> 00:03:27,290
i think we should discuss class cast in the second week you might have some

56
00:03:29,240 --> 00:03:32,550
so very dramatically my my basic would be

57
00:03:32,580 --> 00:03:35,080
tribal from the data and see what you get out of it in the morning

58
00:03:35,080 --> 00:03:38,950
that the way to

59
00:03:55,030 --> 00:03:57,480
yes there is

60
00:03:58,450 --> 00:04:00,650
there's a pretty good reason please

61
00:04:00,700 --> 00:04:02,330
by marks

62
00:04:02,370 --> 00:04:03,530
not to count

63
00:04:04,330 --> 00:04:05,710
that's the marks

64
00:04:05,770 --> 00:04:09,360
a eight x x o marks

65
00:04:09,370 --> 00:04:12,440
it's working paper on combat it's also

66
00:04:13,660 --> 00:04:17,280
recently published an article just came out i think we can come up very soon

67
00:04:17,300 --> 00:04:18,240
on this

68
00:04:18,240 --> 00:04:20,790
question of

69
00:04:20,880 --> 00:04:23,480
the connection between number of cases

70
00:04:23,530 --> 00:04:24,400
and the role of

71
00:04:24,410 --> 00:04:26,990
articles called conditions

72
00:04:27,120 --> 00:04:30,620
there are some sort of some i will admit that may be the time to

73
00:04:30,620 --> 00:04:32,990
elaborates on the exact rules of thumb

74
00:04:33,050 --> 00:04:35,330
but the idea is that

75
00:04:35,330 --> 00:04:39,140
i was going to say because i don't think that's the appropriate common to make

76
00:04:39,140 --> 00:04:44,980
something that particular noise so we have basically k log k sort of thing so

77
00:04:44,980 --> 00:04:49,350
it's it's a polynomial in k as opposed to a naive method which would say

78
00:04:49,350 --> 00:04:53,960
take all possible input patterns to keep statistics on each one separately now about is

79
00:04:53,960 --> 00:04:55,900
going to be dependent on to the k

80
00:04:55,910 --> 00:05:00,150
so you really can sort of teasing apart in this case and

81
00:05:00,160 --> 00:05:04,000
i get something like a like a plant however long it takes to learn whatever

82
00:05:04,000 --> 00:05:07,590
the sub pieces are which in this case was quite trivial but in some of

83
00:05:07,590 --> 00:05:09,210
the harrier cases

84
00:05:09,840 --> 00:05:11,000
more so

85
00:05:11,020 --> 00:05:16,470
in particular the way we use this idea this hidden bit idea to learn structures

86
00:05:16,470 --> 00:05:21,640
is well if we have constant down on the on the the number of parents

87
00:05:21,640 --> 00:05:27,010
that node can have then we can actually enumerate essentially for each node what's possible

88
00:05:27,010 --> 00:05:28,270
parents would be

89
00:05:28,330 --> 00:05:29,610
and those end up being

90
00:05:29,640 --> 00:05:32,710
corresponding to the bit positions an example that i had there before you have to

91
00:05:32,710 --> 00:05:36,820
learn in the example that i showed you have to learn which bit to listen

92
00:05:37,520 --> 00:05:41,420
in the dynamic bayes net stuff you have to learn which parents to listen to

93
00:05:41,420 --> 00:05:44,330
of all the different possible parent sets of size three say

94
00:05:44,330 --> 00:05:47,150
which one is actually doing a good job of

95
00:05:47,160 --> 00:05:51,080
giving me the thing actually condition on to be able to predict so that's what

96
00:05:51,090 --> 00:05:54,670
ends up being and then we had this you know this lovely thing i copied

97
00:05:54,670 --> 00:05:58,150
just because it has lots of plants indulges in it

98
00:05:58,170 --> 00:05:59,720
but is primarily

99
00:05:59,730 --> 00:06:02,400
you know it's it is about as long as the the size of the parent

100
00:06:02,400 --> 00:06:05,940
sets is relatively small that's big then this all goes away you don't know what

101
00:06:05,940 --> 00:06:09,250
your conditioning on and you don't have a good quick about

102
00:06:09,270 --> 00:06:16,090
and eight for that matter and d i think the this is just cut out

103
00:06:16,090 --> 00:06:18,440
of the paper this is he

104
00:06:18,450 --> 00:06:20,060
these k

105
00:06:20,070 --> 00:06:22,720
he was k moment ago

106
00:06:22,890 --> 00:06:24,450
so that's good

107
00:06:24,510 --> 00:06:28,590
and this is why oh no no that's not true

108
00:06:28,610 --> 00:06:32,750
so so these the now is this is the bound on the number of parents

109
00:06:32,750 --> 00:06:36,200
that you have right so this is why if it's unbounded you get something that's

110
00:06:36,200 --> 00:06:40,660
very very big n is the number of state variables so the number of nodes

111
00:06:40,660 --> 00:06:41,710
in your net

112
00:06:43,490 --> 00:06:46,700
a is we're going to say oh actions

113
00:06:46,710 --> 00:06:47,940
i think

114
00:06:47,960 --> 00:06:52,710
we're going actions or something else but i think pretty sure actions it's not playing

115
00:06:52,710 --> 00:06:55,800
a big role in this band anyway the thing that looks scary here's and the

116
00:06:55,800 --> 00:06:59,710
end of the day right that's the number of nodes raced to the size of

117
00:06:59,710 --> 00:07:00,720
parent sets

118
00:07:00,780 --> 00:07:01,510
which is

119
00:07:01,540 --> 00:07:05,460
roughly how many parents possible parents you

120
00:07:05,470 --> 00:07:06,960
so possible

121
00:07:06,970 --> 00:07:11,620
set possible identities of the parents what they could be it's not good when you

122
00:07:11,620 --> 00:07:14,980
have like eleven parents right because i don't even know that means in real life

123
00:07:16,330 --> 00:07:17,850
it seems like two

124
00:07:17,860 --> 00:07:21,160
maybe one remarries you get three of eleven

125
00:07:21,170 --> 00:07:26,230
anyway so we've we've looked at doing this in some artificial examples where this is

126
00:07:26,230 --> 00:07:30,670
an artificial stock-based example we did actually start off with some real stock data to

127
00:07:30,680 --> 00:07:34,620
hard to predict with real stock

128
00:07:34,630 --> 00:07:38,530
so so we have to put that aside and we made up our own articles

129
00:07:38,530 --> 00:07:40,040
much easier to predict

130
00:07:40,050 --> 00:07:45,550
had some very nice structure to where there was never more than i think to

131
00:07:45,550 --> 00:07:48,210
parents like that we don't have an enough could turn to make it a little

132
00:07:48,210 --> 00:07:51,110
bit harder but each node has at most two apparent in the base that the

133
00:07:51,110 --> 00:07:55,480
actual bayesnet underneath but i don't know what to write access to learn which ones

134
00:07:55,490 --> 00:07:56,800
to listen to

135
00:07:56,810 --> 00:08:00,240
and we compared a number of different algorithms in this you know how how well

136
00:08:00,240 --> 00:08:03,460
can you make money in this artificial stock domain and then what can you buy

137
00:08:03,460 --> 00:08:08,720
well we have and one

138
00:08:08,820 --> 00:08:11,010
this is

139
00:08:14,520 --> 00:08:17,230
well there

140
00:08:17,280 --> 00:08:21,920
fraction of

141
00:08:26,460 --> 00:08:34,250
you can do

142
00:08:35,420 --> 00:08:38,360
i also agree

143
00:08:38,380 --> 00:08:41,150
it's all i there

144
00:08:41,160 --> 00:08:44,550
the problem here is that earth

145
00:08:55,700 --> 00:09:02,660
here for

146
00:09:12,550 --> 00:09:21,280
all of these approaches to

147
00:09:24,430 --> 00:09:30,180
o over here it's like twenty one

148
00:09:30,200 --> 00:09:33,440
and we all only

149
00:09:33,450 --> 00:09:38,020
are you on

150
00:09:45,290 --> 00:09:49,850
it was

151
00:09:49,870 --> 00:09:52,680
when you use

152
00:09:52,800 --> 00:09:55,580
early on

153
00:09:55,620 --> 00:09:58,010
there there are

154
00:09:58,040 --> 00:10:00,920
what the label

155
00:10:00,930 --> 00:10:06,590
one of the world

156
00:10:06,600 --> 00:10:10,240
even if you have a

157
00:10:10,470 --> 00:10:15,100
what does

158
00:10:20,440 --> 00:10:25,200
o one of

159
00:10:31,080 --> 00:10:34,640
these are also

160
00:10:34,650 --> 00:10:38,200
two well or four

161
00:10:41,510 --> 00:10:42,860
all the

162
00:10:42,860 --> 00:10:44,820
in was

163
00:10:44,950 --> 00:10:48,140
but they do

164
00:10:49,690 --> 00:10:50,230
so o

165
00:10:50,240 --> 00:10:54,390
well i always wondered why one

166
00:11:06,580 --> 00:11:09,330
i'm going to be

167
00:11:13,300 --> 00:11:23,680
this is the best

168
00:11:32,130 --> 00:11:33,980
on the face

169
00:11:48,660 --> 00:11:53,780
these are all the one

170
00:11:53,800 --> 00:11:59,090
what you

171
00:12:08,910 --> 00:12:13,950
all right

172
00:12:14,290 --> 00:12:15,360
able to

173
00:12:17,840 --> 00:12:20,100
all right

174
00:12:30,520 --> 00:12:33,570
i of these

175
00:12:36,020 --> 00:12:38,840
and again

176
00:12:38,840 --> 00:12:43,780
types of fuzzy norms for for disjunction functions you have s norms and the complement

177
00:12:43,780 --> 00:12:47,740
of s norms that t norms and used the right thing using de morgan's law

178
00:12:47,740 --> 00:12:50,760
but i don't think that's worth doing i've done that you don't you just for

179
00:12:50,770 --> 00:12:51,760
them in

180
00:12:51,880 --> 00:12:55,020
let me skip that just so you know there's things like t norms and s

181
00:12:55,020 --> 00:12:58,920
norms that are used if you want to formalise this idea of different base of

182
00:12:58,920 --> 00:13:00,140
combining fuzzy sets

183
00:13:01,560 --> 00:13:07,760
yes maybe interesting graph that shows you that some of these norms are sort of

184
00:13:07,860 --> 00:13:12,340
more extreme than others at the mean maximum again i have sort of plotted the

185
00:13:12,670 --> 00:13:16,560
degrees of membership for my sets a and b here on the x and y

186
00:13:16,560 --> 00:13:20,460
axis on the z axis we'll plot the the t norm

187
00:13:20,480 --> 00:13:23,820
the minimum norm in this case

188
00:13:24,740 --> 00:13:26,400
using using the minimum

189
00:13:26,560 --> 00:13:29,760
and the maximum norm over here is kind of see this sort of kind of

190
00:13:30,020 --> 00:13:34,160
a cutout piece of apparent apparently sort of the opposite piece and if you go

191
00:13:34,160 --> 00:13:39,180
from the product norm you can already see how the product norm squeezes this edge

192
00:13:39,200 --> 00:13:43,240
but little bit but of course it will need to observe the borderline cases it

193
00:13:43,320 --> 00:13:46,280
will need to observe the fact that this needs to be one and along this

194
00:13:46,280 --> 00:13:49,980
line definitely want an output of zero so those cases fixed but you can do

195
00:13:49,980 --> 00:13:53,580
something with the area in between so it's already got gotten squeezed down a little

196
00:13:53,580 --> 00:13:57,220
bit by using the product norm and this one pulled up a little bit by

197
00:13:58,600 --> 00:14:04,280
but but but some you know the further we go to country it's enormously into

198
00:14:04,300 --> 00:14:07,920
the book you'll find more about this just so this is just another definition of

199
00:14:07,920 --> 00:14:11,900
fuzzy norms you see how it's actually flattened out or they about here to gets

200
00:14:11,910 --> 00:14:15,020
pushed down even more the t norm the s norm gets pulled up even more

201
00:14:15,320 --> 00:14:20,440
appealing to apply to all of these one hundred per platform is zero and then

202
00:14:20,440 --> 00:14:25,080
we have very extreme norms that all they do is essentially follow the the constraints

203
00:14:25,080 --> 00:14:30,400
that defined at the beginning they're all that so the sort of the conjunction the

204
00:14:30,400 --> 00:14:34,380
drastic version of the convention is going to say for everything that i can but

205
00:14:34,380 --> 00:14:39,040
and not constrained otherwise by sort of the extreme of this up with europe and

206
00:14:39,040 --> 00:14:41,540
the same is true for the drastic sum is all is the one i in

207
00:14:41,540 --> 00:14:45,500
the case where we have no other choice so you have sort of very extreme

208
00:14:45,500 --> 00:14:49,040
versions of these norms that are very very critical new very soft versions of these

209
00:14:49,960 --> 00:14:52,340
well get

210
00:14:52,380 --> 00:14:56,000
and you can if you wanted you could sort of operator spectra is in the

211
00:14:56,000 --> 00:14:59,760
drastic product all the in the left drastic sum is all the way over here

212
00:14:59,820 --> 00:15:02,520
and then you can go bounded sum next meaning some in the middle so the

213
00:15:02,520 --> 00:15:08,380
mean not so very soft versions of these norms and the drastic product existence itself

214
00:15:08,420 --> 00:15:12,600
OK so i think the message is just there are many many many different ways

215
00:15:12,600 --> 00:15:15,800
of defining these fuzzy norms and it depends really what you want to do with

216
00:15:15,810 --> 00:15:19,220
the i'll get back to that later for the for the fuzzy rules for two

217
00:15:19,220 --> 00:15:25,540
different ways of using norms result in very different ways of interpreting what will this

218
00:15:26,520 --> 00:15:32,400
OK you have interesting effects you start thinking about that you you start doing things

219
00:15:32,400 --> 00:15:36,960
that you would expect results because you have seen that in on what we for

220
00:15:36,960 --> 00:15:41,280
example if you were conjunction a and the negation of a on

221
00:15:41,300 --> 00:15:42,860
that's tautology in

222
00:15:43,160 --> 00:15:44,320
williams stuff

223
00:15:44,360 --> 00:15:49,540
what be false right or down here a or not they ought to be true

224
00:15:49,600 --> 00:15:53,360
but if you think about the the base defined

225
00:15:53,380 --> 00:15:58,080
the negation you actually don't get that i you for example in this case i'm

226
00:15:58,080 --> 00:16:00,840
using the product norm for

227
00:16:00,860 --> 00:16:04,340
for the for the conjunction and i use one minus the degree of membership of

228
00:16:04,340 --> 00:16:08,720
a for the negation so we have the blue one is something of a the

229
00:16:08,720 --> 00:16:12,740
degree of membership of a the red one is the degree of membership a and

230
00:16:12,740 --> 00:16:18,920
then i just straightforward apply my conjunction operator actually get something that's not always are

231
00:16:18,940 --> 00:16:21,160
it is moderately odd

232
00:16:21,180 --> 00:16:24,300
so have some bizarre little effects like this one and the same is true for

233
00:16:25,600 --> 00:16:31,720
i you don't get all this challenge in this case i'm using the the maximum

234
00:16:31,720 --> 00:16:35,620
norm again the red line that you can hardly see is negation the blue line

235
00:16:35,620 --> 00:16:40,690
here is the original set a and the green line is a or not a

236
00:16:40,730 --> 00:16:44,500
degree of membership to the set of a or not going to get something that

237
00:16:44,500 --> 00:16:49,000
depends of course on the norm exceptions are different norm here they defining disjunction i

238
00:16:49,000 --> 00:16:51,380
would get a different result may not be able to

239
00:16:52,560 --> 00:16:58,140
that's something you should be aware of that leads to interesting little side effects for

240
00:16:58,140 --> 00:17:03,900
example if you arrange fuzzy points and i dimensional space you can actually not show

241
00:17:04,960 --> 00:17:10,220
the the triangular inequality holds but you can show is that it's not violent

242
00:17:10,240 --> 00:17:12,780
there's the great land in between

243
00:17:13,020 --> 00:17:18,400
OK fuzzy implication we go one step further i'm not interested only in

244
00:17:18,960 --> 00:17:21,900
sort of an or writers who also won two

245
00:17:21,980 --> 00:17:26,330
derive sort of an implication it can do that too involved many different ways the

246
00:17:26,330 --> 00:17:32,080
two different two interesting they are deriving it using r one tautology that comes from

247
00:17:32,270 --> 00:17:35,380
logic a implies b is not a or a b

248
00:17:35,400 --> 00:17:40,420
using for example in remarks normal results in something like this

249
00:17:40,460 --> 00:17:44,580
yeah essentially here we have the and operator in here

250
00:17:44,600 --> 00:17:50,480
and if there's one minus that's the not a in the months operators or operator

251
00:17:50,520 --> 00:17:52,440
and there's a very different way

252
00:17:52,480 --> 00:17:56,380
you start to think about challenge implication which is defined by using this is an

253
00:17:56,380 --> 00:18:02,360
application and then from there to derive disjunction and conjunction operators using these two tautologies

254
00:18:02,380 --> 00:18:07,900
why AM i even wondering about OK this is the results that i think we

255
00:18:07,900 --> 00:18:14,160
machine that missed the bit so it's obvious transmitting and then the fire and what

256
00:18:14,190 --> 00:18:20,070
you find the devices that are built into the the biometric passports which are basically

257
00:18:20,490 --> 00:18:26,510
cryptographic smart cards technology that has been used in like a pay-tv

258
00:18:26,530 --> 00:18:28,160
four four

259
00:18:28,200 --> 00:18:34,350
payment systems and so on are all kinds of applications for every smart smartcard always

260
00:18:34,350 --> 00:18:36,880
existed with context based interfaces

261
00:18:36,900 --> 00:18:37,870
and now they

262
00:18:38,290 --> 00:18:40,590
got basically the the

263
00:18:41,360 --> 00:18:44,420
i have received an additional radio frequency interface

264
00:18:44,470 --> 00:18:48,990
so in that case speaking about RFID ashura i mean it's still the same kind

265
00:18:48,990 --> 00:18:56,420
of our protocol some other systems use but the underlying i mean beside the communication

266
00:18:56,420 --> 00:18:58,660
channel devices actually complex

267
00:18:58,670 --> 00:19:06,620
to give you an idea what's inside these passports basically they contain usually something like

268
00:19:06,620 --> 00:19:11,600
an eighteen fifty one microcontroller some some derivative of that running up to

269
00:19:11,610 --> 00:19:18,560
we're somewhere between eight and twenty megahertz they have a higher very large numbers of

270
00:19:18,560 --> 00:19:23,810
water in in order to implement or accelerate hours a day five triple this implemented

271
00:19:23,810 --> 00:19:26,880
and so on so it's a fairly complex system on a chip

272
00:19:26,910 --> 00:19:32,510
it just has an interface that speaks a are probably and therefore you can you

273
00:19:32,510 --> 00:19:36,510
can put into the RFID category

274
00:19:36,530 --> 00:19:40,490
so let's let's have a little bit of a look at the readers which in

275
00:19:42,930 --> 00:19:47,490
in the standard documents always called coupling device which

276
00:19:47,500 --> 00:19:50,900
doesn't make the mistake of calling day even though they can write

277
00:19:50,920 --> 00:19:56,920
usually you connect them using some standard whatever interface might be serial port might be

278
00:19:56,920 --> 00:20:01,550
used be might be in some cases even ethernet and so on

279
00:20:01,560 --> 00:20:06,530
unfortunately there is not a single standard for any kind of RFID reader there is

280
00:20:06,530 --> 00:20:12,900
no standardisation of the harvard no standardisation of the protocol called that these devices talk

281
00:20:12,940 --> 00:20:16,870
and there is no standardization of any software API on on the computer is not

282
00:20:16,870 --> 00:20:18,240
even in the windows

283
00:20:19,050 --> 00:20:25,060
if you want to deploy system then you have a complete vendor lock-in situation to

284
00:20:25,060 --> 00:20:27,250
whoever provides assistance

285
00:20:27,260 --> 00:20:33,010
because the API is used the hardware interfaces and everything uses is completely different from

286
00:20:33,010 --> 00:20:35,240
one product to another product in the market

287
00:20:35,280 --> 00:20:44,490
this is just completely different from from most other it's a simple computer peripherals these

288
00:20:44,490 --> 00:20:49,190
days i mean imagine a world where every keyboards we could probably

289
00:20:49,220 --> 00:20:54,800
ridiculous even even smart card readers foucault contactless smart cards

290
00:20:54,800 --> 00:20:58,890
basically ninety nine percent of what you can buy in the market today conforms to

291
00:20:58,890 --> 00:21:05,200
one specification which is called the CCI g use PCC eighty and that's the standard

292
00:21:05,200 --> 00:21:10,000
interface and you have one single driver worked with almost any hardware but for RFID

293
00:21:10,010 --> 00:21:11,300
the way

294
00:21:11,310 --> 00:21:17,820
then if you look at the actual RF interfaces the number of different parameters that

295
00:21:17,830 --> 00:21:22,300
defined as i have been is obviously you have to low level parameters such as

296
00:21:22,300 --> 00:21:23,640
operating frequency

297
00:21:23,720 --> 00:21:28,470
modulation scheme and what's called operational principle

298
00:21:29,350 --> 00:21:35,420
frequency there are systems in all kinds of different frequency bands are systems operating at

299
00:21:35,420 --> 00:21:38,620
one hundred twenty five hundred thirty five kilohertz

300
00:21:38,640 --> 00:21:43,420
ten megahertz thirteen point five six megahertz

301
00:21:43,420 --> 00:21:48,550
in this UHF stuff hundred sixteen megahertz there's two point four gigahertz stuff and so

302
00:21:48,550 --> 00:21:52,370
on so any any possible our band that people can

303
00:21:52,520 --> 00:21:58,420
use of unlicensed people have invented RFID systems for

304
00:21:58,480 --> 00:22:04,850
modulation obviously is an important parameter and then there's what has gone variational principles

305
00:22:04,910 --> 00:22:09,930
because that's really something that is very different between these systems

306
00:22:09,950 --> 00:22:13,450
what's very common is called magnetic coupling

307
00:22:13,940 --> 00:22:20,730
magnetic coupling basically means you build something like a transformer between the reader and the

308
00:22:21,920 --> 00:22:29,790
over the years and it's basically to magnetic coils and the reader is inducing electric

309
00:22:30,710 --> 00:22:32,190
into the

310
00:22:32,210 --> 00:22:36,040
coil of the transformer and thereby powering transplant

311
00:22:36,050 --> 00:22:41,450
and that system is used for the one hundred twenty five thirty five kilohertz and

312
00:22:41,850 --> 00:22:44,140
thirteen point five six megawatts systems

313
00:22:45,210 --> 00:22:49,930
the one hundred twenty five kilowatts stuff is not really used that much anymore i

314
00:22:49,930 --> 00:22:55,130
think the only really popular application these days is for animal identification

315
00:22:55,140 --> 00:23:01,050
so i'm in many countries you can you can have your chip injected into the

316
00:23:01,050 --> 00:23:02,070
neck of your

317
00:23:02,080 --> 00:23:07,330
that dog or something if that that talk i think some countries even have mandatory

318
00:23:07,380 --> 00:23:13,190
save the targets lost and somebody finds him you can identify with dogs was the

319
00:23:13,210 --> 00:23:20,650
they're interesting even other things there are specific animal identification transponders that

320
00:23:20,660 --> 00:23:26,170
coles ingest and which i shape physically in a way that they stay in the

321
00:23:26,170 --> 00:23:35,840
stomach and not like it's really weird stuff now in any case what's very popular

322
00:23:35,840 --> 00:23:41,210
these days is the third one five six megahertz systems they are used for all

323
00:23:41,210 --> 00:23:47,920
kinds of contactless payment systems for access control for the biometric passports and so on

324
00:23:47,940 --> 00:23:54,180
so this an extremely popular as system or the frequency at the moment

325
00:23:54,540 --> 00:23:58,480
what's upcoming

326
00:23:58,730 --> 00:24:01,750
is basically what's called backscatter

327
00:24:01,770 --> 00:24:08,010
and in that case you're not using magnetic coupling and and some kind of magnetic

328
00:24:08,320 --> 00:24:12,130
transmission but use the electrical field and use very

329
00:24:12,150 --> 00:24:17,250
fairly high frequencies in the UHF range where

330
00:24:17,580 --> 00:24:23,300
actually send the poles and then some energy is reflected in electrical poles and then

331
00:24:23,680 --> 00:24:29,840
some some of that energy is reflected by by changing the amount of reflected energy

332
00:24:29,870 --> 00:24:32,130
some information can be called that

333
00:24:32,430 --> 00:24:39,180
this is really starts to get much more scary than the magnetic coupling systems because

334
00:24:39,180 --> 00:24:45,850
here you can easily arrange something like ten meters some systems probably up to one

335
00:24:45,850 --> 00:24:49,050
hundred meters of distance as long as you have line of sight

336
00:24:49,070 --> 00:24:54,300
and no not too many metal or other objects interviewing

337
00:24:54,310 --> 00:25:01,140
and what's what's even more scary is that this surface acoustic wave stuff which we

338
00:25:01,140 --> 00:25:06,480
use extremely low-power microwave signals to point four gigahertz or even beyond

339
00:25:06,500 --> 00:25:13,190
but those two systems you don't see that much yet the UHF backscatter stuff is

340
00:25:13,190 --> 00:25:14,770
mostly used for

341
00:25:14,790 --> 00:25:19,730
things like container identification in

342
00:25:19,730 --> 00:25:23,430
in any case is not telling us anything about the vertical localization of the of

343
00:25:23,430 --> 00:25:26,930
the person because we had only two microphones and not only informs you about the

344
00:25:26,930 --> 00:25:32,170
bearing from which the sound is coming but now fusing the sound proposals with sound

345
00:25:32,180 --> 00:25:36,970
likelihood and colour likelihood and you see you get a much tighter distribution this is

346
00:25:37,350 --> 00:25:43,600
the estimate of the mean of the posterior distribution computed from that from the particle

347
00:25:45,620 --> 00:25:46,620
and lastly

348
00:25:47,590 --> 00:25:49,270
a variation on

349
00:25:49,280 --> 00:25:52,370
particle filtering that uses kneeling

350
00:25:52,390 --> 00:25:57,230
introduced for monte carlo sampling by radford neal

351
00:25:58,470 --> 00:25:59,860
just before

352
00:25:59,910 --> 00:26:02,860
deutsches paper and then he put it into

353
00:26:03,350 --> 00:26:07,380
temporal form particle filtering and now what you see is

354
00:26:07,460 --> 00:26:10,960
the results of tracking in three views of the camera

355
00:26:10,970 --> 00:26:15,460
the outline of a living person with quite complex model sort of robotic

356
00:26:15,750 --> 00:26:19,680
animated model with joints and angles and all this kind of thing

357
00:26:20,550 --> 00:26:22,680
you can even step over this box

358
00:26:22,700 --> 00:26:24,080
and you see the

359
00:26:24,120 --> 00:26:32,260
motion is effectively track

360
00:26:37,350 --> 00:26:38,670
i've only got about

361
00:26:38,690 --> 00:26:41,970
and now the stuff left now

362
00:26:41,980 --> 00:26:44,150
and lunches in ten minutes so

363
00:26:46,210 --> 00:26:47,410
i'm just going to

364
00:26:47,430 --> 00:26:49,100
sort of

365
00:26:49,130 --> 00:26:51,960
it's on a few highlights of this

366
00:26:52,040 --> 00:26:55,800
this last section which is which you have in your notes at least part of

367
00:26:55,800 --> 00:26:57,320
it and by the way

368
00:26:57,330 --> 00:27:00,060
since i fiddled with the talks whole lot

369
00:27:00,080 --> 00:27:02,430
you know since i reconnect

370
00:27:02,570 --> 00:27:04,360
a month and a half ago

371
00:27:04,410 --> 00:27:09,120
output the new version of the talks on the on my website in this location

372
00:27:09,120 --> 00:27:10,260
when i get back

373
00:27:10,300 --> 00:27:12,330
to cambridge

374
00:27:12,340 --> 00:27:17,560
so you know we very thoroughly with console models the different kinds of state space

375
00:27:17,560 --> 00:27:23,470
the kalman filter for that approximate filters that work in more general settings

376
00:27:26,360 --> 00:27:31,320
you know capturing the contour of an object is not everything what about the appearance

377
00:27:31,320 --> 00:27:36,020
of the object seems we've rather neglected that

378
00:27:36,070 --> 00:27:39,380
and even before we get to modeling appearance of the

379
00:27:39,400 --> 00:27:46,070
and objects this parameter parametric consul thing itself is rather limited

380
00:27:46,120 --> 00:27:49,790
form of model because its topology is fixed so i mean

381
00:27:50,270 --> 00:27:53,540
you know if i e

382
00:27:53,550 --> 00:27:57,770
take a couple something and show it to you sideways and then turned to forward

383
00:27:57,770 --> 00:28:01,010
so the cup is facing you and you see the circular rim and so on

384
00:28:01,010 --> 00:28:05,300
the topology the outline of this this object changes and it's a a bit hard

385
00:28:05,300 --> 00:28:09,500
to see how we make that happen nicely with active contour models actually people try

386
00:28:09,540 --> 00:28:11,260
to do this using

387
00:28:11,310 --> 00:28:17,810
rather in the way that we did the juggling families of models with the discrete

388
00:28:18,400 --> 00:28:23,410
indicator variables associated with them in the markov chain to tell you about how how

389
00:28:23,410 --> 00:28:26,610
how you can move from one model to another but you know it gets very

390
00:28:26,610 --> 00:28:30,040
messy and actually building those models is a lot of work

391
00:28:30,050 --> 00:28:34,400
and somehow it seems messy way to go is there a more automatic way somehow

392
00:28:34,400 --> 00:28:36,840
of building those kinds of models

393
00:28:39,720 --> 00:28:41,070
because the topology

394
00:28:41,070 --> 00:28:43,170
actually it turns out that

395
00:28:43,230 --> 00:28:48,150
daimler daimler chrysler lab working very hard on this for pedestrian detection

396
00:28:48,170 --> 00:28:50,720
and they have these

397
00:28:50,800 --> 00:28:52,370
rather cute

398
00:28:55,090 --> 00:28:56,080
and that's in

399
00:28:56,110 --> 00:28:57,860
magazines like the economist

400
00:28:57,880 --> 00:29:01,710
making scary promises about what they going to be able to do by two thousand

401
00:29:01,710 --> 00:29:06,330
five and the the guy running allowed our value gavrila is someone that i know

402
00:29:06,460 --> 00:29:10,310
feeling sorry for him as two thousand five approaches he must be actually was going

403
00:29:10,310 --> 00:29:14,220
to take a sabbatical in cambridge and then mysteriously didn't come then it has anything

404
00:29:14,230 --> 00:29:17,880
to do the two thousand five deadline but anyway he's going to make

405
00:29:17,890 --> 00:29:23,310
protesters recognise on cars that will turn all of our streets into

406
00:29:23,310 --> 00:29:24,870
zebra crossings

407
00:29:24,890 --> 00:29:27,190
people separate crossings of meaning

408
00:29:27,210 --> 00:29:31,910
so it's going to be as safe as it as if there was ever crossings

409
00:29:31,910 --> 00:29:36,020
everywhere all the streets and

410
00:29:36,030 --> 00:29:37,780
sure enough you know the

411
00:29:37,810 --> 00:29:43,200
traffic incident statistics in the u are pretty horrific and a hundred and fifty five

412
00:29:43,200 --> 00:29:48,480
thousand per year of injuries to pedestrians many other kinds of injuries to more than

413
00:29:48,580 --> 00:29:52,340
a million but perhaps we could do something about

414
00:29:52,390 --> 00:29:55,450
these kinds of

415
00:29:55,500 --> 00:29:58,210
of accident if only we can detect pedestrians

416
00:29:58,230 --> 00:30:02,150
more accurate so that going to mount stereo vision systems on the front of the

417
00:30:02,150 --> 00:30:07,390
cars not expensive ones like these but highly engineered ones can be fitted perhaps into

418
00:30:09,750 --> 00:30:13,850
then when the driver is moving along the pedestrian walks out in front

419
00:30:13,890 --> 00:30:17,000
the system will detect it and bring

420
00:30:17,050 --> 00:30:18,860
the vehicle to a grinding halt

421
00:30:18,910 --> 00:30:23,040
yeah just missed it

422
00:30:23,050 --> 00:30:24,440
OK so the idea is to

423
00:30:27,900 --> 00:30:29,780
the parameterized contour model

424
00:30:30,620 --> 00:30:32,860
so families of templates

425
00:30:32,870 --> 00:30:35,190
and rather like

426
00:30:36,700 --> 00:30:40,730
and to match those templates too

427
00:30:41,400 --> 00:30:46,450
data and images so the idea is that you take in coming image and you

428
00:30:46,450 --> 00:30:50,930
do some low-level signal processing on it to get the edges the edges don't have

429
00:30:50,930 --> 00:30:53,360
to have total topological integrity

430
00:30:53,410 --> 00:30:56,390
they can have breaks in and missing bits and extra bits which is good because

431
00:30:56,390 --> 00:31:01,010
that's how real edge detectors work they can't really give you logical integrity however now

432
00:31:01,010 --> 00:31:05,590
when you take a template which i depicted here just as an outline but it

433
00:31:05,590 --> 00:31:06,660
can have internal

434
00:31:06,710 --> 00:31:09,820
lines as well one slide over that data

435
00:31:10,650 --> 00:31:13,140
there will be

436
00:31:13,160 --> 00:31:16,620
places where degrees in places where it doesn't and

437
00:31:16,620 --> 00:31:23,350
the whole business of actually making an observation likelihood for such objects is difficult

438
00:31:23,390 --> 00:31:26,300
and i'm not going to be able to go into that but suffice it to

439
00:31:26,300 --> 00:31:27,920
say you can make and model

440
00:31:27,940 --> 00:31:31,030
which is a whole family of such templates in the beauty is that you can

441
00:31:32,020 --> 00:31:39,710
training data and pick up all of these families of templates and you simply

442
00:31:39,800 --> 00:31:43,090
make a for

443
00:31:43,100 --> 00:31:48,350
gather ten thousand frames let's say and get the and do it in reasonably good

444
00:31:48,350 --> 00:31:51,900
condition so we perhaps in the in an area where you know the colour of

445
00:31:51,900 --> 00:31:56,170
the background so that for this training phase you can simplify the data removing the

446
00:31:56,170 --> 00:31:58,380
background and just get the

447
00:31:59,310 --> 00:32:03,100
templates that belong to the moving object and now you can do some statistics on

448
00:32:03,100 --> 00:32:09,350
these templates to form clusters and even distributions around the over the clusters well that's

449
00:32:09,350 --> 00:32:10,400
more difficult

450
00:32:10,420 --> 00:32:15,100
why is it difficult because these templates are not vector objects they don't live in

451
00:32:15,100 --> 00:32:19,460
a vector space you can't there's no obvious meaning to having two templates together so

452
00:32:19,460 --> 00:32:25,770
you rather limited in the algebra you can do the templates and that makes

453
00:32:25,790 --> 00:32:27,080
modeling difficult

454
00:32:30,310 --> 00:32:31,980
what does this

455
00:32:31,980 --> 00:32:33,530
all right

456
00:32:35,900 --> 00:32:38,100
right here

457
00:32:38,930 --> 00:32:42,670
i was

458
00:32:55,270 --> 00:33:00,410
all of

459
00:33:18,350 --> 00:33:20,760
we are

460
00:34:57,350 --> 00:35:01,010
and if you were

461
00:37:38,600 --> 00:37:43,610
i think they

462
00:38:08,490 --> 00:38:14,720
you know

463
00:38:21,010 --> 00:38:25,490
so my

464
00:38:39,070 --> 00:38:44,290
on the other

465
00:38:44,420 --> 00:38:48,030
that's all

466
00:39:34,110 --> 00:39:38,780
they are

467
00:40:17,250 --> 00:40:27,310
so the whole

468
00:40:34,570 --> 00:40:39,980
right on

469
00:40:56,540 --> 00:41:00,560
you know

470
00:41:01,170 --> 00:41:10,520
if you have a rule

471
00:41:22,280 --> 00:41:25,350
the world

472
00:41:25,370 --> 00:41:27,960
we want

473
00:41:44,250 --> 00:41:46,510
we already are

474
00:41:46,550 --> 00:41:48,940
one monroe

475
00:41:48,940 --> 00:41:51,640
share of employment in the country

476
00:41:51,750 --> 00:41:53,160
that is in

477
00:41:53,180 --> 00:41:56,900
low-wage jobs private consumer services is

478
00:41:56,910 --> 00:41:59,650
all kinds of services that

479
00:41:59,680 --> 00:42:00,970
we regularly

480
00:42:00,980 --> 00:42:03,630
consume from shoe polishing

481
00:42:03,710 --> 00:42:05,700
two here cutting

482
00:42:05,710 --> 00:42:09,100
and this kind of services typically jobs that are

483
00:42:09,130 --> 00:42:11,080
very badly paid

484
00:42:11,130 --> 00:42:14,450
except if you have presented some relief for

485
00:42:14,470 --> 00:42:16,950
when building work

486
00:42:18,770 --> 00:42:20,720
and the idea here

487
00:42:20,730 --> 00:42:23,310
and this is taken from an article by

488
00:42:23,320 --> 00:42:28,350
lincoln would be in comparative political studies two thousand three

489
00:42:28,390 --> 00:42:29,620
ideas that

490
00:42:29,630 --> 00:42:32,270
the higher the replacement rate

491
00:42:32,330 --> 00:42:34,730
the lower we expect

492
00:42:34,770 --> 00:42:37,890
employment in private consumer services to be

493
00:42:37,950 --> 00:42:40,040
why should it be the case

494
00:42:42,140 --> 00:42:43,730
you get

495
00:42:43,810 --> 00:42:44,540
a lot

496
00:42:44,560 --> 00:42:48,040
so let's say one hundred percent of the previous job

497
00:42:49,400 --> 00:42:53,830
please for a while you're not going to bother about taking on the job

498
00:42:53,830 --> 00:42:56,760
that's less well paid

499
00:42:56,810 --> 00:42:59,950
which is to job in this area

500
00:42:59,950 --> 00:43:02,680
so the high replacement rate

501
00:43:02,690 --> 00:43:08,020
the lower will be demand for employment in this sector

502
00:43:09,190 --> 00:43:10,200
the lower jaw

503
00:43:10,210 --> 00:43:12,000
will be filled

504
00:43:12,010 --> 00:43:13,370
as a consequence

505
00:43:13,380 --> 00:43:17,530
the less chance will be offered

506
00:43:17,880 --> 00:43:23,340
i will dig into the substantive relationship a bit

507
00:43:24,430 --> 00:43:30,310
when we talk about functional forms and conditional effects for the moment i would just

508
00:43:31,430 --> 00:43:35,520
to highlight the straight line

509
00:43:35,530 --> 00:43:37,220
which tells us

510
00:43:37,250 --> 00:43:39,100
if we move at

511
00:43:39,210 --> 00:43:44,120
a certain distance on our independent variable being the replacement rate

512
00:43:44,150 --> 00:43:45,900
we expect

513
00:43:45,950 --> 00:43:54,390
employment in the private consumer services to go down by a certain number of steps

514
00:43:54,450 --> 00:43:56,210
here we are right in the middle

515
00:43:57,870 --> 00:44:01,410
the core issue not regression analysis

516
00:44:01,450 --> 00:44:03,830
because what i've done is

517
00:44:03,950 --> 00:44:05,720
made a causal claims

518
00:44:05,770 --> 00:44:11,380
until during a nice little story wide is called plane might be

519
00:44:15,270 --> 00:44:18,840
i've made a connection between two variables

520
00:44:18,910 --> 00:44:19,760
i said

521
00:44:21,340 --> 00:44:22,400
the country

522
00:44:22,430 --> 00:44:25,580
shifts let's say from point to point four

523
00:44:25,590 --> 00:44:27,060
and the replacement rate

524
00:44:27,100 --> 00:44:31,220
then we expect employment good down by

525
00:44:32,060 --> 00:44:33,710
it would be

526
00:44:33,720 --> 00:44:35,310
something like

527
00:44:35,320 --> 00:44:37,820
four percentage points

528
00:44:37,870 --> 00:44:40,270
which is why the bot

529
00:44:40,330 --> 00:44:41,260
so it's

530
00:44:41,270 --> 00:44:44,190
in standard terms this is a quite substantive

531
00:44:48,060 --> 00:44:49,850
we draw a straight line

532
00:44:49,870 --> 00:44:51,440
through data cloud

533
00:44:51,500 --> 00:44:54,020
that represents

534
00:44:54,070 --> 00:44:56,410
two dimensional

535
00:44:56,430 --> 00:44:58,520
great of

536
00:44:58,960 --> 00:45:01,450
combinations of

537
00:45:01,460 --> 00:45:03,360
values for variables

538
00:45:04,720 --> 00:45:07,880
here we have case that has

539
00:45:07,900 --> 00:45:16,880
o point fifteen something like that for replacement rate and ten percent for employment

540
00:45:16,940 --> 00:45:19,710
and with many other such

541
00:45:23,060 --> 00:45:28,120
and we want to get this straight line representing summarizing

542
00:45:28,180 --> 00:45:32,410
the art of summarizing relationships so which summarizes the relationship

543
00:45:32,430 --> 00:45:37,500
between horizontal and the vertical axis order variables represented by the

544
00:45:37,510 --> 00:45:42,060
the horizontal and the vertical axis

545
00:45:42,100 --> 00:45:44,210
as good as we can

546
00:45:44,250 --> 00:45:47,710
you got the criteria as good as we can

547
00:45:47,760 --> 00:45:50,250
was been quite a lot of time

548
00:45:50,280 --> 00:45:52,340
figuring out what it means

549
00:45:52,350 --> 00:45:54,800
to be as good as can

550
00:45:54,810 --> 00:46:01,580
now look at this one is this good

551
00:46:01,630 --> 00:46:04,200
well it got published hence it must be good

552
00:46:04,210 --> 00:46:13,790
however there are few things that are annoying as we would expect

553
00:46:15,220 --> 00:46:17,550
the line represents the data out

554
00:46:19,010 --> 00:46:21,580
what does it mean that we expect

555
00:46:25,510 --> 00:46:30,850
points to be close to the regression line

556
00:46:30,870 --> 00:46:36,370
we can count those points that are close to the regression line and without going

557
00:46:36,370 --> 00:46:37,820
into details

558
00:46:37,830 --> 00:46:43,480
here we got the ninety five percent confidence intervals i will talk about and tomorrow

559
00:46:44,580 --> 00:46:47,460
that's a that's we argue that's close enough

560
00:46:47,500 --> 00:46:50,660
if it's within the ninety five percent confidence intervals

561
00:46:50,780 --> 00:46:55,840
it's not the majority of points here

562
00:46:55,850 --> 00:47:00,910
and there are other things that are going as it

563
00:47:00,930 --> 00:47:03,330
if four point three

564
00:47:03,400 --> 00:47:06,120
we got lots and lots of points

565
00:47:06,180 --> 00:47:08,580
on the two dimensional grid

566
00:47:10,500 --> 00:47:14,150
the cover quite a wide range

567
00:47:14,220 --> 00:47:19,610
the same year

568
00:47:20,710 --> 00:47:24,540
so apparently their values for our independent variables

569
00:47:24,540 --> 00:47:26,700
and t applied to we have k

570
00:47:26,720 --> 00:47:29,230
that's the areas

571
00:47:29,290 --> 00:47:30,520
and so is enough

572
00:47:30,520 --> 00:47:34,450
mean specifying your favourite procedure to do

573
00:47:34,470 --> 00:47:37,000
they tradition in approximately a

574
00:47:37,060 --> 00:47:41,520
we can just spotted is just a creation

575
00:47:41,520 --> 00:47:43,160
genetic recursion

576
00:47:43,160 --> 00:47:46,100
then we are looking at the edit data process

577
00:47:46,120 --> 00:47:47,310
and what you

578
00:47:47,330 --> 00:47:48,370
is that

579
00:47:48,370 --> 00:47:50,520
in this approximate procedures

580
00:47:50,540 --> 00:47:52,000
you will be able to

581
00:47:52,040 --> 00:47:53,640
control the errors

582
00:47:53,660 --> 00:47:58,810
and what you're looking for is that if you're controlling the atmosphere controlling the absolutes

583
00:47:58,870 --> 00:48:03,720
user powerful functionapproximator regression procedure by the

584
00:48:03,720 --> 00:48:05,720
are many many samples

585
00:48:05,730 --> 00:48:07,620
so you're controlling the errors

586
00:48:07,660 --> 00:48:11,500
what you hope is that eventually you will be able to control the

587
00:48:11,560 --> 00:48:12,700
the and or

588
00:48:12,700 --> 00:48:15,470
approximating the star

589
00:48:15,520 --> 00:48:19,450
in the long run so you this many many times that's

590
00:48:19,450 --> 00:48:24,830
and the question is if you are getting close in a small vicinity at least

591
00:48:24,830 --> 00:48:26,850
i don't know

592
00:48:26,870 --> 00:48:27,790
and so

593
00:48:27,810 --> 00:48:30,060
the next attempt to seize that so

594
00:48:30,060 --> 00:48:32,990
if you can control the supremum at r

595
00:48:33,000 --> 00:48:38,730
after all the error terms in all the variations use large function for so what

596
00:48:38,990 --> 00:48:40,180
i don't know

597
00:48:40,180 --> 00:48:44,140
we talk about that but what you should you should do and then you can

598
00:48:44,140 --> 00:48:47,100
hope to control those editors

599
00:48:48,790 --> 00:48:51,700
imagine that there is controlled so

600
00:48:51,720 --> 00:48:56,350
in other words the maximum of all these editors just bonded by a single value

601
00:48:56,450 --> 00:48:59,560
perhaps that's maybe a small that

602
00:48:59,660 --> 00:49:01,230
then you can conclude

603
00:49:02,180 --> 00:49:06,930
if you take the distance between the k and signed the limit

604
00:49:06,990 --> 00:49:11,250
this distance is not going to be large and three times the government steps along

605
00:49:11,250 --> 00:49:14,290
divided by one minus

606
00:49:14,310 --> 00:49:15,730
so that's good news strength

607
00:49:16,600 --> 00:49:19,350
then if you're making a similar smaller

608
00:49:19,410 --> 00:49:21,580
then the difference between

609
00:49:21,580 --> 00:49:23,730
yeah it rates in the long run

610
00:49:23,790 --> 00:49:25,180
and we start

611
00:49:25,200 --> 00:49:26,370
i'm going to

612
00:49:26,410 --> 00:49:28,640
be smaller space

613
00:49:28,660 --> 00:49:30,970
this is what we are looking for

614
00:49:31,080 --> 00:49:32,910
should be given because

615
00:49:32,930 --> 00:49:34,970
so at the end OK what you

616
00:49:35,000 --> 00:49:38,390
you will have this just you pick some of these traits and you compute the

617
00:49:38,390 --> 00:49:41,430
greedy policy with respect to the tree and and

618
00:49:41,450 --> 00:49:43,560
well if you have this guarantee

619
00:49:43,620 --> 00:49:47,720
you can plug in this grant he was our previous state and said that the

620
00:49:47,720 --> 00:49:49,430
banner of some

621
00:49:49,450 --> 00:49:51,540
guy is small enough

622
00:49:51,580 --> 00:49:56,060
then the value of the policy is close enough to restart close enough to be

623
00:49:57,100 --> 00:49:58,850
we had the previous result

624
00:49:58,910 --> 00:50:02,620
and then there is this other property that in the small vicinity of the optimal

625
00:50:02,680 --> 00:50:05,910
value function the banner to always

626
00:50:05,950 --> 00:50:09,080
this is against just some continuity property

627
00:50:09,140 --> 00:50:13,310
so i'm not going to get into groups the proof is very easy so it's

628
00:50:15,890 --> 00:50:17,600
so that's one way out of

629
00:50:17,640 --> 00:50:24,370
thinking about what if extending the arrogance too large state spaces large action spaces

630
00:50:24,370 --> 00:50:29,520
one problem is that we don't really have a good idea about how to control

631
00:50:29,520 --> 00:50:32,200
the supremum federer's in chennai

632
00:50:32,250 --> 00:50:37,330
so you'd better control after hours and not simply on all matters so when lauren

633
00:50:37,460 --> 00:50:40,810
harris could be very sensitive to lack of samples

634
00:50:40,810 --> 00:50:44,370
at some part of the state space if you don't go to some part of

635
00:50:44,370 --> 00:50:49,120
the state space you don't have a very good way of estimating anything that i

636
00:50:49,160 --> 00:50:51,660
so the supreme power could be really bad

637
00:50:51,680 --> 00:50:55,040
and maybe you have a good reason to avoid some parts of the state space

638
00:50:55,040 --> 00:50:59,580
because those parts are important but you're supporting matter is going to be large there

639
00:50:59,600 --> 00:51:04,330
but why why should go to those unimportant part so it's not clear that

640
00:51:04,330 --> 00:51:08,500
controlling the supply on an is a good idea so

641
00:51:08,540 --> 00:51:12,930
people one and we consider and the is

642
00:51:12,950 --> 00:51:18,970
that builds on the on expansion operators so another way of thinking about this approximate

643
00:51:20,370 --> 00:51:21,470
is that

644
00:51:22,250 --> 00:51:26,430
let's let's sing that we have a really huge

645
00:51:27,390 --> 00:51:28,640
state space

646
00:51:28,640 --> 00:51:33,560
if you really hit state space you cannot hope to represent all values you cannot

647
00:51:33,580 --> 00:51:38,390
you don't you don't you're value functions really functions of infinite space

648
00:51:38,390 --> 00:51:41,580
i know vectors anymore unfortunately

649
00:51:41,640 --> 00:51:45,640
so how do we represent functions so infinite spaces

650
00:51:45,660 --> 00:51:49,680
using your network for what can i regression procedures

651
00:51:49,730 --> 00:51:50,970
or whatever

652
00:51:51,020 --> 00:51:54,560
right so you use that function approximator

653
00:51:54,620 --> 00:51:57,370
so when you are using the function approximator

654
00:51:57,390 --> 00:52:02,720
and i reiterate that you will have to leave sierra functions so what happens

655
00:52:02,790 --> 00:52:06,640
if you want is that you're trying to compute this band of that you apply

656
00:52:06,680 --> 00:52:07,970
the operator

657
00:52:08,020 --> 00:52:10,270
and they project on hold back

658
00:52:10,330 --> 00:52:12,000
two the function space

659
00:52:12,020 --> 00:52:13,220
so one

660
00:52:13,230 --> 00:52:14,450
but the colors

661
00:52:14,450 --> 00:52:17,060
instantiation of this dismantled

662
00:52:17,060 --> 00:52:19,410
so how much time there

663
00:52:24,350 --> 00:52:27,970
so one particular sensation of this matter

664
00:52:27,970 --> 00:52:33,560
is let's imagine that this is the state space is a one into riots began

665
00:52:34,950 --> 00:52:37,430
you compute tf vk

666
00:52:37,430 --> 00:52:39,350
some magical way

667
00:52:39,350 --> 00:52:42,310
but of course you can ask for these functions

668
00:52:42,330 --> 00:52:47,330
actually you can actually come to this function what you can typically culture is

669
00:52:48,310 --> 00:52:55,430
its values at certain policies take a few points

670
00:52:55,430 --> 00:52:56,970
and you can't use

671
00:52:57,020 --> 00:53:01,450
the values of after the k and those spots

672
00:53:01,560 --> 00:53:05,890
and then what you do is that you use your function approximator

673
00:53:05,910 --> 00:53:07,080
to come up

674
00:53:07,100 --> 00:53:09,060
this function

675
00:53:09,080 --> 00:53:12,850
may be go through this line so maybe you consider this that is visible that

676
00:53:12,850 --> 00:53:15,470
are so you just use and interpolate

677
00:53:18,160 --> 00:53:20,140
so this is the function

678
00:53:20,180 --> 00:53:22,270
that you will come up

679
00:53:22,270 --> 00:53:25,340
if you think about what i'm was doing that

680
00:53:35,410 --> 00:53:39,240
i do still feel bad about having been unable to

681
00:53:39,250 --> 00:53:42,390
o that is

682
00:53:42,440 --> 00:53:44,430
illustrating how

683
00:53:44,450 --> 00:53:47,780
drawing a tighter control rather than just the rectangle

684
00:53:47,880 --> 00:53:49,480
some kind of compromise

685
00:53:49,490 --> 00:53:53,360
get a lot more right now the only thing wrong is the blue line the

686
00:53:53,360 --> 00:53:55,700
transparent either because in

687
00:53:55,830 --> 00:53:59,050
now the user has done is you know how to draw curves

688
00:53:59,060 --> 00:54:00,030
which is annoying

689
00:54:00,050 --> 00:54:00,950
well you know

690
00:54:01,350 --> 00:54:06,130
but perhaps slightly you could draw visitors staying outside rather than having to travel

691
00:54:06,180 --> 00:54:08,760
inside and outside

692
00:54:08,830 --> 00:54:10,360
one compromise

693
00:54:10,380 --> 00:54:14,000
and as i said i do still feel bad about the

694
00:54:14,050 --> 00:54:16,990
about having done the binary alpha zero

695
00:54:18,550 --> 00:54:23,520
here is a kind of hybrid idea i hybrid methods i know but you know

696
00:54:23,520 --> 00:54:25,520
better than the

697
00:54:26,940 --> 00:54:30,350
so here's the hard segmentation we have in the

698
00:54:30,360 --> 00:54:31,510
from what

699
00:54:31,570 --> 00:54:37,180
image and our reasonable thing to do for limited transparency the kind of transparency might

700
00:54:37,190 --> 00:54:39,210
friend in the middle suggesting that

701
00:54:39,250 --> 00:54:42,490
associated with the program aliasing in graphics

702
00:54:42,540 --> 00:54:47,040
you know it's the reason we want transparency is really to implement and you see

703
00:54:47,040 --> 00:54:48,940
that kind of thing because

704
00:54:50,290 --> 00:54:53,130
construct automatically try

705
00:54:53,160 --> 00:54:58,680
is simply an expansion of the boundary has been found automatically by doing inference and

706
00:55:01,600 --> 00:55:04,750
so segmentation using alpha

707
00:55:04,770 --> 00:55:06,890
you calling it out

708
00:55:06,900 --> 00:55:12,650
most i called x graphics they call this thing out depending you know where i

709
00:55:12,650 --> 00:55:18,640
snapped the slide from the time have occasionally got out that we have important because

710
00:55:18,640 --> 00:55:22,520
later on in the talk about something called alpha expansion and the opening of the

711
00:55:22,520 --> 00:55:26,890
mine has nothing whatsoever to do with how everybody wants to use out for everything

712
00:55:26,890 --> 00:55:29,120
is very popular

713
00:55:29,510 --> 00:55:33,650
so anyway this this try map is automatically constructed

714
00:55:35,380 --> 00:55:37,840
now we we can afford to do

715
00:55:39,610 --> 00:55:44,340
inference of but with transparency restricted to this

716
00:55:44,380 --> 00:55:48,490
three the extra constraints on keeping that we can narrow is enough

717
00:55:48,500 --> 00:55:52,070
two inhibitor of the artifacts so now

718
00:55:52,120 --> 00:55:55,660
using actually the same

719
00:55:57,710 --> 00:55:59,450
prior as before

720
00:56:00,520 --> 00:56:03,500
allowing the output very

721
00:56:03,510 --> 00:56:06,190
between zero and one

722
00:56:06,450 --> 00:56:11,960
is really quite possible result crazy things are and the is and that both his

723
00:56:12,000 --> 00:56:15,000
realistic looking soft is good when you do

724
00:56:15,050 --> 00:56:19,560
matching one thing on another you expect sort of it the wrong it is hard

725
00:56:20,050 --> 00:56:23,150
and also deals with some extent with the

726
00:56:23,210 --> 00:56:26,110
remaining aliasing artifacts

727
00:56:26,160 --> 00:56:28,770
and the way it's done actually is

728
00:56:28,820 --> 00:56:31,250
by running a dynamic programming

729
00:56:32,690 --> 00:56:37,320
on a an energy function that is now restricted to this report

730
00:56:37,370 --> 00:56:38,370
and the

731
00:56:38,380 --> 00:56:40,430
parameters of the energy function

732
00:56:40,450 --> 00:56:44,090
our parameterisation of this profile

733
00:56:44,140 --> 00:56:46,080
the transition from

734
00:56:46,090 --> 00:56:48,990
total acid the trip transparency

735
00:56:49,060 --> 00:56:52,550
which is running along the normal to the

736
00:56:52,560 --> 00:56:53,670
the river

737
00:56:53,680 --> 00:56:54,790
so now

738
00:56:55,050 --> 00:56:58,490
each you know this but up into many many

739
00:56:58,560 --> 00:57:01,630
segments each segment gets the pair parameters

740
00:57:01,640 --> 00:57:05,570
for the for the width of this profile and the position of the fans transition

741
00:57:05,630 --> 00:57:08,120
now you dynamic programming

742
00:57:08,370 --> 00:57:11,780
along the contour on those two very your not then

743
00:57:11,790 --> 00:57:13,540
programming is

744
00:57:13,550 --> 00:57:15,840
everybody programming get here

745
00:57:15,890 --> 00:57:21,070
you know very sophisticated educated but

746
00:57:21,120 --> 00:57:26,870
the fitted profile that

747
00:57:26,880 --> 00:57:29,750
thank you

748
00:57:29,800 --> 00:57:33,190
and you see even works to some extent these kind of all

749
00:57:33,240 --> 00:57:40,000
every situation providing the with of this region hair is not more than was that

750
00:57:40,100 --> 00:57:44,030
the present the you with the right

751
00:57:44,700 --> 00:57:50,200
OK so that's that's it for

752
00:57:50,210 --> 00:57:56,090
the this problem of inference of foreground and background so you probably want to talk

753
00:57:56,090 --> 00:57:57,070
about before

754
00:57:57,080 --> 00:58:01,110
we get on two algorithms which will probably be my next time

755
00:58:01,150 --> 00:58:03,090
is stereo matching

756
00:58:03,100 --> 00:58:07,150
you know why is stereo matching the problem the right kind to be treated

757
00:58:07,160 --> 00:58:08,590
in this framework

758
00:58:08,600 --> 00:58:10,820
so in summary you

759
00:58:10,850 --> 00:58:12,790
quite i you will

760
00:58:12,840 --> 00:58:16,620
prior how many actually from this institute

761
00:58:16,660 --> 00:58:21,120
the government if actually from the max planck

762
00:58:22,530 --> 00:58:23,640
otherwise i

763
00:58:23,730 --> 00:58:28,810
this is like family hold back the visitors visit to get this through

764
00:58:28,860 --> 00:58:29,830
i see

765
00:58:29,840 --> 00:58:32,240
alright OK go guys

766
00:58:33,130 --> 00:58:36,790
so you know you lost none of you from the fact you're

767
00:58:36,840 --> 00:58:38,850
and i mean you from

768
00:58:38,930 --> 00:58:40,280
max plank

769
00:58:40,290 --> 00:58:44,890
cybernetics like environment or you know you do cybernetics

770
00:58:44,900 --> 00:58:48,820
i thought i was making rope

771
00:58:50,150 --> 00:58:51,330
you do

772
00:58:51,380 --> 00:58:52,320
do you do

773
00:58:52,400 --> 00:58:55,320
human psychology

774
00:58:55,460 --> 00:58:57,180
one does

775
00:58:58,210 --> 00:59:00,920
do you do physiology

776
00:59:02,050 --> 00:59:03,060
do you do

777
00:59:05,110 --> 00:59:06,920
you all do

778
00:59:07,200 --> 00:59:10,210
should have asked this beginning to make of

779
00:59:10,310 --> 00:59:11,910
OK all

780
00:59:16,170 --> 00:59:20,030
have you seen one of these or perhaps about ten years ago everybody was sending

781
00:59:20,030 --> 00:59:21,890
them around in a christmas card

782
00:59:21,940 --> 00:59:26,900
and the one person in the audience who does psychology visual psychology

783
00:59:26,930 --> 00:59:30,920
if so here we have to do this i think which is

784
00:59:30,970 --> 00:59:32,260
we can what you do

785
00:59:34,680 --> 00:59:38,410
cross view exactly yes so you have to cross your eyes

786
00:59:38,450 --> 00:59:40,510
you know with a natural way

787
00:59:40,760 --> 00:59:45,140
in order to see the stereo pair there are actually two images in in this

788
00:59:45,140 --> 00:59:46,730
image is what they

789
00:59:46,740 --> 00:59:52,330
and school wallpaper that if you've woken up in a bedroom with highly patterned wallpaper

790
00:59:52,760 --> 00:59:54,630
after a hard night out

791
00:59:54,650 --> 00:59:58,100
actually never mind the pattern will like that

792
00:59:58,190 --> 01:00:02,080
and you see the pattern kind of you know comes before you leave for the

793
01:00:02,250 --> 01:00:06,430
kind of hovering in the ghostly plain way in front of the wall because actually

794
01:00:06,440 --> 01:00:09,660
your eyes not looking at the same thing in this i am looking at this

795
01:00:10,560 --> 01:00:12,600
and this besides looking at this flower

796
01:00:12,640 --> 01:00:15,550
and so i was gonna be

797
01:00:17,630 --> 01:00:21,420
now we just need to do it without the help of any alcohol to do

798
01:00:22,190 --> 01:00:23,780
to do this deliberately

799
01:00:24,370 --> 01:00:28,390
what if you've never had any does anybody know how to do you not remember

800
01:00:28,510 --> 01:00:33,760
christmas card but also stereograms what the centre and christmas cards and you deliberately cross

801
01:00:33,760 --> 01:00:38,030
your eyes like this when you look at the christmas card in this three-dimensional leaps

802
01:00:38,030 --> 01:00:41,170
out you know i promise you there is a three-dimensional things

803
01:00:41,170 --> 01:00:43,680
times y minus p

804
01:00:48,560 --> 01:00:49,790
i guess here

805
01:00:49,810 --> 01:00:53,980
well we would do is

806
01:00:54,020 --> 01:00:56,510
compute p

807
01:00:59,340 --> 01:01:02,300
we get the training example

808
01:01:02,340 --> 01:01:06,690
we compute the probability p according to the current model

809
01:01:06,690 --> 01:01:07,990
and then

810
01:01:09,420 --> 01:01:13,740
four parameter beta zero to parameter beta d

811
01:01:13,800 --> 01:01:16,450
we update the parameter

812
01:01:16,450 --> 01:01:19,190
and we update the parameters by

813
01:01:19,240 --> 01:01:22,730
adding by adding something to it

814
01:01:22,730 --> 01:01:30,040
which is the step size

815
01:01:30,060 --> 01:01:31,460
and then

816
01:01:32,660 --> 01:01:37,520
partial derivatives

817
01:01:37,580 --> 01:01:42,930
and more than now

818
01:01:42,930 --> 01:01:48,600
and after a done this loop of the baby jays' we've finished learning from this

819
01:01:48,600 --> 01:01:49,750
example x

820
01:01:49,920 --> 01:01:54,310
and we can move on and learn from the next example x

821
01:01:55,540 --> 01:01:57,530
OK so

822
01:01:57,670 --> 01:02:00,070
the reason it's called

823
01:02:00,090 --> 01:02:05,000
so the x question so the algorithm itself

824
01:02:05,030 --> 01:02:07,670
most implementations they are the

825
01:02:07,690 --> 01:02:10,210
itself and not stochastic in any way

826
01:02:10,260 --> 01:02:13,400
it's called stochastic gradient descent because

827
01:02:13,400 --> 01:02:14,320
if you

828
01:02:15,780 --> 01:02:19,150
think of this example axes being a random sample

829
01:02:19,170 --> 01:02:20,900
from the whole training set

830
01:02:20,980 --> 01:02:24,530
and then this is the derivative

831
01:02:24,590 --> 01:02:29,130
with respect to the single examples of the same example a random sample

832
01:02:29,150 --> 01:02:33,880
then this is sort of a randomized approximation to the true derivative

833
01:02:34,610 --> 01:02:36,530
the average

834
01:02:36,530 --> 01:02:41,610
the average right-hand side here with equal to true derivative

835
01:02:41,610 --> 01:02:44,170
but each individual right-hand side here

836
01:02:44,190 --> 01:02:49,400
is just a random approximation of the true derivative and that's why it's called mister

837
01:02:49,400 --> 01:02:51,610
is called stochastic gradient ascent so

838
01:02:52,030 --> 01:02:58,050
it's as and according to a stochastic approximation to the gradient instead of as and

839
01:02:58,050 --> 01:02:59,980
according to the

840
01:03:00,000 --> 01:03:01,480
exact gradient

841
01:03:01,860 --> 01:03:08,280
the room

842
01:03:15,030 --> 01:03:17,380
he fell

843
01:03:17,460 --> 01:03:19,170
yes yes and no

844
01:03:19,980 --> 01:03:25,030
among the the application of this personal experience with

845
01:03:25,090 --> 01:03:30,070
among all involved training sets that are in fact finite and fixed

846
01:03:30,110 --> 01:03:35,960
and then the thing that you always wanted to heuristically is sort those into a

847
01:03:35,960 --> 01:03:37,440
random order

848
01:03:37,480 --> 01:03:40,320
so that

849
01:03:40,340 --> 01:03:47,340
so so so that each prefix of the stream is representative of the whole stream

850
01:03:47,360 --> 01:03:52,670
and if

851
01:03:53,400 --> 01:03:57,530
and you want to be able to throw away each x after you've trained from

852
01:04:03,000 --> 01:04:06,380
then i think

853
01:04:06,440 --> 01:04:10,460
to get some probabilistic guarantees you do need to have some assumptions about how your

854
01:04:10,460 --> 01:04:12,740
training data coming in

855
01:04:14,260 --> 01:04:15,860
in random order

856
01:04:15,920 --> 01:04:21,170
but that's a thing just a special case of the assumption

857
01:04:21,190 --> 01:04:25,690
so normally we make the assumption that training data representative of test data

858
01:04:25,780 --> 01:04:27,630
and i was talking about how here

859
01:04:27,630 --> 01:04:31,300
you don't need to say you seem the axes are representative of the training ex

860
01:04:31,300 --> 01:04:36,900
is represented the need to assume that dx two y mapping stays fixed

861
01:04:39,800 --> 01:04:43,960
if your training data stream

862
01:04:45,610 --> 01:04:49,090
the first part of the stream and are represented in the later part of the

863
01:04:52,940 --> 01:04:55,360
i'm not sure that any learning method can be

864
01:04:55,380 --> 01:04:57,480
successful because

865
01:04:57,480 --> 01:05:01,030
every learning method is just going to use the first part of the stream somehow

866
01:05:01,090 --> 01:05:03,840
and come up with some model that seems to work in the first part of

867
01:05:03,840 --> 01:05:06,460
the stream but that is not going to work on the second part of the

868
01:05:08,240 --> 01:05:12,340
and there are a

869
01:05:12,530 --> 01:05:18,150
stochastic gradient descent is closely related to other algorithms including the perceptron algorithm which are

870
01:05:18,150 --> 01:05:19,840
non probabilistic

871
01:05:19,860 --> 01:05:24,240
and which directly designed for stream data in which give you

872
01:05:24,280 --> 01:05:28,500
guarantees that are called regret guarantees

873
01:05:28,550 --> 01:05:30,530
which is that

874
01:05:33,670 --> 01:05:36,170
so with an online algorithm

875
01:05:36,170 --> 01:05:39,840
it was also here the situation is that we're using stochastic gradient as a training

876
01:05:40,900 --> 01:05:43,780
and we still think we're going to train and then at some point gonna stop

877
01:05:43,780 --> 01:05:46,780
training going to have a classifier we're gonna use it

878
01:05:46,800 --> 01:05:50,960
but the online situation is that you get examples one the time you have to

879
01:05:50,960 --> 01:05:53,400
make a prediction for this specific example

880
01:05:53,400 --> 01:05:56,320
and then you can learn from this specific example and then you move on to

881
01:05:56,340 --> 01:05:57,570
the next example

882
01:05:57,630 --> 01:06:03,300
and then i regret guarantee is a bound for the annual using is not going

883
01:06:03,300 --> 01:06:04,480
to do

884
01:06:04,480 --> 01:06:06,760
more than a certain factor worse

885
01:06:06,760 --> 01:06:09,340
then the best possible algorithm

886
01:06:09,380 --> 01:06:14,530
and stochastic stochastic gradient algorithms can have regret guarantees

887
01:06:14,550 --> 01:06:17,530
but i'm certainly not going to get into

888
01:06:17,570 --> 01:06:18,590
that now

889
01:06:18,610 --> 01:06:23,630
but the the algorithm in practice the algorithm is very useful

890
01:06:23,650 --> 01:06:29,530
much for a lot of reasons and one reason being that it's so easy to

891
01:06:46,800 --> 01:06:50,740
we're going to take a break in a few minutes i think

892
01:06:50,760 --> 01:06:53,000
just one

893
01:06:53,010 --> 01:06:58,480
so this is the algorithm now i just mentioned some heuristics

894
01:06:58,570 --> 01:07:02,550
that make it much more useful in practice

895
01:07:02,590 --> 01:07:05,590
so maybe the most important

896
01:07:05,590 --> 01:07:08,940
there is a scale

897
01:07:08,960 --> 01:07:12,260
each feature

898
01:07:14,380 --> 01:07:16,480
to have

899
01:07:16,530 --> 01:07:18,960
i mean zero

900
01:07:18,980 --> 01:07:27,340
i just a same mean and variance

901
01:07:27,400 --> 01:07:32,320
so it's not going to work while it has some features may be like age

902
01:07:32,400 --> 01:07:35,610
which range between zero and one hundred

903
01:07:35,710 --> 01:07:41,380
and and then among other features range between zero and one

904
01:07:41,400 --> 01:07:45,010
because the dynamic range of the first feature will be a hundred times the dynamic

905
01:07:45,010 --> 01:07:52,000
range of the second feature and this land is the step size and typically only

906
01:07:52,000 --> 01:07:56,530
have one lambda you don't have different lambda three j and so too much

907
01:07:56,550 --> 01:07:59,260
first single and to be a good choice

908
01:07:59,280 --> 01:08:03,820
all the xj is have to be in the same ballpark

909
01:08:05,710 --> 01:08:09,820
since the feature x zero is constant equal to one

910
01:08:09,820 --> 01:08:13,700
september twenty five is open i said that's fine

911
01:08:13,710 --> 01:08:16,250
i was an idiot

912
01:08:16,300 --> 01:08:21,050
i have electricity electricity and magnetism for decades at MIT

913
01:08:21,100 --> 01:08:22,010
and i know

914
01:08:22,020 --> 01:08:23,410
i should know

915
01:08:23,490 --> 01:08:29,420
that the demonstrations that we like to do require very dry air

916
01:08:29,460 --> 01:08:30,940
no humidity

917
01:08:30,940 --> 01:08:36,060
because the humidity makes the charge to leak off the object

918
01:08:36,360 --> 01:08:39,940
and that's the time when you should give this

919
01:08:40,640 --> 01:08:42,540
it's not the time

920
01:08:42,550 --> 01:08:45,060
and i something else to do

921
01:08:45,070 --> 01:08:48,100
i've had sleepless nights for the past week

922
01:08:48,110 --> 01:08:52,500
absolutely sleepless nights following the weather or by our

923
01:08:52,510 --> 01:08:57,160
had arranged this morning

924
01:08:57,170 --> 01:09:00,620
and if you would walk in you with wet clothes

925
01:09:00,630 --> 01:09:03,250
it would have been all of

926
01:09:03,310 --> 01:09:06,080
if you think it is a little chilly here

927
01:09:06,130 --> 01:09:09,790
sixty seven degrees fahrenheit

928
01:09:09,830 --> 01:09:14,350
why do you think that is

929
01:09:14,380 --> 01:09:18,680
i have asked for the air conditioning to be on for forty eight hours in

930
01:09:18,680 --> 01:09:19,480
a row

931
01:09:19,510 --> 01:09:23,920
because and air-conditioning takes the water out of the air and i can't have you

932
01:09:24,840 --> 01:09:26,770
sorry that you pay the price

933
01:09:27,450 --> 01:09:28,760
a low temperature

934
01:09:28,890 --> 01:09:31,660
so now we will do these them we have to do it fast

935
01:09:31,740 --> 01:09:34,690
in february i can do it very slowly to they have to do it very

936
01:09:34,690 --> 01:09:39,240
fast because if i put positive charge only here if i wait one minute it

937
01:09:43,040 --> 01:09:44,380
we're going to run

938
01:09:45,990 --> 01:09:48,910
the glass with silk

939
01:09:48,950 --> 01:09:53,170
and then the glass will become positively charged that's the way we define positive charge

940
01:09:53,220 --> 01:09:55,810
i will try to put positive charge on this balloon

941
01:09:55,810 --> 01:09:59,710
i'll try to put as much fun as i can

942
01:09:59,720 --> 01:10:05,850
put a little bit more all

943
01:10:09,550 --> 01:10:11,590
now it should be

944
01:10:11,600 --> 01:10:14,740
having some charge the positive charge let it

945
01:10:14,760 --> 01:10:15,960
calm down a little

946
01:10:15,970 --> 01:10:20,060
and now i have here it is rolled which is positively charged positive charge

947
01:10:20,090 --> 01:10:23,080
expels positive charge

948
01:10:23,150 --> 01:10:25,110
very good

949
01:10:25,170 --> 01:10:29,860
but now i'm fast while the charge is still there if i take gruber which

950
01:10:29,860 --> 01:10:31,810
becomes negatively charged

951
01:10:31,810 --> 01:10:32,990
and i wrote

952
01:10:33,010 --> 01:10:36,640
robert we were but always with cat fur so if i wrote this was kept

953
01:10:36,640 --> 01:10:41,580
for i should be able to attract that but really that the is positive and

954
01:10:42,100 --> 01:10:45,220
the rubber is negative let's see whether that works

955
01:10:45,330 --> 01:10:46,660
there comes

956
01:10:46,680 --> 01:10:52,670
see now you have positive charge density negative you're seen the positive charge repel positive

957
01:10:52,670 --> 01:10:55,050
charge using the positive charge

958
01:10:56,630 --> 01:11:00,070
negative charge

959
01:11:01,410 --> 01:11:03,210
the idea of conservation

960
01:11:03,270 --> 01:11:05,810
of charge

961
01:11:05,860 --> 01:11:07,220
that means

962
01:11:07,230 --> 01:11:08,910
if i run this

963
01:11:08,920 --> 01:11:10,240
glass wrote

964
01:11:10,300 --> 01:11:12,960
and if the glass rope becomes positive

965
01:11:12,970 --> 01:11:14,070
it means that

966
01:11:14,090 --> 01:11:16,020
this must become negative

967
01:11:17,180 --> 01:11:19,140
benjamin franklin idea

968
01:11:19,150 --> 01:11:21,600
was elected flow with is positive

969
01:11:21,610 --> 01:11:22,970
this becomes negative

970
01:11:23,060 --> 01:11:26,310
so let's see what i can prove that you by now a lot of charge

971
01:11:26,320 --> 01:11:29,230
may already have reached off because it's september

972
01:11:29,240 --> 01:11:32,760
but i put a little bit more on it

973
01:11:32,800 --> 01:11:35,110
well a little bit more positive on it

974
01:11:35,200 --> 01:11:37,130
and then i'll try to make

975
01:11:39,430 --> 01:11:43,810
role again negative very positive overall very positive and so the

976
01:11:43,810 --> 01:11:45,200
silk now

977
01:11:45,210 --> 01:11:46,920
should become negative

978
01:11:46,930 --> 01:11:51,090
conservation of charge

979
01:11:51,140 --> 01:11:53,580
the same attractor

980
01:11:53,660 --> 01:11:55,160
so that

981
01:11:55,310 --> 01:11:59,020
so now you seem to if the world becomes positive that in the still becomes

982
01:12:03,190 --> 01:12:05,560
call my hair

983
01:12:05,580 --> 01:12:08,140
then i really couldn't predict

984
01:12:08,180 --> 01:12:11,080
whether the called becomes positive or negative

985
01:12:11,140 --> 01:12:13,260
but we can bring that to test

986
01:12:13,340 --> 01:12:16,310
because we know now of the balloon is positively charged

987
01:12:16,350 --> 01:12:20,450
and so if i call my hair if the column became positively charged it will

988
01:12:20,450 --> 01:12:23,900
repair the balloon but if michael was negatively charged

989
01:12:23,940 --> 01:12:28,780
it would attract tractable the

990
01:12:28,830 --> 01:12:38,720
make it attracts the blue from was negative

991
01:12:38,770 --> 01:12:40,900
now you may think

992
01:12:40,940 --> 01:12:43,940
that this business of positive

993
01:12:43,950 --> 01:12:46,380
and negative charges

994
01:12:46,390 --> 01:12:48,050
this idea of

995
01:12:49,570 --> 01:12:52,380
and attraction

996
01:12:52,390 --> 01:12:56,220
you may think that that is very similar to magnets

997
01:12:56,230 --> 01:12:57,720
since magnets

998
01:12:57,740 --> 01:13:01,890
can also repel each other and they can attract each other

999
01:13:01,950 --> 01:13:05,010
but there is a complete misconception

1000
01:13:05,020 --> 01:13:07,350
there is a very fundamental

1001
01:13:08,610 --> 01:13:13,070
between plus and minus electric charges on the one hand

1002
01:13:14,560 --> 01:13:17,390
the poles of magnets which we call north

1003
01:13:17,440 --> 01:13:18,810
and which we call

1004
01:13:18,830 --> 01:13:20,820
south pole

1005
01:13:22,690 --> 01:13:27,720
i show you a bar magnet

1006
01:13:27,800 --> 01:13:35,050
and we have one here we have actually two year

1007
01:13:35,140 --> 01:13:36,800
is one

1008
01:13:36,840 --> 01:13:38,610
number one here

1009
01:13:38,640 --> 01:13:42,650
this site is red

1010
01:13:44,440 --> 01:13:45,320
and this

1011
01:13:45,330 --> 01:13:48,010
happens to be what we call the north pole of the

1012
01:13:48,060 --> 01:13:51,410
magnet and this is the south pole

1013
01:13:51,420 --> 01:13:52,430
i have another one

1014
01:13:52,560 --> 01:13:55,400
also colored

1015
01:13:57,680 --> 01:13:59,460
this site is the north pole

1016
01:13:59,520 --> 01:14:01,960
and the sizes assoc

1017
01:14:03,560 --> 01:14:06,430
the north pole attracts the south pole

1018
01:14:06,470 --> 01:14:09,660
i can easily show that

1019
01:14:09,670 --> 01:14:12,020
this is the north pole this is the south pole

1020
01:14:12,060 --> 01:14:14,380
and they attracted to

1021
01:14:14,450 --> 01:14:16,860
the this south pole attractors north

1022
01:14:18,930 --> 01:14:20,290
if i rotate

1023
01:14:20,300 --> 01:14:22,760
the north called will repel each other

1024
01:14:22,770 --> 01:14:24,770
the south pole to repel each other

1025
01:14:24,780 --> 01:14:25,530
look at it

1026
01:14:25,540 --> 01:14:28,610
there was almost all

1027
01:14:28,670 --> 01:14:30,420
what we're

1028
01:14:30,430 --> 01:14:33,290
they repel each other cells both south pole

1029
01:14:33,290 --> 01:14:37,360
everyone many missing you empire max planck institute for informatics strongly

1030
01:14:37,840 --> 01:14:40,820
and this is a joint work with me gentle kind then sheila

1031
01:14:41,470 --> 01:14:45,350
the video here shows that typical street crossings in with many people occluded

1032
01:14:46,120 --> 01:14:51,070
so the goal of our work is to detect and track all the people in such a crowded street scenes

1033
01:14:52,550 --> 01:14:55,850
he has a example images from the crisis in quick sequence

1034
01:14:56,440 --> 01:15:01,730
it is a very challenging to detect and track of the people in such scenes maybe because of occlusions

1035
01:15:02,510 --> 01:15:08,180
on the single image the pedestrian detection becomes very difficult because of significant partial occlusions

1036
01:15:08,880 --> 01:15:10,440
and for tracking across the sequence

1037
01:15:10,850 --> 01:15:16,150
a certain number of people the long-term occlusions even and the intact even farther into

1038
01:15:16,180 --> 01:15:19,760
a sequence which makes the tracking fails such cases

1039
01:15:20,290 --> 01:15:25,000
and our approach based on the observations in such scenes for example when people walk

1040
01:15:25,000 --> 01:15:27,160
side by side across a pedestrian crossing

1041
01:15:27,870 --> 01:15:31,320
a large number of people are going depend on the person so we say the

1042
01:15:31,320 --> 01:15:37,840
dominant occlusion occlusion cases other person person occlusions which actually resulted in a very characteristic

1043
01:15:37,840 --> 01:15:42,750
patterns and can be explicitly train and used to detect the presence of two person

1044
01:15:44,290 --> 01:15:45,500
recent approach about

1045
01:15:46,070 --> 01:15:48,510
michael people tracking in crowded street scenes

1046
01:15:48,930 --> 01:15:50,920
based on tracking by detection approach

1047
01:15:51,410 --> 01:15:53,060
so basically that implies that

1048
01:15:53,540 --> 01:15:55,510
the pedestrian detector on single image

1049
01:15:55,930 --> 01:15:56,950
even though they don't have

1050
01:15:57,820 --> 01:16:00,560
they don't have some detections whether people is occluded

1051
01:16:01,280 --> 01:16:05,390
and what they did is they include some elaborate strategies to link between

1052
01:16:05,900 --> 01:16:07,670
across the occlusion events

1053
01:16:08,050 --> 01:16:13,770
and so this should strategy requires sufficient with abilities for a certain frame before and after the occlusions

1054
01:16:15,070 --> 01:16:19,390
and this doesn't work for other people always under the occlusion the intersequence

1055
01:16:20,910 --> 01:16:25,250
so all so it is so we see that the successful occluded

1056
01:16:25,690 --> 01:16:27,960
person detection is key solution to

1057
01:16:28,470 --> 01:16:32,960
for for addressing the problem of multiple people tracking in crowded street scenes

1058
01:16:34,400 --> 01:16:43,370
state-of-the-art pedestrian detector is able to robustly detect the people and the different imaging conditions pose and viewpoint variations

1059
01:16:43,920 --> 01:16:45,920
but fails at strong occlusion levels

1060
01:16:46,900 --> 01:16:51,470
and there are already several approach has been proposed to detect occluded people

1061
01:16:51,870 --> 01:16:54,600
with explicit occlusions occlusion reasoning

1062
01:16:55,070 --> 01:16:58,490
basically occlusions as distractions or nuisance

1063
01:16:58,940 --> 01:17:05,400
and the implies individual evidence from the individual person which becomes higher highly unreliable with the person

1064
01:17:07,050 --> 01:17:13,340
and here in our approach we explore alternative strategies so it implies joint efforts of two people

1065
01:17:13,760 --> 01:17:20,530
and use the characteristic patterns of people people occlusion between detector and detected the presence of two people

1066
01:17:21,970 --> 01:17:26,790
any our work we use deformable part model is our this baseline and the next

1067
01:17:26,790 --> 01:17:30,590
question is what's the performance followed peon for occluded people

1068
01:17:31,000 --> 01:17:31,980
how can we quantized

1069
01:17:32,780 --> 01:17:36,440
any other data that's we propose anti i two people datasets

1070
01:17:36,860 --> 01:17:42,080
so there about an underage person person occlusion images in our dataset has said he has examples

1071
01:17:42,740 --> 01:17:46,290
and we categorize other images by ten different occlusion levels

1072
01:17:46,940 --> 01:17:53,310
and now is this data set became explicitly evaluate the detection performance for of every occlusion levels

1073
01:17:54,830 --> 01:17:59,810
and this is the debut performance our two people did here we show that you

1074
01:17:59,810 --> 01:18:02,960
collaborate at its different at each occlusion levels

1075
01:18:03,390 --> 01:18:06,370
and you can see with our employees about five percent

1076
01:18:06,890 --> 01:18:08,530
the optimal good performance

1077
01:18:09,140 --> 01:18:13,690
and when the occlusion level is increased the deeply and performance drops dramatically

1078
01:18:14,460 --> 01:18:18,320
ends when the occlusion level is about seventy five percent a mall

1079
01:18:18,730 --> 01:18:24,630
the achieved score is honest actually about fifty percent which shows that at most cases

1080
01:18:25,410 --> 01:18:28,820
the only one of the two people is detected by the deep

1081
01:18:30,110 --> 01:18:32,340
so it is clear that the deep in is

1082
01:18:33,100 --> 01:18:35,580
he is severely challenged spend occlusions

1083
01:18:36,350 --> 01:18:37,840
any other to solve this problem

1084
01:18:38,260 --> 01:18:42,840
for the first iteration young quote should propose occlusion bad apple person detector

1085
01:18:43,310 --> 01:18:45,530
which would be based on the deeply approach

1086
01:18:45,950 --> 01:18:47,970
and we detected the presence of two people

1087
01:18:48,460 --> 01:18:51,480
and the same can we predict the bounding box for individual person

1088
01:18:52,410 --> 01:18:56,840
and for the second iteration we propose a drunk person detector which is trying to

1089
01:18:56,870 --> 01:18:59,270
between single model to detect michael person

1090
01:18:59,860 --> 01:19:01,890
the double person has been a single person

1091
01:19:03,050 --> 01:19:05,480
first at talk about our double person detector

1092
01:19:06,210 --> 01:19:09,340
so that you can use a mix of components

1093
01:19:09,840 --> 01:19:14,230
ends and initialize it by different about the bounding boxes but shows

1094
01:19:14,730 --> 01:19:20,330
and further training the use latent actually ends up reasons and they also do mining what happen active

1095
01:19:20,880 --> 01:19:21,950
exact examples

1096
01:19:22,440 --> 01:19:25,690
and in the end the linear regression functions for each component

1097
01:19:26,080 --> 01:19:27,540
to get more precise

1098
01:19:30,220 --> 01:19:35,650
and in full analogy to and our detector also use a mixture of components

1099
01:19:36,230 --> 01:19:40,700
and each component consists sit up consist of the feature

1100
01:19:41,180 --> 01:19:45,720
which defines the cost location of two people and also deformable part

1101
01:19:45,720 --> 01:19:55,020
do we use Chinese reastaurant processes in model based clustering basically if you think about clustering the

1102
01:19:55,080 --> 01:19:59,620
the natural object that you should be working with is partitions because the partitions tell

1103
01:19:59,620 --> 01:20:06,440
you how you cluster your your your data set right so given a

1104
01:20:06,440 --> 01:20:10,040
data set S we'd like to partition it into a certain number of clusters of similar

1105
01:20:10,040 --> 01:20:16,480
items and so we can describe themodel as follows so for every cluster C

1106
01:20:16,490 --> 01:20:22,860
in our partition we can define we can describe the data items under that cluster

1107
01:20:22,860 --> 01:20:30,000
as coming from a distribution parameterized by theta C okay and a Bayesian approach would

1108
01:20:30,000 --> 01:20:34,780
be of course to introduce a prior over the partitions and then a prior over

1109
01:20:34,780 --> 01:20:42,160
the parameters of the clusters and then compute the posterior distribution over both it's

1110
01:20:42,160 --> 01:20:49,580
nice to use a Chinese restaurant process for our prior over partitions because it

1111
01:20:49,590 --> 01:20:54,800
doesn't limit us it doesn't limit us to using only a small number of clusters

1112
01:20:54,800 --> 01:21:01,000
it just a allows it actually allows us to also infer the number of

1113
01:21:01,000 --> 01:21:06,240
the number of clusters in our data set right because basically the Chinese restaurant

1114
01:21:06,250 --> 01:21:10,640
process tells us not just the the number of clusters but also how the

1115
01:21:10,640 --> 01:21:17,300
data set is clustered together but of course I'll come back to the finite mixture model because

1116
01:21:17,300 --> 01:21:23,680
that also is a model for clustering okay and which we're familiar with

1117
01:21:23,680 --> 01:21:29,240
and the interest we we can actually derive the Chinese restaurant process from this

1118
01:21:29,240 --> 01:21:35,810
finite mixture model as follows basically if you look at the this quantity here zet

1119
01:21:35,840 --> 01:21:40,720
the zet vector it tells us for every data item which cluster which of the

1120
01:21:40,720 --> 01:21:48,260
K clusters that data item belongs to alright and we have already seen that the marginal

1121
01:21:48,260 --> 01:21:55,620
probability of zet with phi integrated out is looks like this form okay now this

1122
01:21:55,620 --> 01:22:01,920
zet actually describes a partition of the data set but not jsut a petition but

1123
01:22:01,920 --> 01:22:06,740
it also tells us for every cluster in our partition a label between one to

1124
01:22:06,740 --> 01:22:13,480
K okay so it des so this zet describes a partition of the data set into

1125
01:22:13,480 --> 01:22:20,740
clusters and a labelling of each cluster with a mixture component index alright so I'll recall

1126
01:22:20,740 --> 01:22:26,160
that the way we define the partition doesn't have index indices for the clusters so

1127
01:22:26,160 --> 01:22:31,270
we can get rid of the indices by basically looking at the induced distribution

1128
01:22:31,300 --> 01:22:36,720
of partitions under this process and we can work out that that basically equals to

1129
01:22:36,720 --> 01:22:41,080
this quantity here where this thing is exactly the same as that and this term

1130
01:22:41,080 --> 01:22:51,750
here takes into account the fact that of the K clusters in the in the

1131
01:22:51,760 --> 01:22:57,380
the K clusters in our partition we have a we could have

1132
01:22:57,380 --> 01:23:02,340
lots of different ways in which we could label the clusters in the partition okay

1133
01:23:02,340 --> 01:23:06,160
and now if we take this quantity here and take big K to go to

1134
01:23:06,160 --> 01:23:13,240
infinity we'll get a proper distribution of a partitions where we don't limit ourselves

1135
01:23:13,240 --> 01:23:18,900
to at most K number of clusters okay and yes you take K go to infinity here

1136
01:23:18,900 --> 01:23:26,940
this probability of a function over partitions converges to the EPP F of Chinese restaurant

1137
01:23:26,940 --> 01:23:36,050
process so so that's it for the Chinese restaurant process we see that

1138
01:23:36,120 --> 01:23:41,960
it's actually important representation of the Dirichlet process and it turns out that of course is

1139
01:23:41,960 --> 01:23:47,160
also an important object of study in its own right mainly because it's such a

1140
01:23:47,160 --> 01:23:54,320
natural object for a clustering interestingly historically it actually predates the Dirichlet process and

1141
01:23:54,320 --> 01:24:00,960
it actually originated in genetics basically is related to a formula called Ewen's sampling

1142
01:24:00,960 --> 01:24:06,660
formula that came out in species sampling so imagine that you have a biologist which

1143
01:24:06,660 --> 01:24:10,920
who goes into a forest and then collects samples you know the first sample could

1144
01:24:10,920 --> 01:24:14,720
be a particular ant the next sample could be leaf or from a tree the

1145
01:24:14,720 --> 01:24:18,610
example could be an ant again an example could be a polar bear or

1146
01:24:18,610 --> 01:24:27,960
something alright so the idea is that this Chinese restaurant process is idealized process for

1147
01:24:27,960 --> 01:24:34,380
describing how the samples that the biologist collects are like clustered into the into

1148
01:24:34,380 --> 01:24:42,640
the different species there are lots on MCMC samplers using this Chinese restaurant process that's

1149
01:24:42,640 --> 01:24:48,740
a nice survey by Redford Neil that talks about this and of course the random partitions

1150
01:24:48,740 --> 01:24:55,430
that the Chinese restaurant process describes are again very useful concepts for clustering problems and

1151
01:24:55,550 --> 01:24:59,560
also some clustering problems in machine learning alright so we can have model based clustering we could

1152
01:24:59,560 --> 01:25:07,480
have clustering nodes of in graphs into different communities we can look at building others also

1153
01:25:07,480 --> 01:25:11,800
commutual structures from this partitions so if we have time we'll get to

1154
01:25:11,800 --> 01:25:23,010
look at hierarchical clusterings using concepts of fragmentation and computation of partitions okay so the

1155
01:25:23,020 --> 01:25:31,440
other representation that I'll talk about is the stick breaking construction and this also relates back

1156
01:25:31,440 --> 01:25:37,140
to the clustering property right so we know that when we have such a generative

1157
01:25:37,140 --> 01:25:43,360
process where G is DP distributed and each of the theta Is are IID G then the

1158
01:25:43,360 --> 01:25:49,320
same values can be repeated among the variables of theta one to theta N okay and this is only

1159
01:25:49,320 --> 01:25:57,420
possible if this G a random distribution is atomic okay atomic in the sense

1160
01:25:57,420 --> 01:26:04,880
that it's infinite possibly infinite sum of of atoms each atom being located at

1161
01:26:04,880 --> 01:26:11,200
some point part of the space and phi K is the probability that one of

1162
01:26:11,200 --> 01:26:20,600
the theta Is takes on that particular value and we already saw that our G distribution has to be

1163
01:26:20,600 --> 01:26:26,260
atomic when we looked at how we might draw from this from our Dirichlet process

1164
01:26:26,260 --> 01:26:32,400
okay so the question now is we know that draws from the Dirichlet process will

1165
01:26:32,400 --> 01:26:37,160
always be of this form where the phis are nonnegative and they have to sum

1166
01:26:37,160 --> 01:26:45,420
to one and the thetas are basically points in our probability space theta the question

1167
01:26:45,420 --> 01:26:47,040
in particular

1168
01:26:47,090 --> 01:26:48,520
we've got to compute the

1169
01:26:48,580 --> 01:26:51,170
the score of an alignment

1170
01:26:52,360 --> 01:26:53,630
in each position

1171
01:26:53,810 --> 01:26:56,000
again the linear function

1172
01:26:56,110 --> 01:27:00,020
instead of just having a number of here and summing over all the positions

1173
01:27:00,060 --> 01:27:04,590
we are allowing out have linear function that takes input sequence

1174
01:27:04,630 --> 01:27:07,270
and the alignment operations that position

1175
01:27:08,150 --> 01:27:13,590
we can tune the weight vectors to give us alignment score for each position

1176
01:27:13,610 --> 01:27:16,460
that includes also

1177
01:27:16,730 --> 01:27:20,650
insertion and deletion

1178
01:27:20,650 --> 01:27:25,670
and i know that there is no doing agenda estimation setting would be really really

1179
01:27:25,670 --> 01:27:30,130
hard because we would have to model dependencies between the feature vector which was

1180
01:27:30,150 --> 01:27:31,290
it seem to model

1181
01:27:31,310 --> 01:27:36,150
but it's not a problem in data set

1182
01:27:38,750 --> 01:27:44,560
so we fix the representation and we figured out to be argmax just and

1183
01:27:44,610 --> 01:27:46,190
the last thing that we have to

1184
01:27:46,210 --> 01:27:48,540
do specify loss function

1185
01:27:48,590 --> 01:27:50,560
and so

1186
01:27:50,610 --> 01:27:52,840
first guess might have

1187
01:27:52,900 --> 01:27:55,980
just count the number of incorrect

1188
01:27:56,000 --> 01:28:01,190
alignment operations for example the correct alignment like it would be

1189
01:28:01,190 --> 01:28:04,360
it was a year you the

1190
01:28:04,360 --> 01:28:08,580
this is the predicted alignment a is actually of here

1191
01:28:08,590 --> 01:28:10,190
and in the correct position

1192
01:28:10,250 --> 01:28:13,190
you would get a lot of what we

1193
01:28:13,230 --> 01:28:16,210
the possible one one

1194
01:28:16,210 --> 01:28:17,250
but actually the

1195
01:28:17,270 --> 01:28:21,590
bioinformatics collaborators told us that this is too hard

1196
01:28:22,380 --> 01:28:24,960
it doesn't really matter if you get it exactly right

1197
01:28:25,020 --> 01:28:28,290
together within window four that's good enough

1198
01:28:28,310 --> 01:28:32,150
from they can be local search probably pi

1199
01:28:33,130 --> 01:28:37,520
that's because q four loss function so if you just get you know

1200
01:28:37,630 --> 01:28:40,040
two amino acids you into the

1201
01:28:40,040 --> 01:28:44,540
to the correct when no one

1202
01:28:44,540 --> 01:28:48,020
and then again if you think about this a little bit then

1203
01:28:48,040 --> 01:28:51,340
separation oracle that you need to the cutting plane training

1204
01:28:51,420 --> 01:28:52,690
again you can do get

1205
01:28:52,710 --> 01:28:54,690
program exactly the same alignment

1206
01:28:54,710 --> 01:28:57,750
this again

1207
01:28:57,810 --> 01:28:59,770
so now we specify everything

1208
01:28:59,810 --> 01:29:02,960
we can just like that the cold and

1209
01:29:02,980 --> 01:29:05,460
look at a couple of weeks

1210
01:29:05,520 --> 01:29:08,480
this is the standard datasets

1211
01:29:08,480 --> 01:29:10,020
we train on

1212
01:29:10,210 --> 01:29:14,040
roughly five thousand non-aligned

1213
01:29:14,150 --> 01:29:18,560
validation of the five live to parameters

1214
01:29:18,610 --> 01:29:21,480
and then we block tests

1215
01:29:23,880 --> 01:29:24,880
we use

1216
01:29:24,940 --> 01:29:31,540
these features here i mean that the secondary structure of surface area

1217
01:29:31,540 --> 01:29:38,230
that's known for the structures and we use the predicted values used to stable

1218
01:29:38,230 --> 01:29:41,770
for the the new sequences

1219
01:29:48,310 --> 01:29:50,310
these are the results went through

1220
01:29:54,090 --> 01:29:56,440
representation and

1221
01:29:56,500 --> 01:29:59,710
the point that i want to make is

1222
01:30:00,770 --> 01:30:07,880
kind of unconventional gender training you would probably build models that have maybe thousand features

1223
01:30:07,900 --> 01:30:09,360
and we try to push it

1224
01:30:10,420 --> 01:30:11,670
actually come up with

1225
01:30:11,690 --> 01:30:12,520
you know

1226
01:30:12,520 --> 01:30:15,440
features that describe here triplets

1227
01:30:15,520 --> 01:30:19,170
describe window around the current location

1228
01:30:19,690 --> 01:30:24,110
putting it all the way to model that have around half a million

1229
01:30:24,150 --> 01:30:27,690
and if you look at the test performance higher is better

1230
01:30:27,690 --> 01:30:31,150
you actually benefit from building

1231
01:30:31,190 --> 01:30:32,790
the best

1232
01:30:32,810 --> 01:30:34,810
the best results we actually go

1233
01:30:34,810 --> 01:30:36,880
the most complex models

1234
01:30:36,920 --> 01:30:38,540
this allows us to

1235
01:30:38,580 --> 01:30:40,650
to build these models that

1236
01:30:40,670 --> 01:30:46,400
you just wouldn't be able to really train easy way with them

1237
01:30:46,480 --> 01:30:48,650
it's a comparison against existing methods

1238
01:30:48,920 --> 01:30:50,630
if you just do

1239
01:30:50,650 --> 01:30:54,360
the blast alignment that's not

1240
01:30:54,360 --> 01:31:00,400
what kind of these all sequences that are hard have no that

1241
01:31:00,440 --> 01:31:02,040
this is the

1242
01:31:02,040 --> 01:31:05,710
best results that we got picked on the validation

1243
01:31:05,710 --> 01:31:08,060
in this case is given by this equation here

1244
01:31:08,060 --> 01:31:12,630
so this is just like you can't matrix something to the main diagonal

1245
01:31:12,690 --> 01:31:14,130
in work that

1246
01:31:14,170 --> 01:31:16,380
multiply by observations

1247
01:31:16,400 --> 01:31:18,750
then you multiply by the

1248
01:31:20,920 --> 01:31:25,020
so this is just like kernel expansion is a computer plantations

1249
01:31:25,060 --> 01:31:27,060
so this part you can do

1250
01:31:29,400 --> 01:31:32,400
and this is what you have to do when your training new test point comes

1251
01:31:34,250 --> 01:31:35,560
if you do that

1252
01:31:35,560 --> 01:31:38,020
you get the green curve

1253
01:31:39,670 --> 01:31:41,750
depending on the value of x

1254
01:31:41,770 --> 01:31:44,560
like at different values he

1255
01:31:47,330 --> 01:31:51,440
now for the variance of this because i'm not always equally shore

1256
01:31:51,770 --> 01:31:55,830
for example for instance here have much darker there

1257
01:31:55,880 --> 01:31:58,270
we get different values for the variance

1258
01:31:58,290 --> 01:32:02,100
this is really integrating out parts of my prior

1259
01:32:02,110 --> 01:32:05,770
so this is the only deviation from what i told you so far

1260
01:32:05,810 --> 01:32:10,560
will usually have to the markov chain monte carlo whatever to really find out what

1261
01:32:10,560 --> 01:32:12,830
your posterior looks like

1262
01:32:12,860 --> 01:32:14,560
if everything is gaussians

1263
01:32:14,580 --> 01:32:17,000
you can actually integrate out things

1264
01:32:17,150 --> 01:32:21,210
the this is what your degree of uncertainty looks like

1265
01:32:21,250 --> 01:32:25,770
again it's the standard deviation

1266
01:32:25,770 --> 01:32:28,750
and what it means is here we're not seeing much darker

1267
01:32:28,770 --> 01:32:31,290
i'm becoming less errors

1268
01:32:31,290 --> 01:32:33,000
you have seen a lot of stuff

1269
01:32:33,290 --> 01:32:34,940
not much more certain

1270
01:32:34,960 --> 01:32:39,190
so the red line is the amount of uncertainty

1271
01:32:39,210 --> 01:32:45,830
and the green line is just the mean plus minus the uncertainty

1272
01:32:45,880 --> 01:32:48,190
we also see something interesting happening here

1273
01:32:48,210 --> 01:32:50,230
this is the boundary of the data

1274
01:32:50,310 --> 01:32:51,920
the uncertainty increases

1275
01:32:51,920 --> 01:32:55,840
so i'm not seeing anything over here

1276
01:32:55,860 --> 01:33:03,190
does anybody have an idea what this is really big problem for high dimensional data

1277
01:33:03,230 --> 01:33:08,080
used for high dimensional data it was things at the boundary

1278
01:33:08,080 --> 01:33:10,730
if you take an hour to number of points

1279
01:33:10,790 --> 01:33:13,340
very high dimensional spaces

1280
01:33:14,100 --> 01:33:18,290
depending on how you scale things they will all be almost equal distance from each

1281
01:33:18,290 --> 01:33:19,580
other and they will be all

1282
01:33:19,600 --> 01:33:23,230
on the surface of football

1283
01:33:23,290 --> 01:33:25,400
this particularly bad news

1284
01:33:25,440 --> 01:33:27,170
this means

1285
01:33:27,190 --> 01:33:32,270
you know we really uncertain pretty much everywhere

1286
01:33:33,540 --> 01:33:34,830
number this

1287
01:33:34,830 --> 01:33:39,630
these are fairly small scratch that in practice usually will be OK

1288
01:33:39,650 --> 01:33:44,920
just bear in mind that there is skeleton in the closet namely a very high

1289
01:33:44,920 --> 01:33:47,500
dimensional data you want to estimation

1290
01:33:47,520 --> 01:33:48,960
things like

1291
01:33:49,000 --> 01:33:51,690
turn really ugly

1292
01:33:51,710 --> 01:33:57,710
this put everything together the lookup is with actually generated by

1293
01:33:57,710 --> 01:34:03,310
and custom semantic noise and you can see my system it wasn't too bad

1294
01:34:03,330 --> 01:34:06,130
OK it wasn't really exact here but

1295
01:34:06,170 --> 01:34:09,210
given take it is actually very nice

1296
01:34:09,230 --> 01:34:10,670
try something else

1297
01:34:10,690 --> 01:34:19,860
so here instance have observed in it out and uncertainty just blows up

1298
01:34:20,790 --> 01:34:23,100
heteroskedastic relation

1299
01:34:23,150 --> 01:34:24,500
tongue twister

1300
01:34:24,500 --> 01:34:27,790
the idea is we make both the linear and quadratic term in y given x

1301
01:34:30,540 --> 01:34:33,380
so i think FA status in life

1302
01:34:33,380 --> 01:34:35,310
to be white and fluffy chicks

1303
01:34:35,330 --> 01:34:37,830
and y squared times y two fix

1304
01:34:37,900 --> 01:34:43,060
my fix cx is just look at the inappropriate between politics and y theta

1305
01:34:43,080 --> 01:34:44,400
the function of y

1306
01:34:44,420 --> 01:34:48,630
and we'll get the quadratic term

1307
01:34:48,650 --> 01:34:51,230
so that's my kernel

1308
01:34:51,270 --> 01:34:56,130
i mean that is make mean and variance simultaneously

1309
01:34:57,590 --> 01:35:01,830
now i'm going to use the representer theorem but there's a time interval problem was

1310
01:35:01,830 --> 01:35:05,840
that the remember in a state of santa theorem i said well this is all

1311
01:35:05,840 --> 01:35:07,190
really great

1312
01:35:07,230 --> 01:35:10,170
if y has finite cardinality

1313
01:35:10,230 --> 01:35:11,520
now obviously are

1314
01:35:11,560 --> 01:35:13,810
the finite cardinality

1315
01:35:15,000 --> 01:35:17,290
if you look at the spam

1316
01:35:17,340 --> 01:35:19,540
of this for all possible lies

1317
01:35:19,540 --> 01:35:22,310
just look at the first time here

1318
01:35:22,310 --> 01:35:24,000
six ix

1319
01:35:24,020 --> 01:35:27,830
so that they have only one x and all possible wise

1320
01:35:27,880 --> 01:35:31,130
what's the dimensionality of the space

1321
01:35:31,150 --> 01:35:35,380
if applied in all possible ways here

1322
01:35:35,400 --> 01:35:38,310
is infinite dimensional

1323
01:35:38,330 --> 01:35:39,830
the step three

1324
01:35:40,710 --> 01:35:42,560
one dimension

1325
01:35:42,560 --> 01:35:45,880
the world for infinity

1326
01:35:48,130 --> 01:35:53,000
who world full three dimensions

1327
01:35:53,960 --> 01:35:58,150
what's the two dimension

1328
01:35:58,170 --> 01:35:59,770
OK five people

1329
01:35:59,790 --> 01:36:03,610
well one dimension

1330
01:36:04,310 --> 01:36:05,630
was asleep

1331
01:36:05,650 --> 01:36:08,060
it must be the complement so it's

1332
01:36:08,060 --> 01:36:14,630
i think overall had like five percent of the hands raised some

1333
01:36:14,770 --> 01:36:16,960
if six

1334
01:36:17,000 --> 01:36:22,520
then look at all possible values of y remember this scale the number

1335
01:36:22,650 --> 01:36:24,110
take that

1336
01:36:24,130 --> 01:36:25,860
convex hull of all these

1337
01:36:25,920 --> 01:36:29,150
what's the dimensionality of the space

1338
01:36:29,190 --> 01:36:35,230
so who thinks it's one-dimensional

1339
01:36:35,290 --> 01:36:36,940
and one person

1340
01:36:36,980 --> 01:36:40,790
who thinks it's two dimensional

1341
01:36:40,790 --> 01:36:44,610
i have zero covariance and so we say that they are uncorrelated

1342
01:36:44,690 --> 01:36:48,480
and yet they are not independent so it doesn't it doesn't work both ways but

1343
01:36:48,480 --> 01:36:52,740
if two variables are independent they are necessarily uncorrelated

1344
01:36:52,750 --> 01:36:55,800
OK we can discuss later maybe some special cases where

1345
01:36:55,850 --> 01:36:59,560
the inverse of the converse is not true

1346
01:36:59,590 --> 01:37:03,300
OK so here is another thing to somehow give you a more intuitive picture of

1347
01:37:03,300 --> 01:37:07,280
what this concept of correlation is is all about here

1348
01:37:07,300 --> 01:37:10,940
are some scatterplots of two random variables x versus y

1349
01:37:10,970 --> 01:37:14,820
so this could make x can represent them the weight of a person and why

1350
01:37:14,850 --> 01:37:19,600
could represent the height of a persons and so what you see is that if

1351
01:37:19,620 --> 01:37:23,240
if x comes out higher than average in this plot here

1352
01:37:23,260 --> 01:37:26,930
then y also has a tendency to come out higher than average

1353
01:37:26,940 --> 01:37:29,790
and that's exactly what you mean by positive correlation

1354
01:37:29,800 --> 01:37:34,300
and for this particular plot the correlation coefficient was o point seven five

1355
01:37:34,360 --> 01:37:37,860
and you can see that if you if you stare back at this equivalence formula

1356
01:37:38,120 --> 01:37:39,530
for the covariance

1357
01:37:39,570 --> 01:37:40,990
you see them here

1358
01:37:41,170 --> 01:37:45,560
the covariance is the expectation value of x minus mu x

1359
01:37:45,570 --> 01:37:47,060
times y minus mu y i

1360
01:37:47,070 --> 01:37:51,540
so if x is bigger than its average in y also has a tendency to

1361
01:37:51,540 --> 01:37:55,920
be bigger than this average then this is going to be positive and similarly if

1362
01:37:55,920 --> 01:37:59,760
x is less than this average and that comes along with having y less than

1363
01:37:59,760 --> 01:38:04,290
its average in both terms would be negative and again the covariance would be positive

1364
01:38:04,360 --> 01:38:08,170
so that gives you some sort of intuitive explanation of what correlations are supposed to

1365
01:38:08,170 --> 01:38:09,610
be about

1366
01:38:09,610 --> 01:38:12,420
so here would be a case where the covariance is negative

1367
01:38:12,470 --> 01:38:18,300
next comes out higher-than-average why has the tendency to come out less than its average

1368
01:38:18,350 --> 01:38:22,790
as the correlation coefficient approaches one then the

1369
01:38:22,800 --> 01:38:27,420
the band here becomes narrower and similarly you can imagine that if rho was close

1370
01:38:27,420 --> 01:38:29,760
to minus one the band would be like that

1371
01:38:29,790 --> 01:38:32,280
whereas if rho is zero then you get to no

1372
01:38:32,290 --> 01:38:33,280
no tilt

1373
01:38:33,290 --> 01:38:35,160
it also here is a fairly low

1374
01:38:35,170 --> 01:38:38,010
value for the correlation coefficient

1375
01:38:38,990 --> 01:38:43,750
OK so one more point emphasizing that is that correlations are very complicated things to

1376
01:38:43,750 --> 01:38:47,970
deal with when you're talking about an error analysis and sometimes people will be doing

1377
01:38:48,050 --> 01:38:52,590
a big complicated analysis you're you're trying to to to defend and some guy in

1378
01:38:52,590 --> 01:38:56,410
the back of the room will say did you take into account correlations

1379
01:38:56,470 --> 01:39:00,990
and the way to answer this person's question is correlations between what and what sort

1380
01:39:01,110 --> 01:39:03,380
of correlation is always defined between

1381
01:39:03,650 --> 01:39:05,550
a pair of quantities

1382
01:39:05,550 --> 01:39:09,740
so so please keep that in mind says correlations it makes no sense to talk

1383
01:39:09,740 --> 01:39:12,700
about this global correlations between

1384
01:39:12,750 --> 01:39:20,040
large quantities many quantities unless you consider the pairwise correlations are always dealt with pairwise

1385
01:39:20,050 --> 01:39:23,360
OK now what i also have in the

1386
01:39:23,360 --> 01:39:28,100
overheads from yesterday are number of slides on error propagation and what i want to

1387
01:39:28,100 --> 01:39:31,540
do now is just to say what the problem is to show you the answer

1388
01:39:31,790 --> 01:39:34,420
and then to leave it to you to go look at these overheads

1389
01:39:34,480 --> 01:39:38,370
the basic idea is the following suppose you have some set of values that you

1390
01:39:39,260 --> 01:39:43,990
x one through x and so symbolically let me call that some vector x

1391
01:39:44,090 --> 01:39:47,630
and suppose you have the covariances for each pair

1392
01:39:47,650 --> 01:39:51,070
of those quantities and i can i can write that is the matrix right i

1393
01:39:51,070 --> 01:39:52,690
can see the i j

1394
01:39:52,720 --> 01:39:54,150
is the covariance of

1395
01:39:54,160 --> 01:39:56,050
x i and x j

1396
01:39:56,050 --> 01:39:59,680
and i would need to specify that for each each pair

1397
01:40:00,610 --> 01:40:03,840
OK now having done that now consider some function

1398
01:40:03,880 --> 01:40:05,490
of these quantities

1399
01:40:05,500 --> 01:40:10,530
y that i specify and the question is then what is the variance of this

1400
01:40:12,090 --> 01:40:13,110
and so

1401
01:40:13,120 --> 01:40:17,550
what you need to do the covariances of the individual x i

1402
01:40:17,610 --> 01:40:22,050
they were they can represent the the errors that the uncertainties if you will on

1403
01:40:22,050 --> 01:40:25,510
the exi and i've been from this function i want to know how do those

1404
01:40:25,510 --> 01:40:27,940
errors propagate into that function

1405
01:40:27,990 --> 01:40:31,690
so this is the kind of thing that i think you've probably seen at some level in

1406
01:40:31,740 --> 01:40:37,180
year undergraduate courses in some undergraduate lab practical

1407
01:40:37,190 --> 01:40:40,840
and so they probably give you several recipes for how to do that and so

1408
01:40:40,840 --> 01:40:44,010
what i would ask you to do in your own time is to look through

1409
01:40:44,050 --> 01:40:46,100
the way have done it here

1410
01:40:46,160 --> 01:40:48,300
and then to finally wind up the answer

1411
01:40:48,370 --> 01:40:51,900
so what the answer tells you is that you have to to construct derivatives of

1412
01:40:51,900 --> 01:40:57,010
the function and you you multiply those derivatives with elements of this

1413
01:40:57,030 --> 01:40:58,910
covariance matrix

1414
01:40:59,000 --> 01:41:03,430
OK so i believe that that and that's really where i wanted to get yesterday

1415
01:41:03,440 --> 01:41:09,650
it's already or were ten minutes into today so let me wrap up lecture one

1416
01:41:09,690 --> 01:41:13,190
and move on to lecture two

1417
01:41:13,260 --> 01:41:16,380
this works

1418
01:41:17,150 --> 01:41:20,940
so what i propose to do now for the next thirty five minutes

1419
01:41:21,150 --> 01:41:26,530
forty is to give you a brief catalogue of probability densities and

1420
01:41:26,980 --> 01:41:29,450
i will go through a number of them

1421
01:41:29,470 --> 01:41:35,010
very quickly and skipping several things that i've written on these overheads so you can

1422
01:41:35,010 --> 01:41:39,350
go back on your own time and read a number of the additional details but

1423
01:41:39,350 --> 01:41:46,430
these include binomial multinomial poisson uniform exponential lcmk squared co she learned are some of

1424
01:41:46,440 --> 01:41:50,650
the example uses in particle physics and we only look at maybe half of these

1425
01:41:50,930 --> 01:41:54,920
will only look at some of the information on each of them

1426
01:41:55,100 --> 01:41:59,730
so let me start with the binomial distribution in this one will discuss a little

1427
01:41:59,730 --> 01:42:00,750
more carefully

1428
01:42:00,790 --> 01:42:03,980
consider first

1429
01:42:04,090 --> 01:42:10,870
capital and large independent experiments called bernoulli trials now what bernoulli trial is is an

1430
01:42:10,870 --> 01:42:14,730
observation that has only two possible outcomes

1431
01:42:14,790 --> 01:42:17,380
so i could label those outcomes to say

1432
01:42:17,400 --> 01:42:19,300
success or failure

1433
01:42:20,840 --> 01:42:24,400
suppose that i have then an independent

1434
01:42:24,410 --> 01:42:25,880
bernoulli trials

1435
01:42:26,130 --> 01:42:31,730
and that the probability of success on any given one is some specified

1436
01:42:31,780 --> 01:42:33,280
constant p

1437
01:42:33,290 --> 01:42:37,510
so that will be a parameter that characterizes this probability law

1438
01:42:37,560 --> 01:42:43,540
and what i'm interested in finding is the probability to find small n successes

1439
01:42:43,550 --> 01:42:45,440
given a large n

1440
01:42:45,500 --> 01:42:47,900
independent bernoulli trials

1441
01:42:48,140 --> 01:42:52,150
so let's think about how you would work that out so

1442
01:42:52,160 --> 01:42:53,160
i've said

1443
01:42:53,170 --> 01:42:56,920
the bernoulli trials are independent and so therefore

1444
01:42:56,920 --> 01:43:00,500
if i consider some

1445
01:43:00,550 --> 01:43:06,030
number if i consider a specific outcome in a particular order for example success success

1446
01:43:06,030 --> 01:43:07,920
failure success failure

1447
01:43:08,850 --> 01:43:13,670
remember what we had from the definition of independent events it means that the probability

1448
01:43:13,670 --> 01:43:17,630
for the whole thing to happen is simply the product of the probabilities for the

1449
01:43:17,630 --> 01:43:19,060
individual events

1450
01:43:19,080 --> 01:43:25,010
that's what independence meant so therefore the probability to find the specific outcomes success success

1451
01:43:25,010 --> 01:43:27,580
failure success failure would simply be

1452
01:43:27,630 --> 01:43:29,020
p times p

1453
01:43:29,040 --> 01:43:33,660
so the the two probabilities for success times one minus p that's the probability for

1454
01:43:34,580 --> 01:43:36,960
times p times one minus p

1455
01:43:36,980 --> 01:43:41,060
and so if you just reordered the terms you see this is key

1456
01:43:41,070 --> 01:43:45,050
to the end power the small and which were that's the number of successes

1457
01:43:45,060 --> 01:43:46,430
times one minus p

1458
01:43:46,440 --> 01:43:49,730
raised to the power of the number of failures

1459
01:43:49,760 --> 01:43:53,510
OK so that's almost the answer but it's not quite the answer because what i

1460
01:43:53,510 --> 01:43:55,210
ask for is simply the total

1461
01:43:55,230 --> 01:44:00,620
number of successes being an idea i don't want to insist on what order they

1462
01:44:00,620 --> 01:44:03,800
come in i don't care on what were they become in so i simply have

1463
01:44:03,800 --> 01:44:08,390
are drawn here for n equals two it's something that we've seen before

1464
01:44:08,410 --> 01:44:11,580
namely if you want to send a zero

1465
01:44:11,660 --> 01:44:15,920
you send a one in the first compound the first degree of freedom is zero

1466
01:44:15,920 --> 01:44:18,500
and the second and if you want to send

1467
01:44:18,510 --> 01:44:21,840
they warn you send the opposite

1468
01:44:21,860 --> 01:44:23,290
we've all seen

1469
01:44:23,300 --> 01:44:27,700
but this is a very sensible thing to do when we look the binary detection

1470
01:44:27,720 --> 01:44:30,500
because when you use scheme like this

1471
01:44:30,540 --> 01:44:32,720
of course the thing that happens

1472
01:44:32,750 --> 01:44:33,690
yes we now

1473
01:44:33,720 --> 01:44:36,410
no that we should look at this in terms of

1474
01:44:36,460 --> 01:44:39,600
just looking at this line along here

1475
01:44:39,600 --> 01:44:45,530
because what we're really transmitting is a is a pilot tone so to speak which

1476
01:44:45,530 --> 01:44:50,540
is half in the middle here which sticks right here

1477
01:44:50,760 --> 01:44:54,090
plus something something that varies from that

1478
01:44:54,110 --> 01:44:56,600
so that when we

1479
01:44:56,620 --> 01:45:02,220
when we take out the pilot what we wind up with one-dimensional system instead of

1480
01:45:02,220 --> 01:45:04,030
a two dimensional system

1481
01:45:05,600 --> 01:45:09,100
which we used to call antipodal communication

1482
01:45:09,120 --> 01:45:10,020
and which

1483
01:45:10,020 --> 01:45:14,240
everybody with any sense cause centripetal communication even now

1484
01:45:14,250 --> 01:45:18,940
but in terms of this is the simplest case of the simplex code

1485
01:45:18,960 --> 01:45:23,940
so the simplex code is simply an orthogonal codes where you've taken the mean and

1486
01:45:23,940 --> 01:45:28,290
moved out and as soon as you remove the main from an orthogonal codes you

1487
01:45:28,290 --> 01:45:32,340
get with the rate of one degree of freedom because one of the signal becomes

1488
01:45:32,340 --> 01:45:36,610
dependent on the others which is exactly what's happened here you just

1489
01:45:36,620 --> 01:45:40,240
four you're signals are in one degree of freedom here when you do the same

1490
01:45:40,240 --> 01:45:41,610
thing down here

1491
01:45:41,690 --> 01:45:45,100
well you get this talk about that later

1492
01:45:45,110 --> 01:45:50,240
so one thing you can do from orthogonal code is good with simplex code the

1493
01:45:50,240 --> 01:45:51,830
other thing you can do

1494
01:45:52,070 --> 01:45:54,280
if you want to transmit one more bit

1495
01:45:54,290 --> 01:45:56,150
this signal said

1496
01:45:56,170 --> 01:46:01,510
just to go to by orthogonal code which says along with transmitting zero one then

1497
01:46:01,510 --> 01:46:06,180
one zero you look this news they say what i like what i also put

1498
01:46:06,180 --> 01:46:09,170
in zero minus one

1499
01:46:09,180 --> 01:46:11,190
n minus one zero

1500
01:46:11,200 --> 01:46:16,720
and zero minus one down here which is exactly what the buyer orthogonal code is

1501
01:46:16,720 --> 01:46:21,650
the prior orthogonal code simply said take your plugin all code and every time you

1502
01:46:21,650 --> 01:46:26,530
have one change one minus one and get an extra codeword out of it

1503
01:46:26,530 --> 01:46:30,840
what to do between the set of codewords in the signal set

1504
01:46:30,860 --> 01:46:35,600
anybody have any idea

1505
01:46:35,620 --> 01:46:39,290
absolutely not both exactly the same thing

1506
01:46:39,300 --> 01:46:40,970
and you

1507
01:46:41,010 --> 01:46:45,860
you think of it as being code usually if what you're doing is thinking of

1508
01:46:46,030 --> 01:46:50,850
generating an error correcting code and then from that error correcting code you think of

1509
01:46:50,850 --> 01:46:56,500
using QAM are PAMI something else out beyond that you call it signal said if

1510
01:46:56,500 --> 01:46:58,200
you're just doing the whole thing

1511
01:46:58,260 --> 01:47:03,240
as one unit what what a lot of systems now do

1512
01:47:03,260 --> 01:47:08,500
is they start out with the code then they turn this into an orthogonal signal

1513
01:47:08,510 --> 01:47:09,860
said OK

1514
01:47:09,910 --> 01:47:12,950
OK OK in other words the code produces bits

1515
01:47:13,000 --> 01:47:16,860
from the bits you group them together into sets of bits from the sets of

1516
01:47:16,860 --> 01:47:19,760
bits you go into a signal which is

1517
01:47:19,760 --> 01:47:26,440
which is for example something from one of these three possibilities here

1518
01:47:27,080 --> 01:47:35,690
the important thing to notice here that is particularly important to think about it for

1519
01:47:35,690 --> 01:47:36,790
a few minutes

1520
01:47:36,810 --> 01:47:40,440
it's because you do so many exercises

1521
01:47:40,490 --> 01:47:43,680
and you've done a number of them already and you will do a few more

1522
01:47:43,680 --> 01:47:47,280
in this course where you deal with the EMI equals two case

1523
01:47:47,320 --> 01:47:50,630
and you can deal with this by orthogonal set here

1524
01:47:50,660 --> 01:47:55,250
you can shift by orthogonal set around by forty five degrees

1525
01:47:55,250 --> 01:48:03,020
in which case it looks like this

1526
01:48:03,030 --> 01:48:07,740
OK so that looks like two PAAM says it looks like a standard QAM

1527
01:48:08,950 --> 01:48:11,520
it looks like standard for two way and

1528
01:48:11,520 --> 01:48:15,160
this is exactly the same as this of course

1529
01:48:15,190 --> 01:48:17,520
when you do detection on this

1530
01:48:17,530 --> 01:48:22,640
you do detection by first saying when you transmit this does the noise carry you

1531
01:48:22,850 --> 01:48:25,340
across that boundary

1532
01:48:25,350 --> 01:48:30,450
and then there's the noise carry across this boundary the noise in this direction is

1533
01:48:30,450 --> 01:48:33,810
orthogonal from noise in this direction

1534
01:48:33,820 --> 01:48:38,500
and therefore finding the probability of error is very very simple

1535
01:48:38,510 --> 01:48:40,990
because because you look at two

1536
01:48:41,040 --> 01:48:47,400
two separate orthogonal kinds of noise and you can just multiply these probabilities together

1537
01:48:47,430 --> 01:48:50,820
in the appropriate way to find out what's happening

1538
01:48:50,820 --> 01:48:55,180
the important thing that have stick in your memory now is it as soon as

1539
01:48:55,180 --> 01:48:57,390
you go there and make all three

1540
01:48:57,410 --> 01:48:59,520
life gets hard

1541
01:48:59,570 --> 01:49:03,090
in fact if you look at this orthogonal set here

1542
01:49:03,140 --> 01:49:08,470
and you try to find the heir probability for you try to find out exactly

1543
01:49:08,520 --> 01:49:12,950
you can't do it like this you can just multiply three terms you look at

1544
01:49:12,950 --> 01:49:15,590
these regions in three dimensional space

1545
01:49:15,600 --> 01:49:21,600
if you want to visualize what they are what do you do

1546
01:49:21,620 --> 01:49:24,720
what picture deal look at

1547
01:49:24,740 --> 01:49:26,930
you look at this picture

1548
01:49:26,950 --> 01:49:31,320
OK because this picture is just this with the mean taken away

1549
01:49:31,330 --> 01:49:36,470
so the air probability here is the same as the air probability here when i

1550
01:49:36,470 --> 01:49:38,930
send this point

1551
01:49:39,490 --> 01:49:42,240
the regions that i'm looking at

1552
01:49:42,320 --> 01:49:43,850
look like

1553
01:49:49,860 --> 01:49:52,370
and they're not orthogonal to each other

1554
01:49:52,380 --> 01:49:58,300
so to find the probability that this point gets outside of this region

1555
01:49:58,320 --> 01:50:01,640
let's just a little bit messy

1556
01:50:02,320 --> 01:50:06,860
and that happens for all and bigger than two

1557
01:50:06,890 --> 01:50:09,870
i never knew this because

1558
01:50:09,920 --> 01:50:15,270
well i think this is this is probably a disaster that happened more teachers and

1559
01:50:15,270 --> 01:50:17,170
the people working in the field

1560
01:50:17,190 --> 01:50:20,360
because so often i have explained to people

1561
01:50:20,380 --> 01:50:23,070
how these two-dimensional pictures work

1562
01:50:23,100 --> 01:50:26,810
but i just get used to thinking that this is an easy problem and in

1563
01:50:26,810 --> 01:50:30,590
fact when you start looking at the problem for equals three

1564
01:50:30,640 --> 01:50:34,660
and the problem gets much more interesting and much more useful and much more practical

1565
01:50:34,960 --> 01:50:39,970
when n becomes three or four five or six or seven or eight beyond data

1566
01:50:39,970 --> 01:50:42,030
becomes a little too hard to do

1567
01:50:42,070 --> 01:50:45,330
but up until their it's it's very easy

1568
01:50:45,350 --> 01:50:54,830
OK so we make use of that a little bit

1569
01:50:54,840 --> 01:50:59,550
so we said orthogonal codes and simplex because if you scale the simplex code from

1570
01:50:59,550 --> 01:51:03,450
the orthogonal code have exactly the same error probability

1571
01:51:03,470 --> 01:51:06,180
OK in other words

1572
01:51:06,200 --> 01:51:11,010
this code here where i made the distances between the points square root of two

1573
01:51:11,010 --> 01:51:15,500
over two which corresponds to the distance between the point here

1574
01:51:16,210 --> 01:51:19,640
this and this have exactly the same error probability

1575
01:51:19,690 --> 01:51:24,620
so you can in fact find their probability for this or for this whatever you

1576
01:51:24,620 --> 01:51:26,010
find easier

1577
01:51:26,020 --> 01:51:29,650
you know i think it's easier to find the their probability for this

1578
01:51:29,900 --> 01:51:34,520
i've let you down the primrose path because in fact this one it's easier to

1579
01:51:34,520 --> 01:51:36,690
find their probability for

1580
01:51:36,700 --> 01:51:41,210
this one again you can find their probability if you want

1581
01:51:41,300 --> 01:51:44,860
but the energy difference between this

1582
01:51:44,910 --> 01:51:46,010
and this

1583
01:51:46,020 --> 01:51:47,120
it's simply

1584
01:51:48,580 --> 01:51:54,100
is simply the added energy that you have to use here to send the mean

1585
01:51:54,910 --> 01:51:55,890
of these

1586
01:51:55,890 --> 01:52:01,000
the mother of all batteries

1587
01:52:01,010 --> 01:52:03,030
it was invented

1588
01:52:03,070 --> 01:52:05,390
by robert

1589
01:52:05,440 --> 01:52:07,230
from the graph

1590
01:52:07,270 --> 01:52:09,880
robert on the graph

1591
01:52:09,880 --> 01:52:11,940
he was a professor

1592
01:52:11,960 --> 01:52:15,240
prince princeton in nineteen twenty nine

1593
01:52:15,300 --> 01:52:17,090
when he designed

1594
01:52:17,170 --> 01:52:20,560
what we now call the van de graaf generator which generated

1595
01:52:20,610 --> 01:52:23,680
i think something like four five hundred thousand volts

1596
01:52:23,760 --> 01:52:27,920
and then he came to MIT in nineteen thirty one was professor here

1597
01:52:27,970 --> 01:52:29,350
he built here

1598
01:52:29,360 --> 01:52:31,180
then the graph generator

1599
01:52:31,220 --> 01:52:33,900
which can generate one and a half million

1600
01:52:34,110 --> 01:52:39,710
we have the super one year

1601
01:52:39,710 --> 01:52:41,850
it's not in the room

1602
01:52:41,930 --> 01:52:44,840
it generates six hundred thousand volts

1603
01:52:44,890 --> 01:52:47,100
we use it in february

1604
01:52:47,180 --> 01:52:49,350
but not in c

1605
01:52:49,400 --> 01:52:51,060
it's sick

1606
01:52:51,070 --> 01:52:52,710
two high

1607
01:52:52,710 --> 01:52:54,880
we have another one which is not bad

1608
01:52:54,890 --> 01:52:56,640
you see that here

1609
01:52:56,740 --> 01:53:01,850
and this one in the winter would easily generate three hundred thousand four hundred thousand

1610
01:53:03,680 --> 01:53:05,840
more like hundred fifty thousand four

1611
01:53:05,880 --> 01:53:09,980
very respectable number nevertheless

1612
01:53:10,100 --> 01:53:14,390
could this machine kills me

1613
01:53:14,440 --> 01:53:16,590
frankly speaking i am not sure

1614
01:53:16,600 --> 01:53:18,190
i have never tried

1615
01:53:18,210 --> 01:53:23,820
but of course we can bring that to test today

1616
01:53:23,840 --> 01:53:27,820
the first thing i want to do with this then the graph

1617
01:53:27,880 --> 01:53:31,810
it is to see whether we can draw any spark there we could only draw

1618
01:53:33,270 --> 01:53:35,130
over half a centimeter

1619
01:53:35,140 --> 01:53:37,820
here we have a much higher voltage

1620
01:53:37,850 --> 01:53:39,390
we can probably do

1621
01:53:39,430 --> 01:53:42,270
much better

1622
01:53:42,310 --> 01:53:45,380
so i'm going to first run this machine

1623
01:53:45,390 --> 01:53:47,390
and then i'm going to turn off

1624
01:53:47,440 --> 01:53:50,680
the lights

1625
01:53:50,720 --> 01:53:54,340
and then we'll see

1626
01:53:54,380 --> 01:53:56,720
it's always nice

1627
01:53:56,780 --> 01:54:03,680
spotlights don't want to go off this is not unusual at MIT

1628
01:54:08,020 --> 01:54:11,240
of oil you can look fantastic

1629
01:54:11,260 --> 01:54:15,010
i don't have to do much myself

1630
01:54:15,020 --> 01:54:21,930
so that's the thing so i'll actually left the tensile there

1631
01:54:21,930 --> 01:54:23,930
note stop

1632
01:54:23,970 --> 01:54:27,060
let's see

1633
01:54:29,480 --> 01:54:31,600
you see we can draw sports

1634
01:54:31,610 --> 01:54:35,100
even though we could only draw them there over a distance of about half a

1635
01:54:36,100 --> 01:54:37,600
this is more like

1636
01:54:37,610 --> 01:54:42,510
four five and so to stand suspension behind

1637
01:54:42,510 --> 01:54:44,340
so that's one thing

1638
01:54:44,360 --> 01:54:45,630
you can do

1639
01:54:45,690 --> 01:54:47,420
with the

1640
01:54:47,470 --> 01:54:49,930
then the graph

1641
01:54:49,930 --> 01:54:53,170
i wonder if i could some confetti on

1642
01:54:53,180 --> 01:54:55,260
public goods

1643
01:54:55,390 --> 01:54:58,800
what do you think what happens

1644
01:54:58,920 --> 01:55:02,560
you good alex what do you think anyone things that it will get

1645
01:55:02,560 --> 01:55:05,940
that may be like on the call

1646
01:55:05,960 --> 01:55:10,630
o would all go boom

1647
01:55:10,770 --> 01:55:14,130
think it will go boom o things it but also

1648
01:55:14,180 --> 01:55:17,430
immediately if they don't raise your hand

1649
01:55:17,470 --> 01:55:21,060
and all things that it may actually some of them may go off but some

1650
01:55:21,060 --> 01:55:22,880
may actually stick to it

1651
01:55:22,880 --> 01:55:26,440
just like in the mysterious way the balloons

1652
01:55:27,350 --> 01:55:31,380
well let's bring this to attest that shouldn't be too hard

1653
01:55:31,460 --> 01:55:35,800
so we going to put

1654
01:55:35,850 --> 01:55:39,350
some on it

1655
01:55:39,430 --> 01:55:42,630
whatever happens it will go fast

1656
01:55:43,920 --> 01:55:46,690
you ready

1657
01:55:46,770 --> 01:55:52,600
so really gone

1658
01:55:52,640 --> 01:56:01,270
other one word

1659
01:56:01,330 --> 01:56:05,970
we understand that no right because whatever the charges on the dome

1660
01:56:06,500 --> 01:56:08,560
negative or positive i don't know

1661
01:56:08,570 --> 01:56:11,000
whatever the charges

1662
01:56:11,010 --> 01:56:12,110
course these

1663
01:56:12,130 --> 01:56:13,050
pieces of

1664
01:56:13,050 --> 01:56:14,010
the air

1665
01:56:39,740 --> 01:56:45,320
the racial

1666
01:57:00,750 --> 01:57:02,490
thank you

1667
01:57:34,370 --> 01:57:43,660
he was

1668
01:58:41,800 --> 01:58:47,250
the three hundred

1669
01:59:12,570 --> 01:59:19,090
all right

1670
01:59:23,600 --> 01:59:25,950
what he heard

1671
01:59:43,320 --> 01:59:47,170
four of

1672
01:59:54,130 --> 01:59:54,700
o thing

1673
02:00:08,960 --> 02:00:10,970
and he

1674
02:00:10,990 --> 02:00:15,140
you get

1675
02:00:33,890 --> 02:00:43,250
and and

1676
02:00:45,620 --> 02:00:51,330
the whole world series that

1677
02:00:51,350 --> 02:00:59,220
and all

1678
02:01:55,590 --> 02:01:58,410
i the mean

