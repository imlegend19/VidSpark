1
00:00:00,000 --> 00:00:04,610
so PAC MDP is very similar kind of property right well would optimal all but

2
00:00:04,630 --> 00:00:09,420
a polynomial number of time steps this is where bayes optimal but a polynomial number

3
00:00:09,420 --> 00:00:13,270
of time steps there are different in each other and the way in which are

4
00:00:13,270 --> 00:00:16,170
different i think it's really interesting

5
00:00:16,190 --> 00:00:20,540
and that this is sort of an open question in some ways so bayes optimal

6
00:00:20,540 --> 00:00:24,540
PAC MDP are not the same thing there's examples and culture and had have one

7
00:00:24,540 --> 00:00:28,480
in their paper in my students lihong li input but is the system is realized

8
00:00:28,650 --> 00:00:33,920
the basically show that here's the task where the bayes optimal action and the

9
00:00:33,940 --> 00:00:37,440
the near optimal action diverge

10
00:00:37,460 --> 00:00:42,810
in particular what happens is these be the bayes optimal algorithm sometimes decides that it's

11
00:00:42,810 --> 00:00:47,440
just not worth learning anything new right so it may take a couple steps before

12
00:00:47,440 --> 00:00:52,270
it actually gets this new information and pick between discounting and what not an opportunity

13
00:00:52,270 --> 00:00:55,250
cost and you know that maybe

14
00:00:55,250 --> 00:00:57,980
the optimal thing to do what you get out there but i don't really ever

15
00:00:57,980 --> 00:01:01,050
want to be there next sort of goes off this other way and by virtue

16
00:01:01,050 --> 00:01:03,610
of the fact that it goes off to so the way it never actually learned

17
00:01:03,610 --> 00:01:06,710
about what really happens in another state if it knew that the state was good

18
00:01:06,710 --> 00:01:08,550
it would have gone there but wasn't sure

19
00:01:10,290 --> 00:01:13,630
the support of

20
00:01:13,630 --> 00:01:20,460
could be solved by weaker prior of

21
00:01:20,480 --> 00:01:23,960
i don't know the examples that being the actually relatively weak priors

22
00:01:24,750 --> 00:01:29,360
so in fact when li hong his thesis to view i read through his profound

23
00:01:29,360 --> 00:01:32,500
like this is broken because you have two stronger prior needs to be true that

24
00:01:32,500 --> 00:01:37,020
strong prior and he just ended back to just read it because i just issue

25
00:01:37,070 --> 00:01:39,770
because they like it can be true ads can be true so i just told

26
00:01:39,770 --> 00:01:43,670
him it wasn't true but that was that was incorrect it was actually true

27
00:01:43,730 --> 00:01:46,040
i can i can point to example

28
00:01:46,360 --> 00:01:49,730
but what what what i think is really interesting about this is a very there's

29
00:01:49,730 --> 00:01:54,750
a fundamental difference in perspective between these two models that actually it's not clear to

30
00:01:54,750 --> 00:01:58,650
me anymore which one is exactly right so what near bayesian or or the bayes

31
00:01:59,730 --> 00:02:04,380
strategy says is my current self needs to get near optimal reward

32
00:02:04,420 --> 00:02:07,230
and it really doesn't care very much about the future self

33
00:02:07,250 --> 00:02:11,230
except have to do in this concert features of making a decision now

34
00:02:11,230 --> 00:02:12,270
and then

35
00:02:12,290 --> 00:02:15,840
now there's a new me essentially right i'd like oh i hate that old guy

36
00:02:15,840 --> 00:02:17,900
who put me in this horrible situation

37
00:02:19,590 --> 00:02:22,040
but there but here i am and i have to do the best i can

38
00:02:22,960 --> 00:02:27,310
and a lot of this sequential decision making is about trades of trade-offs between these

39
00:02:27,310 --> 00:02:33,040
various cells right so i would like some water now all but this is going

40
00:02:33,040 --> 00:02:35,900
help me now it's going to help me when i actually go and get the

41
00:02:35,900 --> 00:02:40,040
water so the me now has to say yes sure whatever it takes additional energy

42
00:02:40,040 --> 00:02:43,770
now but you all have your undying appreciation

43
00:02:43,820 --> 00:02:45,420
thank you

44
00:02:46,570 --> 00:02:48,000
like so so

45
00:02:48,000 --> 00:02:51,040
so this is this is the standard thing in fact the red

46
00:02:51,040 --> 00:02:53,190
a book about human decision making in end

47
00:02:53,250 --> 00:02:56,840
lot of psychologist talk about these things this way as well this the future self

48
00:02:56,840 --> 00:02:58,040
and and

49
00:02:58,070 --> 00:03:01,250
and current so so so so near bayesian is all about you know what's the

50
00:03:01,250 --> 00:03:03,570
what's what's good for me the current self now

51
00:03:03,650 --> 00:03:08,040
what's what's going to maximize my future discounted reward PAC MDP if you think about

52
00:03:08,040 --> 00:03:11,690
the way we talked about this yesterday it really is all about the future self

53
00:03:11,690 --> 00:03:15,190
it's like i'm going to do some things now that might be fairly stupid

54
00:03:15,210 --> 00:03:17,880
they're going to bound the number of stupid things i do but we're gonna but

55
00:03:17,880 --> 00:03:21,500
we still can do stupid things out all to what we want is that we're

56
00:03:21,500 --> 00:03:24,380
going to be in some future time in some state and we want to get

57
00:03:24,380 --> 00:03:26,790
near optimal reward at that time

58
00:03:26,820 --> 00:03:29,320
i don't care what it is now as much i care what is going to

59
00:03:29,320 --> 00:03:30,210
be that

60
00:03:30,210 --> 00:03:31,090
and so

61
00:03:31,110 --> 00:03:34,110
they really are very much focused on two different

62
00:03:39,630 --> 00:03:44,110
that is the world's press

63
00:03:48,040 --> 00:03:50,090
that's right

64
00:03:50,150 --> 00:03:54,940
the result is that she deserves

65
00:03:54,960 --> 00:03:58,360
in the sense that it is

66
00:04:00,480 --> 00:04:04,170
this was that that was not

67
00:04:04,190 --> 00:04:07,630
right and if we really believe it then we can just say we're acting for

68
00:04:07,630 --> 00:04:13,790
our current self and that is just the correct thing to do but it's but

69
00:04:13,790 --> 00:04:18,770
then on the other hand you know i mean this state and i maximizing or

70
00:04:18,790 --> 00:04:22,730
my reward for right now but if only my past self had done this exploration

71
00:04:22,730 --> 00:04:25,440
action i would be getting higher reward right now

72
00:04:25,460 --> 00:04:27,440
so i've been blown off

73
00:04:27,480 --> 00:04:28,940
by my past self

74
00:04:28,960 --> 00:04:34,500
if i'm if my past self was being near bayesian but on average that self

75
00:04:34,500 --> 00:04:40,570
is low probability or something like that is the right so so this right so

76
00:04:40,570 --> 00:04:45,150
the new bayesian does take into consideration future selves but averages them all out that

77
00:04:45,250 --> 00:04:48,310
so maybe maybe one way to look at it is this sort of expected case

78
00:04:48,310 --> 00:04:52,060
in this sort of worst-case right this is actually caring about the states i could

79
00:04:52,060 --> 00:05:00,710
get two with very low probability is a song by the

80
00:05:00,730 --> 00:05:02,750
function is

81
00:05:02,770 --> 00:05:05,250
the the best

82
00:05:05,250 --> 00:05:10,770
or there using sort of two different discount function to choosing one that's that's using

83
00:05:10,770 --> 00:05:12,460
first decision

84
00:05:12,480 --> 00:05:15,860
and yet and so in fact if you look at it if you look at

85
00:05:15,880 --> 00:05:20,270
human behaviour al behavior people people do this they seem to do something sort of

86
00:05:20,270 --> 00:05:24,380
in between future and current self carrying they don't do what appears to be the

87
00:05:24,380 --> 00:05:29,340
bayes optimal and they don't seem to be strictly PAC MDP as they've been described

88
00:05:29,360 --> 00:05:34,340
as are we've been described as having hyperbolic discounting geometric discounting

89
00:05:34,540 --> 00:05:38,610
that you will do

90
00:05:41,590 --> 00:05:45,480
i see what you're saying far more

91
00:05:45,530 --> 00:05:52,460
is requisite OK OK so

92
00:05:52,460 --> 00:05:55,880
i want to say you're bayesian but i mean that in a kind way OK

93
00:05:55,880 --> 00:05:59,590
but in the following sense is this i feel like this is what you just

94
00:05:59,590 --> 00:06:02,900
did to me use said and he said it's just about the test specification but

95
00:06:02,900 --> 00:06:05,500
these are two different has specifications

96
00:06:05,520 --> 00:06:09,430
right this is actually written down like formally defined and there's no hyperbolic discounting in

97
00:06:09,430 --> 00:06:13,940
it but what you said is but because it's got to be in its flexible

98
00:06:13,940 --> 00:06:17,400
and we could put in other things to make it the right path specification which

99
00:06:17,400 --> 00:06:19,210
we could do with this

100
00:06:19,230 --> 00:06:21,150
it's not clear that we could do with this

101
00:06:21,190 --> 00:06:25,790
but but the my point is that these two as they currently are out there

102
00:06:25,790 --> 00:06:29,590
are not quite right and so what the modification you suggest is one possibility and

103
00:06:29,590 --> 00:06:33,700
i am open to it another one i was thinking well or we could just

104
00:06:33,700 --> 00:06:35,380
stick with PAC MDP but either way

105
00:06:37,730 --> 00:06:40,210
we have to explain why it was the right thing to do and i think

106
00:06:40,570 --> 00:06:42,150
i think that's a really good point

107
00:06:42,150 --> 00:06:46,420
so i put this slide not because there's some published paper that covers this topic

108
00:06:46,420 --> 00:06:49,420
but because i thought it was sort of interesting and i was looking for feedback

109
00:06:49,420 --> 00:06:51,090
like that again

110
00:06:51,090 --> 00:06:54,570
OK alright so

111
00:06:54,630 --> 00:06:57,120
but that's not what we did in our so so we decided we're going to

112
00:06:57,120 --> 00:06:59,960
use some bayesian stuff and we decided to do a little bit differently so we

113
00:06:59,960 --> 00:07:05,000
actually putting bayesian priors together with PAC MDP

114
00:07:05,050 --> 00:07:09,500
this make probably no unhappy right because it's not bayes optimal we don't care about

115
00:07:09,500 --> 00:07:11,520
these optimal more trying to PAC MDP

116
00:07:11,520 --> 00:07:14,730
in the next round we randomly chosen x one given y one

117
00:07:14,790 --> 00:07:18,310
and this keeps going we then generate the next the next day y two which

118
00:07:19,040 --> 00:07:23,040
gives us the next output x two and this keeps going until we get to

119
00:07:23,170 --> 00:07:27,540
a final state so it took two n plus one stochastic steps or rules of

120
00:07:27,540 --> 00:07:31,120
some dye to generate the whole sequence x y

121
00:07:31,130 --> 00:07:33,530
everything was produced exactly once

122
00:07:33,540 --> 00:07:40,130
and the features were really restricted to this local decisions as local random steps

123
00:07:40,160 --> 00:07:42,100
OK so

124
00:07:42,170 --> 00:07:45,600
not going to say too much about generative models i imagine most people have seen

125
00:07:45,600 --> 00:07:51,250
these before and recognise an HMM it is is it is an example

126
00:07:51,300 --> 00:07:54,290
the nice thing about the courses that are very easy to train if i have

127
00:07:54,300 --> 00:07:57,710
if i have data as described all i have to do is come up all

128
00:07:57,710 --> 00:08:00,860
the events and normalize maybe do a a little bit of smoothing

129
00:08:00,880 --> 00:08:04,980
the problem of course is that this does not include any notion of loss

130
00:08:04,990 --> 00:08:09,270
apart from this sort of log loss approximation to the full sequence

131
00:08:09,280 --> 00:08:14,050
get a hamming distance of the full sequence on x times y

132
00:08:14,100 --> 00:08:17,740
and you can use any features you want you really have to respect these markovian

133
00:08:17,740 --> 00:08:22,540
independence assumptions if you want to keep the parameter space small

134
00:08:23,720 --> 00:08:26,770
from a machine learning perspective i think these are some of the least satisfying thing

135
00:08:26,770 --> 00:08:27,910
to do in NLP

136
00:08:27,920 --> 00:08:32,600
but we use them all the time because they're just so darn easy all you

137
00:08:32,600 --> 00:08:35,430
have to build this year decoder and a little perl scripts to count the events

138
00:08:35,430 --> 00:08:39,930
from from a from a treebank or some other this

139
00:08:39,980 --> 00:08:44,560
but of course i'm probably preaching to the choir if our goal is to do

140
00:08:44,560 --> 00:08:46,600
a particular type of prediction

141
00:08:46,650 --> 00:08:51,250
from x to y then there's no reason to ever learn the distribution over x

142
00:08:51,300 --> 00:08:52,680
of course

143
00:08:52,770 --> 00:08:55,780
if we really want to model that can do any kind of can kind of

144
00:08:55,780 --> 00:08:59,540
prediction or different types of prediction may be a model that can turn wine tax

145
00:08:59,540 --> 00:09:01,160
in x into y

146
00:09:01,170 --> 00:09:05,290
maybe you want translator can go in both directions that are just take an example

147
00:09:05,970 --> 00:09:09,040
then you may actually want the generative model so there are times when i think

148
00:09:09,040 --> 00:09:14,660
that's appropriate but if we have a particular decoding function we want to implement the

149
00:09:14,660 --> 00:09:17,880
discriminative method is probably the right way to go

150
00:09:20,600 --> 00:09:23,780
there's a there's a typo in the slide

151
00:09:23,790 --> 00:09:30,310
so the first the first thing i thought about discriminative models as they were starting

152
00:09:30,310 --> 00:09:34,160
to to arise in machine learning came about in in the mid nineties and the

153
00:09:34,160 --> 00:09:39,100
idea was that if you break your outputs just drop it into parts

154
00:09:39,150 --> 00:09:40,920
they can be built incrementally

155
00:09:41,000 --> 00:09:45,730
you can think about some kind of probabilistic automaton that that slowly consumes step by

156
00:09:45,730 --> 00:09:47,480
step the the

157
00:09:47,540 --> 00:09:51,350
sorry that slowly step by step reduces the output while looking at the input or

158
00:09:51,350 --> 00:09:53,100
maybe even consuming the input

159
00:09:53,100 --> 00:09:58,630
and we use the different across different classifications step for each part

160
00:09:58,860 --> 00:10:01,190
we can train that discriminative classifiers

161
00:10:01,450 --> 00:10:06,000
for each part on its own and gets it's probably just a classification problem so

162
00:10:06,000 --> 00:10:10,400
it could be multinomial distributions are just aggression SVM whatever you want to the classic

163
00:10:10,400 --> 00:10:14,390
example of this was magnum and sparse and this was the first statistical parser user

164
00:10:14,410 --> 00:10:18,970
decision tree and the idea was that it moved along the sequence step by step

165
00:10:19,060 --> 00:10:24,490
and make the decision about whether to attach the word or start a new constituent

166
00:10:24,730 --> 00:10:29,240
was essentially what should produce sparser you've come in

167
00:10:52,320 --> 00:10:54,470
i'll get there

168
00:10:54,780 --> 00:10:57,150
i organised this way

169
00:10:57,240 --> 00:10:59,730
partly for historical reasons

170
00:10:59,730 --> 00:11:03,850
and partly because i think it's a good get to this kinds of models

171
00:11:04,210 --> 00:11:06,500
our work will definitely get there

172
00:11:10,100 --> 00:11:11,820
OK so

173
00:11:12,080 --> 00:11:15,390
so the the general idea when you have these local decisions is that they tend

174
00:11:15,390 --> 00:11:20,100
to shy very well with with decoding methods that are based on search

175
00:11:20,150 --> 00:11:23,460
so the idea is that if you imagine that your data were generated by some

176
00:11:23,460 --> 00:11:28,780
kind of step-by-step automaton then really doing prediction is just running out of time and

177
00:11:28,930 --> 00:11:34,240
in in the other direction the prediction so the idea is in the greediest version

178
00:11:34,240 --> 00:11:38,300
you start out by making up but by using a predictor for x for peace

179
00:11:38,340 --> 00:11:41,990
structure one and then taking taking the output of that you make the next prediction

180
00:11:42,000 --> 00:11:45,100
about the next piece of structural call h two and you keep going into you

181
00:11:45,100 --> 00:11:49,750
have the whole structure and usually people don't do this greatly although occasionally they do

182
00:11:50,140 --> 00:11:53,770
this this sort of thinking works pretty well if you like to think about state

183
00:11:53,770 --> 00:11:58,850
space search and so this really caught on for a lot of different problems and

184
00:11:58,850 --> 00:12:01,100
and continues to be

185
00:12:01,100 --> 00:12:05,520
so what is see another great example of this is the maximum entropy markov model

186
00:12:05,660 --> 00:12:08,840
where you start out in your start state and then you look at the first

187
00:12:08,840 --> 00:12:13,400
x one and generate y one which is the label for the first the first

188
00:12:13,400 --> 00:12:17,180
input and then you generate the next one given the previous one and the next

189
00:12:17,180 --> 00:12:20,320
observation and you can keep going until you get to the

190
00:12:20,350 --> 00:12:23,560
and this takes and n plus one steps

191
00:12:24,430 --> 00:12:30,690
one one there are some flaws with this is it feels wrong and there's been

192
00:12:30,690 --> 00:12:33,610
a lot of being spilled over why it feels wrong what might be wrong with

193
00:12:33,610 --> 00:12:38,930
the one one thing that people have said is that these kinds of automaton based

194
00:12:38,930 --> 00:12:42,130
models lead to something called label bias

195
00:12:42,180 --> 00:12:43,400
which says

196
00:12:43,600 --> 00:12:48,100
hasyear as you sort of generating the the output structure y

197
00:12:48,120 --> 00:12:50,500
if you made a bad choice at some point

198
00:12:50,590 --> 00:12:54,620
and there's there's nothing you can do this can easily happen in hmm

199
00:12:54,710 --> 00:12:59,230
it might be the case that as you randomly generating from from an HMM

200
00:12:59,230 --> 00:13:03,910
you do so you move into estate it's incredibly unlikely to have generated the next

201
00:13:03,910 --> 00:13:10,390
one started the next the next input symbol x the next observation simple acts

202
00:13:10,480 --> 00:13:14,480
and if that happened then you know this whole sequence was bad and this just

203
00:13:14,480 --> 00:13:17,990
wouldn't have happened so when you're when you think about the generative process

204
00:13:18,030 --> 00:13:22,470
there's there's there's an understanding that there are things that could have gone horribly wrong

205
00:13:22,470 --> 00:13:25,810
that you would never have been able to recover from this can happen in any

206
00:13:25,810 --> 00:13:29,690
and then if you get into a bad state you're still forced to do something

207
00:13:29,700 --> 00:13:33,310
you have to kind of keep going and so what what what i guess the

208
00:13:33,310 --> 00:13:36,570
reason it's called label bias that you can keep paying attention to the previous labels

209
00:13:36,570 --> 00:13:38,460
and ignoring the observations

210
00:13:38,470 --> 00:13:41,980
OK if there's been a lot of question over whether this really matters whether this

211
00:13:41,980 --> 00:13:44,530
is a real problem there are a couple of other things to say about these

212
00:13:44,530 --> 00:13:48,500
kinds of models one is the inside the training doesn't match the testing

213
00:13:48,590 --> 00:13:50,060
when you train the model

214
00:13:50,080 --> 00:13:53,860
you train a bunch of classifiers make a bunch of decisions in the cascade

215
00:13:53,950 --> 00:13:57,740
but you only but you always when your training assume that all the previous decisions

216
00:13:57,750 --> 00:13:59,560
are made correctly

217
00:13:59,580 --> 00:14:04,860
your model is unprepared for the case where at some point something went wrong

218
00:14:04,870 --> 00:14:11,780
another another interesting fact is that some first for certain classes of models and restricting

219
00:14:11,780 --> 00:14:14,590
partition the unit square into rectangles

220
00:14:14,630 --> 00:14:21,240
eighteen years before mean are too i hope that of them but you

221
00:14:25,460 --> 00:14:31,740
first of all the problems of the squid is all this which

222
00:14:31,790 --> 00:14:34,290
this is the core of

223
00:14:34,300 --> 00:14:38,300
this wonderful book problems in discrete geometry by class

224
00:14:38,350 --> 00:14:40,710
william also in you

225
00:14:40,720 --> 00:14:45,320
and all the discrete geometry is which is extending more than one hundred fifty years

226
00:14:45,320 --> 00:14:48,530
it's about to open problems in high school

227
00:14:48,540 --> 00:14:54,540
so can understand which this is what i call that

228
00:14:54,550 --> 00:14:59,950
some of these problems are notoriously difficult and are intimately related to the questions

229
00:14:59,980 --> 00:15:01,940
in other fields of mathematics

230
00:15:01,950 --> 00:15:06,940
many probability of be of clever undergraduate or high-school students

231
00:15:06,950 --> 00:15:08,840
we have ingenious idea

232
00:15:08,840 --> 00:15:13,860
and the kinds of skills used in the mathematical knowledge

233
00:15:13,890 --> 00:15:15,840
so here is

234
00:15:19,570 --> 00:15:21,650
in december of two problems

235
00:15:21,650 --> 00:15:23,720
the first one is part

236
00:15:23,760 --> 00:15:27,130
well first of in nineteen forty six

237
00:15:27,140 --> 00:15:29,970
now we lament and point to the plane

238
00:15:29,990 --> 00:15:34,860
how many times the distance one in the core of these important

239
00:15:34,880 --> 00:15:39,740
the second one is a nineteen fifty followed by now

240
00:15:39,770 --> 00:15:43,900
and this is what is the come number of the unit distance graph an exact

241
00:15:43,900 --> 00:15:46,130
definition in the second

242
00:15:46,140 --> 00:15:49,640
in diameter last week in jerusalem in

243
00:15:50,780 --> 00:15:54,720
a long debate started which one of these two problems is more things

244
00:15:54,730 --> 00:16:01,490
and the information is what is the same as is

245
00:16:01,510 --> 00:16:02,780
i don't know

246
00:16:02,900 --> 00:16:06,290
so maybe you can think about how many people are aware of the first one

247
00:16:06,450 --> 00:16:07,990
and having of second one

248
00:16:08,040 --> 00:16:10,940
and use it as assembled to decide which one of these two

249
00:16:10,960 --> 00:16:12,550
his more famous

250
00:16:12,570 --> 00:16:20,150
but one day when we see her become famous is repeated many many conferences until

251
00:16:20,170 --> 00:16:21,100
one day

252
00:16:21,130 --> 00:16:24,340
progress is made

253
00:16:24,360 --> 00:16:27,150
OK so here is the citation for

254
00:16:28,010 --> 00:16:30,340
geometry junkyard

255
00:16:30,360 --> 00:16:34,600
well the website with all kinds of problems and there was finishing

256
00:16:34,630 --> 00:16:36,820
well already start the

257
00:16:36,830 --> 00:16:38,600
argument about

258
00:16:38,610 --> 00:16:42,220
what is the proper name of the national park

259
00:16:42,290 --> 00:16:45,730
in this particular website called the hot or that some of them

260
00:16:45,760 --> 00:16:49,720
and let q be very well with all points of the plane

261
00:16:49,840 --> 00:16:51,240
as far

262
00:16:51,260 --> 00:16:53,830
this is not locally finite of course

263
00:16:53,940 --> 00:16:57,820
and vx y is is an edge if and only if the points x and

264
00:16:58,490 --> 00:17:00,350
at distance one

265
00:17:00,360 --> 00:17:02,960
what is the chromatic number of g

266
00:17:06,360 --> 00:17:10,490
so these are two things

267
00:17:10,520 --> 00:17:14,260
one of starting configuration in the middle

268
00:17:14,320 --> 00:17:18,320
it's called the moser spindle

269
00:17:18,330 --> 00:17:22,650
is it full that these four colors easy straightforward

270
00:17:23,090 --> 00:17:26,340
this article or hexagonal tiling

271
00:17:26,380 --> 00:17:32,040
of the play was the first part of your body was already in nineteen fifty

272
00:17:32,040 --> 00:17:33,590
one by is bad

273
00:17:33,600 --> 00:17:36,700
that doesn't prove that you can kind the play in

274
00:17:36,710 --> 00:17:39,200
seven colors

275
00:17:39,220 --> 00:17:41,480
i will come back with more

276
00:17:41,510 --> 00:17:45,220
details about the distances guarantees the distance one

277
00:17:45,230 --> 00:17:46,500
it does not

278
00:17:46,620 --> 00:17:49,460
the main simple tree

279
00:17:49,460 --> 00:17:52,010
and in this form is what are called

280
00:17:52,020 --> 00:17:55,600
it can transfer or more precisely this we can't transfer

281
00:17:55,690 --> 00:18:00,300
five small subgraphs such that in any kind of its users

282
00:18:00,310 --> 00:18:04,390
two specific verses must in the same of

283
00:18:04,450 --> 00:18:06,480
in this case is

284
00:18:06,510 --> 00:18:08,500
these two triangles

285
00:18:08,510 --> 00:18:11,310
if you can do it in three colors these two vertices must have the same

286
00:18:11,310 --> 00:18:13,340
kind of thing

287
00:18:13,340 --> 00:18:15,210
so this is one

288
00:18:15,250 --> 00:18:19,300
you need to use force

289
00:18:25,490 --> 00:18:28,990
so what makes this problem for us

290
00:18:29,060 --> 00:18:32,570
it is fifty seven years

291
00:18:32,580 --> 00:18:35,510
he was born in nineteen fifty

292
00:18:35,560 --> 00:18:37,140
very simple to understand

293
00:18:37,160 --> 00:18:43,600
at least five occasions credited with

294
00:18:43,660 --> 00:18:45,890
and so on

295
00:18:45,970 --> 00:18:48,330
who we are we almost

296
00:18:48,360 --> 00:18:49,940
and martin car

297
00:18:49,950 --> 00:18:53,820
so the problem is also famous because right now it is part of the story

298
00:18:53,820 --> 00:18:59,350
because so if there so very nice studies focused on

299
00:18:59,410 --> 00:19:03,240
it trace the true ownership of this problem

300
00:19:03,590 --> 00:19:07,820
he was start to nelson was acquired from the university of chicago

301
00:19:07,870 --> 00:19:09,620
when he came up with this problem

302
00:19:09,630 --> 00:19:12,730
he actually labeled b

303
00:19:12,740 --> 00:19:14,480
of course

304
00:19:15,420 --> 00:19:18,730
the requirements were established in nineteen fifty

305
00:19:19,550 --> 00:19:22,960
and since then no progress has been made

306
00:19:24,720 --> 00:19:26,370
if you want the papers

307
00:19:26,380 --> 00:19:28,580
that are dedicated to this problem

308
00:19:28,690 --> 00:19:32,100
it some places rules that boring

309
00:19:32,160 --> 00:19:34,990
moscow moscow state university

310
00:19:35,000 --> 00:19:36,980
and those who works on

311
00:19:37,040 --> 00:19:39,210
the problem and related to

312
00:19:41,700 --> 00:19:43,240
like what he was

313
00:19:45,380 --> 00:19:46,820
a disaster

314
00:19:46,860 --> 00:19:48,720
the closest

315
00:19:48,730 --> 00:19:53,830
i call it an opportunity

316
00:19:53,840 --> 00:19:57,440
OK so what made famous problem is the answer

317
00:19:58,130 --> 00:20:00,080
all of is still in every

318
00:20:00,090 --> 00:20:04,510
we conferencedisplay with problems and you would be the man who was

319
00:20:04,540 --> 00:20:09,620
and as a result of the more impressed on problem all over the

320
00:20:09,720 --> 00:20:11,720
any other type of mathematics

321
00:20:11,720 --> 00:20:19,430
OK so was a sample of a few variations

322
00:20:20,200 --> 00:20:23,210
this particular problem genoa for example

323
00:20:23,290 --> 00:20:25,350
in order to fooled

324
00:20:26,140 --> 00:20:31,480
you need at least five although needed is a finite graph

325
00:20:31,570 --> 00:20:34,850
they can be embedded in the plane so there

326
00:20:34,860 --> 00:20:40,940
the edges are at distance one in which was five

327
00:20:40,950 --> 00:20:44,630
so for instance since the start with four toes

328
00:20:44,720 --> 00:20:46,460
the simplest example

329
00:20:46,630 --> 00:20:48,000
we saw

330
00:20:48,840 --> 00:20:52,980
as very few words in this triangle harbor trying to flee class

331
00:20:53,000 --> 00:20:54,770
how about graphs larger

332
00:20:54,830 --> 00:20:58,090
all this has been sold

333
00:20:58,100 --> 00:21:02,520
another variation

334
00:21:02,600 --> 00:21:06,620
you can kind of the plane in six colors and quite a few

335
00:21:06,640 --> 00:21:07,920
ways to do it

336
00:21:07,930 --> 00:21:12,460
with different types of tiling such that five of the callers will miss the distance

337
00:21:14,460 --> 00:21:21,230
the the sixth of may is another distance not the distance one

338
00:21:21,270 --> 00:21:24,460
i would mention so in

339
00:21:24,490 --> 00:21:25,640
dimensions we

340
00:21:25,730 --> 00:21:29,320
no the concurrent space fifteen colours

341
00:21:29,330 --> 00:21:33,800
in recent one of queen one great-grandchildren

342
00:21:33,840 --> 00:21:36,160
it's the end of the november alone

343
00:21:36,210 --> 00:21:38,070
now pushed and pulled the need

344
00:21:38,080 --> 00:21:40,350
sixty six dollars

345
00:21:41,820 --> 00:21:44,090
and that is different

346
00:21:44,090 --> 00:21:46,160
that of course

347
00:21:47,110 --> 00:21:49,020
it's all i know

348
00:21:49,130 --> 00:21:50,780
how to use it for

349
00:21:55,930 --> 00:22:01,480
when we look at is the posterior all in each image please

350
00:22:01,500 --> 00:22:06,920
which is based which is simply the ratio of positive to negative three years four

351
00:22:08,160 --> 00:22:09,670
here two to three

352
00:22:09,830 --> 00:22:12,760
one to five years one

353
00:22:12,780 --> 00:22:16,520
positive negative so it you is rather than you would say

354
00:22:16,540 --> 00:22:18,630
this is the first

355
00:22:18,740 --> 00:22:22,750
the second is the belief that fall

356
00:22:22,840 --> 00:22:26,040
instance based looks like this

357
00:22:26,040 --> 00:22:28,700
and ranking it looks like this

358
00:22:28,710 --> 00:22:30,650
OK so quickly

359
00:22:30,710 --> 00:22:33,330
this is the

360
00:22:33,340 --> 00:22:35,540
this the there

361
00:22:35,540 --> 00:22:39,360
this is because we times

362
00:22:39,370 --> 00:22:44,170
in the ranking nationally goal is to going to say only five times in the

363
00:22:44,180 --> 00:22:45,680
right kind of thing

364
00:22:46,590 --> 00:22:47,960
and so

365
00:22:47,990 --> 00:22:52,690
and this is quite nice ranking because you know you go from most of those

366
00:22:52,700 --> 00:22:56,950
on the left side of one that you have the right

367
00:22:56,960 --> 00:22:57,930
the next

368
00:22:58,070 --> 00:23:01,520
so that's fine

369
00:23:02,710 --> 00:23:11,160
what we do we use decision tree probability estimator we use the posterior of themselves

370
00:23:11,820 --> 00:23:18,190
we will lose their all and you converts them into probabilistic one although i actually

371
00:23:18,270 --> 00:23:21,950
think that very often much easier to calculate

372
00:23:22,050 --> 00:23:28,740
this all that is if you want to probability what do you say forty five

373
00:23:28,910 --> 00:23:30,190
platforms one

374
00:23:30,280 --> 00:23:31,880
o point

375
00:23:31,890 --> 00:23:33,310
two five factors

376
00:23:33,330 --> 00:23:34,130
plus three

377
00:23:34,150 --> 00:23:36,230
o point for so long

378
00:23:37,820 --> 00:23:43,690
i that the probability is that in this way so these these hormones are the

379
00:23:43,690 --> 00:23:50,270
same as director but they are distributed along the real axis from one zero

380
00:23:53,180 --> 00:23:58,980
right so this is all new and and you may wonder why i tell you

381
00:23:58,980 --> 00:24:02,520
this now let's look at this a little bit

382
00:24:02,530 --> 00:24:05,200
from analysis so

383
00:24:05,230 --> 00:24:18,780
i use rather than as lot for visualizing not necessarily related to visualize by the

384
00:24:19,390 --> 00:24:25,550
right it is i don't get the question mentions stands for receiver operating characteristic it's

385
00:24:25,550 --> 00:24:27,030
totally obsolete

386
00:24:27,040 --> 00:24:31,740
this is this comes from a signal detection theory has its roots in the second

387
00:24:31,740 --> 00:24:32,840
world war one

388
00:24:32,990 --> 00:24:34,410
very many things do

389
00:24:35,550 --> 00:24:39,530
but that's just treat this as an issue without real

390
00:24:42,550 --> 00:24:45,550
so what you can do do is basically

391
00:24:45,580 --> 00:24:48,040
if you want to visualize performance

392
00:24:48,050 --> 00:24:50,800
the main idea is to be

393
00:24:50,810 --> 00:24:54,030
constitution in in each leaf is that

394
00:24:54,040 --> 00:24:57,380
so for instance this the vector for

395
00:24:57,390 --> 00:25:01,110
four of them on the right is the vision of the two

396
00:25:01,500 --> 00:25:02,670
this is the right

397
00:25:02,690 --> 00:25:04,690
and if you see that was

398
00:25:04,700 --> 00:25:06,290
the ranking order

399
00:25:06,300 --> 00:25:09,300
that could like this

400
00:25:09,310 --> 00:25:14,540
and this is a bit like or you know that was

401
00:25:14,540 --> 00:25:15,510
the about

402
00:25:15,640 --> 00:25:19,760
the wealth distribution so service a twenty percent of people

403
00:25:20,070 --> 00:25:21,970
eighty percent of the

404
00:25:22,260 --> 00:25:27,210
and that's the similarity is so when you could you say something like

405
00:25:27,300 --> 00:25:32,290
you can say things like the numbers on the right you can say things like

406
00:25:32,300 --> 00:25:34,570
forty percent of the negatives

407
00:25:35,530 --> 00:25:37,670
together with six

408
00:25:38,560 --> 00:25:40,040
and of course you want that

409
00:25:41,040 --> 00:25:43,090
b so that basically

410
00:25:43,110 --> 00:25:45,920
when the ROC curve comes from

411
00:25:45,930 --> 00:25:48,310
so in this what you see

412
00:25:48,330 --> 00:25:53,050
it's on the vertical axis you've both examples on the

413
00:25:53,460 --> 00:25:57,880
for example you know some of the

414
00:25:58,480 --> 00:26:00,760
essentially the

415
00:26:01,000 --> 00:26:05,000
all of these offices classifier

416
00:26:06,920 --> 00:26:13,060
you may be more used to actually not having as numbers on the axes

417
00:26:13,300 --> 00:26:16,380
relative numbers namely

418
00:26:16,400 --> 00:26:18,540
you're you're probably used

419
00:26:18,540 --> 00:26:21,340
one of the very few which is the absolute number of

420
00:26:21,350 --> 00:26:23,540
possibly not yet

421
00:26:24,310 --> 00:26:26,010
right so instead

422
00:26:26,010 --> 00:26:28,070
the previous era we would say

423
00:26:30,490 --> 00:26:35,630
in the next the same four out of ten positive examples so we have point

424
00:26:35,630 --> 00:26:41,550
four and true positive rate use except for

425
00:26:41,570 --> 00:26:45,480
this has confused me for a long time here is how i got to grips

426
00:26:45,480 --> 00:26:46,520
with this

427
00:26:46,530 --> 00:26:52,400
i now switch very freely between these two perspectives and the reason is that

428
00:26:52,460 --> 00:26:53,780
if you do

429
00:26:53,810 --> 00:26:57,590
the ROC analysis thank you essentially

430
00:26:57,640 --> 00:27:04,440
distribution of the population and yourselves in your space likelihood ratios

431
00:27:04,450 --> 00:27:08,000
so it's the thing that you multiply with the prior

432
00:27:08,010 --> 00:27:09,470
to get posterior

433
00:27:10,760 --> 00:27:14,340
so this is if you want to racial because you

434
00:27:15,330 --> 00:27:19,230
the prior of the distribution of the equation

435
00:27:19,240 --> 00:27:20,170
this wrong

436
00:27:20,180 --> 00:27:21,520
your thing

437
00:27:21,540 --> 00:27:26,100
if you instead want posterior odds

438
00:27:26,130 --> 00:27:28,540
then you want to

439
00:27:28,560 --> 00:27:34,420
in then this space is this the right place to work and now know can

440
00:27:35,620 --> 00:27:39,980
using it with equal suspicion so it doesn't matter

441
00:27:39,990 --> 00:27:44,490
but you can imagine that if you have a little distribution this space will be

442
00:27:44,490 --> 00:27:46,570
coming back to the rest of the world

443
00:27:48,770 --> 00:27:56,490
just i just choose whatever seems most appropriate the patient and most of the the

444
00:27:56,500 --> 00:28:03,000
stuff that actually use the absolute numbers of interest there are

445
00:28:03,270 --> 00:28:10,250
this is sometimes called coverage space and before possible PN space that

446
00:28:10,920 --> 00:28:13,040
i used the whole

447
00:28:13,050 --> 00:28:19,010
these rocks space is important to distinguish between the two

448
00:28:19,010 --> 00:28:24,800
as you know we have again

449
00:28:29,480 --> 00:28:32,330
presidential elections this year in slovenia

450
00:28:32,350 --> 00:28:35,270
and as you know we have seven candidates for

451
00:28:36,360 --> 00:28:39,120
president but

452
00:28:39,210 --> 00:28:40,660
some time ago

453
00:28:40,670 --> 00:28:44,250
we had about eighty eight presidents

454
00:28:44,270 --> 00:28:46,030
that was in

455
00:28:46,030 --> 00:28:47,450
ninety seven

456
00:28:47,510 --> 00:28:49,750
and these are the names

457
00:28:49,800 --> 00:28:53,360
and then t slovenia decided to

458
00:28:53,420 --> 00:28:57,080
organised debates

459
00:28:57,100 --> 00:28:58,620
eight evenings

460
00:28:58,630 --> 00:29:01,040
and the are three candidates

461
00:29:01,960 --> 00:29:03,600
to appear to each evening

462
00:29:03,630 --> 00:29:04,680
they have

463
00:29:04,690 --> 00:29:05,860
yeah that was

464
00:29:05,910 --> 00:29:08,770
it was running in

465
00:29:08,830 --> 00:29:10,760
two weeks

466
00:29:14,260 --> 00:29:19,160
friday is already can so nothing there

467
00:29:19,160 --> 00:29:21,860
and then the next week again monday

468
00:29:21,940 --> 00:29:24,810
tuesday wednesday thirteen so there were two weeks

469
00:29:24,870 --> 00:29:28,340
three candidates each week

470
00:29:28,380 --> 00:29:32,990
and what happened is one evening in TV studio

471
00:29:33,560 --> 00:29:34,980
there are three candidates

472
00:29:34,980 --> 00:29:37,270
two serious ones and then

473
00:29:37,290 --> 00:29:39,250
one which is not listed this

474
00:29:39,250 --> 00:29:42,510
and there are seven days later

475
00:29:42,530 --> 00:29:45,510
the same two serious candidates appeared and then the

476
00:29:45,570 --> 00:29:46,870
there's a guy

477
00:29:46,890 --> 00:29:48,960
who was not important so

478
00:29:48,960 --> 00:29:52,420
a colleague of mine dragon the shelter

479
00:29:52,440 --> 00:29:54,600
letter to the editor

480
00:29:54,630 --> 00:29:58,360
saying that they should ask mathematicians

481
00:29:59,620 --> 00:30:00,970
why can't they

482
00:30:03,090 --> 00:30:06,540
this evening's debate so that each candidate

483
00:30:06,550 --> 00:30:08,250
it would appear at most

484
00:30:10,080 --> 00:30:13,870
each pair of candidates with appear at most once

485
00:30:13,890 --> 00:30:16,390
so if you ask mathematicians

486
00:30:16,440 --> 00:30:20,640
we could have solved the problem for you so the question is can we group

487
00:30:20,660 --> 00:30:24,390
the eight candidates into eight triples in such a way that no no two of

488
00:30:24,390 --> 00:30:26,820
them will meet more than once

489
00:30:32,800 --> 00:30:34,760
if you want to

490
00:30:34,770 --> 00:30:35,770
so this

491
00:30:35,780 --> 00:30:39,380
the problem you have to find

492
00:30:42,580 --> 00:30:43,800
to make sure

493
00:30:43,820 --> 00:30:48,140
so what we are asking is that there exist it's three

494
00:30:48,190 --> 00:30:51,580
configuration combinatorial configuration

495
00:30:51,640 --> 00:31:00,340
and it turns out there is exactly one of its three combinatorial configuration is called

496
00:31:00,340 --> 00:31:04,910
images configurations so here you may view

497
00:31:05,580 --> 00:31:06,530
o point

498
00:31:06,530 --> 00:31:11,120
eight points of the candidates it lies are TV debates

499
00:31:11,180 --> 00:31:13,350
for instance

500
00:31:13,450 --> 00:31:16,070
if you have such a debate

501
00:31:16,130 --> 00:31:17,860
that would be the first

502
00:31:18,270 --> 00:31:21,310
evening with these three guys

503
00:31:21,330 --> 00:31:25,960
what is meant but then you could schedule differences so that they would meet these

504
00:31:25,960 --> 00:31:28,980
two guys were not meet twice just once

505
00:31:30,690 --> 00:31:32,770
but who knows

506
00:31:32,820 --> 00:31:35,500
history cannot be

507
00:31:37,000 --> 00:31:37,790
we know

508
00:31:37,820 --> 00:31:39,520
who wants

509
00:31:40,440 --> 00:31:44,840
and let me just give you a surprising connection

510
00:31:44,860 --> 00:31:46,950
if you look at this

511
00:31:46,950 --> 00:31:48,660
no presidential

512
00:31:48,670 --> 00:31:51,770
media coverage of presidential election

513
00:31:51,780 --> 00:31:55,760
what we're going to learn is there is a very deep connection

514
00:31:55,770 --> 00:31:56,830
to this

515
00:31:56,840 --> 00:32:00,050
three-dimensional sculpture that is now

516
00:32:00,860 --> 00:32:04,770
technical museum in this

517
00:32:07,480 --> 00:32:12,590
and here is just part of it because it represents the drawing of the graph

518
00:32:12,620 --> 00:32:14,040
on the other tourists

519
00:32:14,050 --> 00:32:17,100
and that the graph is related

520
00:32:17,110 --> 00:32:18,720
to that

521
00:32:18,730 --> 00:32:22,620
maybe this kind of configuration actually

522
00:32:22,640 --> 00:32:24,120
it's not just the graph

523
00:32:24,140 --> 00:32:25,660
is the group itself

524
00:32:25,660 --> 00:32:27,090
so here

525
00:32:27,110 --> 00:32:28,590
the vertices

526
00:32:31,490 --> 00:32:33,750
to the automorphisms

527
00:32:33,840 --> 00:32:35,180
of that

528
00:32:36,440 --> 00:32:41,300
so there is a very deep connection between geometry

529
00:32:41,300 --> 00:32:42,770
what we mean by the air

530
00:32:42,780 --> 00:32:46,870
and which for this particular example you wind up with an estimate of the standard

531
00:32:47,930 --> 00:32:50,050
of the estimate of tau

532
00:32:50,070 --> 00:32:53,060
o point one five one

533
00:32:53,070 --> 00:32:55,860
that's what the distribution of tau values looks like

534
00:32:55,910 --> 00:32:59,590
that's already kind of interesting because what is this curve remind you of

535
00:32:59,600 --> 00:33:01,430
this is sort of gaussians

536
00:33:01,440 --> 00:33:03,060
shaped curve

537
00:33:03,790 --> 00:33:04,580
the data

538
00:33:04,600 --> 00:33:09,230
i have nothing to do with the gas in distribution the data are exponentially distributed

539
00:33:09,240 --> 00:33:12,880
but notice that the estimator itself

540
00:33:12,930 --> 00:33:14,350
is proportional

541
00:33:14,390 --> 00:33:16,140
two of some

542
00:33:16,190 --> 00:33:17,620
of the data values

543
00:33:17,640 --> 00:33:21,760
and there's fifty terms in the summer and recall with the central limit theorem says

544
00:33:21,790 --> 00:33:24,960
it says that if you take a sum of random variables in the limit that

545
00:33:24,960 --> 00:33:28,720
there's a large number of terms in that some of the resulting

546
00:33:28,800 --> 00:33:33,120
value will follow calcium distribution and so that's what we're seeing here and in fact

547
00:33:33,120 --> 00:33:38,140
it's a very general property of maximum likelihood estimators is that for sufficiently large number

548
00:33:38,140 --> 00:33:41,370
of events first sufficiently large data sample

549
00:33:41,430 --> 00:33:44,580
the distribution of the maximum likelihood estimator

550
00:33:44,610 --> 00:33:52,730
asymptotically approaches a calcium distribution very important property

551
00:33:52,740 --> 00:33:55,260
all right so so that's one way of

552
00:33:55,280 --> 00:33:59,880
doing your error analysis to actually write a monte carlo programme to simulate the experiment

553
00:33:59,900 --> 00:34:01,540
there are other ways

554
00:34:01,550 --> 00:34:02,840
and there's another way

555
00:34:02,850 --> 00:34:07,270
it is to use what's called the information inequality sometimes also called the

556
00:34:08,250 --> 00:34:12,130
kramer fresh air inequality are CF inequality

557
00:34:12,180 --> 00:34:17,810
this is an inequality which sets a lower bound on any estimator not only estimators

558
00:34:17,810 --> 00:34:23,340
from the maximum likelihood method but this is the lower bound on the variance of

559
00:34:23,340 --> 00:34:24,980
any estimator

560
00:34:24,980 --> 00:34:28,530
and in in the case where i have just a single parameter the formula is

561
00:34:28,530 --> 00:34:29,810
the following

562
00:34:29,870 --> 00:34:34,250
the variance of any estimator theta had must be greater than or equal to

563
00:34:34,330 --> 00:34:35,500
one plus

564
00:34:35,500 --> 00:34:40,250
the derivative with of the bias with respect to the parameter square

565
00:34:40,300 --> 00:34:42,530
divided by the expectation value

566
00:34:42,540 --> 00:34:47,180
of the negative of the second derivative of the log likelihood function

567
00:34:47,190 --> 00:34:50,150
kind of long formula but was not too bad the kind of thing we can

568
00:34:50,150 --> 00:34:51,080
work out very

569
00:34:51,710 --> 00:34:53,540
easily in many cases

570
00:34:53,580 --> 00:34:57,150
in many problems the biases these are small

571
00:34:57,180 --> 00:35:02,540
negligibly small or in some cases even identically zero so the numerator is very often

572
00:35:02,540 --> 00:35:04,070
just one

573
00:35:06,530 --> 00:35:09,510
furthermore in many problems of practical interest

574
00:35:09,520 --> 00:35:12,460
even though i said that this is an inequality

575
00:35:12,510 --> 00:35:14,650
very often a quality

576
00:35:14,680 --> 00:35:16,370
approximately holds

577
00:35:16,390 --> 00:35:20,710
that is to say that the some approximation i can replace this greater than equal

578
00:35:20,710 --> 00:35:22,360
to sign by

579
00:35:22,370 --> 00:35:25,290
an equal sign or approximately equal size

580
00:35:25,300 --> 00:35:26,750
and in that case

581
00:35:26,760 --> 00:35:29,730
so if i can neglect the bias and if i can regard that as an

582
00:35:29,730 --> 00:35:34,580
approximate equality that means that i can estimate the variance of my

583
00:35:34,600 --> 00:35:39,620
estimator by minus one over the expectation value of the second derivative

584
00:35:39,630 --> 00:35:41,360
of log l

585
00:35:43,570 --> 00:35:45,270
find so actually

586
00:35:45,280 --> 00:35:49,460
that expectation value is a function of the true and unknown parameters i need some

587
00:35:49,460 --> 00:35:50,300
way of

588
00:35:50,320 --> 00:35:52,240
estimating the variance

589
00:35:52,240 --> 00:35:55,970
of the estimator of the parameter so the way you can estimate this is simply

590
00:35:55,970 --> 00:35:58,250
by evaluating the second derivative

591
00:35:58,260 --> 00:36:00,090
of the log likelihood function

592
00:36:02,100 --> 00:36:03,800
maximum likelihood estimator

593
00:36:03,810 --> 00:36:08,930
so that's the formula that you can use in practice two to get the error

594
00:36:08,930 --> 00:36:13,430
bar to get the square root of that we give you the standard deviation of

595
00:36:13,430 --> 00:36:16,280
data had and that's the that's the number the report

596
00:36:16,320 --> 00:36:19,620
as the statistical error

597
00:36:19,630 --> 00:36:23,840
four for your parameter estimates you do i have another example

598
00:36:23,890 --> 00:36:28,360
i should mention here that many of you have probably used route or mean we

599
00:36:28,410 --> 00:36:32,290
argmin we within route to to do some sort of parameter fitting

600
00:36:32,290 --> 00:36:35,340
and you'll notice that when you do the parameter fitting at the end of the

601
00:36:35,340 --> 00:36:40,030
exercise it will spit out some number to the screen that calls the statistical error

602
00:36:40,180 --> 00:36:43,460
and you might ask yourself where is it coming up with that number

603
00:36:43,560 --> 00:36:47,800
and the answer is what it's doing is it's making a couple of assumptions it's

604
00:36:47,800 --> 00:36:51,780
assuming that the bias is zero it's assuming that you can you can put equal

605
00:36:51,780 --> 00:36:56,960
sign there and what route is doing is simply evaluating the second derivatives of the

606
00:36:56,960 --> 00:36:59,120
likelihood function numerically

607
00:36:59,250 --> 00:37:04,780
and using that to basically give you the standard deviation of your parameter estimates so

608
00:37:04,780 --> 00:37:08,430
that somehow is the mathematics behind what route is doing what it gives you the

609
00:37:08,430 --> 00:37:13,180
errors in if it

610
00:37:13,230 --> 00:37:17,130
now we can use this formula as the basis of another

611
00:37:17,210 --> 00:37:21,280
method which are called the graphical methods

612
00:37:21,290 --> 00:37:23,220
and the idea is the following

613
00:37:23,230 --> 00:37:24,180
is it if you

614
00:37:24,190 --> 00:37:27,220
i wish i had a black would actually make a drawing but just use my

615
00:37:27,220 --> 00:37:31,150
hands but you can imagine that the

616
00:37:31,210 --> 00:37:35,590
the log likelihood function is some curves that comes up and goes down

617
00:37:35,680 --> 00:37:38,500
is very often have sort of parabolic shape

618
00:37:38,550 --> 00:37:42,040
and there will be some value of the parameter for which it is the maximum

619
00:37:42,040 --> 00:37:46,540
the value of data for which the log likelihood function is the maximum that's that's

620
00:37:46,540 --> 00:37:51,590
the ML estimator that had so what i want to do now is to expand

621
00:37:51,590 --> 00:37:53,190
the log likelihood function

622
00:37:53,230 --> 00:37:56,500
in a taylor series about its maximum

623
00:37:56,560 --> 00:38:00,800
so say the log likelihood function is logo of data had

624
00:38:00,810 --> 00:38:02,800
plus there's the first derivative terms

625
00:38:02,840 --> 00:38:04,960
with data miners that had

626
00:38:04,960 --> 00:38:07,230
and there is the second derivative term with

627
00:38:07,310 --> 00:38:09,690
miles that's where and so forth

628
00:38:09,730 --> 00:38:14,490
let me stop there let me just take the second order and stuff

629
00:38:14,510 --> 00:38:15,870
right now

630
00:38:15,900 --> 00:38:20,370
the first term was the first term log l of data had by construction that's

631
00:38:20,970 --> 00:38:25,580
that's the maximum of logo because of define had to be the value of data

632
00:38:25,580 --> 00:38:26,640
for which l

633
00:38:26,660 --> 00:38:28,350
is maximal

634
00:38:28,360 --> 00:38:30,610
so therefore the second term is zero

635
00:38:30,660 --> 00:38:35,540
right because the derivatives of log l has to be zero at the maximum

636
00:38:35,540 --> 00:38:36,970
and the third term

637
00:38:38,080 --> 00:38:40,620
this is the second derivative of log l

638
00:38:40,630 --> 00:38:44,880
with respect to the parameter value at the ML estimator

639
00:38:44,980 --> 00:38:46,090
but for that

640
00:38:46,110 --> 00:38:49,250
i can flick back and use the formula that i had on the previous over

641
00:38:49,250 --> 00:38:54,650
here i can relate that to the to the variance of my estimator

642
00:38:54,700 --> 00:38:58,010
OK so if i take that formula and use i use that

643
00:38:58,020 --> 00:38:59,810
to replace

644
00:38:59,810 --> 00:39:04,610
the probability of one because if t one is equal to hand

645
00:39:04,620 --> 00:39:07,680
we can add that you this situation

646
00:39:07,720 --> 00:39:11,170
or this situation

647
00:39:13,310 --> 00:39:16,690
what's the probability of this

648
00:39:16,740 --> 00:39:18,750
i work for

649
00:39:18,800 --> 00:39:21,350
what's the probability of

650
00:39:21,400 --> 00:39:25,930
one four

651
00:39:25,950 --> 00:39:29,030
that's what it

652
00:39:30,750 --> 00:39:33,230
this is exactly the marginalization

653
00:39:35,100 --> 00:39:36,650
that t one

654
00:39:36,680 --> 00:39:38,590
is equal to heads

655
00:39:38,600 --> 00:39:40,470
is equal to the sum

656
00:39:40,480 --> 00:39:42,520
over the values of t

657
00:39:43,910 --> 00:39:46,360
of p of p one

658
00:39:46,410 --> 00:39:48,170
because two had

659
00:39:49,010 --> 00:39:52,140
p two

660
00:39:52,280 --> 00:39:53,820
if the two

661
00:39:53,890 --> 00:39:55,570
is equal

662
00:39:58,340 --> 00:39:59,740
we have p

663
00:39:59,760 --> 00:40:02,910
you are because had

664
00:40:03,850 --> 00:40:05,330
because to head

665
00:40:05,460 --> 00:40:06,750
if the two

666
00:40:10,090 --> 00:40:11,190
two one

667
00:40:11,260 --> 00:40:12,760
because had

668
00:40:12,820 --> 00:40:15,780
because that

669
00:40:15,840 --> 00:40:19,820
that's exactly what marginalization

670
00:40:19,870 --> 00:40:25,810
is it what you know

671
00:40:25,820 --> 00:40:27,090
of course in this case

672
00:40:27,100 --> 00:40:31,450
very very simple why because these two rivals i made them

673
00:40:33,200 --> 00:40:37,070
so in fact when asked what the probability of q when he calls it you

674
00:40:37,070 --> 00:40:40,520
could think immediately is one happen because it doesn't

675
00:40:40,530 --> 00:40:43,740
what happens the other in some cases these viruses will be

676
00:40:45,210 --> 00:40:47,970
in fact some cases you have to three

677
00:40:48,020 --> 00:40:50,780
before and after tn

678
00:40:50,830 --> 00:40:56,510
when you have an network of dependency things by

679
00:40:56,520 --> 00:40:58,970
how do you compute

680
00:40:59,050 --> 00:41:01,060
will see all the

681
00:41:04,300 --> 00:41:06,960
it's fine it's quite

682
00:41:09,580 --> 00:41:16,490
so we say that

683
00:41:16,510 --> 00:41:17,490
we say that

684
00:41:18,730 --> 00:41:20,920
XA and XB

685
00:41:20,960 --> 00:41:24,790
they are independent

686
00:41:24,850 --> 00:41:36,860
if this holds

687
00:41:36,870 --> 00:41:39,100
for all x a

688
00:41:55,410 --> 00:41:56,830
we see that

689
00:41:59,270 --> 00:42:01,660
a b

690
00:42:01,710 --> 00:42:04,550
in the thing

691
00:42:04,640 --> 00:42:07,970
he said to run bibles is independent

692
00:42:08,090 --> 00:42:12,270
for every realisation of these two run

693
00:42:12,280 --> 00:42:14,090
these holes

694
00:42:14,970 --> 00:42:18,380
that's the correct way to write what i've written

695
00:42:18,430 --> 00:42:23,340
it's more formal way to write to be the slide

696
00:42:24,480 --> 00:42:25,630
this is

697
00:42:26,330 --> 00:42:32,970
the definition

698
00:42:33,010 --> 00:42:36,550
now the definition of conditional independence i mean this is critical to the critical the

699
00:42:36,860 --> 00:42:39,720
of this causing it to understand

700
00:42:39,770 --> 00:42:42,770
the good thing that is very simple

701
00:42:42,780 --> 00:42:48,730
conditional independence is just independent

702
00:42:48,740 --> 00:42:52,350
however we require that we have

703
00:42:52,370 --> 00:42:57,680
and observe conditions these on some of

704
00:42:57,700 --> 00:43:00,230
if you look at this expression

705
00:43:00,300 --> 00:43:02,250
p of a

706
00:43:02,260 --> 00:43:03,170
and c

707
00:43:04,470 --> 00:43:05,890
is equal to p of

708
00:43:05,900 --> 00:43:06,640
x a

709
00:43:06,650 --> 00:43:07,990
next that

710
00:43:08,040 --> 00:43:10,310
you fix

711
00:43:11,020 --> 00:43:15,330
equation here is exactly the same as these equations just that

712
00:43:15,380 --> 00:43:19,260
we put a bottleneck c

713
00:43:19,340 --> 00:43:21,100
in every time

714
00:43:21,110 --> 00:43:22,360
never effect

715
00:43:22,360 --> 00:43:25,200
these are our sense

716
00:43:27,570 --> 00:43:31,800
census number one of ring theword

717
00:43:31,830 --> 00:43:34,430
they're not very consistently used it's

718
00:43:34,520 --> 00:43:37,990
not a very good system i think around sense

719
00:43:38,080 --> 00:43:39,010
in addition

720
00:43:39,030 --> 00:43:40,820
a short

721
00:43:40,840 --> 00:43:46,010
there you have very little relies on the sense number

722
00:43:46,010 --> 00:43:47,590
if it were up to me and

723
00:43:47,610 --> 00:43:50,270
removes that argument to change

724
00:43:58,560 --> 00:44:03,250
great company which

725
00:44:03,630 --> 00:44:05,410
i mean it

726
00:44:09,240 --> 00:44:11,220
the senses hearing some

727
00:44:15,710 --> 00:44:17,720
then their meaning

728
00:44:17,770 --> 00:44:19,500
so i think

729
00:44:19,770 --> 00:44:20,760
the fact that

730
00:44:20,770 --> 00:44:23,270
one appears in these two positions

731
00:44:23,340 --> 00:44:25,110
symbolizes something

732
00:44:26,270 --> 00:44:27,530
this is the same

733
00:44:27,550 --> 00:44:29,770
a sense of the word

734
00:44:29,770 --> 00:44:33,920
but if they denote not different concepts for exactly what it means to be the

735
00:44:33,930 --> 00:44:37,750
the sense of the word ending but you could also say something like sense number

736
00:44:38,780 --> 00:44:40,360
ring theword

737
00:44:40,410 --> 00:44:43,010
is the preferred sense

738
00:44:45,990 --> 00:44:50,250
i think it may be that was what they had in mind not

739
00:44:50,470 --> 00:44:53,570
no i mean i guess it's good to have a structure that the

740
00:44:53,590 --> 00:44:57,160
the average but not much is made of

741
00:44:57,160 --> 00:44:58,460
returns numbers

742
00:45:00,830 --> 00:45:03,090
so here's an another example

743
00:45:04,700 --> 00:45:05,950
the singular

744
00:45:06,000 --> 00:45:07,650
but the number has

745
00:45:07,740 --> 00:45:11,070
both that can be both the count noun and mass noun

746
00:45:11,130 --> 00:45:12,440
this is a very now

747
00:45:12,590 --> 00:45:18,220
we do have review on count nouns versus mats nouns

748
00:45:22,030 --> 00:45:28,580
but on the really account people are also ring is accounted one ring two rings

749
00:45:28,650 --> 00:45:30,430
sand is a

750
00:45:30,530 --> 00:45:32,540
the last known

751
00:45:33,350 --> 00:45:34,800
two stands

752
00:45:34,840 --> 00:45:39,360
coke can be either and that's

753
00:45:39,420 --> 00:45:41,170
now so

754
00:45:41,180 --> 00:45:44,200
and seventy can have

755
00:45:48,470 --> 00:45:53,400
only mass nouns take determiner somewhere

756
00:45:54,260 --> 00:45:56,750
plural marking on the noun

757
00:45:56,760 --> 00:45:58,930
you can say can i have some ring

758
00:45:58,940 --> 00:46:00,770
unless you're trying to be funny

759
00:46:01,530 --> 00:46:02,510
he is the

760
00:46:02,520 --> 00:46:04,790
so called universal grinder which

761
00:46:05,670 --> 00:46:08,180
of count nouns into mass nouns

762
00:46:09,550 --> 00:46:16,180
red sands mass nouns you can say can i have some sand although i don't

763
00:46:16,180 --> 00:46:19,320
know i would ask sand and

764
00:46:19,380 --> 00:46:21,220
since coke is ambiguous

765
00:46:21,230 --> 00:46:23,240
it allows books

766
00:46:24,270 --> 00:46:25,740
so coke

767
00:46:25,970 --> 00:46:31,790
as as numbers is a station on it

768
00:46:32,080 --> 00:46:37,160
signifying that the mass noun form of coke theword

769
00:46:37,170 --> 00:46:38,720
it's this thing

770
00:46:38,730 --> 00:46:40,290
but to be

771
00:46:40,310 --> 00:46:42,480
the singular form as well

772
00:46:42,480 --> 00:46:43,760
and singular

773
00:46:43,850 --> 00:46:44,840
goes along with

774
00:46:44,970 --> 00:46:47,770
count now

775
00:46:47,770 --> 00:46:52,320
then then called is also proper noun

776
00:46:52,370 --> 00:46:53,420
so it has

777
00:46:53,460 --> 00:46:55,790
proper now massnumber

778
00:46:57,100 --> 00:47:01,070
as well as proper noun singular

779
00:47:01,620 --> 00:47:04,460
in its proper

780
00:47:04,470 --> 00:47:05,910
how announced

781
00:47:06,000 --> 00:47:09,620
it means one serving of so

782
00:47:09,670 --> 00:47:12,300
i bought a coke

783
00:47:12,840 --> 00:47:15,150
is the sentence in which the

784
00:47:15,190 --> 00:47:16,450
sense appears

785
00:47:22,930 --> 00:47:24,650
there are c

786
00:47:30,220 --> 00:47:32,150
this is an example

787
00:47:32,230 --> 00:47:39,100
thought that take singular agreement when you is it is susceptible to every other laboratories

788
00:47:40,230 --> 00:47:41,800
developing a new

789
00:47:46,300 --> 00:47:49,760
the united states

790
00:47:51,470 --> 00:47:53,050
a big

791
00:47:54,670 --> 00:47:58,210
but there are there are proper nouns that take

792
00:47:58,220 --> 00:48:01,850
laurel agreement i can

793
00:48:01,970 --> 00:48:03,730
example the moment

794
00:48:03,820 --> 00:48:06,340
it it's that's

795
00:48:06,380 --> 00:48:07,440
sort of an issue

796
00:48:08,340 --> 00:48:09,940
she when linguistics

797
00:48:16,120 --> 00:48:20,070
dealing with for all proper nouns this

798
00:48:20,090 --> 00:48:21,870
the motivation for

799
00:48:23,090 --> 00:48:26,610
overhauling house the predicament it's done

800
00:48:26,630 --> 00:48:27,980
generation system

801
00:48:28,550 --> 00:48:30,920
so should be able to come up with an example

802
00:48:33,280 --> 00:48:34,540
i'm not able to

803
00:48:34,610 --> 00:48:36,170
and so

804
00:48:40,500 --> 00:48:44,180
the animals are playing

805
00:48:44,270 --> 00:48:45,910
the animals are playing

806
00:48:49,200 --> 00:48:51,580
well that's british

807
00:48:56,310 --> 00:48:57,980
right so in and british

808
00:48:57,990 --> 00:48:58,840
you use

809
00:48:58,860 --> 00:49:00,090
plural for

810
00:49:01,870 --> 00:49:05,040
cope with difficult

811
00:49:05,150 --> 00:49:08,050
in the sea

812
00:49:08,220 --> 00:49:11,130
words that denote groups collective entities

813
00:49:12,710 --> 00:49:15,270
the parliament has decided

814
00:49:15,510 --> 00:49:23,600
it's OK alright let's get back

815
00:49:23,620 --> 00:49:25,450
i have some coke

816
00:49:25,450 --> 00:49:27,630
the capital c

817
00:49:27,770 --> 00:49:30,210
that sentence

818
00:49:35,260 --> 00:49:36,720
right coke

819
00:49:36,720 --> 00:49:40,730
and back in those days i got and i that the people that were doing

820
00:49:40,730 --> 00:49:42,470
that program actually

821
00:49:42,480 --> 00:49:47,280
to some people at stanford i was actually going on at the stanford research institute

822
00:49:47,280 --> 00:49:50,440
down the road from stanford and they needed statistical help they asked me if i

823
00:49:50,440 --> 00:49:53,230
would be a consultant i got involved in

824
00:49:53,230 --> 00:49:56,990
since i was the only statistician working on this i kind of became the statistician

825
00:49:57,040 --> 00:50:00,430
for the various laboratories around the world that we're doing research

826
00:50:00,480 --> 00:50:03,610
in this area so it actually continued to consult in this area

827
00:50:03,620 --> 00:50:06,050
throughout the years

828
00:50:06,100 --> 00:50:12,610
and as i watched the evidence mounts i also noticed something very interesting psychological sociological

829
00:50:12,610 --> 00:50:14,690
phenomenon which is that

830
00:50:14,710 --> 00:50:16,540
most people ignore the data

831
00:50:16,540 --> 00:50:19,280
most people have prior beliefs

832
00:50:19,290 --> 00:50:22,810
if i asked people have shown the data and said

833
00:50:22,860 --> 00:50:24,870
which would be more convincing to you

834
00:50:24,880 --> 00:50:28,030
another set of statistical studies or one

835
00:50:28,040 --> 00:50:32,580
overwhelming personal experience most people said the personal experience

836
00:50:32,600 --> 00:50:35,190
would be more convincing so here's a case where we had

837
00:50:35,230 --> 00:50:40,080
this mounting statistical evidence that was pretty much been ignored

838
00:50:40,120 --> 00:50:44,790
so this makes it a natural topics in bayesian statistics and i think it's like

839
00:50:44,790 --> 00:50:48,170
i said i was an excellent example for hypothesis testing

840
00:50:48,180 --> 00:50:50,160
so why the bayesian

841
00:50:50,220 --> 00:50:53,870
OK first of all i think there a philosophical reasoning and practical reasons

842
00:50:53,960 --> 00:50:56,770
the philosophical reason is that

843
00:50:56,780 --> 00:51:01,920
the bayesian interpretation of probability actually makes more sense in some situations

844
00:51:01,930 --> 00:51:06,050
the relative frequency interpretation doesn't always apply as you know

845
00:51:06,080 --> 00:51:09,480
so this is just a quick example suppose i in the united states it turns

846
00:51:09,480 --> 00:51:13,100
out that about fifty one point two percent birds are boys

847
00:51:13,150 --> 00:51:16,600
so relative frequency interpretation would be that for

848
00:51:16,650 --> 00:51:19,040
conception the probability that

849
00:51:19,240 --> 00:51:22,630
the then birth will be a boy point five one two

850
00:51:22,680 --> 00:51:24,520
but now suppose woman's pregnant

851
00:51:24,540 --> 00:51:27,490
she doesn't know the sex of her baby but she said it has done so

852
00:51:27,490 --> 00:51:30,290
her doctor does not the sex of baby

853
00:51:30,350 --> 00:51:33,440
now what's the probability that's going to be a boy

854
00:51:33,610 --> 00:51:35,230
it's zero one

855
00:51:35,250 --> 00:51:37,790
is it still point five point two

856
00:51:37,810 --> 00:51:41,660
well it to the woman it's probably still points i want to because she doesn't

857
00:51:42,290 --> 00:51:45,180
but the doctor now so it's either zero or one

858
00:51:45,520 --> 00:51:49,680
so there's a case where bayesian interpretation makes perfect sense it's your degree of belief

859
00:51:50,420 --> 00:51:52,120
the outcome

860
00:51:52,130 --> 00:51:56,470
and then what about non repeatable situations the probability of an earthquake in california something

861
00:51:56,470 --> 00:52:00,680
that we pay attention to the probability of an earthquake here is something want to

862
00:52:00,680 --> 00:52:04,860
pay attention to those are personal or subjective at lists

863
00:52:04,880 --> 00:52:10,000
probabilities because it's not here are completely repeatability that we can't face the probability and

864
00:52:10,000 --> 00:52:12,990
relative frequency

865
00:52:13,000 --> 00:52:17,120
so that's part of the philosophical reason continuing

866
00:52:17,130 --> 00:52:21,080
we all know the p values are not we want them to be our students

867
00:52:21,080 --> 00:52:24,310
things are different than what they are in most cases we would all love to

868
00:52:24,310 --> 00:52:28,670
be able to say that what's the probability that the null hypothesis is true

869
00:52:28,670 --> 00:52:30,990
that's not true value as you know

870
00:52:31,840 --> 00:52:35,030
but you can do that sort of thing bayesian methods

871
00:52:35,040 --> 00:52:39,440
and also people who are highly dependent on sample size we also know

872
00:52:39,470 --> 00:52:41,980
and most people don't understand that

873
00:52:42,540 --> 00:52:47,000
bayesian methods make more sense here as well because there you keep adding data and

874
00:52:47,000 --> 00:52:51,130
keep updating your beliefs or your information

875
00:52:51,150 --> 00:52:56,460
so amazing results actually assess things that we would like to know about it

876
00:52:56,730 --> 00:53:00,650
practical reasons

877
00:53:00,660 --> 00:53:05,520
it's actually rare that we don't have any prior information so just two example suppose

878
00:53:05,520 --> 00:53:08,820
we walk we go into new community where public health worker

879
00:53:08,840 --> 00:53:10,280
we want to estimate the

880
00:53:10,350 --> 00:53:14,460
prevalence of HIV infection in that community

881
00:53:14,470 --> 00:53:17,740
well if we're frequentist we start out assuming it could be anything from zero to

882
00:53:17,740 --> 00:53:20,870
one but that's not really true we know that's not true

883
00:53:20,890 --> 00:53:23,650
and know what the community is probably have a pretty good idea

884
00:53:23,680 --> 00:53:26,910
these range that might fall into

885
00:53:26,930 --> 00:53:30,930
whispers right estimate the mean change in blood pressure after programme in meditation

886
00:53:30,940 --> 00:53:34,530
do we really think it could be anything from minus infinity to infinity

887
00:53:34,560 --> 00:53:36,350
probably not

888
00:53:36,390 --> 00:53:43,100
so it makes sense sometimes leads to incorporate prior expert opinion into our analysis

889
00:53:43,140 --> 00:53:48,030
these days most statistical analyses are done as a collaboration between statisticians

890
00:53:48,780 --> 00:53:53,730
knowledge experts so why not take advantage of that expert knowledge and incorporated into the

891
00:53:56,430 --> 00:54:00,410
now course having said all that i mean not bayesian myself

892
00:54:01,820 --> 00:54:03,060
i do that with one

893
00:54:03,110 --> 00:54:10,030
OK so on this topic i do think because bayesian methods are being used more

894
00:54:10,030 --> 00:54:13,390
and more these days in other fields i do think it's time that we start

895
00:54:13,400 --> 00:54:17,570
talking about them in our courses i'm not sure about the really basic level introductory

896
00:54:17,570 --> 00:54:21,910
course i think is already too much they're getting another talking eleven o'clock session where

897
00:54:21,910 --> 00:54:25,590
i'm going to talk about things i think we should include their we don't but

898
00:54:25,590 --> 00:54:26,550
about one or two

899
00:54:26,650 --> 00:54:28,120
percentage points

900
00:54:28,230 --> 00:54:35,750
as a soft margin increases then my performance degrades are typically go from minimum OK

901
00:54:35,760 --> 00:54:37,520
that was cl one

902
00:54:38,320 --> 00:54:42,780
error norm and this is the other one the web and on a post small

903
00:54:42,780 --> 00:54:44,170
positive quantity

904
00:54:44,180 --> 00:54:44,890
to the

905
00:54:46,690 --> 00:54:47,820
contract law

906
00:54:47,830 --> 00:54:51,010
and course when zero that lambda equal zero

907
00:54:51,020 --> 00:54:54,030
i have my normal what i call hard margin

908
00:54:54,050 --> 00:54:58,000
svm the soft bond became a course that to have seven

909
00:54:58,010 --> 00:55:00,160
percent error on test data

910
00:55:00,190 --> 00:55:02,900
i introduce the alpha OK

911
00:55:02,910 --> 00:55:04,180
goes to a minimum

912
00:55:04,200 --> 00:55:06,190
around about five point three

913
00:55:06,230 --> 00:55:11,130
and then starts climbing because of obvious if used to holiday

914
00:55:11,140 --> 00:55:18,910
soft margin then the i degrade performance OK so we do see the introduce yourself

915
00:55:18,920 --> 00:55:25,780
margin lessens the effect of noise and actually improves performance test error is reduced typically

916
00:55:25,780 --> 00:55:27,550
with noisy data

917
00:55:27,590 --> 00:55:30,400
now there's justification for those two

918
00:55:30,420 --> 00:55:33,980
largely comes from learning theory says well very

919
00:55:34,000 --> 00:55:38,650
to date the picture i can give you is of a sort of relaxation of

920
00:55:38,650 --> 00:55:42,660
the hard margin constraint they introduced earlier on

921
00:55:42,700 --> 00:55:46,670
indeed what we're gonna do just have a look at the l one error norm

922
00:55:46,670 --> 00:55:49,430
and you may remember

923
00:55:49,430 --> 00:55:51,070
from the first lecture

924
00:55:51,080 --> 00:55:52,710
but i had the following conditions

925
00:55:52,770 --> 00:55:57,470
here this thing here constraint came from my canonical hyperplane idea

926
00:55:57,530 --> 00:56:00,000
so that was missing OK

927
00:56:00,040 --> 00:56:02,250
so in my picture

928
00:56:02,350 --> 00:56:03,590
which i have

929
00:56:03,650 --> 00:56:06,690
for the sphere

930
00:56:06,710 --> 00:56:09,180
i had the following

931
00:56:09,230 --> 00:56:10,940
my two types of data

932
00:56:11,130 --> 00:56:13,390
plus plus plus plus

933
00:56:13,410 --> 00:56:16,180
minus minus minus minus minus etcetera

934
00:56:16,200 --> 00:56:19,460
and i had to a separating hyperplane

935
00:56:19,470 --> 00:56:22,640
and indeed this sort of zone here

936
00:56:22,650 --> 00:56:26,020
i call the marching band so this was the margin

937
00:56:27,040 --> 00:56:31,040
and so the zone here was it would be called the marching band

938
00:56:31,050 --> 00:56:34,370
one do now is actually allow

939
00:56:34,450 --> 00:56:38,650
for some data points to actually be within the margin band on the k

940
00:56:39,420 --> 00:56:43,630
on the far side had this anomalous data point given in blue

941
00:56:43,700 --> 00:56:48,250
he could be appointed which can be within the margin band sound like a band

942
00:56:48,250 --> 00:56:53,060
like this OK so here he is going to allow him i can indeed actually

943
00:56:53,060 --> 00:56:57,670
allow him to be on the wrong side altogether OK just a few data points

944
00:56:57,720 --> 00:56:59,890
which could be these outliers

945
00:56:59,910 --> 00:57:02,700
can be in the marching band or even can be

946
00:57:02,710 --> 00:57:04,580
wrongly classified OK

947
00:57:04,600 --> 00:57:06,440
now i choose an idea

948
00:57:06,450 --> 00:57:09,950
by introducing this site here

949
00:57:10,650 --> 00:57:12,980
so here was called slack variable

950
00:57:13,000 --> 00:57:15,920
and if that is about point five

951
00:57:15,930 --> 00:57:17,870
then rather like here

952
00:57:17,920 --> 00:57:20,430
is within the margin band

953
00:57:20,500 --> 00:57:22,680
and if that was greater than one

954
00:57:22,690 --> 00:57:26,550
he would actually be on the wrong side altogether so it actually get married during

955
00:57:26,550 --> 00:57:31,430
my training process but it could be a good idea because it like kinases sample

956
00:57:31,500 --> 00:57:34,680
could be just totally wrongly labelled data point OK

957
00:57:34,730 --> 00:57:40,530
so that's how i relax the hard margin condition when choosing the slack variables

958
00:57:40,620 --> 00:57:43,800
in other words my optimisation task

959
00:57:43,800 --> 00:57:49,590
it will be the following previously without that slack variable i didn't have this term

960
00:57:49,600 --> 00:57:51,400
i wanted to minimize

961
00:57:51,410 --> 00:57:55,850
W's www that was my condition to maximize the margin

962
00:57:55,870 --> 00:57:58,850
but now i've got this extra term OK

963
00:57:58,910 --> 00:58:00,730
because what we really want to do

964
00:58:00,760 --> 00:58:03,810
it is i want to maximize the margin

965
00:58:03,830 --> 00:58:04,810
like so

966
00:58:04,820 --> 00:58:09,070
one of the same time i don't be over-generous with the number of points which

967
00:58:09,800 --> 00:58:12,480
within the margin band or to erroneous

968
00:58:12,560 --> 00:58:17,020
so try penalize that's all trying to minimize the sum of such errors

969
00:58:17,030 --> 00:58:18,910
i call these errors OK

970
00:58:18,920 --> 00:58:21,300
minimizes to

971
00:58:21,400 --> 00:58:26,210
while also doing my expected maximizing the margin my c

972
00:58:26,230 --> 00:58:27,500
with the trade-off

973
00:58:27,510 --> 00:58:28,520
between these two

974
00:58:29,420 --> 00:58:30,790
i could really

975
00:58:30,800 --> 00:58:33,680
they take c infinity

976
00:58:33,690 --> 00:58:35,530
i get rid of these all together

977
00:58:35,530 --> 00:58:36,560
as being

978
00:58:36,580 --> 00:58:38,130
spinning out five

979
00:58:38,140 --> 00:58:40,160
into some of its consequences

980
00:58:40,310 --> 00:58:48,730
however these are all logical consequences

981
00:58:54,540 --> 00:58:56,870
if we can always decide

982
00:59:00,010 --> 00:59:02,840
formulas like this valid

983
00:59:02,860 --> 00:59:06,050
one of the things we have to do is turn their back on this tour

984
00:59:06,120 --> 00:59:07,670
machine gun

985
00:59:07,710 --> 00:59:09,250
with the other side

986
00:59:09,270 --> 00:59:13,800
by effectively checking in nine number steps

987
00:59:13,860 --> 00:59:15,070
whether or not

988
00:59:15,080 --> 00:59:20,970
particular consequences like such and such a machine halts

989
00:59:20,990 --> 00:59:22,970
is in this theory

990
00:59:22,980 --> 00:59:25,030
but then putting it back here

991
00:59:25,090 --> 00:59:28,850
we would have managed once again to find a way of determining the nature an

992
00:59:28,850 --> 00:59:32,460
arbitrary turing machine hold

993
00:59:32,550 --> 00:59:34,850
just so it's not possible

994
00:59:34,910 --> 00:59:37,510
so just the same way

995
00:59:37,520 --> 00:59:38,700
it's not possible

996
00:59:38,700 --> 00:59:41,950
they have decidability in first order logic

997
00:59:41,960 --> 00:59:46,050
that's the general intention of the argumentation

998
00:59:46,110 --> 00:59:48,790
created gigantic mapping

999
00:59:51,060 --> 00:59:55,950
between turing machines and formulas of first-order logic

1000
00:59:55,980 --> 00:59:58,220
which allows us to

1001
00:59:59,520 --> 01:00:02,920
this is something like a universal machine

1002
01:00:02,970 --> 01:00:05,620
and something like moves in the game

1003
01:00:05,680 --> 01:00:09,290
as in the game of carrying out state industry machine

1004
01:00:10,280 --> 01:00:15,270
logical consequence here in first order logic

1005
01:00:15,280 --> 01:00:16,910
we're going to carry out

1006
01:00:16,960 --> 01:00:18,080
in particular

1007
01:00:19,570 --> 01:00:23,490
inference particular derivations to prove this works

1008
01:00:23,490 --> 01:00:26,330
there is such a mapping when OK otherwise two

1009
01:00:26,350 --> 01:00:27,920
because we know it's pointless

1010
01:00:27,960 --> 01:00:29,670
because if we were to do it by

1011
01:00:29,690 --> 01:00:31,460
when using this thing here

1012
01:00:31,470 --> 01:00:35,520
the figure out whether germany she health that we would have once again get out

1013
01:00:35,600 --> 01:00:40,690
of holding machine holding problems so it's not

1014
01:00:42,340 --> 01:00:44,040
the theorem would be

1015
01:00:45,310 --> 01:00:50,820
the decision problem for property there is solvable with his mechanical test

1016
01:00:50,920 --> 01:00:57,010
the and eventually classifies every object correctly one way or another

1017
01:00:57,010 --> 01:00:59,470
OK classifiers correctly

1018
01:00:59,930 --> 01:01:02,440
as having the property or not

1019
01:01:02,440 --> 01:01:06,890
the theorem is that that's not so that there is not a solution to that

1020
01:01:06,950 --> 01:01:09,130
and the proof idea

1021
01:01:10,190 --> 01:01:11,520
is that

1022
01:01:11,570 --> 01:01:14,280
is a very grand reductio

1023
01:01:14,520 --> 01:01:16,570
assuming that there is such a test

1024
01:01:16,620 --> 01:01:20,260
then we would be able to find out that an arbitrary turing machine

1025
01:01:20,260 --> 01:01:22,210
could have halted we would be out

1026
01:01:22,260 --> 01:01:26,730
we would have a solution in general to the halting problem for our future machine

1027
01:01:26,750 --> 01:01:30,570
which can be done

1028
01:01:32,010 --> 01:01:34,290
i'm just going to go through now

1029
01:01:34,420 --> 01:01:39,520
take this sort of general idea of this mapping

1030
01:01:39,530 --> 01:01:41,440
could just gets

1031
01:01:41,480 --> 01:01:44,780
messy enough that you know that you would only want to follow it to prove

1032
01:01:46,000 --> 01:01:47,330
if you had

1033
01:01:47,350 --> 01:01:51,870
book in front of you and your training step twenty one so finally

1034
01:01:51,920 --> 01:01:53,830
and not one but you certainly

1035
01:01:53,850 --> 01:01:57,370
like a big formulas and what sort of stuff it's not the

1036
01:01:57,420 --> 01:01:58,580
the actual

1037
01:01:59,350 --> 01:02:03,970
steps and that had been given in the text we're talking about

1038
01:02:05,880 --> 01:02:08,300
well sorry some course

1039
01:02:08,310 --> 01:02:11,030
so let's say i'm just reading out seeing if i can

1040
01:02:11,080 --> 01:02:15,640
go with the flow here keep your eyes on the head or whatever understand

1041
01:02:15,810 --> 01:02:19,200
so i not a few places i

1042
01:02:19,370 --> 01:02:21,990
this is not in new know and not

1043
01:02:22,660 --> 01:02:24,540
you know

1044
01:02:24,560 --> 01:02:25,790
these pages

1045
01:02:25,810 --> 01:02:30,740
i particularly didn't number so that everything else is still numbering ca

1046
01:02:31,720 --> 01:02:36,060
this is one of the things that i added later tried to

1047
01:02:36,100 --> 01:02:37,870
today still here

1048
01:02:37,930 --> 01:02:41,890
what's the story

1049
01:02:41,910 --> 01:02:44,490
OK let's not start out with such a grand

1050
01:02:44,510 --> 01:02:48,760
the notion as a universal turing machine or paying particular mind about whether to universal

1051
01:02:48,760 --> 01:02:52,490
now let's just imagine we got description of turing machine

1052
01:02:52,540 --> 01:02:54,950
when the flag graph

1053
01:02:54,970 --> 01:02:57,410
that's a little bottle things

1054
01:02:57,410 --> 01:03:02,310
k flowgraph machine type whatever it's got states it's got symbols

1055
01:03:02,350 --> 01:03:03,870
and it's got an important

1056
01:03:03,970 --> 01:03:07,970
it's a it's trying to compute function

1057
01:03:08,010 --> 01:03:10,140
and it's the machine and

1058
01:03:10,160 --> 01:03:11,080
it's got

1059
01:03:11,080 --> 01:03:12,910
input end

1060
01:03:12,910 --> 01:03:15,640
and we want to turn that into first order logic

1061
01:03:15,680 --> 01:03:19,490
what we do is we construct a whole bunch of sentences

1062
01:03:19,490 --> 01:03:23,040
which takes textbook away following called delta

1063
01:03:23,120 --> 01:03:24,640
so we construct

1064
01:03:28,410 --> 01:03:30,600
to describe

1065
01:03:30,930 --> 01:03:32,390
the operation of

1066
01:03:41,010 --> 01:03:44,240
we construct

1067
01:03:44,290 --> 01:03:46,600
a second school here i

1068
01:03:46,600 --> 01:03:51,780
which describes the lies in which is machine could stop

1069
01:03:52,160 --> 01:03:54,790
particular dissolve and sensors

1070
01:03:54,830 --> 01:03:57,790
these are just sentences in first-order logic with

1071
01:03:57,810 --> 01:04:02,490
particular vocabulary corresponding to the traditional

1072
01:04:02,490 --> 01:04:08,410
construct a ge informally corresponding to

1073
01:04:11,330 --> 01:04:14,020
in which

1074
01:04:14,080 --> 01:04:16,930
actually always

1075
01:04:16,950 --> 01:04:21,010
which includes stop

1076
01:04:21,350 --> 01:04:27,120
that's not so hard to do i just continue to say that

1077
01:04:27,140 --> 01:04:30,510
you know let's do that

1078
01:04:31,390 --> 01:04:33,950
it's the second one is and so had to

1079
01:04:33,970 --> 01:04:37,350
remember i said in the discussion of turing machine

1080
01:04:37,410 --> 01:04:40,310
if you think it is a big matrix

1081
01:04:40,350 --> 01:04:43,260
with all the states here

1082
01:04:43,290 --> 01:04:45,930
and the symbols it's reading here

1083
01:04:47,640 --> 01:04:50,540
the ways in which the machine could stop would just be

1084
01:04:50,560 --> 01:04:51,970
places where

1085
01:04:52,020 --> 01:04:54,080
there was a whole

1086
01:04:54,120 --> 01:04:55,620
nothing to do next

1087
01:04:55,620 --> 01:04:57,240
no work to be done

1088
01:04:57,260 --> 01:05:00,680
if it ever gets to state q five

1089
01:05:00,680 --> 01:05:04,470
and it happens to be reading s eleven

1090
01:05:05,890 --> 01:05:07,990
sorry this is solution

1091
01:05:08,040 --> 01:05:11,100
and that's blank the description machine title

1092
01:05:11,120 --> 01:05:13,810
well in the machine stops

1093
01:05:13,870 --> 01:05:14,950
so i je

1094
01:05:14,950 --> 01:05:19,260
would just be the disjunction of the whole of the blanks in this type

1095
01:05:19,310 --> 01:05:21,120
this title is found

1096
01:05:21,180 --> 01:05:25,740
OK maybe all states and all symbols but it's quite it's fun title there's an

1097
01:05:25,740 --> 01:05:28,910
infinite number of ways in which machine

1098
01:05:28,950 --> 01:05:32,660
don't get this confused with fear and with the machine actually reaches one of those

1099
01:05:34,660 --> 01:05:37,470
we can sign advance what is like to hold

1100
01:05:37,720 --> 01:05:43,680
well we don't know is if a particular machine in operation whatever get the

1101
01:05:43,720 --> 01:05:47,740
you might never find itself in q five might be part of the description

1102
01:05:47,790 --> 01:05:50,020
that doesn't mean have achieved that

1103
01:05:50,060 --> 01:05:53,040
particular combination of circumstances

1104
01:05:53,040 --> 01:05:56,890
i je there is a big disjunction essentially all cells

1105
01:05:56,970 --> 01:06:01,200
so it's a description of

1106
01:06:01,260 --> 01:06:02,580
description cell

1107
01:06:02,600 --> 01:06:04,280
you know a blank cell one

1108
01:06:08,310 --> 01:06:09,200
i mean

1109
01:06:09,220 --> 01:06:11,240
description of blank cell two

1110
01:06:12,160 --> 01:06:13,780
how many thanks cells are

1111
01:06:13,790 --> 01:06:15,330
we take a machine

1112
01:06:15,370 --> 01:06:18,970
we can construct to the circumstance in which i je

1113
01:06:18,990 --> 01:06:22,760
since h circumstance in which a sentence

1114
01:06:24,220 --> 01:06:27,780
i'm told you that delta going do that in a second

1115
01:06:27,850 --> 01:06:30,700
so just go on

1116
01:06:30,700 --> 01:06:34,140
and if someone is only interested in the classification at level six let's say whether

1117
01:06:34,140 --> 01:06:37,160
or not i misclassified something that belongs in one

1118
01:06:37,160 --> 01:06:40,240
in class two then it doesn't matter because they don't

1119
01:06:40,260 --> 01:06:45,680
number six wakes for that person that mistake doesn't even make a difference right so

1120
01:06:45,680 --> 01:06:48,930
that's kind of the idea behind it and then the way we model this as

1121
01:06:48,930 --> 01:06:54,830
a structured output in kind of this SVM struct problems basically that we took

1122
01:06:54,870 --> 01:06:58,990
feature vector for so this is the tensor product idea we take a feature vector

1123
01:06:58,990 --> 01:07:03,260
for the class is basically the way we describe the classes is by

1124
01:07:03,300 --> 01:07:09,820
indicator functions of the successor nodes so and then also the themselves are also so

1125
01:07:09,820 --> 01:07:15,780
it's kind of service reflexive relations offences there's no number two it's it's kinda related

1126
01:07:15,780 --> 01:07:20,910
to itself to number six and number nine so you get entries there at position

1127
01:07:20,910 --> 01:07:22,330
two six and nine

1128
01:07:22,330 --> 01:07:26,430
in this spectral representation k and

1129
01:07:26,450 --> 01:07:31,620
right so you have two six nine right and this number three

1130
01:07:31,640 --> 01:07:36,200
so you have an entry at three seven nine and ten so so in some

1131
01:07:36,200 --> 01:07:38,830
sense what these things now do is actually they

1132
01:07:38,870 --> 01:07:42,740
described to think of it as attribute of the classes right the fact that the

1133
01:07:42,740 --> 01:07:48,120
particular class it's below node number ten is a particular property of that class can

1134
01:07:48,120 --> 01:07:53,370
you make it explicit in this in this vector here and then you have whatever

1135
01:07:53,370 --> 01:07:57,930
you have for your x representation of documents right you just stuck it in there

1136
01:07:57,930 --> 01:08:00,050
so basically

1137
01:08:00,060 --> 01:08:04,740
if you do the tensor product but if you do the tensor product sort of

1138
01:08:04,740 --> 01:08:09,850
what happens is that it could just transported transport you copy over that vector into

1139
01:08:09,850 --> 01:08:13,100
the positions that are to have a one here

1140
01:08:13,120 --> 01:08:16,330
and you can see this here and then you know in order to evaluate this

1141
01:08:16,330 --> 01:08:18,680
in approach we talked about this before

1142
01:08:18,680 --> 01:08:22,680
you know get the inner product between the documents but also between the classes

1143
01:08:22,680 --> 01:08:27,120
and what you basically what you basically do is now you can learn across classes

1144
01:08:27,140 --> 01:08:30,990
so you could even have classes that have no training example right just so that

1145
01:08:30,990 --> 01:08:35,530
you conception understand what's going on but you know you have properties of these classes

1146
01:08:35,530 --> 01:08:39,550
and you can try to generalize to new classes that have similar properties in classes

1147
01:08:39,550 --> 01:08:43,470
that you know and you could still try to come up with a meaningful score

1148
01:08:43,470 --> 01:08:44,760
for the new class

1149
01:08:45,390 --> 01:08:50,180
so so anyway and then you can also combine it with with the tree loss

1150
01:08:50,280 --> 01:08:53,870
as i said before you loss function is actually

1151
01:08:53,890 --> 01:08:56,470
taking the tree into account

1152
01:08:56,490 --> 01:08:58,760
and then we did some analysis on

1153
01:08:58,890 --> 01:09:06,430
actually this might qualify dataset pattern databases publicly available and then we compare the two

1154
01:09:06,430 --> 01:09:12,600
and we found like some improvement using the structured approach was on the order of

1155
01:09:12,600 --> 01:09:17,620
what we get the are five to ten percent so it was not a huge

1156
01:09:17,620 --> 01:09:22,330
but it was particularly significant if if you if you're in a sparse data setting

1157
01:09:22,330 --> 01:09:28,300
where the number of training examples per leaf node is very small OK and that

1158
01:09:30,510 --> 01:09:33,700
right so this is this is also an example of

1159
01:09:33,760 --> 01:09:38,300
where you can use the SVM struct you can basically just plug in directly

1160
01:09:38,300 --> 01:09:42,140
now this the next problem that i want to show here

1161
01:09:42,970 --> 01:09:46,800
binary classification with unbalanced classes OK so

1162
01:09:46,820 --> 01:09:48,550
it's it's a problem

1163
01:09:48,580 --> 01:09:51,660
in many applications but definitely in

1164
01:09:51,660 --> 01:09:57,530
in text categorisation information retrieval that often you have classes but there's only a relatively

1165
01:09:57,530 --> 01:10:03,200
small number of positive examples or the fraction of like your prior probability of getting

1166
01:10:03,200 --> 01:10:05,180
a positive example is small

1167
01:10:05,220 --> 01:10:08,120
compared to getting a negative one right so

1168
01:10:08,120 --> 01:10:13,220
there is also true for something like you know the IPC with a large category

1169
01:10:13,220 --> 01:10:14,970
taxonomy right just

1170
01:10:15,030 --> 01:10:19,970
if you have sixty thousand classes and achieved if each document gets assigned to one

1171
01:10:19,970 --> 01:10:21,680
or maybe two classes

1172
01:10:21,720 --> 01:10:26,120
primary and secondary that it just means that

1173
01:10:26,140 --> 01:10:27,700
you know for each

1174
01:10:28,050 --> 01:10:32,510
you only have a small number of positive examples and lots of negative right so

1175
01:10:32,510 --> 01:10:37,280
it's not uncommon that the positive examples have a probability that's less than one percent

1176
01:10:37,320 --> 01:10:42,050
and so the problem now is that if you use the classification error to train

1177
01:10:42,050 --> 01:10:45,140
these things right and you get the degenerate

1178
01:10:45,140 --> 01:10:50,260
the classification function that classifies everything is negative then already you get ninety nine plus

1179
01:10:50,260 --> 01:10:51,830
percent accuracy

1180
01:10:51,850 --> 01:10:55,140
right and on less than one percent error unfortunately

1181
01:10:55,180 --> 01:10:59,870
this is not very helpful because you know your classifier does is basically saying no

1182
01:10:59,870 --> 01:11:04,620
to everything so you never get enough so it looks good according to classification error

1183
01:11:04,620 --> 01:11:09,200
but it's really useless classifier so what people have done that of course is they

1184
01:11:09,200 --> 01:11:13,520
look at other scores right so you can use the one score for instance when

1185
01:11:13,520 --> 01:11:19,780
you look at precision and recall OK and basically the you know two times precision

1186
01:11:19,780 --> 01:11:24,050
recall divided by the sum so its harmonic mean and you can try to optimize

1187
01:11:24,050 --> 01:11:28,490
that and other things like precision recall breakeven point so

1188
01:11:29,470 --> 01:11:33,640
that has always been well known right but what people have often done is

1189
01:11:33,660 --> 01:11:38,800
basically they just trained their classifier using classification loss and then in the end they

1190
01:11:38,800 --> 01:11:40,370
just reported

1191
01:11:40,390 --> 01:11:44,470
you know the good news of the classifier on these other metrics right but if

1192
01:11:44,470 --> 01:11:46,260
you care about these these

1193
01:11:46,280 --> 01:11:49,300
you know the f one score the question is why don't you optimize only the

1194
01:11:49,300 --> 01:11:52,320
f one score right away

1195
01:11:52,330 --> 01:11:55,960
well the problem is that if you look at the f one score right it

1196
01:11:55,960 --> 01:11:57,370
depends on

1197
01:11:57,390 --> 01:12:01,830
basically you know you will write something like the ordering of pluses and minuses is

1198
01:12:01,830 --> 01:12:08,990
one one neuron and the index j indicates another neuron so here and representing only

1199
01:12:08,990 --> 01:12:12,350
one neuron right so so i don't have indexes for

1200
01:12:12,420 --> 01:12:13,680
four white

1201
01:12:13,710 --> 01:12:15,010
for simplicity

1202
01:12:16,280 --> 01:12:19,890
here's the update rule of having just says that you know if you have correlation

1203
01:12:21,210 --> 01:12:25,610
activities then increase the weight

1204
01:12:25,630 --> 01:12:31,410
so the reason i'm presenting this very simple rule that is not used

1205
01:12:32,020 --> 01:12:33,870
so much in practice since

1206
01:12:33,870 --> 01:12:39,610
except that you need for present twelve united by approach is that you can very

1207
01:12:39,610 --> 01:12:42,830
easily understand with this simple rule

1208
01:12:42,920 --> 01:12:45,520
how the kernel trick works

1209
01:12:45,570 --> 01:12:53,590
the country is the basis for a lot of algorithms of chrome method algorithms

1210
01:12:53,760 --> 01:13:02,480
and it shows that there is a correspondence between the perceptual representation and the kernel

1211
01:13:02,480 --> 01:13:06,960
based representation so the two you know

1212
01:13:06,970 --> 01:13:09,870
learning machines have been presenting you the percent

1213
01:13:10,110 --> 01:13:13,960
and the kernel methods they can actually

1214
01:13:13,980 --> 01:13:20,110
be representing the same decision function how is this going happening

1215
01:13:20,110 --> 01:13:25,240
well considering happens rule that i was just mentioning to you but applied to the

1216
01:13:25,240 --> 01:13:35,360
perception so the weight vector is going to be a weighted sum

1217
01:13:35,430 --> 01:13:38,830
of the y i phi of x i so how is this if you go

1218
01:13:38,830 --> 01:13:43,730
back one slide you see that if you use this update rule if you reinforce

1219
01:13:43,730 --> 01:13:50,970
always the weight according to the product of y and you know whatever it's plugging

1220
01:13:50,970 --> 01:13:53,090
into x i j

1221
01:13:53,110 --> 01:13:58,000
then if you will the training examples

1222
01:13:58,020 --> 01:14:03,500
in the end you will be having something which is proportional to the of the

1223
01:14:03,500 --> 01:14:07,180
y and the y i x i j

1224
01:14:07,190 --> 01:14:11,010
i'm running the sum running over all the i

1225
01:14:11,050 --> 01:14:15,250
so for all the training some by every time you show the new training example

1226
01:14:15,290 --> 01:14:22,210
you're going to reinforce all the connections according to products of the outcome

1227
01:14:22,230 --> 01:14:23,810
and the inputs

1228
01:14:23,880 --> 01:14:28,740
and in the end you just something a weighted sum so for the case of

1229
01:14:29,130 --> 01:14:34,570
the simple had one is just a few short link every example only once you're

1230
01:14:34,570 --> 01:14:38,680
going to be just having the sum of all the examples of the y i

1231
01:14:38,680 --> 01:14:43,670
phi of x i so replacing the inputs by the phi functions

1232
01:14:43,830 --> 01:14:49,830
and so  this special way of calculating the weight vector

1233
01:14:49,870 --> 01:14:52,920
makes things very simple

1234
01:14:52,950 --> 01:14:54,870
because now

1235
01:14:54,890 --> 01:15:01,150
since f of x is you know the dot product between w and phy then

1236
01:15:01,150 --> 01:15:02,500
you can plug in

1237
01:15:02,520 --> 01:15:07,560
this form of w into here and expand and you obtain

1238
01:15:07,600 --> 01:15:10,600
this new

1239
01:15:10,620 --> 01:15:16,350
form of the decision function in very interestingly you can exhibit the fact that now

1240
01:15:16,350 --> 01:15:21,310
your weighted sum is running over dot products of your phi i

1241
01:15:21,310 --> 01:15:22,060
which are the phi

1242
01:15:22,540 --> 01:15:26,690
computed for all the training samples and your phi of x

1243
01:15:27,970 --> 01:15:30,170
and so if you define now in new

1244
01:15:31,040 --> 01:15:37,600
OK of x i and x as you know this dot product then you can

1245
01:15:37,600 --> 01:15:42,890
transform your phi f of x into the sum now not over the phi functions

1246
01:15:42,890 --> 01:15:44,810
but over the k functions

1247
01:15:44,810 --> 01:15:46,210
right so

1248
01:15:46,350 --> 01:15:52,750
you have shown here that by the mere fact that using this perhaps will yield

1249
01:15:52,790 --> 01:15:54,190
a weight vector

1250
01:15:54,210 --> 01:15:56,630
which is the weighted sum

1251
01:15:56,640 --> 01:15:58,120
of the original

1252
01:15:58,140 --> 01:16:04,460
templates original patterns so each example it's training sample enters with the positive weight if

1253
01:16:04,460 --> 01:16:09,980
you why i are you know plus and minus one right if your outputs are

1254
01:16:10,020 --> 01:16:15,440
are binary then this simply means that all the examples of the positive class are

1255
01:16:15,440 --> 01:16:21,040
added all the examples of the negative class are subtracted and this is the way

1256
01:16:21,040 --> 01:16:27,020
you compute your w vectors in awaits very twenty right if you want to compute

1257
01:16:27,020 --> 01:16:28,480
for each feature

1258
01:16:28,490 --> 01:16:32,070
the weight with which are going to be

1259
01:16:32,080 --> 01:16:37,100
calculating your decision function

1260
01:16:37,600 --> 01:16:41,790
what you don't like you know is that you say well the weight is going

1261
01:16:41,790 --> 01:16:42,790
to be

1262
01:16:42,790 --> 01:16:50,090
the difference between the frequency of of how often i see my feature positive and

1263
01:16:50,090 --> 01:16:55,390
how often i see my feature negative right so all the features positive added together

1264
01:16:55,390 --> 01:16:59,290
and all the negative ones are added and subtracted and this is how you calculate

1265
01:16:59,290 --> 01:17:01,290
the weight

1266
01:17:01,710 --> 01:17:06,210
of course there are you know much fancier ways of calculating the weight this absolutely

1267
01:17:06,210 --> 01:17:11,560
does not take into account possible redundancies right if you have many examples that are

1268
01:17:11,980 --> 01:17:19,270
similar you don't necessarily want to all add them up you might want you to

1269
01:17:19,310 --> 01:17:27,120
consider that once you've seen one type of example you don't want to again add

1270
01:17:27,120 --> 01:17:29,570
so information Dimitria we're not

1271
01:17:29,590 --> 01:17:40,050
explained you because you mean you presentation to of information geometry Jews grabs stunning squeezes

1272
01:17:40,050 --> 01:17:41,950
the walk of the high that

1273
01:17:42,000 --> 01:17:49,930
the bride and therefore receives an average height next euro on walks along last addition

1274
01:17:49,930 --> 01:17:56,630
to read only a school that certain factored in Baghdad and that could be good

1275
01:17:56,670 --> 01:18:03,280
news could be defined by factional output slows it could be that different users to

1276
01:18:03,280 --> 01:18:05,020
of these exploration

1277
01:18:05,050 --> 01:18:12,410
on you could also will introduce the classical music in Baghdad announced by the a

1278
01:18:12,410 --> 01:18:15,790
fact that you convert community approach

1279
01:18:16,060 --> 01:18:23,430
on using distilling from not to change the of sophisticated Baghdad

1280
01:18:23,520 --> 01:18:32,690
which is bereft important that this issue to which makes it inquisitive tension don't also

1281
01:18:32,690 --> 01:18:38,910
mission that Iraq where you could see that it will have suggested the characteristic functional

1282
01:18:38,910 --> 01:18:44,860
not generating function effectively could've said that it could buy Getty the demise of the

1283
01:18:44,860 --> 01:18:51,360
generating function on India's uniting function could be be obtained by this operation just 2

1284
01:18:51,360 --> 01:18:58,160
of the situation where are you out of the to seasonings with theory in the

1285
01:18:58,160 --> 01:19:04,550
Commission humidity will you you you use a suspension below also mission

1286
01:19:04,790 --> 01:19:16,260
when we try to apply that's good information geometry uh which is even basic places

1287
01:19:16,260 --> 01:19:23,040
would use of differential which we even by the future all official may take all

1288
01:19:23,040 --> 01:19:30,570
it discussed it to provide basic become almost supervisors metric system Italy's as what which

1289
01:19:30,570 --> 01:19:38,480
is in the office of the metrics on when you apply these definitions from action

1290
01:19:38,980 --> 01:19:46,010
did we should know because it will mean you have these equations has the task

1291
01:19:46,010 --> 01:19:54,430
of a test of air at the area where are you could upset of using

1292
01:19:54,430 --> 01:19:58,540
the island by injections so the distance between that this is not a word to

1293
01:19:58,540 --> 01:20:03,700
the distance to the end of the match on you as extension falls amid United

1294
01:20:03,740 --> 01:20:11,960
which procedures this expression and you couldn't sales that 1 is a classic demanded the

1295
01:20:12,000 --> 01:20:18,450
distance because when you consider and you will have to press to men and the

1296
01:20:20,040 --> 01:20:25,370
so if you want to integrate the midfield in fact we have basic questions said

1297
01:20:25,450 --> 01:20:32,740
starts we Salo also square also look at it was extended begin value between the

1298
01:20:32,740 --> 01:20:40,930
2 life is also a value of these countries could use in that state commutes

1299
01:20:40,940 --> 01:20:50,760
between the maturation of investors might wanna come by that the we're seeing my once

1300
01:20:50,760 --> 01:20:55,810
he went to met to what virtuosic worked well on all the systems is equipped

1301
01:20:59,260 --> 01:21:04,660
people so it is also rates he too was dry left in geometry which has

1302
01:21:04,660 --> 01:21:09,980
been presented by Mr. unmarried so you in case of

1303
01:21:10,280 --> 01:21:19,760
like pollution godlike oddity stand on potential functions identifying to cut its stake so you

1304
01:21:19,760 --> 01:21:25,070
can coordinate you can give to a different system for the meeting I did push

1305
01:21:25,070 --> 01:21:28,040
you extract the subset f one with the

1306
01:21:28,050 --> 01:21:30,890
we these smaller than n

1307
01:21:30,900 --> 01:21:32,690
so that

1308
01:21:32,710 --> 01:21:37,350
all functions are similar to atleast one of these functions

1309
01:21:37,360 --> 01:21:38,500
right so it

1310
01:21:38,590 --> 01:21:40,210
picture is simple

1311
01:21:40,220 --> 01:21:46,100
you have again you can consider

1312
01:21:46,120 --> 01:21:50,310
a set of function some object or some subset of a metric space

1313
01:21:51,820 --> 01:21:53,270
among these functions so

1314
01:21:53,290 --> 01:21:56,300
actually here we're considering finally many but

1315
01:21:56,350 --> 01:22:00,840
it doesn't matter so among all these finite many functions

1316
01:22:00,980 --> 01:22:03,450
you isolate

1317
01:22:03,490 --> 01:22:04,680
a subset of them

1318
01:22:04,700 --> 01:22:07,130
the idea of them such that

1319
01:22:07,280 --> 01:22:09,640
every other function is at least

1320
01:22:09,690 --> 01:22:12,180
sorry at most distance

1321
01:22:12,200 --> 01:22:14,260
here i denoted alpha

1322
01:22:14,290 --> 01:22:17,310
of one of these functions

1323
01:22:17,350 --> 01:22:20,520
which means that if you were to draw circles

1324
01:22:20,560 --> 01:22:22,550
the geometry may may not be

1325
01:22:22,570 --> 01:22:27,280
euclidean geometry there but if you were to draw balls if you want around

1326
01:22:28,050 --> 01:22:30,780
if i i d then you would

1327
01:22:30,960 --> 01:22:35,600
cover the whole space so all functions would be in one of these rules

1328
01:22:36,290 --> 01:22:37,600
so that's the notion of color

1329
01:22:37,620 --> 01:22:40,430
and once you've done that

1330
01:22:40,440 --> 01:22:41,400
you can apply

1331
01:22:41,420 --> 01:22:42,850
the result

1332
01:22:42,860 --> 01:22:46,680
that we seen just before and say well if i want to control the deviation

1333
01:22:46,680 --> 01:22:48,810
for any function in the set

1334
01:22:48,860 --> 01:22:49,740
i first

1335
01:22:49,780 --> 01:22:53,120
find out what is the closest function

1336
01:22:53,130 --> 01:22:55,360
among these cover

1337
01:22:55,370 --> 01:22:56,560
if one of the

1338
01:22:57,160 --> 01:22:59,960
and i see the deviation will be those of

1339
01:22:59,970 --> 01:23:00,950
this function

1340
01:23:01,940 --> 01:23:03,560
this little alpha

1341
01:23:07,300 --> 01:23:08,510
and when i do that

1342
01:23:08,650 --> 01:23:13,370
i get so what i call covering number bayes bounds

1343
01:23:14,650 --> 01:23:16,910
i can replace

1344
01:23:16,920 --> 01:23:20,170
so the log cardinality of the set by

1345
01:23:20,180 --> 01:23:23,830
alpha times the locality plus

1346
01:23:23,850 --> 01:23:28,260
i have to count how many functions i have in my cover and this usually

1347
01:23:28,260 --> 01:23:33,560
denoted by the end of the which is the number of functions i need to

1348
01:23:33,560 --> 01:23:36,640
cover my space of two x two accuracy alpha

1349
01:23:40,510 --> 01:23:42,890
so this concerts can be done

1350
01:23:42,940 --> 01:23:47,990
even if you're a set of functions the function you set stimuli but you can

1351
01:23:47,990 --> 01:23:49,760
extract a subset

1352
01:23:49,780 --> 01:23:54,220
four which all functions are similar to at least one point in that subset

1353
01:23:54,230 --> 01:24:01,950
and then what we control the rate of convergence of your empirical process will be

1354
01:24:01,960 --> 01:24:07,200
the behaviour of beams and these covering numbers and

1355
01:24:07,220 --> 01:24:09,070
rule of some

1356
01:24:09,100 --> 01:24:09,920
is that

1357
01:24:09,940 --> 01:24:11,100
more less

1358
01:24:11,170 --> 01:24:13,470
covering number at scale alpha

1359
01:24:13,570 --> 01:24:17,720
four spades for like if you imagine a cube

1360
01:24:18,000 --> 01:24:20,120
in dimension d

1361
01:24:21,240 --> 01:24:22,900
and of alpha is

1362
01:24:22,910 --> 01:24:26,740
one of the fact that right so if you want to cover the cube of

1363
01:24:26,740 --> 01:24:28,350
dimension d with balls

1364
01:24:28,370 --> 01:24:30,520
you need something like

1365
01:24:32,000 --> 01:24:32,930
one of

1366
01:24:32,950 --> 01:24:36,850
to the d when outside the radius of these balls and see the cube the

1367
01:24:36,850 --> 01:24:38,860
unit cube

1368
01:24:40,410 --> 01:24:44,360
so this is an and what interest in the bound is the logarithm of that

1369
01:24:44,380 --> 01:24:45,730
so that's why

1370
01:24:45,750 --> 01:24:51,800
and that's something you obtain the logarithm of this is something like

1371
01:24:51,810 --> 01:24:53,430
the times log

1372
01:24:53,440 --> 01:24:55,230
one of

1373
01:24:55,250 --> 01:24:58,560
and if you plug it in the bounds of something like

1374
01:24:58,580 --> 01:25:03,210
square root of the log at one of our power so

1375
01:25:03,230 --> 01:25:04,620
the message here is that

1376
01:25:04,640 --> 01:25:06,840
if your space of functions where

1377
01:25:07,530 --> 01:25:08,820
so your subset

1378
01:25:08,850 --> 01:25:12,160
of the unit cube in some space of dimension d

1379
01:25:12,180 --> 01:25:16,150
so for example if you have a linear functions with the parameters

1380
01:25:16,270 --> 01:25:22,080
then you would have some some of the older square root of the over

1381
01:25:22,100 --> 01:25:24,720
despite this trick you know

1382
01:25:26,420 --> 01:25:32,620
you can actually go even further and further than that and repeat this construction so

1383
01:25:32,660 --> 01:25:36,340
here i was using only one scale i think

1384
01:25:36,350 --> 01:25:41,140
it i covered my class with balls operators and

1385
01:25:41,150 --> 01:25:42,500
but if you want to

1386
01:25:42,580 --> 01:25:45,820
it was

1387
01:25:45,990 --> 01:25:47,380
if you want to

1388
01:25:47,400 --> 01:25:51,190
capture a little bit more of the structure of q plus you can

1389
01:25:51,200 --> 01:25:52,880
iterate this construction

1390
01:25:53,570 --> 01:25:55,980
compute covers at all possible

1391
01:25:56,000 --> 01:25:58,800
registry actually you start by

1392
01:25:58,820 --> 01:26:01,730
covering your space was big balls

1393
01:26:01,740 --> 01:26:02,770
and then

1394
01:26:02,790 --> 01:26:06,160
you cover with smaller bills

1395
01:26:08,380 --> 01:26:10,650
and you do that

1396
01:26:10,660 --> 01:26:16,950
so four let's say four alpha of the form two to the miners k where

1397
01:26:17,560 --> 01:26:19,700
is from one two

1398
01:26:19,710 --> 01:26:22,810
infinity so it's in figure

1399
01:26:22,820 --> 01:26:26,580
or you stop at some level maybe

1400
01:26:26,590 --> 01:26:28,830
and what you do is now

1401
01:26:28,890 --> 01:26:32,370
you look at the centers of all these rules

1402
01:26:32,390 --> 01:26:36,810
of the small balls and all of the big ones and when you when you

1403
01:26:36,810 --> 01:26:42,470
have a function to for which you want to control the deviations you first look

1404
01:26:42,490 --> 01:26:44,630
at the closest function in the

1405
01:26:44,640 --> 01:26:45,670
the smallest

1406
01:26:47,950 --> 01:26:52,500
and then you approximate this function again by larger one

1407
01:26:52,520 --> 01:26:56,690
i mean from by a point in the larger cover and so on so you

1408
01:26:56,690 --> 01:27:00,360
construct what is called the chain and that's what is the

1409
01:27:00,380 --> 01:27:05,700
the value of things called chamber it's you construct the chain that goes from

1410
01:27:05,710 --> 01:27:07,150
the function space

1411
01:27:08,520 --> 01:27:11,530
centres of covers

1412
01:27:11,540 --> 01:27:16,280
of larger and larger edges and eventually we end up in the centre of the

1413
01:27:16,280 --> 01:27:18,030
whole space

1414
01:27:18,080 --> 01:27:23,000
again this is the picture that does not really correspond to the true reality because

1415
01:27:23,020 --> 01:27:26,940
you're not in the euclidean space the space of functions can be very

1416
01:27:26,950 --> 01:27:29,170
we are in no way

1417
01:27:29,240 --> 01:27:31,530
but this is give you the right information

1418
01:27:31,570 --> 01:27:36,750
and if you do that if you do this chain you can actually show that

1419
01:27:37,680 --> 01:27:42,910
the relevance notion of complexity that enters the barn is of this form this integral

1420
01:27:42,910 --> 01:27:45,080
of the square root of long enough

1421
01:27:45,120 --> 01:27:49,440
so it will again depend on the rate

1422
01:27:49,490 --> 01:27:52,860
of convergence and the rate of growth sorry

1423
01:27:52,910 --> 01:27:54,630
of the covering numbers

1424
01:28:10,290 --> 01:28:12,050
it's one of the symmetric space

1425
01:28:12,100 --> 01:28:13,970
thank for would whole

1426
01:28:15,580 --> 01:28:21,340
but not enough to specify that it means the inner product is finite dimensional

1427
01:28:21,690 --> 01:28:24,330
which is not the case for and it will be

1428
01:28:25,700 --> 01:28:29,750
but if you have a very large and

1429
01:28:40,590 --> 01:28:42,450
this is a view of the whole

1430
01:28:42,470 --> 01:28:44,800
so for you

1431
01:28:44,810 --> 01:28:46,480
the function space

1432
01:28:46,480 --> 01:28:50,830
what do

1433
01:29:19,480 --> 01:29:21,180
one of

1434
01:29:30,780 --> 01:29:40,670
all o

1435
01:29:40,940 --> 01:29:45,810
o eight

1436
01:30:36,550 --> 01:30:38,130
and here

1437
01:31:18,550 --> 01:31:25,870
first of all

1438
01:31:46,770 --> 01:31:51,600
how say

1439
01:32:19,480 --> 01:32:22,670
that said

1440
01:32:40,580 --> 01:32:42,890
it is

1441
01:32:48,990 --> 01:32:54,160
so you know it

1442
01:32:59,460 --> 01:33:01,100
they don't

1443
01:33:32,390 --> 01:33:36,370
if i hear

1444
01:34:14,860 --> 01:34:22,780
it turns out

1445
01:34:40,530 --> 01:34:44,860
now we can

1446
01:35:58,900 --> 01:35:59,880
all right

1447
01:35:59,880 --> 01:36:04,480
red and green points trying to infer the rest we've done some integrating out so we no

1448
01:36:04,480 --> 01:36:09,860
longer have the muse visible we don't care about those we simply care about

1449
01:36:09,860 --> 01:36:13,100
whether or not a X is matched to one of the Ys so that's the

1450
01:36:13,100 --> 01:36:19,060
existence of these matching links notice there are red points that are not observed and there are green

1451
01:36:19,060 --> 01:36:25,180
points that are not observed and we still care about that matching and the transformation

1452
01:36:25,180 --> 01:36:33,060
though I've actually assume now of course that it's a straight line so look quite hard I think

1453
01:36:33,180 --> 01:36:42,480
and then sorry alright I'm gonna assume no more noise from now

1454
01:36:42,480 --> 01:36:45,440
on you can do you know the other settings as well bu that this is a

1455
01:36:45,440 --> 01:36:52,500
Sphaeroccocusian noise and essentially this is this this term is what it is doing is really capturing

1456
01:36:52,500 --> 01:36:59,180
you would've meassure error actually funding this occasion and in the data but also the idea if

1457
01:36:59,180 --> 01:37:04,280
we think about is these hidden point models being somehow the origin of what these proteins are of

1458
01:37:04,290 --> 01:37:10,740
their shape then we're saying that over time of evolutionary proteins there's been some disturbances to

1459
01:37:10,740 --> 01:37:16,140
the to the original pattern so making all those assumptions that simplifying assumption we get we

1460
01:37:16,140 --> 01:37:21,720
now have the basic thing we need for bayesian analysis a joint distribution of the

1461
01:37:21,720 --> 01:37:28,300
unknowns and the data so the unknowns are the matching matrix the linear part

1462
01:37:28,300 --> 01:37:34,220
and the translation part  of the transformation the variants of this noble

1463
01:37:34,220 --> 01:37:42,000
disturbances and later itself and it has that that structure and you could possibly get to

1464
01:37:42,000 --> 01:37:46,380
this model from from other places but but I think this hidden hidden

1465
01:37:46,380 --> 01:37:51,950
process generation of this model is quite an attractive way to think about it

1466
01:37:52,060 --> 01:37:55,300
notice the form it has this is basically all to do with prior stuff the

1467
01:37:55,300 --> 01:38:02,200
the there's a Jacobian there to do with the transformation of the Y space

1468
01:38:02,400 --> 01:38:06,550
and then the main thing in here is you have a product over the matched pairs

1469
01:38:06,760 --> 01:38:10,380
so that's something you know we don't know it's not a fixed number of terms there and what you

1470
01:38:10,380 --> 01:38:14,020
see here is basically a product of two things so you can think of it as

1471
01:38:14,020 --> 01:38:21,060
a ratio of two things really it's to do with expressing the completing competing explanations so

1472
01:38:21,060 --> 01:38:26,580
we don't quite know how to match so wh wh given think of it

1473
01:38:26,580 --> 01:38:32,980
this way given A and we've find X J and this transformed version of Y K A are

1474
01:38:32,980 --> 01:38:39,920
in close proximity and there's two possibilities there either they are matched and that's

1475
01:38:39,920 --> 01:38:44,130
captured by this like this density function all this happened to be close

1476
01:38:44,130 --> 01:38:49,940
coincidentally so remember that lambda was the background rated hidden points so if there's a

1477
01:38:49,940 --> 01:38:55,420
loss of hidden points the coincidental explanation is more likely and that's gonna lower

1478
01:38:55,420 --> 01:39:03,640
this joint probability so I think it makes some sense okay so how can

1479
01:39:03,640 --> 01:39:09,760
you make inference with his model well there's a number of different approaches you

1480
01:39:09,760 --> 01:39:19,340
can use of EM type of algorithms of various you know trading descent or coordinate wise version to gradient

1481
01:39:19,680 --> 01:39:25,920
descent and indeed you could use those with or without a prior information

1482
01:39:26,100 --> 01:39:31,260
and you'd get a solution but actually they are not trivial they're still quite

1483
01:39:31,260 --> 01:39:35,000
a big algorithmic load and easier than what I'm about to suggest but

1484
01:39:35,000 --> 01:39:41,360
not they're not trivial so we'll what we do is go straight into this

1485
01:39:41,360 --> 01:39:50,540
to do that M C M C sampling and that raises a couple of interesting issues

1486
01:39:50,540 --> 01:39:55,860
so we have this I'm not going to suppose that the A matrix is

1487
01:39:55,860 --> 01:40:01,680
a rotation so it's a rigid body transformation now it's not a general transformation there's no

1488
01:40:01,680 --> 01:40:07,700
scale change there's no skewing it's just a rotation and a translation that's a rigid body

1489
01:40:07,700 --> 01:40:12,780
rot rotation the effect of that is that determinant of A is one so that simplifies

1490
01:40:12,780 --> 01:40:16,920
things but the main point is this it turns out that they under the

1491
01:40:16,920 --> 01:40:24,900
Gaussian assumption the  the conditional distribution for A the unknown rotation given the matching then the other

1492
01:40:24,910 --> 01:40:32,120
parameters has this form it involves A in a quite a simple way we have whatever

1493
01:40:32,120 --> 01:40:36,560
mention that i later in that session

1494
01:40:37,720 --> 01:40:40,850
so spectral clustering and racially processes

1495
01:40:42,490 --> 01:40:48,990
i think one thing modern sort of interesting things spectral clustering

1496
01:40:49,010 --> 01:40:52,810
i think it's really interesting but i don't know what the i don't understand what

1497
01:40:52,810 --> 01:40:56,690
the theoretical foundation is enough to see how that's being pushed forward i mean i

1498
01:40:56,690 --> 01:40:59,930
think we seem to have a bit of the doldrums at the moment good technique

1499
01:40:59,930 --> 01:41:03,090
but i don't know where it's going to reshape prices lots of people are working

1500
01:41:03,090 --> 01:41:10,100
on these various different things it's sort of bayesian nonparametric approach to clustering

1501
01:41:11,260 --> 01:41:12,930
so finally so my

1502
01:41:12,940 --> 01:41:16,450
the topic dimensionality reduction

1503
01:41:16,470 --> 01:41:21,310
one could also equally as a latent variable models and i just one

1504
01:41:24,010 --> 01:41:29,260
a little bit why dimensionality reduction is important so far everything i've shown is really

1505
01:41:29,260 --> 01:41:36,270
a trivial toy examples are well and this will be fairly trivial to hopefully the

1506
01:41:36,280 --> 01:41:40,720
one of the the things that's missing is something i think is really important and

1507
01:41:40,720 --> 01:41:41,580
that's the

1508
01:41:41,670 --> 01:41:43,700
very often

1509
01:41:43,720 --> 01:41:46,580
you're working with very high dimensional data so

1510
01:41:47,060 --> 01:41:48,710
a lot of these things

1511
01:41:49,120 --> 01:41:52,720
i'm drawing on the boards of very deceptive

1512
01:41:53,870 --> 01:41:57,280
you can understand three-dimensional dataspaces very well

1513
01:41:58,180 --> 01:42:02,490
and two dimensional space very well but in high dimensional data a lot of intuitions

1514
01:42:02,490 --> 01:42:07,670
collapse and i've done tutorials and that which themselves take a a couple of hours

1515
01:42:07,670 --> 01:42:09,250
so i'm not going to be too much that today

1516
01:42:09,780 --> 01:42:13,960
but i want to give an illustrative example of something like sometimes used try and

1517
01:42:13,960 --> 01:42:18,950
justify why dimensionality reduction is important and what can happen high dimensional data space so

1518
01:42:18,950 --> 01:42:24,650
this is the six digit our favorite machine learning data datasets we like to use

1519
01:42:24,650 --> 01:42:27,400
them all the time to demonstrate our algorithms

1520
01:42:28,760 --> 01:42:30,340
it from the USPS

1521
01:42:31,350 --> 01:42:35,290
and it's got three thousand six hundred forty eight dimensions so x in this case

1522
01:42:35,660 --> 01:42:40,390
is all the pixels in this image and i sixty four rows and fifty seven

1523
01:42:40,390 --> 01:42:43,170
columns so it's a very high dimensional

1524
01:42:43,220 --> 01:42:48,940
eight point but having said that that is not extraordinarily high dimensional if you look

1525
01:42:48,940 --> 01:42:54,100
at the say text datasets with using words the features you've got some minimum six

1526
01:42:54,100 --> 01:43:01,460
thousand there was for some applications and you may have many more so a microarrays

1527
01:43:01,490 --> 01:43:05,080
twenty five thousand genes so an extraordinary

1528
01:43:06,300 --> 01:43:08,600
for the sort of things we might try

1529
01:43:08,750 --> 01:43:14,000
OK so the problem with these spaces is they contain just more than one digit

1530
01:43:14,010 --> 01:43:15,160
right so

1531
01:43:15,170 --> 01:43:19,170
in this is one way of modelling the digit would be to have the probability

1532
01:43:19,170 --> 01:43:24,140
for each pixel being all independently so i'm going to model this it by saying

1533
01:43:24,140 --> 01:43:27,180
well i'm going to count how many pixels are on and how the probability for

1534
01:43:27,180 --> 01:43:32,030
each pixel being on which is the proportion of black pixels over white pixels and

1535
01:43:32,030 --> 01:43:33,300
then i'm going to sample

1536
01:43:33,310 --> 01:43:37,060
right so sampling from my model and here is a sample from my mother

1537
01:43:37,840 --> 01:43:41,130
within that space within that modelling space is obviously a lot more than this one

1538
01:43:41,130 --> 01:43:47,000
digit and in fact even if we sample every nanosecond from now until the end

1539
01:43:47,000 --> 01:43:51,070
of the universe you won't ever see the original six

1540
01:43:51,090 --> 01:43:56,150
the chances of becoming the space is so vast in terms of the dimensionality that

1541
01:43:56,150 --> 01:44:01,410
we're exploring how to model the so-called you will never see that six i mean

1542
01:44:01,410 --> 01:44:07,070
there's more possible samples in that image than there are particles in the observable universe

1543
01:44:07,070 --> 01:44:08,900
many many many times more

1544
01:44:10,690 --> 01:44:12,920
those spaces are kind of extraordinary

1545
01:44:13,430 --> 01:44:16,960
so the things you and these things provide problems because we're trying to explore the

1546
01:44:16,970 --> 01:44:20,950
space is sometimes with probabilistic models we got to some of the spaces that size

1547
01:44:22,270 --> 01:44:25,010
thing i want to say there is another sample and look it wasn't the original

1548
01:44:25,010 --> 01:44:27,800
six so that proves my point

1549
01:44:28,120 --> 01:44:33,340
so here is another way of thinking about how you should be modelling in those

1550
01:44:33,340 --> 01:44:38,660
spaces and it's one that i used to motivate dimensionality reduction so you can rotate

1551
01:44:38,670 --> 01:44:42,930
a prototype so his approach type six what we do is say walking model by

1552
01:44:42,930 --> 01:44:44,900
considering rotations

1553
01:44:44,970 --> 01:44:46,990
well six

1554
01:44:48,450 --> 01:44:53,510
now the interesting thing about that is we can project

1555
01:44:53,530 --> 01:44:56,630
what we can do is we can look in that space that is formed by

1556
01:44:56,630 --> 01:45:00,680
all those sections being rotated around and it's kind of interesting because this is the

1557
01:45:00,690 --> 01:45:02,760
linear two-dimensional projection

1558
01:45:02,810 --> 01:45:06,630
so that space is again three thousand six hundred forty eight dimensional but i found

1559
01:45:06,630 --> 01:45:11,000
the two-dimensional projection which is just the circle and i'm trying to show what these

1560
01:45:11,000 --> 01:45:15,510
images are so is this article because we just rotated

1561
01:45:15,520 --> 01:45:20,010
and this looks very one-dimensional so actually what's going on is three thousand six hundred

1562
01:45:20,010 --> 01:45:22,640
we just covered several different methods

1563
01:45:22,660 --> 01:45:24,980
actually just for value functions

1564
01:45:25,080 --> 01:45:28,320
before from from other comments made the claim that you have to the q function

1565
01:45:28,320 --> 01:45:36,150
is not value function OK sessions surprising there's also a method for controlling temporal difference

1566
01:45:36,150 --> 01:45:40,510
approaches control meaning you want to optimize policy

1567
01:45:40,560 --> 01:45:42,600
they used to control the process

1568
01:45:42,620 --> 01:45:44,780
that is exactly

1569
01:45:46,210 --> 01:45:50,010
the same as the value of the we saw earlier isabella to use

1570
01:45:50,010 --> 01:45:51,870
two values in place of

1571
01:45:51,920 --> 01:45:55,710
q values also known actions in place of your

1572
01:45:57,070 --> 01:46:00,240
you're just state advisory so previously

1573
01:46:01,560 --> 01:46:06,110
so it's a again look like in the coming war

1574
01:46:06,170 --> 01:46:07,540
in the gamma

1575
01:46:07,590 --> 01:46:09,920
for the

1576
01:46:10,210 --> 01:46:16,740
gamma time the q value for the next stage

1577
01:46:16,780 --> 01:46:20,670
with the a priori actually would take the next and the current policy

1578
01:46:21,530 --> 01:46:24,540
but he does it to athens tournaments method with

1579
01:46:24,600 --> 01:46:26,490
two is

1580
01:46:26,510 --> 01:46:31,090
OK so just see this as TV for q functions now

1581
01:46:31,100 --> 01:46:33,700
so one might wonder why this

1582
01:46:33,770 --> 01:46:35,850
the album's called source

1583
01:46:35,950 --> 01:46:41,150
i have to admit that i spent maybe in our two in vain searching for

1584
01:46:41,180 --> 01:46:42,900
this man named source

1585
01:46:42,950 --> 01:46:47,890
only to later realize and i read the book that sources stands for state action

1586
01:46:47,900 --> 01:46:50,180
reward state action

1587
01:46:51,590 --> 01:46:57,150
so yes so we have a current state and action get a reward given xt

1588
01:46:57,450 --> 01:47:00,450
next action source

1589
01:47:01,650 --> 01:47:04,350
but it is the it's just TD

1590
01:47:04,400 --> 01:47:05,560
temporal difference

1591
01:47:05,600 --> 01:47:06,680
approach for

1592
01:47:06,680 --> 01:47:11,150
two values such you can actually what that q values recall you can take to

1593
01:47:11,150 --> 01:47:16,230
maximize the action reinstate determine the best policy

1594
01:47:18,030 --> 01:47:22,890
now more theoretical importance if you know of reinforcement learning you may know about q

1595
01:47:22,890 --> 01:47:28,410
learning this is probably the most exciting temporal difference methods this is probably the most

1596
01:47:28,410 --> 01:47:30,350
well-known algorithms

1597
01:47:30,390 --> 01:47:35,720
i has a beautiful properties in theory no one use in practice

1598
01:47:36,350 --> 01:47:42,300
but let me me are all all introduce it just because of properties

1599
01:47:45,750 --> 01:47:50,580
instead of the sample back up so i sample state as prime and then i

1600
01:47:50,580 --> 01:47:51,510
look at the

1601
01:47:51,520 --> 01:47:54,480
action a prime under my current policy

1602
01:47:55,790 --> 01:47:58,070
four q learning

1603
01:47:58,070 --> 01:48:02,790
instead of choosing the action of macro policy actually just take the max over

1604
01:48:02,800 --> 01:48:06,790
often actions i can do this because i've got the q values

1605
01:48:07,910 --> 01:48:11,200
so if you do this update is because the previous update

1606
01:48:11,220 --> 01:48:14,070
this also converges to the optimal policy

1607
01:48:14,080 --> 01:48:20,320
to the other of the apple two values but more interestingly it doesn't require a

1608
01:48:20,320 --> 01:48:21,890
policy to be fixed

1609
01:48:21,910 --> 01:48:23,600
right there is no policy

1610
01:48:24,080 --> 01:48:26,720
or you don't have to fix policy

1611
01:48:26,860 --> 01:48:30,190
you can take any action many states

1612
01:48:30,200 --> 01:48:33,730
and this will still give you the optimal q values

1613
01:48:33,770 --> 01:48:37,860
and that's why it's so popular in a and so well known is the one

1614
01:48:37,890 --> 01:48:42,320
or lower than that doesn't require policy to find the FAQ values

1615
01:48:45,730 --> 01:48:48,770
no one uses it because

1616
01:48:48,790 --> 01:48:50,820
so in

1617
01:48:50,880 --> 01:48:52,920
the reason is

1618
01:48:53,040 --> 01:48:55,230
this one step updates

1619
01:48:55,700 --> 01:49:01,510
property values very so so so that the problem i'm going to give you also

1620
01:49:01,510 --> 01:49:04,970
applies to the to do this is the TD methods

1621
01:49:05,050 --> 01:49:11,970
that i discovered except that in primitive methods in a moment but you can particularly

1622
01:49:11,980 --> 01:49:15,850
OK so so let me let me let me give you some insight

1623
01:49:15,890 --> 01:49:19,670
imagine that i have a very long

1624
01:49:19,720 --> 01:49:21,980
change the world

1625
01:49:22,040 --> 01:49:29,670
and here is my goal

1626
01:49:29,670 --> 01:49:31,230
start there

1627
01:49:31,270 --> 01:49:35,140
and i think it all literally stated very simple actually i just go left to

1628
01:49:35,770 --> 01:49:39,820
now to make it all the way you write your water one

1629
01:49:39,830 --> 01:49:42,700
and if i may be one here

1630
01:49:42,700 --> 01:49:49,670
and then

1631
01:49:53,180 --> 01:49:55,040
thank you

1632
01:49:55,050 --> 01:49:58,530
one should also fail

1633
01:49:58,560 --> 01:50:01,120
so we need more

1634
01:50:15,730 --> 01:50:19,570
that's why

1635
01:50:19,750 --> 01:50:24,310
so just she was quite a bit

1636
01:50:26,780 --> 01:50:29,280
not all of

1637
01:50:37,790 --> 01:50:39,710
that is

1638
01:50:44,270 --> 01:50:49,200
that's the way to the people who

1639
01:50:49,240 --> 01:50:51,430
it may be

1640
01:50:51,450 --> 01:50:53,070
and then

1641
01:50:53,090 --> 01:50:54,540
that is my

1642
01:50:56,790 --> 01:51:02,150
it does and i here

1643
01:51:02,200 --> 01:51:03,820
the court

1644
01:51:03,840 --> 01:51:05,180
probably you know

1645
01:51:05,200 --> 01:51:07,930
the every

1646
01:51:07,950 --> 01:51:09,290
the main line

1647
01:51:10,210 --> 01:51:12,540
which is exactly the same way

1648
01:51:13,380 --> 01:51:14,840
this can also not

1649
01:51:14,870 --> 01:51:19,400
is there and i think

1650
01:51:23,670 --> 01:51:24,620
they are

1651
01:51:24,640 --> 01:51:28,120
the same way

1652
01:51:28,150 --> 01:51:32,450
so here we have some interesting because of

1653
01:51:33,370 --> 01:51:34,400
you know what

1654
01:51:35,070 --> 01:51:39,400
o twenty five

1655
01:51:39,420 --> 01:51:42,930
there are great

1656
01:51:47,040 --> 01:51:56,980
i think it's likely respect what you do it because want

1657
01:51:57,000 --> 01:51:58,420
it is

1658
01:51:58,450 --> 01:51:59,500
so before

1659
01:51:59,510 --> 01:52:03,290
first it was only two pictures

1660
01:52:04,600 --> 01:52:08,470
everybody agrees that features on the band

1661
01:52:08,630 --> 01:52:13,790
but even then it was sort of my

1662
01:52:14,050 --> 01:52:15,420
some pictures

1663
01:52:16,690 --> 01:52:18,070
by one person

1664
01:52:18,860 --> 01:52:21,700
quite number one hundred sixty four

1665
01:52:21,750 --> 01:52:23,830
especially in the

1666
01:52:26,800 --> 01:52:30,290
i don't know where

1667
01:52:30,320 --> 01:52:36,050
in my

1668
01:52:36,070 --> 01:52:38,760
so here is six

1669
01:52:38,790 --> 01:52:41,950
the reason why they are marked by

1670
01:52:43,130 --> 01:52:45,070
it's that passion

1671
01:52:50,570 --> 01:52:53,040
six thousand nine

1672
01:52:54,760 --> 01:52:58,390
the park is by one person

1673
01:53:01,740 --> 01:53:03,130
the collection

1674
01:53:03,140 --> 01:53:07,140
well it just that's why it's not so much

1675
01:53:07,190 --> 01:53:13,670
so we have two hundred images which are not lie about this and

1676
01:53:13,670 --> 01:53:18,320
every which was marked by someone who is great

1677
01:53:18,440 --> 01:53:20,100
every day

1678
01:53:25,010 --> 01:53:26,670
this image was where

1679
01:53:27,950 --> 01:53:29,220
it's possible

1680
01:53:30,220 --> 01:53:33,550
so far more than eighty percent of the vote

1681
01:53:34,670 --> 01:53:36,890
all the band

1682
01:53:39,200 --> 01:53:42,660
the only

1683
01:53:42,670 --> 01:53:52,420
the day pictures like

1684
01:53:52,450 --> 01:53:54,130
this one

1685
01:53:54,170 --> 01:53:58,740
i have exactly the same people

1686
01:53:59,180 --> 01:54:02,030
so there seems little

1687
01:54:02,880 --> 01:54:04,320
one of the people

1688
01:54:04,330 --> 01:54:07,010
i think that they are

1689
01:54:10,210 --> 01:54:11,700
things that

1690
01:54:16,560 --> 01:54:22,020
there no pictures

1691
01:54:22,050 --> 01:54:23,260
you also

1692
01:54:24,690 --> 01:54:26,200
similarly most of

1693
01:54:26,210 --> 01:54:28,820
there first some all my them is irrelevant

1694
01:54:28,840 --> 01:54:30,250
there's a lot of

1695
01:54:30,320 --> 01:54:36,260
but there are a lot of

1696
01:54:36,260 --> 01:54:38,640
i have also

1697
01:54:38,660 --> 01:54:39,490
so what

1698
01:54:43,630 --> 01:54:46,310
there is

1699
01:54:46,320 --> 01:54:52,950
pictures of the life span of just where i one person might one of the

1700
01:54:56,830 --> 01:54:59,970
and that

1701
01:55:00,000 --> 01:55:01,520
of participants

1702
01:55:01,550 --> 01:55:02,680
come the

1703
01:55:02,720 --> 01:55:06,220
there you

1704
01:55:06,250 --> 01:55:09,200
are some consider them not only have

1705
01:55:09,250 --> 01:55:12,320
i think the most probable the

1706
01:55:12,330 --> 01:55:14,130
and the

1707
01:55:15,560 --> 01:55:17,370
i give you my

1708
01:55:18,650 --> 01:55:20,500
there was

1709
01:55:20,550 --> 01:55:22,340
if you want

