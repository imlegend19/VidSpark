1
00:00:00,000 --> 00:00:01,980
when q

2
00:00:01,990 --> 00:00:06,300
when you choose the proposal mechanism to keep the particles

3
00:00:06,310 --> 00:00:06,970
to be

4
00:00:06,990 --> 00:00:08,470
the transition prior

5
00:00:08,470 --> 00:00:12,530
which is what i did in the pseudo code that i showed you before

6
00:00:12,550 --> 00:00:16,120
i took the particles that first put them through the transition prior

7
00:00:16,160 --> 00:00:18,310
and then i weighted by the likelihood

8
00:00:18,330 --> 00:00:22,000
the reason is because i was using a proposal mechanism that was the prior i

9
00:00:22,000 --> 00:00:24,780
was trying the particles from the prior distribution

10
00:00:24,880 --> 00:00:29,640
if i do that if i choose q q is arbitrary you can choose

11
00:00:29,680 --> 00:00:33,400
but just keep to read prior then this councils with this

12
00:00:33,450 --> 00:00:36,720
and the way is just the likelihood

13
00:00:37,360 --> 00:00:40,700
when the algorithm was proposed in the early nineties and so on was proposed and

14
00:00:40,700 --> 00:00:44,110
the names like survival of the fittest likelihood weighting

15
00:00:44,120 --> 00:00:47,140
and so on and no one actually had bothered to the right thing in its

16
00:00:47,140 --> 00:00:49,020
full generality

17
00:00:49,040 --> 00:00:54,470
so most people are just using this kind of

18
00:00:55,870 --> 00:00:59,730
condensation which is another ignorance computer vision was doing that

19
00:00:59,790 --> 00:01:02,980
in fact they don't even have resampling

20
00:01:03,000 --> 00:01:09,690
the algorithm actually technically speaking with four from one of his sisters

21
00:01:09,700 --> 00:01:11,350
arnaud doucet

22
00:01:11,360 --> 00:01:20,130
so the decision was the first one version formalise this properly

23
00:01:20,200 --> 00:01:23,940
that's one application of it but it's now there's many more

24
00:01:23,990 --> 00:01:27,570
OK and this is the equation i expect all of to memorize

25
00:01:27,580 --> 00:01:34,830
that's rather this equation you can see if you watch the video two white and

26
00:01:35,700 --> 00:01:37,880
it would seem like an hour

27
00:01:37,930 --> 00:01:42,160
that's the question about chronic you haven't seen it i got it in my laptop

28
00:01:42,390 --> 00:01:44,850
a player during the break

29
00:01:44,860 --> 00:01:52,130
and if you have a differential equation lecture bigger particles in boxes and so on

30
00:01:52,220 --> 00:01:55,560
you can transform this an integral form

31
00:01:56,280 --> 00:02:01,140
and what you have here is basically nineteen equation with iron functions

32
00:02:01,150 --> 00:02:04,760
i can think of this as the a matrix i can find

33
00:02:04,790 --> 00:02:09,700
and you have this kernel of the kernel integral the green function

34
00:02:10,940 --> 00:02:14,410
and you don't need to know this because

35
00:02:14,420 --> 00:02:15,450
the details

36
00:02:15,490 --> 00:02:19,810
the important thing is that in that in the area of quantum computers

37
00:02:19,830 --> 00:02:22,500
quantum physics

38
00:02:22,920 --> 00:02:25,790
you can also think of

39
00:02:26,370 --> 00:02:30,100
when you have these particles and you want to find the i can functions of

40
00:02:30,100 --> 00:02:34,600
these particles what you do is you keep passing the particles the

41
00:02:34,610 --> 00:02:38,100
but the kernel of the system in order to solve danger

42
00:02:38,140 --> 00:02:41,770
in other words if you have a vector that's that's all entries that up to

43
00:02:42,640 --> 00:02:46,730
you multiply that by stochastic matrix enough times

44
00:02:46,750 --> 00:02:49,100
that vector converges to a single fact

45
00:02:49,100 --> 00:02:51,560
that's the google algorithm basic

46
00:02:51,580 --> 00:02:55,010
it's a random walk converges to a single fact

47
00:02:55,170 --> 00:02:56,470
they take the web

48
00:02:56,470 --> 00:02:58,630
to construct the matrix for the web

49
00:02:58,640 --> 00:03:02,240
which is a lot smaller than this one in fact this is continuing

50
00:03:02,300 --> 00:03:03,870
and then from that

51
00:03:04,510 --> 00:03:08,940
this is just matrix vector multiplication and if you do this repeated enough times will

52
00:03:08,940 --> 00:03:10,810
converge to the effect

53
00:03:10,820 --> 00:03:13,050
it's called the power method

54
00:03:13,120 --> 00:03:14,970
numerical computation

55
00:03:14,970 --> 00:03:19,190
so we can do the same thing here even though this is continuous we approximated

56
00:03:19,190 --> 00:03:20,540
with particles

57
00:03:20,560 --> 00:03:24,530
and we propagate is particles in the same manner is this is

58
00:03:24,580 --> 00:03:27,370
has the power method or q

59
00:03:27,410 --> 00:03:31,960
if you will the surfer method of google these particles are surfing the web

60
00:03:31,970 --> 00:03:35,870
and out of it you get the i can functions

61
00:03:35,920 --> 00:03:42,540
which other quantities of interest or the eigen values which other ground energy states

62
00:03:42,600 --> 00:03:47,570
so this field of monte carlo even though we knew in machine learning and so

63
00:03:48,760 --> 00:03:50,560
for particle filters

64
00:03:50,620 --> 00:03:54,880
there was a sort of parallel track called quantum monte carlo

65
00:03:54,890 --> 00:03:57,480
they're not as good

66
00:03:57,500 --> 00:04:00,180
the state of the art in the quantum monte carlo world is not as good

67
00:04:00,180 --> 00:04:08,110
as the state of the art in the machine learning literature now

68
00:04:08,150 --> 00:04:10,830
OK so in general the algorithm then it's like this

69
00:04:10,850 --> 00:04:14,470
if we don't make the simplification of sampling from the prior we really have something

70
00:04:14,470 --> 00:04:17,010
from an arbitrary distribution q

71
00:04:17,030 --> 00:04:19,260
and then we awaiting the particles

72
00:04:19,310 --> 00:04:22,320
probably because think the expression that had

73
00:04:22,330 --> 00:04:24,470
we normalize the weight something

74
00:04:24,510 --> 00:04:28,470
the advantage of doing this is that now you have a proposal distribution can depend

75
00:04:28,470 --> 00:04:30,430
on the current observations

76
00:04:30,450 --> 00:04:33,620
you can have a data driven proposal distribution

77
00:04:33,710 --> 00:04:36,790
so if you have an algorithm like

78
00:04:36,790 --> 00:04:40,370
for the hockey players for example what we did is we use a deposed

79
00:04:40,390 --> 00:04:44,700
to generate detections of the what the players were on the scene

80
00:04:44,700 --> 00:04:48,120
and that is this proposals of what these guys might be

81
00:04:48,160 --> 00:04:49,730
we also used

82
00:04:49,730 --> 00:04:55,350
the transitions of the play the dynamics to get her to get likely proposal mechanism

83
00:04:55,350 --> 00:04:58,040
sister where we believe that play with be

84
00:04:58,080 --> 00:05:01,980
so yes all sorts of heuristics to extract good q

85
00:05:02,000 --> 00:05:05,770
and usually that you can look at the data sets data driven

86
00:05:05,830 --> 00:05:10,080
there's a guy in UCLA that has made a career of

87
00:05:10,100 --> 00:05:15,000
doing this sort of thing they prevent what he calls data driven MCMC and statistics

88
00:05:15,000 --> 00:05:17,600
most of us to data driven stuff

89
00:05:17,750 --> 00:05:19,750
to get proposal distributions

90
00:05:19,810 --> 00:05:25,160
in moderate dimensions up to ten dimensions was recall we're doing importance sampling

91
00:05:25,180 --> 00:05:28,140
not expect around the particle filter nine hundred

92
00:05:28,180 --> 00:05:31,060
continuous dimensions and that will work

93
00:05:31,060 --> 00:05:34,410
mike work by because you happen to have a simple model

94
00:05:36,040 --> 00:05:38,330
playing with fire that

95
00:05:38,350 --> 00:05:40,790
this this weighs around

96
00:05:43,540 --> 00:05:45,890
and this the sort of thing that should be you yes so you can see

97
00:05:46,680 --> 00:05:54,330
you're running here by interparticle forces in parallel parking football players and so on

98
00:05:54,370 --> 00:05:56,160
and of course if you can try

99
00:05:56,270 --> 00:05:58,700
player track person

100
00:05:58,710 --> 00:06:00,370
this is the guy called

101
00:06:00,370 --> 00:06:04,500
all these algorithms and this is the

102
00:06:04,520 --> 00:06:06,870
robert this tracking

103
00:06:08,230 --> 00:06:20,830
kenji signed

104
00:06:24,500 --> 00:06:28,790
one class of models for these methods have actually been very useful

105
00:06:28,790 --> 00:06:35,410
the size of the image tracking popular is methods that have this sort of hierarchical

106
00:06:37,890 --> 00:06:42,080
and the reason for this is that

107
00:06:42,080 --> 00:06:45,600
it is the semantics of the graph

108
00:06:45,620 --> 00:06:49,020
given x

109
00:06:49,060 --> 00:06:54,430
y two does not depend on z

110
00:06:54,640 --> 00:06:59,140
if you knew the axis that breaks the dependence of the y and that

111
00:06:59,230 --> 00:07:03,290
and that is an extremely powerful things to note that because

112
00:07:03,310 --> 00:07:06,430
what that allows you to do is that means that all you have to do

113
00:07:06,430 --> 00:07:08,910
is inferred these guys

114
00:07:08,910 --> 00:07:11,930
given those guys you can infer that the guy

115
00:07:11,980 --> 00:07:13,370
so you will win

116
00:07:13,390 --> 00:07:17,850
we will be able to a lot of analytical computations so i think the most

117
00:07:19,160 --> 00:07:21,680
particle filters out there are these that combined

118
00:07:21,680 --> 00:07:23,870
analytical computations

119
00:07:23,870 --> 00:07:28,050
my name is the bush from general electric corporate research

120
00:07:28,060 --> 00:07:29,990
global research center

121
00:07:30,010 --> 00:07:33,560
i want to talk about an introduction to active networks

122
00:07:33,570 --> 00:07:38,380
discuss little bit about not only active networks introduce the topic but also talk about

123
00:07:38,380 --> 00:07:40,250
security and active networks

124
00:07:40,270 --> 00:07:43,900
as well as commander of complexity

125
00:07:43,910 --> 00:07:46,000
and the trade-off between

126
00:07:46,010 --> 00:07:48,970
computation and communication

127
00:07:49,010 --> 00:07:51,430
that active networks is brought about

128
00:07:52,060 --> 00:07:56,290
i also should mention that you can click on the hand icon to submit questions

129
00:07:56,300 --> 00:07:58,500
and click on the globe icon

130
00:07:58,510 --> 00:08:01,140
to download this presentation

131
00:08:04,750 --> 00:08:07,570
so first one like the knowledge that a lot of this work came out of

132
00:08:08,220 --> 00:08:10,460
the dark but several years ago

133
00:08:10,480 --> 00:08:14,220
and i want to thank all the people that helped to put together a lot

134
00:08:14,220 --> 00:08:14,990
of these

135
00:08:15,050 --> 00:08:18,810
i won't say standards but rather frameworks that are being used

136
00:08:18,990 --> 00:08:23,630
to define what active networks are

137
00:08:23,650 --> 00:08:29,030
the outline of my talk i will introduce active networks

138
00:08:29,070 --> 00:08:31,610
and the assumption is that there is no

139
00:08:31,640 --> 00:08:36,970
strict background required and communications two to understand this talk i hope

140
00:08:36,980 --> 00:08:43,940
we'll talk about a little bit more detail about active network execution environments and a

141
00:08:43,940 --> 00:08:46,170
testbed called be able

142
00:08:46,180 --> 00:08:50,250
talk a little bit more about the security behind active networks because that is the

143
00:08:50,250 --> 00:08:52,060
big question is

144
00:08:52,150 --> 00:08:54,660
they offer a lot of flexibility but

145
00:08:54,710 --> 00:08:58,400
they also bring up the question above management security because they are

146
00:08:58,410 --> 00:09:01,240
so flexible

147
00:09:01,260 --> 00:09:04,470
and in that part of the

148
00:09:04,490 --> 00:09:09,970
solution to security we think can be found in understanding commander of complexity in the

149
00:09:11,180 --> 00:09:17,180
of code versus data inside the network

150
00:09:17,700 --> 00:09:19,170
i recently went to

151
00:09:19,180 --> 00:09:21,430
presentation in course

152
00:09:21,480 --> 00:09:23,050
it was told that people

153
00:09:23,100 --> 00:09:26,440
generally come away with only three things after presentation

154
00:09:26,500 --> 00:09:29,900
so the three things i'd like people to come away with from this presentation are

155
00:09:29,900 --> 00:09:31,600
active networks are really cool

156
00:09:32,000 --> 00:09:33,610
it's a neat idea

157
00:09:35,100 --> 00:09:37,320
there is active networks can be secure

158
00:09:37,380 --> 00:09:40,630
at least as secure as legacy networks and

159
00:09:40,640 --> 00:09:42,490
again i think

160
00:09:42,500 --> 00:09:46,100
for people to be throwing stones at active networks they should also think about how

161
00:09:46,100 --> 00:09:48,540
secure legacy networks are

162
00:09:48,560 --> 00:09:50,770
as well

163
00:09:50,880 --> 00:09:55,990
and that the data and algorithm are mutable and fluid within active networks

164
00:09:56,000 --> 00:09:57,330
and there's

165
00:09:57,340 --> 00:10:02,850
the needs to be a real fundamental understanding the trade that can occur

166
00:10:02,900 --> 00:10:04,660
in active networks between

167
00:10:04,670 --> 00:10:10,970
both data and algorithms since both can be injected dynamically inside the network

168
00:10:10,980 --> 00:10:13,730
OK let me start off with the motivation

169
00:10:13,740 --> 00:10:15,600
for active networking

170
00:10:16,510 --> 00:10:19,220
first of all legacy networks u

171
00:10:19,240 --> 00:10:22,000
i have a piece of data static piece of data

172
00:10:22,010 --> 00:10:23,630
you want to send it

173
00:10:23,680 --> 00:10:26,650
to the destination put packet send and

174
00:10:26,660 --> 00:10:27,860
it gets there

175
00:10:27,870 --> 00:10:33,630
with active networks were starting to include code dynamically with the data that interacts deep

176
00:10:33,630 --> 00:10:38,510
inside the network to change the function of the network itself

177
00:10:38,520 --> 00:10:40,600
and why would we want to do this

178
00:10:40,620 --> 00:10:44,600
well first of all hardware is becoming faster and faster

179
00:10:44,610 --> 00:10:48,720
and more efficient and there are times when it is not fully utilized

180
00:10:48,770 --> 00:10:55,390
so can we better food more fully utilize active or networking hardware

181
00:10:55,400 --> 00:10:58,830
and we think active networking can help people to utilize the hardware

182
00:10:58,850 --> 00:11:01,830
it certainly enables more flexible network

183
00:11:01,880 --> 00:11:06,760
in fact extremely flexible network capabilities change on the fly

184
00:11:06,900 --> 00:11:12,510
in fact we illustrate that with the darwin project in network where we put this

185
00:11:12,510 --> 00:11:13,950
on top of a

186
00:11:13,960 --> 00:11:16,130
wireless communications network

187
00:11:16,170 --> 00:11:19,410
but there is talk that i'm giving his focus strictly on active networking which can

188
00:11:19,410 --> 00:11:21,890
be both an fixed network wireless network

189
00:11:21,910 --> 00:11:24,180
any other kind of network

190
00:11:24,280 --> 00:11:26,320
it decouples the protocol

191
00:11:26,340 --> 00:11:27,640
from the transport

192
00:11:27,700 --> 00:11:30,370
so when i go through the slides i'll be talking about a framework for active

193
00:11:32,760 --> 00:11:35,820
it's a very general framework and the idea is that the code is going with

194
00:11:35,820 --> 00:11:36,810
the data

195
00:11:36,820 --> 00:11:40,570
and the code can essentially carry the protocol

196
00:11:40,580 --> 00:11:43,880
with the information that's going with it

197
00:11:43,940 --> 00:11:48,300
one of the host was can minimize requirements for global agreement again because you've decouple

198
00:11:48,300 --> 00:11:49,910
the protocol from the transport

199
00:11:49,970 --> 00:11:51,620
we were

200
00:11:51,630 --> 00:11:55,240
the hope was that you can avoid taking years and years to build

201
00:11:55,290 --> 00:11:57,210
right standards

202
00:11:57,220 --> 00:12:02,730
because people be injecting their own standards dynamically on the fly

203
00:12:02,740 --> 00:12:05,850
obviously it enables on the fly experimentation if you have an active network testbed you

204
00:12:05,850 --> 00:12:07,680
can inject the code for new

205
00:12:08,270 --> 00:12:10,080
types of behaviour

206
00:12:10,090 --> 00:12:11,970
on the fly

207
00:12:12,040 --> 00:12:16,350
and it enables faster deployment of new services obviously

208
00:12:16,360 --> 00:12:21,460
OK two primary goal throughout this talk i've incorporated

209
00:12:21,470 --> 00:12:25,090
quite a few references so the hope is that if you download these slides

210
00:12:25,620 --> 00:12:29,460
you can go right to these references and get the details that you need

211
00:12:29,510 --> 00:12:35,280
the website would point to the work by darpa on this topic

212
00:12:35,390 --> 00:12:38,870
and there's an email list for a

213
00:12:38,900 --> 00:12:44,200
conversations going on related to the development of active networking

214
00:12:44,270 --> 00:12:47,040
just to reiterate again

215
00:12:47,060 --> 00:12:48,820
if you look at it as an evolution

216
00:12:48,830 --> 00:12:51,250
traditional packet networks

217
00:12:51,260 --> 00:12:53,170
there was a header data

218
00:12:53,220 --> 00:12:54,310
and you

219
00:12:54,320 --> 00:12:56,920
inject your data on one side

220
00:12:56,930 --> 00:13:03,300
networks it takes at the other side through routers switches bridges hubs et cetera

221
00:13:05,730 --> 00:13:07,970
with active networking

222
00:13:08,010 --> 00:13:10,530
code can now appear inside the packets

223
00:13:10,540 --> 00:13:14,020
so every packet that you send not only contains data information that you want to

224
00:13:14,860 --> 00:13:16,290
but it can

225
00:13:16,330 --> 00:13:18,400
actually contain code

226
00:13:18,420 --> 00:13:21,860
which can change the behaviour of the network

227
00:13:21,870 --> 00:13:24,510
as the packet is flowing through the network

228
00:13:24,560 --> 00:13:25,500
and it can be

229
00:13:25,520 --> 00:13:26,710
that could behind

230
00:13:26,730 --> 00:13:30,750
to operate in change the behavior of nodes

231
00:13:30,770 --> 00:13:32,920
as it flows through the network

232
00:13:32,930 --> 00:13:36,720
so you can imagine all kinds of new possibilities with active networks that the monitoring

233
00:13:36,720 --> 00:13:38,010
predictive control

234
00:13:40,800 --> 00:13:45,130
making devices even more intelligent and responsive

235
00:13:45,180 --> 00:13:47,120
reducing deployment time

236
00:13:48,590 --> 00:13:50,660
and devices become more

237
00:13:50,710 --> 00:13:55,850
the where their environments

238
00:13:56,860 --> 00:13:59,870
if we look at it again from high level changes in the inevitable if you

239
00:13:59,870 --> 00:14:01,630
look at the internet protocol

240
00:14:01,650 --> 00:14:08,630
some people might claim that is becoming fossilized beginning to become more resistant to change

241
00:14:08,840 --> 00:14:15,540
it's becoming extremely complex is there were four thousand rfcs describing the internet protocol now

242
00:14:15,580 --> 00:14:19,500
and they're coming out at faster rates all the time

243
00:14:19,840 --> 00:14:25,050
takes all quite awhile ago of IPTV fought IPC these six

244
00:14:25,180 --> 00:14:27,350
there's is a lack of

245
00:14:27,360 --> 00:14:30,020
strong security paradigm downward you know

246
00:14:30,020 --> 00:14:32,980
but this one it will be can associated with

247
00:14:33,230 --> 00:14:38,840
and earlier customer native wild fish two thousand my only appear

248
00:14:38,880 --> 00:14:40,820
once we see customer number

249
00:14:40,840 --> 00:14:42,670
o five thousand one thousand something

250
00:14:43,860 --> 00:14:46,860
so the first thing we have to do to see that this thing is infinitely

251
00:14:47,780 --> 00:14:50,710
it's kind forget about the ordering of the dishes

252
00:14:50,710 --> 00:14:56,020
in the original paper in griffiths and ghahramani they count

253
00:14:56,040 --> 00:15:01,110
what with this forgetting of the ordering of the dishes by looking at equivalence classes

254
00:15:03,550 --> 00:15:04,960
of this

255
00:15:04,980 --> 00:15:06,480
of these metrices

256
00:15:06,500 --> 00:15:09,020
where each equivalence class is obtained by

257
00:15:09,050 --> 00:15:11,650
ordering reordering all the dishes

258
00:15:11,670 --> 00:15:15,110
but the way we're going to do in his covered that simple i think was

259
00:15:15,110 --> 00:15:17,050
simply cannot name each dish

260
00:15:17,050 --> 00:15:21,900
so this k is going to be named lambda stocking well under stock is going

261
00:15:21,900 --> 00:15:24,250
to be drawn i i d from

262
00:15:24,270 --> 00:15:26,340
some base distribution h

263
00:15:26,380 --> 00:15:32,400
and of course the the name of the first dish is going to be drawn

264
00:15:32,400 --> 00:15:35,630
from h in the name of the two thousand this is also to be drawn

265
00:15:35,630 --> 00:15:36,780
from h so

266
00:15:36,820 --> 00:15:37,570
if we

267
00:15:37,590 --> 00:15:39,920
replace each dish

268
00:15:39,920 --> 00:15:42,500
the index of each dish by its name

269
00:15:42,520 --> 00:15:46,770
then we have effectively forgotten about the ordering of the dishes

270
00:15:46,800 --> 00:15:51,300
so once we have named this dish is a customer now will in a set

271
00:15:51,300 --> 00:15:52,340
of dishes

272
00:15:52,340 --> 00:15:55,300
this i i simply going to be the set of

273
00:15:55,300 --> 00:16:00,280
lambda stockade whereas i k equal to one and this set is independent of the

274
00:16:00,280 --> 00:16:06,270
ordering of the issues that we can work out the joint probability again this is

275
00:16:06,270 --> 00:16:09,340
not quite a density but

276
00:16:09,400 --> 00:16:13,920
you can easily work out the this probability here because to something which looks like

277
00:16:14,650 --> 00:16:18,320
and it is independent of the ordering of the customers

278
00:16:18,340 --> 00:16:23,130
and now we see that this thing will be infinitely exchangeable

279
00:16:23,520 --> 00:16:27,500
so we can apply definitely this theorem again and that states that there is some

280
00:16:27,500 --> 00:16:29,610
random measure

281
00:16:29,630 --> 00:16:32,750
that makes all of our customers independence

282
00:16:32,750 --> 00:16:36,520
and this measure is what's called the beta process

283
00:16:37,840 --> 00:16:41,150
scott not surprising that is called the beta process because

284
00:16:41,170 --> 00:16:44,820
this random measure we can think of it as the infinite limit of this meals

285
00:16:44,840 --> 00:16:45,610
which are

286
00:16:45,630 --> 00:16:48,460
beta distributed

287
00:16:53,860 --> 00:17:00,800
technically the way they but better process is defined the simplest way of defective of

288
00:17:00,800 --> 00:17:05,630
defining process is as follows is going to have

289
00:17:05,670 --> 00:17:10,800
i can introduce new parameter see here but this is another parameter in the previous

290
00:17:10,800 --> 00:17:16,270
insurgencies equal equal to one so we say that b is distributed according to a

291
00:17:16,280 --> 00:17:21,380
bit process with these parameters if it is a random discrete measure

292
00:17:21,820 --> 00:17:25,820
it's not probability measure is that is a discrete measure

293
00:17:25,840 --> 00:17:29,360
so it's also as an infinite sum of atoms except that the weights on the

294
00:17:29,360 --> 00:17:31,050
atoms in the not sum to one

295
00:17:31,070 --> 00:17:35,590
so b is going to be some from k from one to infinity of UK

296
00:17:35,590 --> 00:17:41,400
delta theta stock so this is a point mass at the test rk with the

297
00:17:41,400 --> 00:17:42,880
mass of UK

298
00:17:42,900 --> 00:17:47,820
and again just as in the stick breaking construction

299
00:17:47,840 --> 00:17:52,340
we can specify the beta process if we can describe the joint distributions

300
00:17:52,420 --> 00:17:55,540
of the weights and the locations of the atoms

301
00:17:55,670 --> 00:18:01,130
and we can specify desired follows so the points here

302
00:18:01,400 --> 00:18:05,210
if we look at this point statehouse star one we wanted to start to be

303
00:18:05,270 --> 00:18:08,250
to disappoint in the two dimensional space

304
00:18:08,280 --> 00:18:15,320
if you think of this as our data access

305
00:18:15,360 --> 00:18:20,730
and this is our email access

306
00:18:20,750 --> 00:18:22,440
right so meal is

307
00:18:22,460 --> 00:18:27,040
atoms and it turns out that the same user the mass on their terms and

308
00:18:27,040 --> 00:18:28,000
ten so that

309
00:18:28,000 --> 00:18:30,270
there will always be between zero and one

310
00:18:30,300 --> 00:18:33,400
so this space goes from zero to one

311
00:18:33,420 --> 00:18:37,000
and this is i think that space which may not be one-dimensional that we can

312
00:18:37,170 --> 00:18:40,170
visualize this in a one dimensional space

313
00:18:40,190 --> 00:18:45,320
and this set of points so for example this could be

314
00:18:45,400 --> 00:18:50,750
they data one mu one

315
00:18:50,840 --> 00:18:51,920
this could be

316
00:18:52,020 --> 00:18:54,550
eighty two

317
00:18:54,550 --> 00:18:55,860
me too

318
00:18:55,900 --> 00:18:57,940
and this is the first tree

319
00:18:58,090 --> 00:18:59,500
new tree

320
00:18:59,500 --> 00:19:06,300
and so on so we lots of points we have infinite number of points here

321
00:19:06,300 --> 00:19:08,500
it's kind of points on this space

322
00:19:08,650 --> 00:19:11,630
and this b is

323
00:19:11,650 --> 00:19:15,860
discrete measures so we can can visualize this as

324
00:19:15,880 --> 00:19:20,190
delta functions right

325
00:19:20,190 --> 00:19:23,610
where the height and location is going to be the tag

326
00:19:23,630 --> 00:19:25,440
theta in UK

327
00:19:25,460 --> 00:19:27,610
and this infinite number of

328
00:19:27,690 --> 00:19:30,880
now look like this

329
00:19:30,900 --> 00:19:37,170
and the way we can express the joint distribution of

330
00:19:38,050 --> 00:19:40,940
both the mass and the location of the

331
00:19:40,960 --> 00:19:42,040
of the atoms

332
00:19:42,050 --> 00:19:43,500
by simply

333
00:19:43,520 --> 00:19:48,840
s basically is the two dimensional puzzle process whether it measures is given by something

334
00:19:48,840 --> 00:19:49,960
like this

335
00:19:49,980 --> 00:19:54,880
is the right measure has can be decomposed as a

336
00:19:54,940 --> 00:19:58,130
measure over state as it is it should be

337
00:19:58,150 --> 00:20:04,840
alpha HT theta and a measure over the email axis

338
00:20:04,860 --> 00:20:07,110
so if you look at this

339
00:20:07,130 --> 00:20:08,820
this bit here

340
00:20:09,380 --> 00:20:12,520
it kind of starts

341
00:20:13,610 --> 00:20:18,500
and it has mu mind meal to the minus one

342
00:20:18,520 --> 00:20:20,520
so it kind of

343
00:20:20,750 --> 00:20:22,040
i'm going to be

344
00:20:22,050 --> 00:20:25,070
draw this as this thing

345
00:20:28,980 --> 00:20:32,940
this is

346
00:20:32,960 --> 00:20:35,840
catholics this thing around so we usually

347
00:20:35,840 --> 00:20:38,380
the way we withdraw this function is

348
00:20:39,250 --> 00:20:42,110
that except that you take this and they around

349
00:20:42,170 --> 00:20:47,840
ninety degrees and goes to infinity here

350
00:20:47,920 --> 00:20:50,900
because of the mill race to the minus one

351
00:20:50,920 --> 00:20:56,250
so this process what this is is that it has

352
00:20:56,300 --> 00:21:02,000
and the total mass and under this curve is going to be infinite

353
00:21:02,130 --> 00:21:05,090
because if you try to integrate this thing from zero to one you gonna get

354
00:21:05,320 --> 00:21:10,340
infinity because it has lots of mass in this tale here is what we want

355
00:21:10,340 --> 00:21:14,650
to say is that if we draw from this to the person process this

356
00:21:14,670 --> 00:21:17,540
people know what suppose some process

357
00:21:17,550 --> 00:21:20,420
the probably most people know

358
00:21:20,420 --> 00:21:22,320
we're gonna get lots of

359
00:21:22,340 --> 00:21:24,320
points down on this axis

360
00:21:24,320 --> 00:21:25,500
on the

361
00:21:25,520 --> 00:21:27,650
on the x axis and a few points

362
00:21:27,670 --> 00:21:29,940
that's further away from the x axis

363
00:21:29,960 --> 00:21:31,920
car high up

364
00:21:33,550 --> 00:21:36,090
a draw from the beta process looks like that

365
00:21:36,150 --> 00:21:41,480
right so the

366
00:21:41,520 --> 00:21:43,050
beta process with

367
00:21:43,050 --> 00:21:47,170
this the parameter goes to one is the affinity measure of the IBP that we

368
00:21:47,170 --> 00:21:52,320
just described and when c is likely to one we have two parameter generalisation of

369
00:21:52,320 --> 00:21:56,480
you can find

370
00:23:03,610 --> 00:23:10,560
but it all

371
00:23:27,980 --> 00:23:29,770
one of

372
00:23:41,260 --> 00:23:42,580
on going

373
00:24:04,550 --> 00:24:06,130
well they

374
00:25:03,170 --> 00:25:06,750
and the problem of

375
00:25:46,750 --> 00:25:48,440
in i

376
00:26:09,270 --> 00:26:12,290
it is

377
00:26:35,790 --> 00:26:38,840
we can

378
00:26:38,840 --> 00:26:41,190
is it a highly specialized

379
00:26:41,250 --> 00:26:45,030
they typically optimized for particular now domain

380
00:26:45,070 --> 00:26:46,690
doesn't work very well

381
00:26:46,710 --> 00:26:48,820
and often work much better than humans

382
00:26:48,880 --> 00:26:50,610
now to my

383
00:26:51,260 --> 00:26:52,730
that operate

384
00:26:52,780 --> 00:26:56,980
although in fact the medical break them and go elsewhere

385
00:26:57,020 --> 00:26:58,520
so that's the kind of

386
00:26:58,530 --> 00:27:00,300
i think they want to achieve

387
00:27:01,420 --> 00:27:05,880
and i guess what i'm trying to do is just one component of intelligence

388
00:27:05,920 --> 00:27:07,440
we should be aiming for

389
00:27:07,480 --> 00:27:09,230
the ability to operate

390
00:27:09,280 --> 00:27:11,000
in many different domains

391
00:27:11,050 --> 00:27:14,880
and i'm not sure i would say that the total machines within of intelligence

392
00:27:14,920 --> 00:27:17,110
o will compensate the hand

393
00:27:17,130 --> 00:27:20,000
first of all i had to be doing

394
00:27:20,000 --> 00:27:23,090
ask how we did was well

395
00:27:23,130 --> 00:27:28,230
any casual when you when you when you see that kind of intelligence you recognise

396
00:27:28,280 --> 00:27:31,000
you just have a conversation with

397
00:27:35,860 --> 00:27:39,880
and that's the whole point we're going to come back to the end

398
00:27:46,030 --> 00:27:48,570
what i want to

399
00:27:50,300 --> 00:27:52,170
a quarter past made before

400
00:27:52,190 --> 00:27:53,360
what i might do was

401
00:27:53,380 --> 00:27:55,440
a little bit of history

402
00:27:55,480 --> 00:27:59,050
and and the end we get on this kind of questions that we just started

403
00:27:59,360 --> 00:28:02,710
how to control them is really important questions

404
00:28:07,110 --> 00:28:09,900
let me try to be a part of history

405
00:28:11,940 --> 00:28:13,650
of artificial intelligence

406
00:28:13,670 --> 00:28:15,690
this is a really rich story

407
00:28:16,280 --> 00:28:19,530
and there's a nice summary this sort of ten to fifteen page summary in in

408
00:28:19,530 --> 00:28:20,690
rosslyn all the

409
00:28:20,710 --> 00:28:22,020
lots of references

410
00:28:22,030 --> 00:28:23,900
and from a good follow-up

411
00:28:24,360 --> 00:28:28,690
of the details if you're interested so this just like the one i'm going do

412
00:28:28,690 --> 00:28:32,800
is just get appointed history as the first part of this impression of the brain

413
00:28:32,960 --> 00:28:35,110
and then

414
00:28:35,130 --> 00:28:36,940
and then i want to go into some of these

415
00:28:36,980 --> 00:28:39,090
philosophical questions

416
00:28:39,150 --> 00:28:43,030
but what is intelligence when we can do it all

417
00:28:43,030 --> 00:28:45,530
and what happens when we get there

418
00:28:45,630 --> 00:28:48,480
we still have control

419
00:28:49,820 --> 00:28:52,550
i suppose you could try say i back

420
00:28:52,550 --> 00:28:55,840
well i guess you can trace back to the greeks if you really want to

421
00:28:55,840 --> 00:28:56,570
go back

422
00:28:56,630 --> 00:28:58,170
presumably before that

423
00:28:58,190 --> 00:29:00,090
but the story starts

424
00:29:00,130 --> 00:29:02,000
in the forties so

425
00:29:02,000 --> 00:29:05,150
there's some work people call mcculloch and pitts

426
00:29:05,710 --> 00:29:07,710
nineteen forty

427
00:29:10,920 --> 00:29:15,340
i made a model of an artificial neural

428
00:29:15,340 --> 00:29:18,610
so the idea was that it was a mathematical model

429
00:29:18,690 --> 00:29:20,670
the try to model

430
00:29:20,690 --> 00:29:23,400
neurons in the human nervous system

431
00:29:23,500 --> 00:29:28,800
roughly speaking the idea is this is a bunch of neurons connected to one another

432
00:29:29,530 --> 00:29:32,340
a particular neuron can either on or off

433
00:29:32,380 --> 00:29:37,750
and it's on with this big enough signal coming from it's connected neurons

434
00:29:40,190 --> 00:29:42,880
the neighborhood neurons causes new under five

435
00:29:42,980 --> 00:29:45,280
and of course it causes the neurons to fire

436
00:29:45,280 --> 00:29:50,330
that's the basic mechanisms going around the time neurons are firing when something input gets

437
00:29:50,330 --> 00:29:51,570
to a certain level

438
00:29:51,590 --> 00:29:53,900
so the metamathematical model of this

439
00:29:54,020 --> 00:29:56,860
which is the start of new networks

440
00:29:58,400 --> 00:30:00,480
amongst other things

441
00:30:00,530 --> 00:30:03,820
they were able to show that they could live with the with the network of

442
00:30:04,110 --> 00:30:05,960
neurons artificial neurons

443
00:30:06,000 --> 00:30:07,170
they could model

444
00:30:07,170 --> 00:30:08,750
computable functions

445
00:30:08,780 --> 00:30:11,300
they can model and the northern not and so on

446
00:30:11,360 --> 00:30:13,860
so mathematically this had some nice properties

447
00:30:13,920 --> 00:30:16,840
was computation device in effect came from

448
00:30:17,650 --> 00:30:19,940
psychology cognitive science

449
00:30:21,940 --> 00:30:24,920
but i had some nice mathematical properties

450
00:30:24,940 --> 00:30:26,320
so that's one input

451
00:30:26,360 --> 00:30:27,980
the artificial neuron from

452
00:30:28,050 --> 00:30:31,980
mcculloch and pitts in this two famous these days because of their work

453
00:30:32,000 --> 00:30:32,750
the other

454
00:30:32,750 --> 00:30:34,400
famous input

455
00:30:34,820 --> 00:30:37,840
and this is more famous and almost everything else put together

456
00:30:37,920 --> 00:30:40,020
is a paper volunteering

457
00:30:40,030 --> 00:30:41,440
nineteen fifty

458
00:30:41,480 --> 00:30:43,070
in the journal mind

459
00:30:43,400 --> 00:30:46,320
the paper called the computing machinery and intelligence and this is

460
00:30:46,360 --> 00:30:49,460
the origin of the famous turing test which i'm going to come to

461
00:30:49,460 --> 00:30:50,860
a little bit later

462
00:30:50,940 --> 00:30:52,520
the theory was

463
00:30:52,530 --> 00:30:55,550
the guy guide say this in the second world war from

464
00:30:59,130 --> 00:31:01,280
the machines to the

465
00:31:01,340 --> 00:31:03,900
the germans cipher

466
00:31:03,900 --> 00:31:06,210
bletchley park

467
00:31:06,260 --> 00:31:09,230
the genius of the first order

468
00:31:09,280 --> 00:31:14,880
very great mathematician a very great magician like the contributions to logic

469
00:31:14,920 --> 00:31:19,500
and after the second world war having built and machines placed plug were

470
00:31:19,980 --> 00:31:21,710
to kind

471
00:31:23,380 --> 00:31:24,960
german signals

472
00:31:24,980 --> 00:31:27,550
he started thinking about

473
00:31:27,590 --> 00:31:30,610
with the machine can be intelligent

474
00:31:30,610 --> 00:31:32,590
and this is the one of the very first

475
00:31:32,590 --> 00:31:34,230
very explicit

476
00:31:35,730 --> 00:31:40,090
whether machines can be computers can be intelligent

477
00:31:40,150 --> 00:31:41,750
so he

478
00:31:41,750 --> 00:31:46,360
the rice festival raises the possibility of the nineteen fifty there were hardly any computers

479
00:31:46,420 --> 00:31:48,690
almost no one knew he was talking about in that sense

480
00:31:48,780 --> 00:31:52,280
just a very small number maybe half a dozen in the world

481
00:31:52,340 --> 00:31:55,070
it's like many more

482
00:31:55,110 --> 00:31:56,630
but already at that stage

483
00:31:56,650 --> 00:31:58,880
if another editor doing

484
00:31:58,900 --> 00:32:03,000
comparatively trivial calculations like ballistic calculations on solving

485
00:32:04,880 --> 00:32:06,710
could gram problems

486
00:32:06,760 --> 00:32:11,210
he was already thinking about the possibility that can become intelligent

487
00:32:11,210 --> 00:32:14,130
and in that paper he suggested the turing test

488
00:32:14,150 --> 00:32:18,300
which i'm going to come back to later

489
00:32:18,320 --> 00:32:21,530
OK a little bit later nineteen fifty six

490
00:32:21,630 --> 00:32:23,820
the some famous names

491
00:32:24,880 --> 00:32:26,170
that college

492
00:32:26,190 --> 00:32:27,320
this is

493
00:32:27,410 --> 00:32:30,840
a very famous meaning in fact fiftieth anniversary

494
00:32:31,280 --> 00:32:33,170
two hundred years ago

495
00:32:33,210 --> 00:32:35,940
so that the college after fifty years

496
00:32:35,960 --> 00:32:38,090
fifty years on from this meeting

497
00:32:40,130 --> 00:32:43,650
thought the occasion was so important that it was important they try to get many

498
00:32:43,650 --> 00:32:44,630
people back

499
00:32:44,650 --> 00:32:46,300
what is there before

500
00:32:46,320 --> 00:32:49,400
the original mating and had a big celebration

501
00:32:50,130 --> 00:32:52,320
the k nine there's john mccarthy

502
00:32:52,360 --> 00:32:56,360
one of the founders of a i say he was that the college at phd

503
00:32:56,400 --> 00:33:00,960
and he over the summer he got some people to come to mind

504
00:33:02,070 --> 00:33:07,020
to think about a nineteen fifty six five six years after the mine paper

505
00:33:07,030 --> 00:33:10,750
and here is the name some people should know some of them

506
00:33:10,760 --> 00:33:13,170
and if you do this you know the rest as you go on to say

507
00:33:13,280 --> 00:33:14,840
john mccarthy

508
00:33:15,230 --> 00:33:16,690
marvin minsky

509
00:33:17,500 --> 00:33:21,320
in some cases stands in miscues it might take

510
00:33:21,440 --> 00:33:22,520
claude shannon

511
00:33:22,520 --> 00:33:25,170
the founder of information theory

512
00:33:25,230 --> 00:33:30,050
one of the one of the great scientists of all time solomonoff probably these information

513
00:33:30,050 --> 00:33:32,380
theorist friends information theorist

514
00:33:33,730 --> 00:33:38,850
herbert simon won the nobel prize in economics and alan newell is famous computer scientist

515
00:33:39,880 --> 00:33:42,550
and i brought along a system called logic theorist

516
00:33:43,980 --> 00:33:45,340
newell and simon

517
00:33:45,380 --> 00:33:48,860
had themselves the theorem prover

518
00:33:48,940 --> 00:33:50,550
was that what this meeting so

519
00:33:51,230 --> 00:33:54,340
thought well i i would be a good idea actually never mind to begin with

520
00:33:54,340 --> 00:33:58,450
and they its transpose and everything is exactly right so this really is

521
00:33:59,100 --> 00:34:01,940
exactly the negative transpose the primal

522
00:34:03,770 --> 00:34:04,610
so first theorem

523
00:34:05,070 --> 00:34:08,080
which is such a trivial thing i like to word down next to it

524
00:34:09,210 --> 00:34:11,210
is that the dual of the dual is just the primal

525
00:34:12,710 --> 00:34:13,960
and that's why it's called the dual

526
00:34:14,950 --> 00:34:17,440
the dual the dual something else we would call it something else

527
00:34:23,610 --> 00:34:27,000
so the first theorem is that the dual the dual is the primal trivial the

528
00:34:27,000 --> 00:34:28,720
second there is also very simple

529
00:34:29,380 --> 00:34:31,700
end fundamentally important

530
00:34:35,250 --> 00:34:38,180
we have the current values of x

531
00:34:39,190 --> 00:34:41,400
there are feasible for the primal problem

532
00:34:42,090 --> 00:34:44,180
and somebody gives us values of why

533
00:34:44,950 --> 00:34:47,290
there are feasible for the dual problem

534
00:34:48,530 --> 00:34:49,530
then the current

535
00:34:49,980 --> 00:34:53,760
objective function for the primal see transpose those those x

536
00:34:54,210 --> 00:34:56,920
will be less than or equal to be transpose why

537
00:34:58,140 --> 00:35:00,870
feasible primal objective values are smaller than

538
00:35:01,640 --> 00:35:04,760
feasible dual objective values and the proof is really trivial

539
00:35:06,010 --> 00:35:10,610
you don't even have to remember it all you have to remember is start here

540
00:35:11,310 --> 00:35:14,290
right down why transpose x

541
00:35:15,390 --> 00:35:18,120
and the news the two assumptions x is feasible

542
00:35:18,530 --> 00:35:19,480
why is feasible

543
00:35:21,400 --> 00:35:23,810
let's try to do them on

544
00:35:27,810 --> 00:35:32,690
let's see here let's take the why transpose eight that's a real vector

545
00:35:33,670 --> 00:35:35,720
being multiplied by a column vector x

546
00:35:36,380 --> 00:35:40,320
let's take a real vector and right is a column vector transposed to make it

547
00:35:40,320 --> 00:35:44,870
a real vector so that is why transpose aiz eight transpose why transpose right

548
00:35:46,300 --> 00:35:47,720
but the dual problem

549
00:35:48,520 --> 00:35:51,600
eight transpose why is greater than or equal to see

550
00:35:52,540 --> 00:35:54,150
as part of dual feasibility

551
00:35:55,210 --> 00:35:57,890
and part primal feasibility is all the become

552
00:35:58,400 --> 00:36:00,260
all the numbers agreement zero

553
00:36:00,890 --> 00:36:02,240
so this is such

554
00:36:02,880 --> 00:36:03,790
summing up

555
00:36:06,030 --> 00:36:10,620
the elements of this vector multiplied by elements in this vector these things are all

556
00:36:12,460 --> 00:36:16,090
and so if i can put smaller values for these and make the whole some

557
00:36:16,090 --> 00:36:20,860
smaller and smaller values of the coefficients in this and see because this inequality

558
00:36:21,250 --> 00:36:24,440
so see transpose x is less the why transpose x

559
00:36:25,420 --> 00:36:29,030
we're doing exactly the same thing on the other side this equality

560
00:36:30,590 --> 00:36:31,810
we just say look okay

561
00:36:32,560 --> 00:36:35,860
i can rewrite why transpose x is x transpose why

562
00:36:36,860 --> 00:36:38,720
obvious but any x

563
00:36:39,770 --> 00:36:41,270
is less than or equal to be

564
00:36:41,850 --> 00:36:43,560
that's part of the primal feasibility

565
00:36:44,250 --> 00:36:47,720
so the same idea all over again we've got this is less than or equal

566
00:36:47,780 --> 00:36:50,190
something and all these things are non-negative therefore though

567
00:36:50,700 --> 00:36:52,820
the whole sum is less than or equal to

568
00:36:53,400 --> 00:36:55,510
those coefficients times these was so

569
00:36:56,140 --> 00:36:59,080
is completely trivial we've used all of these

570
00:36:59,530 --> 00:37:04,650
conditions ov feasibility we use all the conditions the primal feasibility here and here

571
00:37:05,270 --> 00:37:09,810
we use all the conditions for dual feasibility here here so we have an under-used

572
00:37:09,820 --> 00:37:12,450
are over-used anything we used everything exactly once

573
00:37:13,090 --> 00:37:15,180
and we get this nice result that

574
00:37:15,670 --> 00:37:19,610
the primal objective must be smaller than or equal to the dual objective

575
00:37:22,440 --> 00:37:25,490
so we can look at this on a on a plot and just the real

576
00:37:25,490 --> 00:37:28,650
line we just plot objective function values

577
00:37:29,110 --> 00:37:31,550
if i plot the primal values in purple here

578
00:37:32,020 --> 00:37:36,840
that gives me some interval values this interval might extend to minus infinity

579
00:37:38,840 --> 00:37:40,590
and if we plot the dual values

580
00:37:41,640 --> 00:37:44,840
we get another shown green which could extend plus infinity

581
00:37:46,180 --> 00:37:49,490
but all agreed values must be bigger than all the purple values

582
00:37:50,010 --> 00:37:53,530
and the one of the fundamental questions is is there a gap

583
00:37:54,590 --> 00:37:55,980
is this possible or not

584
00:37:56,550 --> 00:38:00,390
where is that as the situation always like this this will be a nice theorem

585
00:38:00,650 --> 00:38:05,400
if you could prove that there's never a gap between the primal and the dual

586
00:38:07,690 --> 00:38:12,330
the answer to this question will come from the strong duality theorem which will have shortly

587
00:38:14,110 --> 00:38:15,550
and it's not quite trivial

588
00:38:22,410 --> 00:38:27,550
now let's think about primal simplex methods in the context of the dual problem

589
00:38:29,310 --> 00:38:30,300
so we have

590
00:38:31,780 --> 00:38:33,170
a primal problem here it is

591
00:38:34,140 --> 00:38:35,210
i write down its dual

592
00:38:35,840 --> 00:38:38,160
so i will put the tools version with which

593
00:38:38,630 --> 00:38:41,920
uses dual notation so i've got wise here

594
00:38:42,440 --> 00:38:46,580
i guess he's ok so on the dual side always uses e for the dual

595
00:38:46,930 --> 00:38:48,630
slacks and inequalities

596
00:38:50,930 --> 00:38:55,600
so in the primal axes and w and do i always have wise enzi's

597
00:38:56,520 --> 00:39:01,550
and the dual is exactly the negative transpose so the to the the minus three

598
00:39:01,550 --> 00:39:04,080
to one become a three minus to minus one

599
00:39:04,690 --> 00:39:06,780
zero three becomes a zero minus three

600
00:39:07,220 --> 00:39:12,670
and this matrix this to a three matrix becomes this three by two matrix which is the

601
00:39:13,260 --> 00:39:14,700
negative transpose

602
00:39:15,450 --> 00:39:19,080
now let's do the simplex method on the primal problem more feasible so i can

603
00:39:19,080 --> 00:39:23,360
do the primal simplex methods and see what happens in this watch what happens to

604
00:39:23,360 --> 00:39:24,150
the dual problem

605
00:39:24,630 --> 00:39:25,160
so i think

606
00:39:25,630 --> 00:39:27,380
x to be my entering variable

607
00:39:28,830 --> 00:39:32,560
and w two has to be misleading variables actually my only choice in this case

608
00:39:33,150 --> 00:39:34,140
and we'll see what happens

609
00:39:35,340 --> 00:39:38,340
so after i do that here's the new primal dictionary

610
00:39:39,760 --> 00:39:40,830
here's the coefficients

611
00:39:45,820 --> 00:39:52,660
if i do exactly analogous to that in the dual problem so i clicked on the uh

612
00:39:54,280 --> 00:39:56,050
the middle column and the

613
00:39:56,530 --> 00:39:57,280
last throw

614
00:39:57,970 --> 00:40:02,630
if i click on the last column in the middle road that's this white to over here

615
00:40:03,280 --> 00:40:04,940
that's this this but here

616
00:40:05,360 --> 00:40:05,730
is that

617
00:40:06,380 --> 00:40:09,640
transpose slot corresponding to the x two but here

618
00:40:10,340 --> 00:40:12,600
was there is the fourier as the minus forty here

619
00:40:13,010 --> 00:40:17,380
this is this button corresponds there but if i click on this button i can

620
00:40:17,380 --> 00:40:21,930
do the algebra it doesn't look like the simplex method because i'm not even feasible

621
00:40:22,240 --> 00:40:26,440
nothing i said befor makes any sense for clicking on this button

622
00:40:27,130 --> 00:40:30,010
but i can do it i could do the algebraic in these equations

623
00:40:30,970 --> 00:40:35,230
and and and and see what i get so i can click on them but

624
00:40:35,300 --> 00:40:36,720
there are in this button here

625
00:40:36,720 --> 00:40:37,810
p of x at time t

626
00:40:38,370 --> 00:40:44,620
given my if i met some values x zero times so basically we interested in

627
00:40:44,620 --> 00:40:45,970
this transition

628
00:40:46,040 --> 00:40:48,300
PDF which defines this

629
00:40:48,310 --> 00:40:50,190
and basically

630
00:40:50,190 --> 00:40:53,120
one can show that

631
00:40:53,150 --> 00:40:54,570
the equations that

632
00:40:54,590 --> 00:40:58,960
the government the evolution of the transition probability

633
00:40:58,990 --> 00:41:01,100
is is p

634
00:41:01,120 --> 00:41:04,710
second what's just dt by p on the

635
00:41:04,720 --> 00:41:09,370
the on the left here and we get terms from the drift

636
00:41:09,400 --> 00:41:11,450
and also from the diffusion

637
00:41:11,560 --> 00:41:15,410
a second order derivatives here with respect to the

638
00:41:15,440 --> 00:41:17,150
the diffusion terms of first

639
00:41:17,170 --> 00:41:20,590
during his respect interests basically what we want to do

640
00:41:20,600 --> 00:41:22,820
is we might want to understand

641
00:41:22,840 --> 00:41:26,030
even this evolution of this transition probability

642
00:41:26,030 --> 00:41:28,460
in terms of fokker plank equation

643
00:41:28,510 --> 00:41:33,290
so basically have to derive this is just just sort of continuous time version of

644
00:41:33,320 --> 00:41:37,400
chapman kolmogorov equation basically we have some probability

645
00:41:39,510 --> 00:41:42,030
time exterior zero what

646
00:41:42,050 --> 00:41:45,820
at time t zero we consider all possible paths to get to

647
00:41:45,970 --> 00:41:49,970
how we can be some next time to

648
00:41:50,030 --> 00:41:54,390
and so this is actually the forward fokker planck equation and is also a backward

649
00:41:54,390 --> 00:41:58,530
fokker planck equation as well

650
00:41:58,540 --> 00:41:59,800
so yes

651
00:42:04,140 --> 00:42:06,090
that sounds bad OK

652
00:42:06,210 --> 00:42:09,280
i guess the square bracket is wrong

653
00:42:10,590 --> 00:42:11,860
so yes

654
00:42:20,400 --> 00:42:28,570
i think because deriving it from the jam content of questionable it is actually it's

655
00:42:28,570 --> 00:42:32,100
guaranteed to do that right you have new building in that condition that's how you

656
00:42:32,100 --> 00:42:34,650
actually driving this thing is actually

657
00:42:36,560 --> 00:42:41,400
let me give you a simple example very simple example of a fokker planck equation

658
00:42:41,420 --> 00:42:45,590
if we just have to be in process with scaling and drift then we'll have

659
00:42:46,150 --> 00:42:50,950
i mean we said this was the stochastic form solution

660
00:42:50,970 --> 00:42:54,900
is then very easy to see that the mean the mean value of this is

661
00:42:55,130 --> 00:42:57,850
the time t is just x zero plus a

662
00:42:57,880 --> 00:42:59,160
and the public

663
00:42:59,200 --> 00:43:00,850
variance in fact which is

664
00:43:00,870 --> 00:43:02,520
going is

665
00:43:02,580 --> 00:43:08,900
sigma squared t OK so this is just because the gaussians distribution at time t

666
00:43:08,900 --> 00:43:13,340
with mean exterior plus eighty and this variance is the square c

667
00:43:13,920 --> 00:43:15,780
three simple example

668
00:43:15,790 --> 00:43:19,650
i think that perhaps the main

669
00:43:19,670 --> 00:43:21,010
o point where

670
00:43:21,010 --> 00:43:24,540
fokker planck equations have a particular value

671
00:43:24,580 --> 00:43:26,070
is in relation to

672
00:43:26,090 --> 00:43:27,230
when we have

673
00:43:27,230 --> 00:43:29,660
certain boundary conditions for

674
00:43:29,690 --> 00:43:32,320
this speedy is for example

675
00:43:32,340 --> 00:43:36,300
one problem which relates to to neural computation is that

676
00:43:36,300 --> 00:43:37,960
if we start off with some

677
00:43:40,630 --> 00:43:45,140
time zero here maybe and we can think of this is likely the resting potential

678
00:43:45,140 --> 00:43:51,340
the neuron after firing we drive it with some noisy input signal and the potential

679
00:43:51,360 --> 00:43:56,650
so in response to that maybe for the wiener process then we might be interested

680
00:43:56,650 --> 00:43:57,550
in when

681
00:43:57,550 --> 00:44:02,650
that potential crosses the threshold for firing neuron would be interested in this passage time

682
00:44:02,650 --> 00:44:05,190
here but when this actually happens OK

683
00:44:06,130 --> 00:44:09,710
in these kinds of conditions i'm looking at

684
00:44:09,710 --> 00:44:12,190
one thing we can do course then is that we will be interested in the

685
00:44:12,190 --> 00:44:18,800
first passage times density is essentially what's the probability distribution function of time when this

686
00:44:18,820 --> 00:44:20,190
when this process

687
00:44:20,190 --> 00:44:23,650
some business process first up across the

688
00:44:23,650 --> 00:44:26,380
this disparity this very

689
00:44:26,860 --> 00:44:33,270
but we can also be interested in maybe if we were just interested in the

690
00:44:33,300 --> 00:44:34,920
probability distribution

691
00:44:34,920 --> 00:44:40,480
at each of you know at some particular time here of what the what the

692
00:44:40,500 --> 00:44:41,920
value of x is

693
00:44:42,420 --> 00:44:46,000
at some time which essentially what we have to do is kind of siphon off

694
00:44:46,000 --> 00:44:47,320
all the probability

695
00:44:47,320 --> 00:44:51,920
there's already crossed here all the positive already crossed we normalize look at that

696
00:44:55,250 --> 00:44:56,070
of these

697
00:44:56,090 --> 00:44:58,520
of the process is sometimes

698
00:45:00,340 --> 00:45:02,400
then we we can obtain this through

699
00:45:02,420 --> 00:45:04,880
fokker plank kind of analysis basically

700
00:45:04,900 --> 00:45:09,730
fokker planck stuff depends crucially on the markov properties of these processes right it's the

701
00:45:09,730 --> 00:45:17,270
mark of structure the chapman kolmogorov equations actually allow us to do this analysis

702
00:45:17,290 --> 00:45:18,730
and so the various

703
00:45:19,250 --> 00:45:22,590
so you can you see that this is not a radically new

704
00:45:22,610 --> 00:45:27,190
there go back to fellow in nineteen fifty two one can characterize various

705
00:45:27,190 --> 00:45:33,820
there's kind of boundary conditions can be interesting make have absorbing barriers we could have

706
00:45:33,820 --> 00:45:37,440
for example of collecting there is maybe could track this particle

707
00:45:37,500 --> 00:45:40,560
in between two barriers here be interested in evolution

708
00:45:40,570 --> 00:45:41,920
of the

709
00:45:41,980 --> 00:45:46,920
from the transition probability as a function of time so some other conditions here as

710
00:45:48,590 --> 00:45:56,020
well the thing that was mentioned in the introduction of course is that we

711
00:45:57,500 --> 00:45:59,290
described a lot about

712
00:45:59,340 --> 00:46:01,630
how we can actually

713
00:46:01,630 --> 00:46:07,400
simulation this process and understand properties of one thing that certainly as machine learning might

714
00:46:07,400 --> 00:46:08,500
well want to do

715
00:46:08,500 --> 00:46:09,730
it is to actually

716
00:46:09,750 --> 00:46:13,020
two maybe to do some parameter estimation

717
00:46:13,040 --> 00:46:18,520
and in the system in the simple case actually fairly straightforward because

718
00:46:18,520 --> 00:46:21,400
if we have some observations of various times

719
00:46:21,460 --> 00:46:23,040
then the

720
00:46:23,070 --> 00:46:27,320
the key property that actually makes casting process to work nice to work with in

721
00:46:27,320 --> 00:46:31,790
respect is a parameter estimation is we can actually just write down a likelihood of

722
00:46:31,790 --> 00:46:33,150
these observations

723
00:46:33,170 --> 00:46:34,840
given the parameters

724
00:46:34,860 --> 00:46:36,440
this is just

725
00:46:36,460 --> 00:46:40,690
this is just a sort of multivariate yes it was just basically saying

726
00:46:40,710 --> 00:46:42,400
i've got some covariance

727
00:46:42,400 --> 00:46:46,480
matrix corresponding to these

728
00:46:46,520 --> 00:46:50,440
he's he's and different points x one and to x t i i can look

729
00:46:50,440 --> 00:46:52,690
at that covariance matrix got me

730
00:46:52,730 --> 00:46:56,540
i got some covariance i really got it just one data point

731
00:46:56,540 --> 00:46:58,500
and then dimensional data datapoints

732
00:46:58,500 --> 00:47:00,270
in that

733
00:47:00,320 --> 00:47:06,190
five thousand someone trying to do is to change the preferences of the gaussians so

734
00:47:06,190 --> 00:47:07,690
as to

735
00:47:07,710 --> 00:47:10,480
to maximize the likelihood that

736
00:47:10,900 --> 00:47:15,000
these in in the simple case that's actually a misconception is fairly straightforward one can

737
00:47:15,000 --> 00:47:18,920
that's essentially probabilities are intrinsic properties of

738
00:47:18,940 --> 00:47:27,810
natural objects say so dice as an intrinsic property and in probability of landing on

739
00:47:27,880 --> 00:47:33,480
one or two or something and this intrinsic property can be measured by experiments may

740
00:47:33,480 --> 00:47:37,480
be in the frequentist way you use for the next over time

741
00:47:37,480 --> 00:47:42,310
and at the other end of the spectrum there are the subjectivist will say

742
00:47:42,320 --> 00:47:47,000
probably do not probabilities do not exist at all is just something that is in

743
00:47:47,000 --> 00:47:49,090
your mind is just

744
00:47:49,840 --> 00:47:54,670
i believe that you have about how likely the dice is to land up

745
00:47:54,750 --> 00:47:56,590
two and to land on

746
00:47:56,610 --> 00:47:58,920
the specific value

747
00:48:01,650 --> 00:48:02,670
OK so

748
00:48:02,690 --> 00:48:05,610
the next slide give more details about this and

749
00:48:05,650 --> 00:48:11,840
i already told you so the frequentist interpretation is that you measure how many times

750
00:48:11,840 --> 00:48:13,840
the sum ever occurred

751
00:48:13,860 --> 00:48:15,020
as the number of

752
00:48:15,040 --> 00:48:19,310
as a function of the number of times due to repeated the experiment and you

753
00:48:19,310 --> 00:48:21,730
take the limit of this ratio

754
00:48:21,750 --> 00:48:24,210
as the definition for the probability

755
00:48:24,230 --> 00:48:27,730
the problem with this approach is that you often can not

756
00:48:27,750 --> 00:48:32,790
do a repetitions of your experiment so for example you

757
00:48:32,810 --> 00:48:35,210
if i tell you

758
00:48:35,230 --> 00:48:39,750
what is the probability that the sun will rise tomorrow

759
00:48:41,650 --> 00:48:44,290
sixteen of february

760
00:48:45,940 --> 00:48:50,040
you cannot the sixteenth of february two thousand six there is only one such state

761
00:48:50,040 --> 00:48:53,840
so you cannot repeat this experiment

762
00:48:53,860 --> 00:48:58,900
so you cannot really define what is the limiting problem limiting frequency of the sun

763
00:48:58,900 --> 00:49:03,520
rising on this specific day

764
00:49:03,590 --> 00:49:08,190
so now

765
00:49:08,190 --> 00:49:13,090
you can still say well even though i cannot measure it maybe still exists this

766
00:49:13,090 --> 00:49:17,940
probability of the sun rising tomorrow may be in the intrinsic property of the sun

767
00:49:17,940 --> 00:49:19,980
itself that's

768
00:49:20,000 --> 00:49:22,290
tomorrow it will rise or not

769
00:49:22,310 --> 00:49:26,310
so that's a matter of

770
00:49:27,310 --> 00:49:31,150
believing that the physical system itself has this property

771
00:49:31,190 --> 00:49:32,420
and some people

772
00:49:32,440 --> 00:49:35,040
do not want to make this

773
00:49:35,090 --> 00:49:38,380
i do not want to believe in that and the rather save

774
00:49:38,400 --> 00:49:43,920
well probabilities are not something the probability that the sun rises tomorrow is not something

775
00:49:43,920 --> 00:49:48,230
property of the sun about the property of someone talking about it

776
00:49:50,210 --> 00:49:53,040
it will not let me

777
00:49:54,630 --> 00:49:58,750
well the right

778
00:50:05,360 --> 00:50:07,130
an interesting thing is that

779
00:50:07,150 --> 00:50:11,810
we have seen these properties these axioms of probability and it happens that if you

780
00:50:12,210 --> 00:50:14,900
try to formalize what it means to believe

781
00:50:14,920 --> 00:50:16,880
with some degree

782
00:50:16,880 --> 00:50:21,380
in some evidence so you try to define a function

783
00:50:21,400 --> 00:50:25,420
the belief function that assigns number two events

784
00:50:25,420 --> 00:50:29,290
and you try to ensure that this function

785
00:50:30,170 --> 00:50:35,810
kind of rational corresponds to natural intuition about how you would combine

786
00:50:35,840 --> 00:50:37,400
belief degrees

787
00:50:37,420 --> 00:50:38,690
but if you do do that

788
00:50:38,690 --> 00:50:45,400
you will end up with the same actions as the one satisfied by probabilities so

789
00:50:45,420 --> 00:50:47,610
you could very well say

790
00:50:47,630 --> 00:50:49,090
degrees of belief

791
00:50:49,110 --> 00:50:54,250
satisfied the same problem properties as probabilities so maybe probabilities are just degrees of belief

792
00:50:54,290 --> 00:50:58,340
i mean there's no no way to tell between them you know

793
00:50:58,380 --> 00:51:04,610
another thing about probabilities

794
00:51:04,670 --> 00:51:06,980
the famous bayes rule

795
00:51:08,460 --> 00:51:13,650
from the previous sections and from the definition of conditional probability you can derive

796
00:51:13,690 --> 00:51:17,000
this rule that tell you how you should update

797
00:51:17,000 --> 00:51:19,230
probabilities are degrees of belief

798
00:51:19,250 --> 00:51:20,790
whatever they are

799
00:51:20,810 --> 00:51:27,400
so whether you're objectivist or frequentist or subjectivist you have certain probabilities

800
00:51:27,460 --> 00:51:29,090
before observing the data

801
00:51:29,130 --> 00:51:33,710
you observe some data and you want to compute the new probabilities after having observed

802
00:51:33,710 --> 00:51:35,060
the data

803
00:51:35,080 --> 00:51:37,610
well the natural way is to use

804
00:51:37,630 --> 00:51:39,810
this bayes rule

805
00:51:39,860 --> 00:51:43,980
OK the only thing is that often

806
00:51:44,000 --> 00:51:46,230
the prior probabilities

807
00:51:46,770 --> 00:51:53,880
cannot be defined in a frequentist way like the prior probability that the data is

808
00:51:53,880 --> 00:51:56,770
generated by some function

809
00:51:56,790 --> 00:52:00,110
cannot be defined in the way that you can measure the frequency

810
00:52:00,130 --> 00:52:02,340
like you cannot repeat

811
00:52:02,810 --> 00:52:07,520
your experiment usually in learning problems you have a learning problem you can

812
00:52:07,560 --> 00:52:11,750
you can maybe get more samples but you cannot get more problems you you just

813
00:52:11,750 --> 00:52:13,250
that one problem

814
00:52:13,270 --> 00:52:19,590
so in this sense the probability that the function that the target function is some

815
00:52:19,590 --> 00:52:20,880
the function h

816
00:52:21,650 --> 00:52:25,790
it is more degree of belief that frequency

817
00:52:25,820 --> 00:52:27,980
OK but it doesn't matter because

818
00:52:28,000 --> 00:52:31,230
these rules they just make sense in himself in the way

819
00:52:35,460 --> 00:52:39,360
but it doesn't mean that because this will make sense

820
00:52:39,380 --> 00:52:40,480
using them

821
00:52:40,500 --> 00:52:42,060
i will give you

822
00:52:42,060 --> 00:52:44,440
good induction system or rather

823
00:52:44,500 --> 00:52:46,750
it doesn't mean it doesn't imply

824
00:52:46,790 --> 00:52:48,980
that you can

825
00:52:49,080 --> 00:52:52,650
justify using these rules

826
00:52:52,670 --> 00:52:55,110
and that's what i see here

827
00:52:55,170 --> 00:52:59,590
it doesn't prove anything but you have used probabilities so if someone comes and tell

828
00:52:59,610 --> 00:53:04,940
you well i have a very nice algorithms for learning that i derived from the

829
00:53:04,940 --> 00:53:11,290
very rigorous statements in probability theory and they have use bayes rule

830
00:53:11,290 --> 00:53:16,920
and they have used this and this property of probabilities so it should work well

831
00:53:17,130 --> 00:53:19,310
that's not the case

832
00:53:20,310 --> 00:53:23,790
it's just a nice way of reasoning

833
00:53:23,810 --> 00:53:26,250
but it doesn't prove anything

834
00:53:26,250 --> 00:53:28,550
see other cover set at this level

835
00:53:28,570 --> 00:53:32,870
and you have to compute the distance from here to here it is in upper

836
00:53:32,870 --> 00:53:35,860
bound of the system and its neighbour because

837
00:53:35,910 --> 00:53:42,170
in the worst case the root is the nearest neighbour

838
00:53:42,190 --> 00:53:43,500
if you go down the tree

839
00:53:44,820 --> 00:53:45,860
at this level

840
00:53:45,870 --> 00:53:48,050
of the tree

841
00:53:48,060 --> 00:53:51,980
we can always do is occurrences can start eliminating

842
00:53:52,010 --> 00:53:54,540
as we go down eliminating branches

843
00:53:54,560 --> 00:53:57,590
but we can eliminate this one yet

844
00:53:57,690 --> 00:54:01,860
up and down the tree still of a great deal from here

845
00:54:01,860 --> 00:54:03,030
to here

846
00:54:03,470 --> 00:54:04,970
but the scale

847
00:54:04,980 --> 00:54:05,920
if you recall

848
00:54:05,980 --> 00:54:12,890
into the back

849
00:54:12,930 --> 00:54:15,520
so we have this scale

850
00:54:15,530 --> 00:54:17,630
so this we can canada to

851
00:54:20,550 --> 00:54:21,460
the children

852
00:54:21,480 --> 00:54:25,070
but there's no could be at least this by the way

853
00:54:25,080 --> 00:54:25,930
and then

854
00:54:26,250 --> 00:54:31,640
that children could be this far

855
00:54:31,670 --> 00:54:34,910
and then the children can be this far away

856
00:54:34,920 --> 00:54:37,430
and keep and an exponential series

857
00:54:37,440 --> 00:54:41,250
which actually get you closer and this upper bound

858
00:54:41,260 --> 00:54:45,020
so that means you can eliminate

859
00:54:45,640 --> 00:54:51,880
in one this note here

860
00:54:51,930 --> 00:54:53,920
because one of his descendants

861
00:54:54,000 --> 00:54:57,600
might be nearest neighbour query point

862
00:54:57,610 --> 00:55:01,950
and when you go one well well well down you can eliminate

863
00:55:01,970 --> 00:55:06,780
to this guy cannot descendants which are national average

864
00:55:06,790 --> 00:55:10,040
so in these two satellite possibilities

865
00:55:10,090 --> 00:55:13,230
these people walking down the tree

866
00:55:13,600 --> 00:55:17,570
and only these three can be like possibilities

867
00:55:17,640 --> 00:55:21,200
and now

868
00:55:21,210 --> 00:55:24,870
the only two possibilities left and at the lowest level of the tree

869
00:55:24,870 --> 00:55:36,100
see just you brute force amongst these two elements

870
00:55:38,820 --> 00:55:40,600
everyone i make a cover tree

871
00:55:40,640 --> 00:55:42,630
is actually several different ways

872
00:55:42,640 --> 00:55:46,610
one is you can do single point insertion so you can you can insert

873
00:55:46,620 --> 00:55:49,540
given the cover tree you can insert a new point into it

874
00:55:49,580 --> 00:55:53,080
you will see that the same way as you do require you walk down the

875
00:55:53,860 --> 00:55:58,420
making this cover that with the semantics is different it's the set of elements

876
00:55:59,180 --> 00:56:02,210
will lead to the parent of the new point

877
00:56:02,260 --> 00:56:04,040
is in doubt

878
00:56:04,060 --> 00:56:08,140
and then you insert when you find the right point

879
00:56:08,150 --> 00:56:11,280
it about construction c

880
00:56:11,280 --> 00:56:14,640
was little here but

881
00:56:14,660 --> 00:56:16,420
you can do it only once

882
00:56:16,870 --> 00:56:20,980
it's a little bit faster period we you can prove any

883
00:56:21,000 --> 00:56:23,610
there's no improvement theoretically

884
00:56:23,630 --> 00:56:27,470
in addition you easy construction this is maybe the most it's interesting one

885
00:56:27,520 --> 00:56:30,680
so now

886
00:56:30,710 --> 00:56:34,320
we do you just say OK at pool of points

887
00:56:34,320 --> 00:56:36,350
and then get one query

888
00:56:37,600 --> 00:56:40,210
figuring out

889
00:56:40,270 --> 00:56:43,660
one is there is is good

890
00:56:55,830 --> 00:56:57,350
he points

891
00:56:57,370 --> 00:56:58,220
which is

892
00:56:59,040 --> 00:57:02,580
and a query

893
00:57:02,590 --> 00:57:09,870
you kind of organised things but the distance to the query

894
00:57:09,910 --> 00:57:11,920
and then you can construct

895
00:57:11,940 --> 00:57:17,090
essentially and i think that the query is being the root of the cover tree

896
00:57:17,310 --> 00:57:23,070
so i think the queries being the root

897
00:57:23,110 --> 00:57:26,060
then you associate

898
00:57:26,070 --> 00:57:30,300
all the points in the set which are nearest to the the

899
00:57:30,310 --> 00:57:31,520
this note here

900
00:57:31,540 --> 00:57:35,530
i wanted to look for the way this now runs all the way there

901
00:57:35,550 --> 00:57:39,520
and so forth going out

902
00:57:39,720 --> 00:57:44,720
and then new query

903
00:57:44,750 --> 00:57:47,420
maybe it's going to be this point

904
00:57:47,450 --> 00:57:48,280
and then

905
00:57:48,280 --> 00:57:52,340
if you take some of the point associated with this no associate them

906
00:57:52,350 --> 00:57:56,620
with this new the ones which are closest ceos kind of meeting the point issue

907
00:57:56,670 --> 00:57:59,320
with equations critical says to

908
00:58:00,000 --> 00:58:03,830
and then you can put some basic things you can prove it

909
00:58:05,760 --> 00:58:09,350
this construction is fully amateur is we just entering

910
00:58:14,200 --> 00:58:19,690
so a node is the query

911
00:58:19,720 --> 00:58:23,160
the test points to the nearest query nodes

912
00:58:23,180 --> 00:58:25,710
prove the answering queries

913
00:58:25,720 --> 00:58:28,110
requires more distance evaluations then

914
00:58:28,700 --> 00:58:29,690
the structure

915
00:58:32,420 --> 00:58:35,760
so if you about about the integration time

916
00:58:35,760 --> 00:58:38,500
you can set the set so that

917
00:58:38,930 --> 00:58:48,440
it is never going to be seen if the worse than brute force

918
00:58:50,180 --> 00:58:54,550
this is the insertion of them

919
00:58:54,550 --> 00:58:56,020
michael skip

920
00:58:56,040 --> 00:59:01,250
it's basically it's

921
00:59:03,870 --> 00:59:04,860
one thing which is

922
00:59:04,870 --> 00:59:08,290
it is an interesting about the size of the campus so well

923
00:59:08,310 --> 00:59:11,590
so for the insertion of an can prove the size the campus it's going to

924
00:59:11,590 --> 00:59:13,010
be see the fourth

925
00:59:13,020 --> 00:59:24,860
the crime rate slightly larger it see the fifth

926
00:59:24,860 --> 00:59:28,890
the whole here the first lecture

927
00:59:28,890 --> 00:59:33,080
can i have

928
00:59:35,450 --> 00:59:38,470
will back

929
00:59:38,470 --> 00:59:43,950
text mining information and fact extraction

930
00:59:47,570 --> 00:59:51,580
i am

931
00:59:53,430 --> 01:00:02,830
b and i might change my

932
01:00:03,040 --> 01:00:06,540
almost surely

933
01:00:06,540 --> 01:00:08,440
in the math

934
01:00:13,750 --> 01:00:20,760
so what about the rest the lack

935
01:00:20,820 --> 01:00:23,570
the school for

936
01:00:23,580 --> 01:00:25,630
but of course they don't

937
01:00:30,070 --> 01:00:34,880
so what

938
01:00:34,880 --> 01:00:43,110
if could write in a whole

939
01:00:48,050 --> 01:00:51,800
well i'm very few

940
01:00:53,000 --> 01:00:58,100
because we have to define the type of steel have been talking about

941
01:00:58,100 --> 01:00:59,800
first of all thanks

942
01:01:00,960 --> 01:01:03,330
we think about things we have need

943
01:01:03,330 --> 01:01:07,540
alan it so are called in natural language

944
01:01:07,580 --> 01:01:09,100
and so the

945
01:01:09,110 --> 01:01:11,740
elements of the linguistic

946
01:01:11,750 --> 01:01:16,790
which we could use it for information extraction or take my

947
01:01:16,830 --> 01:01:24,040
and also important we text tries to accomplish certain communication it's

948
01:01:24,720 --> 01:01:28,240
you interpretation in common gate ac

949
01:01:28,260 --> 01:01:31,890
now we will hear more on right

950
01:01:31,950 --> 01:01:36,740
of course you can do all the information extraction and text mining

951
01:01:36,770 --> 01:01:38,800
OK context speech

952
01:01:38,800 --> 01:01:42,680
our time all of the knowledge

953
01:01:42,700 --> 01:01:47,170
so we need have to take into account the area of transcription

954
01:01:48,830 --> 01:01:49,710
but here

955
01:01:49,730 --> 01:01:51,230
we speak about

956
01:01:51,310 --> 01:01:53,430
they take

957
01:01:53,450 --> 01:02:01,930
now make it all kinds of the john that so you have

958
01:02:02,300 --> 01:02:04,370
well the text

959
01:02:04,390 --> 01:02:06,020
like the news

960
01:02:07,730 --> 01:02:09,700
well maybe not really

961
01:02:09,710 --> 01:02:13,580
you know the greek but the

962
01:02:13,590 --> 01:02:16,830
we have a new which are usually one four

963
01:02:16,870 --> 01:02:19,300
to the right

964
01:02:21,260 --> 01:02:23,430
this is in fact a very

965
01:02:23,860 --> 01:02:26,800
that was very well for

966
01:02:26,800 --> 01:02:28,170
on the contrary

967
01:02:28,180 --> 01:02:29,810
we had block

968
01:02:29,830 --> 01:02:33,330
on the web and is some of range blocks

969
01:02:33,330 --> 01:02:35,230
and if you know french

970
01:02:35,240 --> 01:02:39,520
then you need to really it should be hard work

971
01:02:39,530 --> 01:02:41,050
someone like care

972
01:02:41,060 --> 01:02:45,740
because really the language

973
01:02:46,700 --> 01:02:53,370
contrary to what some call a lot of excellent application that use and also this

974
01:02:53,370 --> 01:02:54,230
in this

975
01:02:54,300 --> 01:02:57,340
tactical for completely

976
01:02:57,390 --> 01:03:01,760
and we have we have emailed we have to take

977
01:03:02,030 --> 01:03:07,960
like his medical encyclopedia by members a group of people

978
01:03:07,960 --> 01:03:10,150
there were two to one

979
01:03:10,170 --> 01:03:13,610
the MIT and text text it

980
01:03:13,620 --> 01:03:15,680
we have affecting many

981
01:03:16,430 --> 01:03:18,480
many forms

982
01:03:18,490 --> 01:03:20,360
now if you want to my

983
01:03:20,370 --> 01:03:22,560
mind the

984
01:03:22,580 --> 01:03:24,020
to gain access

985
01:03:24,080 --> 01:03:27,930
on to the track comes source

986
01:03:30,610 --> 01:03:33,890
ambiguity is somehow and we heard

987
01:03:33,930 --> 01:03:41,060
which personally i think prefer the first of all in the mind

988
01:03:41,200 --> 01:03:47,870
information extraction find information in the text

989
01:03:47,880 --> 01:03:52,410
on the other hand once you have extracted from each

990
01:03:52,410 --> 01:03:53,530
different space

991
01:03:54,530 --> 01:03:57,260
so you can see that in fact if you look at PCA

992
01:03:57,300 --> 01:04:02,700
it is nothing but another optimisation problems are you trying to do is

993
01:04:02,740 --> 01:04:05,740
give me the ten features such that

994
01:04:05,840 --> 01:04:10,350
the in the loss of information in the context is minimal

995
01:04:10,370 --> 01:04:12,160
so you're trying to find those things

996
01:04:12,220 --> 01:04:16,700
so when you do the orthogonal projection like dealer's showing you is basically an orthogonal

997
01:04:16,720 --> 01:04:19,030
positions of that means all these

998
01:04:19,050 --> 01:04:21,070
points are projected to

999
01:04:21,160 --> 01:04:23,240
to that particular dimension

1000
01:04:23,260 --> 01:04:24,470
such that

1001
01:04:24,490 --> 01:04:29,300
the variance of the distance queries from each point in the plane is that the

1002
01:04:29,300 --> 01:04:31,890
minimal and that's PCA

1003
01:04:31,930 --> 01:04:35,350
so this is across quite a nice book i'm sure

1004
01:04:35,370 --> 01:04:41,120
and in time you must have told you the book by christopher m bishop

1005
01:04:41,140 --> 01:04:42,550
you must have said

1006
01:04:42,570 --> 01:04:46,470
that that's a brilliant book and is it's what's writing and reading

1007
01:04:46,490 --> 01:04:49,160
i i read twice and

1008
01:04:49,160 --> 01:04:52,910
quite quite nice and you feel a little bit about it when you read the

1009
01:04:53,620 --> 01:04:56,120
but i think that repetition is good makes you

1010
01:04:56,220 --> 01:05:01,120
you know that you learn fast and second is that it is up to do

1011
01:05:01,140 --> 01:05:04,990
because the techniques on from one chapter to the other

1012
01:05:05,050 --> 01:05:08,530
and always you're trying to find some kind of normality or

1013
01:05:08,550 --> 01:05:10,030
try it

1014
01:05:10,740 --> 01:05:13,200
so the the mathematics is this

1015
01:05:14,200 --> 01:05:18,450
quite good you don't really need to complicated mathematics but the locations you have to

1016
01:05:18,450 --> 01:05:20,800
get used to

1017
01:05:23,260 --> 01:05:27,820
so therefore what we trying to say is that tell me the discriminating features would

1018
01:05:27,820 --> 01:05:31,350
like a single quality measure or useful for

1019
01:05:31,370 --> 01:05:33,160
feature ranking for example

1020
01:05:33,240 --> 01:05:37,030
so basically what you want to say is that you know each feature

1021
01:05:37,620 --> 01:05:39,840
we was some kind of value

1022
01:05:39,840 --> 01:05:45,640
and then you say the higher the value means the benefits it is in

1023
01:05:45,660 --> 01:05:49,660
in making some kind of decision on for example

1024
01:05:49,800 --> 01:05:52,260
emphasis is less on finding

1025
01:05:52,280 --> 01:05:57,680
the contrast and more on evaluating its power generally that that's what is given

1026
01:05:57,740 --> 01:05:58,910
so i give you

1027
01:05:58,970 --> 01:06:01,510
very very simple example

1028
01:06:02,050 --> 01:06:07,320
and this is not a real that i just made it up for exploration purposes

1029
01:06:07,320 --> 01:06:12,300
so let's assume we have a data set with three feet not the feature space

1030
01:06:12,340 --> 01:06:15,820
except that the first one is randomly

1031
01:06:15,850 --> 01:06:17,450
uniquely given

1032
01:06:17,450 --> 01:06:23,200
so therefore we have prior knowledge that the first column doesn't carry any

1033
01:06:23,200 --> 01:06:25,890
meeting with respect to the data except

1034
01:06:25,950 --> 01:06:28,800
that uniquely identifies each

1035
01:06:28,840 --> 01:06:30,340
toppling that relation

1036
01:06:30,350 --> 01:06:33,700
so this correlation because it has to be fixed features

1037
01:06:33,700 --> 01:06:38,510
and each feature has a specific meaning the first one is the identity

1038
01:06:38,570 --> 01:06:41,470
the second one is a feature which is high

1039
01:06:41,510 --> 01:06:42,890
and the third one is

1040
01:06:42,910 --> 01:06:47,050
a class which tells you this individual is a happy or unhappy

1041
01:06:47,050 --> 01:06:52,350
now again classes like any other feature except that we are giving some specific meaning

1042
01:06:52,350 --> 01:06:54,280
to that because we have a purpose

1043
01:06:54,280 --> 01:06:58,070
OK some kind of a target feature and we call it is the class

1044
01:06:58,120 --> 01:06:59,260
but from

1045
01:06:59,300 --> 01:07:01,760
from a mathematics point of view

1046
01:07:01,780 --> 01:07:07,070
it is like any other feature supply as does not hold any special meaning except

1047
01:07:07,070 --> 01:07:09,490
the way we're going to use

1048
01:07:09,600 --> 01:07:10,890
so in this case

1049
01:07:10,910 --> 01:07:15,660
what we would like to ask ask is that is the height somehow

1050
01:07:15,660 --> 01:07:18,070
predict whether an individual is happy

1051
01:07:18,080 --> 01:07:21,760
so that that is the question is is the height is important

1052
01:07:21,800 --> 01:07:23,510
to make the decision

1053
01:07:23,870 --> 01:07:26,570
whether this individual would be happier

1054
01:07:26,680 --> 01:07:30,050
so the question is we can we can do various ways

1055
01:07:30,050 --> 01:07:31,970
we can apply

1056
01:07:31,990 --> 01:07:36,970
some kind of signal to noise ratio kind of algorithms or we can use

1057
01:07:37,470 --> 01:07:41,010
information gain ratio or we can apply some kind of

1058
01:07:41,340 --> 01:07:44,820
a statistical test for basically you're trying to say

1059
01:07:44,820 --> 01:07:47,680
while you know the height is going to

1060
01:07:47,780 --> 01:07:49,140
this communities

1061
01:07:49,140 --> 01:07:53,280
and you have a hypothesis and then you do some statistical test

1062
01:07:54,300 --> 01:07:56,620
OK so we give a very simple one

1063
01:07:56,680 --> 01:08:02,280
let's say wilcoxon rank sum test the nice thing with wilcoxon rank test is that

1064
01:08:02,510 --> 01:08:03,850
it doesn't issue

1065
01:08:03,890 --> 01:08:05,740
any prior distribution

1066
01:08:05,740 --> 01:08:08,720
you are not doing any parametric estimation

1067
01:08:08,850 --> 01:08:11,640
are we doing is just sorting them

1068
01:08:11,660 --> 01:08:12,760
based on

1069
01:08:12,760 --> 01:08:13,950
their values

1070
01:08:13,950 --> 01:08:16,910
and then we find the rank of each individual

1071
01:08:16,930 --> 01:08:21,430
and then we sum the ranks and then of course we look at

1072
01:08:21,430 --> 01:08:24,910
some kind of statistical tables that tells you

1073
01:08:24,930 --> 01:08:28,430
that they are significantly different

1074
01:08:28,490 --> 01:08:31,930
so so let's say here in this case what we've done is we've sort of

1075
01:08:31,970 --> 01:08:34,120
this data set based on the heights

1076
01:08:34,180 --> 01:08:34,990
let's say

1077
01:08:34,990 --> 01:08:39,340
the highest height would be the top of the list and the

1078
01:08:39,340 --> 01:08:42,200
the shortest first in the bottom of the list

1079
01:08:42,990 --> 01:08:44,010
and then

1080
01:08:44,010 --> 01:08:47,950
we have the first thing so so i have done this year as you can

1081
01:08:48,720 --> 01:08:49,990
in this

1082
01:08:50,010 --> 01:08:53,760
the tallest person is a two point two meters

1083
01:08:53,780 --> 01:08:55,890
and the shortest is one point two

1084
01:08:56,660 --> 01:08:59,240
so let's look at the very first item

1085
01:08:59,320 --> 01:09:00,950
this person is happy

1086
01:09:00,950 --> 01:09:05,200
so what we're trying to see is how many people below me are set

1087
01:09:05,260 --> 01:09:10,280
ideally want to say is that suppose let's say arguments say a all the happy

1088
01:09:10,280 --> 01:09:12,870
people on the top all the shock people on the

1089
01:09:12,910 --> 01:09:14,260
the bottom of the list

1090
01:09:14,260 --> 01:09:15,470
and it's very clear

1091
01:09:15,490 --> 01:09:16,870
that happy people

1092
01:09:17,910 --> 01:09:19,530
right because

1093
01:09:19,580 --> 01:09:22,300
all they happy to work on the top of the list

1094
01:09:22,340 --> 01:09:24,930
you know or unhappy ones one

1095
01:09:25,050 --> 01:09:30,030
there clearly knows that the height is a an important feature

1096
01:09:30,050 --> 01:09:33,740
in determining the per person is happy so that's what we try to do so

1097
01:09:33,740 --> 01:09:36,950
in order to do that properly what you do is you look at the first

1098
01:09:36,950 --> 01:09:39,550
one and is allocated is that person

1099
01:09:39,660 --> 01:09:42,030
so for this to be happy

1100
01:09:42,050 --> 01:09:45,430
you would like to many many to be underneath to be sacked

1101
01:09:45,490 --> 01:09:49,550
so in this case there are one two three seven people so the rank of

1102
01:09:49,550 --> 01:09:51,530
the first individual three

1103
01:09:51,530 --> 01:09:53,430
then you go to the sad one

1104
01:09:53,430 --> 01:09:54,660
the sad one

1105
01:09:54,660 --> 01:09:57,240
so the implication that this we want to do is

1106
01:09:57,240 --> 01:10:02,390
tower meteosat person so that means you want all the ones below

1107
01:10:02,390 --> 01:10:04,760
so in this case i'm looking for the once

1108
01:10:04,780 --> 01:10:05,760
OK so

1109
01:10:05,760 --> 01:10:08,430
sorry the the opposite ones happy ones

1110
01:10:08,430 --> 01:10:13,350
so so you look at the there are only too happy one so therefore ranks

1111
01:10:13,410 --> 01:10:17,010
then you go to the next one is so happened to be said again rank

1112
01:10:17,030 --> 01:10:20,240
two happy one set one of so now

1113
01:10:20,280 --> 01:10:23,490
we establish the rank for each individual

1114
01:10:23,490 --> 01:10:25,290
that's not new

1115
01:10:25,430 --> 01:10:29,430
here is a case where you have almost no noise

1116
01:10:29,440 --> 01:10:33,960
and here is a case where you have a large amount of noise

1117
01:10:33,970 --> 01:10:36,400
so it's a bit exaggerated as

1118
01:10:36,820 --> 01:10:41,210
o point but it is the same the same distribution

1119
01:10:46,260 --> 01:10:52,640
there are there exists also wasted relation for centuries reach regulation

1120
01:10:52,690 --> 01:10:58,810
but here i show you to to be consistent with the support vector machines presentation

1121
01:10:58,810 --> 01:11:06,180
i usually use epsilon insensitive regulation but if you come if you start from all

1122
01:11:07,480 --> 01:11:17,390
like your classical relation classical innovation was regularisation then you can go for which relation

1123
01:11:17,430 --> 01:11:19,920
and cannot which relation

1124
01:11:20,000 --> 01:11:22,700
OK so i won't go into details about

1125
01:11:22,740 --> 01:11:24,730
it was just too

1126
01:11:24,740 --> 01:11:28,310
to prepare the party position this afternoon

1127
01:11:28,360 --> 01:11:29,550
and now

1128
01:11:29,560 --> 01:11:31,900
i want to show you

1129
01:11:32,530 --> 01:11:34,370
and also limits on

1130
01:11:34,390 --> 01:11:39,850
which still fits into the representer theorem follows last method of course support vector regression

1131
01:11:39,850 --> 01:11:40,510
also feeds

1132
01:11:41,060 --> 01:11:46,510
the representer theorem so relates exactly is the same case that support vector machines

1133
01:11:46,520 --> 01:11:54,000
that may be issued over the principality component analysis and maybe you use so that

1134
01:11:54,010 --> 01:11:57,910
at the last week maybe seven times

1135
01:11:57,920 --> 01:12:04,920
the information that i want to insist on can in principle components because everybody knows

1136
01:12:04,920 --> 01:12:06,100
the reason

1137
01:12:06,110 --> 01:12:07,410
and the

1138
01:12:07,420 --> 01:12:10,120
it's not usually easy

1139
01:12:10,130 --> 01:12:11,310
to make

1140
01:12:12,350 --> 01:12:16,810
and i know that there is a close link and fits perfectly with the we're

1141
01:12:17,110 --> 01:12:19,000
of so

1142
01:12:19,050 --> 01:12:25,190
this is very interesting because it it is about also space remember we the input

1143
01:12:25,950 --> 01:12:29,120
we have feature space and have also the

1144
01:12:29,130 --> 01:12:32,990
the inverse space function this space so the

1145
01:12:33,010 --> 01:12:34,380
producing can then

1146
01:12:34,790 --> 01:12:36,600
space where you can

1147
01:12:38,950 --> 01:12:43,850
it was function and you can inspire you for other reasons

1148
01:12:43,870 --> 01:12:49,790
so we're going to to see you can in principle components with this point of

1149
01:12:50,930 --> 01:12:57,540
so i put these references in fact there are civil works in team of

1150
01:12:57,560 --> 01:13:02,060
at that time it was the team of close my eyes should go

1151
01:13:02,080 --> 01:13:07,260
and i think that's what i contributed as well and also

1152
01:13:07,270 --> 01:13:10,850
and this is a slide of the energy

1153
01:13:10,870 --> 01:13:15,840
so remember your linear PCA so you have some clouds of points

1154
01:13:15,900 --> 01:13:17,420
so what you want to do

1155
01:13:17,440 --> 01:13:20,310
you you want to change

1156
01:13:20,320 --> 01:13:29,080
in fact your representation space and find a more appropriate replenish space when where when

1157
01:13:29,080 --> 01:13:31,140
you project the data

1158
01:13:31,160 --> 01:13:35,840
on each direction you maximise the variance of your data

1159
01:13:35,860 --> 01:13:38,940
so you really what you're going to do is going to be in

1160
01:13:38,950 --> 01:13:41,660
this new were present in the representation space

1161
01:13:41,670 --> 01:13:44,500
by incrementally find

1162
01:13:45,080 --> 01:13:48,800
the directions that capture

1163
01:13:48,810 --> 01:13:53,860
the most the largest variance of production

1164
01:13:54,950 --> 01:14:00,610
so it so well that in fact he doesn't work for the of data

1165
01:14:00,660 --> 01:14:04,780
each time new are for instance

1166
01:14:04,840 --> 01:14:06,850
different cloud

1167
01:14:06,870 --> 01:14:11,510
of data it for you to be very nice way to

1168
01:14:11,520 --> 01:14:18,080
to get a representation of the data that if you have some clout like here

1169
01:14:18,090 --> 01:14:22,240
which is not there but which is really

1170
01:14:22,340 --> 01:14:27,480
alexis then you can imagine that it is better

1171
01:14:27,500 --> 01:14:29,650
to change your presentation

1172
01:14:29,760 --> 01:14:31,880
space before doing

1173
01:14:31,890 --> 01:14:32,980
this year

1174
01:14:33,000 --> 01:14:37,490
so this is of course the idea of cannon PCA so do

1175
01:14:37,510 --> 01:14:40,180
PCA but in the future

1176
01:14:45,350 --> 01:14:47,280
let's consider the

1177
01:14:47,330 --> 01:14:52,120
classic principal component analysis you have a set of tangent picture

1178
01:14:52,170 --> 01:14:55,330
and what you want to win

1179
01:14:55,350 --> 01:14:58,680
i want to work with is the orthogonal projection

1180
01:14:58,700 --> 01:14:59,870
of x

1181
01:14:59,880 --> 01:15:03,120
on on two direction w

1182
01:15:04,130 --> 01:15:10,160
he space after the core the annual projects it's just this kind of product of

1183
01:15:11,930 --> 01:15:13,670
the direction w

1184
01:15:13,670 --> 01:15:18,940
the case that the average length of your compressed file will actually be as good

1185
01:15:18,940 --> 01:15:23,130
as the source coding theorem says it's possible this thing we call the expected length

1186
01:15:23,130 --> 01:15:24,400
of the code

1187
01:15:24,450 --> 01:15:26,780
here's what it says on the tin

1188
01:15:26,820 --> 01:15:32,420
it is the average length of the next codeword gets encoded so is the average

1189
01:15:32,420 --> 01:15:35,360
of the number of live by these probabilities

1190
01:15:35,380 --> 01:15:38,470
the expected length

1191
01:15:38,490 --> 01:15:43,150
the theorem that says you can't be the entropy which is right

1192
01:15:44,260 --> 01:15:48,190
there's also a result says if you use the hyphen algorithm you will get within

1193
01:15:48,190 --> 01:15:49,210
one day

1194
01:15:50,380 --> 01:15:54,940
but the source coding theorem said you can do which sounds pretty good

1195
01:15:54,940 --> 01:15:57,860
within one minute of optimal wonderful

1196
01:15:57,880 --> 01:16:00,010
any more questions

1197
01:16:00,030 --> 01:16:08,940
OK guess the question is does the receiver have to know the lookup table in

1198
01:16:08,940 --> 01:16:12,880
order to be able to decode and the answer is yes they must have

1199
01:16:12,900 --> 01:16:17,320
the huffman code at the other end and that's a great question yourself anticipating one

1200
01:16:17,320 --> 01:16:20,880
of the issues of huffman code which is in general you don't actually know the

1201
01:16:20,880 --> 01:16:24,590
probability that the receiver doesn't know the probability is so then it got to invent

1202
01:16:24,590 --> 01:16:29,300
some way of communicating the probabilities to the other and so they can run the

1203
01:16:29,300 --> 01:16:34,470
algorithm all you've got to send the table so you actually little head the head

1204
01:16:34,670 --> 01:16:40,380
of your data files has by huffman codes output of sufficient information to reconstruct it

1205
01:16:40,380 --> 01:16:43,030
and that's a bit of a pain is a bit ugly and it makes it

1206
01:16:43,030 --> 01:16:48,170
even more suboptimal and this plus one indicates any other questions

1207
01:16:52,340 --> 01:16:57,590
OK is obviously can smashing together that punctuation well we can just look for example

1208
01:16:57,590 --> 01:17:01,880
of so we encode BAC with this thing one

1209
01:17:05,590 --> 01:17:08,380
and one one o that

1210
01:17:08,400 --> 01:17:12,690
so that's how you would encode BAC and the question is is it easy to

1211
01:17:12,690 --> 01:17:16,800
decode this or the receiver going have to some difficult inference to figure out what

1212
01:17:16,800 --> 01:17:17,670
is going on well

1213
01:17:18,050 --> 01:17:19,280
they know

1214
01:17:19,320 --> 01:17:23,280
every time they start receiving bits that when we started you can we always start

1215
01:17:23,380 --> 01:17:26,630
the top of the tree and then we walked down the tree speaking out characters

1216
01:17:26,630 --> 01:17:28,030
until we reach the bottom

1217
01:17:28,050 --> 01:17:31,800
so the code can just follow this rule you can say top of the tree

1218
01:17:31,880 --> 01:17:35,320
reading the next bit to determine whether it goes left or right it's one that

1219
01:17:35,320 --> 01:17:39,360
can come down now reading the next one is the goal of o spit out

1220
01:17:39,360 --> 01:17:39,990
to be

1221
01:17:40,010 --> 01:17:42,940
he has to be and returns to the top of the tree

1222
01:17:42,950 --> 01:17:46,550
OK that's the decoding algorithm so it just follows the tree so you just follow

1223
01:17:46,570 --> 01:17:48,210
pointers and you will

1224
01:17:48,880 --> 01:17:49,880
we recover

1225
01:17:51,800 --> 01:17:54,240
was encoding right so

1226
01:17:54,260 --> 01:17:58,490
the have algorithm is just optimal it is really easy is one really easy to

1227
01:17:58,490 --> 01:18:00,610
run and real easy to decode yes

1228
01:18:04,220 --> 01:18:05,590
OK so

1229
01:18:05,610 --> 01:18:08,070
the question is if there is an error

1230
01:18:08,090 --> 01:18:11,820
in the compressed file so one of these bits may be split

1231
01:18:11,880 --> 01:18:16,260
then what happens and there is a whole literature on that on what happens to

1232
01:18:16,260 --> 01:18:20,220
your huffman algorithm if you have an error and what you will have noticed with

1233
01:18:20,220 --> 01:18:24,760
and tags and things like that is maybe get whole patch of the image it

1234
01:18:24,760 --> 01:18:31,030
gets messed up and then recovers so you can design have algorithms and codes perhaps

1235
01:18:31,030 --> 01:18:34,990
to have some sort of recovery property that there won't propagate forever

1236
01:18:35,010 --> 01:18:38,670
maybe there's a big literature on on that the best thing to do better assist

1237
01:18:38,670 --> 01:18:41,400
is a really good error correcting code of of course

1238
01:18:42,670 --> 01:18:51,010
this is separate the so called for the

1239
01:18:51,010 --> 01:18:53,240
so what i mean by that is you can you can

1240
01:18:53,440 --> 01:18:58,430
every node in the tree is associated with some particular level some some integer

1241
01:18:59,260 --> 01:19:04,980
and that is one point in every node

1242
01:19:05,000 --> 01:19:06,490
you can

1243
01:19:06,510 --> 01:19:09,840
proof that if you think about the structure given expansion constant

1244
01:19:09,860 --> 01:19:14,360
in the number of children is bounded by c the fourth

1245
01:19:14,390 --> 01:19:18,000
you can prove imagine that the is quite sorry

1246
01:19:18,000 --> 01:19:21,910
so we have expressed constant is going to be

1247
01:19:21,930 --> 01:19:24,100
and not too unbalanced tree with

1248
01:19:25,270 --> 01:19:29,340
maximal number children born out

1249
01:19:29,350 --> 01:19:42,800
so this is the cover tree and a one-dimensional this might look like

1250
01:19:45,660 --> 01:19:46,930
three invariants

1251
01:19:46,950 --> 01:19:48,880
this is the the first one

1252
01:19:48,900 --> 01:19:53,850
the first one says that because of resources about the no other notable so this

1253
01:19:54,820 --> 01:20:01,650
she said maybe three it's easy to see someone support so the index goes down

1254
01:20:01,760 --> 01:20:03,280
as you go down the tree

1255
01:20:07,230 --> 01:20:08,230
he says

1256
01:20:08,230 --> 01:20:11,800
i what's the point appears to resemble this point appears

1257
01:20:11,820 --> 01:20:12,990
at this level

1258
01:20:13,010 --> 01:20:18,160
it appears in all levels

1259
01:20:21,170 --> 01:20:23,170
there's also a scale

1260
01:20:23,190 --> 01:20:24,920
associated with

1261
01:20:24,960 --> 01:20:26,830
the levels so

1262
01:20:27,050 --> 01:20:31,820
OK let me

1263
01:20:32,380 --> 01:20:35,860
there's this covering tree invariant it says that

1264
01:20:36,500 --> 01:20:37,710
o point

1265
01:20:38,580 --> 01:20:39,930
this low

1266
01:20:40,100 --> 01:20:42,810
just some of the points in a high level

1267
01:20:42,840 --> 01:20:44,580
just the distance between

1268
01:20:44,590 --> 01:20:45,480
the point

1269
01:20:45,490 --> 01:20:49,000
in the high and low levels is bounded by to do that

1270
01:20:49,060 --> 01:20:53,020
i so this is is made the distance to the

1271
01:20:53,050 --> 01:20:55,290
that grows exponentially as you go up

1272
01:20:55,310 --> 01:21:15,320
in levels

1273
01:21:18,760 --> 01:21:20,820
what is the

1274
01:21:20,890 --> 01:21:22,990
this is is

1275
01:21:22,990 --> 01:21:26,030
this is the cover tree invariant

1276
01:21:26,090 --> 01:21:31,840
OK so last one separation

1277
01:21:31,890 --> 01:21:35,340
this is that if you have to put some level

1278
01:21:35,350 --> 01:21:40,160
and this is between the point is that we least the scale of the problem

1279
01:21:46,290 --> 01:21:47,930
so this is actually the the

1280
01:21:47,950 --> 01:21:51,080
invariant which forces you to have the integration time

1281
01:21:51,080 --> 01:21:52,680
in the worst case

1282
01:21:52,710 --> 01:21:56,820
the reason why it's very simple dress here

1283
01:21:57,790 --> 01:21:58,820
you could

1284
01:21:58,840 --> 01:21:59,670
the tree

1285
01:21:59,680 --> 01:22:09,080
which looks like this in the worst case

1286
01:22:09,140 --> 01:22:13,120
so between every point is basically one

1287
01:22:13,140 --> 01:22:15,660
one plus my sets on

1288
01:22:15,800 --> 01:22:16,990
and then

1289
01:22:17,520 --> 01:22:20,010
if you want to maintain separation invariant

1290
01:22:20,040 --> 01:22:22,290
we have to do this for every child

1291
01:22:22,300 --> 01:22:24,310
check for every other child

1292
01:22:26,170 --> 01:22:28,770
separation advertisement maintained

1293
01:22:28,780 --> 01:22:32,510
because all of these children at the same level

1294
01:22:34,000 --> 01:22:38,800
in children protect every child every time and that's going to be integrated

1295
01:22:38,820 --> 01:22:41,340
show that

1296
01:22:41,360 --> 01:22:42,810
it is

1297
01:22:44,300 --> 01:22:46,650
is in a sense

1298
01:22:46,690 --> 01:22:52,460
it's not something

1299
01:22:52,520 --> 01:22:59,000
they don't need to tell or together

1300
01:23:05,100 --> 01:23:06,290
first of all

1301
01:23:06,300 --> 01:23:08,660
said these are natural numbers

1302
01:23:08,780 --> 01:23:11,300
in the negative

1303
01:23:11,310 --> 01:23:15,780
so we can support misleadingly scale you care about

1304
01:23:15,800 --> 01:23:17,840
and what you want to

1305
01:23:18,040 --> 01:23:19,870
practices you can just

1306
01:23:19,880 --> 01:23:22,340
in basically go as deep as you need to go

1307
01:23:22,410 --> 01:23:25,090
this is how i need to go

1308
01:23:25,140 --> 01:23:30,240
and as you see the data points distances

1309
01:23:32,340 --> 01:23:36,560
so this is not actually quite cover tree

1310
01:23:36,570 --> 01:23:38,420
there's one more thing which

1311
01:23:38,440 --> 01:23:41,120
doing scale which is important

1312
01:23:41,140 --> 01:23:42,940
if you want to get the

1313
01:23:42,950 --> 01:23:45,600
the exact i told you

1314
01:23:45,600 --> 01:23:50,810
then you can cope with the situation where you have two points which iranian one

1315
01:23:50,820 --> 01:23:52,910
but is very far

1316
01:23:55,230 --> 01:23:56,080
this is

1317
01:23:56,090 --> 01:23:58,630
o light years and that's an angstrom

1318
01:23:58,660 --> 01:24:01,520
it's going to be a lot of levels of the tree that in many maybe

1319
01:24:01,520 --> 01:24:04,310
under something that is not going to have scale

1320
01:24:04,310 --> 01:24:05,960
he was the number of points

1321
01:24:06,020 --> 01:24:09,550
so if you want to do that right

1322
01:24:09,600 --> 01:24:11,140
we have to do

1323
01:24:11,150 --> 01:24:14,340
the discrete implicit tree

1324
01:24:15,630 --> 01:24:20,310
every time a child node has only itself as a child

1325
01:24:20,320 --> 01:24:21,850
and itself was apparent

1326
01:24:24,650 --> 01:24:26,600
go back

1327
01:24:26,660 --> 01:24:30,410
this one is eliminated will this one

1328
01:24:30,480 --> 01:24:33,340
this one this one that one

1329
01:24:33,340 --> 01:24:38,010
this one is not because it does not have itself was apparent

1330
01:24:38,030 --> 01:24:42,880
so this implicit representation which sources can skip all the intermediate skills which don't

1331
01:24:44,660 --> 01:24:53,690
OK so that's the basic cover training maybe

1332
01:24:53,710 --> 01:24:59,110
you care about how you use it

1333
01:24:59,120 --> 01:25:01,670
so if you have some query point you can think about this kind of being

1334
01:25:01,670 --> 01:25:05,690
defined at every level of the tree

1335
01:25:05,710 --> 01:25:09,190
and what you can do is you can compute discovers that

1336
01:25:09,330 --> 01:25:11,130
the current set

1337
01:25:11,150 --> 01:25:13,100
as invariant which is that

1338
01:25:13,350 --> 01:25:17,900
the the elements of the set of the nearest neighbour of the query or the

1339
01:25:17,900 --> 01:25:19,520
distance of the elements

1340
01:25:21,150 --> 01:25:25,460
national neighbours of query

1341
01:25:25,460 --> 01:25:29,260
very far away

1342
01:25:29,330 --> 01:25:30,950
because the

1343
01:25:30,980 --> 01:25:36,530
why do you think it's too left using minus one wins

1344
01:25:36,620 --> 01:25:41,960
by was

1345
01:25:44,960 --> 01:25:45,780
the mind

1346
01:25:45,800 --> 01:25:50,180
five solution right if the test charge

1347
01:25:52,580 --> 01:25:56,150
that's all that's avenue

1348
01:25:56,160 --> 01:26:03,550
you think this this is madness

1349
01:26:03,590 --> 01:26:06,340
who thinks the forces in this direction

1350
01:26:06,400 --> 01:26:08,720
o things is in this direction

1351
01:26:08,850 --> 01:26:10,690
could you help them really

1352
01:26:10,700 --> 01:26:14,960
the force is obviously in that direction because if you very far away

1353
01:26:14,990 --> 01:26:18,010
the field will be the same as if you just had a plus three in

1354
01:26:18,010 --> 01:26:20,720
the minus one somewhere here which is plus two

1355
01:26:20,780 --> 01:26:24,700
so if you far away from the configuration like this even if you're here with

1356
01:26:24,700 --> 01:26:26,940
review there which you way there

1357
01:26:27,940 --> 01:26:30,250
the field is like a plus to charge

1358
01:26:30,300 --> 01:26:33,300
and falls off as one of our square

1359
01:26:33,340 --> 01:26:36,970
so therefore if you far away the forces in this direction and now look was

1360
01:26:36,970 --> 01:26:38,630
is very interesting

1361
01:26:38,660 --> 01:26:42,310
here's a few closer to mine is one of the forces in this area

1362
01:26:42,340 --> 01:26:45,720
he when you very far away we are to be all the way here

1363
01:26:45,860 --> 01:26:50,720
in that direction so that means there must be some way you point where

1364
01:26:50,730 --> 01:26:52,700
the field is zero

1365
01:26:52,700 --> 01:26:57,740
because if the force is here in this direction ultimately clones of in that direction

1366
01:26:57,860 --> 01:26:59,330
must be somewhere point

1367
01:26:59,370 --> 01:27:01,690
he zero and that is part

1368
01:27:01,700 --> 01:27:03,620
of your assignment

1369
01:27:03,680 --> 01:27:10,750
i want you to find that point for particular charge configuration

1370
01:27:10,800 --> 01:27:13,510
so let's not go to

1371
01:27:13,580 --> 01:27:15,930
some graphical representations

1372
01:27:16,930 --> 01:27:19,130
a situation which is actually

1373
01:27:19,150 --> 01:27:20,570
plus three

1374
01:27:20,580 --> 01:27:23,620
minus one

1375
01:27:23,630 --> 01:27:24,700
by two

1376
01:27:24,780 --> 01:27:27,430
improve on the light situation

1377
01:27:27,440 --> 01:27:31,200
and let's see how these electric vectors

1378
01:27:31,210 --> 01:27:32,470
how they

1379
01:27:32,500 --> 01:27:34,570
show up

1380
01:27:35,860 --> 01:27:39,410
the vicinity of these two charges

1381
01:27:39,450 --> 01:27:42,180
so here you see the plus three in the minus one

1382
01:27:42,190 --> 01:27:44,670
relative units

1383
01:27:44,710 --> 01:27:47,080
let's take a look at this

1384
01:27:47,120 --> 01:27:50,150
in some detail

1385
01:27:50,220 --> 01:27:53,840
first of all the length of the arrows again indicates

1386
01:27:53,850 --> 01:27:58,370
the strength to give you a feeling for the strength not very qualitative of course

1387
01:27:58,420 --> 01:28:01,150
so let's first look at the plus three which

1388
01:28:01,160 --> 01:28:02,720
it's very powerful

1389
01:28:02,740 --> 01:28:06,470
you see these arrows all go away from the plus three and when you closer

1390
01:28:06,470 --> 01:28:13,030
to the last three years stronger which is the representation of the inverse square field

1391
01:28:13,080 --> 01:28:15,900
if you're very close to the minus one

1392
01:28:15,910 --> 01:28:21,240
pointing into the minus one because one of our square

1393
01:28:21,300 --> 01:28:23,550
minus one wins

1394
01:28:23,580 --> 01:28:26,800
and so you see there clearly

1395
01:28:26,840 --> 01:28:30,240
going into the direction of the minus one

1396
01:28:30,340 --> 01:28:33,670
if you're in between the plus and minus on this line

1397
01:28:33,710 --> 01:28:37,210
always the field will be pointing from the plaster the minus

1398
01:28:37,230 --> 01:28:41,100
because the pluses pushing out the miners sucking in the two

1399
01:28:41,110 --> 01:28:43,600
support each other

1400
01:28:43,830 --> 01:28:46,580
now if you go very far away

1401
01:28:48,300 --> 01:28:52,830
this charge configuration anywhere but very far away much farther than the distance between the

1402
01:28:52,860 --> 01:28:53,860
two charges

1403
01:28:53,960 --> 01:28:57,150
so somewhere here was somewhere there were somewhere there

1404
01:28:57,190 --> 01:29:01,240
four years notice that always the arrows pointing away

1405
01:29:01,300 --> 01:29:04,630
the reason is the plus three and minus one is as good as a plus

1406
01:29:05,280 --> 01:29:07,130
if you very far far away

1407
01:29:07,180 --> 01:29:09,190
but of course when you very close then

1408
01:29:09,230 --> 01:29:13,990
then the field configuration can be very very complicated but you see very clearly that

1409
01:29:13,990 --> 01:29:15,160
these arrows

1410
01:29:15,230 --> 01:29:18,330
all pointing outwards none of them

1411
01:29:18,400 --> 01:29:22,210
come back to the minors one of them point to the mines one direction

1412
01:29:22,260 --> 01:29:24,230
and that's because the plus three

1413
01:29:24,290 --> 01:29:27,500
it is more powerful and then there is here this point

1414
01:29:27,510 --> 01:29:30,000
and only one point one by the electric field

1415
01:29:30,010 --> 01:29:31,240
it is zero

1416
01:29:31,260 --> 01:29:34,000
if you put a positive test charge here

1417
01:29:34,050 --> 01:29:35,810
the miners were attracted

1418
01:29:35,820 --> 01:29:37,830
plus will repel it

1419
01:29:37,830 --> 01:29:39,620
and therefore there comes a point

1420
01:29:39,630 --> 01:29:42,210
whether to cancel each other

1421
01:29:45,030 --> 01:29:46,840
now there's another way

1422
01:29:47,620 --> 01:29:51,480
electric field representation which is more organised

1423
01:29:51,490 --> 01:29:53,580
and we call these field lines

1424
01:29:53,590 --> 01:29:55,650
so you see uganda plus

1425
01:29:56,590 --> 01:29:57,960
and you see there

1426
01:29:57,960 --> 01:29:59,060
the minus

1427
01:30:03,350 --> 01:30:07,040
if i release right here

1428
01:30:07,080 --> 01:30:08,630
well i place here

1429
01:30:08,650 --> 01:30:10,620
a positive test charge

1430
01:30:10,630 --> 01:30:12,130
all i know is

1431
01:30:12,220 --> 01:30:13,730
that the force

1432
01:30:13,740 --> 01:30:15,440
will be tangential

1433
01:30:15,450 --> 01:30:16,950
to the field lines

1434
01:30:17,030 --> 01:30:20,740
that is the meaning of these lines

1435
01:30:20,800 --> 01:30:22,800
so if i'm here

1436
01:30:22,820 --> 01:30:24,710
the force will be in this direction

1437
01:30:24,760 --> 01:30:27,580
if i put the positive just charge you

1438
01:30:27,620 --> 01:30:29,790
the before will be in this direction and of course

1439
01:30:29,800 --> 01:30:31,510
it's the negative charge

1440
01:30:32,430 --> 01:30:35,300
forceful itself

1441
01:30:35,410 --> 01:30:37,510
so the meaning of the field lines are

1442
01:30:38,150 --> 01:30:40,920
it's always tells you in which direction a charge

1443
01:30:42,450 --> 01:30:43,830
a force force

1444
01:30:43,860 --> 01:30:47,820
the positive charge always in the direction of the arrows

1445
01:30:47,860 --> 01:30:51,450
eventually to the field lines on the negative charge in the opposite direction

1446
01:30:51,580 --> 01:30:56,830
how many field lines either in space well of course an infinite number

1447
01:30:56,870 --> 01:30:59,490
just like these little arrows that we had before

1448
01:30:59,510 --> 01:31:01,540
we only sprinkled in a few

1449
01:31:01,580 --> 01:31:03,580
but of course in every single point

1450
01:31:03,630 --> 01:31:04,540
there is

1451
01:31:04,550 --> 01:31:05,910
an electric field

1452
01:31:05,930 --> 01:31:09,660
and so you can put in an infinite number of field lines and that would

1453
01:31:09,660 --> 01:31:10,960
make this

1454
01:31:10,960 --> 01:31:13,960
OK i guess we may as well start thinking everyone

1455
01:31:13,980 --> 01:31:19,290
actually going off to catch their flight and things to go to the beach was

1456
01:31:20,110 --> 01:31:24,210
so try not to be too too long

1457
01:31:24,210 --> 01:31:25,280
right so

1458
01:31:25,290 --> 01:31:26,160
this is

1459
01:31:26,180 --> 01:31:27,990
second lecture

1460
01:31:28,030 --> 01:31:30,280
on the object

1461
01:31:32,990 --> 01:31:37,300
all kinds of things to do vision basically with machine learning

1462
01:31:38,740 --> 01:31:41,900
things said yesterday will kind of

1463
01:31:41,960 --> 01:31:44,510
generic feature types of things

1464
01:31:44,780 --> 01:31:49,550
and important to know that but today there will be a lot more machine learning

1465
01:31:49,550 --> 01:31:53,900
things but i really just pulled out a number of different

1466
01:31:53,950 --> 01:31:58,270
methods the people replied and giving a serious examples so i'm not going to go

1467
01:31:58,270 --> 01:32:00,570
into series called bibsonomy on anything here

1468
01:32:00,580 --> 01:32:04,160
i think it's more appropriate because we're kind of the end

1469
01:32:04,210 --> 01:32:09,660
long two weeks of work and have probably know what's society three a theory and

1470
01:32:09,660 --> 01:32:14,970
also visions applied field you have to get things working so this is the listeners

1471
01:32:14,980 --> 01:32:21,230
really and what kinds of theoretical techniques actually work in practice

1472
01:32:21,230 --> 01:32:22,690
OK so

1473
01:32:22,700 --> 01:32:25,720
yesterday we saw something about the visual signal

1474
01:32:26,110 --> 01:32:31,250
low level feature extraction today we'll see some things but object detection

1475
01:32:31,280 --> 01:32:33,640
human pose and motion analysis

1476
01:32:34,390 --> 01:32:38,420
and image classification and indexing and a little bit about

1477
01:32:38,450 --> 01:32:43,090
more general image understanding that i haven't got much about the order is a little

1478
01:32:43,090 --> 01:32:46,140
bit problematic in the object station simple

1479
01:32:46,170 --> 01:32:47,390
this stuff gets

1480
01:32:47,400 --> 01:32:51,440
a little bit more complex than something be used in this get simpler again

1481
01:32:51,450 --> 01:32:55,980
but there is no good order this these are two different sort of complementary things

1482
01:32:56,080 --> 01:33:00,080
and this more sensible to put the detection of the human pose and the classification

1483
01:33:00,080 --> 01:33:04,300
of understanding so that's what i've done that

1484
01:33:04,340 --> 01:33:06,480
OK so object detection

1485
01:33:07,230 --> 01:33:09,340
just to kind of reminder

1486
01:33:09,340 --> 01:33:12,250
we're going to want to detect objects in images

1487
01:33:12,340 --> 01:33:15,670
they could occur at any scale at any position in the image

1488
01:33:15,670 --> 01:33:18,530
so typically what we're going to have to do

1489
01:33:18,560 --> 01:33:22,460
is we're going to have to run some kind of it take to across the

1490
01:33:23,550 --> 01:33:26,010
multiple positions and scales

1491
01:33:26,040 --> 01:33:29,860
in figure out what's in the window that we see

1492
01:33:30,380 --> 01:33:33,980
and when we figure out what what's in the window we can calculate a set

1493
01:33:33,980 --> 01:33:34,890
of features

1494
01:33:34,950 --> 01:33:42,010
we're going to typically run some kind of classifier machine learning discriminate talk mister

1495
01:33:42,380 --> 01:33:48,820
probabilistic estimate probability something like that on the window and when will then be clear

1496
01:33:48,850 --> 01:33:51,410
detections particular places

1497
01:33:52,510 --> 01:33:56,270
in order to train the model going to need a set of training data

1498
01:33:56,300 --> 01:34:01,760
typically which has the positions of the objects max and so that we know we're

1499
01:34:01,760 --> 01:34:06,040
looking at the window contains energy which is well centered and things like this

1500
01:34:06,360 --> 01:34:08,110
which gets very tedious

1501
01:34:08,130 --> 01:34:12,630
so is also interest in the community and learning things automatically

1502
01:34:12,640 --> 01:34:15,510
for images without really having to go and knock them

1503
01:34:15,510 --> 01:34:19,760
of touch on the line going to any detail

1504
01:34:20,160 --> 01:34:23,040
it turns out the posters

1505
01:34:23,080 --> 01:34:28,950
important the negatives are also very important position in this limited two giving good negative

1506
01:34:28,950 --> 01:34:31,980
but i can't really talk about that because it's it's kind of

1507
01:34:32,010 --> 01:34:32,890
have i

1508
01:34:32,930 --> 01:34:37,260
so misinformation there's not really anything concrete but i think

1509
01:34:37,260 --> 01:34:39,830
in order to get machine learning methods working

1510
01:34:40,570 --> 01:34:45,380
and in particular in situations like we're vision where you're detections are only a very

1511
01:34:45,450 --> 01:34:50,110
small fraction of the classification c a almost all windows don't contain you should be

1512
01:34:50,130 --> 01:34:51,270
looking for

1513
01:34:51,300 --> 01:34:56,420
because of that the negative team to overwhelming positive sets triples very important to you

1514
01:34:56,480 --> 01:34:57,770
could make

1515
01:34:58,010 --> 01:35:04,990
so we typically will bootstrap an extensive but having failed things false positives victories citizen

1516
01:35:04,990 --> 01:35:07,510
object is not knowledge

1517
01:35:10,830 --> 01:35:12,950
just to bring this home again so the

1518
01:35:12,980 --> 01:35:16,700
typical detection is going to be scanning image at all scales and locations looking in

1519
01:35:16,700 --> 01:35:17,490
a window

1520
01:35:17,510 --> 01:35:21,390
running some classifier and then typically what's going to happen at the end of this

1521
01:35:22,330 --> 01:35:26,080
is that we get multiple detections around the local region

1522
01:35:26,200 --> 01:35:30,610
because the the classifier is really designed for different problem is it's designed to get

1523
01:35:30,630 --> 01:35:37,090
just a isolated examples coming in and to classify them as here we have a

1524
01:35:37,090 --> 01:35:40,690
whole series of examples that some of positions and scales

1525
01:35:40,720 --> 01:35:44,540
and typically take two five someone's going to fire and the others but we don't

1526
01:35:44,540 --> 01:35:47,470
have multiple objects it's just the state has a little bit of

1527
01:35:47,530 --> 01:35:50,350
spatial freedom and so we have to

1528
01:35:50,360 --> 01:35:55,410
to fuse there's multiple detections and reliable way so as not to lose good ones

1529
01:35:55,460 --> 01:35:57,320
water and bad ones

1530
01:35:57,350 --> 01:35:59,910
which is also something of an art

1531
01:36:00,850 --> 01:36:07,030
so just onto list of different kinds of object detectors that people have thought up

1532
01:36:07,060 --> 01:36:10,620
we start with a very simple one so this is an exemplar based method that

1533
01:36:10,620 --> 01:36:14,360
means it's based on basically just comparison two examples

1534
01:36:14,680 --> 01:36:18,770
in this particular take to be examples are going to be represented

1535
01:36:18,780 --> 01:36:22,190
this two-dimensional silhouette so each

1536
01:36:23,130 --> 01:36:25,020
from from

1537
01:36:25,020 --> 01:36:27,140
of the trying

1538
01:36:27,930 --> 01:36:30,230
and the points inside the triangle

1539
01:36:30,620 --> 01:36:34,540
are the points where a has some probability p has some ability in c has

1540
01:36:34,540 --> 01:36:37,790
some probability so that might be that point

1541
01:36:37,810 --> 01:36:44,640
OK every point of the triangle is some points in the distribution space over distributions

1542
01:36:44,640 --> 01:36:47,290
of three items

1543
01:36:48,810 --> 01:36:51,370
the dirichlet

1544
01:36:51,390 --> 01:36:54,500
the place is a distribution over that space

1545
01:36:54,520 --> 01:36:58,770
and the dirichlet one one one is very special if all alpha is equal to

1546
01:36:58,770 --> 01:37:04,640
one then they actually one one one is the uniform distribution on the space every

1547
01:37:04,910 --> 01:37:06,750
there is nuance here which is that

1548
01:37:06,810 --> 01:37:11,310
these actual edges the point on the line are not possible you can have something

1549
01:37:11,310 --> 01:37:14,160
have zero probability hundred years later has

1550
01:37:14,180 --> 01:37:16,870
that has zero probability

1551
01:37:17,620 --> 01:37:22,350
but you can get arbitrarily close so one one one on the interior of this

1552
01:37:22,390 --> 01:37:26,140
triangle puts you put the same probability anywhere

1553
01:37:26,160 --> 01:37:27,890
it's called the uniform distribution

1554
01:37:27,910 --> 01:37:29,730
all right

1555
01:37:32,370 --> 01:37:33,640
in general

1556
01:37:33,660 --> 01:37:35,410
so we're in this triangle

1557
01:37:35,430 --> 01:37:38,430
base where a b and c have equal probability

1558
01:37:39,810 --> 01:37:46,890
OK thank you so

1559
01:37:46,910 --> 01:37:48,000
one one

1560
01:37:48,020 --> 01:37:49,200
so remember

1561
01:37:49,330 --> 01:37:53,660
the dirichlet is parameterized by alpha and alpha is the vector of length k

1562
01:37:53,680 --> 01:37:59,520
of positive values they can be anything so in the first three years out has

1563
01:37:59,520 --> 01:38:00,720
three components

1564
01:38:00,850 --> 01:38:07,700
in one one refers first to alpha one equal opportunity equal to three

1565
01:38:09,910 --> 01:38:13,540
and so those are the parameters of this distribution and what i'm saying is that

1566
01:38:13,540 --> 01:38:16,390
when you set those parameters exactly to one

1567
01:38:16,390 --> 01:38:20,890
like that that leads to a distribution where any point on the simplex has the

1568
01:38:20,890 --> 01:38:22,790
same probability

1569
01:38:24,560 --> 01:38:29,950
but where is the point where alpha a b and c of probability one-third is

1570
01:38:29,950 --> 01:38:31,230
that point

1571
01:38:31,250 --> 01:38:35,060
it's in the middle right right here

1572
01:38:35,080 --> 01:38:42,120
so if you if you have a lay where alpha one alpha two alpha three

1573
01:38:42,120 --> 01:38:43,750
also by

1574
01:38:43,770 --> 01:38:47,410
what that does is put a bomb in the middle

1575
01:38:47,410 --> 01:38:52,000
and read the contours around the public

1576
01:38:57,520 --> 01:38:58,680
in general

1577
01:38:58,700 --> 01:39:04,910
some properties of the dirichlet later work knowing the expectation of data

1578
01:39:05,020 --> 01:39:06,350
given alpha

1579
01:39:09,230 --> 01:39:12,830
so think data that the expectation of the i th component so

1580
01:39:12,890 --> 01:39:17,060
the have random we have a distribution over the space we can contemplate the expectation

1581
01:39:17,060 --> 01:39:21,830
of any of these components of the multivariate distribution expectation of data i given alpha

1582
01:39:22,160 --> 01:39:25,290
equals out over the summer

1583
01:39:25,310 --> 01:39:31,200
OK so with five five five the expectation of each of the components is going

1584
01:39:31,200 --> 01:39:35,680
to be a third because five or fifteen is the third

1585
01:39:35,730 --> 01:39:40,540
and that's always true expectation and that i give alpha is this and so if

1586
01:39:40,540 --> 01:39:45,040
you have a and not in a symmetric dirichlet distribution if you have like alpha

1587
01:39:45,040 --> 01:39:50,560
one and alpha two also say so here we have

1588
01:39:50,560 --> 01:39:55,660
are simplex and here is the distribution centered somewhere not in in the middle

1589
01:39:56,930 --> 01:40:02,790
here's the distribution centre not middle all these are all distribution centered on the

1590
01:40:02,790 --> 01:40:07,390
OK so that that determines the location of this home

1591
01:40:09,960 --> 01:40:16,330
yes that's the next thing i mean

1592
01:40:16,350 --> 01:40:23,000
no yes and no other answers your questions the first question was the greater alpha

1593
01:40:23,430 --> 01:40:27,310
less peaky yes the next question was you can add up to less than one

1594
01:40:28,040 --> 01:40:30,930
but i mean i'm going to explain this in that the

1595
01:40:30,960 --> 01:40:34,450
OK the first just to look for the location of the home location of the

1596
01:40:34,450 --> 01:40:39,350
home is is determined by the expectation

1597
01:40:39,350 --> 01:40:41,890
OK now let's get to the of the heart

1598
01:40:43,430 --> 01:40:46,400
and let's assume for now that alpha is greater than one the alpha less than

1599
01:40:46,400 --> 01:40:50,980
one case will deal with stuff

1600
01:40:51,000 --> 01:40:52,230
OK go or

1601
01:40:57,600 --> 01:41:00,270
and you see that can you all see the board over there that we only

1602
01:41:00,270 --> 01:41:02,500
have to work with one set of lights

1603
01:41:02,520 --> 01:41:03,930
are so

1604
01:41:05,410 --> 01:41:09,350
so this is one important

1605
01:41:09,350 --> 01:41:14,890
a piece of the dirichlet parameterisation the other important piece is the sum of the

1606
01:41:14,890 --> 01:41:19,600
alpha k so the sum of the alphas determines the peak this of the dirichlet

1607
01:41:19,930 --> 01:41:22,080
when the sum of the alphas is

1608
01:41:22,080 --> 01:41:23,930
it is

1609
01:41:25,180 --> 01:41:30,020
small then the deviously is can be very spread out and the greater the sum

1610
01:41:30,020 --> 01:41:34,430
of the alpha is the more peaky hideously becomes at this point

1611
01:41:34,450 --> 01:41:36,930
at its expectation

1612
01:41:38,350 --> 01:41:42,600
and sometimes you see this called

1613
01:41:42,600 --> 01:41:46,830
sometimes called as

1614
01:41:46,950 --> 01:41:54,580
and this is sometimes called and

1615
01:41:54,680 --> 01:41:58,000
so this is like the mean and this is the scaling

1616
01:41:58,020 --> 01:42:03,080
OK and just to to so an alternative parameterisation of the dirichlet is as a

1617
01:42:03,080 --> 01:42:07,870
point on the simplex the mean and a scaling parameter s

1618
01:42:07,870 --> 01:42:12,230
which is which is which term topic it is around the mean and

1619
01:42:12,290 --> 01:42:16,520
i'm saying this to foreshadow the various lectures on nonparametric bayesian methods that are coming

1620
01:42:16,520 --> 01:42:21,950
up next week where you parameterize the infinite dimensional dirichlet in precisely the same way

1621
01:42:21,960 --> 01:42:23,200
OK but

1622
01:42:23,230 --> 01:42:26,290
just put that way in your brains

1623
01:42:26,310 --> 01:42:28,430
somewhere safe

1624
01:42:30,020 --> 01:42:37,500
now i need to go to my safe brain space think about what is in

1625
01:42:37,500 --> 01:42:39,220
taking us outlets one

1626
01:42:39,230 --> 01:42:40,930
and then posterior OK

1627
01:42:40,950 --> 01:42:48,080
this i think will be useful

1628
01:42:50,770 --> 01:42:58,930
these boards are heavier than at my university i can see why

1629
01:42:58,950 --> 01:43:01,310
yes so posterior

1630
01:43:04,060 --> 01:43:08,160
very solid stuff

1631
01:43:12,390 --> 01:43:15,370
not all mathematics is for these boards

1632
01:43:20,770 --> 01:43:22,730
it's kind of invigorating

1633
01:43:22,750 --> 01:43:26,520
alpha less than one

1634
01:43:26,560 --> 01:43:28,060
what happens

1635
01:43:28,080 --> 01:43:38,730
OK equivalently

1636
01:43:38,750 --> 01:43:43,140
as last one

1637
01:43:43,180 --> 01:43:46,370
that was enough is less than one

1638
01:43:46,410 --> 01:43:49,540
is you get a sparsity

1639
01:43:49,560 --> 01:43:51,020
OK so on the

1640
01:43:51,040 --> 01:43:53,390
three simplex

1641
01:43:54,450 --> 01:43:57,680
these are are friendly triangle

1642
01:43:57,680 --> 01:44:02,680
so you have an joint easier ways like somewhere in the middle of the simplex

1643
01:44:02,700 --> 01:44:06,160
but on the on when alpha is less than one you end up with a

1644
01:44:06,160 --> 01:44:07,540
different shape

1645
01:44:07,560 --> 01:44:09,310
and i would say that

1646
01:44:09,310 --> 01:44:11,270
computer architecture

1647
01:44:12,860 --> 01:44:16,780
so that are not shell gives you a feel for

1648
01:44:16,800 --> 01:44:18,810
the space of EECS

1649
01:44:18,930 --> 01:44:24,200
OK this jonckheere here almost a vignette of what easier for might is all about

1650
01:44:25,110 --> 01:44:25,930
this is

1651
01:44:25,970 --> 01:44:31,030
this is the world according to aggarwal because it's teaching double two OK so this

1652
01:44:31,030 --> 01:44:32,600
is this is six two

1653
01:44:32,600 --> 01:44:36,230
the mister vcs is you know something out there

1654
01:44:36,280 --> 01:44:38,480
OK so what right now

1655
01:44:38,510 --> 01:44:40,370
is through discourse

1656
01:44:40,450 --> 01:44:41,910
i want you to think about

1657
01:44:41,970 --> 01:44:45,860
which part in this vignette we are in

1658
01:44:45,900 --> 01:44:48,630
so right now i'm going to start here

1659
01:44:48,650 --> 01:44:50,110
and peculiar

1660
01:44:50,200 --> 01:44:56,890
OK and as you get closer and closer things get simpler and simpler and simpler

1661
01:44:56,890 --> 01:44:59,880
to build the final abstractions are

1662
01:45:02,070 --> 01:45:07,450
steeringwheel that's the abstract interplay game right to fortify very simple interfaces and that's all

1663
01:45:07,450 --> 01:45:08,340
you need to know

1664
01:45:08,380 --> 01:45:10,370
and everywhere in the world can place

1665
01:45:10,430 --> 01:45:11,690
so remember

1666
01:45:11,730 --> 01:45:13,730
this stuff is complicated

1667
01:45:13,860 --> 01:45:16,070
this stuff is very very simple

1668
01:45:16,080 --> 01:45:19,520
OK and the more people that fractions and come to this i think it's simpler

1669
01:45:19,520 --> 01:45:20,870
and simpler

1670
01:45:21,980 --> 01:45:23,870
a large part of what to cover today

1671
01:45:23,900 --> 01:45:27,080
is make the biggest simplifications

1672
01:45:27,090 --> 01:45:31,870
the biggest simplification will make it go from access equation very nice some very very

1673
01:45:31,870 --> 01:45:33,950
simple algebraic groups

1674
01:45:33,990 --> 01:45:36,230
the kind of maxwell's equations myself

1675
01:45:36,240 --> 01:45:40,460
and i tell you that there very interesting stuff but complicated

1676
01:45:40,470 --> 01:45:41,530
you can imagine

1677
01:45:41,590 --> 01:45:43,730
so the equations

1678
01:45:43,880 --> 01:45:46,230
so let's take an example OK

1679
01:45:46,240 --> 01:45:47,940
so let's say a

1680
01:45:47,980 --> 01:45:49,770
let's say i have a battery

1681
01:45:49,780 --> 01:45:58,570
six foot to page three if you're of course notes and let's say iconic that

1682
01:46:04,730 --> 01:46:06,930
OK and so i

1683
01:46:07,070 --> 01:46:11,520
and about batteries apply some voltage v

1684
01:46:11,600 --> 01:46:13,650
and i ask you a simple question

1685
01:46:13,660 --> 01:46:15,700
what is the current

1686
01:46:15,730 --> 01:46:18,150
for the world

1687
01:46:18,210 --> 01:46:22,760
OK so you something that i can build using objects you i can pick around

1688
01:46:22,860 --> 01:46:26,220
from stores and so on and i can connect them up in this manner the

1689
01:46:26,220 --> 01:46:28,690
question what is the current i

1690
01:46:29,150 --> 01:46:32,950
before you have done a lot of maxwell's equations can all this misleads

1691
01:46:32,970 --> 01:46:36,280
and so on the first step is to write down all of michael's equations and

1692
01:46:36,280 --> 01:46:37,610
you can say you know

1693
01:46:37,620 --> 01:46:40,300
the graph is a minor detail

1694
01:46:40,310 --> 01:46:44,270
go on and on and on

1695
01:46:44,280 --> 01:46:48,690
OK and all mass of equation and then now how do i know how to

1696
01:46:48,710 --> 01:46:50,120
get from there to here

1697
01:46:50,130 --> 01:46:52,050
it's because you can do it

1698
01:46:52,060 --> 01:46:54,490
OK you can do it but it's very complicated

1699
01:46:54,530 --> 01:46:56,770
OK so instead what you could do

1700
01:46:56,780 --> 01:47:00,140
is there an easy way

1701
01:47:02,690 --> 01:47:09,350
so i will remind you is that this course is actually very easy to remember

1702
01:47:09,350 --> 01:47:12,990
the building fraction of one abstraction make your life easier

1703
01:47:13,080 --> 01:47:16,990
but think lives are getting more complicated than you not using intuition enough

1704
01:47:17,080 --> 01:47:18,800
can just remember the the i work

1705
01:47:18,990 --> 01:47:20,980
it's all about making things simple

1706
01:47:21,630 --> 01:47:24,730
so i'm going to give you an analogy

1707
01:47:24,740 --> 01:47:26,190
so suppose

1708
01:47:27,260 --> 01:47:30,930
OK and i apply force to the object

1709
01:47:30,980 --> 01:47:34,020
in analogy get to get some insight into how to do this

1710
01:47:34,080 --> 01:47:40,230
so here's an object apply force and ask questions

1711
01:47:40,320 --> 01:47:45,180
what is the acceleration of the object when apply a force f

1712
01:47:45,230 --> 01:47:47,180
so how would you do it

1713
01:47:47,560 --> 01:47:52,180
eight the ninth tenth grader can do this so they would ask me

1714
01:47:52,240 --> 01:47:54,380
what's the mass of the object

1715
01:47:54,440 --> 01:47:59,600
kaski was the acceleration you don't ask me for the mass of the object

1716
01:47:59,610 --> 01:48:02,190
i tell you the mass of the object

1717
01:48:02,220 --> 01:48:04,730
and then you say sure a is

1718
01:48:04,750 --> 01:48:08,460
if you went by and then

1719
01:48:08,460 --> 01:48:10,500
it's as simple as that

1720
01:48:10,520 --> 01:48:13,080
OK i i i could have gone into

1721
01:48:13,080 --> 01:48:16,470
often the differential equations and so on to figure that out but

1722
01:48:16,490 --> 01:48:18,310
you asked me for the mass

1723
01:48:18,410 --> 01:48:21,320
and you give me the answer is estimated by

1724
01:48:21,990 --> 01:48:25,580
you ignored a bunch of things you ignore the shape of the object

1725
01:48:25,630 --> 01:48:27,260
you know its color

1726
01:48:27,270 --> 01:48:29,130
you its temperature

1727
01:48:29,180 --> 01:48:32,900
OK and you ignored the software harder what

1728
01:48:33,090 --> 01:48:35,590
a whole bunch of things

1729
01:48:35,630 --> 01:48:37,880
you focused on one thing

1730
01:48:37,930 --> 01:48:41,080
OK you focused on its mass

1731
01:48:41,110 --> 01:48:43,210
and it turns out there

1732
01:48:43,260 --> 01:48:44,520
the process

1733
01:48:44,540 --> 01:48:50,380
really was developed from a stable set of simplifications that is called

1734
01:48:50,400 --> 01:48:53,390
you remember this

1735
01:48:53,400 --> 01:48:55,610
o point classification

1736
01:48:55,660 --> 01:49:02,150
OK so up in physics you before q simplified lives by viewing objects as point

1737
01:49:02,150 --> 01:49:06,360
objects having mass point and for acting at that point

1738
01:49:06,420 --> 01:49:10,830
but it means that the property of the object that is of interest for this

1739
01:49:10,830 --> 01:49:12,430
process is called

1740
01:49:12,440 --> 01:49:13,370
in physics

1741
01:49:17,270 --> 01:49:26,320
o point mass discretisation

1742
01:49:28,590 --> 01:49:33,690
use that analogy and i'll show you is similar simple process to do the problem

1743
01:49:33,690 --> 01:49:34,360
with the

1744
01:49:34,380 --> 01:49:35,670
light bulbs

1745
01:49:42,920 --> 01:49:48,570
so take my labelled again

1746
01:49:48,760 --> 01:50:01,020
and the focus on the filament of the light bulb

1747
01:50:01,040 --> 01:50:04,500
OK all i care about is the current flowing from the label

1748
01:50:04,600 --> 01:50:06,510
OK i don't care about

1749
01:50:06,510 --> 01:50:08,980
that this is related papers in the literature

1750
01:50:09,150 --> 01:50:14,020
it is i looked at the internet and this is what it looks like and

1751
01:50:14,020 --> 01:50:17,440
if it doesn't really say anything beyond

1752
01:50:17,460 --> 01:50:20,520
just what was found for particular network

1753
01:50:20,530 --> 01:50:26,400
in fact it generally it's and modeling classes networks so we said idea of the

1754
01:50:26,400 --> 01:50:31,520
class networks is is formalising idea of an ensemble

1755
01:50:31,530 --> 01:50:35,800
so the idea of selling an ensemble class networks is that obviously

1756
01:50:35,810 --> 01:50:41,940
you want to identify kind general principles underlying what you might observed in real network

1757
01:50:41,950 --> 01:50:44,970
so for example

1758
01:50:45,000 --> 01:50:48,080
what sort of networks we talk about here well

1759
01:50:48,670 --> 01:50:50,200
new york

1760
01:50:50,220 --> 01:50:53,150
dealing with their own meta ecstasy is if you ask the top we had

1761
01:50:53,790 --> 01:51:00,820
social networks things like collaboration networks epidemiological network involving people basically

1762
01:51:00,870 --> 01:51:03,210
information networks

1763
01:51:03,380 --> 01:51:07,590
things like the internet and the web maybe telecoms networks

1764
01:51:09,440 --> 01:51:12,790
biological networks twenty those neural networks

1765
01:51:12,800 --> 01:51:13,980
things like

1766
01:51:14,000 --> 01:51:24,970
protein protein networks that metabolic interaction networks cells genetic regulatory networks and so on should

1767
01:51:25,040 --> 01:51:26,790
probably be left out through there

1768
01:51:26,810 --> 01:51:30,720
it's exactly what you might call technological networks like air traffic control

1769
01:51:30,930 --> 01:51:32,950
power grids and so on

1770
01:51:33,060 --> 01:51:38,700
and geographic network such as retail networks which are being studied in this project transport

1771
01:51:38,700 --> 01:51:40,940
networks and so

1772
01:51:41,020 --> 01:51:46,360
even citation networks and the starting from a geographical point of view

1773
01:51:46,410 --> 01:51:52,070
so basically what we mean by ensemble we mean such a statistical ensemble

1774
01:51:52,080 --> 01:51:53,260
to be more

1775
01:51:53,290 --> 01:51:57,630
the philosophy behind the summit in statistical mechanics way

1776
01:51:57,700 --> 01:52:01,640
what you don't know understand if you like your model is essentially random

1777
01:52:03,190 --> 01:52:09,960
you can view an ensemble of networks as a probability distribution on the set of

1778
01:52:09,960 --> 01:52:15,010
all possible networks perhaps of a given size whatever

1779
01:52:15,720 --> 01:52:21,430
and then what you what you do is sample networks randomly from the statistical ensemble

1780
01:52:21,430 --> 01:52:24,850
in other words from the distribution of all possible networks

1781
01:52:25,300 --> 01:52:29,500
not not quite nice way to look at is that the adjacency matrix a matrix

1782
01:52:29,500 --> 01:52:30,940
now becomes essentially

1783
01:52:32,060 --> 01:52:34,550
the elements the adjacency

1784
01:52:34,680 --> 01:52:38,630
i matrix now become random variables which are jointly distributed

1785
01:52:38,690 --> 01:52:41,660
so it's it's actually quite equivalent to say

1786
01:52:41,670 --> 01:52:46,130
well i've got random matrix that that could and say got a probability distribution on

1787
01:52:46,130 --> 01:52:47,600
the set of networks

1788
01:52:48,440 --> 01:52:50,880
so if the write to the cap

1789
01:52:50,890 --> 01:52:51,800
i j

1790
01:52:51,880 --> 01:52:57,970
big a little changes so we talk about random variables now

1791
01:52:57,990 --> 01:52:59,740
so i'm going to go through the

1792
01:53:00,130 --> 01:53:03,660
it's like a study of the of the simplest random

1793
01:53:03,670 --> 01:53:07,070
the networks the simplest ensemble of networks if you like

1794
01:53:07,160 --> 01:53:11,900
which are also known as sports on random graphs in particular what's become known as

1795
01:53:11,910 --> 01:53:14,500
the as then the model which is

1796
01:53:14,540 --> 01:53:18,520
it's the simplest and most most study most well understood

1797
01:53:18,760 --> 01:53:23,610
model of random networks for random graphs basically way it works is

1798
01:53:23,620 --> 01:53:26,180
pick and some number n nodes

1799
01:53:26,230 --> 01:53:29,350
and then you take the probability

1800
01:53:29,400 --> 01:53:31,480
some probability p

1801
01:53:31,490 --> 01:53:36,200
and those the two parameters we also model and then what you do is basically

1802
01:53:36,200 --> 01:53:40,230
assigned each to any two edges its origin each

1803
01:53:40,250 --> 01:53:42,830
two tina and ten potential edge if you like

1804
01:53:43,960 --> 01:53:50,720
flip biased coins probability p and sticking an edge or not and this

1805
01:53:50,730 --> 01:53:51,900
he is one

1806
01:53:51,910 --> 01:53:53,110
we prepared earlier

1807
01:53:54,250 --> 01:53:56,170
don't need anything to this

1808
01:53:56,220 --> 01:54:01,170
distribution of notes that this tournament circle but they just like to completely random

1809
01:54:02,730 --> 01:54:05,540
the first thing to say about this is that

1810
01:54:06,200 --> 01:54:11,100
expected number of edges if you take one what kind of mean number of

1811
01:54:11,190 --> 01:54:14,240
of connections of links i'm going see also noted

1812
01:54:14,390 --> 01:54:16,670
and that turns out to be

1813
01:54:17,840 --> 01:54:19,700
to point to the

1814
01:54:19,950 --> 01:54:27,520
and times that he so one consequence of that is

1815
01:54:27,540 --> 01:54:32,910
as increase in fact health p fixed increased at the graph actually

1816
01:54:32,930 --> 01:54:36,660
it gets bigger more nodes but also extends that means that in particular now it

1817
01:54:36,700 --> 01:54:39,420
gets more and more connections which is

1818
01:54:39,500 --> 01:54:42,570
well you might not necessary wanted one that

1819
01:54:42,650 --> 01:54:47,720
the point is that the if you look at the adjacency matrix for that the

1820
01:54:48,110 --> 01:54:50,660
the entries are only

1821
01:54:50,670 --> 01:54:55,140
random variables and system fancy name for a biased coin toss space

1822
01:54:55,210 --> 01:54:57,030
and the independent which is

1823
01:54:57,990 --> 01:54:59,520
a distinguishing feature of of

1824
01:54:59,630 --> 01:55:01,660
these parts on random graphs

1825
01:55:01,850 --> 01:55:04,240
now as i mentioned

1826
01:55:04,260 --> 01:55:08,990
these networks actually gets density just cranked up the number of nodes so what you

1827
01:55:08,990 --> 01:55:11,180
tend to look at is is

1828
01:55:11,200 --> 01:55:14,540
something is large networks one question is why

1829
01:55:14,550 --> 01:55:16,570
might we be looking at

1830
01:55:16,790 --> 01:55:22,330
interesting looking at large networks as people generally do in the networks literature well

1831
01:55:22,440 --> 01:55:24,890
society of songs might be that

1832
01:55:24,900 --> 01:55:27,130
actually makes maths more tractable

1833
01:55:27,140 --> 01:55:28,990
which is certainly the case

1834
01:55:28,990 --> 01:55:30,550
these gradients

1835
01:55:35,530 --> 01:55:36,920
timestamp so you

1836
01:55:37,110 --> 01:55:41,020
the gradient there

1837
01:55:41,020 --> 01:55:45,340
and then you just multiply by w transpose w

1838
01:55:45,360 --> 01:55:52,350
there are some very funny theoretical justification so why you would like to do is

1839
01:55:52,380 --> 01:55:58,490
they say that you know we are not optimisation optimizing in space of matrices which

1840
01:55:58,700 --> 01:55:59,630
was quite

1841
01:55:59,650 --> 01:56:04,620
over the same rules as the ordinary vector space so that's why

1842
01:56:04,630 --> 01:56:09,410
these multiplication of the gradient by this kind of the fact that gives you like

1843
01:56:09,480 --> 01:56:13,310
better direction of steepest descent

1844
01:56:13,360 --> 01:56:18,500
and there are also some complicated statistical justifications for this kind of stuff but

1845
01:56:28,690 --> 01:56:33,760
it doesn't really district you are searching anyway just modifies the direction in which you go

1846
01:56:34,350 --> 01:56:38,080
the gradient algorithms and it is proposed that this

1847
01:56:38,100 --> 01:56:40,330
this direction is actually like better

1848
01:56:40,590 --> 01:56:44,290
i mean it's kind of you know like conjugate gradient all those methods you always

1849
01:56:44,290 --> 01:56:47,900
try to make for find directions which are better than just the direction of the

1850
01:56:47,900 --> 01:56:52,530
gradient so the idea is the same system was much simpler than what the band

1851
01:56:53,990 --> 01:57:00,060
but so for some people say that this gives you a better direction and the

1852
01:57:00,060 --> 01:57:04,540
bonuses that the same time you get rid of this matrix inversion here because you

1853
01:57:04,540 --> 01:57:08,760
spend is you can see that in there and what you end up with this

1854
01:57:08,760 --> 01:57:13,030
kind of very simple reason

1855
01:57:13,050 --> 01:57:19,600
so actually nobody uses the original gradient descent everybody who uses

1856
01:57:19,900 --> 01:57:22,350
gradient algorithms uses this

1857
01:57:22,370 --> 01:57:30,260
what this version which is called either natural or the adaptive gradient

1858
01:57:30,270 --> 01:57:32,660
but then now then

1859
01:57:32,690 --> 01:57:35,350
some advertisement for my algorithm

1860
01:57:35,360 --> 01:57:39,790
and then what i developed is fixed point algorithm that is trying to be a

1861
01:57:39,790 --> 01:57:40,610
kind of

1862
01:57:40,620 --> 01:57:44,260
approximate newton

1863
01:57:44,330 --> 01:57:48,030
the point is that course in the need in algorithm one of the main problem

1864
01:57:48,030 --> 01:57:51,730
is that you need to estimate the hessian and then inverted at every step which

1865
01:57:51,730 --> 01:57:57,430
is quite quite cumbersome but the the point is that because we know

1866
01:57:57,460 --> 01:58:02,150
something about the specific form of of the objective function that we have here

1867
01:58:02,170 --> 01:58:06,830
it turns out that we can actually find a very nice approximation of the needs

1868
01:58:06,960 --> 01:58:11,420
of of the hessian matrix is diagonal and can be

1869
01:58:11,880 --> 01:58:13,300
trivial inverted

1870
01:58:14,100 --> 01:58:15,160
what is so

1871
01:58:15,180 --> 01:58:18,050
if we assume that the data follows the ICA model

1872
01:58:18,060 --> 01:58:20,590
we can actually find another reason

1873
01:58:20,600 --> 01:58:25,010
that has the good properties of of the newton algorithm that means it has quadratic

1874
01:58:25,010 --> 01:58:29,940
convergence actually in some cases you have a cubic convergence

1875
01:58:29,960 --> 01:58:33,530
well it is also the nice thing that you don't need to figure out what

1876
01:58:33,680 --> 01:58:38,920
will step by step size parameter it automatically

1877
01:58:38,930 --> 01:58:45,750
so after some twenty algebraic manipulations what you see is that for estimating one independent

1878
01:58:45,750 --> 01:58:49,840
component you actually get this kind of a rather simple algorithm

1879
01:58:49,890 --> 01:58:53,670
which really doesn't look like anything because it's it's really

1880
01:58:53,790 --> 01:58:59,650
one of the setting sun completely completely disappear when you when you make some assumptions

1881
01:58:59,650 --> 01:59:00,700
and right

1882
01:59:02,990 --> 01:59:07,430
and you can do the same thing for the likelihood in which case you get

1883
01:59:07,430 --> 01:59:09,910
what you see that

1884
01:59:10,050 --> 01:59:12,420
what you get is is something very similar

1885
01:59:12,480 --> 01:59:17,350
so the case of the likelihood we just well this is in the case of

1886
01:59:17,350 --> 01:59:23,480
plasticity is not an adaptive stochastic algorithm we have expectation and then we have

1887
01:59:23,650 --> 01:59:28,070
instead of the identity we have here some diagonal matrix that is to be estimate

1888
01:59:28,130 --> 01:59:31,890
that is to be computed from the data and here we have another diagonal matrix

1889
01:59:32,160 --> 01:59:33,630
which kind of gives you

1890
01:59:34,310 --> 01:59:38,760
the optimal step size and this may explain

1891
01:59:41,410 --> 01:59:43,090
OK so this this whole

1892
01:59:43,110 --> 01:59:47,850
simplification is possible only if we assume that the data exactly follows the ICA model

1893
01:59:47,850 --> 01:59:53,560
and we have infinite data so it is certainly in reality what happens in practice

1894
01:59:53,560 --> 02:00:00,890
we don't have exactly what convergence but it seems to work quite well in

1895
02:00:00,910 --> 02:00:04,600
so yes some examples so

1896
02:00:05,400 --> 02:00:08,090
here we start

1897
02:00:08,190 --> 02:00:13,340
we estimate we have this whitened uniform data as previously and then we start with

1898
02:00:13,340 --> 02:00:19,100
the vector which points in this direction also you can see from what actually happens

1899
02:00:19,380 --> 02:00:22,560
we start from the back that's in this direction

1900
02:00:22,570 --> 02:00:28,110
actually pretty close to one of infinite independent components then

1901
02:00:28,140 --> 02:00:33,200
now sorry we we start somewhere else that the starting by starting point is not

1902
02:00:33,200 --> 02:00:34,060
shown here

1903
02:00:34,060 --> 02:00:38,990
but then which is a bit stupid but then the after the first iteration we

1904
02:00:38,990 --> 02:00:41,440
get here and the second iteration we get here

1905
02:00:41,900 --> 02:00:45,850
so in the second iteration is practically oriented in the direction of one of the

1906
02:00:45,850 --> 02:00:50,850
independent components which are lined with these so edges

1907
02:00:50,910 --> 02:00:53,500
and also what you see here is again this this

1908
02:00:53,510 --> 02:00:58,150
one of this indeterminate is in ICA it doesn't really matter whether we were the

1909
02:00:58,150 --> 02:01:02,600
vector points in this direction of i the very opposite direction it is just the

1910
02:01:02,600 --> 02:01:08,600
same independent component because the size of the different components are not defined

1911
02:01:08,650 --> 02:01:12,790
of this one thousand back again

1912
02:01:14,700 --> 02:01:19,040
and then some words about relations to other matters

1913
02:01:19,050 --> 02:01:25,830
so i ordered mention this technique of projection pursuit that has been proposed by

1914
02:01:25,850 --> 02:01:30,670
statisticians for completely different purposes that i see

1915
02:01:30,710 --> 02:01:35,710
it is expected data analysis where people those people claim that when you find among

1916
02:01:35,740 --> 02:01:41,360
the most nongaussian directions those argued so visualizing your data and so you know

1917
02:01:41,410 --> 02:01:47,520
getting some insight into what happens in and especially people claim that when you find

1918
02:01:47,520 --> 02:01:52,560
the most nongaussian directions projections of your data those will tell you something about the

1919
02:01:52,560 --> 02:01:54,080
clustering structure

1920
02:01:54,100 --> 02:01:59,890
of the data so here is an illustration of why that might happen

1921
02:02:01,610 --> 02:02:05,520
we have basically so god sent clouds

1922
02:02:05,520 --> 02:02:07,440
which are clearly separated

1923
02:02:07,460 --> 02:02:08,370
but now

1924
02:02:08,390 --> 02:02:12,890
if you if you take the principal components of the study that they will be

1925
02:02:14,080 --> 02:02:18,100
and while the projection on the vertical axis will not tell you that you have

1926
02:02:18,110 --> 02:02:19,810
two different cluster

1927
02:02:19,830 --> 02:02:22,600
but if you look at the most nongaussian

1928
02:02:23,370 --> 02:02:27,790
projections while the most nongaussian projection will be

1929
02:02:27,800 --> 02:02:30,300
the horizontal line so that we will tell you

1930
02:02:30,350 --> 02:02:33,880
that there are exactly two clusters so the point is that if you have for

1931
02:02:33,880 --> 02:02:39,460
example if you have a bimodal distributions which take to show you the clustering

1932
02:02:39,480 --> 02:02:41,800
the those projections will be

