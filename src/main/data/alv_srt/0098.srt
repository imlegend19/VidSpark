1
00:00:00,000 --> 00:00:04,490
its velocity certain decreases as a result of this collision

2
00:00:04,750 --> 00:00:06,910
but that's not the end of the story

3
00:00:07,580 --> 00:00:10,240
that's not the end of the story

4
00:00:10,950 --> 00:00:17,370
what happens here is that this electron that's excited is there in an unsustainable basis

5
00:00:17,380 --> 00:00:22,560
it's just a one shot deal so the electrons wants to fall back down to

6
00:00:22,560 --> 00:00:27,200
the lower energy state and it does it falls back down and when it does

7
00:00:27,200 --> 00:00:31,900
so it gives off that energy in the form of a photon

8
00:00:31,960 --> 00:00:35,350
so we have emitted photon

9
00:00:36,160 --> 00:00:45,840
and it has energy each of new on H C over lambda 4 inch Nevada

10
00:00:45,850 --> 00:00:53,280
and it is the set of these emitted photons that comes from studies on Armstrong

11
00:00:53,290 --> 00:01:00,310
was studying these emitted photon and clearly there is some relationship between the wavelengths that

12
00:01:00,310 --> 00:01:05,850
he observed and the energy levels within the atomic hydrogen

13
00:01:06,080 --> 00:01:11,520
and all this this whole process is subject to a conservation so let's get them

14
00:01:11,640 --> 00:01:15,730
with some make sure that we don't violate any basic losses were going to say

15
00:01:15,730 --> 00:01:22,400
that the energy of the incident electron will then be spread across

16
00:01:22,600 --> 00:01:31,750
the electron energy of transition within the target and whatever's left over will remain with

17
00:01:31,770 --> 00:01:33,700
the electron some reduced

18
00:01:36,460 --> 00:01:42,370
and what shown here and 1 applying it to the experiment that strong conducted in

19
00:01:42,370 --> 00:01:47,850
which accelerated electrons but it's the same calculation for any particle

20
00:01:47,920 --> 00:01:56,970
so this is for all incident particles be subject to the all incident particles at

21
00:01:56,970 --> 00:02:03,230
all incident particle so it could be obviously electrons we can accelerate protons we can

22
00:02:03,230 --> 00:02:09,890
accelerate neutrons we can accelerate alpha particles we accelerate argon ions there's a host of

23
00:02:09,890 --> 00:02:17,610
incident particles the consider them only as bearers of energy the energy transfer vehicles and

24
00:02:17,610 --> 00:02:24,220
the transfer energy here provided that the energy is high enough to cause excitation

25
00:02:24,800 --> 00:02:30,810
there another way of looking at conceptually is this is a probe being April being

26
00:02:30,850 --> 00:02:37,340
or maybe a stimulus and the integral of

27
00:02:37,350 --> 00:02:45,150
the energy of the particle after interactions and the emitted photons that collectivity is the

28
00:02:46,560 --> 00:02:51,100
this is the response

29
00:02:51,380 --> 00:02:57,480
this is the response so what is this made this means that we could interrogate

30
00:02:58,580 --> 00:03:07,480
Adam of unknown identity and on the basis of these values in infra deduce the

31
00:03:07,480 --> 00:03:12,440
identity of that species and we talked at the end of last day about how

32
00:03:12,440 --> 00:03:17,420
this is applied in analyzing the chemical composition of stars

33
00:03:17,440 --> 00:03:20,700
so the radiation goes a long way

34
00:03:21,080 --> 00:03:26,800
and were able to use the information contained therein to conduct analysis

35
00:03:28,970 --> 00:03:35,440
let's see how this will help us to reconcile the boar model to bombers analysis

36
00:03:35,470 --> 00:03:40,610
of angstroms data so let's see I think we have to go back to to

37
00:03:40,610 --> 00:03:43,720
the boar model to go over to

38
00:03:43,770 --> 00:03:51,470
things decided that wants to sleep time finally see it's Friday the computers decided it's

39
00:03:51,510 --> 00:03:55,770
a goof off a little bit so we're going to use a partial No. 6

40
00:03:56,100 --> 00:04:00,300
in order to help elucidate what's going on here so we can then resume here

41
00:04:00,300 --> 00:04:06,420
and say that delta of the transition is the energy of the final state minus

42
00:04:06,420 --> 00:04:10,350
the energy of the initial state that's always the case we always say that the

43
00:04:10,390 --> 00:04:16,460
difference in those final minus initial and we can represent that of the product k

44
00:04:16,730 --> 00:04:24,560
squared 1 over n final squared minus 1 over an initial square and conservation of

45
00:04:24,560 --> 00:04:26,770
energy says that must be

46
00:04:26,820 --> 00:04:33,390
the value of the energy imparted to the emitted photons so the emitted photon if

47
00:04:33,390 --> 00:04:39,770
the photon this is emitted when the electron cascades back down and that will be

48
00:04:39,770 --> 00:04:45,730
h c over land so let's look at this thing around let's look this thing

49
00:04:45,730 --> 00:04:49,010
around and we will get there as new bar

50
00:04:49,080 --> 00:04:55,340
1 1 over lambda then will be the case squared over the product of the

51
00:04:55,350 --> 00:05:01,180
Planck constant times the speed of light 1 over n final squared minus 1 over

52
00:05:01,190 --> 00:05:09,610
n initial squared now for the Balmer series for Balmer series

53
00:05:10,030 --> 00:05:15,470
we have a set of lines for which the end member was always equals 2

54
00:05:15,470 --> 00:05:20,960
so we're dealing with the comic hydrogen so let's put z equals 1 and let's

55
00:05:20,960 --> 00:05:25,770
put the final state as any equals to

56
00:05:26,560 --> 00:05:31,970
and furthermore if we evaluate k over H.

57
00:05:32,060 --> 00:05:36,510
using values that we already know we get 1 . 1 times 10 to the

58
00:05:36,510 --> 00:05:41,300
7th reciprocal metres so if you put all that if you get 1 . 1

59
00:05:41,300 --> 00:05:42,870
times 10 to the minus

60
00:05:43,440 --> 00:05:48,600
is we tend to the plus 7 1 over 2 squared minus 1 over and

61
00:05:48,690 --> 00:05:53,680
i squared and i has to be greater than so that necessarily means that it

62
00:05:53,680 --> 00:06:00,060
must begin at 3 could go for 5 etc. will that's exactly the ball in

63
00:06:01,340 --> 00:06:03,970
we got the ball reliance

64
00:06:03,990 --> 00:06:05,340
out of the formal

65
00:06:06,360 --> 00:06:09,940
identical to ball

66
00:06:13,660 --> 00:06:20,080
so border has been able to reconcile with angstroms data taken at half a century

67
00:06:20,080 --> 00:06:27,100
earlier but furthermore boss suggests other experiments for example does not bother you that the

68
00:06:27,100 --> 00:06:32,460
lines and at n equals 2 you know that according to the energetics here the

69
00:06:32,460 --> 00:06:37,340
electrons have to keep cascading down equals 1 uses some funny going on in in

70
00:06:37,340 --> 00:06:42,750
Sweden maybe electrons in Sweden there'll be nervous maybe it's winter maybe they're afraid to

71
00:06:42,750 --> 00:06:47,510
go down and equals 1 how come we don't get any equals still

72
00:06:47,750 --> 00:06:53,250
well let's look for a 2nd here

73
00:06:53,300 --> 00:06:54,580
you look at the

74
00:06:57,070 --> 00:07:00,770
the sleep function is kicking in

75
00:07:01,060 --> 00:07:07,030
this is hard drive the rico so let's take a look at the electromagnetic spectrum

76
00:07:07,110 --> 00:07:10,370
this is taken from your book and you see again I've thinking about you all

77
00:07:10,370 --> 00:07:15,920
the time I got on my pants and his and of and the doctor says

78
00:07:15,920 --> 00:07:17,490
this is the to very good

79
00:07:18,220 --> 00:07:24,900
so here we are with the wavelength on the upper trace showing wavelength increasing from

80
00:07:24,960 --> 00:07:29,560
right to left and energy increases from left to right so high energy is lower

81
00:07:29,560 --> 00:07:35,240
wavelength 1 point here that irritates me that the SI unit of of frequency is

82
00:07:35,240 --> 00:07:41,180
hertz which means that beats per 2nd it's it's not the perceptive and this is

83
00:07:41,180 --> 00:07:42,040
a little

84
00:07:42,060 --> 00:07:46,110
sloppiness on the part of the text and so here we've got is at the

85
00:07:46,110 --> 00:07:51,650
very very low end of wavelength very high energy gamma rays cosmic rays and then

86
00:07:51,650 --> 00:07:55,130
over here at the low end of energy or the gentle

87
00:07:55,350 --> 00:08:02,230
forms of radiation radio TV microwaves over here and then in the center drawn is

88
00:08:02,650 --> 00:08:07,990
visible light 1 expand and here's the visible spectrum which in round numbers I'd like

89
00:08:07,990 --> 00:08:11,180
you to know a little bit of so they get educated person should know a

90
00:08:11,180 --> 00:08:14,740
all networks framework and i want to know about the brain i i went to

91
00:08:14,740 --> 00:08:19,110
the extent of actually getting a peachy in neuroscience because i was interested in how

92
00:08:19,110 --> 00:08:21,930
the brain works i don't care how the brain works and he was much more

93
00:08:21,930 --> 00:08:22,970
fun to do machine learning

94
00:08:25,380 --> 00:08:29,460
so there was a community that came into the field through the route

95
00:08:30,720 --> 00:08:34,200
there is a community that came into the field from databases

96
00:08:34,900 --> 00:08:39,180
where they had lots and databases and they wanted to do interesting operations on those

97
00:08:39,180 --> 00:08:42,450
databases there are more computer science e in their thinking so

98
00:08:42,990 --> 00:08:45,030
they are more the data mining community

99
00:08:45,710 --> 00:08:51,330
community there's the uncertainty in artificial intelligence community which came out from graphical models

100
00:08:52,010 --> 00:08:52,500
there is

101
00:08:52,960 --> 00:08:58,380
the ice email community which is machine that they i people wanted to machine learning with

102
00:08:58,810 --> 00:09:00,560
rule-based machine learning originally

103
00:09:01,140 --> 00:09:02,680
and the interesting thing is there

104
00:09:03,960 --> 00:09:06,330
well and then there's the statisticians let's not forget them

105
00:09:08,630 --> 00:09:12,400
the interesting thing is that all these ships were on a collision course

106
00:09:13,490 --> 00:09:14,040
so now

107
00:09:14,670 --> 00:09:18,720
there are doing much more similar things but when you go into

108
00:09:19,380 --> 00:09:23,690
the data mining conference the flavour of things that you get is much more

109
00:09:24,240 --> 00:09:29,820
historically based on the idea big databases large scale but simple forms a machine learning

110
00:09:33,710 --> 00:09:37,780
that's that's one question i can move on i don't need to question go ahead

111
00:09:38,770 --> 00:09:40,570
model comparison to the

112
00:09:44,170 --> 00:09:45,170
yeah i mean

113
00:09:48,770 --> 00:09:52,300
okay this is a very good question i'll come to this in in admitted

114
00:09:53,130 --> 00:09:55,530
in model comparison here at least

115
00:09:56,000 --> 00:09:59,360
if i wanna compared to models what i need to be able to do is

116
00:09:59,360 --> 00:10:02,720
integrate out the parameters for those two models okay

117
00:10:03,610 --> 00:10:06,210
but if i have a larger space of models

118
00:10:06,830 --> 00:10:11,420
and then one of the tricky things about bayesian inference is that i need to be able to somehow

119
00:10:12,450 --> 00:10:17,750
ah enumerated either implicitly or explicitly all the models i'm willing to consider

120
00:10:18,210 --> 00:10:20,790
this is sometimes called the closed world hypothesis

121
00:10:23,400 --> 00:10:23,640
you know

122
00:10:25,370 --> 00:10:26,030
i have to

123
00:10:26,620 --> 00:10:32,970
somehow specify generative process over models that could be very sophisticated for example

124
00:10:33,360 --> 00:10:34,780
you could have a grammar

125
00:10:35,360 --> 00:10:38,390
over models that would generate more and more complicated models

126
00:10:38,790 --> 00:10:42,910
as long as you're the probability mass that you associate to all possible models in

127
00:10:42,910 --> 00:10:47,230
you are in your clothes world are universal models sums to one

128
00:10:47,860 --> 00:10:49,250
are there are at most one

129
00:10:51,640 --> 00:10:53,190
so it's a very good question

130
00:10:55,560 --> 00:10:56,500
yeah the question

131
00:11:03,080 --> 00:11:03,960
i it seems

132
00:11:08,620 --> 00:11:10,540
i have and i use the symbol key

133
00:11:11,210 --> 00:11:13,020
uniformly on this page okay

134
00:11:13,550 --> 00:11:14,280
which means

135
00:11:15,010 --> 00:11:16,350
they all probabilities

136
00:11:17,710 --> 00:11:20,370
is the probability of the data given the data

137
00:11:22,630 --> 00:11:25,860
a lot of people confusingly say

138
00:11:26,530 --> 00:11:31,710
the likelihood of the data because it you know they use the word likelihood closely for probability

139
00:11:33,790 --> 00:11:39,290
probably if you look in any statistics textbook this is the likelihood of data which

140
00:11:39,290 --> 00:11:42,950
is it's a function of data that is not a probability function okay

141
00:11:44,810 --> 00:11:47,120
you know alternatively four bayesian

142
00:11:47,560 --> 00:11:50,790
the likelihood of the data is just the probability of the data given data

143
00:11:59,760 --> 00:12:02,050
yeah it's it's that this is

144
00:12:02,440 --> 00:12:06,810
properties the probability over theta log likelihood is not a probability over theta

145
00:12:07,410 --> 00:12:09,440
the likelihood is not a probability distribution

146
00:12:14,300 --> 00:12:15,150
right let me move on

147
00:12:16,450 --> 00:12:18,050
so that's it but it's not totally

148
00:12:19,640 --> 00:12:21,860
here the questions i want to address

149
00:12:22,350 --> 00:12:23,350
so first of all

150
00:12:24,700 --> 00:12:25,430
you know what i he

151
00:12:26,230 --> 00:12:30,640
what are the foundations of bayesianism why are some people

152
00:12:31,540 --> 00:12:37,160
using this basic methodology i actually think it's weird i don't like the term bayesian it makes it sound

153
00:12:37,620 --> 00:12:38,470
different somehow

154
00:12:39,210 --> 00:12:40,080
like it's some choice

155
00:12:42,770 --> 00:12:46,000
you know we don't say why are we all calculation lists

156
00:12:46,600 --> 00:12:48,290
right because we use calculus

157
00:12:50,170 --> 00:12:52,330
you know i i think that this is basically

158
00:12:52,800 --> 00:12:56,320
you know this comes down to why why we using probability theory to

159
00:12:56,780 --> 00:12:58,710
answer questions from probability theory well

160
00:12:59,290 --> 00:12:59,520
you know

161
00:12:59,930 --> 00:13:03,280
what else could we use right of-course there is the question

162
00:13:04,020 --> 00:13:07,580
are there are other ways of doing data analysis and modeling and that's certainly true

163
00:13:08,240 --> 00:13:09,990
right there are many other ways of doing

164
00:13:11,430 --> 00:13:13,660
data analysis and machine learning

165
00:13:15,070 --> 00:13:15,890
so i

166
00:13:16,010 --> 00:13:16,710
a lot of people

167
00:13:18,130 --> 00:13:18,650
well you know

168
00:13:19,120 --> 00:13:22,860
are worried about this question prior come from try to talk about the

169
00:13:23,330 --> 00:13:26,120
and then how do we do these integrals is the practical question

170
00:13:28,860 --> 00:13:29,900
here is one way

171
00:13:30,960 --> 00:13:32,250
that you can come at the

172
00:13:32,990 --> 00:13:37,280
bayesian framework and this is if you want to think about artificial intelligence problems

173
00:13:38,890 --> 00:13:40,820
imagine you have a robot ann

174
00:13:41,650 --> 00:13:43,870
in order for this robot to behave intelligently

175
00:13:44,360 --> 00:13:45,020
he needs to

176
00:13:46,040 --> 00:13:49,800
be able to represent its beliefs about some propositions in the world

177
00:13:50,250 --> 00:13:50,580
thank you know

178
00:13:52,750 --> 00:13:52,980
you know

179
00:13:53,600 --> 00:13:56,580
whereas the charging location nor is this

180
00:13:57,330 --> 00:14:01,130
sensor malfunctioning or something like that is about get shot

181
00:14:07,430 --> 00:14:12,330
the question is now not a question about statistics at all it's a question about

182
00:14:12,330 --> 00:14:14,530
how does the robot represent beliefs

183
00:14:16,150 --> 00:14:17,870
we want a framework that

184
00:14:18,380 --> 00:14:23,160
we can use to represent the strength of these beliefs numerically in the brain of the robot

185
00:14:24,960 --> 00:14:28,860
and we want to know the mathematical rules for manipulating these beliefs

186
00:14:29,720 --> 00:14:32,680
so it's basically like you know the rules of logic

187
00:14:33,330 --> 00:14:35,050
right but extended

188
00:14:36,080 --> 00:14:37,380
captures transitivity

189
00:14:39,420 --> 00:14:39,970
so let's

190
00:14:41,680 --> 00:14:42,820
start with china

191
00:14:43,380 --> 00:14:45,030
describe what we mean a little bit

192
00:14:46,300 --> 00:14:51,580
let's say be have x represents the strength of belief or possibility and some proposition x

193
00:14:53,110 --> 00:14:57,630
and we're gonna say it ranges between zero and one will be a lexical zero

194
00:14:58,060 --> 00:14:58,680
means there

195
00:14:59,210 --> 00:15:01,910
the robot believes x is definitely not true

196
00:15:03,160 --> 00:15:07,430
but doesn't have to mean that x is definitely not true this just a statement about robots belief

197
00:15:07,950 --> 00:15:09,600
being x equals one means that

198
00:15:10,570 --> 00:15:12,880
role believes that x is definitely true

199
00:15:13,910 --> 00:15:16,000
and we're going to be a xed given why

200
00:15:17,440 --> 00:15:21,200
to represent the strength of belief that x is true given that we know

201
00:15:21,770 --> 00:15:22,520
otherwise true

202
00:15:24,880 --> 00:15:25,350
so now

203
00:15:25,820 --> 00:15:28,060
we want to be able to manipulate these beliefs

204
00:15:28,710 --> 00:15:29,280
and if

205
00:15:31,790 --> 00:15:32,210
if you do

206
00:15:32,990 --> 00:15:35,130
accept certain axioms

207
00:15:35,870 --> 00:15:39,030
and these go all the way back to cox in nineteen forty six

208
00:15:39,750 --> 00:15:43,470
and i've been sort developed over time by james and there's a really nice

209
00:15:44,720 --> 00:15:46,180
more recently by the harder

210
00:15:47,850 --> 00:15:49,990
so it goes on these lines if we're gonna say

211
00:15:50,490 --> 00:15:54,680
strength beliefs degrees a problem plausibility are represented by real numbers

212
00:15:55,250 --> 00:15:57,790
and we want some qualitative correspondence to

213
00:15:58,380 --> 00:16:01,930
really common sense in terms of how we manipulate these things

214
00:16:01,930 --> 00:16:03,620
OK so what we did now the

215
00:16:03,640 --> 00:16:07,350
let's say that the document first we went to see scores

216
00:16:07,370 --> 00:16:10,410
the site suggested a couple of things that he does not about is good if

217
00:16:10,440 --> 00:16:11,560
you could enter

218
00:16:11,580 --> 00:16:12,520
so there

219
00:16:12,620 --> 00:16:15,670
that it considered in part based on

220
00:16:15,690 --> 00:16:19,210
but it's not about corporations

221
00:16:19,210 --> 00:16:23,390
you know i really really analyse the document it's using these

222
00:16:23,440 --> 00:16:26,310
already using this information with the and two

223
00:16:26,310 --> 00:16:30,370
and new concepts you can that it holds

224
00:16:30,410 --> 00:16:31,640
applicable to the

225
00:16:37,620 --> 00:16:41,230
well let's face it finishes and thing

226
00:16:46,500 --> 00:16:48,410
seeing analyse

227
00:16:48,410 --> 00:16:55,350
having the same sort of

228
00:16:55,370 --> 00:17:05,290
OK it's done now

229
00:17:05,350 --> 00:17:13,270
OK so another thing you can see the document concept

230
00:17:13,310 --> 00:17:15,850
so this is a list of all the things that appear

231
00:17:15,900 --> 00:17:18,640
in this article in the basic

232
00:17:18,730 --> 00:17:22,890
this is the most prominent funny sitka systems

233
00:17:22,940 --> 00:17:25,750
there are also other like area

234
00:17:25,810 --> 00:17:27,930
a billion

235
00:17:28,000 --> 00:17:29,710
router and so on

236
00:17:29,870 --> 00:17:31,940
then we can be control

237
00:17:32,000 --> 00:17:34,160
these parameters so we can go from

238
00:17:34,210 --> 00:17:36,190
more specific

239
00:17:36,250 --> 00:17:37,670
so would be

240
00:17:37,710 --> 00:17:42,230
o devices that actually mention here to ones that our generosity civic landings of the

241
00:17:42,250 --> 00:17:46,730
legalization taxonomy

242
00:17:46,790 --> 00:17:49,210
in inanimate objects

243
00:17:49,580 --> 00:17:54,730
organisation for example

244
00:17:54,730 --> 00:17:57,430
it's more it's a more general

245
00:17:57,500 --> 00:17:58,660
now that

246
00:17:58,750 --> 00:18:02,830
the company of his car business this is

247
00:18:02,830 --> 00:18:04,730
the concept for the of them

248
00:18:04,770 --> 00:18:06,620
this is one thing we can control

249
00:18:06,940 --> 00:18:11,410
the other thing is frequencies so of with the most frequently courses course system

250
00:18:11,540 --> 00:18:13,620
official everything down here

251
00:18:13,730 --> 00:18:15,540
this is

252
00:18:15,560 --> 00:18:18,790
all the things that are identified

253
00:18:18,850 --> 00:18:21,620
you got more general than legend

254
00:18:21,770 --> 00:18:25,390
and i think it's because he here is this disambiguation

255
00:18:25,440 --> 00:18:28,100
problems for example

256
00:18:28,160 --> 00:18:32,370
essential here is if you go to

257
00:18:32,430 --> 00:18:38,310
each equitable nice

258
00:18:38,330 --> 00:18:43,980
you to make it work really hard to find

259
00:18:44,100 --> 00:18:48,040
it's the boom is both part

260
00:18:48,040 --> 00:18:49,660
little MG

261
00:18:49,730 --> 00:18:52,320
and let's make this distance

262
00:18:54,970 --> 00:18:56,110
and now

263
00:18:56,120 --> 00:18:56,980
we're going to

264
00:18:57,030 --> 00:18:59,130
we do all these calculations

265
00:18:59,190 --> 00:19:01,450
we start completely from scratch

266
00:19:01,460 --> 00:19:03,350
the sum of all forces

267
00:19:03,530 --> 00:19:05,940
x direction has to be zero

268
00:19:05,950 --> 00:19:07,530
no change

269
00:19:07,540 --> 00:19:10,130
and of must be

270
00:19:10,220 --> 00:19:11,750
f of f

271
00:19:11,810 --> 00:19:16,190
now the sum of all forces in the y direction has to be or not

272
00:19:17,600 --> 00:19:20,950
so now we have that and of q

273
00:19:20,990 --> 00:19:23,270
must become larger

274
00:19:23,360 --> 00:19:24,910
must be equal to

275
00:19:24,950 --> 00:19:26,650
capital and

276
00:19:26,700 --> 00:19:28,180
please little and

277
00:19:28,200 --> 00:19:29,690
times g

278
00:19:29,700 --> 00:19:32,150
so the maximum

279
00:19:32,160 --> 00:19:33,110
the fraction

280
00:19:33,120 --> 00:19:35,420
which is mute times and q

281
00:19:35,430 --> 00:19:36,960
now becomes mu

282
00:19:36,980 --> 00:19:39,050
times and

283
00:19:40,070 --> 00:19:43,090
i'm to sort of maximum friction goes up

284
00:19:43,960 --> 00:19:48,290
we need to talk and i think my point q relative to point q

285
00:19:49,940 --> 00:19:51,970
well the first two terms

286
00:19:52,030 --> 00:19:55,810
i haven't changed so i have enough

287
00:19:55,900 --> 00:19:57,880
times l

288
00:19:57,930 --> 00:19:59,910
sign of

289
00:20:02,450 --> 00:20:05,000
and one l

290
00:20:05,060 --> 00:20:06,740
times the cosine of

291
00:20:06,800 --> 00:20:10,060
but now we have a third term

292
00:20:10,070 --> 00:20:11,660
namely this

293
00:20:11,670 --> 00:20:13,330
position vector

294
00:20:13,330 --> 00:20:14,950
and this force

295
00:20:15,050 --> 00:20:19,650
so now we're going to get this distance which is the cosine of five

296
00:20:19,700 --> 00:20:21,120
and this force

297
00:20:21,180 --> 00:20:22,430
so we have here

298
00:20:24,110 --> 00:20:25,410
and g

299
00:20:25,420 --> 00:20:26,550
times the

300
00:20:26,600 --> 00:20:27,690
cosine of

301
00:20:28,250 --> 00:20:29,210
that now

302
00:20:29,220 --> 00:20:31,710
equals zero

303
00:20:31,770 --> 00:20:33,870
so i'm going to take an of

304
00:20:33,920 --> 00:20:34,770
out of here

305
00:20:38,440 --> 00:20:39,810
i can take

306
00:20:39,820 --> 00:20:43,110
g cosine of out

307
00:20:43,240 --> 00:20:47,530
so if i take the cosine of out i have

308
00:20:47,570 --> 00:20:51,740
ml divided by two left over

309
00:20:53,180 --> 00:20:54,530
o mt

310
00:20:56,220 --> 00:21:00,550
and then i have to divide by all sin theta

311
00:21:00,590 --> 00:21:02,020
the number of

312
00:21:03,450 --> 00:21:04,740
and so now i can

313
00:21:04,750 --> 00:21:10,060
right for cosine of divided by sign of i can write code tangent of and

314
00:21:10,060 --> 00:21:11,250
i'll bring dl

315
00:21:11,280 --> 00:21:12,880
inside here

316
00:21:12,960 --> 00:21:14,750
so i have g

317
00:21:14,810 --> 00:21:16,620
attention of five

318
00:21:19,200 --> 00:21:21,340
and over two

319
00:21:22,730 --> 00:21:24,130
they and

320
00:21:24,190 --> 00:21:26,050
divided by l

321
00:21:26,160 --> 00:21:28,810
and that now must be

322
00:21:28,850 --> 00:21:31,450
the frictional force because and p

323
00:21:31,480 --> 00:21:32,740
is still

324
00:21:32,750 --> 00:21:35,470
the frictional force

325
00:21:35,620 --> 00:21:37,240
but me make sure

326
00:21:37,250 --> 00:21:39,410
that i have these rights

327
00:21:40,040 --> 00:21:40,770
i do

328
00:21:41,610 --> 00:21:45,120
code engine of m over two little and the of l

329
00:21:45,180 --> 00:21:46,840
that is the frictional force

330
00:21:46,920 --> 00:21:51,550
notice that the frictional force

331
00:21:51,590 --> 00:21:54,160
it's going up because this term is added

332
00:21:54,200 --> 00:21:56,200
and we didn't have that term before

333
00:21:56,240 --> 00:21:58,200
before we only had this term

334
00:21:58,200 --> 00:21:59,910
and you see here

335
00:21:59,950 --> 00:22:04,150
so your first thought maybe that the situation has become more dangerous

336
00:22:04,200 --> 00:22:06,150
because if there is more friction

337
00:22:07,170 --> 00:22:12,070
and then i will set exactly at the critical point is hanging on their farms

338
00:22:12,070 --> 00:22:13,660
so if the friction

339
00:22:13,670 --> 00:22:14,630
goes up

340
00:22:14,650 --> 00:22:15,550
you may say

341
00:22:15,560 --> 00:22:16,570
my goodness

342
00:22:16,620 --> 00:22:18,720
it will probably start slide

343
00:22:20,570 --> 00:22:25,020
which overlooked then is that the maximum friction has also gone

344
00:22:25,030 --> 00:22:25,960
and so

345
00:22:25,970 --> 00:22:27,800
we have to evaluate this now

346
00:22:27,810 --> 00:22:30,600
in comparison with the maximum friction

347
00:22:30,620 --> 00:22:32,110
and the best way to do it

348
00:22:32,120 --> 00:22:34,740
is to think of this first

349
00:22:34,810 --> 00:22:37,380
as making decl zero

350
00:22:37,530 --> 00:22:42,110
so we said the person starts at the bottom of the ladder

351
00:22:42,170 --> 00:22:46,090
and we asked this person to gradually climb up

352
00:22:46,150 --> 00:22:49,240
i notice when d equals zero

353
00:22:49,410 --> 00:22:52,150
that the frictional force is exactly the same

354
00:22:52,200 --> 00:22:53,830
what it was before

355
00:22:53,840 --> 00:22:55,330
there is no difference

356
00:22:55,360 --> 00:22:57,700
that frictional force has not changed

357
00:22:57,750 --> 00:22:59,600
and equals zero

358
00:22:59,650 --> 00:23:01,360
but what has changed

359
00:23:01,420 --> 00:23:03,190
is the maximum friction

360
00:23:03,230 --> 00:23:05,850
the maximum friction has little and it

361
00:23:05,910 --> 00:23:09,450
and that's independent of the there's nobody anywhere here

362
00:23:09,530 --> 00:23:12,700
so if the maximum friction goes up

363
00:23:12,740 --> 00:23:17,360
and the fiction itself remains the same clearly the never has become more stable

364
00:23:17,360 --> 00:23:23,220
how scattered around are are the search results right how many how many other Web pages are inbetween

365
00:23:23,220 --> 00:23:29,900
our disconnected search results and once we'll have this we'll be doing some machine learning to

366
00:23:29,900 --> 00:23:31,960
do predictions on these graphs

367
00:23:32,680 --> 00:23:40,360
so this is one type of question you can ask right I I take such results I create they give me

368
00:23:40,360 --> 00:23:44,820
such a sub graph right these are the box the red boxes are such results

369
00:23:44,820 --> 00:23:48,840
the number in them is the rank so this would be the top rank search results

370
00:23:48,840 --> 00:23:53,140
second rank search results third rank search result and the question is is this

371
00:23:53,140 --> 00:23:57,660
a good set of search results or not so if if given a query and all these webpages

372
00:23:57,660 --> 00:24:01,620
you go to a human and ask for every webpage tell me how good it is for this

373
00:24:01,630 --> 00:24:06,820
query the question is now can you predict the the the quality of this search

374
00:24:06,820 --> 00:24:13,880
results or another another type of question you can ask so given that user typed

375
00:24:13,920 --> 00:24:20,020
in some query and this is the the graph of the search results he gets will the user

376
00:24:20,020 --> 00:24:24,360
change the query or not so will user like this set of search results or not so

377
00:24:24,360 --> 00:24:28,200
these are the types of questions we can answer

378
00:24:28,440 --> 00:24:34,100
the experiments I will show we have two sorts of web graphs one can consider one is a

379
00:24:34,100 --> 00:24:38,040
URL graph as we call it which is a normal web graph where we have nodes as webpages

380
00:24:38,040 --> 00:24:43,760
and edges as hyperlinks and we have a decent portion of the web or some portion

381
00:24:43,760 --> 00:24:47,840
of the web and then you can have a domain graph where nodes are now domains and

382
00:24:47,840 --> 00:24:52,680
there is an edge between the two domains in there is a webpage in one that points

383
00:24:52,690 --> 00:24:59,020
to another one okay so this is sort of a  more more condensed way of

384
00:24:59,020 --> 00:25:04,160
the URL graph the thing the nice thing here is that we have complained complete a web

385
00:25:04,160 --> 00:25:07,000
domain graph of the web so these are all the domains of the on the

386
00:25:07,000 --> 00:25:12,800
web and there is an edge between between the two if there is a web page

387
00:25:13,560 --> 00:25:18,580
so here is an example of what I mean let's if we take query

388
00:25:18,580 --> 00:25:24,980
subaru and we find all the results this is the graph we get the colors

389
00:25:25,000 --> 00:25:32,080
here are go from red which is perfect to blue which is irrelevant and this labels were

390
00:25:32,150 --> 00:25:38,820
assigned by humans okay so this big search engine companies do this on a regularly

391
00:25:38,970 --> 00:25:43,880
basis so they can they can train their rankers right they they use machine learning

392
00:25:43,890 --> 00:25:47,500
to do the ranking so they need is to say okay this is the most relevant page

393
00:25:47,500 --> 00:25:53,980
for query subaru and so on okay so this is as what we call a projection graph

394
00:25:53,980 --> 00:25:58,940
and when you connect it using some other web pages that exist on the web we call this a

395
00:25:59,000 --> 00:26:07,440
connection graph okay the way how we connect these so how you would like to do

396
00:26:07,440 --> 00:26:11,680
this is to introduce us few of this additional nodes as possible and this is

397
00:26:11,680 --> 00:26:16,920
a Steiner tree problem which is NP hard and what we do is we resign to

398
00:26:16,920 --> 00:26:21,350
some heuristic of how we connect this and it's not really important but the idea is that

399
00:26:21,580 --> 00:26:27,640
you find this additional web pages so that you make the whole graph connected

400
00:26:27,920 --> 00:26:32,480
and then what we will do is we want to do sort of prediction on graphs

401
00:26:32,480 --> 00:26:37,840
and there are many different ways how you could do this and we choose sort of a very a very obvious way

402
00:26:37,840 --> 00:26:42,180
to do this so the idea is to classify this graphs for example this would be

403
00:26:42,190 --> 00:26:44,980
a graph from the positive class and this would be a graph from

404
00:26:44,980 --> 00:26:50,040
negative class and what we do is we describe each graph with a set of

405
00:26:50,040 --> 00:26:54,210
features so that then we can go back to the models and see how do

406
00:26:54,210 --> 00:26:57,600
different graphs what are the features of different graphs

407
00:27:01,240 --> 00:27:05,340
so this is the  pipeline as as I explained right we'll take a query we'll take

408
00:27:05,530 --> 00:27:10,360
this projection and connection graphs and then we'll do learning and now I'll explain how how are we doing this learning step

409
00:27:10,370 --> 00:27:14,080
and what exactly are we predicting

410
00:27:14,420 --> 00:27:20,460
so there are we explored several problem domains but one is predicting quality so

411
00:27:20,460 --> 00:27:24,840
for example you can take top twenty results for each query and top forty to sixty results

412
00:27:24,850 --> 00:27:28,900
for each query and you you can try you can try to learn how to differentiate between

413
00:27:28,900 --> 00:27:32,960
the two classes or you can try to predict the rating of the highest-rated document

414
00:27:32,970 --> 00:27:36,860
in the set this is a different task and you can also go and try to predict user

415
00:27:36,860 --> 00:27:40,860
behavior in a sense that just from his graph you're trying to say whether user

416
00:27:40,860 --> 00:27:47,620
will change the query or not let me show you what I mean I can skip so

417
00:27:47,620 --> 00:27:51,880
just for the data we had thirty thousand queries and for each of these thirty thousand queries

418
00:27:52,240 --> 00:27:57,380
we had two hundred results labelled by relevancy by humans

419
00:27:57,560 --> 00:28:03,220
so this is the task given two graphs for example for this

420
00:28:03,220 --> 00:28:07,020
graph I want to predict red because this is a good set of results because

421
00:28:07,020 --> 00:28:11,440
it has a highly relevant result and this is a bad set of results because

422
00:28:11,440 --> 00:28:15,360
there is no red or green or yellow result here okay so this is what

423
00:28:15,360 --> 00:28:22,020
I what what's the task and just even before before I show show anything more

424
00:28:22,020 --> 00:28:27,000
you can see that here we have this this results of highly connected the that are

425
00:28:27,000 --> 00:28:30,500
the good results are in the centre while while in the bad

426
00:28:30,510 --> 00:28:35,520
set of results everything is everything would be disconnected and it is it is not

427
00:28:35,520 --> 00:28:39,920
a search result that is a hub right it's a it's some other web page there

428
00:28:39,990 --> 00:28:41,020
on the web

429
00:28:41,220 --> 00:28:48,840
so these I we would want to predict good and here we would want to predict poor and

430
00:28:48,860 --> 00:28:54,800
this is these are our results is just classification accuracy on ten fold cross validation so

431
00:28:54,800 --> 00:28:59,860
if you would predict just the common class then this is your accuracy point five

432
00:29:00,160 --> 00:29:06,080
if you would be using so this is three hundred fifty features that Microsoft is

433
00:29:06,080 --> 00:29:10,490
using to train the ranker so if you're using those features this is your accuracy

434
00:29:10,510 --> 00:29:14,020
so this is a very strong base line this is what they're using today but if you are

435
00:29:14,020 --> 00:29:19,480
using these these graphs I showed you your accuracy goes up to eighty three per cent so

436
00:29:19,480 --> 00:29:24,500
we get a significant boost in accuracy from if we don't consider the graph

437
00:29:24,780 --> 00:29:31,600
okay so I'll skip this so all I basically all I want to say

438
00:29:31,600 --> 00:29:36,000
is that just the graph itself without even looking at the content of the Web

439
00:29:36,000 --> 00:29:40,360
page is enough to say or helps a lot when saying whether this is a

440
00:29:40,360 --> 00:29:46,040
good set of results or not and as we'll see it's also good for

441
00:29:46,040 --> 00:29:50,990
query formulation task so the question is predict whether the query is likely to be

442
00:29:50,990 --> 00:29:55,760
reformulated or not so user types in a query we return him a set of results that are connected

443
00:29:55,760 --> 00:29:59,840
it means that you select the height of the interaction only after you have selected

444
00:29:59,900 --> 00:30:04,670
all of the smaller interactions places constraint which is often happening

445
00:30:04,750 --> 00:30:07,820
people use a lot in nineteen of section

446
00:30:07,820 --> 00:30:10,860
that's a start simple things first

447
00:30:10,880 --> 00:30:16,030
and what i've been able to do is to use existing work from when you

448
00:30:16,030 --> 00:30:17,230
to design

449
00:30:17,380 --> 00:30:21,980
convex sparsity inducing norm that is actually achieve that

450
00:30:22,030 --> 00:30:25,320
OK so i won't go into the details for that for the

451
00:30:25,730 --> 00:30:31,490
the paper essentially we are able to bring up in their lives instead of minimizing

452
00:30:32,550 --> 00:30:34,090
instead of minimizing

453
00:30:34,090 --> 00:30:39,880
all the WV we when we take the bible v

454
00:30:39,960 --> 00:30:44,990
we also penalize all of all of its descendants so essentially is thinking that's a

455
00:30:44,990 --> 00:30:46,170
one three alone

456
00:30:46,170 --> 00:30:49,820
we select the good one three and all of its descendants

457
00:30:49,820 --> 00:30:55,570
we saw all the goes with some the number those goals as is the effect

458
00:30:55,590 --> 00:30:58,110
of putting some of the zero

459
00:30:58,170 --> 00:31:01,690
so this would have the effect of taking some these and removing all of their

460
00:31:02,940 --> 00:31:05,920
it means that you will just put that

461
00:31:05,980 --> 00:31:08,940
by removing points with all of all of the descendants

462
00:31:09,010 --> 00:31:12,420
so what is left to you is set which is connected to the

463
00:31:12,440 --> 00:31:15,760
which satisfy my constraint of being

464
00:31:16,010 --> 00:31:18,480
finding no the selected

465
00:31:18,570 --> 00:31:24,590
only after all of these ancestors of selected jesus simple way to implement

466
00:31:24,670 --> 00:31:27,650
my constraints according to the dagger

467
00:31:27,670 --> 00:31:29,800
essentially what have shown

468
00:31:29,800 --> 00:31:33,400
if that we could design a polynomial time algorithm for this norm

469
00:31:33,420 --> 00:31:35,480
OK so bear in mind that

470
00:31:35,480 --> 00:31:40,110
the size of the size of these huge to be able to design an algorithm

471
00:31:40,170 --> 00:31:41,380
for that

472
00:31:41,400 --> 00:31:43,690
you can you please

473
00:31:43,710 --> 00:31:50,130
launching that people can play with sparsity inducing norms namely consistent support recovery condition

474
00:31:50,190 --> 00:31:52,530
we have applied to variable selection

475
00:31:52,530 --> 00:31:56,710
and we have obtained good scaling between p q n and just

476
00:31:56,730 --> 00:31:57,960
what obtain

477
00:31:57,980 --> 00:32:01,360
it is that what's important is not the

478
00:32:01,420 --> 00:32:07,860
the size of the data is the number of critical components and the maximum degree

479
00:32:07,920 --> 00:32:08,960
of the DAG

480
00:32:09,010 --> 00:32:12,380
so this is what we have shown here and i won't go into the details

481
00:32:12,490 --> 00:32:17,210
this is something you can find

482
00:32:17,250 --> 00:32:22,190
so of course this can be extended to other kernels and that we have considered

483
00:32:24,860 --> 00:32:29,340
it doesn't cover because it's simpler to consider a simple to use but of course

484
00:32:29,340 --> 00:32:32,960
this can be used to any type of cancer that people have been using the

485
00:32:33,900 --> 00:32:37,800
so we to be a few years ago there was a lot of people that

486
00:32:38,460 --> 00:32:39,820
taking the problem

487
00:32:39,840 --> 00:32:42,210
thanks a discrete that's like stream

488
00:32:42,230 --> 00:32:43,820
designing and as colonel

489
00:32:43,840 --> 00:32:46,440
and saying that the cool thing that can that

490
00:32:46,460 --> 00:32:50,980
it allows to consider a lot of features OK in an image image a huge

491
00:32:50,980 --> 00:32:56,050
feature space but we have two weeks to complete the can only an efficient way

492
00:32:56,070 --> 00:32:57,050
OK so here

493
00:32:57,050 --> 00:33:02,800
what i'm pushing for it that do the opposite still still consider that you have

494
00:33:02,800 --> 00:33:04,280
a lot of features

495
00:33:04,320 --> 00:33:08,510
but not global them in begin to know that try to select them

496
00:33:08,530 --> 00:33:15,210
by everyone knows which type one so essentially so we're exploring

497
00:33:15,260 --> 00:33:20,210
the last spaces that people have been considering in the literature

498
00:33:20,210 --> 00:33:23,730
by the the case of string instead of saying that

499
00:33:23,760 --> 00:33:27,730
all possible strings are important problem this selected substrings

500
00:33:27,750 --> 00:33:31,440
which are being relevant for the problem and this is

501
00:33:31,490 --> 00:33:34,920
something we are trying to do right now

502
00:33:34,980 --> 00:33:38,210
so in a sense the detection problem you make it explode in a lot of

503
00:33:38,210 --> 00:33:42,360
features and news convex optimisation two

504
00:33:42,380 --> 00:33:45,320
essentially find any in the in the haystack

505
00:33:45,320 --> 00:33:52,670
so this has been extended to other types of structures and hierarchies on

506
00:33:52,690 --> 00:33:59,280
and this is his usual and wanted to norms you saw these sentence types of

507
00:33:59,280 --> 00:34:02,130
groups so right now people have been considering

508
00:34:02,550 --> 00:34:07,400
non overlapping groups similar to the partition of groups

509
00:34:07,460 --> 00:34:13,190
and what we consider here is that you can use it for that

510
00:34:13,250 --> 00:34:17,820
you can use overlapping groups so if you use overlapping groups so what you have

511
00:34:17,840 --> 00:34:21,730
always the same we can if you've been analysed by the sum of the norms

512
00:34:21,760 --> 00:34:25,530
of groups we tried to tackle some of the groups and sends them to zero

513
00:34:25,900 --> 00:34:26,860
so we take

514
00:34:26,900 --> 00:34:30,920
one WG set to zero take another one thousand two zero

515
00:34:30,940 --> 00:34:35,260
so at the end but you obtain in terms of zeros is a new union

516
00:34:35,260 --> 00:34:38,920
of self of a certain number of groups which are said to

517
00:34:38,940 --> 00:34:40,360
OK take some groups

518
00:34:40,420 --> 00:34:41,490
put them to zero

519
00:34:41,510 --> 00:34:46,780
is that it's more like you create sparsity by moving parts of the space

520
00:34:47,210 --> 00:34:52,210
twenty which you obtain is a union of groups from you from your producing

521
00:34:52,210 --> 00:34:54,690
so just to make things very clear

522
00:34:54,690 --> 00:34:57,940
if you take she selected by a mausoleum

523
00:34:57,960 --> 00:35:01,170
organised sequence so each other available

524
00:35:01,170 --> 00:35:03,340
yes hello first of all

525
00:35:03,380 --> 00:35:08,300
before nice slides linear for me to apologize they couldn't cope has some health problems

526
00:35:08,300 --> 00:35:10,570
could and just the plain

527
00:35:10,870 --> 00:35:16,260
nothing really but just the plain is content station

528
00:35:17,030 --> 00:35:21,280
given his slides and led me is a very peculiar style

529
00:35:21,290 --> 00:35:22,960
as a result

530
00:35:22,970 --> 00:35:24,790
i'm a bit

531
00:35:24,840 --> 00:35:28,320
i'm will not block someone like flooding as the only thing you can do

532
00:35:28,840 --> 00:35:32,900
this surely slides and try to explain what i understand them

533
00:35:32,950 --> 00:35:34,980
and i think have some

534
00:35:35,000 --> 00:35:36,260
ideas about that

535
00:35:36,270 --> 00:35:42,020
one day if get the changes civil engineers still because it's very provocative and can

536
00:35:42,020 --> 00:35:45,940
be quite entertaining

537
00:35:46,940 --> 00:35:50,220
so this the set of lights up with this summary

538
00:35:50,240 --> 00:35:54,050
and if you the summary you realize that

539
00:35:54,120 --> 00:35:56,910
and it doesn't care too much about machine

540
00:35:56,930 --> 00:35:59,010
you guess about learning

541
00:35:59,030 --> 00:36:02,120
and now includes learning machines which was

542
00:36:02,140 --> 00:36:05,170
we have been studying for a while but also learning by humans

543
00:36:05,180 --> 00:36:08,230
what are the things that humans can learn

544
00:36:08,240 --> 00:36:10,290
and how why

545
00:36:10,410 --> 00:36:14,590
one particular all the men of learning is scientific learning

546
00:36:14,650 --> 00:36:15,930
you have some

547
00:36:15,940 --> 00:36:20,720
fact experimental facts and you're trying to construct an understanding of these factors

548
00:36:20,730 --> 00:36:25,220
and that theory is a theory of learning of machine learning is supposed to apply

549
00:36:25,220 --> 00:36:26,820
to everything

550
00:36:26,880 --> 00:36:29,070
so in this recent

551
00:36:29,930 --> 00:36:34,900
is being moving from looking at learning from computers to learning in general and is

552
00:36:34,900 --> 00:36:37,560
very excited because he believes that

553
00:36:39,640 --> 00:36:42,290
the appearance of computers

554
00:36:42,340 --> 00:36:46,110
is making a big change in the way we live in a new way science

555
00:36:47,040 --> 00:36:48,700
they always

556
00:36:49,400 --> 00:36:53,200
the way this crisis is that before computers

557
00:36:53,240 --> 00:36:59,070
we were able to learn the simple with few parameters simpler model

558
00:36:59,080 --> 00:37:01,230
and that the the and the

559
00:37:01,250 --> 00:37:03,340
i availability of computers

560
00:37:03,390 --> 00:37:07,220
and understanding of learning theory gives us the means to learn what he called the

561
00:37:07,230 --> 00:37:09,100
complex world that is

562
00:37:09,120 --> 00:37:12,390
whether you have many viable

563
00:37:12,400 --> 00:37:13,710
things like

564
00:37:13,720 --> 00:37:16,530
some people call we sciences because

565
00:37:16,540 --> 00:37:20,840
well you know humanities all social science you have many many families doesn't people say

566
00:37:20,840 --> 00:37:25,350
all we count mobility is statistically not your so there is no more than that

567
00:37:25,350 --> 00:37:27,510
and the other thing that we if things that

568
00:37:27,570 --> 00:37:33,580
because we have computers can handle the many variables because we have theoretical understanding of

569
00:37:34,360 --> 00:37:37,520
mathematical essentials of learning we can do it

570
00:37:37,570 --> 00:37:39,170
and this is about shape

571
00:37:39,220 --> 00:37:44,850
so is extremely excited about this this explain why so i would support

572
00:37:47,340 --> 00:37:52,140
is so reasoning start with the pattern recognition problem this is the pattern recognition problem

573
00:37:52,150 --> 00:37:53,070
like the

574
00:37:53,080 --> 00:37:59,950
those of you fly that is that the simple problem that contains sense of learning

575
00:37:59,970 --> 00:38:03,340
and while say we go beyond that

576
00:38:03,390 --> 00:38:07,160
he wants to explain that these methods go beyond the classical concept of science and

577
00:38:07,160 --> 00:38:11,030
this is what the talk is about

578
00:38:11,280 --> 00:38:12,910
maybe i should start

579
00:38:12,920 --> 00:38:14,650
right to the content

580
00:38:14,660 --> 00:38:18,190
after as we pass in this slide the first one

581
00:38:18,200 --> 00:38:22,140
it is and it even found the classical problem of learning

582
00:38:22,190 --> 00:38:27,400
the second part is transitive inference and show that by asking different questions

583
00:38:27,410 --> 00:38:31,440
you can get situations in which you can learn more you can explore

584
00:38:31,830 --> 00:38:35,340
more complicated function spaces more parameters

585
00:38:35,360 --> 00:38:38,450
more capacity with less examples

586
00:38:38,540 --> 00:38:42,670
and then you don't five set of problems and as the end of the talk

587
00:38:42,670 --> 00:38:44,000
about two

588
00:38:44,050 --> 00:38:45,370
well it's been over

589
00:38:45,880 --> 00:38:48,430
proposition one is called universal

590
00:38:48,480 --> 00:38:53,300
and the other one is called the mass of the master learning

591
00:38:53,350 --> 00:38:58,210
and they look like simple more than the quite essential masterclass learning is

592
00:38:59,150 --> 00:39:01,820
if you believe the learning theory two days

593
00:39:01,920 --> 00:39:04,650
students are was was teachers

594
00:39:04,660 --> 00:39:07,750
slightly worse you control how

595
00:39:07,760 --> 00:39:10,320
clauses to then comes the teacher

596
00:39:10,330 --> 00:39:13,180
if it was like that in real life would be in trouble

597
00:39:13,180 --> 00:39:15,940
this you

598
00:39:21,390 --> 00:39:23,490
can this

599
00:39:23,510 --> 00:39:26,610
there was no that was

600
00:39:28,320 --> 00:39:32,290
she and that was when at UCSD the PDP

601
00:39:32,360 --> 00:39:33,920
research group

602
00:39:35,180 --> 00:39:37,420
what used to be known as the bible

603
00:39:37,470 --> 00:39:39,900
and is now more like the old testament

604
00:39:39,910 --> 00:39:44,100
the they basically

605
00:39:44,110 --> 00:39:47,970
put neural networks on the map and that started a revolution in

606
00:39:47,990 --> 00:39:51,960
machine learning and i was over in england and i was really fascinated by neural

607
00:39:51,960 --> 00:39:55,530
networks so then from my phd i went to UCSD

608
00:39:55,570 --> 00:39:59,560
by the time i arrived in most of the research group had left to other

609
00:39:59,560 --> 00:40:01,740
places but there were still some

610
00:40:01,760 --> 00:40:03,850
people have studied my phd

611
00:40:03,860 --> 00:40:08,520
then at the salk institute in san diego with terrence sejnowski

612
00:40:09,490 --> 00:40:13,490
went to post in around the world a bit and ended up here about one

613
00:40:13,490 --> 00:40:14,880
year ago

614
00:40:14,900 --> 00:40:16,780
needless to say nobody

615
00:40:18,000 --> 00:40:19,660
in machine learning

616
00:40:19,670 --> 00:40:21,860
his most neural networks anymore

617
00:40:21,870 --> 00:40:26,490
but actually not all that much has changed it's just the buzzwords are different

618
00:40:26,510 --> 00:40:30,520
there are some new techniques around but under the hood it's still

619
00:40:30,540 --> 00:40:32,220
pretty much

620
00:40:32,230 --> 00:40:37,810
the same i think as as a back when i studied these things

621
00:40:39,680 --> 00:40:43,110
before i launch into my talk i actually want to

622
00:40:43,120 --> 00:40:44,300
give you

623
00:40:44,310 --> 00:40:48,440
an introduction about the things i don't want to talk about

624
00:40:48,460 --> 00:40:52,790
so i'll talk about them at length

625
00:40:52,870 --> 00:40:57,050
basically what i'm concerned with is

626
00:40:57,070 --> 00:40:58,250
the engine

627
00:40:58,260 --> 00:41:03,660
that drives machine learning and so bernard in this talk as i told you about

628
00:41:03,660 --> 00:41:07,820
empirical risk minimization right so that

629
00:41:07,840 --> 00:41:11,680
the typical way you set up a machine learning problem is to find the function

630
00:41:11,680 --> 00:41:13,200
that measures

631
00:41:13,220 --> 00:41:16,730
how much error you're making on some training data

632
00:41:16,740 --> 00:41:19,260
and you try to minimize the function

633
00:41:19,280 --> 00:41:23,940
what interests me is how do you actually do that minimisation

634
00:41:24,470 --> 00:41:28,100
how do you how do you run that and how can we make

635
00:41:28,120 --> 00:41:32,540
that engine that drives machine learning efficient

636
00:41:32,550 --> 00:41:34,670
how can we get answers fast

637
00:41:37,930 --> 00:41:42,780
optimisation methods

638
00:41:44,760 --> 00:41:49,080
it's what i'm concerned with

639
00:41:50,620 --> 00:41:54,980
we can distinguish optimisation methods we can classify them

640
00:41:55,670 --> 00:42:02,950
whether they're using gradient information and what kind of gradient information they're using

641
00:42:03,000 --> 00:42:05,430
does everybody know what the gradient

642
00:42:05,450 --> 00:42:07,600
it is

643
00:42:08,480 --> 00:42:09,770
anybody doesn't

644
00:42:09,790 --> 00:42:17,440
i mean i can go back and explain OK quick introduction we have a function

645
00:42:17,450 --> 00:42:21,100
and let's say it's it's the kind of function we would want to minimize in

646
00:42:21,100 --> 00:42:25,900
machine learning so it could be an empirical risk function so i'll just call it

647
00:42:25,900 --> 00:42:30,240
f and also there there's some parameters theta over which we want to optimise it

648
00:42:30,240 --> 00:42:34,820
and maybe it depends also on some inputs x right

649
00:42:34,920 --> 00:42:40,520
now the gradient of that function if that function is smooth and differentiable we can

650
00:42:40,520 --> 00:42:43,970
take the gradient the gradient is defined

651
00:42:45,120 --> 00:42:47,110
the limit

652
00:42:47,120 --> 00:42:49,540
for some h going to zero

653
00:42:49,680 --> 00:42:53,900
of f

654
00:42:53,920 --> 00:42:55,740
feet are

655
00:42:55,750 --> 00:42:56,900
plus a ge

656
00:42:59,720 --> 00:43:05,750
also this is the direction of the gradient

657
00:43:05,770 --> 00:43:06,800
yeah that's OK

658
00:43:12,490 --> 00:43:18,260
i'll start sticking here a unit vector in direction i OK so the unit vector

659
00:43:18,270 --> 00:43:21,140
in direction i

660
00:43:21,190 --> 00:43:24,440
is all zeros except that it has one

661
00:43:24,450 --> 00:43:27,060
a single one

662
00:43:27,070 --> 00:43:30,010
and this is in the i th position

663
00:43:30,030 --> 00:43:33,170
OK this is the unit vector in direction i

664
00:43:33,190 --> 00:43:35,830
so we have to the parameters

665
00:43:35,840 --> 00:43:40,040
h times the unit vector in direction i and we measure the function at that

666
00:43:41,270 --> 00:43:44,030
mystic in the inputs

667
00:43:44,080 --> 00:43:48,990
we subtract from that the function the original point

668
00:43:49,000 --> 00:43:52,590
and divide by age

669
00:43:52,600 --> 00:43:55,840
if you take the limit as h goes to zero

670
00:43:55,850 --> 00:43:56,930
this is

671
00:43:56,940 --> 00:43:58,400
the gradient

672
00:43:58,450 --> 00:44:00,100
of f

673
00:44:00,110 --> 00:44:02,090
theta x

674
00:44:02,110 --> 00:44:03,980
with respect to

675
00:44:04,030 --> 00:44:06,490
the i th component

676
00:44:06,500 --> 00:44:08,260
of the parameter vector

677
00:44:08,280 --> 00:44:13,020
the i th component because that's the part component perturbing

678
00:44:13,030 --> 00:44:20,460
so this is the definition

679
00:44:22,540 --> 00:44:24,330
it looks like this

680
00:44:24,350 --> 00:44:28,070
if this is your functions

681
00:44:28,080 --> 00:44:29,090
we are

682
00:44:29,110 --> 00:44:32,840
at a certain point data

683
00:44:32,860 --> 00:44:37,680
what we're taking is now i only have one coordinate so this is my

684
00:44:37,690 --> 00:44:39,720
the coordinate and perturbing

685
00:44:39,790 --> 00:44:44,650
i'm going to add a little bit h to that coordinate measure the function at

686
00:44:44,650 --> 00:44:45,960
that point

687
00:44:46,000 --> 00:44:52,230
subtract from that the original function so i'm i'm taking this difference here

688
00:44:52,240 --> 00:44:54,260
and divided by age

689
00:44:54,270 --> 00:44:59,520
and and what that actually gives me is the slope of the tangent

690
00:44:59,530 --> 00:45:08,740
at that point the tangent on the function

691
00:45:10,800 --> 00:45:12,330
how could that be useful

692
00:45:12,340 --> 00:45:14,480
for minimizing a function

693
00:45:14,490 --> 00:45:17,820
well if we know the slope of the tangent we know

694
00:45:17,840 --> 00:45:19,300
which way downhill

695
00:45:19,460 --> 00:45:22,860
we know that if we go out in this case to the left

696
00:45:22,910 --> 00:45:23,930
we're going to

697
00:45:23,940 --> 00:45:25,640
reduce the function

698
00:45:25,670 --> 00:45:29,720
if you just have function values it's hard to tell

699
00:45:29,740 --> 00:45:33,330
so you can classify optimisation methods

700
00:45:33,350 --> 00:45:35,250
in two

701
00:45:35,300 --> 00:45:39,420
direct methods

702
00:45:39,440 --> 00:45:44,100
and these use function values only

703
00:45:49,000 --> 00:45:54,160
the next one would be in direct gradient methods

704
00:45:54,370 --> 00:45:58,940
which use this this gradient information as well

705
00:45:58,960 --> 00:46:01,490
and i would

706
00:46:01,510 --> 00:46:06,930
and as the third category second order

707
00:46:06,930 --> 00:46:11,360
and the degrees centigrade

708
00:46:12,500 --> 00:46:15,080
you take this column with zero hundred

709
00:46:15,190 --> 00:46:20,270
he divided into one hundred equal parts

710
00:46:20,350 --> 00:46:23,940
and that is postulated to be the temperature

711
00:46:23,990 --> 00:46:26,670
anywhere between zero and one hundred

712
00:46:26,680 --> 00:46:31,030
if you've gone seventy nine percent of the way to the top

713
00:46:31,110 --> 00:46:32,330
from here to here

714
00:46:32,380 --> 00:46:37,000
the temperature is seventy nine degrees

715
00:46:37,010 --> 00:46:39,880
so the degrees introduced and that's the centigrade scale

716
00:46:39,900 --> 00:46:42,760
and you guys know that are different skip kind of the fahrenheit scale you can

717
00:46:42,760 --> 00:46:44,310
have any of scale

718
00:46:44,360 --> 00:46:48,140
in which is what you want to call the freezing point is different somebody thinks

719
00:46:48,190 --> 00:46:50,750
zero somebody thirty two

720
00:46:50,760 --> 00:46:53,100
and you can again called the something else

721
00:46:53,110 --> 00:46:56,400
and you can divide into into two hundred parts hundred and eighty parts of the

722
00:46:56,400 --> 00:46:58,930
body like philosophy is the same

723
00:46:59,010 --> 00:47:01,990
you have to fight two points which are reproducible

724
00:47:03,240 --> 00:47:07,500
and the way the region between them into some number of equal steps

725
00:47:07,510 --> 00:47:10,620
if it hundred equal steps said the centigrade scale

726
00:47:10,640 --> 00:47:13,600
probably the lowest one is called zero

727
00:47:13,650 --> 00:47:16,770
so you have thermometers

728
00:47:16,780 --> 00:47:19,540
now there are some problems with this

729
00:47:19,550 --> 00:47:21,300
one problem is that

730
00:47:21,350 --> 00:47:23,770
the boiling point of water

731
00:47:23,790 --> 00:47:26,960
does not seem to be very reliable

732
00:47:27,010 --> 00:47:29,380
because if you boil water on

733
00:47:29,430 --> 00:47:31,240
in aspen for example

734
00:47:31,250 --> 00:47:35,540
you know it doesn't seem to boil it seems more and more readily

735
00:47:36,150 --> 00:47:38,080
in the planes

736
00:47:38,130 --> 00:47:41,590
you can ask how do you know that maybe they're still doing the same thing

737
00:47:41,610 --> 00:47:43,660
i know that because i try to cook something

738
00:47:43,680 --> 00:47:44,870
cook some rice

739
00:47:44,900 --> 00:47:47,290
vegetables i find they don't cook at all

740
00:47:47,300 --> 00:47:49,890
denver points before it cooks

741
00:47:49,900 --> 00:47:55,300
that way we know it's probably boiling earlier in the mountains then in the plane

742
00:47:55,350 --> 00:47:57,640
so who is going to decide what they

743
00:47:57,690 --> 00:47:58,960
the temperature is

744
00:47:59,550 --> 00:48:03,860
so you have be more careful when you say boiling point and freezing point

745
00:48:03,900 --> 00:48:06,190
because things don't boil

746
00:48:06,260 --> 00:48:08,240
they don't seem to boil at a certain

747
00:48:08,250 --> 00:48:11,820
predictable and fixed temperature

748
00:48:11,860 --> 00:48:13,120
this is very

749
00:48:13,130 --> 00:48:19,250
the arguments i've never appreciated fully when i learning the subject is also click definition

750
00:48:19,270 --> 00:48:21,670
because you may not know

751
00:48:21,720 --> 00:48:25,740
the temperature is changing because this is the moment by postulate it's going to be

752
00:48:25,740 --> 00:48:29,250
the temperature by definition how can it be wrong

753
00:48:29,300 --> 00:48:33,130
what's wrong is that it's not you know it's not a reliable method because

754
00:48:33,170 --> 00:48:36,210
physical phenomena like when you're a swim called

755
00:48:36,370 --> 00:48:39,030
not only produced by the boiling point of water

756
00:48:39,040 --> 00:48:39,850
he cooks

757
00:48:39,860 --> 00:48:42,600
in the planes doesn't cook in the modern so we know the boiling point is

758
00:48:42,600 --> 00:48:43,600
to blame

759
00:48:43,610 --> 00:48:44,840
prices the rights

760
00:48:44,850 --> 00:48:47,820
that's so we know that that's not a good measure so now people are much

761
00:48:47,820 --> 00:48:49,450
fancier measures

762
00:48:49,500 --> 00:48:51,350
and i'll tell you a little bit about that

763
00:48:51,360 --> 00:48:54,160
for a long time this was a very good start

764
00:48:54,210 --> 00:48:56,680
don't worry about the fact that water boils

765
00:48:56,780 --> 00:49:00,830
differently in different altitude you could go to sea level

766
00:49:00,880 --> 00:49:04,950
and that's a good enough definitions sea level is pretty much constant of the world

767
00:49:04,950 --> 00:49:08,360
and you can see the pressure at sea level is the presidency level

768
00:49:08,530 --> 00:49:11,530
yes the road to the atmosphere

769
00:49:11,550 --> 00:49:13,470
OK so that's the usual

770
00:49:13,480 --> 00:49:15,030
definition of temperature

771
00:49:15,050 --> 00:49:19,080
now the trouble started when people realize

772
00:49:19,130 --> 00:49:23,300
that if you make a thermometer with your favourite fluid maybe mercury

773
00:49:23,310 --> 00:49:25,590
and they make one with alcohol

774
00:49:25,600 --> 00:49:28,960
they will agree it's zero and they will agree eight hundred because that's how you

775
00:49:28,960 --> 00:49:34,340
fixed your read of zero everyone says zero and everyone says

776
00:49:34,350 --> 00:49:38,210
but how about seventy four degrees seventy five degrees

777
00:49:38,260 --> 00:49:42,350
i said seventy five if my fluid claim before the way to the top

778
00:49:42,400 --> 00:49:48,210
at that point yours may not have claimed before the the way

779
00:49:48,260 --> 00:49:50,260
the what you've got two things

780
00:49:50,310 --> 00:49:51,350
two graphs

781
00:49:51,360 --> 00:49:53,120
with zero one hundred degree

782
00:49:53,130 --> 00:49:56,490
one graph may be like this one like that so that when i think it

783
00:49:56,490 --> 00:49:58,690
is seventy five you might think of seventy two

784
00:49:58,710 --> 00:50:02,210
one hundred people agree because we have cooked up that way

785
00:50:02,230 --> 00:50:04,210
in other words it's not true

786
00:50:04,230 --> 00:50:08,480
all liquids expand at the same rate

787
00:50:08,560 --> 00:50:10,980
so you will have to then pick one liquid and safe

788
00:50:11,030 --> 00:50:12,880
we used to wait wait that liquid

789
00:50:12,890 --> 00:50:14,930
and when delegates going halfway

790
00:50:14,940 --> 00:50:17,930
we would sit fifty degrees

791
00:50:17,930 --> 00:50:22,680
so have typically liquid you have to have an international convention in the alcohol lobby

792
00:50:22,680 --> 00:50:25,780
and i don't know alcohol lobby they argue

793
00:50:25,830 --> 00:50:30,540
finally the founder of much better solutions than his liquids

794
00:50:30,540 --> 00:50:33,200
they found out that diffuser gas

795
00:50:33,250 --> 00:50:35,660
you can define temperature using gases

796
00:50:35,720 --> 00:50:38,970
which have some very very nice property

797
00:50:38,980 --> 00:50:40,310
and this is the

798
00:50:40,340 --> 00:50:41,810
gas thermometer

799
00:50:41,920 --> 00:50:44,060
i'm going to tell you know

800
00:50:44,880 --> 00:50:50,050
yes said you build the gas thermometer

801
00:50:50,060 --> 00:50:52,820
you take some gas in a container

802
00:50:53,630 --> 00:50:55,630
container for me and all

803
00:50:55,670 --> 00:51:00,700
whenever to draw anything thermodynamics is going to be gas inside some cylinder

804
00:51:00,710 --> 00:51:03,360
with some weights on

805
00:51:03,420 --> 00:51:07,100
and that defines the pressure of the gas

806
00:51:07,150 --> 00:51:08,870
of course the pressure will be the

807
00:51:08,880 --> 00:51:10,760
mg of these weights

808
00:51:10,770 --> 00:51:12,790
divided by area of the cylinder

809
00:51:12,840 --> 00:51:13,820
if the pressure

810
00:51:13,840 --> 00:51:17,300
plus atmospheric pressure

811
00:51:17,300 --> 00:51:18,850
the volume is this

812
00:51:18,880 --> 00:51:22,870
whatever the volumes based on height

813
00:51:23,740 --> 00:51:25,740
we ask you to do

814
00:51:25,790 --> 00:51:30,140
take the product of pressure times volume for any sample of gas back some gas

815
00:51:30,450 --> 00:51:32,280
put in this tank

816
00:51:32,300 --> 00:51:35,030
and now i put it on different surfaces

817
00:51:35,030 --> 00:51:36,720
like hot plate like this though

818
00:51:36,840 --> 00:51:38,470
robert water

819
00:51:38,480 --> 00:51:41,480
and measure the temperature using some standard method

820
00:51:41,560 --> 00:51:44,750
up to that point like a mercury thermometer

821
00:51:44,780 --> 00:51:46,690
what you know this

822
00:51:46,710 --> 00:51:48,300
is that

823
00:51:48,370 --> 00:51:50,530
the temperature measured by some

824
00:51:50,540 --> 00:51:52,290
reasonable scheme

825
00:51:52,310 --> 00:51:55,510
shows that the product p times

826
00:51:55,560 --> 00:51:58,800
lies on the straight line

827
00:51:58,920 --> 00:52:00,550
you connect the dots

828
00:52:00,560 --> 00:52:03,650
we find the product pv is linear

829
00:52:03,670 --> 00:52:09,950
in this temperature vt

830
00:52:09,950 --> 00:52:11,940
and this is zero degrees

831
00:52:11,990 --> 00:52:16,880
and this is hundred degrees

832
00:52:16,900 --> 00:52:19,990
now here is the beauty of the gas thermometer

833
00:52:20,030 --> 00:52:22,850
if you take a different gas

834
00:52:22,870 --> 00:52:26,150
and it put different among different gas in different cylinder

835
00:52:26,160 --> 00:52:28,170
you get some of the graph

836
00:52:28,190 --> 00:52:33,910
may look like this

837
00:52:33,940 --> 00:52:36,080
well u that's the one that's hundred

838
00:52:36,890 --> 00:52:41,720
the most important thing is that's also a straight line

839
00:52:41,720 --> 00:52:45,720
there's also a straight line it the following implication you guys can prove

840
00:52:45,820 --> 00:52:47,550
but you only easier

841
00:52:47,570 --> 00:52:49,270
which is that

842
00:52:49,310 --> 00:52:52,240
if i think that my guess claims

843
00:52:52,240 --> 00:52:54,780
fifty six percent of the way

844
00:52:54,800 --> 00:52:56,460
the site the top

845
00:52:56,470 --> 00:52:58,940
the temperature is fifty six degrees

846
00:52:58,950 --> 00:53:01,120
i ask what is your guess

847
00:53:01,140 --> 00:53:05,580
the find is also playing this expression with property straight lines

848
00:53:05,600 --> 00:53:08,120
you can show that if it was straight lines

849
00:53:08,130 --> 00:53:10,320
what whatever be the slope

850
00:53:10,340 --> 00:53:12,260
if they agree

851
00:53:12,300 --> 00:53:15,440
this is zero this hundred got different slope

852
00:53:15,450 --> 00:53:16,760
when you're playing

853
00:53:16,800 --> 00:53:18,670
the halfway point

854
00:53:18,700 --> 00:53:23,170
proline fifty degrees and as what any gas and they were all have claimed the

855
00:53:23,170 --> 00:53:24,510
halfway point

856
00:53:24,520 --> 00:53:28,070
from the zero point eight hundred points

857
00:53:28,150 --> 00:53:30,260
there was gas thermometers

858
00:53:30,260 --> 00:53:33,460
not only agree with the end points where they must

859
00:53:33,510 --> 00:53:35,110
by construction

860
00:53:35,130 --> 00:53:38,570
we seem to agree all the way in between

861
00:53:38,620 --> 00:53:40,530
but that's one equation

862
00:53:40,530 --> 00:53:45,280
and so we can write that down the distribution of x is called the distribution

863
00:53:45,330 --> 00:53:49,140
and it just takes this form when x is what when is one mu when

864
00:53:49,140 --> 00:53:51,620
x zero it's one minus

865
00:53:54,010 --> 00:53:58,390
we can do things like the expectation the variance of the distribution very easy to

866
00:53:58,390 --> 00:54:03,140
compute in general the expectation of course is of some function f is just the

867
00:54:03,140 --> 00:54:07,600
weighted average that function weighted by the probability of observing the value of x or

868
00:54:07,600 --> 00:54:10,870
the interval in the in the density case continuous case

869
00:54:10,930 --> 00:54:17,780
the variance is just the expectation of how much it varies around its mean square

870
00:54:18,030 --> 00:54:19,910
value so this is f minus the

871
00:54:19,930 --> 00:54:24,640
expectation all squared and then we just take the expectation is that if you expand

872
00:54:24,640 --> 00:54:25,600
that the square

873
00:54:25,680 --> 00:54:29,550
then you can see it's the expectation of f squared one this expression of f

874
00:54:31,590 --> 00:54:36,490
so for the boolean case there's the distribution expectation is just new

875
00:54:36,510 --> 00:54:37,550
and the

876
00:54:37,570 --> 00:54:42,240
variances mu times one minus mu so this is a function which is zero if

877
00:54:42,240 --> 00:54:46,140
museo one to five the coin which always gives the heads always gives tails than

878
00:54:46,140 --> 00:54:48,120
that the distribution is zero variance

879
00:54:48,140 --> 00:54:51,180
and the is maximum when musa half

880
00:54:51,240 --> 00:54:55,400
and that's reasonable so the biggest variations of coin which is equally likely to land

881
00:54:55,400 --> 00:54:58,430
heads or tails

882
00:54:58,450 --> 00:55:02,010
misconstrue through this fairly quickly because i think this is pretty basic stuff but but

883
00:55:02,010 --> 00:55:04,930
stop me if i find not clear

884
00:55:04,990 --> 00:55:09,330
OK so that's the distribution that right the likelihood function so got data set of

885
00:55:09,330 --> 00:55:16,830
independent coin flips and of them of which landed heads and minus and the details

886
00:55:16,850 --> 00:55:21,990
so independently the likelihood function which is the probability of the data given viewed as

887
00:55:21,990 --> 00:55:23,570
a function of mu

888
00:55:23,600 --> 00:55:28,180
is just the product of the probabilities for each of these independent events is given

889
00:55:28,180 --> 00:55:29,140
by this

890
00:55:30,240 --> 00:55:34,450
because the because x zero or one and this is just a new

891
00:55:34,470 --> 00:55:37,640
raise the power of the number of times i see headstones one minus mu raise

892
00:55:37,660 --> 00:55:40,720
the power of the number of times i see tails

893
00:55:40,740 --> 00:55:46,120
that's the likelihood functions mutism some power one minus mu to some other

894
00:55:46,140 --> 00:55:48,850
now we need a prior distribution over mu

895
00:55:48,870 --> 00:55:52,600
and it's going to be really convenient i can get a whole lot simpler if

896
00:55:52,620 --> 00:55:54,200
we choose

897
00:55:54,220 --> 00:55:59,510
a family of prior distributions which has the same functional form is the likelihood function

898
00:55:59,530 --> 00:56:02,490
OK this is a simplification

899
00:56:02,510 --> 00:56:06,470
and so this was mu disempower one minus mu to some power

900
00:56:06,490 --> 00:56:09,600
so let's choose a

901
00:56:09,620 --> 00:56:14,090
prior distribution which is mu raised to some power a minus one one minus mu

902
00:56:14,090 --> 00:56:18,490
raised to some power be minus one so the the minus one to just convention

903
00:56:18,740 --> 00:56:24,990
it's called conjugate prior to a conjugate prior is the prior to his functional dependence

904
00:56:24,990 --> 00:56:29,600
on the parameter is the same as the functional dependence of the likelihood function

905
00:56:29,620 --> 00:56:31,390
respect to the parameter

906
00:56:32,640 --> 00:56:36,070
this this is the this is a proportionality here we have two

907
00:56:36,070 --> 00:56:39,550
the coefficient in front to make sure this integrates to one that coefficient turns out

908
00:56:39,550 --> 00:56:43,450
to be this ratio gamma functions and this thing called the beta distribution is the

909
00:56:43,450 --> 00:56:46,510
conjugate prior for the newly distribution

910
00:56:46,510 --> 00:56:50,680
and again we can be its expectation which is just a very plus b and

911
00:56:50,680 --> 00:56:54,590
that's the expression for its variants

912
00:56:54,600 --> 00:56:59,370
there some plots of the beta distribution for various parameters that you have one just

913
00:56:59,370 --> 00:57:02,680
constant and these are different values of a and b

914
00:57:02,700 --> 00:57:07,410
so this is the distribution over parameter which lies between zero and one CMU is

915
00:57:07,410 --> 00:57:11,300
the parameter that we can interpret the probability the province coins heads

916
00:57:11,330 --> 00:57:16,180
we don't know the value mu were uncertain to describe by probability distribution to probability

917
00:57:16,180 --> 00:57:17,890
distribution over the

918
00:57:17,950 --> 00:57:23,030
values of the unknown probability new

919
00:57:23,090 --> 00:57:27,240
so now we've got the likelihood function the private right down the posterior distribution we

920
00:57:27,240 --> 00:57:31,240
take the likelihood function is a function of new multiplied by the prior and then

921
00:57:31,240 --> 00:57:32,680
we need to normalize

922
00:57:32,700 --> 00:57:37,990
this is a really simple because we chose this conjugate form for the prior so

923
00:57:37,990 --> 00:57:40,620
this is the likelihood function

924
00:57:40,950 --> 00:57:45,620
new to the m one minus mu t n minus and beta prior is new

925
00:57:45,620 --> 00:57:49,100
to the wiki practice alpha norton beta nought

926
00:57:49,100 --> 00:57:53,140
so it just takes this form so gets muta somehow one minus mu to some

927
00:57:53,140 --> 00:57:56,890
other powers again it's just a beta distribution so the whole point is we start

928
00:57:56,890 --> 00:58:00,760
with beta prior we multiply the likelihood function is a conjugate prior we end up

929
00:58:00,760 --> 00:58:04,700
with another beta distribution and again in the sequential

930
00:58:04,740 --> 00:58:09,160
bayesian view of learning we observe some more data you multiply by the new likelihood

931
00:58:09,160 --> 00:58:12,870
function and again we end up with another beta prior to the nice thing is

932
00:58:12,870 --> 00:58:16,950
that as a keep observing data we keep absorbing the evidence from that data the

933
00:58:16,950 --> 00:58:21,680
functional form of the distribution doesn't change is always the beta distribution it's the premises

934
00:58:21,680 --> 00:58:24,010
of the distribution which change

935
00:58:26,450 --> 00:58:30,590
because this has the functional form of the beta distribution we can trivially normalise because

936
00:58:30,590 --> 00:58:34,600
we already know how to normalise the distribution so is a beta distribution with parameters

937
00:58:34,600 --> 00:58:37,490
a and b n and

938
00:58:37,530 --> 00:58:39,070
these are the

939
00:58:39,120 --> 00:58:42,120
this is what we get just by inspection

940
00:58:42,410 --> 00:58:44,220
this equation so

941
00:58:44,870 --> 00:58:51,510
a parameter for the posterior distribution after observing n coin flips is a plus an

942
00:58:51,530 --> 00:58:54,280
and for the

943
00:58:54,280 --> 00:58:59,030
for b and b not plus the number of tails so we can interpret the

944
00:58:59,030 --> 00:59:03,590
parameters a norton b or a sort of number of pseudo coin flips the coin

945
00:59:03,590 --> 00:59:08,530
flips that was sort of inverted commas seen in the prior

946
00:59:08,570 --> 00:59:13,850
so it's as if we've already seen a nought heads be not tails that we

947
00:59:13,850 --> 00:59:18,760
observe some new data and additional heads and minus an additional ten

948
00:59:18,780 --> 00:59:20,430
that the general results you'll see

949
00:59:20,430 --> 00:59:23,350
was in moment

950
00:59:23,350 --> 00:59:27,890
so the plot of what's going on here is his prior distribution

951
00:59:30,640 --> 00:59:35,430
beta distribution we multiply by all the newly likelihood functions in this case we have

952
00:59:35,430 --> 00:59:39,720
observed ahead so this function that this one data point is just new is the

953
00:59:39,720 --> 00:59:44,100
function just a straight line from zero to one we multiply this by this

954
00:59:44,120 --> 00:59:46,260
we get and then we normalize

955
00:59:46,280 --> 00:59:47,800
we get this shape

956
00:59:47,870 --> 00:59:53,320
so having observed a coin flip of heads it shifted the distribution to the right

957
00:59:53,320 --> 00:59:54,650
from each other constraints

958
00:59:55,340 --> 00:59:58,090
so put a minus x are only to these constraints

959
00:59:59,580 --> 01:00:04,920
at the moment for the phase one problem change the objective function forget about this because

960
01:00:05,410 --> 01:00:11,190
i for phase one i don't care what the objective is i only wanna get something that satisfies these constraints

961
01:00:11,790 --> 01:00:16,510
and once i get our reinstate object function and start what call phase two

962
01:00:16,950 --> 01:00:19,580
which is the algorithm i just showed you the primal simplex methods

963
01:00:22,190 --> 01:00:23,300
we put the x not on

964
01:00:27,070 --> 01:00:27,860
and so here it is

965
01:00:29,470 --> 01:00:30,250
x not in here

966
01:00:36,090 --> 01:00:36,760
okay so

967
01:00:37,650 --> 01:00:38,520
this is feasible

968
01:00:39,740 --> 01:00:40,650
why is it feasible

969
01:00:41,570 --> 01:00:43,510
i can let x not be a large number

970
01:00:45,460 --> 01:00:48,780
end x one x two x equals zero end

971
01:00:49,340 --> 01:00:50,830
i will satisfy these inequalities

972
01:00:52,100 --> 01:00:53,400
it's a feasible problem

973
01:00:54,370 --> 01:00:56,530
but this is not a dictionary solution that's just

974
01:00:57,580 --> 01:00:58,480
this assertion that there's

975
01:00:59,020 --> 01:01:00,870
that there are solutions to this problem

976
01:01:03,530 --> 01:01:07,480
so i would like to solve this problem using the simplex methods and if i

977
01:01:07,480 --> 01:01:09,790
get to the point where the objective function is equal to zero

978
01:01:11,440 --> 01:01:16,590
the name means that the original problem was feasible because that's not how i changed the original problem

979
01:01:17,320 --> 01:01:19,830
and so if i can make the change the original probably called zero

980
01:01:20,250 --> 01:01:22,470
that i've got a feasible solution there's no problem

981
01:01:25,160 --> 01:01:25,710
the final

982
01:01:27,170 --> 01:01:32,040
dictionary or the final basis if you like the basic variables versus the nonbasic variables

983
01:01:32,510 --> 01:01:36,970
the final bases that you get from this phase one problem can then be used as the initial

984
01:01:37,430 --> 01:01:39,320
starting point for phase two

985
01:01:40,010 --> 01:01:44,520
you just throw away the x zero put the correct object function back in and continue

986
01:01:46,640 --> 01:01:50,820
if the objective function to this phase one problem is strictly less than zero

987
01:01:51,490 --> 01:01:54,810
that means we tried to maximize minus x zero

988
01:01:55,620 --> 01:01:57,930
and we in the end didn't get

989
01:01:58,700 --> 01:02:02,450
two x equals are exactly equal to zero we get the eggs are we call

990
01:02:02,450 --> 01:02:06,070
the something positive so minus x are always negative and

991
01:02:06,750 --> 01:02:09,660
that means that there is no solution to the original

992
01:02:10,120 --> 01:02:12,810
problem that means the original problem is invisible

993
01:02:14,970 --> 01:02:15,940
and so on

994
01:02:17,490 --> 01:02:18,620
so here is how this goes

995
01:02:24,400 --> 01:02:28,030
we can show that the problem with x zero and here

996
01:02:32,840 --> 01:02:35,400
and if and applicant i developed for this

997
01:02:35,910 --> 01:02:38,450
two phase map that shows both e

998
01:02:38,500 --> 01:02:39,780
the original objective function

999
01:02:40,440 --> 01:02:40,870
there are many

1000
01:02:41,210 --> 01:02:43,140
phase one objective function right below it

1001
01:02:44,390 --> 01:02:45,730
and we do pivots

1002
01:02:46,270 --> 01:02:47,180
for phase one

1003
01:02:47,550 --> 01:02:50,320
even though it shows the original objective function we ignored

1004
01:02:51,100 --> 01:02:54,980
and we just look at the phase one objective function with the x minus x

1005
01:02:54,980 --> 01:02:57,560
zero we do are with this one

1006
01:02:58,450 --> 01:03:02,190
and with the right hand side here but the first one is different

1007
01:03:03,000 --> 01:03:07,300
because this is not feasible even for the phase one problem is not feasible because

1008
01:03:07,300 --> 01:03:08,660
we've got these negative signs here

1009
01:03:09,060 --> 01:03:11,900
so the first that in this algorithm is special

1010
01:03:12,490 --> 01:03:14,840
the first is eggs are all winter

1011
01:03:16,220 --> 01:03:18,410
the most negative one will leave

1012
01:03:19,530 --> 01:03:21,280
that's just one that you do there

1013
01:03:22,970 --> 01:03:24,250
and we do that's

1014
01:03:24,940 --> 01:03:27,810
it automatically gets all these things to be critical zero

1015
01:03:28,310 --> 01:03:30,680
and now we have a dictionary solution

1016
01:03:39,030 --> 01:03:41,600
further from the phase one problem and so now we go

1017
01:03:43,010 --> 01:03:44,180
and we put in

1018
01:03:46,120 --> 01:03:48,990
we u is the second wrote to the phase one problem

1019
01:03:49,520 --> 01:03:52,900
and so we pick the forums are entering variables that's x one enters

1020
01:03:53,390 --> 01:03:55,020
we go down here and look at all these things

1021
01:03:56,300 --> 01:03:59,770
from eight going down route four from six going down right to for to an

1022
01:03:59,770 --> 01:04:04,990
area seven turned w five is the one it's zero first so w five must

1023
01:04:04,990 --> 01:04:05,940
be the leaving variable

1024
01:04:06,360 --> 01:04:07,460
so we do the pivot

1025
01:04:09,390 --> 01:04:13,010
and there it is end then we have x

1026
01:04:14,450 --> 01:04:16,430
still has a positive coefficients are

1027
01:04:18,930 --> 01:04:19,640
objective here

1028
01:04:20,200 --> 01:04:21,050
and by the way are

1029
01:04:21,510 --> 01:04:23,770
think object functions value at the

1030
01:04:24,370 --> 01:04:28,050
at the dictionary solution is minus two are trying to push this up to zero

1031
01:04:28,960 --> 01:04:29,760
so the three

1032
01:04:30,560 --> 01:04:35,280
and next to is positive so it enters we look through here to another three

1033
01:04:35,280 --> 01:04:37,190
three going around two and a half so now

1034
01:04:39,250 --> 01:04:43,200
x zero is they one that had zero first one i do all those little

1035
01:04:46,710 --> 01:04:48,960
so i have x two enters end

1036
01:04:49,860 --> 01:04:50,950
x zero leaves

1037
01:04:53,030 --> 01:04:55,540
i'm done with phase one

1038
01:04:55,540 --> 01:04:59,180
so again let's first in the propositional case and not worry about lifting and then

1039
01:05:00,270 --> 01:05:05,140
so the propositional case here's is an example that will hopefully you it'll give intuitions

1040
01:05:05,140 --> 01:05:08,500
for what's going to happen this is the same running example that we have coming

1041
01:05:08,500 --> 01:05:13,200
from the representation part let's suppose that i now tell you that i AM and

1042
01:05:13,460 --> 01:05:14,520
our friends

1043
01:05:14,530 --> 01:05:17,080
OK i just give you that additional facts

1044
01:05:17,100 --> 01:05:21,960
what that fact does is make these two worlds impossible right now the world's from

1045
01:05:21,970 --> 01:05:24,950
these mountains with bob are is not friends with bob

1046
01:05:25,070 --> 01:05:26,290
nine possible

1047
01:05:26,920 --> 01:05:29,550
so that leaves only these two worlds

1048
01:05:30,130 --> 01:05:33,510
the potentials in these two tools are still the same that before it's one for

1049
01:05:33,880 --> 01:05:35,570
point seventy five per day

1050
01:05:36,150 --> 01:05:38,100
and if you ask the question

1051
01:05:38,120 --> 01:05:42,120
what is the problem the body is happy given that a number of our friends

1052
01:05:42,180 --> 01:05:47,040
well that's just this anomalous probability over the some of these anomalous probabilities so it's

1053
01:05:47,040 --> 01:05:50,800
one over one plus point seventy five it's point fifty seven

1054
01:05:51,850 --> 01:05:53,320
so to summarize

1055
01:05:53,330 --> 01:05:55,140
what we do here

1056
01:05:55,150 --> 01:05:56,380
it was to say

1057
01:05:56,420 --> 01:06:00,250
if you tell me that the problem the not friends and above were happy about

1058
01:06:00,250 --> 01:06:01,710
this point eight

1059
01:06:01,730 --> 01:06:07,410
make the maximum entropy assumptions then it follows that the probability that happy given that

1060
01:06:07,410 --> 01:06:10,350
enable references point to fifty seven

1061
01:06:11,910 --> 01:06:15,800
we're assuming we conditional on and being friends with bob but we don't know with

1062
01:06:15,800 --> 01:06:20,070
certainty that that makes bobhappy because there was no probabilistic

1063
01:06:20,170 --> 01:06:23,670
as a result of which the conclusion also now has the probability and we know

1064
01:06:23,670 --> 01:06:25,160
how to compute

1065
01:06:25,430 --> 01:06:31,190
now that's the propositional case but of course the interesting case is going to be

1066
01:06:31,190 --> 01:06:32,980
the first order

1067
01:06:33,000 --> 01:06:36,880
what happens in the first case well suppose that instead of just giving you that

1068
01:06:36,980 --> 01:06:40,000
alpha blob i give you all for everyone

1069
01:06:40,010 --> 01:06:43,440
so i say forever you know like i just so you know

1070
01:06:43,470 --> 01:06:47,010
p of not friends in x or have the access point eight

1071
01:06:47,050 --> 01:06:47,600
right i

1072
01:06:47,630 --> 01:06:51,130
and the meaning of this is that for every x every person in the world

1073
01:06:51,460 --> 01:06:55,350
the probability that this statement is true this point eight

1074
01:06:56,410 --> 01:06:59,940
you can think of it as creating a whole bunch of copies

1075
01:06:59,950 --> 01:07:01,170
of this thing

1076
01:07:01,840 --> 01:07:05,140
one from all but now also one for charles one for a and one for

1077
01:07:05,140 --> 01:07:09,170
everybody else so not just have a whole bunch of copies of you know these

1078
01:07:09,230 --> 01:07:13,460
these guys being highly probable in this guy this world here being less probable

1079
01:07:13,510 --> 01:07:19,370
but of course the interesting part comes when i have different evidence about some objects

1080
01:07:19,370 --> 01:07:21,290
than i do but others

1081
01:07:21,300 --> 01:07:25,050
so let's suppose and this is indeed the typical case when you do logical inference

1082
01:07:25,050 --> 01:07:28,550
or probabilistic for that matter that i have this general rule

1083
01:07:28,560 --> 01:07:30,060
that applies to everybody

1084
01:07:30,100 --> 01:07:34,210
but then i i'm just all the n is friends with bob but i still

1085
01:07:34,210 --> 01:07:37,770
don't know anything about anybody else what does that mean

1086
01:07:37,850 --> 01:07:42,170
what that means is that this big stack of things breaks up into two

1087
01:07:42,210 --> 01:07:45,670
the first one is the case that we saw for bob with these rules are

1088
01:07:45,670 --> 01:07:48,120
impossible and only these two are left

1089
01:07:48,140 --> 01:07:52,730
and then for everybody but bob i still have the case where we don't have

1090
01:07:52,730 --> 01:07:53,970
this knowledge

1091
01:07:54,000 --> 01:07:59,450
notice that in logic the only substitutions that we need to consider as institutions of

1092
01:07:59,450 --> 01:08:02,530
the form x equals blogger x equals y

1093
01:08:02,540 --> 01:08:03,450
but here

1094
01:08:03,460 --> 01:08:07,580
we also need to consider the case where x is not equal to one

1095
01:08:07,600 --> 01:08:12,090
it's actually very important thing in lifted probabilistic inference that the people first noticed in

1096
01:08:12,090 --> 01:08:16,500
two thousand three is that we need to consider the entire substitution as well as

1097
01:08:16,500 --> 01:08:17,900
the substitution

1098
01:08:17,930 --> 01:08:24,420
so this is unlikely small no graphical example let's let's get to the actual machinery

1099
01:08:24,900 --> 01:08:30,300
was about diagram that i think summarizes the current state of inference

1100
01:08:30,330 --> 01:08:34,830
in you know probabilistic logic languages or another way to say this is like what

1101
01:08:34,880 --> 01:08:38,920
we can do with probabilistic theorem proving

1102
01:08:38,930 --> 01:08:41,940
and you know if you remember only two or three things from from this tutorial

1103
01:08:41,950 --> 01:08:45,120
you know this one is certainly should should certainly be one of those two or

1104
01:08:46,450 --> 01:08:49,200
so this is diagram with three axes

1105
01:08:49,210 --> 01:08:53,430
the this axis running from left to right on the left it has decision problems

1106
01:08:53,810 --> 01:08:56,680
and on the right it has counting problems

1107
01:08:56,690 --> 01:09:02,120
the axes running into the back goes from latest to weighted problems

1108
01:09:02,170 --> 01:09:05,090
and the axis running from the bottom to the top goes from

1109
01:09:05,140 --> 01:09:08,630
and lifting propositional to lift or first of problems

1110
01:09:09,350 --> 01:09:15,340
so here in the front left lower corner which is half satisfiability

1111
01:09:15,350 --> 01:09:19,340
it's the decision problem it's a weighted and it's proposition

1112
01:09:21,870 --> 01:09:25,190
propositional theorem proving is reducible to

1113
01:09:25,190 --> 01:09:28,140
was one of the legislation

1114
01:09:28,170 --> 01:09:34,670
there is no one was all just this

1115
01:09:34,690 --> 01:09:36,310
what was it

1116
01:09:37,390 --> 01:09:45,310
but you know so essential to this

1117
01:09:45,330 --> 01:09:52,350
very very fast

1118
01:09:57,640 --> 01:10:01,520
the general theory

1119
01:10:08,270 --> 01:10:16,000
OK so

1120
01:10:16,020 --> 01:10:22,190
there are extensions to this

1121
01:10:22,210 --> 01:10:24,250
o operations

1122
01:10:24,920 --> 01:10:26,810
well said so

1123
01:10:31,060 --> 01:10:32,830
we just the

1124
01:10:33,770 --> 01:10:43,290
usually refers remember we say they are the ones that

1125
01:10:47,870 --> 01:10:56,710
thing was to that so many designers of

1126
01:10:57,440 --> 01:11:03,920
to that end in

1127
01:11:03,940 --> 01:11:07,790
that says the school

1128
01:11:07,810 --> 01:11:09,100
the first place

1129
01:11:16,120 --> 01:11:19,560
this is this is sufficient conditions

1130
01:11:19,580 --> 01:11:23,640
it's not a relationship between

1131
01:11:23,650 --> 01:11:26,710
this is

1132
01:11:27,100 --> 01:11:31,600
so what's that

1133
01:11:34,080 --> 01:11:35,410
this is because

1134
01:11:40,420 --> 01:11:44,690
this is the list

1135
01:12:04,140 --> 01:12:11,120
four hours is like

1136
01:12:11,140 --> 01:12:16,830
the last one

1137
01:12:16,870 --> 01:12:19,960
is it

1138
01:12:23,310 --> 01:12:31,040
and of course there will also that

1139
01:12:31,060 --> 01:12:32,290
so what

1140
01:12:32,290 --> 01:12:39,580
these the results of your next turn

1141
01:12:40,910 --> 01:12:45,020
it is sometimes essential

1142
01:12:46,080 --> 01:12:54,020
to deal with with these issues to build themselves

1143
01:12:54,100 --> 01:12:57,390
a typical optimization all the

1144
01:12:59,290 --> 01:13:01,850
two months

1145
01:13:01,920 --> 01:13:07,020
i swear that this the the

1146
01:13:10,370 --> 01:13:12,390
it is true

1147
01:13:13,500 --> 01:13:20,520
we don't have an organisation is not enough to really run the sort of

1148
01:13:24,640 --> 01:13:25,410
it is

1149
01:13:26,020 --> 01:13:28,810
one then

1150
01:13:28,830 --> 01:13:32,920
what i want to

1151
01:13:33,140 --> 01:13:38,420
it is that it is not a story

1152
01:13:38,480 --> 01:13:42,440
o how the last

1153
01:13:44,690 --> 01:13:45,480
all right

1154
01:13:57,310 --> 01:14:01,540
so in the next lecture

1155
01:14:11,210 --> 01:14:14,140
some results from

1156
01:14:14,170 --> 01:14:17,810
this is like you see

1157
01:14:17,940 --> 01:14:26,370
already know talks specific examples is one of

1158
01:14:26,390 --> 01:14:29,420
optimized solution your

1159
01:14:30,870 --> 01:14:34,230
yes so

1160
01:14:34,250 --> 01:14:36,870
one of the

1161
01:14:37,000 --> 01:14:42,060
we want

1162
01:14:42,060 --> 01:14:45,080
all of these

1163
01:14:45,100 --> 01:14:46,410
going from

1164
01:14:46,420 --> 01:14:50,600
here wrist during using

1165
01:14:57,940 --> 01:15:04,210
that's why someone see what is not is not the right thing

1166
01:15:04,230 --> 01:15:06,910
so you have

1167
01:15:06,920 --> 01:15:09,910
this is the basic

1168
01:15:11,540 --> 01:15:14,270
and what

1169
01:15:14,290 --> 01:15:18,500
but what was the bacteria

1170
01:15:18,870 --> 01:15:22,190
that's so that we will

1171
01:15:23,020 --> 01:15:27,390
how many lines in you know these are all functions

1172
01:15:27,440 --> 01:15:34,420
leads to different type of decomposition approximation is to make

1173
01:15:34,440 --> 01:15:38,850
so that's one of the oscillation

1174
01:15:38,850 --> 01:15:44,230
the approximation probably class and it is a major problem

1175
01:15:44,230 --> 01:15:47,020
this the sort of new complementarity linear zation

1176
01:15:47,620 --> 01:15:49,450
equations middle this middle one

1177
01:15:49,950 --> 01:15:52,000
and uses eliminate

1178
01:15:52,500 --> 01:15:54,270
w i believe lie more eliminate

1179
01:15:54,900 --> 01:15:56,110
so i limited w

1180
01:15:56,590 --> 01:15:58,630
and only a delta x delta why left

1181
01:15:59,030 --> 01:15:59,730
and now the whole

1182
01:16:00,770 --> 01:16:05,430
system equation looks like that's which is similar to what i had before r

1183
01:16:05,550 --> 01:16:07,950
i had eight i a transpose

1184
01:16:08,420 --> 01:16:11,420
i had w why inverse you can almost still read

1185
01:16:12,120 --> 01:16:13,240
and up here

1186
01:16:13,720 --> 01:16:16,300
i had something positive definite

1187
01:16:17,600 --> 01:16:20,660
o minus sign missing or a minus sign here

1188
01:16:21,710 --> 01:16:25,220
it's probably because the minimum maximum or something yes there has to be one and

1189
01:16:25,220 --> 01:16:27,190
this has been applied positive and there has to be

1190
01:16:32,250 --> 01:16:36,860
i i actually introduced the term quasi definite system fore

1191
01:16:37,260 --> 01:16:37,830
the system of

1192
01:16:38,590 --> 01:16:40,310
equations that can be written

1193
01:16:40,700 --> 01:16:44,370
in block form like this with the matrix is the transpose of the matrix

1194
01:16:44,990 --> 01:16:47,780
end and and and the positive

1195
01:16:48,470 --> 01:16:52,580
semi definite here and negative definite there are or vice versa

1196
01:16:54,510 --> 01:16:59,460
and it turns out that that's better than they did these being both positive semi

1197
01:16:59,460 --> 01:17:02,130
definite that's actually harder system itself

1198
01:17:06,140 --> 01:17:09,770
so we solve this reduced cake katie system

1199
01:17:10,640 --> 01:17:17,470
and we short steplength as necessary and we update to a new point and we just iterate and d

1200
01:17:17,850 --> 01:17:19,270
everything is the same basically

1201
01:17:24,350 --> 01:17:25,790
because you not with the view

1202
01:17:35,500 --> 01:17:38,440
interior point methods for convex problems

1203
01:17:39,000 --> 01:17:41,550
find the globally optimal solution

1204
01:17:42,790 --> 01:17:45,450
i'm gonna tell you in a few slides about

1205
01:17:46,460 --> 01:17:51,170
some changes to make to the algorithm which will encourage a defined at least a

1206
01:17:51,170 --> 01:17:54,030
locally optimal solution when the problem is not convex

1207
01:17:54,970 --> 01:17:56,970
but what is not convex now it's the

1208
01:18:00,580 --> 01:18:01,350
it's difficult

1209
01:18:04,720 --> 01:18:05,370
i think we

1210
01:18:05,950 --> 01:18:09,080
algorithms that exist are fairly reliable it five

1211
01:18:09,740 --> 01:18:13,050
at finding a feasible solution if one exists in course that's also hard

1212
01:18:15,300 --> 01:18:19,060
and then find the fact that that's probably the hardest part and once you find

1213
01:18:19,060 --> 01:18:23,120
it funny we optimal solution but what is local mean there's no

1214
01:18:23,880 --> 01:18:25,070
proof that you know this is

1215
01:18:26,430 --> 01:18:32,280
this is you know the closest solution the closest locally optimal solution the point started at

1216
01:18:33,010 --> 01:18:34,310
so it's all love

1217
01:18:35,080 --> 01:18:36,600
just tried is just trying to find

1218
01:18:37,570 --> 01:18:38,590
eighty solution

1219
01:18:39,630 --> 01:18:41,650
which might be close to started

1220
01:18:46,050 --> 01:18:49,970
i don't remember if i have a slide here that elaborates on this

1221
01:18:50,370 --> 01:18:55,080
quasi definite property that so many finished this that what i have here in the first time out

1222
01:18:55,510 --> 01:18:56,090
a mention why

1223
01:18:56,660 --> 01:18:59,870
having what it is positive definite no negative definite is is

1224
01:19:00,300 --> 01:19:02,080
is better than having both in the positive

1225
01:19:05,380 --> 01:19:08,360
o so the next slide here are convex versus non convex

1226
01:19:13,340 --> 01:19:13,740
so we

1227
01:19:14,320 --> 01:19:17,040
the including extend are

1228
01:19:17,800 --> 01:19:22,170
class standard problems to include some with equalities and some with inequalities

1229
01:19:23,380 --> 01:19:25,040
a problem is convex

1230
01:19:26,300 --> 01:19:30,440
if we're really qualities it has to be a find function

1231
01:19:31,170 --> 01:19:35,170
linear function with and not and some right-hand side that doesn't have to be zero

1232
01:19:36,630 --> 01:19:38,210
but this linear constraints

1233
01:19:39,060 --> 01:19:41,030
if if it's equality they learn here

1234
01:19:41,530 --> 01:19:43,860
if it's if you inequalities that

1235
01:19:44,430 --> 01:19:51,940
the thee and we write is greater equal to those the functions age supplies have to be a concave functions

1236
01:19:52,760 --> 01:19:57,650
and if a minimizing the function f has to be convex so it's a convex

1237
01:19:57,650 --> 01:20:01,100
nonlinear programming problem if you have these conditions

1238
01:20:01,600 --> 01:20:06,490
alcholics smooth if all the functions that we see are twice continuously differentiable

1239
01:20:10,390 --> 01:20:14,990
so if forum at situation that's a con the convex situation that's

1240
01:20:16,490 --> 01:20:20,160
almost as good as linear programming but it's not quite as

1241
01:20:20,900 --> 01:20:24,950
simple there are there's atleast one another subtlety that i've dimension

1242
01:20:29,490 --> 01:20:35,560
actually quadratic is still okay you actually go nonquadratic but still convex before the subtlety appears

1243
01:20:36,240 --> 01:20:40,040
it doesn't suffice to choose the step length simply keep u

1244
01:20:42,120 --> 01:20:42,880
from going negative

1245
01:20:43,750 --> 01:20:44,930
here's a trivial example

1246
01:20:45,740 --> 01:20:47,520
to illustrate this simple point

1247
01:20:48,000 --> 01:20:50,130
consider minimizing this function

1248
01:20:51,570 --> 01:20:52,950
i don't have any constraints

1249
01:20:53,390 --> 01:20:54,520
no constraints

1250
01:20:55,780 --> 01:20:59,610
the function of the variable x is a free variable i can go positive and negative

1251
01:21:04,380 --> 01:21:09,170
he iterates of the interior point method which in this case is just exactly newton's methods

1252
01:21:10,540 --> 01:21:11,440
do exactly

1253
01:21:11,920 --> 01:21:12,850
what's shown here

1254
01:21:13,320 --> 01:21:13,930
and i can maybe

1255
01:21:14,450 --> 01:21:16,020
speak a little picture here

1256
01:21:16,990 --> 01:21:17,740
what's happening

1257
01:21:19,330 --> 01:21:20,340
this function here

1258
01:21:21,170 --> 01:21:23,920
the square root of one plus x squared would look like

1259
01:21:25,790 --> 01:21:26,620
it's one

1260
01:21:27,640 --> 01:21:29,270
and when x equals zero

1261
01:21:30,260 --> 01:21:30,780
it's one

1262
01:21:32,770 --> 01:21:35,080
end as x goes to infinity

1263
01:21:36,050 --> 01:21:39,350
one becomes irrelevant looks its asymptotic toward

1264
01:21:42,910 --> 01:21:45,890
and is asymptotic to minus x as you go minus infinity

1265
01:21:47,760 --> 01:21:49,770
it's a very fun function i like it very much

1266
01:21:50,560 --> 01:21:53,850
anyway if you write down what they interior point methods do for this that would

1267
01:21:53,850 --> 01:21:55,900
do exactly what news about the dust because

1268
01:21:56,290 --> 01:22:00,650
there's finally another weird not we're the any other complications

1269
01:22:02,230 --> 01:22:03,520
of nonlinear programming

1270
01:22:05,280 --> 01:22:06,290
end newton's method

1271
01:22:07,540 --> 01:22:09,640
approximates this function with the best

1272
01:22:11,690 --> 01:22:16,950
quadratic function to approximate so barnstar way out here this is almost a linear function

1273
01:22:17,430 --> 01:22:20,560
the best quadratic approximation is almost linear function

1274
01:22:22,730 --> 01:22:23,470
looks like this

1275
01:22:24,140 --> 01:22:27,210
and i find the minimum the minimum of that which is way over here

1276
01:22:27,850 --> 01:22:32,850
which now i take that's my next point is and i take the best quadratic approximation to this

1277
01:22:33,370 --> 01:22:34,770
after this to the original function

1278
01:22:35,250 --> 01:22:38,050
here was even closer to a linear function

1279
01:22:39,310 --> 01:22:41,050
now i do the quadratic approximation

1280
01:22:41,480 --> 01:22:42,820
and as many we are here

1281
01:22:44,580 --> 01:22:45,940
and it gets worse and worse

1282
01:22:47,320 --> 01:22:51,130
well i'm far away from the optimal solution which is x equals zero

1283
01:22:51,550 --> 01:22:55,980
and in fact if you look at the iterates six tell you exactly that's what happens

1284
01:22:56,410 --> 01:22:58,840
you the next iteration is the negative

1285
01:22:59,790 --> 01:23:02,190
other cuba the current iterate

1286
01:23:03,520 --> 01:23:04,310
if i started

1287
01:23:05,430 --> 01:23:07,110
the next one will be minus eight

1288
01:23:07,760 --> 01:23:10,160
and i can even qminer say in my head

1289
01:23:11,970 --> 01:23:13,340
the national be eight cubed

1290
01:23:14,270 --> 01:23:17,830
so it grows fast but it lacks starts out less than one

1291
01:23:18,320 --> 01:23:20,840
the net converges to zero extraordinarily fast

1292
01:23:20,840 --> 01:23:22,230
given q

1293
01:23:22,270 --> 01:23:26,370
we use bayes formula again

1294
01:23:26,390 --> 01:23:29,180
pq is constant for community

1295
01:23:29,190 --> 01:23:30,540
we always

1296
01:23:30,590 --> 01:23:35,180
ran for particular so we can make them at this time

1297
01:23:35,230 --> 01:23:37,400
he is

1298
01:23:37,410 --> 01:23:39,180
often used as a prior

1299
01:23:39,190 --> 01:23:42,660
or it can be the same for all documents

1300
01:23:42,710 --> 01:23:48,020
one possibility for using it as a prior would page so we can say

1301
01:23:48,040 --> 01:23:52,070
you can enter it here is the pagerank of the document that

1302
01:23:52,530 --> 01:23:55,850
pagerank will be integrated into the bank

1303
01:23:58,810 --> 01:24:03,070
the probability of q given t and this is this generation probability that i talked

1304
01:24:03,070 --> 01:24:04,580
about before

1305
01:24:04,600 --> 01:24:10,210
with probability the language model corresponding to document the generation

1306
01:24:10,220 --> 01:24:14,930
the query q

1307
01:24:15,640 --> 01:24:21,140
assumptions we made ranking documents according to PQD PNT p q is equivalent so

1308
01:24:21,150 --> 01:24:24,570
of course what we really have is experience want to know what's

1309
01:24:24,620 --> 01:24:25,720
the document

1310
01:24:26,980 --> 01:24:30,540
what we actually using the base baseform rule

1311
01:24:30,560 --> 01:24:32,870
what we actually rank is

1312
01:24:32,890 --> 01:24:37,720
either this quantity of this quantity

1313
01:24:37,770 --> 01:24:41,070
OK how can you compute pqkt indeed

1314
01:24:41,080 --> 01:24:47,520
you make the same conditional independence assumption as in there

1315
01:24:47,960 --> 01:24:51,090
although it looks a little different but it's basically the same so

1316
01:24:51,140 --> 01:24:53,460
here we have created given the

1317
01:24:53,470 --> 01:24:56,120
language models sometimes i'm going to use the

1318
01:24:56,130 --> 01:24:59,670
to make clear that it

1319
01:24:59,690 --> 01:25:03,700
this is the language model correspond to the document and other documents l so the

1320
01:25:03,700 --> 01:25:06,410
representation of the document is the language model

1321
01:25:07,690 --> 01:25:10,150
q can write is t one the

1322
01:25:10,270 --> 01:25:11,390
thank you so

1323
01:25:11,400 --> 01:25:13,460
there are q

1324
01:25:13,620 --> 01:25:17,900
words in the theory and that just newman here

1325
01:25:17,920 --> 01:25:19,150
and the

1326
01:25:19,190 --> 01:25:23,770
independence assumption here is that the probability of getting the query a whole given the

1327
01:25:24,540 --> 01:25:28,660
is the same as the product of the individual terms given the model

1328
01:25:28,680 --> 01:25:29,820
that's the same

1329
01:25:29,840 --> 01:25:31,230
essentially the same

1330
01:25:31,250 --> 01:25:36,850
binary independence assumption of binary independence assumption we made

1331
01:25:36,990 --> 01:25:39,120
now this is equivalent to this

1332
01:25:39,130 --> 01:25:42,750
but this is course through the tokens in the query one by one

1333
01:25:42,860 --> 01:25:48,050
this goes to the distinct terms that occur in the query and it's clear several

1334
01:25:48,050 --> 01:25:49,960
times just

1335
01:25:50,300 --> 01:25:52,910
those factors are put together

1336
01:25:52,920 --> 01:25:56,190
and raised the power of tf t q which have to is the number of

1337
01:25:56,190 --> 01:25:59,300
occurrences of the term in the query

1338
01:25:59,310 --> 01:26:00,320
and this is

1339
01:26:00,330 --> 01:26:02,430
a multinomial model

1340
01:26:02,440 --> 01:26:07,020
we are maintain a constant factor here the constant factor there

1341
01:26:08,610 --> 01:26:13,180
it tells you how many different combinations of these

1342
01:26:13,190 --> 01:26:15,680
constellation of three terms can order

1343
01:26:16,060 --> 01:26:20,440
but essentially it's the multinomial model so language models

1344
01:26:20,760 --> 01:26:21,730
the language

1345
01:26:21,750 --> 01:26:27,500
the approach in IR is based on martin on march

1346
01:26:27,550 --> 01:26:30,220
the missing pieces parameters p

1347
01:26:30,250 --> 01:26:31,580
given and come from

1348
01:26:31,590 --> 01:26:35,810
how do we estimate the probability that a given time t is generated by the

1349
01:26:35,810 --> 01:26:38,190
document model

1350
01:26:38,240 --> 01:26:41,360
we start with maximum likelihood estimates

1351
01:26:41,370 --> 01:26:46,130
the maximum likelihood estimate is simply the relative frequency of the term

1352
01:26:46,170 --> 01:26:49,140
so we can't often does the term occurs

1353
01:26:49,160 --> 01:26:53,470
and divided by the length of the document

1354
01:26:53,490 --> 01:26:55,500
relative frequency

1355
01:26:55,510 --> 01:26:59,350
now again we have a problem with zeros the same problem we also had with

1356
01:26:59,710 --> 01:27:01,420
and model

1357
01:27:01,530 --> 01:27:05,670
because a single t in the query both peter given an equal zero

1358
01:27:05,680 --> 01:27:06,780
well makes

1359
01:27:06,790 --> 01:27:09,340
this zero

1360
01:27:09,350 --> 01:27:10,600
so if we

1361
01:27:10,620 --> 01:27:12,500
if if you have a theory

1362
01:27:12,540 --> 01:27:15,590
and one of the few times doesn't occur in the document

1363
01:27:15,600 --> 01:27:18,750
the document will get a retrieval status value of zero

1364
01:27:18,770 --> 01:27:22,880
so in effect giving single terms in the community to call

1365
01:27:22,890 --> 01:27:24,060
which is

1366
01:27:24,080 --> 01:27:27,210
basically a conjunction conjunctive flavor of the

1367
01:27:27,570 --> 01:27:29,000
of the language model

1368
01:27:29,060 --> 01:27:31,420
but we don't want that usually

1369
01:27:31,550 --> 01:27:34,420
for example for the query michael jackson top it

1370
01:27:34,470 --> 01:27:38,010
a document about michael jackson top songs but not using the what hits

1371
01:27:38,020 --> 01:27:39,280
i would have

1372
01:27:39,290 --> 01:27:42,370
zero retrieval status value in that span

1373
01:27:42,390 --> 01:27:46,070
so we need to do something against eras

1374
01:27:48,130 --> 01:27:50,910
what we do is moving

1375
01:27:50,920 --> 01:27:56,050
the key intuition is in this movie that a non occurring terms possible even though

1376
01:27:56,050 --> 01:27:57,240
it in the car

1377
01:27:57,260 --> 01:28:02,750
but no more likely than would be expected by chance in the collection

1378
01:28:04,390 --> 01:28:10,580
basically using the collection distribution as a background distribution for smoothing

1379
01:28:11,910 --> 01:28:13,640
this is the collection models

1380
01:28:13,650 --> 01:28:16,710
CF is the number of occurrences of t in the collection

1381
01:28:16,760 --> 01:28:18,030
and t is the

1382
01:28:18,050 --> 01:28:22,190
total length of the collection the total number of tokens in the collection

1383
01:28:22,230 --> 01:28:27,070
then we can estimate the background distribution the distribution of terms in the collection as

1384
01:28:27,070 --> 01:28:27,960
a whole

1385
01:28:27,980 --> 01:28:29,770
again the relative frequency

1386
01:28:29,790 --> 01:28:30,740
it's simply

1387
01:28:30,760 --> 01:28:33,600
the total number of occurrences of the term in the collection

1388
01:28:33,620 --> 01:28:34,790
divided by

1389
01:28:34,800 --> 01:28:39,640
all the tokens in the collection

1390
01:28:39,660 --> 01:28:42,590
and we will not use the CFP this

1391
01:28:42,600 --> 01:28:47,550
relative frequency the collection to smooth the relative frequency in the document

1392
01:28:47,600 --> 01:28:49,270
and we certainly do that by

1393
01:28:49,570 --> 01:28:51,760
doing an interpolation of the two

1394
01:28:51,770 --> 01:28:54,210
so this is a linear combination of

1395
01:28:54,210 --> 01:28:58,080
some magic numbers so magic nuclei

1396
01:28:58,780 --> 01:29:05,060
where you have put on neutron which are magic for example you could ninety

1397
01:29:05,760 --> 01:29:09,480
you have also dublin magic nuclei which are really interested

1398
01:29:09,520 --> 01:29:11,390
so potassium forty

1399
01:29:11,410 --> 01:29:18,370
led to eight and thirteen one hundred the most famous one

1400
01:29:18,410 --> 01:29:22,990
so how do you treat from the microscopic because could become a point of view

1401
01:29:22,990 --> 01:29:27,470
how do you treat the nucleons so it means that you treat the nucleons in

1402
01:29:27,470 --> 01:29:29,240
nuclei want in interaction

1403
01:29:29,260 --> 01:29:32,210
so here it's the strong interaction

1404
01:29:32,220 --> 01:29:36,560
so you have been two problems the first one of the many body problem

1405
01:29:37,490 --> 01:29:42,000
when you're dealing with more than a few constituents then you don't know how to

1406
01:29:42,000 --> 01:29:43,710
solve exactly is the problem

1407
01:29:43,720 --> 01:29:46,200
so it's quite general problem so

1408
01:29:46,220 --> 01:29:50,600
we know how to solve that for and lower than four but for

1409
01:29:50,670 --> 01:29:54,250
bigger system bigger than ten we don't know that

1410
01:29:55,180 --> 01:29:56,120
it's because

1411
01:29:56,130 --> 01:30:00,990
imagine if i i i i want to talk to you i can talk to

1412
01:30:00,990 --> 01:30:01,950
each of you

1413
01:30:01,970 --> 01:30:04,820
i can to a group of two three four

1414
01:30:04,870 --> 01:30:08,150
and then you can talk to each of your colleagues or two three four so

1415
01:30:08,150 --> 01:30:09,990
there are so many combinations

1416
01:30:10,000 --> 01:30:12,720
but it's impossible to treat that exactly

1417
01:30:12,730 --> 01:30:15,360
and for in you only we see that the are pheromone so you have to

1418
01:30:15,360 --> 01:30:16,810
adjust symmetrized that

1419
01:30:16,830 --> 01:30:19,370
so we don't know now how to solve that

1420
01:30:19,420 --> 01:30:25,340
so that's why there are some approximation so the most commonly used approximation are associated

1421
01:30:26,230 --> 01:30:28,350
coming from atomic physics

1422
01:30:28,390 --> 01:30:31,590
and the approach is based on the mean field

1423
01:30:32,650 --> 01:30:37,860
the philosophy is completely different between them from the show with what you consider you

1424
01:30:37,870 --> 01:30:39,570
that you have particles

1425
01:30:39,580 --> 01:30:42,140
so constituent that do not participate

1426
01:30:42,290 --> 01:30:44,110
only consider a few

1427
01:30:44,150 --> 01:30:46,880
that are pertinent to the others

1428
01:30:46,900 --> 01:30:49,360
completely frozen

1429
01:30:49,370 --> 01:30:54,020
on the contrary for the mean field you don't have frozen body you don't have

1430
01:30:54,020 --> 01:30:55,020
enough to core

1431
01:30:55,060 --> 01:30:58,980
but you neglect to learn the correlation interaction between the nucleons

1432
01:30:58,990 --> 01:31:00,070
so this is

1433
01:31:00,120 --> 01:31:02,400
that depends what you want to do so

1434
01:31:02,410 --> 01:31:06,690
this is very well adapted for small system when you can

1435
01:31:06,740 --> 01:31:09,640
i really do that this

1436
01:31:09,650 --> 01:31:12,490
good as you are very aging this is better for

1437
01:31:12,530 --> 01:31:15,640
large system

1438
01:31:15,680 --> 01:31:20,510
and you have another problem here which is the nucleon nucleon force that is known

1439
01:31:20,510 --> 01:31:22,610
as i told you yesterday

1440
01:31:22,630 --> 01:31:27,640
so these two problems seems to be adequate non correlated

1441
01:31:29,420 --> 01:31:33,420
but be careful since you are doing here some approximation

1442
01:31:33,440 --> 01:31:38,820
before you use will be anything before so far is valid for the approximation that

1443
01:31:38,820 --> 01:31:39,750
he was

1444
01:31:39,790 --> 01:31:42,610
so in fact these two problems are

1445
01:31:42,660 --> 01:31:46,120
are trying to so you should be careful when you use the force that is

1446
01:31:46,310 --> 01:31:50,010
effective and valued for the approach that you've chosen

1447
01:31:50,060 --> 01:31:53,090
so this is the difference was that to make physical where you know

1448
01:31:53,130 --> 01:31:58,180
between electrons that you have this coulomb repulsion it's difficult to treat but you know

1449
01:31:58,180 --> 01:31:59,160
the forest

1450
01:32:00,410 --> 01:32:03,830
this is specific to nuclear physics

1451
01:32:04,820 --> 01:32:09,100
in more detail what about the nucleons so

1452
01:32:09,120 --> 01:32:12,670
you know that there are quantum objects so they cannot have all the energies but

1453
01:32:12,670 --> 01:32:13,590
they can

1454
01:32:14,740 --> 01:32:17,820
b on a non discrete number of states

1455
01:32:17,830 --> 01:32:23,590
so they are fermions so that means they cannot occupy the same state suppose printed

1456
01:32:24,420 --> 01:32:27,930
the shown with the first show what it was the plot in nineteen forty eight

1457
01:32:27,930 --> 01:32:29,540
for the

1458
01:32:29,560 --> 01:32:34,280
for nuclear physics in it was my uncle from a year

1459
01:32:34,300 --> 01:32:37,560
and she said that in analogy was atomic structure

1460
01:32:37,580 --> 01:32:40,140
one may postulate that in the nucleus

1461
01:32:40,150 --> 01:32:43,030
the nucleons move fairly independently

1462
01:32:43,040 --> 01:32:46,620
in individual orbits in an average potential

1463
01:32:46,640 --> 01:32:52,050
so what people have done is that they have deduced from experimental results potential which

1464
01:32:52,050 --> 01:32:53,450
is this one

1465
01:32:53,470 --> 01:32:56,730
so this is what we call wood saxon potential

1466
01:32:56,750 --> 01:32:58,160
so negative here

1467
01:32:58,170 --> 01:33:02,010
you have parameters the zero big air

1468
01:33:02,010 --> 01:33:05,320
so this is broken here this is the of air

1469
01:33:05,340 --> 01:33:09,120
you have this in red wood saxon potential

1470
01:33:09,990 --> 01:33:14,590
it's quite difficult to to have the results to obtain the and analytical results of

1471
01:33:14,590 --> 01:33:17,930
this wood saxon potential so what people do that

1472
01:33:17,960 --> 01:33:21,820
this amplify the problem with a square well here in blue

1473
01:33:21,830 --> 01:33:26,090
our army negotiator in rate and the regions the

1474
01:33:26,110 --> 01:33:27,790
the depth and range

1475
01:33:28,390 --> 01:33:32,860
the reason that delivers do not depend so much on the precise shape of the

1476
01:33:32,860 --> 01:33:36,530
potential but you chose

1477
01:33:38,190 --> 01:33:41,560
when you have the potential you can calculate the the labels

1478
01:33:42,210 --> 01:33:46,530
you have different quantum numbers that characterised the nucleon states

1479
01:33:46,550 --> 01:33:51,650
so the quantum numbers are the principal quantum number big n

1480
01:33:51,700 --> 01:33:56,530
the right quantum number small and that characterize special extension

1481
01:33:56,550 --> 01:34:00,550
you have the orbital moment the azimuthal quantum number l

1482
01:34:00,560 --> 01:34:04,070
spain intrinsic spin glass minus one

1483
01:34:04,090 --> 01:34:08,050
to know how to feel these labels you have some selection rules so that you

1484
01:34:08,050 --> 01:34:12,520
know how to feed the different what is the degeneracy of the labels here

1485
01:34:12,560 --> 01:34:16,610
and what we introduce in the total angular momentum which is the sum of the

1486
01:34:16,610 --> 01:34:20,090
orbital moment mississippi

1487
01:34:20,110 --> 01:34:24,040
so this is an example of a stairwell so

1488
01:34:24,100 --> 01:34:29,580
these are the states you have big and zero one too small n one two

1489
01:34:29,590 --> 01:34:31,500
l here

1490
01:34:31,510 --> 01:34:34,170
this correspond to equal zero p

1491
01:34:34,180 --> 01:34:36,430
p two equal one exit around

1492
01:34:36,480 --> 01:34:39,550
so you have the differently abled

1493
01:34:39,580 --> 01:34:43,900
what has been added in nuclear physics is the spin orbit n l s who

1494
01:34:43,900 --> 01:34:46,310
playing in order to construct j

1495
01:34:46,350 --> 01:34:49,830
that change and to be the delivered so these are really the labels that are

1496
01:34:49,830 --> 01:34:52,730
used for nuclear physics

1497
01:34:52,760 --> 01:34:56,540
so you see that you have a situation where to feed the different levels and

1498
01:34:56,540 --> 01:35:02,330
you see that sometimes you have gaps here for these numbers eight twenty twenty eight

1499
01:35:02,390 --> 01:35:07,130
be careful twenty eight this gap is created by the spin orbit there is not

1500
01:35:07,140 --> 01:35:09,490
the case for the other one that is created

1501
01:35:09,500 --> 01:35:11,560
by saskia will

1502
01:35:12,230 --> 01:35:15,970
what does it mean this gap it means that if you want for example

1503
01:35:15,990 --> 01:35:21,120
to have a system where is it produced two body you have to take a

1504
01:35:21,120 --> 01:35:24,190
lot of energy to put a lot of energy in the system

1505
01:35:24,200 --> 01:35:26,360
two had these clues

1506
01:35:26,390 --> 01:35:28,370
or if you want to promote one

1507
01:35:28,390 --> 01:35:32,200
but get to here you really have to to promote to give a lot of

1508
01:35:32,200 --> 01:35:36,290
energy so that's why they really stable it's because of these gaps

1509
01:35:36,300 --> 01:35:41,700
in single particle labels

1510
01:35:42,770 --> 01:35:47,980
more precisely how do you describe the ground state of a nucleus with these approaches

1511
01:35:47,990 --> 01:35:50,080
so it's easy you just feel

1512
01:35:50,080 --> 01:35:54,080
today we continue this for for

1513
01:35:54,090 --> 01:35:58,840
the first course ways introducing by six

1514
01:35:59,260 --> 01:36:08,810
tools nationally and then the second course trucks introduced empirical risk minimization

1515
01:36:08,990 --> 01:36:13,970
or what are the third goal condition that we have

1516
01:36:15,540 --> 01:36:21,290
the calgary twenty is that when we do machine learning we are not doing completely

1517
01:36:21,300 --> 01:36:24,080
crazy things

1518
01:36:33,470 --> 01:36:40,080
the slide you have i show yesterday when i was the two

1519
01:36:40,220 --> 01:36:43,970
the two

1520
01:36:45,040 --> 01:36:46,330
so i show you

1521
01:36:46,350 --> 01:36:49,260
why we need to know algebra

1522
01:36:49,350 --> 01:36:51,910
probability trees so

1523
01:36:52,120 --> 01:37:00,050
as i show you in the in the examples of machine problems i show that

1524
01:37:00,050 --> 01:37:05,830
we have a limited amount of examples so that a source the answer to the

1525
01:37:05,830 --> 01:37:13,100
there is also noise in the measurements probably which is another source of uncertainty and

1526
01:37:13,130 --> 01:37:21,600
there is a row randomness also of course is inherent to the observed phenomena

1527
01:37:24,710 --> 01:37:29,380
i'm going to to give a quick

1528
01:37:29,440 --> 01:37:35,050
fast introduction to the basics that we need in gravity effects at the end of

1529
01:37:35,050 --> 01:37:39,270
the week walking network and then i will give you

1530
01:37:39,270 --> 01:37:41,110
more advanced course

1531
01:37:42,040 --> 01:37:47,070
OK about by using the probability that a lot of things

1532
01:37:47,070 --> 01:37:48,210
so the first

1533
01:37:48,770 --> 01:37:54,160
i'm going to give you some vocabulary that i found that it's

1534
01:37:54,180 --> 01:37:55,460
interesting to have

1535
01:37:55,460 --> 01:37:56,350
because the

1536
01:37:56,360 --> 01:38:01,140
we always talking about these things without knowing what what i mean

1537
01:38:01,160 --> 01:38:04,010
so what do we mean by inference

1538
01:38:04,060 --> 01:38:08,730
inference is drawing conclusions from experiments involving

1539
01:38:08,750 --> 01:38:12,060
uncertainty is exactly what we do in fact machine learning

1540
01:38:13,250 --> 01:38:18,600
it's really of it's really close to what is called inference and statistics

1541
01:38:19,720 --> 01:38:25,470
what is the sample set is the set of all possible outcomes of an experiment

1542
01:38:25,500 --> 01:38:29,090
we're going to always

1543
01:38:29,120 --> 01:38:29,890
will be

1544
01:38:29,920 --> 01:38:36,780
always playing with random variables which useful for our mothers and what's the random variable

1545
01:38:36,780 --> 01:38:39,850
is a variable was value is determined by the

1546
01:38:39,870 --> 01:38:45,600
outcome of a random experiments so we will encounter two different kind of random variables

1547
01:38:45,600 --> 01:38:49,940
shared the discrete random variables and the continuous random variables

1548
01:38:52,910 --> 01:38:54,730
discrete random variables

1549
01:38:56,790 --> 01:38:59,880
really from experiments where you can count

1550
01:38:59,910 --> 01:39:05,160
but what are the outcomes why wait continuous random variables

1551
01:39:05,910 --> 01:39:10,200
yes description it's for example like that i still see

1552
01:39:11,570 --> 01:39:16,890
callings or tossing dice way continuous random variables

1553
01:39:17,660 --> 01:39:21,950
arise from from measurements you have provided no

1554
01:39:22,500 --> 01:39:24,120
you measuring

1555
01:39:24,160 --> 01:39:26,130
the weight of

1556
01:39:28,630 --> 01:39:34,950
that is an immersion of that feels bags was coffee automatically what will be the

1557
01:39:35,980 --> 01:39:37,160
the weight of the

1558
01:39:37,190 --> 01:39:38,290
coffee bags

1559
01:39:38,290 --> 01:39:41,890
that would be also a random variable

1560
01:39:41,940 --> 01:39:43,670
because there's some

1561
01:39:43,690 --> 01:39:45,380
random process and there

1562
01:39:45,380 --> 01:39:49,850
and that would be a continuous random variable

1563
01:39:50,380 --> 01:39:51,480
another thing that

1564
01:39:51,500 --> 01:39:56,000
you will probably encounter or you have already in countries that

1565
01:39:57,290 --> 01:40:00,320
different views of probabilities

1566
01:40:01,090 --> 01:40:05,280
which is the frequentist point of view and the other by using point of view

1567
01:40:06,760 --> 01:40:11,230
usually there really in terms miguel one with the other so

1568
01:40:11,240 --> 01:40:14,430
it's not completely far apart but

1569
01:40:14,480 --> 01:40:17,830
it's two different point of view and the probabilities

1570
01:40:17,840 --> 01:40:23,250
the frequentist et cetera you see the probabilities as frequency

1571
01:40:23,310 --> 01:40:26,370
of random repeatable events

1572
01:40:26,380 --> 01:40:33,080
well by using interpretation of probabilities is that the probability is a measure of uncertainty

1573
01:40:33,080 --> 01:40:36,070
and you don't need to have ripped

1574
01:40:36,080 --> 01:40:41,840
the principle of event behind

1575
01:40:43,840 --> 01:40:49,900
OK so first going to talk about discrete random variables

1576
01:40:49,920 --> 01:40:53,010
and for that let's take a concrete example

1577
01:40:53,010 --> 01:40:54,150
let's say a

1578
01:40:54,200 --> 01:40:57,190
we have two boxes one red and one blue

1579
01:40:57,940 --> 01:40:59,770
apples and

1580
01:40:59,780 --> 01:41:02,010
these orange apple

1581
01:41:02,030 --> 01:41:07,010
and the experiment is really as selector out of the box

1582
01:41:07,010 --> 01:41:12,210
because food look what's what is it and then replace the food in the books

1583
01:41:16,130 --> 01:41:18,710
i can utilize the experiment was to

1584
01:41:18,740 --> 01:41:20,210
random variables

1585
01:41:21,180 --> 01:41:24,690
the first one is the identity is the book of the books it is easy

1586
01:41:24,690 --> 01:41:28,370
to read books or is it the blue books

1587
01:41:28,420 --> 01:41:31,050
they have have selected

1588
01:41:31,090 --> 01:41:33,870
then the second of a random variable

1589
01:41:33,920 --> 01:41:36,950
which is the identity of the fruit

1590
01:41:37,340 --> 01:41:45,070
that i have selected using the box which is why is it an orange or

1591
01:41:45,070 --> 01:41:46,570
an apple

1592
01:41:46,580 --> 01:41:50,440
OK so in the frequentist point of view which is mostly what i'm going to

1593
01:41:50,440 --> 01:41:52,280
present here

1594
01:41:52,310 --> 01:41:54,890
the probability of the box

1595
01:41:54,920 --> 01:41:58,120
of taking of selecting the red box

1596
01:41:58,150 --> 01:41:59,780
it is seen as the

1597
01:41:59,820 --> 01:42:03,280
if a repeat six this experiment number of times

1598
01:42:03,300 --> 01:42:06,010
a lot to learn lot of time

1599
01:42:06,710 --> 01:42:11,920
the probability of selecting a red the red box is is equal to the number

1600
01:42:11,920 --> 01:42:17,700
of times the red boxes selected over the number of trials and that when the

1601
01:42:17,700 --> 01:42:20,610
number of trials goes to infinity

1602
01:42:20,620 --> 01:42:21,950
so that that's how

1603
01:42:21,950 --> 01:42:23,750
all the

1604
01:42:24,430 --> 01:42:26,500
lines the vectors to report

1605
01:42:27,250 --> 01:42:34,590
so this is like religion nationally and it's because of the

1606
01:42:36,580 --> 01:42:42,450
that might not even know I think I would take is this is

1607
01:42:44,450 --> 01:42:48,310
but that the 1 with the majority of the world

1608
01:42:49,190 --> 01:42:51,240
and that

1609
01:42:52,070 --> 01:42:56,150
you can also multiply you can also cut the matrix in the

1610
01:42:58,230 --> 01:43:03,890
and location might want to get so

1611
01:43:05,890 --> 01:43:08,230
useful as I

1612
01:43:14,290 --> 01:43:16,830
collecting money

1613
01:43:16,840 --> 01:43:18,230
and not going

1614
01:43:18,260 --> 01:43:20,650
like many of present

1615
01:43:20,690 --> 01:43:25,670
on the cabinet which imports were not supposed to to work with this

1616
01:43:25,910 --> 01:43:30,520
state and analysis and the whereas

1617
01:43:30,600 --> 01:43:31,790
I think

1618
01:43:34,850 --> 01:43:39,950
but the size of the what they have to do with matter

1619
01:43:40,510 --> 01:43:46,390
you certainly won't so here's the rule of law

1620
01:43:46,950 --> 01:43:53,890
this is loss of life of of any of maybe

1621
01:43:54,490 --> 01:43:56,620
1 and 2 and 3 and 4

1622
01:43:56,690 --> 01:44:01,660
a here and the loss of being 1 of the things we really

1623
01:44:03,290 --> 01:44:10,430
then the end I can find a lot find that what it tells what that

1624
01:44:10,430 --> 01:44:14,510
what going be quiet was about multiple

1625
01:44:15,970 --> 01:44:18,490
what's going on that

1626
01:44:18,890 --> 01:44:25,810
might be this kind of like the matrices might invite 25

1627
01:44:25,820 --> 01:44:27,730
with a lot the

1628
01:44:28,310 --> 01:44:31,090
the take the case of a lot of things

1629
01:44:33,230 --> 01:44:41,790
and what is the moment of what and what to here's what's what's the what

1630
01:44:41,830 --> 01:44:43,390
the what the answer

1631
01:44:43,810 --> 01:44:51,210
it is 1 of the ones that matrix and this is the right side of

1632
01:44:51,210 --> 01:44:54,370
the environment and

1633
01:44:56,270 --> 01:44:58,850
what's the what 1 of them

1634
01:44:59,810 --> 01:45:05,910
aid to be theory is just like a lot of times what

1635
01:45:06,010 --> 01:45:14,250
I don't know nobody not even down to the end of the war

1636
01:45:14,750 --> 01:45:19,470
but somehow of we tend to all clients

1637
01:45:19,490 --> 01:45:21,600
were doing the same thing most likely

1638
01:45:21,620 --> 01:45:28,360
so this is familiar with page is what we're really doing going to we do

1639
01:45:28,360 --> 01:45:33,990
about column by column by column and row and

1640
01:45:34,040 --> 01:45:39,340
OK I just like to get the rules and for many more

1641
01:45:46,150 --> 01:45:48,890
the Committee for the

1642
01:45:49,230 --> 01:45:53,890
for this year's end of the frame

1643
01:45:54,650 --> 01:45:59,930
and you have a square

1644
01:46:01,410 --> 01:46:10,690
OK so I'm going remain

1645
01:46:16,530 --> 01:46:20,890
and it may or may not have the been right not only to the head

1646
01:46:20,890 --> 01:46:28,670
and the fact that the most important question you can ask about matrix is affects

1647
01:46:28,700 --> 01:46:30,430
you know what

1648
01:46:30,760 --> 01:46:36,720
is invertible or not it is invertible and then there is someone matrix

1649
01:46:36,780 --> 01:46:39,120
the holidays

1650
01:46:39,610 --> 01:46:40,560
and 1

1651
01:46:41,930 --> 01:46:54,630
what if any of so this is it the beginning it wants make

1652
01:46:54,750 --> 01:47:02,770
can be really central of figure out whether an image of the power

1653
01:47:02,930 --> 01:47:05,130
but what's

1654
01:47:05,850 --> 01:47:11,130
what you I have the effect

1655
01:47:11,890 --> 01:47:19,990
this matrix as a and this is what the idea

1656
01:47:20,930 --> 01:47:25,970
and little more a normal

1657
01:47:27,500 --> 01:47:36,910
I mean that right now that's like 1 is sitting on the left but really

1658
01:47:36,970 --> 01:47:43,040
and inverted in 1st would be on the right as well so that

1659
01:47:43,530 --> 01:47:45,250
this is true

1660
01:47:47,200 --> 01:47:53,860
that is inherent in practice is not a problem

1661
01:47:55,350 --> 01:48:01,010
this is something that is not the true what works

1662
01:48:02,290 --> 01:48:08,370
that alleged that the square matrices a left inverse is also a

1663
01:48:08,790 --> 01:48:14,550
if I can find the image from the land that is the identity also that

1664
01:48:17,830 --> 01:48:24,430
for rectangular matrices will see 11 universe that is the right from fact the state

1665
01:48:24,600 --> 01:48:30,350
will know what square matrices states of our and hand

1666
01:48:31,290 --> 01:48:35,540
if focus so

1667
01:48:35,560 --> 01:48:37,270
give something

1668
01:48:38,410 --> 01:48:42,590
the at the beginning of the year but let's talk about the case with

1669
01:48:44,160 --> 01:48:46,190
so this is it

1670
01:48:47,190 --> 01:48:50,010
the matrices called internal

1671
01:48:51,990 --> 01:48:55,610
or not you

1672
01:49:00,610 --> 01:49:10,290
and 1 of the issues identify how would you that and I'm talking about the

1673
01:49:10,290 --> 01:49:11,630
welcome everybody

1674
01:49:12,210 --> 01:49:17,670
please ask when i when i talk to ask questions stop me and it's something

1675
01:49:17,670 --> 01:49:22,910
i when i cluster wikipedia something we stop so my name is you really skill

1676
01:49:22,910 --> 01:49:28,610
that's currently i'm my post-doc at cornell and i'll be joining stanford CS in full

1677
01:49:28,610 --> 01:49:33,020
as assistant professor and basically what talk about is how to model

1678
01:49:33,030 --> 01:49:36,820
and large social and information networks so

1679
01:49:36,830 --> 01:49:38,900
basically what the goal of the tutorial

1680
01:49:38,970 --> 01:49:44,350
the goal is to basically introduced statistical properties these networks have how to think about

1681
01:49:44,350 --> 01:49:48,520
this problem is and how to model them on how to then exploit these properties

1682
01:49:48,530 --> 01:49:52,250
so do certain types of tasks better because we know what is the structure of

1683
01:49:52,250 --> 01:49:55,730
this of these networks and parts of

1684
01:49:55,780 --> 01:49:57,200
so the parts of work

1685
01:49:57,210 --> 01:50:01,790
can be done with a bunch of other collaborators or people i borrowed slides from

1686
01:50:01,790 --> 01:50:04,080
and so on so

1687
01:50:04,120 --> 01:50:08,990
this is this is the so what i'm they talking about these networks graphs but

1688
01:50:09,370 --> 01:50:13,940
graphs as they meaning that the data that will be working with natural forms in

1689
01:50:14,450 --> 01:50:18,320
enough or not naturally comes in the form of a graph so what we have

1690
01:50:18,320 --> 01:50:20,990
is some nodes in some relations for some edges

1691
01:50:21,030 --> 01:50:25,600
between them and the there are many kinds of such data that naturally occurs in

1692
01:50:25,600 --> 01:50:30,590
various domains so internet meaning around talking to one another that can be looked at

1693
01:50:30,610 --> 01:50:36,280
the graph safe state scientific citation networks right so i have papers but the nodes

1694
01:50:36,280 --> 01:50:41,770
and i have citations references between them that also again defines very rich and interesting

1695
01:50:41,770 --> 01:50:45,710
network with every node is like a document and all the all the words in

1696
01:50:45,710 --> 01:50:50,510
the time of publication the authors and everything they have citations between them what about

1697
01:50:50,510 --> 01:50:55,680
web meaning of web pages and hyperlinks again is a big graphs who dates to

1698
01:50:55,680 --> 01:51:00,910
whom can be thought of as graph who has sexual intercourse with whom again can

1699
01:51:00,910 --> 01:51:04,210
be thought of the graph and so on the right so these are all different

1700
01:51:04,210 --> 01:51:10,180
types of data that actually comes in the form of a graph as computer scientists

1701
01:51:10,620 --> 01:51:16,010
we are especially interested about in in network data because of one one thing and

1702
01:51:16,010 --> 01:51:22,010
that thing is that we have a large on line computing applications where they naturally

1703
01:51:22,010 --> 01:51:25,590
can be can be represented in the form of a network what i mean by

1704
01:51:25,590 --> 01:51:30,760
that is that we have online communities online social networking websites that have like hundreds

1705
01:51:30,760 --> 01:51:34,810
of millions of users like facebook myspace linkedin and so on we have a very

1706
01:51:34,810 --> 01:51:40,940
rich communication networks is the messenger instant messaging because of that forum has billions of

1707
01:51:40,940 --> 01:51:47,930
users email again is is very rich communication network to be a very interesting temporal

1708
01:51:47,930 --> 01:51:55,340
dynamics also another very rich source of data is news and social media data meaning

1709
01:51:55,530 --> 01:52:01,760
blogging and basically user-generated web two point o type of contract people are now generating

1710
01:52:01,760 --> 01:52:06,630
content creating links and so on and again this content is highly there then emigrated

1711
01:52:06,630 --> 01:52:10,100
each naturally defines me network

1712
01:52:10,120 --> 01:52:14,690
there are also many other domains were networks come come come into play especially in

1713
01:52:14,690 --> 01:52:20,470
systems biology health many where again i can represent interactions between genes in the form

1714
01:52:20,470 --> 01:52:24,940
of a network and so so why would you want to look at the data

1715
01:52:24,950 --> 01:52:28,640
in the form of a network and the reason for that is because you can

1716
01:52:28,640 --> 01:52:32,910
think of a network as some kind of you have entities that typically weakly interacting

1717
01:52:32,910 --> 01:52:36,960
or some somehow related to one another so you have sort of moving away from

1718
01:52:37,000 --> 01:52:41,240
i i d cases you like and there have been a lot of success stories

1719
01:52:41,240 --> 01:52:46,670
showing that using information helps a lot for a lot of the value a prime

1720
01:52:46,740 --> 01:52:52,070
example for that is probably web search right sort of google realizing that you can

1721
01:52:52,080 --> 01:52:55,960
rank web pages based on the link structure right or you know

1722
01:52:55,970 --> 01:53:01,100
topic detection classification where you know links between between your entities so your data points

1723
01:53:02,260 --> 01:53:05,350
so this is why you want to work in networks

1724
01:53:05,360 --> 01:53:08,400
so just to show two slides on

1725
01:53:08,410 --> 01:53:11,050
what kinds of networks data there are

1726
01:53:11,060 --> 01:53:16,210
so i talked a bit about information networks so what the web of hyperlinks citation

1727
01:53:16,210 --> 01:53:19,830
networks i mentioned or if you have a blog networks in a sense who who

1728
01:53:19,830 --> 01:53:22,130
gets who copies information from home

1729
01:53:22,210 --> 01:53:27,400
then at the very source of such data is a social networking they expect especially

1730
01:53:27,400 --> 01:53:34,030
now with the emergence of social networking website and applications you have you this can

1731
01:53:34,030 --> 01:53:38,810
scale to hundreds of millions of nodes so before sort of before the emergence of

1732
01:53:38,810 --> 01:53:40,440
the web two two point o if you like

1733
01:53:40,460 --> 01:53:45,360
these networks are usually were heavily studied but they were usually small because somebody had

1734
01:53:45,360 --> 01:53:49,500
to go on the field ask people place name your best friend or you know

1735
01:53:49,520 --> 01:53:53,770
they would have to conduct interviews give out questionnaires and so on so you think

1736
01:53:53,770 --> 01:53:58,700
that collecting that would be sort of it sort of was a very hard task

1737
01:53:58,710 --> 01:54:03,450
now we develop and everything the network they just comes to us for so

1738
01:54:03,660 --> 01:54:09,010
organisation networks in the sense of relations in companies communication networks who talks whom who

1739
01:54:09,010 --> 01:54:13,240
talks to whom call funds to whom we most whom which experts who with whom

1740
01:54:13,730 --> 01:54:18,360
collaboration networks this can be for example is saying if you collaboration networks in terms

1741
01:54:18,360 --> 01:54:23,680
of who writes where in the next two authors if the right people together i

1742
01:54:23,680 --> 01:54:27,170
mentioned this once and they have topic here

1743
01:54:27,640 --> 01:54:32,080
the other routes source technological networks what i mean by that is so for example

1744
01:54:32,110 --> 01:54:38,660
power or water distribution networks meaning physical pipes or cables connecting different locations that defines

1745
01:54:38,810 --> 01:54:44,940
network neural networks and even networks and line networks extensive looked at where again for

1746
01:54:44,940 --> 01:54:52,040
example in networks my airports are vertices of the graph edges between them are basically

1747
01:54:52,530 --> 01:54:54,130
not like flight

1748
01:54:54,140 --> 01:54:57,360
so that's about five from one airport to to another i create and of course

1749
01:54:57,370 --> 01:55:01,770
the search actually for example weighted i count how many planes fly how many passengers

1750
01:55:01,780 --> 01:55:07,580
fly how often they fly things like that telephone networks have been looked at internet

1751
01:55:07,580 --> 01:55:10,430
autonomous systems and here is sort of pictures of this

1752
01:55:11,420 --> 01:55:17,260
and then the other set of examples come come from like systems biology so biological

1753
01:55:17,260 --> 01:55:24,330
networks from like metabolic networks food webs so who eats whom neural networks gene regulatory

1754
01:55:24,330 --> 01:55:29,200
networks language so words documents can be also looked in the form of a network

1755
01:55:29,430 --> 01:55:34,300
either in terms of sort of semantic network where you are connecting concepts that interact

1756
01:55:34,300 --> 01:55:38,400
or appear together or you can just sort of create network based on the sequence

1757
01:55:38,400 --> 01:55:42,350
of words and see you can see a lot of very interesting things about the

1758
01:55:42,350 --> 01:55:43,510
structure of the

1759
01:55:43,530 --> 01:55:48,250
of the document and software has been looking for of for network either call graphs

1760
01:55:48,250 --> 01:55:53,330
or you know which which had file includes which had five and so on so

1761
01:55:53,330 --> 01:55:57,540
this is sort of the network types of network data and to motivate why would

1762
01:55:57,540 --> 01:56:00,790
you want to study natural they here sort of a quote from jim gray right

1763
01:56:00,790 --> 01:56:05,710
and he said the emergence of cyberspace where and what the web is like the

1764
01:56:05,710 --> 01:56:09,530
discovery of a new continent right so he was trying to say that this networks

1765
01:56:09,530 --> 01:56:15,160
you know phenomena they are not some kind of engineering designed fact that somehow naturally

1766
01:56:15,160 --> 01:56:20,990
occurring so there are some kind of some kind of phenomena or genuine data that

1767
01:56:21,010 --> 01:56:25,510
is what to study so one question that that gets motivated here is what kind

1768
01:56:25,510 --> 01:56:30,230
of pattern sort of kind of properties what kind of common denominator all these different

1769
01:56:30,230 --> 01:56:35,220
sources sort different kinds of networks having what aspects of a similar what aspects of

1770
01:56:35,230 --> 01:56:39,060
the different so that sort of one common question people have been asking

1771
01:56:39,070 --> 01:56:43,140
another question then is you know we want to get this loss of

1772
01:56:43,150 --> 01:56:47,500
the motion of the web right so as a senior statistical irregularities were better and

1773
01:56:47,600 --> 01:56:49,990
i would like to understand why they are there

1774
01:56:50,560 --> 01:56:53,960
so what i want is i want some kind of statistical methods that will allow

1775
01:56:53,960 --> 01:56:55,460
me to first quantify

1776
01:56:55,460 --> 01:56:57,200
so i'm would look like

1777
01:56:57,290 --> 01:56:59,290
well you're going to see something

1778
01:56:59,300 --> 01:57:01,430
like this let's take a moment

1779
01:57:01,780 --> 01:57:03,380
equals zero

1780
01:57:03,390 --> 01:57:05,540
so cosine omega t

1781
01:57:05,550 --> 01:57:08,540
plus one

1782
01:57:08,620 --> 01:57:10,990
so we're going to have occurred like this

1783
01:57:11,040 --> 01:57:12,580
he goes up to

1784
01:57:12,590 --> 01:57:18,260
the y zero

1785
01:57:18,270 --> 01:57:20,440
like this

1786
01:57:20,450 --> 01:57:21,920
and this year

1787
01:57:21,970 --> 01:57:23,880
is that my two y zero

1788
01:57:23,890 --> 01:57:26,790
these points

1789
01:57:26,830 --> 01:57:28,490
i will never move

1790
01:57:28,500 --> 01:57:33,210
always stand still there's nothing like a traveling wave traveling wave these points will see

1791
01:57:33,210 --> 01:57:37,180
to wave goodbye to go up and down they nevertheless they sit still

1792
01:57:37,260 --> 01:57:41,650
they have a name we call the nodes

1793
01:57:41,680 --> 01:57:46,180
let's now look a little later

1794
01:57:46,220 --> 01:57:48,300
let's look at p

1795
01:57:48,330 --> 01:57:50,210
equals one quarter

1796
01:57:50,220 --> 01:57:52,270
over periods

1797
01:57:52,280 --> 01:57:54,630
the cosine is zero

1798
01:57:54,670 --> 01:57:57,530
so it is not a single point on the string

1799
01:57:57,610 --> 01:57:58,900
that is not zero

1800
01:57:58,940 --> 01:58:01,570
string looks like this

1801
01:58:01,580 --> 01:58:03,470
he took a picture of the string

1802
01:58:03,490 --> 01:58:07,520
even though it's also just a straight line

1803
01:58:07,530 --> 01:58:08,370
and i

1804
01:58:08,430 --> 01:58:09,400
if we do

1805
01:58:09,410 --> 01:58:11,540
look a little later we look at

1806
01:58:11,580 --> 01:58:12,990
equals one half

1807
01:58:13,000 --> 01:58:14,400
the period

1808
01:58:14,430 --> 01:58:16,790
then the cosine is minus one

1809
01:58:16,840 --> 01:58:22,960
now the curve look like this

1810
01:58:23,010 --> 01:58:24,680
so what does it mean

1811
01:58:24,720 --> 01:58:25,800
if we just

1812
01:58:25,860 --> 01:58:27,770
look what he happening

1813
01:58:27,810 --> 01:58:29,550
this is what's going to happen

1814
01:58:30,750 --> 01:58:34,550
it's just doing this and the points that stand still nothing is going like this

1815
01:58:34,550 --> 01:58:36,660
nothing is going like this

1816
01:58:36,670 --> 01:58:39,390
you see this point going up and down up and down up and down and

1817
01:58:39,390 --> 01:58:41,680
this will do the same and these nodes

1818
01:58:41,690 --> 01:58:42,410
we'll do

1819
01:58:43,200 --> 01:58:47,090
that's what they a standing wave look like and i think the name standing wave

1820
01:58:47,090 --> 01:58:49,560
is a very appropriate name very descriptive

1821
01:58:49,570 --> 01:58:54,310
because it's really standing it's not it's not moving at least not travelling along the

1822
01:58:54,370 --> 01:58:57,120
x direction

1823
01:58:57,160 --> 01:58:59,490
can we make a standing wave

1824
01:58:59,540 --> 01:59:01,260
yes we can i will do that

1825
01:59:02,990 --> 01:59:04,410
standing wave

1826
01:59:04,430 --> 01:59:07,250
can be made by

1827
01:59:08,620 --> 01:59:10,880
rotating in that fashion

1828
01:59:10,920 --> 01:59:12,990
a string

1829
01:59:13,010 --> 01:59:14,250
have string

1830
01:59:14,290 --> 01:59:15,000
OK i

1831
01:59:15,010 --> 01:59:17,240
that's the string to the wall there

1832
01:59:17,250 --> 01:59:20,750
and i move it up and down here

1833
01:59:20,760 --> 01:59:22,360
it always goes in

1834
01:59:22,370 --> 01:59:24,570
i do just this like the rotating disk

1835
01:59:24,630 --> 01:59:25,780
the wave travels

1836
01:59:25,860 --> 01:59:27,870
the wave is reflected

1837
01:59:27,910 --> 01:59:28,630
and so

1838
01:59:28,640 --> 01:59:31,290
i have always going in never wave coming back

1839
01:59:31,420 --> 01:59:33,940
not always going through each other

1840
01:59:34,000 --> 01:59:37,540
and if the conditions are just right

1841
01:59:37,590 --> 01:59:40,790
then these reflected waves this one will reflect

1842
01:59:40,800 --> 01:59:44,530
what are the right here it will reflect again goes back again and it will

1843
01:59:44,530 --> 01:59:46,640
continue to reflect

1844
01:59:46,690 --> 01:59:48,460
if the conditions are just right

1845
01:59:48,490 --> 01:59:50,720
then these reflected waves real

1846
01:59:50,750 --> 01:59:52,720
support each other

1847
01:59:52,790 --> 01:59:53,750
and they will

1848
01:59:53,760 --> 01:59:55,830
generate a large amplitude

1849
01:59:55,840 --> 01:59:58,130
as i will demonstrate to you

1850
01:59:58,140 --> 01:59:59,790
but that's only the case

1851
01:59:59,800 --> 02:00:02,910
four very specific frequencies

1852
02:00:02,940 --> 02:00:03,990
we call those

1853
02:00:06,540 --> 02:00:11,310
the lowest possible frequency for which this happens which we call the fundamental

1854
02:00:11,320 --> 02:00:12,450
will make the

1855
02:00:12,460 --> 02:00:17,620
the strings vibrate like this

1856
02:00:17,660 --> 02:00:19,430
the whole thing che

1857
02:00:20,320 --> 02:00:27,290
o che che i recall that the fundamental

1858
02:00:27,300 --> 02:00:33,290
the college also the first harmonic

1859
02:00:33,310 --> 02:00:35,760
if not to increase the frequencies

1860
02:00:35,790 --> 02:00:37,310
then i get a second

1861
02:00:37,370 --> 02:00:38,880
resonance frequency

1862
02:00:38,890 --> 02:00:41,160
and a

1863
02:00:41,200 --> 02:00:44,300
note jumps in the middle there is already a note here

1864
02:00:44,320 --> 02:00:45,780
and there is an old here

1865
02:00:45,790 --> 02:00:50,000
this motion of my hand here is very small as i will demonstrate to you

1866
02:00:50,000 --> 02:00:53,290
for all practical purposes you can think of is being the note

1867
02:00:53,450 --> 02:00:55,180
so now to string

1868
02:00:55,190 --> 02:00:56,830
in the second harmonic

1869
02:00:56,890 --> 02:00:59,210
also made like this which which

1870
02:00:59,740 --> 02:01:02,290
this is the second harmonic

