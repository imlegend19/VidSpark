1
00:00:00,000 --> 00:00:01,610
so all

2
00:00:01,620 --> 00:00:02,960
do you by

3
00:00:03,020 --> 00:00:07,860
is this operator right so it takes a vector and returns vector

4
00:00:07,860 --> 00:00:09,050
so you have seen

5
00:00:09,290 --> 00:00:15,280
operators before i believe so linear operator just stinks of actor

6
00:00:15,280 --> 00:00:18,560
apply some interested and that returns of vector

7
00:00:19,580 --> 00:00:20,950
so this does

8
00:00:20,970 --> 00:00:22,650
almost the same thing

9
00:00:22,660 --> 00:00:26,100
so this is sort of a matrix right

10
00:00:26,100 --> 00:00:30,810
so if you if you just constantly on this two things

11
00:00:30,820 --> 00:00:34,310
so this is a matrix vector multiplication

12
00:00:35,950 --> 00:00:39,810
so you could you could write that into a matrix form

13
00:00:39,810 --> 00:00:42,350
so this is almost likely not operator

14
00:00:42,360 --> 00:00:46,260
the only difference is that we are adding this other left

15
00:00:46,270 --> 00:00:50,020
so compactly you could write

16
00:00:50,550 --> 00:00:54,090
well this this thing here

17
00:00:54,140 --> 00:00:56,570
so t of phi

18
00:00:58,710 --> 00:00:59,790
at the

19
00:00:59,800 --> 00:01:04,530
it's just a factor glasgow more times some matrix

20
00:01:04,540 --> 00:01:06,510
apply to be

21
00:01:06,520 --> 00:01:08,330
you could write

22
00:01:08,340 --> 00:01:10,280
so that's the meaning of fear

23
00:01:10,330 --> 00:01:11,180
and so

24
00:01:11,200 --> 00:01:14,440
yup i takes so that editors of actor and so

25
00:01:14,450 --> 00:01:17,160
the meaning of this expedition is that

26
00:01:17,310 --> 00:01:19,640
if you

27
00:01:19,690 --> 00:01:21,180
if you do this

28
00:01:21,200 --> 00:01:22,460
for the

29
00:01:22,530 --> 00:01:26,340
value function underlying the policy you get the same thing

30
00:01:26,360 --> 00:01:28,880
we want to prove that

31
00:01:28,940 --> 00:01:32,260
so they should hold in particle for every state so

32
00:01:32,320 --> 00:01:36,270
you know this is a vector so for all components of the of the fact

33
00:01:36,270 --> 00:01:39,850
that they should hold that the components the components to get by

34
00:01:39,860 --> 00:01:45,430
looking at the values at the states right

35
00:01:45,450 --> 00:01:46,670
OK so

36
00:01:46,690 --> 00:01:47,600
what we do

37
00:01:47,620 --> 00:01:51,160
is that it this is a very simple very straightforward

38
00:01:51,220 --> 00:01:56,030
so you just separate the first and from there the rest so was the first

39
00:01:56,030 --> 00:01:59,020
time it just are zero

40
00:01:59,060 --> 00:02:00,070
so you know

41
00:02:00,090 --> 00:02:02,270
expectations are

42
00:02:02,280 --> 00:02:07,120
and it is so i can just pull out i zero

43
00:02:16,580 --> 00:02:20,650
what's remaining is gonna time so i can pull out a gun laws for active

44
00:02:20,650 --> 00:02:26,450
factor of gun

45
00:02:28,450 --> 00:02:30,580
and here time goes

46
00:02:30,680 --> 00:02:33,680
from one to infinity

47
00:02:33,770 --> 00:02:37,510
and if i could have go on i got t

48
00:02:37,530 --> 00:02:41,160
part of t t minus one here

49
00:02:44,000 --> 00:02:46,230
the series was works

50
00:02:46,280 --> 00:02:48,190
OK and former line

51
00:02:48,250 --> 00:02:52,700
so what's missing here

52
00:02:52,780 --> 00:02:55,820
so we can expand this saying using go

53
00:02:55,830 --> 00:02:57,420
the low

54
00:02:57,500 --> 00:03:01,570
after that probabilities by conditioning on the next day

55
00:03:01,620 --> 00:03:06,080
so this is just

56
00:03:06,090 --> 00:03:09,870
conditioning on next state

57
00:03:09,900 --> 00:03:13,120
we have to take the probability

58
00:03:13,170 --> 00:03:16,100
that x one

59
00:03:16,100 --> 00:03:17,960
because the next state

60
00:03:17,980 --> 00:03:22,260
given that x is zero it was max

61
00:03:24,420 --> 00:03:26,020
take expectation

62
00:03:26,020 --> 00:03:28,020
of are zero

63
00:03:28,070 --> 00:03:29,660
given that x

64
00:03:29,710 --> 00:03:31,610
so you know it was x

65
00:03:31,610 --> 00:03:36,440
and i introduced this condition x money was

66
00:03:37,300 --> 00:03:38,620
so that just

67
00:03:38,680 --> 00:03:40,200
you see it

68
00:03:40,250 --> 00:03:41,610
not anymore

69
00:03:41,660 --> 00:03:44,770
oh my god

70
00:03:44,770 --> 00:03:46,610
OK so

71
00:03:46,620 --> 00:03:48,750
not in use

72
00:03:48,750 --> 00:03:51,670
remember we started to sleep by fax

73
00:03:51,730 --> 00:03:53,890
so i write did here again

74
00:03:55,400 --> 00:03:56,700
you take the sum

75
00:03:56,710 --> 00:04:00,500
all are possible next states what you do is that your condition on the next

76
00:04:01,780 --> 00:04:03,910
so what you will have is that

77
00:04:03,930 --> 00:04:07,550
you have the expectation was this new condition

78
00:04:07,580 --> 00:04:13,300
times the probability of reaching the next given that you started with the first

79
00:04:13,310 --> 00:04:27,970
so that as the after that priority i

80
00:04:28,070 --> 00:04:33,540
and actually you can do that this false expectations so the first expectation was just

81
00:04:33,540 --> 00:04:36,000
the main if you want timestep zero

82
00:04:36,010 --> 00:04:41,820
the second expectation was or the future reward from time step t one

83
00:04:41,830 --> 00:04:52,760
i don't much space

84
00:05:05,580 --> 00:05:10,000
and OK so what i know i'm going to change the next thing that it

85
00:05:10,000 --> 00:05:12,560
previously it it went from one

86
00:05:12,610 --> 00:05:15,800
but i want to go it from zero

87
00:05:15,850 --> 00:05:18,360
so that it looks more similar to what we

88
00:05:18,380 --> 00:05:20,210
his father

89
00:05:20,290 --> 00:05:24,200
if it goes from zero i should write go more to the point of t

90
00:05:24,260 --> 00:05:33,120
but are t plus five

91
00:05:38,140 --> 00:05:41,900
the scare

92
00:05:41,950 --> 00:05:45,320
so now we have just using properties of expectations

93
00:05:45,430 --> 00:05:48,530
i think fast

94
00:05:48,560 --> 00:05:50,610
so what the first probability here

95
00:05:50,620 --> 00:05:55,700
so this is the transition probability of transitioning from state x to state y

96
00:05:58,080 --> 00:06:02,350
probability is just what's written here so it given by the

97
00:06:02,360 --> 00:06:04,880
probability transition kernel p

98
00:06:05,620 --> 00:06:07,300
so by definition

99
00:06:07,340 --> 00:06:09,840
because we are following policy pi

100
00:06:09,880 --> 00:06:14,700
so the probability of transitioning from a given state act

101
00:06:14,710 --> 00:06:18,250
to some other given state of mind which is given by this one slide just

102
00:06:21,590 --> 00:06:24,570
the sum of the y

103
00:06:24,620 --> 00:06:26,270
x and

104
00:06:26,270 --> 00:06:28,520
i have an axe

105
00:06:28,540 --> 00:06:33,050
OK so one of the first one here so this is just the the

106
00:06:33,100 --> 00:06:37,940
the immediate reward received step zero

107
00:06:37,970 --> 00:06:39,830
conditioned on that

108
00:06:39,830 --> 00:06:41,840
we started

109
00:06:41,890 --> 00:06:44,060
that's the that's another

110
00:06:44,140 --> 00:06:45,890
state y

111
00:06:45,900 --> 00:06:47,180
and v

112
00:06:47,200 --> 00:06:49,780
we are following policy pi

113
00:06:49,780 --> 00:06:54,560
so by definition that means that was just this quantity here

114
00:06:54,570 --> 00:06:58,750
so this is exactly the interpretation of you know the process u

115
00:06:58,790 --> 00:07:02,410
so this was by definition

116
00:07:02,510 --> 00:07:05,280
and what's left is good times

117
00:07:05,280 --> 00:07:06,830
this quantity here

118
00:07:06,900 --> 00:07:09,100
so this is the fourth

119
00:07:09,150 --> 00:07:10,460
this contd

120
00:07:10,460 --> 00:07:15,290
so i'm trying to define the notion of by the logical consequence of gamma and

121
00:07:15,290 --> 00:07:18,290
there's a whole lot of mumbo jumbo on board which i'm going to talk you

122
00:07:18,320 --> 00:07:19,900
through OK

123
00:07:19,920 --> 00:07:27,530
it looks frightening it's just notation is everything can be compiled back down to have

124
00:07:27,570 --> 00:07:32,190
truth values of five being true in the world or not

125
00:07:34,440 --> 00:07:39,300
let k be the class of all kripke frames in the particular models

126
00:07:40,510 --> 00:07:42,320
script k the class

127
00:07:43,560 --> 00:07:48,230
that kv the class of all kripke models this when the car the kripke frames

128
00:07:48,910 --> 00:07:50,690
and this particular one if

129
00:07:50,700 --> 00:07:54,160
and let gamma be a set of formulae five one

130
00:07:54,190 --> 00:07:55,790
what i want to try and tell you

131
00:07:55,800 --> 00:07:59,700
is when pfizer logical consequence of gamma them

132
00:08:00,730 --> 00:08:02,760
what i've shown you is that

133
00:08:02,810 --> 00:08:08,550
there are there is one notion of formula being true at the world or not

134
00:08:08,610 --> 00:08:10,940
OK that was what valuation did for

135
00:08:10,980 --> 00:08:12,310
i want to live that

136
00:08:12,330 --> 00:08:16,240
OK i need that in order to get to where i'm here so

137
00:08:16,290 --> 00:08:18,820
and we use the term forces

138
00:08:18,880 --> 00:08:20,930
and there are three types of forcing

139
00:08:20,940 --> 00:08:24,350
forcing in the world in a model in frame and i'm going to define them

140
00:08:26,650 --> 00:08:29,650
so when i say w forces five

141
00:08:29,680 --> 00:08:31,070
i just means

142
00:08:32,130 --> 00:08:35,410
the truth value of the w is equal to true

143
00:08:35,430 --> 00:08:37,080
and i'm going to write it like that

144
00:08:37,100 --> 00:08:38,690
we have vertical

145
00:08:38,710 --> 00:08:40,230
o double bond

146
00:08:40,270 --> 00:08:42,340
it's different to either of those

147
00:08:42,350 --> 00:08:46,680
it's neither of those

148
00:08:46,680 --> 00:08:51,020
right so when i write this i just mean fives true at w that's the

149
00:08:51,020 --> 00:08:54,040
way she pronounces

150
00:08:54,080 --> 00:08:57,270
and if i put a slash through it

151
00:08:57,290 --> 00:09:00,930
that means that phi is false w

152
00:09:00,990 --> 00:09:02,440
so this thing

153
00:09:03,130 --> 00:09:04,930
this classical it's either

154
00:09:04,960 --> 00:09:09,730
five find out the true w or false w so i can neither write that

155
00:09:09,740 --> 00:09:13,540
w forces five or w doesn't force five

156
00:09:13,660 --> 00:09:17,930
just try to keep that classic ality going

157
00:09:17,980 --> 00:09:23,210
what about forcing in the model

158
00:09:23,210 --> 00:09:25,000
so i'm going to say

159
00:09:25,050 --> 00:09:28,220
that that model in november model is the triple

160
00:09:28,270 --> 00:09:30,990
a graph with evaluation

161
00:09:30,990 --> 00:09:31,900
like this

162
00:09:31,920 --> 00:09:33,930
graph with evaluation

163
00:09:33,950 --> 00:09:35,060
i'm going to say

164
00:09:35,080 --> 00:09:38,830
the model forces five and write these

165
00:09:38,840 --> 00:09:40,470
when this whole

166
00:09:40,520 --> 00:09:45,490
what does that mean

167
00:09:52,180 --> 00:09:54,830
so this quantifies the meta

168
00:09:54,960 --> 00:09:58,210
many qualify right it's not part of the object language

169
00:09:58,240 --> 00:10:02,050
i'm just using it as a shorthand notation says

170
00:10:02,110 --> 00:10:07,050
every world makes five true

171
00:10:08,740 --> 00:10:11,240
every world makes pietra

172
00:10:11,250 --> 00:10:14,080
but nothing to do with the reachability relation r

173
00:10:14,150 --> 00:10:19,620
nothing to do with the accessibility induces whatever world you choose it has to make

174
00:10:21,740 --> 00:10:25,370
it's a very strong statement if you think about it right because i can put

175
00:10:25,370 --> 00:10:28,470
an evaluation on the actions across here

176
00:10:28,490 --> 00:10:29,620
thank you

177
00:10:29,640 --> 00:10:31,720
others such things

178
00:10:31,770 --> 00:10:42,800
what's the simple example of the formula that is true at every world

179
00:10:42,900 --> 00:10:47,840
in fact all the classical tautologies are going to be true at every world right

180
00:10:47,870 --> 00:10:50,950
because i don't depend on the reachability relation

181
00:10:50,960 --> 00:10:55,050
so you know the six class is non empty there are only something that we

182
00:10:55,050 --> 00:10:57,150
can put here

183
00:10:57,240 --> 00:11:01,580
so again try to get into the habit of pronunciation

184
00:11:01,610 --> 00:11:07,640
this means five through w this means five true everywhere

185
00:11:07,650 --> 00:11:10,050
five true everywhere

186
00:11:10,050 --> 00:11:11,430
what about

187
00:11:11,430 --> 00:11:25,060
in know frame the one is the same

188
00:11:25,150 --> 00:11:28,540
OK what's the frame is a frame is

189
00:11:29,580 --> 00:11:33,910
the graph without evaluation

190
00:11:33,960 --> 00:11:36,410
so what do i do i take frames

191
00:11:36,460 --> 00:11:37,570
and it's is

192
00:11:37,580 --> 00:11:41,150
what is the evaluation i and

193
00:11:41,170 --> 00:11:45,260
i'm going to get triple here which is going to be wrv

194
00:11:45,310 --> 00:11:47,780
well whatever evaluate whatever

195
00:11:47,790 --> 00:11:49,440
the more i like it here

196
00:11:49,460 --> 00:11:51,520
forces five

197
00:11:51,540 --> 00:11:54,580
in other words

198
00:11:54,600 --> 00:11:59,160
it doesn't matter how you change these valuations

199
00:11:59,210 --> 00:12:01,390
that underlying graphs

200
00:12:10,190 --> 00:12:12,780
sorry i e

201
00:12:12,790 --> 00:12:14,110
i'm showing the data

202
00:12:16,000 --> 00:12:22,150
nine and

203
00:12:22,250 --> 00:12:25,610
you know what i'm saying is i give you a graph

204
00:12:26,960 --> 00:12:30,400
so i give you the w nodes and the edges

205
00:12:31,030 --> 00:12:35,090
what i've done here is to give you one particular valuation right i told you

206
00:12:35,090 --> 00:12:39,210
a bit string that told you where the was true here and a bit string

207
00:12:39,210 --> 00:12:43,020
was that what i'm doing now is saying changed the bit strings in any way

208
00:12:43,020 --> 00:12:45,080
you like

209
00:12:45,890 --> 00:12:49,700
you can change the evaluations in any way you like and the formula still has

210
00:12:49,700 --> 00:12:52,400
to be true everywhere in that model

211
00:12:52,400 --> 00:12:57,830
it's a very strong statement so to property of the underlying graph

212
00:12:57,860 --> 00:12:59,450
it is it doesn't matter

213
00:12:59,460 --> 00:13:05,270
how a change evaluations but the formula will still be true

214
00:13:05,330 --> 00:13:09,470
of course if you delete edges changing the graphs right you can't change the graph

215
00:13:09,470 --> 00:13:12,100
our estimates for the parameters of our model

216
00:13:12,120 --> 00:13:14,270
is the inverse of this

217
00:13:14,270 --> 00:13:15,640
miss here

218
00:13:15,660 --> 00:13:17,200
which is the

219
00:13:17,200 --> 00:13:19,200
the second derivative

220
00:13:20,740 --> 00:13:25,330
the the lost mean squared error and then this expanse spores

221
00:13:25,350 --> 00:13:28,720
t TV in the target values

222
00:13:32,600 --> 00:13:34,710
it is so if we solve

223
00:13:34,710 --> 00:13:35,550
so far

224
00:13:36,710 --> 00:13:41,020
long jump data then we find that intercept

225
00:13:41,060 --> 00:13:44,210
is about two seven sex and the slope is

226
00:13:44,220 --> 00:13:48,310
about three quarters of of the unit

227
00:13:48,320 --> 00:13:53,590
right so we've identified the model which will yield the least squares

228
00:13:55,440 --> 00:13:59,590
but what we're really interested in is making predictions and this

229
00:13:59,610 --> 00:14:01,480
so what we want to know do

230
00:14:01,480 --> 00:14:04,200
it's project what the distance

231
00:14:04,670 --> 00:14:07,200
in the long jump that wins the gold medal will be

232
00:14:07,590 --> 00:14:10,510
in the two thousand twelve olympics in london

233
00:14:10,550 --> 00:14:12,150
and then with that

234
00:14:12,170 --> 00:14:15,570
predictions we're all going go to the bookies and we're going to put some money

235
00:14:16,360 --> 00:14:20,820
on whether that distance will actually be the one that wins the gold medal and

236
00:14:20,820 --> 00:14:24,960
not many of you are a confident about winning some money if we use this

237
00:14:24,960 --> 00:14:27,940
least squares estimator

238
00:14:27,940 --> 00:14:31,170
OK not too many of your see

239
00:14:32,640 --> 00:14:34,140
well we can

240
00:14:34,170 --> 00:14:38,580
we can protect all of target values by using our

241
00:14:38,720 --> 00:14:40,350
the estimator

242
00:14:41,590 --> 00:14:43,630
and what we end up with the course

243
00:14:45,230 --> 00:14:46,290
state line

244
00:14:46,300 --> 00:14:51,870
so nothing new there

245
00:14:52,100 --> 00:14:56,690
it seems to be notable but not about models

246
00:14:56,700 --> 00:14:58,970
to fit the data reasonably well

247
00:14:59,020 --> 00:15:04,340
and to explain the sort of gradual improvement over time

248
00:15:04,360 --> 00:15:07,920
so this is all of find

249
00:15:08,020 --> 00:15:13,940
but again from a machine learning perspective this isn't really particularly important

250
00:15:14,040 --> 00:15:17,570
what's important is how good our predictions are

251
00:15:18,640 --> 00:15:20,100
as money is at stake

252
00:15:22,110 --> 00:15:23,310
make a prediction

253
00:15:23,310 --> 00:15:25,840
so what's the distance heart

254
00:15:25,850 --> 00:15:30,620
in two thousand twelve

255
00:15:30,660 --> 00:15:35,760
well our model predicts

256
00:15:35,820 --> 00:15:41,080
a winning distance of three hundred sixty point five inches

257
00:15:41,140 --> 00:15:42,790
that's all it tells us

258
00:15:42,800 --> 00:15:44,760
that's the that's the answer

259
00:15:48,620 --> 00:15:50,730
two hundred sixty point five inches

260
00:15:50,760 --> 00:15:52,330
doesn't see anything about

261
00:15:52,390 --> 00:15:54,070
that's a sure bet are

262
00:15:54,790 --> 00:15:59,260
maybe this isn't such a sure bet that's all we get

263
00:15:59,300 --> 00:16:02,820
well let's just think about this the moment

264
00:16:02,840 --> 00:16:07,810
the current olympic record stands at three hundred fifty inches

265
00:16:07,810 --> 00:16:09,760
and the current world record

266
00:16:09,800 --> 00:16:11,140
which was sets

267
00:16:11,160 --> 00:16:15,560
a long time ago nineteen ninety one is a distance of three hundred fifty two

268
00:16:18,310 --> 00:16:21,230
what does that say about our production

269
00:16:21,270 --> 00:16:27,050
how confident are we in in our production

270
00:16:27,070 --> 00:16:29,600
we're running off to the book is in edinburgh to

271
00:16:29,650 --> 00:16:31,910
ten time there are no

272
00:16:31,920 --> 00:16:35,450
OK so it seems

273
00:16:35,490 --> 00:16:37,450
there are all our prediction is

274
00:16:37,490 --> 00:16:40,990
rather optimistic

275
00:16:41,010 --> 00:16:41,970
and so

276
00:16:41,990 --> 00:16:46,120
just on that basis alone we probably wouldn't trust us

277
00:16:46,170 --> 00:16:48,740
so we use the linear model

278
00:16:48,840 --> 00:16:52,700
maybe what we need

279
00:16:52,700 --> 00:16:57,340
is a model which captures a lot of these

280
00:16:57,370 --> 00:17:00,810
if you look here what was basically done is we've just looked at the we

281
00:17:00,810 --> 00:17:02,160
just model the

282
00:17:02,250 --> 00:17:04,080
the trained

283
00:17:04,100 --> 00:17:05,790
in this

284
00:17:05,790 --> 00:17:06,970
this data

285
00:17:07,940 --> 00:17:11,610
you know there seems to be some sort of oscillation about the train so so

286
00:17:12,720 --> 00:17:16,720
maybe we should be looking out a more rich class of functions to model this

287
00:17:16,720 --> 00:17:18,400
data which would give us more

288
00:17:18,480 --> 00:17:20,940
and useful predictions

289
00:17:20,940 --> 00:17:27,800
so rather than using a linear model finally use nonlinear models

290
00:17:28,960 --> 00:17:31,720
well use nonlinear model but

291
00:17:31,730 --> 00:17:34,660
we'll stick with a model which is linear

292
00:17:34,800 --> 00:17:36,630
in terms of the parameters

293
00:17:37,970 --> 00:17:43,090
and florence will be talking about creating candles and so forth and how you can

294
00:17:43,470 --> 00:17:51,300
still have a linear model was in the parameters and it employs a nonlinear transformation

295
00:17:51,320 --> 00:17:52,610
and therefore half far

296
00:17:52,620 --> 00:17:56,270
nonlinear class of of models so so we can

297
00:17:56,270 --> 00:18:02,150
apply some sort of nonlinear transformation to attribute values and this will provide us with

298
00:18:02,150 --> 00:18:04,330
a much more flexible models

299
00:18:07,080 --> 00:18:09,170
we have much more flexible model

300
00:18:09,220 --> 00:18:12,270
the model is still linear in the parameters

301
00:18:12,280 --> 00:18:14,440
that's provided

302
00:18:14,460 --> 00:18:15,880
that we

303
00:18:15,910 --> 00:18:18,840
do not add any additional parameters

304
00:18:18,860 --> 00:18:22,750
associated with the linear transformations

305
00:18:22,750 --> 00:18:24,960
it would converge to something reasonable

306
00:18:24,970 --> 00:18:30,130
something that i would like to to see an algorithm like that

307
00:18:30,650 --> 00:18:38,370
one idea if you see this fixed point then you know you could get the

308
00:18:38,370 --> 00:18:43,110
idea well we could use this need to directly update the parameters why not just

309
00:18:43,110 --> 00:18:47,600
say theta t plus one equals to t plus b

310
00:18:47,640 --> 00:18:49,080
and be done with

311
00:18:49,090 --> 00:18:54,160
it turns out when you stochastic approximation this v six incredibly noisy and if you

312
00:18:54,160 --> 00:18:56,390
use that directly it blows up your

313
00:18:56,440 --> 00:18:59,060
yes systems very easily

314
00:18:59,060 --> 00:19:02,180
snb my algorithm avoids that

315
00:19:02,200 --> 00:19:06,670
by kind of doing double integration i use the value of the two

316
00:19:06,680 --> 00:19:09,180
slightly changed my games

317
00:19:09,190 --> 00:19:13,140
and i use the value of the games to slightly change my parameters

318
00:19:13,180 --> 00:19:19,310
this is like a double lowpass filter so it filters away a lot of noise

319
00:19:19,310 --> 00:19:23,900
also and the times g which is what's used for the

320
00:19:23,900 --> 00:19:27,130
updating the games is well behaved because

321
00:19:27,830 --> 00:19:31,390
scales like the inverse of the haitian times the gradient

322
00:19:31,410 --> 00:19:38,550
times the gradient and that's fine invariant quantity so if used rescale you're your system

323
00:19:38,570 --> 00:19:41,090
say by a factor of eight

324
00:19:41,110 --> 00:19:45,800
the haitian skills by a is is square so in recession is a to the

325
00:19:45,800 --> 00:19:48,850
minus two and the gradient skills by a

326
00:19:48,870 --> 00:19:52,460
if a to the minus two times eight times a which is one

327
00:19:52,480 --> 00:19:58,530
so that that means you can rescale your system and the algorithm behaves correctly

328
00:19:58,610 --> 00:20:04,820
one little detail i don't actually use the haitian here i use gauss newton approximation

329
00:20:05,550 --> 00:20:10,160
non convex systems so if the haitian can have negative i can values

330
00:20:10,180 --> 00:20:12,420
you get in trouble if you use the haitian

331
00:20:12,640 --> 00:20:16,650
it's sort of a standard problem in optimisation

332
00:20:16,660 --> 00:20:22,010
and the gauss newton approximation is an approximation of the haitian that is guaranteed to

333
00:20:22,010 --> 00:20:25,660
have on the positive side values

334
00:20:25,710 --> 00:20:30,160
and with the gauss newton approximation you can play a similar trick to get fast

335
00:20:30,160 --> 00:20:33,430
gauss newton matrix vector product

336
00:20:33,480 --> 00:20:38,750
it's just sort of differs in detail but it's really the same idea

337
00:20:38,760 --> 00:20:40,720
OK enough theory

338
00:20:40,930 --> 00:20:43,620
in the remaining time i'll just show you what it's

339
00:20:43,620 --> 00:20:44,910
what's it's good for

340
00:20:45,740 --> 00:20:47,990
here's a little benchmark system

341
00:20:48,310 --> 00:20:52,410
since i grew up with neural networks this is an email

342
00:20:52,420 --> 00:20:55,410
even if it's not politically correct anymore

343
00:20:55,420 --> 00:21:00,060
it has two inputs x and y is given as input the coordinates into this

344
00:21:00,060 --> 00:21:04,200
carpet and that the output it has to tell which color the carpenters

345
00:21:04,370 --> 00:21:07,850
OK just set up with two hidden layers ten h

346
00:21:07,850 --> 00:21:11,030
activation functions

347
00:21:11,050 --> 00:21:15,090
not because i think this is a very good architecture for solving this problem this

348
00:21:15,090 --> 00:21:18,130
is allows the system for solving this problem

349
00:21:18,180 --> 00:21:21,270
but i actually wanted allows the system

350
00:21:21,280 --> 00:21:27,160
because i wanted to hard optimisation problem that is still relatively compact so i can

351
00:21:27,160 --> 00:21:31,380
run lots of experiments and this is the hardest thing i know to set up

352
00:21:31,460 --> 00:21:34,980
in a hundred and eighty four parameters

353
00:21:34,990 --> 00:21:41,180
now compare four algorithms simple standard stochastic gradient that first order gradient descent

354
00:21:41,190 --> 00:21:46,610
with the hand tuned optimized stepsize optimized game

355
00:21:46,650 --> 00:21:52,080
and then one of these conventional game adaptation methods that just used eighty times t

356
00:21:52,100 --> 00:21:53,920
minus one

357
00:21:53,930 --> 00:21:59,160
so a stochastic meta descent SMD is my algorithm that i introduced

358
00:21:59,170 --> 00:22:02,320
and the global extended kalman filter which is the

359
00:22:02,370 --> 00:22:05,140
second order online method

360
00:22:05,180 --> 00:22:10,130
which is really the best thing that pretty much anybody knows how to do

361
00:22:10,140 --> 00:22:11,360
on this

362
00:22:11,380 --> 00:22:13,860
and if you look at

363
00:22:17,500 --> 00:22:20,630
curve so what i'm talking here is the loss

364
00:22:20,630 --> 00:22:24,160
against the number of patterns seen

365
00:22:24,180 --> 00:22:25,160
you'll see b

366
00:22:25,200 --> 00:22:29,730
the common filters of course unbeatable it goes right down

367
00:22:29,740 --> 00:22:32,690
the simple stochastic gradient is terrible

368
00:22:32,710 --> 00:22:35,980
conventional step size the adaptation is better

369
00:22:35,990 --> 00:22:39,180
and SMT is better still an interesting

370
00:22:39,210 --> 00:22:42,240
if you plot this out to the asymptote if you go up to a million

371
00:22:42,240 --> 00:22:46,820
patterns SNP stays close to the common filter

372
00:22:46,830 --> 00:22:47,770
so they

373
00:22:47,810 --> 00:22:53,410
both have the correct asymptote these other ones don't approach the optimum they actually have

374
00:22:53,410 --> 00:22:57,090
the noise floor below which they don't optimise anymore

375
00:22:57,100 --> 00:23:01,580
so this is one of the problems you can have in stochastic approximation the noise

376
00:23:01,580 --> 00:23:05,300
of the approximation can prevent you from actually reaching

377
00:23:05,310 --> 00:23:06,590
the optimal

378
00:23:07,200 --> 00:23:08,610
you have to do

379
00:23:08,610 --> 00:23:12,140
that's one of the reasons why you would want to adapt you gain

380
00:23:12,190 --> 00:23:15,460
because once you're at the optimum you want to turn it down so you actually

381
00:23:15,460 --> 00:23:20,760
can kneel down to the optimum instead of hopping around it

382
00:23:20,780 --> 00:23:23,440
so that's one thing you can see here

383
00:23:23,460 --> 00:23:25,080
of course then

384
00:23:25,120 --> 00:23:26,580
if you look at

385
00:23:26,590 --> 00:23:31,830
the cost of the algorithm computationally here's the storage for parameter

386
00:23:31,840 --> 00:23:36,390
the flops per floating point operations per update parameter update and

387
00:23:36,400 --> 00:23:38,380
actual millisecond

388
00:23:43,760 --> 00:23:47,340
game back adaptation algorithms my newsisfree

389
00:23:47,570 --> 00:23:51,630
numbers per parameter the other one uses four numbers

390
00:23:51,640 --> 00:23:54,710
they all wind up being in practice about twice as

391
00:23:54,730 --> 00:24:00,200
expensive a simple stochastic gradient the common filter of course is order and square and

392
00:24:00,200 --> 00:24:04,510
now you see the difference between order in an audience with algorithm and its hundred

393
00:24:04,520 --> 00:24:06,100
eighty four here

394
00:24:06,160 --> 00:24:11,440
so these numbers wind up being absolutely huge and it's in practice it's forty forty

395
00:24:11,440 --> 00:24:13,520
times slower per iteration

396
00:24:14,640 --> 00:24:17,080
gain adaptation algorithms

397
00:24:17,090 --> 00:24:20,150
so now when you reply on the very same data

398
00:24:20,200 --> 00:24:21,630
but you replied it

399
00:24:21,630 --> 00:24:25,380
we've CPU seconds along the bottom

400
00:24:25,400 --> 00:24:30,680
the picture changes dramatically now the the common filters the worst of the bunch

401
00:24:30,710 --> 00:24:33,730
because you simply don't get many iterations in

402
00:24:33,740 --> 00:24:37,580
in fifty seconds of the common filter

403
00:24:37,590 --> 00:24:43,200
the conventional game adaptation and the simple stochastic gradient i kind of similar actually these

404
00:24:43,200 --> 00:24:48,440
two they cross over each other multiple times if you follow it further out so

405
00:24:49,530 --> 00:24:54,440
basically what you gain for the gain adaptation you lose by the algorithm being

406
00:24:54,460 --> 00:24:58,190
twice as slow so you only get half as many iterations

407
00:24:58,200 --> 00:25:02,920
but seems actually by having this better gain adaptation mechanism

408
00:25:02,950 --> 00:25:08,150
is the head in front of the others

409
00:25:09,270 --> 00:25:13,400
OK here's some of correlated data of skip over that

410
00:25:13,410 --> 00:25:16,950
the point is just that somebody can handle data are

411
00:25:16,950 --> 00:25:19,640
and the sender and the receiver in order to make a compressor

412
00:25:20,490 --> 00:25:22,430
and then we looked at thematic coding

413
00:25:22,890 --> 00:25:27,830
and arithmetic coding is this way of thinking about a binary file as defining a single

414
00:25:28,260 --> 00:25:32,120
in real number between zero and one to an extremely large precision

415
00:25:32,780 --> 00:25:33,330
and any

416
00:25:34,000 --> 00:25:36,880
source file can be represented in the same way as

417
00:25:37,560 --> 00:25:40,760
corresponding to an interval on the real line between zero and one

418
00:25:44,990 --> 00:25:47,980
i argued that when you do arithmetic coding

419
00:25:48,610 --> 00:25:51,790
you will get compression down to within two bits of

420
00:25:52,150 --> 00:25:57,060
the information content of the entire file services in terms of its overhead plus two

421
00:25:57,700 --> 00:26:01,310
is potentially about a factor and smaller than the overhead you get

422
00:26:01,750 --> 00:26:02,990
if you use symbol code so

423
00:26:03,770 --> 00:26:07,240
coding is in almost all cases far far better

424
00:26:07,670 --> 00:26:08,540
and symbol codes

425
00:26:13,600 --> 00:26:18,130
i just wanted to flesh out how you'd use arithmatic coding in practice i i

426
00:26:18,130 --> 00:26:21,240
talked about the idea of an oracle that being a piece of software

427
00:26:21,670 --> 00:26:26,600
which uses some procedure to predict the next character given all the characters x one

428
00:26:26,600 --> 00:26:28,890
to x two minus one that have been seen

429
00:26:29,500 --> 00:26:30,320
i wanted just

430
00:26:30,750 --> 00:26:33,710
give you an example of how this works in real state-of-the-art

431
00:26:35,130 --> 00:26:36,810
so here is a fragment of

432
00:26:37,450 --> 00:26:38,500
jane austen's emma

433
00:26:39,610 --> 00:26:40,600
this is the last

434
00:26:41,420 --> 00:26:45,750
fifty or so characters and the context is alarmed at the bureau

435
00:26:46,180 --> 00:26:50,670
and the question is what comes next so how do real compression algorithms work

436
00:26:52,810 --> 00:26:56,900
fairly new state-of-the-art compressor that very widely used is

437
00:26:57,290 --> 00:26:58,660
compressible pee-pee element

438
00:26:59,080 --> 00:27:00,560
his happy paean works

439
00:27:01,190 --> 00:27:03,180
it's called prediction by partial match

440
00:27:03,790 --> 00:27:09,140
hand it's slightly ad hoc but there's a bunch of theoretical justifications and improvements which

441
00:27:09,220 --> 00:27:12,010
we can make sure let me just described the ad hoc methods

442
00:27:12,560 --> 00:27:14,240
the idea we look at the context

443
00:27:14,740 --> 00:27:16,430
the last five characters

444
00:27:16,950 --> 00:27:17,670
the document

445
00:27:18,160 --> 00:27:20,230
to define the context of all the next character

446
00:27:21,150 --> 00:27:23,350
then we look at the entire file

447
00:27:24,100 --> 00:27:26,240
everything that we've seen so far we say

448
00:27:26,980 --> 00:27:30,330
have we seen the context before in the entire file

449
00:27:30,930 --> 00:27:34,120
so this is how we're going to then predict what happens next we go and

450
00:27:34,120 --> 00:27:37,130
find all those occurrences and here they are shown in blue

451
00:27:37,800 --> 00:27:41,240
so e space we are has happened one two three four

452
00:27:41,730 --> 00:27:44,300
five times already in jane austen's emma

453
00:27:44,900 --> 00:27:48,730
and this shows what happened next movie avi and a paean that's

454
00:27:50,540 --> 00:27:54,450
the first step would be to say let's use these six grams statistics the rules

455
00:27:54,450 --> 00:27:56,570
six expand statistics say alright this

456
00:27:57,000 --> 00:27:57,890
to stick

457
00:27:58,360 --> 00:28:05,110
to one how many with one two three four five seven five times so there's a two-fifths extensive getting yvinec

458
00:28:05,680 --> 00:28:11,230
the ones that can serve as o one fifty chance for paean want chance wellness

459
00:28:12,460 --> 00:28:17,440
it also haven't seen them much data so we better allow some probability for other things happening

460
00:28:18,110 --> 00:28:22,240
how do we allow for other things happening well the people i am approaches to say

461
00:28:22,730 --> 00:28:30,130
let's back of too short a length contexts as well and say what happened in context space hero

462
00:28:30,650 --> 00:28:34,530
we are are o o hand in any context at all

463
00:28:35,280 --> 00:28:37,260
and fold in those other

464
00:28:37,780 --> 00:28:42,850
one two three four five possibilities in addition to the six grams statistics

465
00:28:43,250 --> 00:28:45,560
and weight them together in a small smartish way

466
00:28:47,340 --> 00:28:53,940
hand that's how people and works and that gives you an extremely good compression state-of-the-art

467
00:28:55,670 --> 00:29:00,780
on most files it doesn't do as well as humans do human audiences like yourselves

468
00:29:00,780 --> 00:29:06,070
can predict english a lot better and clearly doesn't understand grammar induction reason and so

469
00:29:06,070 --> 00:29:09,610
forth but it does a lot of learning overall statistics and that's

470
00:29:10,250 --> 00:29:13,330
it gets you a long way towards good compression

471
00:29:14,510 --> 00:29:15,480
so there must be paean

472
00:29:25,530 --> 00:29:30,440
okay so the question was doesn't learn these six crime statistics in the document the

473
00:29:30,470 --> 00:29:34,070
entire document and then compress it what does that do it on the fly and

474
00:29:34,610 --> 00:29:39,860
important answers it's just like you guys that it be identical twin audiences

475
00:29:40,430 --> 00:29:44,490
it does things on the fly so when were asked to predict in this context

476
00:29:44,890 --> 00:29:49,370
we then say how often has e space we are all happened already before now

477
00:29:49,690 --> 00:29:53,410
we don't look ahead into the future so the predictions are based only on what

478
00:29:53,410 --> 00:29:54,480
we have already seen

479
00:29:56,460 --> 00:29:57,150
any other questions

480
00:29:59,630 --> 00:30:01,760
there's a couple things i want to mention about

481
00:30:02,290 --> 00:30:05,350
mean coding one is as i showed you last time we can

482
00:30:06,070 --> 00:30:10,700
make other users over many coding we can make an extremely efficient writing system

483
00:30:11,700 --> 00:30:13,300
and the one i showed you was called dasher

484
00:30:14,310 --> 00:30:15,950
that's based on the idea that

485
00:30:16,510 --> 00:30:21,700
writing involves making gestures of some sort maybe wiggling fingers or scribbling with a stick

486
00:30:22,380 --> 00:30:26,080
i am want to turn those gestures in detects as efficiently as possible

487
00:30:26,880 --> 00:30:30,700
hand compression is all about taking text and turn it into a bit string which

488
00:30:30,710 --> 00:30:33,480
in arithmetic coding is views that viewed is a real number

489
00:30:34,010 --> 00:30:36,930
and so we can make a very good writing system by saying okay let's have

490
00:30:37,030 --> 00:30:40,640
just just define a real number by steering our spaceship

491
00:30:41,000 --> 00:30:42,290
in a two-dimensional world

492
00:30:42,700 --> 00:30:46,030
and not will make us if we turn it on its head very efficient writing

493
00:30:46,030 --> 00:30:50,300
systems to make gesture and outcomes text so that's the dasher concept

494
00:30:51,070 --> 00:30:52,940
and i showed you events and it's free software

495
00:30:54,180 --> 00:30:56,660
help is always welcome to make such better

496
00:30:58,750 --> 00:31:01,600
and there's another application prior arithmetic coding i wanted to mention

497
00:31:02,010 --> 00:31:06,590
and this is the idea for efficiently generating random samples

498
00:31:07,410 --> 00:31:11,270
let me give you an example let's say i ask you to simulate

499
00:31:11,730 --> 00:31:12,230
they bent

500
00:31:12,800 --> 00:31:14,120
coin and i said okay

501
00:31:14,800 --> 00:31:16,840
let's fix pe eh

502
00:31:19,070 --> 00:31:21,160
in an airplane zero one

503
00:31:22,540 --> 00:31:23,760
and he is arrested

504
00:31:25,830 --> 00:31:26,910
please simulate

505
00:31:27,770 --> 00:31:29,900
ten million tosses coin

506
00:31:30,620 --> 00:31:34,350
and i will provide you with random bits a random number generator the question is

507
00:31:34,350 --> 00:31:37,550
how many bits we need to use to generate

508
00:31:41,030 --> 00:31:42,050
tosses of this coin

509
00:31:43,870 --> 00:31:46,370
let's say and from now on instead of ten million

510
00:31:47,970 --> 00:31:51,980
standard way of solving this problem is the size of well i use the random

511
00:31:51,980 --> 00:31:53,300
bits you can provide me with

512
00:31:54,090 --> 00:31:54,580
to make

513
00:31:55,770 --> 00:31:56,650
real numbers

514
00:31:57,170 --> 00:31:58,410
between zero and one

515
00:31:59,160 --> 00:32:02,330
so all generate things that i'll think of as real numbers

516
00:32:03,330 --> 00:32:05,500
uniformly distributed between zero and one

517
00:32:07,150 --> 00:32:09,780
but we got a lot of thirty two bits per

518
00:32:11,010 --> 00:32:11,700
real number

519
00:32:12,940 --> 00:32:13,750
if it's a standard

520
00:32:14,200 --> 00:32:18,780
random number generators are region thirty cubits interpret them as defining a number between zero and one

521
00:32:19,270 --> 00:32:22,940
then we look to see if u is less than one pe eh

522
00:32:23,640 --> 00:32:26,980
if it is then we spit out and they end otherwise it's better to be

523
00:32:28,090 --> 00:32:30,350
this method will cost to thirty two bits

524
00:32:30,900 --> 00:32:31,530
per character

525
00:32:34,780 --> 00:32:36,580
can we do better and do we care

526
00:32:37,420 --> 00:32:38,550
the answer is yes

527
00:32:39,180 --> 00:32:39,920
and maybe

528
00:32:40,710 --> 00:32:44,420
so yes we can do much better because we could take an hour many coder

529
00:32:45,810 --> 00:32:47,660
hand think about it this way less

530
00:32:48,310 --> 00:32:49,050
unlike the board

531
00:32:51,370 --> 00:32:52,790
imagine sticking a pin

532
00:32:53,560 --> 00:32:56,130
into the world of an arithmetic coder

533
00:32:57,320 --> 00:32:59,100
what happens if you stick a pin

534
00:32:59,900 --> 00:33:01,300
uniformly random

535
00:33:01,700 --> 00:33:04,560
into this line that goes from zero to one

536
00:33:10,370 --> 00:33:10,650
in two

537
00:33:11,160 --> 00:33:11,840
one of those lines

538
00:33:12,470 --> 00:33:14,920
what comes out what's the probability

539
00:33:18,210 --> 00:33:21,410
comes out a few sticking up in the news it is selected

540
00:33:22,100 --> 00:33:23,000
a string a text

541
00:33:28,910 --> 00:33:29,680
habitat your neighbour

542
00:33:29,680 --> 00:33:32,860
extra column y

543
00:33:32,900 --> 00:33:36,580
so i won't be talking about unsupervised learning rule is going to be giving you

544
00:33:36,580 --> 00:33:41,680
a lot of the classes about this next week so just for you to understand

545
00:33:41,680 --> 00:33:46,580
the difference and here showing you the with people call heat map of the input

546
00:33:46,580 --> 00:33:52,280
matrix so all the coefficients in the matrix on maps some color so the positive

547
00:33:52,280 --> 00:33:57,340
ones to some orange color in the negative ones to some blue blue-collar

548
00:33:57,360 --> 00:34:03,000
and the matrix b here is the same as a but the columns in lines

549
00:34:03,000 --> 00:34:07,420
have been rearranged so that columns that are similar are close together and lines that

550
00:34:07,420 --> 00:34:12,840
are similar are close together and you see appearing patches of

551
00:34:12,860 --> 00:34:17,620
coefficients of the same color and this indicates that there is some structuring data so

552
00:34:17,620 --> 00:34:22,780
this is what clustering that's for you know if you take the initial matrix and

553
00:34:22,780 --> 00:34:29,400
you're undermines the coefficients so you just prominent the lines in concept in any way

554
00:34:29,500 --> 00:34:34,380
and you try to perform again the same clustering operation then you obtain matrix d

555
00:34:34,380 --> 00:34:38,900
showing that we now the structure is gone and so this is one way you

556
00:34:38,900 --> 00:34:42,500
can test whether or not there is structuring data

557
00:34:42,520 --> 00:34:51,080
in this class only talking about supervised learning so i'll be talking about building functions

558
00:34:51,120 --> 00:34:57,340
f of x x being the inputs and that to allow us to make predictions

559
00:34:57,340 --> 00:34:59,280
about the target y

560
00:34:59,300 --> 00:35:03,260
so f of x should be as close as possible to the y that we

561
00:35:03,260 --> 00:35:05,340
want to predict

562
00:35:05,660 --> 00:35:10,480
the simplest way of building f of x is to make the dot product between

563
00:35:11,600 --> 00:35:16,960
which is the vector so it's it's going to be one line of your matrix

564
00:35:16,960 --> 00:35:18,120
for example

565
00:35:18,520 --> 00:35:24,700
and the weight vector w then eventually adding about so these uprisings equivalent of you

566
00:35:24,700 --> 00:35:30,840
know making a weighted sum of the input features weighted by some coefficients

567
00:35:30,860 --> 00:35:34,180
so for them to the input features at the age the the the way to

568
00:35:34,280 --> 00:35:39,620
the number of children et cetera all the information about your patients each one then

569
00:35:39,630 --> 00:35:44,120
is going to have to wait and the decision is going to be for from

570
00:35:44,120 --> 00:35:50,860
the according to some kind of the voting and the weights the the voting power

571
00:35:50,920 --> 00:35:53,400
of each input feature

572
00:35:53,460 --> 00:35:59,900
now these normals are very simple but they are powerful and then there is the

573
00:35:59,900 --> 00:36:05,300
family of algorithm which are yet more powerful that are not linear in the inputs

574
00:36:05,300 --> 00:36:11,450
like this one that are linear in the in the parameters so here the the

575
00:36:11,820 --> 00:36:17,040
that the decision function i'm showing first is linear both in the inputs and in

576
00:36:17,040 --> 00:36:21,820
the parameters you should retain only linearity in the parameters you obtain a family of

577
00:36:21,820 --> 00:36:28,300
functions which are called percent rules and now the the doctor is carried against not

578
00:36:28,300 --> 00:36:33,680
the original inputs but some transformed input in the future space which i call phi

579
00:36:33,680 --> 00:36:35,120
of x

580
00:36:35,140 --> 00:36:39,860
and if we developed this dot product you obtain this the weighted sum of the

581
00:36:41,020 --> 00:36:47,000
j of x which are so-called military features in the new transformed space to for

582
00:36:47,000 --> 00:36:54,360
example imagine that you are you figure that are instead of waiting in the age

583
00:36:54,360 --> 00:36:58,130
of the patient something more predictive is the product of the two then you would

584
00:36:58,130 --> 00:37:02,740
get another feature which is the product of age and weight and this is what

585
00:37:02,740 --> 00:37:06,180
i would call the phi j of of x

586
00:37:06,320 --> 00:37:12,660
of course you can have you know many much more complex operations that you can

587
00:37:12,660 --> 00:37:18,480
compute on features so if you have images you can compute new features by looking

588
00:37:18,480 --> 00:37:24,820
locally for example at pieces of the imaging and extracting say corner or pieces of

589
00:37:24,820 --> 00:37:30,480
lines and things like that so one lecture will be devoted to feature extraction how

590
00:37:30,480 --> 00:37:36,960
to transform original input into some newer presentation phi of x

591
00:37:37,700 --> 00:37:43,740
now in the same family of methods they are kernel methods in this case you

592
00:37:43,740 --> 00:37:50,060
see that replaced the weights w with all follows because now instead of awaiting the

593
00:37:50,840 --> 00:37:54,620
features that are the columns of the matrix and now weighing the lines of the

594
00:37:56,180 --> 00:38:01,720
and what we're doing is that are placing the phi functions but some special kinds

595
00:38:01,720 --> 00:38:04,620
of so-called basis functions

596
00:38:04,920 --> 00:38:08,360
and the difference is that we are

597
00:38:08,840 --> 00:38:15,640
using the the training examples and comparing the training examples with the new example

598
00:38:15,680 --> 00:38:17,160
that is under study

599
00:38:17,180 --> 00:38:23,480
and the k is a similarity measure between the training example

600
00:38:23,540 --> 00:38:26,200
and the new unknown example

601
00:38:26,220 --> 00:38:33,580
so if you look you know it and the perceptual decision function for vector x

602
00:38:33,580 --> 00:38:39,600
which is your input you're doing a weight weighted sum of the different futures doing

603
00:38:39,600 --> 00:38:46,520
also weighted sum but the future is our special kind the features are similarities with

604
00:38:46,520 --> 00:38:50,240
the original examples

605
00:38:53,220 --> 00:38:59,000
way back you know in the eighties there was a renewal of interest i mean

606
00:38:59,020 --> 00:39:06,360
artificial neural networks people have thought that by imitating the brain of the way you

607
00:39:06,360 --> 00:39:11,180
know people understand how the brain functions we could get better

608
00:39:11,190 --> 00:39:13,500
predictions of that

609
00:39:13,500 --> 00:39:23,680
the machines that make predictions and the basic unit processing unit of the brain is

610
00:39:23,680 --> 00:39:30,260
the neuron and has been modelled very coarsely by immaculately it's in nineteen forty three

611
00:39:30,340 --> 00:39:33,460
and that's still the model of neuron which is

612
00:39:33,580 --> 00:39:41,120
most used in machine learning even though you know people have made many refinements and

613
00:39:41,120 --> 00:39:42,950
this very coarse model

614
00:39:43,100 --> 00:39:45,980
is just simple

615
00:39:46,000 --> 00:39:48,180
the linear model

616
00:39:48,220 --> 00:39:55,620
really the only difference is that you have a so-called activation function at the output

617
00:39:55,650 --> 00:39:59,680
so the unit makes a weighted sum

618
00:39:59,690 --> 00:40:04,370
of its input and then this a weighted sum goes to what people call is

619
00:40:04,790 --> 00:40:11,590
squashing function or activation function so that the input is bounded between between two about

620
00:40:11,590 --> 00:40:14,450
the output sorry is bounded between two values

621
00:40:14,500 --> 00:40:18,260
and as a special case the activation function can be just to step function so

622
00:40:18,260 --> 00:40:21,900
making a decision between two values are low value in

623
00:40:22,220 --> 00:40:27,130
high value in which case the neuron is in no way be performing a classification

624
00:40:27,130 --> 00:40:33,350
task it's deciding whether you are in class zero or in class one

625
00:40:35,720 --> 00:40:37,920
all these

626
00:40:37,930 --> 00:40:42,500
linear methods end up making

627
00:40:42,500 --> 00:40:45,610
close to one is much smaller

628
00:40:45,630 --> 00:40:50,770
so that's the reason why we actually also try to keep to find a very

629
00:40:50,770 --> 00:40:53,580
efficient closed subgraph algorithm

630
00:40:53,600 --> 00:40:59,740
OK then finally we call this graph is called close-cropped it's very interesting i think

631
00:41:00,460 --> 00:41:05,610
actually should see is there are lots of frequent subgraph mining algorithm but there are

632
00:41:05,610 --> 00:41:06,940
not so many

633
00:41:06,960 --> 00:41:12,610
closed frequent subgraph mining algorithm but we think the close one would make more sense

634
00:41:12,610 --> 00:41:13,350
this way

635
00:41:13,360 --> 00:41:15,060
after he published

636
00:41:15,110 --> 00:41:20,870
the closed graph i actually try to find there some more researchers try to champion

637
00:41:20,900 --> 00:41:25,630
to compete with this close was and we haven't seen them many yet but

638
00:41:25,640 --> 00:41:30,620
two to the rear sense what we are really using the closed graph rather than

639
00:41:30,620 --> 00:41:36,140
she spam because the close graph in any large graph region much smaller set these

640
00:41:36,140 --> 00:41:41,500
are non redundant set the covers all the information so i think you know anybody

641
00:41:41,500 --> 00:41:42,280
want to

642
00:41:42,330 --> 00:41:47,770
to do more better graph mining have to think about this close graph method

643
00:41:47,780 --> 00:41:52,310
so the ten idea if you look at the close graph centuries start from here

644
00:41:52,370 --> 00:41:57,370
tried to use pattern girls to go but you try to check whether there's any

645
00:41:57,370 --> 00:42:03,270
way you can do early termination because you you generate you check any parts this

646
00:42:03,270 --> 00:42:07,630
part can be absorbed by the other parties you don't generally part OK

647
00:42:07,650 --> 00:42:11,660
so for the same data set if you see

648
00:42:11,680 --> 00:42:15,540
we actually is say

649
00:42:15,550 --> 00:42:20,930
the generated close graph is much smaller for the same NCI dataset

650
00:42:20,980 --> 00:42:22,380
the closed graph

651
00:42:22,890 --> 00:42:26,740
you see you can lower down the support of ten thirty

652
00:42:26,750 --> 00:42:29,250
the structure is fine it's much bigger

653
00:42:29,300 --> 00:42:33,370
and if you look at the number of patterns generated

654
00:42:33,530 --> 00:42:37,590
even this one generates all the frequent graph no matter what i was near you

655
00:42:37,590 --> 00:42:44,490
think this one generate the closed frequent graph because chloe by close-cropped essentially if you

656
00:42:44,490 --> 00:42:47,320
look at these are all the exponential scale

657
00:42:47,340 --> 00:42:50,400
so these about this reorder magnets smaller

658
00:42:50,480 --> 00:42:56,670
this for example using about the cameras like appearing yesterday we discussed if i give

659
00:42:56,670 --> 00:42:58,630
them five percent support

660
00:42:58,640 --> 00:43:00,310
if you generate one minute

661
00:43:00,320 --> 00:43:01,800
OK patterns

662
00:43:01,810 --> 00:43:03,710
and which in one

663
00:43:03,720 --> 00:43:08,260
about one to two thousand patterns can is definitely like to see this to southern

664
00:43:08,310 --> 00:43:13,410
rather women OK you don't have time to carry on this actually is most redundant

665
00:43:13,410 --> 00:43:18,160
this is you know is this distinct there's no information loss

666
00:43:18,180 --> 00:43:23,420
so if you compare with this was menagerie if you use closed

667
00:43:23,450 --> 00:43:27,900
graph was and it would be order magnitude faster as well

668
00:43:27,950 --> 00:43:32,920
so no matter even the space is smaller than the time is much shorter than

669
00:43:32,920 --> 00:43:33,700
this one

670
00:43:33,740 --> 00:43:39,880
i strongly recommend this close graph if you like to use are like to improve

671
00:43:39,880 --> 00:43:42,600
really just think about this close was

672
00:43:42,620 --> 00:43:47,340
are not give you very detailed error on this mining part if you like we

673
00:43:47,340 --> 00:43:49,790
definitely i love to discuss with anybody

674
00:43:49,810 --> 00:43:51,540
because of the time

675
00:43:51,550 --> 00:43:57,360
the first thing the interesting thing for this new one is in this year PAKDD

676
00:43:57,360 --> 00:44:03,370
conference we publish one car jeep wrong it's like cheese spent index we got this

677
00:44:03,580 --> 00:44:10,290
call different centuries pushing the con straints deeply into this pattern mining process

678
00:44:10,300 --> 00:44:16,880
just thinking about in real life when the chemist comic was computer scientist they want

679
00:44:16,880 --> 00:44:18,380
to mine frequent graph

680
00:44:18,390 --> 00:44:20,750
very often they give you concepts

681
00:44:20,770 --> 00:44:25,520
OK for example the missing all the diameter of the constraints should be

682
00:44:25,580 --> 00:44:30,750
least the greater and something too small i want to see get all of this

683
00:44:30,750 --> 00:44:35,480
is that the maximum degree should be greater than ten thousand chemists quite often they

684
00:44:35,480 --> 00:44:37,770
give you all these different constraints

685
00:44:37,780 --> 00:44:40,020
of course the one way is to say

686
00:44:40,040 --> 00:44:44,870
i mine all the frequent subgraphs based on some kind of the support the user

687
00:44:44,870 --> 00:44:46,070
accounts for cut

688
00:44:46,090 --> 00:44:49,240
OK but that's not efficient at all because

689
00:44:49,610 --> 00:44:53,290
probably you cut ninety percent and why you have to

690
00:44:53,370 --> 00:44:57,420
first find so huge set so the cost seems

691
00:44:58,300 --> 00:45:00,550
graph mining region ideas

692
00:45:00,570 --> 00:45:02,440
you give me the constraints

693
00:45:02,490 --> 00:45:07,740
before the mining i try every to push this constant down at every iteration i

694
00:45:07,740 --> 00:45:09,500
mean uses constant to cut

695
00:45:09,510 --> 00:45:10,970
OK so the

696
00:45:11,000 --> 00:45:16,370
the interesting thing is how can we take this constraint as effective knife to cut

697
00:45:16,370 --> 00:45:18,970
through the search space

698
00:45:19,030 --> 00:45:21,750
so if you think about search space cutting

699
00:45:21,770 --> 00:45:27,160
they are generally generate there are two philosophy what we call pattern pruning what we

700
00:45:27,160 --> 00:45:28,550
call data from

701
00:45:28,570 --> 00:45:32,510
so called pattern pruning means you generate this pattern right in the middle east you

702
00:45:32,510 --> 00:45:37,320
want to grow then you'll find this pattern actually does not satisfy constraints and you

703
00:45:37,320 --> 00:45:38,310
also know

704
00:45:38,360 --> 00:45:42,610
you for the grow this one there's no way you can say that they useful

705
00:45:42,630 --> 00:45:47,190
the whole park we get don't have to grow this state animal because anything grow

706
00:45:47,200 --> 00:45:49,120
out of this pattern is used

707
00:45:49,140 --> 00:45:55,550
and what is data profiling data probably means you have many many graphs OK why

708
00:45:55,550 --> 00:45:58,890
discussing keep growing you find the remaining

709
00:45:58,900 --> 00:46:02,220
the set of this graph has no way to us

710
00:46:02,220 --> 00:46:03,360
to satisfy

711
00:46:03,380 --> 00:46:04,510
your constant

712
00:46:04,510 --> 00:46:06,260
if you can sing for the growing

713
00:46:06,280 --> 00:46:09,660
this one is no it satisfies and throw away this graph set

714
00:46:09,720 --> 00:46:14,050
means if you have a very big so you can cut them you find become

715
00:46:14,050 --> 00:46:15,550
a very smaller set

716
00:46:15,570 --> 00:46:17,270
why do mining so

717
00:46:17,270 --> 00:46:20,730
you have two knives one is cutting the graph the data

718
00:46:20,730 --> 00:46:25,630
meeting point for governments which is seen by the detectors which is

719
00:46:25,640 --> 00:46:30,210
based on the same principle in fact detectors used in particle physics

720
00:46:30,240 --> 00:46:34,750
i was going to make the c word comes in so since there is accordingly

721
00:46:34,760 --> 00:46:40,750
the easy to to call in to construct an image so eighty five percent of

722
00:46:40,750 --> 00:46:44,570
all nuclear medicine exams may

723
00:46:45,440 --> 00:46:47,620
this way how this is going news

724
00:46:48,480 --> 00:46:55,350
at present this molybdenum ninety nine is produced in five plans fractals plants for thirty

725
00:46:55,350 --> 00:46:58,500
million examinations hundred thousand

726
00:46:59,940 --> 00:47:01,590
produced billion

727
00:47:01,610 --> 00:47:02,880
this plant

728
00:47:02,880 --> 00:47:10,150
by a two step process first the initial training the actors like mostly similarly the

729
00:47:10,170 --> 00:47:15,810
ninety nine team and thirty six six percent of the times and then disband

730
00:47:15,820 --> 00:47:22,230
if you have fast process in a very complicated way included in which is that

731
00:47:22,250 --> 00:47:26,310
now what i of this kind this because in the last few years there's been

732
00:47:26,310 --> 00:47:31,840
a big scar city of techniques many people could not get the exam to be

733
00:47:31,840 --> 00:47:36,500
sure that the cancer over the world because these factors

734
00:47:37,570 --> 00:47:40,540
in the netherlands had problems because getting old

735
00:47:40,570 --> 00:47:41,500
and now

736
00:47:41,520 --> 00:47:46,730
actually hopefully i think we come to rescue the situation i think this is very

737
00:47:46,730 --> 00:47:53,150
interesting started a long time ago with neutron capture in practice now accelerators after so

738
00:47:53,150 --> 00:47:58,310
many years of development come up to rescue disk all possible solution of a very

739
00:47:58,310 --> 00:48:03,800
serious problem there are two proposal main but to propose one mostly one made in

740
00:48:03,800 --> 00:48:09,640
canada by triumph which is blah blah blah blah blah and and this company

741
00:48:09,950 --> 00:48:14,460
it is a superconducting linac which we could use electrons

742
00:48:14,490 --> 00:48:16,130
five megawatts

743
00:48:16,140 --> 00:48:21,430
but using a lot of photos and photo fissioning uranium no money doing it

744
00:48:21,440 --> 00:48:27,460
similar to what happens in iraq and other possibilities been proposed we advanced accelerator applications

745
00:48:27,460 --> 00:48:29,640
which is the second spin-off company

746
00:48:29,690 --> 00:48:33,730
which could cover not only ten percent about ten percent of the market in which

747
00:48:33,730 --> 00:48:41,050
is a bit of a superconducting linac falls photos producing one million one thousand people

748
00:48:41,050 --> 00:48:44,580
just one megawatt and produced by capt

749
00:48:44,600 --> 00:48:47,790
now i don't want to go into detail i wanted just to make the point

750
00:48:47,820 --> 00:48:52,350
that this is the main which is being back the main and sitting in lot

751
00:48:52,350 --> 00:48:54,090
of lives full

752
00:48:54,100 --> 00:49:01,520
tell us about maybe will be rescued and i think will be by some of

753
00:49:01,530 --> 00:49:09,390
i part of particle accelerators which are used for producing isotopes

754
00:49:09,410 --> 00:49:14,470
this of cyclotrons this what i was thinking before for doing this on the next

755
00:49:14,470 --> 00:49:20,900
this of the best this the best solution for producing molybdenum but for producing other

756
00:49:20,900 --> 00:49:26,060
isotopes the hospitals and centers use cycle times

757
00:49:26,070 --> 00:49:27,820
there are two types of that

758
00:49:27,830 --> 00:49:29,400
baby cyclotrons

759
00:49:29,420 --> 00:49:31,290
medium energy cyclotrons

760
00:49:31,300 --> 00:49:32,940
i energy cyclotrons

761
00:49:32,950 --> 00:49:34,700
below eighteen

762
00:49:34,710 --> 00:49:39,290
we look forward to be between eighteen and forty by forty three

763
00:49:39,320 --> 00:49:47,810
this now facility using ospital to produce short lived positron emitters which meet and elect

764
00:49:47,900 --> 00:49:53,870
the blasters which useful what i will tell you moment positron emission tomography the the

765
00:49:53,870 --> 00:49:56,310
most important one is for line eighteen

766
00:49:56,390 --> 00:50:03,400
then there are many of you decide to centralized facility for producing majority of isotopes

767
00:50:03,400 --> 00:50:09,010
which need high energy i would tell you something about i hired a but this

768
00:50:10,030 --> 00:50:16,430
either use either for diagnostic or also for family therapy of the past is of

769
00:50:16,430 --> 00:50:24,120
diffuse tumours and finally high energy cyclotrons which centralized which produced the kind of light

770
00:50:24,120 --> 00:50:29,190
isotopes and say something about this one as in two hundred eleven so c

771
00:50:29,210 --> 00:50:34,190
cyclotrons used mainly for diagnostic but also for

772
00:50:35,460 --> 00:50:38,670
this baby cyclotrons by general electric

773
00:50:38,690 --> 00:50:41,850
and i have been replication to axillary

774
00:50:41,860 --> 00:50:43,200
h minuses

775
00:50:43,210 --> 00:50:48,940
a hydrogen with an electron x-ray which is that of

776
00:50:48,940 --> 00:50:51,260
for extracting the gold

777
00:50:51,280 --> 00:50:55,550
and this exist out there is one which is only one kilometre far from here

778
00:50:55,770 --> 00:50:56,950
is one in geneva

779
00:50:56,970 --> 00:51:01,260
in the town of osprey that's all at the cyclotron to produce what

780
00:51:01,310 --> 00:51:03,550
to produce line eighteen

781
00:51:03,560 --> 00:51:08,610
by this reaction oxygen eighteen important news

782
00:51:09,800 --> 00:51:10,970
this is

783
00:51:10,970 --> 00:51:13,620
first of all

784
00:51:13,740 --> 00:51:20,730
like to thank isabelle flowing for organizing is nice workshop and for inviting me so

785
00:51:20,730 --> 00:51:22,730
my talk is

786
00:51:22,770 --> 00:51:26,690
is about a method to uncover the

787
00:51:26,710 --> 00:51:36,130
because of the structure and applying a particular class of econometric models i will present

788
00:51:36,350 --> 00:51:43,550
in a we use graphical models has to learn to uncover his because structure in

789
00:51:43,770 --> 00:51:51,850
in recent development actually we'll go beyond the graphical models despite a little bit to

790
00:51:51,880 --> 00:51:55,990
the title of the presentation so

791
00:51:56,820 --> 00:52:00,370
the classifica nematic models i'm talking about

792
00:52:00,490 --> 00:52:05,140
vector autoregressive models which already

793
00:52:05,160 --> 00:52:06,850
being mentioned in

794
00:52:06,870 --> 00:52:13,200
some of the past in the previous representations of the the space station and when

795
00:52:14,390 --> 00:52:21,750
particular class and what identified we talk about the structure of the vector autoregressive so

796
00:52:23,300 --> 00:52:24,950
oops never view of

797
00:52:25,030 --> 00:52:29,660
one of my talk is the following i'm going to define quite quickly the

798
00:52:30,130 --> 00:52:36,740
and as mothers of vector autoregressive structural vector autoregression models i'm going to talk about

799
00:52:36,740 --> 00:52:41,910
these causal search methods graphical models in order to identify

800
00:52:41,920 --> 00:52:48,050
the structure underlying the unobserved structure like this and then

801
00:52:48,050 --> 00:52:55,730
i will present an empirical application which this setting is is linear and gaussian and

802
00:52:55,730 --> 00:53:03,690
then i will present some briefly some extensions one in the nonparametric setting and one

803
00:53:03,750 --> 00:53:06,800
in the non gaussian case where

804
00:53:06,810 --> 00:53:13,810
we can apply to based on independent component analysis so basically

805
00:53:13,830 --> 00:53:16,720
the model is the following y

806
00:53:16,740 --> 00:53:24,660
he is a vector of variables usually in much use in macroeconomics such

807
00:53:24,680 --> 00:53:30,750
the kind of model than what is the vector of c three four five six

808
00:53:30,750 --> 00:53:32,650
seven was

809
00:53:32,660 --> 00:53:36,720
usually not more than that

810
00:53:36,730 --> 00:53:43,150
the eighties metastasis of coefficients

811
00:53:44,310 --> 00:53:48,730
we see out rest because each of these variables is the latest on

812
00:53:48,750 --> 00:53:53,660
it's it's past that is not is that the UT

813
00:53:53,720 --> 00:53:56,150
is the vector white noise and so on

814
00:53:58,150 --> 00:54:00,230
there is no correlation across time

815
00:54:00,500 --> 00:54:03,650
and the mattress of is

816
00:54:03,680 --> 00:54:07,280
innovation terms is generally

817
00:54:07,310 --> 00:54:13,090
number you got in the sense because of course in equation one we don't model

818
00:54:13,160 --> 00:54:22,150
the contemporaneous interactions among the variables so the contemporaneous interaction will be captured by his

819
00:54:22,900 --> 00:54:26,000
body by the covariance of

820
00:54:26,030 --> 00:54:31,630
the innovation model is why we don't model of the contemporaneous interaction well for in

821
00:54:31,630 --> 00:54:34,960
order to avoid the problems of the same type

822
00:54:38,320 --> 00:54:45,820
economist much interested especially when we talk about microeconomics to study the effects of the

823
00:54:46,710 --> 00:54:48,220
on the on the

824
00:54:48,250 --> 00:54:55,160
relevant variables so in case of stationarity we have it is is the composition in

825
00:54:55,160 --> 00:55:03,980
which these his majesty's feet a function of the original metastasis a and b for

826
00:55:03,980 --> 00:55:09,750
you if you you well they actually shot of the economy in ft

827
00:55:09,780 --> 00:55:15,570
we would have the effect of unit shock on the violence so we could

828
00:55:15,590 --> 00:55:17,910
major these affect cost

829
00:55:17,940 --> 00:55:21,620
but we see that

830
00:55:21,630 --> 00:55:30,620
for any nonsingular metastasis metrics the we get an equivalent formulation an equivalent equivalent rotation

831
00:55:30,890 --> 00:55:36,170
of the shock and and a new series of

832
00:55:36,180 --> 00:55:37,120
mark this is

833
00:55:38,010 --> 00:55:40,320
and so on

834
00:55:41,140 --> 00:55:42,350
we we

835
00:55:42,360 --> 00:55:45,780
the problem here and ification is to find the

836
00:55:45,800 --> 00:55:53,910
actually rotation in which she we get the actual shocks that affect the relevant economic

837
00:55:54,030 --> 00:56:00,660
so we have problems which is similar to get many scientific disciplines to identify

838
00:56:00,670 --> 00:56:03,140
it is it is so

839
00:56:03,150 --> 00:56:10,320
of the variability of the observed variability winter so it is not

840
00:56:10,330 --> 00:56:15,760
not observe the heat and the

841
00:56:16,620 --> 00:56:20,140
we see that if if we if we

842
00:56:20,160 --> 00:56:22,280
have the right to forget

843
00:56:22,290 --> 00:56:23,530
the right p

844
00:56:23,530 --> 00:56:29,660
the light peter to produce the identification of the actual sharks we can

845
00:56:29,660 --> 00:56:31,400
we multiply

846
00:56:31,410 --> 00:56:36,290
model this way and we get to the structure of the animal model which is

847
00:56:36,340 --> 00:56:43,610
not observed in which you have in this metrics gamma not the interactions among the

848
00:56:43,610 --> 00:56:46,470
contemporary news reports

849
00:56:47,300 --> 00:56:52,140
the idea is to divide this model is to find

850
00:56:52,180 --> 00:56:55,370
and a then

851
00:56:55,400 --> 00:56:57,820
in the matrix p government not

852
00:56:57,840 --> 00:57:02,700
which is based on information about the contemporaneous causal structure

853
00:57:02,720 --> 00:57:05,740
in the in the standard literature

854
00:57:05,890 --> 00:57:12,390
these these information is sometimes impose up really

855
00:57:13,220 --> 00:57:16,470
sometimes misunderstand arbitrarily

856
00:57:16,480 --> 00:57:19,470
by imposing at composition of

857
00:57:19,510 --> 00:57:25,290
the matrix governments much of the of the covariance matrix of the innovations terms the

858
00:57:25,290 --> 00:57:26,330
idea here

859
00:57:27,700 --> 00:57:30,430
starting from the estimated residuals

860
00:57:30,450 --> 00:57:34,920
UTC the so-called reduced form for

861
00:57:36,450 --> 00:57:44,650
studying the conditional independence relations among users doing infer the causal relationships among the contemporaneous

862
00:57:44,660 --> 00:57:47,460
it was you know the to impose

863
00:57:47,470 --> 00:57:51,020
these these metrics b

864
00:57:51,030 --> 00:57:55,290
in order to identify them on in extension of the we presented in the last

865
00:57:55,290 --> 00:57:58,980
part of my presentation i will show that this can also in case of non

866
00:57:58,980 --> 00:58:00,090
gaussian eighty

867
00:58:00,100 --> 00:58:05,010
so by an independent component analysis

868
00:58:05,020 --> 00:58:10,910
so now i'm going to to talk about the graphical model it's two two interface

869
00:58:10,910 --> 00:58:15,160
to address these this problem of identifying because action

870
00:58:15,160 --> 00:58:20,790
i think i will go quite fast here because assume much of the audience is

871
00:58:20,790 --> 00:58:22,740
familiar with these terms

872
00:58:22,760 --> 00:58:26,320
graphs have two functions the

873
00:58:26,340 --> 00:58:28,090
the representation

874
00:58:28,110 --> 00:58:34,910
of course the structure it's quite obvious political that anger is directed acyclic graphs

875
00:58:34,930 --> 00:58:39,020
so on the directed edges considered and the

876
00:58:39,030 --> 00:58:47,570
to represent conditional independence relations under some conditions that you hear of graphical causal inference

877
00:58:47,570 --> 00:58:52,000
and today we saw one approach for similar solutions but in fact last time all

878
00:58:52,000 --> 00:58:55,010
we saw a couple of different approaches to resolve

879
00:58:56,920 --> 00:59:01,360
whose values we don't actually know sort of step back and generalize of the couple

880
00:59:01,460 --> 00:59:04,870
for example the three techniques that we saw

881
00:59:06,120 --> 00:59:08,210
the different examples

882
00:59:08,240 --> 00:59:13,050
so the general problem here on the specifics are you know how to identify the

883
00:59:13,050 --> 00:59:16,700
how you can find out where these undefined symbols are defined or in the next

884
00:59:17,660 --> 00:59:21,820
how you can take a big part name and identify which blocks contain which disk

885
00:59:21,820 --> 00:59:25,990
blocks contain the files contain the data for the file that was named in the

886
00:59:27,130 --> 00:59:31,980
but the general problem is you have a set of names

887
00:59:32,040 --> 00:59:35,480
and associated with these names of with each name is

888
00:59:40,410 --> 00:59:45,920
and these names get associated with are mapped

889
00:59:45,920 --> 00:59:50,920
the to the different values in fact the names gabon to the different values and

890
00:59:50,920 --> 00:59:54,650
in this example the link it taken in like the definition of assembling needed to

891
00:59:54,650 --> 00:59:59,380
identify what the value associated with a simple value here in this context is where

892
00:59:59,500 --> 01:00:03,170
is this name the square root or program the square root function where is it

893
01:00:03,170 --> 01:00:05,690
actually find what locations

894
01:00:08,380 --> 01:00:10,030
the way in which that

895
01:00:10,050 --> 01:00:13,710
the solution is going to be done is done in general is using something called

896
01:00:13,710 --> 01:00:16,530
in a mapping of

897
01:00:16,530 --> 01:00:27,030
the mapping of million

898
01:00:27,050 --> 01:00:31,840
on the value being done by name at mapping algorithm takes into account

899
01:00:31,900 --> 01:00:36,210
all it takes is in takes as input something called the context

900
01:00:36,230 --> 01:00:38,750
described this with an example of a minute

901
01:00:38,800 --> 01:00:42,820
so named gets is born to value somebody's born into value and then when you're

902
01:00:42,820 --> 01:00:46,880
link or when you're on you know part of unix filesystem and trying to identify

903
01:00:46,880 --> 01:00:50,670
the value associated with the name of you don't have to all that name find

904
01:00:50,670 --> 01:00:54,150
value and that resolution is going to be done on

905
01:00:54,210 --> 01:00:57,590
in a particular context so for example you might have two files with the same

906
01:00:57,590 --> 01:01:00,280
name to different directories and that's fine

907
01:01:00,300 --> 01:01:03,440
as long as the same name could have two different values

908
01:01:03,460 --> 01:01:06,440
because that resolution from the name to the correct value

909
01:01:06,510 --> 01:01:09,770
in that case in the case and i know number will be done in the

910
01:01:09,770 --> 01:01:13,360
context of the decree in which position is being done

911
01:01:13,380 --> 01:01:18,880
so you know what we've seen over the past couple of days just last lecture

912
01:01:19,270 --> 01:01:22,730
three different ways of doing it and it turns out that in almost every system

913
01:01:23,010 --> 01:01:26,820
or every system that i know of any way of this basically three ways of

914
01:01:26,820 --> 01:01:29,530
doing this in the solution

915
01:01:29,630 --> 01:01:35,730
the first wave the simplest way is the table lookup

916
01:01:35,780 --> 01:01:38,610
within the context of about o five

917
01:01:38,710 --> 01:01:42,550
knowing with simple and which has set of defined symbols

918
01:01:42,570 --> 01:01:46,030
taking one of the symbols the name in that case as input and finding out

919
01:01:46,030 --> 01:01:50,820
where it's been defined in that five is basically a table lookup that's what that's

920
01:01:50,820 --> 01:01:53,960
impossible table section of this graph

921
01:01:54,000 --> 01:01:57,630
from the disk example from last time on the i know table is an example

922
01:01:57,630 --> 01:02:01,650
of a table lookup and this of course of this that has in it or

923
01:02:01,650 --> 01:02:02,830
the mapping between on

924
01:02:03,270 --> 01:02:06,650
you know i know numbers and the corresponding i know that's just the table so

925
01:02:06,650 --> 01:02:08,780
when you want to go from unknown number two nine

926
01:02:08,800 --> 01:02:10,190
he took a book

927
01:02:10,210 --> 01:02:13,710
and that's the simplest baseform these case of how

928
01:02:13,730 --> 01:02:15,030
the name resolution

929
01:02:15,030 --> 01:02:24,750
the second way of doing on name resolution is something called that name resolution

930
01:02:24,800 --> 01:02:27,670
we can see an example of this today but we saw an example of that

931
01:02:27,670 --> 01:02:32,400
name the solution the last you take a big unix pattern slash home slash fools

932
01:02:32,400 --> 01:02:36,400
last part of what we did was start left to right along that path and

933
01:02:36,400 --> 01:02:40,050
that down so are of resolution of the file

934
01:02:40,050 --> 01:02:45,770
you know to get to the block that he wanted by going through a search

935
01:02:46,210 --> 01:02:49,280
through through that same that's going to happen

936
01:02:49,360 --> 01:02:52,110
names the item

937
01:02:52,170 --> 01:02:57,650
the third way of doing on symbolism name resolution is what we saw today and

938
01:02:57,650 --> 01:02:58,880
that's an example

939
01:02:58,920 --> 01:03:04,360
of searching through contexts

940
01:03:04,380 --> 01:03:09,460
there's really no that just tell you here is set about o five and years

941
01:03:09,460 --> 01:03:11,440
that of libraries and

942
01:03:11,440 --> 01:03:16,130
the symbols are undefined or defined different modules of a program effort to in different

943
01:03:16,130 --> 01:03:20,880
parts of program defined somewhere among these modules and not really going to tell the

944
01:03:20,880 --> 01:03:24,380
link what's being defined where it's up to the link to figure it out and

945
01:03:24,380 --> 01:03:27,610
it does that by basically running a search through a variety of different products so

946
01:03:27,900 --> 01:03:32,800
each dot file and each library is a new context in which the search for

947
01:03:32,800 --> 01:03:37,420
previously undefined symbol happens

948
01:03:38,320 --> 01:03:43,380
in this particular case the search within each context takes the form of table lookup

949
01:03:44,420 --> 01:03:45,500
so those are the two

950
01:03:45,510 --> 01:03:49,130
of those of the three techniques for how you do name resolution in general and

951
01:03:49,130 --> 01:03:52,360
we start to of them today and we start two of them on the first

952
01:03:52,860 --> 01:03:59,400
the last time we talked about the unix file system

953
01:04:08,880 --> 01:04:10,670
so the last step of

954
01:04:10,690 --> 01:04:15,320
in the process of what i think are does offer solution relocation

955
01:04:15,360 --> 01:04:16,630
three locations

956
01:04:16,730 --> 01:04:19,860
and in this case a particular kind of linking of i did use the term

957
01:04:19,860 --> 01:04:23,420
but this form of linking is called static linking because we're going to take all

958
01:04:23,420 --> 01:04:27,250
of these different object files and defined on the command line and the library and

959
01:04:27,250 --> 01:04:31,150
built together single a single big binary that has been linked so

960
01:04:31,170 --> 01:04:38,150
once probably because converts constant linking internally location that context is pretty straightforward but also

961
01:04:38,150 --> 01:04:43,880
the talk more about that except to note that on this relocation tables maintain each

962
01:04:43,940 --> 01:04:48,630
for the third step is a little bit more on a slightly more complicated activities

963
01:04:48,630 --> 01:04:50,500
a lot of

964
01:04:50,650 --> 01:04:53,150
depending on the system

965
01:04:53,190 --> 01:04:55,860
and that's program loading

966
01:04:55,920 --> 01:04:59,090
and the

967
01:04:59,110 --> 01:05:04,440
problem solved by loading is that the link produces an output program that's executable and

968
01:05:04,460 --> 01:05:07,770
you can run the program in unix you on it by typing something on the

969
01:05:07,770 --> 01:05:08,570
command line

970
01:05:08,710 --> 01:05:13,090
or you know and you go click on something effectively that invokes causes shell to

971
01:05:13,090 --> 01:05:15,690
execute a program

972
01:05:15,750 --> 01:05:18,590
so somebody has to do the work of

973
01:05:18,630 --> 01:05:22,400
when you type something in common and somebody has to do the work of looking

974
01:05:22,400 --> 01:05:23,610
at what file

975
01:05:23,610 --> 01:05:27,070
o has been types taking the contents of the file

976
01:05:27,130 --> 01:05:28,940
loading it up into memory

977
01:05:28,960 --> 01:05:33,510
passing control to something that can then start running the program and typically the place

978
01:05:33,510 --> 01:05:38,650
where the control as fast is the interpreter correspond to the programme

979
01:05:38,670 --> 01:05:42,010
all of this work is done in unix by program called exact

980
01:05:43,000 --> 01:05:48,130
it's actually an order in unix is a program colleagues at the it's job once

981
01:05:48,130 --> 01:05:49,820
you type the command line is

982
01:05:49,880 --> 01:05:53,280
shall invokes it what it does is to do what i said which is look

983
01:05:53,300 --> 01:05:57,550
at the file name take the contents of a lot of memory and pass control

984
01:05:57,550 --> 01:05:59,380
to the first on

985
01:05:59,400 --> 01:06:03,960
basically the first line of the program it's usually a line of modern object files

986
01:06:04,110 --> 01:06:09,360
more complicated actually having something that says with interpretive programs so you pass it lost

987
01:06:09,360 --> 01:06:10,670
control two

988
01:06:10,690 --> 01:06:15,230
that interpreter which in turn goes through by steps and then invokes the first line

989
01:06:16,210 --> 01:06:20,050
that's what the c program the see of lauder

990
01:06:20,050 --> 01:06:22,620
OK so i know all know mental but this is really

991
01:06:22,640 --> 01:06:23,870
OK now

992
01:06:27,160 --> 01:06:29,410
some definitions

993
01:06:29,410 --> 01:06:34,390
and need to give you this section some definitions because i've been studied around using

994
01:06:34,390 --> 01:06:35,680
some words

995
01:06:38,850 --> 01:06:40,870
number one

996
01:06:41,330 --> 01:06:44,140
the word gene

997
01:06:44,200 --> 01:06:48,200
gene is one of these factors inheritance controlling interest

998
01:06:48,240 --> 01:07:05,870
mendel didn't use the word genes or gene came along much later

999
01:07:05,870 --> 01:07:08,660
various flavors of gene big are little more

1000
01:07:08,660 --> 01:07:11,410
are known as old fields

1001
01:07:11,430 --> 01:07:13,220
from the greek word meaning of

1002
01:07:13,280 --> 01:07:16,580
these are the alternative forms of the gene

1003
01:07:16,640 --> 01:07:24,120
it can come in the form big are little or

1004
01:07:24,180 --> 01:07:28,870
my great big a little a micro cluster normal and m from mutant

1005
01:07:28,890 --> 01:07:33,100
there are a lot of different notations geneticists use for that

1006
01:07:33,280 --> 01:07:35,850
the word phenotype

1007
01:07:35,950 --> 01:07:39,220
it means appearance

1008
01:07:39,240 --> 01:07:47,510
the plant was well the piece around that phenotype

1009
01:07:47,550 --> 01:07:50,760
the individual seven foot seven inches tall

1010
01:07:50,800 --> 01:07:52,530
that's the phenotype

1011
01:07:52,550 --> 01:07:57,390
OK those are phenotype genotype

1012
01:07:57,600 --> 01:08:01,140
it means the pair

1013
01:08:01,200 --> 01:08:03,530
of all only yields

1014
01:08:05,100 --> 01:08:06,800
by the individual

1015
01:08:08,070 --> 01:08:16,160
there are little are is the genocide they are big gaza genotype that are little

1016
01:08:16,850 --> 01:08:20,510
our genotypes important difference between genotypes

1017
01:08:20,600 --> 01:08:21,930
and phenotype

1018
01:08:22,080 --> 01:08:25,800
other important words so we can actually talk to each other

1019
01:08:28,180 --> 01:08:30,760
four homers i go

1020
01:08:30,780 --> 01:08:32,390
homer zygote

1021
01:08:32,410 --> 01:08:34,720
is an individual who has

1022
01:08:34,870 --> 01:08:40,530
genotype that has to have the same wheels

1023
01:08:40,530 --> 01:08:48,330
two copies of the same allele individual said to be homozygous and alternatively

1024
01:08:48,330 --> 01:08:50,830
an individual is said to be

1025
01:08:50,870 --> 01:08:54,990
heterozygous heterozygosity

1026
01:08:55,810 --> 01:09:00,600
if they have two alternatives

1027
01:09:00,640 --> 01:09:08,310
a couple of other important definitions

1028
01:09:13,970 --> 01:09:26,640
phenotype round is said to be dominant over phenotype wrinkled if

1029
01:09:28,530 --> 01:09:31,410
if the header zygote shows that phoenix

1030
01:09:31,430 --> 01:09:35,030
the heterozygous between pure breeding strains so

1031
01:09:35,100 --> 01:09:37,990
phenotype one

1032
01:09:38,010 --> 01:09:39,200
if you know one

1033
01:09:39,200 --> 01:09:40,950
is dominant

1034
01:09:41,070 --> 01:09:44,830
over phenotype so

1035
01:09:48,200 --> 01:09:51,050
f one of pure breeding strains

1036
01:09:58,350 --> 01:10:00,870
shows phenotype one

1037
01:10:00,890 --> 01:10:07,950
similarly we have the word recess

1038
01:10:07,970 --> 01:10:12,450
now mentioned and you will then proceed to probably forget because all my colleagues forget

1039
01:10:12,490 --> 01:10:15,390
dominant and recessive do not refer to williams

1040
01:10:15,410 --> 01:10:19,280
big are is not dominant

1041
01:10:19,280 --> 01:10:21,990
round is dominant

1042
01:10:22,010 --> 01:10:25,140
the gaza we'll

1043
01:10:25,160 --> 01:10:26,700
now you say who cares

1044
01:10:26,700 --> 01:10:30,140
the textbooks get this wrong all the time it's true union find textbooks use this

1045
01:10:30,140 --> 01:10:32,010
cricket built a big garston

1046
01:10:32,140 --> 01:10:36,050
what it turned out the big are controlled three different rates

1047
01:10:36,160 --> 01:10:37,990
maybe wellness

1048
01:10:38,030 --> 01:10:40,530
the ability to grow with low salt in the soil

1049
01:10:40,550 --> 01:10:43,850
and ability to bloom in may

1050
01:10:43,910 --> 01:10:47,970
some of the traits might be recessive some of them might be down

1051
01:10:47,990 --> 01:10:53,200
we know examples of that with the same we'll can control multiple traits some of

1052
01:10:53,200 --> 01:10:58,740
which show dominance some of which are set this so real hard carrying geneticists

1053
01:10:58,780 --> 01:11:03,080
try hard to use the word dominant and recessive to refer to phenotypes

1054
01:11:03,120 --> 01:11:04,890
not too only fuels

1055
01:11:04,910 --> 01:11:06,390
four genotypes

1056
01:11:06,390 --> 01:11:10,100
now since eighty percent of the faculty the biology department to use the word with

1057
01:11:10,100 --> 01:11:11,550
that degree of precision

1058
01:11:11,550 --> 01:11:14,680
i'm not have high hopes that you will either but i'm trying to say

1059
01:11:14,720 --> 01:11:19,220
the words dominant recessive refer to phenotypes because the geneticist kind and we all have

1060
01:11:19,220 --> 01:11:20,580
our but this is

1061
01:11:20,620 --> 01:11:25,120
one of mine is that these really do refer to phenotypes quite important because otherwise

1062
01:11:25,120 --> 01:11:27,970
you can get quite bollocks stop and i'll come to the case for sickle cell

1063
01:11:27,970 --> 01:11:33,070
anaemia where you were you will be able to describe the sickle-cell anaemia

1064
01:11:33,080 --> 01:11:36,890
as recessive dominant record on

1065
01:11:36,890 --> 01:11:44,100
good decision definitions they're worth knowing if we get those definitions right the rest of

1066
01:11:44,100 --> 01:11:50,010
it's pretty easy are

1067
01:11:50,030 --> 01:11:59,280
so many publishers this paper nineteen sixty five it's accepted appears in nature but the

1068
01:11:59,280 --> 01:12:05,850
proceedings of the royal academy of bruno and it's published and what happens

1069
01:12:05,910 --> 01:12:10,970
number sinks like a stone medals papers totally ignored nobody really pays any attention to

1070
01:12:10,970 --> 01:12:12,470
it this paper

1071
01:12:12,470 --> 01:12:17,010
mister what we might call of this digital communication

1072
01:12:17,030 --> 01:12:26,490
because the analog gets represented in binary digits because we've chosen

1073
01:12:27,880 --> 01:12:31,420
essentially because of having studied shannon at some point

1074
01:12:31,450 --> 01:12:36,380
the thoroughly analog sources in the binary bitstream

1075
01:12:36,400 --> 01:12:41,220
namely when you talk about the digital communication what you're really doing is saying i

1076
01:12:41,220 --> 01:12:44,240
have decided there will be a digital interface

1077
01:12:44,240 --> 01:12:46,760
between source and channel

1078
01:12:46,780 --> 01:12:51,670
OK that's what digital communication is that's what most communication today years

1079
01:12:51,690 --> 01:12:54,050
there's very little communication that i

1080
01:12:54,070 --> 01:12:56,820
the starts out with an analog waveform

1081
01:12:56,840 --> 01:13:03,670
find a way to transmit transmitted without first going into a binary sequence

1082
01:13:03,690 --> 01:13:05,800
OK so here's the picture

1083
01:13:05,820 --> 01:13:08,010
one of the noises

1084
01:13:08,130 --> 01:13:10,030
we're going to study

1085
01:13:10,190 --> 01:13:12,690
we have an input

1086
01:13:12,800 --> 01:13:15,880
which is now a waveform

1087
01:13:15,930 --> 01:13:16,780
we have

1088
01:13:18,110 --> 01:13:19,970
which is the form

1089
01:13:20,050 --> 01:13:23,400
we have an output which is the way for

1090
01:13:23,430 --> 01:13:28,700
this input here is going to be created somehow

1091
01:13:28,720 --> 01:13:31,050
in the process of modulation

1092
01:13:31,150 --> 01:13:36,070
from this binary stream this coming into chapter to the channel encoder

1093
01:13:36,110 --> 01:13:39,320
to some our we're taking a binary stream

1094
01:13:39,340 --> 01:13:40,630
turning it into

1095
01:13:42,380 --> 01:13:51,510
now i think a little bit about what might be essential in that process

1096
01:13:51,530 --> 01:13:55,590
OK if you know something about practical

1097
01:13:55,630 --> 01:13:58,430
engineering systems for communications

1098
01:13:58,630 --> 01:14:00,030
you know that there's

1099
01:14:00,050 --> 01:14:03,380
an organisation in the US called the FCC

1100
01:14:03,400 --> 01:14:05,990
and every other country has its

1101
01:14:06,010 --> 01:14:09,010
has its companion set of initials

1102
01:14:09,070 --> 01:14:10,880
which says

1103
01:14:10,970 --> 01:14:14,970
what part of the radio spectrum you can use and what part of the radio

1104
01:14:14,970 --> 01:14:17,190
spectrum you can use

1105
01:14:17,420 --> 01:14:21,380
and you suddenly realise that somehow it's going to be important

1106
01:14:21,420 --> 01:14:26,240
to turn these binary streams into waveforms which are

1107
01:14:26,280 --> 01:14:27,900
more or less

1108
01:14:27,920 --> 01:14:32,200
compressed into some frequency bands that's one of the problems we're going to have to

1109
01:14:32,200 --> 01:14:34,110
worry about

1110
01:14:34,130 --> 01:14:36,650
but at some level

1111
01:14:36,670 --> 01:14:38,220
if i take

1112
01:14:38,240 --> 01:14:41,920
but take a whole bunch of different binary sequences

1113
01:14:41,930 --> 01:14:45,320
one thing i could do for example

1114
01:14:45,320 --> 01:14:48,510
i could take the binary sequence of length one hundred

1115
01:14:48,800 --> 01:14:49,800
how many

1116
01:14:49,840 --> 01:14:52,530
how many such binary sequences author

1117
01:14:52,530 --> 01:14:56,280
has lead to one hundred the first bit can be one or zero second they

1118
01:14:56,280 --> 01:15:00,820
can be one or zero for combinations third but can be one or zero also

1119
01:15:00,820 --> 01:15:06,810
that makes a combination of three bits there two the one hundredth combinations of a

1120
01:15:06,810 --> 01:15:09,700
hundred binary digits

1121
01:15:09,720 --> 01:15:12,150
so theoreticians says

1122
01:15:12,150 --> 01:15:14,090
that those two to the one hundredth

1123
01:15:14,170 --> 01:15:16,880
different combinations of binary digits

1124
01:15:16,920 --> 01:15:23,170
in two evenly spaced numbers between zero and one

1125
01:15:23,880 --> 01:15:29,550
then i will take those evenly spaced numbers between zero and one modulates

1126
01:15:29,550 --> 01:15:32,900
some way form whatever form i happen to choose

1127
01:15:32,920 --> 01:15:35,490
and i was send that way for

1128
01:15:35,510 --> 01:15:37,260
in the absence of noise

1129
01:15:37,260 --> 01:15:38,900
i pick up their waveform

1130
01:15:38,920 --> 01:15:42,430
turn it back into my binary sequence again

1131
01:15:42,450 --> 01:15:48,150
what's the conclusion from this

1132
01:15:48,150 --> 01:15:50,670
the conclusion is if you don't have noise

1133
01:15:50,670 --> 01:15:54,470
there's no constraint on how much data you can send

1134
01:15:54,490 --> 01:15:58,030
hansen is much data is i want to know

1135
01:15:58,050 --> 01:16:03,820
and there's there's nothing to stop me from doing it if i don't know

1136
01:16:10,490 --> 01:16:15,880
somehow i have to keep these things separate i don't necessarily have to

1137
01:16:15,920 --> 01:16:20,570
have to separate one binary digit the next binary digit in time

1138
01:16:20,610 --> 01:16:22,840
i could in fact do something like

1139
01:16:23,930 --> 01:16:27,650
two to one hundred different waveforms

1140
01:16:27,970 --> 01:16:30,400
or i could do with anything

1141
01:16:30,420 --> 01:16:31,720
in between

1142
01:16:31,780 --> 01:16:36,860
i could take each each sequence of eight bits turn them into one of two

1143
01:16:36,860 --> 01:16:43,360
hundred fifty six waveforms transmit one waveform then a little bit later transmit another way

1144
01:16:43,380 --> 01:16:47,950
form and so forth so i can split up the pion anyway i want to

1145
01:16:47,990 --> 01:16:51,240
i will talk about that a lot is weaker one right split them up into

1146
01:16:51,240 --> 01:16:56,760
different waveforms which i send that space times i somehow have have to worry about

1147
01:16:56,760 --> 01:17:00,820
the fact that if i send them over a finite bandwidth

1148
01:17:00,900 --> 01:17:03,280
there's no way in hell

1149
01:17:03,280 --> 01:17:07,070
but you can take a finite bandwidth waveforms

1150
01:17:07,090 --> 01:17:10,610
and send them in finite time

1151
01:17:10,630 --> 01:17:14,930
anything which is which last for a finite amount of time

1152
01:17:15,030 --> 01:17:17,720
spreads out in bandwidth

1153
01:17:17,740 --> 01:17:22,340
anything which is a finite amount of bandwidth spreads out in time that's what you

1154
01:17:22,340 --> 01:17:24,400
want to from six w three

1155
01:17:24,400 --> 01:17:28,130
so so we're dealing with on soluble problem here

1156
01:17:28,150 --> 01:17:31,920
one night was back in nineteen twenty eight solved that problem

1157
01:17:31,920 --> 01:17:35,740
we say well you can make the waveform separate but you can make the samples

1158
01:17:37,900 --> 01:17:42,030
and people earlier i figured out they could do that with the sampling theorem but

1159
01:17:42,030 --> 01:17:44,860
the sampling theorem wasn't very practical

1160
01:17:44,880 --> 01:17:49,530
but not was way of doing it was practical so in fact you can separate

1161
01:17:49,550 --> 01:17:54,550
these waveforms but you still have the problem is how do you deal with noise

1162
01:17:54,780 --> 01:17:59,050
OK one of the favorite ways of dealing with noise

1163
01:17:59,070 --> 01:18:03,400
is to assume that something called white calcium noise

1164
01:18:03,610 --> 01:18:07,110
what is weight cassino noise white calcium noise

1165
01:18:07,150 --> 01:18:10,700
isn't noise which no matter where you look

1166
01:18:10,760 --> 01:18:16,090
it sitting there you can't get away from you move around to different frequencies noise

1167
01:18:16,090 --> 01:18:20,880
is still there you move around to different times it's still there it is somehow

1168
01:18:20,880 --> 01:18:24,920
uniform throughout time and throughout frequency

1169
01:18:25,970 --> 01:18:28,260
kind of an awkward thing because

1170
01:18:28,260 --> 01:18:30,070
if i

1171
01:18:30,090 --> 01:18:32,900
if i developed a receiver

1172
01:18:32,990 --> 01:18:36,150
which don't have any band with constraining on it

1173
01:18:36,150 --> 01:18:41,450
and i looked at this noise coming in which was spread out over all frequencies

1174
01:18:41,450 --> 01:18:43,700
in terms of collecting data

1175
01:18:43,810 --> 01:18:47,510
i can probably find a good number of people in this room and around the

1176
01:18:47,510 --> 01:18:50,300
world who are willing to let me the robot for a couple hours of data

1177
01:18:51,380 --> 01:18:54,190
i'd be hard-pressed to find medical doctors

1178
01:18:54,200 --> 01:18:57,590
we're going to let me to plan a reinforcement learning algorithm

1179
01:18:57,680 --> 01:19:00,110
on this type of system

1180
01:19:00,160 --> 01:19:03,540
the good thing is i don't need people quite yet and we have good partners

1181
01:19:03,630 --> 01:19:08,020
have that would allow where they do in the future work so the slices of

1182
01:19:08,020 --> 01:19:11,310
rat brain in addition they are letting us do a little bit of that

1183
01:19:11,330 --> 01:19:13,660
but again a lot of the same

1184
01:19:13,700 --> 01:19:19,960
difficulties arise in this kind of context which we see in the robotics context

1185
01:19:19,970 --> 01:19:23,560
so this is just the idea to motivate that there is a rich set of

1186
01:19:23,560 --> 01:19:25,600
real world problems out there

1187
01:19:25,660 --> 01:19:30,120
and if we want to be able to tackle these using reinforcement learning type of

1188
01:19:30,120 --> 01:19:33,190
approach we need to solve a few problems

1189
01:19:33,280 --> 01:19:36,820
in big one as far as i'm concerned is really this problem how do we

1190
01:19:36,820 --> 01:19:38,130
learn models

1191
01:19:38,150 --> 01:19:42,040
in a good way how do we do that in terms of being data efficient

1192
01:19:42,060 --> 01:19:45,420
and how do we do that in terms of having some robust models at the

1193
01:19:46,590 --> 01:19:48,240
so this is really

1194
01:19:48,260 --> 01:19:54,020
what's been driving my research since i left CMU getting a sense of how what

1195
01:19:54,020 --> 01:19:59,800
are the right models and algorithms to handle this kind of problems

1196
01:19:59,810 --> 01:20:03,160
one thing hasn't changed since i left CMU and this is the fact that i'm

1197
01:20:03,160 --> 01:20:06,780
really basing a lot of this work in the palm DP framework

1198
01:20:06,930 --> 01:20:11,910
i see most of you are relatively familiar with this work we told me his

1199
01:20:11,910 --> 01:20:15,250
teaching i think it's a machine learning class i think if you

1200
01:20:15,260 --> 01:20:20,000
places where people have picked up the back out the basic definition is quite standard

1201
01:20:20,000 --> 01:20:25,610
for any type of decision making planning task set of states that describe the state

1202
01:20:25,610 --> 01:20:27,340
of the system

1203
01:20:27,360 --> 01:20:32,210
is a set of action this is the decision making practices into passive system can

1204
01:20:32,210 --> 01:20:35,110
take action which causes some change

1205
01:20:35,120 --> 01:20:40,010
in the system and in the case of pompey that changes described using a probabilistic

1206
01:20:40,860 --> 01:20:45,280
and then there's also notion of reward our goal or something that

1207
01:20:45,320 --> 01:20:50,440
gives some indication of whether the right actions to choose in which state

1208
01:20:50,460 --> 01:20:52,250
and the pundit set up

1209
01:20:52,270 --> 01:20:53,380
there's a few

1210
01:20:53,390 --> 01:20:54,900
complicating factors

1211
01:20:54,920 --> 01:21:00,670
the biggest one being the fact that the state of the system isn't directly observable

1212
01:21:00,690 --> 01:21:05,360
so instead there's instead there's a set of observation which captures the part of the

1213
01:21:05,360 --> 01:21:10,910
state that is in fact observable in the case of our epilepsy

1214
01:21:10,930 --> 01:21:16,590
deep brain stimulation system the observations that we get these readings from the electrodes so

1215
01:21:16,590 --> 01:21:20,940
e g type readings which somehow capture what's going on in the brain what we'd

1216
01:21:20,950 --> 01:21:24,970
really like to know is which neurons are firing when and how we can get

1217
01:21:24,970 --> 01:21:29,490
that level of information so we have instead this reading

1218
01:21:29,500 --> 01:21:35,330
this observation probabilities that describes how the observation i made as a function of what's

1219
01:21:35,330 --> 01:21:37,070
going on in the system

1220
01:21:37,120 --> 01:21:41,470
and the last little bit that we have is is a distribution over state

1221
01:21:41,490 --> 01:21:45,750
and this is what captures really a state of information about the system

1222
01:21:45,770 --> 01:21:49,730
we don't know the state of the underlying system but we have a probability distribution

1223
01:21:49,730 --> 01:21:51,240
over the state

1224
01:21:51,290 --> 01:21:55,330
that tells us something about what's going on in the system

1225
01:21:55,810 --> 01:21:56,830
there's too

1226
01:21:56,840 --> 01:22:02,670
interesting things to do when you have a dynamic system models as the azzopardi p

1227
01:22:02,690 --> 01:22:07,430
the first one is tracking this belief so as you learn more about the system

1228
01:22:07,430 --> 01:22:11,730
as you observe the EEG signal you need to make some inference about what's going

1229
01:22:11,730 --> 01:22:15,640
on in the brain how does the probability of state changes

1230
01:22:15,650 --> 01:22:19,910
so you can do that with simple bayes rule bayesian filter in this case

1231
01:22:19,930 --> 01:22:22,080
the other interesting thing to do

1232
01:22:22,100 --> 01:22:23,640
is figuring out

1233
01:22:23,670 --> 01:22:28,930
what's the expected return of applying certain actions on the system

1234
01:22:28,980 --> 01:22:30,580
so this is a simple

1235
01:22:30,620 --> 01:22:34,730
version bellman equation for the palm case

1236
01:22:34,740 --> 01:22:38,880
this value function tells you if i in a certain beliefs if i think there's

1237
01:22:38,880 --> 01:22:41,100
a certain distribution over my state

1238
01:22:41,110 --> 01:22:44,200
what is the expected long-term return

1239
01:22:44,210 --> 01:22:46,130
of applying different actions

1240
01:22:46,140 --> 01:22:49,570
so if i take the best action i can possibly apply

1241
01:22:49,590 --> 01:22:54,140
get some immediate reward maybe it'll stop the seizure may be able to deliver electrical

1242
01:22:55,400 --> 01:22:58,100
born in iran so there's some immediate reward

1243
01:22:58,110 --> 01:22:59,800
and then there's the effect

1244
01:22:59,810 --> 01:23:03,640
of what my action has which takes me to some other part of my belief

1245
01:23:03,640 --> 01:23:05,880
state and then i can acquire more return

1246
01:23:05,950 --> 01:23:10,000
so it's standard dynamic programming kind of equation

1247
01:23:10,370 --> 01:23:15,210
in terms of the model of these things are really well defined they've been defined

1248
01:23:15,210 --> 01:23:17,270
for the the last sixty years

1249
01:23:17,290 --> 01:23:21,510
forty or fifty years or so what's more interesting is how do we do these

1250
01:23:21,530 --> 01:23:23,660
two steps the belief monitoring

1251
01:23:23,710 --> 01:23:26,070
and the value function

1252
01:23:26,090 --> 01:23:28,010
in real time

1253
01:23:28,030 --> 01:23:32,450
in cases where your model is not really clear and by model i really mean

1254
01:23:32,450 --> 01:23:37,710
the transition probabilities and the observation probabilities if you don't know what these are

1255
01:23:37,820 --> 01:23:40,140
a little bit in trouble here

1256
01:23:40,180 --> 01:23:45,450
and in cases where the state space is very large or observation space ultimately is

1257
01:23:45,460 --> 01:23:49,680
really large also

1258
01:23:49,700 --> 01:23:52,520
so i sort of set up a grand agenda is probably going to keep me

1259
01:23:52,520 --> 01:23:54,770
busy for a number of years

1260
01:23:54,790 --> 01:23:57,810
i'll tell you which part of this problem i'm tackling right now

1261
01:23:57,830 --> 01:24:03,200
i will assume we have a problem which can reasonably well be modeled using upon

1262
01:24:03,200 --> 01:24:05,300
the framework

1263
01:24:05,310 --> 01:24:09,810
and we assume we have the ability to sample trajectories somehow get some data from

1264
01:24:09,810 --> 01:24:11,410
the domain

1265
01:24:11,420 --> 01:24:12,510
and so

1266
01:24:12,560 --> 01:24:17,320
for the purposes today's talk i want to talk about two separate result

1267
01:24:17,340 --> 01:24:23,540
in this for the first one is assumed that the trajectories have labeled state information

1268
01:24:23,550 --> 01:24:27,010
someone's gone and annotated the state of the system

1269
01:24:27,060 --> 01:24:28,590
but you can't control it

1270
01:24:28,610 --> 01:24:30,380
the choice of action

1271
01:24:30,400 --> 01:24:32,550
we don't often work in this

1272
01:24:32,550 --> 01:24:35,240
like for example if you want

1273
01:24:35,290 --> 01:24:39,850
what if we are going to use this model several times we have about the

1274
01:24:39,850 --> 01:24:43,550
basis of you may want to perform several years

1275
01:24:44,810 --> 01:24:47,770
probably not going to deal with other bees in the first place

1276
01:24:47,810 --> 01:24:50,480
to be able to answer only a single query

1277
01:24:50,600 --> 01:24:53,720
so usually we have two lecture

1278
01:24:53,770 --> 01:24:58,020
so the number of queries

1279
01:24:58,060 --> 01:25:01,830
so you better way yes there is

1280
01:25:01,900 --> 01:25:05,170
there's something called belief propagation

1281
01:25:05,290 --> 01:25:09,890
are there other names for this plant propagation some problems

1282
01:25:14,220 --> 01:25:19,050
the same thing just different names for the same type of

1283
01:25:19,140 --> 01:25:22,080
essentially what belief propagation thus

1284
01:25:23,130 --> 01:25:29,160
it in the elimination algorithm with a specific schedule for that you avoid computing

1285
01:25:29,410 --> 01:25:30,820
we computing

1286
01:25:30,830 --> 01:25:33,010
the same quantities

1287
01:25:33,020 --> 01:25:35,430
because if have two different ways

1288
01:25:35,550 --> 01:25:37,270
and you don't

1289
01:25:38,850 --> 01:25:42,910
called computations that you've made on previous queries

1290
01:25:42,970 --> 01:25:45,010
after sometimes we compute

1291
01:25:47,740 --> 01:25:52,350
that you have to prove so how do we compute the partition function in that

1292
01:25:52,350 --> 01:25:53,790
year and that

1293
01:25:53,800 --> 01:25:55,680
query time just

1294
01:25:57,080 --> 01:25:58,500
competitions and

1295
01:25:58,620 --> 01:26:03,420
we bring them together to answer your queries so that's what propagation vol

1296
01:26:03,560 --> 01:26:08,230
in the experiment

1297
01:26:10,020 --> 01:26:12,680
the markley a

1298
01:26:12,740 --> 01:26:18,430
in fact it's quite simple

1299
01:26:19,930 --> 01:26:21,220
the idea is

1300
01:26:21,230 --> 01:26:24,380
maybe i should show something different first

1301
01:26:24,390 --> 01:26:27,000
just to illustrate

1302
01:26:32,510 --> 01:26:37,110
well here's the intuition i'm going to write piece for the change in the simplest

1303
01:26:37,110 --> 01:26:40,370
case was generalized by thirty three

1304
01:26:43,480 --> 01:26:44,860
in the world

1305
01:26:45,020 --> 01:26:51,290
my goal is to compute p of x

1306
01:26:51,300 --> 01:26:53,640
your of x

1307
01:26:53,720 --> 01:26:55,170
but x ten

1308
01:26:55,270 --> 01:26:57,810
can be anything and can be from one two

1309
01:27:06,020 --> 01:27:07,350
how compute

1310
01:27:07,360 --> 01:27:12,140
the marginal probability of x as well we need to sum over all the other

1311
01:27:12,140 --> 01:27:14,990
viable the joint distribution correct

1312
01:27:16,850 --> 01:27:21,230
we need to sum over the axes that are smaller than

1313
01:27:21,240 --> 01:27:25,730
and over the axes of the large and

1314
01:27:25,770 --> 01:27:31,740
the joint distribution which is this one

1315
01:27:32,980 --> 01:27:36,450
clique is just not right

1316
01:27:36,570 --> 01:27:38,110
because there is no

1317
01:27:38,150 --> 01:27:41,470
because i feel five

1318
01:27:41,480 --> 01:27:45,580
so these are my clicks on the click simple only two i

1319
01:27:45,600 --> 01:27:48,120
so the problem fixed

1320
01:27:48,170 --> 01:27:53,890
just separated this protein two parts one part of the model another part of the

1321
01:27:56,390 --> 01:27:59,420
and then i can perform this information separately here

1322
01:27:59,420 --> 01:28:03,700
in this summation separately

1323
01:28:03,750 --> 01:28:04,970
right because of the system

1324
01:28:06,730 --> 01:28:12,840
because these is a constant with regard to all the viable large and

1325
01:28:12,860 --> 01:28:19,220
these are also the got all the violence there is more than

1326
01:28:19,360 --> 01:28:23,520
just use the distributive law all these

1327
01:28:23,530 --> 01:28:29,480
a separate this into this problem

1328
01:28:31,990 --> 01:28:33,590
because this is a course

1329
01:28:33,700 --> 01:28:37,360
with respect to one set of summation what's happening again

1330
01:28:42,570 --> 01:28:44,350
a b

1331
01:28:56,410 --> 01:28:57,670
it's true

1332
01:28:57,770 --> 01:28:59,200
that's true

1333
01:28:59,220 --> 01:29:01,790
that's true

1334
01:29:07,720 --> 01:29:10,420
essentially can write is left

1335
01:29:10,430 --> 01:29:14,240
the expression this way to can write this

1336
01:29:14,250 --> 01:29:17,190
right expression is note

1337
01:29:18,890 --> 01:29:22,780
he is only a function of x and

1338
01:29:22,780 --> 01:29:28,850
i don't know exactly what these blocks but they are common spatial patterns which i

1339
01:29:28,850 --> 01:29:31,620
haven't introduced so far

1340
01:29:31,650 --> 01:29:33,870
ARR coefficients

1341
01:29:33,890 --> 01:29:34,910
and maps

1342
01:29:38,410 --> 01:29:43,780
these are all features that i expect with some statistical techniques a-ok efficiencies clear what

1343
01:29:43,780 --> 01:29:47,960
this is so it so if i look at the dynamics of the time series

1344
01:29:48,280 --> 01:29:54,980
i can model this by approximating just the linear coefficients of in a our model

1345
01:29:55,110 --> 01:29:56,700
and and then just

1346
01:29:56,710 --> 01:30:01,160
using these linear coffee coefficients and them plotting

1347
01:30:01,160 --> 01:30:03,900
and if you look at the covariance structure

1348
01:30:03,990 --> 01:30:06,290
then it's clear

1349
01:30:06,700 --> 01:30:09,970
that has the block structure

1350
01:30:10,030 --> 01:30:12,580
so you see that that

1351
01:30:12,620 --> 01:30:13,840
there some

1352
01:30:13,850 --> 01:30:20,290
some covariance of course within the same type of feature that we have to extracting

1353
01:30:20,290 --> 01:30:23,230
but it is actually true that there is almost no

1354
01:30:23,610 --> 01:30:27,680
cross talk so to say between the feature

1355
01:30:27,700 --> 01:30:29,330
which means that we can

1356
01:30:29,340 --> 01:30:31,010
do something like that

1357
01:30:32,960 --> 01:30:33,780
if i

1358
01:30:33,790 --> 01:30:36,510
we we would

1359
01:30:36,530 --> 01:30:43,470
user classifier on every single feature say and then these classifiers make different decisions then

1360
01:30:43,470 --> 01:30:44,600
we can

1361
01:30:44,610 --> 01:30:45,280
you know

1362
01:30:45,300 --> 01:30:46,740
king something

1363
01:30:47,850 --> 01:30:49,180
but again there

1364
01:30:49,260 --> 01:30:51,910
lots of different ways of combining

1365
01:30:54,070 --> 01:30:55,850
sensor fusion

1366
01:30:57,070 --> 01:30:58,970
talk about this

1367
01:31:01,850 --> 01:31:03,980
now this is something

1368
01:31:03,990 --> 01:31:11,830
just some theoretic slide you know what we can actually gained by independent features

1369
01:31:13,830 --> 01:31:17,890
let's let's let's say

1370
01:31:17,940 --> 01:31:24,250
so if if the classificati outputs are gaussians and we have an independent features

1371
01:31:24,330 --> 01:31:29,830
and indeed we have the the same classification error

1372
01:31:30,590 --> 01:31:37,490
for some binary classification of path for each then we can actually prove that

1373
01:31:38,950 --> 01:31:42,440
of all the classification error can decrease

1374
01:31:43,510 --> 01:31:44,480
so if we

1375
01:31:44,490 --> 01:31:49,550
and this is a very very somewhat stupid assumption we are assuming that all features

1376
01:31:49,950 --> 01:31:54,010
i sorta to see equally valuable are or invaluable

1377
01:31:54,020 --> 01:31:59,140
all features have are all of of c so to say

1378
01:31:59,150 --> 01:32:04,070
but if we combine them and their independent then what can we gain

1379
01:32:04,100 --> 01:32:05,760
OK so

1380
01:32:05,770 --> 01:32:10,100
he is he's different curves so if we

1381
01:32:10,110 --> 01:32:13,290
so if we have one feature to six features

1382
01:32:14,350 --> 01:32:19,090
if the single feature has five percent classification error ten and so on

1383
01:32:19,590 --> 01:32:22,140
so we see that this this

1384
01:32:22,150 --> 01:32:28,000
the below quote because or any classic n the number of features grows

1385
01:32:28,020 --> 01:32:29,680
in this direction

1386
01:32:29,720 --> 01:32:32,970
so so clearly

1387
01:32:32,980 --> 01:32:37,050
so even if we have an higher and higher on the single feature we can

1388
01:32:37,090 --> 01:32:41,470
actually improve the overall classification rate strongly

1389
01:32:41,510 --> 01:32:44,440
assuming full independence

1390
01:32:44,480 --> 01:32:49,220
in this very simple model so so we can we can actually hope to gain

1391
01:32:49,220 --> 01:32:50,900
the lot

1392
01:32:50,960 --> 01:32:53,880
in practice this

1393
01:32:53,890 --> 01:32:58,180
gauss in distribution assumption might not be the case in practice it might not be

1394
01:32:58,180 --> 01:33:04,690
the right assumption that to assume that all these these features are given equal classification

1395
01:33:04,690 --> 01:33:10,010
rates this is certainly not true someone want discriminant others are not

1396
01:33:10,230 --> 01:33:12,450
but it just gives you

1397
01:33:12,490 --> 01:33:13,650
an idea

1398
01:33:13,680 --> 01:33:15,350
why this could be

1399
01:33:15,480 --> 01:33:17,590
a very good idea

1400
01:33:18,580 --> 01:33:22,380
this is some small print

1401
01:33:22,390 --> 01:33:24,750
so so how would you

1402
01:33:24,770 --> 01:33:25,720
how would you

1403
01:33:27,840 --> 01:33:33,460
i mean the most obvious way i think it's just concatenate and just below the

1404
01:33:33,470 --> 01:33:36,310
large classifiers e

1405
01:33:36,330 --> 01:33:38,290
so that's the first one

1406
01:33:39,870 --> 01:33:42,790
the second

1407
01:33:42,810 --> 01:33:43,950
a third one

1408
01:33:44,850 --> 01:33:46,560
so you would

1409
01:33:47,460 --> 01:33:49,370
a single classifier

1410
01:33:50,450 --> 01:33:53,090
on each feature vector

1411
01:33:53,130 --> 01:33:55,960
with LDA

1412
01:33:57,300 --> 01:33:59,810
and then you would like to optimally

1413
01:33:59,820 --> 01:34:06,720
combine them in the way and if you're assume that everything is calcium essentially and

1414
01:34:06,740 --> 01:34:08,330
you assume that all the

1415
01:34:08,380 --> 01:34:15,100
covariances of all the classifiers are equal

1416
01:34:16,470 --> 01:34:24,390
it's it's a theoretical question completely theoretical question assume that you have different classifiers that

1417
01:34:24,400 --> 01:34:25,090
that you

1418
01:34:25,140 --> 01:34:26,810
train with LDA

1419
01:34:26,820 --> 01:34:32,740
so that's you have different covariance structure for every classifier

1420
01:34:34,190 --> 01:34:38,710
if you have to have a full covariance for all the classifiers and these folk

1421
01:34:38,720 --> 01:34:43,650
variants which differ between the classifiers it would be very complex model you have to

1422
01:34:43,650 --> 01:34:44,950
to estimate

1423
01:34:44,960 --> 01:34:46,310
a lot of

1424
01:34:48,240 --> 01:34:52,110
but if you assume that the covariances i actually very

1425
01:34:52,120 --> 01:34:54,210
similar or the same as we do

1426
01:34:54,220 --> 01:34:57,760
then you can actually give a closed form solution

1427
01:34:57,790 --> 01:35:02,220
to what is the optimum type of of classifier

1428
01:35:02,240 --> 01:35:05,880
and so it assuming that you have this covariance structure

1429
01:35:05,930 --> 01:35:07,590
then you can

1430
01:35:08,800 --> 01:35:13,270
build a classifier which is

1431
01:35:13,600 --> 01:35:21,460
taking the probabilistic assumptions that we have made of every single classifier into account

1432
01:35:22,920 --> 01:35:29,210
of course it this is not correct because the all the classifiers words have slightly

1433
01:35:29,210 --> 01:35:32,670
different covariance structures

1434
01:35:32,720 --> 01:35:37,170
and maybe it's not appropriate at all to train them all by LDA

1435
01:35:37,220 --> 01:35:43,420
but this is just an approximation would see how far we can get that

1436
01:35:43,890 --> 01:35:50,090
so that the meter strategy is training

1437
01:35:50,100 --> 01:35:51,310
for each

1438
01:35:51,320 --> 01:35:53,220
individual feature

1439
01:35:53,220 --> 01:35:55,390
training a classifier

1440
01:35:55,400 --> 01:36:00,090
getting this to classify output and using those

1441
01:36:00,140 --> 01:36:05,730
to go together with the labels for the second layer of of of classical classification

1442
01:36:05,730 --> 01:36:09,520
so weak keep those classifiers fixed

1443
01:36:09,570 --> 01:36:12,960
so to say in the first layer for every individual feature

1444
01:36:12,970 --> 01:36:14,970
set and then

1445
01:36:14,980 --> 01:36:19,640
combine the train the complete combination with regularisation

1446
01:36:38,440 --> 01:36:44,570
this as i said about thousand and one ways of combining things so these these

1447
01:36:44,570 --> 01:36:46,870
to be able to actually learn them into them from data

1448
01:36:47,650 --> 01:36:52,340
and finally and i think this was sort of underappreciated early

1449
01:36:52,540 --> 01:36:56,120
in a lot of this research is that the algorithm's going to talk about today are

1450
01:36:56,130 --> 01:37:01,680
actually based on a pretty small subset of operations that turn out to scale incredibly well

1451
01:37:02,930 --> 01:37:06,050
so there's sort of long-running phenomenon in machine learning

1452
01:37:06,410 --> 01:37:11,080
where you want to get really great results and go to a conference and published and so

1453
01:37:11,100 --> 01:37:15,890
forth a way to do that just get a lot more data with a lot bigger computer

1454
01:37:16,010 --> 01:37:17,510
you can almost always better

1455
01:37:17,870 --> 01:37:19,850
and with a lot of algorithms going sort of

1456
01:37:20,130 --> 01:37:22,750
build them by hand we have all these rules coded in

1457
01:37:22,920 --> 01:37:26,960
it's really hard to scale up and take advantage of all these things because you have so much

1458
01:37:26,990 --> 01:37:28,400
knowledge wired in

1459
01:37:28,580 --> 01:37:29,970
and what you're more clever

1460
01:37:31,030 --> 01:37:34,470
and you can get increasingly clever it's harder and harder to get better results

1461
01:37:34,670 --> 01:37:36,340
but if we have a learning algorithm

1462
01:37:36,450 --> 01:37:39,060
that scales really well with hardware and with more data

1463
01:37:39,370 --> 01:37:43,340
then it becomes easier to get results as we get better hardware we get

1464
01:37:43,530 --> 01:37:44,550
larger datasets

1465
01:37:44,720 --> 01:37:47,810
so one of those things it's really nice about deep learning

1466
01:37:47,990 --> 01:37:49,830
is that whenever we get a faster computer

1467
01:37:50,080 --> 01:37:51,890
or we get a larger dataset

1468
01:37:52,040 --> 01:37:53,330
we can train a bigger model

1469
01:37:53,660 --> 01:37:55,610
so hopefully you'll see by the end of this

1470
01:37:55,780 --> 01:38:00,040
that most of these things are very simple dense linear algebra operations and they run really

1471
01:38:00,060 --> 01:38:02,230
fast on a computer and it energy you

1472
01:38:05,870 --> 01:38:09,380
so just to sort of also answer another question upfront

1473
01:38:10,270 --> 01:38:12,320
one of the questions as have been here before

1474
01:38:12,490 --> 01:38:14,170
so a lot of deep learning

1475
01:38:14,370 --> 01:38:17,140
spawns from neural networks research

1476
01:38:17,540 --> 01:38:21,430
which some of you may be familiar with if you took like an introductory course

1477
01:38:21,970 --> 01:38:26,700
and a lot that was sort of having its heyday like nineteen eighties and so on

1478
01:38:26,870 --> 01:38:28,560
and so it's worth are

1479
01:38:28,790 --> 01:38:29,770
trying to understand

1480
01:38:29,880 --> 01:38:32,280
as anything really changed since then

1481
01:38:32,720 --> 01:38:37,680
and so the answer to the question have we been here before there are two possible answers yes i know

1482
01:38:39,460 --> 01:38:40,920
and the

1483
01:38:41,300 --> 01:38:45,100
on the yes side we say that the basic ideas here

1484
01:38:45,330 --> 01:38:46,690
are actually pretty common

1485
01:38:47,020 --> 01:38:49,650
to past machine learning algorithms neural networks

1486
01:38:49,850 --> 01:38:53,450
so for example if you want to do supervised learning with labeled data

1487
01:38:53,590 --> 01:38:57,340
it turns out very straightforward procedure i'm walking through that a second

1488
01:38:58,130 --> 01:38:59,550
but also

1489
01:38:59,810 --> 01:39:04,920
sort of standard machine learning development strategies if you take a machine learning course to learn about

1490
01:39:05,040 --> 01:39:06,960
how to debug machine learning algorithms

1491
01:39:07,110 --> 01:39:08,800
all that stuff you can carry-over

1492
01:39:09,090 --> 01:39:12,250
and a lot of knowledge from problem domains like computer vision

1493
01:39:12,460 --> 01:39:13,800
are different applications

1494
01:39:14,020 --> 01:39:15,350
has also been carried over

1495
01:39:15,460 --> 01:39:18,800
so this stuff that we we sort of know that's old hat and so in this

1496
01:39:18,810 --> 01:39:20,560
sense things have changed so much

1497
01:39:20,840 --> 01:39:22,520
but there are few things

1498
01:39:22,680 --> 01:39:24,820
that have changed that are pretty critical

1499
01:39:24,940 --> 01:39:28,210
so for example we have much faster computers and a lot more data

1500
01:39:29,490 --> 01:39:33,830
used to be the case that if you train neural network with a very small amount of data

1501
01:39:34,010 --> 01:39:37,700
in order to get really good performance you had to wire a lot of your own knowledge into

1502
01:39:38,170 --> 01:39:41,760
but now we have huge datasets really big computers to process them

1503
01:39:41,880 --> 01:39:47,120
we can actually get away without that prior knowledge and what we want is really flexible algorithm that

1504
01:39:47,140 --> 01:39:49,540
can just gobble up all data and make decisions

1505
01:39:49,770 --> 01:39:50,920
so that sense

1506
01:39:51,070 --> 01:39:55,450
sort of operating regime where we want to to work on applications has changed

1507
01:39:55,570 --> 01:39:56,860
so that's very new

1508
01:39:57,240 --> 01:39:59,410
we are better optimization

1509
01:39:59,640 --> 01:40:02,710
we are better at figuring out how to initialize the neural networks

1510
01:40:02,910 --> 01:40:05,640
and all these different models we want to train so there's a lot of

1511
01:40:05,860 --> 01:40:08,220
useful research there and in fact

1512
01:40:08,420 --> 01:40:09,830
one of the key results

1513
01:40:10,010 --> 01:40:12,810
this responsible for bringing all this stuff to

1514
01:40:13,080 --> 01:40:13,730
of late

1515
01:40:13,900 --> 01:40:16,510
was from geoff hinton and yoshua bengio

1516
01:40:16,720 --> 01:40:19,660
back in two thousand six which is effectively a very

1517
01:40:20,070 --> 01:40:24,130
very cool initialization procedure that made all the training algorithms work much better

1518
01:40:24,610 --> 01:40:25,740
and finally

1519
01:40:25,920 --> 01:40:29,310
i think there's also another underappreciated point which is that

1520
01:40:30,170 --> 01:40:35,940
we now have time of empirical evidence partly from having faster computers more students

1521
01:40:36,200 --> 01:40:38,430
running lots and lots of experiments with these things

1522
01:40:38,660 --> 01:40:42,820
we know much better what are all the various components that matter

1523
01:40:43,270 --> 01:40:48,360
and sort of neat fall out of using neural networks and using a common framework

1524
01:40:48,690 --> 01:40:52,410
for all this is that when your friend tells you conference ok i just tried this

1525
01:40:52,420 --> 01:40:54,320
new module inside my neural network

1526
01:40:54,480 --> 01:40:58,850
you can go home re-implement that thing very quickly mix and match with all the other components

1527
01:40:59,290 --> 01:41:01,590
so there's been quite a bit of work figuring out

1528
01:41:01,740 --> 01:41:05,380
what the right modules to be plugged into these things that we can all use

1529
01:41:07,990 --> 01:41:10,830
so this is pretty great and this is our this is

1530
01:41:10,940 --> 01:41:12,590
now having real impact

1531
01:41:12,790 --> 01:41:14,730
used to be is pretty tough

1532
01:41:14,900 --> 01:41:19,360
to do a real application because we needed all that prior knowledge had to be a domain expert

1533
01:41:19,640 --> 01:41:22,370
and deep learning systems our starting to really

1534
01:41:22,580 --> 01:41:26,170
come of age and we're seeing a lot of state the-art results not just in computer vision

1535
01:41:26,320 --> 01:41:28,440
but lots of feet different fields

1536
01:41:28,600 --> 01:41:30,050
most recently

1537
01:41:30,160 --> 01:41:32,440
very cool result by alex krizhevsky

1538
01:41:32,650 --> 01:41:34,550
elicits cover and geoff hinton

1539
01:41:34,690 --> 01:41:36,570
from nips twenty twelve

1540
01:41:36,750 --> 01:41:41,630
was this basically deep neural network that is currently holding the state of the art

1541
01:41:41,840 --> 01:41:43,630
results on the imagenet dataset

1542
01:41:43,740 --> 01:41:45,520
so very large data set

1543
01:41:45,730 --> 01:41:48,820
they hold challenge every year called the imagenet

1544
01:41:49,060 --> 01:41:51,150
large scale visual recognition challenge

1545
01:41:51,290 --> 01:41:55,680
and they're deep neural network was the first place system in this challenge

1546
01:41:56,360 --> 01:41:58,050
and i'll show you some results from that later

1547
01:41:58,280 --> 01:42:03,890
hopefully by maybe an hour into this talk you'll have all the basic underpinnings to

1548
01:42:03,910 --> 01:42:05,400
implement system like yourself

1549
01:42:06,230 --> 01:42:08,510
but we're saying that outside vision

1550
01:42:08,640 --> 01:42:11,650
the things are also are doing incredible things in speech

1551
01:42:11,820 --> 01:42:14,090
and natural language processing so

1552
01:42:14,210 --> 01:42:17,290
there's a whole bunch of other stuff to go along those fields

1553
01:42:17,540 --> 01:42:20,600
i that i hope you'll go in and read up on some point

1554
01:42:20,600 --> 01:42:23,040
this filling in distribution is

1555
01:42:23,060 --> 01:42:26,720
the correct posture appears at x

1556
01:42:26,740 --> 01:42:33,160
that is the q which saturates exist and so improve the article already do

1557
01:42:35,530 --> 01:42:37,450
from this

1558
01:42:37,470 --> 01:42:39,100
this is

1559
01:42:39,120 --> 01:42:40,720
that flight

1560
01:42:40,740 --> 01:42:41,870
the here

1561
01:42:41,890 --> 01:42:42,950
and again

1562
01:42:42,950 --> 01:42:49,390
four months later shows that if you apply this exact experience q you actually

1563
01:42:49,760 --> 01:42:54,990
exactly the marginalized so that you will see it

1564
01:42:56,290 --> 01:42:58,060
you can also show this by

1565
01:42:58,060 --> 01:43:00,910
essentially using variational calculus or by

1566
01:43:00,970 --> 01:43:06,240
using the fact that the difference between the likelihood of the ban is the KL

1567
01:43:06,240 --> 01:43:08,470
divergence between two and

1568
01:43:08,510 --> 01:43:10,040
this year

1569
01:43:10,100 --> 01:43:12,410
OK so

1570
01:43:12,430 --> 01:43:20,200
we now have the ingredients that we need and to try to sort concrete example

1571
01:43:20,200 --> 01:43:25,990
in the case sugar so make sure that you have the marginal density x is

1572
01:43:25,990 --> 01:43:27,120
just a sure

1573
01:43:29,140 --> 01:43:32,770
the law likelihood is

1574
01:43:32,790 --> 01:43:34,450
just a lot that

1575
01:43:35,350 --> 01:43:37,600
and here's how to

1576
01:43:38,240 --> 01:43:46,510
we need to compute the correct posterior distributions over cluster assignments given the current and

1577
01:43:48,120 --> 01:43:51,030
that's exactly what

1578
01:43:51,080 --> 01:43:52,640
doing so

1579
01:43:52,680 --> 01:43:55,740
i'm going to do about the probability of the cluster

1580
01:43:55,760 --> 01:43:57,950
signed privacy and data points

1581
01:43:57,990 --> 01:44:01,830
this k given the data and the kernel parameters

1582
01:44:01,830 --> 01:44:04,790
recall that q so can

1583
01:44:04,810 --> 01:44:11,810
and that q is just the ratio of the probability of the data and the

1584
01:44:11,930 --> 01:44:14,740
model divided by the total of the

1585
01:44:14,760 --> 01:44:16,930
so you say how likely is it under

1586
01:44:16,930 --> 01:44:19,830
they divided by is

1587
01:44:19,850 --> 01:44:25,790
some overall and ratio are the responsibility to secure distribution

1588
01:44:25,810 --> 01:44:29,490
so nice that we basically have

1589
01:44:29,510 --> 01:44:31,200
for each data point

1590
01:44:31,260 --> 01:44:32,330
do not want

1591
01:44:32,330 --> 01:44:37,470
which are the prime current parameters when i think about that you want to come

1592
01:44:37,470 --> 01:44:39,890
to london to street

1593
01:44:39,890 --> 01:44:43,620
so if you want to to put tag data

1594
01:44:43,680 --> 01:44:49,410
the tag q which says this point belongs to seven to john f

1595
01:44:49,410 --> 01:44:53,930
three to alex smola and zero zero one two

1596
01:44:54,790 --> 01:44:57,490
and the different data different

1597
01:44:57,490 --> 01:44:59,220
which makes it

1598
01:44:59,240 --> 01:45:04,720
and in the sense that something very interesting in the step i do

1599
01:45:04,760 --> 01:45:07,490
responsibility weighted more

1600
01:45:07,510 --> 01:45:08,430
so now

1601
01:45:08,450 --> 01:45:08,850
the k

1602
01:45:11,270 --> 01:45:14,080
but she got senses mean two

1603
01:45:14,100 --> 01:45:18,790
the responsibility weighted average of

1604
01:45:18,790 --> 01:45:23,060
so it doesn't remember all money to all of the data that was in cluster

1605
01:45:23,060 --> 01:45:24,220
k a b

1606
01:45:24,220 --> 01:45:25,180
back to me

1607
01:45:25,200 --> 01:45:26,720
well here

1608
01:45:26,720 --> 01:45:28,560
we don't really have a hard

1609
01:45:28,600 --> 01:45:31,760
soft labels to just take the weighted average

1610
01:45:31,760 --> 01:45:37,740
the responsibility weighted average of the data that you need cluster k and responsibility with

1611
01:45:37,740 --> 01:45:42,580
covariance is the new covariance and some of the responsibilities

1612
01:45:42,600 --> 01:45:48,640
the fraction the responsibility in total along is the u s

1613
01:45:49,620 --> 01:45:51,990
so it's very intuitive

1614
01:45:54,510 --> 01:45:58,510
so the first step is just the application of these rules but in general this

1615
01:45:58,510 --> 01:46:03,930
will involve writing an inference that and this is just the weighted learning in fully

1616
01:46:03,930 --> 01:46:08,890
observed case and in general to all the same kind of thing that you want

1617
01:46:08,890 --> 01:46:14,030
to learn about with weights on the data points instead of things

1618
01:46:14,080 --> 01:46:17,260
so it's still hard to this this model

1619
01:46:18,220 --> 01:46:20,040
so here's the data

1620
01:46:20,060 --> 01:46:21,160
in greek

1621
01:46:21,180 --> 01:46:29,970
and then when showing you erations each point drawn parameters going make sure to get

1622
01:46:29,970 --> 01:46:34,620
so the parameters the all one graph

1623
01:46:34,640 --> 01:46:40,950
the covariance and from all to blue and also colored points according to their maximum

1624
01:46:40,990 --> 01:46:47,240
posterior assignments so this the first iteration now he's and any

1625
01:46:47,240 --> 01:46:49,870
you want

1626
01:46:54,120 --> 01:46:55,340
are there

1627
01:47:00,190 --> 01:47:02,730
my i

1628
01:47:04,710 --> 01:47:08,270
but i want to are

1629
01:47:08,320 --> 01:47:10,380
o four

1630
01:47:22,270 --> 01:47:24,130
from this

1631
01:47:48,710 --> 01:47:56,320
all right

1632
01:48:46,600 --> 01:48:55,330
and that

1633
01:49:17,900 --> 01:49:39,030
he said

1634
01:49:48,300 --> 01:49:50,940
so here we are

1635
01:49:51,690 --> 01:49:53,280
one of them

1636
01:49:54,070 --> 01:49:58,380
here in manner

1637
01:49:58,390 --> 01:50:00,560
all these

1638
01:50:05,740 --> 01:50:07,040
i think

1639
01:50:12,600 --> 01:50:16,330
of the most

1640
01:50:33,780 --> 01:50:36,560
i was

1641
01:50:53,500 --> 01:50:58,170
you are right

1642
01:51:02,270 --> 01:51:07,080
you are

1643
01:51:07,080 --> 01:51:08,760
when this product

1644
01:51:08,800 --> 01:51:10,880
it's going to be shipped

1645
01:51:10,900 --> 01:51:12,810
from this

1646
01:51:14,250 --> 01:51:15,980
have an exception

1647
01:51:15,990 --> 01:51:17,560
life going on

1648
01:51:19,790 --> 01:51:23,140
our probabilities our small numbers

1649
01:51:23,210 --> 01:51:25,000
and all this

1650
01:51:25,010 --> 01:51:29,320
so is usually very close to zero

1651
01:51:29,380 --> 01:51:33,670
and logarithmic from zero it's

1652
01:51:33,710 --> 01:51:39,530
minus infinity and in many much is be exception

1653
01:51:40,130 --> 01:51:44,530
i can show you very very simple approach how to solve this problem

1654
01:51:44,540 --> 01:51:46,180
you simply need to

1655
01:51:47,150 --> 01:51:50,120
the biggest probability from this

1656
01:51:51,410 --> 01:51:54,550
more and more a movement from this form format

1657
01:51:54,620 --> 01:51:59,180
i don't understand how i got this result

1658
01:51:59,320 --> 01:52:01,140
i'm assuming that

1659
01:52:01,160 --> 01:52:04,540
he one of the biggest probability

1660
01:52:04,550 --> 01:52:05,840
in this case

1661
01:52:06,720 --> 01:52:12,840
is the biggest probability lasts longer is very close to zero

1662
01:52:12,860 --> 01:52:16,580
you can even if you need optimisation and they want to talk

1663
01:52:16,660 --> 01:52:20,760
this calculation you can even still based on

1664
01:52:20,760 --> 01:52:23,240
absolutely of

1665
01:52:26,320 --> 01:52:30,130
of these substructures of licorice

1666
01:52:30,710 --> 01:52:35,220
it's very obvious things but i don't know why but in real life usually people

1667
01:52:35,240 --> 01:52:37,250
i don't understand how to do

1668
01:52:37,330 --> 01:52:39,650
so we know how to work with probabilities

1669
01:52:39,670 --> 01:52:40,780
they have

1670
01:52:40,820 --> 01:52:47,930
our huge files with sculptors with work his by around his driver and set of

1671
01:52:49,950 --> 01:52:52,630
it looks like we have everything to create our

1672
01:52:53,220 --> 01:52:55,990
well suggestions or something like this

1673
01:52:58,900 --> 01:53:01,630
the result is too huge

1674
01:53:03,040 --> 01:53:05,380
use imagine that we have

1675
01:53:05,400 --> 01:53:07,010
dictionary of

1676
01:53:07,010 --> 01:53:09,320
one thousand words

1677
01:53:09,360 --> 01:53:12,510
and we have some big college

1678
01:53:12,520 --> 01:53:16,810
one thousand words not is not big dictionary you can fit into memory

1679
01:53:16,820 --> 01:53:18,760
at the same time

1680
01:53:18,760 --> 01:53:20,460
why graph

1681
01:53:20,470 --> 01:53:21,450
you have

1682
01:53:21,470 --> 01:53:26,900
and square by around in real life usually we don't have all the way around

1683
01:53:26,960 --> 01:53:29,300
so you have

1684
01:53:29,320 --> 01:53:34,320
maybe a number of millions by background this is a problem

1685
01:53:34,370 --> 01:53:37,490
again it cannot fit into memory

1686
01:53:37,490 --> 01:53:40,500
and intuitions about what they can do consider

1687
01:53:41,750 --> 01:53:44,020
we already has smoothing

1688
01:53:46,100 --> 01:53:48,290
i don't really accept that

1689
01:53:48,290 --> 01:53:51,380
a lot of work with low probability

1690
01:53:51,400 --> 01:53:53,820
it's simple and scene

1691
01:53:53,850 --> 01:53:56,220
so let's assume there exist

1692
01:53:56,230 --> 01:53:57,590
three and four

1693
01:53:57,610 --> 01:53:59,560
they're not interested for us

1694
01:53:59,630 --> 01:54:07,080
can simply remove them and to use for their probability estimation from schmoozing

1695
01:54:07,130 --> 01:54:09,200
i believe that this is true so

1696
01:54:09,250 --> 01:54:12,030
how we can estimate that it's true

1697
01:54:12,040 --> 01:54:13,800
we can look at

1698
01:54:13,830 --> 01:54:17,060
very simple very prone

1699
01:54:17,120 --> 01:54:21,480
language model when we have a language model is

1700
01:54:21,480 --> 01:54:23,100
the mosque

1701
01:54:23,120 --> 01:54:27,340
probably more of the english language in this article there

1702
01:54:27,360 --> 01:54:31,200
and the second most probable words

1703
01:54:31,240 --> 01:54:35,230
position or and you can see that

1704
01:54:35,240 --> 01:54:36,790
actually it's not not very

1705
01:54:36,800 --> 01:54:43,610
exact example that doesn't show a lot but you can see that difference in entropy

1706
01:54:43,620 --> 01:54:44,870
here is not so

1707
01:54:45,300 --> 01:54:46,890
so being

1708
01:54:46,950 --> 01:54:48,810
from zero two

1709
01:54:48,900 --> 01:54:53,430
if the using the simplest model is one where we have point seventeen

1710
01:54:55,310 --> 01:54:59,650
adding next word is adding one

1711
01:54:59,700 --> 01:55:02,600
o point one only point one to

1712
01:55:02,600 --> 01:55:04,310
very pretty easy

1713
01:55:04,330 --> 01:55:08,930
he introduced the long-range interactions or long range interactions in general

1714
01:55:08,950 --> 01:55:13,200
it actually can bring dynamic programming grinding halt

1715
01:55:13,250 --> 01:55:16,390
so you can get very very expensive

1716
01:55:16,450 --> 01:55:17,500
to the point where

1717
01:55:17,520 --> 01:55:20,770
exact dynamic programming is not used anymore

1718
01:55:20,770 --> 01:55:22,640
there are lots of

1719
01:55:22,640 --> 01:55:29,410
well beautiful not the beautiful approximations for approximate that program

1720
01:55:29,600 --> 01:55:35,520
this is one of the really i think the weapon right now

1721
01:55:38,620 --> 01:55:40,730
this basically what i told you before

1722
01:55:40,770 --> 01:55:43,370
you do the recurrence back

1723
01:55:47,140 --> 01:55:50,720
there's a lot of extensions of background structure but given the by that we will

1724
01:55:50,720 --> 01:55:54,870
be read to talk about the page tracking i will be doing that anymore

1725
01:55:56,520 --> 01:55:59,270
but the page tracking

1726
01:55:59,310 --> 01:56:04,210
what you want to i mean this is very different

1727
01:56:04,230 --> 01:56:06,270
estimation problem when not tried

1728
01:56:06,290 --> 01:56:12,100
i think when i'm trying to output a private

1729
01:56:12,100 --> 01:56:14,500
the permutation is the structured objects

1730
01:56:14,520 --> 01:56:18,870
so well i want to estimate the right permutation for

1731
01:56:18,930 --> 01:56:24,210
now the problem interested in more specifically is where you have a collection of documents

1732
01:56:24,250 --> 01:56:26,040
and associated query

1733
01:56:26,060 --> 01:56:28,640
and relevance scores

1734
01:56:28,660 --> 01:56:31,520
the reason for that is because this is actually

1735
01:56:31,580 --> 01:56:34,200
the way how at least some search engines

1736
01:56:34,210 --> 01:56:38,230
about it being generated from users

1737
01:56:38,470 --> 01:56:42,290
the reason why need this baseline data because of another like this brought let's say

1738
01:56:42,310 --> 01:56:43,540
for instance

1739
01:56:43,540 --> 01:56:48,640
i look for smaller in germany and somehow i'm opening up as the first hit

1740
01:56:48,640 --> 01:56:51,470
but maybe the third or fourth is incentives

1741
01:56:51,520 --> 01:56:55,850
i mean i actually then hire somebody to click on my link all the time

1742
01:56:55,850 --> 01:56:58,620
or if a little bit smarter i will actually

1743
01:56:58,810 --> 01:57:01,810
right the tried and which will then pick lots of

1744
01:57:01,850 --> 01:57:06,200
computers and they will also create google query for small and then they will become

1745
01:57:07,160 --> 01:57:10,000
this way you can have fairly nasty problem

1746
01:57:10,910 --> 01:57:14,330
and this way google at some point might realise how so many people are clicking

1747
01:57:15,450 --> 01:57:17,100
my name so therefore

1748
01:57:17,140 --> 01:57:19,890
what i'm doing must be super relevant and they will

1749
01:57:19,890 --> 01:57:21,770
increase my writing

1750
01:57:21,770 --> 01:57:23,120
and those that

1751
01:57:23,120 --> 01:57:24,660
OK well

1752
01:57:25,680 --> 01:57:29,000
well not always not with my name

1753
01:57:29,020 --> 01:57:34,230
but but i mean there are companies basically

1754
01:57:34,230 --> 01:57:41,000
well gain some advantage of that we lost when think about it but they can

1755
01:57:41,000 --> 01:57:46,430
decrease the rankings companies hold search thing and that the or loss of the single

1756
01:57:46,660 --> 01:57:47,660
searching you

1757
01:57:47,680 --> 01:57:50,370
find out about the very

1758
01:57:52,080 --> 01:57:54,370
and i know that

1759
01:57:54,370 --> 01:57:57,270
a well-known writer for support vector book

1760
01:57:57,350 --> 01:57:59,540
optimizes the site very much to

1761
01:57:59,560 --> 01:58:01,640
make sure it comes up very highly

1762
01:58:03,600 --> 01:58:07,310
some search engines and now it's not the book with bernard

1763
01:58:07,560 --> 01:58:09,620
OK anyway

1764
01:58:13,120 --> 01:58:15,080
what you get is about documents

1765
01:58:15,100 --> 01:58:18,930
a query and ratings with with that as i want you to do is we

1766
01:58:18,930 --> 01:58:22,730
outsource that india or china depending on which market target

1767
01:58:24,980 --> 01:58:26,560
so you basically get

1768
01:58:26,600 --> 01:58:29,080
user annotated queries

1769
01:58:29,120 --> 01:58:34,060
and and ratings for it and you seem to agree that the resulting saying that

1770
01:58:34,120 --> 01:58:35,980
may be and the pages

1771
01:58:36,040 --> 01:58:37,850
and the query on the right

1772
01:58:37,870 --> 01:58:41,950
and obviously if you look for britney spears then the machine learning summer school web

1773
01:58:41,950 --> 01:58:45,580
pages probably not very meaningful but on the other hand if the machine learning summer

1774
01:58:45,580 --> 01:58:50,410
school this page will be highly aligned they have a query and the document releva

1775
01:58:50,470 --> 01:58:52,520
rate dependent

1776
01:58:55,790 --> 01:58:57,790
and people use all of these are

1777
01:58:57,810 --> 01:59:03,580
scoring function like when discounted cumulative gain or maybe a normalized version of that

1778
01:59:03,580 --> 01:59:05,640
a better of version this is what they

1779
01:59:05,710 --> 01:59:07,910
that the permutation output

1780
01:59:07,980 --> 01:59:09,270
and that's what we have

1781
01:59:10,020 --> 01:59:11,390
there's not enough

1782
01:59:12,430 --> 01:59:14,600
try to the inner product between

1783
01:59:14,870 --> 01:59:18,730
the probit permutation matrix and the model parameters

1784
01:59:19,080 --> 01:59:23,310
why this well this is what i'm asking myself about

1785
01:59:23,330 --> 01:59:25,770
certain communities experiments course

1786
01:59:25,890 --> 01:59:30,000
and every companies still one so for instance

1787
01:59:30,040 --> 01:59:33,600
microsoft exponentiated who who

1788
01:59:33,850 --> 01:59:36,020
explain it but it uses a different

1789
01:59:36,080 --> 01:59:37,770
score for the relevant here

1790
01:59:37,790 --> 01:59:40,730
so and i don't know what to do this

1791
01:59:43,910 --> 01:59:46,600
we have you now highly nontrivial

1792
01:59:46,600 --> 01:59:50,430
scoring function we actually need to turn into a loss function electrons

1793
01:59:50,520 --> 01:59:53,680
and we want to find the optimisation problem for that

1794
01:59:55,870 --> 01:59:57,310
good luck the easy

1795
01:59:57,330 --> 01:59:59,060
i think the article

1796
01:59:59,120 --> 02:00:04,520
basically it performs they could have done anything that everything is sorted array one well

1797
02:00:04,520 --> 02:00:05,890
what i'm actually

