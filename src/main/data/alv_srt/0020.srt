1
00:00:00,000 --> 00:00:04,040
so we have to know my name is written english as an incoming trunk at

2
00:00:04,040 --> 00:00:06,620
stanford university in madrid spain

3
00:00:06,640 --> 00:00:11,540
and then going to present their work sequence classification using statistical pattern recognition

4
00:00:11,540 --> 00:00:15,460
done in collaboration with capitalism as an article isn't

5
00:00:15,520 --> 00:00:18,950
so for as far as i say no they are not getting married

6
00:00:18,980 --> 00:00:19,950
so there

7
00:00:21,020 --> 00:00:25,320
part of this presentation will be the motivation and introduction of this research

8
00:00:25,340 --> 00:00:29,480
then we will explain the main idea of this sequence classification how we have to

9
00:00:29,480 --> 00:00:34,400
talk about this problem our approach after that we will describe the environment where we

10
00:00:34,400 --> 00:00:37,260
have used this approach and the research

11
00:00:37,270 --> 00:00:40,230
finally conclusions and future works

12
00:00:41,020 --> 00:00:44,550
there are many different areas in which we can

13
00:00:44,550 --> 00:00:49,660
it can be very useful to model classifier recognise the behaviour previous to this research

14
00:00:49,660 --> 00:00:53,650
that we're presented today we have been working in the circus simulation the mind

15
00:00:53,660 --> 00:00:55,870
and what we do is to

16
00:00:55,910 --> 00:01:01,410
classify the opening team behaviour in order to improve our own team behavior

17
00:01:01,440 --> 00:01:05,610
in this research we are interesting in their behaviour classification

18
00:01:05,660 --> 00:01:11,280
but where the behaviour is represented that sequences of elements therefore we can talk about

19
00:01:11,280 --> 00:01:13,140
sequence classification

20
00:01:13,140 --> 00:01:19,050
when this sequence is the set of elements where are there is is essential

21
00:01:19,060 --> 00:01:20,500
OK after this brief

22
00:01:20,530 --> 00:01:25,030
into ocean we will describe the sequence classification and the main goal is

23
00:01:25,080 --> 00:01:28,580
given a set of classes and a sequence

24
00:01:28,590 --> 00:01:31,280
to classify we still determine

25
00:01:31,330 --> 00:01:34,890
which class does this sequence belongs to

26
00:01:35,970 --> 00:01:38,580
how we have tackled this problem

27
00:01:38,590 --> 00:01:43,880
OK first in italy what we have here a set of sequences

28
00:01:43,890 --> 00:01:46,980
and a sequence to classify

29
00:01:47,690 --> 00:01:52,000
as we can see in this slide we have used for this example unix commands

30
00:01:52,000 --> 00:01:56,910
sequences but sequences with any kind of elements can be used

31
00:01:57,970 --> 00:01:59,140
the two parts

32
00:01:59,170 --> 00:02:04,030
of our approach the library creation the first part and the second part is the

33
00:02:05,280 --> 00:02:06,810
in the first part

34
00:02:06,810 --> 00:02:08,530
but what we do is

35
00:02:08,530 --> 00:02:09,900
to get there

36
00:02:09,930 --> 00:02:15,560
whole sequence that represent represented by class and we create a set of others that

37
00:02:15,570 --> 00:02:19,470
we are storing the special way that we will explain later

38
00:02:19,490 --> 00:02:22,910
and we start betting what we call but in library

39
00:02:22,910 --> 00:02:24,940
once we have created it

40
00:02:24,960 --> 00:02:27,030
this library what we do is

41
00:02:27,030 --> 00:02:30,620
weak with the same set of patterns for the sequence to classify

42
00:02:30,650 --> 00:02:31,910
and we compare

43
00:02:31,930 --> 00:02:34,820
this set of buttons with their

44
00:02:34,840 --> 00:02:37,880
classes classicist starts in the in the library

45
00:02:37,900 --> 00:02:44,350
and then finally the classification results depend of the results of these comparisons

46
00:02:45,120 --> 00:02:46,380
we will explain now

47
00:02:46,940 --> 00:02:51,310
two parts of our approach and for the first one

48
00:02:51,320 --> 00:02:55,590
we are using their that their structure called try

49
00:02:55,590 --> 00:03:01,810
and it's spatial search tree used for storing elements and its prefixes and in these

50
00:03:01,900 --> 00:03:09,440
structure very node represents an element and every note also stores useful information mainly how

51
00:03:09,440 --> 00:03:12,970
many times this node has been appear in the

52
00:03:13,030 --> 00:03:14,710
in the sequence

53
00:03:15,460 --> 00:03:19,510
we will explain very very quick how it works so it we have sequence like

54
00:03:20,180 --> 00:03:22,620
first what we need is because of their

55
00:03:22,660 --> 00:03:28,780
the length of this sequence we have to split the island into sequences sub sequences

56
00:03:28,780 --> 00:03:33,410
of equal size so for example if we use a a subsequence of

57
00:03:33,500 --> 00:03:34,540
length three

58
00:03:34,560 --> 00:03:36,750
for this example

59
00:03:36,760 --> 00:03:37,720
we have

60
00:03:37,740 --> 00:03:41,470
two different sub sequences that we have to insert in the trie

61
00:03:41,500 --> 00:03:43,220
that is the structure that we

62
00:03:43,280 --> 00:03:44,530
we have

63
00:03:44,530 --> 00:03:45,440
we have

64
00:03:45,440 --> 00:03:47,000
what we have so

65
00:03:47,000 --> 00:03:52,250
first we insert their they hold subsequence and after that because of the relevant because

66
00:03:52,250 --> 00:03:53,030
of their

67
00:03:53,040 --> 00:04:00,150
and the dependence of the between their elements is very relevant so we is also

68
00:04:00,190 --> 00:04:03,760
the suffixes of these sequence

69
00:04:04,720 --> 00:04:06,650
so we have to consider that

70
00:04:06,660 --> 00:04:07,900
they nodes who

71
00:04:07,910 --> 00:04:11,060
the elements who have already been inserted into try

72
00:04:11,060 --> 00:04:13,870
we don't quit and you know for these

73
00:04:13,910 --> 00:04:17,950
for these element what we do is we change the number that we can see

74
00:04:17,950 --> 00:04:23,170
here in brackets represent how many times has this note has been appearing there in

75
00:04:23,170 --> 00:04:24,440
the sequence

76
00:04:24,450 --> 00:04:25,950
after the first

77
00:04:25,980 --> 00:04:30,790
subsequent we at the second one and also its prefixes

78
00:04:31,890 --> 00:04:35,240
as we can see for its sequence we will have

79
00:04:35,250 --> 00:04:37,310
we will start this sequencing

80
00:04:37,380 --> 00:04:39,630
a structure like this

81
00:04:41,420 --> 00:04:46,640
after great this right we have to evaluate the relation between an element and its

82
00:04:48,620 --> 00:04:51,690
for this we have used and statistical approach

83
00:04:51,740 --> 00:04:53,950
so what we do is

84
00:04:53,960 --> 00:04:58,830
we calculate cheese square value for its

85
00:04:58,850 --> 00:04:59,980
in the

86
00:05:01,520 --> 00:05:02,900
we have

87
00:05:02,920 --> 00:05:06,060
to calculate a two by two comprehensive table

88
00:05:06,070 --> 00:05:09,040
and after that we started this tir square

89
00:05:09,060 --> 00:05:10,230
value in

90
00:05:10,250 --> 00:05:12,380
in its nose and we can see here

91
00:05:12,390 --> 00:05:14,010
in red color

92
00:05:15,490 --> 00:05:20,080
these this structure like this is created for each class

93
00:05:20,110 --> 00:05:22,300
and this is the set of these

94
00:05:22,310 --> 00:05:24,180
this structure is what we call

95
00:05:24,190 --> 00:05:27,060
pattern library

96
00:05:27,930 --> 00:05:31,690
once once we have created these library we have two

97
00:05:31,740 --> 00:05:33,290
the classification

98
00:05:33,570 --> 00:05:35,170
for doing that

99
00:05:35,210 --> 00:05:39,620
first what we do is to create their it to it right for this sequence

100
00:05:39,620 --> 00:05:44,040
to classify in the same way that we have already explained

101
00:05:44,060 --> 00:05:45,810
and then we have to compare

102
00:05:46,810 --> 00:05:48,480
these strike with their

103
00:05:48,490 --> 00:05:49,880
tried that we have

104
00:05:50,000 --> 00:05:53,740
story in the library and for this comparison what we do

105
00:05:54,700 --> 00:05:58,190
four heats in the testing trie

106
00:05:58,200 --> 00:05:59,820
we start this set

107
00:05:59,850 --> 00:06:02,000
the same note in the class

108
00:06:02,000 --> 00:06:04,880
try and they try

109
00:06:04,940 --> 00:06:08,620
we can see that if the node and its prefix are in both tries its

110
00:06:10,190 --> 00:06:14,520
we can we have to compare the tears square and if if these two queries

111
00:06:14,520 --> 00:06:18,480
is smaller than a threshold value its mean that there is some kind of similarity

112
00:06:18,480 --> 00:06:20,170
between both tries

113
00:06:21,580 --> 00:06:24,230
what we do is we get a positive value

114
00:06:24,270 --> 00:06:28,260
but in this case we have considered the tir square value of the test

115
00:06:29,940 --> 00:06:31,060
this is the

116
00:06:31,070 --> 00:06:35,820
the value for this example and after that we have to do this one for

117
00:06:35,830 --> 00:06:37,670
all the nodes of the

118
00:06:37,690 --> 00:06:38,730
testing try

119
00:06:39,000 --> 00:06:43,360
for example in this case if the note and also its prefix are only in

120
00:06:43,360 --> 00:06:48,180
the testing try its mean that there is some kind of different so what we

121
00:06:48,180 --> 00:06:49,750
do is we store

122
00:06:49,760 --> 00:06:51,450
a negative value

123
00:06:51,480 --> 00:06:56,440
and and these negative values so that you square of the distance

124
00:06:57,770 --> 00:07:00,680
you will have to do that with every note of the

125
00:07:00,690 --> 00:07:02,120
of the distant right

126
00:07:02,120 --> 00:07:05,750
and after that the were sort of this comparison

127
00:07:05,750 --> 00:07:09,980
is a set of similarities and differences so

128
00:07:09,990 --> 00:07:13,670
the final result is this room of all these

129
00:07:13,700 --> 00:07:14,870
comparing come in

130
00:07:14,880 --> 00:07:16,560
similarities and differences

131
00:07:16,580 --> 00:07:20,080
so for example in this simple example that we have

132
00:07:20,130 --> 00:07:23,630
that we have used this will be the comparison value

133
00:07:23,650 --> 00:07:25,800
so the idea is that we have to

134
00:07:26,300 --> 00:07:28,510
we have to obtain a comparison value

135
00:07:28,520 --> 00:07:30,820
for it's claims that we have already

136
00:07:30,890 --> 00:07:33,620
store in there in the pattern library

137
00:07:33,620 --> 00:07:35,450
and finally

138
00:07:35,450 --> 00:07:41,790
according to john economics american economic review but this particular paper is still working

139
00:07:41,850 --> 00:07:47,440
so i think that he had some problems in the refereeing process in

140
00:07:47,780 --> 00:07:54,840
having his ideas set by by the referees in any case he starts from a

141
00:07:54,890 --> 00:07:56,530
stylised facts

142
00:07:56,580 --> 00:08:01,440
and the cell fact is that firms in terms of size

143
00:08:01,480 --> 00:08:03,550
our power law distribution

144
00:08:03,580 --> 00:08:09,750
there's a huge amount of evidence about this and you can change the measure of

145
00:08:09,750 --> 00:08:14,190
the size in terms of sales in terms of employment

146
00:08:14,200 --> 00:08:15,980
in terms of total capital

147
00:08:16,000 --> 00:08:18,970
you can change countries as well

148
00:08:18,970 --> 00:08:20,360
united states

149
00:08:20,380 --> 00:08:21,770
it's only

150
00:08:21,790 --> 00:08:23,710
israel as well

151
00:08:24,060 --> 00:08:31,480
in germany and some great britain damarcus from the evidence for all these countries and

152
00:08:31,480 --> 00:08:37,500
in all cases it turns out to out that the firm size distribution is is

153
00:08:37,500 --> 00:08:39,040
the power

154
00:08:42,860 --> 00:08:48,580
the idea of the bayes that's well suppose that we have IID shocks

155
00:08:48,600 --> 00:08:52,980
maybe coming from the oceans from normal distribution

156
00:08:53,010 --> 00:08:54,740
so with the five minus

157
00:08:54,750 --> 00:08:58,270
but this sharks affect

158
00:09:00,020 --> 00:09:02,900
which are different in size

159
00:09:02,910 --> 00:09:05,760
so we have sharks two very big firms

160
00:09:05,920 --> 00:09:08,140
shops to small firms

161
00:09:08,160 --> 00:09:10,660
so in this case it's possible to show

162
00:09:10,780 --> 00:09:19,500
he shows hit that if we have an inference and surface firms apologists beauty

163
00:09:19,520 --> 00:09:21,290
well the

164
00:09:22,380 --> 00:09:24,190
i mean if equations

165
00:09:24,200 --> 00:09:27,290
as the size which is proportional assign increases

166
00:09:27,320 --> 00:09:32,880
the size was proposed with the is which is proportional to one over the natural

167
00:09:32,880 --> 00:09:35,270
logarithm of n instead of

168
00:09:35,700 --> 00:09:40,530
one over and square which is the

169
00:09:40,550 --> 00:09:43,140
classical results for the large numbers

170
00:09:43,140 --> 00:09:48,430
so the lower level numbers of as we can slow the application of the rule

171
00:09:48,430 --> 00:09:49,560
of law

172
00:09:49,580 --> 00:09:55,550
OK but this is this is nations

173
00:09:55,560 --> 00:09:57,730
continues to assume that

174
00:09:58,470 --> 00:10:01,480
total factor productivity shocks

175
00:10:02,650 --> 00:10:04,020
five hours

176
00:10:04,040 --> 00:10:07,130
so the idea that try to exploit

177
00:10:09,690 --> 00:10:11,650
what about if shocks

178
00:10:11,660 --> 00:10:14,200
i have infinite lives

179
00:10:14,220 --> 00:10:20,670
so they can be modeled by means of letters simple solution

180
00:10:21,710 --> 00:10:27,520
i have some results on some statistical results i'm not going to a i'm not

181
00:10:27,530 --> 00:10:31,250
i'm not going to address this topic theoretically of i'm just working but i have

182
00:10:31,250 --> 00:10:38,540
not resolved yet i just i have just some statistical results which has been published

183
00:10:38,560 --> 00:10:40,220
two or three months ago

184
00:10:40,220 --> 00:10:45,760
and i i xi started from from the data

185
00:10:45,820 --> 00:10:50,950
from the data about the fact that it we have published a labelled data for

186
00:10:50,950 --> 00:11:01,480
the united states usage amount of data we have data for forty years for more

187
00:11:02,140 --> 00:11:05,780
four hundred and sixty sec tools

188
00:11:05,840 --> 00:11:10,050
so we have really for economics this is a huge amount of data

189
00:11:10,060 --> 00:11:14,950
and try to make some distribution fitting exercise

190
00:11:14,950 --> 00:11:20,190
moving from this from this hints so this is

191
00:11:20,200 --> 00:11:21,690
the time series

192
00:11:21,730 --> 00:11:26,710
of the total factor productivity shocks his official

193
00:11:28,600 --> 00:11:33,140
this artificial data published by the national bureau of economic research so probably one of

194
00:11:33,140 --> 00:11:44,410
the most well-known research institute in economics all over the world

195
00:11:44,450 --> 00:11:50,720
no they obtain the data from the production function approach

196
00:11:51,440 --> 00:11:56,670
so the estimator yes you can find details in the paper by about my and

197
00:11:56,860 --> 00:12:01,350
others in the national bureau of economic research working paper scissors

198
00:12:01,630 --> 00:12:09,890
so here you have three times the standard deviation around me

199
00:12:10,600 --> 00:12:18,290
you have four hundred seventy sectors these are just for example

200
00:12:19,000 --> 00:12:24,930
items for distribution and you find that it's quite common to find some

201
00:12:24,980 --> 00:12:27,690
it's really come common for some plants

202
00:12:27,710 --> 00:12:31,000
recall that's

203
00:12:31,050 --> 00:12:37,170
inside the last mile three standard deviation if you have a normal assumptions more than

204
00:12:37,170 --> 00:12:40,230
ninety nine percent of the mass of the solution

205
00:12:40,240 --> 00:12:42,780
so this is really

206
00:12:42,820 --> 00:12:44,440
this really

207
00:12:45,230 --> 00:12:52,880
events really really far in the tails of the distribution if we have if we

208
00:12:52,880 --> 00:12:54,090
assume might

209
00:12:54,110 --> 00:13:01,670
OK and when this happens basically for for for all the time series of distribution

210
00:13:02,650 --> 00:13:04,460
so i tried to two

211
00:13:04,460 --> 00:13:07,840
two to to see what happens if if we

212
00:13:10,940 --> 00:13:12,780
the the war dead

213
00:13:12,780 --> 00:13:17,690
because the this the separator is with this resource score so the unreasonable ones always

214
00:13:17,690 --> 00:13:20,970
zero reason once deposit would have one

215
00:13:20,990 --> 00:13:24,170
the reason was a negative weight minus one and so

216
00:13:24,300 --> 00:13:30,880
so we need as well if you've got lots of features but most of them

217
00:13:30,880 --> 00:13:32,830
are way zero so

218
00:13:32,830 --> 00:13:36,860
when i was just an example of an algorithm has that kind of property

219
00:13:36,870 --> 00:13:41,360
anyway so this is an example of changing the definition of

220
00:13:41,370 --> 00:13:46,200
still if it's possible that it's still not quite as good as the standard SVM

221
00:13:46,220 --> 00:13:48,230
anyway so

222
00:13:49,700 --> 00:13:50,720
OK so

223
00:13:53,040 --> 00:13:56,760
i because the main point of this is just to say that normally

224
00:13:58,240 --> 00:14:02,900
the notions of kernel functions it has been very much the series all been very

225
00:14:02,900 --> 00:14:07,580
tied to viewing them as implicit mappings and the idea is that we can still

226
00:14:07,580 --> 00:14:12,370
make statements about what makes the similarity function useful for learning without talking about implicit

227
00:14:12,370 --> 00:14:17,630
spaces so it's possible to filter theory includes the usual notion of good kernel functions

228
00:14:17,810 --> 00:14:21,280
although we did end up losing something in the parameters i suspect that there and

229
00:14:21,280 --> 00:14:25,270
actually i think it's this recent ICML the that some people have different notion did

230
00:14:26,550 --> 00:14:30,470
it didn't contain all the notion of good kernel function but when a little bit

231
00:14:30,470 --> 00:14:35,540
different direction i suspect there other directions one goes well developed other sufficient conditions that

232
00:14:35,580 --> 00:14:38,480
that each suggest there are now

233
00:14:39,150 --> 00:14:43,270
so so our definition we're motivated by trying to include the usual notion of good

234
00:14:43,270 --> 00:14:49,550
kernels modulo some parameters theory also holds for some functions are

235
00:14:49,570 --> 00:14:52,990
necessarily positive semi definite or even symmetric

236
00:14:53,010 --> 00:14:56,930
i get to the symmetry aspect an expert in the the hope is this may

237
00:14:56,930 --> 00:15:01,300
be helpful with intuition we're trying to come up with a similarity function for you're

238
00:15:01,300 --> 00:15:04,470
given application you don't have to think OK which it is going to have a

239
00:15:04,470 --> 00:15:08,870
larger margin in its implicit space you can maybe think which it is more likely

240
00:15:09,240 --> 00:15:13,470
to have some so the reasonable points that such that things are on average more

241
00:15:13,470 --> 00:15:16,760
similar me start to have some feel of your domain but it might be easy

242
00:15:16,780 --> 00:15:19,340
to fix

243
00:15:19,450 --> 00:15:22,890
OK so so that the second part

244
00:15:22,930 --> 00:15:27,450
i want to ask the question can we use this angle

245
00:15:27,490 --> 00:15:30,010
and i'm thinking about clustering

246
00:15:36,890 --> 00:15:43,280
OK so i think of the following setting so let's imagine we've got

247
00:15:43,430 --> 00:15:48,180
has the same things before we got a bunch of objects i think the documents

248
00:15:48,320 --> 00:15:51,090
images documents about the documents

249
00:15:51,110 --> 00:15:53,860
and so those are documents

250
00:15:53,910 --> 00:15:58,200
and there is some unknown ground truth clustering

251
00:15:59,470 --> 00:16:01,660
each of these has some true label

252
00:16:01,680 --> 00:16:02,550
let's say

253
00:16:02,570 --> 00:16:07,030
between one and TSC with what what's about to stop and think if t is

254
00:16:07,030 --> 00:16:11,590
a relatively small number compared to n number of data big number maybe like five

255
00:16:11,590 --> 00:16:14,450
topics about data documents

256
00:16:14,470 --> 00:16:16,570
OK so everything has some true

257
00:16:16,590 --> 00:16:20,030
topic this one's about this one's about that since about something else

258
00:16:21,660 --> 00:16:22,820
the problem is

259
00:16:22,840 --> 00:16:25,970
well no one told us what those labels are

260
00:16:25,970 --> 00:16:30,660
we OK so of state state is are topic one topic

261
00:16:30,700 --> 00:16:32,890
our job is to

262
00:16:32,910 --> 00:16:34,200
from this data

263
00:16:34,240 --> 00:16:39,050
produce hypothesis producer clustering that is pretty close to the ground truth

264
00:16:39,130 --> 00:16:42,030
so we want to also cost the by topic and we wanna do pretty well

265
00:16:42,030 --> 00:16:46,010
i'm doing pretty well we get them all writing up to isomorphism the label name

266
00:16:46,050 --> 00:16:48,360
so if this is close to one

267
00:16:48,410 --> 00:16:53,140
in this cluster to our algorithm call this cluster one cluster two

268
00:16:53,180 --> 00:16:54,860
five right

269
00:16:55,890 --> 00:17:00,200
we get lower so the same learning type but the problem is that we don't

270
00:17:00,200 --> 00:17:02,110
have any labelled data

271
00:17:02,110 --> 00:17:07,630
nobody really only unlabelled data so can we do anything at all well maybe we're

272
00:17:07,630 --> 00:17:09,970
given the pairwise similarity function

273
00:17:09,990 --> 00:17:15,030
somehow is related to the topic maps it's not right if a notion of similarity

274
00:17:15,030 --> 00:17:17,410
is just based on how long the documents are

275
00:17:17,410 --> 00:17:19,590
that's probably not going to be useful

276
00:17:19,640 --> 00:17:21,510
first telling what they're about

277
00:17:21,550 --> 00:17:23,240
now maybe

278
00:17:23,260 --> 00:17:27,450
it could be but from this hopefully will be related to what we're trying to

279
00:17:27,450 --> 00:17:32,130
do and the question is you know whether sufficient conditions again for how related it

280
00:17:32,130 --> 00:17:32,780
should be

281
00:17:32,800 --> 00:17:36,090
that we could then cluster one

282
00:17:36,110 --> 00:17:40,610
because what conditions on a similarity function would be enough to allow us to get

283
00:17:40,610 --> 00:17:42,470
a good cost

284
00:17:42,570 --> 00:17:46,200
so what i'm doing and said it so i guess one thing i should mention

285
00:17:46,200 --> 00:17:49,490
is i'm sending this up in a way that's a little bit how the office

286
00:17:49,550 --> 00:17:54,910
of the usual way people so that cluster normally you are clustering is a your

287
00:17:54,910 --> 00:18:00,360
input is maybe a graph attributes embedding of points and our already and you view

288
00:18:00,360 --> 00:18:03,630
that embedding is the ground truth i got these points

289
00:18:03,660 --> 00:18:05,610
OK and that's the ground truth

290
00:18:05,640 --> 00:18:11,050
and then you analyse the ability of of algorithms to achieve different optimisation criteria will

291
00:18:11,070 --> 00:18:13,010
be a good algorithm for

292
00:18:13,210 --> 00:18:15,510
k median problem for

293
00:18:15,530 --> 00:18:18,840
k means from in some parts of some objective you try to find a good

294
00:18:18,840 --> 00:18:20,720
algorithm for that

295
00:18:20,780 --> 00:18:26,010
and then you argue about which criteria produced better-looking results on your well i often

296
00:18:26,130 --> 00:18:30,490
k means to test him into that means some i think spectral clustering to this

297
00:18:30,490 --> 00:18:33,160
other thing this one was the better

298
00:18:33,160 --> 00:18:34,990
this looks

299
00:18:35,010 --> 00:18:37,860
so what we do is turn this around

300
00:18:37,910 --> 00:18:39,780
i'm not going to view the embedding

301
00:18:39,800 --> 00:18:44,030
or the graph is ground truthground truth is the topic for each thing i want

302
00:18:44,030 --> 00:18:47,610
to know whether sufficient conditions on

303
00:18:47,610 --> 00:18:52,890
the kind of graph structure on the on the similarity function allows to find that

304
00:18:53,430 --> 00:18:56,240
and so in this way it's a little bit more like what we you do

305
00:18:56,240 --> 00:18:59,680
in the model we say well do learning mixtures of gaussians are something we assume

306
00:18:59,680 --> 00:19:04,490
the some underlying ground truth or probabilistic models by one thing it's

307
00:19:04,510 --> 00:19:08,820
call discriminative rather than generative model i don't wanna

308
00:19:11,260 --> 00:19:15,760
one thing it's more like a statistical learning theory

309
00:19:17,010 --> 00:19:20,930
so so let's start with something that's totally trivially works and let's see what happens

310
00:19:20,930 --> 00:19:26,280
when we try to weaken tiny bit so here's totally here's a really strong property

311
00:19:26,430 --> 00:19:31,340
suppose any two objects at the same time is called documents and clusters for any

312
00:19:31,340 --> 00:19:36,820
two documents of the same topic have similarity bigger than zero two documents of different

313
00:19:36,820 --> 00:19:41,870
topic of similarity lessons suppose a hand similarity function satisfying the property now i claim

314
00:19:41,870 --> 00:19:46,300
it's very easy to cluster documents but it a quite good symmetry that's went to

315
00:19:46,300 --> 00:19:50,180
somebody else so much later OK

316
00:19:50,200 --> 00:19:54,570
let's weaken this little bit

317
00:19:54,660 --> 00:20:00,180
what if i just tell you that my similar function is the following property

318
00:20:01,280 --> 00:20:07,820
object all documents are more similar to other documents of their own topic than to

319
00:20:07,860 --> 00:20:13,010
any document a other top but i promise you that this is a function of

320
00:20:13,010 --> 00:20:17,320
the property for any object if i were to sort the others by similarity to

321
00:20:17,320 --> 00:20:21,550
this guy its own cluster would come first followed by everything else

322
00:20:21,550 --> 00:20:24,410
that's still pretty strong condition

323
00:20:24,410 --> 00:20:27,240
is that enough to cluster well

324
00:20:28,140 --> 00:20:35,800
here's the problem the same similarity function could satisfy this condition for very different clusterings

325
00:20:35,800 --> 00:20:38,450
of the same data to imagine data

326
00:20:38,590 --> 00:20:43,720
documents like this where you've got four taxes this documents about football basketball math and

327
00:20:43,720 --> 00:20:45,910
physics and let's say the ones

328
00:20:45,970 --> 00:20:48,720
you know all these guys are all very similar to each other these guys are

329
00:20:48,720 --> 00:20:51,450
all very similar to each other these guys here discussed here

330
00:20:51,470 --> 00:20:55,780
the up and down the if the medium similarity in this way is less than

331
00:20:58,220 --> 00:21:01,220
so and also what is this this could be

332
00:21:01,260 --> 00:21:03,910
for clustering

333
00:21:03,930 --> 00:21:06,950
OK four clusters before clustering

334
00:21:06,970 --> 00:21:09,970
it could be two clusters could be sports

335
00:21:12,590 --> 00:21:14,840
it could be two clusters or

336
00:21:14,860 --> 00:21:18,390
i mean for those of us in this room for me it looks like this

337
00:21:18,390 --> 00:21:21,570
is math and physics which is clearly different in sport

338
00:21:21,590 --> 00:21:25,530
for people outside this building it looks like this

339
00:21:25,550 --> 00:21:29,200
football and that's what really different stuff with equations over here

340
00:21:29,220 --> 00:21:30,970
OK so

341
00:21:32,530 --> 00:21:34,970
are all consistent with this property

342
00:21:34,990 --> 00:21:37,840
OK so here this is the right answer

343
00:21:37,860 --> 00:21:42,550
everybody if you you first get all these and then you get sort starting from

344
00:21:42,550 --> 00:21:45,110
and in values for a number of

345
00:21:45,120 --> 00:21:48,860
the problem variables problems variables but that's my not boring

346
00:21:51,420 --> 00:21:52,490
we have

347
00:21:52,640 --> 00:21:54,400
a more complete partial

348
00:21:54,690 --> 00:21:57,630
evaluation of all proposed variables

349
00:21:57,640 --> 00:22:01,810
you still have not you reach unit clause contradiction

350
00:22:02,020 --> 00:22:04,810
and you continue doing the same so basically

351
00:22:04,820 --> 00:22:07,410
going down the search for

352
00:22:07,620 --> 00:22:10,470
browsing on another

353
00:22:10,510 --> 00:22:12,670
from variables

354
00:22:12,680 --> 00:22:17,720
the very best of knowledge this clear a point three

355
00:22:17,770 --> 00:22:21,140
marked in red in the

356
00:22:22,580 --> 00:22:24,050
at this point

357
00:22:25,220 --> 00:22:27,570
you need to actually lasted

358
00:22:27,580 --> 00:22:29,090
one two three

359
00:22:29,100 --> 00:22:31,130
four five

360
00:22:31,670 --> 00:22:34,640
values for

361
00:22:34,690 --> 00:22:40,040
many problems there are in fact that's that's all the problems variables that you have

362
00:22:43,950 --> 00:22:48,130
he that so had reached contradiction that this in class

363
00:22:48,130 --> 00:22:50,520
and of course satisfied

364
00:22:50,530 --> 00:22:51,620
so all

365
00:22:52,990 --> 00:22:54,810
no course

366
00:22:54,810 --> 00:22:58,850
been falsified there's no uniform no

367
00:22:58,880 --> 00:23:03,050
and because it's been deriving this means that all clauses are satisfied

368
00:23:03,090 --> 00:23:04,800
and as i

369
00:23:04,810 --> 00:23:06,270
claimed earlier

370
00:23:06,850 --> 00:23:09,280
this formula

371
00:23:09,310 --> 00:23:16,060
being satisfied to being land that sort of planet problems begin to rise

372
00:23:16,160 --> 00:23:18,630
and now we can

373
00:23:18,650 --> 00:23:20,070
actually read the

374
00:23:20,070 --> 00:23:23,560
planet from the evaluation for the

375
00:23:23,610 --> 00:23:30,130
propositional variables that they lost which is actually what they can so i only those

376
00:23:30,130 --> 00:23:33,090
lines here where there is

377
00:23:33,140 --> 00:23:34,670
their actions

378
00:23:34,680 --> 00:23:37,090
variable value through this

379
00:23:37,100 --> 00:23:40,340
here's some of the variables

380
00:23:41,770 --> 00:23:45,280
what this classes it's starts from

381
00:23:45,280 --> 00:23:46,550
they can be

382
00:23:46,560 --> 00:23:49,330
block e

383
00:23:52,050 --> 00:23:54,870
starts from

384
00:24:02,160 --> 00:24:03,120
this a

385
00:24:03,130 --> 00:24:05,260
the action plan so

386
00:24:07,020 --> 00:24:08,820
there we have

387
00:24:08,820 --> 00:24:12,580
only accident stating one zero

388
00:24:12,600 --> 00:24:15,680
this is taking you from the the top of the page

389
00:24:17,740 --> 00:24:20,180
is what has happened

390
00:24:23,730 --> 00:24:26,590
one so the second

391
00:24:27,470 --> 00:24:31,600
so that they are all that this would not be possible with

392
00:24:31,600 --> 00:24:36,210
first sources and we generally so

393
00:24:37,400 --> 00:24:43,320
taken from the table and we're going to be learned simultaneously sees

394
00:24:43,360 --> 00:24:45,060
from the top of the

395
00:24:46,790 --> 00:24:49,280
these lectures

396
00:24:52,480 --> 00:24:55,020
he was thinking about what you

397
00:24:56,960 --> 00:24:57,910
c was

398
00:24:57,920 --> 00:25:04,880
and to

399
00:25:05,760 --> 00:25:10,030
is the end of the table that

400
00:25:14,660 --> 00:25:18,520
and in the last

401
00:25:24,860 --> 00:25:26,560
this point

402
00:25:27,190 --> 00:25:32,780
taken from the sea

403
00:25:35,010 --> 00:25:39,170
the rest the of thing from the one could see the

404
00:25:39,410 --> 00:25:43,140
well actually is a

405
00:25:46,840 --> 00:25:51,790
so take a

406
00:25:51,830 --> 00:25:53,600
this is the

407
00:25:54,660 --> 00:25:56,570
corresponding to this

408
00:25:57,970 --> 00:26:02,420
they is

409
00:26:04,490 --> 00:26:05,970
that has also

410
00:26:05,990 --> 00:26:10,350
the same number of times more so than for problems from zero to five

411
00:26:10,410 --> 00:26:12,410
but he has

412
00:26:14,300 --> 00:26:16,470
your accent so

413
00:26:16,530 --> 00:26:18,280
they based

414
00:26:18,280 --> 00:26:22,270
this is an unnecessary moment somewhere maybe you can

415
00:26:22,290 --> 00:26:23,610
finally if you will be

416
00:26:23,640 --> 00:26:26,790
i can also find c

417
00:26:26,790 --> 00:26:29,300
this intermediate

418
00:26:29,390 --> 00:26:33,640
want the table that is story i think this block c

419
00:26:33,660 --> 00:26:35,050
so that's the

420
00:26:35,050 --> 00:26:36,460
the first

421
00:26:36,520 --> 00:26:38,930
so basically you

422
00:26:41,570 --> 00:26:45,060
but in the table and then with the

423
00:26:45,060 --> 00:26:48,290
so how how does this

424
00:26:52,730 --> 00:26:56,780
so once you may want to learn to make predictions

425
00:26:58,800 --> 00:27:02,460
associated with the current business is revealed

426
00:27:02,470 --> 00:27:08,450
y which is the label the the true label the one generated church

427
00:27:08,490 --> 00:27:10,710
it is revealed

428
00:27:10,800 --> 00:27:15,020
and so at this point the

429
00:27:15,030 --> 00:27:18,640
the learner has a chance of updating

430
00:27:18,660 --> 00:27:25,790
and the weights

431
00:27:29,190 --> 00:27:32,560
with this i mean some update rule

432
00:27:32,570 --> 00:27:37,330
which is another story

433
00:27:37,340 --> 00:27:41,160
an algorithmic description of on how to

434
00:27:41,570 --> 00:27:46,040
in in include the information

435
00:27:46,060 --> 00:27:50,740
provided by the current example by the example in the stream

436
00:27:50,750 --> 00:27:52,930
combined with the

437
00:27:52,940 --> 00:27:56,230
parents of these produced in your pocket

438
00:27:59,240 --> 00:28:05,320
no probability not sufficient statistics nothing

439
00:28:06,820 --> 00:28:11,290
it's just not that there is an important question i mean

440
00:28:11,330 --> 00:28:16,530
nothing to laugh about because we should be happy no more probability

441
00:28:27,180 --> 00:28:27,970
up now

442
00:28:28,640 --> 00:28:34,670
whatever whatever like i mean i'm not imposing any restrictions but but it's correct i

443
00:28:35,750 --> 00:28:38,970
you might object that if you

444
00:28:38,990 --> 00:28:42,600
if you're going to talk about online learning then

445
00:28:42,610 --> 00:28:45,870
you must have some bounded membership function

446
00:28:45,920 --> 00:28:48,890
i'm not making any bounded memory assumption right now

447
00:28:48,900 --> 00:28:55,770
but you will see that the article many of the that come up with

448
00:28:55,800 --> 00:29:00,980
are reasonably are reasonable in terms of memory

449
00:29:01,000 --> 00:29:02,800
they don't have to

450
00:29:02,850 --> 00:29:06,580
remember i will i'm not going to

451
00:29:06,600 --> 00:29:12,560
explain any three yellow and green which is another remember or the

452
00:29:12,620 --> 00:29:15,880
because is so what you what you could do

453
00:29:15,930 --> 00:29:17,440
you can remember the

454
00:29:17,450 --> 00:29:20,580
tactics of the stimulus is so far the the whole thing and then run at

455
00:29:20,630 --> 00:29:23,160
the money back

456
00:29:23,170 --> 00:29:29,070
this is this framework but probably you wouldn't be happy if i give you this

457
00:29:29,070 --> 00:29:30,940
is the typical on library

458
00:29:32,410 --> 00:29:36,770
there is some information about the fact that i'm not going to i shouldn't be

459
00:29:36,770 --> 00:29:38,330
cheating here

460
00:29:40,660 --> 00:29:43,290
rather give a formal definition

461
00:29:43,310 --> 00:29:48,080
a a bunch of conditions you know full to defined family and got

462
00:29:48,120 --> 00:29:54,370
this in as a as a progress in my description in my lectures so that

463
00:29:54,370 --> 00:29:58,720
the end would be should be more satisfied with the kind of argument

464
00:30:00,290 --> 00:30:04,290
then i mean if you want a things scale especially if you're using kernel then

465
00:30:04,340 --> 00:30:05,870
the thing becomes more

466
00:30:05,870 --> 00:30:10,990
a bit more it's a more subtle but at this point

467
00:30:11,710 --> 00:30:17,790
if you might think OK let's suppose that the something reasonable here

468
00:30:21,940 --> 00:30:27,370
i'm not ruling out online SVM point but them not focused my lectures them because

469
00:30:27,370 --> 00:30:29,940
it's not a good thing we can do it here

470
00:30:29,960 --> 00:30:33,510
OK so why

471
00:30:33,590 --> 00:30:35,430
now why are you

472
00:30:36,500 --> 00:30:39,430
why should we will be interested in this thing here

473
00:30:39,450 --> 00:30:47,390
the first of all i just do not be clear observed that the learner

474
00:30:55,750 --> 00:30:58,150
of classifiers

475
00:31:02,530 --> 00:31:08,140
as it is exposed to more and more of the three things so we have

476
00:31:08,140 --> 00:31:12,240
a sequence of classifiers which is the way the data sequence of

477
00:31:12,260 --> 00:31:14,020
i thought this up by the

478
00:31:14,030 --> 00:31:18,190
in the

479
00:31:18,230 --> 00:31:20,550
and then of course we should be

480
00:31:20,600 --> 00:31:24,450
it this but if you want if you want to evaluate arguments if you want

481
00:31:24,450 --> 00:31:28,150
to compare algorithms in this framework we should have some sort of some sort of

482
00:31:28,150 --> 00:31:33,210
a performance measure and there's no more risk is not worth the risk because i

483
00:31:33,210 --> 00:31:37,030
don't want to make a mistake this kind of statistical assumptions on the way to

484
00:31:37,060 --> 00:31:43,450
generate a stream in is an arbitrary object is an individual object is just produced

485
00:31:43,450 --> 00:31:46,580
by and fibronectin

486
00:31:46,630 --> 00:31:51,690
i don't probability here so i can define race in the statistical in this case

487
00:31:51,690 --> 00:31:58,280
and the only thing i have is the empirical behaviour of the algorithm on the

488
00:31:58,280 --> 00:32:01,720
specific stream it is observing

489
00:32:01,750 --> 00:32:03,030
OK so

490
00:32:03,110 --> 00:32:06,570
the only thing i can i made a reasonable thing can you use use the

491
00:32:06,570 --> 00:32:08,380
performance measure

492
00:32:08,410 --> 00:32:12,130
is the

493
00:32:12,140 --> 00:32:15,410
eleven three number

494
00:32:15,450 --> 00:32:18,890
of mistakes

495
00:32:18,930 --> 00:32:23,770
on the stream

496
00:32:27,830 --> 00:32:29,240
right so

497
00:32:29,260 --> 00:32:30,860
what is the idea here

498
00:32:30,870 --> 00:32:32,850
the idea here is that the

499
00:32:32,890 --> 00:32:33,720
we're going to

500
00:32:33,740 --> 00:32:38,690
there's something about the properties of the out degree by bounding the number

501
00:32:38,690 --> 00:32:45,700
of mistakes were made in terms of specific properties of the tree

502
00:32:45,700 --> 00:32:50,070
your model last year so the class where you're searching your model

503
00:32:51,820 --> 00:32:54,300
it's actually corresponds to reality

504
00:32:54,320 --> 00:32:59,650
so you're not really over overfitting you're not feeling come model that's more complex than

505
00:32:59,650 --> 00:33:01,300
it should

506
00:33:02,550 --> 00:33:09,670
you might have problems with overfitting but typically what love is really with active learning

507
00:33:09,670 --> 00:33:15,050
lab under fitting because you know parts of the space that are important that making

508
00:33:15,090 --> 00:33:19,260
the errors there because you that the model that's

509
00:33:19,280 --> 00:33:21,090
kind of weak

510
00:33:21,490 --> 00:33:24,800
just because you don't have much data that if you're not careful about the way

511
00:33:24,800 --> 00:33:27,410
you are doing active learning

512
00:33:27,430 --> 00:33:32,950
and i just added the notable trying to validate this active learner some hold out

513
00:33:35,430 --> 00:33:40,430
kind of difficult and somewhat at odds with the goal of active learning because the

514
00:33:40,430 --> 00:33:43,900
validation would say you're trying to get some estimate of the

515
00:33:43,950 --> 00:33:46,970
error rate of the classifier

516
00:33:46,970 --> 00:33:50,970
and i want to go to say just a random set of validation data then

517
00:33:50,970 --> 00:33:52,910
you're going to be back to

518
00:33:53,490 --> 00:33:57,990
the ability to get a handle on the areas back to the past kind performance

519
00:33:57,990 --> 00:34:01,930
level and so it's very difficult sometimes no

520
00:34:01,930 --> 00:34:04,380
we are not here active learner

521
00:34:04,380 --> 00:34:11,550
and the model produces is actually performing well you engaging its actual error performance levels

522
00:34:11,550 --> 00:34:14,010
but difficult in practice

523
00:34:14,030 --> 00:34:17,010
so that's why this error a gnostic

524
00:34:17,030 --> 00:34:20,680
procedures were trying to say well we can never know for sure

525
00:34:20,700 --> 00:34:25,820
if our active or has really area but we can sometimes make a reasonable decision

526
00:34:25,820 --> 00:34:27,610
of whether to a

527
00:34:27,610 --> 00:34:30,550
instead of some passively learned model

528
00:34:31,320 --> 00:34:36,430
focusing are sampling between those two models in their disagreement region

529
00:34:36,440 --> 00:34:40,910
that's kind of what i talked about before but actually estimating the error rate of

530
00:34:40,910 --> 00:34:45,170
of your class any classifier could be difficult because then you're back to just say

531
00:34:45,270 --> 00:34:49,430
passively safe

532
00:34:58,170 --> 00:35:00,130
right i

533
00:35:00,190 --> 00:35:09,140
no there is also in terms of the the VC dimension

534
00:35:09,140 --> 00:35:10,490
and active learning

535
00:35:10,510 --> 00:35:12,510
it doesn't really

536
00:35:12,590 --> 00:35:14,410
because of the nature of

537
00:35:16,310 --> 00:35:19,740
he buys all the it's

538
00:35:19,800 --> 00:35:22,270
he also even

539
00:35:22,280 --> 00:35:23,880
four one

540
00:35:23,890 --> 00:35:28,320
the thank all you you cannot be the it's

541
00:35:28,350 --> 00:35:32,630
at a certain point you really the concepts are sort of uniformly over

542
00:35:32,640 --> 00:35:35,570
spread over a certain threshold

543
00:35:35,670 --> 00:35:39,730
a space of some dimension you're being going to be constrained by the way you

544
00:35:39,730 --> 00:35:42,170
want to learn concept that you

545
00:35:42,190 --> 00:35:45,480
you can do and actually in the third part of the last slide that can

546
00:35:45,480 --> 00:35:56,180
illustrates this ability for regression it's not easy

547
00:35:56,350 --> 00:36:01,700
surely you agree that it is very so

548
00:36:01,730 --> 00:36:03,340
so far

549
00:36:03,360 --> 00:36:11,390
yes it should start soon

550
00:36:11,480 --> 00:36:23,300
OK so far

551
00:36:23,320 --> 00:36:24,560
no the third

552
00:36:24,570 --> 00:36:26,090
the final part of the

553
00:36:26,100 --> 00:36:27,020
this one

554
00:36:27,060 --> 00:36:28,180
will switch

555
00:36:28,200 --> 00:36:29,770
in the middle

556
00:36:31,020 --> 00:36:32,990
i'm going to start by talking

557
00:36:33,010 --> 00:36:36,950
it's about the different problem with active learning is a regression problem

558
00:36:36,980 --> 00:36:43,690
we're going to look at it may be more from a practical standpoint then from

559
00:36:43,850 --> 00:36:47,720
all just analysis trying to understand what

560
00:36:47,800 --> 00:36:49,680
but does active learning for you

561
00:36:49,890 --> 00:36:53,520
we actually have some theoretical results but

562
00:36:53,520 --> 00:36:57,730
nine inches of it is trying to find practical ways of doing this

563
00:36:59,020 --> 00:37:04,520
just to pick up where we left functional boundaries and real-world boundaries

564
00:37:05,730 --> 00:37:10,170
very different than this boundary fragments so

565
00:37:10,180 --> 00:37:13,490
what can we do not reduce the problem to one dimensional problems

566
00:37:14,380 --> 00:37:15,490
we go about it

567
00:37:15,530 --> 00:37:19,560
the problem we are going to consider here is a regression problem

568
00:37:19,610 --> 00:37:22,820
so you're trying to learn functions so you have this

569
00:37:22,840 --> 00:37:27,020
airplane here and you have a range measurement so it's like measures the distance from

570
00:37:27,030 --> 00:37:31,390
the point of the terrain and you're trying to learn this terrain the wonderland ellicott

571
00:37:31,390 --> 00:37:34,670
just their stuff like that so

572
00:37:34,680 --> 00:37:39,030
the is OK to train is sort of a kind of sport here's what here

573
00:37:39,030 --> 00:37:43,170
there's a sharp transition there so with these kinds of measurements you see that you'll

574
00:37:43,190 --> 00:37:44,890
need more samples

575
00:37:44,910 --> 00:37:46,310
to locate

576
00:37:46,310 --> 00:37:48,050
this kind of language

577
00:37:48,070 --> 00:37:50,350
two really estimate is smooth

578
00:37:50,350 --> 00:37:53,770
part of the function so in one dimensional

579
00:37:53,810 --> 00:37:57,800
in one-dimensional version of this from this function you have

580
00:37:57,810 --> 00:38:01,590
noisy samples collected nice examples CEO

581
00:38:01,640 --> 00:38:05,730
but it seems that not too much interesting stuff is going on here are here

582
00:38:05,780 --> 00:38:10,220
let me collect more samples may be in between

583
00:38:11,640 --> 00:38:16,950
hopefully if you go about this correctly you will have more samples around this

584
00:38:18,440 --> 00:38:20,400
and fewer samples

585
00:38:20,430 --> 00:38:23,880
around the smooth parts of the function

586
00:38:23,900 --> 00:38:27,730
and this is and that locating discontinuities the bottleneck

587
00:38:29,990 --> 00:38:35,100
and this example rather were shown around since cystatin that was a bunch of sensors

588
00:38:35,650 --> 00:38:37,430
are small tube

589
00:38:37,440 --> 00:38:42,990
low battery life so you don't want to use them up very much so

590
00:38:43,020 --> 00:38:44,910
can you somehow

591
00:38:44,940 --> 00:38:49,930
figure out where what has is to activate to turn on to communicate

592
00:38:49,950 --> 00:38:54,520
so that you can reliably detect this boundary of these i'll still are what's the

593
00:38:54,520 --> 00:38:57,390
oil density around around this

594
00:39:04,140 --> 00:39:06,270
so what are the rules of the game

595
00:39:06,300 --> 00:39:08,360
it's similar to what we had before

596
00:39:08,380 --> 00:39:10,560
we have a class of functions

597
00:39:10,900 --> 00:39:14,280
that's going to consider

598
00:39:14,280 --> 00:39:15,780
that's what we did this morning

599
00:39:15,800 --> 00:39:19,390
now we look at conditioning in feature space

600
00:39:19,390 --> 00:39:25,070
we already touched upon the hammersley clifford decomposition

601
00:39:27,310 --> 00:39:31,430
will be looking at conditional distributions said

602
00:39:31,520 --> 00:39:35,400
i would look at some applications

603
00:39:35,430 --> 00:39:37,190
in fact

604
00:39:37,240 --> 00:39:38,530
the nice thing is

605
00:39:38,550 --> 00:39:41,220
we start straight away with graphical models

606
00:39:41,260 --> 00:39:45,630
so one thing to

607
00:39:45,640 --> 00:39:47,610
now in this context is

608
00:39:47,620 --> 00:39:50,390
what conditional independence means

609
00:39:50,410 --> 00:39:53,680
so here's the formal definition xnx prime

610
00:39:53,700 --> 00:39:55,810
a conditionally independent

611
00:39:55,840 --> 00:39:57,680
given some very e

612
00:39:57,700 --> 00:40:01,460
if the joint probability of x and x pry occurring given c

613
00:40:01,560 --> 00:40:04,590
the same thing as the probability of x occurring given c

614
00:40:04,620 --> 00:40:07,260
the probability of ex-prime occurring giving c

615
00:40:07,320 --> 00:40:09,950
simple example

616
00:40:10,930 --> 00:40:13,450
got the light switch down here

617
00:40:13,510 --> 00:40:16,710
have got a whole bunch of light bulbs

618
00:40:16,800 --> 00:40:20,260
but turn the light switch on some of the labels marko and some of them

619
00:40:20,260 --> 00:40:21,520
might not

620
00:40:21,630 --> 00:40:24,010
some of my break

621
00:40:25,740 --> 00:40:30,820
the probability of each of those breaking probably independent of each of the other of

622
00:40:32,110 --> 00:40:36,560
so the probability of the light car lighting up

623
00:40:36,580 --> 00:40:39,670
given that i know what position the light switches in

624
00:40:39,760 --> 00:40:46,510
it is independent for each of those lightbulbs

625
00:40:48,760 --> 00:40:49,920
will be

626
00:40:49,930 --> 00:40:52,330
a simple example of that or

627
00:40:52,350 --> 00:40:55,540
another case of conditional independence

628
00:40:56,500 --> 00:40:58,270
let's assume

629
00:40:58,390 --> 00:41:03,000
well we take care of the naive view of history and we say that well

630
00:41:03,000 --> 00:41:06,860
what's going to happen tomorrow will only depend on today

631
00:41:06,880 --> 00:41:10,460
and what happened today only dependent on yesterday

632
00:41:10,470 --> 00:41:15,000
so that the words if i know about the entire state of the world today

633
00:41:15,010 --> 00:41:17,860
i don't need to know about the previous states of the world in order to

634
00:41:17,860 --> 00:41:20,690
predict the future

635
00:41:21,890 --> 00:41:25,840
so in this case and also to look back into the past i don't need

636
00:41:25,840 --> 00:41:27,160
to know the future

637
00:41:27,220 --> 00:41:32,860
so in this case well given the present past and future independent of each other

638
00:41:32,870 --> 00:41:34,760
they're conditionally independent

639
00:41:34,770 --> 00:41:37,480
if i don't know the present state exactly

640
00:41:37,500 --> 00:41:38,920
then of course

641
00:41:38,930 --> 00:41:43,880
past and future are dependent on each other so somebody talks about well the policies

642
00:41:43,880 --> 00:41:48,770
relevant for the future all saying is well we don't have full representation of the

643
00:41:48,770 --> 00:41:53,380
current state of the world and looking at the past is necessary in order to

644
00:41:53,380 --> 00:41:57,390
say something about the future

645
00:42:00,210 --> 00:42:02,340
why is this useful because

646
00:42:02,350 --> 00:42:06,830
if i have such conditional independence assumptions i can actually make the joint distribution over

647
00:42:06,830 --> 00:42:10,260
a whole bunch of random variables much easier

648
00:42:10,260 --> 00:42:10,890
is it

649
00:42:10,890 --> 00:42:19,330
greatly reduces the complexity of the space of distributions that are operating in

650
00:42:20,210 --> 00:42:21,610
markov network

651
00:42:21,630 --> 00:42:26,770
it is a way how to draw those distributions quite easily

652
00:42:26,790 --> 00:42:35,130
basically what you do is you switch from a statistics and wall essentially expression based

653
00:42:35,130 --> 00:42:40,010
representation to something that you can easily show somebody who can understand it and pass

654
00:42:40,010 --> 00:42:40,730
it and

655
00:42:40,770 --> 00:42:45,230
well in the layperson can say well you know these things will probably depend on

656
00:42:45,230 --> 00:42:47,030
each other and i'll just run amok

657
00:42:47,180 --> 00:42:51,930
so it's actually something that's good for users to model

658
00:42:51,980 --> 00:42:53,120
OK so

659
00:42:53,130 --> 00:42:54,630
markov network is

660
00:42:55,710 --> 00:42:58,760
with some vertices and edges

661
00:42:58,880 --> 00:43:02,970
eight of the vertices correspond to random variables

662
00:43:02,980 --> 00:43:08,500
and now subsets of those random variables are conditionally independent

663
00:43:09,770 --> 00:43:11,770
given some other random variables

664
00:43:11,780 --> 00:43:17,400
if removing the vertices from this graph decompose the graph into disjoint subsets containing is

665
00:43:17,410 --> 00:43:18,430
in his prime

666
00:43:18,440 --> 00:43:21,110
well that sounds like quite a mouthful

667
00:43:21,120 --> 00:43:23,990
let's have a look at some examples

668
00:43:24,010 --> 00:43:26,190
so here's

669
00:43:28,500 --> 00:43:30,930
graphical model is another one

670
00:43:30,960 --> 00:43:32,030
that would be

671
00:43:32,050 --> 00:43:34,120
the thing about the past and the future

672
00:43:34,160 --> 00:43:36,500
if i conditioned on

673
00:43:36,540 --> 00:43:37,830
the present

674
00:43:37,880 --> 00:43:39,780
remove this

675
00:43:40,520 --> 00:43:44,390
this change and and that change decompose

676
00:43:44,390 --> 00:43:47,240
so in other words there's only a certain place way

677
00:43:48,010 --> 00:43:49,750
this set of random variables here

678
00:43:49,760 --> 00:43:53,350
can influence the otherwise it has to go through this one

679
00:43:53,360 --> 00:43:57,330
and i'll be exploiting this in the article

680
00:43:57,340 --> 00:43:58,330
or here

681
00:43:58,470 --> 00:44:00,500
but the slightly more fancy model

682
00:44:00,550 --> 00:44:02,670
in that case well

683
00:44:02,720 --> 00:44:04,870
conditioning on those right notes here

684
00:44:04,880 --> 00:44:08,490
makes it the rest of the graph decompose

685
00:44:08,530 --> 00:44:11,540
if i didn't condition on that one

686
00:44:11,630 --> 00:44:15,390
then the rest will not be conditionally indepen

687
00:44:15,400 --> 00:44:16,970
but you can see is

688
00:44:17,010 --> 00:44:21,910
this is probably quite the an important random variable to look at

689
00:44:21,970 --> 00:44:25,250
is this really separates using this happening here

690
00:44:25,260 --> 00:44:27,590
everything that's happening over there

691
00:44:27,610 --> 00:44:32,330
so what is also indicates that if afterwards they want to design an efficient algorithm

692
00:44:32,350 --> 00:44:34,630
i should probably take advantage of this

693
00:44:34,630 --> 00:44:38,760
but this will be my conduit through which a lot of the inference information is

694
00:44:38,760 --> 00:44:42,460
channeled through in both directions

695
00:44:43,850 --> 00:44:47,410
but it will also mean that if this variable is only very simple one it

696
00:44:47,410 --> 00:44:51,520
can only takes a states one and zero is not so much information that can

697
00:44:51,520 --> 00:44:54,690
go either way i can exploit this

698
00:44:55,630 --> 00:44:59,590
this is probably a fairly complicated scenario which

699
00:45:01,440 --> 00:45:06,350
transfer information from the live far to the right one

700
00:45:07,690 --> 00:45:11,350
will get to equations to make the distilled with one standard

701
00:45:15,310 --> 00:45:18,790
key concept in this context of cliques

702
00:45:19,020 --> 00:45:20,420
so once i got my graph

703
00:45:22,380 --> 00:45:24,650
define a plague to be

704
00:45:24,670 --> 00:45:28,910
a subset of vertices which are fully connected with each other

705
00:45:28,960 --> 00:45:30,330
prince instance this pair

706
00:45:30,340 --> 00:45:32,790
i fully connected with each other

707
00:45:32,800 --> 00:45:34,910
the green one here

708
00:45:34,930 --> 00:45:39,580
this fully connected with each other all four nodes have a stall

709
00:45:39,600 --> 00:45:40,810
the other three members

710
00:45:40,830 --> 00:45:42,020
in their place

711
00:45:42,040 --> 00:45:44,170
those three are fully connected

712
00:45:44,180 --> 00:45:46,100
OK now

713
00:45:47,560 --> 00:45:55,760
who can tell me the other to maximal cliques in this graph

714
00:45:56,170 --> 00:45:57,920
any volunteers

715
00:45:58,230 --> 00:46:00,410
you know that

716
00:46:00,420 --> 00:46:02,240
it's very good

717
00:46:02,250 --> 00:46:05,330
and yet

718
00:46:05,350 --> 00:46:06,890
very good

719
00:46:06,990 --> 00:46:12,330
so we have four five maximal cliques

720
00:46:16,360 --> 00:46:20,680
o thing is as it will turn out the maximal cliques are exactly those quantities

721
00:46:21,000 --> 00:46:22,620
that will play a role

722
00:46:22,710 --> 00:46:25,040
in performing inference

723
00:46:25,130 --> 00:46:28,100
because they also essentially the

724
00:46:28,110 --> 00:46:33,340
ensembles of random variables that you can't really introduce very much anymore

725
00:46:33,440 --> 00:46:38,660
and what people typically what do is you would write down a tree of cliques

726
00:46:38,720 --> 00:46:44,030
and how they're connected with the separate estates being the points which those cliques have

727
00:46:44,030 --> 00:46:44,430
down here

728
00:46:45,080 --> 00:46:47,040
and then it flattens out like

729
00:46:48,660 --> 00:46:50,350
and so that's what i knew

730
00:46:51,520 --> 00:46:53,100
eighty inland actually looks like

731
00:46:54,930 --> 00:47:00,040
and now the suggestion is take all the data take your one thousand points that you've measured to almost precision

732
00:47:00,800 --> 00:47:02,950
and throw away all the precision and then just up

733
00:47:05,240 --> 00:47:05,670
and people like

734
00:47:06,250 --> 00:47:09,310
why did you completely new thrown away lot information there

735
00:47:09,760 --> 00:47:12,170
we might worry and you have access to information

736
00:47:14,560 --> 00:47:15,340
but you could do it

737
00:47:15,750 --> 00:47:16,160
and then

738
00:47:16,760 --> 00:47:18,290
you've gotta do something with this curve here

739
00:47:20,080 --> 00:47:22,950
so are you work out what the actual data mining is

740
00:47:23,690 --> 00:47:24,830
they bring along x by

741
00:47:26,710 --> 00:47:29,430
we find the actual mean what they call x

742
00:47:33,640 --> 00:47:35,670
which are not defining be the sum of its own

743
00:47:37,660 --> 00:47:40,620
you bring along a news through this line here

744
00:47:41,190 --> 00:47:44,400
plot the downstairs and he said okay there is an estimator

745
00:47:45,110 --> 00:47:45,640
the lambda

746
00:47:46,840 --> 00:47:47,420
what you had in mind

747
00:47:48,690 --> 00:47:49,390
i like the idea

748
00:47:50,350 --> 00:47:51,610
because that's you know that's

749
00:47:52,120 --> 00:47:53,280
i don't have any beans in it

750
00:47:54,090 --> 00:47:56,800
but it did involve a very big step and say i'm going to compute the mean

751
00:47:57,420 --> 00:47:58,450
and you could have said

752
00:47:59,080 --> 00:48:03,340
i'm going to compute the variance or i'm going compute fifth moment or you could

753
00:48:03,340 --> 00:48:05,190
be in the whole load other things to measure

754
00:48:05,680 --> 00:48:07,220
and then you could plot then

755
00:48:08,810 --> 00:48:09,950
the function lambda so

756
00:48:10,560 --> 00:48:11,220
another thing

757
00:48:13,980 --> 00:48:15,590
could be plotted as a function of lambda

758
00:48:16,680 --> 00:48:16,940
like the

759
00:48:18,210 --> 00:48:21,270
and then you get your data and you come down and they could be anybody's three

760
00:48:21,750 --> 00:48:22,360
so is

761
00:48:23,310 --> 00:48:26,470
and now an infinite family of things that you could do i think this is

762
00:48:26,470 --> 00:48:28,380
a brilliant idea and you'll see why in a moment

763
00:48:29,850 --> 00:48:33,130
there is an infinite number of things you could invent which depend on lambda

764
00:48:34,190 --> 00:48:34,730
and you could

765
00:48:35,430 --> 00:48:39,800
pick one of those and then measure it and using it to come back and get an estimate of lambda

766
00:48:41,850 --> 00:48:44,250
that's another thing that is done in classical statistics

767
00:48:45,010 --> 00:48:47,090
hand it can often be done well

768
00:48:48,230 --> 00:48:52,230
hand you can also get in trouble because what if the three data points as they

769
00:48:52,960 --> 00:48:53,390
is being

770
00:48:53,880 --> 00:48:54,510
what if they were

771
00:48:57,450 --> 00:48:58,170
it could happen right

772
00:48:58,830 --> 00:48:59,880
you only three points

773
00:49:00,480 --> 00:49:02,050
hand there are also the writer

774
00:49:04,450 --> 00:49:04,970
the midpoint

775
00:49:05,720 --> 00:49:08,490
and i work out the mean and it's above the midpoint a baby

776
00:49:09,970 --> 00:49:12,120
and i don't know what to do because you methods said

777
00:49:12,600 --> 00:49:14,650
come across from the data value and return

778
00:49:15,220 --> 00:49:16,600
and i haven't got an answer any more

779
00:49:20,240 --> 00:49:23,300
there's lots of ways to solve problems the wrong way

780
00:49:23,980 --> 00:49:29,160
i'm reminded independent television here where penn and teller a sort study a controversial topic

781
00:49:30,780 --> 00:49:31,690
it's nuclear-power

782
00:49:33,020 --> 00:49:37,160
hand they interview who who has also reasons for being anti-nuclear and they say we

783
00:49:37,160 --> 00:49:41,410
believe in balance on our show so we give a chance to everybody you express

784
00:49:41,590 --> 00:49:43,970
your code from the area now let's hear from look guys right

785
00:49:45,960 --> 00:49:46,690
let's now

786
00:49:47,220 --> 00:49:51,610
solve this inference problem using bayes theorem and there will be a unique answer

787
00:49:52,420 --> 00:49:55,710
will be one answer to the problem and it will work in any case with

788
00:49:56,010 --> 00:49:59,930
any case any metadata three data points three thousand data points

789
00:50:00,460 --> 00:50:01,870
and you don't actually need to think

790
00:50:02,540 --> 00:50:07,640
you get the right answer you don't need that she's been sizes you don't need to invent measures the goodness-of-fit

791
00:50:08,690 --> 00:50:12,420
you don't need to invent interesting functions like the mean of the data

792
00:50:12,900 --> 00:50:14,350
which in this case is a brilliant idea

793
00:50:16,680 --> 00:50:17,460
but in other problems

794
00:50:18,690 --> 00:50:20,530
you might struggle to think of the brilliant idea

795
00:50:23,720 --> 00:50:27,520
with base you don't actually need to think that you need to know what you're doing

796
00:50:28,400 --> 00:50:29,100
you get it right

797
00:50:29,920 --> 00:50:31,050
but it is actually mechanical

798
00:50:34,270 --> 00:50:35,190
it doesn't take years

799
00:50:35,810 --> 00:50:37,350
get used to doing things the bayesian way

800
00:50:39,230 --> 00:50:40,190
so how does it work

801
00:50:44,440 --> 00:50:48,500
o has a list of other ideas that people like came come up with i think we've gone through them

802
00:50:49,670 --> 00:50:54,210
often when i asked the question how would you solve this problem people say k i squared

803
00:50:54,990 --> 00:50:58,890
and that's goes back to the beans story have counting the bin

804
00:50:59,370 --> 00:51:03,520
and then measure the difference between the current and expected count using a single kai squared

805
00:51:04,280 --> 00:51:04,860
so that's

806
00:51:05,430 --> 00:51:07,970
we already have that's roughly in methods one and two

807
00:51:10,200 --> 00:51:12,100
measures the error between the theory and data

808
00:51:12,950 --> 00:51:13,790
look at the data mining

809
00:51:14,800 --> 00:51:19,670
and see how the very lambda all pick some other quantity and see how that's very lambda

810
00:51:20,140 --> 00:51:22,060
and okay so that's all the ideas

811
00:51:22,770 --> 00:51:25,300
that are out there and i will do it using bayes theorem

812
00:51:26,190 --> 00:51:30,030
so let's write down again what the probability of the data is

813
00:51:31,610 --> 00:51:32,520
for a single data point

814
00:51:32,880 --> 00:51:35,660
given lambda and assuming that there is one exponential

815
00:51:35,660 --> 00:51:40,590
so the two of them are of the opinion of the euclidean norm

816
00:51:40,590 --> 00:51:42,720
however because the one

817
00:51:42,720 --> 00:51:45,110
so in the perceptron case i have identity

818
00:51:45,130 --> 00:51:47,450
and the inverse of identity which

819
00:51:47,470 --> 00:51:50,110
but if i increase the

820
00:51:50,160 --> 00:51:52,510
the is that the change

821
00:51:52,760 --> 00:51:54,490
so now it

822
00:51:54,510 --> 00:51:59,880
it's interesting to ask what if i take p

823
00:52:01,430 --> 00:52:03,650
what with things change

824
00:52:03,660 --> 00:52:08,380
and i can improve different properties with respect to the property that proven for the

825
00:52:08,430 --> 00:52:11,630
but that's not great

826
00:52:13,430 --> 00:52:17,510
i want to be a little bit of machinery for the analysis

827
00:52:17,530 --> 00:52:24,780
these properties of running experiments not just analytical tools to understand in their it's i

828
00:52:24,780 --> 00:52:27,360
just need one two

829
00:52:27,360 --> 00:52:29,840
the main goal would be in a special

830
00:52:29,860 --> 00:52:33,720
data there

831
00:52:45,300 --> 00:52:46,650
the gradient

832
00:52:52,360 --> 00:52:58,200
integrating vector in the

833
00:52:58,200 --> 00:53:03,010
all a

834
00:53:03,050 --> 00:53:07,430
i don't know if a the vector is a vector from the vector valued function

835
00:53:07,430 --> 00:53:09,410
from the product

836
00:53:09,430 --> 00:53:10,240
the great

837
00:53:10,260 --> 00:53:12,320
mapping vectors in vector

838
00:53:12,360 --> 00:53:16,990
i'm a bit in a vector are are the vector

839
00:53:17,050 --> 00:53:19,910
are the third and and

840
00:53:19,950 --> 00:53:24,450
now i am getting coming and getting a gradient which is the vector of possibilities

841
00:53:26,660 --> 00:53:30,820
they should get the support of s is really valued then begin about the great

842
00:53:30,930 --> 00:53:33,340
the potential at the

843
00:53:36,130 --> 00:53:39,430
that the this thing leaves in rd

844
00:53:39,430 --> 00:53:41,800
these things need not be

845
00:53:41,840 --> 00:53:47,260
so on computing the the the gradient vector of partial derivatives with respect to the

846
00:53:48,410 --> 00:53:50,590
and then a mapping and this

847
00:53:50,630 --> 00:53:57,240
so the gradient is an operator that maps vector are important factors of partial derivatives

848
00:53:57,240 --> 00:53:59,680
in the the inverse mapping

849
00:53:59,720 --> 00:54:03,840
it's me again from back one of the vector which is NOT to be

850
00:54:03,840 --> 00:54:07,360
the gradient of the dual protection

851
00:54:07,450 --> 00:54:09,990
believe me

852
00:54:10,010 --> 00:54:15,820
it's it's it's a stand-up comics analysis nothing more than that i mean

853
00:54:17,320 --> 00:54:19,610
but just use it in

854
00:54:19,660 --> 00:54:21,030
you know in

855
00:54:21,050 --> 00:54:24,320
algorithmic context so

856
00:54:24,390 --> 00:54:28,240
it's very easy i want to just use theorem

857
00:54:28,240 --> 00:54:31,630
so i'm going to expand i'm going to to look at the evolution of the

858
00:54:31,630 --> 00:54:34,070
potential as

859
00:54:34,110 --> 00:54:35,800
time goes by

860
00:54:35,820 --> 00:54:38,280
so i'm looking at the evolution of these

861
00:54:38,300 --> 00:54:39,360
which is

862
00:54:39,360 --> 00:54:45,820
and let me writing gender breakdown potential which is something that

863
00:54:45,820 --> 00:54:49,530
it's already analysis of perception but under the guise now

864
00:54:49,550 --> 00:54:52,860
i'm thinking

865
00:54:55,570 --> 00:55:02,840
OK there are human so that the expand this around these

866
00:55:02,880 --> 00:55:06,660
i get inequality were i have the gradient

867
00:55:08,550 --> 00:55:12,130
at RT minus one

868
00:55:13,130 --> 00:55:18,880
are the mining are the main one with this is the vector in the product

869
00:55:18,930 --> 00:55:20,970
and the last

870
00:55:22,110 --> 00:55:24,050
of art a

871
00:55:24,050 --> 00:55:25,510
that's right you know

872
00:55:25,510 --> 00:55:29,030
i the minus of the minus one

873
00:55:30,490 --> 00:55:33,990
and then let's write this is the passion magic

874
00:55:34,030 --> 00:55:36,200
of phi

875
00:55:36,240 --> 00:55:38,570
evaluate the certain point

876
00:55:38,590 --> 00:55:42,780
are by our lives on the line between the art one

877
00:55:42,840 --> 00:55:46,130
and and i have

878
00:55:47,610 --> 00:55:50,430
but this is the last theorem four

879
00:55:50,800 --> 00:55:54,990
the vector functional the defines function defined on vector

880
00:55:57,320 --> 00:55:59,280
OK nothing nothing more than that

881
00:56:03,590 --> 00:56:06,240
now i know something interesting

882
00:56:08,660 --> 00:56:14,840
not that there is that there are here i can i can spot

883
00:56:15,010 --> 00:56:18,050
i can stop and think about their work

884
00:56:18,070 --> 00:56:19,860
provided i

885
00:56:19,880 --> 00:56:23,990
at some unknown point that is in the line between the two point

886
00:56:25,070 --> 00:56:30,680
in fact good now but but it's important that everything

887
00:56:30,680 --> 00:56:32,950
so it's quality

888
00:56:33,990 --> 00:56:36,450
now there's something interesting here is going on

889
00:56:36,490 --> 00:56:38,820
so let's look at this the

890
00:56:39,590 --> 00:56:43,490
so we would like to bound the guy this is again the caption also like

891
00:56:43,490 --> 00:56:47,780
about the upper bound but let's look at this guy over there

892
00:56:48,320 --> 00:56:49,930
what is the

893
00:56:49,990 --> 00:56:50,970
this is

894
00:56:50,970 --> 00:56:52,380
well that's right it

895
00:56:54,150 --> 00:57:02,990
so we get something nicer so we get this is by definition

896
00:57:03,050 --> 00:57:04,630
w minus one

897
00:57:04,680 --> 00:57:07,820
i find it like that that be minus one

898
00:57:07,840 --> 00:57:10,740
what is the difference between

899
00:57:10,800 --> 00:57:11,590
up there

900
00:57:14,950 --> 00:57:19,220
x the indicator function of

901
00:57:19,970 --> 00:57:23,470
he had a different from white

902
00:57:23,530 --> 00:57:26,340
so if a mistake was made this is the wrong

903
00:57:26,360 --> 00:57:27,930
but this is the right

904
00:57:27,950 --> 00:57:30,110
if a mistake was made and the

905
00:57:30,130 --> 00:57:32,930
and this is the guy that made a mistake for this

906
00:57:33,010 --> 00:57:36,880
thing is negative

907
00:57:37,820 --> 00:57:39,820
o thing

908
00:57:39,840 --> 00:57:42,740
so what you have to remember is the set

909
00:57:42,800 --> 00:57:46,030
is the new of this guy that this guy

910
00:57:49,240 --> 00:57:53,510
so that's all with this will be zero so i i get

911
00:57:54,180 --> 00:57:55,450
excluding these

912
00:57:57,090 --> 00:58:00,320
i get by recursively

913
00:58:01,090 --> 00:58:03,110
the current i get

914
00:58:05,410 --> 00:58:07,160
are you

915
00:58:07,240 --> 00:58:09,470
is one of the liquid then y

916
00:58:09,490 --> 00:58:10,590
at zero

917
00:58:10,610 --> 00:58:12,760
which is the initial thing

918
00:58:12,800 --> 00:58:15,340
the zero vector

919
00:58:17,410 --> 00:58:18,800
the some of the second

920
00:58:21,780 --> 00:58:24,570
let's take this

921
00:58:25,780 --> 00:58:27,470
or the

922
00:58:33,590 --> 00:58:38,490
what is the point that i'm getting back on this guy here and then getting

923
00:58:38,490 --> 00:58:41,800
a lower bound on this guy in terms of the state

924
00:58:41,820 --> 00:58:45,590
just like in the perceptron and then i get to prove mistake bound

925
00:58:45,610 --> 00:58:47,720
four to professional now

926
00:58:47,760 --> 00:58:50,800
second of the first bombings and something more

927
00:58:50,800 --> 00:58:53,450
debate about the structure of the pension so

928
00:58:53,470 --> 00:58:54,820
about the

929
00:58:54,820 --> 00:58:56,700
one the second term

930
00:58:56,720 --> 00:59:00,280
which for those who know is also bregman divergence

931
00:59:00,300 --> 00:59:01,680
i need to all

932
00:59:01,680 --> 00:59:06,320
i saw something about the potential so we do it for the real potential

933
00:59:06,340 --> 00:59:07,970
i mean i want to do

934
00:59:07,990 --> 00:59:10,610
the proof of bounding the second order

935
00:59:10,660 --> 00:59:15,890
per minute there special because it's boring but just give the is that it's easy

936
00:59:19,740 --> 00:59:29,680
when we choose one

937
00:59:29,680 --> 00:59:36,490
intend to be seriously depended dimension dependent getting worse of course as dimension increases now

938
00:59:36,490 --> 00:59:41,210
this second point may be less true of typical machine learning problems but certenatly in in statistic we're very

939
00:59:41,210 --> 00:59:47,310
often modeling data almost as a as a one-off and we we need

940
00:59:47,310 --> 00:59:52,890
numerical methods that are easy to use on a new problem and don't require a lot

941
00:59:52,890 --> 00:59:58,390
of a lot of you know life-time experimentation in exactly how to how to fit this

942
00:59:58,390 --> 01:00:03,350
model and the last point is a little bit tongue in cheek I don't really

943
01:00:03,350 --> 01:00:08,790
bel believe it but I think there is a bit of a fetish about supreme

944
01:00:08,790 --> 01:00:17,230
precision in in computing with data and I mean we're essentially modeling uncertainty in data we know

945
01:00:17,230 --> 01:00:24,110
that the data we're using the numbers could've  been different and

946
01:00:24,110 --> 01:00:32,040
wear that mathematical hats on I think I better restart this in the coffee break and without mathematical hats

947
01:00:32,220 --> 01:00:38,170
on anything less than the best possible precision is something perhaps we don't really

948
01:00:38,170 --> 01:00:44,350
want to contemplate but we should remember that with the sophisticated numerical methods we've have

949
01:00:44,350 --> 01:00:51,870
available now we may be computing inferences more accurately than the data really support

950
01:00:51,870 --> 01:00:58,490
okay typically in Bayesian statistics these are the methods that have been used over the years

951
01:00:58,510 --> 01:01:06,010
I mean quadrature classical methods of the numerical integration asimptothick methods that typically arise using

952
01:01:06,010 --> 01:01:11,830
approximations based on the plus and so on and what really made a difference to the

953
01:01:11,830 --> 01:01:21,370
usability of Bayesian inference is using simulation for computation simulation where essentially we are

954
01:01:21,370 --> 01:01:26,310
therefore at some level doing Monte Carlo integration but usually not explicitly setting out to

955
01:01:26,310 --> 01:01:31,910
do an integral but but doing a simulation calculation that effectively performs an integral for us

956
01:01:31,930 --> 01:01:37,370
and you've seen you've seen four and a half hours of that of that being done earlier this week

957
01:01:37,990 --> 01:01:44,370
and and now direct simulations using rejection sampling or or or or or whatever has some

958
01:01:44,370 --> 01:01:49,670
use but quite limited but what's really made a difference to apply Bayesian statistics

959
01:01:49,670 --> 01:01:57,410
is developer of the MCMC Markov Chain Monte Carlo I'll say very little more about that

960
01:01:57,410 --> 01:02:02,290
something that would a lot of us hopped would be really useful and has been

961
01:02:02,300 --> 01:02:09,290
kind of quite disappointing is the idea of perfect simulation is that is that an area that's

962
01:02:09,290 --> 01:02:16,140
made much impact in machine learning have you heard of it coupling from the past that the that

963
01:02:16,140 --> 01:02:22,850
it turns out there is a way of organizing the computations when you simulate a Markov Chain

964
01:02:22,850 --> 01:02:30,090
so that the mark so the realization itself effectively declares when it reached equilibrium so it's

965
01:02:30,090 --> 01:02:35,870
like a bit of magic this happens after a random time random matter computation and there was

966
01:02:35,870 --> 01:02:41,340
a beautiful idea due to problem Wilson called coupling from the past which which set that

967
01:02:41,470 --> 01:02:46,850
up it's like a mixture of discrete mathematics and algorithms and probability very

968
01:02:46,850 --> 01:02:50,590
nice paper and a lot of us were hoping that would you know

969
01:02:50,590 --> 01:02:55,930
let give us a new I think a new generation of simulation methods that

970
01:02:55,930 --> 01:03:00,170
will be usable in statistics this kind of been a bit disappointing and I think

971
01:03:00,170 --> 01:03:05,740
that the the the problem is that perfect simulation methods turn out not to be easily

972
01:03:05,740 --> 01:03:11,070
generated from the problem itself from the model and the data it has to be something we

973
01:03:11,070 --> 01:03:15,370
have to analyze an algorithm and therefore it doesn't fit in well with this kind

974
01:03:15,370 --> 01:03:21,470
of one-off motive of computing but I mean well are two success stories and it's realy reather a

975
01:03:21,580 --> 01:03:26,190
nice idea that this completely solves the problem well at least it I say completely solves the

976
01:03:26,190 --> 01:03:32,710
problem of knowing if you converged or not it it may still take a long time

977
01:03:32,710 --> 01:03:39,030
to to get to that point and then more recent more recent innovations in

978
01:03:39,040 --> 01:03:45,250
in sampling technology for Bayesian statistics of course include the particle filters that you've heard about yesterday and approximate Bayesian

979
01:03:45,250 --> 01:03:51,890
computation which which is a method or class of methods for doing

980
01:03:51,890 --> 01:03:59,830
Bayesian inference numerically using simulation where you can't write down the likelihood function algebraically but you can

981
01:03:59,890 --> 01:04:07,630
only simulate from the corresponding model and then there's a huge area of variational inference variational

982
01:04:07,630 --> 01:04:13,680
I put approximations in quotes there I'll say a word or two in about a moment so just

983
01:04:13,680 --> 01:04:20,110
very briefly on assimilation in general this are still as far as Bayesian statisticians

984
01:04:20,220 --> 01:04:27,130
concern this are still really the the the the main work holes of applied work and

985
01:04:27,130 --> 01:04:33,310
with the obvious pluses and minuses that that that that you're aware of the fact

986
01:04:33,310 --> 01:04:38,090
that the order of the rationship between the amount of computing you do in the

987
01:04:38,090 --> 01:04:45,330
precision of your answer is effectively independent of dimension is of course fantastic though a pretty invading in quotes

988
01:04:45,330 --> 01:04:52,110
because of course there's a large multiplier that depends on dimension and really the total flexibility

989
01:04:52,120 --> 01:04:58,970
of it is is is being fantastic it's realy liberated statisticians to to to to use

990
01:04:58,970 --> 01:05:03,570
the models they think are appropriate not models for which there is a tractable numerical

991
01:05:03,570 --> 01:05:11,230
method I've spent a little bit of time yesterday talking about loss functions and

992
01:05:11,240 --> 01:05:17,830
you you you might be curious to to know how loss functions inter relate with

993
01:05:17,830 --> 01:05:25,050
Bayesian computation in particular computation with with sampling and this is potentially a little

994
01:05:25,050 --> 01:05:30,870
bit of a problem because if you're doing if you're setting up an inference problem estimation hypothesis testing

995
01:05:30,870 --> 01:05:35,710
or whatever it is if you are setting it up by decision theory with a loss function then we seem

996
01:05:35,710 --> 01:05:40,250
to want to optimize something as well as integrate and we we obviously don't want

997
01:05:40,250 --> 01:05:47,230
to get into a position of iteratively optimizing over something with that something is itself

998
01:05:47,250 --> 01:05:56,850
along numerical computation but it turns out that some loss functions separat in a rather convenient

999
01:05:56,850 --> 01:06:03,830
way it essentially soolves this problem and you you you you kind of know that such things must

1000
01:06:03,830 --> 01:06:08,350
be possible because we know we can compute the posterior expectation numerically by just averaging

1001
01:06:08,350 --> 01:06:15,030
a longer run okay so that's clearly example of loss function based inference that doesn't involve

1002
01:06:15,030 --> 01:06:21,890
an iteration between optimization and integration and that the general and the general paradigm in

1003
01:06:21,890 --> 01:06:26,110
which that works is where the loss function seperates like this into a sum

1004
01:06:26,110 --> 01:06:29,670
which in number of terms a function of decision times a function of

1005
01:06:29,670 --> 01:06:35,270
the parameter and when it has that property because of linearity of expectation and so on

1006
01:06:35,270 --> 01:06:42,870
the posterior expectation of the loss then separates neatly and we essentially have an

1007
01:06:42,870 --> 01:06:55,270
average weighted average of capital R different sample averages from the run and I'll show you

1008
01:06:55,270 --> 01:07:02,190
one or two examples in the second talk today where that where that where that works

1009
01:07:02,190 --> 01:07:07,100
so i such situations you can separate the optimization from the from Monte Carlo simulation

1010
01:07:07,100 --> 01:07:16,090
rather neatly now you may be surprised that I didn't mention anything about message passing

1011
01:07:16,090 --> 01:07:23,450
or or or such algorithms in my initial list of algorithms and but I'll just insert a

1012
01:07:23,450 --> 01:07:28,870
few comments about that that that the reason they're not mainstream in Bayesian statistics

1013
01:07:28,870 --> 01:07:30,290
and four

1014
01:07:30,300 --> 01:07:32,310
two two people don't like it

1015
01:07:32,330 --> 01:07:34,790
don't like now obviously can go further

1016
01:07:34,810 --> 01:07:37,700
to one battery life one of the

1017
01:07:37,720 --> 01:07:39,290
the rest of the same and do

1018
01:07:39,350 --> 01:07:40,730
i mean if you want to

1019
01:07:40,740 --> 01:07:44,270
two of visualization

1020
01:07:44,310 --> 01:07:49,260
you can do pass negative by budget departure

1021
01:07:49,280 --> 01:07:51,220
and today if you want to compare

1022
01:07:51,240 --> 01:07:53,160
different digital cameras

1023
01:07:53,220 --> 01:07:55,040
and then you know which one

1024
01:07:55,260 --> 01:08:00,630
trains so this is basically the possibility of a and a negative balance of each

1025
01:08:00,630 --> 01:08:03,650
individual attributes attributes

1026
01:08:03,700 --> 01:08:05,400
so this is actually

1027
01:08:05,420 --> 01:08:06,990
this model has been

1028
01:08:07,650 --> 01:08:13,690
as we actually actually actions somewhere you probably know simply don't know microsoft live search

1029
01:08:13,860 --> 01:08:15,220
have this can be

1030
01:08:15,230 --> 01:08:16,850
let's search this thing

1031
01:08:16,860 --> 01:08:18,530
if you go down to

1032
01:08:18,540 --> 01:08:20,010
so some products

1033
01:08:20,780 --> 01:08:21,950
the summary is here

1034
01:08:26,760 --> 01:08:30,910
we're talking about lexington based approach and we want to do

1035
01:08:30,970 --> 01:08:32,970
be better than that

1036
01:08:32,980 --> 01:08:35,420
the previous being has been done

1037
01:08:35,480 --> 01:08:38,140
so let's bayes approach essentially says

1038
01:08:38,150 --> 01:08:40,270
you are using

1039
01:08:42,280 --> 01:08:43,900
the opinion words

1040
01:08:43,940 --> 01:08:46,750
OK you are using the opinion words to detect

1041
01:08:46,780 --> 01:08:49,380
what a particular opinions also connected

1042
01:08:52,770 --> 01:08:54,150
so for example you can say

1043
01:08:54,170 --> 01:08:58,170
the battery life and picture quality are great but the view founder small so you

1044
01:08:58,170 --> 01:09:00,310
can see this

1045
01:09:00,380 --> 01:09:06,580
tell you something that something so i the lexicon lexicon based approach because obviously this

1046
01:09:06,610 --> 01:09:08,620
and what for you to express opinions

1047
01:09:08,630 --> 01:09:10,980
that was

1048
01:09:12,880 --> 01:09:16,440
this fact approach so what is what is basically the process of this approach is

1049
01:09:16,440 --> 01:09:19,310
very simple so say i'm just trying to identify

1050
01:09:19,370 --> 01:09:22,360
the words in the sentence which is

1051
01:09:22,370 --> 01:09:23,900
i have some opinions site

1052
01:09:23,910 --> 01:09:28,490
and they can somehow aggregate them as the aggregate them

1053
01:09:28,540 --> 01:09:34,300
so what are the positive neck towards the positive example beautiful wonderful amazing good amazing

1054
01:09:35,020 --> 01:09:37,100
the band were that things will be

1055
01:09:37,110 --> 01:09:40,900
in a lot of things and also idioms phrases

1056
01:09:40,950 --> 01:09:43,780
so this for obvious reasons very useful

1057
01:09:45,360 --> 01:09:47,400
so there's there's two ways of

1058
01:09:47,410 --> 01:09:48,830
getting this bunch works

1059
01:09:48,850 --> 01:09:52,370
OK good friends was no i don't have to do with this research is really

1060
01:09:53,860 --> 01:09:55,190
who said of this work

1061
01:09:55,250 --> 01:09:58,890
so the first approaches is dictionary based on you do

1062
01:09:58,900 --> 01:10:00,380
use one it

1063
01:10:00,390 --> 01:10:02,300
and they also can use the covers

1064
01:10:02,320 --> 01:10:05,280
so the problem the difficult problem is this

1065
01:10:05,320 --> 01:10:07,740
the opinion some of opinion words

1066
01:10:07,750 --> 01:10:09,760
context dependent

1067
01:10:09,770 --> 01:10:11,020
OK which means

1068
01:10:11,050 --> 01:10:13,290
depends on how the guys using

1069
01:10:13,400 --> 01:10:15,630
i was going to use it

1070
01:10:15,700 --> 01:10:21,360
for example something is along sometimes smaller short what i mean

1071
01:10:21,370 --> 01:10:23,860
the i

1072
01:10:23,910 --> 01:10:27,860
so the dictionary based approaches actually you get abundances you will do

1073
01:10:27,870 --> 01:10:30,120
romney around and the go to the warning

1074
01:10:30,140 --> 01:10:31,720
in one

1075
01:10:31,770 --> 01:10:35,220
so the advantage of course this approach is you can really get a lot

1076
01:10:35,230 --> 01:10:36,700
really get a lot

1077
01:10:36,750 --> 01:10:37,610
lots of them

1078
01:10:37,760 --> 01:10:39,170
their weaknesses

1079
01:10:39,180 --> 01:10:43,310
you can't use text in the text dependent opinions thanks

1080
01:10:43,410 --> 01:10:46,280
context dependent context dependent opinions

1081
01:10:46,360 --> 01:10:50,770
because one does not have the

1082
01:10:50,810 --> 01:10:52,870
so corpus based

1083
01:10:53,320 --> 01:10:57,250
again we use a set of keywords set of words start with

1084
01:10:57,280 --> 01:11:01,890
was something which is obviously have good opinions either positive or negative

1085
01:11:01,900 --> 01:11:04,290
and they have a big corpus do

1086
01:11:04,390 --> 01:11:05,980
so to some machine learning

1087
01:11:06,020 --> 01:11:08,790
OK tagging dimension and

1088
01:11:08,870 --> 01:11:10,110
so you need

1089
01:11:10,120 --> 01:11:13,830
so there's this approach is essential to help you define domain

1090
01:11:13,850 --> 01:11:16,190
in dependent domain dependent

1091
01:11:16,190 --> 01:11:20,120
so what we see that domain dependent is not actually good enough

1092
01:11:20,190 --> 01:11:21,580
not good enough

1093
01:11:21,780 --> 01:11:25,780
before talk about that so what is the rule people normally use

1094
01:11:25,790 --> 01:11:29,630
OK what do and use so this a bunch of rules was

1095
01:11:30,080 --> 01:11:32,240
so the proposed nineteen ninety seven

1096
01:11:32,280 --> 01:11:37,020
so what that hammond was studying adjectives what kind of additives policy will converge to

1097
01:11:37,020 --> 01:11:38,360
the negative

1098
01:11:38,370 --> 01:11:41,960
so this is called conjunction is very interesting

1099
01:11:42,000 --> 01:11:44,100
so the car is beautiful and spacious

1100
01:11:44,170 --> 01:11:47,960
for example this this and this and if we know something if we not beautiful

1101
01:11:47,960 --> 01:11:49,020
is positive

1102
01:11:49,080 --> 01:11:52,030
we're pretty sure spacious positive two

1103
01:11:52,860 --> 01:11:54,600
it's a conjunction you don't say

1104
01:11:54,820 --> 01:11:56,780
the guys beautiful and that

1105
01:11:56,780 --> 01:11:58,970
so these issues to see

1106
01:11:58,970 --> 01:12:01,750
see so essentially is using this type of thing

1107
01:12:01,800 --> 01:12:07,340
and using this is overseas and then you keep going green

1108
01:12:07,390 --> 01:12:10,280
now obviously you can sort of doing something

1109
01:12:10,280 --> 01:12:12,270
to this title things as well

1110
01:12:12,280 --> 01:12:16,340
can then you should learning and do prediction

1111
01:12:16,370 --> 01:12:18,530
so similar ideas we used by

1112
01:12:18,580 --> 01:12:23,000
about this thing about this

1113
01:12:23,000 --> 01:12:24,380
so our approach

1114
01:12:24,410 --> 01:12:32,420
we also try to explain the connection collectivity connected connectives can respond connect

1115
01:12:32,470 --> 01:12:34,980
and however we see the context is not to me

1116
01:12:35,020 --> 01:12:37,600
context is not longer the main

1117
01:12:37,630 --> 01:12:39,740
so the contexts you're going to say

1118
01:12:39,740 --> 01:12:42,910
it's going to be the feature and the dolphin

1119
01:12:44,090 --> 01:12:47,120
for the same in the same domain the battery life is lion takes long time

1120
01:12:47,120 --> 01:12:49,340
to focus not to see

1121
01:12:49,360 --> 01:12:54,110
pretty different ways possible negative and also tried extended this

1122
01:12:54,230 --> 01:12:56,920
two pseudo and inter standards

1123
01:12:56,930 --> 01:12:59,590
and again and also we don't need to be called

1124
01:12:59,680 --> 01:13:03,560
we basically accumulating slowly

1125
01:13:03,600 --> 01:13:05,480
so interest and this

1126
01:13:05,520 --> 01:13:10,460
basically internal is which is essentially the same rules as other people are using however

1127
01:13:10,470 --> 01:13:11,570
we not just

1128
01:13:11,620 --> 01:13:14,910
focus and great we also talk about the features

1129
01:13:14,980 --> 01:13:17,950
essentially we we want books the book

1130
01:13:18,030 --> 01:13:21,930
and then what battery life when not just talking about along with

1131
01:13:21,940 --> 01:13:23,090
recorded books

1132
01:13:23,160 --> 01:13:25,480
recall the feature and the

1133
01:13:25,520 --> 01:13:26,750
and the long

1134
01:13:26,960 --> 01:13:30,380
so this is why this working because obviously

1135
01:13:30,450 --> 01:13:33,630
since then no money you have the same

1136
01:13:36,660 --> 01:13:39,070
they can also stretches further

1137
01:13:39,120 --> 01:13:41,120
and they say

1138
01:13:41,130 --> 01:13:44,480
this camera has a long battery life which is great it doesn't have to be

1139
01:13:45,110 --> 01:13:47,170
it doesn't have to be and

1140
01:13:47,170 --> 01:13:48,990
again you have to have

1141
01:13:49,240 --> 01:13:51,230
post opinion words and the

1142
01:13:51,270 --> 01:13:54,060
and the the future

1143
01:13:54,120 --> 01:13:56,020
so again this is a

1144
01:13:56,040 --> 01:14:01,460
idea of the same senders should have same opinion but not always true knowledge

1145
01:14:01,460 --> 01:14:02,610
and also

1146
01:14:02,640 --> 01:14:05,660
another thing you can go into an interest in

1147
01:14:05,670 --> 01:14:08,210
in this and this how also do something like that

1148
01:14:08,210 --> 01:14:14,000
temporal resolution images typically measured thousand hertz or ten thousand hertz even

1149
01:14:16,410 --> 01:14:20,020
but from ri is this is very slow

1150
01:14:20,020 --> 01:14:21,340
same value

1151
01:14:24,250 --> 01:14:30,070
twice the second or if you have very fast once the second that's that's very

1152
01:14:30,070 --> 01:14:32,980
very fine

1153
01:14:35,360 --> 01:14:39,140
there is a diff distinction between

1154
01:14:43,340 --> 01:14:45,730
in fact

1155
01:14:45,730 --> 01:14:49,780
OK to pay between dependent BCI as an independent ones

1156
01:14:49,800 --> 01:14:55,970
so i said in my definition i want to be independent from peripheral nerves and

1157
01:15:00,940 --> 01:15:03,770
brain signals so in fact

1158
01:15:03,880 --> 01:15:10,260
i will be talking all the time about about independent BCCI's but

1159
01:15:10,280 --> 01:15:16,930
there are also people that use additional the eye movement and and things like that

1160
01:15:16,950 --> 01:15:22,510
as i said before if you want to the rehabilitation this is it might be

1161
01:15:22,550 --> 01:15:24,570
very good good idea

1162
01:15:29,650 --> 01:15:33,120
the way to

1163
01:15:33,140 --> 01:15:36,960
it evokes some brain signals saying your brain

1164
01:15:36,970 --> 01:15:44,250
if i can do that by for example handwaving some visual evoked activity of by

1165
01:15:44,250 --> 01:15:50,250
some whistling so there was acoustically evoked potentials

1166
01:15:51,260 --> 01:15:53,350
and so

1167
01:15:54,510 --> 01:15:55,690
requires some

1168
01:15:55,710 --> 01:15:57,150
particular stimuli

1169
01:15:57,180 --> 01:15:59,350
and the set of

1170
01:15:59,370 --> 01:16:03,400
and the ideal way of communicating as i

1171
01:16:03,410 --> 01:16:04,350
i think it is

1172
01:16:04,370 --> 01:16:06,180
is that

1173
01:16:06,220 --> 01:16:07,300
we can

1174
01:16:07,360 --> 01:16:13,530
have something independent of some external trigger or external pacemaker or something like that

1175
01:16:15,320 --> 01:16:17,540
you notice that i'm trying to

1176
01:16:17,540 --> 01:16:23,200
to push everybody on the side of the way in this

1177
01:16:23,210 --> 01:16:25,570
so called operant conditioning

1178
01:16:25,600 --> 01:16:33,460
and the detection of mental states so what does that operant conditioning me conditioning means

1179
01:16:33,480 --> 01:16:35,780
that year

1180
01:16:36,660 --> 01:16:43,770
talk to your learning in long sessions to control your brain signals such that

1181
01:16:44,960 --> 01:16:46,370
signal processing can

1182
01:16:46,390 --> 01:16:49,240
you can read out your intentions

1183
01:16:49,250 --> 01:16:51,330
condition to do that

1184
01:16:51,350 --> 01:16:56,780
and and on the side you would have some particular brain states that you

1185
01:16:56,820 --> 01:16:58,180
we have anyway

1186
01:16:58,190 --> 01:17:00,610
for example think about

1187
01:17:00,630 --> 01:17:03,610
subtracting some numbers are

1188
01:17:03,630 --> 01:17:09,250
rotating in geometric object so why you think about this going from the labor to

1189
01:17:09,250 --> 01:17:10,550
your home

1190
01:17:10,560 --> 01:17:15,290
so that different brain areas that are responsible for the this or

1191
01:17:15,300 --> 01:17:18,690
think about moving your left hand right hand

1192
01:17:18,690 --> 01:17:20,000
well your foot

1193
01:17:20,010 --> 01:17:23,960
so these are all mental states that

1194
01:17:23,980 --> 01:17:26,060
your brain will follow

1195
01:17:27,840 --> 01:17:30,430
the point is to detect them properly

1196
01:17:30,440 --> 01:17:32,850
and there might be no

1197
01:17:32,880 --> 01:17:36,450
big learning involved in the

1198
01:17:36,460 --> 01:17:37,190
but i

1199
01:17:37,230 --> 01:17:39,960
discuss that later

1200
01:17:39,980 --> 01:17:46,560
so there's a lot of people recently two BCI i think when

1201
01:17:46,570 --> 01:17:49,500
when i got interested in this

1202
01:17:49,510 --> 01:17:50,350
there were

1203
01:17:51,350 --> 01:17:56,680
almost a thousand groups in the world to have been pursuing this

1204
01:17:56,690 --> 01:17:58,330
on the

1205
01:17:58,360 --> 01:18:01,700
maybe more non invasive side

1206
01:18:02,100 --> 01:18:04,590
just store

1207
01:18:04,640 --> 01:18:11,430
through some names that you trawl genetically list and i will talk about their work

1208
01:18:11,430 --> 01:18:13,180
and then some

1209
01:18:13,190 --> 01:18:17,820
pioneers in this on this noninvasive side to be bomb into being

1210
01:18:20,350 --> 01:18:22,640
the beckman institute

1211
01:18:24,530 --> 01:18:26,520
and john will paul

1212
01:18:26,560 --> 01:18:27,610
all any

1213
01:18:27,630 --> 01:18:30,870
and get for cello in ghana

1214
01:18:31,270 --> 01:18:37,690
but lots of other people started to get interested also some in this room

1215
01:18:37,700 --> 01:18:41,280
in BCI because

1216
01:18:41,290 --> 01:18:47,910
it it's something very concrete so if you're interested in computational neuroscience

1217
01:18:50,240 --> 01:18:53,410
BCI i actually gives you

1218
01:18:53,430 --> 01:18:57,630
tool quite unique tool in your hands

1219
01:18:59,410 --> 01:19:01,870
test type of is in real time

1220
01:19:01,890 --> 01:19:03,710
men with

1221
01:19:05,140 --> 01:19:06,810
can talk about

1222
01:19:06,820 --> 01:19:07,860
it later

1223
01:19:07,860 --> 01:19:09,400
is it will allow us

1224
01:19:09,410 --> 01:19:12,820
to find the optimal parameters theta one and theta two

1225
01:19:12,870 --> 01:19:14,880
in some inference scheme

1226
01:19:15,770 --> 01:19:17,200
running convex of

1227
01:19:17,210 --> 01:19:21,860
number answer to explain to you why convexity is really really great

1228
01:19:21,870 --> 01:19:26,910
why this is really the quantity that you want to have for inference

1229
01:19:26,920 --> 01:19:30,640
and you know why what i'm doing some setting everything up

1230
01:19:30,690 --> 01:19:33,880
two things to convex overrated

1231
01:19:33,900 --> 01:19:40,140
because well convex always nicely nice to use they have nice convergence guarantees and all

1232
01:19:40,140 --> 01:19:44,820
that they don't get stuck in local minima by construction all that

1233
01:19:45,590 --> 01:19:48,230
who does not know

1234
01:19:48,240 --> 01:19:53,050
that a convex function so who knows that are convex function has only one global

1235
01:19:57,980 --> 01:20:02,160
who would know how to prove it

1236
01:20:02,200 --> 01:20:07,080
not many of you brought the show to the

1237
01:20:07,090 --> 01:20:14,560
OK let me we could all the picture books quite the useful piece of intuition

1238
01:20:19,360 --> 01:20:21,660
so first of all

1239
01:20:21,670 --> 01:20:23,540
a convex function

1240
01:20:23,550 --> 01:20:28,050
the function and that may not be just in one dimension but in several dimensions

1241
01:20:28,050 --> 01:20:30,070
where if i think two points

1242
01:20:30,110 --> 01:20:32,210
and it's craft

1243
01:20:32,230 --> 01:20:35,660
metro line

1244
01:20:35,670 --> 01:20:40,690
then all the function values underneath the line like

1245
01:20:40,700 --> 01:20:42,990
and that none of the points

1246
01:20:43,000 --> 01:20:47,190
underneath this line on the graph can be found about the line so that the

1247
01:20:51,730 --> 01:20:53,810
i wouldn't be a convex function

1248
01:21:00,070 --> 01:21:05,070
how to prove that there is only one local and one global minimum

1249
01:21:05,160 --> 01:21:07,460
well let's assume

1250
01:21:07,500 --> 01:21:09,720
but i have to look at

1251
01:21:09,770 --> 01:21:10,940
so this point

1252
01:21:10,980 --> 01:21:12,890
not quite

1253
01:21:12,900 --> 01:21:15,050
then by convexity i know

1254
01:21:15,060 --> 01:21:17,360
the function values between the those

1255
01:21:17,370 --> 01:21:19,580
o points if i just make the line

1256
01:21:19,630 --> 01:21:23,580
it will be less equal than those two values

1257
01:21:23,590 --> 01:21:27,590
now either those two values are the same

1258
01:21:27,600 --> 01:21:31,110
then well this must all be one global minimum so i get the function it

1259
01:21:31,110 --> 01:21:32,800
looks like a troll

1260
01:21:32,820 --> 01:21:34,560
if one of them is like this

1261
01:21:34,570 --> 01:21:37,940
it was quite obviously i can go from here to there

1262
01:21:38,870 --> 01:21:40,740
for the sake of the argument

1263
01:21:40,750 --> 01:21:45,160
and i have decreased my for my function values get the contradiction

1264
01:21:45,200 --> 01:21:49,070
this also works out this reasoning in more than one dimension

1265
01:21:49,070 --> 01:21:50,670
that's one of the

1266
01:21:50,680 --> 01:21:54,330
reasons why we all like convexity so much

1267
01:21:54,350 --> 01:21:59,760
the entire courses that you can teach and convex analysis so wonderful book by rockefeller

1268
01:22:01,070 --> 01:22:03,000
convex analysis

1269
01:22:03,040 --> 01:22:07,580
i mean basic it's really a treasure trove of a lot of optimisation

1270
01:22:14,160 --> 01:22:15,380
the station

1271
01:22:19,750 --> 01:22:23,550
so what i'm going to try some sampling is going to try

1272
01:22:23,600 --> 01:22:28,600
and find the optimal parameters theta one and theta two

1273
01:22:28,610 --> 01:22:30,530
as i might want to infer

1274
01:22:30,530 --> 01:22:31,830
the probability

1275
01:22:31,860 --> 01:22:35,290
that i'm seeing here it's weird tales

1276
01:22:35,290 --> 01:22:36,670
so therefore

1277
01:22:36,690 --> 01:22:37,850
at some point

1278
01:22:37,900 --> 01:22:43,070
i will need to optimize over those parameters theta one and theta

1279
01:22:43,070 --> 01:22:46,420
so one of the expressions that we might want to optimize

1280
01:22:46,480 --> 01:22:51,350
it is the negative log likelihood of the negative log posterior

1281
01:22:51,410 --> 01:22:54,290
so in this case it would take the negative log

1282
01:22:54,300 --> 01:22:56,820
of that well let's go up here

1283
01:22:57,010 --> 01:23:01,360
we're basically jumping a few slides and the negative log of p of fixed parameters

1284
01:23:01,360 --> 01:23:04,540
by theta within the g theta

1285
01:23:04,550 --> 01:23:08,410
minus some linear function theta

1286
01:23:08,550 --> 01:23:11,670
now that the overall expression is convex

1287
01:23:11,690 --> 01:23:15,290
and so it turns out that they can do things quite easily

1288
01:23:15,300 --> 01:23:21,070
i will be going through the specific equations in a few slides so don't worry

1289
01:23:21,120 --> 01:23:28,760
but that's the main reason for very question after got and so why doing something

1290
01:23:28,770 --> 01:23:34,340
that says that this is the main reason why convex is really really handy here

1291
01:23:34,430 --> 01:23:39,170
and we look at the maximum likelihood maximum history estimators

1292
01:23:39,180 --> 01:23:40,830
OK good so

1293
01:23:40,830 --> 01:23:44,190
it's looking example of binomial distribution

1294
01:23:44,430 --> 01:23:46,730
it doesn't look

1295
01:23:46,730 --> 01:23:51,330
a great deal well that's just one point four point six probabilities

1296
01:23:51,350 --> 01:23:53,540
now let's

1297
01:23:53,540 --> 01:23:55,950
when we start with the

1298
01:23:56,000 --> 01:23:57,930
a second

1299
01:23:57,960 --> 01:24:02,240
so there are various ways of chopping up the field of statistics and

1300
01:24:03,330 --> 01:24:08,720
typical way at least within frequencies frequentist statistics is to treat statistical tests

1301
01:24:08,800 --> 01:24:13,180
on one hand and in parameter estimation on the other and so that's the way

1302
01:24:13,180 --> 01:24:14,520
i've done it

1303
01:24:14,560 --> 01:24:17,740
so that's what we're going to talk about say what i mean by that and

1304
01:24:17,740 --> 01:24:21,770
then there are two very important methods the method of maximum likelihood and the method

1305
01:24:21,770 --> 01:24:23,270
of least squares

1306
01:24:23,270 --> 01:24:27,240
and there's something related to that which is called interval estimation and you probably see

1307
01:24:27,240 --> 01:24:30,780
that in the context of setting limits set limits on the higgs mass or limits

1308
01:24:30,780 --> 01:24:32,780
on various parameters

1309
01:24:32,820 --> 01:24:35,630
so first of all let me say what i specifically what i mean by a

1310
01:24:35,630 --> 01:24:37,020
parameter estimation

1311
01:24:37,060 --> 01:24:39,060
the parameters of of the pdf

1312
01:24:39,070 --> 01:24:41,950
are constants that characterize its shape

1313
01:24:41,980 --> 01:24:43,380
the parameters of

1314
01:24:43,460 --> 01:24:48,850
hypothesis of those numerical values that characterize that hypothesis

1315
01:24:48,870 --> 01:24:51,090
so very often we been using this notation

1316
01:24:51,100 --> 01:24:56,120
i have some pdf for a random variable x and the narrator sort of semicolon

1317
01:24:56,120 --> 01:24:58,510
and then it's characterized by some parameters

1318
01:24:58,600 --> 01:25:01,960
and they appear over here is greek letters

1319
01:25:03,290 --> 01:25:08,340
that's the probability law and then you have some sample of observed values say perhaps

1320
01:25:08,340 --> 01:25:12,790
values x that are sampled from this PDF and you want to find some function

1321
01:25:12,790 --> 01:25:16,240
of those measured values that can be used to

1322
01:25:16,400 --> 01:25:19,560
estimate the value of this parameter

1323
01:25:19,570 --> 01:25:24,350
i suppose that you only know the functional form of this problem probability law

1324
01:25:24,350 --> 01:25:28,260
but the numerical value of the parameter is not yet known so what we want

1325
01:25:28,260 --> 01:25:31,880
to do is we want to construct some function of the data to estimate the

1326
01:25:31,880 --> 01:25:35,620
parameter so once they found such a function what i do is i write it

1327
01:25:35,620 --> 01:25:36,730
with the hat

1328
01:25:36,760 --> 01:25:42,400
all right so theta had is the estimator for the parameter theta

1329
01:25:43,120 --> 01:25:49,520
as you probably noticed physicists are not that good always distinguishing between a function thought

1330
01:25:49,520 --> 01:25:53,870
of as a mapping and the value of the function so mathematicians are much better

1331
01:25:53,950 --> 01:25:58,870
that so if i want to insist on the distinction also estimator for the

1332
01:25:58,960 --> 01:26:02,920
actual function and then estimate for the value of the function

1333
01:26:03,300 --> 01:26:10,050
evaluated with a specific set usually from context it's clear what what i'm talking about

1334
01:26:11,570 --> 01:26:15,970
the function of a random variable is itself a random variable so therefore an estimator

1335
01:26:15,970 --> 01:26:18,950
is the random variable because it is function of the data

1336
01:26:19,010 --> 01:26:23,530
so therefore if i were to repeat the entire measurement many times and to evaluate

1337
01:26:23,530 --> 01:26:27,700
the estimator with the data from each measurement i can make a histogram of those

1338
01:26:27,950 --> 01:26:31,580
and it would have some shape so i could i could envisage looking at the

1339
01:26:31,580 --> 01:26:33,920
the pdf of data had

1340
01:26:33,920 --> 01:26:38,630
and that will depend in general on the true and unknown value data

1341
01:26:38,700 --> 01:26:41,920
but it will it will follow some pdf

1342
01:26:43,330 --> 01:26:48,480
different functions used as estimators will lead to different pdf and i can ask what

1343
01:26:48,480 --> 01:26:51,480
would be a good estimator what would be a bad estimator

1344
01:26:51,670 --> 01:26:55,260
clearly if the true value of the parameter is here

1345
01:26:55,280 --> 01:26:59,720
but if the PDF the estimator was shifted over to one side that would not

1346
01:26:59,720 --> 01:27:00,580
be good

1347
01:27:00,600 --> 01:27:03,230
that would be an example of a biased estimator

1348
01:27:04,230 --> 01:27:08,540
you can define in fact the bias of an estimator is the expectation value of

1349
01:27:08,540 --> 01:27:11,330
the estimator minus the true value

1350
01:27:11,380 --> 01:27:15,570
and so you would prefer to construct a function that has zero or very small

1351
01:27:15,570 --> 01:27:19,280
bias a bias in the sense is the type of systematic error

1352
01:27:19,320 --> 01:27:25,250
so the the green curve here for example that has an expectation value which is

1353
01:27:25,260 --> 01:27:26,510
equal to the

1354
01:27:26,530 --> 01:27:27,510
true value

1355
01:27:27,530 --> 01:27:31,070
it is more or less from the picture that's what i can tell and so

1356
01:27:31,070 --> 01:27:35,220
that we have a zero bias but still that's quite broad distribution i would prefer

1357
01:27:35,470 --> 01:27:40,260
that when i repeat the measurement that i get only a small scatter of values

1358
01:27:40,260 --> 01:27:44,110
so of the PDF of my estimator were characterised by the blue curve that would

1359
01:27:44,110 --> 01:27:48,790
be even better so you want a small variance or standard deviation

1360
01:27:48,830 --> 01:27:50,910
of the pdf

1361
01:27:50,920 --> 01:27:53,780
that corresponds to the statistical error

1362
01:27:53,850 --> 01:27:57,630
of your estimate the statistical error of your measurement

1363
01:27:57,660 --> 01:28:02,980
actually having a small bias or zero bias and small variance can very often those

1364
01:28:02,980 --> 01:28:05,700
are conflicting criteria you can have

1365
01:28:05,850 --> 01:28:07,750
the best of both worlds

1366
01:28:08,630 --> 01:28:13,250
it is what you can do is there's a broad class of estimators that have

1367
01:28:13,670 --> 01:28:17,850
very small bias or almost zero bias amongst those you can choose the ones that

1368
01:28:17,850 --> 01:28:20,990
have the smallest possible variants

1369
01:28:21,000 --> 01:28:25,580
but the point i want to make is that there is no golden rule for

1370
01:28:25,580 --> 01:28:31,300
writing down the right estimator estimators are characterized by certain properties such as the robot

1371
01:28:31,310 --> 01:28:36,330
bias and variance and you simply choose an estimator which has the desired properties optimal

1372
01:28:36,330 --> 01:28:41,350
properties so there are a few additional properties such as robustness and efficiency and a

1373
01:28:41,350 --> 01:28:45,540
few other things but there is no golden rule for writing down an estimator you

1374
01:28:45,540 --> 01:28:51,580
choose the estimated that has the desired properties

1375
01:28:51,650 --> 01:28:53,440
so what i want to do now

1376
01:28:53,460 --> 01:28:57,060
this just to write down formulas for a couple of obvious estimators and to work

1377
01:28:57,060 --> 01:28:59,140
out the bias and variance

1378
01:28:59,180 --> 01:29:04,460
so suppose the parameter in question is the expectation value of x so i've used

1379
01:29:04,540 --> 01:29:09,720
here the greek letter mu to denote the expectation value of some random variable x

1380
01:29:10,620 --> 01:29:15,670
in this case there's an obvious estimator for because recall the expectation value is what

1381
01:29:15,670 --> 01:29:18,310
you would get for the average of the random variable

1382
01:29:18,360 --> 01:29:20,150
if you repeated the observation

1383
01:29:20,160 --> 01:29:22,030
an infinite number of times

1384
01:29:22,080 --> 01:29:24,860
well i don't have an infinite number of observations i have

1385
01:29:24,930 --> 01:29:28,560
a finite data sample with a certain number of n observations

1386
01:29:28,590 --> 01:29:30,770
but still it's pretty obvious that

1387
01:29:30,780 --> 01:29:35,410
the arithmetic average of those observations that seems like a natural estimator

1388
01:29:36,310 --> 01:29:37,900
the expectation value

1389
01:29:37,910 --> 01:29:39,100
and so

1390
01:29:39,110 --> 01:29:42,530
that such an important estimated that we have a special name that's called the sample

1391
01:29:42,530 --> 01:29:47,290
mean and sometimes that's written with the bar instead of instead of the hat but

1392
01:29:47,290 --> 01:29:48,790
that is simply an estimator

1393
01:29:48,850 --> 01:29:51,540
for the for the media

1394
01:29:51,540 --> 01:29:55,940
so once you've specified that function of the data you can work out the bias

1395
01:29:55,940 --> 01:30:00,090
by computing the expectation value of this function and it's easy to show

1396
01:30:00,160 --> 01:30:02,580
the bias is identically zero

1397
01:30:02,780 --> 01:30:07,100
and furthermore you can work out the variance of mu had and it's easy to

1398
01:30:07,100 --> 01:30:08,120
let's assume

1399
01:30:08,140 --> 01:30:11,330
you doing breast cancer study

1400
01:30:11,370 --> 01:30:13,350
and new training set

1401
01:30:13,510 --> 01:30:17,160
it's got a lot of women with breast cancer

1402
01:30:17,200 --> 01:30:21,390
and it's probably heavily biased towards sequence

1403
01:30:21,430 --> 01:30:25,010
maybe not quite uniformly drawn from the entire

1404
01:30:25,040 --> 01:30:26,310
set of breast cancer

1405
01:30:26,410 --> 01:30:30,870
this maybe this is a specific hospital and women only go there if they have

1406
01:30:30,870 --> 01:30:32,810
a really serious cases

1407
01:30:32,830 --> 01:30:36,180
but then you want to actually is on a case that works in reality

1408
01:30:36,220 --> 01:30:39,180
you want to deploy it

1409
01:30:39,180 --> 01:30:41,200
and then what you get

1410
01:30:41,220 --> 01:30:45,600
the normal distribution of women coming in to see the doctor actually they're probably less

1411
01:30:45,600 --> 01:30:50,040
than likely to have breast cancer because they have probably already been screened many many

1412
01:30:50,040 --> 01:30:51,850
times before

1413
01:30:55,100 --> 01:30:59,010
it's very hard to model the distributions

1414
01:30:59,060 --> 01:31:03,200
i mean you would have to make a fairly good assumptions on the psychology of

1415
01:31:03,200 --> 01:31:07,990
women and on whatever the doctors who designed the clinical study in the first place

1416
01:31:08,470 --> 01:31:12,350
in order to really find a good reweighting scheme

1417
01:31:12,410 --> 01:31:15,370
now there are trying to do an exact model is hopeless

1418
01:31:16,220 --> 01:31:18,470
you can actually try to infer it

1419
01:31:19,280 --> 01:31:22,120
well all the data that you get

1420
01:31:22,140 --> 01:31:22,930
and this

1421
01:31:22,950 --> 01:31:26,870
is what's called covariate shift the sample bias correction

1422
01:31:27,870 --> 01:31:31,600
now thing that you could do is feature selection

1423
01:31:31,600 --> 01:31:32,870
so feature selection

1424
01:31:32,890 --> 01:31:35,970
in that case it would let's say i've got

1425
01:31:37,140 --> 01:31:40,100
data set of apples dataset for and

1426
01:31:40,140 --> 01:31:41,870
pictures of them

1427
01:31:42,740 --> 01:31:46,540
in this case might not necessarily want to answer the question are they really different

1428
01:31:46,540 --> 01:31:51,010
entities well short of course they are i mean that's what i've just mentioned but

1429
01:31:51,020 --> 01:31:55,310
you might want to answer the question well which features the doctors should they keep

1430
01:31:55,390 --> 01:31:59,260
in order to keep the data as different as possible

1431
01:31:59,280 --> 01:32:01,580
to feature selection

1432
01:32:01,600 --> 01:32:06,910
if you want to have a very meaningful features to discriminate the two sit

1433
01:32:07,010 --> 01:32:08,280
in that case

1434
01:32:08,350 --> 01:32:12,120
you would like to have criteria that you can maximize

1435
01:32:12,160 --> 01:32:12,910
as you

1436
01:32:12,930 --> 01:32:16,600
at features or as you remove them

1437
01:32:18,020 --> 01:32:20,450
and and obviously the end of the day

1438
01:32:20,720 --> 01:32:23,020
if you have a two sample test it works well

1439
01:32:23,080 --> 01:32:25,450
you can also turn into one sample taste

1440
01:32:25,470 --> 01:32:28,740
in other words around the selection

1441
01:32:28,760 --> 01:32:31,140
for instance let's say some engineer

1442
01:32:31,140 --> 01:32:34,930
or computer scientist has written a program and has a whole bunch of not

1443
01:32:34,970 --> 01:32:36,640
it spits out data

1444
01:32:36,660 --> 01:32:41,220
as some for some sort of stimulating he told you what he really believes that

1445
01:32:41,220 --> 01:32:43,560
this is a good simulator

1446
01:32:43,580 --> 01:32:46,700
and all these really willing to do is tell you i look it looks almost

1447
01:32:46,700 --> 01:32:49,370
like the real thing you just need to fiddle with those knobs and a little

1448
01:32:49,370 --> 01:32:52,410
bit about the world

1449
01:32:54,080 --> 01:32:57,640
and of course you don't quite trust him so well but you would like to

1450
01:32:57,660 --> 01:33:01,450
find an automatic way of tuning does not

1451
01:33:01,510 --> 01:33:03,700
OK so how can you do that

1452
01:33:03,700 --> 01:33:05,780
and we've got some real data

1453
01:33:05,790 --> 01:33:07,870
it's call it x

1454
01:33:07,870 --> 01:33:09,950
and that that is expensive

1455
01:33:09,970 --> 01:33:11,950
so we only have enough them

1456
01:33:13,120 --> 01:33:16,240
this black box if you engineer friend gave you

1457
01:33:16,260 --> 01:33:18,350
can produce an opportune umber of

1458
01:33:20,390 --> 01:33:23,560
so what you can now do is you can say well i want to tune

1459
01:33:23,560 --> 01:33:24,810
the not on the this

1460
01:33:25,020 --> 01:33:27,240
generating distribution for the y

1461
01:33:27,280 --> 01:33:31,560
such states as close as possible to x

1462
01:33:31,660 --> 01:33:37,740
as a matter of fact this is exactly what we've been doing yesterday

1463
01:33:38,950 --> 01:33:41,120
we had the sufficient statistics

1464
01:33:41,290 --> 01:33:43,560
we compute that x

1465
01:33:43,600 --> 01:33:46,290
then we have the natural parameters theta

1466
01:33:46,290 --> 01:33:48,330
if we treat it until

1467
01:33:48,330 --> 01:33:51,100
the expectation

1468
01:33:51,140 --> 01:33:52,890
for that distribution

1469
01:33:52,910 --> 01:33:57,060
i was this close as possible to the extinct

1470
01:33:57,850 --> 01:33:59,430
nothing really new

1471
01:33:59,600 --> 01:34:04,950
so just that this is now a little bit more general

1472
01:34:06,290 --> 01:34:11,600
so any question on the concepts at the moment

1473
01:34:13,740 --> 01:34:16,470
now to the math

1474
01:34:18,990 --> 01:34:20,810
well let me first show you

1475
01:34:21,560 --> 01:34:22,890
you would do that

1476
01:34:24,430 --> 01:34:26,490
in indirect way

1477
01:34:26,510 --> 01:34:27,740
well you could go

1478
01:34:27,760 --> 01:34:31,450
and estimate the density speed think you had

1479
01:34:31,600 --> 01:34:36,510
and then compute some distance between p had think q had

1480
01:34:36,510 --> 01:34:39,890
and that simple density estimator that

1481
01:34:39,910 --> 01:34:41,620
you might want to use

1482
01:34:41,680 --> 01:34:43,930
before you do anything fancy

1483
01:34:43,930 --> 01:34:47,490
this one's called the parzen window density estimator

1484
01:34:47,490 --> 01:34:51,720
who has heard of possible windows before

1485
01:34:53,700 --> 01:34:58,570
so this is probably the single most useful density estimator that you will ever hear

1486
01:34:59,890 --> 01:35:08,200
it works as follows

1487
01:35:09,790 --> 01:35:10,990
let's say

1488
01:35:10,990 --> 01:35:12,700
i've got some dots

1489
01:35:22,830 --> 01:35:30,540
lambertian distribution would look like this

1490
01:35:30,600 --> 01:35:34,160
now i give everybody would agree that this is a bad idea for modelling the

1491
01:35:34,160 --> 01:35:36,100
overall density

1492
01:35:36,140 --> 01:35:41,760
probably much better idea for modeling densities to smooth soul this out

1493
01:35:41,780 --> 01:35:45,790
they need to put some muscle bumpier hospital but there

1494
01:35:45,810 --> 01:35:49,390
same one here there

1495
01:35:49,510 --> 01:35:51,470
then just to add all this up

1496
01:35:52,080 --> 01:35:53,620
so it so that

1497
01:35:53,660 --> 01:36:09,700
we get this function here

1498
01:36:09,990 --> 01:36:13,450
something like that not quite true but

1499
01:36:13,470 --> 01:36:16,470
now this black curve would look much closer to

1500
01:36:16,620 --> 01:36:18,520
a real density estimate

1501
01:36:18,540 --> 01:36:19,430
the red line

1502
01:36:20,850 --> 01:36:24,470
the parzen windows similar does exactly that

1503
01:36:24,490 --> 01:36:25,810
so how do we do that

1504
01:36:26,990 --> 01:36:28,910
we have some

1505
01:36:28,910 --> 01:36:30,780
kernel function here

1506
01:36:30,810 --> 01:36:35,120
from the past windows system and this is not the same kernel function is what

1507
01:36:35,120 --> 01:36:38,200
john deep into been talking about

1508
01:36:38,220 --> 01:36:43,990
and that's a depends on the position

1509
01:36:46,700 --> 01:36:47,890
t minus

1510
01:36:50,020 --> 01:36:52,180
and maybe there's some scale

1511
01:36:52,220 --> 01:36:55,660
OK i'm going to ignore scale at the moment

1512
01:36:55,660 --> 01:36:56,760
so this just

1513
01:36:56,760 --> 01:36:58,060
we place upon

1514
01:36:58,060 --> 01:37:01,100
at location x

1515
01:37:01,100 --> 01:37:02,700
and then

1516
01:37:02,720 --> 01:37:08,060
so for instance this could be just the simple garcia

1517
01:37:08,080 --> 01:37:10,330
obviously what we want is that this thing

1518
01:37:10,350 --> 01:37:13,220
if we integrate it

1519
01:37:13,220 --> 01:37:15,600
OK of t minus six

1520
01:37:16,370 --> 01:37:19,120
equals one

1521
01:37:19,120 --> 01:37:21,080
and then what we do is

1522
01:37:21,160 --> 01:37:23,870
just estimate p had

1523
01:37:24,010 --> 01:37:25,600
of t

1524
01:37:26,890 --> 01:37:28,390
one of in

1525
01:37:28,490 --> 01:37:30,100
the sum of all terms

1526
01:37:30,140 --> 01:37:32,390
so far arriving going from one

1527
01:37:36,120 --> 01:37:37,080
of t

1528
01:37:37,080 --> 01:37:39,790
minus x i

1529
01:37:39,890 --> 01:37:50,890
that's it

1530
01:37:50,890 --> 01:37:55,920
these are the reasons why you might want to do observational studies sorry that's not

1531
01:37:55,920 --> 01:37:58,000
clear the lego

1532
01:37:58,000 --> 01:38:05,450
so even though observational study last second best there's often no alternative was start with

1533
01:38:05,460 --> 01:38:09,740
nevertheless we have to realize their second best work very hard

1534
01:38:09,750 --> 01:38:14,070
to ensure they have first of all the highest quality and and then to understand

1535
01:38:14,070 --> 01:38:17,870
what we can learn from them and

1536
01:38:18,160 --> 01:38:22,010
so the other thing to say is that there is no necessary components is necessary

1537
01:38:22,010 --> 01:38:26,340
connection between how system behaves when you kick it and how it behaves when you

1538
01:38:26,340 --> 01:38:30,040
just want to know about the room bumping into things

1539
01:38:32,390 --> 01:38:37,260
one of the big issues of statistical causality is how do we make the link

1540
01:38:37,320 --> 01:38:41,070
what kind of things can we reasonably making how can we use them between these

1541
01:38:41,080 --> 01:38:45,310
different kinds of regimes different kinds of behaviour

1542
01:38:45,320 --> 01:38:53,050
so that's the thing about the problems again with observational studies you often observing associated

1543
01:38:53,050 --> 01:38:59,070
but it's not really causal spurious in some sense so

1544
01:38:59,110 --> 01:39:03,630
i will give some examples of these reverse causation where you really know which way

1545
01:39:03,630 --> 01:39:05,610
the effect is going so

1546
01:39:05,740 --> 01:39:10,330
there's one night example about this which is the effective

1547
01:39:10,370 --> 01:39:15,380
parents behavior on that children becoming psychotic

1548
01:39:15,450 --> 01:39:21,510
but if you sort of just observe parents pay their children's behavior without it's perfectly

1549
01:39:21,510 --> 01:39:26,650
possible that site having a psychotic child is likely to affect the parents by a

1550
01:39:26,810 --> 01:39:32,190
rather serious way so which weighs about causality going simple observation and study we find

1551
01:39:32,190 --> 01:39:37,720
it hard to untangle that regression to the mean is a very important example because

1552
01:39:37,720 --> 01:39:38,670
a lot of

1553
01:39:38,750 --> 01:39:44,670
so a lot of fuss about two dozen installation of traffic cameras accident hotspots

1554
01:39:44,730 --> 01:39:46,690
actually reduce

1555
01:39:50,890 --> 01:39:55,420
typically i'm sure so many familiar with the idea of an accident hotspot has had

1556
01:39:55,420 --> 01:39:59,640
a lot of accidents for combination of two reasons one is possibly it's actually an

1557
01:39:59,640 --> 01:40:01,460
unsafe and anyway

1558
01:40:01,470 --> 01:40:03,230
but also

1559
01:40:03,250 --> 01:40:07,860
there the random variation and if you pick out those which have particularly high

1560
01:40:07,880 --> 01:40:15,050
accidents then the ones where the random fluctuation has been positive rather than negative and

1561
01:40:15,050 --> 01:40:16,550
so the next time you look

1562
01:40:16,550 --> 01:40:19,190
that random fluctuation part would have gone away

1563
01:40:19,190 --> 01:40:23,230
and you're likely to have the decrease and you're likely to see that decrease even

1564
01:40:23,230 --> 01:40:27,360
if you don't sprinkled holy water on the spot rather than installed traffic at

1565
01:40:27,380 --> 01:40:31,660
so that is not a very good comparison

1566
01:40:31,670 --> 01:40:37,420
and all sorts of problems which are generically called confounding with these relationships with other

1567
01:40:37,420 --> 01:40:42,410
variables which have not been properly taken into account there's so

1568
01:40:42,770 --> 01:40:49,950
there is a common cause well i will come on these moments of some examples

1569
01:40:49,970 --> 01:40:53,220
here's an example of what's called complete confounding

1570
01:40:53,230 --> 01:40:56,560
so this is to say if you what you want to discover which of two

1571
01:40:56,560 --> 01:41:00,140
petrol gives you better fuel economy

1572
01:41:00,160 --> 01:41:04,800
right you use both in some sort of but if you always say OK so

1573
01:41:04,800 --> 01:41:10,310
the idea is that a fellow use to try checking is fuel consumption using BP

1574
01:41:10,310 --> 01:41:14,970
to drive from london to leeds and nodes are still to go from london to

1575
01:41:14,970 --> 01:41:20,090
leeds and BP to go back and he always found that BP was giving better

1576
01:41:20,090 --> 01:41:24,940
fuel economy but possibly because he was driving up you one way

1577
01:41:24,940 --> 01:41:26,610
downhill the other

1578
01:41:26,630 --> 01:41:33,220
so that is completely confounding with something which is of no particular interest mainly the

1579
01:41:33,220 --> 01:41:35,880
height hyperlinks above london

1580
01:41:38,020 --> 01:41:43,810
is if you could randomise maybe and sometimes you should sometimes BP in either direction

1581
01:41:43,830 --> 01:41:46,540
then you can make a fair comparison

1582
01:41:46,590 --> 01:41:54,000
some important epidemiological studies which have been completely overthrown

1583
01:41:54,050 --> 01:41:56,350
by experimental studies so

1584
01:41:56,380 --> 01:42:01,970
for a long time it was thought that was an enormously positive affect women take

1585
01:42:01,980 --> 01:42:10,390
hormone replacement therapy after the menopause tremendously positive over the reduction in coronary heart disease

1586
01:42:10,760 --> 01:42:12,230
and then

1587
01:42:12,340 --> 01:42:14,210
eventually managed to

1588
01:42:14,220 --> 01:42:19,300
do proper clinical trial and in fact found exactly the opposite

1589
01:42:19,310 --> 01:42:22,540
these treatments actually seem to raise the

1590
01:42:24,570 --> 01:42:27,890
coronary heart disease by quite a lot

1591
01:42:28,000 --> 01:42:35,160
why is this is probably because the kind of in the observation and study is

1592
01:42:36,110 --> 01:42:40,740
a middle-class fairly healthy women who are motivated to take

1593
01:42:42,870 --> 01:42:46,870
they're the ones who probably have less incidence of the disease anyway

1594
01:42:46,990 --> 01:42:50,010
and so you are not comparing like with like the ones you didn't take it

1595
01:42:50,010 --> 01:42:53,770
was just a different kind of woman with different health outcomes anyway even if you

1596
01:42:53,770 --> 01:42:58,010
have made any difference to the tree

1597
01:43:00,080 --> 01:43:04,770
i think it was a lot of fuss about the the beneficial

1598
01:43:04,780 --> 01:43:07,580
effects of antioxidants

1599
01:43:07,600 --> 01:43:12,590
to reduce the risk of disease a lot of observational evidence so that was the

1600
01:43:12,590 --> 01:43:16,400
case with exactly the same story really

1601
01:43:16,420 --> 01:43:18,350
when proper studies were done

1602
01:43:18,360 --> 01:43:21,610
it was shown that in fact these are bad for you

1603
01:43:21,670 --> 01:43:28,240
and again there was confounding almost certainly confounding with general health status of type of

1604
01:43:28,240 --> 01:43:32,670
women who supplement and went to health food shops and things like that and the

1605
01:43:32,670 --> 01:43:36,130
time womanhood

1606
01:43:36,140 --> 01:43:43,770
similar sort of story with what's called a calcium channel blocker

1607
01:43:43,780 --> 01:43:45,140
now if it

1608
01:43:45,200 --> 01:43:47,670
that being or whatever

1609
01:43:48,320 --> 01:43:54,010
and again the same story show that actually although it seemed to increase the risk

1610
01:43:54,010 --> 01:43:58,850
and the simple reason for this for the observation of finding which is called confounding

1611
01:43:58,850 --> 01:44:01,280
by indication that gave this drug

1612
01:44:01,320 --> 01:44:05,410
two people with hypertension

1613
01:44:05,460 --> 01:44:06,850
what is for

1614
01:44:06,910 --> 01:44:09,510
and people with hypertension do tend to develop

1615
01:44:09,520 --> 01:44:15,600
my comment function rather more than those so even it was helping them it wasn't

1616
01:44:15,600 --> 01:44:16,790
happening enough

1617
01:44:16,860 --> 01:44:20,800
two to overcome the difference and

1618
01:44:20,800 --> 01:44:25,360
and this a problem because these studies these are cases in which it was one

1619
01:44:25,360 --> 01:44:31,620
of them will eventually to do studies but they were incredibly expensive incredibly complex incredibly

1620
01:44:31,620 --> 01:44:37,050
difficult to actually put into effect and took a long time and

1621
01:44:37,110 --> 01:44:39,600
and often we we simply can't we can't do

1622
01:44:39,620 --> 01:44:43,870
we have to rely on observational data but how can we

1623
01:44:43,890 --> 01:44:48,750
have we believe the message that the story can we analyse the data in ways

1624
01:44:48,750 --> 01:44:52,670
which will somehow bring out the more believable message

1625
01:44:52,720 --> 01:44:58,520
over time example from you may have heard of simpson's paradox

1626
01:45:00,890 --> 01:45:02,770
so this is a simple

1627
01:45:02,770 --> 01:45:11,120
so i example in which we comparing two drugs in hospitals for some nasty disease

1628
01:45:11,160 --> 01:45:13,060
which is quite a lot of people die

1629
01:45:13,370 --> 01:45:15,690
the standard drug

1630
01:45:15,730 --> 01:45:19,700
i was given to four hundred patients and forty percent of them

1631
01:45:20,630 --> 01:45:23,300
they're not very good but that's what was

1632
01:45:23,300 --> 01:45:25,940
and the the new drug out we're testing that

1633
01:45:25,990 --> 01:45:31,980
by this comparative trials that cells are given to four hundred patients fifty percent so

1634
01:45:32,220 --> 01:45:35,640
it looks like a clear ten percentage points advantage

1635
01:45:35,730 --> 01:45:41,590
the new drug over that could get out of the market

1636
01:45:41,610 --> 01:45:48,490
now exactly the same data would now broken down by the sex of the patient

1637
01:45:51,230 --> 01:45:56,090
so these are the men results these are the women adam up and show that

1638
01:45:56,130 --> 01:46:00,520
to the stable of top hundred ninety twenty OK

1639
01:46:02,170 --> 01:46:04,550
will the man

1640
01:46:05,450 --> 01:46:10,520
their cover right on the standard drug was seventy percent but i is gone down

1641
01:46:10,550 --> 01:46:16,100
to sixty percent of the new that doesn't very promising for sale

1642
01:46:16,120 --> 01:46:20,670
so surely the women must be counteracted but now

1643
01:46:20,760 --> 01:46:25,960
of the standard drug they were having a thirty percent survival rate idea that's going

1644
01:46:25,960 --> 01:46:30,410
down to twenty percent within the new drugs so is the case where if you

1645
01:46:30,410 --> 01:46:33,070
want to your doctor prescribing this drug

1646
01:46:33,670 --> 01:46:39,010
somebody water into your surgery with appropriate disease if you knew it was the man

1647
01:46:39,120 --> 01:46:42,580
you state above had the standard treatment of you know the woman what you say

1648
01:46:42,590 --> 01:46:47,220
the site but if it was somebody of indeterminate sex

1649
01:46:47,220 --> 01:46:49,340
because the coefficient here that is just zero

1650
01:46:49,840 --> 01:46:52,640
but is mu is just a tiny bit smaller than eleven

1651
01:46:53,360 --> 01:46:55,610
ten point nine or something like that

1652
01:46:56,950 --> 01:46:58,980
eleven minus ten point nine is

1653
01:46:59,410 --> 01:47:00,450
zero point one

1654
01:47:01,220 --> 01:47:07,220
end that's a positive number and then all the sudden this becomes no longer be optimal

1655
01:47:07,680 --> 01:47:09,420
dictionary and we have to do with it

1656
01:47:10,280 --> 01:47:12,050
and so it suggests a net would be the only

1657
01:47:14,160 --> 01:47:17,410
problem with this dictionary and so i suggest if we

1658
01:47:17,810 --> 01:47:18,960
do appear that money

1659
01:47:19,460 --> 01:47:21,130
x to be the entering variable

1660
01:47:21,830 --> 01:47:22,590
and figure out

1661
01:47:23,520 --> 01:47:26,540
with the leaving variable should be associated with it

1662
01:47:28,090 --> 01:47:31,730
and we do that's in the same way we saw thinking really has a value

1663
01:47:31,730 --> 01:47:34,140
equal to eleven or epsilon minus eleven but

1664
01:47:34,570 --> 01:47:35,970
it's not just thinking is a

1665
01:47:37,950 --> 01:47:40,860
this is eleven plus five sixteen a three

1666
01:47:41,510 --> 01:47:43,910
this is one plus fours fifteen iterative

1667
01:47:44,530 --> 01:47:50,340
three this is all of eleven plus six seventeen iterated three

1668
01:47:51,900 --> 01:47:55,300
this one is the one that has zero first and so w two

1669
01:47:56,400 --> 01:47:56,980
will be

1670
01:47:57,720 --> 01:47:59,480
leaving variable and we can do a pivot

1671
01:48:02,800 --> 01:48:06,040
so that's what i guess i said on this slide i didn't show you

1672
01:48:06,730 --> 01:48:07,680
okay stupid

1673
01:48:10,140 --> 01:48:12,160
well i also did this

1674
01:48:14,290 --> 01:48:15,140
how to figure out

1675
01:48:16,270 --> 01:48:19,150
which variables leaving variables w two leaves

1676
01:48:21,080 --> 01:48:21,980
and we can do that

1677
01:48:22,380 --> 01:48:22,970
and here is the

1678
01:48:23,570 --> 01:48:24,440
new dictionary

1679
01:48:28,850 --> 01:48:31,970
we can write down the new inequalities maybe again it's likely

1680
01:48:33,230 --> 01:48:34,880
useful to do them math the board here

1681
01:48:36,910 --> 01:48:37,680
how does this work

1682
01:48:57,620 --> 01:48:58,250
we want

1683
01:48:59,390 --> 01:49:00,520
minus fourteen

1684
01:49:05,710 --> 01:49:06,870
less than or equal to zero

1685
01:49:07,970 --> 01:49:09,010
it's not so easy

1686
01:49:09,870 --> 01:49:10,620
we want

1687
01:49:12,060 --> 01:49:13,570
three and two thirds

1688
01:49:17,500 --> 01:49:18,760
plus one third new

1689
01:49:21,340 --> 01:49:21,620
to be

1690
01:49:22,410 --> 01:49:23,580
less than or equal to zero

1691
01:49:25,030 --> 01:49:25,670
we want

1692
01:49:30,240 --> 01:49:31,690
to be less than or equal to zero

1693
01:49:33,280 --> 01:49:34,430
we want one

1694
01:49:35,000 --> 01:49:36,330
to be greater than or equal to zero

1695
01:49:37,330 --> 01:49:41,720
we won't four-thirds plus one third mu

1696
01:49:42,730 --> 01:49:43,700
be greater are

1697
01:49:44,960 --> 01:49:46,060
we want to

1698
01:49:46,680 --> 01:49:47,770
to be critical zero

1699
01:49:48,470 --> 01:49:50,270
and we want minus four hour

1700
01:49:51,000 --> 01:49:52,570
plus nudity graphical user

1701
01:49:53,400 --> 01:49:57,780
okay this one sort of automatic and this one sort automatic and this one sort of absent at one

1702
01:49:58,550 --> 01:49:59,670
this one's automatic

1703
01:50:01,740 --> 01:50:03,850
the inequality here says that new

1704
01:50:05,150 --> 01:50:06,480
is less than or equal to

1705
01:50:08,010 --> 01:50:09,050
o to one mistake here

1706
01:50:13,780 --> 01:50:15,830
i lost a minus sign thank you good

1707
01:50:17,200 --> 01:50:20,380
so we bring the three into thursday eleven-thirds

1708
01:50:21,200 --> 01:50:25,110
we multiply by three one you'd less than or equal to eleven

1709
01:50:26,650 --> 01:50:29,800
and here we want new to being greater than or equal to two

1710
01:50:30,990 --> 01:50:33,440
and here bring up to the side

1711
01:50:34,120 --> 01:50:37,190
our community greater equal to minus four r

1712
01:50:37,850 --> 01:50:38,290
for bringing

1713
01:50:38,820 --> 01:50:40,320
forty deciding new

1714
01:50:41,200 --> 01:50:42,610
to be greater than or equal to the fore

1715
01:50:43,410 --> 01:50:44,990
and now again if i plot this on

1716
01:50:45,990 --> 01:50:47,510
why new axis here

1717
01:50:49,930 --> 01:50:51,710
i have new lester equal to eleven

1718
01:50:53,000 --> 01:50:54,520
i so everything this way

1719
01:50:55,330 --> 01:50:56,810
i new great article that to

1720
01:50:58,090 --> 01:50:59,360
everything to the right at

1721
01:51:00,270 --> 01:51:02,920
those things are over new graphical minus far

1722
01:51:03,760 --> 01:51:05,340
i have new greater equal to

1723
01:51:06,000 --> 01:51:07,740
plus four r which so here

1724
01:51:08,730 --> 01:51:10,270
and intersection all these intervals

1725
01:51:10,740 --> 01:51:11,650
is this interval here

1726
01:51:12,060 --> 01:51:12,630
from fore

1727
01:51:16,940 --> 01:51:17,700
it's interesting

1728
01:51:18,170 --> 01:51:19,410
the previous intervals

1729
01:51:19,970 --> 01:51:21,160
was everything bigger than one

1730
01:51:22,460 --> 01:51:23,620
you begin eleven

1731
01:51:24,100 --> 01:51:26,470
not only has to be smaller than eleven

1732
01:51:27,160 --> 01:51:28,030
the bigger than four

1733
01:51:29,070 --> 01:51:32,840
and by the way it's useful to see what this eleven can comes from here

1734
01:51:33,510 --> 01:51:34,470
it comes from

1735
01:51:35,910 --> 01:51:40,220
that's second inequality wrote down which corresponds to the second column

1736
01:51:40,770 --> 01:51:46,580
that's the column like they use they have new greater than eleven before i did

1737
01:51:46,580 --> 01:51:48,640
the pivot so before i did that

1738
01:51:49,140 --> 01:51:49,870
that's same

1739
01:51:51,870 --> 01:51:58,120
give me the constraint that news go i did the pivot that's inverted the inequality

1740
01:51:58,380 --> 01:52:00,490
and now new is less than or equal to eleven

1741
01:52:01,240 --> 01:52:03,250
by doing my that correctly

1742
01:52:03,810 --> 01:52:05,720
by doing illegal pivot

1743
01:52:06,470 --> 01:52:07,700
legal primal

1744
01:52:11,210 --> 01:52:14,160
i preserves the property that for m u

1745
01:52:14,690 --> 01:52:16,810
tiny bit less and less than ten point nine

1746
01:52:17,390 --> 01:52:18,740
i preserves the property

1747
01:52:19,600 --> 01:52:20,190
that this is

1748
01:52:21,360 --> 01:52:22,400
as a legitimate

1749
01:52:23,280 --> 01:52:26,670
dictionary infected the pivot affects one thing at ten point nine

1750
01:52:27,370 --> 01:52:31,560
and so for me large enough close enough to eleven this should still be optimal

1751
01:52:31,560 --> 01:52:35,770
in fact equal to eleven this must still be optimal if i do the pivot

1752
01:52:36,990 --> 01:52:37,500
and i did

1753
01:52:37,500 --> 01:52:38,970
eight bits here

1754
01:52:38,990 --> 01:52:42,720
and i did not commit to a zero or one but i'm going to draw

1755
01:52:42,730 --> 01:52:46,940
this this is an zero one is the fraction of those in need right in

1756
01:52:46,940 --> 01:52:52,510
the middle east row here so i have to learn a little box around each

1757
01:52:52,510 --> 01:52:53,380
one of them

1758
01:52:53,400 --> 01:52:55,590
in this binary search way

1759
01:52:55,840 --> 01:53:00,140
OK and i was big exploited picture of what

1760
01:53:00,150 --> 01:53:03,830
well draw several times to represent a single byte in memory now the most interesting

1761
01:53:03,830 --> 01:53:08,280
thing to take take away from this drawing is that this little box here can

1762
01:53:08,280 --> 01:53:14,480
adopt one of two values independently of whatever value this box choose to adopt that

1763
01:53:14,500 --> 01:53:21,040
centre in factor eight independent choices free tibet assuming it makes sense to everybody came

1764
01:53:21,050 --> 01:53:24,620
that means that this as a group in a bunch of memory with it's a

1765
01:53:24,620 --> 01:53:27,190
bit like an independently taken zeros and ones

1766
01:53:27,210 --> 01:53:33,500
can distinguish between two vehicles for two hundred fifty six different values OK and that's

1767
01:53:33,500 --> 01:53:35,870
why they asked the table is as big as it is

1768
01:53:37,180 --> 01:53:40,850
sixty five to sixty five was twenty five represents

1769
01:53:41,080 --> 01:53:48,160
the alphabet forget where lower starts but every single character that the screen or printing

1770
01:53:48,160 --> 01:53:51,420
to a file is backed by some number i know you know that

1771
01:53:52,040 --> 01:53:58,470
when you look in memory to see how the capital a is represented you would

1772
01:53:58,470 --> 01:53:59,650
actually see

1773
01:53:59,700 --> 01:54:04,180
o one right there i'm sorry of africa was actually a one right there in

1774
01:54:04,200 --> 01:54:07,240
one right there are drawn to explain why that's the case

1775
01:54:07,760 --> 01:54:11,490
because capital is backed by the number sixty five we don't put things down and

1776
01:54:11,490 --> 01:54:15,150
s pole in memory we put them down in base two OK because that's what

1777
01:54:15,150 --> 01:54:19,060
that's the easy thing to represent in a bit oriented system

1778
01:54:19,080 --> 01:54:21,080
OK that makes it to people

1779
01:54:21,090 --> 01:54:24,020
OK so if i say the capitalized

1780
01:54:24,030 --> 01:54:28,650
is it sixty five to stop thinking about sixty five you have to think about

1781
01:54:28,710 --> 01:54:31,270
some some of perfect powers of two

1782
01:54:31,320 --> 01:54:32,750
so it isn't

1783
01:54:32,760 --> 01:54:37,500
sixty four nine thousand sixty five other it's actually sixty four plus one

1784
01:54:38,910 --> 01:54:41,410
is to the zeroth

1785
01:54:41,460 --> 01:54:45,840
two is to the the first there's none of that forest to the second is

1786
01:54:45,840 --> 01:54:48,170
to the third sixteen

1787
01:54:48,190 --> 01:54:53,290
it's for thirty two is five sixty four sixty is is actually to to six

1788
01:54:53,340 --> 01:54:55,250
plus two zero

1789
01:54:55,270 --> 01:54:56,260
make sense

1790
01:54:57,380 --> 01:55:05,020
as far as the representation in a box like this if you went down and

1791
01:55:05,020 --> 01:55:10,720
actually examined all the transistors OK the eight are laid side-by-side in a single character

1792
01:55:10,720 --> 01:55:13,760
but by the memory it would look like that

1793
01:55:13,770 --> 01:55:23,360
and in order to recover the the actual desk equivalent you really do do

1794
01:55:23,370 --> 01:55:26,590
you really do the the power series expansion

1795
01:55:26,600 --> 01:55:30,660
we say contribution of two to six because it's in the six

1796
01:55:30,670 --> 01:55:34,800
counting from zero from the right and the six position from from the end of

1797
01:55:34,800 --> 01:55:35,410
the by

1798
01:55:35,420 --> 01:55:37,300
this contributes

1799
01:55:37,310 --> 01:55:41,800
two this you can look at the location as having contributions of two the first

1800
01:55:41,800 --> 01:55:46,360
into the thirty seventh that are weighted by zero as opposed to one

1801
01:55:46,370 --> 01:55:48,550
OK that makes it to people

1802
01:55:48,560 --> 01:55:52,520
OK so that's good enough for characters

1803
01:55:52,530 --> 01:55:56,190
graduates two shorts

1804
01:55:56,200 --> 01:56:01,400
so people are very clever and when the shorter times that if you know store

1805
01:56:01,410 --> 01:56:05,920
lots of numbers and there are going to be small go with an array of

1806
01:56:05,920 --> 01:56:10,410
short for an hour or vector shorts knowing that they really will be some memory

1807
01:56:10,410 --> 01:56:12,630
savings later on

1808
01:56:12,650 --> 01:56:17,760
the short being two bytes just means the two neighbouring bytes in memory would be

1809
01:56:17,760 --> 01:56:19,110
laid down

1810
01:56:19,130 --> 01:56:22,770
those two bytes at the moment would be laid down and the two to the

1811
01:56:22,770 --> 01:56:27,120
sixteen different patterns that are available to be placed in that space

1812
01:56:27,170 --> 01:56:29,760
in this you can distinguish between

1813
01:56:29,780 --> 01:56:31,990
two the sixteen different values

1814
01:56:32,000 --> 01:56:33,450
that makes sense to people

1815
01:56:34,640 --> 01:56:36,710
so i'll just make this

1816
01:56:36,720 --> 01:56:41,100
put lots of zeros over here one right there

1817
01:56:41,170 --> 01:56:45,730
but many

1818
01:56:48,770 --> 01:56:51,180
and this is the wide

1819
01:56:52,610 --> 01:56:54,420
so as far as the number

1820
01:56:54,660 --> 01:56:58,530
that represents i should emphasise the technically

1821
01:56:58,550 --> 01:57:02,030
you can map that pattern to any number you want to as long as you

1822
01:57:02,030 --> 01:57:07,130
do it consistently but you want to make the computer hardware easy to interpret

1823
01:57:07,140 --> 01:57:10,810
this place right here means that there a contribution to the zero or one is

1824
01:57:10,810 --> 01:57:15,250
the convolution of two first contribution to the second so there is a two and

1825
01:57:15,260 --> 01:57:20,290
a four to be added together to the zero to the seventh to the eighth

1826
01:57:20,290 --> 01:57:21,510
to the night

1827
01:57:21,520 --> 01:57:24,750
OK so there actually is a contribution to the night

1828
01:57:24,770 --> 01:57:27,090
which is five hundred twelve

1829
01:57:28,440 --> 01:57:32,160
this really is the number that represented by this thing it would be

1830
01:57:32,180 --> 01:57:38,360
five twelve five sixteen five eighteen five nineteen we have that bit pattern down

1831
01:57:38,380 --> 01:57:40,550
OK that makes sense to people

1832
01:57:40,570 --> 01:57:44,290
if i have

1833
01:57:44,450 --> 01:57:46,800
another one

1834
01:57:46,820 --> 01:57:50,110
but some of their

1835
01:57:50,130 --> 01:57:54,290
i have one zero followed by all ones

1836
01:57:54,300 --> 01:57:56,670
and all one

1837
01:57:58,490 --> 01:57:59,250
i know

1838
01:57:59,270 --> 01:58:03,710
that if this had been a one right there then there would be no contribution

1839
01:58:03,710 --> 01:58:05,740
of two to fifteen

1840
01:58:05,780 --> 01:58:07,150
that makes it to people

1841
01:58:09,020 --> 01:58:11,200
zero followed by all ones

1842
01:58:11,210 --> 01:58:13,570
in in binary is like

1843
01:58:13,580 --> 01:58:17,780
zero being followed by all nines in some sense into small it's one less than

1844
01:58:17,780 --> 01:58:20,990
some perfect number that has a lot of zeros at the end

1845
01:58:21,010 --> 01:58:25,410
OK that makes sense but they about you have a binary o dominant on your

1846
01:58:25,480 --> 01:58:27,580
car and you want to take a mile off

1847
01:58:27,670 --> 01:58:29,190
OK because you're at

1848
01:58:29,310 --> 01:58:33,630
let's say one followed by fifteen zeros if you back it up you expect all

1849
01:58:33,690 --> 01:58:36,310
these to be demoted not to nine spata one

1850
01:58:37,320 --> 01:58:43,180
as far as the representation is concerned it's one less the two to fifteen

1851
01:58:43,190 --> 01:58:44,320
make sense

1852
01:58:44,340 --> 01:58:45,940
and that number is

1853
01:58:45,990 --> 01:58:47,790
two the fifteen minus one

1854
01:58:47,810 --> 01:58:51,120
which are not going to figure out what it is that we get the just

1855
01:58:51,120 --> 01:58:52,350
what's going on here

1856
01:58:52,370 --> 01:58:56,020
OK so that's enough there's

1857
01:58:56,030 --> 01:58:59,730
there is a little bit more to be said about this page right here

1858
01:58:59,780 --> 01:59:02,180
if i wanted to represent

1859
01:59:02,220 --> 01:59:07,480
the numbers zero to to the sixteen minus one i could do that OK that's

1860
01:59:07,490 --> 01:59:10,420
two sixty different values

1861
01:59:10,440 --> 01:59:13,510
i don't want to say the negative numbers are as common as positive numbers but

1862
01:59:13,510 --> 01:59:16,490
they're not so uncommon that we don't have a contribution

1863
01:59:16,620 --> 01:59:21,030
of the of the mapping to include negative numbers

1864
01:59:21,040 --> 01:59:25,780
so what usually happens is that this bit right there

1865
01:59:25,800 --> 01:59:30,320
has nothing to do with magnitude it's all about sign whether or not you want

1866
01:59:30,340 --> 01:59:35,110
zero there because it's positive or one for negative and that's usually what zero one

1867
01:59:35,110 --> 01:59:37,330
min when inside signed

1868
01:59:37,340 --> 01:59:38,400
make sense

1869
01:59:38,420 --> 01:59:41,760
OK so if i write down the this

1870
01:59:44,610 --> 01:59:51,690
and i have let's say four zeros followed by zero one one one that's seven

1871
01:59:51,710 --> 01:59:54,320
if i put all zeros there

1872
01:59:54,320 --> 01:59:56,780
this picture is the simple the this works fine

1873
01:59:56,800 --> 01:59:58,570
even if you're inserting

1874
01:59:58,590 --> 02:00:02,800
the same key over over

1875
02:00:03,950 --> 02:00:06,600
that seems good one thing we should clearly do

1876
02:00:06,650 --> 02:00:08,360
is inserted

1877
02:00:09,320 --> 02:00:11,410
into the bottomless

1878
02:00:11,430 --> 02:00:13,550
we now know where that

1879
02:00:13,640 --> 02:00:15,820
it should go

1880
02:00:16,360 --> 02:00:24,680
because we want to maintain this invariant the bottom list contains all the elements

1881
02:00:24,860 --> 02:00:28,760
so there we go to maintain the environment

1882
02:00:35,440 --> 02:00:40,640
this contains only on so

1883
02:00:40,700 --> 02:00:44,510
we search for seventy five percent seventy five goes here we just link in seventy

1884
02:00:44,510 --> 02:00:46,800
five you know how to do it close to home

1885
02:00:46,820 --> 02:00:48,970
so here is that pointer

1886
02:00:49,010 --> 02:00:55,570
that's all work on implementing skip list is the linked list manipulation

1887
02:00:55,600 --> 02:00:57,260
so i don't know

1888
02:00:59,160 --> 02:01:00,590
so the fine for now

1889
02:01:00,590 --> 02:01:03,370
because now there is only of a chain of length three here that have to

1890
02:01:03,370 --> 02:01:06,370
walk over here looking for something in this range

1891
02:01:06,410 --> 02:01:10,200
but if i just keep inserting o seventy five seventy six to seventy six percent

1892
02:01:10,200 --> 02:01:14,030
one six poster and so on and so on this pack all the elements and

1893
02:01:14,030 --> 02:01:18,760
here this channel a really long now suddenly things are not so balance applied research

1894
02:01:18,760 --> 02:01:20,010
or pay

1895
02:01:20,030 --> 02:01:21,200
an arbitrarily

1896
02:01:21,260 --> 02:01:23,510
long amount of time here to search for someone

1897
02:01:23,990 --> 02:01:28,010
things will take k time on sticks taylor again

1898
02:01:28,030 --> 02:01:31,640
only certain log analysts had OK for now

1899
02:01:31,660 --> 02:01:32,910
what i wanna do

1900
02:01:32,930 --> 02:01:37,470
is decide which of these lists contain seventy five

1901
02:01:37,470 --> 02:01:40,840
so clearly goes on the bottom every element because in the body should go up

1902
02:01:40,840 --> 02:01:42,340
a level

1903
02:01:45,870 --> 02:01:49,340
depends not clear yet my answer if you edited by some months ago in the

1904
02:01:49,340 --> 02:01:50,280
next level

1905
02:01:50,340 --> 02:01:51,590
so the goal and the

1906
02:01:51,620 --> 02:01:52,860
two levels of

1907
02:01:52,870 --> 02:01:55,990
maybe but even less likely

1908
02:01:57,740 --> 02:02:10,660
what should i do you

1909
02:02:10,720 --> 02:02:24,260
right so you maintain the ideal partition size which may be like the length of

1910
02:02:24,260 --> 02:02:25,390
the string

1911
02:02:25,410 --> 02:02:27,620
and you see well gets too long

1912
02:02:27,640 --> 02:02:30,780
and i should i should put in the middle promote the guy up to the

1913
02:02:30,780 --> 02:02:31,930
next level

1914
02:02:31,930 --> 02:02:35,760
and do the same thing up here this change it's too long between two consecutive

1915
02:02:35,780 --> 02:02:37,430
next level express stops

1916
02:02:38,260 --> 02:02:40,390
o promote the middle guy

1917
02:02:40,450 --> 02:02:41,660
and that's what you'll do

1918
02:02:41,660 --> 02:02:43,410
your problems

1919
02:02:43,410 --> 02:02:45,140
that's too fancy for me

1920
02:02:45,160 --> 02:02:51,910
know i mean i stinking counters

1921
02:02:51,970 --> 02:03:05,280
what else can i do

1922
02:03:06,510 --> 02:03:08,490
i try to maintain the ideal

1923
02:03:08,510 --> 02:03:11,720
skip was structure that will be too expensive because they will seventy five now is

1924
02:03:11,720 --> 02:03:13,780
the only gets promoted this guy gets

1925
02:03:13,870 --> 02:03:18,390
most all the way down and without propagate everything to the right and that that

1926
02:03:18,390 --> 02:03:20,470
could cost linear time of day

1927
02:03:20,470 --> 02:03:22,260
the idea

1928
02:03:24,200 --> 02:03:27,760
i one half the i could flip a coin

1929
02:03:27,820 --> 02:03:30,430
good idea

1930
02:03:30,450 --> 02:03:32,490
i for that

1931
02:03:32,510 --> 02:03:34,300
i'll give you a quarter

1932
02:03:34,300 --> 02:03:37,740
o thing is the color it's a good one

1933
02:03:37,760 --> 02:03:39,590
it's a

1934
02:03:39,640 --> 02:03:42,050
the old line state maryland

1935
02:03:46,320 --> 02:03:49,870
you have to perform services for that corner

1936
02:03:49,950 --> 02:03:53,390
namely for the coin can you for the coin

1937
02:03:55,140 --> 02:03:56,370
what you get

1938
02:03:56,390 --> 02:03:57,450
tales OK

1939
02:03:57,470 --> 02:03:59,090
first round

1940
02:04:00,720 --> 02:04:04,090
we're going to do is build skip list

1941
02:04:04,100 --> 02:04:08,120
maybe i should tell you our first came but the idea is to flip the

1942
02:04:09,200 --> 02:04:13,200
its hands so sorry if it has we will

1943
02:04:13,470 --> 02:04:18,490
promoted to the next level and again

1944
02:04:18,740 --> 02:04:22,320
so this is an answer to the question which

1945
02:04:22,320 --> 02:04:24,890
the other lists

1946
02:04:24,890 --> 02:04:30,140
optimisation problem so that will allow you to reconstruct x o

1947
02:04:30,160 --> 02:04:34,030
so this is the restricted isometry property

1948
02:04:34,040 --> 02:04:40,570
which is basically puts constraints on on the matrix here i used unfortunately different symbol

1949
02:04:40,690 --> 02:04:45,830
terror but replaced that of its feet are so if

1950
02:04:45,840 --> 02:04:47,380
this our IP

1951
02:04:47,390 --> 02:04:50,250
property exists which basically

1952
02:04:50,310 --> 02:04:56,890
say is in the third row all subsets of k columns are are nearly orthogonal

1953
02:04:57,340 --> 02:05:02,110
then one can use many algorithms for recovery indefinitely

1954
02:05:02,580 --> 02:05:07,680
there's one algorithm in particular which is known as the basis pursuit algorithm

1955
02:05:08,780 --> 02:05:13,330
which allows you to find the sparsest solutions now

1956
02:05:13,340 --> 02:05:19,340
this but then we have to know which of these metrices feasible have this IP

1957
02:05:19,350 --> 02:05:24,330
property OK so people have figured out for example of the components of the matrix

1958
02:05:24,330 --> 02:05:29,140
are made of random gaussians are how to mark and so on

1959
02:05:29,340 --> 02:05:34,610
we can have no construct this recovery algorithm in course

1960
02:05:35,160 --> 02:05:41,100
in practice there is no computationally elegant way to really check this property now

1961
02:05:41,110 --> 02:05:46,560
this IP property is is being interpreted in many different ways that is the argument

1962
02:05:46,560 --> 02:05:48,600
that people make about

1963
02:05:48,710 --> 02:05:55,440
incoherence between the basis vectors and this sparse signals and also something on honest phase

1964
02:05:55,440 --> 02:05:58,860
transition diagrams basically there's

1965
02:05:58,880 --> 02:06:01,250
when there is not a one

1966
02:06:01,260 --> 02:06:04,340
o thing cold and when it doesn't hold the

1967
02:06:04,350 --> 02:06:10,570
phase transition diagram is basically a car that separates these two domains and

1968
02:06:10,580 --> 02:06:14,210
i i p p is a very general property

1969
02:06:14,230 --> 02:06:16,340
you can hear if you see

1970
02:06:16,340 --> 02:06:17,500
i the

1971
02:06:17,510 --> 02:06:21,280
they are a key property has been defined using the l two norm can always

1972
02:06:21,280 --> 02:06:25,310
replace can also replace the l one norm and so forth and there is a

1973
02:06:25,310 --> 02:06:28,970
nice paper probably referred to later

1974
02:06:29,100 --> 02:06:34,910
which looks into ordinary be general enough lowercase p property so if you replace the

1975
02:06:34,910 --> 02:06:40,410
l two norm one then you can call it this one OK in english information

1976
02:06:40,410 --> 02:06:44,500
sharing kind of the problem the point actually shows up so this is relevant to

1977
02:06:45,100 --> 02:06:47,170
right so

1978
02:06:47,190 --> 02:06:52,430
the defeat what he can be has been looked at from the late seventies and

1979
02:06:52,430 --> 02:06:56,610
eighties and so forth and you will see now in the bottom of all my

1980
02:06:56,620 --> 02:07:02,050
view whereas i always mentioned with the slides come from because i sign

1981
02:07:02,060 --> 02:07:07,210
that give my permission for this lecture to be no videotape and it's going to

1982
02:07:07,210 --> 02:07:10,940
be on the head and that and so on and so i borrowed some of

1983
02:07:10,940 --> 02:07:12,560
the slides from

1984
02:07:12,570 --> 02:07:14,100
doctor j y so

1985
02:07:14,110 --> 02:07:15,720
i made sure that

1986
02:07:15,740 --> 02:07:21,210
is given proper credit for that so you will see these running acknowledgements or my

1987
02:07:21,210 --> 02:07:23,140
talk now

1988
02:07:23,170 --> 02:07:28,090
find how many such measurements tuning right again this is something that's been done here

1989
02:07:28,090 --> 02:07:30,750
is given

1990
02:07:30,780 --> 02:07:34,010
and one of the things you will see in the literature in terms of recovery

1991
02:07:34,010 --> 02:07:39,460
is always talk about high probability of reconstructions of the mathematical details are kind of

1992
02:07:39,460 --> 02:07:43,410
involved so i won't go into them to but for our purposes we can assume

1993
02:07:43,910 --> 02:07:49,130
that if you have this many compressive measurements and he has this property then you

1994
02:07:49,130 --> 02:07:56,160
can go through one optimisation approach for getting the original signal the the reason this

1995
02:07:56,160 --> 02:08:00,320
is kind of difficult thing for us to accept right away

1996
02:08:00,350 --> 02:08:02,640
is that when you take statistical

1997
02:08:02,670 --> 02:08:05,390
courses on estimation theory and so on

1998
02:08:05,410 --> 02:08:11,710
there are store collectlon of data about data we are wasting more data is useful

1999
02:08:11,730 --> 02:08:15,940
course even though we know that the signals that looking at spas

2000
02:08:16,110 --> 02:08:19,950
we always like to have more data and a very traditional

2001
02:08:19,980 --> 02:08:24,820
estimation problem the variance of the estimate goes down with the number of samples simple

2002
02:08:24,820 --> 02:08:28,200
scale and location estimation and so forth so

2003
02:08:28,210 --> 02:08:35,460
being able to just take the right number of measurements and recover noise is going

2004
02:08:35,460 --> 02:08:38,230
to take some getting used to now

2005
02:08:38,310 --> 02:08:43,110
the prime recovery problem is this given the so-called compressive measurements y now how do

2006
02:08:43,110 --> 02:08:44,130
you get

2007
02:08:44,170 --> 02:08:50,670
that's where the thing is going to be progress is right people who design

2008
02:08:50,680 --> 02:08:53,640
sensors to directly get y

2009
02:08:53,660 --> 02:08:55,790
are the ones who are going to be

2010
02:08:56,370 --> 02:08:57,720
more known

2011
02:08:57,810 --> 02:08:59,730
there will be more famous

2012
02:08:59,750 --> 02:09:06,060
in again the gradient cameras is an example of the caught aperture cameras that nineteen

2013
02:09:06,060 --> 02:09:11,990
years more building is some of the examples because with our current cameras that we

2014
02:09:11,990 --> 02:09:14,290
have always getting x

2015
02:09:14,360 --> 02:09:16,610
OK so but you need to have this

2016
02:09:16,780 --> 02:09:20,990
devices that give you why and then you should be able to get your ex

2017
02:09:20,990 --> 02:09:22,440
that's where the the

2018
02:09:22,450 --> 02:09:24,130
the core of the

2019
02:09:24,140 --> 02:09:30,300
you know the field is OK system but let's assume somebody gave us this why

2020
02:09:30,300 --> 02:09:35,470
and how it so how do you get your x scores you can try true

2021
02:09:35,530 --> 02:09:39,580
and it's not very useful here and you can try not

2022
02:09:39,640 --> 02:09:43,950
but then and not in l one they are a key property gives you the

2023
02:09:43,950 --> 02:09:47,330
sequence for so you will have framed the problem

2024
02:09:47,340 --> 02:09:51,130
as one of finding the reason why not is

